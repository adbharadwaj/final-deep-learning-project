{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from tensorflow.contrib import learn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BasicTextCNN(object):\n",
    "    \"\"\"\n",
    "    A Basic CNN for text classification.\n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    \n",
    "    Refer to https://arxiv.org/pdf/1408.5882.pdf for more details.\n",
    "    \"\"\"\n",
    "    def __init__(self, sequence_length, vocab_size, \n",
    "                 num_classes=2, embedding_size=128, filter_sizes=[3,4,5], \n",
    "                 num_filters=128, batch_size=64, \n",
    "                 l2_reg_lambda=0.0, num_epochs=200,\n",
    "                 num_checkpoints=5, dropout_prob=0.5, \n",
    "                 checkpoint_every=100, evaluate_every=100, \n",
    "                 allow_soft_placement=True,log_device_placement=False,\n",
    "                 results_dir=\"runs\"):\n",
    "        \n",
    "        tf.reset_default_graph() \n",
    "        self.sequence_length = sequence_length\n",
    "        self.num_classes = num_classes\n",
    "        self.vocab_size = vocab_size\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.filter_sizes = filter_sizes\n",
    "        self.num_filters = num_filters\n",
    "        self.l2_reg_lambda = l2_reg_lambda\n",
    "        self.num_epochs = num_epochs\n",
    "        self.results_dir = results_dir\n",
    "        \n",
    "        self.num_checkpoints = num_checkpoints\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.checkpoint_every = checkpoint_every\n",
    "        self.evaluate_every = evaluate_every\n",
    "        \n",
    "        self.allow_soft_placement = allow_soft_placement\n",
    "        self.log_device_placement = log_device_placement\n",
    "        \n",
    "        self.sess = tf.Session()\n",
    "        self._build_network()\n",
    "        \n",
    "    def _build_network(self):\n",
    "        \n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, self.sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, self.num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        self.l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            self.W = tf.Variable(tf.random_uniform([self.vocab_size, self.embedding_size], -1.0, 1.0),name=\"W\") \n",
    "            self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "\n",
    "        # Create a convolution + maxpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(self.filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, self.embedding_size, 1, self.num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[self.num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, self.sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = self.num_filters * len(self.filter_sizes)\n",
    "        self.h_pool = tf.concat(pooled_outputs, 3)\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "\n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "        # Final (unnormalized) scores and predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters_total, self.num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[self.num_classes]), name=\"b\")\n",
    "            self.l2_loss += tf.nn.l2_loss(W)\n",
    "            self.l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "            \n",
    "    def train_network(self, x_train, y_train, x_dev, y_dev):\n",
    "        # Calculate mean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            class_weight = tf.constant([1.0, 100.0])\n",
    "            weights = tf.reduce_sum(class_weight * self.input_y, axis=1)\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "            weighted_losses = losses * weights\n",
    "            self.loss = tf.reduce_mean(weighted_losses) + self.l2_reg_lambda * self.l2_loss\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "            _, self.precision = tf.metrics.precision(labels=tf.argmax(self.input_y, 1), predictions=self.predictions, name='precision')\n",
    "            _, self.recall = tf.metrics.recall(labels=tf.argmax(self.input_y, 1), predictions=self.predictions, name='recall')\n",
    "            \n",
    "        # Define Training procedure\n",
    "        self.global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(self.loss)\n",
    "        self.train_op = optimizer.apply_gradients(grads_and_vars, global_step=self.global_step)\n",
    "\n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, self.results_dir, timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", self.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", self.accuracy)\n",
    "        precision_summary = tf.summary.scalar(\"precision\", self.precision)\n",
    "        recall_summary = tf.summary.scalar(\"recall\", self.recall)\n",
    "\n",
    "        # Train Summaries\n",
    "        self.train_summary_op = tf.summary.merge([loss_summary, acc_summary, precision_summary, recall_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        self.train_summary_writer = tf.summary.FileWriter(train_summary_dir, self.sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        self.dev_summary_op = tf.summary.merge([loss_summary, acc_summary, precision_summary, recall_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        self.dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, self.sess.graph)\n",
    "        \n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=self.num_checkpoints)\n",
    "        \n",
    "        # Write vocabulary\n",
    "        vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "        \n",
    "        # Initialize all variables\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.sess.run(tf.local_variables_initializer())\n",
    "    \n",
    "        print(\"Start training\")\n",
    "        # Generate batches\n",
    "        batches = BasicTextCNN.batch_iter(\n",
    "            list(zip(x_train, y_train)), self.batch_size, self.num_epochs)\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            self.train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(self.sess, self.global_step)\n",
    "            if current_step % self.evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                self.dev_step(x_dev, y_dev, writer=self.dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % self.checkpoint_every == 0:\n",
    "                path = saver.save(self.sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))   \n",
    "        print(\"Training finished\")\n",
    "    \n",
    "    def train_step(self, x_batch, y_batch):\n",
    "        \"\"\"\n",
    "        A single training step\n",
    "        \"\"\"\n",
    "        feed_dict = {\n",
    "          self.input_x: x_batch,\n",
    "          self.input_y: y_batch,\n",
    "          self.dropout_keep_prob: self.dropout_prob\n",
    "        }\n",
    "        _, step, summaries, loss, accuracy, precision, recall = self.sess.run(\n",
    "            [self.train_op, self.global_step, self.train_summary_op, self.loss, self.accuracy, self.precision, self.recall],\n",
    "            feed_dict)\n",
    "        time_str = datetime.datetime.now().isoformat()\n",
    "        print(\"{}: step {}, loss {:g}, acc {:g}, prec {:g}, recall {:g}\".format(time_str, step, loss, accuracy, precision, recall))\n",
    "        self.train_summary_writer.add_summary(summaries, step)\n",
    "        \n",
    "    \n",
    "    def dev_step(self, x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              self.input_x: x_batch,\n",
    "              self.input_y: y_batch,\n",
    "              self.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, accuracy,  precision, recall  = self.sess.run(\n",
    "                [self.global_step, self.dev_summary_op, self.loss, self.accuracy, self.precision, self.recall],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}, prec {:g}, recall {:g}\".format(time_str, step, loss, accuracy, precision, recall))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "                \n",
    "    @staticmethod            \n",
    "    def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "        \"\"\"\n",
    "        Generates a batch iterator for a dataset.\n",
    "        \"\"\"\n",
    "        data = np.array(data)\n",
    "        data_size = len(data)\n",
    "        num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            # Shuffle the data at each epoch\n",
    "            if shuffle:\n",
    "                shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "                shuffled_data = data[shuffle_indices]\n",
    "            else:\n",
    "                shuffled_data = data\n",
    "            for batch_num in range(num_batches_per_epoch):\n",
    "                start_index = batch_num * batch_size\n",
    "                end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "                yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "pathway_to_genes_dict = pickle.load(open( \"data/pathway_to_genes_dict.p\", \"rb\" ))\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "\n",
    "def load_data_and_labels():\n",
    "    x_text = sentence_support_df.tokenizedSentenceFromPaper.as_matrix()\n",
    "    y = sentence_support_df.label.as_matrix()\n",
    "    y = [[0, 1] if x == 1 else [1, 0] for x in y  ]\n",
    "    return [x_text, np.array(y)]\n",
    "\n",
    "def compute_pathway_name_terms(pathway):\n",
    "    pathway = pathway.replace('signaling', '').replace('pathway', '').replace('-', ' ')\n",
    "    return [t for t in pathway.lower().strip().split() if len(t)>1]\n",
    "\n",
    "def tokenize_pathway_names(sentence, pathwayA, pathwayB):\n",
    "    genesA = [gene.lower() for gene in pathway_to_genes_dict[pathwayA]] + compute_pathway_name_terms(pathwayA)\n",
    "    genesB = [gene.lower() for gene in pathway_to_genes_dict[pathwayB]] + compute_pathway_name_terms(pathwayB)\n",
    "    tokenized_sentence = []\n",
    "    for word in sentence.lower().split():\n",
    "        token = None\n",
    "        for gene in genesA:\n",
    "            if gene in word:\n",
    "                token = 'pathwayA'\n",
    "                break\n",
    "                \n",
    "        for gene in genesB:\n",
    "            if gene in word:\n",
    "                token = 'pathwayB'\n",
    "                break\n",
    "        if token is None:\n",
    "            token = word\n",
    "        tokenized_sentence.append(token)\n",
    "    return ' '.join(tokenized_sentence)\n",
    "\n",
    "sentence_support_df = pd.read_csv('data/sentence_support_v3.tsv', delimiter='\\t')\n",
    "sentence_support_df.drop_duplicates(inplace=True)\n",
    "sentence_support_df['tokenizedSentenceFromPaper'] = sentence_support_df.apply(lambda x: tokenize_pathway_names(x.sentenceFromPaper, x.pathwayA, x.pathwayB), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "x_text, y = load_data_and_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 33447\n",
      "Train/Dev split: 31796/10598\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary\n",
    "max_document_length = max([len(x.split(\" \")) for x in x_text])\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "x = np.array(list(vocab_processor.fit_transform(x_text)))\n",
    "\n",
    "# Randomly shuffle data\n",
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "\n",
    "# Split train/test set\n",
    "# TODO: This is very crude, should use cross-validation\n",
    "dev_sample_index = -1 * int(0.25 * float(len(y)))\n",
    "x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
    "print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# np.save(os.path.join(out_dir, 'x_train.npy'), x_train)\n",
    "# np.save(os.path.join(out_dir, 'x_dev.npy'), x_dev)\n",
    "# np.save(os.path.join(out_dir, 'y_train.npy'), y_train)\n",
    "# np.save(os.path.join(out_dir, 'y_dev.npy'), y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/runs/1512704664\n",
      "\n",
      "Start training\n",
      "2017-12-07T22:44:27.458392: step 1, loss 10.6738, acc 0.390625, prec 0, recall 0\n",
      "2017-12-07T22:44:27.964808: step 2, loss 1.9099, acc 0.53125, prec 0.0285714, recall 0.666667\n",
      "2017-12-07T22:44:28.456811: step 3, loss 7.39634, acc 0.625, prec 0.0215054, recall 0.5\n",
      "2017-12-07T22:44:28.955566: step 4, loss 8.8241, acc 0.609375, prec 0.0254237, recall 0.5\n",
      "2017-12-07T22:44:29.439466: step 5, loss 14.4959, acc 0.5, prec 0.0202703, recall 0.375\n",
      "2017-12-07T22:44:29.934743: step 6, loss 9.01997, acc 0.421875, prec 0.0163043, recall 0.333333\n",
      "2017-12-07T22:44:30.462444: step 7, loss 3.32859, acc 0.46875, prec 0.0138249, recall 0.3\n",
      "2017-12-07T22:44:30.966835: step 8, loss 3.61725, acc 0.28125, prec 0.0114068, recall 0.3\n",
      "2017-12-07T22:44:31.478553: step 9, loss 4.18757, acc 0.265625, prec 0.0160256, recall 0.416667\n",
      "2017-12-07T22:44:31.970079: step 10, loss 4.79822, acc 0.234375, prec 0.0138504, recall 0.416667\n",
      "2017-12-07T22:44:32.466608: step 11, loss 4.9128, acc 0.21875, prec 0.0121655, recall 0.416667\n",
      "2017-12-07T22:44:33.024448: step 12, loss 4.55357, acc 0.203125, prec 0.0108225, recall 0.416667\n",
      "2017-12-07T22:44:33.527374: step 13, loss 4.6396, acc 0.1875, prec 0.0135659, recall 0.5\n",
      "2017-12-07T22:44:34.037797: step 14, loss 3.39204, acc 0.296875, prec 0.0142349, recall 0.533333\n",
      "2017-12-07T22:44:34.547290: step 15, loss 4.34496, acc 0.203125, prec 0.0130506, recall 0.533333\n",
      "2017-12-07T22:44:35.046630: step 16, loss 11.3612, acc 0.28125, prec 0.0136571, recall 0.529412\n",
      "2017-12-07T22:44:35.573414: step 17, loss 4.09999, acc 0.21875, prec 0.0126939, recall 0.529412\n",
      "2017-12-07T22:44:36.105619: step 18, loss 3.18773, acc 0.34375, prec 0.011984, recall 0.529412\n",
      "2017-12-07T22:44:36.702618: step 19, loss 2.90366, acc 0.34375, prec 0.0125945, recall 0.555556\n",
      "2017-12-07T22:44:37.222642: step 20, loss 3.09073, acc 0.359375, prec 0.011976, recall 0.555556\n",
      "2017-12-07T22:44:37.733077: step 21, loss 6.06106, acc 0.359375, prec 0.0159272, recall 0.608696\n",
      "2017-12-07T22:44:38.253120: step 22, loss 2.13048, acc 0.484375, prec 0.0153509, recall 0.608696\n",
      "2017-12-07T22:44:38.761954: step 23, loss 1.63364, acc 0.53125, prec 0.014862, recall 0.608696\n",
      "2017-12-07T22:44:39.248315: step 24, loss 1.54004, acc 0.703125, prec 0.0166147, recall 0.64\n",
      "2017-12-07T22:44:39.763316: step 25, loss 1.99377, acc 0.5, prec 0.0160804, recall 0.64\n",
      "2017-12-07T22:44:40.269887: step 26, loss 12.1399, acc 0.578125, prec 0.0156709, recall 0.615385\n",
      "2017-12-07T22:44:40.767215: step 27, loss 1.07734, acc 0.671875, prec 0.0153551, recall 0.615385\n",
      "2017-12-07T22:44:41.264791: step 28, loss 9.30803, acc 0.59375, prec 0.0149953, recall 0.592593\n",
      "2017-12-07T22:44:41.768109: step 29, loss 1.15562, acc 0.671875, prec 0.0156107, recall 0.607143\n",
      "2017-12-07T22:44:42.326422: step 30, loss 17.7277, acc 0.671875, prec 0.0153291, recall 0.586207\n",
      "2017-12-07T22:44:42.861933: step 31, loss 34.4945, acc 0.671875, prec 0.0159574, recall 0.545455\n",
      "2017-12-07T22:44:43.399064: step 32, loss 9.65635, acc 0.6875, prec 0.016565, recall 0.527778\n",
      "2017-12-07T22:44:43.910023: step 33, loss 1.95124, acc 0.53125, prec 0.0161427, recall 0.527778\n",
      "2017-12-07T22:44:44.402246: step 34, loss 2.42443, acc 0.484375, prec 0.0165153, recall 0.540541\n",
      "2017-12-07T22:44:44.893784: step 35, loss 3.63253, acc 0.296875, prec 0.0159236, recall 0.540541\n",
      "2017-12-07T22:44:45.388747: step 36, loss 3.75295, acc 0.28125, prec 0.015361, recall 0.540541\n",
      "2017-12-07T22:44:45.875916: step 37, loss 7.20705, acc 0.34375, prec 0.0156366, recall 0.525\n",
      "2017-12-07T22:44:46.381806: step 38, loss 4.46785, acc 0.15625, prec 0.0157368, recall 0.536585\n",
      "2017-12-07T22:44:46.876280: step 39, loss 4.65615, acc 0.234375, prec 0.015884, recall 0.547619\n",
      "2017-12-07T22:44:47.385822: step 40, loss 4.95246, acc 0.15625, prec 0.0159681, recall 0.55814\n",
      "2017-12-07T22:44:47.874038: step 41, loss 4.38122, acc 0.21875, prec 0.0167203, recall 0.577778\n",
      "2017-12-07T22:44:48.373052: step 42, loss 5.29187, acc 0.21875, prec 0.016812, recall 0.586957\n",
      "2017-12-07T22:44:48.861620: step 43, loss 5.42846, acc 0.15625, prec 0.0180397, recall 0.612245\n",
      "2017-12-07T22:44:49.353168: step 44, loss 4.06675, acc 0.21875, prec 0.0180863, recall 0.62\n",
      "2017-12-07T22:44:49.872986: step 45, loss 4.62994, acc 0.1875, prec 0.0175538, recall 0.62\n",
      "2017-12-07T22:44:50.374018: step 46, loss 3.28893, acc 0.296875, prec 0.0171176, recall 0.62\n",
      "2017-12-07T22:44:50.871569: step 47, loss 2.71033, acc 0.34375, prec 0.01726, recall 0.627451\n",
      "2017-12-07T22:44:51.376562: step 48, loss 3.31013, acc 0.34375, prec 0.0173959, recall 0.634615\n",
      "2017-12-07T22:44:51.882116: step 49, loss 3.68157, acc 0.40625, prec 0.017562, recall 0.641509\n",
      "2017-12-07T22:44:52.392695: step 50, loss 2.89, acc 0.375, prec 0.0172065, recall 0.641509\n",
      "2017-12-07T22:44:52.896559: step 51, loss 2.82191, acc 0.484375, prec 0.0174129, recall 0.648148\n",
      "2017-12-07T22:44:53.408048: step 52, loss 10.825, acc 0.4375, prec 0.0171149, recall 0.636364\n",
      "2017-12-07T22:44:53.919244: step 53, loss 2.03463, acc 0.515625, prec 0.0168593, recall 0.636364\n",
      "2017-12-07T22:44:54.430806: step 54, loss 1.94231, acc 0.5625, prec 0.0171021, recall 0.642857\n",
      "2017-12-07T22:44:54.957997: step 55, loss 5.12208, acc 0.640625, prec 0.0173872, recall 0.637931\n",
      "2017-12-07T22:44:55.475968: step 56, loss 4.48832, acc 0.609375, prec 0.0171933, recall 0.627119\n",
      "2017-12-07T22:44:55.997122: step 57, loss 1.63414, acc 0.59375, prec 0.0174392, recall 0.633333\n",
      "2017-12-07T22:44:56.552630: step 58, loss 1.67274, acc 0.609375, prec 0.0172414, recall 0.633333\n",
      "2017-12-07T22:44:57.084028: step 59, loss 7.70854, acc 0.59375, prec 0.0174888, recall 0.629032\n",
      "2017-12-07T22:44:57.618318: step 60, loss 9.35173, acc 0.546875, prec 0.0177069, recall 0.625\n",
      "2017-12-07T22:44:58.142563: step 61, loss 1.99122, acc 0.578125, prec 0.0174978, recall 0.625\n",
      "2017-12-07T22:44:58.669902: step 62, loss 2.27463, acc 0.5, prec 0.0172563, recall 0.625\n",
      "2017-12-07T22:44:59.197179: step 63, loss 9.61684, acc 0.515625, prec 0.017043, recall 0.606061\n",
      "2017-12-07T22:44:59.731518: step 64, loss 2.5893, acc 0.453125, prec 0.0167926, recall 0.606061\n",
      "2017-12-07T22:45:00.276436: step 65, loss 2.22877, acc 0.453125, prec 0.0165494, recall 0.606061\n",
      "2017-12-07T22:45:00.808834: step 66, loss 12.4936, acc 0.53125, prec 0.0171639, recall 0.6\n",
      "2017-12-07T22:45:01.365459: step 67, loss 2.53131, acc 0.53125, prec 0.0173527, recall 0.605634\n",
      "2017-12-07T22:45:01.888161: step 68, loss 3.50024, acc 0.3125, prec 0.01705, recall 0.605634\n",
      "2017-12-07T22:45:02.415293: step 69, loss 3.2871, acc 0.328125, prec 0.0171473, recall 0.611111\n",
      "2017-12-07T22:45:02.955970: step 70, loss 3.83075, acc 0.359375, prec 0.0172546, recall 0.616438\n",
      "2017-12-07T22:45:03.493961: step 71, loss 3.39178, acc 0.3125, prec 0.0169683, recall 0.616438\n",
      "2017-12-07T22:45:04.062385: step 72, loss 4.38141, acc 0.234375, prec 0.0166605, recall 0.616438\n",
      "2017-12-07T22:45:04.607766: step 73, loss 4.60645, acc 0.234375, prec 0.0163636, recall 0.616438\n",
      "2017-12-07T22:45:05.166993: step 74, loss 3.19663, acc 0.34375, prec 0.0168218, recall 0.626667\n",
      "2017-12-07T22:45:05.706934: step 75, loss 3.52504, acc 0.234375, prec 0.0168776, recall 0.631579\n",
      "2017-12-07T22:45:06.247110: step 76, loss 3.76264, acc 0.359375, prec 0.017319, recall 0.641026\n",
      "2017-12-07T22:45:06.829574: step 77, loss 3.26434, acc 0.296875, prec 0.0170532, recall 0.641026\n",
      "2017-12-07T22:45:07.394326: step 78, loss 3.20974, acc 0.359375, prec 0.016818, recall 0.641026\n",
      "2017-12-07T22:45:07.947328: step 79, loss 2.54594, acc 0.484375, prec 0.0166334, recall 0.641026\n",
      "2017-12-07T22:45:08.496161: step 80, loss 4.07027, acc 0.5, prec 0.0164636, recall 0.632911\n",
      "2017-12-07T22:45:09.040157: step 81, loss 8.79185, acc 0.53125, prec 0.0163079, recall 0.625\n",
      "2017-12-07T22:45:09.590136: step 82, loss 1.16647, acc 0.6875, prec 0.0165209, recall 0.62963\n",
      "2017-12-07T22:45:10.146762: step 83, loss 5.01262, acc 0.578125, prec 0.0163829, recall 0.621951\n",
      "2017-12-07T22:45:10.702705: step 84, loss 1.70767, acc 0.578125, prec 0.016242, recall 0.621951\n",
      "2017-12-07T22:45:11.254458: step 85, loss 1.99056, acc 0.53125, prec 0.0163986, recall 0.626506\n",
      "2017-12-07T22:45:11.812838: step 86, loss 1.56784, acc 0.640625, prec 0.0162805, recall 0.626506\n",
      "2017-12-07T22:45:12.362979: step 87, loss 6.6838, acc 0.6875, prec 0.0164904, recall 0.623529\n",
      "2017-12-07T22:45:12.922023: step 88, loss 1.47042, acc 0.640625, prec 0.0163732, recall 0.623529\n",
      "2017-12-07T22:45:13.480297: step 89, loss 1.41914, acc 0.625, prec 0.0162527, recall 0.623529\n",
      "2017-12-07T22:45:14.040018: step 90, loss 1.50079, acc 0.671875, prec 0.0164484, recall 0.627907\n",
      "2017-12-07T22:45:14.597150: step 91, loss 1.10154, acc 0.734375, prec 0.0163636, recall 0.627907\n",
      "2017-12-07T22:45:15.158902: step 92, loss 4.0956, acc 0.6875, prec 0.0165663, recall 0.625\n",
      "2017-12-07T22:45:15.714453: step 93, loss 1.69406, acc 0.71875, prec 0.0167715, recall 0.629214\n",
      "2017-12-07T22:45:16.263987: step 94, loss 1.27618, acc 0.625, prec 0.0166518, recall 0.629214\n",
      "2017-12-07T22:45:16.870153: step 95, loss 1.30182, acc 0.640625, prec 0.0168291, recall 0.633333\n",
      "2017-12-07T22:45:17.448816: step 96, loss 9.72102, acc 0.65625, prec 0.0167254, recall 0.626374\n",
      "2017-12-07T22:45:18.018114: step 97, loss 1.30191, acc 0.65625, prec 0.0166181, recall 0.626374\n",
      "2017-12-07T22:45:18.580042: step 98, loss 6.11429, acc 0.625, prec 0.0165074, recall 0.619565\n",
      "2017-12-07T22:45:19.146095: step 99, loss 1.12043, acc 0.65625, prec 0.0164029, recall 0.619565\n",
      "2017-12-07T22:45:19.712501: step 100, loss 7.42598, acc 0.71875, prec 0.0168861, recall 0.621053\n",
      "Saved model checkpoint to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/runs/1512704664/checkpoints/model-100\n",
      "\n",
      "2017-12-07T22:45:21.072433: step 101, loss 1.42816, acc 0.5625, prec 0.0170309, recall 0.625\n",
      "2017-12-07T22:45:21.645820: step 102, loss 1.30159, acc 0.765625, prec 0.0172365, recall 0.628866\n",
      "2017-12-07T22:45:22.204137: step 103, loss 1.51083, acc 0.640625, prec 0.0171252, recall 0.628866\n",
      "2017-12-07T22:45:22.765622: step 104, loss 1.81497, acc 0.625, prec 0.0170106, recall 0.628866\n",
      "2017-12-07T22:45:23.320207: step 105, loss 2.25403, acc 0.5, prec 0.0171318, recall 0.632653\n",
      "2017-12-07T22:45:23.876165: step 106, loss 1.72238, acc 0.5625, prec 0.0170003, recall 0.632653\n",
      "2017-12-07T22:45:24.426018: step 107, loss 1.76059, acc 0.53125, prec 0.0168616, recall 0.632653\n",
      "2017-12-07T22:45:24.989805: step 108, loss 1.34668, acc 0.609375, prec 0.0170132, recall 0.636364\n",
      "2017-12-07T22:45:25.553482: step 109, loss 1.61802, acc 0.609375, prec 0.0171628, recall 0.64\n",
      "2017-12-07T22:45:26.113138: step 110, loss 1.61649, acc 0.625, prec 0.0173149, recall 0.643564\n",
      "2017-12-07T22:45:26.718501: step 111, loss 1.82202, acc 0.5625, prec 0.0174465, recall 0.647059\n",
      "2017-12-07T22:45:27.367298: step 112, loss 1.32215, acc 0.65625, prec 0.0173456, recall 0.647059\n",
      "2017-12-07T22:45:27.989415: step 113, loss 3.7054, acc 0.671875, prec 0.0175118, recall 0.644231\n",
      "2017-12-07T22:45:28.597895: step 114, loss 6.29711, acc 0.65625, prec 0.0174162, recall 0.638095\n",
      "2017-12-07T22:45:29.180084: step 115, loss 10.679, acc 0.5625, prec 0.0172948, recall 0.632075\n",
      "2017-12-07T22:45:29.779328: step 116, loss 1.00183, acc 0.765625, prec 0.0174807, recall 0.635514\n",
      "2017-12-07T22:45:30.361943: step 117, loss 3.39565, acc 0.765625, prec 0.017418, recall 0.62963\n",
      "2017-12-07T22:45:30.942399: step 118, loss 6.77089, acc 0.546875, prec 0.017294, recall 0.623853\n",
      "2017-12-07T22:45:31.516112: step 119, loss 0.999664, acc 0.6875, prec 0.0172065, recall 0.623853\n",
      "2017-12-07T22:45:32.072257: step 120, loss 1.30001, acc 0.546875, prec 0.017328, recall 0.627273\n",
      "2017-12-07T22:45:32.631719: step 121, loss 1.6748, acc 0.640625, prec 0.0172285, recall 0.627273\n",
      "2017-12-07T22:45:33.190300: step 122, loss 2.5218, acc 0.546875, prec 0.0173482, recall 0.630631\n",
      "2017-12-07T22:45:33.751337: step 123, loss 2.1024, acc 0.484375, prec 0.0172075, recall 0.630631\n",
      "2017-12-07T22:45:34.325205: step 124, loss 2.1296, acc 0.46875, prec 0.0170648, recall 0.630631\n",
      "2017-12-07T22:45:34.891820: step 125, loss 2.29086, acc 0.5, prec 0.0169328, recall 0.630631\n",
      "2017-12-07T22:45:35.458081: step 126, loss 1.00914, acc 0.671875, prec 0.0168472, recall 0.630631\n",
      "2017-12-07T22:45:36.019912: step 127, loss 2.0079, acc 0.5625, prec 0.0167344, recall 0.630631\n",
      "2017-12-07T22:45:36.594331: step 128, loss 3.463, acc 0.59375, prec 0.0173314, recall 0.640351\n",
      "2017-12-07T22:45:37.192249: step 129, loss 1.57896, acc 0.59375, prec 0.0174569, recall 0.643478\n",
      "2017-12-07T22:45:37.778318: step 130, loss 7.25473, acc 0.59375, prec 0.0173546, recall 0.637931\n",
      "2017-12-07T22:45:38.374567: step 131, loss 1.60127, acc 0.609375, prec 0.0177115, recall 0.644068\n",
      "2017-12-07T22:45:38.955549: step 132, loss 7.04103, acc 0.578125, prec 0.0178323, recall 0.641667\n",
      "2017-12-07T22:45:39.526437: step 133, loss 1.89619, acc 0.578125, prec 0.0177215, recall 0.641667\n",
      "2017-12-07T22:45:40.099157: step 134, loss 2.22987, acc 0.546875, prec 0.017604, recall 0.641667\n",
      "2017-12-07T22:45:40.690542: step 135, loss 2.33861, acc 0.53125, prec 0.0174841, recall 0.641667\n",
      "2017-12-07T22:45:41.264527: step 136, loss 2.55176, acc 0.453125, prec 0.0177888, recall 0.647541\n",
      "2017-12-07T22:45:41.844326: step 137, loss 1.38488, acc 0.59375, prec 0.0179051, recall 0.650406\n",
      "2017-12-07T22:45:42.422758: step 138, loss 5.25478, acc 0.53125, prec 0.018008, recall 0.648\n",
      "2017-12-07T22:45:43.003399: step 139, loss 1.64219, acc 0.625, prec 0.0179124, recall 0.648\n",
      "2017-12-07T22:45:43.584102: step 140, loss 1.76965, acc 0.515625, prec 0.0180061, recall 0.650794\n",
      "2017-12-07T22:45:44.154573: step 141, loss 1.41216, acc 0.640625, prec 0.0183446, recall 0.65625\n",
      "2017-12-07T22:45:44.707029: step 142, loss 1.9613, acc 0.546875, prec 0.0182292, recall 0.65625\n",
      "2017-12-07T22:45:45.280741: step 143, loss 2.16721, acc 0.5, prec 0.018315, recall 0.658915\n",
      "2017-12-07T22:45:45.865611: step 144, loss 1.42209, acc 0.578125, prec 0.0184194, recall 0.661538\n",
      "2017-12-07T22:45:46.438083: step 145, loss 12.5116, acc 0.65625, prec 0.0183369, recall 0.656489\n",
      "2017-12-07T22:45:47.061735: step 146, loss 1.66447, acc 0.53125, prec 0.0184283, recall 0.659091\n",
      "2017-12-07T22:45:47.661186: step 147, loss 1.42424, acc 0.671875, prec 0.0185537, recall 0.661654\n",
      "2017-12-07T22:45:48.294371: step 148, loss 2.40688, acc 0.609375, prec 0.0188679, recall 0.666667\n",
      "2017-12-07T22:45:48.879480: step 149, loss 0.999111, acc 0.734375, prec 0.0188009, recall 0.666667\n",
      "2017-12-07T22:45:49.457391: step 150, loss 1.54292, acc 0.671875, prec 0.0189229, recall 0.669118\n",
      "2017-12-07T22:45:50.040677: step 151, loss 1.6318, acc 0.609375, prec 0.018825, recall 0.669118\n",
      "2017-12-07T22:45:50.611371: step 152, loss 0.720031, acc 0.703125, prec 0.0187513, recall 0.669118\n",
      "2017-12-07T22:45:51.180672: step 153, loss 5.15128, acc 0.640625, prec 0.0190691, recall 0.669065\n",
      "2017-12-07T22:45:51.759448: step 154, loss 1.09026, acc 0.71875, prec 0.0191993, recall 0.671429\n",
      "2017-12-07T22:45:52.339170: step 155, loss 0.750536, acc 0.765625, prec 0.0191407, recall 0.671429\n",
      "2017-12-07T22:45:52.914580: step 156, loss 1.0576, acc 0.75, prec 0.0192776, recall 0.673759\n",
      "2017-12-07T22:45:53.491357: step 157, loss 15.8209, acc 0.65625, prec 0.0193979, recall 0.666667\n",
      "2017-12-07T22:45:54.057722: step 158, loss 1.00266, acc 0.734375, prec 0.0193315, recall 0.666667\n",
      "2017-12-07T22:45:54.636093: step 159, loss 1.29496, acc 0.640625, prec 0.0194389, recall 0.668966\n",
      "2017-12-07T22:45:55.195552: step 160, loss 9.38162, acc 0.5625, prec 0.0195336, recall 0.662162\n",
      "2017-12-07T22:45:55.758777: step 161, loss 2.21508, acc 0.53125, prec 0.0194175, recall 0.662162\n",
      "2017-12-07T22:45:56.336349: step 162, loss 1.96012, acc 0.53125, prec 0.0196889, recall 0.666667\n",
      "2017-12-07T22:45:56.917246: step 163, loss 5.32067, acc 0.546875, prec 0.0199648, recall 0.666667\n",
      "2017-12-07T22:45:57.491264: step 164, loss 3.0854, acc 0.359375, prec 0.0199961, recall 0.668831\n",
      "2017-12-07T22:45:58.070280: step 165, loss 3.45779, acc 0.34375, prec 0.0200231, recall 0.670968\n",
      "2017-12-07T22:45:58.636806: step 166, loss 3.58231, acc 0.359375, prec 0.0202406, recall 0.675159\n",
      "2017-12-07T22:45:59.214400: step 167, loss 2.66255, acc 0.5, prec 0.0203036, recall 0.677215\n",
      "2017-12-07T22:45:59.777656: step 168, loss 3.08158, acc 0.375, prec 0.0203352, recall 0.679245\n",
      "2017-12-07T22:46:00.365944: step 169, loss 4.32973, acc 0.34375, prec 0.0203586, recall 0.68125\n",
      "2017-12-07T22:46:00.942426: step 170, loss 3.03949, acc 0.390625, prec 0.0205746, recall 0.685185\n",
      "2017-12-07T22:46:01.532678: step 171, loss 3.66059, acc 0.328125, prec 0.0204119, recall 0.685185\n",
      "2017-12-07T22:46:02.097888: step 172, loss 2.87299, acc 0.5, prec 0.0204716, recall 0.687117\n",
      "2017-12-07T22:46:02.659587: step 173, loss 2.89797, acc 0.421875, prec 0.0203341, recall 0.687117\n",
      "2017-12-07T22:46:03.231183: step 174, loss 2.29986, acc 0.46875, prec 0.0202093, recall 0.687117\n",
      "2017-12-07T22:46:03.782695: step 175, loss 2.21227, acc 0.5625, prec 0.0202836, recall 0.689024\n",
      "2017-12-07T22:46:04.324116: step 176, loss 2.26855, acc 0.5, prec 0.0201678, recall 0.689024\n",
      "2017-12-07T22:46:04.882211: step 177, loss 1.90569, acc 0.53125, prec 0.0202343, recall 0.690909\n",
      "2017-12-07T22:46:05.424223: step 178, loss 3.1324, acc 0.546875, prec 0.0201342, recall 0.686747\n",
      "2017-12-07T22:46:05.976405: step 179, loss 4.79653, acc 0.5625, prec 0.0200387, recall 0.682635\n",
      "2017-12-07T22:46:06.517351: step 180, loss 9.75888, acc 0.5625, prec 0.0201154, recall 0.680473\n",
      "2017-12-07T22:46:07.093705: step 181, loss 1.8679, acc 0.734375, prec 0.0203975, recall 0.684211\n",
      "2017-12-07T22:46:07.651523: step 182, loss 1.1059, acc 0.703125, prec 0.0203301, recall 0.684211\n",
      "2017-12-07T22:46:08.201230: step 183, loss 1.03987, acc 0.65625, prec 0.0204223, recall 0.686047\n",
      "2017-12-07T22:46:08.742122: step 184, loss 1.11514, acc 0.6875, prec 0.0203518, recall 0.686047\n",
      "2017-12-07T22:46:09.288061: step 185, loss 1.57128, acc 0.609375, prec 0.0204327, recall 0.687861\n",
      "2017-12-07T22:46:09.827122: step 186, loss 1.67383, acc 0.59375, prec 0.0203419, recall 0.687861\n",
      "2017-12-07T22:46:10.378926: step 187, loss 1.41609, acc 0.6875, prec 0.0202726, recall 0.687861\n",
      "2017-12-07T22:46:10.923567: step 188, loss 1.42043, acc 0.6875, prec 0.0202037, recall 0.687861\n",
      "2017-12-07T22:46:11.460839: step 189, loss 1.40859, acc 0.6875, prec 0.0203011, recall 0.689655\n",
      "2017-12-07T22:46:11.993782: step 190, loss 1.09683, acc 0.71875, prec 0.0204047, recall 0.691429\n",
      "2017-12-07T22:46:12.540780: step 191, loss 6.52684, acc 0.65625, prec 0.0203327, recall 0.6875\n",
      "2017-12-07T22:46:13.094690: step 192, loss 0.967514, acc 0.6875, prec 0.0204287, recall 0.689266\n",
      "2017-12-07T22:46:13.656163: step 193, loss 2.66764, acc 0.796875, prec 0.0203877, recall 0.685393\n",
      "2017-12-07T22:46:14.218822: step 194, loss 1.02144, acc 0.78125, prec 0.0205034, recall 0.687151\n",
      "2017-12-07T22:46:14.784937: step 195, loss 0.742833, acc 0.765625, prec 0.0204523, recall 0.687151\n",
      "2017-12-07T22:46:15.353983: step 196, loss 16.7027, acc 0.703125, prec 0.0205536, recall 0.685083\n",
      "2017-12-07T22:46:15.936371: step 197, loss 2.01225, acc 0.625, prec 0.0207955, recall 0.688525\n",
      "2017-12-07T22:46:16.541663: step 198, loss 3.91399, acc 0.6875, prec 0.0207305, recall 0.684783\n",
      "2017-12-07T22:46:17.160313: step 199, loss 1.52251, acc 0.65625, prec 0.0206557, recall 0.684783\n",
      "2017-12-07T22:46:17.787830: step 200, loss 3.22829, acc 0.578125, prec 0.0207279, recall 0.682796\n",
      "Saved model checkpoint to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/runs/1512704664/checkpoints/model-200\n",
      "\n",
      "2017-12-07T22:46:19.310312: step 201, loss 1.87497, acc 0.640625, prec 0.0206504, recall 0.682796\n",
      "2017-12-07T22:46:19.891674: step 202, loss 2.2376, acc 0.609375, prec 0.0207254, recall 0.684492\n",
      "2017-12-07T22:46:20.480495: step 203, loss 2.45206, acc 0.515625, prec 0.0207796, recall 0.68617\n",
      "2017-12-07T22:46:21.086038: step 204, loss 5.34717, acc 0.578125, prec 0.02085, recall 0.684211\n",
      "2017-12-07T22:46:21.669805: step 205, loss 1.73679, acc 0.578125, prec 0.0207601, recall 0.684211\n",
      "2017-12-07T22:46:22.265065: step 206, loss 3.46405, acc 0.609375, prec 0.0208366, recall 0.682292\n",
      "2017-12-07T22:46:22.859348: step 207, loss 3.53158, acc 0.40625, prec 0.021021, recall 0.685567\n",
      "2017-12-07T22:46:23.459598: step 208, loss 1.62064, acc 0.578125, prec 0.0209317, recall 0.685567\n",
      "2017-12-07T22:46:24.053374: step 209, loss 2.19685, acc 0.5, prec 0.0208268, recall 0.685567\n",
      "2017-12-07T22:46:24.677839: step 210, loss 2.17362, acc 0.484375, prec 0.0207197, recall 0.685567\n",
      "2017-12-07T22:46:25.314497: step 211, loss 6.29345, acc 0.5, prec 0.0206202, recall 0.682051\n",
      "2017-12-07T22:46:25.943891: step 212, loss 2.42903, acc 0.578125, prec 0.0206854, recall 0.683673\n",
      "2017-12-07T22:46:26.574528: step 213, loss 2.50803, acc 0.5, prec 0.0205837, recall 0.683673\n",
      "2017-12-07T22:46:27.256812: step 214, loss 2.05034, acc 0.515625, prec 0.0204862, recall 0.683673\n",
      "2017-12-07T22:46:27.911809: step 215, loss 2.24555, acc 0.515625, prec 0.0206876, recall 0.686869\n",
      "2017-12-07T22:46:28.565251: step 216, loss 2.0043, acc 0.53125, prec 0.0207419, recall 0.688442\n",
      "2017-12-07T22:46:29.217795: step 217, loss 2.038, acc 0.5625, prec 0.0208019, recall 0.69\n",
      "2017-12-07T22:46:29.881820: step 218, loss 1.42593, acc 0.59375, prec 0.0207207, recall 0.69\n",
      "2017-12-07T22:46:30.530799: step 219, loss 1.54944, acc 0.640625, prec 0.0207959, recall 0.691542\n",
      "2017-12-07T22:46:31.184258: step 220, loss 3.11154, acc 0.5, prec 0.0208426, recall 0.693069\n",
      "2017-12-07T22:46:31.836827: step 221, loss 2.73827, acc 0.75, prec 0.0209416, recall 0.691176\n",
      "2017-12-07T22:46:32.501275: step 222, loss 3.0354, acc 0.5625, prec 0.020858, recall 0.687805\n",
      "2017-12-07T22:46:33.169269: step 223, loss 0.842229, acc 0.75, prec 0.0208087, recall 0.687805\n",
      "2017-12-07T22:46:33.819960: step 224, loss 4.99394, acc 0.5625, prec 0.0208701, recall 0.68599\n",
      "2017-12-07T22:46:34.493759: step 225, loss 3.89958, acc 0.640625, prec 0.0210896, recall 0.685714\n",
      "2017-12-07T22:46:35.180320: step 226, loss 1.48639, acc 0.75, prec 0.0213263, recall 0.688679\n",
      "2017-12-07T22:46:35.866200: step 227, loss 2.99487, acc 0.4375, prec 0.0212148, recall 0.688679\n",
      "2017-12-07T22:46:36.558404: step 228, loss 9.04885, acc 0.4375, prec 0.0211105, recall 0.682243\n",
      "2017-12-07T22:46:37.285387: step 229, loss 1.88894, acc 0.546875, prec 0.0211633, recall 0.683721\n",
      "2017-12-07T22:46:37.960126: step 230, loss 2.4766, acc 0.5, prec 0.0210662, recall 0.683721\n",
      "2017-12-07T22:46:38.645972: step 231, loss 2.55934, acc 0.546875, prec 0.020979, recall 0.683721\n",
      "2017-12-07T22:46:39.335083: step 232, loss 10.592, acc 0.421875, prec 0.0208748, recall 0.677419\n",
      "2017-12-07T22:46:40.026739: step 233, loss 3.44709, acc 0.375, prec 0.0208951, recall 0.678899\n",
      "2017-12-07T22:46:40.709267: step 234, loss 2.51108, acc 0.46875, prec 0.0207953, recall 0.678899\n",
      "2017-12-07T22:46:41.393366: step 235, loss 3.17526, acc 0.40625, prec 0.0209585, recall 0.681818\n",
      "2017-12-07T22:46:42.076661: step 236, loss 5.20296, acc 0.5, prec 0.0211405, recall 0.681614\n",
      "2017-12-07T22:46:42.754049: step 237, loss 3.54213, acc 0.34375, prec 0.021153, recall 0.683036\n",
      "2017-12-07T22:46:43.418940: step 238, loss 4.1579, acc 0.3125, prec 0.0212941, recall 0.685841\n",
      "2017-12-07T22:46:44.084614: step 239, loss 3.11602, acc 0.359375, prec 0.0211749, recall 0.685841\n",
      "2017-12-07T22:46:44.727096: step 240, loss 4.42601, acc 0.265625, prec 0.0213055, recall 0.688596\n",
      "2017-12-07T22:46:45.374010: step 241, loss 3.28624, acc 0.3125, prec 0.021311, recall 0.689956\n",
      "2017-12-07T22:46:46.025915: step 242, loss 3.50595, acc 0.40625, prec 0.0212024, recall 0.689956\n",
      "2017-12-07T22:46:46.672888: step 243, loss 2.80651, acc 0.453125, prec 0.0213647, recall 0.692641\n",
      "2017-12-07T22:46:47.343880: step 244, loss 8.37455, acc 0.40625, prec 0.0212596, recall 0.689655\n",
      "2017-12-07T22:46:48.030159: step 245, loss 2.45028, acc 0.40625, prec 0.0212822, recall 0.690987\n",
      "2017-12-07T22:46:48.708797: step 246, loss 2.22157, acc 0.515625, prec 0.0211954, recall 0.690987\n",
      "2017-12-07T22:46:49.366955: step 247, loss 2.5022, acc 0.484375, prec 0.021232, recall 0.692308\n",
      "2017-12-07T22:46:50.039411: step 248, loss 2.19659, acc 0.546875, prec 0.0212794, recall 0.693617\n",
      "2017-12-07T22:46:50.722979: step 249, loss 2.45321, acc 0.484375, prec 0.0211881, recall 0.693617\n",
      "2017-12-07T22:46:51.391538: step 250, loss 2.3804, acc 0.59375, prec 0.0211167, recall 0.693617\n",
      "2017-12-07T22:46:52.076473: step 251, loss 0.870774, acc 0.65625, prec 0.0211832, recall 0.694915\n",
      "2017-12-07T22:46:52.739149: step 252, loss 9.27632, acc 0.640625, prec 0.0211259, recall 0.689076\n",
      "2017-12-07T22:46:53.399533: step 253, loss 1.45994, acc 0.65625, prec 0.0210662, recall 0.689076\n",
      "2017-12-07T22:46:54.057257: step 254, loss 1.68152, acc 0.59375, prec 0.0212466, recall 0.691667\n",
      "2017-12-07T22:46:54.715741: step 255, loss 1.24294, acc 0.65625, prec 0.0213119, recall 0.692946\n",
      "2017-12-07T22:46:55.374082: step 256, loss 1.82335, acc 0.578125, prec 0.0212387, recall 0.692946\n",
      "2017-12-07T22:46:56.044070: step 257, loss 0.875926, acc 0.71875, prec 0.0211902, recall 0.692946\n",
      "2017-12-07T22:46:56.705125: step 258, loss 1.16357, acc 0.75, prec 0.0211473, recall 0.692946\n",
      "2017-12-07T22:46:57.419614: step 259, loss 1.00484, acc 0.75, prec 0.0211045, recall 0.692946\n",
      "2017-12-07T22:46:58.134185: step 260, loss 1.84174, acc 0.6875, prec 0.0211747, recall 0.694215\n",
      "2017-12-07T22:46:58.849298: step 261, loss 6.8841, acc 0.71875, prec 0.0211294, recall 0.691358\n",
      "2017-12-07T22:46:59.573634: step 262, loss 2.47549, acc 0.671875, prec 0.0210764, recall 0.688525\n",
      "2017-12-07T22:47:00.287374: step 263, loss 0.794416, acc 0.75, prec 0.0210342, recall 0.688525\n",
      "2017-12-07T22:47:00.975336: step 264, loss 1.07304, acc 0.734375, prec 0.0209895, recall 0.688525\n",
      "2017-12-07T22:47:01.658510: step 265, loss 1.19899, acc 0.765625, prec 0.0210723, recall 0.689796\n",
      "2017-12-07T22:47:02.344878: step 266, loss 11.459, acc 0.6875, prec 0.0210251, recall 0.684211\n",
      "2017-12-07T22:47:03.033348: step 267, loss 0.763849, acc 0.796875, prec 0.0209912, recall 0.684211\n",
      "2017-12-07T22:47:03.724688: step 268, loss 1.23213, acc 0.71875, prec 0.0209444, recall 0.684211\n",
      "2017-12-07T22:47:04.406540: step 269, loss 7.01365, acc 0.703125, prec 0.0208977, recall 0.681452\n",
      "2017-12-07T22:47:05.091029: step 270, loss 1.15055, acc 0.640625, prec 0.0208385, recall 0.681452\n",
      "2017-12-07T22:47:05.773390: step 271, loss 5.89469, acc 0.609375, prec 0.0208999, recall 0.677291\n",
      "2017-12-07T22:47:06.436305: step 272, loss 2.36171, acc 0.65625, prec 0.0212036, recall 0.681102\n",
      "2017-12-07T22:47:07.097268: step 273, loss 6.03507, acc 0.5625, prec 0.0211362, recall 0.675781\n",
      "2017-12-07T22:47:07.781013: step 274, loss 2.81877, acc 0.40625, prec 0.0211576, recall 0.677043\n",
      "2017-12-07T22:47:08.459798: step 275, loss 3.80314, acc 0.328125, prec 0.0210475, recall 0.677043\n",
      "2017-12-07T22:47:09.124440: step 276, loss 3.54217, acc 0.34375, prec 0.0209411, recall 0.677043\n",
      "2017-12-07T22:47:09.779019: step 277, loss 3.77814, acc 0.375, prec 0.0211925, recall 0.680769\n",
      "2017-12-07T22:47:10.434744: step 278, loss 4.13424, acc 0.296875, prec 0.0213121, recall 0.683206\n",
      "2017-12-07T22:47:11.095449: step 279, loss 4.31022, acc 0.296875, prec 0.0213144, recall 0.684411\n",
      "2017-12-07T22:47:11.748864: step 280, loss 4.08835, acc 0.359375, prec 0.0212114, recall 0.684411\n",
      "2017-12-07T22:47:12.403171: step 281, loss 6.08855, acc 0.3125, prec 0.0212192, recall 0.683019\n",
      "2017-12-07T22:47:13.062729: step 282, loss 4.4713, acc 0.265625, prec 0.0212171, recall 0.684211\n",
      "2017-12-07T22:47:13.719671: step 283, loss 3.35528, acc 0.4375, prec 0.021242, recall 0.685393\n",
      "2017-12-07T22:47:14.377017: step 284, loss 3.56727, acc 0.328125, prec 0.0211365, recall 0.685393\n",
      "2017-12-07T22:47:15.038348: step 285, loss 3.51527, acc 0.328125, prec 0.0210321, recall 0.685393\n",
      "2017-12-07T22:47:15.698010: step 286, loss 3.37579, acc 0.34375, prec 0.020931, recall 0.685393\n",
      "2017-12-07T22:47:16.361179: step 287, loss 2.49492, acc 0.484375, prec 0.0208523, recall 0.685393\n",
      "2017-12-07T22:47:17.022436: step 288, loss 2.59824, acc 0.5, prec 0.0209989, recall 0.687732\n",
      "2017-12-07T22:47:17.713755: step 289, loss 6.69059, acc 0.4375, prec 0.0209158, recall 0.685185\n",
      "2017-12-07T22:47:18.375807: step 290, loss 1.99455, acc 0.5625, prec 0.0209601, recall 0.686347\n",
      "2017-12-07T22:47:19.029832: step 291, loss 11.0749, acc 0.640625, prec 0.0210207, recall 0.682482\n",
      "2017-12-07T22:47:19.698712: step 292, loss 5.88861, acc 0.59375, prec 0.0209618, recall 0.68\n",
      "2017-12-07T22:47:20.360633: step 293, loss 3.8256, acc 0.578125, prec 0.0209009, recall 0.677536\n",
      "2017-12-07T22:47:21.023428: step 294, loss 1.93243, acc 0.546875, prec 0.0208333, recall 0.677536\n",
      "2017-12-07T22:47:21.689256: step 295, loss 1.99178, acc 0.484375, prec 0.020757, recall 0.677536\n",
      "2017-12-07T22:47:22.348878: step 296, loss 2.04958, acc 0.59375, prec 0.0208057, recall 0.6787\n",
      "2017-12-07T22:47:23.012030: step 297, loss 2.39965, acc 0.5625, prec 0.0207414, recall 0.6787\n",
      "2017-12-07T22:47:23.672004: step 298, loss 2.23413, acc 0.5625, prec 0.0208929, recall 0.681004\n",
      "2017-12-07T22:47:24.336901: step 299, loss 1.65852, acc 0.640625, prec 0.0209476, recall 0.682143\n",
      "2017-12-07T22:47:24.993710: step 300, loss 1.50238, acc 0.59375, prec 0.020888, recall 0.682143\n",
      "\n",
      "Evaluation:\n",
      "2017-12-07T22:48:17.884103: step 300, loss 1.39536, acc 0.638611, prec 0.0240844, recall 0.744681\n",
      "\n",
      "Saved model checkpoint to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/runs/1512704664/checkpoints/model-300\n",
      "\n",
      "2017-12-07T22:48:20.004569: step 301, loss 1.97031, acc 0.671875, prec 0.0240458, recall 0.744681\n",
      "2017-12-07T22:48:20.612558: step 302, loss 3.77663, acc 0.578125, prec 0.0240725, recall 0.743529\n",
      "2017-12-07T22:48:21.201975: step 303, loss 6.08647, acc 0.671875, prec 0.0240359, recall 0.741784\n",
      "2017-12-07T22:48:21.824283: step 304, loss 1.51669, acc 0.578125, prec 0.0240607, recall 0.742389\n",
      "2017-12-07T22:48:22.429961: step 305, loss 2.89919, acc 0.65625, prec 0.0240224, recall 0.740654\n",
      "2017-12-07T22:48:23.031299: step 306, loss 1.43632, acc 0.640625, prec 0.0242021, recall 0.742459\n",
      "2017-12-07T22:48:23.644642: step 307, loss 1.93011, acc 0.5625, prec 0.0241509, recall 0.742459\n",
      "2017-12-07T22:48:24.278293: step 308, loss 1.64108, acc 0.65625, prec 0.0241109, recall 0.742459\n",
      "2017-12-07T22:48:24.906105: step 309, loss 1.11985, acc 0.734375, prec 0.0241535, recall 0.743056\n",
      "2017-12-07T22:48:25.534193: step 310, loss 1.61935, acc 0.671875, prec 0.0241154, recall 0.743056\n",
      "2017-12-07T22:48:26.184865: step 311, loss 1.08333, acc 0.6875, prec 0.0240792, recall 0.743056\n",
      "2017-12-07T22:48:26.826643: step 312, loss 1.3075, acc 0.65625, prec 0.0240395, recall 0.743056\n",
      "2017-12-07T22:48:27.474539: step 313, loss 3.80174, acc 0.78125, prec 0.0240892, recall 0.741935\n",
      "2017-12-07T22:48:28.180203: step 314, loss 1.23418, acc 0.6875, prec 0.0240532, recall 0.741935\n",
      "2017-12-07T22:48:28.842304: step 315, loss 1.22002, acc 0.625, prec 0.0240829, recall 0.742529\n",
      "2017-12-07T22:48:29.492031: step 316, loss 1.28338, acc 0.703125, prec 0.0241215, recall 0.743119\n",
      "2017-12-07T22:48:30.167915: step 317, loss 1.43528, acc 0.671875, prec 0.0240838, recall 0.743119\n",
      "2017-12-07T22:48:30.806559: step 318, loss 1.45903, acc 0.71875, prec 0.0240517, recall 0.743119\n",
      "2017-12-07T22:48:31.449763: step 319, loss 0.950437, acc 0.671875, prec 0.0240142, recall 0.743119\n",
      "2017-12-07T22:48:32.104896: step 320, loss 0.731477, acc 0.765625, prec 0.0239876, recall 0.743119\n",
      "2017-12-07T22:48:32.765356: step 321, loss 1.45755, acc 0.65625, prec 0.0239486, recall 0.743119\n",
      "2017-12-07T22:48:33.421362: step 322, loss 0.7086, acc 0.765625, prec 0.023922, recall 0.743119\n",
      "2017-12-07T22:48:34.068759: step 323, loss 7.46495, acc 0.71875, prec 0.0238938, recall 0.739726\n",
      "2017-12-07T22:48:34.724257: step 324, loss 1.42297, acc 0.65625, prec 0.0238551, recall 0.739726\n",
      "2017-12-07T22:48:35.381265: step 325, loss 0.923192, acc 0.703125, prec 0.0239653, recall 0.740909\n",
      "2017-12-07T22:48:36.029024: step 326, loss 15.397, acc 0.84375, prec 0.0241645, recall 0.740991\n",
      "2017-12-07T22:48:36.677571: step 327, loss 2.62263, acc 0.671875, prec 0.0242006, recall 0.73991\n",
      "2017-12-07T22:48:37.320477: step 328, loss 3.64908, acc 0.640625, prec 0.0242331, recall 0.738839\n",
      "2017-12-07T22:48:37.991851: step 329, loss 4.11037, acc 0.65625, prec 0.0241959, recall 0.737194\n",
      "2017-12-07T22:48:38.673921: step 330, loss 6.90393, acc 0.5625, prec 0.0242194, recall 0.736142\n",
      "2017-12-07T22:48:39.314147: step 331, loss 2.05405, acc 0.53125, prec 0.0241665, recall 0.736142\n",
      "2017-12-07T22:48:39.950707: step 332, loss 2.26516, acc 0.671875, prec 0.0243424, recall 0.737885\n",
      "2017-12-07T22:48:40.592545: step 333, loss 2.81326, acc 0.515625, prec 0.0242877, recall 0.737885\n",
      "2017-12-07T22:48:41.234472: step 334, loss 3.12999, acc 0.421875, prec 0.0242933, recall 0.738462\n",
      "2017-12-07T22:48:41.880564: step 335, loss 3.00515, acc 0.40625, prec 0.0243674, recall 0.739606\n",
      "2017-12-07T22:48:42.537255: step 336, loss 8.13663, acc 0.421875, prec 0.0244446, recall 0.73913\n",
      "2017-12-07T22:48:43.210190: step 337, loss 4.03497, acc 0.3125, prec 0.0245073, recall 0.74026\n",
      "2017-12-07T22:48:43.901406: step 338, loss 2.65404, acc 0.421875, prec 0.0245123, recall 0.740821\n",
      "2017-12-07T22:48:44.551062: step 339, loss 3.74984, acc 0.40625, prec 0.0245154, recall 0.741379\n",
      "2017-12-07T22:48:45.214813: step 340, loss 3.23256, acc 0.40625, prec 0.0245185, recall 0.741935\n",
      "2017-12-07T22:48:45.877325: step 341, loss 3.19405, acc 0.390625, prec 0.0246581, recall 0.74359\n",
      "2017-12-07T22:48:46.538677: step 342, loss 3.18626, acc 0.34375, prec 0.0246539, recall 0.744136\n",
      "2017-12-07T22:48:47.205709: step 343, loss 3.58683, acc 0.296875, prec 0.0245757, recall 0.744136\n",
      "2017-12-07T22:48:47.847050: step 344, loss 2.54199, acc 0.484375, prec 0.0245188, recall 0.744136\n",
      "2017-12-07T22:48:48.461256: step 345, loss 2.9846, acc 0.4375, prec 0.0244569, recall 0.744136\n",
      "2017-12-07T22:48:49.122951: step 346, loss 2.15701, acc 0.484375, prec 0.0244687, recall 0.744681\n",
      "2017-12-07T22:48:49.769754: step 347, loss 2.63717, acc 0.46875, prec 0.0244107, recall 0.744681\n",
      "2017-12-07T22:48:50.428813: step 348, loss 2.34067, acc 0.53125, prec 0.0244955, recall 0.745763\n",
      "2017-12-07T22:48:51.055264: step 349, loss 1.07147, acc 0.65625, prec 0.024458, recall 0.745763\n",
      "2017-12-07T22:48:51.692971: step 350, loss 1.48627, acc 0.671875, prec 0.0244224, recall 0.745763\n",
      "2017-12-07T22:48:52.348097: step 351, loss 4.52759, acc 0.625, prec 0.0243835, recall 0.744186\n",
      "2017-12-07T22:48:52.982499: step 352, loss 0.979916, acc 0.703125, prec 0.0243514, recall 0.744186\n",
      "2017-12-07T22:48:53.623056: step 353, loss 0.974079, acc 0.796875, prec 0.024397, recall 0.744726\n",
      "2017-12-07T22:48:54.265369: step 354, loss 16.3043, acc 0.78125, prec 0.0244441, recall 0.742138\n",
      "2017-12-07T22:48:54.917973: step 355, loss 0.845709, acc 0.78125, prec 0.0244205, recall 0.742138\n",
      "2017-12-07T22:48:55.557397: step 356, loss 0.912664, acc 0.75, prec 0.0243936, recall 0.742138\n",
      "2017-12-07T22:48:56.203112: step 357, loss 1.61988, acc 0.75, prec 0.0244339, recall 0.742678\n",
      "2017-12-07T22:48:56.833154: step 358, loss 1.10214, acc 0.75, prec 0.024407, recall 0.742678\n",
      "2017-12-07T22:48:57.489064: step 359, loss 13.3034, acc 0.734375, prec 0.0243819, recall 0.739583\n",
      "2017-12-07T22:48:58.153442: step 360, loss 1.31278, acc 0.609375, prec 0.0243401, recall 0.739583\n",
      "2017-12-07T22:48:58.774874: step 361, loss 1.54473, acc 0.671875, prec 0.0243051, recall 0.739583\n",
      "2017-12-07T22:48:59.401908: step 362, loss 1.92232, acc 0.625, prec 0.0242652, recall 0.739583\n",
      "2017-12-07T22:49:00.024513: step 363, loss 1.77491, acc 0.53125, prec 0.0242821, recall 0.740125\n",
      "2017-12-07T22:49:00.639685: step 364, loss 1.76123, acc 0.65625, prec 0.0243122, recall 0.740664\n",
      "2017-12-07T22:49:01.264155: step 365, loss 14.3754, acc 0.5625, prec 0.0242676, recall 0.73913\n",
      "2017-12-07T22:49:01.863106: step 366, loss 2.55099, acc 0.484375, prec 0.0242132, recall 0.73913\n",
      "2017-12-07T22:49:02.464113: step 367, loss 1.83932, acc 0.578125, prec 0.024169, recall 0.73913\n",
      "2017-12-07T22:49:03.061621: step 368, loss 4.53389, acc 0.53125, prec 0.0241876, recall 0.738144\n",
      "2017-12-07T22:49:03.672565: step 369, loss 7.52213, acc 0.53125, prec 0.0241403, recall 0.736625\n",
      "2017-12-07T22:49:04.269102: step 370, loss 3.16785, acc 0.390625, prec 0.0241426, recall 0.737166\n",
      "2017-12-07T22:49:04.883887: step 371, loss 2.4379, acc 0.59375, prec 0.0241004, recall 0.737166\n",
      "2017-12-07T22:49:05.496412: step 372, loss 1.83656, acc 0.5625, prec 0.0240552, recall 0.737166\n",
      "2017-12-07T22:49:06.099736: step 373, loss 2.30139, acc 0.515625, prec 0.0241359, recall 0.738241\n",
      "2017-12-07T22:49:06.694954: step 374, loss 2.6995, acc 0.546875, prec 0.0242845, recall 0.739837\n",
      "2017-12-07T22:49:07.295726: step 375, loss 2.40038, acc 0.59375, prec 0.0244373, recall 0.741414\n",
      "2017-12-07T22:49:07.922003: step 376, loss 2.2884, acc 0.484375, prec 0.0244486, recall 0.741935\n",
      "2017-12-07T22:49:08.549821: step 377, loss 9.16454, acc 0.5, prec 0.024463, recall 0.740964\n",
      "2017-12-07T22:49:09.185634: step 378, loss 2.4487, acc 0.515625, prec 0.0246064, recall 0.742515\n",
      "2017-12-07T22:49:09.802422: step 379, loss 2.28347, acc 0.578125, prec 0.024627, recall 0.743028\n",
      "2017-12-07T22:49:10.417568: step 380, loss 2.09493, acc 0.46875, prec 0.0246361, recall 0.743539\n",
      "2017-12-07T22:49:11.036353: step 381, loss 1.98967, acc 0.5625, prec 0.0246548, recall 0.744048\n",
      "2017-12-07T22:49:11.656626: step 382, loss 1.68493, acc 0.59375, prec 0.0246128, recall 0.744048\n",
      "2017-12-07T22:49:12.285164: step 383, loss 1.6723, acc 0.609375, prec 0.0247642, recall 0.745562\n",
      "2017-12-07T22:49:12.891177: step 384, loss 1.56913, acc 0.640625, prec 0.0247269, recall 0.745562\n",
      "2017-12-07T22:49:13.504432: step 385, loss 2.15339, acc 0.53125, prec 0.0246785, recall 0.745562\n",
      "2017-12-07T22:49:14.108906: step 386, loss 0.915055, acc 0.75, prec 0.0246527, recall 0.745562\n",
      "2017-12-07T22:49:14.774976: step 387, loss 17.0321, acc 0.734375, prec 0.0246938, recall 0.741683\n",
      "2017-12-07T22:49:15.413393: step 388, loss 4.22325, acc 0.734375, prec 0.0246681, recall 0.740234\n",
      "2017-12-07T22:49:16.029052: step 389, loss 2.05129, acc 0.65625, prec 0.0247596, recall 0.741245\n",
      "2017-12-07T22:49:16.647293: step 390, loss 3.93827, acc 0.640625, prec 0.0247242, recall 0.739806\n",
      "2017-12-07T22:49:17.265192: step 391, loss 1.95111, acc 0.578125, prec 0.0247441, recall 0.74031\n",
      "2017-12-07T22:49:17.883038: step 392, loss 1.39629, acc 0.671875, prec 0.0247736, recall 0.740812\n",
      "2017-12-07T22:49:18.526630: step 393, loss 2.68737, acc 0.625, prec 0.0247352, recall 0.740812\n",
      "2017-12-07T22:49:19.144935: step 394, loss 2.6904, acc 0.46875, prec 0.024681, recall 0.740812\n",
      "2017-12-07T22:49:19.760166: step 395, loss 2.20136, acc 0.4375, prec 0.024812, recall 0.742308\n",
      "2017-12-07T22:49:20.376186: step 396, loss 2.04845, acc 0.515625, prec 0.0248877, recall 0.743295\n",
      "2017-12-07T22:49:20.983494: step 397, loss 2.72839, acc 0.515625, prec 0.0248384, recall 0.743295\n",
      "2017-12-07T22:49:21.596023: step 398, loss 3.56221, acc 0.46875, prec 0.0250335, recall 0.745247\n",
      "2017-12-07T22:49:22.219193: step 399, loss 3.03346, acc 0.421875, prec 0.0250366, recall 0.745731\n",
      "2017-12-07T22:49:22.829147: step 400, loss 1.98125, acc 0.515625, prec 0.0249873, recall 0.745731\n",
      "Saved model checkpoint to /Users/adb/Google Drive/Data Analytics/project/xtalkdb/Deep Learning/runs/1512704664/checkpoints/model-400\n",
      "\n",
      "2017-12-07T22:49:24.267655: step 401, loss 2.06347, acc 0.546875, prec 0.0249413, recall 0.745731\n",
      "2017-12-07T22:49:24.927602: step 402, loss 2.05031, acc 0.53125, prec 0.0248939, recall 0.745731\n",
      "2017-12-07T22:49:25.522509: step 403, loss 2.41988, acc 0.46875, prec 0.024902, recall 0.746212\n",
      "2017-12-07T22:49:26.106950: step 404, loss 2.13951, acc 0.546875, prec 0.024918, recall 0.746692\n",
      "2017-12-07T22:49:26.693944: step 405, loss 1.49085, acc 0.65625, prec 0.0248835, recall 0.746692\n",
      "2017-12-07T22:49:27.288669: step 406, loss 1.26945, acc 0.75, prec 0.0249198, recall 0.74717\n",
      "2017-12-07T22:49:27.881296: step 407, loss 1.48358, acc 0.625, prec 0.0248822, recall 0.74717\n",
      "2017-12-07T22:49:28.507931: step 408, loss 1.412, acc 0.6875, prec 0.0249121, recall 0.747646\n",
      "2017-12-07T22:49:29.111180: step 409, loss 1.52934, acc 0.6875, prec 0.0250031, recall 0.748593\n",
      "2017-12-07T22:49:29.804782: step 410, loss 0.859145, acc 0.78125, prec 0.0250423, recall 0.749064\n",
      "2017-12-07T22:49:30.521492: step 411, loss 0.680624, acc 0.796875, prec 0.0250829, recall 0.749533\n",
      "2017-12-07T22:49:31.170955: step 412, loss 1.99147, acc 0.8125, prec 0.0251266, recall 0.748603\n",
      "2017-12-07T22:49:31.804827: step 413, loss 11.4045, acc 0.703125, prec 0.0250983, recall 0.747212\n",
      "2017-12-07T22:49:32.437415: step 414, loss 1.04255, acc 0.78125, prec 0.0251372, recall 0.747681\n",
      "2017-12-07T22:49:33.059118: step 415, loss 14.934, acc 0.78125, prec 0.0251184, recall 0.744917\n",
      "2017-12-07T22:49:33.694210: step 416, loss 0.825324, acc 0.78125, prec 0.0251572, recall 0.745387\n",
      "2017-12-07T22:49:34.325689: step 417, loss 0.965656, acc 0.796875, prec 0.0253188, recall 0.746789\n",
      "2017-12-07T22:49:34.953648: step 418, loss 2.29416, acc 0.796875, prec 0.0253605, recall 0.745887\n",
      "2017-12-07T22:49:35.606704: step 419, loss 1.75455, acc 0.625, prec 0.0253832, recall 0.74635\n",
      "2017-12-07T22:49:36.255465: step 420, loss 1.68979, acc 0.640625, prec 0.0254074, recall 0.746812\n",
      "2017-12-07T22:49:36.899742: step 421, loss 1.10483, acc 0.75, prec 0.0255029, recall 0.747731\n",
      "2017-12-07T22:49:37.559896: step 422, loss 1.52569, acc 0.671875, prec 0.0254698, recall 0.747731\n",
      "2017-12-07T22:49:38.239662: step 423, loss 4.2817, acc 0.609375, prec 0.0254923, recall 0.746835\n",
      "2017-12-07T22:49:38.871642: step 424, loss 2.77782, acc 0.703125, prec 0.025464, recall 0.745487\n",
      "2017-12-07T22:49:39.511918: step 425, loss 1.34508, acc 0.640625, prec 0.0254279, recall 0.745487\n",
      "2017-12-07T22:49:40.141241: step 426, loss 2.12725, acc 0.59375, prec 0.0253873, recall 0.745487\n",
      "2017-12-07T22:49:40.762565: step 427, loss 20.4197, acc 0.5, prec 0.0253405, recall 0.742806\n",
      "2017-12-07T22:49:41.383570: step 428, loss 9.33868, acc 0.53125, prec 0.0252955, recall 0.741472\n",
      "2017-12-07T22:49:42.011402: step 429, loss 1.83059, acc 0.640625, prec 0.0252599, recall 0.741472\n",
      "2017-12-07T22:49:42.646851: step 430, loss 3.24834, acc 0.4375, prec 0.0252639, recall 0.741935\n",
      "2017-12-07T22:49:43.276580: step 431, loss 2.39971, acc 0.515625, prec 0.0252162, recall 0.741935\n",
      "2017-12-07T22:49:43.909130: step 432, loss 3.28903, acc 0.4375, prec 0.0252795, recall 0.742857\n",
      "2017-12-07T22:49:44.530174: step 433, loss 2.80486, acc 0.453125, prec 0.0253441, recall 0.743772\n",
      "2017-12-07T22:49:45.162467: step 434, loss 3.10418, acc 0.390625, prec 0.0252843, recall 0.743772\n",
      "2017-12-07T22:49:45.787419: step 435, loss 3.54018, acc 0.390625, prec 0.0252248, recall 0.743772\n",
      "2017-12-07T22:49:46.412831: step 436, loss 3.05583, acc 0.40625, prec 0.0251671, recall 0.743772\n",
      "2017-12-07T22:49:47.035580: step 437, loss 3.39867, acc 0.375, prec 0.0252237, recall 0.744681\n",
      "2017-12-07T22:49:47.668485: step 438, loss 2.05166, acc 0.5, prec 0.0251753, recall 0.744681\n",
      "2017-12-07T22:49:48.356232: step 439, loss 3.16758, acc 0.4375, prec 0.0251211, recall 0.744681\n",
      "2017-12-07T22:49:49.014982: step 440, loss 2.64044, acc 0.5, prec 0.0251313, recall 0.745133\n",
      "2017-12-07T22:49:49.665029: step 441, loss 2.76667, acc 0.59375, prec 0.0252086, recall 0.746032\n",
      "2017-12-07T22:49:50.309031: step 442, loss 4.0721, acc 0.5, prec 0.0252201, recall 0.745167\n",
      "2017-12-07T22:49:50.955089: step 443, loss 1.52216, acc 0.59375, prec 0.025239, recall 0.745614\n",
      "2017-12-07T22:49:51.613536: step 444, loss 2.00726, acc 0.59375, prec 0.0252579, recall 0.74606\n",
      "2017-12-07T22:49:52.292399: step 445, loss 1.9421, acc 0.59375, prec 0.0252767, recall 0.746503\n",
      "2017-12-07T22:49:52.942127: step 446, loss 1.26961, acc 0.71875, prec 0.0252498, recall 0.746503\n",
      "2017-12-07T22:49:53.565557: step 447, loss 1.50775, acc 0.5625, prec 0.0252081, recall 0.746503\n",
      "2017-12-07T22:49:54.197039: step 448, loss 1.69569, acc 0.625, prec 0.0252299, recall 0.746946\n",
      "2017-12-07T22:49:54.824022: step 449, loss 0.914763, acc 0.75, prec 0.0252061, recall 0.746946\n",
      "2017-12-07T22:49:55.454534: step 450, loss 1.3234, acc 0.78125, prec 0.0253574, recall 0.748264\n",
      "2017-12-07T22:49:56.080860: step 451, loss 1.04298, acc 0.765625, prec 0.0253351, recall 0.748264\n",
      "2017-12-07T22:49:56.694269: step 452, loss 4.10796, acc 0.84375, prec 0.0253789, recall 0.747405\n",
      "2017-12-07T22:49:57.318044: step 453, loss 17.3381, acc 0.75, prec 0.0253566, recall 0.746114\n",
      "2017-12-07T22:49:57.935686: step 454, loss 1.92325, acc 0.84375, prec 0.0254004, recall 0.745267\n",
      "2017-12-07T22:49:58.609285: step 455, loss 8.59481, acc 0.78125, prec 0.0254967, recall 0.74359\n",
      "2017-12-07T22:49:59.246444: step 456, loss 1.00802, acc 0.734375, prec 0.0255284, recall 0.744027\n",
      "2017-12-07T22:49:59.900435: step 457, loss 1.37392, acc 0.578125, prec 0.0254881, recall 0.744027\n",
      "2017-12-07T22:50:00.540360: step 458, loss 4.54092, acc 0.71875, prec 0.0254628, recall 0.74276\n",
      "2017-12-07T22:50:01.222892: step 459, loss 1.80645, acc 0.546875, prec 0.0254198, recall 0.74276\n",
      "2017-12-07T22:50:01.874916: step 460, loss 2.39376, acc 0.5, prec 0.0253724, recall 0.74276\n",
      "2017-12-07T22:50:02.509209: step 461, loss 2.21228, acc 0.515625, prec 0.0253267, recall 0.74276\n",
      "2017-12-07T22:50:03.147293: step 462, loss 14.5769, acc 0.515625, prec 0.0252856, recall 0.738983\n",
      "2017-12-07T22:50:03.791945: step 463, loss 2.89562, acc 0.421875, prec 0.0252879, recall 0.739425\n",
      "2017-12-07T22:50:04.422592: step 464, loss 2.75415, acc 0.390625, prec 0.0252309, recall 0.739425\n",
      "2017-12-07T22:50:05.056241: step 465, loss 4.46475, acc 0.375, prec 0.025229, recall 0.739865\n",
      "2017-12-07T22:50:05.701434: step 466, loss 3.97581, acc 0.359375, prec 0.0252255, recall 0.740304\n",
      "2017-12-07T22:50:06.350639: step 467, loss 3.42044, acc 0.40625, prec 0.0252823, recall 0.741176\n",
      "2017-12-07T22:50:06.961118: step 468, loss 4.22362, acc 0.34375, prec 0.0253888, recall 0.742475\n",
      "2017-12-07T22:50:07.572660: step 469, loss 4.55923, acc 0.21875, prec 0.0253165, recall 0.742475\n",
      "2017-12-07T22:50:08.187231: step 470, loss 3.48526, acc 0.4375, prec 0.0253201, recall 0.742905\n",
      "2017-12-07T22:50:08.806396: step 471, loss 4.412, acc 0.40625, prec 0.0252654, recall 0.742905\n",
      "2017-12-07T22:50:09.433231: step 472, loss 3.17496, acc 0.453125, prec 0.0252706, recall 0.743333\n",
      "2017-12-07T22:50:10.039586: step 473, loss 4.18978, acc 0.40625, prec 0.0253265, recall 0.744186\n",
      "2017-12-07T22:50:10.651991: step 474, loss 2.96286, acc 0.453125, prec 0.0252765, recall 0.744186\n",
      "2017-12-07T22:50:11.265569: step 475, loss 3.39707, acc 0.375, prec 0.0252744, recall 0.74461\n",
      "2017-12-07T22:50:11.918008: step 476, loss 3.1498, acc 0.359375, prec 0.0252162, recall 0.74461\n",
      "2017-12-07T22:50:12.560062: step 477, loss 3.01418, acc 0.390625, prec 0.0252157, recall 0.745033\n",
      "2017-12-07T22:50:13.190313: step 478, loss 2.69402, acc 0.515625, prec 0.025172, recall 0.745033\n",
      "2017-12-07T22:50:13.823876: step 479, loss 1.69432, acc 0.59375, prec 0.0252443, recall 0.745875\n",
      "2017-12-07T22:50:14.538023: step 480, loss 6.05885, acc 0.515625, prec 0.0252021, recall 0.744646\n",
      "2017-12-07T22:50:15.237530: step 481, loss 1.87888, acc 0.484375, prec 0.0251558, recall 0.744646\n",
      "2017-12-07T22:50:15.929901: step 482, loss 1.68546, acc 0.6875, prec 0.0251821, recall 0.745066\n",
      "2017-12-07T22:50:16.559123: step 483, loss 2.69065, acc 0.6875, prec 0.0252623, recall 0.745902\n",
      "2017-12-07T22:50:17.192069: step 484, loss 1.51759, acc 0.671875, prec 0.0252329, recall 0.745902\n",
      "2017-12-07T22:50:17.825821: step 485, loss 1.66833, acc 0.765625, prec 0.02532, recall 0.746732\n",
      "2017-12-07T22:50:18.457984: step 486, loss 0.838963, acc 0.734375, prec 0.0253501, recall 0.747145\n",
      "2017-12-07T22:50:19.089209: step 487, loss 1.26054, acc 0.703125, prec 0.0253235, recall 0.747145\n",
      "2017-12-07T22:50:19.707881: step 488, loss 1.46675, acc 0.65625, prec 0.0252927, recall 0.747145\n",
      "2017-12-07T22:50:20.340071: step 489, loss 1.26568, acc 0.65625, prec 0.0253158, recall 0.747557\n",
      "2017-12-07T22:50:20.948846: step 490, loss 0.905519, acc 0.75, prec 0.0253471, recall 0.747967\n",
      "2017-12-07T22:50:21.564617: step 491, loss 18.4783, acc 0.75, prec 0.0253799, recall 0.747164\n",
      "2017-12-07T22:50:22.180532: step 492, loss 0.622582, acc 0.78125, prec 0.0253603, recall 0.747164\n",
      "2017-12-07T22:50:22.813769: step 493, loss 0.932928, acc 0.765625, prec 0.0253394, recall 0.747164\n",
      "2017-12-07T22:50:23.434463: step 494, loss 6.81831, acc 0.78125, prec 0.0253213, recall 0.745955\n",
      "2017-12-07T22:50:24.068527: step 495, loss 0.720092, acc 0.78125, prec 0.0253019, recall 0.745955\n",
      "2017-12-07T22:50:24.696457: step 496, loss 0.450474, acc 0.875, prec 0.0252908, recall 0.745955\n",
      "2017-12-07T22:50:25.236411: step 497, loss 0.544158, acc 0.826923, prec 0.0252783, recall 0.745955\n",
      "Training finished\n"
     ]
    }
   ],
   "source": [
    "model = BasicTextCNN(sequence_length=x_train.shape[1],\n",
    "            vocab_size=len(vocab_processor.vocabulary_), num_epochs=1, evaluate_every=300)\n",
    "model.train_network(x_train, y_train, x_dev, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31796, 273)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "497"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "497"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
