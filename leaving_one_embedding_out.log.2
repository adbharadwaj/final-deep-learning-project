Vocabulary Size: 33447
encodedPathwayA = 8 encodedPathwayB = 53
Leaving one emebedding out



Starting Experiment - leave_position_embedding_out 



Starting Fold: 0 => Train/Dev split: 31795/10599


 #################### 


SEQUENCE LENGTH 273
BATCH SIZE 64
EMBEDDING SIZE 128
FILTER SIZES [3, 4, 5]
NUMBER OF FILTERS 128
L2 REG LAMBDA 0.0
EPOCHS 10



RESULT DIR leave_position_embedding_out_fold_0
VOCAB SIZE 33447
DROPOUT PROBABILITY 0.5



WORD EMBEDDING True
POSITION EMBEDDING False
POS EMBEDDING True


 #################### 


Writing to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_0/1512931325

Start training
2017-12-10T13:42:08.831371: step 1, loss 1.25897, acc 0.578125, prec 0, recall 0
2017-12-10T13:42:09.022644: step 2, loss 18.0047, acc 0.8125, prec 0, recall 0
2017-12-10T13:42:09.211772: step 3, loss 20.075, acc 0.859375, prec 0, recall 0
2017-12-10T13:42:09.404798: step 4, loss 0.540956, acc 0.828125, prec 0, recall 0
2017-12-10T13:42:09.594579: step 5, loss 19.7015, acc 0.71875, prec 0, recall 0
2017-12-10T13:42:09.782218: step 6, loss 15.8661, acc 0.75, prec 0, recall 0
2017-12-10T13:42:09.983600: step 7, loss 0.915868, acc 0.734375, prec 0, recall 0
2017-12-10T13:42:10.166864: step 8, loss 12.2001, acc 0.484375, prec 0, recall 0
2017-12-10T13:42:10.353070: step 9, loss 3.31772, acc 0.46875, prec 0, recall 0
2017-12-10T13:42:10.543124: step 10, loss 3.76254, acc 0.46875, prec 0.00490196, recall 0.111111
2017-12-10T13:42:10.733439: step 11, loss 13.8343, acc 0.46875, prec 0.0126582, recall 0.214286
2017-12-10T13:42:10.919675: step 12, loss 3.34319, acc 0.375, prec 0.0108303, recall 0.214286
2017-12-10T13:42:11.106799: step 13, loss 4.20441, acc 0.296875, prec 0.00931677, recall 0.214286
2017-12-10T13:42:11.294857: step 14, loss 3.96082, acc 0.140625, prec 0.00795756, recall 0.214286
2017-12-10T13:42:11.479710: step 15, loss 5.16423, acc 0.1875, prec 0.00699301, recall 0.214286
2017-12-10T13:42:11.671118: step 16, loss 5.20334, acc 0.234375, prec 0.00835073, recall 0.266667
2017-12-10T13:42:11.857095: step 17, loss 6.38097, acc 0.140625, prec 0.00934579, recall 0.3125
2017-12-10T13:42:12.041000: step 18, loss 5.38315, acc 0.125, prec 0.00846024, recall 0.3125
2017-12-10T13:42:12.227118: step 19, loss 5.03918, acc 0.171875, prec 0.00776398, recall 0.3125
2017-12-10T13:42:12.413797: step 20, loss 6.47051, acc 0.15625, prec 0.0071736, recall 0.294118
2017-12-10T13:42:12.597010: step 21, loss 4.48942, acc 0.21875, prec 0.00669344, recall 0.294118
2017-12-10T13:42:12.778145: step 22, loss 4.46757, acc 0.140625, prec 0.00747198, recall 0.333333
2017-12-10T13:42:12.963799: step 23, loss 4.75739, acc 0.25, prec 0.00937866, recall 0.4
2017-12-10T13:42:13.147347: step 24, loss 4.36269, acc 0.203125, prec 0.00994475, recall 0.428571
2017-12-10T13:42:13.339004: step 25, loss 3.48092, acc 0.265625, prec 0.0104932, recall 0.454545
2017-12-10T13:42:13.524358: step 26, loss 3.31977, acc 0.359375, prec 0.0100604, recall 0.454545
2017-12-10T13:42:13.715490: step 27, loss 3.17176, acc 0.40625, prec 0.00968992, recall 0.454545
2017-12-10T13:42:13.911442: step 28, loss 2.83021, acc 0.359375, prec 0.00931966, recall 0.454545
2017-12-10T13:42:14.096927: step 29, loss 6.35866, acc 0.421875, prec 0.00901713, recall 0.434783
2017-12-10T13:42:14.282853: step 30, loss 2.16966, acc 0.484375, prec 0.00875657, recall 0.434783
2017-12-10T13:42:14.468436: step 31, loss 1.88917, acc 0.453125, prec 0.00849618, recall 0.434783
2017-12-10T13:42:14.656364: step 32, loss 11.5532, acc 0.625, prec 0.00833333, recall 0.416667
2017-12-10T13:42:14.841727: step 33, loss 1.22889, acc 0.65625, prec 0.00899428, recall 0.44
2017-12-10T13:42:15.025182: step 34, loss 1.67385, acc 0.640625, prec 0.0096231, recall 0.461538
2017-12-10T13:42:15.212930: step 35, loss 0.857517, acc 0.734375, prec 0.0102767, recall 0.481481
2017-12-10T13:42:15.401466: step 36, loss 15.5156, acc 0.65625, prec 0.0101167, recall 0.448276
2017-12-10T13:42:15.590644: step 37, loss 2.90098, acc 0.78125, prec 0.0100154, recall 0.433333
2017-12-10T13:42:15.779646: step 38, loss 14.3135, acc 0.65625, prec 0.00985595, recall 0.419355
2017-12-10T13:42:15.965874: step 39, loss 1.24033, acc 0.640625, prec 0.00968703, recall 0.419355
2017-12-10T13:42:16.149386: step 40, loss 10.491, acc 0.671875, prec 0.010279, recall 0.411765
2017-12-10T13:42:16.336265: step 41, loss 2.34032, acc 0.4375, prec 0.0100143, recall 0.411765
2017-12-10T13:42:16.525592: step 42, loss 19.787, acc 0.4375, prec 0.0104676, recall 0.405405
2017-12-10T13:42:16.713466: step 43, loss 2.00241, acc 0.53125, prec 0.0102529, recall 0.405405
2017-12-10T13:42:16.897890: step 44, loss 7.38325, acc 0.421875, prec 0.0100067, recall 0.394737
2017-12-10T13:42:17.085983: step 45, loss 3.03921, acc 0.375, prec 0.00974659, recall 0.394737
2017-12-10T13:42:17.270398: step 46, loss 3.53758, acc 0.28125, prec 0.0100883, recall 0.410256
2017-12-10T13:42:17.458282: step 47, loss 4.08425, acc 0.25, prec 0.00979192, recall 0.410256
2017-12-10T13:42:17.675271: step 48, loss 4.35405, acc 0.203125, prec 0.00949555, recall 0.410256
2017-12-10T13:42:17.880559: step 49, loss 12.2257, acc 0.21875, prec 0.00923254, recall 0.390244
2017-12-10T13:42:18.073480: step 50, loss 8.34501, acc 0.140625, prec 0.00895355, recall 0.380952
2017-12-10T13:42:18.260469: step 51, loss 10.4822, acc 0.25, prec 0.0087241, recall 0.372093
2017-12-10T13:42:18.446087: step 52, loss 4.33176, acc 0.21875, prec 0.00849257, recall 0.372093
2017-12-10T13:42:18.633229: step 53, loss 5.35592, acc 0.140625, prec 0.00927357, recall 0.4
2017-12-10T13:42:18.818398: step 54, loss 4.70137, acc 0.140625, prec 0.00901804, recall 0.4
2017-12-10T13:42:19.006331: step 55, loss 5.14122, acc 0.109375, prec 0.00876766, recall 0.4
2017-12-10T13:42:19.191981: step 56, loss 5.05656, acc 0.1875, prec 0.00855107, recall 0.4
2017-12-10T13:42:19.380189: step 57, loss 5.62032, acc 0.1875, prec 0.00880445, recall 0.413043
2017-12-10T13:42:19.566629: step 58, loss 4.7777, acc 0.140625, prec 0.00948081, recall 0.4375
2017-12-10T13:42:19.753477: step 59, loss 4.75854, acc 0.25, prec 0.00927972, recall 0.4375
2017-12-10T13:42:19.935967: step 60, loss 4.33621, acc 0.265625, prec 0.00951969, recall 0.44898
2017-12-10T13:42:20.123031: step 61, loss 11.1416, acc 0.28125, prec 0.00933786, recall 0.44
2017-12-10T13:42:20.315042: step 62, loss 14.9545, acc 0.34375, prec 0.00959533, recall 0.433962
2017-12-10T13:42:20.503631: step 63, loss 3.69521, acc 0.296875, prec 0.0102291, recall 0.454545
2017-12-10T13:42:20.689476: step 64, loss 3.94969, acc 0.265625, prec 0.0100361, recall 0.454545
2017-12-10T13:42:20.874304: step 65, loss 3.40758, acc 0.234375, prec 0.00984252, recall 0.454545
2017-12-10T13:42:21.058987: step 66, loss 4.27346, acc 0.265625, prec 0.0104287, recall 0.473684
2017-12-10T13:42:21.248323: step 67, loss 1.75118, acc 0.5, prec 0.011056, recall 0.491525
2017-12-10T13:42:21.435453: step 68, loss 2.85321, acc 0.265625, prec 0.0108614, recall 0.491525
2017-12-10T13:42:21.618411: step 69, loss 3.34274, acc 0.328125, prec 0.0110538, recall 0.5
2017-12-10T13:42:21.804344: step 70, loss 2.6086, acc 0.421875, prec 0.0109051, recall 0.5
2017-12-10T13:42:21.988477: step 71, loss 4.1458, acc 0.4375, prec 0.0111231, recall 0.5
2017-12-10T13:42:22.174904: step 72, loss 1.96782, acc 0.625, prec 0.0110281, recall 0.5
2017-12-10T13:42:22.358286: step 73, loss 9.86309, acc 0.421875, prec 0.011236, recall 0.5
2017-12-10T13:42:22.544672: step 74, loss 1.8541, acc 0.578125, prec 0.0111304, recall 0.5
2017-12-10T13:42:22.730505: step 75, loss 2.3355, acc 0.484375, prec 0.0113441, recall 0.507692
2017-12-10T13:42:22.917386: step 76, loss 1.63817, acc 0.53125, prec 0.0112283, recall 0.507692
2017-12-10T13:42:23.102223: step 77, loss 9.89583, acc 0.53125, prec 0.0111186, recall 0.5
2017-12-10T13:42:23.290438: step 78, loss 2.89365, acc 0.75, prec 0.0110627, recall 0.492537
2017-12-10T13:42:23.479445: step 79, loss 1.82516, acc 0.5625, prec 0.0112882, recall 0.5
2017-12-10T13:42:23.663318: step 80, loss 1.79208, acc 0.515625, prec 0.0111732, recall 0.5
2017-12-10T13:42:23.850154: step 81, loss 8.07764, acc 0.421875, prec 0.0123257, recall 0.520548
2017-12-10T13:42:24.036857: step 82, loss 1.77269, acc 0.546875, prec 0.0122108, recall 0.520548
2017-12-10T13:42:24.220376: step 83, loss 10.1077, acc 0.59375, prec 0.0127429, recall 0.526316
2017-12-10T13:42:24.407773: step 84, loss 2.21713, acc 0.5625, prec 0.0135647, recall 0.544304
2017-12-10T13:42:24.591181: step 85, loss 2.76463, acc 0.53125, prec 0.0137457, recall 0.55
2017-12-10T13:42:24.784957: step 86, loss 3.05222, acc 0.390625, prec 0.0138846, recall 0.555556
2017-12-10T13:42:24.969511: step 87, loss 2.33948, acc 0.5, prec 0.0137489, recall 0.555556
2017-12-10T13:42:25.152947: step 88, loss 3.49988, acc 0.4375, prec 0.0136034, recall 0.548781
2017-12-10T13:42:25.339892: step 89, loss 3.34161, acc 0.578125, prec 0.0134973, recall 0.542169
2017-12-10T13:42:25.528201: step 90, loss 3.14088, acc 0.4375, prec 0.0136458, recall 0.547619
2017-12-10T13:42:25.712207: step 91, loss 2.96895, acc 0.40625, prec 0.013783, recall 0.552941
2017-12-10T13:42:25.898551: step 92, loss 2.55733, acc 0.546875, prec 0.0139535, recall 0.55814
2017-12-10T13:42:26.087411: step 93, loss 2.19248, acc 0.421875, prec 0.0143719, recall 0.568182
2017-12-10T13:42:26.275799: step 94, loss 6.51234, acc 0.390625, prec 0.0142167, recall 0.561798
2017-12-10T13:42:26.462721: step 95, loss 2.98004, acc 0.453125, prec 0.0143541, recall 0.566667
2017-12-10T13:42:26.649476: step 96, loss 2.999, acc 0.375, prec 0.0141943, recall 0.566667
2017-12-10T13:42:26.835728: step 97, loss 1.99859, acc 0.4375, prec 0.0143251, recall 0.571429
2017-12-10T13:42:27.023583: step 98, loss 6.65686, acc 0.484375, prec 0.0141999, recall 0.565217
2017-12-10T13:42:27.209474: step 99, loss 2.53565, acc 0.515625, prec 0.0143476, recall 0.569892
2017-12-10T13:42:27.396475: step 100, loss 7.08246, acc 0.484375, prec 0.0142244, recall 0.56383
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_0/1512931325/checkpoints/model-100

2017-12-10T13:42:28.671607: step 101, loss 1.5215, acc 0.625, prec 0.0141333, recall 0.56383
2017-12-10T13:42:28.866056: step 102, loss 2.10967, acc 0.59375, prec 0.014036, recall 0.56383
2017-12-10T13:42:29.055965: step 103, loss 3.40837, acc 0.453125, prec 0.0141658, recall 0.568421
2017-12-10T13:42:29.247310: step 104, loss 1.38694, acc 0.625, prec 0.0140772, recall 0.568421
2017-12-10T13:42:29.433030: step 105, loss 2.08329, acc 0.5625, prec 0.0139752, recall 0.568421
2017-12-10T13:42:29.622720: step 106, loss 1.77078, acc 0.53125, prec 0.0138675, recall 0.568421
2017-12-10T13:42:29.807873: step 107, loss 1.43957, acc 0.625, prec 0.0137825, recall 0.568421
2017-12-10T13:42:29.998194: step 108, loss 2.38192, acc 0.609375, prec 0.0141952, recall 0.57732
2017-12-10T13:42:30.183387: step 109, loss 23.2165, acc 0.59375, prec 0.0141129, recall 0.56
2017-12-10T13:42:30.376112: step 110, loss 2.02454, acc 0.578125, prec 0.0142643, recall 0.564356
2017-12-10T13:42:30.561729: step 111, loss 3.44859, acc 0.515625, prec 0.0144028, recall 0.563107
2017-12-10T13:42:30.746661: step 112, loss 1.60944, acc 0.578125, prec 0.0143069, recall 0.563107
2017-12-10T13:42:30.932579: step 113, loss 2.27587, acc 0.515625, prec 0.0146807, recall 0.571429
2017-12-10T13:42:31.124922: step 114, loss 1.98705, acc 0.484375, prec 0.0145631, recall 0.571429
2017-12-10T13:42:31.315291: step 115, loss 3.04704, acc 0.5625, prec 0.0147023, recall 0.575472
2017-12-10T13:42:31.500261: step 116, loss 2.11413, acc 0.5, prec 0.0145898, recall 0.575472
2017-12-10T13:42:31.688015: step 117, loss 2.18761, acc 0.484375, prec 0.0144756, recall 0.575472
2017-12-10T13:42:31.874036: step 118, loss 1.76675, acc 0.6875, prec 0.015105, recall 0.587156
2017-12-10T13:42:32.061989: step 119, loss 2.15668, acc 0.453125, prec 0.0149813, recall 0.587156
2017-12-10T13:42:32.249656: step 120, loss 9.61581, acc 0.34375, prec 0.0148389, recall 0.581818
2017-12-10T13:42:32.433447: step 121, loss 1.94754, acc 0.5625, prec 0.0149701, recall 0.585586
2017-12-10T13:42:32.620123: step 122, loss 10.8487, acc 0.5, prec 0.0148639, recall 0.580357
2017-12-10T13:42:32.806948: step 123, loss 2.64116, acc 0.375, prec 0.0149524, recall 0.584071
2017-12-10T13:42:32.989865: step 124, loss 2.98282, acc 0.390625, prec 0.0152637, recall 0.591304
2017-12-10T13:42:33.173346: step 125, loss 2.12926, acc 0.515625, prec 0.0151583, recall 0.591304
2017-12-10T13:42:33.357467: step 126, loss 2.14767, acc 0.453125, prec 0.0150409, recall 0.591304
2017-12-10T13:42:33.545077: step 127, loss 2.72744, acc 0.484375, prec 0.0149319, recall 0.591304
2017-12-10T13:42:33.736994: step 128, loss 2.35473, acc 0.40625, prec 0.0148084, recall 0.591304
2017-12-10T13:42:33.921557: step 129, loss 2.59203, acc 0.484375, prec 0.0149157, recall 0.594828
2017-12-10T13:42:34.106338: step 130, loss 1.81619, acc 0.515625, prec 0.0148164, recall 0.594828
2017-12-10T13:42:34.297394: step 131, loss 12.3798, acc 0.5, prec 0.0147184, recall 0.589744
2017-12-10T13:42:34.485784: step 132, loss 2.39076, acc 0.46875, prec 0.0146125, recall 0.589744
2017-12-10T13:42:34.670314: step 133, loss 2.00196, acc 0.5625, prec 0.0147337, recall 0.59322
2017-12-10T13:42:34.857568: step 134, loss 1.92128, acc 0.453125, prec 0.014626, recall 0.59322
2017-12-10T13:42:35.044437: step 135, loss 1.01965, acc 0.703125, prec 0.0145682, recall 0.59322
2017-12-10T13:42:35.230662: step 136, loss 12.6122, acc 0.625, prec 0.0144988, recall 0.588235
2017-12-10T13:42:35.422853: step 137, loss 11.2275, acc 0.703125, prec 0.0144449, recall 0.583333
2017-12-10T13:42:35.614238: step 138, loss 1.29639, acc 0.703125, prec 0.0143885, recall 0.583333
2017-12-10T13:42:35.800051: step 139, loss 1.30293, acc 0.640625, prec 0.0145224, recall 0.586777
2017-12-10T13:42:35.989642: step 140, loss 4.27834, acc 0.703125, prec 0.0144691, recall 0.581967
2017-12-10T13:42:36.179420: step 141, loss 4.07363, acc 0.625, prec 0.0146015, recall 0.580645
2017-12-10T13:42:36.369663: step 142, loss 1.61281, acc 0.515625, prec 0.0145103, recall 0.580645
2017-12-10T13:42:36.554878: step 143, loss 2.36826, acc 0.46875, prec 0.0146088, recall 0.584
2017-12-10T13:42:36.739754: step 144, loss 1.92592, acc 0.59375, prec 0.0147293, recall 0.587302
2017-12-10T13:42:36.928817: step 145, loss 1.81717, acc 0.625, prec 0.0146593, recall 0.587302
2017-12-10T13:42:37.113450: step 146, loss 2.14621, acc 0.578125, prec 0.0147754, recall 0.590551
2017-12-10T13:42:37.302981: step 147, loss 1.33939, acc 0.640625, prec 0.0147088, recall 0.590551
2017-12-10T13:42:37.489625: step 148, loss 2.14946, acc 0.515625, prec 0.0146199, recall 0.590551
2017-12-10T13:42:37.677623: step 149, loss 1.57654, acc 0.546875, prec 0.0145377, recall 0.590551
2017-12-10T13:42:37.864232: step 150, loss 2.0643, acc 0.546875, prec 0.0146464, recall 0.59375
2017-12-10T13:42:38.053209: step 151, loss 8.79087, acc 0.671875, prec 0.0147793, recall 0.592308
2017-12-10T13:42:38.240432: step 152, loss 1.44773, acc 0.5625, prec 0.0147003, recall 0.592308
2017-12-10T13:42:38.431965: step 153, loss 2.20356, acc 0.53125, prec 0.0149905, recall 0.598485
2017-12-10T13:42:38.616883: step 154, loss 1.44313, acc 0.609375, prec 0.0149197, recall 0.598485
2017-12-10T13:42:38.809802: step 155, loss 1.74118, acc 0.59375, prec 0.0148468, recall 0.598485
2017-12-10T13:42:38.998103: step 156, loss 13.0609, acc 0.5, prec 0.0149477, recall 0.592593
2017-12-10T13:42:39.189325: step 157, loss 1.4797, acc 0.609375, prec 0.0148782, recall 0.592593
2017-12-10T13:42:39.377926: step 158, loss 16.0799, acc 0.546875, prec 0.0149834, recall 0.591241
2017-12-10T13:42:39.570451: step 159, loss 1.85179, acc 0.65625, prec 0.0151041, recall 0.594203
2017-12-10T13:42:39.757564: step 160, loss 29.0448, acc 0.46875, prec 0.0153762, recall 0.591549
2017-12-10T13:42:39.952158: step 161, loss 3.07717, acc 0.375, prec 0.0152644, recall 0.591549
2017-12-10T13:42:40.139683: step 162, loss 3.06875, acc 0.359375, prec 0.0151515, recall 0.591549
2017-12-10T13:42:40.328829: step 163, loss 2.87456, acc 0.421875, prec 0.0150511, recall 0.591549
2017-12-10T13:42:40.523135: step 164, loss 4.26356, acc 0.25, prec 0.0149227, recall 0.591549
2017-12-10T13:42:40.707765: step 165, loss 3.31333, acc 0.28125, prec 0.0148018, recall 0.591549
2017-12-10T13:42:40.892633: step 166, loss 3.59647, acc 0.265625, prec 0.0146802, recall 0.591549
2017-12-10T13:42:41.078830: step 167, loss 3.67419, acc 0.3125, prec 0.0145682, recall 0.591549
2017-12-10T13:42:41.260753: step 168, loss 4.17582, acc 0.203125, prec 0.0144404, recall 0.591549
2017-12-10T13:42:41.449658: step 169, loss 3.72199, acc 0.296875, prec 0.0143296, recall 0.591549
2017-12-10T13:42:41.634383: step 170, loss 3.41843, acc 0.375, prec 0.0142325, recall 0.591549
2017-12-10T13:42:41.821472: step 171, loss 3.09774, acc 0.453125, prec 0.0143146, recall 0.594406
2017-12-10T13:42:42.009749: step 172, loss 2.17734, acc 0.4375, prec 0.0143933, recall 0.597222
2017-12-10T13:42:42.198909: step 173, loss 2.66848, acc 0.4375, prec 0.0143071, recall 0.597222
2017-12-10T13:42:42.384230: step 174, loss 2.07779, acc 0.53125, prec 0.0145623, recall 0.60274
2017-12-10T13:42:42.569720: step 175, loss 1.41543, acc 0.578125, prec 0.0144975, recall 0.60274
2017-12-10T13:42:42.750476: step 176, loss 1.95027, acc 0.515625, prec 0.0144239, recall 0.60274
2017-12-10T13:42:42.938626: step 177, loss 1.2951, acc 0.6875, prec 0.0143767, recall 0.60274
2017-12-10T13:42:43.125711: step 178, loss 1.1048, acc 0.625, prec 0.0143206, recall 0.60274
2017-12-10T13:42:43.312490: step 179, loss 1.19158, acc 0.703125, prec 0.0142764, recall 0.60274
2017-12-10T13:42:43.501276: step 180, loss 0.958114, acc 0.875, prec 0.0144176, recall 0.605442
2017-12-10T13:42:43.693138: step 181, loss 2.24455, acc 0.8125, prec 0.014392, recall 0.601351
2017-12-10T13:42:43.885367: step 182, loss 0.502053, acc 0.8125, prec 0.0143641, recall 0.601351
2017-12-10T13:42:44.079378: step 183, loss 3.92831, acc 0.8125, prec 0.0143386, recall 0.597315
2017-12-10T13:42:44.272081: step 184, loss 0.841869, acc 0.796875, prec 0.0144671, recall 0.6
2017-12-10T13:42:44.462080: step 185, loss 3.80859, acc 0.765625, prec 0.0144346, recall 0.596026
2017-12-10T13:42:44.647141: step 186, loss 0.624911, acc 0.8125, prec 0.0144069, recall 0.596026
2017-12-10T13:42:44.837536: step 187, loss 0.354418, acc 0.84375, prec 0.0143839, recall 0.596026
2017-12-10T13:42:45.025868: step 188, loss 0.587905, acc 0.8125, prec 0.0143564, recall 0.596026
2017-12-10T13:42:45.217070: step 189, loss 0.79256, acc 0.796875, prec 0.0143266, recall 0.596026
2017-12-10T13:42:45.405490: step 190, loss 0.547644, acc 0.8125, prec 0.0142993, recall 0.596026
2017-12-10T13:42:45.597112: step 191, loss 6.06136, acc 0.765625, prec 0.0142676, recall 0.592105
2017-12-10T13:42:45.788627: step 192, loss 1.47292, acc 0.734375, prec 0.0143851, recall 0.594771
2017-12-10T13:42:45.981376: step 193, loss 0.970241, acc 0.765625, prec 0.014351, recall 0.594771
2017-12-10T13:42:46.167426: step 194, loss 7.60724, acc 0.84375, prec 0.014333, recall 0.587097
2017-12-10T13:42:46.358417: step 195, loss 1.51344, acc 0.609375, prec 0.0142767, recall 0.587097
2017-12-10T13:42:46.543073: step 196, loss 1.74852, acc 0.8125, prec 0.0142522, recall 0.583333
2017-12-10T13:42:46.733291: step 197, loss 1.48147, acc 0.5625, prec 0.0143436, recall 0.585987
2017-12-10T13:42:46.922754: step 198, loss 1.17004, acc 0.671875, prec 0.0147561, recall 0.59375
2017-12-10T13:42:47.113639: step 199, loss 0.983163, acc 0.734375, prec 0.0148699, recall 0.596273
2017-12-10T13:42:47.296588: step 200, loss 2.46402, acc 0.65625, prec 0.0151235, recall 0.601227
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_0/1512931325/checkpoints/model-200

2017-12-10T13:42:48.648200: step 201, loss 1.38539, acc 0.65625, prec 0.0150723, recall 0.601227
2017-12-10T13:42:48.831619: step 202, loss 14.9015, acc 0.546875, prec 0.0150077, recall 0.597561
2017-12-10T13:42:49.020746: step 203, loss 4.2256, acc 0.546875, prec 0.0149436, recall 0.593939
2017-12-10T13:42:49.213244: step 204, loss 5.81595, acc 0.578125, prec 0.0148846, recall 0.590361
2017-12-10T13:42:49.400207: step 205, loss 6.24409, acc 0.515625, prec 0.014966, recall 0.589286
2017-12-10T13:42:49.589933: step 206, loss 2.47556, acc 0.484375, prec 0.0148917, recall 0.589286
2017-12-10T13:42:49.775441: step 207, loss 2.78257, acc 0.4375, prec 0.0149589, recall 0.591716
2017-12-10T13:42:49.961520: step 208, loss 3.88544, acc 0.3125, prec 0.0151538, recall 0.596491
2017-12-10T13:42:50.152894: step 209, loss 4.10677, acc 0.234375, prec 0.0150442, recall 0.596491
2017-12-10T13:42:50.341844: step 210, loss 10.914, acc 0.34375, prec 0.0149538, recall 0.593023
2017-12-10T13:42:50.534145: step 211, loss 3.12062, acc 0.421875, prec 0.0150168, recall 0.595376
2017-12-10T13:42:50.723728: step 212, loss 4.14041, acc 0.1875, prec 0.0150463, recall 0.597701
2017-12-10T13:42:50.912880: step 213, loss 3.97292, acc 0.265625, prec 0.0149447, recall 0.597701
2017-12-10T13:42:51.103283: step 214, loss 4.13506, acc 0.359375, prec 0.0148571, recall 0.597701
2017-12-10T13:42:51.286910: step 215, loss 3.0114, acc 0.375, prec 0.0150525, recall 0.602273
2017-12-10T13:42:51.473376: step 216, loss 2.76883, acc 0.375, prec 0.0153846, recall 0.608939
2017-12-10T13:42:51.662245: step 217, loss 3.39343, acc 0.4375, prec 0.0154451, recall 0.611111
2017-12-10T13:42:51.847652: step 218, loss 3.44001, acc 0.375, prec 0.0153588, recall 0.611111
2017-12-10T13:42:52.035232: step 219, loss 2.98623, acc 0.328125, prec 0.0152672, recall 0.611111
2017-12-10T13:42:52.221030: step 220, loss 3.0933, acc 0.453125, prec 0.0153294, recall 0.61326
2017-12-10T13:42:52.406635: step 221, loss 1.91677, acc 0.53125, prec 0.0152661, recall 0.61326
2017-12-10T13:42:52.590842: step 222, loss 1.70812, acc 0.578125, prec 0.0152096, recall 0.61326
2017-12-10T13:42:52.775669: step 223, loss 2.26087, acc 0.515625, prec 0.015414, recall 0.617486
2017-12-10T13:42:52.961263: step 224, loss 1.35345, acc 0.625, prec 0.0153637, recall 0.617486
2017-12-10T13:42:53.151067: step 225, loss 6.94878, acc 0.59375, prec 0.0153117, recall 0.61413
2017-12-10T13:42:53.338488: step 226, loss 4.67643, acc 0.53125, prec 0.0152517, recall 0.610811
2017-12-10T13:42:53.533181: step 227, loss 6.03422, acc 0.65625, prec 0.0152086, recall 0.607527
2017-12-10T13:42:53.721158: step 228, loss 7.8404, acc 0.6875, prec 0.0151719, recall 0.601064
2017-12-10T13:42:53.914009: step 229, loss 1.79569, acc 0.625, prec 0.0151231, recall 0.601064
2017-12-10T13:42:54.096548: step 230, loss 1.25014, acc 0.703125, prec 0.0152162, recall 0.603175
2017-12-10T13:42:54.283828: step 231, loss 4.42484, acc 0.59375, prec 0.0152966, recall 0.602094
2017-12-10T13:42:54.476496: step 232, loss 1.25798, acc 0.65625, prec 0.0153826, recall 0.604167
2017-12-10T13:42:54.667092: step 233, loss 1.96849, acc 0.53125, prec 0.0154517, recall 0.606218
2017-12-10T13:42:54.854734: step 234, loss 2.06434, acc 0.390625, prec 0.0153725, recall 0.606218
2017-12-10T13:42:55.041121: step 235, loss 2.48141, acc 0.484375, prec 0.0153061, recall 0.606218
2017-12-10T13:42:55.225839: step 236, loss 1.88919, acc 0.671875, prec 0.0155211, recall 0.610256
2017-12-10T13:42:55.414886: step 237, loss 16.6119, acc 0.375, prec 0.0154445, recall 0.604061
2017-12-10T13:42:55.602057: step 238, loss 2.28698, acc 0.421875, prec 0.0153707, recall 0.604061
2017-12-10T13:42:55.795601: step 239, loss 2.66644, acc 0.4375, prec 0.0155527, recall 0.60804
2017-12-10T13:42:55.981690: step 240, loss 7.40014, acc 0.5, prec 0.015491, recall 0.605
2017-12-10T13:42:56.171388: step 241, loss 2.57402, acc 0.4375, prec 0.0154199, recall 0.605
2017-12-10T13:42:56.357716: step 242, loss 1.45564, acc 0.640625, prec 0.0153748, recall 0.605
2017-12-10T13:42:56.544494: step 243, loss 2.51836, acc 0.453125, prec 0.0154313, recall 0.606965
2017-12-10T13:42:56.726779: step 244, loss 1.99026, acc 0.453125, prec 0.0153633, recall 0.606965
2017-12-10T13:42:56.913979: step 245, loss 2.9782, acc 0.421875, prec 0.0154155, recall 0.608911
2017-12-10T13:42:57.100154: step 246, loss 2.4169, acc 0.515625, prec 0.0154787, recall 0.610837
2017-12-10T13:42:57.286582: step 247, loss 3.03328, acc 0.453125, prec 0.0154114, recall 0.610837
2017-12-10T13:42:57.470904: step 248, loss 1.63311, acc 0.578125, prec 0.0153598, recall 0.610837
2017-12-10T13:42:57.655591: step 249, loss 1.8634, acc 0.5625, prec 0.0153068, recall 0.610837
2017-12-10T13:42:57.841393: step 250, loss 4.38365, acc 0.625, prec 0.0152634, recall 0.607843
2017-12-10T13:42:58.032739: step 251, loss 1.87364, acc 0.53125, prec 0.015328, recall 0.609756
2017-12-10T13:42:58.224228: step 252, loss 1.12585, acc 0.71875, prec 0.0152943, recall 0.609756
2017-12-10T13:42:58.413565: step 253, loss 0.80428, acc 0.71875, prec 0.0153809, recall 0.61165
2017-12-10T13:42:58.600730: step 254, loss 1.38795, acc 0.609375, prec 0.0153341, recall 0.61165
2017-12-10T13:42:58.786233: step 255, loss 14.1943, acc 0.71875, prec 0.015422, recall 0.610577
2017-12-10T13:42:58.978446: step 256, loss 0.921917, acc 0.75, prec 0.0153921, recall 0.610577
2017-12-10T13:42:59.164667: step 257, loss 1.2104, acc 0.6875, prec 0.0153549, recall 0.610577
2017-12-10T13:42:59.351747: step 258, loss 4.37592, acc 0.75, prec 0.0155647, recall 0.611374
2017-12-10T13:42:59.548695: step 259, loss 1.12608, acc 0.75, prec 0.0155347, recall 0.611374
2017-12-10T13:42:59.738684: step 260, loss 0.899471, acc 0.78125, prec 0.0155085, recall 0.611374
2017-12-10T13:42:59.925959: step 261, loss 9.66005, acc 0.5625, prec 0.0154584, recall 0.608491
2017-12-10T13:43:00.114861: step 262, loss 1.84426, acc 0.59375, prec 0.015528, recall 0.610329
2017-12-10T13:43:00.299118: step 263, loss 1.22905, acc 0.71875, prec 0.015612, recall 0.61215
2017-12-10T13:43:00.488285: step 264, loss 2.26309, acc 0.625, prec 0.0156844, recall 0.613953
2017-12-10T13:43:00.674340: step 265, loss 1.05599, acc 0.71875, prec 0.0157676, recall 0.615741
2017-12-10T13:43:00.861707: step 266, loss 13.953, acc 0.53125, prec 0.0157173, recall 0.607306
2017-12-10T13:43:01.053824: step 267, loss 1.75521, acc 0.609375, prec 0.015671, recall 0.607306
2017-12-10T13:43:01.245338: step 268, loss 2.25198, acc 0.578125, prec 0.0156213, recall 0.607306
2017-12-10T13:43:01.438683: step 269, loss 10.3986, acc 0.421875, prec 0.0156707, recall 0.606335
2017-12-10T13:43:01.626678: step 270, loss 2.79751, acc 0.40625, prec 0.0157159, recall 0.608108
2017-12-10T13:43:01.815670: step 271, loss 3.39572, acc 0.34375, prec 0.0156395, recall 0.608108
2017-12-10T13:43:02.002358: step 272, loss 3.25291, acc 0.34375, prec 0.0155638, recall 0.608108
2017-12-10T13:43:02.189893: step 273, loss 4.11257, acc 0.3125, prec 0.0154852, recall 0.608108
2017-12-10T13:43:02.373928: step 274, loss 4.9624, acc 0.3125, prec 0.0156339, recall 0.608889
2017-12-10T13:43:02.562923: step 275, loss 3.27214, acc 0.40625, prec 0.0157901, recall 0.612335
2017-12-10T13:43:02.747469: step 276, loss 3.12202, acc 0.390625, prec 0.0160543, recall 0.617391
2017-12-10T13:43:02.934881: step 277, loss 3.7763, acc 0.359375, prec 0.0163123, recall 0.622318
2017-12-10T13:43:03.121272: step 278, loss 2.93176, acc 0.4375, prec 0.0162465, recall 0.622318
2017-12-10T13:43:03.306948: step 279, loss 3.0525, acc 0.359375, prec 0.0161722, recall 0.622318
2017-12-10T13:43:03.490479: step 280, loss 3.04185, acc 0.40625, prec 0.0163225, recall 0.625532
2017-12-10T13:43:03.676499: step 281, loss 7.40535, acc 0.46875, prec 0.0163717, recall 0.624473
2017-12-10T13:43:03.863714: step 282, loss 3.08321, acc 0.421875, prec 0.0164133, recall 0.62605
2017-12-10T13:43:04.048348: step 283, loss 2.8777, acc 0.4375, prec 0.0163485, recall 0.62605
2017-12-10T13:43:04.232047: step 284, loss 2.23174, acc 0.46875, prec 0.0162877, recall 0.62605
2017-12-10T13:43:04.418092: step 285, loss 1.98541, acc 0.546875, prec 0.0162362, recall 0.62605
2017-12-10T13:43:04.606389: step 286, loss 1.77997, acc 0.546875, prec 0.0161851, recall 0.62605
2017-12-10T13:43:04.799164: step 287, loss 5.56588, acc 0.546875, prec 0.016136, recall 0.623431
2017-12-10T13:43:04.986953: step 288, loss 2.52451, acc 0.53125, prec 0.0160838, recall 0.623431
2017-12-10T13:43:05.173295: step 289, loss 5.79518, acc 0.65625, prec 0.0160474, recall 0.620833
2017-12-10T13:43:05.364027: step 290, loss 3.50168, acc 0.59375, prec 0.01611, recall 0.619835
2017-12-10T13:43:05.551933: step 291, loss 1.80842, acc 0.59375, prec 0.0160651, recall 0.619835
2017-12-10T13:43:05.739372: step 292, loss 2.21213, acc 0.515625, prec 0.016012, recall 0.619835
2017-12-10T13:43:05.923711: step 293, loss 1.48946, acc 0.609375, prec 0.0160741, recall 0.621399
2017-12-10T13:43:06.109751: step 294, loss 1.07508, acc 0.703125, prec 0.0161462, recall 0.622951
2017-12-10T13:43:06.297058: step 295, loss 1.57511, acc 0.703125, prec 0.0162179, recall 0.62449
2017-12-10T13:43:06.482230: step 296, loss 1.14222, acc 0.6875, prec 0.0163917, recall 0.62753
2017-12-10T13:43:06.667418: step 297, loss 4.43515, acc 0.703125, prec 0.0163606, recall 0.625
2017-12-10T13:43:06.860270: step 298, loss 1.25763, acc 0.671875, prec 0.0163244, recall 0.625
2017-12-10T13:43:07.043278: step 299, loss 2.18563, acc 0.734375, prec 0.0162969, recall 0.62249
2017-12-10T13:43:07.235086: step 300, loss 0.971132, acc 0.78125, prec 0.016273, recall 0.62249
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_0/1512931325/checkpoints/model-300

2017-12-10T13:43:08.457842: step 301, loss 1.58481, acc 0.625, prec 0.0162321, recall 0.62249
2017-12-10T13:43:08.645867: step 302, loss 11.3643, acc 0.65625, prec 0.0161964, recall 0.62
2017-12-10T13:43:08.836984: step 303, loss 1.34763, acc 0.65625, prec 0.0161593, recall 0.62
2017-12-10T13:43:09.023925: step 304, loss 1.42501, acc 0.71875, prec 0.016129, recall 0.62
2017-12-10T13:43:09.211579: step 305, loss 18.1889, acc 0.71875, prec 0.0161006, recall 0.61753
2017-12-10T13:43:09.401008: step 306, loss 1.74253, acc 0.609375, prec 0.0161608, recall 0.619048
2017-12-10T13:43:09.589574: step 307, loss 5.96075, acc 0.578125, prec 0.0162207, recall 0.615686
2017-12-10T13:43:09.776615: step 308, loss 1.33606, acc 0.65625, prec 0.0162853, recall 0.617188
2017-12-10T13:43:09.967757: step 309, loss 2.55399, acc 0.5625, prec 0.0163395, recall 0.618677
2017-12-10T13:43:10.150284: step 310, loss 1.49232, acc 0.546875, prec 0.016291, recall 0.618677
2017-12-10T13:43:10.339999: step 311, loss 2.34741, acc 0.46875, prec 0.0163349, recall 0.620155
2017-12-10T13:43:10.522615: step 312, loss 2.73395, acc 0.390625, prec 0.0162701, recall 0.620155
2017-12-10T13:43:10.706426: step 313, loss 1.61587, acc 0.671875, prec 0.016435, recall 0.623077
2017-12-10T13:43:10.896876: step 314, loss 2.18478, acc 0.4375, prec 0.0164746, recall 0.624521
2017-12-10T13:43:11.082172: step 315, loss 2.05311, acc 0.546875, prec 0.0165256, recall 0.625954
2017-12-10T13:43:11.269852: step 316, loss 1.60279, acc 0.640625, prec 0.0165862, recall 0.627376
2017-12-10T13:43:11.455923: step 317, loss 2.06279, acc 0.578125, prec 0.0165414, recall 0.627376
2017-12-10T13:43:11.640340: step 318, loss 1.82285, acc 0.515625, prec 0.0165884, recall 0.628788
2017-12-10T13:43:11.827156: step 319, loss 1.56053, acc 0.640625, prec 0.0165503, recall 0.628788
2017-12-10T13:43:12.013001: step 320, loss 1.04469, acc 0.71875, prec 0.0165207, recall 0.628788
2017-12-10T13:43:12.202261: step 321, loss 1.152, acc 0.6875, prec 0.0165856, recall 0.630189
2017-12-10T13:43:12.388227: step 322, loss 1.20806, acc 0.640625, prec 0.0167426, recall 0.632959
2017-12-10T13:43:12.574904: step 323, loss 1.91137, acc 0.734375, prec 0.0167161, recall 0.630597
2017-12-10T13:43:12.767721: step 324, loss 0.819555, acc 0.75, prec 0.0166897, recall 0.630597
2017-12-10T13:43:12.953877: step 325, loss 16.2825, acc 0.6875, prec 0.016757, recall 0.627306
2017-12-10T13:43:13.142092: step 326, loss 18.6388, acc 0.65625, prec 0.0168241, recall 0.619565
2017-12-10T13:43:13.334133: step 327, loss 8.90988, acc 0.671875, prec 0.0168892, recall 0.616487
2017-12-10T13:43:13.522100: step 328, loss 1.93043, acc 0.46875, prec 0.016833, recall 0.616487
2017-12-10T13:43:13.712832: step 329, loss 2.90106, acc 0.34375, prec 0.0167641, recall 0.616487
2017-12-10T13:43:13.901766: step 330, loss 2.74781, acc 0.40625, prec 0.0167023, recall 0.616487
2017-12-10T13:43:14.085836: step 331, loss 3.64372, acc 0.3125, prec 0.0166312, recall 0.616487
2017-12-10T13:43:14.277943: step 332, loss 4.09983, acc 0.328125, prec 0.0167517, recall 0.619217
2017-12-10T13:43:14.460179: step 333, loss 3.91292, acc 0.25, prec 0.0167689, recall 0.620567
2017-12-10T13:43:14.650889: step 334, loss 4.2332, acc 0.296875, prec 0.0166969, recall 0.620567
2017-12-10T13:43:14.834908: step 335, loss 4.57795, acc 0.296875, prec 0.0169991, recall 0.625874
2017-12-10T13:43:15.019412: step 336, loss 4.67588, acc 0.3125, prec 0.0170213, recall 0.627178
2017-12-10T13:43:15.214220: step 337, loss 4.46543, acc 0.296875, prec 0.0170417, recall 0.628472
2017-12-10T13:43:15.405226: step 338, loss 4.76763, acc 0.296875, prec 0.0169698, recall 0.628472
2017-12-10T13:43:15.591367: step 339, loss 4.62493, acc 0.3125, prec 0.0170836, recall 0.631034
2017-12-10T13:43:15.780889: step 340, loss 3.10289, acc 0.34375, prec 0.0171997, recall 0.633562
2017-12-10T13:43:15.965007: step 341, loss 3.94959, acc 0.296875, prec 0.017128, recall 0.633562
2017-12-10T13:43:16.151211: step 342, loss 4.16351, acc 0.328125, prec 0.0170601, recall 0.633562
2017-12-10T13:43:16.340046: step 343, loss 2.88083, acc 0.484375, prec 0.0170084, recall 0.633562
2017-12-10T13:43:16.527803: step 344, loss 2.83814, acc 0.390625, prec 0.0170376, recall 0.634812
2017-12-10T13:43:16.716611: step 345, loss 2.93302, acc 0.5, prec 0.0170776, recall 0.636054
2017-12-10T13:43:16.904355: step 346, loss 1.93031, acc 0.484375, prec 0.0171158, recall 0.637288
2017-12-10T13:43:17.089845: step 347, loss 1.61012, acc 0.53125, prec 0.0171584, recall 0.638514
2017-12-10T13:43:17.275737: step 348, loss 2.64828, acc 0.625, prec 0.0173882, recall 0.64214
2017-12-10T13:43:17.464258: step 349, loss 1.26176, acc 0.6875, prec 0.0174455, recall 0.643333
2017-12-10T13:43:17.650086: step 350, loss 1.99197, acc 0.671875, prec 0.0175898, recall 0.645695
2017-12-10T13:43:17.840450: step 351, loss 0.926133, acc 0.71875, prec 0.0177382, recall 0.648026
2017-12-10T13:43:18.027915: step 352, loss 1.04485, acc 0.8125, prec 0.017719, recall 0.648026
2017-12-10T13:43:18.219107: step 353, loss 0.974686, acc 0.734375, prec 0.017692, recall 0.648026
2017-12-10T13:43:18.405241: step 354, loss 3.49139, acc 0.765625, prec 0.0176713, recall 0.643791
2017-12-10T13:43:18.599736: step 355, loss 8.08513, acc 0.796875, prec 0.0176523, recall 0.641694
2017-12-10T13:43:18.793110: step 356, loss 0.821039, acc 0.796875, prec 0.0176318, recall 0.641694
2017-12-10T13:43:18.982636: step 357, loss 7.79571, acc 0.796875, prec 0.0178763, recall 0.643087
2017-12-10T13:43:19.172731: step 358, loss 9.79196, acc 0.78125, prec 0.0179432, recall 0.642173
2017-12-10T13:43:19.358992: step 359, loss 10.4666, acc 0.71875, prec 0.017916, recall 0.640127
2017-12-10T13:43:19.553734: step 360, loss 1.22556, acc 0.703125, prec 0.0179731, recall 0.64127
2017-12-10T13:43:19.740453: step 361, loss 3.74978, acc 0.578125, prec 0.0180188, recall 0.640379
2017-12-10T13:43:19.927915: step 362, loss 2.03853, acc 0.5, prec 0.0180547, recall 0.641509
2017-12-10T13:43:20.112602: step 363, loss 1.93194, acc 0.625, prec 0.0181031, recall 0.642633
2017-12-10T13:43:20.298798: step 364, loss 2.15433, acc 0.578125, prec 0.0180601, recall 0.642633
2017-12-10T13:43:20.483280: step 365, loss 2.56077, acc 0.484375, prec 0.018094, recall 0.64375
2017-12-10T13:43:20.673718: step 366, loss 2.20386, acc 0.546875, prec 0.018134, recall 0.64486
2017-12-10T13:43:20.864789: step 367, loss 3.16394, acc 0.453125, prec 0.0183358, recall 0.648148
2017-12-10T13:43:21.052728: step 368, loss 2.82116, acc 0.515625, prec 0.0185427, recall 0.651376
2017-12-10T13:43:21.239829: step 369, loss 2.37843, acc 0.515625, prec 0.018578, recall 0.652439
2017-12-10T13:43:21.439926: step 370, loss 2.76302, acc 0.375, prec 0.0185986, recall 0.653495
2017-12-10T13:43:21.622336: step 371, loss 2.59081, acc 0.484375, prec 0.0186303, recall 0.654545
2017-12-10T13:43:21.815262: step 372, loss 2.39444, acc 0.453125, prec 0.0185743, recall 0.654545
2017-12-10T13:43:22.002181: step 373, loss 1.62076, acc 0.59375, prec 0.0187012, recall 0.656627
2017-12-10T13:43:22.185467: step 374, loss 2.45593, acc 0.515625, prec 0.0187356, recall 0.657658
2017-12-10T13:43:22.373840: step 375, loss 1.59823, acc 0.625, prec 0.0188647, recall 0.659701
2017-12-10T13:43:22.560553: step 376, loss 2.08076, acc 0.59375, prec 0.0189065, recall 0.660714
2017-12-10T13:43:22.745749: step 377, loss 1.3721, acc 0.6875, prec 0.0188743, recall 0.660714
2017-12-10T13:43:22.933309: step 378, loss 1.39331, acc 0.65625, prec 0.0188391, recall 0.660714
2017-12-10T13:43:23.119962: step 379, loss 1.26083, acc 0.65625, prec 0.018804, recall 0.660714
2017-12-10T13:43:23.308773: step 380, loss 6.83099, acc 0.703125, prec 0.0187754, recall 0.658754
2017-12-10T13:43:23.500416: step 381, loss 0.542909, acc 0.765625, prec 0.0187516, recall 0.658754
2017-12-10T13:43:23.684610: step 382, loss 15.1252, acc 0.609375, prec 0.0187152, recall 0.654867
2017-12-10T13:43:23.877575: step 383, loss 0.974351, acc 0.71875, prec 0.0186869, recall 0.654867
2017-12-10T13:43:24.066556: step 384, loss 3.5504, acc 0.8125, prec 0.0188346, recall 0.654971
2017-12-10T13:43:24.256014: step 385, loss 3.18457, acc 0.71875, prec 0.0188077, recall 0.653061
2017-12-10T13:43:24.443288: step 386, loss 1.82946, acc 0.640625, prec 0.0189359, recall 0.655072
2017-12-10T13:43:24.633509: step 387, loss 0.975222, acc 0.75, prec 0.0189106, recall 0.655072
2017-12-10T13:43:24.820909: step 388, loss 1.33641, acc 0.625, prec 0.0188727, recall 0.655072
2017-12-10T13:43:25.005507: step 389, loss 1.20438, acc 0.6875, prec 0.018923, recall 0.656069
2017-12-10T13:43:25.187846: step 390, loss 1.27051, acc 0.65625, prec 0.0188883, recall 0.656069
2017-12-10T13:43:25.376004: step 391, loss 1.55242, acc 0.640625, prec 0.0188523, recall 0.656069
2017-12-10T13:43:25.565605: step 392, loss 1.81486, acc 0.59375, prec 0.0188929, recall 0.657061
2017-12-10T13:43:25.751363: step 393, loss 1.90814, acc 0.609375, prec 0.0190972, recall 0.66
2017-12-10T13:43:25.935476: step 394, loss 7.03711, acc 0.578125, prec 0.0191372, recall 0.659091
2017-12-10T13:43:26.123696: step 395, loss 2.25006, acc 0.625, prec 0.0191801, recall 0.660057
2017-12-10T13:43:26.313986: step 396, loss 1.21048, acc 0.6875, prec 0.0191486, recall 0.660057
2017-12-10T13:43:26.502594: step 397, loss 1.46473, acc 0.640625, prec 0.0193538, recall 0.662921
2017-12-10T13:43:26.691829: step 398, loss 2.0781, acc 0.515625, prec 0.0193047, recall 0.662921
2017-12-10T13:43:26.878609: step 399, loss 2.20161, acc 0.578125, prec 0.0192622, recall 0.662921
2017-12-10T13:43:27.063594: step 400, loss 1.51794, acc 0.65625, prec 0.0193075, recall 0.663866
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_0/1512931325/checkpoints/model-400

2017-12-10T13:43:28.216061: step 401, loss 7.1145, acc 0.625, prec 0.0192714, recall 0.662011
2017-12-10T13:43:28.406768: step 402, loss 1.04823, acc 0.734375, prec 0.0193245, recall 0.662953
2017-12-10T13:43:28.593541: step 403, loss 1.84457, acc 0.609375, prec 0.0192853, recall 0.662953
2017-12-10T13:43:28.784018: step 404, loss 0.921129, acc 0.71875, prec 0.0192572, recall 0.662953
2017-12-10T13:43:28.976065: step 405, loss 1.71045, acc 0.578125, prec 0.0192152, recall 0.662953
2017-12-10T13:43:29.164281: step 406, loss 1.06189, acc 0.703125, prec 0.0192649, recall 0.663889
2017-12-10T13:43:29.353154: step 407, loss 0.878096, acc 0.734375, prec 0.0194753, recall 0.666667
2017-12-10T13:43:29.542104: step 408, loss 1.08396, acc 0.6875, prec 0.019444, recall 0.666667
2017-12-10T13:43:29.726858: step 409, loss 1.0609, acc 0.71875, prec 0.0194159, recall 0.666667
2017-12-10T13:43:29.911950: step 410, loss 3.95672, acc 0.71875, prec 0.0195466, recall 0.666667
2017-12-10T13:43:30.102792: step 411, loss 0.913242, acc 0.703125, prec 0.0195169, recall 0.666667
2017-12-10T13:43:30.296261: step 412, loss 1.16171, acc 0.671875, prec 0.0194841, recall 0.666667
2017-12-10T13:43:30.483465: step 413, loss 1.2204, acc 0.71875, prec 0.0194562, recall 0.666667
2017-12-10T13:43:30.671492: step 414, loss 1.03231, acc 0.703125, prec 0.0194268, recall 0.666667
2017-12-10T13:43:30.860998: step 415, loss 0.462678, acc 0.828125, prec 0.0194098, recall 0.666667
2017-12-10T13:43:31.050241: step 416, loss 14.6384, acc 0.796875, prec 0.0194707, recall 0.663957
2017-12-10T13:43:31.245307: step 417, loss 28.8389, acc 0.8125, prec 0.019611, recall 0.662198
2017-12-10T13:43:31.443318: step 418, loss 2.80651, acc 0.71875, prec 0.0196623, recall 0.661333
2017-12-10T13:43:31.636259: step 419, loss 1.8647, acc 0.59375, prec 0.0196218, recall 0.661333
2017-12-10T13:43:31.825491: step 420, loss 2.78505, acc 0.53125, prec 0.0196527, recall 0.662234
2017-12-10T13:43:32.013950: step 421, loss 2.09979, acc 0.4375, prec 0.019597, recall 0.662234
2017-12-10T13:43:32.201647: step 422, loss 2.36349, acc 0.578125, prec 0.0196325, recall 0.66313
2017-12-10T13:43:32.384196: step 423, loss 2.81466, acc 0.359375, prec 0.0195695, recall 0.66313
2017-12-10T13:43:32.572466: step 424, loss 2.33439, acc 0.5, prec 0.0195206, recall 0.66313
2017-12-10T13:43:32.755489: step 425, loss 7.05071, acc 0.34375, prec 0.0194583, recall 0.661376
2017-12-10T13:43:32.941522: step 426, loss 8.50291, acc 0.453125, prec 0.0194069, recall 0.659631
2017-12-10T13:43:33.127098: step 427, loss 3.7313, acc 0.40625, prec 0.0195016, recall 0.661417
2017-12-10T13:43:33.316071: step 428, loss 3.48332, acc 0.4375, prec 0.0195231, recall 0.662304
2017-12-10T13:43:33.500638: step 429, loss 3.41808, acc 0.421875, prec 0.0196184, recall 0.664062
2017-12-10T13:43:33.689120: step 430, loss 4.12896, acc 0.21875, prec 0.0195432, recall 0.664062
2017-12-10T13:43:33.874928: step 431, loss 3.87686, acc 0.4375, prec 0.0195644, recall 0.664935
2017-12-10T13:43:34.059827: step 432, loss 3.48166, acc 0.390625, prec 0.0195062, recall 0.664935
2017-12-10T13:43:34.243298: step 433, loss 3.36639, acc 0.390625, prec 0.0195974, recall 0.666667
2017-12-10T13:43:34.430535: step 434, loss 3.46244, acc 0.40625, prec 0.0196153, recall 0.667526
2017-12-10T13:43:34.616858: step 435, loss 3.24153, acc 0.40625, prec 0.019707, recall 0.669231
2017-12-10T13:43:34.805660: step 436, loss 2.90161, acc 0.453125, prec 0.0198027, recall 0.670918
2017-12-10T13:43:34.994815: step 437, loss 3.06565, acc 0.578125, prec 0.019764, recall 0.669211
2017-12-10T13:43:35.183525: step 438, loss 3.9705, acc 0.546875, prec 0.0197225, recall 0.667513
2017-12-10T13:43:35.374275: step 439, loss 2.26363, acc 0.46875, prec 0.0196724, recall 0.667513
2017-12-10T13:43:35.558436: step 440, loss 2.67483, acc 0.46875, prec 0.0197687, recall 0.669192
2017-12-10T13:43:35.744167: step 441, loss 1.94996, acc 0.53125, prec 0.0197246, recall 0.669192
2017-12-10T13:43:35.935230: step 442, loss 1.9049, acc 0.5625, prec 0.0196836, recall 0.669192
2017-12-10T13:43:36.121133: step 443, loss 2.37462, acc 0.65625, prec 0.0197242, recall 0.670025
2017-12-10T13:43:36.313292: step 444, loss 8.72662, acc 0.6875, prec 0.019769, recall 0.669173
2017-12-10T13:43:36.501743: step 445, loss 1.52935, acc 0.796875, prec 0.0198225, recall 0.67
2017-12-10T13:43:36.692312: step 446, loss 2.64274, acc 0.6875, prec 0.0197947, recall 0.668329
2017-12-10T13:43:36.883194: step 447, loss 1.21995, acc 0.625, prec 0.0199042, recall 0.669975
2017-12-10T13:43:37.068725: step 448, loss 1.25104, acc 0.640625, prec 0.0198705, recall 0.669975
2017-12-10T13:43:37.255608: step 449, loss 1.63328, acc 0.625, prec 0.0198354, recall 0.669975
2017-12-10T13:43:37.442279: step 450, loss 2.35789, acc 0.640625, prec 0.0199457, recall 0.671605
2017-12-10T13:43:37.632286: step 451, loss 1.78878, acc 0.5625, prec 0.0199766, recall 0.672414
2017-12-10T13:43:37.821279: step 452, loss 1.08199, acc 0.71875, prec 0.0200219, recall 0.673219
2017-12-10T13:43:38.007598: step 453, loss 0.72051, acc 0.765625, prec 0.020143, recall 0.674817
2017-12-10T13:43:38.194180: step 454, loss 6.39482, acc 0.578125, prec 0.0201049, recall 0.673171
2017-12-10T13:43:38.382667: step 455, loss 6.34949, acc 0.6875, prec 0.0201484, recall 0.67233
2017-12-10T13:43:38.569810: step 456, loss 0.92356, acc 0.703125, prec 0.0201917, recall 0.673123
2017-12-10T13:43:38.755835: step 457, loss 1.20455, acc 0.734375, prec 0.0201668, recall 0.673123
2017-12-10T13:43:38.944156: step 458, loss 11.5861, acc 0.5625, prec 0.0201984, recall 0.672289
2017-12-10T13:43:39.133774: step 459, loss 1.68828, acc 0.59375, prec 0.0202312, recall 0.673077
2017-12-10T13:43:39.323673: step 460, loss 2.32523, acc 0.46875, prec 0.0201816, recall 0.673077
2017-12-10T13:43:39.512347: step 461, loss 1.98643, acc 0.5625, prec 0.0203524, recall 0.675418
2017-12-10T13:43:39.702529: step 462, loss 1.66024, acc 0.65625, prec 0.0203202, recall 0.675418
2017-12-10T13:43:39.886477: step 463, loss 2.0566, acc 0.609375, prec 0.0204243, recall 0.67696
2017-12-10T13:43:40.072529: step 464, loss 1.5659, acc 0.65625, prec 0.0204622, recall 0.677725
2017-12-10T13:43:40.259437: step 465, loss 1.67785, acc 0.578125, prec 0.0204227, recall 0.677725
2017-12-10T13:43:40.447663: step 466, loss 1.69526, acc 0.5625, prec 0.020382, recall 0.677725
2017-12-10T13:43:40.635069: step 467, loss 1.49923, acc 0.5625, prec 0.0203414, recall 0.677725
2017-12-10T13:43:40.825264: step 468, loss 2.09783, acc 0.546875, prec 0.0203691, recall 0.678487
2017-12-10T13:43:41.015509: step 469, loss 1.66161, acc 0.59375, prec 0.0203315, recall 0.678487
2017-12-10T13:43:41.202764: step 470, loss 6.63844, acc 0.6875, prec 0.0203042, recall 0.676887
2017-12-10T13:43:41.391809: step 471, loss 10.3565, acc 0.640625, prec 0.0203419, recall 0.676056
2017-12-10T13:43:41.581556: step 472, loss 1.30454, acc 0.671875, prec 0.0203808, recall 0.676815
2017-12-10T13:43:41.768468: step 473, loss 1.59929, acc 0.65625, prec 0.0204182, recall 0.67757
2017-12-10T13:43:41.960610: step 474, loss 1.89752, acc 0.546875, prec 0.0203766, recall 0.67757
2017-12-10T13:43:42.150063: step 475, loss 1.34802, acc 0.6875, prec 0.0204855, recall 0.67907
2017-12-10T13:43:42.337318: step 476, loss 1.58087, acc 0.625, prec 0.020451, recall 0.67907
2017-12-10T13:43:42.525152: step 477, loss 5.44021, acc 0.625, prec 0.0204182, recall 0.677494
2017-12-10T13:43:42.715936: step 478, loss 1.02886, acc 0.734375, prec 0.0204623, recall 0.678241
2017-12-10T13:43:42.903593: step 479, loss 1.4799, acc 0.765625, prec 0.0205092, recall 0.678984
2017-12-10T13:43:43.093501: step 480, loss 1.06052, acc 0.65625, prec 0.020546, recall 0.679724
2017-12-10T13:43:43.280500: step 481, loss 1.84975, acc 0.59375, prec 0.020577, recall 0.68046
2017-12-10T13:43:43.470331: step 482, loss 4.92247, acc 0.671875, prec 0.0205484, recall 0.678899
2017-12-10T13:43:43.662740: step 483, loss 1.08666, acc 0.640625, prec 0.0205157, recall 0.678899
2017-12-10T13:43:43.856358: step 484, loss 0.866795, acc 0.765625, prec 0.0204944, recall 0.678899
2017-12-10T13:43:44.045086: step 485, loss 2.79349, acc 0.625, prec 0.0205281, recall 0.679634
2017-12-10T13:43:44.229325: step 486, loss 1.73492, acc 0.765625, prec 0.0205744, recall 0.680365
2017-12-10T13:43:44.420336: step 487, loss 1.80423, acc 0.6875, prec 0.0205461, recall 0.680365
2017-12-10T13:43:44.608358: step 488, loss 1.5488, acc 0.546875, prec 0.0207072, recall 0.68254
2017-12-10T13:43:44.795012: step 489, loss 1.18066, acc 0.75, prec 0.0207517, recall 0.683258
2017-12-10T13:43:44.979647: step 490, loss 1.46202, acc 0.671875, prec 0.0207218, recall 0.683258
2017-12-10T13:43:45.167178: step 491, loss 2.25393, acc 0.578125, prec 0.0208847, recall 0.685393
2017-12-10T13:43:45.356611: step 492, loss 9.09247, acc 0.6875, prec 0.0209915, recall 0.685268
2017-12-10T13:43:45.544228: step 493, loss 6.02908, acc 0.6875, prec 0.0209642, recall 0.683742
2017-12-10T13:43:45.732294: step 494, loss 0.992902, acc 0.6875, prec 0.0209356, recall 0.683742
2017-12-10T13:43:45.921481: step 495, loss 8.38433, acc 0.421875, prec 0.0208858, recall 0.68071
2017-12-10T13:43:46.108507: step 496, loss 2.3485, acc 0.453125, prec 0.0208362, recall 0.68071
2017-12-10T13:43:46.278969: step 497, loss 3.18936, acc 0.431373, prec 0.0209942, recall 0.682819
2017-12-10T13:43:46.472065: step 498, loss 1.89164, acc 0.484375, prec 0.0209474, recall 0.682819
2017-12-10T13:43:46.656247: step 499, loss 2.07363, acc 0.515625, prec 0.0210356, recall 0.684211
2017-12-10T13:43:46.840635: step 500, loss 2.24552, acc 0.5, prec 0.0209903, recall 0.684211
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_0/1512931325/checkpoints/model-500

2017-12-10T13:43:48.126581: step 501, loss 2.3774, acc 0.53125, prec 0.020948, recall 0.684211
2017-12-10T13:43:48.312951: step 502, loss 3.19968, acc 0.421875, prec 0.0208961, recall 0.684211
2017-12-10T13:43:48.501946: step 503, loss 2.32824, acc 0.546875, prec 0.0209211, recall 0.684902
2017-12-10T13:43:48.690721: step 504, loss 2.56617, acc 0.53125, prec 0.0209445, recall 0.685589
2017-12-10T13:43:48.878083: step 505, loss 1.76209, acc 0.640625, prec 0.0209776, recall 0.686275
2017-12-10T13:43:49.062670: step 506, loss 2.74796, acc 0.515625, prec 0.0209995, recall 0.686957
2017-12-10T13:43:49.258545: step 507, loss 1.15687, acc 0.65625, prec 0.0211637, recall 0.688985
2017-12-10T13:43:49.442301: step 508, loss 1.48278, acc 0.625, prec 0.02113, recall 0.688985
2017-12-10T13:43:49.630524: step 509, loss 1.83374, acc 0.59375, prec 0.0210937, recall 0.688985
2017-12-10T13:43:49.819405: step 510, loss 1.60738, acc 0.65625, prec 0.0211277, recall 0.689655
2017-12-10T13:43:50.006612: step 511, loss 4.86338, acc 0.59375, prec 0.0210929, recall 0.688172
2017-12-10T13:43:50.195232: step 512, loss 1.02265, acc 0.765625, prec 0.021072, recall 0.688172
2017-12-10T13:43:50.384000: step 513, loss 1.59855, acc 0.703125, prec 0.0211101, recall 0.688841
2017-12-10T13:43:50.574029: step 514, loss 7.08578, acc 0.671875, prec 0.0210824, recall 0.687366
2017-12-10T13:43:50.759710: step 515, loss 0.996164, acc 0.6875, prec 0.0211189, recall 0.688034
2017-12-10T13:43:50.948697: step 516, loss 1.14924, acc 0.71875, prec 0.021094, recall 0.688034
2017-12-10T13:43:51.134800: step 517, loss 0.824551, acc 0.765625, prec 0.0211374, recall 0.688699
2017-12-10T13:43:51.323550: step 518, loss 0.589069, acc 0.859375, prec 0.0211249, recall 0.688699
2017-12-10T13:43:51.512736: step 519, loss 1.02146, acc 0.84375, prec 0.0212391, recall 0.690021
2017-12-10T13:43:51.703844: step 520, loss 0.664513, acc 0.796875, prec 0.021221, recall 0.690021
2017-12-10T13:43:51.891209: step 521, loss 0.544462, acc 0.828125, prec 0.0212697, recall 0.690678
2017-12-10T13:43:52.082529: step 522, loss 0.712061, acc 0.78125, prec 0.0212502, recall 0.690678
2017-12-10T13:43:52.272846: step 523, loss 0.432165, acc 0.890625, prec 0.0213681, recall 0.691983
2017-12-10T13:43:52.464454: step 524, loss 8.01102, acc 0.8125, prec 0.0213528, recall 0.690526
2017-12-10T13:43:52.652751: step 525, loss 5.84973, acc 0.828125, prec 0.0213389, recall 0.689076
2017-12-10T13:43:52.841331: step 526, loss 0.803014, acc 0.796875, prec 0.0213209, recall 0.689076
2017-12-10T13:43:53.031006: step 527, loss 0.505559, acc 0.828125, prec 0.0214327, recall 0.690377
2017-12-10T13:43:53.222258: step 528, loss 0.641919, acc 0.75, prec 0.0214105, recall 0.690377
2017-12-10T13:43:53.411881: step 529, loss 0.756446, acc 0.8125, prec 0.0213938, recall 0.690377
2017-12-10T13:43:53.602630: step 530, loss 0.750566, acc 0.828125, prec 0.0213786, recall 0.690377
2017-12-10T13:43:53.791301: step 531, loss 4.72724, acc 0.765625, prec 0.0214859, recall 0.690229
2017-12-10T13:43:53.981645: step 532, loss 1.11046, acc 0.78125, prec 0.0214664, recall 0.690229
2017-12-10T13:43:54.167541: step 533, loss 24.6006, acc 0.796875, prec 0.0215762, recall 0.690083
2017-12-10T13:43:54.362956: step 534, loss 1.08557, acc 0.65625, prec 0.0216087, recall 0.690722
2017-12-10T13:43:54.552285: step 535, loss 1.49131, acc 0.6875, prec 0.0215809, recall 0.690722
2017-12-10T13:43:54.739982: step 536, loss 1.60403, acc 0.625, prec 0.0215476, recall 0.690722
2017-12-10T13:43:54.926272: step 537, loss 1.62545, acc 0.578125, prec 0.0216359, recall 0.691992
2017-12-10T13:43:55.114241: step 538, loss 1.56149, acc 0.625, prec 0.021728, recall 0.693252
2017-12-10T13:43:55.299574: step 539, loss 1.70583, acc 0.6875, prec 0.021888, recall 0.695122
2017-12-10T13:43:55.490775: step 540, loss 1.80517, acc 0.625, prec 0.0219169, recall 0.69574
2017-12-10T13:43:55.678040: step 541, loss 1.53378, acc 0.578125, prec 0.022004, recall 0.69697
2017-12-10T13:43:55.872544: step 542, loss 1.37935, acc 0.609375, prec 0.0220312, recall 0.697581
2017-12-10T13:43:56.061002: step 543, loss 3.18403, acc 0.546875, prec 0.021992, recall 0.696177
2017-12-10T13:43:56.245213: step 544, loss 1.53642, acc 0.625, prec 0.0220826, recall 0.697395
2017-12-10T13:43:56.431853: step 545, loss 2.37026, acc 0.75, prec 0.0220616, recall 0.696
2017-12-10T13:43:56.624597: step 546, loss 1.58412, acc 0.640625, prec 0.0220295, recall 0.696
2017-12-10T13:43:56.809424: step 547, loss 1.3432, acc 0.625, prec 0.0219961, recall 0.696
2017-12-10T13:43:56.999455: step 548, loss 1.00116, acc 0.765625, prec 0.0222222, recall 0.698413
2017-12-10T13:43:57.184533: step 549, loss 1.27455, acc 0.640625, prec 0.0222516, recall 0.69901
2017-12-10T13:43:57.371981: step 550, loss 1.44712, acc 0.671875, prec 0.0224684, recall 0.701375
2017-12-10T13:43:57.557355: step 551, loss 1.22458, acc 0.75, prec 0.0224458, recall 0.701375
2017-12-10T13:43:57.747665: step 552, loss 0.835853, acc 0.75, prec 0.0224232, recall 0.701375
2017-12-10T13:43:57.938957: step 553, loss 0.624645, acc 0.796875, prec 0.0224663, recall 0.701961
2017-12-10T13:43:58.132542: step 554, loss 0.766229, acc 0.765625, prec 0.0225064, recall 0.702544
2017-12-10T13:43:58.320370: step 555, loss 2.19322, acc 0.765625, prec 0.0224867, recall 0.701172
2017-12-10T13:43:58.510977: step 556, loss 1.04775, acc 0.734375, prec 0.0224628, recall 0.701172
2017-12-10T13:43:58.699396: step 557, loss 0.783354, acc 0.78125, prec 0.0224431, recall 0.701172
2017-12-10T13:43:58.896603: step 558, loss 0.629011, acc 0.84375, prec 0.0224902, recall 0.701754
2017-12-10T13:43:59.084007: step 559, loss 0.83125, acc 0.765625, prec 0.0225301, recall 0.702335
2017-12-10T13:43:59.270751: step 560, loss 0.799901, acc 0.796875, prec 0.0225728, recall 0.702913
2017-12-10T13:43:59.459239: step 561, loss 0.55907, acc 0.8125, prec 0.0225559, recall 0.702913
2017-12-10T13:43:59.649844: step 562, loss 0.863911, acc 0.765625, prec 0.0226565, recall 0.704062
2017-12-10T13:43:59.834846: step 563, loss 0.707425, acc 0.859375, prec 0.0227047, recall 0.704633
2017-12-10T13:44:00.022923: step 564, loss 3.85238, acc 0.90625, prec 0.0227584, recall 0.703846
2017-12-10T13:44:00.218276: step 565, loss 1.48741, acc 0.828125, prec 0.0228035, recall 0.704415
2017-12-10T13:44:00.410169: step 566, loss 16.5795, acc 0.65625, prec 0.0228345, recall 0.703633
2017-12-10T13:44:00.602278: step 567, loss 4.42727, acc 0.765625, prec 0.0228146, recall 0.70229
2017-12-10T13:44:00.789919: step 568, loss 0.605461, acc 0.765625, prec 0.0227934, recall 0.70229
2017-12-10T13:44:00.981757: step 569, loss 3.99204, acc 0.71875, prec 0.0228299, recall 0.701521
2017-12-10T13:44:01.173621: step 570, loss 1.1781, acc 0.703125, prec 0.0228031, recall 0.701521
2017-12-10T13:44:01.364902: step 571, loss 1.54418, acc 0.5625, prec 0.022824, recall 0.702087
2017-12-10T13:44:01.554286: step 572, loss 1.7017, acc 0.59375, prec 0.0228476, recall 0.702652
2017-12-10T13:44:01.748000: step 573, loss 1.9107, acc 0.53125, prec 0.0228055, recall 0.702652
2017-12-10T13:44:01.934588: step 574, loss 1.69921, acc 0.5, prec 0.0227607, recall 0.702652
2017-12-10T13:44:02.123189: step 575, loss 3.05067, acc 0.4375, prec 0.0227704, recall 0.703214
2017-12-10T13:44:02.315206: step 576, loss 1.55712, acc 0.59375, prec 0.0227342, recall 0.703214
2017-12-10T13:44:02.502619: step 577, loss 2.47664, acc 0.453125, prec 0.0227453, recall 0.703774
2017-12-10T13:44:02.686893: step 578, loss 5.8927, acc 0.53125, prec 0.0228241, recall 0.703565
2017-12-10T13:44:02.875986: step 579, loss 2.04261, acc 0.59375, prec 0.0228474, recall 0.70412
2017-12-10T13:44:03.063997: step 580, loss 2.65278, acc 0.4375, prec 0.0228568, recall 0.704673
2017-12-10T13:44:03.248449: step 581, loss 1.34313, acc 0.65625, prec 0.0229447, recall 0.705773
2017-12-10T13:44:03.436877: step 582, loss 1.50775, acc 0.578125, prec 0.0229072, recall 0.705773
2017-12-10T13:44:03.622388: step 583, loss 1.85625, acc 0.578125, prec 0.0228699, recall 0.705773
2017-12-10T13:44:03.809675: step 584, loss 1.73098, acc 0.578125, prec 0.0228916, recall 0.70632
2017-12-10T13:44:03.999862: step 585, loss 1.26562, acc 0.65625, prec 0.0228613, recall 0.70632
2017-12-10T13:44:04.187934: step 586, loss 1.23855, acc 0.75, prec 0.022898, recall 0.706865
2017-12-10T13:44:04.372262: step 587, loss 0.991141, acc 0.71875, prec 0.0228733, recall 0.706865
2017-12-10T13:44:04.562402: step 588, loss 0.780138, acc 0.78125, prec 0.0228541, recall 0.706865
2017-12-10T13:44:04.756033: step 589, loss 0.813432, acc 0.796875, prec 0.0228948, recall 0.707407
2017-12-10T13:44:04.946265: step 590, loss 5.98391, acc 0.75, prec 0.0229328, recall 0.706642
2017-12-10T13:44:05.134694: step 591, loss 1.28003, acc 0.765625, prec 0.0230291, recall 0.707721
2017-12-10T13:44:05.322414: step 592, loss 14.7768, acc 0.8125, prec 0.0231307, recall 0.707495
2017-12-10T13:44:05.514501: step 593, loss 0.451389, acc 0.84375, prec 0.0231169, recall 0.707495
2017-12-10T13:44:05.703752: step 594, loss 1.60405, acc 0.84375, prec 0.0231045, recall 0.706204
2017-12-10T13:44:05.890823: step 595, loss 1.698, acc 0.734375, prec 0.0231976, recall 0.707273
2017-12-10T13:44:06.078698: step 596, loss 1.72578, acc 0.734375, prec 0.0232323, recall 0.707804
2017-12-10T13:44:06.269721: step 597, loss 1.77889, acc 0.671875, prec 0.0232613, recall 0.708333
2017-12-10T13:44:06.459736: step 598, loss 1.24434, acc 0.65625, prec 0.0232309, recall 0.708333
2017-12-10T13:44:06.644774: step 599, loss 0.915846, acc 0.734375, prec 0.0232075, recall 0.708333
2017-12-10T13:44:06.831041: step 600, loss 1.25518, acc 0.671875, prec 0.0231786, recall 0.708333
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_0/1512931325/checkpoints/model-600

2017-12-10T13:44:08.015627: step 601, loss 1.40254, acc 0.640625, prec 0.0232049, recall 0.708861
2017-12-10T13:44:08.201609: step 602, loss 1.46111, acc 0.671875, prec 0.0231761, recall 0.708861
2017-12-10T13:44:08.390024: step 603, loss 1.72902, acc 0.59375, prec 0.0231982, recall 0.709386
2017-12-10T13:44:08.576668: step 604, loss 1.78533, acc 0.703125, prec 0.0232298, recall 0.70991
2017-12-10T13:44:08.765471: step 605, loss 1.22403, acc 0.671875, prec 0.023201, recall 0.70991
2017-12-10T13:44:08.947770: step 606, loss 4.26499, acc 0.78125, prec 0.0232408, recall 0.709156
2017-12-10T13:44:09.139305: step 607, loss 1.1549, acc 0.75, prec 0.0232763, recall 0.709677
2017-12-10T13:44:09.327710: step 608, loss 3.01848, acc 0.609375, prec 0.0233009, recall 0.708929
2017-12-10T13:44:09.519482: step 609, loss 2.70308, acc 0.78125, prec 0.0233404, recall 0.708185
2017-12-10T13:44:09.709905: step 610, loss 1.14295, acc 0.6875, prec 0.023313, recall 0.708185
2017-12-10T13:44:09.893919: step 611, loss 0.988198, acc 0.671875, prec 0.0232844, recall 0.708185
2017-12-10T13:44:10.082490: step 612, loss 1.79279, acc 0.625, prec 0.0233088, recall 0.708703
2017-12-10T13:44:10.270621: step 613, loss 1.71712, acc 0.578125, prec 0.0233291, recall 0.70922
2017-12-10T13:44:10.458664: step 614, loss 1.63457, acc 0.625, prec 0.0233533, recall 0.709734
2017-12-10T13:44:10.643334: step 615, loss 1.35948, acc 0.625, prec 0.0233207, recall 0.709734
2017-12-10T13:44:10.830691: step 616, loss 0.921557, acc 0.796875, prec 0.0234166, recall 0.710758
2017-12-10T13:44:11.018240: step 617, loss 1.24795, acc 0.609375, prec 0.023496, recall 0.711775
2017-12-10T13:44:11.206120: step 618, loss 0.502669, acc 0.828125, prec 0.023481, recall 0.711775
2017-12-10T13:44:11.396286: step 619, loss 0.888734, acc 0.8125, prec 0.0235212, recall 0.712281
2017-12-10T13:44:11.585203: step 620, loss 1.34045, acc 0.6875, prec 0.0235505, recall 0.712785
2017-12-10T13:44:11.777127: step 621, loss 0.924299, acc 0.78125, prec 0.0235879, recall 0.713287
2017-12-10T13:44:11.963753: step 622, loss 0.678564, acc 0.8125, prec 0.0235716, recall 0.713287
2017-12-10T13:44:12.152108: step 623, loss 0.802921, acc 0.796875, prec 0.0235539, recall 0.713287
2017-12-10T13:44:12.338774: step 624, loss 0.974167, acc 0.71875, prec 0.0235294, recall 0.713287
2017-12-10T13:44:12.526797: step 625, loss 14.0043, acc 0.828125, prec 0.0235159, recall 0.712042
2017-12-10T13:44:12.715054: step 626, loss 0.795931, acc 0.78125, prec 0.0234969, recall 0.712042
2017-12-10T13:44:12.903967: step 627, loss 0.491411, acc 0.875, prec 0.0234861, recall 0.712042
2017-12-10T13:44:13.093504: step 628, loss 1.33528, acc 0.890625, prec 0.023589, recall 0.713043
2017-12-10T13:44:13.283265: step 629, loss 0.993054, acc 0.78125, prec 0.02357, recall 0.713043
2017-12-10T13:44:13.471630: step 630, loss 0.532515, acc 0.828125, prec 0.0235551, recall 0.713043
2017-12-10T13:44:13.654983: step 631, loss 0.608252, acc 0.859375, prec 0.023599, recall 0.713542
2017-12-10T13:44:13.847884: step 632, loss 0.683665, acc 0.765625, prec 0.0235787, recall 0.713542
2017-12-10T13:44:14.034687: step 633, loss 1.32589, acc 0.828125, prec 0.0236198, recall 0.714038
2017-12-10T13:44:14.226207: step 634, loss 0.593315, acc 0.765625, prec 0.0235995, recall 0.714038
2017-12-10T13:44:14.415308: step 635, loss 4.49635, acc 0.84375, prec 0.0236432, recall 0.713299
2017-12-10T13:44:14.603775: step 636, loss 1.29827, acc 0.65625, prec 0.0236693, recall 0.713793
2017-12-10T13:44:14.790686: step 637, loss 1.91425, acc 0.78125, prec 0.0236517, recall 0.712565
2017-12-10T13:44:14.983295: step 638, loss 3.84795, acc 0.703125, prec 0.0236274, recall 0.71134
2017-12-10T13:44:15.175806: step 639, loss 0.906713, acc 0.71875, prec 0.0236589, recall 0.711835
2017-12-10T13:44:15.363013: step 640, loss 0.838447, acc 0.765625, prec 0.0236386, recall 0.711835
2017-12-10T13:44:15.546308: step 641, loss 1.25511, acc 0.734375, prec 0.0236713, recall 0.712329
2017-12-10T13:44:15.735238: step 642, loss 1.0661, acc 0.625, prec 0.023639, recall 0.712329
2017-12-10T13:44:15.926061: step 643, loss 1.05567, acc 0.703125, prec 0.0237244, recall 0.713311
2017-12-10T13:44:16.114788: step 644, loss 1.24143, acc 0.6875, prec 0.0238082, recall 0.714286
2017-12-10T13:44:16.301877: step 645, loss 1.12747, acc 0.6875, prec 0.0237812, recall 0.714286
2017-12-10T13:44:16.485596: step 646, loss 1.1596, acc 0.578125, prec 0.0237449, recall 0.714286
2017-12-10T13:44:16.673866: step 647, loss 1.09521, acc 0.671875, prec 0.0237168, recall 0.714286
2017-12-10T13:44:16.864328: step 648, loss 1.35284, acc 0.671875, prec 0.0237437, recall 0.714771
2017-12-10T13:44:17.051892: step 649, loss 1.09215, acc 0.703125, prec 0.0237733, recall 0.715254
2017-12-10T13:44:17.241525: step 650, loss 1.13955, acc 0.78125, prec 0.0238095, recall 0.715736
2017-12-10T13:44:17.426483: step 651, loss 10.2068, acc 0.765625, prec 0.0238457, recall 0.715008
2017-12-10T13:44:17.621085: step 652, loss 14.7388, acc 0.734375, prec 0.0238804, recall 0.713087
2017-12-10T13:44:17.810524: step 653, loss 3.78165, acc 0.6875, prec 0.023855, recall 0.711893
2017-12-10T13:44:17.999479: step 654, loss 1.53525, acc 0.71875, prec 0.0239403, recall 0.712855
2017-12-10T13:44:18.188468: step 655, loss 1.25262, acc 0.640625, prec 0.0239095, recall 0.712855
2017-12-10T13:44:18.373863: step 656, loss 1.51367, acc 0.578125, prec 0.0238734, recall 0.712855
2017-12-10T13:44:18.566066: step 657, loss 1.93917, acc 0.609375, prec 0.0238401, recall 0.712855
2017-12-10T13:44:18.751518: step 658, loss 1.93251, acc 0.546875, prec 0.0239104, recall 0.71381
2017-12-10T13:44:18.940365: step 659, loss 6.03265, acc 0.46875, prec 0.0239751, recall 0.713576
2017-12-10T13:44:19.130348: step 660, loss 2.14352, acc 0.453125, prec 0.0239285, recall 0.713576
2017-12-10T13:44:19.316985: step 661, loss 1.51268, acc 0.578125, prec 0.0239468, recall 0.71405
2017-12-10T13:44:19.511543: step 662, loss 2.32708, acc 0.53125, prec 0.023907, recall 0.71405
2017-12-10T13:44:19.695736: step 663, loss 1.91217, acc 0.5, prec 0.0239187, recall 0.714521
2017-12-10T13:44:19.881233: step 664, loss 2.42083, acc 0.515625, prec 0.0239854, recall 0.715461
2017-12-10T13:44:20.066043: step 665, loss 2.50328, acc 0.484375, prec 0.0239956, recall 0.715928
2017-12-10T13:44:20.251268: step 666, loss 1.847, acc 0.625, prec 0.0239639, recall 0.715928
2017-12-10T13:44:20.435165: step 667, loss 2.07561, acc 0.515625, prec 0.0240303, recall 0.716858
2017-12-10T13:44:20.618529: step 668, loss 1.14178, acc 0.671875, prec 0.0240561, recall 0.71732
2017-12-10T13:44:20.804344: step 669, loss 1.72637, acc 0.625, prec 0.0240245, recall 0.71732
2017-12-10T13:44:20.986784: step 670, loss 3.81978, acc 0.546875, prec 0.0240411, recall 0.716612
2017-12-10T13:44:21.180798: step 671, loss 1.70774, acc 0.625, prec 0.0240096, recall 0.716612
2017-12-10T13:44:21.370279: step 672, loss 1.98659, acc 0.625, prec 0.0239782, recall 0.716612
2017-12-10T13:44:21.556156: step 673, loss 1.24042, acc 0.6875, prec 0.0239521, recall 0.716612
2017-12-10T13:44:21.743139: step 674, loss 1.07601, acc 0.640625, prec 0.0239752, recall 0.717073
2017-12-10T13:44:21.930368: step 675, loss 0.778086, acc 0.78125, prec 0.02401, recall 0.717532
2017-12-10T13:44:22.119562: step 676, loss 0.513446, acc 0.8125, prec 0.0239944, recall 0.717532
2017-12-10T13:44:22.307788: step 677, loss 1.08592, acc 0.671875, prec 0.023967, recall 0.717532
2017-12-10T13:44:22.497729: step 678, loss 0.688889, acc 0.796875, prec 0.0240559, recall 0.718447
2017-12-10T13:44:22.683490: step 679, loss 1.08239, acc 0.734375, prec 0.0240866, recall 0.718901
2017-12-10T13:44:22.871465: step 680, loss 1.04241, acc 0.84375, prec 0.0241264, recall 0.719355
2017-12-10T13:44:23.063835: step 681, loss 7.61885, acc 0.859375, prec 0.0241687, recall 0.71865
2017-12-10T13:44:23.255131: step 682, loss 0.505534, acc 0.859375, prec 0.0241569, recall 0.71865
2017-12-10T13:44:23.440845: step 683, loss 0.453691, acc 0.8125, prec 0.024194, recall 0.719101
2017-12-10T13:44:23.626038: step 684, loss 4.75874, acc 0.796875, prec 0.0241783, recall 0.717949
2017-12-10T13:44:23.811868: step 685, loss 0.485953, acc 0.8125, prec 0.0242679, recall 0.71885
2017-12-10T13:44:24.004596: step 686, loss 3.89501, acc 0.796875, prec 0.0242522, recall 0.717703
2017-12-10T13:44:24.196645: step 687, loss 1.2019, acc 0.765625, prec 0.0242852, recall 0.718153
2017-12-10T13:44:24.384349: step 688, loss 0.757532, acc 0.8125, prec 0.0242695, recall 0.718153
2017-12-10T13:44:24.573257: step 689, loss 0.77105, acc 0.75, prec 0.0242486, recall 0.718153
2017-12-10T13:44:24.755027: step 690, loss 1.07449, acc 0.71875, prec 0.02433, recall 0.719048
2017-12-10T13:44:24.948201: step 691, loss 1.10238, acc 0.75, prec 0.0244138, recall 0.719937
2017-12-10T13:44:25.133171: step 692, loss 1.02453, acc 0.6875, prec 0.0244399, recall 0.720379
2017-12-10T13:44:25.322545: step 693, loss 1.71343, acc 0.78125, prec 0.0245261, recall 0.72126
2017-12-10T13:44:25.514576: step 694, loss 0.948595, acc 0.765625, prec 0.0245064, recall 0.72126
2017-12-10T13:44:25.702598: step 695, loss 1.2996, acc 0.78125, prec 0.0245924, recall 0.722135
2017-12-10T13:44:25.893390: step 696, loss 0.561176, acc 0.75, prec 0.0245713, recall 0.722135
2017-12-10T13:44:26.079212: step 697, loss 0.948502, acc 0.796875, prec 0.0246584, recall 0.723005
2017-12-10T13:44:26.269630: step 698, loss 5.83694, acc 0.6875, prec 0.0246334, recall 0.721875
2017-12-10T13:44:26.457566: step 699, loss 1.08859, acc 0.75, prec 0.0246124, recall 0.721875
2017-12-10T13:44:26.643995: step 700, loss 0.972008, acc 0.75, prec 0.0246434, recall 0.722309
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_0/1512931325/checkpoints/model-700

2017-12-10T13:44:27.843486: step 701, loss 4.03334, acc 0.65625, prec 0.0246159, recall 0.721184
2017-12-10T13:44:28.033231: step 702, loss 1.20106, acc 0.703125, prec 0.0246946, recall 0.72205
2017-12-10T13:44:28.222746: step 703, loss 1.04409, acc 0.65625, prec 0.0246658, recall 0.72205
2017-12-10T13:44:28.409478: step 704, loss 1.04664, acc 0.671875, prec 0.0246901, recall 0.722481
2017-12-10T13:44:28.599358: step 705, loss 0.837976, acc 0.703125, prec 0.0246652, recall 0.722481
2017-12-10T13:44:28.795793: step 706, loss 0.796972, acc 0.734375, prec 0.0246946, recall 0.72291
2017-12-10T13:44:28.983536: step 707, loss 1.00774, acc 0.734375, prec 0.024724, recall 0.723338
2017-12-10T13:44:29.171144: step 708, loss 1.00291, acc 0.71875, prec 0.0247005, recall 0.723338
2017-12-10T13:44:29.365189: step 709, loss 1.31076, acc 0.734375, prec 0.0247298, recall 0.723765
2017-12-10T13:44:29.554661: step 710, loss 3.99604, acc 0.640625, prec 0.0247011, recall 0.72265
2017-12-10T13:44:29.741668: step 711, loss 1.42117, acc 0.6875, prec 0.0247264, recall 0.723077
2017-12-10T13:44:29.930889: step 712, loss 0.800177, acc 0.796875, prec 0.0247095, recall 0.723077
2017-12-10T13:44:30.115642: step 713, loss 1.41838, acc 0.671875, prec 0.0246823, recall 0.723077
2017-12-10T13:44:30.300208: step 714, loss 1.2268, acc 0.734375, prec 0.0247114, recall 0.723502
2017-12-10T13:44:30.483491: step 715, loss 3.26665, acc 0.71875, prec 0.0247405, recall 0.722818
2017-12-10T13:44:30.674591: step 716, loss 1.16539, acc 0.671875, prec 0.0247644, recall 0.723242
2017-12-10T13:44:30.866007: step 717, loss 0.955412, acc 0.75, prec 0.0247947, recall 0.723664
2017-12-10T13:44:31.054949: step 718, loss 0.727728, acc 0.75, prec 0.024774, recall 0.723664
2017-12-10T13:44:31.251068: step 719, loss 1.31149, acc 0.6875, prec 0.0247481, recall 0.723664
2017-12-10T13:44:31.440364: step 720, loss 1.04265, acc 0.71875, prec 0.0247248, recall 0.723664
2017-12-10T13:44:31.624836: step 721, loss 11.9267, acc 0.6875, prec 0.0247512, recall 0.722983
2017-12-10T13:44:31.813690: step 722, loss 0.980529, acc 0.828125, prec 0.0248894, recall 0.724242
2017-12-10T13:44:32.000227: step 723, loss 1.76124, acc 0.625, prec 0.024909, recall 0.72466
2017-12-10T13:44:32.189228: step 724, loss 1.0268, acc 0.734375, prec 0.0249377, recall 0.725076
2017-12-10T13:44:32.377451: step 725, loss 0.76932, acc 0.8125, prec 0.0249221, recall 0.725076
2017-12-10T13:44:32.567909: step 726, loss 1.48914, acc 0.640625, prec 0.024943, recall 0.72549
2017-12-10T13:44:32.752556: step 727, loss 2.60178, acc 0.640625, prec 0.0249145, recall 0.724398
2017-12-10T13:44:32.942798: step 728, loss 1.8325, acc 0.578125, prec 0.0249302, recall 0.724812
2017-12-10T13:44:33.134083: step 729, loss 0.847803, acc 0.84375, prec 0.0250181, recall 0.725637
2017-12-10T13:44:33.319738: step 730, loss 1.226, acc 0.65625, prec 0.02504, recall 0.726048
2017-12-10T13:44:33.507244: step 731, loss 0.847969, acc 0.8125, prec 0.0250245, recall 0.726048
2017-12-10T13:44:33.698273: step 732, loss 1.07457, acc 0.65625, prec 0.0249961, recall 0.726048
2017-12-10T13:44:33.886464: step 733, loss 2.54956, acc 0.71875, prec 0.0250747, recall 0.725782
2017-12-10T13:44:34.078730: step 734, loss 0.92392, acc 0.6875, prec 0.025099, recall 0.72619
2017-12-10T13:44:34.266939: step 735, loss 0.842936, acc 0.78125, prec 0.0250809, recall 0.72619
2017-12-10T13:44:34.452959: step 736, loss 0.792521, acc 0.78125, prec 0.025113, recall 0.726597
2017-12-10T13:44:34.640339: step 737, loss 0.771036, acc 0.765625, prec 0.0250937, recall 0.726597
2017-12-10T13:44:34.829473: step 738, loss 0.870156, acc 0.78125, prec 0.0250756, recall 0.726597
2017-12-10T13:44:35.018974: step 739, loss 0.77557, acc 0.765625, prec 0.0252062, recall 0.727811
2017-12-10T13:44:35.208450: step 740, loss 1.59832, acc 0.734375, prec 0.0252342, recall 0.728213
2017-12-10T13:44:35.394569: step 741, loss 0.75878, acc 0.84375, prec 0.0252213, recall 0.728213
2017-12-10T13:44:35.583017: step 742, loss 0.945783, acc 0.734375, prec 0.0251993, recall 0.728213
2017-12-10T13:44:35.771299: step 743, loss 0.46096, acc 0.875, prec 0.0252388, recall 0.728614
2017-12-10T13:44:35.960494: step 744, loss 13.5864, acc 0.828125, prec 0.0252272, recall 0.726471
2017-12-10T13:44:36.150016: step 745, loss 10.0135, acc 0.78125, prec 0.0252603, recall 0.725806
2017-12-10T13:44:36.345323: step 746, loss 5.18798, acc 0.71875, prec 0.0252881, recall 0.725146
2017-12-10T13:44:36.537528: step 747, loss 3.47122, acc 0.625, prec 0.0253081, recall 0.72449
2017-12-10T13:44:36.727693: step 748, loss 1.61076, acc 0.765625, prec 0.0253384, recall 0.724891
2017-12-10T13:44:36.917353: step 749, loss 1.17502, acc 0.625, prec 0.0253075, recall 0.724891
2017-12-10T13:44:37.105420: step 750, loss 1.83835, acc 0.53125, prec 0.0253184, recall 0.725291
2017-12-10T13:44:37.292814: step 751, loss 1.9469, acc 0.546875, prec 0.0252812, recall 0.725291
2017-12-10T13:44:37.481946: step 752, loss 2.5279, acc 0.515625, prec 0.0253401, recall 0.726087
2017-12-10T13:44:37.671388: step 753, loss 3.33823, acc 0.5, prec 0.0253484, recall 0.726483
2017-12-10T13:44:37.860436: step 754, loss 2.96828, acc 0.421875, prec 0.0253011, recall 0.726483
2017-12-10T13:44:38.048828: step 755, loss 2.08231, acc 0.59375, prec 0.0253171, recall 0.726879
2017-12-10T13:44:38.237200: step 756, loss 2.1871, acc 0.5, prec 0.0252764, recall 0.726879
2017-12-10T13:44:38.419365: step 757, loss 2.81849, acc 0.46875, prec 0.0253311, recall 0.727666
2017-12-10T13:44:38.605131: step 758, loss 2.52468, acc 0.421875, prec 0.0253329, recall 0.728058
2017-12-10T13:44:38.795333: step 759, loss 2.28738, acc 0.5, prec 0.0253411, recall 0.728448
2017-12-10T13:44:38.978327: step 760, loss 2.12121, acc 0.484375, prec 0.0252994, recall 0.728448
2017-12-10T13:44:39.167242: step 761, loss 2.10674, acc 0.5, prec 0.0253076, recall 0.728838
2017-12-10T13:44:39.351151: step 762, loss 2.48388, acc 0.5, prec 0.0254128, recall 0.73
2017-12-10T13:44:39.536240: step 763, loss 1.57083, acc 0.625, prec 0.0254309, recall 0.730385
2017-12-10T13:44:39.720041: step 764, loss 2.26573, acc 0.546875, prec 0.0253943, recall 0.730385
2017-12-10T13:44:39.907389: step 765, loss 1.3467, acc 0.6875, prec 0.0254174, recall 0.730769
2017-12-10T13:44:40.100514: step 766, loss 1.15929, acc 0.640625, prec 0.0253885, recall 0.730769
2017-12-10T13:44:40.287073: step 767, loss 0.970212, acc 0.6875, prec 0.0253634, recall 0.730769
2017-12-10T13:44:40.475848: step 768, loss 3.48549, acc 0.671875, prec 0.0253865, recall 0.730114
2017-12-10T13:44:40.664407: step 769, loss 0.824299, acc 0.75, prec 0.0254145, recall 0.730496
2017-12-10T13:44:40.852468: step 770, loss 0.960223, acc 0.828125, prec 0.0255449, recall 0.731638
2017-12-10T13:44:41.045330: step 771, loss 0.477777, acc 0.921875, prec 0.0255867, recall 0.732017
2017-12-10T13:44:41.236328: step 772, loss 0.70537, acc 0.859375, prec 0.0255753, recall 0.732017
2017-12-10T13:44:41.423133: step 773, loss 0.870332, acc 0.75, prec 0.0255552, recall 0.732017
2017-12-10T13:44:41.610714: step 774, loss 0.47873, acc 0.8125, prec 0.0255401, recall 0.732017
2017-12-10T13:44:41.799773: step 775, loss 0.542181, acc 0.8125, prec 0.025525, recall 0.732017
2017-12-10T13:44:41.985261: step 776, loss 0.220488, acc 0.921875, prec 0.0255187, recall 0.732017
2017-12-10T13:44:42.173224: step 777, loss 0.263784, acc 0.90625, prec 0.0255112, recall 0.732017
2017-12-10T13:44:42.361252: step 778, loss 3.42155, acc 0.84375, prec 0.0256435, recall 0.732118
2017-12-10T13:44:42.551739: step 779, loss 7.76256, acc 0.84375, prec 0.0256801, recall 0.731469
2017-12-10T13:44:42.743746: step 780, loss 0.371915, acc 0.859375, prec 0.0256687, recall 0.731469
2017-12-10T13:44:42.928128: step 781, loss 0.408147, acc 0.859375, prec 0.0256574, recall 0.731469
2017-12-10T13:44:43.115282: step 782, loss 0.56842, acc 0.828125, prec 0.0256913, recall 0.731844
2017-12-10T13:44:43.302446: step 783, loss 0.739194, acc 0.78125, prec 0.0257214, recall 0.732218
2017-12-10T13:44:43.488457: step 784, loss 0.87271, acc 0.828125, prec 0.0257553, recall 0.732591
2017-12-10T13:44:43.675202: step 785, loss 0.899467, acc 0.75, prec 0.0258304, recall 0.733333
2017-12-10T13:44:43.870072: step 786, loss 0.862963, acc 0.71875, prec 0.0258077, recall 0.733333
2017-12-10T13:44:44.058917: step 787, loss 0.521404, acc 0.859375, prec 0.0257964, recall 0.733333
2017-12-10T13:44:44.247830: step 788, loss 0.482201, acc 0.828125, prec 0.0258776, recall 0.734072
2017-12-10T13:44:44.434698: step 789, loss 0.726853, acc 0.84375, prec 0.0259126, recall 0.73444
2017-12-10T13:44:44.624055: step 790, loss 0.779157, acc 0.75, prec 0.0259398, recall 0.734807
2017-12-10T13:44:44.811680: step 791, loss 0.68337, acc 0.828125, prec 0.0259259, recall 0.734807
2017-12-10T13:44:45.001369: step 792, loss 0.719392, acc 0.703125, prec 0.0259019, recall 0.734807
2017-12-10T13:44:45.190412: step 793, loss 0.60646, acc 0.828125, prec 0.0259355, recall 0.735172
2017-12-10T13:44:45.378433: step 794, loss 3.14992, acc 0.921875, prec 0.0259778, recall 0.734525
2017-12-10T13:44:45.574235: step 795, loss 1.2603, acc 0.84375, prec 0.0260125, recall 0.73489
2017-12-10T13:44:45.768087: step 796, loss 1.17387, acc 0.703125, prec 0.0260358, recall 0.735254
2017-12-10T13:44:45.955745: step 797, loss 0.820591, acc 0.75, prec 0.0260629, recall 0.735616
2017-12-10T13:44:46.146058: step 798, loss 0.855009, acc 0.8125, prec 0.0261422, recall 0.736339
2017-12-10T13:44:46.336887: step 799, loss 1.10328, acc 0.796875, prec 0.0261729, recall 0.736699
2017-12-10T13:44:46.526657: step 800, loss 2.12784, acc 0.6875, prec 0.0261947, recall 0.737057
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_0/1512931325/checkpoints/model-800

2017-12-10T13:44:47.860187: step 801, loss 11.3318, acc 0.703125, prec 0.0261719, recall 0.736054
2017-12-10T13:44:48.047193: step 802, loss 0.64455, acc 0.8125, prec 0.0261567, recall 0.736054
2017-12-10T13:44:48.235049: step 803, loss 1.30075, acc 0.75, prec 0.0261836, recall 0.736413
2017-12-10T13:44:48.423400: step 804, loss 0.557912, acc 0.78125, prec 0.0261659, recall 0.736413
2017-12-10T13:44:48.612245: step 805, loss 0.798468, acc 0.71875, prec 0.0261432, recall 0.736413
2017-12-10T13:44:48.799277: step 806, loss 1.56096, acc 0.6875, prec 0.026118, recall 0.736413
2017-12-10T13:44:48.988544: step 807, loss 0.892929, acc 0.71875, prec 0.0261891, recall 0.737127
2017-12-10T13:44:49.175875: step 808, loss 3.74776, acc 0.75, prec 0.026217, recall 0.736486
2017-12-10T13:44:49.372834: step 809, loss 1.21831, acc 0.6875, prec 0.0262386, recall 0.736842
2017-12-10T13:44:49.561495: step 810, loss 4.7004, acc 0.8125, prec 0.0262716, recall 0.736205
2017-12-10T13:44:49.757113: step 811, loss 3.97388, acc 0.703125, prec 0.0262489, recall 0.735215
2017-12-10T13:44:49.945808: step 812, loss 0.973066, acc 0.703125, prec 0.026225, recall 0.735215
2017-12-10T13:44:50.130428: step 813, loss 1.79439, acc 0.53125, prec 0.0262339, recall 0.73557
2017-12-10T13:44:50.315540: step 814, loss 1.70816, acc 0.65625, prec 0.0262994, recall 0.736278
2017-12-10T13:44:50.506764: step 815, loss 2.83787, acc 0.5625, prec 0.0263108, recall 0.736631
2017-12-10T13:44:50.697071: step 816, loss 1.66516, acc 0.671875, prec 0.0263309, recall 0.736983
2017-12-10T13:44:50.887542: step 817, loss 1.37184, acc 0.65625, prec 0.0263496, recall 0.737333
2017-12-10T13:44:51.076510: step 818, loss 2.38162, acc 0.53125, prec 0.026312, recall 0.737333
2017-12-10T13:44:51.264149: step 819, loss 2.24144, acc 0.609375, prec 0.026327, recall 0.737683
2017-12-10T13:44:51.453276: step 820, loss 1.44676, acc 0.640625, prec 0.0263907, recall 0.73838
2017-12-10T13:44:51.639236: step 821, loss 1.7627, acc 0.5625, prec 0.0264019, recall 0.738727
2017-12-10T13:44:51.826123: step 822, loss 1.69397, acc 0.640625, prec 0.0263731, recall 0.738727
2017-12-10T13:44:52.011913: step 823, loss 1.68778, acc 0.671875, prec 0.0263469, recall 0.738727
2017-12-10T13:44:52.199399: step 824, loss 1.56855, acc 0.59375, prec 0.0263145, recall 0.738727
2017-12-10T13:44:52.384567: step 825, loss 1.48231, acc 0.640625, prec 0.026286, recall 0.738727
2017-12-10T13:44:52.574567: step 826, loss 0.886925, acc 0.734375, prec 0.0262649, recall 0.738727
2017-12-10T13:44:52.764982: step 827, loss 1.00991, acc 0.765625, prec 0.0262463, recall 0.738727
2017-12-10T13:44:52.950503: step 828, loss 1.0062, acc 0.734375, prec 0.0262712, recall 0.739073
2017-12-10T13:44:53.139591: step 829, loss 1.03482, acc 0.734375, prec 0.0262502, recall 0.739073
2017-12-10T13:44:53.325877: step 830, loss 1.66243, acc 0.796875, prec 0.0263715, recall 0.740106
2017-12-10T13:44:53.513206: step 831, loss 0.733281, acc 0.765625, prec 0.0264443, recall 0.740789
2017-12-10T13:44:53.703764: step 832, loss 0.629304, acc 0.828125, prec 0.0265221, recall 0.74147
2017-12-10T13:44:53.897042: step 833, loss 1.10147, acc 0.828125, prec 0.0265541, recall 0.741809
2017-12-10T13:44:54.086087: step 834, loss 0.866576, acc 0.765625, prec 0.026581, recall 0.742147
2017-12-10T13:44:54.276951: step 835, loss 3.70715, acc 0.90625, prec 0.026666, recall 0.741851
2017-12-10T13:44:54.465824: step 836, loss 4.46149, acc 0.828125, prec 0.0266536, recall 0.740885
2017-12-10T13:44:54.657900: step 837, loss 0.697613, acc 0.796875, prec 0.0266373, recall 0.740885
2017-12-10T13:44:54.847870: step 838, loss 0.447767, acc 0.890625, prec 0.0266742, recall 0.741222
2017-12-10T13:44:55.038893: step 839, loss 0.361859, acc 0.84375, prec 0.0266617, recall 0.741222
2017-12-10T13:44:55.227812: step 840, loss 0.547474, acc 0.890625, prec 0.026653, recall 0.741222
2017-12-10T13:44:55.416477: step 841, loss 1.14311, acc 0.765625, prec 0.0267252, recall 0.741894
2017-12-10T13:44:55.604459: step 842, loss 0.97262, acc 0.71875, prec 0.0267028, recall 0.741894
2017-12-10T13:44:55.791100: step 843, loss 0.59461, acc 0.828125, prec 0.0267345, recall 0.742228
2017-12-10T13:44:55.982355: step 844, loss 1.10473, acc 0.65625, prec 0.0267071, recall 0.742228
2017-12-10T13:44:56.170582: step 845, loss 1.1863, acc 0.71875, prec 0.02673, recall 0.742561
2017-12-10T13:44:56.357699: step 846, loss 0.523903, acc 0.875, prec 0.02672, recall 0.742561
2017-12-10T13:44:56.544031: step 847, loss 0.766085, acc 0.828125, prec 0.0267064, recall 0.742561
2017-12-10T13:44:56.735097: step 848, loss 0.701648, acc 0.796875, prec 0.0266902, recall 0.742561
2017-12-10T13:44:56.924038: step 849, loss 1.11953, acc 0.859375, prec 0.0267243, recall 0.742894
2017-12-10T13:44:57.115181: step 850, loss 0.573921, acc 0.84375, prec 0.0267571, recall 0.743226
2017-12-10T13:44:57.300502: step 851, loss 1.63518, acc 0.765625, prec 0.0267836, recall 0.743557
2017-12-10T13:44:57.490149: step 852, loss 3.7879, acc 0.8125, prec 0.0268151, recall 0.742931
2017-12-10T13:44:57.682464: step 853, loss 0.779446, acc 0.796875, prec 0.0268892, recall 0.74359
2017-12-10T13:44:57.870844: step 854, loss 0.608348, acc 0.828125, prec 0.0269206, recall 0.743918
2017-12-10T13:44:58.059676: step 855, loss 7.30929, acc 0.796875, prec 0.0269056, recall 0.742967
2017-12-10T13:44:58.246746: step 856, loss 8.19192, acc 0.734375, prec 0.0268869, recall 0.741071
2017-12-10T13:44:58.437070: step 857, loss 1.0723, acc 0.703125, prec 0.0268633, recall 0.741071
2017-12-10T13:44:58.625295: step 858, loss 1.31916, acc 0.65625, prec 0.026836, recall 0.741071
2017-12-10T13:44:58.817668: step 859, loss 1.56522, acc 0.71875, prec 0.0268137, recall 0.741071
2017-12-10T13:44:59.009480: step 860, loss 1.59443, acc 0.640625, prec 0.0267853, recall 0.741071
2017-12-10T13:44:59.195057: step 861, loss 1.76935, acc 0.5625, prec 0.0267508, recall 0.741071
2017-12-10T13:44:59.383170: step 862, loss 1.70851, acc 0.59375, prec 0.0267635, recall 0.741401
2017-12-10T13:44:59.574229: step 863, loss 1.254, acc 0.578125, prec 0.0267304, recall 0.741401
2017-12-10T13:44:59.759306: step 864, loss 2.35688, acc 0.453125, prec 0.0266875, recall 0.741401
2017-12-10T13:44:59.945871: step 865, loss 0.951235, acc 0.6875, prec 0.026663, recall 0.741401
2017-12-10T13:45:00.129657: step 866, loss 1.70243, acc 0.578125, prec 0.0266301, recall 0.741401
2017-12-10T13:45:00.315768: step 867, loss 1.7348, acc 0.609375, prec 0.0266441, recall 0.74173
2017-12-10T13:45:00.499242: step 868, loss 1.28728, acc 0.6875, prec 0.0266198, recall 0.74173
2017-12-10T13:45:00.685522: step 869, loss 1.07926, acc 0.71875, prec 0.0266423, recall 0.742058
2017-12-10T13:45:00.872623: step 870, loss 1.05339, acc 0.671875, prec 0.0266168, recall 0.742058
2017-12-10T13:45:01.065857: step 871, loss 1.06407, acc 0.6875, prec 0.0265926, recall 0.742058
2017-12-10T13:45:01.259738: step 872, loss 4.99594, acc 0.765625, prec 0.0266642, recall 0.741772
2017-12-10T13:45:01.452460: step 873, loss 0.760924, acc 0.734375, prec 0.0266879, recall 0.742099
2017-12-10T13:45:01.638609: step 874, loss 13.8061, acc 0.703125, prec 0.0266661, recall 0.741162
2017-12-10T13:45:01.829481: step 875, loss 0.924713, acc 0.765625, prec 0.0267363, recall 0.741814
2017-12-10T13:45:02.017249: step 876, loss 1.09496, acc 0.671875, prec 0.0267108, recall 0.741814
2017-12-10T13:45:02.206917: step 877, loss 1.70448, acc 0.625, prec 0.0266818, recall 0.741814
2017-12-10T13:45:02.391517: step 878, loss 1.25595, acc 0.640625, prec 0.026654, recall 0.741814
2017-12-10T13:45:02.572731: step 879, loss 1.12218, acc 0.703125, prec 0.0266311, recall 0.741814
2017-12-10T13:45:02.761990: step 880, loss 1.05288, acc 0.75, prec 0.0266998, recall 0.742462
2017-12-10T13:45:02.953561: step 881, loss 0.772079, acc 0.765625, prec 0.0267257, recall 0.742785
2017-12-10T13:45:03.140760: step 882, loss 0.617413, acc 0.796875, prec 0.0267539, recall 0.743108
2017-12-10T13:45:03.328069: step 883, loss 1.29473, acc 0.734375, prec 0.0267334, recall 0.743108
2017-12-10T13:45:03.517059: step 884, loss 0.463028, acc 0.8125, prec 0.0267189, recall 0.743108
2017-12-10T13:45:03.703033: step 885, loss 0.745543, acc 0.765625, prec 0.0267447, recall 0.743429
2017-12-10T13:45:03.889647: step 886, loss 2.48197, acc 0.78125, prec 0.0267729, recall 0.742821
2017-12-10T13:45:04.083838: step 887, loss 0.649855, acc 0.796875, prec 0.0267572, recall 0.742821
2017-12-10T13:45:04.266729: step 888, loss 0.660421, acc 0.84375, prec 0.0267889, recall 0.743142
2017-12-10T13:45:04.456902: step 889, loss 0.985204, acc 0.8125, prec 0.0268182, recall 0.743462
2017-12-10T13:45:04.646214: step 890, loss 0.612606, acc 0.8125, prec 0.0268474, recall 0.743781
2017-12-10T13:45:04.838022: step 891, loss 2.11098, acc 0.828125, prec 0.0269227, recall 0.743494
2017-12-10T13:45:05.030018: step 892, loss 0.348275, acc 0.90625, prec 0.0269155, recall 0.743494
2017-12-10T13:45:05.218672: step 893, loss 0.660639, acc 0.765625, prec 0.0268974, recall 0.743494
2017-12-10T13:45:05.404221: step 894, loss 0.325975, acc 0.828125, prec 0.0268841, recall 0.743494
2017-12-10T13:45:05.591125: step 895, loss 0.740204, acc 0.796875, prec 0.0269121, recall 0.743812
2017-12-10T13:45:05.782495: step 896, loss 0.487487, acc 0.796875, prec 0.0269399, recall 0.744129
2017-12-10T13:45:05.968261: step 897, loss 0.554093, acc 0.828125, prec 0.0269702, recall 0.744444
2017-12-10T13:45:06.155097: step 898, loss 2.24334, acc 0.765625, prec 0.0269968, recall 0.743842
2017-12-10T13:45:06.346150: step 899, loss 0.524756, acc 0.78125, prec 0.0269799, recall 0.743842
2017-12-10T13:45:06.539792: step 900, loss 0.391192, acc 0.90625, prec 0.0269727, recall 0.743842
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_0/1512931325/checkpoints/model-900

2017-12-10T13:45:07.759488: step 901, loss 0.730255, acc 0.796875, prec 0.0270005, recall 0.744157
2017-12-10T13:45:07.950863: step 902, loss 0.230251, acc 0.890625, prec 0.0269921, recall 0.744157
2017-12-10T13:45:08.138554: step 903, loss 1.54592, acc 0.921875, prec 0.0271162, recall 0.745098
2017-12-10T13:45:08.331590: step 904, loss 32.8996, acc 0.875, prec 0.0271957, recall 0.743902
2017-12-10T13:45:08.523633: step 905, loss 1.17535, acc 0.71875, prec 0.0272172, recall 0.744214
2017-12-10T13:45:08.710155: step 906, loss 0.708112, acc 0.78125, prec 0.0272436, recall 0.744526
2017-12-10T13:45:08.896382: step 907, loss 1.933, acc 0.75, prec 0.0272675, recall 0.744836
2017-12-10T13:45:09.089891: step 908, loss 0.577252, acc 0.78125, prec 0.0272505, recall 0.744836
2017-12-10T13:45:09.275971: step 909, loss 5.53193, acc 0.671875, prec 0.0272695, recall 0.744242
2017-12-10T13:45:09.470688: step 910, loss 1.54323, acc 0.71875, prec 0.027334, recall 0.744861
2017-12-10T13:45:09.655157: step 911, loss 1.71623, acc 0.609375, prec 0.0273469, recall 0.745169
2017-12-10T13:45:09.837196: step 912, loss 1.90824, acc 0.65625, prec 0.0273202, recall 0.745169
2017-12-10T13:45:10.023304: step 913, loss 1.78297, acc 0.578125, prec 0.0273736, recall 0.745783
2017-12-10T13:45:10.207680: step 914, loss 1.90112, acc 0.546875, prec 0.0273815, recall 0.746089
2017-12-10T13:45:10.395485: step 915, loss 2.13679, acc 0.609375, prec 0.0273513, recall 0.746089
2017-12-10T13:45:10.581884: step 916, loss 1.88269, acc 0.546875, prec 0.0273592, recall 0.746394
2017-12-10T13:45:10.766864: step 917, loss 2.7889, acc 0.4375, prec 0.0273159, recall 0.746394
2017-12-10T13:45:10.951302: step 918, loss 2.11197, acc 0.515625, prec 0.0273642, recall 0.747002
2017-12-10T13:45:11.137412: step 919, loss 5.72452, acc 0.546875, prec 0.0273306, recall 0.746108
2017-12-10T13:45:11.324888: step 920, loss 2.56277, acc 0.515625, prec 0.0272934, recall 0.746108
2017-12-10T13:45:11.511638: step 921, loss 2.1541, acc 0.515625, prec 0.0272564, recall 0.746108
2017-12-10T13:45:11.699629: step 922, loss 5.98838, acc 0.578125, prec 0.0272255, recall 0.745215
2017-12-10T13:45:11.888167: step 923, loss 2.1368, acc 0.53125, prec 0.0272323, recall 0.74552
2017-12-10T13:45:12.072575: step 924, loss 2.15659, acc 0.484375, prec 0.0272355, recall 0.745823
2017-12-10T13:45:12.259130: step 925, loss 1.76751, acc 0.5625, prec 0.0272023, recall 0.745823
2017-12-10T13:45:12.448144: step 926, loss 1.7671, acc 0.5, prec 0.0271645, recall 0.745823
2017-12-10T13:45:12.632120: step 927, loss 1.75168, acc 0.609375, prec 0.027135, recall 0.745823
2017-12-10T13:45:12.816473: step 928, loss 1.58836, acc 0.59375, prec 0.0271044, recall 0.745823
2017-12-10T13:45:13.001818: step 929, loss 11.2319, acc 0.65625, prec 0.0271219, recall 0.745238
2017-12-10T13:45:13.191401: step 930, loss 1.06988, acc 0.65625, prec 0.0271803, recall 0.745843
2017-12-10T13:45:13.382681: step 931, loss 1.77311, acc 0.640625, prec 0.0271953, recall 0.746145
2017-12-10T13:45:13.571231: step 932, loss 1.02183, acc 0.75, prec 0.0271765, recall 0.746145
2017-12-10T13:45:13.763485: step 933, loss 1.17742, acc 0.6875, prec 0.027195, recall 0.746445
2017-12-10T13:45:13.957122: step 934, loss 0.737618, acc 0.75, prec 0.0272182, recall 0.746746
2017-12-10T13:45:14.144539: step 935, loss 1.50916, acc 0.609375, prec 0.0272308, recall 0.747045
2017-12-10T13:45:14.332264: step 936, loss 2.47039, acc 0.6875, prec 0.0272085, recall 0.746163
2017-12-10T13:45:14.522142: step 937, loss 0.641528, acc 0.859375, prec 0.027198, recall 0.746163
2017-12-10T13:45:14.707904: step 938, loss 1.10037, acc 0.75, prec 0.027263, recall 0.746761
2017-12-10T13:45:14.899529: step 939, loss 1.03277, acc 0.71875, prec 0.0272419, recall 0.746761
2017-12-10T13:45:15.086832: step 940, loss 1.64062, acc 0.828125, prec 0.0273125, recall 0.747356
2017-12-10T13:45:15.277195: step 941, loss 1.60934, acc 0.75, prec 0.0273773, recall 0.747948
2017-12-10T13:45:15.469584: step 942, loss 6.63589, acc 0.703125, prec 0.0274395, recall 0.747664
2017-12-10T13:45:15.662997: step 943, loss 1.02921, acc 0.703125, prec 0.0275005, recall 0.748252
2017-12-10T13:45:15.856167: step 944, loss 0.768645, acc 0.78125, prec 0.0275257, recall 0.748545
2017-12-10T13:45:16.042276: step 945, loss 1.13011, acc 0.84375, prec 0.0275971, recall 0.749129
2017-12-10T13:45:16.233803: step 946, loss 0.842162, acc 0.734375, prec 0.0276186, recall 0.74942
2017-12-10T13:45:16.418326: step 947, loss 1.00769, acc 0.6875, prec 0.0276781, recall 0.75
2017-12-10T13:45:16.604843: step 948, loss 0.907968, acc 0.734375, prec 0.027741, recall 0.750577
2017-12-10T13:45:16.791427: step 949, loss 1.13181, acc 0.6875, prec 0.0277588, recall 0.750865
2017-12-10T13:45:16.980391: step 950, loss 0.854657, acc 0.765625, prec 0.0277825, recall 0.751152
2017-12-10T13:45:17.166751: step 951, loss 1.08602, acc 0.6875, prec 0.0278416, recall 0.751724
2017-12-10T13:45:17.360341: step 952, loss 1.25986, acc 0.59375, prec 0.0278109, recall 0.751724
2017-12-10T13:45:17.546024: step 953, loss 1.04527, acc 0.609375, prec 0.0278226, recall 0.752009
2017-12-10T13:45:17.732492: step 954, loss 0.774968, acc 0.75, prec 0.027845, recall 0.752294
2017-12-10T13:45:17.920355: step 955, loss 0.897282, acc 0.71875, prec 0.027865, recall 0.752577
2017-12-10T13:45:18.110748: step 956, loss 0.782391, acc 0.703125, prec 0.0278425, recall 0.752577
2017-12-10T13:45:18.297359: step 957, loss 1.32175, acc 0.734375, prec 0.0278636, recall 0.75286
2017-12-10T13:45:18.491317: step 958, loss 0.710396, acc 0.828125, prec 0.0278507, recall 0.75286
2017-12-10T13:45:18.681597: step 959, loss 0.493608, acc 0.796875, prec 0.0278354, recall 0.75286
2017-12-10T13:45:18.868532: step 960, loss 0.230992, acc 0.90625, prec 0.0278694, recall 0.753143
2017-12-10T13:45:19.052751: step 961, loss 2.93952, acc 0.796875, prec 0.0278553, recall 0.752283
2017-12-10T13:45:19.239043: step 962, loss 2.73542, acc 0.78125, prec 0.027881, recall 0.751708
2017-12-10T13:45:19.432682: step 963, loss 0.525114, acc 0.796875, prec 0.0279068, recall 0.751991
2017-12-10T13:45:19.621256: step 964, loss 0.704859, acc 0.796875, prec 0.0278915, recall 0.751991
2017-12-10T13:45:19.807141: step 965, loss 4.44205, acc 0.828125, prec 0.0279207, recall 0.751419
2017-12-10T13:45:19.997052: step 966, loss 10.3225, acc 0.78125, prec 0.0279874, recall 0.751131
2017-12-10T13:45:20.189279: step 967, loss 0.663789, acc 0.78125, prec 0.0280118, recall 0.751412
2017-12-10T13:45:20.375755: step 968, loss 1.31996, acc 0.609375, prec 0.0280232, recall 0.751693
2017-12-10T13:45:20.563534: step 969, loss 0.94417, acc 0.734375, prec 0.0280441, recall 0.751973
2017-12-10T13:45:20.751552: step 970, loss 3.30356, acc 0.640625, prec 0.0280181, recall 0.751126
2017-12-10T13:45:20.939363: step 971, loss 1.53516, acc 0.625, prec 0.0280307, recall 0.751406
2017-12-10T13:45:21.123238: step 972, loss 4.63806, acc 0.59375, prec 0.0281236, recall 0.7514
2017-12-10T13:45:21.312005: step 973, loss 1.1525, acc 0.65625, prec 0.0281383, recall 0.751678
2017-12-10T13:45:21.502327: step 974, loss 1.6193, acc 0.546875, prec 0.0282261, recall 0.752508
2017-12-10T13:45:21.687201: step 975, loss 1.31604, acc 0.65625, prec 0.0282002, recall 0.752508
2017-12-10T13:45:21.871118: step 976, loss 1.56849, acc 0.5, prec 0.0282436, recall 0.753059
2017-12-10T13:45:22.055706: step 977, loss 2.02099, acc 0.515625, prec 0.0282476, recall 0.753333
2017-12-10T13:45:22.243243: step 978, loss 1.44064, acc 0.671875, prec 0.028223, recall 0.753333
2017-12-10T13:45:22.428357: step 979, loss 1.41182, acc 0.65625, prec 0.0281971, recall 0.753333
2017-12-10T13:45:22.617248: step 980, loss 1.35255, acc 0.671875, prec 0.0282129, recall 0.753607
2017-12-10T13:45:22.802527: step 981, loss 1.19075, acc 0.625, prec 0.0281848, recall 0.753607
2017-12-10T13:45:22.989217: step 982, loss 1.17032, acc 0.703125, prec 0.0281626, recall 0.753607
2017-12-10T13:45:23.173604: step 983, loss 1.03444, acc 0.65625, prec 0.0281369, recall 0.753607
2017-12-10T13:45:23.357647: step 984, loss 1.36846, acc 0.609375, prec 0.0281078, recall 0.753607
2017-12-10T13:45:23.541687: step 985, loss 0.975741, acc 0.703125, prec 0.0280857, recall 0.753607
2017-12-10T13:45:23.729929: step 986, loss 0.881347, acc 0.71875, prec 0.0280648, recall 0.753607
2017-12-10T13:45:23.918450: step 987, loss 0.443994, acc 0.8125, prec 0.0280509, recall 0.753607
2017-12-10T13:45:24.108451: step 988, loss 0.800139, acc 0.8125, prec 0.0281173, recall 0.754153
2017-12-10T13:45:24.296929: step 989, loss 0.698671, acc 0.828125, prec 0.0281847, recall 0.754696
2017-12-10T13:45:24.485060: step 990, loss 0.660574, acc 0.796875, prec 0.0281696, recall 0.754696
2017-12-10T13:45:24.676920: step 991, loss 0.347279, acc 0.890625, prec 0.0281615, recall 0.754696
2017-12-10T13:45:24.866787: step 992, loss 0.347601, acc 0.875, prec 0.0281522, recall 0.754696
2017-12-10T13:45:25.059559: step 993, loss 2.58814, acc 0.828125, prec 0.0282207, recall 0.754405
2017-12-10T13:45:25.231752: step 994, loss 0.203441, acc 0.901961, prec 0.0282148, recall 0.754405
2017-12-10T13:45:25.428989: step 995, loss 2.51097, acc 0.90625, prec 0.0282491, recall 0.753846
2017-12-10T13:45:25.621401: step 996, loss 0.678658, acc 0.84375, prec 0.0282374, recall 0.753846
2017-12-10T13:45:25.809067: step 997, loss 0.477758, acc 0.828125, prec 0.0282246, recall 0.753846
2017-12-10T13:45:25.996177: step 998, loss 10.4845, acc 0.796875, prec 0.0282518, recall 0.752464
2017-12-10T13:45:26.188596: step 999, loss 0.165485, acc 0.921875, prec 0.028286, recall 0.752735
2017-12-10T13:45:26.375247: step 1000, loss 0.624311, acc 0.828125, prec 0.0282732, recall 0.752735
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_0/1512931325/checkpoints/model-1000

2017-12-10T13:45:27.560380: step 1001, loss 0.601454, acc 0.8125, prec 0.0282992, recall 0.753005
2017-12-10T13:45:27.749910: step 1002, loss 0.588932, acc 0.796875, prec 0.028324, recall 0.753275
2017-12-10T13:45:27.936706: step 1003, loss 0.743463, acc 0.765625, prec 0.0283065, recall 0.753275
2017-12-10T13:45:28.125453: step 1004, loss 2.52408, acc 0.78125, prec 0.0283313, recall 0.752723
2017-12-10T13:45:28.313731: step 1005, loss 0.697052, acc 0.734375, prec 0.0283912, recall 0.753261
2017-12-10T13:45:28.499837: step 1006, loss 0.639871, acc 0.796875, prec 0.0284158, recall 0.753529
2017-12-10T13:45:28.691076: step 1007, loss 1.11786, acc 0.734375, prec 0.0284358, recall 0.753796
2017-12-10T13:45:28.884182: step 1008, loss 1.25775, acc 0.6875, prec 0.0284126, recall 0.753796
2017-12-10T13:45:29.072859: step 1009, loss 1.54697, acc 0.59375, prec 0.0283824, recall 0.753796
2017-12-10T13:45:29.260729: step 1010, loss 2.27724, acc 0.703125, prec 0.0284, recall 0.754063
2017-12-10T13:45:29.449595: step 1011, loss 1.70755, acc 0.671875, prec 0.0284153, recall 0.754329
2017-12-10T13:45:29.637125: step 1012, loss 1.49422, acc 0.578125, prec 0.0284237, recall 0.754595
2017-12-10T13:45:29.824293: step 1013, loss 1.80602, acc 0.640625, prec 0.0284366, recall 0.75486
2017-12-10T13:45:30.016634: step 1014, loss 0.751627, acc 0.765625, prec 0.0284193, recall 0.75486
2017-12-10T13:45:30.201363: step 1015, loss 0.517102, acc 0.796875, prec 0.0284042, recall 0.75486
2017-12-10T13:45:30.391743: step 1016, loss 0.999989, acc 0.6875, prec 0.0284206, recall 0.755124
2017-12-10T13:45:30.580925: step 1017, loss 1.17149, acc 0.71875, prec 0.0285181, recall 0.755914
2017-12-10T13:45:30.770093: step 1018, loss 0.848517, acc 0.71875, prec 0.0285761, recall 0.756438
2017-12-10T13:45:30.959042: step 1019, loss 0.60321, acc 0.796875, prec 0.028561, recall 0.756438
2017-12-10T13:45:31.157331: step 1020, loss 0.584122, acc 0.78125, prec 0.0285842, recall 0.756699
2017-12-10T13:45:31.354488: step 1021, loss 1.45752, acc 0.78125, prec 0.0286073, recall 0.756959
2017-12-10T13:45:31.547285: step 1022, loss 4.86157, acc 0.890625, prec 0.0286396, recall 0.75641
2017-12-10T13:45:31.737553: step 1023, loss 0.669613, acc 0.8125, prec 0.0286257, recall 0.75641
2017-12-10T13:45:31.928073: step 1024, loss 6.20914, acc 0.859375, prec 0.0286165, recall 0.755603
2017-12-10T13:45:32.118844: step 1025, loss 0.426146, acc 0.875, prec 0.0286072, recall 0.755603
2017-12-10T13:45:32.305193: step 1026, loss 0.830813, acc 0.71875, prec 0.0286256, recall 0.755864
2017-12-10T13:45:32.494309: step 1027, loss 0.729528, acc 0.765625, prec 0.0286083, recall 0.755864
2017-12-10T13:45:32.681657: step 1028, loss 0.45266, acc 0.8125, prec 0.0286337, recall 0.756124
2017-12-10T13:45:32.868322: step 1029, loss 0.718223, acc 0.796875, prec 0.0286969, recall 0.756642
2017-12-10T13:45:33.058237: step 1030, loss 0.695879, acc 0.8125, prec 0.0287613, recall 0.757158
2017-12-10T13:45:33.246655: step 1031, loss 0.644356, acc 0.796875, prec 0.0288245, recall 0.757672
2017-12-10T13:45:33.436272: step 1032, loss 0.508104, acc 0.796875, prec 0.0288094, recall 0.757672
2017-12-10T13:45:33.627808: step 1033, loss 1.29611, acc 0.6875, prec 0.0289034, recall 0.758439
2017-12-10T13:45:33.821431: step 1034, loss 1.03666, acc 0.6875, prec 0.0288801, recall 0.758439
2017-12-10T13:45:34.010526: step 1035, loss 0.931147, acc 0.765625, prec 0.0289017, recall 0.758693
2017-12-10T13:45:34.195666: step 1036, loss 0.895721, acc 0.75, prec 0.0289221, recall 0.758947
2017-12-10T13:45:34.384566: step 1037, loss 1.20928, acc 0.734375, prec 0.0289414, recall 0.759201
2017-12-10T13:45:34.575092: step 1038, loss 0.429474, acc 0.875, prec 0.028971, recall 0.759454
2017-12-10T13:45:34.761902: step 1039, loss 0.587761, acc 0.84375, prec 0.0289594, recall 0.759454
2017-12-10T13:45:34.950999: step 1040, loss 0.580724, acc 0.78125, prec 0.0289432, recall 0.759454
2017-12-10T13:45:35.137657: step 1041, loss 0.593587, acc 0.828125, prec 0.0289304, recall 0.759454
2017-12-10T13:45:35.323972: step 1042, loss 0.380903, acc 0.84375, prec 0.0289188, recall 0.759454
2017-12-10T13:45:35.512111: step 1043, loss 0.195821, acc 0.90625, prec 0.0289507, recall 0.759706
2017-12-10T13:45:35.699476: step 1044, loss 0.2425, acc 0.90625, prec 0.0289438, recall 0.759706
2017-12-10T13:45:35.889003: step 1045, loss 8.27433, acc 0.90625, prec 0.028938, recall 0.75891
2017-12-10T13:45:36.076871: step 1046, loss 0.406474, acc 0.875, prec 0.0289288, recall 0.75891
2017-12-10T13:45:36.268455: step 1047, loss 0.583565, acc 0.875, prec 0.0289583, recall 0.759162
2017-12-10T13:45:36.463820: step 1048, loss 2.52881, acc 0.859375, prec 0.0289878, recall 0.758621
2017-12-10T13:45:36.659659: step 1049, loss 0.716782, acc 0.828125, prec 0.0290913, recall 0.759375
2017-12-10T13:45:36.852962: step 1050, loss 10.7122, acc 0.859375, prec 0.0291208, recall 0.758836
2017-12-10T13:45:37.046360: step 1051, loss 0.544807, acc 0.84375, prec 0.0291092, recall 0.758836
2017-12-10T13:45:37.236887: step 1052, loss 1.06782, acc 0.8125, prec 0.029134, recall 0.759086
2017-12-10T13:45:37.424423: step 1053, loss 0.891951, acc 0.796875, prec 0.0291189, recall 0.759086
2017-12-10T13:45:37.613056: step 1054, loss 0.978318, acc 0.78125, prec 0.0291413, recall 0.759336
2017-12-10T13:45:37.797914: step 1055, loss 0.935892, acc 0.8125, prec 0.0292433, recall 0.760083
2017-12-10T13:45:37.985570: step 1056, loss 1.24144, acc 0.65625, prec 0.0292563, recall 0.760331
2017-12-10T13:45:38.171885: step 1057, loss 1.15471, acc 0.6875, prec 0.0292716, recall 0.760578
2017-12-10T13:45:38.359035: step 1058, loss 1.31803, acc 0.671875, prec 0.0292472, recall 0.760578
2017-12-10T13:45:38.551318: step 1059, loss 1.2658, acc 0.671875, prec 0.0292228, recall 0.760578
2017-12-10T13:45:38.739792: step 1060, loss 1.04321, acc 0.71875, prec 0.029202, recall 0.760578
2017-12-10T13:45:38.929423: step 1061, loss 1.38687, acc 0.6875, prec 0.0291789, recall 0.760578
2017-12-10T13:45:39.118768: step 1062, loss 0.838338, acc 0.796875, prec 0.0292023, recall 0.760825
2017-12-10T13:45:39.308028: step 1063, loss 1.17062, acc 0.703125, prec 0.0292187, recall 0.761071
2017-12-10T13:45:39.498763: step 1064, loss 0.716175, acc 0.765625, prec 0.0292014, recall 0.761071
2017-12-10T13:45:39.688702: step 1065, loss 1.11436, acc 0.765625, prec 0.0292224, recall 0.761317
2017-12-10T13:45:39.876290: step 1066, loss 0.557938, acc 0.78125, prec 0.0292829, recall 0.761807
2017-12-10T13:45:40.061733: step 1067, loss 0.820294, acc 0.703125, prec 0.0292993, recall 0.762051
2017-12-10T13:45:40.249344: step 1068, loss 2.98507, acc 0.734375, prec 0.0292808, recall 0.76127
2017-12-10T13:45:40.443341: step 1069, loss 0.930294, acc 0.765625, prec 0.0293017, recall 0.761515
2017-12-10T13:45:40.633063: step 1070, loss 0.591068, acc 0.8125, prec 0.0293261, recall 0.761759
2017-12-10T13:45:40.822812: step 1071, loss 0.686897, acc 0.828125, prec 0.0293134, recall 0.761759
2017-12-10T13:45:41.012278: step 1072, loss 0.480197, acc 0.8125, prec 0.0293377, recall 0.762002
2017-12-10T13:45:41.204298: step 1073, loss 5.00392, acc 0.84375, prec 0.0293655, recall 0.761468
2017-12-10T13:45:41.391260: step 1074, loss 1.22065, acc 0.78125, prec 0.0294256, recall 0.761953
2017-12-10T13:45:41.578986: step 1075, loss 0.761429, acc 0.796875, prec 0.0294106, recall 0.761953
2017-12-10T13:45:41.765501: step 1076, loss 10.1566, acc 0.84375, prec 0.0294002, recall 0.761179
2017-12-10T13:45:41.954856: step 1077, loss 0.652775, acc 0.796875, prec 0.0293852, recall 0.761179
2017-12-10T13:45:42.139669: step 1078, loss 0.921119, acc 0.703125, prec 0.0294014, recall 0.761421
2017-12-10T13:45:42.326924: step 1079, loss 0.838747, acc 0.75, prec 0.029421, recall 0.761663
2017-12-10T13:45:42.513169: step 1080, loss 1.28462, acc 0.703125, prec 0.0294371, recall 0.761905
2017-12-10T13:45:42.700766: step 1081, loss 1.05374, acc 0.6875, prec 0.0294141, recall 0.761905
2017-12-10T13:45:42.887227: step 1082, loss 0.791146, acc 0.84375, prec 0.0294405, recall 0.762146
2017-12-10T13:45:43.077475: step 1083, loss 4.5459, acc 0.65625, prec 0.0294164, recall 0.761375
2017-12-10T13:45:43.266746: step 1084, loss 0.491443, acc 0.796875, prec 0.0294014, recall 0.761375
2017-12-10T13:45:43.455505: step 1085, loss 1.25096, acc 0.78125, prec 0.0294232, recall 0.761616
2017-12-10T13:45:43.641284: step 1086, loss 0.889638, acc 0.75, prec 0.0294049, recall 0.761616
2017-12-10T13:45:43.831410: step 1087, loss 0.794186, acc 0.765625, prec 0.0293877, recall 0.761616
2017-12-10T13:45:44.018877: step 1088, loss 1.38858, acc 0.65625, prec 0.0294003, recall 0.761857
2017-12-10T13:45:44.202012: step 1089, loss 1.03501, acc 0.78125, prec 0.0294221, recall 0.762097
2017-12-10T13:45:44.390923: step 1090, loss 1.42797, acc 0.625, prec 0.0294323, recall 0.762336
2017-12-10T13:45:44.583388: step 1091, loss 0.825189, acc 0.71875, prec 0.0294872, recall 0.762814
2017-12-10T13:45:44.770884: step 1092, loss 0.840486, acc 0.796875, prec 0.0294723, recall 0.762814
2017-12-10T13:45:44.956431: step 1093, loss 0.824813, acc 0.78125, prec 0.0294939, recall 0.763052
2017-12-10T13:45:45.145794: step 1094, loss 2.91766, acc 0.796875, prec 0.0294802, recall 0.762287
2017-12-10T13:45:45.337687: step 1095, loss 0.430293, acc 0.859375, prec 0.0294699, recall 0.762287
2017-12-10T13:45:45.524445: step 1096, loss 0.623901, acc 0.796875, prec 0.0294551, recall 0.762287
2017-12-10T13:45:45.716591: step 1097, loss 1.92876, acc 0.78125, prec 0.0294778, recall 0.761762
2017-12-10T13:45:45.905434: step 1098, loss 0.547205, acc 0.828125, prec 0.0294653, recall 0.761762
2017-12-10T13:45:46.094756: step 1099, loss 0.488079, acc 0.859375, prec 0.029455, recall 0.761762
2017-12-10T13:45:46.280173: step 1100, loss 0.793704, acc 0.828125, prec 0.02948, recall 0.762
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_0/1512931325/checkpoints/model-1100

2017-12-10T13:45:47.412809: step 1101, loss 0.386583, acc 0.890625, prec 0.0295096, recall 0.762238
2017-12-10T13:45:47.600434: step 1102, loss 0.848829, acc 0.765625, prec 0.0294925, recall 0.762238
2017-12-10T13:45:47.788663: step 1103, loss 0.709869, acc 0.78125, prec 0.0294765, recall 0.762238
2017-12-10T13:45:47.977206: step 1104, loss 4.39092, acc 0.890625, prec 0.0295072, recall 0.761715
2017-12-10T13:45:48.166829: step 1105, loss 0.752165, acc 0.796875, prec 0.0295298, recall 0.761952
2017-12-10T13:45:48.352579: step 1106, loss 0.872021, acc 0.75, prec 0.0295865, recall 0.762425
2017-12-10T13:45:48.541608: step 1107, loss 0.756322, acc 0.75, prec 0.0296056, recall 0.762661
2017-12-10T13:45:48.733877: step 1108, loss 0.977114, acc 0.71875, prec 0.0295851, recall 0.762661
2017-12-10T13:45:48.925582: step 1109, loss 0.259377, acc 0.875, prec 0.0296134, recall 0.762897
2017-12-10T13:45:49.110885: step 1110, loss 1.1277, acc 0.828125, prec 0.0296755, recall 0.763366
2017-12-10T13:45:49.302631: step 1111, loss 0.688618, acc 0.84375, prec 0.0296641, recall 0.763366
2017-12-10T13:45:49.494334: step 1112, loss 8.05766, acc 0.765625, prec 0.0296493, recall 0.761858
2017-12-10T13:45:49.686196: step 1113, loss 0.495342, acc 0.78125, prec 0.0296706, recall 0.762093
2017-12-10T13:45:49.871220: step 1114, loss 0.767521, acc 0.75, prec 0.0296897, recall 0.762327
2017-12-10T13:45:50.062496: step 1115, loss 0.630946, acc 0.765625, prec 0.0296726, recall 0.762327
2017-12-10T13:45:50.251558: step 1116, loss 0.879752, acc 0.78125, prec 0.0296939, recall 0.762562
2017-12-10T13:45:50.437155: step 1117, loss 0.733026, acc 0.75, prec 0.02975, recall 0.763029
2017-12-10T13:45:50.624388: step 1118, loss 1.11372, acc 0.734375, prec 0.0297307, recall 0.763029
2017-12-10T13:45:50.815497: step 1119, loss 0.750487, acc 0.796875, prec 0.0297902, recall 0.763494
2017-12-10T13:45:51.007621: step 1120, loss 1.94025, acc 0.71875, prec 0.0298068, recall 0.763726
2017-12-10T13:45:51.199041: step 1121, loss 0.715706, acc 0.71875, prec 0.0297863, recall 0.763726
2017-12-10T13:45:51.385877: step 1122, loss 0.620676, acc 0.8125, prec 0.0297726, recall 0.763726
2017-12-10T13:45:51.571230: step 1123, loss 0.597837, acc 0.8125, prec 0.0297589, recall 0.763726
2017-12-10T13:45:51.760578: step 1124, loss 0.538426, acc 0.859375, prec 0.0297858, recall 0.763957
2017-12-10T13:45:51.952274: step 1125, loss 1.2609, acc 0.703125, prec 0.0298382, recall 0.764418
2017-12-10T13:45:52.142928: step 1126, loss 0.988881, acc 0.796875, prec 0.0298974, recall 0.764878
2017-12-10T13:45:52.336587: step 1127, loss 0.364905, acc 0.859375, prec 0.0299241, recall 0.765107
2017-12-10T13:45:52.523519: step 1128, loss 1.27837, acc 0.765625, prec 0.0300549, recall 0.766019
2017-12-10T13:45:52.714299: step 1129, loss 1.00577, acc 0.703125, prec 0.0300331, recall 0.766019
2017-12-10T13:45:52.904496: step 1130, loss 0.633663, acc 0.78125, prec 0.0300171, recall 0.766019
2017-12-10T13:45:53.091167: step 1131, loss 0.52148, acc 0.828125, prec 0.0300414, recall 0.766246
2017-12-10T13:45:53.279518: step 1132, loss 0.671075, acc 0.765625, prec 0.0300612, recall 0.766473
2017-12-10T13:45:53.466442: step 1133, loss 0.798737, acc 0.796875, prec 0.03012, recall 0.766925
2017-12-10T13:45:53.659221: step 1134, loss 0.583645, acc 0.78125, prec 0.030104, recall 0.766925
2017-12-10T13:45:53.847353: step 1135, loss 0.491061, acc 0.8125, prec 0.0300903, recall 0.766925
2017-12-10T13:45:54.031043: step 1136, loss 3.67147, acc 0.828125, prec 0.0301157, recall 0.766409
2017-12-10T13:45:54.224093: step 1137, loss 4.46177, acc 0.828125, prec 0.0301043, recall 0.76567
2017-12-10T13:45:54.410777: step 1138, loss 0.524279, acc 0.84375, prec 0.0301296, recall 0.765896
2017-12-10T13:45:54.599511: step 1139, loss 0.60074, acc 0.78125, prec 0.0301136, recall 0.765896
2017-12-10T13:45:54.785009: step 1140, loss 0.267679, acc 0.90625, prec 0.0301068, recall 0.765896
2017-12-10T13:45:54.974046: step 1141, loss 0.601976, acc 0.78125, prec 0.0301643, recall 0.766346
2017-12-10T13:45:55.165548: step 1142, loss 0.714029, acc 0.78125, prec 0.030185, recall 0.766571
2017-12-10T13:45:55.352728: step 1143, loss 6.34324, acc 0.8125, prec 0.0302091, recall 0.766059
2017-12-10T13:45:55.545563: step 1144, loss 0.326985, acc 0.828125, prec 0.0301965, recall 0.766059
2017-12-10T13:45:55.732754: step 1145, loss 0.560296, acc 0.84375, prec 0.0302218, recall 0.766284
2017-12-10T13:45:55.919380: step 1146, loss 1.01608, acc 0.703125, prec 0.0303099, recall 0.766953
2017-12-10T13:45:56.111836: step 1147, loss 0.554243, acc 0.8125, prec 0.0303328, recall 0.767176
2017-12-10T13:45:56.303661: step 1148, loss 7.41832, acc 0.765625, prec 0.0303533, recall 0.766667
2017-12-10T13:45:56.494247: step 1149, loss 3.93515, acc 0.734375, prec 0.030335, recall 0.765937
2017-12-10T13:45:56.681432: step 1150, loss 0.871652, acc 0.703125, prec 0.0303498, recall 0.76616
2017-12-10T13:45:56.875043: step 1151, loss 1.30679, acc 0.75, prec 0.0304045, recall 0.766603
2017-12-10T13:45:57.065364: step 1152, loss 1.24151, acc 0.65625, prec 0.0304158, recall 0.766825
2017-12-10T13:45:57.250618: step 1153, loss 1.1552, acc 0.6875, prec 0.0304294, recall 0.767045
2017-12-10T13:45:57.438120: step 1154, loss 1.0497, acc 0.6875, prec 0.0304429, recall 0.767266
2017-12-10T13:45:57.626687: step 1155, loss 1.24959, acc 0.75, prec 0.0304974, recall 0.767705
2017-12-10T13:45:57.813990: step 1156, loss 1.87572, acc 0.5, prec 0.0305335, recall 0.768143
2017-12-10T13:45:57.995127: step 1157, loss 1.48014, acc 0.703125, prec 0.0306206, recall 0.768797
2017-12-10T13:45:58.180834: step 1158, loss 1.6152, acc 0.546875, prec 0.0305874, recall 0.768797
2017-12-10T13:45:58.365743: step 1159, loss 1.25357, acc 0.65625, prec 0.0306347, recall 0.769231
2017-12-10T13:45:58.550587: step 1160, loss 1.08261, acc 0.671875, prec 0.0306469, recall 0.769447
2017-12-10T13:45:58.741134: step 1161, loss 0.87606, acc 0.65625, prec 0.0306218, recall 0.769447
2017-12-10T13:45:58.930640: step 1162, loss 1.70505, acc 0.59375, prec 0.0306643, recall 0.769878
2017-12-10T13:45:59.116284: step 1163, loss 1.13812, acc 0.6875, prec 0.0306776, recall 0.770093
2017-12-10T13:45:59.303607: step 1164, loss 1.00598, acc 0.8125, prec 0.030736, recall 0.770522
2017-12-10T13:45:59.494688: step 1165, loss 0.676017, acc 0.75, prec 0.0307538, recall 0.770736
2017-12-10T13:45:59.685487: step 1166, loss 1.18904, acc 0.6875, prec 0.0307309, recall 0.770736
2017-12-10T13:45:59.873137: step 1167, loss 0.641122, acc 0.8125, prec 0.0307172, recall 0.770736
2017-12-10T13:46:00.063205: step 1168, loss 0.714268, acc 0.84375, prec 0.0307778, recall 0.771163
2017-12-10T13:46:00.250882: step 1169, loss 13.2522, acc 0.703125, prec 0.0307584, recall 0.769731
2017-12-10T13:46:00.442049: step 1170, loss 0.749295, acc 0.8125, prec 0.0307806, recall 0.769944
2017-12-10T13:46:00.628990: step 1171, loss 0.757297, acc 0.8125, prec 0.0308029, recall 0.770158
2017-12-10T13:46:00.814370: step 1172, loss 1.01047, acc 0.75, prec 0.0308923, recall 0.770795
2017-12-10T13:46:01.004436: step 1173, loss 1.0378, acc 0.734375, prec 0.0308729, recall 0.770795
2017-12-10T13:46:01.198718: step 1174, loss 0.579976, acc 0.828125, prec 0.0308603, recall 0.770795
2017-12-10T13:46:01.393713: step 1175, loss 0.918728, acc 0.71875, prec 0.0308398, recall 0.770795
2017-12-10T13:46:01.580915: step 1176, loss 0.557328, acc 0.859375, prec 0.0308295, recall 0.770795
2017-12-10T13:46:01.767487: step 1177, loss 0.620702, acc 0.828125, prec 0.030817, recall 0.770795
2017-12-10T13:46:01.956489: step 1178, loss 0.696664, acc 0.734375, prec 0.0307976, recall 0.770795
2017-12-10T13:46:02.145485: step 1179, loss 2.15965, acc 0.84375, prec 0.0307874, recall 0.770083
2017-12-10T13:46:02.336358: step 1180, loss 0.740114, acc 0.796875, prec 0.0308084, recall 0.770295
2017-12-10T13:46:02.528695: step 1181, loss 0.674398, acc 0.828125, prec 0.0308674, recall 0.770718
2017-12-10T13:46:02.715682: step 1182, loss 2.28592, acc 0.84375, prec 0.0309275, recall 0.77114
2017-12-10T13:46:02.904526: step 1183, loss 0.446435, acc 0.8125, prec 0.0309138, recall 0.77114
2017-12-10T13:46:03.088292: step 1184, loss 1.73866, acc 0.796875, prec 0.0309704, recall 0.77156
2017-12-10T13:46:03.280708: step 1185, loss 0.474772, acc 0.765625, prec 0.0309533, recall 0.77156
2017-12-10T13:46:03.465277: step 1186, loss 0.812932, acc 0.734375, prec 0.0309339, recall 0.77156
2017-12-10T13:46:03.655724: step 1187, loss 0.636115, acc 0.734375, prec 0.0309502, recall 0.771769
2017-12-10T13:46:03.844002: step 1188, loss 0.775142, acc 0.828125, prec 0.0309377, recall 0.771769
2017-12-10T13:46:04.032450: step 1189, loss 1.0604, acc 0.796875, prec 0.0309941, recall 0.772187
2017-12-10T13:46:04.221142: step 1190, loss 0.936818, acc 0.765625, prec 0.0310126, recall 0.772395
2017-12-10T13:46:04.408882: step 1191, loss 1.70884, acc 0.84375, prec 0.0311079, recall 0.773017
2017-12-10T13:46:04.597640: step 1192, loss 0.564884, acc 0.78125, prec 0.0310919, recall 0.773017
2017-12-10T13:46:04.786291: step 1193, loss 0.525111, acc 0.765625, prec 0.0310748, recall 0.773017
2017-12-10T13:46:04.975071: step 1194, loss 0.615431, acc 0.8125, prec 0.0311321, recall 0.77343
2017-12-10T13:46:05.164431: step 1195, loss 0.593129, acc 0.8125, prec 0.0311184, recall 0.77343
2017-12-10T13:46:05.352265: step 1196, loss 0.63438, acc 0.78125, prec 0.0311025, recall 0.77343
2017-12-10T13:46:05.537233: step 1197, loss 0.544774, acc 0.859375, prec 0.0311277, recall 0.773636
2017-12-10T13:46:05.724774: step 1198, loss 0.63863, acc 0.828125, prec 0.031186, recall 0.774047
2017-12-10T13:46:05.910598: step 1199, loss 0.716566, acc 0.859375, prec 0.0312466, recall 0.774457
2017-12-10T13:46:06.097311: step 1200, loss 0.66848, acc 0.78125, prec 0.0312306, recall 0.774457
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_0/1512931325/checkpoints/model-1200

2017-12-10T13:46:07.357772: step 1201, loss 0.690127, acc 0.84375, prec 0.0312192, recall 0.774457
2017-12-10T13:46:07.548600: step 1202, loss 0.618795, acc 0.859375, prec 0.0312443, recall 0.774661
2017-12-10T13:46:07.738541: step 1203, loss 0.556475, acc 0.875, prec 0.0312352, recall 0.774661
2017-12-10T13:46:07.925141: step 1204, loss 0.980335, acc 0.765625, prec 0.0312534, recall 0.774864
2017-12-10T13:46:08.117349: step 1205, loss 0.563923, acc 0.828125, prec 0.0313468, recall 0.775473
2017-12-10T13:46:08.305334: step 1206, loss 0.429773, acc 0.84375, prec 0.0313707, recall 0.775676
2017-12-10T13:46:08.494939: step 1207, loss 0.505233, acc 0.84375, prec 0.0313945, recall 0.775878
2017-12-10T13:46:08.681941: step 1208, loss 1.58298, acc 0.8125, prec 0.0314525, recall 0.775584
2017-12-10T13:46:08.873768: step 1209, loss 0.46175, acc 0.8125, prec 0.0314388, recall 0.775584
2017-12-10T13:46:09.062845: step 1210, loss 0.262916, acc 0.875, prec 0.0314296, recall 0.775584
2017-12-10T13:46:09.251561: step 1211, loss 2.79429, acc 0.921875, prec 0.031425, recall 0.774888
2017-12-10T13:46:09.443564: step 1212, loss 0.293716, acc 0.90625, prec 0.0314182, recall 0.774888
2017-12-10T13:46:09.635570: step 1213, loss 10.3782, acc 0.8125, prec 0.0314408, recall 0.774396
2017-12-10T13:46:09.829504: step 1214, loss 0.410586, acc 0.875, prec 0.0314669, recall 0.774597
2017-12-10T13:46:10.021430: step 1215, loss 7.12485, acc 0.75, prec 0.0314497, recall 0.773905
2017-12-10T13:46:10.208769: step 1216, loss 0.492686, acc 0.84375, prec 0.0314383, recall 0.773905
2017-12-10T13:46:10.396368: step 1217, loss 0.936861, acc 0.71875, prec 0.0314178, recall 0.773905
2017-12-10T13:46:10.586741: step 1218, loss 1.12685, acc 0.734375, prec 0.0314687, recall 0.774309
2017-12-10T13:46:10.775195: step 1219, loss 0.82707, acc 0.6875, prec 0.0314809, recall 0.77451
2017-12-10T13:46:10.960837: step 1220, loss 0.99506, acc 0.734375, prec 0.0314616, recall 0.77451
2017-12-10T13:46:11.145839: step 1221, loss 1.16982, acc 0.640625, prec 0.0314704, recall 0.774711
2017-12-10T13:46:11.332575: step 1222, loss 0.894928, acc 0.671875, prec 0.0314465, recall 0.774711
2017-12-10T13:46:11.515772: step 1223, loss 0.958356, acc 0.71875, prec 0.0314261, recall 0.774711
2017-12-10T13:46:11.704232: step 1224, loss 2.02914, acc 0.53125, prec 0.0313921, recall 0.774711
2017-12-10T13:46:11.894685: step 1225, loss 1.10441, acc 0.6875, prec 0.0314044, recall 0.774911
2017-12-10T13:46:12.080435: step 1226, loss 1.20537, acc 0.609375, prec 0.031411, recall 0.775111
2017-12-10T13:46:12.264454: step 1227, loss 0.982726, acc 0.6875, prec 0.0313884, recall 0.775111
2017-12-10T13:46:12.452286: step 1228, loss 1.18664, acc 0.734375, prec 0.0313692, recall 0.775111
2017-12-10T13:46:12.641330: step 1229, loss 6.19306, acc 0.765625, prec 0.0313882, recall 0.774623
2017-12-10T13:46:12.834108: step 1230, loss 3.64776, acc 0.765625, prec 0.0314072, recall 0.774136
2017-12-10T13:46:13.021428: step 1231, loss 1.10708, acc 0.671875, prec 0.0314183, recall 0.774336
2017-12-10T13:46:13.207218: step 1232, loss 0.706731, acc 0.734375, prec 0.0314339, recall 0.774536
2017-12-10T13:46:13.393928: step 1233, loss 0.816307, acc 0.75, prec 0.0314159, recall 0.774536
2017-12-10T13:46:13.576421: step 1234, loss 0.773358, acc 0.734375, prec 0.0313967, recall 0.774536
2017-12-10T13:46:13.769358: step 1235, loss 1.23392, acc 0.6875, prec 0.0314436, recall 0.774934
2017-12-10T13:46:13.962569: step 1236, loss 1.1305, acc 0.671875, prec 0.0314546, recall 0.775132
2017-12-10T13:46:14.148782: step 1237, loss 1.00911, acc 0.71875, prec 0.0315037, recall 0.775528
2017-12-10T13:46:14.339486: step 1238, loss 1.17686, acc 0.75, prec 0.0314857, recall 0.775528
2017-12-10T13:46:14.527072: step 1239, loss 1.18063, acc 0.703125, prec 0.0314643, recall 0.775528
2017-12-10T13:46:14.712066: step 1240, loss 0.962917, acc 0.703125, prec 0.0314429, recall 0.775528
2017-12-10T13:46:14.903646: step 1241, loss 0.909082, acc 0.703125, prec 0.0314216, recall 0.775528
2017-12-10T13:46:15.093217: step 1242, loss 0.836442, acc 0.765625, prec 0.0314048, recall 0.775528
2017-12-10T13:46:15.283149: step 1243, loss 0.902399, acc 0.65625, prec 0.0314492, recall 0.775923
2017-12-10T13:46:15.475014: step 1244, loss 0.392535, acc 0.875, prec 0.0314748, recall 0.776119
2017-12-10T13:46:15.664567: step 1245, loss 0.893447, acc 0.75, prec 0.0315258, recall 0.776512
2017-12-10T13:46:15.853217: step 1246, loss 0.383723, acc 0.84375, prec 0.0315145, recall 0.776512
2017-12-10T13:46:16.041819: step 1247, loss 0.24014, acc 0.953125, prec 0.0315456, recall 0.776708
2017-12-10T13:46:16.229562: step 1248, loss 0.908923, acc 0.859375, prec 0.03157, recall 0.776903
2017-12-10T13:46:16.420003: step 1249, loss 0.208758, acc 0.9375, prec 0.0315655, recall 0.776903
2017-12-10T13:46:16.611153: step 1250, loss 1.28331, acc 0.90625, prec 0.0316276, recall 0.777293
2017-12-10T13:46:16.799915: step 1251, loss 0.331295, acc 0.921875, prec 0.0316564, recall 0.777487
2017-12-10T13:46:16.989762: step 1252, loss 0.326146, acc 0.921875, prec 0.0316507, recall 0.777487
2017-12-10T13:46:17.176597: step 1253, loss 0.700711, acc 0.96875, prec 0.0316829, recall 0.777681
2017-12-10T13:46:17.366721: step 1254, loss 0.499597, acc 0.890625, prec 0.0317438, recall 0.778068
2017-12-10T13:46:17.554202: step 1255, loss 0.255541, acc 0.921875, prec 0.0317381, recall 0.778068
2017-12-10T13:46:17.750955: step 1256, loss 5.40493, acc 0.875, prec 0.0317646, recall 0.777585
2017-12-10T13:46:17.941013: step 1257, loss 0.343098, acc 0.875, prec 0.0317556, recall 0.777585
2017-12-10T13:46:18.128395: step 1258, loss 9.13937, acc 0.921875, prec 0.0318198, recall 0.777296
2017-12-10T13:46:18.315701: step 1259, loss 0.290379, acc 0.890625, prec 0.0318462, recall 0.777489
2017-12-10T13:46:18.503142: step 1260, loss 7.27699, acc 0.765625, prec 0.0318991, recall 0.777202
2017-12-10T13:46:18.692448: step 1261, loss 5.55258, acc 0.765625, prec 0.0319518, recall 0.776916
2017-12-10T13:46:18.879100: step 1262, loss 1.16167, acc 0.6875, prec 0.0319292, recall 0.776916
2017-12-10T13:46:19.067507: step 1263, loss 1.15619, acc 0.65625, prec 0.0319728, recall 0.7773
2017-12-10T13:46:19.255755: step 1264, loss 1.52266, acc 0.59375, prec 0.0319435, recall 0.7773
2017-12-10T13:46:19.438741: step 1265, loss 1.38759, acc 0.671875, prec 0.0319198, recall 0.7773
2017-12-10T13:46:19.629922: step 1266, loss 2.23042, acc 0.40625, prec 0.031877, recall 0.7773
2017-12-10T13:46:19.817679: step 1267, loss 1.98027, acc 0.421875, prec 0.0318355, recall 0.7773
2017-12-10T13:46:20.001304: step 1268, loss 1.84421, acc 0.578125, prec 0.0319074, recall 0.777873
2017-12-10T13:46:20.186152: step 1269, loss 2.72108, acc 0.390625, prec 0.0319317, recall 0.778253
2017-12-10T13:46:20.368738: step 1270, loss 2.50186, acc 0.453125, prec 0.0319265, recall 0.778443
2017-12-10T13:46:20.555684: step 1271, loss 2.07067, acc 0.53125, prec 0.0318929, recall 0.778443
2017-12-10T13:46:20.740744: step 1272, loss 2.06709, acc 0.546875, prec 0.0318944, recall 0.778632
2017-12-10T13:46:20.929326: step 1273, loss 1.74286, acc 0.609375, prec 0.0319342, recall 0.77901
2017-12-10T13:46:21.112817: step 1274, loss 1.86995, acc 0.578125, prec 0.0319041, recall 0.77901
2017-12-10T13:46:21.300785: step 1275, loss 1.4206, acc 0.625, prec 0.0319112, recall 0.779199
2017-12-10T13:46:21.487986: step 1276, loss 1.0495, acc 0.65625, prec 0.0318867, recall 0.779199
2017-12-10T13:46:21.673949: step 1277, loss 1.39813, acc 0.671875, prec 0.0318633, recall 0.779199
2017-12-10T13:46:21.859749: step 1278, loss 0.834457, acc 0.71875, prec 0.0318771, recall 0.779387
2017-12-10T13:46:22.050104: step 1279, loss 0.531701, acc 0.796875, prec 0.0318627, recall 0.779387
2017-12-10T13:46:22.234582: step 1280, loss 0.831152, acc 0.765625, prec 0.031846, recall 0.779387
2017-12-10T13:46:22.421752: step 1281, loss 0.809239, acc 0.734375, prec 0.0318609, recall 0.779574
2017-12-10T13:46:22.606819: step 1282, loss 0.442725, acc 0.828125, prec 0.0318823, recall 0.779762
2017-12-10T13:46:22.792649: step 1283, loss 0.104738, acc 0.921875, prec 0.0318768, recall 0.779762
2017-12-10T13:46:22.978703: step 1284, loss 0.345393, acc 0.84375, prec 0.0318657, recall 0.779762
2017-12-10T13:46:23.166918: step 1285, loss 1.79538, acc 0.828125, prec 0.0318547, recall 0.779099
2017-12-10T13:46:23.353477: step 1286, loss 14.6138, acc 0.875, prec 0.0319142, recall 0.778814
2017-12-10T13:46:23.543411: step 1287, loss 0.280215, acc 0.9375, prec 0.0319097, recall 0.778814
2017-12-10T13:46:23.729767: step 1288, loss 0.274595, acc 0.921875, prec 0.0319042, recall 0.778814
2017-12-10T13:46:23.919150: step 1289, loss 0.337276, acc 0.84375, prec 0.0318931, recall 0.778814
2017-12-10T13:46:24.107753: step 1290, loss 0.564954, acc 0.875, prec 0.0318843, recall 0.778814
2017-12-10T13:46:24.292760: step 1291, loss 0.0793584, acc 0.96875, prec 0.0319156, recall 0.779001
2017-12-10T13:46:24.485367: step 1292, loss 3.36417, acc 0.9375, prec 0.0319795, recall 0.778716
2017-12-10T13:46:24.678854: step 1293, loss 0.285051, acc 0.890625, prec 0.0319717, recall 0.778716
2017-12-10T13:46:24.862962: step 1294, loss 0.751381, acc 0.859375, prec 0.0319953, recall 0.778903
2017-12-10T13:46:25.047403: step 1295, loss 0.547685, acc 0.78125, prec 0.0320133, recall 0.779089
2017-12-10T13:46:25.238132: step 1296, loss 1.67575, acc 0.828125, prec 0.0320022, recall 0.778433
2017-12-10T13:46:25.427152: step 1297, loss 0.624228, acc 0.828125, prec 0.0320906, recall 0.778992
2017-12-10T13:46:25.620032: step 1298, loss 10.5134, acc 0.84375, prec 0.0320817, recall 0.777685
2017-12-10T13:46:25.810764: step 1299, loss 0.607282, acc 0.78125, prec 0.0320661, recall 0.777685
2017-12-10T13:46:25.998678: step 1300, loss 1.35911, acc 0.640625, prec 0.0320406, recall 0.777685
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_0/1512931325/checkpoints/model-1300

2017-12-10T13:46:27.150538: step 1301, loss 1.44383, acc 0.53125, prec 0.0320743, recall 0.778057
2017-12-10T13:46:27.335929: step 1302, loss 1.16082, acc 0.609375, prec 0.0320466, recall 0.778057
2017-12-10T13:46:27.524100: step 1303, loss 1.7243, acc 0.5625, prec 0.0320157, recall 0.778057
2017-12-10T13:46:27.711557: step 1304, loss 1.39797, acc 0.6875, prec 0.0320937, recall 0.778613
2017-12-10T13:46:27.899272: step 1305, loss 1.34042, acc 0.71875, prec 0.0320738, recall 0.778613
2017-12-10T13:46:28.084890: step 1306, loss 1.706, acc 0.59375, prec 0.0321117, recall 0.778982
2017-12-10T13:46:28.276561: step 1307, loss 1.04945, acc 0.609375, prec 0.0321506, recall 0.779351
2017-12-10T13:46:28.459962: step 1308, loss 1.5891, acc 0.546875, prec 0.0321518, recall 0.779534
2017-12-10T13:46:28.645978: step 1309, loss 1.21981, acc 0.71875, prec 0.0321983, recall 0.7799
2017-12-10T13:46:28.836881: step 1310, loss 1.36549, acc 0.703125, prec 0.0321774, recall 0.7799
2017-12-10T13:46:29.026472: step 1311, loss 1.59297, acc 0.640625, prec 0.0322183, recall 0.780265
2017-12-10T13:46:29.210703: step 1312, loss 1.15388, acc 0.75, prec 0.0322338, recall 0.780447
2017-12-10T13:46:29.399671: step 1313, loss 1.04329, acc 0.6875, prec 0.0322117, recall 0.780447
2017-12-10T13:46:29.586608: step 1314, loss 0.971948, acc 0.734375, prec 0.0322922, recall 0.780992
2017-12-10T13:46:29.775690: step 1315, loss 0.570781, acc 0.796875, prec 0.0322779, recall 0.780992
2017-12-10T13:46:29.963748: step 1316, loss 0.476853, acc 0.828125, prec 0.0322988, recall 0.781173
2017-12-10T13:46:30.150932: step 1317, loss 0.619006, acc 0.8125, prec 0.0323186, recall 0.781353
2017-12-10T13:46:30.336452: step 1318, loss 0.679355, acc 0.765625, prec 0.0323351, recall 0.781533
2017-12-10T13:46:30.526415: step 1319, loss 0.432655, acc 0.859375, prec 0.0323252, recall 0.781533
2017-12-10T13:46:30.715459: step 1320, loss 0.706246, acc 0.828125, prec 0.032313, recall 0.781533
2017-12-10T13:46:30.903501: step 1321, loss 1.05739, acc 0.734375, prec 0.0323273, recall 0.781713
2017-12-10T13:46:31.093528: step 1322, loss 0.355298, acc 0.90625, prec 0.0323536, recall 0.781893
2017-12-10T13:46:31.283669: step 1323, loss 0.191462, acc 0.921875, prec 0.0323811, recall 0.782072
2017-12-10T13:46:31.476347: step 1324, loss 0.159736, acc 0.921875, prec 0.0323756, recall 0.782072
2017-12-10T13:46:31.661437: step 1325, loss 0.0761522, acc 0.96875, prec 0.0323734, recall 0.782072
2017-12-10T13:46:31.844701: step 1326, loss 1.04533, acc 0.921875, prec 0.0324008, recall 0.782251
2017-12-10T13:46:32.034737: step 1327, loss 0.0954707, acc 0.953125, prec 0.0323975, recall 0.782251
2017-12-10T13:46:32.223588: step 1328, loss 1.01648, acc 0.953125, prec 0.0324271, recall 0.78243
2017-12-10T13:46:32.411903: step 1329, loss 8.96304, acc 0.890625, prec 0.0324205, recall 0.781788
2017-12-10T13:46:32.602720: step 1330, loss 4.92933, acc 0.953125, prec 0.0324183, recall 0.781148
2017-12-10T13:46:32.795786: step 1331, loss 2.53474, acc 0.90625, prec 0.0324128, recall 0.780508
2017-12-10T13:46:32.989216: step 1332, loss 0.499286, acc 0.84375, prec 0.0324017, recall 0.780508
2017-12-10T13:46:33.178483: step 1333, loss 0.87741, acc 0.875, prec 0.0325245, recall 0.781224
2017-12-10T13:46:33.368124: step 1334, loss 0.574755, acc 0.796875, prec 0.032543, recall 0.781403
2017-12-10T13:46:33.558456: step 1335, loss 0.83435, acc 0.71875, prec 0.0325231, recall 0.781403
2017-12-10T13:46:33.743904: step 1336, loss 0.892629, acc 0.703125, prec 0.0325349, recall 0.781581
2017-12-10T13:46:33.929500: step 1337, loss 1.02373, acc 0.671875, prec 0.0325118, recall 0.781581
2017-12-10T13:46:34.115876: step 1338, loss 1.14305, acc 0.703125, prec 0.0325236, recall 0.781759
2017-12-10T13:46:34.301254: step 1339, loss 1.18142, acc 0.703125, prec 0.0325027, recall 0.781759
2017-12-10T13:46:34.488369: step 1340, loss 0.902732, acc 0.703125, prec 0.0325473, recall 0.782114
2017-12-10T13:46:34.675324: step 1341, loss 1.46731, acc 0.625, prec 0.0325536, recall 0.782291
2017-12-10T13:46:34.865947: step 1342, loss 1.16132, acc 0.6875, prec 0.0325316, recall 0.782291
2017-12-10T13:46:35.051911: step 1343, loss 0.862959, acc 0.703125, prec 0.0325107, recall 0.782291
2017-12-10T13:46:35.238917: step 1344, loss 2.89062, acc 0.734375, prec 0.0325258, recall 0.781833
2017-12-10T13:46:35.429437: step 1345, loss 0.910859, acc 0.734375, prec 0.0325072, recall 0.781833
2017-12-10T13:46:35.615316: step 1346, loss 0.818073, acc 0.640625, prec 0.0325146, recall 0.78201
2017-12-10T13:46:35.805502: step 1347, loss 1.0261, acc 0.75, prec 0.0325622, recall 0.782362
2017-12-10T13:46:35.991994: step 1348, loss 0.632091, acc 0.796875, prec 0.0325805, recall 0.782538
2017-12-10T13:46:36.178618: step 1349, loss 0.735415, acc 0.78125, prec 0.0325652, recall 0.782538
2017-12-10T13:46:36.363847: step 1350, loss 0.846991, acc 0.84375, prec 0.0325542, recall 0.782538
2017-12-10T13:46:36.549968: step 1351, loss 0.440087, acc 0.84375, prec 0.0325758, recall 0.782714
2017-12-10T13:46:36.740575: step 1352, loss 0.582191, acc 0.828125, prec 0.0325638, recall 0.782714
2017-12-10T13:46:36.929256: step 1353, loss 0.562455, acc 0.859375, prec 0.0325864, recall 0.782889
2017-12-10T13:46:37.119149: step 1354, loss 0.475195, acc 0.828125, prec 0.0325744, recall 0.782889
2017-12-10T13:46:37.306696: step 1355, loss 0.652874, acc 0.796875, prec 0.0325926, recall 0.783065
2017-12-10T13:46:37.494261: step 1356, loss 0.482111, acc 0.9375, prec 0.0326207, recall 0.783239
2017-12-10T13:46:37.683632: step 1357, loss 0.570685, acc 0.828125, prec 0.0326412, recall 0.783414
2017-12-10T13:46:37.871545: step 1358, loss 0.255149, acc 0.90625, prec 0.0326346, recall 0.783414
2017-12-10T13:46:38.060406: step 1359, loss 0.0806895, acc 0.953125, prec 0.0326637, recall 0.783588
2017-12-10T13:46:38.247169: step 1360, loss 0.310481, acc 0.921875, prec 0.0326907, recall 0.783762
2017-12-10T13:46:38.440064: step 1361, loss 0.302661, acc 0.890625, prec 0.0327155, recall 0.783936
2017-12-10T13:46:38.627454: step 1362, loss 0.140892, acc 0.953125, prec 0.0327122, recall 0.783936
2017-12-10T13:46:38.814535: step 1363, loss 0.213536, acc 0.90625, prec 0.032738, recall 0.784109
2017-12-10T13:46:39.003134: step 1364, loss 0.0668231, acc 0.953125, prec 0.0327347, recall 0.784109
2017-12-10T13:46:39.190697: step 1365, loss 0.701997, acc 0.90625, prec 0.0327605, recall 0.784282
2017-12-10T13:46:39.382792: step 1366, loss 1.23311, acc 0.875, prec 0.0328165, recall 0.784628
2017-12-10T13:46:39.578778: step 1367, loss 0.250698, acc 0.953125, prec 0.0328132, recall 0.784628
2017-12-10T13:46:39.766652: step 1368, loss 0.244153, acc 0.9375, prec 0.0328088, recall 0.784628
2017-12-10T13:46:39.953261: step 1369, loss 0.166508, acc 0.90625, prec 0.032867, recall 0.784972
2017-12-10T13:46:40.141746: step 1370, loss 4.52585, acc 0.8125, prec 0.0328873, recall 0.784517
2017-12-10T13:46:40.334175: step 1371, loss 8.73181, acc 0.859375, prec 0.0329108, recall 0.784064
2017-12-10T13:46:40.527724: step 1372, loss 0.386345, acc 0.890625, prec 0.0329354, recall 0.784236
2017-12-10T13:46:40.714254: step 1373, loss 0.272301, acc 0.90625, prec 0.0329612, recall 0.784407
2017-12-10T13:46:40.902211: step 1374, loss 0.270117, acc 0.921875, prec 0.0330203, recall 0.78475
2017-12-10T13:46:41.086760: step 1375, loss 0.632956, acc 0.8125, prec 0.033007, recall 0.78475
2017-12-10T13:46:41.275872: step 1376, loss 4.03605, acc 0.859375, prec 0.0330305, recall 0.784298
2017-12-10T13:46:41.463775: step 1377, loss 0.376436, acc 0.890625, prec 0.0330551, recall 0.784469
2017-12-10T13:46:41.647923: step 1378, loss 0.921569, acc 0.765625, prec 0.0330385, recall 0.784469
2017-12-10T13:46:41.832549: step 1379, loss 0.903232, acc 0.734375, prec 0.0330198, recall 0.784469
2017-12-10T13:46:42.019360: step 1380, loss 0.585855, acc 0.75, prec 0.0330344, recall 0.78464
2017-12-10T13:46:42.208722: step 1381, loss 0.980493, acc 0.703125, prec 0.0330135, recall 0.78464
2017-12-10T13:46:42.399609: step 1382, loss 1.07208, acc 0.65625, prec 0.0330215, recall 0.78481
2017-12-10T13:46:42.587974: step 1383, loss 0.964823, acc 0.703125, prec 0.0330007, recall 0.78481
2017-12-10T13:46:42.775862: step 1384, loss 0.63478, acc 0.828125, prec 0.0330529, recall 0.78515
2017-12-10T13:46:42.965487: step 1385, loss 1.10617, acc 0.640625, prec 0.0330598, recall 0.78532
2017-12-10T13:46:43.157526: step 1386, loss 1.32167, acc 0.703125, prec 0.0331352, recall 0.785827
2017-12-10T13:46:43.347646: step 1387, loss 1.34401, acc 0.671875, prec 0.0331442, recall 0.785995
2017-12-10T13:46:43.537321: step 1388, loss 0.834572, acc 0.734375, prec 0.0331255, recall 0.785995
2017-12-10T13:46:43.724753: step 1389, loss 0.732109, acc 0.734375, prec 0.033171, recall 0.786331
2017-12-10T13:46:43.916529: step 1390, loss 1.0486, acc 0.703125, prec 0.0332141, recall 0.786667
2017-12-10T13:46:44.104774: step 1391, loss 0.860467, acc 0.765625, prec 0.0331976, recall 0.786667
2017-12-10T13:46:44.290677: step 1392, loss 0.565605, acc 0.828125, prec 0.0332175, recall 0.786834
2017-12-10T13:46:44.478569: step 1393, loss 0.45452, acc 0.859375, prec 0.0332076, recall 0.786834
2017-12-10T13:46:44.669435: step 1394, loss 0.721921, acc 0.796875, prec 0.0332253, recall 0.787001
2017-12-10T13:46:44.857819: step 1395, loss 0.196449, acc 0.921875, prec 0.0332518, recall 0.787167
2017-12-10T13:46:45.047361: step 1396, loss 0.301564, acc 0.90625, prec 0.0332772, recall 0.787334
2017-12-10T13:46:45.240745: step 1397, loss 0.59018, acc 0.859375, prec 0.0332992, recall 0.7875
2017-12-10T13:46:45.430108: step 1398, loss 0.418081, acc 0.90625, prec 0.0332926, recall 0.7875
2017-12-10T13:46:45.616660: step 1399, loss 0.268067, acc 0.90625, prec 0.033286, recall 0.7875
2017-12-10T13:46:45.804143: step 1400, loss 4.08455, acc 0.875, prec 0.0333102, recall 0.787051
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_0/1512931325/checkpoints/model-1400

2017-12-10T13:46:47.061154: step 1401, loss 3.17113, acc 0.84375, prec 0.0333003, recall 0.786438
2017-12-10T13:46:47.249544: step 1402, loss 3.11939, acc 0.859375, prec 0.0332915, recall 0.785826
2017-12-10T13:46:47.440636: step 1403, loss 1.9867, acc 0.890625, prec 0.033285, recall 0.785214
2017-12-10T13:46:47.632178: step 1404, loss 0.426896, acc 0.921875, prec 0.0332795, recall 0.785214
2017-12-10T13:46:47.821964: step 1405, loss 0.673773, acc 0.875, prec 0.0332707, recall 0.785214
2017-12-10T13:46:48.009601: step 1406, loss 0.947392, acc 0.671875, prec 0.0332795, recall 0.785381
2017-12-10T13:46:48.197664: step 1407, loss 0.465145, acc 0.8125, prec 0.0332664, recall 0.785381
2017-12-10T13:46:48.383735: step 1408, loss 0.646975, acc 0.796875, prec 0.0333158, recall 0.785714
2017-12-10T13:46:48.570366: step 1409, loss 0.527026, acc 0.859375, prec 0.0333695, recall 0.786047
2017-12-10T13:46:48.755296: step 1410, loss 1.36063, acc 0.6875, prec 0.0334112, recall 0.786378
2017-12-10T13:46:48.941594: step 1411, loss 0.649694, acc 0.796875, prec 0.0333969, recall 0.786378
2017-12-10T13:46:49.131873: step 1412, loss 0.763193, acc 0.71875, prec 0.0333771, recall 0.786378
2017-12-10T13:46:49.317509: step 1413, loss 0.504762, acc 0.78125, prec 0.0333618, recall 0.786378
2017-12-10T13:46:49.505125: step 1414, loss 0.810623, acc 0.75, prec 0.0334394, recall 0.786873
2017-12-10T13:46:49.691220: step 1415, loss 1.33788, acc 0.6875, prec 0.0334809, recall 0.787201
2017-12-10T13:46:49.880914: step 1416, loss 0.765618, acc 0.75, prec 0.033495, recall 0.787365
2017-12-10T13:46:50.071371: step 1417, loss 3.28247, acc 0.765625, prec 0.0334797, recall 0.786759
2017-12-10T13:46:50.260260: step 1418, loss 0.613035, acc 0.8125, prec 0.0334665, recall 0.786759
2017-12-10T13:46:50.447754: step 1419, loss 1.71475, acc 0.8125, prec 0.0335166, recall 0.787087
2017-12-10T13:46:50.636151: step 1420, loss 0.656349, acc 0.78125, prec 0.0335013, recall 0.787087
2017-12-10T13:46:50.823590: step 1421, loss 1.06305, acc 0.75, prec 0.0335154, recall 0.78725
2017-12-10T13:46:51.006904: step 1422, loss 0.705054, acc 0.828125, prec 0.0335033, recall 0.78725
2017-12-10T13:46:51.195188: step 1423, loss 0.830187, acc 0.84375, prec 0.0335239, recall 0.787414
2017-12-10T13:46:51.384977: step 1424, loss 0.592587, acc 0.796875, prec 0.0336359, recall 0.788064
2017-12-10T13:46:51.569802: step 1425, loss 1.52121, acc 0.625, prec 0.0336411, recall 0.788226
2017-12-10T13:46:51.752999: step 1426, loss 0.644907, acc 0.796875, prec 0.0336584, recall 0.788388
2017-12-10T13:46:51.938526: step 1427, loss 0.767003, acc 0.78125, prec 0.033643, recall 0.788388
2017-12-10T13:46:52.123844: step 1428, loss 0.711491, acc 0.8125, prec 0.0336299, recall 0.788388
2017-12-10T13:46:52.313216: step 1429, loss 0.718567, acc 0.765625, prec 0.0336134, recall 0.788388
2017-12-10T13:46:52.500275: step 1430, loss 0.288601, acc 0.890625, prec 0.0336058, recall 0.788388
2017-12-10T13:46:52.688382: step 1431, loss 0.702132, acc 0.859375, prec 0.0336274, recall 0.78855
2017-12-10T13:46:52.878951: step 1432, loss 0.386675, acc 0.828125, prec 0.0336468, recall 0.788711
2017-12-10T13:46:53.067445: step 1433, loss 1.94599, acc 0.796875, prec 0.0336965, recall 0.788432
2017-12-10T13:46:53.262563: step 1434, loss 0.628475, acc 0.765625, prec 0.0336801, recall 0.788432
2017-12-10T13:46:53.450552: step 1435, loss 0.52719, acc 0.828125, prec 0.0336681, recall 0.788432
2017-12-10T13:46:53.639329: step 1436, loss 4.57541, acc 0.828125, prec 0.0336571, recall 0.787833
2017-12-10T13:46:53.833497: step 1437, loss 5.27192, acc 0.84375, prec 0.0336473, recall 0.787234
2017-12-10T13:46:54.024639: step 1438, loss 0.350185, acc 0.90625, prec 0.0336407, recall 0.787234
2017-12-10T13:46:54.212456: step 1439, loss 0.707427, acc 0.765625, prec 0.0336244, recall 0.787234
2017-12-10T13:46:54.400473: step 1440, loss 2.21114, acc 0.75, prec 0.033608, recall 0.786636
2017-12-10T13:46:54.587468: step 1441, loss 0.76784, acc 0.78125, prec 0.0335927, recall 0.786636
2017-12-10T13:46:54.777485: step 1442, loss 0.820287, acc 0.734375, prec 0.0335742, recall 0.786636
2017-12-10T13:46:54.966769: step 1443, loss 1.12003, acc 0.703125, prec 0.0335536, recall 0.786636
2017-12-10T13:46:55.153335: step 1444, loss 0.645194, acc 0.796875, prec 0.0335707, recall 0.786798
2017-12-10T13:46:55.340530: step 1445, loss 0.719742, acc 0.765625, prec 0.0335857, recall 0.78696
2017-12-10T13:46:55.525543: step 1446, loss 0.825734, acc 0.71875, prec 0.0335662, recall 0.78696
2017-12-10T13:46:55.712369: step 1447, loss 1.33999, acc 0.671875, prec 0.0335434, recall 0.78696
2017-12-10T13:46:55.901249: step 1448, loss 0.857161, acc 0.734375, prec 0.033525, recall 0.78696
2017-12-10T13:46:56.084506: step 1449, loss 2.30804, acc 0.71875, prec 0.0335066, recall 0.786364
2017-12-10T13:46:56.273210: step 1450, loss 0.645733, acc 0.78125, prec 0.0334914, recall 0.786364
2017-12-10T13:46:56.457716: step 1451, loss 0.445537, acc 0.75, prec 0.0334742, recall 0.786364
2017-12-10T13:46:56.644245: step 1452, loss 0.651195, acc 0.765625, prec 0.0334891, recall 0.786525
2017-12-10T13:46:56.827852: step 1453, loss 0.38865, acc 0.875, prec 0.0334805, recall 0.786525
2017-12-10T13:46:57.018315: step 1454, loss 0.479241, acc 0.890625, prec 0.0335041, recall 0.786687
2017-12-10T13:46:57.205374: step 1455, loss 5.47351, acc 0.828125, prec 0.0335244, recall 0.786254
2017-12-10T13:46:57.397943: step 1456, loss 1.52277, acc 0.78125, prec 0.0336337, recall 0.786898
2017-12-10T13:46:57.591260: step 1457, loss 0.780429, acc 0.703125, prec 0.0336443, recall 0.787058
2017-12-10T13:46:57.780373: step 1458, loss 1.20462, acc 0.765625, prec 0.0336591, recall 0.787218
2017-12-10T13:46:57.971191: step 1459, loss 0.49529, acc 0.8125, prec 0.0336461, recall 0.787218
2017-12-10T13:46:58.159688: step 1460, loss 0.598334, acc 0.8125, prec 0.0336952, recall 0.787538
2017-12-10T13:46:58.343974: step 1461, loss 0.615946, acc 0.8125, prec 0.0336822, recall 0.787538
2017-12-10T13:46:58.529141: step 1462, loss 0.703972, acc 0.8125, prec 0.0337003, recall 0.787697
2017-12-10T13:46:58.722468: step 1463, loss 0.77008, acc 0.734375, prec 0.0337129, recall 0.787856
2017-12-10T13:46:58.915181: step 1464, loss 0.871593, acc 0.765625, prec 0.0336967, recall 0.787856
2017-12-10T13:46:59.101371: step 1465, loss 7.58913, acc 0.796875, prec 0.0337147, recall 0.787425
2017-12-10T13:46:59.289095: step 1466, loss 0.754178, acc 0.75, prec 0.0337284, recall 0.787584
2017-12-10T13:46:59.475158: step 1467, loss 0.654055, acc 0.8125, prec 0.0337773, recall 0.787901
2017-12-10T13:46:59.663956: step 1468, loss 1.26424, acc 0.78125, prec 0.0338549, recall 0.788376
2017-12-10T13:46:59.852390: step 1469, loss 0.890429, acc 0.8125, prec 0.0339037, recall 0.78869
2017-12-10T13:47:00.036087: step 1470, loss 2.08408, acc 0.734375, prec 0.0339173, recall 0.788262
2017-12-10T13:47:00.228134: step 1471, loss 0.513167, acc 0.84375, prec 0.0339373, recall 0.788419
2017-12-10T13:47:00.415119: step 1472, loss 1.12312, acc 0.8125, prec 0.0339552, recall 0.788576
2017-12-10T13:47:00.604262: step 1473, loss 1.38369, acc 0.640625, prec 0.0339611, recall 0.788732
2017-12-10T13:47:00.792439: step 1474, loss 0.766683, acc 0.75, prec 0.0339745, recall 0.788889
2017-12-10T13:47:00.981901: step 1475, loss 0.903124, acc 0.71875, prec 0.033955, recall 0.788889
2017-12-10T13:47:01.174064: step 1476, loss 0.91031, acc 0.71875, prec 0.0339356, recall 0.788889
2017-12-10T13:47:01.365526: step 1477, loss 0.717651, acc 0.75, prec 0.0339798, recall 0.789201
2017-12-10T13:47:01.550688: step 1478, loss 1.22721, acc 0.734375, prec 0.0339614, recall 0.789201
2017-12-10T13:47:01.737461: step 1479, loss 1.33774, acc 0.734375, prec 0.0339738, recall 0.789357
2017-12-10T13:47:01.924641: step 1480, loss 6.27413, acc 0.71875, prec 0.0339554, recall 0.788774
2017-12-10T13:47:02.115099: step 1481, loss 0.775816, acc 0.765625, prec 0.0339699, recall 0.78893
2017-12-10T13:47:02.304756: step 1482, loss 0.544772, acc 0.8125, prec 0.033957, recall 0.78893
2017-12-10T13:47:02.489587: step 1483, loss 0.713194, acc 0.796875, prec 0.033943, recall 0.78893
2017-12-10T13:47:02.677272: step 1484, loss 1.15974, acc 0.78125, prec 0.0339586, recall 0.789086
2017-12-10T13:47:02.866512: step 1485, loss 0.583333, acc 0.78125, prec 0.0339435, recall 0.789086
2017-12-10T13:47:03.052876: step 1486, loss 1.30787, acc 0.671875, prec 0.0339209, recall 0.789086
2017-12-10T13:47:03.240802: step 1487, loss 1.19696, acc 0.734375, prec 0.0339332, recall 0.789241
2017-12-10T13:47:03.425859: step 1488, loss 0.777506, acc 0.765625, prec 0.0339477, recall 0.789396
2017-12-10T13:47:03.612985: step 1489, loss 0.854586, acc 0.75, prec 0.0339305, recall 0.789396
2017-12-10T13:47:03.802611: step 1490, loss 0.612385, acc 0.78125, prec 0.033946, recall 0.789551
2017-12-10T13:47:03.973640: step 1491, loss 2.21012, acc 0.823529, prec 0.0339985, recall 0.78928
2017-12-10T13:47:04.172490: step 1492, loss 0.746698, acc 0.75, prec 0.0339813, recall 0.78928
2017-12-10T13:47:04.361891: step 1493, loss 0.247865, acc 0.921875, prec 0.0340065, recall 0.789435
2017-12-10T13:47:04.549384: step 1494, loss 0.349481, acc 0.859375, prec 0.0340579, recall 0.789744
2017-12-10T13:47:04.734562: step 1495, loss 0.370456, acc 0.90625, prec 0.0340514, recall 0.789744
2017-12-10T13:47:04.925216: step 1496, loss 0.334486, acc 0.90625, prec 0.0340755, recall 0.789898
2017-12-10T13:47:05.109688: step 1497, loss 0.684648, acc 0.796875, prec 0.0340615, recall 0.789898
2017-12-10T13:47:05.296321: step 1498, loss 2.33656, acc 0.84375, prec 0.0340823, recall 0.789474
2017-12-10T13:47:05.487874: step 1499, loss 0.275806, acc 0.9375, prec 0.034078, recall 0.789474
2017-12-10T13:47:05.677469: step 1500, loss 0.70674, acc 0.8125, prec 0.0340651, recall 0.789474
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_0/1512931325/checkpoints/model-1500

2017-12-10T13:47:06.954633: step 1501, loss 0.468196, acc 0.8125, prec 0.0340827, recall 0.789627
2017-12-10T13:47:07.141915: step 1502, loss 0.537688, acc 0.828125, prec 0.0341317, recall 0.789934
2017-12-10T13:47:07.331851: step 1503, loss 0.40509, acc 0.8125, prec 0.0341188, recall 0.789934
2017-12-10T13:47:07.522251: step 1504, loss 0.659325, acc 0.8125, prec 0.0341364, recall 0.790087
2017-12-10T13:47:07.710512: step 1505, loss 0.296048, acc 0.875, prec 0.0341886, recall 0.790393
2017-12-10T13:47:07.898173: step 1506, loss 5.11346, acc 0.875, prec 0.034181, recall 0.789818
2017-12-10T13:47:08.092443: step 1507, loss 7.96384, acc 0.84375, prec 0.0342028, recall 0.788824
2017-12-10T13:47:08.281509: step 1508, loss 0.36106, acc 0.875, prec 0.0342246, recall 0.788978
2017-12-10T13:47:08.467939: step 1509, loss 0.394675, acc 0.859375, prec 0.0342453, recall 0.78913
2017-12-10T13:47:08.655022: step 1510, loss 0.967466, acc 0.765625, prec 0.0342291, recall 0.78913
2017-12-10T13:47:08.838762: step 1511, loss 0.59659, acc 0.796875, prec 0.0342152, recall 0.78913
2017-12-10T13:47:09.024999: step 1512, loss 1.71356, acc 0.75, prec 0.0342889, recall 0.789588
2017-12-10T13:47:09.217987: step 1513, loss 0.392793, acc 0.8125, prec 0.034276, recall 0.789588
2017-12-10T13:47:09.403201: step 1514, loss 0.950992, acc 0.734375, prec 0.0343183, recall 0.789892
2017-12-10T13:47:09.586554: step 1515, loss 0.713359, acc 0.828125, prec 0.0343065, recall 0.789892
2017-12-10T13:47:09.774832: step 1516, loss 0.758176, acc 0.78125, prec 0.0343217, recall 0.790043
2017-12-10T13:47:09.961712: step 1517, loss 0.644107, acc 0.734375, prec 0.0343034, recall 0.790043
2017-12-10T13:47:10.152323: step 1518, loss 0.928445, acc 0.71875, prec 0.0342841, recall 0.790043
2017-12-10T13:47:10.335848: step 1519, loss 0.796016, acc 0.796875, prec 0.0343306, recall 0.790346
2017-12-10T13:47:10.522115: step 1520, loss 0.913293, acc 0.71875, prec 0.0343717, recall 0.790648
2017-12-10T13:47:10.707541: step 1521, loss 0.564921, acc 0.796875, prec 0.0343577, recall 0.790648
2017-12-10T13:47:10.892807: step 1522, loss 0.869415, acc 0.78125, prec 0.0343729, recall 0.790798
2017-12-10T13:47:11.078054: step 1523, loss 1.0323, acc 0.78125, prec 0.0344181, recall 0.791098
2017-12-10T13:47:11.266414: step 1524, loss 0.448201, acc 0.875, prec 0.0344095, recall 0.791098
2017-12-10T13:47:11.450412: step 1525, loss 0.717684, acc 0.765625, prec 0.0343934, recall 0.791098
2017-12-10T13:47:11.635656: step 1526, loss 0.391259, acc 0.84375, prec 0.034443, recall 0.791398
2017-12-10T13:47:11.822425: step 1527, loss 0.441683, acc 0.859375, prec 0.0344333, recall 0.791398
2017-12-10T13:47:12.009056: step 1528, loss 0.649205, acc 0.78125, prec 0.0344183, recall 0.791398
2017-12-10T13:47:12.194640: step 1529, loss 0.162333, acc 0.9375, prec 0.0344441, recall 0.791547
2017-12-10T13:47:12.380687: step 1530, loss 0.552478, acc 0.859375, prec 0.0344344, recall 0.791547
2017-12-10T13:47:12.570001: step 1531, loss 0.278996, acc 0.921875, prec 0.034429, recall 0.791547
2017-12-10T13:47:12.762720: step 1532, loss 0.955866, acc 0.90625, prec 0.0344828, recall 0.791846
2017-12-10T13:47:12.953036: step 1533, loss 3.14908, acc 0.875, prec 0.0344752, recall 0.791279
2017-12-10T13:47:13.145672: step 1534, loss 0.219928, acc 0.9375, prec 0.0344709, recall 0.791279
2017-12-10T13:47:13.329689: step 1535, loss 0.308394, acc 0.90625, prec 0.0344946, recall 0.791429
2017-12-10T13:47:13.518550: step 1536, loss 0.41857, acc 0.859375, prec 0.034545, recall 0.791726
2017-12-10T13:47:13.710213: step 1537, loss 0.33823, acc 0.875, prec 0.0345364, recall 0.791726
2017-12-10T13:47:13.906998: step 1538, loss 0.42911, acc 0.859375, prec 0.0346168, recall 0.792171
2017-12-10T13:47:14.093432: step 1539, loss 0.630892, acc 0.890625, prec 0.0346693, recall 0.792466
2017-12-10T13:47:14.281993: step 1540, loss 1.74149, acc 0.859375, prec 0.0346907, recall 0.792051
2017-12-10T13:47:14.468385: step 1541, loss 0.539455, acc 0.9375, prec 0.0347164, recall 0.792199
2017-12-10T13:47:14.660944: step 1542, loss 0.197931, acc 0.9375, prec 0.0347421, recall 0.792346
2017-12-10T13:47:14.846335: step 1543, loss 0.485752, acc 0.734375, prec 0.0347837, recall 0.79264
2017-12-10T13:47:15.033287: step 1544, loss 0.363043, acc 0.890625, prec 0.0348061, recall 0.792786
2017-12-10T13:47:15.223444: step 1545, loss 0.427462, acc 0.890625, prec 0.0347985, recall 0.792786
2017-12-10T13:47:15.412707: step 1546, loss 0.549077, acc 0.84375, prec 0.0347877, recall 0.792786
2017-12-10T13:47:15.601254: step 1547, loss 2.39609, acc 0.859375, prec 0.034809, recall 0.792373
2017-12-10T13:47:15.792457: step 1548, loss 0.804024, acc 0.75, prec 0.0348217, recall 0.792519
2017-12-10T13:47:15.980810: step 1549, loss 0.581323, acc 0.796875, prec 0.0348077, recall 0.792519
2017-12-10T13:47:16.174553: step 1550, loss 0.307439, acc 0.890625, prec 0.0348001, recall 0.792519
2017-12-10T13:47:16.364222: step 1551, loss 0.665468, acc 0.78125, prec 0.0348149, recall 0.792666
2017-12-10T13:47:16.551511: step 1552, loss 0.470909, acc 0.828125, prec 0.0348628, recall 0.792958
2017-12-10T13:47:16.741686: step 1553, loss 0.600422, acc 0.78125, prec 0.0348776, recall 0.793103
2017-12-10T13:47:16.927023: step 1554, loss 0.578008, acc 0.8125, prec 0.0348647, recall 0.793103
2017-12-10T13:47:17.117663: step 1555, loss 0.316001, acc 0.875, prec 0.034856, recall 0.793103
2017-12-10T13:47:17.307503: step 1556, loss 0.29596, acc 0.875, prec 0.0348772, recall 0.793249
2017-12-10T13:47:17.491476: step 1557, loss 0.430449, acc 0.890625, prec 0.0349294, recall 0.793539
2017-12-10T13:47:17.682294: step 1558, loss 0.298664, acc 0.875, prec 0.0349506, recall 0.793684
2017-12-10T13:47:17.871982: step 1559, loss 0.431976, acc 0.875, prec 0.0350314, recall 0.794118
2017-12-10T13:47:18.059290: step 1560, loss 0.482092, acc 0.875, prec 0.0350525, recall 0.794262
2017-12-10T13:47:18.248239: step 1561, loss 0.426492, acc 0.875, prec 0.0350736, recall 0.794406
2017-12-10T13:47:18.437822: step 1562, loss 0.194339, acc 0.9375, prec 0.0350693, recall 0.794406
2017-12-10T13:47:18.623524: step 1563, loss 0.722606, acc 0.90625, prec 0.0351224, recall 0.794693
2017-12-10T13:47:18.816381: step 1564, loss 0.282662, acc 0.890625, prec 0.0351148, recall 0.794693
2017-12-10T13:47:19.005503: step 1565, loss 1.7479, acc 0.90625, prec 0.0351094, recall 0.794138
2017-12-10T13:47:19.199954: step 1566, loss 0.260122, acc 0.9375, prec 0.035105, recall 0.794138
2017-12-10T13:47:19.384737: step 1567, loss 0.303234, acc 0.875, prec 0.0350964, recall 0.794138
2017-12-10T13:47:19.576112: step 1568, loss 3.11586, acc 0.9375, prec 0.0350931, recall 0.793584
2017-12-10T13:47:19.767142: step 1569, loss 0.21709, acc 0.9375, prec 0.0351186, recall 0.793728
2017-12-10T13:47:19.956852: step 1570, loss 0.538476, acc 0.859375, prec 0.0352278, recall 0.794302
2017-12-10T13:47:20.143999: step 1571, loss 0.337945, acc 0.90625, prec 0.035251, recall 0.794444
2017-12-10T13:47:20.335606: step 1572, loss 0.444311, acc 0.875, prec 0.0352423, recall 0.794444
2017-12-10T13:47:20.526947: step 1573, loss 0.664465, acc 0.78125, prec 0.0352865, recall 0.79473
2017-12-10T13:47:20.715411: step 1574, loss 0.427403, acc 0.875, prec 0.0352778, recall 0.79473
2017-12-10T13:47:20.898636: step 1575, loss 0.505573, acc 0.78125, prec 0.0352923, recall 0.794872
2017-12-10T13:47:21.087831: step 1576, loss 0.391542, acc 0.84375, prec 0.0353111, recall 0.795014
2017-12-10T13:47:21.277890: step 1577, loss 0.326034, acc 0.90625, prec 0.0353046, recall 0.795014
2017-12-10T13:47:21.462516: step 1578, loss 0.468962, acc 0.84375, prec 0.0352938, recall 0.795014
2017-12-10T13:47:21.650633: step 1579, loss 0.88842, acc 0.890625, prec 0.0353158, recall 0.795156
2017-12-10T13:47:21.842105: step 1580, loss 0.673541, acc 0.78125, prec 0.0353006, recall 0.795156
2017-12-10T13:47:22.028127: step 1581, loss 0.246867, acc 0.890625, prec 0.0353227, recall 0.795297
2017-12-10T13:47:22.217660: step 1582, loss 1.2696, acc 0.90625, prec 0.035405, recall 0.795721
2017-12-10T13:47:22.405717: step 1583, loss 0.486652, acc 0.890625, prec 0.035427, recall 0.795862
2017-12-10T13:47:22.590395: step 1584, loss 0.265143, acc 0.890625, prec 0.0354194, recall 0.795862
2017-12-10T13:47:22.774393: step 1585, loss 0.309022, acc 0.859375, prec 0.0354096, recall 0.795862
2017-12-10T13:47:22.960151: step 1586, loss 0.330406, acc 0.875, prec 0.0354009, recall 0.795862
2017-12-10T13:47:23.150582: step 1587, loss 3.56354, acc 0.859375, prec 0.0353923, recall 0.795314
2017-12-10T13:47:23.345418: step 1588, loss 0.366407, acc 0.796875, prec 0.0353782, recall 0.795314
2017-12-10T13:47:23.536868: step 1589, loss 1.37274, acc 0.890625, prec 0.0354001, recall 0.795455
2017-12-10T13:47:23.723677: step 1590, loss 0.625226, acc 0.765625, prec 0.0354134, recall 0.795595
2017-12-10T13:47:23.914501: step 1591, loss 0.306779, acc 0.828125, prec 0.0354015, recall 0.795595
2017-12-10T13:47:24.101953: step 1592, loss 0.52274, acc 0.84375, prec 0.0353906, recall 0.795595
2017-12-10T13:47:24.295020: step 1593, loss 0.686006, acc 0.859375, prec 0.035499, recall 0.796156
2017-12-10T13:47:24.484864: step 1594, loss 0.953771, acc 0.875, prec 0.0355493, recall 0.796436
2017-12-10T13:47:24.673212: step 1595, loss 0.73403, acc 0.765625, prec 0.0355625, recall 0.796575
2017-12-10T13:47:24.859696: step 1596, loss 0.517323, acc 0.859375, prec 0.0356117, recall 0.796854
2017-12-10T13:47:25.046766: step 1597, loss 0.396102, acc 0.890625, prec 0.035604, recall 0.796854
2017-12-10T13:47:25.231963: step 1598, loss 1.12634, acc 0.71875, prec 0.0355845, recall 0.796854
2017-12-10T13:47:25.417968: step 1599, loss 0.708724, acc 0.75, prec 0.0355671, recall 0.796854
2017-12-10T13:47:25.603391: step 1600, loss 1.19678, acc 0.859375, prec 0.035675, recall 0.797408
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_0/1512931325/checkpoints/model-1600

2017-12-10T13:47:26.955530: step 1601, loss 1.53176, acc 0.84375, prec 0.0356936, recall 0.797546
2017-12-10T13:47:27.148173: step 1602, loss 0.565167, acc 0.8125, prec 0.0357099, recall 0.797684
2017-12-10T13:47:27.331820: step 1603, loss 0.496222, acc 0.8125, prec 0.0357263, recall 0.797822
2017-12-10T13:47:27.522302: step 1604, loss 0.608067, acc 0.84375, prec 0.0357448, recall 0.797959
2017-12-10T13:47:27.708258: step 1605, loss 0.324464, acc 0.921875, prec 0.0357687, recall 0.798097
2017-12-10T13:47:27.895213: step 1606, loss 0.513393, acc 0.828125, prec 0.0357861, recall 0.798234
2017-12-10T13:47:28.079948: step 1607, loss 0.461538, acc 0.8125, prec 0.0358024, recall 0.798371
2017-12-10T13:47:28.272525: step 1608, loss 0.466365, acc 0.8125, prec 0.035848, recall 0.798644
2017-12-10T13:47:28.462680: step 1609, loss 0.793384, acc 0.765625, prec 0.0358316, recall 0.798644
2017-12-10T13:47:28.644653: step 1610, loss 0.496166, acc 0.859375, prec 0.0358511, recall 0.798781
2017-12-10T13:47:28.836847: step 1611, loss 0.292844, acc 0.890625, prec 0.0358728, recall 0.798917
2017-12-10T13:47:29.035026: step 1612, loss 0.454312, acc 0.8125, prec 0.0358597, recall 0.798917
2017-12-10T13:47:29.220494: step 1613, loss 0.416823, acc 0.8125, prec 0.0358467, recall 0.798917
2017-12-10T13:47:29.409808: step 1614, loss 2.99958, acc 0.765625, prec 0.0358607, recall 0.798513
2017-12-10T13:47:29.601536: step 1615, loss 6.01244, acc 0.90625, prec 0.0358845, recall 0.798109
2017-12-10T13:47:29.792433: step 1616, loss 0.45979, acc 0.84375, prec 0.0358736, recall 0.798109
2017-12-10T13:47:29.982627: step 1617, loss 0.440073, acc 0.828125, prec 0.0358617, recall 0.798109
2017-12-10T13:47:30.168086: step 1618, loss 0.542002, acc 0.90625, prec 0.0358844, recall 0.798246
2017-12-10T13:47:30.357058: step 1619, loss 0.608426, acc 0.75, prec 0.035867, recall 0.798246
2017-12-10T13:47:30.546862: step 1620, loss 0.388666, acc 0.859375, prec 0.0359156, recall 0.798518
2017-12-10T13:47:30.735948: step 1621, loss 0.586246, acc 0.796875, prec 0.0359015, recall 0.798518
2017-12-10T13:47:30.921117: step 1622, loss 1.06435, acc 0.828125, prec 0.0359479, recall 0.798789
2017-12-10T13:47:31.116607: step 1623, loss 0.423409, acc 0.859375, prec 0.0359673, recall 0.798924
2017-12-10T13:47:31.309379: step 1624, loss 0.367751, acc 0.875, prec 0.0359586, recall 0.798924
2017-12-10T13:47:31.500502: step 1625, loss 0.628705, acc 0.75, prec 0.0359412, recall 0.798924
2017-12-10T13:47:31.688511: step 1626, loss 0.546079, acc 0.796875, prec 0.0359562, recall 0.799059
2017-12-10T13:47:31.873128: step 1627, loss 1.6093, acc 0.78125, prec 0.0359993, recall 0.799329
2017-12-10T13:47:32.061739: step 1628, loss 0.840094, acc 0.84375, prec 0.0360467, recall 0.799598
2017-12-10T13:47:32.250538: step 1629, loss 1.02465, acc 0.9375, prec 0.0361005, recall 0.799866
2017-12-10T13:47:32.444646: step 1630, loss 0.461836, acc 0.828125, prec 0.0360885, recall 0.799866
2017-12-10T13:47:32.634607: step 1631, loss 0.308477, acc 0.84375, prec 0.0360776, recall 0.799866
2017-12-10T13:47:32.822263: step 1632, loss 0.339927, acc 0.828125, prec 0.0361529, recall 0.800267
2017-12-10T13:47:33.015924: step 1633, loss 0.495115, acc 0.796875, prec 0.0361678, recall 0.800401
2017-12-10T13:47:33.201305: step 1634, loss 0.278835, acc 0.828125, prec 0.0361558, recall 0.800401
2017-12-10T13:47:33.390240: step 1635, loss 0.370536, acc 0.859375, prec 0.0362041, recall 0.800667
2017-12-10T13:47:33.574731: step 1636, loss 0.437436, acc 0.859375, prec 0.0361943, recall 0.800667
2017-12-10T13:47:33.761989: step 1637, loss 0.585704, acc 0.828125, prec 0.0361823, recall 0.800667
2017-12-10T13:47:33.950700: step 1638, loss 0.361686, acc 0.890625, prec 0.0362037, recall 0.800799
2017-12-10T13:47:34.135403: step 1639, loss 0.522139, acc 0.796875, prec 0.0362186, recall 0.800932
2017-12-10T13:47:34.323951: step 1640, loss 0.289633, acc 0.859375, prec 0.0362088, recall 0.800932
2017-12-10T13:47:34.506976: step 1641, loss 0.283831, acc 0.859375, prec 0.036199, recall 0.800932
2017-12-10T13:47:34.689092: step 1642, loss 0.246071, acc 0.890625, prec 0.0362203, recall 0.801065
2017-12-10T13:47:34.881341: step 1643, loss 0.323806, acc 0.921875, prec 0.0362439, recall 0.801197
2017-12-10T13:47:35.072240: step 1644, loss 0.413095, acc 0.84375, prec 0.0362909, recall 0.801461
2017-12-10T13:47:35.262662: step 1645, loss 0.380668, acc 0.90625, prec 0.0363134, recall 0.801593
2017-12-10T13:47:35.450401: step 1646, loss 0.286242, acc 0.9375, prec 0.036309, recall 0.801593
2017-12-10T13:47:35.641441: step 1647, loss 10.1767, acc 0.9375, prec 0.0363636, recall 0.801324
2017-12-10T13:47:35.832833: step 1648, loss 0.310155, acc 0.96875, prec 0.0363904, recall 0.801456
2017-12-10T13:47:36.024613: step 1649, loss 0.217583, acc 0.921875, prec 0.0363849, recall 0.801456
2017-12-10T13:47:36.220063: step 1650, loss 0.230341, acc 0.90625, prec 0.0363784, recall 0.801456
2017-12-10T13:47:36.411904: step 1651, loss 0.284902, acc 0.875, prec 0.0363696, recall 0.801456
2017-12-10T13:47:36.605568: step 1652, loss 0.19121, acc 0.9375, prec 0.0363942, recall 0.801587
2017-12-10T13:47:36.793983: step 1653, loss 0.195985, acc 0.9375, prec 0.0363898, recall 0.801587
2017-12-10T13:47:36.983411: step 1654, loss 0.304732, acc 0.921875, prec 0.0364422, recall 0.801849
2017-12-10T13:47:37.174641: step 1655, loss 0.280444, acc 0.875, prec 0.0364335, recall 0.801849
2017-12-10T13:47:37.359507: step 1656, loss 0.137652, acc 0.9375, prec 0.0364291, recall 0.801849
2017-12-10T13:47:37.554740: step 1657, loss 0.128832, acc 0.9375, prec 0.0364247, recall 0.801849
2017-12-10T13:47:37.742520: step 1658, loss 0.315817, acc 0.90625, prec 0.0364471, recall 0.80198
2017-12-10T13:47:37.929035: step 1659, loss 0.267557, acc 0.921875, prec 0.0364416, recall 0.80198
2017-12-10T13:47:38.119559: step 1660, loss 0.597623, acc 0.96875, prec 0.0364683, recall 0.802111
2017-12-10T13:47:38.309451: step 1661, loss 0.199565, acc 0.921875, prec 0.0364629, recall 0.802111
2017-12-10T13:47:38.496744: step 1662, loss 6.46189, acc 0.921875, prec 0.0364874, recall 0.801713
2017-12-10T13:47:38.689283: step 1663, loss 0.335062, acc 0.953125, prec 0.0365419, recall 0.801974
2017-12-10T13:47:38.881365: step 1664, loss 0.172356, acc 0.9375, prec 0.0365664, recall 0.802104
2017-12-10T13:47:39.068484: step 1665, loss 0.19338, acc 0.921875, prec 0.0365898, recall 0.802234
2017-12-10T13:47:39.251785: step 1666, loss 0.707926, acc 0.859375, prec 0.0366087, recall 0.802364
2017-12-10T13:47:39.441744: step 1667, loss 0.151296, acc 0.96875, prec 0.0366643, recall 0.802623
2017-12-10T13:47:39.632274: step 1668, loss 0.291941, acc 0.859375, prec 0.0366544, recall 0.802623
2017-12-10T13:47:39.821827: step 1669, loss 0.412678, acc 0.875, prec 0.0366745, recall 0.802752
2017-12-10T13:47:40.008763: step 1670, loss 2.39457, acc 0.734375, prec 0.0366569, recall 0.802227
2017-12-10T13:47:40.198404: step 1671, loss 0.430422, acc 0.90625, prec 0.0366791, recall 0.802356
2017-12-10T13:47:40.381690: step 1672, loss 0.838427, acc 0.765625, prec 0.0366915, recall 0.802485
2017-12-10T13:47:40.567107: step 1673, loss 0.607234, acc 0.75, prec 0.0366739, recall 0.802485
2017-12-10T13:47:40.755821: step 1674, loss 0.821332, acc 0.890625, prec 0.0366951, recall 0.802614
2017-12-10T13:47:40.940979: step 1675, loss 0.581976, acc 0.765625, prec 0.0367074, recall 0.802743
2017-12-10T13:47:41.129994: step 1676, loss 0.625784, acc 0.796875, prec 0.0366931, recall 0.802743
2017-12-10T13:47:41.317550: step 1677, loss 0.605333, acc 0.78125, prec 0.0366778, recall 0.802743
2017-12-10T13:47:41.507673: step 1678, loss 0.346608, acc 0.875, prec 0.0366978, recall 0.802872
2017-12-10T13:47:41.696326: step 1679, loss 0.968964, acc 0.75, prec 0.036709, recall 0.803001
2017-12-10T13:47:41.885677: step 1680, loss 0.873392, acc 0.71875, prec 0.0367467, recall 0.803257
2017-12-10T13:47:42.072392: step 1681, loss 0.571906, acc 0.8125, prec 0.0367623, recall 0.803385
2017-12-10T13:47:42.260821: step 1682, loss 0.803446, acc 0.78125, prec 0.0368043, recall 0.803641
2017-12-10T13:47:42.448613: step 1683, loss 0.509849, acc 0.796875, prec 0.0368188, recall 0.803769
2017-12-10T13:47:42.635657: step 1684, loss 0.46506, acc 0.859375, prec 0.0368089, recall 0.803769
2017-12-10T13:47:42.821987: step 1685, loss 1.28045, acc 0.859375, prec 0.0368564, recall 0.804023
2017-12-10T13:47:43.009128: step 1686, loss 0.30844, acc 0.875, prec 0.0368762, recall 0.80415
2017-12-10T13:47:43.195611: step 1687, loss 0.780284, acc 0.859375, prec 0.0369236, recall 0.804404
2017-12-10T13:47:43.388660: step 1688, loss 6.34914, acc 0.8125, prec 0.0369116, recall 0.803883
2017-12-10T13:47:43.582133: step 1689, loss 0.523652, acc 0.828125, prec 0.0369281, recall 0.80401
2017-12-10T13:47:43.772645: step 1690, loss 0.372173, acc 0.796875, prec 0.0369138, recall 0.80401
2017-12-10T13:47:43.971465: step 1691, loss 0.418045, acc 0.828125, prec 0.036959, recall 0.804264
2017-12-10T13:47:44.159446: step 1692, loss 0.517704, acc 0.859375, prec 0.0369491, recall 0.804264
2017-12-10T13:47:44.348794: step 1693, loss 0.94824, acc 0.8125, prec 0.0369359, recall 0.804264
2017-12-10T13:47:44.537811: step 1694, loss 5.58426, acc 0.796875, prec 0.0369799, recall 0.803997
2017-12-10T13:47:44.724439: step 1695, loss 0.658004, acc 0.828125, prec 0.0370535, recall 0.804376
2017-12-10T13:47:44.910796: step 1696, loss 0.686046, acc 0.765625, prec 0.037037, recall 0.804376
2017-12-10T13:47:45.098708: step 1697, loss 0.467981, acc 0.8125, prec 0.0371094, recall 0.804753
2017-12-10T13:47:45.289892: step 1698, loss 1.07994, acc 0.65625, prec 0.0370853, recall 0.804753
2017-12-10T13:47:45.473304: step 1699, loss 0.775547, acc 0.78125, prec 0.0370984, recall 0.804878
2017-12-10T13:47:45.661748: step 1700, loss 0.557663, acc 0.84375, prec 0.0370874, recall 0.804878
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_0/1512931325/checkpoints/model-1700

2017-12-10T13:47:46.973552: step 1701, loss 0.510509, acc 0.84375, prec 0.0371334, recall 0.805128
2017-12-10T13:47:47.159942: step 1702, loss 1.09805, acc 0.671875, prec 0.0371388, recall 0.805253
2017-12-10T13:47:47.350610: step 1703, loss 1.01005, acc 0.859375, prec 0.0371858, recall 0.805502
2017-12-10T13:47:47.538175: step 1704, loss 1.06851, acc 0.796875, prec 0.0372, recall 0.805627
2017-12-10T13:47:47.724537: step 1705, loss 0.752203, acc 0.71875, prec 0.0372086, recall 0.805751
2017-12-10T13:47:47.912880: step 1706, loss 1.18654, acc 0.6875, prec 0.0371867, recall 0.805751
2017-12-10T13:47:48.099215: step 1707, loss 0.756943, acc 0.78125, prec 0.0371713, recall 0.805751
2017-12-10T13:47:48.284896: step 1708, loss 0.694202, acc 0.8125, prec 0.0371866, recall 0.805875
2017-12-10T13:47:48.473424: step 1709, loss 0.865149, acc 0.875, prec 0.0372345, recall 0.806122
2017-12-10T13:47:48.663992: step 1710, loss 0.462175, acc 0.875, prec 0.0372541, recall 0.806246
2017-12-10T13:47:48.852533: step 1711, loss 0.655387, acc 0.78125, prec 0.0372671, recall 0.806369
2017-12-10T13:47:49.040388: step 1712, loss 0.459212, acc 0.90625, prec 0.0372888, recall 0.806493
2017-12-10T13:47:49.231184: step 1713, loss 0.552749, acc 0.84375, prec 0.0373062, recall 0.806616
2017-12-10T13:47:49.422383: step 1714, loss 0.218634, acc 0.921875, prec 0.0373007, recall 0.806616
2017-12-10T13:47:49.611466: step 1715, loss 0.474692, acc 0.875, prec 0.0373202, recall 0.806739
2017-12-10T13:47:49.799628: step 1716, loss 0.210299, acc 0.9375, prec 0.0373158, recall 0.806739
2017-12-10T13:47:49.986504: step 1717, loss 0.335071, acc 0.890625, prec 0.0373648, recall 0.806984
2017-12-10T13:47:50.173255: step 1718, loss 0.221065, acc 0.890625, prec 0.0373571, recall 0.806984
2017-12-10T13:47:50.362571: step 1719, loss 0.489756, acc 0.9375, prec 0.037381, recall 0.807107
2017-12-10T13:47:50.554037: step 1720, loss 0.497859, acc 0.875, prec 0.0374005, recall 0.807229
2017-12-10T13:47:50.744028: step 1721, loss 0.517227, acc 0.859375, prec 0.0373906, recall 0.807229
2017-12-10T13:47:50.931831: step 1722, loss 5.11591, acc 0.875, prec 0.0374112, recall 0.80684
2017-12-10T13:47:51.125372: step 1723, loss 0.0863019, acc 0.96875, prec 0.0374372, recall 0.806962
2017-12-10T13:47:51.313625: step 1724, loss 0.36748, acc 0.921875, prec 0.03746, recall 0.807084
2017-12-10T13:47:51.507679: step 1725, loss 0.228289, acc 0.9375, prec 0.0374556, recall 0.807084
2017-12-10T13:47:51.695894: step 1726, loss 0.361122, acc 0.828125, prec 0.0374435, recall 0.807084
2017-12-10T13:47:51.883643: step 1727, loss 0.558573, acc 0.828125, prec 0.0374314, recall 0.807084
2017-12-10T13:47:52.074406: step 1728, loss 0.355823, acc 0.84375, prec 0.0374205, recall 0.807084
2017-12-10T13:47:52.260468: step 1729, loss 0.184179, acc 0.921875, prec 0.0374714, recall 0.807328
2017-12-10T13:47:52.445206: step 1730, loss 1.89548, acc 0.796875, prec 0.0374864, recall 0.80694
2017-12-10T13:47:52.636430: step 1731, loss 0.465291, acc 0.875, prec 0.0374777, recall 0.80694
2017-12-10T13:47:52.823586: step 1732, loss 0.373332, acc 0.875, prec 0.0374971, recall 0.807062
2017-12-10T13:47:53.007588: step 1733, loss 0.443853, acc 0.859375, prec 0.0375154, recall 0.807183
2017-12-10T13:47:53.196029: step 1734, loss 0.439783, acc 0.875, prec 0.0375066, recall 0.807183
2017-12-10T13:47:53.386868: step 1735, loss 1.59867, acc 0.75, prec 0.0375172, recall 0.807305
2017-12-10T13:47:53.575553: step 1736, loss 0.278052, acc 0.9375, prec 0.0375691, recall 0.807547
2017-12-10T13:47:53.765482: step 1737, loss 0.525063, acc 0.921875, prec 0.0375918, recall 0.807668
2017-12-10T13:47:53.953989: step 1738, loss 0.442406, acc 0.84375, prec 0.0376089, recall 0.807789
2017-12-10T13:47:54.141819: step 1739, loss 0.85609, acc 0.90625, prec 0.0376586, recall 0.80803
2017-12-10T13:47:54.330641: step 1740, loss 0.205549, acc 0.953125, prec 0.0376553, recall 0.80803
2017-12-10T13:47:54.519026: step 1741, loss 0.33155, acc 0.921875, prec 0.0376498, recall 0.80803
2017-12-10T13:47:54.709842: step 1742, loss 0.231554, acc 0.90625, prec 0.0376995, recall 0.808271
2017-12-10T13:47:54.895510: step 1743, loss 3.94836, acc 0.890625, prec 0.037721, recall 0.807885
2017-12-10T13:47:55.085540: step 1744, loss 0.465649, acc 0.859375, prec 0.0377392, recall 0.808005
2017-12-10T13:47:55.277728: step 1745, loss 0.622422, acc 0.8125, prec 0.0377821, recall 0.808245
2017-12-10T13:47:55.469333: step 1746, loss 0.774728, acc 0.78125, prec 0.0377667, recall 0.808245
2017-12-10T13:47:55.661193: step 1747, loss 0.917202, acc 0.796875, prec 0.0377804, recall 0.808365
2017-12-10T13:47:55.845938: step 1748, loss 0.559076, acc 0.8125, prec 0.0377953, recall 0.808484
2017-12-10T13:47:56.039700: step 1749, loss 0.710718, acc 0.859375, prec 0.0378134, recall 0.808603
2017-12-10T13:47:56.228998: step 1750, loss 0.871449, acc 0.765625, prec 0.0378249, recall 0.808723
2017-12-10T13:47:56.417712: step 1751, loss 1.33082, acc 0.765625, prec 0.0378364, recall 0.808842
2017-12-10T13:47:56.608648: step 1752, loss 0.438813, acc 0.890625, prec 0.0378287, recall 0.808842
2017-12-10T13:47:56.795091: step 1753, loss 0.904262, acc 0.703125, prec 0.0378078, recall 0.808842
2017-12-10T13:47:56.982059: step 1754, loss 0.421324, acc 0.84375, prec 0.0378248, recall 0.808961
2017-12-10T13:47:57.171535: step 1755, loss 0.882433, acc 0.75, prec 0.0378072, recall 0.808961
2017-12-10T13:47:57.359082: step 1756, loss 0.808563, acc 0.75, prec 0.0377896, recall 0.808961
2017-12-10T13:47:57.547548: step 1757, loss 0.627235, acc 0.765625, prec 0.037829, recall 0.809198
2017-12-10T13:47:57.734628: step 1758, loss 0.639091, acc 0.8125, prec 0.0378159, recall 0.809198
2017-12-10T13:47:57.920420: step 1759, loss 0.525138, acc 0.90625, prec 0.0378931, recall 0.809553
2017-12-10T13:47:58.109814: step 1760, loss 0.476633, acc 0.875, prec 0.0379401, recall 0.809789
2017-12-10T13:47:58.297713: step 1761, loss 0.601005, acc 0.890625, prec 0.0379604, recall 0.809907
2017-12-10T13:47:58.487155: step 1762, loss 0.491921, acc 0.90625, prec 0.0380096, recall 0.810142
2017-12-10T13:47:58.678144: step 1763, loss 0.262557, acc 0.859375, prec 0.0379997, recall 0.810142
2017-12-10T13:47:58.874360: step 1764, loss 0.559467, acc 0.875, prec 0.0379908, recall 0.810142
2017-12-10T13:47:59.060942: step 1765, loss 0.526375, acc 0.8125, prec 0.0380055, recall 0.81026
2017-12-10T13:47:59.246001: step 1766, loss 0.278501, acc 0.875, prec 0.0380246, recall 0.810377
2017-12-10T13:47:59.437090: step 1767, loss 0.315024, acc 0.90625, prec 0.038018, recall 0.810377
2017-12-10T13:47:59.626988: step 1768, loss 0.25979, acc 0.921875, prec 0.0380125, recall 0.810377
2017-12-10T13:47:59.814808: step 1769, loss 0.56894, acc 0.828125, prec 0.0380003, recall 0.810377
2017-12-10T13:48:00.008803: step 1770, loss 0.817825, acc 0.90625, prec 0.0380216, recall 0.810494
2017-12-10T13:48:00.195461: step 1771, loss 0.347597, acc 0.890625, prec 0.0380417, recall 0.810611
2017-12-10T13:48:00.384313: step 1772, loss 0.274535, acc 0.984375, prec 0.0380685, recall 0.810727
2017-12-10T13:48:00.573125: step 1773, loss 0.423301, acc 0.859375, prec 0.0380586, recall 0.810727
2017-12-10T13:48:00.761791: step 1774, loss 0.231936, acc 0.953125, prec 0.0380831, recall 0.810844
2017-12-10T13:48:00.954306: step 1775, loss 0.154937, acc 0.953125, prec 0.0381076, recall 0.810961
2017-12-10T13:48:01.150470: step 1776, loss 3.00197, acc 0.828125, prec 0.0381244, recall 0.810578
2017-12-10T13:48:01.345764: step 1777, loss 3.30932, acc 0.984375, prec 0.0381523, recall 0.810197
2017-12-10T13:48:01.541344: step 1778, loss 0.0752211, acc 0.96875, prec 0.0382057, recall 0.810429
2017-12-10T13:48:01.728837: step 1779, loss 0.322743, acc 0.921875, prec 0.038228, recall 0.810546
2017-12-10T13:48:01.921167: step 1780, loss 0.197327, acc 0.9375, prec 0.0382236, recall 0.810546
2017-12-10T13:48:02.113589: step 1781, loss 0.329359, acc 0.875, prec 0.0382981, recall 0.810894
2017-12-10T13:48:02.303516: step 1782, loss 0.278071, acc 0.9375, prec 0.0383493, recall 0.811125
2017-12-10T13:48:02.492926: step 1783, loss 0.260861, acc 0.921875, prec 0.0383437, recall 0.811125
2017-12-10T13:48:02.684185: step 1784, loss 0.482723, acc 0.84375, prec 0.0383604, recall 0.81124
2017-12-10T13:48:02.869257: step 1785, loss 0.40831, acc 0.84375, prec 0.0383494, recall 0.81124
2017-12-10T13:48:03.053042: step 1786, loss 0.61735, acc 0.828125, prec 0.0383372, recall 0.81124
2017-12-10T13:48:03.239591: step 1787, loss 0.514093, acc 0.828125, prec 0.038325, recall 0.81124
2017-12-10T13:48:03.425514: step 1788, loss 0.450903, acc 0.84375, prec 0.0383417, recall 0.811355
2017-12-10T13:48:03.614601: step 1789, loss 0.328394, acc 0.875, prec 0.0383329, recall 0.811355
2017-12-10T13:48:03.800542: step 1790, loss 0.389975, acc 0.9375, prec 0.0383284, recall 0.811355
2017-12-10T13:48:03.986346: step 1791, loss 1.37392, acc 0.796875, prec 0.0383695, recall 0.811585
2017-12-10T13:48:04.175818: step 1792, loss 0.458491, acc 0.84375, prec 0.0383585, recall 0.811585
2017-12-10T13:48:04.363468: step 1793, loss 0.379915, acc 0.859375, prec 0.0383485, recall 0.811585
2017-12-10T13:48:04.546956: step 1794, loss 0.353128, acc 0.875, prec 0.0383397, recall 0.811585
2017-12-10T13:48:04.736036: step 1795, loss 0.311237, acc 0.921875, prec 0.0383895, recall 0.811815
2017-12-10T13:48:04.925522: step 1796, loss 0.623932, acc 0.796875, prec 0.0383752, recall 0.811815
2017-12-10T13:48:05.110523: step 1797, loss 0.391755, acc 0.890625, prec 0.0383951, recall 0.811929
2017-12-10T13:48:05.303538: step 1798, loss 0.619063, acc 0.8125, prec 0.0383819, recall 0.811929
2017-12-10T13:48:05.494303: step 1799, loss 0.763497, acc 0.8125, prec 0.0383963, recall 0.812044
2017-12-10T13:48:05.686472: step 1800, loss 0.380394, acc 0.90625, prec 0.0384173, recall 0.812158
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_0/1512931325/checkpoints/model-1800

2017-12-10T13:48:06.960481: step 1801, loss 0.567118, acc 0.828125, prec 0.0384052, recall 0.812158
2017-12-10T13:48:07.151751: step 1802, loss 0.388995, acc 0.890625, prec 0.0383974, recall 0.812158
2017-12-10T13:48:07.342308: step 1803, loss 2.24566, acc 0.90625, prec 0.0384472, recall 0.811893
2017-12-10T13:48:07.530205: step 1804, loss 0.250556, acc 0.90625, prec 0.0384405, recall 0.811893
2017-12-10T13:48:07.721347: step 1805, loss 0.20253, acc 0.9375, prec 0.0384361, recall 0.811893
2017-12-10T13:48:07.911307: step 1806, loss 0.51406, acc 0.859375, prec 0.0384538, recall 0.812007
2017-12-10T13:48:08.098899: step 1807, loss 0.723209, acc 0.859375, prec 0.0384991, recall 0.812235
2017-12-10T13:48:08.285965: step 1808, loss 0.390474, acc 0.890625, prec 0.0384913, recall 0.812235
2017-12-10T13:48:08.480022: step 1809, loss 1.18778, acc 0.9375, prec 0.0385697, recall 0.812576
2017-12-10T13:48:08.666992: step 1810, loss 0.295249, acc 0.90625, prec 0.0385907, recall 0.812689
2017-12-10T13:48:08.854070: step 1811, loss 0.275523, acc 0.875, prec 0.0386094, recall 0.812802
2017-12-10T13:48:09.038941: step 1812, loss 1.66553, acc 0.875, prec 0.0386016, recall 0.812311
2017-12-10T13:48:09.230847: step 1813, loss 3.14931, acc 0.890625, prec 0.0386225, recall 0.811935
2017-12-10T13:48:09.424959: step 1814, loss 0.459321, acc 0.859375, prec 0.0386401, recall 0.812048
2017-12-10T13:48:09.609368: step 1815, loss 0.49095, acc 0.828125, prec 0.038628, recall 0.812048
2017-12-10T13:48:09.795031: step 1816, loss 1.103, acc 0.734375, prec 0.0386367, recall 0.812161
2017-12-10T13:48:09.978573: step 1817, loss 0.760321, acc 0.78125, prec 0.0386212, recall 0.812161
2017-12-10T13:48:10.165109: step 1818, loss 0.648161, acc 0.796875, prec 0.0386894, recall 0.8125
2017-12-10T13:48:10.351637: step 1819, loss 0.515489, acc 0.8125, prec 0.0387311, recall 0.812725
2017-12-10T13:48:10.539135: step 1820, loss 0.491976, acc 0.8125, prec 0.0387178, recall 0.812725
2017-12-10T13:48:10.731112: step 1821, loss 0.732753, acc 0.8125, prec 0.0387869, recall 0.813062
2017-12-10T13:48:10.923005: step 1822, loss 0.601889, acc 0.828125, prec 0.0387748, recall 0.813062
2017-12-10T13:48:11.109695: step 1823, loss 0.509318, acc 0.828125, prec 0.03879, recall 0.813174
2017-12-10T13:48:11.298553: step 1824, loss 0.888728, acc 0.75, prec 0.0387997, recall 0.813285
2017-12-10T13:48:11.486785: step 1825, loss 0.287142, acc 0.828125, prec 0.0387876, recall 0.813285
2017-12-10T13:48:11.671601: step 1826, loss 0.565701, acc 0.859375, prec 0.0387776, recall 0.813285
2017-12-10T13:48:11.857302: step 1827, loss 0.282149, acc 0.9375, prec 0.0388006, recall 0.813397
2017-12-10T13:48:12.039899: step 1828, loss 0.270577, acc 0.90625, prec 0.0388214, recall 0.813509
2017-12-10T13:48:12.224247: step 1829, loss 0.520419, acc 0.859375, prec 0.0388114, recall 0.813509
2017-12-10T13:48:12.412566: step 1830, loss 0.42273, acc 0.84375, prec 0.0388004, recall 0.813509
2017-12-10T13:48:12.600270: step 1831, loss 0.423967, acc 0.890625, prec 0.03882, recall 0.81362
2017-12-10T13:48:12.789634: step 1832, loss 0.266662, acc 0.890625, prec 0.0388123, recall 0.81362
2017-12-10T13:48:12.979008: step 1833, loss 0.617023, acc 0.859375, prec 0.0388571, recall 0.813842
2017-12-10T13:48:13.164798: step 1834, loss 4.10655, acc 0.859375, prec 0.038903, recall 0.813579
2017-12-10T13:48:13.358418: step 1835, loss 0.725242, acc 0.828125, prec 0.0388908, recall 0.813579
2017-12-10T13:48:13.545414: step 1836, loss 0.780785, acc 0.828125, prec 0.038906, recall 0.81369
2017-12-10T13:48:13.733370: step 1837, loss 0.293647, acc 0.90625, prec 0.0389267, recall 0.813801
2017-12-10T13:48:13.926050: step 1838, loss 0.262561, acc 0.9375, prec 0.0389496, recall 0.813912
2017-12-10T13:48:14.114915: step 1839, loss 1.05175, acc 0.90625, prec 0.0390523, recall 0.814354
2017-12-10T13:48:14.303885: step 1840, loss 0.437492, acc 0.84375, prec 0.0390412, recall 0.814354
2017-12-10T13:48:14.491459: step 1841, loss 0.612575, acc 0.875, prec 0.0391142, recall 0.814683
2017-12-10T13:48:14.680305: step 1842, loss 0.428501, acc 0.875, prec 0.0391327, recall 0.814793
2017-12-10T13:48:14.870650: step 1843, loss 0.357041, acc 0.890625, prec 0.0392068, recall 0.815121
2017-12-10T13:48:15.055294: step 1844, loss 0.470586, acc 0.84375, prec 0.0392229, recall 0.81523
2017-12-10T13:48:15.243395: step 1845, loss 0.939522, acc 0.890625, prec 0.0392697, recall 0.815448
2017-12-10T13:48:15.431960: step 1846, loss 1.21173, acc 0.90625, prec 0.0393175, recall 0.815665
2017-12-10T13:48:15.619660: step 1847, loss 0.324517, acc 0.875, prec 0.0393086, recall 0.815665
2017-12-10T13:48:15.805784: step 1848, loss 0.393949, acc 0.890625, prec 0.0393281, recall 0.815774
2017-12-10T13:48:15.992556: step 1849, loss 0.424127, acc 0.796875, prec 0.0393136, recall 0.815774
2017-12-10T13:48:16.178621: step 1850, loss 0.18094, acc 0.90625, prec 0.0393614, recall 0.815991
2017-12-10T13:48:16.371467: step 1851, loss 0.492468, acc 0.9375, prec 0.0394386, recall 0.816315
2017-12-10T13:48:16.559352: step 1852, loss 0.648174, acc 0.796875, prec 0.0394241, recall 0.816315
2017-12-10T13:48:16.752696: step 1853, loss 0.386986, acc 0.875, prec 0.0394151, recall 0.816315
2017-12-10T13:48:16.938861: step 1854, loss 0.346022, acc 0.875, prec 0.0394334, recall 0.816422
2017-12-10T13:48:17.129680: step 1855, loss 2.14415, acc 0.875, prec 0.0394256, recall 0.815944
2017-12-10T13:48:17.322723: step 1856, loss 0.594938, acc 0.84375, prec 0.0394416, recall 0.816052
2017-12-10T13:48:17.513319: step 1857, loss 0.394216, acc 0.84375, prec 0.0394577, recall 0.816159
2017-12-10T13:48:17.699173: step 1858, loss 0.861751, acc 0.796875, prec 0.0394975, recall 0.816374
2017-12-10T13:48:17.891280: step 1859, loss 0.439996, acc 0.875, prec 0.0394886, recall 0.816374
2017-12-10T13:48:18.079605: step 1860, loss 0.794269, acc 0.765625, prec 0.0394718, recall 0.816374
2017-12-10T13:48:18.268496: step 1861, loss 0.545196, acc 0.796875, prec 0.0394845, recall 0.816482
2017-12-10T13:48:18.454994: step 1862, loss 0.263645, acc 0.90625, prec 0.0394778, recall 0.816482
2017-12-10T13:48:18.644503: step 1863, loss 0.481976, acc 0.8125, prec 0.0394644, recall 0.816482
2017-12-10T13:48:18.830259: step 1864, loss 0.333872, acc 0.890625, prec 0.0394566, recall 0.816482
2017-12-10T13:48:19.017994: step 1865, loss 0.609788, acc 0.859375, prec 0.0394466, recall 0.816482
2017-12-10T13:48:19.204773: step 1866, loss 0.510162, acc 0.859375, prec 0.0394365, recall 0.816482
2017-12-10T13:48:19.389725: step 1867, loss 0.59444, acc 0.796875, prec 0.0394492, recall 0.816589
2017-12-10T13:48:19.582009: step 1868, loss 0.175101, acc 0.90625, prec 0.0394425, recall 0.816589
2017-12-10T13:48:19.770182: step 1869, loss 0.381207, acc 0.875, prec 0.0394336, recall 0.816589
2017-12-10T13:48:19.958013: step 1870, loss 3.5371, acc 0.84375, prec 0.0394507, recall 0.816219
2017-12-10T13:48:20.147347: step 1871, loss 0.193914, acc 0.953125, prec 0.0394473, recall 0.816219
2017-12-10T13:48:20.331598: step 1872, loss 0.194559, acc 0.953125, prec 0.039444, recall 0.816219
2017-12-10T13:48:20.519915: step 1873, loss 0.30053, acc 0.875, prec 0.0394622, recall 0.816327
2017-12-10T13:48:20.709200: step 1874, loss 0.305442, acc 0.921875, prec 0.0394837, recall 0.816434
2017-12-10T13:48:20.899210: step 1875, loss 0.105956, acc 0.953125, prec 0.0394804, recall 0.816434
2017-12-10T13:48:21.087786: step 1876, loss 1.23554, acc 0.921875, prec 0.0394759, recall 0.815958
2017-12-10T13:48:21.278225: step 1877, loss 0.556729, acc 0.890625, prec 0.0394681, recall 0.815958
2017-12-10T13:48:21.467569: step 1878, loss 0.848892, acc 0.828125, prec 0.0394829, recall 0.816065
2017-12-10T13:48:21.653406: step 1879, loss 0.408266, acc 0.9375, prec 0.0395055, recall 0.816172
2017-12-10T13:48:21.843610: step 1880, loss 0.29123, acc 0.90625, prec 0.0395259, recall 0.816279
2017-12-10T13:48:22.035322: step 1881, loss 0.134743, acc 0.9375, prec 0.0395215, recall 0.816279
2017-12-10T13:48:22.221600: step 1882, loss 0.473955, acc 0.875, prec 0.0395396, recall 0.816386
2017-12-10T13:48:22.409146: step 1883, loss 2.20201, acc 0.90625, prec 0.0395611, recall 0.816019
2017-12-10T13:48:22.601850: step 1884, loss 0.110499, acc 0.96875, prec 0.0395588, recall 0.816019
2017-12-10T13:48:22.789935: step 1885, loss 1.73867, acc 0.890625, prec 0.0396321, recall 0.816338
2017-12-10T13:48:22.981472: step 1886, loss 0.35182, acc 0.875, prec 0.0396772, recall 0.816551
2017-12-10T13:48:23.168978: step 1887, loss 0.826353, acc 0.796875, prec 0.0396897, recall 0.816657
2017-12-10T13:48:23.364101: step 1888, loss 0.268639, acc 0.90625, prec 0.03971, recall 0.816763
2017-12-10T13:48:23.547325: step 1889, loss 0.666062, acc 0.796875, prec 0.0397494, recall 0.816975
2017-12-10T13:48:23.734954: step 1890, loss 0.60805, acc 0.828125, prec 0.0397371, recall 0.816975
2017-12-10T13:48:23.920458: step 1891, loss 0.963349, acc 0.75, prec 0.0397463, recall 0.81708
2017-12-10T13:48:24.111381: step 1892, loss 0.759082, acc 0.8125, prec 0.0397329, recall 0.81708
2017-12-10T13:48:24.298275: step 1893, loss 0.660218, acc 0.796875, prec 0.0397184, recall 0.81708
2017-12-10T13:48:24.484453: step 1894, loss 0.70738, acc 0.78125, prec 0.0397297, recall 0.817186
2017-12-10T13:48:24.674736: step 1895, loss 0.710675, acc 0.765625, prec 0.039713, recall 0.817186
2017-12-10T13:48:24.860561: step 1896, loss 0.46853, acc 0.84375, prec 0.0397288, recall 0.817291
2017-12-10T13:48:25.048791: step 1897, loss 0.705307, acc 0.78125, prec 0.0397132, recall 0.817291
2017-12-10T13:48:25.236317: step 1898, loss 0.806637, acc 0.796875, prec 0.0397256, recall 0.817396
2017-12-10T13:48:25.423477: step 1899, loss 0.471231, acc 0.890625, prec 0.0397716, recall 0.817606
2017-12-10T13:48:25.610358: step 1900, loss 0.344058, acc 0.875, prec 0.0397627, recall 0.817606
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_0/1512931325/checkpoints/model-1900

2017-12-10T13:48:26.787028: step 1901, loss 0.166073, acc 0.953125, prec 0.0397594, recall 0.817606
2017-12-10T13:48:26.976963: step 1902, loss 0.541694, acc 0.859375, prec 0.0397494, recall 0.817606
2017-12-10T13:48:27.164093: step 1903, loss 0.296554, acc 0.890625, prec 0.0397416, recall 0.817606
2017-12-10T13:48:27.354398: step 1904, loss 5.4573, acc 0.90625, prec 0.0397629, recall 0.817241
2017-12-10T13:48:27.545925: step 1905, loss 0.194148, acc 0.953125, prec 0.0397864, recall 0.817346
2017-12-10T13:48:27.733490: step 1906, loss 0.279286, acc 0.890625, prec 0.0397786, recall 0.817346
2017-12-10T13:48:27.920450: step 1907, loss 0.437358, acc 0.84375, prec 0.0397675, recall 0.817346
2017-12-10T13:48:28.106352: step 1908, loss 2.7444, acc 0.859375, prec 0.0398123, recall 0.817087
2017-12-10T13:48:28.295932: step 1909, loss 0.580682, acc 0.828125, prec 0.0398268, recall 0.817192
2017-12-10T13:48:28.482845: step 1910, loss 0.384099, acc 0.84375, prec 0.0398157, recall 0.817192
2017-12-10T13:48:28.668627: step 1911, loss 0.737265, acc 0.828125, prec 0.0398303, recall 0.817297
2017-12-10T13:48:28.861343: step 1912, loss 0.397136, acc 0.890625, prec 0.0398225, recall 0.817297
2017-12-10T13:48:29.047035: step 1913, loss 0.329601, acc 0.9375, prec 0.0398717, recall 0.817506
2017-12-10T13:48:29.239445: step 1914, loss 0.394375, acc 0.828125, prec 0.0398594, recall 0.817506
2017-12-10T13:48:29.427792: step 1915, loss 0.476551, acc 0.875, prec 0.0398505, recall 0.817506
2017-12-10T13:48:29.616213: step 1916, loss 0.815037, acc 0.875, prec 0.0399219, recall 0.817818
2017-12-10T13:48:29.805945: step 1917, loss 0.478598, acc 0.828125, prec 0.0399365, recall 0.817922
2017-12-10T13:48:29.999112: step 1918, loss 0.77774, acc 0.765625, prec 0.0399465, recall 0.818026
2017-12-10T13:48:30.183526: step 1919, loss 0.601736, acc 0.859375, prec 0.03999, recall 0.818234
2017-12-10T13:48:30.375264: step 1920, loss 0.626713, acc 0.890625, prec 0.0400891, recall 0.818647
2017-12-10T13:48:30.565789: step 1921, loss 0.45927, acc 0.859375, prec 0.040079, recall 0.818647
2017-12-10T13:48:30.750837: step 1922, loss 0.277467, acc 0.875, prec 0.0400701, recall 0.818647
2017-12-10T13:48:30.937355: step 1923, loss 0.611714, acc 0.75, prec 0.0400523, recall 0.818647
2017-12-10T13:48:31.125346: step 1924, loss 0.238387, acc 0.90625, prec 0.0400723, recall 0.81875
2017-12-10T13:48:31.313902: step 1925, loss 0.614597, acc 0.796875, prec 0.0400845, recall 0.818853
2017-12-10T13:48:31.504461: step 1926, loss 0.674085, acc 0.875, prec 0.0401289, recall 0.819058
2017-12-10T13:48:31.695080: step 1927, loss 0.810097, acc 0.90625, prec 0.0401756, recall 0.819263
2017-12-10T13:48:31.886560: step 1928, loss 1.01812, acc 0.875, prec 0.0401933, recall 0.819366
2017-12-10T13:48:32.075865: step 1929, loss 0.405459, acc 0.875, prec 0.0402111, recall 0.819468
2017-12-10T13:48:32.261935: step 1930, loss 0.35358, acc 0.90625, prec 0.0402044, recall 0.819468
2017-12-10T13:48:32.449741: step 1931, loss 10.1238, acc 0.875, prec 0.0402254, recall 0.818182
2017-12-10T13:48:32.641208: step 1932, loss 0.536419, acc 0.84375, prec 0.0402143, recall 0.818182
2017-12-10T13:48:32.829348: step 1933, loss 0.706983, acc 0.828125, prec 0.040202, recall 0.818182
2017-12-10T13:48:33.015630: step 1934, loss 0.501905, acc 0.8125, prec 0.0401886, recall 0.818182
2017-12-10T13:48:33.204343: step 1935, loss 1.04447, acc 0.75, prec 0.0401708, recall 0.818182
2017-12-10T13:48:33.391907: step 1936, loss 1.3032, acc 0.59375, prec 0.0401684, recall 0.818284
2017-12-10T13:48:33.579713: step 1937, loss 1.3245, acc 0.671875, prec 0.0401982, recall 0.818489
2017-12-10T13:48:33.765094: step 1938, loss 1.46041, acc 0.578125, prec 0.0401682, recall 0.818489
2017-12-10T13:48:33.949908: step 1939, loss 1.1148, acc 0.65625, prec 0.0401703, recall 0.818592
2017-12-10T13:48:34.133764: step 1940, loss 1.14453, acc 0.734375, prec 0.040231, recall 0.818898
2017-12-10T13:48:34.322029: step 1941, loss 1.49188, acc 0.625, prec 0.0402308, recall 0.818999
2017-12-10T13:48:34.507817: step 1942, loss 1.19726, acc 0.71875, prec 0.0402109, recall 0.818999
2017-12-10T13:48:34.692778: step 1943, loss 1.29902, acc 0.640625, prec 0.0401853, recall 0.818999
2017-12-10T13:48:34.881294: step 1944, loss 1.68577, acc 0.703125, prec 0.0402437, recall 0.819304
2017-12-10T13:48:35.067440: step 1945, loss 1.43313, acc 0.625, prec 0.0402435, recall 0.819405
2017-12-10T13:48:35.252637: step 1946, loss 1.42425, acc 0.671875, prec 0.0402202, recall 0.819405
2017-12-10T13:48:35.439860: step 1947, loss 0.996115, acc 0.6875, prec 0.0401981, recall 0.819405
2017-12-10T13:48:35.626673: step 1948, loss 1.04179, acc 0.75, prec 0.0401804, recall 0.819405
2017-12-10T13:48:35.815024: step 1949, loss 1.93918, acc 0.734375, prec 0.040188, recall 0.819507
2017-12-10T13:48:36.003093: step 1950, loss 0.574507, acc 0.796875, prec 0.0402264, recall 0.819709
2017-12-10T13:48:36.193000: step 1951, loss 0.318235, acc 0.859375, prec 0.0402165, recall 0.819709
2017-12-10T13:48:36.378386: step 1952, loss 0.544443, acc 0.859375, prec 0.0402065, recall 0.819709
2017-12-10T13:48:36.566233: step 1953, loss 0.777992, acc 0.734375, prec 0.0401878, recall 0.819709
2017-12-10T13:48:36.753945: step 1954, loss 0.529812, acc 0.859375, prec 0.0401778, recall 0.819709
2017-12-10T13:48:36.940598: step 1955, loss 0.391419, acc 0.875, prec 0.0401954, recall 0.81981
2017-12-10T13:48:37.125938: step 1956, loss 0.366261, acc 0.8125, prec 0.0401821, recall 0.81981
2017-12-10T13:48:37.314730: step 1957, loss 2.30873, acc 0.875, prec 0.0401744, recall 0.819351
2017-12-10T13:48:37.505204: step 1958, loss 0.564495, acc 0.796875, prec 0.0402127, recall 0.819553
2017-12-10T13:48:37.693337: step 1959, loss 0.231975, acc 0.921875, prec 0.0402335, recall 0.819654
2017-12-10T13:48:37.884851: step 1960, loss 0.188549, acc 0.90625, prec 0.0402532, recall 0.819754
2017-12-10T13:48:38.072519: step 1961, loss 0.419224, acc 0.875, prec 0.0402707, recall 0.819855
2017-12-10T13:48:38.262411: step 1962, loss 0.661932, acc 0.875, prec 0.0403144, recall 0.820056
2017-12-10T13:48:38.452324: step 1963, loss 0.145114, acc 0.9375, prec 0.04031, recall 0.820056
2017-12-10T13:48:38.640609: step 1964, loss 0.156178, acc 0.953125, prec 0.040333, recall 0.820156
2017-12-10T13:48:38.826971: step 1965, loss 0.523999, acc 0.828125, prec 0.0403208, recall 0.820156
2017-12-10T13:48:39.016742: step 1966, loss 0.716358, acc 0.9375, prec 0.0403689, recall 0.820356
2017-12-10T13:48:39.203547: step 1967, loss 0.343153, acc 0.90625, prec 0.0403623, recall 0.820356
2017-12-10T13:48:39.393542: step 1968, loss 0.0621058, acc 0.96875, prec 0.0403601, recall 0.820356
2017-12-10T13:48:39.581099: step 1969, loss 0.235103, acc 0.890625, prec 0.0404049, recall 0.820556
2017-12-10T13:48:39.771134: step 1970, loss 0.407386, acc 0.859375, prec 0.0403949, recall 0.820556
2017-12-10T13:48:39.958661: step 1971, loss 0.4864, acc 0.875, prec 0.0403861, recall 0.820556
2017-12-10T13:48:40.144064: step 1972, loss 7.35417, acc 0.859375, prec 0.0404035, recall 0.8202
2017-12-10T13:48:40.334701: step 1973, loss 2.54536, acc 0.875, prec 0.0403958, recall 0.819745
2017-12-10T13:48:40.522312: step 1974, loss 0.155717, acc 0.9375, prec 0.0403913, recall 0.819745
2017-12-10T13:48:40.707518: step 1975, loss 0.244389, acc 0.90625, prec 0.0404109, recall 0.819845
2017-12-10T13:48:40.891963: step 1976, loss 0.882952, acc 0.859375, prec 0.0404272, recall 0.819945
2017-12-10T13:48:41.076806: step 1977, loss 0.254479, acc 0.90625, prec 0.0404468, recall 0.820044
2017-12-10T13:48:41.263996: step 1978, loss 0.343897, acc 0.890625, prec 0.0404391, recall 0.820044
2017-12-10T13:48:41.452699: step 1979, loss 3.21169, acc 0.84375, prec 0.0404291, recall 0.819591
2017-12-10T13:48:41.641329: step 1980, loss 0.419988, acc 0.875, prec 0.0404203, recall 0.819591
2017-12-10T13:48:41.827353: step 1981, loss 0.78781, acc 0.75, prec 0.0404027, recall 0.819591
2017-12-10T13:48:42.011582: step 1982, loss 0.914001, acc 0.828125, prec 0.0404429, recall 0.81979
2017-12-10T13:48:42.198626: step 1983, loss 0.576283, acc 0.75, prec 0.0404514, recall 0.819889
2017-12-10T13:48:42.385685: step 1984, loss 0.890705, acc 0.71875, prec 0.0404316, recall 0.819889
2017-12-10T13:48:42.573011: step 1985, loss 0.934324, acc 0.765625, prec 0.0404412, recall 0.819989
2017-12-10T13:48:42.758727: step 1986, loss 2.37916, acc 0.71875, prec 0.0404225, recall 0.819536
2017-12-10T13:48:42.949066: step 1987, loss 1.01426, acc 0.71875, prec 0.0404549, recall 0.819735
2017-12-10T13:48:43.117201: step 1988, loss 1.1715, acc 0.72549, prec 0.0404917, recall 0.819934
2017-12-10T13:48:43.315038: step 1989, loss 1.34711, acc 0.640625, prec 0.0405707, recall 0.82033
2017-12-10T13:48:43.499848: step 1990, loss 1.04896, acc 0.734375, prec 0.040604, recall 0.820527
2017-12-10T13:48:43.690240: step 1991, loss 1.11503, acc 0.71875, prec 0.0406102, recall 0.820625
2017-12-10T13:48:43.884376: step 1992, loss 0.736364, acc 0.796875, prec 0.040648, recall 0.820822
2017-12-10T13:48:44.072334: step 1993, loss 0.812922, acc 0.734375, prec 0.0406292, recall 0.820822
2017-12-10T13:48:44.261526: step 1994, loss 0.95151, acc 0.734375, prec 0.0406365, recall 0.82092
2017-12-10T13:48:44.450739: step 1995, loss 0.557939, acc 0.765625, prec 0.040646, recall 0.821018
2017-12-10T13:48:44.635706: step 1996, loss 0.889982, acc 0.71875, prec 0.0406522, recall 0.821116
2017-12-10T13:48:44.823703: step 1997, loss 0.873578, acc 0.78125, prec 0.0406368, recall 0.821116
2017-12-10T13:48:45.010154: step 1998, loss 0.569134, acc 0.8125, prec 0.0406755, recall 0.821311
2017-12-10T13:48:45.199871: step 1999, loss 0.358394, acc 0.859375, prec 0.0407175, recall 0.821507
2017-12-10T13:48:45.387833: step 2000, loss 0.506591, acc 0.921875, prec 0.0407639, recall 0.821701
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_0/1512931325/checkpoints/model-2000

2017-12-10T13:48:46.569485: step 2001, loss 0.429718, acc 0.84375, prec 0.0407529, recall 0.821701
2017-12-10T13:48:46.755063: step 2002, loss 0.490139, acc 0.921875, prec 0.0407992, recall 0.821895
2017-12-10T13:48:46.940896: step 2003, loss 1.87124, acc 0.96875, prec 0.0408241, recall 0.821545
2017-12-10T13:48:47.132775: step 2004, loss 0.367033, acc 0.890625, prec 0.0408163, recall 0.821545
2017-12-10T13:48:47.320499: step 2005, loss 0.172076, acc 0.96875, prec 0.0408141, recall 0.821545
2017-12-10T13:48:47.504753: step 2006, loss 0.374972, acc 0.890625, prec 0.0408842, recall 0.821836
2017-12-10T13:48:47.691538: step 2007, loss 0.444852, acc 0.875, prec 0.0409271, recall 0.822029
2017-12-10T13:48:47.878781: step 2008, loss 0.466761, acc 0.890625, prec 0.0409453, recall 0.822126
2017-12-10T13:48:48.067826: step 2009, loss 1.30234, acc 0.890625, prec 0.0410153, recall 0.822415
2017-12-10T13:48:48.257536: step 2010, loss 0.281733, acc 0.96875, prec 0.0410648, recall 0.822607
2017-12-10T13:48:48.449808: step 2011, loss 0.363426, acc 0.890625, prec 0.0411088, recall 0.822798
2017-12-10T13:48:48.638220: step 2012, loss 0.183783, acc 0.9375, prec 0.0411044, recall 0.822798
2017-12-10T13:48:48.828180: step 2013, loss 0.402269, acc 0.875, prec 0.0410955, recall 0.822798
2017-12-10T13:48:49.018443: step 2014, loss 0.356772, acc 0.875, prec 0.0410867, recall 0.822798
2017-12-10T13:48:49.210214: step 2015, loss 0.112648, acc 0.921875, prec 0.0410811, recall 0.822798
2017-12-10T13:48:49.396229: step 2016, loss 0.173062, acc 0.921875, prec 0.0411014, recall 0.822894
2017-12-10T13:48:49.583344: step 2017, loss 0.226861, acc 0.921875, prec 0.0410959, recall 0.822894
2017-12-10T13:48:49.773238: step 2018, loss 0.238638, acc 0.90625, prec 0.0410892, recall 0.822894
2017-12-10T13:48:49.962213: step 2019, loss 0.403928, acc 0.828125, prec 0.0410771, recall 0.822894
2017-12-10T13:48:50.151116: step 2020, loss 2.77497, acc 0.9375, prec 0.0411771, recall 0.822833
2017-12-10T13:48:50.344754: step 2021, loss 0.400935, acc 0.875, prec 0.0411682, recall 0.822833
2017-12-10T13:48:50.528589: step 2022, loss 0.405819, acc 0.953125, prec 0.0412166, recall 0.823023
2017-12-10T13:48:50.719772: step 2023, loss 0.0887444, acc 0.953125, prec 0.0412132, recall 0.823023
2017-12-10T13:48:50.909257: step 2024, loss 3.9762, acc 0.859375, prec 0.0412302, recall 0.822676
2017-12-10T13:48:51.101597: step 2025, loss 0.365289, acc 0.90625, prec 0.0412235, recall 0.822676
2017-12-10T13:48:51.288168: step 2026, loss 0.87617, acc 0.90625, prec 0.0413201, recall 0.823056
2017-12-10T13:48:51.480717: step 2027, loss 0.473113, acc 0.84375, prec 0.0413348, recall 0.823151
2017-12-10T13:48:51.671652: step 2028, loss 5.28964, acc 0.84375, prec 0.0413248, recall 0.82271
2017-12-10T13:48:51.864383: step 2029, loss 0.136354, acc 0.90625, prec 0.0413439, recall 0.822805
2017-12-10T13:48:52.049763: step 2030, loss 0.539584, acc 0.8125, prec 0.0413563, recall 0.8229
2017-12-10T13:48:52.234938: step 2031, loss 0.515804, acc 0.8125, prec 0.0413687, recall 0.822995
2017-12-10T13:48:52.424647: step 2032, loss 0.282539, acc 0.859375, prec 0.0413587, recall 0.822995
2017-12-10T13:48:52.612754: step 2033, loss 0.584398, acc 0.78125, prec 0.0413689, recall 0.823089
2017-12-10T13:48:52.798938: step 2034, loss 0.774297, acc 0.765625, prec 0.041378, recall 0.823184
2017-12-10T13:48:52.985917: step 2035, loss 0.684363, acc 0.734375, prec 0.0413849, recall 0.823278
2017-12-10T13:48:53.173818: step 2036, loss 0.787552, acc 0.734375, prec 0.0414174, recall 0.823467
2017-12-10T13:48:53.359303: step 2037, loss 0.53853, acc 0.875, prec 0.0414085, recall 0.823467
2017-12-10T13:48:53.544224: step 2038, loss 0.834821, acc 0.6875, prec 0.0413863, recall 0.823467
2017-12-10T13:48:53.735181: step 2039, loss 0.776097, acc 0.71875, prec 0.0413664, recall 0.823467
2017-12-10T13:48:53.922700: step 2040, loss 0.940467, acc 0.765625, prec 0.0413498, recall 0.823467
2017-12-10T13:48:54.110393: step 2041, loss 0.584386, acc 0.8125, prec 0.0413621, recall 0.823561
2017-12-10T13:48:54.299466: step 2042, loss 0.364952, acc 0.875, prec 0.0413533, recall 0.823561
2017-12-10T13:48:54.489787: step 2043, loss 0.243655, acc 0.890625, prec 0.0413712, recall 0.823655
2017-12-10T13:48:54.676450: step 2044, loss 0.838103, acc 0.75, prec 0.0413791, recall 0.823749
2017-12-10T13:48:54.864705: step 2045, loss 0.274189, acc 0.890625, prec 0.0413714, recall 0.823749
2017-12-10T13:48:55.052698: step 2046, loss 0.53532, acc 0.8125, prec 0.0413837, recall 0.823842
2017-12-10T13:48:55.241462: step 2047, loss 0.416944, acc 0.875, prec 0.0414517, recall 0.824123
2017-12-10T13:48:55.429556: step 2048, loss 0.127093, acc 0.953125, prec 0.0414484, recall 0.824123
2017-12-10T13:48:55.615011: step 2049, loss 0.295014, acc 0.890625, prec 0.0414407, recall 0.824123
2017-12-10T13:48:55.810582: step 2050, loss 0.586199, acc 0.859375, prec 0.0414563, recall 0.824217
2017-12-10T13:48:55.999690: step 2051, loss 0.201081, acc 0.921875, prec 0.0414764, recall 0.82431
2017-12-10T13:48:56.186598: step 2052, loss 0.550862, acc 0.953125, prec 0.0414986, recall 0.824403
2017-12-10T13:48:56.375869: step 2053, loss 3.42674, acc 0.953125, prec 0.0414964, recall 0.823966
2017-12-10T13:48:56.564112: step 2054, loss 0.204089, acc 0.953125, prec 0.0414931, recall 0.823966
2017-12-10T13:48:56.754535: step 2055, loss 0.372607, acc 0.96875, prec 0.0415421, recall 0.824153
2017-12-10T13:48:56.943752: step 2056, loss 3.20149, acc 0.859375, prec 0.0415343, recall 0.82328
2017-12-10T13:48:57.137198: step 2057, loss 0.244132, acc 0.890625, prec 0.0415521, recall 0.823374
2017-12-10T13:48:57.326583: step 2058, loss 0.526503, acc 0.859375, prec 0.0415677, recall 0.823467
2017-12-10T13:48:57.515934: step 2059, loss 0.286505, acc 0.90625, prec 0.0416122, recall 0.823654
2017-12-10T13:48:57.706996: step 2060, loss 1.65741, acc 0.84375, prec 0.0416778, recall 0.823933
2017-12-10T13:48:57.900074: step 2061, loss 0.413504, acc 0.859375, prec 0.0416678, recall 0.823933
2017-12-10T13:48:58.084023: step 2062, loss 0.650629, acc 0.84375, prec 0.0416567, recall 0.823933
2017-12-10T13:48:58.273391: step 2063, loss 0.949764, acc 0.78125, prec 0.0417177, recall 0.824211
2017-12-10T13:48:58.459468: step 2064, loss 1.41825, acc 0.765625, prec 0.0417521, recall 0.824395
2017-12-10T13:48:58.650904: step 2065, loss 0.382597, acc 0.828125, prec 0.0417909, recall 0.82458
2017-12-10T13:48:58.845029: step 2066, loss 0.472467, acc 0.828125, prec 0.0417787, recall 0.82458
2017-12-10T13:48:59.032289: step 2067, loss 0.660153, acc 0.78125, prec 0.0417886, recall 0.824672
2017-12-10T13:48:59.216947: step 2068, loss 0.483041, acc 0.84375, prec 0.041803, recall 0.824764
2017-12-10T13:48:59.401858: step 2069, loss 0.83715, acc 0.78125, prec 0.0418129, recall 0.824856
2017-12-10T13:48:59.586590: step 2070, loss 0.355468, acc 0.890625, prec 0.0418051, recall 0.824856
2017-12-10T13:48:59.774442: step 2071, loss 0.814162, acc 0.796875, prec 0.0418161, recall 0.824948
2017-12-10T13:48:59.962515: step 2072, loss 0.690689, acc 0.8125, prec 0.0418028, recall 0.824948
2017-12-10T13:49:00.149809: step 2073, loss 0.583721, acc 0.890625, prec 0.0418459, recall 0.825131
2017-12-10T13:49:00.337720: step 2074, loss 0.348545, acc 0.90625, prec 0.0419155, recall 0.825405
2017-12-10T13:49:00.527267: step 2075, loss 0.758551, acc 0.8125, prec 0.0419276, recall 0.825496
2017-12-10T13:49:00.717516: step 2076, loss 0.692634, acc 0.796875, prec 0.0419131, recall 0.825496
2017-12-10T13:49:00.904841: step 2077, loss 0.371831, acc 0.890625, prec 0.0419054, recall 0.825496
2017-12-10T13:49:01.093129: step 2078, loss 0.560183, acc 0.84375, prec 0.0419705, recall 0.825769
2017-12-10T13:49:01.288215: step 2079, loss 0.345568, acc 0.875, prec 0.0420124, recall 0.825951
2017-12-10T13:49:01.479727: step 2080, loss 0.382602, acc 0.90625, prec 0.0420564, recall 0.826132
2017-12-10T13:49:01.669859: step 2081, loss 0.474933, acc 0.90625, prec 0.0420751, recall 0.826223
2017-12-10T13:49:01.858572: step 2082, loss 0.179439, acc 0.90625, prec 0.0420685, recall 0.826223
2017-12-10T13:49:02.046513: step 2083, loss 0.259436, acc 0.921875, prec 0.0421136, recall 0.826403
2017-12-10T13:49:02.230529: step 2084, loss 0.156571, acc 0.9375, prec 0.0421092, recall 0.826403
2017-12-10T13:49:02.417103: step 2085, loss 0.102541, acc 0.96875, prec 0.0421069, recall 0.826403
2017-12-10T13:49:02.607639: step 2086, loss 0.417095, acc 0.921875, prec 0.0421014, recall 0.826403
2017-12-10T13:49:02.792424: step 2087, loss 0.433952, acc 0.921875, prec 0.0421719, recall 0.826674
2017-12-10T13:49:02.981562: step 2088, loss 0.136271, acc 0.953125, prec 0.0421685, recall 0.826674
2017-12-10T13:49:03.168747: step 2089, loss 0.239202, acc 0.953125, prec 0.0421652, recall 0.826674
2017-12-10T13:49:03.359229: step 2090, loss 0.139045, acc 0.96875, prec 0.0421883, recall 0.826764
2017-12-10T13:49:03.546024: step 2091, loss 0.091243, acc 0.96875, prec 0.0421861, recall 0.826764
2017-12-10T13:49:03.733158: step 2092, loss 0.266462, acc 0.9375, prec 0.0422323, recall 0.826943
2017-12-10T13:49:03.922652: step 2093, loss 0.467153, acc 0.921875, prec 0.0422774, recall 0.827122
2017-12-10T13:49:04.111665: step 2094, loss 0.0458126, acc 0.984375, prec 0.0422763, recall 0.827122
2017-12-10T13:49:04.300264: step 2095, loss 1.84922, acc 0.984375, prec 0.0423016, recall 0.826784
2017-12-10T13:49:04.491585: step 2096, loss 0.22051, acc 0.9375, prec 0.0422971, recall 0.826784
2017-12-10T13:49:04.681228: step 2097, loss 9.13852, acc 0.921875, prec 0.042318, recall 0.826446
2017-12-10T13:49:04.871411: step 2098, loss 0.166923, acc 0.984375, prec 0.0423675, recall 0.826625
2017-12-10T13:49:05.061394: step 2099, loss 1.0927, acc 0.96875, prec 0.0423906, recall 0.826715
2017-12-10T13:49:05.249312: step 2100, loss 0.612301, acc 0.9375, prec 0.0424367, recall 0.826893
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_0/1512931325/checkpoints/model-2100

2017-12-10T13:49:06.473576: step 2101, loss 0.569568, acc 0.890625, prec 0.0424795, recall 0.827072
2017-12-10T13:49:06.665783: step 2102, loss 0.450281, acc 0.8125, prec 0.0424913, recall 0.82716
2017-12-10T13:49:06.855921: step 2103, loss 0.224445, acc 0.90625, prec 0.0425099, recall 0.827249
2017-12-10T13:49:07.046235: step 2104, loss 0.689128, acc 0.765625, prec 0.0424931, recall 0.827249
2017-12-10T13:49:07.233785: step 2105, loss 0.428966, acc 0.84375, prec 0.0425324, recall 0.827427
2017-12-10T13:49:07.422722: step 2106, loss 1.05587, acc 0.828125, prec 0.0425453, recall 0.827515
2017-12-10T13:49:07.608266: step 2107, loss 1.23131, acc 0.75, prec 0.0425779, recall 0.827692
2017-12-10T13:49:07.797045: step 2108, loss 0.72653, acc 0.796875, prec 0.0425633, recall 0.827692
2017-12-10T13:49:07.984771: step 2109, loss 0.939797, acc 0.640625, prec 0.0425627, recall 0.827781
2017-12-10T13:49:08.174926: step 2110, loss 1.02485, acc 0.75, prec 0.0425448, recall 0.827781
2017-12-10T13:49:08.366394: step 2111, loss 0.906738, acc 0.75, prec 0.0425269, recall 0.827781
2017-12-10T13:49:08.550380: step 2112, loss 0.563171, acc 0.84375, prec 0.0425157, recall 0.827781
2017-12-10T13:49:08.735890: step 2113, loss 0.599389, acc 0.828125, prec 0.0425034, recall 0.827781
2017-12-10T13:49:08.925063: step 2114, loss 1.01455, acc 0.796875, prec 0.042514, recall 0.827869
2017-12-10T13:49:09.114241: step 2115, loss 1.06939, acc 0.6875, prec 0.0425168, recall 0.827957
2017-12-10T13:49:09.303332: step 2116, loss 0.940407, acc 0.734375, prec 0.042523, recall 0.828045
2017-12-10T13:49:09.489897: step 2117, loss 0.586114, acc 0.828125, prec 0.0425359, recall 0.828133
2017-12-10T13:49:09.677153: step 2118, loss 0.536347, acc 0.875, prec 0.0425269, recall 0.828133
2017-12-10T13:49:09.869692: step 2119, loss 0.394127, acc 0.859375, prec 0.042542, recall 0.828221
2017-12-10T13:49:10.053697: step 2120, loss 0.806555, acc 0.71875, prec 0.0425219, recall 0.828221
2017-12-10T13:49:10.244568: step 2121, loss 0.243606, acc 0.90625, prec 0.0425152, recall 0.828221
2017-12-10T13:49:10.433064: step 2122, loss 0.629105, acc 0.859375, prec 0.0425303, recall 0.828309
2017-12-10T13:49:10.622008: step 2123, loss 0.673816, acc 0.796875, prec 0.0425158, recall 0.828309
2017-12-10T13:49:10.807832: step 2124, loss 1.23539, acc 0.796875, prec 0.0425264, recall 0.828396
2017-12-10T13:49:10.999037: step 2125, loss 0.254175, acc 0.875, prec 0.0425175, recall 0.828396
2017-12-10T13:49:11.189681: step 2126, loss 0.229664, acc 0.890625, prec 0.0425097, recall 0.828396
2017-12-10T13:49:11.382073: step 2127, loss 0.949192, acc 0.9375, prec 0.0425303, recall 0.828484
2017-12-10T13:49:11.572108: step 2128, loss 0.272643, acc 0.9375, prec 0.0425259, recall 0.828484
2017-12-10T13:49:11.774976: step 2129, loss 0.121786, acc 0.9375, prec 0.0425214, recall 0.828484
2017-12-10T13:49:11.961834: step 2130, loss 0.624747, acc 0.953125, prec 0.0425432, recall 0.828571
2017-12-10T13:49:12.152199: step 2131, loss 5.45353, acc 0.921875, prec 0.0425638, recall 0.828237
2017-12-10T13:49:12.339907: step 2132, loss 0.227974, acc 0.890625, prec 0.0425811, recall 0.828324
2017-12-10T13:49:12.529880: step 2133, loss 0.757965, acc 0.9375, prec 0.0426017, recall 0.828411
2017-12-10T13:49:12.721490: step 2134, loss 0.311891, acc 0.875, prec 0.0426429, recall 0.828586
2017-12-10T13:49:12.907077: step 2135, loss 0.595147, acc 0.828125, prec 0.0426807, recall 0.82876
2017-12-10T13:49:13.091066: step 2136, loss 0.545828, acc 0.796875, prec 0.0427163, recall 0.828934
2017-12-10T13:49:13.277770: step 2137, loss 0.396427, acc 0.890625, prec 0.0427084, recall 0.828934
2017-12-10T13:49:13.468775: step 2138, loss 0.507421, acc 0.8125, prec 0.0427451, recall 0.829108
2017-12-10T13:49:13.658358: step 2139, loss 0.600363, acc 0.8125, prec 0.0427317, recall 0.829108
2017-12-10T13:49:13.851803: step 2140, loss 0.422548, acc 0.84375, prec 0.0427205, recall 0.829108
2017-12-10T13:49:14.043153: step 2141, loss 0.49393, acc 0.859375, prec 0.0427105, recall 0.829108
2017-12-10T13:49:14.230014: step 2142, loss 0.564623, acc 0.84375, prec 0.0426993, recall 0.829108
2017-12-10T13:49:14.419052: step 2143, loss 0.31065, acc 0.90625, prec 0.0427176, recall 0.829194
2017-12-10T13:49:14.606433: step 2144, loss 0.519365, acc 0.890625, prec 0.0427098, recall 0.829194
2017-12-10T13:49:14.800376: step 2145, loss 0.500706, acc 0.875, prec 0.0427259, recall 0.829281
2017-12-10T13:49:14.985664: step 2146, loss 0.688985, acc 0.796875, prec 0.0427114, recall 0.829281
2017-12-10T13:49:15.173525: step 2147, loss 0.499342, acc 0.859375, prec 0.0427263, recall 0.829367
2017-12-10T13:49:15.360660: step 2148, loss 0.402567, acc 0.921875, prec 0.0427457, recall 0.829453
2017-12-10T13:49:15.547586: step 2149, loss 0.432858, acc 0.84375, prec 0.0427346, recall 0.829453
2017-12-10T13:49:15.737880: step 2150, loss 0.428634, acc 0.90625, prec 0.0427529, recall 0.82954
2017-12-10T13:49:15.927370: step 2151, loss 0.391951, acc 0.921875, prec 0.0427473, recall 0.82954
2017-12-10T13:49:16.120767: step 2152, loss 1.65135, acc 0.890625, prec 0.0427905, recall 0.829293
2017-12-10T13:49:16.311755: step 2153, loss 0.838466, acc 0.890625, prec 0.0428076, recall 0.829379
2017-12-10T13:49:16.505056: step 2154, loss 0.267824, acc 0.890625, prec 0.0428497, recall 0.829551
2017-12-10T13:49:16.693998: step 2155, loss 0.180152, acc 0.921875, prec 0.0428441, recall 0.829551
2017-12-10T13:49:16.883770: step 2156, loss 0.492198, acc 0.90625, prec 0.0428374, recall 0.829551
2017-12-10T13:49:17.070540: step 2157, loss 0.127001, acc 0.953125, prec 0.0428341, recall 0.829551
2017-12-10T13:49:17.260850: step 2158, loss 0.122868, acc 0.953125, prec 0.0428557, recall 0.829637
2017-12-10T13:49:17.451153: step 2159, loss 0.203541, acc 0.921875, prec 0.0428501, recall 0.829637
2017-12-10T13:49:17.641804: step 2160, loss 0.122363, acc 0.953125, prec 0.0428467, recall 0.829637
2017-12-10T13:49:17.831072: step 2161, loss 0.444384, acc 0.859375, prec 0.0428616, recall 0.829723
2017-12-10T13:49:18.022545: step 2162, loss 0.808372, acc 0.84375, prec 0.0428505, recall 0.829723
2017-12-10T13:49:18.213242: step 2163, loss 0.41583, acc 0.921875, prec 0.0428698, recall 0.829809
2017-12-10T13:49:18.403752: step 2164, loss 0.636031, acc 0.859375, prec 0.0428846, recall 0.829894
2017-12-10T13:49:18.594506: step 2165, loss 0.292067, acc 0.921875, prec 0.0429039, recall 0.82998
2017-12-10T13:49:18.787714: step 2166, loss 0.325813, acc 0.921875, prec 0.0429233, recall 0.830065
2017-12-10T13:49:18.973574: step 2167, loss 0.123937, acc 0.953125, prec 0.0429199, recall 0.830065
2017-12-10T13:49:19.158767: step 2168, loss 0.516444, acc 0.859375, prec 0.0429596, recall 0.830236
2017-12-10T13:49:19.347418: step 2169, loss 0.0391699, acc 0.984375, prec 0.0429585, recall 0.830236
2017-12-10T13:49:19.539148: step 2170, loss 0.196858, acc 0.9375, prec 0.042954, recall 0.830236
2017-12-10T13:49:19.726324: step 2171, loss 0.228227, acc 0.890625, prec 0.0429711, recall 0.830321
2017-12-10T13:49:19.913147: step 2172, loss 0.37062, acc 0.90625, prec 0.043039, recall 0.830576
2017-12-10T13:49:20.100345: step 2173, loss 0.146081, acc 0.953125, prec 0.0430853, recall 0.830746
2017-12-10T13:49:20.293083: step 2174, loss 0.194793, acc 0.9375, prec 0.0431057, recall 0.830831
2017-12-10T13:49:20.482433: step 2175, loss 0.195665, acc 0.9375, prec 0.0431261, recall 0.830915
2017-12-10T13:49:20.671574: step 2176, loss 0.494236, acc 0.890625, prec 0.0431679, recall 0.831084
2017-12-10T13:49:20.863405: step 2177, loss 7.02242, acc 0.875, prec 0.0431601, recall 0.830669
2017-12-10T13:49:21.055186: step 2178, loss 0.17539, acc 0.953125, prec 0.0431815, recall 0.830754
2017-12-10T13:49:21.246964: step 2179, loss 0.348124, acc 0.90625, prec 0.0431748, recall 0.830754
2017-12-10T13:49:21.433219: step 2180, loss 1.0894, acc 0.859375, prec 0.0431895, recall 0.830838
2017-12-10T13:49:21.621571: step 2181, loss 0.0457816, acc 0.984375, prec 0.0431884, recall 0.830838
2017-12-10T13:49:21.807501: step 2182, loss 0.504976, acc 0.859375, prec 0.0431783, recall 0.830838
2017-12-10T13:49:21.993902: step 2183, loss 0.472088, acc 0.84375, prec 0.0431671, recall 0.830838
2017-12-10T13:49:22.181600: step 2184, loss 0.18123, acc 0.9375, prec 0.0431875, recall 0.830923
2017-12-10T13:49:22.371088: step 2185, loss 0.289324, acc 0.921875, prec 0.0432315, recall 0.831091
2017-12-10T13:49:22.556105: step 2186, loss 0.659885, acc 0.8125, prec 0.0432428, recall 0.831175
2017-12-10T13:49:22.743067: step 2187, loss 0.653698, acc 0.828125, prec 0.0432553, recall 0.831259
2017-12-10T13:49:22.930210: step 2188, loss 0.320787, acc 0.875, prec 0.0432463, recall 0.831259
2017-12-10T13:49:23.120906: step 2189, loss 0.497735, acc 0.875, prec 0.0432374, recall 0.831259
2017-12-10T13:49:23.307642: step 2190, loss 0.30583, acc 0.90625, prec 0.0432306, recall 0.831259
2017-12-10T13:49:23.498238: step 2191, loss 0.607474, acc 0.84375, prec 0.043269, recall 0.831427
2017-12-10T13:49:23.686041: step 2192, loss 0.417366, acc 0.890625, prec 0.0432611, recall 0.831427
2017-12-10T13:49:23.879637: step 2193, loss 0.357196, acc 0.875, prec 0.0432522, recall 0.831427
2017-12-10T13:49:24.071778: step 2194, loss 0.271215, acc 0.90625, prec 0.0432702, recall 0.831511
2017-12-10T13:49:24.260011: step 2195, loss 0.247498, acc 0.90625, prec 0.0432883, recall 0.831595
2017-12-10T13:49:24.449611: step 2196, loss 0.336843, acc 0.90625, prec 0.0432815, recall 0.831595
2017-12-10T13:49:24.636619: step 2197, loss 0.287534, acc 0.9375, prec 0.0432771, recall 0.831595
2017-12-10T13:49:24.826743: step 2198, loss 0.252339, acc 0.9375, prec 0.0432726, recall 0.831595
2017-12-10T13:49:25.015575: step 2199, loss 0.357206, acc 0.890625, prec 0.0432895, recall 0.831678
2017-12-10T13:49:25.204166: step 2200, loss 0.375096, acc 0.875, prec 0.0432805, recall 0.831678
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_0/1512931325/checkpoints/model-2200

2017-12-10T13:49:26.424521: step 2201, loss 0.434497, acc 0.859375, prec 0.0432952, recall 0.831762
2017-12-10T13:49:26.610550: step 2202, loss 0.129217, acc 0.96875, prec 0.043293, recall 0.831762
2017-12-10T13:49:26.797915: step 2203, loss 0.155661, acc 0.9375, prec 0.0432885, recall 0.831762
2017-12-10T13:49:26.983962: step 2204, loss 0.236806, acc 0.9375, prec 0.043284, recall 0.831762
2017-12-10T13:49:27.172809: step 2205, loss 1.83883, acc 0.96875, prec 0.0432829, recall 0.831349
2017-12-10T13:49:27.365783: step 2206, loss 0.200598, acc 0.953125, prec 0.0432795, recall 0.831349
2017-12-10T13:49:27.554089: step 2207, loss 0.895953, acc 0.9375, prec 0.0432998, recall 0.831433
2017-12-10T13:49:27.741549: step 2208, loss 0.204438, acc 0.921875, prec 0.0433189, recall 0.831516
2017-12-10T13:49:27.928151: step 2209, loss 0.343095, acc 0.875, prec 0.0433099, recall 0.831516
2017-12-10T13:49:28.117186: step 2210, loss 0.102019, acc 0.96875, prec 0.0433077, recall 0.831516
2017-12-10T13:49:28.304107: step 2211, loss 0.107993, acc 0.953125, prec 0.0433043, recall 0.831516
2017-12-10T13:49:28.495068: step 2212, loss 0.234331, acc 0.921875, prec 0.0432988, recall 0.831516
2017-12-10T13:49:28.685763: step 2213, loss 0.550065, acc 0.890625, prec 0.0433156, recall 0.8316
2017-12-10T13:49:28.887132: step 2214, loss 0.408816, acc 0.859375, prec 0.0433056, recall 0.8316
2017-12-10T13:49:29.073252: step 2215, loss 0.127232, acc 0.96875, prec 0.043328, recall 0.831683
2017-12-10T13:49:29.262654: step 2216, loss 0.0721867, acc 0.96875, prec 0.0433258, recall 0.831683
2017-12-10T13:49:29.452206: step 2217, loss 0.112993, acc 0.953125, prec 0.0433224, recall 0.831683
2017-12-10T13:49:29.640264: step 2218, loss 0.146465, acc 0.96875, prec 0.0433202, recall 0.831683
2017-12-10T13:49:29.829611: step 2219, loss 0.163056, acc 0.953125, prec 0.0433168, recall 0.831683
2017-12-10T13:49:30.021554: step 2220, loss 1.53463, acc 0.90625, prec 0.0433359, recall 0.831355
2017-12-10T13:49:30.214366: step 2221, loss 0.139157, acc 0.921875, prec 0.0433303, recall 0.831355
2017-12-10T13:49:30.401664: step 2222, loss 5.68896, acc 0.921875, prec 0.0433505, recall 0.831028
2017-12-10T13:49:30.591298: step 2223, loss 0.0318002, acc 1, prec 0.0433505, recall 0.831028
2017-12-10T13:49:30.776089: step 2224, loss 0.381071, acc 0.890625, prec 0.0433673, recall 0.831111
2017-12-10T13:49:30.969377: step 2225, loss 1.89436, acc 0.84375, prec 0.0434066, recall 0.830868
2017-12-10T13:49:31.163425: step 2226, loss 0.407036, acc 0.859375, prec 0.0434458, recall 0.831034
2017-12-10T13:49:31.349612: step 2227, loss 0.417791, acc 0.875, prec 0.0434368, recall 0.831034
2017-12-10T13:49:31.539791: step 2228, loss 0.418296, acc 0.90625, prec 0.0434548, recall 0.831118
2017-12-10T13:49:31.729663: step 2229, loss 0.517105, acc 0.890625, prec 0.04357, recall 0.831532
2017-12-10T13:49:31.919966: step 2230, loss 0.794549, acc 0.828125, prec 0.0436069, recall 0.831698
2017-12-10T13:49:32.103438: step 2231, loss 0.491632, acc 0.859375, prec 0.0435968, recall 0.831698
2017-12-10T13:49:32.289105: step 2232, loss 0.508125, acc 0.8125, prec 0.0436079, recall 0.83178
2017-12-10T13:49:32.473480: step 2233, loss 0.748942, acc 0.796875, prec 0.043618, recall 0.831863
2017-12-10T13:49:32.657996: step 2234, loss 0.720295, acc 0.828125, prec 0.0436302, recall 0.831945
2017-12-10T13:49:32.846244: step 2235, loss 0.501022, acc 0.859375, prec 0.0436692, recall 0.83211
2017-12-10T13:49:33.033905: step 2236, loss 0.537613, acc 0.84375, prec 0.0436826, recall 0.832192
2017-12-10T13:49:33.222761: step 2237, loss 0.780409, acc 0.78125, prec 0.0436669, recall 0.832192
2017-12-10T13:49:33.410305: step 2238, loss 1.12183, acc 0.765625, prec 0.0436746, recall 0.832274
2017-12-10T13:49:33.596206: step 2239, loss 0.900126, acc 0.78125, prec 0.043708, recall 0.832438
2017-12-10T13:49:33.786847: step 2240, loss 0.52104, acc 0.828125, prec 0.0436957, recall 0.832438
2017-12-10T13:49:33.973195: step 2241, loss 0.602921, acc 0.84375, prec 0.0436845, recall 0.832438
2017-12-10T13:49:34.162113: step 2242, loss 0.647287, acc 0.796875, prec 0.0436944, recall 0.83252
2017-12-10T13:49:34.346651: step 2243, loss 0.692371, acc 0.84375, prec 0.0437077, recall 0.832601
2017-12-10T13:49:34.533955: step 2244, loss 0.313692, acc 0.875, prec 0.0436988, recall 0.832601
2017-12-10T13:49:34.725870: step 2245, loss 10.07, acc 0.875, prec 0.0437154, recall 0.832277
2017-12-10T13:49:34.917373: step 2246, loss 0.640849, acc 0.828125, prec 0.0437276, recall 0.832359
2017-12-10T13:49:35.105452: step 2247, loss 0.280879, acc 0.875, prec 0.0437431, recall 0.83244
2017-12-10T13:49:35.299185: step 2248, loss 0.72027, acc 0.859375, prec 0.043782, recall 0.832603
2017-12-10T13:49:35.487649: step 2249, loss 0.494924, acc 0.890625, prec 0.0437986, recall 0.832685
2017-12-10T13:49:35.677663: step 2250, loss 0.543838, acc 0.828125, prec 0.0437863, recall 0.832685
2017-12-10T13:49:35.866850: step 2251, loss 0.533932, acc 0.828125, prec 0.043774, recall 0.832685
2017-12-10T13:49:36.056925: step 2252, loss 0.610392, acc 0.890625, prec 0.0437906, recall 0.832766
2017-12-10T13:49:36.246974: step 2253, loss 0.384379, acc 0.890625, prec 0.0438072, recall 0.832847
2017-12-10T13:49:36.434640: step 2254, loss 0.292303, acc 0.921875, prec 0.0438016, recall 0.832847
2017-12-10T13:49:36.623346: step 2255, loss 0.492962, acc 0.8125, prec 0.0438126, recall 0.832929
2017-12-10T13:49:36.810199: step 2256, loss 0.209425, acc 0.90625, prec 0.0438303, recall 0.83301
2017-12-10T13:49:36.996184: step 2257, loss 0.42377, acc 0.859375, prec 0.0438202, recall 0.83301
2017-12-10T13:49:37.186851: step 2258, loss 0.106466, acc 0.9375, prec 0.0438402, recall 0.833091
2017-12-10T13:49:37.376607: step 2259, loss 0.379561, acc 0.875, prec 0.0438312, recall 0.833091
2017-12-10T13:49:37.566524: step 2260, loss 0.382927, acc 0.921875, prec 0.04385, recall 0.833172
2017-12-10T13:49:37.755559: step 2261, loss 3.48921, acc 0.953125, prec 0.0438722, recall 0.832849
2017-12-10T13:49:37.947605: step 2262, loss 0.141271, acc 0.9375, prec 0.0438921, recall 0.83293
2017-12-10T13:49:38.136435: step 2263, loss 0.283731, acc 0.875, prec 0.0439075, recall 0.833011
2017-12-10T13:49:38.327264: step 2264, loss 0.125571, acc 0.984375, prec 0.0439064, recall 0.833011
2017-12-10T13:49:38.512109: step 2265, loss 0.21796, acc 0.921875, prec 0.0439008, recall 0.833011
2017-12-10T13:49:38.702927: step 2266, loss 0.221891, acc 0.90625, prec 0.0438941, recall 0.833011
2017-12-10T13:49:38.892087: step 2267, loss 0.650132, acc 0.859375, prec 0.0439084, recall 0.833091
2017-12-10T13:49:39.079617: step 2268, loss 0.310973, acc 0.875, prec 0.0438995, recall 0.833091
2017-12-10T13:49:39.268478: step 2269, loss 0.267912, acc 0.890625, prec 0.043916, recall 0.833172
2017-12-10T13:49:39.457623: step 2270, loss 0.394266, acc 0.9375, prec 0.0439602, recall 0.833333
2017-12-10T13:49:39.643107: step 2271, loss 0.180669, acc 0.9375, prec 0.0439558, recall 0.833333
2017-12-10T13:49:39.832095: step 2272, loss 0.311188, acc 0.875, prec 0.0439468, recall 0.833333
2017-12-10T13:49:40.018701: step 2273, loss 0.372355, acc 0.90625, prec 0.0440131, recall 0.833575
2017-12-10T13:49:40.212970: step 2274, loss 0.269979, acc 0.921875, prec 0.0440319, recall 0.833655
2017-12-10T13:49:40.401420: step 2275, loss 0.107078, acc 0.953125, prec 0.0440285, recall 0.833655
2017-12-10T13:49:40.588456: step 2276, loss 0.139521, acc 0.921875, prec 0.0440229, recall 0.833655
2017-12-10T13:49:40.777874: step 2277, loss 0.0917775, acc 0.96875, prec 0.0440207, recall 0.833655
2017-12-10T13:49:40.969738: step 2278, loss 0.142199, acc 0.953125, prec 0.0440173, recall 0.833655
2017-12-10T13:49:41.160771: step 2279, loss 2.01858, acc 0.953125, prec 0.0440637, recall 0.833414
2017-12-10T13:49:41.356065: step 2280, loss 1.31014, acc 0.953125, prec 0.044109, recall 0.833574
2017-12-10T13:49:41.548230: step 2281, loss 4.96729, acc 0.90625, prec 0.0441289, recall 0.832853
2017-12-10T13:49:41.737602: step 2282, loss 0.0631939, acc 0.96875, prec 0.0441266, recall 0.832853
2017-12-10T13:49:41.924354: step 2283, loss 0.367494, acc 0.90625, prec 0.0441199, recall 0.832853
2017-12-10T13:49:42.112833: step 2284, loss 0.672851, acc 0.8125, prec 0.0441551, recall 0.833013
2017-12-10T13:49:42.299261: step 2285, loss 0.525746, acc 0.875, prec 0.0441461, recall 0.833013
2017-12-10T13:49:42.484021: step 2286, loss 0.583061, acc 0.78125, prec 0.0441547, recall 0.833094
2017-12-10T13:49:42.667919: step 2287, loss 0.337786, acc 0.890625, prec 0.0441711, recall 0.833174
2017-12-10T13:49:42.853191: step 2288, loss 0.43763, acc 0.828125, prec 0.0442073, recall 0.833333
2017-12-10T13:49:43.041578: step 2289, loss 0.605229, acc 0.859375, prec 0.0441972, recall 0.833333
2017-12-10T13:49:43.232008: step 2290, loss 0.888068, acc 0.71875, prec 0.0442013, recall 0.833413
2017-12-10T13:49:43.419944: step 2291, loss 0.785466, acc 0.78125, prec 0.0442098, recall 0.833493
2017-12-10T13:49:43.605603: step 2292, loss 0.75821, acc 0.765625, prec 0.044193, recall 0.833493
2017-12-10T13:49:43.795831: step 2293, loss 0.728069, acc 0.734375, prec 0.0441982, recall 0.833572
2017-12-10T13:49:43.991332: step 2294, loss 0.788762, acc 0.796875, prec 0.0441836, recall 0.833572
2017-12-10T13:49:44.180228: step 2295, loss 1.11739, acc 0.640625, prec 0.0441821, recall 0.833652
2017-12-10T13:49:44.364748: step 2296, loss 0.873346, acc 0.78125, prec 0.0442148, recall 0.833811
2017-12-10T13:49:44.547772: step 2297, loss 0.502244, acc 0.84375, prec 0.044252, recall 0.833969
2017-12-10T13:49:44.737527: step 2298, loss 0.473639, acc 0.875, prec 0.0442431, recall 0.833969
2017-12-10T13:49:44.923749: step 2299, loss 0.364133, acc 0.890625, prec 0.0442352, recall 0.833969
2017-12-10T13:49:45.110828: step 2300, loss 0.63149, acc 0.78125, prec 0.0442438, recall 0.834049
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_0/1512931325/checkpoints/model-2300

2017-12-10T13:49:46.434184: step 2301, loss 0.491843, acc 0.84375, prec 0.0442326, recall 0.834049
2017-12-10T13:49:46.624217: step 2302, loss 0.428687, acc 0.84375, prec 0.0442455, recall 0.834128
2017-12-10T13:49:46.809895: step 2303, loss 0.569412, acc 0.828125, prec 0.0442574, recall 0.834207
2017-12-10T13:49:46.995996: step 2304, loss 0.369101, acc 0.84375, prec 0.0442462, recall 0.834207
2017-12-10T13:49:47.184196: step 2305, loss 0.350015, acc 0.921875, prec 0.0442648, recall 0.834286
2017-12-10T13:49:47.369954: step 2306, loss 0.428407, acc 0.90625, prec 0.0443064, recall 0.834443
2017-12-10T13:49:47.557243: step 2307, loss 0.123828, acc 0.9375, prec 0.0443019, recall 0.834443
2017-12-10T13:49:47.747846: step 2308, loss 0.361006, acc 0.921875, prec 0.0442963, recall 0.834443
2017-12-10T13:49:47.935182: step 2309, loss 0.632225, acc 0.90625, prec 0.0443137, recall 0.834522
2017-12-10T13:49:48.128072: step 2310, loss 0.0745763, acc 1, prec 0.044362, recall 0.834679
2017-12-10T13:49:48.317586: step 2311, loss 0.611705, acc 0.9375, prec 0.0444299, recall 0.834915
2017-12-10T13:49:48.507006: step 2312, loss 0.202723, acc 0.921875, prec 0.0444243, recall 0.834915
2017-12-10T13:49:48.698932: step 2313, loss 0.161129, acc 0.9375, prec 0.0444439, recall 0.834993
2017-12-10T13:49:48.890092: step 2314, loss 8.54331, acc 0.953125, prec 0.0444416, recall 0.834597
2017-12-10T13:49:49.083241: step 2315, loss 0.19153, acc 0.953125, prec 0.0444624, recall 0.834675
2017-12-10T13:49:49.276066: step 2316, loss 0.297544, acc 0.96875, prec 0.0444843, recall 0.834754
2017-12-10T13:49:49.467846: step 2317, loss 0.327648, acc 0.875, prec 0.0444994, recall 0.834832
2017-12-10T13:49:49.657103: step 2318, loss 0.214613, acc 0.953125, prec 0.0445201, recall 0.83491
2017-12-10T13:49:49.848187: step 2319, loss 0.141283, acc 0.953125, prec 0.0445408, recall 0.834988
2017-12-10T13:49:50.036200: step 2320, loss 0.15348, acc 0.921875, prec 0.0445352, recall 0.834988
2017-12-10T13:49:50.226596: step 2321, loss 0.12381, acc 0.9375, prec 0.0445548, recall 0.835066
2017-12-10T13:49:50.416086: step 2322, loss 0.30658, acc 0.9375, prec 0.0445744, recall 0.835144
2017-12-10T13:49:50.608340: step 2323, loss 0.26342, acc 0.9375, prec 0.044594, recall 0.835222
2017-12-10T13:49:50.795971: step 2324, loss 0.464604, acc 0.90625, prec 0.0446354, recall 0.835377
2017-12-10T13:49:50.984793: step 2325, loss 1.40848, acc 0.90625, prec 0.044678, recall 0.835139
2017-12-10T13:49:51.177364: step 2326, loss 0.185004, acc 0.9375, prec 0.0446975, recall 0.835217
2017-12-10T13:49:51.368063: step 2327, loss 0.489024, acc 0.84375, prec 0.0447103, recall 0.835294
2017-12-10T13:49:51.558784: step 2328, loss 0.104112, acc 0.96875, prec 0.0447081, recall 0.835294
2017-12-10T13:49:51.745362: step 2329, loss 0.378669, acc 0.921875, prec 0.0447024, recall 0.835294
2017-12-10T13:49:51.934973: step 2330, loss 0.421955, acc 0.890625, prec 0.0447186, recall 0.835372
2017-12-10T13:49:52.120690: step 2331, loss 0.744077, acc 0.8125, prec 0.0447772, recall 0.835604
2017-12-10T13:49:52.305343: step 2332, loss 0.187019, acc 0.9375, prec 0.0447727, recall 0.835604
2017-12-10T13:49:52.490060: step 2333, loss 0.229341, acc 0.953125, prec 0.0447934, recall 0.835681
2017-12-10T13:49:52.680217: step 2334, loss 0.52876, acc 0.8125, prec 0.0447799, recall 0.835681
2017-12-10T13:49:52.866514: step 2335, loss 0.383943, acc 0.875, prec 0.0447949, recall 0.835758
2017-12-10T13:49:53.057528: step 2336, loss 0.458368, acc 0.875, prec 0.0447859, recall 0.835758
2017-12-10T13:49:53.242742: step 2337, loss 0.675786, acc 0.875, prec 0.0448009, recall 0.835835
2017-12-10T13:49:53.434387: step 2338, loss 0.42906, acc 0.890625, prec 0.044793, recall 0.835835
2017-12-10T13:49:53.624616: step 2339, loss 0.2073, acc 0.90625, prec 0.0448103, recall 0.835912
2017-12-10T13:49:53.817352: step 2340, loss 0.306802, acc 0.90625, prec 0.0448035, recall 0.835912
2017-12-10T13:49:54.010684: step 2341, loss 0.20753, acc 0.9375, prec 0.044799, recall 0.835912
2017-12-10T13:49:54.200937: step 2342, loss 0.170362, acc 0.90625, prec 0.0447922, recall 0.835912
2017-12-10T13:49:54.386976: step 2343, loss 0.178416, acc 0.953125, prec 0.0447889, recall 0.835912
2017-12-10T13:49:54.577143: step 2344, loss 0.098274, acc 0.96875, prec 0.0448586, recall 0.836142
2017-12-10T13:49:54.768204: step 2345, loss 1.5841, acc 0.96875, prec 0.0448575, recall 0.835751
2017-12-10T13:49:54.960118: step 2346, loss 0.153867, acc 0.953125, prec 0.0448781, recall 0.835828
2017-12-10T13:49:55.152896: step 2347, loss 0.13463, acc 0.90625, prec 0.0448953, recall 0.835905
2017-12-10T13:49:55.340528: step 2348, loss 0.571094, acc 0.953125, prec 0.0449399, recall 0.836058
2017-12-10T13:49:55.531117: step 2349, loss 0.0569788, acc 0.984375, prec 0.0449387, recall 0.836058
2017-12-10T13:49:55.721403: step 2350, loss 0.0924151, acc 0.9375, prec 0.0449342, recall 0.836058
2017-12-10T13:49:55.911447: step 2351, loss 1.71694, acc 0.953125, prec 0.044932, recall 0.835668
2017-12-10T13:49:56.108075: step 2352, loss 0.649231, acc 0.96875, prec 0.0449537, recall 0.835744
2017-12-10T13:49:56.298744: step 2353, loss 0.501497, acc 0.875, prec 0.0449926, recall 0.835897
2017-12-10T13:49:56.487018: step 2354, loss 4.20887, acc 0.9375, prec 0.0449903, recall 0.835119
2017-12-10T13:49:56.674385: step 2355, loss 0.247224, acc 0.953125, prec 0.044987, recall 0.835119
2017-12-10T13:49:56.860705: step 2356, loss 0.226453, acc 0.921875, prec 0.0450532, recall 0.835349
2017-12-10T13:49:57.046931: step 2357, loss 0.920657, acc 0.75, prec 0.0450591, recall 0.835425
2017-12-10T13:49:57.231872: step 2358, loss 0.996048, acc 0.78125, prec 0.0450432, recall 0.835425
2017-12-10T13:49:57.415796: step 2359, loss 0.884675, acc 0.71875, prec 0.0450708, recall 0.835578
2017-12-10T13:49:57.603194: step 2360, loss 0.774902, acc 0.703125, prec 0.0450493, recall 0.835578
2017-12-10T13:49:57.788225: step 2361, loss 0.666764, acc 0.78125, prec 0.0450814, recall 0.835731
2017-12-10T13:49:57.977483: step 2362, loss 1.11747, acc 0.6875, prec 0.0450588, recall 0.835731
2017-12-10T13:49:58.161716: step 2363, loss 1.14603, acc 0.703125, prec 0.0450613, recall 0.835807
2017-12-10T13:49:58.348649: step 2364, loss 1.00228, acc 0.75, prec 0.045091, recall 0.835959
2017-12-10T13:49:58.532886: step 2365, loss 1.49851, acc 0.640625, prec 0.0450889, recall 0.836035
2017-12-10T13:49:58.725066: step 2366, loss 1.16051, acc 0.703125, prec 0.0450914, recall 0.836111
2017-12-10T13:49:58.917612: step 2367, loss 1.3099, acc 0.625, prec 0.0450882, recall 0.836187
2017-12-10T13:49:59.106702: step 2368, loss 0.764481, acc 0.78125, prec 0.0450963, recall 0.836263
2017-12-10T13:49:59.293640: step 2369, loss 0.98706, acc 0.6875, prec 0.0450976, recall 0.836338
2017-12-10T13:49:59.482695: step 2370, loss 0.886397, acc 0.75, prec 0.045151, recall 0.836565
2017-12-10T13:49:59.667593: step 2371, loss 0.661786, acc 0.796875, prec 0.0451364, recall 0.836565
2017-12-10T13:49:59.854810: step 2372, loss 0.502639, acc 0.84375, prec 0.0451489, recall 0.836641
2017-12-10T13:50:00.046500: step 2373, loss 0.592491, acc 0.84375, prec 0.0451615, recall 0.836716
2017-12-10T13:50:00.235463: step 2374, loss 0.32273, acc 0.875, prec 0.0451762, recall 0.836791
2017-12-10T13:50:00.422721: step 2375, loss 0.406516, acc 0.859375, prec 0.0451661, recall 0.836791
2017-12-10T13:50:00.608782: step 2376, loss 0.220298, acc 0.890625, prec 0.045182, recall 0.836866
2017-12-10T13:50:00.797780: step 2377, loss 0.105766, acc 0.96875, prec 0.0452035, recall 0.836941
2017-12-10T13:50:00.986929: step 2378, loss 0.314612, acc 0.890625, prec 0.0451956, recall 0.836941
2017-12-10T13:50:01.178018: step 2379, loss 2.51958, acc 0.890625, prec 0.0451889, recall 0.836556
2017-12-10T13:50:01.372766: step 2380, loss 1.33176, acc 0.9375, prec 0.0452319, recall 0.836707
2017-12-10T13:50:01.565363: step 2381, loss 0.112361, acc 0.96875, prec 0.0452771, recall 0.836857
2017-12-10T13:50:01.750178: step 2382, loss 1.15509, acc 0.953125, prec 0.0453449, recall 0.837081
2017-12-10T13:50:01.942685: step 2383, loss 0.364014, acc 0.90625, prec 0.0453382, recall 0.837081
2017-12-10T13:50:02.131520: step 2384, loss 0.268841, acc 0.84375, prec 0.0453269, recall 0.837081
2017-12-10T13:50:02.316389: step 2385, loss 0.118337, acc 0.96875, prec 0.0453721, recall 0.837231
2017-12-10T13:50:02.504050: step 2386, loss 0.30971, acc 0.921875, prec 0.0453665, recall 0.837231
2017-12-10T13:50:02.693997: step 2387, loss 0.395401, acc 0.875, prec 0.0453812, recall 0.837305
2017-12-10T13:50:02.880264: step 2388, loss 0.694032, acc 0.765625, prec 0.0453643, recall 0.837305
2017-12-10T13:50:03.067604: step 2389, loss 0.299795, acc 0.921875, prec 0.0453586, recall 0.837305
2017-12-10T13:50:03.255873: step 2390, loss 0.402668, acc 0.859375, prec 0.0453722, recall 0.83738
2017-12-10T13:50:03.444621: step 2391, loss 0.307234, acc 0.890625, prec 0.045388, recall 0.837454
2017-12-10T13:50:03.631773: step 2392, loss 0.481653, acc 0.875, prec 0.0454264, recall 0.837603
2017-12-10T13:50:03.819474: step 2393, loss 0.517251, acc 0.875, prec 0.0454647, recall 0.837751
2017-12-10T13:50:04.007807: step 2394, loss 0.0954833, acc 0.953125, prec 0.0454613, recall 0.837751
2017-12-10T13:50:04.195718: step 2395, loss 0.0926302, acc 0.96875, prec 0.0454827, recall 0.837825
2017-12-10T13:50:04.384608: step 2396, loss 0.242495, acc 0.90625, prec 0.045476, recall 0.837825
2017-12-10T13:50:04.577316: step 2397, loss 0.208747, acc 0.90625, prec 0.0454929, recall 0.8379
2017-12-10T13:50:04.765447: step 2398, loss 0.47165, acc 0.953125, prec 0.0455605, recall 0.838121
2017-12-10T13:50:04.954405: step 2399, loss 0.118817, acc 0.953125, prec 0.0455807, recall 0.838195
2017-12-10T13:50:05.144353: step 2400, loss 0.511403, acc 0.890625, prec 0.0455965, recall 0.838269
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_0/1512931325/checkpoints/model-2400

2017-12-10T13:50:06.569234: step 2401, loss 0.416189, acc 0.9375, prec 0.045592, recall 0.838269
2017-12-10T13:50:06.758096: step 2402, loss 5.9078, acc 0.84375, prec 0.0456054, recall 0.837961
2017-12-10T13:50:06.945856: step 2403, loss 0.191738, acc 0.953125, prec 0.045673, recall 0.838182
2017-12-10T13:50:07.132865: step 2404, loss 0.256389, acc 0.890625, prec 0.0456887, recall 0.838255
2017-12-10T13:50:07.322751: step 2405, loss 0.528439, acc 0.875, prec 0.0456796, recall 0.838255
2017-12-10T13:50:07.512590: step 2406, loss 0.234956, acc 0.953125, prec 0.0456762, recall 0.838255
2017-12-10T13:50:07.700444: step 2407, loss 0.503302, acc 0.875, prec 0.0456908, recall 0.838329
2017-12-10T13:50:07.890996: step 2408, loss 0.399275, acc 0.84375, prec 0.0456795, recall 0.838329
2017-12-10T13:50:08.079752: step 2409, loss 0.207871, acc 0.9375, prec 0.0456986, recall 0.838402
2017-12-10T13:50:08.269346: step 2410, loss 0.625649, acc 0.828125, prec 0.0457098, recall 0.838476
2017-12-10T13:50:08.457947: step 2411, loss 0.266349, acc 0.890625, prec 0.0457019, recall 0.838476
2017-12-10T13:50:08.652307: step 2412, loss 0.357365, acc 0.890625, prec 0.0457175, recall 0.838549
2017-12-10T13:50:08.834127: step 2413, loss 0.16466, acc 0.96875, prec 0.0457389, recall 0.838622
2017-12-10T13:50:09.021372: step 2414, loss 0.134175, acc 0.921875, prec 0.0457568, recall 0.838695
2017-12-10T13:50:09.208021: step 2415, loss 0.362609, acc 0.921875, prec 0.0457512, recall 0.838695
2017-12-10T13:50:09.397080: step 2416, loss 0.35294, acc 0.890625, prec 0.0457432, recall 0.838695
2017-12-10T13:50:09.587952: step 2417, loss 0.166436, acc 0.9375, prec 0.0457623, recall 0.838768
2017-12-10T13:50:09.780700: step 2418, loss 0.307214, acc 0.90625, prec 0.0457555, recall 0.838768
2017-12-10T13:50:09.967397: step 2419, loss 0.261095, acc 0.921875, prec 0.0457499, recall 0.838768
2017-12-10T13:50:10.151575: step 2420, loss 0.0895011, acc 0.96875, prec 0.0458183, recall 0.838987
2017-12-10T13:50:10.339377: step 2421, loss 0.257933, acc 0.90625, prec 0.0458115, recall 0.838987
2017-12-10T13:50:10.524897: step 2422, loss 0.115803, acc 0.953125, prec 0.0458317, recall 0.83906
2017-12-10T13:50:10.710656: step 2423, loss 1.15353, acc 0.90625, prec 0.0458485, recall 0.839132
2017-12-10T13:50:10.901215: step 2424, loss 0.0777584, acc 0.96875, prec 0.0458933, recall 0.839278
2017-12-10T13:50:11.089484: step 2425, loss 0.416719, acc 0.90625, prec 0.0459101, recall 0.83935
2017-12-10T13:50:11.279055: step 2426, loss 0.153087, acc 0.9375, prec 0.0459291, recall 0.839423
2017-12-10T13:50:11.472910: step 2427, loss 0.206707, acc 0.890625, prec 0.0459447, recall 0.839495
2017-12-10T13:50:11.659902: step 2428, loss 0.181532, acc 0.9375, prec 0.0459637, recall 0.839567
2017-12-10T13:50:11.848382: step 2429, loss 0.369611, acc 0.953125, prec 0.0459838, recall 0.83964
2017-12-10T13:50:12.039626: step 2430, loss 0.338151, acc 0.96875, prec 0.0460051, recall 0.839712
2017-12-10T13:50:12.228534: step 2431, loss 0.395915, acc 0.9375, prec 0.0460476, recall 0.839856
2017-12-10T13:50:12.413628: step 2432, loss 0.153277, acc 0.96875, prec 0.0460689, recall 0.839928
2017-12-10T13:50:12.598335: step 2433, loss 0.409521, acc 0.90625, prec 0.0461091, recall 0.840072
2017-12-10T13:50:12.788415: step 2434, loss 0.29229, acc 0.921875, prec 0.0461269, recall 0.840144
2017-12-10T13:50:12.976601: step 2435, loss 0.123634, acc 0.921875, prec 0.0461212, recall 0.840144
2017-12-10T13:50:13.165183: step 2436, loss 5.56572, acc 0.953125, prec 0.046119, recall 0.839767
2017-12-10T13:50:13.360234: step 2437, loss 0.0956367, acc 0.96875, prec 0.0461167, recall 0.839767
2017-12-10T13:50:13.550772: step 2438, loss 1.47811, acc 0.921875, prec 0.0461121, recall 0.83939
2017-12-10T13:50:13.746871: step 2439, loss 2.45521, acc 0.875, prec 0.0461042, recall 0.839013
2017-12-10T13:50:13.939596: step 2440, loss 0.27639, acc 0.875, prec 0.0460951, recall 0.839013
2017-12-10T13:50:14.124183: step 2441, loss 0.488856, acc 0.875, prec 0.0461095, recall 0.839086
2017-12-10T13:50:14.309460: step 2442, loss 0.621981, acc 0.84375, prec 0.0460982, recall 0.839086
2017-12-10T13:50:14.495614: step 2443, loss 0.340436, acc 0.84375, prec 0.0460868, recall 0.839086
2017-12-10T13:50:14.683529: step 2444, loss 0.478113, acc 0.84375, prec 0.0460755, recall 0.839086
2017-12-10T13:50:14.869936: step 2445, loss 0.660061, acc 0.8125, prec 0.0460619, recall 0.839086
2017-12-10T13:50:15.060489: step 2446, loss 0.702833, acc 0.703125, prec 0.0460403, recall 0.839086
2017-12-10T13:50:15.248539: step 2447, loss 0.902808, acc 0.796875, prec 0.0460256, recall 0.839086
2017-12-10T13:50:15.433749: step 2448, loss 0.359414, acc 0.890625, prec 0.0460646, recall 0.83923
2017-12-10T13:50:15.623181: step 2449, loss 0.717187, acc 0.78125, prec 0.0460956, recall 0.839374
2017-12-10T13:50:15.811858: step 2450, loss 1.03224, acc 0.78125, prec 0.0461266, recall 0.839517
2017-12-10T13:50:16.000784: step 2451, loss 0.437398, acc 0.828125, prec 0.0461142, recall 0.839517
2017-12-10T13:50:16.185762: step 2452, loss 0.478681, acc 0.828125, prec 0.0461486, recall 0.839661
2017-12-10T13:50:16.369201: step 2453, loss 0.474055, acc 0.859375, prec 0.0461618, recall 0.839732
2017-12-10T13:50:16.557049: step 2454, loss 0.723116, acc 0.78125, prec 0.0461693, recall 0.839804
2017-12-10T13:50:16.741015: step 2455, loss 0.533703, acc 0.890625, prec 0.0461848, recall 0.839875
2017-12-10T13:50:16.928076: step 2456, loss 0.401014, acc 0.84375, prec 0.0461735, recall 0.839875
2017-12-10T13:50:17.114974: step 2457, loss 0.620038, acc 0.796875, prec 0.0461821, recall 0.839947
2017-12-10T13:50:17.303482: step 2458, loss 0.746513, acc 0.90625, prec 0.0462455, recall 0.84016
2017-12-10T13:50:17.495963: step 2459, loss 0.474961, acc 0.890625, prec 0.0462609, recall 0.840231
2017-12-10T13:50:17.686492: step 2460, loss 2.77625, acc 0.921875, prec 0.0462564, recall 0.839858
2017-12-10T13:50:17.875422: step 2461, loss 0.776415, acc 0.90625, prec 0.0463197, recall 0.840071
2017-12-10T13:50:18.067060: step 2462, loss 0.443472, acc 0.875, prec 0.0463339, recall 0.840142
2017-12-10T13:50:18.258543: step 2463, loss 0.349811, acc 0.890625, prec 0.046326, recall 0.840142
2017-12-10T13:50:18.449587: step 2464, loss 0.497188, acc 0.890625, prec 0.0463181, recall 0.840142
2017-12-10T13:50:18.640265: step 2465, loss 0.337186, acc 0.890625, prec 0.0463335, recall 0.840213
2017-12-10T13:50:18.828329: step 2466, loss 0.106938, acc 0.9375, prec 0.0463289, recall 0.840213
2017-12-10T13:50:19.015663: step 2467, loss 0.816503, acc 0.859375, prec 0.0463421, recall 0.840284
2017-12-10T13:50:19.210477: step 2468, loss 0.670166, acc 0.90625, prec 0.0463819, recall 0.840426
2017-12-10T13:50:19.402537: step 2469, loss 0.440064, acc 0.875, prec 0.0463728, recall 0.840426
2017-12-10T13:50:19.591302: step 2470, loss 0.389164, acc 0.890625, prec 0.0464115, recall 0.840567
2017-12-10T13:50:19.776328: step 2471, loss 0.570921, acc 0.828125, prec 0.0464457, recall 0.840708
2017-12-10T13:50:19.966221: step 2472, loss 0.647595, acc 0.796875, prec 0.0464542, recall 0.840778
2017-12-10T13:50:20.155991: step 2473, loss 0.335078, acc 0.921875, prec 0.0464719, recall 0.840849
2017-12-10T13:50:20.345467: step 2474, loss 0.352785, acc 0.875, prec 0.0464861, recall 0.840919
2017-12-10T13:50:20.535639: step 2475, loss 0.0736436, acc 0.96875, prec 0.0465071, recall 0.840989
2017-12-10T13:50:20.726763: step 2476, loss 0.211077, acc 0.921875, prec 0.0465014, recall 0.840989
2017-12-10T13:50:20.916456: step 2477, loss 0.238439, acc 0.9375, prec 0.0464969, recall 0.840989
2017-12-10T13:50:21.106453: step 2478, loss 1.12403, acc 0.9375, prec 0.0465622, recall 0.8412
2017-12-10T13:50:21.297737: step 2479, loss 0.194905, acc 0.890625, prec 0.0465542, recall 0.8412
2017-12-10T13:50:21.488848: step 2480, loss 0.505044, acc 0.875, prec 0.0465684, recall 0.84127
2017-12-10T13:50:21.675238: step 2481, loss 0.442889, acc 0.921875, prec 0.046586, recall 0.84134
2017-12-10T13:50:21.865728: step 2482, loss 0.236362, acc 0.921875, prec 0.0465803, recall 0.84134
2017-12-10T13:50:22.052475: step 2483, loss 0.36054, acc 0.859375, prec 0.0465701, recall 0.84134
2017-12-10T13:50:22.239949: step 2484, loss 0.622108, acc 0.921875, prec 0.0465876, recall 0.84141
2017-12-10T13:50:22.408862: step 2485, loss 0.223934, acc 0.901961, prec 0.046582, recall 0.84141
2017-12-10T13:50:22.606260: step 2486, loss 0.405235, acc 0.890625, prec 0.046574, recall 0.84141
2017-12-10T13:50:22.796600: step 2487, loss 0.195169, acc 0.953125, prec 0.0466171, recall 0.841549
2017-12-10T13:50:22.983892: step 2488, loss 0.260034, acc 0.90625, prec 0.0466335, recall 0.841619
2017-12-10T13:50:23.172637: step 2489, loss 0.482498, acc 0.859375, prec 0.0466465, recall 0.841689
2017-12-10T13:50:23.363680: step 2490, loss 0.165715, acc 0.953125, prec 0.0466663, recall 0.841758
2017-12-10T13:50:23.550662: step 2491, loss 0.211797, acc 0.890625, prec 0.0467048, recall 0.841897
2017-12-10T13:50:23.742301: step 2492, loss 0.166504, acc 0.90625, prec 0.046698, recall 0.841897
2017-12-10T13:50:23.935015: step 2493, loss 0.33123, acc 0.890625, prec 0.0467365, recall 0.842036
2017-12-10T13:50:24.124861: step 2494, loss 0.401492, acc 0.859375, prec 0.0467262, recall 0.842036
2017-12-10T13:50:24.310635: step 2495, loss 2.6858, acc 0.921875, prec 0.0467217, recall 0.841667
2017-12-10T13:50:24.504076: step 2496, loss 0.189736, acc 0.90625, prec 0.0467149, recall 0.841667
2017-12-10T13:50:24.693469: step 2497, loss 0.160858, acc 0.921875, prec 0.0467324, recall 0.841736
2017-12-10T13:50:24.885482: step 2498, loss 0.784716, acc 0.953125, prec 0.0467986, recall 0.841944
2017-12-10T13:50:25.077473: step 2499, loss 0.134098, acc 0.953125, prec 0.0468183, recall 0.842013
2017-12-10T13:50:25.263578: step 2500, loss 0.292695, acc 0.90625, prec 0.0468347, recall 0.842082
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_0/1512931325/checkpoints/model-2500

2017-12-10T13:50:26.407105: step 2501, loss 0.211644, acc 0.96875, prec 0.0468324, recall 0.842082
2017-12-10T13:50:26.598463: step 2502, loss 0.248589, acc 0.9375, prec 0.0468279, recall 0.842082
2017-12-10T13:50:26.788472: step 2503, loss 0.231968, acc 0.921875, prec 0.0468685, recall 0.84222
2017-12-10T13:50:26.974685: step 2504, loss 1.98962, acc 0.84375, prec 0.0468583, recall 0.841852
2017-12-10T13:50:27.167798: step 2505, loss 0.146388, acc 0.953125, prec 0.0468549, recall 0.841852
2017-12-10T13:50:27.361216: step 2506, loss 0.422738, acc 0.84375, prec 0.0468666, recall 0.841921
2017-12-10T13:50:27.553063: step 2507, loss 0.273215, acc 0.875, prec 0.0469502, recall 0.842197
2017-12-10T13:50:27.744709: step 2508, loss 0.822274, acc 0.953125, prec 0.0469931, recall 0.842335
2017-12-10T13:50:27.934848: step 2509, loss 0.383388, acc 0.859375, prec 0.0470291, recall 0.842472
2017-12-10T13:50:28.121417: step 2510, loss 0.458458, acc 0.890625, prec 0.0470211, recall 0.842472
2017-12-10T13:50:28.311776: step 2511, loss 0.499476, acc 0.875, prec 0.0470583, recall 0.842609
2017-12-10T13:50:28.501186: step 2512, loss 0.388101, acc 0.90625, prec 0.0470977, recall 0.842745
2017-12-10T13:50:28.690739: step 2513, loss 0.218779, acc 0.890625, prec 0.0470897, recall 0.842745
2017-12-10T13:50:28.886743: step 2514, loss 0.330468, acc 0.90625, prec 0.0470828, recall 0.842745
2017-12-10T13:50:29.077177: step 2515, loss 0.481781, acc 0.875, prec 0.0470737, recall 0.842745
2017-12-10T13:50:29.261646: step 2516, loss 0.367424, acc 0.875, prec 0.0470645, recall 0.842745
2017-12-10T13:50:29.447804: step 2517, loss 0.267175, acc 0.921875, prec 0.0470588, recall 0.842745
2017-12-10T13:50:29.636127: step 2518, loss 0.522647, acc 0.90625, prec 0.0470751, recall 0.842814
2017-12-10T13:50:29.821142: step 2519, loss 0.529638, acc 0.828125, prec 0.0471318, recall 0.843018
2017-12-10T13:50:30.011733: step 2520, loss 0.209714, acc 0.984375, prec 0.0471769, recall 0.843154
2017-12-10T13:50:30.201866: step 2521, loss 0.306376, acc 0.859375, prec 0.0471666, recall 0.843154
2017-12-10T13:50:30.392458: step 2522, loss 0.516185, acc 0.859375, prec 0.0472025, recall 0.84329
2017-12-10T13:50:30.579970: step 2523, loss 0.283886, acc 0.90625, prec 0.0472187, recall 0.843358
2017-12-10T13:50:30.768544: step 2524, loss 0.370633, acc 0.875, prec 0.0472327, recall 0.843426
2017-12-10T13:50:30.958214: step 2525, loss 0.114788, acc 0.96875, prec 0.0472304, recall 0.843426
2017-12-10T13:50:31.152081: step 2526, loss 0.348714, acc 0.875, prec 0.0472443, recall 0.843493
2017-12-10T13:50:31.351354: step 2527, loss 1.69009, acc 0.875, prec 0.0472593, recall 0.843197
2017-12-10T13:50:31.540831: step 2528, loss 0.19498, acc 0.921875, prec 0.0472536, recall 0.843197
2017-12-10T13:50:31.732679: step 2529, loss 0.129903, acc 0.9375, prec 0.0472952, recall 0.843332
2017-12-10T13:50:31.920036: step 2530, loss 0.141648, acc 0.953125, prec 0.0473148, recall 0.843399
2017-12-10T13:50:32.109027: step 2531, loss 0.284501, acc 0.9375, prec 0.0473333, recall 0.843467
2017-12-10T13:50:32.295067: step 2532, loss 0.25093, acc 0.9375, prec 0.0473517, recall 0.843534
2017-12-10T13:50:32.483870: step 2533, loss 0.202494, acc 0.90625, prec 0.0473679, recall 0.843602
2017-12-10T13:50:32.674315: step 2534, loss 0.992097, acc 0.9375, prec 0.0474094, recall 0.843737
2017-12-10T13:50:32.862357: step 2535, loss 2.89995, acc 0.96875, prec 0.0474313, recall 0.843441
2017-12-10T13:50:33.058575: step 2536, loss 0.252112, acc 0.953125, prec 0.047497, recall 0.843643
2017-12-10T13:50:33.247092: step 2537, loss 0.262606, acc 0.890625, prec 0.047512, recall 0.84371
2017-12-10T13:50:33.438139: step 2538, loss 0.322434, acc 0.875, prec 0.0476179, recall 0.844045
2017-12-10T13:50:33.630917: step 2539, loss 0.255536, acc 0.90625, prec 0.047611, recall 0.844045
2017-12-10T13:50:33.815310: step 2540, loss 0.67985, acc 0.796875, prec 0.047596, recall 0.844045
2017-12-10T13:50:34.006387: step 2541, loss 0.524538, acc 0.78125, prec 0.0475799, recall 0.844045
2017-12-10T13:50:34.196062: step 2542, loss 0.742216, acc 0.828125, prec 0.0475673, recall 0.844045
2017-12-10T13:50:34.383455: step 2543, loss 0.421829, acc 0.890625, prec 0.0475823, recall 0.844111
2017-12-10T13:50:34.572256: step 2544, loss 0.560528, acc 0.828125, prec 0.0475696, recall 0.844111
2017-12-10T13:50:34.765474: step 2545, loss 0.616944, acc 0.828125, prec 0.04758, recall 0.844178
2017-12-10T13:50:34.949430: step 2546, loss 0.474804, acc 0.875, prec 0.0476168, recall 0.844311
2017-12-10T13:50:35.133276: step 2547, loss 0.341172, acc 0.921875, prec 0.047634, recall 0.844378
2017-12-10T13:50:35.322172: step 2548, loss 0.509207, acc 0.84375, prec 0.0476225, recall 0.844378
2017-12-10T13:50:35.509470: step 2549, loss 0.074439, acc 0.984375, prec 0.0476443, recall 0.844444
2017-12-10T13:50:35.697205: step 2550, loss 0.400158, acc 0.890625, prec 0.0476592, recall 0.844511
2017-12-10T13:50:35.882723: step 2551, loss 0.304481, acc 0.921875, prec 0.0476764, recall 0.844577
2017-12-10T13:50:36.072904: step 2552, loss 0.731997, acc 0.890625, prec 0.0477143, recall 0.84471
2017-12-10T13:50:36.260597: step 2553, loss 0.231932, acc 0.984375, prec 0.0477361, recall 0.844776
2017-12-10T13:50:36.448592: step 2554, loss 0.301738, acc 0.890625, prec 0.047751, recall 0.844842
2017-12-10T13:50:36.634037: step 2555, loss 0.347411, acc 0.859375, prec 0.0477406, recall 0.844842
2017-12-10T13:50:36.824349: step 2556, loss 0.198109, acc 0.9375, prec 0.0477819, recall 0.844974
2017-12-10T13:50:37.014662: step 2557, loss 0.286145, acc 0.90625, prec 0.0477979, recall 0.84504
2017-12-10T13:50:37.201615: step 2558, loss 0.208373, acc 0.96875, prec 0.0477956, recall 0.84504
2017-12-10T13:50:37.388794: step 2559, loss 0.237597, acc 0.953125, prec 0.0478151, recall 0.845106
2017-12-10T13:50:37.574400: step 2560, loss 0.49656, acc 0.90625, prec 0.0478311, recall 0.845172
2017-12-10T13:50:37.763964: step 2561, loss 0.0886293, acc 0.96875, prec 0.0478517, recall 0.845238
2017-12-10T13:50:37.949620: step 2562, loss 0.319847, acc 0.890625, prec 0.0478666, recall 0.845304
2017-12-10T13:50:38.136039: step 2563, loss 0.238858, acc 0.921875, prec 0.0478837, recall 0.84537
2017-12-10T13:50:38.326882: step 2564, loss 0.297379, acc 0.953125, prec 0.0478803, recall 0.84537
2017-12-10T13:50:38.516853: step 2565, loss 0.209298, acc 0.953125, prec 0.0478768, recall 0.84537
2017-12-10T13:50:38.708468: step 2566, loss 0.236019, acc 0.921875, prec 0.0478711, recall 0.84537
2017-12-10T13:50:38.897615: step 2567, loss 0.136403, acc 0.90625, prec 0.0478871, recall 0.845435
2017-12-10T13:50:39.085791: step 2568, loss 0.863188, acc 0.921875, prec 0.0479271, recall 0.845566
2017-12-10T13:50:39.275342: step 2569, loss 0.158247, acc 0.96875, prec 0.0479248, recall 0.845566
2017-12-10T13:50:39.466939: step 2570, loss 0.14634, acc 0.9375, prec 0.0479431, recall 0.845632
2017-12-10T13:50:39.657760: step 2571, loss 3.26302, acc 0.953125, prec 0.0479408, recall 0.845273
2017-12-10T13:50:39.848935: step 2572, loss 0.166804, acc 0.984375, prec 0.0479396, recall 0.845273
2017-12-10T13:50:40.037183: step 2573, loss 0.296693, acc 0.96875, prec 0.0479373, recall 0.845273
2017-12-10T13:50:40.230989: step 2574, loss 0.0666201, acc 0.96875, prec 0.0479579, recall 0.845339
2017-12-10T13:50:40.421254: step 2575, loss 0.140455, acc 0.96875, prec 0.0480013, recall 0.84547
2017-12-10T13:50:40.609867: step 2576, loss 0.0609186, acc 0.96875, prec 0.0480219, recall 0.845535
2017-12-10T13:50:40.797760: step 2577, loss 0.374944, acc 0.875, prec 0.0480127, recall 0.845535
2017-12-10T13:50:40.984772: step 2578, loss 0.0616136, acc 0.984375, prec 0.0480115, recall 0.845535
2017-12-10T13:50:41.173893: step 2579, loss 0.191435, acc 0.953125, prec 0.0480538, recall 0.845666
2017-12-10T13:50:41.362337: step 2580, loss 0.469859, acc 0.921875, prec 0.0480709, recall 0.845731
2017-12-10T13:50:41.554224: step 2581, loss 0.216943, acc 0.90625, prec 0.048064, recall 0.845731
2017-12-10T13:50:41.744691: step 2582, loss 0.256205, acc 0.90625, prec 0.0480571, recall 0.845731
2017-12-10T13:50:41.931341: step 2583, loss 0.180386, acc 0.921875, prec 0.0480513, recall 0.845731
2017-12-10T13:50:42.121170: step 2584, loss 0.25131, acc 0.90625, prec 0.0480444, recall 0.845731
2017-12-10T13:50:42.311790: step 2585, loss 0.206071, acc 0.921875, prec 0.0480615, recall 0.845796
2017-12-10T13:50:42.502534: step 2586, loss 1.00103, acc 0.921875, prec 0.0481014, recall 0.845927
2017-12-10T13:50:42.695900: step 2587, loss 0.187207, acc 0.90625, prec 0.048163, recall 0.846121
2017-12-10T13:50:42.882950: step 2588, loss 0.552087, acc 0.828125, prec 0.0481731, recall 0.846186
2017-12-10T13:50:43.073359: step 2589, loss 0.189344, acc 0.953125, prec 0.0481697, recall 0.846186
2017-12-10T13:50:43.257508: step 2590, loss 1.39771, acc 0.921875, prec 0.0481879, recall 0.845895
2017-12-10T13:50:43.452219: step 2591, loss 0.705494, acc 0.90625, prec 0.0482266, recall 0.846024
2017-12-10T13:50:43.641166: step 2592, loss 0.328738, acc 0.90625, prec 0.0482425, recall 0.846089
2017-12-10T13:50:43.833408: step 2593, loss 0.418868, acc 0.9375, prec 0.0482607, recall 0.846154
2017-12-10T13:50:44.022771: step 2594, loss 0.256637, acc 0.90625, prec 0.0482765, recall 0.846218
2017-12-10T13:50:44.214900: step 2595, loss 0.340329, acc 0.921875, prec 0.0483164, recall 0.846348
2017-12-10T13:50:44.401152: step 2596, loss 0.517084, acc 0.90625, prec 0.0483322, recall 0.846412
2017-12-10T13:50:44.587567: step 2597, loss 0.311339, acc 0.9375, prec 0.0483276, recall 0.846412
2017-12-10T13:50:44.775225: step 2598, loss 1.34479, acc 0.9375, prec 0.0483913, recall 0.846605
2017-12-10T13:50:44.967149: step 2599, loss 0.372727, acc 0.859375, prec 0.0484037, recall 0.846669
2017-12-10T13:50:45.151967: step 2600, loss 0.389241, acc 0.859375, prec 0.0484388, recall 0.846798
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_0/1512931325/checkpoints/model-2600

2017-12-10T13:50:46.332218: step 2601, loss 0.580296, acc 0.84375, prec 0.0484273, recall 0.846798
2017-12-10T13:50:46.521618: step 2602, loss 0.263289, acc 0.875, prec 0.0484408, recall 0.846862
2017-12-10T13:50:46.705193: step 2603, loss 0.353232, acc 0.828125, prec 0.048428, recall 0.846862
2017-12-10T13:50:46.890400: step 2604, loss 0.468262, acc 0.84375, prec 0.0484164, recall 0.846862
2017-12-10T13:50:47.079741: step 2605, loss 0.328378, acc 0.890625, prec 0.0484083, recall 0.846862
2017-12-10T13:50:47.265733: step 2606, loss 0.343693, acc 0.875, prec 0.0483991, recall 0.846862
2017-12-10T13:50:47.459994: step 2607, loss 0.345563, acc 0.90625, prec 0.0484149, recall 0.846926
2017-12-10T13:50:47.646989: step 2608, loss 0.308199, acc 0.90625, prec 0.0484079, recall 0.846926
2017-12-10T13:50:47.839578: step 2609, loss 0.826596, acc 0.953125, prec 0.0484499, recall 0.847054
2017-12-10T13:50:48.032155: step 2610, loss 0.24053, acc 0.890625, prec 0.0484646, recall 0.847118
2017-12-10T13:50:48.222180: step 2611, loss 0.531417, acc 0.90625, prec 0.0485031, recall 0.847245
2017-12-10T13:50:48.414812: step 2612, loss 0.119953, acc 0.953125, prec 0.0484996, recall 0.847245
2017-12-10T13:50:48.600697: step 2613, loss 0.194808, acc 0.953125, prec 0.0485189, recall 0.847309
2017-12-10T13:50:48.788509: step 2614, loss 0.526624, acc 0.84375, prec 0.0485527, recall 0.847436
2017-12-10T13:50:48.976300: step 2615, loss 0.427825, acc 0.828125, prec 0.0485854, recall 0.847564
2017-12-10T13:50:49.170085: step 2616, loss 0.249591, acc 0.90625, prec 0.0486012, recall 0.847627
2017-12-10T13:50:49.363450: step 2617, loss 0.167971, acc 0.921875, prec 0.0486408, recall 0.847754
2017-12-10T13:50:49.552940: step 2618, loss 0.318946, acc 0.828125, prec 0.0486507, recall 0.847817
2017-12-10T13:50:49.744982: step 2619, loss 0.396057, acc 0.84375, prec 0.0486391, recall 0.847817
2017-12-10T13:50:49.932221: step 2620, loss 0.27871, acc 0.9375, prec 0.0486572, recall 0.84788
2017-12-10T13:50:50.122842: step 2621, loss 0.22125, acc 0.875, prec 0.0486479, recall 0.84788
2017-12-10T13:50:50.314864: step 2622, loss 0.0218488, acc 1, prec 0.0486706, recall 0.847943
2017-12-10T13:50:50.508064: step 2623, loss 0.376097, acc 0.921875, prec 0.0486874, recall 0.848007
2017-12-10T13:50:50.697904: step 2624, loss 0.215855, acc 0.96875, prec 0.0487305, recall 0.848133
2017-12-10T13:50:50.890234: step 2625, loss 0.082508, acc 0.984375, prec 0.0487747, recall 0.848259
2017-12-10T13:50:51.082865: step 2626, loss 0.242176, acc 0.953125, prec 0.0488165, recall 0.848384
2017-12-10T13:50:51.270477: step 2627, loss 2.74701, acc 0.953125, prec 0.0488369, recall 0.848096
2017-12-10T13:50:51.461764: step 2628, loss 0.921743, acc 0.96875, prec 0.0488572, recall 0.848159
2017-12-10T13:50:51.653951: step 2629, loss 0.12585, acc 0.953125, prec 0.0488537, recall 0.848159
2017-12-10T13:50:51.842176: step 2630, loss 0.474638, acc 0.875, prec 0.0488444, recall 0.848159
2017-12-10T13:50:52.028554: step 2631, loss 0.379518, acc 0.859375, prec 0.0488339, recall 0.848159
2017-12-10T13:50:52.224043: step 2632, loss 0.332039, acc 0.9375, prec 0.0488746, recall 0.848284
2017-12-10T13:50:52.412650: step 2633, loss 0.479769, acc 0.890625, prec 0.0489118, recall 0.84841
2017-12-10T13:50:52.598151: step 2634, loss 0.298238, acc 0.890625, prec 0.0489036, recall 0.84841
2017-12-10T13:50:52.789009: step 2635, loss 0.21186, acc 0.90625, prec 0.0489193, recall 0.848472
2017-12-10T13:50:52.975422: step 2636, loss 0.234865, acc 0.859375, prec 0.0489314, recall 0.848535
2017-12-10T13:50:53.164000: step 2637, loss 0.436198, acc 0.875, prec 0.0489221, recall 0.848535
2017-12-10T13:50:53.350319: step 2638, loss 0.634567, acc 0.84375, prec 0.0489557, recall 0.84866
2017-12-10T13:50:53.538485: step 2639, loss 0.285091, acc 0.90625, prec 0.0489487, recall 0.84866
2017-12-10T13:50:53.729895: step 2640, loss 0.604738, acc 0.84375, prec 0.0489371, recall 0.84866
2017-12-10T13:50:53.917879: step 2641, loss 0.693921, acc 0.890625, prec 0.0489515, recall 0.848722
2017-12-10T13:50:54.106631: step 2642, loss 0.132273, acc 0.953125, prec 0.0489481, recall 0.848722
2017-12-10T13:50:54.296567: step 2643, loss 0.146153, acc 0.953125, prec 0.0489446, recall 0.848722
2017-12-10T13:50:54.485757: step 2644, loss 0.370074, acc 0.859375, prec 0.0489341, recall 0.848722
2017-12-10T13:50:54.674654: step 2645, loss 0.326625, acc 0.921875, prec 0.0489509, recall 0.848785
2017-12-10T13:50:54.865045: step 2646, loss 0.404537, acc 0.875, prec 0.0489868, recall 0.848909
2017-12-10T13:50:55.053674: step 2647, loss 0.598131, acc 0.859375, prec 0.0489989, recall 0.848971
2017-12-10T13:50:55.243473: step 2648, loss 0.343484, acc 0.921875, prec 0.0490156, recall 0.849033
2017-12-10T13:50:55.437563: step 2649, loss 0.233324, acc 0.9375, prec 0.0490336, recall 0.849095
2017-12-10T13:50:55.627295: step 2650, loss 0.848455, acc 0.9375, prec 0.0491192, recall 0.849343
2017-12-10T13:50:55.819983: step 2651, loss 0.369545, acc 0.890625, prec 0.0491336, recall 0.849405
2017-12-10T13:50:56.011799: step 2652, loss 0.188779, acc 0.96875, prec 0.0491313, recall 0.849405
2017-12-10T13:50:56.198315: step 2653, loss 0.530932, acc 0.859375, prec 0.0491434, recall 0.849467
2017-12-10T13:50:56.386276: step 2654, loss 0.23333, acc 0.921875, prec 0.0491601, recall 0.849528
2017-12-10T13:50:56.579058: step 2655, loss 0.3319, acc 0.90625, prec 0.0491982, recall 0.849652
2017-12-10T13:50:56.770793: step 2656, loss 0.0909663, acc 0.96875, prec 0.0491959, recall 0.849652
2017-12-10T13:50:56.958466: step 2657, loss 0.15049, acc 0.953125, prec 0.0491924, recall 0.849652
2017-12-10T13:50:57.148167: step 2658, loss 0.243698, acc 0.90625, prec 0.0492079, recall 0.849713
2017-12-10T13:50:57.334988: step 2659, loss 0.262604, acc 0.9375, prec 0.0492484, recall 0.849836
2017-12-10T13:50:57.526792: step 2660, loss 0.916198, acc 0.9375, prec 0.0492888, recall 0.849959
2017-12-10T13:50:57.718101: step 2661, loss 0.292356, acc 0.921875, prec 0.0492829, recall 0.849959
2017-12-10T13:50:57.910671: step 2662, loss 0.0834902, acc 0.953125, prec 0.0492794, recall 0.849959
2017-12-10T13:50:58.098897: step 2663, loss 0.31732, acc 0.90625, prec 0.0492724, recall 0.849959
2017-12-10T13:50:58.292353: step 2664, loss 0.254408, acc 0.9375, prec 0.0492677, recall 0.849959
2017-12-10T13:50:58.478202: step 2665, loss 0.172263, acc 0.96875, prec 0.0492879, recall 0.85002
2017-12-10T13:50:58.663915: step 2666, loss 0.103782, acc 0.953125, prec 0.0493295, recall 0.850143
2017-12-10T13:50:58.858229: step 2667, loss 0.439926, acc 0.859375, prec 0.0493865, recall 0.850326
2017-12-10T13:50:59.047601: step 2668, loss 0.498082, acc 0.96875, prec 0.0494292, recall 0.850448
2017-12-10T13:50:59.239846: step 2669, loss 0.256166, acc 0.96875, prec 0.0494269, recall 0.850448
2017-12-10T13:50:59.427261: step 2670, loss 0.413235, acc 0.921875, prec 0.0494885, recall 0.850631
2017-12-10T13:50:59.612563: step 2671, loss 0.325147, acc 0.90625, prec 0.0494815, recall 0.850631
2017-12-10T13:50:59.797365: step 2672, loss 0.103061, acc 0.984375, prec 0.0495253, recall 0.850752
2017-12-10T13:50:59.985963: step 2673, loss 0.202314, acc 0.9375, prec 0.0495207, recall 0.850752
2017-12-10T13:51:00.174781: step 2674, loss 1.58283, acc 0.90625, prec 0.0495598, recall 0.850528
2017-12-10T13:51:00.371634: step 2675, loss 0.186735, acc 0.953125, prec 0.0495563, recall 0.850528
2017-12-10T13:51:00.567737: step 2676, loss 0.118669, acc 0.96875, prec 0.0495539, recall 0.850528
2017-12-10T13:51:00.758093: step 2677, loss 0.359674, acc 0.875, prec 0.049567, recall 0.850589
2017-12-10T13:51:00.946731: step 2678, loss 0.391465, acc 0.90625, prec 0.04956, recall 0.850589
2017-12-10T13:51:01.140674: step 2679, loss 0.127606, acc 0.96875, prec 0.0495801, recall 0.850649
2017-12-10T13:51:01.331780: step 2680, loss 0.452797, acc 0.921875, prec 0.0495967, recall 0.85071
2017-12-10T13:51:01.522729: step 2681, loss 0.35462, acc 0.953125, prec 0.0496157, recall 0.85077
2017-12-10T13:51:01.713304: step 2682, loss 0.331807, acc 0.875, prec 0.0496063, recall 0.85077
2017-12-10T13:51:01.907633: step 2683, loss 0.151171, acc 0.9375, prec 0.0496016, recall 0.85077
2017-12-10T13:51:02.098383: step 2684, loss 0.252634, acc 0.921875, prec 0.0496182, recall 0.850831
2017-12-10T13:51:02.291629: step 2685, loss 0.210235, acc 0.90625, prec 0.0496112, recall 0.850831
2017-12-10T13:51:02.480907: step 2686, loss 0.127101, acc 0.96875, prec 0.0496313, recall 0.850891
2017-12-10T13:51:02.670140: step 2687, loss 0.151879, acc 0.953125, prec 0.0496503, recall 0.850952
2017-12-10T13:51:02.858042: step 2688, loss 0.517851, acc 0.90625, prec 0.0496432, recall 0.850952
2017-12-10T13:51:03.043712: step 2689, loss 0.33229, acc 0.921875, prec 0.0497047, recall 0.851133
2017-12-10T13:51:03.231427: step 2690, loss 0.113786, acc 0.9375, prec 0.0497, recall 0.851133
2017-12-10T13:51:03.419483: step 2691, loss 0.277529, acc 0.859375, prec 0.0496894, recall 0.851133
2017-12-10T13:51:03.610413: step 2692, loss 0.236356, acc 0.921875, prec 0.0496836, recall 0.851133
2017-12-10T13:51:03.802655: step 2693, loss 0.0751977, acc 0.953125, prec 0.0496801, recall 0.851133
2017-12-10T13:51:03.986515: step 2694, loss 1.59865, acc 0.953125, prec 0.0497001, recall 0.850849
2017-12-10T13:51:04.181116: step 2695, loss 0.818147, acc 0.984375, prec 0.0497214, recall 0.850909
2017-12-10T13:51:04.375185: step 2696, loss 0.0419231, acc 0.984375, prec 0.0497202, recall 0.850909
2017-12-10T13:51:04.562352: step 2697, loss 0.365097, acc 0.90625, prec 0.0498253, recall 0.85121
2017-12-10T13:51:04.752172: step 2698, loss 0.293577, acc 0.921875, prec 0.0498195, recall 0.85121
2017-12-10T13:51:04.939440: step 2699, loss 0.276942, acc 0.9375, prec 0.0498596, recall 0.85133
2017-12-10T13:51:05.130624: step 2700, loss 0.170553, acc 0.953125, prec 0.0498785, recall 0.851389
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_0/1512931325/checkpoints/model-2700

2017-12-10T13:51:06.305444: step 2701, loss 0.905775, acc 0.921875, prec 0.0499174, recall 0.851509
2017-12-10T13:51:06.498288: step 2702, loss 0.084435, acc 0.96875, prec 0.0499151, recall 0.851509
2017-12-10T13:51:06.680971: step 2703, loss 0.203428, acc 0.90625, prec 0.049908, recall 0.851509
2017-12-10T13:51:06.869636: step 2704, loss 0.311152, acc 0.9375, prec 0.0499257, recall 0.851569
2017-12-10T13:51:07.056245: step 2705, loss 0.307107, acc 0.921875, prec 0.0499646, recall 0.851688
2017-12-10T13:51:07.245035: step 2706, loss 0.0785942, acc 0.953125, prec 0.0499611, recall 0.851688
2017-12-10T13:51:07.431689: step 2707, loss 0.259951, acc 0.921875, prec 0.05, recall 0.851807
2017-12-10T13:51:07.620662: step 2708, loss 0.185868, acc 0.9375, prec 0.0499953, recall 0.851807
2017-12-10T13:51:07.807357: step 2709, loss 0.148105, acc 0.953125, prec 0.0500141, recall 0.851867
2017-12-10T13:51:07.994855: step 2710, loss 0.300174, acc 0.921875, prec 0.050053, recall 0.851986
2017-12-10T13:51:08.180837: step 2711, loss 0.292925, acc 0.90625, prec 0.0500683, recall 0.852045
2017-12-10T13:51:08.373045: step 2712, loss 0.826041, acc 0.953125, prec 0.0501319, recall 0.852223
2017-12-10T13:51:08.564893: step 2713, loss 0.151395, acc 0.953125, prec 0.0501508, recall 0.852282
2017-12-10T13:51:08.757968: step 2714, loss 0.0368884, acc 0.984375, prec 0.0501496, recall 0.852282
2017-12-10T13:51:08.950454: step 2715, loss 0.455148, acc 0.875, prec 0.0501401, recall 0.852282
2017-12-10T13:51:09.136264: step 2716, loss 0.0523627, acc 1, prec 0.0501401, recall 0.852282
2017-12-10T13:51:09.326999: step 2717, loss 1.02467, acc 0.953125, prec 0.050159, recall 0.852341
2017-12-10T13:51:09.518853: step 2718, loss 0.131208, acc 0.921875, prec 0.0501754, recall 0.8524
2017-12-10T13:51:09.709810: step 2719, loss 0.485646, acc 0.90625, prec 0.0501907, recall 0.852459
2017-12-10T13:51:09.895909: step 2720, loss 0.178791, acc 0.953125, prec 0.0502319, recall 0.852577
2017-12-10T13:51:10.084830: step 2721, loss 0.396195, acc 0.921875, prec 0.0502483, recall 0.852636
2017-12-10T13:51:10.275131: step 2722, loss 0.236117, acc 0.875, prec 0.0502388, recall 0.852636
2017-12-10T13:51:10.462199: step 2723, loss 0.0827917, acc 0.96875, prec 0.0502588, recall 0.852695
2017-12-10T13:51:10.647640: step 2724, loss 0.292598, acc 0.890625, prec 0.0502505, recall 0.852695
2017-12-10T13:51:10.835429: step 2725, loss 0.256689, acc 0.890625, prec 0.0502646, recall 0.852753
2017-12-10T13:51:11.023968: step 2726, loss 0.248107, acc 0.921875, prec 0.0503034, recall 0.852871
2017-12-10T13:51:11.210747: step 2727, loss 0.178165, acc 0.9375, prec 0.050321, recall 0.852929
2017-12-10T13:51:11.401238: step 2728, loss 0.210531, acc 0.90625, prec 0.0503139, recall 0.852929
2017-12-10T13:51:11.590588: step 2729, loss 0.164474, acc 0.953125, prec 0.0503103, recall 0.852929
2017-12-10T13:51:11.780701: step 2730, loss 2.04341, acc 0.90625, prec 0.0503267, recall 0.852648
2017-12-10T13:51:11.974124: step 2731, loss 0.34932, acc 0.921875, prec 0.0503431, recall 0.852707
2017-12-10T13:51:12.160762: step 2732, loss 0.376192, acc 0.859375, prec 0.0503325, recall 0.852707
2017-12-10T13:51:12.350099: step 2733, loss 0.182619, acc 0.921875, prec 0.0503266, recall 0.852707
2017-12-10T13:51:12.539413: step 2734, loss 1.20723, acc 0.953125, prec 0.05039, recall 0.852883
2017-12-10T13:51:12.729636: step 2735, loss 0.305497, acc 0.875, prec 0.0504028, recall 0.852941
2017-12-10T13:51:12.919057: step 2736, loss 0.571186, acc 0.875, prec 0.0503933, recall 0.852941
2017-12-10T13:51:13.103051: step 2737, loss 0.109465, acc 0.96875, prec 0.050391, recall 0.852941
2017-12-10T13:51:13.287877: step 2738, loss 0.212801, acc 0.921875, prec 0.0504073, recall 0.853
2017-12-10T13:51:13.474881: step 2739, loss 0.331076, acc 0.90625, prec 0.0504002, recall 0.853
2017-12-10T13:51:13.664447: step 2740, loss 0.143766, acc 0.890625, prec 0.0504143, recall 0.853058
2017-12-10T13:51:13.857240: step 2741, loss 0.793821, acc 0.859375, prec 0.0504259, recall 0.853116
2017-12-10T13:51:14.045535: step 2742, loss 0.473026, acc 0.828125, prec 0.0504129, recall 0.853116
2017-12-10T13:51:14.231431: step 2743, loss 0.151362, acc 0.921875, prec 0.050407, recall 0.853116
2017-12-10T13:51:14.420341: step 2744, loss 0.368404, acc 0.84375, prec 0.0504397, recall 0.853233
2017-12-10T13:51:14.608070: step 2745, loss 0.209308, acc 0.90625, prec 0.0504326, recall 0.853233
2017-12-10T13:51:14.796366: step 2746, loss 0.271007, acc 0.90625, prec 0.0504255, recall 0.853233
2017-12-10T13:51:14.984410: step 2747, loss 0.149932, acc 0.9375, prec 0.0504208, recall 0.853233
2017-12-10T13:51:15.177661: step 2748, loss 0.354762, acc 0.9375, prec 0.050416, recall 0.853233
2017-12-10T13:51:15.365336: step 2749, loss 0.385266, acc 0.875, prec 0.0504288, recall 0.853291
2017-12-10T13:51:15.553178: step 2750, loss 0.106511, acc 0.953125, prec 0.0504253, recall 0.853291
2017-12-10T13:51:15.739626: step 2751, loss 0.275948, acc 0.90625, prec 0.0504182, recall 0.853291
2017-12-10T13:51:15.929975: step 2752, loss 0.165635, acc 0.921875, prec 0.0504345, recall 0.853349
2017-12-10T13:51:16.118880: step 2753, loss 0.341796, acc 0.890625, prec 0.0504485, recall 0.853407
2017-12-10T13:51:16.304648: step 2754, loss 0.412162, acc 0.96875, prec 0.0504684, recall 0.853465
2017-12-10T13:51:16.493508: step 2755, loss 0.179015, acc 0.953125, prec 0.0504648, recall 0.853465
2017-12-10T13:51:16.684771: step 2756, loss 0.155621, acc 0.9375, prec 0.0504601, recall 0.853465
2017-12-10T13:51:16.872907: step 2757, loss 0.106326, acc 0.9375, prec 0.0504776, recall 0.853523
2017-12-10T13:51:17.057338: step 2758, loss 0.150902, acc 0.9375, prec 0.0505173, recall 0.853639
2017-12-10T13:51:17.247455: step 2759, loss 0.153243, acc 0.921875, prec 0.0505114, recall 0.853639
2017-12-10T13:51:17.438313: step 2760, loss 0.172966, acc 0.96875, prec 0.0505091, recall 0.853639
2017-12-10T13:51:17.632426: step 2761, loss 2.44822, acc 0.890625, prec 0.0505686, recall 0.853476
2017-12-10T13:51:17.821607: step 2762, loss 0.0674517, acc 0.96875, prec 0.0505663, recall 0.853476
2017-12-10T13:51:18.007358: step 2763, loss 0.105252, acc 0.953125, prec 0.0505627, recall 0.853476
2017-12-10T13:51:18.196625: step 2764, loss 0.772135, acc 0.96875, prec 0.0505826, recall 0.853533
2017-12-10T13:51:18.390591: step 2765, loss 0.32334, acc 0.921875, prec 0.0505766, recall 0.853533
2017-12-10T13:51:18.580527: step 2766, loss 0.291843, acc 0.921875, prec 0.0505929, recall 0.853591
2017-12-10T13:51:18.775386: step 2767, loss 0.417656, acc 0.890625, prec 0.0506513, recall 0.853764
2017-12-10T13:51:18.961802: step 2768, loss 0.560842, acc 0.90625, prec 0.0506442, recall 0.853764
2017-12-10T13:51:19.150283: step 2769, loss 0.361104, acc 0.921875, prec 0.0506382, recall 0.853764
2017-12-10T13:51:19.339498: step 2770, loss 0.219654, acc 0.90625, prec 0.0506533, recall 0.853822
2017-12-10T13:51:19.529514: step 2771, loss 0.240428, acc 0.984375, prec 0.0506965, recall 0.853937
2017-12-10T13:51:19.717586: step 2772, loss 0.397542, acc 0.90625, prec 0.0507338, recall 0.854052
2017-12-10T13:51:19.901354: step 2773, loss 0.371186, acc 0.875, prec 0.0507687, recall 0.854167
2017-12-10T13:51:20.089390: step 2774, loss 0.112733, acc 0.953125, prec 0.0507873, recall 0.854224
2017-12-10T13:51:20.278324: step 2775, loss 0.147747, acc 0.9375, prec 0.0507825, recall 0.854224
2017-12-10T13:51:20.466470: step 2776, loss 0.291004, acc 0.921875, prec 0.0507766, recall 0.854224
2017-12-10T13:51:20.654899: step 2777, loss 0.232319, acc 0.921875, prec 0.0507707, recall 0.854224
2017-12-10T13:51:20.842108: step 2778, loss 0.430126, acc 0.875, prec 0.0507612, recall 0.854224
2017-12-10T13:51:21.031559: step 2779, loss 1.36911, acc 0.9375, prec 0.0508451, recall 0.854453
2017-12-10T13:51:21.224331: step 2780, loss 0.484994, acc 0.828125, prec 0.050832, recall 0.854453
2017-12-10T13:51:21.411449: step 2781, loss 0.302546, acc 0.953125, prec 0.0508285, recall 0.854453
2017-12-10T13:51:21.598168: step 2782, loss 0.175769, acc 0.9375, prec 0.0508237, recall 0.854453
2017-12-10T13:51:21.783144: step 2783, loss 0.138939, acc 0.953125, prec 0.0508423, recall 0.85451
2017-12-10T13:51:21.971577: step 2784, loss 0.220929, acc 0.921875, prec 0.0508585, recall 0.854567
2017-12-10T13:51:22.158825: step 2785, loss 0.156288, acc 0.9375, prec 0.0508759, recall 0.854624
2017-12-10T13:51:22.346630: step 2786, loss 0.115764, acc 0.921875, prec 0.05087, recall 0.854624
2017-12-10T13:51:22.535803: step 2787, loss 0.211428, acc 0.96875, prec 0.050934, recall 0.854795
2017-12-10T13:51:22.723743: step 2788, loss 0.352916, acc 0.921875, prec 0.0509723, recall 0.854908
2017-12-10T13:51:22.913920: step 2789, loss 1.62199, acc 0.90625, prec 0.0509885, recall 0.854631
2017-12-10T13:51:23.106265: step 2790, loss 0.219421, acc 0.953125, prec 0.050985, recall 0.854631
2017-12-10T13:51:23.292428: step 2791, loss 0.115977, acc 0.9375, prec 0.0510023, recall 0.854688
2017-12-10T13:51:23.479311: step 2792, loss 0.208382, acc 0.90625, prec 0.0509952, recall 0.854688
2017-12-10T13:51:23.668014: step 2793, loss 0.178772, acc 0.921875, prec 0.0509893, recall 0.854688
2017-12-10T13:51:23.860651: step 2794, loss 0.446499, acc 0.96875, prec 0.051009, recall 0.854744
2017-12-10T13:51:24.052887: step 2795, loss 0.1591, acc 0.953125, prec 0.0510275, recall 0.854801
2017-12-10T13:51:24.245120: step 2796, loss 0.197277, acc 0.921875, prec 0.0510216, recall 0.854801
2017-12-10T13:51:24.435721: step 2797, loss 0.200357, acc 0.9375, prec 0.0510168, recall 0.854801
2017-12-10T13:51:24.623629: step 2798, loss 0.220393, acc 0.9375, prec 0.0510121, recall 0.854801
2017-12-10T13:51:24.815441: step 2799, loss 5.43527, acc 0.921875, prec 0.0510515, recall 0.854581
2017-12-10T13:51:25.006908: step 2800, loss 0.418532, acc 0.875, prec 0.051042, recall 0.854581
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_0/1512931325/checkpoints/model-2800

2017-12-10T13:51:26.193611: step 2801, loss 0.329978, acc 0.875, prec 0.0510546, recall 0.854638
2017-12-10T13:51:26.382922: step 2802, loss 0.0932726, acc 0.96875, prec 0.0510964, recall 0.854751
2017-12-10T13:51:26.569730: step 2803, loss 0.440801, acc 0.90625, prec 0.0510893, recall 0.854751
2017-12-10T13:51:26.761488: step 2804, loss 0.346339, acc 0.875, prec 0.0510798, recall 0.854751
2017-12-10T13:51:26.944869: step 2805, loss 0.46213, acc 0.875, prec 0.0510703, recall 0.854751
2017-12-10T13:51:27.131939: step 2806, loss 0.456415, acc 0.890625, prec 0.0511502, recall 0.854977
2017-12-10T13:51:27.327022: step 2807, loss 0.793546, acc 0.71875, prec 0.0511509, recall 0.855033
2017-12-10T13:51:27.513292: step 2808, loss 0.571086, acc 0.84375, prec 0.051139, recall 0.855033
2017-12-10T13:51:27.696721: step 2809, loss 0.173754, acc 0.9375, prec 0.0511784, recall 0.855146
2017-12-10T13:51:27.885452: step 2810, loss 0.546982, acc 0.875, prec 0.0511909, recall 0.855202
2017-12-10T13:51:28.072719: step 2811, loss 0.383378, acc 0.859375, prec 0.0512022, recall 0.855258
2017-12-10T13:51:28.264725: step 2812, loss 0.14636, acc 0.921875, prec 0.0511963, recall 0.855258
2017-12-10T13:51:28.452955: step 2813, loss 0.336845, acc 0.90625, prec 0.0512332, recall 0.85537
2017-12-10T13:51:28.643649: step 2814, loss 0.264683, acc 0.90625, prec 0.0512701, recall 0.855482
2017-12-10T13:51:28.838644: step 2815, loss 0.418036, acc 0.890625, prec 0.0512618, recall 0.855482
2017-12-10T13:51:29.028517: step 2816, loss 0.240255, acc 0.90625, prec 0.0513207, recall 0.85565
2017-12-10T13:51:29.216280: step 2817, loss 0.0978194, acc 0.96875, prec 0.0513184, recall 0.85565
2017-12-10T13:51:29.404913: step 2818, loss 0.117612, acc 0.953125, prec 0.0513368, recall 0.855706
2017-12-10T13:51:29.597517: step 2819, loss 0.0751893, acc 0.96875, prec 0.0513344, recall 0.855706
2017-12-10T13:51:29.787882: step 2820, loss 0.353388, acc 0.9375, prec 0.0513296, recall 0.855706
2017-12-10T13:51:29.975359: step 2821, loss 0.0976268, acc 0.96875, prec 0.0513273, recall 0.855706
2017-12-10T13:51:30.165097: step 2822, loss 0.179791, acc 0.9375, prec 0.0513225, recall 0.855706
2017-12-10T13:51:30.359428: step 2823, loss 0.154346, acc 0.9375, prec 0.0513177, recall 0.855706
2017-12-10T13:51:30.546396: step 2824, loss 0.0195625, acc 1, prec 0.0513177, recall 0.855706
2017-12-10T13:51:30.735483: step 2825, loss 0.277264, acc 0.9375, prec 0.051335, recall 0.855762
2017-12-10T13:51:30.928837: step 2826, loss 0.0926318, acc 0.984375, prec 0.0513778, recall 0.855873
2017-12-10T13:51:31.115059: step 2827, loss 0.095264, acc 0.96875, prec 0.0513974, recall 0.855929
2017-12-10T13:51:31.306143: step 2828, loss 0.0685383, acc 0.96875, prec 0.051417, recall 0.855985
2017-12-10T13:51:31.496519: step 2829, loss 0.14363, acc 0.953125, prec 0.0514135, recall 0.855985
2017-12-10T13:51:31.685634: step 2830, loss 0.809743, acc 1, prec 0.0514355, recall 0.85604
2017-12-10T13:51:31.876122: step 2831, loss 0.157841, acc 0.96875, prec 0.0514331, recall 0.85604
2017-12-10T13:51:32.067418: step 2832, loss 0.376647, acc 0.9375, prec 0.0514723, recall 0.856151
2017-12-10T13:51:32.257328: step 2833, loss 0.00687645, acc 1, prec 0.0514723, recall 0.856151
2017-12-10T13:51:32.446812: step 2834, loss 0.0903429, acc 0.953125, prec 0.0514907, recall 0.856207
2017-12-10T13:51:32.635071: step 2835, loss 0.244458, acc 0.9375, prec 0.0514859, recall 0.856207
2017-12-10T13:51:32.823522: step 2836, loss 0.148598, acc 0.984375, prec 0.0515067, recall 0.856262
2017-12-10T13:51:33.012363: step 2837, loss 0.201361, acc 0.90625, prec 0.0514996, recall 0.856262
2017-12-10T13:51:33.203049: step 2838, loss 0.120313, acc 0.96875, prec 0.0515411, recall 0.856373
2017-12-10T13:51:33.391936: step 2839, loss 0.143592, acc 0.9375, prec 0.0515364, recall 0.856373
2017-12-10T13:51:33.582368: step 2840, loss 0.275466, acc 0.984375, prec 0.0515791, recall 0.856483
2017-12-10T13:51:33.769756: step 2841, loss 0.0716901, acc 0.96875, prec 0.0515767, recall 0.856483
2017-12-10T13:51:33.957363: step 2842, loss 0.35554, acc 0.90625, prec 0.0516135, recall 0.856594
2017-12-10T13:51:34.149782: step 2843, loss 2.72016, acc 0.953125, prec 0.0516331, recall 0.85632
2017-12-10T13:51:34.347207: step 2844, loss 0.180194, acc 0.921875, prec 0.051671, recall 0.85643
2017-12-10T13:51:34.533063: step 2845, loss 0.218343, acc 0.921875, prec 0.051665, recall 0.85643
2017-12-10T13:51:34.724211: step 2846, loss 0.123538, acc 0.96875, prec 0.0516846, recall 0.856485
2017-12-10T13:51:34.913555: step 2847, loss 0.3007, acc 0.90625, prec 0.0516774, recall 0.856485
2017-12-10T13:51:35.098876: step 2848, loss 0.0937808, acc 0.96875, prec 0.051675, recall 0.856485
2017-12-10T13:51:35.288829: step 2849, loss 0.0759271, acc 0.96875, prec 0.0516726, recall 0.856485
2017-12-10T13:51:35.476419: step 2850, loss 0.195183, acc 0.953125, prec 0.051691, recall 0.85654
2017-12-10T13:51:35.659438: step 2851, loss 1.38102, acc 0.921875, prec 0.0517509, recall 0.856705
2017-12-10T13:51:35.848413: step 2852, loss 0.183899, acc 0.96875, prec 0.0517704, recall 0.85676
2017-12-10T13:51:36.037659: step 2853, loss 0.0596117, acc 0.984375, prec 0.0517692, recall 0.85676
2017-12-10T13:51:36.225296: step 2854, loss 0.131763, acc 0.96875, prec 0.0517888, recall 0.856815
2017-12-10T13:51:36.410190: step 2855, loss 0.548943, acc 0.859375, prec 0.051778, recall 0.856815
2017-12-10T13:51:36.598256: step 2856, loss 0.482624, acc 0.875, prec 0.0517903, recall 0.85687
2017-12-10T13:51:36.784803: step 2857, loss 0.444606, acc 0.875, prec 0.0518027, recall 0.856924
2017-12-10T13:51:36.973037: step 2858, loss 0.226327, acc 0.953125, prec 0.0518429, recall 0.857034
2017-12-10T13:51:37.161796: step 2859, loss 0.312209, acc 0.90625, prec 0.0518358, recall 0.857034
2017-12-10T13:51:37.350182: step 2860, loss 0.307388, acc 0.890625, prec 0.0518274, recall 0.857034
2017-12-10T13:51:37.541020: step 2861, loss 0.165514, acc 0.9375, prec 0.0518226, recall 0.857034
2017-12-10T13:51:37.727954: step 2862, loss 0.421329, acc 0.90625, prec 0.0518373, recall 0.857088
2017-12-10T13:51:37.915918: step 2863, loss 0.503577, acc 0.859375, prec 0.0518922, recall 0.857252
2017-12-10T13:51:38.108298: step 2864, loss 0.364358, acc 0.890625, prec 0.0518839, recall 0.857252
2017-12-10T13:51:38.298530: step 2865, loss 0.348272, acc 0.890625, prec 0.0518755, recall 0.857252
2017-12-10T13:51:38.484151: step 2866, loss 0.19623, acc 0.921875, prec 0.0518695, recall 0.857252
2017-12-10T13:51:38.672778: step 2867, loss 0.669009, acc 0.84375, prec 0.0519013, recall 0.857361
2017-12-10T13:51:38.866027: step 2868, loss 1.10057, acc 0.921875, prec 0.0519172, recall 0.857415
2017-12-10T13:51:39.056737: step 2869, loss 0.382436, acc 0.90625, prec 0.05191, recall 0.857415
2017-12-10T13:51:39.246571: step 2870, loss 0.0978056, acc 0.953125, prec 0.051972, recall 0.857578
2017-12-10T13:51:39.438321: step 2871, loss 0.0654346, acc 0.96875, prec 0.0519696, recall 0.857578
2017-12-10T13:51:39.627275: step 2872, loss 2.04632, acc 0.9375, prec 0.0519879, recall 0.857306
2017-12-10T13:51:39.819294: step 2873, loss 0.254632, acc 0.890625, prec 0.0519795, recall 0.857306
2017-12-10T13:51:40.014019: step 2874, loss 0.491901, acc 0.84375, prec 0.0519675, recall 0.857306
2017-12-10T13:51:40.202010: step 2875, loss 0.344946, acc 0.921875, prec 0.0519834, recall 0.85736
2017-12-10T13:51:40.393040: step 2876, loss 0.146333, acc 0.96875, prec 0.0520029, recall 0.857414
2017-12-10T13:51:40.581773: step 2877, loss 0.274858, acc 0.90625, prec 0.0520175, recall 0.857469
2017-12-10T13:51:40.770571: step 2878, loss 0.545616, acc 0.828125, prec 0.0520043, recall 0.857469
2017-12-10T13:51:40.958980: step 2879, loss 0.524238, acc 0.875, prec 0.0520166, recall 0.857523
2017-12-10T13:51:41.149712: step 2880, loss 0.365924, acc 0.875, prec 0.052007, recall 0.857523
2017-12-10T13:51:41.337107: step 2881, loss 0.338609, acc 0.875, prec 0.0520193, recall 0.857577
2017-12-10T13:51:41.528940: step 2882, loss 0.182588, acc 0.890625, prec 0.0520109, recall 0.857577
2017-12-10T13:51:41.714886: step 2883, loss 0.547193, acc 0.828125, prec 0.0519977, recall 0.857577
2017-12-10T13:51:41.902335: step 2884, loss 0.414707, acc 0.859375, prec 0.0519869, recall 0.857577
2017-12-10T13:51:42.092744: step 2885, loss 0.565773, acc 0.859375, prec 0.0519762, recall 0.857577
2017-12-10T13:51:42.277810: step 2886, loss 0.303633, acc 0.90625, prec 0.051969, recall 0.857577
2017-12-10T13:51:42.468412: step 2887, loss 0.587469, acc 0.828125, prec 0.0519994, recall 0.857685
2017-12-10T13:51:42.658291: step 2888, loss 0.347133, acc 0.890625, prec 0.0520129, recall 0.857739
2017-12-10T13:51:42.848191: step 2889, loss 0.113175, acc 0.9375, prec 0.0520081, recall 0.857739
2017-12-10T13:51:43.037995: step 2890, loss 0.497731, acc 0.875, prec 0.0519985, recall 0.857739
2017-12-10T13:51:43.225752: step 2891, loss 0.200847, acc 0.90625, prec 0.0520567, recall 0.857901
2017-12-10T13:51:43.415843: step 2892, loss 0.088615, acc 1, prec 0.0520785, recall 0.857955
2017-12-10T13:51:43.605593: step 2893, loss 0.328689, acc 0.875, prec 0.052069, recall 0.857955
2017-12-10T13:51:43.800509: step 2894, loss 0.083646, acc 0.984375, prec 0.0520896, recall 0.858008
2017-12-10T13:51:43.991596: step 2895, loss 0.0776986, acc 0.96875, prec 0.0520872, recall 0.858008
2017-12-10T13:51:44.179797: step 2896, loss 0.221512, acc 0.96875, prec 0.0521283, recall 0.858116
2017-12-10T13:51:44.365012: step 2897, loss 0.0776357, acc 0.984375, prec 0.0521707, recall 0.858223
2017-12-10T13:51:44.555438: step 2898, loss 0.282773, acc 0.921875, prec 0.0522083, recall 0.85833
2017-12-10T13:51:44.748743: step 2899, loss 0.209847, acc 0.9375, prec 0.052247, recall 0.858437
2017-12-10T13:51:44.943326: step 2900, loss 0.214608, acc 0.890625, prec 0.0522386, recall 0.858437
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_0/1512931325/checkpoints/model-2900

2017-12-10T13:51:46.213457: step 2901, loss 0.016626, acc 1, prec 0.0522386, recall 0.858437
2017-12-10T13:51:46.401328: step 2902, loss 0.130818, acc 0.953125, prec 0.0522786, recall 0.858544
2017-12-10T13:51:46.590448: step 2903, loss 0.0413461, acc 0.984375, prec 0.0522774, recall 0.858544
2017-12-10T13:51:46.783813: step 2904, loss 6.88075, acc 0.9375, prec 0.0522738, recall 0.85822
2017-12-10T13:51:46.972941: step 2905, loss 0.255619, acc 0.96875, prec 0.0522931, recall 0.858274
2017-12-10T13:51:47.163408: step 2906, loss 0.627333, acc 0.984375, prec 0.0523355, recall 0.85838
2017-12-10T13:51:47.357190: step 2907, loss 0.40686, acc 0.90625, prec 0.0523283, recall 0.85838
2017-12-10T13:51:47.545095: step 2908, loss 0.364113, acc 0.90625, prec 0.0523863, recall 0.85854
2017-12-10T13:51:47.733152: step 2909, loss 0.265359, acc 0.890625, prec 0.0523996, recall 0.858593
2017-12-10T13:51:47.919635: step 2910, loss 0.180827, acc 0.953125, prec 0.052396, recall 0.858593
2017-12-10T13:51:48.115167: step 2911, loss 1.04767, acc 0.921875, prec 0.0524118, recall 0.858647
2017-12-10T13:51:48.309313: step 2912, loss 0.292344, acc 0.921875, prec 0.0524275, recall 0.8587
2017-12-10T13:51:48.496125: step 2913, loss 0.291832, acc 0.9375, prec 0.0524662, recall 0.858806
2017-12-10T13:51:48.687046: step 2914, loss 0.223006, acc 0.921875, prec 0.0524819, recall 0.858859
2017-12-10T13:51:48.876238: step 2915, loss 0.313316, acc 0.890625, prec 0.0525169, recall 0.858965
2017-12-10T13:51:49.064415: step 2916, loss 0.570088, acc 0.875, prec 0.052529, recall 0.859018
2017-12-10T13:51:49.254921: step 2917, loss 0.217861, acc 0.90625, prec 0.0525435, recall 0.85907
2017-12-10T13:51:49.448555: step 2918, loss 0.543499, acc 0.875, prec 0.0525339, recall 0.85907
2017-12-10T13:51:49.634408: step 2919, loss 0.344605, acc 0.890625, prec 0.0525254, recall 0.85907
2017-12-10T13:51:49.821187: step 2920, loss 0.474495, acc 0.875, prec 0.0525158, recall 0.85907
2017-12-10T13:51:50.007455: step 2921, loss 0.298053, acc 0.890625, prec 0.0525074, recall 0.85907
2017-12-10T13:51:50.195053: step 2922, loss 0.492065, acc 0.859375, prec 0.0525183, recall 0.859123
2017-12-10T13:51:50.385923: step 2923, loss 0.221442, acc 0.921875, prec 0.0525123, recall 0.859123
2017-12-10T13:51:50.572655: step 2924, loss 0.391264, acc 0.890625, prec 0.0525038, recall 0.859123
2017-12-10T13:51:50.763552: step 2925, loss 1.2953, acc 0.890625, prec 0.0525388, recall 0.859229
2017-12-10T13:51:50.958854: step 2926, loss 0.4367, acc 0.84375, prec 0.0525268, recall 0.859229
2017-12-10T13:51:51.150715: step 2927, loss 0.204347, acc 0.90625, prec 0.0525196, recall 0.859229
2017-12-10T13:51:51.338303: step 2928, loss 0.639416, acc 0.875, prec 0.0525316, recall 0.859281
2017-12-10T13:51:51.526850: step 2929, loss 0.293736, acc 0.921875, prec 0.052569, recall 0.859387
2017-12-10T13:51:51.714289: step 2930, loss 0.502718, acc 0.921875, prec 0.052628, recall 0.859544
2017-12-10T13:51:51.901336: step 2931, loss 0.519801, acc 0.890625, prec 0.0526629, recall 0.859649
2017-12-10T13:51:52.091212: step 2932, loss 1.77225, acc 0.875, prec 0.0526761, recall 0.859381
2017-12-10T13:51:52.281286: step 2933, loss 0.318251, acc 0.890625, prec 0.0526893, recall 0.859433
2017-12-10T13:51:52.469530: step 2934, loss 0.3858, acc 0.90625, prec 0.0527687, recall 0.859643
2017-12-10T13:51:52.660384: step 2935, loss 0.626039, acc 0.859375, prec 0.0528011, recall 0.859747
2017-12-10T13:51:52.850610: step 2936, loss 0.300004, acc 0.84375, prec 0.0527891, recall 0.859747
2017-12-10T13:51:53.037027: step 2937, loss 0.144539, acc 0.984375, prec 0.0528528, recall 0.859903
2017-12-10T13:51:53.225360: step 2938, loss 0.320474, acc 0.859375, prec 0.0528419, recall 0.859903
2017-12-10T13:51:53.412876: step 2939, loss 0.275161, acc 0.9375, prec 0.0529452, recall 0.860163
2017-12-10T13:51:53.600123: step 2940, loss 0.559546, acc 0.9375, prec 0.0530052, recall 0.860319
2017-12-10T13:51:53.792148: step 2941, loss 0.394953, acc 0.875, prec 0.0529955, recall 0.860319
2017-12-10T13:51:53.978620: step 2942, loss 0.223008, acc 0.9375, prec 0.0529907, recall 0.860319
2017-12-10T13:51:54.168092: step 2943, loss 0.345274, acc 0.96875, prec 0.0530747, recall 0.860525
2017-12-10T13:51:54.359152: step 2944, loss 0.383116, acc 0.90625, prec 0.0530891, recall 0.860577
2017-12-10T13:51:54.551209: step 2945, loss 0.19442, acc 0.921875, prec 0.053083, recall 0.860577
2017-12-10T13:51:54.738714: step 2946, loss 0.336974, acc 0.90625, prec 0.0530757, recall 0.860577
2017-12-10T13:51:54.927539: step 2947, loss 0.786799, acc 0.9375, prec 0.0531141, recall 0.86068
2017-12-10T13:51:55.120381: step 2948, loss 0.204835, acc 0.953125, prec 0.0531105, recall 0.86068
2017-12-10T13:51:55.310672: step 2949, loss 0.146662, acc 0.9375, prec 0.0531272, recall 0.860731
2017-12-10T13:51:55.502359: step 2950, loss 0.339047, acc 0.921875, prec 0.0531212, recall 0.860731
2017-12-10T13:51:55.690232: step 2951, loss 0.700395, acc 0.921875, prec 0.0531583, recall 0.860834
2017-12-10T13:51:55.876314: step 2952, loss 0.231575, acc 0.921875, prec 0.0531522, recall 0.860834
2017-12-10T13:51:56.064271: step 2953, loss 0.224961, acc 0.9375, prec 0.0531689, recall 0.860886
2017-12-10T13:51:56.257191: step 2954, loss 0.219627, acc 0.921875, prec 0.0531845, recall 0.860937
2017-12-10T13:51:56.445517: step 2955, loss 1.33625, acc 0.90625, prec 0.0531988, recall 0.860988
2017-12-10T13:51:56.633808: step 2956, loss 0.281184, acc 0.921875, prec 0.0531927, recall 0.860988
2017-12-10T13:51:56.820045: step 2957, loss 0.585193, acc 0.890625, prec 0.0531842, recall 0.860988
2017-12-10T13:51:57.007127: step 2958, loss 0.488782, acc 0.828125, prec 0.0531709, recall 0.860988
2017-12-10T13:51:57.194822: step 2959, loss 0.266972, acc 0.921875, prec 0.0531648, recall 0.860988
2017-12-10T13:51:57.379406: step 2960, loss 0.238709, acc 0.90625, prec 0.0531791, recall 0.861039
2017-12-10T13:51:57.567334: step 2961, loss 0.263993, acc 0.890625, prec 0.0531922, recall 0.861091
2017-12-10T13:51:57.759111: step 2962, loss 0.172898, acc 0.9375, prec 0.0531874, recall 0.861091
2017-12-10T13:51:57.949787: step 2963, loss 0.363265, acc 0.875, prec 0.0531777, recall 0.861091
2017-12-10T13:51:58.137465: step 2964, loss 0.268719, acc 0.90625, prec 0.0531704, recall 0.861091
2017-12-10T13:51:58.328869: step 2965, loss 0.210748, acc 0.9375, prec 0.0531871, recall 0.861142
2017-12-10T13:51:58.518414: step 2966, loss 0.253192, acc 0.953125, prec 0.0531835, recall 0.861142
2017-12-10T13:51:58.703387: step 2967, loss 0.208965, acc 0.9375, prec 0.0532002, recall 0.861193
2017-12-10T13:51:58.897531: step 2968, loss 0.167134, acc 0.953125, prec 0.0532181, recall 0.861244
2017-12-10T13:51:59.087716: step 2969, loss 0.112431, acc 0.921875, prec 0.0532121, recall 0.861244
2017-12-10T13:51:59.277326: step 2970, loss 0.614041, acc 0.9375, prec 0.0532287, recall 0.861295
2017-12-10T13:51:59.470247: step 2971, loss 0.102095, acc 0.953125, prec 0.0532251, recall 0.861295
2017-12-10T13:51:59.656230: step 2972, loss 0.0868654, acc 0.9375, prec 0.0532418, recall 0.861346
2017-12-10T13:51:59.842658: step 2973, loss 0.132458, acc 0.9375, prec 0.053237, recall 0.861346
2017-12-10T13:52:00.030077: step 2974, loss 0.0198615, acc 1, prec 0.053237, recall 0.861346
2017-12-10T13:52:00.213090: step 2975, loss 1.29354, acc 0.96875, prec 0.0532776, recall 0.861448
2017-12-10T13:52:00.405592: step 2976, loss 0.0951705, acc 0.953125, prec 0.0532739, recall 0.861448
2017-12-10T13:52:00.589347: step 2977, loss 0.20636, acc 0.9375, prec 0.0533121, recall 0.86155
2017-12-10T13:52:00.779299: step 2978, loss 0.14607, acc 0.984375, prec 0.0533109, recall 0.86155
2017-12-10T13:52:00.967977: step 2979, loss 0.0554346, acc 0.984375, prec 0.0533097, recall 0.86155
2017-12-10T13:52:01.160621: step 2980, loss 0.146285, acc 0.953125, prec 0.0533276, recall 0.861601
2017-12-10T13:52:01.355747: step 2981, loss 0.226985, acc 0.9375, prec 0.0533227, recall 0.861601
2017-12-10T13:52:01.525382: step 2982, loss 0.500366, acc 0.882353, prec 0.0533155, recall 0.861601
2017-12-10T13:52:01.722359: step 2983, loss 2.70782, acc 0.9375, prec 0.0533118, recall 0.861284
2017-12-10T13:52:01.911737: step 2984, loss 0.174109, acc 0.9375, prec 0.0533285, recall 0.861335
2017-12-10T13:52:02.100120: step 2985, loss 0.0850566, acc 0.96875, prec 0.0533691, recall 0.861437
2017-12-10T13:52:02.288220: step 2986, loss 0.525123, acc 0.890625, prec 0.0533821, recall 0.861488
2017-12-10T13:52:02.482362: step 2987, loss 0.288997, acc 0.90625, prec 0.0533748, recall 0.861488
2017-12-10T13:52:02.673370: step 2988, loss 0.269652, acc 0.90625, prec 0.053389, recall 0.861538
2017-12-10T13:52:02.863794: step 2989, loss 0.346916, acc 0.921875, prec 0.0534044, recall 0.861589
2017-12-10T13:52:03.055367: step 2990, loss 0.432376, acc 0.875, prec 0.0533948, recall 0.861589
2017-12-10T13:52:03.243364: step 2991, loss 0.566069, acc 0.828125, prec 0.0534029, recall 0.86164
2017-12-10T13:52:03.432201: step 2992, loss 0.325556, acc 0.90625, prec 0.0534386, recall 0.861741
2017-12-10T13:52:03.618146: step 2993, loss 0.153069, acc 0.953125, prec 0.0534779, recall 0.861842
2017-12-10T13:52:03.805709: step 2994, loss 0.50015, acc 0.859375, prec 0.053467, recall 0.861842
2017-12-10T13:52:03.994852: step 2995, loss 0.221576, acc 0.90625, prec 0.0534812, recall 0.861893
2017-12-10T13:52:04.182687: step 2996, loss 0.505289, acc 0.84375, prec 0.053469, recall 0.861893
2017-12-10T13:52:04.372272: step 2997, loss 0.238036, acc 0.90625, prec 0.0534832, recall 0.861943
2017-12-10T13:52:04.560409: step 2998, loss 0.230453, acc 0.9375, prec 0.0534998, recall 0.861993
2017-12-10T13:52:04.748378: step 2999, loss 0.235952, acc 0.9375, prec 0.0535164, recall 0.862044
2017-12-10T13:52:04.937084: step 3000, loss 0.217853, acc 0.953125, prec 0.0535557, recall 0.862144
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_0/1512931325/checkpoints/model-3000

2017-12-10T13:52:06.170340: step 3001, loss 0.377111, acc 0.921875, prec 0.053571, recall 0.862195
2017-12-10T13:52:06.361073: step 3002, loss 0.125139, acc 0.953125, prec 0.0535674, recall 0.862195
2017-12-10T13:52:06.546264: step 3003, loss 0.177672, acc 0.90625, prec 0.0535815, recall 0.862245
2017-12-10T13:52:06.734919: step 3004, loss 0.145957, acc 0.9375, prec 0.0535767, recall 0.862245
2017-12-10T13:52:06.927443: step 3005, loss 0.225639, acc 0.890625, prec 0.0535682, recall 0.862245
2017-12-10T13:52:07.114644: step 3006, loss 0.0802067, acc 0.96875, prec 0.0535658, recall 0.862245
2017-12-10T13:52:07.306127: step 3007, loss 0.351793, acc 0.90625, prec 0.0535799, recall 0.862295
2017-12-10T13:52:07.495217: step 3008, loss 0.175092, acc 0.96875, prec 0.0535775, recall 0.862295
2017-12-10T13:52:07.683661: step 3009, loss 0.389661, acc 0.9375, prec 0.0535726, recall 0.862295
2017-12-10T13:52:07.869318: step 3010, loss 0.0625538, acc 0.96875, prec 0.0535702, recall 0.862295
2017-12-10T13:52:08.062600: step 3011, loss 0.262652, acc 0.921875, prec 0.0536284, recall 0.862445
2017-12-10T13:52:08.249439: step 3012, loss 0.161616, acc 0.96875, prec 0.0536474, recall 0.862495
2017-12-10T13:52:08.437272: step 3013, loss 0.123633, acc 1, prec 0.0536688, recall 0.862545
2017-12-10T13:52:08.632948: step 3014, loss 1.31245, acc 0.953125, prec 0.0537294, recall 0.862695
2017-12-10T13:52:08.824544: step 3015, loss 0.263023, acc 0.953125, prec 0.0537471, recall 0.862745
2017-12-10T13:52:09.014699: step 3016, loss 2.22322, acc 0.9375, prec 0.0537863, recall 0.862532
2017-12-10T13:52:09.205269: step 3017, loss 0.266393, acc 0.9375, prec 0.0538028, recall 0.862582
2017-12-10T13:52:09.394228: step 3018, loss 0.279717, acc 0.921875, prec 0.0538181, recall 0.862631
2017-12-10T13:52:09.584660: step 3019, loss 0.161455, acc 0.921875, prec 0.0538121, recall 0.862631
2017-12-10T13:52:09.777238: step 3020, loss 0.393501, acc 0.921875, prec 0.0538488, recall 0.862731
2017-12-10T13:52:09.965346: step 3021, loss 0.405046, acc 0.84375, prec 0.0538794, recall 0.86283
2017-12-10T13:52:10.155457: step 3022, loss 0.220215, acc 0.953125, prec 0.0538971, recall 0.86288
2017-12-10T13:52:10.344828: step 3023, loss 0.240556, acc 0.90625, prec 0.0538898, recall 0.86288
2017-12-10T13:52:10.537594: step 3024, loss 0.134335, acc 0.96875, prec 0.0539301, recall 0.862979
2017-12-10T13:52:10.727625: step 3025, loss 0.544714, acc 0.84375, prec 0.0539393, recall 0.863029
2017-12-10T13:52:10.917519: step 3026, loss 0.510395, acc 0.890625, prec 0.0539735, recall 0.863127
2017-12-10T13:52:11.105066: step 3027, loss 0.39804, acc 0.875, prec 0.0539851, recall 0.863177
2017-12-10T13:52:11.295256: step 3028, loss 0.54768, acc 0.890625, prec 0.0539979, recall 0.863226
2017-12-10T13:52:11.481896: step 3029, loss 0.66011, acc 0.828125, prec 0.0540059, recall 0.863276
2017-12-10T13:52:11.665721: step 3030, loss 0.410613, acc 0.859375, prec 0.0540162, recall 0.863325
2017-12-10T13:52:11.851485: step 3031, loss 0.558828, acc 0.890625, prec 0.0540077, recall 0.863325
2017-12-10T13:52:12.039943: step 3032, loss 0.251033, acc 0.875, prec 0.0540193, recall 0.863374
2017-12-10T13:52:12.224981: step 3033, loss 0.34334, acc 0.90625, prec 0.054012, recall 0.863374
2017-12-10T13:52:12.414499: step 3034, loss 0.175137, acc 0.9375, prec 0.0540071, recall 0.863374
2017-12-10T13:52:12.598220: step 3035, loss 0.328545, acc 0.90625, prec 0.0540211, recall 0.863423
2017-12-10T13:52:12.782066: step 3036, loss 0.216296, acc 0.921875, prec 0.054079, recall 0.863571
2017-12-10T13:52:12.973120: step 3037, loss 0.260301, acc 0.890625, prec 0.0540918, recall 0.86362
2017-12-10T13:52:13.162709: step 3038, loss 0.2748, acc 0.890625, prec 0.0541046, recall 0.863669
2017-12-10T13:52:13.349196: step 3039, loss 0.201877, acc 0.921875, prec 0.0541411, recall 0.863767
2017-12-10T13:52:13.535729: step 3040, loss 0.0889998, acc 0.984375, prec 0.0541399, recall 0.863767
2017-12-10T13:52:13.731356: step 3041, loss 2.40078, acc 0.9375, prec 0.0542002, recall 0.863604
2017-12-10T13:52:13.939056: step 3042, loss 0.502018, acc 0.9375, prec 0.0542379, recall 0.863702
2017-12-10T13:52:14.131132: step 3043, loss 0.043365, acc 0.984375, prec 0.0542367, recall 0.863702
2017-12-10T13:52:14.317537: step 3044, loss 0.610566, acc 0.859375, prec 0.0542257, recall 0.863702
2017-12-10T13:52:14.502957: step 3045, loss 0.138082, acc 0.953125, prec 0.054222, recall 0.863702
2017-12-10T13:52:14.692777: step 3046, loss 0.298734, acc 0.875, prec 0.0542123, recall 0.863702
2017-12-10T13:52:14.882372: step 3047, loss 0.350885, acc 0.875, prec 0.0542451, recall 0.863799
2017-12-10T13:52:15.073139: step 3048, loss 0.29666, acc 0.921875, prec 0.0542815, recall 0.863897
2017-12-10T13:52:15.259342: step 3049, loss 0.0761898, acc 0.953125, prec 0.0543204, recall 0.863994
2017-12-10T13:52:15.446864: step 3050, loss 0.0312177, acc 0.984375, prec 0.0543405, recall 0.864043
2017-12-10T13:52:15.639342: step 3051, loss 0.174989, acc 0.9375, prec 0.0543569, recall 0.864092
2017-12-10T13:52:15.830498: step 3052, loss 0.0667399, acc 0.96875, prec 0.0543544, recall 0.864092
2017-12-10T13:52:16.021967: step 3053, loss 0.309559, acc 0.9375, prec 0.0543708, recall 0.86414
2017-12-10T13:52:16.213239: step 3054, loss 0.577361, acc 0.9375, prec 0.0544297, recall 0.864286
2017-12-10T13:52:16.401280: step 3055, loss 0.253962, acc 0.921875, prec 0.0544449, recall 0.864334
2017-12-10T13:52:16.590384: step 3056, loss 0.0917727, acc 0.96875, prec 0.0544637, recall 0.864383
2017-12-10T13:52:16.777714: step 3057, loss 0.255991, acc 0.921875, prec 0.0544788, recall 0.864431
2017-12-10T13:52:16.970991: step 3058, loss 0.107254, acc 0.953125, prec 0.0544751, recall 0.864431
2017-12-10T13:52:17.162296: step 3059, loss 0.134999, acc 0.9375, prec 0.0544702, recall 0.864431
2017-12-10T13:52:17.349146: step 3060, loss 0.546362, acc 0.859375, prec 0.0544805, recall 0.864479
2017-12-10T13:52:17.536895: step 3061, loss 0.195248, acc 0.953125, prec 0.0544981, recall 0.864528
2017-12-10T13:52:17.725592: step 3062, loss 0.977638, acc 0.90625, prec 0.054512, recall 0.864576
2017-12-10T13:52:17.918923: step 3063, loss 0.970119, acc 0.890625, prec 0.0545459, recall 0.864672
2017-12-10T13:52:18.110568: step 3064, loss 0.361943, acc 0.921875, prec 0.0545397, recall 0.864672
2017-12-10T13:52:18.299515: step 3065, loss 0.213408, acc 0.9375, prec 0.0545773, recall 0.864769
2017-12-10T13:52:18.487779: step 3066, loss 0.0866493, acc 0.96875, prec 0.0545749, recall 0.864769
2017-12-10T13:52:18.677129: step 3067, loss 0.270104, acc 0.921875, prec 0.0546112, recall 0.864865
2017-12-10T13:52:18.866875: step 3068, loss 0.361682, acc 0.90625, prec 0.0546463, recall 0.864961
2017-12-10T13:52:19.052666: step 3069, loss 0.405757, acc 0.90625, prec 0.0546601, recall 0.865009
2017-12-10T13:52:19.239378: step 3070, loss 0.356672, acc 0.890625, prec 0.0546516, recall 0.865009
2017-12-10T13:52:19.425862: step 3071, loss 0.34634, acc 0.90625, prec 0.0546442, recall 0.865009
2017-12-10T13:52:19.612987: step 3072, loss 0.142401, acc 0.921875, prec 0.0546381, recall 0.865009
2017-12-10T13:52:19.805694: step 3073, loss 0.169486, acc 0.9375, prec 0.0546968, recall 0.865153
2017-12-10T13:52:19.996563: step 3074, loss 0.157441, acc 0.953125, prec 0.0546931, recall 0.865153
2017-12-10T13:52:20.187636: step 3075, loss 0.126844, acc 0.953125, prec 0.0547318, recall 0.865248
2017-12-10T13:52:20.375370: step 3076, loss 0.222658, acc 0.9375, prec 0.0547481, recall 0.865296
2017-12-10T13:52:20.565369: step 3077, loss 0.0928176, acc 0.953125, prec 0.0547656, recall 0.865344
2017-12-10T13:52:20.750744: step 3078, loss 0.105457, acc 0.953125, prec 0.0547832, recall 0.865391
2017-12-10T13:52:20.942900: step 3079, loss 0.155417, acc 0.96875, prec 0.0547807, recall 0.865391
2017-12-10T13:52:21.130246: step 3080, loss 0.0672473, acc 0.984375, prec 0.0548007, recall 0.865439
2017-12-10T13:52:21.319529: step 3081, loss 0.399114, acc 0.890625, prec 0.0548133, recall 0.865487
2017-12-10T13:52:21.508516: step 3082, loss 0.0702839, acc 0.984375, prec 0.0548544, recall 0.865582
2017-12-10T13:52:21.699072: step 3083, loss 0.0550718, acc 0.984375, prec 0.0548532, recall 0.865582
2017-12-10T13:52:21.885465: step 3084, loss 0.116298, acc 0.96875, prec 0.0548507, recall 0.865582
2017-12-10T13:52:22.074534: step 3085, loss 0.209019, acc 0.921875, prec 0.0548657, recall 0.865629
2017-12-10T13:52:22.268594: step 3086, loss 0.173311, acc 0.953125, prec 0.0549044, recall 0.865724
2017-12-10T13:52:22.459174: step 3087, loss 0.0744345, acc 0.984375, prec 0.0549244, recall 0.865772
2017-12-10T13:52:22.649968: step 3088, loss 0.0994197, acc 0.96875, prec 0.0549219, recall 0.865772
2017-12-10T13:52:22.840825: step 3089, loss 0.070382, acc 0.96875, prec 0.054983, recall 0.865914
2017-12-10T13:52:23.026825: step 3090, loss 1.06963, acc 1, prec 0.0550253, recall 0.866008
2017-12-10T13:52:23.217222: step 3091, loss 0.0735965, acc 0.953125, prec 0.0550216, recall 0.866008
2017-12-10T13:52:23.405210: step 3092, loss 0.229849, acc 0.953125, prec 0.0550179, recall 0.866008
2017-12-10T13:52:23.596214: step 3093, loss 0.175367, acc 0.9375, prec 0.0550342, recall 0.866056
2017-12-10T13:52:23.781066: step 3094, loss 0.0840625, acc 0.953125, prec 0.0550305, recall 0.866056
2017-12-10T13:52:23.971047: step 3095, loss 0.336921, acc 0.984375, prec 0.0550716, recall 0.86615
2017-12-10T13:52:24.161070: step 3096, loss 0.281341, acc 0.9375, prec 0.0550878, recall 0.866197
2017-12-10T13:52:24.349473: step 3097, loss 0.127917, acc 0.96875, prec 0.0551276, recall 0.866291
2017-12-10T13:52:24.535343: step 3098, loss 0.116462, acc 0.9375, prec 0.0551227, recall 0.866291
2017-12-10T13:52:24.722087: step 3099, loss 0.32718, acc 0.921875, prec 0.0551165, recall 0.866291
2017-12-10T13:52:24.912220: step 3100, loss 0.163524, acc 0.9375, prec 0.0551116, recall 0.866291
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_0/1512931325/checkpoints/model-3100

2017-12-10T13:52:26.068009: step 3101, loss 0.126629, acc 0.9375, prec 0.0551278, recall 0.866338
2017-12-10T13:52:26.254117: step 3102, loss 0.290599, acc 0.953125, prec 0.0551875, recall 0.866479
2017-12-10T13:52:26.439219: step 3103, loss 0.0107462, acc 1, prec 0.0551875, recall 0.866479
2017-12-10T13:52:26.628818: step 3104, loss 0.112724, acc 0.953125, prec 0.055205, recall 0.866526
2017-12-10T13:52:26.818522: step 3105, loss 1.12835, acc 0.9375, prec 0.0552423, recall 0.86662
2017-12-10T13:52:27.010194: step 3106, loss 0.0164643, acc 1, prec 0.0552635, recall 0.866667
2017-12-10T13:52:27.197879: step 3107, loss 0.0946928, acc 0.96875, prec 0.0552821, recall 0.866713
2017-12-10T13:52:27.390621: step 3108, loss 0.0923853, acc 0.953125, prec 0.0553207, recall 0.866807
2017-12-10T13:52:27.577413: step 3109, loss 0.10911, acc 0.953125, prec 0.055317, recall 0.866807
2017-12-10T13:52:27.763890: step 3110, loss 1.46627, acc 0.9375, prec 0.0553344, recall 0.86655
2017-12-10T13:52:27.954609: step 3111, loss 0.254512, acc 0.921875, prec 0.0553282, recall 0.86655
2017-12-10T13:52:28.140587: step 3112, loss 0.121514, acc 0.953125, prec 0.0553456, recall 0.866597
2017-12-10T13:52:28.330443: step 3113, loss 0.11596, acc 0.984375, prec 0.0553866, recall 0.86669
2017-12-10T13:52:28.520267: step 3114, loss 0.356778, acc 0.890625, prec 0.0553991, recall 0.866737
2017-12-10T13:52:28.713624: step 3115, loss 0.195045, acc 0.90625, prec 0.0554127, recall 0.866783
2017-12-10T13:52:28.907137: step 3116, loss 0.475146, acc 0.90625, prec 0.0554053, recall 0.866783
2017-12-10T13:52:29.096718: step 3117, loss 0.257984, acc 0.90625, prec 0.0553979, recall 0.866783
2017-12-10T13:52:29.286086: step 3118, loss 0.212672, acc 0.921875, prec 0.0553917, recall 0.866783
2017-12-10T13:52:29.473982: step 3119, loss 0.404568, acc 0.890625, prec 0.0554041, recall 0.86683
2017-12-10T13:52:29.661486: step 3120, loss 0.244674, acc 0.875, prec 0.0554153, recall 0.866876
2017-12-10T13:52:29.850924: step 3121, loss 0.365165, acc 0.921875, prec 0.0554513, recall 0.866969
2017-12-10T13:52:30.040314: step 3122, loss 0.229999, acc 0.9375, prec 0.0554675, recall 0.867016
2017-12-10T13:52:30.227921: step 3123, loss 0.315959, acc 0.890625, prec 0.055501, recall 0.867108
2017-12-10T13:52:30.417471: step 3124, loss 0.119564, acc 0.984375, prec 0.0555419, recall 0.867201
2017-12-10T13:52:30.607645: step 3125, loss 0.175182, acc 0.90625, prec 0.0555766, recall 0.867294
2017-12-10T13:52:30.797405: step 3126, loss 0.0252516, acc 0.984375, prec 0.0555754, recall 0.867294
2017-12-10T13:52:30.988456: step 3127, loss 0.0971402, acc 0.96875, prec 0.055594, recall 0.86734
2017-12-10T13:52:31.182871: step 3128, loss 0.186519, acc 0.9375, prec 0.055589, recall 0.86734
2017-12-10T13:52:31.373108: step 3129, loss 0.144722, acc 0.9375, prec 0.0555841, recall 0.86734
2017-12-10T13:52:31.561626: step 3130, loss 0.497889, acc 0.90625, prec 0.0555766, recall 0.86734
2017-12-10T13:52:31.750108: step 3131, loss 0.309238, acc 0.921875, prec 0.0555704, recall 0.86734
2017-12-10T13:52:31.938433: step 3132, loss 0.341386, acc 0.953125, prec 0.0555878, recall 0.867386
2017-12-10T13:52:32.127532: step 3133, loss 0.209699, acc 0.9375, prec 0.055646, recall 0.867524
2017-12-10T13:52:32.316502: step 3134, loss 0.0242436, acc 1, prec 0.0556671, recall 0.86757
2017-12-10T13:52:32.503380: step 3135, loss 0.165242, acc 0.9375, prec 0.0556832, recall 0.867616
2017-12-10T13:52:32.690852: step 3136, loss 0.0612364, acc 1, prec 0.0557042, recall 0.867662
2017-12-10T13:52:32.887104: step 3137, loss 0.186142, acc 0.921875, prec 0.055698, recall 0.867662
2017-12-10T13:52:33.074550: step 3138, loss 0.203406, acc 0.96875, prec 0.0556955, recall 0.867662
2017-12-10T13:52:33.269295: step 3139, loss 2.31529, acc 0.96875, prec 0.0557574, recall 0.867499
2017-12-10T13:52:33.463445: step 3140, loss 0.148092, acc 0.953125, prec 0.0557537, recall 0.867499
2017-12-10T13:52:33.657599: step 3141, loss 0.0234732, acc 0.984375, prec 0.0557525, recall 0.867499
2017-12-10T13:52:33.843446: step 3142, loss 0.0658879, acc 0.953125, prec 0.0557487, recall 0.867499
2017-12-10T13:52:34.034822: step 3143, loss 0.205285, acc 0.921875, prec 0.0557425, recall 0.867499
2017-12-10T13:52:34.230088: step 3144, loss 0.372499, acc 0.890625, prec 0.055818, recall 0.867683
2017-12-10T13:52:34.420628: step 3145, loss 1.75649, acc 0.90625, prec 0.0558538, recall 0.867474
2017-12-10T13:52:34.610843: step 3146, loss 0.35662, acc 0.875, prec 0.0558649, recall 0.86752
2017-12-10T13:52:34.799855: step 3147, loss 0.603332, acc 0.859375, prec 0.0558537, recall 0.86752
2017-12-10T13:52:34.989172: step 3148, loss 0.304523, acc 0.9375, prec 0.0558908, recall 0.867611
2017-12-10T13:52:35.181286: step 3149, loss 0.217636, acc 0.9375, prec 0.0559068, recall 0.867657
2017-12-10T13:52:35.369618: step 3150, loss 0.52098, acc 0.875, prec 0.0559599, recall 0.867794
2017-12-10T13:52:35.557945: step 3151, loss 0.391122, acc 0.875, prec 0.05595, recall 0.867794
2017-12-10T13:52:35.745437: step 3152, loss 0.29139, acc 0.890625, prec 0.0559833, recall 0.867885
2017-12-10T13:52:35.933310: step 3153, loss 0.19272, acc 0.90625, prec 0.0560178, recall 0.867977
2017-12-10T13:52:36.120073: step 3154, loss 0.223445, acc 0.875, prec 0.0560288, recall 0.868022
2017-12-10T13:52:36.308803: step 3155, loss 0.273942, acc 0.890625, prec 0.0560621, recall 0.868113
2017-12-10T13:52:36.501354: step 3156, loss 0.0931471, acc 0.96875, prec 0.0560596, recall 0.868113
2017-12-10T13:52:36.691828: step 3157, loss 0.41223, acc 0.921875, prec 0.0561163, recall 0.868249
2017-12-10T13:52:36.880955: step 3158, loss 0.295112, acc 0.90625, prec 0.0561298, recall 0.868294
2017-12-10T13:52:37.072421: step 3159, loss 0.290038, acc 0.890625, prec 0.056184, recall 0.86843
2017-12-10T13:52:37.263756: step 3160, loss 0.353628, acc 0.890625, prec 0.0561963, recall 0.868475
2017-12-10T13:52:37.451260: step 3161, loss 0.458567, acc 0.84375, prec 0.0562257, recall 0.868566
2017-12-10T13:52:37.639752: step 3162, loss 0.420702, acc 0.875, prec 0.0562157, recall 0.868566
2017-12-10T13:52:37.825238: step 3163, loss 0.294643, acc 0.90625, prec 0.0562501, recall 0.868656
2017-12-10T13:52:38.013664: step 3164, loss 0.0724013, acc 0.953125, prec 0.0562673, recall 0.868701
2017-12-10T13:52:38.203501: step 3165, loss 0.0374001, acc 0.984375, prec 0.0562871, recall 0.868746
2017-12-10T13:52:38.393359: step 3166, loss 0.205909, acc 0.9375, prec 0.0562821, recall 0.868746
2017-12-10T13:52:38.585979: step 3167, loss 0.245084, acc 0.96875, prec 0.0563005, recall 0.868791
2017-12-10T13:52:38.774954: step 3168, loss 0.142837, acc 0.953125, prec 0.0563177, recall 0.868836
2017-12-10T13:52:38.963666: step 3169, loss 0.0894006, acc 0.96875, prec 0.0563362, recall 0.868881
2017-12-10T13:52:39.153901: step 3170, loss 2.95734, acc 0.90625, prec 0.0563299, recall 0.868583
2017-12-10T13:52:39.343875: step 3171, loss 0.132612, acc 0.953125, prec 0.0563471, recall 0.868628
2017-12-10T13:52:39.531791: step 3172, loss 0.214002, acc 0.9375, prec 0.056363, recall 0.868673
2017-12-10T13:52:39.720752: step 3173, loss 0.303622, acc 0.90625, prec 0.0563765, recall 0.868718
2017-12-10T13:52:39.909426: step 3174, loss 0.365844, acc 0.875, prec 0.0563874, recall 0.868763
2017-12-10T13:52:40.101973: step 3175, loss 0.107428, acc 0.9375, prec 0.0564242, recall 0.868852
2017-12-10T13:52:40.288768: step 3176, loss 0.0872801, acc 0.96875, prec 0.0564217, recall 0.868852
2017-12-10T13:52:40.473035: step 3177, loss 0.186041, acc 0.953125, prec 0.0564389, recall 0.868897
2017-12-10T13:52:40.661718: step 3178, loss 0.407987, acc 0.875, prec 0.0564707, recall 0.868987
2017-12-10T13:52:40.851293: step 3179, loss 0.145621, acc 0.90625, prec 0.0564632, recall 0.868987
2017-12-10T13:52:41.041468: step 3180, loss 0.4423, acc 0.859375, prec 0.056452, recall 0.868987
2017-12-10T13:52:41.232371: step 3181, loss 0.192343, acc 0.984375, prec 0.0564925, recall 0.869076
2017-12-10T13:52:41.424876: step 3182, loss 0.357323, acc 0.953125, prec 0.0564888, recall 0.869076
2017-12-10T13:52:41.611363: step 3183, loss 0.405252, acc 0.90625, prec 0.0565022, recall 0.869121
2017-12-10T13:52:41.800276: step 3184, loss 0.229626, acc 0.890625, prec 0.0565352, recall 0.86921
2017-12-10T13:52:41.985359: step 3185, loss 0.0621377, acc 0.96875, prec 0.0565327, recall 0.86921
2017-12-10T13:52:42.173455: step 3186, loss 0.0577047, acc 0.96875, prec 0.0565302, recall 0.86921
2017-12-10T13:52:42.362476: step 3187, loss 0.12317, acc 0.953125, prec 0.0565474, recall 0.869254
2017-12-10T13:52:42.551679: step 3188, loss 0.497469, acc 0.9375, prec 0.0565841, recall 0.869343
2017-12-10T13:52:42.742988: step 3189, loss 0.216884, acc 0.890625, prec 0.0565754, recall 0.869343
2017-12-10T13:52:42.930597: step 3190, loss 0.0681506, acc 0.96875, prec 0.0565937, recall 0.869388
2017-12-10T13:52:43.119575: step 3191, loss 0.0697431, acc 0.953125, prec 0.05659, recall 0.869388
2017-12-10T13:52:43.309838: step 3192, loss 0.0705924, acc 0.984375, prec 0.0565887, recall 0.869388
2017-12-10T13:52:43.498767: step 3193, loss 2.41687, acc 0.921875, prec 0.0566046, recall 0.869137
2017-12-10T13:52:43.690955: step 3194, loss 0.0523717, acc 0.984375, prec 0.0566034, recall 0.869137
2017-12-10T13:52:43.887227: step 3195, loss 0.269369, acc 0.953125, prec 0.0566414, recall 0.869226
2017-12-10T13:52:44.075257: step 3196, loss 0.0491677, acc 0.984375, prec 0.0566401, recall 0.869226
2017-12-10T13:52:44.263187: step 3197, loss 0.188423, acc 0.953125, prec 0.0566363, recall 0.869226
2017-12-10T13:52:44.453410: step 3198, loss 0.255935, acc 0.90625, prec 0.0566288, recall 0.869226
2017-12-10T13:52:44.639740: step 3199, loss 0.685498, acc 0.859375, prec 0.0566384, recall 0.86927
2017-12-10T13:52:44.825321: step 3200, loss 0.553599, acc 0.921875, prec 0.0566739, recall 0.869359
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_0/1512931325/checkpoints/model-3200

2017-12-10T13:52:46.039758: step 3201, loss 0.233649, acc 0.90625, prec 0.0566664, recall 0.869359
2017-12-10T13:52:46.230296: step 3202, loss 0.174544, acc 0.921875, prec 0.0566601, recall 0.869359
2017-12-10T13:52:46.417683: step 3203, loss 0.35557, acc 0.90625, prec 0.0566526, recall 0.869359
2017-12-10T13:52:46.610001: step 3204, loss 1.16655, acc 0.90625, prec 0.0566659, recall 0.869403
2017-12-10T13:52:46.803979: step 3205, loss 0.0863337, acc 0.96875, prec 0.0566634, recall 0.869403
2017-12-10T13:52:46.994295: step 3206, loss 0.403206, acc 0.859375, prec 0.0566522, recall 0.869403
2017-12-10T13:52:47.181808: step 3207, loss 0.351611, acc 0.921875, prec 0.0566876, recall 0.869492
2017-12-10T13:52:47.370563: step 3208, loss 0.210828, acc 0.90625, prec 0.0566801, recall 0.869492
2017-12-10T13:52:47.555642: step 3209, loss 0.128931, acc 0.96875, prec 0.0566776, recall 0.869492
2017-12-10T13:52:47.742988: step 3210, loss 0.292177, acc 0.90625, prec 0.0566701, recall 0.869492
2017-12-10T13:52:47.933098: step 3211, loss 0.0853394, acc 0.96875, prec 0.0566884, recall 0.869536
2017-12-10T13:52:48.130035: step 3212, loss 0.784745, acc 0.828125, prec 0.0566746, recall 0.869536
2017-12-10T13:52:48.318729: step 3213, loss 0.197148, acc 0.90625, prec 0.0566671, recall 0.869536
2017-12-10T13:52:48.505049: step 3214, loss 0.401564, acc 0.875, prec 0.0566571, recall 0.869536
2017-12-10T13:52:48.694673: step 3215, loss 0.411133, acc 0.90625, prec 0.0566496, recall 0.869536
2017-12-10T13:52:48.880728: step 3216, loss 0.319961, acc 0.921875, prec 0.0566642, recall 0.86958
2017-12-10T13:52:49.070036: step 3217, loss 0.113742, acc 0.953125, prec 0.0567021, recall 0.869668
2017-12-10T13:52:49.264035: step 3218, loss 0.348379, acc 0.90625, prec 0.0566945, recall 0.869668
2017-12-10T13:52:49.455375: step 3219, loss 0.0744419, acc 0.96875, prec 0.056692, recall 0.869668
2017-12-10T13:52:49.642980: step 3220, loss 0.159203, acc 0.921875, prec 0.0566858, recall 0.869668
2017-12-10T13:52:49.830871: step 3221, loss 0.501965, acc 0.9375, prec 0.0567224, recall 0.869756
2017-12-10T13:52:50.017950: step 3222, loss 0.22291, acc 0.953125, prec 0.0567395, recall 0.8698
2017-12-10T13:52:50.207460: step 3223, loss 0.139048, acc 0.9375, prec 0.0567969, recall 0.869932
2017-12-10T13:52:50.398664: step 3224, loss 0.0512333, acc 0.984375, prec 0.0567956, recall 0.869932
2017-12-10T13:52:50.586430: step 3225, loss 0.447303, acc 0.96875, prec 0.0568555, recall 0.870064
2017-12-10T13:52:50.774144: step 3226, loss 0.162399, acc 0.96875, prec 0.0568738, recall 0.870108
2017-12-10T13:52:50.962056: step 3227, loss 0.701895, acc 0.953125, prec 0.0568908, recall 0.870152
2017-12-10T13:52:51.153341: step 3228, loss 0.216643, acc 0.9375, prec 0.0569066, recall 0.870196
2017-12-10T13:52:51.341315: step 3229, loss 1.15541, acc 0.953125, prec 0.0569444, recall 0.870283
2017-12-10T13:52:51.535033: step 3230, loss 0.247839, acc 0.953125, prec 0.0569615, recall 0.870327
2017-12-10T13:52:51.726131: step 3231, loss 0.145816, acc 0.953125, prec 0.0569785, recall 0.87037
2017-12-10T13:52:51.912247: step 3232, loss 0.278382, acc 0.921875, prec 0.056993, recall 0.870414
2017-12-10T13:52:52.102429: step 3233, loss 0.0750321, acc 0.96875, prec 0.0569905, recall 0.870414
2017-12-10T13:52:52.291612: step 3234, loss 0.256423, acc 0.9375, prec 0.0569855, recall 0.870414
2017-12-10T13:52:52.476612: step 3235, loss 0.150573, acc 0.921875, prec 0.0569792, recall 0.870414
2017-12-10T13:52:52.668697: step 3236, loss 0.398204, acc 0.90625, prec 0.0569924, recall 0.870458
2017-12-10T13:52:52.862443: step 3237, loss 0.240522, acc 0.890625, prec 0.0570044, recall 0.870501
2017-12-10T13:52:53.052824: step 3238, loss 0.286357, acc 0.953125, prec 0.0570006, recall 0.870501
2017-12-10T13:52:53.240363: step 3239, loss 0.213764, acc 0.921875, prec 0.0569944, recall 0.870501
2017-12-10T13:52:53.429014: step 3240, loss 0.0991486, acc 0.9375, prec 0.0570309, recall 0.870588
2017-12-10T13:52:53.617354: step 3241, loss 0.155609, acc 0.9375, prec 0.0570466, recall 0.870632
2017-12-10T13:52:53.806409: step 3242, loss 0.155863, acc 0.9375, prec 0.0570416, recall 0.870632
2017-12-10T13:52:53.996115: step 3243, loss 0.0292958, acc 0.984375, prec 0.0570403, recall 0.870632
2017-12-10T13:52:54.188025: step 3244, loss 0.170493, acc 0.953125, prec 0.0570781, recall 0.870719
2017-12-10T13:52:54.381296: step 3245, loss 0.159747, acc 0.984375, prec 0.0570976, recall 0.870762
2017-12-10T13:52:54.578037: step 3246, loss 0.0784746, acc 0.96875, prec 0.0571366, recall 0.870849
2017-12-10T13:52:54.770040: step 3247, loss 0.104786, acc 0.96875, prec 0.0571341, recall 0.870849
2017-12-10T13:52:54.957308: step 3248, loss 0.126077, acc 0.9375, prec 0.057129, recall 0.870849
2017-12-10T13:52:55.145400: step 3249, loss 0.144023, acc 0.953125, prec 0.0571253, recall 0.870849
2017-12-10T13:52:55.333444: step 3250, loss 0.345091, acc 0.890625, prec 0.0571165, recall 0.870849
2017-12-10T13:52:55.523246: step 3251, loss 0.105341, acc 0.953125, prec 0.0571127, recall 0.870849
2017-12-10T13:52:55.714880: step 3252, loss 5.21747, acc 0.9375, prec 0.0571297, recall 0.8706
2017-12-10T13:52:55.907375: step 3253, loss 0.0991225, acc 0.96875, prec 0.0571271, recall 0.8706
2017-12-10T13:52:56.095868: step 3254, loss 0.0824132, acc 0.953125, prec 0.0571234, recall 0.8706
2017-12-10T13:52:56.285438: step 3255, loss 0.172435, acc 0.9375, prec 0.0571806, recall 0.87073
2017-12-10T13:52:56.471769: step 3256, loss 0.249708, acc 0.921875, prec 0.057195, recall 0.870773
2017-12-10T13:52:56.657981: step 3257, loss 0.150633, acc 0.921875, prec 0.0572094, recall 0.870817
2017-12-10T13:52:56.844751: step 3258, loss 0.199033, acc 0.9375, prec 0.0572251, recall 0.87086
2017-12-10T13:52:57.030941: step 3259, loss 0.598261, acc 0.953125, prec 0.0572421, recall 0.870903
2017-12-10T13:52:57.221536: step 3260, loss 0.489207, acc 0.859375, prec 0.0572722, recall 0.870989
2017-12-10T13:52:57.414763: step 3261, loss 0.255677, acc 0.890625, prec 0.0572841, recall 0.871032
2017-12-10T13:52:57.601193: step 3262, loss 0.442942, acc 0.890625, prec 0.057296, recall 0.871076
2017-12-10T13:52:57.793271: step 3263, loss 0.176704, acc 0.9375, prec 0.0573117, recall 0.871119
2017-12-10T13:52:57.979330: step 3264, loss 0.256408, acc 0.90625, prec 0.0573041, recall 0.871119
2017-12-10T13:52:58.171969: step 3265, loss 0.325139, acc 0.890625, prec 0.057316, recall 0.871162
2017-12-10T13:52:58.362244: step 3266, loss 0.0959559, acc 0.953125, prec 0.057333, recall 0.871205
2017-12-10T13:52:58.552980: step 3267, loss 0.295133, acc 0.9375, prec 0.0573693, recall 0.87129
2017-12-10T13:52:58.747285: step 3268, loss 0.0992233, acc 0.953125, prec 0.0573655, recall 0.87129
2017-12-10T13:52:58.940873: step 3269, loss 0.17176, acc 0.953125, prec 0.0574031, recall 0.871376
2017-12-10T13:52:59.130524: step 3270, loss 0.488216, acc 0.890625, prec 0.0573943, recall 0.871376
2017-12-10T13:52:59.315829: step 3271, loss 0.351289, acc 0.953125, prec 0.0574526, recall 0.871505
2017-12-10T13:52:59.501217: step 3272, loss 0.178427, acc 0.921875, prec 0.057467, recall 0.871547
2017-12-10T13:52:59.686973: step 3273, loss 0.278904, acc 0.875, prec 0.0574569, recall 0.871547
2017-12-10T13:52:59.875674: step 3274, loss 0.0969891, acc 0.96875, prec 0.0574544, recall 0.871547
2017-12-10T13:53:00.065024: step 3275, loss 0.0864537, acc 0.96875, prec 0.0574725, recall 0.87159
2017-12-10T13:53:00.252389: step 3276, loss 0.703993, acc 0.90625, prec 0.057527, recall 0.871718
2017-12-10T13:53:00.440767: step 3277, loss 0.16349, acc 0.953125, prec 0.0575232, recall 0.871718
2017-12-10T13:53:00.629946: step 3278, loss 0.0259034, acc 1, prec 0.0575232, recall 0.871718
2017-12-10T13:53:00.818527: step 3279, loss 0.256914, acc 0.90625, prec 0.0575776, recall 0.871846
2017-12-10T13:53:01.010998: step 3280, loss 0.241563, acc 0.9375, prec 0.0575726, recall 0.871846
2017-12-10T13:53:01.203221: step 3281, loss 0.156406, acc 0.9375, prec 0.0575675, recall 0.871846
2017-12-10T13:53:01.393955: step 3282, loss 0.200455, acc 0.890625, prec 0.0576, recall 0.871931
2017-12-10T13:53:01.581727: step 3283, loss 0.0858739, acc 0.96875, prec 0.0576181, recall 0.871973
2017-12-10T13:53:01.769392: step 3284, loss 0.34689, acc 0.921875, prec 0.0576531, recall 0.872058
2017-12-10T13:53:01.960826: step 3285, loss 0.229349, acc 0.921875, prec 0.0576468, recall 0.872058
2017-12-10T13:53:02.148370: step 3286, loss 0.171641, acc 0.9375, prec 0.0576624, recall 0.872101
2017-12-10T13:53:02.337223: step 3287, loss 0.110743, acc 0.96875, prec 0.0577012, recall 0.872185
2017-12-10T13:53:02.527922: step 3288, loss 0.271951, acc 0.953125, prec 0.0577386, recall 0.87227
2017-12-10T13:53:02.716818: step 3289, loss 0.0113895, acc 1, prec 0.0577386, recall 0.87227
2017-12-10T13:53:02.905922: step 3290, loss 0.218169, acc 0.9375, prec 0.0577336, recall 0.87227
2017-12-10T13:53:03.092560: step 3291, loss 0.0302893, acc 0.984375, prec 0.057753, recall 0.872312
2017-12-10T13:53:03.280578: step 3292, loss 0.132075, acc 1, prec 0.0577736, recall 0.872355
2017-12-10T13:53:03.473337: step 3293, loss 0.194333, acc 0.953125, prec 0.0577904, recall 0.872397
2017-12-10T13:53:03.663290: step 3294, loss 0.0508329, acc 0.96875, prec 0.0577879, recall 0.872397
2017-12-10T13:53:03.851210: step 3295, loss 0.137033, acc 0.9375, prec 0.0578035, recall 0.872439
2017-12-10T13:53:04.039469: step 3296, loss 0.087711, acc 0.96875, prec 0.0578009, recall 0.872439
2017-12-10T13:53:04.228157: step 3297, loss 0.0173162, acc 1, prec 0.0578009, recall 0.872439
2017-12-10T13:53:04.419712: step 3298, loss 0.300314, acc 0.953125, prec 0.0578796, recall 0.872607
2017-12-10T13:53:04.613592: step 3299, loss 0.0874953, acc 0.953125, prec 0.0578758, recall 0.872607
2017-12-10T13:53:04.804481: step 3300, loss 0.137868, acc 0.96875, prec 0.0578939, recall 0.872649
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_0/1512931325/checkpoints/model-3300

2017-12-10T13:53:06.098978: step 3301, loss 0.0910062, acc 0.953125, prec 0.0579107, recall 0.872691
2017-12-10T13:53:06.289537: step 3302, loss 2.27443, acc 0.953125, prec 0.0579288, recall 0.872446
2017-12-10T13:53:06.480929: step 3303, loss 0.0519137, acc 0.984375, prec 0.0579276, recall 0.872446
2017-12-10T13:53:06.670528: step 3304, loss 0.943513, acc 0.96875, prec 0.0579663, recall 0.87253
2017-12-10T13:53:06.860743: step 3305, loss 0.0796446, acc 0.953125, prec 0.0579831, recall 0.872572
2017-12-10T13:53:07.045642: step 3306, loss 0.0863195, acc 1, prec 0.0580243, recall 0.872655
2017-12-10T13:53:07.237478: step 3307, loss 0.0514133, acc 0.96875, prec 0.0580217, recall 0.872655
2017-12-10T13:53:07.422880: step 3308, loss 0.207768, acc 0.953125, prec 0.0580179, recall 0.872655
2017-12-10T13:53:07.613153: step 3309, loss 0.488805, acc 0.875, prec 0.0580284, recall 0.872697
2017-12-10T13:53:07.801454: step 3310, loss 0.353714, acc 0.9375, prec 0.0580645, recall 0.872781
2017-12-10T13:53:07.989501: step 3311, loss 0.200552, acc 0.9375, prec 0.05808, recall 0.872823
2017-12-10T13:53:08.178535: step 3312, loss 0.211282, acc 0.9375, prec 0.0580956, recall 0.872865
2017-12-10T13:53:08.369018: step 3313, loss 0.357994, acc 0.875, prec 0.0580854, recall 0.872865
2017-12-10T13:53:08.556404: step 3314, loss 0.191645, acc 0.90625, prec 0.0580984, recall 0.872906
2017-12-10T13:53:08.743332: step 3315, loss 0.233447, acc 0.9375, prec 0.0580933, recall 0.872906
2017-12-10T13:53:08.932621: step 3316, loss 0.598228, acc 0.890625, prec 0.0580844, recall 0.872906
2017-12-10T13:53:09.121174: step 3317, loss 0.259997, acc 0.90625, prec 0.0580768, recall 0.872906
2017-12-10T13:53:09.311454: step 3318, loss 0.202359, acc 0.921875, prec 0.0580704, recall 0.872906
2017-12-10T13:53:09.500287: step 3319, loss 0.154838, acc 0.90625, prec 0.0580834, recall 0.872948
2017-12-10T13:53:09.685839: step 3320, loss 0.786641, acc 0.84375, prec 0.0580913, recall 0.87299
2017-12-10T13:53:09.876049: step 3321, loss 0.65801, acc 0.90625, prec 0.0581248, recall 0.873073
2017-12-10T13:53:10.063844: step 3322, loss 0.233688, acc 0.9375, prec 0.0581403, recall 0.873115
2017-12-10T13:53:10.252686: step 3323, loss 0.423662, acc 0.890625, prec 0.058152, recall 0.873156
2017-12-10T13:53:10.437887: step 3324, loss 0.429979, acc 0.90625, prec 0.0581444, recall 0.873156
2017-12-10T13:53:10.626644: step 3325, loss 0.413397, acc 0.90625, prec 0.0581573, recall 0.873198
2017-12-10T13:53:10.817050: step 3326, loss 0.158527, acc 0.953125, prec 0.0581535, recall 0.873198
2017-12-10T13:53:11.008683: step 3327, loss 0.232463, acc 0.90625, prec 0.0581459, recall 0.873198
2017-12-10T13:53:11.198712: step 3328, loss 0.195171, acc 0.953125, prec 0.0581626, recall 0.873239
2017-12-10T13:53:11.388413: step 3329, loss 0.176433, acc 0.9375, prec 0.0581575, recall 0.873239
2017-12-10T13:53:11.577119: step 3330, loss 0.286147, acc 0.953125, prec 0.0581537, recall 0.873239
2017-12-10T13:53:11.763621: step 3331, loss 0.115851, acc 0.984375, prec 0.0581525, recall 0.873239
2017-12-10T13:53:11.950600: step 3332, loss 0.263139, acc 0.953125, prec 0.0581487, recall 0.873239
2017-12-10T13:53:12.133868: step 3333, loss 0.193849, acc 0.90625, prec 0.0581411, recall 0.873239
2017-12-10T13:53:12.324777: step 3334, loss 0.112283, acc 0.96875, prec 0.0581385, recall 0.873239
2017-12-10T13:53:12.512602: step 3335, loss 0.418733, acc 0.890625, prec 0.0581707, recall 0.873322
2017-12-10T13:53:12.702772: step 3336, loss 0.153271, acc 0.9375, prec 0.0581656, recall 0.873322
2017-12-10T13:53:12.892359: step 3337, loss 0.260679, acc 0.96875, prec 0.0581836, recall 0.873364
2017-12-10T13:53:13.087321: step 3338, loss 0.557727, acc 0.921875, prec 0.0582389, recall 0.873488
2017-12-10T13:53:13.279019: step 3339, loss 0.0250019, acc 1, prec 0.0582389, recall 0.873488
2017-12-10T13:53:13.467463: step 3340, loss 1.03058, acc 0.953125, prec 0.0582556, recall 0.873529
2017-12-10T13:53:13.659604: step 3341, loss 0.099321, acc 0.953125, prec 0.0582518, recall 0.873529
2017-12-10T13:53:13.852397: step 3342, loss 0.137798, acc 0.9375, prec 0.0582672, recall 0.873571
2017-12-10T13:53:14.044925: step 3343, loss 0.140447, acc 0.9375, prec 0.0582622, recall 0.873571
2017-12-10T13:53:14.229590: step 3344, loss 0.205565, acc 0.953125, prec 0.0583199, recall 0.873695
2017-12-10T13:53:14.420953: step 3345, loss 0.0619221, acc 0.96875, prec 0.0583379, recall 0.873736
2017-12-10T13:53:14.608134: step 3346, loss 0.406025, acc 0.953125, prec 0.0584366, recall 0.873941
2017-12-10T13:53:14.798422: step 3347, loss 0.109991, acc 0.984375, prec 0.0584558, recall 0.873982
2017-12-10T13:53:14.989338: step 3348, loss 0.245179, acc 0.921875, prec 0.0584495, recall 0.873982
2017-12-10T13:53:15.182697: step 3349, loss 0.036583, acc 0.984375, prec 0.0584482, recall 0.873982
2017-12-10T13:53:15.369824: step 3350, loss 0.334474, acc 0.953125, prec 0.0585059, recall 0.874105
2017-12-10T13:53:15.556972: step 3351, loss 0.0951734, acc 0.96875, prec 0.0585033, recall 0.874105
2017-12-10T13:53:15.744640: step 3352, loss 0.0420577, acc 0.96875, prec 0.0585008, recall 0.874105
2017-12-10T13:53:15.931662: step 3353, loss 0.221911, acc 0.953125, prec 0.0585175, recall 0.874146
2017-12-10T13:53:16.120072: step 3354, loss 0.117217, acc 0.96875, prec 0.0585354, recall 0.874187
2017-12-10T13:53:16.307372: step 3355, loss 0.988131, acc 0.921875, prec 0.0585495, recall 0.874228
2017-12-10T13:53:16.498958: step 3356, loss 0.274704, acc 0.9375, prec 0.0585649, recall 0.874269
2017-12-10T13:53:16.687248: step 3357, loss 0.0634369, acc 0.96875, prec 0.0585829, recall 0.87431
2017-12-10T13:53:16.879360: step 3358, loss 0.173492, acc 0.921875, prec 0.0585765, recall 0.87431
2017-12-10T13:53:17.066976: step 3359, loss 0.137825, acc 0.9375, prec 0.0585714, recall 0.87431
2017-12-10T13:53:17.259286: step 3360, loss 0.179821, acc 0.96875, prec 0.0585893, recall 0.874351
2017-12-10T13:53:17.448332: step 3361, loss 0.126059, acc 0.921875, prec 0.0586034, recall 0.874391
2017-12-10T13:53:17.634827: step 3362, loss 0.132297, acc 0.953125, prec 0.0586201, recall 0.874432
2017-12-10T13:53:17.822386: step 3363, loss 0.22798, acc 0.96875, prec 0.0586175, recall 0.874432
2017-12-10T13:53:18.009396: step 3364, loss 0.254149, acc 0.9375, prec 0.0586124, recall 0.874432
2017-12-10T13:53:18.196781: step 3365, loss 0.19453, acc 0.96875, prec 0.0586508, recall 0.874514
2017-12-10T13:53:18.383290: step 3366, loss 0.129014, acc 0.96875, prec 0.0586892, recall 0.874595
2017-12-10T13:53:18.571671: step 3367, loss 0.0214565, acc 1, prec 0.0586892, recall 0.874595
2017-12-10T13:53:18.761551: step 3368, loss 0.330675, acc 0.90625, prec 0.0586816, recall 0.874595
2017-12-10T13:53:18.949167: step 3369, loss 0.122709, acc 0.96875, prec 0.0586995, recall 0.874636
2017-12-10T13:53:19.138147: step 3370, loss 0.142745, acc 0.953125, prec 0.0587366, recall 0.874717
2017-12-10T13:53:19.327198: step 3371, loss 1.35375, acc 0.984375, prec 0.0587967, recall 0.874838
2017-12-10T13:53:19.523723: step 3372, loss 0.191839, acc 0.953125, prec 0.0588133, recall 0.874879
2017-12-10T13:53:19.709521: step 3373, loss 0.391704, acc 0.9375, prec 0.0588286, recall 0.874919
2017-12-10T13:53:19.901537: step 3374, loss 0.234541, acc 0.90625, prec 0.058821, recall 0.874919
2017-12-10T13:53:20.088588: step 3375, loss 0.387836, acc 0.890625, prec 0.058812, recall 0.874919
2017-12-10T13:53:20.274100: step 3376, loss 0.193838, acc 0.921875, prec 0.0588261, recall 0.87496
2017-12-10T13:53:20.465672: step 3377, loss 0.153447, acc 0.9375, prec 0.058821, recall 0.87496
2017-12-10T13:53:20.659084: step 3378, loss 0.175562, acc 0.9375, prec 0.0588363, recall 0.875
2017-12-10T13:53:20.845912: step 3379, loss 0.201922, acc 0.90625, prec 0.0588286, recall 0.875
2017-12-10T13:53:21.032788: step 3380, loss 0.166459, acc 0.9375, prec 0.0588235, recall 0.875
2017-12-10T13:53:21.220636: step 3381, loss 0.364344, acc 0.9375, prec 0.0588389, recall 0.87504
2017-12-10T13:53:21.409589: step 3382, loss 0.349464, acc 0.875, prec 0.0588286, recall 0.87504
2017-12-10T13:53:21.597175: step 3383, loss 0.186036, acc 0.921875, prec 0.0588223, recall 0.87504
2017-12-10T13:53:21.783091: step 3384, loss 0.192593, acc 0.9375, prec 0.0588171, recall 0.87504
2017-12-10T13:53:21.968191: step 3385, loss 0.426118, acc 0.890625, prec 0.0588082, recall 0.87504
2017-12-10T13:53:22.153585: step 3386, loss 0.0998664, acc 0.96875, prec 0.0588465, recall 0.875121
2017-12-10T13:53:22.340241: step 3387, loss 0.380548, acc 0.921875, prec 0.0588605, recall 0.875161
2017-12-10T13:53:22.531915: step 3388, loss 0.105428, acc 0.984375, prec 0.0589001, recall 0.875242
2017-12-10T13:53:22.722817: step 3389, loss 0.384167, acc 0.90625, prec 0.0588924, recall 0.875242
2017-12-10T13:53:22.910494: step 3390, loss 0.286391, acc 0.9375, prec 0.0589281, recall 0.875322
2017-12-10T13:53:23.098761: step 3391, loss 0.0471599, acc 0.984375, prec 0.0589677, recall 0.875402
2017-12-10T13:53:23.290518: step 3392, loss 0.20029, acc 0.9375, prec 0.058983, recall 0.875443
2017-12-10T13:53:23.483741: step 3393, loss 0.191828, acc 0.9375, prec 0.0589779, recall 0.875443
2017-12-10T13:53:23.680798: step 3394, loss 0.270879, acc 0.96875, prec 0.0590569, recall 0.875603
2017-12-10T13:53:23.870129: step 3395, loss 0.854641, acc 0.953125, prec 0.0590735, recall 0.875643
2017-12-10T13:53:24.063053: step 3396, loss 0.260502, acc 0.96875, prec 0.0590709, recall 0.875643
2017-12-10T13:53:24.254633: step 3397, loss 0.154833, acc 0.96875, prec 0.0590683, recall 0.875643
2017-12-10T13:53:24.445723: step 3398, loss 0.205782, acc 0.96875, prec 0.0591066, recall 0.875723
2017-12-10T13:53:24.633857: step 3399, loss 0.205386, acc 0.96875, prec 0.0591244, recall 0.875762
2017-12-10T13:53:24.823129: step 3400, loss 0.209434, acc 0.96875, prec 0.0591422, recall 0.875802
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_0/1512931325/checkpoints/model-3400

2017-12-10T13:53:26.027813: step 3401, loss 0.146767, acc 0.9375, prec 0.0591371, recall 0.875802
2017-12-10T13:53:26.215000: step 3402, loss 0.0398579, acc 0.984375, prec 0.0591562, recall 0.875842
2017-12-10T13:53:26.401992: step 3403, loss 0.124103, acc 0.984375, prec 0.0591753, recall 0.875882
2017-12-10T13:53:26.589994: step 3404, loss 0.157384, acc 0.953125, prec 0.0591715, recall 0.875882
2017-12-10T13:53:26.777287: step 3405, loss 0.333173, acc 0.921875, prec 0.0591651, recall 0.875882
2017-12-10T13:53:26.969302: step 3406, loss 0.183626, acc 0.96875, prec 0.0591829, recall 0.875922
2017-12-10T13:53:27.162388: step 3407, loss 0.158365, acc 0.953125, prec 0.0591994, recall 0.875962
2017-12-10T13:53:27.352719: step 3408, loss 0.152518, acc 0.96875, prec 0.0592172, recall 0.876001
2017-12-10T13:53:27.544366: step 3409, loss 0.326715, acc 0.953125, prec 0.0592338, recall 0.876041
2017-12-10T13:53:27.731416: step 3410, loss 0.0121588, acc 1, prec 0.0592338, recall 0.876041
2017-12-10T13:53:27.921579: step 3411, loss 1.26335, acc 0.90625, prec 0.0592273, recall 0.87576
2017-12-10T13:53:28.112063: step 3412, loss 0.149847, acc 0.96875, prec 0.0592451, recall 0.8758
2017-12-10T13:53:28.301082: step 3413, loss 0.148653, acc 0.953125, prec 0.0592413, recall 0.8758
2017-12-10T13:53:28.488217: step 3414, loss 0.0347168, acc 0.96875, prec 0.0592591, recall 0.87584
2017-12-10T13:53:28.679842: step 3415, loss 0.0467809, acc 0.984375, prec 0.0592578, recall 0.87584
2017-12-10T13:53:28.871139: step 3416, loss 0.1373, acc 0.953125, prec 0.059254, recall 0.87584
2017-12-10T13:53:29.065088: step 3417, loss 0.0411303, acc 0.984375, prec 0.0592527, recall 0.87584
2017-12-10T13:53:29.255696: step 3418, loss 0.696825, acc 0.953125, prec 0.0592692, recall 0.87588
2017-12-10T13:53:29.449832: step 3419, loss 0.162279, acc 0.984375, prec 0.0592679, recall 0.87588
2017-12-10T13:53:29.639112: step 3420, loss 0.367472, acc 0.953125, prec 0.0592844, recall 0.875919
2017-12-10T13:53:29.829590: step 3421, loss 0.120683, acc 0.921875, prec 0.0592984, recall 0.875959
2017-12-10T13:53:30.014291: step 3422, loss 0.226333, acc 0.9375, prec 0.0592932, recall 0.875959
2017-12-10T13:53:30.205528: step 3423, loss 0.153398, acc 0.9375, prec 0.0592881, recall 0.875959
2017-12-10T13:53:30.398657: step 3424, loss 0.0508865, acc 0.984375, prec 0.0593072, recall 0.875999
2017-12-10T13:53:30.588349: step 3425, loss 0.366491, acc 0.9375, prec 0.0593224, recall 0.876038
2017-12-10T13:53:30.778129: step 3426, loss 0.26017, acc 0.9375, prec 0.0593173, recall 0.876038
2017-12-10T13:53:30.970190: step 3427, loss 0.301023, acc 0.953125, prec 0.0593541, recall 0.876117
2017-12-10T13:53:31.161050: step 3428, loss 0.165721, acc 0.9375, prec 0.0593897, recall 0.876197
2017-12-10T13:53:31.352301: step 3429, loss 0.0315342, acc 0.984375, prec 0.0593884, recall 0.876197
2017-12-10T13:53:31.539910: step 3430, loss 7.2137, acc 0.90625, prec 0.0594226, recall 0.875996
2017-12-10T13:53:31.731479: step 3431, loss 0.305149, acc 0.875, prec 0.0594327, recall 0.876036
2017-12-10T13:53:31.922182: step 3432, loss 0.177992, acc 0.96875, prec 0.0594911, recall 0.876154
2017-12-10T13:53:32.115922: step 3433, loss 0.194206, acc 0.953125, prec 0.0594873, recall 0.876154
2017-12-10T13:53:32.302816: step 3434, loss 0.3056, acc 0.90625, prec 0.0594796, recall 0.876154
2017-12-10T13:53:32.494048: step 3435, loss 0.338465, acc 0.875, prec 0.0594693, recall 0.876154
2017-12-10T13:53:32.683980: step 3436, loss 0.433371, acc 0.890625, prec 0.0595009, recall 0.876233
2017-12-10T13:53:32.873100: step 3437, loss 0.364217, acc 0.875, prec 0.0594906, recall 0.876233
2017-12-10T13:53:33.061690: step 3438, loss 0.730629, acc 0.8125, prec 0.0595158, recall 0.876312
2017-12-10T13:53:33.248743: step 3439, loss 0.502307, acc 0.8125, prec 0.0595004, recall 0.876312
2017-12-10T13:53:33.439045: step 3440, loss 1.14768, acc 0.78125, prec 0.0594824, recall 0.876312
2017-12-10T13:53:33.629373: step 3441, loss 0.101435, acc 0.9375, prec 0.0594773, recall 0.876312
2017-12-10T13:53:33.817884: step 3442, loss 0.415113, acc 0.90625, prec 0.0595305, recall 0.876429
2017-12-10T13:53:34.009115: step 3443, loss 0.147452, acc 0.953125, prec 0.0595672, recall 0.876508
2017-12-10T13:53:34.197744: step 3444, loss 0.365277, acc 0.890625, prec 0.0595785, recall 0.876547
2017-12-10T13:53:34.385318: step 3445, loss 0.387192, acc 0.90625, prec 0.0595911, recall 0.876586
2017-12-10T13:53:34.575797: step 3446, loss 0.0814223, acc 0.953125, prec 0.0595872, recall 0.876586
2017-12-10T13:53:34.767468: step 3447, loss 0.145266, acc 0.9375, prec 0.0595821, recall 0.876586
2017-12-10T13:53:34.952979: step 3448, loss 0.27555, acc 0.9375, prec 0.0596175, recall 0.876665
2017-12-10T13:53:35.142808: step 3449, loss 1.26429, acc 0.9375, prec 0.0596529, recall 0.876743
2017-12-10T13:53:35.335069: step 3450, loss 0.344127, acc 0.890625, prec 0.0596642, recall 0.876782
2017-12-10T13:53:35.522109: step 3451, loss 0.210767, acc 0.953125, prec 0.0596806, recall 0.876821
2017-12-10T13:53:35.714563: step 3452, loss 0.19724, acc 0.921875, prec 0.0596742, recall 0.876821
2017-12-10T13:53:35.901000: step 3453, loss 0.134041, acc 0.921875, prec 0.0596677, recall 0.876821
2017-12-10T13:53:36.088082: step 3454, loss 1.38678, acc 0.90625, prec 0.0597005, recall 0.876899
2017-12-10T13:53:36.280145: step 3455, loss 0.109748, acc 0.9375, prec 0.0596954, recall 0.876899
2017-12-10T13:53:36.469226: step 3456, loss 0.325928, acc 0.890625, prec 0.0597269, recall 0.876977
2017-12-10T13:53:36.661469: step 3457, loss 0.241269, acc 0.953125, prec 0.059723, recall 0.876977
2017-12-10T13:53:36.847022: step 3458, loss 0.427952, acc 0.875, prec 0.0597127, recall 0.876977
2017-12-10T13:53:37.034415: step 3459, loss 0.23973, acc 0.890625, prec 0.0597037, recall 0.876977
2017-12-10T13:53:37.223343: step 3460, loss 0.0618066, acc 0.984375, prec 0.0597227, recall 0.877015
2017-12-10T13:53:37.416425: step 3461, loss 0.285436, acc 0.953125, prec 0.0597593, recall 0.877093
2017-12-10T13:53:37.608658: step 3462, loss 0.0999168, acc 0.96875, prec 0.059777, recall 0.877132
2017-12-10T13:53:37.796495: step 3463, loss 0.464993, acc 0.890625, prec 0.059768, recall 0.877132
2017-12-10T13:53:37.982845: step 3464, loss 0.14182, acc 0.953125, prec 0.0597844, recall 0.877171
2017-12-10T13:53:38.173202: step 3465, loss 0.0616294, acc 0.984375, prec 0.0598033, recall 0.87721
2017-12-10T13:53:38.361799: step 3466, loss 0.275988, acc 0.96875, prec 0.059821, recall 0.877248
2017-12-10T13:53:38.552788: step 3467, loss 0.116687, acc 0.984375, prec 0.0598197, recall 0.877248
2017-12-10T13:53:38.738651: step 3468, loss 0.298634, acc 0.921875, prec 0.0598335, recall 0.877287
2017-12-10T13:53:38.930018: step 3469, loss 0.318972, acc 0.921875, prec 0.059827, recall 0.877287
2017-12-10T13:53:39.118620: step 3470, loss 0.184584, acc 0.921875, prec 0.0598206, recall 0.877287
2017-12-10T13:53:39.312646: step 3471, loss 0.12213, acc 0.921875, prec 0.0598142, recall 0.877287
2017-12-10T13:53:39.509281: step 3472, loss 0.129027, acc 0.9375, prec 0.059809, recall 0.877287
2017-12-10T13:53:39.701829: step 3473, loss 0.0824478, acc 0.96875, prec 0.0598267, recall 0.877326
2017-12-10T13:53:39.891665: step 3474, loss 0.148916, acc 0.953125, prec 0.0598228, recall 0.877326
2017-12-10T13:53:40.079326: step 3475, loss 0.11736, acc 0.984375, prec 0.0598417, recall 0.877364
2017-12-10T13:53:40.269118: step 3476, loss 0.256938, acc 0.9375, prec 0.0598568, recall 0.877403
2017-12-10T13:53:40.459811: step 3477, loss 0.0925092, acc 0.984375, prec 0.0598757, recall 0.877442
2017-12-10T13:53:40.649168: step 3478, loss 0.130893, acc 0.96875, prec 0.0599338, recall 0.877557
2017-12-10T13:53:40.823469: step 3479, loss 0.146763, acc 0.980392, prec 0.0599527, recall 0.877596
2017-12-10T13:53:41.026783: step 3480, loss 0.190239, acc 0.96875, prec 0.0599703, recall 0.877634
2017-12-10T13:53:41.217814: step 3481, loss 0.0737502, acc 0.96875, prec 0.059988, recall 0.877673
2017-12-10T13:53:41.410800: step 3482, loss 0.0632657, acc 0.984375, prec 0.0600069, recall 0.877711
2017-12-10T13:53:41.597405: step 3483, loss 0.0167617, acc 0.984375, prec 0.0600258, recall 0.87775
2017-12-10T13:53:41.790388: step 3484, loss 0.192825, acc 0.984375, prec 0.0600447, recall 0.877788
2017-12-10T13:53:41.983602: step 3485, loss 0.0984149, acc 0.953125, prec 0.060061, recall 0.877827
2017-12-10T13:53:42.172369: step 3486, loss 9.28561, acc 0.96875, prec 0.0600799, recall 0.877589
2017-12-10T13:53:42.367053: step 3487, loss 0.0350487, acc 1, prec 0.0601203, recall 0.877666
2017-12-10T13:53:42.559561: step 3488, loss 0.416898, acc 1, prec 0.0601405, recall 0.877705
2017-12-10T13:53:42.754919: step 3489, loss 0.103247, acc 0.953125, prec 0.0601366, recall 0.877705
2017-12-10T13:53:42.942618: step 3490, loss 0.0566448, acc 0.96875, prec 0.0601341, recall 0.877705
2017-12-10T13:53:43.130211: step 3491, loss 0.0720565, acc 0.984375, prec 0.060153, recall 0.877743
2017-12-10T13:53:43.313594: step 3492, loss 0.501073, acc 0.90625, prec 0.0602058, recall 0.877858
2017-12-10T13:53:43.498862: step 3493, loss 0.174413, acc 0.921875, prec 0.0602195, recall 0.877896
2017-12-10T13:53:43.683443: step 3494, loss 0.162462, acc 0.953125, prec 0.0602156, recall 0.877896
2017-12-10T13:53:43.876424: step 3495, loss 0.325025, acc 0.90625, prec 0.0602482, recall 0.877972
2017-12-10T13:53:44.063261: step 3496, loss 0.246828, acc 0.921875, prec 0.0602619, recall 0.878011
2017-12-10T13:53:44.256074: step 3497, loss 0.15786, acc 0.9375, prec 0.0602769, recall 0.878049
2017-12-10T13:53:44.443948: step 3498, loss 0.391032, acc 0.890625, prec 0.0602679, recall 0.878049
2017-12-10T13:53:44.633146: step 3499, loss 0.216103, acc 0.921875, prec 0.0602614, recall 0.878049
2017-12-10T13:53:44.821682: step 3500, loss 0.707132, acc 0.828125, prec 0.0602472, recall 0.878049
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_0/1512931325/checkpoints/model-3500

2017-12-10T13:53:45.984238: step 3501, loss 0.260772, acc 0.921875, prec 0.0602609, recall 0.878087
2017-12-10T13:53:46.173604: step 3502, loss 0.246622, acc 0.921875, prec 0.0602947, recall 0.878163
2017-12-10T13:53:46.365833: step 3503, loss 0.331804, acc 0.921875, prec 0.0603084, recall 0.878201
2017-12-10T13:53:46.553929: step 3504, loss 0.537507, acc 0.9375, prec 0.0603435, recall 0.878277
2017-12-10T13:53:46.741584: step 3505, loss 0.31967, acc 0.859375, prec 0.0603722, recall 0.878353
2017-12-10T13:53:46.932055: step 3506, loss 0.305995, acc 0.90625, prec 0.0603644, recall 0.878353
2017-12-10T13:53:47.120158: step 3507, loss 0.54441, acc 0.890625, prec 0.0603554, recall 0.878353
2017-12-10T13:53:47.311215: step 3508, loss 0.230991, acc 0.9375, prec 0.0603905, recall 0.878429
2017-12-10T13:53:47.496076: step 3509, loss 0.0937575, acc 0.96875, prec 0.060408, recall 0.878467
2017-12-10T13:53:47.686653: step 3510, loss 0.316992, acc 0.90625, prec 0.0604002, recall 0.878467
2017-12-10T13:53:47.879113: step 3511, loss 0.101786, acc 0.96875, prec 0.0603977, recall 0.878467
2017-12-10T13:53:48.064053: step 3512, loss 0.458076, acc 0.84375, prec 0.0603847, recall 0.878467
2017-12-10T13:53:48.251866: step 3513, loss 0.149658, acc 0.921875, prec 0.0603984, recall 0.878505
2017-12-10T13:53:48.440480: step 3514, loss 0.340535, acc 0.9375, prec 0.0603932, recall 0.878505
2017-12-10T13:53:48.630815: step 3515, loss 0.232485, acc 0.9375, prec 0.0604081, recall 0.878542
2017-12-10T13:53:48.828182: step 3516, loss 0.257503, acc 0.890625, prec 0.0603991, recall 0.878542
2017-12-10T13:53:49.014791: step 3517, loss 0.142614, acc 0.953125, prec 0.0603952, recall 0.878542
2017-12-10T13:53:49.204400: step 3518, loss 0.217785, acc 0.9375, prec 0.0604102, recall 0.87858
2017-12-10T13:53:49.388930: step 3519, loss 0.15273, acc 0.9375, prec 0.060405, recall 0.87858
2017-12-10T13:53:49.577900: step 3520, loss 0.0544719, acc 0.96875, prec 0.0604024, recall 0.87858
2017-12-10T13:53:49.771505: step 3521, loss 0.0914803, acc 0.953125, prec 0.0604186, recall 0.878618
2017-12-10T13:53:49.961168: step 3522, loss 0.425685, acc 0.984375, prec 0.0604374, recall 0.878656
2017-12-10T13:53:50.161471: step 3523, loss 0.0386277, acc 0.96875, prec 0.0604349, recall 0.878656
2017-12-10T13:53:50.349254: step 3524, loss 0.319123, acc 0.921875, prec 0.0604284, recall 0.878656
2017-12-10T13:53:50.537031: step 3525, loss 0.113909, acc 0.953125, prec 0.0604245, recall 0.878656
2017-12-10T13:53:50.730756: step 3526, loss 0.399605, acc 0.984375, prec 0.0604835, recall 0.878769
2017-12-10T13:53:50.926898: step 3527, loss 0.0844835, acc 0.9375, prec 0.0604784, recall 0.878769
2017-12-10T13:53:51.122364: step 3528, loss 0.00805731, acc 1, prec 0.0604784, recall 0.878769
2017-12-10T13:53:51.309452: step 3529, loss 0.0628024, acc 0.984375, prec 0.0604771, recall 0.878769
2017-12-10T13:53:51.496672: step 3530, loss 0.229872, acc 0.953125, prec 0.0605335, recall 0.878882
2017-12-10T13:53:51.683814: step 3531, loss 0.148708, acc 0.953125, prec 0.0605296, recall 0.878882
2017-12-10T13:53:51.876429: step 3532, loss 0.126956, acc 0.96875, prec 0.0605471, recall 0.87892
2017-12-10T13:53:52.061477: step 3533, loss 0.034182, acc 0.984375, prec 0.0605458, recall 0.87892
2017-12-10T13:53:52.250941: step 3534, loss 0.0881028, acc 0.96875, prec 0.0605633, recall 0.878957
2017-12-10T13:53:52.442153: step 3535, loss 1.93261, acc 0.953125, prec 0.0605607, recall 0.878684
2017-12-10T13:53:52.633638: step 3536, loss 0.119921, acc 0.984375, prec 0.0605795, recall 0.878722
2017-12-10T13:53:52.826973: step 3537, loss 0.0979951, acc 0.9375, prec 0.0605944, recall 0.87876
2017-12-10T13:53:53.018932: step 3538, loss 0.279722, acc 0.9375, prec 0.0606294, recall 0.878835
2017-12-10T13:53:53.210802: step 3539, loss 0.0824351, acc 0.984375, prec 0.0606482, recall 0.878872
2017-12-10T13:53:53.400126: step 3540, loss 0.257777, acc 0.953125, prec 0.0607045, recall 0.878985
2017-12-10T13:53:53.593175: step 3541, loss 0.199314, acc 0.9375, prec 0.0606993, recall 0.878985
2017-12-10T13:53:53.783337: step 3542, loss 0.0994716, acc 0.9375, prec 0.0606941, recall 0.878985
2017-12-10T13:53:53.970827: step 3543, loss 0.094605, acc 0.953125, prec 0.0607103, recall 0.879022
2017-12-10T13:53:54.160273: step 3544, loss 0.271715, acc 0.953125, prec 0.0607064, recall 0.879022
2017-12-10T13:53:54.350255: step 3545, loss 0.128656, acc 0.9375, prec 0.0607012, recall 0.879022
2017-12-10T13:53:54.537423: step 3546, loss 0.169744, acc 0.953125, prec 0.0607174, recall 0.87906
2017-12-10T13:53:54.726700: step 3547, loss 0.0968052, acc 0.984375, prec 0.0607562, recall 0.879134
2017-12-10T13:53:54.914519: step 3548, loss 0.111351, acc 0.96875, prec 0.0607737, recall 0.879172
2017-12-10T13:53:55.104836: step 3549, loss 0.323065, acc 0.9375, prec 0.0607886, recall 0.879209
2017-12-10T13:53:55.297013: step 3550, loss 0.244446, acc 0.90625, prec 0.0607808, recall 0.879209
2017-12-10T13:53:55.486046: step 3551, loss 0.0168079, acc 1, prec 0.0607808, recall 0.879209
2017-12-10T13:53:55.673841: step 3552, loss 0.0110693, acc 1, prec 0.060841, recall 0.879321
2017-12-10T13:53:55.858231: step 3553, loss 0.133946, acc 0.921875, prec 0.0608345, recall 0.879321
2017-12-10T13:53:56.047921: step 3554, loss 0.311117, acc 0.9375, prec 0.0608293, recall 0.879321
2017-12-10T13:53:56.236158: step 3555, loss 0.0896214, acc 0.9375, prec 0.0608241, recall 0.879321
2017-12-10T13:53:56.428170: step 3556, loss 0.108211, acc 0.953125, prec 0.0608603, recall 0.879395
2017-12-10T13:53:56.618530: step 3557, loss 0.11924, acc 0.96875, prec 0.0608577, recall 0.879395
2017-12-10T13:53:56.810029: step 3558, loss 0.171353, acc 0.9375, prec 0.0609126, recall 0.879507
2017-12-10T13:53:56.999420: step 3559, loss 0.157013, acc 0.953125, prec 0.0609087, recall 0.879507
2017-12-10T13:53:57.185868: step 3560, loss 0.238384, acc 0.921875, prec 0.0609022, recall 0.879507
2017-12-10T13:53:57.373911: step 3561, loss 0.146615, acc 0.953125, prec 0.0609184, recall 0.879544
2017-12-10T13:53:57.562857: step 3562, loss 0.139245, acc 0.984375, prec 0.0609571, recall 0.879618
2017-12-10T13:53:57.756750: step 3563, loss 0.0807963, acc 0.96875, prec 0.0609545, recall 0.879618
2017-12-10T13:53:57.946581: step 3564, loss 0.0103986, acc 1, prec 0.0609545, recall 0.879618
2017-12-10T13:53:58.133284: step 3565, loss 0.0482942, acc 0.984375, prec 0.0609733, recall 0.879655
2017-12-10T13:53:58.322089: step 3566, loss 0.0435749, acc 0.984375, prec 0.060972, recall 0.879655
2017-12-10T13:53:58.512184: step 3567, loss 0.0204748, acc 1, prec 0.060992, recall 0.879692
2017-12-10T13:53:58.703437: step 3568, loss 0.908057, acc 1, prec 0.061012, recall 0.879729
2017-12-10T13:53:58.900022: step 3569, loss 0.114043, acc 0.984375, prec 0.0610508, recall 0.879803
2017-12-10T13:53:59.087905: step 3570, loss 0.107314, acc 0.96875, prec 0.0610682, recall 0.87984
2017-12-10T13:53:59.279532: step 3571, loss 0.198841, acc 0.921875, prec 0.0610817, recall 0.879877
2017-12-10T13:53:59.468327: step 3572, loss 0.0687753, acc 0.984375, prec 0.0610804, recall 0.879877
2017-12-10T13:53:59.653510: step 3573, loss 0.0351542, acc 0.984375, prec 0.0610791, recall 0.879877
2017-12-10T13:53:59.842448: step 3574, loss 0.151989, acc 0.9375, prec 0.0610739, recall 0.879877
2017-12-10T13:54:00.031585: step 3575, loss 0.330839, acc 0.921875, prec 0.0611275, recall 0.879988
2017-12-10T13:54:00.221873: step 3576, loss 0.125286, acc 0.96875, prec 0.0611449, recall 0.880025
2017-12-10T13:54:00.413139: step 3577, loss 0.0740524, acc 0.984375, prec 0.0611436, recall 0.880025
2017-12-10T13:54:00.606664: step 3578, loss 0.0407793, acc 1, prec 0.0611836, recall 0.880098
2017-12-10T13:54:00.793443: step 3579, loss 0.141178, acc 0.96875, prec 0.061241, recall 0.880208
2017-12-10T13:54:00.987838: step 3580, loss 0.185342, acc 0.953125, prec 0.0612971, recall 0.880318
2017-12-10T13:54:01.176540: step 3581, loss 0.132881, acc 0.9375, prec 0.0612919, recall 0.880318
2017-12-10T13:54:01.368974: step 3582, loss 0.170055, acc 0.953125, prec 0.061288, recall 0.880318
2017-12-10T13:54:01.554604: step 3583, loss 0.911001, acc 0.96875, prec 0.0613054, recall 0.880355
2017-12-10T13:54:01.746182: step 3584, loss 0.271876, acc 0.984375, prec 0.0613441, recall 0.880428
2017-12-10T13:54:01.933660: step 3585, loss 0.123875, acc 0.953125, prec 0.0613602, recall 0.880465
2017-12-10T13:54:02.120303: step 3586, loss 0.19512, acc 0.96875, prec 0.0613575, recall 0.880465
2017-12-10T13:54:02.307204: step 3587, loss 0.193692, acc 0.953125, prec 0.0613736, recall 0.880501
2017-12-10T13:54:02.494553: step 3588, loss 0.151625, acc 0.953125, prec 0.0613697, recall 0.880501
2017-12-10T13:54:02.683146: step 3589, loss 0.188548, acc 0.9375, prec 0.0613645, recall 0.880501
2017-12-10T13:54:02.868478: step 3590, loss 0.22565, acc 0.921875, prec 0.0613779, recall 0.880538
2017-12-10T13:54:03.060240: step 3591, loss 0.187894, acc 0.890625, prec 0.0614087, recall 0.880611
2017-12-10T13:54:03.249097: step 3592, loss 0.0227097, acc 1, prec 0.0614087, recall 0.880611
2017-12-10T13:54:03.433793: step 3593, loss 0.190405, acc 0.921875, prec 0.0614422, recall 0.880684
2017-12-10T13:54:03.623461: step 3594, loss 0.113413, acc 0.96875, prec 0.0614595, recall 0.88072
2017-12-10T13:54:03.810091: step 3595, loss 0.0260563, acc 1, prec 0.0614795, recall 0.880756
2017-12-10T13:54:03.998886: step 3596, loss 0.0567444, acc 0.984375, prec 0.0614782, recall 0.880756
2017-12-10T13:54:04.183847: step 3597, loss 0.562431, acc 0.953125, prec 0.0615142, recall 0.880829
2017-12-10T13:54:04.374915: step 3598, loss 0.37787, acc 0.96875, prec 0.0615516, recall 0.880902
2017-12-10T13:54:04.568587: step 3599, loss 0.184188, acc 0.9375, prec 0.0615863, recall 0.880974
2017-12-10T13:54:04.757891: step 3600, loss 0.0927387, acc 0.953125, prec 0.0616223, recall 0.881047
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_0/1512931325/checkpoints/model-3600

2017-12-10T13:54:05.977306: step 3601, loss 0.153148, acc 0.953125, prec 0.0616383, recall 0.881083
2017-12-10T13:54:06.167990: step 3602, loss 0.167448, acc 0.9375, prec 0.0616331, recall 0.881083
2017-12-10T13:54:06.358737: step 3603, loss 0.148222, acc 0.9375, prec 0.0616278, recall 0.881083
2017-12-10T13:54:06.547229: step 3604, loss 0.0328293, acc 0.984375, prec 0.0616265, recall 0.881083
2017-12-10T13:54:06.733000: step 3605, loss 0.0564032, acc 0.984375, prec 0.0616451, recall 0.881119
2017-12-10T13:54:06.919037: step 3606, loss 0.0764176, acc 0.984375, prec 0.0616438, recall 0.881119
2017-12-10T13:54:07.107907: step 3607, loss 0.186539, acc 0.953125, prec 0.0616399, recall 0.881119
2017-12-10T13:54:07.293742: step 3608, loss 0.0760461, acc 0.96875, prec 0.0616572, recall 0.881155
2017-12-10T13:54:07.485243: step 3609, loss 0.145674, acc 0.9375, prec 0.0616919, recall 0.881227
2017-12-10T13:54:07.674278: step 3610, loss 0.321145, acc 0.90625, prec 0.0617439, recall 0.881335
2017-12-10T13:54:07.866837: step 3611, loss 0.106321, acc 0.96875, prec 0.0617811, recall 0.881407
2017-12-10T13:54:08.056655: step 3612, loss 0.100339, acc 0.96875, prec 0.0617985, recall 0.881443
2017-12-10T13:54:08.249284: step 3613, loss 0.0782888, acc 0.96875, prec 0.0618158, recall 0.881479
2017-12-10T13:54:08.439340: step 3614, loss 0.289043, acc 0.90625, prec 0.0618278, recall 0.881515
2017-12-10T13:54:08.627290: step 3615, loss 0.243237, acc 0.90625, prec 0.0618399, recall 0.881551
2017-12-10T13:54:08.813769: step 3616, loss 0.108852, acc 0.96875, prec 0.0618373, recall 0.881551
2017-12-10T13:54:09.003983: step 3617, loss 0.222095, acc 0.96875, prec 0.0618346, recall 0.881551
2017-12-10T13:54:09.196811: step 3618, loss 0.0534319, acc 0.96875, prec 0.061832, recall 0.881551
2017-12-10T13:54:09.389102: step 3619, loss 0.199186, acc 0.953125, prec 0.0618281, recall 0.881551
2017-12-10T13:54:09.578578: step 3620, loss 0.236079, acc 0.9375, prec 0.0618228, recall 0.881551
2017-12-10T13:54:09.769835: step 3621, loss 0.570691, acc 0.9375, prec 0.0618574, recall 0.881623
2017-12-10T13:54:09.960573: step 3622, loss 0.101522, acc 0.953125, prec 0.0618933, recall 0.881694
2017-12-10T13:54:10.152280: step 3623, loss 0.26936, acc 0.984375, prec 0.0619119, recall 0.88173
2017-12-10T13:54:10.347852: step 3624, loss 0.26775, acc 0.953125, prec 0.061908, recall 0.88173
2017-12-10T13:54:10.539408: step 3625, loss 0.0169282, acc 1, prec 0.061908, recall 0.88173
2017-12-10T13:54:10.731054: step 3626, loss 0.0851853, acc 0.984375, prec 0.0619266, recall 0.881766
2017-12-10T13:54:10.921437: step 3627, loss 0.0769251, acc 0.96875, prec 0.0619439, recall 0.881802
2017-12-10T13:54:11.108189: step 3628, loss 0.247719, acc 0.9375, prec 0.0619785, recall 0.881873
2017-12-10T13:54:11.301059: step 3629, loss 0.14079, acc 0.984375, prec 0.0619772, recall 0.881873
2017-12-10T13:54:11.491090: step 3630, loss 0.0179466, acc 1, prec 0.0620369, recall 0.88198
2017-12-10T13:54:11.679454: step 3631, loss 2.45264, acc 0.984375, prec 0.0620767, recall 0.881785
2017-12-10T13:54:11.875751: step 3632, loss 0.38321, acc 0.984375, prec 0.0620953, recall 0.881821
2017-12-10T13:54:12.065608: step 3633, loss 0.229459, acc 0.90625, prec 0.0621073, recall 0.881857
2017-12-10T13:54:12.255825: step 3634, loss 0.344393, acc 0.921875, prec 0.0621007, recall 0.881857
2017-12-10T13:54:12.440756: step 3635, loss 0.109161, acc 0.921875, prec 0.062114, recall 0.881892
2017-12-10T13:54:12.632633: step 3636, loss 0.412899, acc 0.90625, prec 0.062126, recall 0.881928
2017-12-10T13:54:12.817572: step 3637, loss 0.120355, acc 0.90625, prec 0.0621181, recall 0.881928
2017-12-10T13:54:13.003996: step 3638, loss 0.197952, acc 0.921875, prec 0.0621513, recall 0.881999
2017-12-10T13:54:13.191890: step 3639, loss 0.177547, acc 0.984375, prec 0.06215, recall 0.881999
2017-12-10T13:54:13.378485: step 3640, loss 0.240869, acc 0.9375, prec 0.0621447, recall 0.881999
2017-12-10T13:54:13.564965: step 3641, loss 0.0620057, acc 0.96875, prec 0.0622018, recall 0.882105
2017-12-10T13:54:13.755929: step 3642, loss 0.306418, acc 0.921875, prec 0.0621952, recall 0.882105
2017-12-10T13:54:13.955481: step 3643, loss 3.06811, acc 0.953125, prec 0.0622124, recall 0.881876
2017-12-10T13:54:14.143889: step 3644, loss 0.419024, acc 0.921875, prec 0.0622257, recall 0.881911
2017-12-10T13:54:14.335362: step 3645, loss 0.328132, acc 0.890625, prec 0.0622165, recall 0.881911
2017-12-10T13:54:14.542449: step 3646, loss 0.265565, acc 0.875, prec 0.0622059, recall 0.881911
2017-12-10T13:54:14.732240: step 3647, loss 0.564534, acc 0.8125, prec 0.0621901, recall 0.881911
2017-12-10T13:54:14.920417: step 3648, loss 0.587087, acc 0.859375, prec 0.0621783, recall 0.881911
2017-12-10T13:54:15.107681: step 3649, loss 0.224493, acc 0.921875, prec 0.0622114, recall 0.881982
2017-12-10T13:54:15.295642: step 3650, loss 0.449008, acc 0.890625, prec 0.0622022, recall 0.881982
2017-12-10T13:54:15.481020: step 3651, loss 0.18268, acc 0.9375, prec 0.0622168, recall 0.882017
2017-12-10T13:54:15.669620: step 3652, loss 0.204671, acc 0.921875, prec 0.0622102, recall 0.882017
2017-12-10T13:54:15.857799: step 3653, loss 0.402428, acc 0.921875, prec 0.0622234, recall 0.882053
2017-12-10T13:54:16.046665: step 3654, loss 0.516192, acc 0.84375, prec 0.0622301, recall 0.882088
2017-12-10T13:54:16.239029: step 3655, loss 0.420725, acc 0.90625, prec 0.0622421, recall 0.882124
2017-12-10T13:54:16.431175: step 3656, loss 0.581547, acc 0.765625, prec 0.0622422, recall 0.882159
2017-12-10T13:54:16.621715: step 3657, loss 0.283466, acc 0.890625, prec 0.0622329, recall 0.882159
2017-12-10T13:54:16.809186: step 3658, loss 0.173568, acc 0.921875, prec 0.0622264, recall 0.882159
2017-12-10T13:54:16.996846: step 3659, loss 0.0909733, acc 0.953125, prec 0.0622422, recall 0.882194
2017-12-10T13:54:17.190891: step 3660, loss 0.241794, acc 0.890625, prec 0.062233, recall 0.882194
2017-12-10T13:54:17.382228: step 3661, loss 0.0316733, acc 1, prec 0.062233, recall 0.882194
2017-12-10T13:54:17.571070: step 3662, loss 0.152231, acc 0.9375, prec 0.0622674, recall 0.882265
2017-12-10T13:54:17.758286: step 3663, loss 0.0714728, acc 0.953125, prec 0.0622635, recall 0.882265
2017-12-10T13:54:17.949054: step 3664, loss 0.252695, acc 0.953125, prec 0.0622595, recall 0.882265
2017-12-10T13:54:18.135868: step 3665, loss 0.323985, acc 0.90625, prec 0.0622516, recall 0.882265
2017-12-10T13:54:18.323834: step 3666, loss 0.252123, acc 0.953125, prec 0.0622675, recall 0.8823
2017-12-10T13:54:18.515346: step 3667, loss 2.86288, acc 0.9375, prec 0.0622636, recall 0.882036
2017-12-10T13:54:18.704894: step 3668, loss 0.0228453, acc 1, prec 0.0622834, recall 0.882071
2017-12-10T13:54:18.890815: step 3669, loss 0.311179, acc 0.90625, prec 0.0623349, recall 0.882177
2017-12-10T13:54:19.081255: step 3670, loss 0.124897, acc 0.96875, prec 0.0623917, recall 0.882283
2017-12-10T13:54:19.272945: step 3671, loss 0.239054, acc 0.953125, prec 0.0623878, recall 0.882283
2017-12-10T13:54:19.464335: step 3672, loss 0.0968772, acc 0.984375, prec 0.0624063, recall 0.882318
2017-12-10T13:54:19.650790: step 3673, loss 0.0978917, acc 0.9375, prec 0.062401, recall 0.882318
2017-12-10T13:54:19.837731: step 3674, loss 0.0422319, acc 0.984375, prec 0.0623997, recall 0.882318
2017-12-10T13:54:20.023646: step 3675, loss 0.139541, acc 0.953125, prec 0.0624155, recall 0.882353
2017-12-10T13:54:20.213600: step 3676, loss 0.18149, acc 0.9375, prec 0.0624498, recall 0.882423
2017-12-10T13:54:20.403062: step 3677, loss 0.161188, acc 0.953125, prec 0.0624459, recall 0.882423
2017-12-10T13:54:20.590820: step 3678, loss 0.455788, acc 0.953125, prec 0.0624419, recall 0.882423
2017-12-10T13:54:20.779738: step 3679, loss 0.0791518, acc 0.96875, prec 0.0624591, recall 0.882458
2017-12-10T13:54:20.971596: step 3680, loss 0.363003, acc 0.96875, prec 0.062496, recall 0.882528
2017-12-10T13:54:21.163555: step 3681, loss 0.14742, acc 0.953125, prec 0.0625119, recall 0.882563
2017-12-10T13:54:21.356384: step 3682, loss 0.240912, acc 0.953125, prec 0.0625079, recall 0.882563
2017-12-10T13:54:21.542891: step 3683, loss 0.305469, acc 0.890625, prec 0.0625185, recall 0.882598
2017-12-10T13:54:21.729710: step 3684, loss 0.241234, acc 0.90625, prec 0.0625501, recall 0.882668
2017-12-10T13:54:21.918976: step 3685, loss 0.2594, acc 0.9375, prec 0.0625646, recall 0.882703
2017-12-10T13:54:22.106047: step 3686, loss 0.0456893, acc 0.984375, prec 0.0625633, recall 0.882703
2017-12-10T13:54:22.294167: step 3687, loss 0.0754515, acc 0.984375, prec 0.0625818, recall 0.882738
2017-12-10T13:54:22.481911: step 3688, loss 0.833729, acc 0.96875, prec 0.0626187, recall 0.882808
2017-12-10T13:54:22.671421: step 3689, loss 0.0913522, acc 0.96875, prec 0.0626358, recall 0.882843
2017-12-10T13:54:22.858953: step 3690, loss 0.0982043, acc 0.953125, prec 0.0626516, recall 0.882878
2017-12-10T13:54:23.047322: step 3691, loss 0.684698, acc 0.96875, prec 0.0626687, recall 0.882912
2017-12-10T13:54:23.237382: step 3692, loss 0.175979, acc 0.953125, prec 0.0626648, recall 0.882912
2017-12-10T13:54:23.426609: step 3693, loss 0.32016, acc 0.9375, prec 0.0626595, recall 0.882912
2017-12-10T13:54:23.612119: step 3694, loss 0.133847, acc 0.96875, prec 0.0626568, recall 0.882912
2017-12-10T13:54:23.796550: step 3695, loss 0.18108, acc 0.9375, prec 0.0626516, recall 0.882912
2017-12-10T13:54:23.984896: step 3696, loss 0.205608, acc 0.921875, prec 0.0626845, recall 0.882982
2017-12-10T13:54:24.172617: step 3697, loss 0.154956, acc 0.953125, prec 0.0627003, recall 0.883017
2017-12-10T13:54:24.357751: step 3698, loss 0.144835, acc 0.9375, prec 0.0627345, recall 0.883086
2017-12-10T13:54:24.548689: step 3699, loss 0.0416923, acc 0.96875, prec 0.0627516, recall 0.883121
2017-12-10T13:54:24.736332: step 3700, loss 0.069089, acc 0.984375, prec 0.0627503, recall 0.883121
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_0/1512931325/checkpoints/model-3700

2017-12-10T13:54:26.015488: step 3701, loss 0.0746285, acc 0.96875, prec 0.0627477, recall 0.883121
2017-12-10T13:54:26.204944: step 3702, loss 0.258182, acc 0.9375, prec 0.0627621, recall 0.883155
2017-12-10T13:54:26.395348: step 3703, loss 0.174547, acc 0.921875, prec 0.0627753, recall 0.88319
2017-12-10T13:54:26.584849: step 3704, loss 0.147278, acc 0.96875, prec 0.0628121, recall 0.883259
2017-12-10T13:54:26.779359: step 3705, loss 0.393205, acc 0.90625, prec 0.0628042, recall 0.883259
2017-12-10T13:54:26.963728: step 3706, loss 0.345974, acc 0.96875, prec 0.0628213, recall 0.883294
2017-12-10T13:54:27.154208: step 3707, loss 0.79869, acc 0.953125, prec 0.062837, recall 0.883328
2017-12-10T13:54:27.344646: step 3708, loss 0.0877471, acc 0.953125, prec 0.0628528, recall 0.883363
2017-12-10T13:54:27.533929: step 3709, loss 0.07791, acc 0.96875, prec 0.0628502, recall 0.883363
2017-12-10T13:54:27.723392: step 3710, loss 0.0986564, acc 0.9375, prec 0.0628646, recall 0.883397
2017-12-10T13:54:27.911788: step 3711, loss 0.221285, acc 0.921875, prec 0.062858, recall 0.883397
2017-12-10T13:54:28.102552: step 3712, loss 0.0123362, acc 1, prec 0.062858, recall 0.883397
2017-12-10T13:54:28.288133: step 3713, loss 0.3401, acc 0.9375, prec 0.0628724, recall 0.883432
2017-12-10T13:54:28.476763: step 3714, loss 0.380377, acc 0.921875, prec 0.0628658, recall 0.883432
2017-12-10T13:54:28.664864: step 3715, loss 0.274293, acc 0.90625, prec 0.0628579, recall 0.883432
2017-12-10T13:54:28.856749: step 3716, loss 0.0303183, acc 1, prec 0.0628776, recall 0.883466
2017-12-10T13:54:29.046107: step 3717, loss 0.182269, acc 0.921875, prec 0.062871, recall 0.883466
2017-12-10T13:54:29.232253: step 3718, loss 0.0426496, acc 0.984375, prec 0.0629091, recall 0.883535
2017-12-10T13:54:29.417979: step 3719, loss 0.155264, acc 0.953125, prec 0.0629248, recall 0.88357
2017-12-10T13:54:29.605350: step 3720, loss 0.11268, acc 0.96875, prec 0.0629222, recall 0.88357
2017-12-10T13:54:29.795705: step 3721, loss 0.0532594, acc 1, prec 0.0629616, recall 0.883639
2017-12-10T13:54:29.985874: step 3722, loss 0.181118, acc 0.96875, prec 0.062959, recall 0.883639
2017-12-10T13:54:30.172858: step 3723, loss 0.215269, acc 0.921875, prec 0.0630115, recall 0.883741
2017-12-10T13:54:30.360662: step 3724, loss 0.180687, acc 0.921875, prec 0.0630049, recall 0.883741
2017-12-10T13:54:30.548401: step 3725, loss 0.0609286, acc 0.96875, prec 0.0630022, recall 0.883741
2017-12-10T13:54:30.737919: step 3726, loss 0.217437, acc 0.953125, prec 0.0630377, recall 0.88381
2017-12-10T13:54:30.928001: step 3727, loss 0.0490974, acc 0.96875, prec 0.063035, recall 0.88381
2017-12-10T13:54:31.120367: step 3728, loss 0.185222, acc 0.984375, prec 0.0630534, recall 0.883844
2017-12-10T13:54:31.314577: step 3729, loss 0.248632, acc 0.921875, prec 0.0630468, recall 0.883844
2017-12-10T13:54:31.504521: step 3730, loss 0.157276, acc 0.953125, prec 0.0630625, recall 0.883879
2017-12-10T13:54:31.695135: step 3731, loss 0.225987, acc 0.953125, prec 0.0630979, recall 0.883947
2017-12-10T13:54:31.885217: step 3732, loss 0.0724526, acc 0.96875, prec 0.0630953, recall 0.883947
2017-12-10T13:54:32.075577: step 3733, loss 0.0429222, acc 0.984375, prec 0.0630939, recall 0.883947
2017-12-10T13:54:32.266475: step 3734, loss 0.103435, acc 0.953125, prec 0.0631097, recall 0.883981
2017-12-10T13:54:32.454394: step 3735, loss 0.126989, acc 0.984375, prec 0.0631083, recall 0.883981
2017-12-10T13:54:32.648197: step 3736, loss 0.108771, acc 0.96875, prec 0.0631648, recall 0.884084
2017-12-10T13:54:32.840580: step 3737, loss 0.179463, acc 0.953125, prec 0.0631608, recall 0.884084
2017-12-10T13:54:33.027676: step 3738, loss 3.94898, acc 0.96875, prec 0.0631791, recall 0.883858
2017-12-10T13:54:33.223652: step 3739, loss 0.207295, acc 0.9375, prec 0.0631935, recall 0.883892
2017-12-10T13:54:33.413980: step 3740, loss 0.0763461, acc 0.953125, prec 0.0631895, recall 0.883892
2017-12-10T13:54:33.603265: step 3741, loss 0.247613, acc 0.953125, prec 0.0632249, recall 0.88396
2017-12-10T13:54:33.794369: step 3742, loss 0.196453, acc 0.96875, prec 0.0632223, recall 0.88396
2017-12-10T13:54:33.985328: step 3743, loss 0.307413, acc 0.921875, prec 0.063255, recall 0.884028
2017-12-10T13:54:34.172297: step 3744, loss 0.317151, acc 0.953125, prec 0.0632707, recall 0.884062
2017-12-10T13:54:34.361104: step 3745, loss 0.531244, acc 0.9375, prec 0.0632653, recall 0.884062
2017-12-10T13:54:34.546974: step 3746, loss 0.181004, acc 0.9375, prec 0.06326, recall 0.884062
2017-12-10T13:54:34.732037: step 3747, loss 0.231295, acc 0.9375, prec 0.0632547, recall 0.884062
2017-12-10T13:54:34.920794: step 3748, loss 0.189351, acc 0.96875, prec 0.0632521, recall 0.884062
2017-12-10T13:54:35.112715: step 3749, loss 0.184166, acc 0.921875, prec 0.0632651, recall 0.884096
2017-12-10T13:54:35.303485: step 3750, loss 0.2576, acc 0.921875, prec 0.0632978, recall 0.884164
2017-12-10T13:54:35.491049: step 3751, loss 0.0853992, acc 0.953125, prec 0.0633135, recall 0.884198
2017-12-10T13:54:35.679304: step 3752, loss 0.3943, acc 0.90625, prec 0.0633055, recall 0.884198
2017-12-10T13:54:35.870400: step 3753, loss 0.167659, acc 0.96875, prec 0.0633225, recall 0.884232
2017-12-10T13:54:36.062288: step 3754, loss 0.136814, acc 0.953125, prec 0.0633382, recall 0.884266
2017-12-10T13:54:36.250584: step 3755, loss 0.494142, acc 0.890625, prec 0.0633485, recall 0.8843
2017-12-10T13:54:36.436098: step 3756, loss 0.0308433, acc 0.984375, prec 0.0633472, recall 0.8843
2017-12-10T13:54:36.627255: step 3757, loss 0.0327546, acc 0.984375, prec 0.0633852, recall 0.884368
2017-12-10T13:54:36.813402: step 3758, loss 0.113678, acc 0.953125, prec 0.0634008, recall 0.884402
2017-12-10T13:54:37.001096: step 3759, loss 0.0468339, acc 0.96875, prec 0.0633982, recall 0.884402
2017-12-10T13:54:37.195955: step 3760, loss 0.0332108, acc 1, prec 0.0634375, recall 0.884469
2017-12-10T13:54:37.385485: step 3761, loss 0.229234, acc 0.90625, prec 0.0634295, recall 0.884469
2017-12-10T13:54:37.573179: step 3762, loss 0.141442, acc 0.953125, prec 0.0634451, recall 0.884503
2017-12-10T13:54:37.760934: step 3763, loss 0.0204589, acc 1, prec 0.0634648, recall 0.884537
2017-12-10T13:54:37.949419: step 3764, loss 0.0382321, acc 1, prec 0.063504, recall 0.884604
2017-12-10T13:54:38.142284: step 3765, loss 0.0638415, acc 0.96875, prec 0.063521, recall 0.884638
2017-12-10T13:54:38.334815: step 3766, loss 0.0948768, acc 0.96875, prec 0.0635184, recall 0.884638
2017-12-10T13:54:38.527721: step 3767, loss 0.113734, acc 0.96875, prec 0.0635157, recall 0.884638
2017-12-10T13:54:38.716926: step 3768, loss 0.0761772, acc 0.953125, prec 0.0635313, recall 0.884672
2017-12-10T13:54:38.903512: step 3769, loss 0.326934, acc 0.90625, prec 0.0635233, recall 0.884672
2017-12-10T13:54:39.090864: step 3770, loss 0.0792655, acc 0.96875, prec 0.0635403, recall 0.884705
2017-12-10T13:54:39.284945: step 3771, loss 0.0754898, acc 0.984375, prec 0.063539, recall 0.884705
2017-12-10T13:54:39.474571: step 3772, loss 0.213434, acc 0.921875, prec 0.0635519, recall 0.884739
2017-12-10T13:54:39.660498: step 3773, loss 0.305807, acc 0.96875, prec 0.0635493, recall 0.884739
2017-12-10T13:54:39.849577: step 3774, loss 1.52752, acc 0.9375, prec 0.0635453, recall 0.884481
2017-12-10T13:54:40.043143: step 3775, loss 0.297327, acc 0.953125, prec 0.0635805, recall 0.884548
2017-12-10T13:54:40.236986: step 3776, loss 0.0726544, acc 0.953125, prec 0.0635765, recall 0.884548
2017-12-10T13:54:40.429224: step 3777, loss 0.163007, acc 0.9375, prec 0.0635908, recall 0.884582
2017-12-10T13:54:40.619760: step 3778, loss 0.134868, acc 0.953125, prec 0.0636065, recall 0.884615
2017-12-10T13:54:40.810514: step 3779, loss 0.0421077, acc 0.984375, prec 0.0636247, recall 0.884649
2017-12-10T13:54:41.003115: step 3780, loss 0.0988305, acc 0.96875, prec 0.0636417, recall 0.884683
2017-12-10T13:54:41.193540: step 3781, loss 0.129048, acc 0.96875, prec 0.0636586, recall 0.884716
2017-12-10T13:54:41.379760: step 3782, loss 0.86346, acc 0.984375, prec 0.0636769, recall 0.88475
2017-12-10T13:54:41.574437: step 3783, loss 0.205549, acc 0.921875, prec 0.0636703, recall 0.88475
2017-12-10T13:54:41.767041: step 3784, loss 0.536921, acc 0.859375, prec 0.0636779, recall 0.884783
2017-12-10T13:54:41.953840: step 3785, loss 0.159629, acc 0.921875, prec 0.06373, recall 0.884884
2017-12-10T13:54:42.139425: step 3786, loss 0.406949, acc 0.953125, prec 0.0637456, recall 0.884917
2017-12-10T13:54:42.329803: step 3787, loss 0.224842, acc 0.90625, prec 0.0637572, recall 0.884951
2017-12-10T13:54:42.520848: step 3788, loss 0.0707483, acc 0.984375, prec 0.0637559, recall 0.884951
2017-12-10T13:54:42.711058: step 3789, loss 0.345949, acc 0.921875, prec 0.0637492, recall 0.884951
2017-12-10T13:54:42.902993: step 3790, loss 0.248842, acc 0.921875, prec 0.0637425, recall 0.884951
2017-12-10T13:54:43.095695: step 3791, loss 0.320563, acc 0.890625, prec 0.0637332, recall 0.884951
2017-12-10T13:54:43.283963: step 3792, loss 0.218399, acc 0.953125, prec 0.0637684, recall 0.885017
2017-12-10T13:54:43.476986: step 3793, loss 0.146264, acc 0.921875, prec 0.0637813, recall 0.885051
2017-12-10T13:54:43.667514: step 3794, loss 0.258706, acc 0.953125, prec 0.0637969, recall 0.885084
2017-12-10T13:54:43.863466: step 3795, loss 0.158331, acc 0.9375, prec 0.0638111, recall 0.885117
2017-12-10T13:54:44.056576: step 3796, loss 0.106785, acc 0.96875, prec 0.0638476, recall 0.885184
2017-12-10T13:54:44.247926: step 3797, loss 0.788137, acc 0.828125, prec 0.0638525, recall 0.885217
2017-12-10T13:54:44.435218: step 3798, loss 0.227329, acc 0.921875, prec 0.0638654, recall 0.885251
2017-12-10T13:54:44.624195: step 3799, loss 0.128901, acc 0.921875, prec 0.0638587, recall 0.885251
2017-12-10T13:54:44.812963: step 3800, loss 0.0780741, acc 0.96875, prec 0.063856, recall 0.885251
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_0/1512931325/checkpoints/model-3800

2017-12-10T13:54:46.092865: step 3801, loss 0.176472, acc 0.90625, prec 0.063848, recall 0.885251
2017-12-10T13:54:46.279399: step 3802, loss 0.00981053, acc 1, prec 0.0638676, recall 0.885284
2017-12-10T13:54:46.467857: step 3803, loss 0.179904, acc 0.96875, prec 0.0638649, recall 0.885284
2017-12-10T13:54:46.656448: step 3804, loss 0.0765133, acc 0.96875, prec 0.0638622, recall 0.885284
2017-12-10T13:54:46.844672: step 3805, loss 0.122898, acc 0.96875, prec 0.0638791, recall 0.885317
2017-12-10T13:54:47.033832: step 3806, loss 0.107738, acc 0.984375, prec 0.0638974, recall 0.88535
2017-12-10T13:54:47.222918: step 3807, loss 0.047835, acc 0.984375, prec 0.063896, recall 0.88535
2017-12-10T13:54:47.411796: step 3808, loss 1.08716, acc 0.9375, prec 0.0639298, recall 0.885417
2017-12-10T13:54:47.606741: step 3809, loss 0.150821, acc 0.9375, prec 0.063944, recall 0.88545
2017-12-10T13:54:47.798472: step 3810, loss 0.102606, acc 0.96875, prec 0.0639413, recall 0.88545
2017-12-10T13:54:47.987983: step 3811, loss 0.532701, acc 0.953125, prec 0.0639569, recall 0.885483
2017-12-10T13:54:48.182910: step 3812, loss 1.13021, acc 0.984375, prec 0.0639764, recall 0.88526
2017-12-10T13:54:48.376135: step 3813, loss 0.121715, acc 0.984375, prec 0.0639947, recall 0.885293
2017-12-10T13:54:48.564252: step 3814, loss 0.164167, acc 0.953125, prec 0.0640102, recall 0.885326
2017-12-10T13:54:48.756647: step 3815, loss 0.305035, acc 0.9375, prec 0.0640439, recall 0.885393
2017-12-10T13:54:48.943211: step 3816, loss 0.0655828, acc 0.96875, prec 0.0640608, recall 0.885426
2017-12-10T13:54:49.132402: step 3817, loss 0.160616, acc 0.953125, prec 0.0640959, recall 0.885492
2017-12-10T13:54:49.323827: step 3818, loss 0.161273, acc 0.953125, prec 0.0641114, recall 0.885525
2017-12-10T13:54:49.514190: step 3819, loss 0.142153, acc 0.984375, prec 0.0641491, recall 0.885591
2017-12-10T13:54:49.703156: step 3820, loss 0.0999546, acc 0.9375, prec 0.0641438, recall 0.885591
2017-12-10T13:54:49.889604: step 3821, loss 0.410921, acc 0.921875, prec 0.0641566, recall 0.885624
2017-12-10T13:54:50.079542: step 3822, loss 0.475108, acc 0.921875, prec 0.0641695, recall 0.885657
2017-12-10T13:54:50.272267: step 3823, loss 0.0462417, acc 0.96875, prec 0.0641668, recall 0.885657
2017-12-10T13:54:50.458857: step 3824, loss 0.0959063, acc 0.9375, prec 0.0642005, recall 0.885723
2017-12-10T13:54:50.651122: step 3825, loss 0.19924, acc 0.96875, prec 0.0641978, recall 0.885723
2017-12-10T13:54:50.843081: step 3826, loss 1.89551, acc 0.859375, prec 0.0642066, recall 0.885501
2017-12-10T13:54:51.033837: step 3827, loss 0.18894, acc 0.9375, prec 0.0642012, recall 0.885501
2017-12-10T13:54:51.223586: step 3828, loss 0.303175, acc 0.921875, prec 0.0642141, recall 0.885534
2017-12-10T13:54:51.413421: step 3829, loss 0.163335, acc 0.953125, prec 0.06421, recall 0.885534
2017-12-10T13:54:51.604130: step 3830, loss 0.445647, acc 0.890625, prec 0.0642202, recall 0.885566
2017-12-10T13:54:51.797087: step 3831, loss 0.608571, acc 0.890625, prec 0.0642108, recall 0.885566
2017-12-10T13:54:51.987003: step 3832, loss 0.275172, acc 0.9375, prec 0.0642055, recall 0.885566
2017-12-10T13:54:52.179472: step 3833, loss 0.187255, acc 0.9375, prec 0.0642196, recall 0.885599
2017-12-10T13:54:52.366653: step 3834, loss 1.05262, acc 0.84375, prec 0.0642257, recall 0.885632
2017-12-10T13:54:52.553204: step 3835, loss 0.298032, acc 0.890625, prec 0.0642359, recall 0.885665
2017-12-10T13:54:52.740096: step 3836, loss 0.483065, acc 0.890625, prec 0.064246, recall 0.885698
2017-12-10T13:54:52.928581: step 3837, loss 0.316236, acc 0.890625, prec 0.0642561, recall 0.885731
2017-12-10T13:54:53.124099: step 3838, loss 0.33853, acc 0.90625, prec 0.0642871, recall 0.885796
2017-12-10T13:54:53.315854: step 3839, loss 0.155314, acc 0.9375, prec 0.0642817, recall 0.885796
2017-12-10T13:54:53.506817: step 3840, loss 0.0930097, acc 0.96875, prec 0.064279, recall 0.885796
2017-12-10T13:54:53.694569: step 3841, loss 0.449169, acc 0.890625, prec 0.0642697, recall 0.885796
2017-12-10T13:54:53.881594: step 3842, loss 0.338467, acc 0.875, prec 0.0642784, recall 0.885829
2017-12-10T13:54:54.069407: step 3843, loss 0.190591, acc 0.921875, prec 0.0642912, recall 0.885862
2017-12-10T13:54:54.262485: step 3844, loss 0.0450191, acc 0.96875, prec 0.064308, recall 0.885894
2017-12-10T13:54:54.453580: step 3845, loss 0.0605536, acc 0.953125, prec 0.064304, recall 0.885894
2017-12-10T13:54:54.646953: step 3846, loss 0.0786377, acc 1, prec 0.0643429, recall 0.88596
2017-12-10T13:54:54.838977: step 3847, loss 0.163784, acc 0.953125, prec 0.0643779, recall 0.886025
2017-12-10T13:54:55.028362: step 3848, loss 0.208746, acc 0.96875, prec 0.0644141, recall 0.88609
2017-12-10T13:54:55.217576: step 3849, loss 3.13164, acc 0.921875, prec 0.0644088, recall 0.885837
2017-12-10T13:54:55.407496: step 3850, loss 0.327218, acc 0.9375, prec 0.0644229, recall 0.88587
2017-12-10T13:54:55.596603: step 3851, loss 0.146758, acc 0.921875, prec 0.0644356, recall 0.885902
2017-12-10T13:54:55.786779: step 3852, loss 0.141574, acc 0.96875, prec 0.0644524, recall 0.885935
2017-12-10T13:54:55.976499: step 3853, loss 0.0152996, acc 1, prec 0.0644718, recall 0.885967
2017-12-10T13:54:56.163361: step 3854, loss 0.492232, acc 0.875, prec 0.0644611, recall 0.885967
2017-12-10T13:54:56.352926: step 3855, loss 0.358893, acc 0.890625, prec 0.0644517, recall 0.885967
2017-12-10T13:54:56.539834: step 3856, loss 0.225999, acc 0.953125, prec 0.0644672, recall 0.886
2017-12-10T13:54:56.726737: step 3857, loss 0.282354, acc 0.9375, prec 0.0644618, recall 0.886
2017-12-10T13:54:56.915635: step 3858, loss 0.272786, acc 0.90625, prec 0.0644538, recall 0.886
2017-12-10T13:54:57.102399: step 3859, loss 0.0402811, acc 0.984375, prec 0.0644524, recall 0.886
2017-12-10T13:54:57.291283: step 3860, loss 0.161524, acc 0.890625, prec 0.0644625, recall 0.886033
2017-12-10T13:54:57.480432: step 3861, loss 0.127317, acc 0.96875, prec 0.0644793, recall 0.886065
2017-12-10T13:54:57.671511: step 3862, loss 0.151707, acc 0.9375, prec 0.0645322, recall 0.886163
2017-12-10T13:54:57.859943: step 3863, loss 0.557403, acc 0.875, prec 0.0645215, recall 0.886163
2017-12-10T13:54:58.044645: step 3864, loss 0.202264, acc 0.9375, prec 0.0645356, recall 0.886195
2017-12-10T13:54:58.236101: step 3865, loss 0.0427291, acc 0.984375, prec 0.0645342, recall 0.886195
2017-12-10T13:54:58.424805: step 3866, loss 0.291689, acc 0.9375, prec 0.0645289, recall 0.886195
2017-12-10T13:54:58.615821: step 3867, loss 0.121422, acc 0.953125, prec 0.0646025, recall 0.886325
2017-12-10T13:54:58.810644: step 3868, loss 0.201682, acc 0.953125, prec 0.0646179, recall 0.886357
2017-12-10T13:54:59.005180: step 3869, loss 0.335918, acc 0.921875, prec 0.0646112, recall 0.886357
2017-12-10T13:54:59.197352: step 3870, loss 0.18066, acc 0.9375, prec 0.0646253, recall 0.886389
2017-12-10T13:54:59.383768: step 3871, loss 0.168071, acc 0.953125, prec 0.0646407, recall 0.886422
2017-12-10T13:54:59.576337: step 3872, loss 0.45434, acc 0.9375, prec 0.0646935, recall 0.886519
2017-12-10T13:54:59.764135: step 3873, loss 0.115751, acc 0.96875, prec 0.0647103, recall 0.886551
2017-12-10T13:54:59.954677: step 3874, loss 0.19693, acc 0.96875, prec 0.064727, recall 0.886583
2017-12-10T13:55:00.146149: step 3875, loss 1.22276, acc 0.921875, prec 0.0647785, recall 0.88668
2017-12-10T13:55:00.336247: step 3876, loss 0.0387654, acc 0.984375, prec 0.0647772, recall 0.88668
2017-12-10T13:55:00.525860: step 3877, loss 0.16217, acc 0.984375, prec 0.0647758, recall 0.88668
2017-12-10T13:55:00.714934: step 3878, loss 0.190658, acc 0.953125, prec 0.0647912, recall 0.886712
2017-12-10T13:55:00.904534: step 3879, loss 0.184335, acc 0.953125, prec 0.0647872, recall 0.886712
2017-12-10T13:55:01.094443: step 3880, loss 0.153374, acc 0.984375, prec 0.0648052, recall 0.886744
2017-12-10T13:55:01.292060: step 3881, loss 0.152211, acc 0.96875, prec 0.0648219, recall 0.886776
2017-12-10T13:55:01.480408: step 3882, loss 0.325051, acc 0.90625, prec 0.0648139, recall 0.886776
2017-12-10T13:55:01.668475: step 3883, loss 0.227985, acc 0.96875, prec 0.0648112, recall 0.886776
2017-12-10T13:55:01.856905: step 3884, loss 0.250842, acc 0.953125, prec 0.0648071, recall 0.886776
2017-12-10T13:55:02.045901: step 3885, loss 0.024399, acc 0.984375, prec 0.0648252, recall 0.886809
2017-12-10T13:55:02.238400: step 3886, loss 0.200342, acc 0.953125, prec 0.0648212, recall 0.886809
2017-12-10T13:55:02.432218: step 3887, loss 0.112733, acc 0.953125, prec 0.0648365, recall 0.886841
2017-12-10T13:55:02.624558: step 3888, loss 0.968805, acc 0.984375, prec 0.0648739, recall 0.886905
2017-12-10T13:55:02.820034: step 3889, loss 0.267793, acc 0.984375, prec 0.064892, recall 0.886937
2017-12-10T13:55:03.019673: step 3890, loss 0.230767, acc 0.96875, prec 0.0649281, recall 0.887001
2017-12-10T13:55:03.208512: step 3891, loss 0.0823637, acc 0.96875, prec 0.0649254, recall 0.887001
2017-12-10T13:55:03.399749: step 3892, loss 0.541803, acc 0.9375, prec 0.0649394, recall 0.887033
2017-12-10T13:55:03.594607: step 3893, loss 0.088415, acc 0.953125, prec 0.0649741, recall 0.887097
2017-12-10T13:55:03.789924: step 3894, loss 0.0564849, acc 0.984375, prec 0.0650309, recall 0.887193
2017-12-10T13:55:03.973941: step 3895, loss 0.184773, acc 0.921875, prec 0.0650823, recall 0.887288
2017-12-10T13:55:04.159566: step 3896, loss 0.392769, acc 0.953125, prec 0.0650976, recall 0.88732
2017-12-10T13:55:04.353023: step 3897, loss 0.175669, acc 0.953125, prec 0.0650935, recall 0.88732
2017-12-10T13:55:04.545138: step 3898, loss 2.08275, acc 0.890625, prec 0.0651048, recall 0.887101
2017-12-10T13:55:04.736816: step 3899, loss 0.245194, acc 0.953125, prec 0.0651589, recall 0.887197
2017-12-10T13:55:04.923752: step 3900, loss 0.42789, acc 0.8125, prec 0.0651427, recall 0.887197
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_0/1512931325/checkpoints/model-3900

2017-12-10T13:55:06.206216: step 3901, loss 0.290662, acc 0.890625, prec 0.0651526, recall 0.887229
2017-12-10T13:55:06.393658: step 3902, loss 0.359045, acc 0.9375, prec 0.0652052, recall 0.887324
2017-12-10T13:55:06.584699: step 3903, loss 1.16292, acc 0.8125, prec 0.0652277, recall 0.887387
2017-12-10T13:55:06.770358: step 3904, loss 0.343348, acc 0.859375, prec 0.0652349, recall 0.887419
2017-12-10T13:55:06.958811: step 3905, loss 0.787366, acc 0.796875, prec 0.0652174, recall 0.887419
2017-12-10T13:55:07.143961: step 3906, loss 0.437425, acc 0.875, prec 0.0652259, recall 0.887451
2017-12-10T13:55:07.332253: step 3907, loss 0.770039, acc 0.828125, prec 0.0652884, recall 0.887577
2017-12-10T13:55:07.518875: step 3908, loss 0.50269, acc 0.859375, prec 0.0652763, recall 0.887577
2017-12-10T13:55:07.702772: step 3909, loss 0.279728, acc 0.890625, prec 0.0652668, recall 0.887577
2017-12-10T13:55:07.890450: step 3910, loss 0.298274, acc 0.890625, prec 0.0653153, recall 0.887672
2017-12-10T13:55:08.081983: step 3911, loss 0.264425, acc 0.859375, prec 0.0653225, recall 0.887704
2017-12-10T13:55:08.270668: step 3912, loss 0.146501, acc 0.953125, prec 0.0653377, recall 0.887735
2017-12-10T13:55:08.460417: step 3913, loss 0.592875, acc 0.84375, prec 0.0653242, recall 0.887735
2017-12-10T13:55:08.647835: step 3914, loss 0.626131, acc 0.9375, prec 0.0653381, recall 0.887767
2017-12-10T13:55:08.838392: step 3915, loss 0.217251, acc 0.9375, prec 0.0653907, recall 0.887861
2017-12-10T13:55:09.027076: step 3916, loss 0.157053, acc 0.953125, prec 0.0654059, recall 0.887892
2017-12-10T13:55:09.218277: step 3917, loss 0.180064, acc 0.9375, prec 0.0654005, recall 0.887892
2017-12-10T13:55:09.406785: step 3918, loss 0.329126, acc 0.90625, prec 0.065431, recall 0.887955
2017-12-10T13:55:09.594473: step 3919, loss 0.0738839, acc 0.953125, prec 0.0654269, recall 0.887955
2017-12-10T13:55:09.784855: step 3920, loss 0.236067, acc 0.953125, prec 0.0654807, recall 0.888049
2017-12-10T13:55:09.977098: step 3921, loss 0.127472, acc 0.9375, prec 0.0654753, recall 0.888049
2017-12-10T13:55:10.163270: step 3922, loss 0.042277, acc 1, prec 0.0654753, recall 0.888049
2017-12-10T13:55:10.354634: step 3923, loss 0.0450854, acc 0.984375, prec 0.0654933, recall 0.888081
2017-12-10T13:55:10.550334: step 3924, loss 0.273857, acc 0.953125, prec 0.0654892, recall 0.888081
2017-12-10T13:55:10.736549: step 3925, loss 1.08087, acc 0.953125, prec 0.0655237, recall 0.888143
2017-12-10T13:55:10.925319: step 3926, loss 6.98718, acc 0.953125, prec 0.0655403, recall 0.887926
2017-12-10T13:55:11.116463: step 3927, loss 0.180727, acc 0.9375, prec 0.0655349, recall 0.887926
2017-12-10T13:55:11.306008: step 3928, loss 0.299037, acc 0.890625, prec 0.0655254, recall 0.887926
2017-12-10T13:55:11.495811: step 3929, loss 0.0145475, acc 1, prec 0.0655447, recall 0.887958
2017-12-10T13:55:11.686491: step 3930, loss 0.210002, acc 0.9375, prec 0.0655586, recall 0.887989
2017-12-10T13:55:11.877232: step 3931, loss 0.217808, acc 0.90625, prec 0.0655504, recall 0.887989
2017-12-10T13:55:12.070299: step 3932, loss 0.312315, acc 0.875, prec 0.0655589, recall 0.88802
2017-12-10T13:55:12.255198: step 3933, loss 0.163622, acc 0.9375, prec 0.065592, recall 0.888083
2017-12-10T13:55:12.443817: step 3934, loss 0.515764, acc 0.859375, prec 0.0655799, recall 0.888083
2017-12-10T13:55:12.630657: step 3935, loss 0.672892, acc 0.84375, prec 0.0656048, recall 0.888145
2017-12-10T13:55:12.821547: step 3936, loss 0.496463, acc 0.859375, prec 0.0655927, recall 0.888145
2017-12-10T13:55:13.010502: step 3937, loss 0.478014, acc 0.875, prec 0.0656396, recall 0.888239
2017-12-10T13:55:13.201038: step 3938, loss 0.350022, acc 0.875, prec 0.065648, recall 0.88827
2017-12-10T13:55:13.393736: step 3939, loss 0.273828, acc 0.921875, prec 0.0656605, recall 0.888301
2017-12-10T13:55:13.587328: step 3940, loss 0.147802, acc 0.90625, prec 0.0656909, recall 0.888363
2017-12-10T13:55:13.781122: step 3941, loss 3.17988, acc 0.84375, prec 0.0656979, recall 0.888147
2017-12-10T13:55:13.981545: step 3942, loss 0.159486, acc 0.9375, prec 0.0657118, recall 0.888178
2017-12-10T13:55:14.170721: step 3943, loss 0.501467, acc 0.90625, prec 0.0657229, recall 0.888209
2017-12-10T13:55:14.357856: step 3944, loss 0.440167, acc 0.875, prec 0.065712, recall 0.888209
2017-12-10T13:55:14.547960: step 3945, loss 0.502097, acc 0.84375, prec 0.065737, recall 0.888271
2017-12-10T13:55:14.734537: step 3946, loss 0.348919, acc 0.921875, prec 0.0657494, recall 0.888302
2017-12-10T13:55:14.927430: step 3947, loss 0.142132, acc 0.9375, prec 0.0658017, recall 0.888395
2017-12-10T13:55:15.117854: step 3948, loss 0.408027, acc 0.890625, prec 0.0658114, recall 0.888426
2017-12-10T13:55:15.308760: step 3949, loss 0.237116, acc 0.921875, prec 0.065843, recall 0.888488
2017-12-10T13:55:15.498121: step 3950, loss 0.219401, acc 0.9375, prec 0.0658376, recall 0.888488
2017-12-10T13:55:15.684449: step 3951, loss 0.0651181, acc 0.96875, prec 0.0658349, recall 0.888488
2017-12-10T13:55:15.871684: step 3952, loss 0.35158, acc 0.921875, prec 0.0658665, recall 0.88855
2017-12-10T13:55:16.062552: step 3953, loss 0.334173, acc 0.875, prec 0.0658749, recall 0.888581
2017-12-10T13:55:16.253777: step 3954, loss 0.313795, acc 0.890625, prec 0.0659038, recall 0.888643
2017-12-10T13:55:16.440176: step 3955, loss 0.209456, acc 0.953125, prec 0.0659189, recall 0.888673
2017-12-10T13:55:16.633560: step 3956, loss 0.086843, acc 0.96875, prec 0.0659162, recall 0.888673
2017-12-10T13:55:16.820813: step 3957, loss 0.291464, acc 0.953125, prec 0.0659505, recall 0.888735
2017-12-10T13:55:17.012755: step 3958, loss 0.27402, acc 0.90625, prec 0.0659616, recall 0.888766
2017-12-10T13:55:17.202377: step 3959, loss 0.92035, acc 0.96875, prec 0.0659973, recall 0.888827
2017-12-10T13:55:17.395772: step 3960, loss 0.105964, acc 0.953125, prec 0.0660124, recall 0.888858
2017-12-10T13:55:17.589413: step 3961, loss 0.201172, acc 0.921875, prec 0.0660248, recall 0.888889
2017-12-10T13:55:17.779454: step 3962, loss 0.283644, acc 0.921875, prec 0.0660372, recall 0.88892
2017-12-10T13:55:17.970228: step 3963, loss 0.539041, acc 0.953125, prec 0.0660714, recall 0.888981
2017-12-10T13:55:18.160859: step 3964, loss 0.0929403, acc 0.96875, prec 0.0661262, recall 0.889073
2017-12-10T13:55:18.346051: step 3965, loss 0.319846, acc 0.90625, prec 0.0661181, recall 0.889073
2017-12-10T13:55:18.534833: step 3966, loss 0.206446, acc 0.921875, prec 0.0661305, recall 0.889103
2017-12-10T13:55:18.722389: step 3967, loss 0.241146, acc 0.9375, prec 0.0661442, recall 0.889134
2017-12-10T13:55:18.910801: step 3968, loss 0.135198, acc 0.9375, prec 0.0661579, recall 0.889165
2017-12-10T13:55:19.101917: step 3969, loss 0.417456, acc 0.90625, prec 0.0661498, recall 0.889165
2017-12-10T13:55:19.294326: step 3970, loss 0.119366, acc 0.96875, prec 0.0661662, recall 0.889195
2017-12-10T13:55:19.480834: step 3971, loss 0.546852, acc 0.90625, prec 0.0661581, recall 0.889195
2017-12-10T13:55:19.671365: step 3972, loss 0.091087, acc 0.96875, prec 0.0661554, recall 0.889195
2017-12-10T13:55:19.859672: step 3973, loss 0.216468, acc 0.90625, prec 0.0661472, recall 0.889195
2017-12-10T13:55:20.050075: step 3974, loss 0.0804815, acc 0.96875, prec 0.0661828, recall 0.889256
2017-12-10T13:55:20.248573: step 3975, loss 0.299589, acc 0.890625, prec 0.0661924, recall 0.889287
2017-12-10T13:55:20.417876: step 3976, loss 0.910934, acc 1, prec 0.0662116, recall 0.889317
2017-12-10T13:55:20.614925: step 3977, loss 0.0220609, acc 0.984375, prec 0.0662294, recall 0.889348
2017-12-10T13:55:20.803345: step 3978, loss 0.148379, acc 0.921875, prec 0.0662226, recall 0.889348
2017-12-10T13:55:20.992128: step 3979, loss 0.180511, acc 0.953125, prec 0.0662568, recall 0.889409
2017-12-10T13:55:21.179721: step 3980, loss 1.36109, acc 0.984375, prec 0.0662951, recall 0.889225
2017-12-10T13:55:21.371239: step 3981, loss 0.258296, acc 0.921875, prec 0.0662883, recall 0.889225
2017-12-10T13:55:21.556554: step 3982, loss 0.320011, acc 0.890625, prec 0.0662979, recall 0.889255
2017-12-10T13:55:21.747991: step 3983, loss 0.161257, acc 0.921875, prec 0.0663102, recall 0.889286
2017-12-10T13:55:21.935439: step 3984, loss 0.477911, acc 0.859375, prec 0.066298, recall 0.889286
2017-12-10T13:55:22.121390: step 3985, loss 0.0933388, acc 0.953125, prec 0.0662939, recall 0.889286
2017-12-10T13:55:22.313729: step 3986, loss 0.595136, acc 0.9375, prec 0.0663076, recall 0.889316
2017-12-10T13:55:22.506058: step 3987, loss 0.0511836, acc 0.984375, prec 0.0663063, recall 0.889316
2017-12-10T13:55:22.692819: step 3988, loss 0.147595, acc 0.953125, prec 0.0663022, recall 0.889316
2017-12-10T13:55:22.884869: step 3989, loss 0.280917, acc 0.90625, prec 0.066294, recall 0.889316
2017-12-10T13:55:23.072557: step 3990, loss 0.235957, acc 0.921875, prec 0.0662873, recall 0.889316
2017-12-10T13:55:23.266835: step 3991, loss 0.0502404, acc 0.984375, prec 0.066305, recall 0.889347
2017-12-10T13:55:23.461506: step 3992, loss 0.279132, acc 0.890625, prec 0.0662955, recall 0.889347
2017-12-10T13:55:23.648550: step 3993, loss 0.530188, acc 0.90625, prec 0.0663065, recall 0.889377
2017-12-10T13:55:23.841540: step 3994, loss 0.462083, acc 0.90625, prec 0.0663174, recall 0.889407
2017-12-10T13:55:24.029707: step 3995, loss 0.366826, acc 0.921875, prec 0.0663107, recall 0.889407
2017-12-10T13:55:24.220099: step 3996, loss 0.343925, acc 0.90625, prec 0.0663216, recall 0.889438
2017-12-10T13:55:24.412711: step 3997, loss 0.224213, acc 0.90625, prec 0.0663326, recall 0.889468
2017-12-10T13:55:24.603896: step 3998, loss 0.235348, acc 0.9375, prec 0.0663272, recall 0.889468
2017-12-10T13:55:24.794064: step 3999, loss 0.168131, acc 0.953125, prec 0.0663422, recall 0.889498
2017-12-10T13:55:24.982961: step 4000, loss 0.269604, acc 0.96875, prec 0.0663586, recall 0.889529

Evaluation:
2017-12-10T13:55:29.347955: step 4000, loss 4.06838, acc 0.941787, prec 0.0671003, recall 0.871459

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_0/1512931325/checkpoints/model-4000

2017-12-10T13:55:30.626052: step 4001, loss 0.33112, acc 0.9375, prec 0.0671137, recall 0.871492
2017-12-10T13:55:30.815139: step 4002, loss 0.173748, acc 0.96875, prec 0.0671487, recall 0.87156
2017-12-10T13:55:31.012325: step 4003, loss 0.117661, acc 0.9375, prec 0.0671809, recall 0.871627
2017-12-10T13:55:31.203190: step 4004, loss 0.142063, acc 0.953125, prec 0.0672145, recall 0.871694
2017-12-10T13:55:31.396113: step 4005, loss 0.299389, acc 0.984375, prec 0.0672696, recall 0.871795
2017-12-10T13:55:31.585416: step 4006, loss 0.146286, acc 0.9375, prec 0.0672642, recall 0.871795
2017-12-10T13:55:31.772864: step 4007, loss 0.137741, acc 0.984375, prec 0.0672629, recall 0.871795
2017-12-10T13:55:31.962930: step 4008, loss 0.0269491, acc 1, prec 0.0672629, recall 0.871795
2017-12-10T13:55:32.153349: step 4009, loss 0.0531332, acc 0.984375, prec 0.0672803, recall 0.871828
2017-12-10T13:55:32.343325: step 4010, loss 0.328012, acc 0.984375, prec 0.0673166, recall 0.871895
2017-12-10T13:55:32.534029: step 4011, loss 0.236025, acc 0.984375, prec 0.0673341, recall 0.871929
2017-12-10T13:55:32.720876: step 4012, loss 0.152899, acc 0.9375, prec 0.0673287, recall 0.871929
2017-12-10T13:55:32.910620: step 4013, loss 0.158275, acc 0.96875, prec 0.0673448, recall 0.871962
2017-12-10T13:55:33.101830: step 4014, loss 0.282906, acc 0.921875, prec 0.0673756, recall 0.872029
2017-12-10T13:55:33.289176: step 4015, loss 0.145998, acc 0.953125, prec 0.0673903, recall 0.872063
2017-12-10T13:55:33.481942: step 4016, loss 0.00807372, acc 1, prec 0.0673903, recall 0.872063
2017-12-10T13:55:33.670038: step 4017, loss 0.120219, acc 0.984375, prec 0.067389, recall 0.872063
2017-12-10T13:55:33.860354: step 4018, loss 0.0677606, acc 0.984375, prec 0.0674253, recall 0.872129
2017-12-10T13:55:34.052369: step 4019, loss 1.35635, acc 0.9375, prec 0.0674212, recall 0.871902
2017-12-10T13:55:34.245087: step 4020, loss 0.0879776, acc 0.953125, prec 0.0674171, recall 0.871902
2017-12-10T13:55:34.436150: step 4021, loss 0.018206, acc 0.984375, prec 0.0674345, recall 0.871935
2017-12-10T13:55:34.626657: step 4022, loss 0.116059, acc 0.953125, prec 0.0674305, recall 0.871935
2017-12-10T13:55:34.810754: step 4023, loss 0.027136, acc 1, prec 0.0674305, recall 0.871935
2017-12-10T13:55:35.001587: step 4024, loss 0.0615923, acc 0.984375, prec 0.0674667, recall 0.872002
2017-12-10T13:55:35.188779: step 4025, loss 0.340226, acc 0.953125, prec 0.0675002, recall 0.872069
2017-12-10T13:55:35.380440: step 4026, loss 0.209499, acc 0.96875, prec 0.0674975, recall 0.872069
2017-12-10T13:55:35.569055: step 4027, loss 0.620873, acc 0.90625, prec 0.0675082, recall 0.872102
2017-12-10T13:55:35.762261: step 4028, loss 0.176923, acc 0.984375, prec 0.0675632, recall 0.872202
2017-12-10T13:55:35.952510: step 4029, loss 0.10809, acc 0.953125, prec 0.0675779, recall 0.872235
2017-12-10T13:55:36.140366: step 4030, loss 0.379664, acc 0.9375, prec 0.0676477, recall 0.872368
2017-12-10T13:55:36.329376: step 4031, loss 0.109234, acc 0.953125, prec 0.0676436, recall 0.872368
2017-12-10T13:55:36.521720: step 4032, loss 0.207002, acc 0.953125, prec 0.0676771, recall 0.872434
2017-12-10T13:55:36.710857: step 4033, loss 0.172623, acc 0.96875, prec 0.0676743, recall 0.872434
2017-12-10T13:55:36.899418: step 4034, loss 0.0435004, acc 0.984375, prec 0.0677105, recall 0.872501
2017-12-10T13:55:37.089279: step 4035, loss 0.131198, acc 0.984375, prec 0.067728, recall 0.872534
2017-12-10T13:55:37.282413: step 4036, loss 0.28161, acc 0.921875, prec 0.0677211, recall 0.872534
2017-12-10T13:55:37.471758: step 4037, loss 0.0772265, acc 0.96875, prec 0.0677184, recall 0.872534
2017-12-10T13:55:37.659373: step 4038, loss 0.216819, acc 0.96875, prec 0.0677532, recall 0.8726
2017-12-10T13:55:37.848909: step 4039, loss 0.217232, acc 0.96875, prec 0.0677505, recall 0.8726
2017-12-10T13:55:38.040547: step 4040, loss 0.134352, acc 0.9375, prec 0.0677638, recall 0.872633
2017-12-10T13:55:38.229322: step 4041, loss 0.0108479, acc 1, prec 0.0677826, recall 0.872666
2017-12-10T13:55:38.417542: step 4042, loss 0.0742058, acc 0.96875, prec 0.0677987, recall 0.872699
2017-12-10T13:55:38.605417: step 4043, loss 0.211757, acc 0.953125, prec 0.0678133, recall 0.872732
2017-12-10T13:55:38.796653: step 4044, loss 0.139641, acc 0.96875, prec 0.0678106, recall 0.872732
2017-12-10T13:55:38.986959: step 4045, loss 2.79546, acc 0.953125, prec 0.0678079, recall 0.872506
2017-12-10T13:55:39.179385: step 4046, loss 0.0286244, acc 0.984375, prec 0.0678253, recall 0.872539
2017-12-10T13:55:39.367525: step 4047, loss 0.150876, acc 0.953125, prec 0.0678212, recall 0.872539
2017-12-10T13:55:39.557735: step 4048, loss 0.137749, acc 0.953125, prec 0.0678546, recall 0.872605
2017-12-10T13:55:39.745085: step 4049, loss 0.202163, acc 0.921875, prec 0.0678478, recall 0.872605
2017-12-10T13:55:39.933761: step 4050, loss 0.814942, acc 0.953125, prec 0.0679188, recall 0.872737
2017-12-10T13:55:40.124578: step 4051, loss 0.239356, acc 0.9375, prec 0.067932, recall 0.87277
2017-12-10T13:55:40.313049: step 4052, loss 0.264825, acc 0.9375, prec 0.0679641, recall 0.872835
2017-12-10T13:55:40.507137: step 4053, loss 0.156335, acc 0.953125, prec 0.0679975, recall 0.872901
2017-12-10T13:55:40.697154: step 4054, loss 0.316814, acc 0.90625, prec 0.0679893, recall 0.872901
2017-12-10T13:55:40.885372: step 4055, loss 0.40933, acc 0.90625, prec 0.0679811, recall 0.872901
2017-12-10T13:55:41.078782: step 4056, loss 0.318347, acc 0.90625, prec 0.0679916, recall 0.872934
2017-12-10T13:55:41.268411: step 4057, loss 0.129966, acc 0.96875, prec 0.0679889, recall 0.872934
2017-12-10T13:55:41.459535: step 4058, loss 0.359508, acc 0.921875, prec 0.0679821, recall 0.872934
2017-12-10T13:55:41.650268: step 4059, loss 0.261274, acc 0.96875, prec 0.0680168, recall 0.872999
2017-12-10T13:55:41.843842: step 4060, loss 0.193362, acc 0.921875, prec 0.0680287, recall 0.873032
2017-12-10T13:55:42.030886: step 4061, loss 0.307564, acc 0.9375, prec 0.068042, recall 0.873065
2017-12-10T13:55:42.219495: step 4062, loss 0.215658, acc 0.9375, prec 0.0680365, recall 0.873065
2017-12-10T13:55:42.408142: step 4063, loss 0.179617, acc 0.953125, prec 0.0680699, recall 0.873131
2017-12-10T13:55:42.598800: step 4064, loss 0.17736, acc 0.921875, prec 0.068063, recall 0.873131
2017-12-10T13:55:42.785592: step 4065, loss 0.188014, acc 0.953125, prec 0.0681151, recall 0.873229
2017-12-10T13:55:42.974130: step 4066, loss 0.0885684, acc 0.9375, prec 0.0681284, recall 0.873261
2017-12-10T13:55:43.163683: step 4067, loss 0.0454153, acc 0.984375, prec 0.0681457, recall 0.873294
2017-12-10T13:55:43.357660: step 4068, loss 0.221037, acc 0.9375, prec 0.0682339, recall 0.873457
2017-12-10T13:55:43.546231: step 4069, loss 0.397658, acc 0.890625, prec 0.0682804, recall 0.873554
2017-12-10T13:55:43.737944: step 4070, loss 0.259534, acc 0.90625, prec 0.0683096, recall 0.873619
2017-12-10T13:55:43.936217: step 4071, loss 0.0357187, acc 0.96875, prec 0.0683069, recall 0.873619
2017-12-10T13:55:44.129165: step 4072, loss 0.0962399, acc 0.953125, prec 0.0683028, recall 0.873619
2017-12-10T13:55:44.319782: step 4073, loss 0.257323, acc 0.984375, prec 0.0683201, recall 0.873652
2017-12-10T13:55:44.511326: step 4074, loss 0.109603, acc 0.96875, prec 0.0683174, recall 0.873652
2017-12-10T13:55:44.703579: step 4075, loss 0.115957, acc 0.96875, prec 0.0683146, recall 0.873652
2017-12-10T13:55:44.891135: step 4076, loss 0.05781, acc 0.984375, prec 0.068332, recall 0.873684
2017-12-10T13:55:45.084669: step 4077, loss 0.125972, acc 0.953125, prec 0.0683278, recall 0.873684
2017-12-10T13:55:45.271749: step 4078, loss 0.115632, acc 0.953125, prec 0.0683424, recall 0.873717
2017-12-10T13:55:45.461920: step 4079, loss 0.1938, acc 0.96875, prec 0.0684145, recall 0.873846
2017-12-10T13:55:45.651617: step 4080, loss 0.501629, acc 0.984375, prec 0.0684505, recall 0.873911
2017-12-10T13:55:45.842772: step 4081, loss 0.0695625, acc 0.984375, prec 0.0684679, recall 0.873943
2017-12-10T13:55:46.034514: step 4082, loss 0.023498, acc 0.984375, prec 0.0685226, recall 0.87404
2017-12-10T13:55:46.223179: step 4083, loss 0.325184, acc 0.921875, prec 0.0685344, recall 0.874072
2017-12-10T13:55:46.412371: step 4084, loss 0.0400935, acc 0.96875, prec 0.0685316, recall 0.874072
2017-12-10T13:55:46.600590: step 4085, loss 0.140186, acc 0.953125, prec 0.0685275, recall 0.874072
2017-12-10T13:55:46.793672: step 4086, loss 0.0515399, acc 0.984375, prec 0.0685448, recall 0.874104
2017-12-10T13:55:46.980330: step 4087, loss 0.0210937, acc 0.984375, prec 0.0685621, recall 0.874137
2017-12-10T13:55:47.175806: step 4088, loss 0.148553, acc 0.984375, prec 0.0685981, recall 0.874201
2017-12-10T13:55:47.365995: step 4089, loss 0.306012, acc 0.9375, prec 0.0685926, recall 0.874201
2017-12-10T13:55:47.555624: step 4090, loss 0.107428, acc 0.96875, prec 0.0686086, recall 0.874233
2017-12-10T13:55:47.748193: step 4091, loss 0.105091, acc 0.984375, prec 0.0686072, recall 0.874233
2017-12-10T13:55:47.939265: step 4092, loss 0.00434644, acc 1, prec 0.0686072, recall 0.874233
2017-12-10T13:55:48.125758: step 4093, loss 0.152019, acc 0.9375, prec 0.0686204, recall 0.874265
2017-12-10T13:55:48.317186: step 4094, loss 0.106548, acc 0.9375, prec 0.0686149, recall 0.874265
2017-12-10T13:55:48.508979: step 4095, loss 0.0857735, acc 0.96875, prec 0.0686121, recall 0.874265
2017-12-10T13:55:48.697798: step 4096, loss 0.306602, acc 0.984375, prec 0.0686481, recall 0.87433
2017-12-10T13:55:48.887328: step 4097, loss 0.0116321, acc 1, prec 0.0686481, recall 0.87433
2017-12-10T13:55:49.082507: step 4098, loss 0.104537, acc 0.953125, prec 0.068644, recall 0.87433
2017-12-10T13:55:49.271599: step 4099, loss 0.14122, acc 0.96875, prec 0.0686599, recall 0.874362
2017-12-10T13:55:49.457662: step 4100, loss 0.220221, acc 0.953125, prec 0.0686558, recall 0.874362
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_0/1512931325/checkpoints/model-4100

2017-12-10T13:55:50.674871: step 4101, loss 0.192168, acc 0.96875, prec 0.0686717, recall 0.874394
2017-12-10T13:55:50.866948: step 4102, loss 0.273838, acc 0.96875, prec 0.0686876, recall 0.874426
2017-12-10T13:55:51.055213: step 4103, loss 0.0681179, acc 0.96875, prec 0.0687035, recall 0.874458
2017-12-10T13:55:51.247611: step 4104, loss 0.0488412, acc 0.96875, prec 0.0687008, recall 0.874458
2017-12-10T13:55:51.440428: step 4105, loss 0.0212842, acc 0.984375, prec 0.0687181, recall 0.87449
2017-12-10T13:55:51.625540: step 4106, loss 0.13199, acc 0.953125, prec 0.0687139, recall 0.87449
2017-12-10T13:55:51.818188: step 4107, loss 0.0920358, acc 0.984375, prec 0.0687499, recall 0.874554
2017-12-10T13:55:52.008911: step 4108, loss 4.69791, acc 0.96875, prec 0.0687858, recall 0.874395
2017-12-10T13:55:52.204330: step 4109, loss 0.0303963, acc 0.984375, prec 0.0687844, recall 0.874395
2017-12-10T13:55:52.397955: step 4110, loss 0.0528182, acc 0.96875, prec 0.0687817, recall 0.874395
2017-12-10T13:55:52.585144: step 4111, loss 0.0753968, acc 0.984375, prec 0.0687803, recall 0.874395
2017-12-10T13:55:52.778135: step 4112, loss 0.191208, acc 0.96875, prec 0.0688149, recall 0.874459
2017-12-10T13:55:52.967765: step 4113, loss 0.0328671, acc 0.984375, prec 0.0688322, recall 0.874491
2017-12-10T13:55:53.159099: step 4114, loss 0.540711, acc 0.953125, prec 0.0688653, recall 0.874555
2017-12-10T13:55:53.349952: step 4115, loss 0.0970754, acc 0.953125, prec 0.0688799, recall 0.874587
2017-12-10T13:55:53.541128: step 4116, loss 0.113889, acc 0.953125, prec 0.0688944, recall 0.874619
2017-12-10T13:55:53.729393: step 4117, loss 0.0899048, acc 0.953125, prec 0.0688902, recall 0.874619
2017-12-10T13:55:53.914427: step 4118, loss 0.0671331, acc 0.984375, prec 0.0689634, recall 0.874746
2017-12-10T13:55:54.109008: step 4119, loss 0.100484, acc 0.9375, prec 0.0689579, recall 0.874746
2017-12-10T13:55:54.301265: step 4120, loss 0.328507, acc 0.921875, prec 0.0690069, recall 0.874841
2017-12-10T13:55:54.490758: step 4121, loss 0.118505, acc 0.96875, prec 0.0690228, recall 0.874873
2017-12-10T13:55:54.683420: step 4122, loss 0.332888, acc 0.921875, prec 0.0690346, recall 0.874905
2017-12-10T13:55:54.875618: step 4123, loss 0.276526, acc 0.90625, prec 0.0690263, recall 0.874905
2017-12-10T13:55:55.065900: step 4124, loss 0.463041, acc 0.875, prec 0.0690152, recall 0.874905
2017-12-10T13:55:55.254433: step 4125, loss 0.104156, acc 0.96875, prec 0.0690124, recall 0.874905
2017-12-10T13:55:55.440836: step 4126, loss 0.166921, acc 0.921875, prec 0.0690055, recall 0.874905
2017-12-10T13:55:55.632968: step 4127, loss 0.0844867, acc 0.953125, prec 0.06902, recall 0.874937
2017-12-10T13:55:55.820304: step 4128, loss 0.105697, acc 0.984375, prec 0.0690186, recall 0.874937
2017-12-10T13:55:56.010228: step 4129, loss 0.125276, acc 0.953125, prec 0.0690145, recall 0.874937
2017-12-10T13:55:56.196768: step 4130, loss 0.0923271, acc 0.96875, prec 0.0690304, recall 0.874968
2017-12-10T13:55:56.385262: step 4131, loss 0.106103, acc 0.984375, prec 0.0690476, recall 0.875
2017-12-10T13:55:56.572121: step 4132, loss 0.219721, acc 0.96875, prec 0.0690449, recall 0.875
2017-12-10T13:55:56.761379: step 4133, loss 0.348915, acc 0.953125, prec 0.0690407, recall 0.875
2017-12-10T13:55:56.947526: step 4134, loss 0.326893, acc 0.953125, prec 0.0690552, recall 0.875032
2017-12-10T13:55:57.135073: step 4135, loss 0.242783, acc 0.921875, prec 0.0690483, recall 0.875032
2017-12-10T13:55:57.323759: step 4136, loss 0.0432404, acc 0.984375, prec 0.0690469, recall 0.875032
2017-12-10T13:55:57.515650: step 4137, loss 0.0295946, acc 0.984375, prec 0.0690641, recall 0.875063
2017-12-10T13:55:57.702086: step 4138, loss 0.110652, acc 0.953125, prec 0.0690786, recall 0.875095
2017-12-10T13:55:57.888099: step 4139, loss 0.154119, acc 0.953125, prec 0.0690931, recall 0.875127
2017-12-10T13:55:58.074136: step 4140, loss 0.0240084, acc 1, prec 0.0690931, recall 0.875127
2017-12-10T13:55:58.260979: step 4141, loss 0.206492, acc 0.9375, prec 0.0690876, recall 0.875127
2017-12-10T13:55:58.451074: step 4142, loss 0.0710692, acc 0.96875, prec 0.0690848, recall 0.875127
2017-12-10T13:55:58.638509: step 4143, loss 0.187619, acc 0.9375, prec 0.0690979, recall 0.875158
2017-12-10T13:55:58.829148: step 4144, loss 0.0685798, acc 0.984375, prec 0.0691151, recall 0.87519
2017-12-10T13:55:59.028590: step 4145, loss 0.14885, acc 0.96875, prec 0.069131, recall 0.875221
2017-12-10T13:55:59.216667: step 4146, loss 0.0694384, acc 0.984375, prec 0.0691482, recall 0.875253
2017-12-10T13:55:59.409334: step 4147, loss 0.058766, acc 0.984375, prec 0.0691654, recall 0.875285
2017-12-10T13:55:59.596754: step 4148, loss 0.0799598, acc 0.984375, prec 0.069164, recall 0.875285
2017-12-10T13:55:59.780871: step 4149, loss 0.598089, acc 0.96875, prec 0.0691799, recall 0.875316
2017-12-10T13:55:59.969300: step 4150, loss 0.205959, acc 0.96875, prec 0.0691771, recall 0.875316
2017-12-10T13:56:00.164548: step 4151, loss 0.00509937, acc 1, prec 0.0691771, recall 0.875316
2017-12-10T13:56:00.352132: step 4152, loss 0.874822, acc 0.96875, prec 0.0692116, recall 0.875379
2017-12-10T13:56:00.546212: step 4153, loss 0.030393, acc 0.984375, prec 0.0692474, recall 0.875442
2017-12-10T13:56:00.742680: step 4154, loss 0.996089, acc 0.984375, prec 0.0692832, recall 0.875505
2017-12-10T13:56:00.939301: step 4155, loss 0.228085, acc 0.953125, prec 0.0692976, recall 0.875537
2017-12-10T13:56:01.131535: step 4156, loss 0.0115157, acc 1, prec 0.0692976, recall 0.875537
2017-12-10T13:56:01.320895: step 4157, loss 0.122408, acc 0.96875, prec 0.0692949, recall 0.875537
2017-12-10T13:56:01.515506: step 4158, loss 0.136079, acc 0.921875, prec 0.0692879, recall 0.875537
2017-12-10T13:56:01.703056: step 4159, loss 0.080361, acc 0.96875, prec 0.0693224, recall 0.875599
2017-12-10T13:56:01.895500: step 4160, loss 0.56269, acc 0.890625, prec 0.0693313, recall 0.875631
2017-12-10T13:56:02.084786: step 4161, loss 0.0457239, acc 0.984375, prec 0.0693671, recall 0.875693
2017-12-10T13:56:02.271736: step 4162, loss 0.652784, acc 0.890625, prec 0.0693759, recall 0.875725
2017-12-10T13:56:02.462547: step 4163, loss 0.290273, acc 0.90625, prec 0.0693676, recall 0.875725
2017-12-10T13:56:02.653956: step 4164, loss 0.230446, acc 0.953125, prec 0.0693635, recall 0.875725
2017-12-10T13:56:02.843896: step 4165, loss 0.153014, acc 0.9375, prec 0.0693579, recall 0.875725
2017-12-10T13:56:03.032250: step 4166, loss 0.141129, acc 0.96875, prec 0.0693737, recall 0.875756
2017-12-10T13:56:03.222433: step 4167, loss 0.174153, acc 0.921875, prec 0.069404, recall 0.875819
2017-12-10T13:56:03.410725: step 4168, loss 0.105997, acc 0.96875, prec 0.0694198, recall 0.87585
2017-12-10T13:56:03.603918: step 4169, loss 0.0880918, acc 0.96875, prec 0.069417, recall 0.87585
2017-12-10T13:56:03.795809: step 4170, loss 0.671541, acc 0.890625, prec 0.0694444, recall 0.875912
2017-12-10T13:56:03.987416: step 4171, loss 0.333297, acc 0.921875, prec 0.0694747, recall 0.875975
2017-12-10T13:56:04.175316: step 4172, loss 0.0792368, acc 0.953125, prec 0.0694891, recall 0.876006
2017-12-10T13:56:04.362912: step 4173, loss 0.28079, acc 0.9375, prec 0.0695021, recall 0.876037
2017-12-10T13:56:04.554280: step 4174, loss 0.215043, acc 0.96875, prec 0.0695364, recall 0.8761
2017-12-10T13:56:04.744238: step 4175, loss 0.046484, acc 0.96875, prec 0.0695522, recall 0.876131
2017-12-10T13:56:04.932774: step 4176, loss 0.081443, acc 0.96875, prec 0.069568, recall 0.876162
2017-12-10T13:56:05.125669: step 4177, loss 0.209624, acc 0.953125, prec 0.0696009, recall 0.876224
2017-12-10T13:56:05.313850: step 4178, loss 0.425689, acc 0.921875, prec 0.0696126, recall 0.876255
2017-12-10T13:56:05.503401: step 4179, loss 0.0485527, acc 0.953125, prec 0.0696084, recall 0.876255
2017-12-10T13:56:05.691943: step 4180, loss 0.0819885, acc 0.953125, prec 0.0696228, recall 0.876286
2017-12-10T13:56:05.877765: step 4181, loss 0.110999, acc 0.96875, prec 0.06962, recall 0.876286
2017-12-10T13:56:06.066312: step 4182, loss 6.64932, acc 0.890625, prec 0.0696117, recall 0.876066
2017-12-10T13:56:06.257593: step 4183, loss 0.0550346, acc 0.96875, prec 0.0696274, recall 0.876097
2017-12-10T13:56:06.445781: step 4184, loss 0.0120683, acc 1, prec 0.0696274, recall 0.876097
2017-12-10T13:56:06.634705: step 4185, loss 0.206099, acc 0.921875, prec 0.069639, recall 0.876128
2017-12-10T13:56:06.826546: step 4186, loss 0.451162, acc 0.921875, prec 0.0696692, recall 0.87619
2017-12-10T13:56:07.017047: step 4187, loss 0.286147, acc 0.921875, prec 0.0696993, recall 0.876253
2017-12-10T13:56:07.202798: step 4188, loss 0.170641, acc 0.9375, prec 0.0696938, recall 0.876253
2017-12-10T13:56:07.394483: step 4189, loss 0.433712, acc 0.90625, prec 0.0696854, recall 0.876253
2017-12-10T13:56:07.582156: step 4190, loss 0.2772, acc 0.984375, prec 0.0697026, recall 0.876283
2017-12-10T13:56:07.773679: step 4191, loss 0.336373, acc 0.921875, prec 0.0697327, recall 0.876345
2017-12-10T13:56:07.964523: step 4192, loss 0.138204, acc 0.921875, prec 0.0697258, recall 0.876345
2017-12-10T13:56:08.154510: step 4193, loss 0.472593, acc 0.890625, prec 0.069716, recall 0.876345
2017-12-10T13:56:08.345095: step 4194, loss 0.12076, acc 0.90625, prec 0.0697262, recall 0.876376
2017-12-10T13:56:08.534100: step 4195, loss 0.124761, acc 0.921875, prec 0.0697378, recall 0.876407
2017-12-10T13:56:08.722367: step 4196, loss 0.168059, acc 0.9375, prec 0.0697323, recall 0.876407
2017-12-10T13:56:08.911370: step 4197, loss 0.034463, acc 1, prec 0.0697508, recall 0.876438
2017-12-10T13:56:09.103181: step 4198, loss 0.591178, acc 0.875, prec 0.0697767, recall 0.8765
2017-12-10T13:56:09.291982: step 4199, loss 0.194693, acc 0.9375, prec 0.0697711, recall 0.8765
2017-12-10T13:56:09.484496: step 4200, loss 0.0667573, acc 0.96875, prec 0.0697869, recall 0.876531
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_0/1512931325/checkpoints/model-4200

2017-12-10T13:56:10.758255: step 4201, loss 0.34737, acc 0.953125, prec 0.0698012, recall 0.876562
2017-12-10T13:56:10.950584: step 4202, loss 0.511473, acc 0.90625, prec 0.0698114, recall 0.876593
2017-12-10T13:56:11.141345: step 4203, loss 0.195743, acc 0.9375, prec 0.0698428, recall 0.876654
2017-12-10T13:56:11.331278: step 4204, loss 0.143443, acc 0.953125, prec 0.0698387, recall 0.876654
2017-12-10T13:56:11.517708: step 4205, loss 0.388261, acc 0.90625, prec 0.0698673, recall 0.876716
2017-12-10T13:56:11.707781: step 4206, loss 0.166405, acc 0.96875, prec 0.0699016, recall 0.876777
2017-12-10T13:56:11.895896: step 4207, loss 0.128379, acc 0.953125, prec 0.0699159, recall 0.876808
2017-12-10T13:56:12.089025: step 4208, loss 0.105283, acc 0.9375, prec 0.0699288, recall 0.876839
2017-12-10T13:56:12.278254: step 4209, loss 0.15434, acc 0.90625, prec 0.0699205, recall 0.876839
2017-12-10T13:56:12.468275: step 4210, loss 0.172346, acc 0.9375, prec 0.0699334, recall 0.876869
2017-12-10T13:56:12.655595: step 4211, loss 0.126399, acc 0.953125, prec 0.0699292, recall 0.876869
2017-12-10T13:56:12.844775: step 4212, loss 0.169193, acc 0.921875, prec 0.0699223, recall 0.876869
2017-12-10T13:56:13.032324: step 4213, loss 0.0533146, acc 0.984375, prec 0.0699394, recall 0.8769
2017-12-10T13:56:13.217141: step 4214, loss 0.481727, acc 0.984375, prec 0.0699934, recall 0.876992
2017-12-10T13:56:13.411695: step 4215, loss 0.185003, acc 0.890625, prec 0.0700022, recall 0.877023
2017-12-10T13:56:13.602780: step 4216, loss 0.0875052, acc 0.953125, prec 0.0700165, recall 0.877053
2017-12-10T13:56:13.795882: step 4217, loss 0.105724, acc 0.953125, prec 0.0700123, recall 0.877053
2017-12-10T13:56:13.988423: step 4218, loss 0.775053, acc 0.953125, prec 0.0700451, recall 0.877114
2017-12-10T13:56:14.177083: step 4219, loss 0.17883, acc 0.953125, prec 0.0700594, recall 0.877145
2017-12-10T13:56:14.368202: step 4220, loss 0.0881346, acc 0.984375, prec 0.070058, recall 0.877145
2017-12-10T13:56:14.558684: step 4221, loss 0.00302707, acc 1, prec 0.070058, recall 0.877145
2017-12-10T13:56:14.746875: step 4222, loss 0.237211, acc 0.9375, prec 0.0700524, recall 0.877145
2017-12-10T13:56:14.933411: step 4223, loss 0.225506, acc 0.96875, prec 0.0700681, recall 0.877176
2017-12-10T13:56:15.122409: step 4224, loss 0.0410406, acc 0.96875, prec 0.0701023, recall 0.877237
2017-12-10T13:56:15.311781: step 4225, loss 0.311319, acc 0.921875, prec 0.0701138, recall 0.877267
2017-12-10T13:56:15.502979: step 4226, loss 0.194363, acc 0.953125, prec 0.0701096, recall 0.877267
2017-12-10T13:56:15.693876: step 4227, loss 0.101502, acc 0.96875, prec 0.0701253, recall 0.877298
2017-12-10T13:56:15.882997: step 4228, loss 0.117935, acc 0.953125, prec 0.070158, recall 0.877358
2017-12-10T13:56:16.075975: step 4229, loss 0.169268, acc 0.9375, prec 0.0701525, recall 0.877358
2017-12-10T13:56:16.263579: step 4230, loss 0.219168, acc 0.921875, prec 0.070164, recall 0.877389
2017-12-10T13:56:16.453551: step 4231, loss 0.102897, acc 0.96875, prec 0.0701612, recall 0.877389
2017-12-10T13:56:16.642230: step 4232, loss 0.020296, acc 1, prec 0.0701796, recall 0.877419
2017-12-10T13:56:16.831093: step 4233, loss 0.0334346, acc 0.984375, prec 0.0701782, recall 0.877419
2017-12-10T13:56:17.025514: step 4234, loss 0.192354, acc 0.921875, prec 0.0701897, recall 0.87745
2017-12-10T13:56:17.213358: step 4235, loss 0.256993, acc 0.9375, prec 0.0702026, recall 0.87748
2017-12-10T13:56:17.401017: step 4236, loss 0.148175, acc 0.953125, prec 0.0702169, recall 0.877511
2017-12-10T13:56:17.596819: step 4237, loss 0.0859285, acc 0.96875, prec 0.0702325, recall 0.877541
2017-12-10T13:56:17.789693: step 4238, loss 0.517672, acc 0.9375, prec 0.0702454, recall 0.877571
2017-12-10T13:56:17.979264: step 4239, loss 0.0509284, acc 0.984375, prec 0.070244, recall 0.877571
2017-12-10T13:56:18.170301: step 4240, loss 0.199495, acc 0.9375, prec 0.0702569, recall 0.877602
2017-12-10T13:56:18.359710: step 4241, loss 0.0805108, acc 0.96875, prec 0.0702725, recall 0.877632
2017-12-10T13:56:18.546216: step 4242, loss 0.182031, acc 0.953125, prec 0.0702683, recall 0.877632
2017-12-10T13:56:18.738141: step 4243, loss 0.0870724, acc 0.96875, prec 0.0702656, recall 0.877632
2017-12-10T13:56:18.929768: step 4244, loss 0.164118, acc 0.953125, prec 0.0702798, recall 0.877662
2017-12-10T13:56:19.121377: step 4245, loss 0.0348026, acc 0.984375, prec 0.0702969, recall 0.877693
2017-12-10T13:56:19.311355: step 4246, loss 0.00999816, acc 1, prec 0.0702969, recall 0.877693
2017-12-10T13:56:19.502383: step 4247, loss 0.0362341, acc 0.984375, prec 0.0702955, recall 0.877693
2017-12-10T13:56:19.690183: step 4248, loss 0.108218, acc 0.96875, prec 0.0702927, recall 0.877693
2017-12-10T13:56:19.882676: step 4249, loss 0.0857065, acc 1, prec 0.0703664, recall 0.877814
2017-12-10T13:56:20.074512: step 4250, loss 0.0352946, acc 1, prec 0.0703848, recall 0.877844
2017-12-10T13:56:20.262999: step 4251, loss 0.0225726, acc 0.984375, prec 0.0704019, recall 0.877874
2017-12-10T13:56:20.453319: step 4252, loss 0.00643148, acc 1, prec 0.0704387, recall 0.877934
2017-12-10T13:56:20.643310: step 4253, loss 0.166245, acc 0.984375, prec 0.0704926, recall 0.878025
2017-12-10T13:56:20.835883: step 4254, loss 0.00105838, acc 1, prec 0.0704926, recall 0.878025
2017-12-10T13:56:21.026384: step 4255, loss 0.0217211, acc 0.984375, prec 0.0705096, recall 0.878055
2017-12-10T13:56:21.223370: step 4256, loss 0.0202524, acc 0.984375, prec 0.0705082, recall 0.878055
2017-12-10T13:56:21.417803: step 4257, loss 0.0272555, acc 0.984375, prec 0.0705437, recall 0.878115
2017-12-10T13:56:21.606089: step 4258, loss 0.275247, acc 1, prec 0.0705621, recall 0.878145
2017-12-10T13:56:21.797267: step 4259, loss 0.0470775, acc 0.96875, prec 0.0705777, recall 0.878175
2017-12-10T13:56:21.987663: step 4260, loss 0.198754, acc 0.953125, prec 0.070592, recall 0.878205
2017-12-10T13:56:22.178982: step 4261, loss 3.16116, acc 0.96875, prec 0.0705906, recall 0.877989
2017-12-10T13:56:22.375055: step 4262, loss 0.00637757, acc 1, prec 0.0705906, recall 0.877989
2017-12-10T13:56:22.562380: step 4263, loss 0.052064, acc 0.96875, prec 0.0706246, recall 0.878049
2017-12-10T13:56:22.751421: step 4264, loss 0.219265, acc 0.96875, prec 0.0706402, recall 0.878079
2017-12-10T13:56:22.937363: step 4265, loss 4.12295, acc 0.984375, prec 0.0706402, recall 0.877863
2017-12-10T13:56:23.130560: step 4266, loss 0.171516, acc 0.96875, prec 0.0706742, recall 0.877923
2017-12-10T13:56:23.320057: step 4267, loss 0.150411, acc 0.921875, prec 0.0707041, recall 0.877983
2017-12-10T13:56:23.506616: step 4268, loss 0.299052, acc 0.890625, prec 0.0706943, recall 0.877983
2017-12-10T13:56:23.699858: step 4269, loss 0.338278, acc 0.9375, prec 0.0706887, recall 0.877983
2017-12-10T13:56:23.891839: step 4270, loss 0.318134, acc 0.921875, prec 0.0706817, recall 0.877983
2017-12-10T13:56:24.078764: step 4271, loss 0.336406, acc 0.875, prec 0.0706705, recall 0.877983
2017-12-10T13:56:24.267892: step 4272, loss 0.477691, acc 0.875, prec 0.0706593, recall 0.877983
2017-12-10T13:56:24.457334: step 4273, loss 0.172401, acc 0.90625, prec 0.0706693, recall 0.878013
2017-12-10T13:56:24.649906: step 4274, loss 0.729862, acc 0.84375, prec 0.0706737, recall 0.878043
2017-12-10T13:56:24.836459: step 4275, loss 0.627425, acc 0.828125, prec 0.0706583, recall 0.878043
2017-12-10T13:56:25.024393: step 4276, loss 0.419204, acc 0.828125, prec 0.0706429, recall 0.878043
2017-12-10T13:56:25.218166: step 4277, loss 0.544973, acc 0.828125, prec 0.0706276, recall 0.878043
2017-12-10T13:56:25.411074: step 4278, loss 0.879648, acc 0.8125, prec 0.0706292, recall 0.878073
2017-12-10T13:56:25.603306: step 4279, loss 0.333034, acc 0.875, prec 0.070618, recall 0.878073
2017-12-10T13:56:25.792722: step 4280, loss 0.559346, acc 0.8125, prec 0.070638, recall 0.878133
2017-12-10T13:56:25.980500: step 4281, loss 0.537714, acc 0.890625, prec 0.0706282, recall 0.878133
2017-12-10T13:56:26.169309: step 4282, loss 0.43903, acc 0.921875, prec 0.0706212, recall 0.878133
2017-12-10T13:56:26.359474: step 4283, loss 0.330551, acc 0.90625, prec 0.0706129, recall 0.878133
2017-12-10T13:56:26.552045: step 4284, loss 0.301447, acc 0.921875, prec 0.0706059, recall 0.878133
2017-12-10T13:56:26.743415: step 4285, loss 0.117005, acc 0.953125, prec 0.0706201, recall 0.878163
2017-12-10T13:56:26.931194: step 4286, loss 0.299541, acc 0.921875, prec 0.0706131, recall 0.878163
2017-12-10T13:56:27.122816: step 4287, loss 0.306185, acc 0.921875, prec 0.0706061, recall 0.878163
2017-12-10T13:56:27.310077: step 4288, loss 0.20854, acc 0.9375, prec 0.0706189, recall 0.878193
2017-12-10T13:56:27.495284: step 4289, loss 0.532412, acc 0.875, prec 0.0706078, recall 0.878193
2017-12-10T13:56:27.680448: step 4290, loss 0.0671152, acc 0.984375, prec 0.0706064, recall 0.878193
2017-12-10T13:56:27.865890: step 4291, loss 1.44256, acc 0.921875, prec 0.0706375, recall 0.878037
2017-12-10T13:56:28.058712: step 4292, loss 0.322531, acc 0.9375, prec 0.0706319, recall 0.878037
2017-12-10T13:56:28.251985: step 4293, loss 0.0237851, acc 0.984375, prec 0.0706305, recall 0.878037
2017-12-10T13:56:28.438736: step 4294, loss 0.177309, acc 0.953125, prec 0.0706263, recall 0.878037
2017-12-10T13:56:28.627991: step 4295, loss 0.1608, acc 0.984375, prec 0.0706249, recall 0.878037
2017-12-10T13:56:28.818366: step 4296, loss 0.00842057, acc 1, prec 0.0706433, recall 0.878067
2017-12-10T13:56:29.016153: step 4297, loss 0.192777, acc 0.921875, prec 0.0706546, recall 0.878097
2017-12-10T13:56:29.205790: step 4298, loss 0.404766, acc 0.9375, prec 0.0706857, recall 0.878156
2017-12-10T13:56:29.396171: step 4299, loss 0.137092, acc 0.96875, prec 0.0707013, recall 0.878186
2017-12-10T13:56:29.584954: step 4300, loss 0.317028, acc 0.953125, prec 0.0707154, recall 0.878216
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_0/1512931325/checkpoints/model-4300

2017-12-10T13:56:30.897892: step 4301, loss 0.289324, acc 0.953125, prec 0.0707296, recall 0.878246
2017-12-10T13:56:31.093889: step 4302, loss 0.260539, acc 0.96875, prec 0.0707818, recall 0.878335
2017-12-10T13:56:31.287278: step 4303, loss 0.0889637, acc 0.96875, prec 0.070779, recall 0.878335
2017-12-10T13:56:31.478956: step 4304, loss 0.0252341, acc 0.984375, prec 0.0707959, recall 0.878365
2017-12-10T13:56:31.667136: step 4305, loss 0.0245696, acc 1, prec 0.0708326, recall 0.878425
2017-12-10T13:56:31.860139: step 4306, loss 0.0173453, acc 1, prec 0.0708509, recall 0.878454
2017-12-10T13:56:32.055588: step 4307, loss 0.137199, acc 0.953125, prec 0.0708834, recall 0.878514
2017-12-10T13:56:32.246321: step 4308, loss 0.0561737, acc 0.953125, prec 0.0708975, recall 0.878543
2017-12-10T13:56:32.431565: step 4309, loss 0.570615, acc 0.953125, prec 0.0709116, recall 0.878573
2017-12-10T13:56:32.620521: step 4310, loss 0.0259802, acc 1, prec 0.07093, recall 0.878603
2017-12-10T13:56:32.811255: step 4311, loss 0.181191, acc 0.984375, prec 0.0709286, recall 0.878603
2017-12-10T13:56:32.997264: step 4312, loss 0.0697662, acc 0.984375, prec 0.0709455, recall 0.878632
2017-12-10T13:56:33.189614: step 4313, loss 0.0312039, acc 0.984375, prec 0.0709441, recall 0.878632
2017-12-10T13:56:33.382076: step 4314, loss 0.19036, acc 0.9375, prec 0.0709568, recall 0.878662
2017-12-10T13:56:33.570715: step 4315, loss 0.154968, acc 0.9375, prec 0.0709695, recall 0.878692
2017-12-10T13:56:33.760810: step 4316, loss 0.0297994, acc 0.984375, prec 0.0709864, recall 0.878721
2017-12-10T13:56:33.952586: step 4317, loss 0.194635, acc 0.921875, prec 0.0709794, recall 0.878721
2017-12-10T13:56:34.140289: step 4318, loss 0.135465, acc 0.984375, prec 0.070978, recall 0.878721
2017-12-10T13:56:34.331604: step 4319, loss 0.203459, acc 0.96875, prec 0.0709752, recall 0.878721
2017-12-10T13:56:34.520331: step 4320, loss 0.255026, acc 0.953125, prec 0.070971, recall 0.878721
2017-12-10T13:56:34.711773: step 4321, loss 0.190975, acc 0.96875, prec 0.0710049, recall 0.87878
2017-12-10T13:56:34.899323: step 4322, loss 0.0267615, acc 1, prec 0.0710232, recall 0.87881
2017-12-10T13:56:35.086644: step 4323, loss 0.884544, acc 0.9375, prec 0.0710359, recall 0.87884
2017-12-10T13:56:35.281931: step 4324, loss 1.03208, acc 0.96875, prec 0.0710697, recall 0.878899
2017-12-10T13:56:35.472035: step 4325, loss 0.171186, acc 0.96875, prec 0.0710669, recall 0.878899
2017-12-10T13:56:35.659640: step 4326, loss 0.0143479, acc 1, prec 0.0710852, recall 0.878928
2017-12-10T13:56:35.848914: step 4327, loss 0.160154, acc 0.953125, prec 0.071081, recall 0.878928
2017-12-10T13:56:36.037235: step 4328, loss 0.164275, acc 0.921875, prec 0.071074, recall 0.878928
2017-12-10T13:56:36.223515: step 4329, loss 0.330194, acc 0.921875, prec 0.071067, recall 0.878928
2017-12-10T13:56:36.414675: step 4330, loss 0.102694, acc 0.96875, prec 0.0711008, recall 0.878987
2017-12-10T13:56:36.602030: step 4331, loss 0.0988856, acc 0.953125, prec 0.0711515, recall 0.879075
2017-12-10T13:56:36.791605: step 4332, loss 0.172585, acc 0.96875, prec 0.0711852, recall 0.879134
2017-12-10T13:56:36.980688: step 4333, loss 0.248885, acc 0.953125, prec 0.0711993, recall 0.879164
2017-12-10T13:56:37.169709: step 4334, loss 0.529279, acc 0.921875, prec 0.0712655, recall 0.879281
2017-12-10T13:56:37.362432: step 4335, loss 0.0272336, acc 0.984375, prec 0.0713006, recall 0.87934
2017-12-10T13:56:37.553018: step 4336, loss 0.178164, acc 0.984375, prec 0.0713175, recall 0.879369
2017-12-10T13:56:37.748481: step 4337, loss 0.165146, acc 0.9375, prec 0.0713119, recall 0.879369
2017-12-10T13:56:37.935946: step 4338, loss 0.134769, acc 0.96875, prec 0.0713456, recall 0.879427
2017-12-10T13:56:38.126541: step 4339, loss 0.374609, acc 0.875, prec 0.0713527, recall 0.879457
2017-12-10T13:56:38.316542: step 4340, loss 0.431152, acc 0.875, prec 0.0713597, recall 0.879486
2017-12-10T13:56:38.505270: step 4341, loss 0.0766518, acc 0.953125, prec 0.0713555, recall 0.879486
2017-12-10T13:56:38.693180: step 4342, loss 0.072328, acc 0.953125, prec 0.0713513, recall 0.879486
2017-12-10T13:56:38.882373: step 4343, loss 0.106528, acc 0.9375, prec 0.0713457, recall 0.879486
2017-12-10T13:56:39.072193: step 4344, loss 0.121562, acc 0.921875, prec 0.0713387, recall 0.879486
2017-12-10T13:56:39.267035: step 4345, loss 0.0679179, acc 0.953125, prec 0.0713345, recall 0.879486
2017-12-10T13:56:39.456336: step 4346, loss 0.0968235, acc 0.96875, prec 0.0713316, recall 0.879486
2017-12-10T13:56:39.649172: step 4347, loss 0.298721, acc 0.953125, prec 0.0713274, recall 0.879486
2017-12-10T13:56:39.835954: step 4348, loss 0.32454, acc 0.921875, prec 0.0713387, recall 0.879515
2017-12-10T13:56:40.028494: step 4349, loss 0.30994, acc 0.921875, prec 0.0713317, recall 0.879515
2017-12-10T13:56:40.220396: step 4350, loss 0.113871, acc 0.953125, prec 0.071364, recall 0.879574
2017-12-10T13:56:40.412995: step 4351, loss 0.232483, acc 0.953125, prec 0.0713598, recall 0.879574
2017-12-10T13:56:40.605339: step 4352, loss 0.181974, acc 0.96875, prec 0.0713935, recall 0.879632
2017-12-10T13:56:40.793440: step 4353, loss 0.152866, acc 0.984375, prec 0.0714468, recall 0.879719
2017-12-10T13:56:40.979400: step 4354, loss 0.0525959, acc 0.96875, prec 0.071444, recall 0.879719
2017-12-10T13:56:41.170737: step 4355, loss 0.102937, acc 0.984375, prec 0.0714609, recall 0.879748
2017-12-10T13:56:41.359494: step 4356, loss 0.0117791, acc 1, prec 0.0714609, recall 0.879748
2017-12-10T13:56:41.552147: step 4357, loss 0.119302, acc 0.96875, prec 0.0714945, recall 0.879807
2017-12-10T13:56:41.740860: step 4358, loss 0.234838, acc 0.921875, prec 0.0714875, recall 0.879807
2017-12-10T13:56:41.934673: step 4359, loss 0.648895, acc 0.96875, prec 0.0715212, recall 0.879865
2017-12-10T13:56:42.130690: step 4360, loss 0.054575, acc 0.984375, prec 0.0715198, recall 0.879865
2017-12-10T13:56:42.319691: step 4361, loss 0.141573, acc 0.96875, prec 0.0715352, recall 0.879894
2017-12-10T13:56:42.510776: step 4362, loss 0.238111, acc 0.96875, prec 0.0715324, recall 0.879894
2017-12-10T13:56:42.700947: step 4363, loss 0.221819, acc 0.953125, prec 0.0715282, recall 0.879894
2017-12-10T13:56:42.891621: step 4364, loss 0.104182, acc 0.953125, prec 0.0715605, recall 0.879952
2017-12-10T13:56:43.079873: step 4365, loss 0.0380534, acc 0.984375, prec 0.0715591, recall 0.879952
2017-12-10T13:56:43.273286: step 4366, loss 0.322113, acc 0.9375, prec 0.0715899, recall 0.88001
2017-12-10T13:56:43.466282: step 4367, loss 0.390732, acc 0.96875, prec 0.0716053, recall 0.880039
2017-12-10T13:56:43.662009: step 4368, loss 0.0506888, acc 1, prec 0.07166, recall 0.880125
2017-12-10T13:56:43.854857: step 4369, loss 0.333759, acc 0.96875, prec 0.0716937, recall 0.880183
2017-12-10T13:56:44.048115: step 4370, loss 0.0351265, acc 0.984375, prec 0.0717105, recall 0.880212
2017-12-10T13:56:44.235856: step 4371, loss 0.124382, acc 0.9375, prec 0.0717049, recall 0.880212
2017-12-10T13:56:44.423033: step 4372, loss 0.304676, acc 0.9375, prec 0.0717174, recall 0.880241
2017-12-10T13:56:44.609702: step 4373, loss 0.17185, acc 0.953125, prec 0.0717132, recall 0.880241
2017-12-10T13:56:44.799587: step 4374, loss 0.0266281, acc 1, prec 0.0717314, recall 0.88027
2017-12-10T13:56:44.989594: step 4375, loss 0.0120418, acc 1, prec 0.0717314, recall 0.88027
2017-12-10T13:56:45.177491: step 4376, loss 0.183451, acc 0.9375, prec 0.0717258, recall 0.88027
2017-12-10T13:56:45.370015: step 4377, loss 0.213478, acc 0.96875, prec 0.0717412, recall 0.880299
2017-12-10T13:56:45.559347: step 4378, loss 0.145589, acc 0.96875, prec 0.0717566, recall 0.880327
2017-12-10T13:56:45.747777: step 4379, loss 0.229929, acc 0.96875, prec 0.0718267, recall 0.880443
2017-12-10T13:56:45.935646: step 4380, loss 0.269931, acc 0.9375, prec 0.0718575, recall 0.8805
2017-12-10T13:56:46.128092: step 4381, loss 1.0466, acc 0.984375, prec 0.0718925, recall 0.880558
2017-12-10T13:56:46.319477: step 4382, loss 0.636878, acc 0.96875, prec 0.0719079, recall 0.880586
2017-12-10T13:56:46.510815: step 4383, loss 0.138901, acc 0.96875, prec 0.071905, recall 0.880586
2017-12-10T13:56:46.697734: step 4384, loss 0.899234, acc 0.984375, prec 0.0719218, recall 0.880615
2017-12-10T13:56:46.889938: step 4385, loss 0.254826, acc 0.90625, prec 0.0719134, recall 0.880615
2017-12-10T13:56:47.076311: step 4386, loss 0.187245, acc 0.953125, prec 0.0719091, recall 0.880615
2017-12-10T13:56:47.265609: step 4387, loss 0.346176, acc 0.96875, prec 0.0719063, recall 0.880615
2017-12-10T13:56:47.458530: step 4388, loss 0.0580965, acc 0.984375, prec 0.0719413, recall 0.880672
2017-12-10T13:56:47.646392: step 4389, loss 0.0775931, acc 0.96875, prec 0.0719385, recall 0.880672
2017-12-10T13:56:47.838710: step 4390, loss 0.230299, acc 0.90625, prec 0.0719664, recall 0.88073
2017-12-10T13:56:48.025272: step 4391, loss 0.476468, acc 0.890625, prec 0.0719929, recall 0.880787
2017-12-10T13:56:48.214731: step 4392, loss 0.138252, acc 0.953125, prec 0.0720069, recall 0.880815
2017-12-10T13:56:48.404425: step 4393, loss 0.227325, acc 0.890625, prec 0.0720152, recall 0.880844
2017-12-10T13:56:48.590712: step 4394, loss 0.467629, acc 0.875, prec 0.0720039, recall 0.880844
2017-12-10T13:56:48.782524: step 4395, loss 0.40572, acc 0.859375, prec 0.0720094, recall 0.880872
2017-12-10T13:56:48.969752: step 4396, loss 0.47407, acc 0.921875, prec 0.0720024, recall 0.880872
2017-12-10T13:56:49.157390: step 4397, loss 0.892799, acc 0.890625, prec 0.0720107, recall 0.880901
2017-12-10T13:56:49.347465: step 4398, loss 0.165726, acc 0.921875, prec 0.0720218, recall 0.88093
2017-12-10T13:56:49.535182: step 4399, loss 0.595739, acc 0.890625, prec 0.0720119, recall 0.88093
2017-12-10T13:56:49.721645: step 4400, loss 0.363393, acc 0.890625, prec 0.0720202, recall 0.880958
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_0/1512931325/checkpoints/model-4400

2017-12-10T13:56:51.108169: step 4401, loss 0.110356, acc 0.96875, prec 0.0720174, recall 0.880958
2017-12-10T13:56:51.300113: step 4402, loss 0.189481, acc 0.9375, prec 0.0720117, recall 0.880958
2017-12-10T13:56:51.492057: step 4403, loss 0.161843, acc 0.9375, prec 0.0720243, recall 0.880987
2017-12-10T13:56:51.682064: step 4404, loss 0.246663, acc 0.890625, prec 0.0720326, recall 0.881015
2017-12-10T13:56:51.868237: step 4405, loss 0.21993, acc 0.9375, prec 0.0720451, recall 0.881044
2017-12-10T13:56:52.055319: step 4406, loss 0.0923603, acc 0.96875, prec 0.0720423, recall 0.881044
2017-12-10T13:56:52.244458: step 4407, loss 0.524682, acc 0.9375, prec 0.072073, recall 0.8811
2017-12-10T13:56:52.434526: step 4408, loss 0.00765786, acc 1, prec 0.0720911, recall 0.881129
2017-12-10T13:56:52.622793: step 4409, loss 0.340797, acc 0.953125, prec 0.072105, recall 0.881157
2017-12-10T13:56:52.817657: step 4410, loss 0.365681, acc 0.953125, prec 0.072119, recall 0.881186
2017-12-10T13:56:53.012240: step 4411, loss 0.28475, acc 0.9375, prec 0.0721315, recall 0.881214
2017-12-10T13:56:53.204701: step 4412, loss 1.92513, acc 0.953125, prec 0.0721468, recall 0.881032
2017-12-10T13:56:53.403367: step 4413, loss 0.0789384, acc 1, prec 0.0721649, recall 0.88106
2017-12-10T13:56:53.592658: step 4414, loss 0.238013, acc 0.984375, prec 0.072218, recall 0.881146
2017-12-10T13:56:53.784601: step 4415, loss 0.178049, acc 0.96875, prec 0.0722152, recall 0.881146
2017-12-10T13:56:53.970966: step 4416, loss 0.475683, acc 0.84375, prec 0.0722192, recall 0.881174
2017-12-10T13:56:54.161565: step 4417, loss 0.239839, acc 0.9375, prec 0.0723224, recall 0.881344
2017-12-10T13:56:54.345879: step 4418, loss 0.232093, acc 0.953125, prec 0.0723544, recall 0.8814
2017-12-10T13:56:54.536621: step 4419, loss 0.142334, acc 0.953125, prec 0.0723683, recall 0.881429
2017-12-10T13:56:54.727253: step 4420, loss 0.408594, acc 0.875, prec 0.0723932, recall 0.881485
2017-12-10T13:56:54.919493: step 4421, loss 0.374298, acc 0.9375, prec 0.0724057, recall 0.881513
2017-12-10T13:56:55.111330: step 4422, loss 0.120378, acc 0.953125, prec 0.0724196, recall 0.881541
2017-12-10T13:56:55.302460: step 4423, loss 0.235057, acc 0.90625, prec 0.0724111, recall 0.881541
2017-12-10T13:56:55.491145: step 4424, loss 0.412478, acc 0.9375, prec 0.0724054, recall 0.881541
2017-12-10T13:56:55.680026: step 4425, loss 0.273107, acc 0.953125, prec 0.0724193, recall 0.88157
2017-12-10T13:56:55.871947: step 4426, loss 0.0535895, acc 0.96875, prec 0.0724165, recall 0.88157
2017-12-10T13:56:56.066755: step 4427, loss 0.0459877, acc 0.984375, prec 0.0724151, recall 0.88157
2017-12-10T13:56:56.255555: step 4428, loss 0.234396, acc 0.953125, prec 0.0724289, recall 0.881598
2017-12-10T13:56:56.443875: step 4429, loss 0.533381, acc 0.890625, prec 0.0724553, recall 0.881654
2017-12-10T13:56:56.633231: step 4430, loss 0.0564374, acc 0.984375, prec 0.072472, recall 0.881682
2017-12-10T13:56:56.820279: step 4431, loss 0.0373202, acc 0.984375, prec 0.0724706, recall 0.881682
2017-12-10T13:56:57.010951: step 4432, loss 0.246637, acc 0.90625, prec 0.0724802, recall 0.88171
2017-12-10T13:56:57.199121: step 4433, loss 0.257038, acc 0.9375, prec 0.0724926, recall 0.881738
2017-12-10T13:56:57.386731: step 4434, loss 0.0604452, acc 0.984375, prec 0.0725274, recall 0.881794
2017-12-10T13:56:57.579280: step 4435, loss 0.0957098, acc 0.96875, prec 0.0725608, recall 0.881851
2017-12-10T13:56:57.766818: step 4436, loss 0.0331075, acc 0.984375, prec 0.0725594, recall 0.881851
2017-12-10T13:56:57.957093: step 4437, loss 0.0949826, acc 0.984375, prec 0.0725761, recall 0.881879
2017-12-10T13:56:58.147377: step 4438, loss 0.105459, acc 0.953125, prec 0.0725899, recall 0.881907
2017-12-10T13:56:58.339203: step 4439, loss 0.812564, acc 0.9375, prec 0.0726205, recall 0.881963
2017-12-10T13:56:58.530418: step 4440, loss 0.230899, acc 0.9375, prec 0.0726329, recall 0.88199
2017-12-10T13:56:58.716704: step 4441, loss 0.0398051, acc 0.984375, prec 0.0726315, recall 0.88199
2017-12-10T13:56:58.915653: step 4442, loss 0.10423, acc 0.96875, prec 0.0726648, recall 0.882046
2017-12-10T13:56:59.108473: step 4443, loss 0.0771042, acc 0.984375, prec 0.0727177, recall 0.88213
2017-12-10T13:56:59.299362: step 4444, loss 0.0809238, acc 0.984375, prec 0.0727163, recall 0.88213
2017-12-10T13:56:59.489514: step 4445, loss 0.0554759, acc 0.984375, prec 0.0727149, recall 0.88213
2017-12-10T13:56:59.680859: step 4446, loss 0.0861274, acc 0.984375, prec 0.0727134, recall 0.88213
2017-12-10T13:56:59.872686: step 4447, loss 0.369962, acc 0.953125, prec 0.0727092, recall 0.88213
2017-12-10T13:57:00.061215: step 4448, loss 0.32703, acc 0.953125, prec 0.0727049, recall 0.88213
2017-12-10T13:57:00.247450: step 4449, loss 0.197458, acc 0.953125, prec 0.0727007, recall 0.88213
2017-12-10T13:57:00.436702: step 4450, loss 0.18306, acc 0.96875, prec 0.072734, recall 0.882186
2017-12-10T13:57:00.627059: step 4451, loss 0.00342463, acc 1, prec 0.0727521, recall 0.882214
2017-12-10T13:57:00.811396: step 4452, loss 0.0503698, acc 0.96875, prec 0.0727673, recall 0.882242
2017-12-10T13:57:01.007752: step 4453, loss 0.13046, acc 0.9375, prec 0.0727617, recall 0.882242
2017-12-10T13:57:01.198123: step 4454, loss 0.772542, acc 0.953125, prec 0.0728297, recall 0.882353
2017-12-10T13:57:01.395605: step 4455, loss 0.069549, acc 0.984375, prec 0.0728464, recall 0.882381
2017-12-10T13:57:01.589668: step 4456, loss 1.77327, acc 0.984375, prec 0.0728645, recall 0.8822
2017-12-10T13:57:01.781237: step 4457, loss 0.0578557, acc 0.984375, prec 0.0728811, recall 0.882228
2017-12-10T13:57:01.968500: step 4458, loss 0.0163965, acc 0.984375, prec 0.0728978, recall 0.882256
2017-12-10T13:57:02.158408: step 4459, loss 0.0614537, acc 0.984375, prec 0.0729325, recall 0.882311
2017-12-10T13:57:02.348603: step 4460, loss 0.239431, acc 0.953125, prec 0.0729644, recall 0.882367
2017-12-10T13:57:02.537519: step 4461, loss 0.147757, acc 0.921875, prec 0.0729573, recall 0.882367
2017-12-10T13:57:02.726854: step 4462, loss 0.0583324, acc 0.96875, prec 0.0729544, recall 0.882367
2017-12-10T13:57:02.917615: step 4463, loss 0.0847869, acc 0.953125, prec 0.0729502, recall 0.882367
2017-12-10T13:57:03.104646: step 4464, loss 0.124909, acc 0.96875, prec 0.0729835, recall 0.882422
2017-12-10T13:57:03.302254: step 4465, loss 0.247824, acc 0.90625, prec 0.072993, recall 0.88245
2017-12-10T13:57:03.494471: step 4466, loss 0.0910282, acc 0.953125, prec 0.0730068, recall 0.882478
2017-12-10T13:57:03.682100: step 4467, loss 0.0801042, acc 0.953125, prec 0.0730025, recall 0.882478
2017-12-10T13:57:03.874840: step 4468, loss 0.0626871, acc 0.96875, prec 0.0730177, recall 0.882505
2017-12-10T13:57:04.064955: step 4469, loss 0.290916, acc 0.9375, prec 0.0730482, recall 0.882561
2017-12-10T13:57:04.256341: step 4470, loss 0.633785, acc 0.890625, prec 0.0730382, recall 0.882561
2017-12-10T13:57:04.445116: step 4471, loss 0.0771567, acc 0.96875, prec 0.0730354, recall 0.882561
2017-12-10T13:57:04.636302: step 4472, loss 0.304572, acc 0.953125, prec 0.0730311, recall 0.882561
2017-12-10T13:57:04.806596: step 4473, loss 0.22169, acc 0.980392, prec 0.0730477, recall 0.882588
2017-12-10T13:57:05.001729: step 4474, loss 0.133613, acc 0.953125, prec 0.0730615, recall 0.882616
2017-12-10T13:57:05.192370: step 4475, loss 0.103975, acc 0.953125, prec 0.0730753, recall 0.882643
2017-12-10T13:57:05.383723: step 4476, loss 0.0143559, acc 1, prec 0.0730933, recall 0.882671
2017-12-10T13:57:05.572655: step 4477, loss 0.866516, acc 0.953125, prec 0.0731071, recall 0.882699
2017-12-10T13:57:05.762958: step 4478, loss 0.244403, acc 0.875, prec 0.0731318, recall 0.882754
2017-12-10T13:57:05.948182: step 4479, loss 0.186006, acc 0.96875, prec 0.073165, recall 0.882809
2017-12-10T13:57:06.137247: step 4480, loss 0.583787, acc 0.90625, prec 0.0731745, recall 0.882836
2017-12-10T13:57:06.328878: step 4481, loss 0.391683, acc 0.90625, prec 0.073184, recall 0.882864
2017-12-10T13:57:06.521592: step 4482, loss 0.121379, acc 0.953125, prec 0.0731798, recall 0.882864
2017-12-10T13:57:06.709498: step 4483, loss 0.436042, acc 0.90625, prec 0.0731892, recall 0.882891
2017-12-10T13:57:06.898610: step 4484, loss 0.244919, acc 0.921875, prec 0.0732362, recall 0.882974
2017-12-10T13:57:07.089612: step 4485, loss 0.530481, acc 0.875, prec 0.0732609, recall 0.883029
2017-12-10T13:57:07.280241: step 4486, loss 0.0825539, acc 0.953125, prec 0.0732566, recall 0.883029
2017-12-10T13:57:07.469774: step 4487, loss 0.214985, acc 0.96875, prec 0.0732718, recall 0.883056
2017-12-10T13:57:07.663215: step 4488, loss 0.1467, acc 0.953125, prec 0.0733215, recall 0.883138
2017-12-10T13:57:07.853009: step 4489, loss 0.419421, acc 0.90625, prec 0.073331, recall 0.883166
2017-12-10T13:57:08.039849: step 4490, loss 0.161199, acc 0.921875, prec 0.0733239, recall 0.883166
2017-12-10T13:57:08.232624: step 4491, loss 0.247455, acc 0.9375, prec 0.0733362, recall 0.883193
2017-12-10T13:57:08.421678: step 4492, loss 0.146326, acc 0.96875, prec 0.0733513, recall 0.88322
2017-12-10T13:57:08.609165: step 4493, loss 0.0165277, acc 1, prec 0.0733694, recall 0.883248
2017-12-10T13:57:08.799574: step 4494, loss 0.0900934, acc 0.984375, prec 0.0734039, recall 0.883302
2017-12-10T13:57:08.989633: step 4495, loss 0.0779937, acc 0.96875, prec 0.0734011, recall 0.883302
2017-12-10T13:57:09.182140: step 4496, loss 0.0637085, acc 0.984375, prec 0.0734357, recall 0.883357
2017-12-10T13:57:09.373381: step 4497, loss 0.140759, acc 0.953125, prec 0.0734314, recall 0.883357
2017-12-10T13:57:09.557668: step 4498, loss 0.107163, acc 0.984375, prec 0.073484, recall 0.883438
2017-12-10T13:57:09.747801: step 4499, loss 0.255519, acc 0.96875, prec 0.0734991, recall 0.883466
2017-12-10T13:57:09.937880: step 4500, loss 0.098085, acc 0.96875, prec 0.0735143, recall 0.883493
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_0/1512931325/checkpoints/model-4500

2017-12-10T13:57:11.202604: step 4501, loss 0.0253971, acc 1, prec 0.0735143, recall 0.883493
2017-12-10T13:57:11.390397: step 4502, loss 0.0980336, acc 0.984375, prec 0.0735128, recall 0.883493
2017-12-10T13:57:11.579058: step 4503, loss 0.163255, acc 0.96875, prec 0.073528, recall 0.88352
2017-12-10T13:57:11.764439: step 4504, loss 0.479725, acc 0.984375, prec 0.0735625, recall 0.883574
2017-12-10T13:57:11.963248: step 4505, loss 0.190946, acc 0.96875, prec 0.0736137, recall 0.883656
2017-12-10T13:57:12.156998: step 4506, loss 0.0626825, acc 0.984375, prec 0.0736662, recall 0.883737
2017-12-10T13:57:12.345524: step 4507, loss 0.0387748, acc 0.984375, prec 0.0736648, recall 0.883737
2017-12-10T13:57:12.534301: step 4508, loss 0.00167482, acc 1, prec 0.0736828, recall 0.883764
2017-12-10T13:57:12.720735: step 4509, loss 0.0455634, acc 0.984375, prec 0.0736993, recall 0.883791
2017-12-10T13:57:12.910652: step 4510, loss 0.120924, acc 0.984375, prec 0.0737339, recall 0.883845
2017-12-10T13:57:13.103410: step 4511, loss 0.257272, acc 0.984375, prec 0.0737684, recall 0.8839
2017-12-10T13:57:13.292954: step 4512, loss 0.164583, acc 0.984375, prec 0.073785, recall 0.883927
2017-12-10T13:57:13.482474: step 4513, loss 0.0738928, acc 0.984375, prec 0.0737835, recall 0.883927
2017-12-10T13:57:13.670487: step 4514, loss 0.0449926, acc 0.984375, prec 0.0737821, recall 0.883927
2017-12-10T13:57:13.861484: step 4515, loss 0.385709, acc 1, prec 0.0738181, recall 0.88398
2017-12-10T13:57:14.059808: step 4516, loss 0.157926, acc 0.96875, prec 0.0738332, recall 0.884007
2017-12-10T13:57:14.251693: step 4517, loss 0.0623919, acc 0.984375, prec 0.0738497, recall 0.884034
2017-12-10T13:57:14.444728: step 4518, loss 0.0240709, acc 0.984375, prec 0.0739022, recall 0.884115
2017-12-10T13:57:14.639566: step 4519, loss 0.0124556, acc 1, prec 0.0739022, recall 0.884115
2017-12-10T13:57:14.831822: step 4520, loss 0.0927273, acc 0.96875, prec 0.0739173, recall 0.884142
2017-12-10T13:57:15.021782: step 4521, loss 0.24244, acc 0.984375, prec 0.0739519, recall 0.884196
2017-12-10T13:57:15.210868: step 4522, loss 0.0220963, acc 0.984375, prec 0.0739504, recall 0.884196
2017-12-10T13:57:15.403778: step 4523, loss 0.0765714, acc 0.984375, prec 0.073949, recall 0.884196
2017-12-10T13:57:15.595888: step 4524, loss 0.0464792, acc 0.984375, prec 0.0739835, recall 0.88425
2017-12-10T13:57:15.784970: step 4525, loss 0.18409, acc 0.96875, prec 0.0739806, recall 0.88425
2017-12-10T13:57:15.975381: step 4526, loss 0.198977, acc 0.953125, prec 0.0740123, recall 0.884303
2017-12-10T13:57:16.165784: step 4527, loss 0.296882, acc 0.9375, prec 0.0740245, recall 0.88433
2017-12-10T13:57:16.357688: step 4528, loss 0.172405, acc 0.96875, prec 0.0740396, recall 0.884357
2017-12-10T13:57:16.548943: step 4529, loss 0.159699, acc 0.96875, prec 0.0740547, recall 0.884384
2017-12-10T13:57:16.741003: step 4530, loss 0.314877, acc 0.9375, prec 0.0740489, recall 0.884384
2017-12-10T13:57:16.929354: step 4531, loss 0.185863, acc 0.921875, prec 0.0740597, recall 0.88441
2017-12-10T13:57:17.121000: step 4532, loss 0.11116, acc 0.9375, prec 0.0740719, recall 0.884437
2017-12-10T13:57:17.309416: step 4533, loss 0.117376, acc 0.9375, prec 0.0741021, recall 0.884491
2017-12-10T13:57:17.498711: step 4534, loss 0.403622, acc 0.90625, prec 0.0740935, recall 0.884491
2017-12-10T13:57:17.685418: step 4535, loss 0.0488459, acc 0.96875, prec 0.0740906, recall 0.884491
2017-12-10T13:57:17.875728: step 4536, loss 0.00798795, acc 1, prec 0.0740906, recall 0.884491
2017-12-10T13:57:18.066774: step 4537, loss 0.290737, acc 0.96875, prec 0.0741236, recall 0.884544
2017-12-10T13:57:18.257110: step 4538, loss 0.00813689, acc 1, prec 0.0741236, recall 0.884544
2017-12-10T13:57:18.444371: step 4539, loss 0.185674, acc 0.96875, prec 0.0741387, recall 0.884571
2017-12-10T13:57:18.637133: step 4540, loss 0.161894, acc 0.96875, prec 0.0741538, recall 0.884598
2017-12-10T13:57:18.823627: step 4541, loss 0.000651366, acc 1, prec 0.0741538, recall 0.884598
2017-12-10T13:57:19.008987: step 4542, loss 0.215808, acc 0.96875, prec 0.0741868, recall 0.884651
2017-12-10T13:57:19.198758: step 4543, loss 0.127929, acc 0.984375, prec 0.0742033, recall 0.884678
2017-12-10T13:57:19.387519: step 4544, loss 0.133496, acc 0.96875, prec 0.0742184, recall 0.884704
2017-12-10T13:57:19.580540: step 4545, loss 0.020447, acc 1, prec 0.0742543, recall 0.884758
2017-12-10T13:57:19.773899: step 4546, loss 0.296539, acc 0.953125, prec 0.0742679, recall 0.884784
2017-12-10T13:57:19.966124: step 4547, loss 0.00138238, acc 1, prec 0.0742679, recall 0.884784
2017-12-10T13:57:20.155122: step 4548, loss 0.33919, acc 1, prec 0.0742858, recall 0.884811
2017-12-10T13:57:20.345846: step 4549, loss 0.054024, acc 0.984375, prec 0.0742844, recall 0.884811
2017-12-10T13:57:20.534719: step 4550, loss 0.0139414, acc 0.984375, prec 0.0743009, recall 0.884837
2017-12-10T13:57:20.725632: step 4551, loss 0.0593391, acc 0.984375, prec 0.0743174, recall 0.884864
2017-12-10T13:57:20.915716: step 4552, loss 0.032914, acc 0.984375, prec 0.0743159, recall 0.884864
2017-12-10T13:57:21.105228: step 4553, loss 0.0286322, acc 0.984375, prec 0.0743145, recall 0.884864
2017-12-10T13:57:21.292708: step 4554, loss 0.00965434, acc 1, prec 0.0743324, recall 0.88489
2017-12-10T13:57:21.482638: step 4555, loss 0.15201, acc 0.953125, prec 0.0743281, recall 0.88489
2017-12-10T13:57:21.673966: step 4556, loss 0.173796, acc 0.96875, prec 0.0743252, recall 0.88489
2017-12-10T13:57:21.865084: step 4557, loss 0.14005, acc 0.984375, prec 0.0743955, recall 0.884997
2017-12-10T13:57:22.056657: step 4558, loss 0.0756696, acc 1, prec 0.0744314, recall 0.88505
2017-12-10T13:57:22.246709: step 4559, loss 0.0657226, acc 0.953125, prec 0.0744629, recall 0.885102
2017-12-10T13:57:22.433232: step 4560, loss 0.134798, acc 1, prec 0.0744988, recall 0.885155
2017-12-10T13:57:22.624709: step 4561, loss 0.314353, acc 0.96875, prec 0.0744959, recall 0.885155
2017-12-10T13:57:22.815441: step 4562, loss 0.11885, acc 0.984375, prec 0.0745303, recall 0.885208
2017-12-10T13:57:23.005454: step 4563, loss 0.0579238, acc 0.984375, prec 0.0745647, recall 0.885261
2017-12-10T13:57:23.197422: step 4564, loss 0.0555792, acc 0.96875, prec 0.0745618, recall 0.885261
2017-12-10T13:57:23.383915: step 4565, loss 0.015151, acc 1, prec 0.0745618, recall 0.885261
2017-12-10T13:57:23.571189: step 4566, loss 0.0389496, acc 1, prec 0.0745798, recall 0.885287
2017-12-10T13:57:23.765625: step 4567, loss 0.00986973, acc 1, prec 0.0745798, recall 0.885287
2017-12-10T13:57:23.954070: step 4568, loss 0.182591, acc 0.96875, prec 0.0745769, recall 0.885287
2017-12-10T13:57:24.146952: step 4569, loss 0.100389, acc 0.984375, prec 0.0745754, recall 0.885287
2017-12-10T13:57:24.338914: step 4570, loss 0.202808, acc 0.953125, prec 0.0746248, recall 0.885366
2017-12-10T13:57:24.536407: step 4571, loss 0.150066, acc 0.96875, prec 0.0746399, recall 0.885393
2017-12-10T13:57:24.728174: step 4572, loss 0.189561, acc 0.96875, prec 0.0746907, recall 0.885472
2017-12-10T13:57:24.919905: step 4573, loss 0.216695, acc 0.953125, prec 0.0747401, recall 0.88555
2017-12-10T13:57:25.109350: step 4574, loss 0.0585629, acc 0.984375, prec 0.0747566, recall 0.885577
2017-12-10T13:57:25.300665: step 4575, loss 0.00176047, acc 1, prec 0.0747566, recall 0.885577
2017-12-10T13:57:25.488250: step 4576, loss 0.245371, acc 0.953125, prec 0.0747881, recall 0.885629
2017-12-10T13:57:25.676065: step 4577, loss 0.190414, acc 0.953125, prec 0.0748016, recall 0.885655
2017-12-10T13:57:25.866066: step 4578, loss 0.158792, acc 0.984375, prec 0.0748181, recall 0.885682
2017-12-10T13:57:26.054037: step 4579, loss 0.154225, acc 0.9375, prec 0.0748302, recall 0.885708
2017-12-10T13:57:26.241202: step 4580, loss 0.00736883, acc 1, prec 0.074866, recall 0.88576
2017-12-10T13:57:26.430731: step 4581, loss 0.036035, acc 0.984375, prec 0.0748646, recall 0.88576
2017-12-10T13:57:26.621873: step 4582, loss 0.00921662, acc 1, prec 0.0748646, recall 0.88576
2017-12-10T13:57:26.806685: step 4583, loss 3.61978, acc 0.953125, prec 0.0748975, recall 0.88561
2017-12-10T13:57:26.998050: step 4584, loss 0.0733261, acc 0.953125, prec 0.074911, recall 0.885636
2017-12-10T13:57:27.188304: step 4585, loss 0.352801, acc 0.9375, prec 0.0749231, recall 0.885662
2017-12-10T13:57:27.375053: step 4586, loss 0.0947326, acc 0.984375, prec 0.0749395, recall 0.885688
2017-12-10T13:57:27.566263: step 4587, loss 0.481236, acc 0.890625, prec 0.0749473, recall 0.885714
2017-12-10T13:57:27.758779: step 4588, loss 0.0507184, acc 0.96875, prec 0.0749444, recall 0.885714
2017-12-10T13:57:27.949425: step 4589, loss 0.422036, acc 0.953125, prec 0.0749579, recall 0.88574
2017-12-10T13:57:28.141176: step 4590, loss 0.0979219, acc 0.96875, prec 0.074955, recall 0.88574
2017-12-10T13:57:28.329030: step 4591, loss 0.0906874, acc 0.953125, prec 0.0749507, recall 0.88574
2017-12-10T13:57:28.518524: step 4592, loss 0.243559, acc 0.9375, prec 0.0749628, recall 0.885767
2017-12-10T13:57:28.710183: step 4593, loss 0.275397, acc 0.890625, prec 0.0749526, recall 0.885767
2017-12-10T13:57:28.903965: step 4594, loss 0.582115, acc 0.921875, prec 0.0749454, recall 0.885767
2017-12-10T13:57:29.093205: step 4595, loss 0.432206, acc 0.890625, prec 0.0749531, recall 0.885793
2017-12-10T13:57:29.285444: step 4596, loss 0.26647, acc 0.9375, prec 0.0749831, recall 0.885845
2017-12-10T13:57:29.472879: step 4597, loss 0.196125, acc 0.9375, prec 0.0749773, recall 0.885845
2017-12-10T13:57:29.667741: step 4598, loss 0.129761, acc 0.921875, prec 0.0749701, recall 0.885845
2017-12-10T13:57:29.857984: step 4599, loss 0.164982, acc 0.921875, prec 0.0749986, recall 0.885897
2017-12-10T13:57:30.042565: step 4600, loss 1.42873, acc 0.9375, prec 0.0749942, recall 0.885695
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_0/1512931325/checkpoints/model-4600

2017-12-10T13:57:31.357312: step 4601, loss 0.569083, acc 0.859375, prec 0.0749812, recall 0.885695
2017-12-10T13:57:31.547658: step 4602, loss 0.201795, acc 0.96875, prec 0.0749783, recall 0.885695
2017-12-10T13:57:31.736717: step 4603, loss 0.537367, acc 0.921875, prec 0.0749889, recall 0.885721
2017-12-10T13:57:31.926589: step 4604, loss 0.280027, acc 0.953125, prec 0.0750024, recall 0.885747
2017-12-10T13:57:32.117460: step 4605, loss 0.433566, acc 0.875, prec 0.0750444, recall 0.885825
2017-12-10T13:57:32.307884: step 4606, loss 0.316881, acc 0.953125, prec 0.0750758, recall 0.885877
2017-12-10T13:57:32.495112: step 4607, loss 0.365509, acc 0.90625, prec 0.0750671, recall 0.885877
2017-12-10T13:57:32.684210: step 4608, loss 0.292316, acc 0.921875, prec 0.0750598, recall 0.885877
2017-12-10T13:57:32.876614: step 4609, loss 0.130922, acc 0.9375, prec 0.0751076, recall 0.885955
2017-12-10T13:57:33.068131: step 4610, loss 0.234551, acc 0.953125, prec 0.0751032, recall 0.885955
2017-12-10T13:57:33.253678: step 4611, loss 0.266486, acc 0.90625, prec 0.0751124, recall 0.885981
2017-12-10T13:57:33.447944: step 4612, loss 0.210102, acc 0.90625, prec 0.0751037, recall 0.885981
2017-12-10T13:57:33.635820: step 4613, loss 0.214509, acc 0.9375, prec 0.0750979, recall 0.885981
2017-12-10T13:57:33.827304: step 4614, loss 0.226855, acc 0.921875, prec 0.0751263, recall 0.886033
2017-12-10T13:57:34.015953: step 4615, loss 0.563275, acc 0.9375, prec 0.0751205, recall 0.886033
2017-12-10T13:57:34.205343: step 4616, loss 0.16001, acc 0.953125, prec 0.0751519, recall 0.886085
2017-12-10T13:57:34.400043: step 4617, loss 0.203974, acc 0.953125, prec 0.0751832, recall 0.886136
2017-12-10T13:57:34.590416: step 4618, loss 0.263412, acc 0.9375, prec 0.0752487, recall 0.88624
2017-12-10T13:57:34.776562: step 4619, loss 0.135793, acc 0.953125, prec 0.07528, recall 0.886291
2017-12-10T13:57:34.967759: step 4620, loss 0.17112, acc 0.921875, prec 0.0753084, recall 0.886343
2017-12-10T13:57:35.158010: step 4621, loss 0.0468659, acc 0.984375, prec 0.0753248, recall 0.886369
2017-12-10T13:57:35.343729: step 4622, loss 0.237367, acc 0.953125, prec 0.0753561, recall 0.88642
2017-12-10T13:57:35.531557: step 4623, loss 0.229234, acc 0.96875, prec 0.075371, recall 0.886446
2017-12-10T13:57:35.722567: step 4624, loss 0.0769501, acc 0.96875, prec 0.0753859, recall 0.886472
2017-12-10T13:57:35.913105: step 4625, loss 0.162894, acc 0.953125, prec 0.0753993, recall 0.886497
2017-12-10T13:57:36.102181: step 4626, loss 0.0430839, acc 0.984375, prec 0.0754157, recall 0.886523
2017-12-10T13:57:36.292658: step 4627, loss 0.112581, acc 0.984375, prec 0.0754321, recall 0.886549
2017-12-10T13:57:36.485941: step 4628, loss 0.00825539, acc 1, prec 0.0754677, recall 0.8866
2017-12-10T13:57:36.675503: step 4629, loss 0.0275874, acc 0.984375, prec 0.0755197, recall 0.886677
2017-12-10T13:57:36.863819: step 4630, loss 0.0258215, acc 0.984375, prec 0.0755182, recall 0.886677
2017-12-10T13:57:37.056330: step 4631, loss 0.0583356, acc 0.984375, prec 0.0755346, recall 0.886703
2017-12-10T13:57:37.242330: step 4632, loss 0.183709, acc 0.96875, prec 0.0755317, recall 0.886703
2017-12-10T13:57:37.432081: step 4633, loss 0.147581, acc 0.96875, prec 0.0755288, recall 0.886703
2017-12-10T13:57:37.618329: step 4634, loss 0.127753, acc 0.9375, prec 0.0755407, recall 0.886728
2017-12-10T13:57:37.812769: step 4635, loss 0.00227258, acc 1, prec 0.0755586, recall 0.886754
2017-12-10T13:57:38.004762: step 4636, loss 0.0893032, acc 0.984375, prec 0.0755571, recall 0.886754
2017-12-10T13:57:38.196083: step 4637, loss 0.0166737, acc 1, prec 0.0755749, recall 0.88678
2017-12-10T13:57:38.383147: step 4638, loss 0.0421323, acc 0.984375, prec 0.0755734, recall 0.88678
2017-12-10T13:57:38.572666: step 4639, loss 3.49124, acc 0.984375, prec 0.0755734, recall 0.886579
2017-12-10T13:57:38.767611: step 4640, loss 0.000714461, acc 1, prec 0.0755734, recall 0.886579
2017-12-10T13:57:38.956926: step 4641, loss 0.0913904, acc 0.984375, prec 0.075572, recall 0.886579
2017-12-10T13:57:39.147610: step 4642, loss 0.0627924, acc 0.984375, prec 0.0755883, recall 0.886605
2017-12-10T13:57:39.338093: step 4643, loss 0.0912421, acc 0.953125, prec 0.0756374, recall 0.886682
2017-12-10T13:57:39.525193: step 4644, loss 1.6025, acc 0.9375, prec 0.075633, recall 0.886482
2017-12-10T13:57:39.721616: step 4645, loss 0.471876, acc 1, prec 0.0756508, recall 0.886507
2017-12-10T13:57:39.917688: step 4646, loss 0.283566, acc 0.9375, prec 0.075645, recall 0.886507
2017-12-10T13:57:40.108593: step 4647, loss 0.106723, acc 0.953125, prec 0.0756584, recall 0.886533
2017-12-10T13:57:40.302055: step 4648, loss 0.112559, acc 0.9375, prec 0.0756704, recall 0.886558
2017-12-10T13:57:40.493923: step 4649, loss 0.129706, acc 0.953125, prec 0.0757016, recall 0.88661
2017-12-10T13:57:40.684794: step 4650, loss 0.191627, acc 0.953125, prec 0.0756972, recall 0.88661
2017-12-10T13:57:40.873250: step 4651, loss 0.177428, acc 0.953125, prec 0.0756928, recall 0.88661
2017-12-10T13:57:41.060238: step 4652, loss 0.2405, acc 0.9375, prec 0.075687, recall 0.88661
2017-12-10T13:57:41.250085: step 4653, loss 0.339186, acc 0.921875, prec 0.0757153, recall 0.886661
2017-12-10T13:57:41.436337: step 4654, loss 0.215137, acc 0.9375, prec 0.0757095, recall 0.886661
2017-12-10T13:57:41.624310: step 4655, loss 0.40991, acc 0.84375, prec 0.0756949, recall 0.886661
2017-12-10T13:57:41.818651: step 4656, loss 0.201547, acc 0.90625, prec 0.075704, recall 0.886686
2017-12-10T13:57:42.005273: step 4657, loss 0.317569, acc 0.890625, prec 0.0757115, recall 0.886712
2017-12-10T13:57:42.195765: step 4658, loss 0.296115, acc 0.90625, prec 0.0757028, recall 0.886712
2017-12-10T13:57:42.385168: step 4659, loss 0.191561, acc 0.984375, prec 0.0757191, recall 0.886737
2017-12-10T13:57:42.574028: step 4660, loss 0.359569, acc 0.890625, prec 0.0757267, recall 0.886763
2017-12-10T13:57:42.762705: step 4661, loss 0.167883, acc 0.9375, prec 0.0757209, recall 0.886763
2017-12-10T13:57:42.953961: step 4662, loss 0.338543, acc 0.90625, prec 0.0757299, recall 0.886788
2017-12-10T13:57:43.141297: step 4663, loss 0.385821, acc 0.921875, prec 0.0757404, recall 0.886814
2017-12-10T13:57:43.335626: step 4664, loss 0.202401, acc 0.96875, prec 0.0757552, recall 0.886839
2017-12-10T13:57:43.526291: step 4665, loss 0.234676, acc 0.921875, prec 0.0757657, recall 0.886865
2017-12-10T13:57:43.717800: step 4666, loss 0.0988792, acc 0.96875, prec 0.0757628, recall 0.886865
2017-12-10T13:57:43.912044: step 4667, loss 0.0181241, acc 1, prec 0.0758161, recall 0.886941
2017-12-10T13:57:44.102359: step 4668, loss 0.140787, acc 0.984375, prec 0.0758324, recall 0.886966
2017-12-10T13:57:44.294711: step 4669, loss 0.200701, acc 0.96875, prec 0.0758472, recall 0.886992
2017-12-10T13:57:44.490405: step 4670, loss 0.0298132, acc 0.96875, prec 0.0758443, recall 0.886992
2017-12-10T13:57:44.683353: step 4671, loss 0.144915, acc 0.96875, prec 0.0758592, recall 0.887017
2017-12-10T13:57:44.874296: step 4672, loss 0.190863, acc 0.96875, prec 0.0758562, recall 0.887017
2017-12-10T13:57:45.067416: step 4673, loss 0.0700185, acc 0.953125, prec 0.0758696, recall 0.887042
2017-12-10T13:57:45.255188: step 4674, loss 0.124208, acc 0.9375, prec 0.0758993, recall 0.887093
2017-12-10T13:57:45.447079: step 4675, loss 0.0902995, acc 0.96875, prec 0.0759319, recall 0.887144
2017-12-10T13:57:45.635851: step 4676, loss 0.00971494, acc 1, prec 0.0759319, recall 0.887144
2017-12-10T13:57:45.825189: step 4677, loss 0.170624, acc 0.984375, prec 0.0759836, recall 0.88722
2017-12-10T13:57:46.018181: step 4678, loss 0.0692116, acc 0.96875, prec 0.0760162, recall 0.88727
2017-12-10T13:57:46.207154: step 4679, loss 0.0658616, acc 1, prec 0.0760339, recall 0.887296
2017-12-10T13:57:46.397407: step 4680, loss 0.00471214, acc 1, prec 0.0760339, recall 0.887296
2017-12-10T13:57:46.585369: step 4681, loss 0.0371255, acc 0.96875, prec 0.0760488, recall 0.887321
2017-12-10T13:57:46.781134: step 4682, loss 0.238039, acc 0.96875, prec 0.0760636, recall 0.887346
2017-12-10T13:57:46.978374: step 4683, loss 0.0882154, acc 0.96875, prec 0.0760607, recall 0.887346
2017-12-10T13:57:47.169128: step 4684, loss 0.0756283, acc 0.984375, prec 0.0760592, recall 0.887346
2017-12-10T13:57:47.361911: step 4685, loss 0.435939, acc 1, prec 0.0760947, recall 0.887396
2017-12-10T13:57:47.553141: step 4686, loss 0.0501734, acc 0.984375, prec 0.0761464, recall 0.887472
2017-12-10T13:57:47.744935: step 4687, loss 0.00295342, acc 1, prec 0.0761641, recall 0.887497
2017-12-10T13:57:47.934042: step 4688, loss 0.360322, acc 0.96875, prec 0.076179, recall 0.887522
2017-12-10T13:57:48.124964: step 4689, loss 0.0262152, acc 0.984375, prec 0.0761952, recall 0.887547
2017-12-10T13:57:48.312782: step 4690, loss 0.246913, acc 0.953125, prec 0.0762086, recall 0.887573
2017-12-10T13:57:48.501566: step 4691, loss 0.358439, acc 0.921875, prec 0.0762013, recall 0.887573
2017-12-10T13:57:48.691993: step 4692, loss 0.0785733, acc 0.96875, prec 0.0761983, recall 0.887573
2017-12-10T13:57:48.883913: step 4693, loss 0.145553, acc 0.953125, prec 0.0762117, recall 0.887598
2017-12-10T13:57:49.073784: step 4694, loss 0.0965859, acc 0.96875, prec 0.0762442, recall 0.887648
2017-12-10T13:57:49.263442: step 4695, loss 0.0806313, acc 0.96875, prec 0.076259, recall 0.887673
2017-12-10T13:57:49.451407: step 4696, loss 0.106948, acc 0.96875, prec 0.0762561, recall 0.887673
2017-12-10T13:57:49.640516: step 4697, loss 0.166637, acc 0.96875, prec 0.0762531, recall 0.887673
2017-12-10T13:57:49.833602: step 4698, loss 0.102813, acc 0.96875, prec 0.0762502, recall 0.887673
2017-12-10T13:57:50.026746: step 4699, loss 0.347839, acc 0.953125, prec 0.0762813, recall 0.887723
2017-12-10T13:57:50.216568: step 4700, loss 0.0650331, acc 0.96875, prec 0.0762961, recall 0.887748
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_0/1512931325/checkpoints/model-4700

2017-12-10T13:57:51.399149: step 4701, loss 0.187389, acc 0.953125, prec 0.0763094, recall 0.887773
2017-12-10T13:57:51.591296: step 4702, loss 0.12532, acc 0.96875, prec 0.0763419, recall 0.887823
2017-12-10T13:57:51.779853: step 4703, loss 0.0855801, acc 0.984375, prec 0.0763758, recall 0.887873
2017-12-10T13:57:51.975547: step 4704, loss 0.0569739, acc 0.96875, prec 0.0763906, recall 0.887898
2017-12-10T13:57:52.166402: step 4705, loss 0.0602532, acc 0.984375, prec 0.0763892, recall 0.887898
2017-12-10T13:57:52.355731: step 4706, loss 0.212099, acc 0.953125, prec 0.0763848, recall 0.887898
2017-12-10T13:57:52.543565: step 4707, loss 0.217589, acc 0.9375, prec 0.0763789, recall 0.887898
2017-12-10T13:57:52.734924: step 4708, loss 0.151895, acc 0.984375, prec 0.0764483, recall 0.887998
2017-12-10T13:57:52.926416: step 4709, loss 0.102582, acc 0.953125, prec 0.0764616, recall 0.888023
2017-12-10T13:57:53.120736: step 4710, loss 0.0777102, acc 0.96875, prec 0.0764763, recall 0.888048
2017-12-10T13:57:53.315219: step 4711, loss 0.179634, acc 0.9375, prec 0.0764882, recall 0.888073
2017-12-10T13:57:53.504313: step 4712, loss 0.020453, acc 0.984375, prec 0.0765044, recall 0.888098
2017-12-10T13:57:53.700521: step 4713, loss 0.0158344, acc 1, prec 0.0765398, recall 0.888148
2017-12-10T13:57:53.889886: step 4714, loss 0.00367373, acc 1, prec 0.0765398, recall 0.888148
2017-12-10T13:57:54.075081: step 4715, loss 0.110878, acc 0.96875, prec 0.0765546, recall 0.888173
2017-12-10T13:57:54.261633: step 4716, loss 0.0161743, acc 1, prec 0.0766077, recall 0.888247
2017-12-10T13:57:54.451427: step 4717, loss 0.0132735, acc 1, prec 0.0766077, recall 0.888247
2017-12-10T13:57:54.639953: step 4718, loss 0.236769, acc 0.921875, prec 0.0766003, recall 0.888247
2017-12-10T13:57:54.833537: step 4719, loss 0.128064, acc 1, prec 0.076618, recall 0.888272
2017-12-10T13:57:55.030082: step 4720, loss 0.0100796, acc 1, prec 0.0766357, recall 0.888297
2017-12-10T13:57:55.219005: step 4721, loss 0.00959427, acc 1, prec 0.0766534, recall 0.888322
2017-12-10T13:57:55.413249: step 4722, loss 0.00778566, acc 1, prec 0.0766888, recall 0.888371
2017-12-10T13:57:55.605011: step 4723, loss 0.00158531, acc 1, prec 0.0767064, recall 0.888396
2017-12-10T13:57:55.792533: step 4724, loss 1.01244, acc 0.984375, prec 0.076758, recall 0.88847
2017-12-10T13:57:55.985961: step 4725, loss 0.0446868, acc 0.984375, prec 0.0767743, recall 0.888495
2017-12-10T13:57:56.173987: step 4726, loss 6.89725, acc 0.96875, prec 0.0768258, recall 0.888372
2017-12-10T13:57:56.368433: step 4727, loss 0.00535525, acc 1, prec 0.0768258, recall 0.888372
2017-12-10T13:57:56.557199: step 4728, loss 0.148602, acc 0.953125, prec 0.0768391, recall 0.888397
2017-12-10T13:57:56.750434: step 4729, loss 0.193086, acc 0.921875, prec 0.0768494, recall 0.888422
2017-12-10T13:57:56.940817: step 4730, loss 0.0991398, acc 0.953125, prec 0.076845, recall 0.888422
2017-12-10T13:57:57.131494: step 4731, loss 0.370562, acc 0.9375, prec 0.0768745, recall 0.888471
2017-12-10T13:57:57.321060: step 4732, loss 0.227244, acc 0.9375, prec 0.0768686, recall 0.888471
2017-12-10T13:57:57.506588: step 4733, loss 0.196066, acc 0.953125, prec 0.0768642, recall 0.888471
2017-12-10T13:57:57.691922: step 4734, loss 0.11469, acc 0.953125, prec 0.0768774, recall 0.888496
2017-12-10T13:57:57.883471: step 4735, loss 0.183529, acc 0.9375, prec 0.0768715, recall 0.888496
2017-12-10T13:57:58.075284: step 4736, loss 0.26534, acc 0.9375, prec 0.0768657, recall 0.888496
2017-12-10T13:57:58.263418: step 4737, loss 0.269251, acc 0.890625, prec 0.0768554, recall 0.888496
2017-12-10T13:57:58.454054: step 4738, loss 0.228557, acc 0.90625, prec 0.0768995, recall 0.88857
2017-12-10T13:57:58.641003: step 4739, loss 0.832609, acc 0.875, prec 0.0768878, recall 0.88857
2017-12-10T13:57:58.828242: step 4740, loss 0.3531, acc 0.921875, prec 0.0769157, recall 0.888619
2017-12-10T13:57:59.024132: step 4741, loss 0.381781, acc 0.859375, prec 0.0769025, recall 0.888619
2017-12-10T13:57:59.215024: step 4742, loss 0.488424, acc 0.890625, prec 0.0769098, recall 0.888643
2017-12-10T13:57:59.404431: step 4743, loss 0.286233, acc 0.875, prec 0.0768981, recall 0.888643
2017-12-10T13:57:59.593814: step 4744, loss 0.358256, acc 0.875, prec 0.0768863, recall 0.888643
2017-12-10T13:57:59.782418: step 4745, loss 0.299985, acc 0.921875, prec 0.0768966, recall 0.888668
2017-12-10T13:57:59.970810: step 4746, loss 0.325453, acc 0.921875, prec 0.0769069, recall 0.888693
2017-12-10T13:58:00.160282: step 4747, loss 0.344814, acc 0.921875, prec 0.0769172, recall 0.888717
2017-12-10T13:58:00.346530: step 4748, loss 0.0874308, acc 0.96875, prec 0.0769319, recall 0.888742
2017-12-10T13:58:00.534509: step 4749, loss 0.103739, acc 0.96875, prec 0.0769466, recall 0.888766
2017-12-10T13:58:00.721377: step 4750, loss 0.352586, acc 0.890625, prec 0.0769363, recall 0.888766
2017-12-10T13:58:00.911528: step 4751, loss 0.239705, acc 0.953125, prec 0.0769319, recall 0.888766
2017-12-10T13:58:01.103425: step 4752, loss 2.7345, acc 0.90625, prec 0.0769245, recall 0.88857
2017-12-10T13:58:01.296251: step 4753, loss 0.401092, acc 0.953125, prec 0.0769378, recall 0.888595
2017-12-10T13:58:01.487371: step 4754, loss 0.134435, acc 0.953125, prec 0.076951, recall 0.888619
2017-12-10T13:58:01.677291: step 4755, loss 0.319506, acc 0.90625, prec 0.0769598, recall 0.888644
2017-12-10T13:58:01.869061: step 4756, loss 0.0467204, acc 0.984375, prec 0.0769583, recall 0.888644
2017-12-10T13:58:02.059727: step 4757, loss 0.279723, acc 0.9375, prec 0.0769524, recall 0.888644
2017-12-10T13:58:02.251390: step 4758, loss 0.895127, acc 0.875, prec 0.0769407, recall 0.888644
2017-12-10T13:58:02.441732: step 4759, loss 0.262043, acc 0.921875, prec 0.076951, recall 0.888668
2017-12-10T13:58:02.633986: step 4760, loss 0.284975, acc 0.953125, prec 0.0769642, recall 0.888693
2017-12-10T13:58:02.823591: step 4761, loss 0.0802806, acc 0.984375, prec 0.0769627, recall 0.888693
2017-12-10T13:58:03.011781: step 4762, loss 0.189205, acc 0.953125, prec 0.0769759, recall 0.888717
2017-12-10T13:58:03.199830: step 4763, loss 0.0603446, acc 0.953125, prec 0.0769891, recall 0.888742
2017-12-10T13:58:03.391654: step 4764, loss 0.16997, acc 0.953125, prec 0.07702, recall 0.888791
2017-12-10T13:58:03.582036: step 4765, loss 0.0984833, acc 0.96875, prec 0.077017, recall 0.888791
2017-12-10T13:58:03.774969: step 4766, loss 0.061439, acc 0.984375, prec 0.0770332, recall 0.888816
2017-12-10T13:58:03.966117: step 4767, loss 0.174866, acc 0.96875, prec 0.0770654, recall 0.888864
2017-12-10T13:58:04.159113: step 4768, loss 0.093924, acc 0.953125, prec 0.0770786, recall 0.888889
2017-12-10T13:58:04.346044: step 4769, loss 0.26863, acc 0.9375, prec 0.0770728, recall 0.888889
2017-12-10T13:58:04.532341: step 4770, loss 0.0300358, acc 1, prec 0.0770904, recall 0.888913
2017-12-10T13:58:04.722144: step 4771, loss 0.0195077, acc 1, prec 0.0770904, recall 0.888913
2017-12-10T13:58:04.907585: step 4772, loss 0.152463, acc 0.953125, prec 0.077086, recall 0.888913
2017-12-10T13:58:05.099625: step 4773, loss 0.00716854, acc 1, prec 0.0771036, recall 0.888938
2017-12-10T13:58:05.290372: step 4774, loss 0.101894, acc 0.984375, prec 0.0771197, recall 0.888962
2017-12-10T13:58:05.482062: step 4775, loss 0.367108, acc 0.9375, prec 0.0771842, recall 0.88906
2017-12-10T13:58:05.672544: step 4776, loss 0.0512623, acc 0.984375, prec 0.0772003, recall 0.889084
2017-12-10T13:58:05.863027: step 4777, loss 0.261211, acc 0.953125, prec 0.0771959, recall 0.889084
2017-12-10T13:58:06.054390: step 4778, loss 0.266714, acc 0.921875, prec 0.0772062, recall 0.889108
2017-12-10T13:58:06.246097: step 4779, loss 0.278586, acc 0.9375, prec 0.0772179, recall 0.889133
2017-12-10T13:58:06.435274: step 4780, loss 0.16845, acc 0.96875, prec 0.0772501, recall 0.889181
2017-12-10T13:58:06.626798: step 4781, loss 0.0341566, acc 0.984375, prec 0.0772486, recall 0.889181
2017-12-10T13:58:06.813384: step 4782, loss 0.423542, acc 0.9375, prec 0.0772603, recall 0.889206
2017-12-10T13:58:07.009667: step 4783, loss 0.134545, acc 0.984375, prec 0.0772765, recall 0.88923
2017-12-10T13:58:07.205690: step 4784, loss 0.0167505, acc 1, prec 0.077294, recall 0.889254
2017-12-10T13:58:07.396400: step 4785, loss 0.0122875, acc 1, prec 0.077294, recall 0.889254
2017-12-10T13:58:07.582087: step 4786, loss 0.0140952, acc 1, prec 0.077294, recall 0.889254
2017-12-10T13:58:07.774504: step 4787, loss 0.0226872, acc 1, prec 0.0773468, recall 0.889327
2017-12-10T13:58:07.962266: step 4788, loss 0.0699101, acc 0.984375, prec 0.0773629, recall 0.889351
2017-12-10T13:58:08.153363: step 4789, loss 0.239172, acc 0.96875, prec 0.0773776, recall 0.889376
2017-12-10T13:58:08.348687: step 4790, loss 0.462534, acc 0.953125, prec 0.0773907, recall 0.8894
2017-12-10T13:58:08.542021: step 4791, loss 0.283727, acc 0.953125, prec 0.0773863, recall 0.8894
2017-12-10T13:58:08.735671: step 4792, loss 0.0115604, acc 1, prec 0.0774039, recall 0.889424
2017-12-10T13:58:08.924651: step 4793, loss 0.0652668, acc 0.96875, prec 0.0774185, recall 0.889448
2017-12-10T13:58:09.116848: step 4794, loss 0.132144, acc 0.984375, prec 0.077417, recall 0.889448
2017-12-10T13:58:09.303474: step 4795, loss 0.0199077, acc 0.984375, prec 0.0774683, recall 0.889521
2017-12-10T13:58:09.492007: step 4796, loss 0.00852794, acc 1, prec 0.0774683, recall 0.889521
2017-12-10T13:58:09.678788: step 4797, loss 1.50453, acc 0.96875, prec 0.077502, recall 0.889375
2017-12-10T13:58:09.871850: step 4798, loss 0.326299, acc 0.9375, prec 0.077496, recall 0.889375
2017-12-10T13:58:10.057482: step 4799, loss 0.363338, acc 0.9375, prec 0.0775429, recall 0.889447
2017-12-10T13:58:10.245592: step 4800, loss 0.279647, acc 0.953125, prec 0.0775384, recall 0.889447
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_0/1512931325/checkpoints/model-4800

2017-12-10T13:58:19.643012: step 4801, loss 0.21239, acc 0.9375, prec 0.0775501, recall 0.889471
2017-12-10T13:58:19.832323: step 4802, loss 0.161697, acc 0.953125, prec 0.0775808, recall 0.88952
2017-12-10T13:58:20.020216: step 4803, loss 4.12002, acc 0.984375, prec 0.0775808, recall 0.889325
2017-12-10T13:58:20.212066: step 4804, loss 0.193561, acc 0.953125, prec 0.0775764, recall 0.889325
2017-12-10T13:58:20.401615: step 4805, loss 0.110151, acc 0.96875, prec 0.0775734, recall 0.889325
2017-12-10T13:58:20.593098: step 4806, loss 0.374675, acc 0.890625, prec 0.0775631, recall 0.889325
2017-12-10T13:58:20.785002: step 4807, loss 0.153362, acc 0.90625, prec 0.0775718, recall 0.88935
2017-12-10T13:58:20.971539: step 4808, loss 0.423548, acc 0.875, prec 0.07756, recall 0.88935
2017-12-10T13:58:21.162381: step 4809, loss 0.403286, acc 0.90625, prec 0.0775511, recall 0.88935
2017-12-10T13:58:21.354806: step 4810, loss 0.56701, acc 0.84375, prec 0.0775539, recall 0.889374
2017-12-10T13:58:21.544864: step 4811, loss 0.967084, acc 0.859375, prec 0.0775406, recall 0.889374
2017-12-10T13:58:21.737236: step 4812, loss 0.489204, acc 0.890625, prec 0.0775478, recall 0.889398
2017-12-10T13:58:21.928771: step 4813, loss 0.217049, acc 0.9375, prec 0.0775419, recall 0.889398
2017-12-10T13:58:22.121562: step 4814, loss 0.363319, acc 0.90625, prec 0.0775506, recall 0.889422
2017-12-10T13:58:22.307854: step 4815, loss 0.532566, acc 0.828125, prec 0.0775344, recall 0.889422
2017-12-10T13:58:22.495930: step 4816, loss 0.529272, acc 0.890625, prec 0.0775592, recall 0.88947
2017-12-10T13:58:22.681067: step 4817, loss 0.290195, acc 0.921875, prec 0.0775518, recall 0.88947
2017-12-10T13:58:22.872135: step 4818, loss 0.14238, acc 0.9375, prec 0.0775459, recall 0.88947
2017-12-10T13:58:23.062103: step 4819, loss 0.458787, acc 0.890625, prec 0.0775356, recall 0.88947
2017-12-10T13:58:23.253263: step 4820, loss 0.264866, acc 0.9375, prec 0.0775297, recall 0.88947
2017-12-10T13:58:23.441450: step 4821, loss 0.551653, acc 0.84375, prec 0.077515, recall 0.88947
2017-12-10T13:58:23.630191: step 4822, loss 0.282385, acc 0.921875, prec 0.0775251, recall 0.889494
2017-12-10T13:58:23.821475: step 4823, loss 0.138561, acc 0.953125, prec 0.0775207, recall 0.889494
2017-12-10T13:58:24.016821: step 4824, loss 0.0383015, acc 0.984375, prec 0.0775368, recall 0.889518
2017-12-10T13:58:24.205915: step 4825, loss 0.225867, acc 0.90625, prec 0.0775279, recall 0.889518
2017-12-10T13:58:24.395831: step 4826, loss 0.478006, acc 0.9375, prec 0.077522, recall 0.889518
2017-12-10T13:58:24.587538: step 4827, loss 0.211442, acc 0.90625, prec 0.0775307, recall 0.889542
2017-12-10T13:58:24.777406: step 4828, loss 0.0670788, acc 0.96875, prec 0.0775278, recall 0.889542
2017-12-10T13:58:24.970149: step 4829, loss 0.175389, acc 0.953125, prec 0.0775409, recall 0.889567
2017-12-10T13:58:25.162845: step 4830, loss 0.270425, acc 0.96875, prec 0.0775379, recall 0.889567
2017-12-10T13:58:25.352958: step 4831, loss 0.172048, acc 0.9375, prec 0.077532, recall 0.889567
2017-12-10T13:58:25.544376: step 4832, loss 0.144624, acc 0.9375, prec 0.0775437, recall 0.889591
2017-12-10T13:58:25.736491: step 4833, loss 0.141036, acc 0.953125, prec 0.0775568, recall 0.889615
2017-12-10T13:58:25.927370: step 4834, loss 0.371427, acc 0.9375, prec 0.0775859, recall 0.889663
2017-12-10T13:58:26.119791: step 4835, loss 0.0155987, acc 1, prec 0.0776034, recall 0.889687
2017-12-10T13:58:26.308393: step 4836, loss 0.026068, acc 0.984375, prec 0.0776019, recall 0.889687
2017-12-10T13:58:26.497063: step 4837, loss 0.0977326, acc 0.96875, prec 0.077599, recall 0.889687
2017-12-10T13:58:26.692002: step 4838, loss 0.043373, acc 0.984375, prec 0.077615, recall 0.889711
2017-12-10T13:58:26.885596: step 4839, loss 0.0857595, acc 0.984375, prec 0.077631, recall 0.889735
2017-12-10T13:58:27.079769: step 4840, loss 0.175883, acc 0.984375, prec 0.0776296, recall 0.889735
2017-12-10T13:58:27.268377: step 4841, loss 0.0249448, acc 1, prec 0.0776471, recall 0.889759
2017-12-10T13:58:27.462275: step 4842, loss 0.00948749, acc 1, prec 0.0776471, recall 0.889759
2017-12-10T13:58:27.650224: step 4843, loss 0.0579671, acc 0.96875, prec 0.0776616, recall 0.889783
2017-12-10T13:58:27.841885: step 4844, loss 0.325717, acc 0.921875, prec 0.0776542, recall 0.889783
2017-12-10T13:58:28.034725: step 4845, loss 0.46165, acc 0.984375, prec 0.0777053, recall 0.889854
2017-12-10T13:58:28.223324: step 4846, loss 0.012668, acc 1, prec 0.0777228, recall 0.889878
2017-12-10T13:58:28.417501: step 4847, loss 0.224794, acc 0.96875, prec 0.0777373, recall 0.889902
2017-12-10T13:58:28.608833: step 4848, loss 0.0273433, acc 1, prec 0.0777548, recall 0.889926
2017-12-10T13:58:28.803352: step 4849, loss 0.0254196, acc 0.984375, prec 0.0777533, recall 0.889926
2017-12-10T13:58:29.002501: step 4850, loss 0.0212503, acc 1, prec 0.0777708, recall 0.88995
2017-12-10T13:58:29.192302: step 4851, loss 0.00483329, acc 1, prec 0.0777708, recall 0.88995
2017-12-10T13:58:29.381704: step 4852, loss 0.190192, acc 1, prec 0.0777883, recall 0.889974
2017-12-10T13:58:29.573937: step 4853, loss 0.00320742, acc 1, prec 0.0777883, recall 0.889974
2017-12-10T13:58:29.763949: step 4854, loss 0.0727544, acc 0.96875, prec 0.0777854, recall 0.889974
2017-12-10T13:58:29.956187: step 4855, loss 0.00187179, acc 1, prec 0.0777854, recall 0.889974
2017-12-10T13:58:30.145501: step 4856, loss 0.014954, acc 1, prec 0.0778029, recall 0.889998
2017-12-10T13:58:30.339116: step 4857, loss 0.170463, acc 0.984375, prec 0.0778189, recall 0.890022
2017-12-10T13:58:30.533332: step 4858, loss 0.00532033, acc 1, prec 0.0778189, recall 0.890022
2017-12-10T13:58:30.728617: step 4859, loss 0.0216007, acc 1, prec 0.0778364, recall 0.890046
2017-12-10T13:58:30.920616: step 4860, loss 0.454632, acc 0.984375, prec 0.0778524, recall 0.890069
2017-12-10T13:58:31.123404: step 4861, loss 0.0986424, acc 0.96875, prec 0.0778494, recall 0.890069
2017-12-10T13:58:31.314448: step 4862, loss 0.0336706, acc 0.984375, prec 0.0778479, recall 0.890069
2017-12-10T13:58:31.510212: step 4863, loss 0.214637, acc 0.96875, prec 0.077845, recall 0.890069
2017-12-10T13:58:31.697069: step 4864, loss 0.101548, acc 0.96875, prec 0.077842, recall 0.890069
2017-12-10T13:58:31.886540: step 4865, loss 0.20584, acc 0.96875, prec 0.0778566, recall 0.890093
2017-12-10T13:58:32.078147: step 4866, loss 0.0418855, acc 0.984375, prec 0.0778726, recall 0.890117
2017-12-10T13:58:32.269551: step 4867, loss 0.0162015, acc 1, prec 0.0778726, recall 0.890117
2017-12-10T13:58:32.458000: step 4868, loss 0.066499, acc 0.984375, prec 0.0778711, recall 0.890117
2017-12-10T13:58:32.649555: step 4869, loss 0.05508, acc 0.96875, prec 0.0778682, recall 0.890117
2017-12-10T13:58:32.837212: step 4870, loss 0.0284937, acc 0.96875, prec 0.0778652, recall 0.890117
2017-12-10T13:58:33.027742: step 4871, loss 0.497155, acc 0.9375, prec 0.0778768, recall 0.890141
2017-12-10T13:58:33.218384: step 4872, loss 0.352707, acc 0.96875, prec 0.0778738, recall 0.890141
2017-12-10T13:58:33.408218: step 4873, loss 0.036164, acc 1, prec 0.0779263, recall 0.890212
2017-12-10T13:58:33.602853: step 4874, loss 0.424366, acc 0.953125, prec 0.0779393, recall 0.890236
2017-12-10T13:58:33.789277: step 4875, loss 0.00760972, acc 1, prec 0.0779743, recall 0.890283
2017-12-10T13:58:33.981416: step 4876, loss 0.0802546, acc 0.984375, prec 0.0779903, recall 0.890307
2017-12-10T13:58:34.170143: step 4877, loss 0.02469, acc 1, prec 0.0780252, recall 0.890355
2017-12-10T13:58:34.359809: step 4878, loss 0.0750714, acc 0.96875, prec 0.0780222, recall 0.890355
2017-12-10T13:58:34.549174: step 4879, loss 0.0790429, acc 0.96875, prec 0.0780368, recall 0.890378
2017-12-10T13:58:34.742196: step 4880, loss 0.27182, acc 0.953125, prec 0.0780673, recall 0.890426
2017-12-10T13:58:34.933262: step 4881, loss 0.380933, acc 0.9375, prec 0.0780613, recall 0.890426
2017-12-10T13:58:35.126384: step 4882, loss 0.482764, acc 0.96875, prec 0.0780759, recall 0.890449
2017-12-10T13:58:35.317410: step 4883, loss 0.0687964, acc 0.984375, prec 0.0780918, recall 0.890473
2017-12-10T13:58:35.512669: step 4884, loss 0.0301172, acc 0.96875, prec 0.0781064, recall 0.890497
2017-12-10T13:58:35.702056: step 4885, loss 0.0101637, acc 1, prec 0.0781238, recall 0.89052
2017-12-10T13:58:35.892326: step 4886, loss 0.0628512, acc 0.984375, prec 0.0781398, recall 0.890544
2017-12-10T13:58:36.082535: step 4887, loss 0.0552409, acc 0.96875, prec 0.0781543, recall 0.890568
2017-12-10T13:58:36.273034: step 4888, loss 1.62368, acc 0.96875, prec 0.0781877, recall 0.890423
2017-12-10T13:58:36.465717: step 4889, loss 0.305736, acc 0.984375, prec 0.0782386, recall 0.890494
2017-12-10T13:58:36.656439: step 4890, loss 0.0823918, acc 0.984375, prec 0.0782372, recall 0.890494
2017-12-10T13:58:36.849729: step 4891, loss 0.243031, acc 0.984375, prec 0.0782706, recall 0.890541
2017-12-10T13:58:37.036965: step 4892, loss 0.0687784, acc 0.984375, prec 0.078304, recall 0.890588
2017-12-10T13:58:37.225197: step 4893, loss 0.0638441, acc 0.953125, prec 0.0782996, recall 0.890588
2017-12-10T13:58:37.419132: step 4894, loss 0.021558, acc 0.984375, prec 0.0783155, recall 0.890612
2017-12-10T13:58:37.610898: step 4895, loss 0.281495, acc 0.921875, prec 0.0783081, recall 0.890612
2017-12-10T13:58:37.801212: step 4896, loss 0.201025, acc 0.9375, prec 0.0783196, recall 0.890635
2017-12-10T13:58:37.991816: step 4897, loss 0.217583, acc 0.90625, prec 0.0783282, recall 0.890659
2017-12-10T13:58:38.183315: step 4898, loss 0.102188, acc 0.96875, prec 0.0783252, recall 0.890659
2017-12-10T13:58:38.370841: step 4899, loss 0.14028, acc 0.953125, prec 0.0783382, recall 0.890682
2017-12-10T13:58:38.556635: step 4900, loss 0.144479, acc 0.953125, prec 0.0783687, recall 0.890729
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_0/1512931325/checkpoints/model-4900

2017-12-10T13:58:39.878976: step 4901, loss 0.134519, acc 0.953125, prec 0.0783642, recall 0.890729
2017-12-10T13:58:40.065371: step 4902, loss 0.184866, acc 0.9375, prec 0.0783757, recall 0.890753
2017-12-10T13:58:40.259019: step 4903, loss 0.12978, acc 0.953125, prec 0.0783713, recall 0.890753
2017-12-10T13:58:40.448577: step 4904, loss 0.846178, acc 0.984375, prec 0.0784047, recall 0.8908
2017-12-10T13:58:40.643919: step 4905, loss 0.117924, acc 0.984375, prec 0.0784032, recall 0.8908
2017-12-10T13:58:40.833898: step 4906, loss 0.0208634, acc 1, prec 0.0784032, recall 0.8908
2017-12-10T13:58:41.022424: step 4907, loss 0.255537, acc 0.90625, prec 0.0784291, recall 0.890847
2017-12-10T13:58:41.211491: step 4908, loss 0.0243491, acc 0.984375, prec 0.0784451, recall 0.89087
2017-12-10T13:58:41.399446: step 4909, loss 0.174437, acc 0.96875, prec 0.0784421, recall 0.89087
2017-12-10T13:58:41.589473: step 4910, loss 0.127251, acc 0.96875, prec 0.0784392, recall 0.89087
2017-12-10T13:58:41.780446: step 4911, loss 0.130176, acc 0.953125, prec 0.0784696, recall 0.890917
2017-12-10T13:58:41.969305: step 4912, loss 0.0744822, acc 0.984375, prec 0.0784681, recall 0.890917
2017-12-10T13:58:42.161448: step 4913, loss 0.0484354, acc 0.96875, prec 0.0784825, recall 0.89094
2017-12-10T13:58:42.350348: step 4914, loss 0.108584, acc 0.96875, prec 0.078497, recall 0.890964
2017-12-10T13:58:42.538737: step 4915, loss 0.218297, acc 0.9375, prec 0.0784911, recall 0.890964
2017-12-10T13:58:42.728232: step 4916, loss 0.415027, acc 0.90625, prec 0.0784996, recall 0.890987
2017-12-10T13:58:42.920390: step 4917, loss 0.0913536, acc 0.96875, prec 0.078514, recall 0.891011
2017-12-10T13:58:43.114227: step 4918, loss 0.0575838, acc 0.96875, prec 0.0785459, recall 0.891057
2017-12-10T13:58:43.305393: step 4919, loss 0.0112881, acc 1, prec 0.0785459, recall 0.891057
2017-12-10T13:58:43.493319: step 4920, loss 0.27057, acc 0.9375, prec 0.0785574, recall 0.891081
2017-12-10T13:58:43.687329: step 4921, loss 0.105138, acc 0.96875, prec 0.0785544, recall 0.891081
2017-12-10T13:58:43.887190: step 4922, loss 0.0530031, acc 0.984375, prec 0.0785529, recall 0.891081
2017-12-10T13:58:44.076251: step 4923, loss 0.0886143, acc 0.984375, prec 0.0785689, recall 0.891104
2017-12-10T13:58:44.272408: step 4924, loss 0.0773096, acc 0.984375, prec 0.0785674, recall 0.891104
2017-12-10T13:58:44.462303: step 4925, loss 0.392841, acc 0.96875, prec 0.0786166, recall 0.891174
2017-12-10T13:58:44.654753: step 4926, loss 0.216899, acc 0.921875, prec 0.0786266, recall 0.891197
2017-12-10T13:58:44.845706: step 4927, loss 0.0525115, acc 0.96875, prec 0.0786237, recall 0.891197
2017-12-10T13:58:45.037833: step 4928, loss 0.314142, acc 0.9375, prec 0.0786525, recall 0.891244
2017-12-10T13:58:45.228665: step 4929, loss 0.0199257, acc 0.984375, prec 0.0786685, recall 0.891267
2017-12-10T13:58:45.420193: step 4930, loss 0.115882, acc 0.953125, prec 0.0786814, recall 0.89129
2017-12-10T13:58:45.609171: step 4931, loss 0.140769, acc 0.96875, prec 0.0786958, recall 0.891314
2017-12-10T13:58:45.797352: step 4932, loss 0.120326, acc 0.984375, prec 0.0787117, recall 0.891337
2017-12-10T13:58:45.987498: step 4933, loss 0.344873, acc 0.953125, prec 0.0787247, recall 0.89136
2017-12-10T13:58:46.176331: step 4934, loss 0.00394663, acc 1, prec 0.0787595, recall 0.891407
2017-12-10T13:58:46.365368: step 4935, loss 0.0197183, acc 0.984375, prec 0.078758, recall 0.891407
2017-12-10T13:58:46.554991: step 4936, loss 0.0910446, acc 0.953125, prec 0.0787535, recall 0.891407
2017-12-10T13:58:46.746634: step 4937, loss 0.0968027, acc 0.984375, prec 0.0787695, recall 0.89143
2017-12-10T13:58:46.938312: step 4938, loss 0.35482, acc 0.90625, prec 0.0787953, recall 0.891476
2017-12-10T13:58:47.126934: step 4939, loss 0.0381659, acc 0.984375, prec 0.078846, recall 0.891546
2017-12-10T13:58:47.319801: step 4940, loss 0.00673208, acc 1, prec 0.0788634, recall 0.891569
2017-12-10T13:58:47.507530: step 4941, loss 0.185467, acc 0.984375, prec 0.0789141, recall 0.891638
2017-12-10T13:58:47.703504: step 4942, loss 0.0187695, acc 0.984375, prec 0.0789126, recall 0.891638
2017-12-10T13:58:47.896579: step 4943, loss 1.16289, acc 0.96875, prec 0.0789285, recall 0.891471
2017-12-10T13:58:48.089600: step 4944, loss 0.0339515, acc 0.984375, prec 0.078927, recall 0.891471
2017-12-10T13:58:48.282258: step 4945, loss 0.243154, acc 0.96875, prec 0.078924, recall 0.891471
2017-12-10T13:58:48.476049: step 4946, loss 0.0130605, acc 0.984375, prec 0.0789225, recall 0.891471
2017-12-10T13:58:48.664508: step 4947, loss 0.167819, acc 0.953125, prec 0.0789354, recall 0.891494
2017-12-10T13:58:48.855351: step 4948, loss 0.266023, acc 0.9375, prec 0.0789295, recall 0.891494
2017-12-10T13:58:49.046342: step 4949, loss 0.455208, acc 0.890625, prec 0.0789191, recall 0.891494
2017-12-10T13:58:49.240216: step 4950, loss 0.0400365, acc 0.984375, prec 0.078935, recall 0.891517
2017-12-10T13:58:49.436776: step 4951, loss 0.0125055, acc 0.984375, prec 0.0789335, recall 0.891517
2017-12-10T13:58:49.627179: step 4952, loss 0.414682, acc 0.953125, prec 0.078929, recall 0.891517
2017-12-10T13:58:49.816456: step 4953, loss 0.27698, acc 0.9375, prec 0.078923, recall 0.891517
2017-12-10T13:58:50.002789: step 4954, loss 0.300642, acc 0.953125, prec 0.0789186, recall 0.891517
2017-12-10T13:58:50.196443: step 4955, loss 0.131592, acc 0.9375, prec 0.07893, recall 0.891541
2017-12-10T13:58:50.389061: step 4956, loss 0.11369, acc 0.96875, prec 0.0789444, recall 0.891564
2017-12-10T13:58:50.580729: step 4957, loss 0.176924, acc 0.953125, prec 0.0789399, recall 0.891564
2017-12-10T13:58:50.779789: step 4958, loss 2.13568, acc 0.953125, prec 0.0789543, recall 0.891397
2017-12-10T13:58:50.972070: step 4959, loss 0.158479, acc 0.96875, prec 0.0789687, recall 0.89142
2017-12-10T13:58:51.160845: step 4960, loss 0.417955, acc 0.90625, prec 0.0789771, recall 0.891443
2017-12-10T13:58:51.347819: step 4961, loss 0.390787, acc 0.9375, prec 0.0789886, recall 0.891466
2017-12-10T13:58:51.538967: step 4962, loss 0.219036, acc 0.96875, prec 0.0789856, recall 0.891466
2017-12-10T13:58:51.727425: step 4963, loss 0.10957, acc 0.953125, prec 0.0789811, recall 0.891466
2017-12-10T13:58:51.919946: step 4964, loss 0.760123, acc 0.90625, prec 0.0789895, recall 0.891489
2017-12-10T13:58:52.111370: step 4965, loss 0.151151, acc 0.9375, prec 0.0789836, recall 0.891489
2017-12-10T13:58:52.303878: step 4966, loss 0.0475413, acc 0.96875, prec 0.078998, recall 0.891512
2017-12-10T13:58:52.500980: step 4967, loss 0.113754, acc 0.96875, prec 0.0790123, recall 0.891536
2017-12-10T13:58:52.688085: step 4968, loss 0.424426, acc 0.90625, prec 0.0790034, recall 0.891536
2017-12-10T13:58:52.878115: step 4969, loss 0.22793, acc 0.921875, prec 0.0790133, recall 0.891559
2017-12-10T13:58:53.049462: step 4970, loss 0.287801, acc 0.921569, prec 0.0790247, recall 0.891582
Training finished
Starting Fold: 1 => Train/Dev split: 31795/10599


 #################### 


SEQUENCE LENGTH 273
BATCH SIZE 64
EMBEDDING SIZE 128
FILTER SIZES [3, 4, 5]
NUMBER OF FILTERS 128
L2 REG LAMBDA 0.0
EPOCHS 10



RESULT DIR leave_position_embedding_out_fold_1
VOCAB SIZE 33447
DROPOUT PROBABILITY 0.5



WORD EMBEDDING True
POSITION EMBEDDING False
POS EMBEDDING True


 #################### 


Writing to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_1/1512932334

Start training
2017-12-10T13:58:56.890580: step 1, loss 2.28677, acc 0.703125, prec 0, recall 0
2017-12-10T13:58:57.076613: step 2, loss 10.0631, acc 0.65625, prec 0, recall 0
2017-12-10T13:58:57.269688: step 3, loss 1.55478, acc 0.671875, prec 0.0163934, recall 0.333333
2017-12-10T13:58:57.458693: step 4, loss 2.55955, acc 0.671875, prec 0.0123457, recall 0.25
2017-12-10T13:58:57.648887: step 5, loss 1.14379, acc 0.6875, prec 0.00990099, recall 0.25
2017-12-10T13:58:57.833180: step 6, loss 1.6876, acc 0.484375, prec 0.00746269, recall 0.25
2017-12-10T13:58:58.017826: step 7, loss 2.34454, acc 0.5625, prec 0.0182927, recall 0.5
2017-12-10T13:58:58.203560: step 8, loss 1.70169, acc 0.546875, prec 0.025641, recall 0.625
2017-12-10T13:58:58.385196: step 9, loss 11.8764, acc 0.671875, prec 0.027907, recall 0.545455
2017-12-10T13:58:58.572436: step 10, loss 1.79606, acc 0.59375, prec 0.0248963, recall 0.545455
2017-12-10T13:58:58.764911: step 11, loss 2.09724, acc 0.515625, prec 0.0220588, recall 0.545455
2017-12-10T13:58:58.953999: step 12, loss 2.55093, acc 0.40625, prec 0.0193548, recall 0.545455
2017-12-10T13:58:59.140405: step 13, loss 3.78115, acc 0.5, prec 0.0261628, recall 0.6
2017-12-10T13:58:59.327699: step 14, loss 3.06167, acc 0.390625, prec 0.0234987, recall 0.6
2017-12-10T13:58:59.512360: step 15, loss 1.42186, acc 0.578125, prec 0.0219512, recall 0.6
2017-12-10T13:58:59.696795: step 16, loss 2.23742, acc 0.515625, prec 0.0248307, recall 0.647059
2017-12-10T13:58:59.884261: step 17, loss 3.27293, acc 0.5, prec 0.0252101, recall 0.666667
2017-12-10T13:59:00.072692: step 18, loss 2.10698, acc 0.5625, prec 0.027668, recall 0.7
2017-12-10T13:59:00.257893: step 19, loss 1.33105, acc 0.625, prec 0.0300752, recall 0.727273
2017-12-10T13:59:00.441987: step 20, loss 3.47949, acc 0.484375, prec 0.0300353, recall 0.73913
2017-12-10T13:59:00.627185: step 21, loss 4.16021, acc 0.578125, prec 0.0287162, recall 0.708333
2017-12-10T13:59:00.812176: step 22, loss 3.65545, acc 0.703125, prec 0.0278689, recall 0.68
2017-12-10T13:59:01.006902: step 23, loss 2.01169, acc 0.53125, prec 0.0265625, recall 0.68
2017-12-10T13:59:01.193583: step 24, loss 5.47073, acc 0.609375, prec 0.0270677, recall 0.666667
2017-12-10T13:59:01.385141: step 25, loss 14.9212, acc 0.515625, prec 0.0258993, recall 0.642857
2017-12-10T13:59:01.571721: step 26, loss 6.86787, acc 0.390625, prec 0.0245566, recall 0.62069
2017-12-10T13:59:01.759929: step 27, loss 3.19488, acc 0.390625, prec 0.0245796, recall 0.633333
2017-12-10T13:59:01.945118: step 28, loss 13.7541, acc 0.328125, prec 0.0233129, recall 0.612903
2017-12-10T13:59:02.134056: step 29, loss 9.68701, acc 0.390625, prec 0.0234192, recall 0.606061
2017-12-10T13:59:02.319765: step 30, loss 15.1225, acc 0.375, prec 0.0234899, recall 0.6
2017-12-10T13:59:02.509506: step 31, loss 4.16172, acc 0.28125, prec 0.0223404, recall 0.6
2017-12-10T13:59:02.697824: step 32, loss 5.71693, acc 0.21875, prec 0.0221998, recall 0.611111
2017-12-10T13:59:02.879519: step 33, loss 4.30514, acc 0.296875, prec 0.0212355, recall 0.611111
2017-12-10T13:59:03.067240: step 34, loss 5.92974, acc 0.15625, prec 0.0228728, recall 0.641026
2017-12-10T13:59:03.253942: step 35, loss 7.08418, acc 0.078125, prec 0.0217014, recall 0.641026
2017-12-10T13:59:03.438546: step 36, loss 4.84882, acc 0.1875, prec 0.0215768, recall 0.65
2017-12-10T13:59:03.623991: step 37, loss 5.36379, acc 0.296875, prec 0.0239234, recall 0.681818
2017-12-10T13:59:03.809752: step 38, loss 4.25628, acc 0.328125, prec 0.0231303, recall 0.681818
2017-12-10T13:59:03.992673: step 39, loss 5.10434, acc 0.28125, prec 0.0245171, recall 0.702128
2017-12-10T13:59:04.175063: step 40, loss 4.10064, acc 0.328125, prec 0.0258621, recall 0.72
2017-12-10T13:59:04.363569: step 41, loss 3.63757, acc 0.390625, prec 0.025838, recall 0.72549
2017-12-10T13:59:04.551206: step 42, loss 2.96062, acc 0.34375, prec 0.0257627, recall 0.730769
2017-12-10T13:59:04.738047: step 43, loss 2.93884, acc 0.40625, prec 0.0251157, recall 0.730769
2017-12-10T13:59:04.924185: step 44, loss 2.44677, acc 0.5, prec 0.0252264, recall 0.735849
2017-12-10T13:59:05.113879: step 45, loss 2.29778, acc 0.484375, prec 0.0253165, recall 0.740741
2017-12-10T13:59:05.298147: step 46, loss 3.66807, acc 0.578125, prec 0.0249066, recall 0.727273
2017-12-10T13:59:05.482644: step 47, loss 3.96227, acc 0.640625, prec 0.02457, recall 0.714286
2017-12-10T13:59:05.670775: step 48, loss 20.4519, acc 0.71875, prec 0.0243309, recall 0.689655
2017-12-10T13:59:05.858472: step 49, loss 3.03268, acc 0.609375, prec 0.0239808, recall 0.677966
2017-12-10T13:59:06.052177: step 50, loss 12.6996, acc 0.65625, prec 0.0236827, recall 0.666667
2017-12-10T13:59:06.239477: step 51, loss 1.34605, acc 0.59375, prec 0.0233236, recall 0.666667
2017-12-10T13:59:06.426044: step 52, loss 1.8429, acc 0.59375, prec 0.0235362, recall 0.672131
2017-12-10T13:59:06.615880: step 53, loss 1.96346, acc 0.578125, prec 0.0231769, recall 0.672131
2017-12-10T13:59:06.809380: step 54, loss 2.86767, acc 0.421875, prec 0.0227021, recall 0.672131
2017-12-10T13:59:06.992383: step 55, loss 2.05272, acc 0.515625, prec 0.022319, recall 0.672131
2017-12-10T13:59:07.179122: step 56, loss 2.68333, acc 0.5, prec 0.0224599, recall 0.677419
2017-12-10T13:59:07.371591: step 57, loss 1.65462, acc 0.65625, prec 0.0221987, recall 0.677419
2017-12-10T13:59:07.559729: step 58, loss 2.07916, acc 0.5625, prec 0.021875, recall 0.677419
2017-12-10T13:59:07.747323: step 59, loss 1.76065, acc 0.578125, prec 0.0225757, recall 0.6875
2017-12-10T13:59:07.935068: step 60, loss 1.57874, acc 0.671875, prec 0.022335, recall 0.6875
2017-12-10T13:59:08.124508: step 61, loss 1.2285, acc 0.671875, prec 0.0220994, recall 0.6875
2017-12-10T13:59:08.306753: step 62, loss 2.26835, acc 0.546875, prec 0.0217822, recall 0.6875
2017-12-10T13:59:08.495262: step 63, loss 1.38538, acc 0.6875, prec 0.0215686, recall 0.6875
2017-12-10T13:59:08.685269: step 64, loss 19.9732, acc 0.578125, prec 0.0212972, recall 0.676923
2017-12-10T13:59:08.873863: step 65, loss 1.13564, acc 0.703125, prec 0.0211031, recall 0.676923
2017-12-10T13:59:09.060902: step 66, loss 29.5152, acc 0.71875, prec 0.0209524, recall 0.647059
2017-12-10T13:59:09.254362: step 67, loss 1.60182, acc 0.640625, prec 0.0207254, recall 0.647059
2017-12-10T13:59:09.444046: step 68, loss 12.6122, acc 0.78125, prec 0.0219729, recall 0.652778
2017-12-10T13:59:09.635538: step 69, loss 2.23559, acc 0.53125, prec 0.021669, recall 0.652778
2017-12-10T13:59:09.823611: step 70, loss 0.907986, acc 0.703125, prec 0.0219278, recall 0.657534
2017-12-10T13:59:10.008747: step 71, loss 1.94249, acc 0.5, prec 0.0216119, recall 0.657534
2017-12-10T13:59:10.196531: step 72, loss 2.93948, acc 0.40625, prec 0.0212483, recall 0.657534
2017-12-10T13:59:10.380560: step 73, loss 2.94957, acc 0.4375, prec 0.0213415, recall 0.662162
2017-12-10T13:59:10.567559: step 74, loss 4.1351, acc 0.578125, prec 0.0215239, recall 0.657895
2017-12-10T13:59:10.757773: step 75, loss 3.07573, acc 0.46875, prec 0.0220432, recall 0.666667
2017-12-10T13:59:10.946736: step 76, loss 2.61805, acc 0.4375, prec 0.0217119, recall 0.666667
2017-12-10T13:59:11.131350: step 77, loss 2.57914, acc 0.46875, prec 0.021408, recall 0.666667
2017-12-10T13:59:11.313709: step 78, loss 2.23103, acc 0.484375, prec 0.0215185, recall 0.670886
2017-12-10T13:59:11.500435: step 79, loss 2.50413, acc 0.484375, prec 0.0220176, recall 0.679012
2017-12-10T13:59:11.684911: step 80, loss 2.71619, acc 0.46875, prec 0.0224941, recall 0.686747
2017-12-10T13:59:11.876062: step 81, loss 3.04686, acc 0.609375, prec 0.022283, recall 0.678571
2017-12-10T13:59:12.062488: step 82, loss 1.87942, acc 0.625, prec 0.0220759, recall 0.678571
2017-12-10T13:59:12.245877: step 83, loss 2.5832, acc 0.515625, prec 0.0225621, recall 0.686047
2017-12-10T13:59:12.439491: step 84, loss 2.0961, acc 0.546875, prec 0.0223147, recall 0.686047
2017-12-10T13:59:12.625334: step 85, loss 1.46363, acc 0.703125, prec 0.0221555, recall 0.686047
2017-12-10T13:59:12.810673: step 86, loss 2.12589, acc 0.546875, prec 0.02228, recall 0.689655
2017-12-10T13:59:12.998591: step 87, loss 1.44282, acc 0.671875, prec 0.0221076, recall 0.689655
2017-12-10T13:59:13.184366: step 88, loss 4.17627, acc 0.625, prec 0.0229927, recall 0.692308
2017-12-10T13:59:13.373411: step 89, loss 16.7697, acc 0.71875, prec 0.0232052, recall 0.688172
2017-12-10T13:59:13.563295: step 90, loss 7.35449, acc 0.671875, prec 0.0230465, recall 0.673684
2017-12-10T13:59:13.747863: step 91, loss 1.24047, acc 0.6875, prec 0.0232309, recall 0.677083
2017-12-10T13:59:13.940666: step 92, loss 4.13497, acc 0.65625, prec 0.0237504, recall 0.676768
2017-12-10T13:59:14.127524: step 93, loss 1.80916, acc 0.59375, prec 0.0235335, recall 0.676768
2017-12-10T13:59:14.314906: step 94, loss 2.2932, acc 0.53125, prec 0.0239667, recall 0.683168
2017-12-10T13:59:14.504925: step 95, loss 1.12172, acc 0.75, prec 0.0241713, recall 0.686275
2017-12-10T13:59:14.690928: step 96, loss 1.85271, acc 0.53125, prec 0.0239234, recall 0.686275
2017-12-10T13:59:14.877053: step 97, loss 4.05259, acc 0.53125, prec 0.024349, recall 0.685714
2017-12-10T13:59:15.064142: step 98, loss 2.04166, acc 0.484375, prec 0.0244066, recall 0.688679
2017-12-10T13:59:15.250073: step 99, loss 2.84265, acc 0.40625, prec 0.0241004, recall 0.688679
2017-12-10T13:59:15.437284: step 100, loss 2.86386, acc 0.46875, prec 0.0238328, recall 0.688679
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_1/1512932334/checkpoints/model-100

2017-12-10T13:59:16.703562: step 101, loss 2.44246, acc 0.53125, prec 0.0236017, recall 0.688679
2017-12-10T13:59:16.889910: step 102, loss 13.1618, acc 0.40625, prec 0.0236422, recall 0.678899
2017-12-10T13:59:17.081371: step 103, loss 2.88579, acc 0.46875, prec 0.0240051, recall 0.684685
2017-12-10T13:59:17.266801: step 104, loss 2.49152, acc 0.5, prec 0.024375, recall 0.690265
2017-12-10T13:59:17.455641: step 105, loss 2.23385, acc 0.546875, prec 0.0250619, recall 0.698276
2017-12-10T13:59:17.642285: step 106, loss 2.62283, acc 0.40625, prec 0.0247706, recall 0.698276
2017-12-10T13:59:17.829324: step 107, loss 3.17615, acc 0.421875, prec 0.0247884, recall 0.700855
2017-12-10T13:59:18.016998: step 108, loss 2.58532, acc 0.421875, prec 0.0248057, recall 0.70339
2017-12-10T13:59:18.204400: step 109, loss 2.86967, acc 0.4375, prec 0.025694, recall 0.713115
2017-12-10T13:59:18.391542: step 110, loss 1.42631, acc 0.625, prec 0.0255132, recall 0.713115
2017-12-10T13:59:18.577659: step 111, loss 5.15228, acc 0.5625, prec 0.0253128, recall 0.707317
2017-12-10T13:59:18.769980: step 112, loss 2.34303, acc 0.53125, prec 0.0250937, recall 0.707317
2017-12-10T13:59:18.958002: step 113, loss 2.866, acc 0.515625, prec 0.02515, recall 0.709677
2017-12-10T13:59:19.144042: step 114, loss 1.77278, acc 0.59375, prec 0.0252411, recall 0.712
2017-12-10T13:59:19.325641: step 115, loss 1.42629, acc 0.640625, prec 0.0250775, recall 0.712
2017-12-10T13:59:19.512061: step 116, loss 1.55932, acc 0.640625, prec 0.024916, recall 0.712
2017-12-10T13:59:19.697055: step 117, loss 7.44384, acc 0.609375, prec 0.0250209, recall 0.708661
2017-12-10T13:59:19.883431: step 118, loss 1.85259, acc 0.625, prec 0.024855, recall 0.708661
2017-12-10T13:59:20.072278: step 119, loss 4.58807, acc 0.71875, prec 0.0247389, recall 0.703125
2017-12-10T13:59:20.264361: step 120, loss 1.42223, acc 0.734375, prec 0.0248906, recall 0.705426
2017-12-10T13:59:20.455393: step 121, loss 2.13063, acc 0.75, prec 0.025313, recall 0.709924
2017-12-10T13:59:20.645749: step 122, loss 1.90358, acc 0.6875, prec 0.0254398, recall 0.712121
2017-12-10T13:59:20.837773: step 123, loss 1.11209, acc 0.703125, prec 0.0253096, recall 0.712121
2017-12-10T13:59:21.027247: step 124, loss 2.86077, acc 0.671875, prec 0.0254351, recall 0.708955
2017-12-10T13:59:21.222729: step 125, loss 2.02444, acc 0.671875, prec 0.0255523, recall 0.711111
2017-12-10T13:59:21.409327: step 126, loss 1.21012, acc 0.734375, prec 0.0256954, recall 0.713235
2017-12-10T13:59:21.595787: step 127, loss 2.43826, acc 0.75, prec 0.0258507, recall 0.710145
2017-12-10T13:59:21.783854: step 128, loss 2.09443, acc 0.53125, prec 0.0256477, recall 0.710145
2017-12-10T13:59:21.970154: step 129, loss 1.96947, acc 0.59375, prec 0.0254744, recall 0.710145
2017-12-10T13:59:22.158973: step 130, loss 10.3203, acc 0.5625, prec 0.0257998, recall 0.70922
2017-12-10T13:59:22.351056: step 131, loss 2.20684, acc 0.546875, prec 0.0256082, recall 0.70922
2017-12-10T13:59:22.535825: step 132, loss 2.97852, acc 0.4375, prec 0.0258686, recall 0.713287
2017-12-10T13:59:22.723197: step 133, loss 1.78804, acc 0.609375, prec 0.0257056, recall 0.713287
2017-12-10T13:59:22.907473: step 134, loss 2.08957, acc 0.578125, prec 0.0255319, recall 0.713287
2017-12-10T13:59:23.093441: step 135, loss 2.21947, acc 0.5625, prec 0.0255964, recall 0.715278
2017-12-10T13:59:23.281329: step 136, loss 1.23133, acc 0.671875, prec 0.0259451, recall 0.719178
2017-12-10T13:59:23.467682: step 137, loss 2.31707, acc 0.53125, prec 0.0262319, recall 0.722973
2017-12-10T13:59:23.656627: step 138, loss 2.00616, acc 0.609375, prec 0.0260721, recall 0.722973
2017-12-10T13:59:23.846093: step 139, loss 1.55169, acc 0.625, prec 0.0261565, recall 0.724832
2017-12-10T13:59:24.035019: step 140, loss 3.36016, acc 0.671875, prec 0.0264996, recall 0.723684
2017-12-10T13:59:24.219993: step 141, loss 1.66767, acc 0.625, prec 0.0265805, recall 0.72549
2017-12-10T13:59:24.410844: step 142, loss 1.26587, acc 0.765625, prec 0.0267176, recall 0.727273
2017-12-10T13:59:24.602145: step 143, loss 0.746265, acc 0.71875, prec 0.0266033, recall 0.727273
2017-12-10T13:59:24.793091: step 144, loss 1.3107, acc 0.625, prec 0.0264525, recall 0.727273
2017-12-10T13:59:24.989609: step 145, loss 17.4215, acc 0.625, prec 0.0269953, recall 0.727848
2017-12-10T13:59:25.178846: step 146, loss 2.25848, acc 0.578125, prec 0.0272791, recall 0.73125
2017-12-10T13:59:25.367553: step 147, loss 1.76333, acc 0.640625, prec 0.0271336, recall 0.73125
2017-12-10T13:59:25.555486: step 148, loss 1.74857, acc 0.609375, prec 0.0272015, recall 0.732919
2017-12-10T13:59:25.749295: step 149, loss 20.4804, acc 0.625, prec 0.0272936, recall 0.721212
2017-12-10T13:59:25.939137: step 150, loss 1.29263, acc 0.65625, prec 0.0271566, recall 0.721212
2017-12-10T13:59:26.130629: step 151, loss 11.1646, acc 0.609375, prec 0.0270086, recall 0.716867
2017-12-10T13:59:26.320676: step 152, loss 2.59198, acc 0.46875, prec 0.02724, recall 0.720238
2017-12-10T13:59:26.510229: step 153, loss 2.40201, acc 0.46875, prec 0.0270331, recall 0.720238
2017-12-10T13:59:26.700991: step 154, loss 3.10167, acc 0.453125, prec 0.027039, recall 0.721893
2017-12-10T13:59:26.886455: step 155, loss 3.04946, acc 0.40625, prec 0.027027, recall 0.723529
2017-12-10T13:59:27.075803: step 156, loss 2.6351, acc 0.46875, prec 0.0270388, recall 0.725146
2017-12-10T13:59:27.263607: step 157, loss 2.72464, acc 0.5, prec 0.0268515, recall 0.725146
2017-12-10T13:59:27.450502: step 158, loss 4.2343, acc 0.40625, prec 0.0266323, recall 0.725146
2017-12-10T13:59:27.633785: step 159, loss 3.2371, acc 0.328125, prec 0.0263886, recall 0.725146
2017-12-10T13:59:27.816529: step 160, loss 4.38561, acc 0.40625, prec 0.0261824, recall 0.72093
2017-12-10T13:59:28.006318: step 161, loss 2.6969, acc 0.46875, prec 0.0262, recall 0.722543
2017-12-10T13:59:28.195406: step 162, loss 3.05896, acc 0.390625, prec 0.0259875, recall 0.722543
2017-12-10T13:59:28.382599: step 163, loss 3.12688, acc 0.390625, prec 0.0257785, recall 0.722543
2017-12-10T13:59:28.567525: step 164, loss 23.5225, acc 0.5625, prec 0.0258356, recall 0.72
2017-12-10T13:59:28.756765: step 165, loss 1.96706, acc 0.515625, prec 0.0258709, recall 0.721591
2017-12-10T13:59:28.949466: step 166, loss 13.9046, acc 0.515625, prec 0.0257137, recall 0.717514
2017-12-10T13:59:29.134987: step 167, loss 2.99795, acc 0.484375, prec 0.025543, recall 0.717514
2017-12-10T13:59:29.319266: step 168, loss 7.29519, acc 0.5625, prec 0.0256, recall 0.715084
2017-12-10T13:59:29.508601: step 169, loss 7.04467, acc 0.546875, prec 0.0254574, recall 0.711111
2017-12-10T13:59:29.699297: step 170, loss 13.0908, acc 0.515625, prec 0.0253064, recall 0.707182
2017-12-10T13:59:29.883556: step 171, loss 2.6994, acc 0.484375, prec 0.0253339, recall 0.708791
2017-12-10T13:59:30.073247: step 172, loss 3.07681, acc 0.484375, prec 0.025741, recall 0.713513
2017-12-10T13:59:30.259966: step 173, loss 2.31521, acc 0.53125, prec 0.0255913, recall 0.713513
2017-12-10T13:59:30.445701: step 174, loss 3.0882, acc 0.421875, prec 0.0255966, recall 0.715054
2017-12-10T13:59:30.631472: step 175, loss 2.74931, acc 0.5, prec 0.0254399, recall 0.715054
2017-12-10T13:59:30.815821: step 176, loss 5.63335, acc 0.421875, prec 0.0254511, recall 0.712766
2017-12-10T13:59:31.007436: step 177, loss 2.4482, acc 0.5, prec 0.0254813, recall 0.714286
2017-12-10T13:59:31.197244: step 178, loss 5.3688, acc 0.390625, prec 0.0252999, recall 0.710526
2017-12-10T13:59:31.387483: step 179, loss 3.67742, acc 0.34375, prec 0.0251023, recall 0.710526
2017-12-10T13:59:31.575826: step 180, loss 4.09399, acc 0.4375, prec 0.0252954, recall 0.713542
2017-12-10T13:59:31.761794: step 181, loss 13.655, acc 0.34375, prec 0.025284, recall 0.71134
2017-12-10T13:59:31.949362: step 182, loss 2.99711, acc 0.328125, prec 0.0250863, recall 0.71134
2017-12-10T13:59:32.134525: step 183, loss 4.40438, acc 0.28125, prec 0.0248783, recall 0.71134
2017-12-10T13:59:32.320629: step 184, loss 3.80835, acc 0.296875, prec 0.0246781, recall 0.71134
2017-12-10T13:59:32.508000: step 185, loss 1.98686, acc 0.515625, prec 0.0245421, recall 0.71134
2017-12-10T13:59:32.694108: step 186, loss 2.66629, acc 0.484375, prec 0.0243989, recall 0.71134
2017-12-10T13:59:32.883970: step 187, loss 3.45211, acc 0.421875, prec 0.0242403, recall 0.71134
2017-12-10T13:59:33.070232: step 188, loss 1.9332, acc 0.53125, prec 0.0242837, recall 0.712821
2017-12-10T13:59:33.260061: step 189, loss 12.8759, acc 0.546875, prec 0.0241655, recall 0.709184
2017-12-10T13:59:33.447441: step 190, loss 1.85288, acc 0.546875, prec 0.0242131, recall 0.71066
2017-12-10T13:59:33.632839: step 191, loss 2.59258, acc 0.53125, prec 0.0240881, recall 0.71066
2017-12-10T13:59:33.823405: step 192, loss 1.8222, acc 0.609375, prec 0.0241521, recall 0.712121
2017-12-10T13:59:34.011263: step 193, loss 8.43794, acc 0.6875, prec 0.0240738, recall 0.708543
2017-12-10T13:59:34.200786: step 194, loss 1.10253, acc 0.703125, prec 0.0239959, recall 0.708543
2017-12-10T13:59:34.384881: step 195, loss 1.2711, acc 0.65625, prec 0.0239064, recall 0.708543
2017-12-10T13:59:34.574000: step 196, loss 10.5461, acc 0.765625, prec 0.0240149, recall 0.706468
2017-12-10T13:59:34.764169: step 197, loss 1.47934, acc 0.6875, prec 0.0239339, recall 0.706468
2017-12-10T13:59:34.955589: step 198, loss 1.43779, acc 0.5625, prec 0.0239852, recall 0.707921
2017-12-10T13:59:35.144306: step 199, loss 5.60505, acc 0.65625, prec 0.0239011, recall 0.704434
2017-12-10T13:59:35.333608: step 200, loss 1.473, acc 0.6875, prec 0.023984, recall 0.705882
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_1/1512932334/checkpoints/model-200

2017-12-10T13:59:36.549465: step 201, loss 2.05197, acc 0.625, prec 0.0240504, recall 0.707317
2017-12-10T13:59:36.740618: step 202, loss 1.18783, acc 0.625, prec 0.0239551, recall 0.707317
2017-12-10T13:59:36.933883: step 203, loss 1.72813, acc 0.65625, prec 0.024029, recall 0.708738
2017-12-10T13:59:37.126301: step 204, loss 1.71065, acc 0.703125, prec 0.0244342, recall 0.712919
2017-12-10T13:59:37.317455: step 205, loss 1.3344, acc 0.671875, prec 0.0246692, recall 0.71564
2017-12-10T13:59:37.502099: step 206, loss 1.34466, acc 0.6875, prec 0.0245888, recall 0.71564
2017-12-10T13:59:37.688233: step 207, loss 9.64373, acc 0.53125, prec 0.0244733, recall 0.712264
2017-12-10T13:59:37.877144: step 208, loss 2.90398, acc 0.703125, prec 0.0245597, recall 0.71028
2017-12-10T13:59:38.072088: step 209, loss 1.40189, acc 0.75, prec 0.0246536, recall 0.711628
2017-12-10T13:59:38.259590: step 210, loss 1.28181, acc 0.734375, prec 0.0245862, recall 0.711628
2017-12-10T13:59:38.449423: step 211, loss 1.81365, acc 0.609375, prec 0.0244878, recall 0.711628
2017-12-10T13:59:38.637068: step 212, loss 1.0743, acc 0.75, prec 0.0244253, recall 0.711628
2017-12-10T13:59:38.820267: step 213, loss 1.64676, acc 0.53125, prec 0.0243089, recall 0.711628
2017-12-10T13:59:39.009421: step 214, loss 1.192, acc 0.703125, prec 0.0242357, recall 0.711628
2017-12-10T13:59:39.203067: step 215, loss 1.72321, acc 0.578125, prec 0.0242864, recall 0.712963
2017-12-10T13:59:39.386036: step 216, loss 0.916222, acc 0.78125, prec 0.0243864, recall 0.714286
2017-12-10T13:59:39.576785: step 217, loss 1.10288, acc 0.65625, prec 0.0243023, recall 0.714286
2017-12-10T13:59:39.766302: step 218, loss 1.66077, acc 0.65625, prec 0.0243712, recall 0.715596
2017-12-10T13:59:39.953584: step 219, loss 0.716396, acc 0.828125, prec 0.0243294, recall 0.715596
2017-12-10T13:59:40.144769: step 220, loss 0.889513, acc 0.734375, prec 0.024265, recall 0.715596
2017-12-10T13:59:40.333174: step 221, loss 0.676156, acc 0.796875, prec 0.0242161, recall 0.715596
2017-12-10T13:59:40.522606: step 222, loss 1.54449, acc 0.765625, prec 0.0243109, recall 0.716895
2017-12-10T13:59:40.710418: step 223, loss 0.579022, acc 0.84375, prec 0.0244242, recall 0.718182
2017-12-10T13:59:40.900583: step 224, loss 0.710582, acc 0.828125, prec 0.0243827, recall 0.718182
2017-12-10T13:59:41.091468: step 225, loss 3.08912, acc 0.859375, prec 0.0243527, recall 0.714932
2017-12-10T13:59:41.288386: step 226, loss 16.0258, acc 0.875, prec 0.0243302, recall 0.70852
2017-12-10T13:59:41.480598: step 227, loss 0.751322, acc 0.765625, prec 0.0242741, recall 0.70852
2017-12-10T13:59:41.671561: step 228, loss 15.5187, acc 0.859375, prec 0.0242443, recall 0.705357
2017-12-10T13:59:41.859885: step 229, loss 0.80564, acc 0.765625, prec 0.0241886, recall 0.705357
2017-12-10T13:59:42.050313: step 230, loss 1.15011, acc 0.703125, prec 0.0241185, recall 0.705357
2017-12-10T13:59:42.239744: step 231, loss 1.23999, acc 0.703125, prec 0.0240487, recall 0.705357
2017-12-10T13:59:42.427245: step 232, loss 1.06849, acc 0.6875, prec 0.0241238, recall 0.706667
2017-12-10T13:59:42.618422: step 233, loss 8.69043, acc 0.671875, prec 0.0240508, recall 0.70354
2017-12-10T13:59:42.808616: step 234, loss 2.32379, acc 0.5625, prec 0.0242433, recall 0.70614
2017-12-10T13:59:42.997628: step 235, loss 1.55935, acc 0.6875, prec 0.024317, recall 0.707424
2017-12-10T13:59:43.184766: step 236, loss 1.27295, acc 0.65625, prec 0.024237, recall 0.707424
2017-12-10T13:59:43.369238: step 237, loss 1.74675, acc 0.671875, prec 0.0243066, recall 0.708696
2017-12-10T13:59:43.559800: step 238, loss 1.49814, acc 0.625, prec 0.0243649, recall 0.709957
2017-12-10T13:59:43.750351: step 239, loss 2.58633, acc 0.515625, prec 0.0242532, recall 0.709957
2017-12-10T13:59:43.942875: step 240, loss 2.45843, acc 0.484375, prec 0.0241354, recall 0.709957
2017-12-10T13:59:44.131098: step 241, loss 2.03154, acc 0.515625, prec 0.0240258, recall 0.709957
2017-12-10T13:59:44.321731: step 242, loss 2.03129, acc 0.5625, prec 0.0239276, recall 0.709957
2017-12-10T13:59:44.510047: step 243, loss 3.44535, acc 0.59375, prec 0.0239826, recall 0.708154
2017-12-10T13:59:44.705214: step 244, loss 1.56589, acc 0.578125, prec 0.0241714, recall 0.710638
2017-12-10T13:59:44.895996: step 245, loss 1.9231, acc 0.53125, prec 0.0240669, recall 0.710638
2017-12-10T13:59:45.085627: step 246, loss 5.77112, acc 0.640625, prec 0.024131, recall 0.708861
2017-12-10T13:59:45.274678: step 247, loss 1.48837, acc 0.71875, prec 0.0242086, recall 0.710084
2017-12-10T13:59:45.461544: step 248, loss 1.4636, acc 0.6875, prec 0.0241394, recall 0.710084
2017-12-10T13:59:45.647285: step 249, loss 1.50454, acc 0.625, prec 0.0240569, recall 0.710084
2017-12-10T13:59:45.833791: step 250, loss 1.56611, acc 0.640625, prec 0.0239784, recall 0.710084
2017-12-10T13:59:46.018601: step 251, loss 1.53768, acc 0.65625, prec 0.0239038, recall 0.710084
2017-12-10T13:59:46.209641: step 252, loss 1.37022, acc 0.65625, prec 0.0239673, recall 0.711297
2017-12-10T13:59:46.402872: step 253, loss 2.02008, acc 0.6875, prec 0.0240371, recall 0.7125
2017-12-10T13:59:46.590001: step 254, loss 5.45258, acc 0.765625, prec 0.0239899, recall 0.709544
2017-12-10T13:59:46.780533: step 255, loss 7.3352, acc 0.625, prec 0.0240492, recall 0.707819
2017-12-10T13:59:46.968078: step 256, loss 1.3375, acc 0.71875, prec 0.0239888, recall 0.707819
2017-12-10T13:59:47.150224: step 257, loss 1.68479, acc 0.6875, prec 0.0241935, recall 0.710204
2017-12-10T13:59:47.339922: step 258, loss 3.41816, acc 0.640625, prec 0.0241198, recall 0.707317
2017-12-10T13:59:47.531198: step 259, loss 1.1937, acc 0.71875, prec 0.0240597, recall 0.707317
2017-12-10T13:59:47.721873: step 260, loss 1.56749, acc 0.609375, prec 0.0241113, recall 0.708502
2017-12-10T13:59:47.907003: step 261, loss 1.44992, acc 0.625, prec 0.0240319, recall 0.708502
2017-12-10T13:59:48.092999: step 262, loss 12.2636, acc 0.578125, prec 0.0240799, recall 0.706827
2017-12-10T13:59:48.284059: step 263, loss 2.37854, acc 0.46875, prec 0.0239684, recall 0.706827
2017-12-10T13:59:48.475409: step 264, loss 7.85998, acc 0.4375, prec 0.0238579, recall 0.701195
2017-12-10T13:59:48.659208: step 265, loss 1.75083, acc 0.59375, prec 0.023906, recall 0.702381
2017-12-10T13:59:48.843522: step 266, loss 8.66363, acc 0.59375, prec 0.0242196, recall 0.703125
2017-12-10T13:59:49.030577: step 267, loss 3.23421, acc 0.3125, prec 0.0242076, recall 0.70428
2017-12-10T13:59:49.214377: step 268, loss 3.24508, acc 0.359375, prec 0.0240756, recall 0.70428
2017-12-10T13:59:49.402247: step 269, loss 13.5854, acc 0.453125, prec 0.0240964, recall 0.702703
2017-12-10T13:59:49.588398: step 270, loss 3.58585, acc 0.3125, prec 0.0240853, recall 0.703846
2017-12-10T13:59:49.775625: step 271, loss 4.44502, acc 0.296875, prec 0.0239435, recall 0.703846
2017-12-10T13:59:49.967556: step 272, loss 3.35998, acc 0.359375, prec 0.0239427, recall 0.704981
2017-12-10T13:59:50.156514: step 273, loss 4.36093, acc 0.359375, prec 0.023942, recall 0.706107
2017-12-10T13:59:50.344674: step 274, loss 4.82273, acc 0.546875, prec 0.0238556, recall 0.703422
2017-12-10T13:59:50.529922: step 275, loss 3.27368, acc 0.390625, prec 0.0241118, recall 0.706767
2017-12-10T13:59:50.717576: step 276, loss 3.63081, acc 0.359375, prec 0.0239857, recall 0.706767
2017-12-10T13:59:50.908119: step 277, loss 3.08794, acc 0.4375, prec 0.024, recall 0.707865
2017-12-10T13:59:51.095967: step 278, loss 2.59572, acc 0.5, prec 0.0241497, recall 0.710037
2017-12-10T13:59:51.285630: step 279, loss 2.40081, acc 0.421875, prec 0.0240373, recall 0.710037
2017-12-10T13:59:51.473896: step 280, loss 12.8074, acc 0.453125, prec 0.0239348, recall 0.707407
2017-12-10T13:59:51.659588: step 281, loss 2.69085, acc 0.46875, prec 0.0239551, recall 0.708487
2017-12-10T13:59:51.846321: step 282, loss 2.04955, acc 0.578125, prec 0.023996, recall 0.709559
2017-12-10T13:59:52.033408: step 283, loss 2.44763, acc 0.46875, prec 0.0240158, recall 0.710623
2017-12-10T13:59:52.217930: step 284, loss 12.4098, acc 0.5625, prec 0.0239358, recall 0.708029
2017-12-10T13:59:52.404607: step 285, loss 2.50213, acc 0.546875, prec 0.0238505, recall 0.708029
2017-12-10T13:59:52.595566: step 286, loss 2.04155, acc 0.609375, prec 0.0240167, recall 0.710145
2017-12-10T13:59:52.787904: step 287, loss 7.02052, acc 0.515625, prec 0.024167, recall 0.709677
2017-12-10T13:59:52.975865: step 288, loss 1.82709, acc 0.609375, prec 0.0242122, recall 0.710714
2017-12-10T13:59:53.163822: step 289, loss 5.49713, acc 0.578125, prec 0.0241358, recall 0.708185
2017-12-10T13:59:53.354730: step 290, loss 2.89228, acc 0.484375, prec 0.0240396, recall 0.708185
2017-12-10T13:59:53.541317: step 291, loss 1.91414, acc 0.546875, prec 0.0239557, recall 0.708185
2017-12-10T13:59:53.730794: step 292, loss 1.8259, acc 0.609375, prec 0.0238838, recall 0.708185
2017-12-10T13:59:53.916175: step 293, loss 1.96827, acc 0.5, prec 0.0237924, recall 0.708185
2017-12-10T13:59:54.106206: step 294, loss 1.89914, acc 0.53125, prec 0.0237074, recall 0.708185
2017-12-10T13:59:54.298694: step 295, loss 0.886945, acc 0.71875, prec 0.0237727, recall 0.70922
2017-12-10T13:59:54.485211: step 296, loss 1.23186, acc 0.65625, prec 0.0239422, recall 0.711268
2017-12-10T13:59:54.673671: step 297, loss 1.37622, acc 0.71875, prec 0.0240066, recall 0.712281
2017-12-10T13:59:54.863785: step 298, loss 1.5604, acc 0.65625, prec 0.0240594, recall 0.713287
2017-12-10T13:59:55.052727: step 299, loss 1.17546, acc 0.734375, prec 0.0241262, recall 0.714286
2017-12-10T13:59:55.239876: step 300, loss 7.43441, acc 0.734375, prec 0.0241983, recall 0.710345
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_1/1512932334/checkpoints/model-300

2017-12-10T13:59:56.497034: step 301, loss 1.21922, acc 0.6875, prec 0.0242559, recall 0.71134
2017-12-10T13:59:56.688118: step 302, loss 1.33178, acc 0.75, prec 0.0244387, recall 0.713311
2017-12-10T13:59:56.875825: step 303, loss 0.967682, acc 0.796875, prec 0.0244016, recall 0.713311
2017-12-10T13:59:57.062127: step 304, loss 0.800708, acc 0.734375, prec 0.024467, recall 0.714286
2017-12-10T13:59:57.248248: step 305, loss 18.3828, acc 0.703125, prec 0.0245292, recall 0.712838
2017-12-10T13:59:57.440892: step 306, loss 2.17275, acc 0.5625, prec 0.0245626, recall 0.713805
2017-12-10T13:59:57.627636: step 307, loss 5.28156, acc 0.734375, prec 0.0245172, recall 0.711409
2017-12-10T13:59:57.824224: step 308, loss 1.16506, acc 0.671875, prec 0.0244578, recall 0.711409
2017-12-10T13:59:58.013727: step 309, loss 4.45028, acc 0.796875, prec 0.0245363, recall 0.71
2017-12-10T13:59:58.204934: step 310, loss 2.11337, acc 0.5625, prec 0.0245695, recall 0.710963
2017-12-10T13:59:58.397607: step 311, loss 5.35699, acc 0.53125, prec 0.0245995, recall 0.709571
2017-12-10T13:59:58.589032: step 312, loss 2.73774, acc 0.546875, prec 0.0247406, recall 0.711475
2017-12-10T13:59:58.776675: step 313, loss 2.15257, acc 0.59375, prec 0.0246675, recall 0.711475
2017-12-10T13:59:58.972038: step 314, loss 1.97717, acc 0.546875, prec 0.0245864, recall 0.711475
2017-12-10T13:59:59.160506: step 315, loss 2.18281, acc 0.453125, prec 0.0245994, recall 0.712418
2017-12-10T13:59:59.352594: step 316, loss 3.1284, acc 0.609375, prec 0.024533, recall 0.710098
2017-12-10T13:59:59.547020: step 317, loss 2.78669, acc 0.53125, prec 0.0244504, recall 0.710098
2017-12-10T13:59:59.733631: step 318, loss 1.89145, acc 0.546875, prec 0.0243712, recall 0.710098
2017-12-10T13:59:59.920771: step 319, loss 2.09148, acc 0.53125, prec 0.0245071, recall 0.711974
2017-12-10T14:00:00.108744: step 320, loss 2.80589, acc 0.5625, prec 0.0246475, recall 0.713826
2017-12-10T14:00:00.293437: step 321, loss 2.93589, acc 0.46875, prec 0.0245548, recall 0.713826
2017-12-10T14:00:00.481669: step 322, loss 2.47479, acc 0.46875, prec 0.0245703, recall 0.714744
2017-12-10T14:00:00.669934: step 323, loss 2.05645, acc 0.546875, prec 0.024492, recall 0.714744
2017-12-10T14:00:00.859956: step 324, loss 13.8339, acc 0.578125, prec 0.0245291, recall 0.713376
2017-12-10T14:00:01.051619: step 325, loss 2.34921, acc 0.484375, prec 0.0244408, recall 0.713376
2017-12-10T14:00:01.246447: step 326, loss 7.50388, acc 0.578125, prec 0.0243717, recall 0.711111
2017-12-10T14:00:01.441125: step 327, loss 2.09252, acc 0.546875, prec 0.0244008, recall 0.712025
2017-12-10T14:00:01.625463: step 328, loss 1.94737, acc 0.546875, prec 0.0243243, recall 0.712025
2017-12-10T14:00:01.815723: step 329, loss 2.00515, acc 0.546875, prec 0.0243534, recall 0.712934
2017-12-10T14:00:02.006949: step 330, loss 1.45515, acc 0.578125, prec 0.0244924, recall 0.714734
2017-12-10T14:00:02.199003: step 331, loss 2.21178, acc 0.6875, prec 0.024649, recall 0.716511
2017-12-10T14:00:02.390684: step 332, loss 3.67779, acc 0.59375, prec 0.0245832, recall 0.714286
2017-12-10T14:00:02.584259: step 333, loss 1.6419, acc 0.5625, prec 0.0247177, recall 0.716049
2017-12-10T14:00:02.772930: step 334, loss 1.98771, acc 0.625, prec 0.0249655, recall 0.718654
2017-12-10T14:00:02.960042: step 335, loss 1.81545, acc 0.59375, prec 0.0248967, recall 0.718654
2017-12-10T14:00:03.148074: step 336, loss 2.42312, acc 0.609375, prec 0.024934, recall 0.719512
2017-12-10T14:00:03.337041: step 337, loss 2.21413, acc 0.625, prec 0.0249737, recall 0.720365
2017-12-10T14:00:03.527965: step 338, loss 1.70512, acc 0.5625, prec 0.0249002, recall 0.720365
2017-12-10T14:00:03.715972: step 339, loss 15.1952, acc 0.609375, prec 0.0248376, recall 0.718182
2017-12-10T14:00:03.903654: step 340, loss 1.46191, acc 0.671875, prec 0.0249869, recall 0.71988
2017-12-10T14:00:04.089435: step 341, loss 1.96038, acc 0.609375, prec 0.0250235, recall 0.720721
2017-12-10T14:00:04.279263: step 342, loss 9.08179, acc 0.59375, prec 0.0250598, recall 0.719403
2017-12-10T14:00:04.470482: step 343, loss 1.82894, acc 0.609375, prec 0.0249948, recall 0.719403
2017-12-10T14:00:04.657312: step 344, loss 2.04969, acc 0.515625, prec 0.0249147, recall 0.719403
2017-12-10T14:00:04.849993: step 345, loss 2.37343, acc 0.484375, prec 0.0250309, recall 0.721068
2017-12-10T14:00:05.036963: step 346, loss 1.65645, acc 0.609375, prec 0.0251669, recall 0.722714
2017-12-10T14:00:05.233059: step 347, loss 1.90847, acc 0.5625, prec 0.0250947, recall 0.722714
2017-12-10T14:00:05.418727: step 348, loss 1.63806, acc 0.59375, prec 0.0250281, recall 0.722714
2017-12-10T14:00:05.603333: step 349, loss 3.48717, acc 0.59375, prec 0.0249643, recall 0.720588
2017-12-10T14:00:05.798069: step 350, loss 1.50879, acc 0.640625, prec 0.0250051, recall 0.721408
2017-12-10T14:00:05.988428: step 351, loss 1.66351, acc 0.625, prec 0.0249442, recall 0.721408
2017-12-10T14:00:06.180139: step 352, loss 11.392, acc 0.5625, prec 0.0248761, recall 0.719298
2017-12-10T14:00:06.370935: step 353, loss 7.74287, acc 0.546875, prec 0.0249042, recall 0.718023
2017-12-10T14:00:06.563329: step 354, loss 2.53778, acc 0.625, prec 0.0249422, recall 0.718841
2017-12-10T14:00:06.747828: step 355, loss 2.34251, acc 0.4375, prec 0.0248522, recall 0.718841
2017-12-10T14:00:06.933028: step 356, loss 1.587, acc 0.609375, prec 0.0247901, recall 0.718841
2017-12-10T14:00:07.118810: step 357, loss 1.74734, acc 0.625, prec 0.0249252, recall 0.720461
2017-12-10T14:00:07.313465: step 358, loss 2.24505, acc 0.515625, prec 0.0248484, recall 0.720461
2017-12-10T14:00:07.506278: step 359, loss 2.80967, acc 0.4375, prec 0.0248564, recall 0.721264
2017-12-10T14:00:07.702010: step 360, loss 1.95816, acc 0.625, prec 0.0248938, recall 0.722063
2017-12-10T14:00:07.888318: step 361, loss 1.79136, acc 0.59375, prec 0.0249261, recall 0.722857
2017-12-10T14:00:08.071658: step 362, loss 1.97052, acc 0.546875, prec 0.0248551, recall 0.722857
2017-12-10T14:00:08.256457: step 363, loss 1.84898, acc 0.59375, prec 0.0247918, recall 0.722857
2017-12-10T14:00:08.448597: step 364, loss 1.45284, acc 0.609375, prec 0.0247312, recall 0.722857
2017-12-10T14:00:08.636813: step 365, loss 1.11968, acc 0.734375, prec 0.0247853, recall 0.723647
2017-12-10T14:00:08.825525: step 366, loss 3.90826, acc 0.671875, prec 0.024737, recall 0.721591
2017-12-10T14:00:09.012541: step 367, loss 11.9697, acc 0.65625, prec 0.0247813, recall 0.720339
2017-12-10T14:00:09.203971: step 368, loss 1.76919, acc 0.609375, prec 0.0248158, recall 0.721127
2017-12-10T14:00:09.395072: step 369, loss 0.809864, acc 0.75, prec 0.0247774, recall 0.721127
2017-12-10T14:00:09.583600: step 370, loss 1.4749, acc 0.609375, prec 0.0247176, recall 0.721127
2017-12-10T14:00:09.775448: step 371, loss 0.65091, acc 0.78125, prec 0.0246842, recall 0.721127
2017-12-10T14:00:09.965501: step 372, loss 1.30449, acc 0.703125, prec 0.0246391, recall 0.721127
2017-12-10T14:00:10.157502: step 373, loss 0.995987, acc 0.75, prec 0.0246012, recall 0.721127
2017-12-10T14:00:10.343860: step 374, loss 1.16292, acc 0.703125, prec 0.0245564, recall 0.721127
2017-12-10T14:00:10.538706: step 375, loss 0.657079, acc 0.828125, prec 0.0245305, recall 0.721127
2017-12-10T14:00:10.724734: step 376, loss 2.20259, acc 0.8125, prec 0.0245046, recall 0.719101
2017-12-10T14:00:10.913254: step 377, loss 0.88033, acc 0.765625, prec 0.0245627, recall 0.719888
2017-12-10T14:00:11.102932: step 378, loss 0.67271, acc 0.828125, prec 0.0246301, recall 0.72067
2017-12-10T14:00:11.293966: step 379, loss 8.89553, acc 0.859375, prec 0.0246136, recall 0.716667
2017-12-10T14:00:11.487334: step 380, loss 5.01514, acc 0.703125, prec 0.0247572, recall 0.716253
2017-12-10T14:00:11.682107: step 381, loss 5.02865, acc 0.8125, prec 0.0247313, recall 0.714286
2017-12-10T14:00:11.874683: step 382, loss 0.949358, acc 0.734375, prec 0.0248766, recall 0.715847
2017-12-10T14:00:12.065670: step 383, loss 7.12931, acc 0.546875, prec 0.0249029, recall 0.714674
2017-12-10T14:00:12.258991: step 384, loss 2.61507, acc 0.4375, prec 0.0248183, recall 0.714674
2017-12-10T14:00:12.445436: step 385, loss 2.06227, acc 0.546875, prec 0.0248424, recall 0.715447
2017-12-10T14:00:12.631100: step 386, loss 2.24553, acc 0.453125, prec 0.0247608, recall 0.715447
2017-12-10T14:00:12.817293: step 387, loss 3.55119, acc 0.40625, prec 0.0246729, recall 0.715447
2017-12-10T14:00:13.002392: step 388, loss 3.89075, acc 0.40625, prec 0.0246764, recall 0.716216
2017-12-10T14:00:13.185847: step 389, loss 4.50036, acc 0.359375, prec 0.0247635, recall 0.717742
2017-12-10T14:00:13.373584: step 390, loss 3.6383, acc 0.453125, prec 0.0249538, recall 0.72
2017-12-10T14:00:13.558846: step 391, loss 3.14246, acc 0.453125, prec 0.0249632, recall 0.720745
2017-12-10T14:00:13.746258: step 392, loss 3.58422, acc 0.3125, prec 0.0249518, recall 0.721485
2017-12-10T14:00:13.937436: step 393, loss 4.75868, acc 0.296875, prec 0.0249383, recall 0.722222
2017-12-10T14:00:14.128444: step 394, loss 3.78409, acc 0.375, prec 0.0249363, recall 0.722955
2017-12-10T14:00:14.310624: step 395, loss 3.36115, acc 0.4375, prec 0.0249433, recall 0.723684
2017-12-10T14:00:14.499372: step 396, loss 2.80658, acc 0.5, prec 0.0248711, recall 0.723684
2017-12-10T14:00:14.686312: step 397, loss 2.69702, acc 0.5, prec 0.0247994, recall 0.723684
2017-12-10T14:00:14.876406: step 398, loss 6.04108, acc 0.359375, prec 0.0247978, recall 0.722513
2017-12-10T14:00:15.071482: step 399, loss 2.51736, acc 0.515625, prec 0.0248163, recall 0.723238
2017-12-10T14:00:15.259415: step 400, loss 2.42979, acc 0.53125, prec 0.0247498, recall 0.723238
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_1/1512932334/checkpoints/model-400

2017-12-10T14:00:16.430777: step 401, loss 2.13693, acc 0.609375, prec 0.0248685, recall 0.724675
2017-12-10T14:00:16.620365: step 402, loss 1.68599, acc 0.625, prec 0.0249022, recall 0.725389
2017-12-10T14:00:16.811349: step 403, loss 1.65143, acc 0.640625, prec 0.0249379, recall 0.726098
2017-12-10T14:00:17.005692: step 404, loss 2.67701, acc 0.53125, prec 0.0250442, recall 0.727506
2017-12-10T14:00:17.195795: step 405, loss 1.04542, acc 0.765625, prec 0.025011, recall 0.727506
2017-12-10T14:00:17.382838: step 406, loss 1.6508, acc 0.6875, prec 0.0250529, recall 0.728205
2017-12-10T14:00:17.573878: step 407, loss 0.688817, acc 0.84375, prec 0.0250308, recall 0.728205
2017-12-10T14:00:17.761592: step 408, loss 0.839787, acc 0.765625, prec 0.0249978, recall 0.728205
2017-12-10T14:00:17.955079: step 409, loss 0.773631, acc 0.75, prec 0.0250483, recall 0.7289
2017-12-10T14:00:18.146613: step 410, loss 0.544735, acc 0.84375, prec 0.025112, recall 0.729592
2017-12-10T14:00:18.333630: step 411, loss 0.554913, acc 0.84375, prec 0.0250899, recall 0.729592
2017-12-10T14:00:18.521923: step 412, loss 0.632125, acc 0.8125, prec 0.025149, recall 0.73028
2017-12-10T14:00:18.712486: step 413, loss 0.613041, acc 0.84375, prec 0.0252123, recall 0.730964
2017-12-10T14:00:18.902800: step 414, loss 0.663636, acc 0.796875, prec 0.0252689, recall 0.731646
2017-12-10T14:00:19.094776: step 415, loss 0.183775, acc 0.96875, prec 0.0252644, recall 0.731646
2017-12-10T14:00:19.286442: step 416, loss 7.24843, acc 0.84375, prec 0.0252446, recall 0.729798
2017-12-10T14:00:19.480190: step 417, loss 16.1659, acc 0.9375, prec 0.0252402, recall 0.726131
2017-12-10T14:00:19.670210: step 418, loss 0.277205, acc 0.90625, prec 0.025227, recall 0.726131
2017-12-10T14:00:19.861593: step 419, loss 0.726286, acc 0.828125, prec 0.0252028, recall 0.726131
2017-12-10T14:00:20.055587: step 420, loss 1.34911, acc 0.84375, prec 0.0252657, recall 0.726817
2017-12-10T14:00:20.244570: step 421, loss 3.90735, acc 0.84375, prec 0.0252459, recall 0.725
2017-12-10T14:00:20.434041: step 422, loss 0.703799, acc 0.890625, prec 0.0253154, recall 0.725686
2017-12-10T14:00:20.623600: step 423, loss 0.794095, acc 0.84375, prec 0.0252934, recall 0.725686
2017-12-10T14:00:20.816585: step 424, loss 12.051, acc 0.703125, prec 0.0252538, recall 0.723881
2017-12-10T14:00:21.008887: step 425, loss 0.656985, acc 0.765625, prec 0.0253055, recall 0.724566
2017-12-10T14:00:21.200113: step 426, loss 1.07123, acc 0.765625, prec 0.0252726, recall 0.724566
2017-12-10T14:00:21.388150: step 427, loss 1.25122, acc 0.75, prec 0.0252377, recall 0.724566
2017-12-10T14:00:21.574337: step 428, loss 1.54632, acc 0.6875, prec 0.0251941, recall 0.724566
2017-12-10T14:00:21.766775: step 429, loss 1.1413, acc 0.75, prec 0.0252434, recall 0.725248
2017-12-10T14:00:21.956601: step 430, loss 1.82334, acc 0.6875, prec 0.0252838, recall 0.725926
2017-12-10T14:00:22.143684: step 431, loss 1.87278, acc 0.5625, prec 0.0252231, recall 0.725926
2017-12-10T14:00:22.335017: step 432, loss 1.11898, acc 0.734375, prec 0.0253533, recall 0.727273
2017-12-10T14:00:22.526602: step 433, loss 2.22456, acc 0.59375, prec 0.0253803, recall 0.727941
2017-12-10T14:00:22.718379: step 434, loss 11.7475, acc 0.734375, prec 0.0253478, recall 0.72439
2017-12-10T14:00:22.906092: step 435, loss 0.907391, acc 0.71875, prec 0.0253089, recall 0.72439
2017-12-10T14:00:23.097932: step 436, loss 11.6526, acc 0.59375, prec 0.025338, recall 0.723301
2017-12-10T14:00:23.290720: step 437, loss 1.88463, acc 0.578125, prec 0.0253626, recall 0.723971
2017-12-10T14:00:23.485482: step 438, loss 2.53374, acc 0.515625, prec 0.0253786, recall 0.724638
2017-12-10T14:00:23.674842: step 439, loss 2.08789, acc 0.59375, prec 0.0253229, recall 0.724638
2017-12-10T14:00:23.864852: step 440, loss 2.2363, acc 0.515625, prec 0.0252568, recall 0.724638
2017-12-10T14:00:24.053525: step 441, loss 3.41228, acc 0.40625, prec 0.025258, recall 0.725301
2017-12-10T14:00:24.239722: step 442, loss 2.65873, acc 0.5625, prec 0.0251988, recall 0.725301
2017-12-10T14:00:24.424210: step 443, loss 2.03176, acc 0.6875, prec 0.0254011, recall 0.727273
2017-12-10T14:00:24.611181: step 444, loss 2.16768, acc 0.5, prec 0.0253333, recall 0.727273
2017-12-10T14:00:24.799250: step 445, loss 1.95719, acc 0.546875, prec 0.0254343, recall 0.728571
2017-12-10T14:00:24.986501: step 446, loss 1.76254, acc 0.5625, prec 0.0253752, recall 0.728571
2017-12-10T14:00:25.174877: step 447, loss 7.42261, acc 0.734375, prec 0.0253416, recall 0.726841
2017-12-10T14:00:25.365331: step 448, loss 1.64425, acc 0.515625, prec 0.0252767, recall 0.726841
2017-12-10T14:00:25.550178: step 449, loss 1.56473, acc 0.671875, prec 0.025233, recall 0.726841
2017-12-10T14:00:25.737086: step 450, loss 1.40742, acc 0.625, prec 0.0251831, recall 0.726841
2017-12-10T14:00:25.924277: step 451, loss 0.765306, acc 0.75, prec 0.0252301, recall 0.727488
2017-12-10T14:00:26.116142: step 452, loss 33.8482, acc 0.671875, prec 0.0251928, recall 0.722353
2017-12-10T14:00:26.308610: step 453, loss 2.15075, acc 0.6875, prec 0.0252314, recall 0.723005
2017-12-10T14:00:26.499375: step 454, loss 8.88622, acc 0.625, prec 0.025184, recall 0.721311
2017-12-10T14:00:26.694896: step 455, loss 1.56432, acc 0.5625, prec 0.0251264, recall 0.721311
2017-12-10T14:00:26.883455: step 456, loss 2.78193, acc 0.46875, prec 0.0252156, recall 0.722611
2017-12-10T14:00:27.070026: step 457, loss 1.89842, acc 0.546875, prec 0.0251562, recall 0.722611
2017-12-10T14:00:27.259024: step 458, loss 2.7951, acc 0.484375, prec 0.0252468, recall 0.723898
2017-12-10T14:00:27.449403: step 459, loss 2.22894, acc 0.515625, prec 0.025341, recall 0.725173
2017-12-10T14:00:27.641697: step 460, loss 3.92641, acc 0.28125, prec 0.0253256, recall 0.725806
2017-12-10T14:00:27.831529: step 461, loss 2.05787, acc 0.390625, prec 0.0253246, recall 0.726437
2017-12-10T14:00:28.015637: step 462, loss 3.05215, acc 0.453125, prec 0.0252537, recall 0.726437
2017-12-10T14:00:28.203321: step 463, loss 3.29017, acc 0.375, prec 0.0254062, recall 0.728311
2017-12-10T14:00:28.389913: step 464, loss 2.1128, acc 0.546875, prec 0.0253476, recall 0.728311
2017-12-10T14:00:28.572460: step 465, loss 2.69609, acc 0.484375, prec 0.0252813, recall 0.728311
2017-12-10T14:00:28.762744: step 466, loss 2.80824, acc 0.375, prec 0.0252785, recall 0.728929
2017-12-10T14:00:28.959177: step 467, loss 1.42782, acc 0.640625, prec 0.0252326, recall 0.728929
2017-12-10T14:00:29.145655: step 468, loss 17.7109, acc 0.53125, prec 0.0252537, recall 0.726244
2017-12-10T14:00:29.338959: step 469, loss 2.13827, acc 0.59375, prec 0.0252787, recall 0.726862
2017-12-10T14:00:29.528881: step 470, loss 1.7359, acc 0.625, prec 0.0252312, recall 0.726862
2017-12-10T14:00:29.720880: step 471, loss 2.12035, acc 0.59375, prec 0.0251799, recall 0.726862
2017-12-10T14:00:29.909835: step 472, loss 1.75517, acc 0.609375, prec 0.0252068, recall 0.727477
2017-12-10T14:00:30.095479: step 473, loss 1.60659, acc 0.53125, prec 0.0252997, recall 0.7287
2017-12-10T14:00:30.286438: step 474, loss 1.73798, acc 0.703125, prec 0.0254138, recall 0.729911
2017-12-10T14:00:30.471039: step 475, loss 1.66369, acc 0.609375, prec 0.0254402, recall 0.730512
2017-12-10T14:00:30.659433: step 476, loss 1.62475, acc 0.609375, prec 0.0253909, recall 0.730512
2017-12-10T14:00:30.853223: step 477, loss 1.06515, acc 0.75, prec 0.0253595, recall 0.730512
2017-12-10T14:00:31.044190: step 478, loss 2.38421, acc 0.65625, prec 0.0253184, recall 0.728889
2017-12-10T14:00:31.238000: step 479, loss 4.33614, acc 0.609375, prec 0.0253467, recall 0.727876
2017-12-10T14:00:31.431127: step 480, loss 1.01798, acc 0.75, prec 0.0254655, recall 0.729075
2017-12-10T14:00:31.621304: step 481, loss 1.45634, acc 0.671875, prec 0.0254244, recall 0.729075
2017-12-10T14:00:31.806407: step 482, loss 0.826284, acc 0.8125, prec 0.0254758, recall 0.72967
2017-12-10T14:00:31.998283: step 483, loss 3.69384, acc 0.78125, prec 0.0254504, recall 0.72807
2017-12-10T14:00:32.185766: step 484, loss 1.32853, acc 0.703125, prec 0.0254879, recall 0.728665
2017-12-10T14:00:32.376805: step 485, loss 3.72075, acc 0.71875, prec 0.0254548, recall 0.727074
2017-12-10T14:00:32.565019: step 486, loss 1.20067, acc 0.65625, prec 0.0254121, recall 0.727074
2017-12-10T14:00:32.751450: step 487, loss 1.74196, acc 0.625, prec 0.0254399, recall 0.727669
2017-12-10T14:00:32.942088: step 488, loss 1.14808, acc 0.671875, prec 0.0254733, recall 0.728261
2017-12-10T14:00:33.132618: step 489, loss 0.988123, acc 0.75, prec 0.0254424, recall 0.728261
2017-12-10T14:00:33.319451: step 490, loss 6.43387, acc 0.71875, prec 0.0254835, recall 0.727273
2017-12-10T14:00:33.509884: step 491, loss 1.36234, acc 0.671875, prec 0.025443, recall 0.727273
2017-12-10T14:00:33.703664: step 492, loss 8.67802, acc 0.71875, prec 0.0254859, recall 0.724731
2017-12-10T14:00:33.896092: step 493, loss 1.37494, acc 0.671875, prec 0.0254455, recall 0.724731
2017-12-10T14:00:34.083597: step 494, loss 1.79936, acc 0.578125, prec 0.0253937, recall 0.724731
2017-12-10T14:00:34.273942: step 495, loss 10.7281, acc 0.578125, prec 0.0253441, recall 0.723176
2017-12-10T14:00:34.466052: step 496, loss 2.4409, acc 0.484375, prec 0.0252813, recall 0.723176
2017-12-10T14:00:34.634189: step 497, loss 1.96539, acc 0.529412, prec 0.0253089, recall 0.723769
2017-12-10T14:00:34.832326: step 498, loss 2.06138, acc 0.5625, prec 0.0253288, recall 0.724359
2017-12-10T14:00:35.020467: step 499, loss 2.36673, acc 0.515625, prec 0.0252702, recall 0.724359
2017-12-10T14:00:35.206826: step 500, loss 2.29056, acc 0.515625, prec 0.025212, recall 0.724359
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_1/1512932334/checkpoints/model-500

2017-12-10T14:00:36.471004: step 501, loss 1.85486, acc 0.625, prec 0.0253841, recall 0.726115
2017-12-10T14:00:36.661219: step 502, loss 2.14327, acc 0.59375, prec 0.0254796, recall 0.727273
2017-12-10T14:00:36.854339: step 503, loss 2.02433, acc 0.5625, prec 0.0254989, recall 0.727848
2017-12-10T14:00:37.046828: step 504, loss 4.0439, acc 0.46875, prec 0.0255087, recall 0.726891
2017-12-10T14:00:37.235872: step 505, loss 1.68051, acc 0.65625, prec 0.0256108, recall 0.728033
2017-12-10T14:00:37.419716: step 506, loss 1.1086, acc 0.6875, prec 0.0255732, recall 0.728033
2017-12-10T14:00:37.603912: step 507, loss 2.09329, acc 0.578125, prec 0.025594, recall 0.728601
2017-12-10T14:00:37.788167: step 508, loss 1.44903, acc 0.609375, prec 0.0255472, recall 0.728601
2017-12-10T14:00:37.976510: step 509, loss 0.939037, acc 0.703125, prec 0.0255117, recall 0.728601
2017-12-10T14:00:38.169595: step 510, loss 1.20367, acc 0.796875, prec 0.0255586, recall 0.729167
2017-12-10T14:00:38.355830: step 511, loss 1.19874, acc 0.6875, prec 0.0256635, recall 0.73029
2017-12-10T14:00:38.544105: step 512, loss 0.915517, acc 0.6875, prec 0.0256261, recall 0.73029
2017-12-10T14:00:38.732109: step 513, loss 0.73057, acc 0.78125, prec 0.0256, recall 0.73029
2017-12-10T14:00:38.916185: step 514, loss 6.63557, acc 0.8125, prec 0.0256522, recall 0.727835
2017-12-10T14:00:39.104981: step 515, loss 0.501013, acc 0.859375, prec 0.0257062, recall 0.728395
2017-12-10T14:00:39.293792: step 516, loss 0.910289, acc 0.71875, prec 0.0256726, recall 0.728395
2017-12-10T14:00:39.487423: step 517, loss 11.1274, acc 0.71875, prec 0.025641, recall 0.726899
2017-12-10T14:00:39.677144: step 518, loss 5.33044, acc 0.765625, prec 0.0256169, recall 0.723926
2017-12-10T14:00:39.865628: step 519, loss 1.28544, acc 0.703125, prec 0.0255817, recall 0.723926
2017-12-10T14:00:40.055087: step 520, loss 2.13059, acc 0.640625, prec 0.0256096, recall 0.72449
2017-12-10T14:00:40.248976: step 521, loss 7.00131, acc 0.75, prec 0.0256521, recall 0.723577
2017-12-10T14:00:40.441206: step 522, loss 1.6001, acc 0.65625, prec 0.0256115, recall 0.723577
2017-12-10T14:00:40.626797: step 523, loss 2.05288, acc 0.53125, prec 0.0256263, recall 0.724138
2017-12-10T14:00:40.815708: step 524, loss 2.04289, acc 0.5625, prec 0.0256447, recall 0.724696
2017-12-10T14:00:41.002177: step 525, loss 2.62473, acc 0.453125, prec 0.0256502, recall 0.725253
2017-12-10T14:00:41.188153: step 526, loss 2.65295, acc 0.546875, prec 0.0255971, recall 0.725253
2017-12-10T14:00:41.378122: step 527, loss 3.32074, acc 0.59375, prec 0.0255516, recall 0.72379
2017-12-10T14:00:41.565497: step 528, loss 2.89711, acc 0.40625, prec 0.0254827, recall 0.72379
2017-12-10T14:00:41.752033: step 529, loss 2.37792, acc 0.53125, prec 0.0254976, recall 0.724346
2017-12-10T14:00:41.939716: step 530, loss 6.11386, acc 0.53125, prec 0.025583, recall 0.724
2017-12-10T14:00:42.127639: step 531, loss 2.47253, acc 0.4375, prec 0.0256555, recall 0.7251
2017-12-10T14:00:42.315338: step 532, loss 2.08837, acc 0.578125, prec 0.0257438, recall 0.72619
2017-12-10T14:00:42.500435: step 533, loss 1.78484, acc 0.546875, prec 0.0258282, recall 0.727273
2017-12-10T14:00:42.690245: step 534, loss 5.51788, acc 0.453125, prec 0.0258349, recall 0.726378
2017-12-10T14:00:42.878405: step 535, loss 1.82113, acc 0.53125, prec 0.0258488, recall 0.726916
2017-12-10T14:00:43.064218: step 536, loss 3.22753, acc 0.4375, prec 0.0259197, recall 0.727984
2017-12-10T14:00:43.254387: step 537, loss 2.65058, acc 0.46875, prec 0.0259262, recall 0.728516
2017-12-10T14:00:43.442634: step 538, loss 1.80284, acc 0.53125, prec 0.0259398, recall 0.729045
2017-12-10T14:00:43.631239: step 539, loss 2.0326, acc 0.5, prec 0.0259498, recall 0.729572
2017-12-10T14:00:43.822864: step 540, loss 1.69835, acc 0.53125, prec 0.0260978, recall 0.731141
2017-12-10T14:00:44.015788: step 541, loss 1.70228, acc 0.625, prec 0.0261217, recall 0.73166
2017-12-10T14:00:44.201613: step 542, loss 2.16135, acc 0.515625, prec 0.026066, recall 0.73166
2017-12-10T14:00:44.391042: step 543, loss 6.36167, acc 0.65625, prec 0.0260284, recall 0.73025
2017-12-10T14:00:44.583876: step 544, loss 1.45806, acc 0.65625, prec 0.0259892, recall 0.73025
2017-12-10T14:00:44.769876: step 545, loss 1.18502, acc 0.71875, prec 0.0259571, recall 0.73025
2017-12-10T14:00:44.960855: step 546, loss 1.31254, acc 0.671875, prec 0.0259198, recall 0.73025
2017-12-10T14:00:45.146006: step 547, loss 1.40806, acc 0.703125, prec 0.0258862, recall 0.73025
2017-12-10T14:00:45.333083: step 548, loss 0.836207, acc 0.765625, prec 0.0258597, recall 0.73025
2017-12-10T14:00:45.526730: step 549, loss 0.836729, acc 0.765625, prec 0.0258997, recall 0.730769
2017-12-10T14:00:45.718805: step 550, loss 1.19656, acc 0.734375, prec 0.0258697, recall 0.730769
2017-12-10T14:00:45.908724: step 551, loss 0.748478, acc 0.734375, prec 0.0258398, recall 0.730769
2017-12-10T14:00:46.100010: step 552, loss 0.760317, acc 0.78125, prec 0.0258814, recall 0.731286
2017-12-10T14:00:46.293156: step 553, loss 0.90814, acc 0.796875, prec 0.0259247, recall 0.731801
2017-12-10T14:00:46.486297: step 554, loss 0.538523, acc 0.8125, prec 0.0259036, recall 0.731801
2017-12-10T14:00:46.671118: step 555, loss 1.12469, acc 0.875, prec 0.0259555, recall 0.732314
2017-12-10T14:00:46.863862: step 556, loss 0.508162, acc 0.875, prec 0.0259415, recall 0.732314
2017-12-10T14:00:47.049719: step 557, loss 0.810995, acc 0.875, prec 0.0259934, recall 0.732824
2017-12-10T14:00:47.238595: step 558, loss 0.284051, acc 0.890625, prec 0.0259811, recall 0.732824
2017-12-10T14:00:47.428441: step 559, loss 15.185, acc 0.828125, prec 0.0260311, recall 0.73055
2017-12-10T14:00:47.622865: step 560, loss 0.648761, acc 0.875, prec 0.0261486, recall 0.731569
2017-12-10T14:00:47.810474: step 561, loss 0.594091, acc 0.84375, prec 0.0261967, recall 0.732075
2017-12-10T14:00:47.998755: step 562, loss 0.330191, acc 0.859375, prec 0.0262465, recall 0.73258
2017-12-10T14:00:48.186281: step 563, loss 3.93239, acc 0.890625, prec 0.0263673, recall 0.73221
2017-12-10T14:00:48.380046: step 564, loss 0.434045, acc 0.890625, prec 0.0264204, recall 0.73271
2017-12-10T14:00:48.570971: step 565, loss 0.812534, acc 0.734375, prec 0.0264557, recall 0.733209
2017-12-10T14:00:48.761206: step 566, loss 1.54416, acc 0.59375, prec 0.0264095, recall 0.733209
2017-12-10T14:00:48.949087: step 567, loss 1.688, acc 0.671875, prec 0.0264376, recall 0.733706
2017-12-10T14:00:49.138489: step 568, loss 1.12123, acc 0.703125, prec 0.026404, recall 0.733706
2017-12-10T14:00:49.324359: step 569, loss 1.21143, acc 0.71875, prec 0.0265676, recall 0.735185
2017-12-10T14:00:49.517396: step 570, loss 1.29189, acc 0.703125, prec 0.0265339, recall 0.735185
2017-12-10T14:00:49.707307: step 571, loss 1.43383, acc 0.578125, prec 0.0266809, recall 0.736648
2017-12-10T14:00:49.895253: step 572, loss 1.66729, acc 0.609375, prec 0.0267013, recall 0.737132
2017-12-10T14:00:50.085827: step 573, loss 1.59415, acc 0.546875, prec 0.0267792, recall 0.738095
2017-12-10T14:00:50.273186: step 574, loss 1.21045, acc 0.6875, prec 0.0268728, recall 0.739051
2017-12-10T14:00:50.464340: step 575, loss 1.75177, acc 0.578125, prec 0.0268247, recall 0.739051
2017-12-10T14:00:50.654809: step 576, loss 0.762337, acc 0.78125, prec 0.0269287, recall 0.74
2017-12-10T14:00:50.841275: step 577, loss 1.1859, acc 0.625, prec 0.0270145, recall 0.740942
2017-12-10T14:00:51.027616: step 578, loss 1.31947, acc 0.734375, prec 0.0270484, recall 0.74141
2017-12-10T14:00:51.217424: step 579, loss 0.985044, acc 0.734375, prec 0.0270822, recall 0.741877
2017-12-10T14:00:51.409060: step 580, loss 1.92999, acc 0.796875, prec 0.0271231, recall 0.742342
2017-12-10T14:00:51.597279: step 581, loss 0.988325, acc 0.765625, prec 0.0272243, recall 0.743267
2017-12-10T14:00:51.780811: step 582, loss 15.0444, acc 0.78125, prec 0.0272011, recall 0.741935
2017-12-10T14:00:51.975187: step 583, loss 0.707435, acc 0.796875, prec 0.0271778, recall 0.741935
2017-12-10T14:00:52.165084: step 584, loss 0.642967, acc 0.78125, prec 0.0272167, recall 0.742397
2017-12-10T14:00:52.358206: step 585, loss 0.611377, acc 0.765625, prec 0.0272537, recall 0.742857
2017-12-10T14:00:52.548050: step 586, loss 0.914404, acc 0.828125, prec 0.027234, recall 0.742857
2017-12-10T14:00:52.736639: step 587, loss 0.860582, acc 0.703125, prec 0.0272638, recall 0.743316
2017-12-10T14:00:52.927795: step 588, loss 1.24855, acc 0.78125, prec 0.0273659, recall 0.744227
2017-12-10T14:00:53.118757: step 589, loss 7.6003, acc 0.875, prec 0.0274169, recall 0.743363
2017-12-10T14:00:53.306300: step 590, loss 4.91353, acc 0.78125, prec 0.0273937, recall 0.742049
2017-12-10T14:00:53.500260: step 591, loss 0.940493, acc 0.71875, prec 0.0274249, recall 0.742504
2017-12-10T14:00:53.687263: step 592, loss 2.32606, acc 0.703125, prec 0.0275808, recall 0.74386
2017-12-10T14:00:53.875716: step 593, loss 1.30029, acc 0.71875, prec 0.0276117, recall 0.744308
2017-12-10T14:00:54.068172: step 594, loss 0.992173, acc 0.734375, prec 0.0275813, recall 0.744308
2017-12-10T14:00:54.255092: step 595, loss 1.31048, acc 0.71875, prec 0.0276752, recall 0.745201
2017-12-10T14:00:54.439857: step 596, loss 9.91181, acc 0.796875, prec 0.0276554, recall 0.742609
2017-12-10T14:00:54.628910: step 597, loss 5.03356, acc 0.5, prec 0.0276, recall 0.741319
2017-12-10T14:00:54.813836: step 598, loss 1.83189, acc 0.625, prec 0.02762, recall 0.741768
2017-12-10T14:00:54.999494: step 599, loss 2.25269, acc 0.578125, prec 0.027572, recall 0.741768
2017-12-10T14:00:55.185949: step 600, loss 2.58387, acc 0.453125, prec 0.02751, recall 0.741768
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_1/1512932334/checkpoints/model-600

2017-12-10T14:00:56.375137: step 601, loss 2.45579, acc 0.578125, prec 0.0275871, recall 0.74266
2017-12-10T14:00:56.562166: step 602, loss 2.05428, acc 0.515625, prec 0.0275323, recall 0.74266
2017-12-10T14:00:56.748112: step 603, loss 2.45403, acc 0.453125, prec 0.0275329, recall 0.743103
2017-12-10T14:00:56.935717: step 604, loss 2.08409, acc 0.625, prec 0.0274908, recall 0.743103
2017-12-10T14:00:57.120865: step 605, loss 4.66521, acc 0.515625, prec 0.0274382, recall 0.741824
2017-12-10T14:00:57.309878: step 606, loss 2.53205, acc 0.421875, prec 0.0274973, recall 0.74271
2017-12-10T14:00:57.498863: step 607, loss 2.46637, acc 0.53125, prec 0.0276299, recall 0.744027
2017-12-10T14:00:57.688607: step 608, loss 2.38378, acc 0.515625, prec 0.0275757, recall 0.744027
2017-12-10T14:00:57.877845: step 609, loss 2.93347, acc 0.46875, prec 0.0275779, recall 0.744463
2017-12-10T14:00:58.067864: step 610, loss 1.59159, acc 0.546875, prec 0.0275888, recall 0.744898
2017-12-10T14:00:58.254217: step 611, loss 2.00279, acc 0.546875, prec 0.0275385, recall 0.744898
2017-12-10T14:00:58.440633: step 612, loss 2.23398, acc 0.546875, prec 0.0274884, recall 0.744898
2017-12-10T14:00:58.631645: step 613, loss 1.58105, acc 0.5625, prec 0.0274402, recall 0.744898
2017-12-10T14:00:58.822659: step 614, loss 1.53622, acc 0.734375, prec 0.0274718, recall 0.745331
2017-12-10T14:00:59.015489: step 615, loss 0.819754, acc 0.78125, prec 0.0274478, recall 0.745331
2017-12-10T14:00:59.203530: step 616, loss 10.8899, acc 0.734375, prec 0.0274811, recall 0.744501
2017-12-10T14:00:59.396262: step 617, loss 5.37084, acc 0.765625, prec 0.0275178, recall 0.743676
2017-12-10T14:00:59.585561: step 618, loss 6.95816, acc 0.78125, prec 0.0274955, recall 0.742424
2017-12-10T14:00:59.773351: step 619, loss 1.28363, acc 0.734375, prec 0.0275269, recall 0.742857
2017-12-10T14:00:59.959896: step 620, loss 1.0908, acc 0.609375, prec 0.0274841, recall 0.742857
2017-12-10T14:01:00.146923: step 621, loss 6.58113, acc 0.609375, prec 0.0274432, recall 0.741611
2017-12-10T14:01:00.336620: step 622, loss 1.12248, acc 0.6875, prec 0.0274092, recall 0.741611
2017-12-10T14:01:00.526563: step 623, loss 1.75878, acc 0.609375, prec 0.0274872, recall 0.742475
2017-12-10T14:01:00.713066: step 624, loss 2.35948, acc 0.5625, prec 0.0274396, recall 0.742475
2017-12-10T14:01:00.904555: step 625, loss 1.92326, acc 0.625, prec 0.027459, recall 0.742905
2017-12-10T14:01:01.096724: step 626, loss 1.53163, acc 0.609375, prec 0.0274167, recall 0.742905
2017-12-10T14:01:01.281479: step 627, loss 1.4738, acc 0.6875, prec 0.0275624, recall 0.744186
2017-12-10T14:01:01.474595: step 628, loss 1.52058, acc 0.609375, prec 0.0275799, recall 0.74461
2017-12-10T14:01:01.665154: step 629, loss 1.35948, acc 0.65625, prec 0.0277216, recall 0.745875
2017-12-10T14:01:01.850970: step 630, loss 1.34313, acc 0.703125, prec 0.0276893, recall 0.745875
2017-12-10T14:01:02.040694: step 631, loss 1.68292, acc 0.75, prec 0.0277217, recall 0.746293
2017-12-10T14:01:02.229841: step 632, loss 1.27007, acc 0.671875, prec 0.0277455, recall 0.746711
2017-12-10T14:01:02.424454: step 633, loss 1.59563, acc 0.640625, prec 0.0277066, recall 0.746711
2017-12-10T14:01:02.614625: step 634, loss 1.37861, acc 0.609375, prec 0.0276644, recall 0.746711
2017-12-10T14:01:02.805680: step 635, loss 0.898011, acc 0.765625, prec 0.0276983, recall 0.747126
2017-12-10T14:01:02.994617: step 636, loss 0.931508, acc 0.78125, prec 0.0277339, recall 0.747541
2017-12-10T14:01:03.182722: step 637, loss 0.960639, acc 0.8125, prec 0.0277136, recall 0.747541
2017-12-10T14:01:03.372339: step 638, loss 3.94652, acc 0.84375, prec 0.0277575, recall 0.746732
2017-12-10T14:01:03.568966: step 639, loss 0.573249, acc 0.8125, prec 0.0277373, recall 0.746732
2017-12-10T14:01:03.754224: step 640, loss 0.55242, acc 0.859375, prec 0.0277222, recall 0.746732
2017-12-10T14:01:03.939887: step 641, loss 3.46363, acc 0.78125, prec 0.0277593, recall 0.745928
2017-12-10T14:01:04.130528: step 642, loss 0.485263, acc 0.828125, prec 0.0277408, recall 0.745928
2017-12-10T14:01:04.318610: step 643, loss 0.853472, acc 0.75, prec 0.0278316, recall 0.746753
2017-12-10T14:01:04.508281: step 644, loss 0.772751, acc 0.796875, prec 0.0278097, recall 0.746753
2017-12-10T14:01:04.699131: step 645, loss 0.93957, acc 0.734375, prec 0.0277811, recall 0.746753
2017-12-10T14:01:04.889147: step 646, loss 1.04038, acc 0.8125, prec 0.0278197, recall 0.747164
2017-12-10T14:01:05.082039: step 647, loss 0.883265, acc 0.765625, prec 0.0277945, recall 0.747164
2017-12-10T14:01:05.268191: step 648, loss 10.2473, acc 0.859375, prec 0.0279569, recall 0.747182
2017-12-10T14:01:05.456877: step 649, loss 0.780703, acc 0.765625, prec 0.0279316, recall 0.747182
2017-12-10T14:01:05.646326: step 650, loss 1.01596, acc 0.796875, prec 0.0279098, recall 0.747182
2017-12-10T14:01:05.835572: step 651, loss 0.350408, acc 0.859375, prec 0.0278947, recall 0.747182
2017-12-10T14:01:06.019990: step 652, loss 0.915485, acc 0.71875, prec 0.0279229, recall 0.747588
2017-12-10T14:01:06.208846: step 653, loss 15.2536, acc 0.71875, prec 0.0280127, recall 0.746006
2017-12-10T14:01:06.399344: step 654, loss 1.00868, acc 0.71875, prec 0.0279825, recall 0.746006
2017-12-10T14:01:06.589943: step 655, loss 1.30757, acc 0.6875, prec 0.027949, recall 0.746006
2017-12-10T14:01:06.777236: step 656, loss 1.61341, acc 0.640625, prec 0.0279687, recall 0.746412
2017-12-10T14:01:06.967780: step 657, loss 1.05755, acc 0.703125, prec 0.027995, recall 0.746815
2017-12-10T14:01:07.154813: step 658, loss 1.56831, acc 0.625, prec 0.0280708, recall 0.747619
2017-12-10T14:01:07.340666: step 659, loss 1.18852, acc 0.65625, prec 0.028034, recall 0.747619
2017-12-10T14:01:07.526815: step 660, loss 1.31968, acc 0.625, prec 0.0280518, recall 0.748019
2017-12-10T14:01:07.715702: step 661, loss 0.957336, acc 0.71875, prec 0.0280218, recall 0.748019
2017-12-10T14:01:07.904448: step 662, loss 1.63535, acc 0.65625, prec 0.0280429, recall 0.748418
2017-12-10T14:01:08.094708: step 663, loss 1.26946, acc 0.65625, prec 0.0280064, recall 0.748418
2017-12-10T14:01:08.282856: step 664, loss 1.18822, acc 0.71875, prec 0.0279766, recall 0.748418
2017-12-10T14:01:08.466855: step 665, loss 0.81304, acc 0.71875, prec 0.0279468, recall 0.748418
2017-12-10T14:01:08.656392: step 666, loss 0.960011, acc 0.765625, prec 0.0279221, recall 0.748418
2017-12-10T14:01:08.844429: step 667, loss 1.11687, acc 0.71875, prec 0.0278924, recall 0.748418
2017-12-10T14:01:09.035925: step 668, loss 0.822708, acc 0.765625, prec 0.0278678, recall 0.748418
2017-12-10T14:01:09.224569: step 669, loss 1.14135, acc 0.71875, prec 0.0278383, recall 0.748418
2017-12-10T14:01:09.412252: step 670, loss 1.49837, acc 0.734375, prec 0.0279248, recall 0.749211
2017-12-10T14:01:09.601450: step 671, loss 0.343519, acc 0.828125, prec 0.0279067, recall 0.749211
2017-12-10T14:01:09.790926: step 672, loss 0.693047, acc 0.84375, prec 0.0278903, recall 0.749211
2017-12-10T14:01:09.978660: step 673, loss 0.366162, acc 0.859375, prec 0.0278756, recall 0.749211
2017-12-10T14:01:10.167437: step 674, loss 0.47573, acc 0.78125, prec 0.0279097, recall 0.749606
2017-12-10T14:01:10.356824: step 675, loss 5.5136, acc 0.765625, prec 0.0278868, recall 0.748428
2017-12-10T14:01:10.546228: step 676, loss 1.54952, acc 0.84375, prec 0.0278721, recall 0.747253
2017-12-10T14:01:10.739828: step 677, loss 0.495344, acc 0.8125, prec 0.0278525, recall 0.747253
2017-12-10T14:01:10.930518: step 678, loss 0.685802, acc 0.8125, prec 0.027833, recall 0.747253
2017-12-10T14:01:11.120115: step 679, loss 1.11956, acc 0.765625, prec 0.0279222, recall 0.748044
2017-12-10T14:01:11.312502: step 680, loss 0.821975, acc 0.84375, prec 0.0279626, recall 0.748438
2017-12-10T14:01:11.498130: step 681, loss 7.98532, acc 0.8125, prec 0.028003, recall 0.746501
2017-12-10T14:01:11.688714: step 682, loss 0.59568, acc 0.78125, prec 0.0279802, recall 0.746501
2017-12-10T14:01:11.876580: step 683, loss 1.02404, acc 0.71875, prec 0.0279509, recall 0.746501
2017-12-10T14:01:12.067789: step 684, loss 8.23369, acc 0.828125, prec 0.0280477, recall 0.74613
2017-12-10T14:01:12.260255: step 685, loss 1.13724, acc 0.71875, prec 0.0280749, recall 0.746522
2017-12-10T14:01:12.452345: step 686, loss 1.28798, acc 0.671875, prec 0.0280406, recall 0.746522
2017-12-10T14:01:12.640404: step 687, loss 1.77807, acc 0.640625, prec 0.0280596, recall 0.746914
2017-12-10T14:01:12.832537: step 688, loss 1.82025, acc 0.609375, prec 0.028019, recall 0.746914
2017-12-10T14:01:13.020201: step 689, loss 2.7517, acc 0.453125, prec 0.0279623, recall 0.746914
2017-12-10T14:01:13.211636: step 690, loss 2.62233, acc 0.609375, prec 0.0279781, recall 0.747304
2017-12-10T14:01:13.398606: step 691, loss 2.10825, acc 0.5625, prec 0.027933, recall 0.747304
2017-12-10T14:01:13.587561: step 692, loss 2.25249, acc 0.515625, prec 0.0280508, recall 0.748466
2017-12-10T14:01:13.778980: step 693, loss 2.58005, acc 0.578125, prec 0.0281189, recall 0.749235
2017-12-10T14:01:13.975986: step 694, loss 1.29321, acc 0.671875, prec 0.0280851, recall 0.749235
2017-12-10T14:01:14.161889: step 695, loss 2.74796, acc 0.4375, prec 0.0280272, recall 0.749235
2017-12-10T14:01:14.346328: step 696, loss 3.43144, acc 0.515625, prec 0.0280347, recall 0.748476
2017-12-10T14:01:14.535924: step 697, loss 5.17657, acc 0.578125, prec 0.0280486, recall 0.74772
2017-12-10T14:01:14.731878: step 698, loss 1.52086, acc 0.640625, prec 0.0281225, recall 0.748485
2017-12-10T14:01:14.920515: step 699, loss 1.586, acc 0.578125, prec 0.0281346, recall 0.748865
2017-12-10T14:01:15.107263: step 700, loss 2.60133, acc 0.546875, prec 0.0280883, recall 0.748865
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_1/1512932334/checkpoints/model-700

2017-12-10T14:01:16.360677: step 701, loss 1.94383, acc 0.609375, prec 0.0281036, recall 0.749245
2017-12-10T14:01:16.547519: step 702, loss 1.70736, acc 0.578125, prec 0.0282256, recall 0.750376
2017-12-10T14:01:16.736899: step 703, loss 1.62932, acc 0.609375, prec 0.0281857, recall 0.750376
2017-12-10T14:01:16.921333: step 704, loss 1.37336, acc 0.6875, prec 0.0282087, recall 0.750751
2017-12-10T14:01:17.109874: step 705, loss 1.399, acc 0.71875, prec 0.0283444, recall 0.751868
2017-12-10T14:01:17.296158: step 706, loss 0.922528, acc 0.71875, prec 0.0284798, recall 0.752976
2017-12-10T14:01:17.481938: step 707, loss 0.761057, acc 0.765625, prec 0.0284557, recall 0.752976
2017-12-10T14:01:17.670361: step 708, loss 1.14133, acc 0.734375, prec 0.0284831, recall 0.753343
2017-12-10T14:01:17.859874: step 709, loss 1.00957, acc 0.6875, prec 0.0284512, recall 0.753343
2017-12-10T14:01:18.048838: step 710, loss 0.976626, acc 0.703125, prec 0.0284209, recall 0.753343
2017-12-10T14:01:18.236084: step 711, loss 0.670192, acc 0.78125, prec 0.028453, recall 0.753709
2017-12-10T14:01:18.420903: step 712, loss 0.69157, acc 0.8125, prec 0.0284883, recall 0.754074
2017-12-10T14:01:18.609321: step 713, loss 0.794324, acc 0.796875, prec 0.0285219, recall 0.754438
2017-12-10T14:01:18.796604: step 714, loss 6.7161, acc 0.734375, prec 0.0285507, recall 0.753687
2017-12-10T14:01:18.989507: step 715, loss 0.467704, acc 0.828125, prec 0.0285331, recall 0.753687
2017-12-10T14:01:19.174913: step 716, loss 0.732094, acc 0.828125, prec 0.0285156, recall 0.753687
2017-12-10T14:01:19.372053: step 717, loss 0.336466, acc 0.90625, prec 0.0285061, recall 0.753687
2017-12-10T14:01:19.556539: step 718, loss 0.336909, acc 0.84375, prec 0.0285443, recall 0.75405
2017-12-10T14:01:19.747296: step 719, loss 0.760751, acc 0.84375, prec 0.0285284, recall 0.75405
2017-12-10T14:01:19.935310: step 720, loss 19.337, acc 0.84375, prec 0.0285141, recall 0.752941
2017-12-10T14:01:20.128187: step 721, loss 1.16833, acc 0.765625, prec 0.0285985, recall 0.753666
2017-12-10T14:01:20.315430: step 722, loss 0.393268, acc 0.84375, prec 0.0285826, recall 0.753666
2017-12-10T14:01:20.502928: step 723, loss 0.977452, acc 0.796875, prec 0.0285619, recall 0.753666
2017-12-10T14:01:20.691839: step 724, loss 1.20839, acc 0.734375, prec 0.0285889, recall 0.754026
2017-12-10T14:01:20.876113: step 725, loss 1.24581, acc 0.8125, prec 0.0286237, recall 0.754386
2017-12-10T14:01:21.065688: step 726, loss 0.557722, acc 0.8125, prec 0.0286047, recall 0.754386
2017-12-10T14:01:21.253528: step 727, loss 0.505243, acc 0.828125, prec 0.0286411, recall 0.754745
2017-12-10T14:01:21.445363: step 728, loss 1.5447, acc 0.796875, prec 0.0286742, recall 0.755102
2017-12-10T14:01:21.639126: step 729, loss 0.329136, acc 0.890625, prec 0.0286631, recall 0.755102
2017-12-10T14:01:21.827045: step 730, loss 0.720261, acc 0.8125, prec 0.0286441, recall 0.755102
2017-12-10T14:01:22.017311: step 731, loss 3.63574, acc 0.8125, prec 0.0286804, recall 0.75436
2017-12-10T14:01:22.210431: step 732, loss 0.612529, acc 0.859375, prec 0.0287734, recall 0.755072
2017-12-10T14:01:22.396899: step 733, loss 1.10307, acc 0.703125, prec 0.0287432, recall 0.755072
2017-12-10T14:01:22.586098: step 734, loss 0.63971, acc 0.765625, prec 0.0287195, recall 0.755072
2017-12-10T14:01:22.774103: step 735, loss 0.553769, acc 0.765625, prec 0.0287492, recall 0.755427
2017-12-10T14:01:22.960199: step 736, loss 0.973847, acc 0.8125, prec 0.0287837, recall 0.75578
2017-12-10T14:01:23.145960: step 737, loss 0.877347, acc 0.71875, prec 0.0287552, recall 0.75578
2017-12-10T14:01:23.334825: step 738, loss 0.975448, acc 0.78125, prec 0.0288398, recall 0.756484
2017-12-10T14:01:23.525308: step 739, loss 0.681781, acc 0.828125, prec 0.028929, recall 0.757184
2017-12-10T14:01:23.716464: step 740, loss 4.00602, acc 0.734375, prec 0.0289036, recall 0.756098
2017-12-10T14:01:23.910818: step 741, loss 1.12707, acc 0.75, prec 0.0289315, recall 0.756447
2017-12-10T14:01:24.100181: step 742, loss 1.01393, acc 0.671875, prec 0.0288983, recall 0.756447
2017-12-10T14:01:24.285668: step 743, loss 0.926896, acc 0.828125, prec 0.028934, recall 0.756795
2017-12-10T14:01:24.476890: step 744, loss 0.665078, acc 0.8125, prec 0.028915, recall 0.756795
2017-12-10T14:01:24.663119: step 745, loss 12.5322, acc 0.75, prec 0.0289444, recall 0.756063
2017-12-10T14:01:24.850054: step 746, loss 0.509793, acc 0.84375, prec 0.0289816, recall 0.75641
2017-12-10T14:01:25.039026: step 747, loss 8.02186, acc 0.734375, prec 0.0289563, recall 0.755334
2017-12-10T14:01:25.225970: step 748, loss 1.03676, acc 0.78125, prec 0.0291458, recall 0.756719
2017-12-10T14:01:25.413901: step 749, loss 1.17725, acc 0.734375, prec 0.0293301, recall 0.758087
2017-12-10T14:01:25.602642: step 750, loss 1.43526, acc 0.59375, prec 0.0293414, recall 0.758427
2017-12-10T14:01:25.791501: step 751, loss 1.12301, acc 0.734375, prec 0.0294197, recall 0.759104
2017-12-10T14:01:25.976062: step 752, loss 1.31121, acc 0.6875, prec 0.0293878, recall 0.759104
2017-12-10T14:01:26.164048: step 753, loss 3.99524, acc 0.734375, prec 0.0293624, recall 0.758042
2017-12-10T14:01:26.352882: step 754, loss 1.1233, acc 0.671875, prec 0.0293815, recall 0.75838
2017-12-10T14:01:26.540353: step 755, loss 1.29069, acc 0.734375, prec 0.0293545, recall 0.75838
2017-12-10T14:01:26.727819: step 756, loss 1.50482, acc 0.578125, prec 0.0294689, recall 0.759388
2017-12-10T14:01:26.915043: step 757, loss 1.14406, acc 0.765625, prec 0.0294451, recall 0.759388
2017-12-10T14:01:27.101503: step 758, loss 1.44708, acc 0.640625, prec 0.0294086, recall 0.759388
2017-12-10T14:01:27.288747: step 759, loss 1.30778, acc 0.625, prec 0.0293706, recall 0.759388
2017-12-10T14:01:27.478099: step 760, loss 1.19041, acc 0.65625, prec 0.0293359, recall 0.759388
2017-12-10T14:01:27.663304: step 761, loss 1.67205, acc 0.609375, prec 0.0292966, recall 0.759388
2017-12-10T14:01:27.849382: step 762, loss 2.2926, acc 0.703125, prec 0.0293724, recall 0.759003
2017-12-10T14:01:28.037209: step 763, loss 1.19455, acc 0.6875, prec 0.0293409, recall 0.759003
2017-12-10T14:01:28.222306: step 764, loss 1.44974, acc 0.609375, prec 0.0293017, recall 0.759003
2017-12-10T14:01:28.412299: step 765, loss 3.56487, acc 0.765625, prec 0.0293316, recall 0.758287
2017-12-10T14:01:28.599541: step 766, loss 1.16568, acc 0.703125, prec 0.0293537, recall 0.758621
2017-12-10T14:01:28.786454: step 767, loss 0.718531, acc 0.828125, prec 0.0293882, recall 0.758953
2017-12-10T14:01:28.980108: step 768, loss 0.821873, acc 0.734375, prec 0.0294133, recall 0.759285
2017-12-10T14:01:29.165137: step 769, loss 4.04954, acc 0.75, prec 0.0293898, recall 0.758242
2017-12-10T14:01:29.359786: step 770, loss 0.826391, acc 0.8125, prec 0.0294227, recall 0.758573
2017-12-10T14:01:29.548577: step 771, loss 1.0826, acc 0.734375, prec 0.0294477, recall 0.758904
2017-12-10T14:01:29.735447: step 772, loss 1.29779, acc 0.65625, prec 0.0295164, recall 0.759563
2017-12-10T14:01:29.920894: step 773, loss 0.800575, acc 0.796875, prec 0.0295475, recall 0.759891
2017-12-10T14:01:30.107758: step 774, loss 1.19119, acc 0.6875, prec 0.029619, recall 0.760544
2017-12-10T14:01:30.296369: step 775, loss 1.8998, acc 0.734375, prec 0.0297465, recall 0.761518
2017-12-10T14:01:30.483117: step 776, loss 1.24188, acc 0.71875, prec 0.0297695, recall 0.76184
2017-12-10T14:01:30.667948: step 777, loss 0.667344, acc 0.796875, prec 0.029749, recall 0.76184
2017-12-10T14:01:30.857718: step 778, loss 1.36759, acc 0.6875, prec 0.0297176, recall 0.76184
2017-12-10T14:01:31.053815: step 779, loss 0.677435, acc 0.75, prec 0.0296925, recall 0.76184
2017-12-10T14:01:31.245755: step 780, loss 2.19994, acc 0.875, prec 0.0297327, recall 0.761134
2017-12-10T14:01:31.437665: step 781, loss 1.16875, acc 0.796875, prec 0.0299168, recall 0.762416
2017-12-10T14:01:31.629795: step 782, loss 1.11153, acc 0.71875, prec 0.0298884, recall 0.762416
2017-12-10T14:01:31.814202: step 783, loss 0.52845, acc 0.8125, prec 0.0299206, recall 0.762735
2017-12-10T14:01:32.003037: step 784, loss 0.709556, acc 0.78125, prec 0.0298986, recall 0.762735
2017-12-10T14:01:32.193305: step 785, loss 0.566083, acc 0.796875, prec 0.0299291, recall 0.763052
2017-12-10T14:01:32.383769: step 786, loss 0.625863, acc 0.75, prec 0.029904, recall 0.763052
2017-12-10T14:01:32.569140: step 787, loss 0.800879, acc 0.84375, prec 0.0298883, recall 0.763052
2017-12-10T14:01:32.759462: step 788, loss 0.391676, acc 0.84375, prec 0.0298726, recall 0.763052
2017-12-10T14:01:32.948516: step 789, loss 0.40887, acc 0.84375, prec 0.029857, recall 0.763052
2017-12-10T14:01:33.133747: step 790, loss 4.79101, acc 0.890625, prec 0.0298492, recall 0.761015
2017-12-10T14:01:33.330189: step 791, loss 4.38325, acc 0.90625, prec 0.0298922, recall 0.76032
2017-12-10T14:01:33.522467: step 792, loss 0.479949, acc 0.84375, prec 0.0298765, recall 0.76032
2017-12-10T14:01:33.710600: step 793, loss 0.538092, acc 0.78125, prec 0.0298546, recall 0.76032
2017-12-10T14:01:33.908198: step 794, loss 27.1857, acc 0.8125, prec 0.029839, recall 0.7583
2017-12-10T14:01:34.099291: step 795, loss 1.13205, acc 0.65625, prec 0.0298048, recall 0.7583
2017-12-10T14:01:34.283927: step 796, loss 1.27081, acc 0.703125, prec 0.0298764, recall 0.75894
2017-12-10T14:01:34.471693: step 797, loss 7.78209, acc 0.609375, prec 0.0298896, recall 0.758256
2017-12-10T14:01:34.658203: step 798, loss 1.37606, acc 0.703125, prec 0.029961, recall 0.758893
2017-12-10T14:01:34.850282: step 799, loss 2.30438, acc 0.453125, prec 0.0299065, recall 0.758893
2017-12-10T14:01:35.037472: step 800, loss 2.69112, acc 0.5, prec 0.0299072, recall 0.759211
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_1/1512932334/checkpoints/model-800

2017-12-10T14:01:36.350387: step 801, loss 2.02552, acc 0.4375, prec 0.0299017, recall 0.759527
2017-12-10T14:01:36.538243: step 802, loss 2.83273, acc 0.5, prec 0.0298523, recall 0.759527
2017-12-10T14:01:36.722454: step 803, loss 2.30253, acc 0.46875, prec 0.0299, recall 0.760157
2017-12-10T14:01:36.912596: step 804, loss 2.36952, acc 0.546875, prec 0.0298554, recall 0.760157
2017-12-10T14:01:37.099385: step 805, loss 2.24973, acc 0.546875, prec 0.0298109, recall 0.760157
2017-12-10T14:01:37.283777: step 806, loss 2.39587, acc 0.453125, prec 0.0298071, recall 0.760471
2017-12-10T14:01:37.473966: step 807, loss 2.66066, acc 0.53125, prec 0.029811, recall 0.760784
2017-12-10T14:01:37.661008: step 808, loss 1.59782, acc 0.46875, prec 0.0298088, recall 0.761097
2017-12-10T14:01:37.846634: step 809, loss 2.10306, acc 0.484375, prec 0.0297586, recall 0.761097
2017-12-10T14:01:38.034461: step 810, loss 1.77138, acc 0.625, prec 0.0297716, recall 0.761408
2017-12-10T14:01:38.220844: step 811, loss 1.35434, acc 0.59375, prec 0.0297816, recall 0.761719
2017-12-10T14:01:38.402990: step 812, loss 0.775305, acc 0.75, prec 0.0298561, recall 0.762338
2017-12-10T14:01:38.589780: step 813, loss 1.10905, acc 0.6875, prec 0.0298257, recall 0.762338
2017-12-10T14:01:38.775267: step 814, loss 1.42975, acc 0.6875, prec 0.0297954, recall 0.762338
2017-12-10T14:01:38.961599: step 815, loss 0.814787, acc 0.78125, prec 0.0297743, recall 0.762338
2017-12-10T14:01:39.152601: step 816, loss 5.07639, acc 0.765625, prec 0.0298515, recall 0.761966
2017-12-10T14:01:39.344403: step 817, loss 1.06606, acc 0.6875, prec 0.0298704, recall 0.762274
2017-12-10T14:01:39.530754: step 818, loss 0.965822, acc 0.75, prec 0.0298953, recall 0.762581
2017-12-10T14:01:39.722030: step 819, loss 0.922172, acc 0.765625, prec 0.0299707, recall 0.763192
2017-12-10T14:01:39.911514: step 820, loss 0.574212, acc 0.84375, prec 0.0299555, recall 0.763192
2017-12-10T14:01:40.095717: step 821, loss 0.336582, acc 0.875, prec 0.0299434, recall 0.763192
2017-12-10T14:01:40.280827: step 822, loss 0.352524, acc 0.84375, prec 0.0299283, recall 0.763192
2017-12-10T14:01:40.471245: step 823, loss 0.276221, acc 0.90625, prec 0.0299193, recall 0.763192
2017-12-10T14:01:40.663235: step 824, loss 1.69199, acc 0.859375, prec 0.0299072, recall 0.762211
2017-12-10T14:01:40.854769: step 825, loss 0.455734, acc 0.890625, prec 0.0299456, recall 0.762516
2017-12-10T14:01:41.043204: step 826, loss 0.440152, acc 0.828125, prec 0.029929, recall 0.762516
2017-12-10T14:01:41.231470: step 827, loss 0.473109, acc 0.828125, prec 0.0299612, recall 0.762821
2017-12-10T14:01:41.419805: step 828, loss 5.6674, acc 0.859375, prec 0.029998, recall 0.762148
2017-12-10T14:01:41.614476: step 829, loss 0.530951, acc 0.90625, prec 0.0300377, recall 0.762452
2017-12-10T14:01:41.804570: step 830, loss 0.2687, acc 0.9375, prec 0.0300317, recall 0.762452
2017-12-10T14:01:41.997972: step 831, loss 0.317602, acc 0.890625, prec 0.0300211, recall 0.762452
2017-12-10T14:01:42.184507: step 832, loss 0.928809, acc 0.84375, prec 0.0300548, recall 0.762755
2017-12-10T14:01:42.376214: step 833, loss 0.35611, acc 0.875, prec 0.0300427, recall 0.762755
2017-12-10T14:01:42.562771: step 834, loss 0.842887, acc 0.734375, prec 0.0300171, recall 0.762755
2017-12-10T14:01:42.755249: step 835, loss 0.558771, acc 0.875, prec 0.030005, recall 0.762755
2017-12-10T14:01:42.948127: step 836, loss 0.519983, acc 0.875, prec 0.0300416, recall 0.763057
2017-12-10T14:01:43.136229: step 837, loss 0.45695, acc 0.84375, prec 0.0300266, recall 0.763057
2017-12-10T14:01:43.325506: step 838, loss 17.8675, acc 0.828125, prec 0.0300115, recall 0.762087
2017-12-10T14:01:43.515958: step 839, loss 0.500061, acc 0.859375, prec 0.029998, recall 0.762087
2017-12-10T14:01:43.705586: step 840, loss 0.440926, acc 0.859375, prec 0.0299845, recall 0.762087
2017-12-10T14:01:43.901784: step 841, loss 9.16212, acc 0.796875, prec 0.030015, recall 0.761421
2017-12-10T14:01:44.090858: step 842, loss 7.26106, acc 0.890625, prec 0.030006, recall 0.760456
2017-12-10T14:01:44.281260: step 843, loss 2.96567, acc 0.8125, prec 0.030038, recall 0.759798
2017-12-10T14:01:44.468907: step 844, loss 3.91789, acc 0.71875, prec 0.0300609, recall 0.759143
2017-12-10T14:01:44.657604: step 845, loss 5.69369, acc 0.546875, prec 0.0300673, recall 0.758491
2017-12-10T14:01:44.847354: step 846, loss 1.43782, acc 0.578125, prec 0.0300269, recall 0.758491
2017-12-10T14:01:45.033696: step 847, loss 2.29808, acc 0.546875, prec 0.0300318, recall 0.758794
2017-12-10T14:01:45.220063: step 848, loss 3.06395, acc 0.359375, prec 0.0299707, recall 0.758794
2017-12-10T14:01:45.409235: step 849, loss 2.86025, acc 0.453125, prec 0.0299668, recall 0.759097
2017-12-10T14:01:45.592688: step 850, loss 3.14751, acc 0.46875, prec 0.0299644, recall 0.759399
2017-12-10T14:01:45.777105: step 851, loss 4.11694, acc 0.296875, prec 0.0299457, recall 0.7597
2017-12-10T14:01:45.961869: step 852, loss 2.57276, acc 0.46875, prec 0.0298956, recall 0.7597
2017-12-10T14:01:46.146545: step 853, loss 4.19747, acc 0.265625, prec 0.0299219, recall 0.7603
2017-12-10T14:01:46.334683: step 854, loss 3.5571, acc 0.359375, prec 0.0299093, recall 0.760598
2017-12-10T14:01:46.518693: step 855, loss 3.00926, acc 0.390625, prec 0.0298997, recall 0.760897
2017-12-10T14:01:46.702578: step 856, loss 4.10475, acc 0.390625, prec 0.0298442, recall 0.75995
2017-12-10T14:01:46.888728: step 857, loss 2.95068, acc 0.515625, prec 0.029941, recall 0.760843
2017-12-10T14:01:47.072147: step 858, loss 2.30734, acc 0.453125, prec 0.0299844, recall 0.761434
2017-12-10T14:01:47.259682: step 859, loss 2.71747, acc 0.5625, prec 0.0299908, recall 0.761728
2017-12-10T14:01:47.447635: step 860, loss 2.55703, acc 0.46875, prec 0.0300354, recall 0.762315
2017-12-10T14:01:47.634833: step 861, loss 2.17075, acc 0.515625, prec 0.0300373, recall 0.762608
2017-12-10T14:01:47.818322: step 862, loss 1.28205, acc 0.59375, prec 0.0299995, recall 0.762608
2017-12-10T14:01:48.004104: step 863, loss 1.58256, acc 0.625, prec 0.0300116, recall 0.762899
2017-12-10T14:01:48.186133: step 864, loss 1.75611, acc 0.609375, prec 0.0299754, recall 0.762899
2017-12-10T14:01:48.374883: step 865, loss 0.734782, acc 0.75, prec 0.029999, recall 0.76319
2017-12-10T14:01:48.561643: step 866, loss 5.5575, acc 0.703125, prec 0.029973, recall 0.762255
2017-12-10T14:01:48.752411: step 867, loss 0.711646, acc 0.71875, prec 0.0299937, recall 0.762546
2017-12-10T14:01:48.946831: step 868, loss 1.03781, acc 0.75, prec 0.0299707, recall 0.762546
2017-12-10T14:01:49.137890: step 869, loss 0.989218, acc 0.734375, prec 0.0299928, recall 0.762836
2017-12-10T14:01:49.325837: step 870, loss 0.28235, acc 0.890625, prec 0.0299827, recall 0.762836
2017-12-10T14:01:49.514366: step 871, loss 0.186532, acc 0.90625, prec 0.0300207, recall 0.763126
2017-12-10T14:01:49.703122: step 872, loss 0.737202, acc 0.765625, prec 0.0300456, recall 0.763415
2017-12-10T14:01:49.888570: step 873, loss 14.9917, acc 0.890625, prec 0.0300398, recall 0.760632
2017-12-10T14:01:50.079497: step 874, loss 0.827432, acc 0.796875, prec 0.0301141, recall 0.761212
2017-12-10T14:01:50.265547: step 875, loss 0.415294, acc 0.84375, prec 0.0300997, recall 0.761212
2017-12-10T14:01:50.454318: step 876, loss 1.03671, acc 0.75, prec 0.0300766, recall 0.761212
2017-12-10T14:01:50.642613: step 877, loss 0.728107, acc 0.765625, prec 0.030055, recall 0.761212
2017-12-10T14:01:50.826229: step 878, loss 1.15392, acc 0.796875, prec 0.0300363, recall 0.761212
2017-12-10T14:01:51.016793: step 879, loss 2.60553, acc 0.625, prec 0.0300497, recall 0.76058
2017-12-10T14:01:51.205282: step 880, loss 0.793223, acc 0.703125, prec 0.0300224, recall 0.76058
2017-12-10T14:01:51.398946: step 881, loss 0.803136, acc 0.84375, prec 0.0300544, recall 0.76087
2017-12-10T14:01:51.590092: step 882, loss 0.786151, acc 0.84375, prec 0.0300863, recall 0.761158
2017-12-10T14:01:51.778884: step 883, loss 8.8913, acc 0.671875, prec 0.0301038, recall 0.760529
2017-12-10T14:01:51.968741: step 884, loss 1.15112, acc 0.75, prec 0.0301732, recall 0.761104
2017-12-10T14:01:52.160509: step 885, loss 1.18085, acc 0.703125, prec 0.030146, recall 0.761104
2017-12-10T14:01:52.347073: step 886, loss 1.9111, acc 0.53125, prec 0.0301491, recall 0.761391
2017-12-10T14:01:52.536746: step 887, loss 2.12106, acc 0.578125, prec 0.0301565, recall 0.761677
2017-12-10T14:01:52.731518: step 888, loss 1.14104, acc 0.640625, prec 0.0301696, recall 0.761962
2017-12-10T14:01:52.920322: step 889, loss 3.85886, acc 0.703125, prec 0.0301439, recall 0.761051
2017-12-10T14:01:53.108115: step 890, loss 1.43289, acc 0.640625, prec 0.0302028, recall 0.761621
2017-12-10T14:01:53.299736: step 891, loss 1.1462, acc 0.703125, prec 0.0302673, recall 0.762188
2017-12-10T14:01:53.486868: step 892, loss 1.49442, acc 0.640625, prec 0.0302344, recall 0.762188
2017-12-10T14:01:53.674368: step 893, loss 1.1859, acc 0.734375, prec 0.0302102, recall 0.762188
2017-12-10T14:01:53.860455: step 894, loss 1.00125, acc 0.734375, prec 0.0302317, recall 0.76247
2017-12-10T14:01:54.046588: step 895, loss 0.96204, acc 0.703125, prec 0.0302047, recall 0.76247
2017-12-10T14:01:54.234100: step 896, loss 1.72109, acc 0.6875, prec 0.0302218, recall 0.762752
2017-12-10T14:01:54.427284: step 897, loss 5.92123, acc 0.65625, prec 0.0302376, recall 0.76213
2017-12-10T14:01:54.615401: step 898, loss 0.879038, acc 0.75, prec 0.0302604, recall 0.762411
2017-12-10T14:01:54.803116: step 899, loss 1.00755, acc 0.671875, prec 0.0302306, recall 0.762411
2017-12-10T14:01:54.988183: step 900, loss 1.68862, acc 0.640625, prec 0.0302434, recall 0.762692
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_1/1512932334/checkpoints/model-900

2017-12-10T14:01:56.294627: step 901, loss 1.19367, acc 0.71875, prec 0.030218, recall 0.762692
2017-12-10T14:01:56.485609: step 902, loss 1.60596, acc 0.640625, prec 0.0302308, recall 0.762972
2017-12-10T14:01:56.675862: step 903, loss 1.73095, acc 0.640625, prec 0.0301984, recall 0.762972
2017-12-10T14:01:56.866148: step 904, loss 1.16734, acc 0.71875, prec 0.0302635, recall 0.763529
2017-12-10T14:01:57.053363: step 905, loss 1.05777, acc 0.703125, prec 0.0302819, recall 0.763807
2017-12-10T14:01:57.240781: step 906, loss 1.25032, acc 0.71875, prec 0.0302565, recall 0.763807
2017-12-10T14:01:57.429418: step 907, loss 0.771787, acc 0.796875, prec 0.0302833, recall 0.764085
2017-12-10T14:01:57.617526: step 908, loss 0.483821, acc 0.84375, prec 0.0303143, recall 0.764361
2017-12-10T14:01:57.804901: step 909, loss 0.875892, acc 0.75, prec 0.0302918, recall 0.764361
2017-12-10T14:01:57.997819: step 910, loss 0.448968, acc 0.921875, prec 0.0302847, recall 0.764361
2017-12-10T14:01:58.190540: step 911, loss 0.645151, acc 0.859375, prec 0.0303621, recall 0.764912
2017-12-10T14:01:58.379225: step 912, loss 0.633822, acc 0.828125, prec 0.0303466, recall 0.764912
2017-12-10T14:01:58.573278: step 913, loss 0.612481, acc 0.796875, prec 0.0303283, recall 0.764912
2017-12-10T14:01:58.763419: step 914, loss 0.329177, acc 0.890625, prec 0.0303185, recall 0.764912
2017-12-10T14:01:58.957095: step 915, loss 0.606118, acc 0.859375, prec 0.0303508, recall 0.765187
2017-12-10T14:01:59.149775: step 916, loss 25.5857, acc 0.828125, prec 0.0303844, recall 0.762791
2017-12-10T14:01:59.344988: step 917, loss 0.436339, acc 0.890625, prec 0.0303746, recall 0.762791
2017-12-10T14:01:59.533002: step 918, loss 0.865821, acc 0.84375, prec 0.0304503, recall 0.763341
2017-12-10T14:01:59.723600: step 919, loss 4.89272, acc 0.875, prec 0.0304853, recall 0.762731
2017-12-10T14:01:59.915330: step 920, loss 0.642864, acc 0.78125, prec 0.0306, recall 0.763552
2017-12-10T14:02:00.106825: step 921, loss 0.866182, acc 0.78125, prec 0.030625, recall 0.763825
2017-12-10T14:02:00.297118: step 922, loss 0.592039, acc 0.875, prec 0.0307927, recall 0.764908
2017-12-10T14:02:00.482737: step 923, loss 1.53599, acc 0.59375, prec 0.0308004, recall 0.765178
2017-12-10T14:02:00.673004: step 924, loss 1.17296, acc 0.734375, prec 0.0308656, recall 0.765714
2017-12-10T14:02:00.866119: step 925, loss 2.20917, acc 0.625, prec 0.0309207, recall 0.766249
2017-12-10T14:02:01.055752: step 926, loss 1.22317, acc 0.671875, prec 0.0309354, recall 0.766515
2017-12-10T14:02:01.247849: step 927, loss 1.34394, acc 0.734375, prec 0.0309113, recall 0.766515
2017-12-10T14:02:01.437905: step 928, loss 1.5699, acc 0.609375, prec 0.0308758, recall 0.766515
2017-12-10T14:02:01.627408: step 929, loss 1.41282, acc 0.609375, prec 0.0308404, recall 0.766515
2017-12-10T14:02:01.812666: step 930, loss 0.703522, acc 0.828125, prec 0.0308249, recall 0.766515
2017-12-10T14:02:01.999558: step 931, loss 1.11682, acc 0.71875, prec 0.0308439, recall 0.76678
2017-12-10T14:02:02.184991: step 932, loss 0.658334, acc 0.84375, prec 0.0308298, recall 0.76678
2017-12-10T14:02:02.380894: step 933, loss 0.843447, acc 0.75, prec 0.0308072, recall 0.76678
2017-12-10T14:02:02.569765: step 934, loss 3.9621, acc 0.703125, prec 0.0308261, recall 0.766175
2017-12-10T14:02:02.759036: step 935, loss 5.33325, acc 0.765625, prec 0.0308507, recall 0.765572
2017-12-10T14:02:02.949775: step 936, loss 0.649795, acc 0.71875, prec 0.0308254, recall 0.765572
2017-12-10T14:02:03.136333: step 937, loss 0.932405, acc 0.6875, prec 0.0308414, recall 0.765837
2017-12-10T14:02:03.322860: step 938, loss 1.38429, acc 0.609375, prec 0.0308504, recall 0.766102
2017-12-10T14:02:03.509613: step 939, loss 0.893889, acc 0.78125, prec 0.0309189, recall 0.766629
2017-12-10T14:02:03.699183: step 940, loss 1.23916, acc 0.640625, prec 0.0308866, recall 0.766629
2017-12-10T14:02:03.886332: step 941, loss 1.60623, acc 0.609375, prec 0.0308516, recall 0.766629
2017-12-10T14:02:04.072197: step 942, loss 0.940606, acc 0.796875, prec 0.0308334, recall 0.766629
2017-12-10T14:02:04.262358: step 943, loss 1.12486, acc 0.71875, prec 0.0308083, recall 0.766629
2017-12-10T14:02:04.449275: step 944, loss 3.24241, acc 0.75, prec 0.0307873, recall 0.765766
2017-12-10T14:02:04.640833: step 945, loss 0.881438, acc 0.75, prec 0.0308527, recall 0.766292
2017-12-10T14:02:04.825356: step 946, loss 2.19798, acc 0.640625, prec 0.0308645, recall 0.766554
2017-12-10T14:02:05.014738: step 947, loss 1.55023, acc 0.71875, prec 0.0308832, recall 0.766816
2017-12-10T14:02:05.204293: step 948, loss 0.878622, acc 0.734375, prec 0.0308595, recall 0.766816
2017-12-10T14:02:05.391438: step 949, loss 5.58644, acc 0.765625, prec 0.03084, recall 0.765957
2017-12-10T14:02:05.579382: step 950, loss 5.21872, acc 0.765625, prec 0.0308642, recall 0.765363
2017-12-10T14:02:05.770097: step 951, loss 1.40831, acc 0.71875, prec 0.0309264, recall 0.765886
2017-12-10T14:02:05.960577: step 952, loss 3.74161, acc 0.671875, prec 0.0309858, recall 0.765556
2017-12-10T14:02:06.154156: step 953, loss 3.42885, acc 0.59375, prec 0.030951, recall 0.764706
2017-12-10T14:02:06.346755: step 954, loss 2.46943, acc 0.421875, prec 0.0308996, recall 0.764706
2017-12-10T14:02:06.538064: step 955, loss 2.53025, acc 0.515625, prec 0.0309001, recall 0.764967
2017-12-10T14:02:06.722825: step 956, loss 2.59807, acc 0.453125, prec 0.0308518, recall 0.764967
2017-12-10T14:02:06.909695: step 957, loss 2.0433, acc 0.53125, prec 0.0310268, recall 0.766262
2017-12-10T14:02:07.097777: step 958, loss 2.33548, acc 0.484375, prec 0.0310243, recall 0.76652
2017-12-10T14:02:07.283181: step 959, loss 2.78737, acc 0.421875, prec 0.0309733, recall 0.76652
2017-12-10T14:02:07.485904: step 960, loss 1.51301, acc 0.546875, prec 0.0309333, recall 0.76652
2017-12-10T14:02:07.673078: step 961, loss 2.77183, acc 0.4375, prec 0.0309699, recall 0.767033
2017-12-10T14:02:07.857103: step 962, loss 5.1213, acc 0.46875, prec 0.0309246, recall 0.766191
2017-12-10T14:02:08.043083: step 963, loss 1.90324, acc 0.515625, prec 0.0309251, recall 0.766447
2017-12-10T14:02:08.230672: step 964, loss 2.2471, acc 0.4375, prec 0.0308759, recall 0.766447
2017-12-10T14:02:08.418702: step 965, loss 2.39833, acc 0.546875, prec 0.0308792, recall 0.766703
2017-12-10T14:02:08.602899: step 966, loss 1.85801, acc 0.578125, prec 0.0308851, recall 0.766958
2017-12-10T14:02:08.787362: step 967, loss 2.39205, acc 0.5625, prec 0.0308897, recall 0.767213
2017-12-10T14:02:08.973571: step 968, loss 1.86307, acc 0.59375, prec 0.0308544, recall 0.767213
2017-12-10T14:02:09.162204: step 969, loss 1.87812, acc 0.59375, prec 0.0308618, recall 0.767467
2017-12-10T14:02:09.349868: step 970, loss 1.40988, acc 0.6875, prec 0.0308347, recall 0.767467
2017-12-10T14:02:09.536403: step 971, loss 1.32437, acc 0.71875, prec 0.0308953, recall 0.767974
2017-12-10T14:02:09.723636: step 972, loss 2.43344, acc 0.65625, prec 0.0308669, recall 0.767138
2017-12-10T14:02:09.916230: step 973, loss 0.718283, acc 0.8125, prec 0.0308931, recall 0.767391
2017-12-10T14:02:10.100680: step 974, loss 1.04776, acc 0.71875, prec 0.0308688, recall 0.767391
2017-12-10T14:02:10.289724: step 975, loss 10.8949, acc 0.8125, prec 0.0308539, recall 0.766558
2017-12-10T14:02:10.480612: step 976, loss 0.920414, acc 0.640625, prec 0.030823, recall 0.766558
2017-12-10T14:02:10.665419: step 977, loss 1.12363, acc 0.703125, prec 0.0308397, recall 0.766811
2017-12-10T14:02:10.851397: step 978, loss 0.831396, acc 0.765625, prec 0.0308195, recall 0.766811
2017-12-10T14:02:11.040244: step 979, loss 0.331267, acc 0.828125, prec 0.030847, recall 0.767064
2017-12-10T14:02:11.227883: step 980, loss 0.684305, acc 0.84375, prec 0.0308336, recall 0.767064
2017-12-10T14:02:11.415919: step 981, loss 1.6657, acc 0.8125, prec 0.0309018, recall 0.767568
2017-12-10T14:02:11.605860: step 982, loss 0.692522, acc 0.8125, prec 0.0309278, recall 0.767819
2017-12-10T14:02:11.794387: step 983, loss 0.52173, acc 0.796875, prec 0.0309525, recall 0.768069
2017-12-10T14:02:11.979049: step 984, loss 0.935018, acc 0.765625, prec 0.0309323, recall 0.768069
2017-12-10T14:02:12.167771: step 985, loss 0.414552, acc 0.84375, prec 0.030961, recall 0.768319
2017-12-10T14:02:12.362472: step 986, loss 0.450397, acc 0.859375, prec 0.0309489, recall 0.768319
2017-12-10T14:02:12.553618: step 987, loss 0.5944, acc 0.875, prec 0.0310222, recall 0.768817
2017-12-10T14:02:12.742043: step 988, loss 0.229773, acc 0.9375, prec 0.0310168, recall 0.768817
2017-12-10T14:02:12.930375: step 989, loss 0.566167, acc 0.875, prec 0.0310061, recall 0.768817
2017-12-10T14:02:13.123407: step 990, loss 0.58646, acc 0.84375, prec 0.0310346, recall 0.769065
2017-12-10T14:02:13.311740: step 991, loss 0.544781, acc 0.875, prec 0.0310659, recall 0.769313
2017-12-10T14:02:13.501631: step 992, loss 0.353335, acc 0.90625, prec 0.0310578, recall 0.769313
2017-12-10T14:02:13.690807: step 993, loss 9.69299, acc 0.953125, prec 0.0310551, recall 0.768489
2017-12-10T14:02:13.869310: step 994, loss 5.30661, acc 0.882353, prec 0.0310484, recall 0.767666
2017-12-10T14:02:14.069391: step 995, loss 0.156573, acc 0.953125, prec 0.0310443, recall 0.767666
2017-12-10T14:02:14.258242: step 996, loss 1.54191, acc 0.84375, prec 0.0311148, recall 0.768162
2017-12-10T14:02:14.450388: step 997, loss 0.554735, acc 0.90625, prec 0.0311486, recall 0.76841
2017-12-10T14:02:14.636482: step 998, loss 0.964314, acc 0.734375, prec 0.0311257, recall 0.76841
2017-12-10T14:02:14.824607: step 999, loss 0.667204, acc 0.828125, prec 0.0311528, recall 0.768657
2017-12-10T14:02:15.013998: step 1000, loss 1.6245, acc 0.75, prec 0.0311731, recall 0.768903
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_1/1512932334/checkpoints/model-1000

2017-12-10T14:02:16.294788: step 1001, loss 4.05143, acc 0.765625, prec 0.0311961, recall 0.768332
2017-12-10T14:02:16.483802: step 1002, loss 0.712013, acc 0.796875, prec 0.0311786, recall 0.768332
2017-12-10T14:02:16.668146: step 1003, loss 3.26814, acc 0.734375, prec 0.0311988, recall 0.767762
2017-12-10T14:02:16.855582: step 1004, loss 1.37348, acc 0.671875, prec 0.0311706, recall 0.767762
2017-12-10T14:02:17.044526: step 1005, loss 5.72093, acc 0.65625, prec 0.0311425, recall 0.766949
2017-12-10T14:02:17.234342: step 1006, loss 1.66442, acc 0.578125, prec 0.0311063, recall 0.766949
2017-12-10T14:02:17.419249: step 1007, loss 1.75792, acc 0.59375, prec 0.0310716, recall 0.766949
2017-12-10T14:02:17.605058: step 1008, loss 2.01774, acc 0.5625, prec 0.0310343, recall 0.766949
2017-12-10T14:02:17.791471: step 1009, loss 1.83524, acc 0.59375, prec 0.0310413, recall 0.767196
2017-12-10T14:02:17.977430: step 1010, loss 2.36873, acc 0.53125, prec 0.0310429, recall 0.767442
2017-12-10T14:02:18.162132: step 1011, loss 2.36523, acc 0.609375, prec 0.0311339, recall 0.768177
2017-12-10T14:02:18.351106: step 1012, loss 2.16867, acc 0.484375, prec 0.0311314, recall 0.768421
2017-12-10T14:02:18.539479: step 1013, loss 1.76084, acc 0.609375, prec 0.0311808, recall 0.768908
2017-12-10T14:02:18.730273: step 1014, loss 1.49216, acc 0.5625, prec 0.0311849, recall 0.76915
2017-12-10T14:02:18.917695: step 1015, loss 1.67516, acc 0.609375, prec 0.0312752, recall 0.769874
2017-12-10T14:02:19.103077: step 1016, loss 1.97046, acc 0.484375, prec 0.0312314, recall 0.769874
2017-12-10T14:02:19.288020: step 1017, loss 1.25842, acc 0.703125, prec 0.0312063, recall 0.769874
2017-12-10T14:02:19.473224: step 1018, loss 1.14029, acc 0.671875, prec 0.0313016, recall 0.770594
2017-12-10T14:02:19.661058: step 1019, loss 0.929955, acc 0.6875, prec 0.0313571, recall 0.771072
2017-12-10T14:02:19.850275: step 1020, loss 2.03167, acc 0.609375, prec 0.0313649, recall 0.77131
2017-12-10T14:02:20.042276: step 1021, loss 1.23636, acc 0.78125, prec 0.0313873, recall 0.771547
2017-12-10T14:02:20.229472: step 1022, loss 1.34014, acc 0.78125, prec 0.0314096, recall 0.771784
2017-12-10T14:02:20.421616: step 1023, loss 0.96389, acc 0.6875, prec 0.0314648, recall 0.772257
2017-12-10T14:02:20.605749: step 1024, loss 0.548099, acc 0.84375, prec 0.0314924, recall 0.772492
2017-12-10T14:02:20.796447: step 1025, loss 0.331571, acc 0.890625, prec 0.0314831, recall 0.772492
2017-12-10T14:02:20.984455: step 1026, loss 0.71996, acc 0.875, prec 0.0315949, recall 0.773196
2017-12-10T14:02:21.176073: step 1027, loss 0.539936, acc 0.84375, prec 0.0316224, recall 0.773429
2017-12-10T14:02:21.365469: step 1028, loss 0.776245, acc 0.921875, prec 0.0316565, recall 0.773663
2017-12-10T14:02:21.556772: step 1029, loss 0.632651, acc 0.828125, prec 0.0316826, recall 0.773895
2017-12-10T14:02:21.747509: step 1030, loss 0.451076, acc 0.875, prec 0.0317534, recall 0.774359
2017-12-10T14:02:21.938053: step 1031, loss 0.174826, acc 0.921875, prec 0.0317874, recall 0.77459
2017-12-10T14:02:22.127383: step 1032, loss 0.842521, acc 0.859375, prec 0.0318161, recall 0.774821
2017-12-10T14:02:22.317980: step 1033, loss 0.436486, acc 0.859375, prec 0.0318041, recall 0.774821
2017-12-10T14:02:22.507199: step 1034, loss 0.49682, acc 0.875, prec 0.0318747, recall 0.775281
2017-12-10T14:02:22.696018: step 1035, loss 0.649916, acc 0.8125, prec 0.0319399, recall 0.775739
2017-12-10T14:02:22.886992: step 1036, loss 0.408155, acc 0.875, prec 0.0319292, recall 0.775739
2017-12-10T14:02:23.074456: step 1037, loss 2.25068, acc 0.859375, prec 0.0319591, recall 0.775178
2017-12-10T14:02:23.264467: step 1038, loss 0.407419, acc 0.890625, prec 0.0320309, recall 0.775635
2017-12-10T14:02:23.455177: step 1039, loss 0.205011, acc 0.90625, prec 0.0320228, recall 0.775635
2017-12-10T14:02:23.643449: step 1040, loss 0.414483, acc 0.84375, prec 0.0320094, recall 0.775635
2017-12-10T14:02:23.837547: step 1041, loss 0.515038, acc 0.859375, prec 0.0319973, recall 0.775635
2017-12-10T14:02:24.026186: step 1042, loss 0.61463, acc 0.859375, prec 0.0319853, recall 0.775635
2017-12-10T14:02:24.212297: step 1043, loss 0.450141, acc 0.859375, prec 0.0320947, recall 0.776316
2017-12-10T14:02:24.398649: step 1044, loss 0.485037, acc 0.84375, prec 0.0320813, recall 0.776316
2017-12-10T14:02:24.587113: step 1045, loss 2.01602, acc 0.890625, prec 0.0321933, recall 0.776993
2017-12-10T14:02:24.779287: step 1046, loss 0.378015, acc 0.875, prec 0.0321826, recall 0.776993
2017-12-10T14:02:24.965407: step 1047, loss 0.30526, acc 0.890625, prec 0.0321731, recall 0.776993
2017-12-10T14:02:25.156310: step 1048, loss 0.215214, acc 0.890625, prec 0.0321637, recall 0.776993
2017-12-10T14:02:25.345901: step 1049, loss 0.443333, acc 0.890625, prec 0.0321543, recall 0.776993
2017-12-10T14:02:25.533077: step 1050, loss 0.312074, acc 0.90625, prec 0.0321463, recall 0.776993
2017-12-10T14:02:25.725278: step 1051, loss 0.474661, acc 0.84375, prec 0.0321329, recall 0.776993
2017-12-10T14:02:25.909150: step 1052, loss 0.452492, acc 0.828125, prec 0.0321585, recall 0.777218
2017-12-10T14:02:26.099745: step 1053, loss 4.79225, acc 0.890625, prec 0.0321908, recall 0.77666
2017-12-10T14:02:26.288210: step 1054, loss 0.542662, acc 0.875, prec 0.0322204, recall 0.776884
2017-12-10T14:02:26.479586: step 1055, loss 0.386431, acc 0.875, prec 0.03225, recall 0.777108
2017-12-10T14:02:26.673678: step 1056, loss 0.19973, acc 0.953125, prec 0.0323266, recall 0.777555
2017-12-10T14:02:26.864476: step 1057, loss 0.621075, acc 0.828125, prec 0.0323118, recall 0.777555
2017-12-10T14:02:27.053645: step 1058, loss 0.611881, acc 0.828125, prec 0.0323373, recall 0.777778
2017-12-10T14:02:27.244042: step 1059, loss 2.38277, acc 0.875, prec 0.0324486, recall 0.777667
2017-12-10T14:02:27.435203: step 1060, loss 0.458687, acc 0.875, prec 0.0324781, recall 0.777888
2017-12-10T14:02:27.626275: step 1061, loss 0.608146, acc 0.796875, prec 0.0325007, recall 0.778109
2017-12-10T14:02:27.818191: step 1062, loss 0.55635, acc 0.8125, prec 0.0324845, recall 0.778109
2017-12-10T14:02:28.002999: step 1063, loss 0.396434, acc 0.84375, prec 0.032471, recall 0.778109
2017-12-10T14:02:28.187351: step 1064, loss 0.867475, acc 0.8125, prec 0.032495, recall 0.77833
2017-12-10T14:02:28.376366: step 1065, loss 2.50592, acc 0.8125, prec 0.0325203, recall 0.777778
2017-12-10T14:02:28.565396: step 1066, loss 6.61907, acc 0.734375, prec 0.0325803, recall 0.77668
2017-12-10T14:02:28.754399: step 1067, loss 13.0405, acc 0.78125, prec 0.0325628, recall 0.775913
2017-12-10T14:02:28.949502: step 1068, loss 1.02378, acc 0.6875, prec 0.0325759, recall 0.776134
2017-12-10T14:02:29.138290: step 1069, loss 5.01221, acc 0.6875, prec 0.0326303, recall 0.775811
2017-12-10T14:02:29.328315: step 1070, loss 1.47819, acc 0.609375, prec 0.0326365, recall 0.776031
2017-12-10T14:02:29.514651: step 1071, loss 1.85496, acc 0.515625, prec 0.0325948, recall 0.776031
2017-12-10T14:02:29.703972: step 1072, loss 2.25439, acc 0.453125, prec 0.0325478, recall 0.776031
2017-12-10T14:02:29.886420: step 1073, loss 2.2712, acc 0.5625, prec 0.0325103, recall 0.776031
2017-12-10T14:02:30.070851: step 1074, loss 1.74152, acc 0.5625, prec 0.0324729, recall 0.776031
2017-12-10T14:02:30.258538: step 1075, loss 2.54072, acc 0.453125, prec 0.0324659, recall 0.776251
2017-12-10T14:02:30.443356: step 1076, loss 3.18573, acc 0.4375, prec 0.0324577, recall 0.776471
2017-12-10T14:02:30.629097: step 1077, loss 3.24999, acc 0.453125, prec 0.0324508, recall 0.77669
2017-12-10T14:02:30.815878: step 1078, loss 2.47222, acc 0.421875, prec 0.0324017, recall 0.77669
2017-12-10T14:02:31.003672: step 1079, loss 3.42753, acc 0.546875, prec 0.0324042, recall 0.776149
2017-12-10T14:02:31.195826: step 1080, loss 2.97817, acc 0.4375, prec 0.0323961, recall 0.776367
2017-12-10T14:02:31.382279: step 1081, loss 3.2875, acc 0.4375, prec 0.0324667, recall 0.77702
2017-12-10T14:02:31.568529: step 1082, loss 2.07496, acc 0.53125, prec 0.0325058, recall 0.777454
2017-12-10T14:02:31.757152: step 1083, loss 1.77302, acc 0.5625, prec 0.0324689, recall 0.777454
2017-12-10T14:02:31.943016: step 1084, loss 2.33043, acc 0.453125, prec 0.0325404, recall 0.778101
2017-12-10T14:02:32.128752: step 1085, loss 2.02729, acc 0.6875, prec 0.0326707, recall 0.778958
2017-12-10T14:02:32.317959: step 1086, loss 4.19929, acc 0.625, prec 0.0326417, recall 0.777457
2017-12-10T14:02:32.505558: step 1087, loss 1.07534, acc 0.734375, prec 0.0326583, recall 0.777671
2017-12-10T14:02:32.690387: step 1088, loss 1.59831, acc 0.71875, prec 0.0327127, recall 0.778098
2017-12-10T14:02:32.880544: step 1089, loss 1.27294, acc 0.703125, prec 0.0326877, recall 0.778098
2017-12-10T14:02:33.068780: step 1090, loss 1.57411, acc 0.65625, prec 0.0326977, recall 0.778311
2017-12-10T14:02:33.259851: step 1091, loss 1.35669, acc 0.625, prec 0.032666, recall 0.778311
2017-12-10T14:02:33.446082: step 1092, loss 0.708645, acc 0.8125, prec 0.0326892, recall 0.778524
2017-12-10T14:02:33.635305: step 1093, loss 1.08052, acc 0.625, prec 0.0326577, recall 0.778524
2017-12-10T14:02:33.823617: step 1094, loss 2.57949, acc 0.734375, prec 0.0326367, recall 0.777778
2017-12-10T14:02:34.010498: step 1095, loss 1.04394, acc 0.765625, prec 0.032617, recall 0.777778
2017-12-10T14:02:34.195995: step 1096, loss 1.02971, acc 0.71875, prec 0.0326711, recall 0.778203
2017-12-10T14:02:34.387001: step 1097, loss 0.619529, acc 0.78125, prec 0.0326527, recall 0.778203
2017-12-10T14:02:34.577264: step 1098, loss 0.674058, acc 0.8125, prec 0.032637, recall 0.778203
2017-12-10T14:02:34.763808: step 1099, loss 0.536635, acc 0.84375, prec 0.0326627, recall 0.778415
2017-12-10T14:02:34.951002: step 1100, loss 0.489213, acc 0.859375, prec 0.0326509, recall 0.778415
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_1/1512932334/checkpoints/model-1100

2017-12-10T14:02:36.148289: step 1101, loss 1.80045, acc 0.875, prec 0.0326418, recall 0.777672
2017-12-10T14:02:36.338644: step 1102, loss 2.13899, acc 0.8125, prec 0.0326274, recall 0.77693
2017-12-10T14:02:36.529623: step 1103, loss 0.479406, acc 0.875, prec 0.032617, recall 0.77693
2017-12-10T14:02:36.714229: step 1104, loss 0.403351, acc 0.90625, prec 0.0326478, recall 0.777143
2017-12-10T14:02:36.903975: step 1105, loss 0.76818, acc 0.734375, prec 0.0326643, recall 0.777355
2017-12-10T14:02:37.095692: step 1106, loss 0.557914, acc 0.828125, prec 0.0326886, recall 0.777567
2017-12-10T14:02:37.283400: step 1107, loss 0.774604, acc 0.8125, prec 0.0327116, recall 0.777778
2017-12-10T14:02:37.470911: step 1108, loss 0.508099, acc 0.875, prec 0.0327398, recall 0.777989
2017-12-10T14:02:37.659368: step 1109, loss 0.331607, acc 0.859375, prec 0.032728, recall 0.777989
2017-12-10T14:02:37.847200: step 1110, loss 2.10968, acc 0.890625, prec 0.0327202, recall 0.777251
2017-12-10T14:02:38.035398: step 1111, loss 8.15963, acc 0.828125, prec 0.032747, recall 0.775992
2017-12-10T14:02:38.224936: step 1112, loss 0.636478, acc 0.828125, prec 0.0327712, recall 0.776204
2017-12-10T14:02:38.413700: step 1113, loss 0.874925, acc 0.78125, prec 0.0327915, recall 0.776415
2017-12-10T14:02:38.601526: step 1114, loss 0.612628, acc 0.828125, prec 0.0327771, recall 0.776415
2017-12-10T14:02:38.786365: step 1115, loss 1.71043, acc 0.78125, prec 0.0327973, recall 0.776626
2017-12-10T14:02:38.972977: step 1116, loss 0.9251, acc 0.734375, prec 0.0327751, recall 0.776626
2017-12-10T14:02:39.154919: step 1117, loss 1.38472, acc 0.625, prec 0.0327439, recall 0.776626
2017-12-10T14:02:39.340604: step 1118, loss 0.935505, acc 0.6875, prec 0.0327179, recall 0.776626
2017-12-10T14:02:39.526321: step 1119, loss 0.996784, acc 0.71875, prec 0.0326945, recall 0.776626
2017-12-10T14:02:39.712632: step 1120, loss 1.22529, acc 0.6875, prec 0.0327069, recall 0.776836
2017-12-10T14:02:39.902257: step 1121, loss 1.30933, acc 0.640625, prec 0.0326772, recall 0.776836
2017-12-10T14:02:40.091230: step 1122, loss 0.976582, acc 0.6875, prec 0.0326896, recall 0.777046
2017-12-10T14:02:40.280382: step 1123, loss 0.880387, acc 0.71875, prec 0.0326663, recall 0.777046
2017-12-10T14:02:40.469584: step 1124, loss 1.40101, acc 0.65625, prec 0.0327143, recall 0.777465
2017-12-10T14:02:40.656906: step 1125, loss 1.56063, acc 0.640625, prec 0.0327228, recall 0.777674
2017-12-10T14:02:40.843993: step 1126, loss 1.37132, acc 0.671875, prec 0.0326957, recall 0.777674
2017-12-10T14:02:41.034920: step 1127, loss 2.53298, acc 0.703125, prec 0.0327106, recall 0.777154
2017-12-10T14:02:41.224493: step 1128, loss 0.897425, acc 0.734375, prec 0.0326887, recall 0.777154
2017-12-10T14:02:41.416889: step 1129, loss 0.990388, acc 0.671875, prec 0.0326998, recall 0.777362
2017-12-10T14:02:41.604468: step 1130, loss 0.912505, acc 0.734375, prec 0.0326779, recall 0.777362
2017-12-10T14:02:41.795254: step 1131, loss 1.05123, acc 0.765625, prec 0.0326587, recall 0.777362
2017-12-10T14:02:41.984062: step 1132, loss 0.581967, acc 0.765625, prec 0.0326394, recall 0.777362
2017-12-10T14:02:42.171484: step 1133, loss 1.21214, acc 0.734375, prec 0.0326556, recall 0.77757
2017-12-10T14:02:42.359924: step 1134, loss 0.458043, acc 0.859375, prec 0.0326441, recall 0.77757
2017-12-10T14:02:42.550952: step 1135, loss 2.43148, acc 0.78125, prec 0.0326654, recall 0.777052
2017-12-10T14:02:42.740571: step 1136, loss 0.29754, acc 0.875, prec 0.0326551, recall 0.777052
2017-12-10T14:02:42.931282: step 1137, loss 0.614412, acc 0.859375, prec 0.0326815, recall 0.77726
2017-12-10T14:02:43.122190: step 1138, loss 0.740849, acc 0.859375, prec 0.03267, recall 0.77726
2017-12-10T14:02:43.304821: step 1139, loss 0.633984, acc 0.796875, prec 0.0326534, recall 0.77726
2017-12-10T14:02:43.492285: step 1140, loss 0.75913, acc 0.84375, prec 0.0326406, recall 0.77726
2017-12-10T14:02:43.680260: step 1141, loss 0.438562, acc 0.875, prec 0.0326304, recall 0.77726
2017-12-10T14:02:43.871965: step 1142, loss 14.5007, acc 0.90625, prec 0.0326618, recall 0.776744
2017-12-10T14:02:44.062631: step 1143, loss 0.491435, acc 0.84375, prec 0.0326869, recall 0.776952
2017-12-10T14:02:44.250096: step 1144, loss 0.756894, acc 0.78125, prec 0.032669, recall 0.776952
2017-12-10T14:02:44.438459: step 1145, loss 0.712388, acc 0.828125, prec 0.032655, recall 0.776952
2017-12-10T14:02:44.626218: step 1146, loss 0.562436, acc 0.84375, prec 0.03268, recall 0.777159
2017-12-10T14:02:44.809877: step 1147, loss 0.533415, acc 0.84375, prec 0.0326672, recall 0.777159
2017-12-10T14:02:44.993603: step 1148, loss 0.304044, acc 0.90625, prec 0.0326596, recall 0.777159
2017-12-10T14:02:45.182268: step 1149, loss 0.563086, acc 0.78125, prec 0.0326795, recall 0.777366
2017-12-10T14:02:45.371477: step 1150, loss 0.401373, acc 0.859375, prec 0.032668, recall 0.777366
2017-12-10T14:02:45.560067: step 1151, loss 4.36315, acc 0.890625, prec 0.0327358, recall 0.777058
2017-12-10T14:02:45.750055: step 1152, loss 0.200745, acc 0.953125, prec 0.0327696, recall 0.777264
2017-12-10T14:02:45.941564: step 1153, loss 0.213058, acc 0.90625, prec 0.032762, recall 0.777264
2017-12-10T14:02:46.129871: step 1154, loss 4.67661, acc 0.875, prec 0.0328284, recall 0.776959
2017-12-10T14:02:46.320446: step 1155, loss 2.10564, acc 0.828125, prec 0.0328156, recall 0.776243
2017-12-10T14:02:46.514373: step 1156, loss 1.51693, acc 0.765625, prec 0.0328341, recall 0.776449
2017-12-10T14:02:46.699863: step 1157, loss 1.31212, acc 0.734375, prec 0.03285, recall 0.776654
2017-12-10T14:02:46.884832: step 1158, loss 0.585871, acc 0.8125, prec 0.0328347, recall 0.776654
2017-12-10T14:02:47.071646: step 1159, loss 1.13783, acc 0.71875, prec 0.0328117, recall 0.776654
2017-12-10T14:02:47.258670: step 1160, loss 0.929659, acc 0.703125, prec 0.0327875, recall 0.776654
2017-12-10T14:02:47.442988: step 1161, loss 1.39574, acc 0.671875, prec 0.0327608, recall 0.776654
2017-12-10T14:02:47.631614: step 1162, loss 1.18703, acc 0.71875, prec 0.032738, recall 0.776654
2017-12-10T14:02:47.817395: step 1163, loss 0.874806, acc 0.671875, prec 0.0327488, recall 0.77686
2017-12-10T14:02:48.004295: step 1164, loss 1.37277, acc 0.609375, prec 0.0327171, recall 0.77686
2017-12-10T14:02:48.191652: step 1165, loss 1.00493, acc 0.78125, prec 0.0327742, recall 0.777269
2017-12-10T14:02:48.377699: step 1166, loss 0.871679, acc 0.734375, prec 0.0328274, recall 0.777676
2017-12-10T14:02:48.568773: step 1167, loss 1.0137, acc 0.734375, prec 0.0328805, recall 0.778082
2017-12-10T14:02:48.760683: step 1168, loss 1.10632, acc 0.703125, prec 0.0328937, recall 0.778285
2017-12-10T14:02:48.952632: step 1169, loss 0.599265, acc 0.859375, prec 0.0329569, recall 0.778689
2017-12-10T14:02:49.140232: step 1170, loss 1.1498, acc 0.78125, prec 0.0329391, recall 0.778689
2017-12-10T14:02:49.329273: step 1171, loss 0.837264, acc 0.78125, prec 0.0329213, recall 0.778689
2017-12-10T14:02:49.515174: step 1172, loss 0.472847, acc 0.828125, prec 0.0329074, recall 0.778689
2017-12-10T14:02:49.700807: step 1173, loss 0.540512, acc 0.84375, prec 0.0329319, recall 0.77889
2017-12-10T14:02:49.892263: step 1174, loss 0.83015, acc 0.796875, prec 0.0329898, recall 0.779292
2017-12-10T14:02:50.080079: step 1175, loss 0.12203, acc 0.9375, prec 0.0329848, recall 0.779292
2017-12-10T14:02:50.264611: step 1176, loss 0.787512, acc 0.859375, prec 0.0329734, recall 0.779292
2017-12-10T14:02:50.450992: step 1177, loss 0.219166, acc 0.921875, prec 0.032967, recall 0.779292
2017-12-10T14:02:50.637772: step 1178, loss 0.37568, acc 0.890625, prec 0.0329953, recall 0.779492
2017-12-10T14:02:50.829339: step 1179, loss 2.55574, acc 0.859375, prec 0.0330223, recall 0.778986
2017-12-10T14:02:51.023619: step 1180, loss 0.308022, acc 0.90625, prec 0.0330147, recall 0.778986
2017-12-10T14:02:51.213363: step 1181, loss 5.69486, acc 0.84375, prec 0.0330033, recall 0.778281
2017-12-10T14:02:51.404935: step 1182, loss 0.168463, acc 0.953125, prec 0.0329995, recall 0.778281
2017-12-10T14:02:51.593560: step 1183, loss 0.118518, acc 0.9375, prec 0.0329944, recall 0.778281
2017-12-10T14:02:51.783950: step 1184, loss 0.833122, acc 0.84375, prec 0.0330189, recall 0.778481
2017-12-10T14:02:51.974708: step 1185, loss 0.239185, acc 0.90625, prec 0.0330483, recall 0.778681
2017-12-10T14:02:52.162705: step 1186, loss 0.431698, acc 0.875, prec 0.0330382, recall 0.778681
2017-12-10T14:02:52.351629: step 1187, loss 0.645272, acc 0.875, prec 0.0330651, recall 0.778881
2017-12-10T14:02:52.540255: step 1188, loss 0.719238, acc 0.859375, prec 0.0330908, recall 0.77908
2017-12-10T14:02:52.730033: step 1189, loss 12.7105, acc 0.828125, prec 0.0331151, recall 0.778578
2017-12-10T14:02:52.924868: step 1190, loss 1.82581, acc 0.796875, prec 0.0330999, recall 0.777878
2017-12-10T14:02:53.115412: step 1191, loss 1.3563, acc 0.84375, prec 0.0331242, recall 0.778077
2017-12-10T14:02:53.304622: step 1192, loss 0.285664, acc 0.890625, prec 0.0331154, recall 0.778077
2017-12-10T14:02:53.489601: step 1193, loss 2.54162, acc 0.78125, prec 0.0330989, recall 0.777379
2017-12-10T14:02:53.682035: step 1194, loss 1.12221, acc 0.75, prec 0.0331525, recall 0.777778
2017-12-10T14:02:53.869638: step 1195, loss 1.44533, acc 0.625, prec 0.0331591, recall 0.777977
2017-12-10T14:02:54.055997: step 1196, loss 1.38189, acc 0.71875, prec 0.0332101, recall 0.778374
2017-12-10T14:02:54.244201: step 1197, loss 2.5284, acc 0.65625, prec 0.0332571, recall 0.778075
2017-12-10T14:02:54.434762: step 1198, loss 1.40764, acc 0.609375, prec 0.0332991, recall 0.77847
2017-12-10T14:02:54.621474: step 1199, loss 1.57258, acc 0.59375, prec 0.0333029, recall 0.778667
2017-12-10T14:02:54.808526: step 1200, loss 1.31905, acc 0.671875, prec 0.0333498, recall 0.779059
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_1/1512932334/checkpoints/model-1200

2017-12-10T14:02:56.014716: step 1201, loss 2.17619, acc 0.609375, prec 0.0333915, recall 0.779451
2017-12-10T14:02:56.203759: step 1202, loss 1.03784, acc 0.734375, prec 0.0334066, recall 0.779646
2017-12-10T14:02:56.387391: step 1203, loss 1.6333, acc 0.59375, prec 0.033447, recall 0.780035
2017-12-10T14:02:56.577111: step 1204, loss 1.21961, acc 0.71875, prec 0.0334242, recall 0.780035
2017-12-10T14:02:56.763530: step 1205, loss 1.63975, acc 0.71875, prec 0.0335111, recall 0.780617
2017-12-10T14:02:56.954318: step 1206, loss 1.67331, acc 0.65625, prec 0.0335563, recall 0.781003
2017-12-10T14:02:57.140369: step 1207, loss 1.7151, acc 0.640625, prec 0.0335271, recall 0.781003
2017-12-10T14:02:57.327789: step 1208, loss 1.28453, acc 0.625, prec 0.0335697, recall 0.781387
2017-12-10T14:02:57.515662: step 1209, loss 0.899774, acc 0.703125, prec 0.0335457, recall 0.781387
2017-12-10T14:02:57.700490: step 1210, loss 0.962384, acc 0.671875, prec 0.0335191, recall 0.781387
2017-12-10T14:02:57.885172: step 1211, loss 0.969432, acc 0.796875, prec 0.0335027, recall 0.781387
2017-12-10T14:02:58.073975: step 1212, loss 0.940268, acc 0.734375, prec 0.0334813, recall 0.781387
2017-12-10T14:02:58.260870: step 1213, loss 0.75386, acc 0.78125, prec 0.0335, recall 0.781579
2017-12-10T14:02:58.450168: step 1214, loss 0.970333, acc 0.71875, prec 0.0335137, recall 0.78177
2017-12-10T14:02:58.640464: step 1215, loss 0.785745, acc 0.78125, prec 0.0334961, recall 0.78177
2017-12-10T14:02:58.834129: step 1216, loss 0.802467, acc 0.78125, prec 0.0334785, recall 0.78177
2017-12-10T14:02:59.031649: step 1217, loss 0.64427, acc 0.828125, prec 0.0334646, recall 0.78177
2017-12-10T14:02:59.217389: step 1218, loss 0.778856, acc 0.796875, prec 0.0334483, recall 0.78177
2017-12-10T14:02:59.404459: step 1219, loss 0.317918, acc 0.890625, prec 0.0334758, recall 0.781962
2017-12-10T14:02:59.594285: step 1220, loss 1.81961, acc 0.828125, prec 0.0334632, recall 0.781277
2017-12-10T14:02:59.785515: step 1221, loss 1.37206, acc 0.953125, prec 0.0334607, recall 0.780594
2017-12-10T14:02:59.978479: step 1222, loss 0.882165, acc 0.890625, prec 0.0335244, recall 0.780977
2017-12-10T14:03:00.173749: step 1223, loss 2.39208, acc 0.859375, prec 0.0335143, recall 0.780296
2017-12-10T14:03:00.368503: step 1224, loss 0.292003, acc 0.890625, prec 0.0335055, recall 0.780296
2017-12-10T14:03:00.559629: step 1225, loss 0.390604, acc 0.875, prec 0.033604, recall 0.78087
2017-12-10T14:03:00.745064: step 1226, loss 0.356912, acc 0.890625, prec 0.0335952, recall 0.78087
2017-12-10T14:03:00.933772: step 1227, loss 0.337173, acc 0.875, prec 0.0336213, recall 0.78106
2017-12-10T14:03:01.126987: step 1228, loss 6.30579, acc 0.890625, prec 0.0336138, recall 0.780382
2017-12-10T14:03:01.324629: step 1229, loss 3.59205, acc 0.796875, prec 0.0336709, recall 0.780087
2017-12-10T14:03:01.515749: step 1230, loss 1.5596, acc 0.828125, prec 0.0337654, recall 0.780656
2017-12-10T14:03:01.705394: step 1231, loss 0.62773, acc 0.8125, prec 0.0337502, recall 0.780656
2017-12-10T14:03:01.896740: step 1232, loss 0.490192, acc 0.8125, prec 0.0337712, recall 0.780846
2017-12-10T14:03:02.092814: step 1233, loss 0.552876, acc 0.890625, prec 0.0337984, recall 0.781034
2017-12-10T14:03:02.278168: step 1234, loss 1.24671, acc 0.65625, prec 0.0338787, recall 0.781599
2017-12-10T14:03:02.468393: step 1235, loss 1.45232, acc 0.765625, prec 0.0339318, recall 0.781974
2017-12-10T14:03:02.658576: step 1236, loss 1.19251, acc 0.671875, prec 0.0339052, recall 0.781974
2017-12-10T14:03:02.842135: step 1237, loss 1.85121, acc 0.671875, prec 0.0339147, recall 0.782161
2017-12-10T14:03:03.024908: step 1238, loss 1.80108, acc 0.546875, prec 0.033914, recall 0.782348
2017-12-10T14:03:03.215827: step 1239, loss 0.77462, acc 0.84375, prec 0.0339373, recall 0.782534
2017-12-10T14:03:03.402147: step 1240, loss 1.00971, acc 0.65625, prec 0.0339096, recall 0.782534
2017-12-10T14:03:03.590179: step 1241, loss 1.6734, acc 0.609375, prec 0.033914, recall 0.78272
2017-12-10T14:03:03.777994: step 1242, loss 1.02046, acc 0.703125, prec 0.0339259, recall 0.782906
2017-12-10T14:03:03.969918: step 1243, loss 1.44424, acc 0.671875, prec 0.0339353, recall 0.783091
2017-12-10T14:03:04.157067: step 1244, loss 0.9304, acc 0.734375, prec 0.033914, recall 0.783091
2017-12-10T14:03:04.340740: step 1245, loss 1.10999, acc 0.734375, prec 0.0339641, recall 0.783461
2017-12-10T14:03:04.529365: step 1246, loss 0.973206, acc 0.765625, prec 0.0340166, recall 0.78383
2017-12-10T14:03:04.719662: step 1247, loss 0.802532, acc 0.796875, prec 0.034036, recall 0.784014
2017-12-10T14:03:04.910887: step 1248, loss 0.777238, acc 0.765625, prec 0.0340528, recall 0.784197
2017-12-10T14:03:05.096614: step 1249, loss 0.678499, acc 0.828125, prec 0.0341102, recall 0.784563
2017-12-10T14:03:05.282739: step 1250, loss 0.385117, acc 0.84375, prec 0.0340976, recall 0.784563
2017-12-10T14:03:05.467879: step 1251, loss 10.2581, acc 0.8125, prec 0.0341194, recall 0.784081
2017-12-10T14:03:05.660832: step 1252, loss 0.65491, acc 0.734375, prec 0.0341336, recall 0.784264
2017-12-10T14:03:05.850040: step 1253, loss 0.394067, acc 0.921875, prec 0.0342695, recall 0.784992
2017-12-10T14:03:06.038636: step 1254, loss 0.693662, acc 0.828125, prec 0.0342556, recall 0.784992
2017-12-10T14:03:06.226854: step 1255, loss 0.717733, acc 0.8125, prec 0.0342405, recall 0.784992
2017-12-10T14:03:06.413892: step 1256, loss 0.621727, acc 0.84375, prec 0.0342634, recall 0.785173
2017-12-10T14:03:06.603455: step 1257, loss 0.675984, acc 0.828125, prec 0.0342496, recall 0.785173
2017-12-10T14:03:06.793714: step 1258, loss 0.932387, acc 0.671875, prec 0.0342586, recall 0.785354
2017-12-10T14:03:06.978183: step 1259, loss 0.577503, acc 0.890625, prec 0.0342498, recall 0.785354
2017-12-10T14:03:07.165927: step 1260, loss 0.546569, acc 0.84375, prec 0.0342373, recall 0.785354
2017-12-10T14:03:07.352488: step 1261, loss 2.63181, acc 0.890625, prec 0.0342297, recall 0.784693
2017-12-10T14:03:07.543168: step 1262, loss 0.412018, acc 0.90625, prec 0.034293, recall 0.785055
2017-12-10T14:03:07.729541: step 1263, loss 1.05384, acc 0.828125, prec 0.0344208, recall 0.785774
2017-12-10T14:03:07.917924: step 1264, loss 0.399086, acc 0.875, prec 0.0344107, recall 0.785774
2017-12-10T14:03:08.103401: step 1265, loss 0.501997, acc 0.828125, prec 0.0343969, recall 0.785774
2017-12-10T14:03:08.291065: step 1266, loss 1.08468, acc 0.8125, prec 0.0344525, recall 0.786132
2017-12-10T14:03:08.480639: step 1267, loss 0.643355, acc 0.84375, prec 0.0344752, recall 0.78631
2017-12-10T14:03:08.669622: step 1268, loss 0.463589, acc 0.796875, prec 0.0344941, recall 0.786489
2017-12-10T14:03:08.856864: step 1269, loss 0.576045, acc 0.8125, prec 0.034479, recall 0.786489
2017-12-10T14:03:09.046213: step 1270, loss 4.02782, acc 0.828125, prec 0.0344664, recall 0.785833
2017-12-10T14:03:09.239387: step 1271, loss 0.587922, acc 0.828125, prec 0.0344525, recall 0.785833
2017-12-10T14:03:09.425974: step 1272, loss 0.850982, acc 0.78125, prec 0.0344702, recall 0.786012
2017-12-10T14:03:09.615324: step 1273, loss 0.346175, acc 0.875, prec 0.0344601, recall 0.786012
2017-12-10T14:03:09.799509: step 1274, loss 2.65675, acc 0.796875, prec 0.0345155, recall 0.785714
2017-12-10T14:03:09.993352: step 1275, loss 0.325922, acc 0.890625, prec 0.0345067, recall 0.785714
2017-12-10T14:03:10.180512: step 1276, loss 0.868024, acc 0.8125, prec 0.034562, recall 0.78607
2017-12-10T14:03:10.369785: step 1277, loss 1.24331, acc 0.78125, prec 0.0346147, recall 0.786424
2017-12-10T14:03:10.560870: step 1278, loss 0.775371, acc 0.765625, prec 0.0345958, recall 0.786424
2017-12-10T14:03:10.749391: step 1279, loss 0.70042, acc 0.796875, prec 0.0346145, recall 0.7866
2017-12-10T14:03:10.936672: step 1280, loss 1.47453, acc 0.625, prec 0.0345843, recall 0.7866
2017-12-10T14:03:11.127257: step 1281, loss 1.22079, acc 0.859375, prec 0.0346081, recall 0.786777
2017-12-10T14:03:11.313594: step 1282, loss 0.763257, acc 0.75, prec 0.0346581, recall 0.787129
2017-12-10T14:03:11.499037: step 1283, loss 0.601282, acc 0.8125, prec 0.0346781, recall 0.787304
2017-12-10T14:03:11.691809: step 1284, loss 0.473623, acc 0.859375, prec 0.0346668, recall 0.787304
2017-12-10T14:03:11.877389: step 1285, loss 0.424733, acc 0.859375, prec 0.0346554, recall 0.787304
2017-12-10T14:03:12.065092: step 1286, loss 1.7922, acc 0.78125, prec 0.0347091, recall 0.787007
2017-12-10T14:03:12.254116: step 1287, loss 1.12009, acc 0.703125, prec 0.0347202, recall 0.787182
2017-12-10T14:03:12.439907: step 1288, loss 0.573259, acc 0.84375, prec 0.0347076, recall 0.787182
2017-12-10T14:03:12.628486: step 1289, loss 0.56446, acc 0.828125, prec 0.0347288, recall 0.787356
2017-12-10T14:03:12.812370: step 1290, loss 11.4876, acc 0.71875, prec 0.0347074, recall 0.78671
2017-12-10T14:03:13.005199: step 1291, loss 5.73306, acc 0.78125, prec 0.034726, recall 0.786241
2017-12-10T14:03:13.192675: step 1292, loss 0.67608, acc 0.8125, prec 0.0347458, recall 0.786416
2017-12-10T14:03:13.382632: step 1293, loss 0.825741, acc 0.734375, prec 0.0347245, recall 0.786416
2017-12-10T14:03:13.575312: step 1294, loss 1.12544, acc 0.6875, prec 0.0346994, recall 0.786416
2017-12-10T14:03:13.762874: step 1295, loss 1.23628, acc 0.6875, prec 0.0346744, recall 0.786416
2017-12-10T14:03:13.958894: step 1296, loss 1.27986, acc 0.703125, prec 0.0346506, recall 0.786416
2017-12-10T14:03:14.149743: step 1297, loss 0.962653, acc 0.78125, prec 0.0347027, recall 0.786765
2017-12-10T14:03:14.335451: step 1298, loss 2.19327, acc 0.609375, prec 0.0347062, recall 0.786939
2017-12-10T14:03:14.522312: step 1299, loss 1.68387, acc 0.609375, prec 0.0347097, recall 0.787113
2017-12-10T14:03:14.709393: step 1300, loss 1.16118, acc 0.734375, prec 0.0347579, recall 0.787459
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_1/1512932334/checkpoints/model-1300

2017-12-10T14:03:15.799291: step 1301, loss 1.51146, acc 0.640625, prec 0.0347639, recall 0.787632
2017-12-10T14:03:15.987510: step 1302, loss 1.4299, acc 0.6875, prec 0.0348428, recall 0.788149
2017-12-10T14:03:16.176835: step 1303, loss 0.866684, acc 0.703125, prec 0.0348191, recall 0.788149
2017-12-10T14:03:16.364708: step 1304, loss 1.56085, acc 0.65625, prec 0.0347916, recall 0.788149
2017-12-10T14:03:16.551452: step 1305, loss 0.920332, acc 0.75, prec 0.0348408, recall 0.788493
2017-12-10T14:03:16.739955: step 1306, loss 1.09341, acc 0.734375, prec 0.0348887, recall 0.788835
2017-12-10T14:03:16.927695: step 1307, loss 0.940599, acc 0.78125, prec 0.0349058, recall 0.789006
2017-12-10T14:03:17.115775: step 1308, loss 0.60973, acc 0.78125, prec 0.0349228, recall 0.789176
2017-12-10T14:03:17.302794: step 1309, loss 0.800205, acc 0.75, prec 0.0349028, recall 0.789176
2017-12-10T14:03:17.490514: step 1310, loss 0.965565, acc 0.765625, prec 0.0349186, recall 0.789346
2017-12-10T14:03:17.679168: step 1311, loss 0.474248, acc 0.859375, prec 0.0349074, recall 0.789346
2017-12-10T14:03:17.862462: step 1312, loss 0.865309, acc 0.765625, prec 0.0349231, recall 0.789516
2017-12-10T14:03:18.053569: step 1313, loss 0.668842, acc 0.8125, prec 0.0349082, recall 0.789516
2017-12-10T14:03:18.242001: step 1314, loss 13.1716, acc 0.890625, prec 0.0349695, recall 0.78922
2017-12-10T14:03:18.430037: step 1315, loss 0.55597, acc 0.828125, prec 0.0349558, recall 0.78922
2017-12-10T14:03:18.617499: step 1316, loss 10.5377, acc 0.84375, prec 0.034979, recall 0.788755
2017-12-10T14:03:18.811133: step 1317, loss 0.241496, acc 0.921875, prec 0.0349728, recall 0.788755
2017-12-10T14:03:18.996855: step 1318, loss 1.12856, acc 0.78125, prec 0.0349897, recall 0.788925
2017-12-10T14:03:19.188125: step 1319, loss 0.485982, acc 0.84375, prec 0.0350459, recall 0.789263
2017-12-10T14:03:19.376842: step 1320, loss 1.64777, acc 0.6875, prec 0.0350896, recall 0.7896
2017-12-10T14:03:19.569234: step 1321, loss 12.1526, acc 0.671875, prec 0.0350647, recall 0.788969
2017-12-10T14:03:19.756860: step 1322, loss 0.44503, acc 0.828125, prec 0.035051, recall 0.788969
2017-12-10T14:03:19.948659: step 1323, loss 0.781585, acc 0.75, prec 0.0350311, recall 0.788969
2017-12-10T14:03:20.135285: step 1324, loss 0.702986, acc 0.75, prec 0.0350454, recall 0.789137
2017-12-10T14:03:20.321558: step 1325, loss 0.814638, acc 0.75, prec 0.0350255, recall 0.789137
2017-12-10T14:03:20.508940: step 1326, loss 0.626528, acc 0.828125, prec 0.0350119, recall 0.789137
2017-12-10T14:03:20.693428: step 1327, loss 0.658577, acc 0.84375, prec 0.0349995, recall 0.789137
2017-12-10T14:03:20.881984: step 1328, loss 0.720018, acc 0.828125, prec 0.03502, recall 0.789306
2017-12-10T14:03:21.069861: step 1329, loss 0.806807, acc 0.796875, prec 0.0350039, recall 0.789306
2017-12-10T14:03:21.261894: step 1330, loss 0.774316, acc 0.765625, prec 0.0349853, recall 0.789306
2017-12-10T14:03:21.447516: step 1331, loss 1.19723, acc 0.6875, prec 0.0349947, recall 0.789474
2017-12-10T14:03:21.635593: step 1332, loss 1.07254, acc 0.8125, prec 0.035014, recall 0.789641
2017-12-10T14:03:21.820790: step 1333, loss 0.906203, acc 0.765625, prec 0.0350636, recall 0.789976
2017-12-10T14:03:22.006436: step 1334, loss 1.24793, acc 0.671875, prec 0.0350376, recall 0.789976
2017-12-10T14:03:22.193028: step 1335, loss 0.255011, acc 0.890625, prec 0.0350289, recall 0.789976
2017-12-10T14:03:22.383370: step 1336, loss 0.625818, acc 0.8125, prec 0.0350141, recall 0.789976
2017-12-10T14:03:22.574641: step 1337, loss 0.715107, acc 0.796875, prec 0.0350321, recall 0.790143
2017-12-10T14:03:22.763148: step 1338, loss 0.682465, acc 0.8125, prec 0.0350852, recall 0.790476
2017-12-10T14:03:22.952236: step 1339, loss 0.422419, acc 0.859375, prec 0.0351081, recall 0.790642
2017-12-10T14:03:23.139437: step 1340, loss 0.491976, acc 0.828125, prec 0.0351285, recall 0.790808
2017-12-10T14:03:23.326951: step 1341, loss 0.741892, acc 0.90625, prec 0.035189, recall 0.791139
2017-12-10T14:03:23.518047: step 1342, loss 0.368339, acc 0.875, prec 0.0351791, recall 0.791139
2017-12-10T14:03:23.703167: step 1343, loss 0.868177, acc 0.875, prec 0.035237, recall 0.791469
2017-12-10T14:03:23.895876: step 1344, loss 0.372095, acc 0.875, prec 0.0352271, recall 0.791469
2017-12-10T14:03:24.083114: step 1345, loss 0.49476, acc 0.875, prec 0.035285, recall 0.791798
2017-12-10T14:03:24.268964: step 1346, loss 0.374218, acc 0.90625, prec 0.0352776, recall 0.791798
2017-12-10T14:03:24.456608: step 1347, loss 0.527485, acc 0.84375, prec 0.0352991, recall 0.791962
2017-12-10T14:03:24.645329: step 1348, loss 6.79302, acc 0.90625, prec 0.0352929, recall 0.791339
2017-12-10T14:03:24.841383: step 1349, loss 0.230384, acc 0.9375, prec 0.0352879, recall 0.791339
2017-12-10T14:03:25.031955: step 1350, loss 0.852085, acc 0.921875, prec 0.0353156, recall 0.791503
2017-12-10T14:03:25.220011: step 1351, loss 1.91435, acc 0.796875, prec 0.0353007, recall 0.790881
2017-12-10T14:03:25.411595: step 1352, loss 0.820586, acc 0.796875, prec 0.0352846, recall 0.790881
2017-12-10T14:03:25.598066: step 1353, loss 1.04072, acc 0.90625, prec 0.035311, recall 0.791045
2017-12-10T14:03:25.791799: step 1354, loss 0.720701, acc 0.859375, prec 0.0353337, recall 0.791209
2017-12-10T14:03:25.977452: step 1355, loss 0.848592, acc 0.8125, prec 0.0353527, recall 0.791373
2017-12-10T14:03:26.166812: step 1356, loss 0.619086, acc 0.796875, prec 0.0353366, recall 0.791373
2017-12-10T14:03:26.350670: step 1357, loss 0.643588, acc 0.828125, prec 0.0353905, recall 0.791699
2017-12-10T14:03:26.536546: step 1358, loss 0.609874, acc 0.84375, prec 0.0353781, recall 0.791699
2017-12-10T14:03:26.726434: step 1359, loss 0.527933, acc 0.796875, prec 0.035362, recall 0.791699
2017-12-10T14:03:26.916693: step 1360, loss 0.951987, acc 0.828125, prec 0.0353821, recall 0.791862
2017-12-10T14:03:27.107361: step 1361, loss 0.717782, acc 0.84375, prec 0.0354035, recall 0.792025
2017-12-10T14:03:27.291132: step 1362, loss 0.881415, acc 0.6875, prec 0.0353788, recall 0.792025
2017-12-10T14:03:27.475981: step 1363, loss 0.569169, acc 0.796875, prec 0.0353964, recall 0.792188
2017-12-10T14:03:27.666670: step 1364, loss 0.967407, acc 0.78125, prec 0.0353791, recall 0.792188
2017-12-10T14:03:27.852905: step 1365, loss 8.19368, acc 0.828125, prec 0.0353667, recall 0.791569
2017-12-10T14:03:28.039484: step 1366, loss 1.46977, acc 0.734375, prec 0.0353794, recall 0.791732
2017-12-10T14:03:28.227605: step 1367, loss 0.952332, acc 0.71875, prec 0.0353572, recall 0.791732
2017-12-10T14:03:28.418095: step 1368, loss 1.17005, acc 0.796875, prec 0.0353748, recall 0.791894
2017-12-10T14:03:28.604522: step 1369, loss 3.94822, acc 0.734375, prec 0.0353887, recall 0.79144
2017-12-10T14:03:28.795573: step 1370, loss 0.673307, acc 0.84375, prec 0.0354099, recall 0.791602
2017-12-10T14:03:28.988322: step 1371, loss 0.844198, acc 0.765625, prec 0.0353915, recall 0.791602
2017-12-10T14:03:29.175054: step 1372, loss 0.768771, acc 0.78125, prec 0.0353742, recall 0.791602
2017-12-10T14:03:29.361571: step 1373, loss 1.76623, acc 0.703125, prec 0.0354849, recall 0.792248
2017-12-10T14:03:29.549536: step 1374, loss 0.893667, acc 0.734375, prec 0.0355309, recall 0.79257
2017-12-10T14:03:29.736975: step 1375, loss 0.761071, acc 0.828125, prec 0.0356846, recall 0.793369
2017-12-10T14:03:29.929075: step 1376, loss 1.23595, acc 0.671875, prec 0.035692, recall 0.793528
2017-12-10T14:03:30.114731: step 1377, loss 1.06157, acc 0.6875, prec 0.0356673, recall 0.793528
2017-12-10T14:03:30.301235: step 1378, loss 0.796057, acc 0.734375, prec 0.0356797, recall 0.793687
2017-12-10T14:03:30.490360: step 1379, loss 0.834282, acc 0.796875, prec 0.035697, recall 0.793846
2017-12-10T14:03:30.680810: step 1380, loss 0.999437, acc 0.640625, prec 0.0357019, recall 0.794005
2017-12-10T14:03:30.867244: step 1381, loss 0.843671, acc 0.71875, prec 0.0356797, recall 0.794005
2017-12-10T14:03:31.055704: step 1382, loss 0.970487, acc 0.734375, prec 0.0356588, recall 0.794005
2017-12-10T14:03:31.245239: step 1383, loss 4.21202, acc 0.78125, prec 0.0356428, recall 0.793395
2017-12-10T14:03:31.442013: step 1384, loss 0.605205, acc 0.828125, prec 0.0356626, recall 0.793553
2017-12-10T14:03:31.633332: step 1385, loss 1.05556, acc 0.703125, prec 0.0356724, recall 0.793712
2017-12-10T14:03:31.821324: step 1386, loss 0.644272, acc 0.796875, prec 0.0356565, recall 0.793712
2017-12-10T14:03:32.007908: step 1387, loss 0.566945, acc 0.734375, prec 0.0356356, recall 0.793712
2017-12-10T14:03:32.196180: step 1388, loss 0.860266, acc 0.71875, prec 0.0356135, recall 0.793712
2017-12-10T14:03:32.384566: step 1389, loss 5.75688, acc 0.796875, prec 0.0356652, recall 0.79342
2017-12-10T14:03:32.578177: step 1390, loss 0.433332, acc 0.828125, prec 0.0356848, recall 0.793578
2017-12-10T14:03:32.765441: step 1391, loss 0.543232, acc 0.84375, prec 0.0356726, recall 0.793578
2017-12-10T14:03:32.949372: step 1392, loss 0.388141, acc 0.828125, prec 0.0356591, recall 0.793578
2017-12-10T14:03:33.135470: step 1393, loss 0.95088, acc 0.78125, prec 0.0356419, recall 0.793578
2017-12-10T14:03:33.320947: step 1394, loss 0.871443, acc 0.796875, prec 0.035626, recall 0.793578
2017-12-10T14:03:33.511166: step 1395, loss 0.500524, acc 0.8125, prec 0.0356114, recall 0.793578
2017-12-10T14:03:33.697667: step 1396, loss 0.819194, acc 0.8125, prec 0.0356628, recall 0.793893
2017-12-10T14:03:33.884186: step 1397, loss 0.476847, acc 0.890625, prec 0.0356543, recall 0.793893
2017-12-10T14:03:34.075317: step 1398, loss 1.08487, acc 0.828125, prec 0.03574, recall 0.794364
2017-12-10T14:03:34.267473: step 1399, loss 0.794556, acc 0.84375, prec 0.0357608, recall 0.794521
2017-12-10T14:03:34.453070: step 1400, loss 0.357679, acc 0.84375, prec 0.0358476, recall 0.794989
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_1/1512932334/checkpoints/model-1400

2017-12-10T14:03:35.664602: step 1401, loss 0.558904, acc 0.8125, prec 0.0358988, recall 0.795299
2017-12-10T14:03:35.854025: step 1402, loss 0.421648, acc 0.828125, prec 0.0359183, recall 0.795455
2017-12-10T14:03:36.041578: step 1403, loss 0.658564, acc 0.8125, prec 0.0359365, recall 0.795609
2017-12-10T14:03:36.227496: step 1404, loss 0.899965, acc 0.828125, prec 0.0360219, recall 0.796072
2017-12-10T14:03:36.420071: step 1405, loss 2.00578, acc 0.84375, prec 0.0360437, recall 0.795626
2017-12-10T14:03:36.615645: step 1406, loss 0.931847, acc 0.765625, prec 0.0360911, recall 0.795934
2017-12-10T14:03:36.804663: step 1407, loss 0.251967, acc 0.890625, prec 0.0360825, recall 0.795934
2017-12-10T14:03:36.989732: step 1408, loss 0.464342, acc 0.84375, prec 0.0361031, recall 0.796087
2017-12-10T14:03:37.175729: step 1409, loss 0.497402, acc 0.875, prec 0.0360932, recall 0.796087
2017-12-10T14:03:37.367352: step 1410, loss 3.15241, acc 0.875, prec 0.0361175, recall 0.795642
2017-12-10T14:03:37.562212: step 1411, loss 0.347737, acc 0.90625, prec 0.0361101, recall 0.795642
2017-12-10T14:03:37.751915: step 1412, loss 0.341588, acc 0.828125, prec 0.0360965, recall 0.795642
2017-12-10T14:03:37.941401: step 1413, loss 1.47351, acc 0.8125, prec 0.0361803, recall 0.796102
2017-12-10T14:03:38.128779: step 1414, loss 0.3811, acc 0.859375, prec 0.0361692, recall 0.796102
2017-12-10T14:03:38.317016: step 1415, loss 0.574896, acc 0.78125, prec 0.036152, recall 0.796102
2017-12-10T14:03:38.506889: step 1416, loss 0.922201, acc 0.78125, prec 0.0362003, recall 0.796407
2017-12-10T14:03:38.699764: step 1417, loss 0.54565, acc 0.828125, prec 0.0362523, recall 0.796712
2017-12-10T14:03:38.891769: step 1418, loss 0.901126, acc 0.765625, prec 0.0362666, recall 0.796863
2017-12-10T14:03:39.079176: step 1419, loss 2.92858, acc 0.765625, prec 0.0363148, recall 0.796572
2017-12-10T14:03:39.271896: step 1420, loss 0.593874, acc 0.8125, prec 0.0363655, recall 0.796875
2017-12-10T14:03:39.463803: step 1421, loss 0.477669, acc 0.8125, prec 0.0363834, recall 0.797026
2017-12-10T14:03:39.646977: step 1422, loss 0.65364, acc 0.8125, prec 0.0363686, recall 0.797026
2017-12-10T14:03:39.832595: step 1423, loss 0.633722, acc 0.78125, prec 0.0364167, recall 0.797327
2017-12-10T14:03:40.024487: step 1424, loss 0.680376, acc 0.75, prec 0.0363969, recall 0.797327
2017-12-10T14:03:40.215109: step 1425, loss 0.655379, acc 0.8125, prec 0.0364148, recall 0.797478
2017-12-10T14:03:40.405506: step 1426, loss 0.639122, acc 0.78125, prec 0.0363975, recall 0.797478
2017-12-10T14:03:40.591134: step 1427, loss 0.560526, acc 0.84375, prec 0.0363852, recall 0.797478
2017-12-10T14:03:40.777640: step 1428, loss 1.07092, acc 0.703125, prec 0.0363944, recall 0.797628
2017-12-10T14:03:40.962706: step 1429, loss 0.339475, acc 0.890625, prec 0.0364184, recall 0.797778
2017-12-10T14:03:41.151133: step 1430, loss 0.489428, acc 0.8125, prec 0.0364036, recall 0.797778
2017-12-10T14:03:41.341246: step 1431, loss 0.871725, acc 0.8125, prec 0.0364214, recall 0.797927
2017-12-10T14:03:41.528384: step 1432, loss 0.592235, acc 0.796875, prec 0.0364379, recall 0.798077
2017-12-10T14:03:41.717720: step 1433, loss 0.345747, acc 0.90625, prec 0.0364305, recall 0.798077
2017-12-10T14:03:41.904959: step 1434, loss 0.0742159, acc 0.96875, prec 0.0364606, recall 0.798226
2017-12-10T14:03:42.091986: step 1435, loss 0.334378, acc 0.859375, prec 0.0364495, recall 0.798226
2017-12-10T14:03:42.277381: step 1436, loss 0.472215, acc 0.890625, prec 0.0364734, recall 0.798375
2017-12-10T14:03:42.467703: step 1437, loss 6.53945, acc 0.9375, prec 0.0365023, recall 0.797935
2017-12-10T14:03:42.664217: step 1438, loss 0.381239, acc 0.875, prec 0.0364924, recall 0.797935
2017-12-10T14:03:42.851927: step 1439, loss 0.660461, acc 0.828125, prec 0.0364789, recall 0.797935
2017-12-10T14:03:43.039114: step 1440, loss 1.07782, acc 0.890625, prec 0.0365027, recall 0.798084
2017-12-10T14:03:43.232551: step 1441, loss 0.449741, acc 0.90625, prec 0.0365278, recall 0.798233
2017-12-10T14:03:43.429508: step 1442, loss 0.587841, acc 0.90625, prec 0.0365854, recall 0.798529
2017-12-10T14:03:43.614250: step 1443, loss 0.411057, acc 0.90625, prec 0.0366104, recall 0.798677
2017-12-10T14:03:43.804076: step 1444, loss 0.516715, acc 0.84375, prec 0.0365981, recall 0.798677
2017-12-10T14:03:43.994489: step 1445, loss 2.72034, acc 0.875, prec 0.0366219, recall 0.798239
2017-12-10T14:03:44.185418: step 1446, loss 0.630545, acc 0.8125, prec 0.0366395, recall 0.798387
2017-12-10T14:03:44.377822: step 1447, loss 0.497809, acc 0.84375, prec 0.0366596, recall 0.798535
2017-12-10T14:03:44.565365: step 1448, loss 0.925002, acc 0.828125, prec 0.0366784, recall 0.798682
2017-12-10T14:03:44.757724: step 1449, loss 0.913759, acc 0.734375, prec 0.0366899, recall 0.79883
2017-12-10T14:03:44.945481: step 1450, loss 0.555191, acc 0.921875, prec 0.036716, recall 0.798977
2017-12-10T14:03:45.132684: step 1451, loss 0.903724, acc 0.828125, prec 0.0367025, recall 0.798977
2017-12-10T14:03:45.322832: step 1452, loss 1.16471, acc 0.625, prec 0.0366729, recall 0.798977
2017-12-10T14:03:45.511982: step 1453, loss 0.70297, acc 0.765625, prec 0.0366868, recall 0.799123
2017-12-10T14:03:45.698042: step 1454, loss 0.572652, acc 0.859375, prec 0.036708, recall 0.79927
2017-12-10T14:03:45.884727: step 1455, loss 0.945197, acc 0.765625, prec 0.0367218, recall 0.799416
2017-12-10T14:03:46.076564: step 1456, loss 0.36703, acc 0.875, prec 0.036712, recall 0.799416
2017-12-10T14:03:46.264215: step 1457, loss 0.958596, acc 0.75, prec 0.0366923, recall 0.799416
2017-12-10T14:03:46.452648: step 1458, loss 0.40429, acc 0.921875, prec 0.0367507, recall 0.799709
2017-12-10T14:03:46.639190: step 1459, loss 0.725546, acc 0.828125, prec 0.0367372, recall 0.799709
2017-12-10T14:03:46.826747: step 1460, loss 2.04212, acc 0.796875, prec 0.0367224, recall 0.799127
2017-12-10T14:03:47.017616: step 1461, loss 1.63253, acc 0.859375, prec 0.036808, recall 0.799564
2017-12-10T14:03:47.205360: step 1462, loss 0.610976, acc 0.859375, prec 0.0368291, recall 0.79971
2017-12-10T14:03:47.393806: step 1463, loss 0.600845, acc 0.828125, prec 0.0368156, recall 0.79971
2017-12-10T14:03:47.582365: step 1464, loss 3.97289, acc 0.859375, prec 0.0368379, recall 0.799275
2017-12-10T14:03:47.775759: step 1465, loss 0.657921, acc 0.796875, prec 0.0368219, recall 0.799275
2017-12-10T14:03:47.963530: step 1466, loss 0.952468, acc 0.734375, prec 0.0368653, recall 0.799566
2017-12-10T14:03:48.148547: step 1467, loss 0.467567, acc 0.84375, prec 0.036853, recall 0.799566
2017-12-10T14:03:48.332969: step 1468, loss 0.616445, acc 0.84375, prec 0.0368407, recall 0.799566
2017-12-10T14:03:48.524531: step 1469, loss 0.963134, acc 0.71875, prec 0.0368507, recall 0.799711
2017-12-10T14:03:48.708002: step 1470, loss 0.790781, acc 0.75, prec 0.0368631, recall 0.799855
2017-12-10T14:03:48.895207: step 1471, loss 0.712571, acc 0.75, prec 0.0368435, recall 0.799855
2017-12-10T14:03:49.087771: step 1472, loss 0.438891, acc 0.8125, prec 0.0368929, recall 0.800144
2017-12-10T14:03:49.273509: step 1473, loss 0.479544, acc 0.828125, prec 0.0369434, recall 0.800432
2017-12-10T14:03:49.454149: step 1474, loss 0.687921, acc 0.78125, prec 0.0369262, recall 0.800432
2017-12-10T14:03:49.645778: step 1475, loss 0.908006, acc 0.765625, prec 0.0369398, recall 0.800576
2017-12-10T14:03:49.833032: step 1476, loss 0.652128, acc 0.765625, prec 0.0369854, recall 0.800863
2017-12-10T14:03:50.021148: step 1477, loss 0.490121, acc 0.8125, prec 0.0370026, recall 0.801006
2017-12-10T14:03:50.210383: step 1478, loss 0.945843, acc 0.78125, prec 0.0369854, recall 0.801006
2017-12-10T14:03:50.398607: step 1479, loss 0.45351, acc 0.890625, prec 0.0369769, recall 0.801006
2017-12-10T14:03:50.592428: step 1480, loss 0.422157, acc 0.84375, prec 0.0369646, recall 0.801006
2017-12-10T14:03:50.782460: step 1481, loss 0.296245, acc 0.875, prec 0.0369867, recall 0.801149
2017-12-10T14:03:50.973697: step 1482, loss 0.63571, acc 0.953125, prec 0.0370149, recall 0.801291
2017-12-10T14:03:51.164949: step 1483, loss 5.20523, acc 0.921875, prec 0.03701, recall 0.800717
2017-12-10T14:03:51.356588: step 1484, loss 0.181578, acc 0.9375, prec 0.037037, recall 0.80086
2017-12-10T14:03:51.545960: step 1485, loss 9.29683, acc 0.90625, prec 0.0370628, recall 0.800429
2017-12-10T14:03:51.738035: step 1486, loss 2.83493, acc 0.828125, prec 0.0370505, recall 0.799857
2017-12-10T14:03:51.928349: step 1487, loss 0.72642, acc 0.8125, prec 0.0370358, recall 0.799857
2017-12-10T14:03:52.114810: step 1488, loss 0.402007, acc 0.859375, prec 0.0370248, recall 0.799857
2017-12-10T14:03:52.305752: step 1489, loss 0.425709, acc 0.8125, prec 0.0370101, recall 0.799857
2017-12-10T14:03:52.496417: step 1490, loss 0.82608, acc 0.71875, prec 0.0370199, recall 0.8
2017-12-10T14:03:52.664426: step 1491, loss 1.18472, acc 0.705882, prec 0.0370334, recall 0.800143
2017-12-10T14:03:52.857976: step 1492, loss 2.53178, acc 0.71875, prec 0.0370444, recall 0.799715
2017-12-10T14:03:53.049733: step 1493, loss 0.760442, acc 0.75, prec 0.0370566, recall 0.799858
2017-12-10T14:03:53.237832: step 1494, loss 1.20424, acc 0.71875, prec 0.0370981, recall 0.800142
2017-12-10T14:03:53.422480: step 1495, loss 0.994615, acc 0.6875, prec 0.0370737, recall 0.800142
2017-12-10T14:03:53.608550: step 1496, loss 0.875606, acc 0.75, prec 0.0370541, recall 0.800142
2017-12-10T14:03:53.795659: step 1497, loss 0.996529, acc 0.8125, prec 0.0370395, recall 0.800142
2017-12-10T14:03:53.984287: step 1498, loss 1.16415, acc 0.671875, prec 0.0370772, recall 0.800426
2017-12-10T14:03:54.170665: step 1499, loss 0.969871, acc 0.796875, prec 0.0370931, recall 0.800568
2017-12-10T14:03:54.354645: step 1500, loss 0.934928, acc 0.703125, prec 0.0371015, recall 0.800709
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_1/1512932334/checkpoints/model-1500

2017-12-10T14:03:55.511216: step 1501, loss 0.845934, acc 0.8125, prec 0.0371186, recall 0.80085
2017-12-10T14:03:55.703151: step 1502, loss 0.34324, acc 0.890625, prec 0.0371416, recall 0.800991
2017-12-10T14:03:55.891659: step 1503, loss 0.819046, acc 0.703125, prec 0.0371185, recall 0.800991
2017-12-10T14:03:56.083343: step 1504, loss 0.418078, acc 0.875, prec 0.0371719, recall 0.801273
2017-12-10T14:03:56.270702: step 1505, loss 1.92354, acc 0.84375, prec 0.0371609, recall 0.800707
2017-12-10T14:03:56.461320: step 1506, loss 0.634432, acc 0.8125, prec 0.0371779, recall 0.800847
2017-12-10T14:03:56.649377: step 1507, loss 0.460352, acc 0.84375, prec 0.0371657, recall 0.800847
2017-12-10T14:03:56.835857: step 1508, loss 0.355988, acc 0.84375, prec 0.0371535, recall 0.800847
2017-12-10T14:03:57.024999: step 1509, loss 0.338272, acc 0.890625, prec 0.0371765, recall 0.800988
2017-12-10T14:03:57.214044: step 1510, loss 0.432152, acc 0.8125, prec 0.0371935, recall 0.801128
2017-12-10T14:03:57.401046: step 1511, loss 0.326967, acc 0.9375, prec 0.0371886, recall 0.801128
2017-12-10T14:03:57.586576: step 1512, loss 0.423915, acc 0.9375, prec 0.0372468, recall 0.801408
2017-12-10T14:03:57.777308: step 1513, loss 1.28837, acc 0.859375, prec 0.0372673, recall 0.801548
2017-12-10T14:03:57.964461: step 1514, loss 0.250132, acc 0.921875, prec 0.0372612, recall 0.801548
2017-12-10T14:03:58.155328: step 1515, loss 0.453641, acc 0.875, prec 0.0372829, recall 0.801688
2017-12-10T14:03:58.346841: step 1516, loss 0.564781, acc 0.84375, prec 0.0373337, recall 0.801966
2017-12-10T14:03:58.538190: step 1517, loss 0.270707, acc 0.921875, prec 0.0373276, recall 0.801966
2017-12-10T14:03:58.724454: step 1518, loss 2.0977, acc 0.921875, prec 0.0373227, recall 0.801404
2017-12-10T14:03:58.922078: step 1519, loss 0.139116, acc 0.921875, prec 0.0373481, recall 0.801543
2017-12-10T14:03:59.113455: step 1520, loss 0.232189, acc 0.9375, prec 0.0373432, recall 0.801543
2017-12-10T14:03:59.300199: step 1521, loss 0.298252, acc 0.90625, prec 0.0373673, recall 0.801682
2017-12-10T14:03:59.487346: step 1522, loss 0.519879, acc 0.890625, prec 0.0374216, recall 0.801959
2017-12-10T14:03:59.677802: step 1523, loss 0.37726, acc 0.890625, prec 0.0374445, recall 0.802098
2017-12-10T14:03:59.867636: step 1524, loss 1.04434, acc 0.90625, prec 0.0375, recall 0.802374
2017-12-10T14:04:00.058940: step 1525, loss 0.230354, acc 0.953125, prec 0.0375277, recall 0.802512
2017-12-10T14:04:00.248464: step 1526, loss 0.313923, acc 0.90625, prec 0.0375204, recall 0.802512
2017-12-10T14:04:00.441487: step 1527, loss 0.308529, acc 0.875, prec 0.0375106, recall 0.802512
2017-12-10T14:04:00.628717: step 1528, loss 0.166637, acc 0.90625, prec 0.0375347, recall 0.80265
2017-12-10T14:04:00.816502: step 1529, loss 0.28148, acc 0.890625, prec 0.0375261, recall 0.80265
2017-12-10T14:04:01.004504: step 1530, loss 0.21078, acc 0.90625, prec 0.0375187, recall 0.80265
2017-12-10T14:04:01.194022: step 1531, loss 1.15021, acc 0.984375, prec 0.0375489, recall 0.802787
2017-12-10T14:04:01.383891: step 1532, loss 0.57343, acc 0.84375, prec 0.0375367, recall 0.802787
2017-12-10T14:04:01.569181: step 1533, loss 0.422356, acc 0.921875, prec 0.0375619, recall 0.802925
2017-12-10T14:04:01.760342: step 1534, loss 0.501944, acc 0.875, prec 0.0375521, recall 0.802925
2017-12-10T14:04:01.954619: step 1535, loss 0.913986, acc 0.875, prec 0.0376363, recall 0.803336
2017-12-10T14:04:02.145467: step 1536, loss 0.250574, acc 0.90625, prec 0.0376603, recall 0.803472
2017-12-10T14:04:02.333207: step 1537, loss 0.643355, acc 0.953125, prec 0.0376879, recall 0.803609
2017-12-10T14:04:02.525338: step 1538, loss 0.328371, acc 0.859375, prec 0.0377082, recall 0.803745
2017-12-10T14:04:02.715704: step 1539, loss 0.586345, acc 0.859375, prec 0.0377285, recall 0.803881
2017-12-10T14:04:02.903156: step 1540, loss 0.469085, acc 0.84375, prec 0.0377475, recall 0.804017
2017-12-10T14:04:03.094913: step 1541, loss 0.569318, acc 0.84375, prec 0.0377665, recall 0.804152
2017-12-10T14:04:03.289623: step 1542, loss 0.195778, acc 0.9375, prec 0.0377616, recall 0.804152
2017-12-10T14:04:03.476426: step 1543, loss 0.495708, acc 0.875, prec 0.0377518, recall 0.804152
2017-12-10T14:04:03.664377: step 1544, loss 0.447956, acc 0.859375, prec 0.037772, recall 0.804288
2017-12-10T14:04:03.851726: step 1545, loss 0.609628, acc 0.84375, prec 0.0377597, recall 0.804288
2017-12-10T14:04:04.037480: step 1546, loss 1.27823, acc 0.90625, prec 0.0377836, recall 0.804423
2017-12-10T14:04:04.227351: step 1547, loss 0.892798, acc 0.921875, prec 0.0378399, recall 0.804693
2017-12-10T14:04:04.415644: step 1548, loss 0.488525, acc 0.859375, prec 0.0378601, recall 0.804828
2017-12-10T14:04:04.611797: step 1549, loss 0.447199, acc 0.875, prec 0.0378503, recall 0.804828
2017-12-10T14:04:04.799470: step 1550, loss 1.31878, acc 0.84375, prec 0.0379004, recall 0.805096
2017-12-10T14:04:04.989241: step 1551, loss 0.43469, acc 0.890625, prec 0.037923, recall 0.805231
2017-12-10T14:04:05.181997: step 1552, loss 0.149317, acc 0.9375, prec 0.0379492, recall 0.805364
2017-12-10T14:04:05.371530: step 1553, loss 0.278146, acc 0.890625, prec 0.0379718, recall 0.805498
2017-12-10T14:04:05.560290: step 1554, loss 0.50624, acc 0.859375, prec 0.0379919, recall 0.805632
2017-12-10T14:04:05.754585: step 1555, loss 0.651624, acc 0.875, prec 0.0379821, recall 0.805632
2017-12-10T14:04:05.940034: step 1556, loss 0.632328, acc 0.875, prec 0.0380034, recall 0.805765
2017-12-10T14:04:06.123833: step 1557, loss 0.404673, acc 0.890625, prec 0.0380259, recall 0.805898
2017-12-10T14:04:06.315606: step 1558, loss 0.375411, acc 0.875, prec 0.0380472, recall 0.806032
2017-12-10T14:04:06.507950: step 1559, loss 0.261227, acc 0.90625, prec 0.0380398, recall 0.806032
2017-12-10T14:04:06.693157: step 1560, loss 0.467746, acc 0.859375, prec 0.0380598, recall 0.806164
2017-12-10T14:04:06.883404: step 1561, loss 0.490805, acc 0.890625, prec 0.0381134, recall 0.80643
2017-12-10T14:04:07.072544: step 1562, loss 0.340642, acc 0.875, prec 0.0381035, recall 0.80643
2017-12-10T14:04:07.258549: step 1563, loss 5.91794, acc 0.90625, prec 0.0380986, recall 0.805328
2017-12-10T14:04:07.447698: step 1564, loss 0.309712, acc 0.84375, prec 0.0381174, recall 0.805461
2017-12-10T14:04:07.637997: step 1565, loss 0.397543, acc 0.90625, prec 0.0381721, recall 0.805726
2017-12-10T14:04:07.825417: step 1566, loss 3.20307, acc 0.875, prec 0.0382256, recall 0.805442
2017-12-10T14:04:08.015004: step 1567, loss 0.511528, acc 0.8125, prec 0.0382108, recall 0.805442
2017-12-10T14:04:08.204670: step 1568, loss 0.661917, acc 0.78125, prec 0.0382246, recall 0.805574
2017-12-10T14:04:08.394098: step 1569, loss 0.607192, acc 0.875, prec 0.0382147, recall 0.805574
2017-12-10T14:04:08.579954: step 1570, loss 0.820934, acc 0.71875, prec 0.0382545, recall 0.805838
2017-12-10T14:04:08.768046: step 1571, loss 0.473624, acc 0.796875, prec 0.0382695, recall 0.80597
2017-12-10T14:04:08.960406: step 1572, loss 0.833739, acc 0.8125, prec 0.0382857, recall 0.806102
2017-12-10T14:04:09.154099: step 1573, loss 0.80568, acc 0.703125, prec 0.0382932, recall 0.806233
2017-12-10T14:04:09.339316: step 1574, loss 1.19816, acc 0.609375, prec 0.0382624, recall 0.806233
2017-12-10T14:04:09.527400: step 1575, loss 0.819279, acc 0.71875, prec 0.0382712, recall 0.806364
2017-12-10T14:04:09.715655: step 1576, loss 0.790282, acc 0.796875, prec 0.0382861, recall 0.806495
2017-12-10T14:04:09.903329: step 1577, loss 0.756309, acc 0.75, prec 0.0383282, recall 0.806757
2017-12-10T14:04:10.091622: step 1578, loss 0.801961, acc 0.78125, prec 0.038311, recall 0.806757
2017-12-10T14:04:10.279306: step 1579, loss 0.750303, acc 0.765625, prec 0.0383234, recall 0.806887
2017-12-10T14:04:10.469777: step 1580, loss 0.567362, acc 0.796875, prec 0.0383074, recall 0.806887
2017-12-10T14:04:10.656645: step 1581, loss 0.438189, acc 0.84375, prec 0.0382951, recall 0.806887
2017-12-10T14:04:10.847301: step 1582, loss 0.938392, acc 0.828125, prec 0.0383125, recall 0.807018
2017-12-10T14:04:11.040832: step 1583, loss 0.938778, acc 0.828125, prec 0.0383606, recall 0.807278
2017-12-10T14:04:11.233288: step 1584, loss 0.510644, acc 0.828125, prec 0.0383778, recall 0.807407
2017-12-10T14:04:11.417974: step 1585, loss 0.672928, acc 0.828125, prec 0.0383951, recall 0.807537
2017-12-10T14:04:11.607401: step 1586, loss 0.565677, acc 0.90625, prec 0.03848, recall 0.807925
2017-12-10T14:04:11.795449: step 1587, loss 0.365203, acc 0.875, prec 0.0384701, recall 0.807925
2017-12-10T14:04:11.981940: step 1588, loss 0.409065, acc 0.9375, prec 0.0385267, recall 0.808182
2017-12-10T14:04:12.167361: step 1589, loss 0.416965, acc 0.875, prec 0.0385169, recall 0.808182
2017-12-10T14:04:12.359401: step 1590, loss 0.245774, acc 0.90625, prec 0.0385709, recall 0.808439
2017-12-10T14:04:12.547521: step 1591, loss 10.2784, acc 0.859375, prec 0.0385611, recall 0.807898
2017-12-10T14:04:12.740006: step 1592, loss 0.336664, acc 0.875, prec 0.0385512, recall 0.807898
2017-12-10T14:04:12.928591: step 1593, loss 0.139174, acc 0.953125, prec 0.0386089, recall 0.808155
2017-12-10T14:04:13.113400: step 1594, loss 0.391856, acc 0.84375, prec 0.038658, recall 0.808411
2017-12-10T14:04:13.300502: step 1595, loss 0.398615, acc 0.859375, prec 0.0386469, recall 0.808411
2017-12-10T14:04:13.485229: step 1596, loss 0.368514, acc 0.90625, prec 0.0386702, recall 0.808539
2017-12-10T14:04:13.672768: step 1597, loss 0.380183, acc 0.890625, prec 0.0386615, recall 0.808539
2017-12-10T14:04:13.865753: step 1598, loss 0.493437, acc 0.84375, prec 0.0387412, recall 0.808921
2017-12-10T14:04:14.059394: step 1599, loss 0.541404, acc 0.84375, prec 0.0387288, recall 0.808921
2017-12-10T14:04:14.253053: step 1600, loss 0.409326, acc 0.890625, prec 0.0387508, recall 0.809049
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_1/1512932334/checkpoints/model-1600

2017-12-10T14:04:15.419985: step 1601, loss 0.331292, acc 0.828125, prec 0.0387678, recall 0.809176
2017-12-10T14:04:15.607513: step 1602, loss 0.603242, acc 0.890625, prec 0.0387898, recall 0.809302
2017-12-10T14:04:15.793410: step 1603, loss 0.52289, acc 0.84375, prec 0.0387775, recall 0.809302
2017-12-10T14:04:15.984709: step 1604, loss 0.233766, acc 0.9375, prec 0.0388031, recall 0.809429
2017-12-10T14:04:16.175240: step 1605, loss 0.238586, acc 0.890625, prec 0.0387945, recall 0.809429
2017-12-10T14:04:16.360207: step 1606, loss 0.306824, acc 0.875, prec 0.0388152, recall 0.809555
2017-12-10T14:04:16.548957: step 1607, loss 0.0950217, acc 0.921875, prec 0.038809, recall 0.809555
2017-12-10T14:04:16.738797: step 1608, loss 0.407867, acc 0.90625, prec 0.0388016, recall 0.809555
2017-12-10T14:04:16.925002: step 1609, loss 1.00005, acc 0.875, prec 0.0388223, recall 0.809682
2017-12-10T14:04:17.118314: step 1610, loss 0.21292, acc 0.90625, prec 0.0388454, recall 0.809808
2017-12-10T14:04:17.305705: step 1611, loss 0.266318, acc 0.890625, prec 0.0388368, recall 0.809808
2017-12-10T14:04:17.494055: step 1612, loss 2.08589, acc 0.953125, prec 0.0388343, recall 0.809272
2017-12-10T14:04:17.681353: step 1613, loss 0.553799, acc 0.859375, prec 0.0388232, recall 0.809272
2017-12-10T14:04:17.871463: step 1614, loss 0.295157, acc 0.921875, prec 0.0388171, recall 0.809272
2017-12-10T14:04:18.058366: step 1615, loss 0.788411, acc 0.9375, prec 0.0388732, recall 0.809524
2017-12-10T14:04:18.252282: step 1616, loss 1.7295, acc 0.921875, prec 0.0389598, recall 0.809367
2017-12-10T14:04:18.441310: step 1617, loss 0.162506, acc 0.921875, prec 0.0389536, recall 0.809367
2017-12-10T14:04:18.625869: step 1618, loss 0.481523, acc 0.8125, prec 0.0389998, recall 0.809618
2017-12-10T14:04:18.815401: step 1619, loss 0.18313, acc 0.9375, prec 0.0390558, recall 0.809868
2017-12-10T14:04:19.005986: step 1620, loss 0.547487, acc 0.828125, prec 0.0391031, recall 0.810118
2017-12-10T14:04:19.192700: step 1621, loss 0.768908, acc 0.796875, prec 0.0391175, recall 0.810243
2017-12-10T14:04:19.383967: step 1622, loss 0.280295, acc 0.90625, prec 0.0391405, recall 0.810367
2017-12-10T14:04:19.573176: step 1623, loss 0.39716, acc 0.859375, prec 0.0391598, recall 0.810492
2017-12-10T14:04:19.761342: step 1624, loss 0.38501, acc 0.921875, prec 0.0392144, recall 0.81074
2017-12-10T14:04:19.948743: step 1625, loss 0.297265, acc 0.90625, prec 0.0392678, recall 0.810988
2017-12-10T14:04:20.136288: step 1626, loss 0.486821, acc 0.828125, prec 0.0392846, recall 0.811111
2017-12-10T14:04:20.325169: step 1627, loss 0.271216, acc 0.890625, prec 0.0392759, recall 0.811111
2017-12-10T14:04:20.514484: step 1628, loss 5.25114, acc 0.90625, prec 0.0393001, recall 0.810705
2017-12-10T14:04:20.703877: step 1629, loss 0.188208, acc 0.9375, prec 0.0392951, recall 0.810705
2017-12-10T14:04:20.888892: step 1630, loss 0.822703, acc 0.796875, prec 0.0393397, recall 0.810952
2017-12-10T14:04:21.073224: step 1631, loss 0.274276, acc 0.90625, prec 0.0393322, recall 0.810952
2017-12-10T14:04:21.259914: step 1632, loss 0.529662, acc 0.796875, prec 0.0394072, recall 0.811321
2017-12-10T14:04:21.447813: step 1633, loss 0.508625, acc 0.828125, prec 0.0394238, recall 0.811443
2017-12-10T14:04:21.636348: step 1634, loss 0.624704, acc 0.796875, prec 0.039438, recall 0.811566
2017-12-10T14:04:21.824692: step 1635, loss 0.538437, acc 0.828125, prec 0.0394546, recall 0.811688
2017-12-10T14:04:22.012980: step 1636, loss 0.420029, acc 0.828125, prec 0.0394712, recall 0.81181
2017-12-10T14:04:22.207005: step 1637, loss 0.742449, acc 0.765625, prec 0.0394525, recall 0.81181
2017-12-10T14:04:22.393368: step 1638, loss 1.52615, acc 0.734375, prec 0.0394919, recall 0.812054
2017-12-10T14:04:22.585576: step 1639, loss 2.10571, acc 0.921875, prec 0.0395475, recall 0.811772
2017-12-10T14:04:22.775551: step 1640, loss 0.458938, acc 0.828125, prec 0.039564, recall 0.811894
2017-12-10T14:04:22.964436: step 1641, loss 0.816834, acc 0.75, prec 0.0395441, recall 0.811894
2017-12-10T14:04:23.153868: step 1642, loss 0.479414, acc 0.8125, prec 0.0395594, recall 0.812016
2017-12-10T14:04:23.341116: step 1643, loss 0.647939, acc 0.84375, prec 0.039547, recall 0.812016
2017-12-10T14:04:23.528345: step 1644, loss 1.21937, acc 0.78125, prec 0.0395899, recall 0.812258
2017-12-10T14:04:23.721013: step 1645, loss 0.803483, acc 0.78125, prec 0.0396329, recall 0.8125
2017-12-10T14:04:23.909524: step 1646, loss 0.591639, acc 0.84375, prec 0.0396204, recall 0.8125
2017-12-10T14:04:24.096537: step 1647, loss 0.563642, acc 0.84375, prec 0.039608, recall 0.8125
2017-12-10T14:04:24.281213: step 1648, loss 0.978745, acc 0.78125, prec 0.0395906, recall 0.8125
2017-12-10T14:04:24.467839: step 1649, loss 1.03414, acc 0.78125, prec 0.0396335, recall 0.812741
2017-12-10T14:04:24.656778: step 1650, loss 0.471182, acc 0.875, prec 0.0396235, recall 0.812741
2017-12-10T14:04:24.848566: step 1651, loss 0.81427, acc 0.703125, prec 0.0395999, recall 0.812741
2017-12-10T14:04:25.036022: step 1652, loss 0.315472, acc 0.875, prec 0.0396502, recall 0.812982
2017-12-10T14:04:25.224311: step 1653, loss 0.470961, acc 0.859375, prec 0.039639, recall 0.812982
2017-12-10T14:04:25.408526: step 1654, loss 0.876713, acc 0.796875, prec 0.039683, recall 0.813222
2017-12-10T14:04:25.594681: step 1655, loss 0.214047, acc 0.921875, prec 0.0397069, recall 0.813342
2017-12-10T14:04:25.780475: step 1656, loss 0.68213, acc 0.796875, prec 0.0397208, recall 0.813462
2017-12-10T14:04:25.967290: step 1657, loss 0.61101, acc 0.84375, prec 0.0397985, recall 0.81382
2017-12-10T14:04:26.155962: step 1658, loss 0.26257, acc 0.90625, prec 0.0398211, recall 0.813939
2017-12-10T14:04:26.342271: step 1659, loss 0.571988, acc 0.828125, prec 0.0398374, recall 0.814058
2017-12-10T14:04:26.530830: step 1660, loss 0.355131, acc 0.859375, prec 0.0398862, recall 0.814295
2017-12-10T14:04:26.717906: step 1661, loss 0.475606, acc 0.890625, prec 0.0398775, recall 0.814295
2017-12-10T14:04:26.906321: step 1662, loss 2.1863, acc 0.859375, prec 0.0398675, recall 0.813776
2017-12-10T14:04:27.096650: step 1663, loss 0.34216, acc 0.90625, prec 0.0398601, recall 0.813776
2017-12-10T14:04:27.283158: step 1664, loss 0.270313, acc 0.921875, prec 0.0399138, recall 0.814013
2017-12-10T14:04:27.467936: step 1665, loss 0.293592, acc 0.90625, prec 0.0399363, recall 0.814131
2017-12-10T14:04:27.655053: step 1666, loss 0.943778, acc 0.84375, prec 0.0399538, recall 0.814249
2017-12-10T14:04:27.841564: step 1667, loss 0.422347, acc 0.890625, prec 0.0399451, recall 0.814249
2017-12-10T14:04:28.029049: step 1668, loss 0.18415, acc 0.9375, prec 0.04003, recall 0.814603
2017-12-10T14:04:28.218465: step 1669, loss 0.444594, acc 0.890625, prec 0.0400212, recall 0.814603
2017-12-10T14:04:28.405416: step 1670, loss 0.148421, acc 0.953125, prec 0.0400474, recall 0.814721
2017-12-10T14:04:28.594848: step 1671, loss 3.70065, acc 0.890625, prec 0.0400399, recall 0.814204
2017-12-10T14:04:28.797371: step 1672, loss 0.569075, acc 0.875, prec 0.0400898, recall 0.81444
2017-12-10T14:04:28.996614: step 1673, loss 0.444679, acc 0.8125, prec 0.0401047, recall 0.814557
2017-12-10T14:04:29.179972: step 1674, loss 0.164553, acc 0.953125, prec 0.0401309, recall 0.814674
2017-12-10T14:04:29.367167: step 1675, loss 0.146978, acc 0.953125, prec 0.040157, recall 0.814791
2017-12-10T14:04:29.556845: step 1676, loss 0.589374, acc 0.828125, prec 0.0401732, recall 0.814908
2017-12-10T14:04:29.745979: step 1677, loss 0.267399, acc 0.90625, prec 0.0401656, recall 0.814908
2017-12-10T14:04:29.935570: step 1678, loss 0.255277, acc 0.921875, prec 0.0401594, recall 0.814908
2017-12-10T14:04:30.122691: step 1679, loss 4.2245, acc 0.875, prec 0.0401805, recall 0.814511
2017-12-10T14:04:30.312209: step 1680, loss 0.908934, acc 0.75, prec 0.0401904, recall 0.814628
2017-12-10T14:04:30.503791: step 1681, loss 0.314025, acc 0.90625, prec 0.0402127, recall 0.814745
2017-12-10T14:04:30.696611: step 1682, loss 0.459928, acc 0.8125, prec 0.0402574, recall 0.814978
2017-12-10T14:04:30.884622: step 1683, loss 0.445102, acc 0.84375, prec 0.0402449, recall 0.814978
2017-12-10T14:04:31.074561: step 1684, loss 0.702103, acc 0.875, prec 0.0402945, recall 0.815211
2017-12-10T14:04:31.268235: step 1685, loss 0.281131, acc 0.890625, prec 0.0403454, recall 0.815443
2017-12-10T14:04:31.458757: step 1686, loss 0.362365, acc 0.859375, prec 0.0403639, recall 0.815558
2017-12-10T14:04:31.647924: step 1687, loss 0.396243, acc 0.84375, prec 0.0403812, recall 0.815674
2017-12-10T14:04:31.838588: step 1688, loss 0.352624, acc 0.859375, prec 0.0403699, recall 0.815674
2017-12-10T14:04:32.028315: step 1689, loss 0.412563, acc 0.890625, prec 0.0403909, recall 0.815789
2017-12-10T14:04:32.215302: step 1690, loss 1.09247, acc 0.828125, prec 0.0404664, recall 0.816135
2017-12-10T14:04:32.400108: step 1691, loss 0.336603, acc 0.8125, prec 0.0404811, recall 0.81625
2017-12-10T14:04:32.587896: step 1692, loss 0.540218, acc 0.875, prec 0.0405305, recall 0.816479
2017-12-10T14:04:32.773692: step 1693, loss 0.335018, acc 0.875, prec 0.0405204, recall 0.816479
2017-12-10T14:04:32.965466: step 1694, loss 0.355914, acc 0.859375, prec 0.0405389, recall 0.816594
2017-12-10T14:04:33.153730: step 1695, loss 0.463641, acc 0.828125, prec 0.0405548, recall 0.816708
2017-12-10T14:04:33.344928: step 1696, loss 0.40399, acc 0.875, prec 0.0405447, recall 0.816708
2017-12-10T14:04:33.533327: step 1697, loss 0.321657, acc 0.875, prec 0.0405347, recall 0.816708
2017-12-10T14:04:33.724450: step 1698, loss 0.453134, acc 0.859375, prec 0.0405531, recall 0.816822
2017-12-10T14:04:33.914853: step 1699, loss 0.29195, acc 0.875, prec 0.0405727, recall 0.816936
2017-12-10T14:04:34.103123: step 1700, loss 0.23834, acc 0.921875, prec 0.0405961, recall 0.81705
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_1/1512932334/checkpoints/model-1700

2017-12-10T14:04:35.362608: step 1701, loss 0.406985, acc 0.890625, prec 0.040617, recall 0.817164
2017-12-10T14:04:35.554141: step 1702, loss 0.359875, acc 0.890625, prec 0.0406082, recall 0.817164
2017-12-10T14:04:35.742863: step 1703, loss 3.0807, acc 0.90625, prec 0.0406019, recall 0.816656
2017-12-10T14:04:35.932738: step 1704, loss 0.255418, acc 0.921875, prec 0.0405957, recall 0.816656
2017-12-10T14:04:36.121985: step 1705, loss 0.290713, acc 0.859375, prec 0.040614, recall 0.81677
2017-12-10T14:04:36.309121: step 1706, loss 0.435525, acc 0.890625, prec 0.0406348, recall 0.816884
2017-12-10T14:04:36.498764: step 1707, loss 0.406357, acc 0.859375, prec 0.0406236, recall 0.816884
2017-12-10T14:04:36.684386: step 1708, loss 0.461493, acc 0.859375, prec 0.0406123, recall 0.816884
2017-12-10T14:04:36.873471: step 1709, loss 0.158074, acc 0.890625, prec 0.0406331, recall 0.816998
2017-12-10T14:04:37.061889: step 1710, loss 4.49779, acc 0.875, prec 0.0406243, recall 0.816491
2017-12-10T14:04:37.250757: step 1711, loss 0.815358, acc 0.921875, prec 0.0406772, recall 0.816718
2017-12-10T14:04:37.443196: step 1712, loss 3.49932, acc 0.90625, prec 0.040671, recall 0.816213
2017-12-10T14:04:37.636515: step 1713, loss 0.347177, acc 0.875, prec 0.0407201, recall 0.81644
2017-12-10T14:04:37.827031: step 1714, loss 0.421038, acc 0.859375, prec 0.0407088, recall 0.81644
2017-12-10T14:04:38.013626: step 1715, loss 1.09189, acc 0.8125, prec 0.0407824, recall 0.81678
2017-12-10T14:04:38.202334: step 1716, loss 0.874051, acc 0.78125, prec 0.0407943, recall 0.816893
2017-12-10T14:04:38.389517: step 1717, loss 0.932147, acc 0.71875, prec 0.0408013, recall 0.817006
2017-12-10T14:04:38.576682: step 1718, loss 0.946184, acc 0.71875, prec 0.0407787, recall 0.817006
2017-12-10T14:04:38.765285: step 1719, loss 1.26888, acc 0.640625, prec 0.0407793, recall 0.817118
2017-12-10T14:04:38.963251: step 1720, loss 0.921881, acc 0.71875, prec 0.0407568, recall 0.817118
2017-12-10T14:04:39.150729: step 1721, loss 0.724, acc 0.84375, prec 0.0407737, recall 0.817231
2017-12-10T14:04:39.336486: step 1722, loss 1.32271, acc 0.671875, prec 0.0408063, recall 0.817455
2017-12-10T14:04:39.529440: step 1723, loss 0.802111, acc 0.734375, prec 0.0408144, recall 0.817568
2017-12-10T14:04:39.717798: step 1724, loss 1.06659, acc 0.734375, prec 0.0407932, recall 0.817568
2017-12-10T14:04:39.905610: step 1725, loss 1.03107, acc 0.765625, prec 0.0408332, recall 0.817791
2017-12-10T14:04:40.092705: step 1726, loss 0.787493, acc 0.78125, prec 0.0408157, recall 0.817791
2017-12-10T14:04:40.281806: step 1727, loss 0.265711, acc 0.875, prec 0.0408057, recall 0.817791
2017-12-10T14:04:40.470491: step 1728, loss 0.583619, acc 0.796875, prec 0.0407895, recall 0.817791
2017-12-10T14:04:40.656898: step 1729, loss 0.484354, acc 0.828125, prec 0.0407757, recall 0.817791
2017-12-10T14:04:40.846940: step 1730, loss 5.66079, acc 0.875, prec 0.040767, recall 0.81729
2017-12-10T14:04:41.035760: step 1731, loss 3.17986, acc 0.828125, prec 0.0407839, recall 0.816901
2017-12-10T14:04:41.228989: step 1732, loss 3.40479, acc 0.734375, prec 0.0407933, recall 0.816514
2017-12-10T14:04:41.419846: step 1733, loss 0.485701, acc 0.84375, prec 0.0407808, recall 0.816514
2017-12-10T14:04:41.607915: step 1734, loss 0.658023, acc 0.734375, prec 0.0408182, recall 0.816738
2017-12-10T14:04:41.797504: step 1735, loss 1.08816, acc 0.6875, prec 0.0407933, recall 0.816738
2017-12-10T14:04:41.984977: step 1736, loss 0.73393, acc 0.75, prec 0.0407734, recall 0.816738
2017-12-10T14:04:42.168298: step 1737, loss 0.838342, acc 0.78125, prec 0.0408145, recall 0.816962
2017-12-10T14:04:42.359936: step 1738, loss 1.29361, acc 0.65625, prec 0.0407871, recall 0.816962
2017-12-10T14:04:42.545551: step 1739, loss 1.15715, acc 0.5625, prec 0.0407815, recall 0.817073
2017-12-10T14:04:42.728528: step 1740, loss 1.31491, acc 0.59375, prec 0.0408076, recall 0.817296
2017-12-10T14:04:42.914366: step 1741, loss 0.448462, acc 0.8125, prec 0.0408219, recall 0.817407
2017-12-10T14:04:43.101105: step 1742, loss 0.904298, acc 0.703125, prec 0.0408275, recall 0.817518
2017-12-10T14:04:43.291594: step 1743, loss 4.79517, acc 0.8125, prec 0.0408721, recall 0.817243
2017-12-10T14:04:43.478254: step 1744, loss 0.466606, acc 0.828125, prec 0.0408876, recall 0.817354
2017-12-10T14:04:43.662349: step 1745, loss 0.753867, acc 0.796875, prec 0.0409296, recall 0.817576
2017-12-10T14:04:43.855303: step 1746, loss 0.563456, acc 0.75, prec 0.0409389, recall 0.817686
2017-12-10T14:04:44.048470: step 1747, loss 0.800763, acc 0.828125, prec 0.0409252, recall 0.817686
2017-12-10T14:04:44.233263: step 1748, loss 0.649131, acc 0.75, prec 0.0409635, recall 0.817907
2017-12-10T14:04:44.417328: step 1749, loss 0.540687, acc 0.75, prec 0.0409436, recall 0.817907
2017-12-10T14:04:44.602152: step 1750, loss 0.950068, acc 0.703125, prec 0.0409201, recall 0.817907
2017-12-10T14:04:44.787202: step 1751, loss 0.630648, acc 0.875, prec 0.0409392, recall 0.818017
2017-12-10T14:04:44.975241: step 1752, loss 0.785747, acc 0.765625, prec 0.0409206, recall 0.818017
2017-12-10T14:04:45.167799: step 1753, loss 0.417448, acc 0.859375, prec 0.0409095, recall 0.818017
2017-12-10T14:04:45.359906: step 1754, loss 1.47801, acc 0.828125, prec 0.0409539, recall 0.818237
2017-12-10T14:04:45.550134: step 1755, loss 0.806803, acc 0.734375, prec 0.0409618, recall 0.818346
2017-12-10T14:04:45.735870: step 1756, loss 0.372375, acc 0.859375, prec 0.0409507, recall 0.818346
2017-12-10T14:04:45.923954: step 1757, loss 0.688873, acc 0.796875, prec 0.0409346, recall 0.818346
2017-12-10T14:04:46.115408: step 1758, loss 0.554288, acc 0.859375, prec 0.0409524, recall 0.818456
2017-12-10T14:04:46.309828: step 1759, loss 0.395488, acc 0.90625, prec 0.040974, recall 0.818565
2017-12-10T14:04:46.497582: step 1760, loss 0.940286, acc 0.9375, prec 0.0409979, recall 0.818675
2017-12-10T14:04:46.691356: step 1761, loss 0.237131, acc 0.9375, prec 0.0410219, recall 0.818784
2017-12-10T14:04:46.878573: step 1762, loss 0.200883, acc 0.9375, prec 0.0410459, recall 0.818893
2017-12-10T14:04:47.064655: step 1763, loss 0.428377, acc 0.859375, prec 0.0410637, recall 0.819002
2017-12-10T14:04:47.251881: step 1764, loss 0.406833, acc 0.875, prec 0.0410538, recall 0.819002
2017-12-10T14:04:47.440598: step 1765, loss 0.304587, acc 0.875, prec 0.0410439, recall 0.819002
2017-12-10T14:04:47.627214: step 1766, loss 0.781741, acc 0.9375, prec 0.0410967, recall 0.819219
2017-12-10T14:04:47.817549: step 1767, loss 0.378943, acc 0.921875, prec 0.0411194, recall 0.819328
2017-12-10T14:04:48.005661: step 1768, loss 0.422474, acc 0.90625, prec 0.0411409, recall 0.819436
2017-12-10T14:04:48.197726: step 1769, loss 2.88622, acc 0.90625, prec 0.0411347, recall 0.818945
2017-12-10T14:04:48.391608: step 1770, loss 0.278462, acc 0.921875, prec 0.0411285, recall 0.818945
2017-12-10T14:04:48.579386: step 1771, loss 0.526659, acc 0.890625, prec 0.0411198, recall 0.818945
2017-12-10T14:04:48.766592: step 1772, loss 0.463418, acc 0.859375, prec 0.0411952, recall 0.81927
2017-12-10T14:04:48.954513: step 1773, loss 0.668248, acc 0.953125, prec 0.0412204, recall 0.819378
2017-12-10T14:04:49.144254: step 1774, loss 0.749697, acc 0.921875, prec 0.0412718, recall 0.819594
2017-12-10T14:04:49.335397: step 1775, loss 0.565093, acc 0.90625, prec 0.0412932, recall 0.819701
2017-12-10T14:04:49.522617: step 1776, loss 0.554366, acc 0.890625, prec 0.0413422, recall 0.819917
2017-12-10T14:04:49.712753: step 1777, loss 3.09971, acc 0.734375, prec 0.0413511, recall 0.819535
2017-12-10T14:04:49.905646: step 1778, loss 0.351046, acc 0.875, prec 0.0413412, recall 0.819535
2017-12-10T14:04:50.097008: step 1779, loss 7.19654, acc 0.796875, prec 0.0413551, recall 0.819155
2017-12-10T14:04:50.285514: step 1780, loss 0.803819, acc 0.71875, prec 0.0413615, recall 0.819263
2017-12-10T14:04:50.474503: step 1781, loss 0.939158, acc 0.71875, prec 0.0413967, recall 0.819477
2017-12-10T14:04:50.666852: step 1782, loss 1.21663, acc 0.671875, prec 0.0413706, recall 0.819477
2017-12-10T14:04:50.854545: step 1783, loss 1.42818, acc 0.671875, prec 0.0413446, recall 0.819477
2017-12-10T14:04:51.040334: step 1784, loss 1.33536, acc 0.71875, prec 0.0413797, recall 0.819692
2017-12-10T14:04:51.226249: step 1785, loss 0.955223, acc 0.703125, prec 0.0414136, recall 0.819905
2017-12-10T14:04:51.418094: step 1786, loss 1.07572, acc 0.71875, prec 0.0414773, recall 0.820225
2017-12-10T14:04:51.609445: step 1787, loss 1.13914, acc 0.59375, prec 0.0414451, recall 0.820225
2017-12-10T14:04:51.796754: step 1788, loss 1.44974, acc 0.609375, prec 0.0414714, recall 0.820437
2017-12-10T14:04:51.985389: step 1789, loss 0.708242, acc 0.765625, prec 0.04151, recall 0.820649
2017-12-10T14:04:52.173204: step 1790, loss 0.766479, acc 0.75, prec 0.0415188, recall 0.820755
2017-12-10T14:04:52.371689: step 1791, loss 1.31144, acc 0.71875, prec 0.0415251, recall 0.82086
2017-12-10T14:04:52.564336: step 1792, loss 1.13877, acc 0.640625, prec 0.0414966, recall 0.82086
2017-12-10T14:04:52.753987: step 1793, loss 0.878155, acc 0.75, prec 0.0415054, recall 0.820966
2017-12-10T14:04:52.943749: step 1794, loss 0.545574, acc 0.8125, prec 0.0415191, recall 0.821071
2017-12-10T14:04:53.134077: step 1795, loss 0.412519, acc 0.875, prec 0.0415948, recall 0.821387
2017-12-10T14:04:53.325504: step 1796, loss 0.236708, acc 0.890625, prec 0.0416431, recall 0.821596
2017-12-10T14:04:53.510654: step 1797, loss 0.472997, acc 0.84375, prec 0.0416592, recall 0.821701
2017-12-10T14:04:53.696540: step 1798, loss 0.340418, acc 0.9375, prec 0.0416543, recall 0.821701
2017-12-10T14:04:53.885873: step 1799, loss 0.767547, acc 0.890625, prec 0.0417311, recall 0.822014
2017-12-10T14:04:54.076261: step 1800, loss 0.488218, acc 0.953125, prec 0.0417558, recall 0.822118
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_1/1512932334/checkpoints/model-1800

2017-12-10T14:04:55.249352: step 1801, loss 3.05262, acc 0.84375, prec 0.0417447, recall 0.821637
2017-12-10T14:04:55.439044: step 1802, loss 0.415023, acc 0.875, prec 0.0417632, recall 0.821742
2017-12-10T14:04:55.629523: step 1803, loss 0.328075, acc 0.9375, prec 0.0418152, recall 0.82195
2017-12-10T14:04:55.818398: step 1804, loss 0.300525, acc 0.875, prec 0.0418052, recall 0.82195
2017-12-10T14:04:56.005941: step 1805, loss 0.244075, acc 0.90625, prec 0.0417978, recall 0.82195
2017-12-10T14:04:56.193346: step 1806, loss 2.67255, acc 0.90625, prec 0.04182, recall 0.821574
2017-12-10T14:04:56.387182: step 1807, loss 0.56813, acc 0.90625, prec 0.041841, recall 0.821678
2017-12-10T14:04:56.575066: step 1808, loss 0.370835, acc 0.84375, prec 0.0418286, recall 0.821678
2017-12-10T14:04:56.766820: step 1809, loss 0.412408, acc 0.875, prec 0.0418187, recall 0.821678
2017-12-10T14:04:56.955956: step 1810, loss 0.371438, acc 0.84375, prec 0.0418347, recall 0.821782
2017-12-10T14:04:57.142983: step 1811, loss 0.275376, acc 0.875, prec 0.0418816, recall 0.82199
2017-12-10T14:04:57.328648: step 1812, loss 0.42856, acc 0.9375, prec 0.0418766, recall 0.82199
2017-12-10T14:04:57.516529: step 1813, loss 0.657651, acc 0.859375, prec 0.0418938, recall 0.822093
2017-12-10T14:04:57.704963: step 1814, loss 1.42926, acc 0.90625, prec 0.041916, recall 0.821719
2017-12-10T14:04:57.898351: step 1815, loss 0.192876, acc 0.921875, prec 0.0419665, recall 0.821926
2017-12-10T14:04:58.088575: step 1816, loss 4.398, acc 0.75, prec 0.0419479, recall 0.821449
2017-12-10T14:04:58.279713: step 1817, loss 0.447033, acc 0.84375, prec 0.0419355, recall 0.821449
2017-12-10T14:04:58.466347: step 1818, loss 0.654689, acc 0.875, prec 0.0419539, recall 0.821553
2017-12-10T14:04:58.654731: step 1819, loss 0.314373, acc 0.84375, prec 0.0419698, recall 0.821656
2017-12-10T14:04:58.852563: step 1820, loss 0.514738, acc 0.84375, prec 0.0419574, recall 0.821656
2017-12-10T14:04:59.051741: step 1821, loss 1.21336, acc 0.828125, prec 0.0420004, recall 0.821862
2017-12-10T14:04:59.242179: step 1822, loss 0.82338, acc 0.75, prec 0.0420089, recall 0.821965
2017-12-10T14:04:59.434351: step 1823, loss 0.413411, acc 0.875, prec 0.0420272, recall 0.822068
2017-12-10T14:04:59.624427: step 1824, loss 0.637625, acc 0.796875, prec 0.0420111, recall 0.822068
2017-12-10T14:04:59.816134: step 1825, loss 0.415088, acc 0.796875, prec 0.0420515, recall 0.822273
2017-12-10T14:05:00.005149: step 1826, loss 0.543394, acc 0.859375, prec 0.0420404, recall 0.822273
2017-12-10T14:05:00.190192: step 1827, loss 0.481517, acc 0.859375, prec 0.0420575, recall 0.822376
2017-12-10T14:05:00.377213: step 1828, loss 0.74248, acc 0.84375, prec 0.0420733, recall 0.822478
2017-12-10T14:05:00.564359: step 1829, loss 0.898704, acc 0.78125, prec 0.0421406, recall 0.822785
2017-12-10T14:05:00.753073: step 1830, loss 0.512217, acc 0.78125, prec 0.0421232, recall 0.822785
2017-12-10T14:05:00.937875: step 1831, loss 0.626606, acc 0.78125, prec 0.0421341, recall 0.822887
2017-12-10T14:05:01.129518: step 1832, loss 0.55509, acc 0.875, prec 0.0421242, recall 0.822887
2017-12-10T14:05:01.324628: step 1833, loss 0.425137, acc 0.890625, prec 0.0421437, recall 0.822989
2017-12-10T14:05:01.514288: step 1834, loss 0.418949, acc 0.875, prec 0.0421619, recall 0.82309
2017-12-10T14:05:01.699886: step 1835, loss 0.401969, acc 0.828125, prec 0.0421483, recall 0.82309
2017-12-10T14:05:01.891406: step 1836, loss 0.557035, acc 0.8125, prec 0.0421334, recall 0.82309
2017-12-10T14:05:02.083791: step 1837, loss 0.235875, acc 0.890625, prec 0.0421529, recall 0.823192
2017-12-10T14:05:02.273504: step 1838, loss 0.401275, acc 0.890625, prec 0.0421442, recall 0.823192
2017-12-10T14:05:02.463660: step 1839, loss 0.110878, acc 0.9375, prec 0.0421674, recall 0.823293
2017-12-10T14:05:02.649310: step 1840, loss 0.234365, acc 0.921875, prec 0.0421612, recall 0.823293
2017-12-10T14:05:02.840107: step 1841, loss 0.330301, acc 0.859375, prec 0.0422345, recall 0.823597
2017-12-10T14:05:03.031272: step 1842, loss 0.233384, acc 0.890625, prec 0.0422821, recall 0.823799
2017-12-10T14:05:03.224777: step 1843, loss 0.175496, acc 0.9375, prec 0.0422771, recall 0.823799
2017-12-10T14:05:03.414471: step 1844, loss 0.286335, acc 0.9375, prec 0.0423002, recall 0.823899
2017-12-10T14:05:03.605351: step 1845, loss 0.0664106, acc 0.984375, prec 0.042299, recall 0.823899
2017-12-10T14:05:03.791560: step 1846, loss 0.681803, acc 0.9375, prec 0.0423221, recall 0.824
2017-12-10T14:05:03.979490: step 1847, loss 0.217991, acc 0.921875, prec 0.0423159, recall 0.824
2017-12-10T14:05:04.165439: step 1848, loss 2.18323, acc 0.921875, prec 0.0423391, recall 0.82363
2017-12-10T14:05:04.355755: step 1849, loss 0.490732, acc 0.953125, prec 0.0423915, recall 0.823831
2017-12-10T14:05:04.545966: step 1850, loss 0.0885277, acc 0.96875, prec 0.042389, recall 0.823831
2017-12-10T14:05:04.736057: step 1851, loss 0.519319, acc 0.875, prec 0.0424072, recall 0.823932
2017-12-10T14:05:04.920430: step 1852, loss 0.171524, acc 0.953125, prec 0.0424034, recall 0.823932
2017-12-10T14:05:05.112343: step 1853, loss 0.261883, acc 0.890625, prec 0.0423947, recall 0.823932
2017-12-10T14:05:05.305150: step 1854, loss 0.128331, acc 0.96875, prec 0.0423923, recall 0.823932
2017-12-10T14:05:05.491155: step 1855, loss 0.254485, acc 0.9375, prec 0.0423873, recall 0.823932
2017-12-10T14:05:05.682861: step 1856, loss 0.53022, acc 0.890625, prec 0.0424347, recall 0.824132
2017-12-10T14:05:05.868106: step 1857, loss 0.373296, acc 0.890625, prec 0.042426, recall 0.824132
2017-12-10T14:05:06.058116: step 1858, loss 0.42499, acc 0.875, prec 0.0424722, recall 0.824332
2017-12-10T14:05:06.246337: step 1859, loss 0.315263, acc 0.90625, prec 0.0424928, recall 0.824432
2017-12-10T14:05:06.432956: step 1860, loss 0.170386, acc 0.953125, prec 0.0425171, recall 0.824531
2017-12-10T14:05:06.619485: step 1861, loss 0.297835, acc 0.9375, prec 0.0425121, recall 0.824531
2017-12-10T14:05:06.805702: step 1862, loss 0.118628, acc 0.96875, prec 0.0425656, recall 0.824731
2017-12-10T14:05:06.994021: step 1863, loss 0.257649, acc 0.90625, prec 0.0425582, recall 0.824731
2017-12-10T14:05:07.183776: step 1864, loss 0.124995, acc 0.96875, prec 0.0425557, recall 0.824731
2017-12-10T14:05:07.373570: step 1865, loss 3.19923, acc 0.90625, prec 0.0426335, recall 0.824561
2017-12-10T14:05:07.566910: step 1866, loss 6.4403, acc 0.921875, prec 0.0426565, recall 0.824194
2017-12-10T14:05:07.757946: step 1867, loss 0.525803, acc 0.890625, prec 0.0426758, recall 0.824294
2017-12-10T14:05:07.946538: step 1868, loss 0.535626, acc 0.859375, prec 0.0426926, recall 0.824393
2017-12-10T14:05:08.132934: step 1869, loss 0.412544, acc 0.84375, prec 0.0427081, recall 0.824492
2017-12-10T14:05:08.320727: step 1870, loss 0.455277, acc 0.875, prec 0.0426981, recall 0.824492
2017-12-10T14:05:08.507088: step 1871, loss 0.513276, acc 0.859375, prec 0.0426868, recall 0.824492
2017-12-10T14:05:08.695316: step 1872, loss 0.539964, acc 0.8125, prec 0.0426998, recall 0.824591
2017-12-10T14:05:08.884122: step 1873, loss 0.69252, acc 0.75, prec 0.0427078, recall 0.82469
2017-12-10T14:05:09.071046: step 1874, loss 0.764931, acc 0.84375, prec 0.0427233, recall 0.824789
2017-12-10T14:05:09.259744: step 1875, loss 0.989288, acc 0.75, prec 0.0427034, recall 0.824789
2017-12-10T14:05:09.450326: step 1876, loss 0.857277, acc 0.734375, prec 0.0426822, recall 0.824789
2017-12-10T14:05:09.639345: step 1877, loss 1.04783, acc 0.671875, prec 0.0426561, recall 0.824789
2017-12-10T14:05:09.825290: step 1878, loss 0.631546, acc 0.75, prec 0.0426362, recall 0.824789
2017-12-10T14:05:10.016100: step 1879, loss 0.72748, acc 0.828125, prec 0.0426504, recall 0.824887
2017-12-10T14:05:10.201432: step 1880, loss 0.842468, acc 0.765625, prec 0.0426597, recall 0.824986
2017-12-10T14:05:10.390852: step 1881, loss 0.65029, acc 0.75, prec 0.0426677, recall 0.825084
2017-12-10T14:05:10.582106: step 1882, loss 0.661321, acc 0.78125, prec 0.0426503, recall 0.825084
2017-12-10T14:05:10.769516: step 1883, loss 0.787734, acc 0.78125, prec 0.042633, recall 0.825084
2017-12-10T14:05:10.960540: step 1884, loss 0.594485, acc 0.8125, prec 0.0426181, recall 0.825084
2017-12-10T14:05:11.146852: step 1885, loss 0.256619, acc 0.890625, prec 0.0426372, recall 0.825183
2017-12-10T14:05:11.334059: step 1886, loss 0.839243, acc 0.859375, prec 0.0426817, recall 0.825379
2017-12-10T14:05:11.523459: step 1887, loss 0.265226, acc 0.875, prec 0.0426996, recall 0.825477
2017-12-10T14:05:11.713132: step 1888, loss 0.289026, acc 0.875, prec 0.0427174, recall 0.825575
2017-12-10T14:05:11.903763: step 1889, loss 0.327228, acc 0.90625, prec 0.04271, recall 0.825575
2017-12-10T14:05:12.095745: step 1890, loss 0.560077, acc 0.875, prec 0.0427279, recall 0.825673
2017-12-10T14:05:12.285710: step 1891, loss 0.905446, acc 1, prec 0.0427556, recall 0.82577
2017-12-10T14:05:12.474658: step 1892, loss 0.0871744, acc 0.96875, prec 0.0427531, recall 0.82577
2017-12-10T14:05:12.665833: step 1893, loss 0.317188, acc 0.890625, prec 0.0427445, recall 0.82577
2017-12-10T14:05:12.853751: step 1894, loss 2.35137, acc 0.953125, prec 0.0427975, recall 0.825503
2017-12-10T14:05:13.044767: step 1895, loss 2.10444, acc 0.859375, prec 0.0428153, recall 0.82514
2017-12-10T14:05:13.232846: step 1896, loss 0.49558, acc 0.8125, prec 0.0428004, recall 0.82514
2017-12-10T14:05:13.423257: step 1897, loss 0.282955, acc 0.890625, prec 0.0427918, recall 0.82514
2017-12-10T14:05:13.612501: step 1898, loss 0.293241, acc 0.90625, prec 0.0427843, recall 0.82514
2017-12-10T14:05:13.802255: step 1899, loss 0.94326, acc 0.84375, prec 0.0427996, recall 0.825237
2017-12-10T14:05:13.999089: step 1900, loss 0.355638, acc 0.921875, prec 0.0428212, recall 0.825335
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_1/1512932334/checkpoints/model-1900

2017-12-10T14:05:15.314963: step 1901, loss 1.73909, acc 0.796875, prec 0.0428328, recall 0.825432
2017-12-10T14:05:15.509416: step 1902, loss 0.743547, acc 0.796875, prec 0.0428166, recall 0.825432
2017-12-10T14:05:15.699557: step 1903, loss 0.503078, acc 0.828125, prec 0.042803, recall 0.825432
2017-12-10T14:05:15.887230: step 1904, loss 0.65473, acc 0.796875, prec 0.0428423, recall 0.825627
2017-12-10T14:05:16.073645: step 1905, loss 1.04973, acc 0.734375, prec 0.0428212, recall 0.825627
2017-12-10T14:05:16.259628: step 1906, loss 0.39299, acc 0.875, prec 0.0428113, recall 0.825627
2017-12-10T14:05:16.446613: step 1907, loss 1.93818, acc 0.765625, prec 0.0428481, recall 0.825821
2017-12-10T14:05:16.636956: step 1908, loss 0.937416, acc 0.78125, prec 0.0428308, recall 0.825821
2017-12-10T14:05:16.820392: step 1909, loss 0.889147, acc 0.75, prec 0.0428386, recall 0.825918
2017-12-10T14:05:17.008186: step 1910, loss 0.565524, acc 0.78125, prec 0.0428489, recall 0.826014
2017-12-10T14:05:17.195877: step 1911, loss 0.740683, acc 0.796875, prec 0.042888, recall 0.826208
2017-12-10T14:05:17.383146: step 1912, loss 0.907662, acc 0.71875, prec 0.0428934, recall 0.826304
2017-12-10T14:05:17.569221: step 1913, loss 0.954261, acc 0.8125, prec 0.0429337, recall 0.826497
2017-12-10T14:05:17.760433: step 1914, loss 0.354143, acc 0.875, prec 0.0429513, recall 0.826593
2017-12-10T14:05:17.944812: step 1915, loss 0.67986, acc 0.734375, prec 0.0429303, recall 0.826593
2017-12-10T14:05:18.132008: step 1916, loss 0.385556, acc 0.84375, prec 0.042918, recall 0.826593
2017-12-10T14:05:18.318540: step 1917, loss 0.479613, acc 0.8125, prec 0.0429032, recall 0.826593
2017-12-10T14:05:18.505840: step 1918, loss 0.261849, acc 0.875, prec 0.0428933, recall 0.826593
2017-12-10T14:05:18.692331: step 1919, loss 2.25578, acc 0.828125, prec 0.0429085, recall 0.826231
2017-12-10T14:05:18.886719: step 1920, loss 0.606742, acc 0.765625, prec 0.04289, recall 0.826231
2017-12-10T14:05:19.073478: step 1921, loss 0.491523, acc 0.84375, prec 0.0428777, recall 0.826231
2017-12-10T14:05:19.264805: step 1922, loss 4.35311, acc 0.890625, prec 0.0428978, recall 0.825871
2017-12-10T14:05:19.454599: step 1923, loss 1.59249, acc 0.859375, prec 0.0429691, recall 0.826159
2017-12-10T14:05:19.644014: step 1924, loss 0.706866, acc 0.78125, prec 0.0429793, recall 0.826255
2017-12-10T14:05:19.838044: step 1925, loss 0.629221, acc 0.8125, prec 0.0429645, recall 0.826255
2017-12-10T14:05:20.024770: step 1926, loss 0.731428, acc 0.796875, prec 0.0429759, recall 0.826351
2017-12-10T14:05:20.215592: step 1927, loss 3.56253, acc 0.75, prec 0.0430397, recall 0.826183
2017-12-10T14:05:20.405713: step 1928, loss 0.700124, acc 0.84375, prec 0.0430548, recall 0.826278
2017-12-10T14:05:20.594506: step 1929, loss 0.863516, acc 0.765625, prec 0.0430363, recall 0.826278
2017-12-10T14:05:20.786870: step 1930, loss 1.1442, acc 0.734375, prec 0.0430154, recall 0.826278
2017-12-10T14:05:20.973610: step 1931, loss 1.35679, acc 0.671875, prec 0.0430169, recall 0.826374
2017-12-10T14:05:21.157551: step 1932, loss 1.49422, acc 0.65625, prec 0.0430719, recall 0.826659
2017-12-10T14:05:21.349526: step 1933, loss 0.655011, acc 0.734375, prec 0.0430783, recall 0.826754
2017-12-10T14:05:21.537091: step 1934, loss 1.032, acc 0.8125, prec 0.0430909, recall 0.826849
2017-12-10T14:05:21.724618: step 1935, loss 0.794566, acc 0.828125, prec 0.0431047, recall 0.826944
2017-12-10T14:05:21.910911: step 1936, loss 0.814471, acc 0.765625, prec 0.0431408, recall 0.827133
2017-12-10T14:05:22.103028: step 1937, loss 0.666483, acc 0.859375, prec 0.0431571, recall 0.827228
2017-12-10T14:05:22.293597: step 1938, loss 0.572235, acc 0.765625, prec 0.0431386, recall 0.827228
2017-12-10T14:05:22.482594: step 1939, loss 0.775303, acc 0.8125, prec 0.0431784, recall 0.827417
2017-12-10T14:05:22.667949: step 1940, loss 0.736552, acc 0.796875, prec 0.0431624, recall 0.827417
2017-12-10T14:05:22.857745: step 1941, loss 1.23461, acc 0.640625, prec 0.0431341, recall 0.827417
2017-12-10T14:05:23.048630: step 1942, loss 1.91916, acc 0.828125, prec 0.0431491, recall 0.827059
2017-12-10T14:05:23.238601: step 1943, loss 0.553684, acc 0.828125, prec 0.0431628, recall 0.827154
2017-12-10T14:05:23.422225: step 1944, loss 0.907863, acc 0.734375, prec 0.0431692, recall 0.827248
2017-12-10T14:05:23.613561: step 1945, loss 0.377963, acc 0.84375, prec 0.0431569, recall 0.827248
2017-12-10T14:05:23.802142: step 1946, loss 0.660023, acc 0.828125, prec 0.0431706, recall 0.827342
2017-12-10T14:05:23.988099: step 1947, loss 0.515969, acc 0.921875, prec 0.0431916, recall 0.827436
2017-12-10T14:05:24.176356: step 1948, loss 0.344772, acc 0.859375, prec 0.0431806, recall 0.827436
2017-12-10T14:05:24.367473: step 1949, loss 0.22365, acc 0.890625, prec 0.043172, recall 0.827436
2017-12-10T14:05:24.553802: step 1950, loss 0.37206, acc 0.90625, prec 0.0431647, recall 0.827436
2017-12-10T14:05:24.745279: step 1951, loss 0.268219, acc 0.921875, prec 0.0431857, recall 0.82753
2017-12-10T14:05:24.933863: step 1952, loss 0.26645, acc 0.875, prec 0.0431759, recall 0.82753
2017-12-10T14:05:25.120239: step 1953, loss 0.160041, acc 0.9375, prec 0.0432253, recall 0.827717
2017-12-10T14:05:25.307584: step 1954, loss 0.868388, acc 0.953125, prec 0.0432759, recall 0.827904
2017-12-10T14:05:25.495834: step 1955, loss 0.309518, acc 0.890625, prec 0.0432945, recall 0.827998
2017-12-10T14:05:25.683184: step 1956, loss 0.241363, acc 0.9375, prec 0.0433167, recall 0.828091
2017-12-10T14:05:25.871843: step 1957, loss 0.399876, acc 0.890625, prec 0.0433352, recall 0.828184
2017-12-10T14:05:26.064426: step 1958, loss 0.397712, acc 0.9375, prec 0.0433574, recall 0.828277
2017-12-10T14:05:26.256629: step 1959, loss 0.821969, acc 0.96875, prec 0.0433821, recall 0.82837
2017-12-10T14:05:26.444369: step 1960, loss 1.11222, acc 0.921875, prec 0.0434302, recall 0.828556
2017-12-10T14:05:26.633483: step 1961, loss 2.61619, acc 0.859375, prec 0.0434203, recall 0.828108
2017-12-10T14:05:26.823606: step 1962, loss 0.108483, acc 0.953125, prec 0.0434167, recall 0.828108
2017-12-10T14:05:27.011678: step 1963, loss 0.350722, acc 0.9375, prec 0.0434388, recall 0.828201
2017-12-10T14:05:27.196829: step 1964, loss 0.503285, acc 0.890625, prec 0.0434302, recall 0.828201
2017-12-10T14:05:27.388468: step 1965, loss 0.376516, acc 0.859375, prec 0.0434192, recall 0.828201
2017-12-10T14:05:27.575131: step 1966, loss 0.591085, acc 0.765625, prec 0.0434278, recall 0.828294
2017-12-10T14:05:27.762563: step 1967, loss 0.337429, acc 0.890625, prec 0.0434192, recall 0.828294
2017-12-10T14:05:27.954114: step 1968, loss 0.766373, acc 0.859375, prec 0.0434893, recall 0.828571
2017-12-10T14:05:28.143676: step 1969, loss 0.245128, acc 0.921875, prec 0.0434832, recall 0.828571
2017-12-10T14:05:28.330670: step 1970, loss 0.323155, acc 0.875, prec 0.0434733, recall 0.828571
2017-12-10T14:05:28.516170: step 1971, loss 0.378086, acc 0.921875, prec 0.0434672, recall 0.828571
2017-12-10T14:05:28.703020: step 1972, loss 0.742983, acc 0.84375, prec 0.0434549, recall 0.828571
2017-12-10T14:05:28.901456: step 1973, loss 0.485463, acc 0.875, prec 0.0434451, recall 0.828571
2017-12-10T14:05:29.096806: step 1974, loss 0.413623, acc 0.859375, prec 0.0434611, recall 0.828664
2017-12-10T14:05:29.285882: step 1975, loss 0.262659, acc 0.921875, prec 0.043509, recall 0.828848
2017-12-10T14:05:29.471827: step 1976, loss 0.33156, acc 0.90625, prec 0.0435016, recall 0.828848
2017-12-10T14:05:29.660278: step 1977, loss 2.73701, acc 0.875, prec 0.04352, recall 0.828495
2017-12-10T14:05:29.851903: step 1978, loss 0.249632, acc 0.875, prec 0.0435102, recall 0.828495
2017-12-10T14:05:30.044991: step 1979, loss 0.613699, acc 0.921875, prec 0.043531, recall 0.828587
2017-12-10T14:05:30.238582: step 1980, loss 0.504729, acc 0.90625, prec 0.0435507, recall 0.828679
2017-12-10T14:05:30.430813: step 1981, loss 0.192417, acc 0.953125, prec 0.043547, recall 0.828679
2017-12-10T14:05:30.618351: step 1982, loss 0.448414, acc 0.875, prec 0.0435371, recall 0.828679
2017-12-10T14:05:30.808463: step 1983, loss 0.364762, acc 0.875, prec 0.0435543, recall 0.828771
2017-12-10T14:05:30.997760: step 1984, loss 0.524681, acc 0.84375, prec 0.043542, recall 0.828771
2017-12-10T14:05:31.190795: step 1985, loss 1.43648, acc 0.859375, prec 0.0435849, recall 0.828954
2017-12-10T14:05:31.388456: step 1986, loss 0.313767, acc 0.890625, prec 0.0435763, recall 0.828954
2017-12-10T14:05:31.576700: step 1987, loss 0.354147, acc 0.90625, prec 0.0436228, recall 0.829138
2017-12-10T14:05:31.751506: step 1988, loss 0.441769, acc 0.901961, prec 0.0436436, recall 0.829229
2017-12-10T14:05:31.948034: step 1989, loss 0.675662, acc 0.8125, prec 0.0436289, recall 0.829229
2017-12-10T14:05:32.133888: step 1990, loss 0.59821, acc 0.859375, prec 0.0436717, recall 0.829412
2017-12-10T14:05:32.321289: step 1991, loss 0.521467, acc 0.90625, prec 0.0436912, recall 0.829503
2017-12-10T14:05:32.508370: step 1992, loss 0.441773, acc 0.875, prec 0.0437083, recall 0.829594
2017-12-10T14:05:32.702032: step 1993, loss 0.479225, acc 0.84375, prec 0.0437229, recall 0.829685
2017-12-10T14:05:32.890590: step 1994, loss 0.158851, acc 0.953125, prec 0.043773, recall 0.829867
2017-12-10T14:05:33.076452: step 1995, loss 0.206117, acc 0.921875, prec 0.0437669, recall 0.829867
2017-12-10T14:05:33.267517: step 1996, loss 0.330693, acc 0.875, prec 0.043757, recall 0.829867
2017-12-10T14:05:33.455581: step 1997, loss 0.211996, acc 0.890625, prec 0.0437753, recall 0.829957
2017-12-10T14:05:33.643053: step 1998, loss 0.507086, acc 0.8125, prec 0.0437874, recall 0.830048
2017-12-10T14:05:33.836173: step 1999, loss 0.455587, acc 0.90625, prec 0.04378, recall 0.830048
2017-12-10T14:05:34.021911: step 2000, loss 0.199673, acc 0.9375, prec 0.0438557, recall 0.830319
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_1/1512932334/checkpoints/model-2000

2017-12-10T14:05:35.598937: step 2001, loss 0.0956634, acc 0.96875, prec 0.043907, recall 0.830499
2017-12-10T14:05:35.788317: step 2002, loss 0.249859, acc 0.96875, prec 0.0439582, recall 0.830679
2017-12-10T14:05:35.974112: step 2003, loss 0.16549, acc 0.921875, prec 0.043952, recall 0.830679
2017-12-10T14:05:36.160949: step 2004, loss 0.266832, acc 0.953125, prec 0.0439483, recall 0.830679
2017-12-10T14:05:36.350241: step 2005, loss 0.358914, acc 0.9375, prec 0.0439702, recall 0.830769
2017-12-10T14:05:36.543200: step 2006, loss 0.337065, acc 0.90625, prec 0.0440165, recall 0.830949
2017-12-10T14:05:36.734505: step 2007, loss 0.224349, acc 0.9375, prec 0.0440921, recall 0.831217
2017-12-10T14:05:36.923645: step 2008, loss 0.144497, acc 0.953125, prec 0.0441152, recall 0.831306
2017-12-10T14:05:37.116143: step 2009, loss 6.16789, acc 0.9375, prec 0.0441115, recall 0.830867
2017-12-10T14:05:37.306474: step 2010, loss 0.312961, acc 0.921875, prec 0.0441589, recall 0.831045
2017-12-10T14:05:37.492721: step 2011, loss 11.2041, acc 0.953125, prec 0.0441845, recall 0.830258
2017-12-10T14:05:37.685619: step 2012, loss 0.476803, acc 0.9375, prec 0.0442331, recall 0.830437
2017-12-10T14:05:37.878493: step 2013, loss 0.38112, acc 0.875, prec 0.0442232, recall 0.830437
2017-12-10T14:05:38.067515: step 2014, loss 0.308158, acc 0.875, prec 0.0442133, recall 0.830437
2017-12-10T14:05:38.254573: step 2015, loss 0.573453, acc 0.859375, prec 0.0442021, recall 0.830437
2017-12-10T14:05:38.445180: step 2016, loss 0.272904, acc 0.890625, prec 0.0442203, recall 0.830526
2017-12-10T14:05:38.633035: step 2017, loss 1.02747, acc 0.8125, prec 0.0442322, recall 0.830615
2017-12-10T14:05:38.816475: step 2018, loss 0.833578, acc 0.78125, prec 0.0442148, recall 0.830615
2017-12-10T14:05:39.003063: step 2019, loss 0.627059, acc 0.859375, prec 0.0442572, recall 0.830793
2017-12-10T14:05:39.189525: step 2020, loss 0.796608, acc 0.765625, prec 0.0442921, recall 0.830971
2017-12-10T14:05:39.377440: step 2021, loss 1.06468, acc 0.75, prec 0.0442723, recall 0.830971
2017-12-10T14:05:39.564872: step 2022, loss 1.0126, acc 0.734375, prec 0.044278, recall 0.83106
2017-12-10T14:05:39.750321: step 2023, loss 2.12873, acc 0.59375, prec 0.0442725, recall 0.831148
2017-12-10T14:05:39.935507: step 2024, loss 1.34812, acc 0.671875, prec 0.0442466, recall 0.831148
2017-12-10T14:05:40.123008: step 2025, loss 0.672832, acc 0.78125, prec 0.0442826, recall 0.831325
2017-12-10T14:05:40.310581: step 2026, loss 0.892539, acc 0.75, prec 0.0442628, recall 0.831325
2017-12-10T14:05:40.496340: step 2027, loss 0.711843, acc 0.765625, prec 0.0442976, recall 0.831502
2017-12-10T14:05:40.681081: step 2028, loss 0.560076, acc 0.84375, prec 0.0443119, recall 0.83159
2017-12-10T14:05:40.870436: step 2029, loss 0.559105, acc 0.8125, prec 0.0443237, recall 0.831678
2017-12-10T14:05:41.059965: step 2030, loss 0.406141, acc 0.84375, prec 0.044338, recall 0.831766
2017-12-10T14:05:41.248210: step 2031, loss 0.50579, acc 0.84375, prec 0.0443256, recall 0.831766
2017-12-10T14:05:41.437718: step 2032, loss 0.677235, acc 0.875, prec 0.044369, recall 0.831942
2017-12-10T14:05:41.623630: step 2033, loss 0.256162, acc 0.90625, prec 0.0444148, recall 0.832117
2017-12-10T14:05:41.809964: step 2034, loss 0.496517, acc 0.859375, prec 0.0444302, recall 0.832204
2017-12-10T14:05:41.999074: step 2035, loss 0.290611, acc 0.921875, prec 0.044424, recall 0.832204
2017-12-10T14:05:42.184428: step 2036, loss 0.128713, acc 0.96875, prec 0.0444216, recall 0.832204
2017-12-10T14:05:42.370735: step 2037, loss 0.310094, acc 0.890625, prec 0.0444129, recall 0.832204
2017-12-10T14:05:42.557087: step 2038, loss 0.220996, acc 0.921875, prec 0.0444599, recall 0.832379
2017-12-10T14:05:42.744441: step 2039, loss 0.327873, acc 0.921875, prec 0.0444537, recall 0.832379
2017-12-10T14:05:42.935010: step 2040, loss 0.0791834, acc 0.984375, prec 0.0444525, recall 0.832379
2017-12-10T14:05:43.123599: step 2041, loss 0.478973, acc 0.875, prec 0.0444691, recall 0.832466
2017-12-10T14:05:43.310067: step 2042, loss 4.50805, acc 0.875, prec 0.0444605, recall 0.832033
2017-12-10T14:05:43.499687: step 2043, loss 5.71734, acc 0.890625, prec 0.0445327, recall 0.831863
2017-12-10T14:05:43.690912: step 2044, loss 0.316711, acc 0.921875, prec 0.0446062, recall 0.832124
2017-12-10T14:05:43.884204: step 2045, loss 0.202505, acc 0.921875, prec 0.0446, recall 0.832124
2017-12-10T14:05:44.073742: step 2046, loss 0.505873, acc 0.828125, prec 0.0446394, recall 0.832298
2017-12-10T14:05:44.262986: step 2047, loss 0.27061, acc 0.90625, prec 0.044685, recall 0.832472
2017-12-10T14:05:44.448259: step 2048, loss 0.419617, acc 0.90625, prec 0.0447306, recall 0.832645
2017-12-10T14:05:44.635184: step 2049, loss 0.305868, acc 0.921875, prec 0.0447774, recall 0.832817
2017-12-10T14:05:44.826426: step 2050, loss 6.71944, acc 0.875, prec 0.0447687, recall 0.832388
2017-12-10T14:05:45.017453: step 2051, loss 0.234576, acc 0.890625, prec 0.04476, recall 0.832388
2017-12-10T14:05:45.210825: step 2052, loss 0.736899, acc 0.859375, prec 0.0447753, recall 0.832474
2017-12-10T14:05:45.397848: step 2053, loss 0.674392, acc 0.8125, prec 0.0447869, recall 0.832561
2017-12-10T14:05:45.585453: step 2054, loss 0.736336, acc 0.765625, prec 0.0447947, recall 0.832647
2017-12-10T14:05:45.770562: step 2055, loss 1.18284, acc 0.6875, prec 0.0448493, recall 0.832905
2017-12-10T14:05:45.962918: step 2056, loss 0.739524, acc 0.84375, prec 0.0448368, recall 0.832905
2017-12-10T14:05:46.147813: step 2057, loss 0.807664, acc 0.78125, prec 0.0448987, recall 0.833162
2017-12-10T14:05:46.334430: step 2058, loss 0.83663, acc 0.734375, prec 0.0448776, recall 0.833162
2017-12-10T14:05:46.526492: step 2059, loss 0.91554, acc 0.71875, prec 0.0448553, recall 0.833162
2017-12-10T14:05:46.715238: step 2060, loss 0.618869, acc 0.796875, prec 0.0448392, recall 0.833162
2017-12-10T14:05:46.902671: step 2061, loss 0.960636, acc 0.78125, prec 0.0448219, recall 0.833162
2017-12-10T14:05:47.087637: step 2062, loss 0.481562, acc 0.875, prec 0.0448383, recall 0.833248
2017-12-10T14:05:47.278879: step 2063, loss 0.947887, acc 0.6875, prec 0.0448663, recall 0.833419
2017-12-10T14:05:47.471227: step 2064, loss 0.856415, acc 0.734375, prec 0.044898, recall 0.833589
2017-12-10T14:05:47.663826: step 2065, loss 0.720731, acc 0.796875, prec 0.0449609, recall 0.833845
2017-12-10T14:05:47.850362: step 2066, loss 0.393889, acc 0.875, prec 0.0449509, recall 0.833845
2017-12-10T14:05:48.039109: step 2067, loss 0.560698, acc 0.859375, prec 0.0449924, recall 0.834014
2017-12-10T14:05:48.225182: step 2068, loss 0.433385, acc 0.875, prec 0.0450088, recall 0.834099
2017-12-10T14:05:48.412082: step 2069, loss 0.405714, acc 0.859375, prec 0.0449977, recall 0.834099
2017-12-10T14:05:48.601258: step 2070, loss 0.404405, acc 0.84375, prec 0.0450116, recall 0.834184
2017-12-10T14:05:48.788524: step 2071, loss 0.233501, acc 0.90625, prec 0.0450304, recall 0.834268
2017-12-10T14:05:48.973685: step 2072, loss 0.361701, acc 0.875, prec 0.0450468, recall 0.834353
2017-12-10T14:05:49.165641: step 2073, loss 0.309748, acc 0.921875, prec 0.0450931, recall 0.834521
2017-12-10T14:05:49.353620: step 2074, loss 0.253932, acc 0.953125, prec 0.0451682, recall 0.834774
2017-12-10T14:05:49.545673: step 2075, loss 0.095381, acc 0.953125, prec 0.0451645, recall 0.834774
2017-12-10T14:05:49.733980: step 2076, loss 0.126274, acc 0.953125, prec 0.0451608, recall 0.834774
2017-12-10T14:05:49.923010: step 2077, loss 0.0235327, acc 0.984375, prec 0.0451595, recall 0.834774
2017-12-10T14:05:50.113549: step 2078, loss 0.0929332, acc 0.96875, prec 0.045157, recall 0.834774
2017-12-10T14:05:50.303436: step 2079, loss 0.360306, acc 0.953125, prec 0.0452058, recall 0.834942
2017-12-10T14:05:50.492101: step 2080, loss 0.285971, acc 1, prec 0.0452321, recall 0.835025
2017-12-10T14:05:50.683894: step 2081, loss 0.280713, acc 0.9375, prec 0.0452271, recall 0.835025
2017-12-10T14:05:50.874396: step 2082, loss 0.183154, acc 0.96875, prec 0.0452771, recall 0.835193
2017-12-10T14:05:51.064448: step 2083, loss 0.12419, acc 0.96875, prec 0.0453271, recall 0.83536
2017-12-10T14:05:51.251892: step 2084, loss 0.139929, acc 0.9375, prec 0.0453746, recall 0.835526
2017-12-10T14:05:51.438974: step 2085, loss 0.190004, acc 0.953125, prec 0.0453709, recall 0.835526
2017-12-10T14:05:51.637130: step 2086, loss 3.92897, acc 0.953125, prec 0.0454208, recall 0.83527
2017-12-10T14:05:51.829528: step 2087, loss 0.0856667, acc 0.953125, prec 0.0454433, recall 0.835354
2017-12-10T14:05:52.023347: step 2088, loss 0.0888896, acc 0.9375, prec 0.0454383, recall 0.835354
2017-12-10T14:05:52.212493: step 2089, loss 2.49119, acc 0.921875, prec 0.0454595, recall 0.835015
2017-12-10T14:05:52.401543: step 2090, loss 0.063529, acc 0.96875, prec 0.045457, recall 0.835015
2017-12-10T14:05:52.588925: step 2091, loss 0.319753, acc 0.890625, prec 0.0455269, recall 0.835265
2017-12-10T14:05:52.778308: step 2092, loss 0.31198, acc 0.90625, prec 0.0455718, recall 0.83543
2017-12-10T14:05:52.966042: step 2093, loss 0.390028, acc 0.875, prec 0.045588, recall 0.835513
2017-12-10T14:05:53.155970: step 2094, loss 0.275906, acc 0.921875, prec 0.0455818, recall 0.835513
2017-12-10T14:05:53.339626: step 2095, loss 0.360236, acc 0.84375, prec 0.0455693, recall 0.835513
2017-12-10T14:05:53.526569: step 2096, loss 0.196087, acc 0.9375, prec 0.0455643, recall 0.835513
2017-12-10T14:05:53.713056: step 2097, loss 0.391778, acc 0.859375, prec 0.045553, recall 0.835513
2017-12-10T14:05:53.899034: step 2098, loss 0.245236, acc 0.890625, prec 0.0455443, recall 0.835513
2017-12-10T14:05:54.086379: step 2099, loss 0.354754, acc 0.890625, prec 0.0456402, recall 0.835843
2017-12-10T14:05:54.275670: step 2100, loss 0.677399, acc 0.796875, prec 0.0456501, recall 0.835926
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_1/1512932334/checkpoints/model-2100

2017-12-10T14:05:55.608771: step 2101, loss 0.126132, acc 0.9375, prec 0.0456712, recall 0.836008
2017-12-10T14:05:55.801918: step 2102, loss 0.351666, acc 0.953125, prec 0.0457459, recall 0.836254
2017-12-10T14:05:55.988056: step 2103, loss 0.663461, acc 0.859375, prec 0.045813, recall 0.8365
2017-12-10T14:05:56.178632: step 2104, loss 0.377171, acc 0.890625, prec 0.0458565, recall 0.836663
2017-12-10T14:05:56.370990: step 2105, loss 0.330115, acc 0.890625, prec 0.0458477, recall 0.836663
2017-12-10T14:05:56.558923: step 2106, loss 0.210458, acc 0.96875, prec 0.0458452, recall 0.836663
2017-12-10T14:05:56.749827: step 2107, loss 0.384815, acc 0.859375, prec 0.04586, recall 0.836745
2017-12-10T14:05:56.936142: step 2108, loss 0.237386, acc 0.90625, prec 0.0458786, recall 0.836826
2017-12-10T14:05:57.126207: step 2109, loss 0.649718, acc 0.90625, prec 0.0459493, recall 0.83707
2017-12-10T14:05:57.315223: step 2110, loss 0.214059, acc 0.921875, prec 0.0459431, recall 0.83707
2017-12-10T14:05:57.500214: step 2111, loss 0.295689, acc 0.921875, prec 0.045989, recall 0.837232
2017-12-10T14:05:57.688362: step 2112, loss 0.175574, acc 0.9375, prec 0.0459839, recall 0.837232
2017-12-10T14:05:57.881957: step 2113, loss 0.105505, acc 0.96875, prec 0.0459814, recall 0.837232
2017-12-10T14:05:58.069174: step 2114, loss 0.154744, acc 0.96875, prec 0.0460571, recall 0.837475
2017-12-10T14:05:58.258088: step 2115, loss 1.16558, acc 0.9375, prec 0.0461042, recall 0.837637
2017-12-10T14:05:58.452262: step 2116, loss 0.150607, acc 0.96875, prec 0.0461278, recall 0.837717
2017-12-10T14:05:58.641269: step 2117, loss 0.196289, acc 0.875, prec 0.0461177, recall 0.837717
2017-12-10T14:05:58.836100: step 2118, loss 0.236975, acc 0.9375, prec 0.0461127, recall 0.837717
2017-12-10T14:05:59.032335: step 2119, loss 0.278319, acc 0.90625, prec 0.0461051, recall 0.837717
2017-12-10T14:05:59.224076: step 2120, loss 0.443782, acc 0.859375, prec 0.0461198, recall 0.837798
2017-12-10T14:05:59.410976: step 2121, loss 0.29533, acc 0.890625, prec 0.046137, recall 0.837878
2017-12-10T14:05:59.600337: step 2122, loss 0.284821, acc 0.921875, prec 0.0461308, recall 0.837878
2017-12-10T14:05:59.787209: step 2123, loss 0.208804, acc 0.890625, prec 0.0461219, recall 0.837878
2017-12-10T14:05:59.974943: step 2124, loss 0.487961, acc 0.9375, prec 0.0461429, recall 0.837958
2017-12-10T14:06:00.166589: step 2125, loss 0.0931708, acc 0.953125, prec 0.0461392, recall 0.837958
2017-12-10T14:06:00.354427: step 2126, loss 0.369499, acc 0.9375, prec 0.0461601, recall 0.838039
2017-12-10T14:06:00.546950: step 2127, loss 0.269506, acc 0.9375, prec 0.0461811, recall 0.838119
2017-12-10T14:06:00.736141: step 2128, loss 0.166314, acc 0.96875, prec 0.0461786, recall 0.838119
2017-12-10T14:06:00.926210: step 2129, loss 0.0774903, acc 0.96875, prec 0.0462021, recall 0.838199
2017-12-10T14:06:01.113156: step 2130, loss 0.129282, acc 0.953125, prec 0.0461983, recall 0.838199
2017-12-10T14:06:01.304680: step 2131, loss 0.175296, acc 0.921875, prec 0.046218, recall 0.838279
2017-12-10T14:06:01.500402: step 2132, loss 0.112251, acc 0.96875, prec 0.0462155, recall 0.838279
2017-12-10T14:06:01.690591: step 2133, loss 0.415728, acc 0.96875, prec 0.046265, recall 0.838439
2017-12-10T14:06:01.882211: step 2134, loss 0.213369, acc 0.9375, prec 0.04626, recall 0.838439
2017-12-10T14:06:02.068563: step 2135, loss 0.205264, acc 0.9375, prec 0.0462809, recall 0.838519
2017-12-10T14:06:02.255600: step 2136, loss 0.229658, acc 0.9375, prec 0.0462759, recall 0.838519
2017-12-10T14:06:02.443494: step 2137, loss 0.208709, acc 0.890625, prec 0.046267, recall 0.838519
2017-12-10T14:06:02.630224: step 2138, loss 1.60139, acc 0.890625, prec 0.0463374, recall 0.838344
2017-12-10T14:06:02.824556: step 2139, loss 1.45416, acc 0.921875, prec 0.046409, recall 0.838583
2017-12-10T14:06:03.015492: step 2140, loss 0.191311, acc 0.953125, prec 0.0464572, recall 0.838741
2017-12-10T14:06:03.205471: step 2141, loss 0.134739, acc 0.953125, prec 0.0464793, recall 0.838821
2017-12-10T14:06:03.393428: step 2142, loss 0.1596, acc 0.9375, prec 0.0464743, recall 0.838821
2017-12-10T14:06:03.580019: step 2143, loss 2.40451, acc 0.890625, prec 0.0464926, recall 0.838488
2017-12-10T14:06:03.770564: step 2144, loss 0.929498, acc 0.875, prec 0.0465085, recall 0.838567
2017-12-10T14:06:03.958490: step 2145, loss 1.25689, acc 0.9375, prec 0.0465553, recall 0.838726
2017-12-10T14:06:04.147671: step 2146, loss 0.698807, acc 0.859375, prec 0.0466476, recall 0.839041
2017-12-10T14:06:04.336501: step 2147, loss 0.668414, acc 0.828125, prec 0.0466337, recall 0.839041
2017-12-10T14:06:04.522573: step 2148, loss 0.637288, acc 0.84375, prec 0.0466469, recall 0.83912
2017-12-10T14:06:04.709664: step 2149, loss 0.807872, acc 0.796875, prec 0.0466563, recall 0.839198
2017-12-10T14:06:04.893282: step 2150, loss 0.873737, acc 0.796875, prec 0.0466658, recall 0.839277
2017-12-10T14:06:05.081688: step 2151, loss 1.11815, acc 0.6875, prec 0.0466404, recall 0.839277
2017-12-10T14:06:05.271478: step 2152, loss 0.810128, acc 0.8125, prec 0.0466252, recall 0.839277
2017-12-10T14:06:05.458063: step 2153, loss 0.749606, acc 0.765625, prec 0.0466063, recall 0.839277
2017-12-10T14:06:05.645058: step 2154, loss 1.16808, acc 0.6875, prec 0.0466327, recall 0.839434
2017-12-10T14:06:05.832894: step 2155, loss 0.841369, acc 0.765625, prec 0.0466137, recall 0.839434
2017-12-10T14:06:06.022447: step 2156, loss 0.559611, acc 0.796875, prec 0.046649, recall 0.83959
2017-12-10T14:06:06.210727: step 2157, loss 0.593567, acc 0.8125, prec 0.0466854, recall 0.839747
2017-12-10T14:06:06.398223: step 2158, loss 0.703711, acc 0.859375, prec 0.0466741, recall 0.839747
2017-12-10T14:06:06.585226: step 2159, loss 0.777839, acc 0.78125, prec 0.0466564, recall 0.839747
2017-12-10T14:06:06.775068: step 2160, loss 0.32807, acc 0.875, prec 0.0466721, recall 0.839825
2017-12-10T14:06:06.963368: step 2161, loss 0.611756, acc 0.859375, prec 0.0467123, recall 0.839981
2017-12-10T14:06:07.150304: step 2162, loss 0.297195, acc 0.859375, prec 0.0467009, recall 0.839981
2017-12-10T14:06:07.340125: step 2163, loss 0.280632, acc 0.875, prec 0.0466908, recall 0.839981
2017-12-10T14:06:07.525634: step 2164, loss 0.872733, acc 0.890625, prec 0.0467335, recall 0.840136
2017-12-10T14:06:07.714810: step 2165, loss 0.596168, acc 0.84375, prec 0.0467209, recall 0.840136
2017-12-10T14:06:07.901425: step 2166, loss 0.348999, acc 0.875, prec 0.0467365, recall 0.840214
2017-12-10T14:06:08.092672: step 2167, loss 0.140823, acc 0.96875, prec 0.046734, recall 0.840214
2017-12-10T14:06:08.280274: step 2168, loss 3.51995, acc 0.890625, prec 0.0467264, recall 0.839806
2017-12-10T14:06:08.473884: step 2169, loss 0.196775, acc 0.953125, prec 0.0467227, recall 0.839806
2017-12-10T14:06:08.661113: step 2170, loss 0.181493, acc 0.921875, prec 0.0467164, recall 0.839806
2017-12-10T14:06:08.848550: step 2171, loss 0.198255, acc 0.96875, prec 0.0467138, recall 0.839806
2017-12-10T14:06:09.039521: step 2172, loss 0.234698, acc 0.90625, prec 0.0467063, recall 0.839806
2017-12-10T14:06:09.227086: step 2173, loss 0.418017, acc 0.9375, prec 0.0467527, recall 0.839961
2017-12-10T14:06:09.420452: step 2174, loss 0.266349, acc 0.90625, prec 0.0467966, recall 0.840116
2017-12-10T14:06:09.609734: step 2175, loss 0.121717, acc 0.9375, prec 0.0467915, recall 0.840116
2017-12-10T14:06:09.800611: step 2176, loss 0.45221, acc 0.921875, prec 0.0468366, recall 0.840271
2017-12-10T14:06:09.987731: step 2177, loss 0.124487, acc 0.9375, prec 0.0468573, recall 0.840348
2017-12-10T14:06:10.176523: step 2178, loss 0.435087, acc 0.90625, prec 0.0468497, recall 0.840348
2017-12-10T14:06:10.367100: step 2179, loss 2.53333, acc 0.921875, prec 0.0468447, recall 0.839942
2017-12-10T14:06:10.556654: step 2180, loss 0.430269, acc 0.890625, prec 0.0468872, recall 0.840097
2017-12-10T14:06:10.743386: step 2181, loss 0.276969, acc 0.90625, prec 0.0468796, recall 0.840097
2017-12-10T14:06:10.927165: step 2182, loss 0.13089, acc 0.953125, prec 0.0469015, recall 0.840174
2017-12-10T14:06:11.113780: step 2183, loss 0.11675, acc 0.953125, prec 0.0468977, recall 0.840174
2017-12-10T14:06:11.305194: step 2184, loss 0.509324, acc 0.859375, prec 0.0468864, recall 0.840174
2017-12-10T14:06:11.492284: step 2185, loss 2.46788, acc 0.921875, prec 0.0468813, recall 0.839768
2017-12-10T14:06:11.683506: step 2186, loss 0.312439, acc 0.890625, prec 0.0468981, recall 0.839846
2017-12-10T14:06:11.871410: step 2187, loss 0.403864, acc 0.890625, prec 0.0469406, recall 0.84
2017-12-10T14:06:12.056508: step 2188, loss 0.462429, acc 0.8125, prec 0.0469255, recall 0.84
2017-12-10T14:06:12.242396: step 2189, loss 0.447357, acc 0.84375, prec 0.0469128, recall 0.84
2017-12-10T14:06:12.433429: step 2190, loss 0.523927, acc 0.890625, prec 0.0469553, recall 0.840154
2017-12-10T14:06:12.621535: step 2191, loss 0.489071, acc 0.78125, prec 0.0469889, recall 0.840308
2017-12-10T14:06:12.813047: step 2192, loss 0.668768, acc 0.78125, prec 0.0469968, recall 0.840385
2017-12-10T14:06:13.000398: step 2193, loss 3.31739, acc 0.84375, prec 0.0470111, recall 0.840058
2017-12-10T14:06:13.189516: step 2194, loss 1.36337, acc 0.828125, prec 0.0470228, recall 0.840134
2017-12-10T14:06:13.375481: step 2195, loss 0.541377, acc 0.84375, prec 0.0470869, recall 0.840364
2017-12-10T14:06:13.566896: step 2196, loss 0.63379, acc 0.796875, prec 0.0470961, recall 0.840441
2017-12-10T14:06:13.754324: step 2197, loss 0.632561, acc 0.78125, prec 0.047104, recall 0.840517
2017-12-10T14:06:13.946820: step 2198, loss 0.82931, acc 0.84375, prec 0.0471169, recall 0.840594
2017-12-10T14:06:14.139431: step 2199, loss 0.807793, acc 0.75, prec 0.0471222, recall 0.84067
2017-12-10T14:06:14.324439: step 2200, loss 0.891217, acc 0.734375, prec 0.0471263, recall 0.840746
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_1/1512932334/checkpoints/model-2200

2017-12-10T14:06:15.546958: step 2201, loss 0.932263, acc 0.6875, prec 0.0471521, recall 0.840898
2017-12-10T14:06:15.732787: step 2202, loss 0.900075, acc 0.71875, prec 0.0471549, recall 0.840974
2017-12-10T14:06:15.918768: step 2203, loss 0.736924, acc 0.859375, prec 0.0471435, recall 0.840974
2017-12-10T14:06:16.104885: step 2204, loss 0.765488, acc 0.734375, prec 0.0471221, recall 0.840974
2017-12-10T14:06:16.289763: step 2205, loss 0.511341, acc 0.828125, prec 0.0471592, recall 0.841126
2017-12-10T14:06:16.477603: step 2206, loss 0.439195, acc 0.84375, prec 0.0471721, recall 0.841202
2017-12-10T14:06:16.665707: step 2207, loss 0.516249, acc 0.84375, prec 0.0471595, recall 0.841202
2017-12-10T14:06:16.855177: step 2208, loss 0.492277, acc 0.84375, prec 0.0471469, recall 0.841202
2017-12-10T14:06:17.045739: step 2209, loss 0.309844, acc 0.859375, prec 0.047161, recall 0.841277
2017-12-10T14:06:17.235763: step 2210, loss 0.580806, acc 0.859375, prec 0.0472006, recall 0.841429
2017-12-10T14:06:17.428467: step 2211, loss 0.279269, acc 0.875, prec 0.0472159, recall 0.841504
2017-12-10T14:06:17.617529: step 2212, loss 0.178949, acc 0.9375, prec 0.0472109, recall 0.841504
2017-12-10T14:06:17.802294: step 2213, loss 0.494668, acc 0.828125, prec 0.0472733, recall 0.84173
2017-12-10T14:06:17.994817: step 2214, loss 0.0743079, acc 0.96875, prec 0.0472708, recall 0.84173
2017-12-10T14:06:18.182937: step 2215, loss 0.535495, acc 0.890625, prec 0.0472874, recall 0.841805
2017-12-10T14:06:18.374270: step 2216, loss 3.70692, acc 0.859375, prec 0.0473281, recall 0.841556
2017-12-10T14:06:18.564531: step 2217, loss 0.426828, acc 0.90625, prec 0.0473205, recall 0.841556
2017-12-10T14:06:18.750965: step 2218, loss 9.12735, acc 0.875, prec 0.0473117, recall 0.841157
2017-12-10T14:06:18.940662: step 2219, loss 0.553978, acc 0.875, prec 0.047327, recall 0.841232
2017-12-10T14:06:19.129654: step 2220, loss 0.518297, acc 0.828125, prec 0.0473385, recall 0.841307
2017-12-10T14:06:19.320727: step 2221, loss 0.227902, acc 0.9375, prec 0.0473843, recall 0.841458
2017-12-10T14:06:19.511174: step 2222, loss 0.522919, acc 0.875, prec 0.0474249, recall 0.841608
2017-12-10T14:06:19.699831: step 2223, loss 0.46364, acc 0.828125, prec 0.0474364, recall 0.841682
2017-12-10T14:06:19.887839: step 2224, loss 0.640775, acc 0.796875, prec 0.04742, recall 0.841682
2017-12-10T14:06:20.077238: step 2225, loss 0.606872, acc 0.8125, prec 0.0474809, recall 0.841907
2017-12-10T14:06:20.268984: step 2226, loss 0.360071, acc 0.84375, prec 0.0474936, recall 0.841981
2017-12-10T14:06:20.454965: step 2227, loss 0.616841, acc 0.765625, prec 0.0474747, recall 0.841981
2017-12-10T14:06:20.644685: step 2228, loss 0.497074, acc 0.765625, prec 0.0474557, recall 0.841981
2017-12-10T14:06:20.833885: step 2229, loss 0.898623, acc 0.828125, prec 0.0474672, recall 0.842056
2017-12-10T14:06:21.023267: step 2230, loss 0.429707, acc 0.90625, prec 0.0474596, recall 0.842056
2017-12-10T14:06:21.214956: step 2231, loss 0.457897, acc 0.859375, prec 0.0474483, recall 0.842056
2017-12-10T14:06:21.403276: step 2232, loss 0.362705, acc 0.828125, prec 0.0474344, recall 0.842056
2017-12-10T14:06:21.594538: step 2233, loss 0.350009, acc 0.890625, prec 0.0474509, recall 0.84213
2017-12-10T14:06:21.784133: step 2234, loss 0.342082, acc 0.84375, prec 0.0474636, recall 0.842204
2017-12-10T14:06:21.972496: step 2235, loss 0.355233, acc 0.875, prec 0.0474788, recall 0.842279
2017-12-10T14:06:22.159333: step 2236, loss 0.266829, acc 0.890625, prec 0.0474952, recall 0.842353
2017-12-10T14:06:22.352837: step 2237, loss 0.345609, acc 0.875, prec 0.0474851, recall 0.842353
2017-12-10T14:06:22.540438: step 2238, loss 0.347809, acc 0.859375, prec 0.0474738, recall 0.842353
2017-12-10T14:06:22.731266: step 2239, loss 0.338817, acc 0.90625, prec 0.0474915, recall 0.842427
2017-12-10T14:06:22.920699: step 2240, loss 0.433487, acc 0.859375, prec 0.0474802, recall 0.842427
2017-12-10T14:06:23.108355: step 2241, loss 0.284479, acc 0.921875, prec 0.0475244, recall 0.842575
2017-12-10T14:06:23.297158: step 2242, loss 0.285416, acc 0.921875, prec 0.0475433, recall 0.842649
2017-12-10T14:06:23.484423: step 2243, loss 0.153791, acc 0.90625, prec 0.0475358, recall 0.842649
2017-12-10T14:06:23.676686: step 2244, loss 0.092565, acc 0.984375, prec 0.0475597, recall 0.842723
2017-12-10T14:06:23.865444: step 2245, loss 0.0450613, acc 0.984375, prec 0.0475585, recall 0.842723
2017-12-10T14:06:24.052266: step 2246, loss 0.201575, acc 0.890625, prec 0.0475497, recall 0.842723
2017-12-10T14:06:24.243331: step 2247, loss 0.108782, acc 0.96875, prec 0.0475724, recall 0.842797
2017-12-10T14:06:24.431297: step 2248, loss 0.0796477, acc 0.96875, prec 0.0475951, recall 0.842871
2017-12-10T14:06:24.621251: step 2249, loss 0.102233, acc 0.953125, prec 0.0475913, recall 0.842871
2017-12-10T14:06:24.810739: step 2250, loss 5.91148, acc 0.96875, prec 0.04759, recall 0.842475
2017-12-10T14:06:25.000962: step 2251, loss 12.146, acc 0.953125, prec 0.0476127, recall 0.842155
2017-12-10T14:06:25.191549: step 2252, loss 3.89936, acc 0.921875, prec 0.0476077, recall 0.84176
2017-12-10T14:06:25.387023: step 2253, loss 2.48523, acc 0.84375, prec 0.0475964, recall 0.841366
2017-12-10T14:06:25.574524: step 2254, loss 0.422423, acc 0.875, prec 0.0475863, recall 0.841366
2017-12-10T14:06:25.764898: step 2255, loss 0.37712, acc 0.84375, prec 0.0475737, recall 0.841366
2017-12-10T14:06:25.952034: step 2256, loss 0.633272, acc 0.75, prec 0.0475787, recall 0.841441
2017-12-10T14:06:26.142281: step 2257, loss 0.71944, acc 0.8125, prec 0.047614, recall 0.841589
2017-12-10T14:06:26.328367: step 2258, loss 1.15087, acc 0.703125, prec 0.0476153, recall 0.841663
2017-12-10T14:06:26.516669: step 2259, loss 1.29132, acc 0.671875, prec 0.0476895, recall 0.841958
2017-12-10T14:06:26.703537: step 2260, loss 0.944475, acc 0.703125, prec 0.0476655, recall 0.841958
2017-12-10T14:06:26.892211: step 2261, loss 0.934101, acc 0.765625, prec 0.0476467, recall 0.841958
2017-12-10T14:06:27.076874: step 2262, loss 1.05629, acc 0.671875, prec 0.0476705, recall 0.842105
2017-12-10T14:06:27.262985: step 2263, loss 1.24178, acc 0.53125, prec 0.0476328, recall 0.842105
2017-12-10T14:06:27.455321: step 2264, loss 1.28507, acc 0.65625, prec 0.0476303, recall 0.842179
2017-12-10T14:06:27.645207: step 2265, loss 0.994695, acc 0.765625, prec 0.0476617, recall 0.842326
2017-12-10T14:06:27.833258: step 2266, loss 1.28874, acc 0.6875, prec 0.0476616, recall 0.842399
2017-12-10T14:06:28.016686: step 2267, loss 1.00714, acc 0.6875, prec 0.0476366, recall 0.842399
2017-12-10T14:06:28.205042: step 2268, loss 0.866154, acc 0.78125, prec 0.0476441, recall 0.842472
2017-12-10T14:06:28.392957: step 2269, loss 0.815158, acc 0.765625, prec 0.0476253, recall 0.842472
2017-12-10T14:06:28.580866: step 2270, loss 0.678081, acc 0.734375, prec 0.0476291, recall 0.842545
2017-12-10T14:06:28.767529: step 2271, loss 0.599035, acc 0.8125, prec 0.047639, recall 0.842618
2017-12-10T14:06:28.960678: step 2272, loss 0.599857, acc 0.859375, prec 0.0476278, recall 0.842618
2017-12-10T14:06:29.149620: step 2273, loss 0.397225, acc 0.90625, prec 0.0476203, recall 0.842618
2017-12-10T14:06:29.335800: step 2274, loss 0.426013, acc 0.859375, prec 0.047634, recall 0.842691
2017-12-10T14:06:29.525910: step 2275, loss 0.142487, acc 0.921875, prec 0.0476278, recall 0.842691
2017-12-10T14:06:29.716419: step 2276, loss 0.360592, acc 0.875, prec 0.0476178, recall 0.842691
2017-12-10T14:06:29.905566: step 2277, loss 0.517557, acc 0.890625, prec 0.0476091, recall 0.842691
2017-12-10T14:06:30.095406: step 2278, loss 0.167073, acc 0.953125, prec 0.0476053, recall 0.842691
2017-12-10T14:06:30.282694: step 2279, loss 0.319211, acc 0.890625, prec 0.0476215, recall 0.842764
2017-12-10T14:06:30.479339: step 2280, loss 0.057918, acc 0.984375, prec 0.0476453, recall 0.842837
2017-12-10T14:06:30.669560: step 2281, loss 0.0944972, acc 0.96875, prec 0.0476677, recall 0.84291
2017-12-10T14:06:30.862312: step 2282, loss 0.142589, acc 0.96875, prec 0.0476902, recall 0.842983
2017-12-10T14:06:31.063291: step 2283, loss 0.809283, acc 1, prec 0.0477401, recall 0.843128
2017-12-10T14:06:31.258581: step 2284, loss 0.330337, acc 0.96875, prec 0.0477625, recall 0.843201
2017-12-10T14:06:31.456879: step 2285, loss 0.142985, acc 0.96875, prec 0.047785, recall 0.843273
2017-12-10T14:06:31.647496: step 2286, loss 6.00066, acc 0.96875, prec 0.0478087, recall 0.842956
2017-12-10T14:06:31.841449: step 2287, loss 0.0732715, acc 0.96875, prec 0.047856, recall 0.843101
2017-12-10T14:06:32.030356: step 2288, loss 0.046422, acc 0.984375, prec 0.0478548, recall 0.843101
2017-12-10T14:06:32.214472: step 2289, loss 0.122412, acc 0.921875, prec 0.0478735, recall 0.843173
2017-12-10T14:06:32.401814: step 2290, loss 0.0834034, acc 0.96875, prec 0.0478959, recall 0.843246
2017-12-10T14:06:32.587879: step 2291, loss 0.43029, acc 0.859375, prec 0.0479344, recall 0.84339
2017-12-10T14:06:32.778787: step 2292, loss 0.0912755, acc 0.984375, prec 0.0479581, recall 0.843462
2017-12-10T14:06:32.967320: step 2293, loss 0.223732, acc 0.90625, prec 0.0479755, recall 0.843534
2017-12-10T14:06:33.152790: step 2294, loss 0.393745, acc 0.84375, prec 0.0479879, recall 0.843606
2017-12-10T14:06:33.340166: step 2295, loss 0.364213, acc 0.921875, prec 0.0479816, recall 0.843606
2017-12-10T14:06:33.526509: step 2296, loss 0.296328, acc 0.90625, prec 0.0479741, recall 0.843606
2017-12-10T14:06:33.712631: step 2297, loss 0.30113, acc 0.875, prec 0.047964, recall 0.843606
2017-12-10T14:06:33.902711: step 2298, loss 0.289436, acc 0.890625, prec 0.0479552, recall 0.843606
2017-12-10T14:06:34.092353: step 2299, loss 0.217686, acc 0.921875, prec 0.047949, recall 0.843606
2017-12-10T14:06:34.278969: step 2300, loss 0.205839, acc 0.9375, prec 0.047944, recall 0.843606
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_1/1512932334/checkpoints/model-2300

2017-12-10T14:06:35.664247: step 2301, loss 0.0761299, acc 0.953125, prec 0.0479402, recall 0.843606
2017-12-10T14:06:35.852320: step 2302, loss 0.307985, acc 0.90625, prec 0.0479327, recall 0.843606
2017-12-10T14:06:36.045267: step 2303, loss 0.461023, acc 0.921875, prec 0.0479762, recall 0.84375
2017-12-10T14:06:36.237157: step 2304, loss 0.137867, acc 0.9375, prec 0.047996, recall 0.843822
2017-12-10T14:06:36.423150: step 2305, loss 0.145531, acc 0.953125, prec 0.0479923, recall 0.843822
2017-12-10T14:06:36.606824: step 2306, loss 0.262983, acc 0.953125, prec 0.0480382, recall 0.843965
2017-12-10T14:06:36.800129: step 2307, loss 0.286137, acc 0.890625, prec 0.0480543, recall 0.844037
2017-12-10T14:06:36.992794: step 2308, loss 0.332566, acc 0.90625, prec 0.0480468, recall 0.844037
2017-12-10T14:06:37.182546: step 2309, loss 3.86211, acc 0.890625, prec 0.0480641, recall 0.843721
2017-12-10T14:06:37.375391: step 2310, loss 0.242748, acc 0.9375, prec 0.0480839, recall 0.843793
2017-12-10T14:06:37.564260: step 2311, loss 0.331162, acc 0.90625, prec 0.0480764, recall 0.843793
2017-12-10T14:06:37.754867: step 2312, loss 0.26634, acc 0.875, prec 0.0480664, recall 0.843793
2017-12-10T14:06:37.939776: step 2313, loss 0.641448, acc 0.875, prec 0.0480812, recall 0.843864
2017-12-10T14:06:38.127995: step 2314, loss 0.691418, acc 0.828125, prec 0.0480674, recall 0.843864
2017-12-10T14:06:38.322726: step 2315, loss 0.368852, acc 0.921875, prec 0.0480611, recall 0.843864
2017-12-10T14:06:38.516941: step 2316, loss 0.344459, acc 0.90625, prec 0.0480784, recall 0.843936
2017-12-10T14:06:38.702245: step 2317, loss 0.318361, acc 0.90625, prec 0.0481205, recall 0.844079
2017-12-10T14:06:38.890717: step 2318, loss 0.434996, acc 0.90625, prec 0.048113, recall 0.844079
2017-12-10T14:06:39.076705: step 2319, loss 0.195935, acc 0.9375, prec 0.048108, recall 0.844079
2017-12-10T14:06:39.266805: step 2320, loss 0.359826, acc 0.921875, prec 0.0481017, recall 0.844079
2017-12-10T14:06:39.458187: step 2321, loss 0.304949, acc 0.90625, prec 0.048119, recall 0.84415
2017-12-10T14:06:39.644448: step 2322, loss 0.138543, acc 0.9375, prec 0.0481388, recall 0.844221
2017-12-10T14:06:39.828606: step 2323, loss 0.130283, acc 0.984375, prec 0.0481375, recall 0.844221
2017-12-10T14:06:40.017942: step 2324, loss 0.990999, acc 0.921875, prec 0.0481561, recall 0.844292
2017-12-10T14:06:40.206961: step 2325, loss 0.282867, acc 0.9375, prec 0.048151, recall 0.844292
2017-12-10T14:06:40.395602: step 2326, loss 0.0377229, acc 0.984375, prec 0.0481498, recall 0.844292
2017-12-10T14:06:40.584421: step 2327, loss 0.689282, acc 0.875, prec 0.0481893, recall 0.844434
2017-12-10T14:06:40.775064: step 2328, loss 0.180858, acc 0.953125, prec 0.0481856, recall 0.844434
2017-12-10T14:06:40.961936: step 2329, loss 0.135661, acc 0.953125, prec 0.0481818, recall 0.844434
2017-12-10T14:06:41.152937: step 2330, loss 0.187477, acc 0.921875, prec 0.0481755, recall 0.844434
2017-12-10T14:06:41.340489: step 2331, loss 0.147555, acc 0.921875, prec 0.0481693, recall 0.844434
2017-12-10T14:06:41.533552: step 2332, loss 0.19899, acc 0.953125, prec 0.0481903, recall 0.844505
2017-12-10T14:06:41.720476: step 2333, loss 0.103402, acc 0.9375, prec 0.0481852, recall 0.844505
2017-12-10T14:06:41.911634: step 2334, loss 0.223916, acc 0.9375, prec 0.048205, recall 0.844576
2017-12-10T14:06:42.104011: step 2335, loss 1.88175, acc 0.921875, prec 0.0482247, recall 0.844262
2017-12-10T14:06:42.294498: step 2336, loss 0.303365, acc 0.9375, prec 0.0482445, recall 0.844333
2017-12-10T14:06:42.482389: step 2337, loss 0.415209, acc 0.96875, prec 0.0482667, recall 0.844404
2017-12-10T14:06:42.672579: step 2338, loss 0.287703, acc 0.921875, prec 0.0482604, recall 0.844404
2017-12-10T14:06:42.864354: step 2339, loss 0.173638, acc 0.953125, prec 0.0482567, recall 0.844404
2017-12-10T14:06:43.048225: step 2340, loss 0.265858, acc 0.9375, prec 0.0482517, recall 0.844404
2017-12-10T14:06:43.237682: step 2341, loss 0.232843, acc 0.90625, prec 0.0482441, recall 0.844404
2017-12-10T14:06:43.429534: step 2342, loss 0.412288, acc 0.890625, prec 0.0482601, recall 0.844475
2017-12-10T14:06:43.618171: step 2343, loss 0.480039, acc 0.921875, prec 0.0482786, recall 0.844545
2017-12-10T14:06:43.810166: step 2344, loss 0.272098, acc 0.9375, prec 0.0482735, recall 0.844545
2017-12-10T14:06:44.004374: step 2345, loss 0.135099, acc 0.9375, prec 0.0482685, recall 0.844545
2017-12-10T14:06:44.190960: step 2346, loss 0.231765, acc 0.890625, prec 0.0482597, recall 0.844545
2017-12-10T14:06:44.377238: step 2347, loss 0.42477, acc 0.890625, prec 0.048251, recall 0.844545
2017-12-10T14:06:44.562322: step 2348, loss 0.482627, acc 0.890625, prec 0.0482916, recall 0.844687
2017-12-10T14:06:44.748563: step 2349, loss 1.76634, acc 0.859375, prec 0.0483063, recall 0.844374
2017-12-10T14:06:44.939113: step 2350, loss 0.168491, acc 0.96875, prec 0.0483285, recall 0.844444
2017-12-10T14:06:45.129622: step 2351, loss 0.270302, acc 0.9375, prec 0.0483235, recall 0.844444
2017-12-10T14:06:45.318406: step 2352, loss 0.532347, acc 0.828125, prec 0.0483591, recall 0.844585
2017-12-10T14:06:45.508281: step 2353, loss 1.81539, acc 0.921875, prec 0.048354, recall 0.844203
2017-12-10T14:06:45.697654: step 2354, loss 0.277606, acc 0.90625, prec 0.0483465, recall 0.844203
2017-12-10T14:06:45.885805: step 2355, loss 0.531611, acc 0.828125, prec 0.0483574, recall 0.844273
2017-12-10T14:06:46.074239: step 2356, loss 0.468226, acc 0.875, prec 0.0483474, recall 0.844273
2017-12-10T14:06:46.263740: step 2357, loss 0.456897, acc 0.84375, prec 0.0483842, recall 0.844414
2017-12-10T14:06:46.451621: step 2358, loss 0.816107, acc 0.796875, prec 0.0483679, recall 0.844414
2017-12-10T14:06:46.637566: step 2359, loss 3.16395, acc 0.84375, prec 0.0483566, recall 0.844033
2017-12-10T14:06:46.832136: step 2360, loss 0.783848, acc 0.859375, prec 0.0483946, recall 0.844173
2017-12-10T14:06:47.020030: step 2361, loss 0.804143, acc 0.796875, prec 0.0484276, recall 0.844314
2017-12-10T14:06:47.206893: step 2362, loss 0.601836, acc 0.765625, prec 0.0484088, recall 0.844314
2017-12-10T14:06:47.398654: step 2363, loss 0.621032, acc 0.8125, prec 0.0483938, recall 0.844314
2017-12-10T14:06:47.584432: step 2364, loss 0.58648, acc 0.796875, prec 0.0484021, recall 0.844384
2017-12-10T14:06:47.774268: step 2365, loss 0.758161, acc 0.765625, prec 0.0483833, recall 0.844384
2017-12-10T14:06:47.961495: step 2366, loss 0.552847, acc 0.875, prec 0.0483733, recall 0.844384
2017-12-10T14:06:48.150469: step 2367, loss 0.892694, acc 0.78125, prec 0.048405, recall 0.844525
2017-12-10T14:06:48.337252: step 2368, loss 0.406626, acc 0.828125, prec 0.0483913, recall 0.844525
2017-12-10T14:06:48.525792: step 2369, loss 0.627826, acc 0.796875, prec 0.048375, recall 0.844525
2017-12-10T14:06:48.715699: step 2370, loss 0.646204, acc 0.84375, prec 0.0483871, recall 0.844595
2017-12-10T14:06:48.902562: step 2371, loss 0.978954, acc 0.828125, prec 0.0483979, recall 0.844665
2017-12-10T14:06:49.090053: step 2372, loss 6.09194, acc 0.90625, prec 0.0484162, recall 0.844354
2017-12-10T14:06:49.280862: step 2373, loss 0.598251, acc 0.828125, prec 0.0484025, recall 0.844354
2017-12-10T14:06:49.467888: step 2374, loss 3.15421, acc 0.84375, prec 0.0484158, recall 0.844045
2017-12-10T14:06:49.653950: step 2375, loss 0.674601, acc 0.828125, prec 0.0484021, recall 0.844045
2017-12-10T14:06:49.837201: step 2376, loss 0.889866, acc 0.890625, prec 0.0484424, recall 0.844185
2017-12-10T14:06:50.025370: step 2377, loss 0.410624, acc 0.890625, prec 0.0484336, recall 0.844185
2017-12-10T14:06:50.214411: step 2378, loss 0.817151, acc 0.8125, prec 0.0484677, recall 0.844325
2017-12-10T14:06:50.401311: step 2379, loss 0.561262, acc 0.8125, prec 0.0484772, recall 0.844395
2017-12-10T14:06:50.590273: step 2380, loss 0.719932, acc 0.796875, prec 0.0484855, recall 0.844464
2017-12-10T14:06:50.773938: step 2381, loss 0.996908, acc 0.6875, prec 0.0484605, recall 0.844464
2017-12-10T14:06:50.964456: step 2382, loss 1.02766, acc 0.75, prec 0.0484651, recall 0.844534
2017-12-10T14:06:51.153544: step 2383, loss 0.813168, acc 0.703125, prec 0.0484658, recall 0.844604
2017-12-10T14:06:51.343147: step 2384, loss 4.37498, acc 0.828125, prec 0.0484534, recall 0.844226
2017-12-10T14:06:51.533742: step 2385, loss 0.303143, acc 0.890625, prec 0.0484447, recall 0.844226
2017-12-10T14:06:51.723919: step 2386, loss 0.497195, acc 0.828125, prec 0.0484799, recall 0.844365
2017-12-10T14:06:51.910869: step 2387, loss 0.923226, acc 0.75, prec 0.0484844, recall 0.844434
2017-12-10T14:06:52.101010: step 2388, loss 0.952295, acc 0.796875, prec 0.0484926, recall 0.844504
2017-12-10T14:06:52.286810: step 2389, loss 0.796006, acc 0.796875, prec 0.0485009, recall 0.844573
2017-12-10T14:06:52.473602: step 2390, loss 0.610471, acc 0.78125, prec 0.0484834, recall 0.844573
2017-12-10T14:06:52.664202: step 2391, loss 0.91371, acc 0.734375, prec 0.0485111, recall 0.844712
2017-12-10T14:06:52.850308: step 2392, loss 0.61223, acc 0.796875, prec 0.0484949, recall 0.844712
2017-12-10T14:06:53.035399: step 2393, loss 0.513921, acc 0.828125, prec 0.0484813, recall 0.844712
2017-12-10T14:06:53.222871: step 2394, loss 0.830654, acc 0.734375, prec 0.0484602, recall 0.844712
2017-12-10T14:06:53.409536: step 2395, loss 0.345794, acc 0.859375, prec 0.0484977, recall 0.844851
2017-12-10T14:06:53.597331: step 2396, loss 0.423415, acc 0.90625, prec 0.0485146, recall 0.84492
2017-12-10T14:06:53.788986: step 2397, loss 0.834248, acc 0.796875, prec 0.0485715, recall 0.845127
2017-12-10T14:06:53.976751: step 2398, loss 0.531867, acc 0.859375, prec 0.0485847, recall 0.845196
2017-12-10T14:06:54.161700: step 2399, loss 0.880892, acc 0.796875, prec 0.0485685, recall 0.845196
2017-12-10T14:06:54.349481: step 2400, loss 0.286161, acc 0.90625, prec 0.0485854, recall 0.845265
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_1/1512932334/checkpoints/model-2400

2017-12-10T14:06:55.656553: step 2401, loss 0.239867, acc 0.9375, prec 0.048629, recall 0.845402
2017-12-10T14:06:55.844950: step 2402, loss 0.253049, acc 0.875, prec 0.0486191, recall 0.845402
2017-12-10T14:06:56.039488: step 2403, loss 0.235002, acc 0.890625, prec 0.0486104, recall 0.845402
2017-12-10T14:06:56.227131: step 2404, loss 0.100737, acc 0.953125, prec 0.0486067, recall 0.845402
2017-12-10T14:06:56.416058: step 2405, loss 0.285744, acc 0.9375, prec 0.0486503, recall 0.845539
2017-12-10T14:06:56.609721: step 2406, loss 0.362295, acc 0.890625, prec 0.0486659, recall 0.845608
2017-12-10T14:06:56.798624: step 2407, loss 0.154916, acc 0.953125, prec 0.0486622, recall 0.845608
2017-12-10T14:06:56.989038: step 2408, loss 0.46866, acc 0.9375, prec 0.0486815, recall 0.845676
2017-12-10T14:06:57.175817: step 2409, loss 0.203917, acc 0.890625, prec 0.0486728, recall 0.845676
2017-12-10T14:06:57.365274: step 2410, loss 0.57786, acc 0.953125, prec 0.0486933, recall 0.845745
2017-12-10T14:06:57.555242: step 2411, loss 0.150172, acc 0.921875, prec 0.0486871, recall 0.845745
2017-12-10T14:06:57.745684: step 2412, loss 3.18957, acc 0.9375, prec 0.0487319, recall 0.845507
2017-12-10T14:06:57.937359: step 2413, loss 4.30334, acc 0.859375, prec 0.0487705, recall 0.84527
2017-12-10T14:06:58.134490: step 2414, loss 0.224791, acc 0.921875, prec 0.0487643, recall 0.84527
2017-12-10T14:06:58.323059: step 2415, loss 0.331221, acc 0.890625, prec 0.0487556, recall 0.84527
2017-12-10T14:06:58.507332: step 2416, loss 0.986373, acc 0.796875, prec 0.0487879, recall 0.845406
2017-12-10T14:06:58.700182: step 2417, loss 0.538135, acc 0.796875, prec 0.0487718, recall 0.845406
2017-12-10T14:06:58.890878: step 2418, loss 0.382061, acc 0.859375, prec 0.0487606, recall 0.845406
2017-12-10T14:06:59.084782: step 2419, loss 0.356865, acc 0.875, prec 0.0487507, recall 0.845406
2017-12-10T14:06:59.274535: step 2420, loss 0.689457, acc 0.828125, prec 0.0488097, recall 0.845611
2017-12-10T14:06:59.460552: step 2421, loss 0.555968, acc 0.890625, prec 0.0488252, recall 0.845679
2017-12-10T14:06:59.649200: step 2422, loss 0.626558, acc 0.765625, prec 0.0488066, recall 0.845679
2017-12-10T14:06:59.838251: step 2423, loss 1.00361, acc 0.703125, prec 0.048783, recall 0.845679
2017-12-10T14:07:00.026128: step 2424, loss 0.509228, acc 0.859375, prec 0.0487718, recall 0.845679
2017-12-10T14:07:00.213716: step 2425, loss 0.416406, acc 0.8125, prec 0.0487811, recall 0.845747
2017-12-10T14:07:00.402014: step 2426, loss 1.51141, acc 0.890625, prec 0.0488208, recall 0.845883
2017-12-10T14:07:00.595659: step 2427, loss 0.630441, acc 0.84375, prec 0.0488325, recall 0.845951
2017-12-10T14:07:00.784107: step 2428, loss 0.527233, acc 0.796875, prec 0.0488406, recall 0.846018
2017-12-10T14:07:00.969676: step 2429, loss 0.539924, acc 0.765625, prec 0.0488703, recall 0.846154
2017-12-10T14:07:01.161972: step 2430, loss 0.221724, acc 0.921875, prec 0.0488641, recall 0.846154
2017-12-10T14:07:01.357004: step 2431, loss 0.251161, acc 0.84375, prec 0.0488999, recall 0.846289
2017-12-10T14:07:01.547419: step 2432, loss 0.416995, acc 0.84375, prec 0.0488875, recall 0.846289
2017-12-10T14:07:01.734666: step 2433, loss 0.553162, acc 0.8125, prec 0.0488968, recall 0.846356
2017-12-10T14:07:01.920252: step 2434, loss 0.651374, acc 0.78125, prec 0.0489276, recall 0.846491
2017-12-10T14:07:02.108606: step 2435, loss 0.406079, acc 0.828125, prec 0.048914, recall 0.846491
2017-12-10T14:07:02.299114: step 2436, loss 1.4942, acc 0.859375, prec 0.0489282, recall 0.846188
2017-12-10T14:07:02.486935: step 2437, loss 0.570288, acc 0.859375, prec 0.0489411, recall 0.846255
2017-12-10T14:07:02.675294: step 2438, loss 0.221412, acc 0.9375, prec 0.0489603, recall 0.846322
2017-12-10T14:07:02.859505: step 2439, loss 0.304017, acc 0.890625, prec 0.0489516, recall 0.846322
2017-12-10T14:07:03.048956: step 2440, loss 0.502107, acc 0.859375, prec 0.0489404, recall 0.846322
2017-12-10T14:07:03.241819: step 2441, loss 0.558718, acc 0.828125, prec 0.0489509, recall 0.846389
2017-12-10T14:07:03.427441: step 2442, loss 0.24445, acc 0.875, prec 0.048941, recall 0.846389
2017-12-10T14:07:03.616316: step 2443, loss 0.612786, acc 0.875, prec 0.0489311, recall 0.846389
2017-12-10T14:07:03.801715: step 2444, loss 0.667963, acc 0.890625, prec 0.0489705, recall 0.846524
2017-12-10T14:07:03.991731: step 2445, loss 0.277612, acc 0.9375, prec 0.0489896, recall 0.846591
2017-12-10T14:07:04.185017: step 2446, loss 0.143054, acc 0.9375, prec 0.0490087, recall 0.846658
2017-12-10T14:07:04.375019: step 2447, loss 0.165415, acc 0.921875, prec 0.0490025, recall 0.846658
2017-12-10T14:07:04.560031: step 2448, loss 0.166984, acc 0.953125, prec 0.0490469, recall 0.846792
2017-12-10T14:07:04.749103: step 2449, loss 0.596168, acc 0.90625, prec 0.0490635, recall 0.846859
2017-12-10T14:07:04.938506: step 2450, loss 1.47851, acc 0.96875, prec 0.0490863, recall 0.846556
2017-12-10T14:07:05.128201: step 2451, loss 0.170706, acc 0.953125, prec 0.0491066, recall 0.846623
2017-12-10T14:07:05.318529: step 2452, loss 0.290647, acc 0.953125, prec 0.0491509, recall 0.846757
2017-12-10T14:07:05.507677: step 2453, loss 0.195512, acc 0.953125, prec 0.0491712, recall 0.846823
2017-12-10T14:07:05.696731: step 2454, loss 1.92818, acc 0.953125, prec 0.0491687, recall 0.846455
2017-12-10T14:07:05.882877: step 2455, loss 0.297171, acc 0.90625, prec 0.0491853, recall 0.846522
2017-12-10T14:07:06.069764: step 2456, loss 0.311937, acc 0.921875, prec 0.0491791, recall 0.846522
2017-12-10T14:07:06.255621: step 2457, loss 0.230226, acc 0.96875, prec 0.0492006, recall 0.846588
2017-12-10T14:07:06.448259: step 2458, loss 0.420753, acc 0.875, prec 0.0492147, recall 0.846655
2017-12-10T14:07:06.640084: step 2459, loss 0.371755, acc 0.890625, prec 0.049278, recall 0.846855
2017-12-10T14:07:06.828504: step 2460, loss 0.828867, acc 0.90625, prec 0.0493425, recall 0.847054
2017-12-10T14:07:07.017965: step 2461, loss 0.389365, acc 0.90625, prec 0.049335, recall 0.847054
2017-12-10T14:07:07.203619: step 2462, loss 0.386014, acc 0.859375, prec 0.0493478, recall 0.84712
2017-12-10T14:07:07.396500: step 2463, loss 0.486947, acc 0.84375, prec 0.0493594, recall 0.847186
2017-12-10T14:07:07.584538: step 2464, loss 0.507358, acc 0.875, prec 0.0494213, recall 0.847384
2017-12-10T14:07:07.771037: step 2465, loss 0.358813, acc 0.8125, prec 0.0494303, recall 0.84745
2017-12-10T14:07:07.965906: step 2466, loss 0.190175, acc 0.921875, prec 0.0494241, recall 0.84745
2017-12-10T14:07:08.151667: step 2467, loss 0.406655, acc 0.875, prec 0.049462, recall 0.847582
2017-12-10T14:07:08.341701: step 2468, loss 0.875212, acc 0.875, prec 0.0495, recall 0.847714
2017-12-10T14:07:08.529396: step 2469, loss 0.231517, acc 0.90625, prec 0.0495643, recall 0.84791
2017-12-10T14:07:08.716508: step 2470, loss 0.64753, acc 0.8125, prec 0.0495733, recall 0.847976
2017-12-10T14:07:08.904146: step 2471, loss 0.704913, acc 0.875, prec 0.0495872, recall 0.848041
2017-12-10T14:07:09.096024: step 2472, loss 1.67828, acc 0.84375, prec 0.049576, recall 0.847676
2017-12-10T14:07:09.292597: step 2473, loss 0.329018, acc 0.84375, prec 0.0495635, recall 0.847676
2017-12-10T14:07:09.479809: step 2474, loss 0.248473, acc 0.9375, prec 0.0495824, recall 0.847742
2017-12-10T14:07:09.672930: step 2475, loss 0.816215, acc 0.859375, prec 0.0496429, recall 0.847938
2017-12-10T14:07:09.863606: step 2476, loss 0.53823, acc 0.859375, prec 0.0496556, recall 0.848003
2017-12-10T14:07:10.053930: step 2477, loss 0.462535, acc 0.890625, prec 0.0496468, recall 0.848003
2017-12-10T14:07:10.241654: step 2478, loss 0.665866, acc 0.765625, prec 0.0496281, recall 0.848003
2017-12-10T14:07:10.431320: step 2479, loss 0.290036, acc 0.90625, prec 0.0496206, recall 0.848003
2017-12-10T14:07:10.616623: step 2480, loss 0.275556, acc 0.921875, prec 0.0496621, recall 0.848134
2017-12-10T14:07:10.806325: step 2481, loss 0.416559, acc 0.90625, prec 0.0496547, recall 0.848134
2017-12-10T14:07:10.995466: step 2482, loss 0.20724, acc 0.9375, prec 0.0496735, recall 0.848199
2017-12-10T14:07:11.186494: step 2483, loss 0.454196, acc 0.921875, prec 0.0496912, recall 0.848264
2017-12-10T14:07:11.381685: step 2484, loss 0.576991, acc 0.875, prec 0.049705, recall 0.848329
2017-12-10T14:07:11.557469: step 2485, loss 0.837504, acc 0.862745, prec 0.0497202, recall 0.848394
2017-12-10T14:07:11.750204: step 2486, loss 0.0885207, acc 0.9375, prec 0.049739, recall 0.848459
2017-12-10T14:07:11.938828: step 2487, loss 0.345339, acc 0.875, prec 0.049729, recall 0.848459
2017-12-10T14:07:12.124702: step 2488, loss 0.125154, acc 0.9375, prec 0.049724, recall 0.848459
2017-12-10T14:07:12.316617: step 2489, loss 0.207894, acc 0.90625, prec 0.0497404, recall 0.848524
2017-12-10T14:07:12.508393: step 2490, loss 0.174704, acc 0.9375, prec 0.0497592, recall 0.848589
2017-12-10T14:07:12.698447: step 2491, loss 0.242409, acc 0.96875, prec 0.0497806, recall 0.848653
2017-12-10T14:07:12.886875: step 2492, loss 0.72103, acc 0.921875, prec 0.0497981, recall 0.848718
2017-12-10T14:07:13.079222: step 2493, loss 0.299748, acc 0.921875, prec 0.0498157, recall 0.848783
2017-12-10T14:07:13.266013: step 2494, loss 0.188302, acc 0.921875, prec 0.0498095, recall 0.848783
2017-12-10T14:07:13.453417: step 2495, loss 0.312887, acc 0.890625, prec 0.0498246, recall 0.848847
2017-12-10T14:07:13.644093: step 2496, loss 0.431603, acc 0.859375, prec 0.0498847, recall 0.849041
2017-12-10T14:07:13.835709: step 2497, loss 0.828246, acc 0.9375, prec 0.0499035, recall 0.849105
2017-12-10T14:07:14.036365: step 2498, loss 0.14714, acc 0.953125, prec 0.0499236, recall 0.849169
2017-12-10T14:07:14.228377: step 2499, loss 0.166591, acc 0.921875, prec 0.0499173, recall 0.849169
2017-12-10T14:07:14.417745: step 2500, loss 0.413215, acc 0.953125, prec 0.0499374, recall 0.849233
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_1/1512932334/checkpoints/model-2500

2017-12-10T14:07:15.708113: step 2501, loss 1.43393, acc 0.9375, prec 0.0499336, recall 0.848872
2017-12-10T14:07:15.895177: step 2502, loss 0.293832, acc 0.90625, prec 0.0499499, recall 0.848936
2017-12-10T14:07:16.083839: step 2503, loss 0.235874, acc 0.9375, prec 0.0499687, recall 0.849
2017-12-10T14:07:16.274366: step 2504, loss 0.0843568, acc 0.96875, prec 0.0499662, recall 0.849
2017-12-10T14:07:16.464315: step 2505, loss 0.161401, acc 0.90625, prec 0.0499587, recall 0.849
2017-12-10T14:07:16.649463: step 2506, loss 0.267221, acc 0.90625, prec 0.0499512, recall 0.849
2017-12-10T14:07:16.837426: step 2507, loss 1.06951, acc 0.9375, prec 0.0499937, recall 0.849129
2017-12-10T14:07:17.031263: step 2508, loss 0.252031, acc 0.90625, prec 0.05001, recall 0.849193
2017-12-10T14:07:17.220336: step 2509, loss 0.261806, acc 0.90625, prec 0.0500025, recall 0.849193
2017-12-10T14:07:17.406949: step 2510, loss 0.385304, acc 0.875, prec 0.0499925, recall 0.849193
2017-12-10T14:07:17.597692: step 2511, loss 0.169393, acc 0.9375, prec 0.050035, recall 0.849321
2017-12-10T14:07:17.786513: step 2512, loss 0.248306, acc 0.921875, prec 0.0500763, recall 0.849449
2017-12-10T14:07:17.977610: step 2513, loss 0.243855, acc 0.921875, prec 0.050165, recall 0.849704
2017-12-10T14:07:18.166850: step 2514, loss 0.349488, acc 0.90625, prec 0.0501574, recall 0.849704
2017-12-10T14:07:18.354244: step 2515, loss 0.221737, acc 0.90625, prec 0.0501499, recall 0.849704
2017-12-10T14:07:18.541352: step 2516, loss 0.231675, acc 0.90625, prec 0.0501899, recall 0.849831
2017-12-10T14:07:18.732151: step 2517, loss 0.342087, acc 0.875, prec 0.0502273, recall 0.849958
2017-12-10T14:07:18.920584: step 2518, loss 1.68826, acc 0.796875, prec 0.0502584, recall 0.850084
2017-12-10T14:07:19.108391: step 2519, loss 0.211393, acc 0.9375, prec 0.0502534, recall 0.850084
2017-12-10T14:07:19.293104: step 2520, loss 0.215454, acc 0.921875, prec 0.0502471, recall 0.850084
2017-12-10T14:07:19.484961: step 2521, loss 0.155893, acc 0.9375, prec 0.0502895, recall 0.850211
2017-12-10T14:07:19.668454: step 2522, loss 0.564682, acc 0.859375, prec 0.0502782, recall 0.850211
2017-12-10T14:07:19.854627: step 2523, loss 0.349981, acc 0.953125, prec 0.0502745, recall 0.850211
2017-12-10T14:07:20.041004: step 2524, loss 0.382868, acc 0.921875, prec 0.0502682, recall 0.850211
2017-12-10T14:07:20.225878: step 2525, loss 0.220797, acc 0.90625, prec 0.0502843, recall 0.850274
2017-12-10T14:07:20.416615: step 2526, loss 0.434247, acc 0.90625, prec 0.0503005, recall 0.850337
2017-12-10T14:07:20.604815: step 2527, loss 0.225379, acc 0.953125, prec 0.0502967, recall 0.850337
2017-12-10T14:07:20.799663: step 2528, loss 0.20186, acc 0.9375, prec 0.0503154, recall 0.8504
2017-12-10T14:07:20.988580: step 2529, loss 0.629446, acc 0.84375, prec 0.0503265, recall 0.850463
2017-12-10T14:07:21.173989: step 2530, loss 0.192072, acc 0.890625, prec 0.0503178, recall 0.850463
2017-12-10T14:07:21.362801: step 2531, loss 0.270374, acc 0.9375, prec 0.0503601, recall 0.850589
2017-12-10T14:07:21.551732: step 2532, loss 0.22083, acc 0.90625, prec 0.0503525, recall 0.850589
2017-12-10T14:07:21.739952: step 2533, loss 0.254871, acc 0.9375, prec 0.0503475, recall 0.850589
2017-12-10T14:07:21.928412: step 2534, loss 0.263026, acc 0.90625, prec 0.0503637, recall 0.850652
2017-12-10T14:07:22.118643: step 2535, loss 0.362664, acc 0.890625, prec 0.0503549, recall 0.850652
2017-12-10T14:07:22.308815: step 2536, loss 0.0611709, acc 0.984375, prec 0.0503536, recall 0.850652
2017-12-10T14:07:22.496804: step 2537, loss 0.0303739, acc 1, prec 0.0503536, recall 0.850652
2017-12-10T14:07:22.684924: step 2538, loss 0.283714, acc 0.9375, prec 0.0503486, recall 0.850652
2017-12-10T14:07:22.875077: step 2539, loss 1.11525, acc 0.953125, prec 0.0503921, recall 0.850778
2017-12-10T14:07:23.066712: step 2540, loss 0.0708704, acc 0.96875, prec 0.0503896, recall 0.850778
2017-12-10T14:07:23.257364: step 2541, loss 0.0346181, acc 1, prec 0.0504133, recall 0.85084
2017-12-10T14:07:23.446065: step 2542, loss 0.0783401, acc 0.953125, prec 0.0504095, recall 0.85084
2017-12-10T14:07:23.636132: step 2543, loss 0.231341, acc 0.984375, prec 0.0504319, recall 0.850903
2017-12-10T14:07:23.827296: step 2544, loss 0.0615853, acc 0.96875, prec 0.0504766, recall 0.851028
2017-12-10T14:07:24.023255: step 2545, loss 0.367768, acc 0.9375, prec 0.0505189, recall 0.851153
2017-12-10T14:07:24.208469: step 2546, loss 0.103897, acc 0.953125, prec 0.0505151, recall 0.851153
2017-12-10T14:07:24.399128: step 2547, loss 0.340646, acc 0.953125, prec 0.0505586, recall 0.851278
2017-12-10T14:07:24.591126: step 2548, loss 0.0268014, acc 1, prec 0.0505586, recall 0.851278
2017-12-10T14:07:24.782981: step 2549, loss 0.0590777, acc 0.96875, prec 0.0505561, recall 0.851278
2017-12-10T14:07:24.971376: step 2550, loss 0.450953, acc 0.90625, prec 0.0505485, recall 0.851278
2017-12-10T14:07:25.157186: step 2551, loss 0.146039, acc 0.96875, prec 0.0505696, recall 0.85134
2017-12-10T14:07:25.344752: step 2552, loss 2.27838, acc 0.9375, prec 0.0506367, recall 0.851171
2017-12-10T14:07:25.537579: step 2553, loss 0.119981, acc 0.953125, prec 0.0506565, recall 0.851233
2017-12-10T14:07:25.729942: step 2554, loss 0.328052, acc 0.921875, prec 0.0506502, recall 0.851233
2017-12-10T14:07:25.920806: step 2555, loss 0.171053, acc 0.90625, prec 0.0506663, recall 0.851295
2017-12-10T14:07:26.111587: step 2556, loss 0.213376, acc 0.90625, prec 0.0506587, recall 0.851295
2017-12-10T14:07:26.303900: step 2557, loss 0.228377, acc 0.9375, prec 0.0506537, recall 0.851295
2017-12-10T14:07:26.492374: step 2558, loss 0.263985, acc 0.921875, prec 0.0506474, recall 0.851295
2017-12-10T14:07:26.682454: step 2559, loss 3.11757, acc 0.9375, prec 0.0506908, recall 0.851064
2017-12-10T14:07:26.872747: step 2560, loss 0.245754, acc 0.90625, prec 0.050754, recall 0.85125
2017-12-10T14:07:27.058272: step 2561, loss 0.229273, acc 0.953125, prec 0.0507738, recall 0.851312
2017-12-10T14:07:27.250282: step 2562, loss 0.22813, acc 0.90625, prec 0.0508369, recall 0.851498
2017-12-10T14:07:27.434355: step 2563, loss 0.332524, acc 0.875, prec 0.0508504, recall 0.851559
2017-12-10T14:07:27.622303: step 2564, loss 0.47357, acc 0.875, prec 0.0508403, recall 0.851559
2017-12-10T14:07:27.812333: step 2565, loss 0.545108, acc 0.8125, prec 0.0508958, recall 0.851744
2017-12-10T14:07:28.001107: step 2566, loss 0.366248, acc 0.921875, prec 0.0508895, recall 0.851744
2017-12-10T14:07:28.189844: step 2567, loss 0.468281, acc 0.859375, prec 0.0509017, recall 0.851806
2017-12-10T14:07:28.377910: step 2568, loss 0.365771, acc 0.859375, prec 0.0509609, recall 0.85199
2017-12-10T14:07:28.564472: step 2569, loss 0.789982, acc 0.8125, prec 0.0509458, recall 0.85199
2017-12-10T14:07:28.752273: step 2570, loss 0.609873, acc 0.78125, prec 0.0509281, recall 0.85199
2017-12-10T14:07:28.944580: step 2571, loss 0.673911, acc 0.78125, prec 0.0509104, recall 0.85199
2017-12-10T14:07:29.131053: step 2572, loss 0.916455, acc 0.875, prec 0.0509709, recall 0.852174
2017-12-10T14:07:29.317198: step 2573, loss 0.56291, acc 0.828125, prec 0.051004, recall 0.852296
2017-12-10T14:07:29.501937: step 2574, loss 0.515293, acc 0.875, prec 0.0510409, recall 0.852418
2017-12-10T14:07:29.693025: step 2575, loss 0.35587, acc 0.90625, prec 0.0510333, recall 0.852418
2017-12-10T14:07:29.884589: step 2576, loss 0.332412, acc 0.875, prec 0.0510467, recall 0.852479
2017-12-10T14:07:30.073582: step 2577, loss 0.321919, acc 0.84375, prec 0.051034, recall 0.852479
2017-12-10T14:07:30.264923: step 2578, loss 0.209292, acc 0.921875, prec 0.0510277, recall 0.852479
2017-12-10T14:07:30.454123: step 2579, loss 0.247991, acc 0.890625, prec 0.0510189, recall 0.852479
2017-12-10T14:07:30.642905: step 2580, loss 0.118924, acc 0.953125, prec 0.0510386, recall 0.85254
2017-12-10T14:07:30.831072: step 2581, loss 0.367499, acc 0.890625, prec 0.0510767, recall 0.852662
2017-12-10T14:07:31.022933: step 2582, loss 5.41499, acc 0.90625, prec 0.0510704, recall 0.85231
2017-12-10T14:07:31.214404: step 2583, loss 0.59048, acc 0.90625, prec 0.0510862, recall 0.852371
2017-12-10T14:07:31.405586: step 2584, loss 0.15853, acc 0.953125, prec 0.0510824, recall 0.852371
2017-12-10T14:07:31.591935: step 2585, loss 0.168849, acc 0.9375, prec 0.0510774, recall 0.852371
2017-12-10T14:07:31.780039: step 2586, loss 0.383309, acc 0.890625, prec 0.0511154, recall 0.852493
2017-12-10T14:07:31.968824: step 2587, loss 0.302247, acc 0.875, prec 0.0511053, recall 0.852493
2017-12-10T14:07:32.157044: step 2588, loss 0.373484, acc 0.9375, prec 0.0511237, recall 0.852554
2017-12-10T14:07:32.345874: step 2589, loss 0.413394, acc 0.828125, prec 0.0511099, recall 0.852554
2017-12-10T14:07:32.533556: step 2590, loss 0.136977, acc 0.921875, prec 0.051127, recall 0.852614
2017-12-10T14:07:32.724455: step 2591, loss 0.442237, acc 0.890625, prec 0.0511181, recall 0.852614
2017-12-10T14:07:32.912248: step 2592, loss 0.258454, acc 0.890625, prec 0.0511093, recall 0.852614
2017-12-10T14:07:33.099184: step 2593, loss 0.280983, acc 0.890625, prec 0.0511239, recall 0.852675
2017-12-10T14:07:33.288730: step 2594, loss 0.266724, acc 0.890625, prec 0.0511151, recall 0.852675
2017-12-10T14:07:33.478662: step 2595, loss 0.458765, acc 0.875, prec 0.051105, recall 0.852675
2017-12-10T14:07:33.666099: step 2596, loss 0.298576, acc 0.90625, prec 0.0511208, recall 0.852736
2017-12-10T14:07:33.853348: step 2597, loss 0.143905, acc 0.953125, prec 0.0511872, recall 0.852917
2017-12-10T14:07:34.043294: step 2598, loss 5.89727, acc 0.90625, prec 0.0512043, recall 0.852627
2017-12-10T14:07:34.231789: step 2599, loss 0.266272, acc 0.9375, prec 0.0511992, recall 0.852627
2017-12-10T14:07:34.420737: step 2600, loss 0.347387, acc 0.890625, prec 0.0512138, recall 0.852688
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_1/1512932334/checkpoints/model-2600

2017-12-10T14:07:35.679814: step 2601, loss 0.35615, acc 0.890625, prec 0.0512517, recall 0.852809
2017-12-10T14:07:35.864875: step 2602, loss 0.35624, acc 0.90625, prec 0.0512675, recall 0.852869
2017-12-10T14:07:36.054823: step 2603, loss 0.684003, acc 0.84375, prec 0.0513016, recall 0.852989
2017-12-10T14:07:36.244768: step 2604, loss 0.226443, acc 0.890625, prec 0.0513862, recall 0.85323
2017-12-10T14:07:36.428904: step 2605, loss 0.488717, acc 0.859375, prec 0.0514449, recall 0.85341
2017-12-10T14:07:36.617898: step 2606, loss 0.798634, acc 0.796875, prec 0.0514284, recall 0.85341
2017-12-10T14:07:36.804411: step 2607, loss 0.177986, acc 0.90625, prec 0.0514208, recall 0.85341
2017-12-10T14:07:36.992736: step 2608, loss 0.317493, acc 0.9375, prec 0.0514624, recall 0.853529
2017-12-10T14:07:37.185060: step 2609, loss 0.462917, acc 0.828125, prec 0.0514952, recall 0.853649
2017-12-10T14:07:37.371927: step 2610, loss 0.705927, acc 0.859375, prec 0.0515304, recall 0.853768
2017-12-10T14:07:37.558520: step 2611, loss 2.52174, acc 0.921875, prec 0.0515254, recall 0.85342
2017-12-10T14:07:37.747398: step 2612, loss 1.08636, acc 0.796875, prec 0.0515322, recall 0.85348
2017-12-10T14:07:37.938221: step 2613, loss 0.472793, acc 0.84375, prec 0.0515661, recall 0.853599
2017-12-10T14:07:38.122457: step 2614, loss 0.613513, acc 0.8125, prec 0.0515742, recall 0.853659
2017-12-10T14:07:38.311400: step 2615, loss 0.392678, acc 0.84375, prec 0.0515849, recall 0.853718
2017-12-10T14:07:38.497657: step 2616, loss 0.743008, acc 0.8125, prec 0.0515697, recall 0.853718
2017-12-10T14:07:38.682031: step 2617, loss 1.09329, acc 0.84375, prec 0.0515803, recall 0.853777
2017-12-10T14:07:38.878448: step 2618, loss 0.591347, acc 0.796875, prec 0.0515638, recall 0.853777
2017-12-10T14:07:39.065515: step 2619, loss 0.636038, acc 0.796875, prec 0.0515707, recall 0.853837
2017-12-10T14:07:39.253155: step 2620, loss 0.611568, acc 0.8125, prec 0.0515555, recall 0.853837
2017-12-10T14:07:39.437307: step 2621, loss 0.898099, acc 0.75, prec 0.0515585, recall 0.853896
2017-12-10T14:07:39.627223: step 2622, loss 0.40596, acc 0.890625, prec 0.0515729, recall 0.853955
2017-12-10T14:07:39.811000: step 2623, loss 0.453203, acc 0.84375, prec 0.0515835, recall 0.854015
2017-12-10T14:07:39.994172: step 2624, loss 0.451729, acc 0.84375, prec 0.0515709, recall 0.854015
2017-12-10T14:07:40.183967: step 2625, loss 0.499604, acc 0.828125, prec 0.0515802, recall 0.854074
2017-12-10T14:07:40.369452: step 2626, loss 0.378901, acc 0.859375, prec 0.0515688, recall 0.854074
2017-12-10T14:07:40.558733: step 2627, loss 0.486588, acc 0.859375, prec 0.0515807, recall 0.854133
2017-12-10T14:07:40.745451: step 2628, loss 0.492934, acc 0.859375, prec 0.0515925, recall 0.854192
2017-12-10T14:07:40.933589: step 2629, loss 0.342649, acc 0.890625, prec 0.0516069, recall 0.854251
2017-12-10T14:07:41.121769: step 2630, loss 1.7411, acc 0.921875, prec 0.0516019, recall 0.853905
2017-12-10T14:07:41.312921: step 2631, loss 0.35404, acc 0.875, prec 0.0515918, recall 0.853905
2017-12-10T14:07:41.508483: step 2632, loss 1.10167, acc 0.953125, prec 0.0516112, recall 0.853964
2017-12-10T14:07:41.698901: step 2633, loss 0.436245, acc 0.875, prec 0.0516011, recall 0.853964
2017-12-10T14:07:41.888840: step 2634, loss 0.316148, acc 0.921875, prec 0.0515948, recall 0.853964
2017-12-10T14:07:42.077104: step 2635, loss 0.287911, acc 0.921875, prec 0.0516348, recall 0.854082
2017-12-10T14:07:42.266143: step 2636, loss 0.359998, acc 0.859375, prec 0.0516235, recall 0.854082
2017-12-10T14:07:42.453570: step 2637, loss 0.304021, acc 0.875, prec 0.0516134, recall 0.854082
2017-12-10T14:07:42.641250: step 2638, loss 0.220146, acc 0.953125, prec 0.0516328, recall 0.854141
2017-12-10T14:07:42.832465: step 2639, loss 0.426447, acc 0.921875, prec 0.0516959, recall 0.854318
2017-12-10T14:07:43.021379: step 2640, loss 0.203106, acc 0.921875, prec 0.0516896, recall 0.854318
2017-12-10T14:07:43.207763: step 2641, loss 0.0639348, acc 0.984375, prec 0.0516884, recall 0.854318
2017-12-10T14:07:43.397704: step 2642, loss 0.536228, acc 0.90625, prec 0.0517039, recall 0.854377
2017-12-10T14:07:43.586246: step 2643, loss 0.219267, acc 0.921875, prec 0.0517208, recall 0.854436
2017-12-10T14:07:43.779330: step 2644, loss 6.39667, acc 0.9375, prec 0.0517401, recall 0.85415
2017-12-10T14:07:43.974761: step 2645, loss 0.193444, acc 0.96875, prec 0.0517607, recall 0.854209
2017-12-10T14:07:44.165146: step 2646, loss 0.407808, acc 0.875, prec 0.0517738, recall 0.854267
2017-12-10T14:07:44.350244: step 2647, loss 0.369142, acc 0.921875, prec 0.0517675, recall 0.854267
2017-12-10T14:07:44.535731: step 2648, loss 0.462229, acc 0.859375, prec 0.0517792, recall 0.854326
2017-12-10T14:07:44.724339: step 2649, loss 0.51949, acc 0.8125, prec 0.0517641, recall 0.854326
2017-12-10T14:07:44.910584: step 2650, loss 0.222764, acc 0.875, prec 0.0517771, recall 0.854385
2017-12-10T14:07:45.095552: step 2651, loss 0.667918, acc 0.8125, prec 0.0517851, recall 0.854443
2017-12-10T14:07:45.281252: step 2652, loss 0.466887, acc 0.890625, prec 0.0517762, recall 0.854443
2017-12-10T14:07:45.468068: step 2653, loss 0.584175, acc 0.84375, prec 0.0518098, recall 0.85456
2017-12-10T14:07:45.654028: step 2654, loss 0.613071, acc 0.8125, prec 0.0517947, recall 0.85456
2017-12-10T14:07:45.839528: step 2655, loss 0.483759, acc 0.84375, prec 0.0517821, recall 0.85456
2017-12-10T14:07:46.028557: step 2656, loss 0.595433, acc 0.84375, prec 0.0517695, recall 0.85456
2017-12-10T14:07:46.218052: step 2657, loss 0.238413, acc 0.90625, prec 0.051808, recall 0.854677
2017-12-10T14:07:46.403470: step 2658, loss 0.238834, acc 0.90625, prec 0.0518236, recall 0.854735
2017-12-10T14:07:46.587011: step 2659, loss 0.384462, acc 0.859375, prec 0.0518122, recall 0.854735
2017-12-10T14:07:46.773425: step 2660, loss 0.263017, acc 0.875, prec 0.0518482, recall 0.854852
2017-12-10T14:07:46.958610: step 2661, loss 0.241188, acc 0.90625, prec 0.0518637, recall 0.85491
2017-12-10T14:07:47.149973: step 2662, loss 0.317358, acc 0.9375, prec 0.0518817, recall 0.854968
2017-12-10T14:07:47.337360: step 2663, loss 0.369745, acc 0.90625, prec 0.0518742, recall 0.854968
2017-12-10T14:07:47.528497: step 2664, loss 0.141977, acc 0.9375, prec 0.0518922, recall 0.855026
2017-12-10T14:07:47.716823: step 2665, loss 0.226987, acc 0.90625, prec 0.0519077, recall 0.855084
2017-12-10T14:07:47.904713: step 2666, loss 0.317845, acc 0.875, prec 0.0518976, recall 0.855084
2017-12-10T14:07:48.091320: step 2667, loss 0.793089, acc 0.921875, prec 0.0519143, recall 0.855142
2017-12-10T14:07:48.280693: step 2668, loss 0.277808, acc 0.875, prec 0.0519272, recall 0.8552
2017-12-10T14:07:48.464857: step 2669, loss 0.107903, acc 0.96875, prec 0.0519247, recall 0.8552
2017-12-10T14:07:48.654833: step 2670, loss 1.88837, acc 0.953125, prec 0.0519913, recall 0.855032
2017-12-10T14:07:48.844194: step 2671, loss 0.641857, acc 0.921875, prec 0.052077, recall 0.855263
2017-12-10T14:07:49.031476: step 2672, loss 0.306448, acc 0.890625, prec 0.0520912, recall 0.855321
2017-12-10T14:07:49.223476: step 2673, loss 0.317692, acc 0.890625, prec 0.0520823, recall 0.855321
2017-12-10T14:07:49.415191: step 2674, loss 0.211999, acc 0.90625, prec 0.0520747, recall 0.855321
2017-12-10T14:07:49.604450: step 2675, loss 0.141658, acc 0.9375, prec 0.0520927, recall 0.855379
2017-12-10T14:07:49.790047: step 2676, loss 0.146138, acc 0.953125, prec 0.0521119, recall 0.855436
2017-12-10T14:07:49.977303: step 2677, loss 0.120988, acc 0.9375, prec 0.0521068, recall 0.855436
2017-12-10T14:07:50.167052: step 2678, loss 0.382614, acc 0.9375, prec 0.0521248, recall 0.855494
2017-12-10T14:07:50.354557: step 2679, loss 0.138622, acc 0.921875, prec 0.0521185, recall 0.855494
2017-12-10T14:07:50.542337: step 2680, loss 0.487997, acc 0.9375, prec 0.0521594, recall 0.855609
2017-12-10T14:07:50.732069: step 2681, loss 0.253819, acc 0.921875, prec 0.052176, recall 0.855666
2017-12-10T14:07:50.922289: step 2682, loss 0.0487663, acc 0.984375, prec 0.0522207, recall 0.855781
2017-12-10T14:07:51.114785: step 2683, loss 0.628466, acc 0.875, prec 0.0522106, recall 0.855781
2017-12-10T14:07:51.299651: step 2684, loss 0.331958, acc 0.890625, prec 0.0522017, recall 0.855781
2017-12-10T14:07:51.486736: step 2685, loss 0.582965, acc 0.96875, prec 0.0522451, recall 0.855895
2017-12-10T14:07:51.678376: step 2686, loss 0.273657, acc 0.921875, prec 0.0522847, recall 0.85601
2017-12-10T14:07:51.864124: step 2687, loss 5.02499, acc 0.953125, prec 0.0522822, recall 0.85567
2017-12-10T14:07:52.056497: step 2688, loss 0.304823, acc 0.890625, prec 0.0523192, recall 0.855784
2017-12-10T14:07:52.245766: step 2689, loss 0.473876, acc 0.90625, prec 0.0523346, recall 0.855842
2017-12-10T14:07:52.434450: step 2690, loss 0.339051, acc 0.9375, prec 0.0523525, recall 0.855899
2017-12-10T14:07:52.623033: step 2691, loss 0.403778, acc 0.890625, prec 0.0523665, recall 0.855956
2017-12-10T14:07:52.816317: step 2692, loss 0.537353, acc 0.890625, prec 0.0523806, recall 0.856013
2017-12-10T14:07:53.004094: step 2693, loss 0.211173, acc 0.953125, prec 0.0523997, recall 0.85607
2017-12-10T14:07:53.193035: step 2694, loss 0.370437, acc 0.875, prec 0.0524125, recall 0.856126
2017-12-10T14:07:53.385628: step 2695, loss 0.189472, acc 0.890625, prec 0.0524036, recall 0.856126
2017-12-10T14:07:53.574091: step 2696, loss 0.352573, acc 0.890625, prec 0.0524177, recall 0.856183
2017-12-10T14:07:53.759501: step 2697, loss 0.751393, acc 0.84375, prec 0.052405, recall 0.856183
2017-12-10T14:07:53.946393: step 2698, loss 0.236709, acc 0.921875, prec 0.0523987, recall 0.856183
2017-12-10T14:07:54.137332: step 2699, loss 0.293667, acc 0.890625, prec 0.0523898, recall 0.856183
2017-12-10T14:07:54.326693: step 2700, loss 0.412143, acc 0.828125, prec 0.0524217, recall 0.856297
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_1/1512932334/checkpoints/model-2700

2017-12-10T14:07:55.529085: step 2701, loss 0.14664, acc 0.921875, prec 0.0524841, recall 0.856467
2017-12-10T14:07:55.714248: step 2702, loss 0.345958, acc 0.859375, prec 0.0524955, recall 0.856523
2017-12-10T14:07:55.901457: step 2703, loss 0.145109, acc 0.984375, prec 0.0525171, recall 0.85658
2017-12-10T14:07:56.093060: step 2704, loss 0.59238, acc 0.953125, prec 0.0525362, recall 0.856636
2017-12-10T14:07:56.279297: step 2705, loss 0.461273, acc 0.859375, prec 0.0525706, recall 0.856749
2017-12-10T14:07:56.466659: step 2706, loss 0.242198, acc 0.953125, prec 0.0525896, recall 0.856806
2017-12-10T14:07:56.654097: step 2707, loss 0.151463, acc 0.953125, prec 0.0526087, recall 0.856862
2017-12-10T14:07:56.843268: step 2708, loss 0.272135, acc 0.890625, prec 0.0525998, recall 0.856862
2017-12-10T14:07:57.033036: step 2709, loss 0.120459, acc 0.96875, prec 0.0526201, recall 0.856918
2017-12-10T14:07:57.225471: step 2710, loss 0.454951, acc 0.921875, prec 0.0526595, recall 0.857031
2017-12-10T14:07:57.411256: step 2711, loss 0.0815751, acc 0.96875, prec 0.0526798, recall 0.857087
2017-12-10T14:07:57.600952: step 2712, loss 0.324845, acc 0.953125, prec 0.0526989, recall 0.857143
2017-12-10T14:07:57.792868: step 2713, loss 0.195879, acc 0.921875, prec 0.0526925, recall 0.857143
2017-12-10T14:07:57.977916: step 2714, loss 0.0716258, acc 0.984375, prec 0.0527141, recall 0.857199
2017-12-10T14:07:58.166177: step 2715, loss 0.0619024, acc 0.96875, prec 0.0527116, recall 0.857199
2017-12-10T14:07:58.354520: step 2716, loss 0.117345, acc 1, prec 0.0527344, recall 0.857255
2017-12-10T14:07:58.541940: step 2717, loss 0.508021, acc 0.953125, prec 0.0527992, recall 0.857423
2017-12-10T14:07:58.729328: step 2718, loss 0.0731141, acc 0.984375, prec 0.0528207, recall 0.857478
2017-12-10T14:07:58.922840: step 2719, loss 2.96461, acc 0.953125, prec 0.0528423, recall 0.856863
2017-12-10T14:07:59.119145: step 2720, loss 0.206826, acc 0.953125, prec 0.0528613, recall 0.856919
2017-12-10T14:07:59.309566: step 2721, loss 0.235098, acc 0.953125, prec 0.052926, recall 0.857087
2017-12-10T14:07:59.496170: step 2722, loss 0.129718, acc 0.9375, prec 0.0529209, recall 0.857087
2017-12-10T14:07:59.687365: step 2723, loss 0.233789, acc 0.9375, prec 0.0529615, recall 0.857199
2017-12-10T14:07:59.873476: step 2724, loss 0.290809, acc 0.921875, prec 0.0529551, recall 0.857199
2017-12-10T14:08:00.060680: step 2725, loss 0.358945, acc 0.9375, prec 0.05295, recall 0.857199
2017-12-10T14:08:00.247450: step 2726, loss 0.237924, acc 0.921875, prec 0.0529436, recall 0.857199
2017-12-10T14:08:00.432296: step 2727, loss 0.599139, acc 0.828125, prec 0.0529296, recall 0.857199
2017-12-10T14:08:00.618968: step 2728, loss 0.597075, acc 0.796875, prec 0.0529358, recall 0.857254
2017-12-10T14:08:00.809193: step 2729, loss 0.196086, acc 0.890625, prec 0.0529953, recall 0.857421
2017-12-10T14:08:01.003234: step 2730, loss 0.387032, acc 0.859375, prec 0.0529838, recall 0.857421
2017-12-10T14:08:01.189426: step 2731, loss 0.2418, acc 0.90625, prec 0.0529989, recall 0.857477
2017-12-10T14:08:01.379226: step 2732, loss 0.308248, acc 0.875, prec 0.0530115, recall 0.857532
2017-12-10T14:08:01.569413: step 2733, loss 0.596728, acc 0.84375, prec 0.0530216, recall 0.857588
2017-12-10T14:08:01.756930: step 2734, loss 0.486585, acc 0.890625, prec 0.0530126, recall 0.857588
2017-12-10T14:08:01.945265: step 2735, loss 0.347279, acc 0.828125, prec 0.0529986, recall 0.857588
2017-12-10T14:08:02.131663: step 2736, loss 0.155323, acc 0.921875, prec 0.0529922, recall 0.857588
2017-12-10T14:08:02.321535: step 2737, loss 0.283843, acc 0.953125, prec 0.0530339, recall 0.857698
2017-12-10T14:08:02.508837: step 2738, loss 0.318351, acc 0.875, prec 0.0530237, recall 0.857698
2017-12-10T14:08:02.696761: step 2739, loss 0.215327, acc 0.953125, prec 0.0530427, recall 0.857754
2017-12-10T14:08:02.888800: step 2740, loss 0.274434, acc 0.90625, prec 0.0530578, recall 0.857809
2017-12-10T14:08:03.079262: step 2741, loss 0.140263, acc 0.953125, prec 0.0530767, recall 0.857864
2017-12-10T14:08:03.266976: step 2742, loss 0.175381, acc 0.953125, prec 0.0530956, recall 0.857919
2017-12-10T14:08:03.461091: step 2743, loss 1.19127, acc 0.921875, prec 0.053112, recall 0.857974
2017-12-10T14:08:03.649688: step 2744, loss 0.360489, acc 0.953125, prec 0.0531309, recall 0.858029
2017-12-10T14:08:03.840932: step 2745, loss 0.368833, acc 0.953125, prec 0.0531498, recall 0.858084
2017-12-10T14:08:04.028479: step 2746, loss 0.602928, acc 0.90625, prec 0.0531649, recall 0.85814
2017-12-10T14:08:04.220980: step 2747, loss 0.27358, acc 0.875, prec 0.0531774, recall 0.858194
2017-12-10T14:08:04.409242: step 2748, loss 0.103409, acc 0.96875, prec 0.0531976, recall 0.858249
2017-12-10T14:08:04.594414: step 2749, loss 0.386679, acc 0.90625, prec 0.0532354, recall 0.858359
2017-12-10T14:08:04.780673: step 2750, loss 0.316676, acc 0.90625, prec 0.0532277, recall 0.858359
2017-12-10T14:08:04.969744: step 2751, loss 0.797271, acc 0.9375, prec 0.0533135, recall 0.858578
2017-12-10T14:08:05.162954: step 2752, loss 0.13169, acc 0.953125, prec 0.0533097, recall 0.858578
2017-12-10T14:08:05.351613: step 2753, loss 0.895929, acc 0.890625, prec 0.0533461, recall 0.858687
2017-12-10T14:08:05.539261: step 2754, loss 0.471576, acc 0.859375, prec 0.0533573, recall 0.858742
2017-12-10T14:08:05.729489: step 2755, loss 0.219304, acc 0.9375, prec 0.0533522, recall 0.858742
2017-12-10T14:08:05.921783: step 2756, loss 0.26427, acc 0.890625, prec 0.0533659, recall 0.858796
2017-12-10T14:08:06.109846: step 2757, loss 0.378415, acc 0.921875, prec 0.0533595, recall 0.858796
2017-12-10T14:08:06.296485: step 2758, loss 0.248937, acc 0.9375, prec 0.0533998, recall 0.858905
2017-12-10T14:08:06.485639: step 2759, loss 0.382454, acc 0.921875, prec 0.0534161, recall 0.85896
2017-12-10T14:08:06.673841: step 2760, loss 1.11406, acc 0.921875, prec 0.0534551, recall 0.859068
2017-12-10T14:08:06.870271: step 2761, loss 0.433811, acc 0.875, prec 0.0534675, recall 0.859122
2017-12-10T14:08:07.061437: step 2762, loss 0.466984, acc 0.90625, prec 0.0534598, recall 0.859122
2017-12-10T14:08:07.252061: step 2763, loss 0.56683, acc 0.859375, prec 0.0534936, recall 0.859231
2017-12-10T14:08:07.439859: step 2764, loss 0.23295, acc 0.9375, prec 0.0534885, recall 0.859231
2017-12-10T14:08:07.627229: step 2765, loss 0.345503, acc 0.921875, prec 0.0535047, recall 0.859285
2017-12-10T14:08:07.819601: step 2766, loss 0.268649, acc 0.921875, prec 0.0534983, recall 0.859285
2017-12-10T14:08:08.011632: step 2767, loss 0.242077, acc 0.890625, prec 0.0534894, recall 0.859285
2017-12-10T14:08:08.197916: step 2768, loss 0.638692, acc 0.9375, prec 0.0535296, recall 0.859393
2017-12-10T14:08:08.389980: step 2769, loss 0.199026, acc 0.921875, prec 0.0535231, recall 0.859393
2017-12-10T14:08:08.575741: step 2770, loss 0.145589, acc 0.9375, prec 0.0535407, recall 0.859447
2017-12-10T14:08:08.761278: step 2771, loss 0.208373, acc 0.890625, prec 0.0535543, recall 0.859501
2017-12-10T14:08:08.954342: step 2772, loss 0.350538, acc 0.890625, prec 0.0535906, recall 0.859609
2017-12-10T14:08:09.142516: step 2773, loss 4.93327, acc 0.90625, prec 0.0536295, recall 0.859387
2017-12-10T14:08:09.332475: step 2774, loss 0.213752, acc 0.9375, prec 0.0536244, recall 0.859387
2017-12-10T14:08:09.517144: step 2775, loss 0.317072, acc 0.890625, prec 0.0536154, recall 0.859387
2017-12-10T14:08:09.703873: step 2776, loss 0.645157, acc 0.8125, prec 0.0536, recall 0.859387
2017-12-10T14:08:09.892443: step 2777, loss 0.626099, acc 0.828125, prec 0.0536312, recall 0.859495
2017-12-10T14:08:10.082112: step 2778, loss 0.395345, acc 0.859375, prec 0.0536422, recall 0.859548
2017-12-10T14:08:10.272567: step 2779, loss 0.408672, acc 0.890625, prec 0.0536333, recall 0.859548
2017-12-10T14:08:10.465596: step 2780, loss 0.398359, acc 0.90625, prec 0.0536256, recall 0.859548
2017-12-10T14:08:10.653480: step 2781, loss 0.815034, acc 0.78125, prec 0.0536302, recall 0.859602
2017-12-10T14:08:10.842458: step 2782, loss 0.375333, acc 0.90625, prec 0.0536677, recall 0.85971
2017-12-10T14:08:11.029339: step 2783, loss 0.931558, acc 0.796875, prec 0.0536737, recall 0.859763
2017-12-10T14:08:11.218548: step 2784, loss 0.604582, acc 0.828125, prec 0.0536822, recall 0.859817
2017-12-10T14:08:11.405604: step 2785, loss 0.408686, acc 0.8125, prec 0.0536894, recall 0.85987
2017-12-10T14:08:11.591605: step 2786, loss 0.434548, acc 0.8125, prec 0.0536966, recall 0.859924
2017-12-10T14:08:11.781857: step 2787, loss 0.384613, acc 0.890625, prec 0.0537101, recall 0.859977
2017-12-10T14:08:11.968628: step 2788, loss 1.3758, acc 0.9375, prec 0.0537501, recall 0.860084
2017-12-10T14:08:12.162903: step 2789, loss 0.43285, acc 0.875, prec 0.0537399, recall 0.860084
2017-12-10T14:08:12.347342: step 2790, loss 0.404016, acc 0.890625, prec 0.0537535, recall 0.860137
2017-12-10T14:08:12.539997: step 2791, loss 0.694051, acc 0.875, prec 0.0537883, recall 0.860244
2017-12-10T14:08:12.729051: step 2792, loss 0.323212, acc 0.859375, prec 0.0537993, recall 0.860297
2017-12-10T14:08:12.916919: step 2793, loss 0.329415, acc 0.875, prec 0.0538116, recall 0.86035
2017-12-10T14:08:13.108022: step 2794, loss 0.417599, acc 0.90625, prec 0.0538264, recall 0.860403
2017-12-10T14:08:13.294508: step 2795, loss 0.23026, acc 0.890625, prec 0.0538174, recall 0.860403
2017-12-10T14:08:13.480853: step 2796, loss 0.56062, acc 0.90625, prec 0.0538322, recall 0.860456
2017-12-10T14:08:13.669733: step 2797, loss 0.503444, acc 0.890625, prec 0.0538233, recall 0.860456
2017-12-10T14:08:13.862416: step 2798, loss 0.129042, acc 0.953125, prec 0.0538194, recall 0.860456
2017-12-10T14:08:14.061692: step 2799, loss 0.472552, acc 0.921875, prec 0.053858, recall 0.860562
2017-12-10T14:08:14.255484: step 2800, loss 0.250461, acc 0.9375, prec 0.0538754, recall 0.860615
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_1/1512932334/checkpoints/model-2800

2017-12-10T14:08:15.501098: step 2801, loss 0.151536, acc 0.921875, prec 0.053869, recall 0.860615
2017-12-10T14:08:15.685003: step 2802, loss 0.302589, acc 0.890625, prec 0.0538825, recall 0.860668
2017-12-10T14:08:15.877530: step 2803, loss 0.401463, acc 0.828125, prec 0.0538685, recall 0.860668
2017-12-10T14:08:16.067485: step 2804, loss 0.0643934, acc 0.96875, prec 0.0538659, recall 0.860668
2017-12-10T14:08:16.253843: step 2805, loss 0.607233, acc 0.90625, prec 0.0539032, recall 0.860774
2017-12-10T14:08:16.441427: step 2806, loss 0.0909597, acc 0.96875, prec 0.0539006, recall 0.860774
2017-12-10T14:08:16.632753: step 2807, loss 0.210509, acc 0.96875, prec 0.053898, recall 0.860774
2017-12-10T14:08:16.821103: step 2808, loss 0.253951, acc 0.9375, prec 0.0539154, recall 0.860827
2017-12-10T14:08:17.008935: step 2809, loss 0.366138, acc 0.9375, prec 0.0539327, recall 0.860879
2017-12-10T14:08:17.200547: step 2810, loss 0.242026, acc 0.9375, prec 0.0539276, recall 0.860879
2017-12-10T14:08:17.388032: step 2811, loss 0.291629, acc 0.953125, prec 0.0539238, recall 0.860879
2017-12-10T14:08:17.578220: step 2812, loss 0.079616, acc 0.96875, prec 0.0539212, recall 0.860879
2017-12-10T14:08:17.766678: step 2813, loss 0.185191, acc 0.9375, prec 0.0539386, recall 0.860932
2017-12-10T14:08:17.953910: step 2814, loss 0.158425, acc 0.953125, prec 0.0539572, recall 0.860985
2017-12-10T14:08:18.143147: step 2815, loss 0.126416, acc 0.96875, prec 0.0539546, recall 0.860985
2017-12-10T14:08:18.332053: step 2816, loss 0.199493, acc 0.90625, prec 0.0539469, recall 0.860985
2017-12-10T14:08:18.518208: step 2817, loss 0.0650988, acc 0.984375, prec 0.0539681, recall 0.861037
2017-12-10T14:08:18.710494: step 2818, loss 0.609843, acc 0.953125, prec 0.0539867, recall 0.86109
2017-12-10T14:08:18.899227: step 2819, loss 0.184081, acc 0.96875, prec 0.0540066, recall 0.861143
2017-12-10T14:08:19.091241: step 2820, loss 0.233513, acc 0.9375, prec 0.0540239, recall 0.861195
2017-12-10T14:08:19.279900: step 2821, loss 0.258039, acc 0.9375, prec 0.0540188, recall 0.861195
2017-12-10T14:08:19.469660: step 2822, loss 0.113997, acc 0.953125, prec 0.0540374, recall 0.861248
2017-12-10T14:08:19.658695: step 2823, loss 4.04117, acc 0.90625, prec 0.054031, recall 0.860922
2017-12-10T14:08:19.848504: step 2824, loss 0.0583397, acc 0.96875, prec 0.0540284, recall 0.860922
2017-12-10T14:08:20.034188: step 2825, loss 0.379835, acc 0.9375, prec 0.0540233, recall 0.860922
2017-12-10T14:08:20.224382: step 2826, loss 0.760478, acc 0.96875, prec 0.0540432, recall 0.860975
2017-12-10T14:08:20.412921: step 2827, loss 0.237749, acc 0.921875, prec 0.0540592, recall 0.861027
2017-12-10T14:08:20.603628: step 2828, loss 0.628289, acc 0.828125, prec 0.0540675, recall 0.86108
2017-12-10T14:08:20.792460: step 2829, loss 0.169568, acc 0.921875, prec 0.0540611, recall 0.86108
2017-12-10T14:08:20.982300: step 2830, loss 0.200033, acc 0.9375, prec 0.054056, recall 0.86108
2017-12-10T14:08:21.168569: step 2831, loss 0.321021, acc 0.9375, prec 0.0540957, recall 0.861184
2017-12-10T14:08:21.357303: step 2832, loss 0.129302, acc 0.921875, prec 0.0541341, recall 0.861289
2017-12-10T14:08:21.551845: step 2833, loss 0.60181, acc 0.984375, prec 0.0541552, recall 0.861341
2017-12-10T14:08:21.740571: step 2834, loss 3.43166, acc 0.859375, prec 0.0541898, recall 0.861122
2017-12-10T14:08:21.930281: step 2835, loss 0.426494, acc 0.84375, prec 0.0541769, recall 0.861122
2017-12-10T14:08:22.117909: step 2836, loss 0.211664, acc 0.921875, prec 0.0542153, recall 0.861226
2017-12-10T14:08:22.307037: step 2837, loss 0.123463, acc 0.9375, prec 0.0542549, recall 0.86133
2017-12-10T14:08:22.494415: step 2838, loss 0.350867, acc 0.875, prec 0.0542894, recall 0.861434
2017-12-10T14:08:22.685378: step 2839, loss 1.18787, acc 0.734375, prec 0.0542676, recall 0.861434
2017-12-10T14:08:22.871077: step 2840, loss 0.952762, acc 0.796875, prec 0.0542733, recall 0.861486
2017-12-10T14:08:23.061386: step 2841, loss 0.559349, acc 0.828125, prec 0.0542815, recall 0.861538
2017-12-10T14:08:23.249526: step 2842, loss 0.740678, acc 0.8125, prec 0.0543108, recall 0.861642
2017-12-10T14:08:23.434266: step 2843, loss 0.598999, acc 0.84375, prec 0.0543203, recall 0.861694
2017-12-10T14:08:23.619862: step 2844, loss 0.41435, acc 0.84375, prec 0.0543075, recall 0.861694
2017-12-10T14:08:23.806537: step 2845, loss 1.12902, acc 0.75, prec 0.0543093, recall 0.861746
2017-12-10T14:08:23.997422: step 2846, loss 0.575402, acc 0.828125, prec 0.0542952, recall 0.861746
2017-12-10T14:08:24.180404: step 2847, loss 0.813682, acc 0.875, prec 0.0543073, recall 0.861798
2017-12-10T14:08:24.365988: step 2848, loss 0.689226, acc 0.796875, prec 0.0542906, recall 0.861798
2017-12-10T14:08:24.552300: step 2849, loss 0.592264, acc 0.890625, prec 0.0543486, recall 0.861953
2017-12-10T14:08:24.737534: step 2850, loss 0.590291, acc 0.84375, prec 0.0543358, recall 0.861953
2017-12-10T14:08:24.920357: step 2851, loss 0.538006, acc 0.828125, prec 0.0543217, recall 0.861953
2017-12-10T14:08:25.110121: step 2852, loss 0.364649, acc 0.859375, prec 0.0543547, recall 0.862056
2017-12-10T14:08:25.296643: step 2853, loss 0.401661, acc 0.90625, prec 0.0543693, recall 0.862108
2017-12-10T14:08:25.484562: step 2854, loss 0.167051, acc 0.9375, prec 0.0544088, recall 0.862211
2017-12-10T14:08:25.671842: step 2855, loss 0.267127, acc 0.90625, prec 0.0544457, recall 0.862313
2017-12-10T14:08:25.858267: step 2856, loss 0.374532, acc 0.84375, prec 0.0544328, recall 0.862313
2017-12-10T14:08:26.043609: step 2857, loss 0.493021, acc 0.875, prec 0.0544448, recall 0.862365
2017-12-10T14:08:26.229816: step 2858, loss 0.251664, acc 0.890625, prec 0.0544804, recall 0.862467
2017-12-10T14:08:26.421896: step 2859, loss 1.56272, acc 0.953125, prec 0.0545223, recall 0.862249
2017-12-10T14:08:26.610636: step 2860, loss 0.49102, acc 0.859375, prec 0.0545775, recall 0.862402
2017-12-10T14:08:26.800119: step 2861, loss 0.0798145, acc 0.953125, prec 0.0545959, recall 0.862454
2017-12-10T14:08:26.991967: step 2862, loss 1.81449, acc 0.96875, prec 0.0545947, recall 0.862133
2017-12-10T14:08:27.186817: step 2863, loss 0.342587, acc 0.890625, prec 0.0545857, recall 0.862133
2017-12-10T14:08:27.376878: step 2864, loss 0.175644, acc 0.9375, prec 0.0546473, recall 0.862287
2017-12-10T14:08:27.568323: step 2865, loss 0.151263, acc 0.953125, prec 0.0546879, recall 0.862389
2017-12-10T14:08:27.757275: step 2866, loss 0.229328, acc 0.90625, prec 0.0547246, recall 0.862491
2017-12-10T14:08:27.944344: step 2867, loss 0.482313, acc 0.890625, prec 0.0547601, recall 0.862593
2017-12-10T14:08:28.134898: step 2868, loss 0.339711, acc 0.875, prec 0.0547942, recall 0.862694
2017-12-10T14:08:28.321332: step 2869, loss 0.365714, acc 0.9375, prec 0.0548335, recall 0.862796
2017-12-10T14:08:28.513393: step 2870, loss 0.184052, acc 0.890625, prec 0.0548245, recall 0.862796
2017-12-10T14:08:28.700073: step 2871, loss 0.255661, acc 0.921875, prec 0.054818, recall 0.862796
2017-12-10T14:08:28.893113: step 2872, loss 0.173519, acc 0.921875, prec 0.0548338, recall 0.862847
2017-12-10T14:08:29.090788: step 2873, loss 0.246922, acc 0.90625, prec 0.0548483, recall 0.862897
2017-12-10T14:08:29.280805: step 2874, loss 0.465764, acc 0.875, prec 0.054838, recall 0.862897
2017-12-10T14:08:29.466598: step 2875, loss 0.295996, acc 0.90625, prec 0.0548302, recall 0.862897
2017-12-10T14:08:29.655612: step 2876, loss 0.283728, acc 0.890625, prec 0.0548434, recall 0.862948
2017-12-10T14:08:29.846478: step 2877, loss 0.165975, acc 0.921875, prec 0.054837, recall 0.862948
2017-12-10T14:08:30.035537: step 2878, loss 0.543552, acc 0.859375, prec 0.0548254, recall 0.862948
2017-12-10T14:08:30.223506: step 2879, loss 0.0933206, acc 0.96875, prec 0.054845, recall 0.862999
2017-12-10T14:08:30.414692: step 2880, loss 0.695675, acc 0.921875, prec 0.0548607, recall 0.863049
2017-12-10T14:08:30.605431: step 2881, loss 0.117295, acc 0.953125, prec 0.0548569, recall 0.863049
2017-12-10T14:08:30.791926: step 2882, loss 0.108385, acc 0.953125, prec 0.0548752, recall 0.8631
2017-12-10T14:08:30.982707: step 2883, loss 0.207312, acc 0.9375, prec 0.0548922, recall 0.86315
2017-12-10T14:08:31.177593: step 2884, loss 0.343056, acc 0.9375, prec 0.0549536, recall 0.863301
2017-12-10T14:08:31.376545: step 2885, loss 0.152436, acc 0.921875, prec 0.0549693, recall 0.863352
2017-12-10T14:08:31.562245: step 2886, loss 0.158984, acc 0.9375, prec 0.0549863, recall 0.863402
2017-12-10T14:08:31.755548: step 2887, loss 0.176811, acc 0.96875, prec 0.0550059, recall 0.863452
2017-12-10T14:08:31.948444: step 2888, loss 0.166343, acc 0.9375, prec 0.0550007, recall 0.863452
2017-12-10T14:08:32.140345: step 2889, loss 0.228049, acc 0.9375, prec 0.0550177, recall 0.863503
2017-12-10T14:08:32.331590: step 2890, loss 0.392684, acc 0.96875, prec 0.0550594, recall 0.863603
2017-12-10T14:08:32.522436: step 2891, loss 3.72287, acc 0.921875, prec 0.0550764, recall 0.863336
2017-12-10T14:08:32.717413: step 2892, loss 0.148191, acc 0.96875, prec 0.055096, recall 0.863386
2017-12-10T14:08:32.905276: step 2893, loss 0.0848707, acc 0.96875, prec 0.0551377, recall 0.863486
2017-12-10T14:08:33.092179: step 2894, loss 0.159158, acc 0.9375, prec 0.0551546, recall 0.863536
2017-12-10T14:08:33.277957: step 2895, loss 0.455833, acc 0.90625, prec 0.0551912, recall 0.863636
2017-12-10T14:08:33.473259: step 2896, loss 0.401749, acc 0.875, prec 0.0551808, recall 0.863636
2017-12-10T14:08:33.663560: step 2897, loss 0.15729, acc 0.953125, prec 0.0552212, recall 0.863736
2017-12-10T14:08:33.851066: step 2898, loss 0.191534, acc 0.953125, prec 0.0552173, recall 0.863736
2017-12-10T14:08:34.039093: step 2899, loss 0.416656, acc 0.875, prec 0.0552291, recall 0.863786
2017-12-10T14:08:34.224236: step 2900, loss 0.0837265, acc 0.953125, prec 0.0552694, recall 0.863886
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_1/1512932334/checkpoints/model-2900

2017-12-10T14:08:35.411429: step 2901, loss 0.404435, acc 0.90625, prec 0.0552838, recall 0.863936
2017-12-10T14:08:35.599584: step 2902, loss 0.332826, acc 0.90625, prec 0.055276, recall 0.863936
2017-12-10T14:08:35.786741: step 2903, loss 0.105624, acc 0.953125, prec 0.0552721, recall 0.863936
2017-12-10T14:08:35.974253: step 2904, loss 0.109256, acc 0.96875, prec 0.0552696, recall 0.863936
2017-12-10T14:08:36.163328: step 2905, loss 0.496797, acc 0.875, prec 0.0552592, recall 0.863936
2017-12-10T14:08:36.350872: step 2906, loss 0.268481, acc 0.890625, prec 0.0552502, recall 0.863936
2017-12-10T14:08:36.536523: step 2907, loss 0.334618, acc 0.90625, prec 0.0552424, recall 0.863936
2017-12-10T14:08:36.724659: step 2908, loss 0.421821, acc 0.859375, prec 0.055275, recall 0.864035
2017-12-10T14:08:36.913207: step 2909, loss 0.458599, acc 0.953125, prec 0.0553153, recall 0.864134
2017-12-10T14:08:37.102233: step 2910, loss 0.229551, acc 0.90625, prec 0.0553075, recall 0.864134
2017-12-10T14:08:37.291412: step 2911, loss 0.201082, acc 0.953125, prec 0.0553257, recall 0.864184
2017-12-10T14:08:37.479462: step 2912, loss 0.256234, acc 0.921875, prec 0.0553192, recall 0.864184
2017-12-10T14:08:37.667460: step 2913, loss 1.9952, acc 0.890625, prec 0.0553336, recall 0.863918
2017-12-10T14:08:37.855903: step 2914, loss 0.217601, acc 0.921875, prec 0.0553492, recall 0.863968
2017-12-10T14:08:38.050047: step 2915, loss 0.389445, acc 0.921875, prec 0.0553648, recall 0.864017
2017-12-10T14:08:38.238918: step 2916, loss 0.165746, acc 0.921875, prec 0.0553583, recall 0.864017
2017-12-10T14:08:38.428712: step 2917, loss 0.232271, acc 0.96875, prec 0.0553557, recall 0.864017
2017-12-10T14:08:38.617859: step 2918, loss 1.42062, acc 0.9375, prec 0.0553726, recall 0.864067
2017-12-10T14:08:38.809857: step 2919, loss 0.211157, acc 0.875, prec 0.0553623, recall 0.864067
2017-12-10T14:08:38.997613: step 2920, loss 0.158679, acc 0.984375, prec 0.055383, recall 0.864117
2017-12-10T14:08:39.186068: step 2921, loss 0.190893, acc 0.9375, prec 0.055422, recall 0.864215
2017-12-10T14:08:39.373425: step 2922, loss 0.212879, acc 0.90625, prec 0.0554363, recall 0.864265
2017-12-10T14:08:39.566357: step 2923, loss 0.269437, acc 0.875, prec 0.0554259, recall 0.864265
2017-12-10T14:08:39.754723: step 2924, loss 0.218599, acc 0.90625, prec 0.0554181, recall 0.864265
2017-12-10T14:08:39.945182: step 2925, loss 0.224088, acc 0.90625, prec 0.0554104, recall 0.864265
2017-12-10T14:08:40.137336: step 2926, loss 0.253424, acc 0.890625, prec 0.0554013, recall 0.864265
2017-12-10T14:08:40.325243: step 2927, loss 0.357243, acc 0.9375, prec 0.0554182, recall 0.864314
2017-12-10T14:08:40.514182: step 2928, loss 0.493848, acc 0.859375, prec 0.0554506, recall 0.864413
2017-12-10T14:08:40.699812: step 2929, loss 0.292401, acc 0.9375, prec 0.0554455, recall 0.864413
2017-12-10T14:08:40.885355: step 2930, loss 0.165903, acc 0.96875, prec 0.0554429, recall 0.864413
2017-12-10T14:08:41.073184: step 2931, loss 2.11956, acc 0.90625, prec 0.0554364, recall 0.864099
2017-12-10T14:08:41.264532: step 2932, loss 6.58878, acc 0.921875, prec 0.0554533, recall 0.863834
2017-12-10T14:08:41.457521: step 2933, loss 0.189767, acc 0.921875, prec 0.0554688, recall 0.863884
2017-12-10T14:08:41.649081: step 2934, loss 0.404426, acc 0.859375, prec 0.0555012, recall 0.863983
2017-12-10T14:08:41.837234: step 2935, loss 0.460011, acc 0.890625, prec 0.0554921, recall 0.863983
2017-12-10T14:08:42.028726: step 2936, loss 0.346639, acc 0.828125, prec 0.0555219, recall 0.864081
2017-12-10T14:08:42.216665: step 2937, loss 0.709076, acc 0.796875, prec 0.0555051, recall 0.864081
2017-12-10T14:08:42.401718: step 2938, loss 0.295893, acc 0.890625, prec 0.0554961, recall 0.864081
2017-12-10T14:08:42.588889: step 2939, loss 0.820782, acc 0.828125, prec 0.0555038, recall 0.86413
2017-12-10T14:08:42.779036: step 2940, loss 0.590988, acc 0.875, prec 0.0555814, recall 0.864327
2017-12-10T14:08:42.969689: step 2941, loss 0.463902, acc 0.828125, prec 0.0556111, recall 0.864425
2017-12-10T14:08:43.156858: step 2942, loss 0.357158, acc 0.921875, prec 0.0556047, recall 0.864425
2017-12-10T14:08:43.345969: step 2943, loss 0.460475, acc 0.859375, prec 0.055615, recall 0.864474
2017-12-10T14:08:43.535272: step 2944, loss 0.728435, acc 0.859375, prec 0.0556253, recall 0.864523
2017-12-10T14:08:43.720405: step 2945, loss 0.525144, acc 0.828125, prec 0.0556111, recall 0.864523
2017-12-10T14:08:43.908922: step 2946, loss 0.306385, acc 0.890625, prec 0.055624, recall 0.864572
2017-12-10T14:08:44.102115: step 2947, loss 0.681916, acc 0.921875, prec 0.0556614, recall 0.86467
2017-12-10T14:08:44.292157: step 2948, loss 0.44049, acc 0.875, prec 0.0556949, recall 0.864767
2017-12-10T14:08:44.483491: step 2949, loss 0.565059, acc 0.84375, prec 0.055682, recall 0.864767
2017-12-10T14:08:44.672235: step 2950, loss 0.631419, acc 0.859375, prec 0.0556703, recall 0.864767
2017-12-10T14:08:44.861263: step 2951, loss 0.251836, acc 0.890625, prec 0.0556613, recall 0.864767
2017-12-10T14:08:45.047302: step 2952, loss 0.599534, acc 0.859375, prec 0.0556497, recall 0.864767
2017-12-10T14:08:45.236386: step 2953, loss 0.215415, acc 0.921875, prec 0.0556432, recall 0.864767
2017-12-10T14:08:45.423866: step 2954, loss 0.129533, acc 0.953125, prec 0.0556613, recall 0.864816
2017-12-10T14:08:45.610258: step 2955, loss 2.14815, acc 0.875, prec 0.055696, recall 0.864602
2017-12-10T14:08:45.801241: step 2956, loss 0.218972, acc 0.875, prec 0.0556857, recall 0.864602
2017-12-10T14:08:45.992397: step 2957, loss 0.284004, acc 0.9375, prec 0.0557024, recall 0.864651
2017-12-10T14:08:46.181255: step 2958, loss 0.469133, acc 0.90625, prec 0.0557166, recall 0.8647
2017-12-10T14:08:46.372613: step 2959, loss 0.0752627, acc 0.984375, prec 0.0557591, recall 0.864797
2017-12-10T14:08:46.560616: step 2960, loss 0.188895, acc 0.921875, prec 0.0557964, recall 0.864894
2017-12-10T14:08:46.749977: step 2961, loss 0.296678, acc 0.90625, prec 0.0558105, recall 0.864943
2017-12-10T14:08:46.937739: step 2962, loss 0.388502, acc 0.921875, prec 0.0558697, recall 0.865088
2017-12-10T14:08:47.128016: step 2963, loss 0.365728, acc 0.953125, prec 0.0558877, recall 0.865136
2017-12-10T14:08:47.317514: step 2964, loss 0.152496, acc 0.96875, prec 0.0558851, recall 0.865136
2017-12-10T14:08:47.503359: step 2965, loss 0.103532, acc 0.96875, prec 0.0558825, recall 0.865136
2017-12-10T14:08:47.691733: step 2966, loss 0.295457, acc 0.921875, prec 0.055876, recall 0.865136
2017-12-10T14:08:47.881106: step 2967, loss 0.318435, acc 0.921875, prec 0.0558914, recall 0.865185
2017-12-10T14:08:48.070713: step 2968, loss 0.411096, acc 0.90625, prec 0.0558836, recall 0.865185
2017-12-10T14:08:48.260125: step 2969, loss 0.619017, acc 0.96875, prec 0.0559466, recall 0.86533
2017-12-10T14:08:48.451442: step 2970, loss 0.41383, acc 0.921875, prec 0.0560057, recall 0.865474
2017-12-10T14:08:48.640990: step 2971, loss 0.307441, acc 0.953125, prec 0.0560237, recall 0.865522
2017-12-10T14:08:48.829196: step 2972, loss 0.293532, acc 0.90625, prec 0.0560378, recall 0.86557
2017-12-10T14:08:49.016213: step 2973, loss 3.90919, acc 0.890625, prec 0.05603, recall 0.865261
2017-12-10T14:08:49.210652: step 2974, loss 0.380622, acc 0.875, prec 0.0560196, recall 0.865261
2017-12-10T14:08:49.398542: step 2975, loss 0.224256, acc 0.90625, prec 0.0560118, recall 0.865261
2017-12-10T14:08:49.588044: step 2976, loss 0.262887, acc 0.9375, prec 0.0560067, recall 0.865261
2017-12-10T14:08:49.775545: step 2977, loss 0.502748, acc 0.859375, prec 0.055995, recall 0.865261
2017-12-10T14:08:49.962037: step 2978, loss 0.37462, acc 0.828125, prec 0.0560026, recall 0.865309
2017-12-10T14:08:50.148594: step 2979, loss 0.665657, acc 0.828125, prec 0.0560102, recall 0.865357
2017-12-10T14:08:50.341124: step 2980, loss 0.654731, acc 0.78125, prec 0.0560139, recall 0.865405
2017-12-10T14:08:50.527115: step 2981, loss 0.464823, acc 0.890625, prec 0.0560048, recall 0.865405
2017-12-10T14:08:50.694544: step 2982, loss 0.68197, acc 0.862745, prec 0.0560176, recall 0.865453
2017-12-10T14:08:50.890852: step 2983, loss 0.406861, acc 0.828125, prec 0.0560251, recall 0.865501
2017-12-10T14:08:51.077504: step 2984, loss 0.605556, acc 0.84375, prec 0.056034, recall 0.865549
2017-12-10T14:08:51.266158: step 2985, loss 0.490725, acc 0.875, prec 0.0560236, recall 0.865549
2017-12-10T14:08:51.457522: step 2986, loss 0.398375, acc 0.890625, prec 0.0560146, recall 0.865549
2017-12-10T14:08:51.647885: step 2987, loss 0.356728, acc 0.875, prec 0.0560042, recall 0.865549
2017-12-10T14:08:51.837492: step 2988, loss 0.45546, acc 0.84375, prec 0.0560349, recall 0.865645
2017-12-10T14:08:52.025868: step 2989, loss 0.371842, acc 0.796875, prec 0.0560181, recall 0.865645
2017-12-10T14:08:52.211319: step 2990, loss 0.441021, acc 0.890625, prec 0.056009, recall 0.865645
2017-12-10T14:08:52.399641: step 2991, loss 0.437933, acc 0.859375, prec 0.0560627, recall 0.865789
2017-12-10T14:08:52.587775: step 2992, loss 0.259243, acc 0.9375, prec 0.0561228, recall 0.865932
2017-12-10T14:08:52.777421: step 2993, loss 0.204191, acc 0.953125, prec 0.0561189, recall 0.865932
2017-12-10T14:08:52.968591: step 2994, loss 0.315819, acc 0.875, prec 0.0561086, recall 0.865932
2017-12-10T14:08:53.157137: step 2995, loss 0.233651, acc 0.9375, prec 0.0561034, recall 0.865932
2017-12-10T14:08:53.344961: step 2996, loss 0.238646, acc 0.921875, prec 0.0561187, recall 0.865979
2017-12-10T14:08:53.529735: step 2997, loss 0.155329, acc 0.90625, prec 0.0561327, recall 0.866027
2017-12-10T14:08:53.721636: step 2998, loss 0.815274, acc 0.96875, prec 0.0561736, recall 0.866122
2017-12-10T14:08:53.911061: step 2999, loss 0.137975, acc 0.9375, prec 0.0561901, recall 0.86617
2017-12-10T14:08:54.099883: step 3000, loss 0.143961, acc 0.953125, prec 0.0561862, recall 0.86617
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_1/1512932334/checkpoints/model-3000

2017-12-10T14:08:55.374236: step 3001, loss 0.221167, acc 0.921875, prec 0.0562015, recall 0.866217
2017-12-10T14:08:55.563477: step 3002, loss 0.102866, acc 0.953125, prec 0.0562194, recall 0.866265
2017-12-10T14:08:55.747888: step 3003, loss 0.110576, acc 0.984375, prec 0.0562398, recall 0.866312
2017-12-10T14:08:55.936677: step 3004, loss 0.13127, acc 0.96875, prec 0.0562806, recall 0.866407
2017-12-10T14:08:56.128886: step 3005, loss 0.0484479, acc 1, prec 0.0563241, recall 0.866501
2017-12-10T14:08:56.317274: step 3006, loss 0.0255358, acc 1, prec 0.0563241, recall 0.866501
2017-12-10T14:08:56.503767: step 3007, loss 0.113268, acc 0.953125, prec 0.0563419, recall 0.866549
2017-12-10T14:08:56.694118: step 3008, loss 0.235727, acc 0.9375, prec 0.0563584, recall 0.866596
2017-12-10T14:08:56.888421: step 3009, loss 0.459736, acc 0.953125, prec 0.0563763, recall 0.866643
2017-12-10T14:08:57.078547: step 3010, loss 0.281068, acc 0.9375, prec 0.0563711, recall 0.866643
2017-12-10T14:08:57.266642: step 3011, loss 0.0800747, acc 0.984375, prec 0.0563698, recall 0.866643
2017-12-10T14:08:57.456667: step 3012, loss 0.0942621, acc 0.9375, prec 0.0563646, recall 0.866643
2017-12-10T14:08:57.647411: step 3013, loss 0.0894673, acc 0.953125, prec 0.0563607, recall 0.866643
2017-12-10T14:08:57.838095: step 3014, loss 0.2119, acc 0.90625, prec 0.0563529, recall 0.866643
2017-12-10T14:08:58.025145: step 3015, loss 0.107311, acc 0.9375, prec 0.0563911, recall 0.866737
2017-12-10T14:08:58.214058: step 3016, loss 0.621616, acc 0.984375, prec 0.056455, recall 0.866879
2017-12-10T14:08:58.406474: step 3017, loss 0.0685997, acc 0.9375, prec 0.0564498, recall 0.866879
2017-12-10T14:08:58.597802: step 3018, loss 0.0529905, acc 0.984375, prec 0.0565135, recall 0.867019
2017-12-10T14:08:58.790202: step 3019, loss 0.633998, acc 0.96875, prec 0.0565326, recall 0.867066
2017-12-10T14:08:58.992214: step 3020, loss 0.173977, acc 0.9375, prec 0.0565491, recall 0.867113
2017-12-10T14:08:59.178934: step 3021, loss 0.156935, acc 0.921875, prec 0.056586, recall 0.867207
2017-12-10T14:08:59.366746: step 3022, loss 0.2555, acc 0.921875, prec 0.0565795, recall 0.867207
2017-12-10T14:08:59.553557: step 3023, loss 0.0194237, acc 0.984375, prec 0.0565782, recall 0.867207
2017-12-10T14:08:59.741953: step 3024, loss 2.36229, acc 0.953125, prec 0.0566189, recall 0.866995
2017-12-10T14:08:59.932600: step 3025, loss 0.125592, acc 0.96875, prec 0.0566163, recall 0.866995
2017-12-10T14:09:00.121455: step 3026, loss 0.146535, acc 0.96875, prec 0.0566788, recall 0.867135
2017-12-10T14:09:00.311090: step 3027, loss 0.257263, acc 0.953125, prec 0.0566749, recall 0.867135
2017-12-10T14:09:00.498033: step 3028, loss 0.303298, acc 0.9375, prec 0.056713, recall 0.867229
2017-12-10T14:09:00.686744: step 3029, loss 0.292245, acc 0.953125, prec 0.0567091, recall 0.867229
2017-12-10T14:09:00.873686: step 3030, loss 0.306157, acc 0.90625, prec 0.0567013, recall 0.867229
2017-12-10T14:09:01.063169: step 3031, loss 0.465045, acc 0.875, prec 0.0567125, recall 0.867275
2017-12-10T14:09:01.257813: step 3032, loss 0.370629, acc 0.90625, prec 0.0567047, recall 0.867275
2017-12-10T14:09:01.447648: step 3033, loss 0.378671, acc 0.921875, prec 0.0566982, recall 0.867275
2017-12-10T14:09:01.635718: step 3034, loss 0.384824, acc 0.921875, prec 0.0566917, recall 0.867275
2017-12-10T14:09:01.823309: step 3035, loss 0.423041, acc 0.890625, prec 0.0567042, recall 0.867322
2017-12-10T14:09:02.015070: step 3036, loss 0.533869, acc 0.890625, prec 0.0567384, recall 0.867415
2017-12-10T14:09:02.203363: step 3037, loss 0.347309, acc 0.921875, prec 0.0567319, recall 0.867415
2017-12-10T14:09:02.395193: step 3038, loss 1.38068, acc 0.890625, prec 0.0567457, recall 0.867157
2017-12-10T14:09:02.584601: step 3039, loss 0.308424, acc 0.890625, prec 0.0567799, recall 0.86725
2017-12-10T14:09:02.773747: step 3040, loss 0.198268, acc 0.9375, prec 0.0567747, recall 0.86725
2017-12-10T14:09:02.960208: step 3041, loss 0.380462, acc 0.921875, prec 0.0567898, recall 0.867297
2017-12-10T14:09:03.150929: step 3042, loss 0.205286, acc 0.953125, prec 0.0568075, recall 0.867343
2017-12-10T14:09:03.339652: step 3043, loss 0.427773, acc 0.875, prec 0.0568619, recall 0.867483
2017-12-10T14:09:03.530493: step 3044, loss 0.415106, acc 0.921875, prec 0.0568554, recall 0.867483
2017-12-10T14:09:03.715560: step 3045, loss 0.377098, acc 0.875, prec 0.056845, recall 0.867483
2017-12-10T14:09:03.903780: step 3046, loss 0.177278, acc 0.921875, prec 0.0568385, recall 0.867483
2017-12-10T14:09:04.090846: step 3047, loss 0.210712, acc 0.96875, prec 0.0568791, recall 0.867575
2017-12-10T14:09:04.282875: step 3048, loss 0.354356, acc 0.921875, prec 0.0568942, recall 0.867621
2017-12-10T14:09:04.473116: step 3049, loss 0.151553, acc 0.921875, prec 0.0568877, recall 0.867621
2017-12-10T14:09:04.657861: step 3050, loss 0.248272, acc 0.90625, prec 0.0568799, recall 0.867621
2017-12-10T14:09:04.843783: step 3051, loss 0.127221, acc 0.953125, prec 0.0568759, recall 0.867621
2017-12-10T14:09:05.035915: step 3052, loss 0.011072, acc 1, prec 0.0568759, recall 0.867621
2017-12-10T14:09:05.223282: step 3053, loss 0.10924, acc 0.96875, prec 0.0568949, recall 0.867668
2017-12-10T14:09:05.411232: step 3054, loss 0.261121, acc 0.953125, prec 0.056891, recall 0.867668
2017-12-10T14:09:05.599594: step 3055, loss 0.296586, acc 0.921875, prec 0.0569061, recall 0.867714
2017-12-10T14:09:05.787687: step 3056, loss 0.0730131, acc 0.984375, prec 0.0569048, recall 0.867714
2017-12-10T14:09:05.973882: step 3057, loss 0.0918645, acc 0.96875, prec 0.0569022, recall 0.867714
2017-12-10T14:09:06.163478: step 3058, loss 0.147989, acc 0.953125, prec 0.0569199, recall 0.86776
2017-12-10T14:09:06.355612: step 3059, loss 2.61574, acc 0.96875, prec 0.0569833, recall 0.867596
2017-12-10T14:09:06.547033: step 3060, loss 0.018295, acc 1, prec 0.0570265, recall 0.867688
2017-12-10T14:09:06.737767: step 3061, loss 0.206029, acc 0.96875, prec 0.0570454, recall 0.867734
2017-12-10T14:09:06.929389: step 3062, loss 0.60375, acc 0.96875, prec 0.0570644, recall 0.86778
2017-12-10T14:09:07.119448: step 3063, loss 0.310675, acc 0.890625, prec 0.0570553, recall 0.86778
2017-12-10T14:09:07.308954: step 3064, loss 0.69016, acc 0.8125, prec 0.0570612, recall 0.867826
2017-12-10T14:09:07.497945: step 3065, loss 0.417423, acc 0.890625, prec 0.0570952, recall 0.867918
2017-12-10T14:09:07.687131: step 3066, loss 0.332834, acc 0.890625, prec 0.057086, recall 0.867918
2017-12-10T14:09:07.873048: step 3067, loss 0.343856, acc 0.875, prec 0.0570971, recall 0.867964
2017-12-10T14:09:08.060815: step 3068, loss 0.326632, acc 0.890625, prec 0.057088, recall 0.867964
2017-12-10T14:09:08.250481: step 3069, loss 0.425572, acc 0.859375, prec 0.0570763, recall 0.867964
2017-12-10T14:09:08.437788: step 3070, loss 0.700119, acc 0.9375, prec 0.0570926, recall 0.86801
2017-12-10T14:09:08.627651: step 3071, loss 0.353332, acc 0.890625, prec 0.057105, recall 0.868056
2017-12-10T14:09:08.818817: step 3072, loss 0.355381, acc 0.90625, prec 0.0571402, recall 0.868147
2017-12-10T14:09:09.002383: step 3073, loss 0.401156, acc 0.875, prec 0.0571298, recall 0.868147
2017-12-10T14:09:09.190253: step 3074, loss 0.430335, acc 0.875, prec 0.0571624, recall 0.868239
2017-12-10T14:09:09.375054: step 3075, loss 0.826502, acc 0.8125, prec 0.0571468, recall 0.868239
2017-12-10T14:09:09.564570: step 3076, loss 0.328568, acc 0.921875, prec 0.0571618, recall 0.868284
2017-12-10T14:09:09.754716: step 3077, loss 0.27547, acc 0.890625, prec 0.0571957, recall 0.868375
2017-12-10T14:09:09.941694: step 3078, loss 0.511905, acc 0.84375, prec 0.0571826, recall 0.868375
2017-12-10T14:09:10.127491: step 3079, loss 0.283624, acc 0.921875, prec 0.0571976, recall 0.868421
2017-12-10T14:09:10.315153: step 3080, loss 0.474423, acc 0.9375, prec 0.0572354, recall 0.868512
2017-12-10T14:09:10.507247: step 3081, loss 0.129087, acc 0.96875, prec 0.0572543, recall 0.868558
2017-12-10T14:09:10.695713: step 3082, loss 0.128614, acc 0.96875, prec 0.0572731, recall 0.868603
2017-12-10T14:09:10.882551: step 3083, loss 0.165369, acc 0.96875, prec 0.057292, recall 0.868648
2017-12-10T14:09:11.072227: step 3084, loss 0.701561, acc 0.953125, prec 0.0573741, recall 0.86883
2017-12-10T14:09:11.261442: step 3085, loss 0.231585, acc 0.921875, prec 0.057389, recall 0.868875
2017-12-10T14:09:11.449538: step 3086, loss 3.61738, acc 0.90625, prec 0.0574254, recall 0.868666
2017-12-10T14:09:11.638488: step 3087, loss 0.388705, acc 0.875, prec 0.0574364, recall 0.868711
2017-12-10T14:09:11.825188: step 3088, loss 0.156776, acc 0.921875, prec 0.0574299, recall 0.868711
2017-12-10T14:09:12.012468: step 3089, loss 0.28077, acc 0.90625, prec 0.057422, recall 0.868711
2017-12-10T14:09:12.203780: step 3090, loss 0.559507, acc 0.90625, prec 0.0574357, recall 0.868756
2017-12-10T14:09:12.394542: step 3091, loss 0.537607, acc 0.890625, prec 0.0574909, recall 0.868892
2017-12-10T14:09:12.586922: step 3092, loss 0.357471, acc 0.890625, prec 0.0574817, recall 0.868892
2017-12-10T14:09:12.772600: step 3093, loss 0.424135, acc 0.890625, prec 0.0574726, recall 0.868892
2017-12-10T14:09:12.963100: step 3094, loss 0.417691, acc 0.84375, prec 0.0574809, recall 0.868937
2017-12-10T14:09:13.154082: step 3095, loss 0.220431, acc 0.953125, prec 0.0574985, recall 0.868982
2017-12-10T14:09:13.345921: step 3096, loss 0.454874, acc 0.8125, prec 0.0575256, recall 0.869072
2017-12-10T14:09:13.532775: step 3097, loss 0.294371, acc 0.90625, prec 0.0575178, recall 0.869072
2017-12-10T14:09:13.719435: step 3098, loss 0.618544, acc 0.875, prec 0.0575288, recall 0.869117
2017-12-10T14:09:13.912117: step 3099, loss 0.11853, acc 0.953125, prec 0.0575248, recall 0.869117
2017-12-10T14:09:14.103766: step 3100, loss 0.820389, acc 0.8125, prec 0.057552, recall 0.869207
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_1/1512932334/checkpoints/model-3100

2017-12-10T14:09:15.360902: step 3101, loss 0.286641, acc 0.953125, prec 0.0576123, recall 0.869342
2017-12-10T14:09:15.546712: step 3102, loss 0.0771579, acc 0.96875, prec 0.0576097, recall 0.869342
2017-12-10T14:09:15.735358: step 3103, loss 0.319043, acc 0.921875, prec 0.0576032, recall 0.869342
2017-12-10T14:09:15.920089: step 3104, loss 0.637345, acc 0.984375, prec 0.0576447, recall 0.869431
2017-12-10T14:09:16.110024: step 3105, loss 0.350105, acc 0.9375, prec 0.0576394, recall 0.869431
2017-12-10T14:09:16.298581: step 3106, loss 0.143142, acc 0.953125, prec 0.0576569, recall 0.869476
2017-12-10T14:09:16.486428: step 3107, loss 0.267422, acc 0.921875, prec 0.0576718, recall 0.869521
2017-12-10T14:09:16.670867: step 3108, loss 0.36453, acc 0.921875, prec 0.0576652, recall 0.869521
2017-12-10T14:09:16.857185: step 3109, loss 0.0697253, acc 0.984375, prec 0.0576639, recall 0.869521
2017-12-10T14:09:17.042582: step 3110, loss 0.358744, acc 0.890625, prec 0.0576762, recall 0.869565
2017-12-10T14:09:17.234081: step 3111, loss 0.29047, acc 0.90625, prec 0.0576897, recall 0.86961
2017-12-10T14:09:17.419928: step 3112, loss 0.139215, acc 0.953125, prec 0.0577071, recall 0.869654
2017-12-10T14:09:17.608978: step 3113, loss 0.149923, acc 0.96875, prec 0.0577045, recall 0.869654
2017-12-10T14:09:17.797855: step 3114, loss 0.258529, acc 0.90625, prec 0.0577394, recall 0.869744
2017-12-10T14:09:17.985124: step 3115, loss 1.97206, acc 0.921875, prec 0.0577556, recall 0.869491
2017-12-10T14:09:18.176477: step 3116, loss 0.495159, acc 0.90625, prec 0.0577691, recall 0.869536
2017-12-10T14:09:18.365247: step 3117, loss 0.100419, acc 0.953125, prec 0.0577652, recall 0.869536
2017-12-10T14:09:18.553032: step 3118, loss 0.0613197, acc 0.984375, prec 0.0577639, recall 0.869536
2017-12-10T14:09:18.744858: step 3119, loss 0.27281, acc 0.890625, prec 0.0577761, recall 0.86958
2017-12-10T14:09:18.932454: step 3120, loss 0.291447, acc 0.96875, prec 0.0577948, recall 0.869625
2017-12-10T14:09:19.121551: step 3121, loss 0.397741, acc 0.90625, prec 0.0577869, recall 0.869625
2017-12-10T14:09:19.312987: step 3122, loss 0.701897, acc 0.90625, prec 0.0578005, recall 0.869669
2017-12-10T14:09:19.504285: step 3123, loss 0.14752, acc 0.953125, prec 0.0578392, recall 0.869758
2017-12-10T14:09:19.689581: step 3124, loss 0.189336, acc 0.96875, prec 0.0578793, recall 0.869847
2017-12-10T14:09:19.878705: step 3125, loss 0.115919, acc 0.953125, prec 0.0578968, recall 0.869891
2017-12-10T14:09:20.066913: step 3126, loss 0.117457, acc 0.9375, prec 0.0578915, recall 0.869891
2017-12-10T14:09:20.258388: step 3127, loss 0.107449, acc 0.984375, prec 0.0579543, recall 0.870024
2017-12-10T14:09:20.449807: step 3128, loss 0.18672, acc 0.9375, prec 0.0579704, recall 0.870068
2017-12-10T14:09:20.641842: step 3129, loss 0.157938, acc 0.921875, prec 0.0579638, recall 0.870068
2017-12-10T14:09:20.832450: step 3130, loss 0.178853, acc 0.953125, prec 0.0580239, recall 0.8702
2017-12-10T14:09:21.022731: step 3131, loss 0.33285, acc 0.90625, prec 0.058016, recall 0.8702
2017-12-10T14:09:21.214415: step 3132, loss 0.170086, acc 0.953125, prec 0.0580334, recall 0.870245
2017-12-10T14:09:21.403094: step 3133, loss 0.0896316, acc 0.96875, prec 0.0580308, recall 0.870245
2017-12-10T14:09:21.590553: step 3134, loss 0.443417, acc 0.921875, prec 0.0580242, recall 0.870245
2017-12-10T14:09:21.787226: step 3135, loss 1.80778, acc 0.953125, prec 0.0580216, recall 0.869949
2017-12-10T14:09:21.977708: step 3136, loss 0.0917964, acc 0.953125, prec 0.0580176, recall 0.869949
2017-12-10T14:09:22.162706: step 3137, loss 0.286973, acc 0.890625, prec 0.0580084, recall 0.869949
2017-12-10T14:09:22.353985: step 3138, loss 0.0125553, acc 1, prec 0.0580084, recall 0.869949
2017-12-10T14:09:22.542694: step 3139, loss 0.261576, acc 0.921875, prec 0.0580445, recall 0.870037
2017-12-10T14:09:22.730719: step 3140, loss 0.266962, acc 0.921875, prec 0.0580379, recall 0.870037
2017-12-10T14:09:22.918605: step 3141, loss 0.185665, acc 0.953125, prec 0.058034, recall 0.870037
2017-12-10T14:09:23.105936: step 3142, loss 0.395086, acc 0.96875, prec 0.0580953, recall 0.87017
2017-12-10T14:09:23.293855: step 3143, loss 0.697126, acc 0.921875, prec 0.0581101, recall 0.870214
2017-12-10T14:09:23.487848: step 3144, loss 0.163893, acc 0.953125, prec 0.0581274, recall 0.870257
2017-12-10T14:09:23.678704: step 3145, loss 1.13493, acc 0.9375, prec 0.0581648, recall 0.870345
2017-12-10T14:09:23.867431: step 3146, loss 0.103737, acc 0.96875, prec 0.0581835, recall 0.870389
2017-12-10T14:09:24.055692: step 3147, loss 0.236263, acc 0.921875, prec 0.0581982, recall 0.870433
2017-12-10T14:09:24.246912: step 3148, loss 0.337623, acc 0.96875, prec 0.0582808, recall 0.870608
2017-12-10T14:09:24.432388: step 3149, loss 0.729224, acc 0.890625, prec 0.0582928, recall 0.870652
2017-12-10T14:09:24.623353: step 3150, loss 0.301496, acc 0.90625, prec 0.0582849, recall 0.870652
2017-12-10T14:09:24.810548: step 3151, loss 0.379192, acc 0.890625, prec 0.0582757, recall 0.870652
2017-12-10T14:09:24.999155: step 3152, loss 0.281423, acc 0.9375, prec 0.0582917, recall 0.870695
2017-12-10T14:09:25.182910: step 3153, loss 0.491619, acc 0.875, prec 0.0583237, recall 0.870783
2017-12-10T14:09:25.373074: step 3154, loss 0.19445, acc 0.953125, prec 0.0583411, recall 0.870826
2017-12-10T14:09:25.563733: step 3155, loss 0.479178, acc 0.875, prec 0.0583518, recall 0.87087
2017-12-10T14:09:25.752212: step 3156, loss 0.308178, acc 0.921875, prec 0.0583665, recall 0.870913
2017-12-10T14:09:25.942716: step 3157, loss 0.65975, acc 0.90625, prec 0.0583586, recall 0.870913
2017-12-10T14:09:26.131826: step 3158, loss 0.0882076, acc 0.96875, prec 0.0583559, recall 0.870913
2017-12-10T14:09:26.324260: step 3159, loss 0.248824, acc 0.921875, prec 0.0583493, recall 0.870913
2017-12-10T14:09:26.510973: step 3160, loss 0.241372, acc 0.875, prec 0.05836, recall 0.870957
2017-12-10T14:09:26.697475: step 3161, loss 0.428395, acc 0.921875, prec 0.058396, recall 0.871044
2017-12-10T14:09:26.884257: step 3162, loss 0.176214, acc 0.921875, prec 0.0584106, recall 0.871087
2017-12-10T14:09:27.071313: step 3163, loss 1.22072, acc 0.921875, prec 0.0584465, recall 0.871174
2017-12-10T14:09:27.263614: step 3164, loss 0.228163, acc 0.921875, prec 0.0584824, recall 0.871261
2017-12-10T14:09:27.451271: step 3165, loss 0.465016, acc 0.921875, prec 0.0584758, recall 0.871261
2017-12-10T14:09:27.647496: step 3166, loss 0.113681, acc 0.9375, prec 0.0584706, recall 0.871261
2017-12-10T14:09:27.835520: step 3167, loss 0.344126, acc 0.921875, prec 0.058464, recall 0.871261
2017-12-10T14:09:28.021816: step 3168, loss 0.120972, acc 0.96875, prec 0.0584613, recall 0.871261
2017-12-10T14:09:28.206875: step 3169, loss 0.223282, acc 0.96875, prec 0.0584799, recall 0.871304
2017-12-10T14:09:28.397810: step 3170, loss 0.253227, acc 0.90625, prec 0.058472, recall 0.871304
2017-12-10T14:09:28.584337: step 3171, loss 0.219178, acc 0.921875, prec 0.0584654, recall 0.871304
2017-12-10T14:09:28.771288: step 3172, loss 0.23237, acc 0.890625, prec 0.0584562, recall 0.871304
2017-12-10T14:09:28.965445: step 3173, loss 0.0826481, acc 0.96875, prec 0.0584748, recall 0.871347
2017-12-10T14:09:29.154304: step 3174, loss 0.113854, acc 0.953125, prec 0.0584708, recall 0.871347
2017-12-10T14:09:29.340795: step 3175, loss 0.416975, acc 0.9375, prec 0.058508, recall 0.871433
2017-12-10T14:09:29.532533: step 3176, loss 0.233936, acc 0.9375, prec 0.0585027, recall 0.871433
2017-12-10T14:09:29.721085: step 3177, loss 0.175368, acc 0.953125, prec 0.0584988, recall 0.871433
2017-12-10T14:09:29.913024: step 3178, loss 0.557522, acc 0.984375, prec 0.0585399, recall 0.87152
2017-12-10T14:09:30.105783: step 3179, loss 0.251598, acc 0.90625, prec 0.0585744, recall 0.871606
2017-12-10T14:09:30.290994: step 3180, loss 0.108875, acc 0.984375, prec 0.0585943, recall 0.871649
2017-12-10T14:09:30.482153: step 3181, loss 0.135707, acc 0.96875, prec 0.0585916, recall 0.871649
2017-12-10T14:09:30.669972: step 3182, loss 0.237623, acc 0.9375, prec 0.0585864, recall 0.871649
2017-12-10T14:09:30.856870: step 3183, loss 0.1763, acc 0.921875, prec 0.0585798, recall 0.871649
2017-12-10T14:09:31.057546: step 3184, loss 0.105969, acc 0.96875, prec 0.0586407, recall 0.871778
2017-12-10T14:09:31.252924: step 3185, loss 0.0938058, acc 0.984375, prec 0.0586394, recall 0.871778
2017-12-10T14:09:31.448097: step 3186, loss 0.143288, acc 0.984375, prec 0.0586593, recall 0.871821
2017-12-10T14:09:31.635698: step 3187, loss 1.13592, acc 0.90625, prec 0.0586726, recall 0.871863
2017-12-10T14:09:31.823436: step 3188, loss 0.336703, acc 0.921875, prec 0.0586659, recall 0.871863
2017-12-10T14:09:32.009713: step 3189, loss 0.685632, acc 0.953125, prec 0.0587044, recall 0.871949
2017-12-10T14:09:32.201748: step 3190, loss 0.0454182, acc 0.96875, prec 0.0587229, recall 0.871992
2017-12-10T14:09:32.395223: step 3191, loss 0.471508, acc 0.90625, prec 0.0587573, recall 0.872077
2017-12-10T14:09:32.586786: step 3192, loss 0.129431, acc 0.953125, prec 0.0587534, recall 0.872077
2017-12-10T14:09:32.774999: step 3193, loss 0.343823, acc 0.921875, prec 0.0588315, recall 0.872248
2017-12-10T14:09:32.962821: step 3194, loss 0.260268, acc 0.921875, prec 0.0588249, recall 0.872248
2017-12-10T14:09:33.151397: step 3195, loss 0.25473, acc 0.921875, prec 0.0588394, recall 0.872291
2017-12-10T14:09:33.338586: step 3196, loss 0.432433, acc 0.90625, prec 0.058895, recall 0.872418
2017-12-10T14:09:33.528956: step 3197, loss 0.249794, acc 0.90625, prec 0.0589082, recall 0.872461
2017-12-10T14:09:33.717457: step 3198, loss 0.194427, acc 0.921875, prec 0.0589227, recall 0.872503
2017-12-10T14:09:33.903908: step 3199, loss 0.473016, acc 0.859375, prec 0.0589108, recall 0.872503
2017-12-10T14:09:34.094401: step 3200, loss 0.468986, acc 0.875, prec 0.0589213, recall 0.872546
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_1/1512932334/checkpoints/model-3200

2017-12-10T14:09:35.276538: step 3201, loss 0.12245, acc 0.953125, prec 0.0589597, recall 0.872631
2017-12-10T14:09:35.464814: step 3202, loss 0.518367, acc 0.859375, prec 0.05899, recall 0.872715
2017-12-10T14:09:35.652114: step 3203, loss 0.572488, acc 0.859375, prec 0.0589781, recall 0.872715
2017-12-10T14:09:35.839047: step 3204, loss 0.373451, acc 0.890625, prec 0.0589688, recall 0.872715
2017-12-10T14:09:36.027442: step 3205, loss 0.47556, acc 0.859375, prec 0.058978, recall 0.872757
2017-12-10T14:09:36.221649: step 3206, loss 1.18815, acc 0.90625, prec 0.0590335, recall 0.872884
2017-12-10T14:09:36.411493: step 3207, loss 0.0238767, acc 1, prec 0.0590757, recall 0.872968
2017-12-10T14:09:36.596986: step 3208, loss 0.287069, acc 0.921875, prec 0.0590691, recall 0.872968
2017-12-10T14:09:36.786970: step 3209, loss 0.131851, acc 0.9375, prec 0.0590849, recall 0.873011
2017-12-10T14:09:36.976810: step 3210, loss 1.9702, acc 0.9375, prec 0.0590809, recall 0.872721
2017-12-10T14:09:37.167145: step 3211, loss 0.162569, acc 0.953125, prec 0.059098, recall 0.872763
2017-12-10T14:09:37.356983: step 3212, loss 0.235653, acc 0.953125, prec 0.0591152, recall 0.872806
2017-12-10T14:09:37.544374: step 3213, loss 0.509785, acc 0.859375, prec 0.0591244, recall 0.872848
2017-12-10T14:09:37.731323: step 3214, loss 0.374889, acc 0.921875, prec 0.0591388, recall 0.87289
2017-12-10T14:09:37.919466: step 3215, loss 0.214241, acc 0.90625, prec 0.059152, recall 0.872932
2017-12-10T14:09:38.104618: step 3216, loss 0.115201, acc 0.953125, prec 0.059148, recall 0.872932
2017-12-10T14:09:38.295706: step 3217, loss 0.210619, acc 0.890625, prec 0.0591598, recall 0.872974
2017-12-10T14:09:38.487484: step 3218, loss 0.398866, acc 0.890625, prec 0.0591505, recall 0.872974
2017-12-10T14:09:38.671772: step 3219, loss 0.177319, acc 0.921875, prec 0.0591439, recall 0.872974
2017-12-10T14:09:38.859784: step 3220, loss 0.204842, acc 0.9375, prec 0.0591386, recall 0.872974
2017-12-10T14:09:39.050754: step 3221, loss 0.344742, acc 0.875, prec 0.0591491, recall 0.873016
2017-12-10T14:09:39.238506: step 3222, loss 0.572833, acc 0.828125, prec 0.0591556, recall 0.873058
2017-12-10T14:09:39.432013: step 3223, loss 0.408823, acc 0.875, prec 0.059145, recall 0.873058
2017-12-10T14:09:39.618490: step 3224, loss 0.1704, acc 0.96875, prec 0.0591423, recall 0.873058
2017-12-10T14:09:39.801983: step 3225, loss 0.309188, acc 0.890625, prec 0.0591541, recall 0.8731
2017-12-10T14:09:39.988198: step 3226, loss 0.114124, acc 0.96875, prec 0.0591725, recall 0.873142
2017-12-10T14:09:40.178796: step 3227, loss 0.198944, acc 0.9375, prec 0.0591672, recall 0.873142
2017-12-10T14:09:40.364966: step 3228, loss 0.112185, acc 0.96875, prec 0.0592067, recall 0.873226
2017-12-10T14:09:40.555257: step 3229, loss 0.152869, acc 0.953125, prec 0.0592238, recall 0.873267
2017-12-10T14:09:40.740722: step 3230, loss 5.49213, acc 0.890625, prec 0.0592369, recall 0.873021
2017-12-10T14:09:40.931827: step 3231, loss 0.383062, acc 0.890625, prec 0.0592487, recall 0.873063
2017-12-10T14:09:41.118741: step 3232, loss 0.126404, acc 0.9375, prec 0.0592434, recall 0.873063
2017-12-10T14:09:41.303964: step 3233, loss 0.137438, acc 0.953125, prec 0.0592815, recall 0.873147
2017-12-10T14:09:41.490127: step 3234, loss 0.159066, acc 0.953125, prec 0.0592985, recall 0.873188
2017-12-10T14:09:41.678327: step 3235, loss 0.50121, acc 0.875, prec 0.059309, recall 0.87323
2017-12-10T14:09:41.871952: step 3236, loss 0.340667, acc 0.921875, prec 0.0593444, recall 0.873314
2017-12-10T14:09:42.061703: step 3237, loss 0.217373, acc 0.921875, prec 0.0593378, recall 0.873314
2017-12-10T14:09:42.250262: step 3238, loss 0.36645, acc 0.921875, prec 0.0593522, recall 0.873355
2017-12-10T14:09:42.438081: step 3239, loss 0.307177, acc 0.890625, prec 0.0593849, recall 0.873439
2017-12-10T14:09:42.625261: step 3240, loss 0.713532, acc 0.90625, prec 0.059419, recall 0.873522
2017-12-10T14:09:42.814314: step 3241, loss 0.641605, acc 0.828125, prec 0.0594884, recall 0.873688
2017-12-10T14:09:43.006663: step 3242, loss 0.212911, acc 0.921875, prec 0.0594818, recall 0.873688
2017-12-10T14:09:43.192657: step 3243, loss 0.464433, acc 0.890625, prec 0.0594935, recall 0.873729
2017-12-10T14:09:43.376177: step 3244, loss 0.090575, acc 0.96875, prec 0.0595328, recall 0.873812
2017-12-10T14:09:43.565575: step 3245, loss 0.214062, acc 0.90625, prec 0.0595249, recall 0.873812
2017-12-10T14:09:43.754123: step 3246, loss 0.229923, acc 0.96875, prec 0.0595432, recall 0.873853
2017-12-10T14:09:43.951098: step 3247, loss 0.147239, acc 0.953125, prec 0.0595812, recall 0.873936
2017-12-10T14:09:44.138216: step 3248, loss 0.177249, acc 0.953125, prec 0.0595772, recall 0.873936
2017-12-10T14:09:44.325399: step 3249, loss 0.187104, acc 0.9375, prec 0.0596139, recall 0.874018
2017-12-10T14:09:44.514199: step 3250, loss 0.234129, acc 0.9375, prec 0.0596086, recall 0.874018
2017-12-10T14:09:44.704278: step 3251, loss 0.146413, acc 0.984375, prec 0.0596492, recall 0.874101
2017-12-10T14:09:44.895946: step 3252, loss 0.194016, acc 0.921875, prec 0.0596425, recall 0.874101
2017-12-10T14:09:45.084324: step 3253, loss 0.313486, acc 0.890625, prec 0.0596542, recall 0.874142
2017-12-10T14:09:45.267048: step 3254, loss 0.154211, acc 0.96875, prec 0.0596935, recall 0.874224
2017-12-10T14:09:45.456719: step 3255, loss 9.9546, acc 0.953125, prec 0.0597118, recall 0.87398
2017-12-10T14:09:45.650238: step 3256, loss 0.339929, acc 0.90625, prec 0.0597458, recall 0.874062
2017-12-10T14:09:45.840209: step 3257, loss 0.417755, acc 0.953125, prec 0.0598047, recall 0.874185
2017-12-10T14:09:46.035593: step 3258, loss 0.209241, acc 0.953125, prec 0.0598216, recall 0.874226
2017-12-10T14:09:46.221504: step 3259, loss 0.301906, acc 0.90625, prec 0.0598136, recall 0.874226
2017-12-10T14:09:46.411313: step 3260, loss 0.197937, acc 0.953125, prec 0.0598306, recall 0.874267
2017-12-10T14:09:46.597961: step 3261, loss 0.187763, acc 0.921875, prec 0.0598239, recall 0.874267
2017-12-10T14:09:46.785478: step 3262, loss 0.330691, acc 0.921875, prec 0.0598382, recall 0.874308
2017-12-10T14:09:46.972416: step 3263, loss 0.24477, acc 0.90625, prec 0.0598302, recall 0.874308
2017-12-10T14:09:47.163563: step 3264, loss 0.152606, acc 0.953125, prec 0.0598262, recall 0.874308
2017-12-10T14:09:47.349322: step 3265, loss 0.228722, acc 0.921875, prec 0.0598195, recall 0.874308
2017-12-10T14:09:47.535980: step 3266, loss 1.0917, acc 0.796875, prec 0.0598232, recall 0.874349
2017-12-10T14:09:47.728316: step 3267, loss 0.75706, acc 0.84375, prec 0.0598308, recall 0.87439
2017-12-10T14:09:47.913912: step 3268, loss 0.239648, acc 0.890625, prec 0.0598424, recall 0.874431
2017-12-10T14:09:48.102748: step 3269, loss 0.622832, acc 0.875, prec 0.0598526, recall 0.874472
2017-12-10T14:09:48.290891: step 3270, loss 0.910294, acc 0.78125, prec 0.059834, recall 0.874472
2017-12-10T14:09:48.479480: step 3271, loss 0.140491, acc 0.953125, prec 0.05983, recall 0.874472
2017-12-10T14:09:48.666551: step 3272, loss 0.47328, acc 0.890625, prec 0.0598625, recall 0.874553
2017-12-10T14:09:48.853724: step 3273, loss 0.854923, acc 0.796875, prec 0.0598452, recall 0.874553
2017-12-10T14:09:49.041435: step 3274, loss 0.180498, acc 0.9375, prec 0.0598399, recall 0.874553
2017-12-10T14:09:49.227413: step 3275, loss 0.285109, acc 0.90625, prec 0.0598319, recall 0.874553
2017-12-10T14:09:49.417396: step 3276, loss 0.167376, acc 0.953125, prec 0.0598906, recall 0.874675
2017-12-10T14:09:49.604427: step 3277, loss 0.43609, acc 0.921875, prec 0.0599049, recall 0.874716
2017-12-10T14:09:49.794609: step 3278, loss 0.0829635, acc 0.953125, prec 0.0599009, recall 0.874716
2017-12-10T14:09:49.987412: step 3279, loss 0.455477, acc 0.953125, prec 0.0599387, recall 0.874797
2017-12-10T14:09:50.174394: step 3280, loss 0.136153, acc 0.9375, prec 0.0599751, recall 0.874878
2017-12-10T14:09:50.360437: step 3281, loss 0.179626, acc 0.9375, prec 0.0599698, recall 0.874878
2017-12-10T14:09:50.550580: step 3282, loss 0.142299, acc 0.953125, prec 0.0599867, recall 0.874919
2017-12-10T14:09:50.740636: step 3283, loss 0.324949, acc 0.90625, prec 0.0599996, recall 0.87496
2017-12-10T14:09:50.929322: step 3284, loss 0.414848, acc 0.875, prec 0.0599889, recall 0.87496
2017-12-10T14:09:51.115333: step 3285, loss 0.122189, acc 0.953125, prec 0.0600266, recall 0.87504
2017-12-10T14:09:51.302362: step 3286, loss 0.225824, acc 0.90625, prec 0.0600604, recall 0.875121
2017-12-10T14:09:51.499375: step 3287, loss 0.09533, acc 0.96875, prec 0.0600577, recall 0.875121
2017-12-10T14:09:51.688537: step 3288, loss 0.314407, acc 0.90625, prec 0.0600706, recall 0.875162
2017-12-10T14:09:51.879754: step 3289, loss 0.129653, acc 0.953125, prec 0.0601083, recall 0.875242
2017-12-10T14:09:52.066446: step 3290, loss 0.423833, acc 0.90625, prec 0.060142, recall 0.875323
2017-12-10T14:09:52.253950: step 3291, loss 0.228361, acc 0.96875, prec 0.0601602, recall 0.875363
2017-12-10T14:09:52.441621: step 3292, loss 0.068099, acc 0.96875, prec 0.0601575, recall 0.875363
2017-12-10T14:09:52.631540: step 3293, loss 0.19368, acc 0.9375, prec 0.0601522, recall 0.875363
2017-12-10T14:09:52.821299: step 3294, loss 0.252368, acc 0.953125, prec 0.0601691, recall 0.875403
2017-12-10T14:09:53.007639: step 3295, loss 0.866057, acc 0.984375, prec 0.0602303, recall 0.875524
2017-12-10T14:09:53.196926: step 3296, loss 0.964805, acc 0.984375, prec 0.0602498, recall 0.875564
2017-12-10T14:09:53.388734: step 3297, loss 0.0308892, acc 0.984375, prec 0.0602901, recall 0.875644
2017-12-10T14:09:53.574398: step 3298, loss 0.134582, acc 0.9375, prec 0.0602848, recall 0.875644
2017-12-10T14:09:53.762317: step 3299, loss 0.201297, acc 0.90625, prec 0.0602768, recall 0.875644
2017-12-10T14:09:53.948922: step 3300, loss 0.205905, acc 0.921875, prec 0.0602701, recall 0.875644
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_1/1512932334/checkpoints/model-3300

2017-12-10T14:09:55.248709: step 3301, loss 6.80562, acc 0.90625, prec 0.0602634, recall 0.875362
2017-12-10T14:09:55.439447: step 3302, loss 0.232326, acc 0.90625, prec 0.0602554, recall 0.875362
2017-12-10T14:09:55.629376: step 3303, loss 0.335901, acc 0.90625, prec 0.0602474, recall 0.875362
2017-12-10T14:09:55.820258: step 3304, loss 0.168152, acc 0.9375, prec 0.0602837, recall 0.875443
2017-12-10T14:09:56.010214: step 3305, loss 0.310832, acc 0.921875, prec 0.0602978, recall 0.875483
2017-12-10T14:09:56.197574: step 3306, loss 0.429125, acc 0.90625, prec 0.0603106, recall 0.875523
2017-12-10T14:09:56.386752: step 3307, loss 0.567145, acc 0.84375, prec 0.0603597, recall 0.875643
2017-12-10T14:09:56.574834: step 3308, loss 0.260447, acc 0.90625, prec 0.0603725, recall 0.875683
2017-12-10T14:09:56.766322: step 3309, loss 0.624514, acc 0.859375, prec 0.0604021, recall 0.875762
2017-12-10T14:09:56.952810: step 3310, loss 0.416863, acc 0.875, prec 0.0604122, recall 0.875802
2017-12-10T14:09:57.138378: step 3311, loss 0.460939, acc 0.875, prec 0.0604223, recall 0.875842
2017-12-10T14:09:57.332826: step 3312, loss 0.458815, acc 0.828125, prec 0.0604492, recall 0.875922
2017-12-10T14:09:57.521707: step 3313, loss 0.400497, acc 0.875, prec 0.0604593, recall 0.875962
2017-12-10T14:09:57.713691: step 3314, loss 0.412585, acc 0.890625, prec 0.0604707, recall 0.876001
2017-12-10T14:09:57.903308: step 3315, loss 0.46275, acc 0.921875, prec 0.0604848, recall 0.876041
2017-12-10T14:09:58.090241: step 3316, loss 0.441641, acc 0.859375, prec 0.0604935, recall 0.876081
2017-12-10T14:09:58.279821: step 3317, loss 0.424802, acc 0.890625, prec 0.0605257, recall 0.87616
2017-12-10T14:09:58.468644: step 3318, loss 0.377, acc 0.828125, prec 0.060511, recall 0.87616
2017-12-10T14:09:58.655608: step 3319, loss 0.070288, acc 0.96875, prec 0.0605083, recall 0.87616
2017-12-10T14:09:58.864821: step 3320, loss 0.464093, acc 0.890625, prec 0.0604989, recall 0.87616
2017-12-10T14:09:59.056080: step 3321, loss 0.373061, acc 0.84375, prec 0.0605271, recall 0.876239
2017-12-10T14:09:59.242841: step 3322, loss 0.400257, acc 0.90625, prec 0.0605398, recall 0.876279
2017-12-10T14:09:59.434955: step 3323, loss 0.35893, acc 0.9375, prec 0.0605345, recall 0.876279
2017-12-10T14:09:59.622300: step 3324, loss 0.510993, acc 0.953125, prec 0.0605719, recall 0.876358
2017-12-10T14:09:59.812938: step 3325, loss 0.141178, acc 0.921875, prec 0.0605652, recall 0.876358
2017-12-10T14:10:00.002826: step 3326, loss 0.169459, acc 0.953125, prec 0.0605612, recall 0.876358
2017-12-10T14:10:00.190998: step 3327, loss 0.0948352, acc 0.96875, prec 0.0605586, recall 0.876358
2017-12-10T14:10:00.377318: step 3328, loss 0.170453, acc 0.90625, prec 0.0605505, recall 0.876358
2017-12-10T14:10:00.564600: step 3329, loss 0.0591487, acc 0.96875, prec 0.0605479, recall 0.876358
2017-12-10T14:10:00.751454: step 3330, loss 0.205868, acc 0.9375, prec 0.0605633, recall 0.876397
2017-12-10T14:10:00.943948: step 3331, loss 0.177043, acc 0.984375, prec 0.0605827, recall 0.876437
2017-12-10T14:10:01.141331: step 3332, loss 0.164532, acc 0.9375, prec 0.0606395, recall 0.876555
2017-12-10T14:10:01.329921: step 3333, loss 1.35368, acc 0.96875, prec 0.0606382, recall 0.876276
2017-12-10T14:10:01.522217: step 3334, loss 0.0428937, acc 0.984375, prec 0.0606368, recall 0.876276
2017-12-10T14:10:01.708983: step 3335, loss 0.31301, acc 0.921875, prec 0.0606716, recall 0.876354
2017-12-10T14:10:01.901862: step 3336, loss 0.595731, acc 0.984375, prec 0.0607117, recall 0.876433
2017-12-10T14:10:02.092819: step 3337, loss 0.194138, acc 0.953125, prec 0.0607491, recall 0.876512
2017-12-10T14:10:02.284346: step 3338, loss 0.50309, acc 0.90625, prec 0.0607825, recall 0.87659
2017-12-10T14:10:02.473691: step 3339, loss 0.494987, acc 0.90625, prec 0.0607952, recall 0.87663
2017-12-10T14:10:02.662971: step 3340, loss 1.25544, acc 0.9375, prec 0.0608312, recall 0.876708
2017-12-10T14:10:02.852079: step 3341, loss 0.297452, acc 0.9375, prec 0.0608466, recall 0.876747
2017-12-10T14:10:03.041615: step 3342, loss 0.471171, acc 0.9375, prec 0.0608826, recall 0.876825
2017-12-10T14:10:03.233016: step 3343, loss 0.844787, acc 0.84375, prec 0.0608899, recall 0.876864
2017-12-10T14:10:03.421558: step 3344, loss 0.284178, acc 0.890625, prec 0.0609219, recall 0.876943
2017-12-10T14:10:03.611110: step 3345, loss 0.64524, acc 0.859375, prec 0.0609305, recall 0.876982
2017-12-10T14:10:03.799666: step 3346, loss 0.409723, acc 0.890625, prec 0.0609624, recall 0.87706
2017-12-10T14:10:03.985526: step 3347, loss 0.379774, acc 0.875, prec 0.0609724, recall 0.877099
2017-12-10T14:10:04.174815: step 3348, loss 4.70496, acc 0.859375, prec 0.0609616, recall 0.876821
2017-12-10T14:10:04.360144: step 3349, loss 0.540842, acc 0.84375, prec 0.0609482, recall 0.876821
2017-12-10T14:10:04.550567: step 3350, loss 0.673049, acc 0.828125, prec 0.0609335, recall 0.876821
2017-12-10T14:10:04.739344: step 3351, loss 0.422139, acc 0.875, prec 0.0609434, recall 0.87686
2017-12-10T14:10:04.927675: step 3352, loss 0.338473, acc 0.859375, prec 0.0609313, recall 0.87686
2017-12-10T14:10:05.117464: step 3353, loss 0.591101, acc 0.828125, prec 0.0609166, recall 0.87686
2017-12-10T14:10:05.303603: step 3354, loss 0.748735, acc 0.78125, prec 0.0608979, recall 0.87686
2017-12-10T14:10:05.491897: step 3355, loss 0.445117, acc 0.859375, prec 0.0609065, recall 0.876899
2017-12-10T14:10:05.679061: step 3356, loss 0.43926, acc 0.84375, prec 0.0609137, recall 0.876938
2017-12-10T14:10:05.867848: step 3357, loss 0.446094, acc 0.859375, prec 0.0609223, recall 0.876977
2017-12-10T14:10:06.057509: step 3358, loss 0.603783, acc 0.84375, prec 0.0609295, recall 0.877015
2017-12-10T14:10:06.244361: step 3359, loss 0.246058, acc 0.90625, prec 0.0609215, recall 0.877015
2017-12-10T14:10:06.433678: step 3360, loss 0.31954, acc 0.890625, prec 0.0609121, recall 0.877015
2017-12-10T14:10:06.623786: step 3361, loss 0.386747, acc 0.890625, prec 0.0609028, recall 0.877015
2017-12-10T14:10:06.809904: step 3362, loss 0.462819, acc 0.859375, prec 0.0608908, recall 0.877015
2017-12-10T14:10:06.998653: step 3363, loss 0.179613, acc 0.9375, prec 0.060906, recall 0.877054
2017-12-10T14:10:07.191056: step 3364, loss 0.235724, acc 0.90625, prec 0.0609186, recall 0.877093
2017-12-10T14:10:07.381715: step 3365, loss 0.415633, acc 0.921875, prec 0.0609119, recall 0.877093
2017-12-10T14:10:07.568625: step 3366, loss 0.422535, acc 0.859375, prec 0.0609205, recall 0.877132
2017-12-10T14:10:07.756476: step 3367, loss 0.189224, acc 0.953125, prec 0.0609165, recall 0.877132
2017-12-10T14:10:07.944606: step 3368, loss 0.204827, acc 0.953125, prec 0.0609125, recall 0.877132
2017-12-10T14:10:08.133261: step 3369, loss 0.101166, acc 0.96875, prec 0.0609098, recall 0.877132
2017-12-10T14:10:08.323318: step 3370, loss 0.259447, acc 0.921875, prec 0.0609237, recall 0.877171
2017-12-10T14:10:08.514793: step 3371, loss 0.393951, acc 0.9375, prec 0.060939, recall 0.87721
2017-12-10T14:10:08.704835: step 3372, loss 0.0799586, acc 0.953125, prec 0.060935, recall 0.87721
2017-12-10T14:10:08.894385: step 3373, loss 0.0721363, acc 0.96875, prec 0.0609529, recall 0.877248
2017-12-10T14:10:09.083410: step 3374, loss 0.121704, acc 0.984375, prec 0.0609927, recall 0.877326
2017-12-10T14:10:09.270511: step 3375, loss 0.499828, acc 0.984375, prec 0.0610531, recall 0.877442
2017-12-10T14:10:09.459959: step 3376, loss 1.36285, acc 0.984375, prec 0.061093, recall 0.877519
2017-12-10T14:10:09.652440: step 3377, loss 0.132238, acc 0.96875, prec 0.0610903, recall 0.877519
2017-12-10T14:10:09.840964: step 3378, loss 0.37164, acc 0.90625, prec 0.0611028, recall 0.877557
2017-12-10T14:10:10.028103: step 3379, loss 0.21113, acc 0.9375, prec 0.0610975, recall 0.877557
2017-12-10T14:10:10.217178: step 3380, loss 0.13917, acc 0.96875, prec 0.0611359, recall 0.877634
2017-12-10T14:10:10.405232: step 3381, loss 0.0247836, acc 1, prec 0.0611565, recall 0.877673
2017-12-10T14:10:10.596485: step 3382, loss 0.0986644, acc 0.96875, prec 0.0611538, recall 0.877673
2017-12-10T14:10:10.789911: step 3383, loss 0.230718, acc 0.9375, prec 0.061169, recall 0.877711
2017-12-10T14:10:10.981357: step 3384, loss 0.0797842, acc 0.953125, prec 0.061165, recall 0.877711
2017-12-10T14:10:11.169182: step 3385, loss 1.44719, acc 0.9375, prec 0.061161, recall 0.877436
2017-12-10T14:10:11.363680: step 3386, loss 0.103934, acc 1, prec 0.0612227, recall 0.877551
2017-12-10T14:10:11.557271: step 3387, loss 0.246697, acc 0.984375, prec 0.0612419, recall 0.877589
2017-12-10T14:10:11.745682: step 3388, loss 0.204571, acc 0.984375, prec 0.0612406, recall 0.877589
2017-12-10T14:10:11.936018: step 3389, loss 0.382105, acc 0.96875, prec 0.0612585, recall 0.877628
2017-12-10T14:10:12.127806: step 3390, loss 0.126657, acc 0.96875, prec 0.0612558, recall 0.877628
2017-12-10T14:10:12.315966: step 3391, loss 0.249679, acc 0.921875, prec 0.0612491, recall 0.877628
2017-12-10T14:10:12.506719: step 3392, loss 0.0923783, acc 0.953125, prec 0.0612656, recall 0.877666
2017-12-10T14:10:12.695417: step 3393, loss 0.0556464, acc 0.96875, prec 0.0612835, recall 0.877705
2017-12-10T14:10:12.887833: step 3394, loss 0.30528, acc 0.890625, prec 0.0612946, recall 0.877743
2017-12-10T14:10:13.072957: step 3395, loss 0.151002, acc 0.96875, prec 0.061333, recall 0.87782
2017-12-10T14:10:13.258841: step 3396, loss 0.109132, acc 0.953125, prec 0.0613701, recall 0.877896
2017-12-10T14:10:13.448840: step 3397, loss 0.120305, acc 0.953125, prec 0.0613661, recall 0.877896
2017-12-10T14:10:13.638134: step 3398, loss 0.348662, acc 0.875, prec 0.0613964, recall 0.877972
2017-12-10T14:10:13.835858: step 3399, loss 1.31347, acc 0.921875, prec 0.061391, recall 0.877698
2017-12-10T14:10:14.029630: step 3400, loss 0.165708, acc 0.90625, prec 0.061383, recall 0.877698
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_1/1512932334/checkpoints/model-3400

2017-12-10T14:10:15.344180: step 3401, loss 0.339965, acc 0.890625, prec 0.0613941, recall 0.877736
2017-12-10T14:10:15.533355: step 3402, loss 0.250945, acc 0.921875, prec 0.0614079, recall 0.877774
2017-12-10T14:10:15.726667: step 3403, loss 0.190823, acc 0.921875, prec 0.0614217, recall 0.877813
2017-12-10T14:10:15.914701: step 3404, loss 0.209669, acc 0.921875, prec 0.0614355, recall 0.877851
2017-12-10T14:10:16.103579: step 3405, loss 0.579003, acc 0.875, prec 0.0614248, recall 0.877851
2017-12-10T14:10:16.294798: step 3406, loss 0.961214, acc 0.8125, prec 0.0614497, recall 0.877927
2017-12-10T14:10:16.481483: step 3407, loss 0.351699, acc 0.921875, prec 0.061443, recall 0.877927
2017-12-10T14:10:16.674235: step 3408, loss 0.543874, acc 0.84375, prec 0.0614706, recall 0.878003
2017-12-10T14:10:16.865205: step 3409, loss 0.390539, acc 0.890625, prec 0.0614817, recall 0.878041
2017-12-10T14:10:17.052950: step 3410, loss 0.292165, acc 0.921875, prec 0.061475, recall 0.878041
2017-12-10T14:10:17.239111: step 3411, loss 0.478411, acc 0.859375, prec 0.0614629, recall 0.878041
2017-12-10T14:10:17.428987: step 3412, loss 0.425005, acc 0.859375, prec 0.0614508, recall 0.878041
2017-12-10T14:10:17.618826: step 3413, loss 0.152639, acc 0.953125, prec 0.0614468, recall 0.878041
2017-12-10T14:10:17.808582: step 3414, loss 0.352684, acc 0.9375, prec 0.0614414, recall 0.878041
2017-12-10T14:10:18.000125: step 3415, loss 0.26462, acc 0.921875, prec 0.0614552, recall 0.878079
2017-12-10T14:10:18.187110: step 3416, loss 0.286842, acc 0.921875, prec 0.0614895, recall 0.878155
2017-12-10T14:10:18.374981: step 3417, loss 0.330539, acc 0.90625, prec 0.0615223, recall 0.878231
2017-12-10T14:10:18.563619: step 3418, loss 0.544657, acc 0.921875, prec 0.0615771, recall 0.878345
2017-12-10T14:10:18.752976: step 3419, loss 0.14492, acc 0.9375, prec 0.0615717, recall 0.878345
2017-12-10T14:10:18.939880: step 3420, loss 0.212852, acc 0.953125, prec 0.0615881, recall 0.878383
2017-12-10T14:10:19.137285: step 3421, loss 0.420198, acc 0.859375, prec 0.061576, recall 0.878383
2017-12-10T14:10:19.322430: step 3422, loss 0.520829, acc 0.921875, prec 0.0615898, recall 0.87842
2017-12-10T14:10:19.512815: step 3423, loss 0.0979989, acc 0.953125, prec 0.0616062, recall 0.878458
2017-12-10T14:10:19.701567: step 3424, loss 0.298305, acc 0.90625, prec 0.0616595, recall 0.878571
2017-12-10T14:10:19.894355: step 3425, loss 0.198266, acc 0.953125, prec 0.0616759, recall 0.878609
2017-12-10T14:10:20.081948: step 3426, loss 0.100707, acc 0.953125, prec 0.0617128, recall 0.878684
2017-12-10T14:10:20.272072: step 3427, loss 0.365776, acc 0.90625, prec 0.0617047, recall 0.878684
2017-12-10T14:10:20.465542: step 3428, loss 0.304594, acc 0.953125, prec 0.0617211, recall 0.878722
2017-12-10T14:10:20.655703: step 3429, loss 0.00443495, acc 1, prec 0.0617211, recall 0.878722
2017-12-10T14:10:20.840650: step 3430, loss 0.122943, acc 0.984375, prec 0.0617607, recall 0.878797
2017-12-10T14:10:21.028050: step 3431, loss 0.19883, acc 0.9375, prec 0.0617553, recall 0.878797
2017-12-10T14:10:21.213694: step 3432, loss 0.166681, acc 0.9375, prec 0.0618112, recall 0.87891
2017-12-10T14:10:21.403035: step 3433, loss 0.0481398, acc 0.984375, prec 0.0618099, recall 0.87891
2017-12-10T14:10:21.589975: step 3434, loss 0.0418574, acc 0.96875, prec 0.0618072, recall 0.87891
2017-12-10T14:10:21.778463: step 3435, loss 0.285738, acc 0.9375, prec 0.0618427, recall 0.878985
2017-12-10T14:10:21.967003: step 3436, loss 11.0955, acc 0.984375, prec 0.0618631, recall 0.87875
2017-12-10T14:10:22.163058: step 3437, loss 0.033417, acc 0.984375, prec 0.0618822, recall 0.878788
2017-12-10T14:10:22.354284: step 3438, loss 0.152398, acc 0.953125, prec 0.0618781, recall 0.878788
2017-12-10T14:10:22.543650: step 3439, loss 0.0308151, acc 1, prec 0.0618781, recall 0.878788
2017-12-10T14:10:22.735182: step 3440, loss 0.230727, acc 0.9375, prec 0.0618932, recall 0.878825
2017-12-10T14:10:22.923719: step 3441, loss 0.171707, acc 0.953125, prec 0.0618891, recall 0.878825
2017-12-10T14:10:23.112450: step 3442, loss 0.30708, acc 0.9375, prec 0.0619041, recall 0.878863
2017-12-10T14:10:23.301907: step 3443, loss 0.270129, acc 0.890625, prec 0.0618947, recall 0.878863
2017-12-10T14:10:23.498524: step 3444, loss 0.20929, acc 0.953125, prec 0.0619111, recall 0.8789
2017-12-10T14:10:23.688478: step 3445, loss 0.219831, acc 0.96875, prec 0.0619288, recall 0.878938
2017-12-10T14:10:23.877880: step 3446, loss 0.455203, acc 0.875, prec 0.0619384, recall 0.878975
2017-12-10T14:10:24.065075: step 3447, loss 0.126802, acc 0.984375, prec 0.0619575, recall 0.879012
2017-12-10T14:10:24.250307: step 3448, loss 0.314495, acc 0.953125, prec 0.0619534, recall 0.879012
2017-12-10T14:10:24.440564: step 3449, loss 0.126406, acc 0.96875, prec 0.0619712, recall 0.87905
2017-12-10T14:10:24.626787: step 3450, loss 0.323031, acc 0.90625, prec 0.0619631, recall 0.87905
2017-12-10T14:10:24.815553: step 3451, loss 0.20928, acc 0.953125, prec 0.061959, recall 0.87905
2017-12-10T14:10:25.002409: step 3452, loss 0.29514, acc 0.921875, prec 0.0619931, recall 0.879124
2017-12-10T14:10:25.191176: step 3453, loss 3.78133, acc 0.890625, prec 0.061985, recall 0.878853
2017-12-10T14:10:25.385398: step 3454, loss 0.394639, acc 0.875, prec 0.0619742, recall 0.878853
2017-12-10T14:10:25.574319: step 3455, loss 1.51831, acc 0.890625, prec 0.0620069, recall 0.878657
2017-12-10T14:10:25.765564: step 3456, loss 0.551464, acc 0.828125, prec 0.0619921, recall 0.878657
2017-12-10T14:10:25.955499: step 3457, loss 0.687894, acc 0.78125, prec 0.0619732, recall 0.878657
2017-12-10T14:10:26.141596: step 3458, loss 0.715212, acc 0.8125, prec 0.0619775, recall 0.878695
2017-12-10T14:10:26.329602: step 3459, loss 0.569262, acc 0.8125, prec 0.0619613, recall 0.878695
2017-12-10T14:10:26.520040: step 3460, loss 1.0033, acc 0.8125, prec 0.0619655, recall 0.878732
2017-12-10T14:10:26.709515: step 3461, loss 0.415825, acc 0.84375, prec 0.0619724, recall 0.878769
2017-12-10T14:10:26.899890: step 3462, loss 0.976059, acc 0.734375, prec 0.0619496, recall 0.878769
2017-12-10T14:10:27.088657: step 3463, loss 0.497914, acc 0.8125, prec 0.0619538, recall 0.878807
2017-12-10T14:10:27.277395: step 3464, loss 0.525099, acc 0.828125, prec 0.0619594, recall 0.878844
2017-12-10T14:10:27.463506: step 3465, loss 0.489068, acc 0.859375, prec 0.0619473, recall 0.878844
2017-12-10T14:10:27.648779: step 3466, loss 0.321274, acc 0.953125, prec 0.0619433, recall 0.878844
2017-12-10T14:10:27.840984: step 3467, loss 0.602358, acc 0.859375, prec 0.0619515, recall 0.878881
2017-12-10T14:10:28.030726: step 3468, loss 0.842832, acc 0.859375, prec 0.0619801, recall 0.878955
2017-12-10T14:10:28.221283: step 3469, loss 0.813159, acc 0.84375, prec 0.061987, recall 0.878993
2017-12-10T14:10:28.409877: step 3470, loss 0.394372, acc 0.9375, prec 0.0620425, recall 0.879104
2017-12-10T14:10:28.598392: step 3471, loss 0.368521, acc 0.859375, prec 0.0620507, recall 0.879141
2017-12-10T14:10:28.789501: step 3472, loss 0.1945, acc 0.9375, prec 0.0620657, recall 0.879178
2017-12-10T14:10:28.987667: step 3473, loss 0.25096, acc 0.90625, prec 0.0620779, recall 0.879215
2017-12-10T14:10:29.173752: step 3474, loss 0.259278, acc 0.9375, prec 0.0620928, recall 0.879252
2017-12-10T14:10:29.361266: step 3475, loss 0.137033, acc 0.953125, prec 0.0621091, recall 0.879289
2017-12-10T14:10:29.550396: step 3476, loss 0.219422, acc 0.9375, prec 0.0621037, recall 0.879289
2017-12-10T14:10:29.739737: step 3477, loss 0.300282, acc 0.90625, prec 0.0620957, recall 0.879289
2017-12-10T14:10:29.926984: step 3478, loss 0.557752, acc 0.921875, prec 0.0621701, recall 0.879437
2017-12-10T14:10:30.098912: step 3479, loss 0.37792, acc 0.882353, prec 0.0621823, recall 0.879474
2017-12-10T14:10:30.294336: step 3480, loss 0.211543, acc 0.953125, prec 0.0621783, recall 0.879474
2017-12-10T14:10:30.487614: step 3481, loss 0.104744, acc 0.984375, prec 0.0622581, recall 0.879621
2017-12-10T14:10:30.676559: step 3482, loss 0.0783083, acc 0.984375, prec 0.0622973, recall 0.879695
2017-12-10T14:10:30.866935: step 3483, loss 0.0483817, acc 0.96875, prec 0.0623351, recall 0.879768
2017-12-10T14:10:31.056598: step 3484, loss 0.355291, acc 0.96875, prec 0.0623527, recall 0.879805
2017-12-10T14:10:31.250342: step 3485, loss 1.01645, acc 0.921875, prec 0.0623662, recall 0.879841
2017-12-10T14:10:31.445118: step 3486, loss 0.144795, acc 0.9375, prec 0.0623608, recall 0.879841
2017-12-10T14:10:31.637409: step 3487, loss 0.937522, acc 0.953125, prec 0.0624379, recall 0.879988
2017-12-10T14:10:31.829563: step 3488, loss 0.0945851, acc 0.984375, prec 0.0625176, recall 0.880134
2017-12-10T14:10:32.023143: step 3489, loss 0.282818, acc 0.953125, prec 0.062554, recall 0.880207
2017-12-10T14:10:32.216643: step 3490, loss 0.206704, acc 0.953125, prec 0.0625905, recall 0.88028
2017-12-10T14:10:32.410443: step 3491, loss 0.0731262, acc 0.96875, prec 0.0625878, recall 0.88028
2017-12-10T14:10:32.602338: step 3492, loss 0.146112, acc 0.96875, prec 0.0626661, recall 0.880425
2017-12-10T14:10:32.788211: step 3493, loss 0.220331, acc 0.9375, prec 0.0626606, recall 0.880425
2017-12-10T14:10:32.975532: step 3494, loss 0.0851476, acc 0.96875, prec 0.0626579, recall 0.880425
2017-12-10T14:10:33.168510: step 3495, loss 0.0483475, acc 0.984375, prec 0.0626768, recall 0.880461
2017-12-10T14:10:33.353443: step 3496, loss 0.223617, acc 0.96875, prec 0.0626944, recall 0.880497
2017-12-10T14:10:33.547073: step 3497, loss 0.324398, acc 0.90625, prec 0.0627065, recall 0.880534
2017-12-10T14:10:33.737432: step 3498, loss 0.265452, acc 0.96875, prec 0.0627038, recall 0.880534
2017-12-10T14:10:33.932488: step 3499, loss 0.0531593, acc 0.984375, prec 0.0627024, recall 0.880534
2017-12-10T14:10:34.116966: step 3500, loss 0.148832, acc 0.96875, prec 0.0626997, recall 0.880534
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_1/1512932334/checkpoints/model-3500

2017-12-10T14:10:35.419116: step 3501, loss 0.374875, acc 0.921875, prec 0.0626929, recall 0.880534
2017-12-10T14:10:35.610807: step 3502, loss 0.242886, acc 0.9375, prec 0.0626875, recall 0.880534
2017-12-10T14:10:35.797897: step 3503, loss 0.0820179, acc 0.96875, prec 0.0627051, recall 0.88057
2017-12-10T14:10:35.989106: step 3504, loss 0.284312, acc 0.875, prec 0.0626942, recall 0.88057
2017-12-10T14:10:36.178586: step 3505, loss 0.222543, acc 0.9375, prec 0.0627495, recall 0.880678
2017-12-10T14:10:36.367548: step 3506, loss 0.113734, acc 0.96875, prec 0.062767, recall 0.880714
2017-12-10T14:10:36.557521: step 3507, loss 1.17851, acc 0.921875, prec 0.0627818, recall 0.880484
2017-12-10T14:10:36.746734: step 3508, loss 0.338134, acc 0.90625, prec 0.0627737, recall 0.880484
2017-12-10T14:10:36.938025: step 3509, loss 0.399827, acc 0.90625, prec 0.062806, recall 0.880556
2017-12-10T14:10:37.129525: step 3510, loss 0.132915, acc 0.953125, prec 0.0628221, recall 0.880593
2017-12-10T14:10:37.317473: step 3511, loss 0.293216, acc 0.90625, prec 0.0628746, recall 0.880701
2017-12-10T14:10:37.508976: step 3512, loss 0.13546, acc 0.921875, prec 0.0628679, recall 0.880701
2017-12-10T14:10:37.694423: step 3513, loss 0.400814, acc 0.890625, prec 0.0628786, recall 0.880737
2017-12-10T14:10:37.880748: step 3514, loss 0.585587, acc 0.84375, prec 0.0628852, recall 0.880773
2017-12-10T14:10:38.068739: step 3515, loss 0.127513, acc 0.96875, prec 0.0628825, recall 0.880773
2017-12-10T14:10:38.261026: step 3516, loss 0.0488538, acc 0.984375, prec 0.0629013, recall 0.880809
2017-12-10T14:10:38.453483: step 3517, loss 0.174186, acc 0.90625, prec 0.0629134, recall 0.880845
2017-12-10T14:10:38.644000: step 3518, loss 0.329906, acc 0.9375, prec 0.062908, recall 0.880845
2017-12-10T14:10:38.831393: step 3519, loss 0.05861, acc 0.984375, prec 0.0629066, recall 0.880845
2017-12-10T14:10:39.018662: step 3520, loss 0.132937, acc 0.953125, prec 0.0629026, recall 0.880845
2017-12-10T14:10:39.210230: step 3521, loss 0.132151, acc 0.96875, prec 0.0628999, recall 0.880845
2017-12-10T14:10:39.405893: step 3522, loss 0.16514, acc 0.953125, prec 0.062916, recall 0.880881
2017-12-10T14:10:39.593980: step 3523, loss 0.134775, acc 0.953125, prec 0.0629523, recall 0.880952
2017-12-10T14:10:39.780104: step 3524, loss 0.308962, acc 0.96875, prec 0.0629899, recall 0.881024
2017-12-10T14:10:39.975219: step 3525, loss 0.21126, acc 0.953125, prec 0.063006, recall 0.88106
2017-12-10T14:10:40.166309: step 3526, loss 1.31152, acc 0.96875, prec 0.0630248, recall 0.880831
2017-12-10T14:10:40.359956: step 3527, loss 0.079475, acc 0.953125, prec 0.0630208, recall 0.880831
2017-12-10T14:10:40.551791: step 3528, loss 3.68784, acc 0.9375, prec 0.0630974, recall 0.880709
2017-12-10T14:10:40.742474: step 3529, loss 0.268646, acc 0.921875, prec 0.0631108, recall 0.880745
2017-12-10T14:10:40.932266: step 3530, loss 0.203446, acc 0.90625, prec 0.0631429, recall 0.880817
2017-12-10T14:10:41.125074: step 3531, loss 0.343826, acc 0.921875, prec 0.0631362, recall 0.880817
2017-12-10T14:10:41.314787: step 3532, loss 0.425974, acc 0.875, prec 0.0631454, recall 0.880852
2017-12-10T14:10:41.500801: step 3533, loss 0.439023, acc 0.90625, prec 0.0631373, recall 0.880852
2017-12-10T14:10:41.691354: step 3534, loss 0.466917, acc 0.859375, prec 0.0631251, recall 0.880852
2017-12-10T14:10:41.880065: step 3535, loss 0.295565, acc 0.90625, prec 0.0631371, recall 0.880888
2017-12-10T14:10:42.071350: step 3536, loss 0.495465, acc 0.890625, prec 0.0631276, recall 0.880888
2017-12-10T14:10:42.256366: step 3537, loss 0.631509, acc 0.8125, prec 0.0631516, recall 0.88096
2017-12-10T14:10:42.449967: step 3538, loss 0.831374, acc 0.78125, prec 0.0631326, recall 0.88096
2017-12-10T14:10:42.635606: step 3539, loss 0.345518, acc 0.890625, prec 0.0631834, recall 0.881067
2017-12-10T14:10:42.820849: step 3540, loss 0.523878, acc 0.859375, prec 0.0632115, recall 0.881138
2017-12-10T14:10:43.009582: step 3541, loss 0.500332, acc 0.828125, prec 0.0632368, recall 0.881209
2017-12-10T14:10:43.195995: step 3542, loss 0.287218, acc 0.90625, prec 0.0632487, recall 0.881244
2017-12-10T14:10:43.383554: step 3543, loss 0.635789, acc 0.859375, prec 0.0632566, recall 0.88128
2017-12-10T14:10:43.571767: step 3544, loss 0.445078, acc 0.84375, prec 0.0632632, recall 0.881315
2017-12-10T14:10:43.759559: step 3545, loss 0.411096, acc 0.90625, prec 0.063255, recall 0.881315
2017-12-10T14:10:43.953686: step 3546, loss 0.289923, acc 0.90625, prec 0.0632469, recall 0.881315
2017-12-10T14:10:44.142055: step 3547, loss 0.313846, acc 0.890625, prec 0.0632575, recall 0.881351
2017-12-10T14:10:44.331370: step 3548, loss 0.342772, acc 0.859375, prec 0.0632453, recall 0.881351
2017-12-10T14:10:44.523365: step 3549, loss 0.233825, acc 0.90625, prec 0.0632974, recall 0.881457
2017-12-10T14:10:44.710674: step 3550, loss 0.120089, acc 0.96875, prec 0.0632947, recall 0.881457
2017-12-10T14:10:44.900906: step 3551, loss 0.390068, acc 0.921875, prec 0.063308, recall 0.881493
2017-12-10T14:10:45.091536: step 3552, loss 0.474386, acc 0.84375, prec 0.0633546, recall 0.881599
2017-12-10T14:10:45.283723: step 3553, loss 0.192986, acc 0.90625, prec 0.0633465, recall 0.881599
2017-12-10T14:10:45.476660: step 3554, loss 0.0709785, acc 0.984375, prec 0.0633652, recall 0.881634
2017-12-10T14:10:45.664266: step 3555, loss 0.133235, acc 0.9375, prec 0.0633798, recall 0.881669
2017-12-10T14:10:45.856333: step 3556, loss 0.475523, acc 0.953125, prec 0.0634159, recall 0.88174
2017-12-10T14:10:46.050144: step 3557, loss 0.0172956, acc 0.984375, prec 0.0634145, recall 0.88174
2017-12-10T14:10:46.240735: step 3558, loss 0.430565, acc 0.921875, prec 0.0634077, recall 0.88174
2017-12-10T14:10:46.432097: step 3559, loss 0.175373, acc 0.96875, prec 0.0634451, recall 0.88181
2017-12-10T14:10:46.621484: step 3560, loss 0.0932251, acc 0.984375, prec 0.0634638, recall 0.881845
2017-12-10T14:10:46.807237: step 3561, loss 0.109933, acc 0.984375, prec 0.0634625, recall 0.881845
2017-12-10T14:10:46.994780: step 3562, loss 0.0775896, acc 0.984375, prec 0.0634611, recall 0.881845
2017-12-10T14:10:47.186818: step 3563, loss 0.213592, acc 1, prec 0.0635012, recall 0.881916
2017-12-10T14:10:47.379803: step 3564, loss 0.0957771, acc 0.984375, prec 0.0634999, recall 0.881916
2017-12-10T14:10:47.567767: step 3565, loss 0.0646882, acc 0.984375, prec 0.0635186, recall 0.881951
2017-12-10T14:10:47.760121: step 3566, loss 0.193207, acc 0.90625, prec 0.0635104, recall 0.881951
2017-12-10T14:10:47.947116: step 3567, loss 0.0232755, acc 1, prec 0.0635104, recall 0.881951
2017-12-10T14:10:48.136814: step 3568, loss 0.0122614, acc 1, prec 0.0635104, recall 0.881951
2017-12-10T14:10:48.327478: step 3569, loss 0.0311812, acc 0.984375, prec 0.0635091, recall 0.881951
2017-12-10T14:10:48.520427: step 3570, loss 0.0188911, acc 1, prec 0.0635091, recall 0.881951
2017-12-10T14:10:48.707468: step 3571, loss 1.76197, acc 0.96875, prec 0.0635077, recall 0.881688
2017-12-10T14:10:48.904771: step 3572, loss 0.00938266, acc 1, prec 0.0635077, recall 0.881688
2017-12-10T14:10:49.095024: step 3573, loss 0.0306503, acc 1, prec 0.0635478, recall 0.881759
2017-12-10T14:10:49.283913: step 3574, loss 0.444746, acc 0.921875, prec 0.063541, recall 0.881759
2017-12-10T14:10:49.470951: step 3575, loss 0.334423, acc 0.953125, prec 0.063577, recall 0.881829
2017-12-10T14:10:49.661539: step 3576, loss 0.118395, acc 0.96875, prec 0.0635943, recall 0.881864
2017-12-10T14:10:49.851531: step 3577, loss 2.29985, acc 0.96875, prec 0.063593, recall 0.881602
2017-12-10T14:10:50.044937: step 3578, loss 0.14406, acc 0.953125, prec 0.0636089, recall 0.881638
2017-12-10T14:10:50.238879: step 3579, loss 0.234648, acc 0.90625, prec 0.0636208, recall 0.881673
2017-12-10T14:10:50.427541: step 3580, loss 0.0792415, acc 0.96875, prec 0.0636181, recall 0.881673
2017-12-10T14:10:50.617142: step 3581, loss 0.305062, acc 0.90625, prec 0.0636299, recall 0.881708
2017-12-10T14:10:50.806922: step 3582, loss 0.166528, acc 0.90625, prec 0.0636218, recall 0.881708
2017-12-10T14:10:50.992943: step 3583, loss 1.8983, acc 0.828125, prec 0.0636082, recall 0.881446
2017-12-10T14:10:51.181880: step 3584, loss 0.216754, acc 0.890625, prec 0.0635986, recall 0.881446
2017-12-10T14:10:51.371624: step 3585, loss 0.332592, acc 0.875, prec 0.0635878, recall 0.881446
2017-12-10T14:10:51.558943: step 3586, loss 4.41571, acc 0.796875, prec 0.0635915, recall 0.88122
2017-12-10T14:10:51.751736: step 3587, loss 0.406431, acc 0.859375, prec 0.0636793, recall 0.881396
2017-12-10T14:10:51.937776: step 3588, loss 0.768244, acc 0.78125, prec 0.0636803, recall 0.881431
2017-12-10T14:10:52.135130: step 3589, loss 1.61311, acc 0.671875, prec 0.0636517, recall 0.881431
2017-12-10T14:10:52.320996: step 3590, loss 0.9568, acc 0.75, prec 0.0636699, recall 0.881501
2017-12-10T14:10:52.506417: step 3591, loss 1.02388, acc 0.828125, prec 0.063675, recall 0.881536
2017-12-10T14:10:52.693309: step 3592, loss 0.81824, acc 0.796875, prec 0.0636773, recall 0.881571
2017-12-10T14:10:52.885424: step 3593, loss 1.40687, acc 0.671875, prec 0.0636488, recall 0.881571
2017-12-10T14:10:53.079072: step 3594, loss 1.12312, acc 0.734375, prec 0.0636257, recall 0.881571
2017-12-10T14:10:53.267784: step 3595, loss 0.681153, acc 0.8125, prec 0.0636294, recall 0.881606
2017-12-10T14:10:53.454521: step 3596, loss 0.556616, acc 0.828125, prec 0.0636344, recall 0.881641
2017-12-10T14:10:53.647758: step 3597, loss 0.447273, acc 0.828125, prec 0.0636793, recall 0.881746
2017-12-10T14:10:53.839127: step 3598, loss 0.91192, acc 0.796875, prec 0.0636617, recall 0.881746
2017-12-10T14:10:54.024532: step 3599, loss 1.0404, acc 0.8125, prec 0.0636853, recall 0.881815
2017-12-10T14:10:54.216728: step 3600, loss 0.252604, acc 0.90625, prec 0.0636971, recall 0.88185
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_1/1512932334/checkpoints/model-3600

2017-12-10T14:10:55.505229: step 3601, loss 0.70436, acc 0.828125, prec 0.0637021, recall 0.881885
2017-12-10T14:10:55.694267: step 3602, loss 1.07261, acc 0.828125, prec 0.0637071, recall 0.88192
2017-12-10T14:10:55.883557: step 3603, loss 0.561534, acc 0.84375, prec 0.0637533, recall 0.882024
2017-12-10T14:10:56.074556: step 3604, loss 0.272947, acc 0.90625, prec 0.0637651, recall 0.882059
2017-12-10T14:10:56.265783: step 3605, loss 0.0695231, acc 0.984375, prec 0.0637637, recall 0.882059
2017-12-10T14:10:56.451095: step 3606, loss 0.255478, acc 0.875, prec 0.0637728, recall 0.882093
2017-12-10T14:10:56.639708: step 3607, loss 0.199694, acc 0.953125, prec 0.0637886, recall 0.882128
2017-12-10T14:10:56.831336: step 3608, loss 0.0709939, acc 0.96875, prec 0.0637859, recall 0.882128
2017-12-10T14:10:57.017648: step 3609, loss 0.151244, acc 0.953125, prec 0.0638217, recall 0.882197
2017-12-10T14:10:57.206667: step 3610, loss 0.0756836, acc 0.96875, prec 0.0638189, recall 0.882197
2017-12-10T14:10:57.396223: step 3611, loss 0.192613, acc 0.953125, prec 0.0638547, recall 0.882267
2017-12-10T14:10:57.583633: step 3612, loss 0.175879, acc 0.953125, prec 0.0638705, recall 0.882301
2017-12-10T14:10:57.771390: step 3613, loss 0.21589, acc 0.984375, prec 0.0639288, recall 0.882405
2017-12-10T14:10:57.961379: step 3614, loss 0.0434553, acc 0.984375, prec 0.0639274, recall 0.882405
2017-12-10T14:10:58.147628: step 3615, loss 0.338995, acc 0.890625, prec 0.0639378, recall 0.882439
2017-12-10T14:10:58.336336: step 3616, loss 1.66909, acc 0.953125, prec 0.063955, recall 0.882215
2017-12-10T14:10:58.530120: step 3617, loss 0.63219, acc 0.984375, prec 0.0639934, recall 0.882284
2017-12-10T14:10:58.727449: step 3618, loss 0.1657, acc 0.953125, prec 0.0639893, recall 0.882284
2017-12-10T14:10:58.925486: step 3619, loss 0.45826, acc 0.921875, prec 0.0640024, recall 0.882318
2017-12-10T14:10:59.116253: step 3620, loss 0.377852, acc 0.9375, prec 0.0640367, recall 0.882387
2017-12-10T14:10:59.307039: step 3621, loss 0.153544, acc 0.953125, prec 0.0640724, recall 0.882456
2017-12-10T14:10:59.504221: step 3622, loss 0.234607, acc 0.96875, prec 0.0641094, recall 0.882525
2017-12-10T14:10:59.699047: step 3623, loss 0.23007, acc 0.9375, prec 0.0641039, recall 0.882525
2017-12-10T14:10:59.886590: step 3624, loss 0.0559856, acc 0.984375, prec 0.0641026, recall 0.882525
2017-12-10T14:11:00.081723: step 3625, loss 0.0477122, acc 0.984375, prec 0.0641012, recall 0.882525
2017-12-10T14:11:00.274214: step 3626, loss 0.282667, acc 0.90625, prec 0.0641129, recall 0.882559
2017-12-10T14:11:00.468723: step 3627, loss 0.215818, acc 0.9375, prec 0.0641273, recall 0.882593
2017-12-10T14:11:00.657287: step 3628, loss 0.134103, acc 0.953125, prec 0.0641431, recall 0.882628
2017-12-10T14:11:00.845987: step 3629, loss 0.36027, acc 0.890625, prec 0.064213, recall 0.882765
2017-12-10T14:11:01.041925: step 3630, loss 0.065741, acc 0.984375, prec 0.0642315, recall 0.882799
2017-12-10T14:11:01.231305: step 3631, loss 0.129278, acc 0.953125, prec 0.0642274, recall 0.882799
2017-12-10T14:11:01.422787: step 3632, loss 0.134032, acc 0.90625, prec 0.0642192, recall 0.882799
2017-12-10T14:11:01.613021: step 3633, loss 0.0266322, acc 1, prec 0.0642192, recall 0.882799
2017-12-10T14:11:01.802896: step 3634, loss 0.0880757, acc 0.96875, prec 0.0642363, recall 0.882833
2017-12-10T14:11:01.991115: step 3635, loss 0.337663, acc 0.890625, prec 0.0642268, recall 0.882833
2017-12-10T14:11:02.178024: step 3636, loss 0.0635678, acc 0.96875, prec 0.0642241, recall 0.882833
2017-12-10T14:11:02.366020: step 3637, loss 0.140737, acc 0.984375, prec 0.0642425, recall 0.882867
2017-12-10T14:11:02.559957: step 3638, loss 0.233767, acc 0.953125, prec 0.0642781, recall 0.882935
2017-12-10T14:11:02.749638: step 3639, loss 0.177665, acc 0.921875, prec 0.0642713, recall 0.882935
2017-12-10T14:11:02.936953: step 3640, loss 0.162781, acc 0.96875, prec 0.0642686, recall 0.882935
2017-12-10T14:11:03.128321: step 3641, loss 0.339428, acc 1, prec 0.0643281, recall 0.883038
2017-12-10T14:11:03.320585: step 3642, loss 0.019303, acc 1, prec 0.0643479, recall 0.883072
2017-12-10T14:11:03.506614: step 3643, loss 0.0692798, acc 0.953125, prec 0.0643637, recall 0.883106
2017-12-10T14:11:03.699036: step 3644, loss 0.184471, acc 0.953125, prec 0.0643794, recall 0.88314
2017-12-10T14:11:03.889124: step 3645, loss 0.128738, acc 0.953125, prec 0.0643951, recall 0.883174
2017-12-10T14:11:04.076080: step 3646, loss 0.192091, acc 0.9375, prec 0.0644095, recall 0.883207
2017-12-10T14:11:04.267455: step 3647, loss 0.209782, acc 0.9375, prec 0.0644635, recall 0.883309
2017-12-10T14:11:04.463831: step 3648, loss 0.0818499, acc 0.984375, prec 0.0644621, recall 0.883309
2017-12-10T14:11:04.654920: step 3649, loss 5.26011, acc 0.953125, prec 0.0644608, recall 0.882797
2017-12-10T14:11:04.848890: step 3650, loss 0.05643, acc 0.96875, prec 0.0644779, recall 0.882831
2017-12-10T14:11:05.034198: step 3651, loss 0.21455, acc 0.9375, prec 0.0644922, recall 0.882865
2017-12-10T14:11:05.222370: step 3652, loss 0.190534, acc 0.953125, prec 0.0644881, recall 0.882865
2017-12-10T14:11:05.413538: step 3653, loss 0.166787, acc 0.921875, prec 0.0645407, recall 0.882966
2017-12-10T14:11:05.599050: step 3654, loss 0.413336, acc 0.9375, prec 0.0645551, recall 0.883
2017-12-10T14:11:05.788211: step 3655, loss 0.324442, acc 0.890625, prec 0.0645455, recall 0.883
2017-12-10T14:11:05.974191: step 3656, loss 0.676541, acc 0.875, prec 0.0645742, recall 0.883068
2017-12-10T14:11:06.161873: step 3657, loss 0.560154, acc 0.859375, prec 0.0646015, recall 0.883136
2017-12-10T14:11:06.350973: step 3658, loss 0.499899, acc 0.875, prec 0.0645905, recall 0.883136
2017-12-10T14:11:06.540407: step 3659, loss 0.530945, acc 0.84375, prec 0.0646164, recall 0.883203
2017-12-10T14:11:06.734642: step 3660, loss 0.394312, acc 0.84375, prec 0.0646028, recall 0.883203
2017-12-10T14:11:06.923065: step 3661, loss 0.432138, acc 0.875, prec 0.0646314, recall 0.883271
2017-12-10T14:11:07.113114: step 3662, loss 0.758704, acc 0.78125, prec 0.0646123, recall 0.883271
2017-12-10T14:11:07.299491: step 3663, loss 0.566545, acc 0.765625, prec 0.0646115, recall 0.883304
2017-12-10T14:11:07.486954: step 3664, loss 0.606353, acc 0.875, prec 0.0646401, recall 0.883372
2017-12-10T14:11:07.674807: step 3665, loss 0.664167, acc 0.828125, prec 0.0646251, recall 0.883372
2017-12-10T14:11:07.861999: step 3666, loss 0.255482, acc 0.921875, prec 0.0646183, recall 0.883372
2017-12-10T14:11:08.051870: step 3667, loss 0.60031, acc 0.859375, prec 0.064606, recall 0.883372
2017-12-10T14:11:08.238009: step 3668, loss 0.428027, acc 0.859375, prec 0.0645938, recall 0.883372
2017-12-10T14:11:08.425717: step 3669, loss 0.56536, acc 0.875, prec 0.0646026, recall 0.883406
2017-12-10T14:11:08.617330: step 3670, loss 0.299705, acc 0.890625, prec 0.0646128, recall 0.883439
2017-12-10T14:11:08.803490: step 3671, loss 0.234309, acc 0.90625, prec 0.0646046, recall 0.883439
2017-12-10T14:11:08.992025: step 3672, loss 0.449757, acc 0.921875, prec 0.0645978, recall 0.883439
2017-12-10T14:11:09.189631: step 3673, loss 0.563978, acc 0.890625, prec 0.0646277, recall 0.883506
2017-12-10T14:11:09.382073: step 3674, loss 0.172208, acc 0.9375, prec 0.0646223, recall 0.883506
2017-12-10T14:11:09.572494: step 3675, loss 0.141955, acc 0.921875, prec 0.0646352, recall 0.88354
2017-12-10T14:11:09.763557: step 3676, loss 0.0979073, acc 0.9375, prec 0.0646692, recall 0.883607
2017-12-10T14:11:09.951719: step 3677, loss 0.294697, acc 0.953125, prec 0.0646848, recall 0.883641
2017-12-10T14:11:10.140373: step 3678, loss 0.361737, acc 1, prec 0.0647045, recall 0.883674
2017-12-10T14:11:10.330418: step 3679, loss 0.0717044, acc 0.984375, prec 0.0647426, recall 0.883741
2017-12-10T14:11:10.521813: step 3680, loss 0.0592958, acc 0.96875, prec 0.0647399, recall 0.883741
2017-12-10T14:11:10.711579: step 3681, loss 0.112598, acc 0.96875, prec 0.0647568, recall 0.883774
2017-12-10T14:11:10.903946: step 3682, loss 0.00385096, acc 1, prec 0.0647766, recall 0.883808
2017-12-10T14:11:11.096565: step 3683, loss 0.0217005, acc 0.984375, prec 0.0647752, recall 0.883808
2017-12-10T14:11:11.288525: step 3684, loss 0.0382778, acc 0.984375, prec 0.0647738, recall 0.883808
2017-12-10T14:11:11.476835: step 3685, loss 0.343243, acc 0.921875, prec 0.0647867, recall 0.883841
2017-12-10T14:11:11.665034: step 3686, loss 0.167438, acc 0.984375, prec 0.0648051, recall 0.883875
2017-12-10T14:11:11.856394: step 3687, loss 0.057618, acc 0.984375, prec 0.0648037, recall 0.883875
2017-12-10T14:11:12.047136: step 3688, loss 0.369007, acc 0.96875, prec 0.0648404, recall 0.883941
2017-12-10T14:11:12.237443: step 3689, loss 0.12695, acc 0.96875, prec 0.0648573, recall 0.883975
2017-12-10T14:11:12.430739: step 3690, loss 0.70764, acc 1, prec 0.064877, recall 0.884008
2017-12-10T14:11:12.620119: step 3691, loss 0.592741, acc 0.96875, prec 0.0649137, recall 0.884075
2017-12-10T14:11:12.813293: step 3692, loss 0.106755, acc 0.984375, prec 0.0649321, recall 0.884108
2017-12-10T14:11:13.003325: step 3693, loss 0.267808, acc 0.953125, prec 0.064928, recall 0.884108
2017-12-10T14:11:13.192186: step 3694, loss 0.114849, acc 0.953125, prec 0.0649238, recall 0.884108
2017-12-10T14:11:13.383172: step 3695, loss 0.191505, acc 0.9375, prec 0.0649381, recall 0.884141
2017-12-10T14:11:13.572341: step 3696, loss 0.0510272, acc 0.96875, prec 0.0649353, recall 0.884141
2017-12-10T14:11:13.764722: step 3697, loss 0.132628, acc 0.953125, prec 0.0649312, recall 0.884141
2017-12-10T14:11:13.961175: step 3698, loss 0.0648572, acc 0.984375, prec 0.0649496, recall 0.884174
2017-12-10T14:11:14.148453: step 3699, loss 0.0812272, acc 0.984375, prec 0.0649482, recall 0.884174
2017-12-10T14:11:14.336953: step 3700, loss 0.216877, acc 0.9375, prec 0.0649821, recall 0.884241
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_1/1512932334/checkpoints/model-3700

2017-12-10T14:11:15.786179: step 3701, loss 0.038861, acc 0.984375, prec 0.0649807, recall 0.884241
2017-12-10T14:11:15.978303: step 3702, loss 0.110978, acc 0.953125, prec 0.0649963, recall 0.884274
2017-12-10T14:11:16.180203: step 3703, loss 0.135764, acc 0.9375, prec 0.0649908, recall 0.884274
2017-12-10T14:11:16.369356: step 3704, loss 0.575957, acc 0.953125, prec 0.0650261, recall 0.88434
2017-12-10T14:11:16.557645: step 3705, loss 0.416301, acc 0.921875, prec 0.0650389, recall 0.884373
2017-12-10T14:11:16.741519: step 3706, loss 0.109015, acc 0.984375, prec 0.0650376, recall 0.884373
2017-12-10T14:11:16.937887: step 3707, loss 0.232413, acc 0.9375, prec 0.0650321, recall 0.884373
2017-12-10T14:11:17.131618: step 3708, loss 0.328561, acc 0.875, prec 0.0650408, recall 0.884406
2017-12-10T14:11:17.322618: step 3709, loss 0.154354, acc 0.9375, prec 0.0650354, recall 0.884406
2017-12-10T14:11:17.508072: step 3710, loss 0.172114, acc 0.921875, prec 0.0650285, recall 0.884406
2017-12-10T14:11:17.702004: step 3711, loss 1.91318, acc 0.9375, prec 0.0650441, recall 0.884186
2017-12-10T14:11:17.893548: step 3712, loss 1.01956, acc 0.96875, prec 0.0650807, recall 0.884253
2017-12-10T14:11:18.085275: step 3713, loss 0.135061, acc 0.953125, prec 0.0650766, recall 0.884253
2017-12-10T14:11:18.275632: step 3714, loss 0.26204, acc 0.890625, prec 0.0650866, recall 0.884286
2017-12-10T14:11:18.466877: step 3715, loss 0.331198, acc 0.90625, prec 0.0650981, recall 0.884319
2017-12-10T14:11:18.655303: step 3716, loss 0.213798, acc 0.890625, prec 0.0650885, recall 0.884319
2017-12-10T14:11:18.842435: step 3717, loss 0.233568, acc 0.90625, prec 0.0650803, recall 0.884319
2017-12-10T14:11:19.032691: step 3718, loss 0.327306, acc 0.90625, prec 0.0650917, recall 0.884352
2017-12-10T14:11:19.223453: step 3719, loss 0.158345, acc 0.90625, prec 0.0650835, recall 0.884352
2017-12-10T14:11:19.409770: step 3720, loss 0.267927, acc 0.921875, prec 0.0650767, recall 0.884352
2017-12-10T14:11:19.597742: step 3721, loss 0.30392, acc 0.9375, prec 0.0650909, recall 0.884385
2017-12-10T14:11:19.786654: step 3722, loss 0.63197, acc 0.84375, prec 0.0650968, recall 0.884418
2017-12-10T14:11:19.978696: step 3723, loss 0.299323, acc 0.921875, prec 0.06509, recall 0.884418
2017-12-10T14:11:20.171460: step 3724, loss 0.0850308, acc 0.96875, prec 0.0650873, recall 0.884418
2017-12-10T14:11:20.361768: step 3725, loss 0.202121, acc 0.9375, prec 0.0651211, recall 0.884484
2017-12-10T14:11:20.549765: step 3726, loss 0.281511, acc 0.921875, prec 0.0651142, recall 0.884484
2017-12-10T14:11:20.739188: step 3727, loss 0.2047, acc 0.9375, prec 0.0651284, recall 0.884517
2017-12-10T14:11:20.923702: step 3728, loss 0.321206, acc 0.921875, prec 0.0651216, recall 0.884517
2017-12-10T14:11:21.112677: step 3729, loss 0.350013, acc 0.921875, prec 0.0651343, recall 0.88455
2017-12-10T14:11:21.298452: step 3730, loss 0.371346, acc 0.953125, prec 0.0651499, recall 0.884583
2017-12-10T14:11:21.488588: step 3731, loss 0.126156, acc 0.9375, prec 0.0651836, recall 0.884648
2017-12-10T14:11:21.677197: step 3732, loss 1.13535, acc 0.890625, prec 0.0652133, recall 0.884714
2017-12-10T14:11:21.865297: step 3733, loss 0.335978, acc 0.921875, prec 0.0652064, recall 0.884714
2017-12-10T14:11:22.049958: step 3734, loss 0.191122, acc 0.953125, prec 0.0652612, recall 0.884812
2017-12-10T14:11:22.239682: step 3735, loss 0.0299729, acc 0.984375, prec 0.0652794, recall 0.884845
2017-12-10T14:11:22.431013: step 3736, loss 0.212542, acc 0.90625, prec 0.0652712, recall 0.884845
2017-12-10T14:11:22.623492: step 3737, loss 0.155966, acc 0.96875, prec 0.0653077, recall 0.88491
2017-12-10T14:11:22.815859: step 3738, loss 0.34063, acc 0.90625, prec 0.065319, recall 0.884943
2017-12-10T14:11:23.007294: step 3739, loss 0.32889, acc 0.90625, prec 0.0653304, recall 0.884976
2017-12-10T14:11:23.197602: step 3740, loss 0.0743706, acc 0.96875, prec 0.0653473, recall 0.885009
2017-12-10T14:11:23.382986: step 3741, loss 0.096969, acc 0.953125, prec 0.0653824, recall 0.885074
2017-12-10T14:11:23.574531: step 3742, loss 0.155545, acc 0.984375, prec 0.0654006, recall 0.885106
2017-12-10T14:11:23.760613: step 3743, loss 0.755559, acc 0.96875, prec 0.0654174, recall 0.885139
2017-12-10T14:11:23.953509: step 3744, loss 1.24889, acc 0.953125, prec 0.0654539, recall 0.884953
2017-12-10T14:11:24.150797: step 3745, loss 0.120372, acc 0.984375, prec 0.0654917, recall 0.885018
2017-12-10T14:11:24.342051: step 3746, loss 0.125014, acc 0.953125, prec 0.0655071, recall 0.885051
2017-12-10T14:11:24.532777: step 3747, loss 0.223277, acc 0.953125, prec 0.0655226, recall 0.885083
2017-12-10T14:11:24.722802: step 3748, loss 0.112607, acc 0.953125, prec 0.0655185, recall 0.885083
2017-12-10T14:11:24.913444: step 3749, loss 0.203804, acc 0.90625, prec 0.0655102, recall 0.885083
2017-12-10T14:11:25.105422: step 3750, loss 0.122867, acc 0.9375, prec 0.0655047, recall 0.885083
2017-12-10T14:11:25.293154: step 3751, loss 0.263317, acc 0.90625, prec 0.0654965, recall 0.885083
2017-12-10T14:11:25.488251: step 3752, loss 0.251826, acc 0.921875, prec 0.0654897, recall 0.885083
2017-12-10T14:11:25.678824: step 3753, loss 0.150713, acc 0.9375, prec 0.0655233, recall 0.885149
2017-12-10T14:11:25.868951: step 3754, loss 0.172292, acc 0.921875, prec 0.0655751, recall 0.885246
2017-12-10T14:11:26.061342: step 3755, loss 0.36677, acc 0.84375, prec 0.0656005, recall 0.885311
2017-12-10T14:11:26.248150: step 3756, loss 0.0270858, acc 0.984375, prec 0.0656383, recall 0.885375
2017-12-10T14:11:26.436979: step 3757, loss 0.144003, acc 0.96875, prec 0.0656746, recall 0.88544
2017-12-10T14:11:26.631218: step 3758, loss 0.257986, acc 0.9375, prec 0.0656691, recall 0.88544
2017-12-10T14:11:26.819341: step 3759, loss 0.1374, acc 0.953125, prec 0.0657432, recall 0.885569
2017-12-10T14:11:27.006000: step 3760, loss 0.141994, acc 0.9375, prec 0.0657377, recall 0.885569
2017-12-10T14:11:27.193363: step 3761, loss 0.421069, acc 0.921875, prec 0.0657504, recall 0.885602
2017-12-10T14:11:27.384856: step 3762, loss 0.340019, acc 0.890625, prec 0.0657408, recall 0.885602
2017-12-10T14:11:27.573518: step 3763, loss 0.247476, acc 0.921875, prec 0.0657339, recall 0.885602
2017-12-10T14:11:27.761878: step 3764, loss 0.310766, acc 0.90625, prec 0.0657842, recall 0.885698
2017-12-10T14:11:27.954704: step 3765, loss 0.30817, acc 0.96875, prec 0.0658401, recall 0.885795
2017-12-10T14:11:28.144221: step 3766, loss 0.0513085, acc 0.984375, prec 0.0658387, recall 0.885795
2017-12-10T14:11:28.331698: step 3767, loss 0.0411806, acc 1, prec 0.0658582, recall 0.885827
2017-12-10T14:11:28.517598: step 3768, loss 0.184617, acc 0.9375, prec 0.0658527, recall 0.885827
2017-12-10T14:11:28.705608: step 3769, loss 0.253516, acc 0.953125, prec 0.0658681, recall 0.885859
2017-12-10T14:11:28.899546: step 3770, loss 0.0521671, acc 0.96875, prec 0.0658654, recall 0.885859
2017-12-10T14:11:29.092606: step 3771, loss 0.193068, acc 0.921875, prec 0.0658975, recall 0.885923
2017-12-10T14:11:29.285495: step 3772, loss 0.0717389, acc 0.96875, prec 0.0658948, recall 0.885923
2017-12-10T14:11:29.469716: step 3773, loss 0.195617, acc 0.96875, prec 0.0659116, recall 0.885955
2017-12-10T14:11:29.658410: step 3774, loss 0.0602947, acc 0.984375, prec 0.0659102, recall 0.885955
2017-12-10T14:11:29.845686: step 3775, loss 0.551103, acc 0.921875, prec 0.0659033, recall 0.885955
2017-12-10T14:11:30.045618: step 3776, loss 0.265666, acc 1, prec 0.0659423, recall 0.886019
2017-12-10T14:11:30.243424: step 3777, loss 0.0698864, acc 0.984375, prec 0.0659605, recall 0.886051
2017-12-10T14:11:30.433898: step 3778, loss 0.249059, acc 0.9375, prec 0.065955, recall 0.886051
2017-12-10T14:11:30.625530: step 3779, loss 0.086729, acc 0.953125, prec 0.0659703, recall 0.886083
2017-12-10T14:11:30.818439: step 3780, loss 0.277198, acc 0.984375, prec 0.066008, recall 0.886147
2017-12-10T14:11:31.010953: step 3781, loss 0.00912842, acc 1, prec 0.0660275, recall 0.886179
2017-12-10T14:11:31.206500: step 3782, loss 0.00463015, acc 1, prec 0.0660275, recall 0.886179
2017-12-10T14:11:31.395180: step 3783, loss 0.262075, acc 0.953125, prec 0.0660429, recall 0.886211
2017-12-10T14:11:31.584799: step 3784, loss 0.110005, acc 0.96875, prec 0.0660401, recall 0.886211
2017-12-10T14:11:31.771754: step 3785, loss 0.294417, acc 0.921875, prec 0.0660332, recall 0.886211
2017-12-10T14:11:31.961248: step 3786, loss 0.197461, acc 0.9375, prec 0.0660472, recall 0.886243
2017-12-10T14:11:32.153890: step 3787, loss 0.207616, acc 0.953125, prec 0.0660431, recall 0.886243
2017-12-10T14:11:32.342850: step 3788, loss 0.144733, acc 0.96875, prec 0.0660403, recall 0.886243
2017-12-10T14:11:32.529616: step 3789, loss 0.131272, acc 0.9375, prec 0.0660738, recall 0.886306
2017-12-10T14:11:32.719093: step 3790, loss 0.0241486, acc 0.984375, prec 0.0660724, recall 0.886306
2017-12-10T14:11:32.902024: step 3791, loss 0.365786, acc 0.9375, prec 0.0660864, recall 0.886338
2017-12-10T14:11:33.092712: step 3792, loss 0.168517, acc 0.953125, prec 0.0661017, recall 0.88637
2017-12-10T14:11:33.282644: step 3793, loss 0.176236, acc 0.9375, prec 0.0660962, recall 0.88637
2017-12-10T14:11:33.469760: step 3794, loss 0.289098, acc 0.953125, prec 0.0661311, recall 0.886434
2017-12-10T14:11:33.659296: step 3795, loss 1.55388, acc 0.921875, prec 0.0661255, recall 0.886186
2017-12-10T14:11:33.849921: step 3796, loss 0.208904, acc 0.921875, prec 0.0661186, recall 0.886186
2017-12-10T14:11:34.038001: step 3797, loss 0.0531847, acc 0.984375, prec 0.0661367, recall 0.886217
2017-12-10T14:11:34.225967: step 3798, loss 0.0138262, acc 1, prec 0.0661367, recall 0.886217
2017-12-10T14:11:34.415527: step 3799, loss 0.00488255, acc 1, prec 0.0661367, recall 0.886217
2017-12-10T14:11:34.605108: step 3800, loss 0.153079, acc 0.953125, prec 0.0661326, recall 0.886217
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_1/1512932334/checkpoints/model-3800

2017-12-10T14:11:35.930705: step 3801, loss 0.187366, acc 0.953125, prec 0.0661479, recall 0.886249
2017-12-10T14:11:36.122127: step 3802, loss 0.0651033, acc 0.984375, prec 0.0661466, recall 0.886249
2017-12-10T14:11:36.311776: step 3803, loss 0.108576, acc 0.984375, prec 0.0661647, recall 0.886281
2017-12-10T14:11:36.502990: step 3804, loss 1.24216, acc 0.90625, prec 0.0662148, recall 0.886376
2017-12-10T14:11:36.695182: step 3805, loss 0.309622, acc 0.921875, prec 0.0662274, recall 0.886408
2017-12-10T14:11:36.883907: step 3806, loss 0.199944, acc 0.953125, prec 0.0662427, recall 0.88644
2017-12-10T14:11:37.072835: step 3807, loss 0.101836, acc 0.96875, prec 0.0662594, recall 0.886471
2017-12-10T14:11:37.258824: step 3808, loss 0.572341, acc 0.875, prec 0.0662873, recall 0.886535
2017-12-10T14:11:37.448329: step 3809, loss 0.33693, acc 0.921875, prec 0.0662804, recall 0.886535
2017-12-10T14:11:37.641255: step 3810, loss 0.408623, acc 0.953125, prec 0.0662957, recall 0.886566
2017-12-10T14:11:37.832796: step 3811, loss 0.271583, acc 0.921875, prec 0.0663082, recall 0.886598
2017-12-10T14:11:38.020285: step 3812, loss 0.148329, acc 0.96875, prec 0.0663055, recall 0.886598
2017-12-10T14:11:38.207527: step 3813, loss 0.599404, acc 0.890625, prec 0.0663347, recall 0.886661
2017-12-10T14:11:38.397650: step 3814, loss 0.0744984, acc 0.953125, prec 0.0663306, recall 0.886661
2017-12-10T14:11:38.584571: step 3815, loss 0.170282, acc 0.953125, prec 0.0663264, recall 0.886661
2017-12-10T14:11:38.770326: step 3816, loss 0.203697, acc 0.921875, prec 0.0663195, recall 0.886661
2017-12-10T14:11:38.959692: step 3817, loss 0.215357, acc 0.921875, prec 0.0663126, recall 0.886661
2017-12-10T14:11:39.148052: step 3818, loss 0.3871, acc 0.921875, prec 0.0663252, recall 0.886693
2017-12-10T14:11:39.336133: step 3819, loss 0.509676, acc 0.890625, prec 0.0663155, recall 0.886693
2017-12-10T14:11:39.523615: step 3820, loss 0.242598, acc 0.890625, prec 0.0663253, recall 0.886724
2017-12-10T14:11:39.710324: step 3821, loss 0.598667, acc 0.859375, prec 0.0663323, recall 0.886756
2017-12-10T14:11:39.906529: step 3822, loss 0.156837, acc 0.9375, prec 0.0663656, recall 0.886819
2017-12-10T14:11:40.094276: step 3823, loss 0.465356, acc 0.96875, prec 0.0663823, recall 0.88685
2017-12-10T14:11:40.284737: step 3824, loss 0.201502, acc 0.9375, prec 0.0664156, recall 0.886913
2017-12-10T14:11:40.473073: step 3825, loss 0.0257758, acc 0.984375, prec 0.0664142, recall 0.886913
2017-12-10T14:11:40.664664: step 3826, loss 0.260047, acc 0.953125, prec 0.0664489, recall 0.886976
2017-12-10T14:11:40.853192: step 3827, loss 0.305916, acc 0.921875, prec 0.0664808, recall 0.887039
2017-12-10T14:11:41.043350: step 3828, loss 0.0707208, acc 0.953125, prec 0.0664961, recall 0.88707
2017-12-10T14:11:41.230998: step 3829, loss 0.254255, acc 0.9375, prec 0.0664906, recall 0.88707
2017-12-10T14:11:41.426360: step 3830, loss 0.210925, acc 0.9375, prec 0.0665045, recall 0.887101
2017-12-10T14:11:41.612630: step 3831, loss 0.126194, acc 0.9375, prec 0.0664989, recall 0.887101
2017-12-10T14:11:41.799321: step 3832, loss 0.153007, acc 0.9375, prec 0.0664934, recall 0.887101
2017-12-10T14:11:41.987969: step 3833, loss 0.114861, acc 0.96875, prec 0.06651, recall 0.887133
2017-12-10T14:11:42.175601: step 3834, loss 0.0141485, acc 1, prec 0.06651, recall 0.887133
2017-12-10T14:11:42.365309: step 3835, loss 0.0417179, acc 1, prec 0.0665295, recall 0.887164
2017-12-10T14:11:42.557601: step 3836, loss 0.0464952, acc 0.96875, prec 0.0665267, recall 0.887164
2017-12-10T14:11:42.745785: step 3837, loss 1.03126, acc 0.96875, prec 0.0665627, recall 0.887226
2017-12-10T14:11:42.941476: step 3838, loss 0.0395635, acc 0.984375, prec 0.0665613, recall 0.887226
2017-12-10T14:11:43.130277: step 3839, loss 0.427235, acc 0.90625, prec 0.066553, recall 0.887226
2017-12-10T14:11:43.319173: step 3840, loss 0.104274, acc 0.953125, prec 0.0665489, recall 0.887226
2017-12-10T14:11:43.512345: step 3841, loss 3.94337, acc 0.90625, prec 0.0665614, recall 0.887012
2017-12-10T14:11:43.701806: step 3842, loss 0.241976, acc 0.921875, prec 0.0665739, recall 0.887043
2017-12-10T14:11:43.894792: step 3843, loss 0.0575737, acc 0.984375, prec 0.0666113, recall 0.887106
2017-12-10T14:11:44.090748: step 3844, loss 0.202735, acc 0.921875, prec 0.0666043, recall 0.887106
2017-12-10T14:11:44.277652: step 3845, loss 0.0771896, acc 0.953125, prec 0.066639, recall 0.887168
2017-12-10T14:11:44.465326: step 3846, loss 0.449544, acc 0.9375, prec 0.0666334, recall 0.887168
2017-12-10T14:11:44.655618: step 3847, loss 0.127099, acc 0.96875, prec 0.0666694, recall 0.887231
2017-12-10T14:11:44.841835: step 3848, loss 0.226821, acc 0.921875, prec 0.0666625, recall 0.887231
2017-12-10T14:11:45.031569: step 3849, loss 0.419977, acc 0.890625, prec 0.0666528, recall 0.887231
2017-12-10T14:11:45.226607: step 3850, loss 0.1947, acc 0.9375, prec 0.0666473, recall 0.887231
2017-12-10T14:11:45.412866: step 3851, loss 0.474499, acc 0.921875, prec 0.0666791, recall 0.887293
2017-12-10T14:11:45.601353: step 3852, loss 0.0527058, acc 0.984375, prec 0.0666777, recall 0.887293
2017-12-10T14:11:45.789461: step 3853, loss 0.244876, acc 0.953125, prec 0.066693, recall 0.887324
2017-12-10T14:11:45.979863: step 3854, loss 0.408187, acc 0.90625, prec 0.0667234, recall 0.887386
2017-12-10T14:11:46.167820: step 3855, loss 0.373168, acc 0.875, prec 0.0667123, recall 0.887386
2017-12-10T14:11:46.358478: step 3856, loss 0.13463, acc 0.9375, prec 0.0667261, recall 0.887417
2017-12-10T14:11:46.547396: step 3857, loss 0.536469, acc 0.890625, prec 0.0667552, recall 0.887479
2017-12-10T14:11:46.739766: step 3858, loss 0.141621, acc 0.953125, prec 0.066751, recall 0.887479
2017-12-10T14:11:46.931309: step 3859, loss 0.291443, acc 0.921875, prec 0.0667441, recall 0.887479
2017-12-10T14:11:47.117429: step 3860, loss 0.0904533, acc 0.96875, prec 0.0667607, recall 0.88751
2017-12-10T14:11:47.302811: step 3861, loss 0.025315, acc 1, prec 0.0667607, recall 0.88751
2017-12-10T14:11:47.493198: step 3862, loss 0.651676, acc 0.921875, prec 0.0667731, recall 0.887541
2017-12-10T14:11:47.684370: step 3863, loss 0.0261518, acc 1, prec 0.0668118, recall 0.887603
2017-12-10T14:11:47.870990: step 3864, loss 0.517503, acc 0.875, prec 0.0668201, recall 0.887634
2017-12-10T14:11:48.057044: step 3865, loss 0.0616975, acc 0.984375, prec 0.0668187, recall 0.887634
2017-12-10T14:11:48.242449: step 3866, loss 0.0695642, acc 0.984375, prec 0.066856, recall 0.887696
2017-12-10T14:11:48.432240: step 3867, loss 0.192937, acc 0.9375, prec 0.0669085, recall 0.887789
2017-12-10T14:11:48.620379: step 3868, loss 0.268229, acc 0.96875, prec 0.0669251, recall 0.88782
2017-12-10T14:11:48.805781: step 3869, loss 0.12849, acc 0.96875, prec 0.0669223, recall 0.88782
2017-12-10T14:11:49.000106: step 3870, loss 0.592579, acc 0.84375, prec 0.0669084, recall 0.88782
2017-12-10T14:11:49.190770: step 3871, loss 0.0379154, acc 1, prec 0.0669084, recall 0.88782
2017-12-10T14:11:49.377021: step 3872, loss 0.934577, acc 1, prec 0.0669471, recall 0.887881
2017-12-10T14:11:49.567416: step 3873, loss 0.0964466, acc 0.96875, prec 0.0669443, recall 0.887881
2017-12-10T14:11:49.756091: step 3874, loss 0.265301, acc 0.953125, prec 0.0669401, recall 0.887881
2017-12-10T14:11:49.944901: step 3875, loss 0.286608, acc 0.9375, prec 0.0669346, recall 0.887881
2017-12-10T14:11:50.135266: step 3876, loss 0.104549, acc 0.96875, prec 0.0669318, recall 0.887881
2017-12-10T14:11:50.324577: step 3877, loss 0.136953, acc 0.9375, prec 0.0669263, recall 0.887881
2017-12-10T14:11:50.518578: step 3878, loss 0.0339615, acc 0.984375, prec 0.0669249, recall 0.887881
2017-12-10T14:11:50.707873: step 3879, loss 0.368605, acc 0.921875, prec 0.066918, recall 0.887881
2017-12-10T14:11:50.895488: step 3880, loss 0.111077, acc 0.96875, prec 0.0669152, recall 0.887881
2017-12-10T14:11:51.088503: step 3881, loss 0.0779936, acc 0.984375, prec 0.0669525, recall 0.887943
2017-12-10T14:11:51.275707: step 3882, loss 0.537213, acc 0.984375, prec 0.0669897, recall 0.888004
2017-12-10T14:11:51.469790: step 3883, loss 0.0253891, acc 1, prec 0.0669897, recall 0.888004
2017-12-10T14:11:51.657512: step 3884, loss 0.289676, acc 0.921875, prec 0.0669828, recall 0.888004
2017-12-10T14:11:51.843358: step 3885, loss 0.307629, acc 0.9375, prec 0.0669772, recall 0.888004
2017-12-10T14:11:52.035459: step 3886, loss 0.948904, acc 0.9375, prec 0.0670103, recall 0.888066
2017-12-10T14:11:52.223679: step 3887, loss 0.0551673, acc 0.984375, prec 0.0670089, recall 0.888066
2017-12-10T14:11:52.409311: step 3888, loss 0.00561814, acc 1, prec 0.0670282, recall 0.888097
2017-12-10T14:11:52.599523: step 3889, loss 0.156593, acc 0.953125, prec 0.0670241, recall 0.888097
2017-12-10T14:11:52.790617: step 3890, loss 0.854692, acc 0.953125, prec 0.0670392, recall 0.888127
2017-12-10T14:11:52.980850: step 3891, loss 1.66562, acc 0.921875, prec 0.067053, recall 0.887914
2017-12-10T14:11:53.173766: step 3892, loss 0.0580043, acc 0.984375, prec 0.0670516, recall 0.887914
2017-12-10T14:11:53.362225: step 3893, loss 0.701465, acc 0.921875, prec 0.0670447, recall 0.887914
2017-12-10T14:11:53.556498: step 3894, loss 0.409547, acc 0.890625, prec 0.0670349, recall 0.887914
2017-12-10T14:11:53.742709: step 3895, loss 0.253477, acc 0.9375, prec 0.0670294, recall 0.887914
2017-12-10T14:11:53.931729: step 3896, loss 0.228126, acc 0.921875, prec 0.0670225, recall 0.887914
2017-12-10T14:11:54.117201: step 3897, loss 0.0978235, acc 0.953125, prec 0.0670376, recall 0.887945
2017-12-10T14:11:54.303203: step 3898, loss 0.477147, acc 0.875, prec 0.0670844, recall 0.888037
2017-12-10T14:11:54.488768: step 3899, loss 0.235788, acc 0.9375, prec 0.0670788, recall 0.888037
2017-12-10T14:11:54.678181: step 3900, loss 0.26903, acc 0.90625, prec 0.0671284, recall 0.888129
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_1/1512932334/checkpoints/model-3900

2017-12-10T14:11:56.086454: step 3901, loss 0.285886, acc 0.890625, prec 0.0671187, recall 0.888129
2017-12-10T14:11:56.275182: step 3902, loss 0.243052, acc 0.9375, prec 0.0671517, recall 0.88819
2017-12-10T14:11:56.459435: step 3903, loss 0.504569, acc 0.90625, prec 0.0671434, recall 0.88819
2017-12-10T14:11:56.647470: step 3904, loss 0.43974, acc 0.890625, prec 0.0671337, recall 0.88819
2017-12-10T14:11:56.838680: step 3905, loss 0.432197, acc 0.921875, prec 0.067146, recall 0.888221
2017-12-10T14:11:57.029715: step 3906, loss 0.17639, acc 0.9375, prec 0.0671597, recall 0.888251
2017-12-10T14:11:57.217970: step 3907, loss 0.343155, acc 0.90625, prec 0.0671706, recall 0.888282
2017-12-10T14:11:57.409967: step 3908, loss 0.181694, acc 0.9375, prec 0.0671844, recall 0.888312
2017-12-10T14:11:57.600644: step 3909, loss 0.0821183, acc 0.953125, prec 0.0671995, recall 0.888343
2017-12-10T14:11:57.792310: step 3910, loss 0.11433, acc 0.953125, prec 0.0672338, recall 0.888404
2017-12-10T14:11:57.984222: step 3911, loss 0.278698, acc 0.921875, prec 0.0672461, recall 0.888434
2017-12-10T14:11:58.172330: step 3912, loss 0.285615, acc 0.921875, prec 0.0672777, recall 0.888495
2017-12-10T14:11:58.362069: step 3913, loss 0.314202, acc 0.953125, prec 0.0672735, recall 0.888495
2017-12-10T14:11:58.548307: step 3914, loss 0.201187, acc 0.890625, prec 0.0672638, recall 0.888495
2017-12-10T14:11:58.740083: step 3915, loss 0.00944463, acc 1, prec 0.0672638, recall 0.888495
2017-12-10T14:11:58.931149: step 3916, loss 1.96998, acc 0.90625, prec 0.0672761, recall 0.888283
2017-12-10T14:11:59.124711: step 3917, loss 0.134444, acc 0.984375, prec 0.0672748, recall 0.888283
2017-12-10T14:11:59.313695: step 3918, loss 0.342931, acc 0.9375, prec 0.0672692, recall 0.888283
2017-12-10T14:11:59.509658: step 3919, loss 0.206241, acc 0.96875, prec 0.0672857, recall 0.888314
2017-12-10T14:11:59.694994: step 3920, loss 0.498092, acc 0.90625, prec 0.0672773, recall 0.888314
2017-12-10T14:11:59.880957: step 3921, loss 0.1008, acc 0.984375, prec 0.067276, recall 0.888314
2017-12-10T14:12:00.076851: step 3922, loss 0.257651, acc 0.953125, prec 0.067291, recall 0.888344
2017-12-10T14:12:00.264466: step 3923, loss 0.257477, acc 0.921875, prec 0.0673033, recall 0.888375
2017-12-10T14:12:00.456528: step 3924, loss 0.248048, acc 0.90625, prec 0.067295, recall 0.888375
2017-12-10T14:12:00.642266: step 3925, loss 0.210998, acc 0.953125, prec 0.0673101, recall 0.888405
2017-12-10T14:12:00.832527: step 3926, loss 0.0815372, acc 0.96875, prec 0.067365, recall 0.888496
2017-12-10T14:12:01.032083: step 3927, loss 0.321266, acc 0.859375, prec 0.0673717, recall 0.888526
2017-12-10T14:12:01.229239: step 3928, loss 0.156765, acc 0.953125, prec 0.067406, recall 0.888587
2017-12-10T14:12:01.423028: step 3929, loss 0.25229, acc 0.90625, prec 0.0674169, recall 0.888617
2017-12-10T14:12:01.614675: step 3930, loss 0.216242, acc 0.953125, prec 0.0674319, recall 0.888647
2017-12-10T14:12:01.806236: step 3931, loss 0.369429, acc 0.90625, prec 0.0674236, recall 0.888647
2017-12-10T14:12:01.998940: step 3932, loss 0.333424, acc 0.953125, prec 0.0674771, recall 0.888738
2017-12-10T14:12:02.190372: step 3933, loss 0.279037, acc 0.921875, prec 0.0674893, recall 0.888768
2017-12-10T14:12:02.377754: step 3934, loss 0.0914582, acc 0.96875, prec 0.0674866, recall 0.888768
2017-12-10T14:12:02.567564: step 3935, loss 0.182616, acc 0.96875, prec 0.0675222, recall 0.888829
2017-12-10T14:12:02.755790: step 3936, loss 0.246389, acc 0.90625, prec 0.0675331, recall 0.888859
2017-12-10T14:12:02.949618: step 3937, loss 0.286366, acc 0.9375, prec 0.0675275, recall 0.888859
2017-12-10T14:12:03.137915: step 3938, loss 0.27721, acc 0.875, prec 0.0675164, recall 0.888859
2017-12-10T14:12:03.325147: step 3939, loss 0.199868, acc 0.921875, prec 0.0675094, recall 0.888859
2017-12-10T14:12:03.516576: step 3940, loss 1.24983, acc 0.9375, prec 0.0675231, recall 0.888889
2017-12-10T14:12:03.703511: step 3941, loss 0.108576, acc 0.953125, prec 0.0675189, recall 0.888889
2017-12-10T14:12:03.891802: step 3942, loss 0.128552, acc 0.9375, prec 0.0675325, recall 0.888919
2017-12-10T14:12:04.082876: step 3943, loss 0.168469, acc 0.953125, prec 0.0675859, recall 0.889009
2017-12-10T14:12:04.273349: step 3944, loss 0.0842736, acc 0.96875, prec 0.0675831, recall 0.889009
2017-12-10T14:12:04.463199: step 3945, loss 0.0366309, acc 0.984375, prec 0.0676009, recall 0.889039
2017-12-10T14:12:04.649294: step 3946, loss 0.124613, acc 0.953125, prec 0.0675968, recall 0.889039
2017-12-10T14:12:04.841055: step 3947, loss 0.250041, acc 0.96875, prec 0.0676132, recall 0.889069
2017-12-10T14:12:05.030431: step 3948, loss 0.215291, acc 0.96875, prec 0.0676104, recall 0.889069
2017-12-10T14:12:05.222628: step 3949, loss 0.176946, acc 0.9375, prec 0.0676048, recall 0.889069
2017-12-10T14:12:05.408606: step 3950, loss 0.200618, acc 0.96875, prec 0.0676404, recall 0.889129
2017-12-10T14:12:05.594304: step 3951, loss 0.0188575, acc 1, prec 0.0676404, recall 0.889129
2017-12-10T14:12:05.779811: step 3952, loss 0.221281, acc 0.953125, prec 0.0676938, recall 0.889219
2017-12-10T14:12:05.971519: step 3953, loss 0.301791, acc 0.96875, prec 0.0677677, recall 0.889339
2017-12-10T14:12:06.162081: step 3954, loss 0.172602, acc 0.953125, prec 0.0677635, recall 0.889339
2017-12-10T14:12:06.348027: step 3955, loss 0.233602, acc 0.953125, prec 0.0677785, recall 0.889369
2017-12-10T14:12:06.541224: step 3956, loss 0.269251, acc 0.984375, prec 0.0678154, recall 0.889428
2017-12-10T14:12:06.730297: step 3957, loss 0.27969, acc 0.9375, prec 0.0678482, recall 0.889488
2017-12-10T14:12:06.919886: step 3958, loss 0.323742, acc 0.953125, prec 0.0678823, recall 0.889547
2017-12-10T14:12:07.112741: step 3959, loss 0.257214, acc 0.90625, prec 0.0678931, recall 0.889577
2017-12-10T14:12:07.302112: step 3960, loss 0.314677, acc 0.9375, prec 0.0679067, recall 0.889607
2017-12-10T14:12:07.492273: step 3961, loss 5.51105, acc 0.953125, prec 0.0679231, recall 0.889397
2017-12-10T14:12:07.685360: step 3962, loss 0.157283, acc 0.953125, prec 0.067938, recall 0.889427
2017-12-10T14:12:07.874291: step 3963, loss 0.0165627, acc 1, prec 0.0679763, recall 0.889486
2017-12-10T14:12:08.063560: step 3964, loss 0.129083, acc 0.96875, prec 0.0679927, recall 0.889516
2017-12-10T14:12:08.254847: step 3965, loss 0.308178, acc 0.953125, prec 0.0680076, recall 0.889546
2017-12-10T14:12:08.441647: step 3966, loss 0.610142, acc 0.875, prec 0.0680348, recall 0.889605
2017-12-10T14:12:08.624652: step 3967, loss 0.169033, acc 0.9375, prec 0.0680675, recall 0.889664
2017-12-10T14:12:08.814991: step 3968, loss 0.084989, acc 0.96875, prec 0.0680838, recall 0.889694
2017-12-10T14:12:09.002880: step 3969, loss 0.25234, acc 0.921875, prec 0.0680959, recall 0.889724
2017-12-10T14:12:09.193375: step 3970, loss 0.398393, acc 0.921875, prec 0.0681272, recall 0.889783
2017-12-10T14:12:09.382203: step 3971, loss 0.480242, acc 0.890625, prec 0.0681366, recall 0.889812
2017-12-10T14:12:09.570306: step 3972, loss 0.192567, acc 0.9375, prec 0.0681501, recall 0.889842
2017-12-10T14:12:09.761293: step 3973, loss 0.183111, acc 0.921875, prec 0.0681622, recall 0.889871
2017-12-10T14:12:09.949609: step 3974, loss 0.273447, acc 0.9375, prec 0.068214, recall 0.88996
2017-12-10T14:12:10.136554: step 3975, loss 0.101615, acc 0.9375, prec 0.0682084, recall 0.88996
2017-12-10T14:12:10.307277: step 3976, loss 0.248813, acc 0.901961, prec 0.0682205, recall 0.889989
2017-12-10T14:12:10.500226: step 3977, loss 0.804642, acc 0.890625, prec 0.0682298, recall 0.890019
2017-12-10T14:12:10.690173: step 3978, loss 0.241378, acc 0.9375, prec 0.0682434, recall 0.890048
2017-12-10T14:12:10.879952: step 3979, loss 0.251097, acc 0.953125, prec 0.0682583, recall 0.890078
2017-12-10T14:12:11.071702: step 3980, loss 0.371687, acc 0.90625, prec 0.0682881, recall 0.890136
2017-12-10T14:12:11.261616: step 3981, loss 0.349654, acc 0.921875, prec 0.0683002, recall 0.890166
2017-12-10T14:12:11.448424: step 3982, loss 0.234448, acc 0.9375, prec 0.0683137, recall 0.890195
2017-12-10T14:12:11.636697: step 3983, loss 0.175107, acc 0.96875, prec 0.06833, recall 0.890224
2017-12-10T14:12:11.829414: step 3984, loss 0.265256, acc 0.953125, prec 0.0683258, recall 0.890224
2017-12-10T14:12:12.019540: step 3985, loss 0.217495, acc 0.9375, prec 0.0683584, recall 0.890283
2017-12-10T14:12:12.205382: step 3986, loss 0.414632, acc 0.90625, prec 0.0683691, recall 0.890312
2017-12-10T14:12:12.395753: step 3987, loss 0.0603611, acc 0.96875, prec 0.0683854, recall 0.890342
2017-12-10T14:12:12.588274: step 3988, loss 0.503977, acc 0.9375, prec 0.068437, recall 0.890429
2017-12-10T14:12:12.778703: step 3989, loss 0.133717, acc 0.953125, prec 0.0684328, recall 0.890429
2017-12-10T14:12:12.964932: step 3990, loss 0.170333, acc 0.953125, prec 0.0684477, recall 0.890458
2017-12-10T14:12:13.152621: step 3991, loss 0.0257592, acc 1, prec 0.0684859, recall 0.890517
2017-12-10T14:12:13.343530: step 3992, loss 0.197292, acc 0.9375, prec 0.0684802, recall 0.890517
2017-12-10T14:12:13.532193: step 3993, loss 0.218211, acc 0.9375, prec 0.0684746, recall 0.890517
2017-12-10T14:12:13.720789: step 3994, loss 0.0911541, acc 0.96875, prec 0.06851, recall 0.890575
2017-12-10T14:12:13.919293: step 3995, loss 0.105906, acc 0.953125, prec 0.0685439, recall 0.890633
2017-12-10T14:12:14.109701: step 3996, loss 0.192395, acc 0.96875, prec 0.0685411, recall 0.890633
2017-12-10T14:12:14.297385: step 3997, loss 0.0552398, acc 0.96875, prec 0.0685383, recall 0.890633
2017-12-10T14:12:14.487351: step 3998, loss 0.262385, acc 0.953125, prec 0.0685341, recall 0.890633
2017-12-10T14:12:14.678304: step 3999, loss 0.0656056, acc 0.984375, prec 0.0685327, recall 0.890633
2017-12-10T14:12:14.867906: step 4000, loss 0.780683, acc 0.953125, prec 0.0685476, recall 0.890662

Evaluation:
2017-12-10T14:12:19.148065: step 4000, loss 4.96198, acc 0.965751, prec 0.0691121, recall 0.868798

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_1/1512932334/checkpoints/model-4000

2017-12-10T14:12:20.338221: step 4001, loss 0.104765, acc 0.953125, prec 0.0691269, recall 0.868832
2017-12-10T14:12:20.525754: step 4002, loss 0.119138, acc 0.953125, prec 0.0691226, recall 0.868832
2017-12-10T14:12:20.718262: step 4003, loss 0.132217, acc 0.984375, prec 0.0691402, recall 0.868865
2017-12-10T14:12:20.905706: step 4004, loss 0.490415, acc 0.96875, prec 0.0691563, recall 0.868899
2017-12-10T14:12:21.094016: step 4005, loss 0.0567535, acc 0.984375, prec 0.0691738, recall 0.868932
2017-12-10T14:12:21.281067: step 4006, loss 0.170359, acc 0.96875, prec 0.0691899, recall 0.868966
2017-12-10T14:12:21.470345: step 4007, loss 0.168897, acc 0.96875, prec 0.0692061, recall 0.868999
2017-12-10T14:12:21.660704: step 4008, loss 0.153347, acc 0.953125, prec 0.0692397, recall 0.869066
2017-12-10T14:12:21.852327: step 4009, loss 0.752626, acc 0.984375, prec 0.0692761, recall 0.869133
2017-12-10T14:12:22.044258: step 4010, loss 0.0127423, acc 1, prec 0.0692951, recall 0.869166
2017-12-10T14:12:22.231242: step 4011, loss 0.0375285, acc 0.984375, prec 0.0692936, recall 0.869166
2017-12-10T14:12:22.421629: step 4012, loss 0.0831615, acc 0.96875, prec 0.0693097, recall 0.869199
2017-12-10T14:12:22.609665: step 4013, loss 0.0462435, acc 0.984375, prec 0.0693083, recall 0.869199
2017-12-10T14:12:22.799994: step 4014, loss 0.328796, acc 0.9375, prec 0.0693216, recall 0.869233
2017-12-10T14:12:22.984352: step 4015, loss 0.160245, acc 0.953125, prec 0.0693363, recall 0.869266
2017-12-10T14:12:23.173568: step 4016, loss 0.225283, acc 0.96875, prec 0.0693335, recall 0.869266
2017-12-10T14:12:23.362096: step 4017, loss 0.228126, acc 0.90625, prec 0.069344, recall 0.869299
2017-12-10T14:12:23.553235: step 4018, loss 0.250141, acc 0.96875, prec 0.0693979, recall 0.869399
2017-12-10T14:12:23.743709: step 4019, loss 0.165552, acc 0.96875, prec 0.069414, recall 0.869432
2017-12-10T14:12:23.933854: step 4020, loss 0.0721964, acc 0.96875, prec 0.0694301, recall 0.869466
2017-12-10T14:12:24.127092: step 4021, loss 0.231884, acc 0.96875, prec 0.069465, recall 0.869532
2017-12-10T14:12:24.317336: step 4022, loss 0.00209413, acc 1, prec 0.069465, recall 0.869532
2017-12-10T14:12:24.503213: step 4023, loss 0.277482, acc 0.953125, prec 0.0694608, recall 0.869532
2017-12-10T14:12:24.694068: step 4024, loss 0.598569, acc 0.984375, prec 0.0694783, recall 0.869565
2017-12-10T14:12:24.884725: step 4025, loss 0.0245006, acc 1, prec 0.069535, recall 0.869665
2017-12-10T14:12:25.074191: step 4026, loss 0.249868, acc 0.96875, prec 0.06957, recall 0.869731
2017-12-10T14:12:25.261702: step 4027, loss 0.0331835, acc 0.984375, prec 0.0695686, recall 0.869731
2017-12-10T14:12:25.449850: step 4028, loss 0.292971, acc 0.9375, prec 0.0695629, recall 0.869731
2017-12-10T14:12:25.640619: step 4029, loss 0.235062, acc 0.96875, prec 0.069579, recall 0.869764
2017-12-10T14:12:25.832592: step 4030, loss 0.235912, acc 0.96875, prec 0.0695762, recall 0.869764
2017-12-10T14:12:26.021124: step 4031, loss 0.120887, acc 0.96875, prec 0.0695922, recall 0.869797
2017-12-10T14:12:26.207490: step 4032, loss 0.0851678, acc 0.953125, prec 0.0696258, recall 0.869863
2017-12-10T14:12:26.397630: step 4033, loss 0.253914, acc 0.984375, prec 0.0696433, recall 0.869896
2017-12-10T14:12:26.585666: step 4034, loss 0.106305, acc 0.953125, prec 0.0696579, recall 0.869929
2017-12-10T14:12:26.774047: step 4035, loss 0.355513, acc 0.921875, prec 0.0696697, recall 0.869962
2017-12-10T14:12:26.964143: step 4036, loss 0.325387, acc 0.9375, prec 0.0697018, recall 0.870028
2017-12-10T14:12:27.153596: step 4037, loss 0.0598948, acc 0.96875, prec 0.069699, recall 0.870028
2017-12-10T14:12:27.344612: step 4038, loss 0.161483, acc 0.9375, prec 0.0697122, recall 0.870061
2017-12-10T14:12:27.534257: step 4039, loss 1.22762, acc 0.90625, prec 0.0697415, recall 0.870127
2017-12-10T14:12:27.725307: step 4040, loss 0.14474, acc 0.96875, prec 0.0697387, recall 0.870127
2017-12-10T14:12:27.918580: step 4041, loss 0.0659695, acc 0.96875, prec 0.0697736, recall 0.870192
2017-12-10T14:12:28.107434: step 4042, loss 0.206862, acc 0.9375, prec 0.0697868, recall 0.870225
2017-12-10T14:12:28.293298: step 4043, loss 0.0583541, acc 0.984375, prec 0.069842, recall 0.870324
2017-12-10T14:12:28.485144: step 4044, loss 0.116403, acc 0.96875, prec 0.0698391, recall 0.870324
2017-12-10T14:12:28.675108: step 4045, loss 0.0291879, acc 0.984375, prec 0.0698566, recall 0.870356
2017-12-10T14:12:28.871123: step 4046, loss 0.0758977, acc 0.984375, prec 0.069874, recall 0.870389
2017-12-10T14:12:29.066546: step 4047, loss 0.334694, acc 0.90625, prec 0.0698655, recall 0.870389
2017-12-10T14:12:29.252336: step 4048, loss 0.157522, acc 0.96875, prec 0.0698816, recall 0.870422
2017-12-10T14:12:29.444803: step 4049, loss 0.112377, acc 0.953125, prec 0.0698773, recall 0.870422
2017-12-10T14:12:29.630741: step 4050, loss 0.346247, acc 0.921875, prec 0.0698702, recall 0.870422
2017-12-10T14:12:29.816546: step 4051, loss 0.033759, acc 0.984375, prec 0.0698688, recall 0.870422
2017-12-10T14:12:30.003005: step 4052, loss 0.142819, acc 0.96875, prec 0.0698848, recall 0.870455
2017-12-10T14:12:30.192851: step 4053, loss 0.136958, acc 0.96875, prec 0.0699009, recall 0.870487
2017-12-10T14:12:30.380565: step 4054, loss 0.926289, acc 1, prec 0.0699197, recall 0.87052
2017-12-10T14:12:30.570759: step 4055, loss 0.219813, acc 0.9375, prec 0.0699141, recall 0.87052
2017-12-10T14:12:30.760542: step 4056, loss 0.0943025, acc 0.9375, prec 0.0699272, recall 0.870553
2017-12-10T14:12:30.950250: step 4057, loss 0.0109392, acc 1, prec 0.0699272, recall 0.870553
2017-12-10T14:12:31.137619: step 4058, loss 0.0178698, acc 0.984375, prec 0.0699258, recall 0.870553
2017-12-10T14:12:31.337199: step 4059, loss 0.0391259, acc 0.96875, prec 0.0699607, recall 0.870618
2017-12-10T14:12:31.527205: step 4060, loss 0.308881, acc 0.9375, prec 0.0699927, recall 0.870683
2017-12-10T14:12:31.716110: step 4061, loss 0.180868, acc 0.96875, prec 0.0700087, recall 0.870716
2017-12-10T14:12:31.909027: step 4062, loss 0.326004, acc 0.921875, prec 0.0700393, recall 0.870781
2017-12-10T14:12:32.100691: step 4063, loss 0.177949, acc 0.921875, prec 0.0700322, recall 0.870781
2017-12-10T14:12:32.287143: step 4064, loss 0.207741, acc 0.9375, prec 0.0700454, recall 0.870813
2017-12-10T14:12:32.474132: step 4065, loss 0.159349, acc 0.984375, prec 0.0700816, recall 0.870878
2017-12-10T14:12:32.665091: step 4066, loss 0.147544, acc 0.96875, prec 0.0700788, recall 0.870878
2017-12-10T14:12:32.850924: step 4067, loss 0.444715, acc 0.921875, prec 0.0700717, recall 0.870878
2017-12-10T14:12:33.040634: step 4068, loss 0.0639305, acc 0.984375, prec 0.0700703, recall 0.870878
2017-12-10T14:12:33.229785: step 4069, loss 0.0118466, acc 1, prec 0.0701079, recall 0.870943
2017-12-10T14:12:33.416223: step 4070, loss 0.119144, acc 0.96875, prec 0.0701239, recall 0.870976
2017-12-10T14:12:33.606498: step 4071, loss 0.131392, acc 0.984375, prec 0.0701225, recall 0.870976
2017-12-10T14:12:33.792823: step 4072, loss 2.09616, acc 0.953125, prec 0.0701197, recall 0.870757
2017-12-10T14:12:33.982794: step 4073, loss 0.158559, acc 0.953125, prec 0.0701531, recall 0.870822
2017-12-10T14:12:34.172847: step 4074, loss 0.167135, acc 0.953125, prec 0.0701488, recall 0.870822
2017-12-10T14:12:34.363776: step 4075, loss 0.504301, acc 0.953125, prec 0.070201, recall 0.870919
2017-12-10T14:12:34.556997: step 4076, loss 0.0594661, acc 0.953125, prec 0.0701967, recall 0.870919
2017-12-10T14:12:34.748660: step 4077, loss 2.52704, acc 0.953125, prec 0.0702127, recall 0.870733
2017-12-10T14:12:34.939697: step 4078, loss 0.198914, acc 0.9375, prec 0.0702635, recall 0.87083
2017-12-10T14:12:35.129963: step 4079, loss 0.402856, acc 0.90625, prec 0.0702926, recall 0.870895
2017-12-10T14:12:35.315772: step 4080, loss 0.269166, acc 0.9375, prec 0.0703433, recall 0.870992
2017-12-10T14:12:35.509385: step 4081, loss 0.273842, acc 0.9375, prec 0.0703376, recall 0.870992
2017-12-10T14:12:35.695784: step 4082, loss 0.625598, acc 0.875, prec 0.0703262, recall 0.870992
2017-12-10T14:12:35.882577: step 4083, loss 0.481723, acc 0.890625, prec 0.0703163, recall 0.870992
2017-12-10T14:12:36.069278: step 4084, loss 0.707041, acc 0.828125, prec 0.0703007, recall 0.870992
2017-12-10T14:12:36.256641: step 4085, loss 0.434435, acc 0.84375, prec 0.0702864, recall 0.870992
2017-12-10T14:12:36.441126: step 4086, loss 0.493743, acc 0.890625, prec 0.0702953, recall 0.871024
2017-12-10T14:12:36.629463: step 4087, loss 0.543822, acc 0.90625, prec 0.0703431, recall 0.871121
2017-12-10T14:12:36.814707: step 4088, loss 0.307751, acc 0.953125, prec 0.0703389, recall 0.871121
2017-12-10T14:12:37.004866: step 4089, loss 0.321848, acc 0.890625, prec 0.0703477, recall 0.871153
2017-12-10T14:12:37.199603: step 4090, loss 0.324759, acc 0.890625, prec 0.0703378, recall 0.871153
2017-12-10T14:12:37.388559: step 4091, loss 0.63979, acc 0.828125, prec 0.0703409, recall 0.871186
2017-12-10T14:12:37.577392: step 4092, loss 0.59748, acc 0.90625, prec 0.0703512, recall 0.871218
2017-12-10T14:12:37.763910: step 4093, loss 0.808303, acc 0.859375, prec 0.0703384, recall 0.871218
2017-12-10T14:12:37.952834: step 4094, loss 0.163884, acc 0.953125, prec 0.0703529, recall 0.87125
2017-12-10T14:12:38.143343: step 4095, loss 0.27357, acc 0.90625, prec 0.0703444, recall 0.87125
2017-12-10T14:12:38.331387: step 4096, loss 0.353935, acc 0.921875, prec 0.0703373, recall 0.87125
2017-12-10T14:12:38.519401: step 4097, loss 0.235937, acc 0.9375, prec 0.0703316, recall 0.87125
2017-12-10T14:12:38.705210: step 4098, loss 0.536686, acc 0.890625, prec 0.0703404, recall 0.871282
2017-12-10T14:12:38.891693: step 4099, loss 0.263091, acc 0.90625, prec 0.0703694, recall 0.871346
2017-12-10T14:12:39.078622: step 4100, loss 0.112346, acc 0.953125, prec 0.0703651, recall 0.871346
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_1/1512932334/checkpoints/model-4100

2017-12-10T14:12:40.307878: step 4101, loss 0.396205, acc 0.859375, prec 0.0703524, recall 0.871346
2017-12-10T14:12:40.494752: step 4102, loss 0.00758568, acc 1, prec 0.0703899, recall 0.871411
2017-12-10T14:12:40.677519: step 4103, loss 0.237913, acc 0.921875, prec 0.0703828, recall 0.871411
2017-12-10T14:12:40.865817: step 4104, loss 0.0108279, acc 1, prec 0.0703828, recall 0.871411
2017-12-10T14:12:41.053183: step 4105, loss 0.136824, acc 0.9375, prec 0.0703771, recall 0.871411
2017-12-10T14:12:41.246147: step 4106, loss 0.0834701, acc 0.984375, prec 0.0704132, recall 0.871475
2017-12-10T14:12:41.433189: step 4107, loss 2.72741, acc 0.96875, prec 0.0704117, recall 0.871257
2017-12-10T14:12:41.626487: step 4108, loss 0.0221719, acc 1, prec 0.0704117, recall 0.871257
2017-12-10T14:12:41.814967: step 4109, loss 0.179471, acc 0.953125, prec 0.0704262, recall 0.87129
2017-12-10T14:12:41.999713: step 4110, loss 0.202029, acc 0.96875, prec 0.0704421, recall 0.871322
2017-12-10T14:12:42.189262: step 4111, loss 0.200799, acc 0.984375, prec 0.0704594, recall 0.871354
2017-12-10T14:12:42.382539: step 4112, loss 0.0343891, acc 0.984375, prec 0.070458, recall 0.871354
2017-12-10T14:12:42.573834: step 4113, loss 0.0730658, acc 0.953125, prec 0.07051, recall 0.87145
2017-12-10T14:12:42.761276: step 4114, loss 0.273812, acc 0.9375, prec 0.0705418, recall 0.871514
2017-12-10T14:12:42.947701: step 4115, loss 0.265357, acc 0.953125, prec 0.0705562, recall 0.871546
2017-12-10T14:12:43.138412: step 4116, loss 0.0743572, acc 0.984375, prec 0.0705735, recall 0.871578
2017-12-10T14:12:43.334363: step 4117, loss 0.191963, acc 0.953125, prec 0.0706067, recall 0.871642
2017-12-10T14:12:43.524473: step 4118, loss 0.376486, acc 0.921875, prec 0.0706183, recall 0.871674
2017-12-10T14:12:43.714150: step 4119, loss 0.101626, acc 0.96875, prec 0.0706904, recall 0.871801
2017-12-10T14:12:43.912648: step 4120, loss 0.2132, acc 0.921875, prec 0.070702, recall 0.871833
2017-12-10T14:12:44.105934: step 4121, loss 0.362688, acc 0.953125, prec 0.0707164, recall 0.871865
2017-12-10T14:12:44.294946: step 4122, loss 0.339705, acc 0.9375, prec 0.0707294, recall 0.871897
2017-12-10T14:12:44.489313: step 4123, loss 0.546456, acc 0.921875, prec 0.0707598, recall 0.87196
2017-12-10T14:12:44.680109: step 4124, loss 0.393979, acc 0.9375, prec 0.0707915, recall 0.872024
2017-12-10T14:12:44.872736: step 4125, loss 0.075629, acc 0.96875, prec 0.0708073, recall 0.872056
2017-12-10T14:12:45.063718: step 4126, loss 0.0829666, acc 0.984375, prec 0.0708059, recall 0.872056
2017-12-10T14:12:45.254883: step 4127, loss 0.0571108, acc 0.984375, prec 0.0708045, recall 0.872056
2017-12-10T14:12:45.442129: step 4128, loss 0.322999, acc 0.953125, prec 0.0708376, recall 0.872119
2017-12-10T14:12:45.633441: step 4129, loss 0.413935, acc 0.859375, prec 0.0708435, recall 0.872151
2017-12-10T14:12:45.819701: step 4130, loss 0.248893, acc 0.90625, prec 0.0708349, recall 0.872151
2017-12-10T14:12:46.007141: step 4131, loss 0.110768, acc 0.984375, prec 0.0708335, recall 0.872151
2017-12-10T14:12:46.194937: step 4132, loss 0.327167, acc 0.921875, prec 0.0708264, recall 0.872151
2017-12-10T14:12:46.383083: step 4133, loss 0.129214, acc 0.953125, prec 0.0708221, recall 0.872151
2017-12-10T14:12:46.569367: step 4134, loss 0.157691, acc 0.96875, prec 0.0708379, recall 0.872182
2017-12-10T14:12:46.759324: step 4135, loss 0.292162, acc 0.90625, prec 0.0708294, recall 0.872182
2017-12-10T14:12:46.946777: step 4136, loss 0.0573857, acc 0.984375, prec 0.0708467, recall 0.872214
2017-12-10T14:12:47.135342: step 4137, loss 0.261292, acc 0.953125, prec 0.0708611, recall 0.872246
2017-12-10T14:12:47.326024: step 4138, loss 0.236607, acc 0.953125, prec 0.0708568, recall 0.872246
2017-12-10T14:12:47.516532: step 4139, loss 0.0249731, acc 0.984375, prec 0.0708554, recall 0.872246
2017-12-10T14:12:47.710017: step 4140, loss 0.264007, acc 0.953125, prec 0.0708511, recall 0.872246
2017-12-10T14:12:47.902213: step 4141, loss 0.0361788, acc 0.96875, prec 0.0708669, recall 0.872277
2017-12-10T14:12:48.089564: step 4142, loss 0.084818, acc 0.9375, prec 0.0708986, recall 0.87234
2017-12-10T14:12:48.277351: step 4143, loss 0.275188, acc 0.953125, prec 0.070913, recall 0.872372
2017-12-10T14:12:48.464574: step 4144, loss 0.278507, acc 0.953125, prec 0.0709274, recall 0.872404
2017-12-10T14:12:48.653663: step 4145, loss 0.0315332, acc 0.984375, prec 0.0709447, recall 0.872435
2017-12-10T14:12:48.843963: step 4146, loss 0.129975, acc 0.96875, prec 0.0709605, recall 0.872467
2017-12-10T14:12:49.033152: step 4147, loss 0.216403, acc 0.953125, prec 0.0709562, recall 0.872467
2017-12-10T14:12:49.225585: step 4148, loss 6.26732, acc 0.96875, prec 0.0709548, recall 0.872251
2017-12-10T14:12:49.417487: step 4149, loss 0.280177, acc 0.9375, prec 0.0709677, recall 0.872283
2017-12-10T14:12:49.611892: step 4150, loss 0.124492, acc 0.921875, prec 0.0709606, recall 0.872283
2017-12-10T14:12:49.803676: step 4151, loss 0.0638347, acc 0.96875, prec 0.0709578, recall 0.872283
2017-12-10T14:12:49.989671: step 4152, loss 1.29803, acc 0.921875, prec 0.0709521, recall 0.872067
2017-12-10T14:12:50.183894: step 4153, loss 0.16959, acc 0.96875, prec 0.0709492, recall 0.872067
2017-12-10T14:12:50.372112: step 4154, loss 0.705394, acc 0.84375, prec 0.0709536, recall 0.872099
2017-12-10T14:12:50.561232: step 4155, loss 0.438589, acc 0.859375, prec 0.0709408, recall 0.872099
2017-12-10T14:12:50.748734: step 4156, loss 0.275847, acc 0.90625, prec 0.0709509, recall 0.87213
2017-12-10T14:12:50.939836: step 4157, loss 0.330721, acc 0.890625, prec 0.0709409, recall 0.87213
2017-12-10T14:12:51.131457: step 4158, loss 0.459032, acc 0.875, prec 0.0709295, recall 0.87213
2017-12-10T14:12:51.325206: step 4159, loss 0.62877, acc 0.859375, prec 0.0709354, recall 0.872162
2017-12-10T14:12:51.514397: step 4160, loss 0.231892, acc 0.921875, prec 0.0709282, recall 0.872162
2017-12-10T14:12:51.701517: step 4161, loss 0.641801, acc 0.890625, prec 0.0709183, recall 0.872162
2017-12-10T14:12:51.886519: step 4162, loss 0.377596, acc 0.921875, prec 0.0709112, recall 0.872162
2017-12-10T14:12:52.079125: step 4163, loss 0.272365, acc 0.90625, prec 0.0709026, recall 0.872162
2017-12-10T14:12:52.268289: step 4164, loss 0.284684, acc 0.9375, prec 0.0709342, recall 0.872225
2017-12-10T14:12:52.464126: step 4165, loss 0.12176, acc 0.96875, prec 0.0709314, recall 0.872225
2017-12-10T14:12:52.651559: step 4166, loss 0.29797, acc 0.9375, prec 0.0709443, recall 0.872256
2017-12-10T14:12:52.844123: step 4167, loss 0.676214, acc 0.890625, prec 0.070953, recall 0.872288
2017-12-10T14:12:53.036265: step 4168, loss 0.144226, acc 0.953125, prec 0.0709487, recall 0.872288
2017-12-10T14:12:53.226974: step 4169, loss 0.220084, acc 0.9375, prec 0.070943, recall 0.872288
2017-12-10T14:12:53.416587: step 4170, loss 0.317871, acc 0.921875, prec 0.0709545, recall 0.872319
2017-12-10T14:12:53.606397: step 4171, loss 0.039699, acc 0.984375, prec 0.0709531, recall 0.872319
2017-12-10T14:12:53.792022: step 4172, loss 0.0855538, acc 0.984375, prec 0.0709517, recall 0.872319
2017-12-10T14:12:53.979472: step 4173, loss 0.166196, acc 0.90625, prec 0.0709618, recall 0.872351
2017-12-10T14:12:54.167930: step 4174, loss 0.0833927, acc 0.984375, prec 0.0709976, recall 0.872414
2017-12-10T14:12:54.357654: step 4175, loss 0.134129, acc 0.953125, prec 0.0709933, recall 0.872414
2017-12-10T14:12:54.541337: step 4176, loss 0.293106, acc 0.9375, prec 0.0709877, recall 0.872414
2017-12-10T14:12:54.727389: step 4177, loss 0.113485, acc 0.96875, prec 0.0709848, recall 0.872414
2017-12-10T14:12:54.912878: step 4178, loss 0.237391, acc 0.9375, prec 0.0709977, recall 0.872445
2017-12-10T14:12:55.100939: step 4179, loss 0.152246, acc 0.953125, prec 0.0709935, recall 0.872445
2017-12-10T14:12:55.288776: step 4180, loss 0.147456, acc 0.953125, prec 0.0709892, recall 0.872445
2017-12-10T14:12:55.478777: step 4181, loss 0.117737, acc 0.9375, prec 0.0710021, recall 0.872477
2017-12-10T14:12:55.669731: step 4182, loss 0.323869, acc 0.96875, prec 0.0710179, recall 0.872508
2017-12-10T14:12:55.861716: step 4183, loss 0.0123283, acc 1, prec 0.0710365, recall 0.872539
2017-12-10T14:12:56.051476: step 4184, loss 0.305466, acc 0.921875, prec 0.0710666, recall 0.872602
2017-12-10T14:12:56.237578: step 4185, loss 5.46315, acc 0.96875, prec 0.0710666, recall 0.872173
2017-12-10T14:12:56.431054: step 4186, loss 0.0906796, acc 0.96875, prec 0.0710637, recall 0.872173
2017-12-10T14:12:56.619922: step 4187, loss 0.0767267, acc 0.953125, prec 0.0710967, recall 0.872236
2017-12-10T14:12:56.808843: step 4188, loss 0.819822, acc 0.953125, prec 0.0711482, recall 0.87233
2017-12-10T14:12:56.997712: step 4189, loss 0.174846, acc 0.9375, prec 0.0711425, recall 0.87233
2017-12-10T14:12:57.188123: step 4190, loss 0.262214, acc 0.953125, prec 0.071194, recall 0.872424
2017-12-10T14:12:57.377206: step 4191, loss 0.54291, acc 0.859375, prec 0.0711998, recall 0.872455
2017-12-10T14:12:57.563521: step 4192, loss 0.253129, acc 0.890625, prec 0.0712084, recall 0.872487
2017-12-10T14:12:57.752737: step 4193, loss 0.355167, acc 0.890625, prec 0.071217, recall 0.872518
2017-12-10T14:12:57.938337: step 4194, loss 0.61187, acc 0.84375, prec 0.0712028, recall 0.872518
2017-12-10T14:12:58.126360: step 4195, loss 0.456376, acc 0.890625, prec 0.0711928, recall 0.872518
2017-12-10T14:12:58.318471: step 4196, loss 0.386011, acc 0.90625, prec 0.0711843, recall 0.872518
2017-12-10T14:12:58.505657: step 4197, loss 0.436321, acc 0.890625, prec 0.0711929, recall 0.872549
2017-12-10T14:12:58.698574: step 4198, loss 0.665065, acc 0.828125, prec 0.0712144, recall 0.872611
2017-12-10T14:12:58.895698: step 4199, loss 0.348319, acc 0.84375, prec 0.0712001, recall 0.872611
2017-12-10T14:12:59.089486: step 4200, loss 0.28947, acc 0.921875, prec 0.0712301, recall 0.872674
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_1/1512932334/checkpoints/model-4200

2017-12-10T14:13:00.473869: step 4201, loss 0.920818, acc 0.828125, prec 0.071233, recall 0.872705
2017-12-10T14:13:00.663394: step 4202, loss 0.410561, acc 0.875, prec 0.0712402, recall 0.872736
2017-12-10T14:13:00.854961: step 4203, loss 0.835539, acc 0.90625, prec 0.0712502, recall 0.872767
2017-12-10T14:13:01.048561: step 4204, loss 0.685475, acc 0.8125, prec 0.0712332, recall 0.872767
2017-12-10T14:13:01.240256: step 4205, loss 0.388527, acc 0.890625, prec 0.0712232, recall 0.872767
2017-12-10T14:13:01.432673: step 4206, loss 0.414927, acc 0.90625, prec 0.0712517, recall 0.87283
2017-12-10T14:13:01.622978: step 4207, loss 0.673527, acc 0.890625, prec 0.0713159, recall 0.872954
2017-12-10T14:13:01.811179: step 4208, loss 0.13657, acc 0.953125, prec 0.0713302, recall 0.872985
2017-12-10T14:13:01.999772: step 4209, loss 0.522649, acc 0.84375, prec 0.0713345, recall 0.873016
2017-12-10T14:13:02.190646: step 4210, loss 0.343241, acc 0.90625, prec 0.071326, recall 0.873016
2017-12-10T14:13:02.382688: step 4211, loss 0.706309, acc 0.859375, prec 0.0713132, recall 0.873016
2017-12-10T14:13:02.573089: step 4212, loss 0.0522609, acc 0.984375, prec 0.0713303, recall 0.873047
2017-12-10T14:13:02.762137: step 4213, loss 0.142429, acc 0.984375, prec 0.0713659, recall 0.873109
2017-12-10T14:13:02.948136: step 4214, loss 0.141451, acc 0.953125, prec 0.0713616, recall 0.873109
2017-12-10T14:13:03.136031: step 4215, loss 0.265067, acc 0.921875, prec 0.071373, recall 0.87314
2017-12-10T14:13:03.326408: step 4216, loss 0.127566, acc 0.953125, prec 0.0714243, recall 0.873233
2017-12-10T14:13:03.513264: step 4217, loss 0.0698536, acc 0.96875, prec 0.07144, recall 0.873263
2017-12-10T14:13:03.703122: step 4218, loss 0.167992, acc 0.96875, prec 0.0714556, recall 0.873294
2017-12-10T14:13:03.891292: step 4219, loss 0.0217186, acc 1, prec 0.0714556, recall 0.873294
2017-12-10T14:13:04.079530: step 4220, loss 0.268186, acc 0.984375, prec 0.0714542, recall 0.873294
2017-12-10T14:13:04.267516: step 4221, loss 0.231509, acc 0.984375, prec 0.0714713, recall 0.873325
2017-12-10T14:13:04.458546: step 4222, loss 0.111147, acc 0.984375, prec 0.0714699, recall 0.873325
2017-12-10T14:13:04.648867: step 4223, loss 0.0318162, acc 0.984375, prec 0.071524, recall 0.873418
2017-12-10T14:13:04.841977: step 4224, loss 0.0182679, acc 1, prec 0.071524, recall 0.873418
2017-12-10T14:13:05.033659: step 4225, loss 0.173474, acc 1, prec 0.0715795, recall 0.87351
2017-12-10T14:13:05.229643: step 4226, loss 0.0501764, acc 0.984375, prec 0.0715781, recall 0.87351
2017-12-10T14:13:05.416139: step 4227, loss 0.579998, acc 0.984375, prec 0.0716322, recall 0.873602
2017-12-10T14:13:05.610811: step 4228, loss 0.12051, acc 0.984375, prec 0.0716677, recall 0.873664
2017-12-10T14:13:05.800068: step 4229, loss 0.06086, acc 0.984375, prec 0.0716663, recall 0.873664
2017-12-10T14:13:05.990959: step 4230, loss 0.2189, acc 0.9375, prec 0.0716791, recall 0.873694
2017-12-10T14:13:06.182739: step 4231, loss 0.255019, acc 0.921875, prec 0.0717089, recall 0.873756
2017-12-10T14:13:06.373823: step 4232, loss 0.107833, acc 0.96875, prec 0.0717061, recall 0.873756
2017-12-10T14:13:06.566517: step 4233, loss 0.46968, acc 0.96875, prec 0.0717402, recall 0.873817
2017-12-10T14:13:06.760795: step 4234, loss 0.008157, acc 1, prec 0.0717402, recall 0.873817
2017-12-10T14:13:06.945910: step 4235, loss 0.095178, acc 0.984375, prec 0.0717388, recall 0.873817
2017-12-10T14:13:07.134889: step 4236, loss 0.116649, acc 0.9375, prec 0.0717331, recall 0.873817
2017-12-10T14:13:07.322639: step 4237, loss 2.32591, acc 0.9375, prec 0.0717658, recall 0.873666
2017-12-10T14:13:07.514963: step 4238, loss 0.209711, acc 0.953125, prec 0.0717615, recall 0.873666
2017-12-10T14:13:07.703076: step 4239, loss 0.256902, acc 0.953125, prec 0.0717572, recall 0.873666
2017-12-10T14:13:07.891611: step 4240, loss 0.581363, acc 0.921875, prec 0.0717685, recall 0.873697
2017-12-10T14:13:08.085520: step 4241, loss 0.0620611, acc 0.96875, prec 0.0718026, recall 0.873758
2017-12-10T14:13:08.277930: step 4242, loss 0.116208, acc 0.9375, prec 0.0718524, recall 0.87385
2017-12-10T14:13:08.471577: step 4243, loss 0.268243, acc 0.890625, prec 0.0718608, recall 0.87388
2017-12-10T14:13:08.661454: step 4244, loss 0.263968, acc 0.90625, prec 0.0718707, recall 0.873911
2017-12-10T14:13:08.849592: step 4245, loss 0.398387, acc 0.921875, prec 0.0718636, recall 0.873911
2017-12-10T14:13:09.037447: step 4246, loss 0.284171, acc 0.875, prec 0.0718891, recall 0.873972
2017-12-10T14:13:09.226985: step 4247, loss 0.188442, acc 0.953125, prec 0.0719032, recall 0.874002
2017-12-10T14:13:09.418288: step 4248, loss 0.30849, acc 0.921875, prec 0.071933, recall 0.874063
2017-12-10T14:13:09.607688: step 4249, loss 0.179626, acc 0.921875, prec 0.0719443, recall 0.874094
2017-12-10T14:13:09.793397: step 4250, loss 0.64762, acc 0.90625, prec 0.0719542, recall 0.874124
2017-12-10T14:13:09.983103: step 4251, loss 0.537641, acc 0.90625, prec 0.072001, recall 0.874215
2017-12-10T14:13:10.170484: step 4252, loss 0.315645, acc 0.875, prec 0.0719895, recall 0.874215
2017-12-10T14:13:10.354827: step 4253, loss 0.46156, acc 0.890625, prec 0.0720164, recall 0.874276
2017-12-10T14:13:10.544415: step 4254, loss 0.494191, acc 0.875, prec 0.0720418, recall 0.874337
2017-12-10T14:13:10.730977: step 4255, loss 0.350195, acc 0.90625, prec 0.0720332, recall 0.874337
2017-12-10T14:13:10.921643: step 4256, loss 0.24116, acc 0.9375, prec 0.0720459, recall 0.874367
2017-12-10T14:13:11.113572: step 4257, loss 0.20008, acc 0.953125, prec 0.0720785, recall 0.874428
2017-12-10T14:13:11.299361: step 4258, loss 0.0829188, acc 0.96875, prec 0.0721494, recall 0.874548
2017-12-10T14:13:11.491684: step 4259, loss 0.241603, acc 0.96875, prec 0.0721834, recall 0.874609
2017-12-10T14:13:11.680565: step 4260, loss 0.410664, acc 0.90625, prec 0.0721932, recall 0.874639
2017-12-10T14:13:11.866141: step 4261, loss 0.37886, acc 0.921875, prec 0.072186, recall 0.874639
2017-12-10T14:13:12.056900: step 4262, loss 0.104493, acc 0.96875, prec 0.0722016, recall 0.874669
2017-12-10T14:13:12.249096: step 4263, loss 0.148608, acc 0.953125, prec 0.0722157, recall 0.874699
2017-12-10T14:13:12.434280: step 4264, loss 0.0383318, acc 0.984375, prec 0.0722327, recall 0.87473
2017-12-10T14:13:12.622082: step 4265, loss 0.121582, acc 0.953125, prec 0.0722468, recall 0.87476
2017-12-10T14:13:12.809139: step 4266, loss 0.131254, acc 0.953125, prec 0.0722609, recall 0.87479
2017-12-10T14:13:12.999629: step 4267, loss 0.215199, acc 0.96875, prec 0.0722765, recall 0.87482
2017-12-10T14:13:13.183800: step 4268, loss 0.0211791, acc 1, prec 0.0722949, recall 0.87485
2017-12-10T14:13:13.372665: step 4269, loss 0.245167, acc 0.953125, prec 0.072309, recall 0.87488
2017-12-10T14:13:13.564655: step 4270, loss 0.0630211, acc 0.984375, prec 0.0723076, recall 0.87488
2017-12-10T14:13:13.752028: step 4271, loss 0.146585, acc 0.9375, prec 0.0723018, recall 0.87488
2017-12-10T14:13:13.944998: step 4272, loss 0.465278, acc 0.953125, prec 0.0723159, recall 0.87491
2017-12-10T14:13:14.141733: step 4273, loss 0.0575481, acc 0.984375, prec 0.0723329, recall 0.87494
2017-12-10T14:13:14.328440: step 4274, loss 0.635092, acc 0.921875, prec 0.0723441, recall 0.87497
2017-12-10T14:13:14.519581: step 4275, loss 0.0727752, acc 0.984375, prec 0.0723611, recall 0.875
2017-12-10T14:13:14.710954: step 4276, loss 0.133699, acc 0.96875, prec 0.0723582, recall 0.875
2017-12-10T14:13:14.900571: step 4277, loss 1.56253, acc 0.9375, prec 0.0723723, recall 0.87482
2017-12-10T14:13:15.091700: step 4278, loss 0.136594, acc 0.96875, prec 0.0724247, recall 0.87491
2017-12-10T14:13:15.278590: step 4279, loss 0.0356692, acc 0.984375, prec 0.0724232, recall 0.87491
2017-12-10T14:13:15.464637: step 4280, loss 0.133773, acc 0.96875, prec 0.0724756, recall 0.875
2017-12-10T14:13:15.656361: step 4281, loss 0.0533575, acc 0.96875, prec 0.0725095, recall 0.87506
2017-12-10T14:13:15.847386: step 4282, loss 0.225051, acc 0.96875, prec 0.0725434, recall 0.87512
2017-12-10T14:13:16.035467: step 4283, loss 0.160287, acc 0.9375, prec 0.072556, recall 0.875149
2017-12-10T14:13:16.226231: step 4284, loss 0.104264, acc 0.953125, prec 0.0725701, recall 0.875179
2017-12-10T14:13:16.414917: step 4285, loss 0.522414, acc 0.890625, prec 0.07256, recall 0.875179
2017-12-10T14:13:16.601436: step 4286, loss 0.157057, acc 0.9375, prec 0.0725726, recall 0.875209
2017-12-10T14:13:16.793783: step 4287, loss 0.23859, acc 0.9375, prec 0.0725853, recall 0.875239
2017-12-10T14:13:16.988045: step 4288, loss 0.120742, acc 0.9375, prec 0.0725795, recall 0.875239
2017-12-10T14:13:17.177561: step 4289, loss 0.223028, acc 0.9375, prec 0.0725738, recall 0.875239
2017-12-10T14:13:17.366978: step 4290, loss 0.184579, acc 0.9375, prec 0.072568, recall 0.875239
2017-12-10T14:13:17.555002: step 4291, loss 0.105323, acc 0.953125, prec 0.0725637, recall 0.875239
2017-12-10T14:13:17.741450: step 4292, loss 0.1216, acc 0.96875, prec 0.0725976, recall 0.875299
2017-12-10T14:13:17.927888: step 4293, loss 0.0968892, acc 0.953125, prec 0.0726116, recall 0.875328
2017-12-10T14:13:18.117002: step 4294, loss 0.205313, acc 0.9375, prec 0.0726059, recall 0.875328
2017-12-10T14:13:18.310229: step 4295, loss 0.314554, acc 0.953125, prec 0.0726199, recall 0.875358
2017-12-10T14:13:18.501722: step 4296, loss 0.32117, acc 0.953125, prec 0.072634, recall 0.875388
2017-12-10T14:13:18.691955: step 4297, loss 0.087067, acc 0.953125, prec 0.0726664, recall 0.875447
2017-12-10T14:13:18.878869: step 4298, loss 0.0365344, acc 0.984375, prec 0.0727017, recall 0.875507
2017-12-10T14:13:19.071928: step 4299, loss 0.0309929, acc 1, prec 0.0727201, recall 0.875537
2017-12-10T14:13:19.258089: step 4300, loss 0.0814622, acc 0.953125, prec 0.0727525, recall 0.875596
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_1/1512932334/checkpoints/model-4300

2017-12-10T14:13:20.472826: step 4301, loss 0.0575383, acc 0.984375, prec 0.072751, recall 0.875596
2017-12-10T14:13:20.662463: step 4302, loss 0.16526, acc 0.96875, prec 0.0727482, recall 0.875596
2017-12-10T14:13:20.852651: step 4303, loss 0.0570165, acc 0.96875, prec 0.072782, recall 0.875655
2017-12-10T14:13:21.042962: step 4304, loss 0.0945112, acc 0.96875, prec 0.0727791, recall 0.875655
2017-12-10T14:13:21.229793: step 4305, loss 0.312937, acc 0.953125, prec 0.0728115, recall 0.875714
2017-12-10T14:13:21.419683: step 4306, loss 0.0278192, acc 0.984375, prec 0.0728468, recall 0.875773
2017-12-10T14:13:21.609050: step 4307, loss 0.133092, acc 0.984375, prec 0.0728453, recall 0.875773
2017-12-10T14:13:21.798966: step 4308, loss 0.136652, acc 0.984375, prec 0.0728439, recall 0.875773
2017-12-10T14:13:21.987254: step 4309, loss 0.0282013, acc 0.984375, prec 0.0728791, recall 0.875833
2017-12-10T14:13:22.183628: step 4310, loss 0.94961, acc 0.96875, prec 0.0728946, recall 0.875862
2017-12-10T14:13:22.375514: step 4311, loss 0.00883408, acc 1, prec 0.0728946, recall 0.875862
2017-12-10T14:13:22.561372: step 4312, loss 0.105784, acc 0.984375, prec 0.0728932, recall 0.875862
2017-12-10T14:13:22.749363: step 4313, loss 0.102556, acc 0.96875, prec 0.072927, recall 0.875921
2017-12-10T14:13:22.937226: step 4314, loss 0.125666, acc 0.953125, prec 0.0729226, recall 0.875921
2017-12-10T14:13:23.123774: step 4315, loss 0.153687, acc 0.9375, prec 0.0729169, recall 0.875921
2017-12-10T14:13:23.315274: step 4316, loss 0.142179, acc 0.96875, prec 0.0729507, recall 0.87598
2017-12-10T14:13:23.505470: step 4317, loss 0.0343432, acc 0.984375, prec 0.0729492, recall 0.87598
2017-12-10T14:13:23.696309: step 4318, loss 0.152554, acc 0.953125, prec 0.0729632, recall 0.87601
2017-12-10T14:13:23.885594: step 4319, loss 0.0677912, acc 0.984375, prec 0.0729801, recall 0.876039
2017-12-10T14:13:24.074400: step 4320, loss 0.228007, acc 0.953125, prec 0.0729941, recall 0.876068
2017-12-10T14:13:24.262032: step 4321, loss 0.0635974, acc 0.953125, prec 0.0729898, recall 0.876068
2017-12-10T14:13:24.450710: step 4322, loss 0.00707689, acc 1, prec 0.0730081, recall 0.876098
2017-12-10T14:13:24.640635: step 4323, loss 0.0947481, acc 0.921875, prec 0.0730193, recall 0.876127
2017-12-10T14:13:24.831437: step 4324, loss 0.0512168, acc 0.96875, prec 0.073053, recall 0.876186
2017-12-10T14:13:25.018683: step 4325, loss 0.417203, acc 0.9375, prec 0.0730656, recall 0.876215
2017-12-10T14:13:25.208772: step 4326, loss 0.15317, acc 0.96875, prec 0.073081, recall 0.876245
2017-12-10T14:13:25.398709: step 4327, loss 0.246358, acc 0.96875, prec 0.0730781, recall 0.876245
2017-12-10T14:13:25.587569: step 4328, loss 0.260362, acc 0.953125, prec 0.0730738, recall 0.876245
2017-12-10T14:13:25.776958: step 4329, loss 0.118958, acc 0.953125, prec 0.0731428, recall 0.876362
2017-12-10T14:13:25.966201: step 4330, loss 0.315882, acc 0.9375, prec 0.0731736, recall 0.87642
2017-12-10T14:13:26.156896: step 4331, loss 0.350437, acc 0.921875, prec 0.0731847, recall 0.87645
2017-12-10T14:13:26.343083: step 4332, loss 0.335627, acc 0.953125, prec 0.073217, recall 0.876508
2017-12-10T14:13:26.534943: step 4333, loss 0.013559, acc 1, prec 0.073217, recall 0.876508
2017-12-10T14:13:26.726985: step 4334, loss 0.0410141, acc 0.984375, prec 0.0732156, recall 0.876508
2017-12-10T14:13:26.915910: step 4335, loss 0.192142, acc 0.953125, prec 0.0732478, recall 0.876567
2017-12-10T14:13:27.102680: step 4336, loss 0.0983901, acc 0.953125, prec 0.0732435, recall 0.876567
2017-12-10T14:13:27.290568: step 4337, loss 0.284263, acc 0.9375, prec 0.0732377, recall 0.876567
2017-12-10T14:13:27.482381: step 4338, loss 0.0353865, acc 0.984375, prec 0.0732546, recall 0.876596
2017-12-10T14:13:27.669031: step 4339, loss 0.0166343, acc 1, prec 0.0732546, recall 0.876596
2017-12-10T14:13:27.855139: step 4340, loss 0.00825349, acc 1, prec 0.0732729, recall 0.876625
2017-12-10T14:13:28.042764: step 4341, loss 0.816047, acc 0.984375, prec 0.0732897, recall 0.876654
2017-12-10T14:13:28.236988: step 4342, loss 0.0599476, acc 0.984375, prec 0.0733249, recall 0.876712
2017-12-10T14:13:28.427983: step 4343, loss 0.099027, acc 0.984375, prec 0.0733418, recall 0.876741
2017-12-10T14:13:28.619791: step 4344, loss 0.102592, acc 0.984375, prec 0.0733586, recall 0.876771
2017-12-10T14:13:28.815820: step 4345, loss 0.225947, acc 0.953125, prec 0.0734275, recall 0.876887
2017-12-10T14:13:29.017034: step 4346, loss 0.0691721, acc 0.96875, prec 0.0734246, recall 0.876887
2017-12-10T14:13:29.205849: step 4347, loss 0.204123, acc 0.953125, prec 0.0734202, recall 0.876887
2017-12-10T14:13:29.397289: step 4348, loss 0.171945, acc 0.984375, prec 0.0734371, recall 0.876916
2017-12-10T14:13:29.585574: step 4349, loss 0.0942207, acc 0.953125, prec 0.073451, recall 0.876945
2017-12-10T14:13:29.777699: step 4350, loss 0.0176962, acc 1, prec 0.073451, recall 0.876945
2017-12-10T14:13:29.969235: step 4351, loss 0.0238014, acc 1, prec 0.0734693, recall 0.876974
2017-12-10T14:13:30.157876: step 4352, loss 0.090881, acc 0.96875, prec 0.0734847, recall 0.877003
2017-12-10T14:13:30.347624: step 4353, loss 0.0984464, acc 0.953125, prec 0.0734803, recall 0.877003
2017-12-10T14:13:30.536137: step 4354, loss 0.0165633, acc 1, prec 0.0734803, recall 0.877003
2017-12-10T14:13:30.725086: step 4355, loss 0.0670173, acc 0.96875, prec 0.0734774, recall 0.877003
2017-12-10T14:13:30.913366: step 4356, loss 0.0203167, acc 1, prec 0.073514, recall 0.877061
2017-12-10T14:13:31.103475: step 4357, loss 0.0457293, acc 0.984375, prec 0.0735492, recall 0.877119
2017-12-10T14:13:31.298315: step 4358, loss 0.204904, acc 1, prec 0.073604, recall 0.877205
2017-12-10T14:13:31.495385: step 4359, loss 0.0449588, acc 0.984375, prec 0.0736208, recall 0.877234
2017-12-10T14:13:31.685894: step 4360, loss 0.223161, acc 0.9375, prec 0.0736516, recall 0.877292
2017-12-10T14:13:31.874001: step 4361, loss 0.0680539, acc 0.96875, prec 0.0736487, recall 0.877292
2017-12-10T14:13:32.059087: step 4362, loss 0.147808, acc 0.984375, prec 0.0736472, recall 0.877292
2017-12-10T14:13:32.246357: step 4363, loss 0.0832748, acc 0.96875, prec 0.0736992, recall 0.877378
2017-12-10T14:13:32.436672: step 4364, loss 0.18053, acc 0.96875, prec 0.0737145, recall 0.877407
2017-12-10T14:13:32.625380: step 4365, loss 0.387372, acc 0.953125, prec 0.0737467, recall 0.877465
2017-12-10T14:13:32.814804: step 4366, loss 0.107538, acc 0.953125, prec 0.0737424, recall 0.877465
2017-12-10T14:13:33.003237: step 4367, loss 0.0777638, acc 0.96875, prec 0.073776, recall 0.877522
2017-12-10T14:13:33.195330: step 4368, loss 0.178536, acc 0.9375, prec 0.0738067, recall 0.87758
2017-12-10T14:13:33.383340: step 4369, loss 0.291897, acc 0.96875, prec 0.0738221, recall 0.877608
2017-12-10T14:13:33.574939: step 4370, loss 0.0176061, acc 0.984375, prec 0.0738206, recall 0.877608
2017-12-10T14:13:33.765812: step 4371, loss 0.0389381, acc 0.984375, prec 0.0738374, recall 0.877637
2017-12-10T14:13:33.952184: step 4372, loss 0.159043, acc 0.953125, prec 0.0738513, recall 0.877666
2017-12-10T14:13:34.139954: step 4373, loss 0.19451, acc 0.9375, prec 0.0738637, recall 0.877694
2017-12-10T14:13:34.327802: step 4374, loss 0.226648, acc 0.953125, prec 0.0738959, recall 0.877752
2017-12-10T14:13:34.516566: step 4375, loss 0.462487, acc 0.9375, prec 0.0739083, recall 0.87778
2017-12-10T14:13:34.703566: step 4376, loss 0.281679, acc 0.921875, prec 0.073901, recall 0.87778
2017-12-10T14:13:34.892431: step 4377, loss 0.135713, acc 0.96875, prec 0.0738981, recall 0.87778
2017-12-10T14:13:35.084183: step 4378, loss 0.0864594, acc 0.96875, prec 0.0738952, recall 0.87778
2017-12-10T14:13:35.276153: step 4379, loss 0.236873, acc 0.96875, prec 0.0739106, recall 0.877809
2017-12-10T14:13:35.468876: step 4380, loss 0.0300558, acc 0.984375, prec 0.0739274, recall 0.877838
2017-12-10T14:13:35.653913: step 4381, loss 0.0658102, acc 0.96875, prec 0.0739244, recall 0.877838
2017-12-10T14:13:35.838869: step 4382, loss 0.00882265, acc 1, prec 0.0739427, recall 0.877866
2017-12-10T14:13:36.025774: step 4383, loss 0.0674454, acc 0.984375, prec 0.0739595, recall 0.877895
2017-12-10T14:13:36.214959: step 4384, loss 0.569835, acc 1, prec 0.0740325, recall 0.878009
2017-12-10T14:13:36.405217: step 4385, loss 0.046335, acc 0.96875, prec 0.0740296, recall 0.878009
2017-12-10T14:13:36.592687: step 4386, loss 0.244136, acc 0.921875, prec 0.0740405, recall 0.878037
2017-12-10T14:13:36.779672: step 4387, loss 0.298356, acc 0.9375, prec 0.0740347, recall 0.878037
2017-12-10T14:13:36.974629: step 4388, loss 0.0100098, acc 1, prec 0.0740347, recall 0.878037
2017-12-10T14:13:37.161928: step 4389, loss 0.0705214, acc 0.96875, prec 0.0740318, recall 0.878037
2017-12-10T14:13:37.352641: step 4390, loss 0.0548775, acc 0.984375, prec 0.0740485, recall 0.878066
2017-12-10T14:13:37.541194: step 4391, loss 0.146701, acc 0.9375, prec 0.0740427, recall 0.878066
2017-12-10T14:13:37.732111: step 4392, loss 0.0155301, acc 1, prec 0.0740427, recall 0.878066
2017-12-10T14:13:37.921688: step 4393, loss 0.428694, acc 0.953125, prec 0.074093, recall 0.878151
2017-12-10T14:13:38.114831: step 4394, loss 0.015631, acc 1, prec 0.074093, recall 0.878151
2017-12-10T14:13:38.303122: step 4395, loss 0.263072, acc 0.984375, prec 0.0741098, recall 0.87818
2017-12-10T14:13:38.492575: step 4396, loss 0.332814, acc 0.984375, prec 0.0741448, recall 0.878237
2017-12-10T14:13:38.686230: step 4397, loss 0.0606783, acc 0.96875, prec 0.0741419, recall 0.878237
2017-12-10T14:13:38.876100: step 4398, loss 0.196902, acc 0.9375, prec 0.0741725, recall 0.878293
2017-12-10T14:13:39.065375: step 4399, loss 0.300183, acc 0.921875, prec 0.0741834, recall 0.878322
2017-12-10T14:13:39.251977: step 4400, loss 0.155789, acc 0.953125, prec 0.0741973, recall 0.87835
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_1/1512932334/checkpoints/model-4400

2017-12-10T14:13:40.868542: step 4401, loss 0.166836, acc 0.953125, prec 0.0742294, recall 0.878407
2017-12-10T14:13:41.059822: step 4402, loss 0.121652, acc 0.96875, prec 0.0742264, recall 0.878407
2017-12-10T14:13:41.247393: step 4403, loss 0.0521453, acc 0.96875, prec 0.0742235, recall 0.878407
2017-12-10T14:13:41.433891: step 4404, loss 0.384479, acc 0.9375, prec 0.0742359, recall 0.878435
2017-12-10T14:13:41.623444: step 4405, loss 0.155263, acc 0.921875, prec 0.0742468, recall 0.878463
2017-12-10T14:13:41.811422: step 4406, loss 0.0308418, acc 1, prec 0.074265, recall 0.878492
2017-12-10T14:13:41.998535: step 4407, loss 0.226406, acc 0.984375, prec 0.0743182, recall 0.878576
2017-12-10T14:13:42.190369: step 4408, loss 0.113183, acc 0.953125, prec 0.0743685, recall 0.878661
2017-12-10T14:13:42.378503: step 4409, loss 0.221399, acc 0.9375, prec 0.0743808, recall 0.878689
2017-12-10T14:13:42.566962: step 4410, loss 0.160547, acc 0.96875, prec 0.0743779, recall 0.878689
2017-12-10T14:13:42.754757: step 4411, loss 0.106016, acc 0.984375, prec 0.0744128, recall 0.878746
2017-12-10T14:13:42.942778: step 4412, loss 0.0571578, acc 0.984375, prec 0.0744114, recall 0.878746
2017-12-10T14:13:43.133652: step 4413, loss 0.156503, acc 0.96875, prec 0.0744267, recall 0.878774
2017-12-10T14:13:43.319959: step 4414, loss 0.164966, acc 0.953125, prec 0.0744405, recall 0.878802
2017-12-10T14:13:43.507569: step 4415, loss 0.0736447, acc 0.96875, prec 0.0744375, recall 0.878802
2017-12-10T14:13:43.695093: step 4416, loss 0.0275515, acc 1, prec 0.0744375, recall 0.878802
2017-12-10T14:13:43.887473: step 4417, loss 0.118824, acc 0.96875, prec 0.0744346, recall 0.878802
2017-12-10T14:13:44.084055: step 4418, loss 0.096219, acc 0.953125, prec 0.0744484, recall 0.87883
2017-12-10T14:13:44.271370: step 4419, loss 0.149457, acc 0.96875, prec 0.0744637, recall 0.878858
2017-12-10T14:13:44.460099: step 4420, loss 0.00781978, acc 1, prec 0.0744637, recall 0.878858
2017-12-10T14:13:44.650331: step 4421, loss 0.536307, acc 0.984375, prec 0.0744986, recall 0.878914
2017-12-10T14:13:44.840824: step 4422, loss 0.212406, acc 0.96875, prec 0.0745139, recall 0.878942
2017-12-10T14:13:45.033479: step 4423, loss 0.0866436, acc 0.96875, prec 0.0745292, recall 0.878971
2017-12-10T14:13:45.226006: step 4424, loss 0.0457696, acc 0.984375, prec 0.0745641, recall 0.879027
2017-12-10T14:13:45.420808: step 4425, loss 0.690375, acc 0.96875, prec 0.0745975, recall 0.879083
2017-12-10T14:13:45.610681: step 4426, loss 0.0525915, acc 0.984375, prec 0.0745961, recall 0.879083
2017-12-10T14:13:45.801088: step 4427, loss 0.00648111, acc 1, prec 0.0745961, recall 0.879083
2017-12-10T14:13:45.992455: step 4428, loss 0.0505581, acc 0.984375, prec 0.0745946, recall 0.879083
2017-12-10T14:13:46.181146: step 4429, loss 0.3432, acc 0.96875, prec 0.074628, recall 0.879139
2017-12-10T14:13:46.376415: step 4430, loss 0.173845, acc 0.96875, prec 0.0746433, recall 0.879167
2017-12-10T14:13:46.565840: step 4431, loss 0.469416, acc 0.90625, prec 0.0746527, recall 0.879195
2017-12-10T14:13:46.750644: step 4432, loss 0.24232, acc 0.890625, prec 0.0746424, recall 0.879195
2017-12-10T14:13:46.940072: step 4433, loss 0.612939, acc 0.890625, prec 0.0746685, recall 0.879251
2017-12-10T14:13:47.132008: step 4434, loss 0.113019, acc 0.9375, prec 0.0746626, recall 0.879251
2017-12-10T14:13:47.315637: step 4435, loss 0.411066, acc 0.90625, prec 0.074672, recall 0.879278
2017-12-10T14:13:47.505719: step 4436, loss 0.0685233, acc 0.984375, prec 0.0747069, recall 0.879334
2017-12-10T14:13:47.695588: step 4437, loss 0.045466, acc 0.96875, prec 0.0747221, recall 0.879362
2017-12-10T14:13:47.884057: step 4438, loss 0.348763, acc 0.96875, prec 0.0747737, recall 0.879446
2017-12-10T14:13:48.075552: step 4439, loss 0.448928, acc 0.921875, prec 0.0747845, recall 0.879474
2017-12-10T14:13:48.266549: step 4440, loss 0.168732, acc 0.9375, prec 0.0747968, recall 0.879501
2017-12-10T14:13:48.455719: step 4441, loss 0.458407, acc 0.921875, prec 0.0747895, recall 0.879501
2017-12-10T14:13:48.645512: step 4442, loss 0.0635041, acc 0.96875, prec 0.0748047, recall 0.879529
2017-12-10T14:13:48.834468: step 4443, loss 0.478162, acc 0.921875, prec 0.0748155, recall 0.879557
2017-12-10T14:13:49.021880: step 4444, loss 0.0377582, acc 0.96875, prec 0.0748126, recall 0.879557
2017-12-10T14:13:49.209958: step 4445, loss 0.37637, acc 0.953125, prec 0.0748445, recall 0.879613
2017-12-10T14:13:49.399634: step 4446, loss 0.0126545, acc 1, prec 0.0748626, recall 0.87964
2017-12-10T14:13:49.584088: step 4447, loss 0.398075, acc 0.96875, prec 0.0749142, recall 0.879723
2017-12-10T14:13:49.774488: step 4448, loss 2.8355, acc 0.953125, prec 0.0749112, recall 0.879521
2017-12-10T14:13:49.966939: step 4449, loss 0.40368, acc 0.890625, prec 0.0749191, recall 0.879549
2017-12-10T14:13:50.159922: step 4450, loss 0.234868, acc 0.953125, prec 0.0749328, recall 0.879576
2017-12-10T14:13:50.352075: step 4451, loss 0.350266, acc 0.90625, prec 0.074924, recall 0.879576
2017-12-10T14:13:50.542649: step 4452, loss 0.0863077, acc 0.984375, prec 0.0749407, recall 0.879604
2017-12-10T14:13:50.731298: step 4453, loss 0.0566223, acc 0.953125, prec 0.0749363, recall 0.879604
2017-12-10T14:13:50.921770: step 4454, loss 0.357341, acc 0.9375, prec 0.0749304, recall 0.879604
2017-12-10T14:13:51.110476: step 4455, loss 0.276174, acc 0.921875, prec 0.074923, recall 0.879604
2017-12-10T14:13:51.301279: step 4456, loss 0.294336, acc 0.9375, prec 0.0749353, recall 0.879632
2017-12-10T14:13:51.486447: step 4457, loss 0.206125, acc 0.953125, prec 0.0749672, recall 0.879687
2017-12-10T14:13:51.680150: step 4458, loss 0.336915, acc 0.875, prec 0.0749554, recall 0.879687
2017-12-10T14:13:51.869322: step 4459, loss 0.368224, acc 0.890625, prec 0.0749451, recall 0.879687
2017-12-10T14:13:52.057870: step 4460, loss 0.425711, acc 0.875, prec 0.0749334, recall 0.879687
2017-12-10T14:13:52.249943: step 4461, loss 0.19719, acc 0.921875, prec 0.0749623, recall 0.879742
2017-12-10T14:13:52.434840: step 4462, loss 0.132476, acc 0.953125, prec 0.074976, recall 0.87977
2017-12-10T14:13:52.622239: step 4463, loss 0.560571, acc 0.859375, prec 0.0749628, recall 0.87977
2017-12-10T14:13:52.806587: step 4464, loss 0.562145, acc 0.875, prec 0.0749873, recall 0.879825
2017-12-10T14:13:52.996357: step 4465, loss 0.365767, acc 0.875, prec 0.0749755, recall 0.879825
2017-12-10T14:13:53.184272: step 4466, loss 0.264911, acc 0.921875, prec 0.0749682, recall 0.879825
2017-12-10T14:13:53.374776: step 4467, loss 0.0558, acc 0.96875, prec 0.0749652, recall 0.879825
2017-12-10T14:13:53.567064: step 4468, loss 0.319096, acc 0.9375, prec 0.0749594, recall 0.879825
2017-12-10T14:13:53.752504: step 4469, loss 0.251605, acc 0.890625, prec 0.0749491, recall 0.879825
2017-12-10T14:13:53.943818: step 4470, loss 0.262084, acc 0.921875, prec 0.0749418, recall 0.879825
2017-12-10T14:13:54.131344: step 4471, loss 0.235795, acc 0.984375, prec 0.0749584, recall 0.879853
2017-12-10T14:13:54.323323: step 4472, loss 0.00426802, acc 1, prec 0.0749765, recall 0.879881
2017-12-10T14:13:54.490395: step 4473, loss 0.103427, acc 0.941176, prec 0.0749721, recall 0.879881
2017-12-10T14:13:54.689451: step 4474, loss 0.0749357, acc 0.96875, prec 0.0749873, recall 0.879908
2017-12-10T14:13:54.879542: step 4475, loss 0.106741, acc 0.96875, prec 0.0749843, recall 0.879908
2017-12-10T14:13:55.069842: step 4476, loss 0.175559, acc 0.9375, prec 0.0749785, recall 0.879908
2017-12-10T14:13:55.263381: step 4477, loss 0.204811, acc 0.90625, prec 0.0749697, recall 0.879908
2017-12-10T14:13:55.453446: step 4478, loss 0.374184, acc 0.953125, prec 0.0749653, recall 0.879908
2017-12-10T14:13:55.638516: step 4479, loss 0.133971, acc 0.96875, prec 0.0749623, recall 0.879908
2017-12-10T14:13:55.831961: step 4480, loss 0.789867, acc 0.96875, prec 0.0749775, recall 0.879936
2017-12-10T14:13:56.027499: step 4481, loss 0.0952566, acc 0.984375, prec 0.074976, recall 0.879936
2017-12-10T14:13:56.214209: step 4482, loss 0.242617, acc 1, prec 0.0750122, recall 0.879991
2017-12-10T14:13:56.409859: step 4483, loss 0.0791315, acc 0.984375, prec 0.0750289, recall 0.880018
2017-12-10T14:13:56.599953: step 4484, loss 0.0213536, acc 0.984375, prec 0.0750455, recall 0.880046
2017-12-10T14:13:56.792221: step 4485, loss 0.0277052, acc 1, prec 0.0750455, recall 0.880046
2017-12-10T14:13:56.978930: step 4486, loss 0.0658672, acc 0.984375, prec 0.075044, recall 0.880046
2017-12-10T14:13:57.168203: step 4487, loss 0.114755, acc 0.96875, prec 0.0750592, recall 0.880073
2017-12-10T14:13:57.356237: step 4488, loss 0.0750696, acc 0.96875, prec 0.0750562, recall 0.880073
2017-12-10T14:13:57.545320: step 4489, loss 0.156586, acc 0.953125, prec 0.0750518, recall 0.880073
2017-12-10T14:13:57.733867: step 4490, loss 0.0862483, acc 0.96875, prec 0.075067, recall 0.880101
2017-12-10T14:13:57.923033: step 4491, loss 0.0790213, acc 0.984375, prec 0.0751017, recall 0.880156
2017-12-10T14:13:58.113893: step 4492, loss 0.119655, acc 0.953125, prec 0.0750973, recall 0.880156
2017-12-10T14:13:58.304150: step 4493, loss 0.0918069, acc 0.96875, prec 0.0750943, recall 0.880156
2017-12-10T14:13:58.490257: step 4494, loss 0.0525535, acc 0.96875, prec 0.0751095, recall 0.880183
2017-12-10T14:13:58.678592: step 4495, loss 0.0840873, acc 0.984375, prec 0.0751261, recall 0.880211
2017-12-10T14:13:58.872299: step 4496, loss 0.134981, acc 0.984375, prec 0.0751427, recall 0.880238
2017-12-10T14:13:59.067864: step 4497, loss 0.141512, acc 0.984375, prec 0.0751412, recall 0.880238
2017-12-10T14:13:59.254006: step 4498, loss 0.683392, acc 0.9375, prec 0.0751534, recall 0.880266
2017-12-10T14:13:59.446086: step 4499, loss 0.0670473, acc 0.984375, prec 0.0752062, recall 0.880348
2017-12-10T14:13:59.636544: step 4500, loss 0.237308, acc 0.953125, prec 0.0752379, recall 0.880402
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_1/1512932334/checkpoints/model-4500

2017-12-10T14:14:01.505768: step 4501, loss 2.36858, acc 0.984375, prec 0.075256, recall 0.880229
2017-12-10T14:14:01.698891: step 4502, loss 0.184093, acc 0.96875, prec 0.0752531, recall 0.880229
2017-12-10T14:14:01.886122: step 4503, loss 0.308229, acc 0.9375, prec 0.0752472, recall 0.880229
2017-12-10T14:14:02.077602: step 4504, loss 0.213488, acc 0.984375, prec 0.0752999, recall 0.880311
2017-12-10T14:14:02.265663: step 4505, loss 0.39712, acc 0.9375, prec 0.075294, recall 0.880311
2017-12-10T14:14:02.456739: step 4506, loss 0.350729, acc 0.9375, prec 0.0752881, recall 0.880311
2017-12-10T14:14:02.646009: step 4507, loss 0.125512, acc 0.953125, prec 0.0752837, recall 0.880311
2017-12-10T14:14:02.838409: step 4508, loss 0.20915, acc 0.90625, prec 0.075293, recall 0.880338
2017-12-10T14:14:03.027689: step 4509, loss 0.363438, acc 0.921875, prec 0.0753037, recall 0.880365
2017-12-10T14:14:03.213327: step 4510, loss 0.211837, acc 0.953125, prec 0.0753173, recall 0.880393
2017-12-10T14:14:03.402339: step 4511, loss 0.165823, acc 0.9375, prec 0.0753114, recall 0.880393
2017-12-10T14:14:03.593031: step 4512, loss 0.328816, acc 0.90625, prec 0.0753207, recall 0.88042
2017-12-10T14:14:03.780822: step 4513, loss 0.496936, acc 0.890625, prec 0.0753284, recall 0.880447
2017-12-10T14:14:03.967567: step 4514, loss 0.356764, acc 0.90625, prec 0.0753196, recall 0.880447
2017-12-10T14:14:04.157695: step 4515, loss 0.21627, acc 0.9375, prec 0.0753498, recall 0.880502
2017-12-10T14:14:04.350003: step 4516, loss 0.21, acc 0.9375, prec 0.0753439, recall 0.880502
2017-12-10T14:14:04.539436: step 4517, loss 0.410267, acc 0.890625, prec 0.0753336, recall 0.880502
2017-12-10T14:14:04.727672: step 4518, loss 0.197621, acc 0.9375, prec 0.0753819, recall 0.880583
2017-12-10T14:14:04.915609: step 4519, loss 0.164931, acc 0.984375, prec 0.0754345, recall 0.880665
2017-12-10T14:14:05.108497: step 4520, loss 0.281851, acc 0.953125, prec 0.0754481, recall 0.880692
2017-12-10T14:14:05.294857: step 4521, loss 0.144563, acc 0.96875, prec 0.0754452, recall 0.880692
2017-12-10T14:14:05.485208: step 4522, loss 0.248081, acc 0.953125, prec 0.0754588, recall 0.880719
2017-12-10T14:14:05.669882: step 4523, loss 0.308529, acc 0.890625, prec 0.0754846, recall 0.880774
2017-12-10T14:14:05.858983: step 4524, loss 0.0504418, acc 0.984375, prec 0.0755192, recall 0.880828
2017-12-10T14:14:06.052008: step 4525, loss 0.256354, acc 0.921875, prec 0.0755118, recall 0.880828
2017-12-10T14:14:06.241673: step 4526, loss 0.118453, acc 0.96875, prec 0.0755449, recall 0.880882
2017-12-10T14:14:06.431527: step 4527, loss 0.0754767, acc 0.953125, prec 0.0755585, recall 0.880909
2017-12-10T14:14:06.618800: step 4528, loss 0.0783791, acc 0.953125, prec 0.0755721, recall 0.880936
2017-12-10T14:14:06.810425: step 4529, loss 1.073, acc 0.984375, prec 0.0755887, recall 0.880963
2017-12-10T14:14:06.999264: step 4530, loss 0.174887, acc 0.921875, prec 0.0756173, recall 0.881017
2017-12-10T14:14:07.187470: step 4531, loss 0.402104, acc 0.921875, prec 0.0756099, recall 0.881017
2017-12-10T14:14:07.376143: step 4532, loss 0.31892, acc 0.921875, prec 0.0756206, recall 0.881044
2017-12-10T14:14:07.568818: step 4533, loss 0.216904, acc 0.9375, prec 0.0756327, recall 0.881071
2017-12-10T14:14:07.758121: step 4534, loss 0.157967, acc 0.921875, prec 0.0756433, recall 0.881098
2017-12-10T14:14:07.951516: step 4535, loss 0.111092, acc 0.96875, prec 0.0756764, recall 0.881152
2017-12-10T14:14:08.138728: step 4536, loss 0.134856, acc 0.984375, prec 0.0756929, recall 0.881179
2017-12-10T14:14:08.328204: step 4537, loss 0.310181, acc 0.9375, prec 0.0757051, recall 0.881206
2017-12-10T14:14:08.513553: step 4538, loss 0.225264, acc 0.96875, prec 0.0757201, recall 0.881233
2017-12-10T14:14:08.702134: step 4539, loss 0.326862, acc 0.890625, prec 0.0757278, recall 0.88126
2017-12-10T14:14:08.887765: step 4540, loss 0.0760851, acc 0.96875, prec 0.0757248, recall 0.88126
2017-12-10T14:14:09.072193: step 4541, loss 0.0548557, acc 0.984375, prec 0.0757414, recall 0.881287
2017-12-10T14:14:09.259564: step 4542, loss 0.120183, acc 0.9375, prec 0.0757354, recall 0.881287
2017-12-10T14:14:09.444914: step 4543, loss 0.0494956, acc 0.984375, prec 0.075734, recall 0.881287
2017-12-10T14:14:09.636321: step 4544, loss 0.103962, acc 0.96875, prec 0.075731, recall 0.881287
2017-12-10T14:14:09.825781: step 4545, loss 0.200843, acc 0.9375, prec 0.0757611, recall 0.881341
2017-12-10T14:14:10.020008: step 4546, loss 0.144836, acc 0.9375, prec 0.0757732, recall 0.881367
2017-12-10T14:14:10.210968: step 4547, loss 0.152063, acc 0.984375, prec 0.0758077, recall 0.881421
2017-12-10T14:14:10.408103: step 4548, loss 0.22085, acc 0.953125, prec 0.0758393, recall 0.881475
2017-12-10T14:14:10.600698: step 4549, loss 0.0921005, acc 0.96875, prec 0.0758363, recall 0.881475
2017-12-10T14:14:10.791736: step 4550, loss 0.528352, acc 0.953125, prec 0.0758499, recall 0.881502
2017-12-10T14:14:10.983666: step 4551, loss 0.0151734, acc 0.984375, prec 0.0758484, recall 0.881502
2017-12-10T14:14:11.174736: step 4552, loss 0.0681977, acc 0.984375, prec 0.0758649, recall 0.881528
2017-12-10T14:14:11.360069: step 4553, loss 0.0838471, acc 0.984375, prec 0.0758634, recall 0.881528
2017-12-10T14:14:11.548785: step 4554, loss 0.0204679, acc 0.984375, prec 0.0758799, recall 0.881555
2017-12-10T14:14:11.734239: step 4555, loss 0.155377, acc 0.953125, prec 0.0758755, recall 0.881555
2017-12-10T14:14:11.927101: step 4556, loss 0.146857, acc 0.96875, prec 0.0758725, recall 0.881555
2017-12-10T14:14:12.113243: step 4557, loss 0.177701, acc 0.953125, prec 0.0759041, recall 0.881609
2017-12-10T14:14:12.303074: step 4558, loss 0.00379771, acc 1, prec 0.0759041, recall 0.881609
2017-12-10T14:14:12.492286: step 4559, loss 0.0322956, acc 0.984375, prec 0.0759026, recall 0.881609
2017-12-10T14:14:12.682189: step 4560, loss 0.00449605, acc 1, prec 0.0759026, recall 0.881609
2017-12-10T14:14:12.865887: step 4561, loss 0.0623153, acc 0.984375, prec 0.0759191, recall 0.881635
2017-12-10T14:14:13.057364: step 4562, loss 0.0302492, acc 0.984375, prec 0.0759715, recall 0.881716
2017-12-10T14:14:13.248325: step 4563, loss 0.0808225, acc 0.96875, prec 0.0759686, recall 0.881716
2017-12-10T14:14:13.444445: step 4564, loss 0.0551819, acc 0.96875, prec 0.0759656, recall 0.881716
2017-12-10T14:14:13.632859: step 4565, loss 0.138509, acc 0.953125, prec 0.0759971, recall 0.881769
2017-12-10T14:14:13.827443: step 4566, loss 0.0785021, acc 0.96875, prec 0.0759942, recall 0.881769
2017-12-10T14:14:14.022469: step 4567, loss 0.295598, acc 0.953125, prec 0.0759897, recall 0.881769
2017-12-10T14:14:14.210701: step 4568, loss 0.0247761, acc 1, prec 0.0760077, recall 0.881796
2017-12-10T14:14:14.399255: step 4569, loss 0.158921, acc 0.96875, prec 0.0760047, recall 0.881796
2017-12-10T14:14:14.588105: step 4570, loss 0.06537, acc 0.96875, prec 0.0760198, recall 0.881822
2017-12-10T14:14:14.780373: step 4571, loss 0.142266, acc 0.953125, prec 0.0760333, recall 0.881849
2017-12-10T14:14:14.969138: step 4572, loss 0.14789, acc 0.984375, prec 0.0760498, recall 0.881876
2017-12-10T14:14:15.155268: step 4573, loss 0.0503612, acc 1, prec 0.0760677, recall 0.881902
2017-12-10T14:14:15.348464: step 4574, loss 0.00478354, acc 1, prec 0.0760677, recall 0.881902
2017-12-10T14:14:15.534872: step 4575, loss 0.186588, acc 0.953125, prec 0.0760813, recall 0.881929
2017-12-10T14:14:15.725475: step 4576, loss 1.57768, acc 0.984375, prec 0.0760813, recall 0.88173
2017-12-10T14:14:15.921997: step 4577, loss 0.0623478, acc 0.984375, prec 0.0761336, recall 0.88181
2017-12-10T14:14:16.110101: step 4578, loss 0.117499, acc 0.96875, prec 0.0761486, recall 0.881837
2017-12-10T14:14:16.298191: step 4579, loss 0.00941038, acc 1, prec 0.0761486, recall 0.881837
2017-12-10T14:14:16.485664: step 4580, loss 0.0317345, acc 0.984375, prec 0.0761472, recall 0.881837
2017-12-10T14:14:16.674770: step 4581, loss 0.0743126, acc 0.96875, prec 0.0761442, recall 0.881837
2017-12-10T14:14:16.863576: step 4582, loss 0.204738, acc 1, prec 0.0761801, recall 0.88189
2017-12-10T14:14:17.055940: step 4583, loss 1.55204, acc 0.953125, prec 0.0761951, recall 0.881718
2017-12-10T14:14:17.249410: step 4584, loss 0.295625, acc 0.9375, prec 0.0762251, recall 0.881771
2017-12-10T14:14:17.437551: step 4585, loss 0.382415, acc 0.9375, prec 0.0762192, recall 0.881771
2017-12-10T14:14:17.626504: step 4586, loss 0.174443, acc 0.953125, prec 0.0762327, recall 0.881798
2017-12-10T14:14:17.818702: step 4587, loss 0.314234, acc 0.921875, prec 0.0762253, recall 0.881798
2017-12-10T14:14:18.006064: step 4588, loss 0.303232, acc 0.921875, prec 0.0762537, recall 0.881851
2017-12-10T14:14:18.192941: step 4589, loss 0.752183, acc 0.890625, prec 0.0762613, recall 0.881877
2017-12-10T14:14:18.381898: step 4590, loss 0.16912, acc 0.921875, prec 0.0762718, recall 0.881904
2017-12-10T14:14:18.570703: step 4591, loss 0.460849, acc 0.921875, prec 0.0763003, recall 0.881957
2017-12-10T14:14:18.764774: step 4592, loss 0.14138, acc 0.953125, prec 0.0763138, recall 0.881983
2017-12-10T14:14:18.954187: step 4593, loss 0.79268, acc 0.828125, prec 0.0763513, recall 0.882063
2017-12-10T14:14:19.145393: step 4594, loss 0.307192, acc 0.890625, prec 0.0763768, recall 0.882116
2017-12-10T14:14:19.337083: step 4595, loss 0.69051, acc 0.875, prec 0.0763828, recall 0.882142
2017-12-10T14:14:19.529518: step 4596, loss 0.664661, acc 0.921875, prec 0.0763933, recall 0.882168
2017-12-10T14:14:19.717209: step 4597, loss 0.311486, acc 0.921875, prec 0.0764397, recall 0.882248
2017-12-10T14:14:19.903755: step 4598, loss 0.188569, acc 0.921875, prec 0.0764323, recall 0.882248
2017-12-10T14:14:20.090584: step 4599, loss 0.253092, acc 0.9375, prec 0.0764263, recall 0.882248
2017-12-10T14:14:20.281164: step 4600, loss 0.438713, acc 0.90625, prec 0.0764353, recall 0.882274
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_1/1512932334/checkpoints/model-4600

2017-12-10T14:14:21.988828: step 4601, loss 0.551184, acc 0.921875, prec 0.0764279, recall 0.882274
2017-12-10T14:14:22.176448: step 4602, loss 0.168427, acc 0.90625, prec 0.076419, recall 0.882274
2017-12-10T14:14:22.366477: step 4603, loss 0.125039, acc 0.953125, prec 0.0764325, recall 0.8823
2017-12-10T14:14:22.556558: step 4604, loss 0.10405, acc 0.96875, prec 0.0764474, recall 0.882327
2017-12-10T14:14:22.746471: step 4605, loss 0.194097, acc 0.953125, prec 0.076443, recall 0.882327
2017-12-10T14:14:22.933275: step 4606, loss 0.248738, acc 0.96875, prec 0.0764579, recall 0.882353
2017-12-10T14:14:23.124412: step 4607, loss 0.0473693, acc 0.953125, prec 0.0764535, recall 0.882353
2017-12-10T14:14:23.311868: step 4608, loss 0.0212055, acc 0.984375, prec 0.0764699, recall 0.882379
2017-12-10T14:14:23.499883: step 4609, loss 5.6415, acc 0.9375, prec 0.0764655, recall 0.882182
2017-12-10T14:14:23.690991: step 4610, loss 0.230141, acc 0.953125, prec 0.0764789, recall 0.882208
2017-12-10T14:14:23.880077: step 4611, loss 0.158567, acc 0.984375, prec 0.0764953, recall 0.882235
2017-12-10T14:14:24.069741: step 4612, loss 0.302036, acc 0.9375, prec 0.0765073, recall 0.882261
2017-12-10T14:14:24.261029: step 4613, loss 0.333647, acc 0.9375, prec 0.0765192, recall 0.882287
2017-12-10T14:14:24.450557: step 4614, loss 0.216183, acc 0.921875, prec 0.0765297, recall 0.882314
2017-12-10T14:14:24.644053: step 4615, loss 0.307973, acc 0.890625, prec 0.0765372, recall 0.88234
2017-12-10T14:14:24.830628: step 4616, loss 0.523258, acc 0.875, prec 0.0765433, recall 0.882366
2017-12-10T14:14:25.017699: step 4617, loss 0.0711875, acc 0.953125, prec 0.0765746, recall 0.882419
2017-12-10T14:14:25.205035: step 4618, loss 0.558841, acc 0.875, prec 0.0765806, recall 0.882445
2017-12-10T14:14:25.393460: step 4619, loss 0.262452, acc 0.921875, prec 0.0765911, recall 0.882471
2017-12-10T14:14:25.584901: step 4620, loss 0.0631745, acc 0.984375, prec 0.0765896, recall 0.882471
2017-12-10T14:14:25.776045: step 4621, loss 0.163455, acc 0.96875, prec 0.0766223, recall 0.882523
2017-12-10T14:14:25.962381: step 4622, loss 0.29475, acc 0.90625, prec 0.0766313, recall 0.88255
2017-12-10T14:14:26.150860: step 4623, loss 0.0786286, acc 0.953125, prec 0.0766447, recall 0.882576
2017-12-10T14:14:26.343821: step 4624, loss 0.219873, acc 0.90625, prec 0.0766358, recall 0.882576
2017-12-10T14:14:26.533187: step 4625, loss 0.199199, acc 0.953125, prec 0.0766671, recall 0.882628
2017-12-10T14:14:26.722863: step 4626, loss 0.247055, acc 0.921875, prec 0.0766597, recall 0.882628
2017-12-10T14:14:26.910775: step 4627, loss 0.0235922, acc 0.984375, prec 0.0766939, recall 0.88268
2017-12-10T14:14:27.099300: step 4628, loss 0.443488, acc 0.921875, prec 0.0766865, recall 0.88268
2017-12-10T14:14:27.289487: step 4629, loss 0.218633, acc 0.96875, prec 0.0766836, recall 0.88268
2017-12-10T14:14:27.482030: step 4630, loss 0.111042, acc 0.96875, prec 0.0767163, recall 0.882733
2017-12-10T14:14:27.672744: step 4631, loss 0.448677, acc 0.953125, prec 0.0767297, recall 0.882759
2017-12-10T14:14:27.862323: step 4632, loss 0.0149847, acc 1, prec 0.0767476, recall 0.882785
2017-12-10T14:14:28.053507: step 4633, loss 0.0104852, acc 1, prec 0.0767654, recall 0.882811
2017-12-10T14:14:28.239603: step 4634, loss 0.274696, acc 0.953125, prec 0.0767967, recall 0.882863
2017-12-10T14:14:28.429249: step 4635, loss 0.161386, acc 0.953125, prec 0.0767922, recall 0.882863
2017-12-10T14:14:28.615097: step 4636, loss 0.0128482, acc 1, prec 0.0768101, recall 0.882889
2017-12-10T14:14:28.810395: step 4637, loss 0.0165602, acc 0.984375, prec 0.0768086, recall 0.882889
2017-12-10T14:14:29.006307: step 4638, loss 0.174366, acc 0.96875, prec 0.0768234, recall 0.882915
2017-12-10T14:14:29.194575: step 4639, loss 1.46481, acc 0.984375, prec 0.0768234, recall 0.882719
2017-12-10T14:14:29.386141: step 4640, loss 0.0532951, acc 0.984375, prec 0.076822, recall 0.882719
2017-12-10T14:14:29.575817: step 4641, loss 0.0200438, acc 0.984375, prec 0.0768205, recall 0.882719
2017-12-10T14:14:29.764252: step 4642, loss 0.0107705, acc 1, prec 0.0768383, recall 0.882745
2017-12-10T14:14:29.950817: step 4643, loss 0.154963, acc 0.96875, prec 0.0768353, recall 0.882745
2017-12-10T14:14:30.137633: step 4644, loss 0.100149, acc 0.96875, prec 0.0768502, recall 0.882771
2017-12-10T14:14:30.326027: step 4645, loss 0.0880912, acc 0.953125, prec 0.0768636, recall 0.882797
2017-12-10T14:14:30.514608: step 4646, loss 0.12645, acc 0.953125, prec 0.076877, recall 0.882823
2017-12-10T14:14:30.702742: step 4647, loss 0.572784, acc 0.921875, prec 0.0768696, recall 0.882823
2017-12-10T14:14:30.888829: step 4648, loss 0.267125, acc 0.921875, prec 0.07688, recall 0.882849
2017-12-10T14:14:31.082194: step 4649, loss 0.119055, acc 0.953125, prec 0.0768755, recall 0.882849
2017-12-10T14:14:31.272532: step 4650, loss 3.05004, acc 0.9375, prec 0.0768711, recall 0.882653
2017-12-10T14:14:31.470480: step 4651, loss 0.0309886, acc 0.984375, prec 0.0768874, recall 0.882679
2017-12-10T14:14:31.659437: step 4652, loss 0.271005, acc 0.9375, prec 0.0768815, recall 0.882679
2017-12-10T14:14:31.846078: step 4653, loss 0.446523, acc 0.890625, prec 0.0768711, recall 0.882679
2017-12-10T14:14:32.036589: step 4654, loss 0.128085, acc 0.96875, prec 0.0768859, recall 0.882705
2017-12-10T14:14:32.222838: step 4655, loss 0.362547, acc 0.921875, prec 0.0768963, recall 0.882731
2017-12-10T14:14:32.409256: step 4656, loss 0.177478, acc 0.9375, prec 0.0769082, recall 0.882757
2017-12-10T14:14:32.600561: step 4657, loss 0.342552, acc 0.9375, prec 0.0769023, recall 0.882757
2017-12-10T14:14:32.788650: step 4658, loss 0.231007, acc 0.9375, prec 0.0769142, recall 0.882783
2017-12-10T14:14:32.980331: step 4659, loss 0.179465, acc 0.921875, prec 0.0769246, recall 0.882809
2017-12-10T14:14:33.167270: step 4660, loss 0.767799, acc 0.859375, prec 0.0769825, recall 0.882913
2017-12-10T14:14:33.356149: step 4661, loss 0.476906, acc 0.875, prec 0.0769706, recall 0.882913
2017-12-10T14:14:33.545359: step 4662, loss 0.341046, acc 0.921875, prec 0.0770344, recall 0.883016
2017-12-10T14:14:33.732227: step 4663, loss 0.28309, acc 0.90625, prec 0.0770611, recall 0.883068
2017-12-10T14:14:33.919413: step 4664, loss 0.195306, acc 0.953125, prec 0.0770922, recall 0.88312
2017-12-10T14:14:34.107504: step 4665, loss 0.280224, acc 0.921875, prec 0.0771026, recall 0.883146
2017-12-10T14:14:34.293435: step 4666, loss 0.541843, acc 0.890625, prec 0.0770922, recall 0.883146
2017-12-10T14:14:34.485488: step 4667, loss 0.142672, acc 0.9375, prec 0.0770862, recall 0.883146
2017-12-10T14:14:34.673304: step 4668, loss 0.40903, acc 0.953125, prec 0.0770996, recall 0.883171
2017-12-10T14:14:34.859454: step 4669, loss 0.054972, acc 0.96875, prec 0.0771322, recall 0.883223
2017-12-10T14:14:35.047717: step 4670, loss 0.302524, acc 0.9375, prec 0.0771796, recall 0.8833
2017-12-10T14:14:35.234729: step 4671, loss 0.658793, acc 0.875, prec 0.0771855, recall 0.883326
2017-12-10T14:14:35.420927: step 4672, loss 0.367775, acc 0.921875, prec 0.0771958, recall 0.883352
2017-12-10T14:14:35.605244: step 4673, loss 0.338272, acc 0.953125, prec 0.0772269, recall 0.883403
2017-12-10T14:14:35.799314: step 4674, loss 0.116043, acc 0.96875, prec 0.0772417, recall 0.883429
2017-12-10T14:14:35.987955: step 4675, loss 0.132152, acc 0.9375, prec 0.0772713, recall 0.88348
2017-12-10T14:14:36.176206: step 4676, loss 0.310737, acc 0.9375, prec 0.0772831, recall 0.883506
2017-12-10T14:14:36.367582: step 4677, loss 0.587144, acc 0.921875, prec 0.077329, recall 0.883583
2017-12-10T14:14:36.556607: step 4678, loss 0.010441, acc 1, prec 0.0773468, recall 0.883608
2017-12-10T14:14:36.742850: step 4679, loss 0.259173, acc 0.984375, prec 0.0773631, recall 0.883634
2017-12-10T14:14:36.934156: step 4680, loss 0.157517, acc 0.9375, prec 0.0773749, recall 0.88366
2017-12-10T14:14:37.125647: step 4681, loss 0.0740744, acc 0.984375, prec 0.0774089, recall 0.883711
2017-12-10T14:14:37.316281: step 4682, loss 0.149805, acc 0.953125, prec 0.0774044, recall 0.883711
2017-12-10T14:14:37.507237: step 4683, loss 0.28552, acc 0.953125, prec 0.0774355, recall 0.883762
2017-12-10T14:14:37.698370: step 4684, loss 0.0353754, acc 0.984375, prec 0.077434, recall 0.883762
2017-12-10T14:14:37.885539: step 4685, loss 1.30332, acc 0.9375, prec 0.0774473, recall 0.883593
2017-12-10T14:14:38.081363: step 4686, loss 0.516771, acc 0.953125, prec 0.0775139, recall 0.883695
2017-12-10T14:14:38.277537: step 4687, loss 0.0102056, acc 1, prec 0.0775139, recall 0.883695
2017-12-10T14:14:38.467137: step 4688, loss 0.0421929, acc 0.984375, prec 0.0775124, recall 0.883695
2017-12-10T14:14:38.663022: step 4689, loss 0.00939568, acc 1, prec 0.0775124, recall 0.883695
2017-12-10T14:14:38.852294: step 4690, loss 0.0431541, acc 0.984375, prec 0.0775109, recall 0.883695
2017-12-10T14:14:39.041770: step 4691, loss 0.339299, acc 0.9375, prec 0.0775404, recall 0.883746
2017-12-10T14:14:39.228385: step 4692, loss 0.158261, acc 0.953125, prec 0.0775359, recall 0.883746
2017-12-10T14:14:39.417348: step 4693, loss 0.041857, acc 0.984375, prec 0.0775344, recall 0.883746
2017-12-10T14:14:39.605489: step 4694, loss 0.0184343, acc 1, prec 0.0775699, recall 0.883797
2017-12-10T14:14:39.792551: step 4695, loss 0.566984, acc 0.9375, prec 0.0775817, recall 0.883823
2017-12-10T14:14:39.984699: step 4696, loss 0.113673, acc 0.96875, prec 0.0776142, recall 0.883874
2017-12-10T14:14:40.173023: step 4697, loss 0.121329, acc 0.984375, prec 0.077666, recall 0.88395
2017-12-10T14:14:40.363353: step 4698, loss 0.209932, acc 0.9375, prec 0.0776777, recall 0.883976
2017-12-10T14:14:40.550221: step 4699, loss 0.0975267, acc 0.953125, prec 0.0776733, recall 0.883976
2017-12-10T14:14:40.740984: step 4700, loss 0.280104, acc 0.9375, prec 0.0777028, recall 0.884026
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_1/1512932334/checkpoints/model-4700

2017-12-10T14:14:42.406879: step 4701, loss 0.0210695, acc 1, prec 0.0777028, recall 0.884026
2017-12-10T14:14:42.594751: step 4702, loss 0.322759, acc 0.953125, prec 0.0777338, recall 0.884077
2017-12-10T14:14:42.782621: step 4703, loss 0.175664, acc 0.953125, prec 0.077747, recall 0.884102
2017-12-10T14:14:42.967959: step 4704, loss 0.296849, acc 0.953125, prec 0.0777603, recall 0.884128
2017-12-10T14:14:43.154442: step 4705, loss 0.124609, acc 0.953125, prec 0.077809, recall 0.884204
2017-12-10T14:14:43.341291: step 4706, loss 0.251277, acc 0.96875, prec 0.0778414, recall 0.884254
2017-12-10T14:14:43.532083: step 4707, loss 0.0806465, acc 0.953125, prec 0.0778547, recall 0.884279
2017-12-10T14:14:43.723842: step 4708, loss 8.27149, acc 0.9375, prec 0.0778502, recall 0.884086
2017-12-10T14:14:43.920088: step 4709, loss 0.136725, acc 0.953125, prec 0.0778457, recall 0.884086
2017-12-10T14:14:44.113837: step 4710, loss 0.178252, acc 0.984375, prec 0.0778619, recall 0.884112
2017-12-10T14:14:44.304265: step 4711, loss 0.15981, acc 0.9375, prec 0.0778737, recall 0.884137
2017-12-10T14:14:44.490345: step 4712, loss 0.194081, acc 0.921875, prec 0.0779193, recall 0.884213
2017-12-10T14:14:44.677212: step 4713, loss 0.254912, acc 0.921875, prec 0.0779118, recall 0.884213
2017-12-10T14:14:44.865987: step 4714, loss 0.249118, acc 0.9375, prec 0.0779059, recall 0.884213
2017-12-10T14:14:45.055151: step 4715, loss 0.224333, acc 0.90625, prec 0.0778969, recall 0.884213
2017-12-10T14:14:45.241070: step 4716, loss 0.150998, acc 0.953125, prec 0.0779278, recall 0.884263
2017-12-10T14:14:45.432578: step 4717, loss 0.140395, acc 0.921875, prec 0.077938, recall 0.884288
2017-12-10T14:14:45.623914: step 4718, loss 0.122884, acc 0.953125, prec 0.0779335, recall 0.884288
2017-12-10T14:14:45.815906: step 4719, loss 0.214698, acc 0.921875, prec 0.0779261, recall 0.884288
2017-12-10T14:14:46.008275: step 4720, loss 0.374275, acc 0.890625, prec 0.0779156, recall 0.884288
2017-12-10T14:14:46.195609: step 4721, loss 1.50225, acc 0.9375, prec 0.0779465, recall 0.884146
2017-12-10T14:14:46.386268: step 4722, loss 0.382736, acc 0.921875, prec 0.077939, recall 0.884146
2017-12-10T14:14:46.575924: step 4723, loss 0.381988, acc 0.9375, prec 0.0779861, recall 0.884222
2017-12-10T14:14:46.763390: step 4724, loss 0.250534, acc 0.90625, prec 0.0779949, recall 0.884247
2017-12-10T14:14:46.951451: step 4725, loss 0.295808, acc 0.921875, prec 0.0780051, recall 0.884272
2017-12-10T14:14:47.145803: step 4726, loss 0.359322, acc 0.90625, prec 0.0779961, recall 0.884272
2017-12-10T14:14:47.335070: step 4727, loss 0.537248, acc 0.921875, prec 0.0780063, recall 0.884298
2017-12-10T14:14:47.520792: step 4728, loss 0.286656, acc 0.9375, prec 0.0780534, recall 0.884373
2017-12-10T14:14:47.707080: step 4729, loss 0.215727, acc 0.875, prec 0.0780591, recall 0.884398
2017-12-10T14:14:47.896268: step 4730, loss 0.216915, acc 0.953125, prec 0.0780899, recall 0.884448
2017-12-10T14:14:48.083524: step 4731, loss 0.424929, acc 0.875, prec 0.0780956, recall 0.884473
2017-12-10T14:14:48.272593: step 4732, loss 0.260054, acc 0.9375, prec 0.0781073, recall 0.884498
2017-12-10T14:14:48.460700: step 4733, loss 0.44318, acc 0.953125, prec 0.0781559, recall 0.884574
2017-12-10T14:14:48.654658: step 4734, loss 0.363313, acc 0.875, prec 0.0781439, recall 0.884574
2017-12-10T14:14:48.843277: step 4735, loss 0.307841, acc 0.890625, prec 0.0781511, recall 0.884599
2017-12-10T14:14:49.032674: step 4736, loss 0.136807, acc 0.921875, prec 0.0781612, recall 0.884624
2017-12-10T14:14:49.222015: step 4737, loss 0.101427, acc 0.9375, prec 0.0781729, recall 0.884649
2017-12-10T14:14:49.412850: step 4738, loss 0.152174, acc 0.96875, prec 0.0781699, recall 0.884649
2017-12-10T14:14:49.612117: step 4739, loss 0.00486038, acc 1, prec 0.0781699, recall 0.884649
2017-12-10T14:14:49.802149: step 4740, loss 0.237586, acc 0.9375, prec 0.0781816, recall 0.884674
2017-12-10T14:14:49.994612: step 4741, loss 0.175267, acc 0.96875, prec 0.0781962, recall 0.884699
2017-12-10T14:14:50.188932: step 4742, loss 0.0798283, acc 0.96875, prec 0.0781932, recall 0.884699
2017-12-10T14:14:50.377894: step 4743, loss 0.107624, acc 0.953125, prec 0.0781887, recall 0.884699
2017-12-10T14:14:50.566636: step 4744, loss 0.0735105, acc 0.96875, prec 0.0781858, recall 0.884699
2017-12-10T14:14:50.757438: step 4745, loss 0.405415, acc 0.921875, prec 0.0781783, recall 0.884699
2017-12-10T14:14:50.947031: step 4746, loss 0.0553109, acc 1, prec 0.0782312, recall 0.884774
2017-12-10T14:14:51.138478: step 4747, loss 0.0214926, acc 0.984375, prec 0.0782297, recall 0.884774
2017-12-10T14:14:51.328607: step 4748, loss 0.117514, acc 0.9375, prec 0.0782414, recall 0.884799
2017-12-10T14:14:51.517634: step 4749, loss 0.0890176, acc 0.953125, prec 0.0782369, recall 0.884799
2017-12-10T14:14:51.711444: step 4750, loss 0.269781, acc 0.96875, prec 0.0782515, recall 0.884824
2017-12-10T14:14:51.902362: step 4751, loss 0.290425, acc 0.90625, prec 0.0782602, recall 0.884848
2017-12-10T14:14:52.093503: step 4752, loss 0.0432488, acc 0.984375, prec 0.0782764, recall 0.884873
2017-12-10T14:14:52.283603: step 4753, loss 0.118801, acc 0.96875, prec 0.078291, recall 0.884898
2017-12-10T14:14:52.473138: step 4754, loss 0.107671, acc 0.984375, prec 0.0783071, recall 0.884923
2017-12-10T14:14:52.661646: step 4755, loss 0.230873, acc 0.953125, prec 0.0783203, recall 0.884948
2017-12-10T14:14:52.853294: step 4756, loss 0.272154, acc 0.96875, prec 0.0783349, recall 0.884973
2017-12-10T14:14:53.042546: step 4757, loss 0.097454, acc 0.96875, prec 0.0783319, recall 0.884973
2017-12-10T14:14:53.232221: step 4758, loss 0.0191145, acc 0.984375, prec 0.0783304, recall 0.884973
2017-12-10T14:14:53.416382: step 4759, loss 0.164532, acc 0.984375, prec 0.0783289, recall 0.884973
2017-12-10T14:14:53.607276: step 4760, loss 2.11966, acc 0.953125, prec 0.0783259, recall 0.884782
2017-12-10T14:14:53.800788: step 4761, loss 0.0677203, acc 0.984375, prec 0.0783421, recall 0.884807
2017-12-10T14:14:53.989308: step 4762, loss 0.148273, acc 0.984375, prec 0.0783406, recall 0.884807
2017-12-10T14:14:54.181726: step 4763, loss 6.82082, acc 0.96875, prec 0.078392, recall 0.88469
2017-12-10T14:14:54.371871: step 4764, loss 0.0287036, acc 0.984375, prec 0.0784081, recall 0.884715
2017-12-10T14:14:54.558860: step 4765, loss 0.216331, acc 0.921875, prec 0.0784006, recall 0.884715
2017-12-10T14:14:54.749301: step 4766, loss 0.486984, acc 0.875, prec 0.0783886, recall 0.884715
2017-12-10T14:14:54.941332: step 4767, loss 0.488764, acc 0.890625, prec 0.0784134, recall 0.884765
2017-12-10T14:14:55.126589: step 4768, loss 0.428437, acc 0.890625, prec 0.0784029, recall 0.884765
2017-12-10T14:14:55.315197: step 4769, loss 0.376457, acc 0.90625, prec 0.0783939, recall 0.884765
2017-12-10T14:14:55.505349: step 4770, loss 0.323851, acc 0.890625, prec 0.0784186, recall 0.884815
2017-12-10T14:14:55.690657: step 4771, loss 0.446297, acc 0.84375, prec 0.0784389, recall 0.884864
2017-12-10T14:14:55.880314: step 4772, loss 0.253657, acc 0.84375, prec 0.0784239, recall 0.884864
2017-12-10T14:14:56.068783: step 4773, loss 0.411902, acc 0.875, prec 0.0784471, recall 0.884914
2017-12-10T14:14:56.258696: step 4774, loss 0.648376, acc 0.859375, prec 0.0784336, recall 0.884914
2017-12-10T14:14:56.449561: step 4775, loss 0.424346, acc 0.859375, prec 0.0784377, recall 0.884939
2017-12-10T14:14:56.639588: step 4776, loss 0.402836, acc 0.875, prec 0.078461, recall 0.884988
2017-12-10T14:14:56.825527: step 4777, loss 0.455592, acc 0.890625, prec 0.0784505, recall 0.884988
2017-12-10T14:14:57.012173: step 4778, loss 0.463847, acc 0.890625, prec 0.07844, recall 0.884988
2017-12-10T14:14:57.200216: step 4779, loss 0.328213, acc 0.890625, prec 0.0784471, recall 0.885013
2017-12-10T14:14:57.386423: step 4780, loss 0.581585, acc 0.859375, prec 0.0784336, recall 0.885013
2017-12-10T14:14:57.571779: step 4781, loss 0.358232, acc 0.953125, prec 0.0784643, recall 0.885062
2017-12-10T14:14:57.760229: step 4782, loss 0.329905, acc 0.90625, prec 0.0785081, recall 0.885137
2017-12-10T14:14:57.949280: step 4783, loss 0.187884, acc 0.890625, prec 0.0785152, recall 0.885161
2017-12-10T14:14:58.142485: step 4784, loss 0.244658, acc 0.921875, prec 0.0785077, recall 0.885161
2017-12-10T14:14:58.332409: step 4785, loss 0.0849066, acc 0.96875, prec 0.0785223, recall 0.885186
2017-12-10T14:14:58.520135: step 4786, loss 0.239464, acc 0.921875, prec 0.0785148, recall 0.885186
2017-12-10T14:14:58.712820: step 4787, loss 0.379655, acc 0.9375, prec 0.0785263, recall 0.885211
2017-12-10T14:14:58.906460: step 4788, loss 0.321276, acc 0.9375, prec 0.0785379, recall 0.885235
2017-12-10T14:14:59.098503: step 4789, loss 0.211431, acc 0.9375, prec 0.0786022, recall 0.885334
2017-12-10T14:14:59.287076: step 4790, loss 0.174745, acc 0.96875, prec 0.0786519, recall 0.885408
2017-12-10T14:14:59.476872: step 4791, loss 0.238408, acc 0.9375, prec 0.0786459, recall 0.885408
2017-12-10T14:14:59.666141: step 4792, loss 0.0757701, acc 0.96875, prec 0.0786605, recall 0.885432
2017-12-10T14:14:59.855588: step 4793, loss 0.356498, acc 0.953125, prec 0.078656, recall 0.885432
2017-12-10T14:15:00.047595: step 4794, loss 0.769662, acc 0.96875, prec 0.0786705, recall 0.885457
2017-12-10T14:15:00.236444: step 4795, loss 0.0884933, acc 1, prec 0.0786881, recall 0.885481
2017-12-10T14:15:00.426046: step 4796, loss 0.195122, acc 0.953125, prec 0.0787363, recall 0.885555
2017-12-10T14:15:00.619560: step 4797, loss 0.226895, acc 0.9375, prec 0.0787478, recall 0.88558
2017-12-10T14:15:00.810167: step 4798, loss 0.0865065, acc 0.96875, prec 0.0787624, recall 0.885604
2017-12-10T14:15:01.000447: step 4799, loss 1.8718, acc 0.96875, prec 0.0787609, recall 0.885414
2017-12-10T14:15:01.197305: step 4800, loss 0.0367503, acc 0.984375, prec 0.0787769, recall 0.885439
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_1/1512932334/checkpoints/model-4800

2017-12-10T14:15:02.356921: step 4801, loss 0.0458762, acc 0.96875, prec 0.0787915, recall 0.885463
2017-12-10T14:15:02.547562: step 4802, loss 0.434545, acc 0.90625, prec 0.0788, recall 0.885488
2017-12-10T14:15:02.737517: step 4803, loss 0.119312, acc 0.9375, prec 0.0788291, recall 0.885537
2017-12-10T14:15:02.927310: step 4804, loss 0.0739608, acc 0.96875, prec 0.0788436, recall 0.885562
2017-12-10T14:15:03.120088: step 4805, loss 0.0142594, acc 1, prec 0.0788612, recall 0.885586
2017-12-10T14:15:03.310936: step 4806, loss 0.224571, acc 0.9375, prec 0.0788552, recall 0.885586
2017-12-10T14:15:03.499612: step 4807, loss 0.418522, acc 0.875, prec 0.0788782, recall 0.885635
2017-12-10T14:15:03.688605: step 4808, loss 0.15599, acc 0.9375, prec 0.0788722, recall 0.885635
2017-12-10T14:15:03.877635: step 4809, loss 0.107392, acc 0.984375, prec 0.0789058, recall 0.885684
2017-12-10T14:15:04.068133: step 4810, loss 0.538293, acc 0.921875, prec 0.0788983, recall 0.885684
2017-12-10T14:15:04.257057: step 4811, loss 0.132182, acc 0.953125, prec 0.0788938, recall 0.885684
2017-12-10T14:15:04.446426: step 4812, loss 0.566944, acc 0.96875, prec 0.0789258, recall 0.885733
2017-12-10T14:15:04.635620: step 4813, loss 0.148226, acc 0.953125, prec 0.0789213, recall 0.885733
2017-12-10T14:15:04.828802: step 4814, loss 0.0279104, acc 0.984375, prec 0.0789549, recall 0.885781
2017-12-10T14:15:05.019736: step 4815, loss 0.270094, acc 0.90625, prec 0.0789809, recall 0.88583
2017-12-10T14:15:05.207152: step 4816, loss 0.314217, acc 0.921875, prec 0.079026, recall 0.885903
2017-12-10T14:15:05.399013: step 4817, loss 0.0388205, acc 0.984375, prec 0.079042, recall 0.885927
2017-12-10T14:15:05.586677: step 4818, loss 0.0773864, acc 0.96875, prec 0.0790565, recall 0.885952
2017-12-10T14:15:05.772532: step 4819, loss 0.154444, acc 0.953125, prec 0.079087, recall 0.886
2017-12-10T14:15:05.962225: step 4820, loss 0.0144261, acc 1, prec 0.0791221, recall 0.886049
2017-12-10T14:15:06.148686: step 4821, loss 0.133068, acc 0.953125, prec 0.079135, recall 0.886073
2017-12-10T14:15:06.334093: step 4822, loss 0.083375, acc 0.984375, prec 0.0791511, recall 0.886097
2017-12-10T14:15:06.525369: step 4823, loss 0.00316577, acc 1, prec 0.0791686, recall 0.886122
2017-12-10T14:15:06.716282: step 4824, loss 0.252189, acc 0.96875, prec 0.0792006, recall 0.88617
2017-12-10T14:15:06.904360: step 4825, loss 0.145186, acc 0.96875, prec 0.0792326, recall 0.886219
2017-12-10T14:15:07.092937: step 4826, loss 0.0637489, acc 0.984375, prec 0.0792486, recall 0.886243
2017-12-10T14:15:07.280458: step 4827, loss 0.149006, acc 0.984375, prec 0.0792471, recall 0.886243
2017-12-10T14:15:07.467764: step 4828, loss 0.0247761, acc 0.984375, prec 0.0792631, recall 0.886267
2017-12-10T14:15:07.656771: step 4829, loss 0.0906036, acc 0.953125, prec 0.0792586, recall 0.886267
2017-12-10T14:15:07.846867: step 4830, loss 0.0924828, acc 0.953125, prec 0.079254, recall 0.886267
2017-12-10T14:15:08.034605: step 4831, loss 0.135854, acc 0.9375, prec 0.0792655, recall 0.886291
2017-12-10T14:15:08.226189: step 4832, loss 0.0430707, acc 0.96875, prec 0.0792625, recall 0.886291
2017-12-10T14:15:08.419316: step 4833, loss 0.137614, acc 0.96875, prec 0.0792595, recall 0.886291
2017-12-10T14:15:08.608484: step 4834, loss 0.0205702, acc 0.984375, prec 0.079258, recall 0.886291
2017-12-10T14:15:08.807523: step 4835, loss 0.0278354, acc 0.984375, prec 0.0792565, recall 0.886291
2017-12-10T14:15:08.996575: step 4836, loss 0.0402995, acc 1, prec 0.0792915, recall 0.886339
2017-12-10T14:15:09.191676: step 4837, loss 0.0690788, acc 0.96875, prec 0.0792885, recall 0.886339
2017-12-10T14:15:09.379625: step 4838, loss 0.157176, acc 0.96875, prec 0.0792854, recall 0.886339
2017-12-10T14:15:09.566587: step 4839, loss 0.0339892, acc 1, prec 0.0793029, recall 0.886364
2017-12-10T14:15:09.758524: step 4840, loss 0.000910153, acc 1, prec 0.0793029, recall 0.886364
2017-12-10T14:15:09.945709: step 4841, loss 0.00083343, acc 1, prec 0.0793029, recall 0.886364
2017-12-10T14:15:10.135122: step 4842, loss 0.213141, acc 1, prec 0.0793204, recall 0.886388
2017-12-10T14:15:10.329773: step 4843, loss 0.0206989, acc 0.984375, prec 0.0793539, recall 0.886436
2017-12-10T14:15:10.523172: step 4844, loss 5.63673, acc 0.984375, prec 0.0793889, recall 0.886296
2017-12-10T14:15:10.719958: step 4845, loss 0.0533617, acc 0.984375, prec 0.0793874, recall 0.886296
2017-12-10T14:15:10.907475: step 4846, loss 0.795832, acc 0.9375, prec 0.0793989, recall 0.88632
2017-12-10T14:15:11.102244: step 4847, loss 0.00889437, acc 1, prec 0.0794338, recall 0.886368
2017-12-10T14:15:11.297698: step 4848, loss 0.348095, acc 0.921875, prec 0.0794263, recall 0.886368
2017-12-10T14:15:11.487076: step 4849, loss 0.20264, acc 0.953125, prec 0.0794218, recall 0.886368
2017-12-10T14:15:11.679569: step 4850, loss 0.330015, acc 0.90625, prec 0.0794302, recall 0.886393
2017-12-10T14:15:11.865318: step 4851, loss 0.356583, acc 0.921875, prec 0.0794576, recall 0.886441
2017-12-10T14:15:12.059266: step 4852, loss 0.406493, acc 0.875, prec 0.0794455, recall 0.886441
2017-12-10T14:15:12.248831: step 4853, loss 0.440036, acc 0.90625, prec 0.0794365, recall 0.886441
2017-12-10T14:15:12.440673: step 4854, loss 0.172783, acc 0.96875, prec 0.0794335, recall 0.886441
2017-12-10T14:15:12.629691: step 4855, loss 0.294878, acc 0.9375, prec 0.0794275, recall 0.886441
2017-12-10T14:15:12.820600: step 4856, loss 0.379733, acc 0.921875, prec 0.0794374, recall 0.886465
2017-12-10T14:15:13.011409: step 4857, loss 0.489348, acc 0.890625, prec 0.0794268, recall 0.886465
2017-12-10T14:15:13.199033: step 4858, loss 0.30674, acc 0.921875, prec 0.0794193, recall 0.886465
2017-12-10T14:15:13.390789: step 4859, loss 0.110647, acc 0.953125, prec 0.0794497, recall 0.886513
2017-12-10T14:15:13.586709: step 4860, loss 0.285358, acc 0.96875, prec 0.0794816, recall 0.886561
2017-12-10T14:15:13.773208: step 4861, loss 0.356517, acc 0.953125, prec 0.0794946, recall 0.886585
2017-12-10T14:15:13.966197: step 4862, loss 0.349321, acc 0.921875, prec 0.079487, recall 0.886585
2017-12-10T14:15:14.153106: step 4863, loss 0.68261, acc 0.859375, prec 0.0794909, recall 0.886609
2017-12-10T14:15:14.340203: step 4864, loss 0.151213, acc 0.96875, prec 0.0795054, recall 0.886633
2017-12-10T14:15:14.529951: step 4865, loss 0.683897, acc 0.890625, prec 0.0794948, recall 0.886633
2017-12-10T14:15:14.718213: step 4866, loss 0.298464, acc 0.875, prec 0.0795177, recall 0.886681
2017-12-10T14:15:14.904804: step 4867, loss 0.116289, acc 0.9375, prec 0.0795116, recall 0.886681
2017-12-10T14:15:15.091406: step 4868, loss 0.165287, acc 0.921875, prec 0.0795215, recall 0.886705
2017-12-10T14:15:15.282182: step 4869, loss 0.310452, acc 0.953125, prec 0.0795345, recall 0.886729
2017-12-10T14:15:15.474815: step 4870, loss 0.193439, acc 0.953125, prec 0.0795299, recall 0.886729
2017-12-10T14:15:15.663804: step 4871, loss 0.11567, acc 0.953125, prec 0.0795429, recall 0.886753
2017-12-10T14:15:15.851266: step 4872, loss 0.121749, acc 0.96875, prec 0.0795573, recall 0.886777
2017-12-10T14:15:16.041193: step 4873, loss 0.603775, acc 0.921875, prec 0.0795672, recall 0.8868
2017-12-10T14:15:16.233134: step 4874, loss 0.174293, acc 0.96875, prec 0.0795816, recall 0.886824
2017-12-10T14:15:16.420569: step 4875, loss 0.195296, acc 0.96875, prec 0.0795786, recall 0.886824
2017-12-10T14:15:16.609909: step 4876, loss 0.347968, acc 0.921875, prec 0.0795711, recall 0.886824
2017-12-10T14:15:16.798563: step 4877, loss 0.187865, acc 0.953125, prec 0.0795666, recall 0.886824
2017-12-10T14:15:16.991265: step 4878, loss 0.143117, acc 0.953125, prec 0.0795795, recall 0.886848
2017-12-10T14:15:17.182461: step 4879, loss 0.0193765, acc 0.984375, prec 0.079578, recall 0.886848
2017-12-10T14:15:17.370207: step 4880, loss 0.317956, acc 0.921875, prec 0.0795879, recall 0.886872
2017-12-10T14:15:17.555868: step 4881, loss 0.249741, acc 0.96875, prec 0.0796023, recall 0.886896
2017-12-10T14:15:17.743750: step 4882, loss 0.0422373, acc 0.984375, prec 0.0796008, recall 0.886896
2017-12-10T14:15:17.931855: step 4883, loss 0.715464, acc 1, prec 0.0796356, recall 0.886944
2017-12-10T14:15:18.119564: step 4884, loss 0.0278848, acc 0.984375, prec 0.0796341, recall 0.886944
2017-12-10T14:15:18.309204: step 4885, loss 0.124197, acc 0.96875, prec 0.0796311, recall 0.886944
2017-12-10T14:15:18.503232: step 4886, loss 0.032901, acc 0.96875, prec 0.0796629, recall 0.886991
2017-12-10T14:15:18.691349: step 4887, loss 0.136338, acc 0.953125, prec 0.0796758, recall 0.887015
2017-12-10T14:15:18.883767: step 4888, loss 0.411974, acc 0.921875, prec 0.0796857, recall 0.887039
2017-12-10T14:15:19.070488: step 4889, loss 0.225925, acc 0.921875, prec 0.0796782, recall 0.887039
2017-12-10T14:15:19.259695: step 4890, loss 0.79907, acc 1, prec 0.0796956, recall 0.887063
2017-12-10T14:15:19.452662: step 4891, loss 0.108468, acc 0.96875, prec 0.07971, recall 0.887087
2017-12-10T14:15:19.645034: step 4892, loss 0.0156025, acc 0.984375, prec 0.0797259, recall 0.88711
2017-12-10T14:15:19.833655: step 4893, loss 0.112598, acc 0.984375, prec 0.0797418, recall 0.887134
2017-12-10T14:15:20.022595: step 4894, loss 0.103857, acc 0.96875, prec 0.0797388, recall 0.887134
2017-12-10T14:15:20.212215: step 4895, loss 0.99569, acc 0.96875, prec 0.0797532, recall 0.887158
2017-12-10T14:15:20.403097: step 4896, loss 0.126632, acc 0.953125, prec 0.0797487, recall 0.887158
2017-12-10T14:15:20.591482: step 4897, loss 0.299751, acc 0.9375, prec 0.0798123, recall 0.887253
2017-12-10T14:15:20.780818: step 4898, loss 0.448345, acc 0.90625, prec 0.0798206, recall 0.887277
2017-12-10T14:15:20.969445: step 4899, loss 0.172374, acc 0.9375, prec 0.0798494, recall 0.887324
2017-12-10T14:15:21.156753: step 4900, loss 0.240655, acc 0.9375, prec 0.0798608, recall 0.887348
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_1/1512932334/checkpoints/model-4900

2017-12-10T14:15:22.495728: step 4901, loss 0.0758547, acc 0.984375, prec 0.0798767, recall 0.887371
2017-12-10T14:15:22.686234: step 4902, loss 0.087673, acc 0.96875, prec 0.0799085, recall 0.887419
2017-12-10T14:15:22.873567: step 4903, loss 0.759623, acc 0.875, prec 0.0798964, recall 0.887419
2017-12-10T14:15:23.059755: step 4904, loss 0.205202, acc 0.953125, prec 0.0798918, recall 0.887419
2017-12-10T14:15:23.246319: step 4905, loss 0.478319, acc 0.9375, prec 0.0799032, recall 0.887442
2017-12-10T14:15:23.433784: step 4906, loss 3.84566, acc 0.90625, prec 0.0798956, recall 0.887256
2017-12-10T14:15:23.624767: step 4907, loss 0.202971, acc 0.9375, prec 0.0799244, recall 0.887303
2017-12-10T14:15:23.812817: step 4908, loss 0.219603, acc 0.9375, prec 0.0799183, recall 0.887303
2017-12-10T14:15:24.006335: step 4909, loss 0.172982, acc 0.9375, prec 0.0799471, recall 0.887351
2017-12-10T14:15:24.192610: step 4910, loss 0.353437, acc 0.921875, prec 0.0799743, recall 0.887398
2017-12-10T14:15:24.379576: step 4911, loss 0.237616, acc 0.921875, prec 0.0799841, recall 0.887421
2017-12-10T14:15:24.566361: step 4912, loss 0.559084, acc 0.890625, prec 0.0799735, recall 0.887421
2017-12-10T14:15:24.757944: step 4913, loss 0.476166, acc 0.9375, prec 0.0799675, recall 0.887421
2017-12-10T14:15:24.946335: step 4914, loss 0.351534, acc 0.921875, prec 0.07996, recall 0.887421
2017-12-10T14:15:25.131542: step 4915, loss 0.403062, acc 0.90625, prec 0.0799683, recall 0.887445
2017-12-10T14:15:25.318943: step 4916, loss 0.415079, acc 0.890625, prec 0.0799577, recall 0.887445
2017-12-10T14:15:25.507471: step 4917, loss 0.449871, acc 0.90625, prec 0.0799486, recall 0.887445
2017-12-10T14:15:25.701555: step 4918, loss 0.118537, acc 0.96875, prec 0.079963, recall 0.887469
2017-12-10T14:15:25.891738: step 4919, loss 0.142708, acc 0.96875, prec 0.07996, recall 0.887469
2017-12-10T14:15:26.081412: step 4920, loss 0.114056, acc 0.984375, prec 0.0800453, recall 0.887586
2017-12-10T14:15:26.272554: step 4921, loss 0.121685, acc 0.984375, prec 0.0800612, recall 0.88761
2017-12-10T14:15:26.460248: step 4922, loss 0.166721, acc 0.953125, prec 0.0800566, recall 0.88761
2017-12-10T14:15:26.650023: step 4923, loss 0.147675, acc 0.96875, prec 0.080071, recall 0.887633
2017-12-10T14:15:26.838521: step 4924, loss 0.0801675, acc 0.984375, prec 0.0800695, recall 0.887633
2017-12-10T14:15:27.027848: step 4925, loss 0.302926, acc 0.953125, prec 0.0800996, recall 0.88768
2017-12-10T14:15:27.216537: step 4926, loss 0.0878735, acc 0.953125, prec 0.0800951, recall 0.88768
2017-12-10T14:15:27.403810: step 4927, loss 0.522963, acc 0.90625, prec 0.0801034, recall 0.887704
2017-12-10T14:15:27.593874: step 4928, loss 0.547871, acc 0.859375, prec 0.0801072, recall 0.887727
2017-12-10T14:15:27.782963: step 4929, loss 0.115076, acc 0.9375, prec 0.0801358, recall 0.887774
2017-12-10T14:15:27.974612: step 4930, loss 0.00604907, acc 1, prec 0.0801705, recall 0.887821
2017-12-10T14:15:28.165722: step 4931, loss 0.273974, acc 0.9375, prec 0.0801818, recall 0.887845
2017-12-10T14:15:28.357216: step 4932, loss 0.132861, acc 0.96875, prec 0.0802308, recall 0.887915
2017-12-10T14:15:28.545624: step 4933, loss 0.153784, acc 0.9375, prec 0.0802595, recall 0.887962
2017-12-10T14:15:28.736551: step 4934, loss 0.261704, acc 0.96875, prec 0.0802738, recall 0.887985
2017-12-10T14:15:28.932158: step 4935, loss 0.0816292, acc 0.9375, prec 0.0802677, recall 0.887985
2017-12-10T14:15:29.121124: step 4936, loss 0.161934, acc 0.953125, prec 0.0802805, recall 0.888008
2017-12-10T14:15:29.316378: step 4937, loss 0.191317, acc 0.921875, prec 0.0802903, recall 0.888032
2017-12-10T14:15:29.504432: step 4938, loss 0.167071, acc 0.953125, prec 0.0803031, recall 0.888055
2017-12-10T14:15:29.695037: step 4939, loss 0.0218898, acc 0.984375, prec 0.0803016, recall 0.888055
2017-12-10T14:15:29.880544: step 4940, loss 0.150371, acc 0.9375, prec 0.0802955, recall 0.888055
2017-12-10T14:15:30.069429: step 4941, loss 0.390874, acc 0.921875, prec 0.080288, recall 0.888055
2017-12-10T14:15:30.257598: step 4942, loss 0.0427343, acc 0.984375, prec 0.0802865, recall 0.888055
2017-12-10T14:15:30.449523: step 4943, loss 0.190259, acc 0.96875, prec 0.0802834, recall 0.888055
2017-12-10T14:15:30.635373: step 4944, loss 0.346186, acc 0.9375, prec 0.0802947, recall 0.888078
2017-12-10T14:15:30.823691: step 4945, loss 0.33013, acc 0.921875, prec 0.0803045, recall 0.888102
2017-12-10T14:15:31.011841: step 4946, loss 0.0171358, acc 0.984375, prec 0.080303, recall 0.888102
2017-12-10T14:15:31.209934: step 4947, loss 0.0133962, acc 1, prec 0.080303, recall 0.888102
2017-12-10T14:15:31.405547: step 4948, loss 0.0687236, acc 0.984375, prec 0.0803015, recall 0.888102
2017-12-10T14:15:31.593549: step 4949, loss 0.178634, acc 0.96875, prec 0.0803331, recall 0.888148
2017-12-10T14:15:31.783589: step 4950, loss 0.0958097, acc 1, prec 0.0803504, recall 0.888172
2017-12-10T14:15:31.974780: step 4951, loss 0.226662, acc 0.96875, prec 0.0803647, recall 0.888195
2017-12-10T14:15:32.162372: step 4952, loss 0.0442205, acc 0.984375, prec 0.0803978, recall 0.888241
2017-12-10T14:15:32.348767: step 4953, loss 0.166112, acc 0.984375, prec 0.0803963, recall 0.888241
2017-12-10T14:15:32.536098: step 4954, loss 0.108403, acc 0.96875, prec 0.0803933, recall 0.888241
2017-12-10T14:15:32.729495: step 4955, loss 0.0191193, acc 1, prec 0.0804106, recall 0.888265
2017-12-10T14:15:32.919733: step 4956, loss 0.153255, acc 0.984375, prec 0.0804091, recall 0.888265
2017-12-10T14:15:33.108423: step 4957, loss 4.4125, acc 0.953125, prec 0.0804407, recall 0.888126
2017-12-10T14:15:33.299392: step 4958, loss 0.37193, acc 0.953125, prec 0.0804708, recall 0.888173
2017-12-10T14:15:33.488420: step 4959, loss 0.083675, acc 0.953125, prec 0.0804836, recall 0.888196
2017-12-10T14:15:33.676037: step 4960, loss 0.00687362, acc 1, prec 0.0805009, recall 0.888219
2017-12-10T14:15:33.864780: step 4961, loss 0.118349, acc 0.953125, prec 0.0805137, recall 0.888243
2017-12-10T14:15:34.053634: step 4962, loss 0.417515, acc 0.90625, prec 0.0805392, recall 0.888289
2017-12-10T14:15:34.243847: step 4963, loss 0.126169, acc 0.96875, prec 0.0805535, recall 0.888312
2017-12-10T14:15:34.433731: step 4964, loss 0.0834141, acc 0.96875, prec 0.0805504, recall 0.888312
2017-12-10T14:15:34.620503: step 4965, loss 0.359066, acc 0.890625, prec 0.0805398, recall 0.888312
2017-12-10T14:15:34.811352: step 4966, loss 2.62851, acc 0.921875, prec 0.0805338, recall 0.888128
2017-12-10T14:15:35.006972: step 4967, loss 0.163893, acc 0.9375, prec 0.0805623, recall 0.888174
2017-12-10T14:15:35.197344: step 4968, loss 0.162462, acc 0.9375, prec 0.0805562, recall 0.888174
2017-12-10T14:15:35.384343: step 4969, loss 0.293632, acc 0.890625, prec 0.0805629, recall 0.888197
2017-12-10T14:15:35.557819: step 4970, loss 0.305792, acc 0.823529, prec 0.0805493, recall 0.888197
Training finished
Starting Fold: 2 => Train/Dev split: 31796/10598


 #################### 


SEQUENCE LENGTH 273
BATCH SIZE 64
EMBEDDING SIZE 128
FILTER SIZES [3, 4, 5]
NUMBER OF FILTERS 128
L2 REG LAMBDA 0.0
EPOCHS 10



RESULT DIR leave_position_embedding_out_fold_2
VOCAB SIZE 33447
DROPOUT PROBABILITY 0.5



WORD EMBEDDING True
POSITION EMBEDDING False
POS EMBEDDING True


 #################### 


Writing to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_2/1512933336

Start training
2017-12-10T14:15:39.275390: step 1, loss 13.3237, acc 0.265625, prec 0, recall 0
2017-12-10T14:15:39.461203: step 2, loss 1.9692, acc 0.5, prec 0.0126582, recall 0.5
2017-12-10T14:15:39.646373: step 3, loss 6.39577, acc 0.53125, prec 0.0183486, recall 0.5
2017-12-10T14:15:39.834813: step 4, loss 22.3452, acc 0.4375, prec 0.0140845, recall 0.285714
2017-12-10T14:15:40.025292: step 5, loss 2.39972, acc 0.5, prec 0.0171429, recall 0.375
2017-12-10T14:15:40.211358: step 6, loss 3.39531, acc 0.390625, prec 0.0140187, recall 0.375
2017-12-10T14:15:40.399965: step 7, loss 2.63316, acc 0.375, prec 0.0156863, recall 0.444444
2017-12-10T14:15:40.583748: step 8, loss 3.41822, acc 0.328125, prec 0.0134228, recall 0.444444
2017-12-10T14:15:40.777191: step 9, loss 3.04921, acc 0.40625, prec 0.0148368, recall 0.5
2017-12-10T14:15:40.961565: step 10, loss 3.69216, acc 0.390625, prec 0.0132979, recall 0.5
2017-12-10T14:15:41.146421: step 11, loss 8.0112, acc 0.40625, prec 0.0121065, recall 0.454545
2017-12-10T14:15:41.334257: step 12, loss 3.94931, acc 0.328125, prec 0.0109649, recall 0.454545
2017-12-10T14:15:41.518591: step 13, loss 3.09311, acc 0.390625, prec 0.010101, recall 0.454545
2017-12-10T14:15:41.702639: step 14, loss 3.39381, acc 0.28125, prec 0.00924214, recall 0.454545
2017-12-10T14:15:41.883816: step 15, loss 12.5062, acc 0.5, prec 0.00875657, recall 0.384615
2017-12-10T14:15:42.072245: step 16, loss 2.00856, acc 0.40625, prec 0.00983607, recall 0.428571
2017-12-10T14:15:42.256833: step 17, loss 8.02748, acc 0.328125, prec 0.00920245, recall 0.4
2017-12-10T14:15:42.441731: step 18, loss 2.75506, acc 0.453125, prec 0.00873362, recall 0.4
2017-12-10T14:15:42.626059: step 19, loss 13.4941, acc 0.453125, prec 0.00969529, recall 0.411765
2017-12-10T14:15:42.818019: step 20, loss 19.4065, acc 0.390625, prec 0.0105125, recall 0.421053
2017-12-10T14:15:43.007884: step 21, loss 2.64892, acc 0.40625, prec 0.01125, recall 0.45
2017-12-10T14:15:43.190993: step 22, loss 3.48359, acc 0.328125, prec 0.0106762, recall 0.45
2017-12-10T14:15:43.375521: step 23, loss 3.19581, acc 0.34375, prec 0.0101695, recall 0.45
2017-12-10T14:15:43.562089: step 24, loss 10.9779, acc 0.359375, prec 0.00972973, recall 0.428571
2017-12-10T14:15:43.749465: step 25, loss 3.43802, acc 0.34375, prec 0.00930714, recall 0.428571
2017-12-10T14:15:43.942101: step 26, loss 3.16025, acc 0.265625, prec 0.00985222, recall 0.454545
2017-12-10T14:15:44.128797: step 27, loss 3.36104, acc 0.375, prec 0.0104167, recall 0.478261
2017-12-10T14:15:44.311537: step 28, loss 2.19906, acc 0.515625, prec 0.0101196, recall 0.478261
2017-12-10T14:15:44.500713: step 29, loss 2.50876, acc 0.40625, prec 0.00977778, recall 0.478261
2017-12-10T14:15:44.684760: step 30, loss 2.50726, acc 0.484375, prec 0.00949914, recall 0.478261
2017-12-10T14:15:44.869928: step 31, loss 3.13705, acc 0.484375, prec 0.0108969, recall 0.52
2017-12-10T14:15:45.059549: step 32, loss 2.5807, acc 0.4375, prec 0.0113821, recall 0.538462
2017-12-10T14:15:45.243954: step 33, loss 3.16508, acc 0.5625, prec 0.0119237, recall 0.535714
2017-12-10T14:15:45.430371: step 34, loss 2.25054, acc 0.53125, prec 0.011646, recall 0.535714
2017-12-10T14:15:45.614319: step 35, loss 1.88497, acc 0.53125, prec 0.0113809, recall 0.535714
2017-12-10T14:15:45.800907: step 36, loss 1.15269, acc 0.6875, prec 0.0112108, recall 0.535714
2017-12-10T14:15:45.990640: step 37, loss 1.30321, acc 0.578125, prec 0.010989, recall 0.535714
2017-12-10T14:15:46.174953: step 38, loss 0.978131, acc 0.671875, prec 0.0108225, recall 0.535714
2017-12-10T14:15:46.359641: step 39, loss 1.95483, acc 0.671875, prec 0.0113636, recall 0.551724
2017-12-10T14:15:46.546094: step 40, loss 0.830423, acc 0.75, prec 0.011236, recall 0.551724
2017-12-10T14:15:46.733640: step 41, loss 6.35767, acc 0.890625, prec 0.0111888, recall 0.533333
2017-12-10T14:15:46.925747: step 42, loss 0.359611, acc 0.859375, prec 0.0111188, recall 0.533333
2017-12-10T14:15:47.114534: step 43, loss 0.737932, acc 0.765625, prec 0.0110041, recall 0.533333
2017-12-10T14:15:47.304258: step 44, loss 13.2647, acc 0.65625, prec 0.0108475, recall 0.516129
2017-12-10T14:15:47.494868: step 45, loss 1.71559, acc 0.859375, prec 0.0114478, recall 0.53125
2017-12-10T14:15:47.684765: step 46, loss 6.34604, acc 0.859375, prec 0.0113865, recall 0.515152
2017-12-10T14:15:47.872141: step 47, loss 9.85911, acc 0.78125, prec 0.0112957, recall 0.485714
2017-12-10T14:15:48.061259: step 48, loss 6.208, acc 0.71875, prec 0.0111695, recall 0.472222
2017-12-10T14:15:48.250830: step 49, loss 1.5099, acc 0.640625, prec 0.0110032, recall 0.472222
2017-12-10T14:15:48.442519: step 50, loss 11.0379, acc 0.5, prec 0.0107868, recall 0.459459
2017-12-10T14:15:48.627584: step 51, loss 9.62314, acc 0.609375, prec 0.010625, recall 0.447368
2017-12-10T14:15:48.812524: step 52, loss 3.24398, acc 0.46875, prec 0.0110092, recall 0.461538
2017-12-10T14:15:49.000693: step 53, loss 2.91808, acc 0.359375, prec 0.0107399, recall 0.461538
2017-12-10T14:15:49.185656: step 54, loss 4.84627, acc 0.375, prec 0.0104956, recall 0.45
2017-12-10T14:15:49.370970: step 55, loss 3.42701, acc 0.3125, prec 0.0102331, recall 0.45
2017-12-10T14:15:49.556273: step 56, loss 4.01639, acc 0.28125, prec 0.0105205, recall 0.463415
2017-12-10T14:15:49.741023: step 57, loss 4.13034, acc 0.234375, prec 0.0102426, recall 0.463415
2017-12-10T14:15:49.927646: step 58, loss 4.83885, acc 0.234375, prec 0.0104987, recall 0.47619
2017-12-10T14:15:50.114828: step 59, loss 4.03958, acc 0.25, prec 0.0102407, recall 0.47619
2017-12-10T14:15:50.300273: step 60, loss 4.04823, acc 0.265625, prec 0.01, recall 0.47619
2017-12-10T14:15:50.488608: step 61, loss 4.56164, acc 0.203125, prec 0.00975134, recall 0.47619
2017-12-10T14:15:50.672287: step 62, loss 6.29566, acc 0.3125, prec 0.0095511, recall 0.465116
2017-12-10T14:15:50.859943: step 63, loss 11.1226, acc 0.28125, prec 0.00935016, recall 0.454545
2017-12-10T14:15:51.042526: step 64, loss 6.02364, acc 0.359375, prec 0.00963303, recall 0.456522
2017-12-10T14:15:51.229006: step 65, loss 3.56073, acc 0.359375, prec 0.0094552, recall 0.456522
2017-12-10T14:15:51.412897: step 66, loss 13.4147, acc 0.234375, prec 0.00969163, recall 0.458333
2017-12-10T14:15:51.601552: step 67, loss 4.14024, acc 0.359375, prec 0.0099481, recall 0.469388
2017-12-10T14:15:51.788925: step 68, loss 4.63546, acc 0.296875, prec 0.00975817, recall 0.469388
2017-12-10T14:15:51.979235: step 69, loss 3.58318, acc 0.265625, prec 0.00956739, recall 0.469388
2017-12-10T14:15:52.166068: step 70, loss 4.34877, acc 0.296875, prec 0.0106036, recall 0.5
2017-12-10T14:15:52.351120: step 71, loss 3.50749, acc 0.265625, prec 0.0108, recall 0.509434
2017-12-10T14:15:52.534837: step 72, loss 4.25428, acc 0.28125, prec 0.0106049, recall 0.509434
2017-12-10T14:15:52.719636: step 73, loss 3.92352, acc 0.234375, prec 0.0107858, recall 0.518519
2017-12-10T14:15:52.902038: step 74, loss 2.64397, acc 0.390625, prec 0.0106262, recall 0.518519
2017-12-10T14:15:53.088074: step 75, loss 2.50757, acc 0.484375, prec 0.0104948, recall 0.518519
2017-12-10T14:15:53.276463: step 76, loss 3.22372, acc 0.390625, prec 0.010709, recall 0.527273
2017-12-10T14:15:53.460050: step 77, loss 1.70953, acc 0.578125, prec 0.0109649, recall 0.535714
2017-12-10T14:15:53.643533: step 78, loss 1.86268, acc 0.5625, prec 0.0112116, recall 0.54386
2017-12-10T14:15:53.826426: step 79, loss 1.70398, acc 0.578125, prec 0.0114572, recall 0.551724
2017-12-10T14:15:54.013864: step 80, loss 8.55721, acc 0.640625, prec 0.0113677, recall 0.542373
2017-12-10T14:15:54.199817: step 81, loss 1.10066, acc 0.75, prec 0.0113034, recall 0.542373
2017-12-10T14:15:54.390339: step 82, loss 0.817674, acc 0.8125, prec 0.0112557, recall 0.542373
2017-12-10T14:15:54.581894: step 83, loss 0.71553, acc 0.75, prec 0.0115385, recall 0.55
2017-12-10T14:15:54.772933: step 84, loss 0.483117, acc 0.859375, prec 0.0115023, recall 0.55
2017-12-10T14:15:54.962810: step 85, loss 0.719052, acc 0.8125, prec 0.0114544, recall 0.55
2017-12-10T14:15:55.149469: step 86, loss 6.37504, acc 0.75, prec 0.011395, recall 0.540984
2017-12-10T14:15:55.339821: step 87, loss 7.35053, acc 0.8125, prec 0.0113519, recall 0.532258
2017-12-10T14:15:55.529428: step 88, loss 0.489442, acc 0.84375, prec 0.011313, recall 0.532258
2017-12-10T14:15:55.718926: step 89, loss 0.328879, acc 0.859375, prec 0.0112782, recall 0.532258
2017-12-10T14:15:55.907084: step 90, loss 2.6007, acc 0.828125, prec 0.0112398, recall 0.52381
2017-12-10T14:15:56.094609: step 91, loss 1.99368, acc 0.75, prec 0.0111826, recall 0.515625
2017-12-10T14:15:56.279439: step 92, loss 0.882743, acc 0.796875, prec 0.0118004, recall 0.530303
2017-12-10T14:15:56.466572: step 93, loss 12.9704, acc 0.734375, prec 0.011741, recall 0.514706
2017-12-10T14:15:56.657392: step 94, loss 5.47535, acc 0.6875, prec 0.0116667, recall 0.507246
2017-12-10T14:15:56.845574: step 95, loss 3.95922, acc 0.59375, prec 0.0115702, recall 0.5
2017-12-10T14:15:57.035822: step 96, loss 6.48687, acc 0.703125, prec 0.0118265, recall 0.5
2017-12-10T14:15:57.225177: step 97, loss 3.66737, acc 0.421875, prec 0.0120052, recall 0.506849
2017-12-10T14:15:57.411226: step 98, loss 3.11094, acc 0.40625, prec 0.012492, recall 0.52
2017-12-10T14:15:57.601634: step 99, loss 3.07897, acc 0.3125, prec 0.0123184, recall 0.52
2017-12-10T14:15:57.784256: step 100, loss 3.84083, acc 0.328125, prec 0.0124611, recall 0.526316
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_2/1512933336/checkpoints/model-100

2017-12-10T14:15:59.204323: step 101, loss 4.84559, acc 0.234375, prec 0.0122737, recall 0.526316
2017-12-10T14:15:59.394364: step 102, loss 3.47167, acc 0.34375, prec 0.0121175, recall 0.526316
2017-12-10T14:15:59.575656: step 103, loss 3.9647, acc 0.3125, prec 0.0122534, recall 0.532468
2017-12-10T14:15:59.760414: step 104, loss 4.71294, acc 0.28125, prec 0.0126694, recall 0.544304
2017-12-10T14:15:59.943047: step 105, loss 4.13104, acc 0.265625, prec 0.0124964, recall 0.544304
2017-12-10T14:16:00.129587: step 106, loss 4.68857, acc 0.234375, prec 0.0131692, recall 0.560976
2017-12-10T14:16:00.312122: step 107, loss 4.24928, acc 0.21875, prec 0.0129833, recall 0.560976
2017-12-10T14:16:00.499470: step 108, loss 4.85698, acc 0.328125, prec 0.0136528, recall 0.576471
2017-12-10T14:16:00.685584: step 109, loss 3.07211, acc 0.34375, prec 0.0134949, recall 0.576471
2017-12-10T14:16:00.875678: step 110, loss 3.78924, acc 0.375, prec 0.0133479, recall 0.576471
2017-12-10T14:16:01.066303: step 111, loss 2.11233, acc 0.5625, prec 0.0137801, recall 0.586207
2017-12-10T14:16:01.259078: step 112, loss 1.9384, acc 0.53125, prec 0.0139335, recall 0.590909
2017-12-10T14:16:01.450056: step 113, loss 1.20498, acc 0.6875, prec 0.0138593, recall 0.590909
2017-12-10T14:16:01.638844: step 114, loss 1.56303, acc 0.59375, prec 0.0137639, recall 0.590909
2017-12-10T14:16:01.824382: step 115, loss 1.84147, acc 0.578125, prec 0.0136662, recall 0.590909
2017-12-10T14:16:02.009355: step 116, loss 5.93915, acc 0.5625, prec 0.0138273, recall 0.588889
2017-12-10T14:16:02.194832: step 117, loss 6.37297, acc 0.65625, prec 0.0140078, recall 0.586957
2017-12-10T14:16:02.383793: step 118, loss 0.920278, acc 0.734375, prec 0.0142009, recall 0.591398
2017-12-10T14:16:02.570816: step 119, loss 13.7969, acc 0.75, prec 0.0141497, recall 0.578947
2017-12-10T14:16:02.762205: step 120, loss 6.29055, acc 0.765625, prec 0.0140989, recall 0.572917
2017-12-10T14:16:02.952053: step 121, loss 1.28433, acc 0.640625, prec 0.0142675, recall 0.57732
2017-12-10T14:16:03.142931: step 122, loss 1.50821, acc 0.640625, prec 0.0141844, recall 0.57732
2017-12-10T14:16:03.327404: step 123, loss 11.7447, acc 0.65625, prec 0.0141093, recall 0.571429
2017-12-10T14:16:03.515039: step 124, loss 1.43273, acc 0.609375, prec 0.0142678, recall 0.575758
2017-12-10T14:16:03.701105: step 125, loss 1.65133, acc 0.671875, prec 0.0144386, recall 0.58
2017-12-10T14:16:03.887840: step 126, loss 10.0965, acc 0.53125, prec 0.0153086, recall 0.590476
2017-12-10T14:16:04.075215: step 127, loss 2.40549, acc 0.546875, prec 0.0156824, recall 0.598131
2017-12-10T14:16:04.260777: step 128, loss 2.49211, acc 0.484375, prec 0.0155566, recall 0.598131
2017-12-10T14:16:04.447141: step 129, loss 2.88676, acc 0.484375, prec 0.0156702, recall 0.601852
2017-12-10T14:16:04.637756: step 130, loss 2.75874, acc 0.546875, prec 0.0155614, recall 0.601852
2017-12-10T14:16:04.824444: step 131, loss 2.53, acc 0.390625, prec 0.0154175, recall 0.601852
2017-12-10T14:16:05.011774: step 132, loss 3.24519, acc 0.421875, prec 0.0155148, recall 0.605505
2017-12-10T14:16:05.197440: step 133, loss 2.28368, acc 0.484375, prec 0.015625, recall 0.609091
2017-12-10T14:16:05.381838: step 134, loss 2.65557, acc 0.375, prec 0.015708, recall 0.612613
2017-12-10T14:16:05.565344: step 135, loss 3.06138, acc 0.3125, prec 0.015775, recall 0.616071
2017-12-10T14:16:05.749873: step 136, loss 2.29373, acc 0.4375, prec 0.0156463, recall 0.616071
2017-12-10T14:16:05.936272: step 137, loss 2.35294, acc 0.515625, prec 0.0157587, recall 0.619469
2017-12-10T14:16:06.122056: step 138, loss 6.17006, acc 0.71875, prec 0.0156986, recall 0.614035
2017-12-10T14:16:06.312330: step 139, loss 1.13582, acc 0.640625, prec 0.015618, recall 0.614035
2017-12-10T14:16:06.497899: step 140, loss 1.54267, acc 0.59375, prec 0.015528, recall 0.614035
2017-12-10T14:16:06.685702: step 141, loss 1.54695, acc 0.625, prec 0.0154457, recall 0.614035
2017-12-10T14:16:06.871379: step 142, loss 6.43141, acc 0.59375, prec 0.015577, recall 0.612069
2017-12-10T14:16:07.060220: step 143, loss 8.30793, acc 0.609375, prec 0.0157137, recall 0.605042
2017-12-10T14:16:07.250904: step 144, loss 1.57255, acc 0.609375, prec 0.015842, recall 0.608333
2017-12-10T14:16:07.432739: step 145, loss 1.90677, acc 0.59375, prec 0.0157531, recall 0.608333
2017-12-10T14:16:07.621452: step 146, loss 1.75496, acc 0.53125, prec 0.0156518, recall 0.608333
2017-12-10T14:16:07.807319: step 147, loss 1.64421, acc 0.65625, prec 0.0155783, recall 0.608333
2017-12-10T14:16:07.992290: step 148, loss 2.44349, acc 0.484375, prec 0.015678, recall 0.61157
2017-12-10T14:16:08.176334: step 149, loss 1.62447, acc 0.625, prec 0.0155987, recall 0.61157
2017-12-10T14:16:08.360865: step 150, loss 1.83339, acc 0.53125, prec 0.0155006, recall 0.61157
2017-12-10T14:16:08.545242: step 151, loss 15.9522, acc 0.65625, prec 0.0158432, recall 0.612903
2017-12-10T14:16:08.736920: step 152, loss 1.41373, acc 0.625, prec 0.0157644, recall 0.612903
2017-12-10T14:16:08.923558: step 153, loss 3.73798, acc 0.578125, prec 0.0156798, recall 0.608
2017-12-10T14:16:09.115419: step 154, loss 2.28385, acc 0.5625, prec 0.0155897, recall 0.608
2017-12-10T14:16:09.301811: step 155, loss 2.1759, acc 0.515625, prec 0.0154912, recall 0.608
2017-12-10T14:16:09.487887: step 156, loss 2.4629, acc 0.5, prec 0.0157895, recall 0.614173
2017-12-10T14:16:09.671327: step 157, loss 1.56488, acc 0.5625, prec 0.0157005, recall 0.614173
2017-12-10T14:16:09.860380: step 158, loss 1.3218, acc 0.640625, prec 0.0158253, recall 0.617188
2017-12-10T14:16:10.047209: step 159, loss 1.60701, acc 0.5625, prec 0.0157371, recall 0.617188
2017-12-10T14:16:10.235887: step 160, loss 1.81056, acc 0.640625, prec 0.0158604, recall 0.620155
2017-12-10T14:16:10.419744: step 161, loss 1.40678, acc 0.640625, prec 0.0157884, recall 0.620155
2017-12-10T14:16:10.603237: step 162, loss 2.34929, acc 0.734375, prec 0.0157387, recall 0.615385
2017-12-10T14:16:10.794306: step 163, loss 1.9397, acc 0.53125, prec 0.0156464, recall 0.615385
2017-12-10T14:16:10.981407: step 164, loss 1.09618, acc 0.703125, prec 0.0157802, recall 0.618321
2017-12-10T14:16:11.173555: step 165, loss 13.1704, acc 0.734375, prec 0.0157343, recall 0.609023
2017-12-10T14:16:11.364312: step 166, loss 0.752264, acc 0.75, prec 0.0156855, recall 0.609023
2017-12-10T14:16:11.551705: step 167, loss 1.29877, acc 0.6875, prec 0.015625, recall 0.609023
2017-12-10T14:16:11.738693: step 168, loss 1.14814, acc 0.75, prec 0.0157662, recall 0.61194
2017-12-10T14:16:11.927006: step 169, loss 0.667545, acc 0.765625, prec 0.0157209, recall 0.61194
2017-12-10T14:16:12.111995: step 170, loss 12.2664, acc 0.609375, prec 0.0162121, recall 0.615942
2017-12-10T14:16:12.304690: step 171, loss 7.00666, acc 0.796875, prec 0.0167364, recall 0.619718
2017-12-10T14:16:12.497633: step 172, loss 1.15352, acc 0.671875, prec 0.0166698, recall 0.619718
2017-12-10T14:16:12.684712: step 173, loss 14.9842, acc 0.5, prec 0.0165725, recall 0.615385
2017-12-10T14:16:12.873966: step 174, loss 1.65585, acc 0.59375, prec 0.0164918, recall 0.615385
2017-12-10T14:16:13.067091: step 175, loss 1.54476, acc 0.609375, prec 0.0164148, recall 0.615385
2017-12-10T14:16:13.252959: step 176, loss 2.56409, acc 0.390625, prec 0.0162963, recall 0.615385
2017-12-10T14:16:13.439002: step 177, loss 2.23682, acc 0.546875, prec 0.0163904, recall 0.618056
2017-12-10T14:16:13.623659: step 178, loss 1.83831, acc 0.53125, prec 0.0164805, recall 0.62069
2017-12-10T14:16:13.808872: step 179, loss 2.52041, acc 0.5625, prec 0.0167547, recall 0.62585
2017-12-10T14:16:14.001135: step 180, loss 2.65866, acc 0.453125, prec 0.0166486, recall 0.62585
2017-12-10T14:16:14.189178: step 181, loss 3.14775, acc 0.515625, prec 0.0165587, recall 0.621622
2017-12-10T14:16:14.378427: step 182, loss 8.32327, acc 0.5, prec 0.0164668, recall 0.61745
2017-12-10T14:16:14.565311: step 183, loss 10.1833, acc 0.609375, prec 0.0165716, recall 0.615894
2017-12-10T14:16:14.749797: step 184, loss 3.00452, acc 0.59375, prec 0.0164981, recall 0.611842
2017-12-10T14:16:14.936217: step 185, loss 2.52175, acc 0.421875, prec 0.0163906, recall 0.611842
2017-12-10T14:16:15.123972: step 186, loss 2.27839, acc 0.375, prec 0.0164479, recall 0.614379
2017-12-10T14:16:15.308266: step 187, loss 3.31913, acc 0.171875, prec 0.0162968, recall 0.614379
2017-12-10T14:16:15.493045: step 188, loss 4.05383, acc 0.265625, prec 0.0163343, recall 0.616883
2017-12-10T14:16:15.679336: step 189, loss 3.41383, acc 0.3125, prec 0.0162116, recall 0.616883
2017-12-10T14:16:15.860291: step 190, loss 5.34909, acc 0.3125, prec 0.0162602, recall 0.615385
2017-12-10T14:16:16.046294: step 191, loss 3.46521, acc 0.3125, prec 0.0161399, recall 0.615385
2017-12-10T14:16:16.232521: step 192, loss 10.0302, acc 0.453125, prec 0.0160481, recall 0.611465
2017-12-10T14:16:16.420244: step 193, loss 3.56585, acc 0.5, prec 0.0162899, recall 0.616352
2017-12-10T14:16:16.607319: step 194, loss 2.01602, acc 0.453125, prec 0.0163582, recall 0.61875
2017-12-10T14:16:16.789807: step 195, loss 2.90146, acc 0.375, prec 0.0164123, recall 0.621118
2017-12-10T14:16:16.974054: step 196, loss 2.3843, acc 0.546875, prec 0.0163345, recall 0.621118
2017-12-10T14:16:17.157142: step 197, loss 2.22379, acc 0.484375, prec 0.016247, recall 0.621118
2017-12-10T14:16:17.342856: step 198, loss 13.7545, acc 0.46875, prec 0.0161629, recall 0.613497
2017-12-10T14:16:17.530753: step 199, loss 5.41375, acc 0.421875, prec 0.0160694, recall 0.609756
2017-12-10T14:16:17.717078: step 200, loss 3.17614, acc 0.421875, prec 0.0162887, recall 0.614458
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_2/1512933336/checkpoints/model-200

2017-12-10T14:16:18.876246: step 201, loss 2.73253, acc 0.375, prec 0.0161853, recall 0.614458
2017-12-10T14:16:19.060673: step 202, loss 2.51509, acc 0.5, prec 0.0162589, recall 0.616766
2017-12-10T14:16:19.243598: step 203, loss 2.43108, acc 0.46875, prec 0.0161721, recall 0.616766
2017-12-10T14:16:19.434142: step 204, loss 1.88063, acc 0.546875, prec 0.0164062, recall 0.621302
2017-12-10T14:16:19.620051: step 205, loss 2.31118, acc 0.484375, prec 0.016475, recall 0.623529
2017-12-10T14:16:19.804546: step 206, loss 1.79231, acc 0.609375, prec 0.0164112, recall 0.623529
2017-12-10T14:16:19.990393: step 207, loss 4.02959, acc 0.375, prec 0.0164615, recall 0.625731
2017-12-10T14:16:20.175857: step 208, loss 5.22417, acc 0.53125, prec 0.0163884, recall 0.622093
2017-12-10T14:16:20.368663: step 209, loss 2.2339, acc 0.484375, prec 0.016306, recall 0.622093
2017-12-10T14:16:20.550371: step 210, loss 1.65811, acc 0.515625, prec 0.0162293, recall 0.622093
2017-12-10T14:16:20.733933: step 211, loss 2.46543, acc 0.46875, prec 0.0161461, recall 0.622093
2017-12-10T14:16:20.917372: step 212, loss 7.10289, acc 0.515625, prec 0.0162211, recall 0.62069
2017-12-10T14:16:21.105107: step 213, loss 15.1208, acc 0.5, prec 0.0162954, recall 0.615819
2017-12-10T14:16:21.291325: step 214, loss 2.5138, acc 0.453125, prec 0.0163569, recall 0.617977
2017-12-10T14:16:21.474788: step 215, loss 3.34692, acc 0.546875, prec 0.0164347, recall 0.616667
2017-12-10T14:16:21.660072: step 216, loss 3.43198, acc 0.34375, prec 0.0164779, recall 0.618785
2017-12-10T14:16:21.842797: step 217, loss 1.93567, acc 0.515625, prec 0.016835, recall 0.625
2017-12-10T14:16:22.030734: step 218, loss 2.82892, acc 0.484375, prec 0.0170405, recall 0.629032
2017-12-10T14:16:22.217499: step 219, loss 2.43135, acc 0.4375, prec 0.0169516, recall 0.629032
2017-12-10T14:16:22.405578: step 220, loss 2.83545, acc 0.40625, prec 0.0168588, recall 0.629032
2017-12-10T14:16:22.593993: step 221, loss 6.15918, acc 0.5, prec 0.0167838, recall 0.625668
2017-12-10T14:16:22.779888: step 222, loss 3.61517, acc 0.5, prec 0.0169879, recall 0.62963
2017-12-10T14:16:22.966302: step 223, loss 1.85115, acc 0.515625, prec 0.0171924, recall 0.633508
2017-12-10T14:16:23.151744: step 224, loss 5.44673, acc 0.46875, prec 0.0171121, recall 0.630208
2017-12-10T14:16:23.338944: step 225, loss 2.94666, acc 0.421875, prec 0.0172996, recall 0.634021
2017-12-10T14:16:23.524594: step 226, loss 6.62813, acc 0.375, prec 0.0172052, recall 0.630769
2017-12-10T14:16:23.712447: step 227, loss 2.53469, acc 0.53125, prec 0.0172702, recall 0.632653
2017-12-10T14:16:23.898198: step 228, loss 2.22666, acc 0.546875, prec 0.017337, recall 0.634518
2017-12-10T14:16:24.085065: step 229, loss 3.37294, acc 0.421875, prec 0.0173841, recall 0.636364
2017-12-10T14:16:24.271555: step 230, loss 1.8025, acc 0.453125, prec 0.0174355, recall 0.638191
2017-12-10T14:16:24.455875: step 231, loss 2.19603, acc 0.5, prec 0.0173592, recall 0.638191
2017-12-10T14:16:24.642921: step 232, loss 2.85256, acc 0.40625, prec 0.0175367, recall 0.641791
2017-12-10T14:16:24.830600: step 233, loss 3.71714, acc 0.5625, prec 0.0174726, recall 0.638614
2017-12-10T14:16:25.018362: step 234, loss 2.06781, acc 0.578125, prec 0.0175415, recall 0.640394
2017-12-10T14:16:25.210075: step 235, loss 1.4593, acc 0.640625, prec 0.0177515, recall 0.643902
2017-12-10T14:16:25.397965: step 236, loss 4.57646, acc 0.515625, prec 0.0176801, recall 0.640777
2017-12-10T14:16:25.588611: step 237, loss 1.90059, acc 0.59375, prec 0.017881, recall 0.644231
2017-12-10T14:16:25.773248: step 238, loss 20.731, acc 0.65625, prec 0.0179665, recall 0.636792
2017-12-10T14:16:25.962798: step 239, loss 2.07232, acc 0.53125, prec 0.0180252, recall 0.638498
2017-12-10T14:16:26.147444: step 240, loss 2.50994, acc 0.4375, prec 0.0179396, recall 0.638498
2017-12-10T14:16:26.333254: step 241, loss 4.4428, acc 0.390625, prec 0.0178501, recall 0.635514
2017-12-10T14:16:26.521117: step 242, loss 2.40902, acc 0.390625, prec 0.0177592, recall 0.635514
2017-12-10T14:16:26.706626: step 243, loss 2.54041, acc 0.421875, prec 0.0176738, recall 0.635514
2017-12-10T14:16:26.895341: step 244, loss 2.80052, acc 0.46875, prec 0.0177232, recall 0.637209
2017-12-10T14:16:27.081452: step 245, loss 3.49694, acc 0.484375, prec 0.0180273, recall 0.642202
2017-12-10T14:16:27.269109: step 246, loss 4.04882, acc 0.421875, prec 0.0179441, recall 0.639269
2017-12-10T14:16:27.453889: step 247, loss 2.9433, acc 0.34375, prec 0.0179732, recall 0.640909
2017-12-10T14:16:27.637881: step 248, loss 2.64181, acc 0.53125, prec 0.0180295, recall 0.642534
2017-12-10T14:16:27.825829: step 249, loss 4.15244, acc 0.390625, prec 0.0183127, recall 0.647321
2017-12-10T14:16:28.007822: step 250, loss 2.84392, acc 0.4375, prec 0.0184766, recall 0.650442
2017-12-10T14:16:28.192941: step 251, loss 2.95378, acc 0.484375, prec 0.0185232, recall 0.651982
2017-12-10T14:16:28.379943: step 252, loss 3.1429, acc 0.390625, prec 0.0184332, recall 0.651982
2017-12-10T14:16:28.567319: step 253, loss 2.37413, acc 0.46875, prec 0.0184772, recall 0.653509
2017-12-10T14:16:28.754424: step 254, loss 1.68802, acc 0.59375, prec 0.0184178, recall 0.653509
2017-12-10T14:16:28.946084: step 255, loss 2.19836, acc 0.484375, prec 0.0184638, recall 0.655022
2017-12-10T14:16:29.136613: step 256, loss 1.93166, acc 0.515625, prec 0.0183936, recall 0.655022
2017-12-10T14:16:29.320024: step 257, loss 6.58132, acc 0.59375, prec 0.0183374, recall 0.652174
2017-12-10T14:16:29.504011: step 258, loss 2.48201, acc 0.515625, prec 0.0182682, recall 0.652174
2017-12-10T14:16:29.691086: step 259, loss 1.85737, acc 0.609375, prec 0.0183319, recall 0.65368
2017-12-10T14:16:29.874523: step 260, loss 3.83044, acc 0.734375, prec 0.0182964, recall 0.650862
2017-12-10T14:16:30.060418: step 261, loss 4.60228, acc 0.625, prec 0.0182455, recall 0.648069
2017-12-10T14:16:30.248311: step 262, loss 5.18555, acc 0.609375, prec 0.0181928, recall 0.645299
2017-12-10T14:16:30.436554: step 263, loss 1.2968, acc 0.703125, prec 0.0181512, recall 0.645299
2017-12-10T14:16:30.623102: step 264, loss 9.58086, acc 0.609375, prec 0.018099, recall 0.642553
2017-12-10T14:16:30.809844: step 265, loss 13.2978, acc 0.46875, prec 0.0180277, recall 0.639831
2017-12-10T14:16:30.998103: step 266, loss 2.54963, acc 0.4375, prec 0.0179505, recall 0.639831
2017-12-10T14:16:31.185573: step 267, loss 2.45878, acc 0.578125, prec 0.0180095, recall 0.64135
2017-12-10T14:16:31.376553: step 268, loss 3.69768, acc 0.515625, prec 0.0179457, recall 0.638655
2017-12-10T14:16:31.566514: step 269, loss 3.59464, acc 0.5, prec 0.0178802, recall 0.635983
2017-12-10T14:16:31.755704: step 270, loss 2.56362, acc 0.453125, prec 0.0178069, recall 0.635983
2017-12-10T14:16:31.941593: step 271, loss 1.99732, acc 0.515625, prec 0.0179718, recall 0.639004
2017-12-10T14:16:32.123168: step 272, loss 4.61732, acc 0.390625, prec 0.0180065, recall 0.63786
2017-12-10T14:16:32.315040: step 273, loss 3.62738, acc 0.375, prec 0.0180368, recall 0.639344
2017-12-10T14:16:32.497018: step 274, loss 2.55953, acc 0.46875, prec 0.0180792, recall 0.640816
2017-12-10T14:16:32.682866: step 275, loss 2.73535, acc 0.5, prec 0.0182381, recall 0.643725
2017-12-10T14:16:32.870112: step 276, loss 2.20362, acc 0.5625, prec 0.0184042, recall 0.646586
2017-12-10T14:16:33.055937: step 277, loss 2.6868, acc 0.40625, prec 0.0184363, recall 0.648
2017-12-10T14:16:33.246638: step 278, loss 3.74646, acc 0.359375, prec 0.0183507, recall 0.648
2017-12-10T14:16:33.429733: step 279, loss 5.76224, acc 0.484375, prec 0.0182844, recall 0.645418
2017-12-10T14:16:33.622947: step 280, loss 2.83783, acc 0.453125, prec 0.0182125, recall 0.645418
2017-12-10T14:16:33.812596: step 281, loss 2.2687, acc 0.484375, prec 0.0181452, recall 0.645418
2017-12-10T14:16:34.000613: step 282, loss 6.78357, acc 0.546875, prec 0.0183077, recall 0.645669
2017-12-10T14:16:34.185959: step 283, loss 2.31409, acc 0.46875, prec 0.0183476, recall 0.647059
2017-12-10T14:16:34.369825: step 284, loss 10.2666, acc 0.5, prec 0.0183934, recall 0.645914
2017-12-10T14:16:34.559827: step 285, loss 2.9031, acc 0.40625, prec 0.0183162, recall 0.645914
2017-12-10T14:16:34.744860: step 286, loss 2.42389, acc 0.5625, prec 0.0182598, recall 0.645914
2017-12-10T14:16:34.928458: step 287, loss 2.71983, acc 0.453125, prec 0.0181898, recall 0.645914
2017-12-10T14:16:35.115554: step 288, loss 2.61369, acc 0.546875, prec 0.0181322, recall 0.645914
2017-12-10T14:16:35.302261: step 289, loss 2.71311, acc 0.375, prec 0.0181601, recall 0.647287
2017-12-10T14:16:35.484924: step 290, loss 2.4693, acc 0.53125, prec 0.018101, recall 0.647287
2017-12-10T14:16:35.670405: step 291, loss 5.34887, acc 0.46875, prec 0.0181425, recall 0.646154
2017-12-10T14:16:35.857302: step 292, loss 2.69925, acc 0.453125, prec 0.0182855, recall 0.648855
2017-12-10T14:16:36.048779: step 293, loss 1.81403, acc 0.53125, prec 0.0183319, recall 0.65019
2017-12-10T14:16:36.235034: step 294, loss 2.0291, acc 0.5, prec 0.0182692, recall 0.65019
2017-12-10T14:16:36.418632: step 295, loss 2.06985, acc 0.515625, prec 0.0182089, recall 0.65019
2017-12-10T14:16:36.600920: step 296, loss 1.91385, acc 0.53125, prec 0.0182551, recall 0.651515
2017-12-10T14:16:36.784063: step 297, loss 1.30136, acc 0.734375, prec 0.0182223, recall 0.651515
2017-12-10T14:16:36.971377: step 298, loss 1.09018, acc 0.6875, prec 0.0182875, recall 0.65283
2017-12-10T14:16:37.159338: step 299, loss 0.873209, acc 0.765625, prec 0.0182586, recall 0.65283
2017-12-10T14:16:37.344672: step 300, loss 0.326157, acc 0.875, prec 0.0183467, recall 0.654135
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_2/1512933336/checkpoints/model-300

2017-12-10T14:16:38.757293: step 301, loss 4.43256, acc 0.671875, prec 0.0184114, recall 0.652985
2017-12-10T14:16:38.945308: step 302, loss 6.97001, acc 0.65625, prec 0.0183727, recall 0.648148
2017-12-10T14:16:39.134831: step 303, loss 8.41892, acc 0.796875, prec 0.0183496, recall 0.645756
2017-12-10T14:16:39.322301: step 304, loss 0.900651, acc 0.796875, prec 0.0183246, recall 0.645756
2017-12-10T14:16:39.508175: step 305, loss 5.52383, acc 0.6875, prec 0.0182882, recall 0.643382
2017-12-10T14:16:39.698691: step 306, loss 1.23154, acc 0.6875, prec 0.0183525, recall 0.644689
2017-12-10T14:16:39.889402: step 307, loss 1.32719, acc 0.671875, prec 0.0183123, recall 0.644689
2017-12-10T14:16:40.076023: step 308, loss 1.38617, acc 0.59375, prec 0.0182629, recall 0.644689
2017-12-10T14:16:40.262489: step 309, loss 1.49407, acc 0.640625, prec 0.0184227, recall 0.647273
2017-12-10T14:16:40.449096: step 310, loss 1.66587, acc 0.625, prec 0.018377, recall 0.647273
2017-12-10T14:16:40.636506: step 311, loss 11.47, acc 0.640625, prec 0.0184365, recall 0.646209
2017-12-10T14:16:40.825911: step 312, loss 5.71869, acc 0.484375, prec 0.0184767, recall 0.645161
2017-12-10T14:16:41.013237: step 313, loss 2.10315, acc 0.609375, prec 0.0185299, recall 0.646429
2017-12-10T14:16:41.197994: step 314, loss 2.42575, acc 0.59375, prec 0.0187813, recall 0.650177
2017-12-10T14:16:41.384994: step 315, loss 1.71615, acc 0.5625, prec 0.0187277, recall 0.650177
2017-12-10T14:16:41.570961: step 316, loss 2.16375, acc 0.5, prec 0.0186669, recall 0.650177
2017-12-10T14:16:41.755210: step 317, loss 1.91816, acc 0.5625, prec 0.0186141, recall 0.650177
2017-12-10T14:16:41.939251: step 318, loss 2.17053, acc 0.4375, prec 0.0185465, recall 0.650177
2017-12-10T14:16:42.122979: step 319, loss 2.18, acc 0.59375, prec 0.0186953, recall 0.652632
2017-12-10T14:16:42.308765: step 320, loss 2.01032, acc 0.53125, prec 0.0188358, recall 0.655052
2017-12-10T14:16:42.494765: step 321, loss 2.60141, acc 0.53125, prec 0.0188773, recall 0.65625
2017-12-10T14:16:42.680842: step 322, loss 2.5586, acc 0.453125, prec 0.0189092, recall 0.657439
2017-12-10T14:16:42.866780: step 323, loss 1.79392, acc 0.546875, prec 0.0190495, recall 0.659794
2017-12-10T14:16:43.054598: step 324, loss 1.44852, acc 0.578125, prec 0.0189986, recall 0.659794
2017-12-10T14:16:43.245462: step 325, loss 1.41527, acc 0.578125, prec 0.0190448, recall 0.660959
2017-12-10T14:16:43.432391: step 326, loss 5.63331, acc 0.671875, prec 0.0191039, recall 0.659864
2017-12-10T14:16:43.619897: step 327, loss 1.87455, acc 0.609375, prec 0.0191533, recall 0.661017
2017-12-10T14:16:43.814081: step 328, loss 8.46475, acc 0.71875, prec 0.0191233, recall 0.656566
2017-12-10T14:16:44.006005: step 329, loss 1.53873, acc 0.640625, prec 0.0192722, recall 0.658863
2017-12-10T14:16:44.191228: step 330, loss 6.06453, acc 0.65625, prec 0.0195198, recall 0.660066
2017-12-10T14:16:44.379130: step 331, loss 1.1541, acc 0.671875, prec 0.0194799, recall 0.660066
2017-12-10T14:16:44.564074: step 332, loss 2.02407, acc 0.53125, prec 0.0194231, recall 0.660066
2017-12-10T14:16:44.750464: step 333, loss 1.44672, acc 0.609375, prec 0.0194711, recall 0.661184
2017-12-10T14:16:44.934990: step 334, loss 2.11074, acc 0.59375, prec 0.0195169, recall 0.662295
2017-12-10T14:16:45.124502: step 335, loss 1.25106, acc 0.640625, prec 0.0194736, recall 0.662295
2017-12-10T14:16:45.312953: step 336, loss 1.53218, acc 0.609375, prec 0.0194268, recall 0.662295
2017-12-10T14:16:45.503756: step 337, loss 1.5989, acc 0.609375, prec 0.0193802, recall 0.662295
2017-12-10T14:16:45.690415: step 338, loss 1.96348, acc 0.5625, prec 0.0193283, recall 0.662295
2017-12-10T14:16:45.882771: step 339, loss 0.761403, acc 0.765625, prec 0.0193006, recall 0.662295
2017-12-10T14:16:46.070643: step 340, loss 0.918013, acc 0.71875, prec 0.0192675, recall 0.662295
2017-12-10T14:16:46.261404: step 341, loss 0.8343, acc 0.765625, prec 0.0192399, recall 0.662295
2017-12-10T14:16:46.447475: step 342, loss 1.18653, acc 0.8125, prec 0.0193113, recall 0.663399
2017-12-10T14:16:46.633256: step 343, loss 6.93959, acc 0.796875, prec 0.0192892, recall 0.661238
2017-12-10T14:16:46.818589: step 344, loss 1.12067, acc 0.78125, prec 0.0192636, recall 0.661238
2017-12-10T14:16:47.007839: step 345, loss 6.76856, acc 0.734375, prec 0.0193273, recall 0.660194
2017-12-10T14:16:47.199078: step 346, loss 7.13031, acc 0.75, prec 0.0193927, recall 0.659164
2017-12-10T14:16:47.383228: step 347, loss 2.43315, acc 0.75, prec 0.0194578, recall 0.658147
2017-12-10T14:16:47.569036: step 348, loss 3.84893, acc 0.65625, prec 0.0196041, recall 0.658228
2017-12-10T14:16:47.759041: step 349, loss 1.7219, acc 0.609375, prec 0.0197424, recall 0.660377
2017-12-10T14:16:47.949111: step 350, loss 1.66536, acc 0.546875, prec 0.0197806, recall 0.661442
2017-12-10T14:16:48.139599: step 351, loss 2.44128, acc 0.46875, prec 0.0197178, recall 0.661442
2017-12-10T14:16:48.326775: step 352, loss 2.18206, acc 0.578125, prec 0.0196682, recall 0.661442
2017-12-10T14:16:48.518912: step 353, loss 3.13318, acc 0.4375, prec 0.0196024, recall 0.661442
2017-12-10T14:16:48.708922: step 354, loss 2.29723, acc 0.5625, prec 0.0197332, recall 0.663551
2017-12-10T14:16:48.898396: step 355, loss 2.876, acc 0.453125, prec 0.0196694, recall 0.663551
2017-12-10T14:16:49.083902: step 356, loss 2.89059, acc 0.53125, prec 0.0196151, recall 0.663551
2017-12-10T14:16:49.273401: step 357, loss 1.82166, acc 0.59375, prec 0.0196583, recall 0.664596
2017-12-10T14:16:49.459412: step 358, loss 2.30269, acc 0.59375, prec 0.0196114, recall 0.664596
2017-12-10T14:16:49.643855: step 359, loss 2.42485, acc 0.375, prec 0.0197188, recall 0.666667
2017-12-10T14:16:49.827835: step 360, loss 1.98276, acc 0.5, prec 0.0196614, recall 0.666667
2017-12-10T14:16:50.014334: step 361, loss 2.84048, acc 0.453125, prec 0.0195989, recall 0.666667
2017-12-10T14:16:50.201048: step 362, loss 1.88752, acc 0.5625, prec 0.019638, recall 0.667692
2017-12-10T14:16:50.391391: step 363, loss 4.42014, acc 0.609375, prec 0.0195954, recall 0.665644
2017-12-10T14:16:50.578634: step 364, loss 1.59187, acc 0.625, prec 0.0195531, recall 0.665644
2017-12-10T14:16:50.764463: step 365, loss 1.73207, acc 0.59375, prec 0.0195955, recall 0.666667
2017-12-10T14:16:50.953210: step 366, loss 1.55392, acc 0.609375, prec 0.0195516, recall 0.666667
2017-12-10T14:16:51.138277: step 367, loss 1.29362, acc 0.734375, prec 0.0196096, recall 0.667683
2017-12-10T14:16:51.323294: step 368, loss 1.23796, acc 0.6875, prec 0.0195745, recall 0.667683
2017-12-10T14:16:51.509449: step 369, loss 0.918598, acc 0.734375, prec 0.0195448, recall 0.667683
2017-12-10T14:16:51.693113: step 370, loss 7.96966, acc 0.828125, prec 0.0196148, recall 0.666667
2017-12-10T14:16:51.882784: step 371, loss 0.514616, acc 0.875, prec 0.0196009, recall 0.666667
2017-12-10T14:16:52.072492: step 372, loss 2.45096, acc 0.6875, prec 0.0195677, recall 0.664653
2017-12-10T14:16:52.259620: step 373, loss 0.9165, acc 0.890625, prec 0.0197298, recall 0.666667
2017-12-10T14:16:52.445512: step 374, loss 0.928382, acc 0.84375, prec 0.0197993, recall 0.667665
2017-12-10T14:16:52.630978: step 375, loss 1.17238, acc 0.78125, prec 0.0198617, recall 0.668657
2017-12-10T14:16:52.822339: step 376, loss 0.908372, acc 0.734375, prec 0.0198318, recall 0.668657
2017-12-10T14:16:53.022512: step 377, loss 1.41784, acc 0.703125, prec 0.0198851, recall 0.669643
2017-12-10T14:16:53.210414: step 378, loss 16.5032, acc 0.84375, prec 0.0198711, recall 0.66568
2017-12-10T14:16:53.402211: step 379, loss 0.474283, acc 0.84375, prec 0.01994, recall 0.666667
2017-12-10T14:16:53.591089: step 380, loss 8.12022, acc 0.6875, prec 0.0199084, recall 0.662757
2017-12-10T14:16:53.782267: step 381, loss 1.02188, acc 0.71875, prec 0.0198769, recall 0.662757
2017-12-10T14:16:53.968156: step 382, loss 1.59147, acc 0.5625, prec 0.019914, recall 0.663743
2017-12-10T14:16:54.152433: step 383, loss 1.21171, acc 0.65625, prec 0.0199615, recall 0.664723
2017-12-10T14:16:54.338961: step 384, loss 1.68548, acc 0.59375, prec 0.0200017, recall 0.665698
2017-12-10T14:16:54.523825: step 385, loss 1.98722, acc 0.578125, prec 0.0199547, recall 0.665698
2017-12-10T14:16:54.719148: step 386, loss 2.87141, acc 0.546875, prec 0.0200747, recall 0.66763
2017-12-10T14:16:54.904732: step 387, loss 2.54141, acc 0.5, prec 0.0200191, recall 0.66763
2017-12-10T14:16:55.092226: step 388, loss 7.28086, acc 0.53125, prec 0.0200536, recall 0.666667
2017-12-10T14:16:55.278816: step 389, loss 2.73708, acc 0.46875, prec 0.0200793, recall 0.667622
2017-12-10T14:16:55.465882: step 390, loss 2.14166, acc 0.5, prec 0.0201925, recall 0.669516
2017-12-10T14:16:55.656944: step 391, loss 1.80198, acc 0.6875, prec 0.0204099, recall 0.672316
2017-12-10T14:16:55.848603: step 392, loss 5.09155, acc 0.5, prec 0.0203558, recall 0.670423
2017-12-10T14:16:56.040406: step 393, loss 3.54192, acc 0.34375, prec 0.0204499, recall 0.672269
2017-12-10T14:16:56.226010: step 394, loss 3.11281, acc 0.40625, prec 0.0204671, recall 0.673184
2017-12-10T14:16:56.411026: step 395, loss 2.56569, acc 0.5, prec 0.0204946, recall 0.674095
2017-12-10T14:16:56.597747: step 396, loss 3.61554, acc 0.4375, prec 0.020515, recall 0.675
2017-12-10T14:16:56.785617: step 397, loss 2.33192, acc 0.5, prec 0.0206246, recall 0.676796
2017-12-10T14:16:56.971732: step 398, loss 2.08385, acc 0.546875, prec 0.0207389, recall 0.678571
2017-12-10T14:16:57.155961: step 399, loss 2.04975, acc 0.484375, prec 0.0208455, recall 0.680328
2017-12-10T14:16:57.341416: step 400, loss 2.3157, acc 0.59375, prec 0.0208003, recall 0.680328
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_2/1512933336/checkpoints/model-400

2017-12-10T14:16:58.981641: step 401, loss 2.7984, acc 0.53125, prec 0.0209114, recall 0.682065
2017-12-10T14:16:59.167922: step 402, loss 5.28958, acc 0.609375, prec 0.0208697, recall 0.680217
2017-12-10T14:16:59.357032: step 403, loss 2.08024, acc 0.578125, prec 0.020823, recall 0.680217
2017-12-10T14:16:59.541025: step 404, loss 2.05575, acc 0.515625, prec 0.0207695, recall 0.680217
2017-12-10T14:16:59.725312: step 405, loss 1.72266, acc 0.6875, prec 0.0208161, recall 0.681081
2017-12-10T14:16:59.912795: step 406, loss 1.28416, acc 0.625, prec 0.0207749, recall 0.681081
2017-12-10T14:17:00.098674: step 407, loss 2.51974, acc 0.6875, prec 0.020823, recall 0.680108
2017-12-10T14:17:00.287557: step 408, loss 7.12665, acc 0.65625, prec 0.0210284, recall 0.680851
2017-12-10T14:17:00.472587: step 409, loss 2.01131, acc 0.546875, prec 0.0209784, recall 0.680851
2017-12-10T14:17:00.664221: step 410, loss 1.65094, acc 0.59375, prec 0.0209338, recall 0.680851
2017-12-10T14:17:00.850387: step 411, loss 7.11351, acc 0.609375, prec 0.0208928, recall 0.679045
2017-12-10T14:17:01.042017: step 412, loss 5.12856, acc 0.625, prec 0.0208537, recall 0.677249
2017-12-10T14:17:01.232669: step 413, loss 2.78233, acc 0.703125, prec 0.0210603, recall 0.67979
2017-12-10T14:17:01.428976: step 414, loss 1.35821, acc 0.640625, prec 0.021021, recall 0.67979
2017-12-10T14:17:01.616708: step 415, loss 3.92498, acc 0.59375, prec 0.0209785, recall 0.67801
2017-12-10T14:17:01.809717: step 416, loss 5.8311, acc 0.546875, prec 0.020931, recall 0.67624
2017-12-10T14:17:01.998329: step 417, loss 1.80798, acc 0.578125, prec 0.0209644, recall 0.677083
2017-12-10T14:17:02.184106: step 418, loss 3.55006, acc 0.53125, prec 0.0209942, recall 0.676166
2017-12-10T14:17:02.370976: step 419, loss 2.2729, acc 0.53125, prec 0.0211008, recall 0.677835
2017-12-10T14:17:02.558305: step 420, loss 2.98776, acc 0.46875, prec 0.0210434, recall 0.677835
2017-12-10T14:17:02.745891: step 421, loss 4.21119, acc 0.421875, prec 0.0209829, recall 0.676093
2017-12-10T14:17:02.930216: step 422, loss 1.67034, acc 0.59375, prec 0.0210174, recall 0.676923
2017-12-10T14:17:03.116281: step 423, loss 2.23264, acc 0.53125, prec 0.021356, recall 0.681013
2017-12-10T14:17:03.303252: step 424, loss 2.26341, acc 0.5, prec 0.0213794, recall 0.681818
2017-12-10T14:17:03.487772: step 425, loss 2.66276, acc 0.4375, prec 0.0213959, recall 0.68262
2017-12-10T14:17:03.674559: step 426, loss 3.59501, acc 0.390625, prec 0.0213302, recall 0.68262
2017-12-10T14:17:03.856519: step 427, loss 2.35254, acc 0.515625, prec 0.0213551, recall 0.683417
2017-12-10T14:17:04.049221: step 428, loss 2.35305, acc 0.546875, prec 0.0214599, recall 0.685
2017-12-10T14:17:04.238203: step 429, loss 1.83236, acc 0.5, prec 0.0214063, recall 0.685
2017-12-10T14:17:04.424901: step 430, loss 2.10243, acc 0.53125, prec 0.0214325, recall 0.685786
2017-12-10T14:17:04.607585: step 431, loss 1.90513, acc 0.53125, prec 0.0214586, recall 0.686567
2017-12-10T14:17:04.792323: step 432, loss 2.16417, acc 0.65625, prec 0.0215738, recall 0.688119
2017-12-10T14:17:04.982128: step 433, loss 1.04538, acc 0.75, prec 0.021547, recall 0.688119
2017-12-10T14:17:05.172867: step 434, loss 1.8064, acc 0.65625, prec 0.0216618, recall 0.689655
2017-12-10T14:17:05.361662: step 435, loss 1.60848, acc 0.640625, prec 0.0216233, recall 0.689655
2017-12-10T14:17:05.551304: step 436, loss 2.76279, acc 0.640625, prec 0.021662, recall 0.688725
2017-12-10T14:17:05.738633: step 437, loss 1.30182, acc 0.734375, prec 0.0216337, recall 0.688725
2017-12-10T14:17:05.927360: step 438, loss 0.725945, acc 0.765625, prec 0.0217592, recall 0.690244
2017-12-10T14:17:06.115676: step 439, loss 1.18343, acc 0.625, prec 0.0217191, recall 0.690244
2017-12-10T14:17:06.306098: step 440, loss 10.1211, acc 0.71875, prec 0.0216908, recall 0.688564
2017-12-10T14:17:06.498296: step 441, loss 1.52147, acc 0.5625, prec 0.0216444, recall 0.688564
2017-12-10T14:17:06.683469: step 442, loss 16.8324, acc 0.765625, prec 0.021696, recall 0.687651
2017-12-10T14:17:06.875063: step 443, loss 2.32415, acc 0.71875, prec 0.0216678, recall 0.68599
2017-12-10T14:17:07.065640: step 444, loss 2.22424, acc 0.734375, prec 0.0216414, recall 0.684337
2017-12-10T14:17:07.254980: step 445, loss 1.19723, acc 0.6875, prec 0.0217573, recall 0.685851
2017-12-10T14:17:07.441761: step 446, loss 1.13287, acc 0.640625, prec 0.0217193, recall 0.685851
2017-12-10T14:17:07.623754: step 447, loss 1.66759, acc 0.625, prec 0.0219022, recall 0.688095
2017-12-10T14:17:07.810675: step 448, loss 2.26312, acc 0.484375, prec 0.0218476, recall 0.688095
2017-12-10T14:17:07.993032: step 449, loss 2.27307, acc 0.59375, prec 0.0218047, recall 0.688095
2017-12-10T14:17:08.179364: step 450, loss 1.42208, acc 0.59375, prec 0.0218357, recall 0.688836
2017-12-10T14:17:08.363638: step 451, loss 1.64537, acc 0.578125, prec 0.0218649, recall 0.689573
2017-12-10T14:17:08.552591: step 452, loss 5.65337, acc 0.625, prec 0.0219005, recall 0.688679
2017-12-10T14:17:08.740551: step 453, loss 6.72175, acc 0.609375, prec 0.0218628, recall 0.685446
2017-12-10T14:17:08.928668: step 454, loss 2.67545, acc 0.515625, prec 0.0218853, recall 0.686183
2017-12-10T14:17:09.118339: step 455, loss 1.89566, acc 0.625, prec 0.0219919, recall 0.687646
2017-12-10T14:17:09.303444: step 456, loss 1.64416, acc 0.671875, prec 0.0219576, recall 0.687646
2017-12-10T14:17:09.496081: step 457, loss 4.81468, acc 0.453125, prec 0.0219748, recall 0.686775
2017-12-10T14:17:09.681496: step 458, loss 1.99448, acc 0.578125, prec 0.0219308, recall 0.686775
2017-12-10T14:17:09.868298: step 459, loss 7.59848, acc 0.65625, prec 0.0220414, recall 0.686636
2017-12-10T14:17:10.054688: step 460, loss 2.40643, acc 0.5, prec 0.0220615, recall 0.687356
2017-12-10T14:17:10.237495: step 461, loss 2.91887, acc 0.46875, prec 0.0220783, recall 0.688073
2017-12-10T14:17:10.430389: step 462, loss 3.53379, acc 0.375, prec 0.0220853, recall 0.688787
2017-12-10T14:17:10.617432: step 463, loss 3.02172, acc 0.453125, prec 0.0220287, recall 0.688787
2017-12-10T14:17:10.804827: step 464, loss 2.24342, acc 0.4375, prec 0.0220422, recall 0.689498
2017-12-10T14:17:10.990000: step 465, loss 2.67368, acc 0.484375, prec 0.0220604, recall 0.690205
2017-12-10T14:17:11.176274: step 466, loss 2.51941, acc 0.515625, prec 0.0220818, recall 0.690909
2017-12-10T14:17:11.368067: step 467, loss 3.07523, acc 0.515625, prec 0.0220322, recall 0.690909
2017-12-10T14:17:11.553490: step 468, loss 1.89774, acc 0.5625, prec 0.0219876, recall 0.690909
2017-12-10T14:17:11.736422: step 469, loss 2.54471, acc 0.5625, prec 0.0220137, recall 0.69161
2017-12-10T14:17:11.925691: step 470, loss 2.28793, acc 0.5625, prec 0.0221806, recall 0.693694
2017-12-10T14:17:12.116718: step 471, loss 1.36983, acc 0.71875, prec 0.0222925, recall 0.695067
2017-12-10T14:17:12.307638: step 472, loss 0.913512, acc 0.71875, prec 0.0222637, recall 0.695067
2017-12-10T14:17:12.498497: step 473, loss 3.28378, acc 0.546875, prec 0.022219, recall 0.693512
2017-12-10T14:17:12.691489: step 474, loss 6.77109, acc 0.765625, prec 0.0222668, recall 0.69265
2017-12-10T14:17:12.878386: step 475, loss 1.2189, acc 0.671875, prec 0.0222333, recall 0.69265
2017-12-10T14:17:13.070991: step 476, loss 1.1915, acc 0.75, prec 0.0222778, recall 0.693333
2017-12-10T14:17:13.258504: step 477, loss 1.06919, acc 0.796875, prec 0.0223268, recall 0.694013
2017-12-10T14:17:13.443600: step 478, loss 7.66471, acc 0.609375, prec 0.0224279, recall 0.693833
2017-12-10T14:17:13.633173: step 479, loss 1.12324, acc 0.65625, prec 0.0223928, recall 0.693833
2017-12-10T14:17:13.822374: step 480, loss 1.09274, acc 0.71875, prec 0.0223642, recall 0.693833
2017-12-10T14:17:14.016820: step 481, loss 1.14482, acc 0.6875, prec 0.0223325, recall 0.693833
2017-12-10T14:17:14.204012: step 482, loss 0.840667, acc 0.75, prec 0.0223072, recall 0.693833
2017-12-10T14:17:14.392950: step 483, loss 1.48248, acc 0.640625, prec 0.0222709, recall 0.693833
2017-12-10T14:17:14.585788: step 484, loss 5.31254, acc 0.640625, prec 0.0223744, recall 0.693654
2017-12-10T14:17:14.776481: step 485, loss 0.946701, acc 0.84375, prec 0.0224965, recall 0.694989
2017-12-10T14:17:14.967218: step 486, loss 1.17518, acc 0.703125, prec 0.022604, recall 0.696312
2017-12-10T14:17:15.149718: step 487, loss 13.3369, acc 0.609375, prec 0.0226346, recall 0.695464
2017-12-10T14:17:15.339704: step 488, loss 0.978272, acc 0.6875, prec 0.02274, recall 0.696774
2017-12-10T14:17:15.524055: step 489, loss 1.75176, acc 0.546875, prec 0.0226938, recall 0.696774
2017-12-10T14:17:15.711963: step 490, loss 1.54478, acc 0.578125, prec 0.0227193, recall 0.697425
2017-12-10T14:17:15.899429: step 491, loss 1.73082, acc 0.578125, prec 0.0226765, recall 0.697425
2017-12-10T14:17:16.085605: step 492, loss 1.21675, acc 0.640625, prec 0.0227083, recall 0.698073
2017-12-10T14:17:16.272735: step 493, loss 1.27104, acc 0.609375, prec 0.0226688, recall 0.698073
2017-12-10T14:17:16.460831: step 494, loss 1.23634, acc 0.671875, prec 0.0226357, recall 0.698073
2017-12-10T14:17:16.645807: step 495, loss 4.04038, acc 0.765625, prec 0.0226816, recall 0.697228
2017-12-10T14:17:16.832705: step 496, loss 1.65674, acc 0.59375, prec 0.0227084, recall 0.697872
2017-12-10T14:17:17.001939: step 497, loss 1.27775, acc 0.730769, prec 0.0226864, recall 0.697872
2017-12-10T14:17:17.193670: step 498, loss 1.41535, acc 0.671875, prec 0.0226535, recall 0.697872
2017-12-10T14:17:17.385875: step 499, loss 1.10318, acc 0.703125, prec 0.0226238, recall 0.697872
2017-12-10T14:17:17.572456: step 500, loss 0.982553, acc 0.796875, prec 0.0226709, recall 0.698514
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_2/1512933336/checkpoints/model-500

2017-12-10T14:17:19.144187: step 501, loss 0.689887, acc 0.75, prec 0.0226459, recall 0.698514
2017-12-10T14:17:19.333025: step 502, loss 0.833966, acc 0.71875, prec 0.0226179, recall 0.698514
2017-12-10T14:17:19.519152: step 503, loss 1.10896, acc 0.71875, prec 0.0226571, recall 0.699153
2017-12-10T14:17:19.708599: step 504, loss 0.712236, acc 0.765625, prec 0.0226337, recall 0.699153
2017-12-10T14:17:19.898606: step 505, loss 1.14012, acc 0.75, prec 0.0226089, recall 0.699153
2017-12-10T14:17:20.092002: step 506, loss 0.653409, acc 0.828125, prec 0.0227257, recall 0.700422
2017-12-10T14:17:20.282660: step 507, loss 0.426805, acc 0.859375, prec 0.0227117, recall 0.700422
2017-12-10T14:17:20.469564: step 508, loss 0.641634, acc 0.796875, prec 0.0228251, recall 0.701681
2017-12-10T14:17:20.658040: step 509, loss 0.498739, acc 0.875, prec 0.0228126, recall 0.701681
2017-12-10T14:17:20.844721: step 510, loss 0.461996, acc 0.875, prec 0.0228002, recall 0.701681
2017-12-10T14:17:21.036428: step 511, loss 0.600439, acc 0.875, prec 0.0228544, recall 0.702306
2017-12-10T14:17:21.227295: step 512, loss 0.513118, acc 0.84375, prec 0.0228388, recall 0.702306
2017-12-10T14:17:21.416223: step 513, loss 14.9906, acc 0.875, prec 0.0228279, recall 0.700837
2017-12-10T14:17:21.604750: step 514, loss 0.370541, acc 0.90625, prec 0.0228186, recall 0.700837
2017-12-10T14:17:21.792275: step 515, loss 0.56465, acc 0.796875, prec 0.0227984, recall 0.700837
2017-12-10T14:17:21.981939: step 516, loss 0.265077, acc 0.90625, prec 0.0227891, recall 0.700837
2017-12-10T14:17:22.167101: step 517, loss 0.64077, acc 0.921875, prec 0.0228478, recall 0.701461
2017-12-10T14:17:22.359257: step 518, loss 0.305556, acc 0.90625, prec 0.0228385, recall 0.701461
2017-12-10T14:17:22.548307: step 519, loss 14.5549, acc 0.921875, prec 0.0228987, recall 0.700624
2017-12-10T14:17:22.738320: step 520, loss 1.34182, acc 0.8125, prec 0.0229464, recall 0.701245
2017-12-10T14:17:22.930024: step 521, loss 1.48035, acc 0.890625, prec 0.0230018, recall 0.701863
2017-12-10T14:17:23.123483: step 522, loss 5.23127, acc 0.765625, prec 0.0230462, recall 0.701031
2017-12-10T14:17:23.314715: step 523, loss 0.622961, acc 0.796875, prec 0.023092, recall 0.701646
2017-12-10T14:17:23.504343: step 524, loss 0.441461, acc 0.84375, prec 0.0231425, recall 0.702259
2017-12-10T14:17:23.696359: step 525, loss 1.83015, acc 0.78125, prec 0.0232527, recall 0.703476
2017-12-10T14:17:23.891780: step 526, loss 3.22946, acc 0.640625, prec 0.0232841, recall 0.702648
2017-12-10T14:17:24.076215: step 527, loss 1.50587, acc 0.65625, prec 0.0232495, recall 0.702648
2017-12-10T14:17:24.265915: step 528, loss 1.32233, acc 0.671875, prec 0.0232824, recall 0.703252
2017-12-10T14:17:24.457653: step 529, loss 1.38102, acc 0.6875, prec 0.0233824, recall 0.704453
2017-12-10T14:17:24.647704: step 530, loss 1.87205, acc 0.625, prec 0.0233447, recall 0.704453
2017-12-10T14:17:24.833588: step 531, loss 2.23405, acc 0.5625, prec 0.0234317, recall 0.705645
2017-12-10T14:17:25.022043: step 532, loss 1.77606, acc 0.59375, prec 0.023391, recall 0.705645
2017-12-10T14:17:25.211540: step 533, loss 2.57585, acc 0.578125, prec 0.0234792, recall 0.706827
2017-12-10T14:17:25.402661: step 534, loss 2.34318, acc 0.59375, prec 0.0235036, recall 0.707415
2017-12-10T14:17:25.585948: step 535, loss 1.9449, acc 0.625, prec 0.0234661, recall 0.707415
2017-12-10T14:17:25.772497: step 536, loss 1.51858, acc 0.546875, prec 0.0234857, recall 0.708
2017-12-10T14:17:25.960444: step 537, loss 1.5139, acc 0.65625, prec 0.0234515, recall 0.708
2017-12-10T14:17:26.150726: step 538, loss 1.26793, acc 0.65625, prec 0.0234819, recall 0.708583
2017-12-10T14:17:26.336067: step 539, loss 1.11715, acc 0.65625, prec 0.0235123, recall 0.709163
2017-12-10T14:17:26.522670: step 540, loss 1.40209, acc 0.671875, prec 0.0235442, recall 0.709742
2017-12-10T14:17:26.706294: step 541, loss 1.16558, acc 0.671875, prec 0.0235759, recall 0.710317
2017-12-10T14:17:26.889344: step 542, loss 7.53591, acc 0.71875, prec 0.0235495, recall 0.708911
2017-12-10T14:17:27.076904: step 543, loss 1.00419, acc 0.765625, prec 0.0237829, recall 0.711198
2017-12-10T14:17:27.269695: step 544, loss 2.6943, acc 0.71875, prec 0.0237564, recall 0.709804
2017-12-10T14:17:27.460086: step 545, loss 3.01734, acc 0.65625, prec 0.0237237, recall 0.708415
2017-12-10T14:17:27.648543: step 546, loss 2.5309, acc 0.65625, prec 0.0237534, recall 0.708984
2017-12-10T14:17:27.837802: step 547, loss 0.597615, acc 0.796875, prec 0.0237332, recall 0.708984
2017-12-10T14:17:28.028125: step 548, loss 1.07619, acc 0.71875, prec 0.0237691, recall 0.709552
2017-12-10T14:17:28.213043: step 549, loss 1.02899, acc 0.703125, prec 0.0237396, recall 0.709552
2017-12-10T14:17:28.399782: step 550, loss 1.44353, acc 0.671875, prec 0.0237708, recall 0.710117
2017-12-10T14:17:28.591333: step 551, loss 1.46081, acc 0.640625, prec 0.0237352, recall 0.710117
2017-12-10T14:17:28.782002: step 552, loss 0.635058, acc 0.828125, prec 0.0237182, recall 0.710117
2017-12-10T14:17:28.967594: step 553, loss 0.924535, acc 0.78125, prec 0.0237601, recall 0.71068
2017-12-10T14:17:29.158132: step 554, loss 0.900527, acc 0.75, prec 0.0237987, recall 0.71124
2017-12-10T14:17:29.348340: step 555, loss 1.28818, acc 0.671875, prec 0.0238296, recall 0.711799
2017-12-10T14:17:29.533048: step 556, loss 0.940545, acc 0.75, prec 0.0238049, recall 0.711799
2017-12-10T14:17:29.718059: step 557, loss 1.41302, acc 0.828125, prec 0.0239142, recall 0.712909
2017-12-10T14:17:29.907886: step 558, loss 7.56132, acc 0.75, prec 0.023954, recall 0.712092
2017-12-10T14:17:30.101195: step 559, loss 6.58158, acc 0.703125, prec 0.0239262, recall 0.710728
2017-12-10T14:17:30.292029: step 560, loss 0.899535, acc 0.75, prec 0.0239016, recall 0.710728
2017-12-10T14:17:30.479891: step 561, loss 0.839403, acc 0.796875, prec 0.0238816, recall 0.710728
2017-12-10T14:17:30.668308: step 562, loss 0.978472, acc 0.71875, prec 0.0238539, recall 0.710728
2017-12-10T14:17:30.857837: step 563, loss 1.49263, acc 0.671875, prec 0.0238844, recall 0.711281
2017-12-10T14:17:31.054716: step 564, loss 0.633019, acc 0.796875, prec 0.0238645, recall 0.711281
2017-12-10T14:17:31.250794: step 565, loss 0.554114, acc 0.8125, prec 0.0239713, recall 0.712381
2017-12-10T14:17:31.441007: step 566, loss 1.0621, acc 0.765625, prec 0.0240108, recall 0.712928
2017-12-10T14:17:31.628875: step 567, loss 0.904479, acc 0.703125, prec 0.0241064, recall 0.714015
2017-12-10T14:17:31.816642: step 568, loss 1.09709, acc 0.6875, prec 0.0240756, recall 0.714015
2017-12-10T14:17:32.002530: step 569, loss 1.24391, acc 0.71875, prec 0.0241102, recall 0.714556
2017-12-10T14:17:32.188232: step 570, loss 0.903757, acc 0.734375, prec 0.0240841, recall 0.714556
2017-12-10T14:17:32.377877: step 571, loss 0.888923, acc 0.765625, prec 0.0241232, recall 0.715094
2017-12-10T14:17:32.567469: step 572, loss 0.584691, acc 0.78125, prec 0.0242259, recall 0.716165
2017-12-10T14:17:32.754387: step 573, loss 0.689138, acc 0.78125, prec 0.0242043, recall 0.716165
2017-12-10T14:17:32.939796: step 574, loss 0.93007, acc 0.75, prec 0.0241797, recall 0.716165
2017-12-10T14:17:33.124360: step 575, loss 0.590641, acc 0.84375, prec 0.0241644, recall 0.716165
2017-12-10T14:17:33.314629: step 576, loss 0.552523, acc 0.859375, prec 0.0242125, recall 0.716698
2017-12-10T14:17:33.506464: step 577, loss 1.06165, acc 0.734375, prec 0.02431, recall 0.717757
2017-12-10T14:17:33.697386: step 578, loss 0.502894, acc 0.890625, prec 0.0243609, recall 0.718284
2017-12-10T14:17:33.887008: step 579, loss 0.249692, acc 0.875, prec 0.0243486, recall 0.718284
2017-12-10T14:17:34.076975: step 580, loss 0.141631, acc 0.90625, prec 0.0243394, recall 0.718284
2017-12-10T14:17:34.268316: step 581, loss 0.852152, acc 0.9375, prec 0.0243949, recall 0.718808
2017-12-10T14:17:34.457078: step 582, loss 12.2064, acc 0.84375, prec 0.0244457, recall 0.715342
2017-12-10T14:17:34.648257: step 583, loss 1.5015, acc 0.890625, prec 0.0245581, recall 0.71639
2017-12-10T14:17:34.838729: step 584, loss 0.731155, acc 0.78125, prec 0.0245364, recall 0.71639
2017-12-10T14:17:35.028343: step 585, loss 0.556896, acc 0.8125, prec 0.0245178, recall 0.71639
2017-12-10T14:17:35.218845: step 586, loss 0.7645, acc 0.78125, prec 0.0244962, recall 0.71639
2017-12-10T14:17:35.403013: step 587, loss 1.18988, acc 0.625, prec 0.0245206, recall 0.716912
2017-12-10T14:17:35.589936: step 588, loss 0.891662, acc 0.75, prec 0.0245572, recall 0.717431
2017-12-10T14:17:35.775751: step 589, loss 1.38999, acc 0.671875, prec 0.0245249, recall 0.717431
2017-12-10T14:17:35.961268: step 590, loss 1.34729, acc 0.609375, prec 0.0244865, recall 0.717431
2017-12-10T14:17:36.153912: step 591, loss 0.79909, acc 0.765625, prec 0.0245855, recall 0.718464
2017-12-10T14:17:36.340978: step 592, loss 1.37176, acc 0.578125, prec 0.0245441, recall 0.718464
2017-12-10T14:17:36.525631: step 593, loss 2.99346, acc 0.734375, prec 0.0246413, recall 0.718182
2017-12-10T14:17:36.713056: step 594, loss 0.867939, acc 0.78125, prec 0.0246198, recall 0.718182
2017-12-10T14:17:36.899987: step 595, loss 1.11779, acc 0.71875, prec 0.0245922, recall 0.718182
2017-12-10T14:17:37.086703: step 596, loss 1.51208, acc 0.65625, prec 0.0245586, recall 0.718182
2017-12-10T14:17:37.274979: step 597, loss 4.61684, acc 0.59375, prec 0.0245205, recall 0.716878
2017-12-10T14:17:37.465737: step 598, loss 1.64782, acc 0.71875, prec 0.0245536, recall 0.717391
2017-12-10T14:17:37.650017: step 599, loss 1.56905, acc 0.640625, prec 0.0245186, recall 0.717391
2017-12-10T14:17:37.839781: step 600, loss 0.639806, acc 0.796875, prec 0.0245592, recall 0.717902
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_2/1512933336/checkpoints/model-600

2017-12-10T14:17:38.982315: step 601, loss 6.85804, acc 0.640625, prec 0.0246464, recall 0.717626
2017-12-10T14:17:39.170620: step 602, loss 1.06404, acc 0.71875, prec 0.024619, recall 0.717626
2017-12-10T14:17:39.360248: step 603, loss 1.15631, acc 0.765625, prec 0.0245962, recall 0.717626
2017-12-10T14:17:39.549598: step 604, loss 1.24245, acc 0.6875, prec 0.0245659, recall 0.717626
2017-12-10T14:17:39.737097: step 605, loss 1.71512, acc 0.6875, prec 0.0245357, recall 0.717626
2017-12-10T14:17:39.922094: step 606, loss 5.52504, acc 0.75, prec 0.0245131, recall 0.716338
2017-12-10T14:17:40.114889: step 607, loss 1.62444, acc 0.75, prec 0.0246088, recall 0.717352
2017-12-10T14:17:40.301326: step 608, loss 1.53987, acc 0.5625, prec 0.0246263, recall 0.717857
2017-12-10T14:17:40.489303: step 609, loss 1.84852, acc 0.625, prec 0.0246498, recall 0.71836
2017-12-10T14:17:40.675888: step 610, loss 2.17886, acc 0.53125, prec 0.0246047, recall 0.71836
2017-12-10T14:17:40.862060: step 611, loss 1.51404, acc 0.625, prec 0.0246876, recall 0.719361
2017-12-10T14:17:41.052464: step 612, loss 1.3931, acc 0.703125, prec 0.0247184, recall 0.719858
2017-12-10T14:17:41.243936: step 613, loss 1.19249, acc 0.75, prec 0.024813, recall 0.720848
2017-12-10T14:17:41.430088: step 614, loss 1.48949, acc 0.625, prec 0.024836, recall 0.72134
2017-12-10T14:17:41.613008: step 615, loss 2.46865, acc 0.671875, prec 0.0248059, recall 0.72007
2017-12-10T14:17:41.808659: step 616, loss 1.25253, acc 0.6875, prec 0.0247759, recall 0.72007
2017-12-10T14:17:41.994201: step 617, loss 1.45111, acc 0.640625, prec 0.0247414, recall 0.72007
2017-12-10T14:17:42.183456: step 618, loss 4.1944, acc 0.640625, prec 0.0247085, recall 0.718805
2017-12-10T14:17:42.368659: step 619, loss 1.02178, acc 0.6875, prec 0.0247375, recall 0.719298
2017-12-10T14:17:42.559694: step 620, loss 0.880893, acc 0.671875, prec 0.0247062, recall 0.719298
2017-12-10T14:17:42.750546: step 621, loss 1.4266, acc 0.65625, prec 0.0246735, recall 0.719298
2017-12-10T14:17:42.935303: step 622, loss 1.43324, acc 0.6875, prec 0.0246439, recall 0.719298
2017-12-10T14:17:43.121676: step 623, loss 0.822807, acc 0.734375, prec 0.0246187, recall 0.719298
2017-12-10T14:17:43.310579: step 624, loss 6.9232, acc 0.859375, prec 0.0246669, recall 0.717277
2017-12-10T14:17:43.501751: step 625, loss 2.70506, acc 0.703125, prec 0.0247557, recall 0.718261
2017-12-10T14:17:43.698232: step 626, loss 1.3282, acc 0.640625, prec 0.02478, recall 0.71875
2017-12-10T14:17:43.892927: step 627, loss 0.906013, acc 0.828125, prec 0.0247637, recall 0.71875
2017-12-10T14:17:44.083088: step 628, loss 0.975863, acc 0.71875, prec 0.0247954, recall 0.719237
2017-12-10T14:17:44.272558: step 629, loss 1.27263, acc 0.71875, prec 0.0247687, recall 0.719237
2017-12-10T14:17:44.460625: step 630, loss 1.76643, acc 0.6875, prec 0.0247392, recall 0.719237
2017-12-10T14:17:44.645872: step 631, loss 1.73699, acc 0.640625, prec 0.0248214, recall 0.720207
2017-12-10T14:17:44.834902: step 632, loss 1.20946, acc 0.71875, prec 0.0249688, recall 0.721649
2017-12-10T14:17:45.024796: step 633, loss 0.951409, acc 0.703125, prec 0.0249406, recall 0.721649
2017-12-10T14:17:45.218822: step 634, loss 1.81936, acc 0.765625, prec 0.0252076, recall 0.72402
2017-12-10T14:17:45.416591: step 635, loss 1.47454, acc 0.65625, prec 0.0253479, recall 0.725424
2017-12-10T14:17:45.603760: step 636, loss 1.45357, acc 0.703125, prec 0.0253771, recall 0.725888
2017-12-10T14:17:45.789352: step 637, loss 0.774989, acc 0.8125, prec 0.0253591, recall 0.725888
2017-12-10T14:17:45.973758: step 638, loss 0.612607, acc 0.796875, prec 0.0253972, recall 0.726351
2017-12-10T14:17:46.164710: step 639, loss 0.493519, acc 0.859375, prec 0.0254412, recall 0.726813
2017-12-10T14:17:46.353673: step 640, loss 0.515198, acc 0.859375, prec 0.0254852, recall 0.727273
2017-12-10T14:17:46.541405: step 641, loss 0.897391, acc 0.703125, prec 0.0254567, recall 0.727273
2017-12-10T14:17:46.732304: step 642, loss 2.08403, acc 0.859375, prec 0.0256169, recall 0.727425
2017-12-10T14:17:46.926291: step 643, loss 1.01944, acc 0.765625, prec 0.0255943, recall 0.727425
2017-12-10T14:17:47.115449: step 644, loss 6.6635, acc 0.84375, prec 0.0255807, recall 0.72621
2017-12-10T14:17:47.303706: step 645, loss 0.511931, acc 0.796875, prec 0.0255612, recall 0.72621
2017-12-10T14:17:47.492165: step 646, loss 0.682575, acc 0.8125, prec 0.0255432, recall 0.72621
2017-12-10T14:17:47.676834: step 647, loss 0.555687, acc 0.828125, prec 0.025641, recall 0.727121
2017-12-10T14:17:47.865074: step 648, loss 1.02584, acc 0.734375, prec 0.0256155, recall 0.727121
2017-12-10T14:17:48.052822: step 649, loss 0.888281, acc 0.6875, prec 0.0256425, recall 0.727575
2017-12-10T14:17:48.237177: step 650, loss 0.569511, acc 0.828125, prec 0.025626, recall 0.727575
2017-12-10T14:17:48.423279: step 651, loss 0.799226, acc 0.84375, prec 0.025611, recall 0.727575
2017-12-10T14:17:48.610868: step 652, loss 0.528578, acc 0.859375, prec 0.0256545, recall 0.728027
2017-12-10T14:17:48.800437: step 653, loss 0.55059, acc 0.796875, prec 0.025635, recall 0.728027
2017-12-10T14:17:48.986019: step 654, loss 1.18187, acc 0.875, prec 0.0256799, recall 0.728477
2017-12-10T14:17:49.174733: step 655, loss 0.462393, acc 0.84375, prec 0.025665, recall 0.728477
2017-12-10T14:17:49.368549: step 656, loss 0.541185, acc 0.859375, prec 0.0256515, recall 0.728477
2017-12-10T14:17:49.551704: step 657, loss 0.474477, acc 0.8125, prec 0.0256336, recall 0.728477
2017-12-10T14:17:49.733833: step 658, loss 0.737375, acc 0.84375, prec 0.0256186, recall 0.728477
2017-12-10T14:17:49.922518: step 659, loss 0.965872, acc 0.859375, prec 0.0257186, recall 0.729373
2017-12-10T14:17:50.114123: step 660, loss 0.458861, acc 0.875, prec 0.0257066, recall 0.729373
2017-12-10T14:17:50.309596: step 661, loss 0.430509, acc 0.84375, prec 0.0256917, recall 0.729373
2017-12-10T14:17:50.494748: step 662, loss 0.442039, acc 0.875, prec 0.0256798, recall 0.729373
2017-12-10T14:17:50.683869: step 663, loss 0.512827, acc 0.84375, prec 0.0256648, recall 0.729373
2017-12-10T14:17:50.873948: step 664, loss 0.363701, acc 0.90625, prec 0.0258255, recall 0.730706
2017-12-10T14:17:51.063401: step 665, loss 6.74939, acc 0.84375, prec 0.0258686, recall 0.729951
2017-12-10T14:17:51.255157: step 666, loss 6.95496, acc 0.90625, prec 0.0259176, recall 0.729201
2017-12-10T14:17:51.442149: step 667, loss 1.21562, acc 0.796875, prec 0.0259545, recall 0.729642
2017-12-10T14:17:51.631519: step 668, loss 5.08825, acc 0.796875, prec 0.0259364, recall 0.728455
2017-12-10T14:17:51.821702: step 669, loss 0.956353, acc 0.765625, prec 0.0259139, recall 0.728455
2017-12-10T14:17:52.008080: step 670, loss 1.82254, acc 0.6875, prec 0.0260528, recall 0.729773
2017-12-10T14:17:52.195083: step 671, loss 1.0274, acc 0.75, prec 0.0260849, recall 0.73021
2017-12-10T14:17:52.381179: step 672, loss 1.02977, acc 0.765625, prec 0.0261185, recall 0.730645
2017-12-10T14:17:52.571846: step 673, loss 9.15507, acc 0.65625, prec 0.026143, recall 0.729904
2017-12-10T14:17:52.761255: step 674, loss 7.55471, acc 0.59375, prec 0.0261615, recall 0.729167
2017-12-10T14:17:52.948096: step 675, loss 1.78308, acc 0.59375, prec 0.0262901, recall 0.730462
2017-12-10T14:17:53.130712: step 676, loss 2.05031, acc 0.484375, prec 0.0262404, recall 0.730462
2017-12-10T14:17:53.316823: step 677, loss 2.41978, acc 0.53125, prec 0.0262511, recall 0.730892
2017-12-10T14:17:53.503836: step 678, loss 6.04871, acc 0.375, prec 0.0261927, recall 0.72973
2017-12-10T14:17:53.693237: step 679, loss 2.91207, acc 0.4375, prec 0.0261944, recall 0.730159
2017-12-10T14:17:53.877520: step 680, loss 3.26013, acc 0.46875, prec 0.0262545, recall 0.731013
2017-12-10T14:17:54.067519: step 681, loss 3.01688, acc 0.40625, prec 0.0261979, recall 0.731013
2017-12-10T14:17:54.250778: step 682, loss 2.68342, acc 0.375, prec 0.0262488, recall 0.731861
2017-12-10T14:17:54.435652: step 683, loss 3.53691, acc 0.375, prec 0.0262445, recall 0.732283
2017-12-10T14:17:54.622117: step 684, loss 2.40741, acc 0.484375, prec 0.0262506, recall 0.732704
2017-12-10T14:17:54.809764: step 685, loss 1.82188, acc 0.453125, prec 0.0261989, recall 0.732704
2017-12-10T14:17:54.997080: step 686, loss 2.95851, acc 0.421875, prec 0.0261445, recall 0.732704
2017-12-10T14:17:55.185400: step 687, loss 3.21723, acc 0.4375, prec 0.0260918, recall 0.732704
2017-12-10T14:17:55.375909: step 688, loss 2.16929, acc 0.5, prec 0.0260452, recall 0.732704
2017-12-10T14:17:55.567191: step 689, loss 1.78121, acc 0.609375, prec 0.0260632, recall 0.733124
2017-12-10T14:17:55.751380: step 690, loss 1.50612, acc 0.640625, prec 0.0260298, recall 0.733124
2017-12-10T14:17:55.938415: step 691, loss 1.49129, acc 0.640625, prec 0.0261049, recall 0.733959
2017-12-10T14:17:56.127212: step 692, loss 2.87684, acc 0.53125, prec 0.0261155, recall 0.734375
2017-12-10T14:17:56.313032: step 693, loss 2.19587, acc 0.59375, prec 0.0260778, recall 0.734375
2017-12-10T14:17:56.508176: step 694, loss 0.698267, acc 0.78125, prec 0.0261115, recall 0.734789
2017-12-10T14:17:56.694139: step 695, loss 1.30186, acc 0.671875, prec 0.0260812, recall 0.734789
2017-12-10T14:17:56.885233: step 696, loss 10.7509, acc 0.703125, prec 0.0260552, recall 0.733645
2017-12-10T14:17:57.075932: step 697, loss 0.782586, acc 0.765625, prec 0.0260336, recall 0.733645
2017-12-10T14:17:57.260995: step 698, loss 0.623484, acc 0.796875, prec 0.0260149, recall 0.733645
2017-12-10T14:17:57.448280: step 699, loss 8.61699, acc 0.828125, prec 0.0260006, recall 0.732504
2017-12-10T14:17:57.641390: step 700, loss 0.720125, acc 0.828125, prec 0.0259848, recall 0.732504
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_2/1512933336/checkpoints/model-700

2017-12-10T14:17:58.858779: step 701, loss 0.922242, acc 0.8125, prec 0.0260213, recall 0.732919
2017-12-10T14:17:59.054192: step 702, loss 0.961108, acc 0.734375, prec 0.0260506, recall 0.733333
2017-12-10T14:17:59.242571: step 703, loss 0.822656, acc 0.78125, prec 0.0260305, recall 0.733333
2017-12-10T14:17:59.431037: step 704, loss 0.626065, acc 0.796875, prec 0.0260119, recall 0.733333
2017-12-10T14:17:59.621234: step 705, loss 1.13719, acc 0.75, prec 0.0260425, recall 0.733746
2017-12-10T14:17:59.809716: step 706, loss 10.2822, acc 0.78125, prec 0.0260774, recall 0.733025
2017-12-10T14:18:00.005276: step 707, loss 0.286667, acc 0.875, prec 0.026066, recall 0.733025
2017-12-10T14:18:00.190395: step 708, loss 0.756045, acc 0.75, prec 0.0260431, recall 0.733025
2017-12-10T14:18:00.380959: step 709, loss 0.432295, acc 0.859375, prec 0.0260302, recall 0.733025
2017-12-10T14:18:00.569566: step 710, loss 0.787907, acc 0.78125, prec 0.0261703, recall 0.734255
2017-12-10T14:18:00.763049: step 711, loss 4.33229, acc 0.734375, prec 0.0262006, recall 0.733537
2017-12-10T14:18:00.956120: step 712, loss 0.740834, acc 0.75, prec 0.0261777, recall 0.733537
2017-12-10T14:18:01.146495: step 713, loss 0.972005, acc 0.765625, prec 0.0262095, recall 0.733945
2017-12-10T14:18:01.341635: step 714, loss 0.55482, acc 0.84375, prec 0.0261952, recall 0.733945
2017-12-10T14:18:01.533814: step 715, loss 0.831993, acc 0.765625, prec 0.0261737, recall 0.733945
2017-12-10T14:18:01.717052: step 716, loss 5.43358, acc 0.75, prec 0.0262599, recall 0.732523
2017-12-10T14:18:01.908375: step 717, loss 0.861562, acc 0.75, prec 0.02629, recall 0.732929
2017-12-10T14:18:02.095504: step 718, loss 1.3413, acc 0.65625, prec 0.0263644, recall 0.733737
2017-12-10T14:18:02.283305: step 719, loss 0.878234, acc 0.75, prec 0.0263415, recall 0.733737
2017-12-10T14:18:02.473419: step 720, loss 1.5333, acc 0.5625, prec 0.0263543, recall 0.734139
2017-12-10T14:18:02.657503: step 721, loss 1.08686, acc 0.65625, prec 0.0263756, recall 0.73454
2017-12-10T14:18:02.842575: step 722, loss 1.13282, acc 0.703125, prec 0.0264012, recall 0.73494
2017-12-10T14:18:03.029687: step 723, loss 1.6425, acc 0.671875, prec 0.0264239, recall 0.735338
2017-12-10T14:18:03.215067: step 724, loss 8.96623, acc 0.546875, prec 0.0263839, recall 0.734234
2017-12-10T14:18:03.401263: step 725, loss 1.1601, acc 0.6875, prec 0.0264604, recall 0.73503
2017-12-10T14:18:03.590529: step 726, loss 0.828896, acc 0.765625, prec 0.0264391, recall 0.73503
2017-12-10T14:18:03.778090: step 727, loss 3.15268, acc 0.71875, prec 0.0264673, recall 0.734328
2017-12-10T14:18:03.970689: step 728, loss 1.3964, acc 0.71875, prec 0.0265463, recall 0.735119
2017-12-10T14:18:04.158354: step 729, loss 2.02891, acc 0.578125, prec 0.0265601, recall 0.735513
2017-12-10T14:18:04.345073: step 730, loss 2.80357, acc 0.75, prec 0.0266953, recall 0.735598
2017-12-10T14:18:04.531835: step 731, loss 1.979, acc 0.5625, prec 0.0266552, recall 0.735598
2017-12-10T14:18:04.718910: step 732, loss 1.55302, acc 0.578125, prec 0.0266168, recall 0.735598
2017-12-10T14:18:04.909154: step 733, loss 2.31019, acc 0.515625, prec 0.0265728, recall 0.735598
2017-12-10T14:18:05.094943: step 734, loss 1.3711, acc 0.703125, prec 0.0265977, recall 0.735988
2017-12-10T14:18:05.285450: step 735, loss 0.929354, acc 0.734375, prec 0.0266773, recall 0.736765
2017-12-10T14:18:05.472462: step 736, loss 0.975462, acc 0.75, prec 0.0267064, recall 0.737151
2017-12-10T14:18:05.662732: step 737, loss 1.26029, acc 0.703125, prec 0.0266794, recall 0.737151
2017-12-10T14:18:05.847314: step 738, loss 1.46278, acc 0.6875, prec 0.0266511, recall 0.737151
2017-12-10T14:18:06.032046: step 739, loss 2.0008, acc 0.75, prec 0.0266801, recall 0.737537
2017-12-10T14:18:06.219374: step 740, loss 0.942137, acc 0.8125, prec 0.0266631, recall 0.737537
2017-12-10T14:18:06.405406: step 741, loss 0.706708, acc 0.75, prec 0.0266405, recall 0.737537
2017-12-10T14:18:06.591031: step 742, loss 0.543168, acc 0.8125, prec 0.0266236, recall 0.737537
2017-12-10T14:18:06.778976: step 743, loss 0.756514, acc 0.734375, prec 0.0266512, recall 0.737921
2017-12-10T14:18:06.971380: step 744, loss 0.80259, acc 0.671875, prec 0.0266216, recall 0.737921
2017-12-10T14:18:07.159814: step 745, loss 1.71806, acc 0.6875, prec 0.0266449, recall 0.738304
2017-12-10T14:18:07.349025: step 746, loss 5.60652, acc 0.78125, prec 0.0266266, recall 0.737226
2017-12-10T14:18:07.540353: step 747, loss 0.566793, acc 0.828125, prec 0.0266625, recall 0.737609
2017-12-10T14:18:07.732538: step 748, loss 9.83187, acc 0.71875, prec 0.0266386, recall 0.736536
2017-12-10T14:18:07.926902: step 749, loss 1.8089, acc 0.734375, prec 0.0268707, recall 0.738439
2017-12-10T14:18:08.115896: step 750, loss 0.751966, acc 0.78125, prec 0.0268509, recall 0.738439
2017-12-10T14:18:08.299530: step 751, loss 1.294, acc 0.625, prec 0.0268171, recall 0.738439
2017-12-10T14:18:08.485866: step 752, loss 3.23394, acc 0.578125, prec 0.0268826, recall 0.738129
2017-12-10T14:18:08.674314: step 753, loss 1.10254, acc 0.71875, prec 0.0268572, recall 0.738129
2017-12-10T14:18:08.861573: step 754, loss 1.45607, acc 0.703125, prec 0.0268814, recall 0.738506
2017-12-10T14:18:09.045075: step 755, loss 1.07664, acc 0.640625, prec 0.0268491, recall 0.738506
2017-12-10T14:18:09.233170: step 756, loss 1.52542, acc 0.703125, prec 0.0268733, recall 0.738881
2017-12-10T14:18:09.418095: step 757, loss 0.916623, acc 0.703125, prec 0.0268467, recall 0.738881
2017-12-10T14:18:09.604586: step 758, loss 2.22362, acc 0.5, prec 0.026802, recall 0.738881
2017-12-10T14:18:09.790040: step 759, loss 1.04655, acc 0.71875, prec 0.0268275, recall 0.739255
2017-12-10T14:18:09.977198: step 760, loss 1.58006, acc 0.703125, prec 0.0269021, recall 0.74
2017-12-10T14:18:10.164806: step 761, loss 1.18624, acc 0.640625, prec 0.0269205, recall 0.740371
2017-12-10T14:18:10.353804: step 762, loss 1.25197, acc 0.703125, prec 0.0269948, recall 0.74111
2017-12-10T14:18:10.541992: step 763, loss 1.4099, acc 0.703125, prec 0.027069, recall 0.741844
2017-12-10T14:18:10.730971: step 764, loss 0.706978, acc 0.796875, prec 0.0270508, recall 0.741844
2017-12-10T14:18:10.916334: step 765, loss 0.596471, acc 0.734375, prec 0.0270773, recall 0.74221
2017-12-10T14:18:11.106083: step 766, loss 1.78358, acc 0.609375, prec 0.0270926, recall 0.742574
2017-12-10T14:18:11.292805: step 767, loss 0.486437, acc 0.859375, prec 0.02708, recall 0.742574
2017-12-10T14:18:11.476361: step 768, loss 0.891606, acc 0.796875, prec 0.027112, recall 0.742938
2017-12-10T14:18:11.663706: step 769, loss 0.62643, acc 0.796875, prec 0.027144, recall 0.7433
2017-12-10T14:18:11.855793: step 770, loss 2.14278, acc 0.703125, prec 0.0271188, recall 0.742254
2017-12-10T14:18:12.050044: step 771, loss 3.76122, acc 0.875, prec 0.0271091, recall 0.74121
2017-12-10T14:18:12.241674: step 772, loss 0.891751, acc 0.828125, prec 0.0271937, recall 0.741935
2017-12-10T14:18:12.431724: step 773, loss 0.322903, acc 0.875, prec 0.0272326, recall 0.742297
2017-12-10T14:18:12.621422: step 774, loss 0.592801, acc 0.84375, prec 0.0272186, recall 0.742297
2017-12-10T14:18:12.812827: step 775, loss 0.797369, acc 0.78125, prec 0.0272489, recall 0.742657
2017-12-10T14:18:13.001978: step 776, loss 1.23441, acc 0.875, prec 0.0273874, recall 0.743733
2017-12-10T14:18:13.189591: step 777, loss 4.90147, acc 0.859375, prec 0.0274261, recall 0.743056
2017-12-10T14:18:13.380365: step 778, loss 21.8521, acc 0.765625, prec 0.0274078, recall 0.740997
2017-12-10T14:18:13.573469: step 779, loss 0.859445, acc 0.703125, prec 0.0273811, recall 0.740997
2017-12-10T14:18:13.758515: step 780, loss 1.07529, acc 0.703125, prec 0.0273545, recall 0.740997
2017-12-10T14:18:13.956296: step 781, loss 1.00002, acc 0.78125, prec 0.027335, recall 0.740997
2017-12-10T14:18:14.149122: step 782, loss 1.14309, acc 0.65625, prec 0.0274532, recall 0.742069
2017-12-10T14:18:14.334546: step 783, loss 1.15984, acc 0.65625, prec 0.0274224, recall 0.742069
2017-12-10T14:18:14.520237: step 784, loss 2.39191, acc 0.65625, prec 0.0274907, recall 0.742779
2017-12-10T14:18:14.712249: step 785, loss 1.69718, acc 0.5625, prec 0.027501, recall 0.743132
2017-12-10T14:18:14.897109: step 786, loss 1.2585, acc 0.59375, prec 0.0275635, recall 0.743836
2017-12-10T14:18:15.083437: step 787, loss 2.0232, acc 0.546875, prec 0.0275722, recall 0.744186
2017-12-10T14:18:15.269778: step 788, loss 1.40541, acc 0.625, prec 0.027588, recall 0.744536
2017-12-10T14:18:15.453098: step 789, loss 1.98816, acc 0.609375, prec 0.0275531, recall 0.744536
2017-12-10T14:18:15.638866: step 790, loss 1.2671, acc 0.609375, prec 0.0275674, recall 0.744884
2017-12-10T14:18:15.822238: step 791, loss 10.8571, acc 0.65625, prec 0.0275382, recall 0.743869
2017-12-10T14:18:16.009492: step 792, loss 2.0849, acc 0.5, prec 0.0275428, recall 0.744218
2017-12-10T14:18:16.199003: step 793, loss 2.12622, acc 0.5, prec 0.0274985, recall 0.744218
2017-12-10T14:18:16.387968: step 794, loss 1.43553, acc 0.65625, prec 0.0275658, recall 0.744912
2017-12-10T14:18:16.578493: step 795, loss 1.41387, acc 0.640625, prec 0.0275827, recall 0.745257
2017-12-10T14:18:16.765831: step 796, loss 1.0595, acc 0.6875, prec 0.0276038, recall 0.745602
2017-12-10T14:18:16.953602: step 797, loss 0.917109, acc 0.734375, prec 0.027629, recall 0.745946
2017-12-10T14:18:17.151419: step 798, loss 1.57368, acc 0.734375, prec 0.0277028, recall 0.746631
2017-12-10T14:18:17.343116: step 799, loss 1.13936, acc 0.703125, prec 0.0276765, recall 0.746631
2017-12-10T14:18:17.533383: step 800, loss 0.672745, acc 0.78125, prec 0.0276571, recall 0.746631
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_2/1512933336/checkpoints/model-800

2017-12-10T14:18:19.036041: step 801, loss 1.08703, acc 0.734375, prec 0.0277307, recall 0.747312
2017-12-10T14:18:19.224616: step 802, loss 0.936973, acc 0.765625, prec 0.0277099, recall 0.747312
2017-12-10T14:18:19.412460: step 803, loss 0.932865, acc 0.765625, prec 0.0276892, recall 0.747312
2017-12-10T14:18:19.602913: step 804, loss 1.14619, acc 0.859375, prec 0.0277252, recall 0.747651
2017-12-10T14:18:19.792092: step 805, loss 0.59525, acc 0.765625, prec 0.0277046, recall 0.747651
2017-12-10T14:18:19.985012: step 806, loss 2.64681, acc 0.890625, prec 0.0276963, recall 0.746649
2017-12-10T14:18:20.175226: step 807, loss 9.54277, acc 0.734375, prec 0.0276743, recall 0.745649
2017-12-10T14:18:20.363284: step 808, loss 0.967129, acc 0.765625, prec 0.0276537, recall 0.745649
2017-12-10T14:18:20.551272: step 809, loss 0.794801, acc 0.75, prec 0.0276317, recall 0.745649
2017-12-10T14:18:20.741713: step 810, loss 3.92871, acc 0.796875, prec 0.0277117, recall 0.745333
2017-12-10T14:18:20.931846: step 811, loss 0.797906, acc 0.796875, prec 0.0276938, recall 0.745333
2017-12-10T14:18:21.119361: step 812, loss 1.02715, acc 0.6875, prec 0.0277627, recall 0.746011
2017-12-10T14:18:21.308392: step 813, loss 1.01502, acc 0.8125, prec 0.0277943, recall 0.746348
2017-12-10T14:18:21.493044: step 814, loss 1.03429, acc 0.75, prec 0.0278684, recall 0.74702
2017-12-10T14:18:21.684199: step 815, loss 1.28003, acc 0.703125, prec 0.0278902, recall 0.747355
2017-12-10T14:18:21.872184: step 816, loss 1.90622, acc 0.8125, prec 0.0280655, recall 0.748684
2017-12-10T14:18:22.061725: step 817, loss 0.881802, acc 0.671875, prec 0.0280844, recall 0.749014
2017-12-10T14:18:22.253300: step 818, loss 0.659262, acc 0.75, prec 0.0280622, recall 0.749014
2017-12-10T14:18:22.445095: step 819, loss 1.8609, acc 0.609375, prec 0.0280277, recall 0.749014
2017-12-10T14:18:22.630221: step 820, loss 1.29356, acc 0.65625, prec 0.0280452, recall 0.749344
2017-12-10T14:18:22.819460: step 821, loss 1.19394, acc 0.75, prec 0.0280709, recall 0.749672
2017-12-10T14:18:23.004408: step 822, loss 1.09791, acc 0.765625, prec 0.0281932, recall 0.750653
2017-12-10T14:18:23.194176: step 823, loss 1.08302, acc 0.71875, prec 0.0281683, recall 0.750653
2017-12-10T14:18:23.380354: step 824, loss 1.11726, acc 0.671875, prec 0.0281394, recall 0.750653
2017-12-10T14:18:23.566820: step 825, loss 1.54039, acc 0.75, prec 0.0281649, recall 0.750978
2017-12-10T14:18:23.750823: step 826, loss 1.05438, acc 0.71875, prec 0.0281876, recall 0.751302
2017-12-10T14:18:23.941925: step 827, loss 1.28512, acc 0.671875, prec 0.0281587, recall 0.751302
2017-12-10T14:18:24.129299: step 828, loss 0.565555, acc 0.84375, prec 0.028145, recall 0.751302
2017-12-10T14:18:24.317471: step 829, loss 0.506196, acc 0.796875, prec 0.0281271, recall 0.751302
2017-12-10T14:18:24.509530: step 830, loss 0.948633, acc 0.78125, prec 0.0281553, recall 0.751625
2017-12-10T14:18:24.703022: step 831, loss 0.498955, acc 0.859375, prec 0.028143, recall 0.751625
2017-12-10T14:18:24.896628: step 832, loss 2.14054, acc 0.78125, prec 0.0281252, recall 0.750649
2017-12-10T14:18:25.088584: step 833, loss 0.826583, acc 0.75, prec 0.0281978, recall 0.751295
2017-12-10T14:18:25.275272: step 834, loss 0.89823, acc 0.8125, prec 0.0282285, recall 0.751617
2017-12-10T14:18:25.462591: step 835, loss 0.389899, acc 0.875, prec 0.0282176, recall 0.751617
2017-12-10T14:18:25.650461: step 836, loss 3.27587, acc 0.796875, prec 0.0282011, recall 0.750646
2017-12-10T14:18:25.840509: step 837, loss 0.255601, acc 0.890625, prec 0.0281916, recall 0.750646
2017-12-10T14:18:26.028664: step 838, loss 0.439226, acc 0.859375, prec 0.0281793, recall 0.750646
2017-12-10T14:18:26.221578: step 839, loss 0.289339, acc 0.890625, prec 0.0282168, recall 0.750968
2017-12-10T14:18:26.410653: step 840, loss 0.921006, acc 0.78125, prec 0.0282918, recall 0.751609
2017-12-10T14:18:26.597637: step 841, loss 0.907972, acc 0.828125, prec 0.0283709, recall 0.752246
2017-12-10T14:18:26.782743: step 842, loss 2.67293, acc 0.71875, prec 0.0283945, recall 0.751601
2017-12-10T14:18:26.968774: step 843, loss 0.972418, acc 0.828125, prec 0.0284734, recall 0.752235
2017-12-10T14:18:27.155618: step 844, loss 4.27525, acc 0.796875, prec 0.0285038, recall 0.751592
2017-12-10T14:18:27.344921: step 845, loss 1.08336, acc 0.796875, prec 0.0285797, recall 0.752224
2017-12-10T14:18:27.533450: step 846, loss 11.6091, acc 0.796875, prec 0.0286569, recall 0.751899
2017-12-10T14:18:27.728383: step 847, loss 1.12911, acc 0.6875, prec 0.0286293, recall 0.751899
2017-12-10T14:18:27.912397: step 848, loss 7.95368, acc 0.71875, prec 0.0286526, recall 0.751263
2017-12-10T14:18:28.100513: step 849, loss 8.10181, acc 0.625, prec 0.0286209, recall 0.750315
2017-12-10T14:18:28.289451: step 850, loss 1.74775, acc 0.609375, prec 0.0286332, recall 0.75063
2017-12-10T14:18:28.475946: step 851, loss 1.66748, acc 0.578125, prec 0.0287825, recall 0.75188
2017-12-10T14:18:28.666810: step 852, loss 2.10989, acc 0.5, prec 0.0287849, recall 0.75219
2017-12-10T14:18:28.851315: step 853, loss 3.70899, acc 0.40625, prec 0.0288255, recall 0.752809
2017-12-10T14:18:29.049670: step 854, loss 2.5714, acc 0.453125, prec 0.0288237, recall 0.753117
2017-12-10T14:18:29.236947: step 855, loss 3.26603, acc 0.4375, prec 0.0288668, recall 0.753731
2017-12-10T14:18:29.425136: step 856, loss 3.73656, acc 0.4375, prec 0.0288635, recall 0.754037
2017-12-10T14:18:29.614325: step 857, loss 2.99978, acc 0.4375, prec 0.0289064, recall 0.754647
2017-12-10T14:18:29.801989: step 858, loss 3.67423, acc 0.40625, prec 0.0289004, recall 0.754951
2017-12-10T14:18:29.986793: step 859, loss 2.87152, acc 0.375, prec 0.0289375, recall 0.755556
2017-12-10T14:18:30.170777: step 860, loss 3.05807, acc 0.4375, prec 0.0289342, recall 0.755857
2017-12-10T14:18:30.355032: step 861, loss 3.37468, acc 0.421875, prec 0.0289295, recall 0.756158
2017-12-10T14:18:30.545674: step 862, loss 2.09783, acc 0.5, prec 0.0289316, recall 0.756458
2017-12-10T14:18:30.732058: step 863, loss 2.37364, acc 0.515625, prec 0.0288895, recall 0.756458
2017-12-10T14:18:30.917698: step 864, loss 2.1408, acc 0.453125, prec 0.0289332, recall 0.757055
2017-12-10T14:18:31.106010: step 865, loss 1.26466, acc 0.671875, prec 0.0289502, recall 0.757353
2017-12-10T14:18:31.299030: step 866, loss 1.13829, acc 0.671875, prec 0.0289672, recall 0.75765
2017-12-10T14:18:31.493642: step 867, loss 1.4139, acc 0.765625, prec 0.0290831, recall 0.758537
2017-12-10T14:18:31.683146: step 868, loss 0.851828, acc 0.734375, prec 0.02906, recall 0.758537
2017-12-10T14:18:31.871588: step 869, loss 1.03281, acc 0.75, prec 0.0291289, recall 0.759124
2017-12-10T14:18:32.056873: step 870, loss 1.08019, acc 0.734375, prec 0.0291058, recall 0.759124
2017-12-10T14:18:32.244603: step 871, loss 1.66232, acc 0.90625, prec 0.0292335, recall 0.76
2017-12-10T14:18:32.432841: step 872, loss 1.03239, acc 0.84375, prec 0.0292651, recall 0.760291
2017-12-10T14:18:32.624405: step 873, loss 0.661763, acc 0.828125, prec 0.0292953, recall 0.76058
2017-12-10T14:18:32.813553: step 874, loss 1.13691, acc 0.859375, prec 0.0293282, recall 0.76087
2017-12-10T14:18:33.001393: step 875, loss 18.3206, acc 0.78125, prec 0.0293105, recall 0.759952
2017-12-10T14:18:33.190295: step 876, loss 3.64475, acc 0.8125, prec 0.0292955, recall 0.759036
2017-12-10T14:18:33.379445: step 877, loss 0.917004, acc 0.828125, prec 0.0293708, recall 0.759615
2017-12-10T14:18:33.571018: step 878, loss 0.724054, acc 0.765625, prec 0.0293954, recall 0.759904
2017-12-10T14:18:33.759054: step 879, loss 0.935926, acc 0.734375, prec 0.0294172, recall 0.760192
2017-12-10T14:18:33.944891: step 880, loss 1.07172, acc 0.78125, prec 0.0294881, recall 0.760766
2017-12-10T14:18:34.135357: step 881, loss 1.07191, acc 0.734375, prec 0.0294649, recall 0.760766
2017-12-10T14:18:34.326231: step 882, loss 1.63541, acc 0.65625, prec 0.0294798, recall 0.761051
2017-12-10T14:18:34.511890: step 883, loss 0.696333, acc 0.75, prec 0.029458, recall 0.761051
2017-12-10T14:18:34.698944: step 884, loss 1.02087, acc 0.828125, prec 0.0294879, recall 0.761337
2017-12-10T14:18:34.889913: step 885, loss 0.9632, acc 0.703125, prec 0.0295068, recall 0.761621
2017-12-10T14:18:35.075733: step 886, loss 2.79316, acc 0.671875, prec 0.0295691, recall 0.761283
2017-12-10T14:18:35.264421: step 887, loss 0.976079, acc 0.734375, prec 0.029546, recall 0.761283
2017-12-10T14:18:35.452733: step 888, loss 1.49893, acc 0.6875, prec 0.0295188, recall 0.761283
2017-12-10T14:18:35.638089: step 889, loss 0.669267, acc 0.78125, prec 0.0295891, recall 0.761848
2017-12-10T14:18:35.826176: step 890, loss 1.3875, acc 0.6875, prec 0.0296957, recall 0.762692
2017-12-10T14:18:36.016607: step 891, loss 5.28379, acc 0.734375, prec 0.029763, recall 0.762353
2017-12-10T14:18:36.203941: step 892, loss 0.981573, acc 0.796875, prec 0.0297898, recall 0.762632
2017-12-10T14:18:36.391520: step 893, loss 3.47447, acc 0.46875, prec 0.0297892, recall 0.762016
2017-12-10T14:18:36.578082: step 894, loss 1.18355, acc 0.703125, prec 0.0297633, recall 0.762016
2017-12-10T14:18:36.765149: step 895, loss 0.924755, acc 0.796875, prec 0.0297456, recall 0.762016
2017-12-10T14:18:36.950186: step 896, loss 1.56494, acc 0.609375, prec 0.0297559, recall 0.762295
2017-12-10T14:18:37.137895: step 897, loss 2.14334, acc 0.546875, prec 0.0298051, recall 0.76285
2017-12-10T14:18:37.329209: step 898, loss 1.27275, acc 0.640625, prec 0.0298181, recall 0.763127
2017-12-10T14:18:37.518620: step 899, loss 1.43977, acc 0.578125, prec 0.0297814, recall 0.763127
2017-12-10T14:18:37.702286: step 900, loss 1.35654, acc 0.6875, prec 0.0297985, recall 0.763403
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_2/1512933336/checkpoints/model-900

2017-12-10T14:18:39.332744: step 901, loss 1.35973, acc 0.578125, prec 0.0297619, recall 0.763403
2017-12-10T14:18:39.522011: step 902, loss 1.43037, acc 0.578125, prec 0.0297254, recall 0.763403
2017-12-10T14:18:39.705574: step 903, loss 0.818794, acc 0.8125, prec 0.0297533, recall 0.763679
2017-12-10T14:18:39.891885: step 904, loss 5.47342, acc 0.6875, prec 0.0297276, recall 0.762791
2017-12-10T14:18:40.078424: step 905, loss 1.14391, acc 0.765625, prec 0.0297075, recall 0.762791
2017-12-10T14:18:40.264364: step 906, loss 1.06013, acc 0.703125, prec 0.0297697, recall 0.763341
2017-12-10T14:18:40.450108: step 907, loss 1.46335, acc 0.65625, prec 0.0297401, recall 0.763341
2017-12-10T14:18:40.636787: step 908, loss 1.16903, acc 0.671875, prec 0.0297995, recall 0.763889
2017-12-10T14:18:40.819410: step 909, loss 1.25241, acc 0.6875, prec 0.0297726, recall 0.763889
2017-12-10T14:18:41.009333: step 910, loss 1.14459, acc 0.6875, prec 0.0297458, recall 0.763889
2017-12-10T14:18:41.196117: step 911, loss 2.15111, acc 0.703125, prec 0.0297654, recall 0.763279
2017-12-10T14:18:41.383361: step 912, loss 0.699304, acc 0.78125, prec 0.0297903, recall 0.763552
2017-12-10T14:18:41.568718: step 913, loss 0.619625, acc 0.796875, prec 0.0298165, recall 0.763825
2017-12-10T14:18:41.756150: step 914, loss 0.820494, acc 0.71875, prec 0.0297924, recall 0.763825
2017-12-10T14:18:41.941777: step 915, loss 0.700388, acc 0.734375, prec 0.0298568, recall 0.764368
2017-12-10T14:18:42.130170: step 916, loss 0.395683, acc 0.828125, prec 0.029842, recall 0.764368
2017-12-10T14:18:42.315674: step 917, loss 0.41297, acc 0.859375, prec 0.02983, recall 0.764368
2017-12-10T14:18:42.504065: step 918, loss 0.794452, acc 0.84375, prec 0.0299036, recall 0.764908
2017-12-10T14:18:42.694608: step 919, loss 0.828726, acc 0.8125, prec 0.029931, recall 0.765178
2017-12-10T14:18:42.886004: step 920, loss 3.80091, acc 0.875, prec 0.0299216, recall 0.764302
2017-12-10T14:18:43.076382: step 921, loss 0.360644, acc 0.859375, prec 0.0299096, recall 0.764302
2017-12-10T14:18:43.265317: step 922, loss 0.248199, acc 0.921875, prec 0.0299029, recall 0.764302
2017-12-10T14:18:43.455240: step 923, loss 0.594301, acc 0.78125, prec 0.0298841, recall 0.764302
2017-12-10T14:18:43.640505: step 924, loss 1.92365, acc 0.765625, prec 0.0299508, recall 0.76484
2017-12-10T14:18:43.830536: step 925, loss 0.355587, acc 0.890625, prec 0.0299415, recall 0.76484
2017-12-10T14:18:44.027762: step 926, loss 3.0733, acc 0.859375, prec 0.0299308, recall 0.763968
2017-12-10T14:18:44.221284: step 927, loss 0.568362, acc 0.796875, prec 0.03, recall 0.764505
2017-12-10T14:18:44.406081: step 928, loss 0.750392, acc 0.8125, prec 0.0300272, recall 0.764773
2017-12-10T14:18:44.593680: step 929, loss 1.69403, acc 0.71875, prec 0.0300464, recall 0.76504
2017-12-10T14:18:44.783486: step 930, loss 9.97942, acc 0.703125, prec 0.0300223, recall 0.764172
2017-12-10T14:18:44.970144: step 931, loss 0.474879, acc 0.78125, prec 0.0300467, recall 0.764439
2017-12-10T14:18:45.157770: step 932, loss 0.999746, acc 0.796875, prec 0.0300294, recall 0.764439
2017-12-10T14:18:45.343037: step 933, loss 0.913513, acc 0.71875, prec 0.0300053, recall 0.764439
2017-12-10T14:18:45.532529: step 934, loss 1.81644, acc 0.625, prec 0.0300164, recall 0.764706
2017-12-10T14:18:45.717538: step 935, loss 1.06191, acc 0.71875, prec 0.0300355, recall 0.764972
2017-12-10T14:18:45.906270: step 936, loss 1.46487, acc 0.625, prec 0.0300465, recall 0.765237
2017-12-10T14:18:46.091966: step 937, loss 1.41041, acc 0.640625, prec 0.0300589, recall 0.765502
2017-12-10T14:18:46.279432: step 938, loss 0.846502, acc 0.734375, prec 0.0300792, recall 0.765766
2017-12-10T14:18:46.465456: step 939, loss 6.38664, acc 0.609375, prec 0.030133, recall 0.765432
2017-12-10T14:18:46.657892: step 940, loss 1.04796, acc 0.75, prec 0.0301117, recall 0.765432
2017-12-10T14:18:46.849000: step 941, loss 1.3687, acc 0.65625, prec 0.030168, recall 0.765957
2017-12-10T14:18:47.036858: step 942, loss 7.69328, acc 0.765625, prec 0.0301494, recall 0.765101
2017-12-10T14:18:47.224250: step 943, loss 1.35207, acc 0.703125, prec 0.0301669, recall 0.765363
2017-12-10T14:18:47.411864: step 944, loss 1.82687, acc 0.578125, prec 0.0302164, recall 0.765886
2017-12-10T14:18:47.598331: step 945, loss 1.53689, acc 0.625, prec 0.0302271, recall 0.766147
2017-12-10T14:18:47.785168: step 946, loss 3.56184, acc 0.53125, prec 0.0301887, recall 0.765295
2017-12-10T14:18:47.973828: step 947, loss 1.45286, acc 0.625, prec 0.0301994, recall 0.765556
2017-12-10T14:18:48.160152: step 948, loss 1.97872, acc 0.640625, prec 0.0302115, recall 0.765816
2017-12-10T14:18:48.345157: step 949, loss 1.84498, acc 0.59375, prec 0.0301771, recall 0.765816
2017-12-10T14:18:48.533525: step 950, loss 1.26841, acc 0.671875, prec 0.0301494, recall 0.765816
2017-12-10T14:18:48.723859: step 951, loss 1.19566, acc 0.65625, prec 0.0301205, recall 0.765816
2017-12-10T14:18:48.906038: step 952, loss 1.43011, acc 0.625, prec 0.0301313, recall 0.766075
2017-12-10T14:18:49.092644: step 953, loss 2.07275, acc 0.53125, prec 0.0301341, recall 0.766334
2017-12-10T14:18:49.276460: step 954, loss 8.39719, acc 0.578125, prec 0.0301844, recall 0.766004
2017-12-10T14:18:49.466164: step 955, loss 1.34894, acc 0.71875, prec 0.0302029, recall 0.766262
2017-12-10T14:18:49.656873: step 956, loss 1.34852, acc 0.671875, prec 0.0302175, recall 0.76652
2017-12-10T14:18:49.846600: step 957, loss 1.64146, acc 0.640625, prec 0.0302294, recall 0.766777
2017-12-10T14:18:50.033083: step 958, loss 1.31452, acc 0.578125, prec 0.0301941, recall 0.766777
2017-12-10T14:18:50.222072: step 959, loss 2.01345, acc 0.671875, prec 0.0302506, recall 0.767289
2017-12-10T14:18:50.411151: step 960, loss 1.8077, acc 0.640625, prec 0.0303043, recall 0.767798
2017-12-10T14:18:50.599395: step 961, loss 1.15252, acc 0.734375, prec 0.0302821, recall 0.767798
2017-12-10T14:18:50.787742: step 962, loss 0.897754, acc 0.71875, prec 0.0302586, recall 0.767798
2017-12-10T14:18:50.974664: step 963, loss 0.904109, acc 0.71875, prec 0.0302351, recall 0.767798
2017-12-10T14:18:51.160676: step 964, loss 0.505089, acc 0.796875, prec 0.0303017, recall 0.768306
2017-12-10T14:18:51.346843: step 965, loss 14.5218, acc 0.765625, prec 0.0302834, recall 0.767467
2017-12-10T14:18:51.535961: step 966, loss 3.54804, acc 0.796875, prec 0.0302678, recall 0.76663
2017-12-10T14:18:51.726977: step 967, loss 0.934282, acc 0.828125, prec 0.0303369, recall 0.767138
2017-12-10T14:18:51.916985: step 968, loss 0.579259, acc 0.828125, prec 0.030406, recall 0.767644
2017-12-10T14:18:52.104881: step 969, loss 0.993583, acc 0.796875, prec 0.0304307, recall 0.767896
2017-12-10T14:18:52.296913: step 970, loss 1.29435, acc 0.6875, prec 0.0304045, recall 0.767896
2017-12-10T14:18:52.485836: step 971, loss 0.72223, acc 0.859375, prec 0.0304344, recall 0.768147
2017-12-10T14:18:52.674188: step 972, loss 0.624338, acc 0.796875, prec 0.030459, recall 0.768398
2017-12-10T14:18:52.866110: step 973, loss 5.27681, acc 0.765625, prec 0.0304823, recall 0.767819
2017-12-10T14:18:53.055175: step 974, loss 0.948768, acc 0.765625, prec 0.0305043, recall 0.768069
2017-12-10T14:18:53.245694: step 975, loss 0.825638, acc 0.71875, prec 0.0304808, recall 0.768069
2017-12-10T14:18:53.431334: step 976, loss 1.22771, acc 0.703125, prec 0.0304975, recall 0.768319
2017-12-10T14:18:53.620492: step 977, loss 1.38308, acc 0.640625, prec 0.0305089, recall 0.768568
2017-12-10T14:18:53.806674: step 978, loss 1.0697, acc 0.640625, prec 0.0305203, recall 0.768817
2017-12-10T14:18:53.992953: step 979, loss 1.28463, acc 0.640625, prec 0.0304904, recall 0.768817
2017-12-10T14:18:54.179707: step 980, loss 0.660622, acc 0.78125, prec 0.0305135, recall 0.769065
2017-12-10T14:18:54.368790: step 981, loss 0.935287, acc 0.734375, prec 0.0305327, recall 0.769313
2017-12-10T14:18:54.554470: step 982, loss 0.659095, acc 0.734375, prec 0.0305519, recall 0.769561
2017-12-10T14:18:54.744008: step 983, loss 0.860168, acc 0.765625, prec 0.0305324, recall 0.769561
2017-12-10T14:18:54.927584: step 984, loss 1.59343, acc 0.734375, prec 0.0305515, recall 0.769807
2017-12-10T14:18:55.114370: step 985, loss 3.57865, acc 0.796875, prec 0.030536, recall 0.768984
2017-12-10T14:18:55.300539: step 986, loss 0.601512, acc 0.84375, prec 0.0305642, recall 0.769231
2017-12-10T14:18:55.485540: step 987, loss 0.55424, acc 0.78125, prec 0.030546, recall 0.769231
2017-12-10T14:18:55.675673: step 988, loss 1.11391, acc 0.75, prec 0.0305253, recall 0.769231
2017-12-10T14:18:55.862135: step 989, loss 0.565186, acc 0.78125, prec 0.0305072, recall 0.769231
2017-12-10T14:18:56.052254: step 990, loss 0.75602, acc 0.8125, prec 0.0304917, recall 0.769231
2017-12-10T14:18:56.239837: step 991, loss 0.45838, acc 0.84375, prec 0.0304788, recall 0.769231
2017-12-10T14:18:56.431737: step 992, loss 0.356845, acc 0.90625, prec 0.0305121, recall 0.769477
2017-12-10T14:18:56.620621: step 993, loss 4.16291, acc 0.765625, prec 0.030535, recall 0.768903
2017-12-10T14:18:56.790449: step 994, loss 0.538005, acc 0.807692, prec 0.0305631, recall 0.769149
2017-12-10T14:18:56.982309: step 995, loss 0.826275, acc 0.78125, prec 0.030545, recall 0.769149
2017-12-10T14:18:57.174158: step 996, loss 0.40313, acc 0.921875, prec 0.0305385, recall 0.769149
2017-12-10T14:18:57.360681: step 997, loss 3.48977, acc 0.8125, prec 0.0305244, recall 0.768332
2017-12-10T14:18:57.549918: step 998, loss 0.580374, acc 0.8125, prec 0.0305907, recall 0.768823
2017-12-10T14:18:57.739256: step 999, loss 0.545891, acc 0.8125, prec 0.0306161, recall 0.769068
2017-12-10T14:18:57.924004: step 1000, loss 1.09718, acc 0.71875, prec 0.0306337, recall 0.769312
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_2/1512933336/checkpoints/model-1000

2017-12-10T14:18:59.264328: step 1001, loss 0.598787, acc 0.78125, prec 0.0306157, recall 0.769312
2017-12-10T14:18:59.451230: step 1002, loss 0.919264, acc 0.75, prec 0.0306359, recall 0.769556
2017-12-10T14:18:59.635546: step 1003, loss 0.602078, acc 0.84375, prec 0.0306638, recall 0.769799
2017-12-10T14:18:59.825027: step 1004, loss 0.787433, acc 0.765625, prec 0.0306444, recall 0.769799
2017-12-10T14:19:00.009467: step 1005, loss 0.676584, acc 0.796875, prec 0.0306277, recall 0.769799
2017-12-10T14:19:00.196540: step 1006, loss 0.618342, acc 0.8125, prec 0.0306529, recall 0.770042
2017-12-10T14:19:00.389756: step 1007, loss 0.972409, acc 0.78125, prec 0.0306349, recall 0.770042
2017-12-10T14:19:00.578656: step 1008, loss 6.42753, acc 0.859375, prec 0.030706, recall 0.769716
2017-12-10T14:19:00.769020: step 1009, loss 2.35113, acc 0.8125, prec 0.0306918, recall 0.768908
2017-12-10T14:19:00.958321: step 1010, loss 0.747208, acc 0.78125, prec 0.0306738, recall 0.768908
2017-12-10T14:19:01.149624: step 1011, loss 0.668776, acc 0.8125, prec 0.0306584, recall 0.768908
2017-12-10T14:19:01.341351: step 1012, loss 1.09347, acc 0.6875, prec 0.0306733, recall 0.76915
2017-12-10T14:19:01.531603: step 1013, loss 0.660962, acc 0.78125, prec 0.0306959, recall 0.769392
2017-12-10T14:19:01.718869: step 1014, loss 0.688209, acc 0.8125, prec 0.030802, recall 0.770115
2017-12-10T14:19:01.903759: step 1015, loss 0.701834, acc 0.78125, prec 0.0308245, recall 0.770355
2017-12-10T14:19:02.089961: step 1016, loss 0.525317, acc 0.859375, prec 0.0308129, recall 0.770355
2017-12-10T14:19:02.278877: step 1017, loss 1.04244, acc 0.734375, prec 0.0307911, recall 0.770355
2017-12-10T14:19:02.465489: step 1018, loss 0.805921, acc 0.734375, prec 0.0307692, recall 0.770355
2017-12-10T14:19:02.653900: step 1019, loss 0.516077, acc 0.78125, prec 0.0307513, recall 0.770355
2017-12-10T14:19:02.844813: step 1020, loss 0.655768, acc 0.828125, prec 0.0307776, recall 0.770594
2017-12-10T14:19:03.034364: step 1021, loss 0.380309, acc 0.828125, prec 0.0307635, recall 0.770594
2017-12-10T14:19:03.220472: step 1022, loss 4.46979, acc 0.8125, prec 0.0307897, recall 0.770031
2017-12-10T14:19:03.412092: step 1023, loss 0.723444, acc 0.84375, prec 0.0308172, recall 0.77027
2017-12-10T14:19:03.598243: step 1024, loss 0.707438, acc 0.75, prec 0.0307967, recall 0.77027
2017-12-10T14:19:03.786802: step 1025, loss 8.64611, acc 0.671875, prec 0.0308127, recall 0.768912
2017-12-10T14:19:03.975397: step 1026, loss 0.529566, acc 0.890625, prec 0.030844, recall 0.769151
2017-12-10T14:19:04.161938: step 1027, loss 0.758363, acc 0.84375, prec 0.0308312, recall 0.769151
2017-12-10T14:19:04.349342: step 1028, loss 0.658987, acc 0.765625, prec 0.030812, recall 0.769151
2017-12-10T14:19:04.538637: step 1029, loss 1.43571, acc 0.609375, prec 0.0307801, recall 0.769151
2017-12-10T14:19:04.726704: step 1030, loss 1.1402, acc 0.6875, prec 0.0307947, recall 0.76939
2017-12-10T14:19:04.913430: step 1031, loss 0.682063, acc 0.734375, prec 0.0308933, recall 0.770103
2017-12-10T14:19:05.104356: step 1032, loss 0.772871, acc 0.78125, prec 0.0309155, recall 0.77034
2017-12-10T14:19:05.293433: step 1033, loss 1.41154, acc 0.734375, prec 0.0309338, recall 0.770576
2017-12-10T14:19:05.485913: step 1034, loss 1.45798, acc 0.65625, prec 0.0309457, recall 0.770812
2017-12-10T14:19:05.669994: step 1035, loss 1.00184, acc 0.765625, prec 0.0310065, recall 0.771282
2017-12-10T14:19:05.856216: step 1036, loss 0.697748, acc 0.828125, prec 0.0309924, recall 0.771282
2017-12-10T14:19:06.046692: step 1037, loss 0.991574, acc 0.671875, prec 0.0309656, recall 0.771282
2017-12-10T14:19:06.234924: step 1038, loss 3.38514, acc 0.796875, prec 0.0309902, recall 0.770727
2017-12-10T14:19:06.427146: step 1039, loss 1.40195, acc 0.625, prec 0.0309596, recall 0.770727
2017-12-10T14:19:06.611225: step 1040, loss 1.61178, acc 0.828125, prec 0.0309855, recall 0.770961
2017-12-10T14:19:06.801364: step 1041, loss 1.0584, acc 0.734375, prec 0.0310036, recall 0.771195
2017-12-10T14:19:06.986871: step 1042, loss 0.736336, acc 0.734375, prec 0.0310217, recall 0.771429
2017-12-10T14:19:07.175687: step 1043, loss 0.969991, acc 0.734375, prec 0.0310399, recall 0.771662
2017-12-10T14:19:07.361897: step 1044, loss 0.416298, acc 0.875, prec 0.0310694, recall 0.771894
2017-12-10T14:19:07.549881: step 1045, loss 0.632418, acc 0.765625, prec 0.03109, recall 0.772126
2017-12-10T14:19:07.739208: step 1046, loss 0.332338, acc 0.875, prec 0.0311592, recall 0.772589
2017-12-10T14:19:07.928711: step 1047, loss 0.965532, acc 0.8125, prec 0.0313024, recall 0.773509
2017-12-10T14:19:08.113744: step 1048, loss 0.622853, acc 0.828125, prec 0.0312883, recall 0.773509
2017-12-10T14:19:08.300425: step 1049, loss 0.600411, acc 0.84375, prec 0.0313152, recall 0.773737
2017-12-10T14:19:08.487655: step 1050, loss 0.388231, acc 0.84375, prec 0.0313419, recall 0.773966
2017-12-10T14:19:08.680246: step 1051, loss 2.19283, acc 0.890625, prec 0.0314134, recall 0.773642
2017-12-10T14:19:08.878126: step 1052, loss 0.935468, acc 0.8125, prec 0.0314771, recall 0.774096
2017-12-10T14:19:09.068167: step 1053, loss 2.13611, acc 0.703125, prec 0.031454, recall 0.77332
2017-12-10T14:19:09.257135: step 1054, loss 3.65842, acc 0.84375, prec 0.0315214, recall 0.773
2017-12-10T14:19:09.446021: step 1055, loss 0.65885, acc 0.796875, prec 0.0315442, recall 0.773227
2017-12-10T14:19:09.631033: step 1056, loss 0.303814, acc 0.859375, prec 0.0315721, recall 0.773453
2017-12-10T14:19:09.816616: step 1057, loss 2.0132, acc 0.84375, prec 0.0316, recall 0.772908
2017-12-10T14:19:10.006120: step 1058, loss 0.870875, acc 0.765625, prec 0.0316201, recall 0.773134
2017-12-10T14:19:10.192236: step 1059, loss 1.10412, acc 0.640625, prec 0.0316299, recall 0.77336
2017-12-10T14:19:10.381961: step 1060, loss 1.15685, acc 0.609375, prec 0.0316371, recall 0.773585
2017-12-10T14:19:10.570222: step 1061, loss 1.74029, acc 0.59375, prec 0.0316037, recall 0.773585
2017-12-10T14:19:10.752365: step 1062, loss 1.44015, acc 0.65625, prec 0.0315755, recall 0.773585
2017-12-10T14:19:10.938883: step 1063, loss 0.902746, acc 0.6875, prec 0.03155, recall 0.773585
2017-12-10T14:19:11.128411: step 1064, loss 1.81148, acc 0.578125, prec 0.0315155, recall 0.773585
2017-12-10T14:19:11.317575: step 1065, loss 1.57668, acc 0.609375, prec 0.0315228, recall 0.77381
2017-12-10T14:19:11.506673: step 1066, loss 1.46072, acc 0.578125, prec 0.0315275, recall 0.774034
2017-12-10T14:19:11.695039: step 1067, loss 1.06808, acc 0.75, prec 0.0315072, recall 0.774034
2017-12-10T14:19:11.887577: step 1068, loss 1.33021, acc 0.6875, prec 0.0315989, recall 0.774704
2017-12-10T14:19:12.075371: step 1069, loss 0.635356, acc 0.78125, prec 0.0316591, recall 0.775148
2017-12-10T14:19:12.262109: step 1070, loss 0.924884, acc 0.765625, prec 0.0317569, recall 0.775811
2017-12-10T14:19:12.452890: step 1071, loss 4.87559, acc 0.765625, prec 0.0318558, recall 0.77571
2017-12-10T14:19:12.647832: step 1072, loss 0.620479, acc 0.828125, prec 0.0318807, recall 0.77593
2017-12-10T14:19:12.840899: step 1073, loss 0.699433, acc 0.734375, prec 0.0318589, recall 0.77593
2017-12-10T14:19:13.025922: step 1074, loss 0.851781, acc 0.828125, prec 0.0318448, recall 0.77593
2017-12-10T14:19:13.214865: step 1075, loss 1.02671, acc 0.71875, prec 0.0318607, recall 0.776149
2017-12-10T14:19:13.402788: step 1076, loss 1.06009, acc 0.71875, prec 0.0318377, recall 0.776149
2017-12-10T14:19:13.585172: step 1077, loss 4.13721, acc 0.828125, prec 0.0318249, recall 0.775391
2017-12-10T14:19:13.774540: step 1078, loss 0.994631, acc 0.71875, prec 0.0318795, recall 0.775828
2017-12-10T14:19:13.965955: step 1079, loss 1.54501, acc 0.671875, prec 0.0318915, recall 0.776047
2017-12-10T14:19:14.157770: step 1080, loss 0.864228, acc 0.734375, prec 0.0319085, recall 0.776265
2017-12-10T14:19:14.344845: step 1081, loss 0.817481, acc 0.734375, prec 0.0319642, recall 0.776699
2017-12-10T14:19:14.530770: step 1082, loss 1.99013, acc 0.625, prec 0.0320109, recall 0.777132
2017-12-10T14:19:14.719010: step 1083, loss 0.675913, acc 0.71875, prec 0.0319879, recall 0.777132
2017-12-10T14:19:14.906838: step 1084, loss 0.86251, acc 0.71875, prec 0.0320035, recall 0.777348
2017-12-10T14:19:15.095959: step 1085, loss 0.841918, acc 0.703125, prec 0.0320178, recall 0.777563
2017-12-10T14:19:15.284267: step 1086, loss 0.768527, acc 0.78125, prec 0.032, recall 0.777563
2017-12-10T14:19:15.469951: step 1087, loss 1.76268, acc 0.828125, prec 0.0320245, recall 0.777778
2017-12-10T14:19:15.662045: step 1088, loss 0.870311, acc 0.765625, prec 0.0320439, recall 0.777992
2017-12-10T14:19:15.854084: step 1089, loss 1.05707, acc 0.765625, prec 0.0320248, recall 0.777992
2017-12-10T14:19:16.042784: step 1090, loss 0.854813, acc 0.78125, prec 0.0320454, recall 0.778206
2017-12-10T14:19:16.231223: step 1091, loss 7.75913, acc 0.796875, prec 0.0321083, recall 0.777137
2017-12-10T14:19:16.418239: step 1092, loss 0.973171, acc 0.75, prec 0.0320879, recall 0.777137
2017-12-10T14:19:16.606273: step 1093, loss 1.06648, acc 0.6875, prec 0.0321008, recall 0.777351
2017-12-10T14:19:16.791126: step 1094, loss 1.04806, acc 0.75, prec 0.0321188, recall 0.777565
2017-12-10T14:19:16.980730: step 1095, loss 0.713174, acc 0.796875, prec 0.0321023, recall 0.777565
2017-12-10T14:19:17.169568: step 1096, loss 1.39208, acc 0.65625, prec 0.0320744, recall 0.777565
2017-12-10T14:19:17.355296: step 1097, loss 1.15116, acc 0.671875, prec 0.0320477, recall 0.777565
2017-12-10T14:19:17.541715: step 1098, loss 0.757674, acc 0.75, prec 0.0320275, recall 0.777565
2017-12-10T14:19:17.730797: step 1099, loss 1.09672, acc 0.703125, prec 0.0320035, recall 0.777565
2017-12-10T14:19:17.916213: step 1100, loss 1.37122, acc 0.671875, prec 0.0320151, recall 0.777778
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_2/1512933336/checkpoints/model-1100

2017-12-10T14:19:19.080832: step 1101, loss 0.647073, acc 0.734375, prec 0.0319937, recall 0.777778
2017-12-10T14:19:19.271649: step 1102, loss 0.777305, acc 0.75, prec 0.0320117, recall 0.77799
2017-12-10T14:19:19.456048: step 1103, loss 1.0077, acc 0.8125, prec 0.0320346, recall 0.778203
2017-12-10T14:19:19.650956: step 1104, loss 0.674884, acc 0.765625, prec 0.0320157, recall 0.778203
2017-12-10T14:19:19.839757: step 1105, loss 0.94044, acc 0.78125, prec 0.0319981, recall 0.778203
2017-12-10T14:19:20.028460: step 1106, loss 0.583821, acc 0.875, prec 0.0319881, recall 0.778203
2017-12-10T14:19:20.216740: step 1107, loss 0.733721, acc 0.765625, prec 0.0319692, recall 0.778203
2017-12-10T14:19:20.403898: step 1108, loss 0.555661, acc 0.8125, prec 0.0319542, recall 0.778203
2017-12-10T14:19:20.591232: step 1109, loss 0.603283, acc 0.84375, prec 0.0319796, recall 0.778415
2017-12-10T14:19:20.776554: step 1110, loss 0.268277, acc 0.90625, prec 0.0319721, recall 0.778415
2017-12-10T14:19:20.962227: step 1111, loss 0.630751, acc 0.8125, prec 0.031957, recall 0.778415
2017-12-10T14:19:21.153000: step 1112, loss 0.155719, acc 0.953125, prec 0.0319533, recall 0.778415
2017-12-10T14:19:21.343632: step 1113, loss 0.460618, acc 0.890625, prec 0.0319824, recall 0.778626
2017-12-10T14:19:21.531841: step 1114, loss 6.49136, acc 0.875, prec 0.0319737, recall 0.777884
2017-12-10T14:19:21.724751: step 1115, loss 0.306623, acc 0.890625, prec 0.0319649, recall 0.777884
2017-12-10T14:19:21.912380: step 1116, loss 3.07594, acc 0.875, prec 0.031994, recall 0.777355
2017-12-10T14:19:22.099248: step 1117, loss 0.433539, acc 0.890625, prec 0.0320232, recall 0.777567
2017-12-10T14:19:22.287165: step 1118, loss 0.272146, acc 0.890625, prec 0.0320144, recall 0.777567
2017-12-10T14:19:22.476600: step 1119, loss 1.62348, acc 0.8125, prec 0.0320751, recall 0.777989
2017-12-10T14:19:22.665036: step 1120, loss 0.396455, acc 0.84375, prec 0.0320626, recall 0.777989
2017-12-10T14:19:22.852740: step 1121, loss 0.490063, acc 0.84375, prec 0.0320879, recall 0.778199
2017-12-10T14:19:23.038639: step 1122, loss 3.55722, acc 0.78125, prec 0.0321094, recall 0.777673
2017-12-10T14:19:23.231026: step 1123, loss 0.57171, acc 0.875, prec 0.0322127, recall 0.778302
2017-12-10T14:19:23.417717: step 1124, loss 0.752636, acc 0.828125, prec 0.0321989, recall 0.778302
2017-12-10T14:19:23.604920: step 1125, loss 0.819045, acc 0.703125, prec 0.0322128, recall 0.778511
2017-12-10T14:19:23.795924: step 1126, loss 0.662176, acc 0.796875, prec 0.0321965, recall 0.778511
2017-12-10T14:19:23.987421: step 1127, loss 2.05767, acc 0.65625, prec 0.0322442, recall 0.778928
2017-12-10T14:19:24.173611: step 1128, loss 1.96819, acc 0.6875, prec 0.0322581, recall 0.778404
2017-12-10T14:19:24.359218: step 1129, loss 1.16486, acc 0.75, prec 0.0322756, recall 0.778612
2017-12-10T14:19:24.547904: step 1130, loss 0.869972, acc 0.71875, prec 0.032253, recall 0.778612
2017-12-10T14:19:24.733547: step 1131, loss 1.29023, acc 0.703125, prec 0.0322668, recall 0.778819
2017-12-10T14:19:24.919922: step 1132, loss 1.47906, acc 0.6875, prec 0.0322793, recall 0.779026
2017-12-10T14:19:25.106364: step 1133, loss 1.16509, acc 0.75, prec 0.0322968, recall 0.779233
2017-12-10T14:19:25.296428: step 1134, loss 1.07277, acc 0.671875, prec 0.032308, recall 0.779439
2017-12-10T14:19:25.480912: step 1135, loss 0.99407, acc 0.671875, prec 0.0323567, recall 0.779851
2017-12-10T14:19:25.672031: step 1136, loss 1.24576, acc 0.640625, prec 0.0324028, recall 0.780261
2017-12-10T14:19:25.859693: step 1137, loss 9.22975, acc 0.671875, prec 0.0324911, recall 0.779425
2017-12-10T14:19:26.046826: step 1138, loss 1.02521, acc 0.765625, prec 0.0325097, recall 0.77963
2017-12-10T14:19:26.231934: step 1139, loss 1.93959, acc 0.703125, prec 0.0325231, recall 0.779833
2017-12-10T14:19:26.418860: step 1140, loss 1.34031, acc 0.625, prec 0.0324931, recall 0.779833
2017-12-10T14:19:26.607449: step 1141, loss 1.19856, acc 0.609375, prec 0.032499, recall 0.780037
2017-12-10T14:19:26.797142: step 1142, loss 1.60197, acc 0.65625, prec 0.0325088, recall 0.78024
2017-12-10T14:19:26.985769: step 1143, loss 1.46549, acc 0.609375, prec 0.0324775, recall 0.78024
2017-12-10T14:19:27.171941: step 1144, loss 3.29775, acc 0.625, prec 0.032486, recall 0.779724
2017-12-10T14:19:27.362044: step 1145, loss 1.12442, acc 0.65625, prec 0.0324586, recall 0.779724
2017-12-10T14:19:27.552114: step 1146, loss 1.35243, acc 0.625, prec 0.0324658, recall 0.779926
2017-12-10T14:19:27.741281: step 1147, loss 1.43848, acc 0.578125, prec 0.0324693, recall 0.780129
2017-12-10T14:19:27.926082: step 1148, loss 1.26093, acc 0.6875, prec 0.0324814, recall 0.780331
2017-12-10T14:19:28.110855: step 1149, loss 0.854758, acc 0.671875, prec 0.0324924, recall 0.780533
2017-12-10T14:19:28.301024: step 1150, loss 1.35439, acc 0.671875, prec 0.0325032, recall 0.780734
2017-12-10T14:19:28.488320: step 1151, loss 1.1236, acc 0.75, prec 0.0325203, recall 0.780935
2017-12-10T14:19:28.674291: step 1152, loss 1.3235, acc 0.640625, prec 0.0325287, recall 0.781136
2017-12-10T14:19:28.859361: step 1153, loss 1.00825, acc 0.71875, prec 0.032617, recall 0.781735
2017-12-10T14:19:29.058103: step 1154, loss 3.13985, acc 0.734375, prec 0.0325971, recall 0.781022
2017-12-10T14:19:29.247940: step 1155, loss 1.21676, acc 0.734375, prec 0.032576, recall 0.781022
2017-12-10T14:19:29.435719: step 1156, loss 0.421721, acc 0.890625, prec 0.0325673, recall 0.781022
2017-12-10T14:19:29.621754: step 1157, loss 0.941738, acc 0.6875, prec 0.0325426, recall 0.781022
2017-12-10T14:19:29.805139: step 1158, loss 0.612564, acc 0.828125, prec 0.0325657, recall 0.781222
2017-12-10T14:19:29.992187: step 1159, loss 2.64605, acc 0.828125, prec 0.0326269, recall 0.780909
2017-12-10T14:19:30.184774: step 1160, loss 0.564219, acc 0.796875, prec 0.0326108, recall 0.780909
2017-12-10T14:19:30.375537: step 1161, loss 0.3571, acc 0.875, prec 0.0326009, recall 0.780909
2017-12-10T14:19:30.563660: step 1162, loss 0.537818, acc 0.796875, prec 0.0325848, recall 0.780909
2017-12-10T14:19:30.750562: step 1163, loss 0.270146, acc 0.90625, prec 0.0326141, recall 0.781108
2017-12-10T14:19:30.938863: step 1164, loss 0.575321, acc 0.875, prec 0.0326775, recall 0.781505
2017-12-10T14:19:31.133797: step 1165, loss 5.42541, acc 0.828125, prec 0.0327397, recall 0.780488
2017-12-10T14:19:31.334380: step 1166, loss 0.489386, acc 0.84375, prec 0.0327273, recall 0.780488
2017-12-10T14:19:31.527578: step 1167, loss 0.546819, acc 0.828125, prec 0.0327869, recall 0.780884
2017-12-10T14:19:31.718712: step 1168, loss 0.590942, acc 0.765625, prec 0.0327683, recall 0.780884
2017-12-10T14:19:31.911021: step 1169, loss 0.902817, acc 0.78125, prec 0.0327509, recall 0.780884
2017-12-10T14:19:32.098510: step 1170, loss 0.578198, acc 0.765625, prec 0.0327689, recall 0.781081
2017-12-10T14:19:32.284383: step 1171, loss 1.07403, acc 0.734375, prec 0.0327844, recall 0.781278
2017-12-10T14:19:32.473326: step 1172, loss 2.75747, acc 0.75, prec 0.0328024, recall 0.780773
2017-12-10T14:19:32.665641: step 1173, loss 3.76898, acc 0.8125, prec 0.0328982, recall 0.780662
2017-12-10T14:19:32.855811: step 1174, loss 4.05161, acc 0.625, prec 0.0328709, recall 0.779267
2017-12-10T14:19:33.043928: step 1175, loss 1.08926, acc 0.71875, prec 0.0328851, recall 0.779464
2017-12-10T14:19:33.232985: step 1176, loss 1.11158, acc 0.734375, prec 0.0329004, recall 0.779661
2017-12-10T14:19:33.424236: step 1177, loss 1.63192, acc 0.546875, prec 0.0329009, recall 0.779857
2017-12-10T14:19:33.609811: step 1178, loss 2.16476, acc 0.59375, prec 0.0328688, recall 0.779857
2017-12-10T14:19:33.793814: step 1179, loss 2.07542, acc 0.625, prec 0.0329118, recall 0.780249
2017-12-10T14:19:33.979922: step 1180, loss 1.14007, acc 0.625, prec 0.0328822, recall 0.780249
2017-12-10T14:19:34.166249: step 1181, loss 1.44778, acc 0.59375, prec 0.0329226, recall 0.780639
2017-12-10T14:19:34.348707: step 1182, loss 1.99851, acc 0.53125, prec 0.0328856, recall 0.780639
2017-12-10T14:19:34.538410: step 1183, loss 1.61611, acc 0.5625, prec 0.0328512, recall 0.780639
2017-12-10T14:19:34.720348: step 1184, loss 1.94, acc 0.59375, prec 0.0328193, recall 0.780639
2017-12-10T14:19:34.913448: step 1185, loss 1.67865, acc 0.609375, prec 0.0328248, recall 0.780834
2017-12-10T14:19:35.103921: step 1186, loss 0.966036, acc 0.796875, prec 0.0328089, recall 0.780834
2017-12-10T14:19:35.292650: step 1187, loss 1.03303, acc 0.703125, prec 0.0328217, recall 0.781028
2017-12-10T14:19:35.477339: step 1188, loss 1.12446, acc 0.625, prec 0.0328284, recall 0.781222
2017-12-10T14:19:35.663691: step 1189, loss 1.59407, acc 0.578125, prec 0.0327954, recall 0.781222
2017-12-10T14:19:35.852678: step 1190, loss 0.730325, acc 0.75, prec 0.0328119, recall 0.781416
2017-12-10T14:19:36.042998: step 1191, loss 0.723785, acc 0.796875, prec 0.0329038, recall 0.781995
2017-12-10T14:19:36.228896: step 1192, loss 0.591102, acc 0.84375, prec 0.0328916, recall 0.781995
2017-12-10T14:19:36.413139: step 1193, loss 7.06595, acc 0.796875, prec 0.0329499, recall 0.781003
2017-12-10T14:19:36.609170: step 1194, loss 2.0389, acc 0.796875, prec 0.0330057, recall 0.781387
2017-12-10T14:19:36.805890: step 1195, loss 0.943749, acc 0.703125, prec 0.0329825, recall 0.781387
2017-12-10T14:19:36.992786: step 1196, loss 0.803981, acc 0.75, prec 0.032963, recall 0.781387
2017-12-10T14:19:37.176254: step 1197, loss 2.69422, acc 0.84375, prec 0.0330236, recall 0.781086
2017-12-10T14:19:37.366347: step 1198, loss 4.73846, acc 0.71875, prec 0.0330028, recall 0.780402
2017-12-10T14:19:37.558792: step 1199, loss 0.775757, acc 0.796875, prec 0.0330227, recall 0.780594
2017-12-10T14:19:37.746733: step 1200, loss 1.4554, acc 0.65625, prec 0.0330316, recall 0.780786
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_2/1512933336/checkpoints/model-1200

2017-12-10T14:19:38.987737: step 1201, loss 2.19227, acc 0.59375, prec 0.0330713, recall 0.781168
2017-12-10T14:19:39.179366: step 1202, loss 1.39628, acc 0.65625, prec 0.0330801, recall 0.781359
2017-12-10T14:19:39.368300: step 1203, loss 1.38121, acc 0.578125, prec 0.0330828, recall 0.781549
2017-12-10T14:19:39.553949: step 1204, loss 1.70021, acc 0.578125, prec 0.0331567, recall 0.782118
2017-12-10T14:19:39.740451: step 1205, loss 2.20157, acc 0.546875, prec 0.0331569, recall 0.782307
2017-12-10T14:19:39.927944: step 1206, loss 3.04216, acc 0.546875, prec 0.0331571, recall 0.782496
2017-12-10T14:19:40.118246: step 1207, loss 1.80726, acc 0.46875, prec 0.0331867, recall 0.782872
2017-12-10T14:19:40.309697: step 1208, loss 2.08569, acc 0.546875, prec 0.0331514, recall 0.782872
2017-12-10T14:19:40.494810: step 1209, loss 1.8197, acc 0.5625, prec 0.0331174, recall 0.782872
2017-12-10T14:19:40.678811: step 1210, loss 1.38678, acc 0.65625, prec 0.0330908, recall 0.782872
2017-12-10T14:19:40.869234: step 1211, loss 1.6509, acc 0.625, prec 0.0330618, recall 0.782872
2017-12-10T14:19:41.060194: step 1212, loss 2.30177, acc 0.65625, prec 0.0330705, recall 0.78306
2017-12-10T14:19:41.249867: step 1213, loss 1.29972, acc 0.71875, prec 0.0330841, recall 0.783247
2017-12-10T14:19:41.440265: step 1214, loss 0.944835, acc 0.796875, prec 0.0330684, recall 0.783247
2017-12-10T14:19:41.627073: step 1215, loss 0.778429, acc 0.78125, prec 0.0330868, recall 0.783434
2017-12-10T14:19:41.813830: step 1216, loss 0.777065, acc 0.765625, prec 0.0330687, recall 0.783434
2017-12-10T14:19:42.000810: step 1217, loss 0.334272, acc 0.859375, prec 0.0330931, recall 0.783621
2017-12-10T14:19:42.187094: step 1218, loss 0.828805, acc 0.75, prec 0.0330738, recall 0.783621
2017-12-10T14:19:42.376810: step 1219, loss 0.475974, acc 0.84375, prec 0.0330618, recall 0.783621
2017-12-10T14:19:42.563705: step 1220, loss 0.533227, acc 0.890625, prec 0.0331237, recall 0.783993
2017-12-10T14:19:42.752962: step 1221, loss 0.402522, acc 0.90625, prec 0.0331867, recall 0.784364
2017-12-10T14:19:42.942453: step 1222, loss 0.478617, acc 0.890625, prec 0.0331783, recall 0.784364
2017-12-10T14:19:43.128653: step 1223, loss 3.33769, acc 0.90625, prec 0.0331723, recall 0.783691
2017-12-10T14:19:43.317485: step 1224, loss 7.97837, acc 0.921875, prec 0.0332026, recall 0.783205
2017-12-10T14:19:43.507863: step 1225, loss 0.350643, acc 0.921875, prec 0.0331965, recall 0.783205
2017-12-10T14:19:43.697464: step 1226, loss 5.16899, acc 0.90625, prec 0.0332256, recall 0.78272
2017-12-10T14:19:43.890702: step 1227, loss 1.68238, acc 0.875, prec 0.0332172, recall 0.782051
2017-12-10T14:19:44.089920: step 1228, loss 0.85142, acc 0.71875, prec 0.0332656, recall 0.782423
2017-12-10T14:19:44.277386: step 1229, loss 0.891847, acc 0.78125, prec 0.0332838, recall 0.782609
2017-12-10T14:19:44.466416: step 1230, loss 0.650151, acc 0.828125, prec 0.0333055, recall 0.782794
2017-12-10T14:19:44.652303: step 1231, loss 0.567362, acc 0.765625, prec 0.0332875, recall 0.782794
2017-12-10T14:19:44.836515: step 1232, loss 1.01175, acc 0.734375, prec 0.033267, recall 0.782794
2017-12-10T14:19:45.023649: step 1233, loss 1.54014, acc 0.65625, prec 0.0332405, recall 0.782794
2017-12-10T14:19:45.210935: step 1234, loss 4.70376, acc 0.625, prec 0.0332129, recall 0.782128
2017-12-10T14:19:45.398689: step 1235, loss 1.13182, acc 0.6875, prec 0.0331889, recall 0.782128
2017-12-10T14:19:45.581981: step 1236, loss 1.3621, acc 0.703125, prec 0.033201, recall 0.782313
2017-12-10T14:19:45.773415: step 1237, loss 2.11905, acc 0.5625, prec 0.0332024, recall 0.782498
2017-12-10T14:19:45.962575: step 1238, loss 1.08993, acc 0.71875, prec 0.0331808, recall 0.782498
2017-12-10T14:19:46.148470: step 1239, loss 1.88344, acc 0.59375, prec 0.0332194, recall 0.782867
2017-12-10T14:19:46.336747: step 1240, loss 1.61965, acc 0.640625, prec 0.0332614, recall 0.783235
2017-12-10T14:19:46.523143: step 1241, loss 1.01972, acc 0.671875, prec 0.0332363, recall 0.783235
2017-12-10T14:19:46.716640: step 1242, loss 1.4154, acc 0.5625, prec 0.0332029, recall 0.783235
2017-12-10T14:19:46.902215: step 1243, loss 1.15072, acc 0.75, prec 0.0332185, recall 0.783418
2017-12-10T14:19:47.090387: step 1244, loss 1.47295, acc 0.796875, prec 0.0332377, recall 0.783601
2017-12-10T14:19:47.280486: step 1245, loss 0.874532, acc 0.65625, prec 0.0332115, recall 0.783601
2017-12-10T14:19:47.468166: step 1246, loss 1.43871, acc 0.6875, prec 0.0332224, recall 0.783784
2017-12-10T14:19:47.651977: step 1247, loss 1.25693, acc 0.75, prec 0.0332725, recall 0.784148
2017-12-10T14:19:47.847798: step 1248, loss 0.891846, acc 0.671875, prec 0.0332475, recall 0.784148
2017-12-10T14:19:48.034235: step 1249, loss 0.540092, acc 0.84375, prec 0.0333047, recall 0.784512
2017-12-10T14:19:48.219121: step 1250, loss 0.892633, acc 0.796875, prec 0.0332893, recall 0.784512
2017-12-10T14:19:48.407668: step 1251, loss 1.50036, acc 0.90625, prec 0.0333857, recall 0.785055
2017-12-10T14:19:48.599137: step 1252, loss 0.492011, acc 0.875, prec 0.0333762, recall 0.785055
2017-12-10T14:19:48.790936: step 1253, loss 1.22819, acc 0.84375, prec 0.0333988, recall 0.785235
2017-12-10T14:19:48.979341: step 1254, loss 0.520369, acc 0.84375, prec 0.0334903, recall 0.785774
2017-12-10T14:19:49.169773: step 1255, loss 0.291381, acc 0.90625, prec 0.0334831, recall 0.785774
2017-12-10T14:19:49.355837: step 1256, loss 0.267159, acc 0.890625, prec 0.0334747, recall 0.785774
2017-12-10T14:19:49.545157: step 1257, loss 0.448102, acc 0.90625, prec 0.033502, recall 0.785953
2017-12-10T14:19:49.737115: step 1258, loss 0.291687, acc 0.84375, prec 0.0334901, recall 0.785953
2017-12-10T14:19:49.925820: step 1259, loss 0.243967, acc 0.890625, prec 0.0334817, recall 0.785953
2017-12-10T14:19:50.115974: step 1260, loss 3.20125, acc 0.890625, prec 0.0334746, recall 0.785297
2017-12-10T14:19:50.314883: step 1261, loss 1.96982, acc 0.890625, prec 0.0335019, recall 0.784821
2017-12-10T14:19:50.509606: step 1262, loss 0.164656, acc 0.875, prec 0.0334923, recall 0.784821
2017-12-10T14:19:50.703030: step 1263, loss 0.528606, acc 0.8125, prec 0.033478, recall 0.784821
2017-12-10T14:19:50.895431: step 1264, loss 8.9094, acc 0.78125, prec 0.0334625, recall 0.784167
2017-12-10T14:19:51.086913: step 1265, loss 0.576591, acc 0.796875, prec 0.0334814, recall 0.784346
2017-12-10T14:19:51.275847: step 1266, loss 0.360474, acc 0.875, prec 0.0335406, recall 0.784705
2017-12-10T14:19:51.463428: step 1267, loss 0.600171, acc 0.84375, prec 0.033563, recall 0.784884
2017-12-10T14:19:51.651886: step 1268, loss 0.971581, acc 0.75, prec 0.0335439, recall 0.784884
2017-12-10T14:19:51.840124: step 1269, loss 0.60614, acc 0.78125, prec 0.0335616, recall 0.785062
2017-12-10T14:19:52.027277: step 1270, loss 0.898141, acc 0.71875, prec 0.0335402, recall 0.785062
2017-12-10T14:19:52.216582: step 1271, loss 0.99368, acc 0.78125, prec 0.0335578, recall 0.78524
2017-12-10T14:19:52.405673: step 1272, loss 1.07997, acc 0.6875, prec 0.0336709, recall 0.78595
2017-12-10T14:19:52.596576: step 1273, loss 1.94792, acc 0.8125, prec 0.033725, recall 0.786304
2017-12-10T14:19:52.783339: step 1274, loss 1.29167, acc 0.6875, prec 0.0337353, recall 0.78648
2017-12-10T14:19:52.969426: step 1275, loss 0.743749, acc 0.734375, prec 0.033715, recall 0.78648
2017-12-10T14:19:53.156897: step 1276, loss 1.08202, acc 0.703125, prec 0.0336924, recall 0.78648
2017-12-10T14:19:53.352491: step 1277, loss 0.914101, acc 0.765625, prec 0.0337087, recall 0.786656
2017-12-10T14:19:53.541303: step 1278, loss 1.04564, acc 0.71875, prec 0.0336873, recall 0.786656
2017-12-10T14:19:53.729398: step 1279, loss 0.64838, acc 0.8125, prec 0.033673, recall 0.786656
2017-12-10T14:19:53.915531: step 1280, loss 0.981177, acc 0.796875, prec 0.0337257, recall 0.787007
2017-12-10T14:19:54.106379: step 1281, loss 0.529011, acc 0.796875, prec 0.0337102, recall 0.787007
2017-12-10T14:19:54.291831: step 1282, loss 3.02682, acc 0.796875, prec 0.033696, recall 0.78636
2017-12-10T14:19:54.483038: step 1283, loss 9.65622, acc 0.890625, prec 0.0337569, recall 0.786066
2017-12-10T14:19:54.677634: step 1284, loss 3.57278, acc 0.796875, prec 0.0338106, recall 0.785773
2017-12-10T14:19:54.867806: step 1285, loss 0.548545, acc 0.828125, prec 0.0337976, recall 0.785773
2017-12-10T14:19:55.055610: step 1286, loss 1.13351, acc 0.78125, prec 0.0338149, recall 0.785948
2017-12-10T14:19:55.242377: step 1287, loss 1.38668, acc 0.640625, prec 0.0337876, recall 0.785948
2017-12-10T14:19:55.429902: step 1288, loss 1.59566, acc 0.640625, prec 0.0337603, recall 0.785948
2017-12-10T14:19:55.614598: step 1289, loss 1.06515, acc 0.75, prec 0.0337414, recall 0.785948
2017-12-10T14:19:55.797652: step 1290, loss 1.52968, acc 0.671875, prec 0.0337504, recall 0.786122
2017-12-10T14:19:55.982377: step 1291, loss 1.10453, acc 0.71875, prec 0.0338645, recall 0.786819
2017-12-10T14:19:56.173051: step 1292, loss 1.48658, acc 0.53125, prec 0.0338289, recall 0.786819
2017-12-10T14:19:56.363940: step 1293, loss 1.28253, acc 0.671875, prec 0.0338716, recall 0.787165
2017-12-10T14:19:56.552607: step 1294, loss 0.747625, acc 0.71875, prec 0.0338503, recall 0.787165
2017-12-10T14:19:56.740037: step 1295, loss 6.02821, acc 0.765625, prec 0.0338675, recall 0.786699
2017-12-10T14:19:56.930801: step 1296, loss 1.00904, acc 0.75, prec 0.033916, recall 0.787045
2017-12-10T14:19:57.122917: step 1297, loss 0.83369, acc 0.78125, prec 0.0338995, recall 0.787045
2017-12-10T14:19:57.313584: step 1298, loss 1.18343, acc 0.671875, prec 0.0339083, recall 0.787217
2017-12-10T14:19:57.504465: step 1299, loss 1.30249, acc 0.609375, prec 0.0338788, recall 0.787217
2017-12-10T14:19:57.693708: step 1300, loss 0.982077, acc 0.625, prec 0.0338842, recall 0.787389
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_2/1512933336/checkpoints/model-1300

2017-12-10T14:19:58.784923: step 1301, loss 0.888962, acc 0.75, prec 0.0338653, recall 0.787389
2017-12-10T14:19:58.987148: step 1302, loss 0.784375, acc 0.734375, prec 0.0338789, recall 0.787561
2017-12-10T14:19:59.178594: step 1303, loss 0.925006, acc 0.6875, prec 0.0338553, recall 0.787561
2017-12-10T14:19:59.366668: step 1304, loss 0.895406, acc 0.734375, prec 0.0338354, recall 0.787561
2017-12-10T14:19:59.555816: step 1305, loss 0.868198, acc 0.75, prec 0.0338166, recall 0.787561
2017-12-10T14:19:59.745703: step 1306, loss 1.05076, acc 0.765625, prec 0.0338325, recall 0.787732
2017-12-10T14:19:59.931307: step 1307, loss 0.321193, acc 0.875, prec 0.0338231, recall 0.787732
2017-12-10T14:20:00.117008: step 1308, loss 0.46656, acc 0.828125, prec 0.0338772, recall 0.788074
2017-12-10T14:20:00.304128: step 1309, loss 0.249743, acc 0.9375, prec 0.0339394, recall 0.788415
2017-12-10T14:20:00.496507: step 1310, loss 0.56876, acc 0.90625, prec 0.0339323, recall 0.788415
2017-12-10T14:20:00.685382: step 1311, loss 4.25056, acc 0.90625, prec 0.0339265, recall 0.787781
2017-12-10T14:20:00.878830: step 1312, loss 0.15684, acc 0.921875, prec 0.0339206, recall 0.787781
2017-12-10T14:20:01.075523: step 1313, loss 2.03125, acc 0.90625, prec 0.0339147, recall 0.787149
2017-12-10T14:20:01.272676: step 1314, loss 0.218753, acc 0.921875, prec 0.0339423, recall 0.787319
2017-12-10T14:20:01.467024: step 1315, loss 0.30387, acc 0.84375, prec 0.0339305, recall 0.787319
2017-12-10T14:20:01.655340: step 1316, loss 0.520936, acc 0.859375, prec 0.0339534, recall 0.78749
2017-12-10T14:20:01.842823: step 1317, loss 0.726336, acc 0.734375, prec 0.0339334, recall 0.78749
2017-12-10T14:20:02.032451: step 1318, loss 0.497294, acc 0.84375, prec 0.0339551, recall 0.78766
2017-12-10T14:20:02.221064: step 1319, loss 0.440578, acc 0.8125, prec 0.033941, recall 0.78766
2017-12-10T14:20:02.406182: step 1320, loss 0.659169, acc 0.859375, prec 0.0339972, recall 0.788
2017-12-10T14:20:02.594413: step 1321, loss 0.435367, acc 0.84375, prec 0.0339854, recall 0.788
2017-12-10T14:20:02.784423: step 1322, loss 0.323742, acc 0.875, prec 0.0339761, recall 0.788
2017-12-10T14:20:02.970376: step 1323, loss 0.382363, acc 0.875, prec 0.0339667, recall 0.788
2017-12-10T14:20:03.159547: step 1324, loss 0.255489, acc 0.9375, prec 0.0340286, recall 0.788339
2017-12-10T14:20:03.348913: step 1325, loss 0.364984, acc 0.84375, prec 0.0340169, recall 0.788339
2017-12-10T14:20:03.534461: step 1326, loss 0.634181, acc 0.859375, prec 0.0340396, recall 0.788508
2017-12-10T14:20:03.725757: step 1327, loss 2.4817, acc 0.8125, prec 0.0340267, recall 0.787879
2017-12-10T14:20:03.913755: step 1328, loss 0.95025, acc 0.875, prec 0.0340506, recall 0.788048
2017-12-10T14:20:04.102809: step 1329, loss 12.1919, acc 0.859375, prec 0.0340424, recall 0.786794
2017-12-10T14:20:04.295701: step 1330, loss 0.219756, acc 0.921875, prec 0.0340698, recall 0.786963
2017-12-10T14:20:04.487970: step 1331, loss 0.712203, acc 0.828125, prec 0.0341566, recall 0.78747
2017-12-10T14:20:04.672276: step 1332, loss 0.617531, acc 0.828125, prec 0.0341769, recall 0.787639
2017-12-10T14:20:04.854716: step 1333, loss 0.728633, acc 0.78125, prec 0.0341936, recall 0.787807
2017-12-10T14:20:05.044572: step 1334, loss 1.18542, acc 0.703125, prec 0.0341713, recall 0.787807
2017-12-10T14:20:05.230136: step 1335, loss 1.69733, acc 0.796875, prec 0.0342224, recall 0.788142
2017-12-10T14:20:05.423096: step 1336, loss 0.958865, acc 0.75, prec 0.0342698, recall 0.788477
2017-12-10T14:20:05.605205: step 1337, loss 1.40174, acc 0.65625, prec 0.034244, recall 0.788477
2017-12-10T14:20:05.796335: step 1338, loss 0.851859, acc 0.703125, prec 0.0342217, recall 0.788477
2017-12-10T14:20:05.982213: step 1339, loss 1.0124, acc 0.75, prec 0.034203, recall 0.788477
2017-12-10T14:20:06.170385: step 1340, loss 0.886417, acc 0.75, prec 0.0342173, recall 0.788644
2017-12-10T14:20:06.355764: step 1341, loss 0.660855, acc 0.765625, prec 0.0342328, recall 0.78881
2017-12-10T14:20:06.544761: step 1342, loss 0.726629, acc 0.703125, prec 0.0342435, recall 0.788976
2017-12-10T14:20:06.735969: step 1343, loss 0.782675, acc 0.828125, prec 0.0342637, recall 0.789142
2017-12-10T14:20:06.921975: step 1344, loss 0.706785, acc 0.796875, prec 0.0342814, recall 0.789308
2017-12-10T14:20:07.111000: step 1345, loss 1.75951, acc 0.796875, prec 0.0342992, recall 0.789474
2017-12-10T14:20:07.302917: step 1346, loss 7.03536, acc 0.75, prec 0.0342816, recall 0.788854
2017-12-10T14:20:07.493439: step 1347, loss 0.789191, acc 0.78125, prec 0.0343311, recall 0.789185
2017-12-10T14:20:07.680274: step 1348, loss 0.545876, acc 0.796875, prec 0.0343488, recall 0.78935
2017-12-10T14:20:07.866707: step 1349, loss 1.26008, acc 0.6875, prec 0.0343583, recall 0.789515
2017-12-10T14:20:08.052775: step 1350, loss 1.11626, acc 0.6875, prec 0.0343349, recall 0.789515
2017-12-10T14:20:08.237569: step 1351, loss 1.35916, acc 0.859375, prec 0.0343901, recall 0.789844
2017-12-10T14:20:08.423075: step 1352, loss 1.35955, acc 0.703125, prec 0.0344335, recall 0.790172
2017-12-10T14:20:08.611474: step 1353, loss 2.44746, acc 0.734375, prec 0.0344148, recall 0.789556
2017-12-10T14:20:08.803256: step 1354, loss 1.14841, acc 0.640625, prec 0.0343879, recall 0.789556
2017-12-10T14:20:08.996010: step 1355, loss 2.07149, acc 0.703125, prec 0.0343985, recall 0.78972
2017-12-10T14:20:09.184757: step 1356, loss 1.22535, acc 0.609375, prec 0.0344021, recall 0.789883
2017-12-10T14:20:09.372710: step 1357, loss 1.07308, acc 0.71875, prec 0.0344138, recall 0.790047
2017-12-10T14:20:09.565676: step 1358, loss 1.65051, acc 0.578125, prec 0.0343824, recall 0.790047
2017-12-10T14:20:09.755082: step 1359, loss 1.28399, acc 0.59375, prec 0.0344175, recall 0.790373
2017-12-10T14:20:09.942763: step 1360, loss 1.36016, acc 0.6875, prec 0.0343942, recall 0.790373
2017-12-10T14:20:10.129216: step 1361, loss 0.874335, acc 0.828125, prec 0.0343814, recall 0.790373
2017-12-10T14:20:10.314569: step 1362, loss 0.856533, acc 0.734375, prec 0.0344595, recall 0.79086
2017-12-10T14:20:10.504129: step 1363, loss 0.732814, acc 0.796875, prec 0.0344444, recall 0.79086
2017-12-10T14:20:10.689237: step 1364, loss 0.638857, acc 0.796875, prec 0.0344618, recall 0.791022
2017-12-10T14:20:10.876790: step 1365, loss 0.475499, acc 0.84375, prec 0.0344828, recall 0.791183
2017-12-10T14:20:11.063362: step 1366, loss 0.322818, acc 0.90625, prec 0.0344758, recall 0.791183
2017-12-10T14:20:11.248336: step 1367, loss 0.410026, acc 0.890625, prec 0.0345002, recall 0.791345
2017-12-10T14:20:11.440662: step 1368, loss 1.31691, acc 0.8125, prec 0.0345188, recall 0.791506
2017-12-10T14:20:11.627784: step 1369, loss 0.545765, acc 0.859375, prec 0.0345408, recall 0.791667
2017-12-10T14:20:11.814135: step 1370, loss 0.81796, acc 0.875, prec 0.034564, recall 0.791827
2017-12-10T14:20:12.004616: step 1371, loss 3.26407, acc 0.921875, prec 0.0345918, recall 0.791378
2017-12-10T14:20:12.197694: step 1372, loss 0.54607, acc 0.875, prec 0.034615, recall 0.791538
2017-12-10T14:20:12.387066: step 1373, loss 0.631251, acc 0.8125, prec 0.0346335, recall 0.791699
2017-12-10T14:20:12.572778: step 1374, loss 2.73884, acc 0.828125, prec 0.0346867, recall 0.791411
2017-12-10T14:20:12.765517: step 1375, loss 0.551797, acc 0.8125, prec 0.0347052, recall 0.791571
2017-12-10T14:20:12.949946: step 1376, loss 0.69649, acc 0.828125, prec 0.0347572, recall 0.79189
2017-12-10T14:20:13.140601: step 1377, loss 0.534621, acc 0.796875, prec 0.034742, recall 0.79189
2017-12-10T14:20:13.328540: step 1378, loss 0.849741, acc 0.796875, prec 0.0347917, recall 0.792208
2017-12-10T14:20:13.516581: step 1379, loss 1.13772, acc 0.75, prec 0.034773, recall 0.792208
2017-12-10T14:20:13.705500: step 1380, loss 0.632005, acc 0.890625, prec 0.0347972, recall 0.792366
2017-12-10T14:20:13.900482: step 1381, loss 0.381682, acc 0.859375, prec 0.0348837, recall 0.792841
2017-12-10T14:20:14.098412: step 1382, loss 1.10226, acc 0.859375, prec 0.0349055, recall 0.792998
2017-12-10T14:20:14.286667: step 1383, loss 0.672374, acc 0.84375, prec 0.0349262, recall 0.793156
2017-12-10T14:20:14.473234: step 1384, loss 1.99235, acc 0.859375, prec 0.0349168, recall 0.792553
2017-12-10T14:20:14.660129: step 1385, loss 1.09413, acc 0.75, prec 0.034995, recall 0.793025
2017-12-10T14:20:14.846852: step 1386, loss 0.664231, acc 0.765625, prec 0.0349774, recall 0.793025
2017-12-10T14:20:15.034936: step 1387, loss 1.0573, acc 0.765625, prec 0.0349921, recall 0.793182
2017-12-10T14:20:15.220296: step 1388, loss 1.56957, acc 0.5625, prec 0.0349594, recall 0.793182
2017-12-10T14:20:15.405137: step 1389, loss 1.15405, acc 0.703125, prec 0.0349695, recall 0.793338
2017-12-10T14:20:15.596462: step 1390, loss 1.22951, acc 0.71875, prec 0.0349807, recall 0.793495
2017-12-10T14:20:15.782404: step 1391, loss 0.792596, acc 0.765625, prec 0.0349632, recall 0.793495
2017-12-10T14:20:15.965637: step 1392, loss 0.841364, acc 0.796875, prec 0.034948, recall 0.793495
2017-12-10T14:20:16.153664: step 1393, loss 1.09425, acc 0.71875, prec 0.0349592, recall 0.793651
2017-12-10T14:20:16.340310: step 1394, loss 1.0518, acc 0.734375, prec 0.0349716, recall 0.793807
2017-12-10T14:20:16.530662: step 1395, loss 0.612581, acc 0.78125, prec 0.0349553, recall 0.793807
2017-12-10T14:20:16.715620: step 1396, loss 0.390903, acc 0.859375, prec 0.0349769, recall 0.793962
2017-12-10T14:20:16.907998: step 1397, loss 0.427822, acc 0.890625, prec 0.0350008, recall 0.794118
2017-12-10T14:20:17.093083: step 1398, loss 0.295085, acc 0.9375, prec 0.0349962, recall 0.794118
2017-12-10T14:20:17.281554: step 1399, loss 0.485315, acc 0.84375, prec 0.0351128, recall 0.794737
2017-12-10T14:20:17.470672: step 1400, loss 0.712967, acc 0.828125, prec 0.035164, recall 0.795045
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_2/1512933336/checkpoints/model-1400

2017-12-10T14:20:20.705229: step 1401, loss 8.47494, acc 0.796875, prec 0.0352141, recall 0.794757
2017-12-10T14:20:20.899291: step 1402, loss 0.657134, acc 0.84375, prec 0.0352344, recall 0.79491
2017-12-10T14:20:21.089314: step 1403, loss 0.452827, acc 0.84375, prec 0.0352227, recall 0.79491
2017-12-10T14:20:21.278296: step 1404, loss 0.383939, acc 0.84375, prec 0.035243, recall 0.795064
2017-12-10T14:20:21.466653: step 1405, loss 0.44856, acc 0.875, prec 0.0352337, recall 0.795064
2017-12-10T14:20:21.650338: step 1406, loss 0.29951, acc 0.90625, prec 0.0352586, recall 0.795217
2017-12-10T14:20:21.838331: step 1407, loss 0.417242, acc 0.90625, prec 0.0352836, recall 0.79537
2017-12-10T14:20:22.027330: step 1408, loss 0.466014, acc 0.890625, prec 0.0353393, recall 0.795675
2017-12-10T14:20:22.216466: step 1409, loss 2.35748, acc 0.875, prec 0.0353631, recall 0.795235
2017-12-10T14:20:22.404942: step 1410, loss 0.655347, acc 0.828125, prec 0.0353502, recall 0.795235
2017-12-10T14:20:22.591254: step 1411, loss 0.303821, acc 0.890625, prec 0.0354058, recall 0.795539
2017-12-10T14:20:22.781929: step 1412, loss 0.5441, acc 0.765625, prec 0.0353883, recall 0.795539
2017-12-10T14:20:22.967732: step 1413, loss 0.38714, acc 0.84375, prec 0.0353766, recall 0.795539
2017-12-10T14:20:23.158191: step 1414, loss 0.379669, acc 0.875, prec 0.0353672, recall 0.795539
2017-12-10T14:20:23.350424: step 1415, loss 0.331244, acc 0.890625, prec 0.035359, recall 0.795539
2017-12-10T14:20:23.543609: step 1416, loss 0.575447, acc 0.875, prec 0.0353497, recall 0.795539
2017-12-10T14:20:23.732505: step 1417, loss 4.86276, acc 0.890625, prec 0.0353427, recall 0.794948
2017-12-10T14:20:23.921450: step 1418, loss 1.65896, acc 0.921875, prec 0.035338, recall 0.794358
2017-12-10T14:20:24.114682: step 1419, loss 0.327492, acc 0.90625, prec 0.0353629, recall 0.79451
2017-12-10T14:20:24.304988: step 1420, loss 0.279011, acc 0.90625, prec 0.0353559, recall 0.79451
2017-12-10T14:20:24.490796: step 1421, loss 0.693079, acc 0.859375, prec 0.035409, recall 0.794815
2017-12-10T14:20:24.679472: step 1422, loss 0.508402, acc 0.859375, prec 0.0354303, recall 0.794967
2017-12-10T14:20:24.864733: step 1423, loss 0.365303, acc 0.859375, prec 0.0354198, recall 0.794967
2017-12-10T14:20:25.053528: step 1424, loss 10.4906, acc 0.8125, prec 0.0354399, recall 0.793944
2017-12-10T14:20:25.243585: step 1425, loss 0.535246, acc 0.703125, prec 0.0354178, recall 0.793944
2017-12-10T14:20:25.428072: step 1426, loss 1.20112, acc 0.765625, prec 0.0354638, recall 0.794248
2017-12-10T14:20:25.617421: step 1427, loss 0.897091, acc 0.75, prec 0.0354451, recall 0.794248
2017-12-10T14:20:25.804378: step 1428, loss 0.824033, acc 0.75, prec 0.0355216, recall 0.794702
2017-12-10T14:20:25.995217: step 1429, loss 0.941201, acc 0.6875, prec 0.0354983, recall 0.794702
2017-12-10T14:20:26.181676: step 1430, loss 0.938634, acc 0.703125, prec 0.0355395, recall 0.795004
2017-12-10T14:20:26.369405: step 1431, loss 1.1228, acc 0.6875, prec 0.0355162, recall 0.795004
2017-12-10T14:20:26.554313: step 1432, loss 0.653273, acc 0.828125, prec 0.0355666, recall 0.795304
2017-12-10T14:20:26.740030: step 1433, loss 1.04941, acc 0.765625, prec 0.0355491, recall 0.795304
2017-12-10T14:20:26.925964: step 1434, loss 1.93574, acc 0.625, prec 0.0355844, recall 0.795604
2017-12-10T14:20:27.116691: step 1435, loss 0.618881, acc 0.828125, prec 0.0356663, recall 0.796053
2017-12-10T14:20:27.305818: step 1436, loss 1.22485, acc 0.734375, prec 0.035678, recall 0.796202
2017-12-10T14:20:27.492980: step 1437, loss 1.21354, acc 0.640625, prec 0.0356512, recall 0.796202
2017-12-10T14:20:27.676183: step 1438, loss 1.50626, acc 0.640625, prec 0.0356244, recall 0.796202
2017-12-10T14:20:27.861028: step 1439, loss 1.05912, acc 0.75, prec 0.0356688, recall 0.796499
2017-12-10T14:20:28.045042: step 1440, loss 0.971984, acc 0.734375, prec 0.035649, recall 0.796499
2017-12-10T14:20:28.229779: step 1441, loss 1.2503, acc 0.8125, prec 0.0356665, recall 0.796647
2017-12-10T14:20:28.416467: step 1442, loss 1.01292, acc 0.8125, prec 0.035684, recall 0.796795
2017-12-10T14:20:28.604241: step 1443, loss 0.744228, acc 0.828125, prec 0.035797, recall 0.797386
2017-12-10T14:20:28.791258: step 1444, loss 0.505013, acc 0.84375, prec 0.0357853, recall 0.797386
2017-12-10T14:20:28.987456: step 1445, loss 0.625191, acc 0.828125, prec 0.0357725, recall 0.797386
2017-12-10T14:20:29.175606: step 1446, loss 1.03769, acc 0.734375, prec 0.0357527, recall 0.797386
2017-12-10T14:20:29.362254: step 1447, loss 4.21631, acc 0.75, prec 0.0357352, recall 0.796807
2017-12-10T14:20:29.555949: step 1448, loss 0.509065, acc 0.890625, prec 0.0357584, recall 0.796954
2017-12-10T14:20:29.745267: step 1449, loss 0.191181, acc 0.921875, prec 0.0357526, recall 0.796954
2017-12-10T14:20:29.934634: step 1450, loss 0.564962, acc 0.796875, prec 0.0357689, recall 0.797101
2017-12-10T14:20:30.125887: step 1451, loss 0.413263, acc 0.8125, prec 0.0357863, recall 0.797248
2017-12-10T14:20:30.313197: step 1452, loss 0.350473, acc 0.875, prec 0.0358083, recall 0.797395
2017-12-10T14:20:30.496788: step 1453, loss 0.650175, acc 0.84375, prec 0.0358593, recall 0.797688
2017-12-10T14:20:30.685877: step 1454, loss 0.843335, acc 0.765625, prec 0.0358418, recall 0.797688
2017-12-10T14:20:30.871797: step 1455, loss 3.60062, acc 0.859375, prec 0.0358325, recall 0.797112
2017-12-10T14:20:31.071888: step 1456, loss 0.209405, acc 0.90625, prec 0.0358881, recall 0.797404
2017-12-10T14:20:31.267851: step 1457, loss 0.546823, acc 0.859375, prec 0.0358776, recall 0.797404
2017-12-10T14:20:31.458658: step 1458, loss 11.8479, acc 0.84375, prec 0.0358672, recall 0.79683
2017-12-10T14:20:31.648603: step 1459, loss 0.384984, acc 0.859375, prec 0.0358567, recall 0.79683
2017-12-10T14:20:31.835957: step 1460, loss 0.643128, acc 0.828125, prec 0.0359064, recall 0.797122
2017-12-10T14:20:32.021482: step 1461, loss 0.393258, acc 0.890625, prec 0.0359295, recall 0.797268
2017-12-10T14:20:32.208211: step 1462, loss 0.6239, acc 0.78125, prec 0.0359132, recall 0.797268
2017-12-10T14:20:32.398623: step 1463, loss 0.657041, acc 0.796875, prec 0.0359293, recall 0.797414
2017-12-10T14:20:32.586244: step 1464, loss 0.545343, acc 0.84375, prec 0.0359177, recall 0.797414
2017-12-10T14:20:32.771049: step 1465, loss 1.33528, acc 0.75, prec 0.0359615, recall 0.797704
2017-12-10T14:20:32.962627: step 1466, loss 0.5941, acc 0.78125, prec 0.0359452, recall 0.797704
2017-12-10T14:20:33.149064: step 1467, loss 0.909972, acc 0.796875, prec 0.0359301, recall 0.797704
2017-12-10T14:20:33.335920: step 1468, loss 0.537223, acc 0.796875, prec 0.0359461, recall 0.797849
2017-12-10T14:20:33.521063: step 1469, loss 0.456061, acc 0.8125, prec 0.0359322, recall 0.797849
2017-12-10T14:20:33.710652: step 1470, loss 0.589848, acc 0.84375, prec 0.0359206, recall 0.797849
2017-12-10T14:20:33.895260: step 1471, loss 0.812044, acc 0.71875, prec 0.0358998, recall 0.797849
2017-12-10T14:20:34.085495: step 1472, loss 2.06536, acc 0.828125, prec 0.0358882, recall 0.797278
2017-12-10T14:20:34.272089: step 1473, loss 0.663126, acc 0.8125, prec 0.0358743, recall 0.797278
2017-12-10T14:20:34.460251: step 1474, loss 0.442908, acc 0.90625, prec 0.0358984, recall 0.797423
2017-12-10T14:20:34.645992: step 1475, loss 0.40283, acc 0.84375, prec 0.0358869, recall 0.797423
2017-12-10T14:20:34.831664: step 1476, loss 0.409411, acc 0.890625, prec 0.0359098, recall 0.797568
2017-12-10T14:20:35.019991: step 1477, loss 0.467782, acc 0.828125, prec 0.0358971, recall 0.797568
2017-12-10T14:20:35.204196: step 1478, loss 0.584403, acc 0.8125, prec 0.0358832, recall 0.797568
2017-12-10T14:20:35.391059: step 1479, loss 0.433195, acc 0.90625, prec 0.0358763, recall 0.797568
2017-12-10T14:20:35.581086: step 1480, loss 1.24828, acc 0.859375, prec 0.035928, recall 0.797857
2017-12-10T14:20:35.767550: step 1481, loss 0.377398, acc 0.859375, prec 0.0359486, recall 0.798001
2017-12-10T14:20:35.953426: step 1482, loss 0.42127, acc 0.859375, prec 0.0359691, recall 0.798146
2017-12-10T14:20:36.138749: step 1483, loss 0.43643, acc 0.890625, prec 0.035992, recall 0.798289
2017-12-10T14:20:36.330427: step 1484, loss 1.41368, acc 0.875, prec 0.0360137, recall 0.798433
2017-12-10T14:20:36.518072: step 1485, loss 0.751978, acc 0.8125, prec 0.0360308, recall 0.798577
2017-12-10T14:20:36.707065: step 1486, loss 0.294749, acc 0.890625, prec 0.0360227, recall 0.798577
2017-12-10T14:20:36.895041: step 1487, loss 0.34275, acc 0.890625, prec 0.0360456, recall 0.79872
2017-12-10T14:20:37.081518: step 1488, loss 5.02027, acc 0.875, prec 0.0360684, recall 0.798295
2017-12-10T14:20:37.276762: step 1489, loss 0.508548, acc 0.8125, prec 0.0360854, recall 0.798439
2017-12-10T14:20:37.465772: step 1490, loss 0.363093, acc 0.84375, prec 0.0360739, recall 0.798439
2017-12-10T14:20:37.639901: step 1491, loss 2.34589, acc 0.846154, prec 0.0360658, recall 0.797872
2017-12-10T14:20:37.834058: step 1492, loss 0.86778, acc 0.8125, prec 0.0360828, recall 0.798016
2017-12-10T14:20:38.024974: step 1493, loss 0.374114, acc 0.859375, prec 0.0360724, recall 0.798016
2017-12-10T14:20:38.212434: step 1494, loss 0.876606, acc 0.734375, prec 0.0360528, recall 0.798016
2017-12-10T14:20:38.397203: step 1495, loss 0.581891, acc 0.78125, prec 0.0360675, recall 0.798159
2017-12-10T14:20:38.584244: step 1496, loss 0.544876, acc 0.84375, prec 0.0360868, recall 0.798301
2017-12-10T14:20:38.767587: step 1497, loss 0.578383, acc 0.765625, prec 0.0361003, recall 0.798444
2017-12-10T14:20:38.953430: step 1498, loss 0.812842, acc 0.734375, prec 0.0361115, recall 0.798587
2017-12-10T14:20:39.135726: step 1499, loss 0.462077, acc 0.859375, prec 0.0361011, recall 0.798587
2017-12-10T14:20:39.323227: step 1500, loss 0.698966, acc 0.78125, prec 0.0360849, recall 0.798587
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_2/1512933336/checkpoints/model-1500

2017-12-10T14:20:40.377964: step 1501, loss 0.841614, acc 0.734375, prec 0.0360654, recall 0.798587
2017-12-10T14:20:40.569600: step 1502, loss 0.838774, acc 0.75, prec 0.0360777, recall 0.798729
2017-12-10T14:20:40.758353: step 1503, loss 0.95354, acc 0.78125, prec 0.0360923, recall 0.798871
2017-12-10T14:20:40.945461: step 1504, loss 0.358353, acc 0.921875, prec 0.0361788, recall 0.799296
2017-12-10T14:20:41.132752: step 1505, loss 0.274734, acc 0.875, prec 0.0362002, recall 0.799437
2017-12-10T14:20:41.318521: step 1506, loss 0.483807, acc 0.8125, prec 0.0361864, recall 0.799437
2017-12-10T14:20:41.507395: step 1507, loss 0.822178, acc 0.890625, prec 0.0362704, recall 0.79986
2017-12-10T14:20:41.695583: step 1508, loss 0.521869, acc 0.84375, prec 0.0362896, recall 0.8
2017-12-10T14:20:41.883594: step 1509, loss 0.210762, acc 0.921875, prec 0.0362838, recall 0.8
2017-12-10T14:20:42.073035: step 1510, loss 0.369992, acc 0.890625, prec 0.0362757, recall 0.8
2017-12-10T14:20:42.262864: step 1511, loss 0.535479, acc 0.78125, prec 0.0362595, recall 0.8
2017-12-10T14:20:42.449054: step 1512, loss 0.238265, acc 0.953125, prec 0.0363174, recall 0.80028
2017-12-10T14:20:42.640054: step 1513, loss 0.21556, acc 0.9375, prec 0.0363434, recall 0.80042
2017-12-10T14:20:42.828172: step 1514, loss 1.03918, acc 0.875, prec 0.0363954, recall 0.800699
2017-12-10T14:20:43.023959: step 1515, loss 2.39018, acc 0.90625, prec 0.0363896, recall 0.80014
2017-12-10T14:20:43.217480: step 1516, loss 0.465542, acc 0.859375, prec 0.0364098, recall 0.800279
2017-12-10T14:20:43.406431: step 1517, loss 0.418054, acc 0.875, prec 0.0364618, recall 0.800558
2017-12-10T14:20:43.590851: step 1518, loss 0.292055, acc 0.90625, prec 0.0364855, recall 0.800697
2017-12-10T14:20:43.779951: step 1519, loss 3.6248, acc 0.9375, prec 0.036482, recall 0.800139
2017-12-10T14:20:43.975568: step 1520, loss 0.315327, acc 0.9375, prec 0.0365385, recall 0.800417
2017-12-10T14:20:44.169418: step 1521, loss 0.689832, acc 0.828125, prec 0.0365563, recall 0.800556
2017-12-10T14:20:44.356520: step 1522, loss 0.414058, acc 0.921875, prec 0.0365811, recall 0.800694
2017-12-10T14:20:44.543617: step 1523, loss 0.411285, acc 0.890625, prec 0.0366341, recall 0.800971
2017-12-10T14:20:44.730762: step 1524, loss 0.359309, acc 0.890625, prec 0.0366565, recall 0.801109
2017-12-10T14:20:44.919610: step 1525, loss 0.351663, acc 0.828125, prec 0.0366437, recall 0.801109
2017-12-10T14:20:45.106303: step 1526, loss 0.934698, acc 0.765625, prec 0.0366568, recall 0.801247
2017-12-10T14:20:45.290882: step 1527, loss 0.473615, acc 0.890625, prec 0.0367403, recall 0.801659
2017-12-10T14:20:45.484038: step 1528, loss 0.344644, acc 0.875, prec 0.0367309, recall 0.801659
2017-12-10T14:20:45.670500: step 1529, loss 0.556458, acc 0.84375, prec 0.0368108, recall 0.802069
2017-12-10T14:20:45.861342: step 1530, loss 0.261255, acc 0.890625, prec 0.0368331, recall 0.802205
2017-12-10T14:20:46.045994: step 1531, loss 0.373974, acc 0.828125, prec 0.0368203, recall 0.802205
2017-12-10T14:20:46.237141: step 1532, loss 0.345256, acc 0.875, prec 0.0368719, recall 0.802478
2017-12-10T14:20:46.428743: step 1533, loss 0.562369, acc 0.8125, prec 0.0368884, recall 0.802613
2017-12-10T14:20:46.614399: step 1534, loss 0.33907, acc 0.84375, prec 0.0369376, recall 0.802885
2017-12-10T14:20:46.804594: step 1535, loss 0.306678, acc 0.875, prec 0.0369891, recall 0.803155
2017-12-10T14:20:46.998934: step 1536, loss 0.370273, acc 0.859375, prec 0.0369786, recall 0.803155
2017-12-10T14:20:47.187656: step 1537, loss 2.89862, acc 0.875, prec 0.0370008, recall 0.80274
2017-12-10T14:20:47.382298: step 1538, loss 0.795453, acc 0.90625, prec 0.0370546, recall 0.80301
2017-12-10T14:20:47.569563: step 1539, loss 0.44603, acc 0.875, prec 0.037106, recall 0.803279
2017-12-10T14:20:47.758444: step 1540, loss 0.333465, acc 0.921875, prec 0.0371001, recall 0.803279
2017-12-10T14:20:47.947858: step 1541, loss 0.189361, acc 0.9375, prec 0.0370954, recall 0.803279
2017-12-10T14:20:48.131598: step 1542, loss 0.341857, acc 0.875, prec 0.0370861, recall 0.803279
2017-12-10T14:20:48.322051: step 1543, loss 0.688902, acc 0.78125, prec 0.0370697, recall 0.803279
2017-12-10T14:20:48.509042: step 1544, loss 0.419394, acc 0.890625, prec 0.0371222, recall 0.803547
2017-12-10T14:20:48.694961: step 1545, loss 0.429886, acc 0.921875, prec 0.0372074, recall 0.803948
2017-12-10T14:20:48.882295: step 1546, loss 2.8329, acc 0.84375, prec 0.0372272, recall 0.803535
2017-12-10T14:20:49.072785: step 1547, loss 0.258206, acc 0.859375, prec 0.0372469, recall 0.803668
2017-12-10T14:20:49.263483: step 1548, loss 0.475336, acc 0.859375, prec 0.0372364, recall 0.803668
2017-12-10T14:20:49.450197: step 1549, loss 0.580307, acc 0.8125, prec 0.0373435, recall 0.804201
2017-12-10T14:20:49.637671: step 1550, loss 0.631755, acc 0.84375, prec 0.0373317, recall 0.804201
2017-12-10T14:20:49.823179: step 1551, loss 0.450092, acc 0.90625, prec 0.0373247, recall 0.804201
2017-12-10T14:20:50.006638: step 1552, loss 3.35342, acc 0.78125, prec 0.0373094, recall 0.803656
2017-12-10T14:20:50.193873: step 1553, loss 0.587241, acc 0.859375, prec 0.0372989, recall 0.803656
2017-12-10T14:20:50.381763: step 1554, loss 0.512746, acc 0.796875, prec 0.0372837, recall 0.803656
2017-12-10T14:20:50.567518: step 1555, loss 0.421926, acc 0.828125, prec 0.037301, recall 0.803789
2017-12-10T14:20:50.756514: step 1556, loss 0.751158, acc 0.8125, prec 0.0373776, recall 0.804186
2017-12-10T14:20:50.943914: step 1557, loss 1.13248, acc 0.84375, prec 0.0373961, recall 0.804318
2017-12-10T14:20:51.131871: step 1558, loss 0.492256, acc 0.828125, prec 0.0374134, recall 0.80445
2017-12-10T14:20:51.323016: step 1559, loss 0.33064, acc 0.84375, prec 0.0374016, recall 0.80445
2017-12-10T14:20:51.510351: step 1560, loss 0.589836, acc 0.859375, prec 0.0373911, recall 0.80445
2017-12-10T14:20:51.698983: step 1561, loss 0.486613, acc 0.828125, prec 0.0373782, recall 0.80445
2017-12-10T14:20:51.885780: step 1562, loss 0.38946, acc 0.875, prec 0.0373688, recall 0.80445
2017-12-10T14:20:52.071221: step 1563, loss 0.627197, acc 0.828125, prec 0.0374162, recall 0.804714
2017-12-10T14:20:52.272662: step 1564, loss 0.368909, acc 0.890625, prec 0.037408, recall 0.804714
2017-12-10T14:20:52.457518: step 1565, loss 0.462522, acc 0.875, prec 0.0374288, recall 0.804845
2017-12-10T14:20:52.644592: step 1566, loss 4.10248, acc 0.8125, prec 0.0374761, recall 0.804567
2017-12-10T14:20:52.837816: step 1567, loss 0.670726, acc 0.78125, prec 0.0374898, recall 0.804698
2017-12-10T14:20:53.026191: step 1568, loss 0.966507, acc 0.90625, prec 0.037543, recall 0.80496
2017-12-10T14:20:53.216832: step 1569, loss 0.580088, acc 0.828125, prec 0.0375301, recall 0.80496
2017-12-10T14:20:53.402225: step 1570, loss 0.369722, acc 0.8125, prec 0.037516, recall 0.80496
2017-12-10T14:20:53.595077: step 1571, loss 0.398953, acc 0.828125, prec 0.0375332, recall 0.80509
2017-12-10T14:20:53.780046: step 1572, loss 0.639848, acc 0.8125, prec 0.0375792, recall 0.805351
2017-12-10T14:20:53.967710: step 1573, loss 0.837225, acc 0.765625, prec 0.0375916, recall 0.805481
2017-12-10T14:20:54.157577: step 1574, loss 1.33901, acc 0.796875, prec 0.0376064, recall 0.805611
2017-12-10T14:20:54.350281: step 1575, loss 0.714606, acc 0.828125, prec 0.0376235, recall 0.805741
2017-12-10T14:20:54.538678: step 1576, loss 0.463908, acc 0.859375, prec 0.037613, recall 0.805741
2017-12-10T14:20:54.728100: step 1577, loss 0.73012, acc 0.828125, prec 0.0376301, recall 0.805871
2017-12-10T14:20:54.921637: step 1578, loss 0.720445, acc 0.828125, prec 0.0376471, recall 0.806
2017-12-10T14:20:55.109342: step 1579, loss 0.56402, acc 0.828125, prec 0.0376642, recall 0.806129
2017-12-10T14:20:55.303614: step 1580, loss 0.449557, acc 0.8125, prec 0.0376501, recall 0.806129
2017-12-10T14:20:55.491729: step 1581, loss 0.499328, acc 0.859375, prec 0.0376995, recall 0.806387
2017-12-10T14:20:55.679350: step 1582, loss 0.56542, acc 0.828125, prec 0.0376866, recall 0.806387
2017-12-10T14:20:55.868476: step 1583, loss 0.443086, acc 0.859375, prec 0.0377059, recall 0.806516
2017-12-10T14:20:56.053087: step 1584, loss 0.317005, acc 0.84375, prec 0.0377241, recall 0.806644
2017-12-10T14:20:56.238252: step 1585, loss 0.550193, acc 0.796875, prec 0.0377388, recall 0.806773
2017-12-10T14:20:56.428560: step 1586, loss 0.366545, acc 0.875, prec 0.0377294, recall 0.806773
2017-12-10T14:20:56.612836: step 1587, loss 0.277165, acc 0.875, prec 0.03772, recall 0.806773
2017-12-10T14:20:56.798666: step 1588, loss 0.419855, acc 0.859375, prec 0.0377394, recall 0.806901
2017-12-10T14:20:56.985599: step 1589, loss 0.472205, acc 0.875, prec 0.0377599, recall 0.807029
2017-12-10T14:20:57.172609: step 1590, loss 0.359761, acc 0.890625, prec 0.0377517, recall 0.807029
2017-12-10T14:20:57.362645: step 1591, loss 1.48674, acc 0.921875, prec 0.0378353, recall 0.807412
2017-12-10T14:20:57.556566: step 1592, loss 0.216038, acc 0.890625, prec 0.0378569, recall 0.80754
2017-12-10T14:20:57.742760: step 1593, loss 0.435457, acc 0.890625, prec 0.0378487, recall 0.80754
2017-12-10T14:20:57.932168: step 1594, loss 0.176454, acc 0.921875, prec 0.0378429, recall 0.80754
2017-12-10T14:20:58.120062: step 1595, loss 0.409804, acc 0.8125, prec 0.0378586, recall 0.807667
2017-12-10T14:20:58.306484: step 1596, loss 1.07177, acc 0.921875, prec 0.0379123, recall 0.807921
2017-12-10T14:20:58.496200: step 1597, loss 0.239276, acc 0.890625, prec 0.0379041, recall 0.807921
2017-12-10T14:20:58.687340: step 1598, loss 5.95915, acc 0.90625, prec 0.0379876, recall 0.807768
2017-12-10T14:20:58.883990: step 1599, loss 0.209401, acc 0.90625, prec 0.0379806, recall 0.807768
2017-12-10T14:20:59.079260: step 1600, loss 0.447236, acc 0.859375, prec 0.03797, recall 0.807768
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_2/1512933336/checkpoints/model-1600

2017-12-10T14:21:00.174757: step 1601, loss 0.608659, acc 0.8125, prec 0.0379856, recall 0.807895
2017-12-10T14:21:00.362555: step 1602, loss 0.107071, acc 0.9375, prec 0.0379809, recall 0.807895
2017-12-10T14:21:00.549578: step 1603, loss 1.63353, acc 0.8125, prec 0.0380275, recall 0.807617
2017-12-10T14:21:00.738751: step 1604, loss 0.270105, acc 0.90625, prec 0.0380502, recall 0.807743
2017-12-10T14:21:00.930902: step 1605, loss 0.447933, acc 0.859375, prec 0.0380396, recall 0.807743
2017-12-10T14:21:01.121972: step 1606, loss 0.207175, acc 0.921875, prec 0.0381526, recall 0.808246
2017-12-10T14:21:01.310671: step 1607, loss 0.763707, acc 0.84375, prec 0.0381705, recall 0.808371
2017-12-10T14:21:01.499749: step 1608, loss 0.46694, acc 0.84375, prec 0.0381884, recall 0.808497
2017-12-10T14:21:01.687601: step 1609, loss 0.63997, acc 0.78125, prec 0.0382016, recall 0.808622
2017-12-10T14:21:01.875502: step 1610, loss 0.716105, acc 0.828125, prec 0.0382777, recall 0.808996
2017-12-10T14:21:02.074400: step 1611, loss 0.499561, acc 0.859375, prec 0.038267, recall 0.808996
2017-12-10T14:21:02.261499: step 1612, loss 0.544964, acc 0.828125, prec 0.0383133, recall 0.809245
2017-12-10T14:21:02.449746: step 1613, loss 0.974664, acc 0.875, prec 0.0383632, recall 0.809493
2017-12-10T14:21:02.639587: step 1614, loss 0.579772, acc 0.796875, prec 0.0383478, recall 0.809493
2017-12-10T14:21:02.830556: step 1615, loss 0.911029, acc 0.734375, prec 0.0383277, recall 0.809493
2017-12-10T14:21:03.017352: step 1616, loss 0.503741, acc 0.828125, prec 0.0383148, recall 0.809493
2017-12-10T14:21:03.204189: step 1617, loss 0.395797, acc 0.859375, prec 0.0383042, recall 0.809493
2017-12-10T14:21:03.390058: step 1618, loss 0.166354, acc 0.921875, prec 0.0382983, recall 0.809493
2017-12-10T14:21:03.574062: step 1619, loss 0.554836, acc 0.796875, prec 0.0383421, recall 0.80974
2017-12-10T14:21:03.761430: step 1620, loss 1.19439, acc 0.8125, prec 0.0383871, recall 0.809987
2017-12-10T14:21:03.947546: step 1621, loss 0.351551, acc 0.859375, prec 0.0384355, recall 0.810233
2017-12-10T14:21:04.138355: step 1622, loss 0.55606, acc 0.875, prec 0.0384556, recall 0.810356
2017-12-10T14:21:04.327361: step 1623, loss 0.213246, acc 0.9375, prec 0.0384509, recall 0.810356
2017-12-10T14:21:04.518359: step 1624, loss 0.396131, acc 0.875, prec 0.038471, recall 0.810479
2017-12-10T14:21:04.708029: step 1625, loss 0.160048, acc 0.9375, prec 0.0384663, recall 0.810479
2017-12-10T14:21:04.898459: step 1626, loss 0.370185, acc 0.90625, prec 0.0384887, recall 0.810601
2017-12-10T14:21:05.087856: step 1627, loss 1.06555, acc 0.921875, prec 0.0385713, recall 0.810968
2017-12-10T14:21:05.278345: step 1628, loss 0.6991, acc 0.875, prec 0.0386208, recall 0.811211
2017-12-10T14:21:05.470609: step 1629, loss 0.296728, acc 0.90625, prec 0.0386137, recall 0.811211
2017-12-10T14:21:05.658212: step 1630, loss 6.26838, acc 0.875, prec 0.0386066, recall 0.810167
2017-12-10T14:21:05.849513: step 1631, loss 0.351735, acc 0.90625, prec 0.0385995, recall 0.810167
2017-12-10T14:21:06.041650: step 1632, loss 0.498981, acc 0.828125, prec 0.038616, recall 0.810289
2017-12-10T14:21:06.229408: step 1633, loss 0.33819, acc 0.890625, prec 0.0386077, recall 0.810289
2017-12-10T14:21:06.416445: step 1634, loss 0.618957, acc 0.828125, prec 0.0386241, recall 0.810411
2017-12-10T14:21:06.599801: step 1635, loss 0.632429, acc 0.859375, prec 0.0386429, recall 0.810533
2017-12-10T14:21:06.790821: step 1636, loss 0.691786, acc 0.796875, prec 0.0386275, recall 0.810533
2017-12-10T14:21:06.976734: step 1637, loss 0.850029, acc 0.75, prec 0.0386968, recall 0.810897
2017-12-10T14:21:07.165469: step 1638, loss 0.874727, acc 0.796875, prec 0.0387403, recall 0.81114
2017-12-10T14:21:07.354400: step 1639, loss 0.966809, acc 0.75, prec 0.0387801, recall 0.811381
2017-12-10T14:21:07.545239: step 1640, loss 1.04977, acc 0.71875, prec 0.0387881, recall 0.811502
2017-12-10T14:21:07.727956: step 1641, loss 0.665144, acc 0.75, prec 0.0387985, recall 0.811622
2017-12-10T14:21:07.915984: step 1642, loss 1.14606, acc 0.703125, prec 0.0388347, recall 0.811862
2017-12-10T14:21:08.103777: step 1643, loss 0.751731, acc 0.75, prec 0.0388157, recall 0.811862
2017-12-10T14:21:08.288433: step 1644, loss 0.788587, acc 0.765625, prec 0.038798, recall 0.811862
2017-12-10T14:21:08.471321: step 1645, loss 0.769226, acc 0.75, prec 0.0387791, recall 0.811862
2017-12-10T14:21:08.662529: step 1646, loss 0.794255, acc 0.78125, prec 0.0387625, recall 0.811862
2017-12-10T14:21:08.851303: step 1647, loss 0.706928, acc 0.78125, prec 0.038746, recall 0.811862
2017-12-10T14:21:09.038428: step 1648, loss 0.697227, acc 0.734375, prec 0.038726, recall 0.811862
2017-12-10T14:21:09.229629: step 1649, loss 0.590341, acc 0.796875, prec 0.0387399, recall 0.811982
2017-12-10T14:21:09.417784: step 1650, loss 1.27474, acc 0.890625, prec 0.0387901, recall 0.812222
2017-12-10T14:21:09.609441: step 1651, loss 1.14249, acc 0.875, prec 0.0388391, recall 0.81246
2017-12-10T14:21:09.801261: step 1652, loss 0.459651, acc 0.875, prec 0.0388296, recall 0.81246
2017-12-10T14:21:09.984790: step 1653, loss 0.488192, acc 0.890625, prec 0.0388506, recall 0.812579
2017-12-10T14:21:10.172688: step 1654, loss 0.320186, acc 0.890625, prec 0.0388423, recall 0.812579
2017-12-10T14:21:10.360778: step 1655, loss 0.151058, acc 0.890625, prec 0.0388341, recall 0.812579
2017-12-10T14:21:10.551388: step 1656, loss 0.145896, acc 0.953125, prec 0.0388305, recall 0.812579
2017-12-10T14:21:10.735641: step 1657, loss 0.411415, acc 0.890625, prec 0.0388515, recall 0.812698
2017-12-10T14:21:10.925889: step 1658, loss 4.57245, acc 0.859375, prec 0.0388712, recall 0.812302
2017-12-10T14:21:11.115016: step 1659, loss 0.38719, acc 0.890625, prec 0.0388921, recall 0.812421
2017-12-10T14:21:11.299995: step 1660, loss 0.232082, acc 0.953125, prec 0.0389469, recall 0.812658
2017-12-10T14:21:11.486389: step 1661, loss 0.355968, acc 0.875, prec 0.0389666, recall 0.812777
2017-12-10T14:21:11.674508: step 1662, loss 0.351415, acc 0.859375, prec 0.0389559, recall 0.812777
2017-12-10T14:21:11.862134: step 1663, loss 0.35382, acc 0.875, prec 0.0389465, recall 0.812777
2017-12-10T14:21:12.047765: step 1664, loss 0.47474, acc 0.890625, prec 0.0389673, recall 0.812895
2017-12-10T14:21:12.233369: step 1665, loss 0.568934, acc 0.84375, prec 0.0389555, recall 0.812895
2017-12-10T14:21:12.420456: step 1666, loss 0.552912, acc 0.84375, prec 0.0389437, recall 0.812895
2017-12-10T14:21:12.605081: step 1667, loss 0.634684, acc 0.78125, prec 0.0389272, recall 0.812895
2017-12-10T14:21:12.788988: step 1668, loss 3.96555, acc 0.796875, prec 0.0389131, recall 0.812382
2017-12-10T14:21:12.977969: step 1669, loss 0.305286, acc 0.890625, prec 0.0389049, recall 0.812382
2017-12-10T14:21:13.167198: step 1670, loss 0.458152, acc 0.90625, prec 0.0389559, recall 0.812618
2017-12-10T14:21:13.348402: step 1671, loss 0.798247, acc 0.875, prec 0.0389756, recall 0.812736
2017-12-10T14:21:13.537647: step 1672, loss 0.371641, acc 0.859375, prec 0.038994, recall 0.812854
2017-12-10T14:21:13.727119: step 1673, loss 12.1615, acc 0.875, prec 0.0390148, recall 0.812461
2017-12-10T14:21:13.918494: step 1674, loss 0.482663, acc 0.78125, prec 0.0389983, recall 0.812461
2017-12-10T14:21:14.115994: step 1675, loss 0.788522, acc 0.734375, prec 0.0389783, recall 0.812461
2017-12-10T14:21:14.303228: step 1676, loss 0.603338, acc 0.8125, prec 0.0389932, recall 0.812579
2017-12-10T14:21:14.492408: step 1677, loss 0.527233, acc 0.78125, prec 0.0390347, recall 0.812814
2017-12-10T14:21:14.680630: step 1678, loss 0.674115, acc 0.78125, prec 0.0390472, recall 0.812932
2017-12-10T14:21:14.868120: step 1679, loss 0.854569, acc 0.734375, prec 0.0390561, recall 0.813049
2017-12-10T14:21:15.053828: step 1680, loss 0.604163, acc 0.8125, prec 0.039042, recall 0.813049
2017-12-10T14:21:15.242391: step 1681, loss 0.944797, acc 0.71875, prec 0.0390498, recall 0.813166
2017-12-10T14:21:15.431865: step 1682, loss 1.11379, acc 0.765625, prec 0.0390322, recall 0.813166
2017-12-10T14:21:15.623262: step 1683, loss 0.417396, acc 0.828125, prec 0.0390482, recall 0.813283
2017-12-10T14:21:15.811108: step 1684, loss 0.546748, acc 0.8125, prec 0.039063, recall 0.8134
2017-12-10T14:21:15.997304: step 1685, loss 0.93046, acc 0.8125, prec 0.0390778, recall 0.813517
2017-12-10T14:21:16.186149: step 1686, loss 1.44803, acc 0.765625, prec 0.039089, recall 0.813634
2017-12-10T14:21:16.372854: step 1687, loss 0.616704, acc 0.78125, prec 0.0390726, recall 0.813634
2017-12-10T14:21:16.557307: step 1688, loss 0.320702, acc 0.828125, prec 0.0390597, recall 0.813634
2017-12-10T14:21:16.747113: step 1689, loss 0.563925, acc 0.796875, prec 0.0390733, recall 0.81375
2017-12-10T14:21:16.938862: step 1690, loss 0.77945, acc 0.796875, prec 0.0390869, recall 0.813866
2017-12-10T14:21:17.123072: step 1691, loss 0.712504, acc 0.78125, prec 0.0390705, recall 0.813866
2017-12-10T14:21:17.309553: step 1692, loss 0.406135, acc 0.84375, prec 0.0390876, recall 0.813983
2017-12-10T14:21:17.497323: step 1693, loss 0.672426, acc 0.859375, prec 0.0391058, recall 0.814099
2017-12-10T14:21:17.689119: step 1694, loss 0.537528, acc 0.8125, prec 0.0391205, recall 0.814214
2017-12-10T14:21:17.876050: step 1695, loss 0.164223, acc 0.9375, prec 0.0391158, recall 0.814214
2017-12-10T14:21:18.063891: step 1696, loss 0.420629, acc 0.859375, prec 0.0391053, recall 0.814214
2017-12-10T14:21:18.251092: step 1697, loss 0.498157, acc 0.859375, prec 0.0391235, recall 0.81433
2017-12-10T14:21:18.439339: step 1698, loss 0.306554, acc 0.875, prec 0.0391429, recall 0.814446
2017-12-10T14:21:18.628170: step 1699, loss 0.390757, acc 0.890625, prec 0.0391347, recall 0.814446
2017-12-10T14:21:18.817014: step 1700, loss 0.111083, acc 0.953125, prec 0.03916, recall 0.814561
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_2/1512933336/checkpoints/model-1700

2017-12-10T14:21:19.979696: step 1701, loss 0.181825, acc 0.90625, prec 0.0391529, recall 0.814561
2017-12-10T14:21:20.169618: step 1702, loss 0.559912, acc 0.875, prec 0.0391723, recall 0.814677
2017-12-10T14:21:20.358279: step 1703, loss 12.1866, acc 0.890625, prec 0.0391653, recall 0.81417
2017-12-10T14:21:20.552330: step 1704, loss 0.285833, acc 0.890625, prec 0.0391571, recall 0.81417
2017-12-10T14:21:20.741699: step 1705, loss 0.217824, acc 0.953125, prec 0.0391536, recall 0.81417
2017-12-10T14:21:20.928423: step 1706, loss 0.277729, acc 0.921875, prec 0.0391477, recall 0.81417
2017-12-10T14:21:21.113957: step 1707, loss 1.18867, acc 0.890625, prec 0.0391682, recall 0.814286
2017-12-10T14:21:21.306295: step 1708, loss 0.268353, acc 0.921875, prec 0.0391911, recall 0.814401
2017-12-10T14:21:21.493485: step 1709, loss 0.283839, acc 0.890625, prec 0.0391829, recall 0.814401
2017-12-10T14:21:21.679104: step 1710, loss 0.581476, acc 0.84375, prec 0.0391999, recall 0.814516
2017-12-10T14:21:21.867179: step 1711, loss 0.445516, acc 0.828125, prec 0.0392157, recall 0.814631
2017-12-10T14:21:22.056771: step 1712, loss 2.14871, acc 0.921875, prec 0.0392397, recall 0.814241
2017-12-10T14:21:22.248955: step 1713, loss 3.52118, acc 0.90625, prec 0.0392911, recall 0.813968
2017-12-10T14:21:22.435442: step 1714, loss 0.271557, acc 0.953125, prec 0.0393163, recall 0.814083
2017-12-10T14:21:22.628370: step 1715, loss 0.469427, acc 0.859375, prec 0.0393057, recall 0.814083
2017-12-10T14:21:22.814720: step 1716, loss 0.914112, acc 0.75, prec 0.0393156, recall 0.814198
2017-12-10T14:21:23.002469: step 1717, loss 0.864784, acc 0.75, prec 0.0392969, recall 0.814198
2017-12-10T14:21:23.186488: step 1718, loss 1.18186, acc 0.6875, prec 0.0393021, recall 0.814312
2017-12-10T14:21:23.374619: step 1719, loss 0.5948, acc 0.765625, prec 0.0393131, recall 0.814427
2017-12-10T14:21:23.558789: step 1720, loss 0.93753, acc 0.75, prec 0.039323, recall 0.814541
2017-12-10T14:21:23.743635: step 1721, loss 0.777442, acc 0.796875, prec 0.0393935, recall 0.814883
2017-12-10T14:21:23.937414: step 1722, loss 1.02639, acc 0.71875, prec 0.039401, recall 0.814997
2017-12-10T14:21:24.127022: step 1723, loss 1.03271, acc 0.734375, prec 0.0394381, recall 0.815224
2017-12-10T14:21:24.314354: step 1724, loss 0.958607, acc 0.765625, prec 0.0394776, recall 0.815451
2017-12-10T14:21:24.503019: step 1725, loss 1.00497, acc 0.6875, prec 0.0394827, recall 0.815564
2017-12-10T14:21:24.691952: step 1726, loss 0.27936, acc 0.921875, prec 0.0395053, recall 0.815677
2017-12-10T14:21:24.877670: step 1727, loss 0.885374, acc 0.75, prec 0.039515, recall 0.815789
2017-12-10T14:21:25.064295: step 1728, loss 0.81984, acc 0.75, prec 0.0395532, recall 0.816015
2017-12-10T14:21:25.259385: step 1729, loss 0.735416, acc 0.84375, prec 0.0395699, recall 0.816127
2017-12-10T14:21:25.445503: step 1730, loss 0.832339, acc 0.78125, prec 0.0396104, recall 0.816351
2017-12-10T14:21:25.637347: step 1731, loss 0.877432, acc 0.796875, prec 0.039652, recall 0.816575
2017-12-10T14:21:25.823370: step 1732, loss 0.9414, acc 0.796875, prec 0.0396368, recall 0.816575
2017-12-10T14:21:26.010082: step 1733, loss 0.33731, acc 0.84375, prec 0.0396534, recall 0.816687
2017-12-10T14:21:26.201931: step 1734, loss 0.411469, acc 0.8125, prec 0.0396961, recall 0.81691
2017-12-10T14:21:26.384962: step 1735, loss 0.453718, acc 0.8125, prec 0.0396821, recall 0.81691
2017-12-10T14:21:26.571428: step 1736, loss 0.215709, acc 0.890625, prec 0.0396739, recall 0.81691
2017-12-10T14:21:26.761287: step 1737, loss 0.308118, acc 0.90625, prec 0.0396952, recall 0.817021
2017-12-10T14:21:26.948276: step 1738, loss 0.304573, acc 0.90625, prec 0.0396882, recall 0.817021
2017-12-10T14:21:27.136468: step 1739, loss 0.458966, acc 0.875, prec 0.0396788, recall 0.817021
2017-12-10T14:21:27.324586: step 1740, loss 0.372808, acc 0.828125, prec 0.0396942, recall 0.817132
2017-12-10T14:21:27.513765: step 1741, loss 2.16936, acc 0.90625, prec 0.0396884, recall 0.816636
2017-12-10T14:21:27.703999: step 1742, loss 0.292274, acc 0.890625, prec 0.0396802, recall 0.816636
2017-12-10T14:21:27.894363: step 1743, loss 4.82897, acc 0.9375, prec 0.0396767, recall 0.816141
2017-12-10T14:21:28.087045: step 1744, loss 0.13541, acc 0.9375, prec 0.039672, recall 0.816141
2017-12-10T14:21:28.275359: step 1745, loss 0.291536, acc 0.875, prec 0.0396626, recall 0.816141
2017-12-10T14:21:28.461617: step 1746, loss 0.195223, acc 0.9375, prec 0.039658, recall 0.816141
2017-12-10T14:21:28.646046: step 1747, loss 0.325007, acc 0.875, prec 0.0396486, recall 0.816141
2017-12-10T14:21:28.835636: step 1748, loss 0.523697, acc 0.890625, prec 0.039697, recall 0.816364
2017-12-10T14:21:29.028779: step 1749, loss 3.34334, acc 0.859375, prec 0.0396877, recall 0.815869
2017-12-10T14:21:29.220199: step 1750, loss 0.597591, acc 0.90625, prec 0.0397938, recall 0.816314
2017-12-10T14:21:29.412978: step 1751, loss 0.597183, acc 0.78125, prec 0.039834, recall 0.816536
2017-12-10T14:21:29.598745: step 1752, loss 0.83776, acc 0.765625, prec 0.0398164, recall 0.816536
2017-12-10T14:21:29.786853: step 1753, loss 0.659424, acc 0.796875, prec 0.0398294, recall 0.816647
2017-12-10T14:21:29.974329: step 1754, loss 0.637591, acc 0.8125, prec 0.0398153, recall 0.816647
2017-12-10T14:21:30.161647: step 1755, loss 0.750366, acc 0.671875, prec 0.039819, recall 0.816757
2017-12-10T14:21:30.350948: step 1756, loss 0.618689, acc 0.78125, prec 0.039859, recall 0.816978
2017-12-10T14:21:30.538773: step 1757, loss 0.782963, acc 0.75, prec 0.0398403, recall 0.816978
2017-12-10T14:21:30.726811: step 1758, loss 0.802611, acc 0.796875, prec 0.0398251, recall 0.816978
2017-12-10T14:21:30.913338: step 1759, loss 0.879734, acc 0.75, prec 0.0398346, recall 0.817088
2017-12-10T14:21:31.100896: step 1760, loss 0.624044, acc 0.859375, prec 0.0399085, recall 0.817417
2017-12-10T14:21:31.292824: step 1761, loss 0.911226, acc 0.859375, prec 0.0399543, recall 0.817636
2017-12-10T14:21:31.485150: step 1762, loss 0.611214, acc 0.828125, prec 0.0399695, recall 0.817746
2017-12-10T14:21:31.676415: step 1763, loss 0.913182, acc 0.71875, prec 0.0399766, recall 0.817855
2017-12-10T14:21:31.861706: step 1764, loss 0.483579, acc 0.84375, prec 0.039993, recall 0.817964
2017-12-10T14:21:32.046605: step 1765, loss 0.498746, acc 0.8125, prec 0.0399789, recall 0.817964
2017-12-10T14:21:32.232948: step 1766, loss 0.576047, acc 0.84375, prec 0.0399672, recall 0.817964
2017-12-10T14:21:32.420258: step 1767, loss 0.269283, acc 0.9375, prec 0.0399626, recall 0.817964
2017-12-10T14:21:32.606620: step 1768, loss 0.949684, acc 0.84375, prec 0.040007, recall 0.818182
2017-12-10T14:21:32.799642: step 1769, loss 0.460892, acc 0.859375, prec 0.0399965, recall 0.818182
2017-12-10T14:21:32.986453: step 1770, loss 0.430016, acc 0.890625, prec 0.0399883, recall 0.818182
2017-12-10T14:21:33.176474: step 1771, loss 8.05783, acc 0.84375, prec 0.0399778, recall 0.817693
2017-12-10T14:21:33.363993: step 1772, loss 0.387161, acc 0.875, prec 0.0399965, recall 0.817802
2017-12-10T14:21:33.554432: step 1773, loss 0.273162, acc 0.875, prec 0.0399871, recall 0.817802
2017-12-10T14:21:33.741808: step 1774, loss 0.853126, acc 0.890625, prec 0.040035, recall 0.818019
2017-12-10T14:21:33.929187: step 1775, loss 0.57869, acc 0.796875, prec 0.0400199, recall 0.818019
2017-12-10T14:21:34.120845: step 1776, loss 0.44914, acc 0.890625, prec 0.0400397, recall 0.818128
2017-12-10T14:21:34.308417: step 1777, loss 0.359943, acc 0.875, prec 0.0400584, recall 0.818236
2017-12-10T14:21:34.498836: step 1778, loss 0.244949, acc 0.890625, prec 0.0400502, recall 0.818236
2017-12-10T14:21:34.685772: step 1779, loss 1.54319, acc 0.859375, prec 0.0400688, recall 0.817857
2017-12-10T14:21:34.876398: step 1780, loss 0.371628, acc 0.90625, prec 0.0401458, recall 0.818182
2017-12-10T14:21:35.062296: step 1781, loss 0.256012, acc 0.859375, prec 0.0401352, recall 0.818182
2017-12-10T14:21:35.251751: step 1782, loss 0.557331, acc 0.84375, prec 0.0401235, recall 0.818182
2017-12-10T14:21:35.442283: step 1783, loss 9.78905, acc 0.78125, prec 0.0401363, recall 0.817804
2017-12-10T14:21:35.633314: step 1784, loss 0.552331, acc 0.796875, prec 0.0402329, recall 0.818236
2017-12-10T14:21:35.820017: step 1785, loss 0.739805, acc 0.75, prec 0.0402421, recall 0.818343
2017-12-10T14:21:36.005753: step 1786, loss 0.719004, acc 0.765625, prec 0.0402245, recall 0.818343
2017-12-10T14:21:36.189520: step 1787, loss 0.649223, acc 0.78125, prec 0.040264, recall 0.818558
2017-12-10T14:21:36.377005: step 1788, loss 0.551939, acc 0.828125, prec 0.0403069, recall 0.818772
2017-12-10T14:21:36.567764: step 1789, loss 0.858585, acc 0.765625, prec 0.0403172, recall 0.818879
2017-12-10T14:21:36.756896: step 1790, loss 0.581107, acc 0.8125, prec 0.0403031, recall 0.818879
2017-12-10T14:21:36.942889: step 1791, loss 0.51006, acc 0.796875, prec 0.0402879, recall 0.818879
2017-12-10T14:21:37.129471: step 1792, loss 0.426991, acc 0.796875, prec 0.0402727, recall 0.818879
2017-12-10T14:21:37.314560: step 1793, loss 0.673056, acc 0.796875, prec 0.0403411, recall 0.819199
2017-12-10T14:21:37.505929: step 1794, loss 7.70038, acc 0.828125, prec 0.040385, recall 0.81893
2017-12-10T14:21:37.699740: step 1795, loss 0.957635, acc 0.71875, prec 0.0404196, recall 0.819143
2017-12-10T14:21:37.893840: step 1796, loss 0.639985, acc 0.78125, prec 0.0404032, recall 0.819143
2017-12-10T14:21:38.087976: step 1797, loss 0.779145, acc 0.734375, prec 0.0403833, recall 0.819143
2017-12-10T14:21:38.274456: step 1798, loss 0.627619, acc 0.8125, prec 0.0403693, recall 0.819143
2017-12-10T14:21:38.462452: step 1799, loss 0.55971, acc 0.90625, prec 0.04039, recall 0.819249
2017-12-10T14:21:38.650637: step 1800, loss 0.583196, acc 0.828125, prec 0.0403772, recall 0.819249
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_2/1512933336/checkpoints/model-1800

2017-12-10T14:21:39.816152: step 1801, loss 0.427998, acc 0.84375, prec 0.040421, recall 0.819461
2017-12-10T14:21:40.004570: step 1802, loss 0.999102, acc 0.796875, prec 0.0404335, recall 0.819566
2017-12-10T14:21:40.193237: step 1803, loss 0.709667, acc 0.78125, prec 0.0404172, recall 0.819566
2017-12-10T14:21:40.378041: step 1804, loss 0.645776, acc 0.78125, prec 0.0404008, recall 0.819566
2017-12-10T14:21:40.563982: step 1805, loss 0.425928, acc 0.84375, prec 0.0404169, recall 0.819672
2017-12-10T14:21:40.749151: step 1806, loss 0.323591, acc 0.875, prec 0.0404075, recall 0.819672
2017-12-10T14:21:40.932937: step 1807, loss 0.63911, acc 0.765625, prec 0.0403901, recall 0.819672
2017-12-10T14:21:41.121465: step 1808, loss 0.296427, acc 0.875, prec 0.0403807, recall 0.819672
2017-12-10T14:21:41.306580: step 1809, loss 0.306964, acc 0.90625, prec 0.0404291, recall 0.819883
2017-12-10T14:21:41.493342: step 1810, loss 0.181826, acc 0.9375, prec 0.0404244, recall 0.819883
2017-12-10T14:21:41.680202: step 1811, loss 0.411246, acc 0.8125, prec 0.0404934, recall 0.820198
2017-12-10T14:21:41.870265: step 1812, loss 0.178186, acc 0.9375, prec 0.0404887, recall 0.820198
2017-12-10T14:21:42.059388: step 1813, loss 0.27524, acc 0.890625, prec 0.0404806, recall 0.820198
2017-12-10T14:21:42.247276: step 1814, loss 0.20919, acc 0.890625, prec 0.0405001, recall 0.820303
2017-12-10T14:21:42.430677: step 1815, loss 0.21199, acc 0.890625, prec 0.0405195, recall 0.820408
2017-12-10T14:21:42.617957: step 1816, loss 9.59765, acc 0.921875, prec 0.0405149, recall 0.81993
2017-12-10T14:21:42.810006: step 1817, loss 0.196959, acc 1, prec 0.0405701, recall 0.82014
2017-12-10T14:21:43.002538: step 1818, loss 1.55988, acc 0.9375, prec 0.0406483, recall 0.820453
2017-12-10T14:21:43.196235: step 1819, loss 0.275857, acc 0.9375, prec 0.0406436, recall 0.820453
2017-12-10T14:21:43.383620: step 1820, loss 0.288656, acc 0.953125, prec 0.0406677, recall 0.820557
2017-12-10T14:21:43.575832: step 1821, loss 0.316847, acc 0.921875, prec 0.0406895, recall 0.820662
2017-12-10T14:21:43.766108: step 1822, loss 0.401405, acc 0.90625, prec 0.0407377, recall 0.82087
2017-12-10T14:21:43.959132: step 1823, loss 0.490755, acc 0.875, prec 0.0407559, recall 0.820973
2017-12-10T14:21:44.153270: step 1824, loss 0.363354, acc 0.890625, prec 0.0407477, recall 0.820973
2017-12-10T14:21:44.340175: step 1825, loss 4.75222, acc 0.90625, prec 0.0407418, recall 0.820498
2017-12-10T14:21:44.533144: step 1826, loss 0.494038, acc 0.796875, prec 0.0407266, recall 0.820498
2017-12-10T14:21:44.717714: step 1827, loss 0.440796, acc 0.859375, prec 0.0407161, recall 0.820498
2017-12-10T14:21:44.905097: step 1828, loss 0.65514, acc 0.78125, prec 0.0406997, recall 0.820498
2017-12-10T14:21:45.091568: step 1829, loss 4.98581, acc 0.765625, prec 0.0406833, recall 0.820023
2017-12-10T14:21:45.279591: step 1830, loss 0.830296, acc 0.765625, prec 0.0406933, recall 0.820127
2017-12-10T14:21:45.465243: step 1831, loss 0.792279, acc 0.765625, prec 0.0407584, recall 0.820439
2017-12-10T14:21:45.660469: step 1832, loss 1.01314, acc 0.734375, prec 0.0407385, recall 0.820439
2017-12-10T14:21:45.849017: step 1833, loss 1.00113, acc 0.703125, prec 0.0407988, recall 0.820749
2017-12-10T14:21:46.034233: step 1834, loss 1.35724, acc 0.609375, prec 0.0407696, recall 0.820749
2017-12-10T14:21:46.218678: step 1835, loss 1.64412, acc 0.640625, prec 0.0407428, recall 0.820749
2017-12-10T14:21:46.402396: step 1836, loss 0.68959, acc 0.796875, prec 0.0407825, recall 0.820956
2017-12-10T14:21:46.590408: step 1837, loss 1.49363, acc 0.640625, prec 0.0408653, recall 0.821367
2017-12-10T14:21:46.775742: step 1838, loss 1.06133, acc 0.6875, prec 0.040842, recall 0.821367
2017-12-10T14:21:46.960139: step 1839, loss 3.86788, acc 0.765625, prec 0.0408804, recall 0.821101
2017-12-10T14:21:47.147380: step 1840, loss 0.445207, acc 0.875, prec 0.0408711, recall 0.821101
2017-12-10T14:21:47.333955: step 1841, loss 1.09168, acc 0.734375, prec 0.0408786, recall 0.821203
2017-12-10T14:21:47.518456: step 1842, loss 0.986366, acc 0.75, prec 0.0409147, recall 0.821408
2017-12-10T14:21:47.704305: step 1843, loss 0.781187, acc 0.828125, prec 0.0409018, recall 0.821408
2017-12-10T14:21:47.893306: step 1844, loss 0.363758, acc 0.859375, prec 0.0408913, recall 0.821408
2017-12-10T14:21:48.080666: step 1845, loss 0.594645, acc 0.796875, prec 0.0409035, recall 0.82151
2017-12-10T14:21:48.266533: step 1846, loss 0.848694, acc 0.828125, prec 0.040918, recall 0.821612
2017-12-10T14:21:48.456993: step 1847, loss 0.275837, acc 0.875, prec 0.0409633, recall 0.821816
2017-12-10T14:21:48.646091: step 1848, loss 0.556173, acc 0.796875, prec 0.0409482, recall 0.821816
2017-12-10T14:21:48.833503: step 1849, loss 0.206676, acc 0.921875, prec 0.0409423, recall 0.821816
2017-12-10T14:21:49.019440: step 1850, loss 0.307887, acc 0.953125, prec 0.0409661, recall 0.821918
2017-12-10T14:21:49.209111: step 1851, loss 0.185301, acc 0.921875, prec 0.0409603, recall 0.821918
2017-12-10T14:21:49.394293: step 1852, loss 0.241213, acc 0.90625, prec 0.0409806, recall 0.822019
2017-12-10T14:21:49.586846: step 1853, loss 7.70844, acc 0.875, prec 0.0409997, recall 0.821652
2017-12-10T14:21:49.780489: step 1854, loss 0.344725, acc 0.890625, prec 0.0410188, recall 0.821754
2017-12-10T14:21:49.969796: step 1855, loss 0.0941115, acc 0.953125, prec 0.0410153, recall 0.821754
2017-12-10T14:21:50.157156: step 1856, loss 0.181446, acc 0.921875, prec 0.0410095, recall 0.821754
2017-12-10T14:21:50.350771: step 1857, loss 1.01595, acc 0.90625, prec 0.0410297, recall 0.821855
2017-12-10T14:21:50.539581: step 1858, loss 0.423798, acc 0.921875, prec 0.0411056, recall 0.822159
2017-12-10T14:21:50.725912: step 1859, loss 0.378362, acc 0.875, prec 0.0410963, recall 0.822159
2017-12-10T14:21:50.913090: step 1860, loss 0.486931, acc 0.859375, prec 0.0410858, recall 0.822159
2017-12-10T14:21:51.100735: step 1861, loss 0.660625, acc 0.875, prec 0.0411037, recall 0.82226
2017-12-10T14:21:51.289502: step 1862, loss 0.257996, acc 0.90625, prec 0.0410967, recall 0.82226
2017-12-10T14:21:51.478576: step 1863, loss 0.901589, acc 0.90625, prec 0.0411169, recall 0.822361
2017-12-10T14:21:51.673081: step 1864, loss 0.648463, acc 0.828125, prec 0.0411584, recall 0.822562
2017-12-10T14:21:51.864684: step 1865, loss 0.648382, acc 0.9375, prec 0.041181, recall 0.822663
2017-12-10T14:21:52.052486: step 1866, loss 0.56313, acc 0.84375, prec 0.0411693, recall 0.822663
2017-12-10T14:21:52.237613: step 1867, loss 0.694376, acc 0.875, prec 0.0412415, recall 0.822964
2017-12-10T14:21:52.427171: step 1868, loss 0.84067, acc 0.84375, prec 0.0412841, recall 0.823164
2017-12-10T14:21:52.616093: step 1869, loss 0.647394, acc 0.8125, prec 0.0412973, recall 0.823264
2017-12-10T14:21:52.799991: step 1870, loss 0.95106, acc 0.703125, prec 0.0412751, recall 0.823264
2017-12-10T14:21:52.990385: step 1871, loss 0.718763, acc 0.828125, prec 0.0413165, recall 0.823463
2017-12-10T14:21:53.176049: step 1872, loss 3.27301, acc 0.84375, prec 0.0413873, recall 0.823298
2017-12-10T14:21:53.367557: step 1873, loss 0.594904, acc 0.8125, prec 0.0414004, recall 0.823397
2017-12-10T14:21:53.557009: step 1874, loss 0.626918, acc 0.8125, prec 0.0414134, recall 0.823496
2017-12-10T14:21:53.745437: step 1875, loss 0.876462, acc 0.78125, prec 0.041397, recall 0.823496
2017-12-10T14:21:53.937044: step 1876, loss 0.640238, acc 0.796875, prec 0.0413818, recall 0.823496
2017-12-10T14:21:54.127239: step 1877, loss 0.489558, acc 0.75, prec 0.0413631, recall 0.823496
2017-12-10T14:21:54.315618: step 1878, loss 2.51701, acc 0.8125, prec 0.0414044, recall 0.823232
2017-12-10T14:21:54.501614: step 1879, loss 1.06403, acc 0.640625, prec 0.0413776, recall 0.823232
2017-12-10T14:21:54.689691: step 1880, loss 0.586831, acc 0.84375, prec 0.0413659, recall 0.823232
2017-12-10T14:21:54.874220: step 1881, loss 1.13821, acc 0.78125, prec 0.0414036, recall 0.82343
2017-12-10T14:21:55.060032: step 1882, loss 0.73135, acc 0.75, prec 0.041412, recall 0.823529
2017-12-10T14:21:55.249325: step 1883, loss 0.517596, acc 0.75, prec 0.0413933, recall 0.823529
2017-12-10T14:21:55.436919: step 1884, loss 1.04589, acc 0.703125, prec 0.0413981, recall 0.823628
2017-12-10T14:21:55.622246: step 1885, loss 0.506606, acc 0.796875, prec 0.041383, recall 0.823628
2017-12-10T14:21:55.806452: step 1886, loss 0.406953, acc 0.84375, prec 0.0413714, recall 0.823628
2017-12-10T14:21:55.990337: step 1887, loss 0.707892, acc 0.78125, prec 0.041382, recall 0.823727
2017-12-10T14:21:56.179083: step 1888, loss 0.85465, acc 0.734375, prec 0.0413892, recall 0.823825
2017-12-10T14:21:56.363220: step 1889, loss 0.482275, acc 0.8125, prec 0.0414022, recall 0.823924
2017-12-10T14:21:56.549456: step 1890, loss 0.598528, acc 0.84375, prec 0.0413905, recall 0.823924
2017-12-10T14:21:56.736354: step 1891, loss 0.330461, acc 0.890625, prec 0.0414362, recall 0.824121
2017-12-10T14:21:56.925722: step 1892, loss 0.441961, acc 0.828125, prec 0.0414234, recall 0.824121
2017-12-10T14:21:57.111114: step 1893, loss 0.206855, acc 0.9375, prec 0.0414457, recall 0.824219
2017-12-10T14:21:57.299803: step 1894, loss 0.271866, acc 0.90625, prec 0.0414656, recall 0.824317
2017-12-10T14:21:57.488573: step 1895, loss 0.14282, acc 0.921875, prec 0.0414598, recall 0.824317
2017-12-10T14:21:57.676983: step 1896, loss 2.46633, acc 0.890625, prec 0.0414797, recall 0.823955
2017-12-10T14:21:57.868025: step 1897, loss 0.0898876, acc 0.984375, prec 0.0415054, recall 0.824053
2017-12-10T14:21:58.056630: step 1898, loss 0.626295, acc 0.875, prec 0.041523, recall 0.824151
2017-12-10T14:21:58.242414: step 1899, loss 0.231075, acc 0.953125, prec 0.0415464, recall 0.824249
2017-12-10T14:21:58.433517: step 1900, loss 0.0993645, acc 0.953125, prec 0.0415429, recall 0.824249
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_2/1512933336/checkpoints/model-1900

2017-12-10T14:21:59.501608: step 1901, loss 1.29599, acc 0.921875, prec 0.0415382, recall 0.823791
2017-12-10T14:21:59.694826: step 1902, loss 1.30124, acc 0.90625, prec 0.0415581, recall 0.823889
2017-12-10T14:21:59.884756: step 1903, loss 0.174073, acc 0.953125, prec 0.0415814, recall 0.823987
2017-12-10T14:22:00.069315: step 1904, loss 0.363581, acc 0.875, prec 0.0415721, recall 0.823987
2017-12-10T14:22:00.255137: step 1905, loss 0.228242, acc 0.890625, prec 0.0415908, recall 0.824084
2017-12-10T14:22:00.440744: step 1906, loss 0.865522, acc 0.859375, prec 0.0416877, recall 0.824474
2017-12-10T14:22:00.631053: step 1907, loss 0.399685, acc 0.859375, prec 0.0416772, recall 0.824474
2017-12-10T14:22:00.820132: step 1908, loss 0.411234, acc 0.859375, prec 0.0416667, recall 0.824474
2017-12-10T14:22:01.009292: step 1909, loss 0.724721, acc 0.765625, prec 0.041676, recall 0.824571
2017-12-10T14:22:01.196870: step 1910, loss 0.615717, acc 0.8125, prec 0.0417156, recall 0.824765
2017-12-10T14:22:01.390462: step 1911, loss 3.92141, acc 0.78125, prec 0.041754, recall 0.824503
2017-12-10T14:22:01.584260: step 1912, loss 0.563895, acc 0.890625, prec 0.0417458, recall 0.824503
2017-12-10T14:22:01.774891: step 1913, loss 0.569934, acc 0.875, prec 0.0417633, recall 0.8246
2017-12-10T14:22:01.964820: step 1914, loss 1.30657, acc 0.671875, prec 0.0417388, recall 0.8246
2017-12-10T14:22:02.150005: step 1915, loss 0.729311, acc 0.75, prec 0.0417202, recall 0.8246
2017-12-10T14:22:02.337906: step 1916, loss 0.647231, acc 0.796875, prec 0.0417585, recall 0.824793
2017-12-10T14:22:02.522251: step 1917, loss 0.791068, acc 0.765625, prec 0.041741, recall 0.824793
2017-12-10T14:22:02.710277: step 1918, loss 0.950676, acc 0.765625, prec 0.041777, recall 0.824986
2017-12-10T14:22:02.897716: step 1919, loss 0.769254, acc 0.75, prec 0.0417584, recall 0.824986
2017-12-10T14:22:03.087116: step 1920, loss 0.54914, acc 0.8125, prec 0.0417978, recall 0.825179
2017-12-10T14:22:03.270775: step 1921, loss 0.568396, acc 0.828125, prec 0.0418383, recall 0.825371
2017-12-10T14:22:03.454384: step 1922, loss 0.28856, acc 0.875, prec 0.0418557, recall 0.825467
2017-12-10T14:22:03.638039: step 1923, loss 0.798651, acc 0.75, prec 0.041837, recall 0.825467
2017-12-10T14:22:03.828405: step 1924, loss 1.09062, acc 0.71875, prec 0.0418161, recall 0.825467
2017-12-10T14:22:04.014277: step 1925, loss 0.727713, acc 0.828125, prec 0.04183, recall 0.825562
2017-12-10T14:22:04.202502: step 1926, loss 0.554423, acc 0.875, prec 0.0418739, recall 0.825753
2017-12-10T14:22:04.389659: step 1927, loss 0.670426, acc 0.8125, prec 0.0419132, recall 0.825944
2017-12-10T14:22:04.582191: step 1928, loss 0.661539, acc 0.78125, prec 0.0419235, recall 0.826039
2017-12-10T14:22:04.767195: step 1929, loss 0.515049, acc 0.8125, prec 0.0419361, recall 0.826135
2017-12-10T14:22:04.950966: step 1930, loss 0.280532, acc 0.859375, prec 0.0419522, recall 0.82623
2017-12-10T14:22:05.141283: step 1931, loss 0.269014, acc 0.90625, prec 0.0419452, recall 0.82623
2017-12-10T14:22:05.331816: step 1932, loss 0.408469, acc 0.859375, prec 0.0419348, recall 0.82623
2017-12-10T14:22:05.517015: step 1933, loss 0.0686896, acc 0.96875, prec 0.041959, recall 0.826324
2017-12-10T14:22:05.707475: step 1934, loss 0.314235, acc 0.890625, prec 0.0419774, recall 0.826419
2017-12-10T14:22:05.899389: step 1935, loss 0.761521, acc 0.90625, prec 0.0420236, recall 0.826608
2017-12-10T14:22:06.093982: step 1936, loss 2.52553, acc 0.90625, prec 0.0420177, recall 0.826158
2017-12-10T14:22:06.283780: step 1937, loss 0.275601, acc 0.9375, prec 0.0420131, recall 0.826158
2017-12-10T14:22:06.467744: step 1938, loss 1.06916, acc 0.921875, prec 0.0420338, recall 0.826253
2017-12-10T14:22:06.663633: step 1939, loss 0.207582, acc 0.921875, prec 0.042028, recall 0.826253
2017-12-10T14:22:06.850838: step 1940, loss 9.35076, acc 0.859375, prec 0.0420717, recall 0.825992
2017-12-10T14:22:07.039711: step 1941, loss 0.536017, acc 0.8125, prec 0.0420843, recall 0.826087
2017-12-10T14:22:07.230878: step 1942, loss 0.717688, acc 0.828125, prec 0.0420715, recall 0.826087
2017-12-10T14:22:07.413994: step 1943, loss 0.7936, acc 0.796875, prec 0.0421358, recall 0.82637
2017-12-10T14:22:07.602728: step 1944, loss 0.758948, acc 0.796875, prec 0.0421737, recall 0.826558
2017-12-10T14:22:07.788352: step 1945, loss 0.660506, acc 0.78125, prec 0.0421574, recall 0.826558
2017-12-10T14:22:07.975147: step 1946, loss 1.11867, acc 0.625, prec 0.0421294, recall 0.826558
2017-12-10T14:22:08.162816: step 1947, loss 1.37117, acc 0.765625, prec 0.0421648, recall 0.826746
2017-12-10T14:22:08.348583: step 1948, loss 1.22668, acc 0.734375, prec 0.0421979, recall 0.826934
2017-12-10T14:22:08.538427: step 1949, loss 0.63664, acc 0.765625, prec 0.0421805, recall 0.826934
2017-12-10T14:22:08.725148: step 1950, loss 1.26293, acc 0.671875, prec 0.0421561, recall 0.826934
2017-12-10T14:22:08.913402: step 1951, loss 0.706174, acc 0.796875, prec 0.0421409, recall 0.826934
2017-12-10T14:22:09.103111: step 1952, loss 0.844621, acc 0.71875, prec 0.0421201, recall 0.826934
2017-12-10T14:22:09.290360: step 1953, loss 0.672977, acc 0.78125, prec 0.0421038, recall 0.826934
2017-12-10T14:22:09.476614: step 1954, loss 0.95791, acc 0.75, prec 0.0421116, recall 0.827027
2017-12-10T14:22:09.660837: step 1955, loss 0.706504, acc 0.75, prec 0.0420931, recall 0.827027
2017-12-10T14:22:09.850327: step 1956, loss 1.08878, acc 0.734375, prec 0.0421524, recall 0.827307
2017-12-10T14:22:10.040384: step 1957, loss 0.756071, acc 0.703125, prec 0.0421568, recall 0.8274
2017-12-10T14:22:10.225984: step 1958, loss 0.401591, acc 0.84375, prec 0.0421452, recall 0.8274
2017-12-10T14:22:10.410284: step 1959, loss 0.381741, acc 0.859375, prec 0.0421348, recall 0.8274
2017-12-10T14:22:10.593018: step 1960, loss 0.234431, acc 0.90625, prec 0.0421278, recall 0.8274
2017-12-10T14:22:10.778737: step 1961, loss 0.588463, acc 0.78125, prec 0.0421116, recall 0.8274
2017-12-10T14:22:10.971671: step 1962, loss 0.427381, acc 0.828125, prec 0.0421515, recall 0.827586
2017-12-10T14:22:11.160745: step 1963, loss 0.493852, acc 0.859375, prec 0.0421936, recall 0.827772
2017-12-10T14:22:11.347981: step 1964, loss 0.931368, acc 0.859375, prec 0.042262, recall 0.828049
2017-12-10T14:22:11.537868: step 1965, loss 0.149943, acc 0.9375, prec 0.0423099, recall 0.828234
2017-12-10T14:22:11.725378: step 1966, loss 0.171529, acc 0.953125, prec 0.0423327, recall 0.828326
2017-12-10T14:22:11.911692: step 1967, loss 7.62173, acc 0.9375, prec 0.0423555, recall 0.827974
2017-12-10T14:22:12.106774: step 1968, loss 0.324001, acc 0.875, prec 0.0423724, recall 0.828066
2017-12-10T14:22:12.294711: step 1969, loss 0.417412, acc 0.859375, prec 0.042362, recall 0.828066
2017-12-10T14:22:12.487303: step 1970, loss 0.223451, acc 0.90625, prec 0.042355, recall 0.828066
2017-12-10T14:22:12.677771: step 1971, loss 0.414633, acc 0.875, prec 0.0423457, recall 0.828066
2017-12-10T14:22:12.865653: step 1972, loss 0.28449, acc 0.90625, prec 0.042365, recall 0.828158
2017-12-10T14:22:13.049073: step 1973, loss 0.370477, acc 0.84375, prec 0.0423534, recall 0.828158
2017-12-10T14:22:13.236538: step 1974, loss 0.285421, acc 0.875, prec 0.0423703, recall 0.82825
2017-12-10T14:22:13.420981: step 1975, loss 0.216639, acc 0.9375, prec 0.0423919, recall 0.828342
2017-12-10T14:22:13.613230: step 1976, loss 0.288317, acc 0.890625, prec 0.04241, recall 0.828434
2017-12-10T14:22:13.804969: step 1977, loss 0.155357, acc 0.96875, prec 0.0424077, recall 0.828434
2017-12-10T14:22:13.998823: step 1978, loss 0.825449, acc 0.984375, prec 0.0424327, recall 0.828526
2017-12-10T14:22:14.190414: step 1979, loss 0.923091, acc 0.9375, prec 0.0425066, recall 0.8288
2017-12-10T14:22:14.381139: step 1980, loss 0.224575, acc 0.921875, prec 0.042527, recall 0.828891
2017-12-10T14:22:14.565527: step 1981, loss 0.398016, acc 0.859375, prec 0.0425165, recall 0.828891
2017-12-10T14:22:14.753824: step 1982, loss 0.268218, acc 0.921875, prec 0.0425107, recall 0.828891
2017-12-10T14:22:14.942274: step 1983, loss 0.437371, acc 0.921875, prec 0.0425573, recall 0.829073
2017-12-10T14:22:15.128944: step 1984, loss 0.39367, acc 0.859375, prec 0.042573, recall 0.829164
2017-12-10T14:22:15.319955: step 1985, loss 0.474192, acc 0.875, prec 0.0425637, recall 0.829164
2017-12-10T14:22:15.506068: step 1986, loss 0.474979, acc 0.890625, prec 0.0425555, recall 0.829164
2017-12-10T14:22:15.696518: step 1987, loss 0.525262, acc 0.890625, prec 0.0425474, recall 0.829164
2017-12-10T14:22:15.865637: step 1988, loss 3.88025, acc 0.923077, prec 0.0425439, recall 0.828723
2017-12-10T14:22:16.060774: step 1989, loss 0.31365, acc 0.875, prec 0.0425346, recall 0.828723
2017-12-10T14:22:16.253613: step 1990, loss 0.262217, acc 0.890625, prec 0.0425265, recall 0.828723
2017-12-10T14:22:16.445984: step 1991, loss 0.431939, acc 0.875, prec 0.0425172, recall 0.828723
2017-12-10T14:22:16.632374: step 1992, loss 0.31109, acc 0.875, prec 0.0425079, recall 0.828723
2017-12-10T14:22:16.818607: step 1993, loss 1.04108, acc 0.859375, prec 0.0425236, recall 0.828814
2017-12-10T14:22:17.006289: step 1994, loss 0.230678, acc 0.921875, prec 0.04257, recall 0.828996
2017-12-10T14:22:17.197297: step 1995, loss 0.77582, acc 0.75, prec 0.0425775, recall 0.829087
2017-12-10T14:22:17.386672: step 1996, loss 0.376821, acc 0.859375, prec 0.0425932, recall 0.829178
2017-12-10T14:22:17.572956: step 1997, loss 0.49913, acc 0.84375, prec 0.0426338, recall 0.829359
2017-12-10T14:22:17.762617: step 1998, loss 0.518079, acc 0.875, prec 0.0427027, recall 0.82963
2017-12-10T14:22:17.951327: step 1999, loss 0.581288, acc 0.828125, prec 0.0426899, recall 0.82963
2017-12-10T14:22:18.137125: step 2000, loss 0.484964, acc 0.8125, prec 0.042702, recall 0.82972
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_2/1512933336/checkpoints/model-2000

2017-12-10T14:22:19.235348: step 2001, loss 0.194926, acc 0.921875, prec 0.0426962, recall 0.82972
2017-12-10T14:22:19.421192: step 2002, loss 0.781603, acc 0.8125, prec 0.0426823, recall 0.82972
2017-12-10T14:22:19.607324: step 2003, loss 0.271663, acc 0.90625, prec 0.0427013, recall 0.82981
2017-12-10T14:22:19.796112: step 2004, loss 0.482953, acc 0.828125, prec 0.0426886, recall 0.82981
2017-12-10T14:22:19.985513: step 2005, loss 0.2745, acc 0.9375, prec 0.0426839, recall 0.82981
2017-12-10T14:22:20.173982: step 2006, loss 0.157708, acc 0.9375, prec 0.0426793, recall 0.82981
2017-12-10T14:22:20.364839: step 2007, loss 0.125184, acc 0.9375, prec 0.0426746, recall 0.82981
2017-12-10T14:22:20.552263: step 2008, loss 0.308618, acc 0.875, prec 0.0426654, recall 0.82981
2017-12-10T14:22:20.743532: step 2009, loss 0.332374, acc 0.921875, prec 0.0426596, recall 0.82981
2017-12-10T14:22:20.938941: step 2010, loss 0.210548, acc 0.90625, prec 0.0426526, recall 0.82981
2017-12-10T14:22:21.124396: step 2011, loss 0.226846, acc 0.921875, prec 0.0426728, recall 0.8299
2017-12-10T14:22:21.315404: step 2012, loss 0.575025, acc 0.984375, prec 0.0426977, recall 0.829989
2017-12-10T14:22:21.501916: step 2013, loss 1.24069, acc 0.9375, prec 0.042745, recall 0.830169
2017-12-10T14:22:21.693084: step 2014, loss 0.201967, acc 0.921875, prec 0.0427912, recall 0.830348
2017-12-10T14:22:21.881937: step 2015, loss 0.284938, acc 0.90625, prec 0.0428102, recall 0.830437
2017-12-10T14:22:22.069628: step 2016, loss 0.319407, acc 0.875, prec 0.0428529, recall 0.830615
2017-12-10T14:22:22.255861: step 2017, loss 0.0571834, acc 0.96875, prec 0.0428506, recall 0.830615
2017-12-10T14:22:22.442748: step 2018, loss 0.101805, acc 0.953125, prec 0.0428471, recall 0.830615
2017-12-10T14:22:22.632087: step 2019, loss 8.52209, acc 0.90625, prec 0.0428932, recall 0.830357
2017-12-10T14:22:22.821447: step 2020, loss 0.380398, acc 0.890625, prec 0.042937, recall 0.830535
2017-12-10T14:22:23.004216: step 2021, loss 6.27262, acc 0.921875, prec 0.0429583, recall 0.830189
2017-12-10T14:22:23.199786: step 2022, loss 0.655863, acc 0.84375, prec 0.0429726, recall 0.830278
2017-12-10T14:22:23.385332: step 2023, loss 0.545615, acc 0.859375, prec 0.0429621, recall 0.830278
2017-12-10T14:22:23.573369: step 2024, loss 0.657592, acc 0.75, prec 0.0429435, recall 0.830278
2017-12-10T14:22:23.758914: step 2025, loss 0.612162, acc 0.796875, prec 0.0429283, recall 0.830278
2017-12-10T14:22:23.943959: step 2026, loss 0.802081, acc 0.75, prec 0.0429357, recall 0.830366
2017-12-10T14:22:24.130107: step 2027, loss 1.16003, acc 0.75, prec 0.0429171, recall 0.830366
2017-12-10T14:22:24.313370: step 2028, loss 1.27879, acc 0.6875, prec 0.0429197, recall 0.830455
2017-12-10T14:22:24.502058: step 2029, loss 1.49401, acc 0.65625, prec 0.0428942, recall 0.830455
2017-12-10T14:22:24.688826: step 2030, loss 0.618632, acc 0.78125, prec 0.0429038, recall 0.830544
2017-12-10T14:22:24.872210: step 2031, loss 0.725776, acc 0.75, prec 0.0429112, recall 0.830633
2017-12-10T14:22:25.058604: step 2032, loss 1.15374, acc 0.625, prec 0.0429092, recall 0.830721
2017-12-10T14:22:25.245234: step 2033, loss 0.951881, acc 0.703125, prec 0.042913, recall 0.830809
2017-12-10T14:22:25.433168: step 2034, loss 0.946083, acc 0.734375, prec 0.0429191, recall 0.830898
2017-12-10T14:22:25.617870: step 2035, loss 0.433507, acc 0.84375, prec 0.0429076, recall 0.830898
2017-12-10T14:22:25.804751: step 2036, loss 0.512002, acc 0.84375, prec 0.0429218, recall 0.830986
2017-12-10T14:22:25.990691: step 2037, loss 0.769623, acc 0.78125, prec 0.0429056, recall 0.830986
2017-12-10T14:22:26.179147: step 2038, loss 0.581952, acc 0.890625, prec 0.0429749, recall 0.83125
2017-12-10T14:22:26.370288: step 2039, loss 0.240734, acc 0.90625, prec 0.0430452, recall 0.831513
2017-12-10T14:22:26.554699: step 2040, loss 0.419109, acc 0.90625, prec 0.0430382, recall 0.831513
2017-12-10T14:22:26.742376: step 2041, loss 0.307615, acc 0.875, prec 0.0430547, recall 0.831601
2017-12-10T14:22:26.928210: step 2042, loss 0.184334, acc 0.9375, prec 0.0430758, recall 0.831688
2017-12-10T14:22:27.111753: step 2043, loss 0.20511, acc 0.953125, prec 0.0430981, recall 0.831776
2017-12-10T14:22:27.297694: step 2044, loss 0.318138, acc 0.890625, prec 0.04309, recall 0.831776
2017-12-10T14:22:27.484060: step 2045, loss 0.384722, acc 0.859375, prec 0.0431053, recall 0.831863
2017-12-10T14:22:27.672538: step 2046, loss 1.29079, acc 0.921875, prec 0.0431007, recall 0.831432
2017-12-10T14:22:27.862500: step 2047, loss 0.404116, acc 0.953125, prec 0.0431486, recall 0.831606
2017-12-10T14:22:28.057451: step 2048, loss 0.0640103, acc 0.96875, prec 0.0431463, recall 0.831606
2017-12-10T14:22:28.250000: step 2049, loss 0.178026, acc 0.953125, prec 0.04322, recall 0.831868
2017-12-10T14:22:28.441648: step 2050, loss 1.98557, acc 0.90625, prec 0.0432656, recall 0.831612
2017-12-10T14:22:28.633094: step 2051, loss 0.213646, acc 0.921875, prec 0.0432598, recall 0.831612
2017-12-10T14:22:28.825378: step 2052, loss 3.25093, acc 0.796875, prec 0.0432459, recall 0.831182
2017-12-10T14:22:29.019950: step 2053, loss 0.358254, acc 0.90625, prec 0.0432389, recall 0.831182
2017-12-10T14:22:29.208279: step 2054, loss 0.314939, acc 0.890625, prec 0.0432565, recall 0.831269
2017-12-10T14:22:29.396512: step 2055, loss 1.34907, acc 0.90625, prec 0.043302, recall 0.831015
2017-12-10T14:22:29.590652: step 2056, loss 1.065, acc 0.9375, prec 0.0433487, recall 0.831189
2017-12-10T14:22:29.779475: step 2057, loss 0.460072, acc 0.859375, prec 0.0433383, recall 0.831189
2017-12-10T14:22:29.965280: step 2058, loss 0.409084, acc 0.890625, prec 0.0433301, recall 0.831189
2017-12-10T14:22:30.159086: step 2059, loss 0.305757, acc 0.921875, prec 0.04335, recall 0.831276
2017-12-10T14:22:30.347014: step 2060, loss 0.288572, acc 0.90625, prec 0.0433943, recall 0.831449
2017-12-10T14:22:30.536546: step 2061, loss 0.481711, acc 0.84375, prec 0.043434, recall 0.831622
2017-12-10T14:22:30.723598: step 2062, loss 1.38139, acc 0.75, prec 0.043441, recall 0.831709
2017-12-10T14:22:30.910934: step 2063, loss 6.81249, acc 0.8125, prec 0.0434293, recall 0.830856
2017-12-10T14:22:31.105503: step 2064, loss 0.772724, acc 0.765625, prec 0.0434119, recall 0.830856
2017-12-10T14:22:31.291570: step 2065, loss 0.764144, acc 0.75, prec 0.0434189, recall 0.830943
2017-12-10T14:22:31.482188: step 2066, loss 1.263, acc 0.65625, prec 0.0434189, recall 0.831029
2017-12-10T14:22:31.669533: step 2067, loss 1.30844, acc 0.625, prec 0.0433911, recall 0.831029
2017-12-10T14:22:31.857555: step 2068, loss 0.915547, acc 0.703125, prec 0.0434202, recall 0.831202
2017-12-10T14:22:32.040836: step 2069, loss 1.61677, acc 0.65625, prec 0.0434458, recall 0.831375
2017-12-10T14:22:32.238848: step 2070, loss 0.99707, acc 0.765625, prec 0.0434284, recall 0.831375
2017-12-10T14:22:32.425112: step 2071, loss 1.07071, acc 0.65625, prec 0.0434029, recall 0.831375
2017-12-10T14:22:32.613131: step 2072, loss 1.11937, acc 0.59375, prec 0.0433728, recall 0.831375
2017-12-10T14:22:32.794285: step 2073, loss 1.02231, acc 0.703125, prec 0.0433763, recall 0.831461
2017-12-10T14:22:32.981445: step 2074, loss 1.42946, acc 0.640625, prec 0.0433498, recall 0.831461
2017-12-10T14:22:33.168531: step 2075, loss 1.17531, acc 0.734375, prec 0.0433556, recall 0.831547
2017-12-10T14:22:33.351126: step 2076, loss 1.44559, acc 0.59375, prec 0.0433511, recall 0.831633
2017-12-10T14:22:33.540535: step 2077, loss 1.2599, acc 0.75, prec 0.0433835, recall 0.831804
2017-12-10T14:22:33.727558: step 2078, loss 0.656936, acc 0.765625, prec 0.0433916, recall 0.83189
2017-12-10T14:22:33.912686: step 2079, loss 0.712999, acc 0.71875, prec 0.0433709, recall 0.83189
2017-12-10T14:22:34.095372: step 2080, loss 0.632215, acc 0.8125, prec 0.0434332, recall 0.832146
2017-12-10T14:22:34.284978: step 2081, loss 0.919105, acc 0.78125, prec 0.0434171, recall 0.832146
2017-12-10T14:22:34.477440: step 2082, loss 0.311967, acc 0.875, prec 0.0434079, recall 0.832146
2017-12-10T14:22:34.666691: step 2083, loss 0.508147, acc 0.859375, prec 0.0434483, recall 0.832317
2017-12-10T14:22:34.856513: step 2084, loss 0.598362, acc 0.828125, prec 0.043461, recall 0.832402
2017-12-10T14:22:35.046583: step 2085, loss 0.4605, acc 0.90625, prec 0.0434794, recall 0.832487
2017-12-10T14:22:35.233397: step 2086, loss 0.807557, acc 0.9375, prec 0.0435255, recall 0.832657
2017-12-10T14:22:35.420316: step 2087, loss 0.123961, acc 0.953125, prec 0.0435221, recall 0.832657
2017-12-10T14:22:35.605192: step 2088, loss 0.147691, acc 0.953125, prec 0.0435439, recall 0.832742
2017-12-10T14:22:35.793753: step 2089, loss 0.125214, acc 0.96875, prec 0.0435923, recall 0.832911
2017-12-10T14:22:35.981511: step 2090, loss 0.101847, acc 0.9375, prec 0.0435877, recall 0.832911
2017-12-10T14:22:36.169485: step 2091, loss 0.227785, acc 0.9375, prec 0.0436338, recall 0.83308
2017-12-10T14:22:36.358792: step 2092, loss 0.55608, acc 0.9375, prec 0.0436545, recall 0.833165
2017-12-10T14:22:36.551730: step 2093, loss 0.247719, acc 0.921875, prec 0.0436487, recall 0.833165
2017-12-10T14:22:36.739302: step 2094, loss 0.945757, acc 0.953125, prec 0.0436705, recall 0.833249
2017-12-10T14:22:36.934267: step 2095, loss 2.01007, acc 0.96875, prec 0.0436694, recall 0.832828
2017-12-10T14:22:37.129805: step 2096, loss 0.016194, acc 1, prec 0.0436694, recall 0.832828
2017-12-10T14:22:37.321910: step 2097, loss 6.49046, acc 0.953125, prec 0.0436924, recall 0.832492
2017-12-10T14:22:37.514413: step 2098, loss 0.343705, acc 0.890625, prec 0.0437096, recall 0.832577
2017-12-10T14:22:37.701926: step 2099, loss 0.26553, acc 0.90625, prec 0.043728, recall 0.832661
2017-12-10T14:22:37.894484: step 2100, loss 0.418144, acc 0.84375, prec 0.043767, recall 0.83283
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_2/1512933336/checkpoints/model-2100

2017-12-10T14:22:39.094779: step 2101, loss 0.41851, acc 0.859375, prec 0.0437566, recall 0.83283
2017-12-10T14:22:39.281492: step 2102, loss 0.401142, acc 0.90625, prec 0.043775, recall 0.832914
2017-12-10T14:22:39.470766: step 2103, loss 0.572151, acc 0.84375, prec 0.0437887, recall 0.832998
2017-12-10T14:22:39.659627: step 2104, loss 0.538458, acc 0.796875, prec 0.0437736, recall 0.832998
2017-12-10T14:22:39.854564: step 2105, loss 0.52757, acc 0.796875, prec 0.0437586, recall 0.832998
2017-12-10T14:22:40.039508: step 2106, loss 0.527056, acc 0.8125, prec 0.0437952, recall 0.833166
2017-12-10T14:22:40.225517: step 2107, loss 0.381432, acc 0.921875, prec 0.0438147, recall 0.83325
2017-12-10T14:22:40.411247: step 2108, loss 0.638084, acc 0.859375, prec 0.0438295, recall 0.833333
2017-12-10T14:22:40.598684: step 2109, loss 0.505913, acc 0.8125, prec 0.0438409, recall 0.833417
2017-12-10T14:22:40.785135: step 2110, loss 0.756853, acc 0.765625, prec 0.043874, recall 0.833584
2017-12-10T14:22:40.969011: step 2111, loss 0.439192, acc 0.84375, prec 0.0438876, recall 0.833667
2017-12-10T14:22:41.154781: step 2112, loss 0.404882, acc 0.859375, prec 0.0438772, recall 0.833667
2017-12-10T14:22:41.350447: step 2113, loss 0.640222, acc 0.84375, prec 0.0439161, recall 0.833834
2017-12-10T14:22:41.542991: step 2114, loss 0.530185, acc 0.828125, prec 0.0439537, recall 0.834
2017-12-10T14:22:41.729212: step 2115, loss 0.634852, acc 0.796875, prec 0.043989, recall 0.834166
2017-12-10T14:22:41.915185: step 2116, loss 0.39567, acc 0.875, prec 0.0439798, recall 0.834166
2017-12-10T14:22:42.101820: step 2117, loss 0.423925, acc 0.875, prec 0.0439705, recall 0.834166
2017-12-10T14:22:42.293607: step 2118, loss 0.413699, acc 0.875, prec 0.0439612, recall 0.834166
2017-12-10T14:22:42.486458: step 2119, loss 0.237788, acc 0.96875, prec 0.0440093, recall 0.834331
2017-12-10T14:22:42.677302: step 2120, loss 0.376424, acc 0.875, prec 0.044, recall 0.834331
2017-12-10T14:22:42.865339: step 2121, loss 0.365431, acc 0.875, prec 0.0440159, recall 0.834414
2017-12-10T14:22:43.057893: step 2122, loss 0.323354, acc 0.9375, prec 0.0440364, recall 0.834496
2017-12-10T14:22:43.245465: step 2123, loss 0.252681, acc 0.875, prec 0.0440271, recall 0.834496
2017-12-10T14:22:43.430312: step 2124, loss 0.20388, acc 0.921875, prec 0.0440214, recall 0.834496
2017-12-10T14:22:43.618772: step 2125, loss 0.311759, acc 0.90625, prec 0.0440144, recall 0.834496
2017-12-10T14:22:43.804503: step 2126, loss 0.146888, acc 0.953125, prec 0.0440361, recall 0.834579
2017-12-10T14:22:43.996341: step 2127, loss 0.122608, acc 0.9375, prec 0.0440566, recall 0.834661
2017-12-10T14:22:44.185419: step 2128, loss 0.231941, acc 0.90625, prec 0.0440496, recall 0.834661
2017-12-10T14:22:44.375024: step 2129, loss 0.0942183, acc 0.984375, prec 0.0440485, recall 0.834661
2017-12-10T14:22:44.563360: step 2130, loss 0.118123, acc 0.9375, prec 0.0440438, recall 0.834661
2017-12-10T14:22:44.755256: step 2131, loss 0.465711, acc 0.96875, prec 0.0440918, recall 0.834826
2017-12-10T14:22:44.943930: step 2132, loss 0.0995468, acc 0.96875, prec 0.0440894, recall 0.834826
2017-12-10T14:22:45.136354: step 2133, loss 0.345487, acc 0.9375, prec 0.0441099, recall 0.834908
2017-12-10T14:22:45.324990: step 2134, loss 0.0474099, acc 0.96875, prec 0.0441076, recall 0.834908
2017-12-10T14:22:45.515890: step 2135, loss 0.0314014, acc 0.984375, prec 0.0441064, recall 0.834908
2017-12-10T14:22:45.704445: step 2136, loss 3.82301, acc 0.96875, prec 0.0441304, recall 0.834575
2017-12-10T14:22:45.895840: step 2137, loss 0.127932, acc 0.953125, prec 0.044152, recall 0.834657
2017-12-10T14:22:46.088560: step 2138, loss 0.26746, acc 0.953125, prec 0.0441736, recall 0.834739
2017-12-10T14:22:46.277370: step 2139, loss 0.254798, acc 0.9375, prec 0.044169, recall 0.834739
2017-12-10T14:22:46.463334: step 2140, loss 6.59116, acc 0.890625, prec 0.0442373, recall 0.834572
2017-12-10T14:22:46.656083: step 2141, loss 0.190907, acc 0.9375, prec 0.0442578, recall 0.834653
2017-12-10T14:22:46.847025: step 2142, loss 1.56622, acc 0.890625, prec 0.044301, recall 0.834404
2017-12-10T14:22:47.034338: step 2143, loss 0.490913, acc 0.859375, prec 0.0442905, recall 0.834404
2017-12-10T14:22:47.218530: step 2144, loss 0.73667, acc 0.8125, prec 0.0443768, recall 0.834731
2017-12-10T14:22:47.408645: step 2145, loss 0.777454, acc 0.765625, prec 0.0443844, recall 0.834813
2017-12-10T14:22:47.598523: step 2146, loss 0.569643, acc 0.828125, prec 0.0444468, recall 0.835057
2017-12-10T14:22:47.787204: step 2147, loss 0.754008, acc 0.8125, prec 0.0444328, recall 0.835057
2017-12-10T14:22:47.969695: step 2148, loss 1.02625, acc 0.734375, prec 0.044413, recall 0.835057
2017-12-10T14:22:48.155919: step 2149, loss 1.18053, acc 0.703125, prec 0.044491, recall 0.835381
2017-12-10T14:22:48.339286: step 2150, loss 1.3965, acc 0.609375, prec 0.0445368, recall 0.835623
2017-12-10T14:22:48.529341: step 2151, loss 0.489547, acc 0.8125, prec 0.0445229, recall 0.835623
2017-12-10T14:22:48.716280: step 2152, loss 1.23004, acc 0.640625, prec 0.0445211, recall 0.835704
2017-12-10T14:22:48.905843: step 2153, loss 1.05008, acc 0.75, prec 0.0445274, recall 0.835784
2017-12-10T14:22:49.094735: step 2154, loss 1.02013, acc 0.734375, prec 0.0445077, recall 0.835784
2017-12-10T14:22:49.281461: step 2155, loss 1.13184, acc 0.6875, prec 0.0445592, recall 0.836025
2017-12-10T14:22:49.474389: step 2156, loss 0.709905, acc 0.78125, prec 0.0445679, recall 0.836106
2017-12-10T14:22:49.659076: step 2157, loss 0.938691, acc 0.703125, prec 0.0445458, recall 0.836106
2017-12-10T14:22:49.845881: step 2158, loss 0.765298, acc 0.8125, prec 0.0445568, recall 0.836186
2017-12-10T14:22:50.035577: step 2159, loss 0.71501, acc 0.75, prec 0.0446129, recall 0.836426
2017-12-10T14:22:50.223653: step 2160, loss 0.700323, acc 0.796875, prec 0.0445978, recall 0.836426
2017-12-10T14:22:50.410567: step 2161, loss 0.32239, acc 0.90625, prec 0.0446405, recall 0.836585
2017-12-10T14:22:50.599167: step 2162, loss 0.727134, acc 0.875, prec 0.0447058, recall 0.836824
2017-12-10T14:22:50.786137: step 2163, loss 0.654759, acc 0.875, prec 0.0447462, recall 0.836983
2017-12-10T14:22:50.975904: step 2164, loss 1.81952, acc 0.796875, prec 0.0447571, recall 0.836655
2017-12-10T14:22:51.164194: step 2165, loss 0.425936, acc 0.859375, prec 0.0447715, recall 0.836735
2017-12-10T14:22:51.353803: step 2166, loss 0.584946, acc 0.828125, prec 0.0447587, recall 0.836735
2017-12-10T14:22:51.548943: step 2167, loss 0.391541, acc 0.90625, prec 0.0447517, recall 0.836735
2017-12-10T14:22:51.736905: step 2168, loss 0.220301, acc 0.90625, prec 0.0447447, recall 0.836735
2017-12-10T14:22:51.926400: step 2169, loss 0.38451, acc 0.875, prec 0.0447602, recall 0.836814
2017-12-10T14:22:52.115694: step 2170, loss 1.08194, acc 0.859375, prec 0.0447994, recall 0.836972
2017-12-10T14:22:52.306730: step 2171, loss 0.321108, acc 0.875, prec 0.0447901, recall 0.836972
2017-12-10T14:22:52.499720: step 2172, loss 0.415616, acc 0.890625, prec 0.0448067, recall 0.837051
2017-12-10T14:22:52.690012: step 2173, loss 0.0771176, acc 0.96875, prec 0.0448044, recall 0.837051
2017-12-10T14:22:52.873364: step 2174, loss 0.180883, acc 0.9375, prec 0.0448245, recall 0.83713
2017-12-10T14:22:53.057661: step 2175, loss 0.358974, acc 0.875, prec 0.04484, recall 0.837209
2017-12-10T14:22:53.248044: step 2176, loss 0.216568, acc 0.9375, prec 0.0448354, recall 0.837209
2017-12-10T14:22:53.437612: step 2177, loss 0.313929, acc 0.875, prec 0.0448508, recall 0.837288
2017-12-10T14:22:53.623871: step 2178, loss 0.153483, acc 0.953125, prec 0.0448721, recall 0.837367
2017-12-10T14:22:53.814753: step 2179, loss 0.777953, acc 0.890625, prec 0.0449135, recall 0.837524
2017-12-10T14:22:54.006853: step 2180, loss 0.181713, acc 0.921875, prec 0.0449077, recall 0.837524
2017-12-10T14:22:54.199853: step 2181, loss 0.356701, acc 0.90625, prec 0.0449007, recall 0.837524
2017-12-10T14:22:54.391450: step 2182, loss 0.137473, acc 0.953125, prec 0.044922, recall 0.837603
2017-12-10T14:22:54.579746: step 2183, loss 0.0461491, acc 0.984375, prec 0.0449208, recall 0.837603
2017-12-10T14:22:54.769804: step 2184, loss 0.312611, acc 0.90625, prec 0.0449633, recall 0.83776
2017-12-10T14:22:54.956780: step 2185, loss 3.27772, acc 0.984375, prec 0.0450128, recall 0.837512
2017-12-10T14:22:55.146353: step 2186, loss 0.233146, acc 0.953125, prec 0.0450836, recall 0.837747
2017-12-10T14:22:55.332441: step 2187, loss 0.293015, acc 0.96875, prec 0.0451307, recall 0.837903
2017-12-10T14:22:55.523651: step 2188, loss 0.212652, acc 0.9375, prec 0.045126, recall 0.837903
2017-12-10T14:22:55.711378: step 2189, loss 1.29415, acc 0.90625, prec 0.0451685, recall 0.838059
2017-12-10T14:22:55.902633: step 2190, loss 0.502793, acc 0.875, prec 0.0451838, recall 0.838136
2017-12-10T14:22:56.093030: step 2191, loss 0.764984, acc 0.84375, prec 0.0452463, recall 0.838369
2017-12-10T14:22:56.281623: step 2192, loss 0.588872, acc 0.828125, prec 0.0452334, recall 0.838369
2017-12-10T14:22:56.466287: step 2193, loss 0.785552, acc 0.765625, prec 0.0452159, recall 0.838369
2017-12-10T14:22:56.655848: step 2194, loss 0.408513, acc 0.84375, prec 0.0452289, recall 0.838447
2017-12-10T14:22:56.842248: step 2195, loss 0.429458, acc 0.859375, prec 0.0452183, recall 0.838447
2017-12-10T14:22:57.030664: step 2196, loss 0.447531, acc 0.859375, prec 0.0452325, recall 0.838524
2017-12-10T14:22:57.216597: step 2197, loss 0.472624, acc 0.859375, prec 0.045222, recall 0.838524
2017-12-10T14:22:57.406090: step 2198, loss 0.46015, acc 0.859375, prec 0.0452115, recall 0.838524
2017-12-10T14:22:57.594882: step 2199, loss 0.489208, acc 0.875, prec 0.0452268, recall 0.838602
2017-12-10T14:22:57.781879: step 2200, loss 1.46698, acc 0.8125, prec 0.0452621, recall 0.838756
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_2/1512933336/checkpoints/model-2200

2017-12-10T14:22:58.858611: step 2201, loss 0.464892, acc 0.859375, prec 0.0453008, recall 0.83891
2017-12-10T14:22:59.048342: step 2202, loss 0.26047, acc 0.90625, prec 0.0452938, recall 0.83891
2017-12-10T14:22:59.241006: step 2203, loss 0.223038, acc 0.890625, prec 0.0453103, recall 0.838987
2017-12-10T14:22:59.430197: step 2204, loss 0.421974, acc 0.890625, prec 0.0453021, recall 0.838987
2017-12-10T14:22:59.615350: step 2205, loss 0.202731, acc 0.90625, prec 0.0452951, recall 0.838987
2017-12-10T14:22:59.801427: step 2206, loss 3.23393, acc 0.875, prec 0.0453608, recall 0.838817
2017-12-10T14:22:59.993114: step 2207, loss 0.224143, acc 0.890625, prec 0.0453526, recall 0.838817
2017-12-10T14:23:00.181192: step 2208, loss 0.411655, acc 0.890625, prec 0.045369, recall 0.838894
2017-12-10T14:23:00.372826: step 2209, loss 0.520978, acc 0.875, prec 0.0453597, recall 0.838894
2017-12-10T14:23:00.559351: step 2210, loss 0.403865, acc 0.875, prec 0.0453503, recall 0.838894
2017-12-10T14:23:00.750289: step 2211, loss 0.600033, acc 0.84375, prec 0.0453632, recall 0.838971
2017-12-10T14:23:00.943029: step 2212, loss 0.824882, acc 0.859375, prec 0.045451, recall 0.839277
2017-12-10T14:23:01.134303: step 2213, loss 0.655492, acc 0.78125, prec 0.0455084, recall 0.839506
2017-12-10T14:23:01.334460: step 2214, loss 0.4442, acc 0.828125, prec 0.0454955, recall 0.839506
2017-12-10T14:23:01.525009: step 2215, loss 0.289602, acc 0.9375, prec 0.0454908, recall 0.839506
2017-12-10T14:23:01.709837: step 2216, loss 0.214018, acc 0.953125, prec 0.0454873, recall 0.839506
2017-12-10T14:23:01.896448: step 2217, loss 0.435817, acc 0.828125, prec 0.0454744, recall 0.839506
2017-12-10T14:23:02.084231: step 2218, loss 0.496846, acc 0.8125, prec 0.0454604, recall 0.839506
2017-12-10T14:23:02.269608: step 2219, loss 0.257969, acc 0.890625, prec 0.0455013, recall 0.839658
2017-12-10T14:23:02.456964: step 2220, loss 0.320517, acc 0.90625, prec 0.0455188, recall 0.839734
2017-12-10T14:23:02.644018: step 2221, loss 0.426107, acc 0.84375, prec 0.0455071, recall 0.839734
2017-12-10T14:23:02.830731: step 2222, loss 0.479441, acc 0.828125, prec 0.0454942, recall 0.839734
2017-12-10T14:23:03.016016: step 2223, loss 0.163716, acc 0.9375, prec 0.0454896, recall 0.839734
2017-12-10T14:23:03.199790: step 2224, loss 0.276953, acc 0.921875, prec 0.0454837, recall 0.839734
2017-12-10T14:23:03.391493: step 2225, loss 0.407686, acc 0.84375, prec 0.0454966, recall 0.83981
2017-12-10T14:23:03.578673: step 2226, loss 0.316543, acc 0.890625, prec 0.0455129, recall 0.839886
2017-12-10T14:23:03.766910: step 2227, loss 0.195945, acc 0.90625, prec 0.0455304, recall 0.839962
2017-12-10T14:23:03.953869: step 2228, loss 0.540553, acc 0.9375, prec 0.0455502, recall 0.840038
2017-12-10T14:23:04.147393: step 2229, loss 0.164183, acc 0.96875, prec 0.0455723, recall 0.840114
2017-12-10T14:23:04.337007: step 2230, loss 0.29787, acc 0.890625, prec 0.0455642, recall 0.840114
2017-12-10T14:23:04.524711: step 2231, loss 3.71422, acc 0.921875, prec 0.0455595, recall 0.839716
2017-12-10T14:23:04.717655: step 2232, loss 0.168263, acc 0.921875, prec 0.0455536, recall 0.839716
2017-12-10T14:23:04.910037: step 2233, loss 0.10767, acc 0.921875, prec 0.0455478, recall 0.839716
2017-12-10T14:23:05.095601: step 2234, loss 0.333699, acc 0.921875, prec 0.0455664, recall 0.839792
2017-12-10T14:23:05.286480: step 2235, loss 0.0965012, acc 0.984375, prec 0.0455653, recall 0.839792
2017-12-10T14:23:05.477564: step 2236, loss 0.289143, acc 0.875, prec 0.0455559, recall 0.839792
2017-12-10T14:23:05.664382: step 2237, loss 0.193563, acc 0.921875, prec 0.0455746, recall 0.839868
2017-12-10T14:23:05.850638: step 2238, loss 0.126953, acc 0.953125, prec 0.0455955, recall 0.839943
2017-12-10T14:23:06.037289: step 2239, loss 2.41201, acc 0.921875, prec 0.0456153, recall 0.839623
2017-12-10T14:23:06.229493: step 2240, loss 0.422677, acc 0.921875, prec 0.0456584, recall 0.839774
2017-12-10T14:23:06.416873: step 2241, loss 0.328448, acc 0.984375, prec 0.0456816, recall 0.839849
2017-12-10T14:23:06.611642: step 2242, loss 0.238135, acc 0.9375, prec 0.045677, recall 0.839849
2017-12-10T14:23:06.801343: step 2243, loss 0.642779, acc 0.8125, prec 0.0456629, recall 0.839849
2017-12-10T14:23:06.993364: step 2244, loss 0.345551, acc 0.90625, prec 0.0456803, recall 0.839925
2017-12-10T14:23:07.183969: step 2245, loss 0.111781, acc 0.953125, prec 0.0457013, recall 0.84
2017-12-10T14:23:07.370430: step 2246, loss 0.288445, acc 0.890625, prec 0.0456931, recall 0.84
2017-12-10T14:23:07.556586: step 2247, loss 0.137768, acc 0.9375, prec 0.0456884, recall 0.84
2017-12-10T14:23:07.746118: step 2248, loss 0.391168, acc 0.875, prec 0.0457035, recall 0.840075
2017-12-10T14:23:07.935337: step 2249, loss 0.26601, acc 0.9375, prec 0.0456988, recall 0.840075
2017-12-10T14:23:08.122889: step 2250, loss 0.688109, acc 0.90625, prec 0.045765, recall 0.840301
2017-12-10T14:23:08.310867: step 2251, loss 0.183285, acc 0.90625, prec 0.045758, recall 0.840301
2017-12-10T14:23:08.501672: step 2252, loss 0.0929488, acc 0.96875, prec 0.0457556, recall 0.840301
2017-12-10T14:23:08.690120: step 2253, loss 0.298018, acc 0.921875, prec 0.0457742, recall 0.840376
2017-12-10T14:23:08.880396: step 2254, loss 0.12918, acc 0.953125, prec 0.0458195, recall 0.840525
2017-12-10T14:23:09.066777: step 2255, loss 0.265795, acc 0.90625, prec 0.0458612, recall 0.840675
2017-12-10T14:23:09.255083: step 2256, loss 0.389264, acc 0.84375, prec 0.0458983, recall 0.840824
2017-12-10T14:23:09.444737: step 2257, loss 0.425552, acc 0.84375, prec 0.0459109, recall 0.840898
2017-12-10T14:23:09.633438: step 2258, loss 0.163518, acc 0.90625, prec 0.0459039, recall 0.840898
2017-12-10T14:23:09.820039: step 2259, loss 0.601411, acc 0.9375, prec 0.0459479, recall 0.841047
2017-12-10T14:23:10.012542: step 2260, loss 4.2297, acc 0.875, prec 0.0459641, recall 0.840729
2017-12-10T14:23:10.203665: step 2261, loss 0.246855, acc 0.9375, prec 0.0459838, recall 0.840803
2017-12-10T14:23:10.392094: step 2262, loss 0.296734, acc 0.859375, prec 0.0460219, recall 0.840952
2017-12-10T14:23:10.581661: step 2263, loss 0.221907, acc 0.953125, prec 0.0460184, recall 0.840952
2017-12-10T14:23:10.766517: step 2264, loss 2.86907, acc 0.859375, prec 0.046009, recall 0.840559
2017-12-10T14:23:10.954752: step 2265, loss 0.403487, acc 0.875, prec 0.0460239, recall 0.840634
2017-12-10T14:23:11.146246: step 2266, loss 0.515167, acc 0.875, prec 0.0460389, recall 0.840708
2017-12-10T14:23:11.336360: step 2267, loss 0.391131, acc 0.84375, prec 0.0460271, recall 0.840708
2017-12-10T14:23:11.527841: step 2268, loss 1.27106, acc 0.890625, prec 0.0460676, recall 0.840856
2017-12-10T14:23:11.717857: step 2269, loss 0.827394, acc 0.765625, prec 0.0460743, recall 0.84093
2017-12-10T14:23:11.901191: step 2270, loss 0.59213, acc 0.84375, prec 0.0460625, recall 0.84093
2017-12-10T14:23:12.090502: step 2271, loss 0.891006, acc 0.703125, prec 0.0460402, recall 0.84093
2017-12-10T14:23:12.276732: step 2272, loss 0.607717, acc 0.796875, prec 0.046025, recall 0.84093
2017-12-10T14:23:12.461265: step 2273, loss 1.80657, acc 0.828125, prec 0.0460133, recall 0.840539
2017-12-10T14:23:12.649346: step 2274, loss 0.427925, acc 0.84375, prec 0.0460501, recall 0.840687
2017-12-10T14:23:12.835583: step 2275, loss 0.834234, acc 0.734375, prec 0.046103, recall 0.840909
2017-12-10T14:23:13.021389: step 2276, loss 0.807224, acc 0.765625, prec 0.0460854, recall 0.840909
2017-12-10T14:23:13.210154: step 2277, loss 0.538205, acc 0.8125, prec 0.0461198, recall 0.841057
2017-12-10T14:23:13.398614: step 2278, loss 0.694109, acc 0.78125, prec 0.0461277, recall 0.84113
2017-12-10T14:23:13.586405: step 2279, loss 0.716788, acc 0.75, prec 0.0461089, recall 0.84113
2017-12-10T14:23:13.771102: step 2280, loss 0.722325, acc 0.796875, prec 0.0460937, recall 0.84113
2017-12-10T14:23:13.965566: step 2281, loss 0.666612, acc 0.78125, prec 0.0461499, recall 0.841351
2017-12-10T14:23:14.155836: step 2282, loss 0.444384, acc 0.859375, prec 0.0461394, recall 0.841351
2017-12-10T14:23:14.343501: step 2283, loss 0.326634, acc 0.84375, prec 0.0461761, recall 0.841497
2017-12-10T14:23:14.529092: step 2284, loss 0.469406, acc 0.8125, prec 0.0461862, recall 0.84157
2017-12-10T14:23:14.714767: step 2285, loss 1.10718, acc 0.828125, prec 0.0462217, recall 0.841717
2017-12-10T14:23:14.905969: step 2286, loss 0.688388, acc 0.84375, prec 0.0462341, recall 0.84179
2017-12-10T14:23:15.093282: step 2287, loss 0.363136, acc 0.8125, prec 0.0462201, recall 0.84179
2017-12-10T14:23:15.284876: step 2288, loss 0.273911, acc 0.921875, prec 0.0462867, recall 0.842008
2017-12-10T14:23:15.474312: step 2289, loss 5.94703, acc 0.84375, prec 0.0462761, recall 0.841621
2017-12-10T14:23:15.663251: step 2290, loss 0.491607, acc 0.828125, prec 0.0462874, recall 0.841694
2017-12-10T14:23:15.850337: step 2291, loss 0.385226, acc 0.859375, prec 0.0462769, recall 0.841694
2017-12-10T14:23:16.037323: step 2292, loss 0.364433, acc 0.875, prec 0.0462675, recall 0.841694
2017-12-10T14:23:16.226068: step 2293, loss 0.374975, acc 0.90625, prec 0.0462846, recall 0.841766
2017-12-10T14:23:16.413601: step 2294, loss 0.640636, acc 0.875, prec 0.0463235, recall 0.841912
2017-12-10T14:23:16.603494: step 2295, loss 0.193089, acc 0.875, prec 0.0463141, recall 0.841912
2017-12-10T14:23:16.792219: step 2296, loss 0.214683, acc 0.921875, prec 0.0463323, recall 0.841984
2017-12-10T14:23:16.978536: step 2297, loss 0.180846, acc 0.90625, prec 0.0463253, recall 0.841984
2017-12-10T14:23:17.169172: step 2298, loss 0.419372, acc 0.828125, prec 0.0463124, recall 0.841984
2017-12-10T14:23:17.356882: step 2299, loss 3.02121, acc 0.84375, prec 0.0463501, recall 0.841743
2017-12-10T14:23:17.546334: step 2300, loss 0.304838, acc 0.890625, prec 0.0463419, recall 0.841743
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_2/1512933336/checkpoints/model-2300

2017-12-10T14:23:18.609617: step 2301, loss 0.378283, acc 0.921875, prec 0.0463842, recall 0.841888
2017-12-10T14:23:18.796429: step 2302, loss 0.248567, acc 0.90625, prec 0.0464253, recall 0.842033
2017-12-10T14:23:18.986496: step 2303, loss 0.380495, acc 0.84375, prec 0.0464377, recall 0.842105
2017-12-10T14:23:19.174891: step 2304, loss 0.882727, acc 0.8125, prec 0.0464717, recall 0.84225
2017-12-10T14:23:19.360767: step 2305, loss 0.55718, acc 0.828125, prec 0.0464588, recall 0.84225
2017-12-10T14:23:19.546872: step 2306, loss 0.484651, acc 0.84375, prec 0.0464471, recall 0.84225
2017-12-10T14:23:19.733956: step 2307, loss 0.398865, acc 0.828125, prec 0.0464583, recall 0.842322
2017-12-10T14:23:19.917878: step 2308, loss 0.509034, acc 0.8125, prec 0.0464442, recall 0.842322
2017-12-10T14:23:20.109358: step 2309, loss 0.26178, acc 0.890625, prec 0.046436, recall 0.842322
2017-12-10T14:23:20.297519: step 2310, loss 0.394412, acc 0.890625, prec 0.0464519, recall 0.842394
2017-12-10T14:23:20.481332: step 2311, loss 0.222244, acc 0.90625, prec 0.0464449, recall 0.842394
2017-12-10T14:23:20.665102: step 2312, loss 0.253057, acc 0.9375, prec 0.0464402, recall 0.842394
2017-12-10T14:23:20.856997: step 2313, loss 0.203037, acc 0.9375, prec 0.0464355, recall 0.842394
2017-12-10T14:23:21.045571: step 2314, loss 0.349061, acc 0.921875, prec 0.0465017, recall 0.842609
2017-12-10T14:23:21.238246: step 2315, loss 0.279899, acc 0.921875, prec 0.0464958, recall 0.842609
2017-12-10T14:23:21.424397: step 2316, loss 0.213459, acc 0.90625, prec 0.0464888, recall 0.842609
2017-12-10T14:23:21.613007: step 2317, loss 0.289942, acc 0.9375, prec 0.0465321, recall 0.842753
2017-12-10T14:23:21.804538: step 2318, loss 0.298472, acc 0.875, prec 0.0465467, recall 0.842825
2017-12-10T14:23:21.994227: step 2319, loss 0.155421, acc 0.90625, prec 0.0465397, recall 0.842825
2017-12-10T14:23:22.181165: step 2320, loss 0.144408, acc 0.921875, prec 0.0465339, recall 0.842825
2017-12-10T14:23:22.372608: step 2321, loss 0.101668, acc 0.96875, prec 0.0465315, recall 0.842825
2017-12-10T14:23:22.559344: step 2322, loss 0.158953, acc 0.96875, prec 0.0465292, recall 0.842825
2017-12-10T14:23:22.744085: step 2323, loss 0.106766, acc 0.953125, prec 0.0465496, recall 0.842896
2017-12-10T14:23:22.932982: step 2324, loss 0.193387, acc 0.921875, prec 0.0465678, recall 0.842968
2017-12-10T14:23:23.138004: step 2325, loss 0.0672419, acc 0.96875, prec 0.0465654, recall 0.842968
2017-12-10T14:23:23.328317: step 2326, loss 0.00662461, acc 1, prec 0.0465894, recall 0.843039
2017-12-10T14:23:23.516207: step 2327, loss 19.953, acc 0.953125, prec 0.046611, recall 0.842727
2017-12-10T14:23:23.707899: step 2328, loss 0.0395373, acc 0.984375, prec 0.0466338, recall 0.842799
2017-12-10T14:23:23.896887: step 2329, loss 0.137181, acc 0.9375, prec 0.0466291, recall 0.842799
2017-12-10T14:23:24.089932: step 2330, loss 0.288977, acc 0.953125, prec 0.0466496, recall 0.84287
2017-12-10T14:23:24.274657: step 2331, loss 0.304468, acc 0.921875, prec 0.0466677, recall 0.842941
2017-12-10T14:23:24.462572: step 2332, loss 0.215859, acc 0.9375, prec 0.0466869, recall 0.843013
2017-12-10T14:23:24.648539: step 2333, loss 0.237374, acc 0.921875, prec 0.046705, recall 0.843084
2017-12-10T14:23:24.835923: step 2334, loss 0.283408, acc 0.921875, prec 0.0467231, recall 0.843155
2017-12-10T14:23:25.025493: step 2335, loss 1.105, acc 0.84375, prec 0.0467832, recall 0.843368
2017-12-10T14:23:25.216984: step 2336, loss 0.160649, acc 0.9375, prec 0.0467785, recall 0.843368
2017-12-10T14:23:25.405796: step 2337, loss 0.0973258, acc 1, prec 0.0468264, recall 0.84351
2017-12-10T14:23:25.593802: step 2338, loss 0.223706, acc 0.9375, prec 0.0468456, recall 0.84358
2017-12-10T14:23:25.783686: step 2339, loss 0.721795, acc 0.875, prec 0.0468601, recall 0.843651
2017-12-10T14:23:25.972900: step 2340, loss 0.0943834, acc 0.96875, prec 0.0468577, recall 0.843651
2017-12-10T14:23:26.161632: step 2341, loss 0.303452, acc 0.953125, prec 0.0469021, recall 0.843792
2017-12-10T14:23:26.349015: step 2342, loss 0.293307, acc 0.875, prec 0.0468926, recall 0.843792
2017-12-10T14:23:26.536244: step 2343, loss 0.547803, acc 0.828125, prec 0.0468797, recall 0.843792
2017-12-10T14:23:26.722409: step 2344, loss 0.462717, acc 0.859375, prec 0.0468691, recall 0.843792
2017-12-10T14:23:26.910427: step 2345, loss 0.316995, acc 0.875, prec 0.0468836, recall 0.843863
2017-12-10T14:23:27.096062: step 2346, loss 0.490874, acc 0.921875, prec 0.0469016, recall 0.843933
2017-12-10T14:23:27.285660: step 2347, loss 0.727911, acc 0.84375, prec 0.0469377, recall 0.844074
2017-12-10T14:23:27.474240: step 2348, loss 0.41688, acc 0.84375, prec 0.0469498, recall 0.844144
2017-12-10T14:23:27.665332: step 2349, loss 0.276824, acc 0.875, prec 0.0469404, recall 0.844144
2017-12-10T14:23:27.856750: step 2350, loss 0.509243, acc 0.875, prec 0.0469787, recall 0.844284
2017-12-10T14:23:28.044468: step 2351, loss 0.125149, acc 0.953125, prec 0.046999, recall 0.844354
2017-12-10T14:23:28.230389: step 2352, loss 0.30512, acc 0.90625, prec 0.0470397, recall 0.844494
2017-12-10T14:23:28.415764: step 2353, loss 0.358303, acc 0.921875, prec 0.0470576, recall 0.844564
2017-12-10T14:23:28.600591: step 2354, loss 0.202584, acc 0.953125, prec 0.047078, recall 0.844634
2017-12-10T14:23:28.788417: step 2355, loss 0.37767, acc 0.859375, prec 0.0470912, recall 0.844704
2017-12-10T14:23:28.983378: step 2356, loss 0.340724, acc 0.875, prec 0.0470818, recall 0.844704
2017-12-10T14:23:29.173909: step 2357, loss 0.546008, acc 0.9375, prec 0.0471009, recall 0.844773
2017-12-10T14:23:29.366010: step 2358, loss 0.346773, acc 0.921875, prec 0.047095, recall 0.844773
2017-12-10T14:23:29.550818: step 2359, loss 0.309871, acc 0.9375, prec 0.0471141, recall 0.844843
2017-12-10T14:23:29.741954: step 2360, loss 0.19318, acc 0.890625, prec 0.0471059, recall 0.844843
2017-12-10T14:23:29.927444: step 2361, loss 0.13854, acc 0.921875, prec 0.0471238, recall 0.844913
2017-12-10T14:23:30.113862: step 2362, loss 0.310866, acc 0.890625, prec 0.0471394, recall 0.844982
2017-12-10T14:23:30.300813: step 2363, loss 0.406694, acc 0.84375, prec 0.0471276, recall 0.844982
2017-12-10T14:23:30.489252: step 2364, loss 3.25286, acc 0.875, prec 0.0471194, recall 0.844604
2017-12-10T14:23:30.681894: step 2365, loss 0.474885, acc 0.890625, prec 0.0471111, recall 0.844604
2017-12-10T14:23:30.868725: step 2366, loss 0.246502, acc 0.921875, prec 0.047129, recall 0.844673
2017-12-10T14:23:31.060393: step 2367, loss 2.61328, acc 0.890625, prec 0.047122, recall 0.844295
2017-12-10T14:23:31.256731: step 2368, loss 0.938999, acc 0.921875, prec 0.0471637, recall 0.844434
2017-12-10T14:23:31.451965: step 2369, loss 0.331567, acc 0.90625, prec 0.0471804, recall 0.844504
2017-12-10T14:23:31.640586: step 2370, loss 0.46638, acc 0.8125, prec 0.0471901, recall 0.844573
2017-12-10T14:23:31.824997: step 2371, loss 0.650484, acc 0.78125, prec 0.0472211, recall 0.844712
2017-12-10T14:23:32.017962: step 2372, loss 0.818451, acc 0.8125, prec 0.047207, recall 0.844712
2017-12-10T14:23:32.205680: step 2373, loss 3.40907, acc 0.828125, prec 0.0471952, recall 0.844335
2017-12-10T14:23:32.395664: step 2374, loss 0.400474, acc 0.859375, prec 0.0471846, recall 0.844335
2017-12-10T14:23:32.585404: step 2375, loss 0.912288, acc 0.75, prec 0.0472608, recall 0.844613
2017-12-10T14:23:32.776548: step 2376, loss 0.91654, acc 0.75, prec 0.0472657, recall 0.844682
2017-12-10T14:23:32.963065: step 2377, loss 0.953141, acc 0.75, prec 0.0472468, recall 0.844682
2017-12-10T14:23:33.149144: step 2378, loss 0.507424, acc 0.78125, prec 0.0472778, recall 0.84482
2017-12-10T14:23:33.334100: step 2379, loss 0.941217, acc 0.71875, prec 0.047304, recall 0.844958
2017-12-10T14:23:33.517494: step 2380, loss 0.847073, acc 0.78125, prec 0.0472876, recall 0.844958
2017-12-10T14:23:33.703405: step 2381, loss 0.753263, acc 0.78125, prec 0.0472948, recall 0.845027
2017-12-10T14:23:33.892669: step 2382, loss 1.30075, acc 0.59375, prec 0.0472642, recall 0.845027
2017-12-10T14:23:34.077668: step 2383, loss 0.723692, acc 0.734375, prec 0.0472916, recall 0.845164
2017-12-10T14:23:34.266008: step 2384, loss 0.490717, acc 0.8125, prec 0.0472775, recall 0.845164
2017-12-10T14:23:34.453837: step 2385, loss 0.866335, acc 0.75, prec 0.0472587, recall 0.845164
2017-12-10T14:23:34.641809: step 2386, loss 0.296119, acc 0.875, prec 0.0472494, recall 0.845164
2017-12-10T14:23:34.825861: step 2387, loss 0.434136, acc 0.8125, prec 0.0472589, recall 0.845233
2017-12-10T14:23:35.020921: step 2388, loss 0.211319, acc 0.9375, prec 0.0473015, recall 0.84537
2017-12-10T14:23:35.208241: step 2389, loss 0.313619, acc 0.890625, prec 0.0472933, recall 0.84537
2017-12-10T14:23:35.399098: step 2390, loss 0.145533, acc 0.9375, prec 0.0473122, recall 0.845438
2017-12-10T14:23:35.590037: step 2391, loss 0.0962447, acc 0.984375, prec 0.0473346, recall 0.845507
2017-12-10T14:23:35.777854: step 2392, loss 0.210681, acc 0.9375, prec 0.0473299, recall 0.845507
2017-12-10T14:23:35.967377: step 2393, loss 0.139471, acc 0.953125, prec 0.04735, recall 0.845575
2017-12-10T14:23:36.161605: step 2394, loss 0.26222, acc 0.921875, prec 0.0473442, recall 0.845575
2017-12-10T14:23:36.346765: step 2395, loss 0.833725, acc 0.921875, prec 0.0473855, recall 0.845712
2017-12-10T14:23:36.533999: step 2396, loss 4.18661, acc 0.953125, prec 0.0473832, recall 0.845338
2017-12-10T14:23:36.724271: step 2397, loss 1.51049, acc 0.953125, prec 0.047428, recall 0.845101
2017-12-10T14:23:36.916511: step 2398, loss 2.49609, acc 0.890625, prec 0.0474445, recall 0.844797
2017-12-10T14:23:37.107224: step 2399, loss 0.186269, acc 0.9375, prec 0.0474634, recall 0.844866
2017-12-10T14:23:37.296641: step 2400, loss 0.196591, acc 0.921875, prec 0.0474575, recall 0.844866
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_2/1512933336/checkpoints/model-2400

2017-12-10T14:23:38.473512: step 2401, loss 0.300624, acc 0.9375, prec 0.0474764, recall 0.844934
2017-12-10T14:23:38.663573: step 2402, loss 0.361528, acc 0.875, prec 0.0474906, recall 0.845002
2017-12-10T14:23:38.853123: step 2403, loss 0.469282, acc 0.875, prec 0.0474812, recall 0.845002
2017-12-10T14:23:39.044376: step 2404, loss 0.497289, acc 0.8125, prec 0.0474671, recall 0.845002
2017-12-10T14:23:39.233754: step 2405, loss 0.588679, acc 0.796875, prec 0.0474518, recall 0.845002
2017-12-10T14:23:39.422476: step 2406, loss 0.505638, acc 0.859375, prec 0.0474413, recall 0.845002
2017-12-10T14:23:39.611841: step 2407, loss 0.438707, acc 0.859375, prec 0.0475249, recall 0.845275
2017-12-10T14:23:39.802531: step 2408, loss 0.330581, acc 0.859375, prec 0.0475379, recall 0.845343
2017-12-10T14:23:39.987594: step 2409, loss 0.534047, acc 0.828125, prec 0.0475249, recall 0.845343
2017-12-10T14:23:40.176332: step 2410, loss 0.554871, acc 0.859375, prec 0.0475379, recall 0.845411
2017-12-10T14:23:40.360892: step 2411, loss 0.579562, acc 0.84375, prec 0.0475262, recall 0.845411
2017-12-10T14:23:40.550588: step 2412, loss 0.3404, acc 0.921875, prec 0.0475438, recall 0.845478
2017-12-10T14:23:40.736257: step 2413, loss 0.584159, acc 0.859375, prec 0.0476038, recall 0.845682
2017-12-10T14:23:40.920863: step 2414, loss 1.19291, acc 0.84375, prec 0.0476155, recall 0.845749
2017-12-10T14:23:41.113488: step 2415, loss 0.549994, acc 0.859375, prec 0.047605, recall 0.845749
2017-12-10T14:23:41.305157: step 2416, loss 0.439038, acc 0.90625, prec 0.0476449, recall 0.845884
2017-12-10T14:23:41.493211: step 2417, loss 0.372463, acc 0.84375, prec 0.0476331, recall 0.845884
2017-12-10T14:23:41.678936: step 2418, loss 0.399184, acc 0.859375, prec 0.047646, recall 0.845952
2017-12-10T14:23:41.866821: step 2419, loss 0.37144, acc 0.90625, prec 0.047639, recall 0.845952
2017-12-10T14:23:42.053475: step 2420, loss 0.457185, acc 0.84375, prec 0.0476742, recall 0.846087
2017-12-10T14:23:42.240247: step 2421, loss 0.253031, acc 0.921875, prec 0.0476683, recall 0.846087
2017-12-10T14:23:42.428854: step 2422, loss 0.406939, acc 0.890625, prec 0.0476601, recall 0.846087
2017-12-10T14:23:42.618005: step 2423, loss 0.519287, acc 0.875, prec 0.0476742, recall 0.846154
2017-12-10T14:23:42.806003: step 2424, loss 6.51387, acc 0.921875, prec 0.0476695, recall 0.845784
2017-12-10T14:23:42.997312: step 2425, loss 0.698597, acc 0.90625, prec 0.0477328, recall 0.845986
2017-12-10T14:23:43.185193: step 2426, loss 0.280481, acc 0.921875, prec 0.0477972, recall 0.846187
2017-12-10T14:23:43.373935: step 2427, loss 7.54426, acc 0.859375, prec 0.0477878, recall 0.845819
2017-12-10T14:23:43.565558: step 2428, loss 1.60915, acc 0.8125, prec 0.0477971, recall 0.845886
2017-12-10T14:23:43.753116: step 2429, loss 0.342443, acc 0.875, prec 0.0477877, recall 0.845886
2017-12-10T14:23:43.950580: step 2430, loss 0.766457, acc 0.8125, prec 0.0477736, recall 0.845886
2017-12-10T14:23:44.143235: step 2431, loss 1.01135, acc 0.734375, prec 0.047777, recall 0.845953
2017-12-10T14:23:44.327205: step 2432, loss 1.07105, acc 0.78125, prec 0.047784, recall 0.84602
2017-12-10T14:23:44.513592: step 2433, loss 0.860378, acc 0.6875, prec 0.0477605, recall 0.84602
2017-12-10T14:23:44.701264: step 2434, loss 0.804588, acc 0.734375, prec 0.047764, recall 0.846087
2017-12-10T14:23:44.886121: step 2435, loss 1.55045, acc 0.59375, prec 0.0477569, recall 0.846154
2017-12-10T14:23:45.073927: step 2436, loss 1.64234, acc 0.640625, prec 0.0477533, recall 0.846221
2017-12-10T14:23:45.263114: step 2437, loss 1.47838, acc 0.546875, prec 0.0477427, recall 0.846287
2017-12-10T14:23:45.452733: step 2438, loss 0.733724, acc 0.765625, prec 0.0477485, recall 0.846354
2017-12-10T14:23:45.647497: step 2439, loss 0.715682, acc 0.78125, prec 0.0477321, recall 0.846354
2017-12-10T14:23:45.840228: step 2440, loss 0.850581, acc 0.78125, prec 0.0477158, recall 0.846354
2017-12-10T14:23:46.025710: step 2441, loss 0.844476, acc 0.828125, prec 0.0477262, recall 0.846421
2017-12-10T14:23:46.212770: step 2442, loss 1.12057, acc 0.640625, prec 0.0476994, recall 0.846421
2017-12-10T14:23:46.397106: step 2443, loss 0.647196, acc 0.796875, prec 0.0477308, recall 0.846554
2017-12-10T14:23:46.585128: step 2444, loss 0.573199, acc 0.8125, prec 0.0477168, recall 0.846554
2017-12-10T14:23:46.774667: step 2445, loss 0.740256, acc 0.75, prec 0.0477679, recall 0.846753
2017-12-10T14:23:46.958398: step 2446, loss 0.667565, acc 0.78125, prec 0.0477748, recall 0.84682
2017-12-10T14:23:47.148078: step 2447, loss 1.1122, acc 0.90625, prec 0.0478375, recall 0.847018
2017-12-10T14:23:47.336989: step 2448, loss 0.703838, acc 0.828125, prec 0.0478479, recall 0.847084
2017-12-10T14:23:47.524977: step 2449, loss 0.328129, acc 0.859375, prec 0.0478607, recall 0.84715
2017-12-10T14:23:47.718684: step 2450, loss 0.383596, acc 0.90625, prec 0.0478769, recall 0.847216
2017-12-10T14:23:47.908469: step 2451, loss 0.223226, acc 0.921875, prec 0.0478943, recall 0.847282
2017-12-10T14:23:48.097854: step 2452, loss 0.228936, acc 0.9375, prec 0.0478896, recall 0.847282
2017-12-10T14:23:48.284799: step 2453, loss 0.111775, acc 0.984375, prec 0.0479116, recall 0.847348
2017-12-10T14:23:48.471690: step 2454, loss 0.307167, acc 0.921875, prec 0.0479058, recall 0.847348
2017-12-10T14:23:48.660495: step 2455, loss 0.467888, acc 0.9375, prec 0.0479475, recall 0.84748
2017-12-10T14:23:48.851582: step 2456, loss 0.227085, acc 0.9375, prec 0.0479429, recall 0.84748
2017-12-10T14:23:49.042231: step 2457, loss 0.233395, acc 0.9375, prec 0.0479614, recall 0.847545
2017-12-10T14:23:49.232588: step 2458, loss 1.62409, acc 0.953125, prec 0.0480275, recall 0.847742
2017-12-10T14:23:49.421329: step 2459, loss 1.34882, acc 0.921875, prec 0.048068, recall 0.847873
2017-12-10T14:23:49.609951: step 2460, loss 6.67438, acc 0.96875, prec 0.04809, recall 0.847574
2017-12-10T14:23:49.801261: step 2461, loss 0.413794, acc 0.921875, prec 0.0481074, recall 0.84764
2017-12-10T14:23:49.990838: step 2462, loss 0.480488, acc 0.875, prec 0.0481675, recall 0.847835
2017-12-10T14:23:50.178884: step 2463, loss 0.221667, acc 0.890625, prec 0.0481825, recall 0.847901
2017-12-10T14:23:50.376492: step 2464, loss 2.29361, acc 0.84375, prec 0.0481951, recall 0.847603
2017-12-10T14:23:50.565492: step 2465, loss 0.652818, acc 0.765625, prec 0.0482007, recall 0.847668
2017-12-10T14:23:50.752013: step 2466, loss 0.520034, acc 0.828125, prec 0.0481878, recall 0.847668
2017-12-10T14:23:50.944553: step 2467, loss 0.673057, acc 0.765625, prec 0.0481934, recall 0.847733
2017-12-10T14:23:51.132354: step 2468, loss 0.743814, acc 0.71875, prec 0.0481954, recall 0.847798
2017-12-10T14:23:51.319662: step 2469, loss 0.777951, acc 0.75, prec 0.0481998, recall 0.847863
2017-12-10T14:23:51.506174: step 2470, loss 0.729125, acc 0.78125, prec 0.0482065, recall 0.847928
2017-12-10T14:23:51.691653: step 2471, loss 0.768301, acc 0.796875, prec 0.0482144, recall 0.847993
2017-12-10T14:23:51.880033: step 2472, loss 0.87026, acc 0.734375, prec 0.0481945, recall 0.847993
2017-12-10T14:23:52.067500: step 2473, loss 0.670784, acc 0.8125, prec 0.0481805, recall 0.847993
2017-12-10T14:23:52.256253: step 2474, loss 1.25111, acc 0.671875, prec 0.048179, recall 0.848058
2017-12-10T14:23:52.443772: step 2475, loss 0.966355, acc 0.734375, prec 0.0481823, recall 0.848123
2017-12-10T14:23:52.628205: step 2476, loss 0.726256, acc 0.875, prec 0.0481729, recall 0.848123
2017-12-10T14:23:52.815149: step 2477, loss 0.761336, acc 0.796875, prec 0.0481808, recall 0.848188
2017-12-10T14:23:53.001817: step 2478, loss 0.751581, acc 0.75, prec 0.0481852, recall 0.848252
2017-12-10T14:23:53.188827: step 2479, loss 0.844902, acc 0.796875, prec 0.04817, recall 0.848252
2017-12-10T14:23:53.377039: step 2480, loss 0.687683, acc 0.8125, prec 0.048156, recall 0.848252
2017-12-10T14:23:53.564065: step 2481, loss 0.515326, acc 0.828125, prec 0.0481662, recall 0.848317
2017-12-10T14:23:53.749146: step 2482, loss 0.330108, acc 0.828125, prec 0.0481534, recall 0.848317
2017-12-10T14:23:53.936624: step 2483, loss 1.32161, acc 0.9375, prec 0.048173, recall 0.84802
2017-12-10T14:23:54.128025: step 2484, loss 0.466251, acc 0.859375, prec 0.0481625, recall 0.84802
2017-12-10T14:23:54.298366: step 2485, loss 0.429909, acc 0.923077, prec 0.0481808, recall 0.848085
2017-12-10T14:23:54.492278: step 2486, loss 0.0825139, acc 0.984375, prec 0.0481797, recall 0.848085
2017-12-10T14:23:54.684133: step 2487, loss 0.112928, acc 0.953125, prec 0.0481762, recall 0.848085
2017-12-10T14:23:54.870311: step 2488, loss 0.581603, acc 0.828125, prec 0.0481864, recall 0.84815
2017-12-10T14:23:55.059461: step 2489, loss 0.126943, acc 0.9375, prec 0.0482047, recall 0.848214
2017-12-10T14:23:55.250982: step 2490, loss 0.202449, acc 0.921875, prec 0.0481989, recall 0.848214
2017-12-10T14:23:55.441601: step 2491, loss 0.31196, acc 0.875, prec 0.0482126, recall 0.848279
2017-12-10T14:23:55.627300: step 2492, loss 0.1759, acc 0.90625, prec 0.0482056, recall 0.848279
2017-12-10T14:23:55.814384: step 2493, loss 0.072115, acc 0.96875, prec 0.0482032, recall 0.848279
2017-12-10T14:23:56.001741: step 2494, loss 0.0186681, acc 1, prec 0.0482262, recall 0.848343
2017-12-10T14:23:56.194388: step 2495, loss 0.109735, acc 0.96875, prec 0.0482469, recall 0.848408
2017-12-10T14:23:56.383219: step 2496, loss 0.311178, acc 1, prec 0.0482928, recall 0.848536
2017-12-10T14:23:56.571684: step 2497, loss 1.49368, acc 0.890625, prec 0.0482859, recall 0.848176
2017-12-10T14:23:56.760490: step 2498, loss 0.209203, acc 0.953125, prec 0.0483283, recall 0.848305
2017-12-10T14:23:56.952954: step 2499, loss 0.0746179, acc 0.96875, prec 0.048326, recall 0.848305
2017-12-10T14:23:57.140059: step 2500, loss 0.216206, acc 0.921875, prec 0.0483201, recall 0.848305
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_2/1512933336/checkpoints/model-2500

2017-12-10T14:23:58.244062: step 2501, loss 0.178278, acc 0.921875, prec 0.0483143, recall 0.848305
2017-12-10T14:23:58.433147: step 2502, loss 0.295052, acc 0.90625, prec 0.0483303, recall 0.848369
2017-12-10T14:23:58.618528: step 2503, loss 1.87905, acc 0.921875, prec 0.0483715, recall 0.848139
2017-12-10T14:23:58.810848: step 2504, loss 0.120928, acc 0.984375, prec 0.0483704, recall 0.848139
2017-12-10T14:23:59.004558: step 2505, loss 0.873218, acc 0.921875, prec 0.0483875, recall 0.848203
2017-12-10T14:23:59.196209: step 2506, loss 0.282461, acc 0.921875, prec 0.0484046, recall 0.848267
2017-12-10T14:23:59.388302: step 2507, loss 0.181305, acc 0.921875, prec 0.0483988, recall 0.848267
2017-12-10T14:23:59.574851: step 2508, loss 0.199443, acc 0.90625, prec 0.0483918, recall 0.848267
2017-12-10T14:23:59.761887: step 2509, loss 0.659495, acc 0.875, prec 0.0484283, recall 0.848395
2017-12-10T14:23:59.949776: step 2510, loss 0.560999, acc 0.875, prec 0.0484648, recall 0.848523
2017-12-10T14:24:00.139563: step 2511, loss 0.248495, acc 0.890625, prec 0.0485025, recall 0.848651
2017-12-10T14:24:00.327146: step 2512, loss 0.658845, acc 0.875, prec 0.048539, recall 0.848778
2017-12-10T14:24:00.514793: step 2513, loss 0.253246, acc 0.875, prec 0.0485526, recall 0.848842
2017-12-10T14:24:00.698517: step 2514, loss 0.341437, acc 0.890625, prec 0.0485444, recall 0.848842
2017-12-10T14:24:00.886544: step 2515, loss 0.197331, acc 0.953125, prec 0.0485638, recall 0.848906
2017-12-10T14:24:01.083446: step 2516, loss 0.413865, acc 0.8125, prec 0.0485727, recall 0.848969
2017-12-10T14:24:01.273113: step 2517, loss 0.298759, acc 0.921875, prec 0.0485668, recall 0.848969
2017-12-10T14:24:01.465010: step 2518, loss 0.189246, acc 0.90625, prec 0.0485827, recall 0.849033
2017-12-10T14:24:01.650728: step 2519, loss 0.432957, acc 0.875, prec 0.0485734, recall 0.849033
2017-12-10T14:24:01.839854: step 2520, loss 0.346832, acc 0.875, prec 0.0485869, recall 0.849096
2017-12-10T14:24:02.022968: step 2521, loss 0.453247, acc 0.84375, prec 0.0485981, recall 0.84916
2017-12-10T14:24:02.213563: step 2522, loss 2.139, acc 0.875, prec 0.0485899, recall 0.848803
2017-12-10T14:24:02.404724: step 2523, loss 0.643437, acc 0.78125, prec 0.0485964, recall 0.848867
2017-12-10T14:24:02.594067: step 2524, loss 0.277625, acc 0.9375, prec 0.0485918, recall 0.848867
2017-12-10T14:24:02.790489: step 2525, loss 0.20261, acc 0.890625, prec 0.0486064, recall 0.84893
2017-12-10T14:24:02.980088: step 2526, loss 0.24168, acc 0.921875, prec 0.0486006, recall 0.84893
2017-12-10T14:24:03.170231: step 2527, loss 0.270796, acc 0.921875, prec 0.0486176, recall 0.848993
2017-12-10T14:24:03.357093: step 2528, loss 0.292208, acc 0.859375, prec 0.0486071, recall 0.848993
2017-12-10T14:24:03.541588: step 2529, loss 0.352365, acc 0.90625, prec 0.048623, recall 0.849057
2017-12-10T14:24:03.734851: step 2530, loss 0.323254, acc 0.921875, prec 0.04864, recall 0.84912
2017-12-10T14:24:03.921722: step 2531, loss 0.125833, acc 0.9375, prec 0.0486353, recall 0.84912
2017-12-10T14:24:04.106098: step 2532, loss 0.272669, acc 0.859375, prec 0.0486248, recall 0.84912
2017-12-10T14:24:04.293903: step 2533, loss 0.193023, acc 0.953125, prec 0.0486213, recall 0.84912
2017-12-10T14:24:04.485274: step 2534, loss 1.30086, acc 0.890625, prec 0.0486359, recall 0.849183
2017-12-10T14:24:04.677477: step 2535, loss 0.271367, acc 0.890625, prec 0.0486278, recall 0.849183
2017-12-10T14:24:04.864451: step 2536, loss 0.24842, acc 0.90625, prec 0.0486208, recall 0.849183
2017-12-10T14:24:05.056065: step 2537, loss 0.275501, acc 0.921875, prec 0.0486834, recall 0.849372
2017-12-10T14:24:05.243681: step 2538, loss 0.235098, acc 0.90625, prec 0.0486992, recall 0.849435
2017-12-10T14:24:05.434143: step 2539, loss 0.382577, acc 0.890625, prec 0.0487138, recall 0.849498
2017-12-10T14:24:05.621174: step 2540, loss 0.134676, acc 0.90625, prec 0.0487068, recall 0.849498
2017-12-10T14:24:05.813248: step 2541, loss 0.315863, acc 0.921875, prec 0.048701, recall 0.849498
2017-12-10T14:24:06.005118: step 2542, loss 0.472032, acc 0.875, prec 0.0486917, recall 0.849498
2017-12-10T14:24:06.192255: step 2543, loss 0.302263, acc 0.921875, prec 0.0487314, recall 0.849624
2017-12-10T14:24:06.382029: step 2544, loss 0.207694, acc 0.9375, prec 0.0487267, recall 0.849624
2017-12-10T14:24:06.570868: step 2545, loss 0.158529, acc 0.953125, prec 0.048746, recall 0.849687
2017-12-10T14:24:06.759371: step 2546, loss 0.0751803, acc 0.984375, prec 0.0487676, recall 0.84975
2017-12-10T14:24:06.952402: step 2547, loss 0.351069, acc 0.890625, prec 0.0487822, recall 0.849812
2017-12-10T14:24:07.141591: step 2548, loss 0.383818, acc 0.9375, prec 0.0488003, recall 0.849875
2017-12-10T14:24:07.332785: step 2549, loss 1.74363, acc 0.90625, prec 0.0488173, recall 0.849583
2017-12-10T14:24:07.523278: step 2550, loss 0.126476, acc 0.953125, prec 0.0488138, recall 0.849583
2017-12-10T14:24:07.719749: step 2551, loss 0.107202, acc 0.96875, prec 0.048857, recall 0.849709
2017-12-10T14:24:07.907138: step 2552, loss 0.108362, acc 0.953125, prec 0.0488762, recall 0.849771
2017-12-10T14:24:08.097845: step 2553, loss 0.398336, acc 0.953125, prec 0.0489182, recall 0.849896
2017-12-10T14:24:08.288552: step 2554, loss 0.141933, acc 0.90625, prec 0.0489112, recall 0.849896
2017-12-10T14:24:08.478194: step 2555, loss 0.218334, acc 0.890625, prec 0.0489258, recall 0.849958
2017-12-10T14:24:08.665958: step 2556, loss 0.35231, acc 0.890625, prec 0.0489631, recall 0.850083
2017-12-10T14:24:08.852391: step 2557, loss 0.28699, acc 0.921875, prec 0.0489572, recall 0.850083
2017-12-10T14:24:09.037918: step 2558, loss 0.529061, acc 0.875, prec 0.0490161, recall 0.85027
2017-12-10T14:24:09.225615: step 2559, loss 0.324492, acc 0.953125, prec 0.049058, recall 0.850394
2017-12-10T14:24:09.418268: step 2560, loss 0.369842, acc 0.859375, prec 0.049093, recall 0.850518
2017-12-10T14:24:09.603846: step 2561, loss 0.336109, acc 0.953125, prec 0.049203, recall 0.850826
2017-12-10T14:24:09.792014: step 2562, loss 0.18732, acc 0.9375, prec 0.0492438, recall 0.85095
2017-12-10T14:24:09.980421: step 2563, loss 0.126373, acc 0.953125, prec 0.049263, recall 0.851011
2017-12-10T14:24:10.165171: step 2564, loss 0.193031, acc 0.90625, prec 0.0492559, recall 0.851011
2017-12-10T14:24:10.353908: step 2565, loss 0.245407, acc 0.9375, prec 0.0492512, recall 0.851011
2017-12-10T14:24:10.545208: step 2566, loss 0.238824, acc 0.96875, prec 0.049317, recall 0.851195
2017-12-10T14:24:10.735734: step 2567, loss 0.14878, acc 0.953125, prec 0.0493134, recall 0.851195
2017-12-10T14:24:10.924485: step 2568, loss 0.275159, acc 0.9375, prec 0.0493314, recall 0.851257
2017-12-10T14:24:11.117458: step 2569, loss 0.285601, acc 0.921875, prec 0.0493482, recall 0.851318
2017-12-10T14:24:11.308947: step 2570, loss 0.291725, acc 0.90625, prec 0.0493639, recall 0.851379
2017-12-10T14:24:11.498110: step 2571, loss 0.0749933, acc 0.96875, prec 0.0493615, recall 0.851379
2017-12-10T14:24:11.685974: step 2572, loss 0.109042, acc 0.953125, prec 0.049358, recall 0.851379
2017-12-10T14:24:11.872016: step 2573, loss 0.21335, acc 0.9375, prec 0.0493759, recall 0.85144
2017-12-10T14:24:12.061386: step 2574, loss 0.424321, acc 0.890625, prec 0.0493677, recall 0.85144
2017-12-10T14:24:12.252720: step 2575, loss 1.20017, acc 0.953125, prec 0.0494322, recall 0.851624
2017-12-10T14:24:12.445656: step 2576, loss 0.817253, acc 0.921875, prec 0.049449, recall 0.851684
2017-12-10T14:24:12.635759: step 2577, loss 0.19175, acc 0.9375, prec 0.0494669, recall 0.851745
2017-12-10T14:24:12.821928: step 2578, loss 0.0789295, acc 0.96875, prec 0.0494646, recall 0.851745
2017-12-10T14:24:13.009221: step 2579, loss 0.229916, acc 0.953125, prec 0.0494837, recall 0.851806
2017-12-10T14:24:13.201755: step 2580, loss 0.251468, acc 0.890625, prec 0.0494754, recall 0.851806
2017-12-10T14:24:13.395366: step 2581, loss 0.228555, acc 0.921875, prec 0.0494922, recall 0.851867
2017-12-10T14:24:13.587568: step 2582, loss 0.14348, acc 0.953125, prec 0.0495113, recall 0.851928
2017-12-10T14:24:13.774948: step 2583, loss 0.406717, acc 0.90625, prec 0.0495722, recall 0.85211
2017-12-10T14:24:13.971610: step 2584, loss 0.622585, acc 0.953125, prec 0.049614, recall 0.852231
2017-12-10T14:24:14.166168: step 2585, loss 0.411771, acc 0.859375, prec 0.049626, recall 0.852291
2017-12-10T14:24:14.354651: step 2586, loss 0.292968, acc 0.921875, prec 0.049688, recall 0.852472
2017-12-10T14:24:14.540896: step 2587, loss 0.256213, acc 0.9375, prec 0.0496832, recall 0.852472
2017-12-10T14:24:14.734764: step 2588, loss 2.94709, acc 0.859375, prec 0.0496738, recall 0.852124
2017-12-10T14:24:14.923842: step 2589, loss 0.246412, acc 0.90625, prec 0.0496667, recall 0.852124
2017-12-10T14:24:15.109219: step 2590, loss 0.411544, acc 0.859375, prec 0.0496786, recall 0.852185
2017-12-10T14:24:15.298465: step 2591, loss 0.168361, acc 0.921875, prec 0.0496954, recall 0.852245
2017-12-10T14:24:15.487848: step 2592, loss 0.36025, acc 0.9375, prec 0.0496906, recall 0.852245
2017-12-10T14:24:15.674800: step 2593, loss 0.566412, acc 0.84375, prec 0.0497014, recall 0.852305
2017-12-10T14:24:15.863419: step 2594, loss 0.308271, acc 0.890625, prec 0.0496931, recall 0.852305
2017-12-10T14:24:16.049764: step 2595, loss 0.179059, acc 0.921875, prec 0.0496872, recall 0.852305
2017-12-10T14:24:16.240651: step 2596, loss 0.45058, acc 0.84375, prec 0.0497432, recall 0.852486
2017-12-10T14:24:16.427497: step 2597, loss 0.315782, acc 0.921875, prec 0.0497373, recall 0.852486
2017-12-10T14:24:16.618756: step 2598, loss 0.278057, acc 0.890625, prec 0.0498194, recall 0.852726
2017-12-10T14:24:16.807953: step 2599, loss 0.511999, acc 0.84375, prec 0.0498301, recall 0.852786
2017-12-10T14:24:16.992435: step 2600, loss 0.136521, acc 0.984375, prec 0.0498515, recall 0.852846
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_2/1512933336/checkpoints/model-2600

2017-12-10T14:24:18.219664: step 2601, loss 0.389596, acc 0.875, prec 0.049842, recall 0.852846
2017-12-10T14:24:18.406090: step 2602, loss 0.146921, acc 0.953125, prec 0.0498385, recall 0.852846
2017-12-10T14:24:18.591582: step 2603, loss 0.386817, acc 0.890625, prec 0.0498302, recall 0.852846
2017-12-10T14:24:18.780714: step 2604, loss 0.683165, acc 0.875, prec 0.0498658, recall 0.852965
2017-12-10T14:24:18.971437: step 2605, loss 0.226229, acc 0.953125, prec 0.0498623, recall 0.852965
2017-12-10T14:24:19.159268: step 2606, loss 0.143601, acc 0.9375, prec 0.0498801, recall 0.853025
2017-12-10T14:24:19.347483: step 2607, loss 0.134601, acc 0.9375, prec 0.0498754, recall 0.853025
2017-12-10T14:24:19.538296: step 2608, loss 0.169512, acc 0.96875, prec 0.0499181, recall 0.853144
2017-12-10T14:24:19.729343: step 2609, loss 0.237575, acc 0.921875, prec 0.0499122, recall 0.853144
2017-12-10T14:24:19.916783: step 2610, loss 0.0624989, acc 0.96875, prec 0.0499324, recall 0.853204
2017-12-10T14:24:20.106286: step 2611, loss 0.362763, acc 0.90625, prec 0.0499478, recall 0.853263
2017-12-10T14:24:20.294057: step 2612, loss 5.00386, acc 0.984375, prec 0.0499929, recall 0.853036
2017-12-10T14:24:20.490354: step 2613, loss 0.081099, acc 0.96875, prec 0.0499905, recall 0.853036
2017-12-10T14:24:20.675731: step 2614, loss 0.144584, acc 0.96875, prec 0.0500332, recall 0.853155
2017-12-10T14:24:20.864208: step 2615, loss 0.15101, acc 0.9375, prec 0.0500285, recall 0.853155
2017-12-10T14:24:21.050105: step 2616, loss 0.439195, acc 0.875, prec 0.050019, recall 0.853155
2017-12-10T14:24:21.239491: step 2617, loss 0.353093, acc 0.875, prec 0.050032, recall 0.853215
2017-12-10T14:24:21.431230: step 2618, loss 0.683745, acc 0.921875, prec 0.0500486, recall 0.853274
2017-12-10T14:24:21.626534: step 2619, loss 0.555898, acc 0.90625, prec 0.050109, recall 0.853452
2017-12-10T14:24:21.818079: step 2620, loss 0.132086, acc 0.90625, prec 0.0501244, recall 0.853511
2017-12-10T14:24:22.004013: step 2621, loss 5.17036, acc 0.875, prec 0.0501611, recall 0.853285
2017-12-10T14:24:22.196119: step 2622, loss 0.629368, acc 0.796875, prec 0.0501457, recall 0.853285
2017-12-10T14:24:22.384613: step 2623, loss 0.482559, acc 0.84375, prec 0.0502013, recall 0.853462
2017-12-10T14:24:22.576640: step 2624, loss 0.722207, acc 0.765625, prec 0.0502284, recall 0.85358
2017-12-10T14:24:22.762988: step 2625, loss 0.933374, acc 0.734375, prec 0.0502082, recall 0.85358
2017-12-10T14:24:22.952618: step 2626, loss 0.566942, acc 0.8125, prec 0.050194, recall 0.85358
2017-12-10T14:24:23.139809: step 2627, loss 0.60507, acc 0.84375, prec 0.0502046, recall 0.853639
2017-12-10T14:24:23.326748: step 2628, loss 0.543285, acc 0.796875, prec 0.0502116, recall 0.853698
2017-12-10T14:24:23.516003: step 2629, loss 0.526918, acc 0.859375, prec 0.0502233, recall 0.853757
2017-12-10T14:24:23.705726: step 2630, loss 0.960996, acc 0.75, prec 0.0502268, recall 0.853815
2017-12-10T14:24:23.891882: step 2631, loss 0.710433, acc 0.875, prec 0.0502622, recall 0.853933
2017-12-10T14:24:24.080241: step 2632, loss 0.496221, acc 0.828125, prec 0.050294, recall 0.85405
2017-12-10T14:24:24.268926: step 2633, loss 0.600404, acc 0.8125, prec 0.0502797, recall 0.85405
2017-12-10T14:24:24.458908: step 2634, loss 0.40531, acc 0.921875, prec 0.0502738, recall 0.85405
2017-12-10T14:24:24.646722: step 2635, loss 0.666695, acc 0.78125, prec 0.050302, recall 0.854167
2017-12-10T14:24:24.833849: step 2636, loss 0.362448, acc 0.84375, prec 0.0503573, recall 0.854342
2017-12-10T14:24:25.024075: step 2637, loss 0.358869, acc 0.828125, prec 0.0503667, recall 0.8544
2017-12-10T14:24:25.210821: step 2638, loss 0.251941, acc 0.921875, prec 0.0503607, recall 0.8544
2017-12-10T14:24:25.396149: step 2639, loss 1.57934, acc 0.8125, prec 0.0503477, recall 0.854058
2017-12-10T14:24:25.583566: step 2640, loss 0.306794, acc 0.921875, prec 0.0503641, recall 0.854117
2017-12-10T14:24:25.771443: step 2641, loss 0.141711, acc 0.96875, prec 0.0503841, recall 0.854175
2017-12-10T14:24:25.963732: step 2642, loss 0.640104, acc 0.890625, prec 0.0504429, recall 0.85435
2017-12-10T14:24:26.154454: step 2643, loss 0.329148, acc 0.90625, prec 0.0504582, recall 0.854408
2017-12-10T14:24:26.345635: step 2644, loss 0.312786, acc 0.90625, prec 0.0504734, recall 0.854466
2017-12-10T14:24:26.533297: step 2645, loss 0.396955, acc 0.875, prec 0.0504863, recall 0.854524
2017-12-10T14:24:26.721639: step 2646, loss 0.740389, acc 0.953125, prec 0.050505, recall 0.854582
2017-12-10T14:24:26.909137: step 2647, loss 0.349191, acc 0.9375, prec 0.0505674, recall 0.854755
2017-12-10T14:24:27.099422: step 2648, loss 0.208149, acc 0.9375, prec 0.0505626, recall 0.854755
2017-12-10T14:24:27.286397: step 2649, loss 0.598622, acc 0.9375, prec 0.0506249, recall 0.854928
2017-12-10T14:24:27.477008: step 2650, loss 0.295211, acc 0.90625, prec 0.0506624, recall 0.855044
2017-12-10T14:24:27.666721: step 2651, loss 0.193411, acc 0.9375, prec 0.05068, recall 0.855101
2017-12-10T14:24:27.855689: step 2652, loss 0.232819, acc 0.921875, prec 0.050674, recall 0.855101
2017-12-10T14:24:28.044627: step 2653, loss 0.268297, acc 0.96875, prec 0.050694, recall 0.855159
2017-12-10T14:24:28.233752: step 2654, loss 0.174208, acc 0.9375, prec 0.0507115, recall 0.855216
2017-12-10T14:24:28.420610: step 2655, loss 0.381207, acc 0.859375, prec 0.0507008, recall 0.855216
2017-12-10T14:24:28.610461: step 2656, loss 0.152195, acc 0.921875, prec 0.0507171, recall 0.855274
2017-12-10T14:24:28.796851: step 2657, loss 0.25076, acc 0.890625, prec 0.0507311, recall 0.855331
2017-12-10T14:24:28.992754: step 2658, loss 0.444113, acc 0.890625, prec 0.0507228, recall 0.855331
2017-12-10T14:24:29.186113: step 2659, loss 0.193525, acc 0.9375, prec 0.0507626, recall 0.855446
2017-12-10T14:24:29.374238: step 2660, loss 0.379388, acc 0.90625, prec 0.0507778, recall 0.855503
2017-12-10T14:24:29.560767: step 2661, loss 0.0511927, acc 0.96875, prec 0.0507754, recall 0.855503
2017-12-10T14:24:29.747851: step 2662, loss 0.316391, acc 0.875, prec 0.0507658, recall 0.855503
2017-12-10T14:24:29.935344: step 2663, loss 0.33263, acc 0.90625, prec 0.050781, recall 0.85556
2017-12-10T14:24:30.125058: step 2664, loss 0.225393, acc 0.953125, prec 0.0507997, recall 0.855617
2017-12-10T14:24:30.311802: step 2665, loss 0.28451, acc 0.90625, prec 0.0508148, recall 0.855674
2017-12-10T14:24:30.499555: step 2666, loss 0.518109, acc 0.921875, prec 0.0508534, recall 0.855788
2017-12-10T14:24:30.690850: step 2667, loss 0.147098, acc 0.9375, prec 0.0508487, recall 0.855788
2017-12-10T14:24:30.878419: step 2668, loss 0.251992, acc 0.9375, prec 0.0508662, recall 0.855845
2017-12-10T14:24:31.070639: step 2669, loss 0.305566, acc 0.890625, prec 0.0508801, recall 0.855902
2017-12-10T14:24:31.262526: step 2670, loss 0.268067, acc 0.921875, prec 0.0508741, recall 0.855902
2017-12-10T14:24:31.452907: step 2671, loss 0.156402, acc 0.953125, prec 0.0508928, recall 0.855959
2017-12-10T14:24:31.638292: step 2672, loss 0.270654, acc 0.90625, prec 0.0509524, recall 0.856129
2017-12-10T14:24:31.827358: step 2673, loss 0.134114, acc 0.96875, prec 0.0509723, recall 0.856186
2017-12-10T14:24:32.012875: step 2674, loss 0.0464022, acc 0.984375, prec 0.0509934, recall 0.856243
2017-12-10T14:24:32.202618: step 2675, loss 0.330842, acc 0.984375, prec 0.0510367, recall 0.856356
2017-12-10T14:24:32.393510: step 2676, loss 0.104664, acc 0.953125, prec 0.0510331, recall 0.856356
2017-12-10T14:24:32.578690: step 2677, loss 0.019295, acc 1, prec 0.0510331, recall 0.856356
2017-12-10T14:24:32.770656: step 2678, loss 0.187058, acc 0.953125, prec 0.0510295, recall 0.856356
2017-12-10T14:24:32.956230: step 2679, loss 0.0356726, acc 0.984375, prec 0.0510283, recall 0.856356
2017-12-10T14:24:33.146389: step 2680, loss 0.198778, acc 0.953125, prec 0.051047, recall 0.856412
2017-12-10T14:24:33.334548: step 2681, loss 0.109483, acc 0.96875, prec 0.0510446, recall 0.856412
2017-12-10T14:24:33.523341: step 2682, loss 0.217499, acc 0.984375, prec 0.0510879, recall 0.856525
2017-12-10T14:24:33.710309: step 2683, loss 0.179716, acc 0.96875, prec 0.0511077, recall 0.856582
2017-12-10T14:24:33.900232: step 2684, loss 0.166352, acc 0.9375, prec 0.0511029, recall 0.856582
2017-12-10T14:24:34.090793: step 2685, loss 1.59403, acc 0.984375, prec 0.0511474, recall 0.856358
2017-12-10T14:24:34.281992: step 2686, loss 0.161356, acc 0.9375, prec 0.0511871, recall 0.856471
2017-12-10T14:24:34.468572: step 2687, loss 0.149147, acc 0.953125, prec 0.0512057, recall 0.856527
2017-12-10T14:24:34.654815: step 2688, loss 0.107751, acc 0.953125, prec 0.0512021, recall 0.856527
2017-12-10T14:24:34.841453: step 2689, loss 1.46579, acc 0.953125, prec 0.051243, recall 0.856639
2017-12-10T14:24:35.030113: step 2690, loss 0.058049, acc 0.96875, prec 0.0512628, recall 0.856695
2017-12-10T14:24:35.217139: step 2691, loss 0.257574, acc 0.953125, prec 0.0513037, recall 0.856808
2017-12-10T14:24:35.409339: step 2692, loss 0.0959574, acc 0.9375, prec 0.0512989, recall 0.856808
2017-12-10T14:24:35.596281: step 2693, loss 0.16256, acc 0.9375, prec 0.0513385, recall 0.856919
2017-12-10T14:24:35.785437: step 2694, loss 0.273744, acc 0.90625, prec 0.0513535, recall 0.856975
2017-12-10T14:24:35.973141: step 2695, loss 0.143409, acc 0.96875, prec 0.0513511, recall 0.856975
2017-12-10T14:24:36.160362: step 2696, loss 0.452792, acc 0.84375, prec 0.0513613, recall 0.857031
2017-12-10T14:24:36.348420: step 2697, loss 0.108595, acc 0.9375, prec 0.0513565, recall 0.857031
2017-12-10T14:24:36.535998: step 2698, loss 0.383939, acc 0.875, prec 0.0513469, recall 0.857031
2017-12-10T14:24:36.727490: step 2699, loss 0.367691, acc 0.875, prec 0.0514038, recall 0.857199
2017-12-10T14:24:36.918157: step 2700, loss 0.28606, acc 0.921875, prec 0.0513978, recall 0.857199
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_2/1512933336/checkpoints/model-2700

2017-12-10T14:24:38.173293: step 2701, loss 0.174308, acc 0.9375, prec 0.0514152, recall 0.857254
2017-12-10T14:24:38.360177: step 2702, loss 0.629231, acc 0.953125, prec 0.0514338, recall 0.85731
2017-12-10T14:24:38.549834: step 2703, loss 0.762256, acc 0.890625, prec 0.0514475, recall 0.857366
2017-12-10T14:24:38.740173: step 2704, loss 0.174843, acc 0.921875, prec 0.0514415, recall 0.857366
2017-12-10T14:24:38.925982: step 2705, loss 1.30826, acc 0.875, prec 0.0514763, recall 0.857477
2017-12-10T14:24:39.113407: step 2706, loss 0.506583, acc 0.859375, prec 0.0515098, recall 0.857588
2017-12-10T14:24:39.305824: step 2707, loss 0.308888, acc 0.90625, prec 0.0515025, recall 0.857588
2017-12-10T14:24:39.492673: step 2708, loss 0.355285, acc 0.90625, prec 0.0515175, recall 0.857643
2017-12-10T14:24:39.684640: step 2709, loss 0.492136, acc 0.8125, prec 0.0515252, recall 0.857698
2017-12-10T14:24:39.872040: step 2710, loss 0.167364, acc 0.9375, prec 0.0515425, recall 0.857754
2017-12-10T14:24:40.058477: step 2711, loss 0.484354, acc 0.84375, prec 0.0515748, recall 0.857864
2017-12-10T14:24:40.243821: step 2712, loss 0.294117, acc 0.90625, prec 0.0515897, recall 0.857919
2017-12-10T14:24:40.434131: step 2713, loss 0.334333, acc 0.890625, prec 0.0516255, recall 0.858029
2017-12-10T14:24:40.619937: step 2714, loss 0.171585, acc 0.921875, prec 0.0516195, recall 0.858029
2017-12-10T14:24:40.804077: step 2715, loss 0.193334, acc 0.9375, prec 0.0516147, recall 0.858029
2017-12-10T14:24:40.987616: step 2716, loss 0.954184, acc 0.953125, prec 0.0516332, recall 0.858084
2017-12-10T14:24:41.177985: step 2717, loss 0.261862, acc 0.90625, prec 0.0516924, recall 0.858249
2017-12-10T14:24:41.364258: step 2718, loss 0.278474, acc 0.9375, prec 0.051776, recall 0.858469
2017-12-10T14:24:41.556418: step 2719, loss 0.639237, acc 0.84375, prec 0.051786, recall 0.858523
2017-12-10T14:24:41.742256: step 2720, loss 0.455208, acc 0.859375, prec 0.0517752, recall 0.858523
2017-12-10T14:24:41.929431: step 2721, loss 0.36962, acc 0.953125, prec 0.0517937, recall 0.858578
2017-12-10T14:24:42.118097: step 2722, loss 0.448308, acc 0.828125, prec 0.0517804, recall 0.858578
2017-12-10T14:24:42.304304: step 2723, loss 0.300731, acc 0.90625, prec 0.0517731, recall 0.858578
2017-12-10T14:24:42.492958: step 2724, loss 0.5521, acc 0.84375, prec 0.0517611, recall 0.858578
2017-12-10T14:24:42.684445: step 2725, loss 0.241129, acc 0.875, prec 0.0517514, recall 0.858578
2017-12-10T14:24:42.872154: step 2726, loss 0.243481, acc 0.921875, prec 0.0517675, recall 0.858633
2017-12-10T14:24:43.058544: step 2727, loss 0.254473, acc 0.90625, prec 0.0517823, recall 0.858687
2017-12-10T14:24:43.246545: step 2728, loss 0.125644, acc 0.9375, prec 0.0517775, recall 0.858687
2017-12-10T14:24:43.434475: step 2729, loss 0.373846, acc 0.921875, prec 0.0517936, recall 0.858742
2017-12-10T14:24:43.622493: step 2730, loss 2.62181, acc 0.921875, prec 0.0517887, recall 0.85841
2017-12-10T14:24:43.814892: step 2731, loss 0.343087, acc 0.859375, prec 0.0517779, recall 0.85841
2017-12-10T14:24:44.008767: step 2732, loss 0.40161, acc 0.90625, prec 0.0517927, recall 0.858465
2017-12-10T14:24:44.196514: step 2733, loss 0.266919, acc 0.859375, prec 0.051804, recall 0.85852
2017-12-10T14:24:44.381009: step 2734, loss 0.407343, acc 0.859375, prec 0.0517931, recall 0.85852
2017-12-10T14:24:44.568663: step 2735, loss 0.274541, acc 0.9375, prec 0.0518103, recall 0.858574
2017-12-10T14:24:44.755340: step 2736, loss 0.42221, acc 0.90625, prec 0.0518252, recall 0.858629
2017-12-10T14:24:44.941032: step 2737, loss 0.395235, acc 0.84375, prec 0.0518131, recall 0.858629
2017-12-10T14:24:45.129594: step 2738, loss 0.4566, acc 0.90625, prec 0.0518059, recall 0.858629
2017-12-10T14:24:45.318441: step 2739, loss 0.370679, acc 0.921875, prec 0.0518439, recall 0.858737
2017-12-10T14:24:45.502964: step 2740, loss 0.223949, acc 0.90625, prec 0.0518587, recall 0.858792
2017-12-10T14:24:45.690035: step 2741, loss 0.152713, acc 0.96875, prec 0.0519004, recall 0.8589
2017-12-10T14:24:45.879464: step 2742, loss 0.349213, acc 0.90625, prec 0.0519152, recall 0.858955
2017-12-10T14:24:46.069663: step 2743, loss 0.101962, acc 0.9375, prec 0.0519324, recall 0.859009
2017-12-10T14:24:46.257665: step 2744, loss 0.333318, acc 0.9375, prec 0.0519496, recall 0.859063
2017-12-10T14:24:46.447708: step 2745, loss 0.127228, acc 0.9375, prec 0.0519447, recall 0.859063
2017-12-10T14:24:46.634050: step 2746, loss 0.36023, acc 0.921875, prec 0.0519827, recall 0.859171
2017-12-10T14:24:46.825401: step 2747, loss 1.14985, acc 0.90625, prec 0.0520195, recall 0.859279
2017-12-10T14:24:47.009664: step 2748, loss 0.0572625, acc 0.96875, prec 0.0520171, recall 0.859279
2017-12-10T14:24:47.203405: step 2749, loss 0.233345, acc 0.90625, prec 0.0520098, recall 0.859279
2017-12-10T14:24:47.391512: step 2750, loss 0.0758648, acc 0.96875, prec 0.0520074, recall 0.859279
2017-12-10T14:24:47.584833: step 2751, loss 0.329205, acc 0.9375, prec 0.0520246, recall 0.859333
2017-12-10T14:24:47.772943: step 2752, loss 0.533343, acc 0.84375, prec 0.0520125, recall 0.859333
2017-12-10T14:24:47.959577: step 2753, loss 0.154631, acc 0.921875, prec 0.0520065, recall 0.859333
2017-12-10T14:24:48.148691: step 2754, loss 0.373592, acc 0.84375, prec 0.0519944, recall 0.859333
2017-12-10T14:24:48.336835: step 2755, loss 0.286664, acc 0.921875, prec 0.0519884, recall 0.859333
2017-12-10T14:24:48.530776: step 2756, loss 0.0901072, acc 0.96875, prec 0.051986, recall 0.859333
2017-12-10T14:24:48.721089: step 2757, loss 0.478401, acc 0.890625, prec 0.0519995, recall 0.859387
2017-12-10T14:24:48.911634: step 2758, loss 0.311738, acc 0.953125, prec 0.0519959, recall 0.859387
2017-12-10T14:24:49.102254: step 2759, loss 0.586556, acc 0.953125, prec 0.0520143, recall 0.859441
2017-12-10T14:24:49.297590: step 2760, loss 0.21189, acc 0.9375, prec 0.0520095, recall 0.859441
2017-12-10T14:24:49.488354: step 2761, loss 0.0451004, acc 0.984375, prec 0.0520083, recall 0.859441
2017-12-10T14:24:49.674250: step 2762, loss 0.124726, acc 0.953125, prec 0.0520486, recall 0.859548
2017-12-10T14:24:49.861596: step 2763, loss 0.0475087, acc 1, prec 0.0520486, recall 0.859548
2017-12-10T14:24:50.048970: step 2764, loss 0.196007, acc 0.953125, prec 0.0520669, recall 0.859602
2017-12-10T14:24:50.238883: step 2765, loss 0.680434, acc 0.953125, prec 0.0521072, recall 0.85971
2017-12-10T14:24:50.432018: step 2766, loss 0.202166, acc 0.9375, prec 0.0521024, recall 0.85971
2017-12-10T14:24:50.619021: step 2767, loss 0.182426, acc 0.9375, prec 0.0520976, recall 0.85971
2017-12-10T14:24:50.807701: step 2768, loss 0.312627, acc 0.96875, prec 0.0521391, recall 0.859817
2017-12-10T14:24:50.997911: step 2769, loss 0.117119, acc 0.953125, prec 0.0521794, recall 0.859924
2017-12-10T14:24:51.182779: step 2770, loss 0.193166, acc 0.9375, prec 0.0521965, recall 0.859977
2017-12-10T14:24:51.374073: step 2771, loss 0.251999, acc 0.953125, prec 0.0522148, recall 0.860031
2017-12-10T14:24:51.564594: step 2772, loss 0.374152, acc 0.890625, prec 0.0522063, recall 0.860031
2017-12-10T14:24:51.753094: step 2773, loss 0.103837, acc 0.953125, prec 0.0522027, recall 0.860031
2017-12-10T14:24:51.940100: step 2774, loss 0.123567, acc 0.96875, prec 0.0522222, recall 0.860084
2017-12-10T14:24:52.128492: step 2775, loss 0.198059, acc 0.953125, prec 0.0522405, recall 0.860137
2017-12-10T14:24:52.318707: step 2776, loss 0.197259, acc 0.96875, prec 0.05226, recall 0.86019
2017-12-10T14:24:52.505104: step 2777, loss 0.00928741, acc 1, prec 0.05226, recall 0.86019
2017-12-10T14:24:52.696356: step 2778, loss 0.182843, acc 0.90625, prec 0.0522528, recall 0.86019
2017-12-10T14:24:52.887108: step 2779, loss 5.07054, acc 0.96875, prec 0.0522954, recall 0.85997
2017-12-10T14:24:53.081310: step 2780, loss 0.159516, acc 0.953125, prec 0.0523357, recall 0.860076
2017-12-10T14:24:53.269482: step 2781, loss 0.104962, acc 0.96875, prec 0.0523552, recall 0.860129
2017-12-10T14:24:53.460329: step 2782, loss 1.15278, acc 0.90625, prec 0.0523698, recall 0.860182
2017-12-10T14:24:53.653881: step 2783, loss 0.912056, acc 0.90625, prec 0.0524064, recall 0.860289
2017-12-10T14:24:53.842204: step 2784, loss 0.828967, acc 0.96875, prec 0.0524478, recall 0.860395
2017-12-10T14:24:54.034564: step 2785, loss 0.236296, acc 0.921875, prec 0.0524417, recall 0.860395
2017-12-10T14:24:54.223429: step 2786, loss 0.157634, acc 0.9375, prec 0.0524369, recall 0.860395
2017-12-10T14:24:54.412971: step 2787, loss 0.313273, acc 0.890625, prec 0.0524503, recall 0.860447
2017-12-10T14:24:54.604064: step 2788, loss 0.271378, acc 0.90625, prec 0.0525087, recall 0.860606
2017-12-10T14:24:54.791708: step 2789, loss 0.340431, acc 0.859375, prec 0.0524978, recall 0.860606
2017-12-10T14:24:54.979458: step 2790, loss 0.502701, acc 0.84375, prec 0.0524857, recall 0.860606
2017-12-10T14:24:55.164262: step 2791, loss 0.563344, acc 0.84375, prec 0.0524736, recall 0.860606
2017-12-10T14:24:55.356030: step 2792, loss 0.414568, acc 0.90625, prec 0.0524663, recall 0.860606
2017-12-10T14:24:55.542639: step 2793, loss 0.518258, acc 0.859375, prec 0.0524773, recall 0.860659
2017-12-10T14:24:55.730063: step 2794, loss 0.3518, acc 0.890625, prec 0.0525125, recall 0.860764
2017-12-10T14:24:55.917401: step 2795, loss 0.633046, acc 0.8125, prec 0.052498, recall 0.860764
2017-12-10T14:24:56.108209: step 2796, loss 0.327056, acc 0.875, prec 0.0524883, recall 0.860764
2017-12-10T14:24:56.291395: step 2797, loss 0.479941, acc 0.828125, prec 0.052475, recall 0.860764
2017-12-10T14:24:56.481851: step 2798, loss 0.382694, acc 0.90625, prec 0.0524896, recall 0.860817
2017-12-10T14:24:56.668706: step 2799, loss 0.397324, acc 0.875, prec 0.0525017, recall 0.86087
2017-12-10T14:24:56.852776: step 2800, loss 0.472411, acc 0.875, prec 0.0525139, recall 0.860922
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_2/1512933336/checkpoints/model-2800

2017-12-10T14:24:57.967439: step 2801, loss 0.406621, acc 0.84375, prec 0.0525236, recall 0.860975
2017-12-10T14:24:58.154133: step 2802, loss 0.59711, acc 0.875, prec 0.0525358, recall 0.861027
2017-12-10T14:24:58.338984: step 2803, loss 0.593686, acc 0.859375, prec 0.0526122, recall 0.861237
2017-12-10T14:24:58.527678: step 2804, loss 0.348901, acc 0.90625, prec 0.0526267, recall 0.861289
2017-12-10T14:24:58.714771: step 2805, loss 0.302156, acc 0.875, prec 0.052617, recall 0.861289
2017-12-10T14:24:58.909206: step 2806, loss 0.209113, acc 0.9375, prec 0.0526122, recall 0.861289
2017-12-10T14:24:59.101727: step 2807, loss 0.282836, acc 0.90625, prec 0.0526267, recall 0.861341
2017-12-10T14:24:59.287992: step 2808, loss 0.291894, acc 0.9375, prec 0.0526437, recall 0.861394
2017-12-10T14:24:59.474837: step 2809, loss 0.1951, acc 0.9375, prec 0.0526607, recall 0.861446
2017-12-10T14:24:59.663464: step 2810, loss 0.307331, acc 0.921875, prec 0.0526546, recall 0.861446
2017-12-10T14:24:59.850800: step 2811, loss 0.134761, acc 0.96875, prec 0.0526522, recall 0.861446
2017-12-10T14:25:00.038152: step 2812, loss 0.238892, acc 0.953125, prec 0.0526703, recall 0.861498
2017-12-10T14:25:00.226390: step 2813, loss 3.12858, acc 0.953125, prec 0.0526679, recall 0.861174
2017-12-10T14:25:00.420693: step 2814, loss 0.0962799, acc 0.96875, prec 0.0526655, recall 0.861174
2017-12-10T14:25:00.608610: step 2815, loss 0.692839, acc 0.984375, prec 0.0527079, recall 0.861278
2017-12-10T14:25:00.803417: step 2816, loss 0.188289, acc 0.921875, prec 0.0527018, recall 0.861278
2017-12-10T14:25:00.994909: step 2817, loss 0.172981, acc 0.9375, prec 0.052697, recall 0.861278
2017-12-10T14:25:01.191229: step 2818, loss 0.241373, acc 0.890625, prec 0.0527103, recall 0.86133
2017-12-10T14:25:01.381937: step 2819, loss 0.478386, acc 0.90625, prec 0.0527248, recall 0.861382
2017-12-10T14:25:01.573564: step 2820, loss 0.943356, acc 0.890625, prec 0.0527381, recall 0.861434
2017-12-10T14:25:01.764479: step 2821, loss 0.42423, acc 0.875, prec 0.0527284, recall 0.861434
2017-12-10T14:25:01.956685: step 2822, loss 0.367686, acc 0.890625, prec 0.0527634, recall 0.861538
2017-12-10T14:25:02.149243: step 2823, loss 0.364181, acc 0.890625, prec 0.0527767, recall 0.86159
2017-12-10T14:25:02.334980: step 2824, loss 0.278065, acc 0.890625, prec 0.0527682, recall 0.86159
2017-12-10T14:25:02.524436: step 2825, loss 0.416234, acc 0.890625, prec 0.052825, recall 0.861746
2017-12-10T14:25:02.712755: step 2826, loss 0.637167, acc 0.90625, prec 0.0528395, recall 0.861798
2017-12-10T14:25:02.903098: step 2827, loss 0.460228, acc 0.875, prec 0.0528515, recall 0.861849
2017-12-10T14:25:03.092237: step 2828, loss 0.432832, acc 0.875, prec 0.052907, recall 0.862004
2017-12-10T14:25:03.280179: step 2829, loss 0.162244, acc 0.96875, prec 0.0529263, recall 0.862056
2017-12-10T14:25:03.469577: step 2830, loss 0.0325635, acc 1, prec 0.0529915, recall 0.862211
2017-12-10T14:25:03.658476: step 2831, loss 0.372919, acc 0.875, prec 0.0530035, recall 0.862262
2017-12-10T14:25:03.850372: step 2832, loss 0.199642, acc 0.90625, prec 0.0529962, recall 0.862262
2017-12-10T14:25:04.040511: step 2833, loss 0.219716, acc 0.953125, prec 0.0529926, recall 0.862262
2017-12-10T14:25:04.229360: step 2834, loss 0.422911, acc 0.875, prec 0.0530046, recall 0.862313
2017-12-10T14:25:04.418183: step 2835, loss 0.817282, acc 0.90625, prec 0.0530407, recall 0.862416
2017-12-10T14:25:04.605818: step 2836, loss 0.289106, acc 0.890625, prec 0.0530539, recall 0.862467
2017-12-10T14:25:04.794968: step 2837, loss 0.115738, acc 0.96875, prec 0.0530515, recall 0.862467
2017-12-10T14:25:04.981881: step 2838, loss 0.299235, acc 0.921875, prec 0.0530888, recall 0.86257
2017-12-10T14:25:05.175429: step 2839, loss 0.161833, acc 0.90625, prec 0.0530815, recall 0.86257
2017-12-10T14:25:05.366056: step 2840, loss 0.267018, acc 0.890625, prec 0.0531164, recall 0.862672
2017-12-10T14:25:05.560233: step 2841, loss 0.22822, acc 0.9375, prec 0.0531115, recall 0.862672
2017-12-10T14:25:05.744637: step 2842, loss 0.19896, acc 0.921875, prec 0.0531055, recall 0.862672
2017-12-10T14:25:05.933388: step 2843, loss 0.31205, acc 0.890625, prec 0.0531403, recall 0.862774
2017-12-10T14:25:06.126285: step 2844, loss 0.10325, acc 0.953125, prec 0.0531584, recall 0.862825
2017-12-10T14:25:06.311323: step 2845, loss 0.364068, acc 0.90625, prec 0.0531727, recall 0.862876
2017-12-10T14:25:06.500833: step 2846, loss 0.787882, acc 0.921875, prec 0.0531883, recall 0.862927
2017-12-10T14:25:06.695879: step 2847, loss 0.21502, acc 0.9375, prec 0.0532051, recall 0.862978
2017-12-10T14:25:06.882634: step 2848, loss 1.23968, acc 0.9375, prec 0.0532015, recall 0.862658
2017-12-10T14:25:07.071244: step 2849, loss 0.187096, acc 0.921875, prec 0.0531954, recall 0.862658
2017-12-10T14:25:07.261009: step 2850, loss 0.0579059, acc 0.984375, prec 0.0532375, recall 0.86276
2017-12-10T14:25:07.447929: step 2851, loss 0.328827, acc 0.875, prec 0.0532278, recall 0.86276
2017-12-10T14:25:07.639796: step 2852, loss 0.227637, acc 0.921875, prec 0.0532433, recall 0.862811
2017-12-10T14:25:07.823509: step 2853, loss 0.203649, acc 0.90625, prec 0.0532577, recall 0.862861
2017-12-10T14:25:08.013455: step 2854, loss 0.128309, acc 0.96875, prec 0.0532769, recall 0.862912
2017-12-10T14:25:08.203700: step 2855, loss 0.0543549, acc 0.96875, prec 0.0532745, recall 0.862912
2017-12-10T14:25:08.388790: step 2856, loss 0.371117, acc 0.90625, prec 0.0532888, recall 0.862963
2017-12-10T14:25:08.576809: step 2857, loss 0.305, acc 0.9375, prec 0.0533056, recall 0.863014
2017-12-10T14:25:08.762095: step 2858, loss 0.303199, acc 0.890625, prec 0.0532971, recall 0.863014
2017-12-10T14:25:08.951964: step 2859, loss 0.807127, acc 0.953125, prec 0.0533367, recall 0.863115
2017-12-10T14:25:09.140113: step 2860, loss 0.612474, acc 0.890625, prec 0.0533498, recall 0.863166
2017-12-10T14:25:09.328955: step 2861, loss 0.296461, acc 0.921875, prec 0.0533437, recall 0.863166
2017-12-10T14:25:09.519070: step 2862, loss 0.0809808, acc 0.96875, prec 0.0533413, recall 0.863166
2017-12-10T14:25:09.708610: step 2863, loss 3.83013, acc 0.921875, prec 0.0533796, recall 0.862948
2017-12-10T14:25:09.903559: step 2864, loss 0.48429, acc 0.875, prec 0.0533915, recall 0.862999
2017-12-10T14:25:10.091513: step 2865, loss 0.179301, acc 0.921875, prec 0.0533854, recall 0.862999
2017-12-10T14:25:10.277409: step 2866, loss 0.425082, acc 0.859375, prec 0.0533744, recall 0.862999
2017-12-10T14:25:10.462886: step 2867, loss 0.443083, acc 0.859375, prec 0.0533635, recall 0.862999
2017-12-10T14:25:10.652599: step 2868, loss 0.385227, acc 0.859375, prec 0.0533741, recall 0.863049
2017-12-10T14:25:10.839776: step 2869, loss 0.176402, acc 0.9375, prec 0.0533692, recall 0.863049
2017-12-10T14:25:11.024691: step 2870, loss 0.423121, acc 0.859375, prec 0.0533583, recall 0.863049
2017-12-10T14:25:11.214425: step 2871, loss 0.148173, acc 0.96875, prec 0.0533775, recall 0.8631
2017-12-10T14:25:11.406453: step 2872, loss 0.291851, acc 0.859375, prec 0.0533881, recall 0.86315
2017-12-10T14:25:11.589219: step 2873, loss 0.707446, acc 0.828125, prec 0.0533963, recall 0.863201
2017-12-10T14:25:11.779291: step 2874, loss 0.699347, acc 0.84375, prec 0.0533841, recall 0.863201
2017-12-10T14:25:11.966108: step 2875, loss 0.334483, acc 0.875, prec 0.053396, recall 0.863251
2017-12-10T14:25:12.156956: step 2876, loss 0.30681, acc 0.875, prec 0.0534078, recall 0.863301
2017-12-10T14:25:12.343299: step 2877, loss 0.308484, acc 0.90625, prec 0.0534221, recall 0.863352
2017-12-10T14:25:12.534558: step 2878, loss 0.332796, acc 0.90625, prec 0.0534363, recall 0.863402
2017-12-10T14:25:12.721189: step 2879, loss 0.352695, acc 0.921875, prec 0.0534302, recall 0.863402
2017-12-10T14:25:12.907504: step 2880, loss 0.217299, acc 0.90625, prec 0.0534445, recall 0.863452
2017-12-10T14:25:13.097847: step 2881, loss 0.929143, acc 0.859375, prec 0.0534767, recall 0.863553
2017-12-10T14:25:13.289212: step 2882, loss 0.225083, acc 0.90625, prec 0.0535125, recall 0.863653
2017-12-10T14:25:13.477926: step 2883, loss 0.0913607, acc 0.96875, prec 0.05351, recall 0.863653
2017-12-10T14:25:13.666389: step 2884, loss 0.0439151, acc 0.984375, prec 0.0535088, recall 0.863653
2017-12-10T14:25:13.851529: step 2885, loss 0.190664, acc 0.9375, prec 0.0535039, recall 0.863653
2017-12-10T14:25:14.048274: step 2886, loss 0.249518, acc 0.9375, prec 0.0535422, recall 0.863753
2017-12-10T14:25:14.237573: step 2887, loss 0.317591, acc 0.90625, prec 0.0535779, recall 0.863853
2017-12-10T14:25:14.428552: step 2888, loss 0.163266, acc 0.953125, prec 0.0535743, recall 0.863853
2017-12-10T14:25:14.617312: step 2889, loss 0.194332, acc 0.9375, prec 0.0535909, recall 0.863903
2017-12-10T14:25:14.807976: step 2890, loss 0.125884, acc 0.953125, prec 0.0535873, recall 0.863903
2017-12-10T14:25:14.997292: step 2891, loss 0.116584, acc 0.953125, prec 0.0535836, recall 0.863903
2017-12-10T14:25:15.187112: step 2892, loss 0.356499, acc 0.96875, prec 0.0536027, recall 0.863953
2017-12-10T14:25:15.377771: step 2893, loss 0.221938, acc 0.953125, prec 0.0536421, recall 0.864053
2017-12-10T14:25:15.573581: step 2894, loss 0.0635237, acc 0.96875, prec 0.0536397, recall 0.864053
2017-12-10T14:25:15.763818: step 2895, loss 0.983911, acc 0.96875, prec 0.0536588, recall 0.864103
2017-12-10T14:25:15.955558: step 2896, loss 0.132465, acc 0.953125, prec 0.0536766, recall 0.864152
2017-12-10T14:25:16.143304: step 2897, loss 1.62807, acc 0.9375, prec 0.053673, recall 0.863836
2017-12-10T14:25:16.334404: step 2898, loss 0.0673503, acc 0.984375, prec 0.0536717, recall 0.863836
2017-12-10T14:25:16.523499: step 2899, loss 0.201043, acc 0.953125, prec 0.0536896, recall 0.863886
2017-12-10T14:25:16.711747: step 2900, loss 0.93857, acc 0.921875, prec 0.0537265, recall 0.863985
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_2/1512933336/checkpoints/model-2900

2017-12-10T14:25:17.777602: step 2901, loss 0.402628, acc 0.90625, prec 0.0537837, recall 0.864134
2017-12-10T14:25:17.965989: step 2902, loss 0.233724, acc 0.921875, prec 0.0538206, recall 0.864234
2017-12-10T14:25:18.151777: step 2903, loss 0.307185, acc 0.890625, prec 0.0538551, recall 0.864333
2017-12-10T14:25:18.340040: step 2904, loss 0.138905, acc 0.953125, prec 0.0538729, recall 0.864382
2017-12-10T14:25:18.528851: step 2905, loss 3.30924, acc 0.84375, prec 0.0538619, recall 0.864067
2017-12-10T14:25:18.723219: step 2906, loss 0.239394, acc 0.90625, prec 0.053876, recall 0.864117
2017-12-10T14:25:18.911083: step 2907, loss 0.550322, acc 0.828125, prec 0.0539055, recall 0.864215
2017-12-10T14:25:19.103243: step 2908, loss 0.700668, acc 0.796875, prec 0.0538896, recall 0.864215
2017-12-10T14:25:19.296179: step 2909, loss 0.542298, acc 0.859375, prec 0.0538786, recall 0.864215
2017-12-10T14:25:19.481491: step 2910, loss 0.686755, acc 0.828125, prec 0.0539296, recall 0.864364
2017-12-10T14:25:19.669789: step 2911, loss 1.00769, acc 0.765625, prec 0.0539112, recall 0.864364
2017-12-10T14:25:19.858880: step 2912, loss 0.62554, acc 0.78125, prec 0.0539156, recall 0.864413
2017-12-10T14:25:20.048075: step 2913, loss 0.247478, acc 0.875, prec 0.0539487, recall 0.864511
2017-12-10T14:25:20.234498: step 2914, loss 0.308522, acc 0.90625, prec 0.0539413, recall 0.864511
2017-12-10T14:25:20.423190: step 2915, loss 0.795835, acc 0.84375, prec 0.053972, recall 0.86461
2017-12-10T14:25:20.616859: step 2916, loss 0.629181, acc 0.78125, prec 0.0539549, recall 0.86461
2017-12-10T14:25:20.807157: step 2917, loss 0.249733, acc 0.90625, prec 0.053969, recall 0.864659
2017-12-10T14:25:20.992243: step 2918, loss 0.339615, acc 0.90625, prec 0.0539616, recall 0.864659
2017-12-10T14:25:21.179882: step 2919, loss 0.273082, acc 0.859375, prec 0.0539506, recall 0.864659
2017-12-10T14:25:21.373437: step 2920, loss 1.19127, acc 0.953125, prec 0.0539482, recall 0.864345
2017-12-10T14:25:21.560903: step 2921, loss 0.152292, acc 0.921875, prec 0.0539421, recall 0.864345
2017-12-10T14:25:21.748757: step 2922, loss 0.25423, acc 0.921875, prec 0.053936, recall 0.864345
2017-12-10T14:25:21.935749: step 2923, loss 0.446128, acc 0.890625, prec 0.0539489, recall 0.864394
2017-12-10T14:25:22.121534: step 2924, loss 0.325707, acc 0.890625, prec 0.0539403, recall 0.864394
2017-12-10T14:25:22.312687: step 2925, loss 0.363773, acc 0.875, prec 0.053952, recall 0.864444
2017-12-10T14:25:22.504317: step 2926, loss 0.252648, acc 0.90625, prec 0.053966, recall 0.864493
2017-12-10T14:25:22.687675: step 2927, loss 0.338123, acc 0.875, prec 0.0539777, recall 0.864542
2017-12-10T14:25:22.875355: step 2928, loss 0.42637, acc 0.859375, prec 0.0539881, recall 0.864591
2017-12-10T14:25:23.059336: step 2929, loss 0.922827, acc 0.875, prec 0.0540211, recall 0.864689
2017-12-10T14:25:23.251828: step 2930, loss 0.177507, acc 0.96875, prec 0.0540186, recall 0.864689
2017-12-10T14:25:23.440919: step 2931, loss 0.593491, acc 0.921875, prec 0.0540553, recall 0.864787
2017-12-10T14:25:23.631421: step 2932, loss 0.664223, acc 0.9375, prec 0.0540718, recall 0.864836
2017-12-10T14:25:23.825423: step 2933, loss 0.272956, acc 0.890625, prec 0.0540632, recall 0.864836
2017-12-10T14:25:24.017093: step 2934, loss 0.313647, acc 0.890625, prec 0.054076, recall 0.864884
2017-12-10T14:25:24.204540: step 2935, loss 0.348479, acc 0.90625, prec 0.0540901, recall 0.864933
2017-12-10T14:25:24.392868: step 2936, loss 0.239498, acc 0.9375, prec 0.0541065, recall 0.864982
2017-12-10T14:25:24.578954: step 2937, loss 0.165227, acc 0.953125, prec 0.0541456, recall 0.865079
2017-12-10T14:25:24.764708: step 2938, loss 0.358925, acc 0.875, prec 0.0541358, recall 0.865079
2017-12-10T14:25:24.950382: step 2939, loss 0.280405, acc 0.90625, prec 0.0541285, recall 0.865079
2017-12-10T14:25:25.137290: step 2940, loss 2.85958, acc 0.9375, prec 0.0541248, recall 0.864767
2017-12-10T14:25:25.331484: step 2941, loss 0.266526, acc 0.921875, prec 0.0541827, recall 0.864914
2017-12-10T14:25:25.523431: step 2942, loss 1.64724, acc 0.90625, prec 0.0542193, recall 0.8647
2017-12-10T14:25:25.718073: step 2943, loss 0.285704, acc 0.890625, prec 0.0542321, recall 0.864748
2017-12-10T14:25:25.903975: step 2944, loss 1.13018, acc 0.859375, prec 0.0542637, recall 0.864845
2017-12-10T14:25:26.098126: step 2945, loss 0.578156, acc 0.84375, prec 0.0542728, recall 0.864894
2017-12-10T14:25:26.289907: step 2946, loss 0.549837, acc 0.84375, prec 0.0542606, recall 0.864894
2017-12-10T14:25:26.475554: step 2947, loss 0.670707, acc 0.8125, prec 0.0542459, recall 0.864894
2017-12-10T14:25:26.659710: step 2948, loss 0.431416, acc 0.875, prec 0.0542361, recall 0.864894
2017-12-10T14:25:26.850593: step 2949, loss 0.521723, acc 0.84375, prec 0.0542878, recall 0.865039
2017-12-10T14:25:27.036826: step 2950, loss 0.421676, acc 0.921875, prec 0.0543243, recall 0.865136
2017-12-10T14:25:27.222682: step 2951, loss 0.628191, acc 0.859375, prec 0.0543133, recall 0.865136
2017-12-10T14:25:27.414271: step 2952, loss 0.467509, acc 0.875, prec 0.0543461, recall 0.865233
2017-12-10T14:25:27.605014: step 2953, loss 0.7368, acc 0.765625, prec 0.054349, recall 0.865281
2017-12-10T14:25:27.790097: step 2954, loss 0.734716, acc 0.828125, prec 0.0543356, recall 0.865281
2017-12-10T14:25:27.976637: step 2955, loss 0.526592, acc 0.90625, prec 0.0543283, recall 0.865281
2017-12-10T14:25:28.162564: step 2956, loss 0.569266, acc 0.921875, prec 0.0543222, recall 0.865281
2017-12-10T14:25:28.350527: step 2957, loss 0.622653, acc 0.828125, prec 0.0543087, recall 0.865281
2017-12-10T14:25:28.541925: step 2958, loss 0.227856, acc 0.890625, prec 0.0543214, recall 0.86533
2017-12-10T14:25:28.738623: step 2959, loss 0.163566, acc 0.953125, prec 0.054339, recall 0.865378
2017-12-10T14:25:28.933580: step 2960, loss 1.07248, acc 0.84375, prec 0.0543693, recall 0.865474
2017-12-10T14:25:29.126940: step 2961, loss 0.48821, acc 0.875, prec 0.0543808, recall 0.865522
2017-12-10T14:25:29.317284: step 2962, loss 0.360397, acc 0.890625, prec 0.0543722, recall 0.865522
2017-12-10T14:25:29.505817: step 2963, loss 1.47349, acc 0.8125, prec 0.0544001, recall 0.865618
2017-12-10T14:25:29.705498: step 2964, loss 0.421648, acc 0.875, prec 0.0544328, recall 0.865714
2017-12-10T14:25:29.894347: step 2965, loss 2.48814, acc 0.890625, prec 0.0544254, recall 0.865405
2017-12-10T14:25:30.089178: step 2966, loss 0.202064, acc 0.9375, prec 0.0544205, recall 0.865405
2017-12-10T14:25:30.285150: step 2967, loss 0.15063, acc 0.953125, prec 0.0544169, recall 0.865405
2017-12-10T14:25:30.471707: step 2968, loss 0.449362, acc 0.90625, prec 0.0544096, recall 0.865405
2017-12-10T14:25:30.660282: step 2969, loss 0.397792, acc 0.796875, prec 0.0544361, recall 0.865501
2017-12-10T14:25:30.847905: step 2970, loss 0.548795, acc 0.859375, prec 0.0544463, recall 0.865549
2017-12-10T14:25:31.040630: step 2971, loss 0.539902, acc 0.828125, prec 0.0544541, recall 0.865597
2017-12-10T14:25:31.230173: step 2972, loss 0.166501, acc 0.9375, prec 0.0544704, recall 0.865645
2017-12-10T14:25:31.420483: step 2973, loss 1.13005, acc 0.90625, prec 0.0545055, recall 0.865741
2017-12-10T14:25:31.614497: step 2974, loss 0.408701, acc 0.875, prec 0.0545381, recall 0.865836
2017-12-10T14:25:31.803858: step 2975, loss 0.397532, acc 0.875, prec 0.0545495, recall 0.865884
2017-12-10T14:25:31.991877: step 2976, loss 3.3353, acc 0.84375, prec 0.0545385, recall 0.865576
2017-12-10T14:25:32.181050: step 2977, loss 0.555614, acc 0.8125, prec 0.0545874, recall 0.865719
2017-12-10T14:25:32.367354: step 2978, loss 0.578317, acc 0.875, prec 0.0545988, recall 0.865767
2017-12-10T14:25:32.557257: step 2979, loss 0.521518, acc 0.796875, prec 0.0546041, recall 0.865815
2017-12-10T14:25:32.742494: step 2980, loss 0.70922, acc 0.765625, prec 0.0545857, recall 0.865815
2017-12-10T14:25:32.928995: step 2981, loss 0.757046, acc 0.78125, prec 0.0545898, recall 0.865862
2017-12-10T14:25:33.101256: step 2982, loss 0.408487, acc 0.846154, prec 0.0546223, recall 0.865957
2017-12-10T14:25:33.296473: step 2983, loss 0.537357, acc 0.828125, prec 0.0546512, recall 0.866052
2017-12-10T14:25:33.481765: step 2984, loss 0.667795, acc 0.8125, prec 0.0546788, recall 0.866147
2017-12-10T14:25:33.670098: step 2985, loss 0.287654, acc 0.890625, prec 0.0546913, recall 0.866195
2017-12-10T14:25:33.856120: step 2986, loss 0.78852, acc 0.796875, prec 0.0546755, recall 0.866195
2017-12-10T14:25:34.041577: step 2987, loss 0.38395, acc 0.828125, prec 0.0547043, recall 0.866289
2017-12-10T14:25:34.230138: step 2988, loss 0.417915, acc 0.875, prec 0.0547156, recall 0.866337
2017-12-10T14:25:34.415385: step 2989, loss 0.270826, acc 0.90625, prec 0.0547083, recall 0.866337
2017-12-10T14:25:34.602773: step 2990, loss 0.41543, acc 0.859375, prec 0.0547184, recall 0.866384
2017-12-10T14:25:34.794187: step 2991, loss 0.248108, acc 0.921875, prec 0.0547334, recall 0.866431
2017-12-10T14:25:34.979872: step 2992, loss 0.224251, acc 0.921875, prec 0.0547694, recall 0.866525
2017-12-10T14:25:35.170528: step 2993, loss 0.303857, acc 0.921875, prec 0.0548266, recall 0.866667
2017-12-10T14:25:35.363462: step 2994, loss 0.0827508, acc 0.9375, prec 0.0548217, recall 0.866667
2017-12-10T14:25:35.554314: step 2995, loss 0.220243, acc 0.921875, prec 0.0548156, recall 0.866667
2017-12-10T14:25:35.745322: step 2996, loss 0.108913, acc 0.9375, prec 0.0548107, recall 0.866667
2017-12-10T14:25:35.932253: step 2997, loss 0.104657, acc 0.953125, prec 0.0548281, recall 0.866714
2017-12-10T14:25:36.121287: step 2998, loss 4.39933, acc 0.9375, prec 0.0548666, recall 0.866502
2017-12-10T14:25:36.313909: step 2999, loss 0.0591737, acc 0.953125, prec 0.054863, recall 0.866502
2017-12-10T14:25:36.502240: step 3000, loss 0.212185, acc 0.9375, prec 0.0548791, recall 0.866549
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_2/1512933336/checkpoints/model-3000

2017-12-10T14:25:37.630155: step 3001, loss 0.187622, acc 0.9375, prec 0.0548953, recall 0.866596
2017-12-10T14:25:37.819140: step 3002, loss 1.0364, acc 0.84375, prec 0.0549041, recall 0.866643
2017-12-10T14:25:38.012962: step 3003, loss 0.338626, acc 0.875, prec 0.0549154, recall 0.86669
2017-12-10T14:25:38.202171: step 3004, loss 0.280933, acc 0.890625, prec 0.0549279, recall 0.866737
2017-12-10T14:25:38.384887: step 3005, loss 0.630072, acc 0.890625, prec 0.0549404, recall 0.866784
2017-12-10T14:25:38.570842: step 3006, loss 0.204128, acc 0.921875, prec 0.0549343, recall 0.866784
2017-12-10T14:25:38.756915: step 3007, loss 0.353044, acc 0.90625, prec 0.054969, recall 0.866877
2017-12-10T14:25:38.945649: step 3008, loss 0.153843, acc 0.953125, prec 0.0549864, recall 0.866924
2017-12-10T14:25:39.133115: step 3009, loss 0.337754, acc 0.890625, prec 0.0549778, recall 0.866924
2017-12-10T14:25:39.324708: step 3010, loss 0.162812, acc 0.953125, prec 0.0549952, recall 0.866971
2017-12-10T14:25:39.509820: step 3011, loss 0.376604, acc 0.90625, prec 0.0549879, recall 0.866971
2017-12-10T14:25:39.697258: step 3012, loss 1.06586, acc 0.921875, prec 0.0550028, recall 0.867018
2017-12-10T14:25:39.884944: step 3013, loss 0.241892, acc 0.90625, prec 0.0550165, recall 0.867064
2017-12-10T14:25:40.072057: step 3014, loss 0.27347, acc 0.921875, prec 0.0550103, recall 0.867064
2017-12-10T14:25:40.257796: step 3015, loss 0.196612, acc 0.9375, prec 0.0550054, recall 0.867064
2017-12-10T14:25:40.442769: step 3016, loss 0.240275, acc 0.953125, prec 0.0550228, recall 0.867111
2017-12-10T14:25:40.632285: step 3017, loss 0.214226, acc 0.921875, prec 0.0550587, recall 0.867204
2017-12-10T14:25:40.819073: step 3018, loss 0.401469, acc 0.90625, prec 0.0550724, recall 0.86725
2017-12-10T14:25:41.008036: step 3019, loss 0.320796, acc 0.875, prec 0.0550626, recall 0.86725
2017-12-10T14:25:41.194927: step 3020, loss 0.306573, acc 0.921875, prec 0.0550565, recall 0.86725
2017-12-10T14:25:41.379789: step 3021, loss 0.135667, acc 0.9375, prec 0.0550936, recall 0.867343
2017-12-10T14:25:41.569752: step 3022, loss 0.29451, acc 0.953125, prec 0.0551109, recall 0.86739
2017-12-10T14:25:41.760535: step 3023, loss 0.53346, acc 0.84375, prec 0.0551197, recall 0.867436
2017-12-10T14:25:41.945154: step 3024, loss 0.453821, acc 0.84375, prec 0.0551074, recall 0.867436
2017-12-10T14:25:42.132396: step 3025, loss 0.22917, acc 0.921875, prec 0.0551013, recall 0.867436
2017-12-10T14:25:42.320908: step 3026, loss 0.234737, acc 0.921875, prec 0.0550952, recall 0.867436
2017-12-10T14:25:42.511573: step 3027, loss 0.14372, acc 0.953125, prec 0.0551125, recall 0.867483
2017-12-10T14:25:42.698254: step 3028, loss 0.166613, acc 0.96875, prec 0.0551101, recall 0.867483
2017-12-10T14:25:42.885119: step 3029, loss 0.220587, acc 0.921875, prec 0.0551039, recall 0.867483
2017-12-10T14:25:43.073673: step 3030, loss 0.172446, acc 0.984375, prec 0.0551657, recall 0.867621
2017-12-10T14:25:43.265563: step 3031, loss 0.0828143, acc 0.953125, prec 0.055183, recall 0.867668
2017-12-10T14:25:43.453474: step 3032, loss 0.19263, acc 0.9375, prec 0.05522, recall 0.86776
2017-12-10T14:25:43.647323: step 3033, loss 0.0845353, acc 0.96875, prec 0.0552176, recall 0.86776
2017-12-10T14:25:43.833756: step 3034, loss 0.0982479, acc 0.984375, prec 0.0552373, recall 0.867806
2017-12-10T14:25:44.028684: step 3035, loss 0.706431, acc 0.984375, prec 0.0552571, recall 0.867852
2017-12-10T14:25:44.226727: step 3036, loss 0.290845, acc 0.9375, prec 0.0552731, recall 0.867898
2017-12-10T14:25:44.416397: step 3037, loss 0.0299829, acc 0.984375, prec 0.0552719, recall 0.867898
2017-12-10T14:25:44.610990: step 3038, loss 0.0510905, acc 0.96875, prec 0.0552695, recall 0.867898
2017-12-10T14:25:44.799792: step 3039, loss 0.0373449, acc 0.984375, prec 0.0552682, recall 0.867898
2017-12-10T14:25:44.985792: step 3040, loss 0.212777, acc 0.90625, prec 0.0552818, recall 0.867944
2017-12-10T14:25:45.175781: step 3041, loss 0.011573, acc 1, prec 0.0552818, recall 0.867944
2017-12-10T14:25:45.361010: step 3042, loss 0.0836408, acc 0.984375, prec 0.0553226, recall 0.868036
2017-12-10T14:25:45.552129: step 3043, loss 0.126485, acc 0.984375, prec 0.0553423, recall 0.868082
2017-12-10T14:25:45.739206: step 3044, loss 0.0793232, acc 0.96875, prec 0.0553817, recall 0.868174
2017-12-10T14:25:45.925214: step 3045, loss 0.0521633, acc 0.96875, prec 0.0554212, recall 0.868266
2017-12-10T14:25:46.115248: step 3046, loss 0.168185, acc 0.9375, prec 0.0554163, recall 0.868266
2017-12-10T14:25:46.303016: step 3047, loss 0.257151, acc 0.921875, prec 0.0554311, recall 0.868311
2017-12-10T14:25:46.491641: step 3048, loss 0.418198, acc 0.953125, prec 0.0554484, recall 0.868357
2017-12-10T14:25:46.683264: step 3049, loss 0.391044, acc 0.953125, prec 0.0554656, recall 0.868403
2017-12-10T14:25:46.870953: step 3050, loss 0.261497, acc 0.96875, prec 0.0554632, recall 0.868403
2017-12-10T14:25:47.058259: step 3051, loss 0.102551, acc 0.9375, prec 0.0554582, recall 0.868403
2017-12-10T14:25:47.246790: step 3052, loss 0.220026, acc 0.984375, prec 0.055457, recall 0.868403
2017-12-10T14:25:47.436323: step 3053, loss 0.166535, acc 0.984375, prec 0.0554767, recall 0.868448
2017-12-10T14:25:47.628050: step 3054, loss 1.64241, acc 0.921875, prec 0.0554718, recall 0.868147
2017-12-10T14:25:47.819539: step 3055, loss 0.170186, acc 0.953125, prec 0.0554681, recall 0.868147
2017-12-10T14:25:48.006291: step 3056, loss 0.1015, acc 0.96875, prec 0.0554656, recall 0.868147
2017-12-10T14:25:48.197305: step 3057, loss 0.470713, acc 0.921875, prec 0.0554804, recall 0.868193
2017-12-10T14:25:48.383959: step 3058, loss 0.426358, acc 0.96875, prec 0.0554989, recall 0.868239
2017-12-10T14:25:48.570151: step 3059, loss 0.0801653, acc 0.96875, prec 0.0555174, recall 0.868284
2017-12-10T14:25:48.755624: step 3060, loss 0.140409, acc 0.953125, prec 0.0555137, recall 0.868284
2017-12-10T14:25:48.946587: step 3061, loss 4.9116, acc 0.921875, prec 0.0555088, recall 0.867983
2017-12-10T14:25:49.135498: step 3062, loss 0.131691, acc 0.9375, prec 0.0555457, recall 0.868075
2017-12-10T14:25:49.324250: step 3063, loss 0.127927, acc 0.9375, prec 0.0556036, recall 0.868212
2017-12-10T14:25:49.514080: step 3064, loss 0.520282, acc 0.875, prec 0.0556146, recall 0.868257
2017-12-10T14:25:49.710302: step 3065, loss 0.442757, acc 0.890625, prec 0.055606, recall 0.868257
2017-12-10T14:25:49.894752: step 3066, loss 0.549171, acc 0.78125, prec 0.0555888, recall 0.868257
2017-12-10T14:25:50.081623: step 3067, loss 0.690894, acc 0.875, prec 0.0556416, recall 0.868394
2017-12-10T14:25:50.271412: step 3068, loss 0.404258, acc 0.859375, prec 0.0556514, recall 0.868439
2017-12-10T14:25:50.457365: step 3069, loss 0.769855, acc 0.796875, prec 0.0556354, recall 0.868439
2017-12-10T14:25:50.644025: step 3070, loss 0.564997, acc 0.8125, prec 0.0556416, recall 0.868485
2017-12-10T14:25:50.831565: step 3071, loss 0.725171, acc 0.859375, prec 0.0556305, recall 0.868485
2017-12-10T14:25:51.018411: step 3072, loss 0.612828, acc 0.828125, prec 0.055617, recall 0.868485
2017-12-10T14:25:51.206099: step 3073, loss 0.32184, acc 0.890625, prec 0.0556292, recall 0.86853
2017-12-10T14:25:51.395113: step 3074, loss 0.422626, acc 0.8125, prec 0.0556353, recall 0.868575
2017-12-10T14:25:51.585514: step 3075, loss 0.541332, acc 0.875, prec 0.0556464, recall 0.868621
2017-12-10T14:25:51.775577: step 3076, loss 0.650501, acc 0.84375, prec 0.0556341, recall 0.868621
2017-12-10T14:25:51.965902: step 3077, loss 0.487099, acc 0.828125, prec 0.0556414, recall 0.868666
2017-12-10T14:25:52.154926: step 3078, loss 0.461317, acc 0.796875, prec 0.0556463, recall 0.868711
2017-12-10T14:25:52.339842: step 3079, loss 0.38794, acc 0.859375, prec 0.0556352, recall 0.868711
2017-12-10T14:25:52.527873: step 3080, loss 0.259886, acc 0.921875, prec 0.0556291, recall 0.868711
2017-12-10T14:25:52.718091: step 3081, loss 0.13059, acc 0.9375, prec 0.0556659, recall 0.868802
2017-12-10T14:25:52.910139: step 3082, loss 0.202075, acc 0.9375, prec 0.055661, recall 0.868802
2017-12-10T14:25:53.101042: step 3083, loss 0.183956, acc 0.953125, prec 0.0556573, recall 0.868802
2017-12-10T14:25:53.290259: step 3084, loss 0.511891, acc 0.890625, prec 0.0556903, recall 0.868892
2017-12-10T14:25:53.479902: step 3085, loss 0.141635, acc 0.921875, prec 0.0556842, recall 0.868892
2017-12-10T14:25:53.668721: step 3086, loss 1.15234, acc 0.96875, prec 0.0557234, recall 0.868982
2017-12-10T14:25:53.859008: step 3087, loss 0.279624, acc 0.90625, prec 0.0557368, recall 0.869027
2017-12-10T14:25:54.047139: step 3088, loss 0.209931, acc 0.921875, prec 0.0557723, recall 0.869117
2017-12-10T14:25:54.235584: step 3089, loss 0.0440678, acc 1, prec 0.0557723, recall 0.869117
2017-12-10T14:25:54.425365: step 3090, loss 0.267941, acc 0.890625, prec 0.0557845, recall 0.869162
2017-12-10T14:25:54.614913: step 3091, loss 0.471043, acc 0.859375, prec 0.0557943, recall 0.869207
2017-12-10T14:25:54.804387: step 3092, loss 0.0922681, acc 0.96875, prec 0.0558126, recall 0.869252
2017-12-10T14:25:54.993724: step 3093, loss 0.22889, acc 0.9375, prec 0.0558285, recall 0.869297
2017-12-10T14:25:55.183397: step 3094, loss 0.268156, acc 0.90625, prec 0.0558211, recall 0.869297
2017-12-10T14:25:55.371791: step 3095, loss 0.101881, acc 0.953125, prec 0.0558382, recall 0.869342
2017-12-10T14:25:55.563828: step 3096, loss 0.144317, acc 0.921875, prec 0.0558529, recall 0.869386
2017-12-10T14:25:55.754852: step 3097, loss 0.639121, acc 0.953125, prec 0.05587, recall 0.869431
2017-12-10T14:25:55.946657: step 3098, loss 0.342703, acc 0.953125, prec 0.0559079, recall 0.869521
2017-12-10T14:25:56.138802: step 3099, loss 0.232678, acc 0.953125, prec 0.0559042, recall 0.869521
2017-12-10T14:25:56.326969: step 3100, loss 0.647378, acc 0.953125, prec 0.0559421, recall 0.86961
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_2/1512933336/checkpoints/model-3100

2017-12-10T14:25:57.466716: step 3101, loss 1.86913, acc 0.9375, prec 0.0559384, recall 0.869312
2017-12-10T14:25:57.658823: step 3102, loss 0.318231, acc 0.90625, prec 0.0559518, recall 0.869357
2017-12-10T14:25:57.846513: step 3103, loss 0.135639, acc 0.90625, prec 0.0559444, recall 0.869357
2017-12-10T14:25:58.038561: step 3104, loss 0.190606, acc 0.921875, prec 0.0559382, recall 0.869357
2017-12-10T14:25:58.225027: step 3105, loss 0.487946, acc 0.90625, prec 0.0559516, recall 0.869402
2017-12-10T14:25:58.419843: step 3106, loss 0.303526, acc 0.875, prec 0.0559625, recall 0.869446
2017-12-10T14:25:58.609464: step 3107, loss 0.260417, acc 0.90625, prec 0.0559551, recall 0.869446
2017-12-10T14:25:58.794857: step 3108, loss 0.502441, acc 0.859375, prec 0.0559441, recall 0.869446
2017-12-10T14:25:58.988658: step 3109, loss 0.271648, acc 0.875, prec 0.0559757, recall 0.869536
2017-12-10T14:25:59.182215: step 3110, loss 0.664101, acc 0.765625, prec 0.055978, recall 0.86958
2017-12-10T14:25:59.370317: step 3111, loss 0.329629, acc 0.875, prec 0.0560097, recall 0.869669
2017-12-10T14:25:59.555545: step 3112, loss 0.351886, acc 0.875, prec 0.0559998, recall 0.869669
2017-12-10T14:25:59.743686: step 3113, loss 1.76486, acc 0.8125, prec 0.056007, recall 0.869417
2017-12-10T14:25:59.934413: step 3114, loss 0.364762, acc 0.953125, prec 0.0560241, recall 0.869461
2017-12-10T14:26:00.121044: step 3115, loss 0.348566, acc 0.890625, prec 0.0560362, recall 0.869506
2017-12-10T14:26:00.313089: step 3116, loss 0.442743, acc 0.875, prec 0.0560263, recall 0.869506
2017-12-10T14:26:00.501911: step 3117, loss 0.414875, acc 0.890625, prec 0.0560592, recall 0.869595
2017-12-10T14:26:00.690112: step 3118, loss 0.629809, acc 0.84375, prec 0.0560676, recall 0.869639
2017-12-10T14:26:00.881327: step 3119, loss 4.4924, acc 0.890625, prec 0.0560602, recall 0.869343
2017-12-10T14:26:01.076527: step 3120, loss 0.735597, acc 0.859375, prec 0.0561113, recall 0.869477
2017-12-10T14:26:01.271214: step 3121, loss 3.61477, acc 0.890625, prec 0.0561246, recall 0.869226
2017-12-10T14:26:01.463399: step 3122, loss 0.498192, acc 0.84375, prec 0.0561123, recall 0.869226
2017-12-10T14:26:01.660833: step 3123, loss 0.631091, acc 0.765625, prec 0.0561145, recall 0.86927
2017-12-10T14:26:01.850573: step 3124, loss 0.993656, acc 0.671875, prec 0.0561094, recall 0.869314
2017-12-10T14:26:02.039293: step 3125, loss 1.41761, acc 0.640625, prec 0.0561018, recall 0.869359
2017-12-10T14:26:02.223675: step 3126, loss 1.30563, acc 0.703125, prec 0.0560784, recall 0.869359
2017-12-10T14:26:02.407535: step 3127, loss 1.81681, acc 0.53125, prec 0.0560416, recall 0.869359
2017-12-10T14:26:02.593824: step 3128, loss 1.38744, acc 0.6875, prec 0.0560584, recall 0.869447
2017-12-10T14:26:02.777157: step 3129, loss 1.07525, acc 0.75, prec 0.0560594, recall 0.869492
2017-12-10T14:26:02.962833: step 3130, loss 1.1475, acc 0.75, prec 0.0560399, recall 0.869492
2017-12-10T14:26:03.147648: step 3131, loss 1.20185, acc 0.671875, prec 0.0560348, recall 0.869536
2017-12-10T14:26:03.338910: step 3132, loss 1.74077, acc 0.6875, prec 0.0560309, recall 0.86958
2017-12-10T14:26:03.526652: step 3133, loss 0.642065, acc 0.796875, prec 0.0560356, recall 0.869624
2017-12-10T14:26:03.716583: step 3134, loss 0.473121, acc 0.84375, prec 0.056044, recall 0.869668
2017-12-10T14:26:03.906342: step 3135, loss 0.594741, acc 0.75, prec 0.0560244, recall 0.869668
2017-12-10T14:26:04.093816: step 3136, loss 0.566702, acc 0.84375, prec 0.0560328, recall 0.869712
2017-12-10T14:26:04.277774: step 3137, loss 0.549355, acc 0.8125, prec 0.0560181, recall 0.869712
2017-12-10T14:26:04.461287: step 3138, loss 0.287395, acc 0.921875, prec 0.0560326, recall 0.869756
2017-12-10T14:26:04.644131: step 3139, loss 0.250289, acc 0.9375, prec 0.0560277, recall 0.869756
2017-12-10T14:26:04.833588: step 3140, loss 0.347647, acc 0.890625, prec 0.0560397, recall 0.8698
2017-12-10T14:26:05.020774: step 3141, loss 0.372628, acc 0.890625, prec 0.0560518, recall 0.869844
2017-12-10T14:26:05.208484: step 3142, loss 0.538457, acc 0.921875, prec 0.0560662, recall 0.869888
2017-12-10T14:26:05.395950: step 3143, loss 0.24482, acc 0.921875, prec 0.0560601, recall 0.869888
2017-12-10T14:26:05.588309: step 3144, loss 0.128874, acc 0.953125, prec 0.056077, recall 0.869932
2017-12-10T14:26:05.777474: step 3145, loss 0.134456, acc 0.953125, prec 0.0560733, recall 0.869932
2017-12-10T14:26:05.966778: step 3146, loss 0.13016, acc 0.953125, prec 0.0560902, recall 0.869976
2017-12-10T14:26:06.157810: step 3147, loss 0.0893331, acc 0.984375, prec 0.056089, recall 0.869976
2017-12-10T14:26:06.344412: step 3148, loss 0.0429098, acc 1, prec 0.0561096, recall 0.87002
2017-12-10T14:26:06.539150: step 3149, loss 0.110138, acc 0.953125, prec 0.0561059, recall 0.87002
2017-12-10T14:26:06.725898: step 3150, loss 0.252836, acc 0.9375, prec 0.0561216, recall 0.870064
2017-12-10T14:26:06.913865: step 3151, loss 0.610245, acc 1, prec 0.0561421, recall 0.870108
2017-12-10T14:26:07.107886: step 3152, loss 0.13354, acc 0.96875, prec 0.0561808, recall 0.870196
2017-12-10T14:26:07.297458: step 3153, loss 3.54183, acc 0.953125, prec 0.0561989, recall 0.869946
2017-12-10T14:26:07.489103: step 3154, loss 0.097849, acc 0.96875, prec 0.0561964, recall 0.869946
2017-12-10T14:26:07.680629: step 3155, loss 0.252501, acc 0.96875, prec 0.0562145, recall 0.86999
2017-12-10T14:26:07.868519: step 3156, loss 0.149656, acc 0.953125, prec 0.0562519, recall 0.870077
2017-12-10T14:26:08.056093: step 3157, loss 0.0556471, acc 0.984375, prec 0.0562712, recall 0.870121
2017-12-10T14:26:08.246508: step 3158, loss 0.126734, acc 0.96875, prec 0.0562893, recall 0.870165
2017-12-10T14:26:08.434967: step 3159, loss 0.278544, acc 0.9375, prec 0.0562844, recall 0.870165
2017-12-10T14:26:08.621298: step 3160, loss 0.62662, acc 0.953125, prec 0.0563013, recall 0.870209
2017-12-10T14:26:08.809687: step 3161, loss 5.27451, acc 0.890625, prec 0.0562939, recall 0.869916
2017-12-10T14:26:09.001080: step 3162, loss 0.375536, acc 0.90625, prec 0.0563071, recall 0.86996
2017-12-10T14:26:09.188585: step 3163, loss 0.74418, acc 0.875, prec 0.0563383, recall 0.870047
2017-12-10T14:26:09.377267: step 3164, loss 0.473126, acc 0.828125, prec 0.0563249, recall 0.870047
2017-12-10T14:26:09.564043: step 3165, loss 0.745685, acc 0.84375, prec 0.0563126, recall 0.870047
2017-12-10T14:26:09.752441: step 3166, loss 0.274527, acc 0.890625, prec 0.0563246, recall 0.870091
2017-12-10T14:26:09.939703: step 3167, loss 0.562256, acc 0.859375, prec 0.0563135, recall 0.870091
2017-12-10T14:26:10.131404: step 3168, loss 0.519816, acc 0.78125, prec 0.0563169, recall 0.870134
2017-12-10T14:26:10.318759: step 3169, loss 0.513427, acc 0.828125, prec 0.0563035, recall 0.870134
2017-12-10T14:26:10.505014: step 3170, loss 0.818056, acc 0.828125, prec 0.05629, recall 0.870134
2017-12-10T14:26:10.689236: step 3171, loss 0.771012, acc 0.828125, prec 0.0562971, recall 0.870178
2017-12-10T14:26:10.877802: step 3172, loss 0.932795, acc 0.765625, prec 0.0563197, recall 0.870265
2017-12-10T14:26:11.065281: step 3173, loss 0.650202, acc 0.8125, prec 0.056346, recall 0.870352
2017-12-10T14:26:11.259305: step 3174, loss 0.463835, acc 0.875, prec 0.0563567, recall 0.870395
2017-12-10T14:26:11.444976: step 3175, loss 0.561068, acc 0.84375, prec 0.0563854, recall 0.870482
2017-12-10T14:26:11.633449: step 3176, loss 0.376726, acc 0.84375, prec 0.0563936, recall 0.870525
2017-12-10T14:26:11.826118: step 3177, loss 0.644362, acc 0.875, prec 0.0564043, recall 0.870569
2017-12-10T14:26:12.014044: step 3178, loss 0.17063, acc 0.953125, prec 0.0564006, recall 0.870569
2017-12-10T14:26:12.205053: step 3179, loss 0.446154, acc 0.859375, prec 0.0564305, recall 0.870655
2017-12-10T14:26:12.396188: step 3180, loss 0.310201, acc 0.9375, prec 0.056446, recall 0.870698
2017-12-10T14:26:12.584073: step 3181, loss 0.435101, acc 0.890625, prec 0.0564988, recall 0.870828
2017-12-10T14:26:12.775564: step 3182, loss 0.162438, acc 0.953125, prec 0.0564951, recall 0.870828
2017-12-10T14:26:12.964858: step 3183, loss 0.356324, acc 0.9375, prec 0.0565515, recall 0.870957
2017-12-10T14:26:13.151845: step 3184, loss 0.12281, acc 0.921875, prec 0.0565658, recall 0.871
2017-12-10T14:26:13.341088: step 3185, loss 0.113836, acc 0.953125, prec 0.0565825, recall 0.871043
2017-12-10T14:26:13.533372: step 3186, loss 0.164053, acc 0.9375, prec 0.0565776, recall 0.871043
2017-12-10T14:26:13.718171: step 3187, loss 0.0642985, acc 0.96875, prec 0.0565752, recall 0.871043
2017-12-10T14:26:13.911989: step 3188, loss 0.25309, acc 0.984375, prec 0.0565944, recall 0.871086
2017-12-10T14:26:14.105416: step 3189, loss 0.118339, acc 0.9375, prec 0.0565895, recall 0.871086
2017-12-10T14:26:14.292764: step 3190, loss 0.616391, acc 0.953125, prec 0.056647, recall 0.871215
2017-12-10T14:26:14.484809: step 3191, loss 0.165676, acc 0.9375, prec 0.0566421, recall 0.871215
2017-12-10T14:26:14.675017: step 3192, loss 0.195749, acc 0.921875, prec 0.0566564, recall 0.871257
2017-12-10T14:26:14.862578: step 3193, loss 0.0373753, acc 0.984375, prec 0.0566756, recall 0.8713
2017-12-10T14:26:15.049418: step 3194, loss 0.362237, acc 0.921875, prec 0.0567307, recall 0.871429
2017-12-10T14:26:15.235808: step 3195, loss 0.0518741, acc 0.984375, prec 0.0567703, recall 0.871514
2017-12-10T14:26:15.423924: step 3196, loss 0.239445, acc 0.953125, prec 0.056787, recall 0.871557
2017-12-10T14:26:15.610611: step 3197, loss 0.103426, acc 0.984375, prec 0.0568265, recall 0.871642
2017-12-10T14:26:15.801037: step 3198, loss 0.323457, acc 0.9375, prec 0.056842, recall 0.871684
2017-12-10T14:26:15.991046: step 3199, loss 0.0397949, acc 0.984375, prec 0.0568408, recall 0.871684
2017-12-10T14:26:16.177292: step 3200, loss 0.0600962, acc 0.96875, prec 0.0568383, recall 0.871684
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_2/1512933336/checkpoints/model-3200

2017-12-10T14:26:17.440214: step 3201, loss 0.0939164, acc 0.953125, prec 0.0568346, recall 0.871684
2017-12-10T14:26:17.629561: step 3202, loss 0.027764, acc 1, prec 0.0568346, recall 0.871684
2017-12-10T14:26:17.814603: step 3203, loss 0.237822, acc 0.96875, prec 0.0568526, recall 0.871727
2017-12-10T14:26:18.003652: step 3204, loss 0.127686, acc 0.96875, prec 0.0568909, recall 0.871812
2017-12-10T14:26:18.197042: step 3205, loss 0.0187282, acc 0.984375, prec 0.05691, recall 0.871854
2017-12-10T14:26:18.387381: step 3206, loss 0.356769, acc 0.9375, prec 0.0569051, recall 0.871854
2017-12-10T14:26:18.574340: step 3207, loss 0.233033, acc 0.9375, prec 0.0569002, recall 0.871854
2017-12-10T14:26:18.765395: step 3208, loss 0.136429, acc 0.96875, prec 0.0569181, recall 0.871897
2017-12-10T14:26:18.955514: step 3209, loss 0.2151, acc 0.953125, prec 0.0569552, recall 0.871982
2017-12-10T14:26:19.143677: step 3210, loss 0.160003, acc 0.9375, prec 0.0569503, recall 0.871982
2017-12-10T14:26:19.332604: step 3211, loss 0.0374062, acc 0.984375, prec 0.056949, recall 0.871982
2017-12-10T14:26:19.521289: step 3212, loss 0.200238, acc 0.921875, prec 0.0569429, recall 0.871982
2017-12-10T14:26:19.710982: step 3213, loss 0.0792154, acc 0.96875, prec 0.0569404, recall 0.871982
2017-12-10T14:26:19.903387: step 3214, loss 0.091735, acc 0.984375, prec 0.0569596, recall 0.872024
2017-12-10T14:26:20.091795: step 3215, loss 0.0949513, acc 0.96875, prec 0.0569978, recall 0.872108
2017-12-10T14:26:20.280474: step 3216, loss 0.0622166, acc 0.984375, prec 0.0569966, recall 0.872108
2017-12-10T14:26:20.471096: step 3217, loss 0.241464, acc 0.96875, prec 0.0569941, recall 0.872108
2017-12-10T14:26:20.659909: step 3218, loss 0.295277, acc 0.984375, prec 0.0570133, recall 0.872151
2017-12-10T14:26:20.848326: step 3219, loss 0.0178313, acc 1, prec 0.0570133, recall 0.872151
2017-12-10T14:26:21.035810: step 3220, loss 0.231865, acc 1, prec 0.057054, recall 0.872235
2017-12-10T14:26:21.227262: step 3221, loss 0.0231618, acc 0.984375, prec 0.0570528, recall 0.872235
2017-12-10T14:26:21.414478: step 3222, loss 0.0888501, acc 0.96875, prec 0.0570707, recall 0.872277
2017-12-10T14:26:21.604250: step 3223, loss 0.183718, acc 0.984375, prec 0.0571102, recall 0.872361
2017-12-10T14:26:21.794338: step 3224, loss 8.03515, acc 0.96875, prec 0.0571102, recall 0.871786
2017-12-10T14:26:21.985654: step 3225, loss 0.132087, acc 0.953125, prec 0.0571472, recall 0.871871
2017-12-10T14:26:22.177268: step 3226, loss 0.263136, acc 0.953125, prec 0.0571638, recall 0.871913
2017-12-10T14:26:22.368449: step 3227, loss 0.234035, acc 0.921875, prec 0.0571577, recall 0.871913
2017-12-10T14:26:22.557517: step 3228, loss 1.21983, acc 0.921875, prec 0.0571718, recall 0.871955
2017-12-10T14:26:22.747121: step 3229, loss 0.322282, acc 0.90625, prec 0.0571848, recall 0.871997
2017-12-10T14:26:22.936581: step 3230, loss 0.275983, acc 0.9375, prec 0.0572205, recall 0.872082
2017-12-10T14:26:23.123290: step 3231, loss 0.377973, acc 0.890625, prec 0.0572526, recall 0.872166
2017-12-10T14:26:23.311367: step 3232, loss 0.401294, acc 0.890625, prec 0.0572846, recall 0.87225
2017-12-10T14:26:23.502210: step 3233, loss 0.40519, acc 0.890625, prec 0.0572963, recall 0.872292
2017-12-10T14:26:23.691073: step 3234, loss 0.305515, acc 0.875, prec 0.0573474, recall 0.872417
2017-12-10T14:26:23.883207: step 3235, loss 0.658256, acc 0.84375, prec 0.0573756, recall 0.872501
2017-12-10T14:26:24.069544: step 3236, loss 0.543284, acc 0.8125, prec 0.0573608, recall 0.872501
2017-12-10T14:26:24.255210: step 3237, loss 0.646586, acc 0.8125, prec 0.0573866, recall 0.872584
2017-12-10T14:26:24.441655: step 3238, loss 0.645118, acc 0.828125, prec 0.057373, recall 0.872584
2017-12-10T14:26:24.629184: step 3239, loss 0.573057, acc 0.90625, prec 0.0573656, recall 0.872584
2017-12-10T14:26:24.818093: step 3240, loss 0.711937, acc 0.78125, prec 0.0573889, recall 0.872668
2017-12-10T14:26:25.004768: step 3241, loss 1.28374, acc 0.765625, prec 0.0573906, recall 0.872709
2017-12-10T14:26:25.192641: step 3242, loss 0.514997, acc 0.828125, prec 0.0573973, recall 0.872751
2017-12-10T14:26:25.378274: step 3243, loss 0.390568, acc 0.84375, prec 0.057385, recall 0.872751
2017-12-10T14:26:25.568009: step 3244, loss 0.358925, acc 0.875, prec 0.0573751, recall 0.872751
2017-12-10T14:26:25.756728: step 3245, loss 0.486964, acc 0.859375, prec 0.0573843, recall 0.872793
2017-12-10T14:26:25.945730: step 3246, loss 0.231858, acc 0.90625, prec 0.0573769, recall 0.872793
2017-12-10T14:26:26.136811: step 3247, loss 0.581468, acc 0.8125, prec 0.0573823, recall 0.872834
2017-12-10T14:26:26.324196: step 3248, loss 0.231891, acc 0.9375, prec 0.0573774, recall 0.872834
2017-12-10T14:26:26.513309: step 3249, loss 0.240368, acc 0.90625, prec 0.0573903, recall 0.872876
2017-12-10T14:26:26.705060: step 3250, loss 0.211816, acc 0.953125, prec 0.0574068, recall 0.872917
2017-12-10T14:26:26.895343: step 3251, loss 0.334704, acc 0.953125, prec 0.0574234, recall 0.872959
2017-12-10T14:26:27.084470: step 3252, loss 0.132725, acc 0.984375, prec 0.0574221, recall 0.872959
2017-12-10T14:26:27.272919: step 3253, loss 0.568833, acc 0.953125, prec 0.0574792, recall 0.873083
2017-12-10T14:26:27.466500: step 3254, loss 0.208538, acc 0.9375, prec 0.0574742, recall 0.873083
2017-12-10T14:26:27.655778: step 3255, loss 0.282893, acc 0.921875, prec 0.0574681, recall 0.873083
2017-12-10T14:26:27.844071: step 3256, loss 0.0173483, acc 1, prec 0.0574681, recall 0.873083
2017-12-10T14:26:28.029226: step 3257, loss 0.40965, acc 0.890625, prec 0.0574999, recall 0.873166
2017-12-10T14:26:28.221647: step 3258, loss 0.20067, acc 0.90625, prec 0.0574925, recall 0.873166
2017-12-10T14:26:28.415011: step 3259, loss 0.0811611, acc 0.984375, prec 0.0575115, recall 0.873207
2017-12-10T14:26:28.602320: step 3260, loss 0.0326607, acc 0.984375, prec 0.0575102, recall 0.873207
2017-12-10T14:26:28.787325: step 3261, loss 0.600474, acc 0.96875, prec 0.057528, recall 0.873249
2017-12-10T14:26:28.981669: step 3262, loss 0.32812, acc 0.9375, prec 0.0575433, recall 0.87329
2017-12-10T14:26:29.171585: step 3263, loss 0.0688927, acc 0.96875, prec 0.0575408, recall 0.87329
2017-12-10T14:26:29.360468: step 3264, loss 0.103681, acc 0.953125, prec 0.0575371, recall 0.87329
2017-12-10T14:26:29.548610: step 3265, loss 0.023613, acc 0.984375, prec 0.0575561, recall 0.873331
2017-12-10T14:26:29.747055: step 3266, loss 0.309654, acc 1, prec 0.0575966, recall 0.873414
2017-12-10T14:26:29.941366: step 3267, loss 0.0625581, acc 0.96875, prec 0.0576143, recall 0.873455
2017-12-10T14:26:30.131314: step 3268, loss 4.24913, acc 0.96875, prec 0.0576535, recall 0.873253
2017-12-10T14:26:30.325211: step 3269, loss 0.116124, acc 0.96875, prec 0.0576713, recall 0.873294
2017-12-10T14:26:30.515969: step 3270, loss 0.158023, acc 0.96875, prec 0.057689, recall 0.873335
2017-12-10T14:26:30.707836: step 3271, loss 0.0495025, acc 0.96875, prec 0.0577067, recall 0.873377
2017-12-10T14:26:30.899647: step 3272, loss 0.374743, acc 0.90625, prec 0.0576993, recall 0.873377
2017-12-10T14:26:31.091347: step 3273, loss 0.432837, acc 0.921875, prec 0.0577133, recall 0.873418
2017-12-10T14:26:31.288404: step 3274, loss 3.01176, acc 0.953125, prec 0.0577311, recall 0.873176
2017-12-10T14:26:31.486230: step 3275, loss 0.482986, acc 0.90625, prec 0.0577236, recall 0.873176
2017-12-10T14:26:31.679312: step 3276, loss 0.602898, acc 0.921875, prec 0.0577377, recall 0.873217
2017-12-10T14:26:31.867340: step 3277, loss 0.528979, acc 0.8125, prec 0.0577228, recall 0.873217
2017-12-10T14:26:32.055933: step 3278, loss 0.929716, acc 0.796875, prec 0.0577067, recall 0.873217
2017-12-10T14:26:32.241637: step 3279, loss 0.622352, acc 0.84375, prec 0.0576944, recall 0.873217
2017-12-10T14:26:32.433073: step 3280, loss 0.961344, acc 0.6875, prec 0.05771, recall 0.873299
2017-12-10T14:26:32.620695: step 3281, loss 0.601955, acc 0.78125, prec 0.0577532, recall 0.873422
2017-12-10T14:26:32.808575: step 3282, loss 0.601877, acc 0.859375, prec 0.0577421, recall 0.873422
2017-12-10T14:26:32.994810: step 3283, loss 0.406915, acc 0.859375, prec 0.057731, recall 0.873422
2017-12-10T14:26:33.182879: step 3284, loss 0.401277, acc 0.90625, prec 0.0577236, recall 0.873422
2017-12-10T14:26:33.370156: step 3285, loss 0.507363, acc 0.890625, prec 0.0577149, recall 0.873422
2017-12-10T14:26:33.557909: step 3286, loss 0.469154, acc 0.859375, prec 0.0577038, recall 0.873422
2017-12-10T14:26:33.743816: step 3287, loss 0.483417, acc 0.84375, prec 0.0577116, recall 0.873463
2017-12-10T14:26:33.932634: step 3288, loss 0.799256, acc 0.828125, prec 0.0576981, recall 0.873463
2017-12-10T14:26:34.121151: step 3289, loss 0.10625, acc 0.921875, prec 0.0577322, recall 0.873545
2017-12-10T14:26:34.305316: step 3290, loss 1.0391, acc 0.828125, prec 0.0577387, recall 0.873586
2017-12-10T14:26:34.497681: step 3291, loss 0.287771, acc 0.890625, prec 0.0577502, recall 0.873626
2017-12-10T14:26:34.685461: step 3292, loss 0.181783, acc 0.890625, prec 0.0577416, recall 0.873626
2017-12-10T14:26:34.873408: step 3293, loss 0.415742, acc 0.890625, prec 0.0577531, recall 0.873667
2017-12-10T14:26:35.063104: step 3294, loss 0.355315, acc 0.828125, prec 0.0577395, recall 0.873667
2017-12-10T14:26:35.251881: step 3295, loss 0.524576, acc 0.84375, prec 0.0577473, recall 0.873708
2017-12-10T14:26:35.440734: step 3296, loss 0.14843, acc 0.953125, prec 0.0577436, recall 0.873708
2017-12-10T14:26:35.631057: step 3297, loss 0.327414, acc 0.890625, prec 0.057735, recall 0.873708
2017-12-10T14:26:35.818136: step 3298, loss 8.58036, acc 0.921875, prec 0.0577301, recall 0.873426
2017-12-10T14:26:36.015237: step 3299, loss 0.346412, acc 0.96875, prec 0.0577477, recall 0.873467
2017-12-10T14:26:36.205869: step 3300, loss 0.114579, acc 0.953125, prec 0.0577842, recall 0.873548
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_2/1512933336/checkpoints/model-3300

2017-12-10T14:26:37.300542: step 3301, loss 0.544938, acc 0.875, prec 0.0577744, recall 0.873548
2017-12-10T14:26:37.486953: step 3302, loss 0.730944, acc 0.921875, prec 0.0578084, recall 0.87363
2017-12-10T14:26:37.678567: step 3303, loss 0.197536, acc 0.921875, prec 0.0578424, recall 0.873711
2017-12-10T14:26:37.869738: step 3304, loss 0.531479, acc 0.796875, prec 0.0578666, recall 0.873793
2017-12-10T14:26:38.059835: step 3305, loss 0.242079, acc 0.90625, prec 0.0578993, recall 0.873874
2017-12-10T14:26:38.248352: step 3306, loss 0.240998, acc 0.9375, prec 0.0579145, recall 0.873914
2017-12-10T14:26:38.434712: step 3307, loss 0.155634, acc 0.984375, prec 0.0579132, recall 0.873914
2017-12-10T14:26:38.619131: step 3308, loss 0.237489, acc 0.953125, prec 0.0579095, recall 0.873914
2017-12-10T14:26:38.805811: step 3309, loss 8.38346, acc 0.921875, prec 0.0579247, recall 0.873674
2017-12-10T14:26:38.999904: step 3310, loss 0.543322, acc 0.828125, prec 0.0579312, recall 0.873715
2017-12-10T14:26:39.189741: step 3311, loss 0.445635, acc 0.84375, prec 0.0579188, recall 0.873715
2017-12-10T14:26:39.377355: step 3312, loss 0.71082, acc 0.8125, prec 0.0579241, recall 0.873755
2017-12-10T14:26:39.565520: step 3313, loss 0.756349, acc 0.765625, prec 0.0579257, recall 0.873796
2017-12-10T14:26:39.754766: step 3314, loss 0.650284, acc 0.78125, prec 0.0579485, recall 0.873877
2017-12-10T14:26:39.943570: step 3315, loss 0.611873, acc 0.828125, prec 0.057955, recall 0.873917
2017-12-10T14:26:40.133148: step 3316, loss 0.690839, acc 0.734375, prec 0.057934, recall 0.873917
2017-12-10T14:26:40.322372: step 3317, loss 0.708037, acc 0.8125, prec 0.0579393, recall 0.873958
2017-12-10T14:26:40.513847: step 3318, loss 0.599378, acc 0.765625, prec 0.0579809, recall 0.874079
2017-12-10T14:26:40.701640: step 3319, loss 0.861333, acc 0.859375, prec 0.0580298, recall 0.8742
2017-12-10T14:26:40.892010: step 3320, loss 0.611425, acc 0.828125, prec 0.0580363, recall 0.87424
2017-12-10T14:26:41.083443: step 3321, loss 0.741486, acc 0.8125, prec 0.0580415, recall 0.87428
2017-12-10T14:26:41.270786: step 3322, loss 0.829004, acc 0.859375, prec 0.0580504, recall 0.87432
2017-12-10T14:26:41.457386: step 3323, loss 0.340911, acc 0.875, prec 0.0580405, recall 0.87432
2017-12-10T14:26:41.648768: step 3324, loss 0.329164, acc 0.953125, prec 0.0580968, recall 0.874441
2017-12-10T14:26:41.837654: step 3325, loss 0.624317, acc 0.859375, prec 0.0581057, recall 0.874481
2017-12-10T14:26:42.022082: step 3326, loss 0.500683, acc 0.90625, prec 0.0581183, recall 0.874521
2017-12-10T14:26:42.212372: step 3327, loss 0.247568, acc 0.875, prec 0.0581484, recall 0.874601
2017-12-10T14:26:42.399328: step 3328, loss 0.747079, acc 0.890625, prec 0.0581997, recall 0.874721
2017-12-10T14:26:42.592526: step 3329, loss 0.303918, acc 0.921875, prec 0.0581935, recall 0.874721
2017-12-10T14:26:42.779804: step 3330, loss 0.280587, acc 0.890625, prec 0.0582049, recall 0.874761
2017-12-10T14:26:42.966553: step 3331, loss 0.42829, acc 0.875, prec 0.058215, recall 0.874801
2017-12-10T14:26:43.152401: step 3332, loss 0.249079, acc 0.921875, prec 0.0582088, recall 0.874801
2017-12-10T14:26:43.340209: step 3333, loss 0.18725, acc 0.9375, prec 0.0582238, recall 0.874841
2017-12-10T14:26:43.528811: step 3334, loss 0.221986, acc 0.90625, prec 0.0582364, recall 0.874881
2017-12-10T14:26:43.721156: step 3335, loss 0.145015, acc 0.96875, prec 0.0582539, recall 0.87492
2017-12-10T14:26:43.910394: step 3336, loss 0.175787, acc 0.953125, prec 0.0582901, recall 0.875
2017-12-10T14:26:44.105591: step 3337, loss 0.252462, acc 0.921875, prec 0.0582839, recall 0.875
2017-12-10T14:26:44.297325: step 3338, loss 0.190988, acc 0.953125, prec 0.0582802, recall 0.875
2017-12-10T14:26:44.487717: step 3339, loss 0.0169049, acc 1, prec 0.0582802, recall 0.875
2017-12-10T14:26:44.671575: step 3340, loss 1.16094, acc 0.9375, prec 0.0583152, recall 0.875079
2017-12-10T14:26:44.864863: step 3341, loss 0.0715499, acc 0.96875, prec 0.0583127, recall 0.875079
2017-12-10T14:26:45.058529: step 3342, loss 0.328171, acc 0.921875, prec 0.0583464, recall 0.875159
2017-12-10T14:26:45.248234: step 3343, loss 0.150688, acc 0.921875, prec 0.0583602, recall 0.875198
2017-12-10T14:26:45.438062: step 3344, loss 2.47866, acc 0.9375, prec 0.0583963, recall 0.875
2017-12-10T14:26:45.627896: step 3345, loss 1.10975, acc 0.953125, prec 0.0584325, recall 0.875079
2017-12-10T14:26:45.820642: step 3346, loss 0.151517, acc 0.9375, prec 0.0584475, recall 0.875119
2017-12-10T14:26:46.008762: step 3347, loss 0.21545, acc 0.96875, prec 0.0584649, recall 0.875158
2017-12-10T14:26:46.195903: step 3348, loss 0.197205, acc 0.921875, prec 0.0584986, recall 0.875237
2017-12-10T14:26:46.378628: step 3349, loss 0.471005, acc 0.828125, prec 0.0585049, recall 0.875277
2017-12-10T14:26:46.565738: step 3350, loss 0.382714, acc 0.890625, prec 0.0585162, recall 0.875316
2017-12-10T14:26:46.752009: step 3351, loss 0.636346, acc 0.796875, prec 0.0585001, recall 0.875316
2017-12-10T14:26:46.939813: step 3352, loss 0.209447, acc 0.875, prec 0.0584902, recall 0.875316
2017-12-10T14:26:47.128767: step 3353, loss 0.208109, acc 0.890625, prec 0.0585014, recall 0.875356
2017-12-10T14:26:47.319922: step 3354, loss 0.303533, acc 0.90625, prec 0.058494, recall 0.875356
2017-12-10T14:26:47.505728: step 3355, loss 0.339828, acc 0.9375, prec 0.0584891, recall 0.875356
2017-12-10T14:26:47.693906: step 3356, loss 0.461015, acc 0.84375, prec 0.0585364, recall 0.875474
2017-12-10T14:26:47.889678: step 3357, loss 0.325058, acc 0.921875, prec 0.0585302, recall 0.875474
2017-12-10T14:26:48.071713: step 3358, loss 0.430219, acc 0.890625, prec 0.0585215, recall 0.875474
2017-12-10T14:26:48.259303: step 3359, loss 0.438803, acc 0.859375, prec 0.0585104, recall 0.875474
2017-12-10T14:26:48.448965: step 3360, loss 0.255625, acc 0.953125, prec 0.0585465, recall 0.875553
2017-12-10T14:26:48.635576: step 3361, loss 0.863445, acc 0.828125, prec 0.0585528, recall 0.875592
2017-12-10T14:26:48.823027: step 3362, loss 0.361279, acc 0.890625, prec 0.0586037, recall 0.87571
2017-12-10T14:26:49.010221: step 3363, loss 0.631059, acc 0.890625, prec 0.0586149, recall 0.875749
2017-12-10T14:26:49.206664: step 3364, loss 0.0558361, acc 0.96875, prec 0.0586522, recall 0.875827
2017-12-10T14:26:49.391623: step 3365, loss 0.168771, acc 0.9375, prec 0.0586473, recall 0.875827
2017-12-10T14:26:49.581238: step 3366, loss 0.283115, acc 0.875, prec 0.0586771, recall 0.875906
2017-12-10T14:26:49.767414: step 3367, loss 0.248681, acc 0.890625, prec 0.0586684, recall 0.875906
2017-12-10T14:26:49.954935: step 3368, loss 0.28163, acc 0.859375, prec 0.0586771, recall 0.875945
2017-12-10T14:26:50.146487: step 3369, loss 0.232573, acc 0.96875, prec 0.0586747, recall 0.875945
2017-12-10T14:26:50.336400: step 3370, loss 0.13515, acc 0.9375, prec 0.0587094, recall 0.876023
2017-12-10T14:26:50.527593: step 3371, loss 0.027431, acc 1, prec 0.0587491, recall 0.876101
2017-12-10T14:26:50.715781: step 3372, loss 0.150465, acc 0.96875, prec 0.0587665, recall 0.87614
2017-12-10T14:26:50.903333: step 3373, loss 0.677507, acc 0.984375, prec 0.0588446, recall 0.876295
2017-12-10T14:26:51.103149: step 3374, loss 0.100378, acc 0.953125, prec 0.0588409, recall 0.876295
2017-12-10T14:26:51.291638: step 3375, loss 1.57906, acc 0.9375, prec 0.0588372, recall 0.87602
2017-12-10T14:26:51.482003: step 3376, loss 0.556364, acc 0.859375, prec 0.0588657, recall 0.876098
2017-12-10T14:26:51.671303: step 3377, loss 0.0620441, acc 0.984375, prec 0.0588644, recall 0.876098
2017-12-10T14:26:51.858362: step 3378, loss 0.213697, acc 0.9375, prec 0.0588793, recall 0.876137
2017-12-10T14:26:52.047530: step 3379, loss 0.142192, acc 0.953125, prec 0.0588954, recall 0.876176
2017-12-10T14:26:52.234418: step 3380, loss 0.120695, acc 0.96875, prec 0.0589326, recall 0.876253
2017-12-10T14:26:52.421580: step 3381, loss 0.296943, acc 0.921875, prec 0.0589859, recall 0.876369
2017-12-10T14:26:52.616104: step 3382, loss 0.367359, acc 0.9375, prec 0.0590007, recall 0.876408
2017-12-10T14:26:52.803452: step 3383, loss 0.30899, acc 0.921875, prec 0.0590143, recall 0.876447
2017-12-10T14:26:52.995196: step 3384, loss 0.246276, acc 0.96875, prec 0.0590317, recall 0.876485
2017-12-10T14:26:53.182719: step 3385, loss 0.185129, acc 0.9375, prec 0.0590465, recall 0.876524
2017-12-10T14:26:53.371305: step 3386, loss 0.35825, acc 0.90625, prec 0.059039, recall 0.876524
2017-12-10T14:26:53.556655: step 3387, loss 0.199407, acc 0.90625, prec 0.0590514, recall 0.876562
2017-12-10T14:26:53.743564: step 3388, loss 0.185529, acc 0.96875, prec 0.0590885, recall 0.87664
2017-12-10T14:26:53.931421: step 3389, loss 0.4504, acc 0.859375, prec 0.0590971, recall 0.876678
2017-12-10T14:26:54.121083: step 3390, loss 0.724967, acc 0.875, prec 0.059107, recall 0.876717
2017-12-10T14:26:54.312010: step 3391, loss 0.236266, acc 0.9375, prec 0.059102, recall 0.876717
2017-12-10T14:26:54.502624: step 3392, loss 0.249714, acc 0.921875, prec 0.059175, recall 0.87687
2017-12-10T14:26:54.697085: step 3393, loss 0.390647, acc 0.921875, prec 0.0591885, recall 0.876909
2017-12-10T14:26:54.885578: step 3394, loss 0.571361, acc 0.921875, prec 0.0592021, recall 0.876947
2017-12-10T14:26:55.071582: step 3395, loss 0.276631, acc 0.953125, prec 0.0592181, recall 0.876985
2017-12-10T14:26:55.262049: step 3396, loss 0.468524, acc 0.9375, prec 0.0592527, recall 0.877062
2017-12-10T14:26:55.453694: step 3397, loss 0.241701, acc 0.921875, prec 0.0592465, recall 0.877062
2017-12-10T14:26:55.644517: step 3398, loss 1.61351, acc 0.90625, prec 0.0592403, recall 0.876789
2017-12-10T14:26:55.839043: step 3399, loss 1.36344, acc 0.921875, prec 0.0592748, recall 0.876593
2017-12-10T14:26:56.031356: step 3400, loss 0.306632, acc 0.890625, prec 0.0592661, recall 0.876593
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_2/1512933336/checkpoints/model-3400

2017-12-10T14:26:57.316576: step 3401, loss 0.871978, acc 0.953125, prec 0.0592821, recall 0.876631
2017-12-10T14:26:57.507601: step 3402, loss 0.144644, acc 0.953125, prec 0.0592982, recall 0.87667
2017-12-10T14:26:57.697254: step 3403, loss 0.963042, acc 0.890625, prec 0.0593487, recall 0.876785
2017-12-10T14:26:57.889720: step 3404, loss 0.487705, acc 0.890625, prec 0.059419, recall 0.876937
2017-12-10T14:26:58.082765: step 3405, loss 0.594179, acc 0.859375, prec 0.0594473, recall 0.877014
2017-12-10T14:26:58.270182: step 3406, loss 0.187988, acc 0.921875, prec 0.0594608, recall 0.877052
2017-12-10T14:26:58.452269: step 3407, loss 0.434515, acc 0.890625, prec 0.0594521, recall 0.877052
2017-12-10T14:26:58.638281: step 3408, loss 0.556489, acc 0.84375, prec 0.0594396, recall 0.877052
2017-12-10T14:26:58.825606: step 3409, loss 0.411698, acc 0.890625, prec 0.0594506, recall 0.87709
2017-12-10T14:26:59.016353: step 3410, loss 0.181879, acc 0.9375, prec 0.0594654, recall 0.877128
2017-12-10T14:26:59.204498: step 3411, loss 0.590779, acc 0.875, prec 0.0594554, recall 0.877128
2017-12-10T14:26:59.398096: step 3412, loss 0.834314, acc 0.875, prec 0.0594454, recall 0.877128
2017-12-10T14:26:59.589834: step 3413, loss 0.332256, acc 0.9375, prec 0.0594404, recall 0.877128
2017-12-10T14:26:59.784834: step 3414, loss 0.614011, acc 0.859375, prec 0.0594292, recall 0.877128
2017-12-10T14:26:59.972846: step 3415, loss 0.438748, acc 0.890625, prec 0.0594599, recall 0.877204
2017-12-10T14:27:00.163351: step 3416, loss 0.327568, acc 0.859375, prec 0.0594881, recall 0.87728
2017-12-10T14:27:00.350698: step 3417, loss 0.638927, acc 0.890625, prec 0.0595188, recall 0.877356
2017-12-10T14:27:00.541588: step 3418, loss 0.34865, acc 0.875, prec 0.059568, recall 0.877469
2017-12-10T14:27:00.727248: step 3419, loss 0.381046, acc 0.890625, prec 0.0595592, recall 0.877469
2017-12-10T14:27:00.913082: step 3420, loss 0.920578, acc 0.828125, prec 0.0595849, recall 0.877545
2017-12-10T14:27:01.108375: step 3421, loss 0.321354, acc 0.890625, prec 0.0595762, recall 0.877545
2017-12-10T14:27:01.300472: step 3422, loss 0.373788, acc 0.90625, prec 0.0596081, recall 0.87762
2017-12-10T14:27:01.494270: step 3423, loss 0.396101, acc 0.890625, prec 0.0596387, recall 0.877696
2017-12-10T14:27:01.684203: step 3424, loss 0.112173, acc 0.953125, prec 0.0596349, recall 0.877696
2017-12-10T14:27:01.869192: step 3425, loss 0.25336, acc 0.953125, prec 0.0596312, recall 0.877696
2017-12-10T14:27:02.057937: step 3426, loss 0.274923, acc 0.90625, prec 0.0596434, recall 0.877733
2017-12-10T14:27:02.247680: step 3427, loss 0.262608, acc 0.921875, prec 0.0596568, recall 0.877771
2017-12-10T14:27:02.438595: step 3428, loss 0.385047, acc 0.96875, prec 0.059674, recall 0.877809
2017-12-10T14:27:02.631112: step 3429, loss 0.856953, acc 0.9375, prec 0.0597084, recall 0.877884
2017-12-10T14:27:02.824096: step 3430, loss 3.66439, acc 0.921875, prec 0.059723, recall 0.877651
2017-12-10T14:27:03.020375: step 3431, loss 0.210426, acc 0.9375, prec 0.0597377, recall 0.877689
2017-12-10T14:27:03.209618: step 3432, loss 0.115343, acc 0.953125, prec 0.0597536, recall 0.877727
2017-12-10T14:27:03.398900: step 3433, loss 0.11853, acc 0.96875, prec 0.0597511, recall 0.877727
2017-12-10T14:27:03.588032: step 3434, loss 0.150796, acc 0.96875, prec 0.0597486, recall 0.877727
2017-12-10T14:27:03.774512: step 3435, loss 0.293586, acc 0.890625, prec 0.0597792, recall 0.877802
2017-12-10T14:27:03.965040: step 3436, loss 0.647916, acc 0.859375, prec 0.0597876, recall 0.877839
2017-12-10T14:27:04.153147: step 3437, loss 0.507477, acc 0.859375, prec 0.0598353, recall 0.877952
2017-12-10T14:27:04.338981: step 3438, loss 0.272402, acc 0.921875, prec 0.0598291, recall 0.877952
2017-12-10T14:27:04.529612: step 3439, loss 0.622979, acc 0.890625, prec 0.05984, recall 0.877989
2017-12-10T14:27:04.717318: step 3440, loss 0.334516, acc 0.921875, prec 0.0598533, recall 0.878026
2017-12-10T14:27:04.904660: step 3441, loss 0.298584, acc 0.90625, prec 0.0598458, recall 0.878026
2017-12-10T14:27:05.091941: step 3442, loss 0.320063, acc 0.890625, prec 0.0598371, recall 0.878026
2017-12-10T14:27:05.279950: step 3443, loss 0.327109, acc 0.890625, prec 0.0598283, recall 0.878026
2017-12-10T14:27:05.471665: step 3444, loss 0.248814, acc 0.921875, prec 0.0598614, recall 0.878101
2017-12-10T14:27:05.660294: step 3445, loss 0.443022, acc 0.859375, prec 0.0598501, recall 0.878101
2017-12-10T14:27:05.852268: step 3446, loss 0.170354, acc 0.9375, prec 0.0598451, recall 0.878101
2017-12-10T14:27:06.038550: step 3447, loss 0.43452, acc 0.859375, prec 0.0598339, recall 0.878101
2017-12-10T14:27:06.226172: step 3448, loss 0.315628, acc 0.9375, prec 0.0598289, recall 0.878101
2017-12-10T14:27:06.416028: step 3449, loss 0.427229, acc 0.875, prec 0.0598189, recall 0.878101
2017-12-10T14:27:06.604218: step 3450, loss 0.217978, acc 0.90625, prec 0.0598114, recall 0.878101
2017-12-10T14:27:06.791923: step 3451, loss 0.285817, acc 0.890625, prec 0.0598027, recall 0.878101
2017-12-10T14:27:06.983520: step 3452, loss 0.546839, acc 0.890625, prec 0.0597939, recall 0.878101
2017-12-10T14:27:07.175817: step 3453, loss 1.76719, acc 0.875, prec 0.059844, recall 0.877944
2017-12-10T14:27:07.363592: step 3454, loss 0.311438, acc 0.90625, prec 0.0598365, recall 0.877944
2017-12-10T14:27:07.549906: step 3455, loss 0.117955, acc 0.953125, prec 0.0598524, recall 0.877982
2017-12-10T14:27:07.745279: step 3456, loss 0.178299, acc 0.921875, prec 0.0598658, recall 0.878019
2017-12-10T14:27:07.936591: step 3457, loss 0.577327, acc 0.859375, prec 0.0598741, recall 0.878056
2017-12-10T14:27:08.124646: step 3458, loss 0.0811982, acc 0.953125, prec 0.05989, recall 0.878093
2017-12-10T14:27:08.314997: step 3459, loss 0.293299, acc 0.921875, prec 0.0599229, recall 0.878168
2017-12-10T14:27:08.506724: step 3460, loss 0.409984, acc 0.90625, prec 0.0599154, recall 0.878168
2017-12-10T14:27:08.698504: step 3461, loss 0.133349, acc 0.96875, prec 0.0599129, recall 0.878168
2017-12-10T14:27:08.890934: step 3462, loss 0.274409, acc 0.90625, prec 0.0599054, recall 0.878168
2017-12-10T14:27:09.080062: step 3463, loss 0.283217, acc 0.9375, prec 0.05992, recall 0.878205
2017-12-10T14:27:09.272486: step 3464, loss 0.241783, acc 0.90625, prec 0.0599125, recall 0.878205
2017-12-10T14:27:09.461787: step 3465, loss 0.290636, acc 0.953125, prec 0.0599479, recall 0.878279
2017-12-10T14:27:09.650963: step 3466, loss 0.176467, acc 0.921875, prec 0.0599417, recall 0.878279
2017-12-10T14:27:09.838840: step 3467, loss 0.126722, acc 0.96875, prec 0.0599588, recall 0.878317
2017-12-10T14:27:10.031978: step 3468, loss 0.165439, acc 0.96875, prec 0.0599759, recall 0.878354
2017-12-10T14:27:10.219943: step 3469, loss 0.816854, acc 0.953125, prec 0.0600112, recall 0.878428
2017-12-10T14:27:10.409031: step 3470, loss 0.0714939, acc 0.96875, prec 0.0600087, recall 0.878428
2017-12-10T14:27:10.594686: step 3471, loss 0.179923, acc 0.9375, prec 0.0600624, recall 0.878539
2017-12-10T14:27:10.788026: step 3472, loss 0.105941, acc 0.96875, prec 0.0600599, recall 0.878539
2017-12-10T14:27:10.979213: step 3473, loss 0.128911, acc 0.96875, prec 0.060077, recall 0.878576
2017-12-10T14:27:11.167226: step 3474, loss 0.170825, acc 0.96875, prec 0.0601136, recall 0.87865
2017-12-10T14:27:11.360007: step 3475, loss 0.163698, acc 0.9375, prec 0.0601086, recall 0.87865
2017-12-10T14:27:11.546757: step 3476, loss 0.296161, acc 0.984375, prec 0.0601269, recall 0.878687
2017-12-10T14:27:11.740262: step 3477, loss 0.0966814, acc 0.953125, prec 0.0601232, recall 0.878687
2017-12-10T14:27:11.926594: step 3478, loss 0.168154, acc 0.953125, prec 0.0601194, recall 0.878687
2017-12-10T14:27:12.099801: step 3479, loss 0.09494, acc 0.980769, prec 0.0601377, recall 0.878723
2017-12-10T14:27:12.300665: step 3480, loss 0.141926, acc 0.96875, prec 0.0601352, recall 0.878723
2017-12-10T14:27:12.492111: step 3481, loss 4.07031, acc 0.953125, prec 0.0601327, recall 0.878456
2017-12-10T14:27:12.687943: step 3482, loss 0.15256, acc 0.953125, prec 0.0601485, recall 0.878493
2017-12-10T14:27:12.877621: step 3483, loss 0.353663, acc 0.953125, prec 0.0601643, recall 0.87853
2017-12-10T14:27:13.071204: step 3484, loss 0.331205, acc 0.9375, prec 0.0601788, recall 0.878567
2017-12-10T14:27:13.264374: step 3485, loss 0.348463, acc 0.953125, prec 0.0602337, recall 0.878678
2017-12-10T14:27:13.453069: step 3486, loss 0.210736, acc 0.921875, prec 0.0602274, recall 0.878678
2017-12-10T14:27:13.642347: step 3487, loss 0.107207, acc 0.9375, prec 0.060242, recall 0.878714
2017-12-10T14:27:13.833626: step 3488, loss 0.263745, acc 0.9375, prec 0.060276, recall 0.878788
2017-12-10T14:27:14.029425: step 3489, loss 0.269666, acc 0.921875, prec 0.0603283, recall 0.878898
2017-12-10T14:27:14.215763: step 3490, loss 0.160722, acc 0.921875, prec 0.0603221, recall 0.878898
2017-12-10T14:27:14.404775: step 3491, loss 0.122975, acc 0.96875, prec 0.0603586, recall 0.878971
2017-12-10T14:27:14.595541: step 3492, loss 0.227897, acc 0.875, prec 0.0603876, recall 0.879044
2017-12-10T14:27:14.781122: step 3493, loss 0.269508, acc 0.953125, prec 0.0604034, recall 0.879081
2017-12-10T14:27:14.971822: step 3494, loss 0.0144769, acc 1, prec 0.0604424, recall 0.879154
2017-12-10T14:27:15.159698: step 3495, loss 0.364358, acc 0.90625, prec 0.0604544, recall 0.879191
2017-12-10T14:27:15.352226: step 3496, loss 0.384377, acc 0.921875, prec 0.0604676, recall 0.879227
2017-12-10T14:27:15.535987: step 3497, loss 0.341035, acc 0.9375, prec 0.0605016, recall 0.8793
2017-12-10T14:27:15.722411: step 3498, loss 0.212494, acc 0.90625, prec 0.0604941, recall 0.8793
2017-12-10T14:27:15.909363: step 3499, loss 0.447995, acc 0.875, prec 0.0605035, recall 0.879336
2017-12-10T14:27:16.097548: step 3500, loss 0.0807078, acc 0.96875, prec 0.0605205, recall 0.879373
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_2/1512933336/checkpoints/model-3500

2017-12-10T14:27:17.164215: step 3501, loss 0.283252, acc 0.890625, prec 0.0605312, recall 0.879409
2017-12-10T14:27:17.351449: step 3502, loss 0.271682, acc 0.875, prec 0.0605407, recall 0.879445
2017-12-10T14:27:17.544649: step 3503, loss 0.0396492, acc 1, prec 0.0605602, recall 0.879482
2017-12-10T14:27:17.731264: step 3504, loss 0.242439, acc 0.9375, prec 0.0605941, recall 0.879554
2017-12-10T14:27:17.921107: step 3505, loss 0.321712, acc 0.875, prec 0.0605841, recall 0.879554
2017-12-10T14:27:18.109051: step 3506, loss 0.149867, acc 0.9375, prec 0.060579, recall 0.879554
2017-12-10T14:27:18.296498: step 3507, loss 0.249669, acc 0.953125, prec 0.0606142, recall 0.879627
2017-12-10T14:27:18.489165: step 3508, loss 0.162961, acc 0.921875, prec 0.0606469, recall 0.879699
2017-12-10T14:27:18.674931: step 3509, loss 0.10612, acc 0.953125, prec 0.0606626, recall 0.879735
2017-12-10T14:27:18.864883: step 3510, loss 0.054088, acc 0.96875, prec 0.0606601, recall 0.879735
2017-12-10T14:27:19.057211: step 3511, loss 0.158152, acc 0.984375, prec 0.0607172, recall 0.879844
2017-12-10T14:27:19.245259: step 3512, loss 0.239007, acc 0.953125, prec 0.0607135, recall 0.879844
2017-12-10T14:27:19.437802: step 3513, loss 0.140202, acc 0.953125, prec 0.0607292, recall 0.87988
2017-12-10T14:27:19.628174: step 3514, loss 0.0919495, acc 0.96875, prec 0.0607266, recall 0.87988
2017-12-10T14:27:19.819461: step 3515, loss 0.150473, acc 0.984375, prec 0.0607254, recall 0.87988
2017-12-10T14:27:20.007858: step 3516, loss 0.252894, acc 0.953125, prec 0.0607216, recall 0.87988
2017-12-10T14:27:20.199266: step 3517, loss 0.0803124, acc 0.96875, prec 0.0607386, recall 0.879916
2017-12-10T14:27:20.390870: step 3518, loss 0.469141, acc 0.953125, prec 0.0607932, recall 0.880024
2017-12-10T14:27:20.579938: step 3519, loss 0.124096, acc 0.984375, prec 0.0607919, recall 0.880024
2017-12-10T14:27:20.770065: step 3520, loss 0.775357, acc 0.96875, prec 0.0608283, recall 0.880096
2017-12-10T14:27:20.960960: step 3521, loss 0.237447, acc 0.96875, prec 0.0608842, recall 0.880204
2017-12-10T14:27:21.152710: step 3522, loss 1.97363, acc 0.953125, prec 0.0609011, recall 0.879976
2017-12-10T14:27:21.340568: step 3523, loss 0.320777, acc 0.90625, prec 0.0608935, recall 0.879976
2017-12-10T14:27:21.530566: step 3524, loss 0.088482, acc 0.921875, prec 0.0609067, recall 0.880012
2017-12-10T14:27:21.724474: step 3525, loss 0.699598, acc 0.890625, prec 0.0609173, recall 0.880048
2017-12-10T14:27:21.918131: step 3526, loss 0.117595, acc 0.96875, prec 0.0609342, recall 0.880084
2017-12-10T14:27:22.107433: step 3527, loss 0.247491, acc 0.9375, prec 0.060968, recall 0.880155
2017-12-10T14:27:22.293858: step 3528, loss 0.167948, acc 0.921875, prec 0.0609617, recall 0.880155
2017-12-10T14:27:22.483370: step 3529, loss 0.151422, acc 0.9375, prec 0.0609761, recall 0.880191
2017-12-10T14:27:22.673169: step 3530, loss 0.206901, acc 0.9375, prec 0.0609905, recall 0.880227
2017-12-10T14:27:22.862686: step 3531, loss 0.248634, acc 0.921875, prec 0.0610036, recall 0.880263
2017-12-10T14:27:23.053263: step 3532, loss 0.329617, acc 0.859375, prec 0.0610311, recall 0.880334
2017-12-10T14:27:23.238662: step 3533, loss 0.158899, acc 0.96875, prec 0.0610674, recall 0.880406
2017-12-10T14:27:23.426659: step 3534, loss 0.310854, acc 0.90625, prec 0.0610793, recall 0.880441
2017-12-10T14:27:23.614688: step 3535, loss 0.0511149, acc 0.984375, prec 0.061078, recall 0.880441
2017-12-10T14:27:23.799002: step 3536, loss 0.386097, acc 0.890625, prec 0.0610692, recall 0.880441
2017-12-10T14:27:23.987423: step 3537, loss 0.391052, acc 0.890625, prec 0.0610798, recall 0.880477
2017-12-10T14:27:24.173271: step 3538, loss 0.388591, acc 0.9375, prec 0.0611329, recall 0.880584
2017-12-10T14:27:24.364467: step 3539, loss 0.171623, acc 0.921875, prec 0.0611266, recall 0.880584
2017-12-10T14:27:24.554244: step 3540, loss 0.194682, acc 0.90625, prec 0.061119, recall 0.880584
2017-12-10T14:27:24.743013: step 3541, loss 0.162856, acc 0.953125, prec 0.0611152, recall 0.880584
2017-12-10T14:27:24.935460: step 3542, loss 0.137956, acc 0.9375, prec 0.0611102, recall 0.880584
2017-12-10T14:27:25.124573: step 3543, loss 0.326631, acc 0.90625, prec 0.061122, recall 0.880619
2017-12-10T14:27:25.315315: step 3544, loss 0.19569, acc 0.921875, prec 0.0611157, recall 0.880619
2017-12-10T14:27:25.507855: step 3545, loss 0.267018, acc 0.90625, prec 0.0611081, recall 0.880619
2017-12-10T14:27:25.699792: step 3546, loss 0.358393, acc 0.90625, prec 0.0611199, recall 0.880655
2017-12-10T14:27:25.893677: step 3547, loss 0.409016, acc 0.875, prec 0.0611098, recall 0.880655
2017-12-10T14:27:26.085691: step 3548, loss 0.137153, acc 0.96875, prec 0.0611849, recall 0.880797
2017-12-10T14:27:26.280657: step 3549, loss 0.304133, acc 0.921875, prec 0.0611979, recall 0.880832
2017-12-10T14:27:26.470858: step 3550, loss 0.158887, acc 0.90625, prec 0.0611904, recall 0.880832
2017-12-10T14:27:26.659664: step 3551, loss 0.0759875, acc 0.953125, prec 0.061206, recall 0.880867
2017-12-10T14:27:26.854952: step 3552, loss 0.0761492, acc 0.953125, prec 0.0612022, recall 0.880867
2017-12-10T14:27:27.042902: step 3553, loss 0.132942, acc 0.953125, prec 0.0612178, recall 0.880903
2017-12-10T14:27:27.232992: step 3554, loss 0.0889081, acc 0.953125, prec 0.0612333, recall 0.880938
2017-12-10T14:27:27.418300: step 3555, loss 0.0140919, acc 1, prec 0.0612527, recall 0.880974
2017-12-10T14:27:27.607845: step 3556, loss 0.14399, acc 0.96875, prec 0.0612696, recall 0.881009
2017-12-10T14:27:27.797216: step 3557, loss 0.107994, acc 0.96875, prec 0.061267, recall 0.881009
2017-12-10T14:27:27.988195: step 3558, loss 0.0982847, acc 0.9375, prec 0.061262, recall 0.881009
2017-12-10T14:27:28.179245: step 3559, loss 0.1671, acc 0.96875, prec 0.0612982, recall 0.881079
2017-12-10T14:27:28.372151: step 3560, loss 0.862412, acc 0.984375, prec 0.0613356, recall 0.88115
2017-12-10T14:27:28.562452: step 3561, loss 0.0586725, acc 0.984375, prec 0.0613537, recall 0.881185
2017-12-10T14:27:28.752129: step 3562, loss 7.54113, acc 0.9375, prec 0.0613899, recall 0.880734
2017-12-10T14:27:28.949719: step 3563, loss 0.0955279, acc 0.984375, prec 0.061408, recall 0.880769
2017-12-10T14:27:29.147613: step 3564, loss 0.158197, acc 0.96875, prec 0.0614249, recall 0.880804
2017-12-10T14:27:29.337235: step 3565, loss 0.157702, acc 0.9375, prec 0.0614391, recall 0.88084
2017-12-10T14:27:29.525371: step 3566, loss 0.369826, acc 0.953125, prec 0.0614741, recall 0.88091
2017-12-10T14:27:29.712174: step 3567, loss 0.306752, acc 0.90625, prec 0.0615052, recall 0.88098
2017-12-10T14:27:29.899617: step 3568, loss 0.418217, acc 0.90625, prec 0.0614975, recall 0.88098
2017-12-10T14:27:30.088372: step 3569, loss 0.411375, acc 0.90625, prec 0.0614899, recall 0.88098
2017-12-10T14:27:30.276258: step 3570, loss 0.80549, acc 0.8125, prec 0.0614941, recall 0.881016
2017-12-10T14:27:30.463341: step 3571, loss 0.28349, acc 0.90625, prec 0.0614865, recall 0.881016
2017-12-10T14:27:30.653909: step 3572, loss 0.253152, acc 0.9375, prec 0.0615007, recall 0.881051
2017-12-10T14:27:30.843370: step 3573, loss 0.277596, acc 0.890625, prec 0.0615305, recall 0.881121
2017-12-10T14:27:31.033467: step 3574, loss 0.650581, acc 0.9375, prec 0.0616028, recall 0.881261
2017-12-10T14:27:31.223712: step 3575, loss 0.351923, acc 0.875, prec 0.0615926, recall 0.881261
2017-12-10T14:27:31.415681: step 3576, loss 0.427595, acc 0.890625, prec 0.0615838, recall 0.881261
2017-12-10T14:27:31.600427: step 3577, loss 0.46659, acc 0.859375, prec 0.0615724, recall 0.881261
2017-12-10T14:27:31.786546: step 3578, loss 0.756852, acc 0.84375, prec 0.0616176, recall 0.881366
2017-12-10T14:27:31.977129: step 3579, loss 0.445385, acc 0.84375, prec 0.0616049, recall 0.881366
2017-12-10T14:27:32.166042: step 3580, loss 0.599754, acc 0.8125, prec 0.061609, recall 0.881401
2017-12-10T14:27:32.354422: step 3581, loss 0.240152, acc 0.890625, prec 0.0616002, recall 0.881401
2017-12-10T14:27:32.542218: step 3582, loss 0.36214, acc 0.859375, prec 0.0616081, recall 0.881436
2017-12-10T14:27:32.731225: step 3583, loss 0.225789, acc 0.921875, prec 0.0616017, recall 0.881436
2017-12-10T14:27:32.918308: step 3584, loss 0.399148, acc 0.921875, prec 0.0616147, recall 0.881471
2017-12-10T14:27:33.108855: step 3585, loss 0.187346, acc 0.921875, prec 0.0616276, recall 0.881505
2017-12-10T14:27:33.294034: step 3586, loss 0.290298, acc 0.90625, prec 0.0616779, recall 0.88161
2017-12-10T14:27:33.483928: step 3587, loss 0.154754, acc 0.953125, prec 0.0616741, recall 0.88161
2017-12-10T14:27:33.672751: step 3588, loss 0.499976, acc 0.90625, prec 0.0617243, recall 0.881714
2017-12-10T14:27:33.863476: step 3589, loss 0.167184, acc 0.953125, prec 0.0617398, recall 0.881749
2017-12-10T14:27:34.051836: step 3590, loss 0.0382272, acc 0.984375, prec 0.0617578, recall 0.881783
2017-12-10T14:27:34.241810: step 3591, loss 0.0780947, acc 0.96875, prec 0.0617553, recall 0.881783
2017-12-10T14:27:34.427841: step 3592, loss 0.13486, acc 0.984375, prec 0.0617926, recall 0.881853
2017-12-10T14:27:34.617666: step 3593, loss 0.294142, acc 0.953125, prec 0.0618273, recall 0.881922
2017-12-10T14:27:34.809818: step 3594, loss 0.203403, acc 0.96875, prec 0.0618248, recall 0.881922
2017-12-10T14:27:34.999944: step 3595, loss 0.151772, acc 0.953125, prec 0.0618209, recall 0.881922
2017-12-10T14:27:35.191568: step 3596, loss 0.0706155, acc 0.984375, prec 0.0618389, recall 0.881957
2017-12-10T14:27:35.381577: step 3597, loss 1.9447, acc 0.890625, prec 0.0618313, recall 0.881698
2017-12-10T14:27:35.575244: step 3598, loss 0.169676, acc 0.953125, prec 0.061866, recall 0.881768
2017-12-10T14:27:35.769308: step 3599, loss 0.264553, acc 0.953125, prec 0.0618815, recall 0.881802
2017-12-10T14:27:35.957701: step 3600, loss 0.349004, acc 0.921875, prec 0.0618944, recall 0.881837
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_2/1512933336/checkpoints/model-3600

2017-12-10T14:27:37.333090: step 3601, loss 0.026282, acc 0.984375, prec 0.0618931, recall 0.881837
2017-12-10T14:27:37.526736: step 3602, loss 1.04486, acc 0.953125, prec 0.0619086, recall 0.881871
2017-12-10T14:27:37.715620: step 3603, loss 0.269316, acc 0.90625, prec 0.0619202, recall 0.881906
2017-12-10T14:27:37.907418: step 3604, loss 0.127206, acc 0.921875, prec 0.0619139, recall 0.881906
2017-12-10T14:27:38.099972: step 3605, loss 0.158287, acc 0.984375, prec 0.0619703, recall 0.882009
2017-12-10T14:27:38.289374: step 3606, loss 0.228484, acc 0.96875, prec 0.0620063, recall 0.882078
2017-12-10T14:27:38.477909: step 3607, loss 0.400523, acc 0.921875, prec 0.0619999, recall 0.882078
2017-12-10T14:27:38.668564: step 3608, loss 0.0770668, acc 0.96875, prec 0.0620359, recall 0.882147
2017-12-10T14:27:38.856511: step 3609, loss 0.136841, acc 0.953125, prec 0.0620705, recall 0.882216
2017-12-10T14:27:39.048824: step 3610, loss 0.442168, acc 0.890625, prec 0.0620616, recall 0.882216
2017-12-10T14:27:39.237889: step 3611, loss 0.131304, acc 0.9375, prec 0.0620565, recall 0.882216
2017-12-10T14:27:39.429791: step 3612, loss 0.0642593, acc 0.96875, prec 0.0620732, recall 0.88225
2017-12-10T14:27:39.615483: step 3613, loss 2.2573, acc 0.96875, prec 0.0620719, recall 0.881993
2017-12-10T14:27:39.804619: step 3614, loss 0.165412, acc 0.953125, prec 0.0621066, recall 0.882062
2017-12-10T14:27:39.990870: step 3615, loss 0.240264, acc 0.921875, prec 0.0621194, recall 0.882096
2017-12-10T14:27:40.179684: step 3616, loss 0.249004, acc 0.921875, prec 0.0621515, recall 0.882165
2017-12-10T14:27:40.373406: step 3617, loss 0.394833, acc 0.890625, prec 0.0621426, recall 0.882165
2017-12-10T14:27:40.559071: step 3618, loss 0.101167, acc 0.96875, prec 0.0621401, recall 0.882165
2017-12-10T14:27:40.747296: step 3619, loss 0.338864, acc 0.90625, prec 0.0621516, recall 0.882199
2017-12-10T14:27:40.936516: step 3620, loss 0.605348, acc 0.890625, prec 0.0621619, recall 0.882233
2017-12-10T14:27:41.126608: step 3621, loss 0.164855, acc 0.9375, prec 0.0622337, recall 0.88237
2017-12-10T14:27:41.317278: step 3622, loss 0.496594, acc 0.875, prec 0.0622619, recall 0.882438
2017-12-10T14:27:41.506264: step 3623, loss 0.116558, acc 0.96875, prec 0.0622786, recall 0.882472
2017-12-10T14:27:41.697490: step 3624, loss 0.166593, acc 0.953125, prec 0.0623131, recall 0.882541
2017-12-10T14:27:41.886488: step 3625, loss 0.443827, acc 0.90625, prec 0.0623247, recall 0.882575
2017-12-10T14:27:42.073841: step 3626, loss 0.291801, acc 0.90625, prec 0.062317, recall 0.882575
2017-12-10T14:27:42.265234: step 3627, loss 0.17725, acc 0.953125, prec 0.0623132, recall 0.882575
2017-12-10T14:27:42.453977: step 3628, loss 0.130969, acc 0.953125, prec 0.062367, recall 0.882677
2017-12-10T14:27:42.638626: step 3629, loss 0.192527, acc 0.890625, prec 0.0623772, recall 0.882711
2017-12-10T14:27:42.826179: step 3630, loss 0.245661, acc 0.90625, prec 0.0623887, recall 0.882745
2017-12-10T14:27:43.013539: step 3631, loss 0.392584, acc 0.890625, prec 0.0623798, recall 0.882745
2017-12-10T14:27:43.204465: step 3632, loss 0.372008, acc 0.921875, prec 0.0623926, recall 0.882779
2017-12-10T14:27:43.397405: step 3633, loss 0.149172, acc 0.921875, prec 0.0623862, recall 0.882779
2017-12-10T14:27:43.592969: step 3634, loss 0.212501, acc 0.953125, prec 0.0623824, recall 0.882779
2017-12-10T14:27:43.782990: step 3635, loss 0.0875362, acc 0.953125, prec 0.0623977, recall 0.882812
2017-12-10T14:27:43.977755: step 3636, loss 0.161418, acc 0.9375, prec 0.0624118, recall 0.882846
2017-12-10T14:27:44.174790: step 3637, loss 0.222488, acc 0.9375, prec 0.0624067, recall 0.882846
2017-12-10T14:27:44.364363: step 3638, loss 0.0500763, acc 0.96875, prec 0.0624042, recall 0.882846
2017-12-10T14:27:44.550100: step 3639, loss 0.212018, acc 0.953125, prec 0.0624003, recall 0.882846
2017-12-10T14:27:44.741325: step 3640, loss 1.60763, acc 0.953125, prec 0.0624361, recall 0.882659
2017-12-10T14:27:44.938023: step 3641, loss 0.111471, acc 0.953125, prec 0.0624706, recall 0.882727
2017-12-10T14:27:45.130914: step 3642, loss 0.0752328, acc 0.96875, prec 0.0624872, recall 0.882761
2017-12-10T14:27:45.318775: step 3643, loss 0.242204, acc 0.921875, prec 0.0625, recall 0.882794
2017-12-10T14:27:45.510959: step 3644, loss 0.0663072, acc 0.96875, prec 0.0625166, recall 0.882828
2017-12-10T14:27:45.702551: step 3645, loss 0.0459394, acc 1, prec 0.0625166, recall 0.882828
2017-12-10T14:27:45.887567: step 3646, loss 0.174547, acc 0.953125, prec 0.0625128, recall 0.882828
2017-12-10T14:27:46.076467: step 3647, loss 0.161438, acc 0.9375, prec 0.0625077, recall 0.882828
2017-12-10T14:27:46.265402: step 3648, loss 0.276446, acc 0.921875, prec 0.0625204, recall 0.882862
2017-12-10T14:27:46.453677: step 3649, loss 1.40071, acc 0.96875, prec 0.0625383, recall 0.882641
2017-12-10T14:27:46.650844: step 3650, loss 0.0449511, acc 0.984375, prec 0.0625562, recall 0.882675
2017-12-10T14:27:46.837639: step 3651, loss 0.260503, acc 0.921875, prec 0.0625498, recall 0.882675
2017-12-10T14:27:47.023542: step 3652, loss 0.431039, acc 0.921875, prec 0.0626008, recall 0.882776
2017-12-10T14:27:47.213343: step 3653, loss 3.09534, acc 0.953125, prec 0.0626174, recall 0.882556
2017-12-10T14:27:47.403588: step 3654, loss 0.231206, acc 0.890625, prec 0.0626085, recall 0.882556
2017-12-10T14:27:47.595237: step 3655, loss 0.33444, acc 0.875, prec 0.0625983, recall 0.882556
2017-12-10T14:27:47.785650: step 3656, loss 0.903452, acc 0.859375, prec 0.0626059, recall 0.88259
2017-12-10T14:27:47.974527: step 3657, loss 0.459675, acc 0.875, prec 0.0626339, recall 0.882657
2017-12-10T14:27:48.160835: step 3658, loss 0.672341, acc 0.84375, prec 0.0626212, recall 0.882657
2017-12-10T14:27:48.352637: step 3659, loss 0.571491, acc 0.921875, prec 0.0626339, recall 0.882691
2017-12-10T14:27:48.544423: step 3660, loss 0.550154, acc 0.890625, prec 0.0626249, recall 0.882691
2017-12-10T14:27:48.734635: step 3661, loss 0.686012, acc 0.765625, prec 0.0626058, recall 0.882691
2017-12-10T14:27:48.924991: step 3662, loss 0.453114, acc 0.859375, prec 0.0626134, recall 0.882725
2017-12-10T14:27:49.112351: step 3663, loss 0.234228, acc 0.90625, prec 0.0626058, recall 0.882725
2017-12-10T14:27:49.301426: step 3664, loss 0.528603, acc 0.859375, prec 0.0626516, recall 0.882826
2017-12-10T14:27:49.489400: step 3665, loss 0.484028, acc 0.859375, prec 0.0626401, recall 0.882826
2017-12-10T14:27:49.676851: step 3666, loss 0.329646, acc 0.859375, prec 0.0626286, recall 0.882826
2017-12-10T14:27:49.862633: step 3667, loss 0.4066, acc 0.921875, prec 0.0626222, recall 0.882826
2017-12-10T14:27:50.050117: step 3668, loss 0.251814, acc 0.90625, prec 0.0626337, recall 0.88286
2017-12-10T14:27:50.237075: step 3669, loss 0.0518299, acc 0.96875, prec 0.0626502, recall 0.882893
2017-12-10T14:27:50.424293: step 3670, loss 0.141858, acc 0.9375, prec 0.0626451, recall 0.882893
2017-12-10T14:27:50.613626: step 3671, loss 0.254981, acc 0.90625, prec 0.0626375, recall 0.882893
2017-12-10T14:27:50.801529: step 3672, loss 0.360911, acc 0.9375, prec 0.0626514, recall 0.882927
2017-12-10T14:27:50.989021: step 3673, loss 0.940282, acc 0.9375, prec 0.0626845, recall 0.882994
2017-12-10T14:27:51.180802: step 3674, loss 0.13563, acc 0.96875, prec 0.062701, recall 0.883027
2017-12-10T14:27:51.368588: step 3675, loss 0.654643, acc 0.921875, prec 0.0627328, recall 0.883095
2017-12-10T14:27:51.557081: step 3676, loss 0.204056, acc 0.90625, prec 0.0627251, recall 0.883095
2017-12-10T14:27:51.744143: step 3677, loss 0.0442642, acc 0.96875, prec 0.0627226, recall 0.883095
2017-12-10T14:27:51.930855: step 3678, loss 0.0626653, acc 0.984375, prec 0.0627213, recall 0.883095
2017-12-10T14:27:52.123138: step 3679, loss 0.0334452, acc 0.984375, prec 0.06272, recall 0.883095
2017-12-10T14:27:52.311532: step 3680, loss 0.299888, acc 0.953125, prec 0.0627544, recall 0.883161
2017-12-10T14:27:52.503337: step 3681, loss 0.324559, acc 0.921875, prec 0.0627861, recall 0.883228
2017-12-10T14:27:52.689130: step 3682, loss 0.162269, acc 0.921875, prec 0.0627988, recall 0.883262
2017-12-10T14:27:52.878235: step 3683, loss 0.0784113, acc 0.96875, prec 0.0627962, recall 0.883262
2017-12-10T14:27:53.066208: step 3684, loss 0.33061, acc 0.921875, prec 0.0627898, recall 0.883262
2017-12-10T14:27:53.256437: step 3685, loss 0.255819, acc 0.9375, prec 0.0628229, recall 0.883329
2017-12-10T14:27:53.447288: step 3686, loss 0.10878, acc 0.953125, prec 0.0628381, recall 0.883362
2017-12-10T14:27:53.631820: step 3687, loss 0.122439, acc 0.96875, prec 0.0628736, recall 0.883429
2017-12-10T14:27:53.821300: step 3688, loss 0.409586, acc 0.921875, prec 0.0628673, recall 0.883429
2017-12-10T14:27:54.011437: step 3689, loss 0.248176, acc 0.9375, prec 0.0628812, recall 0.883462
2017-12-10T14:27:54.200431: step 3690, loss 0.0607786, acc 0.96875, prec 0.0628977, recall 0.883495
2017-12-10T14:27:54.388865: step 3691, loss 0.221928, acc 0.96875, prec 0.0628951, recall 0.883495
2017-12-10T14:27:54.574512: step 3692, loss 0.594659, acc 0.90625, prec 0.0629065, recall 0.883528
2017-12-10T14:27:54.765992: step 3693, loss 0.0567721, acc 0.96875, prec 0.0629039, recall 0.883528
2017-12-10T14:27:54.960283: step 3694, loss 0.0195037, acc 1, prec 0.062942, recall 0.883595
2017-12-10T14:27:55.149840: step 3695, loss 0.42482, acc 0.96875, prec 0.0629585, recall 0.883628
2017-12-10T14:27:55.341190: step 3696, loss 0.379279, acc 0.9375, prec 0.0629724, recall 0.883661
2017-12-10T14:27:55.534989: step 3697, loss 0.170417, acc 0.96875, prec 0.063008, recall 0.883728
2017-12-10T14:27:55.724700: step 3698, loss 0.472367, acc 0.953125, prec 0.0630612, recall 0.883827
2017-12-10T14:27:55.914362: step 3699, loss 0.0652779, acc 0.96875, prec 0.0630777, recall 0.88386
2017-12-10T14:27:56.106237: step 3700, loss 0.0670625, acc 0.96875, prec 0.0631132, recall 0.883926
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_2/1512933336/checkpoints/model-3700

2017-12-10T14:27:57.188297: step 3701, loss 0.795162, acc 0.96875, prec 0.0631487, recall 0.883992
2017-12-10T14:27:57.379404: step 3702, loss 0.108857, acc 0.953125, prec 0.0631449, recall 0.883992
2017-12-10T14:27:57.566550: step 3703, loss 0.178357, acc 0.953125, prec 0.063141, recall 0.883992
2017-12-10T14:27:57.757754: step 3704, loss 0.0431709, acc 0.96875, prec 0.0631384, recall 0.883992
2017-12-10T14:27:57.944470: step 3705, loss 0.181164, acc 0.953125, prec 0.0631346, recall 0.883992
2017-12-10T14:27:58.130785: step 3706, loss 0.206956, acc 0.9375, prec 0.0631295, recall 0.883992
2017-12-10T14:27:58.319555: step 3707, loss 0.0670981, acc 0.96875, prec 0.0631269, recall 0.883992
2017-12-10T14:27:58.507874: step 3708, loss 1.05155, acc 0.953125, prec 0.0631611, recall 0.884058
2017-12-10T14:27:58.701808: step 3709, loss 0.104863, acc 0.984375, prec 0.0631979, recall 0.884124
2017-12-10T14:27:58.891875: step 3710, loss 0.264073, acc 0.921875, prec 0.0631914, recall 0.884124
2017-12-10T14:27:59.082984: step 3711, loss 0.0219251, acc 1, prec 0.0631914, recall 0.884124
2017-12-10T14:27:59.276221: step 3712, loss 0.29313, acc 0.921875, prec 0.063185, recall 0.884124
2017-12-10T14:27:59.465033: step 3713, loss 0.272703, acc 0.953125, prec 0.0632572, recall 0.884255
2017-12-10T14:27:59.654675: step 3714, loss 0.230491, acc 0.9375, prec 0.0632521, recall 0.884255
2017-12-10T14:27:59.843468: step 3715, loss 0.257929, acc 0.921875, prec 0.0632647, recall 0.884288
2017-12-10T14:28:00.031711: step 3716, loss 0.0727695, acc 0.953125, prec 0.0632988, recall 0.884354
2017-12-10T14:28:00.222254: step 3717, loss 0.482359, acc 0.9375, prec 0.0632937, recall 0.884354
2017-12-10T14:28:00.414249: step 3718, loss 0.145381, acc 0.9375, prec 0.0633266, recall 0.884419
2017-12-10T14:28:00.604563: step 3719, loss 0.381279, acc 0.84375, prec 0.0633517, recall 0.884485
2017-12-10T14:28:00.792020: step 3720, loss 0.480962, acc 0.875, prec 0.0633414, recall 0.884485
2017-12-10T14:28:00.981081: step 3721, loss 0.274865, acc 0.890625, prec 0.0633514, recall 0.884517
2017-12-10T14:28:01.173435: step 3722, loss 0.0922813, acc 0.953125, prec 0.0633856, recall 0.884583
2017-12-10T14:28:01.363798: step 3723, loss 0.286548, acc 0.953125, prec 0.0633817, recall 0.884583
2017-12-10T14:28:01.554136: step 3724, loss 0.0929472, acc 0.96875, prec 0.0633981, recall 0.884615
2017-12-10T14:28:01.740782: step 3725, loss 0.33716, acc 0.90625, prec 0.0633904, recall 0.884615
2017-12-10T14:28:01.929920: step 3726, loss 0.0912839, acc 0.96875, prec 0.0633878, recall 0.884615
2017-12-10T14:28:02.125098: step 3727, loss 0.0498969, acc 0.984375, prec 0.0634055, recall 0.884648
2017-12-10T14:28:02.316231: step 3728, loss 0.123729, acc 0.9375, prec 0.0634384, recall 0.884713
2017-12-10T14:28:02.502019: step 3729, loss 0.0584638, acc 0.96875, prec 0.0634358, recall 0.884713
2017-12-10T14:28:02.696167: step 3730, loss 0.0537768, acc 0.96875, prec 0.0634332, recall 0.884713
2017-12-10T14:28:02.886105: step 3731, loss 2.89932, acc 0.96875, prec 0.0634509, recall 0.884496
2017-12-10T14:28:03.077188: step 3732, loss 1.66445, acc 0.90625, prec 0.0634634, recall 0.884279
2017-12-10T14:28:03.268676: step 3733, loss 0.172066, acc 0.921875, prec 0.0635139, recall 0.884377
2017-12-10T14:28:03.458633: step 3734, loss 0.299549, acc 0.921875, prec 0.0635454, recall 0.884442
2017-12-10T14:28:03.650576: step 3735, loss 0.103239, acc 0.953125, prec 0.0635416, recall 0.884442
2017-12-10T14:28:03.837605: step 3736, loss 0.297009, acc 0.9375, prec 0.0635554, recall 0.884475
2017-12-10T14:28:04.026058: step 3737, loss 0.946038, acc 0.84375, prec 0.0635615, recall 0.884507
2017-12-10T14:28:04.217424: step 3738, loss 0.389264, acc 0.859375, prec 0.0635688, recall 0.88454
2017-12-10T14:28:04.405653: step 3739, loss 0.192915, acc 0.90625, prec 0.0635801, recall 0.884572
2017-12-10T14:28:04.592987: step 3740, loss 0.302147, acc 0.859375, prec 0.0635685, recall 0.884572
2017-12-10T14:28:04.782399: step 3741, loss 0.165567, acc 0.96875, prec 0.0636038, recall 0.884637
2017-12-10T14:28:04.966702: step 3742, loss 0.277529, acc 0.9375, prec 0.0635987, recall 0.884637
2017-12-10T14:28:05.155776: step 3743, loss 0.512088, acc 0.875, prec 0.0636263, recall 0.884702
2017-12-10T14:28:05.344333: step 3744, loss 0.563429, acc 0.828125, prec 0.063631, recall 0.884734
2017-12-10T14:28:05.534663: step 3745, loss 0.503909, acc 0.875, prec 0.0636207, recall 0.884734
2017-12-10T14:28:05.721977: step 3746, loss 0.774728, acc 0.8125, prec 0.0636242, recall 0.884767
2017-12-10T14:28:05.911140: step 3747, loss 0.457406, acc 0.84375, prec 0.0636492, recall 0.884831
2017-12-10T14:28:06.098955: step 3748, loss 0.462803, acc 0.859375, prec 0.0636566, recall 0.884864
2017-12-10T14:28:06.283694: step 3749, loss 0.257695, acc 0.890625, prec 0.0636665, recall 0.884896
2017-12-10T14:28:06.469562: step 3750, loss 0.650685, acc 0.828125, prec 0.0636712, recall 0.884928
2017-12-10T14:28:06.658947: step 3751, loss 0.253198, acc 0.921875, prec 0.0636837, recall 0.884961
2017-12-10T14:28:06.847887: step 3752, loss 0.375391, acc 0.875, prec 0.0636734, recall 0.884961
2017-12-10T14:28:07.036115: step 3753, loss 0.472394, acc 0.875, prec 0.0636821, recall 0.884993
2017-12-10T14:28:07.222237: step 3754, loss 0.222661, acc 0.90625, prec 0.0636932, recall 0.885025
2017-12-10T14:28:07.413124: step 3755, loss 0.316805, acc 0.921875, prec 0.0636868, recall 0.885025
2017-12-10T14:28:07.602631: step 3756, loss 0.506758, acc 0.875, prec 0.0636954, recall 0.885057
2017-12-10T14:28:07.788927: step 3757, loss 0.31755, acc 0.953125, prec 0.0637293, recall 0.885122
2017-12-10T14:28:07.980678: step 3758, loss 0.195772, acc 0.953125, prec 0.0637255, recall 0.885122
2017-12-10T14:28:08.172491: step 3759, loss 0.0838772, acc 0.953125, prec 0.0637216, recall 0.885122
2017-12-10T14:28:08.363895: step 3760, loss 0.371951, acc 0.953125, prec 0.0637367, recall 0.885154
2017-12-10T14:28:08.554323: step 3761, loss 0.256491, acc 0.9375, prec 0.0637504, recall 0.885186
2017-12-10T14:28:08.747304: step 3762, loss 0.197107, acc 0.96875, prec 0.0637667, recall 0.885218
2017-12-10T14:28:08.936810: step 3763, loss 0.0800855, acc 0.953125, prec 0.0637629, recall 0.885218
2017-12-10T14:28:09.128752: step 3764, loss 0.0164912, acc 1, prec 0.0637817, recall 0.885251
2017-12-10T14:28:09.322029: step 3765, loss 0.0600128, acc 0.953125, prec 0.0637779, recall 0.885251
2017-12-10T14:28:09.513587: step 3766, loss 0.0334587, acc 1, prec 0.0638156, recall 0.885315
2017-12-10T14:28:09.703407: step 3767, loss 0.0908323, acc 0.953125, prec 0.0638118, recall 0.885315
2017-12-10T14:28:09.894520: step 3768, loss 0.022829, acc 1, prec 0.0638306, recall 0.885347
2017-12-10T14:28:10.088515: step 3769, loss 0.46428, acc 0.921875, prec 0.0638431, recall 0.885379
2017-12-10T14:28:10.277751: step 3770, loss 0.306612, acc 0.96875, prec 0.0638594, recall 0.885411
2017-12-10T14:28:10.466262: step 3771, loss 0.0366096, acc 0.984375, prec 0.0638581, recall 0.885411
2017-12-10T14:28:10.655495: step 3772, loss 0.0253814, acc 0.984375, prec 0.0638757, recall 0.885443
2017-12-10T14:28:10.842574: step 3773, loss 0.251506, acc 0.9375, prec 0.0638705, recall 0.885443
2017-12-10T14:28:11.032356: step 3774, loss 0.153425, acc 0.953125, prec 0.0638855, recall 0.885475
2017-12-10T14:28:11.222405: step 3775, loss 0.153213, acc 0.96875, prec 0.063883, recall 0.885475
2017-12-10T14:28:11.412593: step 3776, loss 4.45271, acc 0.96875, prec 0.0639194, recall 0.885292
2017-12-10T14:28:11.607868: step 3777, loss 0.0340928, acc 0.984375, prec 0.0639181, recall 0.885292
2017-12-10T14:28:11.794298: step 3778, loss 0.214841, acc 0.984375, prec 0.0639357, recall 0.885324
2017-12-10T14:28:11.983054: step 3779, loss 0.0751822, acc 0.96875, prec 0.0640085, recall 0.885451
2017-12-10T14:28:12.172124: step 3780, loss 0.0682933, acc 0.96875, prec 0.064006, recall 0.885451
2017-12-10T14:28:12.363990: step 3781, loss 0.095179, acc 0.953125, prec 0.0640021, recall 0.885451
2017-12-10T14:28:12.552341: step 3782, loss 0.198849, acc 0.921875, prec 0.0640334, recall 0.885515
2017-12-10T14:28:12.740024: step 3783, loss 0.119698, acc 0.96875, prec 0.0640496, recall 0.885547
2017-12-10T14:28:12.929907: step 3784, loss 0.316676, acc 0.921875, prec 0.0640432, recall 0.885547
2017-12-10T14:28:13.117723: step 3785, loss 0.179101, acc 0.953125, prec 0.0640582, recall 0.885579
2017-12-10T14:28:13.312546: step 3786, loss 0.391493, acc 0.953125, prec 0.064092, recall 0.885643
2017-12-10T14:28:13.501187: step 3787, loss 0.313841, acc 0.921875, prec 0.0640855, recall 0.885643
2017-12-10T14:28:13.688142: step 3788, loss 0.229407, acc 0.9375, prec 0.0641181, recall 0.885706
2017-12-10T14:28:13.877039: step 3789, loss 1.77033, acc 0.953125, prec 0.0641908, recall 0.885587
2017-12-10T14:28:14.073509: step 3790, loss 0.175733, acc 0.953125, prec 0.0642058, recall 0.885619
2017-12-10T14:28:14.264191: step 3791, loss 0.208462, acc 0.90625, prec 0.0642169, recall 0.885651
2017-12-10T14:28:14.451882: step 3792, loss 0.203382, acc 0.890625, prec 0.0642266, recall 0.885683
2017-12-10T14:28:14.641069: step 3793, loss 0.249756, acc 0.921875, prec 0.0642202, recall 0.885683
2017-12-10T14:28:14.828242: step 3794, loss 0.168377, acc 0.96875, prec 0.0642176, recall 0.885683
2017-12-10T14:28:15.015129: step 3795, loss 0.479606, acc 0.828125, prec 0.0642222, recall 0.885714
2017-12-10T14:28:15.201198: step 3796, loss 0.827633, acc 0.859375, prec 0.0642106, recall 0.885714
2017-12-10T14:28:15.389603: step 3797, loss 0.700322, acc 0.8125, prec 0.0642327, recall 0.885778
2017-12-10T14:28:15.577589: step 3798, loss 0.232358, acc 0.90625, prec 0.064225, recall 0.885778
2017-12-10T14:28:15.766594: step 3799, loss 0.248961, acc 0.890625, prec 0.0642347, recall 0.885809
2017-12-10T14:28:15.953546: step 3800, loss 0.406034, acc 0.890625, prec 0.0642445, recall 0.885841
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_2/1512933336/checkpoints/model-3800

2017-12-10T14:28:17.111244: step 3801, loss 0.397219, acc 0.875, prec 0.064253, recall 0.885873
2017-12-10T14:28:17.300156: step 3802, loss 0.710534, acc 0.859375, prec 0.0642414, recall 0.885873
2017-12-10T14:28:17.485536: step 3803, loss 0.452288, acc 0.921875, prec 0.0642537, recall 0.885904
2017-12-10T14:28:17.675244: step 3804, loss 0.244882, acc 0.921875, prec 0.0642473, recall 0.885904
2017-12-10T14:28:17.864818: step 3805, loss 0.341555, acc 0.90625, prec 0.0642395, recall 0.885904
2017-12-10T14:28:18.055291: step 3806, loss 0.100421, acc 0.953125, prec 0.0642544, recall 0.885936
2017-12-10T14:28:18.242646: step 3807, loss 0.0276347, acc 1, prec 0.0642544, recall 0.885936
2017-12-10T14:28:18.429711: step 3808, loss 0.0727472, acc 0.953125, prec 0.0642694, recall 0.885967
2017-12-10T14:28:18.619381: step 3809, loss 0.198778, acc 0.953125, prec 0.0643031, recall 0.88603
2017-12-10T14:28:18.810780: step 3810, loss 0.228611, acc 0.90625, prec 0.0643141, recall 0.886062
2017-12-10T14:28:18.998701: step 3811, loss 0.121217, acc 0.984375, prec 0.0643504, recall 0.886125
2017-12-10T14:28:19.187046: step 3812, loss 0.0229625, acc 1, prec 0.0643504, recall 0.886125
2017-12-10T14:28:19.378936: step 3813, loss 0.321227, acc 0.9375, prec 0.064364, recall 0.886156
2017-12-10T14:28:19.569550: step 3814, loss 0.0678688, acc 0.953125, prec 0.0643601, recall 0.886156
2017-12-10T14:28:19.756945: step 3815, loss 0.0174685, acc 1, prec 0.0643789, recall 0.886188
2017-12-10T14:28:19.944959: step 3816, loss 0.0269653, acc 0.96875, prec 0.0643763, recall 0.886188
2017-12-10T14:28:20.130971: step 3817, loss 0.0500518, acc 0.984375, prec 0.0644126, recall 0.886251
2017-12-10T14:28:20.319039: step 3818, loss 0.241711, acc 0.953125, prec 0.0644087, recall 0.886251
2017-12-10T14:28:20.505890: step 3819, loss 0.111172, acc 0.984375, prec 0.0644074, recall 0.886251
2017-12-10T14:28:20.697515: step 3820, loss 0.0584926, acc 0.96875, prec 0.0644048, recall 0.886251
2017-12-10T14:28:20.887295: step 3821, loss 0.172178, acc 0.96875, prec 0.0644022, recall 0.886251
2017-12-10T14:28:21.073354: step 3822, loss 0.0478628, acc 0.984375, prec 0.0644009, recall 0.886251
2017-12-10T14:28:21.264116: step 3823, loss 0.0430406, acc 0.984375, prec 0.0643996, recall 0.886251
2017-12-10T14:28:21.454100: step 3824, loss 0.0138599, acc 1, prec 0.0644184, recall 0.886282
2017-12-10T14:28:21.644524: step 3825, loss 0.171952, acc 0.96875, prec 0.0644158, recall 0.886282
2017-12-10T14:28:21.832063: step 3826, loss 0.112971, acc 0.96875, prec 0.0644132, recall 0.886282
2017-12-10T14:28:22.026706: step 3827, loss 0.0890737, acc 0.984375, prec 0.0644307, recall 0.886313
2017-12-10T14:28:22.216681: step 3828, loss 0.0972672, acc 0.96875, prec 0.0644281, recall 0.886313
2017-12-10T14:28:22.407812: step 3829, loss 0.0214437, acc 0.984375, prec 0.0644268, recall 0.886313
2017-12-10T14:28:22.597696: step 3830, loss 0.199277, acc 0.953125, prec 0.0644417, recall 0.886345
2017-12-10T14:28:22.789265: step 3831, loss 0.273017, acc 0.984375, prec 0.0644592, recall 0.886376
2017-12-10T14:28:22.983822: step 3832, loss 6.91911, acc 0.96875, prec 0.0644767, recall 0.886163
2017-12-10T14:28:23.178988: step 3833, loss 0.0798568, acc 0.984375, prec 0.0645129, recall 0.886226
2017-12-10T14:28:23.369929: step 3834, loss 0.184878, acc 0.984375, prec 0.0645116, recall 0.886226
2017-12-10T14:28:23.558034: step 3835, loss 0.134028, acc 0.96875, prec 0.064509, recall 0.886226
2017-12-10T14:28:23.745505: step 3836, loss 0.0400537, acc 0.96875, prec 0.0645252, recall 0.886257
2017-12-10T14:28:23.933398: step 3837, loss 0.219447, acc 0.921875, prec 0.0645375, recall 0.886289
2017-12-10T14:28:24.122420: step 3838, loss 0.109158, acc 0.9375, prec 0.0645323, recall 0.886289
2017-12-10T14:28:24.307994: step 3839, loss 0.238337, acc 0.921875, prec 0.0645821, recall 0.886382
2017-12-10T14:28:24.499670: step 3840, loss 0.360616, acc 0.90625, prec 0.0645931, recall 0.886414
2017-12-10T14:28:24.690234: step 3841, loss 0.345305, acc 0.921875, prec 0.0646053, recall 0.886445
2017-12-10T14:28:24.881988: step 3842, loss 0.0939381, acc 0.9375, prec 0.0646002, recall 0.886445
2017-12-10T14:28:25.075807: step 3843, loss 0.324905, acc 0.921875, prec 0.0645937, recall 0.886445
2017-12-10T14:28:25.262069: step 3844, loss 0.228835, acc 0.9375, prec 0.0645885, recall 0.886445
2017-12-10T14:28:25.456343: step 3845, loss 2.464, acc 0.90625, prec 0.0646008, recall 0.886232
2017-12-10T14:28:25.649336: step 3846, loss 0.442056, acc 0.859375, prec 0.0646079, recall 0.886264
2017-12-10T14:28:25.839232: step 3847, loss 0.643381, acc 0.890625, prec 0.0646363, recall 0.886326
2017-12-10T14:28:26.027216: step 3848, loss 0.354137, acc 0.890625, prec 0.0646459, recall 0.886357
2017-12-10T14:28:26.213836: step 3849, loss 0.302127, acc 0.890625, prec 0.0646743, recall 0.88642
2017-12-10T14:28:26.404608: step 3850, loss 0.4703, acc 0.859375, prec 0.0646627, recall 0.88642
2017-12-10T14:28:26.597023: step 3851, loss 0.2618, acc 0.90625, prec 0.0646736, recall 0.886451
2017-12-10T14:28:26.785821: step 3852, loss 0.771974, acc 0.796875, prec 0.0646755, recall 0.886482
2017-12-10T14:28:26.974667: step 3853, loss 0.252466, acc 0.90625, prec 0.0647052, recall 0.886544
2017-12-10T14:28:27.164864: step 3854, loss 0.53299, acc 0.875, prec 0.0647135, recall 0.886575
2017-12-10T14:28:27.353372: step 3855, loss 0.478816, acc 0.890625, prec 0.0647232, recall 0.886606
2017-12-10T14:28:27.542071: step 3856, loss 0.294173, acc 0.921875, prec 0.0647354, recall 0.886637
2017-12-10T14:28:27.733585: step 3857, loss 0.394786, acc 0.875, prec 0.064725, recall 0.886637
2017-12-10T14:28:27.921462: step 3858, loss 0.219892, acc 0.921875, prec 0.0647186, recall 0.886637
2017-12-10T14:28:28.110526: step 3859, loss 0.145548, acc 0.90625, prec 0.0647108, recall 0.886637
2017-12-10T14:28:28.299124: step 3860, loss 0.658582, acc 0.796875, prec 0.064694, recall 0.886637
2017-12-10T14:28:28.491846: step 3861, loss 0.399993, acc 0.90625, prec 0.0647423, recall 0.88673
2017-12-10T14:28:28.682607: step 3862, loss 0.158776, acc 0.921875, prec 0.0647358, recall 0.88673
2017-12-10T14:28:28.875073: step 3863, loss 0.101974, acc 0.953125, prec 0.064732, recall 0.88673
2017-12-10T14:28:29.077129: step 3864, loss 0.174605, acc 0.96875, prec 0.0647481, recall 0.886761
2017-12-10T14:28:29.267832: step 3865, loss 0.459643, acc 0.890625, prec 0.064739, recall 0.886761
2017-12-10T14:28:29.456845: step 3866, loss 0.326355, acc 0.921875, prec 0.0647512, recall 0.886792
2017-12-10T14:28:29.650197: step 3867, loss 0.0949618, acc 0.96875, prec 0.064786, recall 0.886854
2017-12-10T14:28:29.837201: step 3868, loss 4.15013, acc 0.9375, prec 0.0648754, recall 0.886767
2017-12-10T14:28:30.030754: step 3869, loss 0.111982, acc 0.953125, prec 0.0649089, recall 0.886828
2017-12-10T14:28:30.220778: step 3870, loss 0.15908, acc 0.953125, prec 0.064905, recall 0.886828
2017-12-10T14:28:30.412139: step 3871, loss 0.0764633, acc 0.984375, prec 0.0649037, recall 0.886828
2017-12-10T14:28:30.602946: step 3872, loss 0.216802, acc 0.9375, prec 0.0649545, recall 0.886921
2017-12-10T14:28:30.795459: step 3873, loss 0.172381, acc 0.96875, prec 0.0649519, recall 0.886921
2017-12-10T14:28:30.987844: step 3874, loss 0.287088, acc 0.921875, prec 0.0649454, recall 0.886921
2017-12-10T14:28:31.183886: step 3875, loss 0.202663, acc 0.9375, prec 0.0649776, recall 0.886983
2017-12-10T14:28:31.376987: step 3876, loss 0.399356, acc 0.859375, prec 0.0649659, recall 0.886983
2017-12-10T14:28:31.568812: step 3877, loss 0.460203, acc 0.90625, prec 0.0649768, recall 0.887013
2017-12-10T14:28:31.759261: step 3878, loss 0.212706, acc 0.90625, prec 0.0649876, recall 0.887044
2017-12-10T14:28:31.949461: step 3879, loss 0.157006, acc 0.953125, prec 0.0650397, recall 0.887136
2017-12-10T14:28:32.135771: step 3880, loss 0.262983, acc 0.875, prec 0.0650293, recall 0.887136
2017-12-10T14:28:32.328973: step 3881, loss 0.157723, acc 0.9375, prec 0.0650428, recall 0.887167
2017-12-10T14:28:32.521608: step 3882, loss 0.311625, acc 0.875, prec 0.065051, recall 0.887198
2017-12-10T14:28:32.712178: step 3883, loss 0.301, acc 0.9375, prec 0.0650645, recall 0.887228
2017-12-10T14:28:32.902939: step 3884, loss 0.0882557, acc 0.96875, prec 0.0650805, recall 0.887259
2017-12-10T14:28:33.090708: step 3885, loss 0.359418, acc 0.875, prec 0.0650888, recall 0.88729
2017-12-10T14:28:33.283778: step 3886, loss 0.231233, acc 0.90625, prec 0.0651182, recall 0.887351
2017-12-10T14:28:33.476912: step 3887, loss 0.329806, acc 0.90625, prec 0.0651291, recall 0.887381
2017-12-10T14:28:33.663629: step 3888, loss 0.269182, acc 0.875, prec 0.0651373, recall 0.887412
2017-12-10T14:28:33.860817: step 3889, loss 1.93784, acc 0.953125, prec 0.0651533, recall 0.887202
2017-12-10T14:28:34.053475: step 3890, loss 0.238332, acc 0.9375, prec 0.0651854, recall 0.887263
2017-12-10T14:28:34.244792: step 3891, loss 0.0637904, acc 0.984375, prec 0.0652027, recall 0.887293
2017-12-10T14:28:34.433613: step 3892, loss 0.424426, acc 0.90625, prec 0.0652135, recall 0.887324
2017-12-10T14:28:34.623626: step 3893, loss 0.611361, acc 0.84375, prec 0.0652005, recall 0.887324
2017-12-10T14:28:34.811832: step 3894, loss 0.29592, acc 0.90625, prec 0.0651927, recall 0.887324
2017-12-10T14:28:35.003458: step 3895, loss 0.312216, acc 0.890625, prec 0.0652023, recall 0.887354
2017-12-10T14:28:35.196485: step 3896, loss 0.118818, acc 0.96875, prec 0.0652369, recall 0.887415
2017-12-10T14:28:35.387509: step 3897, loss 0.366522, acc 0.921875, prec 0.065249, recall 0.887446
2017-12-10T14:28:35.578082: step 3898, loss 0.113314, acc 0.953125, prec 0.0652451, recall 0.887446
2017-12-10T14:28:35.768784: step 3899, loss 0.350247, acc 0.953125, prec 0.0652412, recall 0.887446
2017-12-10T14:28:35.957351: step 3900, loss 0.122052, acc 0.96875, prec 0.0652386, recall 0.887446
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_2/1512933336/checkpoints/model-3900

2017-12-10T14:28:37.172627: step 3901, loss 0.14947, acc 0.984375, prec 0.0652373, recall 0.887446
2017-12-10T14:28:37.366267: step 3902, loss 0.180027, acc 0.984375, prec 0.065236, recall 0.887446
2017-12-10T14:28:37.554620: step 3903, loss 0.0384709, acc 1, prec 0.0652732, recall 0.887507
2017-12-10T14:28:37.746753: step 3904, loss 0.126026, acc 0.9375, prec 0.0652866, recall 0.887537
2017-12-10T14:28:37.936188: step 3905, loss 0.0969134, acc 0.984375, prec 0.0653038, recall 0.887568
2017-12-10T14:28:38.130415: step 3906, loss 0.0675928, acc 0.96875, prec 0.0653013, recall 0.887568
2017-12-10T14:28:38.320813: step 3907, loss 0.130716, acc 0.984375, prec 0.0653, recall 0.887568
2017-12-10T14:28:38.513030: step 3908, loss 0.0553267, acc 0.984375, prec 0.0652987, recall 0.887568
2017-12-10T14:28:38.703244: step 3909, loss 0.0906844, acc 0.96875, prec 0.0652961, recall 0.887568
2017-12-10T14:28:38.892172: step 3910, loss 0.0204838, acc 1, prec 0.0652961, recall 0.887568
2017-12-10T14:28:39.086498: step 3911, loss 0.122696, acc 0.9375, prec 0.0652909, recall 0.887568
2017-12-10T14:28:39.274996: step 3912, loss 0.157708, acc 0.96875, prec 0.0653254, recall 0.887628
2017-12-10T14:28:39.466848: step 3913, loss 0.186912, acc 0.9375, prec 0.0653202, recall 0.887628
2017-12-10T14:28:39.659460: step 3914, loss 0.29745, acc 0.9375, prec 0.0653336, recall 0.887659
2017-12-10T14:28:39.850267: step 3915, loss 0.0670531, acc 1, prec 0.0653522, recall 0.887689
2017-12-10T14:28:40.043392: step 3916, loss 0.0849536, acc 0.96875, prec 0.0653868, recall 0.88775
2017-12-10T14:28:40.231352: step 3917, loss 0.0589586, acc 0.984375, prec 0.0653855, recall 0.88775
2017-12-10T14:28:40.427236: step 3918, loss 1.17218, acc 0.984375, prec 0.0654213, recall 0.88781
2017-12-10T14:28:40.621369: step 3919, loss 0.0737969, acc 0.984375, prec 0.06542, recall 0.88781
2017-12-10T14:28:40.809897: step 3920, loss 0.394481, acc 0.921875, prec 0.0654321, recall 0.88784
2017-12-10T14:28:41.001934: step 3921, loss 2.05788, acc 0.984375, prec 0.0654878, recall 0.887692
2017-12-10T14:28:41.194368: step 3922, loss 0.554508, acc 0.9375, prec 0.0655197, recall 0.887752
2017-12-10T14:28:41.390812: step 3923, loss 0.417218, acc 0.9375, prec 0.0655516, recall 0.887813
2017-12-10T14:28:41.585617: step 3924, loss 0.187129, acc 0.953125, prec 0.0655663, recall 0.887843
2017-12-10T14:28:41.777327: step 3925, loss 0.176605, acc 0.96875, prec 0.0655637, recall 0.887843
2017-12-10T14:28:41.966367: step 3926, loss 0.0550137, acc 0.96875, prec 0.0655982, recall 0.887903
2017-12-10T14:28:42.159840: step 3927, loss 0.28187, acc 0.890625, prec 0.0656076, recall 0.887933
2017-12-10T14:28:42.351148: step 3928, loss 0.27158, acc 0.9375, prec 0.0656024, recall 0.887933
2017-12-10T14:28:42.540808: step 3929, loss 0.493973, acc 0.921875, prec 0.0655959, recall 0.887933
2017-12-10T14:28:42.727680: step 3930, loss 0.300427, acc 0.9375, prec 0.0655907, recall 0.887933
2017-12-10T14:28:42.918256: step 3931, loss 0.351431, acc 0.875, prec 0.0655803, recall 0.887933
2017-12-10T14:28:43.109206: step 3932, loss 0.505487, acc 0.84375, prec 0.0655673, recall 0.887933
2017-12-10T14:28:43.297525: step 3933, loss 0.31223, acc 0.921875, prec 0.0655978, recall 0.887994
2017-12-10T14:28:43.486826: step 3934, loss 0.173987, acc 0.9375, prec 0.0656112, recall 0.888024
2017-12-10T14:28:43.679654: step 3935, loss 0.203559, acc 0.9375, prec 0.065606, recall 0.888024
2017-12-10T14:28:43.872493: step 3936, loss 0.615885, acc 0.890625, prec 0.0655969, recall 0.888024
2017-12-10T14:28:44.069277: step 3937, loss 0.122843, acc 0.9375, prec 0.0655917, recall 0.888024
2017-12-10T14:28:44.276198: step 3938, loss 0.278331, acc 0.890625, prec 0.0655826, recall 0.888024
2017-12-10T14:28:44.470770: step 3939, loss 0.325084, acc 0.921875, prec 0.0655946, recall 0.888054
2017-12-10T14:28:44.688867: step 3940, loss 0.0186378, acc 1, prec 0.0655946, recall 0.888054
2017-12-10T14:28:44.881002: step 3941, loss 0.505075, acc 0.890625, prec 0.0655855, recall 0.888054
2017-12-10T14:28:45.072961: step 3942, loss 0.328163, acc 0.90625, prec 0.0656147, recall 0.888114
2017-12-10T14:28:45.275960: step 3943, loss 0.21405, acc 0.921875, prec 0.0656082, recall 0.888114
2017-12-10T14:28:45.465881: step 3944, loss 1.08633, acc 0.96875, prec 0.0656241, recall 0.888144
2017-12-10T14:28:45.659577: step 3945, loss 0.263043, acc 0.953125, prec 0.0656388, recall 0.888174
2017-12-10T14:28:45.851119: step 3946, loss 0.244167, acc 0.9375, prec 0.0656335, recall 0.888174
2017-12-10T14:28:46.051420: step 3947, loss 0.12416, acc 0.9375, prec 0.0656469, recall 0.888204
2017-12-10T14:28:46.244681: step 3948, loss 0.254717, acc 0.921875, prec 0.0656589, recall 0.888234
2017-12-10T14:28:46.437048: step 3949, loss 0.404128, acc 0.9375, prec 0.0656722, recall 0.888264
2017-12-10T14:28:46.630500: step 3950, loss 0.031536, acc 0.984375, prec 0.0656709, recall 0.888264
2017-12-10T14:28:46.821109: step 3951, loss 0.102725, acc 0.96875, prec 0.0657053, recall 0.888323
2017-12-10T14:28:47.015521: step 3952, loss 0.0799972, acc 0.96875, prec 0.0657212, recall 0.888353
2017-12-10T14:28:47.207773: step 3953, loss 0.978926, acc 0.9375, prec 0.0657715, recall 0.888443
2017-12-10T14:28:47.398443: step 3954, loss 0.0867725, acc 0.953125, prec 0.0657676, recall 0.888443
2017-12-10T14:28:47.586594: step 3955, loss 0.178665, acc 0.953125, prec 0.0657822, recall 0.888473
2017-12-10T14:28:47.776086: step 3956, loss 0.245561, acc 0.890625, prec 0.0658101, recall 0.888532
2017-12-10T14:28:47.964691: step 3957, loss 0.203539, acc 0.953125, prec 0.0658246, recall 0.888562
2017-12-10T14:28:48.156988: step 3958, loss 0.0665072, acc 0.96875, prec 0.065822, recall 0.888562
2017-12-10T14:28:48.348773: step 3959, loss 0.153791, acc 0.953125, prec 0.0658366, recall 0.888592
2017-12-10T14:28:48.542518: step 3960, loss 0.25851, acc 0.953125, prec 0.0658697, recall 0.888652
2017-12-10T14:28:48.733945: step 3961, loss 0.238856, acc 0.9375, prec 0.065883, recall 0.888681
2017-12-10T14:28:48.927963: step 3962, loss 0.209009, acc 0.953125, prec 0.065879, recall 0.888681
2017-12-10T14:28:49.118839: step 3963, loss 2.99944, acc 0.953125, prec 0.0658949, recall 0.888474
2017-12-10T14:28:49.316312: step 3964, loss 0.329334, acc 0.921875, prec 0.0659254, recall 0.888533
2017-12-10T14:28:49.508592: step 3965, loss 0.290619, acc 0.953125, prec 0.0659215, recall 0.888533
2017-12-10T14:28:49.697774: step 3966, loss 0.263037, acc 0.921875, prec 0.0659149, recall 0.888533
2017-12-10T14:28:49.889543: step 3967, loss 0.100177, acc 0.96875, prec 0.0659308, recall 0.888563
2017-12-10T14:28:50.081020: step 3968, loss 0.117662, acc 0.953125, prec 0.0659454, recall 0.888593
2017-12-10T14:28:50.272872: step 3969, loss 0.259635, acc 0.921875, prec 0.0659573, recall 0.888622
2017-12-10T14:28:50.462681: step 3970, loss 0.219661, acc 0.953125, prec 0.0659719, recall 0.888652
2017-12-10T14:28:50.653119: step 3971, loss 0.5713, acc 0.84375, prec 0.0659958, recall 0.888711
2017-12-10T14:28:50.845997: step 3972, loss 0.398648, acc 0.921875, prec 0.0660262, recall 0.888771
2017-12-10T14:28:51.037028: step 3973, loss 0.418031, acc 0.90625, prec 0.0660368, recall 0.8888
2017-12-10T14:28:51.227546: step 3974, loss 0.0951862, acc 0.984375, prec 0.0660355, recall 0.8888
2017-12-10T14:28:51.420064: step 3975, loss 0.393434, acc 0.890625, prec 0.0660448, recall 0.88883
2017-12-10T14:28:51.595369: step 3976, loss 0.519668, acc 0.865385, prec 0.0660357, recall 0.88883
2017-12-10T14:28:51.794202: step 3977, loss 0.316421, acc 0.90625, prec 0.0660463, recall 0.888859
2017-12-10T14:28:51.987272: step 3978, loss 0.263023, acc 0.890625, prec 0.0660372, recall 0.888859
2017-12-10T14:28:52.179574: step 3979, loss 0.0918064, acc 0.953125, prec 0.0660702, recall 0.888918
2017-12-10T14:28:52.373047: step 3980, loss 0.223075, acc 0.9375, prec 0.0661203, recall 0.889007
2017-12-10T14:28:52.564693: step 3981, loss 0.108319, acc 0.921875, prec 0.0661322, recall 0.889036
2017-12-10T14:28:52.755912: step 3982, loss 0.273481, acc 0.890625, prec 0.0661415, recall 0.889066
2017-12-10T14:28:52.946312: step 3983, loss 1.0373, acc 0.890625, prec 0.0661508, recall 0.889095
2017-12-10T14:28:53.137787: step 3984, loss 0.15888, acc 0.96875, prec 0.0661666, recall 0.889125
2017-12-10T14:28:53.332392: step 3985, loss 0.053483, acc 0.96875, prec 0.0661824, recall 0.889154
2017-12-10T14:28:53.522696: step 3986, loss 0.113377, acc 0.96875, prec 0.0661798, recall 0.889154
2017-12-10T14:28:53.710521: step 3987, loss 0.223502, acc 0.953125, prec 0.0661759, recall 0.889154
2017-12-10T14:28:53.898074: step 3988, loss 0.084426, acc 0.96875, prec 0.0661733, recall 0.889154
2017-12-10T14:28:54.085212: step 3989, loss 0.331836, acc 0.921875, prec 0.0661668, recall 0.889154
2017-12-10T14:28:54.276105: step 3990, loss 0.540343, acc 0.9375, prec 0.06618, recall 0.889183
2017-12-10T14:28:54.467803: step 3991, loss 0.521407, acc 0.953125, prec 0.0662497, recall 0.889301
2017-12-10T14:28:54.660473: step 3992, loss 0.161094, acc 0.953125, prec 0.0662458, recall 0.889301
2017-12-10T14:28:54.848974: step 3993, loss 0.443717, acc 0.90625, prec 0.0662564, recall 0.88933
2017-12-10T14:28:55.039653: step 3994, loss 0.0929207, acc 0.953125, prec 0.0662709, recall 0.889359
2017-12-10T14:28:55.229369: step 3995, loss 0.283177, acc 0.9375, prec 0.0663209, recall 0.889447
2017-12-10T14:28:55.419958: step 3996, loss 0.390222, acc 0.953125, prec 0.0663354, recall 0.889476
2017-12-10T14:28:55.609274: step 3997, loss 0.0605844, acc 0.953125, prec 0.0663315, recall 0.889476
2017-12-10T14:28:55.796614: step 3998, loss 0.100233, acc 0.9375, prec 0.0663262, recall 0.889476
2017-12-10T14:28:55.983251: step 3999, loss 0.216285, acc 0.9375, prec 0.066321, recall 0.889476
2017-12-10T14:28:56.176173: step 4000, loss 0.299162, acc 0.921875, prec 0.0663697, recall 0.889564

Evaluation:
2017-12-10T14:29:00.466916: step 4000, loss 3.0928, acc 0.938856, prec 0.0670066, recall 0.875413

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_2/1512933336/checkpoints/model-4000

2017-12-10T14:29:01.689919: step 4001, loss 0.213529, acc 0.9375, prec 0.0670558, recall 0.875508
2017-12-10T14:29:01.883885: step 4002, loss 0.280118, acc 0.90625, prec 0.0670843, recall 0.875571
2017-12-10T14:29:02.071328: step 4003, loss 0.334479, acc 0.921875, prec 0.0670778, recall 0.875571
2017-12-10T14:29:02.260820: step 4004, loss 0.287353, acc 0.921875, prec 0.0670894, recall 0.875603
2017-12-10T14:29:02.452348: step 4005, loss 0.278384, acc 0.9375, prec 0.0671023, recall 0.875634
2017-12-10T14:29:02.641569: step 4006, loss 0.181693, acc 0.984375, prec 0.067101, recall 0.875634
2017-12-10T14:29:02.833588: step 4007, loss 0.0690625, acc 0.953125, prec 0.0670971, recall 0.875634
2017-12-10T14:29:03.024889: step 4008, loss 0.16713, acc 0.953125, prec 0.0670932, recall 0.875634
2017-12-10T14:29:03.216298: step 4009, loss 0.331958, acc 0.9375, prec 0.0671061, recall 0.875666
2017-12-10T14:29:03.410249: step 4010, loss 0.092645, acc 0.90625, prec 0.0670983, recall 0.875666
2017-12-10T14:29:03.598406: step 4011, loss 0.154626, acc 0.953125, prec 0.0671125, recall 0.875698
2017-12-10T14:29:03.792547: step 4012, loss 0.0986637, acc 0.953125, prec 0.0671086, recall 0.875698
2017-12-10T14:29:03.981276: step 4013, loss 0.246757, acc 0.953125, prec 0.0671591, recall 0.875792
2017-12-10T14:29:04.175698: step 4014, loss 0.0498284, acc 0.984375, prec 0.0671759, recall 0.875824
2017-12-10T14:29:04.365527: step 4015, loss 0.0971317, acc 0.96875, prec 0.0671733, recall 0.875824
2017-12-10T14:29:04.556048: step 4016, loss 6.79288, acc 0.96875, prec 0.067172, recall 0.875602
2017-12-10T14:29:04.746716: step 4017, loss 0.766631, acc 0.875, prec 0.0671797, recall 0.875633
2017-12-10T14:29:04.934764: step 4018, loss 0.0116665, acc 1, prec 0.0671978, recall 0.875665
2017-12-10T14:29:05.124184: step 4019, loss 0.205751, acc 0.9375, prec 0.0672107, recall 0.875696
2017-12-10T14:29:05.318952: step 4020, loss 0.155197, acc 0.953125, prec 0.0672249, recall 0.875728
2017-12-10T14:29:05.508458: step 4021, loss 0.186611, acc 0.921875, prec 0.0672184, recall 0.875728
2017-12-10T14:29:05.696358: step 4022, loss 0.124375, acc 0.953125, prec 0.0672507, recall 0.875791
2017-12-10T14:29:05.886925: step 4023, loss 0.210273, acc 0.953125, prec 0.0672649, recall 0.875822
2017-12-10T14:29:06.073096: step 4024, loss 0.287282, acc 0.921875, prec 0.0672584, recall 0.875822
2017-12-10T14:29:06.265140: step 4025, loss 0.182177, acc 0.890625, prec 0.0672674, recall 0.875853
2017-12-10T14:29:06.457055: step 4026, loss 0.28539, acc 0.90625, prec 0.0672595, recall 0.875853
2017-12-10T14:29:06.642777: step 4027, loss 0.749051, acc 0.953125, prec 0.0673099, recall 0.875947
2017-12-10T14:29:06.839361: step 4028, loss 0.22589, acc 0.921875, prec 0.0673034, recall 0.875947
2017-12-10T14:29:07.036264: step 4029, loss 0.361871, acc 0.921875, prec 0.0672969, recall 0.875947
2017-12-10T14:29:07.225127: step 4030, loss 0.331019, acc 0.859375, prec 0.0673213, recall 0.87601
2017-12-10T14:29:07.415024: step 4031, loss 0.782492, acc 0.859375, prec 0.0673277, recall 0.876041
2017-12-10T14:29:07.607901: step 4032, loss 0.858375, acc 0.796875, prec 0.0673469, recall 0.876104
2017-12-10T14:29:07.799930: step 4033, loss 0.26156, acc 0.90625, prec 0.067339, recall 0.876104
2017-12-10T14:29:07.989102: step 4034, loss 5.00587, acc 0.875, prec 0.0673299, recall 0.875883
2017-12-10T14:29:08.178855: step 4035, loss 0.379733, acc 0.890625, prec 0.0673207, recall 0.875883
2017-12-10T14:29:08.366549: step 4036, loss 0.338423, acc 0.90625, prec 0.0673491, recall 0.875946
2017-12-10T14:29:08.559246: step 4037, loss 0.173845, acc 0.9375, prec 0.0673439, recall 0.875946
2017-12-10T14:29:08.746775: step 4038, loss 0.292006, acc 0.90625, prec 0.067336, recall 0.875946
2017-12-10T14:29:08.937608: step 4039, loss 0.352733, acc 0.921875, prec 0.0673295, recall 0.875946
2017-12-10T14:29:09.124173: step 4040, loss 0.589163, acc 0.890625, prec 0.0673565, recall 0.876008
2017-12-10T14:29:09.313267: step 4041, loss 0.609139, acc 0.828125, prec 0.0673422, recall 0.876008
2017-12-10T14:29:09.502320: step 4042, loss 0.322762, acc 0.9375, prec 0.0673369, recall 0.876008
2017-12-10T14:29:09.690157: step 4043, loss 0.281695, acc 0.9375, prec 0.0673317, recall 0.876008
2017-12-10T14:29:09.877824: step 4044, loss 0.199785, acc 0.9375, prec 0.0673265, recall 0.876008
2017-12-10T14:29:10.066907: step 4045, loss 0.331016, acc 0.921875, prec 0.06732, recall 0.876008
2017-12-10T14:29:10.255237: step 4046, loss 0.266708, acc 0.921875, prec 0.0673676, recall 0.876102
2017-12-10T14:29:10.441526: step 4047, loss 0.180926, acc 0.9375, prec 0.0673805, recall 0.876133
2017-12-10T14:29:10.635622: step 4048, loss 0.281742, acc 0.90625, prec 0.0673907, recall 0.876164
2017-12-10T14:29:10.826155: step 4049, loss 0.538794, acc 0.84375, prec 0.0674318, recall 0.876258
2017-12-10T14:29:11.016184: step 4050, loss 0.50334, acc 0.859375, prec 0.0674201, recall 0.876258
2017-12-10T14:29:11.204517: step 4051, loss 0.612901, acc 0.890625, prec 0.0674651, recall 0.876351
2017-12-10T14:29:11.392398: step 4052, loss 0.119157, acc 0.96875, prec 0.0674625, recall 0.876351
2017-12-10T14:29:11.582403: step 4053, loss 0.648209, acc 0.890625, prec 0.0674714, recall 0.876382
2017-12-10T14:29:11.775422: step 4054, loss 0.251132, acc 0.921875, prec 0.0674648, recall 0.876382
2017-12-10T14:29:11.962946: step 4055, loss 0.16043, acc 0.953125, prec 0.067479, recall 0.876413
2017-12-10T14:29:12.147624: step 4056, loss 0.0449451, acc 0.984375, prec 0.0674957, recall 0.876444
2017-12-10T14:29:12.339667: step 4057, loss 0.28166, acc 0.953125, prec 0.0675098, recall 0.876475
2017-12-10T14:29:12.527653: step 4058, loss 1.92599, acc 0.921875, prec 0.0675226, recall 0.876286
2017-12-10T14:29:12.724877: step 4059, loss 0.0755089, acc 0.984375, prec 0.0675393, recall 0.876317
2017-12-10T14:29:12.913795: step 4060, loss 0.159503, acc 0.921875, prec 0.0675508, recall 0.876348
2017-12-10T14:29:13.102565: step 4061, loss 0.196853, acc 0.9375, prec 0.0675456, recall 0.876348
2017-12-10T14:29:13.290341: step 4062, loss 0.496365, acc 0.890625, prec 0.0675365, recall 0.876348
2017-12-10T14:29:13.482088: step 4063, loss 0.0332403, acc 0.984375, prec 0.0675352, recall 0.876348
2017-12-10T14:29:13.674420: step 4064, loss 0.180332, acc 0.96875, prec 0.0675326, recall 0.876348
2017-12-10T14:29:13.865434: step 4065, loss 0.252809, acc 0.90625, prec 0.0675788, recall 0.876441
2017-12-10T14:29:14.061081: step 4066, loss 0.188981, acc 0.953125, prec 0.0675749, recall 0.876441
2017-12-10T14:29:14.249925: step 4067, loss 1.1299, acc 0.921875, prec 0.0676224, recall 0.876534
2017-12-10T14:29:14.438514: step 4068, loss 0.167052, acc 0.96875, prec 0.0676738, recall 0.876627
2017-12-10T14:29:14.629195: step 4069, loss 0.109159, acc 0.984375, prec 0.0676905, recall 0.876657
2017-12-10T14:29:14.821773: step 4070, loss 0.349934, acc 0.921875, prec 0.067684, recall 0.876657
2017-12-10T14:29:15.014354: step 4071, loss 0.149108, acc 0.953125, prec 0.0676801, recall 0.876657
2017-12-10T14:29:15.204431: step 4072, loss 0.322998, acc 0.921875, prec 0.0676735, recall 0.876657
2017-12-10T14:29:15.389505: step 4073, loss 0.179601, acc 0.953125, prec 0.0676696, recall 0.876657
2017-12-10T14:29:15.579054: step 4074, loss 0.364059, acc 0.9375, prec 0.0677004, recall 0.876719
2017-12-10T14:29:15.768081: step 4075, loss 0.359487, acc 0.90625, prec 0.0677285, recall 0.876781
2017-12-10T14:29:15.960296: step 4076, loss 0.0687386, acc 0.96875, prec 0.0677439, recall 0.876812
2017-12-10T14:29:16.147549: step 4077, loss 0.277714, acc 0.921875, prec 0.0677554, recall 0.876842
2017-12-10T14:29:16.337871: step 4078, loss 0.32118, acc 0.96875, prec 0.0677888, recall 0.876904
2017-12-10T14:29:16.526483: step 4079, loss 0.0292261, acc 1, prec 0.0677888, recall 0.876904
2017-12-10T14:29:16.716266: step 4080, loss 0.020123, acc 0.984375, prec 0.0677875, recall 0.876904
2017-12-10T14:29:16.906692: step 4081, loss 0.1412, acc 0.953125, prec 0.0678015, recall 0.876935
2017-12-10T14:29:17.100534: step 4082, loss 0.233983, acc 0.921875, prec 0.067795, recall 0.876935
2017-12-10T14:29:17.292698: step 4083, loss 0.153067, acc 0.921875, prec 0.0678064, recall 0.876965
2017-12-10T14:29:17.483968: step 4084, loss 0.0744453, acc 0.984375, prec 0.0678051, recall 0.876965
2017-12-10T14:29:17.673581: step 4085, loss 0.137122, acc 0.96875, prec 0.0678025, recall 0.876965
2017-12-10T14:29:17.860740: step 4086, loss 0.206448, acc 0.90625, prec 0.0678126, recall 0.876996
2017-12-10T14:29:18.051804: step 4087, loss 0.188153, acc 0.984375, prec 0.0678293, recall 0.877027
2017-12-10T14:29:18.243963: step 4088, loss 0.231151, acc 0.9375, prec 0.0678241, recall 0.877027
2017-12-10T14:29:18.432958: step 4089, loss 0.046996, acc 0.96875, prec 0.0678215, recall 0.877027
2017-12-10T14:29:18.621238: step 4090, loss 0.402376, acc 0.90625, prec 0.0678316, recall 0.877057
2017-12-10T14:29:18.812625: step 4091, loss 0.148151, acc 0.953125, prec 0.0678816, recall 0.877149
2017-12-10T14:29:19.001809: step 4092, loss 0.391662, acc 0.921875, prec 0.0678751, recall 0.877149
2017-12-10T14:29:19.190959: step 4093, loss 0.103481, acc 0.96875, prec 0.0678904, recall 0.87718
2017-12-10T14:29:19.384321: step 4094, loss 0.0776091, acc 0.96875, prec 0.0679058, recall 0.87721
2017-12-10T14:29:19.573735: step 4095, loss 0.16979, acc 0.953125, prec 0.0679378, recall 0.877272
2017-12-10T14:29:19.763464: step 4096, loss 0.0930112, acc 0.96875, prec 0.0679531, recall 0.877302
2017-12-10T14:29:19.956051: step 4097, loss 0.152858, acc 0.984375, prec 0.0679877, recall 0.877363
2017-12-10T14:29:20.149067: step 4098, loss 0.0112645, acc 1, prec 0.0680057, recall 0.877394
2017-12-10T14:29:20.338003: step 4099, loss 0.0416872, acc 0.96875, prec 0.0680031, recall 0.877394
2017-12-10T14:29:20.525592: step 4100, loss 0.00478492, acc 1, prec 0.0680211, recall 0.877424
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_2/1512933336/checkpoints/model-4100

2017-12-10T14:29:21.767092: step 4101, loss 0.158792, acc 0.984375, prec 0.0680377, recall 0.877455
2017-12-10T14:29:21.959433: step 4102, loss 0.00218288, acc 1, prec 0.0680377, recall 0.877455
2017-12-10T14:29:22.148374: step 4103, loss 0.0672638, acc 0.96875, prec 0.0680351, recall 0.877455
2017-12-10T14:29:22.339222: step 4104, loss 0.0300388, acc 0.984375, prec 0.0680338, recall 0.877455
2017-12-10T14:29:22.528726: step 4105, loss 0.0401055, acc 0.984375, prec 0.0680325, recall 0.877455
2017-12-10T14:29:22.722507: step 4106, loss 0.0759675, acc 0.953125, prec 0.0680285, recall 0.877455
2017-12-10T14:29:22.915500: step 4107, loss 0.0134957, acc 1, prec 0.0680644, recall 0.877516
2017-12-10T14:29:23.110632: step 4108, loss 0.0319582, acc 0.984375, prec 0.0680631, recall 0.877516
2017-12-10T14:29:23.303129: step 4109, loss 0.046851, acc 0.984375, prec 0.0681157, recall 0.877607
2017-12-10T14:29:23.492025: step 4110, loss 0.0948478, acc 0.953125, prec 0.0681118, recall 0.877607
2017-12-10T14:29:23.687629: step 4111, loss 0.00219631, acc 1, prec 0.0681118, recall 0.877607
2017-12-10T14:29:23.878371: step 4112, loss 0.300142, acc 0.984375, prec 0.0681643, recall 0.877698
2017-12-10T14:29:24.068291: step 4113, loss 0.149033, acc 0.953125, prec 0.0681783, recall 0.877728
2017-12-10T14:29:24.260058: step 4114, loss 0.186891, acc 0.953125, prec 0.0681744, recall 0.877728
2017-12-10T14:29:24.453650: step 4115, loss 0.0250748, acc 0.984375, prec 0.068191, recall 0.877759
2017-12-10T14:29:24.643229: step 4116, loss 0.0283009, acc 0.984375, prec 0.0681897, recall 0.877759
2017-12-10T14:29:24.834953: step 4117, loss 0.0799501, acc 0.984375, prec 0.0682063, recall 0.877789
2017-12-10T14:29:25.024196: step 4118, loss 0.216472, acc 0.984375, prec 0.068223, recall 0.877819
2017-12-10T14:29:25.214948: step 4119, loss 0.744649, acc 1, prec 0.0682947, recall 0.87794
2017-12-10T14:29:25.409447: step 4120, loss 0.0440827, acc 0.96875, prec 0.0682921, recall 0.87794
2017-12-10T14:29:25.594115: step 4121, loss 4.01975, acc 0.9375, prec 0.0682882, recall 0.877723
2017-12-10T14:29:25.789578: step 4122, loss 0.935498, acc 0.9375, prec 0.0683188, recall 0.877783
2017-12-10T14:29:25.986060: step 4123, loss 0.0617294, acc 0.96875, prec 0.0683162, recall 0.877783
2017-12-10T14:29:26.172770: step 4124, loss 0.314728, acc 0.9375, prec 0.0683468, recall 0.877844
2017-12-10T14:29:26.362387: step 4125, loss 0.181398, acc 0.96875, prec 0.0683621, recall 0.877874
2017-12-10T14:29:26.552541: step 4126, loss 0.157865, acc 0.9375, prec 0.0683568, recall 0.877874
2017-12-10T14:29:26.742833: step 4127, loss 0.32517, acc 0.921875, prec 0.0683502, recall 0.877874
2017-12-10T14:29:26.934606: step 4128, loss 0.165317, acc 0.9375, prec 0.0683629, recall 0.877904
2017-12-10T14:29:27.124439: step 4129, loss 0.342808, acc 0.921875, prec 0.0683563, recall 0.877904
2017-12-10T14:29:27.315216: step 4130, loss 0.310404, acc 0.90625, prec 0.0683664, recall 0.877934
2017-12-10T14:29:27.508531: step 4131, loss 0.464844, acc 0.859375, prec 0.0683725, recall 0.877964
2017-12-10T14:29:27.699841: step 4132, loss 0.357662, acc 0.921875, prec 0.0683659, recall 0.877964
2017-12-10T14:29:27.888721: step 4133, loss 0.182006, acc 0.953125, prec 0.0683798, recall 0.877995
2017-12-10T14:29:28.083038: step 4134, loss 0.867499, acc 0.828125, prec 0.0684012, recall 0.878055
2017-12-10T14:29:28.274441: step 4135, loss 0.611411, acc 0.828125, prec 0.0683867, recall 0.878055
2017-12-10T14:29:28.460942: step 4136, loss 0.423935, acc 0.890625, prec 0.0683955, recall 0.878085
2017-12-10T14:29:28.650114: step 4137, loss 0.16034, acc 0.9375, prec 0.0684081, recall 0.878115
2017-12-10T14:29:28.837594: step 4138, loss 0.373519, acc 0.875, prec 0.0683976, recall 0.878115
2017-12-10T14:29:29.029403: step 4139, loss 0.329925, acc 0.890625, prec 0.0684063, recall 0.878145
2017-12-10T14:29:29.217477: step 4140, loss 0.0639291, acc 0.984375, prec 0.068405, recall 0.878145
2017-12-10T14:29:29.411592: step 4141, loss 0.0505614, acc 0.984375, prec 0.0684037, recall 0.878145
2017-12-10T14:29:29.603026: step 4142, loss 0.387417, acc 0.890625, prec 0.0683945, recall 0.878145
2017-12-10T14:29:29.790448: step 4143, loss 0.101256, acc 0.96875, prec 0.0684097, recall 0.878175
2017-12-10T14:29:29.975567: step 4144, loss 0.32675, acc 0.921875, prec 0.0684568, recall 0.878265
2017-12-10T14:29:30.166397: step 4145, loss 0.0938374, acc 0.96875, prec 0.0684721, recall 0.878295
2017-12-10T14:29:30.357742: step 4146, loss 0.157147, acc 0.953125, prec 0.0684682, recall 0.878295
2017-12-10T14:29:30.547290: step 4147, loss 0.248665, acc 0.953125, prec 0.0684821, recall 0.878325
2017-12-10T14:29:30.737133: step 4148, loss 0.306246, acc 0.90625, prec 0.0685279, recall 0.878415
2017-12-10T14:29:30.928195: step 4149, loss 0.170367, acc 0.9375, prec 0.0685584, recall 0.878475
2017-12-10T14:29:31.120436: step 4150, loss 0.107899, acc 0.9375, prec 0.0685889, recall 0.878535
2017-12-10T14:29:31.312131: step 4151, loss 0.710815, acc 0.890625, prec 0.0686154, recall 0.878594
2017-12-10T14:29:31.505255: step 4152, loss 0.0822008, acc 0.984375, prec 0.0686498, recall 0.878654
2017-12-10T14:29:31.693451: step 4153, loss 0.102081, acc 0.953125, prec 0.0686459, recall 0.878654
2017-12-10T14:29:31.883850: step 4154, loss 0.18112, acc 0.96875, prec 0.0686433, recall 0.878654
2017-12-10T14:29:32.072862: step 4155, loss 0.0421405, acc 0.984375, prec 0.0686419, recall 0.878654
2017-12-10T14:29:32.265782: step 4156, loss 0.0809356, acc 0.96875, prec 0.0686393, recall 0.878654
2017-12-10T14:29:32.456355: step 4157, loss 6.57172, acc 0.96875, prec 0.068638, recall 0.878438
2017-12-10T14:29:32.646519: step 4158, loss 0.0683016, acc 0.984375, prec 0.0686367, recall 0.878438
2017-12-10T14:29:32.838228: step 4159, loss 0.243345, acc 0.90625, prec 0.0686466, recall 0.878468
2017-12-10T14:29:33.024850: step 4160, loss 0.0659466, acc 0.96875, prec 0.0686619, recall 0.878498
2017-12-10T14:29:33.215426: step 4161, loss 0.355101, acc 0.90625, prec 0.0686897, recall 0.878557
2017-12-10T14:29:33.406360: step 4162, loss 0.445165, acc 0.9375, prec 0.0687202, recall 0.878617
2017-12-10T14:29:33.592175: step 4163, loss 0.192716, acc 0.96875, prec 0.0687175, recall 0.878617
2017-12-10T14:29:33.780057: step 4164, loss 0.118277, acc 0.953125, prec 0.0687493, recall 0.878676
2017-12-10T14:29:33.966865: step 4165, loss 0.0872161, acc 0.984375, prec 0.0687837, recall 0.878736
2017-12-10T14:29:34.156764: step 4166, loss 0.229252, acc 0.96875, prec 0.0687989, recall 0.878766
2017-12-10T14:29:34.346035: step 4167, loss 0.0878828, acc 0.96875, prec 0.0688141, recall 0.878795
2017-12-10T14:29:34.536413: step 4168, loss 0.0998783, acc 0.96875, prec 0.0688293, recall 0.878825
2017-12-10T14:29:34.725456: step 4169, loss 0.382253, acc 0.921875, prec 0.0688584, recall 0.878884
2017-12-10T14:29:34.912544: step 4170, loss 0.173331, acc 0.9375, prec 0.068871, recall 0.878914
2017-12-10T14:29:35.100702: step 4171, loss 0.139598, acc 0.953125, prec 0.068867, recall 0.878914
2017-12-10T14:29:35.289671: step 4172, loss 0.173065, acc 0.953125, prec 0.0688988, recall 0.878973
2017-12-10T14:29:35.479879: step 4173, loss 0.164509, acc 0.953125, prec 0.0689127, recall 0.879003
2017-12-10T14:29:35.668469: step 4174, loss 0.0353133, acc 0.984375, prec 0.0689292, recall 0.879032
2017-12-10T14:29:35.863420: step 4175, loss 0.11017, acc 0.984375, prec 0.0689457, recall 0.879062
2017-12-10T14:29:36.050891: step 4176, loss 0.0216909, acc 1, prec 0.0689635, recall 0.879091
2017-12-10T14:29:36.242961: step 4177, loss 0.0947461, acc 0.953125, prec 0.0689596, recall 0.879091
2017-12-10T14:29:36.430489: step 4178, loss 0.210322, acc 0.9375, prec 0.0689543, recall 0.879091
2017-12-10T14:29:36.621606: step 4179, loss 0.0351161, acc 0.984375, prec 0.0689708, recall 0.879121
2017-12-10T14:29:36.809275: step 4180, loss 0.020647, acc 1, prec 0.0689886, recall 0.87915
2017-12-10T14:29:37.000797: step 4181, loss 0.0415574, acc 0.984375, prec 0.0689873, recall 0.87915
2017-12-10T14:29:37.190654: step 4182, loss 0.0432362, acc 0.96875, prec 0.069056, recall 0.879268
2017-12-10T14:29:37.382984: step 4183, loss 0.203956, acc 0.96875, prec 0.0690534, recall 0.879268
2017-12-10T14:29:37.575619: step 4184, loss 0.095021, acc 0.96875, prec 0.0690686, recall 0.879298
2017-12-10T14:29:37.764611: step 4185, loss 0.104026, acc 0.953125, prec 0.0690824, recall 0.879327
2017-12-10T14:29:37.956362: step 4186, loss 0.166948, acc 0.9375, prec 0.0690771, recall 0.879327
2017-12-10T14:29:38.149677: step 4187, loss 0.0181822, acc 0.984375, prec 0.0690758, recall 0.879327
2017-12-10T14:29:38.340951: step 4188, loss 0.143074, acc 0.96875, prec 0.0690732, recall 0.879327
2017-12-10T14:29:38.529369: step 4189, loss 0.0808329, acc 0.984375, prec 0.0690718, recall 0.879327
2017-12-10T14:29:38.719311: step 4190, loss 0.18797, acc 0.9375, prec 0.0691022, recall 0.879386
2017-12-10T14:29:38.909596: step 4191, loss 0.00353662, acc 1, prec 0.06912, recall 0.879415
2017-12-10T14:29:39.100624: step 4192, loss 0.125052, acc 0.9375, prec 0.0691147, recall 0.879415
2017-12-10T14:29:39.290136: step 4193, loss 0.0404777, acc 1, prec 0.0691325, recall 0.879445
2017-12-10T14:29:39.480048: step 4194, loss 0.190949, acc 0.953125, prec 0.0691286, recall 0.879445
2017-12-10T14:29:39.667003: step 4195, loss 0.0361141, acc 0.984375, prec 0.0691272, recall 0.879445
2017-12-10T14:29:39.852690: step 4196, loss 0.214986, acc 0.96875, prec 0.0691246, recall 0.879445
2017-12-10T14:29:40.042732: step 4197, loss 0.135541, acc 0.953125, prec 0.0691206, recall 0.879445
2017-12-10T14:29:40.230461: step 4198, loss 0.033857, acc 0.984375, prec 0.0691193, recall 0.879445
2017-12-10T14:29:40.421079: step 4199, loss 0.397864, acc 0.984375, prec 0.0691536, recall 0.879503
2017-12-10T14:29:40.611787: step 4200, loss 0.0929246, acc 0.96875, prec 0.069151, recall 0.879503
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_2/1512933336/checkpoints/model-4200

2017-12-10T14:29:41.767413: step 4201, loss 0.152282, acc 0.984375, prec 0.0691853, recall 0.879562
2017-12-10T14:29:41.956971: step 4202, loss 0.186522, acc 0.9375, prec 0.06918, recall 0.879562
2017-12-10T14:29:42.147172: step 4203, loss 0.32442, acc 0.96875, prec 0.0691951, recall 0.879591
2017-12-10T14:29:42.337269: step 4204, loss 0.160028, acc 0.953125, prec 0.0691912, recall 0.879591
2017-12-10T14:29:42.530944: step 4205, loss 0.145602, acc 0.953125, prec 0.0691872, recall 0.879591
2017-12-10T14:29:42.721312: step 4206, loss 0.00739065, acc 1, prec 0.0691872, recall 0.879591
2017-12-10T14:29:42.914766: step 4207, loss 0.0229264, acc 1, prec 0.069205, recall 0.879621
2017-12-10T14:29:43.108258: step 4208, loss 0.198123, acc 0.96875, prec 0.0692202, recall 0.87965
2017-12-10T14:29:43.295709: step 4209, loss 0.00610436, acc 1, prec 0.069238, recall 0.879679
2017-12-10T14:29:43.483100: step 4210, loss 0.0167435, acc 1, prec 0.069238, recall 0.879679
2017-12-10T14:29:43.670925: step 4211, loss 0.0664821, acc 0.953125, prec 0.0692518, recall 0.879708
2017-12-10T14:29:43.864049: step 4212, loss 2.38581, acc 0.984375, prec 0.0692518, recall 0.879495
2017-12-10T14:29:44.063472: step 4213, loss 0.0355049, acc 0.984375, prec 0.0692683, recall 0.879524
2017-12-10T14:29:44.252413: step 4214, loss 0.182474, acc 0.9375, prec 0.069263, recall 0.879524
2017-12-10T14:29:44.439665: step 4215, loss 0.0552882, acc 0.984375, prec 0.0692795, recall 0.879553
2017-12-10T14:29:44.629395: step 4216, loss 0.155135, acc 0.953125, prec 0.0692933, recall 0.879582
2017-12-10T14:29:44.825985: step 4217, loss 2.63581, acc 0.890625, prec 0.0692853, recall 0.879369
2017-12-10T14:29:45.018189: step 4218, loss 0.0909132, acc 0.953125, prec 0.0692814, recall 0.879369
2017-12-10T14:29:45.209363: step 4219, loss 0.0286736, acc 0.96875, prec 0.0692965, recall 0.879398
2017-12-10T14:29:45.397555: step 4220, loss 0.317723, acc 0.890625, prec 0.0692872, recall 0.879398
2017-12-10T14:29:45.591051: step 4221, loss 0.212261, acc 0.953125, prec 0.0693011, recall 0.879427
2017-12-10T14:29:45.778619: step 4222, loss 0.0460913, acc 0.984375, prec 0.0693175, recall 0.879457
2017-12-10T14:29:45.972811: step 4223, loss 0.740682, acc 0.84375, prec 0.0693399, recall 0.879515
2017-12-10T14:29:46.157909: step 4224, loss 0.18507, acc 0.953125, prec 0.0693715, recall 0.879574
2017-12-10T14:29:46.346101: step 4225, loss 0.15006, acc 0.9375, prec 0.0693839, recall 0.879603
2017-12-10T14:29:46.534739: step 4226, loss 0.431531, acc 0.890625, prec 0.0693924, recall 0.879632
2017-12-10T14:29:46.723218: step 4227, loss 0.254653, acc 0.9375, prec 0.0693871, recall 0.879632
2017-12-10T14:29:46.909674: step 4228, loss 0.361047, acc 0.921875, prec 0.0693805, recall 0.879632
2017-12-10T14:29:47.094407: step 4229, loss 0.578167, acc 0.875, prec 0.0693877, recall 0.879661
2017-12-10T14:29:47.282918: step 4230, loss 0.666141, acc 0.90625, prec 0.0693797, recall 0.879661
2017-12-10T14:29:47.470657: step 4231, loss 0.617128, acc 0.890625, prec 0.0693705, recall 0.879661
2017-12-10T14:29:47.660133: step 4232, loss 0.446239, acc 0.90625, prec 0.069398, recall 0.879719
2017-12-10T14:29:47.849295: step 4233, loss 0.27936, acc 0.96875, prec 0.0693954, recall 0.879719
2017-12-10T14:29:48.038252: step 4234, loss 0.53961, acc 0.921875, prec 0.0694243, recall 0.879777
2017-12-10T14:29:48.228760: step 4235, loss 0.515113, acc 0.875, prec 0.0694492, recall 0.879836
2017-12-10T14:29:48.423570: step 4236, loss 0.147763, acc 0.953125, prec 0.069463, recall 0.879865
2017-12-10T14:29:48.613606: step 4237, loss 0.561717, acc 0.875, prec 0.0694524, recall 0.879865
2017-12-10T14:29:48.804410: step 4238, loss 0.937528, acc 0.9375, prec 0.0694826, recall 0.879923
2017-12-10T14:29:48.997467: step 4239, loss 0.304761, acc 0.921875, prec 0.069476, recall 0.879923
2017-12-10T14:29:49.186705: step 4240, loss 0.233919, acc 0.9375, prec 0.0694707, recall 0.879923
2017-12-10T14:29:49.379010: step 4241, loss 0.214557, acc 0.953125, prec 0.0695022, recall 0.879981
2017-12-10T14:29:49.566814: step 4242, loss 0.307232, acc 0.9375, prec 0.0694969, recall 0.879981
2017-12-10T14:29:49.754935: step 4243, loss 0.535853, acc 0.875, prec 0.0695218, recall 0.880039
2017-12-10T14:29:49.947374: step 4244, loss 0.170227, acc 0.9375, prec 0.0695342, recall 0.880068
2017-12-10T14:29:50.137674: step 4245, loss 0.403177, acc 0.921875, prec 0.0695276, recall 0.880068
2017-12-10T14:29:50.325117: step 4246, loss 0.150239, acc 0.9375, prec 0.0695223, recall 0.880068
2017-12-10T14:29:50.516685: step 4247, loss 0.0679213, acc 0.96875, prec 0.0695374, recall 0.880096
2017-12-10T14:29:50.706885: step 4248, loss 0.250665, acc 0.953125, prec 0.0695334, recall 0.880096
2017-12-10T14:29:50.895741: step 4249, loss 0.175996, acc 0.953125, prec 0.0695649, recall 0.880154
2017-12-10T14:29:51.084637: step 4250, loss 0.196873, acc 0.953125, prec 0.0695786, recall 0.880183
2017-12-10T14:29:51.273649: step 4251, loss 0.0762101, acc 0.96875, prec 0.0695937, recall 0.880212
2017-12-10T14:29:51.467713: step 4252, loss 0.32077, acc 0.9375, prec 0.0696061, recall 0.880241
2017-12-10T14:29:51.660606: step 4253, loss 0.226009, acc 0.9375, prec 0.0696363, recall 0.880299
2017-12-10T14:29:51.850460: step 4254, loss 0.26022, acc 0.921875, prec 0.0696651, recall 0.880356
2017-12-10T14:29:52.040915: step 4255, loss 0.153474, acc 0.96875, prec 0.0696802, recall 0.880385
2017-12-10T14:29:52.232307: step 4256, loss 0.90921, acc 0.96875, prec 0.0697484, recall 0.8805
2017-12-10T14:29:52.425140: step 4257, loss 0.0442865, acc 0.96875, prec 0.0697457, recall 0.8805
2017-12-10T14:29:52.616720: step 4258, loss 0.192865, acc 0.953125, prec 0.0697418, recall 0.8805
2017-12-10T14:29:52.805359: step 4259, loss 0.2208, acc 0.96875, prec 0.0697745, recall 0.880558
2017-12-10T14:29:52.993196: step 4260, loss 0.0839649, acc 0.96875, prec 0.0697719, recall 0.880558
2017-12-10T14:29:53.184156: step 4261, loss 0.230296, acc 0.9375, prec 0.0697843, recall 0.880586
2017-12-10T14:29:53.377665: step 4262, loss 0.458967, acc 0.90625, prec 0.069794, recall 0.880615
2017-12-10T14:29:53.569848: step 4263, loss 0.324243, acc 0.96875, prec 0.0698445, recall 0.880701
2017-12-10T14:29:53.760574: step 4264, loss 0.0412319, acc 1, prec 0.0698622, recall 0.88073
2017-12-10T14:29:53.949774: step 4265, loss 0.047304, acc 0.984375, prec 0.0698786, recall 0.880758
2017-12-10T14:29:54.139612: step 4266, loss 1.81981, acc 0.953125, prec 0.0698936, recall 0.880576
2017-12-10T14:29:54.331537: step 4267, loss 0.094675, acc 0.953125, prec 0.0699073, recall 0.880604
2017-12-10T14:29:54.520088: step 4268, loss 0.269908, acc 0.875, prec 0.0699144, recall 0.880633
2017-12-10T14:29:54.710561: step 4269, loss 0.392443, acc 0.9375, prec 0.0699444, recall 0.88069
2017-12-10T14:29:54.902614: step 4270, loss 0.352589, acc 0.90625, prec 0.0699541, recall 0.880719
2017-12-10T14:29:55.091060: step 4271, loss 0.352391, acc 0.90625, prec 0.0699462, recall 0.880719
2017-12-10T14:29:55.281695: step 4272, loss 0.225072, acc 0.90625, prec 0.0699913, recall 0.880804
2017-12-10T14:29:55.470350: step 4273, loss 0.538802, acc 0.90625, prec 0.0699833, recall 0.880804
2017-12-10T14:29:55.656968: step 4274, loss 0.14283, acc 0.9375, prec 0.0699779, recall 0.880804
2017-12-10T14:29:55.848410: step 4275, loss 0.207016, acc 0.953125, prec 0.0699916, recall 0.880833
2017-12-10T14:29:56.043393: step 4276, loss 0.331748, acc 0.921875, prec 0.069985, recall 0.880833
2017-12-10T14:29:56.235105: step 4277, loss 0.15162, acc 0.953125, prec 0.0699987, recall 0.880861
2017-12-10T14:29:56.421817: step 4278, loss 0.282158, acc 0.90625, prec 0.0699907, recall 0.880861
2017-12-10T14:29:56.608600: step 4279, loss 0.254084, acc 0.9375, prec 0.0699854, recall 0.880861
2017-12-10T14:29:56.797115: step 4280, loss 0.270345, acc 0.96875, prec 0.0700357, recall 0.880947
2017-12-10T14:29:56.984209: step 4281, loss 0.0634051, acc 0.96875, prec 0.0700684, recall 0.881004
2017-12-10T14:29:57.173468: step 4282, loss 0.244134, acc 0.9375, prec 0.0701338, recall 0.881117
2017-12-10T14:29:57.362277: step 4283, loss 0.217546, acc 0.96875, prec 0.0701311, recall 0.881117
2017-12-10T14:29:57.555898: step 4284, loss 0.178976, acc 0.953125, prec 0.0701448, recall 0.881146
2017-12-10T14:29:57.748395: step 4285, loss 0.0531367, acc 0.953125, prec 0.0701408, recall 0.881146
2017-12-10T14:29:57.938432: step 4286, loss 0.148352, acc 0.9375, prec 0.0701354, recall 0.881146
2017-12-10T14:29:58.129084: step 4287, loss 0.0569132, acc 0.984375, prec 0.0701341, recall 0.881146
2017-12-10T14:29:58.317542: step 4288, loss 0.304372, acc 0.921875, prec 0.0701804, recall 0.881231
2017-12-10T14:29:58.506187: step 4289, loss 0.177861, acc 0.9375, prec 0.0701751, recall 0.881231
2017-12-10T14:29:58.697969: step 4290, loss 0.194963, acc 0.9375, prec 0.0701874, recall 0.881259
2017-12-10T14:29:58.890733: step 4291, loss 0.0372149, acc 0.984375, prec 0.0702038, recall 0.881287
2017-12-10T14:29:59.090146: step 4292, loss 0.181359, acc 0.984375, prec 0.0702377, recall 0.881344
2017-12-10T14:29:59.281181: step 4293, loss 0.0642013, acc 0.953125, prec 0.0702337, recall 0.881344
2017-12-10T14:29:59.466436: step 4294, loss 0.00208685, acc 1, prec 0.0702337, recall 0.881344
2017-12-10T14:29:59.653932: step 4295, loss 0.284723, acc 0.9375, prec 0.0702814, recall 0.881429
2017-12-10T14:29:59.846329: step 4296, loss 0.186852, acc 0.984375, prec 0.0702977, recall 0.881457
2017-12-10T14:30:00.040034: step 4297, loss 0.266272, acc 0.953125, prec 0.0703113, recall 0.881485
2017-12-10T14:30:00.229447: step 4298, loss 0.17311, acc 0.96875, prec 0.0703263, recall 0.881513
2017-12-10T14:30:00.420050: step 4299, loss 0.155635, acc 0.96875, prec 0.0703413, recall 0.881541
2017-12-10T14:30:00.610816: step 4300, loss 0.471054, acc 0.953125, prec 0.0703549, recall 0.88157
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_2/1512933336/checkpoints/model-4300

2017-12-10T14:30:01.835305: step 4301, loss 13.224, acc 0.96875, prec 0.0703712, recall 0.881388
2017-12-10T14:30:02.033285: step 4302, loss 0.007796, acc 1, prec 0.0704065, recall 0.881445
2017-12-10T14:30:02.223041: step 4303, loss 1.13078, acc 0.96875, prec 0.0704744, recall 0.881557
2017-12-10T14:30:02.413196: step 4304, loss 0.339909, acc 0.9375, prec 0.0704867, recall 0.881585
2017-12-10T14:30:02.601195: step 4305, loss 0.217091, acc 0.921875, prec 0.07048, recall 0.881585
2017-12-10T14:30:02.793793: step 4306, loss 0.324905, acc 0.9375, prec 0.0704923, recall 0.881613
2017-12-10T14:30:02.983405: step 4307, loss 0.3104, acc 0.890625, prec 0.0705005, recall 0.881641
2017-12-10T14:30:03.171692: step 4308, loss 0.411336, acc 0.921875, prec 0.0705291, recall 0.881697
2017-12-10T14:30:03.365744: step 4309, loss 0.410043, acc 0.90625, prec 0.0705211, recall 0.881697
2017-12-10T14:30:03.554921: step 4310, loss 0.456336, acc 0.84375, prec 0.0705077, recall 0.881697
2017-12-10T14:30:03.743967: step 4311, loss 0.696669, acc 0.84375, prec 0.0705296, recall 0.881754
2017-12-10T14:30:03.932157: step 4312, loss 0.443783, acc 0.84375, prec 0.0705338, recall 0.881782
2017-12-10T14:30:04.121478: step 4313, loss 0.517612, acc 0.828125, prec 0.0705191, recall 0.881782
2017-12-10T14:30:04.312058: step 4314, loss 0.2364, acc 0.90625, prec 0.0705111, recall 0.881782
2017-12-10T14:30:04.500540: step 4315, loss 0.893515, acc 0.828125, prec 0.0705492, recall 0.881866
2017-12-10T14:30:04.684079: step 4316, loss 0.558796, acc 0.90625, prec 0.0705764, recall 0.881921
2017-12-10T14:30:04.870913: step 4317, loss 0.184553, acc 0.953125, prec 0.0705724, recall 0.881921
2017-12-10T14:30:05.055274: step 4318, loss 0.521104, acc 0.84375, prec 0.0705591, recall 0.881921
2017-12-10T14:30:05.243546: step 4319, loss 0.700372, acc 0.8125, prec 0.070543, recall 0.881921
2017-12-10T14:30:05.431186: step 4320, loss 0.388266, acc 0.90625, prec 0.0705526, recall 0.881949
2017-12-10T14:30:05.619664: step 4321, loss 0.279653, acc 0.921875, prec 0.0705811, recall 0.882005
2017-12-10T14:30:05.805693: step 4322, loss 0.384352, acc 0.828125, prec 0.0706192, recall 0.882089
2017-12-10T14:30:05.998532: step 4323, loss 0.413506, acc 0.9375, prec 0.0706314, recall 0.882117
2017-12-10T14:30:06.188559: step 4324, loss 0.16687, acc 0.9375, prec 0.0706261, recall 0.882117
2017-12-10T14:30:06.376557: step 4325, loss 0.139387, acc 0.953125, prec 0.0706396, recall 0.882145
2017-12-10T14:30:06.566411: step 4326, loss 0.495326, acc 0.9375, prec 0.070687, recall 0.882228
2017-12-10T14:30:06.757844: step 4327, loss 0.0312301, acc 1, prec 0.0707046, recall 0.882256
2017-12-10T14:30:06.947803: step 4328, loss 0.0308688, acc 1, prec 0.0707222, recall 0.882284
2017-12-10T14:30:07.134143: step 4329, loss 0.347392, acc 0.9375, prec 0.0707168, recall 0.882284
2017-12-10T14:30:07.323156: step 4330, loss 0.189847, acc 0.921875, prec 0.0707277, recall 0.882311
2017-12-10T14:30:07.512442: step 4331, loss 0.174946, acc 0.953125, prec 0.0707237, recall 0.882311
2017-12-10T14:30:07.704879: step 4332, loss 0.217265, acc 0.96875, prec 0.0707386, recall 0.882339
2017-12-10T14:30:07.893510: step 4333, loss 0.6698, acc 0.9375, prec 0.0707684, recall 0.882395
2017-12-10T14:30:08.084969: step 4334, loss 0.057407, acc 0.96875, prec 0.0707833, recall 0.882422
2017-12-10T14:30:08.273889: step 4335, loss 3.37581, acc 0.90625, prec 0.0707941, recall 0.882242
2017-12-10T14:30:08.469431: step 4336, loss 0.0681843, acc 0.96875, prec 0.0707914, recall 0.882242
2017-12-10T14:30:08.658213: step 4337, loss 0.253348, acc 0.9375, prec 0.0707861, recall 0.882242
2017-12-10T14:30:08.846056: step 4338, loss 0.254185, acc 0.9375, prec 0.0708334, recall 0.882325
2017-12-10T14:30:09.037191: step 4339, loss 0.119932, acc 0.953125, prec 0.070847, recall 0.882353
2017-12-10T14:30:09.225492: step 4340, loss 0.366928, acc 0.9375, prec 0.0708591, recall 0.882381
2017-12-10T14:30:09.413143: step 4341, loss 1.05698, acc 0.96875, prec 0.0708916, recall 0.882436
2017-12-10T14:30:09.604935: step 4342, loss 0.130358, acc 0.96875, prec 0.0709064, recall 0.882464
2017-12-10T14:30:09.792362: step 4343, loss 0.0429351, acc 0.96875, prec 0.0709389, recall 0.882519
2017-12-10T14:30:09.981436: step 4344, loss 0.247235, acc 0.9375, prec 0.0709686, recall 0.882574
2017-12-10T14:30:10.168121: step 4345, loss 0.120554, acc 0.953125, prec 0.0709821, recall 0.882602
2017-12-10T14:30:10.358495: step 4346, loss 0.370825, acc 0.921875, prec 0.071028, recall 0.882684
2017-12-10T14:30:10.549450: step 4347, loss 0.122233, acc 0.953125, prec 0.0710766, recall 0.882767
2017-12-10T14:30:10.740534: step 4348, loss 0.42674, acc 0.875, prec 0.0710659, recall 0.882767
2017-12-10T14:30:10.931027: step 4349, loss 0.37875, acc 0.921875, prec 0.0710943, recall 0.882822
2017-12-10T14:30:11.119283: step 4350, loss 0.372474, acc 0.890625, prec 0.0710849, recall 0.882822
2017-12-10T14:30:11.307448: step 4351, loss 0.177636, acc 0.9375, prec 0.0710795, recall 0.882822
2017-12-10T14:30:11.500743: step 4352, loss 0.198726, acc 0.9375, prec 0.0710917, recall 0.882849
2017-12-10T14:30:11.692318: step 4353, loss 0.163361, acc 0.9375, prec 0.0710863, recall 0.882849
2017-12-10T14:30:11.885428: step 4354, loss 0.185155, acc 0.96875, prec 0.0710836, recall 0.882849
2017-12-10T14:30:12.073124: step 4355, loss 0.205117, acc 0.921875, prec 0.0710769, recall 0.882849
2017-12-10T14:30:12.260225: step 4356, loss 0.182795, acc 0.921875, prec 0.0710877, recall 0.882877
2017-12-10T14:30:12.449367: step 4357, loss 0.0896225, acc 0.984375, prec 0.0711039, recall 0.882904
2017-12-10T14:30:12.636064: step 4358, loss 0.150002, acc 0.9375, prec 0.0711336, recall 0.882959
2017-12-10T14:30:12.826703: step 4359, loss 0.378436, acc 0.890625, prec 0.0711417, recall 0.882986
2017-12-10T14:30:13.015105: step 4360, loss 0.688168, acc 0.921875, prec 0.0711525, recall 0.883014
2017-12-10T14:30:13.204001: step 4361, loss 0.102059, acc 0.984375, prec 0.0711512, recall 0.883014
2017-12-10T14:30:13.391233: step 4362, loss 0.418179, acc 0.90625, prec 0.0711781, recall 0.883068
2017-12-10T14:30:13.583592: step 4363, loss 0.0260168, acc 1, prec 0.0711781, recall 0.883068
2017-12-10T14:30:13.773966: step 4364, loss 0.038076, acc 0.96875, prec 0.071228, recall 0.88315
2017-12-10T14:30:13.969630: step 4365, loss 0.140875, acc 0.9375, prec 0.0712226, recall 0.88315
2017-12-10T14:30:14.165005: step 4366, loss 0.0294316, acc 0.96875, prec 0.0712374, recall 0.883178
2017-12-10T14:30:14.353862: step 4367, loss 0.234606, acc 0.96875, prec 0.0712697, recall 0.883232
2017-12-10T14:30:14.540616: step 4368, loss 0.0291548, acc 1, prec 0.0712872, recall 0.883259
2017-12-10T14:30:14.726941: step 4369, loss 0.121153, acc 0.96875, prec 0.0712846, recall 0.883259
2017-12-10T14:30:14.919928: step 4370, loss 0.100732, acc 0.96875, prec 0.0712819, recall 0.883259
2017-12-10T14:30:15.108323: step 4371, loss 0.124438, acc 0.953125, prec 0.0713128, recall 0.883314
2017-12-10T14:30:15.293720: step 4372, loss 0.311608, acc 0.9375, prec 0.071325, recall 0.883341
2017-12-10T14:30:15.484351: step 4373, loss 0.108708, acc 0.984375, prec 0.0713411, recall 0.883368
2017-12-10T14:30:15.673366: step 4374, loss 0.155592, acc 0.96875, prec 0.0713384, recall 0.883368
2017-12-10T14:30:15.861215: step 4375, loss 0.086678, acc 0.953125, prec 0.0713344, recall 0.883368
2017-12-10T14:30:16.047773: step 4376, loss 0.156991, acc 0.953125, prec 0.0713304, recall 0.883368
2017-12-10T14:30:16.241772: step 4377, loss 3.88125, acc 0.96875, prec 0.0713465, recall 0.88319
2017-12-10T14:30:16.432687: step 4378, loss 0.0267879, acc 0.984375, prec 0.0713452, recall 0.88319
2017-12-10T14:30:16.625340: step 4379, loss 0.266097, acc 0.953125, prec 0.0713411, recall 0.88319
2017-12-10T14:30:16.812536: step 4380, loss 0.0895106, acc 0.9375, prec 0.0713358, recall 0.88319
2017-12-10T14:30:17.006089: step 4381, loss 0.0453791, acc 0.96875, prec 0.0713331, recall 0.88319
2017-12-10T14:30:17.191786: step 4382, loss 0.187646, acc 0.953125, prec 0.071329, recall 0.88319
2017-12-10T14:30:17.382786: step 4383, loss 0.138755, acc 0.953125, prec 0.0713425, recall 0.883217
2017-12-10T14:30:17.570257: step 4384, loss 0.215674, acc 0.96875, prec 0.0713748, recall 0.883271
2017-12-10T14:30:17.765346: step 4385, loss 0.0835633, acc 0.96875, prec 0.0713721, recall 0.883271
2017-12-10T14:30:17.956756: step 4386, loss 0.0895929, acc 0.953125, prec 0.0713855, recall 0.883298
2017-12-10T14:30:18.145066: step 4387, loss 0.019201, acc 0.984375, prec 0.0714017, recall 0.883326
2017-12-10T14:30:18.332108: step 4388, loss 0.738588, acc 0.984375, prec 0.0714353, recall 0.88338
2017-12-10T14:30:18.520714: step 4389, loss 0.0637852, acc 0.96875, prec 0.0714326, recall 0.88338
2017-12-10T14:30:18.708618: step 4390, loss 0.23526, acc 0.953125, prec 0.071446, recall 0.883407
2017-12-10T14:30:18.897361: step 4391, loss 0.257345, acc 0.984375, prec 0.0714797, recall 0.883461
2017-12-10T14:30:19.087094: step 4392, loss 0.33237, acc 0.921875, prec 0.0714904, recall 0.883488
2017-12-10T14:30:19.276547: step 4393, loss 0.084236, acc 0.96875, prec 0.0715401, recall 0.88357
2017-12-10T14:30:19.465191: step 4394, loss 0.133511, acc 0.984375, prec 0.0715388, recall 0.88357
2017-12-10T14:30:19.657417: step 4395, loss 0.194309, acc 0.90625, prec 0.0715307, recall 0.88357
2017-12-10T14:30:19.851267: step 4396, loss 0.304259, acc 0.9375, prec 0.0715253, recall 0.88357
2017-12-10T14:30:20.040670: step 4397, loss 0.41291, acc 0.90625, prec 0.0715347, recall 0.883597
2017-12-10T14:30:20.231294: step 4398, loss 0.119809, acc 0.96875, prec 0.0715669, recall 0.883651
2017-12-10T14:30:20.421604: step 4399, loss 0.193684, acc 0.96875, prec 0.0715817, recall 0.883678
2017-12-10T14:30:20.616132: step 4400, loss 0.0317686, acc 0.984375, prec 0.0716153, recall 0.883732
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_2/1512933336/checkpoints/model-4400

2017-12-10T14:30:21.820921: step 4401, loss 0.208546, acc 0.953125, prec 0.0716462, recall 0.883786
2017-12-10T14:30:22.009440: step 4402, loss 0.466296, acc 0.96875, prec 0.0716958, recall 0.883866
2017-12-10T14:30:22.197122: step 4403, loss 0.112996, acc 0.96875, prec 0.0717106, recall 0.883893
2017-12-10T14:30:22.384540: step 4404, loss 0.249929, acc 0.9375, prec 0.0717052, recall 0.883893
2017-12-10T14:30:22.570816: step 4405, loss 0.0868522, acc 0.9375, prec 0.0716998, recall 0.883893
2017-12-10T14:30:22.755942: step 4406, loss 0.120562, acc 0.953125, prec 0.0717132, recall 0.88392
2017-12-10T14:30:22.942214: step 4407, loss 0.276883, acc 0.953125, prec 0.0717092, recall 0.88392
2017-12-10T14:30:23.129630: step 4408, loss 0.326329, acc 0.96875, prec 0.0717414, recall 0.883974
2017-12-10T14:30:23.317796: step 4409, loss 0.0777343, acc 0.984375, prec 0.07174, recall 0.883974
2017-12-10T14:30:23.511630: step 4410, loss 0.159269, acc 0.9375, prec 0.0717346, recall 0.883974
2017-12-10T14:30:23.704845: step 4411, loss 0.251317, acc 0.90625, prec 0.0717265, recall 0.883974
2017-12-10T14:30:23.895749: step 4412, loss 0.160208, acc 0.96875, prec 0.0717239, recall 0.883974
2017-12-10T14:30:24.088026: step 4413, loss 0.258675, acc 0.984375, prec 0.0717574, recall 0.884028
2017-12-10T14:30:24.285395: step 4414, loss 0.0691357, acc 0.984375, prec 0.0717735, recall 0.884055
2017-12-10T14:30:24.472107: step 4415, loss 0.132919, acc 0.96875, prec 0.0717708, recall 0.884055
2017-12-10T14:30:24.659393: step 4416, loss 0.0403351, acc 0.96875, prec 0.0717681, recall 0.884055
2017-12-10T14:30:24.854393: step 4417, loss 0.147451, acc 0.96875, prec 0.0718003, recall 0.884108
2017-12-10T14:30:25.047132: step 4418, loss 0.124846, acc 0.953125, prec 0.0717962, recall 0.884108
2017-12-10T14:30:25.237224: step 4419, loss 0.915787, acc 0.9375, prec 0.0718083, recall 0.884135
2017-12-10T14:30:25.430834: step 4420, loss 0.0253734, acc 0.984375, prec 0.0718069, recall 0.884135
2017-12-10T14:30:25.623419: step 4421, loss 0.0663612, acc 0.96875, prec 0.0718216, recall 0.884162
2017-12-10T14:30:25.815667: step 4422, loss 0.136854, acc 0.96875, prec 0.0718364, recall 0.884189
2017-12-10T14:30:26.004819: step 4423, loss 0.146095, acc 0.953125, prec 0.0718323, recall 0.884189
2017-12-10T14:30:26.196712: step 4424, loss 0.184622, acc 0.96875, prec 0.0718471, recall 0.884215
2017-12-10T14:30:26.386296: step 4425, loss 0.0944741, acc 0.984375, prec 0.0718806, recall 0.884269
2017-12-10T14:30:26.575799: step 4426, loss 0.326589, acc 0.9375, prec 0.0719275, recall 0.884349
2017-12-10T14:30:26.765035: step 4427, loss 0.342604, acc 0.9375, prec 0.0719569, recall 0.884402
2017-12-10T14:30:26.957488: step 4428, loss 0.140069, acc 0.953125, prec 0.0719703, recall 0.884429
2017-12-10T14:30:27.143665: step 4429, loss 0.272496, acc 0.9375, prec 0.0719997, recall 0.884482
2017-12-10T14:30:27.331659: step 4430, loss 0.283998, acc 0.921875, prec 0.0719929, recall 0.884482
2017-12-10T14:30:27.520113: step 4431, loss 0.0346347, acc 0.984375, prec 0.072009, recall 0.884509
2017-12-10T14:30:27.707237: step 4432, loss 0.342566, acc 0.9375, prec 0.0720036, recall 0.884509
2017-12-10T14:30:27.900931: step 4433, loss 0.0825434, acc 0.984375, prec 0.0720023, recall 0.884509
2017-12-10T14:30:28.090446: step 4434, loss 0.0759969, acc 0.953125, prec 0.0719982, recall 0.884509
2017-12-10T14:30:28.282642: step 4435, loss 0.514044, acc 0.96875, prec 0.0720129, recall 0.884536
2017-12-10T14:30:28.473725: step 4436, loss 0.106641, acc 0.984375, prec 0.0720464, recall 0.884589
2017-12-10T14:30:28.664070: step 4437, loss 0.227656, acc 0.921875, prec 0.072057, recall 0.884615
2017-12-10T14:30:28.853834: step 4438, loss 0.0572006, acc 0.96875, prec 0.0720543, recall 0.884615
2017-12-10T14:30:29.050603: step 4439, loss 0.0557503, acc 0.984375, prec 0.072053, recall 0.884615
2017-12-10T14:30:29.239105: step 4440, loss 0.296871, acc 0.9375, prec 0.0720824, recall 0.884669
2017-12-10T14:30:29.432209: step 4441, loss 0.0737886, acc 0.984375, prec 0.0720984, recall 0.884695
2017-12-10T14:30:29.624240: step 4442, loss 0.218102, acc 0.96875, prec 0.0721131, recall 0.884722
2017-12-10T14:30:29.812508: step 4443, loss 2.64445, acc 0.90625, prec 0.0721238, recall 0.884545
2017-12-10T14:30:30.005307: step 4444, loss 0.185078, acc 0.9375, prec 0.0721532, recall 0.884598
2017-12-10T14:30:30.195771: step 4445, loss 0.12462, acc 0.96875, prec 0.0721679, recall 0.884624
2017-12-10T14:30:30.385101: step 4446, loss 0.253656, acc 0.9375, prec 0.0721798, recall 0.884651
2017-12-10T14:30:30.576297: step 4447, loss 0.122351, acc 0.984375, prec 0.0722133, recall 0.884704
2017-12-10T14:30:30.764515: step 4448, loss 0.204845, acc 0.9375, prec 0.0722252, recall 0.88473
2017-12-10T14:30:30.953367: step 4449, loss 0.278548, acc 0.859375, prec 0.0722305, recall 0.884757
2017-12-10T14:30:31.141792: step 4450, loss 0.0969545, acc 0.953125, prec 0.0722264, recall 0.884757
2017-12-10T14:30:31.333795: step 4451, loss 0.301077, acc 0.9375, prec 0.072221, recall 0.884757
2017-12-10T14:30:31.531478: step 4452, loss 0.34081, acc 0.890625, prec 0.0722289, recall 0.884783
2017-12-10T14:30:31.724961: step 4453, loss 0.10712, acc 0.953125, prec 0.0722422, recall 0.88481
2017-12-10T14:30:31.911696: step 4454, loss 0.155191, acc 0.9375, prec 0.0722368, recall 0.88481
2017-12-10T14:30:32.101795: step 4455, loss 0.0962621, acc 0.953125, prec 0.0722327, recall 0.88481
2017-12-10T14:30:32.288536: step 4456, loss 0.260551, acc 0.875, prec 0.0722393, recall 0.884836
2017-12-10T14:30:32.477878: step 4457, loss 0.252633, acc 0.921875, prec 0.0722325, recall 0.884836
2017-12-10T14:30:32.666299: step 4458, loss 0.369152, acc 0.921875, prec 0.0722605, recall 0.884889
2017-12-10T14:30:32.860107: step 4459, loss 0.366327, acc 0.890625, prec 0.072251, recall 0.884889
2017-12-10T14:30:33.053046: step 4460, loss 0.21238, acc 0.953125, prec 0.072247, recall 0.884889
2017-12-10T14:30:33.242563: step 4461, loss 0.102918, acc 0.9375, prec 0.0722589, recall 0.884915
2017-12-10T14:30:33.430326: step 4462, loss 0.294276, acc 0.90625, prec 0.0722682, recall 0.884942
2017-12-10T14:30:33.615047: step 4463, loss 0.328645, acc 0.90625, prec 0.0722601, recall 0.884942
2017-12-10T14:30:33.802432: step 4464, loss 0.289692, acc 0.953125, prec 0.0722907, recall 0.884994
2017-12-10T14:30:33.995248: step 4465, loss 0.0460394, acc 0.984375, prec 0.0722894, recall 0.884994
2017-12-10T14:30:34.181655: step 4466, loss 0.260837, acc 0.9375, prec 0.0723013, recall 0.885021
2017-12-10T14:30:34.368872: step 4467, loss 0.492032, acc 0.953125, prec 0.0723493, recall 0.8851
2017-12-10T14:30:34.558350: step 4468, loss 0.262011, acc 0.953125, prec 0.07238, recall 0.885152
2017-12-10T14:30:34.745715: step 4469, loss 0.374426, acc 0.953125, prec 0.072428, recall 0.885231
2017-12-10T14:30:34.938592: step 4470, loss 0.240633, acc 0.953125, prec 0.0724239, recall 0.885231
2017-12-10T14:30:35.129542: step 4471, loss 0.0407676, acc 0.984375, prec 0.0724573, recall 0.885283
2017-12-10T14:30:35.322949: step 4472, loss 0.139226, acc 0.953125, prec 0.0724532, recall 0.885283
2017-12-10T14:30:35.496064: step 4473, loss 0.243947, acc 0.903846, prec 0.0724811, recall 0.885336
2017-12-10T14:30:35.692221: step 4474, loss 0.425664, acc 0.875, prec 0.0724703, recall 0.885336
2017-12-10T14:30:35.881390: step 4475, loss 0.27526, acc 0.9375, prec 0.0724649, recall 0.885336
2017-12-10T14:30:36.072845: step 4476, loss 0.164729, acc 0.9375, prec 0.0724594, recall 0.885336
2017-12-10T14:30:36.261815: step 4477, loss 0.342371, acc 0.9375, prec 0.0724714, recall 0.885362
2017-12-10T14:30:36.449788: step 4478, loss 0.0374544, acc 0.96875, prec 0.072486, recall 0.885388
2017-12-10T14:30:36.638496: step 4479, loss 0.12335, acc 0.953125, prec 0.0724819, recall 0.885388
2017-12-10T14:30:36.830708: step 4480, loss 0.0874847, acc 0.984375, prec 0.0724806, recall 0.885388
2017-12-10T14:30:37.021393: step 4481, loss 0.0412831, acc 1, prec 0.0724979, recall 0.885414
2017-12-10T14:30:37.215497: step 4482, loss 0.840636, acc 0.953125, prec 0.0725285, recall 0.885467
2017-12-10T14:30:37.408206: step 4483, loss 0.136392, acc 0.953125, prec 0.0725244, recall 0.885467
2017-12-10T14:30:37.602510: step 4484, loss 0.0954984, acc 0.953125, prec 0.0725377, recall 0.885493
2017-12-10T14:30:37.792499: step 4485, loss 0.151127, acc 0.953125, prec 0.0725336, recall 0.885493
2017-12-10T14:30:37.981940: step 4486, loss 0.123698, acc 0.953125, prec 0.0725296, recall 0.885493
2017-12-10T14:30:38.174337: step 4487, loss 0.0451889, acc 0.984375, prec 0.0725455, recall 0.885519
2017-12-10T14:30:38.364876: step 4488, loss 0.0448041, acc 0.96875, prec 0.0725428, recall 0.885519
2017-12-10T14:30:38.557574: step 4489, loss 0.273043, acc 0.921875, prec 0.0725361, recall 0.885519
2017-12-10T14:30:38.746997: step 4490, loss 1.29986, acc 0.953125, prec 0.0725507, recall 0.885343
2017-12-10T14:30:38.937553: step 4491, loss 0.0188047, acc 0.984375, prec 0.0725493, recall 0.885343
2017-12-10T14:30:39.133302: step 4492, loss 0.00661597, acc 1, prec 0.0725493, recall 0.885343
2017-12-10T14:30:39.324982: step 4493, loss 0.235683, acc 0.96875, prec 0.0725466, recall 0.885343
2017-12-10T14:30:39.516542: step 4494, loss 0.255884, acc 0.9375, prec 0.0725585, recall 0.885369
2017-12-10T14:30:39.704772: step 4495, loss 0.200305, acc 0.953125, prec 0.0725544, recall 0.885369
2017-12-10T14:30:39.891726: step 4496, loss 0.0645993, acc 0.984375, prec 0.0725704, recall 0.885395
2017-12-10T14:30:40.085648: step 4497, loss 0.314079, acc 0.9375, prec 0.0725823, recall 0.885421
2017-12-10T14:30:40.277827: step 4498, loss 0.132529, acc 0.96875, prec 0.0725969, recall 0.885448
2017-12-10T14:30:40.465599: step 4499, loss 0.0541654, acc 0.984375, prec 0.0725956, recall 0.885448
2017-12-10T14:30:40.657782: step 4500, loss 0.0402398, acc 0.984375, prec 0.0725942, recall 0.885448
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_2/1512933336/checkpoints/model-4500

2017-12-10T14:30:41.757900: step 4501, loss 0.000893895, acc 1, prec 0.0725942, recall 0.885448
2017-12-10T14:30:41.948550: step 4502, loss 0.373619, acc 0.96875, prec 0.0726261, recall 0.8855
2017-12-10T14:30:42.139622: step 4503, loss 0.124008, acc 0.953125, prec 0.0726567, recall 0.885552
2017-12-10T14:30:42.326552: step 4504, loss 0.161719, acc 0.96875, prec 0.0726713, recall 0.885578
2017-12-10T14:30:42.520680: step 4505, loss 0.217523, acc 0.984375, prec 0.0726699, recall 0.885578
2017-12-10T14:30:42.709023: step 4506, loss 0.123495, acc 1, prec 0.0727218, recall 0.885656
2017-12-10T14:30:42.899400: step 4507, loss 0.240517, acc 0.953125, prec 0.0727697, recall 0.885734
2017-12-10T14:30:43.089153: step 4508, loss 0.307918, acc 0.90625, prec 0.0727962, recall 0.885786
2017-12-10T14:30:43.276726: step 4509, loss 0.135216, acc 0.953125, prec 0.0728267, recall 0.885837
2017-12-10T14:30:43.466453: step 4510, loss 0.0203989, acc 1, prec 0.0728786, recall 0.885915
2017-12-10T14:30:43.658588: step 4511, loss 0.0379731, acc 0.984375, prec 0.0729118, recall 0.885967
2017-12-10T14:30:43.850899: step 4512, loss 0.198914, acc 0.953125, prec 0.072925, recall 0.885993
2017-12-10T14:30:44.042063: step 4513, loss 0.10851, acc 0.953125, prec 0.0729728, recall 0.88607
2017-12-10T14:30:44.232441: step 4514, loss 0.332156, acc 0.984375, prec 0.0729888, recall 0.886096
2017-12-10T14:30:44.421415: step 4515, loss 0.0187962, acc 1, prec 0.073006, recall 0.886122
2017-12-10T14:30:44.610634: step 4516, loss 0.360981, acc 0.953125, prec 0.0730193, recall 0.886148
2017-12-10T14:30:44.800484: step 4517, loss 0.109437, acc 0.96875, prec 0.0730165, recall 0.886148
2017-12-10T14:30:44.989340: step 4518, loss 0.0202816, acc 0.984375, prec 0.0730152, recall 0.886148
2017-12-10T14:30:45.177272: step 4519, loss 0.0901661, acc 0.96875, prec 0.0730124, recall 0.886148
2017-12-10T14:30:45.368897: step 4520, loss 0.28679, acc 0.984375, prec 0.0730284, recall 0.886173
2017-12-10T14:30:45.572425: step 4521, loss 0.0852752, acc 0.96875, prec 0.0730429, recall 0.886199
2017-12-10T14:30:45.761691: step 4522, loss 0.00795049, acc 1, prec 0.0730429, recall 0.886199
2017-12-10T14:30:45.947453: step 4523, loss 0.0247628, acc 0.984375, prec 0.0730416, recall 0.886199
2017-12-10T14:30:46.139983: step 4524, loss 0.0276193, acc 0.984375, prec 0.0730402, recall 0.886199
2017-12-10T14:30:46.329784: step 4525, loss 0.138262, acc 0.96875, prec 0.0730548, recall 0.886225
2017-12-10T14:30:46.519387: step 4526, loss 0.0915437, acc 0.984375, prec 0.0730707, recall 0.886251
2017-12-10T14:30:46.708713: step 4527, loss 0.0314488, acc 1, prec 0.0730707, recall 0.886251
2017-12-10T14:30:46.898686: step 4528, loss 0.33235, acc 0.921875, prec 0.0730639, recall 0.886251
2017-12-10T14:30:47.086551: step 4529, loss 0.0219433, acc 0.984375, prec 0.0730971, recall 0.886302
2017-12-10T14:30:47.277909: step 4530, loss 0.127659, acc 0.96875, prec 0.0731289, recall 0.886353
2017-12-10T14:30:47.468622: step 4531, loss 0.0475695, acc 0.984375, prec 0.0731275, recall 0.886353
2017-12-10T14:30:47.655891: step 4532, loss 0.20543, acc 0.9375, prec 0.0731394, recall 0.886379
2017-12-10T14:30:47.845768: step 4533, loss 0.055212, acc 0.96875, prec 0.0731539, recall 0.886405
2017-12-10T14:30:48.034207: step 4534, loss 0.0144514, acc 0.984375, prec 0.0731525, recall 0.886405
2017-12-10T14:30:48.222875: step 4535, loss 0.0681041, acc 1, prec 0.0731698, recall 0.88643
2017-12-10T14:30:48.412331: step 4536, loss 0.264987, acc 0.96875, prec 0.0731844, recall 0.886456
2017-12-10T14:30:48.605329: step 4537, loss 0.0319879, acc 0.984375, prec 0.0732003, recall 0.886482
2017-12-10T14:30:48.797538: step 4538, loss 1.69634, acc 0.96875, prec 0.0731989, recall 0.886282
2017-12-10T14:30:48.986821: step 4539, loss 0.218772, acc 0.9375, prec 0.0732107, recall 0.886307
2017-12-10T14:30:49.181436: step 4540, loss 0.0821153, acc 0.96875, prec 0.073208, recall 0.886307
2017-12-10T14:30:49.370695: step 4541, loss 0.0215656, acc 0.984375, prec 0.0732239, recall 0.886333
2017-12-10T14:30:49.558089: step 4542, loss 0.219017, acc 0.921875, prec 0.0732171, recall 0.886333
2017-12-10T14:30:49.749137: step 4543, loss 0.197073, acc 0.953125, prec 0.0732475, recall 0.886384
2017-12-10T14:30:49.939230: step 4544, loss 0.106504, acc 0.984375, prec 0.0732462, recall 0.886384
2017-12-10T14:30:50.128154: step 4545, loss 0.303244, acc 0.90625, prec 0.073238, recall 0.886384
2017-12-10T14:30:50.319009: step 4546, loss 0.215748, acc 0.953125, prec 0.0732684, recall 0.886435
2017-12-10T14:30:50.506582: step 4547, loss 0.0783402, acc 0.953125, prec 0.0733161, recall 0.886512
2017-12-10T14:30:50.695608: step 4548, loss 0.0552492, acc 0.984375, prec 0.073332, recall 0.886538
2017-12-10T14:30:50.885418: step 4549, loss 0.0560018, acc 0.984375, prec 0.0733479, recall 0.886563
2017-12-10T14:30:51.076454: step 4550, loss 0.211022, acc 0.984375, prec 0.0733465, recall 0.886563
2017-12-10T14:30:51.267637: step 4551, loss 0.193612, acc 0.921875, prec 0.0733397, recall 0.886563
2017-12-10T14:30:51.452247: step 4552, loss 0.0271742, acc 1, prec 0.0733742, recall 0.886614
2017-12-10T14:30:51.641895: step 4553, loss 0.039703, acc 0.984375, prec 0.0734073, recall 0.886665
2017-12-10T14:30:51.831113: step 4554, loss 1.06426, acc 1, prec 0.0734418, recall 0.886716
2017-12-10T14:30:52.022654: step 4555, loss 0.230134, acc 0.9375, prec 0.0734363, recall 0.886716
2017-12-10T14:30:52.222513: step 4556, loss 0.21055, acc 0.9375, prec 0.0734309, recall 0.886716
2017-12-10T14:30:52.409112: step 4557, loss 0.0134035, acc 1, prec 0.0734654, recall 0.886767
2017-12-10T14:30:52.597341: step 4558, loss 0.0307799, acc 1, prec 0.0734826, recall 0.886792
2017-12-10T14:30:52.787611: step 4559, loss 0.322071, acc 0.9375, prec 0.0734944, recall 0.886818
2017-12-10T14:30:52.977297: step 4560, loss 0.197329, acc 0.921875, prec 0.0735048, recall 0.886843
2017-12-10T14:30:53.167841: step 4561, loss 0.0629849, acc 0.96875, prec 0.073502, recall 0.886843
2017-12-10T14:30:53.356697: step 4562, loss 0.318473, acc 0.921875, prec 0.0734952, recall 0.886843
2017-12-10T14:30:53.546783: step 4563, loss 0.268455, acc 0.875, prec 0.0735015, recall 0.886869
2017-12-10T14:30:53.737279: step 4564, loss 0.447421, acc 0.921875, prec 0.0735291, recall 0.886919
2017-12-10T14:30:53.925167: step 4565, loss 0.0316613, acc 1, prec 0.0735291, recall 0.886919
2017-12-10T14:30:54.113870: step 4566, loss 0.184036, acc 0.9375, prec 0.0735237, recall 0.886919
2017-12-10T14:30:54.302265: step 4567, loss 0.132658, acc 0.984375, prec 0.0735395, recall 0.886945
2017-12-10T14:30:54.493168: step 4568, loss 0.0309557, acc 0.984375, prec 0.0735382, recall 0.886945
2017-12-10T14:30:54.682871: step 4569, loss 0.405918, acc 0.890625, prec 0.0735286, recall 0.886945
2017-12-10T14:30:54.875244: step 4570, loss 0.18614, acc 0.953125, prec 0.0735589, recall 0.886995
2017-12-10T14:30:55.061859: step 4571, loss 0.00882358, acc 1, prec 0.0735589, recall 0.886995
2017-12-10T14:30:55.250950: step 4572, loss 0.0956031, acc 0.96875, prec 0.0735562, recall 0.886995
2017-12-10T14:30:55.438951: step 4573, loss 0.0776286, acc 0.953125, prec 0.0735693, recall 0.887021
2017-12-10T14:30:55.628072: step 4574, loss 0.104714, acc 0.953125, prec 0.0735824, recall 0.887046
2017-12-10T14:30:55.820628: step 4575, loss 0.0156831, acc 1, prec 0.0735997, recall 0.887071
2017-12-10T14:30:56.008192: step 4576, loss 0.09777, acc 0.9375, prec 0.0736286, recall 0.887122
2017-12-10T14:30:56.199538: step 4577, loss 0.179204, acc 0.953125, prec 0.0736245, recall 0.887122
2017-12-10T14:30:56.387474: step 4578, loss 0.0789415, acc 0.96875, prec 0.073639, recall 0.887147
2017-12-10T14:30:56.575707: step 4579, loss 0.146658, acc 0.96875, prec 0.0736363, recall 0.887147
2017-12-10T14:30:56.763554: step 4580, loss 0.110473, acc 0.953125, prec 0.0736494, recall 0.887173
2017-12-10T14:30:56.950345: step 4581, loss 0.0746393, acc 0.984375, prec 0.073648, recall 0.887173
2017-12-10T14:30:57.144259: step 4582, loss 0.0113849, acc 1, prec 0.073648, recall 0.887173
2017-12-10T14:30:57.333965: step 4583, loss 0.0635561, acc 0.96875, prec 0.0736625, recall 0.887198
2017-12-10T14:30:57.527872: step 4584, loss 0.0799487, acc 0.953125, prec 0.0736756, recall 0.887223
2017-12-10T14:30:57.715021: step 4585, loss 0.00716478, acc 1, prec 0.0736928, recall 0.887248
2017-12-10T14:30:57.905554: step 4586, loss 0.00948878, acc 1, prec 0.07371, recall 0.887274
2017-12-10T14:30:58.097688: step 4587, loss 0.0609376, acc 0.984375, prec 0.0737087, recall 0.887274
2017-12-10T14:30:58.287106: step 4588, loss 0.213244, acc 0.953125, prec 0.0737218, recall 0.887299
2017-12-10T14:30:58.475407: step 4589, loss 0.012185, acc 1, prec 0.073739, recall 0.887324
2017-12-10T14:30:58.668729: step 4590, loss 0.177715, acc 0.953125, prec 0.0737521, recall 0.887349
2017-12-10T14:30:58.859957: step 4591, loss 1.46718, acc 0.953125, prec 0.0737824, recall 0.887399
2017-12-10T14:30:59.056950: step 4592, loss 0.235844, acc 0.984375, prec 0.0738326, recall 0.887475
2017-12-10T14:30:59.247266: step 4593, loss 1.62358, acc 0.953125, prec 0.0738299, recall 0.887277
2017-12-10T14:30:59.438690: step 4594, loss 0.0258098, acc 0.984375, prec 0.0738285, recall 0.887277
2017-12-10T14:30:59.633523: step 4595, loss 0.111545, acc 0.96875, prec 0.0738258, recall 0.887277
2017-12-10T14:30:59.823093: step 4596, loss 0.0695175, acc 0.953125, prec 0.0738732, recall 0.887352
2017-12-10T14:31:00.014232: step 4597, loss 0.329047, acc 0.9375, prec 0.0738678, recall 0.887352
2017-12-10T14:31:00.205152: step 4598, loss 0.280722, acc 0.90625, prec 0.0738939, recall 0.887402
2017-12-10T14:31:00.389043: step 4599, loss 0.188798, acc 0.921875, prec 0.0739042, recall 0.887428
2017-12-10T14:31:00.576203: step 4600, loss 0.184493, acc 0.90625, prec 0.073896, recall 0.887428
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_2/1512933336/checkpoints/model-4600

2017-12-10T14:31:01.891119: step 4601, loss 0.35282, acc 0.890625, prec 0.0739036, recall 0.887453
2017-12-10T14:31:02.079500: step 4602, loss 0.181377, acc 0.921875, prec 0.0739311, recall 0.887503
2017-12-10T14:31:02.266095: step 4603, loss 0.455254, acc 0.921875, prec 0.0739243, recall 0.887503
2017-12-10T14:31:02.451481: step 4604, loss 0.110704, acc 0.953125, prec 0.0739373, recall 0.887528
2017-12-10T14:31:02.639050: step 4605, loss 0.257632, acc 0.953125, prec 0.0739504, recall 0.887553
2017-12-10T14:31:02.828630: step 4606, loss 0.259324, acc 0.9375, prec 0.0739964, recall 0.887628
2017-12-10T14:31:03.019589: step 4607, loss 0.374717, acc 0.9375, prec 0.0740081, recall 0.887653
2017-12-10T14:31:03.208651: step 4608, loss 0.241792, acc 0.96875, prec 0.0740054, recall 0.887653
2017-12-10T14:31:03.398731: step 4609, loss 0.211851, acc 0.921875, prec 0.0740157, recall 0.887678
2017-12-10T14:31:03.586650: step 4610, loss 0.231544, acc 0.9375, prec 0.0740102, recall 0.887678
2017-12-10T14:31:03.775384: step 4611, loss 0.487674, acc 0.90625, prec 0.074002, recall 0.887678
2017-12-10T14:31:03.966502: step 4612, loss 0.172623, acc 0.9375, prec 0.0740308, recall 0.887728
2017-12-10T14:31:04.156277: step 4613, loss 0.168906, acc 0.9375, prec 0.074094, recall 0.887828
2017-12-10T14:31:04.344428: step 4614, loss 0.230962, acc 0.9375, prec 0.0741057, recall 0.887853
2017-12-10T14:31:04.532518: step 4615, loss 0.0844391, acc 0.984375, prec 0.0741214, recall 0.887877
2017-12-10T14:31:04.723094: step 4616, loss 0.0381376, acc 0.96875, prec 0.0741359, recall 0.887902
2017-12-10T14:31:04.909817: step 4617, loss 0.146604, acc 0.921875, prec 0.0741461, recall 0.887927
2017-12-10T14:31:05.099879: step 4618, loss 0.375815, acc 0.875, prec 0.0741352, recall 0.887927
2017-12-10T14:31:05.288922: step 4619, loss 0.0504906, acc 0.96875, prec 0.0741324, recall 0.887927
2017-12-10T14:31:05.481635: step 4620, loss 0.225845, acc 0.9375, prec 0.0741269, recall 0.887927
2017-12-10T14:31:05.668920: step 4621, loss 0.339391, acc 0.953125, prec 0.0741571, recall 0.887977
2017-12-10T14:31:05.856950: step 4622, loss 0.0848616, acc 0.953125, prec 0.0741701, recall 0.888002
2017-12-10T14:31:06.045251: step 4623, loss 0.0610302, acc 0.984375, prec 0.0742031, recall 0.888051
2017-12-10T14:31:06.238885: step 4624, loss 0.148393, acc 0.953125, prec 0.0742161, recall 0.888076
2017-12-10T14:31:06.425465: step 4625, loss 0.154487, acc 0.96875, prec 0.0742476, recall 0.888126
2017-12-10T14:31:06.613714: step 4626, loss 0.0377267, acc 0.984375, prec 0.0742634, recall 0.888151
2017-12-10T14:31:06.803243: step 4627, loss 0.235487, acc 0.984375, prec 0.0742792, recall 0.888175
2017-12-10T14:31:06.996790: step 4628, loss 0.0431956, acc 0.984375, prec 0.0743292, recall 0.88825
2017-12-10T14:31:07.185242: step 4629, loss 0.0693225, acc 0.984375, prec 0.0743621, recall 0.888299
2017-12-10T14:31:07.378721: step 4630, loss 0.0402046, acc 0.984375, prec 0.0743607, recall 0.888299
2017-12-10T14:31:07.569850: step 4631, loss 0.122478, acc 0.96875, prec 0.0743751, recall 0.888324
2017-12-10T14:31:07.757687: step 4632, loss 0.0127389, acc 1, prec 0.0743923, recall 0.888348
2017-12-10T14:31:07.949983: step 4633, loss 2.7008, acc 0.96875, prec 0.074408, recall 0.888177
2017-12-10T14:31:08.143998: step 4634, loss 0.792887, acc 0.953125, prec 0.0744381, recall 0.888226
2017-12-10T14:31:08.336030: step 4635, loss 0.370693, acc 0.9375, prec 0.0744326, recall 0.888226
2017-12-10T14:31:08.522768: step 4636, loss 0.234327, acc 0.96875, prec 0.074447, recall 0.888251
2017-12-10T14:31:08.717091: step 4637, loss 0.160522, acc 0.921875, prec 0.0744744, recall 0.8883
2017-12-10T14:31:08.908510: step 4638, loss 0.271708, acc 0.921875, prec 0.0744846, recall 0.888325
2017-12-10T14:31:09.097326: step 4639, loss 0.355352, acc 0.921875, prec 0.074512, recall 0.888374
2017-12-10T14:31:09.287510: step 4640, loss 0.218043, acc 0.953125, prec 0.074525, recall 0.888399
2017-12-10T14:31:09.476128: step 4641, loss 0.272669, acc 0.921875, prec 0.0745181, recall 0.888399
2017-12-10T14:31:09.666401: step 4642, loss 0.395619, acc 0.90625, prec 0.0745098, recall 0.888399
2017-12-10T14:31:09.854195: step 4643, loss 0.176869, acc 0.9375, prec 0.0745043, recall 0.888399
2017-12-10T14:31:10.045352: step 4644, loss 0.547388, acc 0.9375, prec 0.0745159, recall 0.888423
2017-12-10T14:31:10.233493: step 4645, loss 0.504461, acc 0.90625, prec 0.0745247, recall 0.888448
2017-12-10T14:31:10.424222: step 4646, loss 0.311663, acc 0.90625, prec 0.0745165, recall 0.888448
2017-12-10T14:31:10.613136: step 4647, loss 0.0765445, acc 0.953125, prec 0.0745295, recall 0.888473
2017-12-10T14:31:10.802278: step 4648, loss 0.542599, acc 0.890625, prec 0.0745198, recall 0.888473
2017-12-10T14:31:10.989559: step 4649, loss 0.371917, acc 0.9375, prec 0.0745143, recall 0.888473
2017-12-10T14:31:11.177900: step 4650, loss 0.189115, acc 0.953125, prec 0.0745102, recall 0.888473
2017-12-10T14:31:11.366266: step 4651, loss 0.196635, acc 0.953125, prec 0.0745231, recall 0.888497
2017-12-10T14:31:11.561674: step 4652, loss 0.380287, acc 0.890625, prec 0.0745306, recall 0.888522
2017-12-10T14:31:11.752809: step 4653, loss 0.177748, acc 0.9375, prec 0.0745251, recall 0.888522
2017-12-10T14:31:11.940534: step 4654, loss 1.61328, acc 0.875, prec 0.0745483, recall 0.888571
2017-12-10T14:31:12.131806: step 4655, loss 0.0895818, acc 0.96875, prec 0.0745968, recall 0.888644
2017-12-10T14:31:12.321523: step 4656, loss 0.313374, acc 0.90625, prec 0.0746056, recall 0.888669
2017-12-10T14:31:12.512135: step 4657, loss 0.164477, acc 0.9375, prec 0.0746001, recall 0.888669
2017-12-10T14:31:12.695938: step 4658, loss 0.204057, acc 0.9375, prec 0.0746117, recall 0.888693
2017-12-10T14:31:12.882991: step 4659, loss 0.147621, acc 0.96875, prec 0.074609, recall 0.888693
2017-12-10T14:31:13.071143: step 4660, loss 0.463801, acc 0.921875, prec 0.0746021, recall 0.888693
2017-12-10T14:31:13.259360: step 4661, loss 0.389809, acc 0.9375, prec 0.0746136, recall 0.888718
2017-12-10T14:31:13.450047: step 4662, loss 0.449475, acc 0.875, prec 0.0746539, recall 0.888791
2017-12-10T14:31:13.638699: step 4663, loss 0.0707343, acc 0.96875, prec 0.0746682, recall 0.888816
2017-12-10T14:31:13.825941: step 4664, loss 0.241284, acc 0.9375, prec 0.0746798, recall 0.88884
2017-12-10T14:31:14.020552: step 4665, loss 0.185555, acc 0.921875, prec 0.0746729, recall 0.88884
2017-12-10T14:31:14.215427: step 4666, loss 0.373201, acc 0.90625, prec 0.0746817, recall 0.888864
2017-12-10T14:31:14.402626: step 4667, loss 0.532071, acc 0.953125, prec 0.0746775, recall 0.888864
2017-12-10T14:31:14.592244: step 4668, loss 0.0941105, acc 0.96875, prec 0.0746919, recall 0.888889
2017-12-10T14:31:14.778239: step 4669, loss 0.253195, acc 0.9375, prec 0.0747205, recall 0.888938
2017-12-10T14:31:14.968435: step 4670, loss 0.117567, acc 0.921875, prec 0.0747136, recall 0.888938
2017-12-10T14:31:15.160035: step 4671, loss 0.0789814, acc 0.953125, prec 0.0747095, recall 0.888938
2017-12-10T14:31:15.345240: step 4672, loss 0.0623037, acc 0.984375, prec 0.0747081, recall 0.888938
2017-12-10T14:31:15.531290: step 4673, loss 0.201424, acc 0.953125, prec 0.074721, recall 0.888962
2017-12-10T14:31:15.720094: step 4674, loss 0.526874, acc 0.9375, prec 0.0747155, recall 0.888962
2017-12-10T14:31:15.913516: step 4675, loss 3.97994, acc 0.9375, prec 0.0747284, recall 0.888791
2017-12-10T14:31:16.104229: step 4676, loss 0.0406587, acc 0.984375, prec 0.0747271, recall 0.888791
2017-12-10T14:31:16.293399: step 4677, loss 0.160368, acc 0.96875, prec 0.0747584, recall 0.88884
2017-12-10T14:31:16.486152: step 4678, loss 0.095541, acc 0.984375, prec 0.0747912, recall 0.888889
2017-12-10T14:31:16.678804: step 4679, loss 0.275893, acc 0.9375, prec 0.0747857, recall 0.888889
2017-12-10T14:31:16.871351: step 4680, loss 0.165539, acc 0.953125, prec 0.0747986, recall 0.888913
2017-12-10T14:31:17.060828: step 4681, loss 0.269188, acc 0.890625, prec 0.0747889, recall 0.888913
2017-12-10T14:31:17.248715: step 4682, loss 0.278738, acc 0.96875, prec 0.0747862, recall 0.888913
2017-12-10T14:31:17.437827: step 4683, loss 0.163485, acc 0.96875, prec 0.0748346, recall 0.888986
2017-12-10T14:31:17.625245: step 4684, loss 0.21479, acc 0.921875, prec 0.0748447, recall 0.88901
2017-12-10T14:31:17.815993: step 4685, loss 0.217779, acc 0.9375, prec 0.0748563, recall 0.889035
2017-12-10T14:31:18.003462: step 4686, loss 0.23603, acc 0.9375, prec 0.0748678, recall 0.889059
2017-12-10T14:31:18.195109: step 4687, loss 0.049589, acc 0.984375, prec 0.0748835, recall 0.889083
2017-12-10T14:31:18.382203: step 4688, loss 0.376561, acc 0.859375, prec 0.074871, recall 0.889083
2017-12-10T14:31:18.574498: step 4689, loss 0.165019, acc 0.984375, prec 0.0748867, recall 0.889108
2017-12-10T14:31:18.762357: step 4690, loss 0.144379, acc 0.953125, prec 0.0749166, recall 0.889156
2017-12-10T14:31:18.953218: step 4691, loss 0.278758, acc 0.9375, prec 0.0749282, recall 0.88918
2017-12-10T14:31:19.144912: step 4692, loss 0.0881081, acc 0.984375, prec 0.0749438, recall 0.889205
2017-12-10T14:31:19.335696: step 4693, loss 0.175758, acc 0.9375, prec 0.0749383, recall 0.889205
2017-12-10T14:31:19.527619: step 4694, loss 0.338199, acc 0.9375, prec 0.0749328, recall 0.889205
2017-12-10T14:31:19.717244: step 4695, loss 0.110101, acc 0.96875, prec 0.0749641, recall 0.889253
2017-12-10T14:31:19.908286: step 4696, loss 0.308309, acc 0.9375, prec 0.0749756, recall 0.889277
2017-12-10T14:31:20.097518: step 4697, loss 0.17639, acc 0.984375, prec 0.0750253, recall 0.88935
2017-12-10T14:31:20.288784: step 4698, loss 0.0577323, acc 0.96875, prec 0.0750396, recall 0.889374
2017-12-10T14:31:20.476378: step 4699, loss 0.22448, acc 0.984375, prec 0.0750382, recall 0.889374
2017-12-10T14:31:20.667062: step 4700, loss 0.197421, acc 0.9375, prec 0.0750327, recall 0.889374
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_2/1512933336/checkpoints/model-4700

2017-12-10T14:31:21.787231: step 4701, loss 0.140422, acc 0.96875, prec 0.0750299, recall 0.889374
2017-12-10T14:31:21.982108: step 4702, loss 0.0974732, acc 0.984375, prec 0.0750456, recall 0.889398
2017-12-10T14:31:22.172710: step 4703, loss 0.0830363, acc 0.96875, prec 0.0750598, recall 0.889422
2017-12-10T14:31:22.361678: step 4704, loss 0.0372692, acc 0.984375, prec 0.0750755, recall 0.889446
2017-12-10T14:31:22.546497: step 4705, loss 0.105513, acc 0.984375, prec 0.0750741, recall 0.889446
2017-12-10T14:31:22.737428: step 4706, loss 0.149277, acc 0.96875, prec 0.0750713, recall 0.889446
2017-12-10T14:31:22.928983: step 4707, loss 0.0917901, acc 1, prec 0.0750883, recall 0.88947
2017-12-10T14:31:23.120669: step 4708, loss 0.133619, acc 0.96875, prec 0.0750856, recall 0.88947
2017-12-10T14:31:23.311698: step 4709, loss 0.0582788, acc 0.984375, prec 0.0750842, recall 0.88947
2017-12-10T14:31:23.502071: step 4710, loss 0.0320283, acc 0.984375, prec 0.0750828, recall 0.88947
2017-12-10T14:31:23.690894: step 4711, loss 0.0457743, acc 0.984375, prec 0.0750814, recall 0.88947
2017-12-10T14:31:23.881225: step 4712, loss 0.0542793, acc 0.984375, prec 0.0750971, recall 0.889494
2017-12-10T14:31:24.072944: step 4713, loss 0.0482513, acc 0.984375, prec 0.0751127, recall 0.889518
2017-12-10T14:31:24.259151: step 4714, loss 0.121523, acc 0.953125, prec 0.0751256, recall 0.889542
2017-12-10T14:31:24.448658: step 4715, loss 0.00237936, acc 1, prec 0.0751256, recall 0.889542
2017-12-10T14:31:24.640034: step 4716, loss 0.0151903, acc 1, prec 0.0751256, recall 0.889542
2017-12-10T14:31:24.828141: step 4717, loss 0.311641, acc 0.984375, prec 0.0751412, recall 0.889567
2017-12-10T14:31:25.017741: step 4718, loss 2.60826, acc 0.96875, prec 0.0751568, recall 0.889397
2017-12-10T14:31:25.209874: step 4719, loss 0.131677, acc 0.984375, prec 0.0751725, recall 0.889421
2017-12-10T14:31:25.397658: step 4720, loss 0.173269, acc 0.953125, prec 0.0751853, recall 0.889445
2017-12-10T14:31:25.588554: step 4721, loss 0.0191562, acc 1, prec 0.0751853, recall 0.889445
2017-12-10T14:31:25.776237: step 4722, loss 0.265248, acc 0.921875, prec 0.0752124, recall 0.889493
2017-12-10T14:31:25.962460: step 4723, loss 0.00683604, acc 1, prec 0.0752124, recall 0.889493
2017-12-10T14:31:26.148751: step 4724, loss 0.0499655, acc 0.984375, prec 0.0752281, recall 0.889517
2017-12-10T14:31:26.340506: step 4725, loss 0.174521, acc 0.9375, prec 0.0752225, recall 0.889517
2017-12-10T14:31:26.530167: step 4726, loss 0.121153, acc 0.96875, prec 0.0752198, recall 0.889517
2017-12-10T14:31:26.721712: step 4727, loss 0.133657, acc 0.96875, prec 0.075217, recall 0.889517
2017-12-10T14:31:26.915937: step 4728, loss 0.236817, acc 0.9375, prec 0.0752115, recall 0.889517
2017-12-10T14:31:27.105226: step 4729, loss 0.135123, acc 0.96875, prec 0.0752087, recall 0.889517
2017-12-10T14:31:27.293364: step 4730, loss 0.501759, acc 0.921875, prec 0.0752018, recall 0.889517
2017-12-10T14:31:27.486896: step 4731, loss 0.0702643, acc 0.984375, prec 0.0752004, recall 0.889517
2017-12-10T14:31:27.675246: step 4732, loss 0.159904, acc 0.96875, prec 0.0751976, recall 0.889517
2017-12-10T14:31:27.868371: step 4733, loss 0.0358185, acc 0.984375, prec 0.0752133, recall 0.889541
2017-12-10T14:31:28.053205: step 4734, loss 0.214844, acc 0.921875, prec 0.0752404, recall 0.889589
2017-12-10T14:31:28.248289: step 4735, loss 0.0326962, acc 1, prec 0.0752574, recall 0.889613
2017-12-10T14:31:28.437185: step 4736, loss 0.134909, acc 0.9375, prec 0.0752688, recall 0.889637
2017-12-10T14:31:28.629777: step 4737, loss 0.186457, acc 0.953125, prec 0.0752817, recall 0.889661
2017-12-10T14:31:28.823849: step 4738, loss 0.119095, acc 0.96875, prec 0.0752789, recall 0.889661
2017-12-10T14:31:29.019937: step 4739, loss 0.351379, acc 0.953125, prec 0.0752917, recall 0.889685
2017-12-10T14:31:29.209313: step 4740, loss 0.0761014, acc 0.984375, prec 0.0753073, recall 0.889709
2017-12-10T14:31:29.401293: step 4741, loss 0.23522, acc 0.921875, prec 0.0753004, recall 0.889709
2017-12-10T14:31:29.593838: step 4742, loss 0.139925, acc 0.953125, prec 0.0752963, recall 0.889709
2017-12-10T14:31:29.789732: step 4743, loss 0.0704353, acc 0.953125, prec 0.0752921, recall 0.889709
2017-12-10T14:31:29.984203: step 4744, loss 0.0348208, acc 1, prec 0.0753431, recall 0.889781
2017-12-10T14:31:30.180686: step 4745, loss 0.175893, acc 0.9375, prec 0.0753376, recall 0.889781
2017-12-10T14:31:30.372788: step 4746, loss 0.155346, acc 0.953125, prec 0.0753844, recall 0.889853
2017-12-10T14:31:30.561185: step 4747, loss 0.308285, acc 0.953125, prec 0.0753972, recall 0.889876
2017-12-10T14:31:30.751719: step 4748, loss 0.202344, acc 0.953125, prec 0.075444, recall 0.889948
2017-12-10T14:31:30.944810: step 4749, loss 0.136602, acc 0.96875, prec 0.0754412, recall 0.889948
2017-12-10T14:31:31.137680: step 4750, loss 0.0099066, acc 1, prec 0.0754412, recall 0.889948
2017-12-10T14:31:31.327059: step 4751, loss 0.174846, acc 0.96875, prec 0.0754724, recall 0.889996
2017-12-10T14:31:31.522566: step 4752, loss 0.0846514, acc 0.96875, prec 0.0754866, recall 0.890019
2017-12-10T14:31:31.712989: step 4753, loss 0.132672, acc 0.984375, prec 0.0755022, recall 0.890043
2017-12-10T14:31:31.903334: step 4754, loss 0.127388, acc 0.96875, prec 0.0755164, recall 0.890067
2017-12-10T14:31:32.092808: step 4755, loss 0.663934, acc 0.984375, prec 0.0755489, recall 0.890115
2017-12-10T14:31:32.282968: step 4756, loss 0.141041, acc 0.984375, prec 0.0755815, recall 0.890162
2017-12-10T14:31:32.471436: step 4757, loss 0.0758916, acc 1, prec 0.0756154, recall 0.89021
2017-12-10T14:31:32.664554: step 4758, loss 0.320534, acc 0.96875, prec 0.0756296, recall 0.890233
2017-12-10T14:31:32.859518: step 4759, loss 0.013996, acc 1, prec 0.0756296, recall 0.890233
2017-12-10T14:31:33.047998: step 4760, loss 0.0637989, acc 0.96875, prec 0.0756438, recall 0.890257
2017-12-10T14:31:33.242581: step 4761, loss 0.192737, acc 0.953125, prec 0.0756566, recall 0.890281
2017-12-10T14:31:33.435231: step 4762, loss 0.0182664, acc 0.984375, prec 0.0756892, recall 0.890328
2017-12-10T14:31:33.623081: step 4763, loss 0.0257771, acc 1, prec 0.0756892, recall 0.890328
2017-12-10T14:31:33.810692: step 4764, loss 0.0702436, acc 0.96875, prec 0.0757203, recall 0.890375
2017-12-10T14:31:34.001103: step 4765, loss 0.395363, acc 0.921875, prec 0.0757303, recall 0.890399
2017-12-10T14:31:34.188101: step 4766, loss 0.159736, acc 0.953125, prec 0.0757262, recall 0.890399
2017-12-10T14:31:34.379258: step 4767, loss 0.390115, acc 0.890625, prec 0.0757164, recall 0.890399
2017-12-10T14:31:34.565814: step 4768, loss 0.0620307, acc 0.984375, prec 0.075732, recall 0.890423
2017-12-10T14:31:34.759033: step 4769, loss 0.267717, acc 0.96875, prec 0.0757462, recall 0.890446
2017-12-10T14:31:34.948239: step 4770, loss 0.0955447, acc 0.96875, prec 0.0757604, recall 0.89047
2017-12-10T14:31:35.136444: step 4771, loss 0.119866, acc 0.984375, prec 0.075759, recall 0.89047
2017-12-10T14:31:35.324660: step 4772, loss 0.127184, acc 0.9375, prec 0.0757704, recall 0.890494
2017-12-10T14:31:35.517226: step 4773, loss 0.224591, acc 0.9375, prec 0.0757648, recall 0.890494
2017-12-10T14:31:35.705244: step 4774, loss 0.0502353, acc 0.96875, prec 0.075779, recall 0.890517
2017-12-10T14:31:35.898179: step 4775, loss 0.504245, acc 0.953125, prec 0.0757918, recall 0.890541
2017-12-10T14:31:36.091067: step 4776, loss 0.0442239, acc 0.984375, prec 0.0758243, recall 0.890588
2017-12-10T14:31:36.279533: step 4777, loss 0.203188, acc 0.984375, prec 0.0758737, recall 0.890659
2017-12-10T14:31:36.467237: step 4778, loss 0.0829512, acc 0.984375, prec 0.0759062, recall 0.890706
2017-12-10T14:31:36.658203: step 4779, loss 2.27636, acc 0.984375, prec 0.0759231, recall 0.890538
2017-12-10T14:31:36.850782: step 4780, loss 0.126179, acc 0.984375, prec 0.0759726, recall 0.890608
2017-12-10T14:31:37.038851: step 4781, loss 0.236199, acc 0.9375, prec 0.0759839, recall 0.890632
2017-12-10T14:31:37.228797: step 4782, loss 0.795647, acc 0.984375, prec 0.0760164, recall 0.890679
2017-12-10T14:31:37.423032: step 4783, loss 0.76505, acc 0.9375, prec 0.0760278, recall 0.890702
2017-12-10T14:31:37.611151: step 4784, loss 0.0509817, acc 0.96875, prec 0.0760589, recall 0.890749
2017-12-10T14:31:37.800462: step 4785, loss 0.166888, acc 0.921875, prec 0.0760519, recall 0.890749
2017-12-10T14:31:37.991815: step 4786, loss 0.23077, acc 0.921875, prec 0.0760619, recall 0.890773
2017-12-10T14:31:38.178934: step 4787, loss 0.239362, acc 0.890625, prec 0.0760521, recall 0.890773
2017-12-10T14:31:38.368828: step 4788, loss 0.326856, acc 0.921875, prec 0.0760621, recall 0.890796
2017-12-10T14:31:38.558542: step 4789, loss 0.121869, acc 0.953125, prec 0.0760579, recall 0.890796
2017-12-10T14:31:38.752722: step 4790, loss 0.521486, acc 0.90625, prec 0.0760495, recall 0.890796
2017-12-10T14:31:38.943296: step 4791, loss 0.410298, acc 0.84375, prec 0.0760694, recall 0.890843
2017-12-10T14:31:39.137897: step 4792, loss 0.167335, acc 0.953125, prec 0.076116, recall 0.890913
2017-12-10T14:31:39.325759: step 4793, loss 0.397146, acc 0.890625, prec 0.0761232, recall 0.890936
2017-12-10T14:31:39.516936: step 4794, loss 0.394003, acc 0.90625, prec 0.0761486, recall 0.890983
2017-12-10T14:31:39.705520: step 4795, loss 0.312744, acc 0.90625, prec 0.0761572, recall 0.891006
2017-12-10T14:31:39.897812: step 4796, loss 0.352358, acc 0.921875, prec 0.0761502, recall 0.891006
2017-12-10T14:31:40.087293: step 4797, loss 0.16446, acc 0.921875, prec 0.0761432, recall 0.891006
2017-12-10T14:31:40.278139: step 4798, loss 0.585856, acc 0.890625, prec 0.0761335, recall 0.891006
2017-12-10T14:31:40.469855: step 4799, loss 0.0823965, acc 0.984375, prec 0.0761659, recall 0.891053
2017-12-10T14:31:40.658124: step 4800, loss 0.240894, acc 0.9375, prec 0.0761772, recall 0.891076
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_2/1512933336/checkpoints/model-4800

2017-12-10T14:31:41.982687: step 4801, loss 0.123289, acc 0.96875, prec 0.0761744, recall 0.891076
2017-12-10T14:31:42.173432: step 4802, loss 0.378467, acc 0.90625, prec 0.0761661, recall 0.891076
2017-12-10T14:31:42.364783: step 4803, loss 0.0455438, acc 0.984375, prec 0.0761647, recall 0.891076
2017-12-10T14:31:42.555605: step 4804, loss 0.122252, acc 0.9375, prec 0.0761591, recall 0.891076
2017-12-10T14:31:42.748830: step 4805, loss 0.0606285, acc 0.984375, prec 0.0761577, recall 0.891076
2017-12-10T14:31:42.943905: step 4806, loss 0.0509529, acc 0.984375, prec 0.0761732, recall 0.8911
2017-12-10T14:31:43.132950: step 4807, loss 0.135037, acc 0.984375, prec 0.0761887, recall 0.891123
2017-12-10T14:31:43.325830: step 4808, loss 0.0160994, acc 1, prec 0.0761887, recall 0.891123
2017-12-10T14:31:43.516911: step 4809, loss 0.239928, acc 0.953125, prec 0.0762859, recall 0.891263
2017-12-10T14:31:43.707651: step 4810, loss 0.242474, acc 0.9375, prec 0.0762972, recall 0.891286
2017-12-10T14:31:43.894942: step 4811, loss 0.0859189, acc 0.953125, prec 0.076293, recall 0.891286
2017-12-10T14:31:44.088234: step 4812, loss 0.0787817, acc 0.984375, prec 0.0762916, recall 0.891286
2017-12-10T14:31:44.279082: step 4813, loss 0.0663376, acc 0.984375, prec 0.076324, recall 0.891332
2017-12-10T14:31:44.469602: step 4814, loss 0.185422, acc 0.953125, prec 0.0763198, recall 0.891332
2017-12-10T14:31:44.657208: step 4815, loss 0.283286, acc 0.953125, prec 0.0763663, recall 0.891402
2017-12-10T14:31:44.843857: step 4816, loss 0.224233, acc 0.96875, prec 0.0763635, recall 0.891402
2017-12-10T14:31:45.034828: step 4817, loss 0.612539, acc 0.984375, prec 0.0764128, recall 0.891471
2017-12-10T14:31:45.228166: step 4818, loss 0.142288, acc 1, prec 0.0764296, recall 0.891494
2017-12-10T14:31:45.424263: step 4819, loss 0.928317, acc 0.984375, prec 0.0764451, recall 0.891517
2017-12-10T14:31:45.618901: step 4820, loss 0.0153214, acc 1, prec 0.0764789, recall 0.891564
2017-12-10T14:31:45.810595: step 4821, loss 0.631662, acc 0.953125, prec 0.0765084, recall 0.89161
2017-12-10T14:31:46.001477: step 4822, loss 0.110404, acc 0.953125, prec 0.0765042, recall 0.89161
2017-12-10T14:31:46.192325: step 4823, loss 0.136086, acc 0.96875, prec 0.0765183, recall 0.891633
2017-12-10T14:31:46.386348: step 4824, loss 0.121702, acc 0.96875, prec 0.0765493, recall 0.891679
2017-12-10T14:31:46.577450: step 4825, loss 0.22966, acc 0.96875, prec 0.0765633, recall 0.891702
2017-12-10T14:31:46.765469: step 4826, loss 0.228692, acc 0.921875, prec 0.0765563, recall 0.891702
2017-12-10T14:31:46.954673: step 4827, loss 0.12008, acc 0.953125, prec 0.0765859, recall 0.891748
2017-12-10T14:31:47.143198: step 4828, loss 0.162357, acc 0.921875, prec 0.0765957, recall 0.891771
2017-12-10T14:31:47.333339: step 4829, loss 0.135773, acc 0.9375, prec 0.076607, recall 0.891794
2017-12-10T14:31:47.517582: step 4830, loss 0.0941048, acc 0.984375, prec 0.0766056, recall 0.891794
2017-12-10T14:31:47.704640: step 4831, loss 0.469001, acc 0.875, prec 0.0765944, recall 0.891794
2017-12-10T14:31:47.895740: step 4832, loss 0.143082, acc 0.953125, prec 0.0766071, recall 0.891817
2017-12-10T14:31:48.084333: step 4833, loss 0.313643, acc 0.984375, prec 0.0766225, recall 0.89184
2017-12-10T14:31:48.272509: step 4834, loss 0.0748335, acc 0.9375, prec 0.076617, recall 0.89184
2017-12-10T14:31:48.464395: step 4835, loss 0.211691, acc 0.9375, prec 0.0766451, recall 0.891886
2017-12-10T14:31:48.656252: step 4836, loss 0.0798921, acc 0.984375, prec 0.0766774, recall 0.891932
2017-12-10T14:31:48.845645: step 4837, loss 0.270965, acc 0.96875, prec 0.0767251, recall 0.892001
2017-12-10T14:31:49.041520: step 4838, loss 0.282879, acc 0.984375, prec 0.0767911, recall 0.892092
2017-12-10T14:31:49.229668: step 4839, loss 0.638121, acc 1, prec 0.076808, recall 0.892115
2017-12-10T14:31:49.421024: step 4840, loss 0.526903, acc 0.90625, prec 0.0767996, recall 0.892115
2017-12-10T14:31:49.608900: step 4841, loss 0.175877, acc 0.953125, prec 0.0767954, recall 0.892115
2017-12-10T14:31:49.797586: step 4842, loss 0.550769, acc 0.859375, prec 0.0767827, recall 0.892115
2017-12-10T14:31:49.987345: step 4843, loss 0.336194, acc 0.90625, prec 0.0767743, recall 0.892115
2017-12-10T14:31:50.177933: step 4844, loss 0.442724, acc 0.90625, prec 0.0767996, recall 0.892161
2017-12-10T14:31:50.363381: step 4845, loss 0.207336, acc 0.9375, prec 0.0768109, recall 0.892184
2017-12-10T14:31:50.550154: step 4846, loss 0.208162, acc 0.96875, prec 0.0768249, recall 0.892207
2017-12-10T14:31:50.738167: step 4847, loss 0.515907, acc 0.859375, prec 0.0768123, recall 0.892207
2017-12-10T14:31:50.930771: step 4848, loss 0.100546, acc 0.96875, prec 0.07686, recall 0.892275
2017-12-10T14:31:51.117811: step 4849, loss 0.180485, acc 0.9375, prec 0.0768712, recall 0.892298
2017-12-10T14:31:51.306893: step 4850, loss 0.198924, acc 0.9375, prec 0.0768824, recall 0.892321
2017-12-10T14:31:51.495941: step 4851, loss 0.299382, acc 0.953125, prec 0.076895, recall 0.892343
2017-12-10T14:31:51.691230: step 4852, loss 0.263358, acc 0.953125, prec 0.0769245, recall 0.892389
2017-12-10T14:31:51.887140: step 4853, loss 0.251668, acc 0.96875, prec 0.0769217, recall 0.892389
2017-12-10T14:31:52.081948: step 4854, loss 0.0874812, acc 0.953125, prec 0.0769175, recall 0.892389
2017-12-10T14:31:52.272548: step 4855, loss 0.210426, acc 0.96875, prec 0.0769147, recall 0.892389
2017-12-10T14:31:52.463313: step 4856, loss 0.111576, acc 0.953125, prec 0.0769105, recall 0.892389
2017-12-10T14:31:52.654864: step 4857, loss 0.225837, acc 0.96875, prec 0.0769581, recall 0.892457
2017-12-10T14:31:52.843556: step 4858, loss 0.0472261, acc 1, prec 0.0769917, recall 0.892503
2017-12-10T14:31:53.035878: step 4859, loss 0.241425, acc 0.9375, prec 0.077003, recall 0.892525
2017-12-10T14:31:53.227948: step 4860, loss 0.0176973, acc 1, prec 0.077003, recall 0.892525
2017-12-10T14:31:53.416513: step 4861, loss 0.194241, acc 0.96875, prec 0.0770001, recall 0.892525
2017-12-10T14:31:53.604623: step 4862, loss 0.162322, acc 0.96875, prec 0.0770142, recall 0.892548
2017-12-10T14:31:53.793876: step 4863, loss 0.242645, acc 0.953125, prec 0.0770099, recall 0.892548
2017-12-10T14:31:53.981316: step 4864, loss 0.00542375, acc 1, prec 0.0770268, recall 0.892571
2017-12-10T14:31:54.172048: step 4865, loss 0.182939, acc 0.984375, prec 0.077059, recall 0.892616
2017-12-10T14:31:54.365447: step 4866, loss 0.325202, acc 0.9375, prec 0.077087, recall 0.892661
2017-12-10T14:31:54.557249: step 4867, loss 0.167853, acc 0.953125, prec 0.0770996, recall 0.892684
2017-12-10T14:31:54.747185: step 4868, loss 0.0195239, acc 1, prec 0.0770996, recall 0.892684
2017-12-10T14:31:54.939632: step 4869, loss 0.16955, acc 0.96875, prec 0.0770968, recall 0.892684
2017-12-10T14:31:55.132951: step 4870, loss 4.65526, acc 0.953125, prec 0.077094, recall 0.892496
2017-12-10T14:31:55.324992: step 4871, loss 0.179495, acc 0.984375, prec 0.0771262, recall 0.892541
2017-12-10T14:31:55.515403: step 4872, loss 0.628271, acc 0.9375, prec 0.0771541, recall 0.892586
2017-12-10T14:31:55.709519: step 4873, loss 0.0567126, acc 0.96875, prec 0.0771681, recall 0.892609
2017-12-10T14:31:55.900530: step 4874, loss 0.25727, acc 0.9375, prec 0.0771625, recall 0.892609
2017-12-10T14:31:56.091390: step 4875, loss 0.098607, acc 0.953125, prec 0.0771751, recall 0.892632
2017-12-10T14:31:56.276341: step 4876, loss 0.288995, acc 0.9375, prec 0.0772031, recall 0.892677
2017-12-10T14:31:56.468495: step 4877, loss 0.35086, acc 0.90625, prec 0.077245, recall 0.892744
2017-12-10T14:31:56.659362: step 4878, loss 0.265142, acc 0.9375, prec 0.0772394, recall 0.892744
2017-12-10T14:31:56.853238: step 4879, loss 0.530504, acc 0.921875, prec 0.0772659, recall 0.89279
2017-12-10T14:31:57.045378: step 4880, loss 0.113662, acc 0.96875, prec 0.0772799, recall 0.892812
2017-12-10T14:31:57.236711: step 4881, loss 0.390701, acc 0.921875, prec 0.0772729, recall 0.892812
2017-12-10T14:31:57.433122: step 4882, loss 0.12469, acc 0.96875, prec 0.0773204, recall 0.89288
2017-12-10T14:31:57.625382: step 4883, loss 0.363134, acc 0.90625, prec 0.0773456, recall 0.892925
2017-12-10T14:31:57.815564: step 4884, loss 0.323874, acc 0.96875, prec 0.0773595, recall 0.892947
2017-12-10T14:31:58.007562: step 4885, loss 0.200162, acc 0.9375, prec 0.0773707, recall 0.89297
2017-12-10T14:31:58.197842: step 4886, loss 3.21304, acc 0.921875, prec 0.0773664, recall 0.892595
2017-12-10T14:31:58.388121: step 4887, loss 0.320949, acc 0.9375, prec 0.0773944, recall 0.89264
2017-12-10T14:31:58.574255: step 4888, loss 0.187144, acc 0.9375, prec 0.0774223, recall 0.892685
2017-12-10T14:31:58.764293: step 4889, loss 0.582762, acc 0.84375, prec 0.0774585, recall 0.892752
2017-12-10T14:31:58.956325: step 4890, loss 0.534372, acc 0.859375, prec 0.0774458, recall 0.892752
2017-12-10T14:31:59.152552: step 4891, loss 0.248311, acc 0.953125, prec 0.0774416, recall 0.892752
2017-12-10T14:31:59.342286: step 4892, loss 0.38807, acc 0.90625, prec 0.0774835, recall 0.89282
2017-12-10T14:31:59.530727: step 4893, loss 3.33159, acc 0.875, prec 0.0774904, recall 0.892655
2017-12-10T14:31:59.725669: step 4894, loss 0.531248, acc 0.828125, prec 0.0774749, recall 0.892655
2017-12-10T14:31:59.912574: step 4895, loss 0.539366, acc 0.859375, prec 0.0774622, recall 0.892655
2017-12-10T14:32:00.101814: step 4896, loss 0.793513, acc 0.765625, prec 0.0774579, recall 0.892678
2017-12-10T14:32:00.287818: step 4897, loss 0.597491, acc 0.859375, prec 0.0774787, recall 0.892723
2017-12-10T14:32:00.474248: step 4898, loss 0.785541, acc 0.859375, prec 0.0774661, recall 0.892723
2017-12-10T14:32:00.666415: step 4899, loss 0.343265, acc 0.875, prec 0.0774883, recall 0.892768
2017-12-10T14:32:00.855122: step 4900, loss 0.640092, acc 0.859375, prec 0.0774756, recall 0.892768
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_2/1512933336/checkpoints/model-4900

2017-12-10T14:32:01.932062: step 4901, loss 0.40206, acc 0.859375, prec 0.077463, recall 0.892768
2017-12-10T14:32:02.121958: step 4902, loss 0.642937, acc 0.84375, prec 0.0775159, recall 0.892857
2017-12-10T14:32:02.312039: step 4903, loss 0.208313, acc 0.953125, prec 0.0775451, recall 0.892902
2017-12-10T14:32:02.503103: step 4904, loss 0.642918, acc 0.859375, prec 0.0775324, recall 0.892902
2017-12-10T14:32:02.689955: step 4905, loss 0.417433, acc 0.875, prec 0.0775379, recall 0.892924
2017-12-10T14:32:02.881583: step 4906, loss 0.222694, acc 0.90625, prec 0.0775295, recall 0.892924
2017-12-10T14:32:03.069991: step 4907, loss 0.317966, acc 0.890625, prec 0.0775531, recall 0.892969
2017-12-10T14:32:03.258580: step 4908, loss 0.116641, acc 0.953125, prec 0.0775656, recall 0.892991
2017-12-10T14:32:03.445045: step 4909, loss 0.331252, acc 0.921875, prec 0.0775586, recall 0.892991
2017-12-10T14:32:03.632841: step 4910, loss 0.564976, acc 0.84375, prec 0.0775779, recall 0.893036
2017-12-10T14:32:03.821650: step 4911, loss 0.452637, acc 0.84375, prec 0.0775806, recall 0.893058
2017-12-10T14:32:04.012792: step 4912, loss 0.611494, acc 0.84375, prec 0.0776166, recall 0.893125
2017-12-10T14:32:04.203408: step 4913, loss 0.208647, acc 0.9375, prec 0.0776444, recall 0.89317
2017-12-10T14:32:04.392329: step 4914, loss 0.688858, acc 0.953125, prec 0.0776736, recall 0.893214
2017-12-10T14:32:04.584928: step 4915, loss 0.0895964, acc 0.953125, prec 0.0776694, recall 0.893214
2017-12-10T14:32:04.775111: step 4916, loss 0.17948, acc 0.9375, prec 0.0776637, recall 0.893214
2017-12-10T14:32:04.963404: step 4917, loss 0.17796, acc 0.9375, prec 0.0776748, recall 0.893236
2017-12-10T14:32:05.152833: step 4918, loss 0.0979214, acc 0.984375, prec 0.0776734, recall 0.893236
2017-12-10T14:32:05.341464: step 4919, loss 0.0799446, acc 0.96875, prec 0.0776706, recall 0.893236
2017-12-10T14:32:05.530597: step 4920, loss 0.305488, acc 0.953125, prec 0.0776831, recall 0.893258
2017-12-10T14:32:05.723694: step 4921, loss 0.149764, acc 0.953125, prec 0.0777122, recall 0.893303
2017-12-10T14:32:05.915573: step 4922, loss 0.211951, acc 0.984375, prec 0.0777275, recall 0.893325
2017-12-10T14:32:06.105534: step 4923, loss 0.0983808, acc 0.9375, prec 0.0777219, recall 0.893325
2017-12-10T14:32:06.297174: step 4924, loss 0.0916819, acc 0.96875, prec 0.0777191, recall 0.893325
2017-12-10T14:32:06.484094: step 4925, loss 0.176412, acc 0.984375, prec 0.0777344, recall 0.893347
2017-12-10T14:32:06.675084: step 4926, loss 0.196277, acc 0.96875, prec 0.0777315, recall 0.893347
2017-12-10T14:32:06.867081: step 4927, loss 0.105774, acc 0.953125, prec 0.0777273, recall 0.893347
2017-12-10T14:32:07.059794: step 4928, loss 0.361537, acc 0.921875, prec 0.0777203, recall 0.893347
2017-12-10T14:32:07.250124: step 4929, loss 0.120171, acc 0.9375, prec 0.0777314, recall 0.893369
2017-12-10T14:32:07.443858: step 4930, loss 0.046503, acc 1, prec 0.0777647, recall 0.893414
2017-12-10T14:32:07.637206: step 4931, loss 0.136174, acc 0.9375, prec 0.0777591, recall 0.893414
2017-12-10T14:32:07.829881: step 4932, loss 0.0627818, acc 0.984375, prec 0.0777577, recall 0.893414
2017-12-10T14:32:08.019621: step 4933, loss 0.111315, acc 0.953125, prec 0.0777701, recall 0.893436
2017-12-10T14:32:08.206918: step 4934, loss 0.0593559, acc 0.984375, prec 0.0777854, recall 0.893458
2017-12-10T14:32:08.399949: step 4935, loss 0.0949681, acc 1, prec 0.0778021, recall 0.89348
2017-12-10T14:32:08.592477: step 4936, loss 0.114643, acc 0.96875, prec 0.0777993, recall 0.89348
2017-12-10T14:32:08.780766: step 4937, loss 0.10365, acc 0.96875, prec 0.0777965, recall 0.89348
2017-12-10T14:32:08.967610: step 4938, loss 0.0429153, acc 0.984375, prec 0.0778117, recall 0.893502
2017-12-10T14:32:09.160878: step 4939, loss 0.0340973, acc 0.984375, prec 0.0778103, recall 0.893502
2017-12-10T14:32:09.352022: step 4940, loss 0.269105, acc 0.953125, prec 0.0778228, recall 0.893524
2017-12-10T14:32:09.541876: step 4941, loss 0.12399, acc 0.984375, prec 0.0778214, recall 0.893524
2017-12-10T14:32:09.728574: step 4942, loss 0.214619, acc 0.953125, prec 0.0778505, recall 0.893568
2017-12-10T14:32:09.920646: step 4943, loss 0.0826294, acc 0.984375, prec 0.0778491, recall 0.893568
2017-12-10T14:32:10.113148: step 4944, loss 0.0793057, acc 0.96875, prec 0.0778463, recall 0.893568
2017-12-10T14:32:10.306916: step 4945, loss 0.0475832, acc 0.984375, prec 0.0778782, recall 0.893613
2017-12-10T14:32:10.498069: step 4946, loss 0.159745, acc 1, prec 0.0778949, recall 0.893635
2017-12-10T14:32:10.687228: step 4947, loss 0.00524156, acc 1, prec 0.0778949, recall 0.893635
2017-12-10T14:32:10.877535: step 4948, loss 0.236441, acc 0.984375, prec 0.0779101, recall 0.893657
2017-12-10T14:32:11.070339: step 4949, loss 0.181565, acc 0.96875, prec 0.0779073, recall 0.893657
2017-12-10T14:32:11.264946: step 4950, loss 0.0176591, acc 1, prec 0.0779573, recall 0.893723
2017-12-10T14:32:11.458218: step 4951, loss 0.353874, acc 1, prec 0.0780073, recall 0.893789
2017-12-10T14:32:11.653062: step 4952, loss 3.12791, acc 0.984375, prec 0.0780239, recall 0.893626
2017-12-10T14:32:11.850682: step 4953, loss 0.00594447, acc 1, prec 0.0780406, recall 0.893648
2017-12-10T14:32:12.040707: step 4954, loss 0.011823, acc 1, prec 0.0780406, recall 0.893648
2017-12-10T14:32:12.233106: step 4955, loss 0.167321, acc 0.984375, prec 0.0780392, recall 0.893648
2017-12-10T14:32:12.426303: step 4956, loss 0.261064, acc 0.953125, prec 0.0780516, recall 0.89367
2017-12-10T14:32:12.616269: step 4957, loss 0.0731685, acc 0.96875, prec 0.0780488, recall 0.89367
2017-12-10T14:32:12.805238: step 4958, loss 0.258527, acc 0.953125, prec 0.0780445, recall 0.89367
2017-12-10T14:32:12.996284: step 4959, loss 0.591052, acc 0.84375, prec 0.0780971, recall 0.893758
2017-12-10T14:32:13.182604: step 4960, loss 0.173356, acc 0.921875, prec 0.0781067, recall 0.89378
2017-12-10T14:32:13.373955: step 4961, loss 0.637828, acc 0.84375, prec 0.0780926, recall 0.89378
2017-12-10T14:32:13.560380: step 4962, loss 0.0427543, acc 0.96875, prec 0.0781064, recall 0.893802
2017-12-10T14:32:13.751834: step 4963, loss 0.145704, acc 0.9375, prec 0.0781007, recall 0.893802
2017-12-10T14:32:13.944058: step 4964, loss 0.402513, acc 0.90625, prec 0.0780923, recall 0.893802
2017-12-10T14:32:14.138977: step 4965, loss 0.0950078, acc 0.96875, prec 0.0781227, recall 0.893845
2017-12-10T14:32:14.328912: step 4966, loss 0.230561, acc 0.953125, prec 0.0781352, recall 0.893867
2017-12-10T14:32:14.521905: step 4967, loss 0.344547, acc 0.890625, prec 0.0781253, recall 0.893867
2017-12-10T14:32:14.712513: step 4968, loss 0.189244, acc 0.96875, prec 0.0781225, recall 0.893867
2017-12-10T14:32:14.899336: step 4969, loss 0.052686, acc 0.984375, prec 0.078171, recall 0.893933
2017-12-10T14:32:15.074817: step 4970, loss 0.134531, acc 0.942308, prec 0.0782, recall 0.893977
Training finished
Starting Fold: 3 => Train/Dev split: 31796/10598


 #################### 


SEQUENCE LENGTH 273
BATCH SIZE 64
EMBEDDING SIZE 128
FILTER SIZES [3, 4, 5]
NUMBER OF FILTERS 128
L2 REG LAMBDA 0.0
EPOCHS 10



RESULT DIR leave_position_embedding_out_fold_3
VOCAB SIZE 33447
DROPOUT PROBABILITY 0.5



WORD EMBEDDING True
POSITION EMBEDDING False
POS EMBEDDING True


 #################### 


Writing to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_3/1512934336

Start training
2017-12-10T14:32:19.017743: step 1, loss 3.46193, acc 0.28125, prec 0.0212766, recall 1
2017-12-10T14:32:19.198766: step 2, loss 1.86996, acc 0.46875, prec 0.0243902, recall 1
2017-12-10T14:32:19.386297: step 3, loss 1.07065, acc 0.578125, prec 0.0183486, recall 1
2017-12-10T14:32:19.570716: step 4, loss 26.7982, acc 0.78125, prec 0.0166667, recall 0.4
2017-12-10T14:32:19.759002: step 5, loss 0.604268, acc 0.8125, prec 0.0151515, recall 0.4
2017-12-10T14:32:19.941747: step 6, loss 8.62919, acc 0.859375, prec 0.0212766, recall 0.428571
2017-12-10T14:32:20.131246: step 7, loss 14.2431, acc 0.828125, prec 0.0198676, recall 0.375
2017-12-10T14:32:20.320062: step 8, loss 0.925996, acc 0.75, prec 0.0179641, recall 0.375
2017-12-10T14:32:20.503875: step 9, loss 16.2742, acc 0.765625, prec 0.0166667, recall 0.3
2017-12-10T14:32:20.694642: step 10, loss 22.281, acc 0.625, prec 0.0148515, recall 0.25
2017-12-10T14:32:20.880657: step 11, loss 1.40066, acc 0.59375, prec 0.0131579, recall 0.25
2017-12-10T14:32:21.063636: step 12, loss 13.0415, acc 0.53125, prec 0.0155039, recall 0.285714
2017-12-10T14:32:21.252684: step 13, loss 8.16076, acc 0.5, prec 0.0138408, recall 0.266667
2017-12-10T14:32:21.442858: step 14, loss 3.18345, acc 0.375, prec 0.0121581, recall 0.266667
2017-12-10T14:32:21.625016: step 15, loss 3.35041, acc 0.28125, prec 0.0132979, recall 0.3125
2017-12-10T14:32:21.808204: step 16, loss 4.02284, acc 0.296875, prec 0.014218, recall 0.352941
2017-12-10T14:32:21.992339: step 17, loss 4.03354, acc 0.25, prec 0.014862, recall 0.388889
2017-12-10T14:32:22.179076: step 18, loss 6.01553, acc 0.1875, prec 0.0171429, recall 0.45
2017-12-10T14:32:22.363158: step 19, loss 5.62738, acc 0.15625, prec 0.015544, recall 0.45
2017-12-10T14:32:22.547334: step 20, loss 5.3147, acc 0.15625, prec 0.0173228, recall 0.5
2017-12-10T14:32:22.735933: step 21, loss 5.38611, acc 0.109375, prec 0.015896, recall 0.5
2017-12-10T14:32:22.922935: step 22, loss 7.15199, acc 0.1875, prec 0.0148048, recall 0.478261
2017-12-10T14:32:23.105248: step 23, loss 4.55753, acc 0.234375, prec 0.0151324, recall 0.5
2017-12-10T14:32:23.288352: step 24, loss 5.42935, acc 0.234375, prec 0.0154211, recall 0.52
2017-12-10T14:32:23.477607: step 25, loss 5.78029, acc 0.1875, prec 0.0156425, recall 0.518519
2017-12-10T14:32:23.663540: step 26, loss 4.13297, acc 0.296875, prec 0.0148936, recall 0.518519
2017-12-10T14:32:23.845279: step 27, loss 4.21532, acc 0.203125, prec 0.015121, recall 0.535714
2017-12-10T14:32:24.025378: step 28, loss 4.46025, acc 0.21875, prec 0.0162835, recall 0.566667
2017-12-10T14:32:24.210650: step 29, loss 4.79975, acc 0.25, prec 0.015582, recall 0.548387
2017-12-10T14:32:24.395923: step 30, loss 3.4563, acc 0.296875, prec 0.0158311, recall 0.5625
2017-12-10T14:32:24.587417: step 31, loss 4.50816, acc 0.34375, prec 0.0161017, recall 0.575758
2017-12-10T14:32:24.770642: step 32, loss 6.35499, acc 0.453125, prec 0.0156507, recall 0.558824
2017-12-10T14:32:24.953510: step 33, loss 2.82219, acc 0.375, prec 0.0151515, recall 0.558824
2017-12-10T14:32:25.141827: step 34, loss 6.41098, acc 0.375, prec 0.0146945, recall 0.542857
2017-12-10T14:32:25.330938: step 35, loss 5.81405, acc 0.46875, prec 0.0165538, recall 0.564103
2017-12-10T14:32:25.518725: step 36, loss 2.96955, acc 0.453125, prec 0.0175695, recall 0.585366
2017-12-10T14:32:25.706188: step 37, loss 3.15824, acc 0.359375, prec 0.0170576, recall 0.585366
2017-12-10T14:32:25.890449: step 38, loss 6.38334, acc 0.40625, prec 0.0166205, recall 0.571429
2017-12-10T14:32:26.079660: step 39, loss 2.00746, acc 0.546875, prec 0.0162933, recall 0.571429
2017-12-10T14:32:26.265588: step 40, loss 2.9992, acc 0.359375, prec 0.015852, recall 0.571429
2017-12-10T14:32:26.452760: step 41, loss 2.71861, acc 0.46875, prec 0.0155039, recall 0.571429
2017-12-10T14:32:26.639602: step 42, loss 1.57428, acc 0.609375, prec 0.0158831, recall 0.581395
2017-12-10T14:32:26.824681: step 43, loss 2.02054, acc 0.421875, prec 0.0155183, recall 0.581395
2017-12-10T14:32:27.009252: step 44, loss 2.15671, acc 0.515625, prec 0.0152253, recall 0.581395
2017-12-10T14:32:27.198434: step 45, loss 10.7194, acc 0.4375, prec 0.0154946, recall 0.577778
2017-12-10T14:32:27.386222: step 46, loss 11.9733, acc 0.609375, prec 0.0158544, recall 0.574468
2017-12-10T14:32:27.574748: step 47, loss 2.68534, acc 0.4375, prec 0.0155262, recall 0.574468
2017-12-10T14:32:27.783106: step 48, loss 1.45445, acc 0.5625, prec 0.0152801, recall 0.574468
2017-12-10T14:32:27.973821: step 49, loss 1.58555, acc 0.515625, prec 0.0150167, recall 0.574468
2017-12-10T14:32:28.158414: step 50, loss 4.31338, acc 0.5625, prec 0.0147945, recall 0.5625
2017-12-10T14:32:28.346905: step 51, loss 1.98928, acc 0.625, prec 0.0151351, recall 0.571429
2017-12-10T14:32:28.533108: step 52, loss 1.48512, acc 0.625, prec 0.0154667, recall 0.58
2017-12-10T14:32:28.717670: step 53, loss 2.29586, acc 0.4375, prec 0.0156904, recall 0.588235
2017-12-10T14:32:28.900535: step 54, loss 1.83604, acc 0.578125, prec 0.0154719, recall 0.588235
2017-12-10T14:32:29.090882: step 55, loss 2.01561, acc 0.546875, prec 0.015744, recall 0.596154
2017-12-10T14:32:29.279991: step 56, loss 14.1768, acc 0.640625, prec 0.0155701, recall 0.584906
2017-12-10T14:32:29.470226: step 57, loss 8.14023, acc 0.65625, prec 0.0154076, recall 0.574074
2017-12-10T14:32:29.658046: step 58, loss 3.63736, acc 0.640625, prec 0.0152409, recall 0.563636
2017-12-10T14:32:29.845293: step 59, loss 2.72466, acc 0.609375, prec 0.0155415, recall 0.561404
2017-12-10T14:32:30.035654: step 60, loss 5.43983, acc 0.6875, prec 0.015873, recall 0.559322
2017-12-10T14:32:30.226153: step 61, loss 2.67813, acc 0.46875, prec 0.0160833, recall 0.566667
2017-12-10T14:32:30.414343: step 62, loss 8.60665, acc 0.46875, prec 0.0162942, recall 0.564516
2017-12-10T14:32:30.601749: step 63, loss 15.8144, acc 0.265625, prec 0.0159526, recall 0.555556
2017-12-10T14:32:30.785015: step 64, loss 7.30119, acc 0.421875, prec 0.0156951, recall 0.546875
2017-12-10T14:32:30.970262: step 65, loss 3.81745, acc 0.34375, prec 0.0162709, recall 0.560606
2017-12-10T14:32:31.157795: step 66, loss 3.38406, acc 0.296875, prec 0.0159552, recall 0.560606
2017-12-10T14:32:31.342378: step 67, loss 3.5303, acc 0.328125, prec 0.0156647, recall 0.560606
2017-12-10T14:32:31.533085: step 68, loss 3.17396, acc 0.3125, prec 0.0157873, recall 0.567164
2017-12-10T14:32:31.720357: step 69, loss 4.76848, acc 0.125, prec 0.0154283, recall 0.567164
2017-12-10T14:32:31.903256: step 70, loss 8.0074, acc 0.3125, prec 0.0155564, recall 0.565217
2017-12-10T14:32:32.089391: step 71, loss 12.071, acc 0.28125, prec 0.015674, recall 0.555556
2017-12-10T14:32:32.275289: step 72, loss 4.98443, acc 0.40625, prec 0.01621, recall 0.56
2017-12-10T14:32:32.467596: step 73, loss 5.69947, acc 0.125, prec 0.0162387, recall 0.565789
2017-12-10T14:32:32.649855: step 74, loss 4.26592, acc 0.21875, prec 0.0159377, recall 0.565789
2017-12-10T14:32:32.830592: step 75, loss 5.53811, acc 0.28125, prec 0.0156706, recall 0.565789
2017-12-10T14:32:33.014107: step 76, loss 4.6003, acc 0.21875, prec 0.0157424, recall 0.571429
2017-12-10T14:32:33.197796: step 77, loss 5.81284, acc 0.203125, prec 0.0161517, recall 0.582278
2017-12-10T14:32:33.384676: step 78, loss 5.19526, acc 0.15625, prec 0.0161901, recall 0.5875
2017-12-10T14:32:33.570582: step 79, loss 5.47313, acc 0.21875, prec 0.0162492, recall 0.592593
2017-12-10T14:32:33.753935: step 80, loss 3.9478, acc 0.234375, prec 0.015984, recall 0.592593
2017-12-10T14:32:33.937884: step 81, loss 4.67573, acc 0.21875, prec 0.0157222, recall 0.592593
2017-12-10T14:32:34.124718: step 82, loss 3.24217, acc 0.375, prec 0.0161551, recall 0.60241
2017-12-10T14:32:34.310656: step 83, loss 3.50039, acc 0.296875, prec 0.0162369, recall 0.607143
2017-12-10T14:32:34.498514: step 84, loss 3.12974, acc 0.359375, prec 0.0163368, recall 0.611765
2017-12-10T14:32:34.682458: step 85, loss 3.80951, acc 0.5, prec 0.0164852, recall 0.609195
2017-12-10T14:32:34.873943: step 86, loss 2.2505, acc 0.5, prec 0.0163228, recall 0.609195
2017-12-10T14:32:35.056929: step 87, loss 2.07905, acc 0.5, prec 0.0164634, recall 0.613636
2017-12-10T14:32:35.240938: step 88, loss 2.30428, acc 0.453125, prec 0.0162896, recall 0.613636
2017-12-10T14:32:35.428279: step 89, loss 14.054, acc 0.546875, prec 0.0161532, recall 0.606742
2017-12-10T14:32:35.614315: step 90, loss 2.01165, acc 0.53125, prec 0.0160095, recall 0.606742
2017-12-10T14:32:35.801079: step 91, loss 0.933388, acc 0.734375, prec 0.0159292, recall 0.606742
2017-12-10T14:32:35.984692: step 92, loss 1.19833, acc 0.671875, prec 0.0158311, recall 0.606742
2017-12-10T14:32:36.170000: step 93, loss 11.4595, acc 0.78125, prec 0.015771, recall 0.6
2017-12-10T14:32:36.356557: step 94, loss 0.989837, acc 0.71875, prec 0.0156886, recall 0.6
2017-12-10T14:32:36.540586: step 95, loss 0.702066, acc 0.765625, prec 0.0156205, recall 0.6
2017-12-10T14:32:36.730817: step 96, loss 1.15289, acc 0.640625, prec 0.0155172, recall 0.6
2017-12-10T14:32:36.920376: step 97, loss 1.60618, acc 0.671875, prec 0.0159863, recall 0.608696
2017-12-10T14:32:37.109896: step 98, loss 8.4463, acc 0.796875, prec 0.0159363, recall 0.595745
2017-12-10T14:32:37.301755: step 99, loss 0.943985, acc 0.765625, prec 0.0161473, recall 0.6
2017-12-10T14:32:37.492863: step 100, loss 4.81417, acc 0.78125, prec 0.0163657, recall 0.597938
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_3/1512934336/checkpoints/model-100

2017-12-10T14:32:38.619005: step 101, loss 1.19358, acc 0.734375, prec 0.0162876, recall 0.597938
2017-12-10T14:32:38.805522: step 102, loss 1.24318, acc 0.6875, prec 0.0164712, recall 0.602041
2017-12-10T14:32:38.990860: step 103, loss 0.933429, acc 0.703125, prec 0.0163843, recall 0.602041
2017-12-10T14:32:39.176729: step 104, loss 4.62367, acc 0.609375, prec 0.0162804, recall 0.59
2017-12-10T14:32:39.365665: step 105, loss 4.53127, acc 0.578125, prec 0.0167032, recall 0.592233
2017-12-10T14:32:39.557217: step 106, loss 2.08023, acc 0.640625, prec 0.0168662, recall 0.596154
2017-12-10T14:32:39.746223: step 107, loss 1.66516, acc 0.515625, prec 0.0167251, recall 0.596154
2017-12-10T14:32:39.934342: step 108, loss 2.20759, acc 0.546875, prec 0.0168584, recall 0.6
2017-12-10T14:32:40.119573: step 109, loss 2.11682, acc 0.59375, prec 0.0170032, recall 0.603774
2017-12-10T14:32:40.306186: step 110, loss 2.46559, acc 0.546875, prec 0.0168732, recall 0.603774
2017-12-10T14:32:40.493209: step 111, loss 2.81399, acc 0.46875, prec 0.0169801, recall 0.607477
2017-12-10T14:32:40.684671: step 112, loss 2.24041, acc 0.4375, prec 0.0168219, recall 0.607477
2017-12-10T14:32:40.870853: step 113, loss 4.37637, acc 0.46875, prec 0.0169318, recall 0.605505
2017-12-10T14:32:41.062639: step 114, loss 2.78075, acc 0.46875, prec 0.0167854, recall 0.605505
2017-12-10T14:32:41.248673: step 115, loss 3.8198, acc 0.40625, prec 0.0166289, recall 0.6
2017-12-10T14:32:41.435829: step 116, loss 3.69325, acc 0.53125, prec 0.0165083, recall 0.594595
2017-12-10T14:32:41.622519: step 117, loss 2.72452, acc 0.4375, prec 0.0166047, recall 0.598214
2017-12-10T14:32:41.805610: step 118, loss 13.4482, acc 0.4375, prec 0.0164619, recall 0.59292
2017-12-10T14:32:41.992452: step 119, loss 2.26699, acc 0.390625, prec 0.016545, recall 0.596491
2017-12-10T14:32:42.181056: step 120, loss 3.14664, acc 0.375, prec 0.0163855, recall 0.596491
2017-12-10T14:32:42.371502: step 121, loss 2.38581, acc 0.40625, prec 0.0164717, recall 0.6
2017-12-10T14:32:42.554077: step 122, loss 2.45799, acc 0.484375, prec 0.016343, recall 0.6
2017-12-10T14:32:42.742529: step 123, loss 14.1384, acc 0.484375, prec 0.01622, recall 0.594828
2017-12-10T14:32:42.928066: step 124, loss 3.09525, acc 0.375, prec 0.016298, recall 0.598291
2017-12-10T14:32:43.120183: step 125, loss 1.42185, acc 0.5625, prec 0.0166474, recall 0.605042
2017-12-10T14:32:43.311395: step 126, loss 1.67186, acc 0.515625, prec 0.0165289, recall 0.605042
2017-12-10T14:32:43.496892: step 127, loss 7.58814, acc 0.484375, prec 0.0164084, recall 0.6
2017-12-10T14:32:43.683782: step 128, loss 1.84867, acc 0.484375, prec 0.0162859, recall 0.6
2017-12-10T14:32:43.868637: step 129, loss 1.84254, acc 0.515625, prec 0.0163934, recall 0.603306
2017-12-10T14:32:44.061785: step 130, loss 18.6472, acc 0.59375, prec 0.0165215, recall 0.601626
2017-12-10T14:32:44.253214: step 131, loss 1.96006, acc 0.546875, prec 0.0168514, recall 0.608
2017-12-10T14:32:44.435574: step 132, loss 1.91247, acc 0.5625, prec 0.0169641, recall 0.611111
2017-12-10T14:32:44.622538: step 133, loss 1.7016, acc 0.625, prec 0.0168749, recall 0.611111
2017-12-10T14:32:44.806736: step 134, loss 2.80611, acc 0.609375, prec 0.0167866, recall 0.606299
2017-12-10T14:32:44.990739: step 135, loss 1.3855, acc 0.59375, prec 0.016692, recall 0.606299
2017-12-10T14:32:45.179660: step 136, loss 2.06042, acc 0.546875, prec 0.0167995, recall 0.609375
2017-12-10T14:32:45.365080: step 137, loss 1.66172, acc 0.640625, prec 0.017138, recall 0.615385
2017-12-10T14:32:45.554538: step 138, loss 12.5278, acc 0.703125, prec 0.0172818, recall 0.613636
2017-12-10T14:32:45.740587: step 139, loss 11.7335, acc 0.5625, prec 0.0171865, recall 0.604478
2017-12-10T14:32:45.930951: step 140, loss 8.52057, acc 0.6875, prec 0.0171175, recall 0.6
2017-12-10T14:32:46.123654: step 141, loss 4.72133, acc 0.53125, prec 0.0172197, recall 0.59854
2017-12-10T14:32:46.314100: step 142, loss 2.32062, acc 0.46875, prec 0.0170976, recall 0.59854
2017-12-10T14:32:46.500246: step 143, loss 2.36364, acc 0.515625, prec 0.0169878, recall 0.59854
2017-12-10T14:32:46.680552: step 144, loss 2.15384, acc 0.4375, prec 0.016862, recall 0.59854
2017-12-10T14:32:46.864044: step 145, loss 7.16429, acc 0.3125, prec 0.0167142, recall 0.594203
2017-12-10T14:32:47.049681: step 146, loss 4.60194, acc 0.328125, prec 0.0165724, recall 0.589928
2017-12-10T14:32:47.241132: step 147, loss 2.68745, acc 0.4375, prec 0.0164526, recall 0.589928
2017-12-10T14:32:47.425247: step 148, loss 7.25424, acc 0.421875, prec 0.0163347, recall 0.585714
2017-12-10T14:32:47.609350: step 149, loss 3.58629, acc 0.3125, prec 0.0165811, recall 0.591549
2017-12-10T14:32:47.792762: step 150, loss 5.53668, acc 0.25, prec 0.0164287, recall 0.587413
2017-12-10T14:32:47.980794: step 151, loss 2.97875, acc 0.375, prec 0.0166828, recall 0.593103
2017-12-10T14:32:48.172047: step 152, loss 4.86843, acc 0.203125, prec 0.0165194, recall 0.593103
2017-12-10T14:32:48.355274: step 153, loss 3.3611, acc 0.34375, prec 0.0163872, recall 0.593103
2017-12-10T14:32:48.544875: step 154, loss 3.65232, acc 0.328125, prec 0.016254, recall 0.593103
2017-12-10T14:32:48.730085: step 155, loss 3.57903, acc 0.421875, prec 0.0163258, recall 0.59589
2017-12-10T14:32:48.915972: step 156, loss 3.23667, acc 0.4375, prec 0.0162162, recall 0.59589
2017-12-10T14:32:49.101912: step 157, loss 2.94517, acc 0.375, prec 0.0162782, recall 0.598639
2017-12-10T14:32:49.291610: step 158, loss 2.90109, acc 0.265625, prec 0.0163183, recall 0.601351
2017-12-10T14:32:49.480788: step 159, loss 6.12468, acc 0.390625, prec 0.0162054, recall 0.597315
2017-12-10T14:32:49.665077: step 160, loss 5.51434, acc 0.4375, prec 0.0164587, recall 0.598684
2017-12-10T14:32:49.853040: step 161, loss 2.29394, acc 0.4375, prec 0.0163522, recall 0.598684
2017-12-10T14:32:50.044839: step 162, loss 1.9866, acc 0.609375, prec 0.0166309, recall 0.603896
2017-12-10T14:32:50.232836: step 163, loss 3.84824, acc 0.546875, prec 0.016548, recall 0.6
2017-12-10T14:32:50.420992: step 164, loss 1.86542, acc 0.484375, prec 0.0167993, recall 0.605096
2017-12-10T14:32:50.605979: step 165, loss 1.90237, acc 0.484375, prec 0.0167018, recall 0.605096
2017-12-10T14:32:50.790777: step 166, loss 11.5885, acc 0.46875, prec 0.0169521, recall 0.602484
2017-12-10T14:32:50.979834: step 167, loss 2.32514, acc 0.53125, prec 0.0168637, recall 0.602484
2017-12-10T14:32:51.165271: step 168, loss 2.13928, acc 0.5625, prec 0.0169521, recall 0.604938
2017-12-10T14:32:51.350297: step 169, loss 3.22284, acc 0.34375, prec 0.0168298, recall 0.604938
2017-12-10T14:32:51.538184: step 170, loss 2.34786, acc 0.5, prec 0.0167378, recall 0.604938
2017-12-10T14:32:51.724881: step 171, loss 2.26953, acc 0.515625, prec 0.0166497, recall 0.604938
2017-12-10T14:32:51.911509: step 172, loss 1.8272, acc 0.46875, prec 0.0165541, recall 0.604938
2017-12-10T14:32:52.093591: step 173, loss 2.56976, acc 0.578125, prec 0.0169748, recall 0.612121
2017-12-10T14:32:52.278133: step 174, loss 7.32049, acc 0.484375, prec 0.0168868, recall 0.60479
2017-12-10T14:32:52.466862: step 175, loss 1.87137, acc 0.5625, prec 0.0169717, recall 0.607143
2017-12-10T14:32:52.651591: step 176, loss 1.67806, acc 0.53125, prec 0.0172128, recall 0.611765
2017-12-10T14:32:52.835888: step 177, loss 1.89608, acc 0.515625, prec 0.0174486, recall 0.616279
2017-12-10T14:32:53.019589: step 178, loss 2.31959, acc 0.515625, prec 0.01736, recall 0.616279
2017-12-10T14:32:53.209032: step 179, loss 7.63676, acc 0.59375, prec 0.0172892, recall 0.612717
2017-12-10T14:32:53.398488: step 180, loss 1.63366, acc 0.609375, prec 0.0175382, recall 0.617143
2017-12-10T14:32:53.585636: step 181, loss 2.7915, acc 0.5625, prec 0.0176176, recall 0.619318
2017-12-10T14:32:53.774824: step 182, loss 1.69869, acc 0.5625, prec 0.0178543, recall 0.623595
2017-12-10T14:32:53.960320: step 183, loss 3.8768, acc 0.484375, prec 0.0182312, recall 0.629834
2017-12-10T14:32:54.151431: step 184, loss 2.24857, acc 0.59375, prec 0.0183121, recall 0.631868
2017-12-10T14:32:54.344197: step 185, loss 1.48192, acc 0.609375, prec 0.0183952, recall 0.63388
2017-12-10T14:32:54.532005: step 186, loss 2.06358, acc 0.515625, prec 0.0183052, recall 0.63388
2017-12-10T14:32:54.719234: step 187, loss 2.02538, acc 0.484375, prec 0.0185185, recall 0.637838
2017-12-10T14:32:54.906628: step 188, loss 1.99723, acc 0.515625, prec 0.0187354, recall 0.641711
2017-12-10T14:32:55.097881: step 189, loss 1.54428, acc 0.59375, prec 0.0188122, recall 0.643617
2017-12-10T14:32:55.285605: step 190, loss 1.98265, acc 0.578125, prec 0.0187335, recall 0.643617
2017-12-10T14:32:55.477645: step 191, loss 2.08626, acc 0.421875, prec 0.0186268, recall 0.643617
2017-12-10T14:32:55.670452: step 192, loss 4.77326, acc 0.515625, prec 0.0185412, recall 0.640212
2017-12-10T14:32:55.855405: step 193, loss 13.5214, acc 0.53125, prec 0.0184592, recall 0.636842
2017-12-10T14:32:56.044839: step 194, loss 1.65523, acc 0.625, prec 0.0183919, recall 0.636842
2017-12-10T14:32:56.230992: step 195, loss 6.10136, acc 0.5625, prec 0.0183167, recall 0.633508
2017-12-10T14:32:56.416912: step 196, loss 1.66714, acc 0.53125, prec 0.0182339, recall 0.633508
2017-12-10T14:32:56.602466: step 197, loss 3.57487, acc 0.578125, prec 0.0181627, recall 0.630208
2017-12-10T14:32:56.788641: step 198, loss 1.65823, acc 0.578125, prec 0.0180894, recall 0.630208
2017-12-10T14:32:56.976744: step 199, loss 1.87794, acc 0.46875, prec 0.0179979, recall 0.630208
2017-12-10T14:32:57.169377: step 200, loss 2.07494, acc 0.5, prec 0.018058, recall 0.632124
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_3/1512934336/checkpoints/model-200

2017-12-10T14:32:58.302267: step 201, loss 1.43132, acc 0.625, prec 0.0179941, recall 0.632124
2017-12-10T14:32:58.490772: step 202, loss 1.89598, acc 0.640625, prec 0.0179333, recall 0.632124
2017-12-10T14:32:58.673928: step 203, loss 1.57897, acc 0.625, prec 0.0180141, recall 0.634021
2017-12-10T14:32:58.862337: step 204, loss 1.78114, acc 0.609375, prec 0.0179483, recall 0.634021
2017-12-10T14:32:59.063081: step 205, loss 2.05664, acc 0.5625, prec 0.0181607, recall 0.637755
2017-12-10T14:32:59.255588: step 206, loss 5.53647, acc 0.5625, prec 0.0182318, recall 0.636364
2017-12-10T14:32:59.446070: step 207, loss 1.29247, acc 0.671875, prec 0.0181766, recall 0.636364
2017-12-10T14:32:59.628551: step 208, loss 4.31682, acc 0.6875, prec 0.0182681, recall 0.635
2017-12-10T14:32:59.818184: step 209, loss 1.11659, acc 0.640625, prec 0.0182079, recall 0.635
2017-12-10T14:33:00.005170: step 210, loss 13.4369, acc 0.65625, prec 0.0181558, recall 0.628713
2017-12-10T14:33:00.192882: step 211, loss 4.39896, acc 0.625, prec 0.0182362, recall 0.627451
2017-12-10T14:33:00.386875: step 212, loss 2.69037, acc 0.640625, prec 0.0183187, recall 0.626214
2017-12-10T14:33:00.577624: step 213, loss 1.83132, acc 0.59375, prec 0.018529, recall 0.629808
2017-12-10T14:33:00.760024: step 214, loss 2.97132, acc 0.5, prec 0.0185837, recall 0.631579
2017-12-10T14:33:00.946487: step 215, loss 1.91998, acc 0.578125, prec 0.0185133, recall 0.631579
2017-12-10T14:33:01.136880: step 216, loss 4.63033, acc 0.5, prec 0.0187072, recall 0.632075
2017-12-10T14:33:01.327642: step 217, loss 2.86284, acc 0.328125, prec 0.0187318, recall 0.633803
2017-12-10T14:33:01.517999: step 218, loss 2.21983, acc 0.46875, prec 0.0187793, recall 0.635514
2017-12-10T14:33:01.707590: step 219, loss 3.41951, acc 0.421875, prec 0.0188187, recall 0.637209
2017-12-10T14:33:01.893401: step 220, loss 2.51451, acc 0.453125, prec 0.0189969, recall 0.640553
2017-12-10T14:33:02.079122: step 221, loss 3.46327, acc 0.3125, prec 0.0190166, recall 0.642202
2017-12-10T14:33:02.267993: step 222, loss 5.08678, acc 0.3125, prec 0.0189061, recall 0.639269
2017-12-10T14:33:02.452405: step 223, loss 3.64797, acc 0.46875, prec 0.0189516, recall 0.640909
2017-12-10T14:33:02.639619: step 224, loss 3.39621, acc 0.34375, prec 0.0189763, recall 0.642534
2017-12-10T14:33:02.830415: step 225, loss 2.82072, acc 0.390625, prec 0.0190084, recall 0.644144
2017-12-10T14:33:03.018844: step 226, loss 8.61633, acc 0.40625, prec 0.0191748, recall 0.644444
2017-12-10T14:33:03.210531: step 227, loss 2.99671, acc 0.4375, prec 0.0192131, recall 0.646018
2017-12-10T14:33:03.397002: step 228, loss 2.63144, acc 0.390625, prec 0.019115, recall 0.646018
2017-12-10T14:33:03.584784: step 229, loss 2.21102, acc 0.53125, prec 0.0190402, recall 0.646018
2017-12-10T14:33:03.768869: step 230, loss 2.33367, acc 0.4375, prec 0.0189512, recall 0.646018
2017-12-10T14:33:03.953227: step 231, loss 2.60065, acc 0.484375, prec 0.0191239, recall 0.649123
2017-12-10T14:33:04.141897: step 232, loss 1.68162, acc 0.5625, prec 0.0191813, recall 0.650655
2017-12-10T14:33:04.327831: step 233, loss 1.88113, acc 0.484375, prec 0.0191001, recall 0.650655
2017-12-10T14:33:04.517442: step 234, loss 1.60945, acc 0.53125, prec 0.0190269, recall 0.650655
2017-12-10T14:33:04.701306: step 235, loss 2.03638, acc 0.609375, prec 0.0189664, recall 0.650655
2017-12-10T14:33:04.885090: step 236, loss 1.20605, acc 0.65625, prec 0.0189134, recall 0.650655
2017-12-10T14:33:05.069017: step 237, loss 1.50105, acc 0.65625, prec 0.0189849, recall 0.652174
2017-12-10T14:33:05.259838: step 238, loss 0.874719, acc 0.71875, prec 0.0190657, recall 0.65368
2017-12-10T14:33:05.445856: step 239, loss 1.63828, acc 0.765625, prec 0.0192768, recall 0.656652
2017-12-10T14:33:05.636243: step 240, loss 2.46222, acc 0.703125, prec 0.0192332, recall 0.653846
2017-12-10T14:33:05.826583: step 241, loss 0.96945, acc 0.796875, prec 0.0193249, recall 0.655319
2017-12-10T14:33:06.016269: step 242, loss 0.560505, acc 0.875, prec 0.0193055, recall 0.655319
2017-12-10T14:33:06.202843: step 243, loss 0.398724, acc 0.859375, prec 0.0192837, recall 0.655319
2017-12-10T14:33:06.395038: step 244, loss 4.68234, acc 0.828125, prec 0.0193823, recall 0.654008
2017-12-10T14:33:06.583971: step 245, loss 1.53227, acc 0.828125, prec 0.0193581, recall 0.65126
2017-12-10T14:33:06.776443: step 246, loss 2.12381, acc 0.875, prec 0.0193412, recall 0.648536
2017-12-10T14:33:06.968232: step 247, loss 0.876101, acc 0.875, prec 0.0194441, recall 0.65
2017-12-10T14:33:07.153465: step 248, loss 0.443167, acc 0.828125, prec 0.0194175, recall 0.65
2017-12-10T14:33:07.340055: step 249, loss 7.89143, acc 0.765625, prec 0.019508, recall 0.646091
2017-12-10T14:33:07.531552: step 250, loss 1.09467, acc 0.71875, prec 0.019586, recall 0.647541
2017-12-10T14:33:07.720736: step 251, loss 0.950861, acc 0.6875, prec 0.0196588, recall 0.64898
2017-12-10T14:33:07.905872: step 252, loss 1.25606, acc 0.75, prec 0.0199827, recall 0.653226
2017-12-10T14:33:08.111705: step 253, loss 1.45564, acc 0.578125, prec 0.0199164, recall 0.653226
2017-12-10T14:33:08.300662: step 254, loss 1.56895, acc 0.65625, prec 0.0198627, recall 0.653226
2017-12-10T14:33:08.488702: step 255, loss 1.69245, acc 0.609375, prec 0.019802, recall 0.653226
2017-12-10T14:33:08.675838: step 256, loss 1.70991, acc 0.515625, prec 0.0198466, recall 0.654619
2017-12-10T14:33:08.867594: step 257, loss 1.72985, acc 0.484375, prec 0.0197672, recall 0.654619
2017-12-10T14:33:09.052992: step 258, loss 1.53054, acc 0.546875, prec 0.0196979, recall 0.654619
2017-12-10T14:33:09.244142: step 259, loss 1.97855, acc 0.53125, prec 0.0197448, recall 0.656
2017-12-10T14:33:09.432993: step 260, loss 1.05744, acc 0.65625, prec 0.0196926, recall 0.656
2017-12-10T14:33:09.621722: step 261, loss 3.9379, acc 0.5, prec 0.0197368, recall 0.654762
2017-12-10T14:33:09.812047: step 262, loss 1.90402, acc 0.546875, prec 0.0196686, recall 0.654762
2017-12-10T14:33:09.997678: step 263, loss 23.6561, acc 0.671875, prec 0.0198573, recall 0.652344
2017-12-10T14:33:10.193714: step 264, loss 4.64623, acc 0.59375, prec 0.0197985, recall 0.649805
2017-12-10T14:33:10.384678: step 265, loss 8.14522, acc 0.65625, prec 0.0198652, recall 0.648649
2017-12-10T14:33:10.573566: step 266, loss 8.636, acc 0.546875, prec 0.0197996, recall 0.646154
2017-12-10T14:33:10.764731: step 267, loss 3.56036, acc 0.328125, prec 0.0196998, recall 0.646154
2017-12-10T14:33:10.952410: step 268, loss 2.97024, acc 0.328125, prec 0.019601, recall 0.646154
2017-12-10T14:33:11.137344: step 269, loss 8.70322, acc 0.25, prec 0.0196078, recall 0.645038
2017-12-10T14:33:11.324968: step 270, loss 4.31687, acc 0.328125, prec 0.0196237, recall 0.646388
2017-12-10T14:33:11.510825: step 271, loss 3.89355, acc 0.328125, prec 0.0195268, recall 0.646388
2017-12-10T14:33:11.700658: step 272, loss 4.26784, acc 0.265625, prec 0.0196459, recall 0.649057
2017-12-10T14:33:11.889636: step 273, loss 4.60443, acc 0.1875, prec 0.0195299, recall 0.649057
2017-12-10T14:33:12.071588: step 274, loss 5.15915, acc 0.265625, prec 0.0196477, recall 0.651685
2017-12-10T14:33:12.258715: step 275, loss 4.90021, acc 0.296875, prec 0.0198787, recall 0.655556
2017-12-10T14:33:12.448181: step 276, loss 5.1096, acc 0.234375, prec 0.0198794, recall 0.656827
2017-12-10T14:33:12.631351: step 277, loss 4.66798, acc 0.1875, prec 0.0197646, recall 0.656827
2017-12-10T14:33:12.818912: step 278, loss 4.19061, acc 0.328125, prec 0.0196707, recall 0.656827
2017-12-10T14:33:13.005991: step 279, loss 4.02605, acc 0.25, prec 0.0195669, recall 0.656827
2017-12-10T14:33:13.189909: step 280, loss 3.51616, acc 0.40625, prec 0.0197001, recall 0.659341
2017-12-10T14:33:13.374460: step 281, loss 3.51024, acc 0.28125, prec 0.0198149, recall 0.661818
2017-12-10T14:33:13.563057: step 282, loss 3.76236, acc 0.359375, prec 0.0199393, recall 0.66426
2017-12-10T14:33:13.746511: step 283, loss 2.81698, acc 0.4375, prec 0.0199676, recall 0.665468
2017-12-10T14:33:13.930753: step 284, loss 2.62802, acc 0.46875, prec 0.0198946, recall 0.665468
2017-12-10T14:33:14.122594: step 285, loss 1.98578, acc 0.546875, prec 0.0198328, recall 0.665468
2017-12-10T14:33:14.305172: step 286, loss 9.30223, acc 0.578125, prec 0.0198824, recall 0.664286
2017-12-10T14:33:14.497281: step 287, loss 8.42024, acc 0.625, prec 0.0198337, recall 0.661922
2017-12-10T14:33:14.688190: step 288, loss 4.05802, acc 0.671875, prec 0.0197914, recall 0.659574
2017-12-10T14:33:14.878948: step 289, loss 5.15312, acc 0.6875, prec 0.0198556, recall 0.658451
2017-12-10T14:33:15.067774: step 290, loss 1.39596, acc 0.640625, prec 0.0198072, recall 0.658451
2017-12-10T14:33:15.258822: step 291, loss 11.2032, acc 0.59375, prec 0.019757, recall 0.653846
2017-12-10T14:33:15.445346: step 292, loss 1.87684, acc 0.53125, prec 0.0196946, recall 0.653846
2017-12-10T14:33:15.636022: step 293, loss 2.24198, acc 0.53125, prec 0.0199412, recall 0.657439
2017-12-10T14:33:15.823038: step 294, loss 3.15549, acc 0.4375, prec 0.0198662, recall 0.657439
2017-12-10T14:33:16.007255: step 295, loss 3.28734, acc 0.375, prec 0.0197834, recall 0.657439
2017-12-10T14:33:16.194966: step 296, loss 2.73378, acc 0.4375, prec 0.0197095, recall 0.657439
2017-12-10T14:33:16.383585: step 297, loss 2.81607, acc 0.46875, prec 0.0196403, recall 0.657439
2017-12-10T14:33:16.571459: step 298, loss 2.099, acc 0.515625, prec 0.0196785, recall 0.658621
2017-12-10T14:33:16.759179: step 299, loss 2.81016, acc 0.5, prec 0.0196139, recall 0.658621
2017-12-10T14:33:16.942484: step 300, loss 1.88078, acc 0.578125, prec 0.01966, recall 0.659794
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_3/1512934336/checkpoints/model-300

2017-12-10T14:33:18.129222: step 301, loss 1.73147, acc 0.609375, prec 0.0196098, recall 0.659794
2017-12-10T14:33:18.320837: step 302, loss 1.98211, acc 0.5625, prec 0.0196538, recall 0.660959
2017-12-10T14:33:18.508338: step 303, loss 3.23948, acc 0.671875, prec 0.0196138, recall 0.658703
2017-12-10T14:33:18.693971: step 304, loss 2.31983, acc 0.625, prec 0.0197648, recall 0.661017
2017-12-10T14:33:18.884752: step 305, loss 1.78212, acc 0.609375, prec 0.019814, recall 0.662162
2017-12-10T14:33:19.076389: step 306, loss 9.72244, acc 0.734375, prec 0.0198809, recall 0.661074
2017-12-10T14:33:19.265247: step 307, loss 1.53637, acc 0.625, prec 0.0199315, recall 0.662207
2017-12-10T14:33:19.456133: step 308, loss 1.01993, acc 0.6875, prec 0.0198915, recall 0.662207
2017-12-10T14:33:19.645078: step 309, loss 1.64978, acc 0.59375, prec 0.0198397, recall 0.662207
2017-12-10T14:33:19.831601: step 310, loss 1.80595, acc 0.5625, prec 0.0198821, recall 0.663333
2017-12-10T14:33:20.019993: step 311, loss 4.8835, acc 0.640625, prec 0.0198385, recall 0.66113
2017-12-10T14:33:20.213358: step 312, loss 0.847668, acc 0.703125, prec 0.019801, recall 0.66113
2017-12-10T14:33:20.404678: step 313, loss 1.50849, acc 0.703125, prec 0.019861, recall 0.662252
2017-12-10T14:33:20.596899: step 314, loss 1.06488, acc 0.734375, prec 0.0198275, recall 0.662252
2017-12-10T14:33:20.804783: step 315, loss 1.49895, acc 0.609375, prec 0.0197785, recall 0.662252
2017-12-10T14:33:20.991532: step 316, loss 1.18853, acc 0.703125, prec 0.0197414, recall 0.662252
2017-12-10T14:33:21.181970: step 317, loss 1.10843, acc 0.78125, prec 0.0198108, recall 0.663366
2017-12-10T14:33:21.409458: step 318, loss 3.60723, acc 0.65625, prec 0.0197698, recall 0.661184
2017-12-10T14:33:21.602394: step 319, loss 1.33537, acc 0.65625, prec 0.0197272, recall 0.661184
2017-12-10T14:33:21.791158: step 320, loss 1.27754, acc 0.703125, prec 0.0196904, recall 0.661184
2017-12-10T14:33:21.996370: step 321, loss 3.57267, acc 0.71875, prec 0.0197536, recall 0.660131
2017-12-10T14:33:22.190192: step 322, loss 0.866194, acc 0.71875, prec 0.0197189, recall 0.660131
2017-12-10T14:33:22.382724: step 323, loss 1.61705, acc 0.609375, prec 0.0196709, recall 0.660131
2017-12-10T14:33:22.571947: step 324, loss 6.54078, acc 0.71875, prec 0.0196383, recall 0.65798
2017-12-10T14:33:22.764065: step 325, loss 1.69442, acc 0.65625, prec 0.0196915, recall 0.659091
2017-12-10T14:33:22.953632: step 326, loss 1.67786, acc 0.546875, prec 0.0196363, recall 0.659091
2017-12-10T14:33:23.145727: step 327, loss 1.63287, acc 0.609375, prec 0.0196835, recall 0.660194
2017-12-10T14:33:23.332727: step 328, loss 9.72404, acc 0.640625, prec 0.0197362, recall 0.659164
2017-12-10T14:33:23.525540: step 329, loss 1.17238, acc 0.65625, prec 0.0196945, recall 0.659164
2017-12-10T14:33:23.718284: step 330, loss 1.18539, acc 0.71875, prec 0.0199425, recall 0.66242
2017-12-10T14:33:23.903067: step 331, loss 8.67509, acc 0.53125, prec 0.0198872, recall 0.660317
2017-12-10T14:33:24.093782: step 332, loss 5.78565, acc 0.609375, prec 0.0199351, recall 0.659306
2017-12-10T14:33:24.282838: step 333, loss 1.99139, acc 0.484375, prec 0.0198726, recall 0.659306
2017-12-10T14:33:24.466129: step 334, loss 4.36706, acc 0.390625, prec 0.0198939, recall 0.658307
2017-12-10T14:33:24.655190: step 335, loss 2.81875, acc 0.421875, prec 0.0198244, recall 0.658307
2017-12-10T14:33:24.845408: step 336, loss 2.43723, acc 0.5625, prec 0.0197721, recall 0.658307
2017-12-10T14:33:25.033833: step 337, loss 2.781, acc 0.40625, prec 0.0197017, recall 0.658307
2017-12-10T14:33:25.217644: step 338, loss 2.87507, acc 0.421875, prec 0.0198168, recall 0.660436
2017-12-10T14:33:25.405188: step 339, loss 2.91742, acc 0.390625, prec 0.0198361, recall 0.661491
2017-12-10T14:33:25.588480: step 340, loss 3.38447, acc 0.375, prec 0.0197625, recall 0.661491
2017-12-10T14:33:25.777090: step 341, loss 2.90707, acc 0.4375, prec 0.019878, recall 0.66358
2017-12-10T14:33:25.963744: step 342, loss 2.7345, acc 0.453125, prec 0.0198138, recall 0.66358
2017-12-10T14:33:26.152591: step 343, loss 1.93064, acc 0.625, prec 0.0197701, recall 0.66358
2017-12-10T14:33:26.341129: step 344, loss 2.10481, acc 0.46875, prec 0.0197983, recall 0.664615
2017-12-10T14:33:26.531690: step 345, loss 2.56595, acc 0.46875, prec 0.0197368, recall 0.664615
2017-12-10T14:33:26.719798: step 346, loss 1.86139, acc 0.546875, prec 0.0196847, recall 0.664615
2017-12-10T14:33:26.907306: step 347, loss 4.50897, acc 0.609375, prec 0.0199091, recall 0.665653
2017-12-10T14:33:27.097741: step 348, loss 1.69993, acc 0.59375, prec 0.0198621, recall 0.665653
2017-12-10T14:33:27.285809: step 349, loss 0.967179, acc 0.71875, prec 0.0199185, recall 0.666667
2017-12-10T14:33:27.470872: step 350, loss 1.23202, acc 0.71875, prec 0.0198861, recall 0.666667
2017-12-10T14:33:27.658296: step 351, loss 1.30865, acc 0.65625, prec 0.0198466, recall 0.666667
2017-12-10T14:33:27.845094: step 352, loss 0.65354, acc 0.828125, prec 0.019827, recall 0.666667
2017-12-10T14:33:28.034672: step 353, loss 1.01247, acc 0.75, prec 0.0197984, recall 0.666667
2017-12-10T14:33:28.219266: step 354, loss 0.891976, acc 0.796875, prec 0.0199515, recall 0.668675
2017-12-10T14:33:28.407382: step 355, loss 1.025, acc 0.765625, prec 0.0199246, recall 0.668675
2017-12-10T14:33:28.594696: step 356, loss 2.2223, acc 0.65625, prec 0.0199731, recall 0.66967
2017-12-10T14:33:28.779448: step 357, loss 12.8515, acc 0.65625, prec 0.0199374, recall 0.665672
2017-12-10T14:33:28.973869: step 358, loss 1.3503, acc 0.65625, prec 0.0198983, recall 0.665672
2017-12-10T14:33:29.170229: step 359, loss 9.5781, acc 0.65625, prec 0.0198611, recall 0.66369
2017-12-10T14:33:29.358331: step 360, loss 1.32297, acc 0.671875, prec 0.0199982, recall 0.66568
2017-12-10T14:33:29.543715: step 361, loss 1.03581, acc 0.71875, prec 0.0200532, recall 0.666667
2017-12-10T14:33:29.733048: step 362, loss 1.71736, acc 0.5, prec 0.0199965, recall 0.666667
2017-12-10T14:33:29.921268: step 363, loss 1.37665, acc 0.640625, prec 0.0200424, recall 0.667647
2017-12-10T14:33:30.110921: step 364, loss 2.01688, acc 0.609375, prec 0.0200846, recall 0.668622
2017-12-10T14:33:30.299915: step 365, loss 0.909743, acc 0.734375, prec 0.0202269, recall 0.670554
2017-12-10T14:33:30.493189: step 366, loss 1.40794, acc 0.6875, prec 0.0202774, recall 0.671512
2017-12-10T14:33:30.682920: step 367, loss 1.51388, acc 0.6875, prec 0.0202419, recall 0.671512
2017-12-10T14:33:30.872963: step 368, loss 1.04838, acc 0.640625, prec 0.0202011, recall 0.671512
2017-12-10T14:33:31.065618: step 369, loss 2.48644, acc 0.609375, prec 0.0201588, recall 0.669565
2017-12-10T14:33:31.258962: step 370, loss 0.848667, acc 0.71875, prec 0.0201272, recall 0.669565
2017-12-10T14:33:31.445236: step 371, loss 0.904331, acc 0.703125, prec 0.0200939, recall 0.669565
2017-12-10T14:33:31.633837: step 372, loss 2.2565, acc 0.65625, prec 0.0202257, recall 0.67147
2017-12-10T14:33:31.823325: step 373, loss 1.0287, acc 0.671875, prec 0.0202738, recall 0.672414
2017-12-10T14:33:32.011560: step 374, loss 11.9538, acc 0.71875, prec 0.020244, recall 0.670487
2017-12-10T14:33:32.203471: step 375, loss 1.38847, acc 0.609375, prec 0.0202003, recall 0.670487
2017-12-10T14:33:32.387964: step 376, loss 4.75309, acc 0.65625, prec 0.0201637, recall 0.668571
2017-12-10T14:33:32.581782: step 377, loss 12.9864, acc 0.625, prec 0.0202923, recall 0.668555
2017-12-10T14:33:32.767618: step 378, loss 6.23933, acc 0.5625, prec 0.0203311, recall 0.66573
2017-12-10T14:33:32.962144: step 379, loss 2.38728, acc 0.46875, prec 0.0203558, recall 0.666667
2017-12-10T14:33:33.144830: step 380, loss 2.98156, acc 0.5, prec 0.0203002, recall 0.666667
2017-12-10T14:33:33.332077: step 381, loss 2.67603, acc 0.4375, prec 0.0202381, recall 0.666667
2017-12-10T14:33:33.517569: step 382, loss 2.71178, acc 0.4375, prec 0.0201763, recall 0.666667
2017-12-10T14:33:33.704732: step 383, loss 2.14528, acc 0.515625, prec 0.0203719, recall 0.669444
2017-12-10T14:33:33.889921: step 384, loss 2.82662, acc 0.5, prec 0.020317, recall 0.669444
2017-12-10T14:33:34.081095: step 385, loss 2.70928, acc 0.4375, prec 0.0202555, recall 0.669444
2017-12-10T14:33:34.266717: step 386, loss 2.97094, acc 0.359375, prec 0.0201859, recall 0.669444
2017-12-10T14:33:34.450354: step 387, loss 2.64949, acc 0.515625, prec 0.0202155, recall 0.67036
2017-12-10T14:33:34.640741: step 388, loss 2.33929, acc 0.421875, prec 0.0202348, recall 0.671271
2017-12-10T14:33:34.825539: step 389, loss 2.51795, acc 0.546875, prec 0.0201861, recall 0.671271
2017-12-10T14:33:35.010906: step 390, loss 2.66922, acc 0.484375, prec 0.0202121, recall 0.672176
2017-12-10T14:33:35.195110: step 391, loss 9.31347, acc 0.515625, prec 0.020162, recall 0.67033
2017-12-10T14:33:35.380140: step 392, loss 1.71237, acc 0.59375, prec 0.0201187, recall 0.67033
2017-12-10T14:33:35.567024: step 393, loss 5.6603, acc 0.578125, prec 0.0202369, recall 0.6703
2017-12-10T14:33:35.759528: step 394, loss 1.43998, acc 0.625, prec 0.0202775, recall 0.671196
2017-12-10T14:33:35.948615: step 395, loss 1.46098, acc 0.578125, prec 0.0202326, recall 0.671196
2017-12-10T14:33:36.136894: step 396, loss 0.906519, acc 0.75, prec 0.0202062, recall 0.671196
2017-12-10T14:33:36.320152: step 397, loss 1.04998, acc 0.671875, prec 0.0201715, recall 0.671196
2017-12-10T14:33:36.507242: step 398, loss 1.76185, acc 0.703125, prec 0.0203, recall 0.672973
2017-12-10T14:33:36.697639: step 399, loss 1.29375, acc 0.625, prec 0.0204995, recall 0.675603
2017-12-10T14:33:36.883232: step 400, loss 1.09897, acc 0.671875, prec 0.0204645, recall 0.675603
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_3/1512934336/checkpoints/model-400

2017-12-10T14:33:37.999304: step 401, loss 1.23761, acc 0.734375, prec 0.0204363, recall 0.675603
2017-12-10T14:33:38.190656: step 402, loss 0.661753, acc 0.765625, prec 0.0204115, recall 0.675603
2017-12-10T14:33:38.387348: step 403, loss 8.50096, acc 0.65625, prec 0.0203768, recall 0.673797
2017-12-10T14:33:38.578903: step 404, loss 1.21562, acc 0.65625, prec 0.0204197, recall 0.674667
2017-12-10T14:33:38.769516: step 405, loss 3.39849, acc 0.828125, prec 0.0204822, recall 0.67374
2017-12-10T14:33:38.963632: step 406, loss 10.3809, acc 0.78125, prec 0.0204608, recall 0.671958
2017-12-10T14:33:39.156549: step 407, loss 0.921762, acc 0.734375, prec 0.0205116, recall 0.672823
2017-12-10T14:33:39.351871: step 408, loss 1.03673, acc 0.734375, prec 0.0205622, recall 0.673684
2017-12-10T14:33:39.538777: step 409, loss 9.33278, acc 0.640625, prec 0.020526, recall 0.671916
2017-12-10T14:33:39.730968: step 410, loss 6.58599, acc 0.515625, prec 0.0204767, recall 0.670157
2017-12-10T14:33:39.920739: step 411, loss 1.59631, acc 0.703125, prec 0.0205239, recall 0.671018
2017-12-10T14:33:40.114718: step 412, loss 1.26054, acc 0.65625, prec 0.0204879, recall 0.671018
2017-12-10T14:33:40.302883: step 413, loss 1.67337, acc 0.625, prec 0.0205267, recall 0.671875
2017-12-10T14:33:40.495909: step 414, loss 1.69419, acc 0.59375, prec 0.0205621, recall 0.672727
2017-12-10T14:33:40.683581: step 415, loss 4.81134, acc 0.546875, prec 0.0206716, recall 0.67268
2017-12-10T14:33:40.873301: step 416, loss 1.89062, acc 0.609375, prec 0.0207856, recall 0.674359
2017-12-10T14:33:41.060233: step 417, loss 2.05039, acc 0.546875, prec 0.0208153, recall 0.675192
2017-12-10T14:33:41.245114: step 418, loss 4.39237, acc 0.578125, prec 0.0209268, recall 0.675127
2017-12-10T14:33:41.431326: step 419, loss 2.60645, acc 0.46875, prec 0.0209477, recall 0.675949
2017-12-10T14:33:41.613852: step 420, loss 1.70845, acc 0.578125, prec 0.0209035, recall 0.675949
2017-12-10T14:33:41.799820: step 421, loss 4.6179, acc 0.46875, prec 0.0208496, recall 0.674242
2017-12-10T14:33:41.984154: step 422, loss 3.87309, acc 0.421875, prec 0.0208658, recall 0.675063
2017-12-10T14:33:42.170450: step 423, loss 2.36431, acc 0.546875, prec 0.0208948, recall 0.675879
2017-12-10T14:33:42.355707: step 424, loss 2.44575, acc 0.546875, prec 0.0209237, recall 0.676692
2017-12-10T14:33:42.543254: step 425, loss 2.08611, acc 0.5625, prec 0.0210298, recall 0.678304
2017-12-10T14:33:42.731296: step 426, loss 3.21268, acc 0.421875, prec 0.0210453, recall 0.679105
2017-12-10T14:33:42.921974: step 427, loss 3.07637, acc 0.390625, prec 0.0210575, recall 0.679901
2017-12-10T14:33:43.111097: step 428, loss 1.96238, acc 0.4375, prec 0.0210744, recall 0.680693
2017-12-10T14:33:43.293640: step 429, loss 2.63763, acc 0.5625, prec 0.021179, recall 0.682266
2017-12-10T14:33:43.480069: step 430, loss 1.99711, acc 0.609375, prec 0.021288, recall 0.683824
2017-12-10T14:33:43.669791: step 431, loss 1.90066, acc 0.578125, prec 0.0212442, recall 0.683824
2017-12-10T14:33:43.856587: step 432, loss 2.41508, acc 0.5, prec 0.0211926, recall 0.683824
2017-12-10T14:33:44.047537: step 433, loss 1.59864, acc 0.5625, prec 0.0212218, recall 0.684597
2017-12-10T14:33:44.234485: step 434, loss 1.53067, acc 0.625, prec 0.0211832, recall 0.684597
2017-12-10T14:33:44.419988: step 435, loss 0.802069, acc 0.765625, prec 0.0211592, recall 0.684597
2017-12-10T14:33:44.608152: step 436, loss 2.14772, acc 0.625, prec 0.0211948, recall 0.685366
2017-12-10T14:33:44.796692: step 437, loss 1.74661, acc 0.6875, prec 0.0212365, recall 0.686131
2017-12-10T14:33:44.984886: step 438, loss 0.601051, acc 0.84375, prec 0.0212206, recall 0.686131
2017-12-10T14:33:45.178266: step 439, loss 0.765502, acc 0.765625, prec 0.0212702, recall 0.686893
2017-12-10T14:33:45.368777: step 440, loss 3.71074, acc 0.765625, prec 0.0212478, recall 0.68523
2017-12-10T14:33:45.556565: step 441, loss 3.38948, acc 0.71875, prec 0.0212208, recall 0.683575
2017-12-10T14:33:45.741292: step 442, loss 0.844098, acc 0.78125, prec 0.0212718, recall 0.684337
2017-12-10T14:33:45.927588: step 443, loss 2.38291, acc 0.734375, prec 0.0213196, recall 0.683453
2017-12-10T14:33:46.116289: step 444, loss 1.45761, acc 0.78125, prec 0.0213704, recall 0.684211
2017-12-10T14:33:46.306788: step 445, loss 0.968057, acc 0.765625, prec 0.0214195, recall 0.684964
2017-12-10T14:33:46.496371: step 446, loss 9.17365, acc 0.6875, prec 0.0213892, recall 0.683333
2017-12-10T14:33:46.683207: step 447, loss 1.33714, acc 0.6875, prec 0.0214302, recall 0.684085
2017-12-10T14:33:46.867422: step 448, loss 6.15033, acc 0.65625, prec 0.0215421, recall 0.683962
2017-12-10T14:33:47.060158: step 449, loss 6.86861, acc 0.625, prec 0.0216505, recall 0.683841
2017-12-10T14:33:47.249255: step 450, loss 2.2333, acc 0.71875, prec 0.0216941, recall 0.684579
2017-12-10T14:33:47.444767: step 451, loss 3.9341, acc 0.53125, prec 0.0216476, recall 0.682984
2017-12-10T14:33:47.634013: step 452, loss 2.31424, acc 0.53125, prec 0.0216718, recall 0.683721
2017-12-10T14:33:47.824303: step 453, loss 2.41673, acc 0.5625, prec 0.0216992, recall 0.684455
2017-12-10T14:33:48.010617: step 454, loss 2.11542, acc 0.546875, prec 0.021653, recall 0.684455
2017-12-10T14:33:48.198182: step 455, loss 2.36005, acc 0.484375, prec 0.0216723, recall 0.685185
2017-12-10T14:33:48.384593: step 456, loss 3.89819, acc 0.296875, prec 0.0216011, recall 0.685185
2017-12-10T14:33:48.569858: step 457, loss 2.97275, acc 0.4375, prec 0.0216869, recall 0.686636
2017-12-10T14:33:48.755412: step 458, loss 3.18377, acc 0.421875, prec 0.0216997, recall 0.687356
2017-12-10T14:33:48.941310: step 459, loss 3.49798, acc 0.40625, prec 0.0217108, recall 0.688073
2017-12-10T14:33:49.128219: step 460, loss 2.45448, acc 0.46875, prec 0.0216575, recall 0.688073
2017-12-10T14:33:49.317460: step 461, loss 2.93463, acc 0.421875, prec 0.0215998, recall 0.688073
2017-12-10T14:33:49.506194: step 462, loss 2.36013, acc 0.515625, prec 0.021622, recall 0.688787
2017-12-10T14:33:49.695598: step 463, loss 2.60512, acc 0.453125, prec 0.0215678, recall 0.688787
2017-12-10T14:33:49.881894: step 464, loss 1.98159, acc 0.53125, prec 0.0215215, recall 0.688787
2017-12-10T14:33:50.067791: step 465, loss 2.05441, acc 0.578125, prec 0.0214801, recall 0.688787
2017-12-10T14:33:50.253577: step 466, loss 1.07563, acc 0.6875, prec 0.0214494, recall 0.688787
2017-12-10T14:33:50.441799: step 467, loss 1.28945, acc 0.65625, prec 0.0214159, recall 0.688787
2017-12-10T14:33:50.626211: step 468, loss 1.66737, acc 0.578125, prec 0.0215832, recall 0.690909
2017-12-10T14:33:50.817369: step 469, loss 1.06826, acc 0.671875, prec 0.0216205, recall 0.69161
2017-12-10T14:33:51.004592: step 470, loss 0.9994, acc 0.8125, prec 0.0216714, recall 0.692308
2017-12-10T14:33:51.191920: step 471, loss 16.9622, acc 0.71875, prec 0.0216484, recall 0.68764
2017-12-10T14:33:51.382339: step 472, loss 0.73144, acc 0.8125, prec 0.02163, recall 0.68764
2017-12-10T14:33:51.570571: step 473, loss 1.25386, acc 0.671875, prec 0.021598, recall 0.68764
2017-12-10T14:33:51.758198: step 474, loss 1.43469, acc 0.625, prec 0.0215614, recall 0.68764
2017-12-10T14:33:51.943743: step 475, loss 2.12511, acc 0.71875, prec 0.021603, recall 0.688341
2017-12-10T14:33:52.137306: step 476, loss 2.12126, acc 0.640625, prec 0.0217055, recall 0.689732
2017-12-10T14:33:52.323622: step 477, loss 1.30788, acc 0.625, prec 0.021669, recall 0.689732
2017-12-10T14:33:52.509525: step 478, loss 13.5214, acc 0.6875, prec 0.0216417, recall 0.686667
2017-12-10T14:33:52.702193: step 479, loss 2.28298, acc 0.484375, prec 0.0215918, recall 0.686667
2017-12-10T14:33:52.890822: step 480, loss 3.09187, acc 0.5625, prec 0.0216179, recall 0.687361
2017-12-10T14:33:53.080482: step 481, loss 2.08809, acc 0.5, prec 0.0216378, recall 0.688053
2017-12-10T14:33:53.267498: step 482, loss 1.69177, acc 0.5625, prec 0.0215957, recall 0.688053
2017-12-10T14:33:53.455075: step 483, loss 3.44678, acc 0.578125, prec 0.0217587, recall 0.69011
2017-12-10T14:33:53.643343: step 484, loss 2.15715, acc 0.453125, prec 0.0218413, recall 0.691466
2017-12-10T14:33:53.833625: step 485, loss 3.10229, acc 0.453125, prec 0.0217886, recall 0.691466
2017-12-10T14:33:54.020046: step 486, loss 2.0985, acc 0.5, prec 0.0218079, recall 0.69214
2017-12-10T14:33:54.203325: step 487, loss 2.02438, acc 0.484375, prec 0.0217585, recall 0.69214
2017-12-10T14:33:54.393570: step 488, loss 2.50934, acc 0.4375, prec 0.0217049, recall 0.69214
2017-12-10T14:33:54.583183: step 489, loss 2.82591, acc 0.4375, prec 0.0217183, recall 0.69281
2017-12-10T14:33:54.769908: step 490, loss 2.37807, acc 0.53125, prec 0.0216739, recall 0.69281
2017-12-10T14:33:54.955831: step 491, loss 1.83119, acc 0.578125, prec 0.0217007, recall 0.693478
2017-12-10T14:33:55.142782: step 492, loss 1.99979, acc 0.53125, prec 0.0216565, recall 0.693478
2017-12-10T14:33:55.332505: step 493, loss 1.4034, acc 0.65625, prec 0.0216905, recall 0.694143
2017-12-10T14:33:55.518741: step 494, loss 1.20215, acc 0.75, prec 0.0217332, recall 0.694805
2017-12-10T14:33:55.703311: step 495, loss 1.05448, acc 0.671875, prec 0.0217024, recall 0.694805
2017-12-10T14:33:55.894658: step 496, loss 0.586559, acc 0.796875, prec 0.0216833, recall 0.694805
2017-12-10T14:33:56.061275: step 497, loss 2.98727, acc 0.730769, prec 0.0216643, recall 0.693305
2017-12-10T14:33:56.261250: step 498, loss 1.6511, acc 0.8125, prec 0.0216482, recall 0.69181
2017-12-10T14:33:56.449643: step 499, loss 6.13395, acc 0.8125, prec 0.0216981, recall 0.690987
2017-12-10T14:33:56.644309: step 500, loss 0.966376, acc 0.734375, prec 0.0217391, recall 0.691649
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_3/1512934336/checkpoints/model-500

2017-12-10T14:33:57.825103: step 501, loss 0.960833, acc 0.78125, prec 0.0217187, recall 0.691649
2017-12-10T14:33:58.012693: step 502, loss 0.728522, acc 0.796875, prec 0.0216997, recall 0.691649
2017-12-10T14:33:58.199058: step 503, loss 0.847272, acc 0.765625, prec 0.0217435, recall 0.692308
2017-12-10T14:33:58.387868: step 504, loss 0.675195, acc 0.78125, prec 0.0217887, recall 0.692964
2017-12-10T14:33:58.574946: step 505, loss 0.813478, acc 0.765625, prec 0.0218323, recall 0.693617
2017-12-10T14:33:58.762804: step 506, loss 1.34946, acc 0.671875, prec 0.0218016, recall 0.693617
2017-12-10T14:33:58.954176: step 507, loss 0.964642, acc 0.8125, prec 0.0219149, recall 0.694915
2017-12-10T14:33:59.150752: step 508, loss 0.798313, acc 0.75, prec 0.0218915, recall 0.694915
2017-12-10T14:33:59.339579: step 509, loss 0.603971, acc 0.859375, prec 0.0219436, recall 0.69556
2017-12-10T14:33:59.527067: step 510, loss 0.596103, acc 0.78125, prec 0.0219883, recall 0.696203
2017-12-10T14:33:59.712169: step 511, loss 1.06763, acc 0.8125, prec 0.0220358, recall 0.696842
2017-12-10T14:33:59.908720: step 512, loss 1.1923, acc 0.796875, prec 0.0222119, recall 0.698745
2017-12-10T14:34:00.098267: step 513, loss 0.93873, acc 0.796875, prec 0.0223226, recall 0.7
2017-12-10T14:34:00.285357: step 514, loss 5.67608, acc 0.78125, prec 0.0223034, recall 0.698545
2017-12-10T14:34:00.473760: step 515, loss 0.884602, acc 0.875, prec 0.0223564, recall 0.69917
2017-12-10T14:34:00.660412: step 516, loss 8.02297, acc 0.78125, prec 0.0224034, recall 0.696907
2017-12-10T14:34:00.848461: step 517, loss 1.37321, acc 0.640625, prec 0.0223693, recall 0.696907
2017-12-10T14:34:01.039462: step 518, loss 1.26974, acc 0.65625, prec 0.0223368, recall 0.696907
2017-12-10T14:34:01.234428: step 519, loss 0.923729, acc 0.671875, prec 0.0223058, recall 0.696907
2017-12-10T14:34:01.423458: step 520, loss 1.20139, acc 0.625, prec 0.022335, recall 0.697531
2017-12-10T14:34:01.610826: step 521, loss 1.89171, acc 0.71875, prec 0.0224372, recall 0.69877
2017-12-10T14:34:01.798311: step 522, loss 3.011, acc 0.5625, prec 0.0223974, recall 0.697342
2017-12-10T14:34:01.984591: step 523, loss 1.26419, acc 0.609375, prec 0.0223607, recall 0.697342
2017-12-10T14:34:02.171740: step 524, loss 1.4499, acc 0.625, prec 0.0223255, recall 0.697342
2017-12-10T14:34:02.360521: step 525, loss 2.50432, acc 0.6875, prec 0.0224881, recall 0.699187
2017-12-10T14:34:02.553361: step 526, loss 1.35657, acc 0.640625, prec 0.0224543, recall 0.699187
2017-12-10T14:34:02.737374: step 527, loss 1.81299, acc 0.609375, prec 0.0224177, recall 0.699187
2017-12-10T14:34:02.923506: step 528, loss 1.31907, acc 0.703125, prec 0.0225172, recall 0.700405
2017-12-10T14:34:03.113617: step 529, loss 1.66885, acc 0.640625, prec 0.0224836, recall 0.700405
2017-12-10T14:34:03.297976: step 530, loss 1.35869, acc 0.640625, prec 0.0225135, recall 0.70101
2017-12-10T14:34:03.482976: step 531, loss 2.01002, acc 0.453125, prec 0.0224625, recall 0.70101
2017-12-10T14:34:03.673401: step 532, loss 1.79274, acc 0.59375, prec 0.0224879, recall 0.701613
2017-12-10T14:34:03.858066: step 533, loss 1.80524, acc 0.59375, prec 0.0225132, recall 0.702213
2017-12-10T14:34:04.048327: step 534, loss 2.94204, acc 0.765625, prec 0.0224929, recall 0.700803
2017-12-10T14:34:04.234425: step 535, loss 0.949798, acc 0.6875, prec 0.022464, recall 0.700803
2017-12-10T14:34:04.422737: step 536, loss 0.965635, acc 0.734375, prec 0.0224394, recall 0.700803
2017-12-10T14:34:04.614848: step 537, loss 1.33523, acc 0.703125, prec 0.022412, recall 0.700803
2017-12-10T14:34:04.800695: step 538, loss 0.806938, acc 0.71875, prec 0.0223861, recall 0.700803
2017-12-10T14:34:04.988232: step 539, loss 1.08244, acc 0.765625, prec 0.0224273, recall 0.701403
2017-12-10T14:34:05.175766: step 540, loss 2.75388, acc 0.796875, prec 0.0224726, recall 0.700599
2017-12-10T14:34:05.368502: step 541, loss 0.878248, acc 0.765625, prec 0.0225136, recall 0.701195
2017-12-10T14:34:05.560327: step 542, loss 0.877128, acc 0.75, prec 0.0224906, recall 0.701195
2017-12-10T14:34:05.749638: step 543, loss 0.361418, acc 0.875, prec 0.0225415, recall 0.701789
2017-12-10T14:34:05.936057: step 544, loss 5.78601, acc 0.8125, prec 0.0225881, recall 0.70099
2017-12-10T14:34:06.129922: step 545, loss 0.465568, acc 0.828125, prec 0.0225722, recall 0.70099
2017-12-10T14:34:06.317973: step 546, loss 0.398141, acc 0.890625, prec 0.0226244, recall 0.701581
2017-12-10T14:34:06.507682: step 547, loss 0.510562, acc 0.828125, prec 0.0226708, recall 0.70217
2017-12-10T14:34:06.690964: step 548, loss 0.672018, acc 0.78125, prec 0.0226506, recall 0.70217
2017-12-10T14:34:06.881607: step 549, loss 6.83255, acc 0.859375, prec 0.0227013, recall 0.701375
2017-12-10T14:34:07.070817: step 550, loss 0.402998, acc 0.875, prec 0.0228139, recall 0.702544
2017-12-10T14:34:07.257045: step 551, loss 0.401762, acc 0.859375, prec 0.0228009, recall 0.702544
2017-12-10T14:34:07.447219: step 552, loss 0.743207, acc 0.890625, prec 0.0229148, recall 0.703704
2017-12-10T14:34:07.640926: step 553, loss 0.579669, acc 0.890625, prec 0.0229046, recall 0.703704
2017-12-10T14:34:07.826429: step 554, loss 10.9175, acc 0.796875, prec 0.0229492, recall 0.702913
2017-12-10T14:34:08.017398: step 555, loss 0.39838, acc 0.828125, prec 0.0230569, recall 0.704062
2017-12-10T14:34:08.209928: step 556, loss 0.780228, acc 0.78125, prec 0.0230365, recall 0.704062
2017-12-10T14:34:08.396774: step 557, loss 1.28148, acc 0.71875, prec 0.0230721, recall 0.704633
2017-12-10T14:34:08.581694: step 558, loss 0.684183, acc 0.796875, prec 0.0230531, recall 0.704633
2017-12-10T14:34:08.769384: step 559, loss 1.73357, acc 0.765625, prec 0.0230929, recall 0.705202
2017-12-10T14:34:08.960040: step 560, loss 1.23992, acc 0.703125, prec 0.0231884, recall 0.706334
2017-12-10T14:34:09.146265: step 561, loss 1.18163, acc 0.65625, prec 0.0231563, recall 0.706334
2017-12-10T14:34:09.338806: step 562, loss 1.26665, acc 0.640625, prec 0.0231842, recall 0.706897
2017-12-10T14:34:09.520695: step 563, loss 1.04481, acc 0.6875, prec 0.0231551, recall 0.706897
2017-12-10T14:34:09.705125: step 564, loss 1.45873, acc 0.671875, prec 0.0233083, recall 0.708571
2017-12-10T14:34:09.893953: step 565, loss 0.864383, acc 0.734375, prec 0.0233446, recall 0.709125
2017-12-10T14:34:10.082315: step 566, loss 9.33165, acc 0.765625, prec 0.0233242, recall 0.70778
2017-12-10T14:34:10.272323: step 567, loss 2.31735, acc 0.734375, prec 0.0233008, recall 0.706439
2017-12-10T14:34:10.460406: step 568, loss 0.779159, acc 0.765625, prec 0.02334, recall 0.706994
2017-12-10T14:34:10.647804: step 569, loss 1.029, acc 0.734375, prec 0.0233153, recall 0.706994
2017-12-10T14:34:10.834098: step 570, loss 0.944012, acc 0.71875, prec 0.0233499, recall 0.707547
2017-12-10T14:34:11.027017: step 571, loss 0.960018, acc 0.71875, prec 0.0233238, recall 0.707547
2017-12-10T14:34:11.212385: step 572, loss 1.4371, acc 0.65625, prec 0.0232919, recall 0.707547
2017-12-10T14:34:11.399329: step 573, loss 1.17257, acc 0.609375, prec 0.0233769, recall 0.708647
2017-12-10T14:34:11.584175: step 574, loss 1.78487, acc 0.53125, prec 0.0233335, recall 0.708647
2017-12-10T14:34:11.770382: step 575, loss 1.16507, acc 0.734375, prec 0.0234298, recall 0.709738
2017-12-10T14:34:11.957535: step 576, loss 2.36106, acc 0.703125, prec 0.023464, recall 0.708955
2017-12-10T14:34:12.144694: step 577, loss 10.6403, acc 0.625, prec 0.0234924, recall 0.706865
2017-12-10T14:34:12.333083: step 578, loss 1.13334, acc 0.734375, prec 0.0234678, recall 0.706865
2017-12-10T14:34:12.521722: step 579, loss 0.894441, acc 0.6875, prec 0.0235591, recall 0.707948
2017-12-10T14:34:12.710293: step 580, loss 1.86115, acc 0.640625, prec 0.0236457, recall 0.709024
2017-12-10T14:34:12.895342: step 581, loss 1.21078, acc 0.65625, prec 0.0236138, recall 0.709024
2017-12-10T14:34:13.086577: step 582, loss 2.12924, acc 0.578125, prec 0.0237541, recall 0.710623
2017-12-10T14:34:13.275891: step 583, loss 1.61329, acc 0.609375, prec 0.0237178, recall 0.710623
2017-12-10T14:34:13.461743: step 584, loss 1.52497, acc 0.625, prec 0.0236831, recall 0.710623
2017-12-10T14:34:13.647565: step 585, loss 1.80782, acc 0.65625, prec 0.0236513, recall 0.710623
2017-12-10T14:34:13.831167: step 586, loss 1.63045, acc 0.53125, prec 0.0236082, recall 0.710623
2017-12-10T14:34:14.020694: step 587, loss 1.8562, acc 0.5625, prec 0.0237459, recall 0.712204
2017-12-10T14:34:14.217071: step 588, loss 1.66819, acc 0.65625, prec 0.0237142, recall 0.712204
2017-12-10T14:34:14.404053: step 589, loss 1.24827, acc 0.65625, prec 0.0237417, recall 0.712727
2017-12-10T14:34:14.593489: step 590, loss 0.970231, acc 0.734375, prec 0.0237764, recall 0.713249
2017-12-10T14:34:14.776183: step 591, loss 3.34104, acc 0.765625, prec 0.0237563, recall 0.711957
2017-12-10T14:34:14.965647: step 592, loss 0.269411, acc 0.890625, prec 0.0238052, recall 0.712477
2017-12-10T14:34:15.151285: step 593, loss 0.878388, acc 0.78125, prec 0.0237851, recall 0.712477
2017-12-10T14:34:15.341307: step 594, loss 0.283768, acc 0.921875, prec 0.0238368, recall 0.712996
2017-12-10T14:34:15.529322: step 595, loss 0.534594, acc 0.796875, prec 0.0238181, recall 0.712996
2017-12-10T14:34:15.718005: step 596, loss 8.63928, acc 0.875, prec 0.0238081, recall 0.711712
2017-12-10T14:34:15.912685: step 597, loss 0.736053, acc 0.75, prec 0.0237852, recall 0.711712
2017-12-10T14:34:16.097999: step 598, loss 0.690534, acc 0.8125, prec 0.023768, recall 0.711712
2017-12-10T14:34:16.280371: step 599, loss 0.842085, acc 0.734375, prec 0.0237437, recall 0.711712
2017-12-10T14:34:16.467577: step 600, loss 0.62621, acc 0.84375, prec 0.0237881, recall 0.71223
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_3/1512934336/checkpoints/model-600

2017-12-10T14:34:17.619757: step 601, loss 0.417773, acc 0.84375, prec 0.0237738, recall 0.71223
2017-12-10T14:34:17.808363: step 602, loss 1.49223, acc 0.84375, prec 0.0238181, recall 0.712747
2017-12-10T14:34:18.000708: step 603, loss 0.543621, acc 0.796875, prec 0.0237995, recall 0.712747
2017-12-10T14:34:18.186608: step 604, loss 0.424565, acc 0.90625, prec 0.023791, recall 0.712747
2017-12-10T14:34:18.377757: step 605, loss 1.54486, acc 0.796875, prec 0.0238309, recall 0.713262
2017-12-10T14:34:18.566882: step 606, loss 0.654783, acc 0.828125, prec 0.023932, recall 0.714286
2017-12-10T14:34:18.758456: step 607, loss 1.44816, acc 0.84375, prec 0.0239761, recall 0.714795
2017-12-10T14:34:18.951265: step 608, loss 0.824063, acc 0.828125, prec 0.0240186, recall 0.715302
2017-12-10T14:34:19.138513: step 609, loss 0.612025, acc 0.890625, prec 0.0240086, recall 0.715302
2017-12-10T14:34:19.326734: step 610, loss 12.0629, acc 0.8125, prec 0.0239943, recall 0.712766
2017-12-10T14:34:19.516654: step 611, loss 1.4091, acc 0.796875, prec 0.0240339, recall 0.713274
2017-12-10T14:34:19.704523: step 612, loss 1.03973, acc 0.734375, prec 0.0240677, recall 0.713781
2017-12-10T14:34:19.891252: step 613, loss 0.990305, acc 0.71875, prec 0.0240419, recall 0.713781
2017-12-10T14:34:20.076704: step 614, loss 1.37442, acc 0.59375, prec 0.0240048, recall 0.713781
2017-12-10T14:34:20.262361: step 615, loss 3.31054, acc 0.453125, prec 0.0240707, recall 0.714789
2017-12-10T14:34:20.453998: step 616, loss 1.65558, acc 0.5625, prec 0.0240885, recall 0.71529
2017-12-10T14:34:20.640217: step 617, loss 2.5023, acc 0.609375, prec 0.0241106, recall 0.715789
2017-12-10T14:34:20.830382: step 618, loss 1.81077, acc 0.640625, prec 0.0241355, recall 0.716287
2017-12-10T14:34:21.014368: step 619, loss 2.33605, acc 0.4375, prec 0.0240843, recall 0.716287
2017-12-10T14:34:21.200987: step 620, loss 1.22278, acc 0.640625, prec 0.0240518, recall 0.716287
2017-12-10T14:34:21.389157: step 621, loss 1.81074, acc 0.609375, prec 0.0240164, recall 0.716287
2017-12-10T14:34:21.574519: step 622, loss 7.65526, acc 0.75, prec 0.0240526, recall 0.715532
2017-12-10T14:34:21.765572: step 623, loss 1.74083, acc 0.5625, prec 0.0240131, recall 0.715532
2017-12-10T14:34:21.949707: step 624, loss 1.81505, acc 0.59375, prec 0.0240908, recall 0.716522
2017-12-10T14:34:22.133474: step 625, loss 1.72101, acc 0.625, prec 0.0242279, recall 0.717993
2017-12-10T14:34:22.319663: step 626, loss 1.23171, acc 0.640625, prec 0.0241954, recall 0.717993
2017-12-10T14:34:22.503589: step 627, loss 1.12091, acc 0.765625, prec 0.0241743, recall 0.717993
2017-12-10T14:34:22.689344: step 628, loss 3.56618, acc 0.671875, prec 0.0241462, recall 0.716753
2017-12-10T14:34:22.876806: step 629, loss 1.43552, acc 0.609375, prec 0.0241111, recall 0.716753
2017-12-10T14:34:23.065937: step 630, loss 1.58133, acc 0.5625, prec 0.0241285, recall 0.717241
2017-12-10T14:34:23.254872: step 631, loss 1.00427, acc 0.671875, prec 0.0240992, recall 0.717241
2017-12-10T14:34:23.443465: step 632, loss 1.55451, acc 0.75, prec 0.0242463, recall 0.718696
2017-12-10T14:34:23.634217: step 633, loss 1.28915, acc 0.734375, prec 0.0242789, recall 0.719178
2017-12-10T14:34:23.824446: step 634, loss 0.891282, acc 0.734375, prec 0.0244804, recall 0.721088
2017-12-10T14:34:24.011381: step 635, loss 0.955263, acc 0.765625, prec 0.024628, recall 0.722504
2017-12-10T14:34:24.201838: step 636, loss 7.87665, acc 0.78125, prec 0.0246672, recall 0.720539
2017-12-10T14:34:24.396461: step 637, loss 0.880489, acc 0.71875, prec 0.0246978, recall 0.721008
2017-12-10T14:34:24.583052: step 638, loss 1.45007, acc 0.703125, prec 0.0247269, recall 0.721476
2017-12-10T14:34:24.768222: step 639, loss 5.62121, acc 0.6875, prec 0.0246999, recall 0.720268
2017-12-10T14:34:24.955203: step 640, loss 1.2469, acc 0.609375, prec 0.0246644, recall 0.720268
2017-12-10T14:34:25.142374: step 641, loss 1.11592, acc 0.578125, prec 0.024738, recall 0.721202
2017-12-10T14:34:25.335672: step 642, loss 1.12442, acc 0.671875, prec 0.0247641, recall 0.721667
2017-12-10T14:34:25.525315: step 643, loss 1.17926, acc 0.703125, prec 0.0248486, recall 0.722591
2017-12-10T14:34:25.714222: step 644, loss 0.805204, acc 0.6875, prec 0.0248759, recall 0.723051
2017-12-10T14:34:25.899240: step 645, loss 0.99996, acc 0.65625, prec 0.0248447, recall 0.723051
2017-12-10T14:34:26.086798: step 646, loss 1.33203, acc 0.6875, prec 0.0248164, recall 0.723051
2017-12-10T14:34:26.271763: step 647, loss 8.1431, acc 0.734375, prec 0.0248507, recall 0.721122
2017-12-10T14:34:26.460475: step 648, loss 1.04107, acc 0.703125, prec 0.0248239, recall 0.721122
2017-12-10T14:34:26.646690: step 649, loss 0.961164, acc 0.703125, prec 0.0248525, recall 0.721582
2017-12-10T14:34:26.831221: step 650, loss 0.913448, acc 0.671875, prec 0.0248229, recall 0.721582
2017-12-10T14:34:27.022212: step 651, loss 1.212, acc 0.734375, prec 0.0248542, recall 0.722039
2017-12-10T14:34:27.206959: step 652, loss 1.18626, acc 0.71875, prec 0.0248841, recall 0.722496
2017-12-10T14:34:27.398388: step 653, loss 1.29964, acc 0.640625, prec 0.0248517, recall 0.722496
2017-12-10T14:34:27.583091: step 654, loss 1.45955, acc 0.640625, prec 0.0248195, recall 0.722496
2017-12-10T14:34:27.766885: step 655, loss 1.24151, acc 0.65625, prec 0.0247887, recall 0.722496
2017-12-10T14:34:27.954098: step 656, loss 1.23327, acc 0.671875, prec 0.0247594, recall 0.722496
2017-12-10T14:34:28.143934: step 657, loss 1.06868, acc 0.734375, prec 0.0247906, recall 0.722951
2017-12-10T14:34:28.336229: step 658, loss 1.07096, acc 0.703125, prec 0.0247642, recall 0.722951
2017-12-10T14:34:28.523344: step 659, loss 1.23022, acc 0.75, prec 0.0247419, recall 0.722951
2017-12-10T14:34:28.709596: step 660, loss 0.806369, acc 0.734375, prec 0.0247183, recall 0.722951
2017-12-10T14:34:28.899675: step 661, loss 0.48606, acc 0.796875, prec 0.0247003, recall 0.722951
2017-12-10T14:34:29.094385: step 662, loss 0.726515, acc 0.890625, prec 0.024909, recall 0.724756
2017-12-10T14:34:29.282167: step 663, loss 0.625834, acc 0.875, prec 0.025007, recall 0.725649
2017-12-10T14:34:29.473236: step 664, loss 0.491323, acc 0.828125, prec 0.0251006, recall 0.726537
2017-12-10T14:34:29.660536: step 665, loss 1.24733, acc 0.9375, prec 0.0250964, recall 0.725363
2017-12-10T14:34:29.848508: step 666, loss 0.283749, acc 0.890625, prec 0.0250866, recall 0.725363
2017-12-10T14:34:30.033641: step 667, loss 0.11161, acc 0.953125, prec 0.0250824, recall 0.725363
2017-12-10T14:34:30.218839: step 668, loss 1.03543, acc 0.859375, prec 0.0251242, recall 0.725806
2017-12-10T14:34:30.411971: step 669, loss 5.66894, acc 0.921875, prec 0.025173, recall 0.72508
2017-12-10T14:34:30.600670: step 670, loss 0.407066, acc 0.859375, prec 0.0251604, recall 0.72508
2017-12-10T14:34:30.788320: step 671, loss 0.156915, acc 0.953125, prec 0.0252106, recall 0.725522
2017-12-10T14:34:30.973574: step 672, loss 7.00008, acc 0.90625, prec 0.0252035, recall 0.724359
2017-12-10T14:34:31.167003: step 673, loss 0.916581, acc 0.859375, prec 0.0252995, recall 0.72524
2017-12-10T14:34:31.359142: step 674, loss 0.577546, acc 0.828125, prec 0.025284, recall 0.72524
2017-12-10T14:34:31.548628: step 675, loss 0.562379, acc 0.765625, prec 0.0252629, recall 0.72524
2017-12-10T14:34:31.735895: step 676, loss 1.29993, acc 0.6875, prec 0.0253974, recall 0.72655
2017-12-10T14:34:31.925470: step 677, loss 0.373185, acc 0.8125, prec 0.0253804, recall 0.72655
2017-12-10T14:34:32.114239: step 678, loss 1.31131, acc 0.734375, prec 0.0254646, recall 0.727417
2017-12-10T14:34:32.304367: step 679, loss 1.07626, acc 0.65625, prec 0.0254336, recall 0.727417
2017-12-10T14:34:32.491794: step 680, loss 0.664341, acc 0.796875, prec 0.0254692, recall 0.727848
2017-12-10T14:34:32.678123: step 681, loss 0.554361, acc 0.796875, prec 0.0255048, recall 0.728278
2017-12-10T14:34:32.865782: step 682, loss 0.850394, acc 0.78125, prec 0.0254851, recall 0.728278
2017-12-10T14:34:33.053239: step 683, loss 1.1629, acc 0.828125, prec 0.0255235, recall 0.728707
2017-12-10T14:34:33.240587: step 684, loss 2.39231, acc 0.765625, prec 0.0255037, recall 0.727559
2017-12-10T14:34:33.431760: step 685, loss 1.09211, acc 0.703125, prec 0.025477, recall 0.727559
2017-12-10T14:34:33.620726: step 686, loss 0.713767, acc 0.796875, prec 0.0254588, recall 0.727559
2017-12-10T14:34:33.807187: step 687, loss 1.13306, acc 0.828125, prec 0.025497, recall 0.727987
2017-12-10T14:34:33.994199: step 688, loss 0.918978, acc 0.734375, prec 0.0255804, recall 0.72884
2017-12-10T14:34:34.184708: step 689, loss 4.45324, acc 0.796875, prec 0.0256171, recall 0.728125
2017-12-10T14:34:34.373539: step 690, loss 0.65132, acc 0.828125, prec 0.0256551, recall 0.728549
2017-12-10T14:34:34.557478: step 691, loss 1.17308, acc 0.6875, prec 0.0257339, recall 0.729393
2017-12-10T14:34:34.745504: step 692, loss 0.865609, acc 0.734375, prec 0.0257099, recall 0.729393
2017-12-10T14:34:34.932152: step 693, loss 7.32536, acc 0.796875, prec 0.025693, recall 0.728261
2017-12-10T14:34:35.124841: step 694, loss 3.03429, acc 0.640625, prec 0.0258206, recall 0.729521
2017-12-10T14:34:35.315800: step 695, loss 1.00062, acc 0.625, prec 0.0257867, recall 0.729521
2017-12-10T14:34:35.499497: step 696, loss 1.69975, acc 0.578125, prec 0.025855, recall 0.730354
2017-12-10T14:34:35.684320: step 697, loss 3.83628, acc 0.59375, prec 0.0258198, recall 0.729231
2017-12-10T14:34:35.871612: step 698, loss 2.16547, acc 0.53125, prec 0.0258307, recall 0.729647
2017-12-10T14:34:36.055018: step 699, loss 2.38649, acc 0.40625, prec 0.0257774, recall 0.729647
2017-12-10T14:34:36.236130: step 700, loss 1.84538, acc 0.53125, prec 0.0257355, recall 0.729647
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_3/1512934336/checkpoints/model-700

2017-12-10T14:34:37.341411: step 701, loss 2.08167, acc 0.484375, prec 0.0257423, recall 0.730061
2017-12-10T14:34:37.526550: step 702, loss 2.59512, acc 0.484375, prec 0.0258016, recall 0.730887
2017-12-10T14:34:37.710537: step 703, loss 7.49721, acc 0.453125, prec 0.0258068, recall 0.730183
2017-12-10T14:34:37.898000: step 704, loss 2.52377, acc 0.515625, prec 0.0258162, recall 0.730594
2017-12-10T14:34:38.085604: step 705, loss 1.6295, acc 0.59375, prec 0.0257801, recall 0.730594
2017-12-10T14:34:38.270056: step 706, loss 2.75426, acc 0.46875, prec 0.0257854, recall 0.731003
2017-12-10T14:34:38.457398: step 707, loss 2.29184, acc 0.578125, prec 0.0258002, recall 0.731411
2017-12-10T14:34:38.640071: step 708, loss 2.01496, acc 0.546875, prec 0.0258644, recall 0.732224
2017-12-10T14:34:38.825249: step 709, loss 1.11259, acc 0.703125, prec 0.0258381, recall 0.732224
2017-12-10T14:34:39.012351: step 710, loss 1.3976, acc 0.640625, prec 0.0258584, recall 0.732628
2017-12-10T14:34:39.200710: step 711, loss 1.90469, acc 0.578125, prec 0.0259768, recall 0.733835
2017-12-10T14:34:39.387175: step 712, loss 1.70257, acc 0.5625, prec 0.0259381, recall 0.733835
2017-12-10T14:34:39.583964: step 713, loss 1.2128, acc 0.625, prec 0.0259051, recall 0.733835
2017-12-10T14:34:39.768579: step 714, loss 0.998239, acc 0.703125, prec 0.0259823, recall 0.734633
2017-12-10T14:34:39.956666: step 715, loss 0.870295, acc 0.75, prec 0.0260119, recall 0.73503
2017-12-10T14:34:40.143134: step 716, loss 1.86705, acc 0.671875, prec 0.0260345, recall 0.735426
2017-12-10T14:34:40.330264: step 717, loss 0.872701, acc 0.78125, prec 0.0260667, recall 0.735821
2017-12-10T14:34:40.518114: step 718, loss 0.69589, acc 0.765625, prec 0.026149, recall 0.736607
2017-12-10T14:34:40.705102: step 719, loss 0.237409, acc 0.875, prec 0.0261379, recall 0.736607
2017-12-10T14:34:40.895258: step 720, loss 1.14597, acc 0.8125, prec 0.0262241, recall 0.737389
2017-12-10T14:34:41.081552: step 721, loss 0.767318, acc 0.875, prec 0.0262644, recall 0.737778
2017-12-10T14:34:41.271279: step 722, loss 1.03797, acc 0.875, prec 0.026356, recall 0.738552
2017-12-10T14:34:41.461359: step 723, loss 0.844822, acc 0.84375, prec 0.0263934, recall 0.738938
2017-12-10T14:34:41.647974: step 724, loss 0.272635, acc 0.890625, prec 0.0263837, recall 0.738938
2017-12-10T14:34:41.837902: step 725, loss 0.371002, acc 0.859375, prec 0.0263712, recall 0.738938
2017-12-10T14:34:42.024224: step 726, loss 0.456898, acc 0.890625, prec 0.0263615, recall 0.738938
2017-12-10T14:34:42.209112: step 727, loss 0.334405, acc 0.890625, prec 0.0263518, recall 0.738938
2017-12-10T14:34:42.394000: step 728, loss 0.2914, acc 0.875, prec 0.0263407, recall 0.738938
2017-12-10T14:34:42.578079: step 729, loss 3.9522, acc 0.8125, prec 0.0264278, recall 0.73862
2017-12-10T14:34:42.768664: step 730, loss 0.885821, acc 0.8125, prec 0.0264622, recall 0.739003
2017-12-10T14:34:42.956392: step 731, loss 1.60677, acc 0.8125, prec 0.026447, recall 0.737921
2017-12-10T14:34:43.146726: step 732, loss 0.166658, acc 0.953125, prec 0.0264428, recall 0.737921
2017-12-10T14:34:43.337253: step 733, loss 0.39948, acc 0.859375, prec 0.0264814, recall 0.738304
2017-12-10T14:34:43.522187: step 734, loss 7.88911, acc 0.921875, prec 0.0265779, recall 0.737991
2017-12-10T14:34:43.711255: step 735, loss 1.02665, acc 0.734375, prec 0.0265542, recall 0.737991
2017-12-10T14:34:43.898342: step 736, loss 0.863806, acc 0.78125, prec 0.0266876, recall 0.73913
2017-12-10T14:34:44.090993: step 737, loss 0.314989, acc 0.859375, prec 0.0267259, recall 0.739508
2017-12-10T14:34:44.279949: step 738, loss 0.554258, acc 0.78125, prec 0.0267573, recall 0.739884
2017-12-10T14:34:44.465026: step 739, loss 0.714422, acc 0.8125, prec 0.0267405, recall 0.739884
2017-12-10T14:34:44.650786: step 740, loss 0.454519, acc 0.875, prec 0.0267293, recall 0.739884
2017-12-10T14:34:44.834977: step 741, loss 0.959675, acc 0.671875, prec 0.0268015, recall 0.740634
2017-12-10T14:34:45.022595: step 742, loss 0.75824, acc 0.8125, prec 0.0267848, recall 0.740634
2017-12-10T14:34:45.209309: step 743, loss 1.01042, acc 0.671875, prec 0.0267555, recall 0.740634
2017-12-10T14:34:45.396919: step 744, loss 1.20169, acc 0.671875, prec 0.0267769, recall 0.741007
2017-12-10T14:34:45.586367: step 745, loss 0.673135, acc 0.796875, prec 0.0267588, recall 0.741007
2017-12-10T14:34:45.775266: step 746, loss 0.741171, acc 0.796875, prec 0.0267407, recall 0.741007
2017-12-10T14:34:45.963403: step 747, loss 0.828395, acc 0.75, prec 0.0267185, recall 0.741007
2017-12-10T14:34:46.150047: step 748, loss 1.13303, acc 0.71875, prec 0.0267441, recall 0.741379
2017-12-10T14:34:46.335907: step 749, loss 0.750615, acc 0.734375, prec 0.0267205, recall 0.741379
2017-12-10T14:34:46.521660: step 750, loss 0.510645, acc 0.859375, prec 0.0267081, recall 0.741379
2017-12-10T14:34:46.709632: step 751, loss 0.473684, acc 0.828125, prec 0.0266929, recall 0.741379
2017-12-10T14:34:46.900210: step 752, loss 0.350103, acc 0.875, prec 0.0266818, recall 0.741379
2017-12-10T14:34:47.086443: step 753, loss 0.366296, acc 0.875, prec 0.0266708, recall 0.741379
2017-12-10T14:34:47.277307: step 754, loss 6.27718, acc 0.890625, prec 0.0266625, recall 0.740316
2017-12-10T14:34:47.468315: step 755, loss 0.413153, acc 0.859375, prec 0.0267507, recall 0.741059
2017-12-10T14:34:47.653939: step 756, loss 0.279817, acc 0.90625, prec 0.0267424, recall 0.741059
2017-12-10T14:34:47.846138: step 757, loss 0.337044, acc 0.859375, prec 0.02673, recall 0.741059
2017-12-10T14:34:48.034477: step 758, loss 1.34538, acc 0.890625, prec 0.0268207, recall 0.741797
2017-12-10T14:34:48.224900: step 759, loss 0.875123, acc 0.84375, prec 0.0270075, recall 0.743262
2017-12-10T14:34:48.412686: step 760, loss 4.79803, acc 0.875, prec 0.0269978, recall 0.74221
2017-12-10T14:34:48.601995: step 761, loss 7.76461, acc 0.875, prec 0.0270382, recall 0.741525
2017-12-10T14:34:48.794323: step 762, loss 0.734506, acc 0.765625, prec 0.0270173, recall 0.741525
2017-12-10T14:34:48.980764: step 763, loss 4.88103, acc 0.859375, prec 0.0271063, recall 0.74121
2017-12-10T14:34:49.168386: step 764, loss 1.50487, acc 0.765625, prec 0.0271354, recall 0.741573
2017-12-10T14:34:49.358693: step 765, loss 1.37127, acc 0.671875, prec 0.0271061, recall 0.741573
2017-12-10T14:34:49.544870: step 766, loss 1.29438, acc 0.703125, prec 0.0271795, recall 0.742297
2017-12-10T14:34:49.730012: step 767, loss 2.00231, acc 0.609375, prec 0.0272443, recall 0.743017
2017-12-10T14:34:49.916660: step 768, loss 1.5451, acc 0.5625, prec 0.0272053, recall 0.743017
2017-12-10T14:34:50.099606: step 769, loss 2.10268, acc 0.5625, prec 0.0272161, recall 0.743375
2017-12-10T14:34:50.287909: step 770, loss 1.38856, acc 0.625, prec 0.0271828, recall 0.743375
2017-12-10T14:34:50.477757: step 771, loss 1.82471, acc 0.5625, prec 0.0271936, recall 0.743733
2017-12-10T14:34:50.662564: step 772, loss 2.84832, acc 0.40625, prec 0.027141, recall 0.743733
2017-12-10T14:34:50.846962: step 773, loss 1.99672, acc 0.578125, prec 0.0271532, recall 0.744089
2017-12-10T14:34:51.031057: step 774, loss 2.44997, acc 0.53125, prec 0.0271612, recall 0.744444
2017-12-10T14:34:51.218558: step 775, loss 1.71449, acc 0.53125, prec 0.0272185, recall 0.745152
2017-12-10T14:34:51.404093: step 776, loss 2.41351, acc 0.53125, prec 0.0272263, recall 0.745505
2017-12-10T14:34:51.592257: step 777, loss 1.54243, acc 0.640625, prec 0.0271948, recall 0.745505
2017-12-10T14:34:51.780750: step 778, loss 0.956045, acc 0.71875, prec 0.0272191, recall 0.745856
2017-12-10T14:34:51.965798: step 779, loss 2.23164, acc 0.703125, prec 0.027291, recall 0.746556
2017-12-10T14:34:52.150574: step 780, loss 0.937407, acc 0.734375, prec 0.0273166, recall 0.746905
2017-12-10T14:34:52.341668: step 781, loss 2.07157, acc 0.734375, prec 0.0273422, recall 0.747253
2017-12-10T14:34:52.529072: step 782, loss 0.728696, acc 0.796875, prec 0.0273243, recall 0.747253
2017-12-10T14:34:52.716456: step 783, loss 6.78863, acc 0.734375, prec 0.0273024, recall 0.746228
2017-12-10T14:34:52.902588: step 784, loss 4.18147, acc 0.71875, prec 0.0273279, recall 0.745554
2017-12-10T14:34:53.093661: step 785, loss 6.46383, acc 0.703125, prec 0.0273032, recall 0.744536
2017-12-10T14:34:53.280211: step 786, loss 1.23393, acc 0.609375, prec 0.0272691, recall 0.744536
2017-12-10T14:34:53.465168: step 787, loss 4.17003, acc 0.578125, prec 0.0272337, recall 0.74352
2017-12-10T14:34:53.654234: step 788, loss 2.74145, acc 0.53125, prec 0.0271943, recall 0.742507
2017-12-10T14:34:53.841627: step 789, loss 1.86882, acc 0.515625, prec 0.0272007, recall 0.742857
2017-12-10T14:34:54.026037: step 790, loss 1.93032, acc 0.578125, prec 0.0272126, recall 0.743207
2017-12-10T14:34:54.210616: step 791, loss 1.26995, acc 0.578125, prec 0.0272244, recall 0.743555
2017-12-10T14:34:54.396421: step 792, loss 2.90059, acc 0.421875, prec 0.0272709, recall 0.744249
2017-12-10T14:34:54.584815: step 793, loss 2.17237, acc 0.578125, prec 0.0273789, recall 0.745283
2017-12-10T14:34:54.771200: step 794, loss 1.84372, acc 0.578125, prec 0.0273905, recall 0.745626
2017-12-10T14:34:54.958970: step 795, loss 1.47952, acc 0.59375, prec 0.0274994, recall 0.746649
2017-12-10T14:34:55.143353: step 796, loss 1.77901, acc 0.53125, prec 0.0274587, recall 0.746649
2017-12-10T14:34:55.326033: step 797, loss 1.91521, acc 0.578125, prec 0.027518, recall 0.747326
2017-12-10T14:34:55.514845: step 798, loss 1.46189, acc 0.609375, prec 0.027532, recall 0.747664
2017-12-10T14:34:55.699405: step 799, loss 1.58651, acc 0.5625, prec 0.0275419, recall 0.748
2017-12-10T14:34:55.885249: step 800, loss 1.63569, acc 0.625, prec 0.0275094, recall 0.748
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_3/1512934336/checkpoints/model-800

2017-12-10T14:34:57.063362: step 801, loss 1.54156, acc 0.5625, prec 0.0275193, recall 0.748336
2017-12-10T14:34:57.251609: step 802, loss 1.17779, acc 0.640625, prec 0.0275359, recall 0.74867
2017-12-10T14:34:57.437816: step 803, loss 1.16388, acc 0.6875, prec 0.0275566, recall 0.749004
2017-12-10T14:34:57.622911: step 804, loss 1.55603, acc 0.640625, prec 0.0275256, recall 0.749004
2017-12-10T14:34:57.810219: step 805, loss 7.16137, acc 0.6875, prec 0.0275001, recall 0.748011
2017-12-10T14:34:57.996242: step 806, loss 0.765434, acc 0.796875, prec 0.0275301, recall 0.748344
2017-12-10T14:34:58.189029: step 807, loss 0.506048, acc 0.84375, prec 0.027564, recall 0.748677
2017-12-10T14:34:58.380739: step 808, loss 0.535667, acc 0.84375, prec 0.0275506, recall 0.748677
2017-12-10T14:34:58.571187: step 809, loss 0.285917, acc 0.890625, prec 0.0275412, recall 0.748677
2017-12-10T14:34:58.758751: step 810, loss 0.627437, acc 0.796875, prec 0.0275238, recall 0.748677
2017-12-10T14:34:58.947550: step 811, loss 1.46925, acc 0.890625, prec 0.027609, recall 0.74934
2017-12-10T14:34:59.144031: step 812, loss 0.407204, acc 0.890625, prec 0.0275996, recall 0.74934
2017-12-10T14:34:59.329411: step 813, loss 5.37096, acc 0.734375, prec 0.0275782, recall 0.748353
2017-12-10T14:34:59.518599: step 814, loss 0.543369, acc 0.84375, prec 0.027612, recall 0.748684
2017-12-10T14:34:59.702594: step 815, loss 15.2459, acc 0.8125, prec 0.0275972, recall 0.7477
2017-12-10T14:34:59.890172: step 816, loss 0.48103, acc 0.84375, prec 0.027631, recall 0.748031
2017-12-10T14:35:00.082293: step 817, loss 0.927029, acc 0.71875, prec 0.0276069, recall 0.748031
2017-12-10T14:35:00.271102: step 818, loss 0.987616, acc 0.734375, prec 0.0276783, recall 0.748691
2017-12-10T14:35:00.457293: step 819, loss 0.828617, acc 0.71875, prec 0.0276542, recall 0.748691
2017-12-10T14:35:00.643464: step 820, loss 1.31873, acc 0.765625, prec 0.0277281, recall 0.749347
2017-12-10T14:35:00.831287: step 821, loss 6.43923, acc 0.71875, prec 0.0278006, recall 0.748052
2017-12-10T14:35:01.018364: step 822, loss 12.1592, acc 0.703125, prec 0.0278247, recall 0.746442
2017-12-10T14:35:01.210176: step 823, loss 1.1523, acc 0.703125, prec 0.0278929, recall 0.747097
2017-12-10T14:35:01.402596: step 824, loss 1.20497, acc 0.734375, prec 0.02787, recall 0.747097
2017-12-10T14:35:01.592422: step 825, loss 1.9411, acc 0.484375, prec 0.0278726, recall 0.747423
2017-12-10T14:35:01.781742: step 826, loss 2.23367, acc 0.53125, prec 0.0278324, recall 0.747423
2017-12-10T14:35:01.965829: step 827, loss 3.19604, acc 0.46875, prec 0.0278337, recall 0.747748
2017-12-10T14:35:02.149777: step 828, loss 2.28003, acc 0.484375, prec 0.0279292, recall 0.748718
2017-12-10T14:35:02.335083: step 829, loss 1.99569, acc 0.515625, prec 0.0279343, recall 0.74904
2017-12-10T14:35:02.519600: step 830, loss 2.89927, acc 0.453125, prec 0.0278877, recall 0.74904
2017-12-10T14:35:02.702848: step 831, loss 9.17313, acc 0.40625, prec 0.0278848, recall 0.748404
2017-12-10T14:35:02.886789: step 832, loss 3.50365, acc 0.296875, prec 0.0278714, recall 0.748724
2017-12-10T14:35:03.070537: step 833, loss 2.3262, acc 0.46875, prec 0.0278726, recall 0.749045
2017-12-10T14:35:03.263035: step 834, loss 3.02206, acc 0.359375, prec 0.0278185, recall 0.749045
2017-12-10T14:35:03.448010: step 835, loss 1.94055, acc 0.5, prec 0.0278224, recall 0.749364
2017-12-10T14:35:03.633216: step 836, loss 3.14199, acc 0.4375, prec 0.027821, recall 0.749682
2017-12-10T14:35:03.817307: step 837, loss 2.26083, acc 0.53125, prec 0.0278275, recall 0.75
2017-12-10T14:35:04.001592: step 838, loss 1.43717, acc 0.625, prec 0.0278418, recall 0.750317
2017-12-10T14:35:04.185136: step 839, loss 1.8337, acc 0.71875, prec 0.0279553, recall 0.751263
2017-12-10T14:35:04.373225: step 840, loss 1.76167, acc 0.59375, prec 0.0279668, recall 0.751576
2017-12-10T14:35:04.558629: step 841, loss 1.75371, acc 0.578125, prec 0.0279314, recall 0.751576
2017-12-10T14:35:04.747586: step 842, loss 1.61362, acc 0.6875, prec 0.0279507, recall 0.751889
2017-12-10T14:35:04.931872: step 843, loss 2.1918, acc 0.5, prec 0.0279544, recall 0.752201
2017-12-10T14:35:05.117355: step 844, loss 1.2053, acc 0.71875, prec 0.0279309, recall 0.752201
2017-12-10T14:35:05.303564: step 845, loss 1.18715, acc 0.71875, prec 0.0279528, recall 0.752513
2017-12-10T14:35:05.488008: step 846, loss 1.16751, acc 0.671875, prec 0.0279254, recall 0.752513
2017-12-10T14:35:05.676480: step 847, loss 5.5981, acc 0.765625, prec 0.0279978, recall 0.75219
2017-12-10T14:35:05.869704: step 848, loss 0.606304, acc 0.859375, prec 0.0280313, recall 0.7525
2017-12-10T14:35:06.059457: step 849, loss 0.594778, acc 0.796875, prec 0.0280143, recall 0.7525
2017-12-10T14:35:06.242003: step 850, loss 1.99418, acc 0.78125, prec 0.0281317, recall 0.753425
2017-12-10T14:35:06.427214: step 851, loss 1.06837, acc 0.71875, prec 0.0281082, recall 0.753425
2017-12-10T14:35:06.614681: step 852, loss 2.75006, acc 0.734375, prec 0.0281775, recall 0.753102
2017-12-10T14:35:06.802489: step 853, loss 0.778068, acc 0.703125, prec 0.0281527, recall 0.753102
2017-12-10T14:35:06.989212: step 854, loss 1.05102, acc 0.671875, prec 0.0281703, recall 0.753408
2017-12-10T14:35:07.175701: step 855, loss 3.96731, acc 0.78125, prec 0.0281547, recall 0.751545
2017-12-10T14:35:07.364917: step 856, loss 1.08878, acc 0.65625, prec 0.028171, recall 0.751852
2017-12-10T14:35:07.547278: step 857, loss 1.06615, acc 0.75, prec 0.0281951, recall 0.752158
2017-12-10T14:35:07.733814: step 858, loss 1.73053, acc 0.640625, prec 0.02821, recall 0.752463
2017-12-10T14:35:07.922150: step 859, loss 1.93645, acc 0.6875, prec 0.0282736, recall 0.753071
2017-12-10T14:35:08.109396: step 860, loss 1.50698, acc 0.578125, prec 0.028328, recall 0.753676
2017-12-10T14:35:08.297181: step 861, loss 15.0018, acc 0.625, prec 0.0283427, recall 0.753056
2017-12-10T14:35:08.482010: step 862, loss 0.863233, acc 0.703125, prec 0.0283179, recall 0.753056
2017-12-10T14:35:08.664495: step 863, loss 2.20068, acc 0.546875, prec 0.0283248, recall 0.753358
2017-12-10T14:35:08.849032: step 864, loss 1.5986, acc 0.625, prec 0.0282937, recall 0.753358
2017-12-10T14:35:09.038511: step 865, loss 1.36017, acc 0.640625, prec 0.0282639, recall 0.753358
2017-12-10T14:35:09.222738: step 866, loss 1.47425, acc 0.546875, prec 0.0282708, recall 0.753659
2017-12-10T14:35:09.407890: step 867, loss 2.33058, acc 0.6875, prec 0.0282463, recall 0.752741
2017-12-10T14:35:09.594287: step 868, loss 1.49545, acc 0.5625, prec 0.0282102, recall 0.752741
2017-12-10T14:35:09.779538: step 869, loss 1.55251, acc 0.59375, prec 0.0281767, recall 0.752741
2017-12-10T14:35:09.962559: step 870, loss 1.47226, acc 0.6875, prec 0.0281953, recall 0.753041
2017-12-10T14:35:10.146996: step 871, loss 1.34325, acc 0.640625, prec 0.0281658, recall 0.753041
2017-12-10T14:35:10.330384: step 872, loss 1.36386, acc 0.65625, prec 0.0281376, recall 0.753041
2017-12-10T14:35:10.513837: step 873, loss 1.12051, acc 0.65625, prec 0.0281095, recall 0.753041
2017-12-10T14:35:10.697337: step 874, loss 1.67646, acc 0.75, prec 0.0281773, recall 0.753641
2017-12-10T14:35:10.880984: step 875, loss 5.62312, acc 0.71875, prec 0.028245, recall 0.752415
2017-12-10T14:35:11.068913: step 876, loss 0.863609, acc 0.75, prec 0.0284006, recall 0.753606
2017-12-10T14:35:11.254348: step 877, loss 0.749236, acc 0.734375, prec 0.0283787, recall 0.753606
2017-12-10T14:35:11.439706: step 878, loss 1.1141, acc 0.75, prec 0.0284022, recall 0.753902
2017-12-10T14:35:11.622777: step 879, loss 1.00567, acc 0.765625, prec 0.0284268, recall 0.754197
2017-12-10T14:35:11.808784: step 880, loss 1.00937, acc 0.734375, prec 0.0285366, recall 0.755078
2017-12-10T14:35:11.993052: step 881, loss 1.45745, acc 0.65625, prec 0.0285959, recall 0.755661
2017-12-10T14:35:12.179507: step 882, loss 0.884294, acc 0.75, prec 0.0285753, recall 0.755661
2017-12-10T14:35:12.365697: step 883, loss 0.869557, acc 0.703125, prec 0.0285946, recall 0.755952
2017-12-10T14:35:12.555644: step 884, loss 10.2894, acc 0.765625, prec 0.0285779, recall 0.754157
2017-12-10T14:35:12.742587: step 885, loss 0.962685, acc 0.703125, prec 0.0285534, recall 0.754157
2017-12-10T14:35:12.929927: step 886, loss 0.785885, acc 0.765625, prec 0.0285342, recall 0.754157
2017-12-10T14:35:13.119177: step 887, loss 0.873136, acc 0.75, prec 0.0285573, recall 0.754448
2017-12-10T14:35:13.310560: step 888, loss 1.51952, acc 0.609375, prec 0.0285253, recall 0.754448
2017-12-10T14:35:13.502765: step 889, loss 0.918971, acc 0.75, prec 0.0285048, recall 0.754448
2017-12-10T14:35:13.687903: step 890, loss 1.22393, acc 0.71875, prec 0.0285254, recall 0.754739
2017-12-10T14:35:13.874589: step 891, loss 1.01736, acc 0.671875, prec 0.0285855, recall 0.755319
2017-12-10T14:35:14.066885: step 892, loss 17.999, acc 0.703125, prec 0.0285625, recall 0.754427
2017-12-10T14:35:14.255183: step 893, loss 0.79243, acc 0.78125, prec 0.0285446, recall 0.754427
2017-12-10T14:35:14.444035: step 894, loss 0.893133, acc 0.78125, prec 0.0286135, recall 0.755006
2017-12-10T14:35:14.630566: step 895, loss 1.6013, acc 0.625, prec 0.0285829, recall 0.755006
2017-12-10T14:35:14.815851: step 896, loss 1.14924, acc 0.734375, prec 0.0286045, recall 0.755294
2017-12-10T14:35:15.002551: step 897, loss 1.06724, acc 0.65625, prec 0.0285765, recall 0.755294
2017-12-10T14:35:15.187515: step 898, loss 0.383894, acc 0.828125, prec 0.0285625, recall 0.755294
2017-12-10T14:35:15.372558: step 899, loss 2.83468, acc 0.84375, prec 0.0286375, recall 0.754982
2017-12-10T14:35:15.561228: step 900, loss 0.852585, acc 0.765625, prec 0.0286184, recall 0.754982
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_3/1512934336/checkpoints/model-900

2017-12-10T14:35:16.785663: step 901, loss 1.08505, acc 0.671875, prec 0.028678, recall 0.755556
2017-12-10T14:35:16.972105: step 902, loss 0.752566, acc 0.734375, prec 0.0286563, recall 0.755556
2017-12-10T14:35:17.157264: step 903, loss 1.34445, acc 0.703125, prec 0.0286753, recall 0.755841
2017-12-10T14:35:17.346360: step 904, loss 0.784385, acc 0.765625, prec 0.0286562, recall 0.755841
2017-12-10T14:35:17.530574: step 905, loss 0.728667, acc 0.796875, prec 0.0286827, recall 0.756126
2017-12-10T14:35:17.719023: step 906, loss 0.832127, acc 0.671875, prec 0.028699, recall 0.75641
2017-12-10T14:35:17.904589: step 907, loss 0.481277, acc 0.8125, prec 0.0287267, recall 0.756694
2017-12-10T14:35:18.092080: step 908, loss 0.443413, acc 0.859375, prec 0.0288011, recall 0.757259
2017-12-10T14:35:18.275987: step 909, loss 0.373174, acc 0.890625, prec 0.0288351, recall 0.757541
2017-12-10T14:35:18.461371: step 910, loss 0.455588, acc 0.828125, prec 0.028864, recall 0.757822
2017-12-10T14:35:18.650315: step 911, loss 1.09825, acc 0.890625, prec 0.0288979, recall 0.758102
2017-12-10T14:35:18.838181: step 912, loss 1.05038, acc 0.875, prec 0.0289305, recall 0.758381
2017-12-10T14:35:19.023529: step 913, loss 0.644521, acc 0.796875, prec 0.028914, recall 0.758381
2017-12-10T14:35:19.214154: step 914, loss 0.33584, acc 0.890625, prec 0.028905, recall 0.758381
2017-12-10T14:35:19.401246: step 915, loss 0.618324, acc 0.8125, prec 0.0288898, recall 0.758381
2017-12-10T14:35:19.589576: step 916, loss 0.408688, acc 0.875, prec 0.0289651, recall 0.758939
2017-12-10T14:35:19.773938: step 917, loss 0.946841, acc 0.828125, prec 0.0290365, recall 0.759494
2017-12-10T14:35:19.961699: step 918, loss 2.7961, acc 0.90625, prec 0.0290301, recall 0.758621
2017-12-10T14:35:20.153905: step 919, loss 0.218993, acc 0.890625, prec 0.0290212, recall 0.758621
2017-12-10T14:35:20.339627: step 920, loss 0.450816, acc 0.90625, prec 0.0290562, recall 0.758898
2017-12-10T14:35:20.528037: step 921, loss 0.492495, acc 0.84375, prec 0.0290435, recall 0.758898
2017-12-10T14:35:20.715312: step 922, loss 1.96594, acc 0.78125, prec 0.0290695, recall 0.758305
2017-12-10T14:35:20.903157: step 923, loss 0.367861, acc 0.890625, prec 0.0290606, recall 0.758305
2017-12-10T14:35:21.088654: step 924, loss 2.60671, acc 0.8125, prec 0.0290892, recall 0.757714
2017-12-10T14:35:21.277105: step 925, loss 0.677983, acc 0.8125, prec 0.0290738, recall 0.757714
2017-12-10T14:35:21.464805: step 926, loss 0.984695, acc 0.75, prec 0.029096, recall 0.757991
2017-12-10T14:35:21.647379: step 927, loss 0.863902, acc 0.734375, prec 0.0291169, recall 0.758267
2017-12-10T14:35:21.833133: step 928, loss 2.31627, acc 0.65625, prec 0.0290901, recall 0.757403
2017-12-10T14:35:22.018561: step 929, loss 0.916602, acc 0.703125, prec 0.029066, recall 0.757403
2017-12-10T14:35:22.206192: step 930, loss 1.06425, acc 0.71875, prec 0.0291279, recall 0.757955
2017-12-10T14:35:22.393660: step 931, loss 3.39833, acc 0.671875, prec 0.0291025, recall 0.757094
2017-12-10T14:35:22.584123: step 932, loss 1.44898, acc 0.625, prec 0.029072, recall 0.757094
2017-12-10T14:35:22.769318: step 933, loss 0.77293, acc 0.78125, prec 0.0290543, recall 0.757094
2017-12-10T14:35:22.955566: step 934, loss 1.18691, acc 0.734375, prec 0.0290328, recall 0.757094
2017-12-10T14:35:23.139257: step 935, loss 0.821106, acc 0.71875, prec 0.0290101, recall 0.757094
2017-12-10T14:35:23.328117: step 936, loss 0.953428, acc 0.734375, prec 0.0289887, recall 0.757094
2017-12-10T14:35:23.513285: step 937, loss 0.564679, acc 0.75, prec 0.0289685, recall 0.757094
2017-12-10T14:35:23.700948: step 938, loss 1.11481, acc 0.734375, prec 0.0289893, recall 0.75737
2017-12-10T14:35:23.885511: step 939, loss 0.766107, acc 0.71875, prec 0.0289667, recall 0.75737
2017-12-10T14:35:24.067653: step 940, loss 0.793896, acc 0.765625, prec 0.0289478, recall 0.75737
2017-12-10T14:35:24.256732: step 941, loss 0.760965, acc 0.78125, prec 0.0289303, recall 0.75737
2017-12-10T14:35:24.443948: step 942, loss 0.441167, acc 0.84375, prec 0.0289177, recall 0.75737
2017-12-10T14:35:24.629531: step 943, loss 3.68754, acc 0.859375, prec 0.0289077, recall 0.756512
2017-12-10T14:35:24.823421: step 944, loss 0.605433, acc 0.90625, prec 0.0289843, recall 0.757062
2017-12-10T14:35:25.007915: step 945, loss 0.599234, acc 0.84375, prec 0.0290137, recall 0.757336
2017-12-10T14:35:25.192242: step 946, loss 0.609506, acc 0.828125, prec 0.0289999, recall 0.757336
2017-12-10T14:35:25.377197: step 947, loss 0.383939, acc 0.890625, prec 0.0290331, recall 0.75761
2017-12-10T14:35:25.563133: step 948, loss 0.682346, acc 0.765625, prec 0.0290562, recall 0.757883
2017-12-10T14:35:25.750577: step 949, loss 8.07949, acc 0.78125, prec 0.0290412, recall 0.75618
2017-12-10T14:35:25.945459: step 950, loss 0.500183, acc 0.8125, prec 0.0290261, recall 0.75618
2017-12-10T14:35:26.133117: step 951, loss 3.49813, acc 0.84375, prec 0.0290986, recall 0.755879
2017-12-10T14:35:26.322978: step 952, loss 1.22874, acc 0.65625, prec 0.029071, recall 0.755879
2017-12-10T14:35:26.509991: step 953, loss 0.916722, acc 0.796875, prec 0.0290548, recall 0.755879
2017-12-10T14:35:26.696834: step 954, loss 1.46037, acc 0.6875, prec 0.0290715, recall 0.756152
2017-12-10T14:35:26.880689: step 955, loss 0.905691, acc 0.734375, prec 0.029092, recall 0.756425
2017-12-10T14:35:27.068652: step 956, loss 0.828303, acc 0.8125, prec 0.0291187, recall 0.756696
2017-12-10T14:35:27.254719: step 957, loss 0.959889, acc 0.734375, prec 0.0290975, recall 0.756696
2017-12-10T14:35:27.444257: step 958, loss 10.3722, acc 0.765625, prec 0.0290812, recall 0.755011
2017-12-10T14:35:27.632116: step 959, loss 1.13059, acc 0.734375, prec 0.02906, recall 0.755011
2017-12-10T14:35:27.814875: step 960, loss 6.59738, acc 0.75, prec 0.0290414, recall 0.754171
2017-12-10T14:35:28.002055: step 961, loss 1.20561, acc 0.65625, prec 0.029014, recall 0.754171
2017-12-10T14:35:28.190997: step 962, loss 1.57792, acc 0.5625, prec 0.0289793, recall 0.754171
2017-12-10T14:35:28.377807: step 963, loss 1.40498, acc 0.640625, prec 0.0289509, recall 0.754171
2017-12-10T14:35:28.563878: step 964, loss 1.51749, acc 0.625, prec 0.0289626, recall 0.754444
2017-12-10T14:35:28.749974: step 965, loss 1.46395, acc 0.5625, prec 0.0289281, recall 0.754444
2017-12-10T14:35:28.940861: step 966, loss 1.88672, acc 0.59375, prec 0.0289374, recall 0.754717
2017-12-10T14:35:29.136056: step 967, loss 1.56308, acc 0.640625, prec 0.0289091, recall 0.754717
2017-12-10T14:35:29.318700: step 968, loss 2.58389, acc 0.46875, prec 0.0288674, recall 0.754717
2017-12-10T14:35:29.502034: step 969, loss 1.34887, acc 0.546875, prec 0.0288319, recall 0.754717
2017-12-10T14:35:29.688866: step 970, loss 1.58365, acc 0.65625, prec 0.0288462, recall 0.754989
2017-12-10T14:35:29.875364: step 971, loss 1.23947, acc 0.671875, prec 0.0288616, recall 0.75526
2017-12-10T14:35:30.064269: step 972, loss 0.703297, acc 0.796875, prec 0.0288868, recall 0.755531
2017-12-10T14:35:30.250966: step 973, loss 1.18591, acc 0.671875, prec 0.0289843, recall 0.75634
2017-12-10T14:35:30.435976: step 974, loss 0.768402, acc 0.796875, prec 0.0290504, recall 0.756876
2017-12-10T14:35:30.618964: step 975, loss 1.59383, acc 0.6875, prec 0.0290668, recall 0.757143
2017-12-10T14:35:30.805109: step 976, loss 1.04042, acc 0.71875, prec 0.0290448, recall 0.757143
2017-12-10T14:35:30.993589: step 977, loss 0.798349, acc 0.8125, prec 0.029071, recall 0.757409
2017-12-10T14:35:31.188666: step 978, loss 0.578194, acc 0.765625, prec 0.0290526, recall 0.757409
2017-12-10T14:35:31.377709: step 979, loss 0.674744, acc 0.78125, prec 0.0290355, recall 0.757409
2017-12-10T14:35:31.567730: step 980, loss 0.850571, acc 0.890625, prec 0.0290678, recall 0.757675
2017-12-10T14:35:31.754020: step 981, loss 1.98427, acc 0.84375, prec 0.0290568, recall 0.756846
2017-12-10T14:35:31.941580: step 982, loss 5.87347, acc 0.84375, prec 0.0290458, recall 0.756018
2017-12-10T14:35:32.133618: step 983, loss 0.840923, acc 0.859375, prec 0.0291164, recall 0.75655
2017-12-10T14:35:32.322026: step 984, loss 7.60663, acc 0.90625, prec 0.0291511, recall 0.755991
2017-12-10T14:35:32.513405: step 985, loss 0.371026, acc 0.875, prec 0.0291413, recall 0.755991
2017-12-10T14:35:32.700812: step 986, loss 1.66011, acc 0.78125, prec 0.0292464, recall 0.756786
2017-12-10T14:35:32.887734: step 987, loss 0.948346, acc 0.71875, prec 0.029265, recall 0.75705
2017-12-10T14:35:33.077813: step 988, loss 0.825503, acc 0.765625, prec 0.0292873, recall 0.757313
2017-12-10T14:35:33.267053: step 989, loss 1.17889, acc 0.71875, prec 0.0293059, recall 0.757576
2017-12-10T14:35:33.451963: step 990, loss 0.809866, acc 0.765625, prec 0.0293281, recall 0.757838
2017-12-10T14:35:33.641341: step 991, loss 1.03917, acc 0.71875, prec 0.029306, recall 0.757838
2017-12-10T14:35:33.824949: step 992, loss 0.99791, acc 0.734375, prec 0.0293258, recall 0.758099
2017-12-10T14:35:34.015308: step 993, loss 1.01062, acc 0.703125, prec 0.0293025, recall 0.758099
2017-12-10T14:35:34.182520: step 994, loss 1.22366, acc 0.673077, prec 0.0292817, recall 0.758099
2017-12-10T14:35:34.377601: step 995, loss 1.45246, acc 0.65625, prec 0.0293762, recall 0.75888
2017-12-10T14:35:34.567666: step 996, loss 1.49388, acc 0.71875, prec 0.0293946, recall 0.75914
2017-12-10T14:35:34.752521: step 997, loss 1.26358, acc 0.71875, prec 0.029413, recall 0.759399
2017-12-10T14:35:34.941292: step 998, loss 1.16855, acc 0.703125, prec 0.0293898, recall 0.759399
2017-12-10T14:35:35.127904: step 999, loss 0.927248, acc 0.734375, prec 0.029369, recall 0.759399
2017-12-10T14:35:35.314225: step 1000, loss 0.44935, acc 0.84375, prec 0.0293568, recall 0.759399
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_3/1512934336/checkpoints/model-1000

2017-12-10T14:35:36.480307: step 1001, loss 0.470647, acc 0.828125, prec 0.0293434, recall 0.759399
2017-12-10T14:35:36.665138: step 1002, loss 0.813785, acc 0.78125, prec 0.0293666, recall 0.759657
2017-12-10T14:35:36.849328: step 1003, loss 0.467939, acc 0.84375, prec 0.0293545, recall 0.759657
2017-12-10T14:35:37.035899: step 1004, loss 0.708809, acc 0.78125, prec 0.0293776, recall 0.759914
2017-12-10T14:35:37.219278: step 1005, loss 0.310888, acc 0.90625, prec 0.0293703, recall 0.759914
2017-12-10T14:35:37.407470: step 1006, loss 0.521871, acc 0.859375, prec 0.0293594, recall 0.759914
2017-12-10T14:35:37.593767: step 1007, loss 0.500338, acc 0.828125, prec 0.029346, recall 0.759914
2017-12-10T14:35:37.782359: step 1008, loss 0.624989, acc 0.875, prec 0.0294568, recall 0.760684
2017-12-10T14:35:37.967890: step 1009, loss 3.66634, acc 0.828125, prec 0.0294446, recall 0.759872
2017-12-10T14:35:38.157603: step 1010, loss 1.11565, acc 0.875, prec 0.029475, recall 0.760128
2017-12-10T14:35:38.347478: step 1011, loss 0.382008, acc 0.875, prec 0.0295054, recall 0.760383
2017-12-10T14:35:38.535629: step 1012, loss 2.71574, acc 0.890625, prec 0.0295782, recall 0.760085
2017-12-10T14:35:38.725606: step 1013, loss 0.630308, acc 0.84375, prec 0.0296061, recall 0.760339
2017-12-10T14:35:38.909824: step 1014, loss 0.374639, acc 0.890625, prec 0.0296776, recall 0.760847
2017-12-10T14:35:39.096732: step 1015, loss 0.395993, acc 0.921875, prec 0.0296715, recall 0.760847
2017-12-10T14:35:39.284780: step 1016, loss 1.0046, acc 0.78125, prec 0.0296944, recall 0.761099
2017-12-10T14:35:39.476770: step 1017, loss 0.413078, acc 0.78125, prec 0.0296773, recall 0.761099
2017-12-10T14:35:39.669651: step 1018, loss 1.80275, acc 0.890625, prec 0.0297487, recall 0.761603
2017-12-10T14:35:39.856400: step 1019, loss 0.600958, acc 0.75, prec 0.029809, recall 0.762105
2017-12-10T14:35:40.043198: step 1020, loss 0.472198, acc 0.8125, prec 0.0297942, recall 0.762105
2017-12-10T14:35:40.229871: step 1021, loss 0.802342, acc 0.75, prec 0.0298145, recall 0.762355
2017-12-10T14:35:40.418341: step 1022, loss 0.635977, acc 0.78125, prec 0.0298373, recall 0.762605
2017-12-10T14:35:40.604378: step 1023, loss 0.74283, acc 0.8125, prec 0.0298624, recall 0.762854
2017-12-10T14:35:40.790181: step 1024, loss 0.558793, acc 0.890625, prec 0.0298936, recall 0.763103
2017-12-10T14:35:40.983218: step 1025, loss 0.84863, acc 0.765625, prec 0.0298752, recall 0.763103
2017-12-10T14:35:41.166710: step 1026, loss 0.981637, acc 0.8125, prec 0.0299003, recall 0.763351
2017-12-10T14:35:41.350384: step 1027, loss 0.674627, acc 0.75, prec 0.0298807, recall 0.763351
2017-12-10T14:35:41.538635: step 1028, loss 0.859265, acc 0.796875, prec 0.0298648, recall 0.763351
2017-12-10T14:35:41.725194: step 1029, loss 0.848643, acc 0.71875, prec 0.0299619, recall 0.764092
2017-12-10T14:35:41.909114: step 1030, loss 0.403742, acc 0.859375, prec 0.03007, recall 0.764828
2017-12-10T14:35:42.098310: step 1031, loss 0.529363, acc 0.796875, prec 0.0301729, recall 0.76556
2017-12-10T14:35:42.282460: step 1032, loss 0.525211, acc 0.828125, prec 0.030199, recall 0.765803
2017-12-10T14:35:42.471057: step 1033, loss 4.60797, acc 0.78125, prec 0.0302226, recall 0.765253
2017-12-10T14:35:42.660273: step 1034, loss 0.597748, acc 0.859375, prec 0.0302907, recall 0.765738
2017-12-10T14:35:42.847207: step 1035, loss 0.641729, acc 0.84375, prec 0.0303179, recall 0.765979
2017-12-10T14:35:43.033519: step 1036, loss 4.46794, acc 0.8125, prec 0.0303451, recall 0.764645
2017-12-10T14:35:43.220185: step 1037, loss 0.630279, acc 0.828125, prec 0.0303314, recall 0.764645
2017-12-10T14:35:43.409061: step 1038, loss 1.8544, acc 0.78125, prec 0.0303154, recall 0.76386
2017-12-10T14:35:43.594663: step 1039, loss 1.56826, acc 0.671875, prec 0.0303289, recall 0.764103
2017-12-10T14:35:43.785150: step 1040, loss 1.38759, acc 0.703125, prec 0.0303449, recall 0.764344
2017-12-10T14:35:43.972633: step 1041, loss 0.86023, acc 0.71875, prec 0.0303622, recall 0.764585
2017-12-10T14:35:44.164698: step 1042, loss 1.14284, acc 0.65625, prec 0.0303744, recall 0.764826
2017-12-10T14:35:44.351215: step 1043, loss 0.933824, acc 0.6875, prec 0.0303891, recall 0.765066
2017-12-10T14:35:44.536321: step 1044, loss 2.16768, acc 0.703125, prec 0.0304443, recall 0.765545
2017-12-10T14:35:44.723171: step 1045, loss 1.01601, acc 0.75, prec 0.0304246, recall 0.765545
2017-12-10T14:35:44.907380: step 1046, loss 2.99855, acc 0.703125, prec 0.0304024, recall 0.764766
2017-12-10T14:35:45.098662: step 1047, loss 1.41779, acc 0.609375, prec 0.0303717, recall 0.764766
2017-12-10T14:35:45.284305: step 1048, loss 1.97014, acc 0.515625, prec 0.030412, recall 0.765244
2017-12-10T14:35:45.472320: step 1049, loss 0.867758, acc 0.734375, prec 0.0303911, recall 0.765244
2017-12-10T14:35:45.658898: step 1050, loss 1.39533, acc 0.671875, prec 0.0304045, recall 0.765482
2017-12-10T14:35:45.843930: step 1051, loss 1.17962, acc 0.6875, prec 0.03038, recall 0.765482
2017-12-10T14:35:46.026272: step 1052, loss 2.56932, acc 0.59375, prec 0.0303872, recall 0.76572
2017-12-10T14:35:46.216238: step 1053, loss 1.41232, acc 0.609375, prec 0.0303566, recall 0.76572
2017-12-10T14:35:46.403481: step 1054, loss 1.40328, acc 0.671875, prec 0.030331, recall 0.76572
2017-12-10T14:35:46.586929: step 1055, loss 0.955849, acc 0.734375, prec 0.0303103, recall 0.76572
2017-12-10T14:35:46.776693: step 1056, loss 0.672392, acc 0.828125, prec 0.0303748, recall 0.766194
2017-12-10T14:35:46.963364: step 1057, loss 0.951501, acc 0.765625, prec 0.0303954, recall 0.766431
2017-12-10T14:35:47.150939: step 1058, loss 0.782903, acc 0.78125, prec 0.0303783, recall 0.766431
2017-12-10T14:35:47.340685: step 1059, loss 1.06655, acc 0.65625, prec 0.0303516, recall 0.766431
2017-12-10T14:35:47.528385: step 1060, loss 0.44472, acc 0.875, prec 0.0303418, recall 0.766431
2017-12-10T14:35:47.714481: step 1061, loss 0.396923, acc 0.84375, prec 0.0303685, recall 0.766667
2017-12-10T14:35:47.903182: step 1062, loss 5.21407, acc 0.8125, prec 0.0303551, recall 0.765893
2017-12-10T14:35:48.092641: step 1063, loss 3.96811, acc 0.8125, prec 0.0303418, recall 0.765121
2017-12-10T14:35:48.282399: step 1064, loss 0.542889, acc 0.828125, prec 0.0303285, recall 0.765121
2017-12-10T14:35:48.471139: step 1065, loss 0.478504, acc 0.8125, prec 0.0303914, recall 0.765594
2017-12-10T14:35:48.657193: step 1066, loss 0.568676, acc 0.765625, prec 0.0303732, recall 0.765594
2017-12-10T14:35:48.844627: step 1067, loss 0.494119, acc 0.8125, prec 0.0303586, recall 0.765594
2017-12-10T14:35:49.031321: step 1068, loss 0.787342, acc 0.796875, prec 0.0303816, recall 0.765829
2017-12-10T14:35:49.216697: step 1069, loss 0.302184, acc 0.921875, prec 0.0304142, recall 0.766064
2017-12-10T14:35:49.406569: step 1070, loss 0.480401, acc 0.8125, prec 0.0303996, recall 0.766064
2017-12-10T14:35:49.592682: step 1071, loss 0.867073, acc 0.8125, prec 0.0303851, recall 0.766064
2017-12-10T14:35:49.777796: step 1072, loss 0.410497, acc 0.921875, prec 0.0304176, recall 0.766299
2017-12-10T14:35:49.966237: step 1073, loss 0.67683, acc 0.78125, prec 0.0304007, recall 0.766299
2017-12-10T14:35:50.152233: step 1074, loss 0.530458, acc 0.859375, prec 0.0303898, recall 0.766299
2017-12-10T14:35:50.338718: step 1075, loss 0.504453, acc 0.84375, prec 0.0304163, recall 0.766533
2017-12-10T14:35:50.525027: step 1076, loss 0.621573, acc 0.875, prec 0.0304452, recall 0.766767
2017-12-10T14:35:50.711206: step 1077, loss 0.511798, acc 0.84375, prec 0.0304331, recall 0.766767
2017-12-10T14:35:50.897532: step 1078, loss 2.59714, acc 0.859375, prec 0.0304619, recall 0.766234
2017-12-10T14:35:51.085624: step 1079, loss 2.03639, acc 0.84375, prec 0.030451, recall 0.765469
2017-12-10T14:35:51.274514: step 1080, loss 0.388167, acc 0.84375, prec 0.0305159, recall 0.765936
2017-12-10T14:35:51.460884: step 1081, loss 0.684836, acc 0.828125, prec 0.030541, recall 0.766169
2017-12-10T14:35:51.648579: step 1082, loss 1.17083, acc 0.796875, prec 0.0306406, recall 0.766865
2017-12-10T14:35:51.838408: step 1083, loss 0.720518, acc 0.78125, prec 0.0306236, recall 0.766865
2017-12-10T14:35:52.026433: step 1084, loss 0.351164, acc 0.859375, prec 0.0306126, recall 0.766865
2017-12-10T14:35:52.214776: step 1085, loss 0.642757, acc 0.703125, prec 0.030628, recall 0.767096
2017-12-10T14:35:52.402789: step 1086, loss 0.228905, acc 0.9375, prec 0.0307382, recall 0.767787
2017-12-10T14:35:52.587071: step 1087, loss 0.565536, acc 0.78125, prec 0.0307595, recall 0.768016
2017-12-10T14:35:52.774212: step 1088, loss 0.558591, acc 0.828125, prec 0.0307461, recall 0.768016
2017-12-10T14:35:52.963957: step 1089, loss 0.196969, acc 0.90625, prec 0.0307771, recall 0.768245
2017-12-10T14:35:53.148722: step 1090, loss 0.943911, acc 0.78125, prec 0.0307601, recall 0.768245
2017-12-10T14:35:53.337169: step 1091, loss 0.769635, acc 0.828125, prec 0.0307468, recall 0.768245
2017-12-10T14:35:53.526629: step 1092, loss 0.492023, acc 0.859375, prec 0.0307358, recall 0.768245
2017-12-10T14:35:53.712760: step 1093, loss 0.98002, acc 0.890625, prec 0.0308038, recall 0.768701
2017-12-10T14:35:53.903555: step 1094, loss 0.975098, acc 0.78125, prec 0.0307868, recall 0.768701
2017-12-10T14:35:54.087967: step 1095, loss 0.675576, acc 0.890625, prec 0.0308929, recall 0.769382
2017-12-10T14:35:54.280021: step 1096, loss 0.443436, acc 0.859375, prec 0.0309201, recall 0.769608
2017-12-10T14:35:54.466530: step 1097, loss 0.355001, acc 0.875, prec 0.0309104, recall 0.769608
2017-12-10T14:35:54.652639: step 1098, loss 0.43557, acc 0.875, prec 0.0309006, recall 0.769608
2017-12-10T14:35:54.837872: step 1099, loss 9.66023, acc 0.84375, prec 0.0308897, recall 0.768854
2017-12-10T14:35:55.032900: step 1100, loss 0.228237, acc 0.890625, prec 0.0309193, recall 0.76908
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_3/1512934336/checkpoints/model-1100

2017-12-10T14:35:56.094898: step 1101, loss 0.757571, acc 0.765625, prec 0.0309392, recall 0.769306
2017-12-10T14:35:56.284881: step 1102, loss 1.4328, acc 0.828125, prec 0.0309639, recall 0.769531
2017-12-10T14:35:56.477565: step 1103, loss 3.55829, acc 0.8125, prec 0.0309505, recall 0.76878
2017-12-10T14:35:56.667944: step 1104, loss 9.45899, acc 0.765625, prec 0.0309335, recall 0.768031
2017-12-10T14:35:56.856303: step 1105, loss 0.694052, acc 0.8125, prec 0.0309189, recall 0.768031
2017-12-10T14:35:57.040765: step 1106, loss 1.39282, acc 0.75, prec 0.0309375, recall 0.768257
2017-12-10T14:35:57.228372: step 1107, loss 0.968123, acc 0.734375, prec 0.0309549, recall 0.768483
2017-12-10T14:35:57.410658: step 1108, loss 1.44748, acc 0.640625, prec 0.030965, recall 0.768707
2017-12-10T14:35:57.597784: step 1109, loss 1.22864, acc 0.625, prec 0.0309359, recall 0.768707
2017-12-10T14:35:57.782842: step 1110, loss 1.36311, acc 0.6875, prec 0.0309117, recall 0.768707
2017-12-10T14:35:57.965698: step 1111, loss 1.73135, acc 0.671875, prec 0.030962, recall 0.769156
2017-12-10T14:35:58.152967: step 1112, loss 2.07169, acc 0.515625, prec 0.0309624, recall 0.76938
2017-12-10T14:35:58.340491: step 1113, loss 1.51111, acc 0.625, prec 0.0309335, recall 0.76938
2017-12-10T14:35:58.525645: step 1114, loss 1.16303, acc 0.640625, prec 0.0309435, recall 0.769603
2017-12-10T14:35:58.715529: step 1115, loss 1.04517, acc 0.734375, prec 0.0309607, recall 0.769826
2017-12-10T14:35:58.903226: step 1116, loss 0.708933, acc 0.734375, prec 0.0309779, recall 0.770048
2017-12-10T14:35:59.096097: step 1117, loss 1.04816, acc 0.703125, prec 0.0309551, recall 0.770048
2017-12-10T14:35:59.280261: step 1118, loss 0.846739, acc 0.734375, prec 0.0309346, recall 0.770048
2017-12-10T14:35:59.466484: step 1119, loss 0.784967, acc 0.71875, prec 0.030913, recall 0.770048
2017-12-10T14:35:59.653288: step 1120, loss 0.804347, acc 0.734375, prec 0.0309678, recall 0.770492
2017-12-10T14:35:59.840963: step 1121, loss 0.81886, acc 0.734375, prec 0.0310225, recall 0.770934
2017-12-10T14:36:00.028845: step 1122, loss 0.685246, acc 0.828125, prec 0.0310843, recall 0.771374
2017-12-10T14:36:00.218005: step 1123, loss 0.7689, acc 0.765625, prec 0.0310662, recall 0.771374
2017-12-10T14:36:00.403124: step 1124, loss 0.90453, acc 0.84375, prec 0.0310917, recall 0.771593
2017-12-10T14:36:00.589447: step 1125, loss 0.888855, acc 0.828125, prec 0.0310785, recall 0.771593
2017-12-10T14:36:00.776338: step 1126, loss 0.385129, acc 0.921875, prec 0.0311099, recall 0.771812
2017-12-10T14:36:00.967127: step 1127, loss 0.547082, acc 0.78125, prec 0.0311305, recall 0.772031
2017-12-10T14:36:01.161411: step 1128, loss 0.710576, acc 0.875, prec 0.0311957, recall 0.772467
2017-12-10T14:36:01.356001: step 1129, loss 0.501114, acc 0.84375, prec 0.0312211, recall 0.772684
2017-12-10T14:36:01.546381: step 1130, loss 0.133388, acc 0.96875, prec 0.031256, recall 0.772901
2017-12-10T14:36:01.734003: step 1131, loss 1.15653, acc 0.890625, prec 0.031285, recall 0.773117
2017-12-10T14:36:01.922357: step 1132, loss 6.16181, acc 0.9375, prec 0.0313561, recall 0.772814
2017-12-10T14:36:02.108447: step 1133, loss 1.62974, acc 0.90625, prec 0.03135, recall 0.77208
2017-12-10T14:36:02.300150: step 1134, loss 3.97531, acc 0.90625, prec 0.031344, recall 0.771347
2017-12-10T14:36:02.487541: step 1135, loss 0.211994, acc 0.90625, prec 0.0314114, recall 0.77178
2017-12-10T14:36:02.672795: step 1136, loss 0.6137, acc 0.859375, prec 0.0314378, recall 0.771996
2017-12-10T14:36:02.857733: step 1137, loss 0.911933, acc 0.71875, prec 0.031416, recall 0.771996
2017-12-10T14:36:03.044382: step 1138, loss 0.885076, acc 0.734375, prec 0.0314327, recall 0.772212
2017-12-10T14:36:03.226635: step 1139, loss 1.69402, acc 0.6875, prec 0.0314458, recall 0.772427
2017-12-10T14:36:03.412405: step 1140, loss 0.981674, acc 0.734375, prec 0.0314625, recall 0.772641
2017-12-10T14:36:03.598631: step 1141, loss 0.985377, acc 0.703125, prec 0.0314395, recall 0.772641
2017-12-10T14:36:03.783000: step 1142, loss 1.18449, acc 0.71875, prec 0.031455, recall 0.772856
2017-12-10T14:36:03.967341: step 1143, loss 1.10193, acc 0.65625, prec 0.0314656, recall 0.77307
2017-12-10T14:36:04.152476: step 1144, loss 1.02413, acc 0.6875, prec 0.0314415, recall 0.77307
2017-12-10T14:36:04.339356: step 1145, loss 1.82037, acc 0.546875, prec 0.0314437, recall 0.773283
2017-12-10T14:36:04.528007: step 1146, loss 1.34336, acc 0.65625, prec 0.0314172, recall 0.773283
2017-12-10T14:36:04.715412: step 1147, loss 0.924207, acc 0.75, prec 0.031509, recall 0.773921
2017-12-10T14:36:04.903306: step 1148, loss 0.725749, acc 0.796875, prec 0.0315303, recall 0.774133
2017-12-10T14:36:05.086862: step 1149, loss 7.1341, acc 0.734375, prec 0.0315111, recall 0.773408
2017-12-10T14:36:05.273992: step 1150, loss 1.39083, acc 0.625, prec 0.0315561, recall 0.773832
2017-12-10T14:36:05.459017: step 1151, loss 1.14497, acc 0.6875, prec 0.031532, recall 0.773832
2017-12-10T14:36:05.643429: step 1152, loss 0.955531, acc 0.703125, prec 0.031583, recall 0.774254
2017-12-10T14:36:05.829972: step 1153, loss 0.568756, acc 0.828125, prec 0.0316434, recall 0.774674
2017-12-10T14:36:06.020727: step 1154, loss 0.948418, acc 0.71875, prec 0.0317322, recall 0.775302
2017-12-10T14:36:06.210183: step 1155, loss 2.38796, acc 0.71875, prec 0.0318575, recall 0.776133
2017-12-10T14:36:06.401516: step 1156, loss 0.934089, acc 0.703125, prec 0.0318713, recall 0.77634
2017-12-10T14:36:06.586138: step 1157, loss 0.599992, acc 0.828125, prec 0.0318947, recall 0.776547
2017-12-10T14:36:06.769081: step 1158, loss 0.32279, acc 0.875, prec 0.031885, recall 0.776547
2017-12-10T14:36:06.954741: step 1159, loss 1.01024, acc 0.71875, prec 0.0318633, recall 0.776547
2017-12-10T14:36:07.137008: step 1160, loss 0.663297, acc 0.8125, prec 0.0318488, recall 0.776547
2017-12-10T14:36:07.323647: step 1161, loss 1.31305, acc 0.78125, prec 0.0319419, recall 0.777164
2017-12-10T14:36:07.520504: step 1162, loss 0.816591, acc 0.8125, prec 0.0319274, recall 0.777164
2017-12-10T14:36:07.705364: step 1163, loss 3.24554, acc 0.765625, prec 0.0319837, recall 0.77686
2017-12-10T14:36:07.892158: step 1164, loss 0.634085, acc 0.765625, prec 0.0319655, recall 0.77686
2017-12-10T14:36:08.078913: step 1165, loss 0.831766, acc 0.796875, prec 0.0319864, recall 0.777064
2017-12-10T14:36:08.267434: step 1166, loss 2.93832, acc 0.734375, prec 0.0320036, recall 0.776557
2017-12-10T14:36:08.453902: step 1167, loss 0.560513, acc 0.828125, prec 0.0320634, recall 0.776965
2017-12-10T14:36:08.639915: step 1168, loss 4.44044, acc 0.65625, prec 0.0320745, recall 0.77646
2017-12-10T14:36:08.827007: step 1169, loss 0.974266, acc 0.65625, prec 0.0320479, recall 0.77646
2017-12-10T14:36:09.014069: step 1170, loss 0.896357, acc 0.703125, prec 0.032025, recall 0.77646
2017-12-10T14:36:09.201127: step 1171, loss 0.761766, acc 0.671875, prec 0.0319997, recall 0.77646
2017-12-10T14:36:09.384644: step 1172, loss 1.19545, acc 0.671875, prec 0.0320108, recall 0.776664
2017-12-10T14:36:09.572396: step 1173, loss 0.824912, acc 0.78125, prec 0.0320667, recall 0.77707
2017-12-10T14:36:09.757424: step 1174, loss 1.39779, acc 0.71875, prec 0.0320813, recall 0.777273
2017-12-10T14:36:09.943143: step 1175, loss 1.46414, acc 0.71875, prec 0.0321686, recall 0.777879
2017-12-10T14:36:10.130984: step 1176, loss 0.655124, acc 0.734375, prec 0.0321843, recall 0.77808
2017-12-10T14:36:10.314288: step 1177, loss 1.27312, acc 0.65625, prec 0.0321941, recall 0.778281
2017-12-10T14:36:10.501228: step 1178, loss 0.563878, acc 0.8125, prec 0.0322158, recall 0.778481
2017-12-10T14:36:10.688496: step 1179, loss 0.624229, acc 0.765625, prec 0.0321977, recall 0.778481
2017-12-10T14:36:10.874424: step 1180, loss 0.629272, acc 0.78125, prec 0.0322894, recall 0.77908
2017-12-10T14:36:11.062716: step 1181, loss 2.68994, acc 0.78125, prec 0.0322737, recall 0.778378
2017-12-10T14:36:11.247663: step 1182, loss 0.929371, acc 0.796875, prec 0.0322581, recall 0.778378
2017-12-10T14:36:11.430703: step 1183, loss 1.98559, acc 0.78125, prec 0.0322424, recall 0.777678
2017-12-10T14:36:11.615455: step 1184, loss 1.21495, acc 0.65625, prec 0.032216, recall 0.777678
2017-12-10T14:36:11.801917: step 1185, loss 0.789223, acc 0.734375, prec 0.0322316, recall 0.777878
2017-12-10T14:36:11.987959: step 1186, loss 0.915514, acc 0.796875, prec 0.032216, recall 0.777878
2017-12-10T14:36:12.171789: step 1187, loss 0.673815, acc 0.8125, prec 0.0322737, recall 0.778277
2017-12-10T14:36:12.359633: step 1188, loss 0.38128, acc 0.890625, prec 0.0322653, recall 0.778277
2017-12-10T14:36:12.543807: step 1189, loss 0.396859, acc 0.890625, prec 0.0322569, recall 0.778277
2017-12-10T14:36:12.729996: step 1190, loss 0.488899, acc 0.828125, prec 0.0322437, recall 0.778277
2017-12-10T14:36:12.922311: step 1191, loss 0.589526, acc 0.796875, prec 0.0323, recall 0.778674
2017-12-10T14:36:13.106693: step 1192, loss 0.623227, acc 0.828125, prec 0.0323587, recall 0.77907
2017-12-10T14:36:13.295114: step 1193, loss 0.373782, acc 0.875, prec 0.0323491, recall 0.77907
2017-12-10T14:36:13.479354: step 1194, loss 0.525956, acc 0.875, prec 0.0323754, recall 0.779267
2017-12-10T14:36:13.663659: step 1195, loss 0.435157, acc 0.859375, prec 0.0323646, recall 0.779267
2017-12-10T14:36:13.851581: step 1196, loss 0.327667, acc 0.9375, prec 0.0323957, recall 0.779464
2017-12-10T14:36:14.042039: step 1197, loss 0.313511, acc 0.921875, prec 0.0323897, recall 0.779464
2017-12-10T14:36:14.232537: step 1198, loss 0.52275, acc 0.90625, prec 0.0324902, recall 0.780053
2017-12-10T14:36:14.419295: step 1199, loss 0.378534, acc 0.90625, prec 0.0325188, recall 0.780249
2017-12-10T14:36:14.605525: step 1200, loss 0.116462, acc 0.96875, prec 0.0325164, recall 0.780249
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_3/1512934336/checkpoints/model-1200

2017-12-10T14:36:15.823584: step 1201, loss 0.440686, acc 0.875, prec 0.0326143, recall 0.780834
2017-12-10T14:36:16.012794: step 1202, loss 1.40315, acc 0.875, prec 0.0327122, recall 0.781416
2017-12-10T14:36:16.203219: step 1203, loss 1.87978, acc 0.90625, prec 0.032742, recall 0.780919
2017-12-10T14:36:16.394182: step 1204, loss 0.289158, acc 0.875, prec 0.0327681, recall 0.781112
2017-12-10T14:36:16.581660: step 1205, loss 0.365995, acc 0.875, prec 0.0327942, recall 0.781305
2017-12-10T14:36:16.766349: step 1206, loss 0.408872, acc 0.90625, prec 0.0328227, recall 0.781498
2017-12-10T14:36:16.955127: step 1207, loss 0.281442, acc 0.90625, prec 0.0328154, recall 0.781498
2017-12-10T14:36:17.142042: step 1208, loss 4.90349, acc 0.84375, prec 0.0328045, recall 0.78081
2017-12-10T14:36:17.330572: step 1209, loss 0.520564, acc 0.84375, prec 0.0327923, recall 0.78081
2017-12-10T14:36:17.518720: step 1210, loss 0.477058, acc 0.828125, prec 0.032779, recall 0.78081
2017-12-10T14:36:17.703330: step 1211, loss 0.714473, acc 0.765625, prec 0.0327609, recall 0.78081
2017-12-10T14:36:17.894167: step 1212, loss 0.48584, acc 0.75, prec 0.0327415, recall 0.78081
2017-12-10T14:36:18.078089: step 1213, loss 0.586762, acc 0.796875, prec 0.0327258, recall 0.78081
2017-12-10T14:36:18.262026: step 1214, loss 0.866383, acc 0.78125, prec 0.0327089, recall 0.78081
2017-12-10T14:36:18.449149: step 1215, loss 0.642706, acc 0.75, prec 0.0326896, recall 0.78081
2017-12-10T14:36:18.637187: step 1216, loss 0.683793, acc 0.765625, prec 0.0326716, recall 0.78081
2017-12-10T14:36:18.820832: step 1217, loss 0.432869, acc 0.828125, prec 0.0326583, recall 0.78081
2017-12-10T14:36:19.003881: step 1218, loss 0.801062, acc 0.75, prec 0.0326391, recall 0.78081
2017-12-10T14:36:19.193190: step 1219, loss 0.29565, acc 0.875, prec 0.0326295, recall 0.78081
2017-12-10T14:36:19.376924: step 1220, loss 1.08945, acc 0.671875, prec 0.0326043, recall 0.78081
2017-12-10T14:36:19.565921: step 1221, loss 0.463992, acc 0.859375, prec 0.0325935, recall 0.78081
2017-12-10T14:36:19.749463: step 1222, loss 1.22993, acc 0.859375, prec 0.0326183, recall 0.781003
2017-12-10T14:36:19.940998: step 1223, loss 0.473255, acc 0.875, prec 0.0326087, recall 0.781003
2017-12-10T14:36:20.125942: step 1224, loss 0.480069, acc 0.8125, prec 0.0326298, recall 0.781195
2017-12-10T14:36:20.312419: step 1225, loss 0.475588, acc 0.875, prec 0.0326203, recall 0.781195
2017-12-10T14:36:20.498925: step 1226, loss 0.446319, acc 0.84375, prec 0.0326083, recall 0.781195
2017-12-10T14:36:20.686766: step 1227, loss 0.371843, acc 0.890625, prec 0.0325999, recall 0.781195
2017-12-10T14:36:20.870759: step 1228, loss 1.36778, acc 0.953125, prec 0.0325975, recall 0.780509
2017-12-10T14:36:21.058403: step 1229, loss 0.406139, acc 0.890625, prec 0.0325892, recall 0.780509
2017-12-10T14:36:21.249386: step 1230, loss 0.328715, acc 0.921875, prec 0.0326187, recall 0.780702
2017-12-10T14:36:21.436494: step 1231, loss 0.291687, acc 0.84375, prec 0.0326067, recall 0.780702
2017-12-10T14:36:21.623683: step 1232, loss 0.368904, acc 0.84375, prec 0.0325948, recall 0.780702
2017-12-10T14:36:21.812705: step 1233, loss 0.205566, acc 0.9375, prec 0.03259, recall 0.780702
2017-12-10T14:36:22.001888: step 1234, loss 4.28965, acc 0.890625, prec 0.0326182, recall 0.78021
2017-12-10T14:36:22.189237: step 1235, loss 0.228811, acc 0.9375, prec 0.0326489, recall 0.780402
2017-12-10T14:36:22.374424: step 1236, loss 3.97519, acc 0.921875, prec 0.0326441, recall 0.77972
2017-12-10T14:36:22.561530: step 1237, loss 0.600904, acc 0.828125, prec 0.0326663, recall 0.779913
2017-12-10T14:36:22.747665: step 1238, loss 0.759128, acc 0.84375, prec 0.0327605, recall 0.780488
2017-12-10T14:36:22.936526: step 1239, loss 0.592402, acc 0.859375, prec 0.0327497, recall 0.780488
2017-12-10T14:36:23.121577: step 1240, loss 0.445556, acc 0.828125, prec 0.0327719, recall 0.780679
2017-12-10T14:36:23.312168: step 1241, loss 0.86287, acc 0.828125, prec 0.0327587, recall 0.780679
2017-12-10T14:36:23.501064: step 1242, loss 0.506923, acc 0.765625, prec 0.0327408, recall 0.780679
2017-12-10T14:36:23.689543: step 1243, loss 1.72763, acc 0.8125, prec 0.0327971, recall 0.78106
2017-12-10T14:36:23.879003: step 1244, loss 0.710901, acc 0.734375, prec 0.0328472, recall 0.78144
2017-12-10T14:36:24.065795: step 1245, loss 0.495233, acc 0.859375, prec 0.0328717, recall 0.781629
2017-12-10T14:36:24.249639: step 1246, loss 0.503974, acc 0.890625, prec 0.0328986, recall 0.781818
2017-12-10T14:36:24.436142: step 1247, loss 0.349095, acc 0.875, prec 0.032889, recall 0.781818
2017-12-10T14:36:24.624222: step 1248, loss 1.05199, acc 0.734375, prec 0.0329038, recall 0.782007
2017-12-10T14:36:24.809819: step 1249, loss 0.824983, acc 0.84375, prec 0.0329271, recall 0.782195
2017-12-10T14:36:24.995707: step 1250, loss 0.697645, acc 0.828125, prec 0.0330194, recall 0.782759
2017-12-10T14:36:25.184341: step 1251, loss 0.842784, acc 0.796875, prec 0.0330389, recall 0.782946
2017-12-10T14:36:25.369684: step 1252, loss 0.785596, acc 0.75, prec 0.03309, recall 0.783319
2017-12-10T14:36:25.558946: step 1253, loss 1.70532, acc 0.84375, prec 0.0331482, recall 0.783691
2017-12-10T14:36:25.749234: step 1254, loss 0.673427, acc 0.8125, prec 0.0331337, recall 0.783691
2017-12-10T14:36:25.936536: step 1255, loss 0.598712, acc 0.796875, prec 0.0331181, recall 0.783691
2017-12-10T14:36:26.119834: step 1256, loss 9.66364, acc 0.78125, prec 0.0331738, recall 0.78272
2017-12-10T14:36:26.309555: step 1257, loss 0.978212, acc 0.765625, prec 0.0332609, recall 0.783276
2017-12-10T14:36:26.497682: step 1258, loss 1.84132, acc 0.90625, prec 0.0333237, recall 0.783646
2017-12-10T14:36:26.686798: step 1259, loss 1.23742, acc 0.65625, prec 0.0333321, recall 0.78383
2017-12-10T14:36:26.873373: step 1260, loss 1.06597, acc 0.71875, prec 0.0333104, recall 0.78383
2017-12-10T14:36:27.057502: step 1261, loss 1.21736, acc 0.640625, prec 0.0333526, recall 0.784197
2017-12-10T14:36:27.244452: step 1262, loss 0.782953, acc 0.765625, prec 0.0333694, recall 0.78438
2017-12-10T14:36:27.430855: step 1263, loss 0.887823, acc 0.765625, prec 0.033456, recall 0.784928
2017-12-10T14:36:27.616306: step 1264, loss 1.07819, acc 0.65625, prec 0.0334295, recall 0.784928
2017-12-10T14:36:27.802110: step 1265, loss 1.72281, acc 0.609375, prec 0.033469, recall 0.785292
2017-12-10T14:36:27.988025: step 1266, loss 1.55784, acc 0.65625, prec 0.0334773, recall 0.785473
2017-12-10T14:36:28.172990: step 1267, loss 1.02281, acc 0.671875, prec 0.0335216, recall 0.785835
2017-12-10T14:36:28.360882: step 1268, loss 1.3693, acc 0.59375, prec 0.0334902, recall 0.785835
2017-12-10T14:36:28.544881: step 1269, loss 0.740884, acc 0.75, prec 0.0335057, recall 0.786015
2017-12-10T14:36:28.730310: step 1270, loss 0.977129, acc 0.6875, prec 0.0335163, recall 0.786195
2017-12-10T14:36:28.912636: step 1271, loss 1.00664, acc 0.65625, prec 0.0334899, recall 0.786195
2017-12-10T14:36:29.105112: step 1272, loss 0.828035, acc 0.765625, prec 0.0335065, recall 0.786375
2017-12-10T14:36:29.291043: step 1273, loss 0.455741, acc 0.8125, prec 0.0335268, recall 0.786555
2017-12-10T14:36:29.478341: step 1274, loss 0.903887, acc 0.65625, prec 0.0335349, recall 0.786734
2017-12-10T14:36:29.662713: step 1275, loss 0.461513, acc 0.875, prec 0.0335254, recall 0.786734
2017-12-10T14:36:29.846907: step 1276, loss 0.573868, acc 0.78125, prec 0.0335086, recall 0.786734
2017-12-10T14:36:30.034310: step 1277, loss 0.848778, acc 0.8125, prec 0.0335633, recall 0.787091
2017-12-10T14:36:30.223607: step 1278, loss 1.15058, acc 0.84375, prec 0.0336204, recall 0.787448
2017-12-10T14:36:30.411736: step 1279, loss 0.425278, acc 0.875, prec 0.0336453, recall 0.787625
2017-12-10T14:36:30.601464: step 1280, loss 0.237856, acc 0.921875, prec 0.0336738, recall 0.787803
2017-12-10T14:36:30.789829: step 1281, loss 0.330019, acc 0.90625, prec 0.0336665, recall 0.787803
2017-12-10T14:36:30.977997: step 1282, loss 0.643603, acc 0.875, prec 0.0336914, recall 0.78798
2017-12-10T14:36:31.169210: step 1283, loss 3.67514, acc 0.890625, prec 0.0337187, recall 0.7875
2017-12-10T14:36:31.361507: step 1284, loss 0.184048, acc 0.953125, prec 0.0337151, recall 0.7875
2017-12-10T14:36:31.554116: step 1285, loss 0.266323, acc 0.859375, prec 0.0337043, recall 0.7875
2017-12-10T14:36:31.740804: step 1286, loss 0.530861, acc 0.890625, prec 0.0337303, recall 0.787677
2017-12-10T14:36:31.930890: step 1287, loss 0.277796, acc 0.875, prec 0.0337207, recall 0.787677
2017-12-10T14:36:32.113897: step 1288, loss 20.2283, acc 0.875, prec 0.0337823, recall 0.786722
2017-12-10T14:36:32.301819: step 1289, loss 0.821082, acc 0.8125, prec 0.0338023, recall 0.786899
2017-12-10T14:36:32.489217: step 1290, loss 0.989902, acc 0.84375, prec 0.0338247, recall 0.787075
2017-12-10T14:36:32.675702: step 1291, loss 0.640905, acc 0.8125, prec 0.0338446, recall 0.787252
2017-12-10T14:36:32.861861: step 1292, loss 0.587552, acc 0.78125, prec 0.0338621, recall 0.787428
2017-12-10T14:36:33.053004: step 1293, loss 0.656235, acc 0.890625, prec 0.0339224, recall 0.787779
2017-12-10T14:36:33.240429: step 1294, loss 0.883448, acc 0.71875, prec 0.0339007, recall 0.787779
2017-12-10T14:36:33.428175: step 1295, loss 0.645134, acc 0.75, prec 0.0338815, recall 0.787779
2017-12-10T14:36:33.614494: step 1296, loss 0.902376, acc 0.734375, prec 0.0338953, recall 0.787954
2017-12-10T14:36:33.802167: step 1297, loss 0.99166, acc 0.734375, prec 0.0339434, recall 0.788303
2017-12-10T14:36:33.987231: step 1298, loss 1.64836, acc 0.734375, prec 0.0340599, recall 0.788998
2017-12-10T14:36:34.176313: step 1299, loss 1.33421, acc 0.75, prec 0.034109, recall 0.789344
2017-12-10T14:36:34.362874: step 1300, loss 1.08484, acc 0.640625, prec 0.0340813, recall 0.789344
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_3/1512934336/checkpoints/model-1300

2017-12-10T14:36:35.401301: step 1301, loss 1.07116, acc 0.6875, prec 0.0340572, recall 0.789344
2017-12-10T14:36:35.584867: step 1302, loss 1.58627, acc 0.703125, prec 0.0341367, recall 0.789861
2017-12-10T14:36:35.775261: step 1303, loss 0.740007, acc 0.765625, prec 0.0341868, recall 0.790204
2017-12-10T14:36:35.960605: step 1304, loss 0.519842, acc 0.765625, prec 0.0341687, recall 0.790204
2017-12-10T14:36:36.149451: step 1305, loss 0.824686, acc 0.796875, prec 0.0342212, recall 0.790546
2017-12-10T14:36:36.338141: step 1306, loss 0.807633, acc 0.734375, prec 0.0342007, recall 0.790546
2017-12-10T14:36:36.523876: step 1307, loss 0.76271, acc 0.734375, prec 0.0341802, recall 0.790546
2017-12-10T14:36:36.711769: step 1308, loss 0.749854, acc 0.796875, prec 0.0341986, recall 0.790717
2017-12-10T14:36:36.899521: step 1309, loss 0.60555, acc 0.828125, prec 0.0341853, recall 0.790717
2017-12-10T14:36:37.087105: step 1310, loss 0.637516, acc 0.8125, prec 0.0341709, recall 0.790717
2017-12-10T14:36:37.275057: step 1311, loss 0.585236, acc 0.796875, prec 0.0341553, recall 0.790717
2017-12-10T14:36:37.457922: step 1312, loss 0.471851, acc 0.84375, prec 0.0341433, recall 0.790717
2017-12-10T14:36:37.644367: step 1313, loss 0.43091, acc 0.875, prec 0.0341337, recall 0.790717
2017-12-10T14:36:37.829104: step 1314, loss 6.87815, acc 0.9375, prec 0.0341301, recall 0.790073
2017-12-10T14:36:38.021682: step 1315, loss 0.809487, acc 0.875, prec 0.0341883, recall 0.790414
2017-12-10T14:36:38.214279: step 1316, loss 0.989772, acc 0.828125, prec 0.034209, recall 0.790584
2017-12-10T14:36:38.401201: step 1317, loss 0.330379, acc 0.890625, prec 0.0342345, recall 0.790754
2017-12-10T14:36:38.589563: step 1318, loss 0.11774, acc 0.9375, prec 0.0342297, recall 0.790754
2017-12-10T14:36:38.772879: step 1319, loss 0.9335, acc 0.953125, prec 0.0342939, recall 0.791093
2017-12-10T14:36:38.960885: step 1320, loss 0.430787, acc 0.875, prec 0.0342843, recall 0.791093
2017-12-10T14:36:39.145234: step 1321, loss 0.267866, acc 0.90625, prec 0.0343787, recall 0.791599
2017-12-10T14:36:39.332405: step 1322, loss 0.505341, acc 0.828125, prec 0.0343655, recall 0.791599
2017-12-10T14:36:39.519208: step 1323, loss 0.42878, acc 0.796875, prec 0.0343498, recall 0.791599
2017-12-10T14:36:39.703189: step 1324, loss 0.321889, acc 0.90625, prec 0.0343426, recall 0.791599
2017-12-10T14:36:39.886243: step 1325, loss 0.346054, acc 0.890625, prec 0.0343342, recall 0.791599
2017-12-10T14:36:40.075224: step 1326, loss 0.431525, acc 0.84375, prec 0.0343221, recall 0.791599
2017-12-10T14:36:40.262442: step 1327, loss 3.07812, acc 0.875, prec 0.0343137, recall 0.79096
2017-12-10T14:36:40.451564: step 1328, loss 0.464407, acc 0.875, prec 0.0343717, recall 0.791297
2017-12-10T14:36:40.636165: step 1329, loss 0.628116, acc 0.796875, prec 0.0343899, recall 0.791465
2017-12-10T14:36:40.820611: step 1330, loss 0.71731, acc 0.8125, prec 0.0345105, recall 0.792135
2017-12-10T14:36:41.006515: step 1331, loss 8.21128, acc 0.765625, prec 0.0345286, recall 0.791033
2017-12-10T14:36:41.195927: step 1332, loss 0.516772, acc 0.828125, prec 0.034549, recall 0.7912
2017-12-10T14:36:41.380312: step 1333, loss 0.639517, acc 0.71875, prec 0.0345273, recall 0.7912
2017-12-10T14:36:41.570056: step 1334, loss 1.22087, acc 0.578125, prec 0.0345285, recall 0.791367
2017-12-10T14:36:41.757888: step 1335, loss 1.45213, acc 0.578125, prec 0.034496, recall 0.791367
2017-12-10T14:36:41.943240: step 1336, loss 0.814193, acc 0.75, prec 0.034544, recall 0.7917
2017-12-10T14:36:42.127368: step 1337, loss 1.31095, acc 0.65625, prec 0.0345511, recall 0.791866
2017-12-10T14:36:42.318767: step 1338, loss 1.38413, acc 0.59375, prec 0.034587, recall 0.792197
2017-12-10T14:36:42.504199: step 1339, loss 1.14223, acc 0.671875, prec 0.0345618, recall 0.792197
2017-12-10T14:36:42.685841: step 1340, loss 1.07071, acc 0.6875, prec 0.0345713, recall 0.792363
2017-12-10T14:36:42.868250: step 1341, loss 1.05655, acc 0.671875, prec 0.0346131, recall 0.792693
2017-12-10T14:36:43.051108: step 1342, loss 1.30234, acc 0.59375, prec 0.0346823, recall 0.793185
2017-12-10T14:36:43.239028: step 1343, loss 1.29037, acc 0.734375, prec 0.0347287, recall 0.793513
2017-12-10T14:36:43.427504: step 1344, loss 1.06499, acc 0.65625, prec 0.0347023, recall 0.793513
2017-12-10T14:36:43.613344: step 1345, loss 0.868945, acc 0.71875, prec 0.0346807, recall 0.793513
2017-12-10T14:36:43.799801: step 1346, loss 1.07658, acc 0.71875, prec 0.0347592, recall 0.794002
2017-12-10T14:36:43.986627: step 1347, loss 0.899254, acc 0.734375, prec 0.0347721, recall 0.794164
2017-12-10T14:36:44.180470: step 1348, loss 0.880405, acc 0.796875, prec 0.0347898, recall 0.794326
2017-12-10T14:36:44.364337: step 1349, loss 0.990921, acc 0.875, prec 0.0348468, recall 0.79465
2017-12-10T14:36:44.552048: step 1350, loss 0.657573, acc 0.78125, prec 0.0348966, recall 0.794972
2017-12-10T14:36:44.738108: step 1351, loss 6.92619, acc 0.734375, prec 0.0348785, recall 0.793725
2017-12-10T14:36:44.927261: step 1352, loss 0.541394, acc 0.84375, prec 0.0348665, recall 0.793725
2017-12-10T14:36:45.111227: step 1353, loss 0.756146, acc 0.765625, prec 0.0348485, recall 0.793725
2017-12-10T14:36:45.298769: step 1354, loss 0.530594, acc 0.8125, prec 0.0348341, recall 0.793725
2017-12-10T14:36:45.487525: step 1355, loss 1.08846, acc 0.890625, prec 0.0348921, recall 0.794049
2017-12-10T14:36:45.676034: step 1356, loss 0.762278, acc 0.8125, prec 0.0348777, recall 0.794049
2017-12-10T14:36:45.860553: step 1357, loss 0.554614, acc 0.84375, prec 0.0349985, recall 0.794692
2017-12-10T14:36:46.047674: step 1358, loss 0.743695, acc 0.78125, prec 0.0349816, recall 0.794692
2017-12-10T14:36:46.232031: step 1359, loss 0.986954, acc 0.703125, prec 0.0349588, recall 0.794692
2017-12-10T14:36:46.419128: step 1360, loss 0.422656, acc 0.828125, prec 0.0349456, recall 0.794692
2017-12-10T14:36:46.608677: step 1361, loss 0.754054, acc 0.734375, prec 0.0349252, recall 0.794692
2017-12-10T14:36:46.795930: step 1362, loss 0.689591, acc 0.8125, prec 0.0349439, recall 0.794852
2017-12-10T14:36:46.986934: step 1363, loss 0.889837, acc 0.78125, prec 0.0349272, recall 0.794852
2017-12-10T14:36:47.173009: step 1364, loss 2.84485, acc 0.84375, prec 0.0349495, recall 0.794393
2017-12-10T14:36:47.361005: step 1365, loss 0.579829, acc 0.859375, prec 0.0349717, recall 0.794553
2017-12-10T14:36:47.546081: step 1366, loss 1.06834, acc 0.71875, prec 0.0349502, recall 0.794553
2017-12-10T14:36:47.734233: step 1367, loss 0.643883, acc 0.828125, prec 0.0349701, recall 0.794712
2017-12-10T14:36:47.920138: step 1368, loss 0.504431, acc 0.859375, prec 0.0349593, recall 0.794712
2017-12-10T14:36:48.108158: step 1369, loss 1.51777, acc 0.8125, prec 0.0349461, recall 0.794095
2017-12-10T14:36:48.295418: step 1370, loss 0.49858, acc 0.84375, prec 0.0349342, recall 0.794095
2017-12-10T14:36:48.484034: step 1371, loss 0.612485, acc 0.78125, prec 0.0349175, recall 0.794095
2017-12-10T14:36:48.669305: step 1372, loss 0.481435, acc 0.859375, prec 0.0349068, recall 0.794095
2017-12-10T14:36:48.855867: step 1373, loss 0.651746, acc 0.8125, prec 0.0349254, recall 0.794255
2017-12-10T14:36:49.043418: step 1374, loss 7.86171, acc 0.859375, prec 0.0349488, recall 0.793798
2017-12-10T14:36:49.234331: step 1375, loss 0.436812, acc 0.90625, prec 0.0349746, recall 0.793958
2017-12-10T14:36:49.418835: step 1376, loss 4.76039, acc 0.828125, prec 0.0349627, recall 0.793344
2017-12-10T14:36:49.608628: step 1377, loss 0.480758, acc 0.875, prec 0.0349531, recall 0.793344
2017-12-10T14:36:49.792037: step 1378, loss 0.5379, acc 0.828125, prec 0.0349729, recall 0.793503
2017-12-10T14:36:49.976346: step 1379, loss 0.873832, acc 0.75, prec 0.0349867, recall 0.793663
2017-12-10T14:36:50.165593: step 1380, loss 0.511841, acc 0.796875, prec 0.0350041, recall 0.793822
2017-12-10T14:36:50.352849: step 1381, loss 0.645371, acc 0.8125, prec 0.0349898, recall 0.793822
2017-12-10T14:36:50.537666: step 1382, loss 4.36465, acc 0.859375, prec 0.0350459, recall 0.793528
2017-12-10T14:36:50.726544: step 1383, loss 0.551915, acc 0.78125, prec 0.0350292, recall 0.793528
2017-12-10T14:36:50.908581: step 1384, loss 0.820111, acc 0.78125, prec 0.0350454, recall 0.793687
2017-12-10T14:36:51.095585: step 1385, loss 1.10213, acc 0.734375, prec 0.0350251, recall 0.793687
2017-12-10T14:36:51.279810: step 1386, loss 0.640521, acc 0.8125, prec 0.0350436, recall 0.793846
2017-12-10T14:36:51.466755: step 1387, loss 0.924918, acc 0.796875, prec 0.0350282, recall 0.793846
2017-12-10T14:36:51.651846: step 1388, loss 1.06381, acc 0.625, prec 0.0350651, recall 0.794163
2017-12-10T14:36:51.837784: step 1389, loss 0.865857, acc 0.796875, prec 0.0350824, recall 0.794321
2017-12-10T14:36:52.022386: step 1390, loss 1.73415, acc 0.765625, prec 0.0350972, recall 0.794479
2017-12-10T14:36:52.209790: step 1391, loss 0.870086, acc 0.71875, prec 0.0350758, recall 0.794479
2017-12-10T14:36:52.396221: step 1392, loss 0.672128, acc 0.78125, prec 0.0350592, recall 0.794479
2017-12-10T14:36:52.580330: step 1393, loss 0.775818, acc 0.78125, prec 0.0350426, recall 0.794479
2017-12-10T14:36:52.766749: step 1394, loss 0.827098, acc 0.734375, prec 0.0350225, recall 0.794479
2017-12-10T14:36:52.949914: step 1395, loss 1.67886, acc 0.8125, prec 0.0350095, recall 0.79387
2017-12-10T14:36:53.134513: step 1396, loss 1.02638, acc 0.765625, prec 0.0349917, recall 0.79387
2017-12-10T14:36:53.320252: step 1397, loss 0.827081, acc 0.765625, prec 0.0350717, recall 0.794343
2017-12-10T14:36:53.504402: step 1398, loss 6.70479, acc 0.75, prec 0.035054, recall 0.793736
2017-12-10T14:36:53.692486: step 1399, loss 0.411607, acc 0.859375, prec 0.0350433, recall 0.793736
2017-12-10T14:36:53.879896: step 1400, loss 1.04182, acc 0.765625, prec 0.0350581, recall 0.793893
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_3/1512934336/checkpoints/model-1400

2017-12-10T14:36:55.025269: step 1401, loss 4.03725, acc 0.671875, prec 0.0350345, recall 0.793288
2017-12-10T14:36:55.213418: step 1402, loss 0.606888, acc 0.8125, prec 0.0350204, recall 0.793288
2017-12-10T14:36:55.396318: step 1403, loss 0.92189, acc 0.671875, prec 0.0350281, recall 0.793445
2017-12-10T14:36:55.585063: step 1404, loss 0.754824, acc 0.75, prec 0.0350741, recall 0.79376
2017-12-10T14:36:55.775094: step 1405, loss 0.812638, acc 0.703125, prec 0.0350842, recall 0.793916
2017-12-10T14:36:55.968692: step 1406, loss 1.17883, acc 0.671875, prec 0.0350594, recall 0.793916
2017-12-10T14:36:56.153193: step 1407, loss 1.35246, acc 0.515625, prec 0.0350554, recall 0.794073
2017-12-10T14:36:56.339715: step 1408, loss 4.53052, acc 0.65625, prec 0.035063, recall 0.793627
2017-12-10T14:36:56.525076: step 1409, loss 0.869425, acc 0.65625, prec 0.0350372, recall 0.793627
2017-12-10T14:36:56.710419: step 1410, loss 0.899384, acc 0.78125, prec 0.0350208, recall 0.793627
2017-12-10T14:36:56.893901: step 1411, loss 0.994648, acc 0.734375, prec 0.0350331, recall 0.793783
2017-12-10T14:36:57.078852: step 1412, loss 0.667901, acc 0.796875, prec 0.0350824, recall 0.794095
2017-12-10T14:36:57.265304: step 1413, loss 1.18055, acc 0.71875, prec 0.0350613, recall 0.794095
2017-12-10T14:36:57.448691: step 1414, loss 0.65769, acc 0.859375, prec 0.0350508, recall 0.794095
2017-12-10T14:36:57.636686: step 1415, loss 0.895944, acc 0.6875, prec 0.0350274, recall 0.794095
2017-12-10T14:36:57.822458: step 1416, loss 1.00717, acc 0.71875, prec 0.0350385, recall 0.794251
2017-12-10T14:36:58.006257: step 1417, loss 0.550539, acc 0.875, prec 0.0350292, recall 0.794251
2017-12-10T14:36:58.191660: step 1418, loss 2.49329, acc 0.8125, prec 0.0350485, recall 0.793807
2017-12-10T14:36:58.378221: step 1419, loss 1.98371, acc 0.796875, prec 0.0350345, recall 0.793208
2017-12-10T14:36:58.566619: step 1420, loss 0.294078, acc 0.84375, prec 0.035055, recall 0.793364
2017-12-10T14:36:58.755706: step 1421, loss 0.64459, acc 0.8125, prec 0.0351053, recall 0.793675
2017-12-10T14:36:58.949345: step 1422, loss 2.73026, acc 0.84375, prec 0.0350947, recall 0.793078
2017-12-10T14:36:59.140966: step 1423, loss 0.737897, acc 0.78125, prec 0.0351105, recall 0.793233
2017-12-10T14:36:59.327906: step 1424, loss 0.608512, acc 0.796875, prec 0.0351274, recall 0.793388
2017-12-10T14:36:59.513565: step 1425, loss 0.70744, acc 0.765625, prec 0.035142, recall 0.793544
2017-12-10T14:36:59.698983: step 1426, loss 0.693179, acc 0.78125, prec 0.0351577, recall 0.793698
2017-12-10T14:36:59.883072: step 1427, loss 1.16104, acc 0.671875, prec 0.0351332, recall 0.793698
2017-12-10T14:37:00.069013: step 1428, loss 0.610327, acc 0.828125, prec 0.0351203, recall 0.793698
2017-12-10T14:37:00.255224: step 1429, loss 0.839687, acc 0.828125, prec 0.0351395, recall 0.793853
2017-12-10T14:37:00.441589: step 1430, loss 0.628007, acc 0.78125, prec 0.0351552, recall 0.794007
2017-12-10T14:37:00.631910: step 1431, loss 1.22618, acc 0.8125, prec 0.0351732, recall 0.794162
2017-12-10T14:37:00.822061: step 1432, loss 0.673031, acc 0.78125, prec 0.0351889, recall 0.794316
2017-12-10T14:37:01.006507: step 1433, loss 0.692461, acc 0.796875, prec 0.0352057, recall 0.794469
2017-12-10T14:37:01.201390: step 1434, loss 0.642415, acc 0.765625, prec 0.0351882, recall 0.794469
2017-12-10T14:37:01.388947: step 1435, loss 0.458044, acc 0.859375, prec 0.0352416, recall 0.794776
2017-12-10T14:37:01.575193: step 1436, loss 0.708925, acc 0.8125, prec 0.0352595, recall 0.794929
2017-12-10T14:37:01.760126: step 1437, loss 0.4286, acc 0.84375, prec 0.0352478, recall 0.794929
2017-12-10T14:37:01.950480: step 1438, loss 0.350492, acc 0.859375, prec 0.0352373, recall 0.794929
2017-12-10T14:37:02.133417: step 1439, loss 0.480939, acc 0.890625, prec 0.0352611, recall 0.795082
2017-12-10T14:37:02.319582: step 1440, loss 0.390812, acc 0.859375, prec 0.0352506, recall 0.795082
2017-12-10T14:37:02.505060: step 1441, loss 0.530998, acc 0.84375, prec 0.0352389, recall 0.795082
2017-12-10T14:37:02.690194: step 1442, loss 0.630839, acc 0.890625, prec 0.0352627, recall 0.795235
2017-12-10T14:37:02.878936: step 1443, loss 0.499584, acc 0.84375, prec 0.0352829, recall 0.795387
2017-12-10T14:37:03.067775: step 1444, loss 0.171824, acc 0.90625, prec 0.0352759, recall 0.795387
2017-12-10T14:37:03.256088: step 1445, loss 8.77293, acc 0.890625, prec 0.0353325, recall 0.7951
2017-12-10T14:37:03.444380: step 1446, loss 10.8434, acc 0.859375, prec 0.0353232, recall 0.79451
2017-12-10T14:37:03.631760: step 1447, loss 0.28478, acc 0.890625, prec 0.0353469, recall 0.794663
2017-12-10T14:37:03.815365: step 1448, loss 5.51141, acc 0.796875, prec 0.0353341, recall 0.793486
2017-12-10T14:37:04.008011: step 1449, loss 0.680823, acc 0.8125, prec 0.0353519, recall 0.793639
2017-12-10T14:37:04.192527: step 1450, loss 2.06278, acc 0.703125, prec 0.0353615, recall 0.793792
2017-12-10T14:37:04.381819: step 1451, loss 1.09311, acc 0.71875, prec 0.0353406, recall 0.793792
2017-12-10T14:37:04.567374: step 1452, loss 1.24193, acc 0.625, prec 0.0353444, recall 0.793944
2017-12-10T14:37:04.753133: step 1453, loss 1.69701, acc 0.609375, prec 0.0353788, recall 0.794248
2017-12-10T14:37:04.937329: step 1454, loss 1.71988, acc 0.59375, prec 0.0354119, recall 0.794551
2017-12-10T14:37:05.119760: step 1455, loss 2.51976, acc 0.390625, prec 0.0353666, recall 0.794551
2017-12-10T14:37:05.302375: step 1456, loss 1.44612, acc 0.5625, prec 0.0353658, recall 0.794702
2017-12-10T14:37:05.484862: step 1457, loss 1.92809, acc 0.484375, prec 0.0353276, recall 0.794702
2017-12-10T14:37:05.671283: step 1458, loss 2.24555, acc 0.453125, prec 0.0352872, recall 0.794702
2017-12-10T14:37:05.856183: step 1459, loss 2.04391, acc 0.515625, prec 0.0353145, recall 0.795004
2017-12-10T14:37:06.038192: step 1460, loss 1.55574, acc 0.515625, prec 0.0353417, recall 0.795304
2017-12-10T14:37:06.221226: step 1461, loss 1.14078, acc 0.71875, prec 0.0353838, recall 0.795604
2017-12-10T14:37:06.407063: step 1462, loss 1.64774, acc 0.640625, prec 0.0353887, recall 0.795754
2017-12-10T14:37:06.594066: step 1463, loss 1.40126, acc 0.671875, prec 0.0354587, recall 0.796202
2017-12-10T14:37:06.778830: step 1464, loss 1.22031, acc 0.625, prec 0.035431, recall 0.796202
2017-12-10T14:37:06.966271: step 1465, loss 1.29524, acc 0.6875, prec 0.035408, recall 0.796202
2017-12-10T14:37:07.151325: step 1466, loss 7.24068, acc 0.734375, prec 0.0354209, recall 0.79577
2017-12-10T14:37:07.339136: step 1467, loss 0.82185, acc 0.765625, prec 0.035435, recall 0.795918
2017-12-10T14:37:07.524268: step 1468, loss 0.744577, acc 0.6875, prec 0.0354433, recall 0.796067
2017-12-10T14:37:07.710017: step 1469, loss 0.712028, acc 0.84375, prec 0.0354631, recall 0.796215
2017-12-10T14:37:07.895486: step 1470, loss 3.65874, acc 0.71875, prec 0.0354748, recall 0.795785
2017-12-10T14:37:08.088331: step 1471, loss 0.66357, acc 0.8125, prec 0.035461, recall 0.795785
2017-12-10T14:37:08.273824: step 1472, loss 0.641109, acc 0.796875, prec 0.0354773, recall 0.795933
2017-12-10T14:37:08.462678: step 1473, loss 4.578, acc 0.78125, prec 0.0354936, recall 0.795504
2017-12-10T14:37:08.651842: step 1474, loss 0.452801, acc 0.875, prec 0.0355156, recall 0.795652
2017-12-10T14:37:08.841200: step 1475, loss 0.486957, acc 0.828125, prec 0.035503, recall 0.795652
2017-12-10T14:37:09.031936: step 1476, loss 0.744139, acc 0.859375, prec 0.035555, recall 0.795948
2017-12-10T14:37:09.221538: step 1477, loss 0.51601, acc 0.78125, prec 0.0355389, recall 0.795948
2017-12-10T14:37:09.407008: step 1478, loss 0.693082, acc 0.78125, prec 0.035554, recall 0.796095
2017-12-10T14:37:09.592282: step 1479, loss 4.87511, acc 0.765625, prec 0.0355379, recall 0.79552
2017-12-10T14:37:09.777915: step 1480, loss 0.769584, acc 0.703125, prec 0.0355161, recall 0.79552
2017-12-10T14:37:09.964002: step 1481, loss 0.59379, acc 0.796875, prec 0.0355323, recall 0.795668
2017-12-10T14:37:10.148541: step 1482, loss 7.80161, acc 0.734375, prec 0.035514, recall 0.795094
2017-12-10T14:37:10.339081: step 1483, loss 0.828706, acc 0.75, prec 0.0354957, recall 0.795094
2017-12-10T14:37:10.527851: step 1484, loss 1.37151, acc 0.765625, prec 0.0355096, recall 0.795242
2017-12-10T14:37:10.714071: step 1485, loss 1.39163, acc 0.5625, prec 0.0354776, recall 0.795242
2017-12-10T14:37:10.897479: step 1486, loss 1.43336, acc 0.515625, prec 0.0354423, recall 0.795242
2017-12-10T14:37:11.083876: step 1487, loss 1.34726, acc 0.609375, prec 0.0354448, recall 0.795389
2017-12-10T14:37:11.269892: step 1488, loss 1.50385, acc 0.5625, prec 0.035413, recall 0.795389
2017-12-10T14:37:11.454266: step 1489, loss 1.36266, acc 0.671875, prec 0.0353892, recall 0.795389
2017-12-10T14:37:11.637737: step 1490, loss 1.43838, acc 0.609375, prec 0.0353917, recall 0.795536
2017-12-10T14:37:11.808431: step 1491, loss 1.06445, acc 0.692308, prec 0.0353736, recall 0.795536
2017-12-10T14:37:11.998956: step 1492, loss 0.83572, acc 0.65625, prec 0.0354104, recall 0.79583
2017-12-10T14:37:12.183513: step 1493, loss 0.957343, acc 0.65625, prec 0.0354163, recall 0.795977
2017-12-10T14:37:12.366707: step 1494, loss 1.75771, acc 0.765625, prec 0.035461, recall 0.79627
2017-12-10T14:37:12.550447: step 1495, loss 1.46753, acc 0.6875, prec 0.0354384, recall 0.79627
2017-12-10T14:37:12.735492: step 1496, loss 0.667621, acc 0.796875, prec 0.0354236, recall 0.79627
2017-12-10T14:37:12.925180: step 1497, loss 0.762829, acc 0.78125, prec 0.0354694, recall 0.796562
2017-12-10T14:37:13.107931: step 1498, loss 0.653168, acc 0.828125, prec 0.0354569, recall 0.796562
2017-12-10T14:37:13.291511: step 1499, loss 0.25667, acc 0.890625, prec 0.035449, recall 0.796562
2017-12-10T14:37:13.479115: step 1500, loss 0.357738, acc 0.9375, prec 0.0354752, recall 0.796707
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_3/1512934336/checkpoints/model-1500

2017-12-10T14:37:14.590012: step 1501, loss 0.545636, acc 0.890625, prec 0.0354981, recall 0.796853
2017-12-10T14:37:14.776293: step 1502, loss 0.633799, acc 0.875, prec 0.0355197, recall 0.796998
2017-12-10T14:37:14.963197: step 1503, loss 0.287882, acc 0.875, prec 0.0355107, recall 0.796998
2017-12-10T14:37:15.148962: step 1504, loss 0.233855, acc 0.90625, prec 0.0355039, recall 0.796998
2017-12-10T14:37:15.338340: step 1505, loss 0.304979, acc 0.875, prec 0.0355256, recall 0.797143
2017-12-10T14:37:15.526012: step 1506, loss 0.132948, acc 0.953125, prec 0.0355529, recall 0.797288
2017-12-10T14:37:15.714713: step 1507, loss 0.19043, acc 0.90625, prec 0.0355768, recall 0.797432
2017-12-10T14:37:15.898835: step 1508, loss 0.318818, acc 0.921875, prec 0.0356018, recall 0.797577
2017-12-10T14:37:16.084774: step 1509, loss 3.21838, acc 0.9375, prec 0.0356291, recall 0.797153
2017-12-10T14:37:16.272246: step 1510, loss 0.0934017, acc 0.96875, prec 0.0356268, recall 0.797153
2017-12-10T14:37:16.462633: step 1511, loss 0.159616, acc 0.9375, prec 0.0356836, recall 0.797441
2017-12-10T14:37:16.653245: step 1512, loss 0.320363, acc 0.921875, prec 0.0357393, recall 0.797729
2017-12-10T14:37:16.839356: step 1513, loss 0.184313, acc 0.90625, prec 0.0357325, recall 0.797729
2017-12-10T14:37:17.026226: step 1514, loss 7.05729, acc 0.90625, prec 0.0357268, recall 0.797163
2017-12-10T14:37:17.217445: step 1515, loss 0.133309, acc 0.921875, prec 0.0357211, recall 0.797163
2017-12-10T14:37:17.403305: step 1516, loss 3.29882, acc 0.90625, prec 0.0357154, recall 0.796598
2017-12-10T14:37:17.592310: step 1517, loss 0.321207, acc 0.890625, prec 0.0357075, recall 0.796598
2017-12-10T14:37:17.776553: step 1518, loss 0.110523, acc 0.953125, prec 0.0357041, recall 0.796598
2017-12-10T14:37:17.961356: step 1519, loss 0.110997, acc 0.9375, prec 0.0356995, recall 0.796598
2017-12-10T14:37:18.144122: step 1520, loss 5.95788, acc 0.859375, prec 0.0357823, recall 0.796466
2017-12-10T14:37:18.330498: step 1521, loss 0.59239, acc 0.84375, prec 0.035771, recall 0.796466
2017-12-10T14:37:18.512669: step 1522, loss 0.465714, acc 0.875, prec 0.0358843, recall 0.79704
2017-12-10T14:37:18.697311: step 1523, loss 1.27407, acc 0.625, prec 0.0358875, recall 0.797183
2017-12-10T14:37:18.885112: step 1524, loss 0.916954, acc 0.765625, prec 0.0359316, recall 0.797468
2017-12-10T14:37:19.075333: step 1525, loss 4.76221, acc 0.734375, prec 0.0359134, recall 0.796908
2017-12-10T14:37:19.264974: step 1526, loss 1.41945, acc 0.625, prec 0.0358861, recall 0.796908
2017-12-10T14:37:19.454550: step 1527, loss 1.95299, acc 0.65625, prec 0.0358916, recall 0.797051
2017-12-10T14:37:19.638509: step 1528, loss 1.42706, acc 0.65625, prec 0.0359276, recall 0.797335
2017-12-10T14:37:19.823289: step 1529, loss 1.30216, acc 0.59375, prec 0.035959, recall 0.797619
2017-12-10T14:37:20.007603: step 1530, loss 1.41175, acc 0.625, prec 0.0359926, recall 0.797902
2017-12-10T14:37:20.192639: step 1531, loss 1.52642, acc 0.546875, prec 0.0359597, recall 0.797902
2017-12-10T14:37:20.380227: step 1532, loss 1.46743, acc 0.65625, prec 0.0359955, recall 0.798184
2017-12-10T14:37:20.564553: step 1533, loss 1.14748, acc 0.671875, prec 0.036002, recall 0.798325
2017-12-10T14:37:20.749677: step 1534, loss 2.00876, acc 0.53125, prec 0.0359681, recall 0.798325
2017-12-10T14:37:20.935292: step 1535, loss 1.19637, acc 0.671875, prec 0.0359746, recall 0.798466
2017-12-10T14:37:21.118974: step 1536, loss 1.49402, acc 0.578125, prec 0.0360046, recall 0.798747
2017-12-10T14:37:21.300799: step 1537, loss 1.43438, acc 0.5625, prec 0.035973, recall 0.798747
2017-12-10T14:37:21.487819: step 1538, loss 0.798436, acc 0.703125, prec 0.0359818, recall 0.798887
2017-12-10T14:37:21.674858: step 1539, loss 0.572012, acc 0.796875, prec 0.0359672, recall 0.798887
2017-12-10T14:37:21.861369: step 1540, loss 0.978473, acc 0.765625, prec 0.0359805, recall 0.799026
2017-12-10T14:37:22.048978: step 1541, loss 0.444453, acc 0.828125, prec 0.0359681, recall 0.799026
2017-12-10T14:37:22.234546: step 1542, loss 0.999415, acc 0.78125, prec 0.0360126, recall 0.799306
2017-12-10T14:37:22.419934: step 1543, loss 0.367752, acc 0.859375, prec 0.0360327, recall 0.799445
2017-12-10T14:37:22.607656: step 1544, loss 0.605556, acc 0.84375, prec 0.0360515, recall 0.799584
2017-12-10T14:37:22.797765: step 1545, loss 0.365618, acc 0.890625, prec 0.0360436, recall 0.799584
2017-12-10T14:37:22.983870: step 1546, loss 0.62471, acc 0.890625, prec 0.036096, recall 0.799861
2017-12-10T14:37:23.172919: step 1547, loss 0.456652, acc 0.78125, prec 0.0360802, recall 0.799861
2017-12-10T14:37:23.355827: step 1548, loss 0.291149, acc 0.890625, prec 0.0360723, recall 0.799861
2017-12-10T14:37:23.538554: step 1549, loss 0.0819861, acc 0.96875, prec 0.0360701, recall 0.799861
2017-12-10T14:37:23.726501: step 1550, loss 0.231375, acc 0.9375, prec 0.0360656, recall 0.799861
2017-12-10T14:37:23.911985: step 1551, loss 0.164067, acc 0.96875, prec 0.0360633, recall 0.799861
2017-12-10T14:37:24.100562: step 1552, loss 0.143069, acc 0.953125, prec 0.0361201, recall 0.800138
2017-12-10T14:37:24.292240: step 1553, loss 2.61448, acc 0.921875, prec 0.0361156, recall 0.799585
2017-12-10T14:37:24.479588: step 1554, loss 0.141995, acc 0.9375, prec 0.0361412, recall 0.799724
2017-12-10T14:37:24.665102: step 1555, loss 0.118216, acc 0.96875, prec 0.0361389, recall 0.799724
2017-12-10T14:37:24.849494: step 1556, loss 3.13889, acc 0.953125, prec 0.0361367, recall 0.799172
2017-12-10T14:37:25.039446: step 1557, loss 0.915272, acc 0.9375, prec 0.0361622, recall 0.79931
2017-12-10T14:37:25.229114: step 1558, loss 0.409697, acc 0.875, prec 0.0361833, recall 0.799449
2017-12-10T14:37:25.417597: step 1559, loss 2.66159, acc 0.953125, prec 0.0362712, recall 0.799313
2017-12-10T14:37:25.604967: step 1560, loss 0.315132, acc 0.90625, prec 0.0362644, recall 0.799313
2017-12-10T14:37:25.791420: step 1561, loss 0.558022, acc 0.859375, prec 0.0362843, recall 0.799451
2017-12-10T14:37:25.981632: step 1562, loss 6.48505, acc 0.828125, prec 0.036273, recall 0.798902
2017-12-10T14:37:26.168125: step 1563, loss 0.320451, acc 0.859375, prec 0.0363229, recall 0.799178
2017-12-10T14:37:26.355724: step 1564, loss 0.950262, acc 0.734375, prec 0.0363036, recall 0.799178
2017-12-10T14:37:26.542895: step 1565, loss 0.59206, acc 0.734375, prec 0.0363144, recall 0.799315
2017-12-10T14:37:26.726775: step 1566, loss 0.874352, acc 0.734375, prec 0.0362952, recall 0.799315
2017-12-10T14:37:26.912258: step 1567, loss 0.882167, acc 0.734375, prec 0.036276, recall 0.799315
2017-12-10T14:37:27.099599: step 1568, loss 0.666413, acc 0.75, prec 0.036258, recall 0.799315
2017-12-10T14:37:27.282768: step 1569, loss 0.752763, acc 0.75, prec 0.03624, recall 0.799315
2017-12-10T14:37:27.467931: step 1570, loss 1.24637, acc 0.765625, prec 0.0363128, recall 0.799727
2017-12-10T14:37:27.653983: step 1571, loss 1.06678, acc 0.734375, prec 0.0363236, recall 0.799863
2017-12-10T14:37:27.841868: step 1572, loss 1.30774, acc 0.703125, prec 0.0363321, recall 0.8
2017-12-10T14:37:28.027759: step 1573, loss 0.971498, acc 0.640625, prec 0.036336, recall 0.800136
2017-12-10T14:37:28.211701: step 1574, loss 0.91536, acc 0.6875, prec 0.0363135, recall 0.800136
2017-12-10T14:37:28.395565: step 1575, loss 1.14316, acc 0.703125, prec 0.0362922, recall 0.800136
2017-12-10T14:37:28.580539: step 1576, loss 0.775534, acc 0.75, prec 0.0362742, recall 0.800136
2017-12-10T14:37:28.769148: step 1577, loss 0.805842, acc 0.859375, prec 0.0363535, recall 0.800545
2017-12-10T14:37:28.959320: step 1578, loss 0.969232, acc 0.796875, prec 0.0363985, recall 0.800816
2017-12-10T14:37:29.153288: step 1579, loss 0.661675, acc 0.78125, prec 0.0364125, recall 0.800951
2017-12-10T14:37:29.342707: step 1580, loss 0.900192, acc 0.671875, prec 0.0364186, recall 0.801086
2017-12-10T14:37:29.527930: step 1581, loss 0.638093, acc 0.8125, prec 0.0364349, recall 0.801221
2017-12-10T14:37:29.711712: step 1582, loss 0.436829, acc 0.90625, prec 0.0364876, recall 0.80149
2017-12-10T14:37:29.898643: step 1583, loss 0.376319, acc 0.859375, prec 0.0364774, recall 0.80149
2017-12-10T14:37:30.083027: step 1584, loss 0.439751, acc 0.84375, prec 0.0364959, recall 0.801625
2017-12-10T14:37:30.270812: step 1585, loss 0.338911, acc 0.875, prec 0.0364869, recall 0.801625
2017-12-10T14:37:30.457120: step 1586, loss 0.295139, acc 0.875, prec 0.0364779, recall 0.801625
2017-12-10T14:37:30.643141: step 1587, loss 0.385911, acc 0.859375, prec 0.0364678, recall 0.801625
2017-12-10T14:37:30.827173: step 1588, loss 0.12192, acc 0.953125, prec 0.0364644, recall 0.801625
2017-12-10T14:37:31.018756: step 1589, loss 0.219048, acc 0.9375, prec 0.0364599, recall 0.801625
2017-12-10T14:37:31.212794: step 1590, loss 4.17683, acc 0.921875, prec 0.0364851, recall 0.801217
2017-12-10T14:37:31.408211: step 1591, loss 0.142132, acc 0.953125, prec 0.0364817, recall 0.801217
2017-12-10T14:37:31.596592: step 1592, loss 1.32637, acc 0.875, prec 0.0365024, recall 0.801351
2017-12-10T14:37:31.785250: step 1593, loss 0.291266, acc 0.90625, prec 0.0365253, recall 0.801485
2017-12-10T14:37:31.975120: step 1594, loss 0.379587, acc 0.9375, prec 0.0365505, recall 0.801619
2017-12-10T14:37:32.160530: step 1595, loss 0.35513, acc 0.890625, prec 0.0366315, recall 0.80202
2017-12-10T14:37:32.347537: step 1596, loss 0.448515, acc 0.90625, prec 0.036684, recall 0.802287
2017-12-10T14:37:32.532112: step 1597, loss 0.409884, acc 0.875, prec 0.036675, recall 0.802287
2017-12-10T14:37:32.716759: step 1598, loss 0.364373, acc 0.875, prec 0.0366956, recall 0.802419
2017-12-10T14:37:32.903005: step 1599, loss 0.358209, acc 0.90625, prec 0.0367184, recall 0.802552
2017-12-10T14:37:33.090423: step 1600, loss 0.27462, acc 0.90625, prec 0.0367116, recall 0.802552
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_3/1512934336/checkpoints/model-1600

2017-12-10T14:37:34.274165: step 1601, loss 0.493701, acc 0.8125, prec 0.0366981, recall 0.802552
2017-12-10T14:37:34.460725: step 1602, loss 0.181478, acc 0.90625, prec 0.0367209, recall 0.802685
2017-12-10T14:37:34.645117: step 1603, loss 0.3715, acc 0.90625, prec 0.0367437, recall 0.802817
2017-12-10T14:37:34.832905: step 1604, loss 0.409579, acc 0.84375, prec 0.0367324, recall 0.802817
2017-12-10T14:37:35.017365: step 1605, loss 0.342008, acc 0.875, prec 0.0367234, recall 0.802817
2017-12-10T14:37:35.199124: step 1606, loss 0.334402, acc 0.90625, prec 0.0367167, recall 0.802817
2017-12-10T14:37:35.383972: step 1607, loss 0.27584, acc 0.875, prec 0.0367077, recall 0.802817
2017-12-10T14:37:35.570942: step 1608, loss 0.0842403, acc 0.96875, prec 0.0367054, recall 0.802817
2017-12-10T14:37:35.756646: step 1609, loss 0.419232, acc 0.921875, prec 0.0367588, recall 0.803081
2017-12-10T14:37:35.945764: step 1610, loss 0.646743, acc 0.90625, prec 0.0367816, recall 0.803213
2017-12-10T14:37:36.133422: step 1611, loss 0.0888752, acc 0.96875, prec 0.0368089, recall 0.803344
2017-12-10T14:37:36.318526: step 1612, loss 5.26344, acc 0.890625, prec 0.0368021, recall 0.802808
2017-12-10T14:37:36.508437: step 1613, loss 0.323596, acc 0.875, prec 0.0367931, recall 0.802808
2017-12-10T14:37:36.692008: step 1614, loss 0.185093, acc 0.953125, prec 0.0368192, recall 0.802939
2017-12-10T14:37:36.882515: step 1615, loss 4.41435, acc 0.84375, prec 0.0368386, recall 0.802535
2017-12-10T14:37:37.075028: step 1616, loss 0.465379, acc 0.921875, prec 0.0368624, recall 0.802667
2017-12-10T14:37:37.261688: step 1617, loss 0.34518, acc 0.90625, prec 0.0368556, recall 0.802667
2017-12-10T14:37:37.450820: step 1618, loss 0.339027, acc 0.8125, prec 0.0368421, recall 0.802667
2017-12-10T14:37:37.637702: step 1619, loss 0.386056, acc 0.875, prec 0.0368331, recall 0.802667
2017-12-10T14:37:37.827276: step 1620, loss 0.76797, acc 0.765625, prec 0.0368456, recall 0.802798
2017-12-10T14:37:38.014606: step 1621, loss 0.642943, acc 0.8125, prec 0.0368321, recall 0.802798
2017-12-10T14:37:38.206947: step 1622, loss 0.88891, acc 0.75, prec 0.0368436, recall 0.802929
2017-12-10T14:37:38.393050: step 1623, loss 1.16385, acc 0.71875, prec 0.0368233, recall 0.802929
2017-12-10T14:37:38.577031: step 1624, loss 0.510033, acc 0.8125, prec 0.0368392, recall 0.803061
2017-12-10T14:37:38.761397: step 1625, loss 1.12887, acc 0.71875, prec 0.0368777, recall 0.803322
2017-12-10T14:37:38.950820: step 1626, loss 0.653039, acc 0.8125, prec 0.036923, recall 0.803583
2017-12-10T14:37:39.138724: step 1627, loss 0.291775, acc 0.890625, prec 0.0369151, recall 0.803583
2017-12-10T14:37:39.326279: step 1628, loss 0.522312, acc 0.796875, prec 0.0369298, recall 0.803714
2017-12-10T14:37:39.513412: step 1629, loss 0.811256, acc 0.734375, prec 0.0369107, recall 0.803714
2017-12-10T14:37:39.697816: step 1630, loss 2.85093, acc 0.9375, prec 0.0369953, recall 0.803571
2017-12-10T14:37:39.886129: step 1631, loss 0.323565, acc 0.90625, prec 0.0370179, recall 0.803701
2017-12-10T14:37:40.072034: step 1632, loss 0.596199, acc 0.828125, prec 0.0370055, recall 0.803701
2017-12-10T14:37:40.255824: step 1633, loss 0.706078, acc 0.78125, prec 0.037019, recall 0.803831
2017-12-10T14:37:40.438567: step 1634, loss 0.988054, acc 0.703125, prec 0.0370269, recall 0.80396
2017-12-10T14:37:40.625594: step 1635, loss 0.54661, acc 0.8125, prec 0.0370134, recall 0.80396
2017-12-10T14:37:40.814245: step 1636, loss 0.752769, acc 0.78125, prec 0.0369977, recall 0.80396
2017-12-10T14:37:41.003168: step 1637, loss 6.68011, acc 0.8125, prec 0.0369853, recall 0.80343
2017-12-10T14:37:41.188285: step 1638, loss 1.59459, acc 0.875, prec 0.0370056, recall 0.80356
2017-12-10T14:37:41.375508: step 1639, loss 0.698072, acc 0.71875, prec 0.0370438, recall 0.803818
2017-12-10T14:37:41.560246: step 1640, loss 0.731516, acc 0.796875, prec 0.0371168, recall 0.804205
2017-12-10T14:37:41.748935: step 1641, loss 0.639296, acc 0.8125, prec 0.0371325, recall 0.804334
2017-12-10T14:37:41.933853: step 1642, loss 0.736572, acc 0.796875, prec 0.0371178, recall 0.804334
2017-12-10T14:37:42.118394: step 1643, loss 0.832264, acc 0.796875, prec 0.0371032, recall 0.804334
2017-12-10T14:37:42.302489: step 1644, loss 0.720838, acc 0.796875, prec 0.0370886, recall 0.804334
2017-12-10T14:37:42.486866: step 1645, loss 0.354788, acc 0.828125, prec 0.0370763, recall 0.804334
2017-12-10T14:37:42.671862: step 1646, loss 0.727999, acc 0.78125, prec 0.0370606, recall 0.804334
2017-12-10T14:37:42.857796: step 1647, loss 0.620813, acc 0.84375, prec 0.0371076, recall 0.80459
2017-12-10T14:37:43.044599: step 1648, loss 0.391995, acc 0.875, prec 0.0371277, recall 0.804718
2017-12-10T14:37:43.227258: step 1649, loss 0.646328, acc 0.828125, prec 0.0371736, recall 0.804974
2017-12-10T14:37:43.413147: step 1650, loss 0.438681, acc 0.859375, prec 0.0371635, recall 0.804974
2017-12-10T14:37:43.596441: step 1651, loss 0.653413, acc 0.875, prec 0.0372127, recall 0.805229
2017-12-10T14:37:43.778716: step 1652, loss 0.727457, acc 0.859375, prec 0.0372316, recall 0.805356
2017-12-10T14:37:43.969432: step 1653, loss 0.799923, acc 0.78125, prec 0.037245, recall 0.805483
2017-12-10T14:37:44.162311: step 1654, loss 0.500582, acc 0.859375, prec 0.0372348, recall 0.805483
2017-12-10T14:37:44.347682: step 1655, loss 0.566321, acc 0.8125, prec 0.0372504, recall 0.80561
2017-12-10T14:37:44.535455: step 1656, loss 0.490356, acc 0.859375, prec 0.0372693, recall 0.805737
2017-12-10T14:37:44.721074: step 1657, loss 0.291475, acc 0.890625, prec 0.0372615, recall 0.805737
2017-12-10T14:37:44.909529: step 1658, loss 0.383084, acc 0.84375, prec 0.0372502, recall 0.805737
2017-12-10T14:37:45.104590: step 1659, loss 0.263759, acc 0.921875, prec 0.0372446, recall 0.805737
2017-12-10T14:37:45.293106: step 1660, loss 1.32447, acc 0.921875, prec 0.037326, recall 0.806116
2017-12-10T14:37:45.484077: step 1661, loss 4.38258, acc 0.953125, prec 0.0373818, recall 0.805844
2017-12-10T14:37:45.675092: step 1662, loss 5.04651, acc 0.875, prec 0.0373739, recall 0.805321
2017-12-10T14:37:45.863754: step 1663, loss 0.811187, acc 0.84375, prec 0.0374206, recall 0.805574
2017-12-10T14:37:46.051205: step 1664, loss 0.285959, acc 0.9375, prec 0.0374451, recall 0.805699
2017-12-10T14:37:46.235347: step 1665, loss 0.583818, acc 0.859375, prec 0.0374349, recall 0.805699
2017-12-10T14:37:46.417998: step 1666, loss 0.428969, acc 0.84375, prec 0.0374526, recall 0.805825
2017-12-10T14:37:46.603250: step 1667, loss 0.594367, acc 0.859375, prec 0.0374425, recall 0.805825
2017-12-10T14:37:46.787005: step 1668, loss 0.677172, acc 0.8125, prec 0.037429, recall 0.805825
2017-12-10T14:37:46.976363: step 1669, loss 0.519358, acc 0.828125, prec 0.0374455, recall 0.805951
2017-12-10T14:37:47.163265: step 1670, loss 0.722768, acc 0.78125, prec 0.0374587, recall 0.806076
2017-12-10T14:37:47.351478: step 1671, loss 0.510698, acc 0.734375, prec 0.0374685, recall 0.806202
2017-12-10T14:37:47.537265: step 1672, loss 0.763986, acc 0.765625, prec 0.0374805, recall 0.806327
2017-12-10T14:37:47.724969: step 1673, loss 1.04625, acc 0.765625, prec 0.0375502, recall 0.806701
2017-12-10T14:37:47.916072: step 1674, loss 1.14206, acc 0.703125, prec 0.0375289, recall 0.806701
2017-12-10T14:37:48.099136: step 1675, loss 0.859736, acc 0.734375, prec 0.0375674, recall 0.80695
2017-12-10T14:37:48.287516: step 1676, loss 0.552028, acc 0.78125, prec 0.0375517, recall 0.80695
2017-12-10T14:37:48.471651: step 1677, loss 0.736565, acc 0.765625, prec 0.0375348, recall 0.80695
2017-12-10T14:37:48.655556: step 1678, loss 0.437501, acc 0.828125, prec 0.0375224, recall 0.80695
2017-12-10T14:37:48.842847: step 1679, loss 0.722837, acc 0.796875, prec 0.0375366, recall 0.807074
2017-12-10T14:37:49.024325: step 1680, loss 0.512854, acc 0.84375, prec 0.0375542, recall 0.807198
2017-12-10T14:37:49.208520: step 1681, loss 0.740438, acc 0.828125, prec 0.0375418, recall 0.807198
2017-12-10T14:37:49.396478: step 1682, loss 2.60328, acc 0.859375, prec 0.0375904, recall 0.806928
2017-12-10T14:37:49.587582: step 1683, loss 0.302177, acc 0.90625, prec 0.0375837, recall 0.806928
2017-12-10T14:37:49.775759: step 1684, loss 0.348651, acc 0.890625, prec 0.0375758, recall 0.806928
2017-12-10T14:37:49.958688: step 1685, loss 0.709434, acc 0.8125, prec 0.0375911, recall 0.807051
2017-12-10T14:37:50.143022: step 1686, loss 0.532283, acc 0.859375, prec 0.0376671, recall 0.807422
2017-12-10T14:37:50.333970: step 1687, loss 0.393804, acc 0.859375, prec 0.037657, recall 0.807422
2017-12-10T14:37:50.520754: step 1688, loss 0.233952, acc 0.90625, prec 0.037679, recall 0.807545
2017-12-10T14:37:50.704668: step 1689, loss 4.4656, acc 0.84375, prec 0.0377263, recall 0.807275
2017-12-10T14:37:50.888636: step 1690, loss 0.470518, acc 0.859375, prec 0.0377162, recall 0.807275
2017-12-10T14:37:51.073071: step 1691, loss 0.262517, acc 0.859375, prec 0.0377634, recall 0.807521
2017-12-10T14:37:51.256003: step 1692, loss 0.61565, acc 0.875, prec 0.0377544, recall 0.807521
2017-12-10T14:37:51.440167: step 1693, loss 0.354625, acc 0.828125, prec 0.037742, recall 0.807521
2017-12-10T14:37:51.626738: step 1694, loss 0.179794, acc 0.921875, prec 0.0377364, recall 0.807521
2017-12-10T14:37:51.813122: step 1695, loss 0.586351, acc 0.84375, prec 0.0377252, recall 0.807521
2017-12-10T14:37:51.996964: step 1696, loss 0.26164, acc 0.90625, prec 0.0377757, recall 0.807766
2017-12-10T14:37:52.180630: step 1697, loss 0.568434, acc 0.921875, prec 0.0377987, recall 0.807888
2017-12-10T14:37:52.366528: step 1698, loss 0.478556, acc 0.875, prec 0.0378184, recall 0.80801
2017-12-10T14:37:52.550050: step 1699, loss 0.200444, acc 0.90625, prec 0.0378116, recall 0.80801
2017-12-10T14:37:52.737016: step 1700, loss 0.315054, acc 0.84375, prec 0.0378004, recall 0.80801
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_3/1512934336/checkpoints/model-1700

2017-12-10T14:37:53.805403: step 1701, loss 0.558659, acc 0.859375, prec 0.0378189, recall 0.808132
2017-12-10T14:37:53.991627: step 1702, loss 0.486841, acc 0.859375, prec 0.0378374, recall 0.808254
2017-12-10T14:37:54.182121: step 1703, loss 0.538343, acc 0.84375, prec 0.0379119, recall 0.808618
2017-12-10T14:37:54.370197: step 1704, loss 2.54442, acc 0.875, prec 0.0379326, recall 0.808228
2017-12-10T14:37:54.560880: step 1705, loss 0.26173, acc 0.9375, prec 0.0380138, recall 0.808591
2017-12-10T14:37:54.744272: step 1706, loss 0.426422, acc 0.890625, prec 0.0380059, recall 0.808591
2017-12-10T14:37:54.930056: step 1707, loss 0.246705, acc 0.890625, prec 0.0380551, recall 0.808833
2017-12-10T14:37:55.115469: step 1708, loss 0.917798, acc 0.90625, prec 0.0380769, recall 0.808953
2017-12-10T14:37:55.304091: step 1709, loss 0.391855, acc 0.921875, prec 0.0380712, recall 0.808953
2017-12-10T14:37:55.492643: step 1710, loss 0.620255, acc 0.828125, prec 0.0380588, recall 0.808953
2017-12-10T14:37:55.678241: step 1711, loss 0.800136, acc 0.921875, prec 0.0380817, recall 0.809074
2017-12-10T14:37:55.862566: step 1712, loss 0.880136, acc 0.796875, prec 0.0380955, recall 0.809194
2017-12-10T14:37:56.049403: step 1713, loss 0.824253, acc 0.875, prec 0.038172, recall 0.809554
2017-12-10T14:37:56.236215: step 1714, loss 1.60947, acc 0.859375, prec 0.038163, recall 0.809045
2017-12-10T14:37:56.423832: step 1715, loss 0.381687, acc 0.890625, prec 0.038155, recall 0.809045
2017-12-10T14:37:56.610574: step 1716, loss 0.338165, acc 0.921875, prec 0.0382064, recall 0.809285
2017-12-10T14:37:56.793664: step 1717, loss 0.461359, acc 0.875, prec 0.0381973, recall 0.809285
2017-12-10T14:37:56.983991: step 1718, loss 0.498972, acc 0.84375, prec 0.038186, recall 0.809285
2017-12-10T14:37:57.171376: step 1719, loss 0.649978, acc 0.8125, prec 0.0381725, recall 0.809285
2017-12-10T14:37:57.359187: step 1720, loss 1.31528, acc 0.796875, prec 0.0382147, recall 0.809524
2017-12-10T14:37:57.546520: step 1721, loss 0.402141, acc 0.890625, prec 0.0382921, recall 0.809881
2017-12-10T14:37:57.729825: step 1722, loss 0.355177, acc 0.875, prec 0.0383115, recall 0.81
2017-12-10T14:37:57.919196: step 1723, loss 0.750936, acc 0.8125, prec 0.0382979, recall 0.81
2017-12-10T14:37:58.106872: step 1724, loss 0.446527, acc 0.84375, prec 0.038315, recall 0.810119
2017-12-10T14:37:58.292476: step 1725, loss 7.00581, acc 0.84375, prec 0.0383332, recall 0.809732
2017-12-10T14:37:58.481949: step 1726, loss 0.5325, acc 0.796875, prec 0.0383185, recall 0.809732
2017-12-10T14:37:58.666677: step 1727, loss 0.644152, acc 0.75, prec 0.0383004, recall 0.809732
2017-12-10T14:37:58.850746: step 1728, loss 0.468848, acc 0.84375, prec 0.0383175, recall 0.80985
2017-12-10T14:37:59.041509: step 1729, loss 0.585888, acc 0.796875, prec 0.0383028, recall 0.80985
2017-12-10T14:37:59.231025: step 1730, loss 0.769422, acc 0.75, prec 0.038313, recall 0.809969
2017-12-10T14:37:59.412323: step 1731, loss 0.771009, acc 0.796875, prec 0.0383267, recall 0.810087
2017-12-10T14:37:59.596342: step 1732, loss 0.781869, acc 0.734375, prec 0.0383075, recall 0.810087
2017-12-10T14:37:59.782064: step 1733, loss 0.818191, acc 0.75, prec 0.0382895, recall 0.810087
2017-12-10T14:37:59.967611: step 1734, loss 0.631841, acc 0.859375, prec 0.0383359, recall 0.810323
2017-12-10T14:38:00.152349: step 1735, loss 0.573297, acc 0.8125, prec 0.0383507, recall 0.810441
2017-12-10T14:38:00.342606: step 1736, loss 0.872746, acc 0.8125, prec 0.0384502, recall 0.810911
2017-12-10T14:38:00.529033: step 1737, loss 0.9111, acc 0.859375, prec 0.0384966, recall 0.811145
2017-12-10T14:38:00.716323: step 1738, loss 0.525917, acc 0.859375, prec 0.0385429, recall 0.811379
2017-12-10T14:38:00.904130: step 1739, loss 0.865482, acc 0.828125, prec 0.0385587, recall 0.811496
2017-12-10T14:38:01.094879: step 1740, loss 3.43153, acc 0.84375, prec 0.0386332, recall 0.811344
2017-12-10T14:38:01.287461: step 1741, loss 0.545214, acc 0.78125, prec 0.0386173, recall 0.811344
2017-12-10T14:38:01.477442: step 1742, loss 0.460858, acc 0.875, prec 0.0386364, recall 0.81146
2017-12-10T14:38:01.664925: step 1743, loss 0.553366, acc 0.828125, prec 0.0386804, recall 0.811692
2017-12-10T14:38:01.852235: step 1744, loss 0.534922, acc 0.84375, prec 0.0387254, recall 0.811924
2017-12-10T14:38:02.036604: step 1745, loss 0.740273, acc 0.796875, prec 0.0387388, recall 0.812039
2017-12-10T14:38:02.223787: step 1746, loss 0.571178, acc 0.84375, prec 0.0387274, recall 0.812039
2017-12-10T14:38:02.407229: step 1747, loss 0.513207, acc 0.78125, prec 0.0387116, recall 0.812039
2017-12-10T14:38:02.589206: step 1748, loss 0.788604, acc 0.796875, prec 0.0386968, recall 0.812039
2017-12-10T14:38:02.774080: step 1749, loss 0.66908, acc 0.796875, prec 0.0386821, recall 0.812039
2017-12-10T14:38:02.963032: step 1750, loss 0.668198, acc 0.84375, prec 0.0386708, recall 0.812039
2017-12-10T14:38:03.146515: step 1751, loss 0.618331, acc 0.796875, prec 0.0386561, recall 0.812039
2017-12-10T14:38:03.333768: step 1752, loss 0.870693, acc 0.71875, prec 0.038692, recall 0.81227
2017-12-10T14:38:03.521270: step 1753, loss 0.338659, acc 0.8125, prec 0.0386784, recall 0.81227
2017-12-10T14:38:03.707141: step 1754, loss 0.306194, acc 0.90625, prec 0.0386997, recall 0.812385
2017-12-10T14:38:03.894692: step 1755, loss 0.116764, acc 0.96875, prec 0.0387255, recall 0.8125
2017-12-10T14:38:04.082075: step 1756, loss 0.422117, acc 0.875, prec 0.0387445, recall 0.812615
2017-12-10T14:38:04.276280: step 1757, loss 0.429736, acc 0.875, prec 0.0387916, recall 0.812844
2017-12-10T14:38:04.460543: step 1758, loss 0.185421, acc 0.90625, prec 0.0387848, recall 0.812844
2017-12-10T14:38:04.646264: step 1759, loss 0.221388, acc 0.9375, prec 0.0387803, recall 0.812844
2017-12-10T14:38:04.832272: step 1760, loss 0.290477, acc 0.875, prec 0.0387993, recall 0.812958
2017-12-10T14:38:05.019251: step 1761, loss 0.11845, acc 0.953125, prec 0.0387959, recall 0.812958
2017-12-10T14:38:05.209796: step 1762, loss 0.28425, acc 0.890625, prec 0.0387879, recall 0.812958
2017-12-10T14:38:05.400461: step 1763, loss 5.4948, acc 0.953125, prec 0.0387857, recall 0.812462
2017-12-10T14:38:05.590046: step 1764, loss 0.109248, acc 0.96875, prec 0.0387834, recall 0.812462
2017-12-10T14:38:05.773919: step 1765, loss 0.591822, acc 0.859375, prec 0.0388013, recall 0.812576
2017-12-10T14:38:05.962952: step 1766, loss 0.177187, acc 0.921875, prec 0.0388236, recall 0.812691
2017-12-10T14:38:06.149273: step 1767, loss 0.135521, acc 0.953125, prec 0.0388483, recall 0.812805
2017-12-10T14:38:06.338002: step 1768, loss 0.233732, acc 0.921875, prec 0.0388426, recall 0.812805
2017-12-10T14:38:06.528368: step 1769, loss 0.400418, acc 0.9375, prec 0.0388941, recall 0.813033
2017-12-10T14:38:06.716935: step 1770, loss 0.208242, acc 0.9375, prec 0.0388895, recall 0.813033
2017-12-10T14:38:06.900801: step 1771, loss 10.1798, acc 0.90625, prec 0.038913, recall 0.812158
2017-12-10T14:38:07.089618: step 1772, loss 0.267513, acc 0.890625, prec 0.0389051, recall 0.812158
2017-12-10T14:38:07.274834: step 1773, loss 0.837226, acc 0.859375, prec 0.0389229, recall 0.812272
2017-12-10T14:38:07.466365: step 1774, loss 0.751025, acc 0.890625, prec 0.0389429, recall 0.812386
2017-12-10T14:38:07.651480: step 1775, loss 0.537014, acc 0.75, prec 0.0389248, recall 0.812386
2017-12-10T14:38:07.837983: step 1776, loss 1.89237, acc 0.84375, prec 0.0389146, recall 0.811893
2017-12-10T14:38:08.025417: step 1777, loss 0.981247, acc 0.703125, prec 0.038921, recall 0.812007
2017-12-10T14:38:08.209965: step 1778, loss 0.681149, acc 0.765625, prec 0.038932, recall 0.812121
2017-12-10T14:38:08.401428: step 1779, loss 0.844686, acc 0.75, prec 0.0389139, recall 0.812121
2017-12-10T14:38:08.588578: step 1780, loss 0.868432, acc 0.6875, prec 0.0388913, recall 0.812121
2017-12-10T14:38:08.771746: step 1781, loss 1.03497, acc 0.671875, prec 0.0388676, recall 0.812121
2017-12-10T14:38:08.957003: step 1782, loss 1.36173, acc 0.59375, prec 0.038894, recall 0.812349
2017-12-10T14:38:09.137659: step 1783, loss 3.32062, acc 0.6875, prec 0.0389005, recall 0.811971
2017-12-10T14:38:09.321755: step 1784, loss 0.912016, acc 0.71875, prec 0.038908, recall 0.812085
2017-12-10T14:38:09.509064: step 1785, loss 1.30773, acc 0.640625, prec 0.0388821, recall 0.812085
2017-12-10T14:38:09.687927: step 1786, loss 0.95713, acc 0.734375, prec 0.038863, recall 0.812085
2017-12-10T14:38:09.872821: step 1787, loss 0.985215, acc 0.734375, prec 0.0388717, recall 0.812198
2017-12-10T14:38:10.053913: step 1788, loss 1.33469, acc 0.671875, prec 0.0388481, recall 0.812198
2017-12-10T14:38:10.237584: step 1789, loss 1.14147, acc 0.703125, prec 0.0388546, recall 0.812311
2017-12-10T14:38:10.425385: step 1790, loss 0.961886, acc 0.75, prec 0.0388366, recall 0.812311
2017-12-10T14:38:10.608859: step 1791, loss 0.871865, acc 0.734375, prec 0.0388453, recall 0.812425
2017-12-10T14:38:10.794940: step 1792, loss 0.74755, acc 0.71875, prec 0.0388252, recall 0.812425
2017-12-10T14:38:10.979173: step 1793, loss 0.604029, acc 0.84375, prec 0.0388417, recall 0.812538
2017-12-10T14:38:11.167689: step 1794, loss 0.632074, acc 0.765625, prec 0.0388249, recall 0.812538
2017-12-10T14:38:11.350469: step 1795, loss 0.821124, acc 0.75, prec 0.0388347, recall 0.812651
2017-12-10T14:38:11.538033: step 1796, loss 0.406946, acc 0.859375, prec 0.0388246, recall 0.812651
2017-12-10T14:38:11.720112: step 1797, loss 0.735322, acc 0.796875, prec 0.0388654, recall 0.812876
2017-12-10T14:38:11.908342: step 1798, loss 0.341463, acc 0.890625, prec 0.0389129, recall 0.813101
2017-12-10T14:38:12.092911: step 1799, loss 0.412898, acc 0.875, prec 0.0389868, recall 0.813437
2017-12-10T14:38:12.279407: step 1800, loss 0.692886, acc 0.875, prec 0.0390331, recall 0.813661
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_3/1512934336/checkpoints/model-1800

2017-12-10T14:38:13.356080: step 1801, loss 0.166344, acc 0.921875, prec 0.0390275, recall 0.813661
2017-12-10T14:38:13.540458: step 1802, loss 0.219413, acc 0.9375, prec 0.039023, recall 0.813661
2017-12-10T14:38:13.727602: step 1803, loss 2.90835, acc 0.9375, prec 0.0390748, recall 0.813397
2017-12-10T14:38:13.915683: step 1804, loss 2.08538, acc 0.921875, prec 0.039098, recall 0.813023
2017-12-10T14:38:14.114876: step 1805, loss 0.460277, acc 0.859375, prec 0.0390879, recall 0.813023
2017-12-10T14:38:14.298669: step 1806, loss 1.75402, acc 0.9375, prec 0.0391121, recall 0.812649
2017-12-10T14:38:14.486529: step 1807, loss 0.646361, acc 0.8125, prec 0.0391262, recall 0.812761
2017-12-10T14:38:14.672295: step 1808, loss 0.238155, acc 0.90625, prec 0.0391195, recall 0.812761
2017-12-10T14:38:14.859664: step 1809, loss 0.466803, acc 0.796875, prec 0.0391049, recall 0.812761
2017-12-10T14:38:15.045359: step 1810, loss 0.662077, acc 0.8125, prec 0.0391741, recall 0.813095
2017-12-10T14:38:15.229061: step 1811, loss 0.624387, acc 0.8125, prec 0.0391606, recall 0.813095
2017-12-10T14:38:15.412984: step 1812, loss 1.53295, acc 0.75, prec 0.0391977, recall 0.813317
2017-12-10T14:38:15.600888: step 1813, loss 0.598926, acc 0.765625, prec 0.0391809, recall 0.813317
2017-12-10T14:38:15.789011: step 1814, loss 0.774988, acc 0.78125, prec 0.0391652, recall 0.813317
2017-12-10T14:38:15.972158: step 1815, loss 0.681257, acc 0.75, prec 0.0391472, recall 0.813317
2017-12-10T14:38:16.156842: step 1816, loss 1.16228, acc 0.609375, prec 0.0391467, recall 0.813428
2017-12-10T14:38:16.337543: step 1817, loss 0.712502, acc 0.703125, prec 0.0391255, recall 0.813428
2017-12-10T14:38:16.525714: step 1818, loss 0.882997, acc 0.765625, prec 0.0391636, recall 0.81365
2017-12-10T14:38:16.712674: step 1819, loss 1.17776, acc 0.703125, prec 0.0391698, recall 0.81376
2017-12-10T14:38:16.904271: step 1820, loss 1.86135, acc 0.75, prec 0.0391793, recall 0.813871
2017-12-10T14:38:17.096760: step 1821, loss 0.745491, acc 0.765625, prec 0.0392174, recall 0.814091
2017-12-10T14:38:17.282836: step 1822, loss 0.713897, acc 0.78125, prec 0.0392565, recall 0.814311
2017-12-10T14:38:17.468187: step 1823, loss 0.667175, acc 0.734375, prec 0.0392375, recall 0.814311
2017-12-10T14:38:17.653857: step 1824, loss 0.642019, acc 0.859375, prec 0.0392274, recall 0.814311
2017-12-10T14:38:17.840545: step 1825, loss 0.878892, acc 0.75, prec 0.0392095, recall 0.814311
2017-12-10T14:38:18.027113: step 1826, loss 0.476709, acc 0.90625, prec 0.0392302, recall 0.814421
2017-12-10T14:38:18.210557: step 1827, loss 2.43648, acc 0.765625, prec 0.0392146, recall 0.81394
2017-12-10T14:38:18.398931: step 1828, loss 0.633551, acc 0.84375, prec 0.0392307, recall 0.81405
2017-12-10T14:38:18.586683: step 1829, loss 1.0146, acc 0.828125, prec 0.0392731, recall 0.814269
2017-12-10T14:38:18.769882: step 1830, loss 0.282469, acc 0.90625, prec 0.0393211, recall 0.814488
2017-12-10T14:38:18.958016: step 1831, loss 0.132737, acc 0.9375, prec 0.0393166, recall 0.814488
2017-12-10T14:38:19.143724: step 1832, loss 0.304329, acc 0.84375, prec 0.0393054, recall 0.814488
2017-12-10T14:38:19.333548: step 1833, loss 1.20389, acc 0.8125, prec 0.0393466, recall 0.814706
2017-12-10T14:38:19.524477: step 1834, loss 0.640673, acc 0.828125, prec 0.0393343, recall 0.814706
2017-12-10T14:38:19.713036: step 1835, loss 0.38524, acc 0.796875, prec 0.0393471, recall 0.814815
2017-12-10T14:38:19.900459: step 1836, loss 0.542576, acc 0.859375, prec 0.039337, recall 0.814815
2017-12-10T14:38:20.087118: step 1837, loss 4.8916, acc 0.8125, prec 0.039352, recall 0.814445
2017-12-10T14:38:20.274733: step 1838, loss 0.667122, acc 0.84375, prec 0.0393408, recall 0.814445
2017-12-10T14:38:20.460437: step 1839, loss 0.561604, acc 0.828125, prec 0.0393558, recall 0.814554
2017-12-10T14:38:20.644255: step 1840, loss 0.476134, acc 0.875, prec 0.0393469, recall 0.814554
2017-12-10T14:38:20.833106: step 1841, loss 0.66445, acc 0.796875, prec 0.0393596, recall 0.814663
2017-12-10T14:38:21.016754: step 1842, loss 0.798545, acc 0.765625, prec 0.0393701, recall 0.814771
2017-12-10T14:38:21.204206: step 1843, loss 0.916233, acc 0.734375, prec 0.0393783, recall 0.81488
2017-12-10T14:38:21.389607: step 1844, loss 0.381009, acc 0.890625, prec 0.0393705, recall 0.81488
2017-12-10T14:38:21.572254: step 1845, loss 0.701772, acc 0.765625, prec 0.0393538, recall 0.81488
2017-12-10T14:38:21.754403: step 1846, loss 0.345096, acc 0.84375, prec 0.0393427, recall 0.81488
2017-12-10T14:38:21.940659: step 1847, loss 1.05317, acc 0.71875, prec 0.0393227, recall 0.81488
2017-12-10T14:38:22.125028: step 1848, loss 0.658005, acc 0.796875, prec 0.0393082, recall 0.81488
2017-12-10T14:38:22.316010: step 1849, loss 2.11371, acc 0.90625, prec 0.0393027, recall 0.814403
2017-12-10T14:38:22.503223: step 1850, loss 0.352409, acc 0.90625, prec 0.0393503, recall 0.81462
2017-12-10T14:38:22.694000: step 1851, loss 0.334452, acc 0.875, prec 0.0393685, recall 0.814728
2017-12-10T14:38:22.878764: step 1852, loss 0.487016, acc 0.875, prec 0.0393868, recall 0.814836
2017-12-10T14:38:23.064446: step 1853, loss 0.345488, acc 0.90625, prec 0.0394072, recall 0.814945
2017-12-10T14:38:23.251549: step 1854, loss 0.25837, acc 0.9375, prec 0.0394027, recall 0.814945
2017-12-10T14:38:23.437222: step 1855, loss 0.908676, acc 0.953125, prec 0.0394265, recall 0.815053
2017-12-10T14:38:23.625526: step 1856, loss 0.103999, acc 0.953125, prec 0.0394232, recall 0.815053
2017-12-10T14:38:23.812301: step 1857, loss 0.687303, acc 0.9375, prec 0.0395, recall 0.815376
2017-12-10T14:38:23.999856: step 1858, loss 0.43596, acc 0.9375, prec 0.0395227, recall 0.815483
2017-12-10T14:38:24.188381: step 1859, loss 0.40217, acc 0.890625, prec 0.0395149, recall 0.815483
2017-12-10T14:38:24.373978: step 1860, loss 4.68114, acc 0.921875, prec 0.0395386, recall 0.814643
2017-12-10T14:38:24.566855: step 1861, loss 0.322417, acc 0.90625, prec 0.0395861, recall 0.814858
2017-12-10T14:38:24.754263: step 1862, loss 0.523535, acc 0.859375, prec 0.039576, recall 0.814858
2017-12-10T14:38:24.938297: step 1863, loss 0.463978, acc 0.828125, prec 0.0395908, recall 0.814965
2017-12-10T14:38:25.123106: step 1864, loss 1.02117, acc 0.6875, prec 0.0395956, recall 0.815072
2017-12-10T14:38:25.307899: step 1865, loss 0.885587, acc 0.703125, prec 0.0395744, recall 0.815072
2017-12-10T14:38:25.492246: step 1866, loss 0.68079, acc 0.828125, prec 0.0395892, recall 0.81518
2017-12-10T14:38:25.678756: step 1867, loss 0.736046, acc 0.78125, prec 0.0396006, recall 0.815287
2017-12-10T14:38:25.864535: step 1868, loss 0.751778, acc 0.78125, prec 0.039585, recall 0.815287
2017-12-10T14:38:26.047665: step 1869, loss 0.633248, acc 0.796875, prec 0.0395706, recall 0.815287
2017-12-10T14:38:26.233917: step 1870, loss 0.881947, acc 0.75, prec 0.0395798, recall 0.815394
2017-12-10T14:38:26.418790: step 1871, loss 1.32062, acc 0.8125, prec 0.0396204, recall 0.815607
2017-12-10T14:38:26.605105: step 1872, loss 1.02991, acc 0.75, prec 0.0396026, recall 0.815607
2017-12-10T14:38:26.789178: step 1873, loss 0.704435, acc 0.75, prec 0.0395848, recall 0.815607
2017-12-10T14:38:26.973355: step 1874, loss 0.547574, acc 0.890625, prec 0.039577, recall 0.815607
2017-12-10T14:38:27.157744: step 1875, loss 0.475867, acc 0.796875, prec 0.0395626, recall 0.815607
2017-12-10T14:38:27.345650: step 1876, loss 0.596733, acc 0.796875, prec 0.0395482, recall 0.815607
2017-12-10T14:38:27.530239: step 1877, loss 0.7688, acc 0.8125, prec 0.0395349, recall 0.815607
2017-12-10T14:38:27.715427: step 1878, loss 0.306146, acc 0.90625, prec 0.0395282, recall 0.815607
2017-12-10T14:38:27.901622: step 1879, loss 0.60124, acc 0.84375, prec 0.0395441, recall 0.815713
2017-12-10T14:38:28.090490: step 1880, loss 0.617063, acc 0.890625, prec 0.0395901, recall 0.815926
2017-12-10T14:38:28.280323: step 1881, loss 0.37554, acc 0.859375, prec 0.0395801, recall 0.815926
2017-12-10T14:38:28.466817: step 1882, loss 0.328977, acc 0.921875, prec 0.0396015, recall 0.816032
2017-12-10T14:38:28.656170: step 1883, loss 0.455794, acc 0.890625, prec 0.0397012, recall 0.816456
2017-12-10T14:38:28.843516: step 1884, loss 0.800175, acc 0.921875, prec 0.0397494, recall 0.816667
2017-12-10T14:38:29.036283: step 1885, loss 0.593615, acc 0.875, prec 0.0397942, recall 0.816877
2017-12-10T14:38:29.224374: step 1886, loss 0.431077, acc 0.90625, prec 0.039868, recall 0.817192
2017-12-10T14:38:29.411663: step 1887, loss 0.431759, acc 0.953125, prec 0.0399184, recall 0.817401
2017-12-10T14:38:29.595735: step 1888, loss 0.171223, acc 0.921875, prec 0.0399396, recall 0.817506
2017-12-10T14:38:29.780017: step 1889, loss 0.277948, acc 0.90625, prec 0.0399329, recall 0.817506
2017-12-10T14:38:29.966587: step 1890, loss 0.102022, acc 0.953125, prec 0.0399564, recall 0.81761
2017-12-10T14:38:30.156394: step 1891, loss 1.13284, acc 0.875, prec 0.0400011, recall 0.817818
2017-12-10T14:38:30.344502: step 1892, loss 0.112421, acc 0.953125, prec 0.0400246, recall 0.817922
2017-12-10T14:38:30.534491: step 1893, loss 0.358965, acc 0.9375, prec 0.0400469, recall 0.818026
2017-12-10T14:38:30.724394: step 1894, loss 0.292518, acc 0.890625, prec 0.0400391, recall 0.818026
2017-12-10T14:38:30.908434: step 1895, loss 0.295362, acc 0.84375, prec 0.0400279, recall 0.818026
2017-12-10T14:38:31.097700: step 1896, loss 0.373425, acc 0.890625, prec 0.0400201, recall 0.818026
2017-12-10T14:38:31.290573: step 1897, loss 0.249427, acc 0.890625, prec 0.0400123, recall 0.818026
2017-12-10T14:38:31.480935: step 1898, loss 0.194363, acc 0.921875, prec 0.040087, recall 0.818337
2017-12-10T14:38:31.670322: step 1899, loss 0.229423, acc 0.890625, prec 0.0400792, recall 0.818337
2017-12-10T14:38:31.855853: step 1900, loss 1.5763, acc 0.953125, prec 0.0401037, recall 0.817975
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_3/1512934336/checkpoints/model-1900

2017-12-10T14:38:33.191737: step 1901, loss 0.936988, acc 0.90625, prec 0.0401773, recall 0.818285
2017-12-10T14:38:33.381195: step 1902, loss 0.653513, acc 0.875, prec 0.0401951, recall 0.818388
2017-12-10T14:38:33.567939: step 1903, loss 0.813411, acc 0.890625, prec 0.040214, recall 0.818491
2017-12-10T14:38:33.757769: step 1904, loss 0.684954, acc 0.84375, prec 0.0402563, recall 0.818697
2017-12-10T14:38:33.943688: step 1905, loss 0.614198, acc 0.84375, prec 0.0402451, recall 0.818697
2017-12-10T14:38:34.133109: step 1906, loss 0.272256, acc 0.90625, prec 0.0402918, recall 0.818902
2017-12-10T14:38:34.317798: step 1907, loss 0.15925, acc 0.921875, prec 0.0403396, recall 0.819107
2017-12-10T14:38:34.502497: step 1908, loss 0.436813, acc 0.890625, prec 0.0403585, recall 0.819209
2017-12-10T14:38:34.692474: step 1909, loss 0.55961, acc 0.84375, prec 0.0403473, recall 0.819209
2017-12-10T14:38:34.876928: step 1910, loss 0.548748, acc 0.84375, prec 0.040336, recall 0.819209
2017-12-10T14:38:35.064338: step 1911, loss 0.426002, acc 0.90625, prec 0.040356, recall 0.819311
2017-12-10T14:38:35.252200: step 1912, loss 0.345999, acc 0.921875, prec 0.0403504, recall 0.819311
2017-12-10T14:38:35.443947: step 1913, loss 0.436964, acc 0.84375, prec 0.0403658, recall 0.819413
2017-12-10T14:38:35.632546: step 1914, loss 2.51861, acc 0.8125, prec 0.0403802, recall 0.819053
2017-12-10T14:38:35.821364: step 1915, loss 0.318966, acc 0.84375, prec 0.0403956, recall 0.819155
2017-12-10T14:38:36.009678: step 1916, loss 0.414236, acc 0.84375, prec 0.0403844, recall 0.819155
2017-12-10T14:38:36.193879: step 1917, loss 1.49556, acc 0.859375, prec 0.0404809, recall 0.819562
2017-12-10T14:38:36.385613: step 1918, loss 0.689131, acc 0.859375, prec 0.0405773, recall 0.819966
2017-12-10T14:38:36.571568: step 1919, loss 0.286623, acc 0.875, prec 0.0405949, recall 0.820067
2017-12-10T14:38:36.756986: step 1920, loss 0.327442, acc 0.890625, prec 0.040587, recall 0.820067
2017-12-10T14:38:36.946775: step 1921, loss 0.543537, acc 0.796875, prec 0.0405724, recall 0.820067
2017-12-10T14:38:37.129897: step 1922, loss 0.487016, acc 0.84375, prec 0.0406143, recall 0.820269
2017-12-10T14:38:37.318891: step 1923, loss 0.691655, acc 0.78125, prec 0.0406252, recall 0.820369
2017-12-10T14:38:37.505722: step 1924, loss 0.549583, acc 0.78125, prec 0.0406094, recall 0.820369
2017-12-10T14:38:37.693418: step 1925, loss 1.84097, acc 0.84375, prec 0.0405993, recall 0.819911
2017-12-10T14:38:37.880830: step 1926, loss 0.494019, acc 0.84375, prec 0.0405881, recall 0.819911
2017-12-10T14:38:38.066202: step 1927, loss 1.17898, acc 0.84375, prec 0.0406034, recall 0.820011
2017-12-10T14:38:38.259623: step 1928, loss 0.231225, acc 0.90625, prec 0.0405966, recall 0.820011
2017-12-10T14:38:38.443543: step 1929, loss 0.786258, acc 0.8125, prec 0.0405832, recall 0.820011
2017-12-10T14:38:38.631546: step 1930, loss 0.363137, acc 0.875, prec 0.0405742, recall 0.820011
2017-12-10T14:38:38.816544: step 1931, loss 0.340499, acc 0.921875, prec 0.0406216, recall 0.820212
2017-12-10T14:38:39.005196: step 1932, loss 2.46485, acc 0.921875, prec 0.0406437, recall 0.819855
2017-12-10T14:38:39.197887: step 1933, loss 0.585541, acc 0.859375, prec 0.0406601, recall 0.819955
2017-12-10T14:38:39.387399: step 1934, loss 0.585554, acc 0.8125, prec 0.0406466, recall 0.819955
2017-12-10T14:38:39.574287: step 1935, loss 0.573479, acc 0.90625, prec 0.0406664, recall 0.820056
2017-12-10T14:38:39.757876: step 1936, loss 0.235155, acc 0.875, prec 0.0406574, recall 0.820056
2017-12-10T14:38:39.950477: step 1937, loss 2.16488, acc 0.875, prec 0.0406495, recall 0.819599
2017-12-10T14:38:40.140834: step 1938, loss 0.413839, acc 0.859375, prec 0.0406394, recall 0.819599
2017-12-10T14:38:40.326429: step 1939, loss 0.50071, acc 0.78125, prec 0.0406237, recall 0.819599
2017-12-10T14:38:40.512735: step 1940, loss 0.45286, acc 0.796875, prec 0.0406091, recall 0.819599
2017-12-10T14:38:40.698400: step 1941, loss 0.760564, acc 0.75, prec 0.0406177, recall 0.8197
2017-12-10T14:38:40.884753: step 1942, loss 0.495637, acc 0.84375, prec 0.0406329, recall 0.8198
2017-12-10T14:38:41.071512: step 1943, loss 0.669627, acc 0.8125, prec 0.0406195, recall 0.8198
2017-12-10T14:38:41.255630: step 1944, loss 0.956967, acc 0.796875, prec 0.0406049, recall 0.8198
2017-12-10T14:38:41.442538: step 1945, loss 0.44706, acc 0.859375, prec 0.0407006, recall 0.8202
2017-12-10T14:38:41.630429: step 1946, loss 0.684485, acc 0.828125, prec 0.0407146, recall 0.8203
2017-12-10T14:38:41.817701: step 1947, loss 5.58686, acc 0.84375, prec 0.040731, recall 0.819945
2017-12-10T14:38:42.007631: step 1948, loss 3.08369, acc 0.75, prec 0.0407405, recall 0.819591
2017-12-10T14:38:42.199336: step 1949, loss 0.782209, acc 0.8125, prec 0.0407798, recall 0.81979
2017-12-10T14:38:42.383681: step 1950, loss 1.42206, acc 0.78125, prec 0.0408169, recall 0.819989
2017-12-10T14:38:42.569979: step 1951, loss 0.746452, acc 0.78125, prec 0.0408539, recall 0.820188
2017-12-10T14:38:42.756719: step 1952, loss 0.752938, acc 0.796875, prec 0.040892, recall 0.820386
2017-12-10T14:38:42.942639: step 1953, loss 0.653596, acc 0.78125, prec 0.0409289, recall 0.820583
2017-12-10T14:38:43.124325: step 1954, loss 0.920492, acc 0.71875, prec 0.040935, recall 0.820682
2017-12-10T14:38:43.307782: step 1955, loss 1.06544, acc 0.71875, prec 0.0409148, recall 0.820682
2017-12-10T14:38:43.490792: step 1956, loss 0.591799, acc 0.78125, prec 0.0409254, recall 0.820781
2017-12-10T14:38:43.675334: step 1957, loss 0.793267, acc 0.734375, prec 0.0409064, recall 0.820781
2017-12-10T14:38:43.865193: step 1958, loss 1.01332, acc 0.71875, prec 0.0409125, recall 0.820879
2017-12-10T14:38:44.055642: step 1959, loss 0.921396, acc 0.75, prec 0.040947, recall 0.821076
2017-12-10T14:38:44.240626: step 1960, loss 0.855796, acc 0.84375, prec 0.0409883, recall 0.821272
2017-12-10T14:38:44.427187: step 1961, loss 0.78337, acc 0.796875, prec 0.0409737, recall 0.821272
2017-12-10T14:38:44.614700: step 1962, loss 0.866841, acc 0.765625, prec 0.0410356, recall 0.821565
2017-12-10T14:38:44.799465: step 1963, loss 0.508263, acc 0.859375, prec 0.0410517, recall 0.821663
2017-12-10T14:38:44.981191: step 1964, loss 0.526041, acc 0.75, prec 0.0410862, recall 0.821858
2017-12-10T14:38:45.167668: step 1965, loss 0.665229, acc 0.796875, prec 0.0410978, recall 0.821955
2017-12-10T14:38:45.353855: step 1966, loss 2.51798, acc 0.828125, prec 0.0410865, recall 0.821507
2017-12-10T14:38:45.543182: step 1967, loss 0.505406, acc 0.796875, prec 0.041072, recall 0.821507
2017-12-10T14:38:45.728722: step 1968, loss 0.286119, acc 0.859375, prec 0.041088, recall 0.821604
2017-12-10T14:38:45.912781: step 1969, loss 0.58701, acc 0.828125, prec 0.0411019, recall 0.821701
2017-12-10T14:38:46.098927: step 1970, loss 0.450308, acc 0.84375, prec 0.0411691, recall 0.821992
2017-12-10T14:38:46.286107: step 1971, loss 0.705324, acc 0.84375, prec 0.0412101, recall 0.822186
2017-12-10T14:38:46.473132: step 1972, loss 3.03379, acc 0.890625, prec 0.0412034, recall 0.821739
2017-12-10T14:38:46.666432: step 1973, loss 0.686256, acc 0.765625, prec 0.0411866, recall 0.821739
2017-12-10T14:38:46.852704: step 1974, loss 0.283094, acc 0.90625, prec 0.0411798, recall 0.821739
2017-12-10T14:38:47.037092: step 1975, loss 0.671477, acc 0.828125, prec 0.0411675, recall 0.821739
2017-12-10T14:38:47.222444: step 1976, loss 0.340889, acc 0.84375, prec 0.0411824, recall 0.821836
2017-12-10T14:38:47.409262: step 1977, loss 1.47847, acc 0.90625, prec 0.0412279, recall 0.822029
2017-12-10T14:38:47.597619: step 1978, loss 0.274269, acc 0.890625, prec 0.04122, recall 0.822029
2017-12-10T14:38:47.784443: step 1979, loss 1.28989, acc 0.8125, prec 0.0412587, recall 0.822222
2017-12-10T14:38:47.969492: step 1980, loss 0.466179, acc 0.84375, prec 0.0412475, recall 0.822222
2017-12-10T14:38:48.156555: step 1981, loss 0.654875, acc 0.921875, prec 0.041294, recall 0.822415
2017-12-10T14:38:48.342654: step 1982, loss 0.444536, acc 0.859375, prec 0.04131, recall 0.822511
2017-12-10T14:38:48.530824: step 1983, loss 0.39991, acc 0.828125, prec 0.0412976, recall 0.822511
2017-12-10T14:38:48.717018: step 1984, loss 0.523718, acc 0.875, prec 0.0412886, recall 0.822511
2017-12-10T14:38:48.902797: step 1985, loss 0.640261, acc 0.8125, prec 0.0413012, recall 0.822607
2017-12-10T14:38:49.090347: step 1986, loss 6.48833, acc 0.78125, prec 0.0412866, recall 0.822162
2017-12-10T14:38:49.277810: step 1987, loss 2.73264, acc 0.78125, prec 0.0412981, recall 0.821814
2017-12-10T14:38:49.449552: step 1988, loss 0.622138, acc 0.807692, prec 0.0412869, recall 0.821814
2017-12-10T14:38:49.642392: step 1989, loss 1.15648, acc 0.703125, prec 0.0413176, recall 0.822006
2017-12-10T14:38:49.827999: step 1990, loss 1.16671, acc 0.703125, prec 0.0412963, recall 0.822006
2017-12-10T14:38:50.016769: step 1991, loss 0.643491, acc 0.84375, prec 0.0413371, recall 0.822198
2017-12-10T14:38:50.200604: step 1992, loss 0.924452, acc 0.734375, prec 0.0413181, recall 0.822198
2017-12-10T14:38:50.387779: step 1993, loss 1.27348, acc 0.65625, prec 0.0413194, recall 0.822294
2017-12-10T14:38:50.576048: step 1994, loss 0.954227, acc 0.703125, prec 0.0412982, recall 0.822294
2017-12-10T14:38:50.759952: step 1995, loss 0.628431, acc 0.71875, prec 0.0412781, recall 0.822294
2017-12-10T14:38:50.943882: step 1996, loss 1.46495, acc 0.703125, prec 0.0412828, recall 0.82239
2017-12-10T14:38:51.128644: step 1997, loss 0.732118, acc 0.75, prec 0.041265, recall 0.82239
2017-12-10T14:38:51.313530: step 1998, loss 0.921298, acc 0.75, prec 0.0412471, recall 0.82239
2017-12-10T14:38:51.497240: step 1999, loss 1.16628, acc 0.671875, prec 0.0412496, recall 0.822485
2017-12-10T14:38:51.680602: step 2000, loss 0.924851, acc 0.765625, prec 0.0412329, recall 0.822485
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_3/1512934336/checkpoints/model-2000

2017-12-10T14:38:52.850993: step 2001, loss 0.65169, acc 0.859375, prec 0.0412488, recall 0.822581
2017-12-10T14:38:53.038840: step 2002, loss 1.15423, acc 0.78125, prec 0.0412849, recall 0.822771
2017-12-10T14:38:53.225512: step 2003, loss 0.434606, acc 0.84375, prec 0.0412996, recall 0.822866
2017-12-10T14:38:53.408625: step 2004, loss 0.666301, acc 0.921875, prec 0.0413457, recall 0.823056
2017-12-10T14:38:53.602091: step 2005, loss 0.447798, acc 0.796875, prec 0.0413312, recall 0.823056
2017-12-10T14:38:53.785401: step 2006, loss 0.318712, acc 0.96875, prec 0.0413806, recall 0.823246
2017-12-10T14:38:53.967965: step 2007, loss 0.645915, acc 0.8125, prec 0.041393, recall 0.82334
2017-12-10T14:38:54.156899: step 2008, loss 0.112586, acc 0.953125, prec 0.0414155, recall 0.823435
2017-12-10T14:38:54.342260: step 2009, loss 1.07264, acc 0.890625, prec 0.0414593, recall 0.823624
2017-12-10T14:38:54.534158: step 2010, loss 0.213435, acc 0.921875, prec 0.0414537, recall 0.823624
2017-12-10T14:38:54.721364: step 2011, loss 0.344963, acc 0.890625, prec 0.0414459, recall 0.823624
2017-12-10T14:38:54.910497: step 2012, loss 0.178803, acc 0.9375, prec 0.0414414, recall 0.823624
2017-12-10T14:38:55.097547: step 2013, loss 0.265665, acc 0.9375, prec 0.0414628, recall 0.823718
2017-12-10T14:38:55.280784: step 2014, loss 0.126287, acc 0.9375, prec 0.0414841, recall 0.823812
2017-12-10T14:38:55.469311: step 2015, loss 2.85742, acc 0.84375, prec 0.0414998, recall 0.823467
2017-12-10T14:38:55.660023: step 2016, loss 0.332268, acc 0.9375, prec 0.0415469, recall 0.823655
2017-12-10T14:38:55.847219: step 2017, loss 0.288985, acc 0.90625, prec 0.0415917, recall 0.823842
2017-12-10T14:38:56.032957: step 2018, loss 0.159399, acc 0.9375, prec 0.0416387, recall 0.82403
2017-12-10T14:38:56.217190: step 2019, loss 0.275778, acc 0.9375, prec 0.04166, recall 0.824123
2017-12-10T14:38:56.402726: step 2020, loss 0.785891, acc 0.984375, prec 0.041736, recall 0.824403
2017-12-10T14:38:56.589705: step 2021, loss 2.85673, acc 0.921875, prec 0.0417573, recall 0.824059
2017-12-10T14:38:56.781349: step 2022, loss 0.244921, acc 0.921875, prec 0.0418031, recall 0.824246
2017-12-10T14:38:56.966525: step 2023, loss 0.167971, acc 0.953125, prec 0.0418255, recall 0.824339
2017-12-10T14:38:57.153606: step 2024, loss 0.317243, acc 0.921875, prec 0.0418713, recall 0.824524
2017-12-10T14:38:57.340058: step 2025, loss 0.819174, acc 0.796875, prec 0.0418567, recall 0.824524
2017-12-10T14:38:57.529297: step 2026, loss 0.397993, acc 0.859375, prec 0.0418466, recall 0.824524
2017-12-10T14:38:57.716032: step 2027, loss 0.596496, acc 0.8125, prec 0.0418332, recall 0.824524
2017-12-10T14:38:57.899742: step 2028, loss 0.472179, acc 0.828125, prec 0.0418208, recall 0.824524
2017-12-10T14:38:58.087380: step 2029, loss 0.604733, acc 0.796875, prec 0.0418319, recall 0.824617
2017-12-10T14:38:58.271198: step 2030, loss 0.564219, acc 0.78125, prec 0.0418419, recall 0.82471
2017-12-10T14:38:58.455124: step 2031, loss 0.559152, acc 0.78125, prec 0.0418262, recall 0.82471
2017-12-10T14:38:58.638688: step 2032, loss 0.933785, acc 0.765625, prec 0.0418094, recall 0.82471
2017-12-10T14:38:58.823571: step 2033, loss 0.307171, acc 0.90625, prec 0.0418284, recall 0.824802
2017-12-10T14:38:59.015010: step 2034, loss 0.2459, acc 0.890625, prec 0.0418205, recall 0.824802
2017-12-10T14:38:59.206448: step 2035, loss 0.543126, acc 0.796875, prec 0.0418316, recall 0.824894
2017-12-10T14:38:59.392595: step 2036, loss 0.400973, acc 0.84375, prec 0.041846, recall 0.824987
2017-12-10T14:38:59.579738: step 2037, loss 0.29291, acc 0.890625, prec 0.0418382, recall 0.824987
2017-12-10T14:38:59.767458: step 2038, loss 0.301248, acc 0.90625, prec 0.0418315, recall 0.824987
2017-12-10T14:38:59.953501: step 2039, loss 0.956014, acc 0.953125, prec 0.0418538, recall 0.825079
2017-12-10T14:39:00.142816: step 2040, loss 0.484098, acc 0.875, prec 0.0418704, recall 0.825171
2017-12-10T14:39:00.327845: step 2041, loss 0.174176, acc 0.921875, prec 0.0418648, recall 0.825171
2017-12-10T14:39:00.513884: step 2042, loss 0.231427, acc 0.9375, prec 0.0418603, recall 0.825171
2017-12-10T14:39:00.701065: step 2043, loss 0.161702, acc 0.90625, prec 0.0418536, recall 0.825171
2017-12-10T14:39:00.888268: step 2044, loss 0.146116, acc 0.890625, prec 0.0418714, recall 0.825263
2017-12-10T14:39:01.078707: step 2045, loss 0.356556, acc 0.859375, prec 0.0418613, recall 0.825263
2017-12-10T14:39:01.270079: step 2046, loss 0.289808, acc 0.984375, prec 0.0418858, recall 0.825355
2017-12-10T14:39:01.459917: step 2047, loss 0.195716, acc 0.9375, prec 0.0418813, recall 0.825355
2017-12-10T14:39:01.646238: step 2048, loss 0.262157, acc 0.96875, prec 0.0418791, recall 0.825355
2017-12-10T14:39:01.834124: step 2049, loss 0.157205, acc 0.953125, prec 0.0419013, recall 0.825447
2017-12-10T14:39:02.021535: step 2050, loss 0.0470514, acc 0.984375, prec 0.0419258, recall 0.825539
2017-12-10T14:39:02.208741: step 2051, loss 0.195029, acc 0.9375, prec 0.0419213, recall 0.825539
2017-12-10T14:39:02.402285: step 2052, loss 0.247918, acc 0.921875, prec 0.0419157, recall 0.825539
2017-12-10T14:39:02.589705: step 2053, loss 0.101833, acc 0.984375, prec 0.0419401, recall 0.82563
2017-12-10T14:39:02.774537: step 2054, loss 0.215982, acc 0.921875, prec 0.0419601, recall 0.825722
2017-12-10T14:39:02.961785: step 2055, loss 0.263959, acc 0.96875, prec 0.042009, recall 0.825905
2017-12-10T14:39:03.149654: step 2056, loss 1.12805, acc 0.953125, prec 0.0421078, recall 0.826269
2017-12-10T14:39:03.340666: step 2057, loss 6.68393, acc 0.9375, prec 0.0421044, recall 0.825837
2017-12-10T14:39:03.531426: step 2058, loss 8.11798, acc 0.875, prec 0.0421221, recall 0.825496
2017-12-10T14:39:03.718399: step 2059, loss 0.422789, acc 0.90625, prec 0.0421664, recall 0.825678
2017-12-10T14:39:03.905737: step 2060, loss 0.151564, acc 0.9375, prec 0.0421619, recall 0.825678
2017-12-10T14:39:04.086639: step 2061, loss 0.568277, acc 0.78125, prec 0.0421462, recall 0.825678
2017-12-10T14:39:04.271272: step 2062, loss 1.26896, acc 0.859375, prec 0.0421871, recall 0.82586
2017-12-10T14:39:04.457555: step 2063, loss 0.354699, acc 0.890625, prec 0.0422048, recall 0.825951
2017-12-10T14:39:04.643711: step 2064, loss 0.883251, acc 0.6875, prec 0.0422078, recall 0.826042
2017-12-10T14:39:04.827144: step 2065, loss 1.19896, acc 0.796875, prec 0.0422187, recall 0.826132
2017-12-10T14:39:05.013794: step 2066, loss 1.00292, acc 0.71875, prec 0.0422749, recall 0.826403
2017-12-10T14:39:05.197551: step 2067, loss 1.18933, acc 0.625, prec 0.0422479, recall 0.826403
2017-12-10T14:39:05.382238: step 2068, loss 1.19318, acc 0.671875, prec 0.0422243, recall 0.826403
2017-12-10T14:39:05.569261: step 2069, loss 1.31735, acc 0.640625, prec 0.0422494, recall 0.826584
2017-12-10T14:39:05.752509: step 2070, loss 1.4432, acc 0.625, prec 0.0422225, recall 0.826584
2017-12-10T14:39:05.943158: step 2071, loss 1.0917, acc 0.609375, prec 0.0421945, recall 0.826584
2017-12-10T14:39:06.130731: step 2072, loss 1.1561, acc 0.6875, prec 0.0421976, recall 0.826674
2017-12-10T14:39:06.315723: step 2073, loss 1.34645, acc 0.640625, prec 0.0421972, recall 0.826764
2017-12-10T14:39:06.502952: step 2074, loss 1.21556, acc 0.640625, prec 0.0421715, recall 0.826764
2017-12-10T14:39:06.690973: step 2075, loss 0.787745, acc 0.78125, prec 0.0421559, recall 0.826764
2017-12-10T14:39:06.875750: step 2076, loss 1.28392, acc 0.625, prec 0.0421798, recall 0.826943
2017-12-10T14:39:07.057512: step 2077, loss 0.641871, acc 0.765625, prec 0.0421884, recall 0.827033
2017-12-10T14:39:07.242063: step 2078, loss 0.464429, acc 0.84375, prec 0.0422026, recall 0.827122
2017-12-10T14:39:07.428648: step 2079, loss 0.636058, acc 0.796875, prec 0.0422134, recall 0.827212
2017-12-10T14:39:07.612745: step 2080, loss 0.470746, acc 0.828125, prec 0.0422264, recall 0.827301
2017-12-10T14:39:07.798882: step 2081, loss 0.791796, acc 0.8125, prec 0.0422383, recall 0.82739
2017-12-10T14:39:07.984623: step 2082, loss 0.243179, acc 0.921875, prec 0.0422832, recall 0.827568
2017-12-10T14:39:08.171252: step 2083, loss 0.158917, acc 0.90625, prec 0.0422766, recall 0.827568
2017-12-10T14:39:08.360030: step 2084, loss 1.81159, acc 0.890625, prec 0.0423456, recall 0.827409
2017-12-10T14:39:08.547127: step 2085, loss 0.151786, acc 0.9375, prec 0.0423412, recall 0.827409
2017-12-10T14:39:08.731851: step 2086, loss 0.28585, acc 0.921875, prec 0.0423861, recall 0.827586
2017-12-10T14:39:08.914349: step 2087, loss 0.166076, acc 0.953125, prec 0.0423827, recall 0.827586
2017-12-10T14:39:09.100639: step 2088, loss 0.304723, acc 0.921875, prec 0.0424276, recall 0.827763
2017-12-10T14:39:09.285984: step 2089, loss 0.292943, acc 0.921875, prec 0.0424472, recall 0.827852
2017-12-10T14:39:09.470764: step 2090, loss 0.502907, acc 0.984375, prec 0.0424966, recall 0.828029
2017-12-10T14:39:09.659242: step 2091, loss 0.112472, acc 0.953125, prec 0.0425184, recall 0.828117
2017-12-10T14:39:09.844614: step 2092, loss 0.221082, acc 0.890625, prec 0.0425106, recall 0.828117
2017-12-10T14:39:10.032334: step 2093, loss 0.274372, acc 0.921875, prec 0.0425554, recall 0.828293
2017-12-10T14:39:10.217747: step 2094, loss 0.247617, acc 0.984375, prec 0.0426047, recall 0.828469
2017-12-10T14:39:10.404906: step 2095, loss 0.256583, acc 0.953125, prec 0.042677, recall 0.828732
2017-12-10T14:39:10.592252: step 2096, loss 0.155753, acc 0.921875, prec 0.0426714, recall 0.828732
2017-12-10T14:39:10.780858: step 2097, loss 0.740658, acc 0.890625, prec 0.0426887, recall 0.82882
2017-12-10T14:39:10.968416: step 2098, loss 0.0589103, acc 0.96875, prec 0.0426865, recall 0.82882
2017-12-10T14:39:11.152879: step 2099, loss 0.20597, acc 0.953125, prec 0.0427083, recall 0.828907
2017-12-10T14:39:11.340584: step 2100, loss 1.87139, acc 0.953125, prec 0.042706, recall 0.828484
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_3/1512934336/checkpoints/model-2100

2017-12-10T14:39:12.505604: step 2101, loss 3.11776, acc 0.890625, prec 0.0427748, recall 0.828324
2017-12-10T14:39:12.691325: step 2102, loss 0.118907, acc 0.96875, prec 0.0427726, recall 0.828324
2017-12-10T14:39:12.873557: step 2103, loss 0.214414, acc 0.90625, prec 0.0427658, recall 0.828324
2017-12-10T14:39:13.060420: step 2104, loss 0.445414, acc 0.859375, prec 0.0427557, recall 0.828324
2017-12-10T14:39:13.248073: step 2105, loss 0.354759, acc 0.84375, prec 0.0427445, recall 0.828324
2017-12-10T14:39:13.432606: step 2106, loss 0.361856, acc 0.9375, prec 0.04274, recall 0.828324
2017-12-10T14:39:13.620867: step 2107, loss 0.507004, acc 0.859375, prec 0.0427299, recall 0.828324
2017-12-10T14:39:13.807635: step 2108, loss 0.805117, acc 0.78125, prec 0.0427142, recall 0.828324
2017-12-10T14:39:13.995462: step 2109, loss 0.456358, acc 0.84375, prec 0.0427029, recall 0.828324
2017-12-10T14:39:14.186587: step 2110, loss 0.688274, acc 0.859375, prec 0.0426929, recall 0.828324
2017-12-10T14:39:14.373919: step 2111, loss 0.416434, acc 0.828125, prec 0.0427057, recall 0.828411
2017-12-10T14:39:14.559300: step 2112, loss 0.762073, acc 0.6875, prec 0.0426832, recall 0.828411
2017-12-10T14:39:14.741226: step 2113, loss 0.466909, acc 0.796875, prec 0.0426938, recall 0.828499
2017-12-10T14:39:14.927211: step 2114, loss 0.377752, acc 0.84375, prec 0.0426826, recall 0.828499
2017-12-10T14:39:15.117483: step 2115, loss 0.58081, acc 0.828125, prec 0.0426954, recall 0.828586
2017-12-10T14:39:15.304059: step 2116, loss 0.471061, acc 0.796875, prec 0.0427059, recall 0.828673
2017-12-10T14:39:15.491032: step 2117, loss 0.4441, acc 0.890625, prec 0.0426981, recall 0.828673
2017-12-10T14:39:15.676456: step 2118, loss 1.38042, acc 0.84375, prec 0.0427131, recall 0.828339
2017-12-10T14:39:15.867636: step 2119, loss 0.405479, acc 0.859375, prec 0.042703, recall 0.828339
2017-12-10T14:39:16.058426: step 2120, loss 0.313737, acc 0.859375, prec 0.0427682, recall 0.8286
2017-12-10T14:39:16.250871: step 2121, loss 0.427702, acc 0.890625, prec 0.0427603, recall 0.8286
2017-12-10T14:39:16.437750: step 2122, loss 0.631468, acc 0.8125, prec 0.0427469, recall 0.8286
2017-12-10T14:39:16.627623: step 2123, loss 0.603806, acc 0.8125, prec 0.0427585, recall 0.828687
2017-12-10T14:39:16.816482: step 2124, loss 0.415201, acc 0.90625, prec 0.0427768, recall 0.828774
2017-12-10T14:39:17.005480: step 2125, loss 3.52687, acc 0.875, prec 0.042794, recall 0.828441
2017-12-10T14:39:17.197917: step 2126, loss 0.661628, acc 0.8125, prec 0.0428056, recall 0.828528
2017-12-10T14:39:17.383469: step 2127, loss 0.253875, acc 0.90625, prec 0.0427989, recall 0.828528
2017-12-10T14:39:17.575793: step 2128, loss 0.617905, acc 0.75, prec 0.042806, recall 0.828615
2017-12-10T14:39:17.766031: step 2129, loss 0.428254, acc 0.84375, prec 0.0427948, recall 0.828615
2017-12-10T14:39:17.950751: step 2130, loss 0.559611, acc 0.84375, prec 0.0427837, recall 0.828615
2017-12-10T14:39:18.141379: step 2131, loss 0.233602, acc 0.921875, prec 0.0427781, recall 0.828615
2017-12-10T14:39:18.327772: step 2132, loss 0.666169, acc 0.875, prec 0.0428191, recall 0.828788
2017-12-10T14:39:18.514847: step 2133, loss 0.27642, acc 0.921875, prec 0.0428135, recall 0.828788
2017-12-10T14:39:18.699431: step 2134, loss 0.554443, acc 0.84375, prec 0.0428273, recall 0.828874
2017-12-10T14:39:18.889045: step 2135, loss 1.09781, acc 0.84375, prec 0.0428661, recall 0.829047
2017-12-10T14:39:19.077506: step 2136, loss 0.337021, acc 0.890625, prec 0.0428832, recall 0.829133
2017-12-10T14:39:19.266904: step 2137, loss 0.282302, acc 0.921875, prec 0.0428776, recall 0.829133
2017-12-10T14:39:19.452920: step 2138, loss 0.36985, acc 0.921875, prec 0.0429219, recall 0.829305
2017-12-10T14:39:19.641381: step 2139, loss 0.135795, acc 0.9375, prec 0.0429174, recall 0.829305
2017-12-10T14:39:19.832087: step 2140, loss 0.289225, acc 0.921875, prec 0.0429119, recall 0.829305
2017-12-10T14:39:20.019618: step 2141, loss 0.268223, acc 0.890625, prec 0.042929, recall 0.829391
2017-12-10T14:39:20.205944: step 2142, loss 0.44612, acc 0.9375, prec 0.0429494, recall 0.829477
2017-12-10T14:39:20.392182: step 2143, loss 0.308112, acc 0.890625, prec 0.0429416, recall 0.829477
2017-12-10T14:39:20.577789: step 2144, loss 0.171878, acc 0.921875, prec 0.0429609, recall 0.829563
2017-12-10T14:39:20.765555: step 2145, loss 0.303934, acc 0.890625, prec 0.0429531, recall 0.829563
2017-12-10T14:39:20.953877: step 2146, loss 0.347973, acc 0.90625, prec 0.0429713, recall 0.829648
2017-12-10T14:39:21.138909: step 2147, loss 0.204135, acc 0.9375, prec 0.0429668, recall 0.829648
2017-12-10T14:39:21.329192: step 2148, loss 3.58538, acc 0.890625, prec 0.0430099, recall 0.829403
2017-12-10T14:39:21.518490: step 2149, loss 0.945534, acc 0.890625, prec 0.043027, recall 0.829488
2017-12-10T14:39:21.707917: step 2150, loss 2.01266, acc 0.84375, prec 0.0430169, recall 0.829073
2017-12-10T14:39:21.897648: step 2151, loss 0.34809, acc 0.90625, prec 0.0430351, recall 0.829158
2017-12-10T14:39:22.085367: step 2152, loss 0.672206, acc 0.90625, prec 0.0430532, recall 0.829244
2017-12-10T14:39:22.272590: step 2153, loss 0.618232, acc 0.78125, prec 0.0430624, recall 0.829329
2017-12-10T14:39:22.457340: step 2154, loss 0.501736, acc 0.828125, prec 0.0430501, recall 0.829329
2017-12-10T14:39:22.645978: step 2155, loss 0.770359, acc 0.75, prec 0.0430323, recall 0.829329
2017-12-10T14:39:22.830770: step 2156, loss 0.897263, acc 0.78125, prec 0.0430415, recall 0.829415
2017-12-10T14:39:23.014006: step 2157, loss 0.631937, acc 0.765625, prec 0.0430247, recall 0.829415
2017-12-10T14:39:23.200787: step 2158, loss 5.67907, acc 0.71875, prec 0.0430554, recall 0.829171
2017-12-10T14:39:23.388221: step 2159, loss 0.851443, acc 0.75, prec 0.0430623, recall 0.829256
2017-12-10T14:39:23.574125: step 2160, loss 0.601658, acc 0.8125, prec 0.0430737, recall 0.829341
2017-12-10T14:39:23.756958: step 2161, loss 1.22563, acc 0.609375, prec 0.0431202, recall 0.829596
2017-12-10T14:39:23.939508: step 2162, loss 0.993702, acc 0.703125, prec 0.0431238, recall 0.829681
2017-12-10T14:39:24.124228: step 2163, loss 0.666023, acc 0.75, prec 0.0431307, recall 0.829766
2017-12-10T14:39:24.309166: step 2164, loss 1.05434, acc 0.6875, prec 0.0431084, recall 0.829766
2017-12-10T14:39:24.498903: step 2165, loss 0.81964, acc 0.78125, prec 0.0430928, recall 0.829766
2017-12-10T14:39:24.689420: step 2166, loss 0.825046, acc 0.703125, prec 0.043121, recall 0.829935
2017-12-10T14:39:24.876038: step 2167, loss 0.687366, acc 0.8125, prec 0.0431571, recall 0.830104
2017-12-10T14:39:25.060314: step 2168, loss 0.694904, acc 0.828125, prec 0.043219, recall 0.830357
2017-12-10T14:39:25.244175: step 2169, loss 0.899931, acc 0.796875, prec 0.0432292, recall 0.830441
2017-12-10T14:39:25.431701: step 2170, loss 0.542343, acc 0.828125, prec 0.0432416, recall 0.830525
2017-12-10T14:39:25.619523: step 2171, loss 0.469657, acc 0.828125, prec 0.0432293, recall 0.830525
2017-12-10T14:39:25.807638: step 2172, loss 0.251372, acc 0.9375, prec 0.0432495, recall 0.830609
2017-12-10T14:39:25.996863: step 2173, loss 0.343447, acc 0.84375, prec 0.0432384, recall 0.830609
2017-12-10T14:39:26.182978: step 2174, loss 0.423908, acc 0.859375, prec 0.0432777, recall 0.830777
2017-12-10T14:39:26.367487: step 2175, loss 0.387955, acc 0.8125, prec 0.0432643, recall 0.830777
2017-12-10T14:39:26.551563: step 2176, loss 0.264353, acc 0.875, prec 0.04328, recall 0.830861
2017-12-10T14:39:26.737014: step 2177, loss 0.215257, acc 0.921875, prec 0.0432744, recall 0.830861
2017-12-10T14:39:26.924146: step 2178, loss 2.37266, acc 0.921875, prec 0.04327, recall 0.83045
2017-12-10T14:39:27.113696: step 2179, loss 0.207556, acc 0.953125, prec 0.0432913, recall 0.830534
2017-12-10T14:39:27.301864: step 2180, loss 0.162558, acc 0.953125, prec 0.0432879, recall 0.830534
2017-12-10T14:39:27.489200: step 2181, loss 0.476831, acc 0.875, prec 0.0433036, recall 0.830617
2017-12-10T14:39:27.679565: step 2182, loss 0.232164, acc 0.90625, prec 0.043297, recall 0.830617
2017-12-10T14:39:27.864102: step 2183, loss 0.178595, acc 0.96875, prec 0.043344, recall 0.830784
2017-12-10T14:39:28.050667: step 2184, loss 0.205047, acc 0.96875, prec 0.0433417, recall 0.830784
2017-12-10T14:39:28.237294: step 2185, loss 0.573638, acc 0.953125, prec 0.043363, recall 0.830868
2017-12-10T14:39:28.426323: step 2186, loss 0.0905905, acc 0.96875, prec 0.0433608, recall 0.830868
2017-12-10T14:39:28.611638: step 2187, loss 0.265689, acc 0.859375, prec 0.0433507, recall 0.830868
2017-12-10T14:39:28.800488: step 2188, loss 0.390635, acc 0.921875, prec 0.0433698, recall 0.830951
2017-12-10T14:39:28.993337: step 2189, loss 4.0792, acc 0.90625, prec 0.0433899, recall 0.830217
2017-12-10T14:39:29.187704: step 2190, loss 0.299195, acc 0.9375, prec 0.0433855, recall 0.830217
2017-12-10T14:39:29.372573: step 2191, loss 0.516719, acc 0.859375, prec 0.0434246, recall 0.830383
2017-12-10T14:39:29.560526: step 2192, loss 0.563785, acc 0.859375, prec 0.0434146, recall 0.830383
2017-12-10T14:39:29.746616: step 2193, loss 0.597454, acc 0.875, prec 0.0434548, recall 0.83055
2017-12-10T14:39:29.933506: step 2194, loss 0.541983, acc 0.859375, prec 0.0434693, recall 0.830633
2017-12-10T14:39:30.118751: step 2195, loss 0.365764, acc 0.890625, prec 0.0435352, recall 0.830882
2017-12-10T14:39:30.304607: step 2196, loss 1.56155, acc 0.75, prec 0.043591, recall 0.831131
2017-12-10T14:39:30.497801: step 2197, loss 0.401155, acc 0.859375, prec 0.0435809, recall 0.831131
2017-12-10T14:39:30.687666: step 2198, loss 0.911914, acc 0.765625, prec 0.0435642, recall 0.831131
2017-12-10T14:39:30.873740: step 2199, loss 0.75912, acc 0.71875, prec 0.0435931, recall 0.831296
2017-12-10T14:39:31.058200: step 2200, loss 0.792334, acc 0.828125, prec 0.0435808, recall 0.831296
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_3/1512934336/checkpoints/model-2200

2017-12-10T14:39:32.112330: step 2201, loss 0.576869, acc 0.796875, prec 0.0436153, recall 0.831461
2017-12-10T14:39:32.296003: step 2202, loss 0.910922, acc 0.765625, prec 0.043623, recall 0.831543
2017-12-10T14:39:32.484336: step 2203, loss 0.852915, acc 0.765625, prec 0.0436063, recall 0.831543
2017-12-10T14:39:32.671651: step 2204, loss 0.419184, acc 0.84375, prec 0.0436196, recall 0.831625
2017-12-10T14:39:32.858108: step 2205, loss 0.495961, acc 0.859375, prec 0.0436096, recall 0.831625
2017-12-10T14:39:33.044236: step 2206, loss 0.529403, acc 0.828125, prec 0.0435973, recall 0.831625
2017-12-10T14:39:33.229240: step 2207, loss 0.308076, acc 0.84375, prec 0.0435861, recall 0.831625
2017-12-10T14:39:33.415612: step 2208, loss 0.579006, acc 0.890625, prec 0.0436273, recall 0.831789
2017-12-10T14:39:33.601911: step 2209, loss 0.312234, acc 0.890625, prec 0.0436439, recall 0.831871
2017-12-10T14:39:33.792294: step 2210, loss 3.01739, acc 0.890625, prec 0.0436616, recall 0.831548
2017-12-10T14:39:33.980932: step 2211, loss 0.559639, acc 0.84375, prec 0.0437238, recall 0.831794
2017-12-10T14:39:34.167723: step 2212, loss 0.450795, acc 0.890625, prec 0.043716, recall 0.831794
2017-12-10T14:39:34.354847: step 2213, loss 0.551479, acc 0.8125, prec 0.0437759, recall 0.832039
2017-12-10T14:39:34.540669: step 2214, loss 0.467619, acc 0.84375, prec 0.0437647, recall 0.832039
2017-12-10T14:39:34.727952: step 2215, loss 1.98306, acc 0.90625, prec 0.0438079, recall 0.831798
2017-12-10T14:39:34.917304: step 2216, loss 0.359282, acc 0.90625, prec 0.0438012, recall 0.831798
2017-12-10T14:39:35.106025: step 2217, loss 0.424354, acc 0.859375, prec 0.0437911, recall 0.831798
2017-12-10T14:39:35.295129: step 2218, loss 0.246775, acc 0.875, prec 0.0438066, recall 0.83188
2017-12-10T14:39:35.478947: step 2219, loss 0.271698, acc 0.875, prec 0.0438221, recall 0.831961
2017-12-10T14:39:35.662226: step 2220, loss 0.597039, acc 0.8125, prec 0.0438087, recall 0.831961
2017-12-10T14:39:35.847310: step 2221, loss 0.530668, acc 0.828125, prec 0.0438207, recall 0.832043
2017-12-10T14:39:36.033347: step 2222, loss 1.25261, acc 0.953125, prec 0.0438673, recall 0.831803
2017-12-10T14:39:36.219891: step 2223, loss 0.229003, acc 0.921875, prec 0.0438617, recall 0.831803
2017-12-10T14:39:36.406154: step 2224, loss 0.517495, acc 0.84375, prec 0.0438748, recall 0.831884
2017-12-10T14:39:36.597004: step 2225, loss 0.610298, acc 0.84375, prec 0.0438637, recall 0.831884
2017-12-10T14:39:36.782552: step 2226, loss 0.400685, acc 0.890625, prec 0.0438558, recall 0.831884
2017-12-10T14:39:36.967398: step 2227, loss 0.475394, acc 0.859375, prec 0.0438458, recall 0.831884
2017-12-10T14:39:37.152488: step 2228, loss 0.408709, acc 0.890625, prec 0.0438623, recall 0.831965
2017-12-10T14:39:37.339206: step 2229, loss 0.243443, acc 0.84375, prec 0.0438755, recall 0.832046
2017-12-10T14:39:37.528199: step 2230, loss 0.358853, acc 0.90625, prec 0.0438931, recall 0.832127
2017-12-10T14:39:37.719421: step 2231, loss 2.34321, acc 0.859375, prec 0.0438842, recall 0.831726
2017-12-10T14:39:37.908850: step 2232, loss 0.547473, acc 0.859375, prec 0.0438742, recall 0.831726
2017-12-10T14:39:38.095748: step 2233, loss 0.417419, acc 0.90625, prec 0.0438675, recall 0.831726
2017-12-10T14:39:38.284664: step 2234, loss 0.312095, acc 0.9375, prec 0.0438873, recall 0.831807
2017-12-10T14:39:38.472130: step 2235, loss 0.341689, acc 0.9375, prec 0.0439072, recall 0.831888
2017-12-10T14:39:38.656921: step 2236, loss 0.334788, acc 0.890625, prec 0.0438993, recall 0.831888
2017-12-10T14:39:38.843373: step 2237, loss 0.237745, acc 0.890625, prec 0.0438915, recall 0.831888
2017-12-10T14:39:39.029020: step 2238, loss 0.513267, acc 0.828125, prec 0.0439278, recall 0.83205
2017-12-10T14:39:39.214510: step 2239, loss 0.145691, acc 0.921875, prec 0.0439223, recall 0.83205
2017-12-10T14:39:39.403651: step 2240, loss 4.94437, acc 0.9375, prec 0.0439675, recall 0.831812
2017-12-10T14:39:39.594631: step 2241, loss 0.24862, acc 0.921875, prec 0.0439862, recall 0.831892
2017-12-10T14:39:39.779833: step 2242, loss 14.9188, acc 0.890625, prec 0.0440038, recall 0.831574
2017-12-10T14:39:39.967956: step 2243, loss 0.34194, acc 0.875, prec 0.0439948, recall 0.831574
2017-12-10T14:39:40.154643: step 2244, loss 0.74802, acc 0.78125, prec 0.0440277, recall 0.831735
2017-12-10T14:39:40.341746: step 2245, loss 0.76603, acc 0.8125, prec 0.0440386, recall 0.831816
2017-12-10T14:39:40.534150: step 2246, loss 0.62312, acc 0.796875, prec 0.044024, recall 0.831816
2017-12-10T14:39:40.722493: step 2247, loss 1.27731, acc 0.625, prec 0.0440457, recall 0.831977
2017-12-10T14:39:40.905730: step 2248, loss 1.301, acc 0.671875, prec 0.0440465, recall 0.832057
2017-12-10T14:39:41.095759: step 2249, loss 1.56598, acc 0.71875, prec 0.0440506, recall 0.832138
2017-12-10T14:39:41.282028: step 2250, loss 1.10753, acc 0.65625, prec 0.0440503, recall 0.832218
2017-12-10T14:39:41.476502: step 2251, loss 1.10142, acc 0.75, prec 0.0440325, recall 0.832218
2017-12-10T14:39:41.664278: step 2252, loss 0.774905, acc 0.78125, prec 0.0440411, recall 0.832298
2017-12-10T14:39:41.846940: step 2253, loss 1.33938, acc 0.640625, prec 0.0440155, recall 0.832298
2017-12-10T14:39:42.033301: step 2254, loss 0.729812, acc 0.734375, prec 0.0440207, recall 0.832378
2017-12-10T14:39:42.220850: step 2255, loss 1.47243, acc 0.75, prec 0.0440753, recall 0.832618
2017-12-10T14:39:42.404791: step 2256, loss 0.712466, acc 0.8125, prec 0.0440861, recall 0.832698
2017-12-10T14:39:42.588350: step 2257, loss 0.863361, acc 0.75, prec 0.0440683, recall 0.832698
2017-12-10T14:39:42.776511: step 2258, loss 0.751937, acc 0.796875, prec 0.0441021, recall 0.832857
2017-12-10T14:39:42.964205: step 2259, loss 0.769868, acc 0.78125, prec 0.0441106, recall 0.832937
2017-12-10T14:39:43.149523: step 2260, loss 0.526888, acc 0.8125, prec 0.0441454, recall 0.833096
2017-12-10T14:39:43.333355: step 2261, loss 0.62812, acc 0.78125, prec 0.0441299, recall 0.833096
2017-12-10T14:39:43.523603: step 2262, loss 0.459351, acc 0.859375, prec 0.0441439, recall 0.833175
2017-12-10T14:39:43.709222: step 2263, loss 0.485143, acc 0.8125, prec 0.0441547, recall 0.833254
2017-12-10T14:39:43.899214: step 2264, loss 0.298205, acc 0.90625, prec 0.044148, recall 0.833254
2017-12-10T14:39:44.094033: step 2265, loss 0.185946, acc 0.890625, prec 0.0441643, recall 0.833333
2017-12-10T14:39:44.280223: step 2266, loss 0.24116, acc 0.921875, prec 0.0441828, recall 0.833412
2017-12-10T14:39:44.465657: step 2267, loss 0.888808, acc 0.90625, prec 0.0442242, recall 0.83357
2017-12-10T14:39:44.656933: step 2268, loss 0.364895, acc 0.9375, prec 0.0442678, recall 0.833728
2017-12-10T14:39:44.844922: step 2269, loss 0.209679, acc 0.90625, prec 0.0442852, recall 0.833807
2017-12-10T14:39:45.028907: step 2270, loss 0.447837, acc 0.921875, prec 0.0443036, recall 0.833885
2017-12-10T14:39:45.221179: step 2271, loss 0.49936, acc 0.953125, prec 0.0443724, recall 0.834121
2017-12-10T14:39:45.409619: step 2272, loss 0.131331, acc 0.96875, prec 0.0443701, recall 0.834121
2017-12-10T14:39:45.592826: step 2273, loss 4.84386, acc 0.953125, prec 0.0443679, recall 0.833727
2017-12-10T14:39:45.787335: step 2274, loss 0.223882, acc 0.953125, prec 0.0444126, recall 0.833884
2017-12-10T14:39:45.975524: step 2275, loss 4.84948, acc 0.875, prec 0.0444048, recall 0.833491
2017-12-10T14:39:46.164548: step 2276, loss 0.705593, acc 0.8125, prec 0.0443914, recall 0.833491
2017-12-10T14:39:46.353439: step 2277, loss 0.257718, acc 0.890625, prec 0.0443836, recall 0.833491
2017-12-10T14:39:46.538547: step 2278, loss 0.559217, acc 0.859375, prec 0.0443976, recall 0.833569
2017-12-10T14:39:46.725787: step 2279, loss 0.117392, acc 0.953125, prec 0.0444182, recall 0.833647
2017-12-10T14:39:46.916496: step 2280, loss 0.322605, acc 0.890625, prec 0.0444104, recall 0.833647
2017-12-10T14:39:47.103399: step 2281, loss 0.484448, acc 0.921875, prec 0.0444528, recall 0.833804
2017-12-10T14:39:47.292158: step 2282, loss 0.374476, acc 0.90625, prec 0.0444701, recall 0.833882
2017-12-10T14:39:47.479742: step 2283, loss 0.20028, acc 0.921875, prec 0.0444645, recall 0.833882
2017-12-10T14:39:47.667184: step 2284, loss 0.313222, acc 0.90625, prec 0.0444578, recall 0.833882
2017-12-10T14:39:47.853516: step 2285, loss 0.631112, acc 0.859375, prec 0.0444957, recall 0.834039
2017-12-10T14:39:48.040675: step 2286, loss 0.802034, acc 0.828125, prec 0.0444834, recall 0.834039
2017-12-10T14:39:48.227641: step 2287, loss 0.479865, acc 0.859375, prec 0.0445213, recall 0.834194
2017-12-10T14:39:48.413032: step 2288, loss 2.93319, acc 0.859375, prec 0.0445363, recall 0.833881
2017-12-10T14:39:48.599612: step 2289, loss 0.272142, acc 0.921875, prec 0.0445308, recall 0.833881
2017-12-10T14:39:48.784088: step 2290, loss 0.273784, acc 0.90625, prec 0.0445241, recall 0.833881
2017-12-10T14:39:48.970756: step 2291, loss 0.409272, acc 0.84375, prec 0.0445368, recall 0.833959
2017-12-10T14:39:49.158647: step 2292, loss 0.26076, acc 0.890625, prec 0.044529, recall 0.833959
2017-12-10T14:39:49.346603: step 2293, loss 0.458243, acc 0.859375, prec 0.044519, recall 0.833959
2017-12-10T14:39:49.532596: step 2294, loss 0.623164, acc 0.8125, prec 0.0445295, recall 0.834037
2017-12-10T14:39:49.717786: step 2295, loss 0.822076, acc 0.765625, prec 0.0445606, recall 0.834192
2017-12-10T14:39:49.905741: step 2296, loss 0.420862, acc 0.8125, prec 0.0445473, recall 0.834192
2017-12-10T14:39:50.094063: step 2297, loss 0.499604, acc 0.875, prec 0.0445384, recall 0.834192
2017-12-10T14:39:50.282300: step 2298, loss 0.530553, acc 0.8125, prec 0.0445489, recall 0.83427
2017-12-10T14:39:50.471709: step 2299, loss 0.674473, acc 0.84375, prec 0.0445378, recall 0.83427
2017-12-10T14:39:50.663897: step 2300, loss 0.437405, acc 0.828125, prec 0.0445255, recall 0.83427
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_3/1512934336/checkpoints/model-2300

2017-12-10T14:39:51.903693: step 2301, loss 0.326668, acc 0.875, prec 0.0445405, recall 0.834347
2017-12-10T14:39:52.091652: step 2302, loss 0.14534, acc 0.90625, prec 0.0445338, recall 0.834347
2017-12-10T14:39:52.276279: step 2303, loss 0.437658, acc 0.859375, prec 0.0445238, recall 0.834347
2017-12-10T14:39:52.461112: step 2304, loss 0.276976, acc 0.9375, prec 0.0445193, recall 0.834347
2017-12-10T14:39:52.646819: step 2305, loss 0.328723, acc 0.859375, prec 0.0445332, recall 0.834425
2017-12-10T14:39:52.833225: step 2306, loss 0.099399, acc 0.96875, prec 0.0446025, recall 0.834657
2017-12-10T14:39:53.025067: step 2307, loss 2.54606, acc 0.921875, prec 0.0446219, recall 0.834344
2017-12-10T14:39:53.217657: step 2308, loss 0.256377, acc 0.921875, prec 0.0446163, recall 0.834344
2017-12-10T14:39:53.400887: step 2309, loss 0.188617, acc 0.953125, prec 0.0446607, recall 0.834499
2017-12-10T14:39:53.586323: step 2310, loss 0.165766, acc 0.984375, prec 0.0447072, recall 0.834653
2017-12-10T14:39:53.772457: step 2311, loss 0.140458, acc 0.953125, prec 0.0447277, recall 0.83473
2017-12-10T14:39:53.957941: step 2312, loss 0.375163, acc 0.875, prec 0.0447426, recall 0.834807
2017-12-10T14:39:54.147269: step 2313, loss 1.12009, acc 0.9375, prec 0.0448096, recall 0.835037
2017-12-10T14:39:54.335298: step 2314, loss 0.116817, acc 0.9375, prec 0.0448051, recall 0.835037
2017-12-10T14:39:54.523434: step 2315, loss 0.382711, acc 0.890625, prec 0.044845, recall 0.83519
2017-12-10T14:39:54.712058: step 2316, loss 0.0744594, acc 0.96875, prec 0.0448427, recall 0.83519
2017-12-10T14:39:54.899087: step 2317, loss 0.434547, acc 0.890625, prec 0.0448825, recall 0.835343
2017-12-10T14:39:55.086571: step 2318, loss 0.335909, acc 0.953125, prec 0.0449505, recall 0.835572
2017-12-10T14:39:55.272167: step 2319, loss 0.392245, acc 0.875, prec 0.0449654, recall 0.835648
2017-12-10T14:39:55.456727: step 2320, loss 0.0882889, acc 0.96875, prec 0.0449631, recall 0.835648
2017-12-10T14:39:55.641693: step 2321, loss 0.259651, acc 0.921875, prec 0.0449575, recall 0.835648
2017-12-10T14:39:55.827998: step 2322, loss 0.299448, acc 0.890625, prec 0.0449497, recall 0.835648
2017-12-10T14:39:56.016108: step 2323, loss 0.251906, acc 0.890625, prec 0.0449419, recall 0.835648
2017-12-10T14:39:56.201859: step 2324, loss 9.5807, acc 0.953125, prec 0.0449396, recall 0.835261
2017-12-10T14:39:56.392749: step 2325, loss 0.111602, acc 0.953125, prec 0.04496, recall 0.835338
2017-12-10T14:39:56.576833: step 2326, loss 0.373733, acc 0.875, prec 0.0449511, recall 0.835338
2017-12-10T14:39:56.767141: step 2327, loss 0.924824, acc 0.875, prec 0.0450372, recall 0.835642
2017-12-10T14:39:56.951785: step 2328, loss 12.3761, acc 0.875, prec 0.0450769, recall 0.835408
2017-12-10T14:39:57.142551: step 2329, loss 0.407252, acc 0.890625, prec 0.0450928, recall 0.835484
2017-12-10T14:39:57.334779: step 2330, loss 0.384545, acc 0.890625, prec 0.0450849, recall 0.835484
2017-12-10T14:39:57.517670: step 2331, loss 0.642091, acc 0.84375, prec 0.0451212, recall 0.835635
2017-12-10T14:39:57.703427: step 2332, loss 0.816828, acc 0.71875, prec 0.045101, recall 0.835635
2017-12-10T14:39:57.892332: step 2333, loss 0.803852, acc 0.796875, prec 0.0451102, recall 0.835711
2017-12-10T14:39:58.074975: step 2334, loss 0.880517, acc 0.71875, prec 0.0451374, recall 0.835862
2017-12-10T14:39:58.262770: step 2335, loss 0.728964, acc 0.84375, prec 0.0451499, recall 0.835938
2017-12-10T14:39:58.446592: step 2336, loss 0.790838, acc 0.75, prec 0.045132, recall 0.835938
2017-12-10T14:39:58.632142: step 2337, loss 0.710376, acc 0.765625, prec 0.0451389, recall 0.836013
2017-12-10T14:39:58.815786: step 2338, loss 0.711618, acc 0.765625, prec 0.0451458, recall 0.836088
2017-12-10T14:39:59.008257: step 2339, loss 0.985554, acc 0.671875, prec 0.0451223, recall 0.836088
2017-12-10T14:39:59.199151: step 2340, loss 0.649355, acc 0.78125, prec 0.0451539, recall 0.836239
2017-12-10T14:39:59.382708: step 2341, loss 1.18057, acc 0.78125, prec 0.0451383, recall 0.836239
2017-12-10T14:39:59.568019: step 2342, loss 0.466738, acc 0.84375, prec 0.0451507, recall 0.836314
2017-12-10T14:39:59.755394: step 2343, loss 0.356997, acc 0.875, prec 0.0451418, recall 0.836314
2017-12-10T14:39:59.938404: step 2344, loss 0.554998, acc 0.890625, prec 0.045134, recall 0.836314
2017-12-10T14:40:00.124394: step 2345, loss 0.635575, acc 0.8125, prec 0.0451678, recall 0.836464
2017-12-10T14:40:00.313615: step 2346, loss 0.835957, acc 0.78125, prec 0.0451522, recall 0.836464
2017-12-10T14:40:00.500648: step 2347, loss 0.406964, acc 0.859375, prec 0.0451658, recall 0.836538
2017-12-10T14:40:00.686384: step 2348, loss 0.279169, acc 0.890625, prec 0.0451579, recall 0.836538
2017-12-10T14:40:00.876229: step 2349, loss 0.392462, acc 0.84375, prec 0.0451468, recall 0.836538
2017-12-10T14:40:01.066526: step 2350, loss 0.485984, acc 0.90625, prec 0.0451873, recall 0.836688
2017-12-10T14:40:01.257927: step 2351, loss 1.96144, acc 0.890625, prec 0.0451806, recall 0.836305
2017-12-10T14:40:01.450411: step 2352, loss 0.28425, acc 0.9375, prec 0.0451997, recall 0.83638
2017-12-10T14:40:01.639097: step 2353, loss 6.46654, acc 0.96875, prec 0.0451986, recall 0.835998
2017-12-10T14:40:01.825870: step 2354, loss 2.12578, acc 0.859375, prec 0.0451896, recall 0.835616
2017-12-10T14:40:02.014459: step 2355, loss 0.455263, acc 0.84375, prec 0.0452021, recall 0.835691
2017-12-10T14:40:02.201360: step 2356, loss 0.408731, acc 0.84375, prec 0.0452145, recall 0.835766
2017-12-10T14:40:02.388311: step 2357, loss 0.157557, acc 0.921875, prec 0.0452089, recall 0.835766
2017-12-10T14:40:02.572549: step 2358, loss 0.641871, acc 0.828125, prec 0.0452202, recall 0.835841
2017-12-10T14:40:02.755950: step 2359, loss 1.89492, acc 0.828125, prec 0.045209, recall 0.83546
2017-12-10T14:40:02.941964: step 2360, loss 0.674376, acc 0.84375, prec 0.0452214, recall 0.835535
2017-12-10T14:40:03.129575: step 2361, loss 3.87541, acc 0.671875, prec 0.0451991, recall 0.835155
2017-12-10T14:40:03.319484: step 2362, loss 0.917747, acc 0.6875, prec 0.0452004, recall 0.83523
2017-12-10T14:40:03.503273: step 2363, loss 0.638272, acc 0.796875, prec 0.0452094, recall 0.835305
2017-12-10T14:40:03.690949: step 2364, loss 0.894379, acc 0.796875, prec 0.045195, recall 0.835305
2017-12-10T14:40:03.874912: step 2365, loss 1.26183, acc 0.640625, prec 0.0451929, recall 0.83538
2017-12-10T14:40:04.060247: step 2366, loss 1.74167, acc 0.59375, prec 0.045164, recall 0.83538
2017-12-10T14:40:04.244955: step 2367, loss 1.68631, acc 0.5625, prec 0.0451564, recall 0.835455
2017-12-10T14:40:04.431629: step 2368, loss 1.4023, acc 0.703125, prec 0.0451353, recall 0.835455
2017-12-10T14:40:04.613978: step 2369, loss 1.01639, acc 0.734375, prec 0.0451399, recall 0.835529
2017-12-10T14:40:04.799069: step 2370, loss 1.24693, acc 0.703125, prec 0.0451189, recall 0.835529
2017-12-10T14:40:04.983743: step 2371, loss 0.635769, acc 0.703125, prec 0.0450978, recall 0.835529
2017-12-10T14:40:05.166887: step 2372, loss 0.920618, acc 0.671875, prec 0.0451214, recall 0.835679
2017-12-10T14:40:05.351530: step 2373, loss 0.863402, acc 0.765625, prec 0.0451516, recall 0.835828
2017-12-10T14:40:05.536341: step 2374, loss 0.732229, acc 0.765625, prec 0.0451351, recall 0.835828
2017-12-10T14:40:05.721772: step 2375, loss 0.676535, acc 0.78125, prec 0.0451196, recall 0.835828
2017-12-10T14:40:05.908832: step 2376, loss 1.89884, acc 0.734375, prec 0.0451709, recall 0.836051
2017-12-10T14:40:06.103157: step 2377, loss 0.508214, acc 0.875, prec 0.0452789, recall 0.836421
2017-12-10T14:40:06.290234: step 2378, loss 0.793058, acc 0.828125, prec 0.0452667, recall 0.836421
2017-12-10T14:40:06.474354: step 2379, loss 0.951428, acc 0.8125, prec 0.0452767, recall 0.836495
2017-12-10T14:40:06.658286: step 2380, loss 0.13198, acc 0.96875, prec 0.0452979, recall 0.836569
2017-12-10T14:40:06.841123: step 2381, loss 0.490406, acc 0.875, prec 0.0453123, recall 0.836643
2017-12-10T14:40:07.032731: step 2382, loss 4.41001, acc 0.890625, prec 0.045329, recall 0.836339
2017-12-10T14:40:07.224411: step 2383, loss 0.241037, acc 0.890625, prec 0.0453213, recall 0.836339
2017-12-10T14:40:07.410524: step 2384, loss 2.47355, acc 0.859375, prec 0.0453357, recall 0.836036
2017-12-10T14:40:07.601529: step 2385, loss 0.10871, acc 0.9375, prec 0.0453546, recall 0.83611
2017-12-10T14:40:07.787039: step 2386, loss 0.515275, acc 0.859375, prec 0.045368, recall 0.836184
2017-12-10T14:40:07.975757: step 2387, loss 0.539952, acc 0.84375, prec 0.0453802, recall 0.836257
2017-12-10T14:40:08.160008: step 2388, loss 0.513383, acc 0.859375, prec 0.0453702, recall 0.836257
2017-12-10T14:40:08.345990: step 2389, loss 0.228391, acc 0.921875, prec 0.045388, recall 0.836331
2017-12-10T14:40:08.533620: step 2390, loss 0.494518, acc 0.796875, prec 0.0453736, recall 0.836331
2017-12-10T14:40:08.722351: step 2391, loss 0.871753, acc 0.71875, prec 0.0453537, recall 0.836331
2017-12-10T14:40:08.910431: step 2392, loss 0.513538, acc 0.8125, prec 0.0453637, recall 0.836405
2017-12-10T14:40:09.098358: step 2393, loss 3.68012, acc 0.875, prec 0.0453559, recall 0.836029
2017-12-10T14:40:09.291247: step 2394, loss 1.56219, acc 0.796875, prec 0.0453881, recall 0.836176
2017-12-10T14:40:09.479496: step 2395, loss 0.427093, acc 0.890625, prec 0.0453804, recall 0.836176
2017-12-10T14:40:09.663882: step 2396, loss 0.77939, acc 0.75, prec 0.0453627, recall 0.836176
2017-12-10T14:40:09.851881: step 2397, loss 0.366642, acc 0.8125, prec 0.0453727, recall 0.836249
2017-12-10T14:40:10.037606: step 2398, loss 0.956377, acc 0.6875, prec 0.0453738, recall 0.836323
2017-12-10T14:40:10.219919: step 2399, loss 0.86372, acc 0.765625, prec 0.0453573, recall 0.836323
2017-12-10T14:40:10.407388: step 2400, loss 0.875306, acc 0.734375, prec 0.0453385, recall 0.836323
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_3/1512934336/checkpoints/model-2400

2017-12-10T14:40:11.575138: step 2401, loss 1.27669, acc 0.671875, prec 0.0453386, recall 0.836396
2017-12-10T14:40:11.758530: step 2402, loss 0.726494, acc 0.8125, prec 0.0453486, recall 0.83647
2017-12-10T14:40:11.941554: step 2403, loss 0.794877, acc 0.765625, prec 0.045332, recall 0.83647
2017-12-10T14:40:12.129191: step 2404, loss 0.885694, acc 0.765625, prec 0.0453155, recall 0.83647
2017-12-10T14:40:12.314630: step 2405, loss 0.785046, acc 0.765625, prec 0.0453454, recall 0.836616
2017-12-10T14:40:12.500818: step 2406, loss 0.742955, acc 0.765625, prec 0.0453289, recall 0.836616
2017-12-10T14:40:12.686553: step 2407, loss 0.585924, acc 0.796875, prec 0.0453609, recall 0.836762
2017-12-10T14:40:12.871342: step 2408, loss 0.444061, acc 0.875, prec 0.0453752, recall 0.836835
2017-12-10T14:40:13.055191: step 2409, loss 1.27852, acc 0.875, prec 0.0453896, recall 0.836908
2017-12-10T14:40:13.245439: step 2410, loss 0.30926, acc 0.890625, prec 0.045405, recall 0.836981
2017-12-10T14:40:13.431633: step 2411, loss 0.609493, acc 0.84375, prec 0.0454402, recall 0.837126
2017-12-10T14:40:13.621632: step 2412, loss 0.145557, acc 0.953125, prec 0.04546, recall 0.837199
2017-12-10T14:40:13.806022: step 2413, loss 0.359499, acc 0.875, prec 0.0454512, recall 0.837199
2017-12-10T14:40:13.996788: step 2414, loss 0.364047, acc 0.875, prec 0.0454656, recall 0.837272
2017-12-10T14:40:14.191385: step 2415, loss 0.342877, acc 0.875, prec 0.0454567, recall 0.837272
2017-12-10T14:40:14.378752: step 2416, loss 0.305583, acc 0.90625, prec 0.0454732, recall 0.837344
2017-12-10T14:40:14.566795: step 2417, loss 0.290348, acc 0.953125, prec 0.045493, recall 0.837416
2017-12-10T14:40:14.755860: step 2418, loss 0.621859, acc 0.875, prec 0.0455073, recall 0.837489
2017-12-10T14:40:14.946492: step 2419, loss 0.825142, acc 0.953125, prec 0.0455271, recall 0.837561
2017-12-10T14:40:15.133526: step 2420, loss 1.66169, acc 0.90625, prec 0.0455216, recall 0.837189
2017-12-10T14:40:15.322275: step 2421, loss 0.167137, acc 0.921875, prec 0.0455392, recall 0.837261
2017-12-10T14:40:15.506806: step 2422, loss 0.402701, acc 0.859375, prec 0.0455524, recall 0.837333
2017-12-10T14:40:15.691497: step 2423, loss 0.419109, acc 0.875, prec 0.0455666, recall 0.837406
2017-12-10T14:40:15.879093: step 2424, loss 0.299541, acc 0.875, prec 0.0456039, recall 0.83755
2017-12-10T14:40:16.068556: step 2425, loss 0.189046, acc 0.9375, prec 0.0455995, recall 0.83755
2017-12-10T14:40:16.253773: step 2426, loss 1.58714, acc 0.84375, prec 0.0455896, recall 0.837178
2017-12-10T14:40:16.437795: step 2427, loss 0.845406, acc 0.921875, prec 0.0456763, recall 0.837467
2017-12-10T14:40:16.626576: step 2428, loss 0.326704, acc 0.890625, prec 0.0456686, recall 0.837467
2017-12-10T14:40:16.814266: step 2429, loss 0.285125, acc 0.921875, prec 0.0456861, recall 0.837539
2017-12-10T14:40:17.000851: step 2430, loss 2.40161, acc 0.859375, prec 0.0457004, recall 0.83724
2017-12-10T14:40:17.185613: step 2431, loss 0.781588, acc 0.78125, prec 0.0457079, recall 0.837312
2017-12-10T14:40:17.371596: step 2432, loss 0.345699, acc 0.875, prec 0.0457221, recall 0.837384
2017-12-10T14:40:17.559064: step 2433, loss 0.642823, acc 0.84375, prec 0.0457341, recall 0.837456
2017-12-10T14:40:17.743114: step 2434, loss 0.592182, acc 0.8125, prec 0.0457439, recall 0.837528
2017-12-10T14:40:17.930811: step 2435, loss 0.584641, acc 0.765625, prec 0.0457274, recall 0.837528
2017-12-10T14:40:18.121609: step 2436, loss 0.433939, acc 0.84375, prec 0.0457164, recall 0.837528
2017-12-10T14:40:18.307798: step 2437, loss 0.635883, acc 0.8125, prec 0.0457261, recall 0.837599
2017-12-10T14:40:18.493321: step 2438, loss 1.22259, acc 0.78125, prec 0.0457337, recall 0.837671
2017-12-10T14:40:18.681515: step 2439, loss 0.55148, acc 0.75, prec 0.045739, recall 0.837743
2017-12-10T14:40:18.867429: step 2440, loss 0.352601, acc 0.890625, prec 0.0457313, recall 0.837743
2017-12-10T14:40:19.053699: step 2441, loss 0.574246, acc 0.84375, prec 0.0458122, recall 0.838028
2017-12-10T14:40:19.241071: step 2442, loss 0.54416, acc 0.84375, prec 0.0458241, recall 0.838099
2017-12-10T14:40:19.424719: step 2443, loss 0.481388, acc 0.859375, prec 0.0458371, recall 0.838171
2017-12-10T14:40:19.614056: step 2444, loss 0.810575, acc 0.8125, prec 0.0458698, recall 0.838313
2017-12-10T14:40:19.802576: step 2445, loss 1.70022, acc 0.8125, prec 0.0458806, recall 0.838016
2017-12-10T14:40:19.988621: step 2446, loss 0.723083, acc 0.828125, prec 0.0458685, recall 0.838016
2017-12-10T14:40:20.174181: step 2447, loss 0.344253, acc 0.859375, prec 0.0458815, recall 0.838087
2017-12-10T14:40:20.363680: step 2448, loss 0.226489, acc 0.9375, prec 0.0459, recall 0.838158
2017-12-10T14:40:20.546786: step 2449, loss 0.737394, acc 0.828125, prec 0.0458879, recall 0.838158
2017-12-10T14:40:20.734841: step 2450, loss 0.520034, acc 0.828125, prec 0.0458757, recall 0.838158
2017-12-10T14:40:20.920251: step 2451, loss 0.403328, acc 0.875, prec 0.0459127, recall 0.8383
2017-12-10T14:40:21.102532: step 2452, loss 0.490075, acc 0.859375, prec 0.0459257, recall 0.838371
2017-12-10T14:40:21.288118: step 2453, loss 5.25221, acc 0.921875, prec 0.0459442, recall 0.838074
2017-12-10T14:40:21.476240: step 2454, loss 0.994353, acc 0.890625, prec 0.0459822, recall 0.838216
2017-12-10T14:40:21.666322: step 2455, loss 0.333399, acc 0.875, prec 0.0460192, recall 0.838357
2017-12-10T14:40:21.854324: step 2456, loss 0.367884, acc 0.890625, prec 0.0460343, recall 0.838428
2017-12-10T14:40:22.042223: step 2457, loss 0.259028, acc 0.90625, prec 0.0460506, recall 0.838498
2017-12-10T14:40:22.228917: step 2458, loss 0.725336, acc 0.84375, prec 0.0460853, recall 0.838639
2017-12-10T14:40:22.418011: step 2459, loss 0.583735, acc 0.8125, prec 0.046072, recall 0.838639
2017-12-10T14:40:22.606321: step 2460, loss 0.29956, acc 0.90625, prec 0.0460654, recall 0.838639
2017-12-10T14:40:22.795975: step 2461, loss 0.355635, acc 0.828125, prec 0.046099, recall 0.83878
2017-12-10T14:40:22.981098: step 2462, loss 0.950858, acc 0.765625, prec 0.0461052, recall 0.83885
2017-12-10T14:40:23.172835: step 2463, loss 0.741732, acc 0.78125, prec 0.0461126, recall 0.83892
2017-12-10T14:40:23.357956: step 2464, loss 0.2913, acc 0.890625, prec 0.0461505, recall 0.83906
2017-12-10T14:40:23.547456: step 2465, loss 0.285546, acc 0.859375, prec 0.0461406, recall 0.83906
2017-12-10T14:40:23.736067: step 2466, loss 0.366318, acc 0.875, prec 0.0461318, recall 0.83906
2017-12-10T14:40:23.924581: step 2467, loss 0.0968134, acc 0.96875, prec 0.0461296, recall 0.83906
2017-12-10T14:40:24.108712: step 2468, loss 0.486744, acc 0.875, prec 0.0461207, recall 0.83906
2017-12-10T14:40:24.297209: step 2469, loss 0.602798, acc 0.875, prec 0.0461119, recall 0.83906
2017-12-10T14:40:24.489245: step 2470, loss 1.24672, acc 0.921875, prec 0.046152, recall 0.8392
2017-12-10T14:40:24.683887: step 2471, loss 0.160633, acc 0.9375, prec 0.0461476, recall 0.8392
2017-12-10T14:40:24.868688: step 2472, loss 0.360187, acc 0.90625, prec 0.0461866, recall 0.83934
2017-12-10T14:40:25.057458: step 2473, loss 0.143855, acc 0.921875, prec 0.046181, recall 0.83934
2017-12-10T14:40:25.242656: step 2474, loss 0.478901, acc 0.890625, prec 0.0461961, recall 0.83941
2017-12-10T14:40:25.433384: step 2475, loss 0.164293, acc 0.953125, prec 0.0462384, recall 0.839549
2017-12-10T14:40:25.626512: step 2476, loss 0.104535, acc 0.9375, prec 0.0462567, recall 0.839619
2017-12-10T14:40:25.814724: step 2477, loss 1.15839, acc 0.875, prec 0.0462934, recall 0.839757
2017-12-10T14:40:26.003148: step 2478, loss 9.04847, acc 0.859375, prec 0.0462857, recall 0.839031
2017-12-10T14:40:26.196248: step 2479, loss 0.316874, acc 0.921875, prec 0.0463029, recall 0.8391
2017-12-10T14:40:26.385191: step 2480, loss 0.344457, acc 0.890625, prec 0.0462952, recall 0.8391
2017-12-10T14:40:26.569301: step 2481, loss 0.411855, acc 0.859375, prec 0.0462852, recall 0.8391
2017-12-10T14:40:26.753767: step 2482, loss 0.42711, acc 0.828125, prec 0.0462959, recall 0.83917
2017-12-10T14:40:26.939859: step 2483, loss 1.1218, acc 0.703125, prec 0.0462976, recall 0.839239
2017-12-10T14:40:27.125921: step 2484, loss 0.862504, acc 0.6875, prec 0.0462756, recall 0.839239
2017-12-10T14:40:27.295230: step 2485, loss 0.993652, acc 0.730769, prec 0.0462828, recall 0.839309
2017-12-10T14:40:27.484989: step 2486, loss 0.822328, acc 0.71875, prec 0.0462857, recall 0.839378
2017-12-10T14:40:27.673630: step 2487, loss 0.797426, acc 0.75, prec 0.0462908, recall 0.839448
2017-12-10T14:40:27.859648: step 2488, loss 0.704361, acc 0.765625, prec 0.0462743, recall 0.839448
2017-12-10T14:40:28.048379: step 2489, loss 1.41801, acc 0.59375, prec 0.0462683, recall 0.839517
2017-12-10T14:40:28.232973: step 2490, loss 0.85692, acc 0.671875, prec 0.0462452, recall 0.839517
2017-12-10T14:40:28.417928: step 2491, loss 0.595022, acc 0.75, prec 0.0462277, recall 0.839517
2017-12-10T14:40:28.603213: step 2492, loss 0.862084, acc 0.765625, prec 0.0462565, recall 0.839655
2017-12-10T14:40:28.787443: step 2493, loss 0.745789, acc 0.71875, prec 0.0462367, recall 0.839655
2017-12-10T14:40:28.979206: step 2494, loss 0.55824, acc 0.875, prec 0.0462506, recall 0.839724
2017-12-10T14:40:29.173956: step 2495, loss 0.524929, acc 0.84375, prec 0.0462849, recall 0.839862
2017-12-10T14:40:29.365389: step 2496, loss 0.291175, acc 0.890625, prec 0.0462772, recall 0.839862
2017-12-10T14:40:29.552312: step 2497, loss 0.229044, acc 0.9375, prec 0.0462728, recall 0.839862
2017-12-10T14:40:29.741715: step 2498, loss 0.71003, acc 0.84375, prec 0.0463071, recall 0.84
2017-12-10T14:40:29.928823: step 2499, loss 0.377628, acc 0.890625, prec 0.0463446, recall 0.840138
2017-12-10T14:40:30.116069: step 2500, loss 0.112976, acc 0.984375, prec 0.0463887, recall 0.840275
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_3/1512934336/checkpoints/model-2500

2017-12-10T14:40:31.336096: step 2501, loss 0.430064, acc 0.96875, prec 0.0464317, recall 0.840412
2017-12-10T14:40:31.531268: step 2502, loss 0.598684, acc 0.96875, prec 0.0464521, recall 0.84048
2017-12-10T14:40:31.718338: step 2503, loss 0.339649, acc 0.9375, prec 0.0464703, recall 0.840549
2017-12-10T14:40:31.904511: step 2504, loss 0.112378, acc 0.953125, prec 0.0465122, recall 0.840685
2017-12-10T14:40:32.091502: step 2505, loss 0.128761, acc 0.953125, prec 0.0465089, recall 0.840685
2017-12-10T14:40:32.278809: step 2506, loss 9.82015, acc 0.9375, prec 0.0465067, recall 0.839966
2017-12-10T14:40:32.470325: step 2507, loss 1.25268, acc 0.9375, prec 0.04657, recall 0.840171
2017-12-10T14:40:32.660102: step 2508, loss 0.145972, acc 0.921875, prec 0.0465871, recall 0.840239
2017-12-10T14:40:32.847873: step 2509, loss 0.328075, acc 0.890625, prec 0.0465794, recall 0.840239
2017-12-10T14:40:33.034907: step 2510, loss 0.317767, acc 0.875, prec 0.0465931, recall 0.840307
2017-12-10T14:40:33.222986: step 2511, loss 0.430517, acc 0.859375, prec 0.0465832, recall 0.840307
2017-12-10T14:40:33.408057: step 2512, loss 0.684542, acc 0.75, prec 0.0465656, recall 0.840307
2017-12-10T14:40:33.592914: step 2513, loss 0.793831, acc 0.796875, prec 0.0465738, recall 0.840376
2017-12-10T14:40:33.777566: step 2514, loss 0.530324, acc 0.8125, prec 0.0465606, recall 0.840376
2017-12-10T14:40:33.964618: step 2515, loss 0.329948, acc 0.875, prec 0.0465518, recall 0.840376
2017-12-10T14:40:34.149382: step 2516, loss 0.556437, acc 0.84375, prec 0.0466309, recall 0.840648
2017-12-10T14:40:34.337205: step 2517, loss 0.687878, acc 0.8125, prec 0.0466853, recall 0.840851
2017-12-10T14:40:34.523646: step 2518, loss 0.656085, acc 0.78125, prec 0.0466923, recall 0.840919
2017-12-10T14:40:34.705758: step 2519, loss 0.304343, acc 0.890625, prec 0.0467296, recall 0.841054
2017-12-10T14:40:34.889410: step 2520, loss 0.61536, acc 0.796875, prec 0.0467378, recall 0.841121
2017-12-10T14:40:35.076673: step 2521, loss 0.533345, acc 0.78125, prec 0.0467673, recall 0.841256
2017-12-10T14:40:35.264571: step 2522, loss 0.515723, acc 0.84375, prec 0.0467788, recall 0.841324
2017-12-10T14:40:35.453290: step 2523, loss 0.413561, acc 0.8125, prec 0.046788, recall 0.841391
2017-12-10T14:40:35.644046: step 2524, loss 0.176172, acc 0.9375, prec 0.0468735, recall 0.84166
2017-12-10T14:40:35.830501: step 2525, loss 0.252324, acc 0.890625, prec 0.0468883, recall 0.841727
2017-12-10T14:40:36.023566: step 2526, loss 0.443345, acc 0.859375, prec 0.0469008, recall 0.841794
2017-12-10T14:40:36.210040: step 2527, loss 0.285328, acc 0.9375, prec 0.0469188, recall 0.84186
2017-12-10T14:40:36.396088: step 2528, loss 0.355597, acc 0.828125, prec 0.0469291, recall 0.841927
2017-12-10T14:40:36.585669: step 2529, loss 0.270143, acc 0.921875, prec 0.046946, recall 0.841994
2017-12-10T14:40:36.774563: step 2530, loss 0.215717, acc 0.9375, prec 0.0469416, recall 0.841994
2017-12-10T14:40:36.967519: step 2531, loss 0.343568, acc 0.953125, prec 0.0469832, recall 0.842128
2017-12-10T14:40:37.156749: step 2532, loss 0.140029, acc 0.953125, prec 0.0470023, recall 0.842194
2017-12-10T14:40:37.348022: step 2533, loss 0.0821679, acc 1, prec 0.0470472, recall 0.842327
2017-12-10T14:40:37.537415: step 2534, loss 0.186656, acc 0.921875, prec 0.0470417, recall 0.842327
2017-12-10T14:40:37.724500: step 2535, loss 0.178394, acc 0.921875, prec 0.0470585, recall 0.842394
2017-12-10T14:40:37.910936: step 2536, loss 0.200629, acc 0.953125, prec 0.0471225, recall 0.842593
2017-12-10T14:40:38.093847: step 2537, loss 0.146935, acc 0.9375, prec 0.0471629, recall 0.842725
2017-12-10T14:40:38.284091: step 2538, loss 0.156223, acc 0.9375, prec 0.0471809, recall 0.842791
2017-12-10T14:40:38.474411: step 2539, loss 0.483694, acc 0.96875, prec 0.0472011, recall 0.842857
2017-12-10T14:40:38.666393: step 2540, loss 0.081687, acc 0.953125, prec 0.0471978, recall 0.842857
2017-12-10T14:40:38.857110: step 2541, loss 5.30815, acc 0.921875, prec 0.0471944, recall 0.842149
2017-12-10T14:40:39.046734: step 2542, loss 2.16992, acc 0.9375, prec 0.0471911, recall 0.841796
2017-12-10T14:40:39.233753: step 2543, loss 0.482778, acc 0.953125, prec 0.0472102, recall 0.841862
2017-12-10T14:40:39.422013: step 2544, loss 0.271405, acc 0.921875, prec 0.0472046, recall 0.841862
2017-12-10T14:40:39.612486: step 2545, loss 0.131126, acc 0.9375, prec 0.0472002, recall 0.841862
2017-12-10T14:40:39.800032: step 2546, loss 0.372887, acc 0.9375, prec 0.0472182, recall 0.841929
2017-12-10T14:40:39.986556: step 2547, loss 0.757369, acc 0.8125, prec 0.0472272, recall 0.841995
2017-12-10T14:40:40.173078: step 2548, loss 0.308145, acc 0.90625, prec 0.0472654, recall 0.842127
2017-12-10T14:40:40.360637: step 2549, loss 0.64783, acc 0.84375, prec 0.047299, recall 0.842259
2017-12-10T14:40:40.545670: step 2550, loss 0.60869, acc 0.875, prec 0.0473125, recall 0.842325
2017-12-10T14:40:40.733876: step 2551, loss 0.766495, acc 0.734375, prec 0.047316, recall 0.842391
2017-12-10T14:40:40.918433: step 2552, loss 0.788742, acc 0.765625, prec 0.0472994, recall 0.842391
2017-12-10T14:40:41.107302: step 2553, loss 0.690298, acc 0.796875, prec 0.0473296, recall 0.842523
2017-12-10T14:40:41.294804: step 2554, loss 1.0948, acc 0.65625, prec 0.0473499, recall 0.842654
2017-12-10T14:40:41.486678: step 2555, loss 1.34722, acc 0.640625, prec 0.0473467, recall 0.84272
2017-12-10T14:40:41.670372: step 2556, loss 0.501558, acc 0.78125, prec 0.0473758, recall 0.842851
2017-12-10T14:40:41.859398: step 2557, loss 0.660884, acc 0.828125, prec 0.0473636, recall 0.842851
2017-12-10T14:40:42.047140: step 2558, loss 0.792235, acc 0.890625, prec 0.0473782, recall 0.842917
2017-12-10T14:40:42.236371: step 2559, loss 0.641531, acc 0.75, prec 0.0474273, recall 0.843113
2017-12-10T14:40:42.418852: step 2560, loss 0.239853, acc 0.90625, prec 0.0474207, recall 0.843113
2017-12-10T14:40:42.602957: step 2561, loss 0.501733, acc 0.859375, prec 0.0474552, recall 0.843243
2017-12-10T14:40:42.788511: step 2562, loss 0.916078, acc 0.78125, prec 0.0475065, recall 0.843439
2017-12-10T14:40:42.977557: step 2563, loss 0.301257, acc 0.875, prec 0.0475645, recall 0.843633
2017-12-10T14:40:43.166788: step 2564, loss 0.467023, acc 0.84375, prec 0.0475534, recall 0.843633
2017-12-10T14:40:43.350312: step 2565, loss 0.681375, acc 0.8125, prec 0.0475623, recall 0.843698
2017-12-10T14:40:43.535837: step 2566, loss 0.452394, acc 0.859375, prec 0.0475745, recall 0.843763
2017-12-10T14:40:43.721726: step 2567, loss 0.497375, acc 0.890625, prec 0.0476113, recall 0.843892
2017-12-10T14:40:43.909401: step 2568, loss 0.287197, acc 0.90625, prec 0.0476268, recall 0.843957
2017-12-10T14:40:44.105177: step 2569, loss 0.319424, acc 0.953125, prec 0.0476457, recall 0.844021
2017-12-10T14:40:44.293144: step 2570, loss 0.096581, acc 0.96875, prec 0.0476435, recall 0.844021
2017-12-10T14:40:44.480457: step 2571, loss 0.446949, acc 0.9375, prec 0.0476835, recall 0.84415
2017-12-10T14:40:44.668176: step 2572, loss 0.0875307, acc 0.9375, prec 0.0476791, recall 0.84415
2017-12-10T14:40:44.854407: step 2573, loss 0.160817, acc 0.953125, prec 0.0476757, recall 0.84415
2017-12-10T14:40:45.040574: step 2574, loss 0.153217, acc 0.9375, prec 0.0476713, recall 0.84415
2017-12-10T14:40:45.228720: step 2575, loss 0.0617612, acc 0.984375, prec 0.0476924, recall 0.844215
2017-12-10T14:40:45.414951: step 2576, loss 0.0905241, acc 0.953125, prec 0.0476891, recall 0.844215
2017-12-10T14:40:45.605677: step 2577, loss 0.0732189, acc 0.96875, prec 0.0477091, recall 0.844279
2017-12-10T14:40:45.793309: step 2578, loss 0.125913, acc 0.96875, prec 0.0477291, recall 0.844344
2017-12-10T14:40:45.983500: step 2579, loss 0.854495, acc 0.96875, prec 0.0477491, recall 0.844408
2017-12-10T14:40:46.175507: step 2580, loss 0.0583806, acc 0.984375, prec 0.0478591, recall 0.844728
2017-12-10T14:40:46.360888: step 2581, loss 0.122253, acc 0.953125, prec 0.0478557, recall 0.844728
2017-12-10T14:40:46.546311: step 2582, loss 0.241277, acc 0.9375, prec 0.0478735, recall 0.844792
2017-12-10T14:40:46.731322: step 2583, loss 0.178631, acc 0.921875, prec 0.0478679, recall 0.844792
2017-12-10T14:40:46.917037: step 2584, loss 0.224919, acc 0.9375, prec 0.0478856, recall 0.844856
2017-12-10T14:40:47.105512: step 2585, loss 0.509202, acc 0.953125, prec 0.0479267, recall 0.844984
2017-12-10T14:40:47.295440: step 2586, loss 0.555154, acc 0.9375, prec 0.0479888, recall 0.845175
2017-12-10T14:40:47.487423: step 2587, loss 0.24282, acc 0.984375, prec 0.0480321, recall 0.845302
2017-12-10T14:40:47.681385: step 2588, loss 0.537551, acc 0.984375, prec 0.0480975, recall 0.845492
2017-12-10T14:40:47.867736: step 2589, loss 0.148219, acc 0.921875, prec 0.0480919, recall 0.845492
2017-12-10T14:40:48.058204: step 2590, loss 0.217726, acc 0.953125, prec 0.0481108, recall 0.845555
2017-12-10T14:40:48.245476: step 2591, loss 0.378842, acc 0.90625, prec 0.048104, recall 0.845555
2017-12-10T14:40:48.429794: step 2592, loss 0.886989, acc 0.96875, prec 0.0481462, recall 0.845682
2017-12-10T14:40:48.619898: step 2593, loss 0.208156, acc 0.921875, prec 0.0482071, recall 0.845871
2017-12-10T14:40:48.806215: step 2594, loss 0.249315, acc 0.953125, prec 0.0482259, recall 0.845934
2017-12-10T14:40:48.993714: step 2595, loss 0.438536, acc 0.890625, prec 0.0482624, recall 0.84606
2017-12-10T14:40:49.181836: step 2596, loss 0.346735, acc 0.890625, prec 0.0482767, recall 0.846122
2017-12-10T14:40:49.367596: step 2597, loss 0.1826, acc 0.921875, prec 0.0483154, recall 0.846248
2017-12-10T14:40:49.555665: step 2598, loss 0.0625885, acc 0.953125, prec 0.048312, recall 0.846248
2017-12-10T14:40:49.742986: step 2599, loss 0.227059, acc 0.921875, prec 0.0483064, recall 0.846248
2017-12-10T14:40:49.928419: step 2600, loss 0.304879, acc 0.921875, prec 0.0483007, recall 0.846248
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_3/1512934336/checkpoints/model-2600

2017-12-10T14:40:51.212236: step 2601, loss 0.380726, acc 0.859375, prec 0.0482906, recall 0.846248
2017-12-10T14:40:51.399074: step 2602, loss 1.42796, acc 0.84375, prec 0.0483237, recall 0.846373
2017-12-10T14:40:51.585909: step 2603, loss 0.349046, acc 0.859375, prec 0.0483136, recall 0.846373
2017-12-10T14:40:51.772541: step 2604, loss 1.04159, acc 0.890625, prec 0.04835, recall 0.846498
2017-12-10T14:40:51.961562: step 2605, loss 0.369175, acc 0.859375, prec 0.048362, recall 0.846561
2017-12-10T14:40:52.154209: step 2606, loss 0.267709, acc 0.875, prec 0.0483972, recall 0.846686
2017-12-10T14:40:52.341772: step 2607, loss 0.279071, acc 0.890625, prec 0.0484115, recall 0.846748
2017-12-10T14:40:52.528996: step 2608, loss 2.92647, acc 0.859375, prec 0.0484025, recall 0.846404
2017-12-10T14:40:52.719748: step 2609, loss 0.322368, acc 0.890625, prec 0.0484167, recall 0.846466
2017-12-10T14:40:52.907364: step 2610, loss 0.309728, acc 0.90625, prec 0.0484542, recall 0.846591
2017-12-10T14:40:53.094248: step 2611, loss 0.537119, acc 0.84375, prec 0.0484429, recall 0.846591
2017-12-10T14:40:53.280541: step 2612, loss 0.536053, acc 0.828125, prec 0.0484305, recall 0.846591
2017-12-10T14:40:53.477523: step 2613, loss 0.237524, acc 0.921875, prec 0.048447, recall 0.846653
2017-12-10T14:40:53.663799: step 2614, loss 0.769142, acc 0.828125, prec 0.0484346, recall 0.846653
2017-12-10T14:40:53.846576: step 2615, loss 0.330572, acc 0.875, prec 0.0484477, recall 0.846715
2017-12-10T14:40:54.032322: step 2616, loss 1.12768, acc 0.796875, prec 0.0484331, recall 0.846715
2017-12-10T14:40:54.223303: step 2617, loss 0.368508, acc 0.90625, prec 0.0484264, recall 0.846715
2017-12-10T14:40:54.411136: step 2618, loss 0.354084, acc 0.890625, prec 0.0484185, recall 0.846715
2017-12-10T14:40:54.598299: step 2619, loss 0.484803, acc 0.84375, prec 0.0484073, recall 0.846715
2017-12-10T14:40:54.782288: step 2620, loss 0.723558, acc 0.75, prec 0.0483893, recall 0.846715
2017-12-10T14:40:54.972621: step 2621, loss 0.137248, acc 0.953125, prec 0.048386, recall 0.846715
2017-12-10T14:40:55.156945: step 2622, loss 0.305758, acc 0.875, prec 0.0483991, recall 0.846777
2017-12-10T14:40:55.348485: step 2623, loss 0.413446, acc 0.8125, prec 0.0483856, recall 0.846777
2017-12-10T14:40:55.534493: step 2624, loss 0.191963, acc 0.921875, prec 0.04838, recall 0.846777
2017-12-10T14:40:55.724850: step 2625, loss 0.75769, acc 0.953125, prec 0.0483987, recall 0.84684
2017-12-10T14:40:55.915636: step 2626, loss 0.132023, acc 0.953125, prec 0.0483953, recall 0.84684
2017-12-10T14:40:56.104265: step 2627, loss 0.334526, acc 0.90625, prec 0.0484106, recall 0.846902
2017-12-10T14:40:56.290701: step 2628, loss 0.283479, acc 0.921875, prec 0.048405, recall 0.846902
2017-12-10T14:40:56.474995: step 2629, loss 0.342331, acc 0.90625, prec 0.0483983, recall 0.846902
2017-12-10T14:40:56.660215: step 2630, loss 0.143698, acc 0.9375, prec 0.0483938, recall 0.846902
2017-12-10T14:40:56.846843: step 2631, loss 0.418673, acc 0.90625, prec 0.0483871, recall 0.846902
2017-12-10T14:40:57.033534: step 2632, loss 0.155508, acc 0.953125, prec 0.0483837, recall 0.846902
2017-12-10T14:40:57.219234: step 2633, loss 0.128544, acc 0.953125, prec 0.0483804, recall 0.846902
2017-12-10T14:40:57.405859: step 2634, loss 0.0892127, acc 0.953125, prec 0.048377, recall 0.846902
2017-12-10T14:40:57.591666: step 2635, loss 0.136024, acc 0.953125, prec 0.0483737, recall 0.846902
2017-12-10T14:40:57.778916: step 2636, loss 0.226767, acc 0.953125, prec 0.0483703, recall 0.846902
2017-12-10T14:40:57.968563: step 2637, loss 0.184741, acc 0.9375, prec 0.0483658, recall 0.846902
2017-12-10T14:40:58.157998: step 2638, loss 0.0385018, acc 0.984375, prec 0.0483647, recall 0.846902
2017-12-10T14:40:58.347049: step 2639, loss 0.108152, acc 0.9375, prec 0.0483822, recall 0.846964
2017-12-10T14:40:58.536303: step 2640, loss 4.0096, acc 0.96875, prec 0.0483811, recall 0.846621
2017-12-10T14:40:58.729475: step 2641, loss 0.316127, acc 0.953125, prec 0.0484218, recall 0.846745
2017-12-10T14:40:58.918680: step 2642, loss 0.0328619, acc 0.984375, prec 0.0484207, recall 0.846745
2017-12-10T14:40:59.111741: step 2643, loss 0.408522, acc 0.921875, prec 0.0484371, recall 0.846807
2017-12-10T14:40:59.298922: step 2644, loss 0.0907426, acc 0.953125, prec 0.0484557, recall 0.846869
2017-12-10T14:40:59.485847: step 2645, loss 0.387522, acc 0.9375, prec 0.0484952, recall 0.846992
2017-12-10T14:40:59.673573: step 2646, loss 0.071638, acc 0.953125, prec 0.0485138, recall 0.847054
2017-12-10T14:40:59.859312: step 2647, loss 0.0401761, acc 0.984375, prec 0.0485127, recall 0.847054
2017-12-10T14:41:00.044677: step 2648, loss 1.12979, acc 0.921875, prec 0.0485291, recall 0.847116
2017-12-10T14:41:00.230572: step 2649, loss 0.14055, acc 0.953125, prec 0.0485257, recall 0.847116
2017-12-10T14:41:00.415954: step 2650, loss 0.134732, acc 0.953125, prec 0.0485224, recall 0.847116
2017-12-10T14:41:00.602004: step 2651, loss 0.415788, acc 0.875, prec 0.0485134, recall 0.847116
2017-12-10T14:41:00.787528: step 2652, loss 0.172535, acc 0.953125, prec 0.04851, recall 0.847116
2017-12-10T14:41:00.973224: step 2653, loss 0.187542, acc 0.9375, prec 0.0485275, recall 0.847177
2017-12-10T14:41:01.166374: step 2654, loss 0.264176, acc 0.953125, prec 0.0485462, recall 0.847239
2017-12-10T14:41:01.353928: step 2655, loss 0.337362, acc 0.890625, prec 0.0485603, recall 0.847301
2017-12-10T14:41:01.543380: step 2656, loss 0.503653, acc 0.921875, prec 0.0486206, recall 0.847485
2017-12-10T14:41:01.731697: step 2657, loss 0.315168, acc 0.90625, prec 0.0486578, recall 0.847608
2017-12-10T14:41:01.915460: step 2658, loss 0.588349, acc 0.828125, prec 0.0486674, recall 0.847669
2017-12-10T14:41:02.101899: step 2659, loss 0.233844, acc 0.9375, prec 0.0486629, recall 0.847669
2017-12-10T14:41:02.284762: step 2660, loss 0.564742, acc 0.875, prec 0.0486978, recall 0.847791
2017-12-10T14:41:02.474282: step 2661, loss 0.574437, acc 0.875, prec 0.0487107, recall 0.847852
2017-12-10T14:41:02.658521: step 2662, loss 0.461978, acc 0.859375, prec 0.0487006, recall 0.847852
2017-12-10T14:41:02.844916: step 2663, loss 0.196729, acc 0.890625, prec 0.0486928, recall 0.847852
2017-12-10T14:41:03.032647: step 2664, loss 0.300477, acc 0.921875, prec 0.0486872, recall 0.847852
2017-12-10T14:41:03.221146: step 2665, loss 0.136536, acc 0.953125, prec 0.0486838, recall 0.847852
2017-12-10T14:41:03.408726: step 2666, loss 0.109444, acc 0.953125, prec 0.0486804, recall 0.847852
2017-12-10T14:41:03.595718: step 2667, loss 0.438418, acc 0.875, prec 0.0486934, recall 0.847913
2017-12-10T14:41:03.781869: step 2668, loss 0.326449, acc 0.921875, prec 0.0486878, recall 0.847913
2017-12-10T14:41:03.967422: step 2669, loss 0.273454, acc 0.921875, prec 0.0487041, recall 0.847974
2017-12-10T14:41:04.155555: step 2670, loss 0.214607, acc 0.890625, prec 0.0486962, recall 0.847974
2017-12-10T14:41:04.342563: step 2671, loss 2.00245, acc 0.921875, prec 0.0486917, recall 0.847634
2017-12-10T14:41:04.530641: step 2672, loss 0.137987, acc 0.921875, prec 0.048708, recall 0.847695
2017-12-10T14:41:04.714484: step 2673, loss 0.254824, acc 0.953125, prec 0.0487485, recall 0.847817
2017-12-10T14:41:04.900364: step 2674, loss 0.400377, acc 0.890625, prec 0.0487625, recall 0.847878
2017-12-10T14:41:05.086749: step 2675, loss 0.225896, acc 0.921875, prec 0.0487788, recall 0.847939
2017-12-10T14:41:05.272780: step 2676, loss 2.3713, acc 0.921875, prec 0.0487743, recall 0.8476
2017-12-10T14:41:05.460600: step 2677, loss 0.420822, acc 0.90625, prec 0.0487676, recall 0.8476
2017-12-10T14:41:05.647192: step 2678, loss 0.277282, acc 0.9375, prec 0.0487631, recall 0.8476
2017-12-10T14:41:05.831252: step 2679, loss 0.109515, acc 0.96875, prec 0.0487608, recall 0.8476
2017-12-10T14:41:06.017892: step 2680, loss 0.182517, acc 0.953125, prec 0.0488231, recall 0.847783
2017-12-10T14:41:06.202131: step 2681, loss 0.253772, acc 0.96875, prec 0.0488428, recall 0.847843
2017-12-10T14:41:06.389337: step 2682, loss 0.169113, acc 0.953125, prec 0.0488394, recall 0.847843
2017-12-10T14:41:06.575750: step 2683, loss 0.483595, acc 0.9375, prec 0.0488787, recall 0.847965
2017-12-10T14:41:06.762804: step 2684, loss 0.322842, acc 0.921875, prec 0.0488949, recall 0.848026
2017-12-10T14:41:06.947118: step 2685, loss 0.403655, acc 0.90625, prec 0.04891, recall 0.848086
2017-12-10T14:41:07.133355: step 2686, loss 0.379695, acc 0.875, prec 0.048901, recall 0.848086
2017-12-10T14:41:07.321242: step 2687, loss 0.300726, acc 0.9375, prec 0.0488966, recall 0.848086
2017-12-10T14:41:07.507518: step 2688, loss 0.497358, acc 0.828125, prec 0.0489061, recall 0.848147
2017-12-10T14:41:07.694124: step 2689, loss 0.199201, acc 0.921875, prec 0.0489223, recall 0.848207
2017-12-10T14:41:07.880965: step 2690, loss 0.260813, acc 0.921875, prec 0.0489385, recall 0.848268
2017-12-10T14:41:08.070027: step 2691, loss 0.478559, acc 0.875, prec 0.0489514, recall 0.848328
2017-12-10T14:41:08.258508: step 2692, loss 0.759372, acc 0.890625, prec 0.0489872, recall 0.848449
2017-12-10T14:41:08.446840: step 2693, loss 0.162124, acc 0.921875, prec 0.0490034, recall 0.848509
2017-12-10T14:41:08.633604: step 2694, loss 0.40965, acc 0.890625, prec 0.0490174, recall 0.848569
2017-12-10T14:41:08.818077: step 2695, loss 0.494664, acc 0.859375, prec 0.0490072, recall 0.848569
2017-12-10T14:41:09.008837: step 2696, loss 0.192833, acc 0.9375, prec 0.0490246, recall 0.848629
2017-12-10T14:41:09.194988: step 2697, loss 0.222252, acc 0.9375, prec 0.0490201, recall 0.848629
2017-12-10T14:41:09.380327: step 2698, loss 0.547014, acc 0.90625, prec 0.0490788, recall 0.84881
2017-12-10T14:41:09.566703: step 2699, loss 0.182098, acc 0.953125, prec 0.0490972, recall 0.84887
2017-12-10T14:41:09.751844: step 2700, loss 0.210451, acc 0.9375, prec 0.0491145, recall 0.848929
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_3/1512934336/checkpoints/model-2700

2017-12-10T14:41:10.929512: step 2701, loss 0.133384, acc 0.921875, prec 0.0491089, recall 0.848929
2017-12-10T14:41:11.115811: step 2702, loss 0.358962, acc 0.875, prec 0.0490999, recall 0.848929
2017-12-10T14:41:11.305624: step 2703, loss 0.317161, acc 0.921875, prec 0.0490942, recall 0.848929
2017-12-10T14:41:11.491026: step 2704, loss 0.20056, acc 0.984375, prec 0.0490931, recall 0.848929
2017-12-10T14:41:11.677942: step 2705, loss 0.154861, acc 0.953125, prec 0.0491115, recall 0.848989
2017-12-10T14:41:11.864237: step 2706, loss 0.146382, acc 0.96875, prec 0.0491093, recall 0.848989
2017-12-10T14:41:12.048444: step 2707, loss 0.0714938, acc 0.984375, prec 0.04913, recall 0.849049
2017-12-10T14:41:12.235668: step 2708, loss 0.0472067, acc 0.96875, prec 0.0491277, recall 0.849049
2017-12-10T14:41:12.423786: step 2709, loss 0.144582, acc 0.953125, prec 0.0491461, recall 0.849109
2017-12-10T14:41:12.611894: step 2710, loss 0.146221, acc 0.953125, prec 0.0491428, recall 0.849109
2017-12-10T14:41:12.798406: step 2711, loss 0.0864727, acc 0.953125, prec 0.0491612, recall 0.849169
2017-12-10T14:41:12.985440: step 2712, loss 3.41055, acc 0.921875, prec 0.0491567, recall 0.848833
2017-12-10T14:41:13.171737: step 2713, loss 0.763377, acc 0.96875, prec 0.049198, recall 0.848952
2017-12-10T14:41:13.358212: step 2714, loss 0.328576, acc 0.96875, prec 0.0492175, recall 0.849012
2017-12-10T14:41:13.545729: step 2715, loss 0.0640395, acc 0.984375, prec 0.0492164, recall 0.849012
2017-12-10T14:41:13.729117: step 2716, loss 0.144675, acc 0.96875, prec 0.0492359, recall 0.849072
2017-12-10T14:41:13.918931: step 2717, loss 0.237413, acc 0.9375, prec 0.0492532, recall 0.849131
2017-12-10T14:41:14.117557: step 2718, loss 0.176886, acc 0.90625, prec 0.0492682, recall 0.849191
2017-12-10T14:41:14.304638: step 2719, loss 0.265784, acc 0.9375, prec 0.0492637, recall 0.849191
2017-12-10T14:41:14.495060: step 2720, loss 1.90831, acc 0.921875, prec 0.0492809, recall 0.848915
2017-12-10T14:41:14.683284: step 2721, loss 0.504441, acc 0.875, prec 0.0492937, recall 0.848975
2017-12-10T14:41:14.868476: step 2722, loss 0.269976, acc 0.921875, prec 0.049288, recall 0.848975
2017-12-10T14:41:15.056492: step 2723, loss 0.307246, acc 0.90625, prec 0.0493248, recall 0.849094
2017-12-10T14:41:15.240633: step 2724, loss 0.433838, acc 0.875, prec 0.0493593, recall 0.849213
2017-12-10T14:41:15.425780: step 2725, loss 0.392181, acc 0.90625, prec 0.0493525, recall 0.849213
2017-12-10T14:41:15.616533: step 2726, loss 0.46137, acc 0.890625, prec 0.0493446, recall 0.849213
2017-12-10T14:41:15.799435: step 2727, loss 0.282323, acc 0.890625, prec 0.0493802, recall 0.849331
2017-12-10T14:41:15.986417: step 2728, loss 0.644293, acc 0.859375, prec 0.0494135, recall 0.84945
2017-12-10T14:41:16.173866: step 2729, loss 0.260667, acc 0.875, prec 0.0494262, recall 0.849509
2017-12-10T14:41:16.357199: step 2730, loss 0.37828, acc 0.890625, prec 0.04944, recall 0.849568
2017-12-10T14:41:16.544333: step 2731, loss 0.569615, acc 0.875, prec 0.049431, recall 0.849568
2017-12-10T14:41:16.733951: step 2732, loss 0.590295, acc 0.859375, prec 0.0494425, recall 0.849627
2017-12-10T14:41:16.920391: step 2733, loss 0.323382, acc 0.875, prec 0.0494335, recall 0.849627
2017-12-10T14:41:17.105923: step 2734, loss 0.36703, acc 0.875, prec 0.0494244, recall 0.849627
2017-12-10T14:41:17.294072: step 2735, loss 0.288542, acc 0.875, prec 0.0494154, recall 0.849627
2017-12-10T14:41:17.483224: step 2736, loss 0.143968, acc 0.90625, prec 0.0494086, recall 0.849627
2017-12-10T14:41:17.670378: step 2737, loss 0.254439, acc 0.90625, prec 0.0494236, recall 0.849686
2017-12-10T14:41:17.859559: step 2738, loss 0.455771, acc 0.84375, prec 0.0494123, recall 0.849686
2017-12-10T14:41:18.044740: step 2739, loss 0.358663, acc 0.890625, prec 0.0494261, recall 0.849745
2017-12-10T14:41:18.233949: step 2740, loss 0.104735, acc 0.9375, prec 0.0494216, recall 0.849745
2017-12-10T14:41:18.423012: step 2741, loss 0.45441, acc 0.890625, prec 0.0494354, recall 0.849804
2017-12-10T14:41:18.609197: step 2742, loss 14.6696, acc 0.890625, prec 0.0494286, recall 0.849471
2017-12-10T14:41:18.796540: step 2743, loss 1.23135, acc 0.953125, prec 0.0494469, recall 0.84953
2017-12-10T14:41:18.987278: step 2744, loss 0.269359, acc 0.921875, prec 0.0494846, recall 0.849648
2017-12-10T14:41:19.174319: step 2745, loss 0.544236, acc 0.84375, prec 0.049495, recall 0.849706
2017-12-10T14:41:19.361909: step 2746, loss 0.892607, acc 0.90625, prec 0.0495099, recall 0.849765
2017-12-10T14:41:19.548687: step 2747, loss 0.537304, acc 0.859375, prec 0.0494998, recall 0.849765
2017-12-10T14:41:19.732541: step 2748, loss 0.3876, acc 0.875, prec 0.0494907, recall 0.849765
2017-12-10T14:41:19.918312: step 2749, loss 0.759363, acc 0.8125, prec 0.0494772, recall 0.849765
2017-12-10T14:41:20.104734: step 2750, loss 0.664047, acc 0.84375, prec 0.0494659, recall 0.849765
2017-12-10T14:41:20.293326: step 2751, loss 0.535363, acc 0.890625, prec 0.0494797, recall 0.849824
2017-12-10T14:41:20.477054: step 2752, loss 0.451508, acc 0.84375, prec 0.0494684, recall 0.849824
2017-12-10T14:41:20.663612: step 2753, loss 0.579958, acc 0.859375, prec 0.0495016, recall 0.849941
2017-12-10T14:41:20.851896: step 2754, loss 0.459981, acc 0.875, prec 0.0495142, recall 0.85
2017-12-10T14:41:21.035630: step 2755, loss 0.558451, acc 0.875, prec 0.0495484, recall 0.850117
2017-12-10T14:41:21.222788: step 2756, loss 0.671319, acc 0.8125, prec 0.0495349, recall 0.850117
2017-12-10T14:41:21.407182: step 2757, loss 0.413935, acc 0.84375, prec 0.0495236, recall 0.850117
2017-12-10T14:41:21.590544: step 2758, loss 0.625418, acc 0.84375, prec 0.049534, recall 0.850176
2017-12-10T14:41:21.777146: step 2759, loss 0.432289, acc 0.859375, prec 0.0495239, recall 0.850176
2017-12-10T14:41:21.963026: step 2760, loss 0.407147, acc 0.890625, prec 0.0495592, recall 0.850292
2017-12-10T14:41:22.149457: step 2761, loss 0.291198, acc 0.921875, prec 0.0495535, recall 0.850292
2017-12-10T14:41:22.335354: step 2762, loss 0.329804, acc 0.890625, prec 0.0495888, recall 0.850409
2017-12-10T14:41:22.519334: step 2763, loss 0.600983, acc 0.90625, prec 0.0496037, recall 0.850467
2017-12-10T14:41:22.707521: step 2764, loss 0.257129, acc 0.9375, prec 0.0495992, recall 0.850467
2017-12-10T14:41:22.890637: step 2765, loss 0.34932, acc 0.875, prec 0.0495902, recall 0.850467
2017-12-10T14:41:23.078475: step 2766, loss 0.285465, acc 0.890625, prec 0.0495823, recall 0.850467
2017-12-10T14:41:23.263295: step 2767, loss 0.123148, acc 0.96875, prec 0.04958, recall 0.850467
2017-12-10T14:41:23.455171: step 2768, loss 1.59151, acc 0.875, prec 0.0496142, recall 0.850584
2017-12-10T14:41:23.641695: step 2769, loss 0.161684, acc 0.9375, prec 0.0496528, recall 0.8507
2017-12-10T14:41:23.827029: step 2770, loss 0.231152, acc 0.9375, prec 0.0496699, recall 0.850758
2017-12-10T14:41:24.018310: step 2771, loss 0.407654, acc 0.84375, prec 0.0497233, recall 0.850932
2017-12-10T14:41:24.208223: step 2772, loss 0.245829, acc 0.90625, prec 0.0497811, recall 0.851105
2017-12-10T14:41:24.396974: step 2773, loss 0.129983, acc 0.96875, prec 0.0497789, recall 0.851105
2017-12-10T14:41:24.583061: step 2774, loss 0.225988, acc 0.90625, prec 0.0497937, recall 0.851163
2017-12-10T14:41:24.771255: step 2775, loss 0.318077, acc 0.921875, prec 0.049788, recall 0.851163
2017-12-10T14:41:24.961829: step 2776, loss 0.0753914, acc 0.96875, prec 0.0497858, recall 0.851163
2017-12-10T14:41:25.152027: step 2777, loss 0.177986, acc 0.921875, prec 0.0497801, recall 0.851163
2017-12-10T14:41:25.339047: step 2778, loss 1.56931, acc 0.9375, prec 0.0498187, recall 0.851278
2017-12-10T14:41:25.529562: step 2779, loss 0.244015, acc 0.90625, prec 0.0498119, recall 0.851278
2017-12-10T14:41:25.717709: step 2780, loss 0.124312, acc 0.96875, prec 0.0498096, recall 0.851278
2017-12-10T14:41:25.908371: step 2781, loss 0.102, acc 0.96875, prec 0.0498074, recall 0.851278
2017-12-10T14:41:26.096173: step 2782, loss 0.11718, acc 0.921875, prec 0.0498233, recall 0.851336
2017-12-10T14:41:26.279800: step 2783, loss 0.202015, acc 0.921875, prec 0.0498176, recall 0.851336
2017-12-10T14:41:26.466872: step 2784, loss 0.20168, acc 0.90625, prec 0.0498109, recall 0.851336
2017-12-10T14:41:26.657126: step 2785, loss 0.257428, acc 0.921875, prec 0.0498267, recall 0.851393
2017-12-10T14:41:26.849292: step 2786, loss 0.0819381, acc 0.96875, prec 0.049846, recall 0.851451
2017-12-10T14:41:27.036628: step 2787, loss 0.245752, acc 0.875, prec 0.0498585, recall 0.851508
2017-12-10T14:41:27.223016: step 2788, loss 0.289834, acc 0.9375, prec 0.049854, recall 0.851508
2017-12-10T14:41:27.408801: step 2789, loss 0.196951, acc 0.953125, prec 0.0498936, recall 0.851623
2017-12-10T14:41:27.596117: step 2790, loss 0.140789, acc 0.921875, prec 0.0499095, recall 0.85168
2017-12-10T14:41:27.780819: step 2791, loss 0.0657067, acc 0.96875, prec 0.0499502, recall 0.851795
2017-12-10T14:41:27.965953: step 2792, loss 0.168764, acc 0.953125, prec 0.0499468, recall 0.851795
2017-12-10T14:41:28.154077: step 2793, loss 0.0990661, acc 0.984375, prec 0.0499672, recall 0.851852
2017-12-10T14:41:28.341659: step 2794, loss 0.373688, acc 0.90625, prec 0.0499604, recall 0.851852
2017-12-10T14:41:28.529459: step 2795, loss 0.105322, acc 0.953125, prec 0.049957, recall 0.851852
2017-12-10T14:41:28.721500: step 2796, loss 0.442801, acc 0.859375, prec 0.0499468, recall 0.851852
2017-12-10T14:41:28.911842: step 2797, loss 0.315054, acc 0.9375, prec 0.0499638, recall 0.851909
2017-12-10T14:41:29.105270: step 2798, loss 0.362858, acc 0.96875, prec 0.049983, recall 0.851966
2017-12-10T14:41:29.292304: step 2799, loss 0.186436, acc 0.921875, prec 0.0499774, recall 0.851966
2017-12-10T14:41:29.483032: step 2800, loss 0.099455, acc 0.96875, prec 0.0499966, recall 0.852023
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_3/1512934336/checkpoints/model-2800

2017-12-10T14:41:30.783503: step 2801, loss 0.114646, acc 0.953125, prec 0.0499932, recall 0.852023
2017-12-10T14:41:30.972677: step 2802, loss 0.122238, acc 0.984375, prec 0.0499921, recall 0.852023
2017-12-10T14:41:31.166091: step 2803, loss 0.190593, acc 0.984375, prec 0.0500554, recall 0.852194
2017-12-10T14:41:31.358427: step 2804, loss 0.320381, acc 0.9375, prec 0.0500723, recall 0.852251
2017-12-10T14:41:31.553040: step 2805, loss 0.179204, acc 0.953125, prec 0.0500689, recall 0.852251
2017-12-10T14:41:31.744588: step 2806, loss 0.0802414, acc 0.953125, prec 0.0500655, recall 0.852251
2017-12-10T14:41:31.929669: step 2807, loss 0.127499, acc 0.96875, prec 0.0500633, recall 0.852251
2017-12-10T14:41:32.114922: step 2808, loss 0.487769, acc 0.921875, prec 0.0501006, recall 0.852364
2017-12-10T14:41:32.304476: step 2809, loss 0.0474165, acc 0.984375, prec 0.0500994, recall 0.852364
2017-12-10T14:41:32.490478: step 2810, loss 0.0995591, acc 0.953125, prec 0.050096, recall 0.852364
2017-12-10T14:41:32.677499: step 2811, loss 1.04977, acc 0.953125, prec 0.050157, recall 0.852535
2017-12-10T14:41:32.862996: step 2812, loss 1.83051, acc 0.953125, prec 0.050218, recall 0.852704
2017-12-10T14:41:33.053286: step 2813, loss 0.296551, acc 0.90625, prec 0.0502326, recall 0.852761
2017-12-10T14:41:33.239600: step 2814, loss 1.20318, acc 0.9375, prec 0.0502496, recall 0.852817
2017-12-10T14:41:33.428069: step 2815, loss 0.122677, acc 0.96875, prec 0.0502473, recall 0.852817
2017-12-10T14:41:33.616682: step 2816, loss 4.01983, acc 0.9375, prec 0.0502868, recall 0.852603
2017-12-10T14:41:33.807330: step 2817, loss 0.555415, acc 0.859375, prec 0.0502766, recall 0.852603
2017-12-10T14:41:33.995936: step 2818, loss 0.636925, acc 0.828125, prec 0.0503069, recall 0.852716
2017-12-10T14:41:34.181462: step 2819, loss 0.276548, acc 0.859375, prec 0.0502967, recall 0.852716
2017-12-10T14:41:34.367404: step 2820, loss 0.393532, acc 0.828125, prec 0.0503271, recall 0.852829
2017-12-10T14:41:34.552206: step 2821, loss 0.453351, acc 0.859375, prec 0.0503169, recall 0.852829
2017-12-10T14:41:34.738043: step 2822, loss 0.586102, acc 0.796875, prec 0.0503235, recall 0.852885
2017-12-10T14:41:34.928257: step 2823, loss 0.528579, acc 0.8125, prec 0.0503313, recall 0.852941
2017-12-10T14:41:35.119860: step 2824, loss 0.752949, acc 0.8125, prec 0.0503605, recall 0.853053
2017-12-10T14:41:35.306595: step 2825, loss 0.995745, acc 0.828125, prec 0.0503694, recall 0.853109
2017-12-10T14:41:35.488343: step 2826, loss 1.03551, acc 0.78125, prec 0.0503749, recall 0.853166
2017-12-10T14:41:35.671879: step 2827, loss 0.529068, acc 0.796875, prec 0.050403, recall 0.853277
2017-12-10T14:41:35.854535: step 2828, loss 7.18934, acc 0.65625, prec 0.0504005, recall 0.853008
2017-12-10T14:41:36.040095: step 2829, loss 0.56272, acc 0.71875, prec 0.0504228, recall 0.85312
2017-12-10T14:41:36.226317: step 2830, loss 1.24357, acc 0.703125, prec 0.0504226, recall 0.853176
2017-12-10T14:41:36.411524: step 2831, loss 1.04675, acc 0.671875, prec 0.0504202, recall 0.853232
2017-12-10T14:41:36.596772: step 2832, loss 0.630107, acc 0.828125, prec 0.050429, recall 0.853288
2017-12-10T14:41:36.781958: step 2833, loss 0.284259, acc 0.875, prec 0.0504626, recall 0.853399
2017-12-10T14:41:36.969856: step 2834, loss 0.528648, acc 0.828125, prec 0.0504715, recall 0.853455
2017-12-10T14:41:37.160795: step 2835, loss 0.576281, acc 0.84375, prec 0.0504602, recall 0.853455
2017-12-10T14:41:37.344372: step 2836, loss 1.09415, acc 0.796875, prec 0.0504667, recall 0.85351
2017-12-10T14:41:37.531780: step 2837, loss 0.435018, acc 0.921875, prec 0.0504611, recall 0.85351
2017-12-10T14:41:37.718507: step 2838, loss 0.507011, acc 0.78125, prec 0.0504452, recall 0.85351
2017-12-10T14:41:37.906555: step 2839, loss 0.415376, acc 0.890625, prec 0.0504586, recall 0.853566
2017-12-10T14:41:38.091250: step 2840, loss 0.489127, acc 0.875, prec 0.0504709, recall 0.853622
2017-12-10T14:41:38.278061: step 2841, loss 0.167633, acc 0.90625, prec 0.0504641, recall 0.853622
2017-12-10T14:41:38.469543: step 2842, loss 0.422919, acc 0.875, prec 0.050455, recall 0.853622
2017-12-10T14:41:38.658057: step 2843, loss 0.218081, acc 0.90625, prec 0.0504482, recall 0.853622
2017-12-10T14:41:38.842814: step 2844, loss 0.208298, acc 0.90625, prec 0.0504414, recall 0.853622
2017-12-10T14:41:39.027496: step 2845, loss 0.963741, acc 0.859375, prec 0.0504525, recall 0.853677
2017-12-10T14:41:39.217659: step 2846, loss 0.920519, acc 0.859375, prec 0.0504849, recall 0.853788
2017-12-10T14:41:39.404635: step 2847, loss 0.230754, acc 0.90625, prec 0.0504781, recall 0.853788
2017-12-10T14:41:39.593085: step 2848, loss 0.0826479, acc 0.953125, prec 0.0505173, recall 0.853899
2017-12-10T14:41:39.786006: step 2849, loss 0.316591, acc 0.890625, prec 0.0505306, recall 0.853954
2017-12-10T14:41:39.978290: step 2850, loss 0.707446, acc 0.921875, prec 0.0505675, recall 0.854064
2017-12-10T14:41:40.166800: step 2851, loss 0.210156, acc 0.96875, prec 0.0505652, recall 0.854064
2017-12-10T14:41:40.355627: step 2852, loss 0.434625, acc 0.9375, prec 0.0505819, recall 0.854119
2017-12-10T14:41:40.544700: step 2853, loss 1.62865, acc 0.90625, prec 0.0505763, recall 0.853797
2017-12-10T14:41:40.735727: step 2854, loss 0.187815, acc 0.921875, prec 0.0505706, recall 0.853797
2017-12-10T14:41:40.926874: step 2855, loss 0.292695, acc 0.953125, prec 0.0506309, recall 0.853962
2017-12-10T14:41:41.116944: step 2856, loss 0.310615, acc 0.96875, prec 0.0507136, recall 0.854182
2017-12-10T14:41:41.302600: step 2857, loss 0.167024, acc 0.9375, prec 0.0507091, recall 0.854182
2017-12-10T14:41:41.490456: step 2858, loss 0.177862, acc 0.90625, prec 0.0507235, recall 0.854237
2017-12-10T14:41:41.678699: step 2859, loss 0.219938, acc 0.921875, prec 0.0507178, recall 0.854237
2017-12-10T14:41:41.864241: step 2860, loss 0.25347, acc 0.90625, prec 0.0507535, recall 0.854347
2017-12-10T14:41:42.048110: step 2861, loss 0.117429, acc 0.96875, prec 0.0507512, recall 0.854347
2017-12-10T14:41:42.238454: step 2862, loss 0.256491, acc 0.890625, prec 0.0507433, recall 0.854347
2017-12-10T14:41:42.427104: step 2863, loss 0.313253, acc 0.9375, prec 0.0507387, recall 0.854347
2017-12-10T14:41:42.620914: step 2864, loss 0.208167, acc 0.953125, prec 0.0507565, recall 0.854402
2017-12-10T14:41:42.807701: step 2865, loss 0.262014, acc 0.90625, prec 0.0507497, recall 0.854402
2017-12-10T14:41:42.995081: step 2866, loss 0.530724, acc 0.890625, prec 0.0508054, recall 0.854566
2017-12-10T14:41:43.186766: step 2867, loss 0.540926, acc 0.90625, prec 0.0508198, recall 0.854621
2017-12-10T14:41:43.374722: step 2868, loss 2.64511, acc 0.90625, prec 0.0508141, recall 0.8543
2017-12-10T14:41:43.564323: step 2869, loss 0.207599, acc 0.90625, prec 0.0508073, recall 0.8543
2017-12-10T14:41:43.755818: step 2870, loss 0.196475, acc 0.96875, prec 0.0508475, recall 0.854409
2017-12-10T14:41:43.942608: step 2871, loss 0.265415, acc 0.90625, prec 0.050883, recall 0.854518
2017-12-10T14:41:44.137850: step 2872, loss 0.342138, acc 0.90625, prec 0.0508974, recall 0.854573
2017-12-10T14:41:44.326200: step 2873, loss 0.401873, acc 0.90625, prec 0.0509965, recall 0.854845
2017-12-10T14:41:44.512138: step 2874, loss 1.51049, acc 0.859375, prec 0.0510498, recall 0.855007
2017-12-10T14:41:44.700738: step 2875, loss 3.2675, acc 0.859375, prec 0.0510618, recall 0.854742
2017-12-10T14:41:44.890459: step 2876, loss 0.307582, acc 0.90625, prec 0.051055, recall 0.854742
2017-12-10T14:41:45.079874: step 2877, loss 0.397144, acc 0.8125, prec 0.0510837, recall 0.854851
2017-12-10T14:41:45.263217: step 2878, loss 0.706288, acc 0.796875, prec 0.0510689, recall 0.854851
2017-12-10T14:41:45.452656: step 2879, loss 0.861084, acc 0.8125, prec 0.0510552, recall 0.854851
2017-12-10T14:41:45.637370: step 2880, loss 0.363911, acc 0.84375, prec 0.0511073, recall 0.855013
2017-12-10T14:41:45.823091: step 2881, loss 0.768726, acc 0.78125, prec 0.0510913, recall 0.855013
2017-12-10T14:41:46.007591: step 2882, loss 0.466051, acc 0.828125, prec 0.0510999, recall 0.855067
2017-12-10T14:41:46.194949: step 2883, loss 0.703191, acc 0.84375, prec 0.0511097, recall 0.855121
2017-12-10T14:41:46.384453: step 2884, loss 0.864479, acc 0.796875, prec 0.051116, recall 0.855175
2017-12-10T14:41:46.568764: step 2885, loss 0.879413, acc 0.78125, prec 0.0511001, recall 0.855175
2017-12-10T14:41:46.758268: step 2886, loss 0.994782, acc 0.71875, prec 0.0510796, recall 0.855175
2017-12-10T14:41:46.941956: step 2887, loss 0.73704, acc 0.875, prec 0.0511127, recall 0.855283
2017-12-10T14:41:47.128536: step 2888, loss 0.461139, acc 0.875, prec 0.0511037, recall 0.855283
2017-12-10T14:41:47.315883: step 2889, loss 0.65799, acc 0.875, prec 0.0510946, recall 0.855283
2017-12-10T14:41:47.507799: step 2890, loss 0.384182, acc 0.890625, prec 0.0510866, recall 0.855283
2017-12-10T14:41:47.692609: step 2891, loss 0.939082, acc 0.890625, prec 0.0510998, recall 0.855337
2017-12-10T14:41:47.883103: step 2892, loss 0.408865, acc 0.84375, prec 0.0510884, recall 0.855337
2017-12-10T14:41:48.067454: step 2893, loss 0.189596, acc 0.921875, prec 0.0510827, recall 0.855337
2017-12-10T14:41:48.252361: step 2894, loss 0.43282, acc 0.84375, prec 0.0510714, recall 0.855337
2017-12-10T14:41:48.439496: step 2895, loss 0.273894, acc 0.890625, prec 0.0510635, recall 0.855337
2017-12-10T14:41:48.623255: step 2896, loss 0.436309, acc 0.859375, prec 0.0510954, recall 0.855444
2017-12-10T14:41:48.810407: step 2897, loss 0.111787, acc 0.953125, prec 0.0511341, recall 0.855551
2017-12-10T14:41:49.000800: step 2898, loss 0.314824, acc 0.921875, prec 0.0511284, recall 0.855551
2017-12-10T14:41:49.188154: step 2899, loss 0.131469, acc 0.96875, prec 0.0511683, recall 0.855659
2017-12-10T14:41:49.376566: step 2900, loss 0.312995, acc 0.921875, prec 0.0512047, recall 0.855766
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_3/1512934336/checkpoints/model-2900

2017-12-10T14:41:50.622037: step 2901, loss 4.09213, acc 0.921875, prec 0.0512434, recall 0.855239
2017-12-10T14:41:50.814053: step 2902, loss 0.252061, acc 0.921875, prec 0.0512587, recall 0.855292
2017-12-10T14:41:51.000944: step 2903, loss 0.168633, acc 0.90625, prec 0.0512519, recall 0.855292
2017-12-10T14:41:51.187765: step 2904, loss 0.394772, acc 0.84375, prec 0.0512616, recall 0.855346
2017-12-10T14:41:51.374619: step 2905, loss 0.155324, acc 0.9375, prec 0.051257, recall 0.855346
2017-12-10T14:41:51.562056: step 2906, loss 0.421554, acc 0.859375, prec 0.0512468, recall 0.855346
2017-12-10T14:41:51.749784: step 2907, loss 0.207347, acc 0.921875, prec 0.0512411, recall 0.855346
2017-12-10T14:41:51.936723: step 2908, loss 0.385296, acc 0.875, prec 0.0512321, recall 0.855346
2017-12-10T14:41:52.126972: step 2909, loss 0.377181, acc 0.859375, prec 0.0512218, recall 0.855346
2017-12-10T14:41:52.312832: step 2910, loss 0.403379, acc 0.875, prec 0.0512338, recall 0.855399
2017-12-10T14:41:52.499961: step 2911, loss 0.275145, acc 0.890625, prec 0.0513099, recall 0.855613
2017-12-10T14:41:52.689094: step 2912, loss 0.28874, acc 0.890625, prec 0.0513229, recall 0.855666
2017-12-10T14:41:52.876072: step 2913, loss 0.703783, acc 0.796875, prec 0.0513292, recall 0.85572
2017-12-10T14:41:53.063746: step 2914, loss 0.194505, acc 0.96875, prec 0.0513689, recall 0.855826
2017-12-10T14:41:53.249267: step 2915, loss 0.615279, acc 0.875, prec 0.0514018, recall 0.855932
2017-12-10T14:41:53.436660: step 2916, loss 0.54951, acc 0.890625, prec 0.0514148, recall 0.855985
2017-12-10T14:41:53.624771: step 2917, loss 0.418242, acc 0.875, prec 0.0514477, recall 0.856091
2017-12-10T14:41:53.813435: step 2918, loss 0.079844, acc 0.96875, prec 0.0514454, recall 0.856091
2017-12-10T14:41:54.003806: step 2919, loss 0.256609, acc 0.921875, prec 0.0514816, recall 0.856197
2017-12-10T14:41:54.198628: step 2920, loss 0.280702, acc 0.90625, prec 0.0514748, recall 0.856197
2017-12-10T14:41:54.388209: step 2921, loss 0.301554, acc 0.90625, prec 0.051468, recall 0.856197
2017-12-10T14:41:54.575864: step 2922, loss 0.422223, acc 0.890625, prec 0.0515229, recall 0.856356
2017-12-10T14:41:54.760439: step 2923, loss 0.138965, acc 0.9375, prec 0.0515184, recall 0.856356
2017-12-10T14:41:54.950456: step 2924, loss 0.156767, acc 0.96875, prec 0.0515161, recall 0.856356
2017-12-10T14:41:55.136784: step 2925, loss 0.287351, acc 0.90625, prec 0.0515302, recall 0.856408
2017-12-10T14:41:55.329715: step 2926, loss 1.8436, acc 0.96875, prec 0.05155, recall 0.856147
2017-12-10T14:41:55.522537: step 2927, loss 0.210428, acc 0.921875, prec 0.0515443, recall 0.856147
2017-12-10T14:41:55.706202: step 2928, loss 0.293231, acc 0.875, prec 0.0515562, recall 0.8562
2017-12-10T14:41:55.894389: step 2929, loss 0.103585, acc 0.96875, prec 0.0515539, recall 0.8562
2017-12-10T14:41:56.083352: step 2930, loss 0.533331, acc 0.875, prec 0.0515657, recall 0.856252
2017-12-10T14:41:56.268295: step 2931, loss 0.387004, acc 0.9375, prec 0.0515821, recall 0.856305
2017-12-10T14:41:56.452042: step 2932, loss 0.112552, acc 0.96875, prec 0.0516217, recall 0.85641
2017-12-10T14:41:56.637267: step 2933, loss 0.205229, acc 0.953125, prec 0.0516393, recall 0.856463
2017-12-10T14:41:56.820618: step 2934, loss 0.204167, acc 0.921875, prec 0.0516545, recall 0.856515
2017-12-10T14:41:57.004397: step 2935, loss 0.26904, acc 1, prec 0.0516754, recall 0.856568
2017-12-10T14:41:57.191013: step 2936, loss 0.625321, acc 0.84375, prec 0.0517059, recall 0.856673
2017-12-10T14:41:57.377959: step 2937, loss 0.194488, acc 0.9375, prec 0.0517013, recall 0.856673
2017-12-10T14:41:57.566308: step 2938, loss 0.0793515, acc 0.984375, prec 0.0517211, recall 0.856725
2017-12-10T14:41:57.754248: step 2939, loss 0.32099, acc 0.9375, prec 0.0517375, recall 0.856777
2017-12-10T14:41:57.942370: step 2940, loss 0.0843742, acc 0.953125, prec 0.0517549, recall 0.85683
2017-12-10T14:41:58.126393: step 2941, loss 0.303385, acc 0.828125, prec 0.0517633, recall 0.856882
2017-12-10T14:41:58.313745: step 2942, loss 0.0810482, acc 0.984375, prec 0.0517622, recall 0.856882
2017-12-10T14:41:58.501408: step 2943, loss 1.1068, acc 0.984375, prec 0.0518028, recall 0.856987
2017-12-10T14:41:58.689150: step 2944, loss 0.860533, acc 0.90625, prec 0.0518169, recall 0.857039
2017-12-10T14:41:58.878405: step 2945, loss 0.171771, acc 0.921875, prec 0.0518321, recall 0.857091
2017-12-10T14:41:59.070145: step 2946, loss 8.09228, acc 0.921875, prec 0.0518484, recall 0.856831
2017-12-10T14:41:59.265913: step 2947, loss 0.416601, acc 0.859375, prec 0.051859, recall 0.856883
2017-12-10T14:41:59.451565: step 2948, loss 0.655118, acc 0.8125, prec 0.0518453, recall 0.856883
2017-12-10T14:41:59.639349: step 2949, loss 0.564255, acc 0.796875, prec 0.0518305, recall 0.856883
2017-12-10T14:41:59.828132: step 2950, loss 0.601603, acc 0.828125, prec 0.0518179, recall 0.856883
2017-12-10T14:42:00.018356: step 2951, loss 0.981477, acc 0.734375, prec 0.0517985, recall 0.856883
2017-12-10T14:42:00.205882: step 2952, loss 0.641934, acc 0.84375, prec 0.0518289, recall 0.856987
2017-12-10T14:42:00.389491: step 2953, loss 0.915744, acc 0.734375, prec 0.0518095, recall 0.856987
2017-12-10T14:42:00.578732: step 2954, loss 0.506885, acc 0.859375, prec 0.0518618, recall 0.857143
2017-12-10T14:42:00.765335: step 2955, loss 0.927212, acc 0.78125, prec 0.0518667, recall 0.857195
2017-12-10T14:42:00.948705: step 2956, loss 0.703293, acc 0.75, prec 0.0518693, recall 0.857247
2017-12-10T14:42:01.137169: step 2957, loss 0.362007, acc 0.859375, prec 0.051859, recall 0.857247
2017-12-10T14:42:01.327030: step 2958, loss 0.589934, acc 0.828125, prec 0.0518465, recall 0.857247
2017-12-10T14:42:01.514035: step 2959, loss 0.662573, acc 0.796875, prec 0.0518317, recall 0.857247
2017-12-10T14:42:01.699054: step 2960, loss 0.880983, acc 0.875, prec 0.0518434, recall 0.857298
2017-12-10T14:42:01.888303: step 2961, loss 0.444978, acc 0.875, prec 0.0518551, recall 0.85735
2017-12-10T14:42:02.074866: step 2962, loss 0.465458, acc 0.890625, prec 0.0518471, recall 0.85735
2017-12-10T14:42:02.263705: step 2963, loss 0.687248, acc 0.828125, prec 0.0518554, recall 0.857402
2017-12-10T14:42:02.451747: step 2964, loss 4.06374, acc 0.84375, prec 0.0518868, recall 0.857195
2017-12-10T14:42:02.638253: step 2965, loss 0.692546, acc 0.796875, prec 0.0518928, recall 0.857246
2017-12-10T14:42:02.824834: step 2966, loss 0.497531, acc 0.890625, prec 0.0519056, recall 0.857298
2017-12-10T14:42:03.012984: step 2967, loss 0.627216, acc 0.84375, prec 0.0519358, recall 0.857401
2017-12-10T14:42:03.205856: step 2968, loss 0.511196, acc 0.921875, prec 0.0519509, recall 0.857453
2017-12-10T14:42:03.394016: step 2969, loss 0.492069, acc 0.859375, prec 0.0519614, recall 0.857505
2017-12-10T14:42:03.577915: step 2970, loss 0.690815, acc 0.828125, prec 0.0519904, recall 0.857608
2017-12-10T14:42:03.763212: step 2971, loss 0.70833, acc 0.75, prec 0.0520138, recall 0.85771
2017-12-10T14:42:03.951053: step 2972, loss 0.823367, acc 0.78125, prec 0.0520393, recall 0.857813
2017-12-10T14:42:04.141235: step 2973, loss 0.391699, acc 0.90625, prec 0.0520532, recall 0.857864
2017-12-10T14:42:04.327738: step 2974, loss 0.959969, acc 0.796875, prec 0.0520384, recall 0.857864
2017-12-10T14:42:04.512470: step 2975, loss 0.270639, acc 0.90625, prec 0.0520316, recall 0.857864
2017-12-10T14:42:04.696127: step 2976, loss 0.415214, acc 0.921875, prec 0.0520466, recall 0.857916
2017-12-10T14:42:04.884091: step 2977, loss 0.169853, acc 0.890625, prec 0.0520594, recall 0.857967
2017-12-10T14:42:05.073835: step 2978, loss 0.374203, acc 0.890625, prec 0.0520929, recall 0.858069
2017-12-10T14:42:05.258735: step 2979, loss 0.200809, acc 0.890625, prec 0.0520849, recall 0.858069
2017-12-10T14:42:05.449403: step 2980, loss 0.226174, acc 0.921875, prec 0.0521207, recall 0.858171
2017-12-10T14:42:05.639801: step 2981, loss 0.121402, acc 0.984375, prec 0.0521195, recall 0.858171
2017-12-10T14:42:05.811549: step 2982, loss 0.499016, acc 0.884615, prec 0.0521127, recall 0.858171
2017-12-10T14:42:06.003650: step 2983, loss 0.423763, acc 0.890625, prec 0.0521255, recall 0.858222
2017-12-10T14:42:06.190040: step 2984, loss 0.409481, acc 0.859375, prec 0.0521566, recall 0.858324
2017-12-10T14:42:06.376635: step 2985, loss 0.462692, acc 0.953125, prec 0.0521946, recall 0.858426
2017-12-10T14:42:06.563795: step 2986, loss 0.10879, acc 0.953125, prec 0.0522119, recall 0.858477
2017-12-10T14:42:06.748987: step 2987, loss 0.0593671, acc 0.984375, prec 0.0522315, recall 0.858528
2017-12-10T14:42:06.935988: step 2988, loss 0.033666, acc 0.984375, prec 0.0522303, recall 0.858528
2017-12-10T14:42:07.121282: step 2989, loss 0.0872107, acc 0.953125, prec 0.0522269, recall 0.858528
2017-12-10T14:42:07.308891: step 2990, loss 0.0556462, acc 0.96875, prec 0.0522246, recall 0.858528
2017-12-10T14:42:07.496461: step 2991, loss 0.4127, acc 0.890625, prec 0.052258, recall 0.858629
2017-12-10T14:42:07.689846: step 2992, loss 0.179398, acc 0.96875, prec 0.0522558, recall 0.858629
2017-12-10T14:42:07.878484: step 2993, loss 0.870791, acc 0.921875, prec 0.0522707, recall 0.85868
2017-12-10T14:42:08.067172: step 2994, loss 0.0715153, acc 0.96875, prec 0.0522892, recall 0.858731
2017-12-10T14:42:08.258414: step 2995, loss 0.706613, acc 0.9375, prec 0.0523467, recall 0.858883
2017-12-10T14:42:08.448707: step 2996, loss 0.137321, acc 0.953125, prec 0.0523432, recall 0.858883
2017-12-10T14:42:08.636409: step 2997, loss 6.99758, acc 0.953125, prec 0.0523616, recall 0.858626
2017-12-10T14:42:08.828061: step 2998, loss 0.132424, acc 0.984375, prec 0.0523812, recall 0.858676
2017-12-10T14:42:09.018440: step 2999, loss 0.536607, acc 0.953125, prec 0.0523984, recall 0.858727
2017-12-10T14:42:09.206128: step 3000, loss 0.171416, acc 0.921875, prec 0.0524134, recall 0.858777
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_3/1512934336/checkpoints/model-3000

2017-12-10T14:42:10.517115: step 3001, loss 0.852673, acc 0.90625, prec 0.0524892, recall 0.858979
2017-12-10T14:42:10.709959: step 3002, loss 0.230215, acc 0.921875, prec 0.0524835, recall 0.858979
2017-12-10T14:42:10.900395: step 3003, loss 0.535392, acc 0.859375, prec 0.0525145, recall 0.85908
2017-12-10T14:42:11.088482: step 3004, loss 0.714061, acc 0.75, prec 0.0524962, recall 0.85908
2017-12-10T14:42:11.277091: step 3005, loss 0.650941, acc 0.8125, prec 0.0524825, recall 0.85908
2017-12-10T14:42:11.465854: step 3006, loss 0.525764, acc 0.828125, prec 0.0524699, recall 0.85908
2017-12-10T14:42:11.651899: step 3007, loss 0.389071, acc 0.859375, prec 0.0524802, recall 0.85913
2017-12-10T14:42:11.835847: step 3008, loss 0.400469, acc 0.875, prec 0.0524711, recall 0.85913
2017-12-10T14:42:12.020140: step 3009, loss 0.559777, acc 0.8125, prec 0.052478, recall 0.85918
2017-12-10T14:42:12.206654: step 3010, loss 0.554304, acc 0.828125, prec 0.0524861, recall 0.85923
2017-12-10T14:42:12.391420: step 3011, loss 0.770413, acc 0.78125, prec 0.0524907, recall 0.85928
2017-12-10T14:42:12.578621: step 3012, loss 0.851382, acc 0.75, prec 0.0524724, recall 0.85928
2017-12-10T14:42:12.765663: step 3013, loss 0.339348, acc 0.90625, prec 0.0524656, recall 0.85928
2017-12-10T14:42:12.953197: step 3014, loss 0.488539, acc 0.859375, prec 0.0524759, recall 0.85933
2017-12-10T14:42:13.137793: step 3015, loss 0.264619, acc 0.890625, prec 0.0524885, recall 0.859381
2017-12-10T14:42:13.326002: step 3016, loss 0.364228, acc 0.875, prec 0.0525412, recall 0.859531
2017-12-10T14:42:13.510567: step 3017, loss 0.421244, acc 0.859375, prec 0.0525515, recall 0.859581
2017-12-10T14:42:13.697553: step 3018, loss 0.153603, acc 0.9375, prec 0.0525881, recall 0.85968
2017-12-10T14:42:13.883022: step 3019, loss 0.580603, acc 0.859375, prec 0.052619, recall 0.85978
2017-12-10T14:42:14.074517: step 3020, loss 0.5913, acc 0.8125, prec 0.0526464, recall 0.859879
2017-12-10T14:42:14.260581: step 3021, loss 0.318391, acc 0.890625, prec 0.0526384, recall 0.859879
2017-12-10T14:42:14.445047: step 3022, loss 0.164991, acc 0.9375, prec 0.0526339, recall 0.859879
2017-12-10T14:42:14.635893: step 3023, loss 0.216094, acc 0.921875, prec 0.0526487, recall 0.859929
2017-12-10T14:42:14.823070: step 3024, loss 0.145558, acc 0.96875, prec 0.0526464, recall 0.859929
2017-12-10T14:42:15.009767: step 3025, loss 1.01672, acc 0.9375, prec 0.0526624, recall 0.859979
2017-12-10T14:42:15.199123: step 3026, loss 0.0369087, acc 0.984375, prec 0.0526613, recall 0.859979
2017-12-10T14:42:15.384570: step 3027, loss 0.178074, acc 0.9375, prec 0.0526567, recall 0.859979
2017-12-10T14:42:15.573018: step 3028, loss 0.0411077, acc 0.984375, prec 0.0526556, recall 0.859979
2017-12-10T14:42:15.761808: step 3029, loss 0.180049, acc 0.9375, prec 0.0526716, recall 0.860028
2017-12-10T14:42:15.947918: step 3030, loss 0.15255, acc 0.96875, prec 0.0526693, recall 0.860028
2017-12-10T14:42:16.135434: step 3031, loss 0.0352648, acc 0.984375, prec 0.0526681, recall 0.860028
2017-12-10T14:42:16.321462: step 3032, loss 0.0472379, acc 0.984375, prec 0.0526875, recall 0.860078
2017-12-10T14:42:16.506977: step 3033, loss 0.05501, acc 0.96875, prec 0.0526853, recall 0.860078
2017-12-10T14:42:16.696130: step 3034, loss 0.466, acc 0.96875, prec 0.0527035, recall 0.860127
2017-12-10T14:42:16.888806: step 3035, loss 3.13605, acc 0.953125, prec 0.0527218, recall 0.859873
2017-12-10T14:42:17.080143: step 3036, loss 0.129292, acc 0.96875, prec 0.0527195, recall 0.859873
2017-12-10T14:42:17.269552: step 3037, loss 0.196727, acc 0.9375, prec 0.0527355, recall 0.859922
2017-12-10T14:42:17.460915: step 3038, loss 0.109344, acc 0.9375, prec 0.0527309, recall 0.859922
2017-12-10T14:42:17.651377: step 3039, loss 0.0975568, acc 0.96875, prec 0.0527697, recall 0.860021
2017-12-10T14:42:17.838775: step 3040, loss 0.107868, acc 0.96875, prec 0.0527674, recall 0.860021
2017-12-10T14:42:18.026885: step 3041, loss 0.140289, acc 0.921875, prec 0.0527617, recall 0.860021
2017-12-10T14:42:18.220262: step 3042, loss 0.258762, acc 0.90625, prec 0.0527548, recall 0.860021
2017-12-10T14:42:18.409075: step 3043, loss 0.139503, acc 0.953125, prec 0.0527514, recall 0.860021
2017-12-10T14:42:18.600626: step 3044, loss 0.40389, acc 0.953125, prec 0.0527685, recall 0.860071
2017-12-10T14:42:18.790290: step 3045, loss 0.342489, acc 0.90625, prec 0.0528232, recall 0.860219
2017-12-10T14:42:18.977892: step 3046, loss 0.113973, acc 0.9375, prec 0.0528187, recall 0.860219
2017-12-10T14:42:19.163505: step 3047, loss 0.160637, acc 0.90625, prec 0.0528118, recall 0.860219
2017-12-10T14:42:19.349533: step 3048, loss 0.210454, acc 0.90625, prec 0.0528049, recall 0.860219
2017-12-10T14:42:19.534295: step 3049, loss 0.233715, acc 0.890625, prec 0.0528174, recall 0.860268
2017-12-10T14:42:19.723958: step 3050, loss 0.343198, acc 0.921875, prec 0.0528117, recall 0.860268
2017-12-10T14:42:19.908785: step 3051, loss 1.61089, acc 0.890625, prec 0.0528049, recall 0.859965
2017-12-10T14:42:20.098106: step 3052, loss 0.238016, acc 0.9375, prec 0.0528413, recall 0.860063
2017-12-10T14:42:20.288419: step 3053, loss 0.375292, acc 0.859375, prec 0.052872, recall 0.860162
2017-12-10T14:42:20.474831: step 3054, loss 0.437035, acc 0.890625, prec 0.0528845, recall 0.860211
2017-12-10T14:42:20.663904: step 3055, loss 0.392686, acc 0.84375, prec 0.0528936, recall 0.86026
2017-12-10T14:42:20.853482: step 3056, loss 0.124163, acc 0.953125, prec 0.0529106, recall 0.86031
2017-12-10T14:42:21.039722: step 3057, loss 0.383322, acc 0.875, prec 0.0529015, recall 0.86031
2017-12-10T14:42:21.226098: step 3058, loss 0.928936, acc 0.890625, prec 0.0529139, recall 0.860359
2017-12-10T14:42:21.417200: step 3059, loss 0.241367, acc 0.921875, prec 0.0529082, recall 0.860359
2017-12-10T14:42:21.603544: step 3060, loss 0.231997, acc 0.921875, prec 0.0529639, recall 0.860506
2017-12-10T14:42:21.789698: step 3061, loss 0.101727, acc 0.96875, prec 0.0529821, recall 0.860555
2017-12-10T14:42:21.981821: step 3062, loss 0.531998, acc 0.859375, prec 0.0530128, recall 0.860653
2017-12-10T14:42:22.172335: step 3063, loss 0.409426, acc 0.875, prec 0.0530036, recall 0.860653
2017-12-10T14:42:22.362017: step 3064, loss 0.277626, acc 0.90625, prec 0.0529967, recall 0.860653
2017-12-10T14:42:22.553930: step 3065, loss 0.21456, acc 0.921875, prec 0.0530115, recall 0.860702
2017-12-10T14:42:22.743178: step 3066, loss 0.174819, acc 0.921875, prec 0.0530057, recall 0.860702
2017-12-10T14:42:22.927487: step 3067, loss 0.211195, acc 0.921875, prec 0.053, recall 0.860702
2017-12-10T14:42:23.112426: step 3068, loss 0.307726, acc 0.921875, prec 0.0530148, recall 0.860751
2017-12-10T14:42:23.297273: step 3069, loss 0.166562, acc 0.9375, prec 0.0530102, recall 0.860751
2017-12-10T14:42:23.484934: step 3070, loss 0.26948, acc 0.90625, prec 0.0530033, recall 0.860751
2017-12-10T14:42:23.672669: step 3071, loss 0.469744, acc 0.90625, prec 0.0529964, recall 0.860751
2017-12-10T14:42:23.861034: step 3072, loss 0.141818, acc 0.96875, prec 0.0530146, recall 0.860799
2017-12-10T14:42:24.049048: step 3073, loss 0.15838, acc 0.953125, prec 0.0530316, recall 0.860848
2017-12-10T14:42:24.235413: step 3074, loss 0.143244, acc 0.96875, prec 0.0530498, recall 0.860897
2017-12-10T14:42:24.421207: step 3075, loss 0.251939, acc 0.9375, prec 0.0530656, recall 0.860946
2017-12-10T14:42:24.607836: step 3076, loss 1.29202, acc 0.953125, prec 0.0530633, recall 0.860644
2017-12-10T14:42:24.796486: step 3077, loss 0.137998, acc 0.953125, prec 0.0530803, recall 0.860693
2017-12-10T14:42:24.985176: step 3078, loss 0.0410363, acc 0.984375, prec 0.0530792, recall 0.860693
2017-12-10T14:42:25.177738: step 3079, loss 0.391454, acc 0.9375, prec 0.0530746, recall 0.860693
2017-12-10T14:42:25.364802: step 3080, loss 0.152311, acc 0.9375, prec 0.0530905, recall 0.860742
2017-12-10T14:42:25.555378: step 3081, loss 0.179504, acc 0.96875, prec 0.053129, recall 0.860839
2017-12-10T14:42:25.742608: step 3082, loss 0.0991847, acc 0.953125, prec 0.0531256, recall 0.860839
2017-12-10T14:42:25.927855: step 3083, loss 0.233492, acc 0.90625, prec 0.0531392, recall 0.860888
2017-12-10T14:42:26.115615: step 3084, loss 0.622607, acc 0.953125, prec 0.053197, recall 0.861033
2017-12-10T14:42:26.302734: step 3085, loss 0.34676, acc 0.84375, prec 0.0532059, recall 0.861082
2017-12-10T14:42:26.490469: step 3086, loss 0.389224, acc 0.953125, prec 0.0532433, recall 0.861179
2017-12-10T14:42:26.682983: step 3087, loss 0.169494, acc 0.921875, prec 0.0532376, recall 0.861179
2017-12-10T14:42:26.871527: step 3088, loss 0.300727, acc 0.921875, prec 0.0532319, recall 0.861179
2017-12-10T14:42:27.057676: step 3089, loss 0.670788, acc 0.96875, prec 0.0533112, recall 0.861372
2017-12-10T14:42:27.247648: step 3090, loss 0.0341049, acc 1, prec 0.0533316, recall 0.861421
2017-12-10T14:42:27.435709: step 3091, loss 0.309455, acc 0.890625, prec 0.0533236, recall 0.861421
2017-12-10T14:42:27.625917: step 3092, loss 0.189786, acc 0.921875, prec 0.0533178, recall 0.861421
2017-12-10T14:42:27.812185: step 3093, loss 0.326793, acc 0.90625, prec 0.0533517, recall 0.861517
2017-12-10T14:42:28.001220: step 3094, loss 0.250852, acc 0.921875, prec 0.0533664, recall 0.861565
2017-12-10T14:42:28.185640: step 3095, loss 0.385439, acc 0.921875, prec 0.0533606, recall 0.861565
2017-12-10T14:42:28.374401: step 3096, loss 0.266232, acc 0.9375, prec 0.053356, recall 0.861565
2017-12-10T14:42:28.560832: step 3097, loss 0.143643, acc 0.9375, prec 0.0533718, recall 0.861613
2017-12-10T14:42:28.748863: step 3098, loss 0.135586, acc 0.953125, prec 0.0533684, recall 0.861613
2017-12-10T14:42:28.940229: step 3099, loss 0.515776, acc 0.921875, prec 0.0534034, recall 0.86171
2017-12-10T14:42:29.133044: step 3100, loss 0.170087, acc 0.96875, prec 0.0534011, recall 0.86171
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_3/1512934336/checkpoints/model-3100

2017-12-10T14:42:30.302503: step 3101, loss 0.453506, acc 0.9375, prec 0.0534373, recall 0.861806
2017-12-10T14:42:30.494592: step 3102, loss 0.240733, acc 0.9375, prec 0.0534734, recall 0.861901
2017-12-10T14:42:30.680710: step 3103, loss 0.0757747, acc 0.984375, prec 0.0534926, recall 0.861949
2017-12-10T14:42:30.869976: step 3104, loss 0.262477, acc 0.9375, prec 0.0535491, recall 0.862093
2017-12-10T14:42:31.066633: step 3105, loss 0.481423, acc 0.921875, prec 0.0535637, recall 0.862141
2017-12-10T14:42:31.262841: step 3106, loss 0.160418, acc 0.96875, prec 0.0535818, recall 0.862188
2017-12-10T14:42:31.450141: step 3107, loss 0.067111, acc 0.96875, prec 0.0535999, recall 0.862236
2017-12-10T14:42:31.641123: step 3108, loss 0.177021, acc 0.9375, prec 0.0536156, recall 0.862284
2017-12-10T14:42:31.829979: step 3109, loss 0.161767, acc 0.953125, prec 0.0536122, recall 0.862284
2017-12-10T14:42:32.017214: step 3110, loss 0.150177, acc 0.984375, prec 0.0536314, recall 0.862331
2017-12-10T14:42:32.205818: step 3111, loss 0.265506, acc 0.890625, prec 0.053664, recall 0.862427
2017-12-10T14:42:32.393375: step 3112, loss 3.65945, acc 0.90625, prec 0.0536582, recall 0.862129
2017-12-10T14:42:32.582535: step 3113, loss 0.301403, acc 0.984375, prec 0.0536978, recall 0.862224
2017-12-10T14:42:32.773924: step 3114, loss 0.180793, acc 0.984375, prec 0.053717, recall 0.862271
2017-12-10T14:42:32.962678: step 3115, loss 0.171062, acc 0.9375, prec 0.053753, recall 0.862366
2017-12-10T14:42:33.152679: step 3116, loss 0.155024, acc 0.96875, prec 0.0537914, recall 0.862461
2017-12-10T14:42:33.339578: step 3117, loss 0.289745, acc 0.9375, prec 0.0538071, recall 0.862509
2017-12-10T14:42:33.529656: step 3118, loss 0.598028, acc 0.9375, prec 0.0538432, recall 0.862603
2017-12-10T14:42:33.716537: step 3119, loss 0.418535, acc 0.890625, prec 0.0538554, recall 0.862651
2017-12-10T14:42:33.902882: step 3120, loss 0.166752, acc 0.921875, prec 0.0538496, recall 0.862651
2017-12-10T14:42:34.089803: step 3121, loss 0.277159, acc 0.90625, prec 0.053863, recall 0.862698
2017-12-10T14:42:34.277408: step 3122, loss 0.525341, acc 0.921875, prec 0.0539182, recall 0.862839
2017-12-10T14:42:34.465869: step 3123, loss 0.47564, acc 0.84375, prec 0.0539473, recall 0.862934
2017-12-10T14:42:34.652181: step 3124, loss 0.477086, acc 0.84375, prec 0.0539357, recall 0.862934
2017-12-10T14:42:34.836313: step 3125, loss 0.257379, acc 0.90625, prec 0.053949, recall 0.862981
2017-12-10T14:42:35.025387: step 3126, loss 0.425712, acc 0.890625, prec 0.0539409, recall 0.862981
2017-12-10T14:42:35.209765: step 3127, loss 0.232607, acc 0.921875, prec 0.0539757, recall 0.863075
2017-12-10T14:42:35.394984: step 3128, loss 0.479173, acc 0.875, prec 0.0539868, recall 0.863122
2017-12-10T14:42:35.580127: step 3129, loss 0.26361, acc 0.921875, prec 0.053981, recall 0.863122
2017-12-10T14:42:35.766737: step 3130, loss 0.402501, acc 0.890625, prec 0.0539932, recall 0.863169
2017-12-10T14:42:35.952632: step 3131, loss 0.360841, acc 0.8125, prec 0.0540199, recall 0.863263
2017-12-10T14:42:36.138875: step 3132, loss 0.341788, acc 0.953125, prec 0.0540569, recall 0.863356
2017-12-10T14:42:36.325048: step 3133, loss 0.243639, acc 0.90625, prec 0.0540906, recall 0.86345
2017-12-10T14:42:36.509529: step 3134, loss 0.257726, acc 0.890625, prec 0.0540824, recall 0.86345
2017-12-10T14:42:36.694507: step 3135, loss 0.335535, acc 0.90625, prec 0.0540958, recall 0.863496
2017-12-10T14:42:36.888848: step 3136, loss 0.351538, acc 0.9375, prec 0.0541114, recall 0.863543
2017-12-10T14:42:37.075253: step 3137, loss 0.313549, acc 0.90625, prec 0.0541247, recall 0.86359
2017-12-10T14:42:37.264804: step 3138, loss 0.160155, acc 0.96875, prec 0.0541427, recall 0.863636
2017-12-10T14:42:37.453284: step 3139, loss 0.264579, acc 0.9375, prec 0.0541583, recall 0.863683
2017-12-10T14:42:37.644068: step 3140, loss 0.132486, acc 0.96875, prec 0.054156, recall 0.863683
2017-12-10T14:42:37.835682: step 3141, loss 0.0743265, acc 0.96875, prec 0.0541536, recall 0.863683
2017-12-10T14:42:38.024782: step 3142, loss 0.0910877, acc 0.953125, prec 0.0541502, recall 0.863683
2017-12-10T14:42:38.214585: step 3143, loss 0.167265, acc 0.921875, prec 0.0541444, recall 0.863683
2017-12-10T14:42:38.402241: step 3144, loss 0.0759402, acc 0.984375, prec 0.0541432, recall 0.863683
2017-12-10T14:42:38.590247: step 3145, loss 0.268788, acc 0.953125, prec 0.0541802, recall 0.863776
2017-12-10T14:42:38.781460: step 3146, loss 0.0945653, acc 0.96875, prec 0.0541982, recall 0.863823
2017-12-10T14:42:38.966620: step 3147, loss 0.183595, acc 0.953125, prec 0.0541947, recall 0.863823
2017-12-10T14:42:39.155359: step 3148, loss 0.21574, acc 0.9375, prec 0.05419, recall 0.863823
2017-12-10T14:42:39.341462: step 3149, loss 0.0375309, acc 1, prec 0.0542103, recall 0.863869
2017-12-10T14:42:39.530326: step 3150, loss 0.0486234, acc 0.96875, prec 0.054208, recall 0.863869
2017-12-10T14:42:39.720902: step 3151, loss 0.0324247, acc 0.96875, prec 0.0542056, recall 0.863869
2017-12-10T14:42:39.909351: step 3152, loss 0.112999, acc 0.96875, prec 0.0542033, recall 0.863869
2017-12-10T14:42:40.094792: step 3153, loss 0.125073, acc 0.984375, prec 0.0542224, recall 0.863915
2017-12-10T14:42:40.283176: step 3154, loss 0.062489, acc 0.984375, prec 0.0542617, recall 0.864008
2017-12-10T14:42:40.468511: step 3155, loss 0.100652, acc 0.984375, prec 0.0542606, recall 0.864008
2017-12-10T14:42:40.654182: step 3156, loss 0.0325454, acc 0.984375, prec 0.0542797, recall 0.864055
2017-12-10T14:42:40.843587: step 3157, loss 0.00885292, acc 1, prec 0.0542797, recall 0.864055
2017-12-10T14:42:41.032373: step 3158, loss 1.94287, acc 0.953125, prec 0.0542773, recall 0.86376
2017-12-10T14:42:41.229707: step 3159, loss 0.275358, acc 0.953125, prec 0.0542941, recall 0.863807
2017-12-10T14:42:41.417557: step 3160, loss 0.209446, acc 0.953125, prec 0.0542906, recall 0.863807
2017-12-10T14:42:41.603856: step 3161, loss 0.17093, acc 0.96875, prec 0.0543085, recall 0.863853
2017-12-10T14:42:41.791474: step 3162, loss 0.121991, acc 0.953125, prec 0.054305, recall 0.863853
2017-12-10T14:42:41.979908: step 3163, loss 0.224923, acc 0.9375, prec 0.0543206, recall 0.863899
2017-12-10T14:42:42.171025: step 3164, loss 0.577065, acc 1, prec 0.054442, recall 0.864177
2017-12-10T14:42:42.359549: step 3165, loss 0.0941701, acc 0.984375, prec 0.0544813, recall 0.864269
2017-12-10T14:42:42.544501: step 3166, loss 0.302057, acc 0.90625, prec 0.0544945, recall 0.864315
2017-12-10T14:42:42.734328: step 3167, loss 0.162545, acc 0.953125, prec 0.054491, recall 0.864315
2017-12-10T14:42:42.924800: step 3168, loss 0.15125, acc 0.953125, prec 0.0544875, recall 0.864315
2017-12-10T14:42:43.115323: step 3169, loss 0.175501, acc 0.9375, prec 0.0545233, recall 0.864407
2017-12-10T14:42:43.302810: step 3170, loss 0.187678, acc 0.9375, prec 0.0545793, recall 0.864545
2017-12-10T14:42:43.489605: step 3171, loss 0.804438, acc 0.921875, prec 0.0546543, recall 0.864728
2017-12-10T14:42:43.677496: step 3172, loss 0.58476, acc 0.96875, prec 0.0546923, recall 0.864819
2017-12-10T14:42:43.865206: step 3173, loss 0.123308, acc 0.984375, prec 0.0547114, recall 0.864865
2017-12-10T14:42:44.060293: step 3174, loss 0.254514, acc 0.90625, prec 0.0547044, recall 0.864865
2017-12-10T14:42:44.252375: step 3175, loss 0.149462, acc 0.921875, prec 0.0547187, recall 0.86491
2017-12-10T14:42:44.440429: step 3176, loss 0.233068, acc 0.921875, prec 0.0547331, recall 0.864956
2017-12-10T14:42:44.628183: step 3177, loss 0.199345, acc 0.96875, prec 0.0547509, recall 0.865002
2017-12-10T14:42:44.812507: step 3178, loss 0.571416, acc 0.921875, prec 0.0547653, recall 0.865047
2017-12-10T14:42:44.998256: step 3179, loss 0.452772, acc 0.890625, prec 0.0547974, recall 0.865138
2017-12-10T14:42:45.185664: step 3180, loss 0.203044, acc 0.953125, prec 0.0547939, recall 0.865138
2017-12-10T14:42:45.372421: step 3181, loss 0.329494, acc 0.890625, prec 0.0548059, recall 0.865184
2017-12-10T14:42:45.558683: step 3182, loss 0.282274, acc 0.921875, prec 0.0548001, recall 0.865184
2017-12-10T14:42:45.750808: step 3183, loss 0.10619, acc 0.953125, prec 0.0548167, recall 0.865229
2017-12-10T14:42:45.936857: step 3184, loss 0.34908, acc 0.921875, prec 0.0548512, recall 0.86532
2017-12-10T14:42:46.122100: step 3185, loss 0.566017, acc 0.90625, prec 0.0548644, recall 0.865365
2017-12-10T14:42:46.313601: step 3186, loss 3.12814, acc 0.875, prec 0.0548764, recall 0.865119
2017-12-10T14:42:46.504987: step 3187, loss 0.371564, acc 0.8125, prec 0.0548825, recall 0.865165
2017-12-10T14:42:46.695329: step 3188, loss 0.141967, acc 0.953125, prec 0.0549193, recall 0.865255
2017-12-10T14:42:46.881368: step 3189, loss 0.262465, acc 0.921875, prec 0.0549336, recall 0.865301
2017-12-10T14:42:47.068605: step 3190, loss 0.187173, acc 0.96875, prec 0.0549715, recall 0.865391
2017-12-10T14:42:47.253089: step 3191, loss 0.329718, acc 0.828125, prec 0.0550191, recall 0.865526
2017-12-10T14:42:47.438402: step 3192, loss 0.376775, acc 0.859375, prec 0.0550085, recall 0.865526
2017-12-10T14:42:47.625577: step 3193, loss 0.429262, acc 0.78125, prec 0.0550123, recall 0.865572
2017-12-10T14:42:47.810953: step 3194, loss 0.513349, acc 0.828125, prec 0.0549994, recall 0.865572
2017-12-10T14:42:47.997415: step 3195, loss 0.444325, acc 0.8125, prec 0.0550256, recall 0.865662
2017-12-10T14:42:48.182307: step 3196, loss 0.162206, acc 0.90625, prec 0.0550386, recall 0.865707
2017-12-10T14:42:48.367894: step 3197, loss 0.359249, acc 0.90625, prec 0.0550517, recall 0.865752
2017-12-10T14:42:48.554664: step 3198, loss 0.324422, acc 0.875, prec 0.0550424, recall 0.865752
2017-12-10T14:42:48.742230: step 3199, loss 0.356618, acc 0.890625, prec 0.0550543, recall 0.865797
2017-12-10T14:42:48.929888: step 3200, loss 0.310289, acc 0.921875, prec 0.0550685, recall 0.865841
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_3/1512934336/checkpoints/model-3200

2017-12-10T14:42:50.104389: step 3201, loss 0.173982, acc 0.921875, prec 0.0550828, recall 0.865886
2017-12-10T14:42:50.290489: step 3202, loss 0.229814, acc 0.9375, prec 0.0550982, recall 0.865931
2017-12-10T14:42:50.478477: step 3203, loss 0.112562, acc 0.9375, prec 0.0551337, recall 0.866021
2017-12-10T14:42:50.669334: step 3204, loss 0.108099, acc 0.96875, prec 0.0551715, recall 0.86611
2017-12-10T14:42:50.857666: step 3205, loss 0.0995115, acc 0.96875, prec 0.0551893, recall 0.866155
2017-12-10T14:42:51.048117: step 3206, loss 0.298041, acc 0.984375, prec 0.0552283, recall 0.866244
2017-12-10T14:42:51.235483: step 3207, loss 0.121586, acc 0.953125, prec 0.0552248, recall 0.866244
2017-12-10T14:42:51.419425: step 3208, loss 0.244187, acc 0.9375, prec 0.0552201, recall 0.866244
2017-12-10T14:42:51.608389: step 3209, loss 0.0960306, acc 0.953125, prec 0.0552366, recall 0.866289
2017-12-10T14:42:51.793897: step 3210, loss 1.34672, acc 0.96875, prec 0.0552355, recall 0.866
2017-12-10T14:42:51.983740: step 3211, loss 0.042335, acc 0.984375, prec 0.0552343, recall 0.866
2017-12-10T14:42:52.171222: step 3212, loss 0.367181, acc 0.875, prec 0.055245, recall 0.866045
2017-12-10T14:42:52.358435: step 3213, loss 0.0443562, acc 0.984375, prec 0.0552438, recall 0.866045
2017-12-10T14:42:52.542367: step 3214, loss 0.0355377, acc 0.984375, prec 0.0552426, recall 0.866045
2017-12-10T14:42:52.730190: step 3215, loss 0.172498, acc 0.984375, prec 0.0552415, recall 0.866045
2017-12-10T14:42:52.917294: step 3216, loss 0.190734, acc 0.90625, prec 0.0552545, recall 0.866089
2017-12-10T14:42:53.108282: step 3217, loss 0.309067, acc 0.90625, prec 0.0552675, recall 0.866134
2017-12-10T14:42:53.294300: step 3218, loss 0.0494042, acc 0.984375, prec 0.0552663, recall 0.866134
2017-12-10T14:42:53.488252: step 3219, loss 0.11211, acc 0.953125, prec 0.0552829, recall 0.866178
2017-12-10T14:42:53.676059: step 3220, loss 0.0661159, acc 0.984375, prec 0.0552817, recall 0.866178
2017-12-10T14:42:53.864254: step 3221, loss 0.344096, acc 0.96875, prec 0.0552994, recall 0.866223
2017-12-10T14:42:54.050343: step 3222, loss 0.155125, acc 0.9375, prec 0.0553349, recall 0.866312
2017-12-10T14:42:54.241393: step 3223, loss 0.137942, acc 0.953125, prec 0.0553514, recall 0.866356
2017-12-10T14:42:54.428696: step 3224, loss 0.095752, acc 0.953125, prec 0.0553479, recall 0.866356
2017-12-10T14:42:54.616896: step 3225, loss 0.0721214, acc 0.984375, prec 0.0553467, recall 0.866356
2017-12-10T14:42:54.801977: step 3226, loss 0.136356, acc 0.90625, prec 0.0553397, recall 0.866356
2017-12-10T14:42:54.992568: step 3227, loss 0.199219, acc 0.96875, prec 0.0553574, recall 0.866401
2017-12-10T14:42:55.181330: step 3228, loss 0.298068, acc 0.9375, prec 0.0553928, recall 0.86649
2017-12-10T14:42:55.367465: step 3229, loss 0.082327, acc 0.9375, prec 0.0554081, recall 0.866534
2017-12-10T14:42:55.553600: step 3230, loss 0.123351, acc 0.953125, prec 0.0554247, recall 0.866578
2017-12-10T14:42:55.741075: step 3231, loss 0.253701, acc 0.96875, prec 0.0554824, recall 0.866711
2017-12-10T14:42:55.931186: step 3232, loss 0.151984, acc 0.96875, prec 0.0555202, recall 0.866799
2017-12-10T14:42:56.122147: step 3233, loss 0.192554, acc 0.96875, prec 0.0555379, recall 0.866843
2017-12-10T14:42:56.308709: step 3234, loss 0.118168, acc 0.953125, prec 0.0555343, recall 0.866843
2017-12-10T14:42:56.495399: step 3235, loss 0.128252, acc 0.953125, prec 0.0555308, recall 0.866843
2017-12-10T14:42:56.683481: step 3236, loss 0.0318055, acc 0.984375, prec 0.0555296, recall 0.866843
2017-12-10T14:42:56.868098: step 3237, loss 0.0756818, acc 0.984375, prec 0.0555485, recall 0.866887
2017-12-10T14:42:57.053212: step 3238, loss 0.081671, acc 0.984375, prec 0.0555673, recall 0.866931
2017-12-10T14:42:57.239780: step 3239, loss 0.227818, acc 0.953125, prec 0.0555838, recall 0.866975
2017-12-10T14:42:57.426769: step 3240, loss 0.0221058, acc 1, prec 0.0555838, recall 0.866975
2017-12-10T14:42:57.614180: step 3241, loss 2.83543, acc 0.984375, prec 0.0556039, recall 0.866733
2017-12-10T14:42:57.803624: step 3242, loss 1.06589, acc 0.953125, prec 0.0556204, recall 0.866777
2017-12-10T14:42:57.994062: step 3243, loss 0.0856944, acc 0.96875, prec 0.055618, recall 0.866777
2017-12-10T14:42:58.180818: step 3244, loss 0.0444858, acc 0.96875, prec 0.0556357, recall 0.866821
2017-12-10T14:42:58.368507: step 3245, loss 0.31486, acc 0.96875, prec 0.0556734, recall 0.866909
2017-12-10T14:42:58.557975: step 3246, loss 0.144115, acc 0.953125, prec 0.0556698, recall 0.866909
2017-12-10T14:42:58.746663: step 3247, loss 0.238281, acc 0.890625, prec 0.0556816, recall 0.866953
2017-12-10T14:42:58.935870: step 3248, loss 0.160343, acc 0.96875, prec 0.0556792, recall 0.866953
2017-12-10T14:42:59.132244: step 3249, loss 0.67001, acc 0.828125, prec 0.0557263, recall 0.867084
2017-12-10T14:42:59.316997: step 3250, loss 0.50212, acc 0.875, prec 0.0557369, recall 0.867128
2017-12-10T14:42:59.503395: step 3251, loss 0.555713, acc 0.796875, prec 0.0557415, recall 0.867172
2017-12-10T14:42:59.691774: step 3252, loss 0.340733, acc 0.875, prec 0.0557321, recall 0.867172
2017-12-10T14:42:59.875531: step 3253, loss 0.763716, acc 0.859375, prec 0.0557415, recall 0.867216
2017-12-10T14:43:00.063657: step 3254, loss 0.559889, acc 0.765625, prec 0.0557238, recall 0.867216
2017-12-10T14:43:00.252001: step 3255, loss 0.337377, acc 0.859375, prec 0.0557331, recall 0.86726
2017-12-10T14:43:00.440347: step 3256, loss 0.795375, acc 0.78125, prec 0.0557366, recall 0.867303
2017-12-10T14:43:00.626218: step 3257, loss 0.404096, acc 0.84375, prec 0.0557248, recall 0.867303
2017-12-10T14:43:00.810470: step 3258, loss 0.441633, acc 0.890625, prec 0.0557365, recall 0.867347
2017-12-10T14:43:00.998420: step 3259, loss 0.432017, acc 0.890625, prec 0.0557682, recall 0.867434
2017-12-10T14:43:01.188854: step 3260, loss 0.23481, acc 0.90625, prec 0.0557811, recall 0.867478
2017-12-10T14:43:01.377450: step 3261, loss 0.405412, acc 0.921875, prec 0.0557952, recall 0.867521
2017-12-10T14:43:01.570328: step 3262, loss 0.205948, acc 0.9375, prec 0.0558104, recall 0.867565
2017-12-10T14:43:01.758608: step 3263, loss 0.202333, acc 0.9375, prec 0.0558057, recall 0.867565
2017-12-10T14:43:01.946048: step 3264, loss 0.144628, acc 0.9375, prec 0.0558209, recall 0.867608
2017-12-10T14:43:02.137502: step 3265, loss 0.378004, acc 0.90625, prec 0.0558338, recall 0.867652
2017-12-10T14:43:02.327491: step 3266, loss 1.15888, acc 0.859375, prec 0.0558431, recall 0.867695
2017-12-10T14:43:02.518614: step 3267, loss 0.261125, acc 0.90625, prec 0.055876, recall 0.867782
2017-12-10T14:43:02.703170: step 3268, loss 0.185601, acc 0.953125, prec 0.0559522, recall 0.867955
2017-12-10T14:43:02.890518: step 3269, loss 0.121719, acc 0.953125, prec 0.0559486, recall 0.867955
2017-12-10T14:43:03.077202: step 3270, loss 0.42525, acc 0.9375, prec 0.0560236, recall 0.868128
2017-12-10T14:43:03.267601: step 3271, loss 0.137526, acc 0.96875, prec 0.0560412, recall 0.868171
2017-12-10T14:43:03.453938: step 3272, loss 0.0760045, acc 0.953125, prec 0.0560377, recall 0.868171
2017-12-10T14:43:03.638011: step 3273, loss 0.111796, acc 0.953125, prec 0.056054, recall 0.868215
2017-12-10T14:43:03.825996: step 3274, loss 0.170888, acc 0.9375, prec 0.0560493, recall 0.868215
2017-12-10T14:43:04.013084: step 3275, loss 0.0669995, acc 0.96875, prec 0.0560469, recall 0.868215
2017-12-10T14:43:04.200812: step 3276, loss 7.892, acc 0.9375, prec 0.0560832, recall 0.868017
2017-12-10T14:43:04.392843: step 3277, loss 0.22147, acc 0.9375, prec 0.0561184, recall 0.868103
2017-12-10T14:43:04.582032: step 3278, loss 0.162645, acc 0.96875, prec 0.0561359, recall 0.868146
2017-12-10T14:43:04.766112: step 3279, loss 0.0783676, acc 0.984375, prec 0.0561546, recall 0.868189
2017-12-10T14:43:04.955146: step 3280, loss 0.210363, acc 0.953125, prec 0.056171, recall 0.868232
2017-12-10T14:43:05.142445: step 3281, loss 0.188464, acc 0.921875, prec 0.0562049, recall 0.868318
2017-12-10T14:43:05.328098: step 3282, loss 0.581318, acc 0.859375, prec 0.0561942, recall 0.868318
2017-12-10T14:43:05.516870: step 3283, loss 0.419152, acc 0.890625, prec 0.0561859, recall 0.868318
2017-12-10T14:43:05.703255: step 3284, loss 0.426202, acc 0.875, prec 0.0561765, recall 0.868318
2017-12-10T14:43:05.888682: step 3285, loss 0.573061, acc 0.96875, prec 0.056194, recall 0.868361
2017-12-10T14:43:06.077744: step 3286, loss 0.571336, acc 0.90625, prec 0.0562267, recall 0.868447
2017-12-10T14:43:06.268947: step 3287, loss 0.295847, acc 0.875, prec 0.0562172, recall 0.868447
2017-12-10T14:43:06.454539: step 3288, loss 0.24179, acc 0.90625, prec 0.05623, recall 0.86849
2017-12-10T14:43:06.641380: step 3289, loss 0.414653, acc 0.90625, prec 0.0562229, recall 0.86849
2017-12-10T14:43:06.825746: step 3290, loss 0.252569, acc 0.890625, prec 0.0562146, recall 0.86849
2017-12-10T14:43:07.012417: step 3291, loss 0.331488, acc 0.90625, prec 0.0562075, recall 0.86849
2017-12-10T14:43:07.195563: step 3292, loss 0.229726, acc 0.859375, prec 0.0561968, recall 0.86849
2017-12-10T14:43:07.382435: step 3293, loss 0.317517, acc 0.90625, prec 0.0562295, recall 0.868575
2017-12-10T14:43:07.569045: step 3294, loss 0.384423, acc 0.90625, prec 0.0562422, recall 0.868618
2017-12-10T14:43:07.754135: step 3295, loss 0.734675, acc 0.859375, prec 0.056311, recall 0.868789
2017-12-10T14:43:07.941466: step 3296, loss 0.228892, acc 0.9375, prec 0.0563262, recall 0.868831
2017-12-10T14:43:08.134455: step 3297, loss 0.337386, acc 0.9375, prec 0.0563611, recall 0.868916
2017-12-10T14:43:08.324960: step 3298, loss 0.0793439, acc 0.96875, prec 0.0563985, recall 0.869001
2017-12-10T14:43:08.513103: step 3299, loss 0.410465, acc 0.890625, prec 0.0563902, recall 0.869001
2017-12-10T14:43:08.701413: step 3300, loss 0.38147, acc 0.921875, prec 0.0564041, recall 0.869044
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_3/1512934336/checkpoints/model-3300

2017-12-10T14:43:10.196181: step 3301, loss 0.131376, acc 0.953125, prec 0.0564601, recall 0.869171
2017-12-10T14:43:10.380137: step 3302, loss 0.128529, acc 0.921875, prec 0.056474, recall 0.869213
2017-12-10T14:43:10.567656: step 3303, loss 0.329746, acc 0.90625, prec 0.0564867, recall 0.869256
2017-12-10T14:43:10.755076: step 3304, loss 0.172445, acc 0.890625, prec 0.0564982, recall 0.869298
2017-12-10T14:43:10.940439: step 3305, loss 0.889811, acc 0.953125, prec 0.0565542, recall 0.869425
2017-12-10T14:43:11.130236: step 3306, loss 0.0644467, acc 0.96875, prec 0.0565518, recall 0.869425
2017-12-10T14:43:11.316166: step 3307, loss 0.216618, acc 0.921875, prec 0.0565459, recall 0.869425
2017-12-10T14:43:11.502314: step 3308, loss 0.386601, acc 0.890625, prec 0.0565375, recall 0.869425
2017-12-10T14:43:11.688495: step 3309, loss 0.267777, acc 0.953125, prec 0.0565538, recall 0.869467
2017-12-10T14:43:11.873620: step 3310, loss 0.132813, acc 0.953125, prec 0.0565701, recall 0.869509
2017-12-10T14:43:12.062322: step 3311, loss 3.54371, acc 0.890625, prec 0.0566026, recall 0.869313
2017-12-10T14:43:12.252000: step 3312, loss 0.254684, acc 0.90625, prec 0.0566153, recall 0.869355
2017-12-10T14:43:12.440091: step 3313, loss 0.170406, acc 0.921875, prec 0.056649, recall 0.869439
2017-12-10T14:43:12.626104: step 3314, loss 2.12538, acc 0.890625, prec 0.0566418, recall 0.869159
2017-12-10T14:43:12.811779: step 3315, loss 0.570162, acc 0.890625, prec 0.0566335, recall 0.869159
2017-12-10T14:43:12.997963: step 3316, loss 0.314182, acc 0.890625, prec 0.056645, recall 0.869201
2017-12-10T14:43:13.186642: step 3317, loss 0.250721, acc 0.921875, prec 0.0566786, recall 0.869285
2017-12-10T14:43:13.372118: step 3318, loss 0.333145, acc 0.921875, prec 0.0566727, recall 0.869285
2017-12-10T14:43:13.560425: step 3319, loss 0.237821, acc 0.9375, prec 0.0566679, recall 0.869285
2017-12-10T14:43:13.746542: step 3320, loss 0.690506, acc 0.828125, prec 0.0566944, recall 0.869369
2017-12-10T14:43:13.933786: step 3321, loss 0.680795, acc 0.765625, prec 0.0566766, recall 0.869369
2017-12-10T14:43:14.127941: step 3322, loss 0.60876, acc 0.828125, prec 0.0566635, recall 0.869369
2017-12-10T14:43:14.319223: step 3323, loss 0.821847, acc 0.828125, prec 0.0566505, recall 0.869369
2017-12-10T14:43:14.508624: step 3324, loss 0.42609, acc 0.921875, prec 0.0566445, recall 0.869369
2017-12-10T14:43:14.695908: step 3325, loss 0.601143, acc 0.828125, prec 0.056671, recall 0.869453
2017-12-10T14:43:14.883778: step 3326, loss 0.22546, acc 0.890625, prec 0.0566825, recall 0.869495
2017-12-10T14:43:15.070136: step 3327, loss 0.391085, acc 0.859375, prec 0.0566915, recall 0.869537
2017-12-10T14:43:15.255771: step 3328, loss 0.24227, acc 0.90625, prec 0.0567239, recall 0.869621
2017-12-10T14:43:15.445912: step 3329, loss 0.508847, acc 0.875, prec 0.0567144, recall 0.869621
2017-12-10T14:43:15.630557: step 3330, loss 0.287693, acc 0.875, prec 0.0567049, recall 0.869621
2017-12-10T14:43:15.815588: step 3331, loss 1.48144, acc 0.875, prec 0.0567164, recall 0.869384
2017-12-10T14:43:16.008057: step 3332, loss 0.282703, acc 0.9375, prec 0.0567116, recall 0.869384
2017-12-10T14:43:16.200780: step 3333, loss 0.398827, acc 0.90625, prec 0.056744, recall 0.869468
2017-12-10T14:43:16.391805: step 3334, loss 0.250445, acc 0.921875, prec 0.056738, recall 0.869468
2017-12-10T14:43:16.581833: step 3335, loss 0.281628, acc 0.96875, prec 0.0567357, recall 0.869468
2017-12-10T14:43:16.769727: step 3336, loss 0.401017, acc 0.890625, prec 0.0567471, recall 0.869509
2017-12-10T14:43:16.954501: step 3337, loss 3.2604, acc 0.953125, prec 0.0567447, recall 0.869231
2017-12-10T14:43:17.143530: step 3338, loss 0.336378, acc 0.90625, prec 0.0567573, recall 0.869273
2017-12-10T14:43:17.337680: step 3339, loss 0.127291, acc 0.9375, prec 0.0567526, recall 0.869273
2017-12-10T14:43:17.523824: step 3340, loss 0.327991, acc 0.890625, prec 0.0567443, recall 0.869273
2017-12-10T14:43:17.709384: step 3341, loss 0.292607, acc 0.90625, prec 0.0567371, recall 0.869273
2017-12-10T14:43:17.894944: step 3342, loss 0.505233, acc 0.875, prec 0.0567474, recall 0.869315
2017-12-10T14:43:18.082426: step 3343, loss 0.223238, acc 0.90625, prec 0.0567403, recall 0.869315
2017-12-10T14:43:18.269085: step 3344, loss 0.817444, acc 0.875, prec 0.0567702, recall 0.869398
2017-12-10T14:43:18.460225: step 3345, loss 0.331853, acc 0.875, prec 0.0567607, recall 0.869398
2017-12-10T14:43:18.649853: step 3346, loss 0.337636, acc 0.921875, prec 0.0567548, recall 0.869398
2017-12-10T14:43:18.839337: step 3347, loss 0.602954, acc 0.78125, prec 0.0567382, recall 0.869398
2017-12-10T14:43:19.023596: step 3348, loss 0.322094, acc 0.921875, prec 0.0567914, recall 0.869524
2017-12-10T14:43:19.213345: step 3349, loss 0.41871, acc 0.859375, prec 0.0568004, recall 0.869565
2017-12-10T14:43:19.399333: step 3350, loss 0.243788, acc 0.953125, prec 0.0568165, recall 0.869607
2017-12-10T14:43:19.585407: step 3351, loss 0.268187, acc 0.875, prec 0.0568267, recall 0.869649
2017-12-10T14:43:19.775446: step 3352, loss 0.35071, acc 0.859375, prec 0.056816, recall 0.869649
2017-12-10T14:43:19.964102: step 3353, loss 0.402149, acc 0.890625, prec 0.0568274, recall 0.86969
2017-12-10T14:43:20.151468: step 3354, loss 1.16673, acc 0.953125, prec 0.0568251, recall 0.869413
2017-12-10T14:43:20.339605: step 3355, loss 0.126352, acc 0.921875, prec 0.0568388, recall 0.869454
2017-12-10T14:43:20.526151: step 3356, loss 0.323887, acc 0.90625, prec 0.056871, recall 0.869537
2017-12-10T14:43:20.714396: step 3357, loss 0.0918691, acc 0.96875, prec 0.056908, recall 0.869621
2017-12-10T14:43:20.899118: step 3358, loss 0.474143, acc 0.890625, prec 0.0569194, recall 0.869662
2017-12-10T14:43:21.086822: step 3359, loss 0.369745, acc 0.90625, prec 0.0569123, recall 0.869662
2017-12-10T14:43:21.275286: step 3360, loss 0.125379, acc 0.953125, prec 0.0569087, recall 0.869662
2017-12-10T14:43:21.464685: step 3361, loss 0.365419, acc 0.921875, prec 0.0569028, recall 0.869662
2017-12-10T14:43:21.648703: step 3362, loss 0.201305, acc 0.953125, prec 0.0569582, recall 0.869787
2017-12-10T14:43:21.835770: step 3363, loss 0.197218, acc 0.921875, prec 0.0569719, recall 0.869828
2017-12-10T14:43:22.021402: step 3364, loss 0.411469, acc 0.921875, prec 0.0570053, recall 0.869911
2017-12-10T14:43:22.206967: step 3365, loss 0.304119, acc 0.890625, prec 0.056997, recall 0.869911
2017-12-10T14:43:22.398790: step 3366, loss 0.155593, acc 0.90625, prec 0.0570095, recall 0.869952
2017-12-10T14:43:22.583316: step 3367, loss 0.557753, acc 0.921875, prec 0.0570429, recall 0.870035
2017-12-10T14:43:22.774206: step 3368, loss 0.480115, acc 0.875, prec 0.0570333, recall 0.870035
2017-12-10T14:43:22.960517: step 3369, loss 0.21131, acc 0.9375, prec 0.0570286, recall 0.870035
2017-12-10T14:43:23.149642: step 3370, loss 0.297724, acc 0.90625, prec 0.0570215, recall 0.870035
2017-12-10T14:43:23.333577: step 3371, loss 0.107965, acc 0.953125, prec 0.0570179, recall 0.870035
2017-12-10T14:43:23.520138: step 3372, loss 0.365212, acc 0.921875, prec 0.0570709, recall 0.870159
2017-12-10T14:43:23.705896: step 3373, loss 0.268042, acc 0.9375, prec 0.0570661, recall 0.870159
2017-12-10T14:43:23.894409: step 3374, loss 0.364525, acc 0.890625, prec 0.0570578, recall 0.870159
2017-12-10T14:43:24.082882: step 3375, loss 0.254397, acc 0.9375, prec 0.0570531, recall 0.870159
2017-12-10T14:43:24.271253: step 3376, loss 0.355123, acc 0.90625, prec 0.0570459, recall 0.870159
2017-12-10T14:43:24.458959: step 3377, loss 0.150355, acc 0.953125, prec 0.0570424, recall 0.870159
2017-12-10T14:43:24.646483: step 3378, loss 0.408473, acc 0.96875, prec 0.0570596, recall 0.8702
2017-12-10T14:43:24.836080: step 3379, loss 0.0984644, acc 0.953125, prec 0.0570561, recall 0.8702
2017-12-10T14:43:25.021109: step 3380, loss 0.104535, acc 0.96875, prec 0.0570733, recall 0.870241
2017-12-10T14:43:25.207523: step 3381, loss 0.0965785, acc 0.96875, prec 0.0570709, recall 0.870241
2017-12-10T14:43:25.394070: step 3382, loss 0.219676, acc 0.9375, prec 0.0570662, recall 0.870241
2017-12-10T14:43:25.581312: step 3383, loss 0.302992, acc 0.9375, prec 0.0570614, recall 0.870241
2017-12-10T14:43:25.766662: step 3384, loss 0.00527718, acc 1, prec 0.0570614, recall 0.870241
2017-12-10T14:43:25.952780: step 3385, loss 0.201417, acc 0.953125, prec 0.0570775, recall 0.870282
2017-12-10T14:43:26.140184: step 3386, loss 1.53254, acc 0.953125, prec 0.0570947, recall 0.870048
2017-12-10T14:43:26.333048: step 3387, loss 0.514335, acc 0.96875, prec 0.0571316, recall 0.87013
2017-12-10T14:43:26.524595: step 3388, loss 0.272329, acc 0.9375, prec 0.0571464, recall 0.870171
2017-12-10T14:43:26.712806: step 3389, loss 0.122785, acc 0.96875, prec 0.0572029, recall 0.870294
2017-12-10T14:43:26.901621: step 3390, loss 1.87724, acc 0.96875, prec 0.0572017, recall 0.870019
2017-12-10T14:43:27.090586: step 3391, loss 0.623971, acc 0.90625, prec 0.0572141, recall 0.87006
2017-12-10T14:43:27.276622: step 3392, loss 0.107222, acc 0.953125, prec 0.0572106, recall 0.87006
2017-12-10T14:43:27.466201: step 3393, loss 0.272926, acc 0.921875, prec 0.0572242, recall 0.870101
2017-12-10T14:43:27.652408: step 3394, loss 0.227217, acc 0.9375, prec 0.0572587, recall 0.870183
2017-12-10T14:43:27.838954: step 3395, loss 0.44431, acc 0.890625, prec 0.0572503, recall 0.870183
2017-12-10T14:43:28.024631: step 3396, loss 0.321126, acc 0.890625, prec 0.0572812, recall 0.870265
2017-12-10T14:43:28.210993: step 3397, loss 0.450516, acc 0.8125, prec 0.0572865, recall 0.870306
2017-12-10T14:43:28.400556: step 3398, loss 0.109638, acc 0.9375, prec 0.0572817, recall 0.870306
2017-12-10T14:43:28.585899: step 3399, loss 0.088891, acc 0.953125, prec 0.0572781, recall 0.870306
2017-12-10T14:43:28.773850: step 3400, loss 0.618411, acc 0.765625, prec 0.0572603, recall 0.870306
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_3/1512934336/checkpoints/model-3400

2017-12-10T14:43:29.988088: step 3401, loss 0.356991, acc 0.84375, prec 0.0572484, recall 0.870306
2017-12-10T14:43:30.171489: step 3402, loss 0.564104, acc 0.90625, prec 0.0572804, recall 0.870388
2017-12-10T14:43:30.359403: step 3403, loss 0.383374, acc 0.921875, prec 0.0573136, recall 0.87047
2017-12-10T14:43:30.545311: step 3404, loss 0.729473, acc 0.765625, prec 0.0573349, recall 0.870551
2017-12-10T14:43:30.733066: step 3405, loss 0.3026, acc 0.90625, prec 0.0573277, recall 0.870551
2017-12-10T14:43:30.920686: step 3406, loss 0.585876, acc 0.84375, prec 0.0573354, recall 0.870592
2017-12-10T14:43:31.108300: step 3407, loss 0.540374, acc 0.828125, prec 0.0573419, recall 0.870633
2017-12-10T14:43:31.299371: step 3408, loss 0.41849, acc 0.9375, prec 0.0573567, recall 0.870673
2017-12-10T14:43:31.488320: step 3409, loss 0.230035, acc 0.921875, prec 0.0573703, recall 0.870714
2017-12-10T14:43:31.676942: step 3410, loss 0.715022, acc 0.90625, prec 0.0574022, recall 0.870795
2017-12-10T14:43:31.866679: step 3411, loss 0.328575, acc 0.90625, prec 0.0573951, recall 0.870795
2017-12-10T14:43:32.054957: step 3412, loss 0.271628, acc 0.96875, prec 0.0574122, recall 0.870836
2017-12-10T14:43:32.244708: step 3413, loss 0.27984, acc 0.890625, prec 0.0574234, recall 0.870877
2017-12-10T14:43:32.431427: step 3414, loss 0.927043, acc 0.9375, prec 0.0574382, recall 0.870917
2017-12-10T14:43:32.620784: step 3415, loss 0.0872028, acc 0.96875, prec 0.0574553, recall 0.870958
2017-12-10T14:43:32.807931: step 3416, loss 0.208915, acc 0.90625, prec 0.0574482, recall 0.870958
2017-12-10T14:43:32.996039: step 3417, loss 0.244943, acc 0.96875, prec 0.0574848, recall 0.871039
2017-12-10T14:43:33.183536: step 3418, loss 0.155119, acc 0.953125, prec 0.0574813, recall 0.871039
2017-12-10T14:43:33.368883: step 3419, loss 0.109646, acc 0.953125, prec 0.0575557, recall 0.8712
2017-12-10T14:43:33.554365: step 3420, loss 0.0679915, acc 0.984375, prec 0.0575936, recall 0.871281
2017-12-10T14:43:33.743089: step 3421, loss 0.626099, acc 0.890625, prec 0.0576242, recall 0.871361
2017-12-10T14:43:33.930743: step 3422, loss 0.143443, acc 0.9375, prec 0.0576195, recall 0.871361
2017-12-10T14:43:34.116832: step 3423, loss 0.100683, acc 0.96875, prec 0.0576171, recall 0.871361
2017-12-10T14:43:34.303956: step 3424, loss 0.22695, acc 0.921875, prec 0.0576111, recall 0.871361
2017-12-10T14:43:34.490945: step 3425, loss 0.163442, acc 0.9375, prec 0.0576064, recall 0.871361
2017-12-10T14:43:34.678635: step 3426, loss 3.07695, acc 0.96875, prec 0.0576052, recall 0.871089
2017-12-10T14:43:34.878028: step 3427, loss 0.148538, acc 0.953125, prec 0.0576406, recall 0.87117
2017-12-10T14:43:35.066449: step 3428, loss 0.151133, acc 0.90625, prec 0.0576919, recall 0.87129
2017-12-10T14:43:35.254394: step 3429, loss 0.205292, acc 0.953125, prec 0.0577078, recall 0.87133
2017-12-10T14:43:35.443266: step 3430, loss 0.20187, acc 0.9375, prec 0.057703, recall 0.87133
2017-12-10T14:43:35.628092: step 3431, loss 0.248088, acc 0.9375, prec 0.0576983, recall 0.87133
2017-12-10T14:43:35.814668: step 3432, loss 0.30606, acc 0.90625, prec 0.0577301, recall 0.871411
2017-12-10T14:43:36.004171: step 3433, loss 0.517575, acc 0.859375, prec 0.0577583, recall 0.871491
2017-12-10T14:43:36.191961: step 3434, loss 0.264401, acc 0.890625, prec 0.0577499, recall 0.871491
2017-12-10T14:43:36.379322: step 3435, loss 2.49289, acc 0.9375, prec 0.0577464, recall 0.871219
2017-12-10T14:43:36.571694: step 3436, loss 0.255169, acc 0.9375, prec 0.0577416, recall 0.871219
2017-12-10T14:43:36.759355: step 3437, loss 0.604316, acc 0.875, prec 0.057771, recall 0.871299
2017-12-10T14:43:36.946665: step 3438, loss 0.303099, acc 0.859375, prec 0.0577602, recall 0.871299
2017-12-10T14:43:37.135112: step 3439, loss 0.67135, acc 0.8125, prec 0.0577848, recall 0.87138
2017-12-10T14:43:37.320589: step 3440, loss 0.658999, acc 0.8125, prec 0.0577705, recall 0.87138
2017-12-10T14:43:37.510525: step 3441, loss 0.528646, acc 0.796875, prec 0.057755, recall 0.87138
2017-12-10T14:43:37.698167: step 3442, loss 0.285884, acc 0.890625, prec 0.0577856, recall 0.87146
2017-12-10T14:43:37.889279: step 3443, loss 0.344249, acc 0.90625, prec 0.0577979, recall 0.8715
2017-12-10T14:43:38.074817: step 3444, loss 0.691119, acc 0.828125, prec 0.0578042, recall 0.87154
2017-12-10T14:43:38.261103: step 3445, loss 0.563049, acc 0.84375, prec 0.0577923, recall 0.87154
2017-12-10T14:43:38.452707: step 3446, loss 0.30436, acc 0.921875, prec 0.0577863, recall 0.87154
2017-12-10T14:43:38.638679: step 3447, loss 0.57291, acc 0.90625, prec 0.0578374, recall 0.871659
2017-12-10T14:43:38.825663: step 3448, loss 0.402217, acc 0.921875, prec 0.0578703, recall 0.871739
2017-12-10T14:43:39.013271: step 3449, loss 0.92353, acc 0.90625, prec 0.0579214, recall 0.871859
2017-12-10T14:43:39.202712: step 3450, loss 0.363128, acc 0.90625, prec 0.0579337, recall 0.871898
2017-12-10T14:43:39.393383: step 3451, loss 0.360888, acc 0.90625, prec 0.0579459, recall 0.871938
2017-12-10T14:43:39.580563: step 3452, loss 0.123084, acc 0.953125, prec 0.0579618, recall 0.871978
2017-12-10T14:43:39.768285: step 3453, loss 0.291831, acc 0.875, prec 0.0579522, recall 0.871978
2017-12-10T14:43:39.956823: step 3454, loss 0.457578, acc 0.828125, prec 0.0579391, recall 0.871978
2017-12-10T14:43:40.146776: step 3455, loss 0.394633, acc 0.953125, prec 0.0579743, recall 0.872057
2017-12-10T14:43:40.333574: step 3456, loss 0.192266, acc 0.9375, prec 0.0579695, recall 0.872057
2017-12-10T14:43:40.520473: step 3457, loss 0.318746, acc 0.90625, prec 0.0579624, recall 0.872057
2017-12-10T14:43:40.706458: step 3458, loss 0.102617, acc 0.96875, prec 0.0579988, recall 0.872136
2017-12-10T14:43:40.892638: step 3459, loss 0.144452, acc 0.96875, prec 0.0579964, recall 0.872136
2017-12-10T14:43:41.084363: step 3460, loss 0.136347, acc 0.953125, prec 0.0579928, recall 0.872136
2017-12-10T14:43:41.270012: step 3461, loss 0.205501, acc 0.921875, prec 0.0579868, recall 0.872136
2017-12-10T14:43:41.460297: step 3462, loss 0.111994, acc 0.9375, prec 0.057982, recall 0.872136
2017-12-10T14:43:41.649686: step 3463, loss 0.359539, acc 0.890625, prec 0.0579737, recall 0.872136
2017-12-10T14:43:41.837027: step 3464, loss 0.191255, acc 0.9375, prec 0.0579689, recall 0.872136
2017-12-10T14:43:42.024509: step 3465, loss 0.142339, acc 0.953125, prec 0.0579653, recall 0.872136
2017-12-10T14:43:42.208727: step 3466, loss 1.05934, acc 0.984375, prec 0.0579835, recall 0.872176
2017-12-10T14:43:42.400251: step 3467, loss 0.32016, acc 0.953125, prec 0.0579993, recall 0.872215
2017-12-10T14:43:42.588981: step 3468, loss 0.644232, acc 0.953125, prec 0.0580151, recall 0.872255
2017-12-10T14:43:42.780364: step 3469, loss 0.279312, acc 0.9375, prec 0.0580491, recall 0.872334
2017-12-10T14:43:42.967135: step 3470, loss 3.28929, acc 0.9375, prec 0.0580455, recall 0.872064
2017-12-10T14:43:43.155463: step 3471, loss 0.164332, acc 0.953125, prec 0.058042, recall 0.872064
2017-12-10T14:43:43.343232: step 3472, loss 0.271129, acc 0.9375, prec 0.0580372, recall 0.872064
2017-12-10T14:43:43.529462: step 3473, loss 0.261343, acc 0.921875, prec 0.0580312, recall 0.872064
2017-12-10T14:43:43.715074: step 3474, loss 2.10446, acc 0.921875, prec 0.0580264, recall 0.871795
2017-12-10T14:43:43.905144: step 3475, loss 0.542915, acc 0.796875, prec 0.0580497, recall 0.871874
2017-12-10T14:43:44.096166: step 3476, loss 0.194354, acc 0.921875, prec 0.0580437, recall 0.871874
2017-12-10T14:43:44.282076: step 3477, loss 0.177653, acc 0.921875, prec 0.0580377, recall 0.871874
2017-12-10T14:43:44.467474: step 3478, loss 0.542758, acc 0.875, prec 0.0580282, recall 0.871874
2017-12-10T14:43:44.638108: step 3479, loss 0.494595, acc 0.807692, prec 0.058055, recall 0.871953
2017-12-10T14:43:44.832727: step 3480, loss 0.46112, acc 0.875, prec 0.0580841, recall 0.872032
2017-12-10T14:43:45.018014: step 3481, loss 0.28196, acc 0.921875, prec 0.0580975, recall 0.872072
2017-12-10T14:43:45.207391: step 3482, loss 0.39528, acc 0.875, prec 0.058088, recall 0.872072
2017-12-10T14:43:45.396478: step 3483, loss 0.519628, acc 0.796875, prec 0.0580918, recall 0.872111
2017-12-10T14:43:45.584343: step 3484, loss 0.422778, acc 0.859375, prec 0.0581004, recall 0.87215
2017-12-10T14:43:45.772780: step 3485, loss 0.494995, acc 0.859375, prec 0.0580897, recall 0.87215
2017-12-10T14:43:45.959343: step 3486, loss 0.466633, acc 0.796875, prec 0.0580935, recall 0.87219
2017-12-10T14:43:46.144665: step 3487, loss 0.510041, acc 0.828125, prec 0.0580997, recall 0.872229
2017-12-10T14:43:46.331217: step 3488, loss 0.355383, acc 0.859375, prec 0.0581083, recall 0.872268
2017-12-10T14:43:46.517371: step 3489, loss 0.430101, acc 0.890625, prec 0.0581386, recall 0.872347
2017-12-10T14:43:46.705764: step 3490, loss 0.0804653, acc 0.96875, prec 0.0581362, recall 0.872347
2017-12-10T14:43:46.893692: step 3491, loss 0.146386, acc 0.921875, prec 0.0581302, recall 0.872347
2017-12-10T14:43:47.083077: step 3492, loss 0.292333, acc 0.90625, prec 0.0581231, recall 0.872347
2017-12-10T14:43:47.270137: step 3493, loss 0.0935533, acc 0.953125, prec 0.0581195, recall 0.872347
2017-12-10T14:43:47.457969: step 3494, loss 0.678508, acc 0.8125, prec 0.0581245, recall 0.872386
2017-12-10T14:43:47.645529: step 3495, loss 0.314959, acc 0.890625, prec 0.0581355, recall 0.872425
2017-12-10T14:43:47.833595: step 3496, loss 0.188809, acc 0.921875, prec 0.0581295, recall 0.872425
2017-12-10T14:43:48.023953: step 3497, loss 0.179319, acc 0.90625, prec 0.0581417, recall 0.872465
2017-12-10T14:43:48.211522: step 3498, loss 0.276271, acc 0.96875, prec 0.0581586, recall 0.872504
2017-12-10T14:43:48.398527: step 3499, loss 0.0931304, acc 0.984375, prec 0.0581574, recall 0.872504
2017-12-10T14:43:48.584538: step 3500, loss 0.192493, acc 0.921875, prec 0.0581514, recall 0.872504
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_3/1512934336/checkpoints/model-3500

2017-12-10T14:43:50.154344: step 3501, loss 0.05518, acc 0.96875, prec 0.0581491, recall 0.872504
2017-12-10T14:43:50.341420: step 3502, loss 0.0161462, acc 1, prec 0.0581491, recall 0.872504
2017-12-10T14:43:50.526436: step 3503, loss 0.079676, acc 0.96875, prec 0.0581467, recall 0.872504
2017-12-10T14:43:50.712521: step 3504, loss 0.0135654, acc 1, prec 0.0581467, recall 0.872504
2017-12-10T14:43:50.900064: step 3505, loss 0.347477, acc 1, prec 0.058166, recall 0.872543
2017-12-10T14:43:51.091049: step 3506, loss 0.128401, acc 0.96875, prec 0.0581829, recall 0.872582
2017-12-10T14:43:51.280127: step 3507, loss 0.0172457, acc 1, prec 0.0582021, recall 0.872621
2017-12-10T14:43:51.470868: step 3508, loss 0.429294, acc 0.96875, prec 0.058219, recall 0.87266
2017-12-10T14:43:51.663011: step 3509, loss 0.117009, acc 0.953125, prec 0.0582347, recall 0.872699
2017-12-10T14:43:51.850541: step 3510, loss 5.30098, acc 0.953125, prec 0.0582528, recall 0.872203
2017-12-10T14:43:52.040872: step 3511, loss 2.03551, acc 0.96875, prec 0.0582902, recall 0.872015
2017-12-10T14:43:52.230911: step 3512, loss 0.0316675, acc 1, prec 0.0582902, recall 0.872015
2017-12-10T14:43:52.415722: step 3513, loss 0.427026, acc 0.890625, prec 0.0582818, recall 0.872015
2017-12-10T14:43:52.602971: step 3514, loss 0.226404, acc 0.921875, prec 0.0582951, recall 0.872054
2017-12-10T14:43:52.790812: step 3515, loss 0.572587, acc 0.890625, prec 0.0582868, recall 0.872054
2017-12-10T14:43:52.975113: step 3516, loss 0.403692, acc 0.796875, prec 0.0582713, recall 0.872054
2017-12-10T14:43:53.159145: step 3517, loss 0.589566, acc 0.8125, prec 0.0582763, recall 0.872093
2017-12-10T14:43:53.344246: step 3518, loss 0.466399, acc 0.875, prec 0.058286, recall 0.872132
2017-12-10T14:43:53.531327: step 3519, loss 0.618392, acc 0.828125, prec 0.0582729, recall 0.872132
2017-12-10T14:43:53.721874: step 3520, loss 0.861568, acc 0.78125, prec 0.0582754, recall 0.872171
2017-12-10T14:43:53.907096: step 3521, loss 0.505952, acc 0.828125, prec 0.0583008, recall 0.872249
2017-12-10T14:43:54.096203: step 3522, loss 1.42655, acc 0.6875, prec 0.058277, recall 0.872249
2017-12-10T14:43:54.284470: step 3523, loss 0.748176, acc 0.75, prec 0.0582772, recall 0.872288
2017-12-10T14:43:54.469298: step 3524, loss 0.51477, acc 0.8125, prec 0.0583206, recall 0.872405
2017-12-10T14:43:54.655898: step 3525, loss 0.591782, acc 0.84375, prec 0.0583471, recall 0.872483
2017-12-10T14:43:54.843392: step 3526, loss 0.720816, acc 0.78125, prec 0.0583497, recall 0.872522
2017-12-10T14:43:55.027773: step 3527, loss 0.488348, acc 0.90625, prec 0.0583809, recall 0.8726
2017-12-10T14:43:55.213645: step 3528, loss 0.680904, acc 0.8125, prec 0.058405, recall 0.872677
2017-12-10T14:43:55.398559: step 3529, loss 0.380849, acc 0.90625, prec 0.0584171, recall 0.872716
2017-12-10T14:43:55.583184: step 3530, loss 0.415345, acc 0.90625, prec 0.0584291, recall 0.872755
2017-12-10T14:43:55.774195: step 3531, loss 0.608172, acc 0.90625, prec 0.0584603, recall 0.872832
2017-12-10T14:43:55.962708: step 3532, loss 0.304102, acc 0.90625, prec 0.0584724, recall 0.872871
2017-12-10T14:43:56.148587: step 3533, loss 0.164167, acc 0.953125, prec 0.058488, recall 0.87291
2017-12-10T14:43:56.335971: step 3534, loss 0.125686, acc 0.9375, prec 0.0585216, recall 0.872987
2017-12-10T14:43:56.522723: step 3535, loss 0.169914, acc 0.9375, prec 0.0585168, recall 0.872987
2017-12-10T14:43:56.713359: step 3536, loss 0.135869, acc 0.953125, prec 0.0585324, recall 0.873026
2017-12-10T14:43:56.899193: step 3537, loss 0.439058, acc 0.953125, prec 0.058548, recall 0.873064
2017-12-10T14:43:57.089654: step 3538, loss 0.244316, acc 0.9375, prec 0.0585432, recall 0.873064
2017-12-10T14:43:57.274388: step 3539, loss 0.553673, acc 1, prec 0.0585816, recall 0.873141
2017-12-10T14:43:57.465856: step 3540, loss 0.110618, acc 0.96875, prec 0.0585984, recall 0.87318
2017-12-10T14:43:57.656902: step 3541, loss 0.22569, acc 0.890625, prec 0.0586092, recall 0.873218
2017-12-10T14:43:57.842297: step 3542, loss 0.243365, acc 0.9375, prec 0.0586044, recall 0.873218
2017-12-10T14:43:58.029134: step 3543, loss 0.239568, acc 0.890625, prec 0.0585961, recall 0.873218
2017-12-10T14:43:58.219070: step 3544, loss 0.1671, acc 0.953125, prec 0.05865, recall 0.873333
2017-12-10T14:43:58.407879: step 3545, loss 0.10924, acc 0.953125, prec 0.0586847, recall 0.87341
2017-12-10T14:43:58.593099: step 3546, loss 0.157559, acc 0.9375, prec 0.0587374, recall 0.873525
2017-12-10T14:43:58.781889: step 3547, loss 0.398618, acc 0.9375, prec 0.0587326, recall 0.873525
2017-12-10T14:43:58.974336: step 3548, loss 0.473452, acc 0.890625, prec 0.0587434, recall 0.873563
2017-12-10T14:43:59.165158: step 3549, loss 0.238572, acc 0.96875, prec 0.058741, recall 0.873563
2017-12-10T14:43:59.353879: step 3550, loss 0.142242, acc 0.984375, prec 0.0587589, recall 0.873601
2017-12-10T14:43:59.541842: step 3551, loss 0.187839, acc 0.921875, prec 0.0587721, recall 0.87364
2017-12-10T14:43:59.730293: step 3552, loss 0.0536773, acc 0.984375, prec 0.0587709, recall 0.87364
2017-12-10T14:43:59.914317: step 3553, loss 0.0427589, acc 0.984375, prec 0.0587697, recall 0.87364
2017-12-10T14:44:00.102483: step 3554, loss 0.0492984, acc 0.984375, prec 0.0587876, recall 0.873678
2017-12-10T14:44:00.292785: step 3555, loss 0.0219597, acc 1, prec 0.0588068, recall 0.873716
2017-12-10T14:44:00.481831: step 3556, loss 0.0231094, acc 0.984375, prec 0.0588247, recall 0.873754
2017-12-10T14:44:00.672488: step 3557, loss 0.220066, acc 0.9375, prec 0.0588199, recall 0.873754
2017-12-10T14:44:00.861454: step 3558, loss 0.116871, acc 0.953125, prec 0.0588164, recall 0.873754
2017-12-10T14:44:01.047947: step 3559, loss 0.198844, acc 0.953125, prec 0.0588128, recall 0.873754
2017-12-10T14:44:01.237082: step 3560, loss 0.0317127, acc 0.984375, prec 0.0588116, recall 0.873754
2017-12-10T14:44:01.425060: step 3561, loss 0.128615, acc 0.96875, prec 0.0588283, recall 0.873792
2017-12-10T14:44:01.611945: step 3562, loss 0.0050108, acc 1, prec 0.0588283, recall 0.873792
2017-12-10T14:44:01.800158: step 3563, loss 0.127793, acc 0.953125, prec 0.0588247, recall 0.873792
2017-12-10T14:44:01.993441: step 3564, loss 0.023563, acc 0.984375, prec 0.0588235, recall 0.873792
2017-12-10T14:44:02.181244: step 3565, loss 0.341186, acc 0.96875, prec 0.0588403, recall 0.87383
2017-12-10T14:44:02.372690: step 3566, loss 0.0797795, acc 0.984375, prec 0.0588582, recall 0.873868
2017-12-10T14:44:02.561839: step 3567, loss 0.068781, acc 0.953125, prec 0.0588737, recall 0.873906
2017-12-10T14:44:02.751893: step 3568, loss 6.03943, acc 0.984375, prec 0.0588737, recall 0.873643
2017-12-10T14:44:02.941319: step 3569, loss 0.0273986, acc 0.984375, prec 0.0588725, recall 0.873643
2017-12-10T14:44:03.131639: step 3570, loss 0.119118, acc 0.953125, prec 0.0589454, recall 0.873795
2017-12-10T14:44:03.322414: step 3571, loss 0.0253425, acc 1, prec 0.0589454, recall 0.873795
2017-12-10T14:44:03.512292: step 3572, loss 0.201132, acc 0.90625, prec 0.0589383, recall 0.873795
2017-12-10T14:44:03.700421: step 3573, loss 0.0156904, acc 1, prec 0.0589383, recall 0.873795
2017-12-10T14:44:03.890097: step 3574, loss 0.115694, acc 0.953125, prec 0.0589538, recall 0.873833
2017-12-10T14:44:04.078227: step 3575, loss 0.28713, acc 0.9375, prec 0.0589681, recall 0.873871
2017-12-10T14:44:04.264335: step 3576, loss 0.0942589, acc 0.96875, prec 0.0589657, recall 0.873871
2017-12-10T14:44:04.453128: step 3577, loss 0.261963, acc 0.953125, prec 0.0590003, recall 0.873947
2017-12-10T14:44:04.644400: step 3578, loss 0.380908, acc 0.96875, prec 0.0590362, recall 0.874023
2017-12-10T14:44:04.832022: step 3579, loss 0.248685, acc 0.9375, prec 0.0590505, recall 0.874061
2017-12-10T14:44:05.019273: step 3580, loss 0.280835, acc 0.90625, prec 0.0590624, recall 0.874099
2017-12-10T14:44:05.203602: step 3581, loss 0.25549, acc 0.9375, prec 0.0590576, recall 0.874099
2017-12-10T14:44:05.390841: step 3582, loss 0.306855, acc 0.921875, prec 0.0590707, recall 0.874136
2017-12-10T14:44:05.578459: step 3583, loss 0.164245, acc 0.921875, prec 0.0590647, recall 0.874136
2017-12-10T14:44:05.763987: step 3584, loss 0.143518, acc 0.9375, prec 0.0590981, recall 0.874212
2017-12-10T14:44:05.951505: step 3585, loss 0.351369, acc 0.921875, prec 0.0591494, recall 0.874325
2017-12-10T14:44:06.137674: step 3586, loss 0.25807, acc 0.890625, prec 0.059141, recall 0.874325
2017-12-10T14:44:06.324028: step 3587, loss 0.150377, acc 0.96875, prec 0.0591386, recall 0.874325
2017-12-10T14:44:06.513061: step 3588, loss 0.306402, acc 0.921875, prec 0.0591517, recall 0.874363
2017-12-10T14:44:06.700507: step 3589, loss 0.255283, acc 0.890625, prec 0.0591433, recall 0.874363
2017-12-10T14:44:06.888544: step 3590, loss 0.0875219, acc 0.96875, prec 0.0591981, recall 0.874476
2017-12-10T14:44:07.082080: step 3591, loss 0.29277, acc 0.9375, prec 0.0591933, recall 0.874476
2017-12-10T14:44:07.269526: step 3592, loss 0.2934, acc 0.90625, prec 0.0591861, recall 0.874476
2017-12-10T14:44:07.456134: step 3593, loss 0.1072, acc 0.984375, prec 0.0591849, recall 0.874476
2017-12-10T14:44:07.642536: step 3594, loss 0.136525, acc 0.953125, prec 0.0592385, recall 0.874588
2017-12-10T14:44:07.828088: step 3595, loss 0.531958, acc 0.96875, prec 0.0592743, recall 0.874663
2017-12-10T14:44:08.018414: step 3596, loss 0.23807, acc 0.921875, prec 0.0593255, recall 0.874776
2017-12-10T14:44:08.206821: step 3597, loss 0.80383, acc 0.9375, prec 0.0593588, recall 0.874851
2017-12-10T14:44:08.398311: step 3598, loss 0.156863, acc 0.96875, prec 0.0593564, recall 0.874851
2017-12-10T14:44:08.587725: step 3599, loss 0.133516, acc 0.953125, prec 0.0593528, recall 0.874851
2017-12-10T14:44:08.775263: step 3600, loss 0.0990016, acc 0.953125, prec 0.0594254, recall 0.875
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_3/1512934336/checkpoints/model-3600

2017-12-10T14:44:09.944586: step 3601, loss 0.11295, acc 0.953125, prec 0.0594218, recall 0.875
2017-12-10T14:44:10.129541: step 3602, loss 0.0758873, acc 0.953125, prec 0.0594182, recall 0.875
2017-12-10T14:44:10.315151: step 3603, loss 0.465042, acc 0.90625, prec 0.0594491, recall 0.875075
2017-12-10T14:44:10.503245: step 3604, loss 0.0824522, acc 0.953125, prec 0.0594454, recall 0.875075
2017-12-10T14:44:10.696572: step 3605, loss 0.028791, acc 1, prec 0.0594454, recall 0.875075
2017-12-10T14:44:10.887491: step 3606, loss 0.160144, acc 0.9375, prec 0.0594597, recall 0.875112
2017-12-10T14:44:11.075908: step 3607, loss 0.0738826, acc 0.984375, prec 0.0594775, recall 0.875149
2017-12-10T14:44:11.264094: step 3608, loss 0.0555497, acc 0.984375, prec 0.0595144, recall 0.875223
2017-12-10T14:44:11.451675: step 3609, loss 0.142286, acc 0.953125, prec 0.0595298, recall 0.87526
2017-12-10T14:44:11.640312: step 3610, loss 0.386279, acc 0.9375, prec 0.059525, recall 0.87526
2017-12-10T14:44:11.826779: step 3611, loss 0.0739346, acc 0.984375, prec 0.0595619, recall 0.875335
2017-12-10T14:44:12.014878: step 3612, loss 3.21492, acc 0.90625, prec 0.0595559, recall 0.875074
2017-12-10T14:44:12.205313: step 3613, loss 0.0239772, acc 1, prec 0.0595939, recall 0.875149
2017-12-10T14:44:12.393911: step 3614, loss 0.193712, acc 0.890625, prec 0.0596045, recall 0.875186
2017-12-10T14:44:12.581850: step 3615, loss 0.0617382, acc 0.953125, prec 0.059639, recall 0.87526
2017-12-10T14:44:12.771983: step 3616, loss 0.208772, acc 0.921875, prec 0.059671, recall 0.875334
2017-12-10T14:44:12.959693: step 3617, loss 0.181732, acc 0.953125, prec 0.0596864, recall 0.875371
2017-12-10T14:44:13.146433: step 3618, loss 0.398307, acc 0.875, prec 0.0597718, recall 0.875556
2017-12-10T14:44:13.337114: step 3619, loss 0.506939, acc 0.9375, prec 0.059786, recall 0.875592
2017-12-10T14:44:13.522933: step 3620, loss 0.098775, acc 0.953125, prec 0.0597824, recall 0.875592
2017-12-10T14:44:13.711401: step 3621, loss 0.432934, acc 0.875, prec 0.0597917, recall 0.875629
2017-12-10T14:44:13.900344: step 3622, loss 0.284106, acc 0.9375, prec 0.0598059, recall 0.875666
2017-12-10T14:44:14.092258: step 3623, loss 0.146511, acc 0.9375, prec 0.0598011, recall 0.875666
2017-12-10T14:44:14.281454: step 3624, loss 0.348832, acc 0.90625, prec 0.0598318, recall 0.87574
2017-12-10T14:44:14.467046: step 3625, loss 0.150486, acc 0.953125, prec 0.0598472, recall 0.875776
2017-12-10T14:44:14.652992: step 3626, loss 0.406614, acc 0.90625, prec 0.0598589, recall 0.875813
2017-12-10T14:44:14.839982: step 3627, loss 0.244721, acc 0.9375, prec 0.0598541, recall 0.875813
2017-12-10T14:44:15.026353: step 3628, loss 0.284423, acc 0.90625, prec 0.0598658, recall 0.87585
2017-12-10T14:44:15.210242: step 3629, loss 0.485583, acc 0.921875, prec 0.0598788, recall 0.875886
2017-12-10T14:44:15.397180: step 3630, loss 0.417427, acc 0.890625, prec 0.0599083, recall 0.87596
2017-12-10T14:44:15.583788: step 3631, loss 0.316261, acc 0.875, prec 0.0598986, recall 0.87596
2017-12-10T14:44:15.769656: step 3632, loss 0.34738, acc 0.953125, prec 0.059914, recall 0.875996
2017-12-10T14:44:15.958056: step 3633, loss 0.150718, acc 0.9375, prec 0.0599091, recall 0.875996
2017-12-10T14:44:16.146188: step 3634, loss 0.340964, acc 0.875, prec 0.0598995, recall 0.875996
2017-12-10T14:44:16.333994: step 3635, loss 0.160167, acc 0.9375, prec 0.0598946, recall 0.875996
2017-12-10T14:44:16.521438: step 3636, loss 0.0612056, acc 0.984375, prec 0.0599124, recall 0.876033
2017-12-10T14:44:16.712841: step 3637, loss 0.338711, acc 0.90625, prec 0.0599051, recall 0.876033
2017-12-10T14:44:16.900105: step 3638, loss 0.20969, acc 0.953125, prec 0.0599015, recall 0.876033
2017-12-10T14:44:17.087458: step 3639, loss 0.0783438, acc 0.984375, prec 0.0599003, recall 0.876033
2017-12-10T14:44:17.273286: step 3640, loss 3.29259, acc 0.953125, prec 0.0598979, recall 0.875775
2017-12-10T14:44:17.466443: step 3641, loss 0.381764, acc 0.984375, prec 0.0599726, recall 0.875921
2017-12-10T14:44:17.654587: step 3642, loss 0.0866351, acc 0.984375, prec 0.0599713, recall 0.875921
2017-12-10T14:44:17.844870: step 3643, loss 0.162419, acc 0.9375, prec 0.0599855, recall 0.875958
2017-12-10T14:44:18.031057: step 3644, loss 0.0466469, acc 0.984375, prec 0.0600412, recall 0.876067
2017-12-10T14:44:18.220647: step 3645, loss 0.396154, acc 0.875, prec 0.0600315, recall 0.876067
2017-12-10T14:44:18.407181: step 3646, loss 0.162237, acc 0.953125, prec 0.0600278, recall 0.876067
2017-12-10T14:44:18.593978: step 3647, loss 0.343117, acc 0.90625, prec 0.0600206, recall 0.876067
2017-12-10T14:44:18.780493: step 3648, loss 0.242226, acc 0.921875, prec 0.0600145, recall 0.876067
2017-12-10T14:44:18.971133: step 3649, loss 0.253368, acc 0.953125, prec 0.0600298, recall 0.876104
2017-12-10T14:44:19.155705: step 3650, loss 0.111905, acc 0.9375, prec 0.060025, recall 0.876104
2017-12-10T14:44:19.340469: step 3651, loss 0.27345, acc 0.921875, prec 0.0600379, recall 0.87614
2017-12-10T14:44:19.528498: step 3652, loss 0.250982, acc 0.90625, prec 0.0600685, recall 0.876213
2017-12-10T14:44:19.718023: step 3653, loss 0.490776, acc 0.890625, prec 0.0600601, recall 0.876213
2017-12-10T14:44:19.905155: step 3654, loss 0.26515, acc 0.90625, prec 0.0600528, recall 0.876213
2017-12-10T14:44:20.093255: step 3655, loss 0.365597, acc 0.9375, prec 0.0600669, recall 0.876249
2017-12-10T14:44:20.283934: step 3656, loss 0.454086, acc 0.890625, prec 0.0600774, recall 0.876286
2017-12-10T14:44:20.469976: step 3657, loss 0.136476, acc 0.953125, prec 0.0600737, recall 0.876286
2017-12-10T14:44:20.655834: step 3658, loss 0.248078, acc 0.9375, prec 0.0600878, recall 0.876322
2017-12-10T14:44:20.843425: step 3659, loss 0.150835, acc 0.953125, prec 0.0600842, recall 0.876322
2017-12-10T14:44:21.028749: step 3660, loss 0.0436169, acc 0.96875, prec 0.0600818, recall 0.876322
2017-12-10T14:44:21.219197: step 3661, loss 0.0742999, acc 0.96875, prec 0.0600794, recall 0.876322
2017-12-10T14:44:21.409111: step 3662, loss 0.231372, acc 0.953125, prec 0.0601325, recall 0.876431
2017-12-10T14:44:21.598442: step 3663, loss 0.119092, acc 0.96875, prec 0.0601301, recall 0.876431
2017-12-10T14:44:21.786197: step 3664, loss 0.123672, acc 0.96875, prec 0.0601277, recall 0.876431
2017-12-10T14:44:21.970787: step 3665, loss 0.154421, acc 0.96875, prec 0.0601442, recall 0.876467
2017-12-10T14:44:22.157200: step 3666, loss 2.57041, acc 0.96875, prec 0.060143, recall 0.87621
2017-12-10T14:44:22.345217: step 3667, loss 0.133749, acc 0.953125, prec 0.0601772, recall 0.876283
2017-12-10T14:44:22.532011: step 3668, loss 0.190564, acc 0.9375, prec 0.0602291, recall 0.876391
2017-12-10T14:44:22.721607: step 3669, loss 0.0743764, acc 0.96875, prec 0.0602456, recall 0.876428
2017-12-10T14:44:22.909438: step 3670, loss 0.135499, acc 0.953125, prec 0.0602419, recall 0.876428
2017-12-10T14:44:23.095292: step 3671, loss 0.253226, acc 0.953125, prec 0.0602572, recall 0.876464
2017-12-10T14:44:23.286001: step 3672, loss 0.142228, acc 0.953125, prec 0.0602914, recall 0.876536
2017-12-10T14:44:23.474008: step 3673, loss 0.359169, acc 0.875, prec 0.0603006, recall 0.876572
2017-12-10T14:44:23.669255: step 3674, loss 0.0661718, acc 0.96875, prec 0.0603171, recall 0.876608
2017-12-10T14:44:23.857888: step 3675, loss 0.310529, acc 0.90625, prec 0.0603476, recall 0.87668
2017-12-10T14:44:24.045062: step 3676, loss 0.148953, acc 0.9375, prec 0.0603427, recall 0.87668
2017-12-10T14:44:24.232960: step 3677, loss 0.590124, acc 0.90625, prec 0.0603544, recall 0.876716
2017-12-10T14:44:24.423891: step 3678, loss 0.147065, acc 0.953125, prec 0.0603696, recall 0.876752
2017-12-10T14:44:24.613353: step 3679, loss 0.699364, acc 0.921875, prec 0.0604202, recall 0.87686
2017-12-10T14:44:24.805083: step 3680, loss 0.0240825, acc 0.984375, prec 0.060419, recall 0.87686
2017-12-10T14:44:24.992951: step 3681, loss 0.670825, acc 0.96875, prec 0.0604355, recall 0.876896
2017-12-10T14:44:25.183979: step 3682, loss 0.0515284, acc 0.984375, prec 0.0604531, recall 0.876932
2017-12-10T14:44:25.372417: step 3683, loss 0.247307, acc 0.9375, prec 0.0604861, recall 0.877004
2017-12-10T14:44:25.561223: step 3684, loss 0.21875, acc 0.921875, prec 0.06048, recall 0.877004
2017-12-10T14:44:25.752593: step 3685, loss 0.085905, acc 0.96875, prec 0.0605153, recall 0.877075
2017-12-10T14:44:25.939509: step 3686, loss 0.252293, acc 0.90625, prec 0.0605269, recall 0.877111
2017-12-10T14:44:26.126344: step 3687, loss 0.336998, acc 0.921875, prec 0.0605208, recall 0.877111
2017-12-10T14:44:26.312135: step 3688, loss 0.246338, acc 0.953125, prec 0.0605549, recall 0.877183
2017-12-10T14:44:26.502516: step 3689, loss 0.112927, acc 0.984375, prec 0.0605537, recall 0.877183
2017-12-10T14:44:26.687102: step 3690, loss 0.319432, acc 0.90625, prec 0.0605653, recall 0.877218
2017-12-10T14:44:26.873424: step 3691, loss 0.260961, acc 0.890625, prec 0.0606134, recall 0.877326
2017-12-10T14:44:27.061893: step 3692, loss 0.559489, acc 0.921875, prec 0.0606261, recall 0.877361
2017-12-10T14:44:27.250509: step 3693, loss 0.21623, acc 0.953125, prec 0.0606414, recall 0.877397
2017-12-10T14:44:27.438329: step 3694, loss 0.0976073, acc 0.96875, prec 0.0606766, recall 0.877468
2017-12-10T14:44:27.623037: step 3695, loss 0.0614184, acc 0.96875, prec 0.0606742, recall 0.877468
2017-12-10T14:44:27.813073: step 3696, loss 0.0937234, acc 0.953125, prec 0.0607083, recall 0.877539
2017-12-10T14:44:27.998148: step 3697, loss 0.363661, acc 0.890625, prec 0.0607186, recall 0.877575
2017-12-10T14:44:28.185673: step 3698, loss 0.134219, acc 0.953125, prec 0.0607149, recall 0.877575
2017-12-10T14:44:28.371455: step 3699, loss 0.26606, acc 0.921875, prec 0.0607277, recall 0.87761
2017-12-10T14:44:28.560105: step 3700, loss 0.242846, acc 0.953125, prec 0.0607617, recall 0.877681
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_3/1512934336/checkpoints/model-3700

2017-12-10T14:44:30.096186: step 3701, loss 0.0691616, acc 0.984375, prec 0.0607982, recall 0.877752
2017-12-10T14:44:30.283843: step 3702, loss 0.347277, acc 0.953125, prec 0.0608134, recall 0.877787
2017-12-10T14:44:30.474158: step 3703, loss 0.131902, acc 0.953125, prec 0.0608097, recall 0.877787
2017-12-10T14:44:30.661760: step 3704, loss 0.190574, acc 0.953125, prec 0.0608061, recall 0.877787
2017-12-10T14:44:30.849810: step 3705, loss 0.0472264, acc 0.96875, prec 0.0608225, recall 0.877823
2017-12-10T14:44:31.034567: step 3706, loss 0.192873, acc 0.9375, prec 0.0608176, recall 0.877823
2017-12-10T14:44:31.227114: step 3707, loss 0.365121, acc 0.953125, prec 0.0608516, recall 0.877894
2017-12-10T14:44:31.424534: step 3708, loss 0.245288, acc 0.96875, prec 0.0608492, recall 0.877894
2017-12-10T14:44:31.611570: step 3709, loss 0.147063, acc 0.9375, prec 0.0608443, recall 0.877894
2017-12-10T14:44:31.795849: step 3710, loss 0.0485874, acc 0.984375, prec 0.0608619, recall 0.877929
2017-12-10T14:44:31.981175: step 3711, loss 0.0427768, acc 0.984375, prec 0.0608607, recall 0.877929
2017-12-10T14:44:32.166464: step 3712, loss 0.0936213, acc 0.96875, prec 0.0608582, recall 0.877929
2017-12-10T14:44:32.353596: step 3713, loss 0.239328, acc 0.9375, prec 0.0608534, recall 0.877929
2017-12-10T14:44:32.541344: step 3714, loss 0.120844, acc 0.953125, prec 0.0608497, recall 0.877929
2017-12-10T14:44:32.724950: step 3715, loss 0.140876, acc 0.96875, prec 0.0608472, recall 0.877929
2017-12-10T14:44:32.914658: step 3716, loss 0.174614, acc 1, prec 0.0608661, recall 0.877964
2017-12-10T14:44:33.104331: step 3717, loss 0.0815677, acc 0.984375, prec 0.0608837, recall 0.877999
2017-12-10T14:44:33.291137: step 3718, loss 1.71837, acc 0.96875, prec 0.0608825, recall 0.877746
2017-12-10T14:44:33.482411: step 3719, loss 0.0885165, acc 0.96875, prec 0.0609365, recall 0.877852
2017-12-10T14:44:33.674181: step 3720, loss 0.311909, acc 1, prec 0.060993, recall 0.877957
2017-12-10T14:44:33.860613: step 3721, loss 0.0948169, acc 0.953125, prec 0.0610081, recall 0.877993
2017-12-10T14:44:34.050231: step 3722, loss 0.165892, acc 0.953125, prec 0.0610233, recall 0.878028
2017-12-10T14:44:34.239436: step 3723, loss 0.3218, acc 0.9375, prec 0.061056, recall 0.878098
2017-12-10T14:44:34.424289: step 3724, loss 0.238832, acc 0.953125, prec 0.06109, recall 0.878168
2017-12-10T14:44:34.609539: step 3725, loss 0.0766409, acc 0.953125, prec 0.0611051, recall 0.878203
2017-12-10T14:44:34.794608: step 3726, loss 0.117371, acc 0.9375, prec 0.0611002, recall 0.878203
2017-12-10T14:44:34.981031: step 3727, loss 0.13156, acc 0.96875, prec 0.0610978, recall 0.878203
2017-12-10T14:44:35.165746: step 3728, loss 0.280764, acc 0.921875, prec 0.0611292, recall 0.878273
2017-12-10T14:44:35.354732: step 3729, loss 0.196187, acc 0.96875, prec 0.0611832, recall 0.878378
2017-12-10T14:44:35.542004: step 3730, loss 0.128138, acc 0.9375, prec 0.0611783, recall 0.878378
2017-12-10T14:44:35.725692: step 3731, loss 0.601223, acc 0.890625, prec 0.0612449, recall 0.878518
2017-12-10T14:44:35.913811: step 3732, loss 0.168893, acc 0.90625, prec 0.0612564, recall 0.878553
2017-12-10T14:44:36.102006: step 3733, loss 0.033269, acc 0.984375, prec 0.0612551, recall 0.878553
2017-12-10T14:44:36.288471: step 3734, loss 0.0917864, acc 0.9375, prec 0.061269, recall 0.878588
2017-12-10T14:44:36.476171: step 3735, loss 0.132057, acc 0.9375, prec 0.0612641, recall 0.878588
2017-12-10T14:44:36.661430: step 3736, loss 0.163925, acc 0.953125, prec 0.0613168, recall 0.878692
2017-12-10T14:44:36.846507: step 3737, loss 0.126877, acc 0.984375, prec 0.0613156, recall 0.878692
2017-12-10T14:44:37.034561: step 3738, loss 0.0611266, acc 0.984375, prec 0.0613331, recall 0.878727
2017-12-10T14:44:37.221352: step 3739, loss 0.280878, acc 0.9375, prec 0.0613658, recall 0.878797
2017-12-10T14:44:37.408075: step 3740, loss 0.183707, acc 0.9375, prec 0.0613984, recall 0.878866
2017-12-10T14:44:37.600083: step 3741, loss 0.02729, acc 0.984375, prec 0.0613972, recall 0.878866
2017-12-10T14:44:37.786578: step 3742, loss 0.18643, acc 0.9375, prec 0.0614111, recall 0.878901
2017-12-10T14:44:37.972780: step 3743, loss 0.095497, acc 0.9375, prec 0.0614249, recall 0.878935
2017-12-10T14:44:38.157628: step 3744, loss 0.142756, acc 0.9375, prec 0.06142, recall 0.878935
2017-12-10T14:44:38.345484: step 3745, loss 0.121945, acc 0.96875, prec 0.0614175, recall 0.878935
2017-12-10T14:44:38.530820: step 3746, loss 0.0986411, acc 0.984375, prec 0.0614163, recall 0.878935
2017-12-10T14:44:38.718155: step 3747, loss 0.0954078, acc 0.953125, prec 0.0614126, recall 0.878935
2017-12-10T14:44:38.906954: step 3748, loss 0.0227777, acc 1, prec 0.0614126, recall 0.878935
2017-12-10T14:44:39.092898: step 3749, loss 0.273596, acc 1, prec 0.0614502, recall 0.879005
2017-12-10T14:44:39.288847: step 3750, loss 0.434899, acc 0.90625, prec 0.0614428, recall 0.879005
2017-12-10T14:44:39.475649: step 3751, loss 0.582657, acc 0.984375, prec 0.0614791, recall 0.879074
2017-12-10T14:44:39.666698: step 3752, loss 0.150599, acc 0.953125, prec 0.0614754, recall 0.879074
2017-12-10T14:44:39.854654: step 3753, loss 0.0680786, acc 0.984375, prec 0.0614929, recall 0.879108
2017-12-10T14:44:40.043675: step 3754, loss 0.305514, acc 0.9375, prec 0.0615068, recall 0.879143
2017-12-10T14:44:40.234232: step 3755, loss 0.108469, acc 0.96875, prec 0.0615043, recall 0.879143
2017-12-10T14:44:40.420457: step 3756, loss 0.142725, acc 0.96875, prec 0.0615019, recall 0.879143
2017-12-10T14:44:40.607835: step 3757, loss 0.0230367, acc 0.984375, prec 0.0615006, recall 0.879143
2017-12-10T14:44:40.795313: step 3758, loss 0.118086, acc 0.953125, prec 0.061497, recall 0.879143
2017-12-10T14:44:40.983730: step 3759, loss 0.276124, acc 0.953125, prec 0.061512, recall 0.879177
2017-12-10T14:44:41.172966: step 3760, loss 4.56632, acc 0.953125, prec 0.0615283, recall 0.878961
2017-12-10T14:44:41.363360: step 3761, loss 0.0533764, acc 0.96875, prec 0.0615259, recall 0.878961
2017-12-10T14:44:41.550007: step 3762, loss 0.328753, acc 0.921875, prec 0.0615385, recall 0.878995
2017-12-10T14:44:41.736583: step 3763, loss 0.306403, acc 0.921875, prec 0.0615698, recall 0.879064
2017-12-10T14:44:41.925590: step 3764, loss 0.332494, acc 0.90625, prec 0.0615624, recall 0.879064
2017-12-10T14:44:42.112067: step 3765, loss 0.168785, acc 0.953125, prec 0.0615962, recall 0.879133
2017-12-10T14:44:42.298140: step 3766, loss 0.101623, acc 0.9375, prec 0.0615913, recall 0.879133
2017-12-10T14:44:42.486212: step 3767, loss 0.0965192, acc 0.953125, prec 0.0615876, recall 0.879133
2017-12-10T14:44:42.672614: step 3768, loss 0.454979, acc 0.875, prec 0.0615778, recall 0.879133
2017-12-10T14:44:42.858455: step 3769, loss 0.061096, acc 0.96875, prec 0.0616128, recall 0.879202
2017-12-10T14:44:43.042120: step 3770, loss 0.35788, acc 0.921875, prec 0.0616628, recall 0.879305
2017-12-10T14:44:43.233279: step 3771, loss 0.329323, acc 0.9375, prec 0.0616766, recall 0.87934
2017-12-10T14:44:43.417595: step 3772, loss 0.382869, acc 0.890625, prec 0.061668, recall 0.87934
2017-12-10T14:44:43.605912: step 3773, loss 0.418807, acc 0.875, prec 0.0616582, recall 0.87934
2017-12-10T14:44:43.796224: step 3774, loss 0.453095, acc 0.90625, prec 0.0616508, recall 0.87934
2017-12-10T14:44:43.986273: step 3775, loss 0.387032, acc 0.875, prec 0.061641, recall 0.87934
2017-12-10T14:44:44.176951: step 3776, loss 0.0742321, acc 0.96875, prec 0.0616572, recall 0.879374
2017-12-10T14:44:44.364224: step 3777, loss 0.347317, acc 0.84375, prec 0.0616636, recall 0.879408
2017-12-10T14:44:44.551060: step 3778, loss 0.786911, acc 0.859375, prec 0.0616526, recall 0.879408
2017-12-10T14:44:44.736719: step 3779, loss 0.445425, acc 0.875, prec 0.0616802, recall 0.879477
2017-12-10T14:44:44.921272: step 3780, loss 0.313444, acc 0.90625, prec 0.0616915, recall 0.879511
2017-12-10T14:44:45.106194: step 3781, loss 0.206024, acc 0.953125, prec 0.0616878, recall 0.879511
2017-12-10T14:44:45.297557: step 3782, loss 2.44123, acc 0.90625, prec 0.061719, recall 0.87933
2017-12-10T14:44:45.489508: step 3783, loss 0.228727, acc 0.890625, prec 0.0617104, recall 0.87933
2017-12-10T14:44:45.675844: step 3784, loss 0.432441, acc 0.875, prec 0.0617006, recall 0.87933
2017-12-10T14:44:45.861515: step 3785, loss 0.29886, acc 0.9375, prec 0.0617144, recall 0.879364
2017-12-10T14:44:46.045540: step 3786, loss 0.470983, acc 0.84375, prec 0.0617021, recall 0.879364
2017-12-10T14:44:46.233720: step 3787, loss 0.336906, acc 0.90625, prec 0.0616947, recall 0.879364
2017-12-10T14:44:46.419390: step 3788, loss 0.33192, acc 0.90625, prec 0.0616873, recall 0.879364
2017-12-10T14:44:46.605265: step 3789, loss 0.81227, acc 0.953125, prec 0.0617023, recall 0.879398
2017-12-10T14:44:46.794769: step 3790, loss 0.0974422, acc 0.9375, prec 0.0617161, recall 0.879433
2017-12-10T14:44:46.978707: step 3791, loss 0.123869, acc 0.953125, prec 0.0617498, recall 0.879501
2017-12-10T14:44:47.173816: step 3792, loss 0.167053, acc 0.96875, prec 0.061766, recall 0.879535
2017-12-10T14:44:47.358769: step 3793, loss 0.395938, acc 0.890625, prec 0.0617574, recall 0.879535
2017-12-10T14:44:47.545282: step 3794, loss 0.172895, acc 0.9375, prec 0.0617711, recall 0.879569
2017-12-10T14:44:47.735027: step 3795, loss 0.153775, acc 0.9375, prec 0.0617849, recall 0.879603
2017-12-10T14:44:47.928026: step 3796, loss 0.172465, acc 0.9375, prec 0.06178, recall 0.879603
2017-12-10T14:44:48.114667: step 3797, loss 0.205262, acc 0.90625, prec 0.0617913, recall 0.879637
2017-12-10T14:44:48.299157: step 3798, loss 0.453388, acc 0.9375, prec 0.0618423, recall 0.87974
2017-12-10T14:44:48.485486: step 3799, loss 0.0958454, acc 0.96875, prec 0.0618399, recall 0.87974
2017-12-10T14:44:48.672733: step 3800, loss 0.110702, acc 0.953125, prec 0.0618362, recall 0.87974
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_3/1512934336/checkpoints/model-3800

2017-12-10T14:44:49.874349: step 3801, loss 0.962766, acc 0.9375, prec 0.0619059, recall 0.879876
2017-12-10T14:44:50.064255: step 3802, loss 0.442665, acc 0.984375, prec 0.0619233, recall 0.87991
2017-12-10T14:44:50.255417: step 3803, loss 0.0208382, acc 1, prec 0.061942, recall 0.879943
2017-12-10T14:44:50.448326: step 3804, loss 0.129837, acc 0.96875, prec 0.0619582, recall 0.879977
2017-12-10T14:44:50.638198: step 3805, loss 0.0374025, acc 0.984375, prec 0.0619942, recall 0.880045
2017-12-10T14:44:50.828389: step 3806, loss 0.306163, acc 0.875, prec 0.0619844, recall 0.880045
2017-12-10T14:44:51.016921: step 3807, loss 0.302288, acc 0.890625, prec 0.062013, recall 0.880113
2017-12-10T14:44:51.206534: step 3808, loss 0.0637011, acc 0.96875, prec 0.0620292, recall 0.880147
2017-12-10T14:44:51.393625: step 3809, loss 0.314559, acc 0.90625, prec 0.0620218, recall 0.880147
2017-12-10T14:44:51.581558: step 3810, loss 0.142247, acc 0.9375, prec 0.0620169, recall 0.880147
2017-12-10T14:44:51.768489: step 3811, loss 0.240248, acc 0.96875, prec 0.0620331, recall 0.88018
2017-12-10T14:44:51.959587: step 3812, loss 0.324202, acc 0.953125, prec 0.0620294, recall 0.88018
2017-12-10T14:44:52.144840: step 3813, loss 0.219694, acc 0.953125, prec 0.0620629, recall 0.880248
2017-12-10T14:44:52.329818: step 3814, loss 0.202132, acc 0.921875, prec 0.0620568, recall 0.880248
2017-12-10T14:44:52.514436: step 3815, loss 0.151681, acc 0.953125, prec 0.0620717, recall 0.880282
2017-12-10T14:44:52.700450: step 3816, loss 0.400791, acc 0.90625, prec 0.0620643, recall 0.880282
2017-12-10T14:44:52.890276: step 3817, loss 0.154151, acc 0.953125, prec 0.0620792, recall 0.880315
2017-12-10T14:44:53.078657: step 3818, loss 0.132351, acc 0.96875, prec 0.062114, recall 0.880383
2017-12-10T14:44:53.266465: step 3819, loss 0.348011, acc 0.9375, prec 0.0621091, recall 0.880383
2017-12-10T14:44:53.453753: step 3820, loss 0.174185, acc 0.96875, prec 0.0621252, recall 0.880416
2017-12-10T14:44:53.639335: step 3821, loss 0.365208, acc 0.921875, prec 0.0621749, recall 0.880517
2017-12-10T14:44:53.826837: step 3822, loss 0.0198503, acc 1, prec 0.0621749, recall 0.880517
2017-12-10T14:44:54.017498: step 3823, loss 0.138163, acc 0.96875, prec 0.0621911, recall 0.880551
2017-12-10T14:44:54.204999: step 3824, loss 0.0856212, acc 0.984375, prec 0.0622085, recall 0.880584
2017-12-10T14:44:54.393674: step 3825, loss 0.0877104, acc 0.96875, prec 0.0622432, recall 0.880651
2017-12-10T14:44:54.580800: step 3826, loss 0.153191, acc 0.921875, prec 0.062237, recall 0.880651
2017-12-10T14:44:54.768951: step 3827, loss 0.210468, acc 0.953125, prec 0.0622333, recall 0.880651
2017-12-10T14:44:54.953896: step 3828, loss 0.105713, acc 0.96875, prec 0.0622309, recall 0.880651
2017-12-10T14:44:55.139546: step 3829, loss 0.321504, acc 0.9375, prec 0.0622259, recall 0.880651
2017-12-10T14:44:55.327495: step 3830, loss 0.127593, acc 0.953125, prec 0.0622222, recall 0.880651
2017-12-10T14:44:55.517546: step 3831, loss 0.208983, acc 0.953125, prec 0.0622371, recall 0.880685
2017-12-10T14:44:55.703268: step 3832, loss 0.225428, acc 0.9375, prec 0.0622322, recall 0.880685
2017-12-10T14:44:55.891763: step 3833, loss 1.18959, acc 0.953125, prec 0.0622297, recall 0.880438
2017-12-10T14:44:56.086103: step 3834, loss 2.20241, acc 0.953125, prec 0.0622272, recall 0.880191
2017-12-10T14:44:56.279645: step 3835, loss 0.354376, acc 0.953125, prec 0.0622793, recall 0.880292
2017-12-10T14:44:56.467676: step 3836, loss 0.0848225, acc 0.984375, prec 0.0623153, recall 0.880359
2017-12-10T14:44:56.659265: step 3837, loss 0.0548018, acc 0.96875, prec 0.0623314, recall 0.880392
2017-12-10T14:44:56.847839: step 3838, loss 0.137116, acc 0.953125, prec 0.0623277, recall 0.880392
2017-12-10T14:44:57.033362: step 3839, loss 0.109082, acc 0.96875, prec 0.0623253, recall 0.880392
2017-12-10T14:44:57.221729: step 3840, loss 1.3094, acc 0.96875, prec 0.062324, recall 0.880146
2017-12-10T14:44:57.412739: step 3841, loss 0.242849, acc 0.921875, prec 0.0623178, recall 0.880146
2017-12-10T14:44:57.603626: step 3842, loss 0.393943, acc 0.90625, prec 0.0623104, recall 0.880146
2017-12-10T14:44:57.791316: step 3843, loss 0.313355, acc 0.9375, prec 0.0623055, recall 0.880146
2017-12-10T14:44:57.977237: step 3844, loss 0.403753, acc 0.890625, prec 0.0622968, recall 0.880146
2017-12-10T14:44:58.165804: step 3845, loss 0.356563, acc 0.875, prec 0.0623055, recall 0.880179
2017-12-10T14:44:58.355676: step 3846, loss 0.455433, acc 0.90625, prec 0.0623539, recall 0.88028
2017-12-10T14:44:58.549499: step 3847, loss 0.626328, acc 0.84375, prec 0.0623415, recall 0.88028
2017-12-10T14:44:58.737932: step 3848, loss 0.434758, acc 0.875, prec 0.0623316, recall 0.88028
2017-12-10T14:44:58.923708: step 3849, loss 0.474017, acc 0.859375, prec 0.0623391, recall 0.880313
2017-12-10T14:44:59.117533: step 3850, loss 0.362767, acc 0.859375, prec 0.0623837, recall 0.880414
2017-12-10T14:44:59.305983: step 3851, loss 0.257418, acc 0.9375, prec 0.0623973, recall 0.880447
2017-12-10T14:44:59.493318: step 3852, loss 0.223565, acc 0.921875, prec 0.0625025, recall 0.880647
2017-12-10T14:44:59.681783: step 3853, loss 0.419246, acc 0.84375, prec 0.0625087, recall 0.88068
2017-12-10T14:44:59.868984: step 3854, loss 0.205101, acc 0.921875, prec 0.0625396, recall 0.880747
2017-12-10T14:45:00.056416: step 3855, loss 0.328319, acc 0.90625, prec 0.0625507, recall 0.88078
2017-12-10T14:45:00.241719: step 3856, loss 0.398265, acc 0.921875, prec 0.0626001, recall 0.880879
2017-12-10T14:45:00.431625: step 3857, loss 0.340544, acc 0.90625, prec 0.0626112, recall 0.880913
2017-12-10T14:45:00.618837: step 3858, loss 0.160713, acc 0.953125, prec 0.0626075, recall 0.880913
2017-12-10T14:45:00.805488: step 3859, loss 0.29422, acc 0.90625, prec 0.0626186, recall 0.880946
2017-12-10T14:45:00.992478: step 3860, loss 0.265584, acc 0.890625, prec 0.06261, recall 0.880946
2017-12-10T14:45:01.180318: step 3861, loss 0.18087, acc 0.921875, prec 0.0626038, recall 0.880946
2017-12-10T14:45:01.369456: step 3862, loss 0.0405111, acc 1, prec 0.0626408, recall 0.881012
2017-12-10T14:45:01.563167: step 3863, loss 0.293849, acc 0.9375, prec 0.0626359, recall 0.881012
2017-12-10T14:45:01.753418: step 3864, loss 0.028072, acc 1, prec 0.0626729, recall 0.881078
2017-12-10T14:45:01.939441: step 3865, loss 0.248086, acc 0.953125, prec 0.0626692, recall 0.881078
2017-12-10T14:45:02.126783: step 3866, loss 0.409416, acc 0.9375, prec 0.0626643, recall 0.881078
2017-12-10T14:45:02.312062: step 3867, loss 0.104704, acc 0.96875, prec 0.0626618, recall 0.881078
2017-12-10T14:45:02.496914: step 3868, loss 4.09238, acc 0.921875, prec 0.0626568, recall 0.880833
2017-12-10T14:45:02.689743: step 3869, loss 0.28449, acc 0.890625, prec 0.0626667, recall 0.880866
2017-12-10T14:45:02.877485: step 3870, loss 5.43608, acc 0.875, prec 0.0626765, recall 0.880655
2017-12-10T14:45:03.068843: step 3871, loss 0.277537, acc 0.9375, prec 0.0626716, recall 0.880655
2017-12-10T14:45:03.254046: step 3872, loss 0.120023, acc 0.96875, prec 0.0626691, recall 0.880655
2017-12-10T14:45:03.444352: step 3873, loss 0.331662, acc 0.90625, prec 0.0626617, recall 0.880655
2017-12-10T14:45:03.627707: step 3874, loss 0.683043, acc 0.859375, prec 0.0626506, recall 0.880655
2017-12-10T14:45:03.815565: step 3875, loss 0.457615, acc 0.84375, prec 0.0626567, recall 0.880688
2017-12-10T14:45:04.004725: step 3876, loss 0.342482, acc 0.890625, prec 0.0627405, recall 0.880853
2017-12-10T14:45:04.193224: step 3877, loss 0.288345, acc 0.890625, prec 0.0627319, recall 0.880853
2017-12-10T14:45:04.380002: step 3878, loss 0.676012, acc 0.78125, prec 0.0627145, recall 0.880853
2017-12-10T14:45:04.564070: step 3879, loss 0.546475, acc 0.84375, prec 0.0627022, recall 0.880853
2017-12-10T14:45:04.748147: step 3880, loss 0.655924, acc 0.828125, prec 0.0626886, recall 0.880853
2017-12-10T14:45:04.933366: step 3881, loss 0.249974, acc 0.90625, prec 0.0626996, recall 0.880886
2017-12-10T14:45:05.119892: step 3882, loss 0.228214, acc 0.90625, prec 0.0627107, recall 0.880919
2017-12-10T14:45:05.307761: step 3883, loss 0.288642, acc 0.90625, prec 0.0627402, recall 0.880985
2017-12-10T14:45:05.499230: step 3884, loss 0.678085, acc 0.8125, prec 0.0627623, recall 0.881051
2017-12-10T14:45:05.688151: step 3885, loss 0.647958, acc 0.875, prec 0.0627894, recall 0.881117
2017-12-10T14:45:05.874503: step 3886, loss 0.497606, acc 0.875, prec 0.0627795, recall 0.881117
2017-12-10T14:45:06.062206: step 3887, loss 0.312304, acc 0.921875, prec 0.0627917, recall 0.88115
2017-12-10T14:45:06.247649: step 3888, loss 0.324309, acc 0.890625, prec 0.0627831, recall 0.88115
2017-12-10T14:45:06.434982: step 3889, loss 0.183305, acc 0.9375, prec 0.0627966, recall 0.881183
2017-12-10T14:45:06.621954: step 3890, loss 0.28053, acc 0.953125, prec 0.0627929, recall 0.881183
2017-12-10T14:45:06.810526: step 3891, loss 0.136514, acc 0.921875, prec 0.0628052, recall 0.881215
2017-12-10T14:45:06.998906: step 3892, loss 0.650741, acc 0.96875, prec 0.0628211, recall 0.881248
2017-12-10T14:45:07.186779: step 3893, loss 0.270094, acc 0.921875, prec 0.0628334, recall 0.881281
2017-12-10T14:45:07.373464: step 3894, loss 0.11412, acc 0.984375, prec 0.0628322, recall 0.881281
2017-12-10T14:45:07.561398: step 3895, loss 0.240292, acc 0.953125, prec 0.0628285, recall 0.881281
2017-12-10T14:45:07.755414: step 3896, loss 0.570235, acc 0.984375, prec 0.0628826, recall 0.881379
2017-12-10T14:45:07.946067: step 3897, loss 0.185656, acc 0.9375, prec 0.0628776, recall 0.881379
2017-12-10T14:45:08.134420: step 3898, loss 0.555555, acc 0.9375, prec 0.0629095, recall 0.881445
2017-12-10T14:45:08.322214: step 3899, loss 0.519035, acc 0.921875, prec 0.0629218, recall 0.881477
2017-12-10T14:45:08.510572: step 3900, loss 0.329311, acc 0.90625, prec 0.0629144, recall 0.881477
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_3/1512934336/checkpoints/model-3900

2017-12-10T14:45:09.859976: step 3901, loss 0.335311, acc 0.921875, prec 0.0629266, recall 0.88151
2017-12-10T14:45:10.049092: step 3902, loss 1.29459, acc 0.953125, prec 0.0629413, recall 0.881543
2017-12-10T14:45:10.240557: step 3903, loss 0.399808, acc 0.921875, prec 0.0629351, recall 0.881543
2017-12-10T14:45:10.427712: step 3904, loss 0.206065, acc 0.9375, prec 0.0629302, recall 0.881543
2017-12-10T14:45:10.614798: step 3905, loss 0.18167, acc 0.90625, prec 0.0629228, recall 0.881543
2017-12-10T14:45:10.801929: step 3906, loss 0.253378, acc 0.9375, prec 0.0629731, recall 0.88164
2017-12-10T14:45:10.988571: step 3907, loss 0.201452, acc 0.921875, prec 0.0630037, recall 0.881706
2017-12-10T14:45:11.178258: step 3908, loss 0.153516, acc 0.9375, prec 0.0630172, recall 0.881738
2017-12-10T14:45:11.366855: step 3909, loss 0.561294, acc 0.90625, prec 0.0630834, recall 0.881868
2017-12-10T14:45:11.556016: step 3910, loss 0.316493, acc 0.9375, prec 0.0630969, recall 0.881901
2017-12-10T14:45:11.742985: step 3911, loss 0.459767, acc 0.859375, prec 0.0630857, recall 0.881901
2017-12-10T14:45:11.933083: step 3912, loss 0.140954, acc 0.9375, prec 0.0630808, recall 0.881901
2017-12-10T14:45:12.120061: step 3913, loss 3.30169, acc 0.9375, prec 0.0631139, recall 0.881723
2017-12-10T14:45:12.311836: step 3914, loss 0.463529, acc 0.875, prec 0.0631223, recall 0.881756
2017-12-10T14:45:12.499368: step 3915, loss 2.17465, acc 0.859375, prec 0.0631492, recall 0.881579
2017-12-10T14:45:12.690882: step 3916, loss 0.16261, acc 0.921875, prec 0.0631614, recall 0.881611
2017-12-10T14:45:12.879728: step 3917, loss 0.181833, acc 0.921875, prec 0.063192, recall 0.881676
2017-12-10T14:45:13.066944: step 3918, loss 0.366731, acc 0.890625, prec 0.0631833, recall 0.881676
2017-12-10T14:45:13.252364: step 3919, loss 1.19951, acc 0.875, prec 0.0631918, recall 0.881709
2017-12-10T14:45:13.438686: step 3920, loss 0.928977, acc 0.84375, prec 0.0632713, recall 0.88187
2017-12-10T14:45:13.624354: step 3921, loss 0.40751, acc 0.90625, prec 0.0632822, recall 0.881903
2017-12-10T14:45:13.807075: step 3922, loss 0.294589, acc 0.921875, prec 0.063276, recall 0.881903
2017-12-10T14:45:13.999101: step 3923, loss 0.448537, acc 0.84375, prec 0.0632636, recall 0.881903
2017-12-10T14:45:14.194739: step 3924, loss 0.362827, acc 0.890625, prec 0.0632733, recall 0.881935
2017-12-10T14:45:14.382573: step 3925, loss 0.27615, acc 0.90625, prec 0.0632842, recall 0.881967
2017-12-10T14:45:14.566807: step 3926, loss 0.645116, acc 0.84375, prec 0.0632901, recall 0.881999
2017-12-10T14:45:14.752833: step 3927, loss 0.379719, acc 0.875, prec 0.0632986, recall 0.882032
2017-12-10T14:45:14.941495: step 3928, loss 0.356597, acc 0.921875, prec 0.0632924, recall 0.882032
2017-12-10T14:45:15.130258: step 3929, loss 0.420181, acc 0.90625, prec 0.0632849, recall 0.882032
2017-12-10T14:45:15.319262: step 3930, loss 0.340707, acc 0.84375, prec 0.0632725, recall 0.882032
2017-12-10T14:45:15.505983: step 3931, loss 0.665762, acc 0.84375, prec 0.0632968, recall 0.882096
2017-12-10T14:45:15.691482: step 3932, loss 0.264437, acc 0.9375, prec 0.0633102, recall 0.882128
2017-12-10T14:45:15.881596: step 3933, loss 0.320049, acc 0.90625, prec 0.0633028, recall 0.882128
2017-12-10T14:45:16.066916: step 3934, loss 0.322876, acc 0.890625, prec 0.0632941, recall 0.882128
2017-12-10T14:45:16.256967: step 3935, loss 0.399938, acc 0.890625, prec 0.0632854, recall 0.882128
2017-12-10T14:45:16.440861: step 3936, loss 0.312754, acc 0.9375, prec 0.0632805, recall 0.882128
2017-12-10T14:45:16.627893: step 3937, loss 0.182526, acc 0.90625, prec 0.0633097, recall 0.882193
2017-12-10T14:45:16.814693: step 3938, loss 0.437758, acc 0.890625, prec 0.063301, recall 0.882193
2017-12-10T14:45:17.000558: step 3939, loss 0.193862, acc 0.9375, prec 0.0633144, recall 0.882225
2017-12-10T14:45:17.185606: step 3940, loss 0.12227, acc 0.921875, prec 0.0633082, recall 0.882225
2017-12-10T14:45:17.374049: step 3941, loss 0.0955027, acc 0.96875, prec 0.0633241, recall 0.882257
2017-12-10T14:45:17.562722: step 3942, loss 0.176307, acc 0.96875, prec 0.0633766, recall 0.882353
2017-12-10T14:45:17.748333: step 3943, loss 0.399973, acc 0.90625, prec 0.0634241, recall 0.882449
2017-12-10T14:45:17.933685: step 3944, loss 0.293985, acc 0.9375, prec 0.0634374, recall 0.882481
2017-12-10T14:45:18.119891: step 3945, loss 0.235926, acc 0.9375, prec 0.0634325, recall 0.882481
2017-12-10T14:45:18.306328: step 3946, loss 0.158017, acc 0.96875, prec 0.06343, recall 0.882481
2017-12-10T14:45:18.497193: step 3947, loss 0.0841911, acc 0.96875, prec 0.0634275, recall 0.882481
2017-12-10T14:45:18.688325: step 3948, loss 0.201427, acc 0.953125, prec 0.0634238, recall 0.882481
2017-12-10T14:45:18.874330: step 3949, loss 1.12955, acc 0.96875, prec 0.0634579, recall 0.882545
2017-12-10T14:45:19.063069: step 3950, loss 2.7681, acc 0.921875, prec 0.0635079, recall 0.882401
2017-12-10T14:45:19.254935: step 3951, loss 0.124655, acc 0.953125, prec 0.0635225, recall 0.882433
2017-12-10T14:45:19.441385: step 3952, loss 5.87156, acc 0.921875, prec 0.0635358, recall 0.882225
2017-12-10T14:45:19.632593: step 3953, loss 1.67236, acc 0.921875, prec 0.0635674, recall 0.88205
2017-12-10T14:45:19.822548: step 3954, loss 0.169883, acc 0.9375, prec 0.0635808, recall 0.882082
2017-12-10T14:45:20.010488: step 3955, loss 1.11883, acc 0.8125, prec 0.0635659, recall 0.882082
2017-12-10T14:45:20.198692: step 3956, loss 0.368103, acc 0.875, prec 0.0635559, recall 0.882082
2017-12-10T14:45:20.385694: step 3957, loss 0.567984, acc 0.890625, prec 0.0635472, recall 0.882082
2017-12-10T14:45:20.574973: step 3958, loss 0.373722, acc 0.890625, prec 0.0635386, recall 0.882082
2017-12-10T14:45:20.760250: step 3959, loss 0.561331, acc 0.78125, prec 0.0635578, recall 0.882146
2017-12-10T14:45:20.949386: step 3960, loss 0.753273, acc 0.796875, prec 0.0635599, recall 0.882178
2017-12-10T14:45:21.134313: step 3961, loss 1.05799, acc 0.828125, prec 0.0635463, recall 0.882178
2017-12-10T14:45:21.327892: step 3962, loss 0.521307, acc 0.859375, prec 0.0635351, recall 0.882178
2017-12-10T14:45:21.514135: step 3963, loss 0.722067, acc 0.828125, prec 0.0635215, recall 0.882178
2017-12-10T14:45:21.701936: step 3964, loss 1.32094, acc 0.6875, prec 0.0634967, recall 0.882178
2017-12-10T14:45:21.887634: step 3965, loss 1.08969, acc 0.703125, prec 0.0634914, recall 0.88221
2017-12-10T14:45:22.075237: step 3966, loss 2.00275, acc 0.859375, prec 0.0634815, recall 0.881971
2017-12-10T14:45:22.260864: step 3967, loss 0.420944, acc 0.84375, prec 0.0634874, recall 0.882003
2017-12-10T14:45:22.447644: step 3968, loss 0.452788, acc 0.8125, prec 0.0635091, recall 0.882067
2017-12-10T14:45:22.633085: step 3969, loss 1.00874, acc 0.8125, prec 0.0634942, recall 0.882067
2017-12-10T14:45:22.818327: step 3970, loss 0.712188, acc 0.875, prec 0.0634843, recall 0.882067
2017-12-10T14:45:23.002740: step 3971, loss 0.350029, acc 0.859375, prec 0.0634732, recall 0.882067
2017-12-10T14:45:23.191813: step 3972, loss 0.762697, acc 0.84375, prec 0.0634973, recall 0.88213
2017-12-10T14:45:23.376028: step 3973, loss 0.548906, acc 0.890625, prec 0.0635251, recall 0.882194
2017-12-10T14:45:23.568129: step 3974, loss 0.508979, acc 0.859375, prec 0.0635504, recall 0.882258
2017-12-10T14:45:23.755908: step 3975, loss 0.231885, acc 0.90625, prec 0.0635612, recall 0.882289
2017-12-10T14:45:23.927257: step 3976, loss 0.403519, acc 0.865385, prec 0.0635526, recall 0.882289
2017-12-10T14:45:24.120462: step 3977, loss 0.48448, acc 0.890625, prec 0.0635439, recall 0.882289
2017-12-10T14:45:24.308991: step 3978, loss 0.609875, acc 0.859375, prec 0.0635328, recall 0.882289
2017-12-10T14:45:24.496392: step 3979, loss 0.394537, acc 0.90625, prec 0.0635254, recall 0.882289
2017-12-10T14:45:24.683014: step 3980, loss 0.106381, acc 0.96875, prec 0.0635229, recall 0.882289
2017-12-10T14:45:24.871480: step 3981, loss 0.383812, acc 0.9375, prec 0.063518, recall 0.882289
2017-12-10T14:45:25.056767: step 3982, loss 0.450451, acc 0.90625, prec 0.0635288, recall 0.882321
2017-12-10T14:45:25.245045: step 3983, loss 0.0611957, acc 0.96875, prec 0.0635263, recall 0.882321
2017-12-10T14:45:25.435798: step 3984, loss 0.436024, acc 0.9375, prec 0.0635578, recall 0.882385
2017-12-10T14:45:25.623470: step 3985, loss 0.174914, acc 0.921875, prec 0.0635698, recall 0.882416
2017-12-10T14:45:25.812985: step 3986, loss 0.114233, acc 0.96875, prec 0.0636401, recall 0.882543
2017-12-10T14:45:26.006234: step 3987, loss 0.285803, acc 0.9375, prec 0.0636533, recall 0.882575
2017-12-10T14:45:26.192820: step 3988, loss 0.0578584, acc 0.984375, prec 0.0636703, recall 0.882606
2017-12-10T14:45:26.383104: step 3989, loss 0.0571059, acc 0.96875, prec 0.0636678, recall 0.882606
2017-12-10T14:45:26.572606: step 3990, loss 0.27044, acc 0.953125, prec 0.0636823, recall 0.882638
2017-12-10T14:45:26.761323: step 3991, loss 0.482738, acc 0.96875, prec 0.063698, recall 0.88267
2017-12-10T14:45:26.953273: step 3992, loss 0.300405, acc 0.984375, prec 0.0637513, recall 0.882764
2017-12-10T14:45:27.140625: step 3993, loss 0.110939, acc 0.984375, prec 0.0637501, recall 0.882764
2017-12-10T14:45:27.327103: step 3994, loss 0.0587616, acc 0.953125, prec 0.0637645, recall 0.882796
2017-12-10T14:45:27.514354: step 3995, loss 0.1556, acc 0.96875, prec 0.0637802, recall 0.882827
2017-12-10T14:45:27.705498: step 3996, loss 0.044253, acc 0.984375, prec 0.0637972, recall 0.882859
2017-12-10T14:45:27.892889: step 3997, loss 0.0549689, acc 0.984375, prec 0.0637959, recall 0.882859
2017-12-10T14:45:28.081643: step 3998, loss 0.109533, acc 0.953125, prec 0.0637922, recall 0.882859
2017-12-10T14:45:28.270363: step 3999, loss 0.879276, acc 0.984375, prec 0.0638273, recall 0.882922
2017-12-10T14:45:28.464285: step 4000, loss 0.0621593, acc 0.984375, prec 0.0638261, recall 0.882922

Evaluation:
2017-12-10T14:45:32.770011: step 4000, loss 5.26211, acc 0.962446, prec 0.0643009, recall 0.8595

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_3/1512934336/checkpoints/model-4000

2017-12-10T14:45:33.888822: step 4001, loss 0.111101, acc 0.96875, prec 0.0642984, recall 0.8595
2017-12-10T14:45:34.077024: step 4002, loss 0.134247, acc 0.96875, prec 0.0642959, recall 0.8595
2017-12-10T14:45:34.267235: step 4003, loss 1.47577, acc 0.984375, prec 0.064314, recall 0.859315
2017-12-10T14:45:34.459275: step 4004, loss 0.274426, acc 0.984375, prec 0.0643488, recall 0.859387
2017-12-10T14:45:34.649367: step 4005, loss 0.214227, acc 0.96875, prec 0.0644004, recall 0.859496
2017-12-10T14:45:34.833495: step 4006, loss 0.251916, acc 0.96875, prec 0.064398, recall 0.859496
2017-12-10T14:45:35.020834: step 4007, loss 0.0851145, acc 0.984375, prec 0.0644148, recall 0.859532
2017-12-10T14:45:35.210025: step 4008, loss 0.126753, acc 0.953125, prec 0.064411, recall 0.859532
2017-12-10T14:45:35.398662: step 4009, loss 0.209013, acc 0.9375, prec 0.0644421, recall 0.859604
2017-12-10T14:45:35.584351: step 4010, loss 0.111685, acc 0.984375, prec 0.0644409, recall 0.859604
2017-12-10T14:45:35.771203: step 4011, loss 0.207253, acc 0.953125, prec 0.0644372, recall 0.859604
2017-12-10T14:45:35.958523: step 4012, loss 0.0414212, acc 0.984375, prec 0.064454, recall 0.85964
2017-12-10T14:45:36.145091: step 4013, loss 0.225607, acc 0.9375, prec 0.064467, recall 0.859676
2017-12-10T14:45:36.329609: step 4014, loss 1.80635, acc 0.953125, prec 0.0644826, recall 0.859491
2017-12-10T14:45:36.525265: step 4015, loss 0.37078, acc 0.875, prec 0.0644726, recall 0.859491
2017-12-10T14:45:36.715964: step 4016, loss 0.491746, acc 0.875, prec 0.0644987, recall 0.859564
2017-12-10T14:45:36.905749: step 4017, loss 0.286864, acc 0.921875, prec 0.0645105, recall 0.8596
2017-12-10T14:45:37.092322: step 4018, loss 0.429967, acc 0.859375, prec 0.0645174, recall 0.859636
2017-12-10T14:45:37.279626: step 4019, loss 0.429909, acc 0.84375, prec 0.064505, recall 0.859636
2017-12-10T14:45:37.463453: step 4020, loss 0.296255, acc 0.90625, prec 0.0645155, recall 0.859672
2017-12-10T14:45:37.649632: step 4021, loss 0.560449, acc 0.875, prec 0.0645056, recall 0.859672
2017-12-10T14:45:37.836784: step 4022, loss 0.500096, acc 0.796875, prec 0.0645074, recall 0.859708
2017-12-10T14:45:38.025734: step 4023, loss 0.646936, acc 0.875, prec 0.0644975, recall 0.859708
2017-12-10T14:45:38.213484: step 4024, loss 0.907167, acc 0.828125, prec 0.0644839, recall 0.859708
2017-12-10T14:45:38.398608: step 4025, loss 0.152478, acc 0.9375, prec 0.0645149, recall 0.85978
2017-12-10T14:45:38.583839: step 4026, loss 0.544595, acc 0.796875, prec 0.0645347, recall 0.859851
2017-12-10T14:45:38.770431: step 4027, loss 0.107157, acc 0.953125, prec 0.064567, recall 0.859923
2017-12-10T14:45:38.954836: step 4028, loss 0.276255, acc 0.9375, prec 0.06458, recall 0.859959
2017-12-10T14:45:39.141778: step 4029, loss 0.435559, acc 0.890625, prec 0.0645893, recall 0.859995
2017-12-10T14:45:39.333085: step 4030, loss 0.0698336, acc 0.96875, prec 0.0646048, recall 0.860031
2017-12-10T14:45:39.519054: step 4031, loss 0.136261, acc 0.96875, prec 0.0646203, recall 0.860067
2017-12-10T14:45:39.703901: step 4032, loss 0.397738, acc 0.921875, prec 0.0646321, recall 0.860102
2017-12-10T14:45:39.892127: step 4033, loss 0.777619, acc 0.90625, prec 0.0646606, recall 0.860174
2017-12-10T14:45:40.080157: step 4034, loss 0.214876, acc 0.96875, prec 0.064676, recall 0.86021
2017-12-10T14:45:40.267997: step 4035, loss 0.5321, acc 0.859375, prec 0.0646828, recall 0.860245
2017-12-10T14:45:40.455450: step 4036, loss 0.316971, acc 0.90625, prec 0.0646933, recall 0.860281
2017-12-10T14:45:40.646685: step 4037, loss 0.248522, acc 0.90625, prec 0.0646859, recall 0.860281
2017-12-10T14:45:40.833121: step 4038, loss 0.192421, acc 0.953125, prec 0.0646822, recall 0.860281
2017-12-10T14:45:41.018121: step 4039, loss 0.0243505, acc 1, prec 0.0647001, recall 0.860317
2017-12-10T14:45:41.202426: step 4040, loss 0.118698, acc 0.9375, prec 0.0646951, recall 0.860317
2017-12-10T14:45:41.389632: step 4041, loss 0.0956173, acc 0.96875, prec 0.0647106, recall 0.860352
2017-12-10T14:45:41.576835: step 4042, loss 0.0169946, acc 1, prec 0.0647106, recall 0.860352
2017-12-10T14:45:41.761800: step 4043, loss 0.601478, acc 0.953125, prec 0.0647249, recall 0.860388
2017-12-10T14:45:41.954204: step 4044, loss 0.0627858, acc 1, prec 0.0647787, recall 0.860495
2017-12-10T14:45:42.146858: step 4045, loss 0.163939, acc 0.9375, prec 0.0647917, recall 0.86053
2017-12-10T14:45:42.334586: step 4046, loss 0.0817266, acc 0.96875, prec 0.0647892, recall 0.86053
2017-12-10T14:45:42.518795: step 4047, loss 0.0210472, acc 1, prec 0.0648251, recall 0.860601
2017-12-10T14:45:42.706094: step 4048, loss 0.124511, acc 0.953125, prec 0.0648214, recall 0.860601
2017-12-10T14:45:42.894123: step 4049, loss 0.0764954, acc 0.984375, prec 0.0648381, recall 0.860637
2017-12-10T14:45:43.079202: step 4050, loss 0.137829, acc 0.953125, prec 0.0648523, recall 0.860672
2017-12-10T14:45:43.264873: step 4051, loss 0.168657, acc 0.96875, prec 0.0648498, recall 0.860672
2017-12-10T14:45:43.456338: step 4052, loss 0.213079, acc 0.953125, prec 0.064864, recall 0.860708
2017-12-10T14:45:43.645430: step 4053, loss 2.15423, acc 0.984375, prec 0.064882, recall 0.860524
2017-12-10T14:45:43.834021: step 4054, loss 0.0342982, acc 0.984375, prec 0.0648807, recall 0.860524
2017-12-10T14:45:44.025699: step 4055, loss 0.320646, acc 0.953125, prec 0.0648949, recall 0.86056
2017-12-10T14:45:44.219103: step 4056, loss 0.0694658, acc 0.984375, prec 0.0649475, recall 0.860666
2017-12-10T14:45:44.412223: step 4057, loss 0.480095, acc 0.875, prec 0.0649914, recall 0.860772
2017-12-10T14:45:44.601601: step 4058, loss 0.416584, acc 0.90625, prec 0.0649839, recall 0.860772
2017-12-10T14:45:44.790721: step 4059, loss 0.117938, acc 0.953125, prec 0.0650339, recall 0.860878
2017-12-10T14:45:44.978281: step 4060, loss 0.137806, acc 0.96875, prec 0.0650315, recall 0.860878
2017-12-10T14:45:45.164932: step 4061, loss 0.395825, acc 0.9375, prec 0.0650623, recall 0.860949
2017-12-10T14:45:45.353569: step 4062, loss 0.101558, acc 0.953125, prec 0.0650586, recall 0.860949
2017-12-10T14:45:45.540386: step 4063, loss 0.109212, acc 0.96875, prec 0.0651099, recall 0.861055
2017-12-10T14:45:45.731530: step 4064, loss 0.166916, acc 0.96875, prec 0.0651074, recall 0.861055
2017-12-10T14:45:45.921785: step 4065, loss 0.0973929, acc 0.9375, prec 0.0651024, recall 0.861055
2017-12-10T14:45:46.109455: step 4066, loss 3.6411, acc 0.9375, prec 0.0650986, recall 0.860837
2017-12-10T14:45:46.299045: step 4067, loss 0.338117, acc 0.90625, prec 0.0650911, recall 0.860837
2017-12-10T14:45:46.487771: step 4068, loss 0.430146, acc 0.90625, prec 0.0650837, recall 0.860837
2017-12-10T14:45:46.674010: step 4069, loss 0.130392, acc 0.953125, prec 0.0650799, recall 0.860837
2017-12-10T14:45:46.862809: step 4070, loss 0.21261, acc 0.890625, prec 0.065107, recall 0.860907
2017-12-10T14:45:47.048334: step 4071, loss 0.409208, acc 0.921875, prec 0.0651008, recall 0.860907
2017-12-10T14:45:47.237025: step 4072, loss 0.142495, acc 0.9375, prec 0.0651137, recall 0.860942
2017-12-10T14:45:47.425022: step 4073, loss 0.133907, acc 0.9375, prec 0.0651266, recall 0.860977
2017-12-10T14:45:47.607535: step 4074, loss 0.472611, acc 0.90625, prec 0.0651191, recall 0.860977
2017-12-10T14:45:47.795476: step 4075, loss 0.33887, acc 0.921875, prec 0.0651308, recall 0.861013
2017-12-10T14:45:47.985281: step 4076, loss 0.3356, acc 0.921875, prec 0.0651425, recall 0.861048
2017-12-10T14:45:48.173197: step 4077, loss 0.155288, acc 0.9375, prec 0.0651554, recall 0.861083
2017-12-10T14:45:48.363142: step 4078, loss 0.207214, acc 0.90625, prec 0.0651479, recall 0.861083
2017-12-10T14:45:48.550629: step 4079, loss 0.412002, acc 0.921875, prec 0.0651417, recall 0.861083
2017-12-10T14:45:48.738733: step 4080, loss 0.229601, acc 0.9375, prec 0.0651546, recall 0.861118
2017-12-10T14:45:48.923809: step 4081, loss 0.214928, acc 0.921875, prec 0.0651483, recall 0.861118
2017-12-10T14:45:49.112046: step 4082, loss 0.107131, acc 0.9375, prec 0.0651612, recall 0.861153
2017-12-10T14:45:49.301682: step 4083, loss 0.685681, acc 0.875, prec 0.0651691, recall 0.861188
2017-12-10T14:45:49.491019: step 4084, loss 0.195355, acc 0.90625, prec 0.0651617, recall 0.861188
2017-12-10T14:45:49.677672: step 4085, loss 0.234849, acc 0.9375, prec 0.0651567, recall 0.861188
2017-12-10T14:45:49.868790: step 4086, loss 0.287126, acc 0.921875, prec 0.0651504, recall 0.861188
2017-12-10T14:45:50.057936: step 4087, loss 0.0869289, acc 0.953125, prec 0.0651467, recall 0.861188
2017-12-10T14:45:50.244829: step 4088, loss 0.335564, acc 0.96875, prec 0.06518, recall 0.861259
2017-12-10T14:45:50.435462: step 4089, loss 0.0312867, acc 0.984375, prec 0.0651787, recall 0.861259
2017-12-10T14:45:50.629581: step 4090, loss 0.157019, acc 0.90625, prec 0.065207, recall 0.861329
2017-12-10T14:45:50.817744: step 4091, loss 1.4065, acc 0.921875, prec 0.0652199, recall 0.861146
2017-12-10T14:45:51.006632: step 4092, loss 0.368586, acc 0.953125, prec 0.0652519, recall 0.861216
2017-12-10T14:45:51.196055: step 4093, loss 0.172522, acc 0.921875, prec 0.0652457, recall 0.861216
2017-12-10T14:45:51.386625: step 4094, loss 0.256752, acc 0.9375, prec 0.0652407, recall 0.861216
2017-12-10T14:45:51.576268: step 4095, loss 0.965879, acc 0.9375, prec 0.0652714, recall 0.861286
2017-12-10T14:45:51.767223: step 4096, loss 0.145988, acc 0.96875, prec 0.0652689, recall 0.861286
2017-12-10T14:45:51.953637: step 4097, loss 0.283487, acc 0.9375, prec 0.0652639, recall 0.861286
2017-12-10T14:45:52.140569: step 4098, loss 0.643139, acc 0.8125, prec 0.065249, recall 0.861286
2017-12-10T14:45:52.326739: step 4099, loss 0.315575, acc 0.9375, prec 0.0652797, recall 0.861356
2017-12-10T14:45:52.517865: step 4100, loss 0.116052, acc 0.96875, prec 0.0653486, recall 0.861496
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_3/1512934336/checkpoints/model-4100

2017-12-10T14:45:53.672382: step 4101, loss 0.653676, acc 0.90625, prec 0.0653768, recall 0.861566
2017-12-10T14:45:53.865173: step 4102, loss 0.416124, acc 0.921875, prec 0.0653884, recall 0.8616
2017-12-10T14:45:54.053779: step 4103, loss 0.275982, acc 0.9375, prec 0.0654013, recall 0.861635
2017-12-10T14:45:54.240520: step 4104, loss 0.416473, acc 0.859375, prec 0.0654079, recall 0.86167
2017-12-10T14:45:54.426617: step 4105, loss 0.463846, acc 0.953125, prec 0.065422, recall 0.861705
2017-12-10T14:45:54.615284: step 4106, loss 0.229509, acc 0.90625, prec 0.0654145, recall 0.861705
2017-12-10T14:45:54.801174: step 4107, loss 0.202098, acc 0.9375, prec 0.0654273, recall 0.86174
2017-12-10T14:45:54.989320: step 4108, loss 0.50945, acc 0.890625, prec 0.0654364, recall 0.861774
2017-12-10T14:45:55.175112: step 4109, loss 0.477087, acc 0.859375, prec 0.0654609, recall 0.861844
2017-12-10T14:45:55.364115: step 4110, loss 0.24536, acc 0.96875, prec 0.0654762, recall 0.861878
2017-12-10T14:45:55.553074: step 4111, loss 0.182461, acc 0.96875, prec 0.0654737, recall 0.861878
2017-12-10T14:45:55.738954: step 4112, loss 0.464287, acc 0.90625, prec 0.065484, recall 0.861913
2017-12-10T14:45:55.924420: step 4113, loss 0.267844, acc 0.921875, prec 0.0654778, recall 0.861913
2017-12-10T14:45:56.109680: step 4114, loss 0.19575, acc 0.953125, prec 0.065474, recall 0.861913
2017-12-10T14:45:56.298521: step 4115, loss 0.13851, acc 0.96875, prec 0.0654715, recall 0.861913
2017-12-10T14:45:56.496979: step 4116, loss 0.210214, acc 0.953125, prec 0.0654678, recall 0.861913
2017-12-10T14:45:56.682835: step 4117, loss 0.305475, acc 0.921875, prec 0.0654794, recall 0.861948
2017-12-10T14:45:56.869489: step 4118, loss 0.208938, acc 0.96875, prec 0.0655303, recall 0.862052
2017-12-10T14:45:57.060301: step 4119, loss 0.281676, acc 0.9375, prec 0.0655253, recall 0.862052
2017-12-10T14:45:57.252696: step 4120, loss 0.189242, acc 0.921875, prec 0.0655191, recall 0.862052
2017-12-10T14:45:57.437226: step 4121, loss 0.100424, acc 0.953125, prec 0.0655331, recall 0.862086
2017-12-10T14:45:57.624793: step 4122, loss 0.108514, acc 0.96875, prec 0.0655307, recall 0.862086
2017-12-10T14:45:57.813717: step 4123, loss 0.213634, acc 0.921875, prec 0.0655244, recall 0.862086
2017-12-10T14:45:58.001451: step 4124, loss 0.0688526, acc 0.953125, prec 0.0655563, recall 0.862155
2017-12-10T14:45:58.191388: step 4125, loss 0.0753588, acc 0.96875, prec 0.0655538, recall 0.862155
2017-12-10T14:45:58.377582: step 4126, loss 0.00883737, acc 1, prec 0.0655538, recall 0.862155
2017-12-10T14:45:58.564980: step 4127, loss 0.509614, acc 0.90625, prec 0.0655819, recall 0.862224
2017-12-10T14:45:58.753579: step 4128, loss 0.122984, acc 0.96875, prec 0.0655794, recall 0.862224
2017-12-10T14:45:58.940449: step 4129, loss 0.206219, acc 1, prec 0.0655972, recall 0.862259
2017-12-10T14:45:59.133336: step 4130, loss 0.0280356, acc 0.984375, prec 0.0656137, recall 0.862293
2017-12-10T14:45:59.321623: step 4131, loss 0.24595, acc 0.9375, prec 0.0656087, recall 0.862293
2017-12-10T14:45:59.509661: step 4132, loss 1.00058, acc 0.96875, prec 0.0656418, recall 0.862362
2017-12-10T14:45:59.704031: step 4133, loss 0.00718104, acc 1, prec 0.0656596, recall 0.862397
2017-12-10T14:45:59.890744: step 4134, loss 0.0643755, acc 0.984375, prec 0.065694, recall 0.862466
2017-12-10T14:46:00.078731: step 4135, loss 0.00781767, acc 1, prec 0.065694, recall 0.862466
2017-12-10T14:46:00.267888: step 4136, loss 0.0622744, acc 0.953125, prec 0.0656902, recall 0.862466
2017-12-10T14:46:00.459282: step 4137, loss 0.0896918, acc 0.984375, prec 0.065689, recall 0.862466
2017-12-10T14:46:00.645343: step 4138, loss 0.0266176, acc 1, prec 0.0657246, recall 0.862534
2017-12-10T14:46:00.833173: step 4139, loss 0.119351, acc 0.96875, prec 0.0657221, recall 0.862534
2017-12-10T14:46:01.022500: step 4140, loss 0.0307615, acc 0.984375, prec 0.0657208, recall 0.862534
2017-12-10T14:46:01.216195: step 4141, loss 0.12678, acc 0.984375, prec 0.0657374, recall 0.862569
2017-12-10T14:46:01.410884: step 4142, loss 0.68633, acc 0.984375, prec 0.0657717, recall 0.862637
2017-12-10T14:46:01.606147: step 4143, loss 0.280065, acc 0.984375, prec 0.0658416, recall 0.862774
2017-12-10T14:46:01.792504: step 4144, loss 0.0674271, acc 0.96875, prec 0.0658569, recall 0.862809
2017-12-10T14:46:01.977699: step 4145, loss 0.124452, acc 0.96875, prec 0.0658544, recall 0.862809
2017-12-10T14:46:02.162478: step 4146, loss 0.152481, acc 0.96875, prec 0.0658874, recall 0.862877
2017-12-10T14:46:02.349665: step 4147, loss 0.24872, acc 0.9375, prec 0.0658824, recall 0.862877
2017-12-10T14:46:02.539464: step 4148, loss 0.194834, acc 0.953125, prec 0.0659142, recall 0.862945
2017-12-10T14:46:02.726126: step 4149, loss 0.370368, acc 0.953125, prec 0.065946, recall 0.863014
2017-12-10T14:46:02.914003: step 4150, loss 0.0306432, acc 0.984375, prec 0.0659625, recall 0.863048
2017-12-10T14:46:03.097024: step 4151, loss 0.411108, acc 0.890625, prec 0.0659537, recall 0.863048
2017-12-10T14:46:03.289443: step 4152, loss 0.37958, acc 0.890625, prec 0.0659805, recall 0.863116
2017-12-10T14:46:03.477414: step 4153, loss 0.0762369, acc 0.96875, prec 0.0659957, recall 0.86315
2017-12-10T14:46:03.667039: step 4154, loss 0.0421117, acc 0.984375, prec 0.0660122, recall 0.863184
2017-12-10T14:46:03.854282: step 4155, loss 0.593549, acc 0.953125, prec 0.0660795, recall 0.86332
2017-12-10T14:46:04.044939: step 4156, loss 0.226652, acc 0.96875, prec 0.0661126, recall 0.863388
2017-12-10T14:46:04.232349: step 4157, loss 0.0764977, acc 0.96875, prec 0.06611, recall 0.863388
2017-12-10T14:46:04.418311: step 4158, loss 0.0465559, acc 1, prec 0.0661456, recall 0.863456
2017-12-10T14:46:04.605477: step 4159, loss 0.168714, acc 0.984375, prec 0.0661443, recall 0.863456
2017-12-10T14:46:04.789934: step 4160, loss 0.193709, acc 0.90625, prec 0.0661545, recall 0.86349
2017-12-10T14:46:04.981309: step 4161, loss 0.244205, acc 0.984375, prec 0.0662243, recall 0.863625
2017-12-10T14:46:05.170690: step 4162, loss 0.266644, acc 0.9375, prec 0.0662547, recall 0.863693
2017-12-10T14:46:05.355918: step 4163, loss 0.116441, acc 0.953125, prec 0.0662687, recall 0.863726
2017-12-10T14:46:05.546369: step 4164, loss 0.231795, acc 0.90625, prec 0.0662789, recall 0.86376
2017-12-10T14:46:05.735068: step 4165, loss 0.130828, acc 0.953125, prec 0.0662929, recall 0.863794
2017-12-10T14:46:05.919891: step 4166, loss 0.420932, acc 0.921875, prec 0.0663043, recall 0.863828
2017-12-10T14:46:06.104852: step 4167, loss 0.174612, acc 0.953125, prec 0.0663183, recall 0.863861
2017-12-10T14:46:06.295461: step 4168, loss 0.258383, acc 0.96875, prec 0.0663512, recall 0.863929
2017-12-10T14:46:06.485955: step 4169, loss 0.257928, acc 0.953125, prec 0.0663475, recall 0.863929
2017-12-10T14:46:06.675828: step 4170, loss 0.0896586, acc 0.96875, prec 0.0663627, recall 0.863962
2017-12-10T14:46:06.862055: step 4171, loss 0.187755, acc 0.9375, prec 0.0663754, recall 0.863996
2017-12-10T14:46:07.053875: step 4172, loss 0.141717, acc 0.96875, prec 0.0663906, recall 0.86403
2017-12-10T14:46:07.241092: step 4173, loss 0.0640868, acc 0.96875, prec 0.0664058, recall 0.864063
2017-12-10T14:46:07.430656: step 4174, loss 0.11173, acc 0.9375, prec 0.0664008, recall 0.864063
2017-12-10T14:46:07.617923: step 4175, loss 0.10735, acc 0.953125, prec 0.0664147, recall 0.864097
2017-12-10T14:46:07.806444: step 4176, loss 0.0944727, acc 0.96875, prec 0.0664299, recall 0.86413
2017-12-10T14:46:07.995093: step 4177, loss 0.12055, acc 0.96875, prec 0.0664451, recall 0.864164
2017-12-10T14:46:08.182121: step 4178, loss 0.127309, acc 0.953125, prec 0.0664768, recall 0.864231
2017-12-10T14:46:08.373807: step 4179, loss 0.0722062, acc 0.984375, prec 0.0664933, recall 0.864265
2017-12-10T14:46:08.565870: step 4180, loss 0.0722777, acc 0.9375, prec 0.0664882, recall 0.864265
2017-12-10T14:46:08.752469: step 4181, loss 0.16334, acc 0.96875, prec 0.0665211, recall 0.864332
2017-12-10T14:46:08.941206: step 4182, loss 0.25804, acc 0.96875, prec 0.0665363, recall 0.864365
2017-12-10T14:46:09.133663: step 4183, loss 0.0228357, acc 1, prec 0.0665363, recall 0.864365
2017-12-10T14:46:09.322000: step 4184, loss 0.13556, acc 0.9375, prec 0.0665313, recall 0.864365
2017-12-10T14:46:09.511671: step 4185, loss 0.029826, acc 0.984375, prec 0.06653, recall 0.864365
2017-12-10T14:46:09.699315: step 4186, loss 0.386517, acc 0.953125, prec 0.0665262, recall 0.864365
2017-12-10T14:46:09.885329: step 4187, loss 0.0706377, acc 0.984375, prec 0.066525, recall 0.864365
2017-12-10T14:46:10.074311: step 4188, loss 0.0774671, acc 0.984375, prec 0.0665414, recall 0.864398
2017-12-10T14:46:10.264441: step 4189, loss 0.0074766, acc 1, prec 0.0665591, recall 0.864432
2017-12-10T14:46:10.455931: step 4190, loss 0.345522, acc 1, prec 0.0665945, recall 0.864499
2017-12-10T14:46:10.645798: step 4191, loss 0.211392, acc 1, prec 0.0666123, recall 0.864532
2017-12-10T14:46:10.838218: step 4192, loss 0.328196, acc 0.96875, prec 0.0666097, recall 0.864532
2017-12-10T14:46:11.030478: step 4193, loss 0.658766, acc 0.90625, prec 0.0666199, recall 0.864565
2017-12-10T14:46:11.221572: step 4194, loss 0.122221, acc 0.984375, prec 0.0666186, recall 0.864565
2017-12-10T14:46:11.408469: step 4195, loss 0.0117905, acc 1, prec 0.0666363, recall 0.864599
2017-12-10T14:46:11.597704: step 4196, loss 0.311777, acc 0.953125, prec 0.0666679, recall 0.864665
2017-12-10T14:46:11.787278: step 4197, loss 0.321732, acc 0.953125, prec 0.0666641, recall 0.864665
2017-12-10T14:46:11.973525: step 4198, loss 0.347758, acc 0.953125, prec 0.0666603, recall 0.864665
2017-12-10T14:46:12.161727: step 4199, loss 0.153153, acc 0.953125, prec 0.0666566, recall 0.864665
2017-12-10T14:46:12.349747: step 4200, loss 0.547618, acc 0.9375, prec 0.0666692, recall 0.864699
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_3/1512934336/checkpoints/model-4200

2017-12-10T14:46:13.662509: step 4201, loss 0.110826, acc 0.96875, prec 0.0666667, recall 0.864699
2017-12-10T14:46:13.851087: step 4202, loss 0.0435432, acc 0.96875, prec 0.0666995, recall 0.864765
2017-12-10T14:46:14.044682: step 4203, loss 0.307633, acc 0.96875, prec 0.0667324, recall 0.864832
2017-12-10T14:46:14.240132: step 4204, loss 0.232898, acc 0.9375, prec 0.066745, recall 0.864865
2017-12-10T14:46:14.430473: step 4205, loss 0.35104, acc 0.984375, prec 0.0667615, recall 0.864898
2017-12-10T14:46:14.619357: step 4206, loss 0.19272, acc 0.953125, prec 0.0667577, recall 0.864898
2017-12-10T14:46:14.807474: step 4207, loss 0.027969, acc 0.984375, prec 0.0667918, recall 0.864964
2017-12-10T14:46:14.991411: step 4208, loss 0.0250123, acc 0.984375, prec 0.0667905, recall 0.864964
2017-12-10T14:46:15.178027: step 4209, loss 0.0495836, acc 0.984375, prec 0.0667893, recall 0.864964
2017-12-10T14:46:15.366978: step 4210, loss 0.203485, acc 1, prec 0.0668423, recall 0.865064
2017-12-10T14:46:15.555821: step 4211, loss 0.0562127, acc 0.984375, prec 0.0668941, recall 0.865163
2017-12-10T14:46:15.745946: step 4212, loss 0.335668, acc 0.921875, prec 0.0669055, recall 0.865196
2017-12-10T14:46:15.935939: step 4213, loss 0.101775, acc 0.953125, prec 0.0669194, recall 0.865229
2017-12-10T14:46:16.122380: step 4214, loss 0.0749957, acc 0.953125, prec 0.0669332, recall 0.865262
2017-12-10T14:46:16.315601: step 4215, loss 0.00452501, acc 1, prec 0.0669332, recall 0.865262
2017-12-10T14:46:16.503219: step 4216, loss 0.100959, acc 0.953125, prec 0.0669294, recall 0.865262
2017-12-10T14:46:16.694332: step 4217, loss 0.171491, acc 0.953125, prec 0.0669433, recall 0.865295
2017-12-10T14:46:16.884727: step 4218, loss 0.0042925, acc 1, prec 0.0669433, recall 0.865295
2017-12-10T14:46:17.073528: step 4219, loss 0.0108711, acc 1, prec 0.066961, recall 0.865328
2017-12-10T14:46:17.260208: step 4220, loss 0.0889944, acc 0.953125, prec 0.0669749, recall 0.865361
2017-12-10T14:46:17.448674: step 4221, loss 0.0453834, acc 0.984375, prec 0.0669736, recall 0.865361
2017-12-10T14:46:17.637431: step 4222, loss 0.195661, acc 0.96875, prec 0.066971, recall 0.865361
2017-12-10T14:46:17.826641: step 4223, loss 2.83671, acc 0.96875, prec 0.0669875, recall 0.865182
2017-12-10T14:46:18.016053: step 4224, loss 0.0708611, acc 0.984375, prec 0.0669862, recall 0.865182
2017-12-10T14:46:18.204075: step 4225, loss 0.143323, acc 0.96875, prec 0.0670013, recall 0.865215
2017-12-10T14:46:18.391110: step 4226, loss 0.066519, acc 0.96875, prec 0.0669988, recall 0.865215
2017-12-10T14:46:18.578739: step 4227, loss 0.165697, acc 0.953125, prec 0.066995, recall 0.865215
2017-12-10T14:46:18.766508: step 4228, loss 0.244594, acc 0.9375, prec 0.0670252, recall 0.865281
2017-12-10T14:46:18.954681: step 4229, loss 0.273627, acc 0.9375, prec 0.0670555, recall 0.865347
2017-12-10T14:46:19.141018: step 4230, loss 0.213375, acc 0.984375, prec 0.0670719, recall 0.86538
2017-12-10T14:46:19.328586: step 4231, loss 0.131702, acc 0.984375, prec 0.0670883, recall 0.865413
2017-12-10T14:46:19.520842: step 4232, loss 0.180378, acc 0.953125, prec 0.0671021, recall 0.865446
2017-12-10T14:46:19.710042: step 4233, loss 0.231355, acc 0.953125, prec 0.0671513, recall 0.865544
2017-12-10T14:46:19.896915: step 4234, loss 2.86143, acc 0.859375, prec 0.0671588, recall 0.865366
2017-12-10T14:46:20.091684: step 4235, loss 0.732107, acc 0.9375, prec 0.067189, recall 0.865431
2017-12-10T14:46:20.284670: step 4236, loss 0.368356, acc 0.921875, prec 0.0672003, recall 0.865464
2017-12-10T14:46:20.470765: step 4237, loss 0.344508, acc 0.921875, prec 0.0672646, recall 0.865595
2017-12-10T14:46:20.660016: step 4238, loss 0.247248, acc 0.9375, prec 0.0672771, recall 0.865628
2017-12-10T14:46:20.846805: step 4239, loss 0.52033, acc 0.890625, prec 0.0672682, recall 0.865628
2017-12-10T14:46:21.030898: step 4240, loss 0.768043, acc 0.828125, prec 0.0672719, recall 0.865661
2017-12-10T14:46:21.219714: step 4241, loss 0.753433, acc 0.875, prec 0.0672793, recall 0.865693
2017-12-10T14:46:21.407802: step 4242, loss 0.42515, acc 0.84375, prec 0.0672666, recall 0.865693
2017-12-10T14:46:21.595642: step 4243, loss 0.313369, acc 0.875, prec 0.0672917, recall 0.865759
2017-12-10T14:46:21.783183: step 4244, loss 0.315511, acc 0.90625, prec 0.0672841, recall 0.865759
2017-12-10T14:46:21.967344: step 4245, loss 1.33445, acc 0.953125, prec 0.0673331, recall 0.865857
2017-12-10T14:46:22.156265: step 4246, loss 0.35316, acc 0.890625, prec 0.0673595, recall 0.865922
2017-12-10T14:46:22.344225: step 4247, loss 0.34606, acc 0.90625, prec 0.0673518, recall 0.865922
2017-12-10T14:46:22.535153: step 4248, loss 0.460971, acc 0.875, prec 0.0673593, recall 0.865954
2017-12-10T14:46:22.718716: step 4249, loss 0.531814, acc 0.875, prec 0.0673667, recall 0.865987
2017-12-10T14:46:22.909330: step 4250, loss 0.440146, acc 0.921875, prec 0.067378, recall 0.866019
2017-12-10T14:46:23.093681: step 4251, loss 0.285432, acc 0.90625, prec 0.0673703, recall 0.866019
2017-12-10T14:46:23.285877: step 4252, loss 0.919966, acc 0.8125, prec 0.0673903, recall 0.866084
2017-12-10T14:46:23.470742: step 4253, loss 0.484081, acc 0.859375, prec 0.0673788, recall 0.866084
2017-12-10T14:46:23.658652: step 4254, loss 0.175287, acc 0.9375, prec 0.0673913, recall 0.866117
2017-12-10T14:46:23.845384: step 4255, loss 0.887506, acc 0.8125, prec 0.0673761, recall 0.866117
2017-12-10T14:46:24.031720: step 4256, loss 0.387913, acc 0.859375, prec 0.0673822, recall 0.866149
2017-12-10T14:46:24.219840: step 4257, loss 0.128054, acc 0.953125, prec 0.067396, recall 0.866182
2017-12-10T14:46:24.406472: step 4258, loss 0.380337, acc 0.90625, prec 0.067406, recall 0.866214
2017-12-10T14:46:24.597739: step 4259, loss 0.377959, acc 0.921875, prec 0.0674348, recall 0.866279
2017-12-10T14:46:24.784417: step 4260, loss 0.160785, acc 0.921875, prec 0.0674284, recall 0.866279
2017-12-10T14:46:24.972610: step 4261, loss 0.156733, acc 0.9375, prec 0.0674234, recall 0.866279
2017-12-10T14:46:25.164619: step 4262, loss 0.258761, acc 0.9375, prec 0.0674359, recall 0.866311
2017-12-10T14:46:25.352933: step 4263, loss 0.132649, acc 0.9375, prec 0.0674308, recall 0.866311
2017-12-10T14:46:25.539086: step 4264, loss 0.220038, acc 0.9375, prec 0.0674784, recall 0.866409
2017-12-10T14:46:25.730508: step 4265, loss 0.19367, acc 0.90625, prec 0.0675059, recall 0.866473
2017-12-10T14:46:25.918067: step 4266, loss 0.337391, acc 0.953125, prec 0.0675724, recall 0.866602
2017-12-10T14:46:26.104029: step 4267, loss 0.25057, acc 0.921875, prec 0.067566, recall 0.866602
2017-12-10T14:46:26.291352: step 4268, loss 0.149319, acc 0.984375, prec 0.0675999, recall 0.866667
2017-12-10T14:46:26.480475: step 4269, loss 0.0499451, acc 0.96875, prec 0.0676325, recall 0.866731
2017-12-10T14:46:26.667210: step 4270, loss 0.19177, acc 0.953125, prec 0.0676287, recall 0.866731
2017-12-10T14:46:26.855963: step 4271, loss 0.0661715, acc 0.984375, prec 0.0676274, recall 0.866731
2017-12-10T14:46:27.045596: step 4272, loss 0.0603021, acc 0.984375, prec 0.0676612, recall 0.866795
2017-12-10T14:46:27.237484: step 4273, loss 0.108219, acc 0.953125, prec 0.0676574, recall 0.866795
2017-12-10T14:46:27.430858: step 4274, loss 0.272235, acc 0.921875, prec 0.0676686, recall 0.866827
2017-12-10T14:46:27.623076: step 4275, loss 0.405148, acc 0.984375, prec 0.06772, recall 0.866924
2017-12-10T14:46:27.811105: step 4276, loss 0.0153864, acc 1, prec 0.06772, recall 0.866924
2017-12-10T14:46:27.995599: step 4277, loss 0.192167, acc 0.96875, prec 0.067735, recall 0.866956
2017-12-10T14:46:28.187939: step 4278, loss 0.0217044, acc 0.984375, prec 0.0677513, recall 0.866988
2017-12-10T14:46:28.379637: step 4279, loss 0.122302, acc 0.96875, prec 0.0677487, recall 0.866988
2017-12-10T14:46:28.567894: step 4280, loss 0.190706, acc 0.96875, prec 0.0677462, recall 0.866988
2017-12-10T14:46:28.755950: step 4281, loss 0.0105711, acc 1, prec 0.0677637, recall 0.86702
2017-12-10T14:46:28.945482: step 4282, loss 0.0223639, acc 1, prec 0.0677988, recall 0.867084
2017-12-10T14:46:29.144615: step 4283, loss 0.188427, acc 1, prec 0.0678339, recall 0.867148
2017-12-10T14:46:29.335853: step 4284, loss 3.05785, acc 0.984375, prec 0.0678515, recall 0.866971
2017-12-10T14:46:29.526480: step 4285, loss 0.0536638, acc 0.984375, prec 0.0678853, recall 0.867035
2017-12-10T14:46:29.718684: step 4286, loss 0.190553, acc 0.9375, prec 0.0678802, recall 0.867035
2017-12-10T14:46:29.906506: step 4287, loss 0.535165, acc 0.90625, prec 0.0678725, recall 0.867035
2017-12-10T14:46:30.095287: step 4288, loss 0.117655, acc 0.9375, prec 0.0679025, recall 0.867099
2017-12-10T14:46:30.282982: step 4289, loss 0.0515126, acc 0.96875, prec 0.0679, recall 0.867099
2017-12-10T14:46:30.472000: step 4290, loss 0.183192, acc 0.953125, prec 0.0679137, recall 0.867131
2017-12-10T14:46:30.665630: step 4291, loss 0.224583, acc 0.953125, prec 0.0679274, recall 0.867163
2017-12-10T14:46:30.852751: step 4292, loss 0.141353, acc 0.96875, prec 0.0679774, recall 0.867259
2017-12-10T14:46:31.041824: step 4293, loss 0.0570628, acc 0.96875, prec 0.0679924, recall 0.867291
2017-12-10T14:46:31.232826: step 4294, loss 0.240382, acc 0.953125, prec 0.0680236, recall 0.867354
2017-12-10T14:46:31.425437: step 4295, loss 0.0204614, acc 0.984375, prec 0.0680399, recall 0.867386
2017-12-10T14:46:31.617707: step 4296, loss 0.0965639, acc 0.96875, prec 0.0680373, recall 0.867386
2017-12-10T14:46:31.804043: step 4297, loss 0.105937, acc 0.953125, prec 0.068051, recall 0.867418
2017-12-10T14:46:31.994838: step 4298, loss 0.235693, acc 0.921875, prec 0.0680621, recall 0.86745
2017-12-10T14:46:32.185627: step 4299, loss 0.113762, acc 0.984375, prec 0.0680609, recall 0.86745
2017-12-10T14:46:32.371128: step 4300, loss 0.114998, acc 0.953125, prec 0.068057, recall 0.86745
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_3/1512934336/checkpoints/model-4300

2017-12-10T14:46:33.520565: step 4301, loss 0.285809, acc 0.9375, prec 0.0680694, recall 0.867481
2017-12-10T14:46:33.710732: step 4302, loss 0.173908, acc 0.921875, prec 0.068063, recall 0.867481
2017-12-10T14:46:33.897637: step 4303, loss 0.177588, acc 0.953125, prec 0.0680942, recall 0.867545
2017-12-10T14:46:34.084254: step 4304, loss 0.817203, acc 0.875, prec 0.068084, recall 0.867545
2017-12-10T14:46:34.270831: step 4305, loss 0.0515516, acc 0.984375, prec 0.0681703, recall 0.867703
2017-12-10T14:46:34.456600: step 4306, loss 0.245674, acc 0.921875, prec 0.0681989, recall 0.867767
2017-12-10T14:46:34.643156: step 4307, loss 1.00027, acc 0.953125, prec 0.0682126, recall 0.867798
2017-12-10T14:46:34.834655: step 4308, loss 0.164252, acc 0.96875, prec 0.068245, recall 0.867861
2017-12-10T14:46:35.025135: step 4309, loss 0.0852834, acc 0.984375, prec 0.0682437, recall 0.867861
2017-12-10T14:46:35.211789: step 4310, loss 0.058115, acc 0.96875, prec 0.0682412, recall 0.867861
2017-12-10T14:46:35.398429: step 4311, loss 0.174257, acc 0.96875, prec 0.0682911, recall 0.867956
2017-12-10T14:46:35.585095: step 4312, loss 0.577487, acc 0.953125, prec 0.0683048, recall 0.867988
2017-12-10T14:46:35.772235: step 4313, loss 0.0267339, acc 0.984375, prec 0.0683035, recall 0.867988
2017-12-10T14:46:35.958867: step 4314, loss 0.0906593, acc 0.96875, prec 0.0683184, recall 0.868019
2017-12-10T14:46:36.150261: step 4315, loss 0.136041, acc 0.921875, prec 0.0683295, recall 0.868051
2017-12-10T14:46:36.337866: step 4316, loss 0.298891, acc 0.875, prec 0.0683542, recall 0.868114
2017-12-10T14:46:36.526488: step 4317, loss 0.265701, acc 0.890625, prec 0.0683453, recall 0.868114
2017-12-10T14:46:36.714083: step 4318, loss 0.0355478, acc 0.984375, prec 0.068344, recall 0.868114
2017-12-10T14:46:36.905501: step 4319, loss 0.124432, acc 0.953125, prec 0.0683401, recall 0.868114
2017-12-10T14:46:37.095855: step 4320, loss 0.126823, acc 0.953125, prec 0.0683538, recall 0.868145
2017-12-10T14:46:37.282342: step 4321, loss 0.31563, acc 0.953125, prec 0.0683674, recall 0.868176
2017-12-10T14:46:37.469447: step 4322, loss 0.0665959, acc 0.984375, prec 0.0683661, recall 0.868176
2017-12-10T14:46:37.662333: step 4323, loss 0.84717, acc 0.9375, prec 0.0683785, recall 0.868208
2017-12-10T14:46:37.854914: step 4324, loss 0.0753391, acc 0.96875, prec 0.0683934, recall 0.868239
2017-12-10T14:46:38.041621: step 4325, loss 0.0813215, acc 0.953125, prec 0.068407, recall 0.868271
2017-12-10T14:46:38.225297: step 4326, loss 0.38116, acc 0.921875, prec 0.0684006, recall 0.868271
2017-12-10T14:46:38.415688: step 4327, loss 0.206482, acc 0.921875, prec 0.0684117, recall 0.868302
2017-12-10T14:46:38.603378: step 4328, loss 0.589182, acc 0.96875, prec 0.0684441, recall 0.868365
2017-12-10T14:46:38.794111: step 4329, loss 0.364394, acc 0.9375, prec 0.0684739, recall 0.868427
2017-12-10T14:46:38.980414: step 4330, loss 0.139874, acc 0.96875, prec 0.0684888, recall 0.868459
2017-12-10T14:46:39.165177: step 4331, loss 0.547365, acc 0.9375, prec 0.0684836, recall 0.868459
2017-12-10T14:46:39.352346: step 4332, loss 0.150994, acc 1, prec 0.0685011, recall 0.86849
2017-12-10T14:46:39.538697: step 4333, loss 0.164046, acc 0.96875, prec 0.0684985, recall 0.86849
2017-12-10T14:46:39.729882: step 4334, loss 0.190848, acc 0.9375, prec 0.0684934, recall 0.86849
2017-12-10T14:46:39.917306: step 4335, loss 0.355692, acc 0.953125, prec 0.0685245, recall 0.868552
2017-12-10T14:46:40.105607: step 4336, loss 0.0839596, acc 0.953125, prec 0.0685206, recall 0.868552
2017-12-10T14:46:40.291206: step 4337, loss 0.191849, acc 0.9375, prec 0.0685155, recall 0.868552
2017-12-10T14:46:40.483876: step 4338, loss 0.274265, acc 0.921875, prec 0.0685091, recall 0.868552
2017-12-10T14:46:40.672511: step 4339, loss 0.312486, acc 0.9375, prec 0.0685214, recall 0.868584
2017-12-10T14:46:40.862020: step 4340, loss 0.170689, acc 0.953125, prec 0.0685175, recall 0.868584
2017-12-10T14:46:41.048054: step 4341, loss 0.395361, acc 0.890625, prec 0.0685086, recall 0.868584
2017-12-10T14:46:41.238647: step 4342, loss 0.0748601, acc 0.953125, prec 0.0685047, recall 0.868584
2017-12-10T14:46:41.424877: step 4343, loss 0.101969, acc 0.96875, prec 0.0685021, recall 0.868584
2017-12-10T14:46:41.614867: step 4344, loss 0.178723, acc 0.9375, prec 0.0685145, recall 0.868615
2017-12-10T14:46:41.803219: step 4345, loss 0.121414, acc 0.953125, prec 0.0685106, recall 0.868615
2017-12-10T14:46:41.993188: step 4346, loss 0.127922, acc 0.984375, prec 0.0685268, recall 0.868646
2017-12-10T14:46:42.180091: step 4347, loss 0.0688104, acc 0.953125, prec 0.0685229, recall 0.868646
2017-12-10T14:46:42.366284: step 4348, loss 0.0799907, acc 0.96875, prec 0.0685378, recall 0.868677
2017-12-10T14:46:42.553629: step 4349, loss 0.0309944, acc 0.984375, prec 0.0685889, recall 0.868771
2017-12-10T14:46:42.741900: step 4350, loss 0.22523, acc 0.9375, prec 0.0685837, recall 0.868771
2017-12-10T14:46:42.933500: step 4351, loss 0.10668, acc 0.9375, prec 0.0685786, recall 0.868771
2017-12-10T14:46:43.121025: step 4352, loss 0.0214869, acc 0.984375, prec 0.0685773, recall 0.868771
2017-12-10T14:46:43.310029: step 4353, loss 0.101071, acc 1, prec 0.0686122, recall 0.868833
2017-12-10T14:46:43.501239: step 4354, loss 0.268518, acc 0.953125, prec 0.0686258, recall 0.868864
2017-12-10T14:46:43.689412: step 4355, loss 0.22916, acc 0.96875, prec 0.0686232, recall 0.868864
2017-12-10T14:46:43.880861: step 4356, loss 0.12689, acc 0.953125, prec 0.0686194, recall 0.868864
2017-12-10T14:46:44.074151: step 4357, loss 0.23012, acc 0.9375, prec 0.0686142, recall 0.868864
2017-12-10T14:46:44.268529: step 4358, loss 0.194848, acc 0.953125, prec 0.0686278, recall 0.868895
2017-12-10T14:46:44.458457: step 4359, loss 0.0602492, acc 0.984375, prec 0.0686265, recall 0.868895
2017-12-10T14:46:44.648320: step 4360, loss 0.0954516, acc 0.984375, prec 0.0686252, recall 0.868895
2017-12-10T14:46:44.838495: step 4361, loss 0.0266603, acc 0.984375, prec 0.068624, recall 0.868895
2017-12-10T14:46:45.028663: step 4362, loss 0.00105532, acc 1, prec 0.068624, recall 0.868895
2017-12-10T14:46:45.213535: step 4363, loss 0.0746059, acc 0.984375, prec 0.0686401, recall 0.868926
2017-12-10T14:46:45.403729: step 4364, loss 0.0634633, acc 0.984375, prec 0.0686388, recall 0.868926
2017-12-10T14:46:45.592165: step 4365, loss 0.148665, acc 0.9375, prec 0.0686511, recall 0.868957
2017-12-10T14:46:45.781028: step 4366, loss 0.148556, acc 0.96875, prec 0.0686486, recall 0.868957
2017-12-10T14:46:45.969694: step 4367, loss 0.0446518, acc 0.96875, prec 0.0686634, recall 0.868988
2017-12-10T14:46:46.156785: step 4368, loss 0.141123, acc 0.96875, prec 0.0686608, recall 0.868988
2017-12-10T14:46:46.344076: step 4369, loss 0.000794755, acc 1, prec 0.0686957, recall 0.86905
2017-12-10T14:46:46.534823: step 4370, loss 0.0760124, acc 0.984375, prec 0.0686944, recall 0.86905
2017-12-10T14:46:46.719294: step 4371, loss 0.0140274, acc 1, prec 0.0686944, recall 0.86905
2017-12-10T14:46:46.906436: step 4372, loss 0.103615, acc 0.96875, prec 0.0686919, recall 0.86905
2017-12-10T14:46:47.094492: step 4373, loss 0.0136143, acc 1, prec 0.0687093, recall 0.869081
2017-12-10T14:46:47.282974: step 4374, loss 0.000173694, acc 1, prec 0.0687093, recall 0.869081
2017-12-10T14:46:47.468187: step 4375, loss 0.0519816, acc 0.984375, prec 0.068708, recall 0.869081
2017-12-10T14:46:47.655973: step 4376, loss 0.214424, acc 0.96875, prec 0.0687403, recall 0.869143
2017-12-10T14:46:47.845376: step 4377, loss 0.00120049, acc 1, prec 0.0687403, recall 0.869143
2017-12-10T14:46:48.034539: step 4378, loss 0.0294214, acc 0.984375, prec 0.0687564, recall 0.869174
2017-12-10T14:46:48.223737: step 4379, loss 0.00110955, acc 1, prec 0.0687564, recall 0.869174
2017-12-10T14:46:48.406871: step 4380, loss 0.566724, acc 0.96875, prec 0.0688061, recall 0.869267
2017-12-10T14:46:48.597237: step 4381, loss 0.00917583, acc 1, prec 0.0688061, recall 0.869267
2017-12-10T14:46:48.788263: step 4382, loss 0.368941, acc 1, prec 0.068841, recall 0.869329
2017-12-10T14:46:48.979672: step 4383, loss 0.00682348, acc 1, prec 0.068841, recall 0.869329
2017-12-10T14:46:49.169505: step 4384, loss 0.00156192, acc 1, prec 0.068841, recall 0.869329
2017-12-10T14:46:49.354439: step 4385, loss 0.00827424, acc 1, prec 0.068841, recall 0.869329
2017-12-10T14:46:49.541965: step 4386, loss 0.000638408, acc 1, prec 0.068841, recall 0.869329
2017-12-10T14:46:49.726234: step 4387, loss 0.118188, acc 0.984375, prec 0.0688571, recall 0.86936
2017-12-10T14:46:49.915949: step 4388, loss 0.301455, acc 0.984375, prec 0.0688733, recall 0.869391
2017-12-10T14:46:50.106767: step 4389, loss 2.75824, acc 0.921875, prec 0.0689042, recall 0.869042
2017-12-10T14:46:50.299157: step 4390, loss 0.0242056, acc 1, prec 0.0689216, recall 0.869073
2017-12-10T14:46:50.485360: step 4391, loss 9.66447, acc 0.953125, prec 0.0689539, recall 0.86893
2017-12-10T14:46:50.677759: step 4392, loss 0.27453, acc 0.921875, prec 0.0689475, recall 0.86893
2017-12-10T14:46:50.864546: step 4393, loss 0.0949661, acc 0.953125, prec 0.0689436, recall 0.86893
2017-12-10T14:46:51.055106: step 4394, loss 0.255941, acc 0.890625, prec 0.0689346, recall 0.86893
2017-12-10T14:46:51.242887: step 4395, loss 0.281142, acc 0.96875, prec 0.068932, recall 0.86893
2017-12-10T14:46:51.435052: step 4396, loss 0.202398, acc 0.953125, prec 0.0689629, recall 0.868991
2017-12-10T14:46:51.623773: step 4397, loss 0.384196, acc 0.875, prec 0.0689874, recall 0.869053
2017-12-10T14:46:51.815504: step 4398, loss 0.448412, acc 0.90625, prec 0.0689797, recall 0.869053
2017-12-10T14:46:52.004133: step 4399, loss 0.695446, acc 0.859375, prec 0.0689855, recall 0.869084
2017-12-10T14:46:52.191874: step 4400, loss 0.382194, acc 0.890625, prec 0.0689765, recall 0.869084
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_3/1512934336/checkpoints/model-4400

2017-12-10T14:46:53.256080: step 4401, loss 0.932606, acc 0.796875, prec 0.0689771, recall 0.869115
2017-12-10T14:46:53.442782: step 4402, loss 0.338836, acc 0.90625, prec 0.0689868, recall 0.869146
2017-12-10T14:46:53.629700: step 4403, loss 0.723033, acc 0.8125, prec 0.0689887, recall 0.869176
2017-12-10T14:46:53.818670: step 4404, loss 0.212879, acc 0.859375, prec 0.0689771, recall 0.869176
2017-12-10T14:46:54.009661: step 4405, loss 0.887999, acc 0.765625, prec 0.0689578, recall 0.869176
2017-12-10T14:46:54.192753: step 4406, loss 0.613725, acc 0.84375, prec 0.0689623, recall 0.869207
2017-12-10T14:46:54.375426: step 4407, loss 0.307426, acc 0.921875, prec 0.0689559, recall 0.869207
2017-12-10T14:46:54.563648: step 4408, loss 0.930719, acc 0.828125, prec 0.0689591, recall 0.869238
2017-12-10T14:46:54.748531: step 4409, loss 0.462165, acc 0.890625, prec 0.0689848, recall 0.869299
2017-12-10T14:46:54.937602: step 4410, loss 0.505922, acc 0.921875, prec 0.0690131, recall 0.869361
2017-12-10T14:46:55.123806: step 4411, loss 0.458638, acc 0.9375, prec 0.06906, recall 0.869453
2017-12-10T14:46:55.313588: step 4412, loss 0.220138, acc 0.890625, prec 0.069051, recall 0.869453
2017-12-10T14:46:55.498605: step 4413, loss 0.0822321, acc 0.96875, prec 0.0690832, recall 0.869514
2017-12-10T14:46:55.685511: step 4414, loss 0.209758, acc 0.96875, prec 0.06915, recall 0.869637
2017-12-10T14:46:55.873069: step 4415, loss 0.240659, acc 0.90625, prec 0.0691423, recall 0.869637
2017-12-10T14:46:56.059906: step 4416, loss 0.0363281, acc 0.984375, prec 0.069141, recall 0.869637
2017-12-10T14:46:56.248360: step 4417, loss 0.254118, acc 0.9375, prec 0.0691532, recall 0.869667
2017-12-10T14:46:56.433175: step 4418, loss 0.176073, acc 0.9375, prec 0.069148, recall 0.869667
2017-12-10T14:46:56.628223: step 4419, loss 0.155101, acc 0.953125, prec 0.0691962, recall 0.869759
2017-12-10T14:46:56.820279: step 4420, loss 0.243531, acc 0.921875, prec 0.0692418, recall 0.86985
2017-12-10T14:46:57.007237: step 4421, loss 0.110894, acc 0.953125, prec 0.0692379, recall 0.86985
2017-12-10T14:46:57.196607: step 4422, loss 0.150873, acc 0.9375, prec 0.0692328, recall 0.86985
2017-12-10T14:46:57.383087: step 4423, loss 3.50032, acc 0.9375, prec 0.0692636, recall 0.869708
2017-12-10T14:46:57.574778: step 4424, loss 0.180795, acc 0.9375, prec 0.0692758, recall 0.869738
2017-12-10T14:46:57.763885: step 4425, loss 0.180435, acc 0.96875, prec 0.0692905, recall 0.869769
2017-12-10T14:46:57.956102: step 4426, loss 0.646439, acc 0.96875, prec 0.0693053, recall 0.869799
2017-12-10T14:46:58.150480: step 4427, loss 1.42771, acc 0.875, prec 0.0693296, recall 0.86986
2017-12-10T14:46:58.342689: step 4428, loss 0.309043, acc 0.90625, prec 0.0693392, recall 0.86989
2017-12-10T14:46:58.529187: step 4429, loss 0.223271, acc 0.9375, prec 0.0693514, recall 0.869921
2017-12-10T14:46:58.716677: step 4430, loss 0.405811, acc 0.890625, prec 0.0693596, recall 0.869951
2017-12-10T14:46:58.906892: step 4431, loss 0.164024, acc 0.953125, prec 0.0693731, recall 0.869981
2017-12-10T14:46:59.102622: step 4432, loss 0.293875, acc 0.875, prec 0.0693801, recall 0.870012
2017-12-10T14:46:59.290374: step 4433, loss 0.58008, acc 0.890625, prec 0.069371, recall 0.870012
2017-12-10T14:46:59.478301: step 4434, loss 0.551059, acc 0.8125, prec 0.0693556, recall 0.870012
2017-12-10T14:46:59.665817: step 4435, loss 0.732942, acc 0.796875, prec 0.0693734, recall 0.870072
2017-12-10T14:46:59.851772: step 4436, loss 0.252504, acc 0.890625, prec 0.0693644, recall 0.870072
2017-12-10T14:47:00.040604: step 4437, loss 0.370199, acc 0.890625, prec 0.0694246, recall 0.870193
2017-12-10T14:47:00.230140: step 4438, loss 0.645706, acc 0.921875, prec 0.0694527, recall 0.870254
2017-12-10T14:47:00.416407: step 4439, loss 0.298685, acc 0.890625, prec 0.0694956, recall 0.870345
2017-12-10T14:47:00.606189: step 4440, loss 0.0987151, acc 0.984375, prec 0.0694943, recall 0.870345
2017-12-10T14:47:00.794354: step 4441, loss 0.308819, acc 0.875, prec 0.0694839, recall 0.870345
2017-12-10T14:47:00.978975: step 4442, loss 0.14807, acc 0.953125, prec 0.0694801, recall 0.870345
2017-12-10T14:47:01.169209: step 4443, loss 0.363474, acc 0.890625, prec 0.0694883, recall 0.870375
2017-12-10T14:47:01.358239: step 4444, loss 0.741976, acc 0.921875, prec 0.0694819, recall 0.870375
2017-12-10T14:47:01.551806: step 4445, loss 0.356291, acc 0.96875, prec 0.0694793, recall 0.870375
2017-12-10T14:47:01.740846: step 4446, loss 0.264431, acc 0.9375, prec 0.0694741, recall 0.870375
2017-12-10T14:47:01.930679: step 4447, loss 0.0861949, acc 0.953125, prec 0.0694702, recall 0.870375
2017-12-10T14:47:02.117480: step 4448, loss 0.208587, acc 0.9375, prec 0.0694824, recall 0.870405
2017-12-10T14:47:02.309744: step 4449, loss 0.138625, acc 0.953125, prec 0.0694958, recall 0.870435
2017-12-10T14:47:02.498749: step 4450, loss 0.301983, acc 0.953125, prec 0.0695092, recall 0.870465
2017-12-10T14:47:02.684338: step 4451, loss 0.339666, acc 0.921875, prec 0.0695546, recall 0.870555
2017-12-10T14:47:02.871483: step 4452, loss 0.225029, acc 0.96875, prec 0.0695693, recall 0.870586
2017-12-10T14:47:03.059942: step 4453, loss 0.186796, acc 0.9375, prec 0.0695986, recall 0.870646
2017-12-10T14:47:03.251084: step 4454, loss 0.695322, acc 0.9375, prec 0.0696107, recall 0.870676
2017-12-10T14:47:03.440500: step 4455, loss 0.136367, acc 0.96875, prec 0.0696254, recall 0.870706
2017-12-10T14:47:03.627464: step 4456, loss 0.0267988, acc 1, prec 0.0696427, recall 0.870736
2017-12-10T14:47:03.818526: step 4457, loss 0.207114, acc 0.90625, prec 0.0696522, recall 0.870766
2017-12-10T14:47:04.004301: step 4458, loss 0.133912, acc 0.96875, prec 0.0696496, recall 0.870766
2017-12-10T14:47:04.195156: step 4459, loss 0.0654603, acc 0.96875, prec 0.0696643, recall 0.870796
2017-12-10T14:47:04.385867: step 4460, loss 0.172, acc 0.9375, prec 0.0696764, recall 0.870826
2017-12-10T14:47:04.576651: step 4461, loss 0.126745, acc 0.96875, prec 0.0696738, recall 0.870826
2017-12-10T14:47:04.762852: step 4462, loss 0.0995044, acc 0.984375, prec 0.0696725, recall 0.870826
2017-12-10T14:47:04.948074: step 4463, loss 0.108976, acc 0.984375, prec 0.0696712, recall 0.870826
2017-12-10T14:47:05.137691: step 4464, loss 1.54538, acc 0.96875, prec 0.0696699, recall 0.870624
2017-12-10T14:47:05.330237: step 4465, loss 0.0525489, acc 0.96875, prec 0.0696673, recall 0.870624
2017-12-10T14:47:05.514221: step 4466, loss 0.95813, acc 0.921875, prec 0.0696781, recall 0.870654
2017-12-10T14:47:05.703519: step 4467, loss 0.0826349, acc 0.96875, prec 0.0696928, recall 0.870684
2017-12-10T14:47:05.891948: step 4468, loss 0.12606, acc 0.921875, prec 0.0697036, recall 0.870714
2017-12-10T14:47:06.080984: step 4469, loss 0.139588, acc 0.96875, prec 0.0697355, recall 0.870773
2017-12-10T14:47:06.271635: step 4470, loss 0.0698768, acc 0.96875, prec 0.0697329, recall 0.870773
2017-12-10T14:47:06.460111: step 4471, loss 0.083144, acc 0.9375, prec 0.0697623, recall 0.870833
2017-12-10T14:47:06.648770: step 4472, loss 0.425077, acc 0.90625, prec 0.0697718, recall 0.870863
2017-12-10T14:47:06.816790: step 4473, loss 0.212511, acc 0.961538, prec 0.0697864, recall 0.870893
2017-12-10T14:47:07.014242: step 4474, loss 0.143347, acc 0.96875, prec 0.0698183, recall 0.870953
2017-12-10T14:47:07.202409: step 4475, loss 0.211414, acc 0.953125, prec 0.0698489, recall 0.871013
2017-12-10T14:47:07.389559: step 4476, loss 0.355938, acc 0.921875, prec 0.0698597, recall 0.871042
2017-12-10T14:47:07.575960: step 4477, loss 0.0487464, acc 0.96875, prec 0.0698916, recall 0.871102
2017-12-10T14:47:07.763682: step 4478, loss 0.604804, acc 0.90625, prec 0.069901, recall 0.871132
2017-12-10T14:47:07.949550: step 4479, loss 0.130693, acc 0.9375, prec 0.0699131, recall 0.871161
2017-12-10T14:47:08.135344: step 4480, loss 0.070794, acc 0.984375, prec 0.069929, recall 0.871191
2017-12-10T14:47:08.325674: step 4481, loss 0.227787, acc 0.921875, prec 0.069957, recall 0.871251
2017-12-10T14:47:08.515808: step 4482, loss 0.178453, acc 0.96875, prec 0.0699889, recall 0.87131
2017-12-10T14:47:08.702875: step 4483, loss 0.130715, acc 0.953125, prec 0.069985, recall 0.87131
2017-12-10T14:47:08.889323: step 4484, loss 0.22864, acc 0.96875, prec 0.0700341, recall 0.871399
2017-12-10T14:47:09.076636: step 4485, loss 0.151504, acc 0.96875, prec 0.0700487, recall 0.871429
2017-12-10T14:47:09.267765: step 4486, loss 0.112342, acc 0.96875, prec 0.0700633, recall 0.871458
2017-12-10T14:47:09.455542: step 4487, loss 0.52297, acc 0.890625, prec 0.0700715, recall 0.871488
2017-12-10T14:47:09.643870: step 4488, loss 0.127982, acc 0.96875, prec 0.0700861, recall 0.871517
2017-12-10T14:47:09.836994: step 4489, loss 0.339991, acc 0.953125, prec 0.0700994, recall 0.871547
2017-12-10T14:47:10.024542: step 4490, loss 0.0856775, acc 0.96875, prec 0.070114, recall 0.871577
2017-12-10T14:47:10.216660: step 4491, loss 0.132049, acc 0.96875, prec 0.0701459, recall 0.871636
2017-12-10T14:47:10.404627: step 4492, loss 0.0732382, acc 0.96875, prec 0.0701433, recall 0.871636
2017-12-10T14:47:10.591244: step 4493, loss 0.0471185, acc 0.984375, prec 0.0701592, recall 0.871665
2017-12-10T14:47:10.778361: step 4494, loss 0.126212, acc 0.953125, prec 0.0701553, recall 0.871665
2017-12-10T14:47:10.968522: step 4495, loss 0.0240843, acc 0.984375, prec 0.070154, recall 0.871665
2017-12-10T14:47:11.161708: step 4496, loss 0.251999, acc 0.9375, prec 0.0701488, recall 0.871665
2017-12-10T14:47:11.354592: step 4497, loss 0.142473, acc 0.984375, prec 0.0701647, recall 0.871695
2017-12-10T14:47:11.546844: step 4498, loss 0.212214, acc 0.953125, prec 0.070178, recall 0.871724
2017-12-10T14:47:11.736407: step 4499, loss 0.492277, acc 0.984375, prec 0.0702284, recall 0.871813
2017-12-10T14:47:11.927842: step 4500, loss 0.00998791, acc 1, prec 0.0702284, recall 0.871813
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_3/1512934336/checkpoints/model-4500

2017-12-10T14:47:13.113451: step 4501, loss 0.349504, acc 0.9375, prec 0.0702404, recall 0.871842
2017-12-10T14:47:13.302785: step 4502, loss 0.175294, acc 0.9375, prec 0.0702524, recall 0.871871
2017-12-10T14:47:13.490317: step 4503, loss 0.0685176, acc 0.96875, prec 0.070267, recall 0.871901
2017-12-10T14:47:13.676374: step 4504, loss 0.176329, acc 0.9375, prec 0.0703306, recall 0.872018
2017-12-10T14:47:13.863212: step 4505, loss 0.0066573, acc 1, prec 0.0703306, recall 0.872018
2017-12-10T14:47:14.056913: step 4506, loss 0.215374, acc 0.984375, prec 0.0703637, recall 0.872077
2017-12-10T14:47:14.254316: step 4507, loss 0.032819, acc 0.984375, prec 0.0703624, recall 0.872077
2017-12-10T14:47:14.443758: step 4508, loss 0.0529178, acc 0.984375, prec 0.0703611, recall 0.872077
2017-12-10T14:47:14.631521: step 4509, loss 0.129236, acc 0.96875, prec 0.0703756, recall 0.872106
2017-12-10T14:47:14.822350: step 4510, loss 0.0592719, acc 0.96875, prec 0.0703902, recall 0.872136
2017-12-10T14:47:15.008834: step 4511, loss 0.00767176, acc 1, prec 0.0704074, recall 0.872165
2017-12-10T14:47:15.197802: step 4512, loss 0.437323, acc 0.984375, prec 0.0704405, recall 0.872223
2017-12-10T14:47:15.388730: step 4513, loss 0.124259, acc 0.953125, prec 0.0704538, recall 0.872253
2017-12-10T14:47:15.577814: step 4514, loss 0.317351, acc 0.953125, prec 0.0704671, recall 0.872282
2017-12-10T14:47:15.764570: step 4515, loss 0.131484, acc 0.96875, prec 0.0704817, recall 0.872311
2017-12-10T14:47:15.950885: step 4516, loss 0.0343698, acc 0.984375, prec 0.0704803, recall 0.872311
2017-12-10T14:47:16.141704: step 4517, loss 0.245198, acc 0.953125, prec 0.070528, recall 0.872399
2017-12-10T14:47:16.332339: step 4518, loss 0.17927, acc 0.953125, prec 0.0705241, recall 0.872399
2017-12-10T14:47:16.521416: step 4519, loss 0.00754965, acc 1, prec 0.0705756, recall 0.872486
2017-12-10T14:47:16.709187: step 4520, loss 0.123686, acc 0.953125, prec 0.0705889, recall 0.872515
2017-12-10T14:47:16.895689: step 4521, loss 0.0517496, acc 0.984375, prec 0.0706048, recall 0.872545
2017-12-10T14:47:17.081870: step 4522, loss 0.0369433, acc 0.96875, prec 0.0706021, recall 0.872545
2017-12-10T14:47:17.268370: step 4523, loss 0.102862, acc 0.953125, prec 0.0705982, recall 0.872545
2017-12-10T14:47:17.457938: step 4524, loss 0.0350469, acc 0.984375, prec 0.0706313, recall 0.872603
2017-12-10T14:47:17.646828: step 4525, loss 0.0631054, acc 0.96875, prec 0.070663, recall 0.872661
2017-12-10T14:47:17.831548: step 4526, loss 0.294166, acc 0.953125, prec 0.0706763, recall 0.87269
2017-12-10T14:47:18.019320: step 4527, loss 0.388108, acc 0.921875, prec 0.0706697, recall 0.87269
2017-12-10T14:47:18.206368: step 4528, loss 0.0767045, acc 0.96875, prec 0.0706671, recall 0.87269
2017-12-10T14:47:18.394044: step 4529, loss 0.179658, acc 0.96875, prec 0.0706817, recall 0.872719
2017-12-10T14:47:18.581805: step 4530, loss 0.0454197, acc 0.96875, prec 0.0706962, recall 0.872748
2017-12-10T14:47:18.767705: step 4531, loss 0.0106115, acc 1, prec 0.0707306, recall 0.872806
2017-12-10T14:47:18.954719: step 4532, loss 0.069742, acc 0.953125, prec 0.0707267, recall 0.872806
2017-12-10T14:47:19.140761: step 4533, loss 0.0225005, acc 0.984375, prec 0.0707254, recall 0.872806
2017-12-10T14:47:19.331396: step 4534, loss 0.0369773, acc 0.984375, prec 0.0707412, recall 0.872835
2017-12-10T14:47:19.520303: step 4535, loss 0.00533044, acc 1, prec 0.0707584, recall 0.872864
2017-12-10T14:47:19.706274: step 4536, loss 0.181835, acc 0.96875, prec 0.0707729, recall 0.872893
2017-12-10T14:47:19.896721: step 4537, loss 0.0649734, acc 0.984375, prec 0.0707716, recall 0.872893
2017-12-10T14:47:20.083615: step 4538, loss 0.120202, acc 0.953125, prec 0.0707849, recall 0.872922
2017-12-10T14:47:20.274328: step 4539, loss 0.244771, acc 0.953125, prec 0.0707981, recall 0.872951
2017-12-10T14:47:20.464980: step 4540, loss 0.0310379, acc 0.984375, prec 0.0708139, recall 0.87298
2017-12-10T14:47:20.655433: step 4541, loss 0.0757264, acc 0.96875, prec 0.0708285, recall 0.873009
2017-12-10T14:47:20.841137: step 4542, loss 0.0148486, acc 1, prec 0.0708456, recall 0.873038
2017-12-10T14:47:21.030119: step 4543, loss 0.144399, acc 0.953125, prec 0.0708417, recall 0.873038
2017-12-10T14:47:21.217844: step 4544, loss 0.00844361, acc 1, prec 0.0708589, recall 0.873066
2017-12-10T14:47:21.412256: step 4545, loss 0.119254, acc 0.984375, prec 0.0708747, recall 0.873095
2017-12-10T14:47:21.599125: step 4546, loss 0.0335619, acc 1, prec 0.0708919, recall 0.873124
2017-12-10T14:47:21.786601: step 4547, loss 0.0535918, acc 0.96875, prec 0.0709236, recall 0.873182
2017-12-10T14:47:21.973228: step 4548, loss 0.0358974, acc 0.984375, prec 0.0709222, recall 0.873182
2017-12-10T14:47:22.158627: step 4549, loss 1.31559, acc 0.984375, prec 0.0709222, recall 0.872983
2017-12-10T14:47:22.349561: step 4550, loss 0.131808, acc 0.984375, prec 0.0709209, recall 0.872983
2017-12-10T14:47:22.537303: step 4551, loss 0.185866, acc 0.96875, prec 0.0709183, recall 0.872983
2017-12-10T14:47:22.727676: step 4552, loss 0.00478013, acc 1, prec 0.0709183, recall 0.872983
2017-12-10T14:47:22.912283: step 4553, loss 0.00314336, acc 1, prec 0.0709183, recall 0.872983
2017-12-10T14:47:23.102920: step 4554, loss 0.0594913, acc 0.984375, prec 0.070917, recall 0.872983
2017-12-10T14:47:23.290612: step 4555, loss 0.228456, acc 0.984375, prec 0.07095, recall 0.873041
2017-12-10T14:47:23.481873: step 4556, loss 0.195648, acc 0.953125, prec 0.0709632, recall 0.87307
2017-12-10T14:47:23.668297: step 4557, loss 0.0297784, acc 0.984375, prec 0.0709962, recall 0.873128
2017-12-10T14:47:23.856308: step 4558, loss 0.0100305, acc 1, prec 0.0710133, recall 0.873156
2017-12-10T14:47:24.040475: step 4559, loss 0.099374, acc 0.96875, prec 0.071045, recall 0.873214
2017-12-10T14:47:24.230725: step 4560, loss 0.00612184, acc 1, prec 0.0710621, recall 0.873243
2017-12-10T14:47:24.418108: step 4561, loss 0.340555, acc 0.921875, prec 0.0710727, recall 0.873271
2017-12-10T14:47:24.603908: step 4562, loss 0.08017, acc 0.984375, prec 0.0710886, recall 0.8733
2017-12-10T14:47:24.791485: step 4563, loss 0.242726, acc 0.984375, prec 0.0711387, recall 0.873386
2017-12-10T14:47:24.983537: step 4564, loss 0.18451, acc 0.953125, prec 0.071169, recall 0.873443
2017-12-10T14:47:25.174305: step 4565, loss 0.251875, acc 0.953125, prec 0.0711651, recall 0.873443
2017-12-10T14:47:25.363325: step 4566, loss 0.12301, acc 0.953125, prec 0.0711783, recall 0.873472
2017-12-10T14:47:25.553341: step 4567, loss 0.0779329, acc 0.96875, prec 0.0711756, recall 0.873472
2017-12-10T14:47:25.742604: step 4568, loss 0.0570154, acc 0.96875, prec 0.071173, recall 0.873472
2017-12-10T14:47:25.930781: step 4569, loss 0.284013, acc 0.921875, prec 0.0712349, recall 0.873587
2017-12-10T14:47:26.118175: step 4570, loss 0.0144171, acc 1, prec 0.0712349, recall 0.873587
2017-12-10T14:47:26.304733: step 4571, loss 0.251482, acc 0.921875, prec 0.0712284, recall 0.873587
2017-12-10T14:47:26.495323: step 4572, loss 0.133918, acc 0.96875, prec 0.0712258, recall 0.873587
2017-12-10T14:47:26.684338: step 4573, loss 0.447215, acc 0.9375, prec 0.0712205, recall 0.873587
2017-12-10T14:47:26.873700: step 4574, loss 0.116432, acc 0.984375, prec 0.0712363, recall 0.873615
2017-12-10T14:47:27.061357: step 4575, loss 0.2511, acc 0.921875, prec 0.0712469, recall 0.873644
2017-12-10T14:47:27.252626: step 4576, loss 0.00767472, acc 1, prec 0.071264, recall 0.873672
2017-12-10T14:47:27.444990: step 4577, loss 0.0485347, acc 1, prec 0.0712982, recall 0.873729
2017-12-10T14:47:27.633512: step 4578, loss 0.102145, acc 0.96875, prec 0.0713127, recall 0.873758
2017-12-10T14:47:27.819065: step 4579, loss 0.151961, acc 0.953125, prec 0.0713088, recall 0.873758
2017-12-10T14:47:28.008598: step 4580, loss 0.268687, acc 0.96875, prec 0.0713061, recall 0.873758
2017-12-10T14:47:28.196685: step 4581, loss 0.0646105, acc 0.953125, prec 0.0713193, recall 0.873786
2017-12-10T14:47:28.384639: step 4582, loss 0.259473, acc 0.953125, prec 0.0713325, recall 0.873815
2017-12-10T14:47:28.573911: step 4583, loss 0.546966, acc 0.96875, prec 0.0713299, recall 0.873815
2017-12-10T14:47:28.763137: step 4584, loss 0.0236789, acc 1, prec 0.0713299, recall 0.873815
2017-12-10T14:47:28.950690: step 4585, loss 0.0439886, acc 0.984375, prec 0.0713457, recall 0.873843
2017-12-10T14:47:29.143985: step 4586, loss 0.127657, acc 0.96875, prec 0.0713772, recall 0.8739
2017-12-10T14:47:29.331799: step 4587, loss 0.174412, acc 0.953125, prec 0.0713733, recall 0.8739
2017-12-10T14:47:29.520059: step 4588, loss 0.0669254, acc 0.984375, prec 0.0714062, recall 0.873957
2017-12-10T14:47:29.711113: step 4589, loss 0.0412925, acc 0.96875, prec 0.0714207, recall 0.873986
2017-12-10T14:47:29.898483: step 4590, loss 0.111433, acc 1, prec 0.0714549, recall 0.874042
2017-12-10T14:47:30.090048: step 4591, loss 0.0676188, acc 0.96875, prec 0.0714523, recall 0.874042
2017-12-10T14:47:30.280788: step 4592, loss 0.025648, acc 1, prec 0.0714694, recall 0.874071
2017-12-10T14:47:30.470242: step 4593, loss 0.0374114, acc 0.984375, prec 0.071468, recall 0.874071
2017-12-10T14:47:30.655674: step 4594, loss 0.0381927, acc 0.984375, prec 0.0715009, recall 0.874127
2017-12-10T14:47:30.844063: step 4595, loss 0.102834, acc 0.96875, prec 0.0715325, recall 0.874184
2017-12-10T14:47:31.030693: step 4596, loss 0.1079, acc 0.96875, prec 0.0715299, recall 0.874184
2017-12-10T14:47:31.225516: step 4597, loss 0.00478066, acc 1, prec 0.0715299, recall 0.874184
2017-12-10T14:47:31.426144: step 4598, loss 0.00350443, acc 1, prec 0.071547, recall 0.874212
2017-12-10T14:47:31.616380: step 4599, loss 0.0439082, acc 0.984375, prec 0.0715456, recall 0.874212
2017-12-10T14:47:31.801895: step 4600, loss 0.15817, acc 0.96875, prec 0.0715601, recall 0.874241
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_3/1512934336/checkpoints/model-4600

2017-12-10T14:47:32.989015: step 4601, loss 0.103815, acc 0.96875, prec 0.0715746, recall 0.874269
2017-12-10T14:47:33.176084: step 4602, loss 0.0502959, acc 1, prec 0.0716259, recall 0.874354
2017-12-10T14:47:33.362013: step 4603, loss 0.0664214, acc 0.96875, prec 0.0716403, recall 0.874382
2017-12-10T14:47:33.550266: step 4604, loss 0.0660032, acc 0.96875, prec 0.0716377, recall 0.874382
2017-12-10T14:47:33.737510: step 4605, loss 0.0132451, acc 1, prec 0.0716548, recall 0.87441
2017-12-10T14:47:33.922657: step 4606, loss 0.0922346, acc 0.984375, prec 0.0716534, recall 0.87441
2017-12-10T14:47:34.111498: step 4607, loss 3.56319, acc 0.96875, prec 0.0716692, recall 0.874242
2017-12-10T14:47:34.304101: step 4608, loss 0.0248123, acc 0.984375, prec 0.071685, recall 0.87427
2017-12-10T14:47:34.493183: step 4609, loss 0.462941, acc 0.96875, prec 0.0717336, recall 0.874355
2017-12-10T14:47:34.682589: step 4610, loss 0.192722, acc 1, prec 0.0717507, recall 0.874383
2017-12-10T14:47:34.871533: step 4611, loss 0.274336, acc 1, prec 0.0717849, recall 0.874439
2017-12-10T14:47:35.059034: step 4612, loss 0.0194628, acc 1, prec 0.071802, recall 0.874468
2017-12-10T14:47:35.243925: step 4613, loss 0.038551, acc 0.984375, prec 0.0718006, recall 0.874468
2017-12-10T14:47:35.429988: step 4614, loss 1.36938, acc 0.96875, prec 0.0717993, recall 0.874272
2017-12-10T14:47:35.620493: step 4615, loss 0.204459, acc 0.96875, prec 0.0717967, recall 0.874272
2017-12-10T14:47:35.807077: step 4616, loss 0.163503, acc 0.921875, prec 0.0718071, recall 0.8743
2017-12-10T14:47:35.993508: step 4617, loss 0.0466363, acc 0.96875, prec 0.0718387, recall 0.874356
2017-12-10T14:47:36.180465: step 4618, loss 0.356614, acc 0.84375, prec 0.0718254, recall 0.874356
2017-12-10T14:47:36.366687: step 4619, loss 0.171661, acc 0.9375, prec 0.0718714, recall 0.87444
2017-12-10T14:47:36.556813: step 4620, loss 0.355658, acc 0.9375, prec 0.0718832, recall 0.874469
2017-12-10T14:47:36.745822: step 4621, loss 0.171197, acc 0.9375, prec 0.0718779, recall 0.874469
2017-12-10T14:47:36.931746: step 4622, loss 0.444974, acc 0.84375, prec 0.0718647, recall 0.874469
2017-12-10T14:47:37.118029: step 4623, loss 0.469234, acc 0.875, prec 0.0719223, recall 0.874581
2017-12-10T14:47:37.302296: step 4624, loss 0.367575, acc 0.90625, prec 0.0719315, recall 0.874609
2017-12-10T14:47:37.493288: step 4625, loss 0.238829, acc 0.953125, prec 0.0719446, recall 0.874637
2017-12-10T14:47:37.678219: step 4626, loss 0.242801, acc 0.890625, prec 0.0719353, recall 0.874637
2017-12-10T14:47:37.866483: step 4627, loss 0.373663, acc 0.890625, prec 0.0719431, recall 0.874665
2017-12-10T14:47:38.051747: step 4628, loss 0.508403, acc 0.875, prec 0.0719496, recall 0.874693
2017-12-10T14:47:38.238488: step 4629, loss 0.150731, acc 0.9375, prec 0.0719613, recall 0.874721
2017-12-10T14:47:38.423462: step 4630, loss 0.383445, acc 0.859375, prec 0.0719495, recall 0.874721
2017-12-10T14:47:38.617219: step 4631, loss 0.184486, acc 0.953125, prec 0.0719625, recall 0.874749
2017-12-10T14:47:38.801231: step 4632, loss 0.254126, acc 0.9375, prec 0.0719743, recall 0.874777
2017-12-10T14:47:38.992714: step 4633, loss 0.305694, acc 0.921875, prec 0.0719847, recall 0.874805
2017-12-10T14:47:39.179504: step 4634, loss 0.382548, acc 0.921875, prec 0.0719781, recall 0.874805
2017-12-10T14:47:39.369382: step 4635, loss 0.256794, acc 0.890625, prec 0.0719859, recall 0.874833
2017-12-10T14:47:39.553867: step 4636, loss 0.347101, acc 0.9375, prec 0.0719976, recall 0.874861
2017-12-10T14:47:39.744367: step 4637, loss 0.202445, acc 0.9375, prec 0.0720094, recall 0.874888
2017-12-10T14:47:39.932719: step 4638, loss 0.152574, acc 0.953125, prec 0.0720054, recall 0.874888
2017-12-10T14:47:40.122749: step 4639, loss 0.142632, acc 0.9375, prec 0.0720001, recall 0.874888
2017-12-10T14:47:40.308704: step 4640, loss 0.17477, acc 0.953125, prec 0.0719962, recall 0.874888
2017-12-10T14:47:40.494088: step 4641, loss 0.141654, acc 0.96875, prec 0.0720106, recall 0.874916
2017-12-10T14:47:40.679308: step 4642, loss 0.109718, acc 0.96875, prec 0.072042, recall 0.874972
2017-12-10T14:47:40.866102: step 4643, loss 0.14372, acc 0.953125, prec 0.072038, recall 0.874972
2017-12-10T14:47:41.056318: step 4644, loss 0.148477, acc 0.96875, prec 0.0720694, recall 0.875028
2017-12-10T14:47:41.245512: step 4645, loss 0.183587, acc 0.9375, prec 0.0720641, recall 0.875028
2017-12-10T14:47:41.430864: step 4646, loss 0.308398, acc 0.953125, prec 0.0720602, recall 0.875028
2017-12-10T14:47:41.621428: step 4647, loss 0.143745, acc 0.984375, prec 0.0720588, recall 0.875028
2017-12-10T14:47:41.811671: step 4648, loss 0.0821823, acc 0.96875, prec 0.0720562, recall 0.875028
2017-12-10T14:47:42.003327: step 4649, loss 0.0243314, acc 0.984375, prec 0.0720719, recall 0.875056
2017-12-10T14:47:42.194935: step 4650, loss 0.146974, acc 0.953125, prec 0.072085, recall 0.875084
2017-12-10T14:47:42.384728: step 4651, loss 0.101486, acc 0.96875, prec 0.0721334, recall 0.875167
2017-12-10T14:47:42.573723: step 4652, loss 0.0974051, acc 0.953125, prec 0.0721464, recall 0.875195
2017-12-10T14:47:42.762751: step 4653, loss 0.316215, acc 0.96875, prec 0.0721778, recall 0.87525
2017-12-10T14:47:42.950523: step 4654, loss 0.0319024, acc 1, prec 0.0721948, recall 0.875278
2017-12-10T14:47:43.136779: step 4655, loss 0.00084714, acc 1, prec 0.0722118, recall 0.875306
2017-12-10T14:47:43.320940: step 4656, loss 0.134824, acc 0.984375, prec 0.0722615, recall 0.875389
2017-12-10T14:47:43.507657: step 4657, loss 0.0770357, acc 0.984375, prec 0.0722602, recall 0.875389
2017-12-10T14:47:43.695636: step 4658, loss 0.197771, acc 0.96875, prec 0.0722576, recall 0.875389
2017-12-10T14:47:43.883581: step 4659, loss 0.181235, acc 0.953125, prec 0.0722706, recall 0.875416
2017-12-10T14:47:44.084411: step 4660, loss 1.55595, acc 0.984375, prec 0.0722876, recall 0.87525
2017-12-10T14:47:44.280331: step 4661, loss 0.001606, acc 1, prec 0.0723046, recall 0.875277
2017-12-10T14:47:44.468956: step 4662, loss 0.030892, acc 0.984375, prec 0.0723373, recall 0.875333
2017-12-10T14:47:44.664529: step 4663, loss 0.909329, acc 1, prec 0.0723713, recall 0.875388
2017-12-10T14:47:44.854958: step 4664, loss 1.27284, acc 0.96875, prec 0.07237, recall 0.875194
2017-12-10T14:47:45.047751: step 4665, loss 1.16921, acc 0.953125, prec 0.072383, recall 0.875222
2017-12-10T14:47:45.237403: step 4666, loss 0.347937, acc 0.921875, prec 0.0723934, recall 0.875249
2017-12-10T14:47:45.423279: step 4667, loss 0.144522, acc 0.96875, prec 0.0724077, recall 0.875277
2017-12-10T14:47:45.612058: step 4668, loss 0.141418, acc 0.9375, prec 0.0724024, recall 0.875277
2017-12-10T14:47:45.799280: step 4669, loss 0.192023, acc 0.953125, prec 0.0724324, recall 0.875332
2017-12-10T14:47:45.989959: step 4670, loss 0.197946, acc 0.90625, prec 0.0724415, recall 0.87536
2017-12-10T14:47:46.177538: step 4671, loss 0.182197, acc 0.9375, prec 0.0724362, recall 0.87536
2017-12-10T14:47:46.367361: step 4672, loss 0.149641, acc 0.9375, prec 0.0724648, recall 0.875415
2017-12-10T14:47:46.557894: step 4673, loss 0.453625, acc 0.875, prec 0.0724712, recall 0.875443
2017-12-10T14:47:46.744543: step 4674, loss 0.435469, acc 0.890625, prec 0.0724789, recall 0.87547
2017-12-10T14:47:46.929542: step 4675, loss 0.953547, acc 0.8125, prec 0.072463, recall 0.87547
2017-12-10T14:47:47.120584: step 4676, loss 0.405614, acc 0.859375, prec 0.072485, recall 0.875525
2017-12-10T14:47:47.308603: step 4677, loss 0.101197, acc 0.953125, prec 0.072481, recall 0.875525
2017-12-10T14:47:47.495073: step 4678, loss 0.693842, acc 0.796875, prec 0.0724977, recall 0.87558
2017-12-10T14:47:47.683063: step 4679, loss 0.467834, acc 0.875, prec 0.0724871, recall 0.87558
2017-12-10T14:47:47.871784: step 4680, loss 0.511041, acc 0.859375, prec 0.0724921, recall 0.875608
2017-12-10T14:47:48.057380: step 4681, loss 0.37433, acc 0.90625, prec 0.0725011, recall 0.875635
2017-12-10T14:47:48.243076: step 4682, loss 0.394888, acc 0.890625, prec 0.0724919, recall 0.875635
2017-12-10T14:47:48.426846: step 4683, loss 0.2779, acc 0.890625, prec 0.0725335, recall 0.875717
2017-12-10T14:47:48.614430: step 4684, loss 0.121878, acc 0.953125, prec 0.0725464, recall 0.875745
2017-12-10T14:47:48.803779: step 4685, loss 0.243365, acc 0.9375, prec 0.0725581, recall 0.875772
2017-12-10T14:47:48.993255: step 4686, loss 0.339803, acc 0.90625, prec 0.0725501, recall 0.875772
2017-12-10T14:47:49.180508: step 4687, loss 0.240243, acc 0.96875, prec 0.0725475, recall 0.875772
2017-12-10T14:47:49.370369: step 4688, loss 0.29207, acc 0.90625, prec 0.0725395, recall 0.875772
2017-12-10T14:47:49.559670: step 4689, loss 0.050707, acc 0.96875, prec 0.0725369, recall 0.875772
2017-12-10T14:47:49.743686: step 4690, loss 0.319481, acc 0.9375, prec 0.0725485, recall 0.8758
2017-12-10T14:47:49.931848: step 4691, loss 0.517164, acc 0.90625, prec 0.0725575, recall 0.875827
2017-12-10T14:47:50.118152: step 4692, loss 0.183534, acc 0.984375, prec 0.0725562, recall 0.875827
2017-12-10T14:47:50.305701: step 4693, loss 0.12318, acc 0.96875, prec 0.0725705, recall 0.875854
2017-12-10T14:47:50.492143: step 4694, loss 0.326279, acc 0.9375, prec 0.0725652, recall 0.875854
2017-12-10T14:47:50.679711: step 4695, loss 0.0342967, acc 0.984375, prec 0.0725638, recall 0.875854
2017-12-10T14:47:50.868450: step 4696, loss 4.82418, acc 0.96875, prec 0.0725625, recall 0.875661
2017-12-10T14:47:51.058483: step 4697, loss 0.0853857, acc 0.984375, prec 0.0725612, recall 0.875661
2017-12-10T14:47:51.245479: step 4698, loss 0.00612168, acc 1, prec 0.0725781, recall 0.875689
2017-12-10T14:47:51.434329: step 4699, loss 0.0545057, acc 0.96875, prec 0.0725924, recall 0.875716
2017-12-10T14:47:51.623930: step 4700, loss 0.11336, acc 0.96875, prec 0.0725898, recall 0.875716
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_3/1512934336/checkpoints/model-4700

2017-12-10T14:47:52.874719: step 4701, loss 0.0202044, acc 1, prec 0.0726067, recall 0.875744
2017-12-10T14:47:53.065317: step 4702, loss 0.327572, acc 0.921875, prec 0.072617, recall 0.875771
2017-12-10T14:47:53.252410: step 4703, loss 0.047111, acc 0.96875, prec 0.0726144, recall 0.875771
2017-12-10T14:47:53.440034: step 4704, loss 0.333208, acc 0.90625, prec 0.0726064, recall 0.875771
2017-12-10T14:47:53.627224: step 4705, loss 0.281284, acc 0.90625, prec 0.0725985, recall 0.875771
2017-12-10T14:47:53.817063: step 4706, loss 0.0831346, acc 0.984375, prec 0.0725971, recall 0.875771
2017-12-10T14:47:54.004184: step 4707, loss 0.082389, acc 0.96875, prec 0.0725945, recall 0.875771
2017-12-10T14:47:54.194888: step 4708, loss 0.339474, acc 0.921875, prec 0.0726048, recall 0.875798
2017-12-10T14:47:54.384344: step 4709, loss 0.0387337, acc 0.96875, prec 0.0726529, recall 0.87588
2017-12-10T14:47:54.573908: step 4710, loss 0.114784, acc 0.953125, prec 0.0726659, recall 0.875908
2017-12-10T14:47:54.761991: step 4711, loss 0.223988, acc 0.921875, prec 0.0726762, recall 0.875935
2017-12-10T14:47:54.948184: step 4712, loss 0.0910468, acc 0.984375, prec 0.0726918, recall 0.875962
2017-12-10T14:47:55.132488: step 4713, loss 0.4103, acc 0.890625, prec 0.0726994, recall 0.875989
2017-12-10T14:47:55.322470: step 4714, loss 0.804333, acc 0.859375, prec 0.0727044, recall 0.876017
2017-12-10T14:47:55.512820: step 4715, loss 0.203059, acc 0.953125, prec 0.0727004, recall 0.876017
2017-12-10T14:47:55.699461: step 4716, loss 0.0205876, acc 1, prec 0.0727173, recall 0.876044
2017-12-10T14:47:55.901996: step 4717, loss 0.194107, acc 0.953125, prec 0.0727303, recall 0.876071
2017-12-10T14:47:56.089191: step 4718, loss 0.417197, acc 0.90625, prec 0.0727223, recall 0.876071
2017-12-10T14:47:56.276207: step 4719, loss 0.175851, acc 0.96875, prec 0.0727366, recall 0.876098
2017-12-10T14:47:56.463780: step 4720, loss 0.0415009, acc 1, prec 0.0727535, recall 0.876126
2017-12-10T14:47:56.649698: step 4721, loss 0.233121, acc 0.9375, prec 0.0727482, recall 0.876126
2017-12-10T14:47:56.839491: step 4722, loss 0.18303, acc 0.96875, prec 0.0727624, recall 0.876153
2017-12-10T14:47:57.031242: step 4723, loss 0.046386, acc 0.96875, prec 0.0727767, recall 0.87618
2017-12-10T14:47:57.218605: step 4724, loss 0.137578, acc 0.96875, prec 0.0728247, recall 0.876262
2017-12-10T14:47:57.407179: step 4725, loss 3.16067, acc 0.953125, prec 0.0728221, recall 0.876069
2017-12-10T14:47:57.598547: step 4726, loss 0.0595089, acc 0.984375, prec 0.0728546, recall 0.876124
2017-12-10T14:47:57.789788: step 4727, loss 0.183895, acc 0.953125, prec 0.0728506, recall 0.876124
2017-12-10T14:47:57.980589: step 4728, loss 0.244955, acc 0.9375, prec 0.0728453, recall 0.876124
2017-12-10T14:47:58.169886: step 4729, loss 0.215974, acc 0.953125, prec 0.0728582, recall 0.876151
2017-12-10T14:47:58.358662: step 4730, loss 0.176984, acc 0.96875, prec 0.0728724, recall 0.876178
2017-12-10T14:47:58.547561: step 4731, loss 0.343852, acc 0.921875, prec 0.0728658, recall 0.876178
2017-12-10T14:47:58.737910: step 4732, loss 0.0767396, acc 0.96875, prec 0.0728631, recall 0.876178
2017-12-10T14:47:58.925566: step 4733, loss 0.204092, acc 0.9375, prec 0.0728578, recall 0.876178
2017-12-10T14:47:59.121857: step 4734, loss 0.237987, acc 0.921875, prec 0.0728512, recall 0.876178
2017-12-10T14:47:59.310959: step 4735, loss 0.300069, acc 0.984375, prec 0.0728667, recall 0.876205
2017-12-10T14:47:59.501690: step 4736, loss 0.496027, acc 0.953125, prec 0.0728628, recall 0.876205
2017-12-10T14:47:59.688738: step 4737, loss 0.294282, acc 0.953125, prec 0.0728757, recall 0.876232
2017-12-10T14:47:59.877185: step 4738, loss 0.72002, acc 0.9375, prec 0.0728873, recall 0.876259
2017-12-10T14:48:00.065266: step 4739, loss 0.0812032, acc 0.96875, prec 0.0729015, recall 0.876286
2017-12-10T14:48:00.255212: step 4740, loss 0.0688284, acc 0.984375, prec 0.0729002, recall 0.876286
2017-12-10T14:48:00.444364: step 4741, loss 0.114135, acc 0.921875, prec 0.0729442, recall 0.876368
2017-12-10T14:48:00.632031: step 4742, loss 0.061157, acc 0.984375, prec 0.0729597, recall 0.876395
2017-12-10T14:48:00.818543: step 4743, loss 0.191362, acc 0.96875, prec 0.072974, recall 0.876422
2017-12-10T14:48:01.012054: step 4744, loss 0.162579, acc 0.953125, prec 0.07297, recall 0.876422
2017-12-10T14:48:01.203638: step 4745, loss 0.140197, acc 0.9375, prec 0.0729647, recall 0.876422
2017-12-10T14:48:01.392558: step 4746, loss 0.402763, acc 0.96875, prec 0.0729789, recall 0.876449
2017-12-10T14:48:01.584731: step 4747, loss 0.19826, acc 0.9375, prec 0.0730073, recall 0.876503
2017-12-10T14:48:01.773568: step 4748, loss 0.0912254, acc 0.953125, prec 0.073054, recall 0.876584
2017-12-10T14:48:01.960608: step 4749, loss 0.204315, acc 0.953125, prec 0.0730668, recall 0.876611
2017-12-10T14:48:02.148965: step 4750, loss 0.15932, acc 0.90625, prec 0.0730589, recall 0.876611
2017-12-10T14:48:02.339921: step 4751, loss 0.196484, acc 0.921875, prec 0.0730522, recall 0.876611
2017-12-10T14:48:02.526567: step 4752, loss 0.220051, acc 0.9375, prec 0.0730638, recall 0.876638
2017-12-10T14:48:02.713727: step 4753, loss 0.503195, acc 0.921875, prec 0.0730909, recall 0.876691
2017-12-10T14:48:02.900045: step 4754, loss 0.0820372, acc 0.953125, prec 0.0731037, recall 0.876718
2017-12-10T14:48:03.089791: step 4755, loss 0.157223, acc 0.953125, prec 0.0731166, recall 0.876745
2017-12-10T14:48:03.278079: step 4756, loss 0.371207, acc 0.96875, prec 0.0731477, recall 0.876799
2017-12-10T14:48:03.469416: step 4757, loss 0.0626213, acc 0.96875, prec 0.073145, recall 0.876799
2017-12-10T14:48:03.656388: step 4758, loss 0.429013, acc 0.890625, prec 0.0731357, recall 0.876799
2017-12-10T14:48:03.847118: step 4759, loss 0.174798, acc 0.921875, prec 0.0731459, recall 0.876826
2017-12-10T14:48:04.032765: step 4760, loss 0.101439, acc 0.984375, prec 0.0731446, recall 0.876826
2017-12-10T14:48:04.220073: step 4761, loss 0.0508275, acc 0.96875, prec 0.0731588, recall 0.876853
2017-12-10T14:48:04.409213: step 4762, loss 0.0951364, acc 0.96875, prec 0.0731561, recall 0.876853
2017-12-10T14:48:04.596987: step 4763, loss 0.0249352, acc 0.984375, prec 0.0731548, recall 0.876853
2017-12-10T14:48:04.783439: step 4764, loss 0.113553, acc 0.984375, prec 0.0731703, recall 0.87688
2017-12-10T14:48:04.977480: step 4765, loss 0.0657712, acc 0.96875, prec 0.0732013, recall 0.876933
2017-12-10T14:48:05.164995: step 4766, loss 0.0144017, acc 1, prec 0.0732013, recall 0.876933
2017-12-10T14:48:05.352659: step 4767, loss 0.0023172, acc 1, prec 0.0732013, recall 0.876933
2017-12-10T14:48:05.538606: step 4768, loss 0.0831136, acc 0.984375, prec 0.0732168, recall 0.87696
2017-12-10T14:48:05.727273: step 4769, loss 0.213424, acc 0.90625, prec 0.0732257, recall 0.876987
2017-12-10T14:48:05.912400: step 4770, loss 0.296793, acc 0.984375, prec 0.0732412, recall 0.877014
2017-12-10T14:48:06.098873: step 4771, loss 0.0596524, acc 0.984375, prec 0.0732399, recall 0.877014
2017-12-10T14:48:06.286996: step 4772, loss 0.206811, acc 0.96875, prec 0.0732372, recall 0.877014
2017-12-10T14:48:06.477098: step 4773, loss 0.48181, acc 0.96875, prec 0.0732683, recall 0.877067
2017-12-10T14:48:06.667548: step 4774, loss 0.0305023, acc 1, prec 0.0733019, recall 0.87712
2017-12-10T14:48:06.858489: step 4775, loss 5.18465, acc 0.984375, prec 0.0733019, recall 0.87693
2017-12-10T14:48:07.052210: step 4776, loss 0.197336, acc 0.953125, prec 0.0733316, recall 0.876983
2017-12-10T14:48:07.240559: step 4777, loss 0.0932365, acc 0.984375, prec 0.0733303, recall 0.876983
2017-12-10T14:48:07.429624: step 4778, loss 0.119107, acc 0.96875, prec 0.0733445, recall 0.87701
2017-12-10T14:48:07.617168: step 4779, loss 0.00240781, acc 1, prec 0.0733782, recall 0.877063
2017-12-10T14:48:07.801865: step 4780, loss 0.191699, acc 0.953125, prec 0.073391, recall 0.87709
2017-12-10T14:48:07.991850: step 4781, loss 0.23568, acc 0.953125, prec 0.073387, recall 0.87709
2017-12-10T14:48:08.180567: step 4782, loss 0.13152, acc 0.9375, prec 0.0733817, recall 0.87709
2017-12-10T14:48:08.370739: step 4783, loss 0.0158748, acc 1, prec 0.0733985, recall 0.877117
2017-12-10T14:48:08.556963: step 4784, loss 0.249183, acc 0.9375, prec 0.07341, recall 0.877144
2017-12-10T14:48:08.744172: step 4785, loss 0.204706, acc 0.953125, prec 0.0734397, recall 0.877197
2017-12-10T14:48:08.931115: step 4786, loss 0.0193391, acc 1, prec 0.0734565, recall 0.877223
2017-12-10T14:48:09.118955: step 4787, loss 0.187009, acc 0.96875, prec 0.0734538, recall 0.877223
2017-12-10T14:48:09.303610: step 4788, loss 0.290096, acc 0.953125, prec 0.0735171, recall 0.87733
2017-12-10T14:48:09.491904: step 4789, loss 0.283484, acc 0.9375, prec 0.0735286, recall 0.877356
2017-12-10T14:48:09.682781: step 4790, loss 0.182503, acc 0.921875, prec 0.0735388, recall 0.877383
2017-12-10T14:48:09.874468: step 4791, loss 0.531736, acc 0.9375, prec 0.0735334, recall 0.877383
2017-12-10T14:48:10.061133: step 4792, loss 0.319227, acc 0.921875, prec 0.0735267, recall 0.877383
2017-12-10T14:48:10.253034: step 4793, loss 0.210158, acc 0.9375, prec 0.073555, recall 0.877436
2017-12-10T14:48:10.442623: step 4794, loss 0.127335, acc 0.96875, prec 0.0735692, recall 0.877463
2017-12-10T14:48:10.627520: step 4795, loss 0.147986, acc 0.921875, prec 0.073613, recall 0.877542
2017-12-10T14:48:10.820612: step 4796, loss 0.0422425, acc 0.984375, prec 0.0736116, recall 0.877542
2017-12-10T14:48:11.006263: step 4797, loss 0.0913808, acc 0.953125, prec 0.0736412, recall 0.877595
2017-12-10T14:48:11.192032: step 4798, loss 0.566546, acc 0.90625, prec 0.0736836, recall 0.877675
2017-12-10T14:48:11.378583: step 4799, loss 0.128345, acc 0.9375, prec 0.0736951, recall 0.877701
2017-12-10T14:48:11.566404: step 4800, loss 0.182732, acc 0.953125, prec 0.0737583, recall 0.877807
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_3/1512934336/checkpoints/model-4800

2017-12-10T14:48:12.964040: step 4801, loss 0.199339, acc 0.953125, prec 0.0737711, recall 0.877833
2017-12-10T14:48:13.153966: step 4802, loss 0.309443, acc 0.921875, prec 0.073798, recall 0.877886
2017-12-10T14:48:13.341382: step 4803, loss 0.068647, acc 0.984375, prec 0.0738302, recall 0.877938
2017-12-10T14:48:13.528938: step 4804, loss 0.0417715, acc 0.984375, prec 0.0738289, recall 0.877938
2017-12-10T14:48:13.717581: step 4805, loss 0.0244584, acc 1, prec 0.0738457, recall 0.877965
2017-12-10T14:48:13.905214: step 4806, loss 1.48867, acc 0.9375, prec 0.0738417, recall 0.877775
2017-12-10T14:48:14.100800: step 4807, loss 0.00885885, acc 1, prec 0.0738417, recall 0.877775
2017-12-10T14:48:14.290124: step 4808, loss 0.187977, acc 0.9375, prec 0.0738531, recall 0.877802
2017-12-10T14:48:14.481030: step 4809, loss 0.134021, acc 0.96875, prec 0.073884, recall 0.877854
2017-12-10T14:48:14.669660: step 4810, loss 0.237857, acc 0.9375, prec 0.0738955, recall 0.877881
2017-12-10T14:48:14.855639: step 4811, loss 0.629064, acc 0.921875, prec 0.0738888, recall 0.877881
2017-12-10T14:48:15.044656: step 4812, loss 0.193796, acc 0.9375, prec 0.0739002, recall 0.877907
2017-12-10T14:48:15.230434: step 4813, loss 0.549033, acc 0.890625, prec 0.0738908, recall 0.877907
2017-12-10T14:48:15.415650: step 4814, loss 0.153239, acc 0.984375, prec 0.0739231, recall 0.87796
2017-12-10T14:48:15.603914: step 4815, loss 0.342735, acc 0.953125, prec 0.073919, recall 0.87796
2017-12-10T14:48:15.792121: step 4816, loss 0.999574, acc 0.875, prec 0.0739083, recall 0.87796
2017-12-10T14:48:15.981175: step 4817, loss 0.0522546, acc 0.96875, prec 0.0739224, recall 0.877986
2017-12-10T14:48:16.167841: step 4818, loss 0.0667899, acc 0.96875, prec 0.0739365, recall 0.878012
2017-12-10T14:48:16.356599: step 4819, loss 0.0804742, acc 0.953125, prec 0.073966, recall 0.878065
2017-12-10T14:48:16.543043: step 4820, loss 0.1625, acc 0.9375, prec 0.0739775, recall 0.878091
2017-12-10T14:48:16.729548: step 4821, loss 1.07055, acc 0.890625, prec 0.0739849, recall 0.878117
2017-12-10T14:48:16.916488: step 4822, loss 0.428108, acc 0.953125, prec 0.0740144, recall 0.878169
2017-12-10T14:48:17.104743: step 4823, loss 0.254228, acc 0.953125, prec 0.0740271, recall 0.878195
2017-12-10T14:48:17.295970: step 4824, loss 0.324496, acc 0.9375, prec 0.0740385, recall 0.878222
2017-12-10T14:48:17.486864: step 4825, loss 0.116361, acc 0.96875, prec 0.0740526, recall 0.878248
2017-12-10T14:48:17.675429: step 4826, loss 5.90292, acc 0.9375, prec 0.0740486, recall 0.878059
2017-12-10T14:48:17.865293: step 4827, loss 0.151513, acc 0.96875, prec 0.0740627, recall 0.878085
2017-12-10T14:48:18.052875: step 4828, loss 0.26235, acc 0.9375, prec 0.0740573, recall 0.878085
2017-12-10T14:48:18.238609: step 4829, loss 0.0860327, acc 0.953125, prec 0.0740533, recall 0.878085
2017-12-10T14:48:18.424966: step 4830, loss 0.468866, acc 0.9375, prec 0.0740647, recall 0.878112
2017-12-10T14:48:18.611944: step 4831, loss 0.193873, acc 0.953125, prec 0.0741109, recall 0.87819
2017-12-10T14:48:18.798226: step 4832, loss 0.383122, acc 0.9375, prec 0.0741223, recall 0.878216
2017-12-10T14:48:18.982113: step 4833, loss 0.0613222, acc 0.953125, prec 0.0741183, recall 0.878216
2017-12-10T14:48:19.166770: step 4834, loss 0.502626, acc 0.875, prec 0.0741076, recall 0.878216
2017-12-10T14:48:19.353126: step 4835, loss 0.187627, acc 0.921875, prec 0.0741176, recall 0.878242
2017-12-10T14:48:19.538399: step 4836, loss 0.329417, acc 0.90625, prec 0.0741096, recall 0.878242
2017-12-10T14:48:19.726293: step 4837, loss 0.313807, acc 0.875, prec 0.0741156, recall 0.878268
2017-12-10T14:48:19.911591: step 4838, loss 0.295033, acc 0.9375, prec 0.074127, recall 0.878294
2017-12-10T14:48:20.096485: step 4839, loss 0.565818, acc 0.890625, prec 0.0741343, recall 0.87832
2017-12-10T14:48:20.284887: step 4840, loss 0.541667, acc 0.859375, prec 0.0741223, recall 0.87832
2017-12-10T14:48:20.472436: step 4841, loss 0.198367, acc 0.9375, prec 0.0741337, recall 0.878347
2017-12-10T14:48:20.659318: step 4842, loss 0.419283, acc 0.875, prec 0.0741397, recall 0.878373
2017-12-10T14:48:20.850448: step 4843, loss 0.344787, acc 0.921875, prec 0.074133, recall 0.878373
2017-12-10T14:48:21.036916: step 4844, loss 0.749001, acc 0.890625, prec 0.0741236, recall 0.878373
2017-12-10T14:48:21.225039: step 4845, loss 0.538334, acc 0.890625, prec 0.0741477, recall 0.878425
2017-12-10T14:48:21.415799: step 4846, loss 0.201552, acc 0.9375, prec 0.0741423, recall 0.878425
2017-12-10T14:48:21.602675: step 4847, loss 0.0288467, acc 0.984375, prec 0.074141, recall 0.878425
2017-12-10T14:48:21.795294: step 4848, loss 0.119179, acc 0.984375, prec 0.0741564, recall 0.878451
2017-12-10T14:48:21.987046: step 4849, loss 0.108444, acc 0.96875, prec 0.0741704, recall 0.878477
2017-12-10T14:48:22.176350: step 4850, loss 0.102079, acc 0.96875, prec 0.0741677, recall 0.878477
2017-12-10T14:48:22.368667: step 4851, loss 0.0596759, acc 0.96875, prec 0.0741818, recall 0.878503
2017-12-10T14:48:22.555949: step 4852, loss 0.0698859, acc 0.953125, prec 0.0741778, recall 0.878503
2017-12-10T14:48:22.741354: step 4853, loss 0.18478, acc 0.96875, prec 0.0741918, recall 0.878529
2017-12-10T14:48:22.928624: step 4854, loss 2.41492, acc 0.96875, prec 0.0742239, recall 0.878393
2017-12-10T14:48:23.121771: step 4855, loss 0.218935, acc 0.921875, prec 0.0742172, recall 0.878393
2017-12-10T14:48:23.307579: step 4856, loss 0.225269, acc 0.96875, prec 0.0742145, recall 0.878393
2017-12-10T14:48:23.495223: step 4857, loss 0.153166, acc 0.9375, prec 0.0742426, recall 0.878445
2017-12-10T14:48:23.687008: step 4858, loss 0.185352, acc 0.96875, prec 0.0742399, recall 0.878445
2017-12-10T14:48:23.875727: step 4859, loss 0.196328, acc 0.953125, prec 0.0742359, recall 0.878445
2017-12-10T14:48:24.061730: step 4860, loss 0.0563827, acc 0.96875, prec 0.0742499, recall 0.878471
2017-12-10T14:48:24.254866: step 4861, loss 0.0484271, acc 0.984375, prec 0.0742653, recall 0.878497
2017-12-10T14:48:24.439755: step 4862, loss 4.58726, acc 0.984375, prec 0.074282, recall 0.878335
2017-12-10T14:48:24.629308: step 4863, loss 0.0205258, acc 0.984375, prec 0.0742974, recall 0.878361
2017-12-10T14:48:24.813973: step 4864, loss 0.138203, acc 0.9375, prec 0.074292, recall 0.878361
2017-12-10T14:48:25.000975: step 4865, loss 0.316329, acc 0.890625, prec 0.0742993, recall 0.878387
2017-12-10T14:48:25.186898: step 4866, loss 0.46234, acc 0.9375, prec 0.074294, recall 0.878387
2017-12-10T14:48:25.373214: step 4867, loss 0.17272, acc 0.9375, prec 0.0742886, recall 0.878387
2017-12-10T14:48:25.558073: step 4868, loss 0.282464, acc 0.9375, prec 0.0742832, recall 0.878387
2017-12-10T14:48:25.749407: step 4869, loss 0.307675, acc 0.890625, prec 0.0742739, recall 0.878387
2017-12-10T14:48:25.938569: step 4870, loss 0.611575, acc 0.890625, prec 0.0742645, recall 0.878387
2017-12-10T14:48:26.127770: step 4871, loss 0.484934, acc 0.921875, prec 0.0742745, recall 0.878413
2017-12-10T14:48:26.316445: step 4872, loss 0.102281, acc 0.9375, prec 0.0742858, recall 0.878439
2017-12-10T14:48:26.505123: step 4873, loss 0.221553, acc 0.9375, prec 0.0742972, recall 0.878465
2017-12-10T14:48:26.693009: step 4874, loss 0.255406, acc 0.90625, prec 0.0743058, recall 0.878491
2017-12-10T14:48:26.880218: step 4875, loss 0.522979, acc 0.875, prec 0.0743118, recall 0.878517
2017-12-10T14:48:27.068417: step 4876, loss 0.174908, acc 0.9375, prec 0.0743231, recall 0.878542
2017-12-10T14:48:27.254308: step 4877, loss 0.213842, acc 0.921875, prec 0.0743331, recall 0.878568
2017-12-10T14:48:27.445444: step 4878, loss 0.21552, acc 0.90625, prec 0.0743584, recall 0.87862
2017-12-10T14:48:27.634787: step 4879, loss 0.211527, acc 0.921875, prec 0.0743684, recall 0.878646
2017-12-10T14:48:27.828951: step 4880, loss 0.308269, acc 0.90625, prec 0.074377, recall 0.878672
2017-12-10T14:48:28.015894: step 4881, loss 0.0290213, acc 0.984375, prec 0.0743757, recall 0.878672
2017-12-10T14:48:28.203012: step 4882, loss 0.0134706, acc 1, prec 0.0743757, recall 0.878672
2017-12-10T14:48:28.387994: step 4883, loss 0.273944, acc 0.96875, prec 0.0744064, recall 0.878723
2017-12-10T14:48:28.578216: step 4884, loss 0.297673, acc 0.96875, prec 0.0744204, recall 0.878749
2017-12-10T14:48:28.767377: step 4885, loss 0.136868, acc 0.953125, prec 0.0744163, recall 0.878749
2017-12-10T14:48:28.961054: step 4886, loss 0.140126, acc 0.96875, prec 0.0744137, recall 0.878749
2017-12-10T14:48:29.155113: step 4887, loss 0.437172, acc 0.921875, prec 0.0744403, recall 0.878801
2017-12-10T14:48:29.345132: step 4888, loss 0.090876, acc 0.96875, prec 0.0744543, recall 0.878827
2017-12-10T14:48:29.533914: step 4889, loss 0.164938, acc 0.96875, prec 0.0744683, recall 0.878852
2017-12-10T14:48:29.725785: step 4890, loss 0.0714245, acc 0.984375, prec 0.0744836, recall 0.878878
2017-12-10T14:48:29.915100: step 4891, loss 0.175673, acc 0.9375, prec 0.0744782, recall 0.878878
2017-12-10T14:48:30.102755: step 4892, loss 0.0647064, acc 0.984375, prec 0.0744936, recall 0.878904
2017-12-10T14:48:30.291549: step 4893, loss 0.13758, acc 0.984375, prec 0.0745089, recall 0.878929
2017-12-10T14:48:30.478075: step 4894, loss 0.137744, acc 0.96875, prec 0.0745062, recall 0.878929
2017-12-10T14:48:30.666075: step 4895, loss 0.0425035, acc 0.96875, prec 0.0745202, recall 0.878955
2017-12-10T14:48:30.854482: step 4896, loss 0.123503, acc 0.984375, prec 0.0745355, recall 0.878981
2017-12-10T14:48:31.046559: step 4897, loss 0.0166763, acc 1, prec 0.0745355, recall 0.878981
2017-12-10T14:48:31.239144: step 4898, loss 0.0892552, acc 0.984375, prec 0.0745675, recall 0.879032
2017-12-10T14:48:31.435540: step 4899, loss 0.288425, acc 0.9375, prec 0.0745788, recall 0.879058
2017-12-10T14:48:31.621190: step 4900, loss 0.049225, acc 0.984375, prec 0.0745774, recall 0.879058
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_position_embedding_out_fold_3/1512934336/checkpoints/model-4900

2017-12-10T14:48:33.158667: step 4901, loss 0.125968, acc 0.9375, prec 0.0745887, recall 0.879084
2017-12-10T14:48:33.346514: step 4902, loss 0.165871, acc 1, prec 0.0746054, recall 0.879109
2017-12-10T14:48:33.541045: step 4903, loss 0.151522, acc 0.984375, prec 0.0746373, recall 0.87916
2017-12-10T14:48:33.730949: step 4904, loss 0.0896914, acc 0.984375, prec 0.0746526, recall 0.879186
2017-12-10T14:48:33.919822: step 4905, loss 0.207014, acc 1, prec 0.074686, recall 0.879237
2017-12-10T14:48:34.109037: step 4906, loss 0.0412656, acc 0.96875, prec 0.0746833, recall 0.879237
2017-12-10T14:48:34.297361: step 4907, loss 0.817218, acc 1, prec 0.0746999, recall 0.879263
2017-12-10T14:48:34.484362: step 4908, loss 0.0581248, acc 1, prec 0.0747499, recall 0.87934
2017-12-10T14:48:34.673507: step 4909, loss 0.116581, acc 0.96875, prec 0.0747638, recall 0.879365
2017-12-10T14:48:34.861208: step 4910, loss 0.0230962, acc 0.984375, prec 0.0747625, recall 0.879365
2017-12-10T14:48:35.051402: step 4911, loss 0.176291, acc 0.96875, prec 0.0748097, recall 0.879442
2017-12-10T14:48:35.240460: step 4912, loss 0.171807, acc 0.96875, prec 0.0748736, recall 0.879544
2017-12-10T14:48:35.427078: step 4913, loss 0.232711, acc 0.9375, prec 0.0749015, recall 0.879594
2017-12-10T14:48:35.618196: step 4914, loss 0.24495, acc 0.953125, prec 0.0749474, recall 0.879671
2017-12-10T14:48:35.805260: step 4915, loss 0.33131, acc 0.9375, prec 0.074942, recall 0.879671
2017-12-10T14:48:35.993269: step 4916, loss 0.0905094, acc 0.96875, prec 0.0749726, recall 0.879721
2017-12-10T14:48:36.181417: step 4917, loss 0.0667664, acc 0.984375, prec 0.0749712, recall 0.879721
2017-12-10T14:48:36.368976: step 4918, loss 0.212564, acc 0.890625, prec 0.0749618, recall 0.879721
2017-12-10T14:48:36.557540: step 4919, loss 0.048099, acc 0.984375, prec 0.0749604, recall 0.879721
2017-12-10T14:48:36.745874: step 4920, loss 0.183472, acc 0.9375, prec 0.0749717, recall 0.879747
2017-12-10T14:48:36.934145: step 4921, loss 0.0201738, acc 1, prec 0.0749883, recall 0.879772
2017-12-10T14:48:37.126727: step 4922, loss 0.20298, acc 0.9375, prec 0.0749829, recall 0.879772
2017-12-10T14:48:37.312755: step 4923, loss 0.1442, acc 0.953125, prec 0.0749955, recall 0.879798
2017-12-10T14:48:37.499633: step 4924, loss 0.0661513, acc 0.96875, prec 0.0750094, recall 0.879823
2017-12-10T14:48:37.686881: step 4925, loss 0.158828, acc 0.953125, prec 0.075022, recall 0.879848
2017-12-10T14:48:37.876010: step 4926, loss 0.456052, acc 0.921875, prec 0.0750485, recall 0.879899
2017-12-10T14:48:38.069982: step 4927, loss 0.160814, acc 0.984375, prec 0.0750638, recall 0.879924
2017-12-10T14:48:38.259053: step 4928, loss 0.04638, acc 0.984375, prec 0.0751123, recall 0.88
2017-12-10T14:48:38.445872: step 4929, loss 0.0321584, acc 0.984375, prec 0.0751276, recall 0.880025
2017-12-10T14:48:38.631882: step 4930, loss 0.119161, acc 0.984375, prec 0.0751595, recall 0.880076
2017-12-10T14:48:38.823778: step 4931, loss 0.290404, acc 0.953125, prec 0.075172, recall 0.880101
2017-12-10T14:48:39.012303: step 4932, loss 0.150388, acc 0.984375, prec 0.0751707, recall 0.880101
2017-12-10T14:48:39.203109: step 4933, loss 0.0387935, acc 0.96875, prec 0.075168, recall 0.880101
2017-12-10T14:48:39.389767: step 4934, loss 0.0952706, acc 0.96875, prec 0.0751985, recall 0.880151
2017-12-10T14:48:39.577179: step 4935, loss 0.0050789, acc 1, prec 0.0751985, recall 0.880151
2017-12-10T14:48:39.764496: step 4936, loss 0.0421271, acc 0.96875, prec 0.0751958, recall 0.880151
2017-12-10T14:48:39.950758: step 4937, loss 0.0833459, acc 0.96875, prec 0.0752097, recall 0.880177
2017-12-10T14:48:40.136706: step 4938, loss 0.093367, acc 0.96875, prec 0.0752236, recall 0.880202
2017-12-10T14:48:40.326527: step 4939, loss 0.142889, acc 0.953125, prec 0.0752362, recall 0.880227
2017-12-10T14:48:40.515218: step 4940, loss 0.0159734, acc 1, prec 0.0752362, recall 0.880227
2017-12-10T14:48:40.703240: step 4941, loss 0.136251, acc 0.96875, prec 0.0752667, recall 0.880277
2017-12-10T14:48:40.893223: step 4942, loss 0.0912802, acc 0.96875, prec 0.0753138, recall 0.880353
2017-12-10T14:48:41.078390: step 4943, loss 0.102596, acc 0.96875, prec 0.0753111, recall 0.880353
2017-12-10T14:48:41.262945: step 4944, loss 0.0139311, acc 0.984375, prec 0.0753098, recall 0.880353
2017-12-10T14:48:41.451029: step 4945, loss 0.0412583, acc 0.984375, prec 0.075325, recall 0.880378
2017-12-10T14:48:41.639110: step 4946, loss 0.193633, acc 0.9375, prec 0.0753196, recall 0.880378
2017-12-10T14:48:41.829666: step 4947, loss 0.201912, acc 0.96875, prec 0.0753169, recall 0.880378
2017-12-10T14:48:42.015750: step 4948, loss 0.0802839, acc 0.984375, prec 0.0753155, recall 0.880378
2017-12-10T14:48:42.205519: step 4949, loss 0.1137, acc 0.9375, prec 0.0753267, recall 0.880403
2017-12-10T14:48:42.391876: step 4950, loss 0.168906, acc 0.96875, prec 0.0753572, recall 0.880453
2017-12-10T14:48:42.581531: step 4951, loss 0.0771496, acc 0.96875, prec 0.0753545, recall 0.880453
2017-12-10T14:48:42.770307: step 4952, loss 0.555555, acc 0.984375, prec 0.0753864, recall 0.880503
2017-12-10T14:48:42.960207: step 4953, loss 0.0838232, acc 0.9375, prec 0.0753809, recall 0.880503
2017-12-10T14:48:43.148256: step 4954, loss 0.115684, acc 0.984375, prec 0.0753962, recall 0.880528
2017-12-10T14:48:43.335636: step 4955, loss 0.0847421, acc 0.96875, prec 0.0754101, recall 0.880553
2017-12-10T14:48:43.525861: step 4956, loss 2.61312, acc 0.984375, prec 0.0754433, recall 0.880419
2017-12-10T14:48:43.718759: step 4957, loss 0.0141395, acc 1, prec 0.0754433, recall 0.880419
2017-12-10T14:48:43.908898: step 4958, loss 0.0380109, acc 0.984375, prec 0.0754585, recall 0.880444
2017-12-10T14:48:44.102650: step 4959, loss 0.400342, acc 0.953125, prec 0.0754544, recall 0.880444
2017-12-10T14:48:44.295658: step 4960, loss 0.202042, acc 0.96875, prec 0.0754683, recall 0.880469
2017-12-10T14:48:44.480931: step 4961, loss 0.415546, acc 0.921875, prec 0.0754615, recall 0.880469
2017-12-10T14:48:44.668284: step 4962, loss 0.275649, acc 0.9375, prec 0.0754727, recall 0.880494
2017-12-10T14:48:44.853927: step 4963, loss 0.160294, acc 0.953125, prec 0.075535, recall 0.880594
2017-12-10T14:48:45.044113: step 4964, loss 0.270627, acc 0.9375, prec 0.0755296, recall 0.880594
2017-12-10T14:48:45.231378: step 4965, loss 0.0377829, acc 0.984375, prec 0.0755282, recall 0.880594
2017-12-10T14:48:45.418304: step 4966, loss 0.69032, acc 0.921875, prec 0.0755214, recall 0.880594
2017-12-10T14:48:45.603712: step 4967, loss 0.259564, acc 0.9375, prec 0.0755326, recall 0.880619
2017-12-10T14:48:45.790497: step 4968, loss 0.00646534, acc 1, prec 0.0755492, recall 0.880644
2017-12-10T14:48:45.979872: step 4969, loss 0.233994, acc 0.921875, prec 0.0755424, recall 0.880644
2017-12-10T14:48:46.161618: step 4970, loss 0.283365, acc 0.942308, prec 0.0755549, recall 0.880669
Training finished



Starting Experiment - leave_pos_embedding_out 



Starting Fold: 0 => Train/Dev split: 31795/10599


 #################### 


SEQUENCE LENGTH 273
BATCH SIZE 64
EMBEDDING SIZE 128
FILTER SIZES [3, 4, 5]
NUMBER OF FILTERS 128
L2 REG LAMBDA 0.0
EPOCHS 10



RESULT DIR leave_pos_embedding_out_fold_0
VOCAB SIZE 33447
DROPOUT PROBABILITY 0.5



WORD EMBEDDING True
POSITION EMBEDDING True
POS EMBEDDING False


 #################### 


Writing to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_0/1512935327

Start training
2017-12-10T14:48:49.918419: step 1, loss 8.23763, acc 0.0625, prec 0.0163934, recall 1
2017-12-10T14:48:50.109229: step 2, loss 3.60755, acc 0.328125, prec 0.00961538, recall 1
2017-12-10T14:48:50.302500: step 3, loss 1.27208, acc 0.703125, prec 0.016129, recall 1
2017-12-10T14:48:50.497109: step 4, loss 0.468767, acc 0.84375, prec 0.0149254, recall 1
2017-12-10T14:48:50.685953: step 5, loss 37.1322, acc 0.9375, prec 0.0147059, recall 0.5
2017-12-10T14:48:50.880085: step 6, loss 19.6081, acc 0.96875, prec 0.0145985, recall 0.4
2017-12-10T14:48:51.075179: step 7, loss 0.0828923, acc 0.953125, prec 0.0142857, recall 0.4
2017-12-10T14:48:51.267669: step 8, loss 13.9505, acc 0.953125, prec 0.0140845, recall 0.333333
2017-12-10T14:48:51.466534: step 9, loss 13.6997, acc 0.9375, prec 0.0137931, recall 0.285714
2017-12-10T14:48:51.664542: step 10, loss 0.274803, acc 0.90625, prec 0.013245, recall 0.285714
2017-12-10T14:48:51.856650: step 11, loss 0.473758, acc 0.828125, prec 0.0123457, recall 0.285714
2017-12-10T14:48:52.048675: step 12, loss 10.3478, acc 0.78125, prec 0.0114286, recall 0.25
2017-12-10T14:48:52.243096: step 13, loss 1.41342, acc 0.625, prec 0.0100503, recall 0.25
2017-12-10T14:48:52.433261: step 14, loss 7.59229, acc 0.625, prec 0.00900901, recall 0.222222
2017-12-10T14:48:52.626076: step 15, loss 11.0104, acc 0.5625, prec 0.00803213, recall 0.2
2017-12-10T14:48:52.816230: step 16, loss 2.82736, acc 0.484375, prec 0.0106007, recall 0.272727
2017-12-10T14:48:53.008320: step 17, loss 3.55811, acc 0.328125, prec 0.0122324, recall 0.333333
2017-12-10T14:48:53.199301: step 18, loss 2.84021, acc 0.375, prec 0.0108992, recall 0.333333
2017-12-10T14:48:53.389460: step 19, loss 3.72662, acc 0.296875, prec 0.00970874, recall 0.333333
2017-12-10T14:48:53.581609: step 20, loss 4.50555, acc 0.265625, prec 0.0108696, recall 0.384615
2017-12-10T14:48:53.771417: step 21, loss 4.27082, acc 0.25, prec 0.00984252, recall 0.384615
2017-12-10T14:48:53.961339: step 22, loss 3.64721, acc 0.296875, prec 0.0108303, recall 0.428571
2017-12-10T14:48:54.159889: step 23, loss 4.50631, acc 0.265625, prec 0.0116279, recall 0.466667
2017-12-10T14:48:54.352000: step 24, loss 2.49525, acc 0.40625, prec 0.0109375, recall 0.466667
2017-12-10T14:48:54.543445: step 25, loss 2.13444, acc 0.46875, prec 0.0133136, recall 0.529412
2017-12-10T14:48:54.732652: step 26, loss 1.63692, acc 0.546875, prec 0.012766, recall 0.529412
2017-12-10T14:48:54.924101: step 27, loss 1.45851, acc 0.640625, prec 0.0123626, recall 0.529412
2017-12-10T14:48:55.115728: step 28, loss 20.0861, acc 0.703125, prec 0.0120643, recall 0.5
2017-12-10T14:48:55.310357: step 29, loss 3.01378, acc 0.6875, prec 0.0117647, recall 0.473684
2017-12-10T14:48:55.501138: step 30, loss 1.01818, acc 0.734375, prec 0.011509, recall 0.473684
2017-12-10T14:48:55.691565: step 31, loss 16.1507, acc 0.78125, prec 0.0113208, recall 0.45
2017-12-10T14:48:55.886201: step 32, loss 8.00693, acc 0.625, prec 0.0110159, recall 0.409091
2017-12-10T14:48:56.075622: step 33, loss 2.7138, acc 0.6875, prec 0.0119474, recall 0.416667
2017-12-10T14:48:56.268023: step 34, loss 1.82672, acc 0.53125, prec 0.011534, recall 0.416667
2017-12-10T14:48:56.457728: step 35, loss 18.3373, acc 0.5, prec 0.0111359, recall 0.4
2017-12-10T14:48:56.650902: step 36, loss 2.2622, acc 0.515625, prec 0.0107643, recall 0.4
2017-12-10T14:48:56.840021: step 37, loss 2.50955, acc 0.390625, prec 0.0103306, recall 0.4
2017-12-10T14:48:57.029379: step 38, loss 2.80058, acc 0.390625, prec 0.0109127, recall 0.423077
2017-12-10T14:48:57.219479: step 39, loss 3.4602, acc 0.296875, prec 0.0104463, recall 0.423077
2017-12-10T14:48:57.410933: step 40, loss 3.40157, acc 0.40625, prec 0.0118939, recall 0.464286
2017-12-10T14:48:57.606122: step 41, loss 4.08825, acc 0.421875, prec 0.0132626, recall 0.483871
2017-12-10T14:48:57.797377: step 42, loss 3.35369, acc 0.234375, prec 0.0127119, recall 0.483871
2017-12-10T14:48:57.988486: step 43, loss 2.81665, acc 0.359375, prec 0.012285, recall 0.483871
2017-12-10T14:48:58.178127: step 44, loss 3.32605, acc 0.421875, prec 0.0119237, recall 0.483871
2017-12-10T14:48:58.366223: step 45, loss 2.6008, acc 0.421875, prec 0.011583, recall 0.483871
2017-12-10T14:48:58.558018: step 46, loss 2.38575, acc 0.484375, prec 0.0120391, recall 0.5
2017-12-10T14:48:58.749702: step 47, loss 4.77125, acc 0.625, prec 0.0118343, recall 0.484848
2017-12-10T14:48:58.944734: step 48, loss 7.64744, acc 0.6875, prec 0.0116703, recall 0.470588
2017-12-10T14:48:59.141965: step 49, loss 2.51984, acc 0.6875, prec 0.0122126, recall 0.485714
2017-12-10T14:48:59.330974: step 50, loss 1.51958, acc 0.625, prec 0.0120056, recall 0.485714
2017-12-10T14:48:59.522849: step 51, loss 9.1473, acc 0.609375, prec 0.0118056, recall 0.472222
2017-12-10T14:48:59.716927: step 52, loss 1.8317, acc 0.59375, prec 0.0115962, recall 0.472222
2017-12-10T14:48:59.908782: step 53, loss 1.25742, acc 0.640625, prec 0.0114171, recall 0.472222
2017-12-10T14:49:00.101514: step 54, loss 1.84267, acc 0.484375, prec 0.0111695, recall 0.472222
2017-12-10T14:49:00.290776: step 55, loss 3.4053, acc 0.640625, prec 0.0116505, recall 0.473684
2017-12-10T14:49:00.485784: step 56, loss 5.67423, acc 0.625, prec 0.0121096, recall 0.475
2017-12-10T14:49:00.681223: step 57, loss 2.30654, acc 0.546875, prec 0.0125078, recall 0.487805
2017-12-10T14:49:00.874129: step 58, loss 25.9266, acc 0.625, prec 0.0123381, recall 0.465116
2017-12-10T14:49:01.069745: step 59, loss 2.42539, acc 0.453125, prec 0.0120773, recall 0.465116
2017-12-10T14:49:01.269471: step 60, loss 3.89695, acc 0.390625, prec 0.0129641, recall 0.488889
2017-12-10T14:49:01.461701: step 61, loss 3.28441, acc 0.375, prec 0.0126655, recall 0.488889
2017-12-10T14:49:01.651192: step 62, loss 3.88934, acc 0.3125, prec 0.0134605, recall 0.510638
2017-12-10T14:49:01.841181: step 63, loss 5.8165, acc 0.40625, prec 0.01427, recall 0.52
2017-12-10T14:49:02.031044: step 64, loss 4.13026, acc 0.265625, prec 0.0144385, recall 0.529412
2017-12-10T14:49:02.227453: step 65, loss 3.88744, acc 0.359375, prec 0.0146444, recall 0.538462
2017-12-10T14:49:02.418428: step 66, loss 3.57062, acc 0.3125, prec 0.0148186, recall 0.54717
2017-12-10T14:49:02.607906: step 67, loss 3.5944, acc 0.3125, prec 0.0144928, recall 0.54717
2017-12-10T14:49:02.798041: step 68, loss 3.15636, acc 0.375, prec 0.0146915, recall 0.555556
2017-12-10T14:49:02.988765: step 69, loss 2.57186, acc 0.484375, prec 0.0144578, recall 0.555556
2017-12-10T14:49:03.185307: step 70, loss 2.68174, acc 0.453125, prec 0.014218, recall 0.555556
2017-12-10T14:49:03.374929: step 71, loss 2.54826, acc 0.5625, prec 0.0144928, recall 0.563636
2017-12-10T14:49:03.568984: step 72, loss 19.6745, acc 0.515625, prec 0.0142923, recall 0.553571
2017-12-10T14:49:03.760918: step 73, loss 1.80717, acc 0.640625, prec 0.0141423, recall 0.553571
2017-12-10T14:49:03.952210: step 74, loss 1.78971, acc 0.5625, prec 0.0144079, recall 0.561404
2017-12-10T14:49:04.145694: step 75, loss 1.77275, acc 0.625, prec 0.0142539, recall 0.561404
2017-12-10T14:49:04.333673: step 76, loss 1.1918, acc 0.703125, prec 0.0141343, recall 0.561404
2017-12-10T14:49:04.524730: step 77, loss 12.1582, acc 0.734375, prec 0.0140351, recall 0.551724
2017-12-10T14:49:04.720326: step 78, loss 13.1955, acc 0.65625, prec 0.013907, recall 0.542373
2017-12-10T14:49:04.914080: step 79, loss 1.20311, acc 0.6875, prec 0.0137872, recall 0.542373
2017-12-10T14:49:05.105405: step 80, loss 1.33893, acc 0.640625, prec 0.0136519, recall 0.542373
2017-12-10T14:49:05.295247: step 81, loss 1.59913, acc 0.65625, prec 0.0135249, recall 0.542373
2017-12-10T14:49:05.484803: step 82, loss 1.54435, acc 0.625, prec 0.0138018, recall 0.55
2017-12-10T14:49:05.675582: step 83, loss 1.48961, acc 0.65625, prec 0.0136759, recall 0.55
2017-12-10T14:49:05.866433: step 84, loss 10.9774, acc 0.703125, prec 0.0135747, recall 0.540984
2017-12-10T14:49:06.060413: step 85, loss 1.35278, acc 0.703125, prec 0.0134694, recall 0.540984
2017-12-10T14:49:06.251250: step 86, loss 0.596011, acc 0.828125, prec 0.0138099, recall 0.548387
2017-12-10T14:49:06.442854: step 87, loss 15.6357, acc 0.75, prec 0.0141243, recall 0.546875
2017-12-10T14:49:06.638559: step 88, loss 1.44897, acc 0.671875, prec 0.0144, recall 0.553846
2017-12-10T14:49:06.830888: step 89, loss 7.05172, acc 0.71875, prec 0.0146942, recall 0.552239
2017-12-10T14:49:07.023795: step 90, loss 1.0391, acc 0.71875, prec 0.0145899, recall 0.552239
2017-12-10T14:49:07.215862: step 91, loss 1.17409, acc 0.734375, prec 0.0144928, recall 0.552239
2017-12-10T14:49:07.404914: step 92, loss 15.8747, acc 0.71875, prec 0.0147802, recall 0.550725
2017-12-10T14:49:07.599022: step 93, loss 1.29202, acc 0.65625, prec 0.0146548, recall 0.550725
2017-12-10T14:49:07.789879: step 94, loss 2.25369, acc 0.5, prec 0.0144762, recall 0.550725
2017-12-10T14:49:07.981624: step 95, loss 2.10705, acc 0.640625, prec 0.0143505, recall 0.550725
2017-12-10T14:49:08.179470: step 96, loss 1.77679, acc 0.671875, prec 0.0146067, recall 0.557143
2017-12-10T14:49:08.379234: step 97, loss 2.15258, acc 0.640625, prec 0.0148478, recall 0.56338
2017-12-10T14:49:08.574041: step 98, loss 2.22588, acc 0.546875, prec 0.0146897, recall 0.56338
2017-12-10T14:49:08.769229: step 99, loss 2.91739, acc 0.5625, prec 0.0148983, recall 0.569444
2017-12-10T14:49:08.964650: step 100, loss 2.07655, acc 0.6875, prec 0.0151461, recall 0.575342
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_0/1512935327/checkpoints/model-100

2017-12-10T14:49:10.256043: step 101, loss 1.63339, acc 0.5625, prec 0.0149946, recall 0.575342
2017-12-10T14:49:10.450364: step 102, loss 6.73122, acc 0.6875, prec 0.0152428, recall 0.573333
2017-12-10T14:49:10.649348: step 103, loss 2.02961, acc 0.5625, prec 0.015093, recall 0.573333
2017-12-10T14:49:10.844725: step 104, loss 9.26254, acc 0.578125, prec 0.0156413, recall 0.576923
2017-12-10T14:49:11.035327: step 105, loss 2.24325, acc 0.5625, prec 0.0165062, recall 0.592593
2017-12-10T14:49:11.226868: step 106, loss 2.54809, acc 0.484375, prec 0.0166553, recall 0.597561
2017-12-10T14:49:11.415127: step 107, loss 2.40447, acc 0.5625, prec 0.0164983, recall 0.597561
2017-12-10T14:49:11.605269: step 108, loss 2.90018, acc 0.4375, prec 0.0172815, recall 0.611765
2017-12-10T14:49:11.793450: step 109, loss 2.55596, acc 0.4375, prec 0.0177223, recall 0.62069
2017-12-10T14:49:11.987220: step 110, loss 1.69771, acc 0.625, prec 0.0179036, recall 0.625
2017-12-10T14:49:12.176755: step 111, loss 2.90612, acc 0.40625, prec 0.0180006, recall 0.629214
2017-12-10T14:49:12.370817: step 112, loss 6.18205, acc 0.515625, prec 0.0178287, recall 0.622222
2017-12-10T14:49:12.564254: step 113, loss 1.90799, acc 0.59375, prec 0.0176823, recall 0.622222
2017-12-10T14:49:12.758095: step 114, loss 2.35536, acc 0.609375, prec 0.0178516, recall 0.626374
2017-12-10T14:49:12.951397: step 115, loss 2.11003, acc 0.53125, prec 0.0176854, recall 0.626374
2017-12-10T14:49:13.140575: step 116, loss 1.42996, acc 0.71875, prec 0.0178902, recall 0.630435
2017-12-10T14:49:13.338070: step 117, loss 2.38255, acc 0.625, prec 0.0192602, recall 0.649485
2017-12-10T14:49:13.528817: step 118, loss 1.24067, acc 0.671875, prec 0.0194352, recall 0.653061
2017-12-10T14:49:13.720365: step 119, loss 1.78, acc 0.5625, prec 0.0192713, recall 0.653061
2017-12-10T14:49:13.907325: step 120, loss 6.15295, acc 0.796875, prec 0.0192019, recall 0.646465
2017-12-10T14:49:14.107870: step 121, loss 0.863509, acc 0.765625, prec 0.0191159, recall 0.646465
2017-12-10T14:49:14.301740: step 122, loss 13.5214, acc 0.78125, prec 0.019042, recall 0.64
2017-12-10T14:49:14.493630: step 123, loss 0.959658, acc 0.71875, prec 0.0189405, recall 0.64
2017-12-10T14:49:14.684359: step 124, loss 1.52435, acc 0.625, prec 0.0188069, recall 0.64
2017-12-10T14:49:14.873575: step 125, loss 9.00379, acc 0.671875, prec 0.0189892, recall 0.631068
2017-12-10T14:49:15.067346: step 126, loss 15.5534, acc 0.734375, prec 0.0194711, recall 0.632075
2017-12-10T14:49:15.259351: step 127, loss 2.41324, acc 0.578125, prec 0.0196022, recall 0.635514
2017-12-10T14:49:15.449935: step 128, loss 2.8275, acc 0.4375, prec 0.0194009, recall 0.635514
2017-12-10T14:49:15.648837: step 129, loss 3.06105, acc 0.375, prec 0.0194585, recall 0.638889
2017-12-10T14:49:15.837296: step 130, loss 2.7858, acc 0.484375, prec 0.0192791, recall 0.638889
2017-12-10T14:49:16.027534: step 131, loss 3.44003, acc 0.359375, prec 0.0190608, recall 0.638889
2017-12-10T14:49:16.218130: step 132, loss 3.91811, acc 0.359375, prec 0.0188473, recall 0.638889
2017-12-10T14:49:16.410970: step 133, loss 3.05049, acc 0.4375, prec 0.0186638, recall 0.638889
2017-12-10T14:49:16.602498: step 134, loss 4.43201, acc 0.296875, prec 0.0187016, recall 0.642202
2017-12-10T14:49:16.793183: step 135, loss 2.93831, acc 0.4375, prec 0.0187831, recall 0.645455
2017-12-10T14:49:16.985369: step 136, loss 2.71388, acc 0.484375, prec 0.0186205, recall 0.645455
2017-12-10T14:49:17.175352: step 137, loss 1.39878, acc 0.65625, prec 0.0187696, recall 0.648649
2017-12-10T14:49:17.363011: step 138, loss 1.51666, acc 0.609375, prec 0.0189021, recall 0.651786
2017-12-10T14:49:17.556604: step 139, loss 10.3573, acc 0.59375, prec 0.0190329, recall 0.649123
2017-12-10T14:49:17.750019: step 140, loss 1.12339, acc 0.71875, prec 0.0191963, recall 0.652174
2017-12-10T14:49:17.939633: step 141, loss 1.20396, acc 0.671875, prec 0.0190937, recall 0.652174
2017-12-10T14:49:18.137468: step 142, loss 17.2491, acc 0.828125, prec 0.0192942, recall 0.649573
2017-12-10T14:49:18.332185: step 143, loss 1.14549, acc 0.75, prec 0.0194641, recall 0.652542
2017-12-10T14:49:18.520472: step 144, loss 2.09276, acc 0.734375, prec 0.0193857, recall 0.647059
2017-12-10T14:49:18.712840: step 145, loss 1.32956, acc 0.671875, prec 0.0192837, recall 0.647059
2017-12-10T14:49:18.905811: step 146, loss 1.18668, acc 0.765625, prec 0.0192116, recall 0.647059
2017-12-10T14:49:19.095437: step 147, loss 1.13812, acc 0.796875, prec 0.0191495, recall 0.647059
2017-12-10T14:49:19.288879: step 148, loss 11.5839, acc 0.640625, prec 0.0192878, recall 0.644628
2017-12-10T14:49:19.483507: step 149, loss 36.7361, acc 0.53125, prec 0.0191599, recall 0.629032
2017-12-10T14:49:19.677593: step 150, loss 10.4272, acc 0.640625, prec 0.0192965, recall 0.626984
2017-12-10T14:49:19.869480: step 151, loss 1.54531, acc 0.46875, prec 0.0196126, recall 0.632812
2017-12-10T14:49:20.061238: step 152, loss 2.99167, acc 0.5, prec 0.0194618, recall 0.632812
2017-12-10T14:49:20.252479: step 153, loss 4.6689, acc 0.296875, prec 0.0194867, recall 0.635659
2017-12-10T14:49:20.444340: step 154, loss 3.19419, acc 0.3125, prec 0.0197461, recall 0.641221
2017-12-10T14:49:20.639206: step 155, loss 4.33312, acc 0.203125, prec 0.0199675, recall 0.646617
2017-12-10T14:49:20.831248: step 156, loss 4.47307, acc 0.25, prec 0.0197474, recall 0.646617
2017-12-10T14:49:21.021530: step 157, loss 4.52695, acc 0.296875, prec 0.0197682, recall 0.649254
2017-12-10T14:49:21.210737: step 158, loss 4.48655, acc 0.25, prec 0.019555, recall 0.649254
2017-12-10T14:49:21.400013: step 159, loss 3.95888, acc 0.3125, prec 0.0195817, recall 0.651852
2017-12-10T14:49:21.592662: step 160, loss 3.87765, acc 0.375, prec 0.0196251, recall 0.654412
2017-12-10T14:49:21.779465: step 161, loss 2.47433, acc 0.4375, prec 0.0194706, recall 0.654412
2017-12-10T14:49:21.971708: step 162, loss 3.54218, acc 0.3125, prec 0.0194974, recall 0.656934
2017-12-10T14:49:22.160838: step 163, loss 2.24811, acc 0.5, prec 0.0195741, recall 0.65942
2017-12-10T14:49:22.351797: step 164, loss 2.4434, acc 0.484375, prec 0.0194361, recall 0.65942
2017-12-10T14:49:22.541792: step 165, loss 6.60967, acc 0.578125, prec 0.0197452, recall 0.659574
2017-12-10T14:49:22.736574: step 166, loss 1.34253, acc 0.625, prec 0.0198522, recall 0.661972
2017-12-10T14:49:22.925632: step 167, loss 4.73037, acc 0.578125, prec 0.0199496, recall 0.659722
2017-12-10T14:49:23.118550: step 168, loss 4.35837, acc 0.609375, prec 0.0198496, recall 0.655172
2017-12-10T14:49:23.308914: step 169, loss 1.29785, acc 0.640625, prec 0.0197546, recall 0.655172
2017-12-10T14:49:23.499688: step 170, loss 8.97585, acc 0.5, prec 0.0196281, recall 0.650685
2017-12-10T14:49:23.691883: step 171, loss 1.54776, acc 0.609375, prec 0.0195272, recall 0.650685
2017-12-10T14:49:23.883725: step 172, loss 3.55931, acc 0.578125, prec 0.0194234, recall 0.646259
2017-12-10T14:49:24.074204: step 173, loss 2.245, acc 0.546875, prec 0.0193089, recall 0.646259
2017-12-10T14:49:24.263464: step 174, loss 2.03205, acc 0.421875, prec 0.0191648, recall 0.646259
2017-12-10T14:49:24.456281: step 175, loss 1.6448, acc 0.578125, prec 0.0192578, recall 0.648649
2017-12-10T14:49:24.645289: step 176, loss 1.79835, acc 0.546875, prec 0.019342, recall 0.651007
2017-12-10T14:49:24.833420: step 177, loss 1.6646, acc 0.625, prec 0.0194444, recall 0.653333
2017-12-10T14:49:25.026945: step 178, loss 1.48143, acc 0.59375, prec 0.0193447, recall 0.653333
2017-12-10T14:49:25.220006: step 179, loss 1.65196, acc 0.625, prec 0.0194461, recall 0.655629
2017-12-10T14:49:25.412924: step 180, loss 7.54472, acc 0.4375, prec 0.0193133, recall 0.651316
2017-12-10T14:49:25.603695: step 181, loss 1.3485, acc 0.640625, prec 0.019227, recall 0.651316
2017-12-10T14:49:25.799308: step 182, loss 1.68511, acc 0.640625, prec 0.0195207, recall 0.655844
2017-12-10T14:49:25.993395: step 183, loss 8.26054, acc 0.609375, prec 0.0196229, recall 0.649682
2017-12-10T14:49:26.185787: step 184, loss 0.80173, acc 0.8125, prec 0.0195777, recall 0.649682
2017-12-10T14:49:26.374690: step 185, loss 1.75555, acc 0.625, prec 0.019488, recall 0.649682
2017-12-10T14:49:26.567781: step 186, loss 1.94606, acc 0.578125, prec 0.0193879, recall 0.649682
2017-12-10T14:49:26.758420: step 187, loss 1.47356, acc 0.671875, prec 0.0194965, recall 0.651899
2017-12-10T14:49:26.946788: step 188, loss 1.6278, acc 0.546875, prec 0.0193901, recall 0.651899
2017-12-10T14:49:27.137540: step 189, loss 1.47933, acc 0.65625, prec 0.0193101, recall 0.651899
2017-12-10T14:49:27.328894: step 190, loss 2.0338, acc 0.609375, prec 0.019403, recall 0.654088
2017-12-10T14:49:27.524931: step 191, loss 1.36936, acc 0.609375, prec 0.0193129, recall 0.654088
2017-12-10T14:49:27.716986: step 192, loss 2.00443, acc 0.6875, prec 0.0197855, recall 0.660494
2017-12-10T14:49:27.907230: step 193, loss 15.6127, acc 0.703125, prec 0.0197235, recall 0.652439
2017-12-10T14:49:28.098568: step 194, loss 1.38022, acc 0.71875, prec 0.0198384, recall 0.654545
2017-12-10T14:49:28.291663: step 195, loss 9.63855, acc 0.703125, prec 0.0199524, recall 0.652695
2017-12-10T14:49:28.484455: step 196, loss 4.29945, acc 0.484375, prec 0.0200146, recall 0.650888
2017-12-10T14:49:28.676133: step 197, loss 2.21758, acc 0.515625, prec 0.0199023, recall 0.650888
2017-12-10T14:49:28.868440: step 198, loss 2.67723, acc 0.4375, prec 0.0201258, recall 0.654971
2017-12-10T14:49:29.066157: step 199, loss 2.81221, acc 0.375, prec 0.020157, recall 0.656977
2017-12-10T14:49:29.268755: step 200, loss 2.77226, acc 0.3125, prec 0.02, recall 0.656977
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_0/1512935327/checkpoints/model-200

2017-12-10T14:49:30.399898: step 201, loss 4.01607, acc 0.390625, prec 0.0202109, recall 0.657143
2017-12-10T14:49:30.587884: step 202, loss 3.56285, acc 0.265625, prec 0.0200453, recall 0.657143
2017-12-10T14:49:30.782037: step 203, loss 2.72882, acc 0.46875, prec 0.020097, recall 0.659091
2017-12-10T14:49:30.971413: step 204, loss 5.93461, acc 0.46875, prec 0.0201516, recall 0.657303
2017-12-10T14:49:31.165671: step 205, loss 2.12673, acc 0.421875, prec 0.020024, recall 0.657303
2017-12-10T14:49:31.362043: step 206, loss 2.70554, acc 0.421875, prec 0.019898, recall 0.657303
2017-12-10T14:49:31.556896: step 207, loss 3.64253, acc 0.359375, prec 0.0200912, recall 0.661111
2017-12-10T14:49:31.748956: step 208, loss 2.80162, acc 0.515625, prec 0.0201511, recall 0.662983
2017-12-10T14:49:31.938086: step 209, loss 2.05165, acc 0.5, prec 0.0200434, recall 0.662983
2017-12-10T14:49:32.126805: step 210, loss 2.15233, acc 0.609375, prec 0.0204489, recall 0.668478
2017-12-10T14:49:32.319501: step 211, loss 2.98001, acc 0.5625, prec 0.0205162, recall 0.67027
2017-12-10T14:49:32.515251: step 212, loss 5.81677, acc 0.546875, prec 0.0204216, recall 0.666667
2017-12-10T14:49:32.710279: step 213, loss 8.43959, acc 0.640625, prec 0.0203479, recall 0.663102
2017-12-10T14:49:32.906328: step 214, loss 1.62716, acc 0.5625, prec 0.0204148, recall 0.664894
2017-12-10T14:49:33.098914: step 215, loss 1.85736, acc 0.515625, prec 0.0206303, recall 0.668421
2017-12-10T14:49:33.288684: step 216, loss 1.81383, acc 0.609375, prec 0.0207053, recall 0.670157
2017-12-10T14:49:33.478635: step 217, loss 3.77168, acc 0.609375, prec 0.0210984, recall 0.671795
2017-12-10T14:49:33.677083: step 218, loss 1.58471, acc 0.609375, prec 0.0210138, recall 0.671795
2017-12-10T14:49:33.871266: step 219, loss 1.57603, acc 0.609375, prec 0.0209299, recall 0.671795
2017-12-10T14:49:34.062876: step 220, loss 1.79925, acc 0.5625, prec 0.021148, recall 0.675127
2017-12-10T14:49:34.253563: step 221, loss 3.7901, acc 0.515625, prec 0.0210476, recall 0.671717
2017-12-10T14:49:34.448278: step 222, loss 1.61821, acc 0.546875, prec 0.0211057, recall 0.673367
2017-12-10T14:49:34.640220: step 223, loss 1.49106, acc 0.6875, prec 0.0210394, recall 0.673367
2017-12-10T14:49:34.834223: step 224, loss 1.16071, acc 0.625, prec 0.0209604, recall 0.673367
2017-12-10T14:49:35.025095: step 225, loss 4.06276, acc 0.515625, prec 0.0211673, recall 0.673267
2017-12-10T14:49:35.218134: step 226, loss 1.5963, acc 0.671875, prec 0.0210984, recall 0.673267
2017-12-10T14:49:35.412614: step 227, loss 2.32147, acc 0.609375, prec 0.0213193, recall 0.676471
2017-12-10T14:49:35.605502: step 228, loss 1.32071, acc 0.671875, prec 0.0215517, recall 0.679612
2017-12-10T14:49:35.797089: step 229, loss 1.18195, acc 0.65625, prec 0.021479, recall 0.679612
2017-12-10T14:49:35.988325: step 230, loss 1.02123, acc 0.75, prec 0.0217258, recall 0.682692
2017-12-10T14:49:36.179253: step 231, loss 0.746335, acc 0.8125, prec 0.021686, recall 0.682692
2017-12-10T14:49:36.371342: step 232, loss 1.15092, acc 0.65625, prec 0.0216134, recall 0.682692
2017-12-10T14:49:36.563324: step 233, loss 0.988238, acc 0.734375, prec 0.0215576, recall 0.682692
2017-12-10T14:49:36.752703: step 234, loss 10.8404, acc 0.75, prec 0.0216601, recall 0.677725
2017-12-10T14:49:36.944375: step 235, loss 2.26202, acc 0.78125, prec 0.0219099, recall 0.680751
2017-12-10T14:49:37.136134: step 236, loss 10.1858, acc 0.765625, prec 0.0220145, recall 0.675926
2017-12-10T14:49:37.328592: step 237, loss 6.66611, acc 0.65625, prec 0.0219483, recall 0.669725
2017-12-10T14:49:37.522856: step 238, loss 1.50068, acc 0.625, prec 0.0218694, recall 0.669725
2017-12-10T14:49:37.722754: step 239, loss 5.61887, acc 0.40625, prec 0.0217488, recall 0.666667
2017-12-10T14:49:37.913054: step 240, loss 3.11391, acc 0.40625, prec 0.0219162, recall 0.669683
2017-12-10T14:49:38.103218: step 241, loss 4.18293, acc 0.34375, prec 0.0217807, recall 0.669683
2017-12-10T14:49:38.294149: step 242, loss 3.2304, acc 0.3125, prec 0.0216406, recall 0.669683
2017-12-10T14:49:38.486279: step 243, loss 5.53659, acc 0.15625, prec 0.0214711, recall 0.669683
2017-12-10T14:49:38.674527: step 244, loss 4.61881, acc 0.265625, prec 0.0216076, recall 0.672646
2017-12-10T14:49:38.863005: step 245, loss 5.57251, acc 0.15625, prec 0.0215807, recall 0.674107
2017-12-10T14:49:39.050354: step 246, loss 4.99265, acc 0.15625, prec 0.0215542, recall 0.675556
2017-12-10T14:49:39.239961: step 247, loss 4.39864, acc 0.21875, prec 0.0216779, recall 0.678414
2017-12-10T14:49:39.429984: step 248, loss 3.69703, acc 0.296875, prec 0.0218151, recall 0.681223
2017-12-10T14:49:39.620394: step 249, loss 4.13186, acc 0.234375, prec 0.0216667, recall 0.681223
2017-12-10T14:49:39.811088: step 250, loss 3.15257, acc 0.359375, prec 0.021544, recall 0.681223
2017-12-10T14:49:40.000474: step 251, loss 2.41694, acc 0.40625, prec 0.0214315, recall 0.681223
2017-12-10T14:49:40.188102: step 252, loss 2.03704, acc 0.5625, prec 0.0214833, recall 0.682609
2017-12-10T14:49:40.383411: step 253, loss 1.8995, acc 0.546875, prec 0.0213984, recall 0.682609
2017-12-10T14:49:40.575561: step 254, loss 3.11947, acc 0.5625, prec 0.0213199, recall 0.679654
2017-12-10T14:49:40.765409: step 255, loss 1.01527, acc 0.796875, prec 0.021415, recall 0.681035
2017-12-10T14:49:40.956602: step 256, loss 0.593347, acc 0.828125, prec 0.0215156, recall 0.682403
2017-12-10T14:49:41.148747: step 257, loss 0.988668, acc 0.828125, prec 0.0216158, recall 0.683761
2017-12-10T14:49:41.345227: step 258, loss 18.2206, acc 0.8125, prec 0.0217157, recall 0.682203
2017-12-10T14:49:41.537759: step 259, loss 0.240415, acc 0.90625, prec 0.0216981, recall 0.682203
2017-12-10T14:49:41.728991: step 260, loss 2.13675, acc 0.796875, prec 0.0217947, recall 0.680672
2017-12-10T14:49:41.928256: step 261, loss 1.60896, acc 0.84375, prec 0.0220282, recall 0.683333
2017-12-10T14:49:42.124308: step 262, loss 5.80735, acc 0.8125, prec 0.0219987, recall 0.677686
2017-12-10T14:49:42.319623: step 263, loss 0.398918, acc 0.90625, prec 0.021981, recall 0.677686
2017-12-10T14:49:42.511504: step 264, loss 0.797662, acc 0.78125, prec 0.0220706, recall 0.679012
2017-12-10T14:49:42.701065: step 265, loss 19.315, acc 0.796875, prec 0.0220382, recall 0.673469
2017-12-10T14:49:42.896722: step 266, loss 0.96995, acc 0.671875, prec 0.0219766, recall 0.673469
2017-12-10T14:49:43.091286: step 267, loss 1.49856, acc 0.59375, prec 0.0220305, recall 0.674797
2017-12-10T14:49:43.281887: step 268, loss 1.69895, acc 0.578125, prec 0.0219519, recall 0.674797
2017-12-10T14:49:43.471468: step 269, loss 1.58386, acc 0.625, prec 0.0218824, recall 0.674797
2017-12-10T14:49:43.662062: step 270, loss 1.4897, acc 0.640625, prec 0.0220733, recall 0.677419
2017-12-10T14:49:43.852541: step 271, loss 1.77374, acc 0.578125, prec 0.0219953, recall 0.677419
2017-12-10T14:49:44.055474: step 272, loss 2.55275, acc 0.40625, prec 0.0218864, recall 0.677419
2017-12-10T14:49:44.251412: step 273, loss 2.47654, acc 0.4375, prec 0.0217842, recall 0.677419
2017-12-10T14:49:44.442272: step 274, loss 2.06915, acc 0.515625, prec 0.021697, recall 0.677419
2017-12-10T14:49:44.631630: step 275, loss 2.33504, acc 0.53125, prec 0.021865, recall 0.68
2017-12-10T14:49:44.820269: step 276, loss 2.55202, acc 0.453125, prec 0.0220174, recall 0.68254
2017-12-10T14:49:45.009879: step 277, loss 1.14383, acc 0.625, prec 0.02195, recall 0.68254
2017-12-10T14:49:45.203171: step 278, loss 1.87969, acc 0.609375, prec 0.0220046, recall 0.683794
2017-12-10T14:49:45.394924: step 279, loss 5.49228, acc 0.765625, prec 0.0219655, recall 0.681102
2017-12-10T14:49:45.588012: step 280, loss 4.16669, acc 0.65625, prec 0.0224022, recall 0.683398
2017-12-10T14:49:45.784728: step 281, loss 1.49136, acc 0.5625, prec 0.0223231, recall 0.683398
2017-12-10T14:49:45.974691: step 282, loss 1.54591, acc 0.546875, prec 0.0222418, recall 0.683398
2017-12-10T14:49:46.166187: step 283, loss 1.70446, acc 0.625, prec 0.0221749, recall 0.683398
2017-12-10T14:49:46.354277: step 284, loss 5.01066, acc 0.515625, prec 0.0220919, recall 0.680769
2017-12-10T14:49:46.544738: step 285, loss 2.32339, acc 0.640625, prec 0.022272, recall 0.683206
2017-12-10T14:49:46.734714: step 286, loss 1.34344, acc 0.703125, prec 0.0223408, recall 0.684411
2017-12-10T14:49:46.925820: step 287, loss 10.8967, acc 0.546875, prec 0.0223844, recall 0.683019
2017-12-10T14:49:47.118488: step 288, loss 2.40131, acc 0.53125, prec 0.0223016, recall 0.683019
2017-12-10T14:49:47.307798: step 289, loss 2.12679, acc 0.46875, prec 0.0222086, recall 0.683019
2017-12-10T14:49:47.506394: step 290, loss 5.26524, acc 0.609375, prec 0.0221434, recall 0.680451
2017-12-10T14:49:47.698955: step 291, loss 1.88195, acc 0.421875, prec 0.0220436, recall 0.680451
2017-12-10T14:49:47.890569: step 292, loss 2.15208, acc 0.53125, prec 0.0219634, recall 0.680451
2017-12-10T14:49:48.082516: step 293, loss 1.90157, acc 0.546875, prec 0.0221228, recall 0.682836
2017-12-10T14:49:48.273192: step 294, loss 2.67395, acc 0.484375, prec 0.0220349, recall 0.682836
2017-12-10T14:49:48.464703: step 295, loss 2.47389, acc 0.5, prec 0.0219503, recall 0.682836
2017-12-10T14:49:48.657833: step 296, loss 2.42002, acc 0.53125, prec 0.0218716, recall 0.682836
2017-12-10T14:49:48.850090: step 297, loss 1.11552, acc 0.671875, prec 0.0219335, recall 0.684015
2017-12-10T14:49:49.042328: step 298, loss 2.20782, acc 0.578125, prec 0.0219793, recall 0.685185
2017-12-10T14:49:49.232041: step 299, loss 1.40743, acc 0.765625, prec 0.0222881, recall 0.688645
2017-12-10T14:49:49.422687: step 300, loss 13.8029, acc 0.609375, prec 0.0223404, recall 0.687273
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_0/1512935327/checkpoints/model-300

2017-12-10T14:49:50.485395: step 301, loss 0.684697, acc 0.734375, prec 0.0224109, recall 0.688406
2017-12-10T14:49:50.677284: step 302, loss 0.910876, acc 0.703125, prec 0.0225909, recall 0.690647
2017-12-10T14:49:50.865635: step 303, loss 1.06847, acc 0.671875, prec 0.0226499, recall 0.691756
2017-12-10T14:49:51.054662: step 304, loss 1.00948, acc 0.71875, prec 0.0226022, recall 0.691756
2017-12-10T14:49:51.250188: step 305, loss 1.67574, acc 0.65625, prec 0.0227724, recall 0.69395
2017-12-10T14:49:51.445860: step 306, loss 1.58251, acc 0.640625, prec 0.0228252, recall 0.695035
2017-12-10T14:49:51.633236: step 307, loss 1.02986, acc 0.65625, prec 0.0228804, recall 0.696113
2017-12-10T14:49:51.827011: step 308, loss 1.87284, acc 0.625, prec 0.0229299, recall 0.697183
2017-12-10T14:49:52.024785: step 309, loss 0.843666, acc 0.6875, prec 0.0229898, recall 0.698246
2017-12-10T14:49:52.219076: step 310, loss 0.596239, acc 0.875, prec 0.0229686, recall 0.698246
2017-12-10T14:49:52.410609: step 311, loss 1.27732, acc 0.765625, prec 0.0230415, recall 0.699301
2017-12-10T14:49:52.600361: step 312, loss 0.680295, acc 0.75, prec 0.0229991, recall 0.699301
2017-12-10T14:49:52.792107: step 313, loss 7.42617, acc 0.734375, prec 0.0229568, recall 0.696864
2017-12-10T14:49:52.985566: step 314, loss 1.69682, acc 0.90625, prec 0.0235011, recall 0.702055
2017-12-10T14:49:53.175458: step 315, loss 0.698519, acc 0.84375, prec 0.023586, recall 0.703072
2017-12-10T14:49:53.363062: step 316, loss 0.782548, acc 0.75, prec 0.0236544, recall 0.704082
2017-12-10T14:49:53.555725: step 317, loss 0.730968, acc 0.8125, prec 0.0237335, recall 0.705085
2017-12-10T14:49:53.744190: step 318, loss 4.93738, acc 0.828125, prec 0.0237064, recall 0.702703
2017-12-10T14:49:53.941673: step 319, loss 0.93936, acc 0.75, prec 0.0236633, recall 0.702703
2017-12-10T14:49:54.133620: step 320, loss 1.37179, acc 0.671875, prec 0.0236069, recall 0.702703
2017-12-10T14:49:54.326864: step 321, loss 1.00373, acc 0.734375, prec 0.0237826, recall 0.704698
2017-12-10T14:49:54.516739: step 322, loss 8.77167, acc 0.59375, prec 0.0237154, recall 0.702341
2017-12-10T14:49:54.709696: step 323, loss 2.1564, acc 0.59375, prec 0.0239757, recall 0.705298
2017-12-10T14:49:54.900799: step 324, loss 13.3796, acc 0.578125, prec 0.0241248, recall 0.704918
2017-12-10T14:49:55.091066: step 325, loss 2.24067, acc 0.515625, prec 0.0240411, recall 0.704918
2017-12-10T14:49:55.284711: step 326, loss 1.95518, acc 0.578125, prec 0.0240776, recall 0.705882
2017-12-10T14:49:55.475521: step 327, loss 2.43673, acc 0.453125, prec 0.0240924, recall 0.70684
2017-12-10T14:49:55.663306: step 328, loss 2.94769, acc 0.421875, prec 0.0239938, recall 0.70684
2017-12-10T14:49:55.853764: step 329, loss 2.4196, acc 0.375, prec 0.0238882, recall 0.70684
2017-12-10T14:49:56.044934: step 330, loss 2.42857, acc 0.4375, prec 0.0239009, recall 0.707792
2017-12-10T14:49:56.237591: step 331, loss 2.03686, acc 0.484375, prec 0.0239214, recall 0.708738
2017-12-10T14:49:56.428800: step 332, loss 2.4695, acc 0.375, prec 0.0239234, recall 0.709677
2017-12-10T14:49:56.619173: step 333, loss 1.69897, acc 0.5625, prec 0.0238508, recall 0.709677
2017-12-10T14:49:56.807468: step 334, loss 1.43494, acc 0.546875, prec 0.0237761, recall 0.709677
2017-12-10T14:49:56.999023: step 335, loss 1.45833, acc 0.640625, prec 0.0237171, recall 0.709677
2017-12-10T14:49:57.190044: step 336, loss 1.37048, acc 0.609375, prec 0.0236534, recall 0.709677
2017-12-10T14:49:57.386797: step 337, loss 0.893919, acc 0.765625, prec 0.0237201, recall 0.710611
2017-12-10T14:49:57.579408: step 338, loss 0.866627, acc 0.671875, prec 0.0236667, recall 0.710611
2017-12-10T14:49:57.767852: step 339, loss 0.368695, acc 0.859375, prec 0.023644, recall 0.710611
2017-12-10T14:49:57.960899: step 340, loss 10.4575, acc 0.796875, prec 0.0236162, recall 0.70607
2017-12-10T14:49:58.151272: step 341, loss 2.09555, acc 0.84375, prec 0.0236977, recall 0.704762
2017-12-10T14:49:58.344683: step 342, loss 0.310107, acc 0.875, prec 0.0236775, recall 0.704762
2017-12-10T14:49:58.537943: step 343, loss 14.6641, acc 0.6875, prec 0.0236321, recall 0.700315
2017-12-10T14:49:58.731312: step 344, loss 0.598791, acc 0.75, prec 0.0236957, recall 0.701258
2017-12-10T14:49:58.926905: step 345, loss 1.22512, acc 0.65625, prec 0.0237439, recall 0.702194
2017-12-10T14:49:59.125997: step 346, loss 1.25023, acc 0.609375, prec 0.0237844, recall 0.703125
2017-12-10T14:49:59.322780: step 347, loss 0.990646, acc 0.703125, prec 0.0237367, recall 0.703125
2017-12-10T14:49:59.511675: step 348, loss 1.56757, acc 0.625, prec 0.0236767, recall 0.703125
2017-12-10T14:49:59.699325: step 349, loss 1.63128, acc 0.625, prec 0.0236171, recall 0.703125
2017-12-10T14:49:59.886947: step 350, loss 1.11615, acc 0.640625, prec 0.0236624, recall 0.70405
2017-12-10T14:50:00.076013: step 351, loss 1.20812, acc 0.703125, prec 0.0237175, recall 0.704969
2017-12-10T14:50:00.268919: step 352, loss 1.28258, acc 0.65625, prec 0.0236631, recall 0.704969
2017-12-10T14:50:00.459063: step 353, loss 1.12921, acc 0.671875, prec 0.0236114, recall 0.704969
2017-12-10T14:50:00.650091: step 354, loss 1.53298, acc 0.65625, prec 0.0237601, recall 0.70679
2017-12-10T14:50:00.840438: step 355, loss 1.1233, acc 0.6875, prec 0.0237109, recall 0.70679
2017-12-10T14:50:01.034079: step 356, loss 1.36161, acc 0.546875, prec 0.0237407, recall 0.707692
2017-12-10T14:50:01.231761: step 357, loss 0.858249, acc 0.71875, prec 0.0236967, recall 0.707692
2017-12-10T14:50:01.433689: step 358, loss 0.968611, acc 0.78125, prec 0.0236626, recall 0.707692
2017-12-10T14:50:01.627908: step 359, loss 0.799745, acc 0.78125, prec 0.0236285, recall 0.707692
2017-12-10T14:50:01.819476: step 360, loss 0.740819, acc 0.75, prec 0.02379, recall 0.70948
2017-12-10T14:50:02.010525: step 361, loss 2.74958, acc 0.796875, prec 0.0237608, recall 0.707317
2017-12-10T14:50:02.203103: step 362, loss 6.52119, acc 0.84375, prec 0.0237389, recall 0.705167
2017-12-10T14:50:02.400976: step 363, loss 0.291213, acc 0.859375, prec 0.023717, recall 0.705167
2017-12-10T14:50:02.594766: step 364, loss 0.591441, acc 0.734375, prec 0.0236759, recall 0.705167
2017-12-10T14:50:02.791112: step 365, loss 1.93208, acc 0.8125, prec 0.0238483, recall 0.704819
2017-12-10T14:50:02.983393: step 366, loss 0.587483, acc 0.78125, prec 0.0239137, recall 0.705706
2017-12-10T14:50:03.175269: step 367, loss 0.732589, acc 0.78125, prec 0.0238797, recall 0.705706
2017-12-10T14:50:03.363435: step 368, loss 0.653015, acc 0.796875, prec 0.0239472, recall 0.706587
2017-12-10T14:50:03.552465: step 369, loss 0.939668, acc 0.71875, prec 0.0239036, recall 0.706587
2017-12-10T14:50:03.745229: step 370, loss 1.1079, acc 0.671875, prec 0.0239515, recall 0.707463
2017-12-10T14:50:03.936827: step 371, loss 4.64021, acc 0.765625, prec 0.0241146, recall 0.707101
2017-12-10T14:50:04.128640: step 372, loss 0.842413, acc 0.75, prec 0.0240758, recall 0.707101
2017-12-10T14:50:04.322929: step 373, loss 1.62282, acc 0.765625, prec 0.0241376, recall 0.707965
2017-12-10T14:50:04.519994: step 374, loss 1.24967, acc 0.71875, prec 0.0242899, recall 0.709677
2017-12-10T14:50:04.709547: step 375, loss 1.45142, acc 0.5625, prec 0.0242218, recall 0.709677
2017-12-10T14:50:04.900190: step 376, loss 1.89423, acc 0.8125, prec 0.0243878, recall 0.71137
2017-12-10T14:50:05.092240: step 377, loss 5.07114, acc 0.703125, prec 0.0244413, recall 0.710145
2017-12-10T14:50:05.284735: step 378, loss 1.34867, acc 0.671875, prec 0.0244874, recall 0.710983
2017-12-10T14:50:05.475005: step 379, loss 1.63144, acc 0.5, prec 0.0245064, recall 0.711816
2017-12-10T14:50:05.665478: step 380, loss 1.34887, acc 0.5625, prec 0.0244385, recall 0.711816
2017-12-10T14:50:05.858661: step 381, loss 1.74092, acc 0.515625, prec 0.0243638, recall 0.711816
2017-12-10T14:50:06.049652: step 382, loss 4.81603, acc 0.515625, prec 0.0242919, recall 0.70977
2017-12-10T14:50:06.241467: step 383, loss 2.04296, acc 0.484375, prec 0.0242133, recall 0.70977
2017-12-10T14:50:06.435146: step 384, loss 2.06237, acc 0.484375, prec 0.0241352, recall 0.70977
2017-12-10T14:50:06.627083: step 385, loss 1.7538, acc 0.625, prec 0.0244592, recall 0.713068
2017-12-10T14:50:06.817870: step 386, loss 2.34143, acc 0.5, prec 0.0244779, recall 0.713881
2017-12-10T14:50:07.006581: step 387, loss 2.32624, acc 0.625, prec 0.0245155, recall 0.714689
2017-12-10T14:50:07.201940: step 388, loss 4.19805, acc 0.515625, prec 0.0246329, recall 0.714286
2017-12-10T14:50:07.400844: step 389, loss 1.57451, acc 0.578125, prec 0.0247568, recall 0.715877
2017-12-10T14:50:07.589195: step 390, loss 1.39521, acc 0.59375, prec 0.0246949, recall 0.715877
2017-12-10T14:50:07.783007: step 391, loss 2.00317, acc 0.484375, prec 0.0246169, recall 0.715877
2017-12-10T14:50:07.969955: step 392, loss 3.2508, acc 0.625, prec 0.024656, recall 0.714681
2017-12-10T14:50:08.162839: step 393, loss 1.8245, acc 0.640625, prec 0.0247879, recall 0.716253
2017-12-10T14:50:08.358198: step 394, loss 1.26481, acc 0.609375, prec 0.0247289, recall 0.716253
2017-12-10T14:50:08.551933: step 395, loss 1.89079, acc 0.65625, prec 0.0247699, recall 0.717033
2017-12-10T14:50:08.742616: step 396, loss 3.91692, acc 0.640625, prec 0.0249029, recall 0.716621
2017-12-10T14:50:08.940542: step 397, loss 1.67461, acc 0.65625, prec 0.0249433, recall 0.717391
2017-12-10T14:50:09.130376: step 398, loss 0.730347, acc 0.703125, prec 0.0248986, recall 0.717391
2017-12-10T14:50:09.321402: step 399, loss 1.09125, acc 0.71875, prec 0.0248564, recall 0.717391
2017-12-10T14:50:09.511622: step 400, loss 1.66959, acc 0.609375, prec 0.0249812, recall 0.718919
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_0/1512935327/checkpoints/model-400

2017-12-10T14:50:10.732218: step 401, loss 1.1754, acc 0.671875, prec 0.024932, recall 0.718919
2017-12-10T14:50:10.922294: step 402, loss 4.40349, acc 0.625, prec 0.0250608, recall 0.718499
2017-12-10T14:50:11.117576: step 403, loss 4.28767, acc 0.75, prec 0.0250257, recall 0.716578
2017-12-10T14:50:11.308823: step 404, loss 1.71632, acc 0.578125, prec 0.0250536, recall 0.717333
2017-12-10T14:50:11.505388: step 405, loss 1.71356, acc 0.515625, prec 0.025072, recall 0.718085
2017-12-10T14:50:11.695529: step 406, loss 1.70192, acc 0.578125, prec 0.0250093, recall 0.718085
2017-12-10T14:50:11.885640: step 407, loss 2.00039, acc 0.5, prec 0.0250254, recall 0.718833
2017-12-10T14:50:12.077533: step 408, loss 1.30509, acc 0.59375, prec 0.0249655, recall 0.718833
2017-12-10T14:50:12.269364: step 409, loss 1.51238, acc 0.578125, prec 0.0249931, recall 0.719577
2017-12-10T14:50:12.464895: step 410, loss 1.88179, acc 0.578125, prec 0.0249313, recall 0.719577
2017-12-10T14:50:12.653953: step 411, loss 1.68028, acc 0.578125, prec 0.0249589, recall 0.720317
2017-12-10T14:50:12.853110: step 412, loss 5.1211, acc 0.578125, prec 0.0249886, recall 0.71916
2017-12-10T14:50:13.043550: step 413, loss 10.3184, acc 0.609375, prec 0.024934, recall 0.717277
2017-12-10T14:50:13.235421: step 414, loss 1.59759, acc 0.65625, prec 0.0248842, recall 0.717277
2017-12-10T14:50:13.430756: step 415, loss 1.07131, acc 0.6875, prec 0.0248391, recall 0.717277
2017-12-10T14:50:13.623438: step 416, loss 1.60295, acc 0.515625, prec 0.0248576, recall 0.718016
2017-12-10T14:50:13.810175: step 417, loss 3.09401, acc 0.515625, prec 0.0248761, recall 0.71875
2017-12-10T14:50:14.009423: step 418, loss 0.966699, acc 0.609375, prec 0.0248201, recall 0.71875
2017-12-10T14:50:14.207631: step 419, loss 1.83944, acc 0.515625, prec 0.0247511, recall 0.71875
2017-12-10T14:50:14.402734: step 420, loss 7.42576, acc 0.59375, prec 0.0246958, recall 0.716883
2017-12-10T14:50:14.594917: step 421, loss 1.69348, acc 0.546875, prec 0.0246319, recall 0.716883
2017-12-10T14:50:14.788389: step 422, loss 1.76475, acc 0.640625, prec 0.0247551, recall 0.718346
2017-12-10T14:50:14.981087: step 423, loss 2.93651, acc 0.59375, prec 0.0249578, recall 0.720513
2017-12-10T14:50:15.171593: step 424, loss 1.03029, acc 0.65625, prec 0.0249091, recall 0.720513
2017-12-10T14:50:15.360548: step 425, loss 0.860829, acc 0.71875, prec 0.0248695, recall 0.720513
2017-12-10T14:50:15.552206: step 426, loss 4.40642, acc 0.46875, prec 0.0248831, recall 0.719388
2017-12-10T14:50:15.747387: step 427, loss 1.60798, acc 0.5625, prec 0.0248218, recall 0.719388
2017-12-10T14:50:15.939534: step 428, loss 2.36192, acc 0.53125, prec 0.0247564, recall 0.719388
2017-12-10T14:50:16.132025: step 429, loss 2.02933, acc 0.65625, prec 0.0247941, recall 0.720102
2017-12-10T14:50:16.329437: step 430, loss 1.82068, acc 0.546875, prec 0.0248165, recall 0.720812
2017-12-10T14:50:16.521876: step 431, loss 1.91202, acc 0.515625, prec 0.0247495, recall 0.720812
2017-12-10T14:50:16.714135: step 432, loss 1.70635, acc 0.59375, prec 0.0246935, recall 0.720812
2017-12-10T14:50:16.902289: step 433, loss 1.33028, acc 0.59375, prec 0.0247224, recall 0.721519
2017-12-10T14:50:17.091155: step 434, loss 1.02925, acc 0.65625, prec 0.0248442, recall 0.722922
2017-12-10T14:50:17.280824: step 435, loss 0.6769, acc 0.75, prec 0.0248941, recall 0.723618
2017-12-10T14:50:17.472963: step 436, loss 1.39221, acc 0.671875, prec 0.0249331, recall 0.724311
2017-12-10T14:50:17.663864: step 437, loss 0.68025, acc 0.78125, prec 0.0249871, recall 0.725
2017-12-10T14:50:17.853191: step 438, loss 0.42891, acc 0.84375, prec 0.0249656, recall 0.725
2017-12-10T14:50:18.042435: step 439, loss 0.385485, acc 0.875, prec 0.0249484, recall 0.725
2017-12-10T14:50:18.231829: step 440, loss 0.761174, acc 0.796875, prec 0.0250043, recall 0.725686
2017-12-10T14:50:18.423222: step 441, loss 0.291789, acc 0.859375, prec 0.0250687, recall 0.726368
2017-12-10T14:50:18.615086: step 442, loss 0.465782, acc 0.84375, prec 0.0250472, recall 0.726368
2017-12-10T14:50:18.805907: step 443, loss 9.79215, acc 0.90625, prec 0.0250364, recall 0.724566
2017-12-10T14:50:18.999737: step 444, loss 4.69326, acc 0.90625, prec 0.0251093, recall 0.723457
2017-12-10T14:50:19.190316: step 445, loss 1.40514, acc 0.890625, prec 0.0252612, recall 0.724816
2017-12-10T14:50:19.379676: step 446, loss 0.876007, acc 0.890625, prec 0.0253295, recall 0.72549
2017-12-10T14:50:19.577919: step 447, loss 0.523451, acc 0.84375, prec 0.0254744, recall 0.726829
2017-12-10T14:50:19.772648: step 448, loss 0.649517, acc 0.828125, prec 0.0256169, recall 0.728155
2017-12-10T14:50:19.963435: step 449, loss 1.75112, acc 0.765625, prec 0.0256673, recall 0.728814
2017-12-10T14:50:20.155794: step 450, loss 1.24608, acc 0.765625, prec 0.0257174, recall 0.729469
2017-12-10T14:50:20.344015: step 451, loss 1.04293, acc 0.65625, prec 0.0256694, recall 0.729469
2017-12-10T14:50:20.534969: step 452, loss 0.944021, acc 0.703125, prec 0.025628, recall 0.729469
2017-12-10T14:50:20.726815: step 453, loss 1.2682, acc 0.6875, prec 0.0256671, recall 0.73012
2017-12-10T14:50:20.923681: step 454, loss 1.35183, acc 0.5625, prec 0.0256064, recall 0.73012
2017-12-10T14:50:21.116527: step 455, loss 2.20362, acc 0.65625, prec 0.0257232, recall 0.731415
2017-12-10T14:50:21.307948: step 456, loss 1.36722, acc 0.671875, prec 0.0257597, recall 0.732057
2017-12-10T14:50:21.499759: step 457, loss 1.28014, acc 0.65625, prec 0.0257121, recall 0.732057
2017-12-10T14:50:21.694174: step 458, loss 3.34624, acc 0.65625, prec 0.0256668, recall 0.73031
2017-12-10T14:50:21.893756: step 459, loss 1.37633, acc 0.59375, prec 0.025611, recall 0.73031
2017-12-10T14:50:22.086697: step 460, loss 1.28345, acc 0.640625, prec 0.0255618, recall 0.73031
2017-12-10T14:50:22.277369: step 461, loss 1.03608, acc 0.75, prec 0.0256089, recall 0.730952
2017-12-10T14:50:22.468226: step 462, loss 0.843788, acc 0.734375, prec 0.0255727, recall 0.730952
2017-12-10T14:50:22.659935: step 463, loss 0.660334, acc 0.765625, prec 0.0255408, recall 0.730952
2017-12-10T14:50:22.852849: step 464, loss 13.6752, acc 0.640625, prec 0.0254962, recall 0.727488
2017-12-10T14:50:23.043877: step 465, loss 1.21003, acc 0.65625, prec 0.0254497, recall 0.727488
2017-12-10T14:50:23.234331: step 466, loss 1.48498, acc 0.625, prec 0.0254798, recall 0.728132
2017-12-10T14:50:23.427032: step 467, loss 1.35026, acc 0.625, prec 0.0255098, recall 0.728774
2017-12-10T14:50:23.620574: step 468, loss 1.77499, acc 0.53125, prec 0.025527, recall 0.729412
2017-12-10T14:50:23.812240: step 469, loss 2.15719, acc 0.625, prec 0.0255567, recall 0.730047
2017-12-10T14:50:24.006491: step 470, loss 1.77145, acc 0.640625, prec 0.0255085, recall 0.730047
2017-12-10T14:50:24.197637: step 471, loss 1.52189, acc 0.59375, prec 0.0254542, recall 0.730047
2017-12-10T14:50:24.389914: step 472, loss 1.25475, acc 0.5625, prec 0.025396, recall 0.730047
2017-12-10T14:50:24.582279: step 473, loss 1.13467, acc 0.703125, prec 0.0254362, recall 0.730679
2017-12-10T14:50:24.775401: step 474, loss 0.974192, acc 0.734375, prec 0.0255596, recall 0.731935
2017-12-10T14:50:24.967526: step 475, loss 1.52849, acc 0.546875, prec 0.0255786, recall 0.732558
2017-12-10T14:50:25.162618: step 476, loss 1.06759, acc 0.734375, prec 0.0255433, recall 0.732558
2017-12-10T14:50:25.351270: step 477, loss 11.3476, acc 0.75, prec 0.0255143, recall 0.729167
2017-12-10T14:50:25.543362: step 478, loss 0.812189, acc 0.78125, prec 0.0254854, recall 0.729167
2017-12-10T14:50:25.732922: step 479, loss 1.9497, acc 0.640625, prec 0.0255168, recall 0.729792
2017-12-10T14:50:25.924686: step 480, loss 1.45283, acc 0.71875, prec 0.0255583, recall 0.730415
2017-12-10T14:50:26.114612: step 481, loss 1.1716, acc 0.6875, prec 0.025674, recall 0.731651
2017-12-10T14:50:26.308731: step 482, loss 1.30956, acc 0.71875, prec 0.0256369, recall 0.731651
2017-12-10T14:50:26.499792: step 483, loss 1.4139, acc 0.625, prec 0.0255876, recall 0.731651
2017-12-10T14:50:26.696435: step 484, loss 0.943152, acc 0.734375, prec 0.0255527, recall 0.731651
2017-12-10T14:50:26.891592: step 485, loss 1.31752, acc 0.703125, prec 0.0257476, recall 0.733485
2017-12-10T14:50:27.079741: step 486, loss 1.0024, acc 0.65625, prec 0.0257024, recall 0.733485
2017-12-10T14:50:27.271770: step 487, loss 0.9655, acc 0.78125, prec 0.0257514, recall 0.734091
2017-12-10T14:50:27.459640: step 488, loss 1.32249, acc 0.75, prec 0.0258737, recall 0.735294
2017-12-10T14:50:27.647040: step 489, loss 0.668668, acc 0.796875, prec 0.025847, recall 0.735294
2017-12-10T14:50:27.837122: step 490, loss 0.626009, acc 0.84375, prec 0.0259812, recall 0.736486
2017-12-10T14:50:28.026688: step 491, loss 0.934381, acc 0.78125, prec 0.0259524, recall 0.736486
2017-12-10T14:50:28.219031: step 492, loss 1.32587, acc 0.796875, prec 0.0262344, recall 0.738839
2017-12-10T14:50:28.410558: step 493, loss 0.784938, acc 0.75, prec 0.0262012, recall 0.738839
2017-12-10T14:50:28.598876: step 494, loss 2.291, acc 0.6875, prec 0.0263906, recall 0.740577
2017-12-10T14:50:28.791264: step 495, loss 0.583517, acc 0.84375, prec 0.0264467, recall 0.74115
2017-12-10T14:50:28.986354: step 496, loss 1.75944, acc 0.734375, prec 0.026488, recall 0.741722
2017-12-10T14:50:29.166436: step 497, loss 0.533631, acc 0.803922, prec 0.0265438, recall 0.742291
2017-12-10T14:50:29.363492: step 498, loss 1.48233, acc 0.65625, prec 0.0264979, recall 0.742291
2017-12-10T14:50:29.551863: step 499, loss 2.37518, acc 0.75, prec 0.0265431, recall 0.741228
2017-12-10T14:50:29.743166: step 500, loss 0.767817, acc 0.8125, prec 0.0265945, recall 0.741794
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_0/1512935327/checkpoints/model-500

2017-12-10T14:50:30.945281: step 501, loss 1.72199, acc 0.703125, prec 0.0267074, recall 0.742919
2017-12-10T14:50:31.147363: step 502, loss 0.567706, acc 0.8125, prec 0.0266823, recall 0.742919
2017-12-10T14:50:31.343530: step 503, loss 8.75768, acc 0.75, prec 0.026651, recall 0.741304
2017-12-10T14:50:31.547555: step 504, loss 0.719511, acc 0.765625, prec 0.0266958, recall 0.741866
2017-12-10T14:50:31.736472: step 505, loss 0.717129, acc 0.828125, prec 0.0267488, recall 0.742424
2017-12-10T14:50:31.923790: step 506, loss 0.701026, acc 0.671875, prec 0.0267051, recall 0.742424
2017-12-10T14:50:32.114750: step 507, loss 1.05552, acc 0.71875, prec 0.0266677, recall 0.742424
2017-12-10T14:50:32.304518: step 508, loss 1.30519, acc 0.671875, prec 0.0266242, recall 0.742424
2017-12-10T14:50:32.499073: step 509, loss 6.03294, acc 0.734375, prec 0.0265912, recall 0.740821
2017-12-10T14:50:32.693542: step 510, loss 0.582486, acc 0.8125, prec 0.0267173, recall 0.741935
2017-12-10T14:50:32.883023: step 511, loss 1.6367, acc 0.75, prec 0.0267595, recall 0.742489
2017-12-10T14:50:33.078008: step 512, loss 2.69041, acc 0.734375, prec 0.0267264, recall 0.740899
2017-12-10T14:50:33.271270: step 513, loss 1.42374, acc 0.5625, prec 0.0266687, recall 0.740899
2017-12-10T14:50:33.467464: step 514, loss 1.78438, acc 0.671875, prec 0.0268503, recall 0.742553
2017-12-10T14:50:33.661694: step 515, loss 5.10374, acc 0.65625, prec 0.026807, recall 0.740977
2017-12-10T14:50:33.856445: step 516, loss 1.43236, acc 0.625, prec 0.0268323, recall 0.741525
2017-12-10T14:50:34.048102: step 517, loss 9.72749, acc 0.515625, prec 0.0267707, recall 0.739958
2017-12-10T14:50:34.242751: step 518, loss 2.33374, acc 0.46875, prec 0.0267013, recall 0.739958
2017-12-10T14:50:34.436077: step 519, loss 2.91397, acc 0.421875, prec 0.0266261, recall 0.739958
2017-12-10T14:50:34.631664: step 520, loss 2.85519, acc 0.390625, prec 0.0266212, recall 0.740506
2017-12-10T14:50:34.820457: step 521, loss 2.96819, acc 0.390625, prec 0.0266163, recall 0.741053
2017-12-10T14:50:35.013401: step 522, loss 2.86131, acc 0.34375, prec 0.026532, recall 0.741053
2017-12-10T14:50:35.204795: step 523, loss 3.09352, acc 0.359375, prec 0.0266697, recall 0.742678
2017-12-10T14:50:35.394227: step 524, loss 2.65038, acc 0.421875, prec 0.0266687, recall 0.743215
2017-12-10T14:50:35.582342: step 525, loss 2.25803, acc 0.453125, prec 0.0265989, recall 0.743215
2017-12-10T14:50:35.772381: step 526, loss 1.8353, acc 0.609375, prec 0.0266945, recall 0.744283
2017-12-10T14:50:35.960804: step 527, loss 1.51212, acc 0.53125, prec 0.0267073, recall 0.744813
2017-12-10T14:50:36.149009: step 528, loss 1.34284, acc 0.65625, prec 0.026736, recall 0.745342
2017-12-10T14:50:36.343638: step 529, loss 1.31988, acc 0.609375, prec 0.0266864, recall 0.745342
2017-12-10T14:50:36.531093: step 530, loss 0.965827, acc 0.71875, prec 0.0266509, recall 0.745342
2017-12-10T14:50:36.719508: step 531, loss 0.562083, acc 0.828125, prec 0.0266292, recall 0.745342
2017-12-10T14:50:36.911124: step 532, loss 0.589637, acc 0.828125, prec 0.0267514, recall 0.746392
2017-12-10T14:50:37.101946: step 533, loss 0.380537, acc 0.875, prec 0.0268075, recall 0.746914
2017-12-10T14:50:37.297263: step 534, loss 2.89941, acc 0.765625, prec 0.0267798, recall 0.74538
2017-12-10T14:50:37.490676: step 535, loss 0.215607, acc 0.9375, prec 0.0268437, recall 0.745902
2017-12-10T14:50:37.686997: step 536, loss 0.383664, acc 0.84375, prec 0.0268956, recall 0.746421
2017-12-10T14:50:37.877076: step 537, loss 2.59223, acc 0.875, prec 0.0268817, recall 0.744898
2017-12-10T14:50:38.072118: step 538, loss 0.442463, acc 0.84375, prec 0.0268619, recall 0.744898
2017-12-10T14:50:38.264013: step 539, loss 1.95708, acc 0.90625, prec 0.0269952, recall 0.744422
2017-12-10T14:50:38.456214: step 540, loss 0.643619, acc 0.8125, prec 0.0269714, recall 0.744422
2017-12-10T14:50:38.654862: step 541, loss 8.92179, acc 0.90625, prec 0.027033, recall 0.743434
2017-12-10T14:50:38.853765: step 542, loss 0.385534, acc 0.859375, prec 0.0270865, recall 0.743952
2017-12-10T14:50:39.044300: step 543, loss 0.724158, acc 0.796875, prec 0.0270607, recall 0.743952
2017-12-10T14:50:39.234424: step 544, loss 0.650665, acc 0.8125, prec 0.0272507, recall 0.745491
2017-12-10T14:50:39.429370: step 545, loss 6.62464, acc 0.734375, prec 0.0272188, recall 0.744
2017-12-10T14:50:39.624059: step 546, loss 0.429301, acc 0.8125, prec 0.027195, recall 0.744
2017-12-10T14:50:39.813677: step 547, loss 0.454189, acc 0.859375, prec 0.0272482, recall 0.744511
2017-12-10T14:50:40.008358: step 548, loss 0.942505, acc 0.765625, prec 0.0273603, recall 0.745527
2017-12-10T14:50:40.202079: step 549, loss 0.635323, acc 0.796875, prec 0.0274052, recall 0.746032
2017-12-10T14:50:40.395787: step 550, loss 4.81657, acc 0.703125, prec 0.0275109, recall 0.745562
2017-12-10T14:50:40.595281: step 551, loss 2.87352, acc 0.609375, prec 0.0275336, recall 0.744597
2017-12-10T14:50:40.787638: step 552, loss 1.43345, acc 0.703125, prec 0.0275662, recall 0.745098
2017-12-10T14:50:40.976018: step 553, loss 1.12533, acc 0.75, prec 0.0275342, recall 0.745098
2017-12-10T14:50:41.168375: step 554, loss 1.19326, acc 0.640625, prec 0.0275588, recall 0.745597
2017-12-10T14:50:41.356182: step 555, loss 1.90203, acc 0.53125, prec 0.0275693, recall 0.746094
2017-12-10T14:50:41.550868: step 556, loss 2.19176, acc 0.578125, prec 0.0275857, recall 0.746589
2017-12-10T14:50:41.747317: step 557, loss 1.79024, acc 0.5, prec 0.0275922, recall 0.747082
2017-12-10T14:50:41.937686: step 558, loss 1.92451, acc 0.578125, prec 0.0276085, recall 0.747573
2017-12-10T14:50:42.132549: step 559, loss 1.64375, acc 0.578125, prec 0.0276247, recall 0.748062
2017-12-10T14:50:42.320921: step 560, loss 1.16548, acc 0.625, prec 0.0277163, recall 0.749035
2017-12-10T14:50:42.509501: step 561, loss 1.23471, acc 0.59375, prec 0.0276649, recall 0.749035
2017-12-10T14:50:42.700124: step 562, loss 1.39226, acc 0.65625, prec 0.0276216, recall 0.749035
2017-12-10T14:50:42.895548: step 563, loss 0.909816, acc 0.6875, prec 0.0277896, recall 0.75048
2017-12-10T14:50:43.089978: step 564, loss 0.844097, acc 0.765625, prec 0.02776, recall 0.75048
2017-12-10T14:50:43.283088: step 565, loss 1.28809, acc 0.765625, prec 0.0277994, recall 0.750958
2017-12-10T14:50:43.480354: step 566, loss 0.850546, acc 0.796875, prec 0.0279116, recall 0.751908
2017-12-10T14:50:43.671359: step 567, loss 1.64082, acc 0.71875, prec 0.0280823, recall 0.753321
2017-12-10T14:50:43.862856: step 568, loss 0.96589, acc 0.65625, prec 0.0281073, recall 0.753788
2017-12-10T14:50:44.064830: step 569, loss 0.824345, acc 0.75, prec 0.0280756, recall 0.753788
2017-12-10T14:50:44.263272: step 570, loss 0.455614, acc 0.84375, prec 0.0281243, recall 0.754253
2017-12-10T14:50:44.455554: step 571, loss 0.930447, acc 0.84375, prec 0.028173, recall 0.754717
2017-12-10T14:50:44.646107: step 572, loss 0.674103, acc 0.765625, prec 0.0281432, recall 0.754717
2017-12-10T14:50:44.839264: step 573, loss 8.16589, acc 0.78125, prec 0.0281858, recall 0.753759
2017-12-10T14:50:45.033454: step 574, loss 0.444532, acc 0.78125, prec 0.0281581, recall 0.753759
2017-12-10T14:50:45.226282: step 575, loss 3.5139, acc 0.71875, prec 0.0281246, recall 0.752345
2017-12-10T14:50:45.429224: step 576, loss 1.94918, acc 0.890625, prec 0.0283171, recall 0.752328
2017-12-10T14:50:45.621535: step 577, loss 0.705037, acc 0.75, prec 0.0282854, recall 0.752328
2017-12-10T14:50:45.817250: step 578, loss 0.585298, acc 0.8125, prec 0.0283976, recall 0.753247
2017-12-10T14:50:46.012499: step 579, loss 3.21366, acc 0.8125, prec 0.0283757, recall 0.751852
2017-12-10T14:50:46.207809: step 580, loss 0.916501, acc 0.796875, prec 0.02835, recall 0.751852
2017-12-10T14:50:46.397142: step 581, loss 1.12464, acc 0.65625, prec 0.0283065, recall 0.751852
2017-12-10T14:50:46.588875: step 582, loss 1.37253, acc 0.65625, prec 0.0283308, recall 0.752311
2017-12-10T14:50:46.779383: step 583, loss 1.32897, acc 0.625, prec 0.0284186, recall 0.753223
2017-12-10T14:50:46.972775: step 584, loss 1.04262, acc 0.765625, prec 0.028389, recall 0.753223
2017-12-10T14:50:47.162046: step 585, loss 1.44911, acc 0.609375, prec 0.0284744, recall 0.754128
2017-12-10T14:50:47.354544: step 586, loss 1.3757, acc 0.578125, prec 0.0284213, recall 0.754128
2017-12-10T14:50:47.546126: step 587, loss 3.35577, acc 0.71875, prec 0.028455, recall 0.753199
2017-12-10T14:50:47.744617: step 588, loss 1.00149, acc 0.75, prec 0.0284236, recall 0.753199
2017-12-10T14:50:47.938495: step 589, loss 1.30597, acc 0.625, prec 0.0283766, recall 0.753199
2017-12-10T14:50:48.130661: step 590, loss 1.31362, acc 0.65625, prec 0.0284673, recall 0.754098
2017-12-10T14:50:48.323567: step 591, loss 1.331, acc 0.640625, prec 0.0284224, recall 0.754098
2017-12-10T14:50:48.515809: step 592, loss 0.827795, acc 0.796875, prec 0.0285303, recall 0.754991
2017-12-10T14:50:48.708429: step 593, loss 0.715663, acc 0.703125, prec 0.0285597, recall 0.755435
2017-12-10T14:50:48.899233: step 594, loss 0.779094, acc 0.765625, prec 0.0285304, recall 0.755435
2017-12-10T14:50:49.085685: step 595, loss 6.84898, acc 0.671875, prec 0.0284914, recall 0.754069
2017-12-10T14:50:49.278449: step 596, loss 1.16459, acc 0.6875, prec 0.0284525, recall 0.754069
2017-12-10T14:50:49.472388: step 597, loss 0.749706, acc 0.765625, prec 0.0284234, recall 0.754069
2017-12-10T14:50:49.666167: step 598, loss 0.691616, acc 0.765625, prec 0.0284605, recall 0.754513
2017-12-10T14:50:49.860055: step 599, loss 0.561622, acc 0.796875, prec 0.0285015, recall 0.754955
2017-12-10T14:50:50.051888: step 600, loss 0.840141, acc 0.71875, prec 0.0284666, recall 0.754955
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_0/1512935327/checkpoints/model-600

2017-12-10T14:50:51.269509: step 601, loss 0.489192, acc 0.859375, prec 0.0284492, recall 0.754955
2017-12-10T14:50:51.460134: step 602, loss 0.464379, acc 0.859375, prec 0.0285637, recall 0.755835
2017-12-10T14:50:51.656183: step 603, loss 1.10895, acc 0.796875, prec 0.0286044, recall 0.756272
2017-12-10T14:50:51.845372: step 604, loss 2.10811, acc 0.8125, prec 0.028583, recall 0.754919
2017-12-10T14:50:52.044098: step 605, loss 1.25383, acc 0.796875, prec 0.0286236, recall 0.755357
2017-12-10T14:50:52.235451: step 606, loss 0.461829, acc 0.84375, prec 0.0286043, recall 0.755357
2017-12-10T14:50:52.427816: step 607, loss 0.472906, acc 0.828125, prec 0.0286486, recall 0.755793
2017-12-10T14:50:52.618375: step 608, loss 2.27502, acc 0.78125, prec 0.0286891, recall 0.754885
2017-12-10T14:50:52.816700: step 609, loss 0.874815, acc 0.8125, prec 0.0287314, recall 0.755319
2017-12-10T14:50:53.007593: step 610, loss 0.497624, acc 0.828125, prec 0.0287101, recall 0.755319
2017-12-10T14:50:53.203016: step 611, loss 0.557465, acc 0.8125, prec 0.0287523, recall 0.755752
2017-12-10T14:50:53.393959: step 612, loss 1.07345, acc 0.671875, prec 0.0287117, recall 0.755752
2017-12-10T14:50:53.587107: step 613, loss 0.776252, acc 0.765625, prec 0.0288132, recall 0.756614
2017-12-10T14:50:53.783391: step 614, loss 0.611664, acc 0.84375, prec 0.0287939, recall 0.756614
2017-12-10T14:50:53.974010: step 615, loss 1.06626, acc 0.765625, prec 0.0287649, recall 0.756614
2017-12-10T14:50:54.168855: step 616, loss 0.721266, acc 0.78125, prec 0.0287379, recall 0.756614
2017-12-10T14:50:54.357996: step 617, loss 0.846241, acc 0.796875, prec 0.0287129, recall 0.756614
2017-12-10T14:50:54.548682: step 618, loss 0.573878, acc 0.828125, prec 0.0288217, recall 0.757469
2017-12-10T14:50:54.748124: step 619, loss 0.462996, acc 0.828125, prec 0.0288654, recall 0.757895
2017-12-10T14:50:54.937536: step 620, loss 0.449717, acc 0.84375, prec 0.0290407, recall 0.759162
2017-12-10T14:50:55.126346: step 621, loss 0.318864, acc 0.90625, prec 0.029029, recall 0.759162
2017-12-10T14:50:55.317605: step 622, loss 0.440688, acc 0.828125, prec 0.0290077, recall 0.759162
2017-12-10T14:50:55.511833: step 623, loss 0.431809, acc 0.875, prec 0.0289923, recall 0.759162
2017-12-10T14:50:55.700619: step 624, loss 0.391968, acc 0.90625, prec 0.0289807, recall 0.759162
2017-12-10T14:50:55.892896: step 625, loss 0.249907, acc 0.90625, prec 0.0289691, recall 0.759162
2017-12-10T14:50:56.087617: step 626, loss 1.55675, acc 0.90625, prec 0.0289595, recall 0.75784
2017-12-10T14:50:56.280980: step 627, loss 6.69272, acc 0.828125, prec 0.0289402, recall 0.756522
2017-12-10T14:50:56.471955: step 628, loss 6.25356, acc 0.890625, prec 0.0289286, recall 0.755208
2017-12-10T14:50:56.667611: step 629, loss 0.314841, acc 0.90625, prec 0.0289171, recall 0.755208
2017-12-10T14:50:56.857720: step 630, loss 0.588901, acc 0.9375, prec 0.0290385, recall 0.756055
2017-12-10T14:50:57.048144: step 631, loss 0.367364, acc 0.890625, prec 0.029025, recall 0.756055
2017-12-10T14:50:57.239551: step 632, loss 0.41439, acc 0.828125, prec 0.0290682, recall 0.756477
2017-12-10T14:50:57.435131: step 633, loss 0.467054, acc 0.796875, prec 0.0290432, recall 0.756477
2017-12-10T14:50:57.628999: step 634, loss 0.90427, acc 0.78125, prec 0.0291449, recall 0.757315
2017-12-10T14:50:57.825120: step 635, loss 0.796857, acc 0.75, prec 0.0291782, recall 0.757732
2017-12-10T14:50:58.019250: step 636, loss 0.434947, acc 0.84375, prec 0.029159, recall 0.757732
2017-12-10T14:50:58.210529: step 637, loss 0.645027, acc 0.8125, prec 0.0292641, recall 0.758562
2017-12-10T14:50:58.403551: step 638, loss 0.793776, acc 0.734375, prec 0.0292953, recall 0.758974
2017-12-10T14:50:58.598381: step 639, loss 3.1473, acc 0.828125, prec 0.029276, recall 0.757679
2017-12-10T14:50:58.789649: step 640, loss 1.03746, acc 0.609375, prec 0.0292278, recall 0.757679
2017-12-10T14:50:58.982032: step 641, loss 1.13055, acc 0.703125, prec 0.0291913, recall 0.757679
2017-12-10T14:50:59.182623: step 642, loss 0.962365, acc 0.734375, prec 0.0292225, recall 0.758092
2017-12-10T14:50:59.372761: step 643, loss 0.749524, acc 0.703125, prec 0.0292497, recall 0.758503
2017-12-10T14:50:59.568931: step 644, loss 6.39721, acc 0.71875, prec 0.0292808, recall 0.757627
2017-12-10T14:50:59.768240: step 645, loss 0.667642, acc 0.78125, prec 0.0292539, recall 0.757627
2017-12-10T14:50:59.959166: step 646, loss 1.0763, acc 0.65625, prec 0.0292753, recall 0.758037
2017-12-10T14:51:00.152845: step 647, loss 1.22777, acc 0.734375, prec 0.0294329, recall 0.759259
2017-12-10T14:51:00.350740: step 648, loss 1.91749, acc 0.765625, prec 0.0295939, recall 0.760469
2017-12-10T14:51:00.545522: step 649, loss 1.17148, acc 0.578125, prec 0.0295419, recall 0.760469
2017-12-10T14:51:00.740799: step 650, loss 1.45621, acc 0.65625, prec 0.0294997, recall 0.760469
2017-12-10T14:51:00.932722: step 651, loss 1.56083, acc 0.609375, prec 0.0295777, recall 0.761269
2017-12-10T14:51:01.126764: step 652, loss 1.53446, acc 0.65625, prec 0.0295984, recall 0.761667
2017-12-10T14:51:01.321198: step 653, loss 0.861832, acc 0.71875, prec 0.0296268, recall 0.762063
2017-12-10T14:51:01.519595: step 654, loss 3.92852, acc 0.65625, prec 0.0296512, recall 0.759934
2017-12-10T14:51:01.709985: step 655, loss 4.32317, acc 0.71875, prec 0.0296812, recall 0.759076
2017-12-10T14:51:01.906316: step 656, loss 1.28221, acc 0.640625, prec 0.0296373, recall 0.759076
2017-12-10T14:51:02.105437: step 657, loss 1.80062, acc 0.5625, prec 0.0295839, recall 0.759076
2017-12-10T14:51:02.300235: step 658, loss 2.15139, acc 0.5625, prec 0.029593, recall 0.759473
2017-12-10T14:51:02.491730: step 659, loss 1.46593, acc 0.609375, prec 0.0295456, recall 0.759473
2017-12-10T14:51:02.684372: step 660, loss 1.72708, acc 0.515625, prec 0.029487, recall 0.759473
2017-12-10T14:51:02.880708: step 661, loss 1.63621, acc 0.59375, prec 0.029562, recall 0.760263
2017-12-10T14:51:03.071695: step 662, loss 1.26433, acc 0.59375, prec 0.029513, recall 0.760263
2017-12-10T14:51:03.263023: step 663, loss 1.48179, acc 0.65625, prec 0.0294717, recall 0.760263
2017-12-10T14:51:03.456283: step 664, loss 1.22338, acc 0.703125, prec 0.0294978, recall 0.760656
2017-12-10T14:51:03.645578: step 665, loss 1.09124, acc 0.703125, prec 0.0294622, recall 0.760656
2017-12-10T14:51:03.841084: step 666, loss 1.18132, acc 0.65625, prec 0.0294211, recall 0.760656
2017-12-10T14:51:04.031527: step 667, loss 1.30973, acc 0.671875, prec 0.029382, recall 0.760656
2017-12-10T14:51:04.220581: step 668, loss 0.867094, acc 0.78125, prec 0.0294173, recall 0.761047
2017-12-10T14:51:04.411555: step 669, loss 0.791724, acc 0.796875, prec 0.0295772, recall 0.762215
2017-12-10T14:51:04.606356: step 670, loss 7.25933, acc 0.671875, prec 0.0296011, recall 0.761364
2017-12-10T14:51:04.800104: step 671, loss 3.18672, acc 0.765625, prec 0.0296362, recall 0.760518
2017-12-10T14:51:04.990568: step 672, loss 0.628021, acc 0.828125, prec 0.0296156, recall 0.760518
2017-12-10T14:51:05.185205: step 673, loss 0.429879, acc 0.8125, prec 0.0295932, recall 0.760518
2017-12-10T14:51:05.377236: step 674, loss 0.828028, acc 0.8125, prec 0.029754, recall 0.761675
2017-12-10T14:51:05.570062: step 675, loss 0.629352, acc 0.8125, prec 0.0297316, recall 0.761675
2017-12-10T14:51:05.764922: step 676, loss 1.01844, acc 0.890625, prec 0.0299014, recall 0.762821
2017-12-10T14:51:05.958015: step 677, loss 0.826927, acc 0.765625, prec 0.0299341, recall 0.7632
2017-12-10T14:51:06.153232: step 678, loss 0.593011, acc 0.734375, prec 0.0299022, recall 0.7632
2017-12-10T14:51:06.347401: step 679, loss 0.618229, acc 0.765625, prec 0.0298741, recall 0.7632
2017-12-10T14:51:06.537850: step 680, loss 0.266162, acc 0.921875, prec 0.0299862, recall 0.763955
2017-12-10T14:51:06.734098: step 681, loss 0.918774, acc 0.75, prec 0.0300169, recall 0.764331
2017-12-10T14:51:06.925843: step 682, loss 0.517342, acc 0.84375, prec 0.0300587, recall 0.764706
2017-12-10T14:51:07.114255: step 683, loss 0.363058, acc 0.84375, prec 0.03004, recall 0.764706
2017-12-10T14:51:07.302829: step 684, loss 0.233678, acc 0.9375, prec 0.0300325, recall 0.764706
2017-12-10T14:51:07.494507: step 685, loss 5.40844, acc 0.875, prec 0.0300193, recall 0.763492
2017-12-10T14:51:07.690769: step 686, loss 0.258867, acc 0.9375, prec 0.0300119, recall 0.763492
2017-12-10T14:51:07.882843: step 687, loss 0.490894, acc 0.875, prec 0.0299969, recall 0.763492
2017-12-10T14:51:08.077437: step 688, loss 2.26651, acc 0.953125, prec 0.0299931, recall 0.762282
2017-12-10T14:51:08.272121: step 689, loss 0.670873, acc 0.84375, prec 0.0300349, recall 0.762658
2017-12-10T14:51:08.461440: step 690, loss 0.539137, acc 0.796875, prec 0.0300106, recall 0.762658
2017-12-10T14:51:08.650625: step 691, loss 0.327385, acc 0.875, prec 0.0299956, recall 0.762658
2017-12-10T14:51:08.843044: step 692, loss 0.779092, acc 0.75, prec 0.0300261, recall 0.763033
2017-12-10T14:51:09.032119: step 693, loss 0.807567, acc 0.765625, prec 0.0300584, recall 0.763407
2017-12-10T14:51:09.221995: step 694, loss 1.35431, acc 0.765625, prec 0.0301508, recall 0.764151
2017-12-10T14:51:09.417953: step 695, loss 0.728807, acc 0.765625, prec 0.0301227, recall 0.764151
2017-12-10T14:51:09.608479: step 696, loss 0.699653, acc 0.84375, prec 0.0301641, recall 0.764521
2017-12-10T14:51:09.807185: step 697, loss 1.76494, acc 0.828125, prec 0.0302055, recall 0.763693
2017-12-10T14:51:10.000946: step 698, loss 0.497807, acc 0.828125, prec 0.0301849, recall 0.763693
2017-12-10T14:51:10.193754: step 699, loss 0.891485, acc 0.796875, prec 0.0302206, recall 0.764063
2017-12-10T14:51:10.388611: step 700, loss 0.788478, acc 0.765625, prec 0.0301926, recall 0.764063
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_0/1512935327/checkpoints/model-700

2017-12-10T14:51:11.559286: step 701, loss 1.70096, acc 0.734375, prec 0.0302806, recall 0.764798
2017-12-10T14:51:11.751050: step 702, loss 0.732427, acc 0.8125, prec 0.0302582, recall 0.764798
2017-12-10T14:51:11.944006: step 703, loss 3.77128, acc 0.84375, prec 0.0303012, recall 0.763975
2017-12-10T14:51:12.136790: step 704, loss 0.983862, acc 0.65625, prec 0.0302602, recall 0.763975
2017-12-10T14:51:12.327564: step 705, loss 0.77568, acc 0.734375, prec 0.0302881, recall 0.764341
2017-12-10T14:51:12.516392: step 706, loss 7.16621, acc 0.765625, prec 0.0302621, recall 0.763158
2017-12-10T14:51:12.712877: step 707, loss 1.04013, acc 0.703125, prec 0.0302269, recall 0.763158
2017-12-10T14:51:12.902704: step 708, loss 15.0844, acc 0.65625, prec 0.0303086, recall 0.761538
2017-12-10T14:51:13.100347: step 709, loss 1.35781, acc 0.625, prec 0.0303827, recall 0.76227
2017-12-10T14:51:13.294099: step 710, loss 2.15681, acc 0.5, prec 0.0303825, recall 0.762634
2017-12-10T14:51:13.482861: step 711, loss 2.45085, acc 0.46875, prec 0.0303787, recall 0.762997
2017-12-10T14:51:13.673939: step 712, loss 2.13869, acc 0.53125, prec 0.0303233, recall 0.762997
2017-12-10T14:51:13.861357: step 713, loss 2.32675, acc 0.421875, prec 0.0303141, recall 0.763359
2017-12-10T14:51:14.054375: step 714, loss 2.4571, acc 0.40625, prec 0.0302444, recall 0.763359
2017-12-10T14:51:14.255557: step 715, loss 1.66415, acc 0.46875, prec 0.0304164, recall 0.764795
2017-12-10T14:51:14.448114: step 716, loss 2.24567, acc 0.34375, prec 0.0303979, recall 0.765152
2017-12-10T14:51:14.639610: step 717, loss 1.77438, acc 0.4375, prec 0.0304486, recall 0.765861
2017-12-10T14:51:14.828439: step 718, loss 2.07738, acc 0.515625, prec 0.0304502, recall 0.766214
2017-12-10T14:51:15.019349: step 719, loss 1.66804, acc 0.515625, prec 0.0303937, recall 0.766214
2017-12-10T14:51:15.215641: step 720, loss 1.48579, acc 0.59375, prec 0.0303465, recall 0.766214
2017-12-10T14:51:15.407798: step 721, loss 1.20421, acc 0.609375, prec 0.0303012, recall 0.766214
2017-12-10T14:51:15.603038: step 722, loss 1.22712, acc 0.703125, prec 0.0302669, recall 0.766214
2017-12-10T14:51:15.797781: step 723, loss 1.43305, acc 0.59375, prec 0.0303355, recall 0.766917
2017-12-10T14:51:15.990458: step 724, loss 0.826416, acc 0.65625, prec 0.0302958, recall 0.766917
2017-12-10T14:51:16.182452: step 725, loss 0.38048, acc 0.84375, prec 0.0303354, recall 0.767267
2017-12-10T14:51:16.376082: step 726, loss 0.568629, acc 0.828125, prec 0.0304307, recall 0.767964
2017-12-10T14:51:16.572225: step 727, loss 0.523684, acc 0.796875, prec 0.0304647, recall 0.768311
2017-12-10T14:51:16.761509: step 728, loss 0.357018, acc 0.84375, prec 0.0304466, recall 0.768311
2017-12-10T14:51:16.954876: step 729, loss 0.28753, acc 0.921875, prec 0.0304376, recall 0.768311
2017-12-10T14:51:17.145064: step 730, loss 0.166442, acc 0.9375, prec 0.0304304, recall 0.768311
2017-12-10T14:51:17.340719: step 731, loss 0.108306, acc 0.953125, prec 0.030425, recall 0.768311
2017-12-10T14:51:17.532263: step 732, loss 0.4306, acc 0.890625, prec 0.0304124, recall 0.768311
2017-12-10T14:51:17.722031: step 733, loss 0.525087, acc 0.953125, prec 0.0304644, recall 0.768657
2017-12-10T14:51:17.924546: step 734, loss 9.25898, acc 0.9375, prec 0.0305163, recall 0.767857
2017-12-10T14:51:18.121328: step 735, loss 3.41915, acc 0.90625, prec 0.0305091, recall 0.765579
2017-12-10T14:51:18.315844: step 736, loss 1.12277, acc 0.921875, prec 0.0305574, recall 0.765926
2017-12-10T14:51:18.512373: step 737, loss 11.188, acc 0.84375, prec 0.0305447, recall 0.762537
2017-12-10T14:51:18.707501: step 738, loss 0.139019, acc 0.953125, prec 0.0305393, recall 0.762537
2017-12-10T14:51:18.900495: step 739, loss 0.287696, acc 0.890625, prec 0.0305839, recall 0.762887
2017-12-10T14:51:19.090883: step 740, loss 0.356135, acc 0.859375, prec 0.0306249, recall 0.763235
2017-12-10T14:51:19.281542: step 741, loss 0.584245, acc 0.8125, prec 0.0306032, recall 0.763235
2017-12-10T14:51:19.476395: step 742, loss 1.05761, acc 0.640625, prec 0.0306189, recall 0.763583
2017-12-10T14:51:19.668704: step 743, loss 1.24432, acc 0.640625, prec 0.0306344, recall 0.76393
2017-12-10T14:51:19.858538: step 744, loss 0.877945, acc 0.671875, prec 0.0307105, recall 0.76462
2017-12-10T14:51:20.047818: step 745, loss 1.13786, acc 0.625, prec 0.0309514, recall 0.766328
2017-12-10T14:51:20.234715: step 746, loss 1.43898, acc 0.625, prec 0.0309646, recall 0.766667
2017-12-10T14:51:20.424719: step 747, loss 1.86725, acc 0.515625, prec 0.0309086, recall 0.766667
2017-12-10T14:51:20.618874: step 748, loss 1.78558, acc 0.578125, prec 0.0309164, recall 0.767004
2017-12-10T14:51:20.810185: step 749, loss 1.3754, acc 0.578125, prec 0.0308678, recall 0.767004
2017-12-10T14:51:21.003736: step 750, loss 1.55537, acc 0.59375, prec 0.0309338, recall 0.767677
2017-12-10T14:51:21.196662: step 751, loss 1.63304, acc 0.5625, prec 0.0308835, recall 0.767677
2017-12-10T14:51:21.387748: step 752, loss 1.27835, acc 0.625, prec 0.0308968, recall 0.768012
2017-12-10T14:51:21.581656: step 753, loss 1.5518, acc 0.59375, prec 0.0308503, recall 0.768012
2017-12-10T14:51:21.774797: step 754, loss 5.74575, acc 0.765625, prec 0.0308253, recall 0.766906
2017-12-10T14:51:21.968581: step 755, loss 0.711152, acc 0.765625, prec 0.0307986, recall 0.766906
2017-12-10T14:51:22.158373: step 756, loss 0.993485, acc 0.734375, prec 0.0308243, recall 0.767241
2017-12-10T14:51:22.351134: step 757, loss 0.889952, acc 0.703125, prec 0.0307905, recall 0.767241
2017-12-10T14:51:22.545955: step 758, loss 5.17763, acc 0.796875, prec 0.0308251, recall 0.766476
2017-12-10T14:51:22.742215: step 759, loss 0.766022, acc 0.71875, prec 0.0307931, recall 0.766476
2017-12-10T14:51:22.933592: step 760, loss 0.626411, acc 0.8125, prec 0.0308276, recall 0.76681
2017-12-10T14:51:23.125490: step 761, loss 4.36031, acc 0.671875, prec 0.0307922, recall 0.765714
2017-12-10T14:51:23.323327: step 762, loss 0.931077, acc 0.765625, prec 0.0308213, recall 0.766048
2017-12-10T14:51:23.515655: step 763, loss 0.461003, acc 0.84375, prec 0.0308036, recall 0.766048
2017-12-10T14:51:23.704188: step 764, loss 1.60232, acc 0.6875, prec 0.0308239, recall 0.766382
2017-12-10T14:51:23.898611: step 765, loss 0.92367, acc 0.703125, prec 0.0307904, recall 0.766382
2017-12-10T14:51:24.088294: step 766, loss 0.797319, acc 0.796875, prec 0.0307675, recall 0.766382
2017-12-10T14:51:24.283664: step 767, loss 1.09684, acc 0.578125, prec 0.03072, recall 0.766382
2017-12-10T14:51:24.470607: step 768, loss 0.925292, acc 0.671875, prec 0.0306832, recall 0.766382
2017-12-10T14:51:24.667724: step 769, loss 0.721027, acc 0.734375, prec 0.0306535, recall 0.766382
2017-12-10T14:51:24.857392: step 770, loss 0.853343, acc 0.6875, prec 0.0306738, recall 0.766714
2017-12-10T14:51:25.049480: step 771, loss 1.38215, acc 0.796875, prec 0.0307614, recall 0.767376
2017-12-10T14:51:25.237919: step 772, loss 3.10351, acc 0.84375, prec 0.0308007, recall 0.76662
2017-12-10T14:51:25.429209: step 773, loss 0.584576, acc 0.796875, prec 0.030778, recall 0.76662
2017-12-10T14:51:25.621622: step 774, loss 0.806192, acc 0.796875, prec 0.0308103, recall 0.766949
2017-12-10T14:51:25.812697: step 775, loss 0.732896, acc 0.703125, prec 0.0307771, recall 0.766949
2017-12-10T14:51:26.003084: step 776, loss 0.595172, acc 0.78125, prec 0.0307527, recall 0.766949
2017-12-10T14:51:26.194174: step 777, loss 2.26767, acc 0.75, prec 0.0307266, recall 0.765867
2017-12-10T14:51:26.388086: step 778, loss 0.599971, acc 0.765625, prec 0.0307005, recall 0.765867
2017-12-10T14:51:26.583126: step 779, loss 0.85585, acc 0.703125, prec 0.0307223, recall 0.766197
2017-12-10T14:51:26.772446: step 780, loss 0.623485, acc 0.8125, prec 0.0307015, recall 0.766197
2017-12-10T14:51:26.961683: step 781, loss 0.928503, acc 0.75, prec 0.0306738, recall 0.766197
2017-12-10T14:51:27.157563: step 782, loss 1.33477, acc 0.8125, prec 0.0308169, recall 0.767181
2017-12-10T14:51:27.353607: step 783, loss 0.686479, acc 0.78125, prec 0.0307926, recall 0.767181
2017-12-10T14:51:27.551266: step 784, loss 0.960729, acc 0.859375, prec 0.0308316, recall 0.767507
2017-12-10T14:51:27.744238: step 785, loss 0.847883, acc 0.671875, prec 0.0308496, recall 0.767832
2017-12-10T14:51:27.939443: step 786, loss 0.501672, acc 0.796875, prec 0.0308271, recall 0.767832
2017-12-10T14:51:28.132805: step 787, loss 10.1978, acc 0.78125, prec 0.030859, recall 0.767085
2017-12-10T14:51:28.326397: step 788, loss 0.60473, acc 0.734375, prec 0.0308296, recall 0.767085
2017-12-10T14:51:28.522097: step 789, loss 2.03738, acc 0.734375, prec 0.030802, recall 0.766017
2017-12-10T14:51:28.713477: step 790, loss 0.99912, acc 0.65625, prec 0.0308183, recall 0.766342
2017-12-10T14:51:28.912206: step 791, loss 1.86715, acc 0.828125, prec 0.0308011, recall 0.765278
2017-12-10T14:51:29.113860: step 792, loss 0.453385, acc 0.84375, prec 0.0307838, recall 0.765278
2017-12-10T14:51:29.313489: step 793, loss 0.805242, acc 0.75, prec 0.0307563, recall 0.765278
2017-12-10T14:51:29.502524: step 794, loss 1.76769, acc 0.734375, prec 0.0307812, recall 0.765603
2017-12-10T14:51:29.692822: step 795, loss 0.769154, acc 0.75, prec 0.0307538, recall 0.765603
2017-12-10T14:51:29.884362: step 796, loss 0.929244, acc 0.671875, prec 0.0307179, recall 0.765603
2017-12-10T14:51:30.078267: step 797, loss 0.764436, acc 0.765625, prec 0.0307461, recall 0.765928
2017-12-10T14:51:30.271255: step 798, loss 1.2018, acc 0.734375, prec 0.0308248, recall 0.766575
2017-12-10T14:51:30.464625: step 799, loss 1.2611, acc 0.578125, prec 0.0308324, recall 0.766897
2017-12-10T14:51:30.654927: step 800, loss 0.961574, acc 0.6875, prec 0.0309592, recall 0.767857
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_0/1512935327/checkpoints/model-800

2017-12-10T14:51:31.870818: step 801, loss 1.11262, acc 0.65625, prec 0.0309216, recall 0.767857
2017-12-10T14:51:32.064484: step 802, loss 0.867516, acc 0.75, prec 0.0308942, recall 0.767857
2017-12-10T14:51:32.253258: step 803, loss 0.510151, acc 0.828125, prec 0.0308754, recall 0.767857
2017-12-10T14:51:32.440976: step 804, loss 0.79993, acc 0.734375, prec 0.0309, recall 0.768176
2017-12-10T14:51:32.637602: step 805, loss 2.39146, acc 0.671875, prec 0.0308659, recall 0.767123
2017-12-10T14:51:32.829564: step 806, loss 0.62581, acc 0.796875, prec 0.0308438, recall 0.767123
2017-12-10T14:51:33.023512: step 807, loss 0.694387, acc 0.734375, prec 0.0308149, recall 0.767123
2017-12-10T14:51:33.215620: step 808, loss 0.773929, acc 0.75, prec 0.0307878, recall 0.767123
2017-12-10T14:51:33.408957: step 809, loss 0.833482, acc 0.6875, prec 0.030754, recall 0.767123
2017-12-10T14:51:33.600372: step 810, loss 0.634465, acc 0.765625, prec 0.0307287, recall 0.767123
2017-12-10T14:51:33.792014: step 811, loss 2.8031, acc 0.765625, prec 0.0308114, recall 0.766712
2017-12-10T14:51:33.982043: step 812, loss 0.37195, acc 0.78125, prec 0.0307878, recall 0.766712
2017-12-10T14:51:34.174296: step 813, loss 0.739373, acc 0.734375, prec 0.0308122, recall 0.76703
2017-12-10T14:51:34.368036: step 814, loss 0.248277, acc 0.875, prec 0.0308517, recall 0.767347
2017-12-10T14:51:34.560621: step 815, loss 7.97231, acc 0.859375, prec 0.0308382, recall 0.766304
2017-12-10T14:51:34.756665: step 816, loss 0.536418, acc 0.8125, prec 0.030818, recall 0.766304
2017-12-10T14:51:34.948909: step 817, loss 0.609963, acc 0.796875, prec 0.030849, recall 0.766621
2017-12-10T14:51:35.143107: step 818, loss 0.437057, acc 0.859375, prec 0.0308339, recall 0.766621
2017-12-10T14:51:35.332943: step 819, loss 0.705052, acc 0.859375, prec 0.0309773, recall 0.767568
2017-12-10T14:51:35.521244: step 820, loss 0.783832, acc 0.71875, prec 0.0309997, recall 0.767881
2017-12-10T14:51:35.710128: step 821, loss 0.822223, acc 0.765625, prec 0.0310272, recall 0.768194
2017-12-10T14:51:35.897728: step 822, loss 0.448241, acc 0.8125, prec 0.0310069, recall 0.768194
2017-12-10T14:51:36.089110: step 823, loss 0.439143, acc 0.796875, prec 0.030985, recall 0.768194
2017-12-10T14:51:36.282245: step 824, loss 2.16968, acc 0.703125, prec 0.0309547, recall 0.76716
2017-12-10T14:51:36.478350: step 825, loss 0.569607, acc 0.8125, prec 0.0309345, recall 0.76716
2017-12-10T14:51:36.670016: step 826, loss 0.571272, acc 0.796875, prec 0.0309653, recall 0.767473
2017-12-10T14:51:36.864543: step 827, loss 0.680978, acc 0.84375, prec 0.031001, recall 0.767785
2017-12-10T14:51:37.056894: step 828, loss 3.20741, acc 0.890625, prec 0.0311484, recall 0.76769
2017-12-10T14:51:37.254047: step 829, loss 1.92321, acc 0.8125, prec 0.0311299, recall 0.766667
2017-12-10T14:51:37.448357: step 830, loss 0.510867, acc 0.796875, prec 0.031108, recall 0.766667
2017-12-10T14:51:37.641119: step 831, loss 2.01983, acc 0.71875, prec 0.0311318, recall 0.765957
2017-12-10T14:51:37.830432: step 832, loss 0.543537, acc 0.828125, prec 0.0311656, recall 0.766268
2017-12-10T14:51:38.021871: step 833, loss 0.947142, acc 0.75, prec 0.031191, recall 0.766578
2017-12-10T14:51:38.213297: step 834, loss 1.20251, acc 0.71875, prec 0.0312129, recall 0.766887
2017-12-10T14:51:38.405524: step 835, loss 0.958578, acc 0.6875, prec 0.0312315, recall 0.767196
2017-12-10T14:51:38.594607: step 836, loss 1.32961, acc 0.625, prec 0.0312433, recall 0.767503
2017-12-10T14:51:38.788144: step 837, loss 2.5632, acc 0.59375, prec 0.0312517, recall 0.76781
2017-12-10T14:51:38.979220: step 838, loss 0.849593, acc 0.703125, prec 0.0312198, recall 0.76781
2017-12-10T14:51:39.176809: step 839, loss 0.95564, acc 0.71875, prec 0.0311897, recall 0.76781
2017-12-10T14:51:39.365398: step 840, loss 1.35635, acc 0.671875, prec 0.0312584, recall 0.768421
2017-12-10T14:51:39.552035: step 841, loss 3.14925, acc 0.71875, prec 0.0313853, recall 0.768325
2017-12-10T14:51:39.746714: step 842, loss 1.7022, acc 0.65625, prec 0.0314519, recall 0.768929
2017-12-10T14:51:39.939588: step 843, loss 0.700425, acc 0.6875, prec 0.0314184, recall 0.768929
2017-12-10T14:51:40.130019: step 844, loss 1.39177, acc 0.59375, prec 0.0314264, recall 0.769231
2017-12-10T14:51:40.320595: step 845, loss 0.948857, acc 0.65625, prec 0.0314412, recall 0.769531
2017-12-10T14:51:40.512597: step 846, loss 0.730165, acc 0.71875, prec 0.0314111, recall 0.769531
2017-12-10T14:51:40.704115: step 847, loss 0.512671, acc 0.8125, prec 0.0313911, recall 0.769531
2017-12-10T14:51:40.901618: step 848, loss 0.79393, acc 0.75, prec 0.0313644, recall 0.769531
2017-12-10T14:51:41.096006: step 849, loss 0.730857, acc 0.75, prec 0.0313378, recall 0.769531
2017-12-10T14:51:41.289425: step 850, loss 0.919568, acc 0.75, prec 0.0313113, recall 0.769531
2017-12-10T14:51:41.479968: step 851, loss 4.70031, acc 0.796875, prec 0.0313427, recall 0.768831
2017-12-10T14:51:41.676383: step 852, loss 1.40897, acc 0.765625, prec 0.031369, recall 0.769131
2017-12-10T14:51:41.866599: step 853, loss 0.965324, acc 0.78125, prec 0.0314482, recall 0.769728
2017-12-10T14:51:42.059443: step 854, loss 0.846149, acc 0.703125, prec 0.0314167, recall 0.769728
2017-12-10T14:51:42.253270: step 855, loss 0.795379, acc 0.71875, prec 0.0314379, recall 0.770026
2017-12-10T14:51:42.444129: step 856, loss 0.848797, acc 0.671875, prec 0.0314031, recall 0.770026
2017-12-10T14:51:42.640149: step 857, loss 0.717411, acc 0.75, prec 0.0314277, recall 0.770323
2017-12-10T14:51:42.830658: step 858, loss 0.536566, acc 0.796875, prec 0.0314062, recall 0.770323
2017-12-10T14:51:43.025481: step 859, loss 0.643476, acc 0.765625, prec 0.0314832, recall 0.770914
2017-12-10T14:51:43.215707: step 860, loss 0.314037, acc 0.890625, prec 0.0315225, recall 0.771208
2017-12-10T14:51:43.406879: step 861, loss 0.528847, acc 0.828125, prec 0.0315552, recall 0.771502
2017-12-10T14:51:43.600583: step 862, loss 1.15571, acc 0.890625, prec 0.0316452, recall 0.772087
2017-12-10T14:51:43.797395: step 863, loss 0.414931, acc 0.859375, prec 0.0316303, recall 0.772087
2017-12-10T14:51:43.990269: step 864, loss 0.617768, acc 0.84375, prec 0.0316645, recall 0.772379
2017-12-10T14:51:44.186323: step 865, loss 3.18357, acc 0.828125, prec 0.0317003, recall 0.770701
2017-12-10T14:51:44.386762: step 866, loss 0.589935, acc 0.828125, prec 0.031682, recall 0.770701
2017-12-10T14:51:44.583204: step 867, loss 0.6698, acc 0.796875, prec 0.0316605, recall 0.770701
2017-12-10T14:51:44.774325: step 868, loss 2.36257, acc 0.84375, prec 0.0316962, recall 0.770013
2017-12-10T14:51:44.971722: step 869, loss 0.935742, acc 0.78125, prec 0.0318248, recall 0.770886
2017-12-10T14:51:45.164050: step 870, loss 0.389366, acc 0.8125, prec 0.0318554, recall 0.771176
2017-12-10T14:51:45.354219: step 871, loss 1.09593, acc 0.671875, prec 0.0318206, recall 0.771176
2017-12-10T14:51:45.547109: step 872, loss 1.01881, acc 0.75, prec 0.0318949, recall 0.771753
2017-12-10T14:51:45.742994: step 873, loss 0.722403, acc 0.75, prec 0.0318684, recall 0.771753
2017-12-10T14:51:45.937862: step 874, loss 0.901944, acc 0.765625, prec 0.0319442, recall 0.772327
2017-12-10T14:51:46.136722: step 875, loss 0.861266, acc 0.703125, prec 0.0319127, recall 0.772327
2017-12-10T14:51:46.327884: step 876, loss 1.48572, acc 0.71875, prec 0.0320336, recall 0.773183
2017-12-10T14:51:46.523560: step 877, loss 3.49927, acc 0.71875, prec 0.0320556, recall 0.7725
2017-12-10T14:51:46.714583: step 878, loss 0.722098, acc 0.765625, prec 0.0320809, recall 0.772784
2017-12-10T14:51:46.906606: step 879, loss 0.878484, acc 0.75, prec 0.0320543, recall 0.772784
2017-12-10T14:51:47.095800: step 880, loss 1.72542, acc 0.640625, prec 0.0320662, recall 0.773067
2017-12-10T14:51:47.288587: step 881, loss 0.868954, acc 0.703125, prec 0.0320347, recall 0.773067
2017-12-10T14:51:47.477839: step 882, loss 0.714396, acc 0.78125, prec 0.0320615, recall 0.77335
2017-12-10T14:51:47.668722: step 883, loss 1.07104, acc 0.578125, prec 0.0320169, recall 0.77335
2017-12-10T14:51:47.857319: step 884, loss 1.19117, acc 0.609375, prec 0.0320255, recall 0.773632
2017-12-10T14:51:48.047381: step 885, loss 0.87896, acc 0.625, prec 0.0320856, recall 0.774194
2017-12-10T14:51:48.236358: step 886, loss 0.94815, acc 0.640625, prec 0.0320477, recall 0.774194
2017-12-10T14:51:48.425770: step 887, loss 0.938986, acc 0.75, prec 0.0321207, recall 0.774752
2017-12-10T14:51:48.624601: step 888, loss 0.594135, acc 0.78125, prec 0.0320976, recall 0.774752
2017-12-10T14:51:48.822266: step 889, loss 0.456514, acc 0.8125, prec 0.0320779, recall 0.774752
2017-12-10T14:51:49.011729: step 890, loss 0.7225, acc 0.765625, prec 0.0321524, recall 0.775309
2017-12-10T14:51:49.200171: step 891, loss 0.686267, acc 0.75, prec 0.0321261, recall 0.775309
2017-12-10T14:51:49.402377: step 892, loss 0.602969, acc 0.765625, prec 0.0321014, recall 0.775309
2017-12-10T14:51:49.595364: step 893, loss 0.109164, acc 0.953125, prec 0.0320965, recall 0.775309
2017-12-10T14:51:49.785562: step 894, loss 0.1817, acc 0.9375, prec 0.0320899, recall 0.775309
2017-12-10T14:51:49.980771: step 895, loss 0.261609, acc 0.859375, prec 0.0320752, recall 0.775309
2017-12-10T14:51:50.171294: step 896, loss 0.21321, acc 0.9375, prec 0.0321181, recall 0.775586
2017-12-10T14:51:50.364815: step 897, loss 0.493331, acc 0.921875, prec 0.0321593, recall 0.775862
2017-12-10T14:51:50.560060: step 898, loss 1.11493, acc 0.796875, prec 0.0321873, recall 0.776138
2017-12-10T14:51:50.752946: step 899, loss 0.263946, acc 0.90625, prec 0.0321775, recall 0.776138
2017-12-10T14:51:50.945591: step 900, loss 2.99278, acc 0.953125, prec 0.0321742, recall 0.775184
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_0/1512935327/checkpoints/model-900

2017-12-10T14:51:52.146770: step 901, loss 0.650078, acc 0.875, prec 0.0322104, recall 0.77546
2017-12-10T14:51:52.343658: step 902, loss 1.31114, acc 0.890625, prec 0.0322482, recall 0.775735
2017-12-10T14:51:52.543290: step 903, loss 0.311675, acc 0.90625, prec 0.0322383, recall 0.775735
2017-12-10T14:51:52.742522: step 904, loss 2.2261, acc 0.875, prec 0.0322269, recall 0.774786
2017-12-10T14:51:52.933562: step 905, loss 1.17673, acc 0.875, prec 0.032263, recall 0.775061
2017-12-10T14:51:53.129695: step 906, loss 2.75291, acc 0.90625, prec 0.032304, recall 0.77439
2017-12-10T14:51:53.325219: step 907, loss 0.7447, acc 0.71875, prec 0.0323236, recall 0.774665
2017-12-10T14:51:53.513827: step 908, loss 0.541118, acc 0.765625, prec 0.0323482, recall 0.774939
2017-12-10T14:51:53.704708: step 909, loss 0.808543, acc 0.671875, prec 0.0323137, recall 0.774939
2017-12-10T14:51:53.896807: step 910, loss 1.13859, acc 0.6875, prec 0.03233, recall 0.775213
2017-12-10T14:51:54.088799: step 911, loss 1.18976, acc 0.625, prec 0.0322907, recall 0.775213
2017-12-10T14:51:54.281639: step 912, loss 1.23264, acc 0.6875, prec 0.032307, recall 0.775485
2017-12-10T14:51:54.474169: step 913, loss 1.17134, acc 0.5, prec 0.0322548, recall 0.775485
2017-12-10T14:51:54.664080: step 914, loss 1.02548, acc 0.671875, prec 0.0322207, recall 0.775485
2017-12-10T14:51:54.858082: step 915, loss 1.37353, acc 0.71875, prec 0.0322402, recall 0.775758
2017-12-10T14:51:55.053922: step 916, loss 1.2576, acc 0.5625, prec 0.0322921, recall 0.7763
2017-12-10T14:51:55.247149: step 917, loss 0.950096, acc 0.609375, prec 0.0322516, recall 0.7763
2017-12-10T14:51:55.436929: step 918, loss 1.19357, acc 0.734375, prec 0.0323212, recall 0.77684
2017-12-10T14:51:55.623133: step 919, loss 0.783842, acc 0.734375, prec 0.0323907, recall 0.777377
2017-12-10T14:51:55.817931: step 920, loss 0.570219, acc 0.75, prec 0.0324132, recall 0.777644
2017-12-10T14:51:56.009661: step 921, loss 0.937137, acc 0.8125, prec 0.0324906, recall 0.778177
2017-12-10T14:51:56.205544: step 922, loss 2.06108, acc 0.8125, prec 0.0326163, recall 0.778973
2017-12-10T14:51:56.397680: step 923, loss 0.488111, acc 0.84375, prec 0.0326484, recall 0.779236
2017-12-10T14:51:56.592855: step 924, loss 0.783835, acc 0.71875, prec 0.032619, recall 0.779236
2017-12-10T14:51:56.783778: step 925, loss 0.743925, acc 0.765625, prec 0.0326912, recall 0.779762
2017-12-10T14:51:56.972860: step 926, loss 3.25682, acc 0.875, prec 0.032728, recall 0.779097
2017-12-10T14:51:57.167659: step 927, loss 0.244463, acc 0.9375, prec 0.0327697, recall 0.779359
2017-12-10T14:51:57.357704: step 928, loss 0.635346, acc 0.8125, prec 0.0327983, recall 0.779621
2017-12-10T14:51:57.547102: step 929, loss 4.88035, acc 0.765625, prec 0.0328236, recall 0.77896
2017-12-10T14:51:57.740196: step 930, loss 1.19484, acc 0.78125, prec 0.0328489, recall 0.779221
2017-12-10T14:51:57.931904: step 931, loss 0.799507, acc 0.703125, prec 0.0328179, recall 0.779221
2017-12-10T14:51:58.124074: step 932, loss 0.581211, acc 0.75, prec 0.0327918, recall 0.779221
2017-12-10T14:51:58.320843: step 933, loss 0.917004, acc 0.671875, prec 0.0328056, recall 0.779481
2017-12-10T14:51:58.517814: step 934, loss 0.841334, acc 0.65625, prec 0.0328178, recall 0.779741
2017-12-10T14:51:58.707747: step 935, loss 0.7668, acc 0.71875, prec 0.0327885, recall 0.779741
2017-12-10T14:51:58.897411: step 936, loss 0.817353, acc 0.75, prec 0.0327625, recall 0.779741
2017-12-10T14:51:59.091252: step 937, loss 0.783151, acc 0.765625, prec 0.0327382, recall 0.779741
2017-12-10T14:51:59.288399: step 938, loss 0.996329, acc 0.78125, prec 0.0328112, recall 0.780259
2017-12-10T14:51:59.480616: step 939, loss 0.809162, acc 0.734375, prec 0.0327836, recall 0.780259
2017-12-10T14:51:59.671171: step 940, loss 0.35444, acc 0.84375, prec 0.0327675, recall 0.780259
2017-12-10T14:51:59.863413: step 941, loss 0.764703, acc 0.765625, prec 0.0328386, recall 0.780774
2017-12-10T14:52:00.051989: step 942, loss 0.565545, acc 0.84375, prec 0.0328224, recall 0.780774
2017-12-10T14:52:00.243569: step 943, loss 0.263319, acc 0.84375, prec 0.0328063, recall 0.780774
2017-12-10T14:52:00.437907: step 944, loss 0.383178, acc 0.84375, prec 0.0328377, recall 0.78103
2017-12-10T14:52:00.635022: step 945, loss 0.706027, acc 0.859375, prec 0.0329184, recall 0.781542
2017-12-10T14:52:00.832037: step 946, loss 0.203219, acc 0.9375, prec 0.0329119, recall 0.781542
2017-12-10T14:52:01.020481: step 947, loss 5.09486, acc 0.9375, prec 0.032907, recall 0.78063
2017-12-10T14:52:01.216263: step 948, loss 0.424054, acc 0.890625, prec 0.0329433, recall 0.780886
2017-12-10T14:52:01.410390: step 949, loss 0.936819, acc 0.9375, prec 0.0330319, recall 0.781395
2017-12-10T14:52:01.604010: step 950, loss 4.79646, acc 0.890625, prec 0.0330221, recall 0.780488
2017-12-10T14:52:01.798844: step 951, loss 0.639139, acc 0.875, prec 0.0330566, recall 0.780742
2017-12-10T14:52:01.993257: step 952, loss 0.424126, acc 0.84375, prec 0.0330404, recall 0.780742
2017-12-10T14:52:02.181795: step 953, loss 0.468691, acc 0.828125, prec 0.0330226, recall 0.780742
2017-12-10T14:52:02.375465: step 954, loss 0.674667, acc 0.75, prec 0.0329967, recall 0.780742
2017-12-10T14:52:02.570409: step 955, loss 0.839623, acc 0.671875, prec 0.0331048, recall 0.781503
2017-12-10T14:52:02.760445: step 956, loss 3.70899, acc 0.75, prec 0.0331751, recall 0.781106
2017-12-10T14:52:02.955216: step 957, loss 0.641737, acc 0.765625, prec 0.0331981, recall 0.781358
2017-12-10T14:52:03.147134: step 958, loss 1.06546, acc 0.765625, prec 0.033221, recall 0.781609
2017-12-10T14:52:03.341484: step 959, loss 2.35734, acc 0.65625, prec 0.0331869, recall 0.780712
2017-12-10T14:52:03.536141: step 960, loss 0.711698, acc 0.75, prec 0.033161, recall 0.780712
2017-12-10T14:52:03.725776: step 961, loss 1.39594, acc 0.5, prec 0.0331564, recall 0.780963
2017-12-10T14:52:03.916143: step 962, loss 1.29036, acc 0.625, prec 0.0331177, recall 0.780963
2017-12-10T14:52:04.107189: step 963, loss 2.25206, acc 0.671875, prec 0.0332249, recall 0.781714
2017-12-10T14:52:04.297479: step 964, loss 1.20865, acc 0.671875, prec 0.0332379, recall 0.781963
2017-12-10T14:52:04.485364: step 965, loss 0.852863, acc 0.6875, prec 0.0332057, recall 0.781963
2017-12-10T14:52:04.675577: step 966, loss 2.76317, acc 0.609375, prec 0.0332139, recall 0.781321
2017-12-10T14:52:04.869730: step 967, loss 1.41291, acc 0.546875, prec 0.0332141, recall 0.78157
2017-12-10T14:52:05.061679: step 968, loss 1.28136, acc 0.515625, prec 0.0331644, recall 0.78157
2017-12-10T14:52:05.251494: step 969, loss 1.14506, acc 0.625, prec 0.0331726, recall 0.781818
2017-12-10T14:52:05.443408: step 970, loss 1.01169, acc 0.6875, prec 0.0331872, recall 0.782066
2017-12-10T14:52:05.639450: step 971, loss 1.37728, acc 0.65625, prec 0.0331986, recall 0.782313
2017-12-10T14:52:05.835084: step 972, loss 0.880336, acc 0.6875, prec 0.0331667, recall 0.782313
2017-12-10T14:52:06.023684: step 973, loss 1.06001, acc 0.640625, prec 0.0332229, recall 0.782805
2017-12-10T14:52:06.211825: step 974, loss 0.627284, acc 0.765625, prec 0.0332454, recall 0.783051
2017-12-10T14:52:06.401677: step 975, loss 0.71661, acc 0.78125, prec 0.0332694, recall 0.783296
2017-12-10T14:52:06.593223: step 976, loss 0.45613, acc 0.890625, prec 0.0333046, recall 0.78354
2017-12-10T14:52:06.786411: step 977, loss 1.01915, acc 0.78125, prec 0.0333285, recall 0.783784
2017-12-10T14:52:06.976745: step 978, loss 0.425309, acc 0.859375, prec 0.0333142, recall 0.783784
2017-12-10T14:52:07.168022: step 979, loss 4.78962, acc 0.78125, prec 0.0333397, recall 0.783146
2017-12-10T14:52:07.363828: step 980, loss 0.423444, acc 0.859375, prec 0.0333716, recall 0.783389
2017-12-10T14:52:07.561520: step 981, loss 1.37495, acc 0.796875, prec 0.033397, recall 0.783632
2017-12-10T14:52:07.755974: step 982, loss 5.1497, acc 0.8125, prec 0.0333795, recall 0.782755
2017-12-10T14:52:07.949988: step 983, loss 1.14933, acc 0.828125, prec 0.0334542, recall 0.78324
2017-12-10T14:52:08.143667: step 984, loss 1.12242, acc 0.796875, prec 0.0335257, recall 0.783724
2017-12-10T14:52:08.335416: step 985, loss 0.959945, acc 0.640625, prec 0.0334889, recall 0.783724
2017-12-10T14:52:08.529776: step 986, loss 1.1024, acc 0.765625, prec 0.033557, recall 0.784205
2017-12-10T14:52:08.723358: step 987, loss 1.66187, acc 0.703125, prec 0.0335727, recall 0.784444
2017-12-10T14:52:08.919016: step 988, loss 2.37158, acc 0.703125, prec 0.0337276, recall 0.78453
2017-12-10T14:52:09.111128: step 989, loss 1.23258, acc 0.71875, prec 0.0337447, recall 0.784768
2017-12-10T14:52:09.304953: step 990, loss 1.52946, acc 0.65625, prec 0.0337553, recall 0.785006
2017-12-10T14:52:09.496357: step 991, loss 1.09291, acc 0.578125, prec 0.0337121, recall 0.785006
2017-12-10T14:52:09.692512: step 992, loss 1.55794, acc 0.515625, prec 0.0336627, recall 0.785006
2017-12-10T14:52:09.885428: step 993, loss 1.38017, acc 0.578125, prec 0.0336198, recall 0.785006
2017-12-10T14:52:10.057874: step 994, loss 1.55868, acc 0.529412, prec 0.0336273, recall 0.785242
2017-12-10T14:52:10.254981: step 995, loss 2.06948, acc 0.46875, prec 0.0335735, recall 0.785242
2017-12-10T14:52:10.443115: step 996, loss 0.957614, acc 0.609375, prec 0.033534, recall 0.785242
2017-12-10T14:52:10.636812: step 997, loss 1.70249, acc 0.546875, prec 0.0334883, recall 0.785242
2017-12-10T14:52:10.831201: step 998, loss 1.52576, acc 0.640625, prec 0.0334522, recall 0.785242
2017-12-10T14:52:11.023148: step 999, loss 1.47204, acc 0.671875, prec 0.0335552, recall 0.78595
2017-12-10T14:52:11.213863: step 1000, loss 0.859523, acc 0.6875, prec 0.0335237, recall 0.78595
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_0/1512935327/checkpoints/model-1000

2017-12-10T14:52:12.897603: step 1001, loss 1.07333, acc 0.6875, prec 0.0334924, recall 0.78595
2017-12-10T14:52:13.092665: step 1002, loss 0.878648, acc 0.734375, prec 0.0334658, recall 0.78595
2017-12-10T14:52:13.281457: step 1003, loss 0.718467, acc 0.796875, prec 0.0334906, recall 0.786184
2017-12-10T14:52:13.473267: step 1004, loss 0.376227, acc 0.84375, prec 0.033475, recall 0.786184
2017-12-10T14:52:13.666032: step 1005, loss 0.513598, acc 0.828125, prec 0.0335931, recall 0.786885
2017-12-10T14:52:13.856039: step 1006, loss 0.859117, acc 0.84375, prec 0.0336675, recall 0.78735
2017-12-10T14:52:14.053847: step 1007, loss 0.612958, acc 0.890625, prec 0.0337016, recall 0.787582
2017-12-10T14:52:14.255168: step 1008, loss 0.174988, acc 0.921875, prec 0.0336937, recall 0.787582
2017-12-10T14:52:14.448773: step 1009, loss 0.447076, acc 0.9375, prec 0.0337325, recall 0.787813
2017-12-10T14:52:14.642359: step 1010, loss 3.86517, acc 0.921875, prec 0.0337262, recall 0.786957
2017-12-10T14:52:14.834544: step 1011, loss 0.624388, acc 0.90625, prec 0.0337618, recall 0.787188
2017-12-10T14:52:15.026768: step 1012, loss 0.317832, acc 0.859375, prec 0.0337926, recall 0.787419
2017-12-10T14:52:15.218594: step 1013, loss 0.442275, acc 0.8125, prec 0.0337737, recall 0.787419
2017-12-10T14:52:15.409247: step 1014, loss 0.180116, acc 0.9375, prec 0.0337674, recall 0.787419
2017-12-10T14:52:15.607864: step 1015, loss 0.241997, acc 0.921875, prec 0.0338045, recall 0.787649
2017-12-10T14:52:15.798435: step 1016, loss 0.397969, acc 0.859375, prec 0.0338353, recall 0.787879
2017-12-10T14:52:15.990549: step 1017, loss 0.393873, acc 0.890625, prec 0.0338692, recall 0.788108
2017-12-10T14:52:16.179685: step 1018, loss 1.30461, acc 0.921875, prec 0.033951, recall 0.788565
2017-12-10T14:52:16.371772: step 1019, loss 0.290296, acc 0.84375, prec 0.0339353, recall 0.788565
2017-12-10T14:52:16.561507: step 1020, loss 0.282313, acc 0.953125, prec 0.0339754, recall 0.788793
2017-12-10T14:52:16.756771: step 1021, loss 2.6165, acc 0.9375, prec 0.0340155, recall 0.788172
2017-12-10T14:52:16.949768: step 1022, loss 0.364714, acc 0.84375, prec 0.0339997, recall 0.788172
2017-12-10T14:52:17.144805: step 1023, loss 1.95524, acc 0.828125, prec 0.033984, recall 0.787325
2017-12-10T14:52:17.338644: step 1024, loss 0.344314, acc 0.9375, prec 0.0340224, recall 0.787554
2017-12-10T14:52:17.527831: step 1025, loss 0.411982, acc 0.828125, prec 0.0340051, recall 0.787554
2017-12-10T14:52:17.717844: step 1026, loss 0.627986, acc 0.78125, prec 0.0340278, recall 0.787781
2017-12-10T14:52:17.906844: step 1027, loss 0.39681, acc 0.8125, prec 0.0340536, recall 0.788009
2017-12-10T14:52:18.097439: step 1028, loss 5.01753, acc 0.828125, prec 0.0340378, recall 0.787166
2017-12-10T14:52:18.289920: step 1029, loss 0.67872, acc 0.796875, prec 0.0340174, recall 0.787166
2017-12-10T14:52:18.480621: step 1030, loss 0.676893, acc 0.8125, prec 0.0340431, recall 0.787393
2017-12-10T14:52:18.674463: step 1031, loss 0.690873, acc 0.8125, prec 0.0341135, recall 0.787847
2017-12-10T14:52:18.874303: step 1032, loss 0.721323, acc 0.734375, prec 0.0340867, recall 0.787847
2017-12-10T14:52:19.065680: step 1033, loss 0.724611, acc 0.75, prec 0.0341506, recall 0.788298
2017-12-10T14:52:19.256563: step 1034, loss 1.18908, acc 0.59375, prec 0.0341097, recall 0.788298
2017-12-10T14:52:19.453845: step 1035, loss 0.789872, acc 0.75, prec 0.0341291, recall 0.788523
2017-12-10T14:52:19.643646: step 1036, loss 0.81338, acc 0.734375, prec 0.0341468, recall 0.788747
2017-12-10T14:52:19.838615: step 1037, loss 0.983836, acc 0.671875, prec 0.0341139, recall 0.788747
2017-12-10T14:52:20.028634: step 1038, loss 0.570374, acc 0.796875, prec 0.0341822, recall 0.789195
2017-12-10T14:52:20.217697: step 1039, loss 0.440898, acc 0.796875, prec 0.0342061, recall 0.789418
2017-12-10T14:52:20.409808: step 1040, loss 0.616732, acc 0.78125, prec 0.0342726, recall 0.789863
2017-12-10T14:52:20.600576: step 1041, loss 0.647164, acc 0.828125, prec 0.0342554, recall 0.789863
2017-12-10T14:52:20.789399: step 1042, loss 0.378566, acc 0.84375, prec 0.0342839, recall 0.790084
2017-12-10T14:52:20.982561: step 1043, loss 0.458372, acc 0.828125, prec 0.0343108, recall 0.790306
2017-12-10T14:52:21.170335: step 1044, loss 1.17512, acc 0.859375, prec 0.0343408, recall 0.790526
2017-12-10T14:52:21.365779: step 1045, loss 0.0796989, acc 0.96875, prec 0.0343377, recall 0.790526
2017-12-10T14:52:21.558670: step 1046, loss 0.472599, acc 0.84375, prec 0.034322, recall 0.790526
2017-12-10T14:52:21.753773: step 1047, loss 0.29967, acc 0.859375, prec 0.034352, recall 0.790747
2017-12-10T14:52:21.948191: step 1048, loss 7.88582, acc 0.796875, prec 0.0344229, recall 0.789529
2017-12-10T14:52:22.143425: step 1049, loss 1.32163, acc 0.875, prec 0.0344985, recall 0.789969
2017-12-10T14:52:22.341522: step 1050, loss 1.81262, acc 0.828125, prec 0.0345708, recall 0.789583
2017-12-10T14:52:22.539280: step 1051, loss 0.428768, acc 0.859375, prec 0.0346007, recall 0.789802
2017-12-10T14:52:22.734546: step 1052, loss 0.654, acc 0.765625, prec 0.034577, recall 0.789802
2017-12-10T14:52:22.928056: step 1053, loss 0.869383, acc 0.828125, prec 0.0346037, recall 0.790021
2017-12-10T14:52:23.121100: step 1054, loss 0.521397, acc 0.84375, prec 0.0346758, recall 0.790456
2017-12-10T14:52:23.314419: step 1055, loss 0.690288, acc 0.796875, prec 0.0346992, recall 0.790674
2017-12-10T14:52:23.510467: step 1056, loss 0.781806, acc 0.703125, prec 0.0346692, recall 0.790674
2017-12-10T14:52:23.700788: step 1057, loss 0.56162, acc 0.78125, prec 0.034691, recall 0.79089
2017-12-10T14:52:23.890894: step 1058, loss 0.836455, acc 0.734375, prec 0.0346642, recall 0.79089
2017-12-10T14:52:24.082276: step 1059, loss 0.854682, acc 0.765625, prec 0.0346844, recall 0.791107
2017-12-10T14:52:24.274409: step 1060, loss 0.821088, acc 0.71875, prec 0.0347873, recall 0.791753
2017-12-10T14:52:24.461767: step 1061, loss 0.795596, acc 0.671875, prec 0.0348416, recall 0.792181
2017-12-10T14:52:24.654358: step 1062, loss 1.26391, acc 0.640625, prec 0.034849, recall 0.792395
2017-12-10T14:52:24.842881: step 1063, loss 0.743237, acc 0.71875, prec 0.0349079, recall 0.792821
2017-12-10T14:52:25.035066: step 1064, loss 0.843554, acc 0.734375, prec 0.0348811, recall 0.792821
2017-12-10T14:52:25.229607: step 1065, loss 0.497664, acc 0.734375, prec 0.0349414, recall 0.793245
2017-12-10T14:52:25.420769: step 1066, loss 0.469473, acc 0.84375, prec 0.0349691, recall 0.793456
2017-12-10T14:52:25.610923: step 1067, loss 0.547242, acc 0.890625, prec 0.0350016, recall 0.793667
2017-12-10T14:52:25.801450: step 1068, loss 0.302093, acc 0.90625, prec 0.0349921, recall 0.793667
2017-12-10T14:52:25.993653: step 1069, loss 0.462486, acc 0.9375, prec 0.0350727, recall 0.794088
2017-12-10T14:52:26.185800: step 1070, loss 0.321941, acc 0.84375, prec 0.0350569, recall 0.794088
2017-12-10T14:52:26.376955: step 1071, loss 0.226665, acc 0.90625, prec 0.0350909, recall 0.794297
2017-12-10T14:52:26.566525: step 1072, loss 0.431049, acc 0.84375, prec 0.0350751, recall 0.794297
2017-12-10T14:52:26.757072: step 1073, loss 0.264279, acc 0.890625, prec 0.0351074, recall 0.794507
2017-12-10T14:52:26.953180: step 1074, loss 0.349872, acc 0.875, prec 0.0351382, recall 0.794715
2017-12-10T14:52:27.146788: step 1075, loss 0.258746, acc 0.875, prec 0.0351689, recall 0.794924
2017-12-10T14:52:27.337800: step 1076, loss 0.189275, acc 0.953125, prec 0.0351641, recall 0.794924
2017-12-10T14:52:27.532834: step 1077, loss 1.68643, acc 0.90625, prec 0.0352862, recall 0.794742
2017-12-10T14:52:27.729124: step 1078, loss 0.610446, acc 0.9375, prec 0.0354097, recall 0.795363
2017-12-10T14:52:27.928890: step 1079, loss 0.257351, acc 0.90625, prec 0.0354002, recall 0.795363
2017-12-10T14:52:28.118430: step 1080, loss 5.36852, acc 0.9375, prec 0.0353955, recall 0.794562
2017-12-10T14:52:28.309646: step 1081, loss 2.7286, acc 0.84375, prec 0.0354244, recall 0.79397
2017-12-10T14:52:28.501645: step 1082, loss 0.452903, acc 0.921875, prec 0.035503, recall 0.794383
2017-12-10T14:52:28.695813: step 1083, loss 0.162363, acc 0.96875, prec 0.035543, recall 0.794589
2017-12-10T14:52:28.886027: step 1084, loss 0.585046, acc 0.796875, prec 0.0355223, recall 0.794589
2017-12-10T14:52:29.078890: step 1085, loss 0.321065, acc 0.890625, prec 0.0355112, recall 0.794589
2017-12-10T14:52:29.275976: step 1086, loss 0.281998, acc 0.890625, prec 0.0355, recall 0.794589
2017-12-10T14:52:29.469452: step 1087, loss 0.558807, acc 0.796875, prec 0.0355657, recall 0.795
2017-12-10T14:52:29.658397: step 1088, loss 0.450526, acc 0.859375, prec 0.0356376, recall 0.795409
2017-12-10T14:52:29.847432: step 1089, loss 4.42566, acc 0.78125, prec 0.0356169, recall 0.794616
2017-12-10T14:52:30.041602: step 1090, loss 0.640729, acc 0.75, prec 0.0356345, recall 0.794821
2017-12-10T14:52:30.231783: step 1091, loss 0.889146, acc 0.671875, prec 0.0356012, recall 0.794821
2017-12-10T14:52:30.420583: step 1092, loss 0.693123, acc 0.796875, prec 0.0356235, recall 0.795025
2017-12-10T14:52:30.615158: step 1093, loss 0.791492, acc 0.734375, prec 0.0355965, recall 0.795025
2017-12-10T14:52:30.808380: step 1094, loss 0.826063, acc 0.71875, prec 0.035568, recall 0.795025
2017-12-10T14:52:30.997689: step 1095, loss 0.610278, acc 0.75, prec 0.0355427, recall 0.795025
2017-12-10T14:52:31.188477: step 1096, loss 0.643032, acc 0.734375, prec 0.0355587, recall 0.795229
2017-12-10T14:52:31.378104: step 1097, loss 0.836392, acc 0.71875, prec 0.0355303, recall 0.795229
2017-12-10T14:52:31.571792: step 1098, loss 0.69128, acc 0.796875, prec 0.0355098, recall 0.795229
2017-12-10T14:52:31.761259: step 1099, loss 0.704902, acc 0.71875, prec 0.0354814, recall 0.795229
2017-12-10T14:52:31.952644: step 1100, loss 0.664211, acc 0.78125, prec 0.0355022, recall 0.795432
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_0/1512935327/checkpoints/model-1100

2017-12-10T14:52:33.157299: step 1101, loss 0.64641, acc 0.734375, prec 0.0354754, recall 0.795432
2017-12-10T14:52:33.349805: step 1102, loss 0.552997, acc 0.78125, prec 0.0354961, recall 0.795635
2017-12-10T14:52:33.542072: step 1103, loss 0.252075, acc 0.84375, prec 0.0354804, recall 0.795635
2017-12-10T14:52:33.734345: step 1104, loss 0.412044, acc 0.921875, prec 0.0354726, recall 0.795635
2017-12-10T14:52:33.926742: step 1105, loss 0.471513, acc 0.90625, prec 0.0355485, recall 0.79604
2017-12-10T14:52:34.123015: step 1106, loss 3.41763, acc 0.828125, prec 0.035618, recall 0.795656
2017-12-10T14:52:34.317099: step 1107, loss 0.376223, acc 0.875, prec 0.0356054, recall 0.795656
2017-12-10T14:52:34.510615: step 1108, loss 0.442989, acc 0.875, prec 0.0355928, recall 0.795656
2017-12-10T14:52:34.712495: step 1109, loss 2.40659, acc 0.890625, prec 0.035626, recall 0.795074
2017-12-10T14:52:34.906147: step 1110, loss 0.750162, acc 0.8125, prec 0.0356922, recall 0.795477
2017-12-10T14:52:35.096280: step 1111, loss 0.15805, acc 0.921875, prec 0.0357694, recall 0.795878
2017-12-10T14:52:35.287070: step 1112, loss 0.802991, acc 0.875, prec 0.0357993, recall 0.796078
2017-12-10T14:52:35.483219: step 1113, loss 0.410307, acc 0.890625, prec 0.0357883, recall 0.796078
2017-12-10T14:52:35.676904: step 1114, loss 1.40921, acc 0.75, prec 0.0358055, recall 0.796278
2017-12-10T14:52:35.869591: step 1115, loss 2.18999, acc 0.875, prec 0.0358794, recall 0.795898
2017-12-10T14:52:36.064408: step 1116, loss 0.650095, acc 0.75, prec 0.0358541, recall 0.795898
2017-12-10T14:52:36.256657: step 1117, loss 0.518216, acc 0.78125, prec 0.0358321, recall 0.795898
2017-12-10T14:52:36.448927: step 1118, loss 0.798946, acc 0.703125, prec 0.0358869, recall 0.796296
2017-12-10T14:52:36.637687: step 1119, loss 1.01534, acc 0.75, prec 0.0358616, recall 0.796296
2017-12-10T14:52:36.829538: step 1120, loss 1.18368, acc 0.734375, prec 0.0358349, recall 0.796296
2017-12-10T14:52:37.020311: step 1121, loss 1.31183, acc 0.671875, prec 0.0358019, recall 0.796296
2017-12-10T14:52:37.214863: step 1122, loss 0.92653, acc 0.734375, prec 0.0358597, recall 0.796693
2017-12-10T14:52:37.406195: step 1123, loss 1.03365, acc 0.71875, prec 0.0358315, recall 0.796693
2017-12-10T14:52:37.594372: step 1124, loss 1.20634, acc 0.796875, prec 0.0359376, recall 0.797284
2017-12-10T14:52:37.789046: step 1125, loss 0.506361, acc 0.8125, prec 0.0359608, recall 0.797481
2017-12-10T14:52:37.978991: step 1126, loss 0.67043, acc 0.796875, prec 0.0359825, recall 0.797677
2017-12-10T14:52:38.172517: step 1127, loss 0.624394, acc 0.78125, prec 0.0360026, recall 0.797872
2017-12-10T14:52:38.365174: step 1128, loss 0.330915, acc 0.90625, prec 0.0360353, recall 0.798068
2017-12-10T14:52:38.559927: step 1129, loss 0.781556, acc 0.6875, prec 0.0360038, recall 0.798068
2017-12-10T14:52:38.750615: step 1130, loss 0.524463, acc 0.84375, prec 0.0360301, recall 0.798263
2017-12-10T14:52:38.948481: step 1131, loss 0.998741, acc 0.84375, prec 0.0360984, recall 0.798651
2017-12-10T14:52:39.142410: step 1132, loss 1.0437, acc 0.84375, prec 0.0361666, recall 0.799038
2017-12-10T14:52:39.335320: step 1133, loss 0.319501, acc 0.890625, prec 0.0361556, recall 0.799038
2017-12-10T14:52:39.525897: step 1134, loss 0.338271, acc 0.875, prec 0.036143, recall 0.799038
2017-12-10T14:52:39.717455: step 1135, loss 2.34735, acc 0.8125, prec 0.0361257, recall 0.798271
2017-12-10T14:52:39.913993: step 1136, loss 0.4583, acc 0.828125, prec 0.0361085, recall 0.798271
2017-12-10T14:52:40.107117: step 1137, loss 1.94813, acc 0.90625, prec 0.0361006, recall 0.797505
2017-12-10T14:52:40.301786: step 1138, loss 1.73933, acc 0.828125, prec 0.0362089, recall 0.798086
2017-12-10T14:52:40.494975: step 1139, loss 1.22664, acc 0.84375, prec 0.0362769, recall 0.798472
2017-12-10T14:52:40.689176: step 1140, loss 0.73246, acc 0.828125, prec 0.0363849, recall 0.799048
2017-12-10T14:52:40.883503: step 1141, loss 0.507982, acc 0.890625, prec 0.0364157, recall 0.799239
2017-12-10T14:52:41.077443: step 1142, loss 0.58398, acc 0.765625, prec 0.036392, recall 0.799239
2017-12-10T14:52:41.268795: step 1143, loss 0.818745, acc 0.734375, prec 0.0363652, recall 0.799239
2017-12-10T14:52:41.459936: step 1144, loss 0.686769, acc 0.796875, prec 0.0364281, recall 0.79962
2017-12-10T14:52:41.648003: step 1145, loss 0.52104, acc 0.796875, prec 0.0364077, recall 0.79962
2017-12-10T14:52:41.842686: step 1146, loss 1.15521, acc 0.734375, prec 0.0363809, recall 0.79962
2017-12-10T14:52:42.035946: step 1147, loss 0.874049, acc 0.78125, prec 0.0364005, recall 0.79981
2017-12-10T14:52:42.228502: step 1148, loss 0.850765, acc 0.734375, prec 0.0364986, recall 0.800378
2017-12-10T14:52:42.419474: step 1149, loss 1.83121, acc 0.6875, prec 0.0365501, recall 0.800755
2017-12-10T14:52:42.612560: step 1150, loss 0.949567, acc 0.703125, prec 0.0366032, recall 0.801131
2017-12-10T14:52:42.802799: step 1151, loss 0.724785, acc 0.671875, prec 0.0365702, recall 0.801131
2017-12-10T14:52:42.990523: step 1152, loss 0.668192, acc 0.703125, prec 0.0365403, recall 0.801131
2017-12-10T14:52:43.181362: step 1153, loss 0.757926, acc 0.71875, prec 0.0365948, recall 0.801505
2017-12-10T14:52:43.375678: step 1154, loss 0.801205, acc 0.71875, prec 0.0365665, recall 0.801505
2017-12-10T14:52:43.565183: step 1155, loss 0.542746, acc 0.765625, prec 0.0365843, recall 0.801692
2017-12-10T14:52:43.762336: step 1156, loss 0.684114, acc 0.796875, prec 0.0365639, recall 0.801692
2017-12-10T14:52:43.953115: step 1157, loss 1.97624, acc 0.890625, prec 0.0365958, recall 0.801126
2017-12-10T14:52:44.152408: step 1158, loss 0.60676, acc 0.828125, prec 0.0366198, recall 0.801312
2017-12-10T14:52:44.342371: step 1159, loss 0.51742, acc 0.84375, prec 0.0366042, recall 0.801312
2017-12-10T14:52:44.541120: step 1160, loss 0.4663, acc 0.875, prec 0.0366329, recall 0.801498
2017-12-10T14:52:44.741668: step 1161, loss 0.325401, acc 0.890625, prec 0.0366219, recall 0.801498
2017-12-10T14:52:44.935135: step 1162, loss 0.207777, acc 0.90625, prec 0.0366537, recall 0.801684
2017-12-10T14:52:45.124361: step 1163, loss 0.301998, acc 0.921875, prec 0.0366459, recall 0.801684
2017-12-10T14:52:45.317350: step 1164, loss 0.533928, acc 0.921875, prec 0.0366792, recall 0.801869
2017-12-10T14:52:45.508308: step 1165, loss 0.503441, acc 0.828125, prec 0.036662, recall 0.801869
2017-12-10T14:52:45.697279: step 1166, loss 0.945562, acc 0.953125, prec 0.0366984, recall 0.802054
2017-12-10T14:52:45.887895: step 1167, loss 0.986605, acc 0.875, prec 0.036727, recall 0.802239
2017-12-10T14:52:46.080917: step 1168, loss 2.67922, acc 0.9375, prec 0.0367223, recall 0.801491
2017-12-10T14:52:46.279382: step 1169, loss 0.204463, acc 0.890625, prec 0.0367525, recall 0.801676
2017-12-10T14:52:46.474586: step 1170, loss 0.39432, acc 0.875, prec 0.0367399, recall 0.801676
2017-12-10T14:52:46.664735: step 1171, loss 0.318174, acc 0.875, prec 0.0367685, recall 0.80186
2017-12-10T14:52:46.857339: step 1172, loss 1.83093, acc 0.921875, prec 0.0367622, recall 0.801115
2017-12-10T14:52:47.055148: step 1173, loss 0.495417, acc 0.8125, prec 0.0367434, recall 0.801115
2017-12-10T14:52:47.249039: step 1174, loss 0.206514, acc 0.90625, prec 0.036734, recall 0.801115
2017-12-10T14:52:47.442949: step 1175, loss 0.331664, acc 0.890625, prec 0.0367641, recall 0.8013
2017-12-10T14:52:47.635497: step 1176, loss 0.522681, acc 0.796875, prec 0.0367437, recall 0.8013
2017-12-10T14:52:47.827330: step 1177, loss 0.577147, acc 0.8125, prec 0.036766, recall 0.801484
2017-12-10T14:52:48.017442: step 1178, loss 0.577725, acc 0.765625, prec 0.0367425, recall 0.801484
2017-12-10T14:52:48.217138: step 1179, loss 0.975991, acc 0.84375, prec 0.0368088, recall 0.801852
2017-12-10T14:52:48.407297: step 1180, loss 0.444519, acc 0.78125, prec 0.0367869, recall 0.801852
2017-12-10T14:52:48.598704: step 1181, loss 0.635837, acc 0.765625, prec 0.0367635, recall 0.801852
2017-12-10T14:52:48.792738: step 1182, loss 0.654717, acc 0.765625, prec 0.0367401, recall 0.801852
2017-12-10T14:52:48.986406: step 1183, loss 4.29362, acc 0.828125, prec 0.0367245, recall 0.80111
2017-12-10T14:52:49.179668: step 1184, loss 0.488986, acc 0.796875, prec 0.0367042, recall 0.80111
2017-12-10T14:52:49.370373: step 1185, loss 3.03883, acc 0.796875, prec 0.0367264, recall 0.800554
2017-12-10T14:52:49.560910: step 1186, loss 0.455448, acc 0.828125, prec 0.0367093, recall 0.800554
2017-12-10T14:52:49.754474: step 1187, loss 0.746671, acc 0.78125, prec 0.0367691, recall 0.800922
2017-12-10T14:52:49.945014: step 1188, loss 0.973037, acc 0.671875, prec 0.0367364, recall 0.800922
2017-12-10T14:52:50.139674: step 1189, loss 0.690737, acc 0.734375, prec 0.03671, recall 0.800922
2017-12-10T14:52:50.331885: step 1190, loss 0.841606, acc 0.75, prec 0.0366852, recall 0.800922
2017-12-10T14:52:50.523916: step 1191, loss 0.857172, acc 0.75, prec 0.0367011, recall 0.801105
2017-12-10T14:52:50.712667: step 1192, loss 0.810724, acc 0.71875, prec 0.0367139, recall 0.801288
2017-12-10T14:52:50.902972: step 1193, loss 0.569842, acc 0.734375, prec 0.0366876, recall 0.801288
2017-12-10T14:52:51.097557: step 1194, loss 0.74738, acc 0.75, prec 0.0367034, recall 0.801471
2017-12-10T14:52:51.295119: step 1195, loss 0.534328, acc 0.8125, prec 0.0367254, recall 0.801653
2017-12-10T14:52:51.486681: step 1196, loss 0.551089, acc 0.765625, prec 0.0367023, recall 0.801653
2017-12-10T14:52:51.675663: step 1197, loss 0.736994, acc 0.765625, prec 0.0367196, recall 0.801835
2017-12-10T14:52:51.865523: step 1198, loss 0.473003, acc 0.8125, prec 0.0367416, recall 0.802016
2017-12-10T14:52:52.054061: step 1199, loss 0.252363, acc 0.921875, prec 0.0367338, recall 0.802016
2017-12-10T14:52:52.249393: step 1200, loss 0.556432, acc 0.859375, prec 0.0368008, recall 0.802379
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_0/1512935327/checkpoints/model-1200

2017-12-10T14:52:53.374099: step 1201, loss 0.370226, acc 0.859375, prec 0.0368273, recall 0.802559
2017-12-10T14:52:53.569251: step 1202, loss 0.431845, acc 0.90625, prec 0.0368988, recall 0.80292
2017-12-10T14:52:53.764342: step 1203, loss 0.687658, acc 0.828125, prec 0.0369625, recall 0.803279
2017-12-10T14:52:53.959918: step 1204, loss 0.191454, acc 0.90625, prec 0.0369532, recall 0.803279
2017-12-10T14:52:54.150720: step 1205, loss 0.295409, acc 0.90625, prec 0.036944, recall 0.803279
2017-12-10T14:52:54.340247: step 1206, loss 1.7215, acc 0.9375, prec 0.03702, recall 0.802906
2017-12-10T14:52:54.531972: step 1207, loss 1.31756, acc 0.921875, prec 0.0370138, recall 0.802178
2017-12-10T14:52:54.726720: step 1208, loss 0.302902, acc 0.875, prec 0.0370014, recall 0.802178
2017-12-10T14:52:54.922342: step 1209, loss 1.05273, acc 0.875, prec 0.0370293, recall 0.802357
2017-12-10T14:52:55.123002: step 1210, loss 0.637492, acc 0.875, prec 0.0370975, recall 0.802715
2017-12-10T14:52:55.312518: step 1211, loss 0.250283, acc 0.9375, prec 0.0370912, recall 0.802715
2017-12-10T14:52:55.504761: step 1212, loss 0.319123, acc 0.875, prec 0.0370788, recall 0.802715
2017-12-10T14:52:55.696381: step 1213, loss 0.904357, acc 0.90625, prec 0.0371903, recall 0.803249
2017-12-10T14:52:55.887648: step 1214, loss 0.831331, acc 0.8125, prec 0.0372118, recall 0.803427
2017-12-10T14:52:56.085071: step 1215, loss 0.667433, acc 0.734375, prec 0.0371854, recall 0.803427
2017-12-10T14:52:56.275107: step 1216, loss 0.6464, acc 0.859375, prec 0.0372518, recall 0.80378
2017-12-10T14:52:56.465875: step 1217, loss 0.522305, acc 0.828125, prec 0.0372748, recall 0.803957
2017-12-10T14:52:56.654553: step 1218, loss 0.357482, acc 0.875, prec 0.0372624, recall 0.803957
2017-12-10T14:52:56.849130: step 1219, loss 0.34204, acc 0.84375, prec 0.0372469, recall 0.803957
2017-12-10T14:52:57.039354: step 1220, loss 0.218938, acc 0.96875, prec 0.0372839, recall 0.804133
2017-12-10T14:52:57.236552: step 1221, loss 0.552308, acc 0.8125, prec 0.0373054, recall 0.804309
2017-12-10T14:52:57.431179: step 1222, loss 0.291235, acc 0.90625, prec 0.0373361, recall 0.804484
2017-12-10T14:52:57.625351: step 1223, loss 0.397793, acc 0.875, prec 0.0373237, recall 0.804484
2017-12-10T14:52:57.816252: step 1224, loss 0.206045, acc 0.90625, prec 0.0373144, recall 0.804484
2017-12-10T14:52:58.007446: step 1225, loss 0.207674, acc 0.9375, prec 0.0373082, recall 0.804484
2017-12-10T14:52:58.199311: step 1226, loss 0.21879, acc 0.921875, prec 0.0373004, recall 0.804484
2017-12-10T14:52:58.385431: step 1227, loss 1.5627, acc 0.890625, prec 0.0373311, recall 0.803939
2017-12-10T14:52:58.579345: step 1228, loss 0.317315, acc 0.875, prec 0.0373187, recall 0.803939
2017-12-10T14:52:58.769722: step 1229, loss 0.718055, acc 0.9375, prec 0.0373525, recall 0.804114
2017-12-10T14:52:58.965858: step 1230, loss 0.394902, acc 0.859375, prec 0.0373385, recall 0.804114
2017-12-10T14:52:59.164883: step 1231, loss 0.296361, acc 0.875, prec 0.0373661, recall 0.80429
2017-12-10T14:52:59.353994: step 1232, loss 0.315495, acc 0.890625, prec 0.0373952, recall 0.804464
2017-12-10T14:52:59.545134: step 1233, loss 0.297661, acc 0.890625, prec 0.0373843, recall 0.804464
2017-12-10T14:52:59.735792: step 1234, loss 0.637882, acc 0.96875, prec 0.037501, recall 0.804987
2017-12-10T14:52:59.925987: step 1235, loss 0.351552, acc 0.875, prec 0.0374886, recall 0.804987
2017-12-10T14:53:00.115967: step 1236, loss 0.396692, acc 0.859375, prec 0.0374746, recall 0.804987
2017-12-10T14:53:00.306247: step 1237, loss 0.48076, acc 0.84375, prec 0.0374591, recall 0.804987
2017-12-10T14:53:00.498793: step 1238, loss 0.398415, acc 0.828125, prec 0.037442, recall 0.804987
2017-12-10T14:53:00.686678: step 1239, loss 0.903682, acc 0.828125, prec 0.0374648, recall 0.80516
2017-12-10T14:53:00.876644: step 1240, loss 0.257931, acc 0.90625, prec 0.0374555, recall 0.80516
2017-12-10T14:53:01.073184: step 1241, loss 1.43508, acc 0.9375, prec 0.0374509, recall 0.804444
2017-12-10T14:53:01.271350: step 1242, loss 2.43663, acc 0.90625, prec 0.0374431, recall 0.80373
2017-12-10T14:53:01.467816: step 1243, loss 3.09023, acc 0.875, prec 0.0375915, recall 0.803714
2017-12-10T14:53:01.660858: step 1244, loss 0.221095, acc 0.859375, prec 0.0375775, recall 0.803714
2017-12-10T14:53:01.850284: step 1245, loss 2.55028, acc 0.8125, prec 0.0376002, recall 0.803177
2017-12-10T14:53:02.042285: step 1246, loss 0.558794, acc 0.765625, prec 0.0375769, recall 0.803177
2017-12-10T14:53:02.232336: step 1247, loss 0.793413, acc 0.75, prec 0.0376315, recall 0.803524
2017-12-10T14:53:02.420822: step 1248, loss 0.688792, acc 0.796875, prec 0.0376114, recall 0.803524
2017-12-10T14:53:02.609986: step 1249, loss 1.17653, acc 0.6875, prec 0.0376597, recall 0.80387
2017-12-10T14:53:02.805951: step 1250, loss 1.00267, acc 0.640625, prec 0.0377032, recall 0.804214
2017-12-10T14:53:02.992936: step 1251, loss 0.935067, acc 0.671875, prec 0.0376707, recall 0.804214
2017-12-10T14:53:03.183822: step 1252, loss 1.12964, acc 0.578125, prec 0.0376684, recall 0.804386
2017-12-10T14:53:03.379285: step 1253, loss 1.50206, acc 0.5, prec 0.0376584, recall 0.804557
2017-12-10T14:53:03.572690: step 1254, loss 1.48686, acc 0.5, prec 0.0376485, recall 0.804729
2017-12-10T14:53:03.767809: step 1255, loss 1.27433, acc 0.640625, prec 0.0376918, recall 0.80507
2017-12-10T14:53:03.960909: step 1256, loss 1.14872, acc 0.609375, prec 0.0376533, recall 0.80507
2017-12-10T14:53:04.152535: step 1257, loss 1.02779, acc 0.671875, prec 0.0376996, recall 0.80541
2017-12-10T14:53:04.343729: step 1258, loss 1.151, acc 0.71875, prec 0.0377112, recall 0.80558
2017-12-10T14:53:04.532812: step 1259, loss 0.743609, acc 0.78125, prec 0.0377682, recall 0.805918
2017-12-10T14:53:04.725310: step 1260, loss 1.53075, acc 0.75, prec 0.037822, recall 0.806255
2017-12-10T14:53:04.914615: step 1261, loss 0.614812, acc 0.828125, prec 0.0378834, recall 0.806592
2017-12-10T14:53:05.105580: step 1262, loss 0.388162, acc 0.828125, prec 0.0379056, recall 0.806759
2017-12-10T14:53:05.296561: step 1263, loss 0.884561, acc 0.796875, prec 0.0379247, recall 0.806926
2017-12-10T14:53:05.489282: step 1264, loss 0.47238, acc 0.828125, prec 0.0379469, recall 0.807093
2017-12-10T14:53:05.682596: step 1265, loss 0.964893, acc 0.78125, prec 0.0379644, recall 0.80726
2017-12-10T14:53:05.876188: step 1266, loss 0.405569, acc 0.90625, prec 0.0379942, recall 0.807427
2017-12-10T14:53:06.072689: step 1267, loss 0.301276, acc 0.875, prec 0.038021, recall 0.807593
2017-12-10T14:53:06.264500: step 1268, loss 0.3386, acc 0.84375, prec 0.0380055, recall 0.807593
2017-12-10T14:53:06.456856: step 1269, loss 0.352851, acc 0.859375, prec 0.0379916, recall 0.807593
2017-12-10T14:53:06.653424: step 1270, loss 0.0605294, acc 0.984375, prec 0.0379901, recall 0.807593
2017-12-10T14:53:06.841768: step 1271, loss 5.12715, acc 0.953125, prec 0.0380261, recall 0.807063
2017-12-10T14:53:07.035194: step 1272, loss 0.431881, acc 0.921875, prec 0.0380574, recall 0.807229
2017-12-10T14:53:07.229900: step 1273, loss 0.228306, acc 0.890625, prec 0.0380856, recall 0.807395
2017-12-10T14:53:07.420705: step 1274, loss 0.990852, acc 0.859375, prec 0.0381107, recall 0.80756
2017-12-10T14:53:07.611984: step 1275, loss 2.4435, acc 0.859375, prec 0.0381373, recall 0.807033
2017-12-10T14:53:07.802638: step 1276, loss 0.366563, acc 0.890625, prec 0.0381265, recall 0.807033
2017-12-10T14:53:07.996152: step 1277, loss 0.521506, acc 0.8125, prec 0.0381469, recall 0.807198
2017-12-10T14:53:08.183572: step 1278, loss 0.933756, acc 0.78125, prec 0.0382421, recall 0.807692
2017-12-10T14:53:08.375588: step 1279, loss 0.372141, acc 0.828125, prec 0.0383807, recall 0.808348
2017-12-10T14:53:08.568513: step 1280, loss 0.517915, acc 0.8125, prec 0.038362, recall 0.808348
2017-12-10T14:53:08.759861: step 1281, loss 0.668757, acc 0.796875, prec 0.0383807, recall 0.808511
2017-12-10T14:53:08.949542: step 1282, loss 0.490164, acc 0.796875, prec 0.0383994, recall 0.808673
2017-12-10T14:53:09.140056: step 1283, loss 0.411392, acc 0.90625, prec 0.0384677, recall 0.808998
2017-12-10T14:53:09.329653: step 1284, loss 0.296769, acc 0.890625, prec 0.0385345, recall 0.809322
2017-12-10T14:53:09.524601: step 1285, loss 0.565454, acc 0.75, prec 0.0385096, recall 0.809322
2017-12-10T14:53:09.719313: step 1286, loss 0.659223, acc 0.75, prec 0.0384848, recall 0.809322
2017-12-10T14:53:09.908964: step 1287, loss 0.582283, acc 0.875, prec 0.0385886, recall 0.809806
2017-12-10T14:53:10.098304: step 1288, loss 0.310128, acc 0.90625, prec 0.0385793, recall 0.809806
2017-12-10T14:53:10.287664: step 1289, loss 0.390752, acc 0.859375, prec 0.0385653, recall 0.809806
2017-12-10T14:53:10.477885: step 1290, loss 0.311272, acc 0.90625, prec 0.038556, recall 0.809806
2017-12-10T14:53:10.669680: step 1291, loss 0.324743, acc 0.90625, prec 0.0385853, recall 0.809966
2017-12-10T14:53:10.861817: step 1292, loss 1.3473, acc 0.875, prec 0.0386116, recall 0.810127
2017-12-10T14:53:11.059721: step 1293, loss 0.290902, acc 0.875, prec 0.0385992, recall 0.810127
2017-12-10T14:53:11.252132: step 1294, loss 0.263401, acc 0.90625, prec 0.0385899, recall 0.810127
2017-12-10T14:53:11.446367: step 1295, loss 0.147249, acc 0.921875, prec 0.0386207, recall 0.810287
2017-12-10T14:53:11.634153: step 1296, loss 0.545735, acc 0.828125, prec 0.0386423, recall 0.810447
2017-12-10T14:53:11.828550: step 1297, loss 0.321877, acc 0.9375, prec 0.0386747, recall 0.810606
2017-12-10T14:53:12.020673: step 1298, loss 0.37033, acc 0.859375, prec 0.0386607, recall 0.810606
2017-12-10T14:53:12.215356: step 1299, loss 0.863812, acc 0.953125, prec 0.0387332, recall 0.810924
2017-12-10T14:53:12.410466: step 1300, loss 6.36114, acc 0.828125, prec 0.0387193, recall 0.809564
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_0/1512935327/checkpoints/model-1300

2017-12-10T14:53:13.789369: step 1301, loss 0.1253, acc 0.96875, prec 0.0387161, recall 0.809564
2017-12-10T14:53:13.983190: step 1302, loss 0.346139, acc 0.890625, prec 0.0387438, recall 0.809723
2017-12-10T14:53:14.179963: step 1303, loss 0.300949, acc 0.921875, prec 0.0387361, recall 0.809723
2017-12-10T14:53:14.371678: step 1304, loss 1.16954, acc 0.78125, prec 0.0388299, recall 0.810201
2017-12-10T14:53:14.564789: step 1305, loss 0.324952, acc 0.921875, prec 0.0388991, recall 0.810518
2017-12-10T14:53:14.757147: step 1306, loss 0.291006, acc 0.875, prec 0.0389636, recall 0.810833
2017-12-10T14:53:14.948001: step 1307, loss 0.605752, acc 0.921875, prec 0.0389943, recall 0.810991
2017-12-10T14:53:15.140712: step 1308, loss 0.404868, acc 0.859375, prec 0.0389803, recall 0.810991
2017-12-10T14:53:15.334004: step 1309, loss 0.566851, acc 0.859375, prec 0.0389662, recall 0.810991
2017-12-10T14:53:15.523028: step 1310, loss 0.747395, acc 0.84375, prec 0.0389891, recall 0.811148
2017-12-10T14:53:15.711950: step 1311, loss 0.213384, acc 0.953125, prec 0.0390228, recall 0.811305
2017-12-10T14:53:15.901810: step 1312, loss 0.493207, acc 0.8125, prec 0.0390809, recall 0.811618
2017-12-10T14:53:16.092271: step 1313, loss 0.574004, acc 0.75, prec 0.0390559, recall 0.811618
2017-12-10T14:53:16.282665: step 1314, loss 0.502771, acc 0.796875, prec 0.0390357, recall 0.811618
2017-12-10T14:53:16.472492: step 1315, loss 0.545295, acc 0.875, prec 0.0390616, recall 0.811774
2017-12-10T14:53:16.664109: step 1316, loss 0.651842, acc 0.78125, prec 0.0390781, recall 0.81193
2017-12-10T14:53:16.853778: step 1317, loss 0.344266, acc 0.84375, prec 0.0390625, recall 0.81193
2017-12-10T14:53:17.046765: step 1318, loss 0.781705, acc 0.796875, prec 0.0390806, recall 0.812086
2017-12-10T14:53:17.238569: step 1319, loss 0.350581, acc 0.84375, prec 0.0391033, recall 0.812241
2017-12-10T14:53:17.425829: step 1320, loss 0.263663, acc 0.9375, prec 0.039097, recall 0.812241
2017-12-10T14:53:17.618079: step 1321, loss 0.354159, acc 0.84375, prec 0.0390815, recall 0.812241
2017-12-10T14:53:17.809643: step 1322, loss 0.360173, acc 0.84375, prec 0.0391041, recall 0.812397
2017-12-10T14:53:18.003525: step 1323, loss 1.0072, acc 0.828125, prec 0.0391635, recall 0.812706
2017-12-10T14:53:18.196711: step 1324, loss 1.30457, acc 0.828125, prec 0.0391845, recall 0.812861
2017-12-10T14:53:18.388141: step 1325, loss 0.248559, acc 0.890625, prec 0.0391736, recall 0.812861
2017-12-10T14:53:18.578815: step 1326, loss 0.30077, acc 0.90625, prec 0.0391643, recall 0.812861
2017-12-10T14:53:18.771973: step 1327, loss 0.302776, acc 0.890625, prec 0.0391534, recall 0.812861
2017-12-10T14:53:18.967327: step 1328, loss 0.377988, acc 0.96875, prec 0.0391884, recall 0.813015
2017-12-10T14:53:19.160866: step 1329, loss 3.18565, acc 0.875, prec 0.0392538, recall 0.812654
2017-12-10T14:53:19.355377: step 1330, loss 0.954452, acc 0.875, prec 0.0392795, recall 0.812808
2017-12-10T14:53:19.545433: step 1331, loss 0.242619, acc 0.9375, prec 0.0393114, recall 0.812961
2017-12-10T14:53:19.737229: step 1332, loss 0.40707, acc 0.875, prec 0.0392989, recall 0.812961
2017-12-10T14:53:19.927545: step 1333, loss 0.283935, acc 0.921875, prec 0.0393673, recall 0.813268
2017-12-10T14:53:20.123237: step 1334, loss 0.509328, acc 0.921875, prec 0.0393975, recall 0.813421
2017-12-10T14:53:20.316322: step 1335, loss 1.15836, acc 0.875, prec 0.0394612, recall 0.813725
2017-12-10T14:53:20.509631: step 1336, loss 0.27817, acc 0.921875, prec 0.0394914, recall 0.813878
2017-12-10T14:53:20.698924: step 1337, loss 0.583717, acc 0.765625, prec 0.039506, recall 0.814029
2017-12-10T14:53:20.892349: step 1338, loss 1.0239, acc 0.8125, prec 0.0396012, recall 0.814483
2017-12-10T14:53:21.081590: step 1339, loss 0.406321, acc 0.828125, prec 0.039584, recall 0.814483
2017-12-10T14:53:21.274680: step 1340, loss 1.84886, acc 0.75, prec 0.0396364, recall 0.814123
2017-12-10T14:53:21.468490: step 1341, loss 0.688618, acc 0.75, prec 0.0396114, recall 0.814123
2017-12-10T14:53:21.656401: step 1342, loss 0.722225, acc 0.703125, prec 0.0395817, recall 0.814123
2017-12-10T14:53:21.844567: step 1343, loss 0.565394, acc 0.765625, prec 0.0395962, recall 0.814274
2017-12-10T14:53:22.040239: step 1344, loss 1.34311, acc 0.703125, prec 0.0396044, recall 0.814425
2017-12-10T14:53:22.236595: step 1345, loss 0.684155, acc 0.8125, prec 0.0396613, recall 0.814725
2017-12-10T14:53:22.422813: step 1346, loss 0.592291, acc 0.75, prec 0.0396363, recall 0.814725
2017-12-10T14:53:22.611648: step 1347, loss 0.61679, acc 0.8125, prec 0.0396176, recall 0.814725
2017-12-10T14:53:22.803640: step 1348, loss 0.658008, acc 0.75, prec 0.0396304, recall 0.814875
2017-12-10T14:53:22.996724: step 1349, loss 0.655738, acc 0.765625, prec 0.0396825, recall 0.815174
2017-12-10T14:53:23.186002: step 1350, loss 0.701847, acc 0.84375, prec 0.0397047, recall 0.815323
2017-12-10T14:53:23.378691: step 1351, loss 0.808845, acc 0.828125, prec 0.0397629, recall 0.81562
2017-12-10T14:53:23.567639: step 1352, loss 0.369856, acc 0.796875, prec 0.0397803, recall 0.815768
2017-12-10T14:53:23.760973: step 1353, loss 0.762404, acc 0.796875, prec 0.0397977, recall 0.815916
2017-12-10T14:53:23.950664: step 1354, loss 0.369416, acc 0.84375, prec 0.0397821, recall 0.815916
2017-12-10T14:53:24.143511: step 1355, loss 0.647822, acc 0.8125, prec 0.039801, recall 0.816064
2017-12-10T14:53:24.338290: step 1356, loss 0.587629, acc 0.8125, prec 0.0398199, recall 0.816212
2017-12-10T14:53:24.530271: step 1357, loss 0.681727, acc 0.703125, prec 0.0397903, recall 0.816212
2017-12-10T14:53:24.724761: step 1358, loss 0.333984, acc 0.90625, prec 0.0398185, recall 0.816359
2017-12-10T14:53:24.921328: step 1359, loss 0.723073, acc 0.9375, prec 0.0398498, recall 0.816506
2017-12-10T14:53:25.115055: step 1360, loss 0.670634, acc 0.921875, prec 0.0399171, recall 0.8168
2017-12-10T14:53:25.307458: step 1361, loss 0.378834, acc 0.875, prec 0.0399422, recall 0.816946
2017-12-10T14:53:25.497650: step 1362, loss 0.221955, acc 0.921875, prec 0.0399719, recall 0.817093
2017-12-10T14:53:25.695074: step 1363, loss 0.248375, acc 0.875, prec 0.0399594, recall 0.817093
2017-12-10T14:53:25.887772: step 1364, loss 3.7668, acc 0.921875, prec 0.0399906, recall 0.816587
2017-12-10T14:53:26.085567: step 1365, loss 0.107101, acc 0.96875, prec 0.040025, recall 0.816733
2017-12-10T14:53:26.280417: step 1366, loss 0.339364, acc 0.84375, prec 0.0400094, recall 0.816733
2017-12-10T14:53:26.471786: step 1367, loss 0.153451, acc 0.9375, prec 0.0400031, recall 0.816733
2017-12-10T14:53:26.668957: step 1368, loss 0.304025, acc 0.890625, prec 0.0400671, recall 0.817025
2017-12-10T14:53:26.860154: step 1369, loss 4.29732, acc 0.796875, prec 0.0400484, recall 0.816375
2017-12-10T14:53:27.058887: step 1370, loss 0.282185, acc 0.875, prec 0.0400359, recall 0.816375
2017-12-10T14:53:27.247737: step 1371, loss 0.414211, acc 0.859375, prec 0.0400592, recall 0.816521
2017-12-10T14:53:27.439994: step 1372, loss 0.891183, acc 0.765625, prec 0.0400732, recall 0.816667
2017-12-10T14:53:27.629079: step 1373, loss 0.677772, acc 0.78125, prec 0.0400887, recall 0.816812
2017-12-10T14:53:27.818491: step 1374, loss 1.16223, acc 0.84375, prec 0.0401478, recall 0.817102
2017-12-10T14:53:28.015622: step 1375, loss 0.449829, acc 0.828125, prec 0.040168, recall 0.817247
2017-12-10T14:53:28.210461: step 1376, loss 0.295861, acc 0.875, prec 0.0401555, recall 0.817247
2017-12-10T14:53:28.398281: step 1377, loss 0.421841, acc 0.875, prec 0.0402176, recall 0.817536
2017-12-10T14:53:28.589512: step 1378, loss 0.526139, acc 0.78125, prec 0.0401957, recall 0.817536
2017-12-10T14:53:28.777283: step 1379, loss 0.514558, acc 0.8125, prec 0.040177, recall 0.817536
2017-12-10T14:53:28.969472: step 1380, loss 0.771181, acc 0.859375, prec 0.0402002, recall 0.81768
2017-12-10T14:53:29.165862: step 1381, loss 0.982066, acc 0.890625, prec 0.0402638, recall 0.817967
2017-12-10T14:53:29.361607: step 1382, loss 0.452602, acc 0.890625, prec 0.0402901, recall 0.81811
2017-12-10T14:53:29.554109: step 1383, loss 0.207311, acc 0.890625, prec 0.0402791, recall 0.81811
2017-12-10T14:53:29.744666: step 1384, loss 0.420087, acc 0.859375, prec 0.0402651, recall 0.81811
2017-12-10T14:53:29.939523: step 1385, loss 0.488048, acc 0.84375, prec 0.0403238, recall 0.818396
2017-12-10T14:53:30.132693: step 1386, loss 0.312881, acc 0.84375, prec 0.0403082, recall 0.818396
2017-12-10T14:53:30.323280: step 1387, loss 0.493187, acc 0.84375, prec 0.0403298, recall 0.818539
2017-12-10T14:53:30.513306: step 1388, loss 0.301024, acc 0.875, prec 0.0403544, recall 0.818681
2017-12-10T14:53:30.707560: step 1389, loss 0.733141, acc 0.84375, prec 0.0403759, recall 0.818824
2017-12-10T14:53:30.897798: step 1390, loss 0.490223, acc 0.8125, prec 0.0403943, recall 0.818965
2017-12-10T14:53:31.090715: step 1391, loss 1.85333, acc 0.78125, prec 0.0404481, recall 0.818608
2017-12-10T14:53:31.287223: step 1392, loss 0.500832, acc 0.734375, prec 0.0404216, recall 0.818608
2017-12-10T14:53:31.482644: step 1393, loss 0.239954, acc 0.90625, prec 0.0404493, recall 0.81875
2017-12-10T14:53:31.675652: step 1394, loss 0.31311, acc 0.921875, prec 0.0404415, recall 0.81875
2017-12-10T14:53:31.864725: step 1395, loss 1.09392, acc 0.875, prec 0.040466, recall 0.818891
2017-12-10T14:53:32.060050: step 1396, loss 0.662401, acc 0.890625, prec 0.0405291, recall 0.819174
2017-12-10T14:53:32.251557: step 1397, loss 1.45514, acc 0.921875, prec 0.0405952, recall 0.819455
2017-12-10T14:53:32.443849: step 1398, loss 0.52912, acc 0.796875, prec 0.0405749, recall 0.819455
2017-12-10T14:53:32.635880: step 1399, loss 0.58512, acc 0.84375, prec 0.0406332, recall 0.819736
2017-12-10T14:53:32.827997: step 1400, loss 0.311824, acc 0.84375, prec 0.0406545, recall 0.819876
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_0/1512935327/checkpoints/model-1400

2017-12-10T14:53:34.065342: step 1401, loss 0.296073, acc 0.890625, prec 0.0406435, recall 0.819876
2017-12-10T14:53:34.256030: step 1402, loss 0.876373, acc 0.765625, prec 0.0406201, recall 0.819876
2017-12-10T14:53:34.448938: step 1403, loss 0.776039, acc 0.765625, prec 0.0406704, recall 0.820155
2017-12-10T14:53:34.640547: step 1404, loss 0.383441, acc 0.828125, prec 0.0407269, recall 0.820433
2017-12-10T14:53:34.833597: step 1405, loss 0.36859, acc 0.84375, prec 0.0407113, recall 0.820433
2017-12-10T14:53:35.028227: step 1406, loss 0.549135, acc 0.765625, prec 0.0406879, recall 0.820433
2017-12-10T14:53:35.222341: step 1407, loss 0.442317, acc 0.8125, prec 0.0406691, recall 0.820433
2017-12-10T14:53:35.412859: step 1408, loss 0.712878, acc 0.890625, prec 0.040695, recall 0.820572
2017-12-10T14:53:35.605436: step 1409, loss 0.352619, acc 0.90625, prec 0.0407224, recall 0.820711
2017-12-10T14:53:35.799139: step 1410, loss 0.343175, acc 0.890625, prec 0.0407115, recall 0.820711
2017-12-10T14:53:35.987200: step 1411, loss 0.288681, acc 0.890625, prec 0.0407373, recall 0.820849
2017-12-10T14:53:36.178470: step 1412, loss 0.219805, acc 0.9375, prec 0.0407678, recall 0.820988
2017-12-10T14:53:36.368819: step 1413, loss 0.403899, acc 0.90625, prec 0.0407952, recall 0.821126
2017-12-10T14:53:36.561575: step 1414, loss 0.32181, acc 0.828125, prec 0.040778, recall 0.821126
2017-12-10T14:53:36.754424: step 1415, loss 0.225988, acc 0.921875, prec 0.0407702, recall 0.821126
2017-12-10T14:53:36.950011: step 1416, loss 4.75493, acc 0.859375, prec 0.0407945, recall 0.820631
2017-12-10T14:53:37.144973: step 1417, loss 0.494157, acc 0.921875, prec 0.04086, recall 0.820907
2017-12-10T14:53:37.340829: step 1418, loss 0.239572, acc 0.90625, prec 0.0408507, recall 0.820907
2017-12-10T14:53:37.537884: step 1419, loss 2.44477, acc 0.859375, prec 0.0408382, recall 0.820276
2017-12-10T14:53:37.729677: step 1420, loss 0.241553, acc 0.890625, prec 0.0409006, recall 0.820552
2017-12-10T14:53:37.921105: step 1421, loss 0.373047, acc 0.890625, prec 0.0408896, recall 0.820552
2017-12-10T14:53:38.114261: step 1422, loss 0.412716, acc 0.875, prec 0.0409504, recall 0.820827
2017-12-10T14:53:38.306488: step 1423, loss 0.706092, acc 0.734375, prec 0.0409238, recall 0.820827
2017-12-10T14:53:38.494474: step 1424, loss 0.517478, acc 0.796875, prec 0.0409035, recall 0.820827
2017-12-10T14:53:38.686072: step 1425, loss 0.940981, acc 0.8125, prec 0.0409214, recall 0.820964
2017-12-10T14:53:38.879359: step 1426, loss 0.301396, acc 0.875, prec 0.0409455, recall 0.821101
2017-12-10T14:53:39.075067: step 1427, loss 2.82473, acc 0.765625, prec 0.0409602, recall 0.820611
2017-12-10T14:53:39.272774: step 1428, loss 0.704598, acc 0.6875, prec 0.040929, recall 0.820611
2017-12-10T14:53:39.466429: step 1429, loss 0.443211, acc 0.8125, prec 0.0409103, recall 0.820611
2017-12-10T14:53:39.657605: step 1430, loss 0.714881, acc 0.71875, prec 0.0408823, recall 0.820611
2017-12-10T14:53:39.851122: step 1431, loss 0.542311, acc 0.734375, prec 0.0408923, recall 0.820747
2017-12-10T14:53:40.041450: step 1432, loss 0.273028, acc 0.875, prec 0.0408799, recall 0.820747
2017-12-10T14:53:40.235909: step 1433, loss 0.778884, acc 0.6875, prec 0.0408853, recall 0.820884
2017-12-10T14:53:40.426745: step 1434, loss 0.659749, acc 0.75, prec 0.0408605, recall 0.820884
2017-12-10T14:53:40.616020: step 1435, loss 0.747975, acc 0.78125, prec 0.0409115, recall 0.821157
2017-12-10T14:53:40.808873: step 1436, loss 0.396372, acc 0.828125, prec 0.0408944, recall 0.821157
2017-12-10T14:53:41.003873: step 1437, loss 0.423097, acc 0.84375, prec 0.0409153, recall 0.821293
2017-12-10T14:53:41.200474: step 1438, loss 0.363618, acc 0.875, prec 0.0409029, recall 0.821293
2017-12-10T14:53:41.390614: step 1439, loss 1.10427, acc 0.71875, prec 0.0410202, recall 0.821835
2017-12-10T14:53:41.588356: step 1440, loss 0.283456, acc 0.890625, prec 0.0410093, recall 0.821835
2017-12-10T14:53:41.782334: step 1441, loss 6.23733, acc 0.875, prec 0.041, recall 0.82059
2017-12-10T14:53:41.976492: step 1442, loss 0.966411, acc 0.828125, prec 0.0410192, recall 0.820726
2017-12-10T14:53:42.168193: step 1443, loss 0.610446, acc 0.78125, prec 0.0409975, recall 0.820726
2017-12-10T14:53:42.357118: step 1444, loss 0.228139, acc 0.90625, prec 0.0409883, recall 0.820726
2017-12-10T14:53:42.546282: step 1445, loss 0.533823, acc 0.8125, prec 0.0410059, recall 0.820862
2017-12-10T14:53:42.733501: step 1446, loss 1.23484, acc 0.765625, prec 0.0410189, recall 0.820997
2017-12-10T14:53:42.928788: step 1447, loss 1.0655, acc 0.78125, prec 0.0410334, recall 0.821132
2017-12-10T14:53:43.120305: step 1448, loss 0.960161, acc 0.671875, prec 0.041037, recall 0.821267
2017-12-10T14:53:43.313386: step 1449, loss 0.533836, acc 0.78125, prec 0.0411237, recall 0.82167
2017-12-10T14:53:43.505032: step 1450, loss 0.599712, acc 0.78125, prec 0.0411021, recall 0.82167
2017-12-10T14:53:43.696717: step 1451, loss 0.841566, acc 0.75, prec 0.0410773, recall 0.82167
2017-12-10T14:53:43.886960: step 1452, loss 0.994874, acc 0.71875, prec 0.0411216, recall 0.821938
2017-12-10T14:53:44.081187: step 1453, loss 0.443295, acc 0.765625, prec 0.0410985, recall 0.821938
2017-12-10T14:53:44.278076: step 1454, loss 0.476798, acc 0.875, prec 0.0410861, recall 0.821938
2017-12-10T14:53:44.466034: step 1455, loss 0.685841, acc 0.75, prec 0.0410614, recall 0.821938
2017-12-10T14:53:44.653281: step 1456, loss 0.57427, acc 0.796875, prec 0.0410774, recall 0.822072
2017-12-10T14:53:44.846021: step 1457, loss 1.59029, acc 0.71875, prec 0.0410856, recall 0.822206
2017-12-10T14:53:45.041527: step 1458, loss 0.748106, acc 0.796875, prec 0.0411015, recall 0.822339
2017-12-10T14:53:45.231363: step 1459, loss 0.473035, acc 0.8125, prec 0.041119, recall 0.822472
2017-12-10T14:53:45.424660: step 1460, loss 0.781532, acc 0.9375, prec 0.0411846, recall 0.822737
2017-12-10T14:53:45.619680: step 1461, loss 1.36245, acc 0.921875, prec 0.0412487, recall 0.823002
2017-12-10T14:53:45.813021: step 1462, loss 0.422286, acc 0.8125, prec 0.0412302, recall 0.823002
2017-12-10T14:53:46.002507: step 1463, loss 0.334365, acc 0.90625, prec 0.0412209, recall 0.823002
2017-12-10T14:53:46.191710: step 1464, loss 0.129947, acc 0.9375, prec 0.0412147, recall 0.823002
2017-12-10T14:53:46.382617: step 1465, loss 0.501864, acc 0.8125, prec 0.0411963, recall 0.823002
2017-12-10T14:53:46.574376: step 1466, loss 0.446303, acc 0.828125, prec 0.0411793, recall 0.823002
2017-12-10T14:53:46.766432: step 1467, loss 3.53871, acc 0.9375, prec 0.0412464, recall 0.822653
2017-12-10T14:53:46.959096: step 1468, loss 0.479711, acc 0.84375, prec 0.0413026, recall 0.822917
2017-12-10T14:53:47.148499: step 1469, loss 0.438135, acc 0.8125, prec 0.0412841, recall 0.822917
2017-12-10T14:53:47.347470: step 1470, loss 0.444597, acc 0.84375, prec 0.0413044, recall 0.823048
2017-12-10T14:53:47.536489: step 1471, loss 0.567999, acc 0.8125, prec 0.0413217, recall 0.82318
2017-12-10T14:53:47.730415: step 1472, loss 0.455861, acc 0.875, prec 0.0413094, recall 0.82318
2017-12-10T14:53:47.918941: step 1473, loss 1.20611, acc 0.734375, prec 0.0413546, recall 0.823442
2017-12-10T14:53:48.108534: step 1474, loss 0.404491, acc 0.875, prec 0.0413423, recall 0.823442
2017-12-10T14:53:48.296651: step 1475, loss 0.518295, acc 0.78125, prec 0.0413208, recall 0.823442
2017-12-10T14:53:48.489984: step 1476, loss 1.79407, acc 0.78125, prec 0.0413008, recall 0.822832
2017-12-10T14:53:48.681184: step 1477, loss 0.630007, acc 0.796875, prec 0.0413165, recall 0.822963
2017-12-10T14:53:48.870780: step 1478, loss 0.782618, acc 0.78125, prec 0.0413306, recall 0.823094
2017-12-10T14:53:49.061434: step 1479, loss 0.715972, acc 0.796875, prec 0.0413819, recall 0.823355
2017-12-10T14:53:49.249557: step 1480, loss 0.558717, acc 0.796875, prec 0.0413619, recall 0.823355
2017-12-10T14:53:49.436757: step 1481, loss 0.644114, acc 0.828125, prec 0.0413806, recall 0.823486
2017-12-10T14:53:49.626597: step 1482, loss 0.546174, acc 0.8125, prec 0.0413622, recall 0.823486
2017-12-10T14:53:49.817817: step 1483, loss 0.382831, acc 0.828125, prec 0.0413808, recall 0.823616
2017-12-10T14:53:50.012973: step 1484, loss 0.45182, acc 0.90625, prec 0.0414427, recall 0.823876
2017-12-10T14:53:50.204026: step 1485, loss 0.62058, acc 0.75, prec 0.0414181, recall 0.823876
2017-12-10T14:53:50.397978: step 1486, loss 0.515073, acc 0.8125, prec 0.0414707, recall 0.824135
2017-12-10T14:53:50.587477: step 1487, loss 0.425051, acc 0.828125, prec 0.0414893, recall 0.824265
2017-12-10T14:53:50.779683: step 1488, loss 0.203916, acc 0.921875, prec 0.0414816, recall 0.824265
2017-12-10T14:53:50.970354: step 1489, loss 3.18327, acc 0.875, prec 0.0414709, recall 0.823659
2017-12-10T14:53:51.164592: step 1490, loss 0.311661, acc 0.859375, prec 0.0414571, recall 0.823659
2017-12-10T14:53:51.337675: step 1491, loss 0.405808, acc 0.882353, prec 0.0414833, recall 0.823789
2017-12-10T14:53:51.536721: step 1492, loss 0.412629, acc 0.859375, prec 0.041505, recall 0.823918
2017-12-10T14:53:51.727965: step 1493, loss 1.08121, acc 0.921875, prec 0.0415327, recall 0.824047
2017-12-10T14:53:51.925241: step 1494, loss 0.48717, acc 0.875, prec 0.0415559, recall 0.824176
2017-12-10T14:53:52.118068: step 1495, loss 0.147506, acc 0.90625, prec 0.0415466, recall 0.824176
2017-12-10T14:53:52.306708: step 1496, loss 0.327628, acc 0.828125, prec 0.0415652, recall 0.824305
2017-12-10T14:53:52.500606: step 1497, loss 0.372144, acc 0.890625, prec 0.0415544, recall 0.824305
2017-12-10T14:53:52.694222: step 1498, loss 0.321043, acc 0.890625, prec 0.0415437, recall 0.824305
2017-12-10T14:53:52.882927: step 1499, loss 0.378792, acc 0.859375, prec 0.0415299, recall 0.824305
2017-12-10T14:53:53.074859: step 1500, loss 0.191954, acc 0.9375, prec 0.0415591, recall 0.824433
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_0/1512935327/checkpoints/model-1500

2017-12-10T14:53:54.559475: step 1501, loss 0.330011, acc 0.859375, prec 0.0415453, recall 0.824433
2017-12-10T14:53:54.750615: step 1502, loss 0.182162, acc 0.953125, prec 0.0415761, recall 0.824561
2017-12-10T14:53:54.941378: step 1503, loss 0.246633, acc 0.90625, prec 0.0416022, recall 0.82469
2017-12-10T14:53:55.129796: step 1504, loss 1.3859, acc 0.921875, prec 0.0416651, recall 0.824945
2017-12-10T14:53:55.321110: step 1505, loss 0.167049, acc 0.921875, prec 0.0416575, recall 0.824945
2017-12-10T14:53:55.514631: step 1506, loss 1.07604, acc 0.984375, prec 0.0417265, recall 0.8252
2017-12-10T14:53:55.710271: step 1507, loss 0.116685, acc 0.96875, prec 0.0417234, recall 0.8252
2017-12-10T14:53:55.901078: step 1508, loss 0.14772, acc 0.9375, prec 0.0417173, recall 0.8252
2017-12-10T14:53:56.093607: step 1509, loss 0.181483, acc 0.921875, prec 0.0417096, recall 0.8252
2017-12-10T14:53:56.285969: step 1510, loss 0.181698, acc 0.953125, prec 0.0417403, recall 0.825328
2017-12-10T14:53:56.477214: step 1511, loss 0.14843, acc 0.953125, prec 0.0417709, recall 0.825455
2017-12-10T14:53:56.670966: step 1512, loss 0.15267, acc 0.9375, prec 0.0417648, recall 0.825455
2017-12-10T14:53:56.866654: step 1513, loss 0.258723, acc 0.84375, prec 0.0417847, recall 0.825581
2017-12-10T14:53:57.059220: step 1514, loss 1.03742, acc 0.90625, prec 0.0418107, recall 0.825708
2017-12-10T14:53:57.254943: step 1515, loss 1.02708, acc 0.90625, prec 0.0418719, recall 0.825961
2017-12-10T14:53:57.456107: step 1516, loss 0.185428, acc 0.9375, prec 0.0418658, recall 0.825961
2017-12-10T14:53:57.650714: step 1517, loss 0.288958, acc 0.875, prec 0.0418535, recall 0.825961
2017-12-10T14:53:57.838688: step 1518, loss 0.328614, acc 0.84375, prec 0.0418381, recall 0.825961
2017-12-10T14:53:58.027432: step 1519, loss 0.260853, acc 0.9375, prec 0.0419023, recall 0.826213
2017-12-10T14:53:58.218531: step 1520, loss 0.111682, acc 0.953125, prec 0.0418977, recall 0.826213
2017-12-10T14:53:58.414390: step 1521, loss 0.540491, acc 0.9375, prec 0.0419619, recall 0.826464
2017-12-10T14:53:58.603622: step 1522, loss 0.49408, acc 0.875, prec 0.0419847, recall 0.82659
2017-12-10T14:53:58.800061: step 1523, loss 0.557546, acc 0.84375, prec 0.0420396, recall 0.82684
2017-12-10T14:53:58.992294: step 1524, loss 0.491448, acc 0.78125, prec 0.0420532, recall 0.826965
2017-12-10T14:53:59.195340: step 1525, loss 0.341446, acc 0.90625, prec 0.0420439, recall 0.826965
2017-12-10T14:53:59.385501: step 1526, loss 0.265238, acc 0.953125, prec 0.0420744, recall 0.827089
2017-12-10T14:53:59.579411: step 1527, loss 0.353679, acc 0.875, prec 0.0420972, recall 0.827214
2017-12-10T14:53:59.768205: step 1528, loss 0.462678, acc 0.890625, prec 0.0421565, recall 0.827462
2017-12-10T14:53:59.961006: step 1529, loss 0.371179, acc 0.859375, prec 0.0421777, recall 0.827586
2017-12-10T14:54:00.151433: step 1530, loss 0.1821, acc 0.9375, prec 0.0421715, recall 0.827586
2017-12-10T14:54:00.340829: step 1531, loss 2.28351, acc 0.84375, prec 0.0421577, recall 0.826992
2017-12-10T14:54:00.533852: step 1532, loss 0.612019, acc 0.859375, prec 0.0422138, recall 0.82724
2017-12-10T14:54:00.729564: step 1533, loss 0.303615, acc 0.859375, prec 0.0422, recall 0.82724
2017-12-10T14:54:00.917377: step 1534, loss 0.303128, acc 0.859375, prec 0.0421861, recall 0.82724
2017-12-10T14:54:01.111534: step 1535, loss 0.295986, acc 0.859375, prec 0.0421722, recall 0.82724
2017-12-10T14:54:01.310268: step 1536, loss 0.645387, acc 0.84375, prec 0.0421918, recall 0.827364
2017-12-10T14:54:01.502305: step 1537, loss 0.143591, acc 0.953125, prec 0.0421872, recall 0.827364
2017-12-10T14:54:01.692575: step 1538, loss 0.517639, acc 0.84375, prec 0.0422417, recall 0.827611
2017-12-10T14:54:01.884821: step 1539, loss 0.308879, acc 0.921875, prec 0.0422689, recall 0.827734
2017-12-10T14:54:02.075217: step 1540, loss 0.3538, acc 0.84375, prec 0.0422535, recall 0.827734
2017-12-10T14:54:02.265725: step 1541, loss 0.361419, acc 0.921875, prec 0.0422808, recall 0.827857
2017-12-10T14:54:02.458444: step 1542, loss 0.427194, acc 0.90625, prec 0.0423064, recall 0.82798
2017-12-10T14:54:02.649429: step 1543, loss 0.540699, acc 0.90625, prec 0.0423321, recall 0.828103
2017-12-10T14:54:02.844124: step 1544, loss 0.402194, acc 0.859375, prec 0.0423182, recall 0.828103
2017-12-10T14:54:03.037098: step 1545, loss 0.130638, acc 0.953125, prec 0.0423136, recall 0.828103
2017-12-10T14:54:03.228041: step 1546, loss 0.981601, acc 0.953125, prec 0.0423439, recall 0.828225
2017-12-10T14:54:03.422561: step 1547, loss 0.144327, acc 0.9375, prec 0.0423726, recall 0.828348
2017-12-10T14:54:03.613140: step 1548, loss 0.723059, acc 0.9375, prec 0.0424013, recall 0.82847
2017-12-10T14:54:03.806117: step 1549, loss 0.344419, acc 0.921875, prec 0.0423936, recall 0.82847
2017-12-10T14:54:04.000813: step 1550, loss 0.261997, acc 0.921875, prec 0.0424207, recall 0.828592
2017-12-10T14:54:04.194700: step 1551, loss 0.427315, acc 0.90625, prec 0.0424463, recall 0.828714
2017-12-10T14:54:04.385403: step 1552, loss 0.164389, acc 0.96875, prec 0.0424781, recall 0.828835
2017-12-10T14:54:04.577683: step 1553, loss 0.32109, acc 0.921875, prec 0.0424703, recall 0.828835
2017-12-10T14:54:04.770418: step 1554, loss 0.156504, acc 0.953125, prec 0.0425354, recall 0.829078
2017-12-10T14:54:04.962722: step 1555, loss 0.209228, acc 0.9375, prec 0.042564, recall 0.829199
2017-12-10T14:54:05.155177: step 1556, loss 0.440019, acc 0.90625, prec 0.0425896, recall 0.82932
2017-12-10T14:54:05.347603: step 1557, loss 0.272539, acc 0.921875, prec 0.0425818, recall 0.82932
2017-12-10T14:54:05.537413: step 1558, loss 0.349649, acc 0.859375, prec 0.0425679, recall 0.82932
2017-12-10T14:54:05.726184: step 1559, loss 4.946, acc 0.921875, prec 0.0425617, recall 0.828733
2017-12-10T14:54:05.918661: step 1560, loss 0.308908, acc 0.859375, prec 0.0425826, recall 0.828854
2017-12-10T14:54:06.112897: step 1561, loss 0.235742, acc 0.96875, prec 0.0426143, recall 0.828975
2017-12-10T14:54:06.306556: step 1562, loss 0.567582, acc 0.796875, prec 0.0426289, recall 0.829096
2017-12-10T14:54:06.504617: step 1563, loss 0.611301, acc 0.8125, prec 0.0426798, recall 0.829337
2017-12-10T14:54:06.698003: step 1564, loss 0.568826, acc 0.71875, prec 0.042652, recall 0.829337
2017-12-10T14:54:06.895639: step 1565, loss 0.358776, acc 0.84375, prec 0.0426365, recall 0.829337
2017-12-10T14:54:07.085702: step 1566, loss 0.543172, acc 0.796875, prec 0.0426511, recall 0.829457
2017-12-10T14:54:07.275335: step 1567, loss 0.647439, acc 0.8125, prec 0.0428059, recall 0.830056
2017-12-10T14:54:07.465468: step 1568, loss 0.474404, acc 0.828125, prec 0.0427889, recall 0.830056
2017-12-10T14:54:07.656482: step 1569, loss 0.236774, acc 0.921875, prec 0.0428851, recall 0.830413
2017-12-10T14:54:07.843762: step 1570, loss 0.316683, acc 0.875, prec 0.0428726, recall 0.830413
2017-12-10T14:54:08.032794: step 1571, loss 0.311003, acc 0.921875, prec 0.0428649, recall 0.830413
2017-12-10T14:54:08.226949: step 1572, loss 0.264631, acc 0.890625, prec 0.0428887, recall 0.830532
2017-12-10T14:54:08.417486: step 1573, loss 0.277216, acc 0.875, prec 0.0428763, recall 0.830532
2017-12-10T14:54:08.612048: step 1574, loss 1.32697, acc 0.859375, prec 0.0428969, recall 0.830651
2017-12-10T14:54:08.804944: step 1575, loss 1.25827, acc 0.828125, prec 0.0429144, recall 0.830769
2017-12-10T14:54:09.005683: step 1576, loss 0.38722, acc 0.859375, prec 0.042935, recall 0.830887
2017-12-10T14:54:09.195027: step 1577, loss 0.529674, acc 0.84375, prec 0.0429195, recall 0.830887
2017-12-10T14:54:09.386595: step 1578, loss 0.30088, acc 0.859375, prec 0.0429401, recall 0.831006
2017-12-10T14:54:09.582216: step 1579, loss 0.39204, acc 0.921875, prec 0.0430014, recall 0.831241
2017-12-10T14:54:09.773015: step 1580, loss 0.970987, acc 0.890625, prec 0.0430941, recall 0.831594
2017-12-10T14:54:09.967567: step 1581, loss 0.549334, acc 0.921875, prec 0.0431554, recall 0.831828
2017-12-10T14:54:10.160028: step 1582, loss 0.426542, acc 0.90625, prec 0.043146, recall 0.831828
2017-12-10T14:54:10.348622: step 1583, loss 0.457607, acc 0.890625, prec 0.0432041, recall 0.832061
2017-12-10T14:54:10.540136: step 1584, loss 0.339638, acc 0.890625, prec 0.0432621, recall 0.832294
2017-12-10T14:54:10.731761: step 1585, loss 0.351548, acc 0.890625, prec 0.0432857, recall 0.83241
2017-12-10T14:54:10.923797: step 1586, loss 0.276616, acc 0.90625, prec 0.0433108, recall 0.832526
2017-12-10T14:54:11.121128: step 1587, loss 0.240443, acc 0.890625, prec 0.0432999, recall 0.832526
2017-12-10T14:54:11.313227: step 1588, loss 0.416367, acc 0.859375, prec 0.0432858, recall 0.832526
2017-12-10T14:54:11.505739: step 1589, loss 0.345365, acc 0.859375, prec 0.0432718, recall 0.832526
2017-12-10T14:54:11.693679: step 1590, loss 0.305793, acc 0.90625, prec 0.0432625, recall 0.832526
2017-12-10T14:54:11.885514: step 1591, loss 0.29336, acc 0.890625, prec 0.0432516, recall 0.832526
2017-12-10T14:54:12.075654: step 1592, loss 0.19875, acc 0.90625, prec 0.0432767, recall 0.832642
2017-12-10T14:54:12.269407: step 1593, loss 0.353033, acc 0.890625, prec 0.0432658, recall 0.832642
2017-12-10T14:54:12.460383: step 1594, loss 0.788321, acc 0.890625, prec 0.0432893, recall 0.832757
2017-12-10T14:54:12.649443: step 1595, loss 0.164134, acc 0.953125, prec 0.0432846, recall 0.832757
2017-12-10T14:54:12.844522: step 1596, loss 1.05994, acc 0.90625, prec 0.043344, recall 0.832988
2017-12-10T14:54:13.036300: step 1597, loss 0.505218, acc 0.921875, prec 0.0434049, recall 0.833218
2017-12-10T14:54:13.230507: step 1598, loss 0.448842, acc 0.859375, prec 0.0433909, recall 0.833218
2017-12-10T14:54:13.419831: step 1599, loss 0.151709, acc 0.96875, prec 0.0434221, recall 0.833333
2017-12-10T14:54:13.612522: step 1600, loss 0.305386, acc 0.84375, prec 0.0434408, recall 0.833448
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_0/1512935327/checkpoints/model-1600

2017-12-10T14:54:15.159664: step 1601, loss 0.388696, acc 0.90625, prec 0.0434658, recall 0.833563
2017-12-10T14:54:15.350954: step 1602, loss 0.224994, acc 0.890625, prec 0.0434549, recall 0.833563
2017-12-10T14:54:15.540680: step 1603, loss 0.256811, acc 0.9375, prec 0.0435515, recall 0.833905
2017-12-10T14:54:15.734214: step 1604, loss 0.165475, acc 0.90625, prec 0.0435421, recall 0.833905
2017-12-10T14:54:15.929521: step 1605, loss 0.413181, acc 0.890625, prec 0.0435998, recall 0.834133
2017-12-10T14:54:16.124798: step 1606, loss 0.138489, acc 0.953125, prec 0.0436293, recall 0.834247
2017-12-10T14:54:16.318228: step 1607, loss 0.132233, acc 0.953125, prec 0.0436931, recall 0.834473
2017-12-10T14:54:16.507404: step 1608, loss 1.00607, acc 0.9375, prec 0.0437211, recall 0.834586
2017-12-10T14:54:16.705426: step 1609, loss 1.34614, acc 0.8125, prec 0.043805, recall 0.834925
2017-12-10T14:54:16.901611: step 1610, loss 1.24208, acc 0.921875, prec 0.0438998, recall 0.835262
2017-12-10T14:54:17.099309: step 1611, loss 0.3253, acc 0.953125, prec 0.0439635, recall 0.835486
2017-12-10T14:54:17.292746: step 1612, loss 0.350366, acc 0.921875, prec 0.0439898, recall 0.835598
2017-12-10T14:54:17.487919: step 1613, loss 0.328511, acc 0.875, prec 0.0439773, recall 0.835598
2017-12-10T14:54:17.677365: step 1614, loss 0.52494, acc 0.796875, prec 0.0439568, recall 0.835598
2017-12-10T14:54:17.869190: step 1615, loss 0.446761, acc 0.859375, prec 0.0439769, recall 0.835709
2017-12-10T14:54:18.057592: step 1616, loss 0.491819, acc 0.78125, prec 0.0439549, recall 0.835709
2017-12-10T14:54:18.250598: step 1617, loss 0.459724, acc 0.84375, prec 0.0439392, recall 0.835709
2017-12-10T14:54:18.441678: step 1618, loss 0.611125, acc 0.828125, prec 0.0439902, recall 0.835932
2017-12-10T14:54:18.639147: step 1619, loss 0.447843, acc 0.84375, prec 0.0440767, recall 0.836265
2017-12-10T14:54:18.831324: step 1620, loss 0.4025, acc 0.828125, prec 0.0440595, recall 0.836265
2017-12-10T14:54:19.022467: step 1621, loss 0.296622, acc 0.890625, prec 0.0440825, recall 0.836376
2017-12-10T14:54:19.216931: step 1622, loss 0.629493, acc 0.828125, prec 0.0440993, recall 0.836486
2017-12-10T14:54:19.409643: step 1623, loss 0.522238, acc 0.859375, prec 0.0440852, recall 0.836486
2017-12-10T14:54:19.600533: step 1624, loss 0.436457, acc 0.890625, prec 0.0440742, recall 0.836486
2017-12-10T14:54:19.793082: step 1625, loss 0.437755, acc 0.90625, prec 0.0440988, recall 0.836597
2017-12-10T14:54:19.982752: step 1626, loss 0.301947, acc 0.90625, prec 0.0441234, recall 0.836707
2017-12-10T14:54:20.177076: step 1627, loss 0.152275, acc 0.9375, prec 0.0441171, recall 0.836707
2017-12-10T14:54:20.368381: step 1628, loss 0.368086, acc 0.859375, prec 0.044103, recall 0.836707
2017-12-10T14:54:20.559212: step 1629, loss 0.249416, acc 0.90625, prec 0.0441616, recall 0.836927
2017-12-10T14:54:20.753904: step 1630, loss 0.941113, acc 0.875, prec 0.044183, recall 0.837037
2017-12-10T14:54:20.947462: step 1631, loss 0.502281, acc 0.921875, prec 0.0442091, recall 0.837147
2017-12-10T14:54:21.136107: step 1632, loss 0.97778, acc 0.90625, prec 0.0442336, recall 0.837256
2017-12-10T14:54:21.330142: step 1633, loss 0.350833, acc 0.859375, prec 0.0442195, recall 0.837256
2017-12-10T14:54:21.520118: step 1634, loss 2.23264, acc 0.921875, prec 0.0442472, recall 0.836803
2017-12-10T14:54:21.713072: step 1635, loss 0.215951, acc 0.9375, prec 0.0442409, recall 0.836803
2017-12-10T14:54:21.909135: step 1636, loss 0.15931, acc 0.9375, prec 0.0442685, recall 0.836913
2017-12-10T14:54:22.103321: step 1637, loss 0.317999, acc 0.875, prec 0.0442899, recall 0.837022
2017-12-10T14:54:22.298433: step 1638, loss 0.970323, acc 0.90625, prec 0.0443144, recall 0.837131
2017-12-10T14:54:22.495209: step 1639, loss 0.657278, acc 0.796875, prec 0.0442939, recall 0.837131
2017-12-10T14:54:22.690881: step 1640, loss 0.348864, acc 0.828125, prec 0.0442767, recall 0.837131
2017-12-10T14:54:22.880572: step 1641, loss 0.708823, acc 0.828125, prec 0.0443271, recall 0.837349
2017-12-10T14:54:23.074525: step 1642, loss 0.523499, acc 0.890625, prec 0.04435, recall 0.837458
2017-12-10T14:54:23.268263: step 1643, loss 0.514132, acc 0.828125, prec 0.0443327, recall 0.837458
2017-12-10T14:54:23.461125: step 1644, loss 0.625299, acc 0.71875, prec 0.0443045, recall 0.837458
2017-12-10T14:54:23.656098: step 1645, loss 0.595186, acc 0.84375, prec 0.0442888, recall 0.837458
2017-12-10T14:54:23.847778: step 1646, loss 0.35492, acc 0.890625, prec 0.0443116, recall 0.837567
2017-12-10T14:54:24.043466: step 1647, loss 0.685595, acc 0.890625, prec 0.0443345, recall 0.837675
2017-12-10T14:54:24.237104: step 1648, loss 0.694596, acc 0.8125, prec 0.0443494, recall 0.837784
2017-12-10T14:54:24.431681: step 1649, loss 0.29574, acc 0.875, prec 0.0443369, recall 0.837784
2017-12-10T14:54:24.622544: step 1650, loss 0.332412, acc 0.875, prec 0.0443581, recall 0.837892
2017-12-10T14:54:24.814234: step 1651, loss 0.435518, acc 0.828125, prec 0.0443409, recall 0.837892
2017-12-10T14:54:25.008853: step 1652, loss 0.58551, acc 0.796875, prec 0.0443205, recall 0.837892
2017-12-10T14:54:25.203796: step 1653, loss 0.402641, acc 0.90625, prec 0.0443786, recall 0.838108
2017-12-10T14:54:25.394065: step 1654, loss 0.313643, acc 0.84375, prec 0.0443629, recall 0.838108
2017-12-10T14:54:25.584216: step 1655, loss 0.352705, acc 0.859375, prec 0.0443826, recall 0.838216
2017-12-10T14:54:25.771926: step 1656, loss 0.353963, acc 0.875, prec 0.04437, recall 0.838216
2017-12-10T14:54:25.964286: step 1657, loss 0.15764, acc 0.921875, prec 0.0443959, recall 0.838323
2017-12-10T14:54:26.160480: step 1658, loss 0.44018, acc 0.859375, prec 0.0443818, recall 0.838323
2017-12-10T14:54:26.355722: step 1659, loss 1.2369, acc 0.90625, prec 0.0444061, recall 0.838431
2017-12-10T14:54:26.547130: step 1660, loss 0.259217, acc 0.890625, prec 0.0443952, recall 0.838431
2017-12-10T14:54:26.742085: step 1661, loss 0.951624, acc 0.90625, prec 0.044453, recall 0.838645
2017-12-10T14:54:26.934648: step 1662, loss 0.430295, acc 0.859375, prec 0.044439, recall 0.838645
2017-12-10T14:54:27.127647: step 1663, loss 0.621653, acc 0.90625, prec 0.0444968, recall 0.838859
2017-12-10T14:54:27.319915: step 1664, loss 0.213623, acc 0.921875, prec 0.044489, recall 0.838859
2017-12-10T14:54:27.512100: step 1665, loss 0.54912, acc 0.875, prec 0.0445437, recall 0.839073
2017-12-10T14:54:27.707369: step 1666, loss 0.208697, acc 0.890625, prec 0.0445327, recall 0.839073
2017-12-10T14:54:27.897613: step 1667, loss 0.33215, acc 0.84375, prec 0.0445506, recall 0.839179
2017-12-10T14:54:28.091524: step 1668, loss 0.223054, acc 0.9375, prec 0.0445444, recall 0.839179
2017-12-10T14:54:28.282072: step 1669, loss 0.307081, acc 0.890625, prec 0.0445334, recall 0.839179
2017-12-10T14:54:28.476569: step 1670, loss 0.446015, acc 0.8125, prec 0.0445147, recall 0.839179
2017-12-10T14:54:28.669551: step 1671, loss 0.361276, acc 0.90625, prec 0.0445388, recall 0.839286
2017-12-10T14:54:28.862716: step 1672, loss 2.11612, acc 0.90625, prec 0.0445645, recall 0.838838
2017-12-10T14:54:29.067750: step 1673, loss 0.21195, acc 0.921875, prec 0.0446573, recall 0.839156
2017-12-10T14:54:29.264412: step 1674, loss 0.329001, acc 0.875, prec 0.0446447, recall 0.839156
2017-12-10T14:54:29.456042: step 1675, loss 0.557968, acc 0.828125, prec 0.0446945, recall 0.839368
2017-12-10T14:54:29.647211: step 1676, loss 0.178316, acc 0.921875, prec 0.0446867, recall 0.839368
2017-12-10T14:54:29.845974: step 1677, loss 0.268371, acc 0.90625, prec 0.0447442, recall 0.839579
2017-12-10T14:54:30.038082: step 1678, loss 1.03499, acc 0.890625, prec 0.0447667, recall 0.839685
2017-12-10T14:54:30.233197: step 1679, loss 0.445321, acc 0.8125, prec 0.0448148, recall 0.839895
2017-12-10T14:54:30.425955: step 1680, loss 0.663786, acc 0.875, prec 0.0448691, recall 0.840105
2017-12-10T14:54:30.621485: step 1681, loss 0.337112, acc 0.890625, prec 0.0448581, recall 0.840105
2017-12-10T14:54:30.816381: step 1682, loss 1.93382, acc 0.859375, prec 0.044879, recall 0.83966
2017-12-10T14:54:31.010835: step 1683, loss 0.388817, acc 0.90625, prec 0.0449698, recall 0.839974
2017-12-10T14:54:31.205867: step 1684, loss 0.568379, acc 0.828125, prec 0.0449858, recall 0.840078
2017-12-10T14:54:31.403871: step 1685, loss 0.209094, acc 0.90625, prec 0.0450432, recall 0.840287
2017-12-10T14:54:31.595515: step 1686, loss 0.370514, acc 0.859375, prec 0.0450624, recall 0.840391
2017-12-10T14:54:31.785236: step 1687, loss 0.407848, acc 0.875, prec 0.0450498, recall 0.840391
2017-12-10T14:54:31.974420: step 1688, loss 4.6522, acc 0.90625, prec 0.0451086, recall 0.840052
2017-12-10T14:54:32.168820: step 1689, loss 0.70365, acc 0.8125, prec 0.0450897, recall 0.840052
2017-12-10T14:54:32.358822: step 1690, loss 0.533226, acc 0.78125, prec 0.045101, recall 0.840156
2017-12-10T14:54:32.547656: step 1691, loss 0.573547, acc 0.875, prec 0.0450884, recall 0.840156
2017-12-10T14:54:32.736001: step 1692, loss 0.50794, acc 0.828125, prec 0.0450711, recall 0.840156
2017-12-10T14:54:32.925327: step 1693, loss 0.474901, acc 0.828125, prec 0.0450538, recall 0.840156
2017-12-10T14:54:33.118233: step 1694, loss 0.541805, acc 0.859375, prec 0.0450397, recall 0.840156
2017-12-10T14:54:33.310309: step 1695, loss 0.397353, acc 0.890625, prec 0.0451285, recall 0.840467
2017-12-10T14:54:33.501797: step 1696, loss 0.484521, acc 0.8125, prec 0.0451096, recall 0.840467
2017-12-10T14:54:33.694593: step 1697, loss 0.677921, acc 0.828125, prec 0.0450924, recall 0.840467
2017-12-10T14:54:33.881912: step 1698, loss 0.306226, acc 0.921875, prec 0.0450845, recall 0.840467
2017-12-10T14:54:34.079058: step 1699, loss 0.466701, acc 0.828125, prec 0.0450673, recall 0.840467
2017-12-10T14:54:34.269946: step 1700, loss 0.628628, acc 0.8125, prec 0.0450817, recall 0.84057
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_0/1512935327/checkpoints/model-1700

2017-12-10T14:54:35.790827: step 1701, loss 0.505176, acc 0.828125, prec 0.0450645, recall 0.84057
2017-12-10T14:54:35.979421: step 1702, loss 0.401861, acc 0.90625, prec 0.0450882, recall 0.840674
2017-12-10T14:54:36.171975: step 1703, loss 0.259031, acc 0.890625, prec 0.0451104, recall 0.840777
2017-12-10T14:54:36.364670: step 1704, loss 0.983773, acc 0.84375, prec 0.0451942, recall 0.841085
2017-12-10T14:54:36.556663: step 1705, loss 0.242245, acc 0.921875, prec 0.0452858, recall 0.841393
2017-12-10T14:54:36.752863: step 1706, loss 0.200289, acc 0.921875, prec 0.0452779, recall 0.841393
2017-12-10T14:54:36.945337: step 1707, loss 0.209642, acc 0.9375, prec 0.0452716, recall 0.841393
2017-12-10T14:54:37.138619: step 1708, loss 0.382856, acc 0.828125, prec 0.0452875, recall 0.841495
2017-12-10T14:54:37.330290: step 1709, loss 0.119551, acc 0.953125, prec 0.0452828, recall 0.841495
2017-12-10T14:54:37.518941: step 1710, loss 0.16093, acc 0.90625, prec 0.0452733, recall 0.841495
2017-12-10T14:54:37.711943: step 1711, loss 0.127667, acc 0.9375, prec 0.0452671, recall 0.841495
2017-12-10T14:54:37.904065: step 1712, loss 0.334804, acc 0.921875, prec 0.0452923, recall 0.841597
2017-12-10T14:54:38.096139: step 1713, loss 0.109727, acc 0.953125, prec 0.0452876, recall 0.841597
2017-12-10T14:54:38.287353: step 1714, loss 0.161628, acc 0.953125, prec 0.0452829, recall 0.841597
2017-12-10T14:54:38.485961: step 1715, loss 0.736598, acc 0.984375, prec 0.0453144, recall 0.841699
2017-12-10T14:54:38.681242: step 1716, loss 0.100163, acc 0.984375, prec 0.0453128, recall 0.841699
2017-12-10T14:54:38.876096: step 1717, loss 0.262155, acc 0.9375, prec 0.0453396, recall 0.841801
2017-12-10T14:54:39.073232: step 1718, loss 0.156635, acc 0.953125, prec 0.045401, recall 0.842004
2017-12-10T14:54:39.262693: step 1719, loss 0.38704, acc 0.953125, prec 0.0454294, recall 0.842105
2017-12-10T14:54:39.457005: step 1720, loss 0.0742672, acc 0.96875, prec 0.0454262, recall 0.842105
2017-12-10T14:54:39.646495: step 1721, loss 2.17257, acc 0.953125, prec 0.0454561, recall 0.841667
2017-12-10T14:54:39.844382: step 1722, loss 0.188597, acc 0.953125, prec 0.0454844, recall 0.841768
2017-12-10T14:54:40.033981: step 1723, loss 0.435635, acc 0.9375, prec 0.0455112, recall 0.841869
2017-12-10T14:54:40.223962: step 1724, loss 0.43772, acc 0.890625, prec 0.0455332, recall 0.841971
2017-12-10T14:54:40.419851: step 1725, loss 0.280736, acc 0.875, prec 0.0455536, recall 0.842072
2017-12-10T14:54:40.612168: step 1726, loss 0.310608, acc 0.90625, prec 0.0455441, recall 0.842072
2017-12-10T14:54:40.799450: step 1727, loss 0.311105, acc 0.859375, prec 0.04553, recall 0.842072
2017-12-10T14:54:40.991896: step 1728, loss 0.431931, acc 0.828125, prec 0.0455456, recall 0.842173
2017-12-10T14:54:41.179491: step 1729, loss 0.904887, acc 0.8125, prec 0.0455927, recall 0.842374
2017-12-10T14:54:41.369602: step 1730, loss 0.436142, acc 0.875, prec 0.0456131, recall 0.842474
2017-12-10T14:54:41.561581: step 1731, loss 0.332844, acc 0.890625, prec 0.045602, recall 0.842474
2017-12-10T14:54:41.750883: step 1732, loss 0.224389, acc 0.90625, prec 0.0456255, recall 0.842575
2017-12-10T14:54:41.940554: step 1733, loss 0.492678, acc 0.890625, prec 0.0456804, recall 0.842775
2017-12-10T14:54:42.131910: step 1734, loss 0.241998, acc 0.90625, prec 0.0457038, recall 0.842875
2017-12-10T14:54:42.320882: step 1735, loss 0.481345, acc 0.9375, prec 0.0457304, recall 0.842975
2017-12-10T14:54:42.516182: step 1736, loss 2.06547, acc 0.921875, prec 0.0457899, recall 0.84264
2017-12-10T14:54:42.707224: step 1737, loss 0.254586, acc 0.890625, prec 0.0457789, recall 0.84264
2017-12-10T14:54:42.898037: step 1738, loss 0.125566, acc 0.9375, prec 0.0457726, recall 0.84264
2017-12-10T14:54:43.087814: step 1739, loss 0.886318, acc 0.90625, prec 0.0458946, recall 0.843038
2017-12-10T14:54:43.282091: step 1740, loss 0.162115, acc 0.984375, prec 0.0458931, recall 0.843038
2017-12-10T14:54:43.474072: step 1741, loss 0.653577, acc 0.9375, prec 0.0459196, recall 0.843137
2017-12-10T14:54:43.672306: step 1742, loss 0.454408, acc 0.75, prec 0.0459271, recall 0.843236
2017-12-10T14:54:43.860975: step 1743, loss 0.698312, acc 0.875, prec 0.0459802, recall 0.843434
2017-12-10T14:54:44.057105: step 1744, loss 0.393204, acc 0.890625, prec 0.0460347, recall 0.843632
2017-12-10T14:54:44.257843: step 1745, loss 0.487336, acc 0.796875, prec 0.046047, recall 0.84373
2017-12-10T14:54:44.450275: step 1746, loss 0.231823, acc 0.890625, prec 0.0460687, recall 0.843829
2017-12-10T14:54:44.638997: step 1747, loss 0.725062, acc 0.734375, prec 0.0460418, recall 0.843829
2017-12-10T14:54:44.829292: step 1748, loss 0.565271, acc 0.84375, prec 0.046026, recall 0.843829
2017-12-10T14:54:45.026734: step 1749, loss 0.341261, acc 0.890625, prec 0.0460149, recall 0.843829
2017-12-10T14:54:45.221407: step 1750, loss 0.351373, acc 0.90625, prec 0.0460382, recall 0.843927
2017-12-10T14:54:45.416262: step 1751, loss 0.285726, acc 0.890625, prec 0.0460599, recall 0.844025
2017-12-10T14:54:45.613310: step 1752, loss 0.471105, acc 0.875, prec 0.0460799, recall 0.844123
2017-12-10T14:54:45.808077: step 1753, loss 0.290109, acc 0.90625, prec 0.0461032, recall 0.844221
2017-12-10T14:54:45.998185: step 1754, loss 0.402432, acc 0.84375, prec 0.0460874, recall 0.844221
2017-12-10T14:54:46.192796: step 1755, loss 0.181166, acc 0.921875, prec 0.0460795, recall 0.844221
2017-12-10T14:54:46.383527: step 1756, loss 0.542198, acc 0.90625, prec 0.0461027, recall 0.844319
2017-12-10T14:54:46.577786: step 1757, loss 1.41192, acc 0.921875, prec 0.0461291, recall 0.843887
2017-12-10T14:54:46.775621: step 1758, loss 3.06027, acc 0.8125, prec 0.046177, recall 0.843554
2017-12-10T14:54:46.978041: step 1759, loss 0.400598, acc 0.890625, prec 0.046166, recall 0.843554
2017-12-10T14:54:47.174833: step 1760, loss 1.31169, acc 0.9375, prec 0.0461612, recall 0.843027
2017-12-10T14:54:47.369454: step 1761, loss 0.274652, acc 0.890625, prec 0.0461502, recall 0.843027
2017-12-10T14:54:47.560053: step 1762, loss 2.52585, acc 0.796875, prec 0.0461312, recall 0.8425
2017-12-10T14:54:47.755635: step 1763, loss 0.888939, acc 0.75, prec 0.0461386, recall 0.842598
2017-12-10T14:54:47.949737: step 1764, loss 0.784775, acc 0.78125, prec 0.0462143, recall 0.842893
2017-12-10T14:54:48.140508: step 1765, loss 0.960064, acc 0.734375, prec 0.0461875, recall 0.842893
2017-12-10T14:54:48.331455: step 1766, loss 0.549711, acc 0.84375, prec 0.0462043, recall 0.842991
2017-12-10T14:54:48.530704: step 1767, loss 0.687405, acc 0.78125, prec 0.0462148, recall 0.843088
2017-12-10T14:54:48.728177: step 1768, loss 0.563583, acc 0.75, prec 0.0461895, recall 0.843088
2017-12-10T14:54:48.922154: step 1769, loss 0.661754, acc 0.734375, prec 0.0461628, recall 0.843088
2017-12-10T14:54:49.114305: step 1770, loss 0.698095, acc 0.6875, prec 0.0461963, recall 0.843284
2017-12-10T14:54:49.310157: step 1771, loss 0.505371, acc 0.8125, prec 0.0462099, recall 0.843381
2017-12-10T14:54:49.505182: step 1772, loss 0.717578, acc 0.765625, prec 0.0461863, recall 0.843381
2017-12-10T14:54:49.696568: step 1773, loss 0.434842, acc 0.859375, prec 0.0462371, recall 0.843575
2017-12-10T14:54:49.887510: step 1774, loss 0.358083, acc 0.890625, prec 0.0462585, recall 0.843672
2017-12-10T14:54:50.077457: step 1775, loss 0.364824, acc 0.828125, prec 0.0463061, recall 0.843866
2017-12-10T14:54:50.267007: step 1776, loss 0.490964, acc 0.875, prec 0.0463259, recall 0.843963
2017-12-10T14:54:50.464130: step 1777, loss 5.17237, acc 0.859375, prec 0.0463133, recall 0.843441
2017-12-10T14:54:50.661696: step 1778, loss 1.79492, acc 0.875, prec 0.0463023, recall 0.842919
2017-12-10T14:54:50.854972: step 1779, loss 0.41989, acc 0.765625, prec 0.0463111, recall 0.843016
2017-12-10T14:54:51.047958: step 1780, loss 1.84302, acc 0.765625, prec 0.0462891, recall 0.842495
2017-12-10T14:54:51.245267: step 1781, loss 0.583549, acc 0.8125, prec 0.0463026, recall 0.842593
2017-12-10T14:54:51.434964: step 1782, loss 0.72974, acc 0.71875, prec 0.0463067, recall 0.84269
2017-12-10T14:54:51.624827: step 1783, loss 0.768228, acc 0.78125, prec 0.0462847, recall 0.84269
2017-12-10T14:54:51.814826: step 1784, loss 0.476835, acc 0.796875, prec 0.0462966, recall 0.842787
2017-12-10T14:54:52.011953: step 1785, loss 0.434453, acc 0.796875, prec 0.0463085, recall 0.842884
2017-12-10T14:54:52.207716: step 1786, loss 0.804516, acc 0.671875, prec 0.0462756, recall 0.842884
2017-12-10T14:54:52.404253: step 1787, loss 0.393923, acc 0.859375, prec 0.0462938, recall 0.84298
2017-12-10T14:54:52.599473: step 1788, loss 0.519965, acc 0.78125, prec 0.0463041, recall 0.843077
2017-12-10T14:54:52.789105: step 1789, loss 0.460719, acc 0.8125, prec 0.0463498, recall 0.84327
2017-12-10T14:54:52.981526: step 1790, loss 0.447677, acc 0.796875, prec 0.0463294, recall 0.84327
2017-12-10T14:54:53.171801: step 1791, loss 0.470892, acc 0.84375, prec 0.0463138, recall 0.84327
2017-12-10T14:54:53.361504: step 1792, loss 0.456786, acc 0.828125, prec 0.0463288, recall 0.843366
2017-12-10T14:54:53.557468: step 1793, loss 0.408365, acc 0.859375, prec 0.0463791, recall 0.843558
2017-12-10T14:54:53.752287: step 1794, loss 0.715327, acc 0.90625, prec 0.046434, recall 0.84375
2017-12-10T14:54:53.945471: step 1795, loss 0.387784, acc 0.796875, prec 0.0464458, recall 0.843846
2017-12-10T14:54:54.133449: step 1796, loss 0.475892, acc 0.828125, prec 0.0464286, recall 0.843846
2017-12-10T14:54:54.327483: step 1797, loss 0.434713, acc 0.90625, prec 0.0464513, recall 0.843941
2017-12-10T14:54:54.520256: step 1798, loss 0.0999255, acc 0.96875, prec 0.0464803, recall 0.844037
2017-12-10T14:54:54.711409: step 1799, loss 0.359441, acc 0.953125, prec 0.0465077, recall 0.844132
2017-12-10T14:54:54.901066: step 1800, loss 0.0772259, acc 0.96875, prec 0.0465046, recall 0.844132
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_0/1512935327/checkpoints/model-1800

2017-12-10T14:54:56.180571: step 1801, loss 0.181626, acc 0.921875, prec 0.0464968, recall 0.844132
2017-12-10T14:54:56.371417: step 1802, loss 0.888633, acc 0.9375, prec 0.046651, recall 0.844607
2017-12-10T14:54:56.569093: step 1803, loss 0.121505, acc 0.984375, prec 0.0466494, recall 0.844607
2017-12-10T14:54:56.760568: step 1804, loss 0.173301, acc 0.953125, prec 0.0466768, recall 0.844702
2017-12-10T14:54:56.962661: step 1805, loss 1.80501, acc 0.921875, prec 0.0467026, recall 0.844282
2017-12-10T14:54:57.162741: step 1806, loss 0.327116, acc 0.921875, prec 0.0467268, recall 0.844377
2017-12-10T14:54:57.351995: step 1807, loss 0.238996, acc 0.9375, prec 0.0467525, recall 0.844471
2017-12-10T14:54:57.545851: step 1808, loss 3.34729, acc 0.890625, prec 0.0467447, recall 0.843447
2017-12-10T14:54:57.741333: step 1809, loss 0.559958, acc 0.890625, prec 0.0468298, recall 0.843731
2017-12-10T14:54:57.932491: step 1810, loss 0.238591, acc 0.890625, prec 0.0468188, recall 0.843731
2017-12-10T14:54:58.122907: step 1811, loss 0.97175, acc 0.796875, prec 0.0468624, recall 0.84392
2017-12-10T14:54:58.315423: step 1812, loss 0.538395, acc 0.875, prec 0.0468498, recall 0.84392
2017-12-10T14:54:58.508814: step 1813, loss 0.474558, acc 0.84375, prec 0.0468661, recall 0.844015
2017-12-10T14:54:58.698970: step 1814, loss 0.574814, acc 0.734375, prec 0.0468393, recall 0.844015
2017-12-10T14:54:58.888258: step 1815, loss 0.331035, acc 0.890625, prec 0.0468284, recall 0.844015
2017-12-10T14:54:59.085246: step 1816, loss 1.25962, acc 0.734375, prec 0.0469934, recall 0.844578
2017-12-10T14:54:59.288655: step 1817, loss 0.6023, acc 0.765625, prec 0.0470336, recall 0.844765
2017-12-10T14:54:59.477594: step 1818, loss 0.649455, acc 0.734375, prec 0.0470068, recall 0.844765
2017-12-10T14:54:59.667148: step 1819, loss 0.460258, acc 0.828125, prec 0.0470214, recall 0.844859
2017-12-10T14:54:59.853720: step 1820, loss 0.679779, acc 0.78125, prec 0.0469994, recall 0.844859
2017-12-10T14:55:00.045795: step 1821, loss 0.846951, acc 0.734375, prec 0.0470364, recall 0.845045
2017-12-10T14:55:00.238249: step 1822, loss 0.621385, acc 0.765625, prec 0.0470765, recall 0.845231
2017-12-10T14:55:00.430967: step 1823, loss 0.39713, acc 0.875, prec 0.0470639, recall 0.845231
2017-12-10T14:55:00.620233: step 1824, loss 0.409201, acc 0.859375, prec 0.0471134, recall 0.845416
2017-12-10T14:55:00.811534: step 1825, loss 0.389421, acc 0.890625, prec 0.0471024, recall 0.845416
2017-12-10T14:55:01.001446: step 1826, loss 0.414493, acc 0.78125, prec 0.0471122, recall 0.845509
2017-12-10T14:55:01.197464: step 1827, loss 0.430811, acc 0.84375, prec 0.0470965, recall 0.845509
2017-12-10T14:55:01.388914: step 1828, loss 0.487636, acc 0.78125, prec 0.0470745, recall 0.845509
2017-12-10T14:55:01.588568: step 1829, loss 0.118252, acc 0.96875, prec 0.0470714, recall 0.845509
2017-12-10T14:55:01.780207: step 1830, loss 0.432882, acc 0.875, prec 0.0470588, recall 0.845509
2017-12-10T14:55:01.974522: step 1831, loss 0.531277, acc 0.9375, prec 0.0471478, recall 0.845786
2017-12-10T14:55:02.169570: step 1832, loss 0.180377, acc 0.953125, prec 0.0471748, recall 0.845878
2017-12-10T14:55:02.357578: step 1833, loss 0.133461, acc 0.953125, prec 0.0471701, recall 0.845878
2017-12-10T14:55:02.549264: step 1834, loss 0.237315, acc 0.96875, prec 0.0471987, recall 0.84597
2017-12-10T14:55:02.747247: step 1835, loss 1.97314, acc 0.96875, prec 0.0472606, recall 0.84565
2017-12-10T14:55:02.946717: step 1836, loss 0.136871, acc 0.9375, prec 0.0472543, recall 0.84565
2017-12-10T14:55:03.142352: step 1837, loss 0.305702, acc 0.921875, prec 0.0472465, recall 0.84565
2017-12-10T14:55:03.337417: step 1838, loss 0.547657, acc 0.90625, prec 0.0473322, recall 0.845925
2017-12-10T14:55:03.531415: step 1839, loss 0.275686, acc 0.90625, prec 0.0473227, recall 0.845925
2017-12-10T14:55:03.723530: step 1840, loss 1.8882, acc 0.9375, prec 0.0473497, recall 0.845514
2017-12-10T14:55:03.914188: step 1841, loss 0.141351, acc 0.953125, prec 0.0474083, recall 0.845697
2017-12-10T14:55:04.107146: step 1842, loss 0.371296, acc 0.90625, prec 0.0474306, recall 0.845789
2017-12-10T14:55:04.300479: step 1843, loss 0.310773, acc 0.875, prec 0.0474179, recall 0.845789
2017-12-10T14:55:04.491526: step 1844, loss 0.307519, acc 0.859375, prec 0.0474354, recall 0.84588
2017-12-10T14:55:04.684146: step 1845, loss 0.529345, acc 0.9375, prec 0.0474608, recall 0.845972
2017-12-10T14:55:04.879769: step 1846, loss 0.152257, acc 0.953125, prec 0.0474877, recall 0.846063
2017-12-10T14:55:05.071305: step 1847, loss 0.410961, acc 0.8125, prec 0.0474688, recall 0.846063
2017-12-10T14:55:05.267297: step 1848, loss 0.254495, acc 0.890625, prec 0.0474577, recall 0.846063
2017-12-10T14:55:05.458854: step 1849, loss 0.19377, acc 0.953125, prec 0.0474846, recall 0.846154
2017-12-10T14:55:05.650497: step 1850, loss 0.438624, acc 0.875, prec 0.0475353, recall 0.846336
2017-12-10T14:55:05.844697: step 1851, loss 0.865117, acc 0.90625, prec 0.0475574, recall 0.846426
2017-12-10T14:55:06.037128: step 1852, loss 0.417982, acc 0.90625, prec 0.0475479, recall 0.846426
2017-12-10T14:55:06.233025: step 1853, loss 0.656426, acc 0.875, prec 0.0475985, recall 0.846608
2017-12-10T14:55:06.425224: step 1854, loss 0.292628, acc 0.90625, prec 0.047589, recall 0.846608
2017-12-10T14:55:06.622740: step 1855, loss 0.415911, acc 0.859375, prec 0.047638, recall 0.846788
2017-12-10T14:55:06.812652: step 1856, loss 0.970664, acc 0.859375, prec 0.0476869, recall 0.846969
2017-12-10T14:55:07.003616: step 1857, loss 0.355127, acc 0.84375, prec 0.0476711, recall 0.846969
2017-12-10T14:55:07.200757: step 1858, loss 0.133161, acc 0.96875, prec 0.0476679, recall 0.846969
2017-12-10T14:55:07.394963: step 1859, loss 0.329771, acc 0.875, prec 0.0476553, recall 0.846969
2017-12-10T14:55:07.588932: step 1860, loss 1.76064, acc 0.84375, prec 0.0476726, recall 0.846561
2017-12-10T14:55:07.783528: step 1861, loss 0.398383, acc 0.84375, prec 0.0476569, recall 0.846561
2017-12-10T14:55:07.975979: step 1862, loss 0.312119, acc 0.890625, prec 0.0476458, recall 0.846561
2017-12-10T14:55:08.167052: step 1863, loss 0.266581, acc 0.890625, prec 0.0476348, recall 0.846561
2017-12-10T14:55:08.358545: step 1864, loss 0.421005, acc 0.90625, prec 0.0476253, recall 0.846561
2017-12-10T14:55:08.549695: step 1865, loss 0.429616, acc 0.859375, prec 0.0476427, recall 0.846651
2017-12-10T14:55:08.740395: step 1866, loss 0.383597, acc 0.921875, prec 0.0476977, recall 0.846831
2017-12-10T14:55:08.934398: step 1867, loss 0.322393, acc 0.859375, prec 0.047715, recall 0.846921
2017-12-10T14:55:09.129432: step 1868, loss 0.344238, acc 0.90625, prec 0.0477056, recall 0.846921
2017-12-10T14:55:09.322902: step 1869, loss 0.940109, acc 0.921875, prec 0.0477291, recall 0.847011
2017-12-10T14:55:09.514498: step 1870, loss 0.45087, acc 0.828125, prec 0.0477433, recall 0.8471
2017-12-10T14:55:09.706642: step 1871, loss 0.555347, acc 0.875, prec 0.0477621, recall 0.84719
2017-12-10T14:55:09.907419: step 1872, loss 0.309942, acc 0.84375, prec 0.0477463, recall 0.84719
2017-12-10T14:55:10.098335: step 1873, loss 0.408304, acc 0.84375, prec 0.0477306, recall 0.84719
2017-12-10T14:55:10.291317: step 1874, loss 0.306155, acc 0.859375, prec 0.0477478, recall 0.847279
2017-12-10T14:55:10.485710: step 1875, loss 0.248151, acc 0.90625, prec 0.0477384, recall 0.847279
2017-12-10T14:55:10.678176: step 1876, loss 0.385189, acc 0.890625, prec 0.0477587, recall 0.847368
2017-12-10T14:55:10.870268: step 1877, loss 0.352895, acc 0.890625, prec 0.0478105, recall 0.847547
2017-12-10T14:55:11.063281: step 1878, loss 0.283472, acc 0.84375, prec 0.0477947, recall 0.847547
2017-12-10T14:55:11.255548: step 1879, loss 2.13467, acc 0.9375, prec 0.0478214, recall 0.847141
2017-12-10T14:55:11.448104: step 1880, loss 0.401863, acc 0.875, prec 0.0478088, recall 0.847141
2017-12-10T14:55:11.636896: step 1881, loss 0.396193, acc 0.859375, prec 0.0478259, recall 0.84723
2017-12-10T14:55:11.827885: step 1882, loss 0.495773, acc 0.875, prec 0.0478447, recall 0.847319
2017-12-10T14:55:12.022054: step 1883, loss 0.240765, acc 0.921875, prec 0.0478368, recall 0.847319
2017-12-10T14:55:12.218653: step 1884, loss 0.18697, acc 0.96875, prec 0.0478963, recall 0.847497
2017-12-10T14:55:12.410448: step 1885, loss 0.443515, acc 0.859375, prec 0.0479761, recall 0.847763
2017-12-10T14:55:12.602659: step 1886, loss 0.203594, acc 0.90625, prec 0.0479666, recall 0.847763
2017-12-10T14:55:12.797194: step 1887, loss 0.371333, acc 0.8125, prec 0.0479477, recall 0.847763
2017-12-10T14:55:12.989826: step 1888, loss 2.2559, acc 0.90625, prec 0.0479711, recall 0.847359
2017-12-10T14:55:13.182401: step 1889, loss 0.778627, acc 0.90625, prec 0.0479929, recall 0.847448
2017-12-10T14:55:13.384257: step 1890, loss 0.33296, acc 0.875, prec 0.0480116, recall 0.847536
2017-12-10T14:55:13.577475: step 1891, loss 0.231285, acc 0.921875, prec 0.0480037, recall 0.847536
2017-12-10T14:55:13.768657: step 1892, loss 0.586536, acc 0.890625, prec 0.0480864, recall 0.847801
2017-12-10T14:55:13.961562: step 1893, loss 0.276439, acc 0.875, prec 0.048105, recall 0.847889
2017-12-10T14:55:14.161283: step 1894, loss 0.273412, acc 0.890625, prec 0.048094, recall 0.847889
2017-12-10T14:55:14.353307: step 1895, loss 0.437908, acc 0.875, prec 0.0481126, recall 0.847977
2017-12-10T14:55:14.546817: step 1896, loss 0.531117, acc 0.859375, prec 0.0481296, recall 0.848065
2017-12-10T14:55:14.739617: step 1897, loss 0.514196, acc 0.828125, prec 0.0481122, recall 0.848065
2017-12-10T14:55:14.932697: step 1898, loss 0.303195, acc 0.90625, prec 0.048134, recall 0.848152
2017-12-10T14:55:15.127492: step 1899, loss 0.332966, acc 0.859375, prec 0.0481821, recall 0.848328
2017-12-10T14:55:15.317213: step 1900, loss 0.421531, acc 0.859375, prec 0.0481991, recall 0.848415
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_0/1512935327/checkpoints/model-1900

2017-12-10T14:55:16.556652: step 1901, loss 0.351569, acc 0.90625, prec 0.0482208, recall 0.848502
2017-12-10T14:55:16.750962: step 1902, loss 0.219504, acc 0.90625, prec 0.0482113, recall 0.848502
2017-12-10T14:55:16.949690: step 1903, loss 1.26304, acc 0.859375, prec 0.0482594, recall 0.848677
2017-12-10T14:55:17.142581: step 1904, loss 0.398399, acc 0.890625, prec 0.0482795, recall 0.848764
2017-12-10T14:55:17.338318: step 1905, loss 0.281072, acc 0.875, prec 0.048298, recall 0.848851
2017-12-10T14:55:17.531010: step 1906, loss 0.287079, acc 0.890625, prec 0.0482869, recall 0.848851
2017-12-10T14:55:17.722303: step 1907, loss 0.592065, acc 0.890625, prec 0.048307, recall 0.848937
2017-12-10T14:55:17.914994: step 1908, loss 0.401643, acc 0.890625, prec 0.048327, recall 0.849024
2017-12-10T14:55:18.106379: step 1909, loss 0.436665, acc 0.84375, prec 0.0483112, recall 0.849024
2017-12-10T14:55:18.299522: step 1910, loss 0.214456, acc 0.921875, prec 0.0483033, recall 0.849024
2017-12-10T14:55:18.494353: step 1911, loss 0.335632, acc 0.875, prec 0.0482907, recall 0.849024
2017-12-10T14:55:18.687826: step 1912, loss 0.529261, acc 0.9375, prec 0.0483466, recall 0.849197
2017-12-10T14:55:18.880402: step 1913, loss 0.878488, acc 0.9375, prec 0.0484024, recall 0.84937
2017-12-10T14:55:19.071330: step 1914, loss 0.830588, acc 0.953125, prec 0.0484597, recall 0.849542
2017-12-10T14:55:19.265050: step 1915, loss 0.207451, acc 0.875, prec 0.0484471, recall 0.849542
2017-12-10T14:55:19.452792: step 1916, loss 2.47333, acc 0.859375, prec 0.0484655, recall 0.849143
2017-12-10T14:55:19.648087: step 1917, loss 0.264922, acc 0.875, prec 0.0484528, recall 0.849143
2017-12-10T14:55:19.838211: step 1918, loss 0.343166, acc 0.90625, prec 0.0484744, recall 0.849229
2017-12-10T14:55:20.032492: step 1919, loss 0.402865, acc 0.8125, prec 0.0484864, recall 0.849315
2017-12-10T14:55:20.220795: step 1920, loss 0.25323, acc 0.921875, prec 0.0484785, recall 0.849315
2017-12-10T14:55:20.415934: step 1921, loss 1.34081, acc 0.828125, prec 0.0485231, recall 0.849487
2017-12-10T14:55:20.608881: step 1922, loss 0.234574, acc 0.9375, prec 0.0485168, recall 0.849487
2017-12-10T14:55:20.799800: step 1923, loss 0.577747, acc 0.859375, prec 0.0485336, recall 0.849573
2017-12-10T14:55:20.989190: step 1924, loss 0.364517, acc 0.875, prec 0.0485209, recall 0.849573
2017-12-10T14:55:21.180890: step 1925, loss 0.434236, acc 0.875, prec 0.0485393, recall 0.849658
2017-12-10T14:55:21.376933: step 1926, loss 0.335662, acc 0.84375, prec 0.0485235, recall 0.849658
2017-12-10T14:55:21.575888: step 1927, loss 0.412788, acc 0.859375, prec 0.0485712, recall 0.849829
2017-12-10T14:55:21.767350: step 1928, loss 0.292077, acc 0.875, prec 0.0485585, recall 0.849829
2017-12-10T14:55:21.957427: step 1929, loss 0.299574, acc 0.875, prec 0.0485459, recall 0.849829
2017-12-10T14:55:22.150235: step 1930, loss 0.429019, acc 0.859375, prec 0.0485317, recall 0.849829
2017-12-10T14:55:22.341813: step 1931, loss 0.257291, acc 0.90625, prec 0.048584, recall 0.85
2017-12-10T14:55:22.531474: step 1932, loss 0.242262, acc 0.859375, prec 0.0485699, recall 0.85
2017-12-10T14:55:22.726502: step 1933, loss 0.920313, acc 0.9375, prec 0.0486253, recall 0.85017
2017-12-10T14:55:22.920484: step 1934, loss 0.423268, acc 0.890625, prec 0.048676, recall 0.85034
2017-12-10T14:55:23.116517: step 1935, loss 0.304056, acc 0.953125, prec 0.0487021, recall 0.850425
2017-12-10T14:55:23.311885: step 1936, loss 0.157353, acc 0.953125, prec 0.0486974, recall 0.850425
2017-12-10T14:55:23.506881: step 1937, loss 1.03035, acc 0.90625, prec 0.0487188, recall 0.85051
2017-12-10T14:55:23.700104: step 1938, loss 1.44505, acc 0.890625, prec 0.0488003, recall 0.850763
2017-12-10T14:55:23.892265: step 1939, loss 0.263639, acc 0.875, prec 0.0488184, recall 0.850847
2017-12-10T14:55:24.084412: step 1940, loss 0.297928, acc 0.875, prec 0.0488058, recall 0.850847
2017-12-10T14:55:24.275199: step 1941, loss 0.335545, acc 0.890625, prec 0.0487947, recall 0.850847
2017-12-10T14:55:24.471594: step 1942, loss 0.309939, acc 0.890625, prec 0.0487836, recall 0.850847
2017-12-10T14:55:24.664611: step 1943, loss 0.413564, acc 0.859375, prec 0.0488002, recall 0.850932
2017-12-10T14:55:24.860468: step 1944, loss 0.279016, acc 0.9375, prec 0.0488555, recall 0.8511
2017-12-10T14:55:25.052202: step 1945, loss 0.123197, acc 0.96875, prec 0.0488523, recall 0.8511
2017-12-10T14:55:25.243321: step 1946, loss 0.23895, acc 0.921875, prec 0.048906, recall 0.851268
2017-12-10T14:55:25.435718: step 1947, loss 0.252263, acc 0.921875, prec 0.0488981, recall 0.851268
2017-12-10T14:55:25.635347: step 1948, loss 1.9128, acc 0.875, prec 0.0490101, recall 0.851124
2017-12-10T14:55:25.829855: step 1949, loss 0.395173, acc 0.84375, prec 0.049025, recall 0.851207
2017-12-10T14:55:26.022017: step 1950, loss 0.215013, acc 0.90625, prec 0.049077, recall 0.851374
2017-12-10T14:55:26.217993: step 1951, loss 0.334978, acc 0.890625, prec 0.0490659, recall 0.851374
2017-12-10T14:55:26.418076: step 1952, loss 0.748335, acc 0.890625, prec 0.0491162, recall 0.851541
2017-12-10T14:55:26.610724: step 1953, loss 0.345027, acc 0.84375, prec 0.0491004, recall 0.851541
2017-12-10T14:55:26.802675: step 1954, loss 0.235092, acc 0.921875, prec 0.0491846, recall 0.85179
2017-12-10T14:55:26.995037: step 1955, loss 0.208919, acc 0.953125, prec 0.0492105, recall 0.851873
2017-12-10T14:55:27.192870: step 1956, loss 0.936287, acc 0.859375, prec 0.0492576, recall 0.852038
2017-12-10T14:55:27.388806: step 1957, loss 0.184577, acc 0.9375, prec 0.0492512, recall 0.852038
2017-12-10T14:55:27.582367: step 1958, loss 0.589187, acc 0.875, prec 0.0492692, recall 0.852121
2017-12-10T14:55:27.771695: step 1959, loss 0.472791, acc 0.84375, prec 0.049284, recall 0.852203
2017-12-10T14:55:27.965567: step 1960, loss 0.489437, acc 0.890625, prec 0.0493035, recall 0.852285
2017-12-10T14:55:28.157840: step 1961, loss 0.212759, acc 0.921875, prec 0.0492955, recall 0.852285
2017-12-10T14:55:28.349017: step 1962, loss 0.488132, acc 0.90625, prec 0.0493167, recall 0.852368
2017-12-10T14:55:28.542040: step 1963, loss 0.698999, acc 0.828125, prec 0.0493298, recall 0.85245
2017-12-10T14:55:28.740692: step 1964, loss 0.584261, acc 0.828125, prec 0.049343, recall 0.852532
2017-12-10T14:55:28.934081: step 1965, loss 0.259892, acc 0.90625, prec 0.049364, recall 0.852614
2017-12-10T14:55:29.133010: step 1966, loss 0.234893, acc 0.90625, prec 0.0493545, recall 0.852614
2017-12-10T14:55:29.326080: step 1967, loss 0.194501, acc 0.953125, prec 0.0493497, recall 0.852614
2017-12-10T14:55:29.520351: step 1968, loss 0.283509, acc 0.90625, prec 0.0493402, recall 0.852614
2017-12-10T14:55:29.712171: step 1969, loss 0.418702, acc 0.828125, prec 0.0493533, recall 0.852696
2017-12-10T14:55:29.905636: step 1970, loss 0.446883, acc 0.84375, prec 0.0493986, recall 0.852859
2017-12-10T14:55:30.099543: step 1971, loss 0.609891, acc 0.90625, prec 0.0494196, recall 0.852941
2017-12-10T14:55:30.292466: step 1972, loss 0.679197, acc 0.90625, prec 0.0494712, recall 0.853104
2017-12-10T14:55:30.487347: step 1973, loss 0.259221, acc 0.9375, prec 0.0494954, recall 0.853186
2017-12-10T14:55:30.681069: step 1974, loss 1.70585, acc 0.9375, prec 0.0494906, recall 0.852713
2017-12-10T14:55:30.874322: step 1975, loss 0.274061, acc 0.90625, prec 0.0495116, recall 0.852795
2017-12-10T14:55:31.067457: step 1976, loss 0.0978362, acc 0.984375, prec 0.04951, recall 0.852795
2017-12-10T14:55:31.266811: step 1977, loss 0.419516, acc 0.90625, prec 0.049531, recall 0.852876
2017-12-10T14:55:31.469269: step 1978, loss 0.0885176, acc 0.96875, prec 0.0495278, recall 0.852876
2017-12-10T14:55:31.661619: step 1979, loss 0.256657, acc 0.90625, prec 0.0495488, recall 0.852957
2017-12-10T14:55:31.859094: step 1980, loss 0.219873, acc 0.921875, prec 0.0495409, recall 0.852957
2017-12-10T14:55:32.053575: step 1981, loss 0.20043, acc 0.953125, prec 0.0495361, recall 0.852957
2017-12-10T14:55:32.247858: step 1982, loss 0.405782, acc 0.859375, prec 0.0495218, recall 0.852957
2017-12-10T14:55:32.440591: step 1983, loss 0.221465, acc 0.953125, prec 0.049578, recall 0.85312
2017-12-10T14:55:32.633898: step 1984, loss 0.17897, acc 0.921875, prec 0.0495701, recall 0.85312
2017-12-10T14:55:32.823643: step 1985, loss 0.303916, acc 0.9375, prec 0.0496247, recall 0.853282
2017-12-10T14:55:33.014253: step 1986, loss 1.15722, acc 0.9375, prec 0.0496488, recall 0.853363
2017-12-10T14:55:33.208880: step 1987, loss 0.693882, acc 0.90625, prec 0.0496697, recall 0.853444
2017-12-10T14:55:33.383152: step 1988, loss 0.339929, acc 0.901961, prec 0.0496922, recall 0.853524
2017-12-10T14:55:33.582519: step 1989, loss 0.471051, acc 0.9375, prec 0.0498077, recall 0.853846
2017-12-10T14:55:33.774524: step 1990, loss 0.328259, acc 0.890625, prec 0.0498574, recall 0.854007
2017-12-10T14:55:33.965289: step 1991, loss 0.162958, acc 0.953125, prec 0.0498831, recall 0.854087
2017-12-10T14:55:34.158339: step 1992, loss 0.340883, acc 0.875, prec 0.0498703, recall 0.854087
2017-12-10T14:55:34.353940: step 1993, loss 0.284716, acc 0.90625, prec 0.0499215, recall 0.854247
2017-12-10T14:55:34.545574: step 1994, loss 0.262973, acc 0.890625, prec 0.0499408, recall 0.854326
2017-12-10T14:55:34.738119: step 1995, loss 0.350427, acc 0.90625, prec 0.0499312, recall 0.854326
2017-12-10T14:55:34.929321: step 1996, loss 0.263941, acc 0.9375, prec 0.0499552, recall 0.854406
2017-12-10T14:55:35.120315: step 1997, loss 0.368227, acc 0.84375, prec 0.05, recall 0.854565
2017-12-10T14:55:35.315602: step 1998, loss 0.178642, acc 0.890625, prec 0.0500192, recall 0.854645
2017-12-10T14:55:35.507196: step 1999, loss 0.130225, acc 0.9375, prec 0.0500128, recall 0.854645
2017-12-10T14:55:35.697016: step 2000, loss 0.128579, acc 0.96875, prec 0.05004, recall 0.854724
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_0/1512935327/checkpoints/model-2000

2017-12-10T14:55:36.927245: step 2001, loss 0.301049, acc 0.84375, prec 0.050024, recall 0.854724
2017-12-10T14:55:37.121813: step 2002, loss 0.182438, acc 0.953125, prec 0.0501406, recall 0.855041
2017-12-10T14:55:37.315127: step 2003, loss 0.225394, acc 0.9375, prec 0.0501949, recall 0.855199
2017-12-10T14:55:37.507337: step 2004, loss 0.273879, acc 0.890625, prec 0.0502444, recall 0.855356
2017-12-10T14:55:37.699966: step 2005, loss 0.16666, acc 0.9375, prec 0.0502379, recall 0.855356
2017-12-10T14:55:37.895182: step 2006, loss 0.188822, acc 0.953125, prec 0.0502635, recall 0.855435
2017-12-10T14:55:38.087195: step 2007, loss 1.31485, acc 0.984375, prec 0.0502635, recall 0.85497
2017-12-10T14:55:38.285217: step 2008, loss 0.110545, acc 0.953125, prec 0.050289, recall 0.855049
2017-12-10T14:55:38.474747: step 2009, loss 0.264828, acc 0.90625, prec 0.0503096, recall 0.855128
2017-12-10T14:55:38.666963: step 2010, loss 0.15632, acc 0.9375, prec 0.0503032, recall 0.855128
2017-12-10T14:55:38.859013: step 2011, loss 0.265677, acc 0.90625, prec 0.0502936, recall 0.855128
2017-12-10T14:55:39.049318: step 2012, loss 0.323008, acc 0.90625, prec 0.0503143, recall 0.855206
2017-12-10T14:55:39.241357: step 2013, loss 0.30083, acc 0.875, prec 0.0503317, recall 0.855285
2017-12-10T14:55:39.431233: step 2014, loss 0.163825, acc 0.921875, prec 0.050354, recall 0.855363
2017-12-10T14:55:39.626974: step 2015, loss 1.92292, acc 0.953125, prec 0.0503508, recall 0.8549
2017-12-10T14:55:39.827028: step 2016, loss 0.151796, acc 0.9375, prec 0.0504049, recall 0.855057
2017-12-10T14:55:40.019522: step 2017, loss 0.695136, acc 0.921875, prec 0.0504271, recall 0.855135
2017-12-10T14:55:40.211158: step 2018, loss 0.274694, acc 0.890625, prec 0.0504159, recall 0.855135
2017-12-10T14:55:40.404018: step 2019, loss 0.384268, acc 0.875, prec 0.0504333, recall 0.855213
2017-12-10T14:55:40.592988: step 2020, loss 0.428912, acc 0.875, prec 0.0504507, recall 0.855292
2017-12-10T14:55:40.789681: step 2021, loss 0.451942, acc 0.875, prec 0.0504681, recall 0.85537
2017-12-10T14:55:40.978620: step 2022, loss 0.193536, acc 0.921875, prec 0.05046, recall 0.85537
2017-12-10T14:55:41.165776: step 2023, loss 0.176703, acc 0.953125, prec 0.0505157, recall 0.855526
2017-12-10T14:55:41.355599: step 2024, loss 0.182629, acc 0.921875, prec 0.0505076, recall 0.855526
2017-12-10T14:55:41.548682: step 2025, loss 0.237433, acc 0.921875, prec 0.0504996, recall 0.855526
2017-12-10T14:55:41.741113: step 2026, loss 0.277221, acc 0.921875, prec 0.0505218, recall 0.855603
2017-12-10T14:55:41.933177: step 2027, loss 0.353548, acc 0.90625, prec 0.0505725, recall 0.855759
2017-12-10T14:55:42.123463: step 2028, loss 0.296098, acc 0.890625, prec 0.0505915, recall 0.855836
2017-12-10T14:55:42.314219: step 2029, loss 0.345881, acc 0.875, prec 0.0506389, recall 0.855991
2017-12-10T14:55:42.503023: step 2030, loss 0.754873, acc 0.96875, prec 0.0506659, recall 0.856069
2017-12-10T14:55:42.696336: step 2031, loss 0.128786, acc 0.9375, prec 0.0506595, recall 0.856069
2017-12-10T14:55:42.885671: step 2032, loss 0.237027, acc 0.921875, prec 0.0506514, recall 0.856069
2017-12-10T14:55:43.078409: step 2033, loss 0.306205, acc 0.921875, prec 0.0506434, recall 0.856069
2017-12-10T14:55:43.267737: step 2034, loss 0.158272, acc 0.96875, prec 0.0506402, recall 0.856069
2017-12-10T14:55:43.456968: step 2035, loss 0.131899, acc 0.9375, prec 0.0506337, recall 0.856069
2017-12-10T14:55:43.649230: step 2036, loss 0.318604, acc 0.9375, prec 0.0506574, recall 0.856146
2017-12-10T14:55:43.843099: step 2037, loss 0.238034, acc 0.953125, prec 0.0506526, recall 0.856146
2017-12-10T14:55:44.043013: step 2038, loss 0.163273, acc 0.9375, prec 0.0506462, recall 0.856146
2017-12-10T14:55:44.244643: step 2039, loss 0.207306, acc 0.9375, prec 0.0506397, recall 0.856146
2017-12-10T14:55:44.435388: step 2040, loss 0.191373, acc 0.9375, prec 0.0506333, recall 0.856146
2017-12-10T14:55:44.626672: step 2041, loss 0.218923, acc 0.9375, prec 0.0506269, recall 0.856146
2017-12-10T14:55:44.819470: step 2042, loss 2.95336, acc 0.96875, prec 0.0506554, recall 0.855764
2017-12-10T14:55:45.016116: step 2043, loss 1.09886, acc 0.84375, prec 0.0506695, recall 0.855841
2017-12-10T14:55:45.210807: step 2044, loss 0.374767, acc 0.953125, prec 0.0507249, recall 0.855996
2017-12-10T14:55:45.412684: step 2045, loss 0.231098, acc 0.9375, prec 0.0507184, recall 0.855996
2017-12-10T14:55:45.609421: step 2046, loss 2.01278, acc 0.984375, prec 0.0507485, recall 0.855615
2017-12-10T14:55:45.809674: step 2047, loss 0.153636, acc 0.9375, prec 0.0507421, recall 0.855615
2017-12-10T14:55:46.003774: step 2048, loss 0.229123, acc 0.90625, prec 0.0507324, recall 0.855615
2017-12-10T14:55:46.194842: step 2049, loss 0.460732, acc 0.875, prec 0.0507798, recall 0.855769
2017-12-10T14:55:46.384123: step 2050, loss 0.421425, acc 0.84375, prec 0.0507637, recall 0.855769
2017-12-10T14:55:46.577711: step 2051, loss 0.366786, acc 0.875, prec 0.050841, recall 0.856
2017-12-10T14:55:46.765731: step 2052, loss 0.3244, acc 0.859375, prec 0.0508566, recall 0.856077
2017-12-10T14:55:46.961277: step 2053, loss 0.540529, acc 0.8125, prec 0.0508673, recall 0.856153
2017-12-10T14:55:47.155953: step 2054, loss 0.539696, acc 0.8125, prec 0.050878, recall 0.85623
2017-12-10T14:55:47.346913: step 2055, loss 0.540186, acc 0.796875, prec 0.0508571, recall 0.85623
2017-12-10T14:55:47.542370: step 2056, loss 0.177071, acc 0.9375, prec 0.0508807, recall 0.856307
2017-12-10T14:55:47.737328: step 2057, loss 0.389187, acc 0.84375, prec 0.0509546, recall 0.856536
2017-12-10T14:55:47.928804: step 2058, loss 0.257139, acc 0.859375, prec 0.0509401, recall 0.856536
2017-12-10T14:55:48.118339: step 2059, loss 0.432941, acc 0.890625, prec 0.0509888, recall 0.856688
2017-12-10T14:55:48.312564: step 2060, loss 0.479804, acc 0.8125, prec 0.0509695, recall 0.856688
2017-12-10T14:55:48.502940: step 2061, loss 0.469072, acc 0.828125, prec 0.0509518, recall 0.856688
2017-12-10T14:55:48.693965: step 2062, loss 0.272917, acc 0.90625, prec 0.0509721, recall 0.856764
2017-12-10T14:55:48.888859: step 2063, loss 0.334772, acc 0.875, prec 0.0509592, recall 0.856764
2017-12-10T14:55:49.085720: step 2064, loss 0.251575, acc 0.921875, prec 0.0509811, recall 0.85684
2017-12-10T14:55:49.277232: step 2065, loss 0.168959, acc 0.953125, prec 0.0510062, recall 0.856916
2017-12-10T14:55:49.468840: step 2066, loss 0.148702, acc 0.953125, prec 0.0510314, recall 0.856992
2017-12-10T14:55:49.658829: step 2067, loss 0.0733697, acc 1, prec 0.0510912, recall 0.857143
2017-12-10T14:55:49.852895: step 2068, loss 0.187379, acc 0.921875, prec 0.0510832, recall 0.857143
2017-12-10T14:55:50.044354: step 2069, loss 0.133961, acc 0.953125, prec 0.0510783, recall 0.857143
2017-12-10T14:55:50.239221: step 2070, loss 0.510165, acc 0.953125, prec 0.0511333, recall 0.857294
2017-12-10T14:55:50.436722: step 2071, loss 0.213268, acc 0.953125, prec 0.0511883, recall 0.857445
2017-12-10T14:55:50.633252: step 2072, loss 0.109838, acc 0.96875, prec 0.0511851, recall 0.857445
2017-12-10T14:55:50.823870: step 2073, loss 0.11535, acc 0.9375, prec 0.0511786, recall 0.857445
2017-12-10T14:55:51.014181: step 2074, loss 0.0178337, acc 1, prec 0.0511786, recall 0.857445
2017-12-10T14:55:51.208660: step 2075, loss 2.00192, acc 0.953125, prec 0.0511754, recall 0.856992
2017-12-10T14:55:51.400384: step 2076, loss 0.0804399, acc 0.984375, prec 0.0511738, recall 0.856992
2017-12-10T14:55:51.591902: step 2077, loss 0.279354, acc 0.921875, prec 0.0511657, recall 0.856992
2017-12-10T14:55:51.783110: step 2078, loss 0.310445, acc 0.96875, prec 0.0511924, recall 0.857068
2017-12-10T14:55:51.979761: step 2079, loss 0.0296022, acc 1, prec 0.0511924, recall 0.857068
2017-12-10T14:55:52.178496: step 2080, loss 0.137478, acc 0.9375, prec 0.0511859, recall 0.857068
2017-12-10T14:55:52.370779: step 2081, loss 0.0564884, acc 0.984375, prec 0.0511843, recall 0.857068
2017-12-10T14:55:52.566551: step 2082, loss 1.44767, acc 0.9375, prec 0.0512675, recall 0.857293
2017-12-10T14:55:52.761271: step 2083, loss 0.188334, acc 0.984375, prec 0.0513256, recall 0.857443
2017-12-10T14:55:52.954016: step 2084, loss 0.0683132, acc 0.984375, prec 0.051324, recall 0.857443
2017-12-10T14:55:53.145784: step 2085, loss 0.161387, acc 0.953125, prec 0.0513192, recall 0.857443
2017-12-10T14:55:53.338004: step 2086, loss 0.567115, acc 0.9375, prec 0.0513426, recall 0.857518
2017-12-10T14:55:53.528995: step 2087, loss 0.135197, acc 0.953125, prec 0.0513676, recall 0.857593
2017-12-10T14:55:53.721417: step 2088, loss 0.068791, acc 0.984375, prec 0.051366, recall 0.857593
2017-12-10T14:55:53.911772: step 2089, loss 0.371274, acc 0.84375, prec 0.0513498, recall 0.857593
2017-12-10T14:55:54.106318: step 2090, loss 0.0754253, acc 0.96875, prec 0.0513764, recall 0.857668
2017-12-10T14:55:54.299306: step 2091, loss 0.381948, acc 0.875, prec 0.0514232, recall 0.857817
2017-12-10T14:55:54.493686: step 2092, loss 0.270636, acc 0.921875, prec 0.0514748, recall 0.857966
2017-12-10T14:55:54.688313: step 2093, loss 0.294309, acc 0.9375, prec 0.0514683, recall 0.857966
2017-12-10T14:55:54.880209: step 2094, loss 0.145354, acc 0.953125, prec 0.0514932, recall 0.858041
2017-12-10T14:55:55.073922: step 2095, loss 0.364596, acc 0.875, prec 0.0515101, recall 0.858115
2017-12-10T14:55:55.267697: step 2096, loss 0.475939, acc 0.953125, prec 0.0515351, recall 0.858189
2017-12-10T14:55:55.468966: step 2097, loss 0.245862, acc 0.890625, prec 0.0515237, recall 0.858189
2017-12-10T14:55:55.660351: step 2098, loss 0.324162, acc 0.859375, prec 0.0515092, recall 0.858189
2017-12-10T14:55:55.852912: step 2099, loss 0.151102, acc 0.921875, prec 0.0515011, recall 0.858189
2017-12-10T14:55:56.045415: step 2100, loss 0.265813, acc 0.875, prec 0.0515179, recall 0.858264
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_0/1512935327/checkpoints/model-2100

2017-12-10T14:55:57.287438: step 2101, loss 0.308164, acc 0.9375, prec 0.051571, recall 0.858412
2017-12-10T14:55:57.483068: step 2102, loss 0.370065, acc 0.890625, prec 0.0516192, recall 0.858559
2017-12-10T14:55:57.673948: step 2103, loss 0.122321, acc 0.96875, prec 0.0516159, recall 0.858559
2017-12-10T14:55:57.861653: step 2104, loss 0.230284, acc 0.890625, prec 0.0516046, recall 0.858559
2017-12-10T14:55:58.053290: step 2105, loss 0.162471, acc 0.953125, prec 0.0516295, recall 0.858633
2017-12-10T14:55:58.248155: step 2106, loss 0.397234, acc 0.953125, prec 0.0516544, recall 0.858707
2017-12-10T14:55:58.445546: step 2107, loss 5.55083, acc 0.921875, prec 0.0517074, recall 0.858407
2017-12-10T14:55:58.638934: step 2108, loss 0.145882, acc 0.9375, prec 0.0517009, recall 0.858407
2017-12-10T14:55:58.832437: step 2109, loss 0.321943, acc 0.859375, prec 0.0516863, recall 0.858407
2017-12-10T14:55:59.025638: step 2110, loss 0.32843, acc 0.890625, prec 0.0517047, recall 0.858481
2017-12-10T14:55:59.231190: step 2111, loss 0.648931, acc 0.90625, prec 0.0517247, recall 0.858554
2017-12-10T14:55:59.421900: step 2112, loss 0.314488, acc 0.828125, prec 0.0517366, recall 0.858628
2017-12-10T14:55:59.614242: step 2113, loss 0.340887, acc 0.890625, prec 0.0517252, recall 0.858628
2017-12-10T14:55:59.806004: step 2114, loss 0.500948, acc 0.859375, prec 0.0517106, recall 0.858628
2017-12-10T14:55:59.996646: step 2115, loss 0.634037, acc 0.765625, prec 0.0516864, recall 0.858628
2017-12-10T14:56:00.188395: step 2116, loss 0.440048, acc 0.859375, prec 0.0517015, recall 0.858701
2017-12-10T14:56:00.388350: step 2117, loss 0.371883, acc 0.84375, prec 0.051715, recall 0.858775
2017-12-10T14:56:00.577964: step 2118, loss 0.349746, acc 0.890625, prec 0.0517037, recall 0.858775
2017-12-10T14:56:00.770088: step 2119, loss 0.439458, acc 0.890625, prec 0.0517516, recall 0.858921
2017-12-10T14:56:00.963148: step 2120, loss 0.332583, acc 0.890625, prec 0.0517403, recall 0.858921
2017-12-10T14:56:01.160858: step 2121, loss 0.205739, acc 0.921875, prec 0.0517618, recall 0.858994
2017-12-10T14:56:01.356648: step 2122, loss 0.743023, acc 0.890625, prec 0.0518098, recall 0.85914
2017-12-10T14:56:01.554415: step 2123, loss 0.29748, acc 0.875, prec 0.0517968, recall 0.85914
2017-12-10T14:56:01.747442: step 2124, loss 0.303686, acc 0.828125, prec 0.051779, recall 0.85914
2017-12-10T14:56:01.940265: step 2125, loss 0.22856, acc 0.890625, prec 0.0518269, recall 0.859286
2017-12-10T14:56:02.132679: step 2126, loss 0.32986, acc 0.875, prec 0.0518435, recall 0.859359
2017-12-10T14:56:02.328662: step 2127, loss 0.248315, acc 0.921875, prec 0.0518354, recall 0.859359
2017-12-10T14:56:02.523347: step 2128, loss 0.280749, acc 0.90625, prec 0.0519144, recall 0.859577
2017-12-10T14:56:02.718204: step 2129, loss 0.137967, acc 0.984375, prec 0.0519128, recall 0.859577
2017-12-10T14:56:02.916044: step 2130, loss 0.243876, acc 0.921875, prec 0.0519047, recall 0.859577
2017-12-10T14:56:03.110468: step 2131, loss 0.0662514, acc 0.984375, prec 0.0519327, recall 0.859649
2017-12-10T14:56:03.306140: step 2132, loss 0.598743, acc 0.96875, prec 0.051959, recall 0.859721
2017-12-10T14:56:03.502717: step 2133, loss 0.0998159, acc 0.9375, prec 0.0519525, recall 0.859721
2017-12-10T14:56:03.697263: step 2134, loss 0.0692053, acc 0.984375, prec 0.0519509, recall 0.859721
2017-12-10T14:56:03.889237: step 2135, loss 0.300202, acc 0.9375, prec 0.051974, recall 0.859794
2017-12-10T14:56:04.081603: step 2136, loss 0.0655902, acc 0.984375, prec 0.0519723, recall 0.859794
2017-12-10T14:56:04.271883: step 2137, loss 0.338731, acc 0.984375, prec 0.0520298, recall 0.859938
2017-12-10T14:56:04.473242: step 2138, loss 0.529732, acc 0.96875, prec 0.0520561, recall 0.86001
2017-12-10T14:56:04.669366: step 2139, loss 0.0454169, acc 0.984375, prec 0.0520545, recall 0.86001
2017-12-10T14:56:04.865647: step 2140, loss 0.95786, acc 0.9375, prec 0.0520775, recall 0.860082
2017-12-10T14:56:05.057744: step 2141, loss 0.194373, acc 0.96875, prec 0.0521038, recall 0.860154
2017-12-10T14:56:05.250788: step 2142, loss 3.39886, acc 0.90625, prec 0.0521858, recall 0.859487
2017-12-10T14:56:05.447345: step 2143, loss 0.15197, acc 0.953125, prec 0.0522695, recall 0.859703
2017-12-10T14:56:05.643240: step 2144, loss 0.174079, acc 0.953125, prec 0.0522941, recall 0.859775
2017-12-10T14:56:05.835477: step 2145, loss 0.310337, acc 0.890625, prec 0.0522827, recall 0.859775
2017-12-10T14:56:06.028375: step 2146, loss 0.311352, acc 0.921875, prec 0.0523041, recall 0.859847
2017-12-10T14:56:06.220592: step 2147, loss 0.657584, acc 0.859375, prec 0.0523484, recall 0.85999
2017-12-10T14:56:06.416906: step 2148, loss 0.539352, acc 0.8125, prec 0.0523288, recall 0.85999
2017-12-10T14:56:06.608102: step 2149, loss 0.870094, acc 0.796875, prec 0.0523077, recall 0.85999
2017-12-10T14:56:06.798901: step 2150, loss 0.533431, acc 0.84375, prec 0.0522914, recall 0.85999
2017-12-10T14:56:06.989541: step 2151, loss 0.501659, acc 0.859375, prec 0.0522768, recall 0.85999
2017-12-10T14:56:07.180640: step 2152, loss 0.543213, acc 0.859375, prec 0.0522622, recall 0.85999
2017-12-10T14:56:07.371317: step 2153, loss 0.627747, acc 0.8125, prec 0.0522722, recall 0.860061
2017-12-10T14:56:07.560814: step 2154, loss 0.385764, acc 0.875, prec 0.0522592, recall 0.860061
2017-12-10T14:56:07.750996: step 2155, loss 0.675025, acc 0.8125, prec 0.0522397, recall 0.860061
2017-12-10T14:56:07.942202: step 2156, loss 0.425866, acc 0.84375, prec 0.0522529, recall 0.860133
2017-12-10T14:56:08.135393: step 2157, loss 0.633015, acc 0.75, prec 0.0522858, recall 0.860275
2017-12-10T14:56:08.327137: step 2158, loss 0.493789, acc 0.765625, prec 0.0522615, recall 0.860275
2017-12-10T14:56:08.519723: step 2159, loss 0.441117, acc 0.84375, prec 0.0522746, recall 0.860347
2017-12-10T14:56:08.712316: step 2160, loss 0.46551, acc 0.953125, prec 0.0522991, recall 0.860418
2017-12-10T14:56:08.905666: step 2161, loss 0.363255, acc 0.875, prec 0.0523448, recall 0.86056
2017-12-10T14:56:09.098456: step 2162, loss 0.164175, acc 0.9375, prec 0.0523384, recall 0.86056
2017-12-10T14:56:09.288950: step 2163, loss 0.0775097, acc 0.96875, prec 0.0523644, recall 0.860631
2017-12-10T14:56:09.481455: step 2164, loss 0.442695, acc 0.90625, prec 0.0523547, recall 0.860631
2017-12-10T14:56:09.676662: step 2165, loss 0.116773, acc 0.953125, prec 0.0524085, recall 0.860772
2017-12-10T14:56:09.864368: step 2166, loss 0.139812, acc 0.9375, prec 0.052402, recall 0.860772
2017-12-10T14:56:10.052408: step 2167, loss 0.0898105, acc 0.9375, prec 0.0523955, recall 0.860772
2017-12-10T14:56:10.244040: step 2168, loss 0.417123, acc 0.984375, prec 0.0524818, recall 0.860984
2017-12-10T14:56:10.438945: step 2169, loss 0.127224, acc 0.9375, prec 0.0524753, recall 0.860984
2017-12-10T14:56:10.635718: step 2170, loss 0.249459, acc 0.9375, prec 0.0525274, recall 0.861125
2017-12-10T14:56:10.828198: step 2171, loss 0.166356, acc 0.921875, prec 0.0525193, recall 0.861125
2017-12-10T14:56:11.020200: step 2172, loss 0.39739, acc 0.921875, prec 0.0525405, recall 0.861196
2017-12-10T14:56:11.212882: step 2173, loss 0.183233, acc 0.953125, prec 0.0525942, recall 0.861336
2017-12-10T14:56:11.403932: step 2174, loss 0.0998545, acc 0.984375, prec 0.0525925, recall 0.861336
2017-12-10T14:56:11.598041: step 2175, loss 2.7471, acc 0.96875, prec 0.0526202, recall 0.860971
2017-12-10T14:56:11.797775: step 2176, loss 1.89794, acc 0.921875, prec 0.0526137, recall 0.860536
2017-12-10T14:56:11.996026: step 2177, loss 0.114782, acc 0.953125, prec 0.0526088, recall 0.860536
2017-12-10T14:56:12.189388: step 2178, loss 1.0718, acc 0.921875, prec 0.05263, recall 0.860606
2017-12-10T14:56:12.385899: step 2179, loss 0.693156, acc 0.90625, prec 0.0526787, recall 0.860747
2017-12-10T14:56:12.580332: step 2180, loss 0.252271, acc 0.890625, prec 0.0526966, recall 0.860817
2017-12-10T14:56:12.769286: step 2181, loss 0.784457, acc 0.921875, prec 0.0527177, recall 0.860887
2017-12-10T14:56:12.965889: step 2182, loss 0.338059, acc 0.890625, prec 0.0527063, recall 0.860887
2017-12-10T14:56:13.165908: step 2183, loss 0.571058, acc 0.78125, prec 0.0527127, recall 0.860957
2017-12-10T14:56:13.356721: step 2184, loss 0.486768, acc 0.859375, prec 0.0526981, recall 0.860957
2017-12-10T14:56:13.545988: step 2185, loss 0.65125, acc 0.796875, prec 0.0527062, recall 0.861027
2017-12-10T14:56:13.738927: step 2186, loss 0.560304, acc 0.78125, prec 0.0527418, recall 0.861167
2017-12-10T14:56:13.933436: step 2187, loss 0.649338, acc 0.71875, prec 0.0527126, recall 0.861167
2017-12-10T14:56:14.132644: step 2188, loss 0.485812, acc 0.84375, prec 0.0527255, recall 0.861237
2017-12-10T14:56:14.326431: step 2189, loss 0.706299, acc 0.75, prec 0.0527287, recall 0.861307
2017-12-10T14:56:14.513625: step 2190, loss 0.415944, acc 0.84375, prec 0.0527999, recall 0.861515
2017-12-10T14:56:14.708842: step 2191, loss 0.665999, acc 0.828125, prec 0.052782, recall 0.861515
2017-12-10T14:56:14.904520: step 2192, loss 0.599179, acc 0.765625, prec 0.0527868, recall 0.861585
2017-12-10T14:56:15.101567: step 2193, loss 0.301817, acc 0.90625, prec 0.0527771, recall 0.861585
2017-12-10T14:56:15.292570: step 2194, loss 0.425863, acc 0.84375, prec 0.0527609, recall 0.861585
2017-12-10T14:56:15.483520: step 2195, loss 0.291053, acc 0.921875, prec 0.0527819, recall 0.861654
2017-12-10T14:56:15.670974: step 2196, loss 0.335569, acc 0.90625, prec 0.0528303, recall 0.861793
2017-12-10T14:56:15.863844: step 2197, loss 0.448926, acc 0.859375, prec 0.0528157, recall 0.861793
2017-12-10T14:56:16.060129: step 2198, loss 0.378194, acc 0.921875, prec 0.0528657, recall 0.861931
2017-12-10T14:56:16.252268: step 2199, loss 0.229164, acc 0.9375, prec 0.0528883, recall 0.862
2017-12-10T14:56:16.446614: step 2200, loss 0.182448, acc 0.9375, prec 0.0529109, recall 0.862069
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_0/1512935327/checkpoints/model-2200

2017-12-10T14:56:17.970879: step 2201, loss 0.106827, acc 0.953125, prec 0.052935, recall 0.862138
2017-12-10T14:56:18.161486: step 2202, loss 0.751837, acc 0.890625, prec 0.0529527, recall 0.862207
2017-12-10T14:56:18.358837: step 2203, loss 0.210162, acc 0.921875, prec 0.0529446, recall 0.862207
2017-12-10T14:56:18.551879: step 2204, loss 0.156849, acc 0.953125, prec 0.0529397, recall 0.862207
2017-12-10T14:56:18.741435: step 2205, loss 1.05991, acc 0.875, prec 0.0529558, recall 0.862275
2017-12-10T14:56:18.937100: step 2206, loss 0.36838, acc 0.953125, prec 0.0529799, recall 0.862344
2017-12-10T14:56:19.130985: step 2207, loss 0.165459, acc 0.953125, prec 0.0529751, recall 0.862344
2017-12-10T14:56:19.320508: step 2208, loss 0.264253, acc 0.921875, prec 0.0529669, recall 0.862344
2017-12-10T14:56:19.522581: step 2209, loss 0.150533, acc 0.953125, prec 0.0529911, recall 0.862413
2017-12-10T14:56:19.712198: step 2210, loss 1.7161, acc 0.96875, prec 0.0531328, recall 0.862755
2017-12-10T14:56:19.908797: step 2211, loss 0.245538, acc 0.890625, prec 0.0531215, recall 0.862755
2017-12-10T14:56:20.098098: step 2212, loss 0.204194, acc 0.90625, prec 0.0531117, recall 0.862755
2017-12-10T14:56:20.293980: step 2213, loss 0.374885, acc 0.9375, prec 0.0532211, recall 0.863027
2017-12-10T14:56:20.483902: step 2214, loss 0.197582, acc 0.921875, prec 0.053213, recall 0.863027
2017-12-10T14:56:20.678646: step 2215, loss 0.269732, acc 0.921875, prec 0.0532338, recall 0.863095
2017-12-10T14:56:20.873398: step 2216, loss 0.104254, acc 0.96875, prec 0.0532885, recall 0.863231
2017-12-10T14:56:21.063988: step 2217, loss 0.986006, acc 0.90625, prec 0.0533076, recall 0.863299
2017-12-10T14:56:21.258991: step 2218, loss 0.294498, acc 0.90625, prec 0.0532979, recall 0.863299
2017-12-10T14:56:21.447208: step 2219, loss 0.252778, acc 0.9375, prec 0.0532913, recall 0.863299
2017-12-10T14:56:21.642450: step 2220, loss 0.581083, acc 0.875, prec 0.0533362, recall 0.863434
2017-12-10T14:56:21.832925: step 2221, loss 0.248946, acc 0.90625, prec 0.0533553, recall 0.863501
2017-12-10T14:56:22.025307: step 2222, loss 0.233687, acc 0.90625, prec 0.0533745, recall 0.863569
2017-12-10T14:56:22.215864: step 2223, loss 0.344076, acc 0.84375, prec 0.0533582, recall 0.863569
2017-12-10T14:56:22.409269: step 2224, loss 0.215901, acc 0.921875, prec 0.05335, recall 0.863569
2017-12-10T14:56:22.603699: step 2225, loss 0.261278, acc 0.90625, prec 0.0533692, recall 0.863636
2017-12-10T14:56:22.797094: step 2226, loss 0.141511, acc 0.96875, prec 0.0533659, recall 0.863636
2017-12-10T14:56:22.989290: step 2227, loss 0.105429, acc 0.9375, prec 0.0533594, recall 0.863636
2017-12-10T14:56:23.187746: step 2228, loss 0.449556, acc 0.9375, prec 0.0533818, recall 0.863704
2017-12-10T14:56:23.384240: step 2229, loss 0.223139, acc 0.90625, prec 0.053372, recall 0.863704
2017-12-10T14:56:23.580418: step 2230, loss 0.143857, acc 0.921875, prec 0.0533927, recall 0.863771
2017-12-10T14:56:23.777583: step 2231, loss 0.188864, acc 0.921875, prec 0.0534423, recall 0.863905
2017-12-10T14:56:23.968000: step 2232, loss 0.164664, acc 0.921875, prec 0.0534342, recall 0.863905
2017-12-10T14:56:24.157267: step 2233, loss 0.239845, acc 0.90625, prec 0.0534244, recall 0.863905
2017-12-10T14:56:24.348073: step 2234, loss 0.284933, acc 0.921875, prec 0.0534163, recall 0.863905
2017-12-10T14:56:24.540738: step 2235, loss 1.01667, acc 0.84375, prec 0.0534288, recall 0.863972
2017-12-10T14:56:24.734622: step 2236, loss 0.342322, acc 0.890625, prec 0.0534751, recall 0.864106
2017-12-10T14:56:24.933512: step 2237, loss 0.0874081, acc 0.984375, prec 0.0535312, recall 0.86424
2017-12-10T14:56:25.129354: step 2238, loss 0.175902, acc 0.90625, prec 0.0535214, recall 0.86424
2017-12-10T14:56:25.327389: step 2239, loss 0.252544, acc 0.953125, prec 0.0535741, recall 0.864373
2017-12-10T14:56:25.529304: step 2240, loss 0.130085, acc 0.953125, prec 0.0535981, recall 0.86444
2017-12-10T14:56:25.725433: step 2241, loss 0.257001, acc 0.90625, prec 0.0536171, recall 0.864507
2017-12-10T14:56:25.914394: step 2242, loss 0.390813, acc 0.984375, prec 0.0536443, recall 0.864573
2017-12-10T14:56:26.109986: step 2243, loss 0.156172, acc 0.953125, prec 0.0536394, recall 0.864573
2017-12-10T14:56:26.300254: step 2244, loss 0.0443135, acc 1, prec 0.0536394, recall 0.864573
2017-12-10T14:56:26.494268: step 2245, loss 0.237293, acc 0.953125, prec 0.0536633, recall 0.86464
2017-12-10T14:56:26.691343: step 2246, loss 0.218666, acc 0.953125, prec 0.0536872, recall 0.864706
2017-12-10T14:56:26.884536: step 2247, loss 0.174483, acc 0.953125, prec 0.0536823, recall 0.864706
2017-12-10T14:56:27.082171: step 2248, loss 0.972532, acc 0.96875, prec 0.0537078, recall 0.864772
2017-12-10T14:56:27.275754: step 2249, loss 0.261986, acc 0.9375, prec 0.0537301, recall 0.864838
2017-12-10T14:56:27.469117: step 2250, loss 0.273659, acc 0.84375, prec 0.0537713, recall 0.864971
2017-12-10T14:56:27.662119: step 2251, loss 0.130661, acc 0.953125, prec 0.0537664, recall 0.864971
2017-12-10T14:56:27.854401: step 2252, loss 0.0776975, acc 0.96875, prec 0.0538207, recall 0.865103
2017-12-10T14:56:28.051052: step 2253, loss 0.313716, acc 0.921875, prec 0.0538412, recall 0.865169
2017-12-10T14:56:28.247203: step 2254, loss 0.228882, acc 0.921875, prec 0.0538618, recall 0.865234
2017-12-10T14:56:28.444628: step 2255, loss 0.458964, acc 0.9375, prec 0.053884, recall 0.8653
2017-12-10T14:56:28.640916: step 2256, loss 0.103547, acc 0.953125, prec 0.0538791, recall 0.8653
2017-12-10T14:56:28.834940: step 2257, loss 0.287736, acc 0.921875, prec 0.0538709, recall 0.8653
2017-12-10T14:56:29.032437: step 2258, loss 0.189807, acc 0.921875, prec 0.0538915, recall 0.865366
2017-12-10T14:56:29.227387: step 2259, loss 0.191795, acc 0.96875, prec 0.0538882, recall 0.865366
2017-12-10T14:56:29.418769: step 2260, loss 0.401717, acc 0.953125, prec 0.0539408, recall 0.865497
2017-12-10T14:56:29.618730: step 2261, loss 1.14951, acc 0.921875, prec 0.0540475, recall 0.865759
2017-12-10T14:56:29.813009: step 2262, loss 0.820773, acc 0.921875, prec 0.0540967, recall 0.865889
2017-12-10T14:56:30.016137: step 2263, loss 0.0982512, acc 0.96875, prec 0.0540934, recall 0.865889
2017-12-10T14:56:30.214132: step 2264, loss 1.00468, acc 0.921875, prec 0.0541139, recall 0.865954
2017-12-10T14:56:30.406285: step 2265, loss 0.209598, acc 0.90625, prec 0.0541328, recall 0.866019
2017-12-10T14:56:30.603942: step 2266, loss 0.20834, acc 0.921875, prec 0.0541246, recall 0.866019
2017-12-10T14:56:30.795660: step 2267, loss 0.218417, acc 0.90625, prec 0.0541147, recall 0.866019
2017-12-10T14:56:30.990706: step 2268, loss 0.274908, acc 0.859375, prec 0.0541286, recall 0.866084
2017-12-10T14:56:31.188643: step 2269, loss 0.466246, acc 0.84375, prec 0.0541409, recall 0.866149
2017-12-10T14:56:31.386946: step 2270, loss 0.260627, acc 0.9375, prec 0.0541343, recall 0.866149
2017-12-10T14:56:31.585472: step 2271, loss 0.310516, acc 0.859375, prec 0.0541196, recall 0.866149
2017-12-10T14:56:31.781517: step 2272, loss 0.347673, acc 0.890625, prec 0.0541654, recall 0.866279
2017-12-10T14:56:31.970953: step 2273, loss 0.222246, acc 0.953125, prec 0.0542178, recall 0.866409
2017-12-10T14:56:32.165013: step 2274, loss 0.265455, acc 0.890625, prec 0.0542349, recall 0.866473
2017-12-10T14:56:32.354825: step 2275, loss 0.488574, acc 0.890625, prec 0.0542521, recall 0.866538
2017-12-10T14:56:32.546261: step 2276, loss 0.235244, acc 0.96875, prec 0.0542488, recall 0.866538
2017-12-10T14:56:32.743788: step 2277, loss 0.277289, acc 0.90625, prec 0.0543248, recall 0.866731
2017-12-10T14:56:32.931911: step 2278, loss 0.478233, acc 0.890625, prec 0.0543705, recall 0.86686
2017-12-10T14:56:33.125906: step 2279, loss 0.339411, acc 0.890625, prec 0.0543876, recall 0.866924
2017-12-10T14:56:33.318015: step 2280, loss 0.290446, acc 0.890625, prec 0.0543761, recall 0.866924
2017-12-10T14:56:33.512052: step 2281, loss 0.135711, acc 0.953125, prec 0.0543998, recall 0.866988
2017-12-10T14:56:33.707831: step 2282, loss 0.12432, acc 0.96875, prec 0.0544536, recall 0.867116
2017-12-10T14:56:33.905331: step 2283, loss 0.0804246, acc 0.984375, prec 0.054452, recall 0.867116
2017-12-10T14:56:34.102761: step 2284, loss 0.354629, acc 0.890625, prec 0.0544405, recall 0.867116
2017-12-10T14:56:34.293894: step 2285, loss 0.108306, acc 0.9375, prec 0.0544339, recall 0.867116
2017-12-10T14:56:34.488831: step 2286, loss 0.271758, acc 1, prec 0.0544911, recall 0.867244
2017-12-10T14:56:34.681280: step 2287, loss 0.72469, acc 0.90625, prec 0.0545383, recall 0.867371
2017-12-10T14:56:34.873343: step 2288, loss 0.132967, acc 0.9375, prec 0.0545317, recall 0.867371
2017-12-10T14:56:35.067954: step 2289, loss 1.22718, acc 0.953125, prec 0.0545839, recall 0.867499
2017-12-10T14:56:35.261880: step 2290, loss 0.126254, acc 0.9375, prec 0.0546059, recall 0.867562
2017-12-10T14:56:35.455832: step 2291, loss 0.575002, acc 0.96875, prec 0.0546882, recall 0.867753
2017-12-10T14:56:35.650957: step 2292, loss 0.287097, acc 0.921875, prec 0.05468, recall 0.867753
2017-12-10T14:56:35.844946: step 2293, loss 0.789072, acc 0.953125, prec 0.0547035, recall 0.867816
2017-12-10T14:56:36.041494: step 2294, loss 0.142147, acc 0.953125, prec 0.0547271, recall 0.867879
2017-12-10T14:56:36.235609: step 2295, loss 0.269065, acc 0.9375, prec 0.054749, recall 0.867943
2017-12-10T14:56:36.428571: step 2296, loss 0.127411, acc 0.953125, prec 0.0548011, recall 0.868069
2017-12-10T14:56:36.618116: step 2297, loss 0.244451, acc 0.921875, prec 0.0548499, recall 0.868195
2017-12-10T14:56:36.814007: step 2298, loss 0.552002, acc 0.890625, prec 0.0548668, recall 0.868258
2017-12-10T14:56:37.003631: step 2299, loss 0.331963, acc 0.921875, prec 0.0548871, recall 0.868321
2017-12-10T14:56:37.195300: step 2300, loss 0.221491, acc 0.921875, prec 0.0548788, recall 0.868321
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_0/1512935327/checkpoints/model-2300

2017-12-10T14:56:38.441287: step 2301, loss 0.294437, acc 0.890625, prec 0.0548957, recall 0.868383
2017-12-10T14:56:38.634123: step 2302, loss 0.703872, acc 0.90625, prec 0.0549427, recall 0.868509
2017-12-10T14:56:38.834749: step 2303, loss 0.493831, acc 0.84375, prec 0.0549547, recall 0.868571
2017-12-10T14:56:39.027981: step 2304, loss 0.185509, acc 0.9375, prec 0.0549765, recall 0.868634
2017-12-10T14:56:39.218309: step 2305, loss 0.52043, acc 0.828125, prec 0.0549867, recall 0.868696
2017-12-10T14:56:39.408676: step 2306, loss 0.31576, acc 0.875, prec 0.0549735, recall 0.868696
2017-12-10T14:56:39.598920: step 2307, loss 0.435975, acc 0.890625, prec 0.0550188, recall 0.868821
2017-12-10T14:56:39.793826: step 2308, loss 0.220014, acc 0.90625, prec 0.0550089, recall 0.868821
2017-12-10T14:56:39.989035: step 2309, loss 0.225808, acc 0.90625, prec 0.0550274, recall 0.868884
2017-12-10T14:56:40.185061: step 2310, loss 0.308823, acc 0.890625, prec 0.0550158, recall 0.868884
2017-12-10T14:56:40.384172: step 2311, loss 0.285719, acc 0.953125, prec 0.0550677, recall 0.869008
2017-12-10T14:56:40.580013: step 2312, loss 0.310589, acc 0.890625, prec 0.0550845, recall 0.86907
2017-12-10T14:56:40.768037: step 2313, loss 1.18323, acc 0.9375, prec 0.0551347, recall 0.869194
2017-12-10T14:56:40.962773: step 2314, loss 0.139802, acc 0.984375, prec 0.0551614, recall 0.869256
2017-12-10T14:56:41.162097: step 2315, loss 0.169441, acc 0.953125, prec 0.0551565, recall 0.869256
2017-12-10T14:56:41.354299: step 2316, loss 0.487234, acc 0.859375, prec 0.0551699, recall 0.869318
2017-12-10T14:56:41.548401: step 2317, loss 0.181489, acc 0.9375, prec 0.0551917, recall 0.86938
2017-12-10T14:56:41.741210: step 2318, loss 0.322069, acc 0.875, prec 0.0552068, recall 0.869442
2017-12-10T14:56:41.931597: step 2319, loss 0.062176, acc 0.984375, prec 0.0552051, recall 0.869442
2017-12-10T14:56:42.124475: step 2320, loss 0.197815, acc 0.90625, prec 0.0551952, recall 0.869442
2017-12-10T14:56:42.325709: step 2321, loss 0.228281, acc 0.921875, prec 0.0551869, recall 0.869442
2017-12-10T14:56:42.515665: step 2322, loss 0.373206, acc 0.9375, prec 0.055237, recall 0.869565
2017-12-10T14:56:42.711489: step 2323, loss 0.552562, acc 0.921875, prec 0.0553138, recall 0.86975
2017-12-10T14:56:42.906468: step 2324, loss 0.157486, acc 0.953125, prec 0.0553088, recall 0.86975
2017-12-10T14:56:43.100023: step 2325, loss 0.69912, acc 0.9375, prec 0.0554155, recall 0.869995
2017-12-10T14:56:43.294961: step 2326, loss 0.161758, acc 0.921875, prec 0.0554072, recall 0.869995
2017-12-10T14:56:43.485529: step 2327, loss 0.149301, acc 0.96875, prec 0.0554322, recall 0.870057
2017-12-10T14:56:43.678770: step 2328, loss 0.864562, acc 0.921875, prec 0.0554806, recall 0.870179
2017-12-10T14:56:43.871505: step 2329, loss 0.187139, acc 0.921875, prec 0.0555006, recall 0.87024
2017-12-10T14:56:44.067565: step 2330, loss 0.25642, acc 0.875, prec 0.0554873, recall 0.87024
2017-12-10T14:56:44.260190: step 2331, loss 0.0979199, acc 0.96875, prec 0.0555406, recall 0.870362
2017-12-10T14:56:44.457009: step 2332, loss 1.29683, acc 0.984375, prec 0.0555689, recall 0.870014
2017-12-10T14:56:44.650248: step 2333, loss 0.258102, acc 0.90625, prec 0.0555872, recall 0.870075
2017-12-10T14:56:44.847659: step 2334, loss 0.793981, acc 0.90625, prec 0.0556621, recall 0.870258
2017-12-10T14:56:45.040694: step 2335, loss 0.521125, acc 0.875, prec 0.0557053, recall 0.870379
2017-12-10T14:56:45.232068: step 2336, loss 0.293586, acc 0.90625, prec 0.0557236, recall 0.87044
2017-12-10T14:56:45.425992: step 2337, loss 0.215685, acc 0.953125, prec 0.0557751, recall 0.870561
2017-12-10T14:56:45.618974: step 2338, loss 0.248189, acc 0.90625, prec 0.0557933, recall 0.870621
2017-12-10T14:56:45.814030: step 2339, loss 0.225858, acc 0.921875, prec 0.055785, recall 0.870621
2017-12-10T14:56:46.004063: step 2340, loss 0.475775, acc 0.84375, prec 0.0557966, recall 0.870682
2017-12-10T14:56:46.193869: step 2341, loss 0.211472, acc 0.96875, prec 0.0558215, recall 0.870742
2017-12-10T14:56:46.391252: step 2342, loss 0.188007, acc 0.9375, prec 0.0558148, recall 0.870742
2017-12-10T14:56:46.586204: step 2343, loss 0.181885, acc 0.953125, prec 0.055838, recall 0.870802
2017-12-10T14:56:46.775508: step 2344, loss 0.547961, acc 0.84375, prec 0.0558496, recall 0.870862
2017-12-10T14:56:46.963851: step 2345, loss 0.231447, acc 0.9375, prec 0.0558711, recall 0.870923
2017-12-10T14:56:47.152847: step 2346, loss 0.372146, acc 0.921875, prec 0.0559474, recall 0.871103
2017-12-10T14:56:47.344146: step 2347, loss 0.207219, acc 0.890625, prec 0.0559639, recall 0.871163
2017-12-10T14:56:47.536107: step 2348, loss 1.08476, acc 0.921875, prec 0.0560119, recall 0.871283
2017-12-10T14:56:47.727733: step 2349, loss 0.180099, acc 0.9375, prec 0.0560335, recall 0.871342
2017-12-10T14:56:47.924126: step 2350, loss 0.22722, acc 0.921875, prec 0.0560533, recall 0.871402
2017-12-10T14:56:48.113639: step 2351, loss 0.0687855, acc 1, prec 0.0560533, recall 0.871402
2017-12-10T14:56:48.307668: step 2352, loss 0.171804, acc 0.953125, prec 0.0560764, recall 0.871462
2017-12-10T14:56:48.496379: step 2353, loss 0.324339, acc 0.9375, prec 0.0560979, recall 0.871521
2017-12-10T14:56:48.685836: step 2354, loss 0.289392, acc 0.890625, prec 0.0560862, recall 0.871521
2017-12-10T14:56:48.881318: step 2355, loss 0.303806, acc 0.9375, prec 0.0560795, recall 0.871521
2017-12-10T14:56:49.072615: step 2356, loss 0.274238, acc 0.90625, prec 0.0560976, recall 0.871581
2017-12-10T14:56:49.263172: step 2357, loss 0.43383, acc 0.84375, prec 0.0560809, recall 0.871581
2017-12-10T14:56:49.454311: step 2358, loss 0.0962452, acc 0.984375, prec 0.0561074, recall 0.87164
2017-12-10T14:56:49.645608: step 2359, loss 0.321715, acc 0.9375, prec 0.0561288, recall 0.8717
2017-12-10T14:56:49.842950: step 2360, loss 0.279279, acc 0.9375, prec 0.0561503, recall 0.871759
2017-12-10T14:56:50.036333: step 2361, loss 0.141679, acc 0.96875, prec 0.0561469, recall 0.871759
2017-12-10T14:56:50.231805: step 2362, loss 0.134881, acc 0.9375, prec 0.0561684, recall 0.871819
2017-12-10T14:56:50.424307: step 2363, loss 0.315206, acc 0.890625, prec 0.0561567, recall 0.871819
2017-12-10T14:56:50.618394: step 2364, loss 0.217053, acc 0.9375, prec 0.0561781, recall 0.871878
2017-12-10T14:56:50.811736: step 2365, loss 0.184135, acc 0.9375, prec 0.0561995, recall 0.871937
2017-12-10T14:56:51.001489: step 2366, loss 1.9414, acc 0.921875, prec 0.056221, recall 0.871594
2017-12-10T14:56:51.192684: step 2367, loss 0.175693, acc 0.96875, prec 0.0562738, recall 0.871712
2017-12-10T14:56:51.389015: step 2368, loss 0.0744107, acc 0.984375, prec 0.0562722, recall 0.871712
2017-12-10T14:56:51.581020: step 2369, loss 0.085245, acc 0.984375, prec 0.0562986, recall 0.871771
2017-12-10T14:56:51.773291: step 2370, loss 0.0789753, acc 0.984375, prec 0.0562969, recall 0.871771
2017-12-10T14:56:51.963587: step 2371, loss 0.0985052, acc 0.96875, prec 0.0563217, recall 0.87183
2017-12-10T14:56:52.156520: step 2372, loss 0.0612001, acc 0.96875, prec 0.0563183, recall 0.87183
2017-12-10T14:56:52.347469: step 2373, loss 0.189486, acc 0.953125, prec 0.0563414, recall 0.871889
2017-12-10T14:56:52.542618: step 2374, loss 1.05333, acc 0.984375, prec 0.0563959, recall 0.872007
2017-12-10T14:56:52.735482: step 2375, loss 0.146203, acc 0.921875, prec 0.0563875, recall 0.872007
2017-12-10T14:56:52.928630: step 2376, loss 0.656821, acc 0.9375, prec 0.056437, recall 0.872125
2017-12-10T14:56:53.122273: step 2377, loss 0.190002, acc 0.96875, prec 0.0564617, recall 0.872184
2017-12-10T14:56:53.317557: step 2378, loss 0.0913103, acc 0.984375, prec 0.05646, recall 0.872184
2017-12-10T14:56:53.507044: step 2379, loss 0.113508, acc 0.9375, prec 0.0564533, recall 0.872184
2017-12-10T14:56:53.701109: step 2380, loss 0.634265, acc 0.921875, prec 0.056501, recall 0.872301
2017-12-10T14:56:53.895756: step 2381, loss 0.164437, acc 0.921875, prec 0.0564926, recall 0.872301
2017-12-10T14:56:54.085327: step 2382, loss 0.224521, acc 0.9375, prec 0.0564859, recall 0.872301
2017-12-10T14:56:54.274495: step 2383, loss 0.218717, acc 0.96875, prec 0.0565387, recall 0.872419
2017-12-10T14:56:54.466370: step 2384, loss 0.394531, acc 0.84375, prec 0.0565499, recall 0.872477
2017-12-10T14:56:54.658288: step 2385, loss 0.28324, acc 0.890625, prec 0.0565382, recall 0.872477
2017-12-10T14:56:54.849008: step 2386, loss 0.221558, acc 0.890625, prec 0.0565264, recall 0.872477
2017-12-10T14:56:55.037278: step 2387, loss 0.446476, acc 0.875, prec 0.0566531, recall 0.872769
2017-12-10T14:56:55.230343: step 2388, loss 0.336233, acc 0.921875, prec 0.0567287, recall 0.872943
2017-12-10T14:56:55.423602: step 2389, loss 1.33958, acc 0.90625, prec 0.0567747, recall 0.873059
2017-12-10T14:56:55.615871: step 2390, loss 0.54195, acc 0.875, prec 0.0567892, recall 0.873117
2017-12-10T14:56:55.802283: step 2391, loss 0.151006, acc 0.9375, prec 0.0567824, recall 0.873117
2017-12-10T14:56:55.994111: step 2392, loss 0.584891, acc 0.828125, prec 0.0567639, recall 0.873117
2017-12-10T14:56:56.190683: step 2393, loss 0.355482, acc 0.828125, prec 0.0567454, recall 0.873117
2017-12-10T14:56:56.382257: step 2394, loss 0.168147, acc 0.9375, prec 0.0568226, recall 0.873291
2017-12-10T14:56:56.573893: step 2395, loss 0.353903, acc 0.84375, prec 0.0568057, recall 0.873291
2017-12-10T14:56:56.769461: step 2396, loss 0.318544, acc 0.921875, prec 0.0567973, recall 0.873291
2017-12-10T14:56:56.958705: step 2397, loss 0.393825, acc 0.875, prec 0.0568397, recall 0.873406
2017-12-10T14:56:57.155550: step 2398, loss 0.483628, acc 0.859375, prec 0.0568246, recall 0.873406
2017-12-10T14:56:57.348551: step 2399, loss 0.387808, acc 0.875, prec 0.0568111, recall 0.873406
2017-12-10T14:56:57.540293: step 2400, loss 0.244803, acc 0.890625, prec 0.0568273, recall 0.873464
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_0/1512935327/checkpoints/model-2400

2017-12-10T14:56:59.053483: step 2401, loss 0.251377, acc 0.921875, prec 0.0568747, recall 0.873579
2017-12-10T14:56:59.252956: step 2402, loss 0.237113, acc 0.921875, prec 0.0568663, recall 0.873579
2017-12-10T14:56:59.447090: step 2403, loss 0.123988, acc 0.953125, prec 0.056945, recall 0.873751
2017-12-10T14:56:59.639976: step 2404, loss 0.268783, acc 0.90625, prec 0.0569628, recall 0.873808
2017-12-10T14:56:59.832114: step 2405, loss 0.701395, acc 0.921875, prec 0.0569822, recall 0.873866
2017-12-10T14:57:00.023798: step 2406, loss 0.0652992, acc 0.984375, prec 0.0570085, recall 0.873923
2017-12-10T14:57:00.219376: step 2407, loss 0.19272, acc 0.9375, prec 0.0570017, recall 0.873923
2017-12-10T14:57:00.411387: step 2408, loss 0.426661, acc 0.890625, prec 0.0570178, recall 0.87398
2017-12-10T14:57:00.604295: step 2409, loss 0.228217, acc 0.90625, prec 0.0570077, recall 0.87398
2017-12-10T14:57:00.794842: step 2410, loss 0.144149, acc 0.953125, prec 0.0570026, recall 0.87398
2017-12-10T14:57:00.987469: step 2411, loss 0.210548, acc 0.953125, prec 0.0570255, recall 0.874037
2017-12-10T14:57:01.184586: step 2412, loss 0.632395, acc 0.96875, prec 0.05705, recall 0.874094
2017-12-10T14:57:01.379045: step 2413, loss 0.135238, acc 0.953125, prec 0.0570449, recall 0.874094
2017-12-10T14:57:01.572652: step 2414, loss 0.245566, acc 0.921875, prec 0.0570643, recall 0.874151
2017-12-10T14:57:01.771700: step 2415, loss 1.96958, acc 0.9375, prec 0.0571429, recall 0.873927
2017-12-10T14:57:01.970388: step 2416, loss 0.0767709, acc 0.984375, prec 0.057169, recall 0.873984
2017-12-10T14:57:02.163272: step 2417, loss 0.127344, acc 0.9375, prec 0.0571623, recall 0.873984
2017-12-10T14:57:02.351367: step 2418, loss 0.212113, acc 0.921875, prec 0.0571817, recall 0.874041
2017-12-10T14:57:02.541188: step 2419, loss 0.336069, acc 0.921875, prec 0.0572011, recall 0.874097
2017-12-10T14:57:02.730262: step 2420, loss 0.161788, acc 0.921875, prec 0.0572205, recall 0.874154
2017-12-10T14:57:02.924870: step 2421, loss 0.371617, acc 0.921875, prec 0.0572399, recall 0.874211
2017-12-10T14:57:03.118282: step 2422, loss 0.334712, acc 0.890625, prec 0.057228, recall 0.874211
2017-12-10T14:57:03.311094: step 2423, loss 0.47717, acc 0.875, prec 0.0572423, recall 0.874268
2017-12-10T14:57:03.503962: step 2424, loss 0.352663, acc 0.859375, prec 0.0572271, recall 0.874268
2017-12-10T14:57:03.698204: step 2425, loss 0.293301, acc 0.921875, prec 0.0572187, recall 0.874268
2017-12-10T14:57:03.889098: step 2426, loss 3.49229, acc 0.84375, prec 0.0572052, recall 0.87348
2017-12-10T14:57:04.086020: step 2427, loss 0.297981, acc 0.875, prec 0.0572195, recall 0.873537
2017-12-10T14:57:04.273720: step 2428, loss 0.993615, acc 0.859375, prec 0.0572877, recall 0.873708
2017-12-10T14:57:04.473056: step 2429, loss 0.478704, acc 0.84375, prec 0.0573541, recall 0.873878
2017-12-10T14:57:04.666397: step 2430, loss 0.809119, acc 0.78125, prec 0.057386, recall 0.873991
2017-12-10T14:57:04.862712: step 2431, loss 0.638049, acc 0.78125, prec 0.0573623, recall 0.873991
2017-12-10T14:57:05.054350: step 2432, loss 0.855708, acc 0.75, prec 0.0573353, recall 0.873991
2017-12-10T14:57:05.246117: step 2433, loss 0.586528, acc 0.75, prec 0.0573084, recall 0.873991
2017-12-10T14:57:05.441579: step 2434, loss 0.437685, acc 0.84375, prec 0.0573192, recall 0.874048
2017-12-10T14:57:05.637529: step 2435, loss 0.37244, acc 0.828125, prec 0.0573284, recall 0.874104
2017-12-10T14:57:05.828201: step 2436, loss 0.725969, acc 0.78125, prec 0.0573325, recall 0.87416
2017-12-10T14:57:06.024425: step 2437, loss 0.591601, acc 0.8125, prec 0.0573123, recall 0.87416
2017-12-10T14:57:06.217064: step 2438, loss 0.364139, acc 0.84375, prec 0.0572955, recall 0.87416
2017-12-10T14:57:06.413607: step 2439, loss 0.233351, acc 0.90625, prec 0.0573407, recall 0.874273
2017-12-10T14:57:06.603677: step 2440, loss 0.512851, acc 0.875, prec 0.0573273, recall 0.874273
2017-12-10T14:57:06.793053: step 2441, loss 0.275917, acc 0.9375, prec 0.0573482, recall 0.874329
2017-12-10T14:57:06.986057: step 2442, loss 0.235833, acc 0.890625, prec 0.0573917, recall 0.874441
2017-12-10T14:57:07.181587: step 2443, loss 0.0953049, acc 0.96875, prec 0.0573883, recall 0.874441
2017-12-10T14:57:07.373222: step 2444, loss 0.175496, acc 0.953125, prec 0.0573833, recall 0.874441
2017-12-10T14:57:07.569514: step 2445, loss 0.27923, acc 0.953125, prec 0.0573783, recall 0.874441
2017-12-10T14:57:07.760501: step 2446, loss 0.248113, acc 0.90625, prec 0.0573682, recall 0.874441
2017-12-10T14:57:07.952044: step 2447, loss 0.236537, acc 0.90625, prec 0.0573581, recall 0.874441
2017-12-10T14:57:08.144220: step 2448, loss 0.0387189, acc 0.984375, prec 0.057384, recall 0.874498
2017-12-10T14:57:08.338474: step 2449, loss 0.296819, acc 0.984375, prec 0.05741, recall 0.874554
2017-12-10T14:57:08.528527: step 2450, loss 0.697902, acc 0.984375, prec 0.0574359, recall 0.87461
2017-12-10T14:57:08.728996: step 2451, loss 0.0807025, acc 0.96875, prec 0.0574325, recall 0.87461
2017-12-10T14:57:08.921608: step 2452, loss 0.240335, acc 1, prec 0.0574878, recall 0.874721
2017-12-10T14:57:09.119439: step 2453, loss 0.0358556, acc 0.984375, prec 0.0574861, recall 0.874721
2017-12-10T14:57:09.312649: step 2454, loss 0.0588953, acc 0.984375, prec 0.0574844, recall 0.874721
2017-12-10T14:57:09.503319: step 2455, loss 0.0292629, acc 0.984375, prec 0.0575103, recall 0.874777
2017-12-10T14:57:09.701646: step 2456, loss 0.121254, acc 0.96875, prec 0.0575622, recall 0.874889
2017-12-10T14:57:09.896435: step 2457, loss 0.171881, acc 0.953125, prec 0.0575847, recall 0.874944
2017-12-10T14:57:10.091844: step 2458, loss 0.0336809, acc 0.984375, prec 0.057583, recall 0.874944
2017-12-10T14:57:10.284201: step 2459, loss 0.278799, acc 0.921875, prec 0.0575746, recall 0.874944
2017-12-10T14:57:10.480930: step 2460, loss 0.216661, acc 0.96875, prec 0.0575988, recall 0.875
2017-12-10T14:57:10.672879: step 2461, loss 0.740691, acc 0.984375, prec 0.0576523, recall 0.875111
2017-12-10T14:57:10.866155: step 2462, loss 0.0566191, acc 0.96875, prec 0.0576765, recall 0.875167
2017-12-10T14:57:11.058302: step 2463, loss 0.0616541, acc 0.96875, prec 0.0576732, recall 0.875167
2017-12-10T14:57:11.254370: step 2464, loss 0.0797235, acc 0.96875, prec 0.0576974, recall 0.875222
2017-12-10T14:57:11.444734: step 2465, loss 0.225717, acc 0.96875, prec 0.0577216, recall 0.875277
2017-12-10T14:57:11.640353: step 2466, loss 4.52393, acc 0.953125, prec 0.0577458, recall 0.874945
2017-12-10T14:57:11.837410: step 2467, loss 0.796014, acc 0.96875, prec 0.05777, recall 0.875
2017-12-10T14:57:12.034448: step 2468, loss 0.0743249, acc 0.96875, prec 0.0577666, recall 0.875
2017-12-10T14:57:12.227798: step 2469, loss 0.445561, acc 0.90625, prec 0.057784, recall 0.875055
2017-12-10T14:57:12.418982: step 2470, loss 3.03486, acc 0.90625, prec 0.0578031, recall 0.874723
2017-12-10T14:57:12.610982: step 2471, loss 0.275329, acc 0.875, prec 0.0577896, recall 0.874723
2017-12-10T14:57:12.805693: step 2472, loss 0.525807, acc 0.765625, prec 0.0577643, recall 0.874723
2017-12-10T14:57:12.998876: step 2473, loss 0.560521, acc 0.78125, prec 0.0577682, recall 0.874779
2017-12-10T14:57:13.188790: step 2474, loss 0.791466, acc 0.71875, prec 0.0577653, recall 0.874834
2017-12-10T14:57:13.378157: step 2475, loss 0.924094, acc 0.734375, prec 0.0577642, recall 0.874889
2017-12-10T14:57:13.570780: step 2476, loss 0.844781, acc 0.765625, prec 0.0577389, recall 0.874889
2017-12-10T14:57:13.765843: step 2477, loss 1.23575, acc 0.671875, prec 0.0577035, recall 0.874889
2017-12-10T14:57:13.953916: step 2478, loss 0.717369, acc 0.78125, prec 0.0577074, recall 0.874945
2017-12-10T14:57:14.153253: step 2479, loss 0.686169, acc 0.765625, prec 0.0577097, recall 0.875
2017-12-10T14:57:14.343120: step 2480, loss 1.0777, acc 0.65625, prec 0.0577001, recall 0.875055
2017-12-10T14:57:14.536765: step 2481, loss 0.601177, acc 0.765625, prec 0.057675, recall 0.875055
2017-12-10T14:57:14.726556: step 2482, loss 0.685049, acc 0.796875, prec 0.0577354, recall 0.87522
2017-12-10T14:57:14.918443: step 2483, loss 0.343931, acc 0.796875, prec 0.0577409, recall 0.875275
2017-12-10T14:57:15.108312: step 2484, loss 0.26708, acc 0.921875, prec 0.0577326, recall 0.875275
2017-12-10T14:57:15.281428: step 2485, loss 0.709506, acc 0.843137, prec 0.0577465, recall 0.87533
2017-12-10T14:57:15.478849: step 2486, loss 0.426651, acc 0.875, prec 0.0577605, recall 0.875385
2017-12-10T14:57:15.670133: step 2487, loss 0.415336, acc 0.859375, prec 0.0577727, recall 0.87544
2017-12-10T14:57:15.864453: step 2488, loss 0.332881, acc 0.890625, prec 0.0578157, recall 0.87555
2017-12-10T14:57:16.060187: step 2489, loss 0.202707, acc 0.921875, prec 0.0578073, recall 0.87555
2017-12-10T14:57:16.254195: step 2490, loss 0.235247, acc 0.953125, prec 0.0578023, recall 0.87555
2017-12-10T14:57:16.444593: step 2491, loss 0.132762, acc 0.96875, prec 0.0578536, recall 0.875659
2017-12-10T14:57:16.640350: step 2492, loss 0.757921, acc 0.90625, prec 0.0579803, recall 0.875932
2017-12-10T14:57:16.833681: step 2493, loss 0.179953, acc 0.921875, prec 0.0580265, recall 0.87604
2017-12-10T14:57:17.028834: step 2494, loss 0.706307, acc 0.9375, prec 0.0580744, recall 0.876149
2017-12-10T14:57:17.226706: step 2495, loss 0.143223, acc 0.953125, prec 0.0580967, recall 0.876203
2017-12-10T14:57:17.418347: step 2496, loss 4.0322, acc 0.9375, prec 0.0580916, recall 0.87582
2017-12-10T14:57:17.611868: step 2497, loss 0.14655, acc 0.921875, prec 0.0580832, recall 0.87582
2017-12-10T14:57:17.809727: step 2498, loss 0.405793, acc 0.890625, prec 0.0580714, recall 0.87582
2017-12-10T14:57:18.003895: step 2499, loss 0.408126, acc 0.921875, prec 0.0580903, recall 0.875874
2017-12-10T14:57:18.196937: step 2500, loss 0.198599, acc 0.921875, prec 0.0580819, recall 0.875874
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_0/1512935327/checkpoints/model-2500

2017-12-10T14:57:19.494397: step 2501, loss 0.26973, acc 0.890625, prec 0.0581247, recall 0.875983
2017-12-10T14:57:19.686251: step 2502, loss 0.270517, acc 0.875, prec 0.0581658, recall 0.876091
2017-12-10T14:57:19.875289: step 2503, loss 0.554856, acc 0.859375, prec 0.0581779, recall 0.876145
2017-12-10T14:57:20.064881: step 2504, loss 0.451377, acc 0.890625, prec 0.0581934, recall 0.876199
2017-12-10T14:57:20.257351: step 2505, loss 0.459735, acc 0.84375, prec 0.0581766, recall 0.876199
2017-12-10T14:57:20.446183: step 2506, loss 0.485686, acc 0.921875, prec 0.0581954, recall 0.876253
2017-12-10T14:57:20.643100: step 2507, loss 0.359817, acc 0.890625, prec 0.0581836, recall 0.876253
2017-12-10T14:57:20.832784: step 2508, loss 0.26385, acc 0.875, prec 0.0581974, recall 0.876307
2017-12-10T14:57:21.022181: step 2509, loss 0.381076, acc 0.890625, prec 0.0582128, recall 0.87636
2017-12-10T14:57:21.211759: step 2510, loss 0.348309, acc 0.90625, prec 0.05823, recall 0.876414
2017-12-10T14:57:21.408345: step 2511, loss 0.0870388, acc 0.96875, prec 0.0582538, recall 0.876468
2017-12-10T14:57:21.603923: step 2512, loss 0.124568, acc 0.953125, prec 0.058276, recall 0.876522
2017-12-10T14:57:21.799616: step 2513, loss 0.205222, acc 0.890625, prec 0.0582642, recall 0.876522
2017-12-10T14:57:21.997417: step 2514, loss 0.443632, acc 0.90625, prec 0.0582541, recall 0.876522
2017-12-10T14:57:22.192864: step 2515, loss 0.311784, acc 0.90625, prec 0.058244, recall 0.876522
2017-12-10T14:57:22.390299: step 2516, loss 0.28897, acc 0.921875, prec 0.0582356, recall 0.876522
2017-12-10T14:57:22.583134: step 2517, loss 0.696003, acc 0.9375, prec 0.0582833, recall 0.876629
2017-12-10T14:57:22.775414: step 2518, loss 0.132568, acc 0.9375, prec 0.0582765, recall 0.876629
2017-12-10T14:57:22.970764: step 2519, loss 0.242252, acc 0.984375, prec 0.0583564, recall 0.87679
2017-12-10T14:57:23.168356: step 2520, loss 0.136096, acc 0.953125, prec 0.0583514, recall 0.87679
2017-12-10T14:57:23.361924: step 2521, loss 0.17056, acc 0.9375, prec 0.0583446, recall 0.87679
2017-12-10T14:57:23.552667: step 2522, loss 0.337245, acc 0.859375, prec 0.0583567, recall 0.876843
2017-12-10T14:57:23.749273: step 2523, loss 0.180338, acc 0.9375, prec 0.0583499, recall 0.876843
2017-12-10T14:57:23.943243: step 2524, loss 0.42378, acc 0.984375, prec 0.0584026, recall 0.87695
2017-12-10T14:57:24.139430: step 2525, loss 0.150019, acc 0.9375, prec 0.058423, recall 0.877003
2017-12-10T14:57:24.329045: step 2526, loss 0.169534, acc 0.9375, prec 0.0584434, recall 0.877056
2017-12-10T14:57:24.522546: step 2527, loss 0.0500662, acc 0.984375, prec 0.0584417, recall 0.877056
2017-12-10T14:57:24.716080: step 2528, loss 0.185595, acc 0.953125, prec 0.0584638, recall 0.877109
2017-12-10T14:57:24.907382: step 2529, loss 0.199893, acc 0.953125, prec 0.0584588, recall 0.877109
2017-12-10T14:57:25.104591: step 2530, loss 0.0255119, acc 1, prec 0.0584588, recall 0.877109
2017-12-10T14:57:25.303502: step 2531, loss 0.104299, acc 0.984375, prec 0.0584571, recall 0.877109
2017-12-10T14:57:25.500996: step 2532, loss 0.0288639, acc 1, prec 0.0584571, recall 0.877109
2017-12-10T14:57:25.694326: step 2533, loss 0.038448, acc 0.984375, prec 0.0584554, recall 0.877109
2017-12-10T14:57:25.885807: step 2534, loss 0.0646445, acc 0.984375, prec 0.0584537, recall 0.877109
2017-12-10T14:57:26.077058: step 2535, loss 0.125764, acc 0.953125, prec 0.0584487, recall 0.877109
2017-12-10T14:57:26.268814: step 2536, loss 0.180742, acc 0.953125, prec 0.0584708, recall 0.877163
2017-12-10T14:57:26.457373: step 2537, loss 0.0704829, acc 0.96875, prec 0.0584674, recall 0.877163
2017-12-10T14:57:26.653377: step 2538, loss 0.0522648, acc 0.984375, prec 0.05852, recall 0.877269
2017-12-10T14:57:26.844956: step 2539, loss 0.252654, acc 0.953125, prec 0.0585963, recall 0.877428
2017-12-10T14:57:27.033025: step 2540, loss 0.11035, acc 0.984375, prec 0.0586489, recall 0.877533
2017-12-10T14:57:27.231060: step 2541, loss 0.0373442, acc 0.984375, prec 0.0586472, recall 0.877533
2017-12-10T14:57:27.424005: step 2542, loss 0.0577932, acc 0.984375, prec 0.0586455, recall 0.877533
2017-12-10T14:57:27.617379: step 2543, loss 0.133396, acc 0.96875, prec 0.0586422, recall 0.877533
2017-12-10T14:57:27.810784: step 2544, loss 0.0556989, acc 0.984375, prec 0.0586676, recall 0.877586
2017-12-10T14:57:28.002648: step 2545, loss 0.117663, acc 1, prec 0.0586947, recall 0.877639
2017-12-10T14:57:28.192593: step 2546, loss 2.98293, acc 0.96875, prec 0.058693, recall 0.877261
2017-12-10T14:57:28.389030: step 2547, loss 0.125196, acc 0.9375, prec 0.0586863, recall 0.877261
2017-12-10T14:57:28.580550: step 2548, loss 0.636717, acc 0.953125, prec 0.0587083, recall 0.877314
2017-12-10T14:57:28.779172: step 2549, loss 0.12368, acc 0.984375, prec 0.0587066, recall 0.877314
2017-12-10T14:57:28.973644: step 2550, loss 0.353709, acc 0.953125, prec 0.0587287, recall 0.877367
2017-12-10T14:57:29.172689: step 2551, loss 0.0470456, acc 0.984375, prec 0.0587541, recall 0.877419
2017-12-10T14:57:29.368190: step 2552, loss 0.31168, acc 0.921875, prec 0.0587998, recall 0.877525
2017-12-10T14:57:29.562352: step 2553, loss 0.0246528, acc 1, prec 0.0587998, recall 0.877525
2017-12-10T14:57:29.752452: step 2554, loss 0.208359, acc 0.9375, prec 0.058793, recall 0.877525
2017-12-10T14:57:29.943281: step 2555, loss 0.316763, acc 0.921875, prec 0.0588117, recall 0.877577
2017-12-10T14:57:30.137803: step 2556, loss 0.121658, acc 0.953125, prec 0.0588608, recall 0.877682
2017-12-10T14:57:30.333833: step 2557, loss 0.0801719, acc 0.96875, prec 0.0588574, recall 0.877682
2017-12-10T14:57:30.529989: step 2558, loss 0.186704, acc 0.96875, prec 0.0588811, recall 0.877735
2017-12-10T14:57:30.721608: step 2559, loss 0.0705144, acc 0.984375, prec 0.0588794, recall 0.877735
2017-12-10T14:57:30.911715: step 2560, loss 0.223194, acc 0.9375, prec 0.0588726, recall 0.877735
2017-12-10T14:57:31.104364: step 2561, loss 0.18954, acc 0.96875, prec 0.0588963, recall 0.877787
2017-12-10T14:57:31.299986: step 2562, loss 0.0937163, acc 0.953125, prec 0.0588912, recall 0.877787
2017-12-10T14:57:31.504364: step 2563, loss 0.225648, acc 0.90625, prec 0.0589081, recall 0.87784
2017-12-10T14:57:31.696012: step 2564, loss 0.286582, acc 0.90625, prec 0.058925, recall 0.877892
2017-12-10T14:57:31.889287: step 2565, loss 0.188362, acc 0.921875, prec 0.0589166, recall 0.877892
2017-12-10T14:57:32.083757: step 2566, loss 0.0764427, acc 0.96875, prec 0.0589132, recall 0.877892
2017-12-10T14:57:32.278926: step 2567, loss 3.9997, acc 0.9375, prec 0.0590163, recall 0.877726
2017-12-10T14:57:32.476921: step 2568, loss 0.830708, acc 0.953125, prec 0.0590653, recall 0.87783
2017-12-10T14:57:32.672340: step 2569, loss 0.458368, acc 0.90625, prec 0.0591092, recall 0.877934
2017-12-10T14:57:32.864698: step 2570, loss 0.319405, acc 0.859375, prec 0.0590939, recall 0.877934
2017-12-10T14:57:33.058026: step 2571, loss 0.251072, acc 0.875, prec 0.0591074, recall 0.877986
2017-12-10T14:57:33.247355: step 2572, loss 0.162348, acc 0.953125, prec 0.0591293, recall 0.878038
2017-12-10T14:57:33.439780: step 2573, loss 0.390643, acc 0.84375, prec 0.0591663, recall 0.878142
2017-12-10T14:57:33.631591: step 2574, loss 0.551302, acc 0.8125, prec 0.059173, recall 0.878194
2017-12-10T14:57:33.822965: step 2575, loss 0.368198, acc 0.890625, prec 0.0591611, recall 0.878194
2017-12-10T14:57:34.015520: step 2576, loss 0.356518, acc 0.90625, prec 0.0591779, recall 0.878246
2017-12-10T14:57:34.208124: step 2577, loss 0.426909, acc 0.8125, prec 0.0591845, recall 0.878298
2017-12-10T14:57:34.399190: step 2578, loss 0.377747, acc 0.875, prec 0.0591979, recall 0.87835
2017-12-10T14:57:34.594711: step 2579, loss 0.24288, acc 0.890625, prec 0.059213, recall 0.878401
2017-12-10T14:57:34.785667: step 2580, loss 0.292454, acc 0.875, prec 0.0592264, recall 0.878453
2017-12-10T14:57:34.978347: step 2581, loss 0.304476, acc 0.875, prec 0.0592397, recall 0.878505
2017-12-10T14:57:35.167396: step 2582, loss 0.314827, acc 0.921875, prec 0.0592313, recall 0.878505
2017-12-10T14:57:35.358594: step 2583, loss 0.415161, acc 0.875, prec 0.0592446, recall 0.878556
2017-12-10T14:57:35.546608: step 2584, loss 0.330474, acc 0.875, prec 0.059258, recall 0.878608
2017-12-10T14:57:35.734544: step 2585, loss 0.197802, acc 0.9375, prec 0.0593051, recall 0.878711
2017-12-10T14:57:35.924293: step 2586, loss 0.274142, acc 0.875, prec 0.0593184, recall 0.878762
2017-12-10T14:57:36.118579: step 2587, loss 0.120374, acc 0.9375, prec 0.0593116, recall 0.878762
2017-12-10T14:57:36.310418: step 2588, loss 0.204208, acc 0.921875, prec 0.05933, recall 0.878814
2017-12-10T14:57:36.499923: step 2589, loss 0.505368, acc 0.859375, prec 0.0593955, recall 0.878967
2017-12-10T14:57:36.696630: step 2590, loss 0.155674, acc 0.9375, prec 0.0593887, recall 0.878967
2017-12-10T14:57:36.887376: step 2591, loss 0.110608, acc 0.953125, prec 0.0593836, recall 0.878967
2017-12-10T14:57:37.078510: step 2592, loss 0.151854, acc 0.9375, prec 0.0593768, recall 0.878967
2017-12-10T14:57:37.272050: step 2593, loss 0.212568, acc 0.890625, prec 0.0593918, recall 0.879019
2017-12-10T14:57:38.068668: step 2594, loss 0.279869, acc 0.953125, prec 0.0594136, recall 0.87907
2017-12-10T14:57:38.264458: step 2595, loss 0.26665, acc 0.96875, prec 0.0594639, recall 0.879172
2017-12-10T14:57:38.456790: step 2596, loss 0.0359812, acc 0.984375, prec 0.0594622, recall 0.879172
2017-12-10T14:57:38.649131: step 2597, loss 0.125893, acc 0.9375, prec 0.0594823, recall 0.879223
2017-12-10T14:57:38.841391: step 2598, loss 0.164423, acc 0.953125, prec 0.0595041, recall 0.879274
2017-12-10T14:57:39.032463: step 2599, loss 0.0951832, acc 0.953125, prec 0.059499, recall 0.879274
2017-12-10T14:57:39.222578: step 2600, loss 0.0375707, acc 0.984375, prec 0.0594973, recall 0.879274
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_0/1512935327/checkpoints/model-2600

2017-12-10T14:57:41.156213: step 2601, loss 3.195, acc 0.984375, prec 0.059551, recall 0.879005
2017-12-10T14:57:41.353295: step 2602, loss 0.249875, acc 0.984375, prec 0.0595762, recall 0.879056
2017-12-10T14:57:41.548925: step 2603, loss 0.0692381, acc 0.96875, prec 0.0595728, recall 0.879056
2017-12-10T14:57:41.742870: step 2604, loss 0.186024, acc 0.9375, prec 0.0595928, recall 0.879107
2017-12-10T14:57:41.934352: step 2605, loss 0.130854, acc 0.953125, prec 0.0596146, recall 0.879158
2017-12-10T14:57:42.128487: step 2606, loss 0.15041, acc 0.96875, prec 0.059638, recall 0.879209
2017-12-10T14:57:42.320731: step 2607, loss 0.0972348, acc 0.96875, prec 0.0596346, recall 0.879209
2017-12-10T14:57:42.514560: step 2608, loss 0.295421, acc 0.9375, prec 0.0596546, recall 0.87926
2017-12-10T14:57:42.703617: step 2609, loss 0.283183, acc 0.890625, prec 0.0596427, recall 0.87926
2017-12-10T14:57:42.899062: step 2610, loss 0.161618, acc 0.96875, prec 0.0596661, recall 0.87931
2017-12-10T14:57:43.094403: step 2611, loss 0.269845, acc 0.921875, prec 0.0596576, recall 0.87931
2017-12-10T14:57:43.287064: step 2612, loss 0.926035, acc 0.921875, prec 0.0597028, recall 0.879412
2017-12-10T14:57:43.479523: step 2613, loss 0.24842, acc 0.953125, prec 0.0597245, recall 0.879462
2017-12-10T14:57:43.672067: step 2614, loss 0.183974, acc 0.953125, prec 0.0597462, recall 0.879513
2017-12-10T14:57:43.867081: step 2615, loss 0.40795, acc 0.890625, prec 0.0597611, recall 0.879564
2017-12-10T14:57:44.066105: step 2616, loss 0.512574, acc 0.921875, prec 0.059833, recall 0.879715
2017-12-10T14:57:44.270353: step 2617, loss 0.174614, acc 0.875, prec 0.0598461, recall 0.879765
2017-12-10T14:57:44.459587: step 2618, loss 0.274931, acc 0.890625, prec 0.0598342, recall 0.879765
2017-12-10T14:57:44.649006: step 2619, loss 0.13299, acc 0.9375, prec 0.0598274, recall 0.879765
2017-12-10T14:57:44.843048: step 2620, loss 0.235083, acc 0.921875, prec 0.0598456, recall 0.879816
2017-12-10T14:57:45.035513: step 2621, loss 0.200391, acc 0.9375, prec 0.0598388, recall 0.879816
2017-12-10T14:57:45.232396: step 2622, loss 0.226968, acc 0.90625, prec 0.0598286, recall 0.879816
2017-12-10T14:57:45.422666: step 2623, loss 0.769654, acc 0.953125, prec 0.059877, recall 0.879916
2017-12-10T14:57:45.614134: step 2624, loss 0.316548, acc 0.890625, prec 0.0598651, recall 0.879916
2017-12-10T14:57:45.804352: step 2625, loss 0.30567, acc 0.890625, prec 0.0598531, recall 0.879916
2017-12-10T14:57:45.994344: step 2626, loss 0.209954, acc 0.9375, prec 0.0598731, recall 0.879967
2017-12-10T14:57:46.190637: step 2627, loss 0.205628, acc 0.9375, prec 0.059893, recall 0.880017
2017-12-10T14:57:46.382181: step 2628, loss 0.0993623, acc 0.96875, prec 0.0598896, recall 0.880017
2017-12-10T14:57:46.575015: step 2629, loss 0.311929, acc 0.9375, prec 0.0598828, recall 0.880017
2017-12-10T14:57:46.767620: step 2630, loss 0.266484, acc 0.90625, prec 0.0598726, recall 0.880017
2017-12-10T14:57:46.957332: step 2631, loss 0.392581, acc 0.921875, prec 0.0599175, recall 0.880117
2017-12-10T14:57:47.149611: step 2632, loss 0.0763674, acc 0.984375, prec 0.0599158, recall 0.880117
2017-12-10T14:57:47.349004: step 2633, loss 0.100004, acc 0.953125, prec 0.0599107, recall 0.880117
2017-12-10T14:57:47.541783: step 2634, loss 0.164032, acc 0.984375, prec 0.0599357, recall 0.880167
2017-12-10T14:57:47.732891: step 2635, loss 0.258104, acc 0.90625, prec 0.0599255, recall 0.880167
2017-12-10T14:57:47.931120: step 2636, loss 0.186173, acc 0.921875, prec 0.059917, recall 0.880167
2017-12-10T14:57:48.124167: step 2637, loss 0.136868, acc 0.96875, prec 0.0599937, recall 0.880317
2017-12-10T14:57:48.321492: step 2638, loss 0.159013, acc 0.96875, prec 0.060017, recall 0.880367
2017-12-10T14:57:48.515185: step 2639, loss 0.104935, acc 0.96875, prec 0.0600136, recall 0.880367
2017-12-10T14:57:48.708529: step 2640, loss 1.53254, acc 0.984375, prec 0.0600403, recall 0.88005
2017-12-10T14:57:48.906494: step 2641, loss 0.195922, acc 1, prec 0.0601205, recall 0.8802
2017-12-10T14:57:49.099642: step 2642, loss 0.0714592, acc 0.96875, prec 0.0601171, recall 0.8802
2017-12-10T14:57:49.295431: step 2643, loss 0.116472, acc 0.921875, prec 0.0601085, recall 0.8802
2017-12-10T14:57:49.486589: step 2644, loss 0.0842334, acc 0.96875, prec 0.0601318, recall 0.88025
2017-12-10T14:57:49.679434: step 2645, loss 0.115927, acc 0.96875, prec 0.0602085, recall 0.880399
2017-12-10T14:57:49.873176: step 2646, loss 0.308374, acc 0.953125, prec 0.06023, recall 0.880448
2017-12-10T14:57:50.067075: step 2647, loss 0.330255, acc 0.921875, prec 0.0602481, recall 0.880498
2017-12-10T14:57:50.266658: step 2648, loss 0.0560742, acc 0.984375, prec 0.0602731, recall 0.880547
2017-12-10T14:57:50.463442: step 2649, loss 0.799387, acc 0.9375, prec 0.0603196, recall 0.880647
2017-12-10T14:57:50.656453: step 2650, loss 0.220694, acc 0.921875, prec 0.0603111, recall 0.880647
2017-12-10T14:57:50.846221: step 2651, loss 3.97599, acc 0.90625, prec 0.0603292, recall 0.880331
2017-12-10T14:57:51.040684: step 2652, loss 0.226136, acc 0.921875, prec 0.0603206, recall 0.880331
2017-12-10T14:57:51.231338: step 2653, loss 0.27209, acc 0.90625, prec 0.060337, recall 0.880381
2017-12-10T14:57:51.426569: step 2654, loss 0.208447, acc 0.890625, prec 0.0603517, recall 0.88043
2017-12-10T14:57:51.623874: step 2655, loss 0.571576, acc 0.84375, prec 0.0603612, recall 0.88048
2017-12-10T14:57:51.814577: step 2656, loss 0.540277, acc 0.78125, prec 0.0603905, recall 0.880579
2017-12-10T14:57:52.005078: step 2657, loss 0.931814, acc 0.78125, prec 0.0603932, recall 0.880628
2017-12-10T14:57:52.196497: step 2658, loss 0.778966, acc 0.796875, prec 0.0604242, recall 0.880726
2017-12-10T14:57:52.384763: step 2659, loss 0.501551, acc 0.84375, prec 0.0604336, recall 0.880776
2017-12-10T14:57:52.579540: step 2660, loss 0.502383, acc 0.796875, prec 0.0604114, recall 0.880776
2017-12-10T14:57:52.767391: step 2661, loss 0.540744, acc 0.75, prec 0.0604107, recall 0.880825
2017-12-10T14:57:52.956046: step 2662, loss 0.511589, acc 0.734375, prec 0.0604347, recall 0.880923
2017-12-10T14:57:53.144026: step 2663, loss 0.849348, acc 0.765625, prec 0.0604091, recall 0.880923
2017-12-10T14:57:53.335446: step 2664, loss 0.436161, acc 0.875, prec 0.0603955, recall 0.880923
2017-12-10T14:57:53.527874: step 2665, loss 0.210613, acc 0.921875, prec 0.0604135, recall 0.880972
2017-12-10T14:57:53.721721: step 2666, loss 0.258687, acc 0.875, prec 0.0603998, recall 0.880972
2017-12-10T14:57:53.914555: step 2667, loss 0.389728, acc 0.90625, prec 0.0603896, recall 0.880972
2017-12-10T14:57:54.108358: step 2668, loss 0.339943, acc 0.90625, prec 0.0603794, recall 0.880972
2017-12-10T14:57:54.296299: step 2669, loss 0.305658, acc 0.890625, prec 0.0603675, recall 0.880972
2017-12-10T14:57:54.491944: step 2670, loss 0.205991, acc 0.921875, prec 0.0603855, recall 0.881021
2017-12-10T14:57:54.682985: step 2671, loss 0.153044, acc 0.9375, prec 0.0603786, recall 0.881021
2017-12-10T14:57:54.874454: step 2672, loss 0.253941, acc 0.890625, prec 0.0603667, recall 0.881021
2017-12-10T14:57:55.068753: step 2673, loss 0.321151, acc 0.9375, prec 0.0604129, recall 0.881119
2017-12-10T14:57:55.258431: step 2674, loss 0.89204, acc 0.859375, prec 0.0604241, recall 0.881168
2017-12-10T14:57:55.452384: step 2675, loss 0.0874434, acc 0.984375, prec 0.0604489, recall 0.881217
2017-12-10T14:57:55.641813: step 2676, loss 0.452647, acc 0.921875, prec 0.0604933, recall 0.881314
2017-12-10T14:57:55.832910: step 2677, loss 0.154507, acc 0.953125, prec 0.0605147, recall 0.881363
2017-12-10T14:57:56.024809: step 2678, loss 0.190794, acc 0.9375, prec 0.0605078, recall 0.881363
2017-12-10T14:57:56.217507: step 2679, loss 0.180539, acc 0.921875, prec 0.0604993, recall 0.881363
2017-12-10T14:57:56.407391: step 2680, loss 0.228989, acc 0.953125, prec 0.0605736, recall 0.881509
2017-12-10T14:57:56.601924: step 2681, loss 0.332524, acc 0.984375, prec 0.0605984, recall 0.881557
2017-12-10T14:57:56.796204: step 2682, loss 0.288936, acc 0.90625, prec 0.0605881, recall 0.881557
2017-12-10T14:57:56.986351: step 2683, loss 0.353972, acc 0.859375, prec 0.0605728, recall 0.881557
2017-12-10T14:57:57.176672: step 2684, loss 0.226687, acc 0.953125, prec 0.0605941, recall 0.881606
2017-12-10T14:57:57.367752: step 2685, loss 0.246877, acc 0.984375, prec 0.0606189, recall 0.881654
2017-12-10T14:57:57.558991: step 2686, loss 0.20947, acc 0.921875, prec 0.0606103, recall 0.881654
2017-12-10T14:57:57.749001: step 2687, loss 0.0786885, acc 0.96875, prec 0.0606069, recall 0.881654
2017-12-10T14:57:57.947579: step 2688, loss 0.126727, acc 0.9375, prec 0.0606265, recall 0.881703
2017-12-10T14:57:58.144432: step 2689, loss 0.1359, acc 0.9375, prec 0.0606461, recall 0.881751
2017-12-10T14:57:58.337056: step 2690, loss 0.175515, acc 0.953125, prec 0.060641, recall 0.881751
2017-12-10T14:57:58.531937: step 2691, loss 0.191908, acc 0.921875, prec 0.0606589, recall 0.8818
2017-12-10T14:57:58.727548: step 2692, loss 0.230173, acc 0.953125, prec 0.0606802, recall 0.881848
2017-12-10T14:57:58.923843: step 2693, loss 1.80247, acc 0.96875, prec 0.0606785, recall 0.881488
2017-12-10T14:57:59.128836: step 2694, loss 1.42337, acc 0.90625, prec 0.0607228, recall 0.881225
2017-12-10T14:57:59.328591: step 2695, loss 0.0477959, acc 0.984375, prec 0.0607211, recall 0.881225
2017-12-10T14:57:59.520477: step 2696, loss 0.202726, acc 0.953125, prec 0.0607424, recall 0.881273
2017-12-10T14:57:59.710418: step 2697, loss 0.21385, acc 0.90625, prec 0.0607322, recall 0.881273
2017-12-10T14:57:59.902425: step 2698, loss 0.276841, acc 0.890625, prec 0.0607202, recall 0.881273
2017-12-10T14:58:00.092926: step 2699, loss 0.170465, acc 0.953125, prec 0.0607151, recall 0.881273
2017-12-10T14:58:00.284140: step 2700, loss 0.382753, acc 0.890625, prec 0.0607031, recall 0.881273
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_0/1512935327/checkpoints/model-2700

2017-12-10T14:58:01.611981: step 2701, loss 0.269289, acc 0.875, prec 0.0606895, recall 0.881273
2017-12-10T14:58:01.805230: step 2702, loss 0.350801, acc 0.921875, prec 0.0607338, recall 0.88137
2017-12-10T14:58:01.996197: step 2703, loss 0.543731, acc 0.890625, prec 0.0607482, recall 0.881418
2017-12-10T14:58:02.186310: step 2704, loss 0.331319, acc 0.9375, prec 0.0607941, recall 0.881515
2017-12-10T14:58:02.379593: step 2705, loss 0.518804, acc 0.921875, prec 0.0608383, recall 0.881611
2017-12-10T14:58:02.574974: step 2706, loss 0.431676, acc 0.890625, prec 0.0608527, recall 0.881659
2017-12-10T14:58:02.766484: step 2707, loss 0.671615, acc 0.859375, prec 0.0608374, recall 0.881659
2017-12-10T14:58:02.956431: step 2708, loss 0.354643, acc 0.890625, prec 0.0608781, recall 0.881755
2017-12-10T14:58:03.152002: step 2709, loss 0.492822, acc 0.90625, prec 0.0608942, recall 0.881803
2017-12-10T14:58:03.346235: step 2710, loss 0.243284, acc 0.90625, prec 0.060884, recall 0.881803
2017-12-10T14:58:03.538213: step 2711, loss 0.231996, acc 0.921875, prec 0.0609281, recall 0.881899
2017-12-10T14:58:03.733914: step 2712, loss 0.527895, acc 0.84375, prec 0.0609373, recall 0.881947
2017-12-10T14:58:03.923811: step 2713, loss 0.105357, acc 0.96875, prec 0.0609339, recall 0.881947
2017-12-10T14:58:04.116026: step 2714, loss 0.303233, acc 0.890625, prec 0.060922, recall 0.881947
2017-12-10T14:58:04.311551: step 2715, loss 0.235133, acc 0.9375, prec 0.0609151, recall 0.881947
2017-12-10T14:58:04.507306: step 2716, loss 0.28529, acc 0.9375, prec 0.0609346, recall 0.881995
2017-12-10T14:58:04.697244: step 2717, loss 0.162395, acc 0.890625, prec 0.060949, recall 0.882043
2017-12-10T14:58:04.888839: step 2718, loss 0.371482, acc 0.890625, prec 0.0609896, recall 0.882138
2017-12-10T14:58:05.083347: step 2719, loss 0.921942, acc 0.921875, prec 0.0610599, recall 0.882282
2017-12-10T14:58:05.277397: step 2720, loss 0.0487646, acc 0.984375, prec 0.0610845, recall 0.882329
2017-12-10T14:58:05.472987: step 2721, loss 0.230216, acc 0.921875, prec 0.0611811, recall 0.882519
2017-12-10T14:58:05.663389: step 2722, loss 0.481947, acc 0.890625, prec 0.0611954, recall 0.882567
2017-12-10T14:58:05.854135: step 2723, loss 0.105736, acc 0.953125, prec 0.0612165, recall 0.882614
2017-12-10T14:58:06.044397: step 2724, loss 0.328442, acc 0.953125, prec 0.0612376, recall 0.882661
2017-12-10T14:58:06.238456: step 2725, loss 0.184672, acc 0.9375, prec 0.0612308, recall 0.882661
2017-12-10T14:58:06.432290: step 2726, loss 0.166687, acc 0.953125, prec 0.0612256, recall 0.882661
2017-12-10T14:58:06.626231: step 2727, loss 0.208482, acc 0.921875, prec 0.0612696, recall 0.882756
2017-12-10T14:58:06.815063: step 2728, loss 0.136295, acc 0.96875, prec 0.0612661, recall 0.882756
2017-12-10T14:58:07.004855: step 2729, loss 0.123418, acc 0.984375, prec 0.0612907, recall 0.882803
2017-12-10T14:58:07.200177: step 2730, loss 0.115156, acc 0.953125, prec 0.0613118, recall 0.88285
2017-12-10T14:58:07.393451: step 2731, loss 0.197265, acc 0.9375, prec 0.0613049, recall 0.88285
2017-12-10T14:58:07.585776: step 2732, loss 0.0538737, acc 0.984375, prec 0.0613295, recall 0.882897
2017-12-10T14:58:07.776177: step 2733, loss 0.0989427, acc 0.953125, prec 0.0613243, recall 0.882897
2017-12-10T14:58:07.969607: step 2734, loss 0.284781, acc 0.953125, prec 0.0613979, recall 0.883039
2017-12-10T14:58:08.161916: step 2735, loss 1.15999, acc 0.96875, prec 0.0614207, recall 0.883086
2017-12-10T14:58:08.358647: step 2736, loss 0.177296, acc 0.9375, prec 0.06144, recall 0.883133
2017-12-10T14:58:08.546248: step 2737, loss 0.0215001, acc 1, prec 0.06144, recall 0.883133
2017-12-10T14:58:08.737634: step 2738, loss 0.0423067, acc 0.96875, prec 0.0614366, recall 0.883133
2017-12-10T14:58:08.932710: step 2739, loss 0.238092, acc 0.9375, prec 0.0614822, recall 0.883226
2017-12-10T14:58:09.126873: step 2740, loss 0.107192, acc 0.921875, prec 0.0614736, recall 0.883226
2017-12-10T14:58:09.316240: step 2741, loss 0.103435, acc 0.921875, prec 0.061465, recall 0.883226
2017-12-10T14:58:09.508309: step 2742, loss 0.0436088, acc 0.984375, prec 0.0614633, recall 0.883226
2017-12-10T14:58:09.704412: step 2743, loss 0.326296, acc 0.921875, prec 0.0614809, recall 0.883273
2017-12-10T14:58:09.897036: step 2744, loss 0.0866782, acc 0.984375, prec 0.0615316, recall 0.883367
2017-12-10T14:58:10.087346: step 2745, loss 0.770043, acc 0.890625, prec 0.061572, recall 0.88346
2017-12-10T14:58:10.282102: step 2746, loss 0.984618, acc 0.921875, prec 0.0616157, recall 0.883553
2017-12-10T14:58:10.478064: step 2747, loss 0.297354, acc 0.96875, prec 0.0616908, recall 0.883693
2017-12-10T14:58:10.671580: step 2748, loss 0.184097, acc 0.921875, prec 0.0616822, recall 0.883693
2017-12-10T14:58:10.871929: step 2749, loss 0.162444, acc 0.96875, prec 0.0616788, recall 0.883693
2017-12-10T14:58:11.064392: step 2750, loss 0.220056, acc 0.875, prec 0.061665, recall 0.883693
2017-12-10T14:58:11.260511: step 2751, loss 0.640015, acc 0.921875, prec 0.0616826, recall 0.88374
2017-12-10T14:58:11.458521: step 2752, loss 0.382553, acc 0.890625, prec 0.0616706, recall 0.88374
2017-12-10T14:58:11.652643: step 2753, loss 0.131671, acc 0.953125, prec 0.0616654, recall 0.88374
2017-12-10T14:58:11.846404: step 2754, loss 0.564671, acc 0.890625, prec 0.0616795, recall 0.883786
2017-12-10T14:58:12.038364: step 2755, loss 0.118866, acc 0.96875, prec 0.0616761, recall 0.883786
2017-12-10T14:58:12.233600: step 2756, loss 0.554126, acc 0.921875, prec 0.0617198, recall 0.883879
2017-12-10T14:58:12.425659: step 2757, loss 0.483816, acc 0.875, prec 0.0617322, recall 0.883925
2017-12-10T14:58:12.619143: step 2758, loss 0.412476, acc 0.859375, prec 0.0617428, recall 0.883971
2017-12-10T14:58:12.812472: step 2759, loss 0.343939, acc 0.9375, prec 0.0617882, recall 0.884064
2017-12-10T14:58:13.006325: step 2760, loss 0.353223, acc 0.875, prec 0.0618006, recall 0.88411
2017-12-10T14:58:13.196219: step 2761, loss 0.271284, acc 0.859375, prec 0.0618112, recall 0.884156
2017-12-10T14:58:13.388364: step 2762, loss 0.149045, acc 0.953125, prec 0.0618321, recall 0.884202
2017-12-10T14:58:13.581978: step 2763, loss 1.90084, acc 0.875, prec 0.0618201, recall 0.88385
2017-12-10T14:58:13.776658: step 2764, loss 0.407219, acc 0.890625, prec 0.0618342, recall 0.883897
2017-12-10T14:58:13.970490: step 2765, loss 0.350536, acc 0.921875, prec 0.0618777, recall 0.883989
2017-12-10T14:58:14.169175: step 2766, loss 0.260454, acc 0.890625, prec 0.0618657, recall 0.883989
2017-12-10T14:58:14.366224: step 2767, loss 0.21122, acc 0.953125, prec 0.0618866, recall 0.884035
2017-12-10T14:58:14.556364: step 2768, loss 0.20475, acc 0.90625, prec 0.0618763, recall 0.884035
2017-12-10T14:58:14.754861: step 2769, loss 0.123718, acc 0.953125, prec 0.0618711, recall 0.884035
2017-12-10T14:58:14.947217: step 2770, loss 0.359777, acc 0.921875, prec 0.0618625, recall 0.884035
2017-12-10T14:58:15.141687: step 2771, loss 0.232559, acc 0.90625, prec 0.0618783, recall 0.884081
2017-12-10T14:58:15.332949: step 2772, loss 0.126125, acc 0.953125, prec 0.0618731, recall 0.884081
2017-12-10T14:58:15.524625: step 2773, loss 0.171324, acc 0.9375, prec 0.0618663, recall 0.884081
2017-12-10T14:58:15.715224: step 2774, loss 0.234277, acc 0.9375, prec 0.0618594, recall 0.884081
2017-12-10T14:58:15.906568: step 2775, loss 0.139861, acc 0.953125, prec 0.0618542, recall 0.884081
2017-12-10T14:58:16.100001: step 2776, loss 0.109224, acc 0.953125, prec 0.0618491, recall 0.884081
2017-12-10T14:58:16.300660: step 2777, loss 0.154008, acc 0.96875, prec 0.0618978, recall 0.884173
2017-12-10T14:58:16.499825: step 2778, loss 0.18618, acc 0.953125, prec 0.0619186, recall 0.884219
2017-12-10T14:58:16.693586: step 2779, loss 0.410614, acc 0.96875, prec 0.0619413, recall 0.884265
2017-12-10T14:58:16.886355: step 2780, loss 0.212575, acc 0.9375, prec 0.0619604, recall 0.884311
2017-12-10T14:58:17.078192: step 2781, loss 0.471101, acc 0.984375, prec 0.0620108, recall 0.884402
2017-12-10T14:58:17.271626: step 2782, loss 0.248684, acc 0.921875, prec 0.0620022, recall 0.884402
2017-12-10T14:58:17.463238: step 2783, loss 0.173321, acc 0.96875, prec 0.0620508, recall 0.884494
2017-12-10T14:58:17.656395: step 2784, loss 0.267635, acc 0.984375, prec 0.0621011, recall 0.884585
2017-12-10T14:58:17.850211: step 2785, loss 0.192082, acc 0.9375, prec 0.0621202, recall 0.884631
2017-12-10T14:58:18.050959: step 2786, loss 0.0389067, acc 0.984375, prec 0.0621445, recall 0.884676
2017-12-10T14:58:18.242245: step 2787, loss 0.0614026, acc 0.984375, prec 0.0621688, recall 0.884722
2017-12-10T14:58:18.431584: step 2788, loss 0.19145, acc 0.96875, prec 0.0621914, recall 0.884767
2017-12-10T14:58:18.623725: step 2789, loss 0.0617802, acc 0.984375, prec 0.0621897, recall 0.884767
2017-12-10T14:58:18.817279: step 2790, loss 0.193595, acc 0.953125, prec 0.0622105, recall 0.884813
2017-12-10T14:58:19.009967: step 2791, loss 0.0190054, acc 1, prec 0.0622105, recall 0.884813
2017-12-10T14:58:19.206208: step 2792, loss 0.0485594, acc 0.984375, prec 0.0622348, recall 0.884858
2017-12-10T14:58:19.406904: step 2793, loss 0.220514, acc 0.984375, prec 0.0622591, recall 0.884903
2017-12-10T14:58:19.601317: step 2794, loss 0.213166, acc 0.953125, prec 0.0622799, recall 0.884949
2017-12-10T14:58:19.791845: step 2795, loss 0.0855663, acc 0.984375, prec 0.0622782, recall 0.884949
2017-12-10T14:58:19.985122: step 2796, loss 0.0966707, acc 0.96875, prec 0.0622747, recall 0.884949
2017-12-10T14:58:20.177782: step 2797, loss 0.106382, acc 0.96875, prec 0.0622713, recall 0.884949
2017-12-10T14:58:20.370947: step 2798, loss 0.12189, acc 0.9375, prec 0.0622644, recall 0.884949
2017-12-10T14:58:20.566152: step 2799, loss 0.618177, acc 0.984375, prec 0.0623406, recall 0.885085
2017-12-10T14:58:20.762121: step 2800, loss 0.0330014, acc 0.984375, prec 0.0623649, recall 0.88513
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_0/1512935327/checkpoints/model-2800

2017-12-10T14:58:21.964831: step 2801, loss 0.280548, acc 0.96875, prec 0.0624134, recall 0.88522
2017-12-10T14:58:22.161633: step 2802, loss 1.35135, acc 0.984375, prec 0.0624896, recall 0.885355
2017-12-10T14:58:22.359562: step 2803, loss 0.132699, acc 0.96875, prec 0.0625121, recall 0.8854
2017-12-10T14:58:22.555659: step 2804, loss 0.466405, acc 0.984375, prec 0.0625364, recall 0.885445
2017-12-10T14:58:22.754382: step 2805, loss 0.156472, acc 0.9375, prec 0.0625554, recall 0.88549
2017-12-10T14:58:22.951358: step 2806, loss 0.179239, acc 0.96875, prec 0.0625519, recall 0.88549
2017-12-10T14:58:23.146483: step 2807, loss 0.393686, acc 0.9375, prec 0.062571, recall 0.885535
2017-12-10T14:58:23.338396: step 2808, loss 0.278346, acc 0.90625, prec 0.0625865, recall 0.88558
2017-12-10T14:58:23.532788: step 2809, loss 0.17686, acc 0.9375, prec 0.0626315, recall 0.88567
2017-12-10T14:58:23.725429: step 2810, loss 0.240116, acc 0.984375, prec 0.0626817, recall 0.885759
2017-12-10T14:58:23.917422: step 2811, loss 0.348095, acc 0.859375, prec 0.062692, recall 0.885804
2017-12-10T14:58:24.106021: step 2812, loss 0.218146, acc 0.953125, prec 0.0627387, recall 0.885893
2017-12-10T14:58:24.295133: step 2813, loss 0.337794, acc 0.8125, prec 0.0627179, recall 0.885893
2017-12-10T14:58:24.485974: step 2814, loss 0.291347, acc 0.875, prec 0.062704, recall 0.885893
2017-12-10T14:58:24.678798: step 2815, loss 0.455258, acc 0.875, prec 0.0626901, recall 0.885893
2017-12-10T14:58:24.872676: step 2816, loss 0.282931, acc 0.875, prec 0.0627022, recall 0.885938
2017-12-10T14:58:25.067132: step 2817, loss 0.324505, acc 0.90625, prec 0.0626918, recall 0.885938
2017-12-10T14:58:25.258421: step 2818, loss 0.316941, acc 0.875, prec 0.0627297, recall 0.886027
2017-12-10T14:58:25.450411: step 2819, loss 0.26746, acc 0.9375, prec 0.0627228, recall 0.886027
2017-12-10T14:58:25.647707: step 2820, loss 0.848079, acc 0.890625, prec 0.0627365, recall 0.886071
2017-12-10T14:58:25.844970: step 2821, loss 0.414263, acc 0.90625, prec 0.0627261, recall 0.886071
2017-12-10T14:58:26.036532: step 2822, loss 0.11732, acc 0.9375, prec 0.0627451, recall 0.886115
2017-12-10T14:58:26.230451: step 2823, loss 0.607958, acc 0.890625, prec 0.0627588, recall 0.88616
2017-12-10T14:58:26.418895: step 2824, loss 0.274903, acc 0.9375, prec 0.0628037, recall 0.886249
2017-12-10T14:58:26.613931: step 2825, loss 0.252923, acc 0.9375, prec 0.0628485, recall 0.886337
2017-12-10T14:58:26.807136: step 2826, loss 0.0559632, acc 0.96875, prec 0.062845, recall 0.886337
2017-12-10T14:58:27.001341: step 2827, loss 0.0677709, acc 0.96875, prec 0.0628415, recall 0.886337
2017-12-10T14:58:27.200203: step 2828, loss 0.105229, acc 0.953125, prec 0.0628622, recall 0.886381
2017-12-10T14:58:27.391647: step 2829, loss 0.157532, acc 0.953125, prec 0.062857, recall 0.886381
2017-12-10T14:58:27.583406: step 2830, loss 0.0304833, acc 1, prec 0.062857, recall 0.886381
2017-12-10T14:58:27.775761: step 2831, loss 0.119903, acc 0.921875, prec 0.0628483, recall 0.886381
2017-12-10T14:58:27.971019: step 2832, loss 0.171403, acc 0.953125, prec 0.062869, recall 0.886425
2017-12-10T14:58:28.162242: step 2833, loss 0.291493, acc 1, prec 0.0629207, recall 0.886514
2017-12-10T14:58:28.362029: step 2834, loss 0.0790236, acc 1, prec 0.0629465, recall 0.886558
2017-12-10T14:58:28.556079: step 2835, loss 0.0736551, acc 0.96875, prec 0.062943, recall 0.886558
2017-12-10T14:58:28.752573: step 2836, loss 0.107076, acc 0.96875, prec 0.0629654, recall 0.886602
2017-12-10T14:58:28.943628: step 2837, loss 0.460901, acc 0.96875, prec 0.0630136, recall 0.88669
2017-12-10T14:58:29.142101: step 2838, loss 0.101964, acc 0.953125, prec 0.0630084, recall 0.88669
2017-12-10T14:58:29.337595: step 2839, loss 0.0932152, acc 0.984375, prec 0.0630067, recall 0.88669
2017-12-10T14:58:29.530320: step 2840, loss 0.319905, acc 0.96875, prec 0.0630807, recall 0.886822
2017-12-10T14:58:29.726720: step 2841, loss 0.171085, acc 0.953125, prec 0.0631013, recall 0.886866
2017-12-10T14:58:29.918408: step 2842, loss 0.199047, acc 0.921875, prec 0.0630926, recall 0.886866
2017-12-10T14:58:30.110971: step 2843, loss 0.133164, acc 0.9375, prec 0.0630857, recall 0.886866
2017-12-10T14:58:30.304103: step 2844, loss 0.116381, acc 0.953125, prec 0.0630804, recall 0.886866
2017-12-10T14:58:30.498274: step 2845, loss 0.977433, acc 0.9375, prec 0.0631251, recall 0.886953
2017-12-10T14:58:30.694461: step 2846, loss 0.496514, acc 0.9375, prec 0.063144, recall 0.886997
2017-12-10T14:58:30.889141: step 2847, loss 0.102018, acc 1, prec 0.0631698, recall 0.887041
2017-12-10T14:58:31.082065: step 2848, loss 0.109737, acc 0.96875, prec 0.0631921, recall 0.887084
2017-12-10T14:58:31.280166: step 2849, loss 0.240686, acc 0.9375, prec 0.0632109, recall 0.887128
2017-12-10T14:58:31.477329: step 2850, loss 0.188195, acc 0.921875, prec 0.063228, recall 0.887172
2017-12-10T14:58:31.669334: step 2851, loss 0.369728, acc 1, prec 0.0632538, recall 0.887215
2017-12-10T14:58:31.865590: step 2852, loss 0.112254, acc 0.953125, prec 0.0632486, recall 0.887215
2017-12-10T14:58:32.062077: step 2853, loss 0.270169, acc 0.90625, prec 0.063264, recall 0.887259
2017-12-10T14:58:32.257806: step 2854, loss 0.123412, acc 0.96875, prec 0.0632863, recall 0.887302
2017-12-10T14:58:32.448968: step 2855, loss 0.214407, acc 0.9375, prec 0.0633051, recall 0.887346
2017-12-10T14:58:32.648255: step 2856, loss 0.192806, acc 0.9375, prec 0.0633497, recall 0.887433
2017-12-10T14:58:32.840664: step 2857, loss 0.0984999, acc 0.96875, prec 0.0633719, recall 0.887476
2017-12-10T14:58:33.033918: step 2858, loss 0.288062, acc 0.9375, prec 0.0633907, recall 0.887519
2017-12-10T14:58:33.228840: step 2859, loss 0.173366, acc 0.953125, prec 0.063437, recall 0.887606
2017-12-10T14:58:33.429246: step 2860, loss 0.457542, acc 0.84375, prec 0.0634196, recall 0.887606
2017-12-10T14:58:33.619250: step 2861, loss 0.328537, acc 0.921875, prec 0.0634109, recall 0.887606
2017-12-10T14:58:33.808315: step 2862, loss 0.23178, acc 0.9375, prec 0.0634297, recall 0.887649
2017-12-10T14:58:34.001697: step 2863, loss 0.230307, acc 0.953125, prec 0.0634502, recall 0.887692
2017-12-10T14:58:34.194038: step 2864, loss 0.143344, acc 0.953125, prec 0.0634707, recall 0.887735
2017-12-10T14:58:34.388795: step 2865, loss 0.419241, acc 0.96875, prec 0.0635444, recall 0.887865
2017-12-10T14:58:34.587908: step 2866, loss 0.407598, acc 0.953125, prec 0.0635649, recall 0.887908
2017-12-10T14:58:34.779606: step 2867, loss 0.103736, acc 0.96875, prec 0.0635614, recall 0.887908
2017-12-10T14:58:34.972137: step 2868, loss 0.247833, acc 0.859375, prec 0.0635457, recall 0.887908
2017-12-10T14:58:35.163635: step 2869, loss 0.223409, acc 0.890625, prec 0.0635849, recall 0.887994
2017-12-10T14:58:35.358224: step 2870, loss 0.245496, acc 0.90625, prec 0.0636002, recall 0.888037
2017-12-10T14:58:35.554321: step 2871, loss 0.257058, acc 0.9375, prec 0.0635932, recall 0.888037
2017-12-10T14:58:35.743583: step 2872, loss 0.227678, acc 0.921875, prec 0.0635844, recall 0.888037
2017-12-10T14:58:35.936920: step 2873, loss 0.178605, acc 0.953125, prec 0.0635792, recall 0.888037
2017-12-10T14:58:36.129984: step 2874, loss 0.126116, acc 0.953125, prec 0.0636254, recall 0.888123
2017-12-10T14:58:36.326450: step 2875, loss 0.208554, acc 0.921875, prec 0.0636167, recall 0.888123
2017-12-10T14:58:36.522203: step 2876, loss 0.379375, acc 0.921875, prec 0.0636593, recall 0.888208
2017-12-10T14:58:36.716310: step 2877, loss 0.17438, acc 0.921875, prec 0.0636506, recall 0.888208
2017-12-10T14:58:36.911478: step 2878, loss 0.75464, acc 0.9375, prec 0.063695, recall 0.888294
2017-12-10T14:58:37.109226: step 2879, loss 0.169385, acc 0.9375, prec 0.063688, recall 0.888294
2017-12-10T14:58:37.301714: step 2880, loss 0.314238, acc 0.96875, prec 0.0637358, recall 0.888379
2017-12-10T14:58:37.494759: step 2881, loss 0.521489, acc 0.9375, prec 0.0637802, recall 0.888464
2017-12-10T14:58:37.691246: step 2882, loss 0.13471, acc 0.9375, prec 0.0638245, recall 0.88855
2017-12-10T14:58:37.882397: step 2883, loss 0.172839, acc 0.921875, prec 0.0638158, recall 0.88855
2017-12-10T14:58:38.080087: step 2884, loss 0.198378, acc 0.9375, prec 0.0638088, recall 0.88855
2017-12-10T14:58:38.274779: step 2885, loss 0.246087, acc 0.96875, prec 0.0638566, recall 0.888635
2017-12-10T14:58:38.473274: step 2886, loss 0.365478, acc 0.921875, prec 0.0638992, recall 0.888719
2017-12-10T14:58:38.664621: step 2887, loss 0.357959, acc 0.953125, prec 0.0639452, recall 0.888804
2017-12-10T14:58:38.860266: step 2888, loss 0.0906486, acc 0.953125, prec 0.06394, recall 0.888804
2017-12-10T14:58:39.056913: step 2889, loss 0.283855, acc 0.953125, prec 0.0639603, recall 0.888847
2017-12-10T14:58:39.252391: step 2890, loss 0.210233, acc 0.9375, prec 0.063979, recall 0.888889
2017-12-10T14:58:39.447415: step 2891, loss 0.268666, acc 0.90625, prec 0.0639685, recall 0.888889
2017-12-10T14:58:39.641230: step 2892, loss 0.367228, acc 0.859375, prec 0.0639527, recall 0.888889
2017-12-10T14:58:39.831711: step 2893, loss 0.25055, acc 0.9375, prec 0.0639969, recall 0.888973
2017-12-10T14:58:40.026437: step 2894, loss 0.306131, acc 0.921875, prec 0.0639882, recall 0.888973
2017-12-10T14:58:40.215183: step 2895, loss 0.338012, acc 0.890625, prec 0.0639759, recall 0.888973
2017-12-10T14:58:40.411254: step 2896, loss 0.285096, acc 0.875, prec 0.0639875, recall 0.889016
2017-12-10T14:58:40.610110: step 2897, loss 0.732064, acc 0.90625, prec 0.0640794, recall 0.889184
2017-12-10T14:58:40.807374: step 2898, loss 0.343389, acc 0.890625, prec 0.0640927, recall 0.889226
2017-12-10T14:58:41.006200: step 2899, loss 0.0938507, acc 0.96875, prec 0.0641148, recall 0.889268
2017-12-10T14:58:41.198124: step 2900, loss 0.18081, acc 0.953125, prec 0.0641352, recall 0.88931
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_0/1512935327/checkpoints/model-2900

2017-12-10T14:58:42.428154: step 2901, loss 0.0764124, acc 0.96875, prec 0.0641828, recall 0.889394
2017-12-10T14:58:42.620877: step 2902, loss 0.149785, acc 0.9375, prec 0.0641758, recall 0.889394
2017-12-10T14:58:42.813103: step 2903, loss 0.27077, acc 0.9375, prec 0.0641688, recall 0.889394
2017-12-10T14:58:43.005501: step 2904, loss 0.114107, acc 0.96875, prec 0.0641908, recall 0.889436
2017-12-10T14:58:43.201620: step 2905, loss 0.148321, acc 0.9375, prec 0.0642094, recall 0.889478
2017-12-10T14:58:43.397785: step 2906, loss 0.213943, acc 0.921875, prec 0.0642006, recall 0.889478
2017-12-10T14:58:43.591440: step 2907, loss 0.156287, acc 0.9375, prec 0.0642192, recall 0.88952
2017-12-10T14:58:43.783922: step 2908, loss 0.415069, acc 0.953125, prec 0.064265, recall 0.889603
2017-12-10T14:58:43.981270: step 2909, loss 0.124052, acc 0.953125, prec 0.0642853, recall 0.889645
2017-12-10T14:58:44.181344: step 2910, loss 0.123031, acc 0.96875, prec 0.0643074, recall 0.889686
2017-12-10T14:58:44.373260: step 2911, loss 0.0380884, acc 0.984375, prec 0.0643312, recall 0.889728
2017-12-10T14:58:44.567318: step 2912, loss 0.143285, acc 0.96875, prec 0.0643532, recall 0.88977
2017-12-10T14:58:44.761437: step 2913, loss 0.053125, acc 0.984375, prec 0.0643514, recall 0.88977
2017-12-10T14:58:44.955459: step 2914, loss 0.367793, acc 0.90625, prec 0.0643664, recall 0.889811
2017-12-10T14:58:45.150672: step 2915, loss 0.0773896, acc 0.953125, prec 0.0643612, recall 0.889811
2017-12-10T14:58:45.346563: step 2916, loss 0.0526045, acc 0.984375, prec 0.0643849, recall 0.889853
2017-12-10T14:58:45.540311: step 2917, loss 4.62523, acc 0.96875, prec 0.0644343, recall 0.889601
2017-12-10T14:58:45.743848: step 2918, loss 0.402154, acc 0.96875, prec 0.0644818, recall 0.889684
2017-12-10T14:58:45.940011: step 2919, loss 0.202813, acc 0.9375, prec 0.0645003, recall 0.889725
2017-12-10T14:58:46.132953: step 2920, loss 0.600801, acc 0.859375, prec 0.06451, recall 0.889767
2017-12-10T14:58:46.325254: step 2921, loss 0.403538, acc 0.921875, prec 0.0645012, recall 0.889767
2017-12-10T14:58:46.517939: step 2922, loss 0.150442, acc 0.9375, prec 0.0645452, recall 0.88985
2017-12-10T14:58:46.710634: step 2923, loss 0.291295, acc 0.921875, prec 0.0646384, recall 0.890015
2017-12-10T14:58:46.906617: step 2924, loss 0.148992, acc 0.9375, prec 0.0646313, recall 0.890015
2017-12-10T14:58:47.101370: step 2925, loss 0.164522, acc 0.921875, prec 0.0646225, recall 0.890015
2017-12-10T14:58:47.294250: step 2926, loss 1.39085, acc 0.828125, prec 0.0646541, recall 0.890097
2017-12-10T14:58:47.488170: step 2927, loss 0.282197, acc 0.9375, prec 0.0646726, recall 0.890139
2017-12-10T14:58:47.682525: step 2928, loss 0.513001, acc 0.84375, prec 0.0647059, recall 0.890221
2017-12-10T14:58:47.875810: step 2929, loss 0.318631, acc 0.875, prec 0.0646918, recall 0.890221
2017-12-10T14:58:48.065705: step 2930, loss 0.414744, acc 0.90625, prec 0.0647067, recall 0.890262
2017-12-10T14:58:48.257615: step 2931, loss 0.797216, acc 0.875, prec 0.064769, recall 0.890385
2017-12-10T14:58:48.455789: step 2932, loss 0.373377, acc 0.890625, prec 0.0647566, recall 0.890385
2017-12-10T14:58:48.646900: step 2933, loss 0.436419, acc 0.90625, prec 0.0647715, recall 0.890426
2017-12-10T14:58:48.842741: step 2934, loss 0.233512, acc 0.921875, prec 0.0647627, recall 0.890426
2017-12-10T14:58:49.035269: step 2935, loss 0.247123, acc 0.890625, prec 0.0647758, recall 0.890467
2017-12-10T14:58:49.221392: step 2936, loss 0.481767, acc 0.875, prec 0.0647871, recall 0.890508
2017-12-10T14:58:49.420657: step 2937, loss 0.563233, acc 0.8125, prec 0.0648168, recall 0.89059
2017-12-10T14:58:49.612571: step 2938, loss 0.41914, acc 0.828125, prec 0.0647975, recall 0.89059
2017-12-10T14:58:49.811210: step 2939, loss 0.553015, acc 0.921875, prec 0.0648903, recall 0.890753
2017-12-10T14:58:50.005675: step 2940, loss 0.601874, acc 0.9375, prec 0.0649594, recall 0.890875
2017-12-10T14:58:50.198116: step 2941, loss 0.0612586, acc 0.984375, prec 0.0649576, recall 0.890875
2017-12-10T14:58:50.395250: step 2942, loss 0.128497, acc 0.9375, prec 0.064976, recall 0.890916
2017-12-10T14:58:50.592299: step 2943, loss 0.216741, acc 0.953125, prec 0.0649707, recall 0.890916
2017-12-10T14:58:50.787017: step 2944, loss 0.476301, acc 0.90625, prec 0.0649855, recall 0.890956
2017-12-10T14:58:50.978873: step 2945, loss 0.110552, acc 0.953125, prec 0.0649802, recall 0.890956
2017-12-10T14:58:51.170822: step 2946, loss 0.151445, acc 0.9375, prec 0.0650239, recall 0.891038
2017-12-10T14:58:51.368066: step 2947, loss 0.193287, acc 0.953125, prec 0.065044, recall 0.891078
2017-12-10T14:58:51.559251: step 2948, loss 0.109142, acc 0.9375, prec 0.0650876, recall 0.891159
2017-12-10T14:58:51.752570: step 2949, loss 0.185408, acc 0.96875, prec 0.0651095, recall 0.891199
2017-12-10T14:58:51.944133: step 2950, loss 0.142414, acc 0.953125, prec 0.0651042, recall 0.891199
2017-12-10T14:58:52.141490: step 2951, loss 0.119638, acc 0.953125, prec 0.0651242, recall 0.89124
2017-12-10T14:58:52.331302: step 2952, loss 0.13768, acc 0.953125, prec 0.0651443, recall 0.89128
2017-12-10T14:58:52.520048: step 2953, loss 1.79793, acc 0.921875, prec 0.0651372, recall 0.89095
2017-12-10T14:58:52.717383: step 2954, loss 0.166758, acc 0.9375, prec 0.0651555, recall 0.89099
2017-12-10T14:58:52.912504: step 2955, loss 0.125439, acc 0.953125, prec 0.0651755, recall 0.89103
2017-12-10T14:58:53.106916: step 2956, loss 0.259836, acc 0.921875, prec 0.0651921, recall 0.891071
2017-12-10T14:58:53.299990: step 2957, loss 0.421855, acc 0.875, prec 0.0652286, recall 0.891151
2017-12-10T14:58:53.491943: step 2958, loss 0.407279, acc 0.875, prec 0.0652398, recall 0.891192
2017-12-10T14:58:53.683674: step 2959, loss 0.0569509, acc 0.96875, prec 0.0652362, recall 0.891192
2017-12-10T14:58:53.881729: step 2960, loss 0.196672, acc 0.90625, prec 0.0652256, recall 0.891192
2017-12-10T14:58:54.078082: step 2961, loss 0.292403, acc 0.96875, prec 0.0652474, recall 0.891232
2017-12-10T14:58:54.277394: step 2962, loss 0.264947, acc 0.890625, prec 0.0652604, recall 0.891272
2017-12-10T14:58:54.475725: step 2963, loss 0.193461, acc 0.9375, prec 0.0652786, recall 0.891312
2017-12-10T14:58:54.666499: step 2964, loss 0.279102, acc 0.96875, prec 0.065351, recall 0.891433
2017-12-10T14:58:54.863019: step 2965, loss 0.427775, acc 0.875, prec 0.0653621, recall 0.891473
2017-12-10T14:58:55.053987: step 2966, loss 0.189162, acc 0.9375, prec 0.0653803, recall 0.891513
2017-12-10T14:58:55.253280: step 2967, loss 0.192741, acc 0.9375, prec 0.0653733, recall 0.891513
2017-12-10T14:58:55.442808: step 2968, loss 0.152325, acc 0.953125, prec 0.0653933, recall 0.891553
2017-12-10T14:58:55.633603: step 2969, loss 0.211687, acc 0.9375, prec 0.0654367, recall 0.891633
2017-12-10T14:58:55.822738: step 2970, loss 0.233529, acc 0.9375, prec 0.0654549, recall 0.891673
2017-12-10T14:58:56.017022: step 2971, loss 0.295998, acc 0.96875, prec 0.0654767, recall 0.891713
2017-12-10T14:58:56.216618: step 2972, loss 0.42785, acc 0.953125, prec 0.0654966, recall 0.891753
2017-12-10T14:58:56.417356: step 2973, loss 0.127982, acc 0.953125, prec 0.0655419, recall 0.891832
2017-12-10T14:58:56.611661: step 2974, loss 0.178711, acc 0.953125, prec 0.0655365, recall 0.891832
2017-12-10T14:58:56.807588: step 2975, loss 0.143985, acc 1, prec 0.0655618, recall 0.891872
2017-12-10T14:58:57.005729: step 2976, loss 0.349271, acc 0.890625, prec 0.0655999, recall 0.891952
2017-12-10T14:58:57.196151: step 2977, loss 0.344387, acc 0.9375, prec 0.0656181, recall 0.891991
2017-12-10T14:58:57.387571: step 2978, loss 0.0533456, acc 0.96875, prec 0.0656145, recall 0.891991
2017-12-10T14:58:57.579573: step 2979, loss 2.46947, acc 0.96875, prec 0.0656128, recall 0.891664
2017-12-10T14:58:57.777198: step 2980, loss 0.352275, acc 0.890625, prec 0.0656003, recall 0.891664
2017-12-10T14:58:57.972638: step 2981, loss 0.354655, acc 0.90625, prec 0.0655897, recall 0.891664
2017-12-10T14:58:58.150314: step 2982, loss 0.295002, acc 0.941176, prec 0.0656096, recall 0.891703
2017-12-10T14:58:58.354972: step 2983, loss 0.282143, acc 0.953125, prec 0.0656296, recall 0.891743
2017-12-10T14:58:58.547158: step 2984, loss 0.403854, acc 0.875, prec 0.0656406, recall 0.891783
2017-12-10T14:58:58.744315: step 2985, loss 0.160613, acc 0.90625, prec 0.0656552, recall 0.891823
2017-12-10T14:58:58.935444: step 2986, loss 0.200979, acc 0.921875, prec 0.0656463, recall 0.891823
2017-12-10T14:58:59.133776: step 2987, loss 0.301007, acc 0.9375, prec 0.0657149, recall 0.891941
2017-12-10T14:58:59.333196: step 2988, loss 0.202617, acc 0.890625, prec 0.0657025, recall 0.891941
2017-12-10T14:58:59.525661: step 2989, loss 0.0713408, acc 0.953125, prec 0.0657224, recall 0.891981
2017-12-10T14:58:59.720426: step 2990, loss 0.940638, acc 0.921875, prec 0.0657639, recall 0.89206
2017-12-10T14:58:59.916078: step 2991, loss 0.17685, acc 0.90625, prec 0.0658289, recall 0.892178
2017-12-10T14:59:00.106878: step 2992, loss 0.108298, acc 0.9375, prec 0.0658721, recall 0.892257
2017-12-10T14:59:00.299113: step 2993, loss 0.170938, acc 0.9375, prec 0.0659406, recall 0.892375
2017-12-10T14:59:00.490305: step 2994, loss 0.152447, acc 0.9375, prec 0.0659335, recall 0.892375
2017-12-10T14:59:00.680915: step 2995, loss 0.117223, acc 0.953125, prec 0.0659533, recall 0.892414
2017-12-10T14:59:00.873526: step 2996, loss 0.60943, acc 0.90625, prec 0.0659678, recall 0.892453
2017-12-10T14:59:01.065330: step 2997, loss 0.179102, acc 0.953125, prec 0.0659625, recall 0.892453
2017-12-10T14:59:01.260517: step 2998, loss 0.217098, acc 0.921875, prec 0.0659536, recall 0.892453
2017-12-10T14:59:01.457780: step 2999, loss 0.0690917, acc 0.96875, prec 0.0659501, recall 0.892453
2017-12-10T14:59:01.654627: step 3000, loss 0.38744, acc 0.90625, prec 0.0659897, recall 0.892532
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_0/1512935327/checkpoints/model-3000

2017-12-10T14:59:02.937753: step 3001, loss 0.106834, acc 0.953125, prec 0.0659844, recall 0.892532
2017-12-10T14:59:03.131616: step 3002, loss 0.23514, acc 0.953125, prec 0.0660042, recall 0.892571
2017-12-10T14:59:03.325175: step 3003, loss 0.215999, acc 0.921875, prec 0.0660456, recall 0.892649
2017-12-10T14:59:03.521189: step 3004, loss 0.192009, acc 0.921875, prec 0.0660619, recall 0.892688
2017-12-10T14:59:03.715293: step 3005, loss 0.0431716, acc 0.984375, prec 0.0660852, recall 0.892727
2017-12-10T14:59:03.906372: step 3006, loss 0.214346, acc 0.9375, prec 0.0661032, recall 0.892766
2017-12-10T14:59:04.100136: step 3007, loss 0.16387, acc 0.921875, prec 0.0661446, recall 0.892844
2017-12-10T14:59:04.291604: step 3008, loss 0.185822, acc 0.953125, prec 0.0661895, recall 0.892922
2017-12-10T14:59:04.491391: step 3009, loss 0.300895, acc 0.953125, prec 0.0662093, recall 0.892961
2017-12-10T14:59:04.683902: step 3010, loss 0.0311039, acc 0.984375, prec 0.0662075, recall 0.892961
2017-12-10T14:59:04.876070: step 3011, loss 0.265116, acc 0.90625, prec 0.066222, recall 0.893
2017-12-10T14:59:05.074100: step 3012, loss 0.125365, acc 0.96875, prec 0.0662435, recall 0.893038
2017-12-10T14:59:05.265853: step 3013, loss 0.303082, acc 0.90625, prec 0.0662579, recall 0.893077
2017-12-10T14:59:05.456525: step 3014, loss 0.103081, acc 0.953125, prec 0.0662777, recall 0.893116
2017-12-10T14:59:05.649912: step 3015, loss 0.0737466, acc 0.96875, prec 0.0662992, recall 0.893155
2017-12-10T14:59:05.842023: step 3016, loss 0.139507, acc 0.953125, prec 0.0662939, recall 0.893155
2017-12-10T14:59:06.033260: step 3017, loss 0.0578446, acc 0.984375, prec 0.0663172, recall 0.893193
2017-12-10T14:59:06.225175: step 3018, loss 0.053098, acc 0.984375, prec 0.0663154, recall 0.893193
2017-12-10T14:59:06.417442: step 3019, loss 0.159501, acc 0.9375, prec 0.0663083, recall 0.893193
2017-12-10T14:59:06.611020: step 3020, loss 0.228053, acc 0.984375, prec 0.0663818, recall 0.893309
2017-12-10T14:59:06.807998: step 3021, loss 0.0400223, acc 1, prec 0.0664069, recall 0.893348
2017-12-10T14:59:07.003047: step 3022, loss 0.0860056, acc 0.953125, prec 0.0664015, recall 0.893348
2017-12-10T14:59:07.199784: step 3023, loss 0.0272656, acc 1, prec 0.0664015, recall 0.893348
2017-12-10T14:59:07.395594: step 3024, loss 0.0656085, acc 0.96875, prec 0.066423, recall 0.893386
2017-12-10T14:59:07.598543: step 3025, loss 0.142959, acc 1, prec 0.0664732, recall 0.893463
2017-12-10T14:59:07.794761: step 3026, loss 0.0654114, acc 0.96875, prec 0.0664696, recall 0.893463
2017-12-10T14:59:07.986910: step 3027, loss 0.0122795, acc 1, prec 0.0664947, recall 0.893502
2017-12-10T14:59:08.181317: step 3028, loss 0.0273353, acc 0.984375, prec 0.0664929, recall 0.893502
2017-12-10T14:59:08.376617: step 3029, loss 0.0269213, acc 1, prec 0.066518, recall 0.89354
2017-12-10T14:59:08.572365: step 3030, loss 0.0798393, acc 0.984375, prec 0.0665914, recall 0.893655
2017-12-10T14:59:08.768859: step 3031, loss 1.12094, acc 0.96875, prec 0.066638, recall 0.893732
2017-12-10T14:59:08.967319: step 3032, loss 0.107639, acc 0.984375, prec 0.0666362, recall 0.893732
2017-12-10T14:59:09.158235: step 3033, loss 0.0172018, acc 1, prec 0.0666362, recall 0.893732
2017-12-10T14:59:09.354855: step 3034, loss 0.14572, acc 0.9375, prec 0.0666541, recall 0.89377
2017-12-10T14:59:09.552904: step 3035, loss 0.0275912, acc 1, prec 0.0666541, recall 0.89377
2017-12-10T14:59:09.749265: step 3036, loss 0.101738, acc 0.984375, prec 0.0666774, recall 0.893808
2017-12-10T14:59:09.938972: step 3037, loss 0.0492335, acc 0.984375, prec 0.0667007, recall 0.893847
2017-12-10T14:59:10.135811: step 3038, loss 0.0468568, acc 1, prec 0.0667759, recall 0.893961
2017-12-10T14:59:10.328567: step 3039, loss 0.635937, acc 0.96875, prec 0.0667973, recall 0.893999
2017-12-10T14:59:10.525811: step 3040, loss 0.0464651, acc 0.984375, prec 0.0668206, recall 0.894037
2017-12-10T14:59:10.723734: step 3041, loss 1.12365, acc 0.890625, prec 0.0668581, recall 0.894113
2017-12-10T14:59:10.922381: step 3042, loss 0.049074, acc 0.96875, prec 0.0668545, recall 0.894113
2017-12-10T14:59:11.114837: step 3043, loss 0.114475, acc 0.9375, prec 0.0668474, recall 0.894113
2017-12-10T14:59:11.307458: step 3044, loss 0.13744, acc 0.96875, prec 0.0668688, recall 0.894151
2017-12-10T14:59:11.500054: step 3045, loss 0.101815, acc 0.96875, prec 0.0668903, recall 0.894189
2017-12-10T14:59:11.695661: step 3046, loss 0.0567046, acc 0.984375, prec 0.0669135, recall 0.894227
2017-12-10T14:59:11.889549: step 3047, loss 0.114437, acc 0.921875, prec 0.0669045, recall 0.894227
2017-12-10T14:59:12.085878: step 3048, loss 0.230227, acc 0.96875, prec 0.066951, recall 0.894303
2017-12-10T14:59:12.280106: step 3049, loss 0.179602, acc 0.953125, prec 0.0669456, recall 0.894303
2017-12-10T14:59:12.475266: step 3050, loss 0.0407848, acc 1, prec 0.0669456, recall 0.894303
2017-12-10T14:59:12.664966: step 3051, loss 0.189462, acc 0.90625, prec 0.0669348, recall 0.894303
2017-12-10T14:59:12.865324: step 3052, loss 0.340554, acc 0.953125, prec 0.0669545, recall 0.894341
2017-12-10T14:59:13.062888: step 3053, loss 0.296196, acc 0.9375, prec 0.0669973, recall 0.894417
2017-12-10T14:59:13.257519: step 3054, loss 0.368556, acc 0.96875, prec 0.0670187, recall 0.894454
2017-12-10T14:59:13.454731: step 3055, loss 0.0925917, acc 0.96875, prec 0.0670151, recall 0.894454
2017-12-10T14:59:13.647128: step 3056, loss 0.0925108, acc 1, prec 0.0670402, recall 0.894492
2017-12-10T14:59:13.845456: step 3057, loss 0.0461988, acc 1, prec 0.0670652, recall 0.89453
2017-12-10T14:59:14.041992: step 3058, loss 0.0559256, acc 1, prec 0.0670652, recall 0.89453
2017-12-10T14:59:14.239934: step 3059, loss 0.0760905, acc 0.953125, prec 0.0670598, recall 0.89453
2017-12-10T14:59:14.430980: step 3060, loss 0.551619, acc 0.90625, prec 0.067149, recall 0.894681
2017-12-10T14:59:14.626013: step 3061, loss 0.0671636, acc 0.96875, prec 0.0671704, recall 0.894718
2017-12-10T14:59:14.815637: step 3062, loss 0.146913, acc 0.96875, prec 0.0671668, recall 0.894718
2017-12-10T14:59:15.013954: step 3063, loss 0.0286164, acc 1, prec 0.0671918, recall 0.894756
2017-12-10T14:59:15.206814: step 3064, loss 0.195225, acc 0.921875, prec 0.0672078, recall 0.894793
2017-12-10T14:59:15.399257: step 3065, loss 0.139146, acc 0.96875, prec 0.0672291, recall 0.894831
2017-12-10T14:59:15.595715: step 3066, loss 0.132594, acc 0.953125, prec 0.0672737, recall 0.894906
2017-12-10T14:59:15.790517: step 3067, loss 0.0893082, acc 0.96875, prec 0.0672701, recall 0.894906
2017-12-10T14:59:15.984262: step 3068, loss 0.077079, acc 0.953125, prec 0.0672647, recall 0.894906
2017-12-10T14:59:16.182923: step 3069, loss 0.0944666, acc 0.96875, prec 0.0672861, recall 0.894943
2017-12-10T14:59:16.371798: step 3070, loss 0.0767631, acc 0.984375, prec 0.0673092, recall 0.89498
2017-12-10T14:59:16.565660: step 3071, loss 0.350172, acc 0.921875, prec 0.0673252, recall 0.895018
2017-12-10T14:59:16.759590: step 3072, loss 0.0508768, acc 0.984375, prec 0.0673234, recall 0.895018
2017-12-10T14:59:16.954450: step 3073, loss 0.104123, acc 0.96875, prec 0.0673198, recall 0.895018
2017-12-10T14:59:17.151644: step 3074, loss 0.0456056, acc 1, prec 0.0673448, recall 0.895055
2017-12-10T14:59:17.344808: step 3075, loss 0.212899, acc 0.9375, prec 0.0673375, recall 0.895055
2017-12-10T14:59:17.537921: step 3076, loss 0.114932, acc 0.96875, prec 0.0673589, recall 0.895092
2017-12-10T14:59:17.734887: step 3077, loss 0.0549354, acc 0.984375, prec 0.0673571, recall 0.895092
2017-12-10T14:59:17.938045: step 3078, loss 0.0132642, acc 1, prec 0.0673571, recall 0.895092
2017-12-10T14:59:18.126590: step 3079, loss 0.03665, acc 1, prec 0.0673571, recall 0.895092
2017-12-10T14:59:18.318316: step 3080, loss 0.0488055, acc 0.96875, prec 0.0674034, recall 0.895167
2017-12-10T14:59:18.513950: step 3081, loss 0.0647601, acc 0.96875, prec 0.0674248, recall 0.895204
2017-12-10T14:59:18.707544: step 3082, loss 0.344668, acc 0.984375, prec 0.0674728, recall 0.895279
2017-12-10T14:59:18.903269: step 3083, loss 0.00861624, acc 1, prec 0.0674728, recall 0.895279
2017-12-10T14:59:19.095598: step 3084, loss 0.10511, acc 0.96875, prec 0.0674942, recall 0.895316
2017-12-10T14:59:19.288026: step 3085, loss 0.173146, acc 0.96875, prec 0.0675155, recall 0.895353
2017-12-10T14:59:19.486294: step 3086, loss 1.87175, acc 0.96875, prec 0.0675885, recall 0.895147
2017-12-10T14:59:19.682077: step 3087, loss 0.0347961, acc 0.984375, prec 0.0675867, recall 0.895147
2017-12-10T14:59:19.876300: step 3088, loss 0.0193874, acc 1, prec 0.0676117, recall 0.895184
2017-12-10T14:59:20.070042: step 3089, loss 1.10084, acc 0.96875, prec 0.067633, recall 0.895221
2017-12-10T14:59:20.268153: step 3090, loss 0.202673, acc 0.984375, prec 0.067681, recall 0.895295
2017-12-10T14:59:20.466500: step 3091, loss 0.0344247, acc 1, prec 0.067706, recall 0.895332
2017-12-10T14:59:20.655821: step 3092, loss 0.24367, acc 0.890625, prec 0.0677431, recall 0.895406
2017-12-10T14:59:20.845415: step 3093, loss 0.229694, acc 0.90625, prec 0.0677323, recall 0.895406
2017-12-10T14:59:21.041848: step 3094, loss 0.294451, acc 0.921875, prec 0.0677731, recall 0.89548
2017-12-10T14:59:21.234819: step 3095, loss 0.104332, acc 0.984375, prec 0.0677962, recall 0.895517
2017-12-10T14:59:21.434217: step 3096, loss 0.340014, acc 0.890625, prec 0.0678582, recall 0.895628
2017-12-10T14:59:21.624229: step 3097, loss 0.284388, acc 0.90625, prec 0.0678473, recall 0.895628
2017-12-10T14:59:21.814797: step 3098, loss 0.364268, acc 0.859375, prec 0.067831, recall 0.895628
2017-12-10T14:59:22.006608: step 3099, loss 0.283729, acc 0.890625, prec 0.0678681, recall 0.895701
2017-12-10T14:59:22.193471: step 3100, loss 0.328557, acc 0.890625, prec 0.0679052, recall 0.895775
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_0/1512935327/checkpoints/model-3100

2017-12-10T14:59:23.463284: step 3101, loss 0.37255, acc 0.921875, prec 0.067921, recall 0.895811
2017-12-10T14:59:23.659257: step 3102, loss 0.230116, acc 0.90625, prec 0.067935, recall 0.895848
2017-12-10T14:59:23.849719: step 3103, loss 0.26156, acc 0.90625, prec 0.067949, recall 0.895885
2017-12-10T14:59:24.041394: step 3104, loss 0.224117, acc 0.90625, prec 0.067963, recall 0.895921
2017-12-10T14:59:24.236019: step 3105, loss 0.320561, acc 0.875, prec 0.0679733, recall 0.895958
2017-12-10T14:59:24.429257: step 3106, loss 0.130491, acc 0.953125, prec 0.0679679, recall 0.895958
2017-12-10T14:59:24.623269: step 3107, loss 0.09901, acc 0.953125, prec 0.0679625, recall 0.895958
2017-12-10T14:59:24.815247: step 3108, loss 0.142201, acc 0.96875, prec 0.0679837, recall 0.895994
2017-12-10T14:59:25.006510: step 3109, loss 0.253779, acc 0.9375, prec 0.0680013, recall 0.896031
2017-12-10T14:59:25.199505: step 3110, loss 0.178887, acc 0.953125, prec 0.0680207, recall 0.896067
2017-12-10T14:59:25.389215: step 3111, loss 0.192391, acc 0.953125, prec 0.0680401, recall 0.896104
2017-12-10T14:59:25.579100: step 3112, loss 0.0660971, acc 0.984375, prec 0.0680383, recall 0.896104
2017-12-10T14:59:25.770239: step 3113, loss 0.0659923, acc 0.96875, prec 0.0680346, recall 0.896104
2017-12-10T14:59:25.964990: step 3114, loss 0.0541548, acc 0.984375, prec 0.0680825, recall 0.896177
2017-12-10T14:59:26.159257: step 3115, loss 0.0773191, acc 0.96875, prec 0.0680789, recall 0.896177
2017-12-10T14:59:26.353719: step 3116, loss 0.0654804, acc 0.984375, prec 0.0680771, recall 0.896177
2017-12-10T14:59:26.549448: step 3117, loss 0.0593652, acc 0.984375, prec 0.0681001, recall 0.896213
2017-12-10T14:59:26.745618: step 3118, loss 0.4362, acc 0.984375, prec 0.0681727, recall 0.896322
2017-12-10T14:59:26.940312: step 3119, loss 0.126377, acc 0.9375, prec 0.0681903, recall 0.896359
2017-12-10T14:59:27.138672: step 3120, loss 0.0309625, acc 0.984375, prec 0.0681885, recall 0.896359
2017-12-10T14:59:27.329919: step 3121, loss 0.0182539, acc 1, prec 0.0682133, recall 0.896395
2017-12-10T14:59:27.522391: step 3122, loss 0.0468057, acc 1, prec 0.0682381, recall 0.896431
2017-12-10T14:59:27.719322: step 3123, loss 2.14181, acc 0.96875, prec 0.0682363, recall 0.896118
2017-12-10T14:59:27.914494: step 3124, loss 0.00956718, acc 1, prec 0.0682363, recall 0.896118
2017-12-10T14:59:28.105874: step 3125, loss 0.060892, acc 0.96875, prec 0.0682327, recall 0.896118
2017-12-10T14:59:28.297169: step 3126, loss 0.153054, acc 0.953125, prec 0.068252, recall 0.896154
2017-12-10T14:59:28.493238: step 3127, loss 0.463704, acc 0.953125, prec 0.0683458, recall 0.896299
2017-12-10T14:59:28.686823: step 3128, loss 0.223876, acc 0.921875, prec 0.0683367, recall 0.896299
2017-12-10T14:59:28.881744: step 3129, loss 0.153235, acc 0.96875, prec 0.0683331, recall 0.896299
2017-12-10T14:59:29.077097: step 3130, loss 0.10169, acc 0.953125, prec 0.0683276, recall 0.896299
2017-12-10T14:59:29.277860: step 3131, loss 0.252023, acc 0.953125, prec 0.0683222, recall 0.896299
2017-12-10T14:59:29.473439: step 3132, loss 0.367034, acc 0.921875, prec 0.0683379, recall 0.896335
2017-12-10T14:59:29.666492: step 3133, loss 0.25802, acc 0.921875, prec 0.0683536, recall 0.896371
2017-12-10T14:59:29.860813: step 3134, loss 0.15234, acc 0.953125, prec 0.0683729, recall 0.896407
2017-12-10T14:59:30.059124: step 3135, loss 0.113299, acc 0.96875, prec 0.0683692, recall 0.896407
2017-12-10T14:59:30.255197: step 3136, loss 0.0981822, acc 0.953125, prec 0.0683638, recall 0.896407
2017-12-10T14:59:30.448665: step 3137, loss 0.108142, acc 0.9375, prec 0.0683813, recall 0.896443
2017-12-10T14:59:30.643536: step 3138, loss 0.190482, acc 0.9375, prec 0.0683988, recall 0.89648
2017-12-10T14:59:30.839411: step 3139, loss 0.254035, acc 0.90625, prec 0.0684127, recall 0.896516
2017-12-10T14:59:31.031858: step 3140, loss 0.241413, acc 0.9375, prec 0.0684549, recall 0.896588
2017-12-10T14:59:31.229557: step 3141, loss 0.17261, acc 0.953125, prec 0.0684495, recall 0.896588
2017-12-10T14:59:31.430617: step 3142, loss 0.0648597, acc 0.984375, prec 0.0684476, recall 0.896588
2017-12-10T14:59:31.626460: step 3143, loss 0.123322, acc 0.90625, prec 0.0684367, recall 0.896588
2017-12-10T14:59:31.829143: step 3144, loss 0.194891, acc 0.90625, prec 0.0684258, recall 0.896588
2017-12-10T14:59:32.026621: step 3145, loss 0.280558, acc 0.9375, prec 0.0684185, recall 0.896588
2017-12-10T14:59:32.224814: step 3146, loss 0.143731, acc 0.953125, prec 0.0684378, recall 0.896624
2017-12-10T14:59:32.415263: step 3147, loss 0.189969, acc 0.953125, prec 0.0684324, recall 0.896624
2017-12-10T14:59:32.612746: step 3148, loss 0.24578, acc 0.9375, prec 0.0684746, recall 0.896696
2017-12-10T14:59:32.804989: step 3149, loss 0.155823, acc 0.9375, prec 0.0684673, recall 0.896696
2017-12-10T14:59:33.001274: step 3150, loss 0.123665, acc 0.9375, prec 0.06846, recall 0.896696
2017-12-10T14:59:33.195765: step 3151, loss 0.0408645, acc 1, prec 0.06846, recall 0.896696
2017-12-10T14:59:33.388803: step 3152, loss 0.0360998, acc 1, prec 0.0684848, recall 0.896732
2017-12-10T14:59:33.582987: step 3153, loss 0.077696, acc 0.9375, prec 0.068527, recall 0.896803
2017-12-10T14:59:33.776077: step 3154, loss 0.318973, acc 1, prec 0.0685517, recall 0.896839
2017-12-10T14:59:33.974335: step 3155, loss 0.163395, acc 0.953125, prec 0.0685462, recall 0.896839
2017-12-10T14:59:34.175499: step 3156, loss 0.968676, acc 0.96875, prec 0.0685673, recall 0.896875
2017-12-10T14:59:34.371599: step 3157, loss 0.0664539, acc 0.96875, prec 0.0686131, recall 0.896947
2017-12-10T14:59:34.566274: step 3158, loss 0.117974, acc 0.953125, prec 0.0686077, recall 0.896947
2017-12-10T14:59:34.765619: step 3159, loss 0.0140686, acc 1, prec 0.0686077, recall 0.896947
2017-12-10T14:59:34.955355: step 3160, loss 0.0348696, acc 0.96875, prec 0.068604, recall 0.896947
2017-12-10T14:59:35.147380: step 3161, loss 0.0309879, acc 0.984375, prec 0.0686022, recall 0.896947
2017-12-10T14:59:35.342550: step 3162, loss 0.15814, acc 0.96875, prec 0.0686233, recall 0.896982
2017-12-10T14:59:35.539978: step 3163, loss 0.169812, acc 0.953125, prec 0.0686178, recall 0.896982
2017-12-10T14:59:35.734750: step 3164, loss 0.0769125, acc 0.96875, prec 0.0686142, recall 0.896982
2017-12-10T14:59:35.926211: step 3165, loss 0.161308, acc 0.96875, prec 0.0686105, recall 0.896982
2017-12-10T14:59:36.118143: step 3166, loss 0.708026, acc 0.984375, prec 0.0686334, recall 0.897018
2017-12-10T14:59:36.312615: step 3167, loss 0.0651128, acc 1, prec 0.0686828, recall 0.897089
2017-12-10T14:59:36.509731: step 3168, loss 0.150249, acc 0.96875, prec 0.0687039, recall 0.897125
2017-12-10T14:59:36.702442: step 3169, loss 0.242447, acc 0.953125, prec 0.0687231, recall 0.897161
2017-12-10T14:59:36.892879: step 3170, loss 0.159655, acc 0.953125, prec 0.0687177, recall 0.897161
2017-12-10T14:59:37.083305: step 3171, loss 0.235818, acc 0.9375, prec 0.0687104, recall 0.897161
2017-12-10T14:59:37.274188: step 3172, loss 0.749706, acc 0.984375, prec 0.0687333, recall 0.897196
2017-12-10T14:59:37.471732: step 3173, loss 0.0252399, acc 1, prec 0.068758, recall 0.897232
2017-12-10T14:59:37.663675: step 3174, loss 0.0305577, acc 1, prec 0.0687826, recall 0.897267
2017-12-10T14:59:37.857955: step 3175, loss 0.073985, acc 0.96875, prec 0.0688037, recall 0.897303
2017-12-10T14:59:38.047995: step 3176, loss 0.114393, acc 0.953125, prec 0.0687982, recall 0.897303
2017-12-10T14:59:38.240296: step 3177, loss 0.263847, acc 0.921875, prec 0.0687891, recall 0.897303
2017-12-10T14:59:38.434721: step 3178, loss 0.186624, acc 0.921875, prec 0.0688047, recall 0.897338
2017-12-10T14:59:38.630404: step 3179, loss 0.141717, acc 0.953125, prec 0.0687992, recall 0.897338
2017-12-10T14:59:38.822740: step 3180, loss 0.213764, acc 0.9375, prec 0.0688412, recall 0.897409
2017-12-10T14:59:39.014817: step 3181, loss 0.267136, acc 0.90625, prec 0.0688303, recall 0.897409
2017-12-10T14:59:39.209449: step 3182, loss 1.47643, acc 0.890625, prec 0.0688915, recall 0.897516
2017-12-10T14:59:39.404852: step 3183, loss 0.224298, acc 0.921875, prec 0.0688824, recall 0.897516
2017-12-10T14:59:39.596608: step 3184, loss 0.758872, acc 0.859375, prec 0.068866, recall 0.897516
2017-12-10T14:59:39.789386: step 3185, loss 0.133732, acc 0.953125, prec 0.0688852, recall 0.897551
2017-12-10T14:59:39.985141: step 3186, loss 0.421507, acc 0.875, prec 0.0688706, recall 0.897551
2017-12-10T14:59:40.176526: step 3187, loss 0.303681, acc 0.890625, prec 0.0688578, recall 0.897551
2017-12-10T14:59:40.371609: step 3188, loss 0.130016, acc 0.953125, prec 0.0688524, recall 0.897551
2017-12-10T14:59:40.564142: step 3189, loss 0.135326, acc 0.921875, prec 0.0688679, recall 0.897586
2017-12-10T14:59:40.757293: step 3190, loss 0.234199, acc 0.953125, prec 0.0688871, recall 0.897622
2017-12-10T14:59:40.950057: step 3191, loss 0.25269, acc 0.953125, prec 0.0688816, recall 0.897622
2017-12-10T14:59:41.139978: step 3192, loss 0.169017, acc 0.953125, prec 0.0688761, recall 0.897622
2017-12-10T14:59:41.334933: step 3193, loss 0.174013, acc 0.96875, prec 0.0688725, recall 0.897622
2017-12-10T14:59:41.526953: step 3194, loss 0.143112, acc 0.90625, prec 0.0688862, recall 0.897657
2017-12-10T14:59:41.715194: step 3195, loss 0.0738351, acc 0.96875, prec 0.0688825, recall 0.897657
2017-12-10T14:59:41.910838: step 3196, loss 0.106087, acc 0.9375, prec 0.0688753, recall 0.897657
2017-12-10T14:59:42.102727: step 3197, loss 0.189753, acc 0.953125, prec 0.068919, recall 0.897727
2017-12-10T14:59:42.299937: step 3198, loss 0.101875, acc 0.953125, prec 0.0689628, recall 0.897798
2017-12-10T14:59:42.496438: step 3199, loss 0.125081, acc 0.9375, prec 0.0689801, recall 0.897833
2017-12-10T14:59:42.694087: step 3200, loss 0.0424796, acc 0.984375, prec 0.0690029, recall 0.897868
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_0/1512935327/checkpoints/model-3200

2017-12-10T14:59:43.849704: step 3201, loss 0.0756509, acc 0.984375, prec 0.0690257, recall 0.897903
2017-12-10T14:59:44.051488: step 3202, loss 0.921555, acc 0.96875, prec 0.0690958, recall 0.898008
2017-12-10T14:59:44.249743: step 3203, loss 0.017526, acc 1, prec 0.0690958, recall 0.898008
2017-12-10T14:59:44.445536: step 3204, loss 0.184965, acc 0.953125, prec 0.0690903, recall 0.898008
2017-12-10T14:59:44.638588: step 3205, loss 0.208322, acc 0.9375, prec 0.0691322, recall 0.898078
2017-12-10T14:59:44.833062: step 3206, loss 0.433431, acc 0.953125, prec 0.0691759, recall 0.898148
2017-12-10T14:59:45.029667: step 3207, loss 0.127441, acc 0.96875, prec 0.069246, recall 0.898253
2017-12-10T14:59:45.230970: step 3208, loss 0.0307983, acc 0.984375, prec 0.0692442, recall 0.898253
2017-12-10T14:59:45.425893: step 3209, loss 0.0259224, acc 1, prec 0.0693179, recall 0.898357
2017-12-10T14:59:45.617469: step 3210, loss 0.111681, acc 0.96875, prec 0.0693388, recall 0.898392
2017-12-10T14:59:45.815603: step 3211, loss 0.229491, acc 0.9375, prec 0.0693561, recall 0.898427
2017-12-10T14:59:46.007617: step 3212, loss 0.0949125, acc 0.96875, prec 0.0693524, recall 0.898427
2017-12-10T14:59:46.203956: step 3213, loss 0.376484, acc 0.9375, prec 0.0693942, recall 0.898496
2017-12-10T14:59:46.402787: step 3214, loss 0.100506, acc 0.9375, prec 0.0693869, recall 0.898496
2017-12-10T14:59:46.593530: step 3215, loss 0.125756, acc 0.96875, prec 0.0694078, recall 0.898531
2017-12-10T14:59:46.787571: step 3216, loss 0.109978, acc 0.9375, prec 0.069425, recall 0.898566
2017-12-10T14:59:46.986529: step 3217, loss 0.033758, acc 0.984375, prec 0.0694232, recall 0.898566
2017-12-10T14:59:47.183312: step 3218, loss 0.476632, acc 0.921875, prec 0.0694386, recall 0.8986
2017-12-10T14:59:47.383457: step 3219, loss 0.100788, acc 0.96875, prec 0.0694349, recall 0.8986
2017-12-10T14:59:47.576292: step 3220, loss 0.880663, acc 0.984375, prec 0.0695558, recall 0.898773
2017-12-10T14:59:47.773131: step 3221, loss 1.2639, acc 0.9375, prec 0.0695975, recall 0.898842
2017-12-10T14:59:47.969803: step 3222, loss 0.0310473, acc 1, prec 0.0695975, recall 0.898842
2017-12-10T14:59:48.161766: step 3223, loss 0.79997, acc 0.953125, prec 0.0696166, recall 0.898876
2017-12-10T14:59:48.356198: step 3224, loss 0.480637, acc 0.90625, prec 0.0696546, recall 0.898945
2017-12-10T14:59:48.551285: step 3225, loss 0.343171, acc 0.921875, prec 0.0696454, recall 0.898945
2017-12-10T14:59:48.746651: step 3226, loss 0.295498, acc 0.921875, prec 0.0696608, recall 0.89898
2017-12-10T14:59:48.940204: step 3227, loss 0.246803, acc 0.90625, prec 0.0696988, recall 0.899048
2017-12-10T14:59:49.132158: step 3228, loss 0.467139, acc 0.765625, prec 0.0696958, recall 0.899083
2017-12-10T14:59:49.320488: step 3229, loss 0.417596, acc 0.859375, prec 0.0696793, recall 0.899083
2017-12-10T14:59:49.516812: step 3230, loss 0.291414, acc 0.890625, prec 0.0696909, recall 0.899117
2017-12-10T14:59:49.706234: step 3231, loss 0.169671, acc 0.921875, prec 0.0696817, recall 0.899117
2017-12-10T14:59:49.902753: step 3232, loss 0.19733, acc 0.96875, prec 0.0697026, recall 0.899151
2017-12-10T14:59:50.094630: step 3233, loss 0.188284, acc 0.9375, prec 0.0696952, recall 0.899151
2017-12-10T14:59:50.289123: step 3234, loss 0.376452, acc 0.875, prec 0.069705, recall 0.899185
2017-12-10T14:59:50.479232: step 3235, loss 0.481879, acc 0.890625, prec 0.0697167, recall 0.89922
2017-12-10T14:59:50.672045: step 3236, loss 0.452399, acc 0.890625, prec 0.0697283, recall 0.899254
2017-12-10T14:59:50.869008: step 3237, loss 0.142242, acc 0.9375, prec 0.0697454, recall 0.899288
2017-12-10T14:59:51.065296: step 3238, loss 0.556965, acc 0.859375, prec 0.0697289, recall 0.899288
2017-12-10T14:59:51.257295: step 3239, loss 0.362424, acc 0.90625, prec 0.0697424, recall 0.899322
2017-12-10T14:59:51.449965: step 3240, loss 0.402285, acc 0.859375, prec 0.0697259, recall 0.899322
2017-12-10T14:59:51.648986: step 3241, loss 0.631647, acc 0.9375, prec 0.069743, recall 0.899356
2017-12-10T14:59:51.845362: step 3242, loss 0.183732, acc 0.9375, prec 0.0697601, recall 0.89939
2017-12-10T14:59:52.045242: step 3243, loss 0.166649, acc 0.953125, prec 0.0697546, recall 0.89939
2017-12-10T14:59:52.243030: step 3244, loss 0.115287, acc 0.96875, prec 0.0697754, recall 0.899424
2017-12-10T14:59:52.437433: step 3245, loss 0.0482679, acc 0.984375, prec 0.069798, recall 0.899458
2017-12-10T14:59:52.628651: step 3246, loss 0.566953, acc 0.953125, prec 0.0698414, recall 0.899526
2017-12-10T14:59:52.824569: step 3247, loss 0.364306, acc 0.90625, prec 0.0698548, recall 0.89956
2017-12-10T14:59:53.015116: step 3248, loss 0.0171853, acc 1, prec 0.0698548, recall 0.89956
2017-12-10T14:59:53.211450: step 3249, loss 0.115376, acc 0.921875, prec 0.0698456, recall 0.89956
2017-12-10T14:59:53.408593: step 3250, loss 0.130079, acc 0.9375, prec 0.0698383, recall 0.89956
2017-12-10T14:59:53.606511: step 3251, loss 0.170867, acc 0.96875, prec 0.069859, recall 0.899594
2017-12-10T14:59:53.799652: step 3252, loss 0.217637, acc 0.9375, prec 0.0698517, recall 0.899594
2017-12-10T14:59:53.995026: step 3253, loss 1.38467, acc 0.96875, prec 0.0698499, recall 0.89929
2017-12-10T14:59:54.196624: step 3254, loss 0.234486, acc 0.96875, prec 0.0698706, recall 0.899324
2017-12-10T14:59:54.392204: step 3255, loss 0.366416, acc 0.9375, prec 0.0698877, recall 0.899358
2017-12-10T14:59:54.587712: step 3256, loss 0.285181, acc 0.953125, prec 0.0699066, recall 0.899392
2017-12-10T14:59:54.779522: step 3257, loss 1.61682, acc 0.9375, prec 0.069948, recall 0.89946
2017-12-10T14:59:54.971406: step 3258, loss 0.15128, acc 0.96875, prec 0.070042, recall 0.899596
2017-12-10T14:59:55.165182: step 3259, loss 0.345814, acc 0.84375, prec 0.0700236, recall 0.899596
2017-12-10T14:59:55.355504: step 3260, loss 0.353844, acc 0.875, prec 0.0700089, recall 0.899596
2017-12-10T14:59:55.547646: step 3261, loss 0.287119, acc 0.90625, prec 0.0700467, recall 0.899663
2017-12-10T14:59:55.736368: step 3262, loss 0.561015, acc 0.890625, prec 0.0700338, recall 0.899663
2017-12-10T14:59:55.933302: step 3263, loss 0.672993, acc 0.875, prec 0.0700435, recall 0.899697
2017-12-10T14:59:56.132493: step 3264, loss 0.511445, acc 0.78125, prec 0.0700422, recall 0.899731
2017-12-10T14:59:56.330335: step 3265, loss 0.392095, acc 0.875, prec 0.0700518, recall 0.899765
2017-12-10T14:59:56.526006: step 3266, loss 0.458547, acc 0.859375, prec 0.0700353, recall 0.899765
2017-12-10T14:59:56.719549: step 3267, loss 0.251802, acc 0.90625, prec 0.0700243, recall 0.899765
2017-12-10T14:59:56.914366: step 3268, loss 0.457396, acc 0.84375, prec 0.0700304, recall 0.899798
2017-12-10T14:59:57.108484: step 3269, loss 0.465266, acc 0.828125, prec 0.0700589, recall 0.899866
2017-12-10T14:59:57.305484: step 3270, loss 0.26416, acc 0.921875, prec 0.070074, recall 0.899899
2017-12-10T14:59:57.495671: step 3271, loss 0.315205, acc 0.90625, prec 0.070063, recall 0.899899
2017-12-10T14:59:57.690809: step 3272, loss 0.115939, acc 0.9375, prec 0.0700557, recall 0.899899
2017-12-10T14:59:57.885230: step 3273, loss 0.171984, acc 0.9375, prec 0.0700484, recall 0.899899
2017-12-10T14:59:58.080631: step 3274, loss 0.249193, acc 0.953125, prec 0.0700915, recall 0.899966
2017-12-10T14:59:58.277830: step 3275, loss 0.219899, acc 0.875, prec 0.0700768, recall 0.899966
2017-12-10T14:59:58.466623: step 3276, loss 0.169276, acc 0.953125, prec 0.0700714, recall 0.899966
2017-12-10T14:59:58.659280: step 3277, loss 0.481652, acc 0.9375, prec 0.0701126, recall 0.900034
2017-12-10T14:59:58.851458: step 3278, loss 0.388241, acc 0.890625, prec 0.0701484, recall 0.900101
2017-12-10T14:59:59.049343: step 3279, loss 0.248647, acc 0.953125, prec 0.0701429, recall 0.900101
2017-12-10T14:59:59.251509: step 3280, loss 0.154456, acc 0.921875, prec 0.0701337, recall 0.900101
2017-12-10T14:59:59.450292: step 3281, loss 0.0831038, acc 0.953125, prec 0.0702011, recall 0.900201
2017-12-10T14:59:59.640620: step 3282, loss 0.117639, acc 0.96875, prec 0.0701974, recall 0.900201
2017-12-10T14:59:59.834169: step 3283, loss 0.0926412, acc 0.984375, prec 0.0701956, recall 0.900201
2017-12-10T15:00:00.027142: step 3284, loss 0.167066, acc 0.9375, prec 0.0702611, recall 0.900301
2017-12-10T15:00:00.219146: step 3285, loss 0.0499887, acc 0.984375, prec 0.0702835, recall 0.900334
2017-12-10T15:00:00.414330: step 3286, loss 0.0528583, acc 0.96875, prec 0.0702799, recall 0.900334
2017-12-10T15:00:00.604001: step 3287, loss 0.109665, acc 0.96875, prec 0.0703005, recall 0.900368
2017-12-10T15:00:00.800231: step 3288, loss 0.0266822, acc 1, prec 0.0703005, recall 0.900368
2017-12-10T15:00:01.003050: step 3289, loss 0.0146683, acc 1, prec 0.0703005, recall 0.900368
2017-12-10T15:00:01.197504: step 3290, loss 0.891854, acc 0.96875, prec 0.0703453, recall 0.900434
2017-12-10T15:00:01.397228: step 3291, loss 0.113099, acc 0.984375, prec 0.0703435, recall 0.900434
2017-12-10T15:00:01.593546: step 3292, loss 0.0713865, acc 0.953125, prec 0.070338, recall 0.900434
2017-12-10T15:00:01.787675: step 3293, loss 0.0896459, acc 0.984375, prec 0.0703362, recall 0.900434
2017-12-10T15:00:01.984578: step 3294, loss 0.718104, acc 0.96875, prec 0.0703567, recall 0.900468
2017-12-10T15:00:02.178825: step 3295, loss 0.0954568, acc 0.96875, prec 0.0704016, recall 0.900534
2017-12-10T15:00:02.369007: step 3296, loss 0.0678897, acc 0.953125, prec 0.0703961, recall 0.900534
2017-12-10T15:00:02.559304: step 3297, loss 0.0725146, acc 0.96875, prec 0.0703924, recall 0.900534
2017-12-10T15:00:02.749891: step 3298, loss 0.197405, acc 1, prec 0.0704652, recall 0.900634
2017-12-10T15:00:02.945138: step 3299, loss 0.0309721, acc 1, prec 0.0705137, recall 0.9007
2017-12-10T15:00:03.141027: step 3300, loss 0.407394, acc 0.96875, prec 0.0705585, recall 0.900766
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_0/1512935327/checkpoints/model-3300

2017-12-10T15:00:04.349562: step 3301, loss 0.0361311, acc 0.984375, prec 0.0705809, recall 0.900799
2017-12-10T15:00:04.543245: step 3302, loss 0.127446, acc 0.96875, prec 0.0706741, recall 0.900931
2017-12-10T15:00:04.736103: step 3303, loss 0.100394, acc 0.984375, prec 0.0706965, recall 0.900964
2017-12-10T15:00:04.932059: step 3304, loss 0.158648, acc 0.953125, prec 0.0707395, recall 0.90103
2017-12-10T15:00:05.126470: step 3305, loss 0.139892, acc 0.9375, prec 0.0707321, recall 0.90103
2017-12-10T15:00:05.320378: step 3306, loss 0.0357981, acc 1, prec 0.0707563, recall 0.901062
2017-12-10T15:00:05.515877: step 3307, loss 0.0771811, acc 0.984375, prec 0.0707787, recall 0.901095
2017-12-10T15:00:05.707564: step 3308, loss 2.70331, acc 0.921875, prec 0.0707713, recall 0.900796
2017-12-10T15:00:05.907306: step 3309, loss 0.281476, acc 0.953125, prec 0.07079, recall 0.900829
2017-12-10T15:00:06.109593: step 3310, loss 0.0911219, acc 0.953125, prec 0.0707845, recall 0.900829
2017-12-10T15:00:06.303577: step 3311, loss 0.194372, acc 0.953125, prec 0.0708032, recall 0.900862
2017-12-10T15:00:06.498144: step 3312, loss 0.271428, acc 0.921875, prec 0.0708181, recall 0.900895
2017-12-10T15:00:06.695062: step 3313, loss 0.382286, acc 0.859375, prec 0.0708257, recall 0.900928
2017-12-10T15:00:06.889140: step 3314, loss 0.248783, acc 0.890625, prec 0.0709096, recall 0.901059
2017-12-10T15:00:07.085950: step 3315, loss 0.302996, acc 0.890625, prec 0.0709451, recall 0.901124
2017-12-10T15:00:07.278418: step 3316, loss 0.392605, acc 0.921875, prec 0.0710084, recall 0.901222
2017-12-10T15:00:07.471431: step 3317, loss 0.212835, acc 0.9375, prec 0.071001, recall 0.901222
2017-12-10T15:00:07.663335: step 3318, loss 0.319857, acc 0.875, prec 0.0709862, recall 0.901222
2017-12-10T15:00:07.860848: step 3319, loss 0.270368, acc 0.921875, prec 0.0710011, recall 0.901255
2017-12-10T15:00:08.054285: step 3320, loss 0.281963, acc 0.9375, prec 0.0710179, recall 0.901288
2017-12-10T15:00:08.248081: step 3321, loss 0.218981, acc 0.921875, prec 0.0710087, recall 0.901288
2017-12-10T15:00:08.436347: step 3322, loss 0.217508, acc 0.953125, prec 0.0710515, recall 0.901353
2017-12-10T15:00:08.632830: step 3323, loss 0.220636, acc 0.890625, prec 0.0710868, recall 0.901418
2017-12-10T15:00:08.827353: step 3324, loss 0.177606, acc 0.9375, prec 0.0710794, recall 0.901418
2017-12-10T15:00:09.022304: step 3325, loss 0.0931327, acc 0.984375, prec 0.0711018, recall 0.90145
2017-12-10T15:00:09.212836: step 3326, loss 0.299156, acc 0.890625, prec 0.0710888, recall 0.90145
2017-12-10T15:00:09.409838: step 3327, loss 0.284986, acc 0.984375, prec 0.0711594, recall 0.901548
2017-12-10T15:00:09.601398: step 3328, loss 0.085302, acc 0.984375, prec 0.0712058, recall 0.901612
2017-12-10T15:00:09.795234: step 3329, loss 0.126502, acc 0.984375, prec 0.0712281, recall 0.901645
2017-12-10T15:00:09.985995: step 3330, loss 0.196451, acc 0.90625, prec 0.071217, recall 0.901645
2017-12-10T15:00:10.176720: step 3331, loss 0.275263, acc 0.90625, prec 0.07123, recall 0.901677
2017-12-10T15:00:10.374873: step 3332, loss 0.148656, acc 0.96875, prec 0.0712505, recall 0.901709
2017-12-10T15:00:10.563562: step 3333, loss 0.788121, acc 0.921875, prec 0.0712653, recall 0.901742
2017-12-10T15:00:10.762697: step 3334, loss 0.069873, acc 0.984375, prec 0.0712876, recall 0.901774
2017-12-10T15:00:10.956483: step 3335, loss 0.0694376, acc 1, prec 0.0713117, recall 0.901806
2017-12-10T15:00:11.148010: step 3336, loss 0.248266, acc 0.96875, prec 0.0713803, recall 0.901903
2017-12-10T15:00:11.344619: step 3337, loss 0.131473, acc 0.953125, prec 0.0713989, recall 0.901935
2017-12-10T15:00:11.541113: step 3338, loss 0.0360885, acc 1, prec 0.0713989, recall 0.901935
2017-12-10T15:00:11.735173: step 3339, loss 0.163654, acc 0.9375, prec 0.0714156, recall 0.901967
2017-12-10T15:00:11.926769: step 3340, loss 0.0278367, acc 0.984375, prec 0.0714378, recall 0.901999
2017-12-10T15:00:12.120518: step 3341, loss 0.727271, acc 0.96875, prec 0.0714582, recall 0.902031
2017-12-10T15:00:12.312498: step 3342, loss 0.199028, acc 0.953125, prec 0.0714768, recall 0.902064
2017-12-10T15:00:12.508222: step 3343, loss 0.228554, acc 0.96875, prec 0.0714731, recall 0.902064
2017-12-10T15:00:12.702628: step 3344, loss 0.108989, acc 0.984375, prec 0.0714953, recall 0.902096
2017-12-10T15:00:12.896787: step 3345, loss 0.0403267, acc 1, prec 0.0714953, recall 0.902096
2017-12-10T15:00:13.089752: step 3346, loss 0.147393, acc 1, prec 0.0715194, recall 0.902128
2017-12-10T15:00:13.293168: step 3347, loss 0.0429285, acc 1, prec 0.0715435, recall 0.90216
2017-12-10T15:00:13.489210: step 3348, loss 0.234988, acc 0.953125, prec 0.0715379, recall 0.90216
2017-12-10T15:00:13.685594: step 3349, loss 0.303218, acc 1, prec 0.0716102, recall 0.902256
2017-12-10T15:00:13.881039: step 3350, loss 0.266399, acc 0.96875, prec 0.0716546, recall 0.902319
2017-12-10T15:00:14.088344: step 3351, loss 0.137453, acc 1, prec 0.0716787, recall 0.902351
2017-12-10T15:00:14.290822: step 3352, loss 0.100218, acc 0.9375, prec 0.0716713, recall 0.902351
2017-12-10T15:00:14.483518: step 3353, loss 0.134587, acc 0.953125, prec 0.0716657, recall 0.902351
2017-12-10T15:00:14.677046: step 3354, loss 0.383102, acc 0.953125, prec 0.0717083, recall 0.902415
2017-12-10T15:00:14.874955: step 3355, loss 0.0784524, acc 0.984375, prec 0.0717305, recall 0.902447
2017-12-10T15:00:15.073346: step 3356, loss 0.117878, acc 0.9375, prec 0.0717231, recall 0.902447
2017-12-10T15:00:15.268453: step 3357, loss 0.176661, acc 0.921875, prec 0.0717138, recall 0.902447
2017-12-10T15:00:15.462751: step 3358, loss 0.214949, acc 0.921875, prec 0.0717285, recall 0.902479
2017-12-10T15:00:15.660208: step 3359, loss 0.117722, acc 0.984375, prec 0.0717267, recall 0.902479
2017-12-10T15:00:15.855461: step 3360, loss 0.182633, acc 0.984375, prec 0.0717489, recall 0.902511
2017-12-10T15:00:16.050130: step 3361, loss 0.0922581, acc 0.953125, prec 0.0717433, recall 0.902511
2017-12-10T15:00:16.241224: step 3362, loss 0.106863, acc 0.9375, prec 0.0717359, recall 0.902511
2017-12-10T15:00:16.442609: step 3363, loss 0.147165, acc 0.9375, prec 0.0717284, recall 0.902511
2017-12-10T15:00:16.639015: step 3364, loss 0.0667701, acc 0.96875, prec 0.0717247, recall 0.902511
2017-12-10T15:00:16.836785: step 3365, loss 0.0668795, acc 0.953125, prec 0.0717191, recall 0.902511
2017-12-10T15:00:17.037847: step 3366, loss 0.109211, acc 0.953125, prec 0.0717136, recall 0.902511
2017-12-10T15:00:17.230130: step 3367, loss 0.0601726, acc 0.984375, prec 0.0717117, recall 0.902511
2017-12-10T15:00:17.427597: step 3368, loss 0.490042, acc 0.9375, prec 0.0717283, recall 0.902542
2017-12-10T15:00:17.621135: step 3369, loss 0.0584001, acc 0.96875, prec 0.0717486, recall 0.902574
2017-12-10T15:00:17.819212: step 3370, loss 0.0598157, acc 0.984375, prec 0.0717468, recall 0.902574
2017-12-10T15:00:18.011146: step 3371, loss 0.0151951, acc 1, prec 0.0717468, recall 0.902574
2017-12-10T15:00:18.203553: step 3372, loss 0.298606, acc 0.9375, prec 0.0718355, recall 0.902701
2017-12-10T15:00:18.400370: step 3373, loss 0.118259, acc 0.96875, prec 0.0718558, recall 0.902733
2017-12-10T15:00:18.596468: step 3374, loss 0.190154, acc 0.96875, prec 0.0718521, recall 0.902733
2017-12-10T15:00:18.787027: step 3375, loss 0.173033, acc 0.96875, prec 0.0718724, recall 0.902764
2017-12-10T15:00:18.980756: step 3376, loss 0.182717, acc 0.953125, prec 0.0718668, recall 0.902764
2017-12-10T15:00:19.172426: step 3377, loss 1.98545, acc 0.953125, prec 0.0718871, recall 0.902502
2017-12-10T15:00:19.369189: step 3378, loss 0.0577917, acc 0.96875, prec 0.0718834, recall 0.902502
2017-12-10T15:00:19.562679: step 3379, loss 0.0239401, acc 0.984375, prec 0.0718816, recall 0.902502
2017-12-10T15:00:19.754508: step 3380, loss 0.124699, acc 0.984375, prec 0.0719277, recall 0.902566
2017-12-10T15:00:19.951341: step 3381, loss 0.10303, acc 0.953125, prec 0.0719222, recall 0.902566
2017-12-10T15:00:20.142861: step 3382, loss 0.194701, acc 0.9375, prec 0.0719627, recall 0.902629
2017-12-10T15:00:20.334847: step 3383, loss 0.174807, acc 0.921875, prec 0.0719534, recall 0.902629
2017-12-10T15:00:20.534343: step 3384, loss 0.265432, acc 0.9375, prec 0.071946, recall 0.902629
2017-12-10T15:00:20.726901: step 3385, loss 0.36819, acc 0.875, prec 0.0719551, recall 0.902661
2017-12-10T15:00:20.920888: step 3386, loss 0.290517, acc 0.9375, prec 0.0719477, recall 0.902661
2017-12-10T15:00:21.111636: step 3387, loss 0.157681, acc 0.953125, prec 0.0719661, recall 0.902692
2017-12-10T15:00:21.300722: step 3388, loss 0.199819, acc 0.921875, prec 0.0719568, recall 0.902692
2017-12-10T15:00:21.491236: step 3389, loss 0.329661, acc 0.921875, prec 0.0719475, recall 0.902692
2017-12-10T15:00:21.689426: step 3390, loss 0.0500641, acc 0.96875, prec 0.0719437, recall 0.902692
2017-12-10T15:00:21.882272: step 3391, loss 0.296449, acc 0.921875, prec 0.0719584, recall 0.902724
2017-12-10T15:00:22.072257: step 3392, loss 0.168114, acc 0.921875, prec 0.0719731, recall 0.902755
2017-12-10T15:00:22.271860: step 3393, loss 0.171595, acc 0.953125, prec 0.0719675, recall 0.902755
2017-12-10T15:00:22.465518: step 3394, loss 0.281805, acc 0.90625, prec 0.0720043, recall 0.902818
2017-12-10T15:00:22.657745: step 3395, loss 0.104694, acc 0.953125, prec 0.0719988, recall 0.902818
2017-12-10T15:00:22.847070: step 3396, loss 1.63406, acc 0.859375, prec 0.0719839, recall 0.902526
2017-12-10T15:00:23.039130: step 3397, loss 0.242372, acc 0.953125, prec 0.0720262, recall 0.902589
2017-12-10T15:00:23.231261: step 3398, loss 0.106315, acc 0.9375, prec 0.0720188, recall 0.902589
2017-12-10T15:00:23.419940: step 3399, loss 0.194328, acc 0.9375, prec 0.0720353, recall 0.90262
2017-12-10T15:00:23.615556: step 3400, loss 0.124856, acc 0.984375, prec 0.0720574, recall 0.902652
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_0/1512935327/checkpoints/model-3400

2017-12-10T15:00:24.814870: step 3401, loss 0.1958, acc 0.921875, prec 0.0720481, recall 0.902652
2017-12-10T15:00:25.010395: step 3402, loss 0.279075, acc 0.9375, prec 0.0720646, recall 0.902683
2017-12-10T15:00:25.204804: step 3403, loss 0.153296, acc 0.96875, prec 0.0720849, recall 0.902715
2017-12-10T15:00:25.402944: step 3404, loss 0.0958837, acc 0.96875, prec 0.0721051, recall 0.902746
2017-12-10T15:00:26.318413: step 3405, loss 0.341475, acc 0.953125, prec 0.0721953, recall 0.902872
2017-12-10T15:00:26.622433: step 3406, loss 0.277231, acc 0.921875, prec 0.072186, recall 0.902872
2017-12-10T15:00:26.961262: step 3407, loss 0.376318, acc 0.859375, prec 0.0721692, recall 0.902872
2017-12-10T15:00:27.260844: step 3408, loss 1.05092, acc 0.96875, prec 0.0721894, recall 0.902903
2017-12-10T15:00:27.465901: step 3409, loss 0.177171, acc 0.96875, prec 0.0722814, recall 0.903028
2017-12-10T15:00:27.662312: step 3410, loss 0.376811, acc 0.90625, prec 0.0722941, recall 0.90306
2017-12-10T15:00:27.857549: step 3411, loss 0.192753, acc 0.9375, prec 0.0722867, recall 0.90306
2017-12-10T15:00:28.050533: step 3412, loss 0.0951726, acc 0.984375, prec 0.0723087, recall 0.903091
2017-12-10T15:00:28.245744: step 3413, loss 0.310263, acc 0.90625, prec 0.0722975, recall 0.903091
2017-12-10T15:00:28.445177: step 3414, loss 0.383814, acc 0.875, prec 0.0723065, recall 0.903122
2017-12-10T15:00:28.637136: step 3415, loss 0.276008, acc 0.921875, prec 0.0723211, recall 0.903153
2017-12-10T15:00:28.829130: step 3416, loss 0.224745, acc 0.9375, prec 0.0723615, recall 0.903215
2017-12-10T15:00:29.023093: step 3417, loss 0.355692, acc 0.921875, prec 0.0724477, recall 0.90334
2017-12-10T15:00:29.221659: step 3418, loss 0.221887, acc 0.9375, prec 0.0725119, recall 0.903433
2017-12-10T15:00:29.413112: step 3419, loss 0.113903, acc 0.984375, prec 0.0725339, recall 0.903464
2017-12-10T15:00:29.609118: step 3420, loss 0.26997, acc 0.953125, prec 0.0725283, recall 0.903464
2017-12-10T15:00:29.802548: step 3421, loss 0.0813146, acc 0.96875, prec 0.0725246, recall 0.903464
2017-12-10T15:00:29.995735: step 3422, loss 0.0500523, acc 0.984375, prec 0.0725466, recall 0.903495
2017-12-10T15:00:30.186537: step 3423, loss 0.108027, acc 0.96875, prec 0.0725429, recall 0.903495
2017-12-10T15:00:30.376654: step 3424, loss 0.355257, acc 0.9375, prec 0.0725593, recall 0.903526
2017-12-10T15:00:30.572672: step 3425, loss 0.271476, acc 0.953125, prec 0.0726014, recall 0.903587
2017-12-10T15:00:30.769963: step 3426, loss 0.144265, acc 0.96875, prec 0.0726454, recall 0.903649
2017-12-10T15:00:30.960285: step 3427, loss 0.442166, acc 0.921875, prec 0.0726599, recall 0.90368
2017-12-10T15:00:31.160370: step 3428, loss 0.0729359, acc 0.96875, prec 0.0726562, recall 0.90368
2017-12-10T15:00:31.356734: step 3429, loss 0.157425, acc 0.953125, prec 0.0726744, recall 0.903711
2017-12-10T15:00:31.553453: step 3430, loss 0.205146, acc 0.96875, prec 0.0726945, recall 0.903742
2017-12-10T15:00:31.745095: step 3431, loss 0.0251975, acc 0.984375, prec 0.0727165, recall 0.903772
2017-12-10T15:00:31.936798: step 3432, loss 0.206606, acc 0.984375, prec 0.0727623, recall 0.903834
2017-12-10T15:00:32.130448: step 3433, loss 1.02529, acc 0.90625, prec 0.072775, recall 0.903865
2017-12-10T15:00:32.328260: step 3434, loss 0.0746172, acc 0.953125, prec 0.0727694, recall 0.903865
2017-12-10T15:00:32.520606: step 3435, loss 0.41452, acc 0.96875, prec 0.0727894, recall 0.903895
2017-12-10T15:00:32.709137: step 3436, loss 0.0547997, acc 0.984375, prec 0.0727876, recall 0.903895
2017-12-10T15:00:32.901511: step 3437, loss 0.126434, acc 0.984375, prec 0.0727857, recall 0.903895
2017-12-10T15:00:33.097436: step 3438, loss 0.090266, acc 0.984375, prec 0.0728077, recall 0.903926
2017-12-10T15:00:33.297538: step 3439, loss 0.202449, acc 0.96875, prec 0.0728278, recall 0.903957
2017-12-10T15:00:33.493426: step 3440, loss 0.151017, acc 0.953125, prec 0.072846, recall 0.903987
2017-12-10T15:00:33.687823: step 3441, loss 0.519425, acc 0.859375, prec 0.072853, recall 0.904018
2017-12-10T15:00:33.881361: step 3442, loss 0.330067, acc 0.9375, prec 0.0728931, recall 0.904079
2017-12-10T15:00:34.076432: step 3443, loss 0.0435007, acc 0.984375, prec 0.0728912, recall 0.904079
2017-12-10T15:00:34.271021: step 3444, loss 0.510175, acc 0.953125, prec 0.0729094, recall 0.90411
2017-12-10T15:00:34.471047: step 3445, loss 0.116023, acc 0.953125, prec 0.0729038, recall 0.90411
2017-12-10T15:00:34.662567: step 3446, loss 0.108241, acc 0.953125, prec 0.072922, recall 0.90414
2017-12-10T15:00:34.858081: step 3447, loss 1.82066, acc 0.96875, prec 0.0729201, recall 0.903852
2017-12-10T15:00:35.054280: step 3448, loss 0.0929524, acc 0.96875, prec 0.0729164, recall 0.903852
2017-12-10T15:00:35.247032: step 3449, loss 0.127948, acc 0.953125, prec 0.0729346, recall 0.903883
2017-12-10T15:00:35.438230: step 3450, loss 0.858068, acc 0.9375, prec 0.0729509, recall 0.903913
2017-12-10T15:00:35.637587: step 3451, loss 0.0984161, acc 0.96875, prec 0.0730424, recall 0.904036
2017-12-10T15:00:35.829978: step 3452, loss 0.210728, acc 0.921875, prec 0.0730806, recall 0.904097
2017-12-10T15:00:36.024300: step 3453, loss 0.341294, acc 0.890625, prec 0.0730674, recall 0.904097
2017-12-10T15:00:36.217832: step 3454, loss 0.451111, acc 0.875, prec 0.0730525, recall 0.904097
2017-12-10T15:00:36.411387: step 3455, loss 0.278714, acc 0.90625, prec 0.073065, recall 0.904127
2017-12-10T15:00:36.603166: step 3456, loss 0.365822, acc 0.90625, prec 0.0730537, recall 0.904127
2017-12-10T15:00:36.793197: step 3457, loss 0.234527, acc 0.90625, prec 0.0730425, recall 0.904127
2017-12-10T15:00:36.985867: step 3458, loss 0.152185, acc 0.921875, prec 0.0730331, recall 0.904127
2017-12-10T15:00:37.181749: step 3459, loss 0.164331, acc 0.921875, prec 0.0730713, recall 0.904188
2017-12-10T15:00:37.372700: step 3460, loss 0.326864, acc 0.921875, prec 0.0730619, recall 0.904188
2017-12-10T15:00:37.566187: step 3461, loss 0.257863, acc 0.921875, prec 0.0730763, recall 0.904218
2017-12-10T15:00:37.762012: step 3462, loss 0.641139, acc 0.890625, prec 0.0731107, recall 0.904279
2017-12-10T15:00:37.958604: step 3463, loss 0.468992, acc 0.875, prec 0.0730957, recall 0.904279
2017-12-10T15:00:38.153156: step 3464, loss 0.183885, acc 0.9375, prec 0.073112, recall 0.904309
2017-12-10T15:00:38.350071: step 3465, loss 0.352956, acc 0.921875, prec 0.0731739, recall 0.9044
2017-12-10T15:00:38.547228: step 3466, loss 0.0770204, acc 0.96875, prec 0.0731938, recall 0.90443
2017-12-10T15:00:38.740991: step 3467, loss 0.87452, acc 0.96875, prec 0.0732376, recall 0.904491
2017-12-10T15:00:38.939111: step 3468, loss 0.14891, acc 0.96875, prec 0.0732575, recall 0.904521
2017-12-10T15:00:39.135423: step 3469, loss 0.161614, acc 0.96875, prec 0.0733487, recall 0.904642
2017-12-10T15:00:39.328620: step 3470, loss 0.28125, acc 0.90625, prec 0.0733612, recall 0.904672
2017-12-10T15:00:39.523787: step 3471, loss 0.159045, acc 0.984375, prec 0.0734067, recall 0.904732
2017-12-10T15:00:39.725307: step 3472, loss 0.0550941, acc 0.96875, prec 0.0734029, recall 0.904732
2017-12-10T15:00:39.918182: step 3473, loss 0.454468, acc 0.921875, prec 0.0733936, recall 0.904732
2017-12-10T15:00:40.110567: step 3474, loss 0.903763, acc 0.953125, prec 0.0734353, recall 0.904792
2017-12-10T15:00:40.309120: step 3475, loss 0.237191, acc 0.953125, prec 0.0735008, recall 0.904882
2017-12-10T15:00:40.503556: step 3476, loss 0.367958, acc 0.84375, prec 0.0735057, recall 0.904912
2017-12-10T15:00:40.697534: step 3477, loss 0.268091, acc 0.921875, prec 0.07352, recall 0.904942
2017-12-10T15:00:40.890148: step 3478, loss 0.215773, acc 0.90625, prec 0.0735324, recall 0.904972
2017-12-10T15:00:41.066410: step 3479, loss 0.247901, acc 0.901961, prec 0.073523, recall 0.904972
2017-12-10T15:00:41.266986: step 3480, loss 0.177239, acc 0.921875, prec 0.0735136, recall 0.904972
2017-12-10T15:00:41.459545: step 3481, loss 0.14682, acc 0.96875, prec 0.0735099, recall 0.904972
2017-12-10T15:00:41.654602: step 3482, loss 0.130249, acc 0.9375, prec 0.0735024, recall 0.904972
2017-12-10T15:00:41.845992: step 3483, loss 0.270071, acc 0.90625, prec 0.0734911, recall 0.904972
2017-12-10T15:00:42.039638: step 3484, loss 0.245957, acc 0.90625, prec 0.0735035, recall 0.905002
2017-12-10T15:00:42.233489: step 3485, loss 0.245518, acc 0.90625, prec 0.0734922, recall 0.905002
2017-12-10T15:00:42.426805: step 3486, loss 0.0747746, acc 0.984375, prec 0.0734903, recall 0.905002
2017-12-10T15:00:42.619796: step 3487, loss 0.51225, acc 0.84375, prec 0.0734716, recall 0.905002
2017-12-10T15:00:42.808647: step 3488, loss 0.152255, acc 0.96875, prec 0.0735151, recall 0.905061
2017-12-10T15:00:43.002839: step 3489, loss 0.674122, acc 0.9375, prec 0.0735313, recall 0.905091
2017-12-10T15:00:43.196268: step 3490, loss 0.0722723, acc 0.984375, prec 0.0735531, recall 0.905121
2017-12-10T15:00:43.392424: step 3491, loss 0.167473, acc 0.9375, prec 0.0735692, recall 0.905151
2017-12-10T15:00:43.587484: step 3492, loss 0.362412, acc 0.96875, prec 0.0736127, recall 0.90521
2017-12-10T15:00:43.783200: step 3493, loss 0.0656117, acc 0.96875, prec 0.073609, recall 0.90521
2017-12-10T15:00:43.980249: step 3494, loss 0.0727022, acc 0.953125, prec 0.0736033, recall 0.90521
2017-12-10T15:00:44.176059: step 3495, loss 0.32011, acc 0.921875, prec 0.0736176, recall 0.90524
2017-12-10T15:00:44.371543: step 3496, loss 0.293565, acc 0.953125, prec 0.073612, recall 0.90524
2017-12-10T15:00:44.562085: step 3497, loss 0.290489, acc 0.90625, prec 0.0736243, recall 0.90527
2017-12-10T15:00:44.756804: step 3498, loss 0.0839103, acc 0.96875, prec 0.0736206, recall 0.90527
2017-12-10T15:00:44.949455: step 3499, loss 0.276537, acc 0.90625, prec 0.0736329, recall 0.905299
2017-12-10T15:00:45.149149: step 3500, loss 0.144079, acc 1, prec 0.0737274, recall 0.905418
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_0/1512935327/checkpoints/model-3500

2017-12-10T15:00:46.381776: step 3501, loss 0.107823, acc 0.96875, prec 0.0737473, recall 0.905448
2017-12-10T15:00:46.575240: step 3502, loss 0.107491, acc 0.96875, prec 0.0737435, recall 0.905448
2017-12-10T15:00:46.769765: step 3503, loss 0.0183082, acc 1, prec 0.0737435, recall 0.905448
2017-12-10T15:00:46.966298: step 3504, loss 0.0744866, acc 0.96875, prec 0.0737634, recall 0.905477
2017-12-10T15:00:47.164346: step 3505, loss 0.174131, acc 0.984375, prec 0.0737851, recall 0.905507
2017-12-10T15:00:47.359782: step 3506, loss 0.0796491, acc 0.984375, prec 0.0738069, recall 0.905536
2017-12-10T15:00:47.552388: step 3507, loss 0.0884549, acc 0.984375, prec 0.0738286, recall 0.905566
2017-12-10T15:00:47.749764: step 3508, loss 0.0174893, acc 1, prec 0.0738286, recall 0.905566
2017-12-10T15:00:47.945997: step 3509, loss 0.0693249, acc 1, prec 0.0738522, recall 0.905595
2017-12-10T15:00:48.145319: step 3510, loss 0.150425, acc 0.953125, prec 0.0738465, recall 0.905595
2017-12-10T15:00:48.343049: step 3511, loss 0.0699871, acc 1, prec 0.0739174, recall 0.905684
2017-12-10T15:00:48.538202: step 3512, loss 0.0031311, acc 1, prec 0.0739174, recall 0.905684
2017-12-10T15:00:48.728637: step 3513, loss 0.151519, acc 0.96875, prec 0.0739844, recall 0.905772
2017-12-10T15:00:48.926285: step 3514, loss 0.238873, acc 0.921875, prec 0.073975, recall 0.905772
2017-12-10T15:00:49.123686: step 3515, loss 0.016946, acc 0.984375, prec 0.0739731, recall 0.905772
2017-12-10T15:00:49.324500: step 3516, loss 0.025056, acc 0.984375, prec 0.0739948, recall 0.905802
2017-12-10T15:00:49.518193: step 3517, loss 0.111489, acc 0.984375, prec 0.0740401, recall 0.90586
2017-12-10T15:00:49.709223: step 3518, loss 0.024537, acc 0.984375, prec 0.0740382, recall 0.90586
2017-12-10T15:00:49.900237: step 3519, loss 0.0240817, acc 1, prec 0.0740382, recall 0.90586
2017-12-10T15:00:50.099466: step 3520, loss 0.00400545, acc 1, prec 0.0740382, recall 0.90586
2017-12-10T15:00:50.293352: step 3521, loss 0.0542347, acc 0.96875, prec 0.0740344, recall 0.90586
2017-12-10T15:00:50.487145: step 3522, loss 0.0543326, acc 0.984375, prec 0.0740797, recall 0.905919
2017-12-10T15:00:50.682937: step 3523, loss 0.225913, acc 1, prec 0.0741505, recall 0.906007
2017-12-10T15:00:50.877874: step 3524, loss 0.167314, acc 0.953125, prec 0.0741684, recall 0.906036
2017-12-10T15:00:51.072546: step 3525, loss 0.0969121, acc 0.984375, prec 0.0741665, recall 0.906036
2017-12-10T15:00:51.266317: step 3526, loss 0.112325, acc 0.96875, prec 0.0741863, recall 0.906065
2017-12-10T15:00:51.463141: step 3527, loss 0.0999953, acc 0.96875, prec 0.0741825, recall 0.906065
2017-12-10T15:00:51.656709: step 3528, loss 1.00256, acc 0.96875, prec 0.0742023, recall 0.906095
2017-12-10T15:00:51.857272: step 3529, loss 0.0210907, acc 0.984375, prec 0.0742005, recall 0.906095
2017-12-10T15:00:52.054097: step 3530, loss 4.09575, acc 0.984375, prec 0.0742476, recall 0.905871
2017-12-10T15:00:52.249687: step 3531, loss 0.0550406, acc 0.96875, prec 0.074291, recall 0.90593
2017-12-10T15:00:52.445868: step 3532, loss 0.127036, acc 0.953125, prec 0.0742853, recall 0.90593
2017-12-10T15:00:52.641518: step 3533, loss 0.147301, acc 0.9375, prec 0.0743013, recall 0.905959
2017-12-10T15:00:52.837747: step 3534, loss 0.150934, acc 0.9375, prec 0.0742937, recall 0.905959
2017-12-10T15:00:53.033088: step 3535, loss 0.364451, acc 0.859375, prec 0.0743474, recall 0.906047
2017-12-10T15:00:53.228599: step 3536, loss 0.388676, acc 0.921875, prec 0.074385, recall 0.906105
2017-12-10T15:00:53.423390: step 3537, loss 0.488538, acc 0.859375, prec 0.074368, recall 0.906105
2017-12-10T15:00:53.619840: step 3538, loss 0.641374, acc 0.734375, prec 0.0743829, recall 0.906163
2017-12-10T15:00:53.813157: step 3539, loss 0.629654, acc 0.796875, prec 0.0743583, recall 0.906163
2017-12-10T15:00:54.002814: step 3540, loss 0.375503, acc 0.875, prec 0.0743902, recall 0.906221
2017-12-10T15:00:54.192976: step 3541, loss 0.865444, acc 0.75, prec 0.0743835, recall 0.90625
2017-12-10T15:00:54.383628: step 3542, loss 0.42863, acc 0.84375, prec 0.0743881, recall 0.906279
2017-12-10T15:00:54.577366: step 3543, loss 0.558749, acc 0.90625, prec 0.0744238, recall 0.906337
2017-12-10T15:00:54.769307: step 3544, loss 0.486936, acc 0.90625, prec 0.074436, recall 0.906366
2017-12-10T15:00:54.962862: step 3545, loss 0.568741, acc 0.8125, prec 0.0744603, recall 0.906424
2017-12-10T15:00:55.155939: step 3546, loss 0.360005, acc 0.875, prec 0.0744921, recall 0.906482
2017-12-10T15:00:55.350177: step 3547, loss 0.17437, acc 0.921875, prec 0.0744827, recall 0.906482
2017-12-10T15:00:55.544696: step 3548, loss 0.432717, acc 0.890625, prec 0.0745164, recall 0.906539
2017-12-10T15:00:55.736213: step 3549, loss 0.23718, acc 0.859375, prec 0.0744994, recall 0.906539
2017-12-10T15:00:55.929437: step 3550, loss 0.179697, acc 0.953125, prec 0.0745172, recall 0.906568
2017-12-10T15:00:56.125324: step 3551, loss 0.086667, acc 0.953125, prec 0.0745115, recall 0.906568
2017-12-10T15:00:56.315559: step 3552, loss 0.0649596, acc 0.96875, prec 0.0745077, recall 0.906568
2017-12-10T15:00:56.508743: step 3553, loss 0.0255059, acc 1, prec 0.0745077, recall 0.906568
2017-12-10T15:00:56.706014: step 3554, loss 0.135443, acc 0.953125, prec 0.0745255, recall 0.906597
2017-12-10T15:00:56.899277: step 3555, loss 0.0942664, acc 0.9375, prec 0.0745414, recall 0.906626
2017-12-10T15:00:57.097776: step 3556, loss 0.075425, acc 0.953125, prec 0.0745357, recall 0.906626
2017-12-10T15:00:57.288136: step 3557, loss 0.10919, acc 0.953125, prec 0.0745301, recall 0.906626
2017-12-10T15:00:57.479074: step 3558, loss 0.0666839, acc 0.96875, prec 0.0745263, recall 0.906626
2017-12-10T15:00:57.671369: step 3559, loss 0.0426014, acc 0.984375, prec 0.0745478, recall 0.906654
2017-12-10T15:00:57.865825: step 3560, loss 0.144409, acc 0.984375, prec 0.0745694, recall 0.906683
2017-12-10T15:00:58.057407: step 3561, loss 0.951331, acc 1, prec 0.0746163, recall 0.906741
2017-12-10T15:00:58.255098: step 3562, loss 0.0732689, acc 0.984375, prec 0.0746144, recall 0.906741
2017-12-10T15:00:58.448032: step 3563, loss 0.3809, acc 0.953125, prec 0.0746322, recall 0.906769
2017-12-10T15:00:58.645757: step 3564, loss 3.02228, acc 0.9375, prec 0.0746265, recall 0.90649
2017-12-10T15:00:58.844212: step 3565, loss 0.112624, acc 0.953125, prec 0.0746208, recall 0.90649
2017-12-10T15:00:59.038142: step 3566, loss 0.0742242, acc 0.984375, prec 0.0746424, recall 0.906519
2017-12-10T15:00:59.238256: step 3567, loss 0.130761, acc 0.953125, prec 0.0746601, recall 0.906548
2017-12-10T15:00:59.433243: step 3568, loss 0.185168, acc 0.953125, prec 0.0746544, recall 0.906548
2017-12-10T15:00:59.628618: step 3569, loss 0.110351, acc 0.953125, prec 0.0746956, recall 0.906605
2017-12-10T15:00:59.822909: step 3570, loss 0.123957, acc 0.984375, prec 0.0747172, recall 0.906634
2017-12-10T15:01:00.019143: step 3571, loss 0.207076, acc 0.953125, prec 0.0747115, recall 0.906634
2017-12-10T15:01:00.210746: step 3572, loss 0.626419, acc 0.96875, prec 0.0747311, recall 0.906663
2017-12-10T15:01:00.403417: step 3573, loss 0.378535, acc 0.90625, prec 0.0747432, recall 0.906691
2017-12-10T15:01:00.599472: step 3574, loss 0.203836, acc 0.890625, prec 0.0747768, recall 0.906748
2017-12-10T15:01:00.788928: step 3575, loss 0.565736, acc 0.84375, prec 0.0747578, recall 0.906748
2017-12-10T15:01:00.977909: step 3576, loss 0.212592, acc 0.9375, prec 0.0747737, recall 0.906777
2017-12-10T15:01:01.174521: step 3577, loss 0.133676, acc 0.921875, prec 0.0747642, recall 0.906777
2017-12-10T15:01:01.372745: step 3578, loss 0.157846, acc 0.953125, prec 0.0747586, recall 0.906777
2017-12-10T15:01:01.574637: step 3579, loss 0.23955, acc 0.90625, prec 0.0747706, recall 0.906806
2017-12-10T15:01:01.764573: step 3580, loss 0.0757444, acc 0.96875, prec 0.074837, recall 0.906891
2017-12-10T15:01:01.961275: step 3581, loss 0.135701, acc 0.953125, prec 0.0748781, recall 0.906948
2017-12-10T15:01:02.161315: step 3582, loss 0.263701, acc 0.953125, prec 0.0748958, recall 0.906977
2017-12-10T15:01:02.355494: step 3583, loss 0.411085, acc 0.875, prec 0.074904, recall 0.907005
2017-12-10T15:01:02.546617: step 3584, loss 0.393398, acc 0.921875, prec 0.0749179, recall 0.907034
2017-12-10T15:01:02.743592: step 3585, loss 0.15831, acc 0.953125, prec 0.074959, recall 0.90709
2017-12-10T15:01:02.940643: step 3586, loss 0.359674, acc 0.9375, prec 0.0749747, recall 0.907119
2017-12-10T15:01:03.137278: step 3587, loss 0.275428, acc 0.921875, prec 0.0749653, recall 0.907119
2017-12-10T15:01:03.327772: step 3588, loss 0.246724, acc 0.921875, prec 0.0749792, recall 0.907147
2017-12-10T15:01:03.518122: step 3589, loss 0.18608, acc 0.9375, prec 0.074995, recall 0.907176
2017-12-10T15:01:03.710938: step 3590, loss 0.249238, acc 0.953125, prec 0.0750126, recall 0.907204
2017-12-10T15:01:03.904798: step 3591, loss 0.31533, acc 0.96875, prec 0.0750322, recall 0.907232
2017-12-10T15:01:04.097876: step 3592, loss 0.510024, acc 0.96875, prec 0.0750751, recall 0.907289
2017-12-10T15:01:04.292380: step 3593, loss 0.0992047, acc 0.953125, prec 0.0750694, recall 0.907289
2017-12-10T15:01:04.484833: step 3594, loss 0.081731, acc 0.96875, prec 0.0750889, recall 0.907317
2017-12-10T15:01:04.675187: step 3595, loss 0.26055, acc 0.921875, prec 0.0750795, recall 0.907317
2017-12-10T15:01:04.867849: step 3596, loss 0.177449, acc 0.953125, prec 0.0751671, recall 0.90743
2017-12-10T15:01:05.060496: step 3597, loss 0.139263, acc 0.953125, prec 0.0751614, recall 0.90743
2017-12-10T15:01:05.255796: step 3598, loss 0.147698, acc 0.96875, prec 0.0751576, recall 0.90743
2017-12-10T15:01:05.452694: step 3599, loss 0.0915941, acc 0.96875, prec 0.0752238, recall 0.907514
2017-12-10T15:01:05.649207: step 3600, loss 0.00524999, acc 1, prec 0.0752238, recall 0.907514
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_0/1512935327/checkpoints/model-3600

2017-12-10T15:01:06.883116: step 3601, loss 0.0675924, acc 0.984375, prec 0.0752219, recall 0.907514
2017-12-10T15:01:07.076728: step 3602, loss 0.95257, acc 0.953125, prec 0.0752629, recall 0.907571
2017-12-10T15:01:07.272349: step 3603, loss 0.124641, acc 0.921875, prec 0.0752534, recall 0.907571
2017-12-10T15:01:07.469417: step 3604, loss 0.00465631, acc 1, prec 0.0752767, recall 0.907599
2017-12-10T15:01:07.663025: step 3605, loss 0.123199, acc 0.9375, prec 0.0752691, recall 0.907599
2017-12-10T15:01:07.857550: step 3606, loss 0.51515, acc 0.953125, prec 0.07531, recall 0.907655
2017-12-10T15:01:08.053394: step 3607, loss 0.542251, acc 0.90625, prec 0.0753219, recall 0.907683
2017-12-10T15:01:08.250478: step 3608, loss 0.0219359, acc 0.984375, prec 0.07532, recall 0.907683
2017-12-10T15:01:08.449181: step 3609, loss 0.250161, acc 1, prec 0.0753666, recall 0.907739
2017-12-10T15:01:08.649244: step 3610, loss 0.0326871, acc 0.984375, prec 0.075388, recall 0.907767
2017-12-10T15:01:08.845532: step 3611, loss 0.0491016, acc 0.96875, prec 0.0754075, recall 0.907795
2017-12-10T15:01:09.038668: step 3612, loss 0.126655, acc 0.96875, prec 0.075427, recall 0.907823
2017-12-10T15:01:09.234982: step 3613, loss 0.15577, acc 0.984375, prec 0.0754484, recall 0.907851
2017-12-10T15:01:09.433785: step 3614, loss 0.0893175, acc 0.96875, prec 0.0754679, recall 0.907879
2017-12-10T15:01:09.629787: step 3615, loss 0.0451959, acc 0.96875, prec 0.0754641, recall 0.907879
2017-12-10T15:01:09.825633: step 3616, loss 0.0243594, acc 0.984375, prec 0.0754622, recall 0.907879
2017-12-10T15:01:10.018578: step 3617, loss 0.0140716, acc 1, prec 0.0755088, recall 0.907935
2017-12-10T15:01:10.211631: step 3618, loss 0.176024, acc 0.953125, prec 0.0755031, recall 0.907935
2017-12-10T15:01:10.403609: step 3619, loss 0.179635, acc 0.953125, prec 0.0755206, recall 0.907962
2017-12-10T15:01:10.600244: step 3620, loss 0.168162, acc 0.9375, prec 0.0755363, recall 0.90799
2017-12-10T15:01:10.793140: step 3621, loss 0.793736, acc 1, prec 0.0755596, recall 0.908018
2017-12-10T15:01:10.994073: step 3622, loss 0.10433, acc 0.96875, prec 0.0755558, recall 0.908018
2017-12-10T15:01:11.191467: step 3623, loss 0.0406775, acc 0.984375, prec 0.0755539, recall 0.908018
2017-12-10T15:01:11.389811: step 3624, loss 0.579513, acc 0.984375, prec 0.0755985, recall 0.908074
2017-12-10T15:01:11.584273: step 3625, loss 8.02387, acc 0.9375, prec 0.0756161, recall 0.907827
2017-12-10T15:01:11.781249: step 3626, loss 0.0410255, acc 0.984375, prec 0.0756374, recall 0.907855
2017-12-10T15:01:11.974614: step 3627, loss 0.197216, acc 0.921875, prec 0.0756512, recall 0.907883
2017-12-10T15:01:12.167061: step 3628, loss 0.218036, acc 0.90625, prec 0.0756398, recall 0.907883
2017-12-10T15:01:12.359783: step 3629, loss 0.359543, acc 0.90625, prec 0.0756283, recall 0.907883
2017-12-10T15:01:12.550767: step 3630, loss 0.224884, acc 0.90625, prec 0.0756867, recall 0.907966
2017-12-10T15:01:12.747071: step 3631, loss 0.205535, acc 0.921875, prec 0.0756772, recall 0.907966
2017-12-10T15:01:12.940486: step 3632, loss 0.445112, acc 0.859375, prec 0.0757065, recall 0.908022
2017-12-10T15:01:13.132200: step 3633, loss 0.303307, acc 0.84375, prec 0.0757107, recall 0.908049
2017-12-10T15:01:13.323265: step 3634, loss 0.329066, acc 0.921875, prec 0.0757244, recall 0.908077
2017-12-10T15:01:13.518832: step 3635, loss 0.513819, acc 0.875, prec 0.0757092, recall 0.908077
2017-12-10T15:01:13.713330: step 3636, loss 0.316246, acc 0.921875, prec 0.0757694, recall 0.90816
2017-12-10T15:01:13.909877: step 3637, loss 0.229324, acc 0.921875, prec 0.0757831, recall 0.908188
2017-12-10T15:01:14.106771: step 3638, loss 0.559691, acc 0.875, prec 0.0757911, recall 0.908215
2017-12-10T15:01:14.306771: step 3639, loss 0.265375, acc 0.90625, prec 0.075826, recall 0.908271
2017-12-10T15:01:14.501388: step 3640, loss 0.349613, acc 0.90625, prec 0.0758378, recall 0.908298
2017-12-10T15:01:14.697991: step 3641, loss 0.22813, acc 0.890625, prec 0.0758477, recall 0.908326
2017-12-10T15:01:14.890572: step 3642, loss 0.265807, acc 0.875, prec 0.0758789, recall 0.908381
2017-12-10T15:01:15.085459: step 3643, loss 0.294721, acc 0.890625, prec 0.0758655, recall 0.908381
2017-12-10T15:01:15.281099: step 3644, loss 0.207882, acc 0.921875, prec 0.075856, recall 0.908381
2017-12-10T15:01:15.480632: step 3645, loss 0.19528, acc 0.953125, prec 0.0758967, recall 0.908436
2017-12-10T15:01:15.671782: step 3646, loss 0.194496, acc 0.953125, prec 0.0759373, recall 0.908491
2017-12-10T15:01:15.864590: step 3647, loss 0.235009, acc 0.96875, prec 0.0759798, recall 0.908546
2017-12-10T15:01:16.060953: step 3648, loss 0.234383, acc 0.921875, prec 0.0760166, recall 0.908601
2017-12-10T15:01:16.256649: step 3649, loss 0.156669, acc 0.921875, prec 0.0760534, recall 0.908655
2017-12-10T15:01:16.450591: step 3650, loss 0.02749, acc 0.984375, prec 0.0760747, recall 0.908683
2017-12-10T15:01:16.643775: step 3651, loss 0.171353, acc 0.921875, prec 0.0760883, recall 0.90871
2017-12-10T15:01:16.840369: step 3652, loss 0.0622625, acc 0.953125, prec 0.0761289, recall 0.908765
2017-12-10T15:01:17.041346: step 3653, loss 0.110639, acc 0.984375, prec 0.0761501, recall 0.908792
2017-12-10T15:01:17.237333: step 3654, loss 0.121698, acc 0.921875, prec 0.0761638, recall 0.908819
2017-12-10T15:01:17.438109: step 3655, loss 0.0233605, acc 1, prec 0.0761638, recall 0.908819
2017-12-10T15:01:17.632108: step 3656, loss 0.0654621, acc 0.984375, prec 0.0761618, recall 0.908819
2017-12-10T15:01:17.826968: step 3657, loss 0.0324835, acc 1, prec 0.0762081, recall 0.908874
2017-12-10T15:01:18.020310: step 3658, loss 0.355673, acc 0.96875, prec 0.0762275, recall 0.908901
2017-12-10T15:01:18.218540: step 3659, loss 1.06024, acc 0.984375, prec 0.0762487, recall 0.908928
2017-12-10T15:01:18.417917: step 3660, loss 0.127446, acc 0.953125, prec 0.076243, recall 0.908928
2017-12-10T15:01:18.614109: step 3661, loss 0.244117, acc 0.9375, prec 0.0762353, recall 0.908928
2017-12-10T15:01:18.806904: step 3662, loss 0.11858, acc 0.96875, prec 0.0762315, recall 0.908928
2017-12-10T15:01:19.005914: step 3663, loss 0.00349074, acc 1, prec 0.0762315, recall 0.908928
2017-12-10T15:01:19.198492: step 3664, loss 0.0706768, acc 0.96875, prec 0.0762277, recall 0.908928
2017-12-10T15:01:19.390919: step 3665, loss 0.109268, acc 0.953125, prec 0.076222, recall 0.908928
2017-12-10T15:01:19.581543: step 3666, loss 0.515346, acc 1, prec 0.0762451, recall 0.908955
2017-12-10T15:01:19.780879: step 3667, loss 0.250408, acc 0.953125, prec 0.0762625, recall 0.908982
2017-12-10T15:01:19.977214: step 3668, loss 0.280989, acc 0.9375, prec 0.0763242, recall 0.909064
2017-12-10T15:01:20.172784: step 3669, loss 0.0883962, acc 0.984375, prec 0.0763454, recall 0.909091
2017-12-10T15:01:20.365389: step 3670, loss 0.480637, acc 0.953125, prec 0.0763859, recall 0.909145
2017-12-10T15:01:20.556203: step 3671, loss 0.293285, acc 0.953125, prec 0.0764264, recall 0.909199
2017-12-10T15:01:20.749337: step 3672, loss 0.122552, acc 0.96875, prec 0.0764919, recall 0.90928
2017-12-10T15:01:20.945969: step 3673, loss 0.389665, acc 0.921875, prec 0.0765055, recall 0.909307
2017-12-10T15:01:21.137147: step 3674, loss 0.202606, acc 0.921875, prec 0.0764959, recall 0.909307
2017-12-10T15:01:21.332379: step 3675, loss 0.193067, acc 0.9375, prec 0.0764882, recall 0.909307
2017-12-10T15:01:21.524736: step 3676, loss 0.124653, acc 0.953125, prec 0.0765287, recall 0.909361
2017-12-10T15:01:21.721414: step 3677, loss 0.460327, acc 0.90625, prec 0.0765634, recall 0.909415
2017-12-10T15:01:21.920849: step 3678, loss 0.0762074, acc 0.984375, prec 0.0766077, recall 0.909469
2017-12-10T15:01:22.116392: step 3679, loss 0.311487, acc 0.921875, prec 0.0766212, recall 0.909496
2017-12-10T15:01:22.310983: step 3680, loss 0.122549, acc 0.953125, prec 0.0766154, recall 0.909496
2017-12-10T15:01:22.515073: step 3681, loss 0.366717, acc 0.890625, prec 0.0766251, recall 0.909522
2017-12-10T15:01:22.712400: step 3682, loss 0.156042, acc 0.921875, prec 0.0766155, recall 0.909522
2017-12-10T15:01:22.908119: step 3683, loss 0.22079, acc 0.9375, prec 0.0766771, recall 0.909603
2017-12-10T15:01:23.103402: step 3684, loss 0.290599, acc 0.953125, prec 0.0767405, recall 0.909683
2017-12-10T15:01:23.296358: step 3685, loss 0.28275, acc 0.890625, prec 0.0767271, recall 0.909683
2017-12-10T15:01:23.489928: step 3686, loss 0.222802, acc 0.953125, prec 0.0767444, recall 0.90971
2017-12-10T15:01:23.682890: step 3687, loss 0.325036, acc 0.875, prec 0.0767752, recall 0.909763
2017-12-10T15:01:23.881268: step 3688, loss 0.135946, acc 0.953125, prec 0.0768155, recall 0.909817
2017-12-10T15:01:24.077406: step 3689, loss 0.250127, acc 0.921875, prec 0.0768059, recall 0.909817
2017-12-10T15:01:24.273624: step 3690, loss 0.322044, acc 0.96875, prec 0.0768252, recall 0.909843
2017-12-10T15:01:24.468517: step 3691, loss 0.137159, acc 0.96875, prec 0.0768213, recall 0.909843
2017-12-10T15:01:24.662604: step 3692, loss 0.0965774, acc 0.984375, prec 0.0768424, recall 0.90987
2017-12-10T15:01:24.855761: step 3693, loss 0.012744, acc 1, prec 0.0768424, recall 0.90987
2017-12-10T15:01:25.056465: step 3694, loss 0.0657384, acc 0.96875, prec 0.0768616, recall 0.909897
2017-12-10T15:01:25.248959: step 3695, loss 0.0744672, acc 0.984375, prec 0.0769058, recall 0.90995
2017-12-10T15:01:25.444812: step 3696, loss 0.24703, acc 0.96875, prec 0.076902, recall 0.90995
2017-12-10T15:01:25.640660: step 3697, loss 0.0226327, acc 0.984375, prec 0.0769, recall 0.90995
2017-12-10T15:01:25.840583: step 3698, loss 0.0466038, acc 1, prec 0.0769461, recall 0.910003
2017-12-10T15:01:26.038175: step 3699, loss 0.190383, acc 0.984375, prec 0.0769672, recall 0.910029
2017-12-10T15:01:26.237450: step 3700, loss 0.471685, acc 0.96875, prec 0.0770094, recall 0.910083
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_0/1512935327/checkpoints/model-3700

2017-12-10T15:01:27.378968: step 3701, loss 0.128823, acc 0.984375, prec 0.0770305, recall 0.910109
2017-12-10T15:01:27.574608: step 3702, loss 0.114843, acc 0.953125, prec 0.0770248, recall 0.910109
2017-12-10T15:01:27.768645: step 3703, loss 0.08999, acc 0.984375, prec 0.0770459, recall 0.910136
2017-12-10T15:01:27.967566: step 3704, loss 0.214841, acc 0.96875, prec 0.0770881, recall 0.910188
2017-12-10T15:01:28.159988: step 3705, loss 0.307193, acc 0.9375, prec 0.0771264, recall 0.910241
2017-12-10T15:01:28.358894: step 3706, loss 0.0469931, acc 0.984375, prec 0.0771245, recall 0.910241
2017-12-10T15:01:28.553311: step 3707, loss 0.153752, acc 0.9375, prec 0.0771398, recall 0.910268
2017-12-10T15:01:28.747101: step 3708, loss 0.14042, acc 0.953125, prec 0.077157, recall 0.910294
2017-12-10T15:01:28.938638: step 3709, loss 0.00590214, acc 1, prec 0.077157, recall 0.910294
2017-12-10T15:01:29.136540: step 3710, loss 0.0738254, acc 0.9375, prec 0.0771493, recall 0.910294
2017-12-10T15:01:29.331469: step 3711, loss 0.0689512, acc 0.984375, prec 0.0771474, recall 0.910294
2017-12-10T15:01:29.531058: step 3712, loss 0.077732, acc 0.984375, prec 0.0771685, recall 0.910321
2017-12-10T15:01:29.726930: step 3713, loss 8.81447, acc 0.953125, prec 0.0772336, recall 0.910132
2017-12-10T15:01:29.925570: step 3714, loss 0.118206, acc 0.96875, prec 0.0772528, recall 0.910159
2017-12-10T15:01:30.115743: step 3715, loss 0.0766751, acc 0.984375, prec 0.0772509, recall 0.910159
2017-12-10T15:01:30.309311: step 3716, loss 0.459826, acc 0.90625, prec 0.0772393, recall 0.910159
2017-12-10T15:01:30.505762: step 3717, loss 0.419612, acc 0.921875, prec 0.0772297, recall 0.910159
2017-12-10T15:01:30.697483: step 3718, loss 0.106434, acc 0.953125, prec 0.0772469, recall 0.910185
2017-12-10T15:01:30.893182: step 3719, loss 0.169514, acc 0.96875, prec 0.077266, recall 0.910211
2017-12-10T15:01:31.096079: step 3720, loss 0.657959, acc 0.84375, prec 0.0772468, recall 0.910211
2017-12-10T15:01:31.291550: step 3721, loss 0.191173, acc 0.953125, prec 0.077287, recall 0.910264
2017-12-10T15:01:31.492715: step 3722, loss 0.259791, acc 0.921875, prec 0.0773233, recall 0.910317
2017-12-10T15:01:31.685445: step 3723, loss 0.490213, acc 0.8125, prec 0.0773461, recall 0.910369
2017-12-10T15:01:31.880602: step 3724, loss 0.19597, acc 0.921875, prec 0.0773365, recall 0.910369
2017-12-10T15:01:32.074877: step 3725, loss 0.391328, acc 0.890625, prec 0.0773919, recall 0.910448
2017-12-10T15:01:32.268102: step 3726, loss 0.257969, acc 0.9375, prec 0.0773842, recall 0.910448
2017-12-10T15:01:32.460152: step 3727, loss 0.209186, acc 0.953125, prec 0.0773784, recall 0.910448
2017-12-10T15:01:32.653059: step 3728, loss 0.348449, acc 0.90625, prec 0.0773898, recall 0.910474
2017-12-10T15:01:32.851257: step 3729, loss 0.17129, acc 0.90625, prec 0.0774012, recall 0.9105
2017-12-10T15:01:33.049212: step 3730, loss 0.304468, acc 0.921875, prec 0.0774604, recall 0.910579
2017-12-10T15:01:33.246924: step 3731, loss 0.125234, acc 0.90625, prec 0.0774947, recall 0.910631
2017-12-10T15:01:33.440696: step 3732, loss 0.136783, acc 0.90625, prec 0.0774832, recall 0.910631
2017-12-10T15:01:33.631412: step 3733, loss 0.318028, acc 0.953125, prec 0.0774774, recall 0.910631
2017-12-10T15:01:33.822419: step 3734, loss 0.323441, acc 0.90625, prec 0.0774888, recall 0.910657
2017-12-10T15:01:34.016810: step 3735, loss 0.157722, acc 0.953125, prec 0.0775288, recall 0.910709
2017-12-10T15:01:34.209098: step 3736, loss 0.359198, acc 0.875, prec 0.0775134, recall 0.910709
2017-12-10T15:01:34.402093: step 3737, loss 0.384414, acc 0.90625, prec 0.0775477, recall 0.910761
2017-12-10T15:01:34.601262: step 3738, loss 0.0546681, acc 0.984375, prec 0.0775457, recall 0.910761
2017-12-10T15:01:34.797088: step 3739, loss 0.0172938, acc 1, prec 0.0775457, recall 0.910761
2017-12-10T15:01:34.991809: step 3740, loss 0.0533829, acc 1, prec 0.0775687, recall 0.910787
2017-12-10T15:01:35.189581: step 3741, loss 0.0433858, acc 1, prec 0.0775916, recall 0.910813
2017-12-10T15:01:35.385441: step 3742, loss 2.4628, acc 0.984375, prec 0.0776145, recall 0.910574
2017-12-10T15:01:35.581607: step 3743, loss 0.184961, acc 0.96875, prec 0.0776335, recall 0.9106
2017-12-10T15:01:35.774333: step 3744, loss 7.42911, acc 0.9375, prec 0.0776277, recall 0.910335
2017-12-10T15:01:35.971949: step 3745, loss 0.0642714, acc 0.96875, prec 0.0776239, recall 0.910335
2017-12-10T15:01:36.164671: step 3746, loss 0.165442, acc 0.9375, prec 0.0776391, recall 0.910361
2017-12-10T15:01:36.361155: step 3747, loss 0.276661, acc 0.90625, prec 0.0776275, recall 0.910361
2017-12-10T15:01:36.555931: step 3748, loss 0.4235, acc 0.921875, prec 0.0776179, recall 0.910361
2017-12-10T15:01:36.751031: step 3749, loss 0.579064, acc 0.84375, prec 0.0775986, recall 0.910361
2017-12-10T15:01:36.941465: step 3750, loss 0.600946, acc 0.828125, prec 0.0775774, recall 0.910361
2017-12-10T15:01:37.133324: step 3751, loss 0.457861, acc 0.890625, prec 0.0775868, recall 0.910387
2017-12-10T15:01:37.329101: step 3752, loss 0.69654, acc 0.75, prec 0.0775561, recall 0.910387
2017-12-10T15:01:37.521349: step 3753, loss 0.868877, acc 0.75, prec 0.077571, recall 0.910439
2017-12-10T15:01:37.719117: step 3754, loss 0.554271, acc 0.796875, prec 0.0775461, recall 0.910439
2017-12-10T15:01:37.910611: step 3755, loss 0.324365, acc 0.875, prec 0.0775307, recall 0.910439
2017-12-10T15:01:38.106142: step 3756, loss 0.377504, acc 0.828125, prec 0.0775553, recall 0.910491
2017-12-10T15:01:38.302996: step 3757, loss 0.289164, acc 0.890625, prec 0.0775647, recall 0.910517
2017-12-10T15:01:38.496002: step 3758, loss 0.557167, acc 0.8125, prec 0.0775416, recall 0.910517
2017-12-10T15:01:38.686542: step 3759, loss 0.495013, acc 0.828125, prec 0.0775205, recall 0.910517
2017-12-10T15:01:38.880362: step 3760, loss 0.323669, acc 0.84375, prec 0.0775014, recall 0.910517
2017-12-10T15:01:39.077637: step 3761, loss 0.344572, acc 0.890625, prec 0.0775336, recall 0.910569
2017-12-10T15:01:39.273729: step 3762, loss 0.318995, acc 0.921875, prec 0.0775468, recall 0.910595
2017-12-10T15:01:39.468517: step 3763, loss 0.473926, acc 0.875, prec 0.0775542, recall 0.910621
2017-12-10T15:01:39.658688: step 3764, loss 0.186779, acc 0.953125, prec 0.0775485, recall 0.910621
2017-12-10T15:01:39.856244: step 3765, loss 0.123153, acc 0.953125, prec 0.0775883, recall 0.910673
2017-12-10T15:01:40.046878: step 3766, loss 0.0920472, acc 0.953125, prec 0.0775826, recall 0.910673
2017-12-10T15:01:40.244940: step 3767, loss 0.0821727, acc 0.96875, prec 0.0775788, recall 0.910673
2017-12-10T15:01:40.442981: step 3768, loss 0.279955, acc 0.96875, prec 0.0775977, recall 0.910699
2017-12-10T15:01:40.642283: step 3769, loss 0.487803, acc 0.96875, prec 0.0776167, recall 0.910725
2017-12-10T15:01:40.840205: step 3770, loss 0.0518211, acc 1, prec 0.0776394, recall 0.910751
2017-12-10T15:01:41.033344: step 3771, loss 0.0703759, acc 0.984375, prec 0.0776375, recall 0.910751
2017-12-10T15:01:41.227400: step 3772, loss 0.0524658, acc 0.984375, prec 0.0776356, recall 0.910751
2017-12-10T15:01:41.422555: step 3773, loss 0.0657321, acc 0.96875, prec 0.0776318, recall 0.910751
2017-12-10T15:01:41.620992: step 3774, loss 0.945584, acc 0.984375, prec 0.077721, recall 0.910854
2017-12-10T15:01:41.820851: step 3775, loss 0.139855, acc 0.96875, prec 0.0777171, recall 0.910854
2017-12-10T15:01:42.019368: step 3776, loss 0.147814, acc 0.984375, prec 0.077738, recall 0.91088
2017-12-10T15:01:42.209962: step 3777, loss 0.128741, acc 0.953125, prec 0.0777322, recall 0.91088
2017-12-10T15:01:42.404260: step 3778, loss 0.148306, acc 0.96875, prec 0.0777967, recall 0.910957
2017-12-10T15:01:42.594668: step 3779, loss 0.0947943, acc 0.953125, prec 0.0778137, recall 0.910983
2017-12-10T15:01:42.789347: step 3780, loss 0.149164, acc 0.984375, prec 0.0778346, recall 0.911008
2017-12-10T15:01:42.984116: step 3781, loss 0.480229, acc 0.96875, prec 0.077899, recall 0.911085
2017-12-10T15:01:43.186460: step 3782, loss 0.0897027, acc 0.96875, prec 0.0778951, recall 0.911085
2017-12-10T15:01:43.379141: step 3783, loss 0.102955, acc 0.96875, prec 0.0779141, recall 0.911111
2017-12-10T15:01:43.571798: step 3784, loss 0.502441, acc 0.96875, prec 0.077933, recall 0.911137
2017-12-10T15:01:43.769882: step 3785, loss 0.125286, acc 0.953125, prec 0.07795, recall 0.911162
2017-12-10T15:01:43.964366: step 3786, loss 0.316315, acc 0.953125, prec 0.0780124, recall 0.911239
2017-12-10T15:01:44.165136: step 3787, loss 0.193255, acc 0.9375, prec 0.0780502, recall 0.91129
2017-12-10T15:01:44.358670: step 3788, loss 0.158266, acc 0.9375, prec 0.0780425, recall 0.91129
2017-12-10T15:01:44.550293: step 3789, loss 0.237631, acc 0.921875, prec 0.0780556, recall 0.911316
2017-12-10T15:01:44.748051: step 3790, loss 0.0370303, acc 1, prec 0.0781011, recall 0.911367
2017-12-10T15:01:44.942628: step 3791, loss 0.223523, acc 0.921875, prec 0.0781142, recall 0.911392
2017-12-10T15:01:45.134991: step 3792, loss 0.224241, acc 0.9375, prec 0.0781292, recall 0.911418
2017-12-10T15:01:45.325646: step 3793, loss 0.203685, acc 0.9375, prec 0.078167, recall 0.911469
2017-12-10T15:01:45.518918: step 3794, loss 0.389916, acc 0.90625, prec 0.0781554, recall 0.911469
2017-12-10T15:01:45.711876: step 3795, loss 0.219865, acc 0.953125, prec 0.0781496, recall 0.911469
2017-12-10T15:01:45.909168: step 3796, loss 0.0561036, acc 0.984375, prec 0.0781477, recall 0.911469
2017-12-10T15:01:46.106914: step 3797, loss 0.128038, acc 0.984375, prec 0.0781458, recall 0.911469
2017-12-10T15:01:46.300052: step 3798, loss 0.0576624, acc 0.96875, prec 0.0781419, recall 0.911469
2017-12-10T15:01:46.494010: step 3799, loss 0.0762665, acc 0.984375, prec 0.07814, recall 0.911469
2017-12-10T15:01:46.690843: step 3800, loss 0.456152, acc 0.90625, prec 0.0781966, recall 0.911545
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_0/1512935327/checkpoints/model-3800

2017-12-10T15:01:47.979983: step 3801, loss 0.0836166, acc 0.96875, prec 0.0781927, recall 0.911545
2017-12-10T15:01:48.174749: step 3802, loss 0.0475374, acc 0.96875, prec 0.0782116, recall 0.91157
2017-12-10T15:01:48.368377: step 3803, loss 0.0671921, acc 0.96875, prec 0.0782077, recall 0.91157
2017-12-10T15:01:48.563598: step 3804, loss 0.388567, acc 0.953125, prec 0.0782247, recall 0.911596
2017-12-10T15:01:48.761595: step 3805, loss 2.59005, acc 0.953125, prec 0.0782208, recall 0.911334
2017-12-10T15:01:48.956295: step 3806, loss 1.29001, acc 0.921875, prec 0.0782131, recall 0.911073
2017-12-10T15:01:49.153270: step 3807, loss 0.0861421, acc 0.984375, prec 0.0782566, recall 0.911124
2017-12-10T15:01:49.349084: step 3808, loss 0.38324, acc 0.90625, prec 0.0782677, recall 0.911149
2017-12-10T15:01:49.546244: step 3809, loss 0.22389, acc 0.921875, prec 0.0782808, recall 0.911175
2017-12-10T15:01:49.741919: step 3810, loss 0.511774, acc 0.84375, prec 0.0782842, recall 0.9112
2017-12-10T15:01:49.938526: step 3811, loss 0.301917, acc 0.859375, prec 0.0782669, recall 0.9112
2017-12-10T15:01:50.130650: step 3812, loss 0.597324, acc 0.84375, prec 0.0782703, recall 0.911226
2017-12-10T15:01:50.325499: step 3813, loss 0.230734, acc 0.859375, prec 0.0782983, recall 0.911276
2017-12-10T15:01:50.519154: step 3814, loss 0.279617, acc 0.90625, prec 0.0782867, recall 0.911276
2017-12-10T15:01:50.715287: step 3815, loss 0.407682, acc 0.84375, prec 0.0782675, recall 0.911276
2017-12-10T15:01:50.908030: step 3816, loss 0.322448, acc 0.921875, prec 0.0783032, recall 0.911327
2017-12-10T15:01:51.106826: step 3817, loss 0.424446, acc 0.859375, prec 0.0782859, recall 0.911327
2017-12-10T15:01:51.299558: step 3818, loss 0.673071, acc 0.78125, prec 0.0782589, recall 0.911327
2017-12-10T15:01:51.495910: step 3819, loss 0.658234, acc 0.890625, prec 0.0782681, recall 0.911353
2017-12-10T15:01:51.689170: step 3820, loss 0.33286, acc 0.921875, prec 0.0783038, recall 0.911403
2017-12-10T15:01:51.882722: step 3821, loss 1.35685, acc 0.84375, prec 0.0783072, recall 0.911429
2017-12-10T15:01:52.077958: step 3822, loss 0.513033, acc 0.9375, prec 0.0783447, recall 0.911479
2017-12-10T15:01:52.267650: step 3823, loss 0.190887, acc 0.9375, prec 0.0784049, recall 0.911555
2017-12-10T15:01:52.463180: step 3824, loss 0.202134, acc 0.9375, prec 0.0783972, recall 0.911555
2017-12-10T15:01:52.659548: step 3825, loss 0.268552, acc 0.890625, prec 0.0784064, recall 0.91158
2017-12-10T15:01:52.855479: step 3826, loss 0.159225, acc 0.90625, prec 0.0783948, recall 0.91158
2017-12-10T15:01:53.051932: step 3827, loss 0.15112, acc 0.9375, prec 0.0783871, recall 0.91158
2017-12-10T15:01:53.246709: step 3828, loss 0.240911, acc 0.921875, prec 0.0784227, recall 0.911631
2017-12-10T15:01:53.437845: step 3829, loss 0.14416, acc 0.96875, prec 0.0784415, recall 0.911656
2017-12-10T15:01:53.630729: step 3830, loss 0.0186178, acc 1, prec 0.0784415, recall 0.911656
2017-12-10T15:01:53.822758: step 3831, loss 0.264997, acc 0.90625, prec 0.0784525, recall 0.911681
2017-12-10T15:01:54.016271: step 3832, loss 0.39173, acc 0.984375, prec 0.0784732, recall 0.911706
2017-12-10T15:01:54.217925: step 3833, loss 0.0592735, acc 0.96875, prec 0.0784693, recall 0.911706
2017-12-10T15:01:54.414960: step 3834, loss 0.0895379, acc 0.96875, prec 0.0785333, recall 0.911781
2017-12-10T15:01:54.618377: step 3835, loss 0.0943365, acc 0.984375, prec 0.0785539, recall 0.911807
2017-12-10T15:01:54.817863: step 3836, loss 0.0540483, acc 0.96875, prec 0.0785501, recall 0.911807
2017-12-10T15:01:55.013248: step 3837, loss 0.0526689, acc 0.984375, prec 0.0785481, recall 0.911807
2017-12-10T15:01:55.208136: step 3838, loss 0.0924249, acc 0.96875, prec 0.0785443, recall 0.911807
2017-12-10T15:01:55.410530: step 3839, loss 0.247427, acc 1, prec 0.0785669, recall 0.911832
2017-12-10T15:01:55.604823: step 3840, loss 0.0744359, acc 0.984375, prec 0.0786101, recall 0.911882
2017-12-10T15:01:55.800858: step 3841, loss 0.0352103, acc 0.984375, prec 0.0786082, recall 0.911882
2017-12-10T15:01:55.996344: step 3842, loss 0.106418, acc 0.984375, prec 0.0786288, recall 0.911907
2017-12-10T15:01:56.195102: step 3843, loss 0.285304, acc 0.9375, prec 0.0786437, recall 0.911932
2017-12-10T15:01:56.387025: step 3844, loss 0.215913, acc 0.96875, prec 0.0786624, recall 0.911957
2017-12-10T15:01:56.585233: step 3845, loss 0.119125, acc 0.984375, prec 0.0786605, recall 0.911957
2017-12-10T15:01:56.779052: step 3846, loss 0.131219, acc 0.9375, prec 0.0786528, recall 0.911957
2017-12-10T15:01:56.973351: step 3847, loss 5.80118, acc 0.953125, prec 0.0786715, recall 0.911723
2017-12-10T15:01:57.175517: step 3848, loss 0.0537276, acc 0.984375, prec 0.0786696, recall 0.911723
2017-12-10T15:01:57.372261: step 3849, loss 0.0751248, acc 0.96875, prec 0.0786657, recall 0.911723
2017-12-10T15:01:57.567479: step 3850, loss 0.0305983, acc 1, prec 0.0786657, recall 0.911723
2017-12-10T15:01:57.757616: step 3851, loss 0.670446, acc 0.875, prec 0.0786503, recall 0.911723
2017-12-10T15:01:57.950582: step 3852, loss 0.245807, acc 0.984375, prec 0.0787161, recall 0.911798
2017-12-10T15:01:58.145678: step 3853, loss 0.201487, acc 0.9375, prec 0.0787084, recall 0.911798
2017-12-10T15:01:58.340919: step 3854, loss 0.184394, acc 0.90625, prec 0.0786968, recall 0.911798
2017-12-10T15:01:58.533853: step 3855, loss 0.202863, acc 0.953125, prec 0.0787136, recall 0.911823
2017-12-10T15:01:58.732852: step 3856, loss 0.461927, acc 0.859375, prec 0.0787639, recall 0.911898
2017-12-10T15:01:58.924895: step 3857, loss 0.693287, acc 0.9375, prec 0.0788012, recall 0.911948
2017-12-10T15:01:59.124747: step 3858, loss 0.279847, acc 0.921875, prec 0.0787916, recall 0.911948
2017-12-10T15:01:59.322377: step 3859, loss 0.417737, acc 0.9375, prec 0.0788289, recall 0.911998
2017-12-10T15:01:59.517558: step 3860, loss 0.268406, acc 0.90625, prec 0.0788174, recall 0.911998
2017-12-10T15:01:59.711051: step 3861, loss 0.175595, acc 0.953125, prec 0.0788116, recall 0.911998
2017-12-10T15:01:59.903227: step 3862, loss 0.400913, acc 0.9375, prec 0.0788264, recall 0.912023
2017-12-10T15:02:00.096330: step 3863, loss 0.240241, acc 0.9375, prec 0.0788637, recall 0.912072
2017-12-10T15:02:00.295945: step 3864, loss 0.443803, acc 0.90625, prec 0.0788522, recall 0.912072
2017-12-10T15:02:00.487046: step 3865, loss 0.157486, acc 0.953125, prec 0.0788689, recall 0.912097
2017-12-10T15:02:00.683854: step 3866, loss 0.15656, acc 0.9375, prec 0.0788612, recall 0.912097
2017-12-10T15:02:00.878095: step 3867, loss 0.271677, acc 0.9375, prec 0.078876, recall 0.912122
2017-12-10T15:02:01.073599: step 3868, loss 0.232247, acc 0.90625, prec 0.0788644, recall 0.912122
2017-12-10T15:02:01.274528: step 3869, loss 0.128485, acc 0.953125, prec 0.0789262, recall 0.912197
2017-12-10T15:02:01.472810: step 3870, loss 0.203517, acc 0.90625, prec 0.0789371, recall 0.912221
2017-12-10T15:02:01.664981: step 3871, loss 0.798632, acc 0.921875, prec 0.0789499, recall 0.912246
2017-12-10T15:02:01.860974: step 3872, loss 0.315235, acc 0.90625, prec 0.0789384, recall 0.912246
2017-12-10T15:02:02.056485: step 3873, loss 0.0828387, acc 0.953125, prec 0.0789776, recall 0.912296
2017-12-10T15:02:02.248216: step 3874, loss 0.152445, acc 0.953125, prec 0.0790167, recall 0.912345
2017-12-10T15:02:02.444858: step 3875, loss 0.162219, acc 0.953125, prec 0.079011, recall 0.912345
2017-12-10T15:02:02.637419: step 3876, loss 0.0489749, acc 0.96875, prec 0.0790071, recall 0.912345
2017-12-10T15:02:02.830316: step 3877, loss 0.504811, acc 0.890625, prec 0.0790386, recall 0.912394
2017-12-10T15:02:03.028009: step 3878, loss 0.211664, acc 0.9375, prec 0.0790533, recall 0.912419
2017-12-10T15:02:03.225734: step 3879, loss 0.140154, acc 0.953125, prec 0.07907, recall 0.912444
2017-12-10T15:02:03.418124: step 3880, loss 0.193556, acc 0.953125, prec 0.0790642, recall 0.912444
2017-12-10T15:02:03.611801: step 3881, loss 0.360509, acc 0.859375, prec 0.0790469, recall 0.912444
2017-12-10T15:02:03.805296: step 3882, loss 0.787603, acc 0.9375, prec 0.0790841, recall 0.912493
2017-12-10T15:02:04.002645: step 3883, loss 0.341984, acc 0.90625, prec 0.0790725, recall 0.912493
2017-12-10T15:02:04.199567: step 3884, loss 0.213484, acc 0.9375, prec 0.0790648, recall 0.912493
2017-12-10T15:02:04.397891: step 3885, loss 0.164757, acc 0.921875, prec 0.0791, recall 0.912542
2017-12-10T15:02:04.596545: step 3886, loss 0.0966485, acc 0.96875, prec 0.0790962, recall 0.912542
2017-12-10T15:02:04.789686: step 3887, loss 0.268997, acc 0.921875, prec 0.079109, recall 0.912567
2017-12-10T15:02:04.983210: step 3888, loss 0.0521385, acc 0.984375, prec 0.0791071, recall 0.912567
2017-12-10T15:02:05.178991: step 3889, loss 0.0800201, acc 0.96875, prec 0.0791032, recall 0.912567
2017-12-10T15:02:05.373271: step 3890, loss 0.198978, acc 0.96875, prec 0.0791218, recall 0.912591
2017-12-10T15:02:05.569381: step 3891, loss 0.0853016, acc 0.953125, prec 0.0791384, recall 0.912616
2017-12-10T15:02:05.763466: step 3892, loss 0.145385, acc 0.9375, prec 0.0791532, recall 0.91264
2017-12-10T15:02:05.957132: step 3893, loss 0.201507, acc 0.921875, prec 0.079166, recall 0.912665
2017-12-10T15:02:06.149469: step 3894, loss 0.157074, acc 0.984375, prec 0.0791865, recall 0.91269
2017-12-10T15:02:06.343393: step 3895, loss 0.184821, acc 0.953125, prec 0.0792031, recall 0.912714
2017-12-10T15:02:06.540682: step 3896, loss 0.0626121, acc 0.96875, prec 0.0791992, recall 0.912714
2017-12-10T15:02:06.736191: step 3897, loss 0.067814, acc 0.984375, prec 0.0792197, recall 0.912739
2017-12-10T15:02:06.931674: step 3898, loss 0.17108, acc 0.9375, prec 0.079212, recall 0.912739
2017-12-10T15:02:07.128101: step 3899, loss 0.00836292, acc 1, prec 0.079212, recall 0.912739
2017-12-10T15:02:07.319962: step 3900, loss 0.0945685, acc 0.984375, prec 0.0792101, recall 0.912739
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_0/1512935327/checkpoints/model-3900

2017-12-10T15:02:08.507464: step 3901, loss 0.566662, acc 0.953125, prec 0.0792491, recall 0.912787
2017-12-10T15:02:08.702619: step 3902, loss 0.1376, acc 1, prec 0.0792716, recall 0.912812
2017-12-10T15:02:08.899331: step 3903, loss 0.538796, acc 0.96875, prec 0.0792901, recall 0.912836
2017-12-10T15:02:09.095146: step 3904, loss 0.126303, acc 0.96875, prec 0.0792863, recall 0.912836
2017-12-10T15:02:09.288390: step 3905, loss 0.147388, acc 0.96875, prec 0.0793048, recall 0.912861
2017-12-10T15:02:09.485006: step 3906, loss 1.43945, acc 0.953125, prec 0.0793458, recall 0.912654
2017-12-10T15:02:09.682013: step 3907, loss 0.345941, acc 0.9375, prec 0.0793604, recall 0.912678
2017-12-10T15:02:09.874299: step 3908, loss 0.0953059, acc 0.953125, prec 0.0793547, recall 0.912678
2017-12-10T15:02:10.065654: step 3909, loss 0.0700289, acc 0.96875, prec 0.0793732, recall 0.912703
2017-12-10T15:02:10.260238: step 3910, loss 0.398061, acc 0.984375, prec 0.0793937, recall 0.912727
2017-12-10T15:02:10.456947: step 3911, loss 0.0321624, acc 0.984375, prec 0.0793917, recall 0.912727
2017-12-10T15:02:10.652573: step 3912, loss 0.0927043, acc 0.96875, prec 0.0794551, recall 0.9128
2017-12-10T15:02:10.845055: step 3913, loss 0.18038, acc 0.953125, prec 0.079494, recall 0.912849
2017-12-10T15:02:11.034251: step 3914, loss 0.236067, acc 0.90625, prec 0.0794824, recall 0.912849
2017-12-10T15:02:11.233266: step 3915, loss 0.344975, acc 0.890625, prec 0.0794689, recall 0.912849
2017-12-10T15:02:11.422761: step 3916, loss 0.408189, acc 0.90625, prec 0.0794573, recall 0.912849
2017-12-10T15:02:11.615942: step 3917, loss 0.230104, acc 0.9375, prec 0.0795167, recall 0.912922
2017-12-10T15:02:11.807969: step 3918, loss 0.475323, acc 0.828125, prec 0.0794955, recall 0.912922
2017-12-10T15:02:11.998200: step 3919, loss 0.33049, acc 0.890625, prec 0.0795043, recall 0.912946
2017-12-10T15:02:12.196664: step 3920, loss 0.663097, acc 0.890625, prec 0.0794908, recall 0.912946
2017-12-10T15:02:12.392229: step 3921, loss 0.253841, acc 0.84375, prec 0.0794715, recall 0.912946
2017-12-10T15:02:12.581450: step 3922, loss 0.298233, acc 0.921875, prec 0.0794842, recall 0.912971
2017-12-10T15:02:12.771042: step 3923, loss 0.272615, acc 0.890625, prec 0.0794707, recall 0.912971
2017-12-10T15:02:12.972684: step 3924, loss 0.322705, acc 0.875, prec 0.0794776, recall 0.912995
2017-12-10T15:02:13.170208: step 3925, loss 0.176599, acc 0.921875, prec 0.0795126, recall 0.913043
2017-12-10T15:02:13.370519: step 3926, loss 0.130418, acc 0.953125, prec 0.0795515, recall 0.913092
2017-12-10T15:02:13.568340: step 3927, loss 0.175103, acc 0.953125, prec 0.0795681, recall 0.913116
2017-12-10T15:02:13.762902: step 3928, loss 0.0452788, acc 0.984375, prec 0.0795885, recall 0.91314
2017-12-10T15:02:13.955984: step 3929, loss 0.247878, acc 0.921875, prec 0.0796011, recall 0.913164
2017-12-10T15:02:14.156880: step 3930, loss 0.167452, acc 0.953125, prec 0.07964, recall 0.913213
2017-12-10T15:02:14.351719: step 3931, loss 1.54075, acc 0.9375, prec 0.0796565, recall 0.912983
2017-12-10T15:02:14.547609: step 3932, loss 0.045454, acc 1, prec 0.0796789, recall 0.913007
2017-12-10T15:02:14.744275: step 3933, loss 0.163001, acc 0.921875, prec 0.0796692, recall 0.913007
2017-12-10T15:02:14.936517: step 3934, loss 0.122929, acc 0.9375, prec 0.0796615, recall 0.913007
2017-12-10T15:02:15.131869: step 3935, loss 1.39242, acc 0.859375, prec 0.079711, recall 0.91308
2017-12-10T15:02:15.330557: step 3936, loss 0.365643, acc 0.890625, prec 0.0796975, recall 0.91308
2017-12-10T15:02:15.523215: step 3937, loss 0.182481, acc 0.921875, prec 0.0797101, recall 0.913104
2017-12-10T15:02:15.718451: step 3938, loss 0.144088, acc 0.9375, prec 0.0797247, recall 0.913128
2017-12-10T15:02:15.908545: step 3939, loss 0.320006, acc 0.875, prec 0.0797093, recall 0.913128
2017-12-10T15:02:16.104472: step 3940, loss 0.251778, acc 0.921875, prec 0.0797219, recall 0.913152
2017-12-10T15:02:16.300574: step 3941, loss 0.292704, acc 0.921875, prec 0.0797345, recall 0.913176
2017-12-10T15:02:16.490996: step 3942, loss 0.362355, acc 0.875, prec 0.0797414, recall 0.9132
2017-12-10T15:02:16.682105: step 3943, loss 0.325959, acc 0.859375, prec 0.079724, recall 0.9132
2017-12-10T15:02:16.875419: step 3944, loss 0.533788, acc 0.890625, prec 0.0797328, recall 0.913224
2017-12-10T15:02:17.069504: step 3945, loss 0.259394, acc 0.921875, prec 0.0797899, recall 0.913296
2017-12-10T15:02:17.260696: step 3946, loss 0.29801, acc 0.921875, prec 0.0798248, recall 0.913344
2017-12-10T15:02:17.453493: step 3947, loss 0.175028, acc 0.9375, prec 0.0798394, recall 0.913368
2017-12-10T15:02:17.647867: step 3948, loss 0.141853, acc 0.9375, prec 0.0798539, recall 0.913392
2017-12-10T15:02:17.843403: step 3949, loss 0.342033, acc 0.875, prec 0.0798384, recall 0.913392
2017-12-10T15:02:18.036260: step 3950, loss 0.136178, acc 0.921875, prec 0.079851, recall 0.913416
2017-12-10T15:02:18.232649: step 3951, loss 0.279154, acc 0.921875, prec 0.0798636, recall 0.91344
2017-12-10T15:02:18.427237: step 3952, loss 0.129013, acc 0.953125, prec 0.0798801, recall 0.913464
2017-12-10T15:02:18.620900: step 3953, loss 0.11192, acc 0.953125, prec 0.0798743, recall 0.913464
2017-12-10T15:02:18.816134: step 3954, loss 0.0347267, acc 0.984375, prec 0.0798724, recall 0.913464
2017-12-10T15:02:19.014130: step 3955, loss 0.0156139, acc 1, prec 0.0798724, recall 0.913464
2017-12-10T15:02:19.209733: step 3956, loss 0.0599346, acc 0.984375, prec 0.0799149, recall 0.913512
2017-12-10T15:02:19.403971: step 3957, loss 0.105765, acc 0.96875, prec 0.0799333, recall 0.913536
2017-12-10T15:02:19.597955: step 3958, loss 0.113162, acc 0.984375, prec 0.0799536, recall 0.91356
2017-12-10T15:02:19.796461: step 3959, loss 0.217275, acc 0.984375, prec 0.0799961, recall 0.913607
2017-12-10T15:02:19.991176: step 3960, loss 0.101611, acc 0.96875, prec 0.0800145, recall 0.913631
2017-12-10T15:02:20.190813: step 3961, loss 0.192724, acc 0.96875, prec 0.0800329, recall 0.913655
2017-12-10T15:02:20.386036: step 3962, loss 0.0100215, acc 1, prec 0.0800329, recall 0.913655
2017-12-10T15:02:20.577920: step 3963, loss 0.502565, acc 0.984375, prec 0.0800754, recall 0.913703
2017-12-10T15:02:20.775702: step 3964, loss 0.0307025, acc 0.984375, prec 0.0800735, recall 0.913703
2017-12-10T15:02:20.971569: step 3965, loss 0.321235, acc 0.90625, prec 0.0800841, recall 0.913727
2017-12-10T15:02:21.163942: step 3966, loss 0.127494, acc 0.984375, prec 0.0800821, recall 0.913727
2017-12-10T15:02:21.357803: step 3967, loss 0.0183411, acc 1, prec 0.0801044, recall 0.91375
2017-12-10T15:02:21.550971: step 3968, loss 0.150979, acc 0.96875, prec 0.0801005, recall 0.91375
2017-12-10T15:02:21.742783: step 3969, loss 0.282529, acc 0.953125, prec 0.0800947, recall 0.91375
2017-12-10T15:02:21.934835: step 3970, loss 0.0378795, acc 0.984375, prec 0.0800928, recall 0.91375
2017-12-10T15:02:22.126791: step 3971, loss 0.506781, acc 0.984375, prec 0.080113, recall 0.913774
2017-12-10T15:02:22.323588: step 3972, loss 0.121268, acc 0.96875, prec 0.0801092, recall 0.913774
2017-12-10T15:02:22.520009: step 3973, loss 0.0697535, acc 0.953125, prec 0.0801034, recall 0.913774
2017-12-10T15:02:22.711777: step 3974, loss 0.218849, acc 0.953125, prec 0.080142, recall 0.913822
2017-12-10T15:02:22.904321: step 3975, loss 0.127352, acc 0.96875, prec 0.0801381, recall 0.913822
2017-12-10T15:02:23.080979: step 3976, loss 0.0221761, acc 1, prec 0.0801381, recall 0.913822
2017-12-10T15:02:23.286945: step 3977, loss 0.00941037, acc 1, prec 0.0801381, recall 0.913822
2017-12-10T15:02:23.481636: step 3978, loss 0.0738372, acc 0.96875, prec 0.0801342, recall 0.913822
2017-12-10T15:02:23.675820: step 3979, loss 0.0236316, acc 0.984375, prec 0.0801323, recall 0.913822
2017-12-10T15:02:23.873479: step 3980, loss 0.0380353, acc 0.984375, prec 0.0801304, recall 0.913822
2017-12-10T15:02:24.070020: step 3981, loss 0.0196348, acc 1, prec 0.0801526, recall 0.913845
2017-12-10T15:02:24.263595: step 3982, loss 0.0862869, acc 0.96875, prec 0.0801709, recall 0.913869
2017-12-10T15:02:24.454867: step 3983, loss 0.227691, acc 0.953125, prec 0.0802317, recall 0.91394
2017-12-10T15:02:24.649600: step 3984, loss 0.789198, acc 1, prec 0.0802539, recall 0.913964
2017-12-10T15:02:24.846384: step 3985, loss 0.0403853, acc 0.984375, prec 0.0802742, recall 0.913987
2017-12-10T15:02:25.037158: step 3986, loss 0.029737, acc 1, prec 0.0802964, recall 0.914011
2017-12-10T15:02:25.230086: step 3987, loss 0.0464483, acc 0.984375, prec 0.0802944, recall 0.914011
2017-12-10T15:02:25.426794: step 3988, loss 0.0966986, acc 0.96875, prec 0.0802906, recall 0.914011
2017-12-10T15:02:25.619728: step 3989, loss 0.0436939, acc 0.984375, prec 0.0802886, recall 0.914011
2017-12-10T15:02:25.817846: step 3990, loss 0.0591728, acc 0.984375, prec 0.0802867, recall 0.914011
2017-12-10T15:02:26.011221: step 3991, loss 0.187187, acc 0.984375, prec 0.0803069, recall 0.914035
2017-12-10T15:02:26.205536: step 3992, loss 0.182177, acc 0.953125, prec 0.0803011, recall 0.914035
2017-12-10T15:02:26.398726: step 3993, loss 0.0915003, acc 0.9375, prec 0.0803156, recall 0.914058
2017-12-10T15:02:26.596640: step 3994, loss 0.100396, acc 0.96875, prec 0.0803339, recall 0.914082
2017-12-10T15:02:26.787313: step 3995, loss 0.198773, acc 0.90625, prec 0.0803223, recall 0.914082
2017-12-10T15:02:26.981640: step 3996, loss 0.174248, acc 0.921875, prec 0.0803126, recall 0.914082
2017-12-10T15:02:27.174661: step 3997, loss 0.16299, acc 0.96875, prec 0.0803309, recall 0.914105
2017-12-10T15:02:27.369323: step 3998, loss 0.161897, acc 0.9375, prec 0.0803231, recall 0.914105
2017-12-10T15:02:27.562011: step 3999, loss 0.157819, acc 0.953125, prec 0.0803173, recall 0.914105
2017-12-10T15:02:27.760038: step 4000, loss 0.0981086, acc 0.96875, prec 0.0803578, recall 0.914153

Evaluation:
2017-12-10T15:02:32.394240: step 4000, loss 3.54404, acc 0.954618, prec 0.0811114, recall 0.892651

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_0/1512935327/checkpoints/model-4000

2017-12-10T15:02:33.629590: step 4001, loss 0.103761, acc 0.96875, prec 0.0811075, recall 0.892651
2017-12-10T15:02:33.826131: step 4002, loss 0.0226625, acc 1, prec 0.0811075, recall 0.892651
2017-12-10T15:02:34.029884: step 4003, loss 0.185465, acc 0.953125, prec 0.0811455, recall 0.892707
2017-12-10T15:02:34.222530: step 4004, loss 0.0773079, acc 0.96875, prec 0.0811417, recall 0.892707
2017-12-10T15:02:34.413381: step 4005, loss 0.0598181, acc 0.96875, prec 0.0811378, recall 0.892707
2017-12-10T15:02:34.607460: step 4006, loss 0.133894, acc 0.953125, prec 0.0811539, recall 0.892735
2017-12-10T15:02:34.799859: step 4007, loss 0.126192, acc 0.984375, prec 0.0811958, recall 0.892792
2017-12-10T15:02:34.996354: step 4008, loss 0.72679, acc 0.953125, prec 0.0812338, recall 0.892848
2017-12-10T15:02:35.196776: step 4009, loss 0.0876937, acc 0.984375, prec 0.0812537, recall 0.892876
2017-12-10T15:02:35.387359: step 4010, loss 0.20309, acc 0.953125, prec 0.0812479, recall 0.892876
2017-12-10T15:02:35.584070: step 4011, loss 0.138791, acc 0.9375, prec 0.0812402, recall 0.892876
2017-12-10T15:02:35.782299: step 4012, loss 0.0481342, acc 0.984375, prec 0.0812601, recall 0.892904
2017-12-10T15:02:35.975202: step 4013, loss 0.114844, acc 0.96875, prec 0.0812781, recall 0.892932
2017-12-10T15:02:36.167243: step 4014, loss 0.113676, acc 0.984375, prec 0.0813638, recall 0.893044
2017-12-10T15:02:36.356855: step 4015, loss 0.0429452, acc 0.96875, prec 0.0813599, recall 0.893044
2017-12-10T15:02:36.550491: step 4016, loss 0.110971, acc 0.953125, prec 0.0813541, recall 0.893044
2017-12-10T15:02:36.742716: step 4017, loss 0.223355, acc 0.953125, prec 0.081392, recall 0.8931
2017-12-10T15:02:36.939206: step 4018, loss 0.0569234, acc 0.984375, prec 0.081412, recall 0.893128
2017-12-10T15:02:37.135922: step 4019, loss 0.15029, acc 0.96875, prec 0.0814518, recall 0.893184
2017-12-10T15:02:37.332225: step 4020, loss 0.0612989, acc 0.984375, prec 0.0814718, recall 0.893211
2017-12-10T15:02:37.529196: step 4021, loss 0.179378, acc 0.953125, prec 0.081466, recall 0.893211
2017-12-10T15:02:37.727727: step 4022, loss 0.10797, acc 0.953125, prec 0.081482, recall 0.893239
2017-12-10T15:02:37.926390: step 4023, loss 0.0234609, acc 1, prec 0.081482, recall 0.893239
2017-12-10T15:02:38.118784: step 4024, loss 0.274488, acc 0.953125, prec 0.0814762, recall 0.893239
2017-12-10T15:02:38.313810: step 4025, loss 0.0598602, acc 0.96875, prec 0.0814723, recall 0.893239
2017-12-10T15:02:38.515633: step 4026, loss 0.0496999, acc 0.984375, prec 0.0814704, recall 0.893239
2017-12-10T15:02:38.713126: step 4027, loss 0.070295, acc 0.984375, prec 0.0814684, recall 0.893239
2017-12-10T15:02:38.905691: step 4028, loss 0.0716889, acc 0.96875, prec 0.0815083, recall 0.893295
2017-12-10T15:02:39.097850: step 4029, loss 0.0335948, acc 1, prec 0.081552, recall 0.893351
2017-12-10T15:02:39.293184: step 4030, loss 0.0960042, acc 0.984375, prec 0.0815938, recall 0.893406
2017-12-10T15:02:39.488666: step 4031, loss 0.0747409, acc 0.96875, prec 0.0816118, recall 0.893434
2017-12-10T15:02:39.684276: step 4032, loss 0.244901, acc 0.96875, prec 0.0816297, recall 0.893462
2017-12-10T15:02:39.878943: step 4033, loss 0.0606731, acc 0.984375, prec 0.0816278, recall 0.893462
2017-12-10T15:02:40.071582: step 4034, loss 0.409419, acc 0.953125, prec 0.0816657, recall 0.893517
2017-12-10T15:02:40.266906: step 4035, loss 0.0464071, acc 1, prec 0.0817094, recall 0.893573
2017-12-10T15:02:40.461570: step 4036, loss 0.00896439, acc 1, prec 0.0817094, recall 0.893573
2017-12-10T15:02:40.655247: step 4037, loss 0.0999165, acc 0.953125, prec 0.0817035, recall 0.893573
2017-12-10T15:02:40.848389: step 4038, loss 1.45436, acc 0.984375, prec 0.0817472, recall 0.893396
2017-12-10T15:02:41.048297: step 4039, loss 0.0444738, acc 0.984375, prec 0.0817453, recall 0.893396
2017-12-10T15:02:41.245663: step 4040, loss 0.0292996, acc 1, prec 0.0817453, recall 0.893396
2017-12-10T15:02:41.439004: step 4041, loss 0.110982, acc 0.96875, prec 0.0817414, recall 0.893396
2017-12-10T15:02:41.633532: step 4042, loss 0.0187164, acc 1, prec 0.0817633, recall 0.893423
2017-12-10T15:02:41.826400: step 4043, loss 0.0213161, acc 1, prec 0.0817851, recall 0.893451
2017-12-10T15:02:42.022371: step 4044, loss 0.040572, acc 1, prec 0.0818069, recall 0.893479
2017-12-10T15:02:42.221020: step 4045, loss 0.272583, acc 0.96875, prec 0.0818249, recall 0.893506
2017-12-10T15:02:42.418095: step 4046, loss 0.0723935, acc 0.984375, prec 0.0818666, recall 0.893562
2017-12-10T15:02:42.613133: step 4047, loss 0.0561469, acc 1, prec 0.0818885, recall 0.893589
2017-12-10T15:02:42.806333: step 4048, loss 1.07407, acc 0.9375, prec 0.0819025, recall 0.893617
2017-12-10T15:02:43.000125: step 4049, loss 0.197374, acc 0.96875, prec 0.0818986, recall 0.893617
2017-12-10T15:02:43.192762: step 4050, loss 0.0910196, acc 0.96875, prec 0.0818947, recall 0.893617
2017-12-10T15:02:43.387642: step 4051, loss 0.351566, acc 0.921875, prec 0.0819286, recall 0.893672
2017-12-10T15:02:43.583146: step 4052, loss 0.135533, acc 0.921875, prec 0.0819189, recall 0.893672
2017-12-10T15:02:43.778023: step 4053, loss 0.106592, acc 0.96875, prec 0.0819368, recall 0.8937
2017-12-10T15:02:43.975122: step 4054, loss 0.338073, acc 0.921875, prec 0.0819707, recall 0.893755
2017-12-10T15:02:44.177541: step 4055, loss 0.191511, acc 0.890625, prec 0.0819571, recall 0.893755
2017-12-10T15:02:44.371755: step 4056, loss 0.213883, acc 0.953125, prec 0.0819512, recall 0.893755
2017-12-10T15:02:44.565016: step 4057, loss 0.220331, acc 0.953125, prec 0.0819454, recall 0.893755
2017-12-10T15:02:44.765471: step 4058, loss 0.300851, acc 0.890625, prec 0.0819536, recall 0.893782
2017-12-10T15:02:44.958264: step 4059, loss 0.180543, acc 0.921875, prec 0.0819438, recall 0.893782
2017-12-10T15:02:45.151564: step 4060, loss 0.277851, acc 0.921875, prec 0.0819341, recall 0.893782
2017-12-10T15:02:45.348495: step 4061, loss 0.365644, acc 0.921875, prec 0.0819244, recall 0.893782
2017-12-10T15:02:45.543749: step 4062, loss 0.067406, acc 0.96875, prec 0.0819641, recall 0.893837
2017-12-10T15:02:45.737088: step 4063, loss 0.194203, acc 0.9375, prec 0.0820217, recall 0.89392
2017-12-10T15:02:45.931004: step 4064, loss 0.286976, acc 0.875, prec 0.0820061, recall 0.89392
2017-12-10T15:02:46.124553: step 4065, loss 0.47322, acc 0.921875, prec 0.0820182, recall 0.893947
2017-12-10T15:02:46.315823: step 4066, loss 0.147001, acc 0.96875, prec 0.0820579, recall 0.894002
2017-12-10T15:02:46.510659: step 4067, loss 0.118831, acc 0.953125, prec 0.0820738, recall 0.894029
2017-12-10T15:02:46.704979: step 4068, loss 0.0392288, acc 0.984375, prec 0.0820936, recall 0.894057
2017-12-10T15:02:46.900475: step 4069, loss 0.163437, acc 0.9375, prec 0.0821294, recall 0.894112
2017-12-10T15:02:47.099083: step 4070, loss 0.0170201, acc 1, prec 0.0821512, recall 0.894139
2017-12-10T15:02:47.296702: step 4071, loss 0.0830707, acc 0.984375, prec 0.082171, recall 0.894166
2017-12-10T15:02:47.492047: step 4072, loss 0.0249357, acc 1, prec 0.082171, recall 0.894166
2017-12-10T15:02:47.683138: step 4073, loss 0.20999, acc 0.9375, prec 0.082185, recall 0.894194
2017-12-10T15:02:47.875726: step 4074, loss 2.1859, acc 0.9375, prec 0.0822444, recall 0.894045
2017-12-10T15:02:48.074276: step 4075, loss 0.336176, acc 0.984375, prec 0.0823077, recall 0.894127
2017-12-10T15:02:48.273856: step 4076, loss 0.202358, acc 0.96875, prec 0.0823038, recall 0.894127
2017-12-10T15:02:48.468983: step 4077, loss 0.021209, acc 1, prec 0.0823691, recall 0.894208
2017-12-10T15:02:48.665222: step 4078, loss 0.349848, acc 0.90625, prec 0.0823574, recall 0.894208
2017-12-10T15:02:48.864260: step 4079, loss 0.113252, acc 0.953125, prec 0.0824168, recall 0.89429
2017-12-10T15:02:49.059348: step 4080, loss 0.285838, acc 0.921875, prec 0.0824723, recall 0.894372
2017-12-10T15:02:49.249345: step 4081, loss 0.178889, acc 0.9375, prec 0.0824862, recall 0.894399
2017-12-10T15:02:49.439373: step 4082, loss 0.276295, acc 0.921875, prec 0.0824982, recall 0.894426
2017-12-10T15:02:49.633039: step 4083, loss 0.261922, acc 0.890625, prec 0.0825062, recall 0.894453
2017-12-10T15:02:49.827492: step 4084, loss 0.441092, acc 0.953125, prec 0.0825221, recall 0.89448
2017-12-10T15:02:50.022111: step 4085, loss 0.257937, acc 0.859375, prec 0.0825045, recall 0.89448
2017-12-10T15:02:50.219580: step 4086, loss 0.242615, acc 0.921875, prec 0.0824947, recall 0.89448
2017-12-10T15:02:50.416459: step 4087, loss 0.364606, acc 0.9375, prec 0.0825086, recall 0.894507
2017-12-10T15:02:50.606740: step 4088, loss 0.184854, acc 0.9375, prec 0.0825008, recall 0.894507
2017-12-10T15:02:50.802291: step 4089, loss 0.243159, acc 0.921875, prec 0.0825128, recall 0.894534
2017-12-10T15:02:50.996727: step 4090, loss 0.257138, acc 0.984375, prec 0.082576, recall 0.894615
2017-12-10T15:02:51.188925: step 4091, loss 0.162955, acc 0.921875, prec 0.0825662, recall 0.894615
2017-12-10T15:02:51.384006: step 4092, loss 0.342235, acc 0.921875, prec 0.0825781, recall 0.894642
2017-12-10T15:02:51.577181: step 4093, loss 0.294739, acc 0.890625, prec 0.0826079, recall 0.894696
2017-12-10T15:02:51.771257: step 4094, loss 0.137499, acc 0.953125, prec 0.082602, recall 0.894696
2017-12-10T15:02:51.963872: step 4095, loss 0.186069, acc 0.953125, prec 0.0826612, recall 0.894777
2017-12-10T15:02:52.162697: step 4096, loss 0.0961217, acc 0.953125, prec 0.0826554, recall 0.894777
2017-12-10T15:02:52.359944: step 4097, loss 0.0621973, acc 0.96875, prec 0.0826515, recall 0.894777
2017-12-10T15:02:52.552998: step 4098, loss 0.0865194, acc 0.953125, prec 0.0826456, recall 0.894777
2017-12-10T15:02:52.748262: step 4099, loss 0.120171, acc 0.953125, prec 0.0826397, recall 0.894777
2017-12-10T15:02:52.940876: step 4100, loss 0.0762269, acc 0.984375, prec 0.0826595, recall 0.894804
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_0/1512935327/checkpoints/model-4100

2017-12-10T15:02:54.819686: step 4101, loss 0.190638, acc 0.984375, prec 0.0826792, recall 0.894831
2017-12-10T15:02:55.017217: step 4102, loss 0.0902674, acc 0.96875, prec 0.0826753, recall 0.894831
2017-12-10T15:02:55.208846: step 4103, loss 0.174207, acc 0.96875, prec 0.0827148, recall 0.894885
2017-12-10T15:02:55.401875: step 4104, loss 0.219245, acc 0.984375, prec 0.0827345, recall 0.894912
2017-12-10T15:02:55.601606: step 4105, loss 0.0728673, acc 0.96875, prec 0.0827306, recall 0.894912
2017-12-10T15:02:55.794768: step 4106, loss 0.0589735, acc 0.984375, prec 0.0827937, recall 0.894992
2017-12-10T15:02:55.988862: step 4107, loss 1.34201, acc 0.96875, prec 0.0827917, recall 0.894764
2017-12-10T15:02:56.191596: step 4108, loss 0.0597962, acc 0.96875, prec 0.0827878, recall 0.894764
2017-12-10T15:02:56.387478: step 4109, loss 0.0239441, acc 0.984375, prec 0.0827858, recall 0.894764
2017-12-10T15:02:56.582311: step 4110, loss 0.0201314, acc 0.984375, prec 0.0828056, recall 0.894791
2017-12-10T15:02:56.773918: step 4111, loss 0.0318256, acc 0.984375, prec 0.0828036, recall 0.894791
2017-12-10T15:02:56.965110: step 4112, loss 0.114687, acc 0.953125, prec 0.0827977, recall 0.894791
2017-12-10T15:02:57.156464: step 4113, loss 0.314756, acc 0.921875, prec 0.0828313, recall 0.894844
2017-12-10T15:02:57.352991: step 4114, loss 0.0482953, acc 0.984375, prec 0.082851, recall 0.894871
2017-12-10T15:02:57.547771: step 4115, loss 0.0864198, acc 0.96875, prec 0.0828471, recall 0.894871
2017-12-10T15:02:57.746448: step 4116, loss 0.256128, acc 0.9375, prec 0.0828393, recall 0.894871
2017-12-10T15:02:57.938922: step 4117, loss 0.238375, acc 0.921875, prec 0.0828511, recall 0.894898
2017-12-10T15:02:58.132746: step 4118, loss 0.0333655, acc 1, prec 0.0828511, recall 0.894898
2017-12-10T15:02:58.324277: step 4119, loss 0.213042, acc 0.9375, prec 0.0828433, recall 0.894898
2017-12-10T15:02:58.518427: step 4120, loss 0.0251952, acc 1, prec 0.0828433, recall 0.894898
2017-12-10T15:02:58.709427: step 4121, loss 0.0976401, acc 0.953125, prec 0.0829024, recall 0.894978
2017-12-10T15:02:58.904537: step 4122, loss 0.421789, acc 1, prec 0.0829457, recall 0.895032
2017-12-10T15:02:59.103140: step 4123, loss 0.248169, acc 0.9375, prec 0.0829595, recall 0.895059
2017-12-10T15:02:59.300987: step 4124, loss 0.266415, acc 0.90625, prec 0.0829911, recall 0.895112
2017-12-10T15:02:59.493010: step 4125, loss 0.104326, acc 0.953125, prec 0.0829852, recall 0.895112
2017-12-10T15:02:59.686714: step 4126, loss 0.129217, acc 1, prec 0.0830501, recall 0.895192
2017-12-10T15:02:59.888367: step 4127, loss 0.270545, acc 0.96875, prec 0.0830678, recall 0.895219
2017-12-10T15:03:00.084930: step 4128, loss 0.544323, acc 0.9375, prec 0.0831033, recall 0.895272
2017-12-10T15:03:00.281103: step 4129, loss 0.231355, acc 0.90625, prec 0.0830915, recall 0.895272
2017-12-10T15:03:00.475008: step 4130, loss 0.273912, acc 0.96875, prec 0.0830876, recall 0.895272
2017-12-10T15:03:00.671881: step 4131, loss 0.0960815, acc 0.96875, prec 0.0831053, recall 0.895299
2017-12-10T15:03:00.867249: step 4132, loss 0.139377, acc 0.96875, prec 0.0831014, recall 0.895299
2017-12-10T15:03:01.061956: step 4133, loss 0.284856, acc 0.9375, prec 0.0830935, recall 0.895299
2017-12-10T15:03:01.259849: step 4134, loss 0.028579, acc 0.984375, prec 0.0830916, recall 0.895299
2017-12-10T15:03:01.454546: step 4135, loss 0.0524433, acc 0.984375, prec 0.0831329, recall 0.895352
2017-12-10T15:03:01.648054: step 4136, loss 0.0717034, acc 0.984375, prec 0.0831309, recall 0.895352
2017-12-10T15:03:01.843125: step 4137, loss 0.201247, acc 0.96875, prec 0.0831486, recall 0.895378
2017-12-10T15:03:02.036324: step 4138, loss 0.171071, acc 0.953125, prec 0.0831427, recall 0.895378
2017-12-10T15:03:02.229469: step 4139, loss 0.181095, acc 0.96875, prec 0.0831388, recall 0.895378
2017-12-10T15:03:02.420055: step 4140, loss 0.0729107, acc 0.96875, prec 0.0831349, recall 0.895378
2017-12-10T15:03:02.616418: step 4141, loss 0.103738, acc 0.96875, prec 0.0831958, recall 0.895458
2017-12-10T15:03:02.810749: step 4142, loss 0.142829, acc 0.984375, prec 0.0831939, recall 0.895458
2017-12-10T15:03:03.009515: step 4143, loss 0.0254033, acc 1, prec 0.0832155, recall 0.895485
2017-12-10T15:03:03.204615: step 4144, loss 0.0934772, acc 0.953125, prec 0.0832096, recall 0.895485
2017-12-10T15:03:03.399978: step 4145, loss 0.0369863, acc 0.984375, prec 0.0832292, recall 0.895511
2017-12-10T15:03:03.594216: step 4146, loss 0.379838, acc 0.953125, prec 0.0832233, recall 0.895511
2017-12-10T15:03:03.789808: step 4147, loss 0.0685539, acc 0.984375, prec 0.0832646, recall 0.895564
2017-12-10T15:03:03.984947: step 4148, loss 0.0734552, acc 0.984375, prec 0.0832842, recall 0.89559
2017-12-10T15:03:04.181011: step 4149, loss 0.0092575, acc 1, prec 0.0833274, recall 0.895643
2017-12-10T15:03:04.377667: step 4150, loss 0.491962, acc 0.984375, prec 0.0833687, recall 0.895696
2017-12-10T15:03:04.576613: step 4151, loss 0.0110354, acc 1, prec 0.0833687, recall 0.895696
2017-12-10T15:03:04.778038: step 4152, loss 0.0168372, acc 1, prec 0.0834119, recall 0.895749
2017-12-10T15:03:04.970945: step 4153, loss 0.0445944, acc 1, prec 0.0834335, recall 0.895775
2017-12-10T15:03:05.166946: step 4154, loss 0.21961, acc 0.96875, prec 0.0834727, recall 0.895828
2017-12-10T15:03:05.365366: step 4155, loss 0.570242, acc 0.953125, prec 0.0834884, recall 0.895854
2017-12-10T15:03:05.560406: step 4156, loss 0.139685, acc 0.953125, prec 0.0834825, recall 0.895854
2017-12-10T15:03:05.754408: step 4157, loss 0.221333, acc 0.9375, prec 0.0834747, recall 0.895854
2017-12-10T15:03:05.949030: step 4158, loss 0.194708, acc 0.984375, prec 0.0835159, recall 0.895907
2017-12-10T15:03:06.150455: step 4159, loss 0.222662, acc 0.953125, prec 0.0835315, recall 0.895933
2017-12-10T15:03:06.344069: step 4160, loss 0.241614, acc 0.9375, prec 0.0835453, recall 0.89596
2017-12-10T15:03:06.537808: step 4161, loss 0.00467472, acc 1, prec 0.0835668, recall 0.895986
2017-12-10T15:03:06.730456: step 4162, loss 0.317157, acc 0.859375, prec 0.0835491, recall 0.895986
2017-12-10T15:03:06.927711: step 4163, loss 0.168308, acc 0.96875, prec 0.0835452, recall 0.895986
2017-12-10T15:03:07.120747: step 4164, loss 0.0399206, acc 0.984375, prec 0.0835432, recall 0.895986
2017-12-10T15:03:07.315788: step 4165, loss 0.0212389, acc 0.984375, prec 0.0835413, recall 0.895986
2017-12-10T15:03:07.508701: step 4166, loss 0.0962573, acc 0.9375, prec 0.0835981, recall 0.896065
2017-12-10T15:03:07.706735: step 4167, loss 0.198909, acc 0.953125, prec 0.0836138, recall 0.896091
2017-12-10T15:03:07.902840: step 4168, loss 0.0164284, acc 1, prec 0.0836138, recall 0.896091
2017-12-10T15:03:08.095575: step 4169, loss 0.157289, acc 0.953125, prec 0.0836079, recall 0.896091
2017-12-10T15:03:08.290809: step 4170, loss 0.1297, acc 0.953125, prec 0.083602, recall 0.896091
2017-12-10T15:03:08.491140: step 4171, loss 0.277623, acc 0.96875, prec 0.0836196, recall 0.896117
2017-12-10T15:03:08.685444: step 4172, loss 0.0736623, acc 0.953125, prec 0.0836137, recall 0.896117
2017-12-10T15:03:08.879797: step 4173, loss 0.0293758, acc 1, prec 0.0836352, recall 0.896143
2017-12-10T15:03:09.072235: step 4174, loss 0.0668924, acc 0.96875, prec 0.0836313, recall 0.896143
2017-12-10T15:03:09.266470: step 4175, loss 0.104689, acc 0.984375, prec 0.0836293, recall 0.896143
2017-12-10T15:03:09.463240: step 4176, loss 0.248373, acc 0.984375, prec 0.0837136, recall 0.896248
2017-12-10T15:03:09.660758: step 4177, loss 0.0711311, acc 0.96875, prec 0.0837528, recall 0.8963
2017-12-10T15:03:09.855743: step 4178, loss 0.0755667, acc 0.984375, prec 0.0837723, recall 0.896326
2017-12-10T15:03:10.046796: step 4179, loss 0.0988499, acc 0.984375, prec 0.0837919, recall 0.896352
2017-12-10T15:03:10.241970: step 4180, loss 0.0279319, acc 1, prec 0.0838135, recall 0.896378
2017-12-10T15:03:10.434775: step 4181, loss 0.186856, acc 0.953125, prec 0.0838506, recall 0.89643
2017-12-10T15:03:10.636702: step 4182, loss 0.0340574, acc 0.984375, prec 0.0838917, recall 0.896482
2017-12-10T15:03:10.833516: step 4183, loss 0.152251, acc 0.953125, prec 0.0838858, recall 0.896482
2017-12-10T15:03:11.026818: step 4184, loss 0.0620741, acc 0.96875, prec 0.0838819, recall 0.896482
2017-12-10T15:03:11.219361: step 4185, loss 0.0860168, acc 0.96875, prec 0.0838779, recall 0.896482
2017-12-10T15:03:11.414367: step 4186, loss 0.122211, acc 0.9375, prec 0.0838916, recall 0.896508
2017-12-10T15:03:11.609691: step 4187, loss 0.00654153, acc 1, prec 0.0838916, recall 0.896508
2017-12-10T15:03:11.806538: step 4188, loss 0.2434, acc 0.984375, prec 0.0839327, recall 0.89656
2017-12-10T15:03:12.003884: step 4189, loss 3.56389, acc 0.96875, prec 0.0839738, recall 0.896387
2017-12-10T15:03:12.200994: step 4190, loss 0.274017, acc 0.984375, prec 0.0840149, recall 0.896439
2017-12-10T15:03:12.396787: step 4191, loss 0.197761, acc 0.96875, prec 0.084054, recall 0.896491
2017-12-10T15:03:12.592405: step 4192, loss 0.0217505, acc 1, prec 0.084054, recall 0.896491
2017-12-10T15:03:12.789038: step 4193, loss 0.200497, acc 0.953125, prec 0.084048, recall 0.896491
2017-12-10T15:03:12.987257: step 4194, loss 0.128599, acc 0.9375, prec 0.0840616, recall 0.896517
2017-12-10T15:03:13.183955: step 4195, loss 0.335863, acc 0.9375, prec 0.0840537, recall 0.896517
2017-12-10T15:03:13.379368: step 4196, loss 0.207639, acc 0.921875, prec 0.0840654, recall 0.896543
2017-12-10T15:03:13.570245: step 4197, loss 0.382048, acc 0.84375, prec 0.0840456, recall 0.896543
2017-12-10T15:03:13.762845: step 4198, loss 0.411058, acc 0.875, prec 0.0840299, recall 0.896543
2017-12-10T15:03:13.955514: step 4199, loss 0.463574, acc 0.84375, prec 0.0841176, recall 0.896672
2017-12-10T15:03:14.158285: step 4200, loss 0.468602, acc 0.90625, prec 0.0841058, recall 0.896672
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_0/1512935327/checkpoints/model-4200

2017-12-10T15:03:15.494720: step 4201, loss 0.355706, acc 0.90625, prec 0.0841154, recall 0.896698
2017-12-10T15:03:15.689151: step 4202, loss 0.773916, acc 0.84375, prec 0.0841387, recall 0.89675
2017-12-10T15:03:15.887578: step 4203, loss 0.341821, acc 0.921875, prec 0.0841503, recall 0.896776
2017-12-10T15:03:16.078503: step 4204, loss 0.290926, acc 0.921875, prec 0.0841834, recall 0.896827
2017-12-10T15:03:16.270006: step 4205, loss 0.710394, acc 0.84375, prec 0.0841636, recall 0.896827
2017-12-10T15:03:16.467805: step 4206, loss 0.357543, acc 0.921875, prec 0.0841538, recall 0.896827
2017-12-10T15:03:16.658932: step 4207, loss 0.11429, acc 0.953125, prec 0.0841479, recall 0.896827
2017-12-10T15:03:16.855703: step 4208, loss 0.144809, acc 0.96875, prec 0.0841439, recall 0.896827
2017-12-10T15:03:17.047746: step 4209, loss 0.19563, acc 0.953125, prec 0.0841595, recall 0.896853
2017-12-10T15:03:17.241818: step 4210, loss 0.276543, acc 0.9375, prec 0.084216, recall 0.89693
2017-12-10T15:03:17.434319: step 4211, loss 0.154393, acc 0.9375, prec 0.084251, recall 0.896982
2017-12-10T15:03:17.631851: step 4212, loss 0.93938, acc 0.96875, prec 0.0842685, recall 0.897007
2017-12-10T15:03:17.826119: step 4213, loss 0.211195, acc 0.921875, prec 0.0842801, recall 0.897033
2017-12-10T15:03:18.018227: step 4214, loss 0.28192, acc 0.90625, prec 0.0843111, recall 0.897084
2017-12-10T15:03:18.211362: step 4215, loss 0.233324, acc 0.921875, prec 0.0843227, recall 0.89711
2017-12-10T15:03:18.406947: step 4216, loss 0.0923705, acc 0.984375, prec 0.0843207, recall 0.89711
2017-12-10T15:03:18.600828: step 4217, loss 0.136522, acc 0.921875, prec 0.0843108, recall 0.89711
2017-12-10T15:03:18.791345: step 4218, loss 0.118822, acc 0.96875, prec 0.0843283, recall 0.897136
2017-12-10T15:03:18.985109: step 4219, loss 0.103104, acc 0.96875, prec 0.0843244, recall 0.897136
2017-12-10T15:03:19.182057: step 4220, loss 0.31164, acc 0.953125, prec 0.0843399, recall 0.897161
2017-12-10T15:03:19.375024: step 4221, loss 0.036966, acc 0.984375, prec 0.0843379, recall 0.897161
2017-12-10T15:03:19.574010: step 4222, loss 0.095652, acc 0.953125, prec 0.0843534, recall 0.897187
2017-12-10T15:03:19.768104: step 4223, loss 0.0261307, acc 0.984375, prec 0.0843515, recall 0.897187
2017-12-10T15:03:19.959101: step 4224, loss 0.0971027, acc 0.953125, prec 0.084367, recall 0.897213
2017-12-10T15:03:20.153264: step 4225, loss 0.0362814, acc 1, prec 0.084367, recall 0.897213
2017-12-10T15:03:20.348951: step 4226, loss 0.105425, acc 0.953125, prec 0.084361, recall 0.897213
2017-12-10T15:03:20.543573: step 4227, loss 0.170215, acc 0.96875, prec 0.0843571, recall 0.897213
2017-12-10T15:03:20.739354: step 4228, loss 0.114544, acc 0.96875, prec 0.0843746, recall 0.897238
2017-12-10T15:03:20.939561: step 4229, loss 0.011737, acc 1, prec 0.084396, recall 0.897264
2017-12-10T15:03:21.137143: step 4230, loss 0.17043, acc 0.96875, prec 0.0844135, recall 0.897289
2017-12-10T15:03:21.330643: step 4231, loss 0.0081956, acc 1, prec 0.0844349, recall 0.897315
2017-12-10T15:03:21.531709: step 4232, loss 0.152829, acc 0.96875, prec 0.0844738, recall 0.897366
2017-12-10T15:03:21.726614: step 4233, loss 0.046068, acc 0.96875, prec 0.0844698, recall 0.897366
2017-12-10T15:03:21.918990: step 4234, loss 0.175976, acc 0.953125, prec 0.0844853, recall 0.897391
2017-12-10T15:03:22.117063: step 4235, loss 0.0483369, acc 0.96875, prec 0.0844813, recall 0.897391
2017-12-10T15:03:22.311335: step 4236, loss 0.0150547, acc 1, prec 0.0845028, recall 0.897417
2017-12-10T15:03:22.506974: step 4237, loss 0.0264766, acc 1, prec 0.0845456, recall 0.897468
2017-12-10T15:03:22.703507: step 4238, loss 1.53792, acc 0.96875, prec 0.0845436, recall 0.897245
2017-12-10T15:03:22.905101: step 4239, loss 4.71834, acc 0.96875, prec 0.0845416, recall 0.897022
2017-12-10T15:03:23.105993: step 4240, loss 0.150784, acc 0.953125, prec 0.0845571, recall 0.897048
2017-12-10T15:03:23.299727: step 4241, loss 0.380609, acc 0.90625, prec 0.0845452, recall 0.897048
2017-12-10T15:03:23.496754: step 4242, loss 0.0546925, acc 0.984375, prec 0.0845861, recall 0.897099
2017-12-10T15:03:23.692471: step 4243, loss 0.202307, acc 0.890625, prec 0.0845936, recall 0.897124
2017-12-10T15:03:23.890495: step 4244, loss 0.141508, acc 0.96875, prec 0.0845897, recall 0.897124
2017-12-10T15:03:24.084521: step 4245, loss 0.161119, acc 0.9375, prec 0.0846032, recall 0.89715
2017-12-10T15:03:24.275465: step 4246, loss 0.337535, acc 0.90625, prec 0.0846341, recall 0.897201
2017-12-10T15:03:24.469683: step 4247, loss 0.173415, acc 0.890625, prec 0.0846416, recall 0.897226
2017-12-10T15:03:24.663452: step 4248, loss 0.458351, acc 0.875, prec 0.0846472, recall 0.897252
2017-12-10T15:03:24.856507: step 4249, loss 0.381355, acc 0.9375, prec 0.0846393, recall 0.897252
2017-12-10T15:03:25.049602: step 4250, loss 0.364121, acc 0.859375, prec 0.0846642, recall 0.897303
2017-12-10T15:03:25.243936: step 4251, loss 0.449491, acc 0.828125, prec 0.0846852, recall 0.897353
2017-12-10T15:03:25.438692: step 4252, loss 0.241808, acc 0.953125, prec 0.0846793, recall 0.897353
2017-12-10T15:03:25.631925: step 4253, loss 0.504373, acc 0.9375, prec 0.0846714, recall 0.897353
2017-12-10T15:03:25.825557: step 4254, loss 0.551022, acc 0.8125, prec 0.0847118, recall 0.89743
2017-12-10T15:03:26.021858: step 4255, loss 0.154754, acc 0.9375, prec 0.0847252, recall 0.897455
2017-12-10T15:03:26.215904: step 4256, loss 0.329679, acc 0.921875, prec 0.0847367, recall 0.89748
2017-12-10T15:03:26.411128: step 4257, loss 1.8076, acc 0.828125, prec 0.0847383, recall 0.897284
2017-12-10T15:03:26.606062: step 4258, loss 0.253724, acc 0.921875, prec 0.0847284, recall 0.897284
2017-12-10T15:03:26.801808: step 4259, loss 0.383664, acc 0.890625, prec 0.0847572, recall 0.897335
2017-12-10T15:03:26.994846: step 4260, loss 0.313672, acc 0.90625, prec 0.0847454, recall 0.897335
2017-12-10T15:03:27.189948: step 4261, loss 0.400453, acc 0.84375, prec 0.0847256, recall 0.897335
2017-12-10T15:03:27.390217: step 4262, loss 0.568254, acc 0.875, prec 0.0847312, recall 0.89736
2017-12-10T15:03:27.585032: step 4263, loss 0.396517, acc 0.859375, prec 0.0847347, recall 0.897385
2017-12-10T15:03:27.780096: step 4264, loss 0.457075, acc 0.890625, prec 0.0847422, recall 0.897411
2017-12-10T15:03:27.970149: step 4265, loss 0.45741, acc 0.8125, prec 0.0847185, recall 0.897411
2017-12-10T15:03:28.165205: step 4266, loss 0.323801, acc 0.875, prec 0.0847241, recall 0.897436
2017-12-10T15:03:28.359281: step 4267, loss 0.165005, acc 0.921875, prec 0.0847568, recall 0.897486
2017-12-10T15:03:28.553565: step 4268, loss 0.985662, acc 0.953125, prec 0.0847935, recall 0.897537
2017-12-10T15:03:28.752665: step 4269, loss 0.15265, acc 0.953125, prec 0.0848089, recall 0.897562
2017-12-10T15:03:28.946421: step 4270, loss 0.231487, acc 0.953125, prec 0.0848455, recall 0.897613
2017-12-10T15:03:29.140866: step 4271, loss 0.179002, acc 0.90625, prec 0.084855, recall 0.897638
2017-12-10T15:03:29.339691: step 4272, loss 0.268179, acc 0.921875, prec 0.0848451, recall 0.897638
2017-12-10T15:03:29.531479: step 4273, loss 0.30046, acc 0.90625, prec 0.0848545, recall 0.897663
2017-12-10T15:03:29.728321: step 4274, loss 0.122141, acc 0.953125, prec 0.0849125, recall 0.897738
2017-12-10T15:03:29.918907: step 4275, loss 0.315536, acc 0.9375, prec 0.0849046, recall 0.897738
2017-12-10T15:03:30.110564: step 4276, loss 0.0473383, acc 0.984375, prec 0.0849239, recall 0.897764
2017-12-10T15:03:30.307927: step 4277, loss 0.135962, acc 0.984375, prec 0.0849644, recall 0.897814
2017-12-10T15:03:30.501955: step 4278, loss 0.0185696, acc 1, prec 0.0849644, recall 0.897814
2017-12-10T15:03:30.701239: step 4279, loss 0.0706218, acc 0.96875, prec 0.0849605, recall 0.897814
2017-12-10T15:03:30.904420: step 4280, loss 0.462499, acc 0.921875, prec 0.0850144, recall 0.897889
2017-12-10T15:03:31.102291: step 4281, loss 0.189317, acc 0.953125, prec 0.0850297, recall 0.897914
2017-12-10T15:03:31.300836: step 4282, loss 0.041247, acc 0.984375, prec 0.0850278, recall 0.897914
2017-12-10T15:03:31.500232: step 4283, loss 0.119924, acc 0.984375, prec 0.0850471, recall 0.897939
2017-12-10T15:03:31.692156: step 4284, loss 0.128349, acc 0.9375, prec 0.0850392, recall 0.897939
2017-12-10T15:03:31.887817: step 4285, loss 0.14411, acc 0.96875, prec 0.0850352, recall 0.897939
2017-12-10T15:03:32.085665: step 4286, loss 0.177207, acc 0.953125, prec 0.0850293, recall 0.897939
2017-12-10T15:03:32.280653: step 4287, loss 0.0477993, acc 0.984375, prec 0.0850273, recall 0.897939
2017-12-10T15:03:32.475713: step 4288, loss 0.0564737, acc 0.984375, prec 0.0850466, recall 0.897964
2017-12-10T15:03:32.672926: step 4289, loss 0.0235038, acc 0.984375, prec 0.0850446, recall 0.897964
2017-12-10T15:03:32.870481: step 4290, loss 0.186304, acc 1, prec 0.0850659, recall 0.897989
2017-12-10T15:03:33.064088: step 4291, loss 0.0705364, acc 0.953125, prec 0.0850599, recall 0.897989
2017-12-10T15:03:33.259708: step 4292, loss 0.155073, acc 0.96875, prec 0.085056, recall 0.897989
2017-12-10T15:03:33.454324: step 4293, loss 0.0162334, acc 1, prec 0.0850772, recall 0.898014
2017-12-10T15:03:33.649390: step 4294, loss 0.22397, acc 0.953125, prec 0.0850713, recall 0.898014
2017-12-10T15:03:33.841118: step 4295, loss 0.112423, acc 0.953125, prec 0.0850654, recall 0.898014
2017-12-10T15:03:34.032871: step 4296, loss 1.3761, acc 0.984375, prec 0.0851271, recall 0.898089
2017-12-10T15:03:34.232381: step 4297, loss 0.307284, acc 0.984375, prec 0.0851889, recall 0.898164
2017-12-10T15:03:34.430106: step 4298, loss 0.0224971, acc 1, prec 0.0851889, recall 0.898164
2017-12-10T15:03:34.626601: step 4299, loss 0.142585, acc 0.953125, prec 0.0852042, recall 0.898189
2017-12-10T15:03:34.820338: step 4300, loss 0.103104, acc 1, prec 0.0852254, recall 0.898214
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_0/1512935327/checkpoints/model-4300

2017-12-10T15:03:35.999047: step 4301, loss 0.0914208, acc 0.9375, prec 0.0852387, recall 0.898239
2017-12-10T15:03:36.198103: step 4302, loss 0.272633, acc 0.953125, prec 0.085254, recall 0.898264
2017-12-10T15:03:36.396700: step 4303, loss 0.194133, acc 0.953125, prec 0.0852906, recall 0.898313
2017-12-10T15:03:36.589723: step 4304, loss 0.0901024, acc 0.96875, prec 0.0852866, recall 0.898313
2017-12-10T15:03:36.785952: step 4305, loss 0.225143, acc 0.890625, prec 0.0853152, recall 0.898363
2017-12-10T15:03:36.985214: step 4306, loss 0.461971, acc 0.890625, prec 0.0853438, recall 0.898413
2017-12-10T15:03:37.181433: step 4307, loss 0.0833619, acc 0.96875, prec 0.0853398, recall 0.898413
2017-12-10T15:03:37.376616: step 4308, loss 0.331708, acc 0.96875, prec 0.0853359, recall 0.898413
2017-12-10T15:03:37.574192: step 4309, loss 0.261375, acc 0.890625, prec 0.085322, recall 0.898413
2017-12-10T15:03:37.769780: step 4310, loss 0.264194, acc 0.921875, prec 0.0853121, recall 0.898413
2017-12-10T15:03:37.968524: step 4311, loss 0.0909206, acc 0.96875, prec 0.0853082, recall 0.898413
2017-12-10T15:03:38.156690: step 4312, loss 0.23398, acc 0.921875, prec 0.0853195, recall 0.898438
2017-12-10T15:03:38.352273: step 4313, loss 0.129027, acc 0.953125, prec 0.0853348, recall 0.898462
2017-12-10T15:03:38.546258: step 4314, loss 0.14954, acc 0.953125, prec 0.0853288, recall 0.898462
2017-12-10T15:03:38.739063: step 4315, loss 0.434042, acc 0.9375, prec 0.0853421, recall 0.898487
2017-12-10T15:03:38.933525: step 4316, loss 0.0616366, acc 0.984375, prec 0.0853613, recall 0.898512
2017-12-10T15:03:39.127599: step 4317, loss 0.0594227, acc 0.984375, prec 0.0853806, recall 0.898537
2017-12-10T15:03:39.324543: step 4318, loss 0.0682819, acc 0.953125, prec 0.0853746, recall 0.898537
2017-12-10T15:03:39.520170: step 4319, loss 0.416433, acc 0.953125, prec 0.0854535, recall 0.898636
2017-12-10T15:03:39.713138: step 4320, loss 0.0289374, acc 1, prec 0.0854746, recall 0.89866
2017-12-10T15:03:39.908648: step 4321, loss 0.304028, acc 0.96875, prec 0.0855342, recall 0.898734
2017-12-10T15:03:40.115158: step 4322, loss 0.240116, acc 0.96875, prec 0.0855515, recall 0.898759
2017-12-10T15:03:40.307958: step 4323, loss 0.0408494, acc 0.984375, prec 0.0855707, recall 0.898783
2017-12-10T15:03:40.505666: step 4324, loss 0.113612, acc 0.984375, prec 0.0855899, recall 0.898808
2017-12-10T15:03:40.699973: step 4325, loss 0.327554, acc 0.875, prec 0.085574, recall 0.898808
2017-12-10T15:03:40.893129: step 4326, loss 0.0765565, acc 0.953125, prec 0.0856104, recall 0.898857
2017-12-10T15:03:41.088328: step 4327, loss 0.306722, acc 0.90625, prec 0.0855985, recall 0.898857
2017-12-10T15:03:41.280498: step 4328, loss 0.0464355, acc 0.984375, prec 0.0855965, recall 0.898857
2017-12-10T15:03:41.475275: step 4329, loss 0.126096, acc 0.984375, prec 0.0856157, recall 0.898882
2017-12-10T15:03:41.674202: step 4330, loss 0.0836556, acc 0.9375, prec 0.085629, recall 0.898906
2017-12-10T15:03:41.872680: step 4331, loss 0.0254314, acc 1, prec 0.0856501, recall 0.898931
2017-12-10T15:03:42.065950: step 4332, loss 0.0835453, acc 0.96875, prec 0.0856673, recall 0.898956
2017-12-10T15:03:42.260814: step 4333, loss 0.270594, acc 0.9375, prec 0.0856806, recall 0.89898
2017-12-10T15:03:42.455659: step 4334, loss 0.235808, acc 0.96875, prec 0.0857401, recall 0.899054
2017-12-10T15:03:42.656012: step 4335, loss 0.0326446, acc 0.984375, prec 0.0857381, recall 0.899054
2017-12-10T15:03:42.849658: step 4336, loss 0.0403724, acc 0.984375, prec 0.0857361, recall 0.899054
2017-12-10T15:03:43.046876: step 4337, loss 0.12696, acc 0.9375, prec 0.0857282, recall 0.899054
2017-12-10T15:03:43.243826: step 4338, loss 0.0145042, acc 1, prec 0.0857282, recall 0.899054
2017-12-10T15:03:43.439655: step 4339, loss 0.0986727, acc 0.96875, prec 0.0857665, recall 0.899103
2017-12-10T15:03:43.636882: step 4340, loss 0.041361, acc 0.984375, prec 0.0857857, recall 0.899127
2017-12-10T15:03:43.834146: step 4341, loss 0.578507, acc 0.9375, prec 0.0857989, recall 0.899152
2017-12-10T15:03:44.028272: step 4342, loss 0.0755715, acc 0.984375, prec 0.085818, recall 0.899176
2017-12-10T15:03:44.228982: step 4343, loss 0.0100194, acc 1, prec 0.0858392, recall 0.8992
2017-12-10T15:03:44.426202: step 4344, loss 3.12458, acc 0.96875, prec 0.0858372, recall 0.898983
2017-12-10T15:03:44.624094: step 4345, loss 0.020445, acc 1, prec 0.0858584, recall 0.899007
2017-12-10T15:03:44.822677: step 4346, loss 0.168945, acc 0.984375, prec 0.0858564, recall 0.899007
2017-12-10T15:03:45.016079: step 4347, loss 0.0901251, acc 0.96875, prec 0.0858735, recall 0.899031
2017-12-10T15:03:45.216672: step 4348, loss 0.0202246, acc 0.984375, prec 0.0858715, recall 0.899031
2017-12-10T15:03:45.410336: step 4349, loss 0.159614, acc 0.953125, prec 0.0858656, recall 0.899031
2017-12-10T15:03:45.605677: step 4350, loss 0.227535, acc 0.921875, prec 0.0858768, recall 0.899056
2017-12-10T15:03:45.796816: step 4351, loss 0.064003, acc 0.984375, prec 0.0859171, recall 0.899105
2017-12-10T15:03:45.990235: step 4352, loss 0.152961, acc 0.90625, prec 0.0859263, recall 0.899129
2017-12-10T15:03:46.185899: step 4353, loss 0.0580473, acc 0.984375, prec 0.0859877, recall 0.899202
2017-12-10T15:03:46.387514: step 4354, loss 0.570808, acc 0.890625, prec 0.0860372, recall 0.899275
2017-12-10T15:03:46.579511: step 4355, loss 0.131542, acc 0.96875, prec 0.0860543, recall 0.8993
2017-12-10T15:03:46.776371: step 4356, loss 0.263291, acc 0.9375, prec 0.0860886, recall 0.899348
2017-12-10T15:03:46.973025: step 4357, loss 0.208958, acc 0.9375, prec 0.0860806, recall 0.899348
2017-12-10T15:03:47.169651: step 4358, loss 0.0628999, acc 0.96875, prec 0.0860978, recall 0.899373
2017-12-10T15:03:47.367688: step 4359, loss 0.587098, acc 0.9375, prec 0.0861531, recall 0.899445
2017-12-10T15:03:47.564067: step 4360, loss 0.28745, acc 0.9375, prec 0.0861663, recall 0.89947
2017-12-10T15:03:47.765401: step 4361, loss 0.218485, acc 0.953125, prec 0.0862236, recall 0.899542
2017-12-10T15:03:47.977129: step 4362, loss 0.134618, acc 0.953125, prec 0.0862387, recall 0.899566
2017-12-10T15:03:48.171656: step 4363, loss 0.0318622, acc 1, prec 0.0862387, recall 0.899566
2017-12-10T15:03:48.364342: step 4364, loss 0.229104, acc 0.921875, prec 0.086271, recall 0.899615
2017-12-10T15:03:48.561491: step 4365, loss 0.244131, acc 0.9375, prec 0.086263, recall 0.899615
2017-12-10T15:03:48.753432: step 4366, loss 0.0969284, acc 0.96875, prec 0.086259, recall 0.899615
2017-12-10T15:03:48.956124: step 4367, loss 0.0814773, acc 0.96875, prec 0.0862761, recall 0.899639
2017-12-10T15:03:49.153601: step 4368, loss 0.182876, acc 0.953125, prec 0.0863123, recall 0.899687
2017-12-10T15:03:49.347517: step 4369, loss 0.20411, acc 0.9375, prec 0.0863255, recall 0.899711
2017-12-10T15:03:49.541372: step 4370, loss 0.247068, acc 0.9375, prec 0.0863175, recall 0.899711
2017-12-10T15:03:49.737668: step 4371, loss 0.111754, acc 0.953125, prec 0.0863326, recall 0.899736
2017-12-10T15:03:49.932543: step 4372, loss 0.264625, acc 0.953125, prec 0.0863898, recall 0.899808
2017-12-10T15:03:50.125852: step 4373, loss 0.318894, acc 0.984375, prec 0.0864089, recall 0.899832
2017-12-10T15:03:50.322427: step 4374, loss 0.497044, acc 0.96875, prec 0.0864682, recall 0.899904
2017-12-10T15:03:50.518034: step 4375, loss 0.0248309, acc 1, prec 0.0864682, recall 0.899904
2017-12-10T15:03:50.719151: step 4376, loss 0.0519555, acc 0.984375, prec 0.0864662, recall 0.899904
2017-12-10T15:03:50.912693: step 4377, loss 0.167738, acc 0.953125, prec 0.0864813, recall 0.899928
2017-12-10T15:03:51.112271: step 4378, loss 0.166173, acc 0.953125, prec 0.0864753, recall 0.899928
2017-12-10T15:03:51.311837: step 4379, loss 0.034955, acc 0.984375, prec 0.0864733, recall 0.899928
2017-12-10T15:03:51.509558: step 4380, loss 2.3774, acc 0.9375, prec 0.0865094, recall 0.89976
2017-12-10T15:03:51.705910: step 4381, loss 0.126109, acc 0.96875, prec 0.0865265, recall 0.899784
2017-12-10T15:03:51.901239: step 4382, loss 0.387729, acc 0.921875, prec 0.0865586, recall 0.899832
2017-12-10T15:03:52.098371: step 4383, loss 0.402609, acc 0.875, prec 0.0865427, recall 0.899832
2017-12-10T15:03:52.299257: step 4384, loss 0.199695, acc 0.9375, prec 0.0865347, recall 0.899832
2017-12-10T15:03:52.496233: step 4385, loss 0.195657, acc 0.96875, prec 0.0865518, recall 0.899856
2017-12-10T15:03:52.688663: step 4386, loss 0.605065, acc 0.90625, prec 0.0865819, recall 0.899904
2017-12-10T15:03:52.884085: step 4387, loss 0.28257, acc 0.921875, prec 0.0865719, recall 0.899904
2017-12-10T15:03:53.077128: step 4388, loss 0.158006, acc 0.921875, prec 0.0865619, recall 0.899904
2017-12-10T15:03:53.275359: step 4389, loss 1.58069, acc 0.953125, prec 0.086579, recall 0.899713
2017-12-10T15:03:53.469859: step 4390, loss 0.273546, acc 0.9375, prec 0.0866131, recall 0.899761
2017-12-10T15:03:53.661691: step 4391, loss 0.45513, acc 0.828125, prec 0.0865911, recall 0.899761
2017-12-10T15:03:53.851929: step 4392, loss 0.332011, acc 0.859375, prec 0.0866363, recall 0.899833
2017-12-10T15:03:54.043850: step 4393, loss 0.533241, acc 0.84375, prec 0.0866163, recall 0.899833
2017-12-10T15:03:54.235574: step 4394, loss 0.447765, acc 0.875, prec 0.0866214, recall 0.899857
2017-12-10T15:03:54.427275: step 4395, loss 0.233398, acc 0.90625, prec 0.0866515, recall 0.899904
2017-12-10T15:03:54.627077: step 4396, loss 0.319548, acc 0.890625, prec 0.0866375, recall 0.899904
2017-12-10T15:03:54.817366: step 4397, loss 0.389326, acc 0.859375, prec 0.0866196, recall 0.899904
2017-12-10T15:03:55.015202: step 4398, loss 0.444889, acc 0.90625, prec 0.0866497, recall 0.899952
2017-12-10T15:03:55.208442: step 4399, loss 0.171443, acc 0.96875, prec 0.0866877, recall 0.9
2017-12-10T15:03:55.402957: step 4400, loss 0.496748, acc 0.875, prec 0.0866717, recall 0.9
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_0/1512935327/checkpoints/model-4400

2017-12-10T15:03:56.654312: step 4401, loss 0.465953, acc 0.90625, prec 0.0866808, recall 0.900024
2017-12-10T15:03:56.848854: step 4402, loss 0.271315, acc 0.921875, prec 0.0866918, recall 0.900048
2017-12-10T15:03:57.040681: step 4403, loss 0.101311, acc 0.953125, prec 0.0866858, recall 0.900048
2017-12-10T15:03:57.232095: step 4404, loss 0.181176, acc 0.9375, prec 0.0867198, recall 0.900095
2017-12-10T15:03:57.422453: step 4405, loss 0.141136, acc 0.953125, prec 0.0867138, recall 0.900095
2017-12-10T15:03:57.621264: step 4406, loss 0.207269, acc 0.953125, prec 0.0867288, recall 0.900119
2017-12-10T15:03:57.814304: step 4407, loss 0.332572, acc 0.9375, prec 0.0867628, recall 0.900167
2017-12-10T15:03:58.009012: step 4408, loss 0.0538817, acc 0.984375, prec 0.0867608, recall 0.900167
2017-12-10T15:03:58.211133: step 4409, loss 0.188584, acc 0.984375, prec 0.0868008, recall 0.900214
2017-12-10T15:03:58.408455: step 4410, loss 0.0277713, acc 1, prec 0.0868427, recall 0.900262
2017-12-10T15:03:58.603774: step 4411, loss 0.140216, acc 0.96875, prec 0.0868387, recall 0.900262
2017-12-10T15:03:58.797589: step 4412, loss 0.732348, acc 0.953125, prec 0.0868747, recall 0.900309
2017-12-10T15:03:58.999877: step 4413, loss 0.316931, acc 0.953125, prec 0.0868896, recall 0.900333
2017-12-10T15:03:59.204451: step 4414, loss 0.0695866, acc 0.984375, prec 0.0869086, recall 0.900357
2017-12-10T15:03:59.401440: step 4415, loss 0.0981149, acc 0.96875, prec 0.0869256, recall 0.90038
2017-12-10T15:03:59.595671: step 4416, loss 0.161122, acc 0.953125, prec 0.0869196, recall 0.90038
2017-12-10T15:03:59.788625: step 4417, loss 0.142525, acc 0.9375, prec 0.0869116, recall 0.90038
2017-12-10T15:03:59.986796: step 4418, loss 0.0422831, acc 0.96875, prec 0.0869076, recall 0.90038
2017-12-10T15:04:00.180768: step 4419, loss 0.16394, acc 0.96875, prec 0.0869246, recall 0.900404
2017-12-10T15:04:00.372203: step 4420, loss 0.550156, acc 0.890625, prec 0.0869316, recall 0.900428
2017-12-10T15:04:00.570439: step 4421, loss 0.0489996, acc 0.984375, prec 0.0869505, recall 0.900451
2017-12-10T15:04:00.765473: step 4422, loss 0.227696, acc 0.921875, prec 0.0869406, recall 0.900451
2017-12-10T15:04:00.957423: step 4423, loss 0.115771, acc 0.9375, prec 0.0869326, recall 0.900451
2017-12-10T15:04:01.156728: step 4424, loss 0.133205, acc 0.953125, prec 0.0869475, recall 0.900475
2017-12-10T15:04:01.354018: step 4425, loss 0.3222, acc 0.90625, prec 0.0869356, recall 0.900475
2017-12-10T15:04:01.551419: step 4426, loss 0.133126, acc 0.953125, prec 0.0869296, recall 0.900475
2017-12-10T15:04:01.747490: step 4427, loss 0.168981, acc 0.953125, prec 0.0869236, recall 0.900475
2017-12-10T15:04:01.942539: step 4428, loss 0.225672, acc 0.921875, prec 0.0869137, recall 0.900475
2017-12-10T15:04:02.135745: step 4429, loss 0.15728, acc 0.953125, prec 0.0869077, recall 0.900475
2017-12-10T15:04:02.328859: step 4430, loss 0.211964, acc 0.921875, prec 0.0869187, recall 0.900499
2017-12-10T15:04:02.523485: step 4431, loss 0.0298941, acc 1, prec 0.0869187, recall 0.900499
2017-12-10T15:04:02.720022: step 4432, loss 0.133306, acc 0.953125, prec 0.0869127, recall 0.900499
2017-12-10T15:04:02.917399: step 4433, loss 0.370586, acc 0.96875, prec 0.0869715, recall 0.90057
2017-12-10T15:04:03.118057: step 4434, loss 0.322623, acc 0.953125, prec 0.0870073, recall 0.900617
2017-12-10T15:04:03.320440: step 4435, loss 0.339776, acc 0.953125, prec 0.0870014, recall 0.900617
2017-12-10T15:04:03.513651: step 4436, loss 0.263078, acc 0.953125, prec 0.0870163, recall 0.90064
2017-12-10T15:04:03.708017: step 4437, loss 0.208519, acc 0.96875, prec 0.0870332, recall 0.900664
2017-12-10T15:04:03.908275: step 4438, loss 0.0794979, acc 0.96875, prec 0.0870292, recall 0.900664
2017-12-10T15:04:04.101149: step 4439, loss 0.0748686, acc 0.984375, prec 0.0870482, recall 0.900687
2017-12-10T15:04:04.294456: step 4440, loss 0.0322427, acc 0.984375, prec 0.0870462, recall 0.900687
2017-12-10T15:04:04.495086: step 4441, loss 0.022603, acc 1, prec 0.0870671, recall 0.900711
2017-12-10T15:04:04.689004: step 4442, loss 0.703878, acc 1, prec 0.0871089, recall 0.900758
2017-12-10T15:04:04.885918: step 4443, loss 0.139833, acc 0.96875, prec 0.0871049, recall 0.900758
2017-12-10T15:04:05.078852: step 4444, loss 0.207498, acc 0.9375, prec 0.0870969, recall 0.900758
2017-12-10T15:04:05.274521: step 4445, loss 0.0388877, acc 0.96875, prec 0.0870929, recall 0.900758
2017-12-10T15:04:05.470103: step 4446, loss 0.122726, acc 0.984375, prec 0.0871118, recall 0.900781
2017-12-10T15:04:05.669228: step 4447, loss 0.143479, acc 0.96875, prec 0.0871079, recall 0.900781
2017-12-10T15:04:05.874700: step 4448, loss 0.194521, acc 0.96875, prec 0.0871039, recall 0.900781
2017-12-10T15:04:06.066829: step 4449, loss 0.2925, acc 0.96875, prec 0.0871626, recall 0.900852
2017-12-10T15:04:06.261485: step 4450, loss 0.255812, acc 0.921875, prec 0.0871735, recall 0.900875
2017-12-10T15:04:06.455091: step 4451, loss 0.253877, acc 0.9375, prec 0.0872282, recall 0.900946
2017-12-10T15:04:06.648973: step 4452, loss 0.821175, acc 0.90625, prec 0.0872371, recall 0.900969
2017-12-10T15:04:06.844240: step 4453, loss 0.032189, acc 1, prec 0.0872371, recall 0.900969
2017-12-10T15:04:07.044385: step 4454, loss 0.0583694, acc 0.96875, prec 0.087254, recall 0.900992
2017-12-10T15:04:07.242672: step 4455, loss 0.127975, acc 0.96875, prec 0.08725, recall 0.900992
2017-12-10T15:04:07.437527: step 4456, loss 0.371406, acc 0.96875, prec 0.087246, recall 0.900992
2017-12-10T15:04:07.637548: step 4457, loss 0.0611034, acc 0.96875, prec 0.087242, recall 0.900992
2017-12-10T15:04:07.830853: step 4458, loss 0.0320471, acc 0.984375, prec 0.08724, recall 0.900992
2017-12-10T15:04:08.024511: step 4459, loss 0.112966, acc 0.953125, prec 0.087234, recall 0.900992
2017-12-10T15:04:08.220194: step 4460, loss 0.196569, acc 0.953125, prec 0.0872907, recall 0.901063
2017-12-10T15:04:08.413351: step 4461, loss 0.197655, acc 0.953125, prec 0.0873265, recall 0.901109
2017-12-10T15:04:08.611204: step 4462, loss 0.134707, acc 0.96875, prec 0.0873642, recall 0.901156
2017-12-10T15:04:08.809124: step 4463, loss 0.195008, acc 0.9375, prec 0.0873562, recall 0.901156
2017-12-10T15:04:09.002977: step 4464, loss 0.116288, acc 0.953125, prec 0.087392, recall 0.901203
2017-12-10T15:04:09.197435: step 4465, loss 0.0962094, acc 0.96875, prec 0.087388, recall 0.901203
2017-12-10T15:04:09.396671: step 4466, loss 0.0823455, acc 0.96875, prec 0.087384, recall 0.901203
2017-12-10T15:04:09.595664: step 4467, loss 0.00950629, acc 1, prec 0.0874257, recall 0.901249
2017-12-10T15:04:09.792039: step 4468, loss 0.126306, acc 0.953125, prec 0.0874197, recall 0.901249
2017-12-10T15:04:09.986040: step 4469, loss 0.0591, acc 0.984375, prec 0.0874594, recall 0.901296
2017-12-10T15:04:10.182822: step 4470, loss 0.042512, acc 1, prec 0.0875011, recall 0.901342
2017-12-10T15:04:10.376833: step 4471, loss 0.115769, acc 0.96875, prec 0.087518, recall 0.901365
2017-12-10T15:04:10.571994: step 4472, loss 0.094535, acc 0.984375, prec 0.087516, recall 0.901365
2017-12-10T15:04:10.752447: step 4473, loss 0.253497, acc 1, prec 0.0875577, recall 0.901412
2017-12-10T15:04:10.954087: step 4474, loss 0.145377, acc 0.96875, prec 0.0875746, recall 0.901435
2017-12-10T15:04:11.152782: step 4475, loss 0.648302, acc 0.921875, prec 0.0876063, recall 0.901481
2017-12-10T15:04:11.349870: step 4476, loss 0.196339, acc 0.96875, prec 0.0876648, recall 0.901551
2017-12-10T15:04:11.544967: step 4477, loss 0.0189062, acc 0.984375, prec 0.0876628, recall 0.901551
2017-12-10T15:04:11.738932: step 4478, loss 0.270704, acc 0.9375, prec 0.0876548, recall 0.901551
2017-12-10T15:04:11.933011: step 4479, loss 0.134593, acc 0.984375, prec 0.0876736, recall 0.901574
2017-12-10T15:04:12.133199: step 4480, loss 0.142566, acc 0.96875, prec 0.0876696, recall 0.901574
2017-12-10T15:04:12.328836: step 4481, loss 0.0325343, acc 0.984375, prec 0.0876884, recall 0.901597
2017-12-10T15:04:12.523814: step 4482, loss 0.128881, acc 0.96875, prec 0.0876844, recall 0.901597
2017-12-10T15:04:12.719567: step 4483, loss 0.0317349, acc 1, prec 0.0877053, recall 0.90162
2017-12-10T15:04:12.911990: step 4484, loss 0.0324657, acc 0.984375, prec 0.0877241, recall 0.901643
2017-12-10T15:04:13.107794: step 4485, loss 0.111499, acc 0.953125, prec 0.0877389, recall 0.901666
2017-12-10T15:04:13.304699: step 4486, loss 0.216722, acc 0.953125, prec 0.0877329, recall 0.901666
2017-12-10T15:04:13.499463: step 4487, loss 0.289646, acc 0.953125, prec 0.0877477, recall 0.901689
2017-12-10T15:04:13.698711: step 4488, loss 0.84975, acc 0.984375, prec 0.0877874, recall 0.901735
2017-12-10T15:04:13.898227: step 4489, loss 0.0500114, acc 0.984375, prec 0.0878062, recall 0.901758
2017-12-10T15:04:14.095384: step 4490, loss 0.19507, acc 0.953125, prec 0.087821, recall 0.901781
2017-12-10T15:04:14.297070: step 4491, loss 0.0184004, acc 1, prec 0.087821, recall 0.901781
2017-12-10T15:04:14.489186: step 4492, loss 0.118082, acc 0.96875, prec 0.087817, recall 0.901781
2017-12-10T15:04:14.684814: step 4493, loss 0.0219621, acc 1, prec 0.087817, recall 0.901781
2017-12-10T15:04:14.878870: step 4494, loss 0.0736914, acc 0.96875, prec 0.087813, recall 0.901781
2017-12-10T15:04:15.071966: step 4495, loss 0.237566, acc 0.953125, prec 0.087807, recall 0.901781
2017-12-10T15:04:15.263913: step 4496, loss 0.167808, acc 0.953125, prec 0.087801, recall 0.901781
2017-12-10T15:04:15.462844: step 4497, loss 0.126673, acc 0.9375, prec 0.087793, recall 0.901781
2017-12-10T15:04:15.658800: step 4498, loss 0.15812, acc 0.9375, prec 0.0878058, recall 0.901805
2017-12-10T15:04:15.851138: step 4499, loss 0.193851, acc 0.9375, prec 0.0877978, recall 0.901805
2017-12-10T15:04:16.050099: step 4500, loss 0.110763, acc 0.96875, prec 0.0877937, recall 0.901805
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_0/1512935327/checkpoints/model-4500

2017-12-10T15:04:17.549038: step 4501, loss 0.188653, acc 0.953125, prec 0.0878502, recall 0.901874
2017-12-10T15:04:17.744462: step 4502, loss 0.0562048, acc 0.984375, prec 0.0878898, recall 0.901919
2017-12-10T15:04:17.938633: step 4503, loss 1.42772, acc 0.96875, prec 0.0878878, recall 0.901708
2017-12-10T15:04:18.137244: step 4504, loss 0.0443774, acc 0.984375, prec 0.0878858, recall 0.901708
2017-12-10T15:04:18.332179: step 4505, loss 0.13554, acc 0.953125, prec 0.0878798, recall 0.901708
2017-12-10T15:04:18.529628: step 4506, loss 0.0527779, acc 0.96875, prec 0.0878965, recall 0.901731
2017-12-10T15:04:18.720234: step 4507, loss 0.13918, acc 0.953125, prec 0.0879321, recall 0.901777
2017-12-10T15:04:18.922462: step 4508, loss 0.132237, acc 0.953125, prec 0.0879677, recall 0.901823
2017-12-10T15:04:19.117050: step 4509, loss 0.159017, acc 0.9375, prec 0.0879597, recall 0.901823
2017-12-10T15:04:19.314150: step 4510, loss 0.165913, acc 0.921875, prec 0.0879912, recall 0.901869
2017-12-10T15:04:19.510645: step 4511, loss 0.232297, acc 0.90625, prec 0.0879792, recall 0.901869
2017-12-10T15:04:19.706978: step 4512, loss 0.262575, acc 0.921875, prec 0.08799, recall 0.901892
2017-12-10T15:04:19.909656: step 4513, loss 0.265314, acc 0.921875, prec 0.08798, recall 0.901892
2017-12-10T15:04:20.103109: step 4514, loss 0.0576757, acc 0.984375, prec 0.0879779, recall 0.901892
2017-12-10T15:04:20.301987: step 4515, loss 0.148049, acc 0.953125, prec 0.0879719, recall 0.901892
2017-12-10T15:04:20.495695: step 4516, loss 0.137706, acc 0.921875, prec 0.0879827, recall 0.901915
2017-12-10T15:04:20.689482: step 4517, loss 0.0258523, acc 1, prec 0.088045, recall 0.901984
2017-12-10T15:04:20.884251: step 4518, loss 0.0425476, acc 0.984375, prec 0.0880638, recall 0.902007
2017-12-10T15:04:21.083832: step 4519, loss 0.107005, acc 0.96875, prec 0.0880598, recall 0.902007
2017-12-10T15:04:21.279648: step 4520, loss 0.277612, acc 0.90625, prec 0.0880685, recall 0.902029
2017-12-10T15:04:21.477171: step 4521, loss 0.130737, acc 0.96875, prec 0.0880645, recall 0.902029
2017-12-10T15:04:21.673342: step 4522, loss 0.106056, acc 0.984375, prec 0.088104, recall 0.902075
2017-12-10T15:04:21.870007: step 4523, loss 0.175113, acc 0.984375, prec 0.0881228, recall 0.902098
2017-12-10T15:04:22.065570: step 4524, loss 0.0525522, acc 0.984375, prec 0.0881415, recall 0.902121
2017-12-10T15:04:22.266574: step 4525, loss 0.252169, acc 0.953125, prec 0.088177, recall 0.902166
2017-12-10T15:04:22.460788: step 4526, loss 0.125081, acc 0.953125, prec 0.0881918, recall 0.902189
2017-12-10T15:04:22.657215: step 4527, loss 0.0905719, acc 0.96875, prec 0.0882085, recall 0.902212
2017-12-10T15:04:22.848198: step 4528, loss 0.340377, acc 0.953125, prec 0.0882025, recall 0.902212
2017-12-10T15:04:23.042606: step 4529, loss 0.67255, acc 0.984375, prec 0.088242, recall 0.902257
2017-12-10T15:04:23.241507: step 4530, loss 0.0802333, acc 0.984375, prec 0.0882607, recall 0.90228
2017-12-10T15:04:23.441049: step 4531, loss 0.0666305, acc 0.96875, prec 0.0882982, recall 0.902326
2017-12-10T15:04:23.632443: step 4532, loss 0.500135, acc 0.984375, prec 0.0883377, recall 0.902371
2017-12-10T15:04:23.833266: step 4533, loss 0.191009, acc 0.921875, prec 0.0883484, recall 0.902394
2017-12-10T15:04:24.030467: step 4534, loss 0.0116656, acc 1, prec 0.0883484, recall 0.902394
2017-12-10T15:04:24.227253: step 4535, loss 0.0947516, acc 0.96875, prec 0.0883444, recall 0.902394
2017-12-10T15:04:24.427166: step 4536, loss 0.0447295, acc 0.984375, prec 0.0883424, recall 0.902394
2017-12-10T15:04:24.623576: step 4537, loss 0.164345, acc 1, prec 0.0884461, recall 0.902507
2017-12-10T15:04:24.821732: step 4538, loss 0.171333, acc 0.96875, prec 0.0884628, recall 0.90253
2017-12-10T15:04:25.022566: step 4539, loss 0.0149152, acc 1, prec 0.0884628, recall 0.90253
2017-12-10T15:04:25.214886: step 4540, loss 0.0424961, acc 0.984375, prec 0.0884608, recall 0.90253
2017-12-10T15:04:25.407633: step 4541, loss 0.0995587, acc 0.96875, prec 0.0884775, recall 0.902552
2017-12-10T15:04:25.605381: step 4542, loss 0.0410459, acc 0.984375, prec 0.0884755, recall 0.902552
2017-12-10T15:04:25.807880: step 4543, loss 0.17572, acc 0.953125, prec 0.0884901, recall 0.902575
2017-12-10T15:04:26.003295: step 4544, loss 0.00282636, acc 1, prec 0.0885109, recall 0.902597
2017-12-10T15:04:26.200215: step 4545, loss 0.0272068, acc 1, prec 0.0885316, recall 0.90262
2017-12-10T15:04:26.399645: step 4546, loss 0.111348, acc 0.984375, prec 0.0885503, recall 0.902643
2017-12-10T15:04:26.594886: step 4547, loss 0.186654, acc 0.96875, prec 0.088567, recall 0.902665
2017-12-10T15:04:26.789609: step 4548, loss 0.143234, acc 0.984375, prec 0.0885857, recall 0.902688
2017-12-10T15:04:26.988590: step 4549, loss 0.28347, acc 0.953125, prec 0.0886004, recall 0.90271
2017-12-10T15:04:27.186761: step 4550, loss 0.0953699, acc 0.953125, prec 0.0885944, recall 0.90271
2017-12-10T15:04:27.385718: step 4551, loss 0.0705573, acc 0.984375, prec 0.0886131, recall 0.902733
2017-12-10T15:04:27.583083: step 4552, loss 0.213821, acc 0.984375, prec 0.0886732, recall 0.9028
2017-12-10T15:04:27.778587: step 4553, loss 0.214415, acc 0.9375, prec 0.0886858, recall 0.902823
2017-12-10T15:04:27.977243: step 4554, loss 0.0725081, acc 0.984375, prec 0.088746, recall 0.90289
2017-12-10T15:04:28.174946: step 4555, loss 0.018968, acc 1, prec 0.088746, recall 0.90289
2017-12-10T15:04:28.369295: step 4556, loss 0.0428452, acc 0.984375, prec 0.0887439, recall 0.90289
2017-12-10T15:04:28.570768: step 4557, loss 0.0587003, acc 0.984375, prec 0.0887626, recall 0.902913
2017-12-10T15:04:28.766313: step 4558, loss 0.166551, acc 0.96875, prec 0.0887793, recall 0.902935
2017-12-10T15:04:28.967399: step 4559, loss 0.0219073, acc 1, prec 0.0887793, recall 0.902935
2017-12-10T15:04:29.169741: step 4560, loss 0.0217126, acc 0.984375, prec 0.088798, recall 0.902957
2017-12-10T15:04:29.370816: step 4561, loss 0.0709606, acc 0.96875, prec 0.0888147, recall 0.90298
2017-12-10T15:04:29.569146: step 4562, loss 0.0476534, acc 0.984375, prec 0.0888126, recall 0.90298
2017-12-10T15:04:29.765255: step 4563, loss 0.0705857, acc 0.96875, prec 0.0888293, recall 0.903002
2017-12-10T15:04:29.960424: step 4564, loss 0.21029, acc 0.984375, prec 0.0888687, recall 0.903047
2017-12-10T15:04:30.160236: step 4565, loss 0.195947, acc 0.984375, prec 0.0889081, recall 0.903092
2017-12-10T15:04:30.356434: step 4566, loss 0.139374, acc 1, prec 0.0889288, recall 0.903114
2017-12-10T15:04:30.557893: step 4567, loss 0.167701, acc 1, prec 0.0889495, recall 0.903137
2017-12-10T15:04:30.756436: step 4568, loss 0.0391511, acc 0.984375, prec 0.0889474, recall 0.903137
2017-12-10T15:04:30.959409: step 4569, loss 0.0225103, acc 1, prec 0.0889681, recall 0.903159
2017-12-10T15:04:31.159540: step 4570, loss 0.0481015, acc 0.96875, prec 0.0889641, recall 0.903159
2017-12-10T15:04:31.355004: step 4571, loss 0.215703, acc 1, prec 0.0890469, recall 0.903248
2017-12-10T15:04:31.553198: step 4572, loss 0.0547025, acc 0.96875, prec 0.0890428, recall 0.903248
2017-12-10T15:04:31.743585: step 4573, loss 0.0799993, acc 0.984375, prec 0.0890408, recall 0.903248
2017-12-10T15:04:31.940846: step 4574, loss 0.0843437, acc 0.953125, prec 0.0890554, recall 0.90327
2017-12-10T15:04:32.134335: step 4575, loss 0.0747981, acc 0.984375, prec 0.0890741, recall 0.903293
2017-12-10T15:04:32.329688: step 4576, loss 0.189281, acc 0.953125, prec 0.0891094, recall 0.903337
2017-12-10T15:04:32.524061: step 4577, loss 0.306306, acc 0.90625, prec 0.0891179, recall 0.903359
2017-12-10T15:04:32.721118: step 4578, loss 0.149412, acc 0.953125, prec 0.0891118, recall 0.903359
2017-12-10T15:04:32.918795: step 4579, loss 0.0276079, acc 0.984375, prec 0.0891098, recall 0.903359
2017-12-10T15:04:33.111495: step 4580, loss 0.0279195, acc 1, prec 0.0891305, recall 0.903382
2017-12-10T15:04:33.305894: step 4581, loss 0.0836012, acc 0.984375, prec 0.0891698, recall 0.903426
2017-12-10T15:04:33.500945: step 4582, loss 0.121741, acc 0.96875, prec 0.0892071, recall 0.90347
2017-12-10T15:04:33.700498: step 4583, loss 0.265338, acc 0.9375, prec 0.0892403, recall 0.903515
2017-12-10T15:04:33.893377: step 4584, loss 0.0400815, acc 1, prec 0.0892817, recall 0.903559
2017-12-10T15:04:34.090414: step 4585, loss 0.230529, acc 0.984375, prec 0.089321, recall 0.903603
2017-12-10T15:04:34.288654: step 4586, loss 0.401006, acc 0.890625, prec 0.0893068, recall 0.903603
2017-12-10T15:04:34.483420: step 4587, loss 0.0688131, acc 0.96875, prec 0.0893027, recall 0.903603
2017-12-10T15:04:34.678901: step 4588, loss 0.179436, acc 1, prec 0.0893647, recall 0.90367
2017-12-10T15:04:34.877099: step 4589, loss 0.0661459, acc 0.953125, prec 0.0893999, recall 0.903714
2017-12-10T15:04:35.072887: step 4590, loss 0.386346, acc 0.984375, prec 0.0894392, recall 0.903758
2017-12-10T15:04:35.273591: step 4591, loss 0.0547374, acc 0.984375, prec 0.0894372, recall 0.903758
2017-12-10T15:04:35.471209: step 4592, loss 0.0684757, acc 0.96875, prec 0.0894538, recall 0.90378
2017-12-10T15:04:35.665584: step 4593, loss 0.0727693, acc 0.953125, prec 0.0894477, recall 0.90378
2017-12-10T15:04:35.866486: step 4594, loss 0.161939, acc 0.9375, prec 0.0894396, recall 0.90378
2017-12-10T15:04:36.061344: step 4595, loss 0.00319296, acc 1, prec 0.0894396, recall 0.90378
2017-12-10T15:04:36.257842: step 4596, loss 0.0860722, acc 0.984375, prec 0.0894788, recall 0.903824
2017-12-10T15:04:36.458664: step 4597, loss 4.41644, acc 0.984375, prec 0.0894995, recall 0.903639
2017-12-10T15:04:36.658633: step 4598, loss 0.183126, acc 0.953125, prec 0.0894934, recall 0.903639
2017-12-10T15:04:36.855783: step 4599, loss 0.108333, acc 0.984375, prec 0.089512, recall 0.903661
2017-12-10T15:04:37.053654: step 4600, loss 0.0606087, acc 0.984375, prec 0.0895306, recall 0.903683
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_0/1512935327/checkpoints/model-4600

2017-12-10T15:04:38.602093: step 4601, loss 0.423219, acc 0.875, prec 0.0895144, recall 0.903683
2017-12-10T15:04:38.793616: step 4602, loss 0.302181, acc 0.90625, prec 0.0895228, recall 0.903705
2017-12-10T15:04:38.992669: step 4603, loss 0.421908, acc 0.875, prec 0.0895272, recall 0.903727
2017-12-10T15:04:39.189305: step 4604, loss 0.451814, acc 0.828125, prec 0.0895049, recall 0.903727
2017-12-10T15:04:39.385246: step 4605, loss 0.417381, acc 0.890625, prec 0.0895113, recall 0.903749
2017-12-10T15:04:39.580690: step 4606, loss 0.249854, acc 0.9375, prec 0.0895239, recall 0.903771
2017-12-10T15:04:39.777528: step 4607, loss 0.265236, acc 0.90625, prec 0.0895323, recall 0.903793
2017-12-10T15:04:39.971992: step 4608, loss 0.347299, acc 0.8125, prec 0.089508, recall 0.903793
2017-12-10T15:04:40.166288: step 4609, loss 0.289834, acc 0.84375, prec 0.0895289, recall 0.903837
2017-12-10T15:04:40.359579: step 4610, loss 0.366308, acc 0.90625, prec 0.0895168, recall 0.903837
2017-12-10T15:04:40.554154: step 4611, loss 0.305638, acc 0.875, prec 0.0895006, recall 0.903837
2017-12-10T15:04:40.748335: step 4612, loss 0.239124, acc 0.921875, prec 0.0895317, recall 0.903881
2017-12-10T15:04:40.942553: step 4613, loss 0.212984, acc 0.984375, prec 0.0895914, recall 0.903947
2017-12-10T15:04:41.140725: step 4614, loss 0.193506, acc 0.90625, prec 0.0895998, recall 0.903969
2017-12-10T15:04:41.339308: step 4615, loss 0.310555, acc 0.890625, prec 0.0895856, recall 0.903969
2017-12-10T15:04:41.534875: step 4616, loss 0.128821, acc 0.9375, prec 0.0895775, recall 0.903969
2017-12-10T15:04:41.730617: step 4617, loss 0.118533, acc 0.953125, prec 0.089592, recall 0.903991
2017-12-10T15:04:41.924086: step 4618, loss 0.0639932, acc 0.953125, prec 0.089586, recall 0.903991
2017-12-10T15:04:42.117774: step 4619, loss 0.324106, acc 0.921875, prec 0.0895758, recall 0.903991
2017-12-10T15:04:42.311378: step 4620, loss 0.0098004, acc 1, prec 0.0895964, recall 0.904013
2017-12-10T15:04:42.500485: step 4621, loss 0.0265959, acc 0.984375, prec 0.089615, recall 0.904035
2017-12-10T15:04:42.695076: step 4622, loss 0.181172, acc 0.984375, prec 0.0896335, recall 0.904056
2017-12-10T15:04:42.891473: step 4623, loss 0.17037, acc 0.96875, prec 0.0896295, recall 0.904056
2017-12-10T15:04:43.084566: step 4624, loss 0.266513, acc 0.953125, prec 0.089644, recall 0.904078
2017-12-10T15:04:43.284242: step 4625, loss 0.0168867, acc 1, prec 0.089644, recall 0.904078
2017-12-10T15:04:43.482327: step 4626, loss 0.0852053, acc 0.96875, prec 0.0896605, recall 0.9041
2017-12-10T15:04:43.678676: step 4627, loss 1.03927, acc 0.96875, prec 0.089677, recall 0.904122
2017-12-10T15:04:43.881149: step 4628, loss 0.0213731, acc 0.984375, prec 0.089675, recall 0.904122
2017-12-10T15:04:44.078814: step 4629, loss 0.0442904, acc 0.984375, prec 0.0896935, recall 0.904144
2017-12-10T15:04:44.279802: step 4630, loss 0.0242249, acc 0.984375, prec 0.0896915, recall 0.904144
2017-12-10T15:04:44.472271: step 4631, loss 0.00749677, acc 1, prec 0.0896915, recall 0.904144
2017-12-10T15:04:44.668133: step 4632, loss 0.316254, acc 0.953125, prec 0.0897265, recall 0.904188
2017-12-10T15:04:44.867879: step 4633, loss 0.0227439, acc 1, prec 0.0897265, recall 0.904188
2017-12-10T15:04:45.061156: step 4634, loss 0.0399408, acc 0.984375, prec 0.0897245, recall 0.904188
2017-12-10T15:04:45.259527: step 4635, loss 0.210321, acc 0.9375, prec 0.0897369, recall 0.904209
2017-12-10T15:04:45.452495: step 4636, loss 1.51321, acc 0.984375, prec 0.0897369, recall 0.904004
2017-12-10T15:04:45.649634: step 4637, loss 0.121118, acc 0.9375, prec 0.0897288, recall 0.904004
2017-12-10T15:04:45.845239: step 4638, loss 0.138513, acc 0.953125, prec 0.0897433, recall 0.904025
2017-12-10T15:04:46.043186: step 4639, loss 0.170424, acc 0.953125, prec 0.0897372, recall 0.904025
2017-12-10T15:04:46.237247: step 4640, loss 0.333861, acc 0.921875, prec 0.0897476, recall 0.904047
2017-12-10T15:04:46.433440: step 4641, loss 0.154939, acc 0.921875, prec 0.0897375, recall 0.904047
2017-12-10T15:04:46.632438: step 4642, loss 0.266069, acc 0.953125, prec 0.089752, recall 0.904069
2017-12-10T15:04:46.826149: step 4643, loss 0.166194, acc 0.96875, prec 0.0897479, recall 0.904069
2017-12-10T15:04:47.019115: step 4644, loss 0.254437, acc 0.921875, prec 0.0897378, recall 0.904069
2017-12-10T15:04:47.215201: step 4645, loss 0.219017, acc 0.9375, prec 0.0897297, recall 0.904069
2017-12-10T15:04:47.408389: step 4646, loss 0.274965, acc 0.9375, prec 0.0897627, recall 0.904113
2017-12-10T15:04:47.604151: step 4647, loss 0.0767221, acc 0.953125, prec 0.0897566, recall 0.904113
2017-12-10T15:04:47.803834: step 4648, loss 0.337306, acc 0.921875, prec 0.0897875, recall 0.904156
2017-12-10T15:04:48.004283: step 4649, loss 0.244295, acc 0.921875, prec 0.0897774, recall 0.904156
2017-12-10T15:04:48.204042: step 4650, loss 0.0998508, acc 0.96875, prec 0.0897734, recall 0.904156
2017-12-10T15:04:48.400649: step 4651, loss 0.168471, acc 0.9375, prec 0.0898268, recall 0.904222
2017-12-10T15:04:48.591476: step 4652, loss 0.394232, acc 0.96875, prec 0.0898638, recall 0.904265
2017-12-10T15:04:48.790904: step 4653, loss 0.0227319, acc 1, prec 0.0899049, recall 0.904308
2017-12-10T15:04:48.992200: step 4654, loss 0.125152, acc 0.953125, prec 0.0899193, recall 0.90433
2017-12-10T15:04:49.187313: step 4655, loss 0.0104863, acc 1, prec 0.0899193, recall 0.90433
2017-12-10T15:04:49.385436: step 4656, loss 0.171008, acc 0.921875, prec 0.0899092, recall 0.90433
2017-12-10T15:04:49.582977: step 4657, loss 0.358798, acc 0.921875, prec 0.089899, recall 0.90433
2017-12-10T15:04:49.780563: step 4658, loss 0.10127, acc 0.953125, prec 0.0899135, recall 0.904352
2017-12-10T15:04:49.979940: step 4659, loss 0.346772, acc 0.96875, prec 0.0899504, recall 0.904395
2017-12-10T15:04:50.177584: step 4660, loss 0.445608, acc 0.90625, prec 0.0899383, recall 0.904395
2017-12-10T15:04:50.375658: step 4661, loss 0.0625595, acc 0.953125, prec 0.0899322, recall 0.904395
2017-12-10T15:04:50.578775: step 4662, loss 0.388773, acc 0.9375, prec 0.0899651, recall 0.904438
2017-12-10T15:04:50.778252: step 4663, loss 0.162136, acc 0.953125, prec 0.0899795, recall 0.90446
2017-12-10T15:04:50.974616: step 4664, loss 0.014406, acc 1, prec 0.0899795, recall 0.90446
2017-12-10T15:04:51.166401: step 4665, loss 0.26276, acc 0.9375, prec 0.0899919, recall 0.904482
2017-12-10T15:04:51.361247: step 4666, loss 0.0360997, acc 0.984375, prec 0.0899899, recall 0.904482
2017-12-10T15:04:51.558260: step 4667, loss 0.438646, acc 0.9375, prec 0.0899818, recall 0.904482
2017-12-10T15:04:51.790991: step 4668, loss 0.0479095, acc 0.984375, prec 0.0899797, recall 0.904482
2017-12-10T15:04:52.002037: step 4669, loss 0.195399, acc 0.9375, prec 0.0899716, recall 0.904482
2017-12-10T15:04:52.213543: step 4670, loss 0.109874, acc 0.96875, prec 0.0899881, recall 0.904503
2017-12-10T15:04:52.412446: step 4671, loss 0.132352, acc 0.953125, prec 0.0900025, recall 0.904525
2017-12-10T15:04:52.610779: step 4672, loss 0.10635, acc 0.96875, prec 0.0900189, recall 0.904546
2017-12-10T15:04:52.806102: step 4673, loss 0.0569945, acc 0.96875, prec 0.0900149, recall 0.904546
2017-12-10T15:04:53.002928: step 4674, loss 0.782171, acc 0.9375, prec 0.0900272, recall 0.904568
2017-12-10T15:04:53.200707: step 4675, loss 0.0753483, acc 0.984375, prec 0.0900252, recall 0.904568
2017-12-10T15:04:53.396678: step 4676, loss 0.0803815, acc 0.96875, prec 0.0900212, recall 0.904568
2017-12-10T15:04:53.594016: step 4677, loss 0.0351778, acc 0.984375, prec 0.0900396, recall 0.90459
2017-12-10T15:04:53.797606: step 4678, loss 0.0563859, acc 0.984375, prec 0.0900581, recall 0.904611
2017-12-10T15:04:54.013634: step 4679, loss 0.151637, acc 0.984375, prec 0.090097, recall 0.904654
2017-12-10T15:04:54.210775: step 4680, loss 0.108595, acc 0.984375, prec 0.0901359, recall 0.904697
2017-12-10T15:04:54.406192: step 4681, loss 0.740426, acc 0.984375, prec 0.0901543, recall 0.904719
2017-12-10T15:04:54.602241: step 4682, loss 0.0266608, acc 1, prec 0.0901953, recall 0.904762
2017-12-10T15:04:54.798768: step 4683, loss 0.0715837, acc 0.96875, prec 0.0901912, recall 0.904762
2017-12-10T15:04:54.993468: step 4684, loss 0.251749, acc 0.9375, prec 0.090224, recall 0.904805
2017-12-10T15:04:55.188592: step 4685, loss 0.00320619, acc 1, prec 0.090224, recall 0.904805
2017-12-10T15:04:55.382554: step 4686, loss 0.708315, acc 0.984375, prec 0.0902425, recall 0.904826
2017-12-10T15:04:55.580028: step 4687, loss 0.564606, acc 0.984375, prec 0.0902814, recall 0.904869
2017-12-10T15:04:55.776947: step 4688, loss 0.242593, acc 0.984375, prec 0.0903203, recall 0.904912
2017-12-10T15:04:55.972949: step 4689, loss 0.150549, acc 0.96875, prec 0.0903571, recall 0.904955
2017-12-10T15:04:56.170346: step 4690, loss 0.106835, acc 0.984375, prec 0.090396, recall 0.904998
2017-12-10T15:04:56.366813: step 4691, loss 0.253178, acc 0.9375, prec 0.0904083, recall 0.905019
2017-12-10T15:04:56.560594: step 4692, loss 0.258258, acc 0.9375, prec 0.0904206, recall 0.905041
2017-12-10T15:04:56.753713: step 4693, loss 0.130831, acc 0.953125, prec 0.0904554, recall 0.905083
2017-12-10T15:04:56.945916: step 4694, loss 0.200424, acc 0.9375, prec 0.0904473, recall 0.905083
2017-12-10T15:04:57.145620: step 4695, loss 0.135345, acc 0.96875, prec 0.0904841, recall 0.905126
2017-12-10T15:04:57.345851: step 4696, loss 0.0536971, acc 0.96875, prec 0.0905005, recall 0.905147
2017-12-10T15:04:57.542201: step 4697, loss 0.122911, acc 0.96875, prec 0.0905373, recall 0.90519
2017-12-10T15:04:57.738929: step 4698, loss 0.146273, acc 0.953125, prec 0.0905312, recall 0.90519
2017-12-10T15:04:57.940219: step 4699, loss 0.370674, acc 0.90625, prec 0.0905394, recall 0.905211
2017-12-10T15:04:58.132786: step 4700, loss 0.137066, acc 0.90625, prec 0.0905272, recall 0.905211
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_0/1512935327/checkpoints/model-4700

2017-12-10T15:04:59.506546: step 4701, loss 0.258979, acc 0.90625, prec 0.090515, recall 0.905211
2017-12-10T15:04:59.702366: step 4702, loss 0.109963, acc 0.9375, prec 0.0905273, recall 0.905232
2017-12-10T15:04:59.898339: step 4703, loss 1.84621, acc 0.90625, prec 0.0905171, recall 0.905029
2017-12-10T15:05:00.101589: step 4704, loss 0.22343, acc 0.9375, prec 0.0905294, recall 0.905051
2017-12-10T15:05:00.294039: step 4705, loss 0.417282, acc 0.84375, prec 0.0905295, recall 0.905072
2017-12-10T15:05:00.488476: step 4706, loss 0.216194, acc 0.921875, prec 0.0905398, recall 0.905093
2017-12-10T15:05:00.681806: step 4707, loss 0.35357, acc 0.90625, prec 0.090548, recall 0.905114
2017-12-10T15:05:00.878697: step 4708, loss 0.158446, acc 0.921875, prec 0.0905991, recall 0.905178
2017-12-10T15:05:01.079059: step 4709, loss 0.41445, acc 0.953125, prec 0.0906338, recall 0.905221
2017-12-10T15:05:01.283983: step 4710, loss 0.238588, acc 0.90625, prec 0.090642, recall 0.905242
2017-12-10T15:05:01.482378: step 4711, loss 0.15125, acc 0.921875, prec 0.0906318, recall 0.905242
2017-12-10T15:05:01.682452: step 4712, loss 0.246726, acc 0.90625, prec 0.0906604, recall 0.905284
2017-12-10T15:05:01.880084: step 4713, loss 0.262524, acc 0.921875, prec 0.0906706, recall 0.905306
2017-12-10T15:05:02.076570: step 4714, loss 0.0890014, acc 0.96875, prec 0.0906869, recall 0.905327
2017-12-10T15:05:02.271056: step 4715, loss 0.288283, acc 0.953125, prec 0.090742, recall 0.90539
2017-12-10T15:05:02.464610: step 4716, loss 0.222884, acc 0.921875, prec 0.0907522, recall 0.905411
2017-12-10T15:05:02.660093: step 4717, loss 0.245387, acc 0.90625, prec 0.09074, recall 0.905411
2017-12-10T15:05:02.853622: step 4718, loss 0.111237, acc 0.96875, prec 0.0908174, recall 0.905496
2017-12-10T15:05:03.050069: step 4719, loss 0.429914, acc 0.9375, prec 0.0908093, recall 0.905496
2017-12-10T15:05:03.249357: step 4720, loss 0.110378, acc 0.96875, prec 0.0908663, recall 0.905559
2017-12-10T15:05:03.442914: step 4721, loss 0.600865, acc 0.9375, prec 0.0908582, recall 0.905559
2017-12-10T15:05:03.637133: step 4722, loss 0.140296, acc 0.953125, prec 0.0908521, recall 0.905559
2017-12-10T15:05:03.833558: step 4723, loss 0.0439259, acc 0.984375, prec 0.09085, recall 0.905559
2017-12-10T15:05:04.028195: step 4724, loss 0.0190661, acc 1, prec 0.09085, recall 0.905559
2017-12-10T15:05:04.223986: step 4725, loss 0.371415, acc 0.9375, prec 0.0908623, recall 0.90558
2017-12-10T15:05:04.422499: step 4726, loss 0.281679, acc 0.96875, prec 0.0908582, recall 0.90558
2017-12-10T15:05:04.617500: step 4727, loss 0.0957479, acc 0.96875, prec 0.0908541, recall 0.90558
2017-12-10T15:05:04.812637: step 4728, loss 0.00457286, acc 1, prec 0.0908541, recall 0.90558
2017-12-10T15:05:05.004525: step 4729, loss 0.0822579, acc 0.984375, prec 0.0908724, recall 0.905601
2017-12-10T15:05:05.201587: step 4730, loss 0.0358915, acc 0.984375, prec 0.0908908, recall 0.905622
2017-12-10T15:05:05.397816: step 4731, loss 0.0609968, acc 0.984375, prec 0.0908887, recall 0.905622
2017-12-10T15:05:05.599831: step 4732, loss 0.197186, acc 0.984375, prec 0.0909071, recall 0.905644
2017-12-10T15:05:05.805530: step 4733, loss 0.113544, acc 0.96875, prec 0.0909233, recall 0.905665
2017-12-10T15:05:06.003301: step 4734, loss 0.21422, acc 0.953125, prec 0.0909172, recall 0.905665
2017-12-10T15:05:06.202705: step 4735, loss 0.042087, acc 0.984375, prec 0.0909152, recall 0.905665
2017-12-10T15:05:06.401444: step 4736, loss 0.00463981, acc 1, prec 0.0909152, recall 0.905665
2017-12-10T15:05:06.600608: step 4737, loss 0.0649329, acc 0.96875, prec 0.0909518, recall 0.905707
2017-12-10T15:05:06.798660: step 4738, loss 0.021027, acc 0.984375, prec 0.0909701, recall 0.905728
2017-12-10T15:05:06.998867: step 4739, loss 0.00470087, acc 1, prec 0.0909701, recall 0.905728
2017-12-10T15:05:07.198279: step 4740, loss 0.139053, acc 0.984375, prec 0.0909884, recall 0.905749
2017-12-10T15:05:07.401713: step 4741, loss 0.410111, acc 1, prec 0.0910495, recall 0.905812
2017-12-10T15:05:07.602450: step 4742, loss 0.25028, acc 0.9375, prec 0.0910413, recall 0.905812
2017-12-10T15:05:07.798830: step 4743, loss 0.076527, acc 0.984375, prec 0.09108, recall 0.905854
2017-12-10T15:05:07.994309: step 4744, loss 0.109559, acc 0.984375, prec 0.0910983, recall 0.905874
2017-12-10T15:05:08.188599: step 4745, loss 0.0563054, acc 0.953125, prec 0.0910922, recall 0.905874
2017-12-10T15:05:08.389321: step 4746, loss 0.00709326, acc 1, prec 0.0910922, recall 0.905874
2017-12-10T15:05:08.582382: step 4747, loss 0.161457, acc 0.953125, prec 0.0911267, recall 0.905916
2017-12-10T15:05:08.782169: step 4748, loss 0.00190891, acc 1, prec 0.0911267, recall 0.905916
2017-12-10T15:05:08.975500: step 4749, loss 0.207206, acc 0.953125, prec 0.0911206, recall 0.905916
2017-12-10T15:05:09.171442: step 4750, loss 0.669666, acc 0.984375, prec 0.0911389, recall 0.905937
2017-12-10T15:05:09.374922: step 4751, loss 0.110356, acc 0.984375, prec 0.0911572, recall 0.905958
2017-12-10T15:05:09.571631: step 4752, loss 0.169933, acc 0.984375, prec 0.0911755, recall 0.905979
2017-12-10T15:05:09.769843: step 4753, loss 1.56913, acc 0.984375, prec 0.0911958, recall 0.905799
2017-12-10T15:05:09.972448: step 4754, loss 0.168605, acc 0.984375, prec 0.0912141, recall 0.90582
2017-12-10T15:05:10.168442: step 4755, loss 0.112472, acc 0.96875, prec 0.09121, recall 0.90582
2017-12-10T15:05:10.366615: step 4756, loss 0.130394, acc 0.9375, prec 0.0912019, recall 0.90582
2017-12-10T15:05:10.560508: step 4757, loss 0.153231, acc 0.96875, prec 0.0911978, recall 0.90582
2017-12-10T15:05:10.757490: step 4758, loss 0.0774035, acc 0.953125, prec 0.091212, recall 0.905841
2017-12-10T15:05:10.953106: step 4759, loss 0.278755, acc 0.890625, prec 0.0912383, recall 0.905882
2017-12-10T15:05:11.149122: step 4760, loss 0.505484, acc 0.890625, prec 0.0912241, recall 0.905882
2017-12-10T15:05:11.346930: step 4761, loss 0.254022, acc 0.9375, prec 0.0912362, recall 0.905903
2017-12-10T15:05:11.541435: step 4762, loss 0.258021, acc 0.90625, prec 0.091224, recall 0.905903
2017-12-10T15:05:11.739001: step 4763, loss 0.522311, acc 0.890625, prec 0.09123, recall 0.905924
2017-12-10T15:05:11.937381: step 4764, loss 0.264094, acc 0.953125, prec 0.0912442, recall 0.905945
2017-12-10T15:05:12.136420: step 4765, loss 0.223717, acc 0.921875, prec 0.0912949, recall 0.906008
2017-12-10T15:05:12.328344: step 4766, loss 0.180533, acc 0.96875, prec 0.0913314, recall 0.906049
2017-12-10T15:05:12.524413: step 4767, loss 0.75977, acc 0.875, prec 0.091376, recall 0.906112
2017-12-10T15:05:12.725084: step 4768, loss 0.23151, acc 0.953125, prec 0.0913902, recall 0.906132
2017-12-10T15:05:12.921801: step 4769, loss 0.535924, acc 0.890625, prec 0.0913759, recall 0.906132
2017-12-10T15:05:13.117518: step 4770, loss 0.210451, acc 0.953125, prec 0.0913901, recall 0.906153
2017-12-10T15:05:13.311844: step 4771, loss 0.253948, acc 0.9375, prec 0.0914022, recall 0.906174
2017-12-10T15:05:13.511641: step 4772, loss 0.224529, acc 0.921875, prec 0.091392, recall 0.906174
2017-12-10T15:05:13.707078: step 4773, loss 0.131296, acc 0.953125, prec 0.0914061, recall 0.906195
2017-12-10T15:05:13.900681: step 4774, loss 0.180528, acc 0.953125, prec 0.0914, recall 0.906195
2017-12-10T15:05:14.098006: step 4775, loss 0.141149, acc 0.984375, prec 0.0914182, recall 0.906215
2017-12-10T15:05:14.303360: step 4776, loss 0.211974, acc 0.9375, prec 0.0914709, recall 0.906278
2017-12-10T15:05:14.505176: step 4777, loss 0.0373577, acc 0.984375, prec 0.0915094, recall 0.906319
2017-12-10T15:05:14.702069: step 4778, loss 0.0214382, acc 1, prec 0.0915094, recall 0.906319
2017-12-10T15:05:14.901805: step 4779, loss 0.432214, acc 0.953125, prec 0.0915438, recall 0.90636
2017-12-10T15:05:15.097328: step 4780, loss 0.067494, acc 0.96875, prec 0.09156, recall 0.906381
2017-12-10T15:05:15.292760: step 4781, loss 0.125846, acc 0.96875, prec 0.0915559, recall 0.906381
2017-12-10T15:05:15.489543: step 4782, loss 0.0630681, acc 0.96875, prec 0.0915721, recall 0.906402
2017-12-10T15:05:15.686084: step 4783, loss 0.238647, acc 0.953125, prec 0.0915862, recall 0.906422
2017-12-10T15:05:15.883403: step 4784, loss 0.381406, acc 0.953125, prec 0.0916003, recall 0.906443
2017-12-10T15:05:16.080167: step 4785, loss 0.179561, acc 0.96875, prec 0.0915962, recall 0.906443
2017-12-10T15:05:16.280586: step 4786, loss 0.11883, acc 0.953125, prec 0.0916104, recall 0.906464
2017-12-10T15:05:16.477375: step 4787, loss 0.343237, acc 0.9375, prec 0.0916224, recall 0.906484
2017-12-10T15:05:16.673116: step 4788, loss 0.202185, acc 0.953125, prec 0.0916366, recall 0.906505
2017-12-10T15:05:16.870737: step 4789, loss 0.0972568, acc 1, prec 0.0916568, recall 0.906526
2017-12-10T15:05:17.066796: step 4790, loss 0.04638, acc 0.96875, prec 0.091673, recall 0.906546
2017-12-10T15:05:17.265800: step 4791, loss 0.0134617, acc 1, prec 0.0916932, recall 0.906567
2017-12-10T15:05:17.463365: step 4792, loss 0.0709296, acc 0.96875, prec 0.0916891, recall 0.906567
2017-12-10T15:05:17.660403: step 4793, loss 0.0826388, acc 0.96875, prec 0.0917053, recall 0.906587
2017-12-10T15:05:17.855000: step 4794, loss 0.0230757, acc 1, prec 0.0917863, recall 0.90667
2017-12-10T15:05:18.054216: step 4795, loss 0.088232, acc 0.984375, prec 0.0918044, recall 0.90669
2017-12-10T15:05:18.252092: step 4796, loss 0.108625, acc 0.96875, prec 0.0918206, recall 0.906711
2017-12-10T15:05:18.446776: step 4797, loss 1.73248, acc 0.984375, prec 0.0918206, recall 0.906511
2017-12-10T15:05:18.645111: step 4798, loss 0.141754, acc 0.9375, prec 0.0918326, recall 0.906532
2017-12-10T15:05:18.837948: step 4799, loss 0.0288091, acc 1, prec 0.0918731, recall 0.906573
2017-12-10T15:05:19.033009: step 4800, loss 0.13605, acc 0.921875, prec 0.0919033, recall 0.906614
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_0/1512935327/checkpoints/model-4800

2017-12-10T15:05:20.857022: step 4801, loss 0.0821019, acc 0.953125, prec 0.0919174, recall 0.906634
2017-12-10T15:05:21.054187: step 4802, loss 0.0329155, acc 0.984375, prec 0.0919154, recall 0.906634
2017-12-10T15:05:21.247333: step 4803, loss 0.108203, acc 0.96875, prec 0.0919315, recall 0.906655
2017-12-10T15:05:21.441100: step 4804, loss 0.0972398, acc 0.96875, prec 0.0919274, recall 0.906655
2017-12-10T15:05:21.637280: step 4805, loss 0.0402125, acc 0.984375, prec 0.0919254, recall 0.906655
2017-12-10T15:05:21.829450: step 4806, loss 0.130361, acc 0.9375, prec 0.0919172, recall 0.906655
2017-12-10T15:05:22.023982: step 4807, loss 0.0846479, acc 0.953125, prec 0.0919312, recall 0.906675
2017-12-10T15:05:22.218945: step 4808, loss 0.319384, acc 0.953125, prec 0.0919251, recall 0.906675
2017-12-10T15:05:22.415023: step 4809, loss 0.019092, acc 1, prec 0.0919655, recall 0.906716
2017-12-10T15:05:22.609906: step 4810, loss 0.0842575, acc 0.96875, prec 0.0919614, recall 0.906716
2017-12-10T15:05:22.802895: step 4811, loss 0.4202, acc 0.90625, prec 0.0919694, recall 0.906737
2017-12-10T15:05:22.995727: step 4812, loss 0.437876, acc 0.890625, prec 0.091955, recall 0.906737
2017-12-10T15:05:23.193251: step 4813, loss 0.212167, acc 0.9375, prec 0.0919469, recall 0.906737
2017-12-10T15:05:23.393422: step 4814, loss 0.501565, acc 0.921875, prec 0.0919568, recall 0.906757
2017-12-10T15:05:23.591185: step 4815, loss 0.192299, acc 0.96875, prec 0.0919729, recall 0.906778
2017-12-10T15:05:23.793383: step 4816, loss 0.0637756, acc 0.984375, prec 0.0919911, recall 0.906798
2017-12-10T15:05:23.988791: step 4817, loss 0.162562, acc 0.953125, prec 0.091985, recall 0.906798
2017-12-10T15:05:24.190195: step 4818, loss 0.113514, acc 0.96875, prec 0.0920011, recall 0.906819
2017-12-10T15:05:24.384494: step 4819, loss 0.0434938, acc 0.96875, prec 0.0920374, recall 0.90686
2017-12-10T15:05:24.579769: step 4820, loss 0.040553, acc 0.96875, prec 0.0920333, recall 0.90686
2017-12-10T15:05:24.776603: step 4821, loss 0.360106, acc 0.96875, prec 0.0920292, recall 0.90686
2017-12-10T15:05:24.978519: step 4822, loss 0.00952832, acc 1, prec 0.0920292, recall 0.90686
2017-12-10T15:05:25.180088: step 4823, loss 0.250322, acc 0.984375, prec 0.0920675, recall 0.9069
2017-12-10T15:05:25.377698: step 4824, loss 0.0120513, acc 1, prec 0.0920675, recall 0.9069
2017-12-10T15:05:25.572703: step 4825, loss 0.111038, acc 0.96875, prec 0.0920836, recall 0.906921
2017-12-10T15:05:25.772790: step 4826, loss 0.0112315, acc 1, prec 0.092124, recall 0.906962
2017-12-10T15:05:25.970015: step 4827, loss 0.152301, acc 0.9375, prec 0.092136, recall 0.906982
2017-12-10T15:05:26.163399: step 4828, loss 0.0504253, acc 0.96875, prec 0.0921319, recall 0.906982
2017-12-10T15:05:26.364189: step 4829, loss 0.041179, acc 0.984375, prec 0.0921298, recall 0.906982
2017-12-10T15:05:26.564422: step 4830, loss 0.395392, acc 0.984375, prec 0.0921682, recall 0.907023
2017-12-10T15:05:26.759951: step 4831, loss 0.0320758, acc 0.984375, prec 0.0921863, recall 0.907043
2017-12-10T15:05:26.954516: step 4832, loss 0.231383, acc 1, prec 0.0922266, recall 0.907084
2017-12-10T15:05:27.156226: step 4833, loss 0.177296, acc 0.9375, prec 0.0922184, recall 0.907084
2017-12-10T15:05:27.355112: step 4834, loss 0.1606, acc 0.953125, prec 0.0922325, recall 0.907104
2017-12-10T15:05:27.554789: step 4835, loss 0.0314994, acc 0.984375, prec 0.0922304, recall 0.907104
2017-12-10T15:05:27.753957: step 4836, loss 0.144083, acc 0.953125, prec 0.0922444, recall 0.907124
2017-12-10T15:05:27.953733: step 4837, loss 0.153941, acc 0.96875, prec 0.0922605, recall 0.907144
2017-12-10T15:05:28.154245: step 4838, loss 0.184707, acc 0.984375, prec 0.0922786, recall 0.907165
2017-12-10T15:05:28.353273: step 4839, loss 0.0547261, acc 0.984375, prec 0.0923169, recall 0.907205
2017-12-10T15:05:28.552115: step 4840, loss 0.0136447, acc 1, prec 0.0923371, recall 0.907225
2017-12-10T15:05:28.749142: step 4841, loss 0.0285847, acc 0.984375, prec 0.0923552, recall 0.907246
2017-12-10T15:05:28.943914: step 4842, loss 0.0219929, acc 1, prec 0.0924157, recall 0.907306
2017-12-10T15:05:29.143927: step 4843, loss 0.126208, acc 0.953125, prec 0.0924297, recall 0.907327
2017-12-10T15:05:29.346844: step 4844, loss 0.0622545, acc 1, prec 0.0924499, recall 0.907347
2017-12-10T15:05:29.546183: step 4845, loss 0.025677, acc 0.984375, prec 0.0924478, recall 0.907347
2017-12-10T15:05:29.739497: step 4846, loss 0.2475, acc 0.9375, prec 0.0924396, recall 0.907347
2017-12-10T15:05:29.942101: step 4847, loss 0.00343563, acc 1, prec 0.0924597, recall 0.907367
2017-12-10T15:05:30.137558: step 4848, loss 0.225425, acc 0.96875, prec 0.0924758, recall 0.907387
2017-12-10T15:05:30.331041: step 4849, loss 0.0333782, acc 1, prec 0.0925161, recall 0.907428
2017-12-10T15:05:30.529337: step 4850, loss 0.242855, acc 0.984375, prec 0.0925543, recall 0.907468
2017-12-10T15:05:30.727311: step 4851, loss 0.0970969, acc 0.96875, prec 0.0925704, recall 0.907488
2017-12-10T15:05:30.920314: step 4852, loss 0.0139777, acc 1, prec 0.0925905, recall 0.907508
2017-12-10T15:05:31.119070: step 4853, loss 0.00968437, acc 1, prec 0.0925905, recall 0.907508
2017-12-10T15:05:31.314673: step 4854, loss 0.0804053, acc 0.96875, prec 0.0925864, recall 0.907508
2017-12-10T15:05:31.517783: step 4855, loss 0.029191, acc 0.984375, prec 0.0926045, recall 0.907528
2017-12-10T15:05:31.716508: step 4856, loss 0.0336138, acc 0.984375, prec 0.0926025, recall 0.907528
2017-12-10T15:05:31.913313: step 4857, loss 0.0730608, acc 0.984375, prec 0.0926206, recall 0.907548
2017-12-10T15:05:32.115405: step 4858, loss 0.440122, acc 0.984375, prec 0.0926386, recall 0.907569
2017-12-10T15:05:32.314703: step 4859, loss 0.215639, acc 0.953125, prec 0.0926526, recall 0.907589
2017-12-10T15:05:32.513031: step 4860, loss 0.049531, acc 0.984375, prec 0.0926707, recall 0.907609
2017-12-10T15:05:32.708168: step 4861, loss 0.0990177, acc 0.984375, prec 0.0927089, recall 0.907649
2017-12-10T15:05:32.907813: step 4862, loss 0.0461829, acc 0.984375, prec 0.0927471, recall 0.907689
2017-12-10T15:05:33.108037: step 4863, loss 0.303816, acc 0.9375, prec 0.0927792, recall 0.907729
2017-12-10T15:05:33.307542: step 4864, loss 0.314376, acc 0.953125, prec 0.0927931, recall 0.907749
2017-12-10T15:05:33.503123: step 4865, loss 0.178942, acc 0.984375, prec 0.0928313, recall 0.907789
2017-12-10T15:05:33.709652: step 4866, loss 0.029322, acc 0.984375, prec 0.0928292, recall 0.907789
2017-12-10T15:05:33.905410: step 4867, loss 0.296929, acc 0.9375, prec 0.092821, recall 0.907789
2017-12-10T15:05:34.101896: step 4868, loss 0.0721501, acc 0.96875, prec 0.092837, recall 0.907809
2017-12-10T15:05:34.300539: step 4869, loss 0.0148783, acc 1, prec 0.0928571, recall 0.907829
2017-12-10T15:05:34.494573: step 4870, loss 0.0453292, acc 0.96875, prec 0.0928731, recall 0.907849
2017-12-10T15:05:34.688369: step 4871, loss 0.128387, acc 0.96875, prec 0.092869, recall 0.907849
2017-12-10T15:05:34.887843: step 4872, loss 0.151946, acc 0.9375, prec 0.092901, recall 0.907889
2017-12-10T15:05:35.086266: step 4873, loss 0.0479444, acc 0.96875, prec 0.0928969, recall 0.907889
2017-12-10T15:05:35.283500: step 4874, loss 0.0427746, acc 0.984375, prec 0.0929351, recall 0.907929
2017-12-10T15:05:35.481279: step 4875, loss 0.249242, acc 0.953125, prec 0.0929289, recall 0.907929
2017-12-10T15:05:35.678258: step 4876, loss 0.285029, acc 0.984375, prec 0.0929469, recall 0.907949
2017-12-10T15:05:35.874135: step 4877, loss 0.274981, acc 0.96875, prec 0.0929428, recall 0.907949
2017-12-10T15:05:36.070442: step 4878, loss 0.0500758, acc 0.984375, prec 0.0929609, recall 0.907969
2017-12-10T15:05:36.269322: step 4879, loss 0.061496, acc 0.984375, prec 0.0929789, recall 0.907989
2017-12-10T15:05:36.463396: step 4880, loss 0.139163, acc 0.984375, prec 0.092997, recall 0.908009
2017-12-10T15:05:36.662197: step 4881, loss 0.0768996, acc 0.96875, prec 0.0929928, recall 0.908009
2017-12-10T15:05:36.858851: step 4882, loss 0.300799, acc 0.96875, prec 0.0930289, recall 0.908048
2017-12-10T15:05:37.058073: step 4883, loss 0.0419832, acc 0.96875, prec 0.0930248, recall 0.908048
2017-12-10T15:05:37.252023: step 4884, loss 0.00312459, acc 1, prec 0.0930449, recall 0.908068
2017-12-10T15:05:37.449824: step 4885, loss 0.338192, acc 1, prec 0.0931052, recall 0.908128
2017-12-10T15:05:37.651251: step 4886, loss 0.125711, acc 0.96875, prec 0.0931011, recall 0.908128
2017-12-10T15:05:37.849454: step 4887, loss 0.248466, acc 0.953125, prec 0.093115, recall 0.908148
2017-12-10T15:05:38.052153: step 4888, loss 0.133261, acc 0.96875, prec 0.0931109, recall 0.908148
2017-12-10T15:05:38.247394: step 4889, loss 0.163944, acc 0.96875, prec 0.0931067, recall 0.908148
2017-12-10T15:05:38.443652: step 4890, loss 0.00883204, acc 1, prec 0.0931067, recall 0.908148
2017-12-10T15:05:38.637791: step 4891, loss 0.012833, acc 1, prec 0.0931268, recall 0.908168
2017-12-10T15:05:38.831961: step 4892, loss 0.41305, acc 0.984375, prec 0.0931449, recall 0.908188
2017-12-10T15:05:39.027635: step 4893, loss 0.064839, acc 0.984375, prec 0.0931428, recall 0.908188
2017-12-10T15:05:39.224700: step 4894, loss 0.0331525, acc 1, prec 0.0931629, recall 0.908207
2017-12-10T15:05:39.419880: step 4895, loss 0.0125773, acc 1, prec 0.0931629, recall 0.908207
2017-12-10T15:05:39.616783: step 4896, loss 0.36076, acc 0.984375, prec 0.0932211, recall 0.908267
2017-12-10T15:05:39.812632: step 4897, loss 0.0495967, acc 0.984375, prec 0.0932391, recall 0.908287
2017-12-10T15:05:40.004711: step 4898, loss 0.249304, acc 0.96875, prec 0.0932752, recall 0.908326
2017-12-10T15:05:40.203028: step 4899, loss 0.0738734, acc 0.984375, prec 0.0932932, recall 0.908346
2017-12-10T15:05:40.407669: step 4900, loss 0.0351722, acc 0.984375, prec 0.0932911, recall 0.908346
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_0/1512935327/checkpoints/model-4900

2017-12-10T15:05:41.629942: step 4901, loss 0.0312559, acc 0.984375, prec 0.0933091, recall 0.908366
2017-12-10T15:05:41.825127: step 4902, loss 0.015305, acc 0.984375, prec 0.093307, recall 0.908366
2017-12-10T15:05:42.024655: step 4903, loss 0.0559029, acc 0.984375, prec 0.0933251, recall 0.908385
2017-12-10T15:05:42.217803: step 4904, loss 0.278582, acc 0.96875, prec 0.0933812, recall 0.908445
2017-12-10T15:05:42.415987: step 4905, loss 0.34629, acc 0.921875, prec 0.0933909, recall 0.908464
2017-12-10T15:05:42.610385: step 4906, loss 0.0536767, acc 0.984375, prec 0.0933888, recall 0.908464
2017-12-10T15:05:42.810642: step 4907, loss 0.155349, acc 0.9375, prec 0.0933806, recall 0.908464
2017-12-10T15:05:43.004602: step 4908, loss 0.10596, acc 0.953125, prec 0.0933944, recall 0.908484
2017-12-10T15:05:43.199395: step 4909, loss 0.111746, acc 0.953125, prec 0.0934083, recall 0.908504
2017-12-10T15:05:43.395825: step 4910, loss 0.117124, acc 0.984375, prec 0.0934263, recall 0.908523
2017-12-10T15:05:43.596363: step 4911, loss 0.109664, acc 0.984375, prec 0.0935446, recall 0.908641
2017-12-10T15:05:43.791565: step 4912, loss 0.356134, acc 0.953125, prec 0.0935585, recall 0.908661
2017-12-10T15:05:43.992640: step 4913, loss 0.0928262, acc 0.96875, prec 0.0935543, recall 0.908661
2017-12-10T15:05:44.190343: step 4914, loss 0.253421, acc 0.9375, prec 0.093546, recall 0.908661
2017-12-10T15:05:44.387016: step 4915, loss 0.0842591, acc 0.96875, prec 0.093582, recall 0.9087
2017-12-10T15:05:44.584968: step 4916, loss 0.110382, acc 0.9375, prec 0.0935737, recall 0.9087
2017-12-10T15:05:44.787071: step 4917, loss 0.140482, acc 0.9375, prec 0.0935654, recall 0.9087
2017-12-10T15:05:44.988419: step 4918, loss 0.0928906, acc 0.984375, prec 0.0935634, recall 0.9087
2017-12-10T15:05:45.185215: step 4919, loss 0.0300145, acc 0.984375, prec 0.0935613, recall 0.9087
2017-12-10T15:05:45.383857: step 4920, loss 0.192539, acc 1, prec 0.0935813, recall 0.90872
2017-12-10T15:05:45.585088: step 4921, loss 0.0161179, acc 1, prec 0.0936014, recall 0.90874
2017-12-10T15:05:45.782608: step 4922, loss 0.23724, acc 0.953125, prec 0.0935952, recall 0.90874
2017-12-10T15:05:45.982029: step 4923, loss 0.0326615, acc 1, prec 0.0936353, recall 0.908779
2017-12-10T15:05:46.178132: step 4924, loss 0.126146, acc 0.96875, prec 0.0936512, recall 0.908798
2017-12-10T15:05:46.373315: step 4925, loss 0.056333, acc 0.984375, prec 0.0936892, recall 0.908837
2017-12-10T15:05:46.567726: step 4926, loss 0.0482763, acc 1, prec 0.0937092, recall 0.908857
2017-12-10T15:05:46.762832: step 4927, loss 0.670995, acc 0.984375, prec 0.0937472, recall 0.908896
2017-12-10T15:05:46.962385: step 4928, loss 0.176466, acc 0.9375, prec 0.0937389, recall 0.908896
2017-12-10T15:05:47.174582: step 4929, loss 0.128112, acc 0.9375, prec 0.0937307, recall 0.908896
2017-12-10T15:05:47.370761: step 4930, loss 0.112367, acc 0.953125, prec 0.0937445, recall 0.908916
2017-12-10T15:05:47.567224: step 4931, loss 0.0941396, acc 0.96875, prec 0.0937804, recall 0.908955
2017-12-10T15:05:47.764351: step 4932, loss 0.00924612, acc 1, prec 0.0938204, recall 0.908994
2017-12-10T15:05:47.956514: step 4933, loss 0.201867, acc 0.953125, prec 0.0938343, recall 0.909013
2017-12-10T15:05:48.156722: step 4934, loss 0.309121, acc 0.953125, prec 0.0938681, recall 0.909052
2017-12-10T15:05:48.350967: step 4935, loss 0.193716, acc 0.90625, prec 0.0938757, recall 0.909071
2017-12-10T15:05:48.546871: step 4936, loss 0.114655, acc 0.984375, prec 0.0938736, recall 0.909071
2017-12-10T15:05:48.741406: step 4937, loss 0.0637272, acc 0.984375, prec 0.0938915, recall 0.909091
2017-12-10T15:05:48.935724: step 4938, loss 0.269744, acc 0.953125, prec 0.0938853, recall 0.909091
2017-12-10T15:05:49.131459: step 4939, loss 0.0477773, acc 0.96875, prec 0.0938812, recall 0.909091
2017-12-10T15:05:49.330195: step 4940, loss 0.225121, acc 0.953125, prec 0.0938749, recall 0.909091
2017-12-10T15:05:49.527307: step 4941, loss 0.114548, acc 0.9375, prec 0.0939067, recall 0.90913
2017-12-10T15:05:49.722104: step 4942, loss 0.0890286, acc 0.984375, prec 0.0939446, recall 0.909169
2017-12-10T15:05:49.921463: step 4943, loss 0.00736192, acc 1, prec 0.0939646, recall 0.909188
2017-12-10T15:05:50.120304: step 4944, loss 0.699632, acc 0.984375, prec 0.0939826, recall 0.909207
2017-12-10T15:05:50.323185: step 4945, loss 1.11309, acc 0.984375, prec 0.0940205, recall 0.909246
2017-12-10T15:05:50.521563: step 4946, loss 0.433172, acc 0.96875, prec 0.0940764, recall 0.909304
2017-12-10T15:05:50.715500: step 4947, loss 0.230558, acc 0.953125, prec 0.0940701, recall 0.909304
2017-12-10T15:05:50.911747: step 4948, loss 0.106298, acc 0.96875, prec 0.094086, recall 0.909324
2017-12-10T15:05:51.107649: step 4949, loss 0.198698, acc 0.96875, prec 0.0941018, recall 0.909343
2017-12-10T15:05:51.304541: step 4950, loss 0.259144, acc 0.9375, prec 0.0941135, recall 0.909362
2017-12-10T15:05:51.503146: step 4951, loss 0.087865, acc 0.984375, prec 0.0941314, recall 0.909382
2017-12-10T15:05:51.696755: step 4952, loss 0.105219, acc 0.953125, prec 0.0941452, recall 0.909401
2017-12-10T15:05:51.892231: step 4953, loss 0.300894, acc 0.90625, prec 0.0941327, recall 0.909401
2017-12-10T15:05:52.087385: step 4954, loss 0.115316, acc 0.96875, prec 0.0941885, recall 0.909459
2017-12-10T15:05:52.282145: step 4955, loss 0.162142, acc 0.96875, prec 0.0941844, recall 0.909459
2017-12-10T15:05:52.483423: step 4956, loss 0.0663533, acc 0.984375, prec 0.0942222, recall 0.909497
2017-12-10T15:05:52.675878: step 4957, loss 0.241703, acc 0.90625, prec 0.0942098, recall 0.909497
2017-12-10T15:05:52.870169: step 4958, loss 0.155053, acc 0.921875, prec 0.0941994, recall 0.909497
2017-12-10T15:05:53.068092: step 4959, loss 0.0400748, acc 1, prec 0.0942194, recall 0.909517
2017-12-10T15:05:53.270302: step 4960, loss 0.0185126, acc 1, prec 0.0942593, recall 0.909555
2017-12-10T15:05:53.464257: step 4961, loss 0.042742, acc 0.984375, prec 0.0942572, recall 0.909555
2017-12-10T15:05:53.660049: step 4962, loss 0.122669, acc 0.953125, prec 0.094251, recall 0.909555
2017-12-10T15:05:53.862440: step 4963, loss 0.242105, acc 0.953125, prec 0.0942448, recall 0.909555
2017-12-10T15:05:54.060671: step 4964, loss 0.0367158, acc 0.984375, prec 0.0942427, recall 0.909555
2017-12-10T15:05:54.256015: step 4965, loss 0.287253, acc 0.96875, prec 0.0942585, recall 0.909574
2017-12-10T15:05:54.450274: step 4966, loss 0.12076, acc 0.96875, prec 0.0943142, recall 0.909632
2017-12-10T15:05:54.648174: step 4967, loss 0.0191516, acc 1, prec 0.0943142, recall 0.909632
2017-12-10T15:05:54.841863: step 4968, loss 0.0926214, acc 0.96875, prec 0.0943101, recall 0.909632
2017-12-10T15:05:55.037121: step 4969, loss 0.0630246, acc 0.984375, prec 0.094308, recall 0.909632
2017-12-10T15:05:55.211975: step 4970, loss 0.119825, acc 0.960784, prec 0.0943238, recall 0.909651
Training finished
Starting Fold: 1 => Train/Dev split: 31795/10599


 #################### 


SEQUENCE LENGTH 273
BATCH SIZE 64
EMBEDDING SIZE 128
FILTER SIZES [3, 4, 5]
NUMBER OF FILTERS 128
L2 REG LAMBDA 0.0
EPOCHS 10



RESULT DIR leave_pos_embedding_out_fold_1
VOCAB SIZE 33447
DROPOUT PROBABILITY 0.5



WORD EMBEDDING True
POSITION EMBEDDING True
POS EMBEDDING False


 #################### 


Writing to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_1/1512936356

Start training
2017-12-10T15:05:58.996752: step 1, loss 0.916506, acc 0.6875, prec 0.047619, recall 1
2017-12-10T15:05:59.199269: step 2, loss 28.0798, acc 0.921875, prec 0.04, recall 0.5
2017-12-10T15:05:59.399892: step 3, loss 23.0456, acc 0.90625, prec 0.0357143, recall 0.2
2017-12-10T15:05:59.591596: step 4, loss 7.77587, acc 0.890625, prec 0.0294118, recall 0.166667
2017-12-10T15:05:59.783858: step 5, loss 1.37643, acc 0.828125, prec 0.0434783, recall 0.285714
2017-12-10T15:05:59.982326: step 6, loss 9.80884, acc 0.625, prec 0.0434783, recall 0.3
2017-12-10T15:06:00.175412: step 7, loss 18.5822, acc 0.40625, prec 0.0283019, recall 0.272727
2017-12-10T15:06:00.364136: step 8, loss 2.89448, acc 0.46875, prec 0.0283688, recall 0.333333
2017-12-10T15:06:00.553551: step 9, loss 4.28047, acc 0.265625, prec 0.0212766, recall 0.333333
2017-12-10T15:06:00.756045: step 10, loss 6.44906, acc 0.0625, prec 0.024, recall 0.428571
2017-12-10T15:06:00.953427: step 11, loss 7.5356, acc 0.109375, prec 0.0227273, recall 0.466667
2017-12-10T15:06:01.145692: step 12, loss 7.06352, acc 0.109375, prec 0.0218579, recall 0.5
2017-12-10T15:06:01.345608: step 13, loss 8.52004, acc 0.046875, prec 0.021028, recall 0.529412
2017-12-10T15:06:01.542108: step 14, loss 7.35175, acc 0.109375, prec 0.0205761, recall 0.555556
2017-12-10T15:06:01.733701: step 15, loss 6.29183, acc 0.046875, prec 0.0182815, recall 0.555556
2017-12-10T15:06:01.928264: step 16, loss 6.45585, acc 0.109375, prec 0.0165563, recall 0.555556
2017-12-10T15:06:02.117407: step 17, loss 5.45816, acc 0.203125, prec 0.0152672, recall 0.555556
2017-12-10T15:06:02.306917: step 18, loss 4.48985, acc 0.15625, prec 0.0141044, recall 0.555556
2017-12-10T15:06:02.500512: step 19, loss 3.62981, acc 0.34375, prec 0.0146277, recall 0.578947
2017-12-10T15:06:02.690467: step 20, loss 4.96598, acc 0.421875, prec 0.0139594, recall 0.55
2017-12-10T15:06:02.884401: step 21, loss 2.54747, acc 0.4375, prec 0.0133495, recall 0.55
2017-12-10T15:06:03.074020: step 22, loss 9.30997, acc 0.546875, prec 0.0152225, recall 0.565217
2017-12-10T15:06:03.270393: step 23, loss 1.68298, acc 0.671875, prec 0.0148571, recall 0.565217
2017-12-10T15:06:03.464887: step 24, loss 14.9973, acc 0.671875, prec 0.0145414, recall 0.52
2017-12-10T15:06:03.655913: step 25, loss 3.88873, acc 0.515625, prec 0.0140693, recall 0.5
2017-12-10T15:06:03.847744: step 26, loss 1.58585, acc 0.609375, prec 0.0136986, recall 0.5
2017-12-10T15:06:04.038861: step 27, loss 1.79626, acc 0.546875, prec 0.0132924, recall 0.5
2017-12-10T15:06:04.231289: step 28, loss 2.21012, acc 0.5625, prec 0.0139027, recall 0.518519
2017-12-10T15:06:04.430815: step 29, loss 2.01425, acc 0.640625, prec 0.0155039, recall 0.551724
2017-12-10T15:06:04.643642: step 30, loss 1.87193, acc 0.59375, prec 0.0160529, recall 0.566667
2017-12-10T15:06:04.839652: step 31, loss 1.69282, acc 0.625, prec 0.0156971, recall 0.566667
2017-12-10T15:06:05.029381: step 32, loss 2.2202, acc 0.578125, prec 0.0162016, recall 0.580645
2017-12-10T15:06:05.219504: step 33, loss 2.50438, acc 0.546875, prec 0.0157895, recall 0.580645
2017-12-10T15:06:05.415257: step 34, loss 1.98905, acc 0.578125, prec 0.0154242, recall 0.580645
2017-12-10T15:06:05.611591: step 35, loss 1.60022, acc 0.671875, prec 0.0151515, recall 0.580645
2017-12-10T15:06:05.801680: step 36, loss 11.1254, acc 0.703125, prec 0.0149254, recall 0.5625
2017-12-10T15:06:05.999519: step 37, loss 9.75971, acc 0.515625, prec 0.0145631, recall 0.545455
2017-12-10T15:06:06.190069: step 38, loss 12.3764, acc 0.671875, prec 0.0143312, recall 0.529412
2017-12-10T15:06:06.383857: step 39, loss 1.2798, acc 0.734375, prec 0.0141398, recall 0.529412
2017-12-10T15:06:06.580421: step 40, loss 6.93103, acc 0.671875, prec 0.0146832, recall 0.527778
2017-12-10T15:06:06.777660: step 41, loss 1.64251, acc 0.671875, prec 0.0151976, recall 0.540541
2017-12-10T15:06:06.967037: step 42, loss 2.58155, acc 0.453125, prec 0.0155325, recall 0.552632
2017-12-10T15:06:07.158694: step 43, loss 2.78479, acc 0.4375, prec 0.0172538, recall 0.585366
2017-12-10T15:06:07.353934: step 44, loss 1.57933, acc 0.578125, prec 0.0169252, recall 0.585366
2017-12-10T15:06:07.549581: step 45, loss 2.37359, acc 0.5, prec 0.0165517, recall 0.585366
2017-12-10T15:06:07.741608: step 46, loss 10.5926, acc 0.53125, prec 0.0162272, recall 0.571429
2017-12-10T15:06:07.938361: step 47, loss 2.35995, acc 0.515625, prec 0.015894, recall 0.571429
2017-12-10T15:06:08.131489: step 48, loss 1.49824, acc 0.6875, prec 0.0163292, recall 0.581395
2017-12-10T15:06:08.323686: step 49, loss 5.44374, acc 0.46875, prec 0.0166134, recall 0.577778
2017-12-10T15:06:08.514702: step 50, loss 2.30042, acc 0.46875, prec 0.016875, recall 0.586957
2017-12-10T15:06:08.708210: step 51, loss 2.28986, acc 0.578125, prec 0.017199, recall 0.595745
2017-12-10T15:06:08.897397: step 52, loss 2.09331, acc 0.546875, prec 0.0180832, recall 0.612245
2017-12-10T15:06:09.088372: step 53, loss 2.29285, acc 0.4375, prec 0.0176991, recall 0.612245
2017-12-10T15:06:09.278500: step 54, loss 2.63295, acc 0.515625, prec 0.0185185, recall 0.627451
2017-12-10T15:06:09.469978: step 55, loss 2.32306, acc 0.484375, prec 0.0181715, recall 0.627451
2017-12-10T15:06:09.661407: step 56, loss 2.65637, acc 0.453125, prec 0.0189099, recall 0.641509
2017-12-10T15:06:09.855908: step 57, loss 4.25704, acc 0.671875, prec 0.0187019, recall 0.62963
2017-12-10T15:06:10.048953: step 58, loss 8.80165, acc 0.484375, prec 0.0183883, recall 0.607143
2017-12-10T15:06:10.244458: step 59, loss 1.83308, acc 0.578125, prec 0.0186468, recall 0.614035
2017-12-10T15:06:10.439883: step 60, loss 1.27838, acc 0.671875, prec 0.0184405, recall 0.614035
2017-12-10T15:06:10.629689: step 61, loss 10.8859, acc 0.5625, prec 0.0181818, recall 0.603448
2017-12-10T15:06:10.823568: step 62, loss 2.04403, acc 0.546875, prec 0.0184143, recall 0.610169
2017-12-10T15:06:11.019551: step 63, loss 3.33603, acc 0.484375, prec 0.0190955, recall 0.622951
2017-12-10T15:06:11.214690: step 64, loss 2.84534, acc 0.484375, prec 0.018784, recall 0.622951
2017-12-10T15:06:11.408013: step 65, loss 2.37438, acc 0.5625, prec 0.0185275, recall 0.622951
2017-12-10T15:06:11.599619: step 66, loss 5.60477, acc 0.421875, prec 0.0182167, recall 0.603175
2017-12-10T15:06:11.789963: step 67, loss 3.90776, acc 0.375, prec 0.0183357, recall 0.609375
2017-12-10T15:06:11.985866: step 68, loss 3.84099, acc 0.390625, prec 0.0180055, recall 0.609375
2017-12-10T15:06:12.173793: step 69, loss 2.51798, acc 0.515625, prec 0.0181984, recall 0.615385
2017-12-10T15:06:12.369227: step 70, loss 3.62002, acc 0.375, prec 0.01875, recall 0.626866
2017-12-10T15:06:12.565902: step 71, loss 2.46245, acc 0.46875, prec 0.0184697, recall 0.626866
2017-12-10T15:06:12.759318: step 72, loss 3.19266, acc 0.421875, prec 0.0181739, recall 0.626866
2017-12-10T15:06:12.949803: step 73, loss 2.2667, acc 0.40625, prec 0.0178799, recall 0.626866
2017-12-10T15:06:13.140511: step 74, loss 1.57514, acc 0.578125, prec 0.0176768, recall 0.626866
2017-12-10T15:06:13.331483: step 75, loss 1.93242, acc 0.640625, prec 0.0175073, recall 0.626866
2017-12-10T15:06:13.524259: step 76, loss 1.60995, acc 0.65625, prec 0.0181593, recall 0.637681
2017-12-10T15:06:13.715322: step 77, loss 7.17768, acc 0.578125, prec 0.0183673, recall 0.633803
2017-12-10T15:06:13.912917: step 78, loss 1.08238, acc 0.671875, prec 0.0190053, recall 0.643836
2017-12-10T15:06:14.108875: step 79, loss 1.67384, acc 0.703125, prec 0.0188604, recall 0.643836
2017-12-10T15:06:14.307187: step 80, loss 1.37699, acc 0.609375, prec 0.018673, recall 0.643836
2017-12-10T15:06:14.497933: step 81, loss 12.589, acc 0.734375, prec 0.0185551, recall 0.635135
2017-12-10T15:06:14.689630: step 82, loss 11.4591, acc 0.75, prec 0.0192157, recall 0.636364
2017-12-10T15:06:14.886972: step 83, loss 13.9527, acc 0.703125, prec 0.019081, recall 0.628205
2017-12-10T15:06:15.085437: step 84, loss 9.74402, acc 0.59375, prec 0.0189043, recall 0.6125
2017-12-10T15:06:15.283546: step 85, loss 2.96195, acc 0.640625, prec 0.019488, recall 0.621951
2017-12-10T15:06:15.479593: step 86, loss 2.71211, acc 0.4375, prec 0.0195931, recall 0.626506
2017-12-10T15:06:15.669835: step 87, loss 6.83992, acc 0.234375, prec 0.0203402, recall 0.625
2017-12-10T15:06:15.864572: step 88, loss 3.48539, acc 0.375, prec 0.0207575, recall 0.633333
2017-12-10T15:06:16.063352: step 89, loss 4.47134, acc 0.25, prec 0.0211016, recall 0.641304
2017-12-10T15:06:16.253612: step 90, loss 4.97308, acc 0.15625, prec 0.0207018, recall 0.641304
2017-12-10T15:06:16.441635: step 91, loss 5.45123, acc 0.28125, prec 0.0213867, recall 0.652632
2017-12-10T15:06:16.634074: step 92, loss 6.41146, acc 0.109375, prec 0.0209743, recall 0.652632
2017-12-10T15:06:16.824846: step 93, loss 6.1536, acc 0.171875, prec 0.0206049, recall 0.652632
2017-12-10T15:06:17.017024: step 94, loss 5.20266, acc 0.21875, prec 0.0202681, recall 0.652632
2017-12-10T15:06:17.207223: step 95, loss 5.22068, acc 0.1875, prec 0.0205589, recall 0.659794
2017-12-10T15:06:17.400962: step 96, loss 4.84667, acc 0.3125, prec 0.0202724, recall 0.659794
2017-12-10T15:06:17.592803: step 97, loss 4.55551, acc 0.28125, prec 0.0199813, recall 0.659794
2017-12-10T15:06:17.787384: step 98, loss 2.83262, acc 0.4375, prec 0.0200617, recall 0.663265
2017-12-10T15:06:17.976996: step 99, loss 4.0981, acc 0.359375, prec 0.0204082, recall 0.67
2017-12-10T15:06:18.164528: step 100, loss 1.77553, acc 0.625, prec 0.0205562, recall 0.673267
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_1/1512936356/checkpoints/model-100

2017-12-10T15:06:19.334997: step 101, loss 2.64965, acc 0.53125, prec 0.0203715, recall 0.673267
2017-12-10T15:06:19.527452: step 102, loss 1.60696, acc 0.609375, prec 0.02022, recall 0.673267
2017-12-10T15:06:19.717652: step 103, loss 0.5967, acc 0.78125, prec 0.0201362, recall 0.673267
2017-12-10T15:06:19.910623: step 104, loss 5.15111, acc 0.765625, prec 0.020342, recall 0.669903
2017-12-10T15:06:20.109212: step 105, loss 0.817051, acc 0.8125, prec 0.0202703, recall 0.669903
2017-12-10T15:06:20.303271: step 106, loss 0.474652, acc 0.859375, prec 0.0202168, recall 0.669903
2017-12-10T15:06:20.493491: step 107, loss 13.9794, acc 0.890625, prec 0.0201813, recall 0.663462
2017-12-10T15:06:20.689329: step 108, loss 7.11796, acc 0.8125, prec 0.0204022, recall 0.660377
2017-12-10T15:06:20.886759: step 109, loss 0.792849, acc 0.78125, prec 0.0203193, recall 0.660377
2017-12-10T15:06:21.081693: step 110, loss 0.942067, acc 0.828125, prec 0.020538, recall 0.663551
2017-12-10T15:06:21.275660: step 111, loss 0.442175, acc 0.84375, prec 0.0210435, recall 0.669725
2017-12-10T15:06:21.469426: step 112, loss 0.797516, acc 0.828125, prec 0.020977, recall 0.669725
2017-12-10T15:06:21.663144: step 113, loss 5.72063, acc 0.75, prec 0.020893, recall 0.657658
2017-12-10T15:06:21.854791: step 114, loss 15.1643, acc 0.71875, prec 0.0208036, recall 0.640351
2017-12-10T15:06:22.047214: step 115, loss 1.26636, acc 0.734375, prec 0.0207033, recall 0.640351
2017-12-10T15:06:22.242233: step 116, loss 1.5126, acc 0.6875, prec 0.0208627, recall 0.643478
2017-12-10T15:06:22.436646: step 117, loss 2.40532, acc 0.5625, prec 0.0206993, recall 0.643478
2017-12-10T15:06:22.628090: step 118, loss 2.05295, acc 0.640625, prec 0.020567, recall 0.643478
2017-12-10T15:06:22.819530: step 119, loss 2.51983, acc 0.5, prec 0.0203857, recall 0.643478
2017-12-10T15:06:23.009132: step 120, loss 7.37785, acc 0.484375, prec 0.0207424, recall 0.644068
2017-12-10T15:06:23.196992: step 121, loss 3.51257, acc 0.40625, prec 0.0205294, recall 0.644068
2017-12-10T15:06:23.392477: step 122, loss 4.66862, acc 0.359375, prec 0.0208278, recall 0.65
2017-12-10T15:06:23.586244: step 123, loss 3.65837, acc 0.375, prec 0.0208663, recall 0.652893
2017-12-10T15:06:23.780110: step 124, loss 3.34501, acc 0.46875, prec 0.0209369, recall 0.655738
2017-12-10T15:06:23.970088: step 125, loss 3.83783, acc 0.359375, prec 0.0212215, recall 0.66129
2017-12-10T15:06:24.161931: step 126, loss 13.8851, acc 0.3125, prec 0.020988, recall 0.656
2017-12-10T15:06:24.357395: step 127, loss 4.63704, acc 0.234375, prec 0.020728, recall 0.656
2017-12-10T15:06:24.550705: step 128, loss 4.28453, acc 0.1875, prec 0.0207034, recall 0.65873
2017-12-10T15:06:24.746452: step 129, loss 4.18232, acc 0.40625, prec 0.0209928, recall 0.664062
2017-12-10T15:06:24.935886: step 130, loss 3.2119, acc 0.390625, prec 0.0215106, recall 0.671756
2017-12-10T15:06:25.125228: step 131, loss 15.1591, acc 0.453125, prec 0.0213333, recall 0.666667
2017-12-10T15:06:25.323461: step 132, loss 2.43327, acc 0.53125, prec 0.0221207, recall 0.676471
2017-12-10T15:06:25.514507: step 133, loss 3.22638, acc 0.421875, prec 0.0221587, recall 0.678832
2017-12-10T15:06:25.710714: step 134, loss 3.44077, acc 0.5, prec 0.0222222, recall 0.681159
2017-12-10T15:06:25.904222: step 135, loss 12.6193, acc 0.40625, prec 0.0224877, recall 0.680851
2017-12-10T15:06:26.092201: step 136, loss 2.58052, acc 0.46875, prec 0.0225372, recall 0.683099
2017-12-10T15:06:26.281883: step 137, loss 2.4524, acc 0.5, prec 0.0223708, recall 0.683099
2017-12-10T15:06:26.473418: step 138, loss 2.24235, acc 0.578125, prec 0.0229043, recall 0.689655
2017-12-10T15:06:26.666577: step 139, loss 2.0458, acc 0.515625, prec 0.0227428, recall 0.689655
2017-12-10T15:06:26.863545: step 140, loss 14.1389, acc 0.5625, prec 0.02283, recall 0.682432
2017-12-10T15:06:27.056840: step 141, loss 1.96984, acc 0.578125, prec 0.0231305, recall 0.686667
2017-12-10T15:06:27.250850: step 142, loss 1.98826, acc 0.546875, prec 0.0234166, recall 0.690789
2017-12-10T15:06:27.443788: step 143, loss 4.20796, acc 0.609375, prec 0.0235085, recall 0.688312
2017-12-10T15:06:27.636431: step 144, loss 1.83178, acc 0.5625, prec 0.0235787, recall 0.690323
2017-12-10T15:06:27.829141: step 145, loss 2.18755, acc 0.546875, prec 0.0234289, recall 0.690323
2017-12-10T15:06:28.023741: step 146, loss 10.7979, acc 0.53125, prec 0.0232811, recall 0.685897
2017-12-10T15:06:28.220114: step 147, loss 2.49375, acc 0.515625, prec 0.0231251, recall 0.685897
2017-12-10T15:06:28.415815: step 148, loss 3.24463, acc 0.46875, prec 0.023166, recall 0.687898
2017-12-10T15:06:28.608849: step 149, loss 4.56227, acc 0.46875, prec 0.0230032, recall 0.683544
2017-12-10T15:06:28.819399: step 150, loss 1.93867, acc 0.515625, prec 0.0228523, recall 0.683544
2017-12-10T15:06:29.026695: step 151, loss 6.50535, acc 0.421875, prec 0.0226795, recall 0.679245
2017-12-10T15:06:29.228477: step 152, loss 4.33781, acc 0.5625, prec 0.0227557, recall 0.677019
2017-12-10T15:06:29.421019: step 153, loss 2.67471, acc 0.484375, prec 0.0230052, recall 0.680982
2017-12-10T15:06:29.609434: step 154, loss 3.36434, acc 0.34375, prec 0.0228067, recall 0.680982
2017-12-10T15:06:29.802710: step 155, loss 3.13379, acc 0.40625, prec 0.0228292, recall 0.682927
2017-12-10T15:06:29.992204: step 156, loss 2.64746, acc 0.34375, prec 0.0226354, recall 0.682927
2017-12-10T15:06:30.199962: step 157, loss 3.21575, acc 0.34375, prec 0.0224449, recall 0.682927
2017-12-10T15:06:30.393972: step 158, loss 2.58167, acc 0.5, prec 0.0226911, recall 0.686747
2017-12-10T15:06:30.585285: step 159, loss 3.03154, acc 0.359375, prec 0.0225074, recall 0.686747
2017-12-10T15:06:30.779119: step 160, loss 3.20298, acc 0.421875, prec 0.0223442, recall 0.686747
2017-12-10T15:06:30.966949: step 161, loss 4.87507, acc 0.5625, prec 0.0222266, recall 0.682635
2017-12-10T15:06:31.160923: step 162, loss 1.22457, acc 0.6875, prec 0.0221402, recall 0.682635
2017-12-10T15:06:31.377837: step 163, loss 4.75972, acc 0.640625, prec 0.022046, recall 0.678571
2017-12-10T15:06:31.575471: step 164, loss 12.3999, acc 0.53125, prec 0.0221111, recall 0.676471
2017-12-10T15:06:31.771926: step 165, loss 1.23763, acc 0.65625, prec 0.022018, recall 0.676471
2017-12-10T15:06:31.964843: step 166, loss 1.5583, acc 0.6875, prec 0.0221205, recall 0.678363
2017-12-10T15:06:32.156636: step 167, loss 3.77129, acc 0.546875, prec 0.0221885, recall 0.676301
2017-12-10T15:06:32.350584: step 168, loss 2.34818, acc 0.546875, prec 0.0220671, recall 0.676301
2017-12-10T15:06:32.545886: step 169, loss 13.3786, acc 0.5, prec 0.021943, recall 0.668571
2017-12-10T15:06:32.739823: step 170, loss 3.39589, acc 0.484375, prec 0.0219903, recall 0.670455
2017-12-10T15:06:32.933824: step 171, loss 2.30941, acc 0.515625, prec 0.0220452, recall 0.672316
2017-12-10T15:06:33.125986: step 172, loss 5.51424, acc 0.328125, prec 0.021875, recall 0.668539
2017-12-10T15:06:33.317318: step 173, loss 2.25006, acc 0.53125, prec 0.0221126, recall 0.672222
2017-12-10T15:06:33.510420: step 174, loss 2.29183, acc 0.546875, prec 0.0221738, recall 0.674033
2017-12-10T15:06:33.702464: step 175, loss 2.77507, acc 0.46875, prec 0.0223908, recall 0.677596
2017-12-10T15:06:33.894507: step 176, loss 2.7162, acc 0.40625, prec 0.0224135, recall 0.679348
2017-12-10T15:06:34.087558: step 177, loss 2.95019, acc 0.40625, prec 0.0226099, recall 0.682796
2017-12-10T15:06:34.283910: step 178, loss 3.03322, acc 0.40625, prec 0.0226308, recall 0.684492
2017-12-10T15:06:34.480139: step 179, loss 9.58377, acc 0.453125, prec 0.0228391, recall 0.684211
2017-12-10T15:06:34.672331: step 180, loss 3.36723, acc 0.4375, prec 0.0226955, recall 0.684211
2017-12-10T15:06:34.868087: step 181, loss 1.94605, acc 0.5, prec 0.0225694, recall 0.684211
2017-12-10T15:06:35.062951: step 182, loss 2.92008, acc 0.453125, prec 0.0224331, recall 0.684211
2017-12-10T15:06:35.257131: step 183, loss 11.4951, acc 0.4375, prec 0.0224661, recall 0.682292
2017-12-10T15:06:35.447154: step 184, loss 2.34519, acc 0.53125, prec 0.0223511, recall 0.682292
2017-12-10T15:06:35.638290: step 185, loss 5.78339, acc 0.640625, prec 0.0224337, recall 0.680412
2017-12-10T15:06:35.833776: step 186, loss 1.73372, acc 0.578125, prec 0.0224966, recall 0.682051
2017-12-10T15:06:36.027436: step 187, loss 13.3376, acc 0.65625, prec 0.0224208, recall 0.675127
2017-12-10T15:06:36.225381: step 188, loss 1.50868, acc 0.5625, prec 0.0226434, recall 0.678392
2017-12-10T15:06:36.415510: step 189, loss 2.47338, acc 0.515625, prec 0.0225263, recall 0.678392
2017-12-10T15:06:36.604676: step 190, loss 5.29117, acc 0.59375, prec 0.0224327, recall 0.675
2017-12-10T15:06:36.799932: step 191, loss 2.98604, acc 0.421875, prec 0.0224571, recall 0.676617
2017-12-10T15:06:36.989715: step 192, loss 3.08289, acc 0.390625, prec 0.0223134, recall 0.676617
2017-12-10T15:06:37.179897: step 193, loss 3.2108, acc 0.328125, prec 0.0221571, recall 0.676617
2017-12-10T15:06:37.374012: step 194, loss 4.40426, acc 0.421875, prec 0.0220279, recall 0.673267
2017-12-10T15:06:37.571626: step 195, loss 4.63841, acc 0.296875, prec 0.0218685, recall 0.673267
2017-12-10T15:06:37.762197: step 196, loss 3.39908, acc 0.34375, prec 0.0217218, recall 0.673267
2017-12-10T15:06:37.951031: step 197, loss 2.25282, acc 0.546875, prec 0.0217771, recall 0.674877
2017-12-10T15:06:38.146725: step 198, loss 2.89012, acc 0.484375, prec 0.0216635, recall 0.674877
2017-12-10T15:06:38.340344: step 199, loss 2.26618, acc 0.453125, prec 0.0218519, recall 0.678049
2017-12-10T15:06:38.536274: step 200, loss 4.2208, acc 0.546875, prec 0.0219092, recall 0.676328
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_1/1512936356/checkpoints/model-200

2017-12-10T15:06:39.820366: step 201, loss 5.24945, acc 0.421875, prec 0.0219387, recall 0.674641
2017-12-10T15:06:40.016427: step 202, loss 9.75501, acc 0.59375, prec 0.0221568, recall 0.674528
2017-12-10T15:06:40.210363: step 203, loss 2.13228, acc 0.515625, prec 0.0220509, recall 0.674528
2017-12-10T15:06:40.401233: step 204, loss 14.8219, acc 0.4375, prec 0.0220825, recall 0.672897
2017-12-10T15:06:40.598512: step 205, loss 2.77151, acc 0.515625, prec 0.0221273, recall 0.674419
2017-12-10T15:06:40.792101: step 206, loss 3.00277, acc 0.375, prec 0.021993, recall 0.674419
2017-12-10T15:06:40.988865: step 207, loss 2.61249, acc 0.40625, prec 0.021867, recall 0.674419
2017-12-10T15:06:41.180105: step 208, loss 2.96262, acc 0.34375, prec 0.0217294, recall 0.674419
2017-12-10T15:06:41.378674: step 209, loss 5.27082, acc 0.390625, prec 0.0220435, recall 0.675799
2017-12-10T15:06:41.567982: step 210, loss 2.86361, acc 0.390625, prec 0.0222058, recall 0.678733
2017-12-10T15:06:41.758935: step 211, loss 2.52632, acc 0.515625, prec 0.0221043, recall 0.678733
2017-12-10T15:06:41.952865: step 212, loss 2.65907, acc 0.390625, prec 0.0221213, recall 0.68018
2017-12-10T15:06:42.146229: step 213, loss 6.1649, acc 0.484375, prec 0.0220181, recall 0.67713
2017-12-10T15:06:42.342234: step 214, loss 1.80687, acc 0.546875, prec 0.0219254, recall 0.67713
2017-12-10T15:06:42.535764: step 215, loss 1.97712, acc 0.515625, prec 0.0218271, recall 0.67713
2017-12-10T15:06:42.726988: step 216, loss 1.72471, acc 0.59375, prec 0.0218863, recall 0.678571
2017-12-10T15:06:42.920181: step 217, loss 2.39604, acc 0.578125, prec 0.0218015, recall 0.678571
2017-12-10T15:06:43.110794: step 218, loss 1.37711, acc 0.640625, prec 0.0217298, recall 0.678571
2017-12-10T15:06:43.306093: step 219, loss 1.19572, acc 0.65625, prec 0.0216617, recall 0.678571
2017-12-10T15:06:43.506250: step 220, loss 0.588544, acc 0.828125, prec 0.021767, recall 0.68
2017-12-10T15:06:43.698441: step 221, loss 9.62641, acc 0.8125, prec 0.0218719, recall 0.678414
2017-12-10T15:06:43.892185: step 222, loss 1.04344, acc 0.734375, prec 0.0218192, recall 0.678414
2017-12-10T15:06:44.089885: step 223, loss 0.709075, acc 0.78125, prec 0.021776, recall 0.678414
2017-12-10T15:06:44.291360: step 224, loss 20.6119, acc 0.828125, prec 0.0218834, recall 0.676856
2017-12-10T15:06:44.485835: step 225, loss 0.992457, acc 0.71875, prec 0.0218279, recall 0.676856
2017-12-10T15:06:44.679316: step 226, loss 4.2313, acc 0.796875, prec 0.0220661, recall 0.676724
2017-12-10T15:06:44.870888: step 227, loss 0.940185, acc 0.78125, prec 0.022297, recall 0.679487
2017-12-10T15:06:45.065310: step 228, loss 12.1322, acc 0.796875, prec 0.0225364, recall 0.676471
2017-12-10T15:06:45.264593: step 229, loss 1.72317, acc 0.5625, prec 0.0225847, recall 0.677824
2017-12-10T15:06:45.455577: step 230, loss 1.46982, acc 0.6875, prec 0.0226578, recall 0.679167
2017-12-10T15:06:45.647575: step 231, loss 2.15924, acc 0.46875, prec 0.0225512, recall 0.679167
2017-12-10T15:06:45.842230: step 232, loss 1.89307, acc 0.546875, prec 0.0224611, recall 0.679167
2017-12-10T15:06:46.032852: step 233, loss 2.98532, acc 0.5, prec 0.0223625, recall 0.679167
2017-12-10T15:06:46.224391: step 234, loss 3.17471, acc 0.46875, prec 0.0223921, recall 0.680498
2017-12-10T15:06:46.417602: step 235, loss 2.62694, acc 0.484375, prec 0.0222917, recall 0.680498
2017-12-10T15:06:46.614131: step 236, loss 2.69265, acc 0.53125, prec 0.0223335, recall 0.681818
2017-12-10T15:06:46.805278: step 237, loss 2.46497, acc 0.453125, prec 0.0222282, recall 0.681818
2017-12-10T15:06:46.997627: step 238, loss 2.11208, acc 0.5, prec 0.0221328, recall 0.681818
2017-12-10T15:06:47.189008: step 239, loss 3.96378, acc 0.453125, prec 0.0220323, recall 0.679012
2017-12-10T15:06:47.381681: step 240, loss 4.6806, acc 0.5625, prec 0.0219532, recall 0.67623
2017-12-10T15:06:47.573286: step 241, loss 1.73041, acc 0.578125, prec 0.0220042, recall 0.677551
2017-12-10T15:06:47.764410: step 242, loss 2.83652, acc 0.515625, prec 0.0223014, recall 0.681452
2017-12-10T15:06:47.962184: step 243, loss 2.20322, acc 0.453125, prec 0.0223273, recall 0.682731
2017-12-10T15:06:48.159975: step 244, loss 6.21332, acc 0.578125, prec 0.0223793, recall 0.681275
2017-12-10T15:06:48.354537: step 245, loss 2.74596, acc 0.59375, prec 0.0224309, recall 0.68254
2017-12-10T15:06:48.544675: step 246, loss 2.11741, acc 0.421875, prec 0.02245, recall 0.683794
2017-12-10T15:06:48.738701: step 247, loss 2.28086, acc 0.578125, prec 0.0223717, recall 0.683794
2017-12-10T15:06:48.930798: step 248, loss 2.14827, acc 0.609375, prec 0.0224256, recall 0.685039
2017-12-10T15:06:49.122923: step 249, loss 2.13462, acc 0.546875, prec 0.0224676, recall 0.686275
2017-12-10T15:06:49.315085: step 250, loss 1.93691, acc 0.59375, prec 0.022643, recall 0.688716
2017-12-10T15:06:49.511255: step 251, loss 2.19805, acc 0.640625, prec 0.0228258, recall 0.69112
2017-12-10T15:06:49.700607: step 252, loss 2.0074, acc 0.6875, prec 0.022892, recall 0.692308
2017-12-10T15:06:49.889368: step 253, loss 8.91356, acc 0.59375, prec 0.0228195, recall 0.689655
2017-12-10T15:06:50.084853: step 254, loss 1.73164, acc 0.59375, prec 0.0227445, recall 0.689655
2017-12-10T15:06:50.281673: step 255, loss 1.21934, acc 0.640625, prec 0.0226786, recall 0.689655
2017-12-10T15:06:50.475493: step 256, loss 3.82388, acc 0.671875, prec 0.0227444, recall 0.688213
2017-12-10T15:06:50.670152: step 257, loss 4.67011, acc 0.640625, prec 0.0228042, recall 0.686792
2017-12-10T15:06:50.862254: step 258, loss 2.05135, acc 0.53125, prec 0.0228407, recall 0.68797
2017-12-10T15:06:51.050998: step 259, loss 2.10115, acc 0.515625, prec 0.0227527, recall 0.68797
2017-12-10T15:06:51.240971: step 260, loss 5.43683, acc 0.6875, prec 0.0229415, recall 0.687732
2017-12-10T15:06:51.432544: step 261, loss 3.67727, acc 0.625, prec 0.0228762, recall 0.685185
2017-12-10T15:06:51.628797: step 262, loss 1.80862, acc 0.53125, prec 0.0227917, recall 0.685185
2017-12-10T15:06:51.822450: step 263, loss 6.82461, acc 0.453125, prec 0.0228165, recall 0.683824
2017-12-10T15:06:52.020152: step 264, loss 1.99049, acc 0.53125, prec 0.0227328, recall 0.683824
2017-12-10T15:06:52.219905: step 265, loss 2.09754, acc 0.515625, prec 0.022647, recall 0.683824
2017-12-10T15:06:52.415329: step 266, loss 3.14841, acc 0.453125, prec 0.0226694, recall 0.684982
2017-12-10T15:06:52.609326: step 267, loss 3.14631, acc 0.421875, prec 0.0225682, recall 0.684982
2017-12-10T15:06:52.801541: step 268, loss 2.61994, acc 0.46875, prec 0.0227109, recall 0.687273
2017-12-10T15:06:52.994407: step 269, loss 1.74818, acc 0.5625, prec 0.0227518, recall 0.688406
2017-12-10T15:06:53.187574: step 270, loss 3.25986, acc 0.625, prec 0.022806, recall 0.68705
2017-12-10T15:06:53.381259: step 271, loss 1.83417, acc 0.640625, prec 0.0228599, recall 0.688172
2017-12-10T15:06:53.572804: step 272, loss 2.40897, acc 0.546875, prec 0.0227812, recall 0.688172
2017-12-10T15:06:53.762444: step 273, loss 2.94999, acc 0.453125, prec 0.0229179, recall 0.690391
2017-12-10T15:06:53.952480: step 274, loss 1.1906, acc 0.65625, prec 0.0228585, recall 0.690391
2017-12-10T15:06:54.145896: step 275, loss 1.0517, acc 0.734375, prec 0.0228128, recall 0.690391
2017-12-10T15:06:54.338258: step 276, loss 1.37012, acc 0.671875, prec 0.0227566, recall 0.690391
2017-12-10T15:06:54.527790: step 277, loss 1.64986, acc 0.625, prec 0.0226927, recall 0.690391
2017-12-10T15:06:54.721540: step 278, loss 1.31036, acc 0.765625, prec 0.0227671, recall 0.691489
2017-12-10T15:06:54.913336: step 279, loss 2.1013, acc 0.828125, prec 0.0229684, recall 0.691228
2017-12-10T15:06:55.108227: step 280, loss 0.816505, acc 0.734375, prec 0.022923, recall 0.691228
2017-12-10T15:06:55.302555: step 281, loss 1.27814, acc 0.71875, prec 0.0229885, recall 0.692308
2017-12-10T15:06:55.491516: step 282, loss 1.32526, acc 0.8125, prec 0.023183, recall 0.694444
2017-12-10T15:06:55.688296: step 283, loss 0.865999, acc 0.765625, prec 0.0231428, recall 0.694444
2017-12-10T15:06:55.880955: step 284, loss 14.8453, acc 0.84375, prec 0.0232316, recall 0.693103
2017-12-10T15:06:56.072941: step 285, loss 6.34179, acc 0.75, prec 0.0231914, recall 0.690722
2017-12-10T15:06:56.269753: step 286, loss 0.779898, acc 0.8125, prec 0.0231593, recall 0.690722
2017-12-10T15:06:56.463652: step 287, loss 1.04737, acc 0.78125, prec 0.0232344, recall 0.691781
2017-12-10T15:06:56.655522: step 288, loss 4.30971, acc 0.671875, prec 0.0231811, recall 0.68942
2017-12-10T15:06:56.848336: step 289, loss 2.38592, acc 0.609375, prec 0.0232265, recall 0.690476
2017-12-10T15:06:57.048302: step 290, loss 2.47912, acc 0.578125, prec 0.0231577, recall 0.688136
2017-12-10T15:06:57.246467: step 291, loss 1.43352, acc 0.6875, prec 0.0231049, recall 0.688136
2017-12-10T15:06:57.438064: step 292, loss 1.08324, acc 0.671875, prec 0.0230498, recall 0.688136
2017-12-10T15:06:57.632299: step 293, loss 2.03986, acc 0.515625, prec 0.022969, recall 0.688136
2017-12-10T15:06:57.820477: step 294, loss 1.28627, acc 0.59375, prec 0.0230118, recall 0.689189
2017-12-10T15:06:58.011071: step 295, loss 1.4847, acc 0.59375, prec 0.0229446, recall 0.689189
2017-12-10T15:06:58.205402: step 296, loss 2.21696, acc 0.5, prec 0.0228623, recall 0.689189
2017-12-10T15:06:58.400951: step 297, loss 2.75347, acc 0.59375, prec 0.0230142, recall 0.691275
2017-12-10T15:06:58.592732: step 298, loss 1.69455, acc 0.578125, prec 0.022945, recall 0.691275
2017-12-10T15:06:58.780144: step 299, loss 1.28031, acc 0.65625, prec 0.0228889, recall 0.691275
2017-12-10T15:06:58.975282: step 300, loss 1.67769, acc 0.671875, prec 0.0228356, recall 0.691275
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_1/1512936356/checkpoints/model-300

2017-12-10T15:07:00.269802: step 301, loss 0.810224, acc 0.75, prec 0.0229033, recall 0.692308
2017-12-10T15:07:00.463203: step 302, loss 1.16479, acc 0.78125, prec 0.0228679, recall 0.692308
2017-12-10T15:07:00.653427: step 303, loss 1.62976, acc 0.734375, prec 0.0229327, recall 0.693333
2017-12-10T15:07:00.846884: step 304, loss 0.773054, acc 0.734375, prec 0.0229974, recall 0.694352
2017-12-10T15:07:01.040695: step 305, loss 0.771344, acc 0.796875, prec 0.0231792, recall 0.69637
2017-12-10T15:07:01.234972: step 306, loss 7.10418, acc 0.765625, prec 0.0231436, recall 0.694079
2017-12-10T15:07:01.433462: step 307, loss 1.22106, acc 0.796875, prec 0.0233246, recall 0.696078
2017-12-10T15:07:01.631912: step 308, loss 19.0636, acc 0.6875, prec 0.0232812, recall 0.68932
2017-12-10T15:07:01.823258: step 309, loss 2.65951, acc 0.765625, prec 0.0233522, recall 0.688103
2017-12-10T15:07:02.014930: step 310, loss 1.82041, acc 0.53125, prec 0.0233823, recall 0.689103
2017-12-10T15:07:02.203571: step 311, loss 2.58791, acc 0.46875, prec 0.0234019, recall 0.690096
2017-12-10T15:07:02.393376: step 312, loss 1.90837, acc 0.625, prec 0.0234468, recall 0.691083
2017-12-10T15:07:02.582729: step 313, loss 3.28254, acc 0.421875, prec 0.0233534, recall 0.691083
2017-12-10T15:07:02.782728: step 314, loss 3.74895, acc 0.390625, prec 0.0233605, recall 0.692064
2017-12-10T15:07:02.977316: step 315, loss 3.47904, acc 0.296875, prec 0.0232484, recall 0.692064
2017-12-10T15:07:03.164714: step 316, loss 2.80665, acc 0.4375, prec 0.023367, recall 0.694006
2017-12-10T15:07:03.354727: step 317, loss 2.9909, acc 0.515625, prec 0.0237037, recall 0.697819
2017-12-10T15:07:03.544792: step 318, loss 2.93961, acc 0.375, prec 0.0239124, recall 0.700617
2017-12-10T15:07:03.738381: step 319, loss 2.23888, acc 0.484375, prec 0.0238295, recall 0.700617
2017-12-10T15:07:03.936604: step 320, loss 2.80746, acc 0.421875, prec 0.0237373, recall 0.700617
2017-12-10T15:07:04.132778: step 321, loss 5.1726, acc 0.578125, prec 0.0237748, recall 0.699386
2017-12-10T15:07:04.328273: step 322, loss 8.17784, acc 0.640625, prec 0.0237204, recall 0.697248
2017-12-10T15:07:04.527606: step 323, loss 2.67347, acc 0.453125, prec 0.0236343, recall 0.697248
2017-12-10T15:07:04.716302: step 324, loss 1.71757, acc 0.578125, prec 0.0236693, recall 0.698171
2017-12-10T15:07:04.906112: step 325, loss 3.99096, acc 0.484375, prec 0.0235912, recall 0.696049
2017-12-10T15:07:05.099143: step 326, loss 1.59409, acc 0.59375, prec 0.0235282, recall 0.696049
2017-12-10T15:07:05.297013: step 327, loss 2.19147, acc 0.5625, prec 0.0235607, recall 0.69697
2017-12-10T15:07:05.489007: step 328, loss 1.35107, acc 0.65625, prec 0.0236076, recall 0.697885
2017-12-10T15:07:05.680228: step 329, loss 2.46352, acc 0.6875, prec 0.0235618, recall 0.695783
2017-12-10T15:07:05.876639: step 330, loss 1.20988, acc 0.640625, prec 0.0238047, recall 0.698507
2017-12-10T15:07:06.070826: step 331, loss 1.33053, acc 0.703125, prec 0.0238579, recall 0.699405
2017-12-10T15:07:06.264194: step 332, loss 1.92595, acc 0.546875, prec 0.0237878, recall 0.699405
2017-12-10T15:07:06.455075: step 333, loss 1.1725, acc 0.65625, prec 0.0239321, recall 0.701183
2017-12-10T15:07:06.646546: step 334, loss 1.95863, acc 0.625, prec 0.0240709, recall 0.702941
2017-12-10T15:07:06.841231: step 335, loss 0.717042, acc 0.734375, prec 0.0240298, recall 0.702941
2017-12-10T15:07:07.034692: step 336, loss 10.8632, acc 0.796875, prec 0.0241968, recall 0.702624
2017-12-10T15:07:07.233189: step 337, loss 2.03827, acc 0.65625, prec 0.0242412, recall 0.703488
2017-12-10T15:07:07.423581: step 338, loss 1.3896, acc 0.703125, prec 0.0243902, recall 0.705202
2017-12-10T15:07:07.615156: step 339, loss 1.34483, acc 0.71875, prec 0.0244438, recall 0.706052
2017-12-10T15:07:07.807781: step 340, loss 0.898842, acc 0.734375, prec 0.0244996, recall 0.706897
2017-12-10T15:07:08.001818: step 341, loss 5.46126, acc 0.765625, prec 0.0245625, recall 0.705714
2017-12-10T15:07:08.196342: step 342, loss 0.862086, acc 0.75, prec 0.0245234, recall 0.705714
2017-12-10T15:07:08.388873: step 343, loss 0.863486, acc 0.796875, prec 0.0244918, recall 0.705714
2017-12-10T15:07:08.582521: step 344, loss 6.13726, acc 0.734375, prec 0.0246461, recall 0.705382
2017-12-10T15:07:08.776601: step 345, loss 0.674427, acc 0.84375, prec 0.0247182, recall 0.706215
2017-12-10T15:07:08.966968: step 346, loss 1.0464, acc 0.734375, prec 0.0246767, recall 0.706215
2017-12-10T15:07:09.159021: step 347, loss 1.24807, acc 0.6875, prec 0.0246281, recall 0.706215
2017-12-10T15:07:09.349787: step 348, loss 1.02499, acc 0.78125, prec 0.0245942, recall 0.706215
2017-12-10T15:07:09.539397: step 349, loss 1.11452, acc 0.78125, prec 0.0245604, recall 0.706215
2017-12-10T15:07:09.738785: step 350, loss 2.00931, acc 0.625, prec 0.0245026, recall 0.706215
2017-12-10T15:07:09.932719: step 351, loss 1.30553, acc 0.65625, prec 0.0244499, recall 0.706215
2017-12-10T15:07:10.128833: step 352, loss 3.76532, acc 0.71875, prec 0.0245045, recall 0.705056
2017-12-10T15:07:10.325259: step 353, loss 1.12844, acc 0.765625, prec 0.0244687, recall 0.705056
2017-12-10T15:07:10.523389: step 354, loss 0.981387, acc 0.734375, prec 0.0245232, recall 0.705882
2017-12-10T15:07:10.718502: step 355, loss 1.22733, acc 0.671875, prec 0.0245679, recall 0.706704
2017-12-10T15:07:10.908366: step 356, loss 1.06531, acc 0.796875, prec 0.0245369, recall 0.706704
2017-12-10T15:07:11.104892: step 357, loss 1.23173, acc 0.640625, prec 0.0244823, recall 0.706704
2017-12-10T15:07:11.294936: step 358, loss 5.36252, acc 0.6875, prec 0.0245316, recall 0.705556
2017-12-10T15:07:11.488262: step 359, loss 0.629571, acc 0.796875, prec 0.0245949, recall 0.706371
2017-12-10T15:07:11.681228: step 360, loss 0.947494, acc 0.734375, prec 0.0245546, recall 0.706371
2017-12-10T15:07:11.881262: step 361, loss 2.27835, acc 0.609375, prec 0.0245894, recall 0.707182
2017-12-10T15:07:12.076918: step 362, loss 1.83027, acc 0.828125, prec 0.0245658, recall 0.705234
2017-12-10T15:07:12.268218: step 363, loss 1.27019, acc 0.640625, prec 0.0245117, recall 0.705234
2017-12-10T15:07:12.463394: step 364, loss 0.774924, acc 0.78125, prec 0.0244789, recall 0.705234
2017-12-10T15:07:12.660312: step 365, loss 1.29694, acc 0.6875, prec 0.0244321, recall 0.705234
2017-12-10T15:07:12.852015: step 366, loss 1.0473, acc 0.71875, prec 0.0243902, recall 0.705234
2017-12-10T15:07:13.042185: step 367, loss 1.50043, acc 0.796875, prec 0.0244529, recall 0.706044
2017-12-10T15:07:13.242570: step 368, loss 4.17403, acc 0.703125, prec 0.024689, recall 0.706522
2017-12-10T15:07:13.436708: step 369, loss 0.880828, acc 0.78125, prec 0.0246562, recall 0.706522
2017-12-10T15:07:13.630155: step 370, loss 1.00829, acc 0.75, prec 0.0248036, recall 0.708108
2017-12-10T15:07:13.826107: step 371, loss 1.1196, acc 0.765625, prec 0.0247684, recall 0.708108
2017-12-10T15:07:14.024566: step 372, loss 1.2802, acc 0.65625, prec 0.024717, recall 0.708108
2017-12-10T15:07:14.224486: step 373, loss 1.32967, acc 0.703125, prec 0.0246728, recall 0.708108
2017-12-10T15:07:14.420536: step 374, loss 0.990566, acc 0.765625, prec 0.024638, recall 0.708108
2017-12-10T15:07:14.614179: step 375, loss 0.715429, acc 0.8125, prec 0.0246102, recall 0.708108
2017-12-10T15:07:14.803735: step 376, loss 0.573672, acc 0.828125, prec 0.0246763, recall 0.708895
2017-12-10T15:07:14.997400: step 377, loss 0.689892, acc 0.875, prec 0.0247492, recall 0.709677
2017-12-10T15:07:15.194102: step 378, loss 0.697286, acc 0.78125, prec 0.0248081, recall 0.710456
2017-12-10T15:07:15.388653: step 379, loss 9.35149, acc 0.734375, prec 0.0248621, recall 0.709333
2017-12-10T15:07:15.578550: step 380, loss 1.95674, acc 0.765625, prec 0.0249207, recall 0.708223
2017-12-10T15:07:15.769745: step 381, loss 1.11203, acc 0.78125, prec 0.024979, recall 0.708995
2017-12-10T15:07:15.960130: step 382, loss 0.834546, acc 0.75, prec 0.0249418, recall 0.708995
2017-12-10T15:07:16.152892: step 383, loss 1.15449, acc 0.703125, prec 0.0248978, recall 0.708995
2017-12-10T15:07:16.342244: step 384, loss 0.650389, acc 0.8125, prec 0.0248701, recall 0.708995
2017-12-10T15:07:16.532811: step 385, loss 0.778762, acc 0.796875, prec 0.0249305, recall 0.709763
2017-12-10T15:07:16.729969: step 386, loss 0.992656, acc 0.703125, prec 0.0248867, recall 0.709763
2017-12-10T15:07:16.921332: step 387, loss 1.00124, acc 0.6875, prec 0.0249307, recall 0.710526
2017-12-10T15:07:17.113445: step 388, loss 0.975198, acc 0.765625, prec 0.0248963, recall 0.710526
2017-12-10T15:07:17.305535: step 389, loss 0.489886, acc 0.828125, prec 0.024871, recall 0.710526
2017-12-10T15:07:17.502796: step 390, loss 1.0172, acc 0.765625, prec 0.0248367, recall 0.710526
2017-12-10T15:07:17.696672: step 391, loss 1.3417, acc 0.859375, prec 0.0249954, recall 0.712042
2017-12-10T15:07:17.889585: step 392, loss 0.549951, acc 0.796875, prec 0.0249656, recall 0.712042
2017-12-10T15:07:18.080964: step 393, loss 0.496631, acc 0.84375, prec 0.0249427, recall 0.712042
2017-12-10T15:07:18.274658: step 394, loss 5.07429, acc 0.84375, prec 0.0249221, recall 0.710183
2017-12-10T15:07:18.473617: step 395, loss 0.814485, acc 0.78125, prec 0.0248902, recall 0.710183
2017-12-10T15:07:18.663044: step 396, loss 0.52547, acc 0.875, prec 0.0249611, recall 0.710938
2017-12-10T15:07:18.853396: step 397, loss 1.63924, acc 0.8125, prec 0.0250228, recall 0.711688
2017-12-10T15:07:19.045691: step 398, loss 1.29, acc 0.71875, prec 0.0250707, recall 0.712435
2017-12-10T15:07:19.237959: step 399, loss 0.89166, acc 0.78125, prec 0.0251275, recall 0.713178
2017-12-10T15:07:19.429254: step 400, loss 3.82042, acc 0.75, prec 0.0250932, recall 0.71134
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_1/1512936356/checkpoints/model-400

2017-12-10T15:07:20.751113: step 401, loss 1.23848, acc 0.8125, prec 0.0251544, recall 0.712082
2017-12-10T15:07:20.945460: step 402, loss 0.85845, acc 0.78125, prec 0.0251224, recall 0.712082
2017-12-10T15:07:21.139703: step 403, loss 0.792085, acc 0.8125, prec 0.0250951, recall 0.712082
2017-12-10T15:07:21.331191: step 404, loss 1.02971, acc 0.6875, prec 0.0250497, recall 0.712082
2017-12-10T15:07:21.521299: step 405, loss 1.50229, acc 0.765625, prec 0.0251038, recall 0.712821
2017-12-10T15:07:21.715324: step 406, loss 1.08195, acc 0.71875, prec 0.025151, recall 0.713555
2017-12-10T15:07:21.909092: step 407, loss 1.29027, acc 0.671875, prec 0.0251912, recall 0.714286
2017-12-10T15:07:22.102815: step 408, loss 1.3215, acc 0.765625, prec 0.0252448, recall 0.715013
2017-12-10T15:07:22.297108: step 409, loss 0.9907, acc 0.71875, prec 0.0252915, recall 0.715736
2017-12-10T15:07:22.487869: step 410, loss 0.905386, acc 0.703125, prec 0.0252485, recall 0.715736
2017-12-10T15:07:22.680313: step 411, loss 1.01731, acc 0.734375, prec 0.0252101, recall 0.715736
2017-12-10T15:07:22.874490: step 412, loss 13.3536, acc 0.703125, prec 0.0252566, recall 0.714646
2017-12-10T15:07:23.070948: step 413, loss 1.50086, acc 0.65625, prec 0.0253807, recall 0.71608
2017-12-10T15:07:23.264520: step 414, loss 0.895489, acc 0.75, prec 0.0255179, recall 0.7175
2017-12-10T15:07:23.455280: step 415, loss 1.11801, acc 0.59375, prec 0.0254591, recall 0.7175
2017-12-10T15:07:23.645033: step 416, loss 16.3315, acc 0.765625, prec 0.0254275, recall 0.715711
2017-12-10T15:07:23.841232: step 417, loss 1.06271, acc 0.6875, prec 0.0254687, recall 0.716418
2017-12-10T15:07:24.032996: step 418, loss 2.50103, acc 0.625, prec 0.025417, recall 0.71464
2017-12-10T15:07:24.223948: step 419, loss 1.39604, acc 0.703125, prec 0.0254603, recall 0.715347
2017-12-10T15:07:24.418032: step 420, loss 2.34116, acc 0.59375, prec 0.0254878, recall 0.716049
2017-12-10T15:07:24.614585: step 421, loss 1.91417, acc 0.640625, prec 0.0256073, recall 0.717445
2017-12-10T15:07:24.809641: step 422, loss 1.34776, acc 0.625, prec 0.0256388, recall 0.718137
2017-12-10T15:07:25.006348: step 423, loss 1.46807, acc 0.640625, prec 0.0255873, recall 0.718137
2017-12-10T15:07:25.206594: step 424, loss 1.46625, acc 0.59375, prec 0.0256142, recall 0.718826
2017-12-10T15:07:25.398877: step 425, loss 5.00407, acc 0.59375, prec 0.0255585, recall 0.717073
2017-12-10T15:07:25.593037: step 426, loss 1.43688, acc 0.484375, prec 0.0254854, recall 0.717073
2017-12-10T15:07:25.785520: step 427, loss 1.38633, acc 0.625, prec 0.0254325, recall 0.717073
2017-12-10T15:07:25.980621: step 428, loss 1.4626, acc 0.65625, prec 0.0254684, recall 0.717762
2017-12-10T15:07:26.172933: step 429, loss 0.962769, acc 0.6875, prec 0.0255924, recall 0.719128
2017-12-10T15:07:26.362586: step 430, loss 1.82474, acc 0.5625, prec 0.0256146, recall 0.719807
2017-12-10T15:07:26.553737: step 431, loss 7.16958, acc 0.625, prec 0.025564, recall 0.718072
2017-12-10T15:07:26.749100: step 432, loss 2.03418, acc 0.53125, prec 0.0254984, recall 0.718072
2017-12-10T15:07:26.947562: step 433, loss 1.25848, acc 0.640625, prec 0.0255316, recall 0.71875
2017-12-10T15:07:27.139395: step 434, loss 0.927911, acc 0.703125, prec 0.0254902, recall 0.71875
2017-12-10T15:07:27.332107: step 435, loss 1.30936, acc 0.65625, prec 0.0254425, recall 0.71875
2017-12-10T15:07:27.527433: step 436, loss 1.17082, acc 0.609375, prec 0.0253885, recall 0.71875
2017-12-10T15:07:27.724885: step 437, loss 1.12762, acc 0.796875, prec 0.0253605, recall 0.71875
2017-12-10T15:07:27.915974: step 438, loss 11.7824, acc 0.765625, prec 0.025413, recall 0.717703
2017-12-10T15:07:28.113367: step 439, loss 2.31565, acc 0.796875, prec 0.0255499, recall 0.719048
2017-12-10T15:07:28.308734: step 440, loss 0.999637, acc 0.703125, prec 0.0255912, recall 0.719715
2017-12-10T15:07:28.503123: step 441, loss 1.04688, acc 0.703125, prec 0.0257967, recall 0.721698
2017-12-10T15:07:28.698949: step 442, loss 0.814519, acc 0.75, prec 0.0257619, recall 0.721698
2017-12-10T15:07:28.895514: step 443, loss 5.6149, acc 0.859375, prec 0.0258265, recall 0.720657
2017-12-10T15:07:29.089027: step 444, loss 5.74767, acc 0.65625, prec 0.0258628, recall 0.719626
2017-12-10T15:07:29.290098: step 445, loss 0.82519, acc 0.71875, prec 0.0259054, recall 0.72028
2017-12-10T15:07:29.483572: step 446, loss 1.68743, acc 0.5625, prec 0.0259262, recall 0.72093
2017-12-10T15:07:29.675549: step 447, loss 1.58533, acc 0.515625, prec 0.0259404, recall 0.721578
2017-12-10T15:07:29.863327: step 448, loss 1.58788, acc 0.59375, prec 0.0259654, recall 0.722222
2017-12-10T15:07:30.059438: step 449, loss 2.15994, acc 0.515625, prec 0.0258986, recall 0.722222
2017-12-10T15:07:30.252144: step 450, loss 1.66415, acc 0.640625, prec 0.0260913, recall 0.724138
2017-12-10T15:07:30.444916: step 451, loss 1.6344, acc 0.53125, prec 0.0260266, recall 0.724138
2017-12-10T15:07:30.640658: step 452, loss 1.47351, acc 0.609375, prec 0.025973, recall 0.724138
2017-12-10T15:07:30.840326: step 453, loss 2.10514, acc 0.609375, prec 0.0260798, recall 0.7254
2017-12-10T15:07:31.036665: step 454, loss 1.7582, acc 0.609375, prec 0.0261862, recall 0.726651
2017-12-10T15:07:31.235099: step 455, loss 1.0055, acc 0.6875, prec 0.0262231, recall 0.727273
2017-12-10T15:07:31.429338: step 456, loss 1.56977, acc 0.609375, prec 0.0262491, recall 0.727891
2017-12-10T15:07:31.620683: step 457, loss 0.97143, acc 0.640625, prec 0.0262793, recall 0.728507
2017-12-10T15:07:31.815416: step 458, loss 0.715717, acc 0.828125, prec 0.0262557, recall 0.728507
2017-12-10T15:07:32.009584: step 459, loss 8.32651, acc 0.734375, prec 0.0262215, recall 0.726862
2017-12-10T15:07:32.201420: step 460, loss 0.490615, acc 0.765625, prec 0.0261895, recall 0.726862
2017-12-10T15:07:32.392517: step 461, loss 1.1056, acc 0.703125, prec 0.0261491, recall 0.726862
2017-12-10T15:07:32.586957: step 462, loss 4.20152, acc 0.796875, prec 0.0262026, recall 0.725843
2017-12-10T15:07:32.782593: step 463, loss 0.748983, acc 0.796875, prec 0.026175, recall 0.725843
2017-12-10T15:07:32.972381: step 464, loss 0.645179, acc 0.8125, prec 0.0261496, recall 0.725843
2017-12-10T15:07:33.160645: step 465, loss 0.645376, acc 0.828125, prec 0.0261263, recall 0.725843
2017-12-10T15:07:33.351680: step 466, loss 0.582561, acc 0.796875, prec 0.0260989, recall 0.725843
2017-12-10T15:07:33.542458: step 467, loss 0.665439, acc 0.84375, prec 0.0261565, recall 0.726457
2017-12-10T15:07:33.738272: step 468, loss 3.24254, acc 0.8125, prec 0.0261332, recall 0.724832
2017-12-10T15:07:33.934322: step 469, loss 0.91106, acc 0.78125, prec 0.0261038, recall 0.724832
2017-12-10T15:07:34.125516: step 470, loss 1.0593, acc 0.75, prec 0.0261485, recall 0.725446
2017-12-10T15:07:34.316044: step 471, loss 1.81713, acc 0.734375, prec 0.0261911, recall 0.726058
2017-12-10T15:07:34.510872: step 472, loss 0.619151, acc 0.8125, prec 0.0261658, recall 0.726058
2017-12-10T15:07:34.703055: step 473, loss 0.948269, acc 0.734375, prec 0.0262082, recall 0.726667
2017-12-10T15:07:34.894537: step 474, loss 1.13761, acc 0.640625, prec 0.02616, recall 0.726667
2017-12-10T15:07:35.085111: step 475, loss 1.05476, acc 0.765625, prec 0.0261286, recall 0.726667
2017-12-10T15:07:35.281515: step 476, loss 0.766437, acc 0.703125, prec 0.026089, recall 0.726667
2017-12-10T15:07:35.470073: step 477, loss 0.732696, acc 0.71875, prec 0.0260516, recall 0.726667
2017-12-10T15:07:35.663102: step 478, loss 3.63867, acc 0.703125, prec 0.0260143, recall 0.725055
2017-12-10T15:07:35.854345: step 479, loss 0.695728, acc 0.734375, prec 0.0259792, recall 0.725055
2017-12-10T15:07:36.049081: step 480, loss 0.667928, acc 0.828125, prec 0.0259565, recall 0.725055
2017-12-10T15:07:36.243848: step 481, loss 1.0734, acc 0.828125, prec 0.0260111, recall 0.725664
2017-12-10T15:07:36.438146: step 482, loss 4.18441, acc 0.796875, prec 0.0259864, recall 0.724062
2017-12-10T15:07:36.638647: step 483, loss 0.963678, acc 0.765625, prec 0.0259555, recall 0.724062
2017-12-10T15:07:36.830127: step 484, loss 9.28327, acc 0.765625, prec 0.0259289, recall 0.720879
2017-12-10T15:07:37.024084: step 485, loss 0.875456, acc 0.75, prec 0.025973, recall 0.721491
2017-12-10T15:07:37.216254: step 486, loss 1.58384, acc 0.59375, prec 0.0259965, recall 0.722101
2017-12-10T15:07:37.408605: step 487, loss 1.2693, acc 0.625, prec 0.0260241, recall 0.722707
2017-12-10T15:07:37.597183: step 488, loss 1.53225, acc 0.609375, prec 0.025973, recall 0.722707
2017-12-10T15:07:37.791405: step 489, loss 1.30429, acc 0.5625, prec 0.0259161, recall 0.722707
2017-12-10T15:07:37.981437: step 490, loss 1.65804, acc 0.65625, prec 0.0259476, recall 0.723312
2017-12-10T15:07:38.173030: step 491, loss 2.22557, acc 0.546875, prec 0.0259649, recall 0.723913
2017-12-10T15:07:38.363531: step 492, loss 1.8465, acc 0.5625, prec 0.0259841, recall 0.724512
2017-12-10T15:07:38.553415: step 493, loss 1.12769, acc 0.578125, prec 0.0259297, recall 0.724512
2017-12-10T15:07:38.749776: step 494, loss 1.95434, acc 0.640625, prec 0.0260344, recall 0.725702
2017-12-10T15:07:38.940903: step 495, loss 1.55949, acc 0.609375, prec 0.0259841, recall 0.725702
2017-12-10T15:07:39.134587: step 496, loss 1.14132, acc 0.796875, prec 0.0261085, recall 0.726882
2017-12-10T15:07:39.310910: step 497, loss 3.88132, acc 0.686275, prec 0.0261534, recall 0.72591
2017-12-10T15:07:39.516102: step 498, loss 1.08659, acc 0.640625, prec 0.026182, recall 0.726496
2017-12-10T15:07:39.711351: step 499, loss 0.695583, acc 0.796875, prec 0.0261559, recall 0.726496
2017-12-10T15:07:39.903830: step 500, loss 1.19173, acc 0.59375, prec 0.0261784, recall 0.727079
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_1/1512936356/checkpoints/model-500

2017-12-10T15:07:41.183829: step 501, loss 1.0258, acc 0.765625, prec 0.0261483, recall 0.727079
2017-12-10T15:07:41.371622: step 502, loss 1.0344, acc 0.65625, prec 0.0261788, recall 0.72766
2017-12-10T15:07:41.562598: step 503, loss 7.97162, acc 0.734375, prec 0.0262957, recall 0.727273
2017-12-10T15:07:41.758598: step 504, loss 0.90247, acc 0.78125, prec 0.0264162, recall 0.728421
2017-12-10T15:07:41.951246: step 505, loss 0.676106, acc 0.796875, prec 0.0263901, recall 0.728421
2017-12-10T15:07:42.140615: step 506, loss 0.735052, acc 0.765625, prec 0.0264341, recall 0.728992
2017-12-10T15:07:42.339556: step 507, loss 0.509487, acc 0.78125, prec 0.0264059, recall 0.728992
2017-12-10T15:07:42.532394: step 508, loss 0.369451, acc 0.84375, prec 0.0263858, recall 0.728992
2017-12-10T15:07:42.726138: step 509, loss 1.01402, acc 0.703125, prec 0.0264217, recall 0.72956
2017-12-10T15:07:42.916954: step 510, loss 0.656139, acc 0.75, prec 0.0263896, recall 0.72956
2017-12-10T15:07:43.109921: step 511, loss 9.63985, acc 0.65625, prec 0.0264214, recall 0.728601
2017-12-10T15:07:43.306405: step 512, loss 0.624415, acc 0.8125, prec 0.0263974, recall 0.728601
2017-12-10T15:07:43.501768: step 513, loss 0.684285, acc 0.875, prec 0.026455, recall 0.729167
2017-12-10T15:07:43.692199: step 514, loss 0.443068, acc 0.875, prec 0.026439, recall 0.729167
2017-12-10T15:07:43.880610: step 515, loss 0.676021, acc 0.828125, prec 0.0264171, recall 0.729167
2017-12-10T15:07:44.076457: step 516, loss 0.545394, acc 0.78125, prec 0.0263892, recall 0.729167
2017-12-10T15:07:44.272921: step 517, loss 2.20073, acc 0.734375, prec 0.0263574, recall 0.727651
2017-12-10T15:07:44.470016: step 518, loss 0.550511, acc 0.828125, prec 0.0264821, recall 0.728778
2017-12-10T15:07:44.664227: step 519, loss 1.47036, acc 0.71875, prec 0.0265194, recall 0.729339
2017-12-10T15:07:44.860509: step 520, loss 0.595645, acc 0.765625, prec 0.0266357, recall 0.730453
2017-12-10T15:07:45.052940: step 521, loss 1.34391, acc 0.796875, prec 0.0267556, recall 0.731557
2017-12-10T15:07:45.248513: step 522, loss 0.948476, acc 0.75, prec 0.0267964, recall 0.732106
2017-12-10T15:07:45.449075: step 523, loss 0.906263, acc 0.8125, prec 0.0267724, recall 0.732106
2017-12-10T15:07:45.647454: step 524, loss 1.10835, acc 0.890625, prec 0.0268311, recall 0.732653
2017-12-10T15:07:45.843086: step 525, loss 1.68943, acc 0.703125, prec 0.0268657, recall 0.733198
2017-12-10T15:07:46.034921: step 526, loss 0.8386, acc 0.75, prec 0.0269062, recall 0.73374
2017-12-10T15:07:46.225950: step 527, loss 0.934331, acc 0.75, prec 0.0269466, recall 0.73428
2017-12-10T15:07:46.421946: step 528, loss 1.31434, acc 0.765625, prec 0.0269888, recall 0.734818
2017-12-10T15:07:46.616574: step 529, loss 0.734807, acc 0.8125, prec 0.0269648, recall 0.734818
2017-12-10T15:07:46.814344: step 530, loss 0.638418, acc 0.828125, prec 0.027015, recall 0.735354
2017-12-10T15:07:47.012481: step 531, loss 0.829745, acc 0.828125, prec 0.0270651, recall 0.735887
2017-12-10T15:07:47.206699: step 532, loss 0.34415, acc 0.84375, prec 0.0271171, recall 0.736418
2017-12-10T15:07:47.401898: step 533, loss 0.741235, acc 0.78125, prec 0.027161, recall 0.736948
2017-12-10T15:07:47.597292: step 534, loss 0.581424, acc 0.796875, prec 0.0272069, recall 0.737475
2017-12-10T15:07:47.794117: step 535, loss 0.801153, acc 0.765625, prec 0.0271767, recall 0.737475
2017-12-10T15:07:47.993144: step 536, loss 0.395412, acc 0.84375, prec 0.0271567, recall 0.737475
2017-12-10T15:07:48.184658: step 537, loss 0.575354, acc 0.84375, prec 0.0271366, recall 0.737475
2017-12-10T15:07:48.382243: step 538, loss 0.411968, acc 0.890625, prec 0.0271943, recall 0.738
2017-12-10T15:07:48.570777: step 539, loss 0.688041, acc 0.75, prec 0.0271623, recall 0.738
2017-12-10T15:07:48.766829: step 540, loss 6.03401, acc 0.890625, prec 0.0272219, recall 0.737052
2017-12-10T15:07:48.963271: step 541, loss 1.62268, acc 0.75, prec 0.0272614, recall 0.737575
2017-12-10T15:07:49.162768: step 542, loss 0.956823, acc 0.78125, prec 0.0272334, recall 0.737575
2017-12-10T15:07:49.354359: step 543, loss 2.55843, acc 0.828125, prec 0.0272134, recall 0.736111
2017-12-10T15:07:49.549090: step 544, loss 0.544484, acc 0.84375, prec 0.0272647, recall 0.736634
2017-12-10T15:07:49.738638: step 545, loss 0.631151, acc 0.84375, prec 0.027316, recall 0.737154
2017-12-10T15:07:49.931097: step 546, loss 0.838701, acc 0.734375, prec 0.027282, recall 0.737154
2017-12-10T15:07:50.124975: step 547, loss 7.73034, acc 0.8125, prec 0.0272601, recall 0.7357
2017-12-10T15:07:50.325572: step 548, loss 0.683593, acc 0.78125, prec 0.0273033, recall 0.73622
2017-12-10T15:07:50.520967: step 549, loss 0.879754, acc 0.75, prec 0.0272714, recall 0.73622
2017-12-10T15:07:50.712247: step 550, loss 3.20327, acc 0.734375, prec 0.0273105, recall 0.735294
2017-12-10T15:07:50.908348: step 551, loss 1.24786, acc 0.546875, prec 0.0273943, recall 0.736328
2017-12-10T15:07:51.101019: step 552, loss 1.66303, acc 0.625, prec 0.0273466, recall 0.736328
2017-12-10T15:07:51.293366: step 553, loss 1.54996, acc 0.671875, prec 0.0274459, recall 0.737354
2017-12-10T15:07:51.485388: step 554, loss 1.74832, acc 0.578125, prec 0.0274626, recall 0.737864
2017-12-10T15:07:51.675808: step 555, loss 1.57696, acc 0.609375, prec 0.0274832, recall 0.738372
2017-12-10T15:07:51.871545: step 556, loss 1.50783, acc 0.5625, prec 0.0274278, recall 0.738372
2017-12-10T15:07:52.067883: step 557, loss 1.75771, acc 0.53125, prec 0.0273687, recall 0.738372
2017-12-10T15:07:52.260025: step 558, loss 1.77625, acc 0.5625, prec 0.0273835, recall 0.738878
2017-12-10T15:07:52.455813: step 559, loss 0.950937, acc 0.703125, prec 0.0274159, recall 0.739382
2017-12-10T15:07:52.647882: step 560, loss 1.01863, acc 0.609375, prec 0.0273669, recall 0.739382
2017-12-10T15:07:52.842988: step 561, loss 1.24766, acc 0.65625, prec 0.0273934, recall 0.739884
2017-12-10T15:07:53.036867: step 562, loss 1.18822, acc 0.640625, prec 0.0273485, recall 0.739884
2017-12-10T15:07:53.240026: step 563, loss 2.67019, acc 0.734375, prec 0.0274557, recall 0.739464
2017-12-10T15:07:53.435256: step 564, loss 0.674879, acc 0.796875, prec 0.0276377, recall 0.740952
2017-12-10T15:07:53.629511: step 565, loss 5.86909, acc 0.8125, prec 0.0276161, recall 0.739544
2017-12-10T15:07:53.822350: step 566, loss 0.708429, acc 0.71875, prec 0.0275808, recall 0.739544
2017-12-10T15:07:54.010615: step 567, loss 0.84773, acc 0.828125, prec 0.0275593, recall 0.739544
2017-12-10T15:07:54.201326: step 568, loss 0.794743, acc 0.78125, prec 0.027532, recall 0.739544
2017-12-10T15:07:54.392584: step 569, loss 0.605554, acc 0.765625, prec 0.0277091, recall 0.741021
2017-12-10T15:07:54.586917: step 570, loss 0.498548, acc 0.828125, prec 0.0277562, recall 0.741509
2017-12-10T15:07:54.777122: step 571, loss 0.850127, acc 0.734375, prec 0.0277229, recall 0.741509
2017-12-10T15:07:54.971495: step 572, loss 0.598821, acc 0.875, prec 0.0277758, recall 0.741996
2017-12-10T15:07:55.169036: step 573, loss 0.831555, acc 0.78125, prec 0.0278169, recall 0.742481
2017-12-10T15:07:55.360713: step 574, loss 2.13637, acc 0.75, prec 0.0277875, recall 0.741088
2017-12-10T15:07:55.555051: step 575, loss 0.361196, acc 0.921875, prec 0.0279145, recall 0.742056
2017-12-10T15:07:55.753631: step 576, loss 6.79246, acc 0.6875, prec 0.0278773, recall 0.740672
2017-12-10T15:07:55.952472: step 577, loss 2.75489, acc 0.734375, prec 0.027846, recall 0.739292
2017-12-10T15:07:56.146322: step 578, loss 0.709966, acc 0.765625, prec 0.0278848, recall 0.739777
2017-12-10T15:07:56.338852: step 579, loss 0.7465, acc 0.734375, prec 0.0279877, recall 0.740741
2017-12-10T15:07:56.537414: step 580, loss 0.925461, acc 0.71875, prec 0.0280204, recall 0.74122
2017-12-10T15:07:56.729932: step 581, loss 1.29657, acc 0.703125, prec 0.0279833, recall 0.74122
2017-12-10T15:07:56.923448: step 582, loss 0.85234, acc 0.765625, prec 0.0280217, recall 0.741697
2017-12-10T15:07:57.112466: step 583, loss 0.68589, acc 0.734375, prec 0.0279886, recall 0.741697
2017-12-10T15:07:57.305323: step 584, loss 6.83169, acc 0.734375, prec 0.0280926, recall 0.741284
2017-12-10T15:07:57.501184: step 585, loss 1.09121, acc 0.703125, prec 0.0281905, recall 0.74223
2017-12-10T15:07:57.694942: step 586, loss 1.1003, acc 0.671875, prec 0.0281495, recall 0.74223
2017-12-10T15:07:57.888500: step 587, loss 1.45518, acc 0.625, prec 0.02817, recall 0.742701
2017-12-10T15:07:58.078227: step 588, loss 9.12286, acc 0.515625, prec 0.0282459, recall 0.742287
2017-12-10T15:07:58.276443: step 589, loss 0.972945, acc 0.71875, prec 0.0282108, recall 0.742287
2017-12-10T15:07:58.471516: step 590, loss 1.08151, acc 0.640625, prec 0.0281661, recall 0.742287
2017-12-10T15:07:58.660766: step 591, loss 1.41325, acc 0.5, prec 0.0281042, recall 0.742287
2017-12-10T15:07:58.851308: step 592, loss 3.87046, acc 0.609375, prec 0.0281246, recall 0.74141
2017-12-10T15:07:59.045304: step 593, loss 0.971936, acc 0.765625, prec 0.028362, recall 0.743267
2017-12-10T15:07:59.243313: step 594, loss 1.18776, acc 0.578125, prec 0.0283761, recall 0.743728
2017-12-10T15:07:59.433207: step 595, loss 1.62179, acc 0.6875, prec 0.02847, recall 0.744643
2017-12-10T15:07:59.629118: step 596, loss 1.84155, acc 0.53125, prec 0.028478, recall 0.745098
2017-12-10T15:07:59.822943: step 597, loss 3.07777, acc 0.59375, prec 0.0285598, recall 0.746004
2017-12-10T15:08:00.014400: step 598, loss 1.64361, acc 0.578125, prec 0.0285074, recall 0.746004
2017-12-10T15:08:00.210287: step 599, loss 1.31144, acc 0.6875, prec 0.0285346, recall 0.746454
2017-12-10T15:08:00.400973: step 600, loss 1.4153, acc 0.546875, prec 0.0284787, recall 0.746454
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_1/1512936356/checkpoints/model-600

2017-12-10T15:08:01.648770: step 601, loss 0.993172, acc 0.671875, prec 0.0284383, recall 0.746454
2017-12-10T15:08:01.842847: step 602, loss 1.12797, acc 0.65625, prec 0.0285271, recall 0.74735
2017-12-10T15:08:02.034498: step 603, loss 1.07174, acc 0.75, prec 0.0284964, recall 0.74735
2017-12-10T15:08:02.224467: step 604, loss 0.853161, acc 0.765625, prec 0.0286637, recall 0.748682
2017-12-10T15:08:02.417964: step 605, loss 0.635491, acc 0.796875, prec 0.0288345, recall 0.75
2017-12-10T15:08:02.613765: step 606, loss 0.719231, acc 0.796875, prec 0.0288093, recall 0.75
2017-12-10T15:08:02.806028: step 607, loss 0.805969, acc 0.78125, prec 0.0287823, recall 0.75
2017-12-10T15:08:02.998023: step 608, loss 0.684937, acc 0.8125, prec 0.0288893, recall 0.750871
2017-12-10T15:08:03.193881: step 609, loss 0.613956, acc 0.828125, prec 0.0289981, recall 0.751736
2017-12-10T15:08:03.384363: step 610, loss 14.7509, acc 0.828125, prec 0.0289787, recall 0.750433
2017-12-10T15:08:03.576513: step 611, loss 0.356487, acc 0.796875, prec 0.0289535, recall 0.750433
2017-12-10T15:08:03.772816: step 612, loss 3.05334, acc 0.828125, prec 0.0289991, recall 0.749568
2017-12-10T15:08:03.965152: step 613, loss 0.231549, acc 0.90625, prec 0.0289874, recall 0.749568
2017-12-10T15:08:04.159491: step 614, loss 3.12547, acc 0.734375, prec 0.0289565, recall 0.748276
2017-12-10T15:08:04.358267: step 615, loss 1.10734, acc 0.765625, prec 0.0289923, recall 0.748709
2017-12-10T15:08:04.556088: step 616, loss 0.597384, acc 0.765625, prec 0.029028, recall 0.749141
2017-12-10T15:08:04.752388: step 617, loss 1.05077, acc 0.6875, prec 0.0289894, recall 0.749141
2017-12-10T15:08:04.949180: step 618, loss 0.981648, acc 0.796875, prec 0.0289643, recall 0.749141
2017-12-10T15:08:05.141324: step 619, loss 0.682245, acc 0.765625, prec 0.0289355, recall 0.749141
2017-12-10T15:08:05.332705: step 620, loss 2.05149, acc 0.75, prec 0.0289067, recall 0.747856
2017-12-10T15:08:05.530301: step 621, loss 1.33322, acc 0.578125, prec 0.0288551, recall 0.747856
2017-12-10T15:08:05.726172: step 622, loss 1.14276, acc 0.703125, prec 0.0288188, recall 0.747856
2017-12-10T15:08:05.922593: step 623, loss 1.42976, acc 0.640625, prec 0.0289673, recall 0.749147
2017-12-10T15:08:06.114822: step 624, loss 0.793149, acc 0.75, prec 0.0290008, recall 0.749574
2017-12-10T15:08:06.309082: step 625, loss 1.53764, acc 0.640625, prec 0.0290847, recall 0.750424
2017-12-10T15:08:06.503680: step 626, loss 1.30835, acc 0.671875, prec 0.0291721, recall 0.751269
2017-12-10T15:08:06.700062: step 627, loss 1.41477, acc 0.71875, prec 0.0292651, recall 0.752108
2017-12-10T15:08:06.892364: step 628, loss 0.912679, acc 0.765625, prec 0.0292363, recall 0.752108
2017-12-10T15:08:07.085254: step 629, loss 0.839244, acc 0.671875, prec 0.0292597, recall 0.752525
2017-12-10T15:08:07.276692: step 630, loss 0.539619, acc 0.765625, prec 0.029231, recall 0.752525
2017-12-10T15:08:07.474115: step 631, loss 0.708853, acc 0.703125, prec 0.0291947, recall 0.752525
2017-12-10T15:08:07.668120: step 632, loss 3.83244, acc 0.859375, prec 0.0291795, recall 0.751261
2017-12-10T15:08:07.866600: step 633, loss 3.10054, acc 0.765625, prec 0.0292161, recall 0.750419
2017-12-10T15:08:08.060958: step 634, loss 1.69388, acc 0.796875, prec 0.0293179, recall 0.751252
2017-12-10T15:08:08.253168: step 635, loss 0.89059, acc 0.796875, prec 0.0293562, recall 0.751667
2017-12-10T15:08:08.443430: step 636, loss 0.569964, acc 0.75, prec 0.0293257, recall 0.751667
2017-12-10T15:08:08.640243: step 637, loss 1.15679, acc 0.71875, prec 0.0293545, recall 0.75208
2017-12-10T15:08:08.835510: step 638, loss 0.825055, acc 0.71875, prec 0.0293202, recall 0.75208
2017-12-10T15:08:09.036230: step 639, loss 3.14987, acc 0.734375, prec 0.0293527, recall 0.751244
2017-12-10T15:08:09.231508: step 640, loss 1.14265, acc 0.65625, prec 0.0293109, recall 0.751244
2017-12-10T15:08:09.425438: step 641, loss 1.36628, acc 0.65625, prec 0.0293947, recall 0.752066
2017-12-10T15:08:09.618836: step 642, loss 2.32404, acc 0.65625, prec 0.0294801, recall 0.751645
2017-12-10T15:08:09.809386: step 643, loss 0.873523, acc 0.78125, prec 0.0294535, recall 0.751645
2017-12-10T15:08:10.000640: step 644, loss 9.43289, acc 0.578125, prec 0.0294666, recall 0.75082
2017-12-10T15:08:10.197432: step 645, loss 1.39153, acc 0.609375, prec 0.0294193, recall 0.75082
2017-12-10T15:08:10.389128: step 646, loss 1.61513, acc 0.625, prec 0.0294985, recall 0.751634
2017-12-10T15:08:10.581724: step 647, loss 1.41059, acc 0.625, prec 0.0295153, recall 0.752039
2017-12-10T15:08:10.775816: step 648, loss 1.39334, acc 0.625, prec 0.0295941, recall 0.752846
2017-12-10T15:08:10.971472: step 649, loss 2.14945, acc 0.46875, prec 0.0295299, recall 0.752846
2017-12-10T15:08:11.166881: step 650, loss 1.43574, acc 0.6875, prec 0.0294923, recall 0.752846
2017-12-10T15:08:11.359975: step 651, loss 1.43759, acc 0.578125, prec 0.0295034, recall 0.753247
2017-12-10T15:08:11.551080: step 652, loss 1.42489, acc 0.546875, prec 0.0294491, recall 0.753247
2017-12-10T15:08:11.741323: step 653, loss 2.19891, acc 0.71875, prec 0.0295385, recall 0.754045
2017-12-10T15:08:11.933915: step 654, loss 1.31533, acc 0.609375, prec 0.0294918, recall 0.754045
2017-12-10T15:08:12.125027: step 655, loss 3.36942, acc 0.71875, prec 0.0295828, recall 0.753623
2017-12-10T15:08:12.315861: step 656, loss 1.09956, acc 0.71875, prec 0.0296105, recall 0.754019
2017-12-10T15:08:12.508194: step 657, loss 1.57684, acc 0.703125, prec 0.0296362, recall 0.754414
2017-12-10T15:08:12.701613: step 658, loss 1.39331, acc 0.765625, prec 0.0297304, recall 0.7552
2017-12-10T15:08:12.898245: step 659, loss 1.40514, acc 0.671875, prec 0.0296911, recall 0.7552
2017-12-10T15:08:13.092426: step 660, loss 0.62241, acc 0.796875, prec 0.0297279, recall 0.755591
2017-12-10T15:08:13.289130: step 661, loss 1.22767, acc 0.625, prec 0.0296831, recall 0.755591
2017-12-10T15:08:13.479825: step 662, loss 0.683691, acc 0.78125, prec 0.0297179, recall 0.755981
2017-12-10T15:08:13.672931: step 663, loss 0.839329, acc 0.84375, prec 0.02976, recall 0.756369
2017-12-10T15:08:13.865101: step 664, loss 1.34953, acc 0.6875, prec 0.0297835, recall 0.756757
2017-12-10T15:08:14.064806: step 665, loss 0.930558, acc 0.734375, prec 0.0298731, recall 0.757528
2017-12-10T15:08:14.263026: step 666, loss 0.656001, acc 0.8125, prec 0.0298507, recall 0.757528
2017-12-10T15:08:14.455596: step 667, loss 0.560811, acc 0.796875, prec 0.0298871, recall 0.757911
2017-12-10T15:08:14.646082: step 668, loss 0.684715, acc 0.890625, prec 0.029995, recall 0.758675
2017-12-10T15:08:14.845510: step 669, loss 2.24284, acc 0.8125, prec 0.0299744, recall 0.75748
2017-12-10T15:08:15.037094: step 670, loss 1.39806, acc 0.84375, prec 0.0300766, recall 0.758242
2017-12-10T15:08:15.229647: step 671, loss 0.804924, acc 0.765625, prec 0.0300485, recall 0.758242
2017-12-10T15:08:15.421005: step 672, loss 3.12861, acc 0.8125, prec 0.030028, recall 0.757053
2017-12-10T15:08:15.620165: step 673, loss 1.80554, acc 0.765625, prec 0.0301807, recall 0.75819
2017-12-10T15:08:15.821168: step 674, loss 1.00167, acc 0.6875, prec 0.0301433, recall 0.75819
2017-12-10T15:08:16.017851: step 675, loss 1.35551, acc 0.71875, prec 0.0301697, recall 0.758567
2017-12-10T15:08:16.214294: step 676, loss 0.879492, acc 0.75, prec 0.0301999, recall 0.758942
2017-12-10T15:08:16.403626: step 677, loss 1.85603, acc 0.671875, prec 0.0302806, recall 0.75969
2017-12-10T15:08:16.597749: step 678, loss 1.45227, acc 0.75, prec 0.0303105, recall 0.760062
2017-12-10T15:08:16.791645: step 679, loss 5.61541, acc 0.6875, prec 0.0303348, recall 0.759259
2017-12-10T15:08:16.987545: step 680, loss 0.967411, acc 0.71875, prec 0.0304206, recall 0.76
2017-12-10T15:08:17.179108: step 681, loss 1.06262, acc 0.734375, prec 0.0303888, recall 0.76
2017-12-10T15:08:17.371830: step 682, loss 1.32083, acc 0.625, prec 0.030344, recall 0.76
2017-12-10T15:08:17.564992: step 683, loss 0.917786, acc 0.6875, prec 0.0303067, recall 0.76
2017-12-10T15:08:17.759254: step 684, loss 1.48028, acc 0.609375, prec 0.0303197, recall 0.760369
2017-12-10T15:08:17.953151: step 685, loss 0.911168, acc 0.78125, prec 0.0302938, recall 0.760369
2017-12-10T15:08:18.148839: step 686, loss 1.48402, acc 0.65625, prec 0.0303123, recall 0.760736
2017-12-10T15:08:18.338860: step 687, loss 1.52177, acc 0.703125, prec 0.0302771, recall 0.760736
2017-12-10T15:08:18.529662: step 688, loss 1.31753, acc 0.71875, prec 0.0303622, recall 0.761468
2017-12-10T15:08:18.721948: step 689, loss 0.932385, acc 0.734375, prec 0.0303898, recall 0.761832
2017-12-10T15:08:18.914942: step 690, loss 0.742983, acc 0.78125, prec 0.0303639, recall 0.761832
2017-12-10T15:08:19.106791: step 691, loss 0.899134, acc 0.796875, prec 0.0303988, recall 0.762195
2017-12-10T15:08:19.302996: step 692, loss 0.817618, acc 0.75, prec 0.0304282, recall 0.762557
2017-12-10T15:08:19.493918: step 693, loss 0.918845, acc 0.765625, prec 0.0304593, recall 0.762918
2017-12-10T15:08:19.690309: step 694, loss 0.42561, acc 0.828125, prec 0.030439, recall 0.762918
2017-12-10T15:08:19.884897: step 695, loss 0.708851, acc 0.8125, prec 0.0304756, recall 0.763278
2017-12-10T15:08:20.073844: step 696, loss 0.457495, acc 0.828125, prec 0.0305727, recall 0.763994
2017-12-10T15:08:20.265617: step 697, loss 2.42565, acc 0.890625, prec 0.0305616, recall 0.76284
2017-12-10T15:08:20.459620: step 698, loss 3.45354, acc 0.84375, prec 0.0306036, recall 0.762048
2017-12-10T15:08:20.653204: step 699, loss 0.642014, acc 0.78125, prec 0.0305777, recall 0.762048
2017-12-10T15:08:20.843240: step 700, loss 9.46303, acc 0.8125, prec 0.0306159, recall 0.761261
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_1/1512936356/checkpoints/model-700

2017-12-10T15:08:22.583870: step 701, loss 0.974892, acc 0.765625, prec 0.0305882, recall 0.761261
2017-12-10T15:08:22.780674: step 702, loss 4.79636, acc 0.75, prec 0.0305606, recall 0.76012
2017-12-10T15:08:22.975005: step 703, loss 0.777424, acc 0.734375, prec 0.0305877, recall 0.760479
2017-12-10T15:08:23.168795: step 704, loss 0.663766, acc 0.765625, prec 0.0306184, recall 0.760837
2017-12-10T15:08:23.362259: step 705, loss 1.43772, acc 0.640625, prec 0.0306925, recall 0.76155
2017-12-10T15:08:23.552867: step 706, loss 1.04255, acc 0.625, prec 0.0307646, recall 0.762259
2017-12-10T15:08:23.747626: step 707, loss 1.39333, acc 0.6875, prec 0.0309019, recall 0.763314
2017-12-10T15:08:23.940505: step 708, loss 1.05655, acc 0.65625, prec 0.0309192, recall 0.763663
2017-12-10T15:08:24.131574: step 709, loss 1.22451, acc 0.71875, prec 0.030886, recall 0.763663
2017-12-10T15:08:24.326402: step 710, loss 1.77346, acc 0.578125, prec 0.030894, recall 0.764012
2017-12-10T15:08:24.518360: step 711, loss 1.62295, acc 0.625, prec 0.0310807, recall 0.765396
2017-12-10T15:08:24.710277: step 712, loss 1.36687, acc 0.65625, prec 0.0310976, recall 0.765739
2017-12-10T15:08:24.903929: step 713, loss 1.04049, acc 0.640625, prec 0.0310552, recall 0.765739
2017-12-10T15:08:25.098494: step 714, loss 1.10835, acc 0.65625, prec 0.0310146, recall 0.765739
2017-12-10T15:08:25.289379: step 715, loss 0.67622, acc 0.828125, prec 0.0310519, recall 0.766082
2017-12-10T15:08:25.482173: step 716, loss 1.19844, acc 0.640625, prec 0.0310669, recall 0.766423
2017-12-10T15:08:25.679601: step 717, loss 2.59152, acc 0.71875, prec 0.031093, recall 0.765648
2017-12-10T15:08:25.877085: step 718, loss 0.658123, acc 0.8125, prec 0.0310709, recall 0.765648
2017-12-10T15:08:26.072461: step 719, loss 0.701899, acc 0.828125, prec 0.0311652, recall 0.766328
2017-12-10T15:08:26.270263: step 720, loss 0.667447, acc 0.78125, prec 0.0311394, recall 0.766328
2017-12-10T15:08:26.461495: step 721, loss 1.10339, acc 0.8125, prec 0.0312316, recall 0.767004
2017-12-10T15:08:26.654155: step 722, loss 0.55928, acc 0.796875, prec 0.0312077, recall 0.767004
2017-12-10T15:08:26.846982: step 723, loss 0.841486, acc 0.765625, prec 0.0311801, recall 0.767004
2017-12-10T15:08:27.039881: step 724, loss 0.763299, acc 0.765625, prec 0.0311526, recall 0.767004
2017-12-10T15:08:27.237240: step 725, loss 4.16678, acc 0.859375, prec 0.0311949, recall 0.766234
2017-12-10T15:08:27.429115: step 726, loss 0.530271, acc 0.828125, prec 0.0312317, recall 0.766571
2017-12-10T15:08:27.624839: step 727, loss 0.533229, acc 0.828125, prec 0.0312683, recall 0.766906
2017-12-10T15:08:27.821859: step 728, loss 1.22844, acc 0.875, prec 0.0313673, recall 0.767575
2017-12-10T15:08:28.018102: step 729, loss 0.378027, acc 0.859375, prec 0.0313507, recall 0.767575
2017-12-10T15:08:28.214417: step 730, loss 0.688538, acc 0.8125, prec 0.0313287, recall 0.767575
2017-12-10T15:08:28.406763: step 731, loss 0.707673, acc 0.75, prec 0.0312994, recall 0.767575
2017-12-10T15:08:28.599618: step 732, loss 0.588229, acc 0.765625, prec 0.0313852, recall 0.76824
2017-12-10T15:08:28.791259: step 733, loss 0.323065, acc 0.890625, prec 0.0314289, recall 0.768571
2017-12-10T15:08:28.983086: step 734, loss 0.594122, acc 0.8125, prec 0.03152, recall 0.769231
2017-12-10T15:08:29.187246: step 735, loss 0.814213, acc 0.765625, prec 0.0314924, recall 0.769231
2017-12-10T15:08:29.382703: step 736, loss 0.391784, acc 0.859375, prec 0.0314759, recall 0.769231
2017-12-10T15:08:29.578026: step 737, loss 0.47904, acc 0.875, prec 0.0315176, recall 0.769559
2017-12-10T15:08:29.772304: step 738, loss 0.416151, acc 0.890625, prec 0.0315048, recall 0.769559
2017-12-10T15:08:29.968555: step 739, loss 0.219736, acc 0.9375, prec 0.0314974, recall 0.769559
2017-12-10T15:08:30.162619: step 740, loss 1.21992, acc 0.765625, prec 0.0315263, recall 0.769886
2017-12-10T15:08:30.355891: step 741, loss 0.183417, acc 0.9375, prec 0.031519, recall 0.769886
2017-12-10T15:08:30.550693: step 742, loss 0.25812, acc 0.90625, prec 0.0315643, recall 0.770213
2017-12-10T15:08:30.742398: step 743, loss 0.544984, acc 0.875, prec 0.0315496, recall 0.770213
2017-12-10T15:08:30.935691: step 744, loss 0.338533, acc 0.859375, prec 0.0315331, recall 0.770213
2017-12-10T15:08:31.134505: step 745, loss 0.468306, acc 0.953125, prec 0.0315838, recall 0.770538
2017-12-10T15:08:31.335203: step 746, loss 0.367754, acc 0.921875, prec 0.0315747, recall 0.770538
2017-12-10T15:08:31.532169: step 747, loss 0.370487, acc 0.875, prec 0.03156, recall 0.770538
2017-12-10T15:08:31.725681: step 748, loss 0.181058, acc 0.921875, prec 0.0315509, recall 0.770538
2017-12-10T15:08:31.916636: step 749, loss 2.40527, acc 0.859375, prec 0.0316485, recall 0.770099
2017-12-10T15:08:32.107808: step 750, loss 3.21278, acc 0.9375, prec 0.031643, recall 0.769014
2017-12-10T15:08:32.304329: step 751, loss 0.145561, acc 0.96875, prec 0.0316393, recall 0.769014
2017-12-10T15:08:32.495697: step 752, loss 0.371601, acc 0.875, prec 0.0316247, recall 0.769014
2017-12-10T15:08:32.690557: step 753, loss 13.3024, acc 0.859375, prec 0.0316679, recall 0.767181
2017-12-10T15:08:32.887382: step 754, loss 1.14743, acc 0.765625, prec 0.0316965, recall 0.767507
2017-12-10T15:08:33.086664: step 755, loss 3.74537, acc 0.765625, prec 0.0316708, recall 0.766434
2017-12-10T15:08:33.281445: step 756, loss 1.06303, acc 0.609375, prec 0.0316251, recall 0.766434
2017-12-10T15:08:33.478203: step 757, loss 1.25383, acc 0.546875, prec 0.0315723, recall 0.766434
2017-12-10T15:08:33.668301: step 758, loss 1.29377, acc 0.515625, prec 0.031516, recall 0.766434
2017-12-10T15:08:33.865669: step 759, loss 1.63375, acc 0.46875, prec 0.0314545, recall 0.766434
2017-12-10T15:08:34.060307: step 760, loss 2.20603, acc 0.484375, prec 0.0314505, recall 0.76676
2017-12-10T15:08:34.250639: step 761, loss 2.51908, acc 0.375, prec 0.031434, recall 0.767085
2017-12-10T15:08:34.442948: step 762, loss 3.37403, acc 0.53125, prec 0.0314372, recall 0.766342
2017-12-10T15:08:34.638694: step 763, loss 2.07536, acc 0.46875, prec 0.0313763, recall 0.766342
2017-12-10T15:08:34.830703: step 764, loss 2.9045, acc 0.546875, prec 0.0313815, recall 0.765603
2017-12-10T15:08:35.021634: step 765, loss 1.5699, acc 0.53125, prec 0.031383, recall 0.765928
2017-12-10T15:08:35.217677: step 766, loss 1.83638, acc 0.5625, prec 0.0313881, recall 0.766252
2017-12-10T15:08:35.407900: step 767, loss 2.41334, acc 0.5, prec 0.0313861, recall 0.766575
2017-12-10T15:08:35.597959: step 768, loss 1.77528, acc 0.53125, prec 0.0313329, recall 0.766575
2017-12-10T15:08:35.787556: step 769, loss 1.5992, acc 0.578125, prec 0.0313398, recall 0.766897
2017-12-10T15:08:35.977568: step 770, loss 1.32105, acc 0.71875, prec 0.0313626, recall 0.767218
2017-12-10T15:08:36.169002: step 771, loss 1.04723, acc 0.703125, prec 0.0313291, recall 0.767218
2017-12-10T15:08:36.359479: step 772, loss 0.81419, acc 0.71875, prec 0.0314063, recall 0.767857
2017-12-10T15:08:36.553610: step 773, loss 0.675795, acc 0.75, prec 0.0315411, recall 0.76881
2017-12-10T15:08:36.746427: step 774, loss 0.813026, acc 0.75, prec 0.0315128, recall 0.76881
2017-12-10T15:08:36.939084: step 775, loss 6.00238, acc 0.765625, prec 0.0315424, recall 0.768076
2017-12-10T15:08:37.137390: step 776, loss 0.624249, acc 0.78125, prec 0.0315177, recall 0.768076
2017-12-10T15:08:37.332978: step 777, loss 0.749684, acc 0.75, prec 0.0314895, recall 0.768076
2017-12-10T15:08:37.524369: step 778, loss 0.664677, acc 0.796875, prec 0.0314666, recall 0.768076
2017-12-10T15:08:37.717625: step 779, loss 0.679866, acc 0.703125, prec 0.0314332, recall 0.768076
2017-12-10T15:08:37.906091: step 780, loss 0.846287, acc 0.78125, prec 0.0315167, recall 0.768707
2017-12-10T15:08:38.102363: step 781, loss 0.470384, acc 0.859375, prec 0.0315009, recall 0.768707
2017-12-10T15:08:38.294011: step 782, loss 5.02609, acc 0.921875, prec 0.0316018, recall 0.768293
2017-12-10T15:08:38.490550: step 783, loss 2.33469, acc 0.921875, prec 0.0315948, recall 0.767253
2017-12-10T15:08:38.689626: step 784, loss 4.37928, acc 0.8125, prec 0.0315754, recall 0.766216
2017-12-10T15:08:38.883374: step 785, loss 0.597826, acc 0.8125, prec 0.0315543, recall 0.766216
2017-12-10T15:08:39.074878: step 786, loss 0.520392, acc 0.828125, prec 0.031535, recall 0.766216
2017-12-10T15:08:39.271859: step 787, loss 1.16115, acc 0.703125, prec 0.0317169, recall 0.767473
2017-12-10T15:08:39.464353: step 788, loss 0.56771, acc 0.828125, prec 0.0316976, recall 0.767473
2017-12-10T15:08:39.653577: step 789, loss 0.717529, acc 0.734375, prec 0.0316677, recall 0.767473
2017-12-10T15:08:39.848766: step 790, loss 3.28784, acc 0.734375, prec 0.0316396, recall 0.766443
2017-12-10T15:08:40.040196: step 791, loss 8.23055, acc 0.875, prec 0.0317882, recall 0.766355
2017-12-10T15:08:40.234413: step 792, loss 1.32921, acc 0.65625, prec 0.0317495, recall 0.766355
2017-12-10T15:08:40.429876: step 793, loss 1.50581, acc 0.578125, prec 0.0317022, recall 0.766355
2017-12-10T15:08:40.618581: step 794, loss 1.25105, acc 0.671875, prec 0.0317189, recall 0.766667
2017-12-10T15:08:40.812319: step 795, loss 2.05512, acc 0.5625, prec 0.0317233, recall 0.766977
2017-12-10T15:08:41.005462: step 796, loss 2.98876, acc 0.4375, prec 0.0316605, recall 0.766977
2017-12-10T15:08:41.200792: step 797, loss 1.77414, acc 0.453125, prec 0.0316529, recall 0.767287
2017-12-10T15:08:41.400927: step 798, loss 2.12465, acc 0.609375, prec 0.0316095, recall 0.767287
2017-12-10T15:08:41.589090: step 799, loss 2.56471, acc 0.421875, prec 0.0316515, recall 0.767905
2017-12-10T15:08:41.783325: step 800, loss 1.64622, acc 0.640625, prec 0.0316646, recall 0.768212
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_1/1512936356/checkpoints/model-800

2017-12-10T15:08:43.004337: step 801, loss 2.05692, acc 0.5, prec 0.0316094, recall 0.768212
2017-12-10T15:08:43.201436: step 802, loss 1.54882, acc 0.5625, prec 0.0315612, recall 0.768212
2017-12-10T15:08:43.391695: step 803, loss 1.46726, acc 0.65625, prec 0.0315235, recall 0.768212
2017-12-10T15:08:43.581895: step 804, loss 2.06793, acc 0.65625, prec 0.0315384, recall 0.768519
2017-12-10T15:08:43.775750: step 805, loss 1.90162, acc 0.59375, prec 0.0315464, recall 0.768824
2017-12-10T15:08:43.968507: step 806, loss 0.690712, acc 0.75, prec 0.031624, recall 0.769433
2017-12-10T15:08:44.164642: step 807, loss 1.31959, acc 0.765625, prec 0.0317555, recall 0.770341
2017-12-10T15:08:44.364535: step 808, loss 0.55704, acc 0.859375, prec 0.03174, recall 0.770341
2017-12-10T15:08:44.560917: step 809, loss 0.507105, acc 0.828125, prec 0.0317212, recall 0.770341
2017-12-10T15:08:44.753318: step 810, loss 0.502354, acc 0.765625, prec 0.0318523, recall 0.771242
2017-12-10T15:08:44.949029: step 811, loss 0.434191, acc 0.859375, prec 0.0318891, recall 0.77154
2017-12-10T15:08:45.145959: step 812, loss 0.295925, acc 0.90625, prec 0.0318787, recall 0.77154
2017-12-10T15:08:45.337102: step 813, loss 0.983413, acc 0.890625, prec 0.0319711, recall 0.772135
2017-12-10T15:08:45.534786: step 814, loss 0.352014, acc 0.8125, prec 0.0320026, recall 0.772432
2017-12-10T15:08:45.731105: step 815, loss 0.458713, acc 0.890625, prec 0.0320948, recall 0.773022
2017-12-10T15:08:45.926578: step 816, loss 5.46609, acc 0.875, prec 0.0321348, recall 0.772316
2017-12-10T15:08:46.120904: step 817, loss 0.542166, acc 0.828125, prec 0.0321158, recall 0.772316
2017-12-10T15:08:46.312366: step 818, loss 0.417216, acc 0.890625, prec 0.0321037, recall 0.772316
2017-12-10T15:08:46.506655: step 819, loss 5.24794, acc 0.75, prec 0.0321818, recall 0.771907
2017-12-10T15:08:46.705157: step 820, loss 1.26343, acc 0.90625, prec 0.0322754, recall 0.772494
2017-12-10T15:08:46.904720: step 821, loss 11.3857, acc 0.734375, prec 0.0323031, recall 0.769821
2017-12-10T15:08:47.097705: step 822, loss 0.693778, acc 0.75, prec 0.0322754, recall 0.769821
2017-12-10T15:08:47.291654: step 823, loss 1.44155, acc 0.609375, prec 0.0322322, recall 0.769821
2017-12-10T15:08:47.485524: step 824, loss 1.16747, acc 0.6875, prec 0.0321977, recall 0.769821
2017-12-10T15:08:47.680867: step 825, loss 2.19623, acc 0.5, prec 0.0321943, recall 0.770115
2017-12-10T15:08:47.875970: step 826, loss 1.71294, acc 0.53125, prec 0.0321429, recall 0.770115
2017-12-10T15:08:48.065079: step 827, loss 3.12781, acc 0.4375, prec 0.0320813, recall 0.770115
2017-12-10T15:08:48.260831: step 828, loss 2.15328, acc 0.421875, prec 0.0320697, recall 0.770408
2017-12-10T15:08:48.454528: step 829, loss 2.34685, acc 0.40625, prec 0.0320051, recall 0.770408
2017-12-10T15:08:48.647630: step 830, loss 1.99079, acc 0.5625, prec 0.0319577, recall 0.770408
2017-12-10T15:08:48.843025: step 831, loss 1.6819, acc 0.59375, prec 0.0319649, recall 0.770701
2017-12-10T15:08:49.044367: step 832, loss 4.28974, acc 0.5, prec 0.0319637, recall 0.770013
2017-12-10T15:08:49.239586: step 833, loss 1.53098, acc 0.609375, prec 0.0319216, recall 0.770013
2017-12-10T15:08:49.431428: step 834, loss 2.04965, acc 0.546875, prec 0.0318729, recall 0.770013
2017-12-10T15:08:49.625732: step 835, loss 1.36237, acc 0.609375, prec 0.0318819, recall 0.770305
2017-12-10T15:08:49.825467: step 836, loss 1.64925, acc 0.59375, prec 0.0318892, recall 0.770596
2017-12-10T15:08:50.022559: step 837, loss 0.917402, acc 0.734375, prec 0.0319115, recall 0.770886
2017-12-10T15:08:50.216361: step 838, loss 1.38106, acc 0.65625, prec 0.0319761, recall 0.771465
2017-12-10T15:08:50.409391: step 839, loss 1.28337, acc 0.65625, prec 0.03199, recall 0.771753
2017-12-10T15:08:50.601904: step 840, loss 1.19885, acc 0.71875, prec 0.0319599, recall 0.771753
2017-12-10T15:08:50.797203: step 841, loss 1.40951, acc 0.65625, prec 0.0319737, recall 0.77204
2017-12-10T15:08:50.993819: step 842, loss 1.26186, acc 0.671875, prec 0.0319387, recall 0.77204
2017-12-10T15:08:51.190745: step 843, loss 0.889977, acc 0.765625, prec 0.0319642, recall 0.772327
2017-12-10T15:08:51.383284: step 844, loss 0.48064, acc 0.8125, prec 0.0319946, recall 0.772613
2017-12-10T15:08:51.578976: step 845, loss 1.77169, acc 0.890625, prec 0.0320853, recall 0.772215
2017-12-10T15:08:51.773476: step 846, loss 0.306563, acc 0.890625, prec 0.0321239, recall 0.7725
2017-12-10T15:08:51.966398: step 847, loss 0.330293, acc 0.890625, prec 0.0321625, recall 0.772784
2017-12-10T15:08:52.158391: step 848, loss 1.89643, acc 0.921875, prec 0.0323552, recall 0.773913
2017-12-10T15:08:52.364125: step 849, loss 0.608095, acc 0.875, prec 0.032392, recall 0.774194
2017-12-10T15:08:52.556553: step 850, loss 0.356137, acc 0.921875, prec 0.0324338, recall 0.774473
2017-12-10T15:08:52.747373: step 851, loss 0.207812, acc 0.921875, prec 0.0325258, recall 0.775031
2017-12-10T15:08:52.935143: step 852, loss 0.358289, acc 0.875, prec 0.0325625, recall 0.775309
2017-12-10T15:08:53.128419: step 853, loss 0.673091, acc 0.90625, prec 0.0327028, recall 0.776138
2017-12-10T15:08:53.324080: step 854, loss 6.48178, acc 0.953125, prec 0.0327495, recall 0.77546
2017-12-10T15:08:53.517700: step 855, loss 0.405734, acc 0.890625, prec 0.0328378, recall 0.77601
2017-12-10T15:08:53.707499: step 856, loss 0.6434, acc 0.90625, prec 0.0329778, recall 0.776829
2017-12-10T15:08:53.905985: step 857, loss 0.664198, acc 0.796875, prec 0.0329557, recall 0.776829
2017-12-10T15:08:54.103319: step 858, loss 0.532454, acc 0.859375, prec 0.0329903, recall 0.777101
2017-12-10T15:08:54.296559: step 859, loss 0.684488, acc 0.828125, prec 0.0330715, recall 0.777643
2017-12-10T15:08:54.493334: step 860, loss 0.542942, acc 0.78125, prec 0.0330476, recall 0.777643
2017-12-10T15:08:54.689323: step 861, loss 9.01923, acc 0.84375, prec 0.033034, recall 0.775758
2017-12-10T15:08:54.884229: step 862, loss 0.884387, acc 0.765625, prec 0.0330084, recall 0.775758
2017-12-10T15:08:55.078125: step 863, loss 0.887941, acc 0.71875, prec 0.0330276, recall 0.776029
2017-12-10T15:08:55.272984: step 864, loss 0.896095, acc 0.75, prec 0.0330502, recall 0.7763
2017-12-10T15:08:55.466629: step 865, loss 1.57559, acc 0.609375, prec 0.0330077, recall 0.7763
2017-12-10T15:08:55.656155: step 866, loss 0.53003, acc 0.828125, prec 0.0329891, recall 0.7763
2017-12-10T15:08:55.846206: step 867, loss 1.43748, acc 0.703125, prec 0.0331058, recall 0.777108
2017-12-10T15:08:56.045157: step 868, loss 1.15201, acc 0.75, prec 0.0331282, recall 0.777377
2017-12-10T15:08:56.233485: step 869, loss 2.40641, acc 0.640625, prec 0.0331404, recall 0.776711
2017-12-10T15:08:56.429995: step 870, loss 1.11345, acc 0.71875, prec 0.0331099, recall 0.776711
2017-12-10T15:08:56.621427: step 871, loss 1.18631, acc 0.71875, prec 0.0330794, recall 0.776711
2017-12-10T15:08:56.811092: step 872, loss 1.18682, acc 0.640625, prec 0.0331887, recall 0.777512
2017-12-10T15:08:57.003264: step 873, loss 1.1481, acc 0.671875, prec 0.0331531, recall 0.777512
2017-12-10T15:08:57.195725: step 874, loss 1.12367, acc 0.75, prec 0.0331754, recall 0.777778
2017-12-10T15:08:57.390049: step 875, loss 0.73429, acc 0.78125, prec 0.0331517, recall 0.777778
2017-12-10T15:08:57.584170: step 876, loss 0.807262, acc 0.75, prec 0.0331247, recall 0.777778
2017-12-10T15:08:57.777993: step 877, loss 0.730182, acc 0.796875, prec 0.0331028, recall 0.777778
2017-12-10T15:08:57.975087: step 878, loss 0.499731, acc 0.890625, prec 0.0331402, recall 0.778043
2017-12-10T15:08:58.168945: step 879, loss 0.984536, acc 0.75, prec 0.0331133, recall 0.778043
2017-12-10T15:08:58.359691: step 880, loss 0.576509, acc 0.75, prec 0.0330864, recall 0.778043
2017-12-10T15:08:58.551116: step 881, loss 0.666248, acc 0.765625, prec 0.0330612, recall 0.778043
2017-12-10T15:08:58.744379: step 882, loss 0.820613, acc 0.734375, prec 0.0330327, recall 0.778043
2017-12-10T15:08:58.937481: step 883, loss 0.471075, acc 0.828125, prec 0.0330143, recall 0.778043
2017-12-10T15:08:59.133128: step 884, loss 0.906051, acc 0.75, prec 0.0329876, recall 0.778043
2017-12-10T15:08:59.330663: step 885, loss 7.2524, acc 0.90625, prec 0.0330282, recall 0.777381
2017-12-10T15:08:59.528350: step 886, loss 0.430889, acc 0.875, prec 0.0330637, recall 0.777646
2017-12-10T15:08:59.719639: step 887, loss 0.496753, acc 0.84375, prec 0.033047, recall 0.777646
2017-12-10T15:08:59.915429: step 888, loss 0.498845, acc 0.84375, prec 0.0330303, recall 0.777646
2017-12-10T15:09:00.107322: step 889, loss 1.03623, acc 0.84375, prec 0.0330624, recall 0.77791
2017-12-10T15:09:00.300855: step 890, loss 3.03065, acc 0.84375, prec 0.0330962, recall 0.777251
2017-12-10T15:09:00.495432: step 891, loss 0.751304, acc 0.859375, prec 0.0331299, recall 0.777515
2017-12-10T15:09:00.691393: step 892, loss 0.446609, acc 0.828125, prec 0.0331116, recall 0.777515
2017-12-10T15:09:00.884886: step 893, loss 0.334553, acc 0.875, prec 0.0330982, recall 0.777515
2017-12-10T15:09:01.079318: step 894, loss 0.57094, acc 0.828125, prec 0.0331773, recall 0.77804
2017-12-10T15:09:01.283180: step 895, loss 5.29292, acc 0.890625, prec 0.0331672, recall 0.777123
2017-12-10T15:09:01.484932: step 896, loss 0.74365, acc 0.71875, prec 0.0331858, recall 0.777385
2017-12-10T15:09:01.676168: step 897, loss 0.947961, acc 0.71875, prec 0.033253, recall 0.777908
2017-12-10T15:09:01.873049: step 898, loss 0.893864, acc 0.78125, prec 0.0333266, recall 0.778429
2017-12-10T15:09:02.066476: step 899, loss 1.08371, acc 0.734375, prec 0.0332982, recall 0.778429
2017-12-10T15:09:02.261457: step 900, loss 1.01571, acc 0.765625, prec 0.0332732, recall 0.778429
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_1/1512936356/checkpoints/model-900

2017-12-10T15:09:03.543478: step 901, loss 0.57863, acc 0.765625, prec 0.0332966, recall 0.778689
2017-12-10T15:09:03.737023: step 902, loss 0.680013, acc 0.765625, prec 0.0332716, recall 0.778689
2017-12-10T15:09:03.932602: step 903, loss 0.989969, acc 0.671875, prec 0.033285, recall 0.778947
2017-12-10T15:09:04.121365: step 904, loss 0.677594, acc 0.75, prec 0.0332584, recall 0.778947
2017-12-10T15:09:04.318487: step 905, loss 1.00331, acc 0.75, prec 0.0332801, recall 0.779206
2017-12-10T15:09:04.514322: step 906, loss 0.707859, acc 0.703125, prec 0.0332968, recall 0.779463
2017-12-10T15:09:04.703057: step 907, loss 0.769252, acc 0.84375, prec 0.0333284, recall 0.77972
2017-12-10T15:09:04.899829: step 908, loss 2.21436, acc 0.78125, prec 0.0333068, recall 0.778813
2017-12-10T15:09:05.094029: step 909, loss 2.84948, acc 0.765625, prec 0.0333317, recall 0.778165
2017-12-10T15:09:05.291386: step 910, loss 0.928434, acc 0.765625, prec 0.0333068, recall 0.778165
2017-12-10T15:09:05.484400: step 911, loss 0.71277, acc 0.796875, prec 0.0333333, recall 0.778422
2017-12-10T15:09:05.677475: step 912, loss 0.733278, acc 0.8125, prec 0.0333615, recall 0.778679
2017-12-10T15:09:05.873647: step 913, loss 0.877462, acc 0.75, prec 0.0333829, recall 0.778935
2017-12-10T15:09:06.067011: step 914, loss 0.642045, acc 0.796875, prec 0.0333614, recall 0.778935
2017-12-10T15:09:06.265656: step 915, loss 2.67031, acc 0.703125, prec 0.0333317, recall 0.778035
2017-12-10T15:09:06.460566: step 916, loss 3.16074, acc 0.6875, prec 0.0333003, recall 0.777136
2017-12-10T15:09:06.651635: step 917, loss 0.353208, acc 0.828125, prec 0.0332822, recall 0.777136
2017-12-10T15:09:06.842536: step 918, loss 0.985482, acc 0.78125, prec 0.0333547, recall 0.77765
2017-12-10T15:09:07.036435: step 919, loss 1.00628, acc 0.6875, prec 0.0333218, recall 0.77765
2017-12-10T15:09:07.229500: step 920, loss 1.04699, acc 0.78125, prec 0.0333465, recall 0.777906
2017-12-10T15:09:07.424891: step 921, loss 0.751635, acc 0.78125, prec 0.0333711, recall 0.778161
2017-12-10T15:09:07.614723: step 922, loss 0.553349, acc 0.859375, prec 0.033404, recall 0.778416
2017-12-10T15:09:07.805847: step 923, loss 2.89775, acc 0.75, prec 0.0334269, recall 0.777778
2017-12-10T15:09:08.003196: step 924, loss 2.05294, acc 0.765625, prec 0.0334038, recall 0.776888
2017-12-10T15:09:08.199146: step 925, loss 0.917869, acc 0.765625, prec 0.0334267, recall 0.777143
2017-12-10T15:09:08.394210: step 926, loss 0.957947, acc 0.75, prec 0.0334479, recall 0.777397
2017-12-10T15:09:08.586635: step 927, loss 4.73095, acc 0.765625, prec 0.033425, recall 0.776511
2017-12-10T15:09:08.780766: step 928, loss 1.15619, acc 0.765625, prec 0.0334952, recall 0.777019
2017-12-10T15:09:08.970212: step 929, loss 1.2198, acc 0.640625, prec 0.0334574, recall 0.777019
2017-12-10T15:09:09.163537: step 930, loss 1.07101, acc 0.65625, prec 0.033516, recall 0.777526
2017-12-10T15:09:09.355231: step 931, loss 1.30098, acc 0.609375, prec 0.0334751, recall 0.777526
2017-12-10T15:09:09.547633: step 932, loss 1.60783, acc 0.640625, prec 0.0334846, recall 0.777778
2017-12-10T15:09:09.737632: step 933, loss 2.72532, acc 0.71875, prec 0.0335983, recall 0.777652
2017-12-10T15:09:09.930422: step 934, loss 1.28179, acc 0.65625, prec 0.0335623, recall 0.777652
2017-12-10T15:09:10.127013: step 935, loss 1.21471, acc 0.6875, prec 0.0335296, recall 0.777652
2017-12-10T15:09:10.318817: step 936, loss 1.03551, acc 0.6875, prec 0.033497, recall 0.777652
2017-12-10T15:09:10.510731: step 937, loss 1.37627, acc 0.5625, prec 0.0335453, recall 0.778153
2017-12-10T15:09:10.703093: step 938, loss 1.31772, acc 0.640625, prec 0.0335079, recall 0.778153
2017-12-10T15:09:10.891866: step 939, loss 0.674469, acc 0.765625, prec 0.0334835, recall 0.778153
2017-12-10T15:09:11.085593: step 940, loss 0.742205, acc 0.78125, prec 0.0334608, recall 0.778153
2017-12-10T15:09:11.275778: step 941, loss 0.463193, acc 0.8125, prec 0.0334414, recall 0.778153
2017-12-10T15:09:11.466874: step 942, loss 0.824811, acc 0.734375, prec 0.0334607, recall 0.778403
2017-12-10T15:09:11.662789: step 943, loss 0.712938, acc 0.828125, prec 0.0334429, recall 0.778403
2017-12-10T15:09:11.858728: step 944, loss 14.649, acc 0.765625, prec 0.0334235, recall 0.775785
2017-12-10T15:09:12.058954: step 945, loss 1.91349, acc 0.65625, prec 0.0334347, recall 0.776036
2017-12-10T15:09:12.258609: step 946, loss 3.22046, acc 0.59375, prec 0.0334875, recall 0.77567
2017-12-10T15:09:12.448774: step 947, loss 0.971173, acc 0.65625, prec 0.0334986, recall 0.77592
2017-12-10T15:09:12.640706: step 948, loss 1.2882, acc 0.59375, prec 0.0334567, recall 0.77592
2017-12-10T15:09:12.831968: step 949, loss 1.91034, acc 0.40625, prec 0.0334421, recall 0.776169
2017-12-10T15:09:13.021699: step 950, loss 1.61757, acc 0.578125, prec 0.0334451, recall 0.776418
2017-12-10T15:09:13.211481: step 951, loss 1.49995, acc 0.59375, prec 0.0334035, recall 0.776418
2017-12-10T15:09:13.402816: step 952, loss 1.64447, acc 0.484375, prec 0.0334432, recall 0.776915
2017-12-10T15:09:13.597632: step 953, loss 2.09909, acc 0.484375, prec 0.0333906, recall 0.776915
2017-12-10T15:09:13.787532: step 954, loss 2.61637, acc 0.421875, prec 0.0333317, recall 0.776915
2017-12-10T15:09:13.987023: step 955, loss 1.44613, acc 0.546875, prec 0.0333317, recall 0.777162
2017-12-10T15:09:14.184011: step 956, loss 1.23159, acc 0.734375, prec 0.0333048, recall 0.777162
2017-12-10T15:09:14.380292: step 957, loss 4.72577, acc 0.71875, prec 0.0334156, recall 0.777042
2017-12-10T15:09:14.578501: step 958, loss 1.52091, acc 0.578125, prec 0.0333728, recall 0.777042
2017-12-10T15:09:14.770927: step 959, loss 0.854953, acc 0.71875, prec 0.0333444, recall 0.777042
2017-12-10T15:09:14.965290: step 960, loss 1.19309, acc 0.640625, prec 0.0333081, recall 0.777042
2017-12-10T15:09:15.159215: step 961, loss 0.970715, acc 0.71875, prec 0.0332798, recall 0.777042
2017-12-10T15:09:15.354027: step 962, loss 1.26891, acc 0.59375, prec 0.0332845, recall 0.777288
2017-12-10T15:09:15.549606: step 963, loss 0.67991, acc 0.8125, prec 0.0332657, recall 0.777288
2017-12-10T15:09:15.738345: step 964, loss 2.35467, acc 0.6875, prec 0.0332815, recall 0.776678
2017-12-10T15:09:15.934205: step 965, loss 0.660718, acc 0.8125, prec 0.0333082, recall 0.776923
2017-12-10T15:09:16.129857: step 966, loss 0.593098, acc 0.796875, prec 0.0332878, recall 0.776923
2017-12-10T15:09:16.322673: step 967, loss 0.882141, acc 0.78125, prec 0.0333114, recall 0.777168
2017-12-10T15:09:16.515443: step 968, loss 0.574346, acc 0.765625, prec 0.0332879, recall 0.777168
2017-12-10T15:09:16.710090: step 969, loss 0.35845, acc 0.875, prec 0.0333208, recall 0.777412
2017-12-10T15:09:16.901957: step 970, loss 0.586727, acc 0.8125, prec 0.033302, recall 0.777412
2017-12-10T15:09:17.096495: step 971, loss 0.318813, acc 0.875, prec 0.0332895, recall 0.777412
2017-12-10T15:09:17.287022: step 972, loss 0.422579, acc 0.921875, prec 0.0333724, recall 0.777899
2017-12-10T15:09:17.477915: step 973, loss 0.336411, acc 0.859375, prec 0.0333584, recall 0.777899
2017-12-10T15:09:17.668089: step 974, loss 0.334874, acc 0.828125, prec 0.0333412, recall 0.777899
2017-12-10T15:09:17.862318: step 975, loss 5.32072, acc 0.921875, prec 0.0333349, recall 0.777049
2017-12-10T15:09:18.056651: step 976, loss 5.83327, acc 0.84375, prec 0.0333224, recall 0.775354
2017-12-10T15:09:18.254411: step 977, loss 1.09497, acc 0.875, prec 0.0333552, recall 0.775599
2017-12-10T15:09:18.453875: step 978, loss 0.403161, acc 0.890625, prec 0.0333443, recall 0.775599
2017-12-10T15:09:18.644576: step 979, loss 0.428852, acc 0.890625, prec 0.0333333, recall 0.775599
2017-12-10T15:09:18.837612: step 980, loss 0.184339, acc 0.921875, prec 0.0333708, recall 0.775843
2017-12-10T15:09:19.031564: step 981, loss 0.593115, acc 0.828125, prec 0.0333536, recall 0.775843
2017-12-10T15:09:19.231078: step 982, loss 0.367786, acc 0.90625, prec 0.0333894, recall 0.776087
2017-12-10T15:09:19.421418: step 983, loss 0.518914, acc 0.828125, prec 0.0334175, recall 0.77633
2017-12-10T15:09:19.616524: step 984, loss 0.611516, acc 0.78125, prec 0.0334859, recall 0.776815
2017-12-10T15:09:19.809278: step 985, loss 2.53781, acc 0.765625, prec 0.0335542, recall 0.776458
2017-12-10T15:09:20.004911: step 986, loss 1.06833, acc 0.703125, prec 0.0335245, recall 0.776458
2017-12-10T15:09:20.195070: step 987, loss 0.996401, acc 0.75, prec 0.0334995, recall 0.776458
2017-12-10T15:09:20.389941: step 988, loss 0.467242, acc 0.859375, prec 0.0334855, recall 0.776458
2017-12-10T15:09:20.584895: step 989, loss 1.02587, acc 0.875, prec 0.033518, recall 0.776699
2017-12-10T15:09:20.778045: step 990, loss 0.678179, acc 0.8125, prec 0.0335892, recall 0.77718
2017-12-10T15:09:20.974005: step 991, loss 1.1041, acc 0.671875, prec 0.0335564, recall 0.77718
2017-12-10T15:09:21.165670: step 992, loss 0.987262, acc 0.84375, prec 0.0336755, recall 0.777897
2017-12-10T15:09:21.358419: step 993, loss 1.20871, acc 0.734375, prec 0.0337386, recall 0.778373
2017-12-10T15:09:21.533106: step 994, loss 0.647852, acc 0.764706, prec 0.0337199, recall 0.778373
2017-12-10T15:09:21.736985: step 995, loss 0.746691, acc 0.734375, prec 0.0336933, recall 0.778373
2017-12-10T15:09:21.927259: step 996, loss 0.685455, acc 0.8125, prec 0.0337193, recall 0.77861
2017-12-10T15:09:22.122721: step 997, loss 0.664715, acc 0.8125, prec 0.0337453, recall 0.778846
2017-12-10T15:09:22.315338: step 998, loss 0.496556, acc 0.78125, prec 0.0337235, recall 0.778846
2017-12-10T15:09:22.505796: step 999, loss 0.432384, acc 0.859375, prec 0.0337094, recall 0.778846
2017-12-10T15:09:22.694018: step 1000, loss 0.543573, acc 0.875, prec 0.0337863, recall 0.779318
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_1/1512936356/checkpoints/model-1000

2017-12-10T15:09:24.188614: step 1001, loss 0.673464, acc 0.828125, prec 0.0337691, recall 0.779318
2017-12-10T15:09:24.377865: step 1002, loss 0.417358, acc 0.859375, prec 0.0337551, recall 0.779318
2017-12-10T15:09:24.568554: step 1003, loss 0.539231, acc 0.8125, prec 0.0337364, recall 0.779318
2017-12-10T15:09:24.761214: step 1004, loss 0.425519, acc 0.84375, prec 0.0337654, recall 0.779553
2017-12-10T15:09:24.954610: step 1005, loss 0.444219, acc 0.890625, prec 0.0338436, recall 0.780021
2017-12-10T15:09:25.144233: step 1006, loss 0.473543, acc 0.84375, prec 0.0338725, recall 0.780255
2017-12-10T15:09:25.343321: step 1007, loss 0.350737, acc 0.875, prec 0.03386, recall 0.780255
2017-12-10T15:09:25.537496: step 1008, loss 1.32231, acc 0.921875, prec 0.0339412, recall 0.78072
2017-12-10T15:09:25.733038: step 1009, loss 0.263128, acc 0.90625, prec 0.0339763, recall 0.780952
2017-12-10T15:09:25.925436: step 1010, loss 0.471487, acc 0.90625, prec 0.0340114, recall 0.781184
2017-12-10T15:09:26.119224: step 1011, loss 0.539813, acc 0.921875, prec 0.034048, recall 0.781415
2017-12-10T15:09:26.311469: step 1012, loss 5.43709, acc 0.90625, prec 0.0340846, recall 0.780822
2017-12-10T15:09:26.504945: step 1013, loss 0.110094, acc 0.953125, prec 0.0340799, recall 0.780822
2017-12-10T15:09:26.698711: step 1014, loss 0.242177, acc 0.921875, prec 0.0341609, recall 0.781283
2017-12-10T15:09:26.896116: step 1015, loss 0.281298, acc 0.875, prec 0.0342371, recall 0.781742
2017-12-10T15:09:27.096587: step 1016, loss 0.188635, acc 0.90625, prec 0.0342721, recall 0.781971
2017-12-10T15:09:27.286860: step 1017, loss 0.280696, acc 0.90625, prec 0.0342626, recall 0.781971
2017-12-10T15:09:27.476528: step 1018, loss 0.316209, acc 0.890625, prec 0.0342516, recall 0.781971
2017-12-10T15:09:27.668799: step 1019, loss 0.87668, acc 0.78125, prec 0.0342739, recall 0.782199
2017-12-10T15:09:27.860670: step 1020, loss 0.4523, acc 0.859375, prec 0.0342598, recall 0.782199
2017-12-10T15:09:28.058978: step 1021, loss 0.397788, acc 0.859375, prec 0.0342456, recall 0.782199
2017-12-10T15:09:28.251323: step 1022, loss 0.146331, acc 0.953125, prec 0.0342409, recall 0.782199
2017-12-10T15:09:28.449774: step 1023, loss 1.18333, acc 0.78125, prec 0.0342632, recall 0.782427
2017-12-10T15:09:28.645977: step 1024, loss 0.282524, acc 0.90625, prec 0.0342538, recall 0.782427
2017-12-10T15:09:28.840851: step 1025, loss 0.338717, acc 0.828125, prec 0.0342807, recall 0.782654
2017-12-10T15:09:29.041655: step 1026, loss 1.66741, acc 0.84375, prec 0.0343534, recall 0.783107
2017-12-10T15:09:29.246100: step 1027, loss 2.90638, acc 0.875, prec 0.0343424, recall 0.782292
2017-12-10T15:09:29.447903: step 1028, loss 0.247883, acc 0.890625, prec 0.0343756, recall 0.782518
2017-12-10T15:09:29.641411: step 1029, loss 0.387158, acc 0.84375, prec 0.0343599, recall 0.782518
2017-12-10T15:09:29.833174: step 1030, loss 1.01074, acc 0.75, prec 0.0344229, recall 0.78297
2017-12-10T15:09:30.027825: step 1031, loss 0.669597, acc 0.84375, prec 0.0345394, recall 0.783644
2017-12-10T15:09:30.222977: step 1032, loss 0.366837, acc 0.859375, prec 0.0345252, recall 0.783644
2017-12-10T15:09:30.415504: step 1033, loss 0.699287, acc 0.78125, prec 0.0345032, recall 0.783644
2017-12-10T15:09:30.604837: step 1034, loss 0.388719, acc 0.875, prec 0.0344906, recall 0.783644
2017-12-10T15:09:30.798863: step 1035, loss 0.365594, acc 0.859375, prec 0.0345204, recall 0.783868
2017-12-10T15:09:30.989026: step 1036, loss 0.946389, acc 0.6875, prec 0.034489, recall 0.783868
2017-12-10T15:09:31.185379: step 1037, loss 0.724383, acc 0.8125, prec 0.034558, recall 0.784314
2017-12-10T15:09:31.382373: step 1038, loss 0.683337, acc 0.75, prec 0.0345768, recall 0.784536
2017-12-10T15:09:31.582281: step 1039, loss 0.60109, acc 0.8125, prec 0.0346018, recall 0.784758
2017-12-10T15:09:31.772833: step 1040, loss 0.65527, acc 0.828125, prec 0.0345845, recall 0.784758
2017-12-10T15:09:31.964754: step 1041, loss 1.29296, acc 0.859375, prec 0.0347455, recall 0.785641
2017-12-10T15:09:32.160186: step 1042, loss 0.763882, acc 0.828125, prec 0.034772, recall 0.785861
2017-12-10T15:09:32.354391: step 1043, loss 0.591304, acc 0.859375, prec 0.0348453, recall 0.786299
2017-12-10T15:09:32.546315: step 1044, loss 0.401973, acc 0.875, prec 0.0348326, recall 0.786299
2017-12-10T15:09:32.740807: step 1045, loss 1.2502, acc 0.859375, prec 0.0349058, recall 0.786735
2017-12-10T15:09:32.937728: step 1046, loss 0.80885, acc 0.8125, prec 0.0350615, recall 0.787602
2017-12-10T15:09:33.136385: step 1047, loss 1.82177, acc 0.78125, prec 0.0351266, recall 0.788032
2017-12-10T15:09:33.332439: step 1048, loss 0.485359, acc 0.859375, prec 0.0351559, recall 0.788247
2017-12-10T15:09:33.524848: step 1049, loss 0.34944, acc 0.828125, prec 0.035182, recall 0.788462
2017-12-10T15:09:33.715324: step 1050, loss 0.417751, acc 0.84375, prec 0.0351661, recall 0.788462
2017-12-10T15:09:33.912748: step 1051, loss 0.841077, acc 0.703125, prec 0.0351795, recall 0.788675
2017-12-10T15:09:34.110621: step 1052, loss 0.392528, acc 0.90625, prec 0.035257, recall 0.789102
2017-12-10T15:09:34.306560: step 1053, loss 4.89868, acc 0.765625, prec 0.0352782, recall 0.78852
2017-12-10T15:09:34.505883: step 1054, loss 0.523294, acc 0.734375, prec 0.0352512, recall 0.78852
2017-12-10T15:09:34.697589: step 1055, loss 0.449571, acc 0.8125, prec 0.0352322, recall 0.78852
2017-12-10T15:09:34.892778: step 1056, loss 0.873277, acc 0.8125, prec 0.0352566, recall 0.788732
2017-12-10T15:09:35.085105: step 1057, loss 0.526304, acc 0.828125, prec 0.0352825, recall 0.788945
2017-12-10T15:09:35.279573: step 1058, loss 1.26031, acc 0.875, prec 0.0353565, recall 0.789368
2017-12-10T15:09:35.478945: step 1059, loss 0.697971, acc 0.75, prec 0.0353311, recall 0.789368
2017-12-10T15:09:35.674566: step 1060, loss 0.858257, acc 0.734375, prec 0.0353041, recall 0.789368
2017-12-10T15:09:35.866130: step 1061, loss 0.751419, acc 0.75, prec 0.0353221, recall 0.789579
2017-12-10T15:09:36.059037: step 1062, loss 2.13497, acc 0.765625, prec 0.0352999, recall 0.788789
2017-12-10T15:09:36.254168: step 1063, loss 0.741997, acc 0.734375, prec 0.0352731, recall 0.788789
2017-12-10T15:09:36.448456: step 1064, loss 0.5362, acc 0.78125, prec 0.035251, recall 0.788789
2017-12-10T15:09:36.644133: step 1065, loss 1.18524, acc 0.765625, prec 0.0353567, recall 0.789421
2017-12-10T15:09:36.840942: step 1066, loss 0.969051, acc 0.75, prec 0.0354176, recall 0.789841
2017-12-10T15:09:37.034127: step 1067, loss 0.779193, acc 0.75, prec 0.0353923, recall 0.789841
2017-12-10T15:09:37.227570: step 1068, loss 0.751068, acc 0.75, prec 0.0354101, recall 0.79005
2017-12-10T15:09:37.418340: step 1069, loss 0.565585, acc 0.875, prec 0.0354834, recall 0.790467
2017-12-10T15:09:37.611352: step 1070, loss 0.413778, acc 0.84375, prec 0.0355106, recall 0.790675
2017-12-10T15:09:37.805700: step 1071, loss 0.696222, acc 0.765625, prec 0.0355728, recall 0.791089
2017-12-10T15:09:38.001951: step 1072, loss 5.35072, acc 0.75, prec 0.0355919, recall 0.790514
2017-12-10T15:09:38.194961: step 1073, loss 0.507486, acc 0.828125, prec 0.0356174, recall 0.790721
2017-12-10T15:09:38.391170: step 1074, loss 0.48407, acc 0.828125, prec 0.0356, recall 0.790721
2017-12-10T15:09:38.583812: step 1075, loss 0.354307, acc 0.90625, prec 0.0356334, recall 0.790927
2017-12-10T15:09:38.774586: step 1076, loss 0.705474, acc 0.75, prec 0.0356509, recall 0.791133
2017-12-10T15:09:38.964427: step 1077, loss 0.957866, acc 0.734375, prec 0.035624, recall 0.791133
2017-12-10T15:09:39.154953: step 1078, loss 0.751621, acc 0.75, prec 0.0356415, recall 0.791339
2017-12-10T15:09:39.343419: step 1079, loss 1.03197, acc 0.796875, prec 0.0356637, recall 0.791544
2017-12-10T15:09:39.535449: step 1080, loss 0.710757, acc 0.734375, prec 0.0357649, recall 0.792157
2017-12-10T15:09:39.729526: step 1081, loss 0.850672, acc 0.765625, prec 0.0357838, recall 0.79236
2017-12-10T15:09:39.923651: step 1082, loss 0.537281, acc 0.796875, prec 0.0357632, recall 0.79236
2017-12-10T15:09:40.116826: step 1083, loss 0.536976, acc 0.859375, prec 0.0358342, recall 0.792766
2017-12-10T15:09:40.310649: step 1084, loss 0.343803, acc 0.859375, prec 0.0358626, recall 0.792969
2017-12-10T15:09:40.506449: step 1085, loss 0.754783, acc 0.859375, prec 0.0358909, recall 0.793171
2017-12-10T15:09:40.697972: step 1086, loss 0.214452, acc 0.90625, prec 0.0359239, recall 0.793372
2017-12-10T15:09:40.891101: step 1087, loss 0.344972, acc 0.921875, prec 0.0359585, recall 0.793573
2017-12-10T15:09:41.088517: step 1088, loss 0.4274, acc 0.875, prec 0.0359884, recall 0.793774
2017-12-10T15:09:41.278279: step 1089, loss 0.771227, acc 0.890625, prec 0.0360197, recall 0.793975
2017-12-10T15:09:41.471251: step 1090, loss 0.383939, acc 0.84375, prec 0.0360464, recall 0.794175
2017-12-10T15:09:41.662208: step 1091, loss 1.92001, acc 0.96875, prec 0.0360448, recall 0.793404
2017-12-10T15:09:41.855713: step 1092, loss 0.979554, acc 0.84375, prec 0.0360713, recall 0.793605
2017-12-10T15:09:42.049409: step 1093, loss 0.197006, acc 0.875, prec 0.0360586, recall 0.793605
2017-12-10T15:09:42.246931: step 1094, loss 0.375889, acc 0.84375, prec 0.0360428, recall 0.793605
2017-12-10T15:09:42.443209: step 1095, loss 0.42375, acc 0.890625, prec 0.0360741, recall 0.793804
2017-12-10T15:09:42.642275: step 1096, loss 0.199864, acc 0.953125, prec 0.0360693, recall 0.793804
2017-12-10T15:09:42.835850: step 1097, loss 1.54456, acc 0.890625, prec 0.0360598, recall 0.793037
2017-12-10T15:09:43.029618: step 1098, loss 0.294852, acc 0.875, prec 0.0360895, recall 0.793237
2017-12-10T15:09:43.223284: step 1099, loss 1.47306, acc 0.875, prec 0.0360784, recall 0.792471
2017-12-10T15:09:43.415613: step 1100, loss 1.19527, acc 0.75, prec 0.0360954, recall 0.792671
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_1/1512936356/checkpoints/model-1100

2017-12-10T15:09:44.711348: step 1101, loss 0.760688, acc 0.734375, prec 0.0360685, recall 0.792671
2017-12-10T15:09:44.900798: step 1102, loss 0.606846, acc 0.78125, prec 0.0360886, recall 0.792871
2017-12-10T15:09:45.093848: step 1103, loss 0.827093, acc 0.78125, prec 0.0361509, recall 0.793269
2017-12-10T15:09:45.283640: step 1104, loss 0.528402, acc 0.796875, prec 0.0361303, recall 0.793269
2017-12-10T15:09:45.477005: step 1105, loss 0.634048, acc 0.796875, prec 0.0361098, recall 0.793269
2017-12-10T15:09:45.667444: step 1106, loss 0.769039, acc 0.75, prec 0.0360845, recall 0.793269
2017-12-10T15:09:45.856181: step 1107, loss 5.82363, acc 0.796875, prec 0.0360656, recall 0.792507
2017-12-10T15:09:46.054220: step 1108, loss 0.742232, acc 0.71875, prec 0.0360372, recall 0.792507
2017-12-10T15:09:46.257319: step 1109, loss 0.619227, acc 0.71875, prec 0.036051, recall 0.792706
2017-12-10T15:09:46.446324: step 1110, loss 0.511744, acc 0.84375, prec 0.0361193, recall 0.793103
2017-12-10T15:09:46.635891: step 1111, loss 0.850924, acc 0.78125, prec 0.0361393, recall 0.793301
2017-12-10T15:09:46.829012: step 1112, loss 0.512042, acc 0.828125, prec 0.036206, recall 0.793696
2017-12-10T15:09:47.018799: step 1113, loss 0.526379, acc 0.78125, prec 0.0361839, recall 0.793696
2017-12-10T15:09:47.215345: step 1114, loss 0.367898, acc 0.890625, prec 0.0361729, recall 0.793696
2017-12-10T15:09:47.407631: step 1115, loss 0.31479, acc 0.875, prec 0.0361603, recall 0.793696
2017-12-10T15:09:47.600143: step 1116, loss 0.759606, acc 0.828125, prec 0.0361849, recall 0.793893
2017-12-10T15:09:47.793434: step 1117, loss 0.657948, acc 0.796875, prec 0.0362483, recall 0.794286
2017-12-10T15:09:47.986710: step 1118, loss 0.380856, acc 0.859375, prec 0.036276, recall 0.794481
2017-12-10T15:09:48.178289: step 1119, loss 0.432165, acc 0.796875, prec 0.0362555, recall 0.794481
2017-12-10T15:09:48.371262: step 1120, loss 0.375733, acc 0.890625, prec 0.0362445, recall 0.794481
2017-12-10T15:09:48.559707: step 1121, loss 0.416563, acc 0.796875, prec 0.036224, recall 0.794481
2017-12-10T15:09:48.755829: step 1122, loss 0.40084, acc 0.875, prec 0.036295, recall 0.794872
2017-12-10T15:09:48.949618: step 1123, loss 8.15351, acc 0.875, prec 0.036284, recall 0.794118
2017-12-10T15:09:49.143578: step 1124, loss 1.65983, acc 0.875, prec 0.036273, recall 0.793365
2017-12-10T15:09:49.347417: step 1125, loss 0.287422, acc 0.90625, prec 0.0363053, recall 0.793561
2017-12-10T15:09:49.541408: step 1126, loss 0.359515, acc 0.875, prec 0.0363762, recall 0.793951
2017-12-10T15:09:49.731586: step 1127, loss 0.274818, acc 0.859375, prec 0.0363621, recall 0.793951
2017-12-10T15:09:49.924836: step 1128, loss 3.64248, acc 0.8125, prec 0.0363448, recall 0.793201
2017-12-10T15:09:50.121498: step 1129, loss 0.441718, acc 0.828125, prec 0.0364108, recall 0.793591
2017-12-10T15:09:50.311275: step 1130, loss 0.689307, acc 0.765625, prec 0.0363872, recall 0.793591
2017-12-10T15:09:50.506432: step 1131, loss 0.666583, acc 0.765625, prec 0.0364053, recall 0.793785
2017-12-10T15:09:50.696402: step 1132, loss 1.08006, acc 0.703125, prec 0.0363754, recall 0.793785
2017-12-10T15:09:50.887288: step 1133, loss 0.856205, acc 0.71875, prec 0.0363887, recall 0.793979
2017-12-10T15:09:51.076909: step 1134, loss 0.923512, acc 0.75, prec 0.0363636, recall 0.793979
2017-12-10T15:09:51.269066: step 1135, loss 1.01565, acc 0.71875, prec 0.0363355, recall 0.793979
2017-12-10T15:09:51.458727: step 1136, loss 2.18518, acc 0.703125, prec 0.0363488, recall 0.793427
2017-12-10T15:09:51.654293: step 1137, loss 0.908757, acc 0.75, prec 0.0363652, recall 0.793621
2017-12-10T15:09:51.848402: step 1138, loss 1.82382, acc 0.765625, prec 0.0363433, recall 0.792877
2017-12-10T15:09:52.041503: step 1139, loss 1.57234, acc 0.75, prec 0.0363597, recall 0.793071
2017-12-10T15:09:52.236623: step 1140, loss 0.852972, acc 0.703125, prec 0.0363714, recall 0.793265
2017-12-10T15:09:52.428254: step 1141, loss 0.761527, acc 0.703125, prec 0.0363418, recall 0.793265
2017-12-10T15:09:52.618205: step 1142, loss 1.00749, acc 0.734375, prec 0.0363979, recall 0.793651
2017-12-10T15:09:52.809881: step 1143, loss 0.729862, acc 0.75, prec 0.0364142, recall 0.793843
2017-12-10T15:09:53.002579: step 1144, loss 0.819961, acc 0.71875, prec 0.0363862, recall 0.793843
2017-12-10T15:09:53.195077: step 1145, loss 1.11255, acc 0.6875, prec 0.0363551, recall 0.793843
2017-12-10T15:09:53.388409: step 1146, loss 0.758282, acc 0.78125, prec 0.0363334, recall 0.793843
2017-12-10T15:09:53.579287: step 1147, loss 0.699375, acc 0.734375, prec 0.0363481, recall 0.794035
2017-12-10T15:09:53.770941: step 1148, loss 0.599463, acc 0.84375, prec 0.036497, recall 0.7948
2017-12-10T15:09:53.965092: step 1149, loss 0.638186, acc 0.78125, prec 0.0364752, recall 0.7948
2017-12-10T15:09:54.157849: step 1150, loss 0.903113, acc 0.875, prec 0.0365859, recall 0.79537
2017-12-10T15:09:54.351480: step 1151, loss 0.534707, acc 0.765625, prec 0.0366035, recall 0.79556
2017-12-10T15:09:54.546713: step 1152, loss 0.22941, acc 0.921875, prec 0.0365957, recall 0.79556
2017-12-10T15:09:54.739932: step 1153, loss 1.0578, acc 0.859375, prec 0.0367456, recall 0.796313
2017-12-10T15:09:54.933279: step 1154, loss 0.387379, acc 0.84375, prec 0.0368119, recall 0.796688
2017-12-10T15:09:55.129880: step 1155, loss 0.243019, acc 0.90625, prec 0.0368025, recall 0.796688
2017-12-10T15:09:55.321958: step 1156, loss 0.291503, acc 0.875, prec 0.03679, recall 0.796688
2017-12-10T15:09:55.517246: step 1157, loss 0.507918, acc 0.90625, prec 0.0368215, recall 0.796875
2017-12-10T15:09:55.712844: step 1158, loss 0.41648, acc 0.921875, prec 0.0368546, recall 0.797062
2017-12-10T15:09:55.906990: step 1159, loss 0.227946, acc 0.9375, prec 0.0368892, recall 0.797248
2017-12-10T15:09:56.098620: step 1160, loss 0.123598, acc 0.953125, prec 0.0368846, recall 0.797248
2017-12-10T15:09:56.292749: step 1161, loss 0.27347, acc 0.859375, prec 0.0368705, recall 0.797248
2017-12-10T15:09:56.482344: step 1162, loss 0.144614, acc 0.96875, prec 0.0368673, recall 0.797248
2017-12-10T15:09:56.675672: step 1163, loss 0.147069, acc 0.953125, prec 0.0369443, recall 0.797619
2017-12-10T15:09:56.865686: step 1164, loss 0.135393, acc 0.953125, prec 0.0369397, recall 0.797619
2017-12-10T15:09:57.058171: step 1165, loss 0.278583, acc 0.9375, prec 0.0369742, recall 0.797804
2017-12-10T15:09:57.252920: step 1166, loss 0.299959, acc 0.859375, prec 0.0369601, recall 0.797804
2017-12-10T15:09:57.444405: step 1167, loss 1.42885, acc 0.890625, prec 0.0369915, recall 0.79726
2017-12-10T15:09:57.638461: step 1168, loss 0.0662327, acc 0.984375, prec 0.0370308, recall 0.797445
2017-12-10T15:09:57.832717: step 1169, loss 0.162237, acc 0.96875, prec 0.0370276, recall 0.797445
2017-12-10T15:09:58.028862: step 1170, loss 0.178206, acc 0.90625, prec 0.0370182, recall 0.797445
2017-12-10T15:09:58.223023: step 1171, loss 0.323557, acc 0.921875, prec 0.0370512, recall 0.79763
2017-12-10T15:09:58.412813: step 1172, loss 0.195575, acc 0.96875, prec 0.0370888, recall 0.797814
2017-12-10T15:09:58.606007: step 1173, loss 0.23416, acc 0.9375, prec 0.0371233, recall 0.797998
2017-12-10T15:09:58.795049: step 1174, loss 0.191875, acc 0.921875, prec 0.0371154, recall 0.797998
2017-12-10T15:09:58.986239: step 1175, loss 0.111852, acc 0.953125, prec 0.0371107, recall 0.797998
2017-12-10T15:09:59.189142: step 1176, loss 0.102643, acc 0.984375, prec 0.0371906, recall 0.798365
2017-12-10T15:09:59.382538: step 1177, loss 0.456059, acc 0.9375, prec 0.0372658, recall 0.798731
2017-12-10T15:09:59.577380: step 1178, loss 0.161428, acc 0.9375, prec 0.0373002, recall 0.798913
2017-12-10T15:09:59.773530: step 1179, loss 0.127627, acc 0.953125, prec 0.0372954, recall 0.798913
2017-12-10T15:09:59.963224: step 1180, loss 0.192788, acc 0.921875, prec 0.0372876, recall 0.798913
2017-12-10T15:10:00.154715: step 1181, loss 0.0812026, acc 0.984375, prec 0.0373267, recall 0.799095
2017-12-10T15:10:00.346118: step 1182, loss 0.241437, acc 0.953125, prec 0.0373626, recall 0.799277
2017-12-10T15:10:00.539654: step 1183, loss 0.175951, acc 0.90625, prec 0.0373938, recall 0.799458
2017-12-10T15:10:00.731384: step 1184, loss 5.41944, acc 0.890625, prec 0.0373844, recall 0.798736
2017-12-10T15:10:00.926552: step 1185, loss 2.48323, acc 0.90625, prec 0.0373765, recall 0.798016
2017-12-10T15:10:01.122722: step 1186, loss 0.409626, acc 0.90625, prec 0.0374076, recall 0.798198
2017-12-10T15:10:01.322276: step 1187, loss 0.419665, acc 0.875, prec 0.0374356, recall 0.79838
2017-12-10T15:10:01.515916: step 1188, loss 0.320157, acc 0.84375, prec 0.0374605, recall 0.798561
2017-12-10T15:10:01.710549: step 1189, loss 0.357577, acc 0.875, prec 0.0374478, recall 0.798561
2017-12-10T15:10:01.905426: step 1190, loss 1.82406, acc 0.875, prec 0.0374368, recall 0.797844
2017-12-10T15:10:02.100951: step 1191, loss 1.81371, acc 0.84375, prec 0.0374226, recall 0.797127
2017-12-10T15:10:02.297517: step 1192, loss 0.469285, acc 0.8125, prec 0.0374847, recall 0.797491
2017-12-10T15:10:02.492441: step 1193, loss 0.598386, acc 0.90625, prec 0.0375158, recall 0.797672
2017-12-10T15:10:02.682715: step 1194, loss 1.17685, acc 0.6875, prec 0.0375652, recall 0.798034
2017-12-10T15:10:02.874693: step 1195, loss 0.606175, acc 0.84375, prec 0.0375899, recall 0.798214
2017-12-10T15:10:03.066986: step 1196, loss 0.816489, acc 0.71875, prec 0.0376019, recall 0.798394
2017-12-10T15:10:03.257264: step 1197, loss 0.868407, acc 0.765625, prec 0.0376186, recall 0.798574
2017-12-10T15:10:03.449944: step 1198, loss 0.560982, acc 0.78125, prec 0.0375965, recall 0.798574
2017-12-10T15:10:03.644044: step 1199, loss 0.798221, acc 0.6875, prec 0.0376053, recall 0.798753
2017-12-10T15:10:03.834309: step 1200, loss 0.621871, acc 0.71875, prec 0.037577, recall 0.798753
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_1/1512936356/checkpoints/model-1200

2017-12-10T15:10:05.028592: step 1201, loss 0.73046, acc 0.65625, prec 0.0375827, recall 0.798932
2017-12-10T15:10:05.217274: step 1202, loss 0.386064, acc 0.890625, prec 0.0376522, recall 0.79929
2017-12-10T15:10:05.407910: step 1203, loss 1.62576, acc 0.8125, prec 0.0376349, recall 0.79858
2017-12-10T15:10:05.599814: step 1204, loss 0.675638, acc 0.765625, prec 0.0376113, recall 0.79858
2017-12-10T15:10:05.793211: step 1205, loss 0.296462, acc 0.859375, prec 0.0376775, recall 0.798937
2017-12-10T15:10:05.985555: step 1206, loss 0.959341, acc 0.671875, prec 0.0376445, recall 0.798937
2017-12-10T15:10:06.175688: step 1207, loss 3.32534, acc 0.78125, prec 0.0377059, recall 0.797882
2017-12-10T15:10:06.371591: step 1208, loss 1.23528, acc 0.71875, prec 0.037798, recall 0.798415
2017-12-10T15:10:06.564296: step 1209, loss 0.647729, acc 0.78125, prec 0.0377759, recall 0.798415
2017-12-10T15:10:06.755840: step 1210, loss 0.54488, acc 0.765625, prec 0.0377523, recall 0.798415
2017-12-10T15:10:06.950454: step 1211, loss 0.756678, acc 0.65625, prec 0.0377578, recall 0.798593
2017-12-10T15:10:07.137939: step 1212, loss 0.791785, acc 0.78125, prec 0.0378158, recall 0.798946
2017-12-10T15:10:07.330218: step 1213, loss 0.568202, acc 0.75, prec 0.0378307, recall 0.799123
2017-12-10T15:10:07.522830: step 1214, loss 0.795291, acc 0.71875, prec 0.0378024, recall 0.799123
2017-12-10T15:10:07.711792: step 1215, loss 10.9232, acc 0.828125, prec 0.0378266, recall 0.798599
2017-12-10T15:10:07.904707: step 1216, loss 5.12603, acc 0.78125, prec 0.0378062, recall 0.7979
2017-12-10T15:10:08.102813: step 1217, loss 0.583282, acc 0.765625, prec 0.0377828, recall 0.7979
2017-12-10T15:10:08.297955: step 1218, loss 1.28919, acc 0.734375, prec 0.037796, recall 0.798077
2017-12-10T15:10:08.496314: step 1219, loss 1.66625, acc 0.65625, prec 0.0378014, recall 0.798253
2017-12-10T15:10:08.688572: step 1220, loss 1.13194, acc 0.671875, prec 0.0378481, recall 0.798605
2017-12-10T15:10:08.887317: step 1221, loss 0.877807, acc 0.671875, prec 0.037855, recall 0.798781
2017-12-10T15:10:09.075553: step 1222, loss 1.30508, acc 0.734375, prec 0.0378682, recall 0.798956
2017-12-10T15:10:09.271423: step 1223, loss 1.37215, acc 0.609375, prec 0.0378688, recall 0.79913
2017-12-10T15:10:09.463618: step 1224, loss 1.19016, acc 0.578125, prec 0.0378267, recall 0.79913
2017-12-10T15:10:09.660385: step 1225, loss 1.36609, acc 0.578125, prec 0.0379034, recall 0.799653
2017-12-10T15:10:09.856706: step 1226, loss 1.00014, acc 0.65625, prec 0.0378691, recall 0.799653
2017-12-10T15:10:10.047936: step 1227, loss 1.36076, acc 0.53125, prec 0.0378225, recall 0.799653
2017-12-10T15:10:10.239645: step 1228, loss 0.781053, acc 0.671875, prec 0.03779, recall 0.799653
2017-12-10T15:10:10.431691: step 1229, loss 1.17451, acc 0.671875, prec 0.0377969, recall 0.799827
2017-12-10T15:10:10.620860: step 1230, loss 1.06764, acc 0.6875, prec 0.037766, recall 0.799827
2017-12-10T15:10:10.808776: step 1231, loss 0.939301, acc 0.625, prec 0.0378076, recall 0.800173
2017-12-10T15:10:10.999435: step 1232, loss 0.573579, acc 0.734375, prec 0.0377813, recall 0.800173
2017-12-10T15:10:11.193924: step 1233, loss 1.10462, acc 0.71875, prec 0.0377928, recall 0.800346
2017-12-10T15:10:11.387998: step 1234, loss 0.792854, acc 0.75, prec 0.0377682, recall 0.800346
2017-12-10T15:10:11.585743: step 1235, loss 0.41558, acc 0.875, prec 0.0378735, recall 0.800862
2017-12-10T15:10:11.779263: step 1236, loss 0.412972, acc 0.875, prec 0.0379004, recall 0.801034
2017-12-10T15:10:11.972599: step 1237, loss 0.27853, acc 0.90625, prec 0.0378911, recall 0.801034
2017-12-10T15:10:12.162653: step 1238, loss 0.67682, acc 0.875, prec 0.0379572, recall 0.801376
2017-12-10T15:10:12.360892: step 1239, loss 0.254394, acc 0.921875, prec 0.0379886, recall 0.801546
2017-12-10T15:10:12.556607: step 1240, loss 4.89527, acc 0.828125, prec 0.0380514, recall 0.8012
2017-12-10T15:10:12.750129: step 1241, loss 1.75646, acc 0.890625, prec 0.0380422, recall 0.800514
2017-12-10T15:10:12.944553: step 1242, loss 0.594125, acc 0.890625, prec 0.0380704, recall 0.800684
2017-12-10T15:10:13.139302: step 1243, loss 2.24277, acc 0.890625, prec 0.0381394, recall 0.800341
2017-12-10T15:10:13.333558: step 1244, loss 0.845026, acc 0.796875, prec 0.0381974, recall 0.800681
2017-12-10T15:10:13.526008: step 1245, loss 0.893672, acc 0.828125, prec 0.0382585, recall 0.80102
2017-12-10T15:10:13.717687: step 1246, loss 1.16549, acc 0.71875, prec 0.0382695, recall 0.801189
2017-12-10T15:10:13.908568: step 1247, loss 0.529112, acc 0.828125, prec 0.0382525, recall 0.801189
2017-12-10T15:10:14.105110: step 1248, loss 0.526262, acc 0.828125, prec 0.0383134, recall 0.801527
2017-12-10T15:10:14.302675: step 1249, loss 0.934712, acc 0.75, prec 0.0383275, recall 0.801695
2017-12-10T15:10:14.492104: step 1250, loss 0.375773, acc 0.859375, prec 0.0383136, recall 0.801695
2017-12-10T15:10:14.685760: step 1251, loss 0.74607, acc 0.796875, prec 0.0382934, recall 0.801695
2017-12-10T15:10:14.876138: step 1252, loss 0.974611, acc 0.796875, prec 0.03839, recall 0.802198
2017-12-10T15:10:15.068346: step 1253, loss 1.63587, acc 0.671875, prec 0.0384351, recall 0.802532
2017-12-10T15:10:15.262822: step 1254, loss 0.904497, acc 0.734375, prec 0.0384087, recall 0.802532
2017-12-10T15:10:15.455956: step 1255, loss 1.00525, acc 0.71875, prec 0.0384196, recall 0.802698
2017-12-10T15:10:15.645485: step 1256, loss 0.752413, acc 0.765625, prec 0.0385127, recall 0.803196
2017-12-10T15:10:15.843326: step 1257, loss 1.18253, acc 0.796875, prec 0.03857, recall 0.803526
2017-12-10T15:10:16.035878: step 1258, loss 0.812059, acc 0.71875, prec 0.0385808, recall 0.803691
2017-12-10T15:10:16.235269: step 1259, loss 0.997239, acc 0.78125, prec 0.0386751, recall 0.804184
2017-12-10T15:10:16.426516: step 1260, loss 1.43547, acc 0.71875, prec 0.0387245, recall 0.804511
2017-12-10T15:10:16.623361: step 1261, loss 0.74037, acc 0.765625, prec 0.0387398, recall 0.804674
2017-12-10T15:10:16.811564: step 1262, loss 0.588852, acc 0.8125, prec 0.0387211, recall 0.804674
2017-12-10T15:10:17.003604: step 1263, loss 0.78025, acc 0.734375, prec 0.0387332, recall 0.804837
2017-12-10T15:10:17.193537: step 1264, loss 0.536473, acc 0.828125, prec 0.0387161, recall 0.804837
2017-12-10T15:10:17.383437: step 1265, loss 0.515149, acc 0.828125, prec 0.0386991, recall 0.804837
2017-12-10T15:10:17.575524: step 1266, loss 0.634581, acc 0.8125, prec 0.038719, recall 0.805
2017-12-10T15:10:17.767385: step 1267, loss 0.75923, acc 0.75, prec 0.0387327, recall 0.805162
2017-12-10T15:10:17.959332: step 1268, loss 0.532098, acc 0.828125, prec 0.0387156, recall 0.805162
2017-12-10T15:10:18.150175: step 1269, loss 0.41117, acc 0.78125, prec 0.0386939, recall 0.805162
2017-12-10T15:10:18.340654: step 1270, loss 0.672047, acc 0.828125, prec 0.0386769, recall 0.805162
2017-12-10T15:10:18.536569: step 1271, loss 0.398626, acc 0.859375, prec 0.0387399, recall 0.805486
2017-12-10T15:10:18.730860: step 1272, loss 0.226181, acc 0.90625, prec 0.038769, recall 0.805648
2017-12-10T15:10:18.922346: step 1273, loss 0.39443, acc 0.84375, prec 0.0387919, recall 0.805809
2017-12-10T15:10:19.113781: step 1274, loss 0.180695, acc 0.921875, prec 0.0387842, recall 0.805809
2017-12-10T15:10:19.312017: step 1275, loss 1.30783, acc 0.84375, prec 0.038807, recall 0.80597
2017-12-10T15:10:19.508141: step 1276, loss 0.633507, acc 0.890625, prec 0.0388729, recall 0.806291
2017-12-10T15:10:19.702344: step 1277, loss 9.51812, acc 0.859375, prec 0.0389372, recall 0.805946
2017-12-10T15:10:19.901259: step 1278, loss 0.413041, acc 0.890625, prec 0.0389263, recall 0.805946
2017-12-10T15:10:20.097670: step 1279, loss 0.251112, acc 0.90625, prec 0.038917, recall 0.805946
2017-12-10T15:10:20.290637: step 1280, loss 0.198172, acc 0.9375, prec 0.0389491, recall 0.806106
2017-12-10T15:10:20.489480: step 1281, loss 0.27278, acc 0.90625, prec 0.0389781, recall 0.806265
2017-12-10T15:10:20.684852: step 1282, loss 0.350228, acc 0.875, prec 0.0389657, recall 0.806265
2017-12-10T15:10:20.875642: step 1283, loss 0.306446, acc 0.875, prec 0.0389533, recall 0.806265
2017-12-10T15:10:21.070594: step 1284, loss 0.181049, acc 0.9375, prec 0.0389471, recall 0.806265
2017-12-10T15:10:21.265995: step 1285, loss 0.982246, acc 0.921875, prec 0.0390158, recall 0.806584
2017-12-10T15:10:21.460251: step 1286, loss 0.325978, acc 0.90625, prec 0.0390448, recall 0.806743
2017-12-10T15:10:21.653686: step 1287, loss 1.48312, acc 0.84375, prec 0.0390308, recall 0.806081
2017-12-10T15:10:21.843897: step 1288, loss 0.309239, acc 0.859375, prec 0.039055, recall 0.80624
2017-12-10T15:10:22.037802: step 1289, loss 1.32479, acc 0.875, prec 0.039119, recall 0.806557
2017-12-10T15:10:22.230083: step 1290, loss 0.350507, acc 0.90625, prec 0.0391479, recall 0.806716
2017-12-10T15:10:22.425018: step 1291, loss 0.733783, acc 0.8125, prec 0.0391674, recall 0.806874
2017-12-10T15:10:22.621374: step 1292, loss 0.485465, acc 0.90625, prec 0.0391962, recall 0.807032
2017-12-10T15:10:22.813927: step 1293, loss 0.616951, acc 0.8125, prec 0.0392157, recall 0.80719
2017-12-10T15:10:23.004083: step 1294, loss 0.230846, acc 0.921875, prec 0.0392842, recall 0.807504
2017-12-10T15:10:23.196660: step 1295, loss 0.56696, acc 0.84375, prec 0.0392686, recall 0.807504
2017-12-10T15:10:23.387858: step 1296, loss 0.290996, acc 0.90625, prec 0.0392973, recall 0.807661
2017-12-10T15:10:23.577986: step 1297, loss 0.736614, acc 0.828125, prec 0.0393183, recall 0.807818
2017-12-10T15:10:23.770243: step 1298, loss 0.250258, acc 0.90625, prec 0.0393089, recall 0.807818
2017-12-10T15:10:23.966883: step 1299, loss 0.325116, acc 0.875, prec 0.0392965, recall 0.807818
2017-12-10T15:10:24.158952: step 1300, loss 0.303102, acc 0.859375, prec 0.0393205, recall 0.807974
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_1/1512936356/checkpoints/model-1300

2017-12-10T15:10:25.324570: step 1301, loss 0.559916, acc 0.8125, prec 0.0393399, recall 0.80813
2017-12-10T15:10:25.519947: step 1302, loss 0.493517, acc 0.890625, prec 0.039443, recall 0.808597
2017-12-10T15:10:25.717420: step 1303, loss 0.374952, acc 0.921875, prec 0.0394732, recall 0.808752
2017-12-10T15:10:25.916099: step 1304, loss 0.98692, acc 0.90625, prec 0.0395018, recall 0.808907
2017-12-10T15:10:26.107407: step 1305, loss 0.173904, acc 0.921875, prec 0.039494, recall 0.808907
2017-12-10T15:10:26.299621: step 1306, loss 0.211942, acc 0.9375, prec 0.0394877, recall 0.808907
2017-12-10T15:10:26.493350: step 1307, loss 1.5974, acc 0.9375, prec 0.039483, recall 0.808252
2017-12-10T15:10:26.686794: step 1308, loss 0.382325, acc 0.90625, prec 0.0395116, recall 0.808407
2017-12-10T15:10:26.882657: step 1309, loss 0.324193, acc 0.828125, prec 0.0394945, recall 0.808407
2017-12-10T15:10:27.072989: step 1310, loss 0.30409, acc 0.875, prec 0.0395199, recall 0.808562
2017-12-10T15:10:27.267485: step 1311, loss 0.16623, acc 0.90625, prec 0.0395485, recall 0.808717
2017-12-10T15:10:27.462307: step 1312, loss 2.4096, acc 0.828125, prec 0.0395329, recall 0.808065
2017-12-10T15:10:27.658354: step 1313, loss 1.85758, acc 0.90625, prec 0.039563, recall 0.807568
2017-12-10T15:10:27.853665: step 1314, loss 0.834993, acc 0.90625, prec 0.0395915, recall 0.807723
2017-12-10T15:10:28.049819: step 1315, loss 0.482743, acc 0.859375, prec 0.0395774, recall 0.807723
2017-12-10T15:10:28.244413: step 1316, loss 0.44646, acc 0.8125, prec 0.0395587, recall 0.807723
2017-12-10T15:10:28.438151: step 1317, loss 4.69348, acc 0.8125, prec 0.039581, recall 0.806581
2017-12-10T15:10:28.631844: step 1318, loss 0.476366, acc 0.828125, prec 0.0395638, recall 0.806581
2017-12-10T15:10:28.821438: step 1319, loss 2.54815, acc 0.71875, prec 0.0395374, recall 0.805934
2017-12-10T15:10:29.018652: step 1320, loss 0.530716, acc 0.75, prec 0.039588, recall 0.806245
2017-12-10T15:10:29.220110: step 1321, loss 1.0743, acc 0.625, prec 0.0395507, recall 0.806245
2017-12-10T15:10:29.416611: step 1322, loss 0.918775, acc 0.75, prec 0.0395635, recall 0.8064
2017-12-10T15:10:29.609449: step 1323, loss 0.749491, acc 0.671875, prec 0.039531, recall 0.8064
2017-12-10T15:10:29.800202: step 1324, loss 0.894306, acc 0.65625, prec 0.0395345, recall 0.806555
2017-12-10T15:10:29.991657: step 1325, loss 0.854221, acc 0.6875, prec 0.0395788, recall 0.806864
2017-12-10T15:10:30.186013: step 1326, loss 0.905175, acc 0.578125, prec 0.0395745, recall 0.807018
2017-12-10T15:10:30.377083: step 1327, loss 1.84301, acc 0.609375, prec 0.0396484, recall 0.807478
2017-12-10T15:10:30.567068: step 1328, loss 0.815105, acc 0.703125, prec 0.039619, recall 0.807478
2017-12-10T15:10:30.756861: step 1329, loss 1.1696, acc 0.578125, prec 0.0395773, recall 0.807478
2017-12-10T15:10:30.949963: step 1330, loss 1.03383, acc 0.59375, prec 0.0395372, recall 0.807478
2017-12-10T15:10:31.145576: step 1331, loss 0.818398, acc 0.734375, prec 0.0395485, recall 0.807631
2017-12-10T15:10:31.345255: step 1332, loss 0.697474, acc 0.6875, prec 0.0395177, recall 0.807631
2017-12-10T15:10:31.539798: step 1333, loss 0.739825, acc 0.71875, prec 0.0394901, recall 0.807631
2017-12-10T15:10:31.731362: step 1334, loss 0.909063, acc 0.78125, prec 0.0395805, recall 0.808089
2017-12-10T15:10:31.924219: step 1335, loss 0.795039, acc 0.734375, prec 0.0395544, recall 0.808089
2017-12-10T15:10:32.117826: step 1336, loss 2.74107, acc 0.8125, prec 0.0396865, recall 0.808057
2017-12-10T15:10:32.316210: step 1337, loss 0.375654, acc 0.8125, prec 0.0396681, recall 0.808057
2017-12-10T15:10:32.508989: step 1338, loss 0.662641, acc 0.828125, prec 0.0396884, recall 0.808208
2017-12-10T15:10:32.701487: step 1339, loss 0.752493, acc 0.765625, prec 0.0397025, recall 0.80836
2017-12-10T15:10:32.892918: step 1340, loss 0.372349, acc 0.90625, prec 0.0397305, recall 0.808511
2017-12-10T15:10:33.085338: step 1341, loss 0.541739, acc 0.796875, prec 0.0397105, recall 0.808511
2017-12-10T15:10:33.282465: step 1342, loss 0.546884, acc 0.75, prec 0.0397231, recall 0.808661
2017-12-10T15:10:33.473846: step 1343, loss 1.3537, acc 0.875, prec 0.039785, recall 0.808962
2017-12-10T15:10:33.668569: step 1344, loss 0.346569, acc 0.8125, prec 0.0398037, recall 0.809112
2017-12-10T15:10:33.861063: step 1345, loss 0.525933, acc 0.828125, prec 0.0397868, recall 0.809112
2017-12-10T15:10:34.058212: step 1346, loss 0.261293, acc 0.921875, prec 0.0398162, recall 0.809262
2017-12-10T15:10:34.250499: step 1347, loss 0.279253, acc 0.875, prec 0.0398409, recall 0.809412
2017-12-10T15:10:34.442033: step 1348, loss 0.584471, acc 0.90625, prec 0.0399058, recall 0.80971
2017-12-10T15:10:34.639193: step 1349, loss 0.277749, acc 0.890625, prec 0.0398951, recall 0.80971
2017-12-10T15:10:34.835166: step 1350, loss 0.372177, acc 0.859375, prec 0.0398812, recall 0.80971
2017-12-10T15:10:35.026885: step 1351, loss 1.33549, acc 0.8125, prec 0.0399368, recall 0.810008
2017-12-10T15:10:35.220762: step 1352, loss 0.393319, acc 0.875, prec 0.0399985, recall 0.810304
2017-12-10T15:10:35.411067: step 1353, loss 0.381255, acc 0.875, prec 0.0399861, recall 0.810304
2017-12-10T15:10:35.602398: step 1354, loss 0.43519, acc 0.84375, prec 0.0399707, recall 0.810304
2017-12-10T15:10:35.795784: step 1355, loss 0.405468, acc 0.90625, prec 0.0399985, recall 0.810452
2017-12-10T15:10:35.987581: step 1356, loss 0.204143, acc 0.9375, prec 0.0399923, recall 0.810452
2017-12-10T15:10:36.181963: step 1357, loss 0.247435, acc 0.890625, prec 0.0399815, recall 0.810452
2017-12-10T15:10:36.372050: step 1358, loss 0.287033, acc 0.890625, prec 0.0399708, recall 0.810452
2017-12-10T15:10:36.564925: step 1359, loss 0.375888, acc 0.96875, prec 0.0400415, recall 0.810748
2017-12-10T15:10:36.764823: step 1360, loss 0.34155, acc 0.90625, prec 0.0400323, recall 0.810748
2017-12-10T15:10:36.964168: step 1361, loss 0.300331, acc 0.90625, prec 0.0400231, recall 0.810748
2017-12-10T15:10:37.153367: step 1362, loss 0.240854, acc 0.890625, prec 0.0400123, recall 0.810748
2017-12-10T15:10:37.343730: step 1363, loss 0.303736, acc 0.875, prec 0.04, recall 0.810748
2017-12-10T15:10:37.540275: step 1364, loss 2.25493, acc 0.875, prec 0.0399892, recall 0.810117
2017-12-10T15:10:37.734007: step 1365, loss 0.331878, acc 0.859375, prec 0.0400492, recall 0.810412
2017-12-10T15:10:37.925572: step 1366, loss 11.9046, acc 0.9375, prec 0.0400445, recall 0.809783
2017-12-10T15:10:38.118709: step 1367, loss 0.872171, acc 0.921875, prec 0.0401474, recall 0.810225
2017-12-10T15:10:38.311512: step 1368, loss 0.412376, acc 0.9375, prec 0.0402149, recall 0.810518
2017-12-10T15:10:38.506883: step 1369, loss 1.32572, acc 0.8125, prec 0.0402332, recall 0.810665
2017-12-10T15:10:38.700919: step 1370, loss 0.531244, acc 0.828125, prec 0.0402162, recall 0.810665
2017-12-10T15:10:38.895772: step 1371, loss 0.559834, acc 0.828125, prec 0.0401993, recall 0.810665
2017-12-10T15:10:39.088349: step 1372, loss 0.496902, acc 0.8125, prec 0.0401808, recall 0.810665
2017-12-10T15:10:39.280709: step 1373, loss 0.543721, acc 0.78125, prec 0.0401593, recall 0.810665
2017-12-10T15:10:39.474174: step 1374, loss 0.317044, acc 0.859375, prec 0.0401822, recall 0.810811
2017-12-10T15:10:39.667497: step 1375, loss 0.720189, acc 0.71875, prec 0.0401912, recall 0.810957
2017-12-10T15:10:39.853797: step 1376, loss 0.441194, acc 0.828125, prec 0.0401743, recall 0.810957
2017-12-10T15:10:40.044673: step 1377, loss 0.500303, acc 0.8125, prec 0.0401559, recall 0.810957
2017-12-10T15:10:40.235688: step 1378, loss 1.42353, acc 0.828125, prec 0.0401757, recall 0.811103
2017-12-10T15:10:40.430279: step 1379, loss 0.434756, acc 0.765625, prec 0.0401527, recall 0.811103
2017-12-10T15:10:40.618937: step 1380, loss 0.540326, acc 0.875, prec 0.0402503, recall 0.811538
2017-12-10T15:10:40.814897: step 1381, loss 0.896369, acc 0.703125, prec 0.0402577, recall 0.811683
2017-12-10T15:10:41.007953: step 1382, loss 1.16929, acc 0.765625, prec 0.0403444, recall 0.812117
2017-12-10T15:10:41.202350: step 1383, loss 0.499576, acc 0.859375, prec 0.0403671, recall 0.812261
2017-12-10T15:10:41.397781: step 1384, loss 0.402726, acc 0.8125, prec 0.0403487, recall 0.812261
2017-12-10T15:10:41.589080: step 1385, loss 0.656142, acc 0.78125, prec 0.0403272, recall 0.812261
2017-12-10T15:10:41.778949: step 1386, loss 0.676175, acc 0.765625, prec 0.0403042, recall 0.812261
2017-12-10T15:10:41.968116: step 1387, loss 0.470649, acc 0.875, prec 0.0403284, recall 0.812404
2017-12-10T15:10:42.161956: step 1388, loss 0.607275, acc 0.8125, prec 0.04031, recall 0.812404
2017-12-10T15:10:42.359393: step 1389, loss 0.487961, acc 0.796875, prec 0.0402901, recall 0.812404
2017-12-10T15:10:42.554189: step 1390, loss 0.498305, acc 0.8125, prec 0.0402718, recall 0.812404
2017-12-10T15:10:42.744613: step 1391, loss 0.12909, acc 0.984375, prec 0.0402702, recall 0.812404
2017-12-10T15:10:42.935863: step 1392, loss 1.53134, acc 0.890625, prec 0.0402975, recall 0.811927
2017-12-10T15:10:43.126763: step 1393, loss 2.54973, acc 0.84375, prec 0.0402837, recall 0.811306
2017-12-10T15:10:43.318853: step 1394, loss 0.378592, acc 0.90625, prec 0.040311, recall 0.81145
2017-12-10T15:10:43.513355: step 1395, loss 0.442025, acc 0.890625, prec 0.0403003, recall 0.81145
2017-12-10T15:10:43.709541: step 1396, loss 0.434758, acc 0.828125, prec 0.0403198, recall 0.811594
2017-12-10T15:10:43.900631: step 1397, loss 0.233123, acc 0.875, prec 0.040344, recall 0.811738
2017-12-10T15:10:44.091814: step 1398, loss 0.675043, acc 0.8125, prec 0.0404346, recall 0.812167
2017-12-10T15:10:44.293811: step 1399, loss 0.217083, acc 0.953125, prec 0.04043, recall 0.812167
2017-12-10T15:10:44.482292: step 1400, loss 0.593386, acc 0.796875, prec 0.0404102, recall 0.812167
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_1/1512936356/checkpoints/model-1400

2017-12-10T15:10:45.625672: step 1401, loss 0.371344, acc 0.859375, prec 0.0403964, recall 0.812167
2017-12-10T15:10:45.816179: step 1402, loss 0.937997, acc 0.90625, prec 0.0404235, recall 0.81231
2017-12-10T15:10:46.011355: step 1403, loss 0.43591, acc 0.84375, prec 0.0404082, recall 0.81231
2017-12-10T15:10:46.203450: step 1404, loss 6.8227, acc 0.859375, prec 0.0404323, recall 0.811836
2017-12-10T15:10:46.404598: step 1405, loss 4.14007, acc 0.890625, prec 0.0404594, recall 0.811364
2017-12-10T15:10:46.597170: step 1406, loss 5.58073, acc 0.828125, prec 0.0404803, recall 0.810893
2017-12-10T15:10:46.789668: step 1407, loss 0.696584, acc 0.75, prec 0.0404559, recall 0.810893
2017-12-10T15:10:46.981078: step 1408, loss 0.578397, acc 0.78125, prec 0.0404707, recall 0.811036
2017-12-10T15:10:47.170214: step 1409, loss 1.02482, acc 0.640625, prec 0.0404718, recall 0.811178
2017-12-10T15:10:47.361180: step 1410, loss 1.2233, acc 0.5625, prec 0.0404291, recall 0.811178
2017-12-10T15:10:47.551355: step 1411, loss 1.36477, acc 0.59375, prec 0.0404618, recall 0.811463
2017-12-10T15:10:47.741950: step 1412, loss 1.51016, acc 0.5625, prec 0.0404192, recall 0.811463
2017-12-10T15:10:47.932115: step 1413, loss 1.8782, acc 0.53125, prec 0.0403737, recall 0.811463
2017-12-10T15:10:48.122967: step 1414, loss 1.02431, acc 0.625, prec 0.0404813, recall 0.81203
2017-12-10T15:10:48.314502: step 1415, loss 1.16732, acc 0.65625, prec 0.0405198, recall 0.812312
2017-12-10T15:10:48.510061: step 1416, loss 1.1547, acc 0.640625, prec 0.0404849, recall 0.812312
2017-12-10T15:10:48.707811: step 1417, loss 1.32032, acc 0.65625, prec 0.0405234, recall 0.812594
2017-12-10T15:10:48.900081: step 1418, loss 1.35239, acc 0.640625, prec 0.0405244, recall 0.812734
2017-12-10T15:10:49.090404: step 1419, loss 0.882785, acc 0.75, prec 0.040536, recall 0.812874
2017-12-10T15:10:49.285841: step 1420, loss 1.49214, acc 0.578125, prec 0.040531, recall 0.813014
2017-12-10T15:10:49.478604: step 1421, loss 0.654889, acc 0.8125, prec 0.0405486, recall 0.813154
2017-12-10T15:10:49.669031: step 1422, loss 0.570377, acc 0.796875, prec 0.0406004, recall 0.813433
2017-12-10T15:10:49.865720: step 1423, loss 0.951328, acc 0.78125, prec 0.040615, recall 0.813572
2017-12-10T15:10:50.061309: step 1424, loss 0.362013, acc 0.875, prec 0.0406029, recall 0.813572
2017-12-10T15:10:50.252247: step 1425, loss 0.509768, acc 0.859375, prec 0.0405893, recall 0.813572
2017-12-10T15:10:50.442387: step 1426, loss 0.823533, acc 0.828125, prec 0.0406441, recall 0.81385
2017-12-10T15:10:50.633023: step 1427, loss 0.352223, acc 0.859375, prec 0.0406305, recall 0.81385
2017-12-10T15:10:50.829129: step 1428, loss 0.241812, acc 0.921875, prec 0.0406229, recall 0.81385
2017-12-10T15:10:51.021364: step 1429, loss 0.338901, acc 0.8125, prec 0.0406404, recall 0.813988
2017-12-10T15:10:51.218157: step 1430, loss 0.48413, acc 0.84375, prec 0.040661, recall 0.814126
2017-12-10T15:10:51.410774: step 1431, loss 0.173623, acc 0.9375, prec 0.0406549, recall 0.814126
2017-12-10T15:10:51.606319: step 1432, loss 0.159718, acc 0.953125, prec 0.040686, recall 0.814264
2017-12-10T15:10:51.798615: step 1433, loss 0.273575, acc 0.921875, prec 0.0406785, recall 0.814264
2017-12-10T15:10:51.991462: step 1434, loss 0.239115, acc 0.96875, prec 0.0407111, recall 0.814402
2017-12-10T15:10:52.186902: step 1435, loss 0.126069, acc 0.984375, prec 0.0407451, recall 0.81454
2017-12-10T15:10:52.394331: step 1436, loss 1.27388, acc 0.953125, prec 0.0408118, recall 0.814815
2017-12-10T15:10:52.593285: step 1437, loss 1.35805, acc 0.953125, prec 0.0408088, recall 0.814212
2017-12-10T15:10:52.789214: step 1438, loss 0.216148, acc 0.921875, prec 0.0408012, recall 0.814212
2017-12-10T15:10:52.986091: step 1439, loss 0.254948, acc 0.96875, prec 0.0408693, recall 0.814486
2017-12-10T15:10:53.194912: step 1440, loss 0.441273, acc 0.90625, prec 0.0408602, recall 0.814486
2017-12-10T15:10:53.389662: step 1441, loss 0.112929, acc 0.953125, prec 0.0408557, recall 0.814486
2017-12-10T15:10:53.585859: step 1442, loss 1.07999, acc 0.921875, prec 0.0408836, recall 0.814623
2017-12-10T15:10:53.780432: step 1443, loss 0.158164, acc 0.9375, prec 0.0408776, recall 0.814623
2017-12-10T15:10:53.973460: step 1444, loss 6.58606, acc 0.9375, prec 0.0409086, recall 0.814159
2017-12-10T15:10:54.172483: step 1445, loss 0.185533, acc 0.953125, prec 0.040904, recall 0.814159
2017-12-10T15:10:54.366541: step 1446, loss 0.364667, acc 0.90625, prec 0.0409305, recall 0.814296
2017-12-10T15:10:54.557496: step 1447, loss 0.47218, acc 0.875, prec 0.0409539, recall 0.814433
2017-12-10T15:10:54.749680: step 1448, loss 1.09579, acc 0.828125, prec 0.0409727, recall 0.81457
2017-12-10T15:10:54.945004: step 1449, loss 0.36902, acc 0.859375, prec 0.040959, recall 0.81457
2017-12-10T15:10:55.141950: step 1450, loss 0.929019, acc 0.84375, prec 0.0410503, recall 0.814978
2017-12-10T15:10:55.337636: step 1451, loss 0.255304, acc 0.875, prec 0.0410382, recall 0.814978
2017-12-10T15:10:55.535651: step 1452, loss 0.684045, acc 0.8125, prec 0.04102, recall 0.814978
2017-12-10T15:10:55.725806: step 1453, loss 0.748267, acc 0.8125, prec 0.0410372, recall 0.815114
2017-12-10T15:10:55.919724: step 1454, loss 0.639171, acc 0.796875, prec 0.0410529, recall 0.815249
2017-12-10T15:10:56.112061: step 1455, loss 0.552216, acc 0.796875, prec 0.041104, recall 0.81552
2017-12-10T15:10:56.309999: step 1456, loss 0.660552, acc 0.78125, prec 0.0411535, recall 0.815789
2017-12-10T15:10:56.502456: step 1457, loss 0.806782, acc 0.796875, prec 0.0413104, recall 0.81646
2017-12-10T15:10:56.697798: step 1458, loss 0.435877, acc 0.890625, prec 0.0413351, recall 0.816594
2017-12-10T15:10:56.892000: step 1459, loss 0.520431, acc 0.828125, prec 0.0413184, recall 0.816594
2017-12-10T15:10:57.085340: step 1460, loss 0.848553, acc 0.78125, prec 0.0413324, recall 0.816727
2017-12-10T15:10:57.279919: step 1461, loss 0.816951, acc 0.765625, prec 0.0413095, recall 0.816727
2017-12-10T15:10:57.469020: step 1462, loss 0.501678, acc 0.859375, prec 0.0413311, recall 0.81686
2017-12-10T15:10:57.662436: step 1463, loss 2.83217, acc 0.796875, prec 0.0413129, recall 0.816267
2017-12-10T15:10:57.858944: step 1464, loss 0.821102, acc 0.859375, prec 0.0413345, recall 0.816401
2017-12-10T15:10:58.060229: step 1465, loss 0.520923, acc 0.859375, prec 0.041356, recall 0.816534
2017-12-10T15:10:58.252910: step 1466, loss 0.537613, acc 0.796875, prec 0.0414067, recall 0.816799
2017-12-10T15:10:58.445779: step 1467, loss 0.586299, acc 0.828125, prec 0.0414251, recall 0.816932
2017-12-10T15:10:58.640296: step 1468, loss 0.680138, acc 0.875, prec 0.0414481, recall 0.817064
2017-12-10T15:10:58.836418: step 1469, loss 0.491252, acc 0.765625, prec 0.0414253, recall 0.817064
2017-12-10T15:10:59.030170: step 1470, loss 0.388004, acc 0.875, prec 0.0414132, recall 0.817064
2017-12-10T15:10:59.226700: step 1471, loss 0.540571, acc 0.875, prec 0.0414713, recall 0.817329
2017-12-10T15:10:59.418829: step 1472, loss 0.334679, acc 0.859375, prec 0.0414927, recall 0.81746
2017-12-10T15:10:59.614382: step 1473, loss 2.12107, acc 0.78125, prec 0.0415081, recall 0.817003
2017-12-10T15:10:59.809427: step 1474, loss 0.360102, acc 0.84375, prec 0.0414929, recall 0.817003
2017-12-10T15:11:00.002195: step 1475, loss 0.454653, acc 0.828125, prec 0.0415112, recall 0.817135
2017-12-10T15:11:00.194115: step 1476, loss 0.336137, acc 0.859375, prec 0.0415326, recall 0.817266
2017-12-10T15:11:00.387893: step 1477, loss 0.435837, acc 0.90625, prec 0.0415585, recall 0.817398
2017-12-10T15:11:00.583771: step 1478, loss 0.331909, acc 0.859375, prec 0.0415449, recall 0.817398
2017-12-10T15:11:00.776498: step 1479, loss 0.188433, acc 0.890625, prec 0.0415342, recall 0.817398
2017-12-10T15:11:00.966171: step 1480, loss 0.136965, acc 0.953125, prec 0.0415297, recall 0.817398
2017-12-10T15:11:01.167747: step 1481, loss 0.32288, acc 0.890625, prec 0.0415191, recall 0.817398
2017-12-10T15:11:01.365992: step 1482, loss 0.270396, acc 0.9375, prec 0.041513, recall 0.817398
2017-12-10T15:11:01.560003: step 1483, loss 0.241905, acc 0.9375, prec 0.041507, recall 0.817398
2017-12-10T15:11:01.754718: step 1484, loss 0.31584, acc 0.90625, prec 0.0414979, recall 0.817398
2017-12-10T15:11:01.945299: step 1485, loss 6.31068, acc 0.921875, prec 0.0415618, recall 0.817073
2017-12-10T15:11:02.143241: step 1486, loss 2.96417, acc 0.859375, prec 0.0415496, recall 0.816487
2017-12-10T15:11:02.339918: step 1487, loss 4.68675, acc 0.875, prec 0.0416089, recall 0.816166
2017-12-10T15:11:02.531635: step 1488, loss 0.30035, acc 0.890625, prec 0.0415983, recall 0.816166
2017-12-10T15:11:02.729053: step 1489, loss 1.4875, acc 0.78125, prec 0.041612, recall 0.816297
2017-12-10T15:11:02.923490: step 1490, loss 0.431843, acc 0.8125, prec 0.0416636, recall 0.81656
2017-12-10T15:11:03.097429: step 1491, loss 0.48488, acc 0.745098, prec 0.0416439, recall 0.81656
2017-12-10T15:11:03.296053: step 1492, loss 1.20819, acc 0.5625, prec 0.0416364, recall 0.81669
2017-12-10T15:11:03.486666: step 1493, loss 1.41206, acc 0.609375, prec 0.041703, recall 0.817082
2017-12-10T15:11:03.679659: step 1494, loss 1.14424, acc 0.625, prec 0.0416667, recall 0.817082
2017-12-10T15:11:03.870589: step 1495, loss 1.00925, acc 0.703125, prec 0.0417075, recall 0.817342
2017-12-10T15:11:04.061836: step 1496, loss 1.30243, acc 0.578125, prec 0.0417014, recall 0.817472
2017-12-10T15:11:04.250877: step 1497, loss 1.12671, acc 0.609375, prec 0.0416983, recall 0.817601
2017-12-10T15:11:04.440483: step 1498, loss 1.24893, acc 0.671875, prec 0.0416667, recall 0.817601
2017-12-10T15:11:04.636122: step 1499, loss 0.886446, acc 0.71875, prec 0.0416396, recall 0.817601
2017-12-10T15:11:04.826023: step 1500, loss 1.16249, acc 0.625, prec 0.0416727, recall 0.81786
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_1/1512936356/checkpoints/model-1500

2017-12-10T15:11:06.063802: step 1501, loss 0.797093, acc 0.796875, prec 0.0416877, recall 0.817989
2017-12-10T15:11:06.254765: step 1502, loss 0.701423, acc 0.75, prec 0.0416637, recall 0.817989
2017-12-10T15:11:06.443802: step 1503, loss 0.639769, acc 0.765625, prec 0.0416757, recall 0.818117
2017-12-10T15:11:06.633328: step 1504, loss 0.320757, acc 0.90625, prec 0.0417012, recall 0.818246
2017-12-10T15:11:06.828079: step 1505, loss 0.661326, acc 0.828125, prec 0.0416847, recall 0.818246
2017-12-10T15:11:07.019203: step 1506, loss 0.323183, acc 0.890625, prec 0.0417087, recall 0.818375
2017-12-10T15:11:07.210191: step 1507, loss 0.290309, acc 0.890625, prec 0.0416982, recall 0.818375
2017-12-10T15:11:07.403576: step 1508, loss 0.405747, acc 0.859375, prec 0.0416847, recall 0.818375
2017-12-10T15:11:07.593745: step 1509, loss 0.229717, acc 0.921875, prec 0.0416772, recall 0.818375
2017-12-10T15:11:07.784287: step 1510, loss 0.226646, acc 0.9375, prec 0.0416712, recall 0.818375
2017-12-10T15:11:07.977350: step 1511, loss 0.268948, acc 0.9375, prec 0.0416652, recall 0.818375
2017-12-10T15:11:08.172167: step 1512, loss 0.40822, acc 0.90625, prec 0.0417251, recall 0.818631
2017-12-10T15:11:08.368668: step 1513, loss 0.224918, acc 0.953125, prec 0.0417206, recall 0.818631
2017-12-10T15:11:08.559722: step 1514, loss 0.477555, acc 0.890625, prec 0.041779, recall 0.818887
2017-12-10T15:11:08.756218: step 1515, loss 3.07979, acc 0.9375, prec 0.041809, recall 0.818438
2017-12-10T15:11:08.953660: step 1516, loss 0.184115, acc 0.953125, prec 0.0418389, recall 0.818565
2017-12-10T15:11:09.148770: step 1517, loss 3.18537, acc 0.953125, prec 0.0419392, recall 0.818373
2017-12-10T15:11:09.351879: step 1518, loss 1.8497, acc 0.9375, prec 0.0419691, recall 0.817927
2017-12-10T15:11:09.553386: step 1519, loss 0.161819, acc 0.9375, prec 0.0420319, recall 0.818182
2017-12-10T15:11:09.744239: step 1520, loss 0.192097, acc 0.921875, prec 0.0420244, recall 0.818182
2017-12-10T15:11:09.935949: step 1521, loss 0.726629, acc 0.8125, prec 0.042075, recall 0.818436
2017-12-10T15:11:10.128908: step 1522, loss 0.398696, acc 0.875, prec 0.0421317, recall 0.818689
2017-12-10T15:11:10.322563: step 1523, loss 0.886676, acc 0.78125, prec 0.0421449, recall 0.818815
2017-12-10T15:11:10.515638: step 1524, loss 0.521934, acc 0.8125, prec 0.0421955, recall 0.819067
2017-12-10T15:11:10.708183: step 1525, loss 0.435294, acc 0.859375, prec 0.0422162, recall 0.819193
2017-12-10T15:11:10.902427: step 1526, loss 0.533992, acc 0.75, prec 0.0423292, recall 0.819695
2017-12-10T15:11:11.094828: step 1527, loss 0.675905, acc 0.78125, prec 0.0423422, recall 0.81982
2017-12-10T15:11:11.286182: step 1528, loss 0.768879, acc 0.734375, prec 0.0423165, recall 0.81982
2017-12-10T15:11:11.484220: step 1529, loss 0.505133, acc 0.84375, prec 0.0423698, recall 0.820069
2017-12-10T15:11:11.677072: step 1530, loss 0.808806, acc 0.765625, prec 0.0423814, recall 0.820194
2017-12-10T15:11:11.869122: step 1531, loss 0.457752, acc 0.828125, prec 0.0423647, recall 0.820194
2017-12-10T15:11:12.061491: step 1532, loss 0.831927, acc 0.78125, prec 0.0423435, recall 0.820194
2017-12-10T15:11:12.263254: step 1533, loss 0.272477, acc 0.859375, prec 0.0423641, recall 0.820318
2017-12-10T15:11:12.455097: step 1534, loss 0.968079, acc 0.8125, prec 0.0424485, recall 0.82069
2017-12-10T15:11:12.648577: step 1535, loss 0.511378, acc 0.765625, prec 0.0424599, recall 0.820813
2017-12-10T15:11:12.841956: step 1536, loss 0.24669, acc 0.890625, prec 0.0424834, recall 0.820937
2017-12-10T15:11:13.035493: step 1537, loss 0.274831, acc 0.921875, prec 0.0425441, recall 0.821183
2017-12-10T15:11:13.228469: step 1538, loss 0.634054, acc 0.84375, prec 0.0426653, recall 0.821674
2017-12-10T15:11:13.422807: step 1539, loss 0.972815, acc 0.90625, prec 0.0426903, recall 0.821796
2017-12-10T15:11:13.622189: step 1540, loss 0.389315, acc 0.890625, prec 0.0427137, recall 0.821918
2017-12-10T15:11:13.814478: step 1541, loss 3.37095, acc 0.9375, prec 0.0427433, recall 0.821477
2017-12-10T15:11:14.008530: step 1542, loss 0.360772, acc 0.890625, prec 0.0427326, recall 0.821477
2017-12-10T15:11:14.205779: step 1543, loss 0.655733, acc 0.875, prec 0.0427885, recall 0.821721
2017-12-10T15:11:14.401689: step 1544, loss 0.348689, acc 0.890625, prec 0.04288, recall 0.822086
2017-12-10T15:11:14.595213: step 1545, loss 0.25745, acc 0.890625, prec 0.0428693, recall 0.822086
2017-12-10T15:11:14.789565: step 1546, loss 0.889624, acc 0.890625, prec 0.0428927, recall 0.822207
2017-12-10T15:11:14.981527: step 1547, loss 0.164986, acc 0.921875, prec 0.0428851, recall 0.822207
2017-12-10T15:11:15.172662: step 1548, loss 0.676738, acc 0.828125, prec 0.0429023, recall 0.822328
2017-12-10T15:11:15.362747: step 1549, loss 0.607425, acc 0.765625, prec 0.0428795, recall 0.822328
2017-12-10T15:11:15.555427: step 1550, loss 0.487452, acc 0.8125, prec 0.0428952, recall 0.822449
2017-12-10T15:11:15.745354: step 1551, loss 0.700983, acc 0.84375, prec 0.0428799, recall 0.822449
2017-12-10T15:11:15.935809: step 1552, loss 0.513429, acc 0.84375, prec 0.0429665, recall 0.822811
2017-12-10T15:11:16.127295: step 1553, loss 0.438774, acc 0.84375, prec 0.0429513, recall 0.822811
2017-12-10T15:11:16.323608: step 1554, loss 0.406358, acc 0.890625, prec 0.0430085, recall 0.823051
2017-12-10T15:11:16.512831: step 1555, loss 0.861179, acc 0.875, prec 0.0430302, recall 0.823171
2017-12-10T15:11:16.708853: step 1556, loss 0.954834, acc 0.84375, prec 0.0430827, recall 0.82341
2017-12-10T15:11:16.905279: step 1557, loss 0.240081, acc 0.90625, prec 0.0431074, recall 0.823529
2017-12-10T15:11:17.099425: step 1558, loss 0.269523, acc 0.875, prec 0.0431629, recall 0.823768
2017-12-10T15:11:17.296222: step 1559, loss 0.455304, acc 0.84375, prec 0.0431477, recall 0.823768
2017-12-10T15:11:17.491742: step 1560, loss 0.425302, acc 0.875, prec 0.0431355, recall 0.823768
2017-12-10T15:11:17.685616: step 1561, loss 0.34938, acc 0.828125, prec 0.0432201, recall 0.824124
2017-12-10T15:11:17.879091: step 1562, loss 0.350018, acc 0.875, prec 0.0432755, recall 0.824361
2017-12-10T15:11:18.071415: step 1563, loss 0.338554, acc 0.875, prec 0.0433309, recall 0.824597
2017-12-10T15:11:18.266030: step 1564, loss 0.606568, acc 0.9375, prec 0.0433923, recall 0.824832
2017-12-10T15:11:18.456373: step 1565, loss 1.87078, acc 0.859375, prec 0.0434138, recall 0.824397
2017-12-10T15:11:18.651813: step 1566, loss 0.335034, acc 0.875, prec 0.0434016, recall 0.824397
2017-12-10T15:11:18.842049: step 1567, loss 0.38725, acc 0.859375, prec 0.0433878, recall 0.824397
2017-12-10T15:11:19.032872: step 1568, loss 0.954657, acc 0.828125, prec 0.0434721, recall 0.824749
2017-12-10T15:11:19.227119: step 1569, loss 3.43061, acc 0.828125, prec 0.0435242, recall 0.824433
2017-12-10T15:11:19.426188: step 1570, loss 0.317115, acc 0.90625, prec 0.043515, recall 0.824433
2017-12-10T15:11:19.623484: step 1571, loss 0.654182, acc 0.75, prec 0.0435242, recall 0.82455
2017-12-10T15:11:19.817475: step 1572, loss 0.840169, acc 0.859375, prec 0.0435441, recall 0.824667
2017-12-10T15:11:20.012195: step 1573, loss 0.65421, acc 0.765625, prec 0.0435211, recall 0.824667
2017-12-10T15:11:20.206844: step 1574, loss 0.392026, acc 0.859375, prec 0.0435073, recall 0.824667
2017-12-10T15:11:20.402115: step 1575, loss 0.536691, acc 0.875, prec 0.043596, recall 0.825017
2017-12-10T15:11:20.594895: step 1576, loss 0.671253, acc 0.765625, prec 0.043573, recall 0.825017
2017-12-10T15:11:20.784215: step 1577, loss 0.646575, acc 0.78125, prec 0.0435852, recall 0.825133
2017-12-10T15:11:20.972136: step 1578, loss 0.445336, acc 0.84375, prec 0.0435698, recall 0.825133
2017-12-10T15:11:21.161240: step 1579, loss 0.523835, acc 0.859375, prec 0.0435897, recall 0.825249
2017-12-10T15:11:21.354504: step 1580, loss 0.459786, acc 0.84375, prec 0.0436079, recall 0.825365
2017-12-10T15:11:21.543169: step 1581, loss 0.400899, acc 0.890625, prec 0.0436308, recall 0.825481
2017-12-10T15:11:21.736481: step 1582, loss 0.453261, acc 0.875, prec 0.0436856, recall 0.825712
2017-12-10T15:11:21.927358: step 1583, loss 0.389022, acc 0.828125, prec 0.0436687, recall 0.825712
2017-12-10T15:11:22.123430: step 1584, loss 0.302398, acc 0.890625, prec 0.0436915, recall 0.825828
2017-12-10T15:11:22.313707: step 1585, loss 0.398743, acc 0.890625, prec 0.0436808, recall 0.825828
2017-12-10T15:11:22.504116: step 1586, loss 0.390018, acc 0.890625, prec 0.0437036, recall 0.825943
2017-12-10T15:11:22.695194: step 1587, loss 0.3208, acc 0.9375, prec 0.0436975, recall 0.825943
2017-12-10T15:11:22.890714: step 1588, loss 0.152569, acc 0.9375, prec 0.0437248, recall 0.826058
2017-12-10T15:11:23.082911: step 1589, loss 0.277213, acc 0.828125, prec 0.043708, recall 0.826058
2017-12-10T15:11:23.275477: step 1590, loss 0.274749, acc 0.90625, prec 0.0437657, recall 0.826288
2017-12-10T15:11:23.468104: step 1591, loss 0.194249, acc 0.953125, prec 0.0437611, recall 0.826288
2017-12-10T15:11:23.660404: step 1592, loss 1.20938, acc 0.953125, prec 0.0437915, recall 0.825858
2017-12-10T15:11:23.855020: step 1593, loss 2.78293, acc 0.984375, prec 0.0438584, recall 0.825543
2017-12-10T15:11:24.048114: step 1594, loss 0.328852, acc 0.90625, prec 0.0438827, recall 0.825658
2017-12-10T15:11:24.239259: step 1595, loss 0.690051, acc 0.875, prec 0.0439038, recall 0.825773
2017-12-10T15:11:24.432359: step 1596, loss 0.310924, acc 0.890625, prec 0.0438931, recall 0.825773
2017-12-10T15:11:24.622664: step 1597, loss 0.447119, acc 0.859375, prec 0.0438793, recall 0.825773
2017-12-10T15:11:24.818098: step 1598, loss 0.395522, acc 0.890625, prec 0.0438685, recall 0.825773
2017-12-10T15:11:25.009997: step 1599, loss 0.771102, acc 0.828125, prec 0.0439184, recall 0.826001
2017-12-10T15:11:25.203643: step 1600, loss 0.66072, acc 0.703125, prec 0.043956, recall 0.82623
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_1/1512936356/checkpoints/model-1600

2017-12-10T15:11:26.416983: step 1601, loss 0.32642, acc 0.890625, prec 0.0439453, recall 0.82623
2017-12-10T15:11:26.609823: step 1602, loss 0.1995, acc 0.890625, prec 0.0439346, recall 0.82623
2017-12-10T15:11:26.801342: step 1603, loss 0.402837, acc 0.84375, prec 0.0439193, recall 0.82623
2017-12-10T15:11:26.989874: step 1604, loss 0.280309, acc 0.890625, prec 0.0439086, recall 0.82623
2017-12-10T15:11:27.181782: step 1605, loss 0.217232, acc 0.921875, prec 0.0439009, recall 0.82623
2017-12-10T15:11:27.373058: step 1606, loss 0.547574, acc 0.8125, prec 0.0439159, recall 0.826343
2017-12-10T15:11:27.563828: step 1607, loss 0.303012, acc 0.828125, prec 0.0439323, recall 0.826457
2017-12-10T15:11:27.757010: step 1608, loss 0.128881, acc 0.9375, prec 0.0439595, recall 0.826571
2017-12-10T15:11:27.950308: step 1609, loss 0.421873, acc 0.859375, prec 0.043979, recall 0.826684
2017-12-10T15:11:28.142438: step 1610, loss 0.444864, acc 0.8125, prec 0.0439606, recall 0.826684
2017-12-10T15:11:28.335490: step 1611, loss 0.0977797, acc 0.96875, prec 0.0439576, recall 0.826684
2017-12-10T15:11:28.529189: step 1612, loss 0.294444, acc 0.875, prec 0.0439453, recall 0.826684
2017-12-10T15:11:28.722716: step 1613, loss 0.267265, acc 0.90625, prec 0.0439694, recall 0.826797
2017-12-10T15:11:28.913686: step 1614, loss 0.277769, acc 0.90625, prec 0.0439602, recall 0.826797
2017-12-10T15:11:29.105182: step 1615, loss 0.679534, acc 0.90625, prec 0.0440507, recall 0.827136
2017-12-10T15:11:29.307520: step 1616, loss 1.0545, acc 0.9375, prec 0.0440778, recall 0.827249
2017-12-10T15:11:29.500764: step 1617, loss 0.246938, acc 0.90625, prec 0.0440686, recall 0.827249
2017-12-10T15:11:29.696847: step 1618, loss 0.142916, acc 0.921875, prec 0.0440942, recall 0.827362
2017-12-10T15:11:29.889503: step 1619, loss 0.264732, acc 0.90625, prec 0.044085, recall 0.827362
2017-12-10T15:11:30.081561: step 1620, loss 0.623522, acc 0.9375, prec 0.0441452, recall 0.827586
2017-12-10T15:11:30.280683: step 1621, loss 0.810869, acc 0.9375, prec 0.0442054, recall 0.82781
2017-12-10T15:11:30.472785: step 1622, loss 0.277824, acc 0.921875, prec 0.0442309, recall 0.827922
2017-12-10T15:11:30.665953: step 1623, loss 0.127994, acc 0.953125, prec 0.0442595, recall 0.828034
2017-12-10T15:11:30.859531: step 1624, loss 0.197422, acc 0.90625, prec 0.0442502, recall 0.828034
2017-12-10T15:11:31.052491: step 1625, loss 0.13884, acc 0.953125, prec 0.0442456, recall 0.828034
2017-12-10T15:11:31.251756: step 1626, loss 0.40092, acc 0.921875, prec 0.0443042, recall 0.828257
2017-12-10T15:11:31.451241: step 1627, loss 0.269102, acc 0.890625, prec 0.0443266, recall 0.828368
2017-12-10T15:11:31.645275: step 1628, loss 0.643952, acc 0.828125, prec 0.0443428, recall 0.828479
2017-12-10T15:11:31.838728: step 1629, loss 0.187831, acc 0.9375, prec 0.0443367, recall 0.828479
2017-12-10T15:11:32.036552: step 1630, loss 0.612665, acc 0.953125, prec 0.0443652, recall 0.82859
2017-12-10T15:11:32.234479: step 1631, loss 0.455096, acc 0.828125, prec 0.0443814, recall 0.828701
2017-12-10T15:11:32.424921: step 1632, loss 0.768386, acc 0.921875, prec 0.0444729, recall 0.829032
2017-12-10T15:11:32.618616: step 1633, loss 0.324724, acc 0.921875, prec 0.0444652, recall 0.829032
2017-12-10T15:11:32.809530: step 1634, loss 0.220528, acc 0.9375, prec 0.0444921, recall 0.829143
2017-12-10T15:11:33.002691: step 1635, loss 0.974091, acc 0.875, prec 0.0445789, recall 0.829472
2017-12-10T15:11:33.195151: step 1636, loss 0.471778, acc 0.921875, prec 0.0446043, recall 0.829582
2017-12-10T15:11:33.386320: step 1637, loss 4.71061, acc 0.875, prec 0.0446265, recall 0.829159
2017-12-10T15:11:33.579759: step 1638, loss 0.411588, acc 0.828125, prec 0.0446425, recall 0.829268
2017-12-10T15:11:33.769969: step 1639, loss 0.243272, acc 0.890625, prec 0.0446318, recall 0.829268
2017-12-10T15:11:33.963221: step 1640, loss 0.604514, acc 0.75, prec 0.0446401, recall 0.829378
2017-12-10T15:11:34.154606: step 1641, loss 0.570956, acc 0.828125, prec 0.0446561, recall 0.829487
2017-12-10T15:11:34.345561: step 1642, loss 0.390568, acc 0.875, prec 0.0447097, recall 0.829705
2017-12-10T15:11:34.537927: step 1643, loss 0.400033, acc 0.859375, prec 0.0447288, recall 0.829814
2017-12-10T15:11:34.731193: step 1644, loss 0.487079, acc 0.8125, prec 0.0447103, recall 0.829814
2017-12-10T15:11:34.923948: step 1645, loss 0.498571, acc 0.828125, prec 0.0447262, recall 0.829923
2017-12-10T15:11:35.118671: step 1646, loss 0.734318, acc 0.734375, prec 0.0447329, recall 0.830032
2017-12-10T15:11:35.317725: step 1647, loss 0.6201, acc 0.75, prec 0.0447083, recall 0.830032
2017-12-10T15:11:35.512807: step 1648, loss 0.499482, acc 0.828125, prec 0.0447243, recall 0.83014
2017-12-10T15:11:35.705658: step 1649, loss 0.648965, acc 0.8125, prec 0.0447058, recall 0.83014
2017-12-10T15:11:35.899268: step 1650, loss 1.6367, acc 0.890625, prec 0.0446966, recall 0.829611
2017-12-10T15:11:36.094388: step 1651, loss 0.368534, acc 0.90625, prec 0.0447859, recall 0.829936
2017-12-10T15:11:36.287704: step 1652, loss 0.617333, acc 0.90625, prec 0.0448423, recall 0.830153
2017-12-10T15:11:36.485079: step 1653, loss 0.621564, acc 0.796875, prec 0.0448551, recall 0.830261
2017-12-10T15:11:36.677988: step 1654, loss 0.78128, acc 0.71875, prec 0.0448929, recall 0.830476
2017-12-10T15:11:36.868899: step 1655, loss 0.428258, acc 0.90625, prec 0.0449164, recall 0.830584
2017-12-10T15:11:37.066706: step 1656, loss 2.09592, acc 0.859375, prec 0.0449041, recall 0.830057
2017-12-10T15:11:37.261415: step 1657, loss 0.550115, acc 0.828125, prec 0.0448872, recall 0.830057
2017-12-10T15:11:37.457486: step 1658, loss 0.20305, acc 0.953125, prec 0.0448826, recall 0.830057
2017-12-10T15:11:37.652873: step 1659, loss 0.341247, acc 0.890625, prec 0.0449045, recall 0.830165
2017-12-10T15:11:37.844135: step 1660, loss 0.723226, acc 0.859375, prec 0.0450543, recall 0.830701
2017-12-10T15:11:38.039702: step 1661, loss 0.602647, acc 0.796875, prec 0.0450342, recall 0.830701
2017-12-10T15:11:38.230288: step 1662, loss 0.439701, acc 0.84375, prec 0.0450188, recall 0.830701
2017-12-10T15:11:38.427223: step 1663, loss 0.38911, acc 0.828125, prec 0.0450019, recall 0.830701
2017-12-10T15:11:38.616121: step 1664, loss 0.567053, acc 0.84375, prec 0.0450192, recall 0.830808
2017-12-10T15:11:38.809224: step 1665, loss 0.4053, acc 0.890625, prec 0.045041, recall 0.830915
2017-12-10T15:11:39.002001: step 1666, loss 0.563642, acc 0.859375, prec 0.0450925, recall 0.831128
2017-12-10T15:11:39.190693: step 1667, loss 0.544114, acc 0.78125, prec 0.0450709, recall 0.831128
2017-12-10T15:11:39.382195: step 1668, loss 0.650613, acc 0.8125, prec 0.0450524, recall 0.831128
2017-12-10T15:11:39.574136: step 1669, loss 0.546311, acc 0.8125, prec 0.045034, recall 0.831128
2017-12-10T15:11:39.768856: step 1670, loss 0.25977, acc 0.875, prec 0.0450217, recall 0.831128
2017-12-10T15:11:39.960112: step 1671, loss 0.217393, acc 0.921875, prec 0.0451118, recall 0.831447
2017-12-10T15:11:40.155399: step 1672, loss 0.226682, acc 0.921875, prec 0.0451041, recall 0.831447
2017-12-10T15:11:40.346020: step 1673, loss 0.215819, acc 0.90625, prec 0.0450948, recall 0.831447
2017-12-10T15:11:40.536314: step 1674, loss 0.395677, acc 0.875, prec 0.0451151, recall 0.831553
2017-12-10T15:11:40.728608: step 1675, loss 0.419782, acc 0.875, prec 0.0451028, recall 0.831553
2017-12-10T15:11:40.921355: step 1676, loss 0.140189, acc 0.953125, prec 0.0451307, recall 0.831658
2017-12-10T15:11:41.118261: step 1677, loss 0.373963, acc 0.90625, prec 0.0451215, recall 0.831658
2017-12-10T15:11:41.310533: step 1678, loss 0.270981, acc 0.953125, prec 0.045182, recall 0.831869
2017-12-10T15:11:41.505754: step 1679, loss 0.155145, acc 0.921875, prec 0.0451743, recall 0.831869
2017-12-10T15:11:41.696100: step 1680, loss 0.18554, acc 0.921875, prec 0.0451666, recall 0.831869
2017-12-10T15:11:41.887791: step 1681, loss 0.203353, acc 0.890625, prec 0.0451558, recall 0.831869
2017-12-10T15:11:42.080744: step 1682, loss 0.107762, acc 0.9375, prec 0.0451496, recall 0.831869
2017-12-10T15:11:42.271972: step 1683, loss 0.159884, acc 0.953125, prec 0.045145, recall 0.831869
2017-12-10T15:11:42.469291: step 1684, loss 0.0847297, acc 1, prec 0.0451775, recall 0.831975
2017-12-10T15:11:42.660018: step 1685, loss 0.224282, acc 0.953125, prec 0.0451729, recall 0.831975
2017-12-10T15:11:42.856669: step 1686, loss 0.336905, acc 0.96875, prec 0.0452024, recall 0.83208
2017-12-10T15:11:43.050116: step 1687, loss 0.873764, acc 0.9375, prec 0.0452287, recall 0.832185
2017-12-10T15:11:43.251174: step 1688, loss 5.4933, acc 0.921875, prec 0.0452566, recall 0.83125
2017-12-10T15:11:43.450035: step 1689, loss 0.442953, acc 0.921875, prec 0.0452814, recall 0.831355
2017-12-10T15:11:43.646470: step 1690, loss 0.422375, acc 0.96875, prec 0.0453107, recall 0.831461
2017-12-10T15:11:43.841420: step 1691, loss 0.158535, acc 0.953125, prec 0.0453386, recall 0.831566
2017-12-10T15:11:44.039035: step 1692, loss 1.43722, acc 0.90625, prec 0.0453309, recall 0.831047
2017-12-10T15:11:44.238533: step 1693, loss 0.210843, acc 0.90625, prec 0.0453541, recall 0.831153
2017-12-10T15:11:44.433752: step 1694, loss 0.417629, acc 0.890625, prec 0.0453433, recall 0.831153
2017-12-10T15:11:44.631059: step 1695, loss 0.2207, acc 0.890625, prec 0.0453325, recall 0.831153
2017-12-10T15:11:44.827537: step 1696, loss 0.527331, acc 0.828125, prec 0.0453156, recall 0.831153
2017-12-10T15:11:45.017991: step 1697, loss 0.535886, acc 0.828125, prec 0.0452987, recall 0.831153
2017-12-10T15:11:45.209731: step 1698, loss 0.76821, acc 0.828125, prec 0.0453141, recall 0.831258
2017-12-10T15:11:45.404774: step 1699, loss 0.962395, acc 0.875, prec 0.0453666, recall 0.831468
2017-12-10T15:11:45.598698: step 1700, loss 0.448796, acc 0.8125, prec 0.0454129, recall 0.831677
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_1/1512936356/checkpoints/model-1700

2017-12-10T15:11:46.964919: step 1701, loss 0.790369, acc 0.875, prec 0.045433, recall 0.831782
2017-12-10T15:11:47.156833: step 1702, loss 0.462458, acc 0.828125, prec 0.045416, recall 0.831782
2017-12-10T15:11:47.350400: step 1703, loss 0.936862, acc 0.859375, prec 0.0454669, recall 0.83199
2017-12-10T15:11:47.548215: step 1704, loss 0.529209, acc 0.828125, prec 0.0454823, recall 0.832094
2017-12-10T15:11:47.743334: step 1705, loss 0.526016, acc 0.796875, prec 0.0454945, recall 0.832198
2017-12-10T15:11:47.933006: step 1706, loss 1.00744, acc 0.703125, prec 0.0454653, recall 0.832198
2017-12-10T15:11:48.120683: step 1707, loss 0.664671, acc 0.796875, prec 0.0454776, recall 0.832302
2017-12-10T15:11:48.312401: step 1708, loss 0.553807, acc 0.828125, prec 0.0454607, recall 0.832302
2017-12-10T15:11:48.504074: step 1709, loss 0.584259, acc 0.8125, prec 0.0455068, recall 0.832509
2017-12-10T15:11:48.695176: step 1710, loss 0.567847, acc 0.78125, prec 0.0454852, recall 0.832509
2017-12-10T15:11:48.885689: step 1711, loss 0.401457, acc 0.8125, prec 0.0454668, recall 0.832509
2017-12-10T15:11:49.084132: step 1712, loss 0.415513, acc 0.84375, prec 0.0455159, recall 0.832716
2017-12-10T15:11:49.278245: step 1713, loss 2.24383, acc 0.890625, prec 0.0455067, recall 0.832202
2017-12-10T15:11:49.484969: step 1714, loss 1.40107, acc 0.765625, prec 0.0455158, recall 0.832306
2017-12-10T15:11:49.680907: step 1715, loss 0.445568, acc 0.796875, prec 0.0455281, recall 0.832409
2017-12-10T15:11:49.874704: step 1716, loss 0.306584, acc 0.859375, prec 0.0455464, recall 0.832512
2017-12-10T15:11:50.067748: step 1717, loss 1.25754, acc 0.78125, prec 0.0455571, recall 0.832615
2017-12-10T15:11:50.257940: step 1718, loss 0.342387, acc 0.890625, prec 0.0455785, recall 0.832718
2017-12-10T15:11:50.448321: step 1719, loss 0.290368, acc 0.890625, prec 0.0455677, recall 0.832718
2017-12-10T15:11:50.641597: step 1720, loss 0.593433, acc 0.8125, prec 0.0456136, recall 0.832924
2017-12-10T15:11:50.840166: step 1721, loss 0.634114, acc 0.765625, prec 0.0455906, recall 0.832924
2017-12-10T15:11:51.029471: step 1722, loss 0.223258, acc 0.9375, prec 0.0456165, recall 0.833026
2017-12-10T15:11:51.219448: step 1723, loss 0.448547, acc 0.78125, prec 0.0455951, recall 0.833026
2017-12-10T15:11:51.410120: step 1724, loss 0.394689, acc 0.8125, prec 0.0456087, recall 0.833129
2017-12-10T15:11:51.601541: step 1725, loss 1.51811, acc 0.859375, prec 0.045627, recall 0.833231
2017-12-10T15:11:51.797761: step 1726, loss 0.182136, acc 0.921875, prec 0.0456834, recall 0.833435
2017-12-10T15:11:51.990031: step 1727, loss 0.293344, acc 0.890625, prec 0.0457367, recall 0.833639
2017-12-10T15:11:52.179975: step 1728, loss 0.199036, acc 0.9375, prec 0.0457626, recall 0.833741
2017-12-10T15:11:52.368525: step 1729, loss 0.32986, acc 0.859375, prec 0.0457488, recall 0.833741
2017-12-10T15:11:52.559231: step 1730, loss 0.201707, acc 0.9375, prec 0.0457426, recall 0.833741
2017-12-10T15:11:52.749005: step 1731, loss 0.188255, acc 0.921875, prec 0.045767, recall 0.833842
2017-12-10T15:11:52.943625: step 1732, loss 0.419485, acc 0.90625, prec 0.0457898, recall 0.833944
2017-12-10T15:11:53.136063: step 1733, loss 0.145001, acc 0.96875, prec 0.0458187, recall 0.834045
2017-12-10T15:11:53.328325: step 1734, loss 0.425845, acc 0.90625, prec 0.0458414, recall 0.834146
2017-12-10T15:11:53.520442: step 1735, loss 0.188251, acc 0.890625, prec 0.0458307, recall 0.834146
2017-12-10T15:11:53.711717: step 1736, loss 1.21858, acc 0.921875, prec 0.045855, recall 0.834247
2017-12-10T15:11:53.904713: step 1737, loss 0.182859, acc 0.9375, prec 0.0458488, recall 0.834247
2017-12-10T15:11:54.096692: step 1738, loss 0.131766, acc 0.953125, prec 0.0458762, recall 0.834348
2017-12-10T15:11:54.289511: step 1739, loss 0.506958, acc 0.875, prec 0.0458958, recall 0.834449
2017-12-10T15:11:54.485829: step 1740, loss 2.39844, acc 0.921875, prec 0.0459535, recall 0.834143
2017-12-10T15:11:54.684987: step 1741, loss 0.201849, acc 0.96875, prec 0.0459824, recall 0.834244
2017-12-10T15:11:54.880206: step 1742, loss 0.271973, acc 0.90625, prec 0.0459732, recall 0.834244
2017-12-10T15:11:55.076614: step 1743, loss 0.827352, acc 0.890625, prec 0.0460581, recall 0.834545
2017-12-10T15:11:55.272689: step 1744, loss 0.625428, acc 0.859375, prec 0.0460762, recall 0.834646
2017-12-10T15:11:55.470758: step 1745, loss 0.867935, acc 0.875, prec 0.0460957, recall 0.834746
2017-12-10T15:11:55.666350: step 1746, loss 0.339143, acc 0.875, prec 0.0461153, recall 0.834846
2017-12-10T15:11:55.858424: step 1747, loss 0.557344, acc 0.859375, prec 0.0461014, recall 0.834846
2017-12-10T15:11:56.047885: step 1748, loss 0.353756, acc 0.859375, prec 0.0461513, recall 0.835045
2017-12-10T15:11:56.237841: step 1749, loss 0.566538, acc 0.796875, prec 0.0461631, recall 0.835145
2017-12-10T15:11:56.426461: step 1750, loss 0.552626, acc 0.84375, prec 0.0461795, recall 0.835244
2017-12-10T15:11:56.616911: step 1751, loss 0.398713, acc 0.84375, prec 0.0462277, recall 0.835443
2017-12-10T15:11:56.809170: step 1752, loss 0.535792, acc 0.8125, prec 0.046241, recall 0.835542
2017-12-10T15:11:56.999397: step 1753, loss 0.296869, acc 0.921875, prec 0.0462651, recall 0.835641
2017-12-10T15:11:57.190166: step 1754, loss 0.249023, acc 0.9375, prec 0.0463543, recall 0.835938
2017-12-10T15:11:57.386849: step 1755, loss 0.201878, acc 0.90625, prec 0.046345, recall 0.835938
2017-12-10T15:11:57.583010: step 1756, loss 0.39576, acc 0.8125, prec 0.0463583, recall 0.836036
2017-12-10T15:11:57.771109: step 1757, loss 0.445023, acc 0.90625, prec 0.046349, recall 0.836036
2017-12-10T15:11:57.960956: step 1758, loss 0.441658, acc 0.890625, prec 0.04637, recall 0.836134
2017-12-10T15:11:58.157097: step 1759, loss 1.18303, acc 0.875, prec 0.046548, recall 0.836722
2017-12-10T15:11:58.349622: step 1760, loss 0.241572, acc 0.921875, prec 0.046572, recall 0.83682
2017-12-10T15:11:58.541188: step 1761, loss 0.383772, acc 0.875, prec 0.0465596, recall 0.83682
2017-12-10T15:11:58.730706: step 1762, loss 0.101467, acc 0.96875, prec 0.0465565, recall 0.83682
2017-12-10T15:11:58.927018: step 1763, loss 0.267224, acc 0.921875, prec 0.0466121, recall 0.837015
2017-12-10T15:11:59.124334: step 1764, loss 0.290127, acc 0.859375, prec 0.0465982, recall 0.837015
2017-12-10T15:11:59.325545: step 1765, loss 2.88031, acc 0.890625, prec 0.0466206, recall 0.836613
2017-12-10T15:11:59.518291: step 1766, loss 0.266381, acc 0.921875, prec 0.0466128, recall 0.836613
2017-12-10T15:11:59.714010: step 1767, loss 1.50929, acc 0.90625, prec 0.0466051, recall 0.836114
2017-12-10T15:11:59.908005: step 1768, loss 0.310167, acc 0.875, prec 0.0465927, recall 0.836114
2017-12-10T15:12:00.101484: step 1769, loss 0.319279, acc 0.84375, prec 0.0465773, recall 0.836114
2017-12-10T15:12:00.297819: step 1770, loss 0.361968, acc 0.859375, prec 0.0465633, recall 0.836114
2017-12-10T15:12:00.489637: step 1771, loss 0.303128, acc 0.875, prec 0.0465826, recall 0.836212
2017-12-10T15:12:00.677998: step 1772, loss 1.08471, acc 0.859375, prec 0.046632, recall 0.836407
2017-12-10T15:12:00.867827: step 1773, loss 0.625402, acc 0.796875, prec 0.0466435, recall 0.836504
2017-12-10T15:12:01.059918: step 1774, loss 0.477056, acc 0.875, prec 0.0466311, recall 0.836504
2017-12-10T15:12:01.256622: step 1775, loss 0.681318, acc 0.765625, prec 0.0466395, recall 0.836601
2017-12-10T15:12:01.453235: step 1776, loss 0.837964, acc 0.75, prec 0.0466464, recall 0.836698
2017-12-10T15:12:01.647460: step 1777, loss 0.581627, acc 0.75, prec 0.0466848, recall 0.836892
2017-12-10T15:12:01.847534: step 1778, loss 1.44963, acc 0.8125, prec 0.0466978, recall 0.836989
2017-12-10T15:12:02.041671: step 1779, loss 0.571491, acc 0.796875, prec 0.0466777, recall 0.836989
2017-12-10T15:12:02.234846: step 1780, loss 0.907695, acc 0.734375, prec 0.046683, recall 0.837085
2017-12-10T15:12:02.428934: step 1781, loss 0.457042, acc 0.828125, prec 0.046666, recall 0.837085
2017-12-10T15:12:02.621852: step 1782, loss 0.593361, acc 0.78125, prec 0.0466444, recall 0.837085
2017-12-10T15:12:02.815194: step 1783, loss 0.47962, acc 0.859375, prec 0.046662, recall 0.837182
2017-12-10T15:12:03.005224: step 1784, loss 0.87395, acc 0.75, prec 0.0467003, recall 0.837374
2017-12-10T15:12:03.197603: step 1785, loss 0.581813, acc 0.796875, prec 0.0466803, recall 0.837374
2017-12-10T15:12:03.391450: step 1786, loss 0.384275, acc 0.828125, prec 0.0466948, recall 0.83747
2017-12-10T15:12:03.580185: step 1787, loss 0.38831, acc 0.859375, prec 0.0467438, recall 0.837662
2017-12-10T15:12:03.779293: step 1788, loss 0.266757, acc 0.90625, prec 0.0467973, recall 0.837854
2017-12-10T15:12:03.971380: step 1789, loss 0.258782, acc 0.9375, prec 0.0468225, recall 0.837949
2017-12-10T15:12:04.166759: step 1790, loss 0.233245, acc 0.921875, prec 0.0468462, recall 0.838045
2017-12-10T15:12:04.357505: step 1791, loss 0.338011, acc 0.90625, prec 0.0468369, recall 0.838045
2017-12-10T15:12:04.550536: step 1792, loss 5.67397, acc 0.875, prec 0.0468591, recall 0.837155
2017-12-10T15:12:04.746417: step 1793, loss 0.341271, acc 0.875, prec 0.0468467, recall 0.837155
2017-12-10T15:12:04.941041: step 1794, loss 0.386627, acc 0.890625, prec 0.0468359, recall 0.837155
2017-12-10T15:12:05.132090: step 1795, loss 0.283957, acc 0.890625, prec 0.0468252, recall 0.837155
2017-12-10T15:12:05.333764: step 1796, loss 1.14912, acc 0.8125, prec 0.0469007, recall 0.837441
2017-12-10T15:12:05.529345: step 1797, loss 0.752551, acc 0.890625, prec 0.0469838, recall 0.837727
2017-12-10T15:12:05.724637: step 1798, loss 0.357283, acc 0.890625, prec 0.046973, recall 0.837727
2017-12-10T15:12:05.915598: step 1799, loss 0.836309, acc 0.859375, prec 0.0470217, recall 0.837917
2017-12-10T15:12:06.107832: step 1800, loss 0.545648, acc 0.859375, prec 0.0471017, recall 0.838201
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_1/1512936356/checkpoints/model-1800

2017-12-10T15:12:07.359659: step 1801, loss 0.687869, acc 0.78125, prec 0.0471113, recall 0.838295
2017-12-10T15:12:07.548295: step 1802, loss 0.494892, acc 0.78125, prec 0.0471209, recall 0.83839
2017-12-10T15:12:07.745152: step 1803, loss 0.486448, acc 0.84375, prec 0.0471055, recall 0.83839
2017-12-10T15:12:07.937985: step 1804, loss 0.689973, acc 0.734375, prec 0.0471105, recall 0.838484
2017-12-10T15:12:08.131999: step 1805, loss 0.498735, acc 0.84375, prec 0.047095, recall 0.838484
2017-12-10T15:12:08.324622: step 1806, loss 0.760359, acc 0.78125, prec 0.0470735, recall 0.838484
2017-12-10T15:12:08.516984: step 1807, loss 0.489419, acc 0.765625, prec 0.0470815, recall 0.838578
2017-12-10T15:12:08.717101: step 1808, loss 0.613524, acc 0.84375, prec 0.0471285, recall 0.838766
2017-12-10T15:12:08.907023: step 1809, loss 0.550647, acc 0.796875, prec 0.0471084, recall 0.838766
2017-12-10T15:12:09.102104: step 1810, loss 1.61761, acc 0.8125, prec 0.0470915, recall 0.838278
2017-12-10T15:12:09.294656: step 1811, loss 0.482006, acc 0.8125, prec 0.047073, recall 0.838278
2017-12-10T15:12:09.492593: step 1812, loss 1.30499, acc 0.875, prec 0.047123, recall 0.838466
2017-12-10T15:12:09.688569: step 1813, loss 0.608166, acc 0.84375, prec 0.0471387, recall 0.83856
2017-12-10T15:12:09.884167: step 1814, loss 0.512285, acc 0.875, prec 0.0471886, recall 0.838747
2017-12-10T15:12:10.084816: step 1815, loss 0.208537, acc 0.9375, prec 0.0472135, recall 0.838841
2017-12-10T15:12:10.281208: step 1816, loss 0.478662, acc 0.921875, prec 0.0473301, recall 0.839213
2017-12-10T15:12:10.475174: step 1817, loss 0.449309, acc 0.875, prec 0.0473799, recall 0.839399
2017-12-10T15:12:10.669764: step 1818, loss 0.328335, acc 0.875, prec 0.0474297, recall 0.839585
2017-12-10T15:12:10.861943: step 1819, loss 0.612275, acc 0.765625, prec 0.0474065, recall 0.839585
2017-12-10T15:12:11.055788: step 1820, loss 0.535606, acc 0.78125, prec 0.0474159, recall 0.839677
2017-12-10T15:12:11.245586: step 1821, loss 0.60717, acc 0.8125, prec 0.0474284, recall 0.839769
2017-12-10T15:12:11.440261: step 1822, loss 0.343927, acc 0.90625, prec 0.0474501, recall 0.839862
2017-12-10T15:12:11.633421: step 1823, loss 1.48033, acc 0.90625, prec 0.0474734, recall 0.839471
2017-12-10T15:12:11.832006: step 1824, loss 0.345172, acc 0.84375, prec 0.047458, recall 0.839471
2017-12-10T15:12:12.026331: step 1825, loss 0.216849, acc 0.921875, prec 0.0474812, recall 0.839563
2017-12-10T15:12:12.220693: step 1826, loss 0.350986, acc 0.84375, prec 0.0474658, recall 0.839563
2017-12-10T15:12:12.412386: step 1827, loss 0.179083, acc 0.921875, prec 0.047489, recall 0.839655
2017-12-10T15:12:12.607582: step 1828, loss 0.219007, acc 0.90625, prec 0.0474798, recall 0.839655
2017-12-10T15:12:12.800109: step 1829, loss 0.286884, acc 0.90625, prec 0.0475015, recall 0.839747
2017-12-10T15:12:12.991728: step 1830, loss 0.476248, acc 0.90625, prec 0.047585, recall 0.840023
2017-12-10T15:12:13.181896: step 1831, loss 0.304439, acc 0.859375, prec 0.047602, recall 0.840115
2017-12-10T15:12:13.377411: step 1832, loss 0.507566, acc 0.84375, prec 0.0476793, recall 0.840389
2017-12-10T15:12:13.573668: step 1833, loss 0.21069, acc 0.921875, prec 0.0476716, recall 0.840389
2017-12-10T15:12:13.767522: step 1834, loss 1.34456, acc 0.90625, prec 0.0476948, recall 0.84
2017-12-10T15:12:13.960410: step 1835, loss 0.257665, acc 0.90625, prec 0.0476855, recall 0.84
2017-12-10T15:12:14.161100: step 1836, loss 0.142145, acc 0.953125, prec 0.0476808, recall 0.84
2017-12-10T15:12:14.359960: step 1837, loss 1.09146, acc 0.9375, prec 0.0477364, recall 0.840183
2017-12-10T15:12:14.554929: step 1838, loss 0.460281, acc 0.828125, prec 0.0477194, recall 0.840183
2017-12-10T15:12:14.746577: step 1839, loss 0.349573, acc 0.859375, prec 0.0477055, recall 0.840183
2017-12-10T15:12:14.935586: step 1840, loss 0.334844, acc 0.890625, prec 0.0476946, recall 0.840183
2017-12-10T15:12:15.130876: step 1841, loss 0.275874, acc 0.921875, prec 0.0477178, recall 0.840274
2017-12-10T15:12:15.322086: step 1842, loss 1.20623, acc 0.921875, prec 0.0477409, recall 0.840365
2017-12-10T15:12:15.518919: step 1843, loss 0.251669, acc 0.96875, prec 0.0477686, recall 0.840456
2017-12-10T15:12:15.717777: step 1844, loss 0.346658, acc 0.890625, prec 0.0477578, recall 0.840456
2017-12-10T15:12:15.908762: step 1845, loss 0.393403, acc 0.8125, prec 0.0477393, recall 0.840456
2017-12-10T15:12:16.099239: step 1846, loss 0.46848, acc 0.84375, prec 0.0477546, recall 0.840547
2017-12-10T15:12:16.290208: step 1847, loss 0.312398, acc 0.890625, prec 0.0477438, recall 0.840547
2017-12-10T15:12:16.483312: step 1848, loss 2.80588, acc 0.84375, prec 0.0477299, recall 0.840068
2017-12-10T15:12:16.678372: step 1849, loss 0.406988, acc 0.90625, prec 0.0477207, recall 0.840068
2017-12-10T15:12:16.873148: step 1850, loss 1.61528, acc 0.84375, prec 0.0477668, recall 0.84025
2017-12-10T15:12:17.069431: step 1851, loss 0.473891, acc 0.8125, prec 0.0477483, recall 0.84025
2017-12-10T15:12:17.260345: step 1852, loss 0.631644, acc 0.828125, prec 0.0478236, recall 0.840522
2017-12-10T15:12:17.456557: step 1853, loss 0.772666, acc 0.78125, prec 0.0478327, recall 0.840613
2017-12-10T15:12:17.656770: step 1854, loss 0.669882, acc 0.796875, prec 0.0478433, recall 0.840703
2017-12-10T15:12:17.849530: step 1855, loss 0.616178, acc 0.828125, prec 0.0478878, recall 0.840883
2017-12-10T15:12:18.041943: step 1856, loss 0.614609, acc 0.796875, prec 0.0478677, recall 0.840883
2017-12-10T15:12:18.232959: step 1857, loss 0.561462, acc 0.84375, prec 0.047883, recall 0.840973
2017-12-10T15:12:18.428655: step 1858, loss 0.386659, acc 0.828125, prec 0.0478967, recall 0.841063
2017-12-10T15:12:18.623482: step 1859, loss 0.695592, acc 0.84375, prec 0.0480039, recall 0.841422
2017-12-10T15:12:18.820976: step 1860, loss 0.276391, acc 0.890625, prec 0.047993, recall 0.841422
2017-12-10T15:12:19.014813: step 1861, loss 0.284705, acc 0.90625, prec 0.0480144, recall 0.841512
2017-12-10T15:12:19.211636: step 1862, loss 0.20846, acc 0.90625, prec 0.048097, recall 0.841779
2017-12-10T15:12:19.405524: step 1863, loss 0.256029, acc 0.875, prec 0.0480847, recall 0.841779
2017-12-10T15:12:19.597598: step 1864, loss 0.37021, acc 0.859375, prec 0.0481013, recall 0.841868
2017-12-10T15:12:19.790238: step 1865, loss 0.325313, acc 0.875, prec 0.048089, recall 0.841868
2017-12-10T15:12:19.981673: step 1866, loss 0.402496, acc 0.859375, prec 0.0481057, recall 0.841957
2017-12-10T15:12:20.175036: step 1867, loss 1.60728, acc 0.90625, prec 0.0480979, recall 0.841484
2017-12-10T15:12:20.371236: step 1868, loss 0.484153, acc 0.890625, prec 0.0481177, recall 0.841573
2017-12-10T15:12:20.563776: step 1869, loss 0.335314, acc 0.890625, prec 0.0481069, recall 0.841573
2017-12-10T15:12:20.756860: step 1870, loss 0.437141, acc 0.875, prec 0.0480945, recall 0.841573
2017-12-10T15:12:20.949076: step 1871, loss 0.392056, acc 0.859375, prec 0.0481112, recall 0.841662
2017-12-10T15:12:21.140689: step 1872, loss 0.358786, acc 0.9375, prec 0.048105, recall 0.841662
2017-12-10T15:12:21.336824: step 1873, loss 0.857252, acc 0.921875, prec 0.0481584, recall 0.84184
2017-12-10T15:12:21.536223: step 1874, loss 0.198361, acc 0.9375, prec 0.0481522, recall 0.84184
2017-12-10T15:12:21.731156: step 1875, loss 0.210309, acc 0.953125, prec 0.0481781, recall 0.841928
2017-12-10T15:12:21.922371: step 1876, loss 0.301989, acc 0.890625, prec 0.0481978, recall 0.842017
2017-12-10T15:12:22.112195: step 1877, loss 0.376085, acc 0.875, prec 0.0481854, recall 0.842017
2017-12-10T15:12:22.302892: step 1878, loss 0.230993, acc 0.90625, prec 0.0481762, recall 0.842017
2017-12-10T15:12:22.497167: step 1879, loss 2.55829, acc 0.890625, prec 0.0482279, recall 0.841723
2017-12-10T15:12:22.690374: step 1880, loss 0.4377, acc 0.875, prec 0.0482155, recall 0.841723
2017-12-10T15:12:22.880310: step 1881, loss 0.260036, acc 0.921875, prec 0.0482383, recall 0.841811
2017-12-10T15:12:23.068762: step 1882, loss 0.262677, acc 0.859375, prec 0.0482244, recall 0.841811
2017-12-10T15:12:23.260447: step 1883, loss 0.348495, acc 0.890625, prec 0.0482136, recall 0.841811
2017-12-10T15:12:23.451431: step 1884, loss 0.344572, acc 0.921875, prec 0.0482059, recall 0.841811
2017-12-10T15:12:23.641280: step 1885, loss 0.478664, acc 0.890625, prec 0.0482255, recall 0.841899
2017-12-10T15:12:23.831655: step 1886, loss 0.440869, acc 0.875, prec 0.0483045, recall 0.842164
2017-12-10T15:12:24.024455: step 1887, loss 0.948574, acc 0.671875, prec 0.0483025, recall 0.842252
2017-12-10T15:12:24.215116: step 1888, loss 0.634533, acc 0.78125, prec 0.0482809, recall 0.842252
2017-12-10T15:12:24.408961: step 1889, loss 0.565938, acc 0.765625, prec 0.0482578, recall 0.842252
2017-12-10T15:12:24.598306: step 1890, loss 0.221089, acc 0.90625, prec 0.0482486, recall 0.842252
2017-12-10T15:12:24.792211: step 1891, loss 0.407241, acc 0.8125, prec 0.0482605, recall 0.84234
2017-12-10T15:12:24.983522: step 1892, loss 0.466939, acc 0.859375, prec 0.048277, recall 0.842428
2017-12-10T15:12:25.173643: step 1893, loss 0.433894, acc 0.859375, prec 0.0482935, recall 0.842515
2017-12-10T15:12:25.369584: step 1894, loss 0.175705, acc 0.921875, prec 0.0482858, recall 0.842515
2017-12-10T15:12:25.565821: step 1895, loss 0.114944, acc 0.953125, prec 0.0482811, recall 0.842515
2017-12-10T15:12:25.757134: step 1896, loss 0.357673, acc 0.953125, prec 0.0483372, recall 0.84269
2017-12-10T15:12:25.948138: step 1897, loss 0.259073, acc 0.953125, prec 0.0483326, recall 0.84269
2017-12-10T15:12:26.141061: step 1898, loss 0.165953, acc 0.953125, prec 0.0483583, recall 0.842778
2017-12-10T15:12:26.333584: step 1899, loss 0.204895, acc 0.921875, prec 0.0483506, recall 0.842778
2017-12-10T15:12:26.523547: step 1900, loss 0.291429, acc 0.90625, prec 0.0483414, recall 0.842778
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_1/1512936356/checkpoints/model-1900

2017-12-10T15:12:27.722024: step 1901, loss 0.13791, acc 0.9375, prec 0.0483352, recall 0.842778
2017-12-10T15:12:27.914077: step 1902, loss 1.49089, acc 0.875, prec 0.0483835, recall 0.842952
2017-12-10T15:12:28.104838: step 1903, loss 0.0787295, acc 0.984375, prec 0.0484123, recall 0.843039
2017-12-10T15:12:28.298157: step 1904, loss 2.29178, acc 0.953125, prec 0.0484092, recall 0.842572
2017-12-10T15:12:28.492536: step 1905, loss 0.0643782, acc 0.984375, prec 0.0484076, recall 0.842572
2017-12-10T15:12:28.681616: step 1906, loss 1.8427, acc 0.921875, prec 0.0484015, recall 0.842105
2017-12-10T15:12:28.874312: step 1907, loss 0.410595, acc 0.890625, prec 0.0483907, recall 0.842105
2017-12-10T15:12:29.065326: step 1908, loss 0.282755, acc 0.890625, prec 0.0483799, recall 0.842105
2017-12-10T15:12:29.261984: step 1909, loss 0.332784, acc 0.84375, prec 0.0483645, recall 0.842105
2017-12-10T15:12:29.453406: step 1910, loss 0.496731, acc 0.8125, prec 0.0483763, recall 0.842193
2017-12-10T15:12:29.646305: step 1911, loss 0.588662, acc 0.859375, prec 0.0483927, recall 0.84228
2017-12-10T15:12:29.836371: step 1912, loss 0.164566, acc 0.9375, prec 0.0484168, recall 0.842367
2017-12-10T15:12:30.027334: step 1913, loss 0.499409, acc 0.84375, prec 0.0484014, recall 0.842367
2017-12-10T15:12:30.221289: step 1914, loss 0.480599, acc 0.828125, prec 0.0483845, recall 0.842367
2017-12-10T15:12:30.413695: step 1915, loss 0.43326, acc 0.8125, prec 0.0483661, recall 0.842367
2017-12-10T15:12:30.605068: step 1916, loss 0.408507, acc 0.9375, prec 0.0483902, recall 0.842454
2017-12-10T15:12:30.795678: step 1917, loss 0.342932, acc 0.84375, prec 0.0483748, recall 0.842454
2017-12-10T15:12:30.985902: step 1918, loss 0.309019, acc 0.84375, prec 0.0483595, recall 0.842454
2017-12-10T15:12:31.184460: step 1919, loss 0.793148, acc 0.859375, prec 0.0483758, recall 0.842541
2017-12-10T15:12:31.385076: step 1920, loss 0.24695, acc 0.890625, prec 0.0483953, recall 0.842628
2017-12-10T15:12:31.584981: step 1921, loss 0.128077, acc 0.96875, prec 0.0483922, recall 0.842628
2017-12-10T15:12:31.775151: step 1922, loss 0.432061, acc 0.859375, prec 0.0483784, recall 0.842628
2017-12-10T15:12:31.966400: step 1923, loss 0.399873, acc 0.890625, prec 0.048428, recall 0.842802
2017-12-10T15:12:32.157412: step 1924, loss 0.357602, acc 0.875, prec 0.0484459, recall 0.842889
2017-12-10T15:12:32.350028: step 1925, loss 1.68946, acc 0.90625, prec 0.0484683, recall 0.842511
2017-12-10T15:12:32.542474: step 1926, loss 0.333024, acc 0.875, prec 0.0484862, recall 0.842598
2017-12-10T15:12:32.734084: step 1927, loss 0.21556, acc 0.921875, prec 0.0485086, recall 0.842684
2017-12-10T15:12:32.929939: step 1928, loss 0.288095, acc 0.921875, prec 0.048501, recall 0.842684
2017-12-10T15:12:33.125862: step 1929, loss 0.155017, acc 0.9375, prec 0.0484948, recall 0.842684
2017-12-10T15:12:33.321379: step 1930, loss 0.185508, acc 0.921875, prec 0.0485474, recall 0.842857
2017-12-10T15:12:33.511462: step 1931, loss 0.268864, acc 0.890625, prec 0.0485366, recall 0.842857
2017-12-10T15:12:33.702398: step 1932, loss 0.151383, acc 0.921875, prec 0.0485289, recall 0.842857
2017-12-10T15:12:33.899351: step 1933, loss 0.275207, acc 0.875, prec 0.0485468, recall 0.842943
2017-12-10T15:12:34.090695: step 1934, loss 0.202689, acc 0.921875, prec 0.0485391, recall 0.842943
2017-12-10T15:12:34.288143: step 1935, loss 0.379003, acc 0.90625, prec 0.04856, recall 0.84303
2017-12-10T15:12:34.481500: step 1936, loss 1.56093, acc 0.859375, prec 0.0485477, recall 0.842567
2017-12-10T15:12:34.680491: step 1937, loss 0.224095, acc 0.875, prec 0.0485655, recall 0.842654
2017-12-10T15:12:34.878026: step 1938, loss 0.280401, acc 0.890625, prec 0.0485848, recall 0.84274
2017-12-10T15:12:35.069648: step 1939, loss 0.235055, acc 0.90625, prec 0.0485756, recall 0.84274
2017-12-10T15:12:35.260372: step 1940, loss 0.751582, acc 0.859375, prec 0.0486219, recall 0.842912
2017-12-10T15:12:35.455235: step 1941, loss 0.453714, acc 0.9375, prec 0.0487058, recall 0.843169
2017-12-10T15:12:35.652919: step 1942, loss 0.224302, acc 0.953125, prec 0.0487012, recall 0.843169
2017-12-10T15:12:35.842132: step 1943, loss 6.20952, acc 0.84375, prec 0.0486874, recall 0.842709
2017-12-10T15:12:36.035284: step 1944, loss 0.501055, acc 0.90625, prec 0.0486781, recall 0.842709
2017-12-10T15:12:36.230421: step 1945, loss 0.431752, acc 0.84375, prec 0.0487228, recall 0.842881
2017-12-10T15:12:36.424489: step 1946, loss 0.425602, acc 0.890625, prec 0.048712, recall 0.842881
2017-12-10T15:12:36.621313: step 1947, loss 0.380194, acc 0.828125, prec 0.0486952, recall 0.842881
2017-12-10T15:12:36.813112: step 1948, loss 0.214214, acc 0.890625, prec 0.0486844, recall 0.842881
2017-12-10T15:12:37.004323: step 1949, loss 0.248466, acc 0.890625, prec 0.0486737, recall 0.842881
2017-12-10T15:12:37.195865: step 1950, loss 0.543619, acc 0.890625, prec 0.0487229, recall 0.843052
2017-12-10T15:12:37.388951: step 1951, loss 0.765193, acc 0.90625, prec 0.0487436, recall 0.843137
2017-12-10T15:12:37.581308: step 1952, loss 0.242177, acc 0.9375, prec 0.0487375, recall 0.843137
2017-12-10T15:12:37.776145: step 1953, loss 0.695586, acc 0.78125, prec 0.0487459, recall 0.843223
2017-12-10T15:12:37.974250: step 1954, loss 0.288803, acc 0.875, prec 0.0487337, recall 0.843223
2017-12-10T15:12:38.168020: step 1955, loss 0.855841, acc 0.890625, prec 0.0487529, recall 0.843308
2017-12-10T15:12:38.360026: step 1956, loss 0.590193, acc 0.84375, prec 0.0487375, recall 0.843308
2017-12-10T15:12:38.553088: step 1957, loss 0.52171, acc 0.8125, prec 0.0487192, recall 0.843308
2017-12-10T15:12:38.745608: step 1958, loss 0.492654, acc 0.875, prec 0.0487667, recall 0.843478
2017-12-10T15:12:38.942324: step 1959, loss 0.351159, acc 0.859375, prec 0.0488127, recall 0.843648
2017-12-10T15:12:39.139088: step 1960, loss 0.271731, acc 0.890625, prec 0.0488318, recall 0.843733
2017-12-10T15:12:39.333024: step 1961, loss 0.827041, acc 0.84375, prec 0.0488762, recall 0.843902
2017-12-10T15:12:39.530295: step 1962, loss 0.246596, acc 0.890625, prec 0.0488953, recall 0.843987
2017-12-10T15:12:39.721932: step 1963, loss 0.189174, acc 0.90625, prec 0.0488861, recall 0.843987
2017-12-10T15:12:39.913989: step 1964, loss 0.205638, acc 0.9375, prec 0.0489098, recall 0.844071
2017-12-10T15:12:40.109726: step 1965, loss 0.405105, acc 0.859375, prec 0.0489557, recall 0.84424
2017-12-10T15:12:40.305703: step 1966, loss 0.254599, acc 0.875, prec 0.0489434, recall 0.84424
2017-12-10T15:12:40.499780: step 1967, loss 0.177114, acc 0.953125, prec 0.0489388, recall 0.84424
2017-12-10T15:12:40.697234: step 1968, loss 1.06012, acc 0.96875, prec 0.0490549, recall 0.844576
2017-12-10T15:12:40.893844: step 1969, loss 0.606374, acc 0.9375, prec 0.0490786, recall 0.84466
2017-12-10T15:12:41.092245: step 1970, loss 0.425231, acc 0.890625, prec 0.0490678, recall 0.84466
2017-12-10T15:12:41.282689: step 1971, loss 0.111742, acc 0.96875, prec 0.0490648, recall 0.84466
2017-12-10T15:12:41.476098: step 1972, loss 0.163962, acc 0.9375, prec 0.0490586, recall 0.84466
2017-12-10T15:12:41.668678: step 1973, loss 0.195992, acc 0.9375, prec 0.0490525, recall 0.84466
2017-12-10T15:12:41.860899: step 1974, loss 0.106477, acc 0.953125, prec 0.0490479, recall 0.84466
2017-12-10T15:12:42.058999: step 1975, loss 0.42429, acc 0.9375, prec 0.049131, recall 0.844911
2017-12-10T15:12:42.252798: step 1976, loss 0.230193, acc 0.90625, prec 0.0491218, recall 0.844911
2017-12-10T15:12:42.444393: step 1977, loss 0.278187, acc 0.90625, prec 0.0491126, recall 0.844911
2017-12-10T15:12:42.639599: step 1978, loss 0.16718, acc 0.890625, prec 0.0491018, recall 0.844911
2017-12-10T15:12:42.834022: step 1979, loss 0.248194, acc 1, prec 0.0491613, recall 0.845078
2017-12-10T15:12:43.026288: step 1980, loss 0.443205, acc 0.90625, prec 0.0492116, recall 0.845244
2017-12-10T15:12:43.225447: step 1981, loss 0.40027, acc 0.890625, prec 0.0492306, recall 0.845328
2017-12-10T15:12:43.418630: step 1982, loss 0.0899817, acc 0.96875, prec 0.0492275, recall 0.845328
2017-12-10T15:12:43.610022: step 1983, loss 0.46686, acc 0.953125, prec 0.0493121, recall 0.845576
2017-12-10T15:12:43.801843: step 1984, loss 0.172139, acc 0.90625, prec 0.0493028, recall 0.845576
2017-12-10T15:12:43.997036: step 1985, loss 0.257256, acc 0.96875, prec 0.0493592, recall 0.845742
2017-12-10T15:12:44.196386: step 1986, loss 0.252825, acc 0.875, prec 0.0493765, recall 0.845824
2017-12-10T15:12:44.391676: step 1987, loss 0.130648, acc 0.96875, prec 0.0493735, recall 0.845824
2017-12-10T15:12:44.565773: step 1988, loss 0.216401, acc 0.862745, prec 0.0493627, recall 0.845824
2017-12-10T15:12:44.768956: step 1989, loss 0.622117, acc 0.875, prec 0.0494097, recall 0.845989
2017-12-10T15:12:44.958884: step 1990, loss 0.486293, acc 0.90625, prec 0.0494598, recall 0.846154
2017-12-10T15:12:45.150392: step 1991, loss 0.265681, acc 0.921875, prec 0.0494818, recall 0.846236
2017-12-10T15:12:45.344471: step 1992, loss 0.139107, acc 0.953125, prec 0.0494771, recall 0.846236
2017-12-10T15:12:45.536785: step 1993, loss 0.205353, acc 0.90625, prec 0.0494679, recall 0.846236
2017-12-10T15:12:45.735098: step 1994, loss 0.394293, acc 0.953125, prec 0.0495226, recall 0.8464
2017-12-10T15:12:45.931695: step 1995, loss 0.122434, acc 1, prec 0.0495522, recall 0.846482
2017-12-10T15:12:46.129984: step 1996, loss 0.505254, acc 0.875, prec 0.0495695, recall 0.846564
2017-12-10T15:12:46.325280: step 1997, loss 0.115127, acc 0.953125, prec 0.0495649, recall 0.846564
2017-12-10T15:12:46.519880: step 1998, loss 0.192817, acc 0.96875, prec 0.0495914, recall 0.846645
2017-12-10T15:12:46.715882: step 1999, loss 0.36279, acc 0.890625, prec 0.0496399, recall 0.846808
2017-12-10T15:12:46.908814: step 2000, loss 0.0997715, acc 1, prec 0.0496991, recall 0.846971
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_1/1512936356/checkpoints/model-2000

2017-12-10T15:12:48.084350: step 2001, loss 1.1945, acc 0.96875, prec 0.0497553, recall 0.847134
2017-12-10T15:12:48.276389: step 2002, loss 1.92631, acc 0.953125, prec 0.0497818, recall 0.846766
2017-12-10T15:12:48.476947: step 2003, loss 0.251717, acc 0.96875, prec 0.0498083, recall 0.846847
2017-12-10T15:12:48.669725: step 2004, loss 0.229981, acc 0.921875, prec 0.0498302, recall 0.846928
2017-12-10T15:12:48.858049: step 2005, loss 0.0890873, acc 0.953125, prec 0.0498551, recall 0.847009
2017-12-10T15:12:49.051872: step 2006, loss 2.12552, acc 0.875, prec 0.0499034, recall 0.846723
2017-12-10T15:12:49.245182: step 2007, loss 0.349844, acc 0.90625, prec 0.0498941, recall 0.846723
2017-12-10T15:12:49.437332: step 2008, loss 0.292917, acc 0.890625, prec 0.0498832, recall 0.846723
2017-12-10T15:12:49.629105: step 2009, loss 0.568575, acc 0.859375, prec 0.0498988, recall 0.846804
2017-12-10T15:12:49.821648: step 2010, loss 0.355102, acc 0.859375, prec 0.0498849, recall 0.846804
2017-12-10T15:12:50.014360: step 2011, loss 0.344101, acc 0.921875, prec 0.0498771, recall 0.846804
2017-12-10T15:12:50.206243: step 2012, loss 0.756583, acc 0.75, prec 0.0498523, recall 0.846804
2017-12-10T15:12:50.393605: step 2013, loss 0.82622, acc 0.765625, prec 0.049829, recall 0.846804
2017-12-10T15:12:50.584101: step 2014, loss 0.379852, acc 0.84375, prec 0.0498135, recall 0.846804
2017-12-10T15:12:50.776585: step 2015, loss 0.402153, acc 0.8125, prec 0.049795, recall 0.846804
2017-12-10T15:12:50.968024: step 2016, loss 0.729062, acc 0.78125, prec 0.0497733, recall 0.846804
2017-12-10T15:12:51.159532: step 2017, loss 0.472096, acc 0.84375, prec 0.0498169, recall 0.846966
2017-12-10T15:12:51.357075: step 2018, loss 0.37993, acc 0.8125, prec 0.0497983, recall 0.846966
2017-12-10T15:12:51.547078: step 2019, loss 0.341513, acc 0.890625, prec 0.049817, recall 0.847046
2017-12-10T15:12:51.743895: step 2020, loss 0.417488, acc 0.84375, prec 0.0498015, recall 0.847046
2017-12-10T15:12:51.934208: step 2021, loss 0.247208, acc 0.875, prec 0.0497892, recall 0.847046
2017-12-10T15:12:52.125404: step 2022, loss 0.516486, acc 0.796875, prec 0.049828, recall 0.847208
2017-12-10T15:12:52.321428: step 2023, loss 0.271321, acc 0.90625, prec 0.0498188, recall 0.847208
2017-12-10T15:12:52.512974: step 2024, loss 0.514122, acc 0.78125, prec 0.0498266, recall 0.847288
2017-12-10T15:12:52.711794: step 2025, loss 0.341012, acc 0.90625, prec 0.0498173, recall 0.847288
2017-12-10T15:12:52.899770: step 2026, loss 0.122033, acc 0.953125, prec 0.0498127, recall 0.847288
2017-12-10T15:12:53.092966: step 2027, loss 0.169882, acc 0.96875, prec 0.0498096, recall 0.847288
2017-12-10T15:12:53.287765: step 2028, loss 0.222689, acc 0.90625, prec 0.0498592, recall 0.847449
2017-12-10T15:12:53.479386: step 2029, loss 0.17151, acc 0.953125, prec 0.0498546, recall 0.847449
2017-12-10T15:12:53.672514: step 2030, loss 0.137392, acc 0.921875, prec 0.0498762, recall 0.847529
2017-12-10T15:12:53.865208: step 2031, loss 0.206539, acc 0.9375, prec 0.0498995, recall 0.847609
2017-12-10T15:12:54.055676: step 2032, loss 5.63577, acc 0.96875, prec 0.0499273, recall 0.847244
2017-12-10T15:12:54.254215: step 2033, loss 0.210649, acc 0.921875, prec 0.0500077, recall 0.847484
2017-12-10T15:12:54.452883: step 2034, loss 0.151841, acc 0.953125, prec 0.0500031, recall 0.847484
2017-12-10T15:12:54.650360: step 2035, loss 0.181188, acc 0.96875, prec 0.0500294, recall 0.847564
2017-12-10T15:12:54.844978: step 2036, loss 0.0890225, acc 0.96875, prec 0.050085, recall 0.847724
2017-12-10T15:12:55.036504: step 2037, loss 0.948231, acc 0.921875, prec 0.0501654, recall 0.847962
2017-12-10T15:12:55.233963: step 2038, loss 1.44687, acc 0.953125, prec 0.0502488, recall 0.8482
2017-12-10T15:12:55.425712: step 2039, loss 0.156244, acc 0.953125, prec 0.0502735, recall 0.848279
2017-12-10T15:12:55.617007: step 2040, loss 0.435047, acc 0.921875, prec 0.0502657, recall 0.848279
2017-12-10T15:12:55.805680: step 2041, loss 0.162197, acc 0.96875, prec 0.0502919, recall 0.848359
2017-12-10T15:12:56.000614: step 2042, loss 0.321391, acc 0.875, prec 0.0502795, recall 0.848359
2017-12-10T15:12:56.193721: step 2043, loss 0.476924, acc 0.875, prec 0.0503257, recall 0.848516
2017-12-10T15:12:56.388689: step 2044, loss 0.255699, acc 0.890625, prec 0.0503149, recall 0.848516
2017-12-10T15:12:56.586100: step 2045, loss 0.533889, acc 0.8125, prec 0.0502962, recall 0.848516
2017-12-10T15:12:56.776168: step 2046, loss 0.335257, acc 0.890625, prec 0.0502854, recall 0.848516
2017-12-10T15:12:56.970187: step 2047, loss 0.243405, acc 0.90625, prec 0.0503053, recall 0.848595
2017-12-10T15:12:57.159477: step 2048, loss 0.544508, acc 0.828125, prec 0.0502883, recall 0.848595
2017-12-10T15:12:57.354620: step 2049, loss 0.534065, acc 0.859375, prec 0.0502743, recall 0.848595
2017-12-10T15:12:57.548531: step 2050, loss 0.450803, acc 0.859375, prec 0.0502897, recall 0.848674
2017-12-10T15:12:57.741951: step 2051, loss 0.417276, acc 0.859375, prec 0.050305, recall 0.848753
2017-12-10T15:12:57.934831: step 2052, loss 0.450836, acc 0.859375, prec 0.050291, recall 0.848753
2017-12-10T15:12:58.123673: step 2053, loss 0.421141, acc 0.875, prec 0.0502786, recall 0.848753
2017-12-10T15:12:58.318382: step 2054, loss 2.11768, acc 0.859375, prec 0.0502663, recall 0.848312
2017-12-10T15:12:58.509569: step 2055, loss 0.481727, acc 0.765625, prec 0.0503015, recall 0.848469
2017-12-10T15:12:58.699574: step 2056, loss 0.403017, acc 0.890625, prec 0.0503199, recall 0.848548
2017-12-10T15:12:58.890628: step 2057, loss 0.353501, acc 0.90625, prec 0.0503106, recall 0.848548
2017-12-10T15:12:59.083806: step 2058, loss 0.268827, acc 0.921875, prec 0.0503321, recall 0.848626
2017-12-10T15:12:59.286013: step 2059, loss 0.488394, acc 0.890625, prec 0.0503796, recall 0.848783
2017-12-10T15:12:59.477080: step 2060, loss 0.348205, acc 0.90625, prec 0.0503995, recall 0.848861
2017-12-10T15:12:59.674278: step 2061, loss 0.408198, acc 0.921875, prec 0.0505085, recall 0.849174
2017-12-10T15:12:59.868430: step 2062, loss 0.218999, acc 0.953125, prec 0.0505038, recall 0.849174
2017-12-10T15:13:00.059556: step 2063, loss 0.157717, acc 0.96875, prec 0.0505299, recall 0.849251
2017-12-10T15:13:00.250047: step 2064, loss 0.265251, acc 0.9375, prec 0.0505237, recall 0.849251
2017-12-10T15:13:00.452305: step 2065, loss 1.06317, acc 0.921875, prec 0.0505742, recall 0.849407
2017-12-10T15:13:00.649544: step 2066, loss 0.247437, acc 0.875, prec 0.0505618, recall 0.849407
2017-12-10T15:13:00.842641: step 2067, loss 0.271746, acc 0.9375, prec 0.0505847, recall 0.849485
2017-12-10T15:13:01.036589: step 2068, loss 0.350288, acc 0.921875, prec 0.050577, recall 0.849485
2017-12-10T15:13:01.231433: step 2069, loss 0.065982, acc 0.984375, prec 0.0505754, recall 0.849485
2017-12-10T15:13:01.430654: step 2070, loss 0.25804, acc 0.96875, prec 0.0506014, recall 0.849562
2017-12-10T15:13:01.625718: step 2071, loss 0.17145, acc 0.984375, prec 0.050629, recall 0.84964
2017-12-10T15:13:01.819605: step 2072, loss 0.320083, acc 0.921875, prec 0.0506795, recall 0.849794
2017-12-10T15:13:02.011914: step 2073, loss 0.316289, acc 0.90625, prec 0.0506993, recall 0.849871
2017-12-10T15:13:02.202311: step 2074, loss 0.178694, acc 0.921875, prec 0.0507206, recall 0.849949
2017-12-10T15:13:02.394365: step 2075, loss 0.498434, acc 0.90625, prec 0.0507404, recall 0.850026
2017-12-10T15:13:02.589970: step 2076, loss 0.274204, acc 0.890625, prec 0.0507586, recall 0.850103
2017-12-10T15:13:02.781716: step 2077, loss 0.234671, acc 0.921875, prec 0.0507508, recall 0.850103
2017-12-10T15:13:02.977792: step 2078, loss 0.189501, acc 0.921875, prec 0.0507722, recall 0.85018
2017-12-10T15:13:03.166460: step 2079, loss 0.257603, acc 0.953125, prec 0.0508256, recall 0.850333
2017-12-10T15:13:03.360496: step 2080, loss 0.13518, acc 0.953125, prec 0.0508501, recall 0.85041
2017-12-10T15:13:03.551807: step 2081, loss 0.203871, acc 0.921875, prec 0.0508423, recall 0.85041
2017-12-10T15:13:03.743335: step 2082, loss 0.203678, acc 0.953125, prec 0.0508376, recall 0.85041
2017-12-10T15:13:03.938782: step 2083, loss 0.526057, acc 0.9375, prec 0.0508604, recall 0.850486
2017-12-10T15:13:04.136866: step 2084, loss 0.24051, acc 0.90625, prec 0.0508511, recall 0.850486
2017-12-10T15:13:04.330651: step 2085, loss 0.155449, acc 0.953125, prec 0.0508464, recall 0.850486
2017-12-10T15:13:04.523026: step 2086, loss 0.46215, acc 0.890625, prec 0.0508936, recall 0.850639
2017-12-10T15:13:04.721061: step 2087, loss 0.22535, acc 0.9375, prec 0.0509164, recall 0.850716
2017-12-10T15:13:04.915404: step 2088, loss 0.739316, acc 0.953125, prec 0.0509989, recall 0.850944
2017-12-10T15:13:05.112299: step 2089, loss 0.12456, acc 0.953125, prec 0.0509942, recall 0.850944
2017-12-10T15:13:05.303269: step 2090, loss 0.427388, acc 0.921875, prec 0.0510444, recall 0.851096
2017-12-10T15:13:05.501817: step 2091, loss 0.29334, acc 0.890625, prec 0.0510625, recall 0.851172
2017-12-10T15:13:05.692917: step 2092, loss 0.240489, acc 0.953125, prec 0.0510869, recall 0.851248
2017-12-10T15:13:05.889810: step 2093, loss 0.18525, acc 0.90625, prec 0.0511065, recall 0.851324
2017-12-10T15:13:06.084228: step 2094, loss 1.03439, acc 0.921875, prec 0.0511857, recall 0.851551
2017-12-10T15:13:06.280930: step 2095, loss 0.660585, acc 0.84375, prec 0.051228, recall 0.851701
2017-12-10T15:13:06.473862: step 2096, loss 1.78977, acc 0.921875, prec 0.0513376, recall 0.85157
2017-12-10T15:13:06.672563: step 2097, loss 0.632646, acc 0.9375, prec 0.0513893, recall 0.851721
2017-12-10T15:13:06.867730: step 2098, loss 0.229356, acc 0.921875, prec 0.0513815, recall 0.851721
2017-12-10T15:13:07.064195: step 2099, loss 0.483993, acc 0.828125, prec 0.0513642, recall 0.851721
2017-12-10T15:13:07.256674: step 2100, loss 0.212611, acc 0.90625, prec 0.0513838, recall 0.851796
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_1/1512936356/checkpoints/model-2100

2017-12-10T15:13:08.395217: step 2101, loss 0.592968, acc 0.828125, prec 0.0513955, recall 0.851871
2017-12-10T15:13:08.592149: step 2102, loss 0.465536, acc 0.828125, prec 0.0513782, recall 0.851871
2017-12-10T15:13:08.781365: step 2103, loss 0.652626, acc 0.8125, prec 0.0513594, recall 0.851871
2017-12-10T15:13:08.971722: step 2104, loss 0.733568, acc 0.8125, prec 0.0513406, recall 0.851871
2017-12-10T15:13:09.163592: step 2105, loss 0.831764, acc 0.75, prec 0.0513445, recall 0.851945
2017-12-10T15:13:09.351528: step 2106, loss 0.267307, acc 0.890625, prec 0.0513913, recall 0.852095
2017-12-10T15:13:09.545942: step 2107, loss 0.461, acc 0.90625, prec 0.051382, recall 0.852095
2017-12-10T15:13:09.737830: step 2108, loss 0.421571, acc 0.765625, prec 0.0513585, recall 0.852095
2017-12-10T15:13:09.934330: step 2109, loss 0.368381, acc 0.875, prec 0.0513749, recall 0.85217
2017-12-10T15:13:10.123765: step 2110, loss 0.659747, acc 0.84375, prec 0.0514169, recall 0.852319
2017-12-10T15:13:10.321018: step 2111, loss 0.409694, acc 0.890625, prec 0.0514348, recall 0.852393
2017-12-10T15:13:10.511622: step 2112, loss 0.476486, acc 0.828125, prec 0.0514176, recall 0.852393
2017-12-10T15:13:10.704418: step 2113, loss 0.300448, acc 0.90625, prec 0.0514371, recall 0.852467
2017-12-10T15:13:10.899183: step 2114, loss 0.354307, acc 0.875, prec 0.0514246, recall 0.852467
2017-12-10T15:13:11.091147: step 2115, loss 0.761357, acc 0.828125, prec 0.0514938, recall 0.85269
2017-12-10T15:13:11.285215: step 2116, loss 0.15171, acc 0.953125, prec 0.0514891, recall 0.85269
2017-12-10T15:13:11.475893: step 2117, loss 1.22035, acc 0.90625, prec 0.0515373, recall 0.852838
2017-12-10T15:13:11.672430: step 2118, loss 0.110207, acc 0.9375, prec 0.0515311, recall 0.852838
2017-12-10T15:13:11.864466: step 2119, loss 0.148662, acc 0.9375, prec 0.0515824, recall 0.852985
2017-12-10T15:13:12.057449: step 2120, loss 0.552904, acc 0.890625, prec 0.051629, recall 0.853133
2017-12-10T15:13:12.253628: step 2121, loss 0.140328, acc 0.953125, prec 0.051653, recall 0.853206
2017-12-10T15:13:12.448122: step 2122, loss 0.213506, acc 0.90625, prec 0.0516436, recall 0.853206
2017-12-10T15:13:12.642575: step 2123, loss 0.285559, acc 0.953125, prec 0.0516677, recall 0.85328
2017-12-10T15:13:12.832932: step 2124, loss 0.28826, acc 0.9375, prec 0.0516614, recall 0.85328
2017-12-10T15:13:13.028805: step 2125, loss 0.119078, acc 0.96875, prec 0.051687, recall 0.853353
2017-12-10T15:13:13.221402: step 2126, loss 0.219195, acc 0.921875, prec 0.0516792, recall 0.853353
2017-12-10T15:13:13.421938: step 2127, loss 0.252925, acc 0.921875, prec 0.0516714, recall 0.853353
2017-12-10T15:13:13.611432: step 2128, loss 0.0648261, acc 0.953125, prec 0.0516667, recall 0.853353
2017-12-10T15:13:13.803930: step 2129, loss 0.0494226, acc 0.984375, prec 0.0516651, recall 0.853353
2017-12-10T15:13:14.001174: step 2130, loss 0.0522035, acc 0.984375, prec 0.0516635, recall 0.853353
2017-12-10T15:13:14.199907: step 2131, loss 0.0628528, acc 0.984375, prec 0.051662, recall 0.853353
2017-12-10T15:13:14.403047: step 2132, loss 0.722456, acc 0.921875, prec 0.0517403, recall 0.853573
2017-12-10T15:13:14.595893: step 2133, loss 0.0402564, acc 0.984375, prec 0.0517388, recall 0.853573
2017-12-10T15:13:14.788137: step 2134, loss 0.989502, acc 0.96875, prec 0.0517931, recall 0.853719
2017-12-10T15:13:14.985996: step 2135, loss 0.0401563, acc 0.984375, prec 0.0517915, recall 0.853719
2017-12-10T15:13:15.182029: step 2136, loss 1.11183, acc 0.984375, prec 0.0518187, recall 0.853792
2017-12-10T15:13:15.384066: step 2137, loss 0.110641, acc 0.953125, prec 0.0518139, recall 0.853792
2017-12-10T15:13:15.582831: step 2138, loss 2.07322, acc 0.90625, prec 0.0518061, recall 0.853367
2017-12-10T15:13:15.780537: step 2139, loss 0.226005, acc 0.9375, prec 0.0518285, recall 0.85344
2017-12-10T15:13:15.977166: step 2140, loss 0.119598, acc 0.90625, prec 0.0518191, recall 0.85344
2017-12-10T15:13:16.168875: step 2141, loss 0.782613, acc 0.953125, prec 0.0518431, recall 0.853513
2017-12-10T15:13:16.363363: step 2142, loss 0.263255, acc 0.953125, prec 0.0518671, recall 0.853586
2017-12-10T15:13:16.554797: step 2143, loss 0.337323, acc 0.921875, prec 0.0519166, recall 0.853731
2017-12-10T15:13:16.754319: step 2144, loss 0.352, acc 0.875, prec 0.0519327, recall 0.853804
2017-12-10T15:13:16.958419: step 2145, loss 0.315609, acc 0.90625, prec 0.051952, recall 0.853877
2017-12-10T15:13:17.151960: step 2146, loss 0.486554, acc 0.8125, prec 0.0519331, recall 0.853877
2017-12-10T15:13:17.344769: step 2147, loss 0.19839, acc 0.953125, prec 0.0519571, recall 0.853949
2017-12-10T15:13:17.533865: step 2148, loss 0.405122, acc 0.875, prec 0.0519732, recall 0.854022
2017-12-10T15:13:17.730104: step 2149, loss 0.402902, acc 0.890625, prec 0.0519622, recall 0.854022
2017-12-10T15:13:17.923338: step 2150, loss 0.390984, acc 0.890625, prec 0.0519512, recall 0.854022
2017-12-10T15:13:18.113728: step 2151, loss 0.271164, acc 0.875, prec 0.0519386, recall 0.854022
2017-12-10T15:13:18.307381: step 2152, loss 0.517886, acc 0.78125, prec 0.0519453, recall 0.854094
2017-12-10T15:13:18.496435: step 2153, loss 0.355713, acc 0.9375, prec 0.0519676, recall 0.854167
2017-12-10T15:13:18.688669: step 2154, loss 0.470274, acc 0.78125, prec 0.0519457, recall 0.854167
2017-12-10T15:13:18.877806: step 2155, loss 0.460428, acc 0.828125, prec 0.0519285, recall 0.854167
2017-12-10T15:13:19.069906: step 2156, loss 0.178136, acc 0.9375, prec 0.0519794, recall 0.854311
2017-12-10T15:13:19.266526: step 2157, loss 0.524573, acc 0.875, prec 0.052024, recall 0.854455
2017-12-10T15:13:19.461382: step 2158, loss 0.314118, acc 0.84375, prec 0.0520083, recall 0.854455
2017-12-10T15:13:19.657741: step 2159, loss 0.204327, acc 0.90625, prec 0.0519989, recall 0.854455
2017-12-10T15:13:19.850331: step 2160, loss 0.120071, acc 0.96875, prec 0.0520529, recall 0.854599
2017-12-10T15:13:20.045425: step 2161, loss 2.08997, acc 0.921875, prec 0.0520752, recall 0.854249
2017-12-10T15:13:20.243468: step 2162, loss 0.823623, acc 0.96875, prec 0.0521577, recall 0.854465
2017-12-10T15:13:20.439387: step 2163, loss 0.399993, acc 0.875, prec 0.0521451, recall 0.854465
2017-12-10T15:13:20.631735: step 2164, loss 0.374604, acc 0.921875, prec 0.0521943, recall 0.854608
2017-12-10T15:13:20.823431: step 2165, loss 0.191369, acc 0.921875, prec 0.0521865, recall 0.854608
2017-12-10T15:13:21.016078: step 2166, loss 0.181753, acc 0.9375, prec 0.0522087, recall 0.85468
2017-12-10T15:13:21.211813: step 2167, loss 0.146775, acc 0.9375, prec 0.0522024, recall 0.85468
2017-12-10T15:13:21.403783: step 2168, loss 0.261131, acc 0.890625, prec 0.0521914, recall 0.85468
2017-12-10T15:13:21.593841: step 2169, loss 0.36571, acc 0.84375, prec 0.0521757, recall 0.85468
2017-12-10T15:13:21.788056: step 2170, loss 0.209866, acc 0.9375, prec 0.0521695, recall 0.85468
2017-12-10T15:13:21.983214: step 2171, loss 0.37087, acc 0.9375, prec 0.0521917, recall 0.854751
2017-12-10T15:13:22.178116: step 2172, loss 0.574058, acc 0.859375, prec 0.0522345, recall 0.854894
2017-12-10T15:13:22.369811: step 2173, loss 0.237024, acc 0.890625, prec 0.0522236, recall 0.854894
2017-12-10T15:13:22.565199: step 2174, loss 0.295274, acc 0.890625, prec 0.0522126, recall 0.854894
2017-12-10T15:13:22.759004: step 2175, loss 0.409817, acc 0.921875, prec 0.0522617, recall 0.855037
2017-12-10T15:13:22.950875: step 2176, loss 0.867029, acc 0.875, prec 0.0522776, recall 0.855108
2017-12-10T15:13:23.148999: step 2177, loss 0.173099, acc 0.921875, prec 0.0522697, recall 0.855108
2017-12-10T15:13:23.340265: step 2178, loss 0.16114, acc 0.921875, prec 0.0522619, recall 0.855108
2017-12-10T15:13:23.532667: step 2179, loss 0.515768, acc 0.921875, prec 0.0523109, recall 0.85525
2017-12-10T15:13:23.726583: step 2180, loss 0.265979, acc 0.890625, prec 0.0523284, recall 0.855321
2017-12-10T15:13:23.921308: step 2181, loss 0.202215, acc 0.921875, prec 0.052349, recall 0.855392
2017-12-10T15:13:24.111412: step 2182, loss 0.313422, acc 0.890625, prec 0.052338, recall 0.855392
2017-12-10T15:13:24.301521: step 2183, loss 0.413386, acc 0.890625, prec 0.0523554, recall 0.855463
2017-12-10T15:13:24.493873: step 2184, loss 0.375549, acc 0.90625, prec 0.0523744, recall 0.855534
2017-12-10T15:13:24.687561: step 2185, loss 0.255782, acc 0.953125, prec 0.0524265, recall 0.855675
2017-12-10T15:13:24.884196: step 2186, loss 3.1184, acc 0.890625, prec 0.0524455, recall 0.855327
2017-12-10T15:13:25.076580: step 2187, loss 0.271173, acc 0.9375, prec 0.052496, recall 0.855469
2017-12-10T15:13:25.275383: step 2188, loss 0.540747, acc 0.84375, prec 0.0525086, recall 0.855539
2017-12-10T15:13:25.467262: step 2189, loss 0.479901, acc 0.8125, prec 0.0525465, recall 0.85568
2017-12-10T15:13:25.659412: step 2190, loss 0.29555, acc 0.84375, prec 0.0525308, recall 0.85568
2017-12-10T15:13:25.848588: step 2191, loss 0.239015, acc 0.921875, prec 0.0525229, recall 0.85568
2017-12-10T15:13:26.043452: step 2192, loss 0.436123, acc 0.875, prec 0.0525387, recall 0.855751
2017-12-10T15:13:26.235723: step 2193, loss 0.145794, acc 0.9375, prec 0.0525324, recall 0.855751
2017-12-10T15:13:26.427282: step 2194, loss 0.486453, acc 0.859375, prec 0.0525466, recall 0.855821
2017-12-10T15:13:26.621275: step 2195, loss 0.15363, acc 0.9375, prec 0.0525403, recall 0.855821
2017-12-10T15:13:26.815781: step 2196, loss 0.355916, acc 0.890625, prec 0.0525859, recall 0.855961
2017-12-10T15:13:27.008079: step 2197, loss 0.382733, acc 0.890625, prec 0.0526033, recall 0.856031
2017-12-10T15:13:27.199147: step 2198, loss 0.177464, acc 0.953125, prec 0.0526552, recall 0.856171
2017-12-10T15:13:27.395541: step 2199, loss 0.461529, acc 0.875, prec 0.0526709, recall 0.856241
2017-12-10T15:13:27.590378: step 2200, loss 0.263908, acc 0.875, prec 0.0526866, recall 0.856311
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_1/1512936356/checkpoints/model-2200

2017-12-10T15:13:28.806064: step 2201, loss 0.535874, acc 0.84375, prec 0.0526709, recall 0.856311
2017-12-10T15:13:29.001816: step 2202, loss 0.796669, acc 0.875, prec 0.0526866, recall 0.85638
2017-12-10T15:13:29.197067: step 2203, loss 0.301958, acc 0.9375, prec 0.0526803, recall 0.85638
2017-12-10T15:13:29.391548: step 2204, loss 0.185475, acc 0.90625, prec 0.0526708, recall 0.85638
2017-12-10T15:13:29.584234: step 2205, loss 0.0968198, acc 0.953125, prec 0.0526661, recall 0.85638
2017-12-10T15:13:29.777151: step 2206, loss 0.216644, acc 0.921875, prec 0.0526583, recall 0.85638
2017-12-10T15:13:29.968516: step 2207, loss 0.172714, acc 0.9375, prec 0.0527085, recall 0.85652
2017-12-10T15:13:30.160068: step 2208, loss 0.8168, acc 0.953125, prec 0.0527603, recall 0.856659
2017-12-10T15:13:30.358515: step 2209, loss 0.768039, acc 0.90625, prec 0.0528073, recall 0.856797
2017-12-10T15:13:30.550332: step 2210, loss 0.269679, acc 0.9375, prec 0.0528293, recall 0.856867
2017-12-10T15:13:30.739319: step 2211, loss 0.4823, acc 0.953125, prec 0.0528528, recall 0.856936
2017-12-10T15:13:30.936048: step 2212, loss 0.0723416, acc 0.984375, prec 0.0528512, recall 0.856936
2017-12-10T15:13:31.134236: step 2213, loss 0.109756, acc 0.953125, prec 0.0528465, recall 0.856936
2017-12-10T15:13:31.332193: step 2214, loss 0.148886, acc 0.921875, prec 0.0528669, recall 0.857005
2017-12-10T15:13:31.528564: step 2215, loss 1.2331, acc 0.9375, prec 0.0529186, recall 0.856729
2017-12-10T15:13:31.722187: step 2216, loss 0.184687, acc 0.9375, prec 0.0529123, recall 0.856729
2017-12-10T15:13:31.913504: step 2217, loss 0.519774, acc 0.890625, prec 0.0529294, recall 0.856798
2017-12-10T15:13:32.108013: step 2218, loss 0.151251, acc 0.9375, prec 0.0529513, recall 0.856867
2017-12-10T15:13:32.301003: step 2219, loss 0.646486, acc 0.90625, prec 0.0529701, recall 0.856936
2017-12-10T15:13:32.495142: step 2220, loss 0.328719, acc 0.890625, prec 0.052959, recall 0.856936
2017-12-10T15:13:32.685171: step 2221, loss 0.684781, acc 0.90625, prec 0.0529778, recall 0.857005
2017-12-10T15:13:32.877797: step 2222, loss 0.214166, acc 0.96875, prec 0.0530028, recall 0.857074
2017-12-10T15:13:33.073240: step 2223, loss 0.649897, acc 0.875, prec 0.0530184, recall 0.857143
2017-12-10T15:13:33.267172: step 2224, loss 0.962922, acc 0.859375, prec 0.0530323, recall 0.857212
2017-12-10T15:13:33.460113: step 2225, loss 0.468874, acc 0.875, prec 0.053076, recall 0.857349
2017-12-10T15:13:33.655227: step 2226, loss 0.268054, acc 0.90625, prec 0.0530666, recall 0.857349
2017-12-10T15:13:33.845209: step 2227, loss 0.593279, acc 0.8125, prec 0.0531039, recall 0.857486
2017-12-10T15:13:34.039823: step 2228, loss 0.374156, acc 0.84375, prec 0.0530881, recall 0.857486
2017-12-10T15:13:34.233777: step 2229, loss 0.41251, acc 0.90625, prec 0.0531068, recall 0.857554
2017-12-10T15:13:34.427629: step 2230, loss 0.57719, acc 0.859375, prec 0.0531207, recall 0.857622
2017-12-10T15:13:34.619382: step 2231, loss 0.133472, acc 0.953125, prec 0.053116, recall 0.857622
2017-12-10T15:13:34.809633: step 2232, loss 0.287057, acc 0.84375, prec 0.0531283, recall 0.85769
2017-12-10T15:13:35.000802: step 2233, loss 0.30453, acc 0.84375, prec 0.0531407, recall 0.857759
2017-12-10T15:13:35.197537: step 2234, loss 0.404227, acc 0.84375, prec 0.0531811, recall 0.857895
2017-12-10T15:13:35.390006: step 2235, loss 0.366356, acc 0.953125, prec 0.0532044, recall 0.857963
2017-12-10T15:13:35.583978: step 2236, loss 0.289554, acc 0.90625, prec 0.0532511, recall 0.858098
2017-12-10T15:13:35.772246: step 2237, loss 0.420121, acc 0.828125, prec 0.0533179, recall 0.858302
2017-12-10T15:13:35.967440: step 2238, loss 0.293934, acc 0.921875, prec 0.0533942, recall 0.858504
2017-12-10T15:13:36.156585: step 2239, loss 0.167125, acc 0.953125, prec 0.0534175, recall 0.858571
2017-12-10T15:13:36.347711: step 2240, loss 0.196747, acc 0.953125, prec 0.0534688, recall 0.858706
2017-12-10T15:13:36.539294: step 2241, loss 0.233909, acc 0.953125, prec 0.0534641, recall 0.858706
2017-12-10T15:13:36.731293: step 2242, loss 0.179836, acc 0.9375, prec 0.0535138, recall 0.85884
2017-12-10T15:13:36.924091: step 2243, loss 0.411635, acc 0.890625, prec 0.0535027, recall 0.85884
2017-12-10T15:13:37.112153: step 2244, loss 0.237019, acc 0.9375, prec 0.0534964, recall 0.85884
2017-12-10T15:13:37.306288: step 2245, loss 0.186066, acc 0.953125, prec 0.0535196, recall 0.858907
2017-12-10T15:13:37.499501: step 2246, loss 0.145429, acc 0.921875, prec 0.0535117, recall 0.858907
2017-12-10T15:13:37.691370: step 2247, loss 0.981555, acc 0.921875, prec 0.0535598, recall 0.859041
2017-12-10T15:13:37.887559: step 2248, loss 0.181077, acc 0.9375, prec 0.0535535, recall 0.859041
2017-12-10T15:13:38.076309: step 2249, loss 0.314997, acc 0.90625, prec 0.053572, recall 0.859108
2017-12-10T15:13:38.266903: step 2250, loss 0.274293, acc 0.890625, prec 0.0536168, recall 0.859242
2017-12-10T15:13:38.460088: step 2251, loss 0.135212, acc 0.953125, prec 0.0536401, recall 0.859308
2017-12-10T15:13:38.654906: step 2252, loss 0.20286, acc 0.921875, prec 0.0536321, recall 0.859308
2017-12-10T15:13:38.845657: step 2253, loss 0.121408, acc 0.96875, prec 0.053629, recall 0.859308
2017-12-10T15:13:39.037087: step 2254, loss 0.856837, acc 0.921875, prec 0.053677, recall 0.859442
2017-12-10T15:13:39.228823: step 2255, loss 0.220597, acc 0.9375, prec 0.0536986, recall 0.859508
2017-12-10T15:13:39.420824: step 2256, loss 0.443172, acc 0.90625, prec 0.0536891, recall 0.859508
2017-12-10T15:13:39.615686: step 2257, loss 0.755328, acc 0.96875, prec 0.0537139, recall 0.859574
2017-12-10T15:13:39.812352: step 2258, loss 0.455127, acc 0.96875, prec 0.0537387, recall 0.859641
2017-12-10T15:13:40.007898: step 2259, loss 0.596659, acc 0.96875, prec 0.0537914, recall 0.859773
2017-12-10T15:13:40.198820: step 2260, loss 0.446111, acc 0.9375, prec 0.0538689, recall 0.859972
2017-12-10T15:13:40.391513: step 2261, loss 0.371612, acc 0.859375, prec 0.0538546, recall 0.859972
2017-12-10T15:13:40.581241: step 2262, loss 1.00595, acc 0.921875, prec 0.0538745, recall 0.860038
2017-12-10T15:13:40.777089: step 2263, loss 0.103139, acc 0.953125, prec 0.0538698, recall 0.860038
2017-12-10T15:13:40.975019: step 2264, loss 0.214218, acc 0.921875, prec 0.0538618, recall 0.860038
2017-12-10T15:13:41.172676: step 2265, loss 1.73455, acc 0.859375, prec 0.0538491, recall 0.859633
2017-12-10T15:13:41.363280: step 2266, loss 0.247995, acc 0.90625, prec 0.0538675, recall 0.859699
2017-12-10T15:13:41.552486: step 2267, loss 0.417485, acc 0.90625, prec 0.053858, recall 0.859699
2017-12-10T15:13:41.744035: step 2268, loss 0.414861, acc 0.921875, prec 0.0539337, recall 0.859897
2017-12-10T15:13:41.937733: step 2269, loss 0.437566, acc 0.796875, prec 0.053913, recall 0.859897
2017-12-10T15:13:42.129548: step 2270, loss 0.504897, acc 0.796875, prec 0.0539203, recall 0.859962
2017-12-10T15:13:42.319279: step 2271, loss 0.216398, acc 0.890625, prec 0.0539091, recall 0.859962
2017-12-10T15:13:42.509870: step 2272, loss 0.488974, acc 0.796875, prec 0.0538885, recall 0.859962
2017-12-10T15:13:42.699406: step 2273, loss 0.455637, acc 0.8125, prec 0.0538695, recall 0.859962
2017-12-10T15:13:42.893007: step 2274, loss 0.169805, acc 0.921875, prec 0.0538616, recall 0.859962
2017-12-10T15:13:43.083984: step 2275, loss 0.335718, acc 0.859375, prec 0.0538751, recall 0.860028
2017-12-10T15:13:43.278592: step 2276, loss 0.380545, acc 0.921875, prec 0.0539229, recall 0.86016
2017-12-10T15:13:43.468464: step 2277, loss 0.648863, acc 0.796875, prec 0.0539301, recall 0.860225
2017-12-10T15:13:43.658252: step 2278, loss 0.451079, acc 0.828125, prec 0.0539126, recall 0.860225
2017-12-10T15:13:43.852250: step 2279, loss 0.263582, acc 0.90625, prec 0.0539031, recall 0.860225
2017-12-10T15:13:44.046824: step 2280, loss 0.215996, acc 0.953125, prec 0.0539262, recall 0.860291
2017-12-10T15:13:44.246849: step 2281, loss 0.260067, acc 0.921875, prec 0.0539738, recall 0.860422
2017-12-10T15:13:44.440338: step 2282, loss 0.312551, acc 0.921875, prec 0.0539937, recall 0.860487
2017-12-10T15:13:44.633295: step 2283, loss 0.13314, acc 0.9375, prec 0.0539874, recall 0.860487
2017-12-10T15:13:44.827126: step 2284, loss 0.301879, acc 0.890625, prec 0.0539763, recall 0.860487
2017-12-10T15:13:45.022536: step 2285, loss 0.112854, acc 0.953125, prec 0.0539715, recall 0.860487
2017-12-10T15:13:45.217502: step 2286, loss 0.374835, acc 0.921875, prec 0.0540191, recall 0.860617
2017-12-10T15:13:45.412051: step 2287, loss 0.198531, acc 0.984375, prec 0.0540453, recall 0.860683
2017-12-10T15:13:45.604733: step 2288, loss 0.112614, acc 0.96875, prec 0.0540699, recall 0.860748
2017-12-10T15:13:45.801578: step 2289, loss 0.0542762, acc 0.96875, prec 0.0540667, recall 0.860748
2017-12-10T15:13:45.993847: step 2290, loss 0.0755537, acc 0.984375, prec 0.0540652, recall 0.860748
2017-12-10T15:13:46.186700: step 2291, loss 0.223451, acc 0.984375, prec 0.0540913, recall 0.860813
2017-12-10T15:13:46.379204: step 2292, loss 0.0911433, acc 0.953125, prec 0.0540866, recall 0.860813
2017-12-10T15:13:46.571508: step 2293, loss 0.513702, acc 0.953125, prec 0.0541096, recall 0.860878
2017-12-10T15:13:46.768111: step 2294, loss 1.6143, acc 0.953125, prec 0.0541619, recall 0.860606
2017-12-10T15:13:46.960611: step 2295, loss 1.96663, acc 0.9375, prec 0.0541849, recall 0.86027
2017-12-10T15:13:47.158182: step 2296, loss 0.584192, acc 0.984375, prec 0.054211, recall 0.860335
2017-12-10T15:13:47.359804: step 2297, loss 0.138865, acc 0.96875, prec 0.0542356, recall 0.8604
2017-12-10T15:13:47.554789: step 2298, loss 0.12177, acc 0.953125, prec 0.0542586, recall 0.860465
2017-12-10T15:13:47.747141: step 2299, loss 0.252784, acc 0.90625, prec 0.054249, recall 0.860465
2017-12-10T15:13:47.939296: step 2300, loss 0.249205, acc 0.875, prec 0.0542363, recall 0.860465
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_1/1512936356/checkpoints/model-2300

2017-12-10T15:13:49.135389: step 2301, loss 0.518547, acc 0.84375, prec 0.0542481, recall 0.86053
2017-12-10T15:13:49.328105: step 2302, loss 0.297041, acc 0.90625, prec 0.0542663, recall 0.860595
2017-12-10T15:13:49.521477: step 2303, loss 0.477014, acc 0.8125, prec 0.0542472, recall 0.860595
2017-12-10T15:13:49.713282: step 2304, loss 0.902256, acc 0.828125, prec 0.0542851, recall 0.860724
2017-12-10T15:13:49.905792: step 2305, loss 0.39777, acc 0.84375, prec 0.0542692, recall 0.860724
2017-12-10T15:13:50.095810: step 2306, loss 0.420726, acc 0.875, prec 0.0542565, recall 0.860724
2017-12-10T15:13:50.287706: step 2307, loss 0.456689, acc 0.78125, prec 0.0542896, recall 0.860853
2017-12-10T15:13:50.481001: step 2308, loss 0.558976, acc 0.828125, prec 0.0542722, recall 0.860853
2017-12-10T15:13:50.669597: step 2309, loss 0.27921, acc 0.875, prec 0.0542871, recall 0.860918
2017-12-10T15:13:50.864135: step 2310, loss 0.54593, acc 0.890625, prec 0.054276, recall 0.860918
2017-12-10T15:13:51.057929: step 2311, loss 0.322249, acc 0.890625, prec 0.0542649, recall 0.860918
2017-12-10T15:13:51.250658: step 2312, loss 0.210446, acc 0.90625, prec 0.0543383, recall 0.861111
2017-12-10T15:13:51.444034: step 2313, loss 0.201387, acc 0.90625, prec 0.054384, recall 0.86124
2017-12-10T15:13:51.638336: step 2314, loss 0.231349, acc 0.921875, prec 0.0543761, recall 0.86124
2017-12-10T15:13:51.830221: step 2315, loss 0.250028, acc 0.90625, prec 0.0543666, recall 0.86124
2017-12-10T15:13:52.027618: step 2316, loss 0.106611, acc 0.953125, prec 0.0543618, recall 0.86124
2017-12-10T15:13:52.218920: step 2317, loss 0.125885, acc 0.984375, prec 0.0543602, recall 0.86124
2017-12-10T15:13:52.412004: step 2318, loss 0.164938, acc 0.953125, prec 0.0544106, recall 0.861368
2017-12-10T15:13:52.606651: step 2319, loss 0.140176, acc 0.96875, prec 0.0544351, recall 0.861432
2017-12-10T15:13:52.803619: step 2320, loss 0.214909, acc 0.9375, prec 0.0544287, recall 0.861432
2017-12-10T15:13:52.997047: step 2321, loss 0.407474, acc 0.9375, prec 0.0544775, recall 0.86156
2017-12-10T15:13:53.189864: step 2322, loss 0.158248, acc 0.9375, prec 0.0544988, recall 0.861624
2017-12-10T15:13:53.383846: step 2323, loss 0.0600664, acc 0.984375, prec 0.0545248, recall 0.861687
2017-12-10T15:13:53.574270: step 2324, loss 0.039361, acc 1, prec 0.0545799, recall 0.861815
2017-12-10T15:13:53.764408: step 2325, loss 2.23984, acc 0.9375, prec 0.0546027, recall 0.861482
2017-12-10T15:13:53.960058: step 2326, loss 0.072657, acc 1, prec 0.0546303, recall 0.861546
2017-12-10T15:13:54.149221: step 2327, loss 0.143524, acc 0.953125, prec 0.0546255, recall 0.861546
2017-12-10T15:13:54.344223: step 2328, loss 0.0882497, acc 0.96875, prec 0.0546223, recall 0.861546
2017-12-10T15:13:54.538283: step 2329, loss 0.192709, acc 0.921875, prec 0.0546419, recall 0.861609
2017-12-10T15:13:54.729998: step 2330, loss 0.240498, acc 0.984375, prec 0.0546679, recall 0.861673
2017-12-10T15:13:54.927334: step 2331, loss 0.0802306, acc 0.96875, prec 0.0546647, recall 0.861673
2017-12-10T15:13:55.123271: step 2332, loss 1.2674, acc 0.953125, prec 0.0546891, recall 0.861341
2017-12-10T15:13:55.322940: step 2333, loss 0.240958, acc 0.953125, prec 0.0547119, recall 0.861404
2017-12-10T15:13:55.515299: step 2334, loss 0.167771, acc 0.984375, prec 0.0547378, recall 0.861468
2017-12-10T15:13:55.707818: step 2335, loss 0.127213, acc 0.953125, prec 0.0547606, recall 0.861531
2017-12-10T15:13:55.896456: step 2336, loss 0.300643, acc 0.9375, prec 0.0548093, recall 0.861658
2017-12-10T15:13:56.092183: step 2337, loss 0.0974332, acc 0.96875, prec 0.0548336, recall 0.861722
2017-12-10T15:13:56.290606: step 2338, loss 0.137645, acc 0.90625, prec 0.0548241, recall 0.861722
2017-12-10T15:13:56.487422: step 2339, loss 0.564658, acc 0.96875, prec 0.0548484, recall 0.861785
2017-12-10T15:13:56.685148: step 2340, loss 0.185183, acc 0.953125, prec 0.0548711, recall 0.861848
2017-12-10T15:13:56.875554: step 2341, loss 0.369415, acc 0.890625, prec 0.0548599, recall 0.861848
2017-12-10T15:13:57.067569: step 2342, loss 0.21268, acc 0.921875, prec 0.054907, recall 0.861974
2017-12-10T15:13:57.258849: step 2343, loss 0.258195, acc 0.921875, prec 0.054899, recall 0.861974
2017-12-10T15:13:57.451366: step 2344, loss 1.04437, acc 0.890625, prec 0.0549978, recall 0.862226
2017-12-10T15:13:57.644593: step 2345, loss 0.283739, acc 0.875, prec 0.054985, recall 0.862226
2017-12-10T15:13:57.842070: step 2346, loss 0.323025, acc 0.875, prec 0.0549722, recall 0.862226
2017-12-10T15:13:58.037949: step 2347, loss 0.135441, acc 0.953125, prec 0.0549674, recall 0.862226
2017-12-10T15:13:58.231906: step 2348, loss 0.234243, acc 0.875, prec 0.0549821, recall 0.862289
2017-12-10T15:13:58.422143: step 2349, loss 0.848665, acc 0.84375, prec 0.0550485, recall 0.862477
2017-12-10T15:13:58.617039: step 2350, loss 0.204439, acc 0.90625, prec 0.0550939, recall 0.862602
2017-12-10T15:13:58.813565: step 2351, loss 0.378813, acc 0.890625, prec 0.0550827, recall 0.862602
2017-12-10T15:13:59.005126: step 2352, loss 0.452914, acc 0.875, prec 0.0550699, recall 0.862602
2017-12-10T15:13:59.204753: step 2353, loss 0.209562, acc 0.90625, prec 0.0550603, recall 0.862602
2017-12-10T15:13:59.404476: step 2354, loss 0.330276, acc 0.890625, prec 0.0550491, recall 0.862602
2017-12-10T15:13:59.595045: step 2355, loss 0.174647, acc 0.96875, prec 0.0551282, recall 0.86279
2017-12-10T15:13:59.785696: step 2356, loss 0.262147, acc 0.90625, prec 0.0551186, recall 0.86279
2017-12-10T15:13:59.974346: step 2357, loss 0.514696, acc 0.921875, prec 0.055138, recall 0.862852
2017-12-10T15:14:00.167436: step 2358, loss 0.208042, acc 0.921875, prec 0.05513, recall 0.862852
2017-12-10T15:14:00.357053: step 2359, loss 0.454003, acc 0.875, prec 0.0551172, recall 0.862852
2017-12-10T15:14:00.547994: step 2360, loss 0.731977, acc 0.90625, prec 0.055135, recall 0.862914
2017-12-10T15:14:00.740370: step 2361, loss 1.42456, acc 0.953125, prec 0.0551318, recall 0.862523
2017-12-10T15:14:00.937685: step 2362, loss 0.256799, acc 0.921875, prec 0.0551512, recall 0.862585
2017-12-10T15:14:01.136585: step 2363, loss 0.142632, acc 0.921875, prec 0.0551432, recall 0.862585
2017-12-10T15:14:01.334362: step 2364, loss 0.205197, acc 0.96875, prec 0.0551948, recall 0.86271
2017-12-10T15:14:01.534451: step 2365, loss 0.253518, acc 0.921875, prec 0.0551868, recall 0.86271
2017-12-10T15:14:01.730800: step 2366, loss 0.302156, acc 0.875, prec 0.055174, recall 0.86271
2017-12-10T15:14:01.923689: step 2367, loss 0.304802, acc 0.90625, prec 0.0551644, recall 0.86271
2017-12-10T15:14:02.115928: step 2368, loss 0.216234, acc 0.96875, prec 0.055216, recall 0.862834
2017-12-10T15:14:02.309212: step 2369, loss 0.208938, acc 0.953125, prec 0.0552385, recall 0.862896
2017-12-10T15:14:02.502300: step 2370, loss 0.155343, acc 0.96875, prec 0.0552627, recall 0.862958
2017-12-10T15:14:02.692713: step 2371, loss 0.396136, acc 0.921875, prec 0.0552821, recall 0.86302
2017-12-10T15:14:02.886791: step 2372, loss 0.228056, acc 0.96875, prec 0.0553062, recall 0.863082
2017-12-10T15:14:03.079861: step 2373, loss 0.431819, acc 0.890625, prec 0.0553497, recall 0.863205
2017-12-10T15:14:03.271425: step 2374, loss 0.12861, acc 0.96875, prec 0.0554012, recall 0.863329
2017-12-10T15:14:03.463876: step 2375, loss 0.153086, acc 0.921875, prec 0.0553932, recall 0.863329
2017-12-10T15:14:03.658949: step 2376, loss 0.541304, acc 0.859375, prec 0.0553787, recall 0.863329
2017-12-10T15:14:03.854864: step 2377, loss 0.537529, acc 0.953125, prec 0.0554286, recall 0.863452
2017-12-10T15:14:04.051876: step 2378, loss 0.221662, acc 0.921875, prec 0.0554479, recall 0.863514
2017-12-10T15:14:04.243643: step 2379, loss 0.447542, acc 0.921875, prec 0.0554672, recall 0.863575
2017-12-10T15:14:04.438659: step 2380, loss 0.206715, acc 0.921875, prec 0.0555138, recall 0.863698
2017-12-10T15:14:04.629946: step 2381, loss 0.347372, acc 0.859375, prec 0.0555267, recall 0.863759
2017-12-10T15:14:04.825008: step 2382, loss 0.248063, acc 0.90625, prec 0.0555443, recall 0.86382
2017-12-10T15:14:05.024104: step 2383, loss 0.446542, acc 0.875, prec 0.0555588, recall 0.863881
2017-12-10T15:14:05.217894: step 2384, loss 0.290724, acc 0.921875, prec 0.055578, recall 0.863943
2017-12-10T15:14:05.411081: step 2385, loss 0.202623, acc 0.921875, prec 0.05557, recall 0.863943
2017-12-10T15:14:05.601946: step 2386, loss 0.190327, acc 0.921875, prec 0.0555892, recall 0.864004
2017-12-10T15:14:05.793322: step 2387, loss 0.531251, acc 0.875, prec 0.0556037, recall 0.864065
2017-12-10T15:14:05.987953: step 2388, loss 0.123179, acc 0.9375, prec 0.0556245, recall 0.864126
2017-12-10T15:14:06.178673: step 2389, loss 0.23526, acc 0.890625, prec 0.0556133, recall 0.864126
2017-12-10T15:14:06.370808: step 2390, loss 2.47298, acc 0.921875, prec 0.0556069, recall 0.863738
2017-12-10T15:14:06.564298: step 2391, loss 0.0787363, acc 0.984375, prec 0.0556598, recall 0.86386
2017-12-10T15:14:06.756148: step 2392, loss 0.102433, acc 0.96875, prec 0.0556565, recall 0.86386
2017-12-10T15:14:06.948418: step 2393, loss 0.345896, acc 0.875, prec 0.0556709, recall 0.863921
2017-12-10T15:14:07.139248: step 2394, loss 0.410221, acc 0.828125, prec 0.0556805, recall 0.863982
2017-12-10T15:14:07.334232: step 2395, loss 0.188996, acc 0.90625, prec 0.0556981, recall 0.864043
2017-12-10T15:14:07.530120: step 2396, loss 0.272656, acc 0.9375, prec 0.0557189, recall 0.864104
2017-12-10T15:14:07.723496: step 2397, loss 0.246354, acc 0.984375, prec 0.0557717, recall 0.864225
2017-12-10T15:14:07.918187: step 2398, loss 0.301178, acc 0.890625, prec 0.0557605, recall 0.864225
2017-12-10T15:14:08.111369: step 2399, loss 0.544605, acc 0.953125, prec 0.0558101, recall 0.864346
2017-12-10T15:14:08.311939: step 2400, loss 0.395959, acc 0.859375, prec 0.05585, recall 0.864467
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_1/1512936356/checkpoints/model-2400

2017-12-10T15:14:09.535696: step 2401, loss 0.223224, acc 0.890625, prec 0.0558387, recall 0.864467
2017-12-10T15:14:09.734001: step 2402, loss 0.287213, acc 0.890625, prec 0.0558547, recall 0.864528
2017-12-10T15:14:09.927528: step 2403, loss 0.469535, acc 0.84375, prec 0.0558386, recall 0.864528
2017-12-10T15:14:10.118538: step 2404, loss 0.214704, acc 0.90625, prec 0.0558289, recall 0.864528
2017-12-10T15:14:10.310729: step 2405, loss 0.322354, acc 0.9375, prec 0.0558225, recall 0.864528
2017-12-10T15:14:10.501448: step 2406, loss 0.292902, acc 0.875, prec 0.055864, recall 0.864648
2017-12-10T15:14:10.692678: step 2407, loss 0.955317, acc 0.890625, prec 0.0559071, recall 0.864769
2017-12-10T15:14:10.891346: step 2408, loss 0.249831, acc 0.890625, prec 0.0559229, recall 0.864829
2017-12-10T15:14:11.085329: step 2409, loss 0.70551, acc 0.9375, prec 0.0559437, recall 0.864889
2017-12-10T15:14:11.281917: step 2410, loss 0.227122, acc 0.9375, prec 0.0559372, recall 0.864889
2017-12-10T15:14:11.473775: step 2411, loss 0.590612, acc 0.90625, prec 0.0559547, recall 0.864949
2017-12-10T15:14:11.670853: step 2412, loss 0.531238, acc 0.90625, prec 0.0559722, recall 0.865009
2017-12-10T15:14:11.865944: step 2413, loss 0.177802, acc 0.921875, prec 0.0559641, recall 0.865009
2017-12-10T15:14:12.058838: step 2414, loss 0.306115, acc 0.921875, prec 0.0559832, recall 0.865069
2017-12-10T15:14:12.252593: step 2415, loss 0.237911, acc 0.9375, prec 0.0560039, recall 0.865129
2017-12-10T15:14:12.443723: step 2416, loss 0.412163, acc 0.859375, prec 0.0559894, recall 0.865129
2017-12-10T15:14:12.637422: step 2417, loss 0.206326, acc 0.921875, prec 0.0559814, recall 0.865129
2017-12-10T15:14:12.832454: step 2418, loss 0.221686, acc 0.9375, prec 0.0560021, recall 0.865188
2017-12-10T15:14:13.027704: step 2419, loss 0.209517, acc 0.890625, prec 0.0560179, recall 0.865248
2017-12-10T15:14:13.218765: step 2420, loss 0.159024, acc 0.953125, prec 0.0560402, recall 0.865308
2017-12-10T15:14:13.411932: step 2421, loss 0.268661, acc 0.90625, prec 0.0560847, recall 0.865427
2017-12-10T15:14:13.608106: step 2422, loss 0.619989, acc 0.90625, prec 0.0561021, recall 0.865487
2017-12-10T15:14:13.802015: step 2423, loss 0.271711, acc 0.9375, prec 0.0561227, recall 0.865546
2017-12-10T15:14:13.994156: step 2424, loss 0.225401, acc 0.90625, prec 0.0561402, recall 0.865606
2017-12-10T15:14:14.193447: step 2425, loss 0.428305, acc 0.984375, prec 0.0561656, recall 0.865665
2017-12-10T15:14:14.393635: step 2426, loss 0.285888, acc 0.90625, prec 0.05621, recall 0.865784
2017-12-10T15:14:14.588727: step 2427, loss 0.272186, acc 0.859375, prec 0.0561956, recall 0.865784
2017-12-10T15:14:14.786024: step 2428, loss 0.149023, acc 0.9375, prec 0.0561891, recall 0.865784
2017-12-10T15:14:14.981969: step 2429, loss 0.20606, acc 0.921875, prec 0.0562622, recall 0.865961
2017-12-10T15:14:15.174221: step 2430, loss 0.949865, acc 0.9375, prec 0.0563098, recall 0.866079
2017-12-10T15:14:15.364178: step 2431, loss 0.155243, acc 0.96875, prec 0.0563876, recall 0.866256
2017-12-10T15:14:15.560931: step 2432, loss 0.320775, acc 0.921875, prec 0.0564336, recall 0.866374
2017-12-10T15:14:15.756245: step 2433, loss 0.235608, acc 0.890625, prec 0.0564223, recall 0.866374
2017-12-10T15:14:15.949939: step 2434, loss 0.259234, acc 0.875, prec 0.0564364, recall 0.866432
2017-12-10T15:14:16.139312: step 2435, loss 0.295926, acc 0.90625, prec 0.0564537, recall 0.866491
2017-12-10T15:14:16.337228: step 2436, loss 0.136124, acc 0.921875, prec 0.0564726, recall 0.86655
2017-12-10T15:14:16.533185: step 2437, loss 0.229721, acc 0.890625, prec 0.0564613, recall 0.86655
2017-12-10T15:14:16.722333: step 2438, loss 0.405301, acc 0.921875, prec 0.0565342, recall 0.866725
2017-12-10T15:14:16.921343: step 2439, loss 0.178593, acc 0.953125, prec 0.0565563, recall 0.866783
2017-12-10T15:14:17.114853: step 2440, loss 0.103808, acc 0.96875, prec 0.0565531, recall 0.866783
2017-12-10T15:14:17.312026: step 2441, loss 0.420364, acc 0.953125, prec 0.0566022, recall 0.8669
2017-12-10T15:14:17.506817: step 2442, loss 0.300713, acc 0.953125, prec 0.0566512, recall 0.867017
2017-12-10T15:14:17.701169: step 2443, loss 0.285184, acc 0.921875, prec 0.0566701, recall 0.867075
2017-12-10T15:14:17.894373: step 2444, loss 0.893375, acc 0.953125, prec 0.0567191, recall 0.867191
2017-12-10T15:14:18.088525: step 2445, loss 0.13214, acc 0.953125, prec 0.0567143, recall 0.867191
2017-12-10T15:14:18.284636: step 2446, loss 0.0675401, acc 0.984375, prec 0.0567396, recall 0.867249
2017-12-10T15:14:18.476058: step 2447, loss 0.218383, acc 0.9375, prec 0.0567331, recall 0.867249
2017-12-10T15:14:18.670420: step 2448, loss 0.15135, acc 0.984375, prec 0.0567854, recall 0.867365
2017-12-10T15:14:18.861575: step 2449, loss 0.262492, acc 0.9375, prec 0.0568328, recall 0.86748
2017-12-10T15:14:19.058532: step 2450, loss 0.343798, acc 0.84375, prec 0.0568704, recall 0.867596
2017-12-10T15:14:19.248860: step 2451, loss 0.21799, acc 0.9375, prec 0.0568639, recall 0.867596
2017-12-10T15:14:19.439911: step 2452, loss 0.240504, acc 0.96875, prec 0.0569414, recall 0.867769
2017-12-10T15:14:19.637170: step 2453, loss 0.165247, acc 0.96875, prec 0.0569382, recall 0.867769
2017-12-10T15:14:19.832007: step 2454, loss 0.0810969, acc 0.953125, prec 0.0569333, recall 0.867769
2017-12-10T15:14:20.025845: step 2455, loss 0.147884, acc 0.921875, prec 0.056979, recall 0.867884
2017-12-10T15:14:20.217565: step 2456, loss 0.149121, acc 0.953125, prec 0.0569741, recall 0.867884
2017-12-10T15:14:20.412045: step 2457, loss 0.185818, acc 0.953125, prec 0.057023, recall 0.867998
2017-12-10T15:14:20.605007: step 2458, loss 0.323948, acc 0.90625, prec 0.0570402, recall 0.868056
2017-12-10T15:14:20.795532: step 2459, loss 1.02143, acc 0.96875, prec 0.0571176, recall 0.868227
2017-12-10T15:14:20.992864: step 2460, loss 0.149086, acc 0.9375, prec 0.057138, recall 0.868284
2017-12-10T15:14:21.183400: step 2461, loss 0.215301, acc 0.953125, prec 0.05716, recall 0.868341
2017-12-10T15:14:21.379302: step 2462, loss 0.177667, acc 0.96875, prec 0.0571836, recall 0.868398
2017-12-10T15:14:21.573722: step 2463, loss 0.357662, acc 0.921875, prec 0.057256, recall 0.868569
2017-12-10T15:14:21.766141: step 2464, loss 1.29874, acc 0.96875, prec 0.0572544, recall 0.868194
2017-12-10T15:14:21.963015: step 2465, loss 0.756301, acc 0.890625, prec 0.0572699, recall 0.868251
2017-12-10T15:14:22.160152: step 2466, loss 0.107781, acc 0.96875, prec 0.0572934, recall 0.868307
2017-12-10T15:14:22.352232: step 2467, loss 0.409628, acc 0.9375, prec 0.0572869, recall 0.868307
2017-12-10T15:14:22.545403: step 2468, loss 0.136539, acc 0.953125, prec 0.057282, recall 0.868307
2017-12-10T15:14:22.740838: step 2469, loss 0.613805, acc 0.90625, prec 0.0573259, recall 0.868421
2017-12-10T15:14:22.933457: step 2470, loss 0.273902, acc 0.90625, prec 0.0573161, recall 0.868421
2017-12-10T15:14:23.125743: step 2471, loss 0.354186, acc 0.921875, prec 0.0573348, recall 0.868478
2017-12-10T15:14:23.324057: step 2472, loss 0.317474, acc 0.90625, prec 0.0573519, recall 0.868535
2017-12-10T15:14:23.513270: step 2473, loss 0.518467, acc 0.875, prec 0.0574193, recall 0.868704
2017-12-10T15:14:23.706179: step 2474, loss 0.558337, acc 0.859375, prec 0.0574314, recall 0.868761
2017-12-10T15:14:23.906217: step 2475, loss 0.249317, acc 0.9375, prec 0.0574785, recall 0.868874
2017-12-10T15:14:24.100526: step 2476, loss 0.310852, acc 0.90625, prec 0.0574686, recall 0.868874
2017-12-10T15:14:24.290625: step 2477, loss 0.432748, acc 0.859375, prec 0.0574807, recall 0.86893
2017-12-10T15:14:24.486860: step 2478, loss 0.374753, acc 0.875, prec 0.0575748, recall 0.869155
2017-12-10T15:14:24.677166: step 2479, loss 0.24193, acc 0.875, prec 0.0575617, recall 0.869155
2017-12-10T15:14:24.870184: step 2480, loss 0.311666, acc 0.875, prec 0.0575754, recall 0.869211
2017-12-10T15:14:25.064737: step 2481, loss 0.235372, acc 0.9375, prec 0.0575689, recall 0.869211
2017-12-10T15:14:25.259666: step 2482, loss 0.27627, acc 0.890625, prec 0.0575574, recall 0.869211
2017-12-10T15:14:25.452974: step 2483, loss 2.89758, acc 0.84375, prec 0.0575427, recall 0.868838
2017-12-10T15:14:25.651290: step 2484, loss 0.866421, acc 0.90625, prec 0.0575864, recall 0.868951
2017-12-10T15:14:25.826588: step 2485, loss 0.186634, acc 0.921569, prec 0.0575799, recall 0.868951
2017-12-10T15:14:26.026965: step 2486, loss 0.363155, acc 0.890625, prec 0.0575684, recall 0.868951
2017-12-10T15:14:26.222407: step 2487, loss 0.27954, acc 0.875, prec 0.0576089, recall 0.869063
2017-12-10T15:14:26.417134: step 2488, loss 0.377336, acc 0.875, prec 0.057676, recall 0.869231
2017-12-10T15:14:26.606860: step 2489, loss 0.325565, acc 0.90625, prec 0.0576929, recall 0.869287
2017-12-10T15:14:26.799394: step 2490, loss 0.451164, acc 0.828125, prec 0.0576749, recall 0.869287
2017-12-10T15:14:26.992155: step 2491, loss 0.458792, acc 0.796875, prec 0.0576536, recall 0.869287
2017-12-10T15:14:27.187845: step 2492, loss 0.205033, acc 0.890625, prec 0.0576422, recall 0.869287
2017-12-10T15:14:27.382015: step 2493, loss 0.488074, acc 0.84375, prec 0.0576259, recall 0.869287
2017-12-10T15:14:27.573682: step 2494, loss 0.206567, acc 0.921875, prec 0.0576177, recall 0.869287
2017-12-10T15:14:27.768880: step 2495, loss 0.200489, acc 0.953125, prec 0.0576128, recall 0.869287
2017-12-10T15:14:27.962001: step 2496, loss 0.180612, acc 0.9375, prec 0.057633, recall 0.869342
2017-12-10T15:14:28.160521: step 2497, loss 0.509095, acc 0.875, prec 0.0576466, recall 0.869398
2017-12-10T15:14:28.354007: step 2498, loss 0.0859431, acc 0.953125, prec 0.0576417, recall 0.869398
2017-12-10T15:14:28.549381: step 2499, loss 0.63594, acc 0.90625, prec 0.0577119, recall 0.869565
2017-12-10T15:14:28.739913: step 2500, loss 0.361262, acc 0.9375, prec 0.057812, recall 0.869787
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_1/1512936356/checkpoints/model-2500

2017-12-10T15:14:30.045333: step 2501, loss 0.220513, acc 0.953125, prec 0.0578604, recall 0.869898
2017-12-10T15:14:30.234229: step 2502, loss 0.12192, acc 0.96875, prec 0.0578837, recall 0.869953
2017-12-10T15:14:30.427900: step 2503, loss 0.181651, acc 0.953125, prec 0.0578788, recall 0.869953
2017-12-10T15:14:30.622048: step 2504, loss 0.170319, acc 0.921875, prec 0.0578706, recall 0.869953
2017-12-10T15:14:30.810486: step 2505, loss 0.164818, acc 0.921875, prec 0.0578891, recall 0.870008
2017-12-10T15:14:31.008073: step 2506, loss 0.13496, acc 0.984375, prec 0.0579407, recall 0.870119
2017-12-10T15:14:31.204593: step 2507, loss 0.0565386, acc 0.96875, prec 0.0579641, recall 0.870174
2017-12-10T15:14:31.405059: step 2508, loss 0.0962792, acc 0.96875, prec 0.0579608, recall 0.870174
2017-12-10T15:14:31.601405: step 2509, loss 0.106194, acc 0.9375, prec 0.0580075, recall 0.870284
2017-12-10T15:14:31.797906: step 2510, loss 0.629422, acc 0.953125, prec 0.0580558, recall 0.870394
2017-12-10T15:14:31.992114: step 2511, loss 0.192817, acc 0.9375, prec 0.0580758, recall 0.870449
2017-12-10T15:14:32.183026: step 2512, loss 0.63004, acc 0.984375, prec 0.058154, recall 0.870613
2017-12-10T15:14:32.380383: step 2513, loss 0.124033, acc 0.9375, prec 0.0581474, recall 0.870613
2017-12-10T15:14:32.573457: step 2514, loss 0.0416313, acc 0.984375, prec 0.0581458, recall 0.870613
2017-12-10T15:14:32.765484: step 2515, loss 0.196634, acc 0.953125, prec 0.0581674, recall 0.870668
2017-12-10T15:14:32.960400: step 2516, loss 0.114909, acc 0.953125, prec 0.0581891, recall 0.870722
2017-12-10T15:14:33.150961: step 2517, loss 0.1754, acc 0.96875, prec 0.0582124, recall 0.870777
2017-12-10T15:14:33.344811: step 2518, loss 0.106205, acc 0.96875, prec 0.0582357, recall 0.870832
2017-12-10T15:14:33.534869: step 2519, loss 0.194117, acc 0.953125, prec 0.0582839, recall 0.870941
2017-12-10T15:14:33.728833: step 2520, loss 0.107254, acc 1, prec 0.0583105, recall 0.870995
2017-12-10T15:14:33.928360: step 2521, loss 0.0297144, acc 1, prec 0.0583105, recall 0.870995
2017-12-10T15:14:34.125140: step 2522, loss 0.0832146, acc 0.96875, prec 0.0583072, recall 0.870995
2017-12-10T15:14:34.320969: step 2523, loss 0.199095, acc 0.96875, prec 0.0583837, recall 0.871158
2017-12-10T15:14:34.515363: step 2524, loss 0.163285, acc 0.9375, prec 0.0584302, recall 0.871266
2017-12-10T15:14:34.706456: step 2525, loss 0.215721, acc 0.90625, prec 0.0584203, recall 0.871266
2017-12-10T15:14:34.901392: step 2526, loss 0.925146, acc 0.953125, prec 0.0584419, recall 0.87132
2017-12-10T15:14:35.099660: step 2527, loss 0.118646, acc 0.96875, prec 0.0584917, recall 0.871429
2017-12-10T15:14:35.296788: step 2528, loss 0.0449304, acc 0.984375, prec 0.0584901, recall 0.871429
2017-12-10T15:14:35.490266: step 2529, loss 0.216151, acc 0.921875, prec 0.0585349, recall 0.871537
2017-12-10T15:14:35.683299: step 2530, loss 0.140103, acc 0.96875, prec 0.0585582, recall 0.87159
2017-12-10T15:14:35.875241: step 2531, loss 0.16193, acc 0.9375, prec 0.0585516, recall 0.87159
2017-12-10T15:14:36.065529: step 2532, loss 0.114387, acc 0.953125, prec 0.0585466, recall 0.87159
2017-12-10T15:14:36.257510: step 2533, loss 0.158887, acc 0.921875, prec 0.0585649, recall 0.871644
2017-12-10T15:14:36.451744: step 2534, loss 0.367892, acc 0.90625, prec 0.0586081, recall 0.871752
2017-12-10T15:14:36.646405: step 2535, loss 0.0631932, acc 0.96875, prec 0.0586578, recall 0.871859
2017-12-10T15:14:36.842308: step 2536, loss 0.645677, acc 0.9375, prec 0.0587042, recall 0.871967
2017-12-10T15:14:37.041846: step 2537, loss 0.237182, acc 0.9375, prec 0.0587506, recall 0.872074
2017-12-10T15:14:37.235939: step 2538, loss 0.490977, acc 0.84375, prec 0.0587871, recall 0.87218
2017-12-10T15:14:37.429057: step 2539, loss 0.069926, acc 0.96875, prec 0.0587838, recall 0.87218
2017-12-10T15:14:37.621531: step 2540, loss 0.343026, acc 0.890625, prec 0.0587987, recall 0.872234
2017-12-10T15:14:37.816964: step 2541, loss 0.0630889, acc 0.984375, prec 0.058797, recall 0.872234
2017-12-10T15:14:38.013998: step 2542, loss 0.133721, acc 0.96875, prec 0.0588202, recall 0.872287
2017-12-10T15:14:38.207193: step 2543, loss 0.317079, acc 0.90625, prec 0.0588103, recall 0.872287
2017-12-10T15:14:38.397856: step 2544, loss 0.209209, acc 0.9375, prec 0.0588301, recall 0.87234
2017-12-10T15:14:38.589597: step 2545, loss 0.0920768, acc 0.953125, prec 0.0588252, recall 0.87234
2017-12-10T15:14:38.784422: step 2546, loss 0.219398, acc 0.9375, prec 0.058845, recall 0.872394
2017-12-10T15:14:38.979237: step 2547, loss 0.114118, acc 0.953125, prec 0.0588401, recall 0.872394
2017-12-10T15:14:39.175209: step 2548, loss 0.263058, acc 0.90625, prec 0.0588301, recall 0.872394
2017-12-10T15:14:39.366166: step 2549, loss 0.829878, acc 0.984375, prec 0.058855, recall 0.872447
2017-12-10T15:14:39.560034: step 2550, loss 0.203828, acc 0.953125, prec 0.0589029, recall 0.872553
2017-12-10T15:14:39.751874: step 2551, loss 0.229126, acc 0.921875, prec 0.0589211, recall 0.872606
2017-12-10T15:14:39.944385: step 2552, loss 0.103034, acc 0.96875, prec 0.0589178, recall 0.872606
2017-12-10T15:14:40.134891: step 2553, loss 0.0694202, acc 0.984375, prec 0.0589161, recall 0.872606
2017-12-10T15:14:40.326843: step 2554, loss 0.12007, acc 1, prec 0.0590219, recall 0.872818
2017-12-10T15:14:40.521332: step 2555, loss 0.637202, acc 0.96875, prec 0.0590715, recall 0.872924
2017-12-10T15:14:40.715962: step 2556, loss 0.154128, acc 0.953125, prec 0.059093, recall 0.872976
2017-12-10T15:14:40.910991: step 2557, loss 0.118046, acc 0.9375, prec 0.0591127, recall 0.873029
2017-12-10T15:14:41.109506: step 2558, loss 0.456786, acc 0.9375, prec 0.059159, recall 0.873134
2017-12-10T15:14:41.302469: step 2559, loss 0.0997342, acc 0.953125, prec 0.059154, recall 0.873134
2017-12-10T15:14:41.492608: step 2560, loss 0.196715, acc 0.953125, prec 0.059149, recall 0.873134
2017-12-10T15:14:41.688016: step 2561, loss 0.0598759, acc 0.984375, prec 0.0591738, recall 0.873187
2017-12-10T15:14:41.880363: step 2562, loss 0.357536, acc 0.9375, prec 0.0592464, recall 0.873344
2017-12-10T15:14:42.076108: step 2563, loss 0.188735, acc 0.96875, prec 0.0592695, recall 0.873397
2017-12-10T15:14:42.268786: step 2564, loss 0.259529, acc 0.921875, prec 0.0592611, recall 0.873397
2017-12-10T15:14:42.462656: step 2565, loss 0.186369, acc 0.96875, prec 0.0592842, recall 0.873449
2017-12-10T15:14:42.658041: step 2566, loss 0.299769, acc 0.890625, prec 0.059299, recall 0.873501
2017-12-10T15:14:42.849599: step 2567, loss 0.14212, acc 0.953125, prec 0.059294, recall 0.873501
2017-12-10T15:14:43.043478: step 2568, loss 0.120916, acc 0.953125, prec 0.059289, recall 0.873501
2017-12-10T15:14:43.240614: step 2569, loss 0.528825, acc 0.875, prec 0.0593021, recall 0.873554
2017-12-10T15:14:43.433997: step 2570, loss 0.337812, acc 0.921875, prec 0.0592937, recall 0.873554
2017-12-10T15:14:43.624551: step 2571, loss 0.211061, acc 0.96875, prec 0.0593432, recall 0.873658
2017-12-10T15:14:43.822973: step 2572, loss 0.22628, acc 0.9375, prec 0.0593365, recall 0.873658
2017-12-10T15:14:44.015855: step 2573, loss 0.205645, acc 0.9375, prec 0.0593562, recall 0.87371
2017-12-10T15:14:44.214489: step 2574, loss 0.103608, acc 0.96875, prec 0.0593529, recall 0.87371
2017-12-10T15:14:44.409400: step 2575, loss 0.252742, acc 0.921875, prec 0.0593973, recall 0.873814
2017-12-10T15:14:44.611342: step 2576, loss 0.203006, acc 0.921875, prec 0.0594417, recall 0.873918
2017-12-10T15:14:44.802078: step 2577, loss 0.159877, acc 0.953125, prec 0.0594631, recall 0.87397
2017-12-10T15:14:44.995879: step 2578, loss 0.137867, acc 0.953125, prec 0.0595108, recall 0.874074
2017-12-10T15:14:45.194345: step 2579, loss 0.216077, acc 0.953125, prec 0.0595321, recall 0.874126
2017-12-10T15:14:45.388086: step 2580, loss 0.102667, acc 0.96875, prec 0.0595552, recall 0.874178
2017-12-10T15:14:45.579629: step 2581, loss 0.0752781, acc 0.96875, prec 0.0595782, recall 0.874229
2017-12-10T15:14:45.777710: step 2582, loss 0.0670296, acc 1, prec 0.0596308, recall 0.874333
2017-12-10T15:14:45.975060: step 2583, loss 0.121731, acc 0.984375, prec 0.0596555, recall 0.874384
2017-12-10T15:14:46.174038: step 2584, loss 0.455774, acc 0.9375, prec 0.0597015, recall 0.874487
2017-12-10T15:14:46.368896: step 2585, loss 0.195205, acc 0.953125, prec 0.0597228, recall 0.874539
2017-12-10T15:14:46.562704: step 2586, loss 0.0983816, acc 0.984375, prec 0.0597211, recall 0.874539
2017-12-10T15:14:46.752866: step 2587, loss 0.236531, acc 0.921875, prec 0.0597917, recall 0.874693
2017-12-10T15:14:46.944589: step 2588, loss 0.0981834, acc 0.96875, prec 0.0597884, recall 0.874693
2017-12-10T15:14:47.141355: step 2589, loss 2.8757, acc 0.96875, prec 0.0598393, recall 0.874438
2017-12-10T15:14:47.333697: step 2590, loss 0.404981, acc 0.96875, prec 0.0598886, recall 0.87454
2017-12-10T15:14:47.531410: step 2591, loss 0.105218, acc 0.953125, prec 0.0598836, recall 0.87454
2017-12-10T15:14:47.723825: step 2592, loss 0.194738, acc 0.96875, prec 0.0599328, recall 0.874643
2017-12-10T15:14:47.917260: step 2593, loss 0.229726, acc 0.96875, prec 0.0599558, recall 0.874694
2017-12-10T15:14:48.111417: step 2594, loss 0.198662, acc 0.890625, prec 0.0599703, recall 0.874745
2017-12-10T15:14:48.304368: step 2595, loss 0.204462, acc 0.921875, prec 0.059962, recall 0.874745
2017-12-10T15:14:48.494923: step 2596, loss 0.160732, acc 0.9375, prec 0.0599815, recall 0.874796
2017-12-10T15:14:48.685626: step 2597, loss 1.23852, acc 0.890625, prec 0.0600486, recall 0.874949
2017-12-10T15:14:48.880269: step 2598, loss 0.404483, acc 0.875, prec 0.0600352, recall 0.874949
2017-12-10T15:14:49.073082: step 2599, loss 0.320167, acc 0.890625, prec 0.060076, recall 0.875051
2017-12-10T15:14:49.266611: step 2600, loss 0.40634, acc 0.921875, prec 0.0600676, recall 0.875051
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_1/1512936356/checkpoints/model-2600

2017-12-10T15:14:50.525341: step 2601, loss 0.447551, acc 0.859375, prec 0.0600788, recall 0.875102
2017-12-10T15:14:50.715873: step 2602, loss 0.193199, acc 0.921875, prec 0.0600966, recall 0.875153
2017-12-10T15:14:50.906437: step 2603, loss 0.20179, acc 0.890625, prec 0.0601374, recall 0.875254
2017-12-10T15:14:51.101527: step 2604, loss 0.594579, acc 0.859375, prec 0.0601223, recall 0.875254
2017-12-10T15:14:51.291483: step 2605, loss 0.643285, acc 0.796875, prec 0.0601004, recall 0.875254
2017-12-10T15:14:51.487580: step 2606, loss 0.257277, acc 0.875, prec 0.0601133, recall 0.875305
2017-12-10T15:14:51.681145: step 2607, loss 0.484442, acc 0.84375, prec 0.0600965, recall 0.875305
2017-12-10T15:14:51.874732: step 2608, loss 0.368006, acc 0.890625, prec 0.060111, recall 0.875355
2017-12-10T15:14:52.070773: step 2609, loss 0.304682, acc 0.859375, prec 0.0601221, recall 0.875406
2017-12-10T15:14:52.261670: step 2610, loss 0.249344, acc 0.921875, prec 0.0601661, recall 0.875507
2017-12-10T15:14:52.457238: step 2611, loss 0.16164, acc 0.953125, prec 0.0601611, recall 0.875507
2017-12-10T15:14:52.649656: step 2612, loss 0.267995, acc 0.875, prec 0.0601477, recall 0.875507
2017-12-10T15:14:52.843776: step 2613, loss 0.0655077, acc 0.96875, prec 0.0601705, recall 0.875557
2017-12-10T15:14:53.035350: step 2614, loss 0.375281, acc 0.9375, prec 0.06019, recall 0.875608
2017-12-10T15:14:53.228808: step 2615, loss 0.132898, acc 0.953125, prec 0.0602111, recall 0.875658
2017-12-10T15:14:53.418090: step 2616, loss 0.391881, acc 0.9375, prec 0.0602567, recall 0.875759
2017-12-10T15:14:53.609337: step 2617, loss 0.109201, acc 0.953125, prec 0.0602779, recall 0.875809
2017-12-10T15:14:53.805361: step 2618, loss 0.310148, acc 0.9375, prec 0.0603235, recall 0.875909
2017-12-10T15:14:53.998178: step 2619, loss 0.307722, acc 0.953125, prec 0.0603446, recall 0.87596
2017-12-10T15:14:54.190919: step 2620, loss 0.117131, acc 0.953125, prec 0.0603657, recall 0.87601
2017-12-10T15:14:54.384059: step 2621, loss 0.277852, acc 0.921875, prec 0.0603573, recall 0.87601
2017-12-10T15:14:54.577003: step 2622, loss 0.77158, acc 0.921875, prec 0.0604273, recall 0.87616
2017-12-10T15:14:54.775132: step 2623, loss 0.0221851, acc 1, prec 0.0604273, recall 0.87616
2017-12-10T15:14:54.973394: step 2624, loss 1.82117, acc 0.96875, prec 0.0604518, recall 0.875857
2017-12-10T15:14:55.172194: step 2625, loss 1.18623, acc 0.953125, prec 0.060499, recall 0.875956
2017-12-10T15:14:55.370498: step 2626, loss 0.179021, acc 0.9375, prec 0.0604923, recall 0.875956
2017-12-10T15:14:55.566532: step 2627, loss 0.265765, acc 0.90625, prec 0.0605083, recall 0.876006
2017-12-10T15:14:55.762021: step 2628, loss 0.234033, acc 0.9375, prec 0.0605799, recall 0.876156
2017-12-10T15:14:55.955650: step 2629, loss 0.26872, acc 0.9375, prec 0.0605993, recall 0.876206
2017-12-10T15:14:56.153369: step 2630, loss 0.470254, acc 0.859375, prec 0.0606103, recall 0.876256
2017-12-10T15:14:56.340689: step 2631, loss 0.148781, acc 0.984375, prec 0.0606347, recall 0.876305
2017-12-10T15:14:56.532573: step 2632, loss 0.173519, acc 0.9375, prec 0.060628, recall 0.876305
2017-12-10T15:14:56.724514: step 2633, loss 0.926384, acc 0.921875, prec 0.0607239, recall 0.876504
2017-12-10T15:14:56.919392: step 2634, loss 0.318658, acc 0.9375, prec 0.0607693, recall 0.876603
2017-12-10T15:14:57.113911: step 2635, loss 0.109067, acc 1, prec 0.0608476, recall 0.876751
2017-12-10T15:14:57.307338: step 2636, loss 0.198537, acc 0.9375, prec 0.0608408, recall 0.876751
2017-12-10T15:14:57.505007: step 2637, loss 0.48428, acc 0.90625, prec 0.0608828, recall 0.876849
2017-12-10T15:14:57.702392: step 2638, loss 0.270535, acc 0.9375, prec 0.0609022, recall 0.876898
2017-12-10T15:14:57.897309: step 2639, loss 0.640535, acc 0.859375, prec 0.0609391, recall 0.876997
2017-12-10T15:14:58.096317: step 2640, loss 0.200647, acc 0.921875, prec 0.0609567, recall 0.877046
2017-12-10T15:14:58.294035: step 2641, loss 0.229752, acc 0.875, prec 0.0609952, recall 0.877144
2017-12-10T15:14:58.492137: step 2642, loss 0.247289, acc 0.921875, prec 0.0609868, recall 0.877144
2017-12-10T15:14:58.685983: step 2643, loss 0.124533, acc 0.9375, prec 0.06098, recall 0.877144
2017-12-10T15:14:58.877806: step 2644, loss 0.206177, acc 0.96875, prec 0.0610287, recall 0.877242
2017-12-10T15:14:59.069957: step 2645, loss 0.138295, acc 0.921875, prec 0.0610463, recall 0.877291
2017-12-10T15:14:59.271353: step 2646, loss 0.272193, acc 0.90625, prec 0.0610361, recall 0.877291
2017-12-10T15:14:59.465598: step 2647, loss 0.149309, acc 0.953125, prec 0.0610571, recall 0.87734
2017-12-10T15:14:59.655098: step 2648, loss 0.190663, acc 0.890625, prec 0.0610712, recall 0.877389
2017-12-10T15:14:59.848294: step 2649, loss 0.337354, acc 0.921875, prec 0.0611148, recall 0.877486
2017-12-10T15:15:00.043406: step 2650, loss 0.214358, acc 0.90625, prec 0.0611046, recall 0.877486
2017-12-10T15:15:00.239234: step 2651, loss 0.163376, acc 0.96875, prec 0.0611273, recall 0.877535
2017-12-10T15:15:00.437029: step 2652, loss 1.07132, acc 0.921875, prec 0.0611968, recall 0.877681
2017-12-10T15:15:00.632472: step 2653, loss 0.192002, acc 0.921875, prec 0.0612143, recall 0.877729
2017-12-10T15:15:00.827836: step 2654, loss 0.129279, acc 0.96875, prec 0.0612629, recall 0.877826
2017-12-10T15:15:01.023595: step 2655, loss 0.103625, acc 0.9375, prec 0.0612821, recall 0.877875
2017-12-10T15:15:01.219926: step 2656, loss 0.166987, acc 0.9375, prec 0.0613013, recall 0.877923
2017-12-10T15:15:01.416670: step 2657, loss 0.312888, acc 0.890625, prec 0.0613154, recall 0.877971
2017-12-10T15:15:01.612654: step 2658, loss 0.222598, acc 0.890625, prec 0.0613035, recall 0.877971
2017-12-10T15:15:01.807006: step 2659, loss 0.107093, acc 0.96875, prec 0.0613001, recall 0.877971
2017-12-10T15:15:01.998212: step 2660, loss 0.279134, acc 0.90625, prec 0.0613678, recall 0.878116
2017-12-10T15:15:02.186743: step 2661, loss 0.418149, acc 0.953125, prec 0.0613887, recall 0.878165
2017-12-10T15:15:02.388964: step 2662, loss 0.115208, acc 0.984375, prec 0.061387, recall 0.878165
2017-12-10T15:15:02.580132: step 2663, loss 0.138584, acc 0.9375, prec 0.0613802, recall 0.878165
2017-12-10T15:15:02.770319: step 2664, loss 0.052255, acc 0.984375, prec 0.0613785, recall 0.878165
2017-12-10T15:15:02.965439: step 2665, loss 0.15483, acc 0.921875, prec 0.06137, recall 0.878165
2017-12-10T15:15:03.158690: step 2666, loss 0.248826, acc 0.953125, prec 0.061365, recall 0.878165
2017-12-10T15:15:03.347839: step 2667, loss 0.607493, acc 0.984375, prec 0.0614151, recall 0.878261
2017-12-10T15:15:03.538849: step 2668, loss 0.181019, acc 0.9375, prec 0.0614602, recall 0.878357
2017-12-10T15:15:03.734747: step 2669, loss 0.1563, acc 0.9375, prec 0.0614534, recall 0.878357
2017-12-10T15:15:03.924181: step 2670, loss 0.87978, acc 0.921875, prec 0.0614709, recall 0.878405
2017-12-10T15:15:04.118656: step 2671, loss 0.281322, acc 0.96875, prec 0.0615193, recall 0.878501
2017-12-10T15:15:04.315065: step 2672, loss 0.13881, acc 0.953125, prec 0.0615402, recall 0.878549
2017-12-10T15:15:04.505902: step 2673, loss 0.147499, acc 0.96875, prec 0.0615368, recall 0.878549
2017-12-10T15:15:04.702165: step 2674, loss 0.0758947, acc 0.96875, prec 0.0615334, recall 0.878549
2017-12-10T15:15:04.894739: step 2675, loss 0.238801, acc 0.921875, prec 0.0615249, recall 0.878549
2017-12-10T15:15:05.089276: step 2676, loss 0.0237884, acc 1, prec 0.0615249, recall 0.878549
2017-12-10T15:15:05.283029: step 2677, loss 0.0706, acc 0.984375, prec 0.0615232, recall 0.878549
2017-12-10T15:15:05.473529: step 2678, loss 0.266871, acc 0.953125, prec 0.0615181, recall 0.878549
2017-12-10T15:15:05.667063: step 2679, loss 0.222315, acc 0.96875, prec 0.0615665, recall 0.878645
2017-12-10T15:15:05.860508: step 2680, loss 0.136148, acc 0.96875, prec 0.0616408, recall 0.878788
2017-12-10T15:15:06.059183: step 2681, loss 0.102403, acc 0.953125, prec 0.0616357, recall 0.878788
2017-12-10T15:15:06.251855: step 2682, loss 0.207741, acc 0.953125, prec 0.0616306, recall 0.878788
2017-12-10T15:15:06.442950: step 2683, loss 0.0334297, acc 0.984375, prec 0.0616289, recall 0.878788
2017-12-10T15:15:06.635360: step 2684, loss 0.25913, acc 0.9375, prec 0.0616221, recall 0.878788
2017-12-10T15:15:06.829731: step 2685, loss 0.0342127, acc 0.984375, prec 0.0616463, recall 0.878836
2017-12-10T15:15:07.024211: step 2686, loss 0.0254046, acc 1, prec 0.0616463, recall 0.878836
2017-12-10T15:15:07.216770: step 2687, loss 0.0700254, acc 0.953125, prec 0.0616412, recall 0.878836
2017-12-10T15:15:07.407911: step 2688, loss 0.0167782, acc 1, prec 0.0616412, recall 0.878836
2017-12-10T15:15:07.602362: step 2689, loss 0.111028, acc 0.953125, prec 0.061662, recall 0.878883
2017-12-10T15:15:07.791568: step 2690, loss 0.13019, acc 0.953125, prec 0.0616828, recall 0.878931
2017-12-10T15:15:07.985492: step 2691, loss 0.301448, acc 0.90625, prec 0.0616984, recall 0.878978
2017-12-10T15:15:08.178194: step 2692, loss 0.100266, acc 0.953125, prec 0.0616933, recall 0.878978
2017-12-10T15:15:08.369952: step 2693, loss 0.702591, acc 0.953125, prec 0.06174, recall 0.879073
2017-12-10T15:15:08.566618: step 2694, loss 0.194423, acc 0.96875, prec 0.0617624, recall 0.879121
2017-12-10T15:15:08.759614: step 2695, loss 0.179795, acc 0.953125, prec 0.0617832, recall 0.879168
2017-12-10T15:15:08.953096: step 2696, loss 0.0139536, acc 1, prec 0.0617832, recall 0.879168
2017-12-10T15:15:09.151482: step 2697, loss 0.00653576, acc 1, prec 0.0618091, recall 0.879216
2017-12-10T15:15:09.345974: step 2698, loss 0.113027, acc 0.953125, prec 0.0618039, recall 0.879216
2017-12-10T15:15:09.544331: step 2699, loss 0.115308, acc 0.984375, prec 0.0618281, recall 0.879263
2017-12-10T15:15:09.738172: step 2700, loss 0.549105, acc 1, prec 0.0619057, recall 0.879405
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_1/1512936356/checkpoints/model-2700

2017-12-10T15:15:10.912361: step 2701, loss 0.190786, acc 0.9375, prec 0.0619247, recall 0.879452
2017-12-10T15:15:11.107926: step 2702, loss 0.999295, acc 0.875, prec 0.0619627, recall 0.879546
2017-12-10T15:15:11.303648: step 2703, loss 0.188074, acc 0.953125, prec 0.0619576, recall 0.879546
2017-12-10T15:15:11.501771: step 2704, loss 0.139085, acc 0.9375, prec 0.0619508, recall 0.879546
2017-12-10T15:15:11.699295: step 2705, loss 0.0831949, acc 0.96875, prec 0.0619474, recall 0.879546
2017-12-10T15:15:11.892579: step 2706, loss 2.00595, acc 0.9375, prec 0.0619681, recall 0.87925
2017-12-10T15:15:12.091209: step 2707, loss 0.295953, acc 0.921875, prec 0.0619854, recall 0.879297
2017-12-10T15:15:12.283849: step 2708, loss 0.265189, acc 0.921875, prec 0.0620027, recall 0.879344
2017-12-10T15:15:12.477344: step 2709, loss 0.0922886, acc 0.953125, prec 0.0619976, recall 0.879344
2017-12-10T15:15:12.668115: step 2710, loss 0.259163, acc 0.890625, prec 0.0620373, recall 0.879438
2017-12-10T15:15:12.856816: step 2711, loss 0.321417, acc 0.875, prec 0.0620494, recall 0.879485
2017-12-10T15:15:13.051676: step 2712, loss 0.246708, acc 0.90625, prec 0.0620392, recall 0.879485
2017-12-10T15:15:13.243514: step 2713, loss 0.829776, acc 0.90625, prec 0.0620547, recall 0.879532
2017-12-10T15:15:13.434901: step 2714, loss 0.191642, acc 0.9375, prec 0.0620737, recall 0.879579
2017-12-10T15:15:13.627081: step 2715, loss 0.472739, acc 0.859375, prec 0.0620583, recall 0.879579
2017-12-10T15:15:13.821974: step 2716, loss 0.251558, acc 0.890625, prec 0.0620464, recall 0.879579
2017-12-10T15:15:14.015380: step 2717, loss 0.229115, acc 0.921875, prec 0.0620379, recall 0.879579
2017-12-10T15:15:14.210211: step 2718, loss 0.41505, acc 0.859375, prec 0.0620483, recall 0.879626
2017-12-10T15:15:14.405701: step 2719, loss 0.344054, acc 0.90625, prec 0.0620381, recall 0.879626
2017-12-10T15:15:14.595493: step 2720, loss 0.132382, acc 0.9375, prec 0.0620313, recall 0.879626
2017-12-10T15:15:14.795970: step 2721, loss 0.283956, acc 0.90625, prec 0.0620468, recall 0.879673
2017-12-10T15:15:14.988658: step 2722, loss 0.197748, acc 0.90625, prec 0.0620366, recall 0.879673
2017-12-10T15:15:15.181853: step 2723, loss 0.227962, acc 0.953125, prec 0.0620572, recall 0.87972
2017-12-10T15:15:15.377488: step 2724, loss 0.38557, acc 0.84375, prec 0.0620659, recall 0.879767
2017-12-10T15:15:15.568460: step 2725, loss 0.293306, acc 0.890625, prec 0.062054, recall 0.879767
2017-12-10T15:15:15.759630: step 2726, loss 0.327586, acc 0.859375, prec 0.0620644, recall 0.879813
2017-12-10T15:15:15.950061: step 2727, loss 0.22056, acc 0.921875, prec 0.0620816, recall 0.87986
2017-12-10T15:15:16.145944: step 2728, loss 0.233161, acc 0.921875, prec 0.0620731, recall 0.87986
2017-12-10T15:15:16.337127: step 2729, loss 0.235624, acc 0.9375, prec 0.062092, recall 0.879907
2017-12-10T15:15:16.533235: step 2730, loss 0.65596, acc 0.953125, prec 0.0621384, recall 0.88
2017-12-10T15:15:16.724899: step 2731, loss 0.127084, acc 0.96875, prec 0.062135, recall 0.88
2017-12-10T15:15:16.919355: step 2732, loss 0.0546734, acc 0.96875, prec 0.0621316, recall 0.88
2017-12-10T15:15:17.113227: step 2733, loss 0.223289, acc 0.9375, prec 0.0621247, recall 0.88
2017-12-10T15:15:17.309500: step 2734, loss 0.0708302, acc 0.96875, prec 0.062147, recall 0.880047
2017-12-10T15:15:17.505445: step 2735, loss 0.090523, acc 0.953125, prec 0.0621419, recall 0.880047
2017-12-10T15:15:17.700098: step 2736, loss 0.0896816, acc 0.96875, prec 0.0621642, recall 0.880093
2017-12-10T15:15:17.896077: step 2737, loss 0.0478138, acc 0.96875, prec 0.0621608, recall 0.880093
2017-12-10T15:15:18.088027: step 2738, loss 0.857354, acc 0.96875, prec 0.0622088, recall 0.880186
2017-12-10T15:15:18.284412: step 2739, loss 0.106233, acc 0.96875, prec 0.0622311, recall 0.880233
2017-12-10T15:15:18.479983: step 2740, loss 0.0566929, acc 0.96875, prec 0.0622277, recall 0.880233
2017-12-10T15:15:18.677741: step 2741, loss 0.213572, acc 0.984375, prec 0.0622517, recall 0.880279
2017-12-10T15:15:18.869425: step 2742, loss 0.0318582, acc 1, prec 0.0622517, recall 0.880279
2017-12-10T15:15:19.059999: step 2743, loss 3.44678, acc 0.921875, prec 0.0622706, recall 0.879984
2017-12-10T15:15:19.251321: step 2744, loss 0.115338, acc 0.953125, prec 0.0622654, recall 0.879984
2017-12-10T15:15:19.444677: step 2745, loss 0.293069, acc 0.890625, prec 0.0622535, recall 0.879984
2017-12-10T15:15:19.637435: step 2746, loss 0.145847, acc 0.9375, prec 0.0622467, recall 0.879984
2017-12-10T15:15:19.829584: step 2747, loss 0.16921, acc 0.90625, prec 0.0622365, recall 0.879984
2017-12-10T15:15:20.020357: step 2748, loss 0.385511, acc 0.90625, prec 0.0622519, recall 0.880031
2017-12-10T15:15:20.211763: step 2749, loss 0.320532, acc 0.859375, prec 0.0622366, recall 0.880031
2017-12-10T15:15:20.403962: step 2750, loss 0.34608, acc 0.859375, prec 0.0622726, recall 0.880124
2017-12-10T15:15:20.594963: step 2751, loss 0.192297, acc 0.921875, prec 0.062264, recall 0.880124
2017-12-10T15:15:20.792765: step 2752, loss 0.317899, acc 0.890625, prec 0.0622521, recall 0.880124
2017-12-10T15:15:20.985791: step 2753, loss 0.228006, acc 0.890625, prec 0.0622659, recall 0.88017
2017-12-10T15:15:21.181735: step 2754, loss 0.51244, acc 0.859375, prec 0.0622762, recall 0.880216
2017-12-10T15:15:21.373046: step 2755, loss 0.438954, acc 0.84375, prec 0.0623104, recall 0.880309
2017-12-10T15:15:21.570597: step 2756, loss 0.284161, acc 0.90625, prec 0.0623002, recall 0.880309
2017-12-10T15:15:21.759481: step 2757, loss 0.354681, acc 0.90625, prec 0.06229, recall 0.880309
2017-12-10T15:15:21.952911: step 2758, loss 0.190374, acc 0.96875, prec 0.0622866, recall 0.880309
2017-12-10T15:15:22.148114: step 2759, loss 0.230907, acc 0.90625, prec 0.062302, recall 0.880355
2017-12-10T15:15:22.340749: step 2760, loss 0.107591, acc 0.953125, prec 0.0623225, recall 0.880401
2017-12-10T15:15:22.531228: step 2761, loss 0.338376, acc 0.890625, prec 0.0623618, recall 0.880493
2017-12-10T15:15:22.726688: step 2762, loss 0.328251, acc 0.875, prec 0.0623482, recall 0.880493
2017-12-10T15:15:22.917759: step 2763, loss 0.207403, acc 0.90625, prec 0.0623891, recall 0.880585
2017-12-10T15:15:23.110599: step 2764, loss 0.116211, acc 0.96875, prec 0.0624113, recall 0.880632
2017-12-10T15:15:23.309412: step 2765, loss 0.182728, acc 0.9375, prec 0.0624045, recall 0.880632
2017-12-10T15:15:23.502261: step 2766, loss 0.0746775, acc 0.96875, prec 0.0624011, recall 0.880632
2017-12-10T15:15:23.700909: step 2767, loss 0.425897, acc 0.9375, prec 0.0624454, recall 0.880723
2017-12-10T15:15:23.893312: step 2768, loss 0.196542, acc 0.96875, prec 0.0624676, recall 0.880769
2017-12-10T15:15:24.088774: step 2769, loss 0.0873423, acc 0.984375, prec 0.062517, recall 0.880861
2017-12-10T15:15:24.284951: step 2770, loss 0.060623, acc 0.984375, prec 0.0625409, recall 0.880907
2017-12-10T15:15:24.480012: step 2771, loss 0.0997215, acc 0.96875, prec 0.0625631, recall 0.880952
2017-12-10T15:15:24.672089: step 2772, loss 0.10755, acc 0.96875, prec 0.0625852, recall 0.880998
2017-12-10T15:15:24.864895: step 2773, loss 0.0420309, acc 0.984375, prec 0.0625835, recall 0.880998
2017-12-10T15:15:25.058811: step 2774, loss 0.0851156, acc 0.953125, prec 0.062604, recall 0.881044
2017-12-10T15:15:25.255071: step 2775, loss 0.318875, acc 0.96875, prec 0.0626261, recall 0.881089
2017-12-10T15:15:25.447813: step 2776, loss 0.111753, acc 0.96875, prec 0.0626482, recall 0.881135
2017-12-10T15:15:25.642599: step 2777, loss 0.185271, acc 0.953125, prec 0.0626431, recall 0.881135
2017-12-10T15:15:25.837513: step 2778, loss 0.111104, acc 0.953125, prec 0.0626891, recall 0.881226
2017-12-10T15:15:26.031736: step 2779, loss 0.0703994, acc 0.953125, prec 0.062684, recall 0.881226
2017-12-10T15:15:26.227389: step 2780, loss 0.32608, acc 0.890625, prec 0.0626975, recall 0.881272
2017-12-10T15:15:26.423317: step 2781, loss 0.246121, acc 0.921875, prec 0.0627145, recall 0.881317
2017-12-10T15:15:26.620690: step 2782, loss 3.34909, acc 0.984375, prec 0.0627401, recall 0.881025
2017-12-10T15:15:26.817687: step 2783, loss 0.22653, acc 0.984375, prec 0.0627894, recall 0.881116
2017-12-10T15:15:27.011784: step 2784, loss 0.688172, acc 0.953125, prec 0.0628098, recall 0.881162
2017-12-10T15:15:27.205772: step 2785, loss 0.223772, acc 0.953125, prec 0.0628302, recall 0.881207
2017-12-10T15:15:27.397936: step 2786, loss 0.088609, acc 0.96875, prec 0.0628778, recall 0.881298
2017-12-10T15:15:27.592863: step 2787, loss 0.0771314, acc 0.984375, prec 0.0629016, recall 0.881343
2017-12-10T15:15:27.787007: step 2788, loss 0.0983517, acc 0.96875, prec 0.0629237, recall 0.881388
2017-12-10T15:15:27.980048: step 2789, loss 0.0929055, acc 0.953125, prec 0.0629441, recall 0.881433
2017-12-10T15:15:28.176464: step 2790, loss 0.124411, acc 0.9375, prec 0.0629373, recall 0.881433
2017-12-10T15:15:28.369254: step 2791, loss 0.959947, acc 0.90625, prec 0.062978, recall 0.881524
2017-12-10T15:15:28.562341: step 2792, loss 0.164178, acc 0.921875, prec 0.0629694, recall 0.881524
2017-12-10T15:15:28.759648: step 2793, loss 0.287921, acc 0.90625, prec 0.0629591, recall 0.881524
2017-12-10T15:15:28.951923: step 2794, loss 0.419692, acc 0.875, prec 0.0630219, recall 0.881659
2017-12-10T15:15:29.144472: step 2795, loss 1.33657, acc 0.921875, prec 0.0630898, recall 0.881794
2017-12-10T15:15:29.345306: step 2796, loss 0.0836783, acc 0.984375, prec 0.0630881, recall 0.881794
2017-12-10T15:15:29.538212: step 2797, loss 0.219305, acc 0.90625, prec 0.0631032, recall 0.881839
2017-12-10T15:15:29.729577: step 2798, loss 0.215985, acc 0.890625, prec 0.0631422, recall 0.881929
2017-12-10T15:15:29.924713: step 2799, loss 0.278716, acc 0.953125, prec 0.063137, recall 0.881929
2017-12-10T15:15:30.113614: step 2800, loss 0.426996, acc 0.890625, prec 0.0632014, recall 0.882063
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_1/1512936356/checkpoints/model-2800

2017-12-10T15:15:31.314500: step 2801, loss 0.235081, acc 0.9375, prec 0.0631945, recall 0.882063
2017-12-10T15:15:31.506081: step 2802, loss 0.328767, acc 0.859375, prec 0.0632045, recall 0.882108
2017-12-10T15:15:31.696962: step 2803, loss 0.430481, acc 0.890625, prec 0.0631925, recall 0.882108
2017-12-10T15:15:31.887130: step 2804, loss 0.41403, acc 0.84375, prec 0.0631753, recall 0.882108
2017-12-10T15:15:32.077860: step 2805, loss 0.334647, acc 0.890625, prec 0.0631633, recall 0.882108
2017-12-10T15:15:32.269293: step 2806, loss 0.105244, acc 0.953125, prec 0.0631836, recall 0.882152
2017-12-10T15:15:32.457487: step 2807, loss 0.17963, acc 0.9375, prec 0.0631768, recall 0.882152
2017-12-10T15:15:32.650933: step 2808, loss 0.168725, acc 0.953125, prec 0.063197, recall 0.882197
2017-12-10T15:15:32.844088: step 2809, loss 0.129765, acc 0.96875, prec 0.0632444, recall 0.882286
2017-12-10T15:15:33.035453: step 2810, loss 0.128524, acc 0.96875, prec 0.0632918, recall 0.882375
2017-12-10T15:15:33.229362: step 2811, loss 0.429404, acc 0.859375, prec 0.0633272, recall 0.882464
2017-12-10T15:15:33.418759: step 2812, loss 0.257766, acc 0.90625, prec 0.0633423, recall 0.882509
2017-12-10T15:15:33.614756: step 2813, loss 0.213779, acc 0.9375, prec 0.0633354, recall 0.882509
2017-12-10T15:15:33.806403: step 2814, loss 0.106841, acc 0.9375, prec 0.0633539, recall 0.882553
2017-12-10T15:15:33.998246: step 2815, loss 0.227155, acc 0.90625, prec 0.0633436, recall 0.882553
2017-12-10T15:15:34.193249: step 2816, loss 0.171797, acc 0.96875, prec 0.0633402, recall 0.882553
2017-12-10T15:15:34.385484: step 2817, loss 0.0791046, acc 0.96875, prec 0.0633368, recall 0.882553
2017-12-10T15:15:34.577668: step 2818, loss 0.0885694, acc 0.953125, prec 0.0633316, recall 0.882553
2017-12-10T15:15:34.770840: step 2819, loss 0.577098, acc 0.96875, prec 0.0633789, recall 0.882641
2017-12-10T15:15:34.965468: step 2820, loss 0.0174642, acc 1, prec 0.0633789, recall 0.882641
2017-12-10T15:15:35.155477: step 2821, loss 0.155881, acc 1, prec 0.0634043, recall 0.882686
2017-12-10T15:15:35.352137: step 2822, loss 0.8723, acc 1, prec 0.0635058, recall 0.882863
2017-12-10T15:15:35.545278: step 2823, loss 0.0964843, acc 0.921875, prec 0.0634972, recall 0.882863
2017-12-10T15:15:35.742960: step 2824, loss 0.35849, acc 0.9375, prec 0.0635664, recall 0.882995
2017-12-10T15:15:35.934452: step 2825, loss 0.1905, acc 0.96875, prec 0.0635884, recall 0.883039
2017-12-10T15:15:36.125780: step 2826, loss 0.135578, acc 0.9375, prec 0.0635815, recall 0.883039
2017-12-10T15:15:36.319072: step 2827, loss 0.249393, acc 0.90625, prec 0.0635711, recall 0.883039
2017-12-10T15:15:36.512003: step 2828, loss 0.475408, acc 0.953125, prec 0.063642, recall 0.883171
2017-12-10T15:15:36.708461: step 2829, loss 0.207763, acc 0.9375, prec 0.0636605, recall 0.883214
2017-12-10T15:15:36.901449: step 2830, loss 0.564094, acc 0.9375, prec 0.0637043, recall 0.883302
2017-12-10T15:15:37.095563: step 2831, loss 0.0893341, acc 0.96875, prec 0.0637008, recall 0.883302
2017-12-10T15:15:37.290418: step 2832, loss 0.247563, acc 0.953125, prec 0.0637463, recall 0.88339
2017-12-10T15:15:37.483825: step 2833, loss 0.159408, acc 0.9375, prec 0.0637394, recall 0.88339
2017-12-10T15:15:37.678271: step 2834, loss 0.138835, acc 0.96875, prec 0.0637613, recall 0.883433
2017-12-10T15:15:37.872457: step 2835, loss 0.144803, acc 0.9375, prec 0.0637544, recall 0.883433
2017-12-10T15:15:38.063166: step 2836, loss 0.343381, acc 0.859375, prec 0.0637389, recall 0.883433
2017-12-10T15:15:38.254725: step 2837, loss 0.265352, acc 0.90625, prec 0.0637285, recall 0.883433
2017-12-10T15:15:38.443726: step 2838, loss 0.214713, acc 0.890625, prec 0.0637165, recall 0.883433
2017-12-10T15:15:38.639474: step 2839, loss 0.125138, acc 0.9375, prec 0.0637349, recall 0.883477
2017-12-10T15:15:38.831124: step 2840, loss 0.32671, acc 0.890625, prec 0.0637481, recall 0.883521
2017-12-10T15:15:39.019345: step 2841, loss 1.08544, acc 0.96875, prec 0.06377, recall 0.883564
2017-12-10T15:15:39.212917: step 2842, loss 0.240318, acc 0.90625, prec 0.0637597, recall 0.883564
2017-12-10T15:15:39.404954: step 2843, loss 0.171093, acc 0.9375, prec 0.0637781, recall 0.883608
2017-12-10T15:15:39.595965: step 2844, loss 6.06673, acc 0.9375, prec 0.0637729, recall 0.883277
2017-12-10T15:15:39.793423: step 2845, loss 0.581934, acc 0.90625, prec 0.0637878, recall 0.883321
2017-12-10T15:15:39.990585: step 2846, loss 0.282234, acc 0.90625, prec 0.0637775, recall 0.883321
2017-12-10T15:15:40.184222: step 2847, loss 0.355494, acc 0.859375, prec 0.063762, recall 0.883321
2017-12-10T15:15:40.378999: step 2848, loss 0.521838, acc 0.921875, prec 0.0637534, recall 0.883321
2017-12-10T15:15:40.567915: step 2849, loss 0.420916, acc 0.859375, prec 0.0637885, recall 0.883408
2017-12-10T15:15:40.757668: step 2850, loss 0.322962, acc 0.875, prec 0.0638252, recall 0.883495
2017-12-10T15:15:40.946546: step 2851, loss 0.485888, acc 0.875, prec 0.0638114, recall 0.883495
2017-12-10T15:15:41.139884: step 2852, loss 0.423508, acc 0.859375, prec 0.0638212, recall 0.883539
2017-12-10T15:15:41.328715: step 2853, loss 0.22556, acc 0.953125, prec 0.0638413, recall 0.883582
2017-12-10T15:15:41.521764: step 2854, loss 0.580616, acc 0.84375, prec 0.0638241, recall 0.883582
2017-12-10T15:15:41.717221: step 2855, loss 0.518258, acc 0.875, prec 0.0638103, recall 0.883582
2017-12-10T15:15:41.912367: step 2856, loss 0.453487, acc 0.84375, prec 0.0637931, recall 0.883582
2017-12-10T15:15:42.101945: step 2857, loss 0.100328, acc 0.953125, prec 0.0637879, recall 0.883582
2017-12-10T15:15:42.294940: step 2858, loss 0.302338, acc 0.890625, prec 0.0638264, recall 0.883669
2017-12-10T15:15:42.485866: step 2859, loss 0.19498, acc 0.890625, prec 0.0638143, recall 0.883669
2017-12-10T15:15:42.677836: step 2860, loss 0.391778, acc 0.875, prec 0.0638006, recall 0.883669
2017-12-10T15:15:42.868984: step 2861, loss 0.110983, acc 0.96875, prec 0.0638727, recall 0.883799
2017-12-10T15:15:43.063585: step 2862, loss 0.25746, acc 0.90625, prec 0.0638876, recall 0.883842
2017-12-10T15:15:43.263836: step 2863, loss 0.284432, acc 0.9375, prec 0.0638807, recall 0.883842
2017-12-10T15:15:43.462389: step 2864, loss 1.63834, acc 0.859375, prec 0.0639174, recall 0.8836
2017-12-10T15:15:43.660327: step 2865, loss 0.446401, acc 0.859375, prec 0.0639019, recall 0.8836
2017-12-10T15:15:43.848252: step 2866, loss 0.211851, acc 0.9375, prec 0.0639454, recall 0.883686
2017-12-10T15:15:44.044702: step 2867, loss 0.297211, acc 0.921875, prec 0.0639368, recall 0.883686
2017-12-10T15:15:44.242528: step 2868, loss 0.832468, acc 0.90625, prec 0.0639768, recall 0.883773
2017-12-10T15:15:44.436237: step 2869, loss 0.20855, acc 0.875, prec 0.0640133, recall 0.883859
2017-12-10T15:15:44.631502: step 2870, loss 0.0941008, acc 0.96875, prec 0.0640099, recall 0.883859
2017-12-10T15:15:44.823505: step 2871, loss 0.42556, acc 0.875, prec 0.0639961, recall 0.883859
2017-12-10T15:15:45.019913: step 2872, loss 0.0698975, acc 0.96875, prec 0.0639927, recall 0.883859
2017-12-10T15:15:45.210347: step 2873, loss 0.0883347, acc 0.96875, prec 0.0640144, recall 0.883902
2017-12-10T15:15:45.408626: step 2874, loss 0.114367, acc 0.9375, prec 0.0640327, recall 0.883945
2017-12-10T15:15:45.599762: step 2875, loss 0.0438439, acc 0.984375, prec 0.0640561, recall 0.883988
2017-12-10T15:15:45.789126: step 2876, loss 0.331764, acc 0.921875, prec 0.0640475, recall 0.883988
2017-12-10T15:15:45.986331: step 2877, loss 0.396299, acc 0.90625, prec 0.0640874, recall 0.884074
2017-12-10T15:15:46.177603: step 2878, loss 0.0524188, acc 1, prec 0.0641125, recall 0.884117
2017-12-10T15:15:46.373735: step 2879, loss 0.177897, acc 0.953125, prec 0.0641325, recall 0.88416
2017-12-10T15:15:46.562386: step 2880, loss 0.180706, acc 0.921875, prec 0.064149, recall 0.884203
2017-12-10T15:15:46.756718: step 2881, loss 0.160819, acc 0.953125, prec 0.064169, recall 0.884246
2017-12-10T15:15:46.951482: step 2882, loss 0.3731, acc 0.875, prec 0.0641552, recall 0.884246
2017-12-10T15:15:47.146926: step 2883, loss 0.155957, acc 0.96875, prec 0.0641769, recall 0.884288
2017-12-10T15:15:47.340743: step 2884, loss 0.161083, acc 0.96875, prec 0.0641985, recall 0.884331
2017-12-10T15:15:47.531734: step 2885, loss 0.118943, acc 0.96875, prec 0.0642453, recall 0.884417
2017-12-10T15:15:47.722961: step 2886, loss 0.2147, acc 0.9375, prec 0.0642635, recall 0.884459
2017-12-10T15:15:47.919060: step 2887, loss 1.00565, acc 0.953125, prec 0.0643085, recall 0.884544
2017-12-10T15:15:48.112124: step 2888, loss 0.323874, acc 1, prec 0.0643838, recall 0.884672
2017-12-10T15:15:48.309973: step 2889, loss 0.326439, acc 0.953125, prec 0.0644037, recall 0.884715
2017-12-10T15:15:48.502966: step 2890, loss 0.128519, acc 0.9375, prec 0.0643968, recall 0.884715
2017-12-10T15:15:48.702366: step 2891, loss 0.564678, acc 0.921875, prec 0.0644132, recall 0.884757
2017-12-10T15:15:48.895241: step 2892, loss 0.11715, acc 0.953125, prec 0.064408, recall 0.884757
2017-12-10T15:15:49.088231: step 2893, loss 0.235098, acc 0.90625, prec 0.0643977, recall 0.884757
2017-12-10T15:15:49.282490: step 2894, loss 0.256356, acc 0.921875, prec 0.0644392, recall 0.884842
2017-12-10T15:15:49.475540: step 2895, loss 1.06725, acc 0.96875, prec 0.0644608, recall 0.884884
2017-12-10T15:15:49.668871: step 2896, loss 0.125679, acc 0.921875, prec 0.0644522, recall 0.884884
2017-12-10T15:15:49.861750: step 2897, loss 0.191718, acc 0.9375, prec 0.0644703, recall 0.884926
2017-12-10T15:15:50.057939: step 2898, loss 0.110679, acc 0.96875, prec 0.0644669, recall 0.884926
2017-12-10T15:15:50.252808: step 2899, loss 0.144421, acc 0.921875, prec 0.0644833, recall 0.884969
2017-12-10T15:15:50.451178: step 2900, loss 0.113566, acc 0.953125, prec 0.0645282, recall 0.885053
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_1/1512936356/checkpoints/model-2900

2017-12-10T15:15:51.631396: step 2901, loss 4.11621, acc 0.90625, prec 0.0645196, recall 0.884728
2017-12-10T15:15:51.824683: step 2902, loss 0.180308, acc 0.9375, prec 0.0645127, recall 0.884728
2017-12-10T15:15:52.017484: step 2903, loss 0.336771, acc 0.921875, prec 0.0645291, recall 0.884771
2017-12-10T15:15:52.211210: step 2904, loss 0.203207, acc 0.90625, prec 0.0645438, recall 0.884813
2017-12-10T15:15:52.404048: step 2905, loss 0.590609, acc 0.921875, prec 0.0645852, recall 0.884897
2017-12-10T15:15:52.597998: step 2906, loss 0.680057, acc 0.796875, prec 0.0646378, recall 0.885024
2017-12-10T15:15:52.790472: step 2907, loss 0.653141, acc 0.78125, prec 0.0646136, recall 0.885024
2017-12-10T15:15:52.984155: step 2908, loss 0.424658, acc 0.859375, prec 0.064623, recall 0.885066
2017-12-10T15:15:53.173452: step 2909, loss 0.754676, acc 0.8125, prec 0.0646023, recall 0.885066
2017-12-10T15:15:53.366308: step 2910, loss 0.422074, acc 0.875, prec 0.0646135, recall 0.885108
2017-12-10T15:15:53.556103: step 2911, loss 0.283526, acc 0.875, prec 0.0645997, recall 0.885108
2017-12-10T15:15:53.744660: step 2912, loss 0.727603, acc 0.8125, prec 0.064604, recall 0.88515
2017-12-10T15:15:53.941101: step 2913, loss 0.493414, acc 0.84375, prec 0.0646117, recall 0.885192
2017-12-10T15:15:54.135865: step 2914, loss 0.851511, acc 0.828125, prec 0.0646426, recall 0.885276
2017-12-10T15:15:54.332693: step 2915, loss 0.611583, acc 0.8125, prec 0.0646469, recall 0.885318
2017-12-10T15:15:54.525132: step 2916, loss 0.846539, acc 0.8125, prec 0.0646761, recall 0.885401
2017-12-10T15:15:54.720634: step 2917, loss 0.482842, acc 0.90625, prec 0.0647156, recall 0.885485
2017-12-10T15:15:54.913648: step 2918, loss 0.367742, acc 0.90625, prec 0.0648049, recall 0.885652
2017-12-10T15:15:55.111082: step 2919, loss 0.444335, acc 0.90625, prec 0.0647946, recall 0.885652
2017-12-10T15:15:55.303038: step 2920, loss 0.390518, acc 0.890625, prec 0.0647825, recall 0.885652
2017-12-10T15:15:55.494476: step 2921, loss 0.222158, acc 0.921875, prec 0.0647739, recall 0.885652
2017-12-10T15:15:55.688768: step 2922, loss 0.279465, acc 0.921875, prec 0.0647902, recall 0.885693
2017-12-10T15:15:55.878653: step 2923, loss 0.550433, acc 0.9375, prec 0.0648082, recall 0.885735
2017-12-10T15:15:56.071844: step 2924, loss 1.32656, acc 0.90625, prec 0.0648244, recall 0.885455
2017-12-10T15:15:56.269826: step 2925, loss 0.167348, acc 0.921875, prec 0.0648407, recall 0.885496
2017-12-10T15:15:56.460227: step 2926, loss 1.09415, acc 0.953125, prec 0.0648604, recall 0.885538
2017-12-10T15:15:56.655732: step 2927, loss 0.266603, acc 0.9375, prec 0.0649282, recall 0.885662
2017-12-10T15:15:56.850389: step 2928, loss 0.322315, acc 0.921875, prec 0.0649444, recall 0.885704
2017-12-10T15:15:57.045761: step 2929, loss 0.215583, acc 0.90625, prec 0.064934, recall 0.885704
2017-12-10T15:15:57.240437: step 2930, loss 0.115059, acc 0.984375, prec 0.0649323, recall 0.885704
2017-12-10T15:15:57.433558: step 2931, loss 0.259297, acc 0.90625, prec 0.0649468, recall 0.885745
2017-12-10T15:15:57.626988: step 2932, loss 0.295362, acc 0.90625, prec 0.0649364, recall 0.885745
2017-12-10T15:15:57.828158: step 2933, loss 0.271604, acc 0.90625, prec 0.064951, recall 0.885787
2017-12-10T15:15:58.018099: step 2934, loss 0.211685, acc 0.90625, prec 0.0649406, recall 0.885787
2017-12-10T15:15:58.211042: step 2935, loss 0.123077, acc 0.96875, prec 0.0649868, recall 0.88587
2017-12-10T15:15:58.404952: step 2936, loss 0.240126, acc 0.953125, prec 0.0650065, recall 0.885911
2017-12-10T15:15:58.595758: step 2937, loss 0.165286, acc 0.890625, prec 0.0649944, recall 0.885911
2017-12-10T15:15:58.785061: step 2938, loss 0.155275, acc 0.9375, prec 0.0649875, recall 0.885911
2017-12-10T15:15:58.976957: step 2939, loss 0.0816233, acc 0.953125, prec 0.0650072, recall 0.885952
2017-12-10T15:15:59.188158: step 2940, loss 0.113265, acc 0.953125, prec 0.0650517, recall 0.886035
2017-12-10T15:15:59.388248: step 2941, loss 0.0580569, acc 0.984375, prec 0.0650499, recall 0.886035
2017-12-10T15:15:59.582736: step 2942, loss 0.0536965, acc 0.984375, prec 0.065073, recall 0.886076
2017-12-10T15:15:59.779826: step 2943, loss 0.0924883, acc 0.984375, prec 0.0650961, recall 0.886117
2017-12-10T15:15:59.977055: step 2944, loss 0.0888836, acc 0.984375, prec 0.0651192, recall 0.886158
2017-12-10T15:16:00.169382: step 2945, loss 0.146598, acc 0.953125, prec 0.0651141, recall 0.886158
2017-12-10T15:16:00.363271: step 2946, loss 0.31226, acc 0.96875, prec 0.0651354, recall 0.886199
2017-12-10T15:16:00.558124: step 2947, loss 0.0460705, acc 0.96875, prec 0.065132, recall 0.886199
2017-12-10T15:16:00.755675: step 2948, loss 0.09293, acc 0.984375, prec 0.0651551, recall 0.886241
2017-12-10T15:16:00.957105: step 2949, loss 0.0762043, acc 0.96875, prec 0.0651516, recall 0.886241
2017-12-10T15:16:01.156832: step 2950, loss 0.0649386, acc 0.984375, prec 0.0651499, recall 0.886241
2017-12-10T15:16:01.354475: step 2951, loss 0.121908, acc 0.96875, prec 0.0651464, recall 0.886241
2017-12-10T15:16:01.549643: step 2952, loss 2.83621, acc 0.90625, prec 0.0651378, recall 0.885921
2017-12-10T15:16:01.749799: step 2953, loss 0.769827, acc 0.96875, prec 0.0651839, recall 0.886003
2017-12-10T15:16:01.948350: step 2954, loss 0.469098, acc 0.96875, prec 0.0652301, recall 0.886085
2017-12-10T15:16:02.140591: step 2955, loss 0.0544586, acc 0.984375, prec 0.0652532, recall 0.886126
2017-12-10T15:16:02.334220: step 2956, loss 0.10536, acc 0.953125, prec 0.065248, recall 0.886126
2017-12-10T15:16:02.529435: step 2957, loss 0.148434, acc 0.90625, prec 0.0652872, recall 0.886208
2017-12-10T15:16:02.725474: step 2958, loss 0.311103, acc 0.90625, prec 0.0653016, recall 0.886249
2017-12-10T15:16:02.920463: step 2959, loss 0.467338, acc 0.875, prec 0.0653621, recall 0.886372
2017-12-10T15:16:03.110586: step 2960, loss 0.491313, acc 0.90625, prec 0.0653765, recall 0.886413
2017-12-10T15:16:03.305802: step 2961, loss 0.284937, acc 0.9375, prec 0.0653943, recall 0.886453
2017-12-10T15:16:03.501015: step 2962, loss 0.442367, acc 0.859375, prec 0.0654282, recall 0.886535
2017-12-10T15:16:03.688336: step 2963, loss 0.181237, acc 0.9375, prec 0.0654213, recall 0.886535
2017-12-10T15:16:03.879388: step 2964, loss 0.759607, acc 0.796875, prec 0.0654483, recall 0.886616
2017-12-10T15:16:04.074141: step 2965, loss 0.432781, acc 0.921875, prec 0.0654644, recall 0.886657
2017-12-10T15:16:04.267029: step 2966, loss 0.253882, acc 0.875, prec 0.0654505, recall 0.886657
2017-12-10T15:16:04.461236: step 2967, loss 0.5846, acc 0.890625, prec 0.0654878, recall 0.886738
2017-12-10T15:16:04.651858: step 2968, loss 0.750304, acc 0.96875, prec 0.0655091, recall 0.886779
2017-12-10T15:16:04.846768: step 2969, loss 0.89351, acc 0.9375, prec 0.0655269, recall 0.886819
2017-12-10T15:16:05.039362: step 2970, loss 0.251594, acc 0.875, prec 0.065513, recall 0.886819
2017-12-10T15:16:05.227937: step 2971, loss 0.394961, acc 0.84375, prec 0.0654957, recall 0.886819
2017-12-10T15:16:05.423222: step 2972, loss 0.372428, acc 0.921875, prec 0.0655365, recall 0.8869
2017-12-10T15:16:05.613693: step 2973, loss 0.454619, acc 0.859375, prec 0.0655456, recall 0.886941
2017-12-10T15:16:05.801596: step 2974, loss 0.497623, acc 0.921875, prec 0.0655616, recall 0.886981
2017-12-10T15:16:05.996032: step 2975, loss 0.381944, acc 0.828125, prec 0.0656167, recall 0.887103
2017-12-10T15:16:06.185557: step 2976, loss 0.39045, acc 0.84375, prec 0.065624, recall 0.887143
2017-12-10T15:16:06.376375: step 2977, loss 0.258226, acc 0.953125, prec 0.0656188, recall 0.887143
2017-12-10T15:16:06.565377: step 2978, loss 0.154648, acc 0.921875, prec 0.0656101, recall 0.887143
2017-12-10T15:16:06.754953: step 2979, loss 0.22833, acc 0.9375, prec 0.0656279, recall 0.887183
2017-12-10T15:16:06.952190: step 2980, loss 0.316604, acc 0.890625, prec 0.0656158, recall 0.887183
2017-12-10T15:16:07.140853: step 2981, loss 0.260033, acc 0.9375, prec 0.0656088, recall 0.887183
2017-12-10T15:16:07.315644: step 2982, loss 0.178633, acc 0.901961, prec 0.0656248, recall 0.887223
2017-12-10T15:16:07.514972: step 2983, loss 0.262045, acc 0.859375, prec 0.0656092, recall 0.887223
2017-12-10T15:16:07.713218: step 2984, loss 0.234964, acc 0.890625, prec 0.0655971, recall 0.887223
2017-12-10T15:16:07.905242: step 2985, loss 0.210215, acc 0.9375, prec 0.0656149, recall 0.887264
2017-12-10T15:16:08.097347: step 2986, loss 0.662316, acc 0.90625, prec 0.0656291, recall 0.887304
2017-12-10T15:16:08.291673: step 2987, loss 0.202483, acc 0.921875, prec 0.0656698, recall 0.887384
2017-12-10T15:16:08.482113: step 2988, loss 4.14309, acc 0.9375, prec 0.0656892, recall 0.887108
2017-12-10T15:16:08.683610: step 2989, loss 0.208671, acc 0.890625, prec 0.0657017, recall 0.887148
2017-12-10T15:16:08.882356: step 2990, loss 0.390886, acc 0.875, prec 0.0657125, recall 0.887189
2017-12-10T15:16:09.070627: step 2991, loss 0.279911, acc 0.921875, prec 0.0657038, recall 0.887189
2017-12-10T15:16:09.265386: step 2992, loss 0.286017, acc 0.90625, prec 0.0657427, recall 0.887269
2017-12-10T15:16:09.455238: step 2993, loss 0.281182, acc 0.90625, prec 0.0657569, recall 0.887309
2017-12-10T15:16:09.652237: step 2994, loss 0.174703, acc 0.9375, prec 0.0657746, recall 0.887349
2017-12-10T15:16:09.841609: step 2995, loss 0.241869, acc 0.921875, prec 0.0658151, recall 0.887429
2017-12-10T15:16:10.033334: step 2996, loss 0.11161, acc 0.984375, prec 0.0658626, recall 0.887509
2017-12-10T15:16:10.224067: step 2997, loss 0.490107, acc 0.78125, prec 0.0659121, recall 0.887628
2017-12-10T15:16:10.419386: step 2998, loss 0.364567, acc 0.90625, prec 0.0659263, recall 0.887668
2017-12-10T15:16:10.612445: step 2999, loss 0.196793, acc 0.953125, prec 0.0659456, recall 0.887708
2017-12-10T15:16:10.805606: step 3000, loss 0.234734, acc 0.890625, prec 0.0659581, recall 0.887748
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_1/1512936356/checkpoints/model-3000

2017-12-10T15:16:12.081852: step 3001, loss 0.508683, acc 0.859375, prec 0.065967, recall 0.887788
2017-12-10T15:16:12.274777: step 3002, loss 0.348501, acc 0.875, prec 0.0660023, recall 0.887867
2017-12-10T15:16:12.467740: step 3003, loss 0.304804, acc 0.921875, prec 0.0659936, recall 0.887867
2017-12-10T15:16:12.659179: step 3004, loss 0.381372, acc 0.890625, prec 0.066006, recall 0.887907
2017-12-10T15:16:12.850893: step 3005, loss 0.184486, acc 0.953125, prec 0.0660008, recall 0.887907
2017-12-10T15:16:13.043298: step 3006, loss 1.02172, acc 0.921875, prec 0.0660412, recall 0.887986
2017-12-10T15:16:13.238343: step 3007, loss 0.281884, acc 0.90625, prec 0.0660308, recall 0.887986
2017-12-10T15:16:13.434044: step 3008, loss 0.223146, acc 0.921875, prec 0.0660712, recall 0.888065
2017-12-10T15:16:13.625835: step 3009, loss 0.344898, acc 0.921875, prec 0.0661116, recall 0.888144
2017-12-10T15:16:13.823653: step 3010, loss 0.128682, acc 0.921875, prec 0.0661029, recall 0.888144
2017-12-10T15:16:14.017071: step 3011, loss 0.138801, acc 0.953125, prec 0.0661222, recall 0.888183
2017-12-10T15:16:14.215630: step 3012, loss 0.222594, acc 0.921875, prec 0.0661871, recall 0.888302
2017-12-10T15:16:14.412982: step 3013, loss 0.110497, acc 0.9375, prec 0.0661801, recall 0.888302
2017-12-10T15:16:14.605214: step 3014, loss 0.258572, acc 0.921875, prec 0.066196, recall 0.888341
2017-12-10T15:16:14.803405: step 3015, loss 0.136111, acc 1, prec 0.066245, recall 0.88842
2017-12-10T15:16:14.999170: step 3016, loss 0.236938, acc 0.9375, prec 0.0662625, recall 0.888459
2017-12-10T15:16:15.189757: step 3017, loss 0.0625801, acc 0.96875, prec 0.0662591, recall 0.888459
2017-12-10T15:16:15.388053: step 3018, loss 0.117134, acc 0.9375, prec 0.0662766, recall 0.888498
2017-12-10T15:16:15.582212: step 3019, loss 0.18403, acc 1, prec 0.0663011, recall 0.888537
2017-12-10T15:16:15.775592: step 3020, loss 0.0497498, acc 0.984375, prec 0.0662994, recall 0.888537
2017-12-10T15:16:15.969797: step 3021, loss 0.170153, acc 0.984375, prec 0.0663221, recall 0.888576
2017-12-10T15:16:16.163139: step 3022, loss 0.110333, acc 0.96875, prec 0.0663431, recall 0.888616
2017-12-10T15:16:16.356739: step 3023, loss 0.170315, acc 0.953125, prec 0.0663624, recall 0.888655
2017-12-10T15:16:16.546232: step 3024, loss 0.106773, acc 0.953125, prec 0.0664061, recall 0.888733
2017-12-10T15:16:16.738758: step 3025, loss 0.181026, acc 0.96875, prec 0.0664027, recall 0.888733
2017-12-10T15:16:16.932325: step 3026, loss 0.33227, acc 0.96875, prec 0.0664237, recall 0.888772
2017-12-10T15:16:17.130425: step 3027, loss 0.530828, acc 0.96875, prec 0.0664936, recall 0.888889
2017-12-10T15:16:17.326508: step 3028, loss 0.153725, acc 0.9375, prec 0.0665111, recall 0.888928
2017-12-10T15:16:17.520959: step 3029, loss 0.0572527, acc 0.96875, prec 0.0665076, recall 0.888928
2017-12-10T15:16:17.714428: step 3030, loss 0.115543, acc 1, prec 0.0665321, recall 0.888967
2017-12-10T15:16:17.910716: step 3031, loss 2.7008, acc 0.953125, prec 0.0665775, recall 0.888733
2017-12-10T15:16:18.106584: step 3032, loss 1.09236, acc 0.9375, prec 0.0666195, recall 0.888811
2017-12-10T15:16:18.298439: step 3033, loss 0.157941, acc 0.953125, prec 0.0666387, recall 0.88885
2017-12-10T15:16:18.492970: step 3034, loss 0.345078, acc 0.9375, prec 0.0667295, recall 0.889005
2017-12-10T15:16:18.687812: step 3035, loss 0.563551, acc 0.90625, prec 0.0667435, recall 0.889044
2017-12-10T15:16:18.881486: step 3036, loss 0.375847, acc 0.921875, prec 0.0667592, recall 0.889083
2017-12-10T15:16:19.072974: step 3037, loss 0.18056, acc 0.9375, prec 0.0667522, recall 0.889083
2017-12-10T15:16:19.264211: step 3038, loss 0.387354, acc 0.859375, prec 0.0667365, recall 0.889083
2017-12-10T15:16:19.454434: step 3039, loss 0.589554, acc 0.78125, prec 0.0667609, recall 0.88916
2017-12-10T15:16:19.645028: step 3040, loss 0.539305, acc 0.78125, prec 0.0667364, recall 0.88916
2017-12-10T15:16:19.840454: step 3041, loss 0.510635, acc 0.84375, prec 0.0667922, recall 0.889276
2017-12-10T15:16:20.031732: step 3042, loss 0.206192, acc 0.921875, prec 0.0667835, recall 0.889276
2017-12-10T15:16:20.226246: step 3043, loss 0.705697, acc 0.828125, prec 0.0667887, recall 0.889314
2017-12-10T15:16:20.419533: step 3044, loss 0.603097, acc 0.8125, prec 0.0667677, recall 0.889314
2017-12-10T15:16:20.614738: step 3045, loss 0.614515, acc 0.875, prec 0.0667781, recall 0.889353
2017-12-10T15:16:20.806610: step 3046, loss 0.542331, acc 0.8125, prec 0.0667816, recall 0.889391
2017-12-10T15:16:20.999408: step 3047, loss 0.267406, acc 0.90625, prec 0.0668199, recall 0.889468
2017-12-10T15:16:21.192490: step 3048, loss 0.671114, acc 0.859375, prec 0.0668042, recall 0.889468
2017-12-10T15:16:21.382728: step 3049, loss 0.443251, acc 0.765625, prec 0.066778, recall 0.889468
2017-12-10T15:16:21.575248: step 3050, loss 0.199784, acc 0.9375, prec 0.066771, recall 0.889468
2017-12-10T15:16:21.769168: step 3051, loss 0.469225, acc 0.90625, prec 0.0667849, recall 0.889507
2017-12-10T15:16:21.969210: step 3052, loss 0.294615, acc 0.921875, prec 0.0668006, recall 0.889545
2017-12-10T15:16:22.166255: step 3053, loss 0.176876, acc 0.921875, prec 0.0668405, recall 0.889622
2017-12-10T15:16:22.366271: step 3054, loss 0.28766, acc 0.875, prec 0.0668752, recall 0.889698
2017-12-10T15:16:22.561793: step 3055, loss 0.274323, acc 0.90625, prec 0.0668891, recall 0.889736
2017-12-10T15:16:22.756978: step 3056, loss 0.400886, acc 0.90625, prec 0.0669273, recall 0.889813
2017-12-10T15:16:22.948502: step 3057, loss 0.263688, acc 0.984375, prec 0.0669499, recall 0.889851
2017-12-10T15:16:23.148226: step 3058, loss 0.354509, acc 0.90625, prec 0.0670123, recall 0.889965
2017-12-10T15:16:23.344771: step 3059, loss 0.315371, acc 0.984375, prec 0.0670349, recall 0.890003
2017-12-10T15:16:23.537797: step 3060, loss 0.062742, acc 0.984375, prec 0.0670574, recall 0.890041
2017-12-10T15:16:23.731296: step 3061, loss 0.117019, acc 0.9375, prec 0.0670748, recall 0.890079
2017-12-10T15:16:23.926841: step 3062, loss 0.733139, acc 0.875, prec 0.0670851, recall 0.890117
2017-12-10T15:16:24.122834: step 3063, loss 0.246548, acc 0.96875, prec 0.0671545, recall 0.890231
2017-12-10T15:16:24.315741: step 3064, loss 0.0705175, acc 0.984375, prec 0.0671527, recall 0.890231
2017-12-10T15:16:24.510754: step 3065, loss 0.118052, acc 0.953125, prec 0.0671475, recall 0.890231
2017-12-10T15:16:24.706979: step 3066, loss 0.0907137, acc 0.984375, prec 0.06717, recall 0.890269
2017-12-10T15:16:24.901214: step 3067, loss 0.749341, acc 0.953125, prec 0.0672133, recall 0.890345
2017-12-10T15:16:25.104412: step 3068, loss 0.0797082, acc 0.96875, prec 0.0672098, recall 0.890345
2017-12-10T15:16:25.295694: step 3069, loss 0.20189, acc 0.9375, prec 0.0672028, recall 0.890345
2017-12-10T15:16:25.487490: step 3070, loss 0.0847955, acc 0.953125, prec 0.0671976, recall 0.890345
2017-12-10T15:16:25.685855: step 3071, loss 0.184921, acc 0.9375, prec 0.0672149, recall 0.890383
2017-12-10T15:16:25.881796: step 3072, loss 0.198381, acc 0.90625, prec 0.0672044, recall 0.890383
2017-12-10T15:16:26.074083: step 3073, loss 0.10374, acc 0.953125, prec 0.0672234, recall 0.89042
2017-12-10T15:16:26.264782: step 3074, loss 0.0972074, acc 0.96875, prec 0.0672442, recall 0.890458
2017-12-10T15:16:26.457785: step 3075, loss 0.159118, acc 0.96875, prec 0.0672649, recall 0.890496
2017-12-10T15:16:26.648052: step 3076, loss 0.0154875, acc 1, prec 0.0672649, recall 0.890496
2017-12-10T15:16:26.841681: step 3077, loss 0.103085, acc 0.9375, prec 0.0672579, recall 0.890496
2017-12-10T15:16:27.034302: step 3078, loss 0.345229, acc 0.96875, prec 0.0673272, recall 0.890609
2017-12-10T15:16:27.226489: step 3079, loss 0.277548, acc 0.953125, prec 0.0673704, recall 0.890684
2017-12-10T15:16:27.419098: step 3080, loss 0.0574794, acc 0.984375, prec 0.0673929, recall 0.890722
2017-12-10T15:16:27.613522: step 3081, loss 0.192272, acc 0.9375, prec 0.0673859, recall 0.890722
2017-12-10T15:16:27.804992: step 3082, loss 0.62746, acc 0.9375, prec 0.0674032, recall 0.890759
2017-12-10T15:16:27.998082: step 3083, loss 0.171407, acc 0.96875, prec 0.0674239, recall 0.890797
2017-12-10T15:16:28.191729: step 3084, loss 1.10158, acc 1, prec 0.0674724, recall 0.890872
2017-12-10T15:16:28.390190: step 3085, loss 0.120596, acc 0.9375, prec 0.0674654, recall 0.890872
2017-12-10T15:16:28.587100: step 3086, loss 0.287853, acc 0.90625, prec 0.0674549, recall 0.890872
2017-12-10T15:16:28.779758: step 3087, loss 0.102411, acc 0.96875, prec 0.0674514, recall 0.890872
2017-12-10T15:16:28.972601: step 3088, loss 0.0419168, acc 0.984375, prec 0.0674496, recall 0.890872
2017-12-10T15:16:29.171662: step 3089, loss 0.268542, acc 0.921875, prec 0.0674893, recall 0.890947
2017-12-10T15:16:29.368697: step 3090, loss 0.24373, acc 0.921875, prec 0.067529, recall 0.891021
2017-12-10T15:16:29.561474: step 3091, loss 0.0793233, acc 0.984375, prec 0.0675756, recall 0.891096
2017-12-10T15:16:29.757296: step 3092, loss 0.876805, acc 0.96875, prec 0.0676448, recall 0.891208
2017-12-10T15:16:29.949142: step 3093, loss 0.0762577, acc 0.96875, prec 0.0676413, recall 0.891208
2017-12-10T15:16:30.142325: step 3094, loss 0.233927, acc 0.90625, prec 0.0676791, recall 0.891282
2017-12-10T15:16:30.332203: step 3095, loss 0.26001, acc 0.921875, prec 0.0676945, recall 0.891319
2017-12-10T15:16:30.527474: step 3096, loss 0.185655, acc 0.921875, prec 0.0676858, recall 0.891319
2017-12-10T15:16:30.720033: step 3097, loss 0.21528, acc 0.96875, prec 0.0677064, recall 0.891356
2017-12-10T15:16:30.913584: step 3098, loss 0.265254, acc 0.96875, prec 0.0677271, recall 0.891393
2017-12-10T15:16:31.113888: step 3099, loss 0.158637, acc 0.953125, prec 0.0677218, recall 0.891393
2017-12-10T15:16:31.315590: step 3100, loss 0.303198, acc 0.90625, prec 0.0677355, recall 0.89143
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_1/1512936356/checkpoints/model-3100

2017-12-10T15:16:32.499062: step 3101, loss 0.377766, acc 0.90625, prec 0.0677733, recall 0.891505
2017-12-10T15:16:32.697462: step 3102, loss 0.226239, acc 0.90625, prec 0.0677628, recall 0.891505
2017-12-10T15:16:32.892113: step 3103, loss 0.10515, acc 0.96875, prec 0.0677834, recall 0.891542
2017-12-10T15:16:33.086379: step 3104, loss 2.19038, acc 0.96875, prec 0.06783, recall 0.891312
2017-12-10T15:16:33.283664: step 3105, loss 0.111138, acc 0.953125, prec 0.0678489, recall 0.891349
2017-12-10T15:16:33.481363: step 3106, loss 0.071432, acc 0.984375, prec 0.0678471, recall 0.891349
2017-12-10T15:16:33.677357: step 3107, loss 0.512324, acc 0.84375, prec 0.0678537, recall 0.891386
2017-12-10T15:16:33.867755: step 3108, loss 0.36918, acc 0.953125, prec 0.0678726, recall 0.891423
2017-12-10T15:16:34.063345: step 3109, loss 0.254181, acc 0.890625, prec 0.0678603, recall 0.891423
2017-12-10T15:16:34.257671: step 3110, loss 0.31778, acc 0.9375, prec 0.0678774, recall 0.89146
2017-12-10T15:16:34.450461: step 3111, loss 0.209622, acc 0.9375, prec 0.0678704, recall 0.89146
2017-12-10T15:16:34.646756: step 3112, loss 0.144216, acc 0.90625, prec 0.067884, recall 0.891497
2017-12-10T15:16:34.841882: step 3113, loss 0.361258, acc 0.859375, prec 0.0678923, recall 0.891533
2017-12-10T15:16:35.032387: step 3114, loss 0.096738, acc 0.96875, prec 0.0679129, recall 0.89157
2017-12-10T15:16:35.225898: step 3115, loss 0.243102, acc 0.9375, prec 0.0679541, recall 0.891644
2017-12-10T15:16:35.418241: step 3116, loss 0.212541, acc 0.953125, prec 0.0679971, recall 0.891718
2017-12-10T15:16:35.613944: step 3117, loss 0.305181, acc 0.90625, prec 0.0680348, recall 0.891791
2017-12-10T15:16:35.806874: step 3118, loss 0.347265, acc 0.953125, prec 0.0680536, recall 0.891828
2017-12-10T15:16:36.004936: step 3119, loss 0.276223, acc 0.890625, prec 0.0680413, recall 0.891828
2017-12-10T15:16:36.204644: step 3120, loss 0.39027, acc 0.90625, prec 0.0680548, recall 0.891864
2017-12-10T15:16:36.394570: step 3121, loss 0.552263, acc 0.890625, prec 0.0680666, recall 0.891901
2017-12-10T15:16:36.587951: step 3122, loss 0.133487, acc 0.9375, prec 0.0680837, recall 0.891938
2017-12-10T15:16:36.777520: step 3123, loss 0.442132, acc 0.9375, prec 0.0681007, recall 0.891974
2017-12-10T15:16:36.975440: step 3124, loss 0.119062, acc 0.96875, prec 0.0681213, recall 0.892011
2017-12-10T15:16:37.164848: step 3125, loss 0.292915, acc 0.9375, prec 0.0681624, recall 0.892084
2017-12-10T15:16:37.362336: step 3126, loss 0.171012, acc 0.9375, prec 0.0681554, recall 0.892084
2017-12-10T15:16:37.556802: step 3127, loss 0.604873, acc 0.9375, prec 0.0681724, recall 0.89212
2017-12-10T15:16:37.750073: step 3128, loss 0.166855, acc 0.96875, prec 0.0682171, recall 0.892193
2017-12-10T15:16:37.949767: step 3129, loss 0.106395, acc 0.96875, prec 0.0682135, recall 0.892193
2017-12-10T15:16:38.142399: step 3130, loss 0.0864393, acc 0.953125, prec 0.0682323, recall 0.89223
2017-12-10T15:16:38.338909: step 3131, loss 0.130307, acc 0.953125, prec 0.068227, recall 0.89223
2017-12-10T15:16:38.535950: step 3132, loss 0.136903, acc 0.96875, prec 0.0682235, recall 0.89223
2017-12-10T15:16:38.726706: step 3133, loss 0.194928, acc 0.953125, prec 0.0682182, recall 0.89223
2017-12-10T15:16:38.916844: step 3134, loss 0.130673, acc 0.96875, prec 0.0682388, recall 0.892266
2017-12-10T15:16:39.111216: step 3135, loss 0.129928, acc 0.9375, prec 0.0682558, recall 0.892303
2017-12-10T15:16:39.306130: step 3136, loss 0.172474, acc 0.953125, prec 0.0682505, recall 0.892303
2017-12-10T15:16:39.501181: step 3137, loss 0.302957, acc 0.90625, prec 0.068264, recall 0.892339
2017-12-10T15:16:39.697068: step 3138, loss 0.156347, acc 0.984375, prec 0.0683103, recall 0.892411
2017-12-10T15:16:39.896596: step 3139, loss 0.0854574, acc 0.953125, prec 0.068305, recall 0.892411
2017-12-10T15:16:40.093534: step 3140, loss 0.179202, acc 0.9375, prec 0.068322, recall 0.892448
2017-12-10T15:16:40.284753: step 3141, loss 0.151892, acc 0.953125, prec 0.0683408, recall 0.892484
2017-12-10T15:16:40.481519: step 3142, loss 0.0685845, acc 0.984375, prec 0.068339, recall 0.892484
2017-12-10T15:16:40.672931: step 3143, loss 0.221113, acc 0.9375, prec 0.068332, recall 0.892484
2017-12-10T15:16:40.870185: step 3144, loss 0.114578, acc 0.96875, prec 0.0683284, recall 0.892484
2017-12-10T15:16:41.067298: step 3145, loss 0.0431156, acc 0.984375, prec 0.0683747, recall 0.892556
2017-12-10T15:16:41.260605: step 3146, loss 0.297081, acc 0.9375, prec 0.0683917, recall 0.892593
2017-12-10T15:16:41.454563: step 3147, loss 0.111539, acc 0.96875, prec 0.0684122, recall 0.892629
2017-12-10T15:16:41.651776: step 3148, loss 0.0422645, acc 0.984375, prec 0.0684105, recall 0.892629
2017-12-10T15:16:41.846850: step 3149, loss 0.799217, acc 0.984375, prec 0.0684568, recall 0.892701
2017-12-10T15:16:42.047963: step 3150, loss 0.108993, acc 0.984375, prec 0.068455, recall 0.892701
2017-12-10T15:16:42.239484: step 3151, loss 0.0230277, acc 1, prec 0.068479, recall 0.892737
2017-12-10T15:16:42.431148: step 3152, loss 0.124398, acc 0.96875, prec 0.0684995, recall 0.892773
2017-12-10T15:16:42.625930: step 3153, loss 0.16037, acc 0.96875, prec 0.06852, recall 0.892809
2017-12-10T15:16:42.821409: step 3154, loss 0.125175, acc 0.984375, prec 0.0685663, recall 0.892881
2017-12-10T15:16:43.015641: step 3155, loss 0.167241, acc 0.984375, prec 0.0686125, recall 0.892953
2017-12-10T15:16:43.212425: step 3156, loss 0.118454, acc 0.984375, prec 0.0686108, recall 0.892953
2017-12-10T15:16:43.407146: step 3157, loss 0.133494, acc 0.96875, prec 0.0686553, recall 0.893025
2017-12-10T15:16:43.597887: step 3158, loss 0.0765783, acc 0.984375, prec 0.0686535, recall 0.893025
2017-12-10T15:16:43.796186: step 3159, loss 0.081331, acc 0.96875, prec 0.068674, recall 0.893061
2017-12-10T15:16:43.992168: step 3160, loss 0.163532, acc 0.953125, prec 0.0686686, recall 0.893061
2017-12-10T15:16:44.190640: step 3161, loss 0.413815, acc 1, prec 0.0687647, recall 0.893204
2017-12-10T15:16:44.391730: step 3162, loss 0.0483462, acc 0.96875, prec 0.0687611, recall 0.893204
2017-12-10T15:16:44.588392: step 3163, loss 0.149109, acc 0.9375, prec 0.068802, recall 0.893275
2017-12-10T15:16:44.778657: step 3164, loss 0.256985, acc 0.96875, prec 0.0688225, recall 0.893311
2017-12-10T15:16:44.979222: step 3165, loss 0.483076, acc 0.9375, prec 0.0688634, recall 0.893382
2017-12-10T15:16:45.174075: step 3166, loss 0.0207155, acc 1, prec 0.0688634, recall 0.893382
2017-12-10T15:16:45.367576: step 3167, loss 0.228091, acc 0.96875, prec 0.0689318, recall 0.893489
2017-12-10T15:16:45.557955: step 3168, loss 0.0576595, acc 0.96875, prec 0.0689762, recall 0.89356
2017-12-10T15:16:45.750853: step 3169, loss 0.468367, acc 0.890625, prec 0.0689637, recall 0.89356
2017-12-10T15:16:45.944212: step 3170, loss 0.173507, acc 0.96875, prec 0.0689842, recall 0.893596
2017-12-10T15:16:46.137915: step 3171, loss 0.109519, acc 0.96875, prec 0.0690046, recall 0.893631
2017-12-10T15:16:46.333413: step 3172, loss 0.24029, acc 0.953125, prec 0.0690472, recall 0.893702
2017-12-10T15:16:46.529010: step 3173, loss 0.0838955, acc 0.96875, prec 0.0690916, recall 0.893773
2017-12-10T15:16:46.721947: step 3174, loss 0.0566618, acc 0.984375, prec 0.0690898, recall 0.893773
2017-12-10T15:16:46.920276: step 3175, loss 0.177692, acc 0.9375, prec 0.0690827, recall 0.893773
2017-12-10T15:16:47.112691: step 3176, loss 0.0459623, acc 1, prec 0.0690827, recall 0.893773
2017-12-10T15:16:47.303610: step 3177, loss 0.117783, acc 0.953125, prec 0.0691013, recall 0.893808
2017-12-10T15:16:47.497372: step 3178, loss 0.129468, acc 0.9375, prec 0.0690942, recall 0.893808
2017-12-10T15:16:47.697005: step 3179, loss 2.19663, acc 0.96875, prec 0.0691643, recall 0.893617
2017-12-10T15:16:47.894940: step 3180, loss 0.166227, acc 0.921875, prec 0.0691793, recall 0.893652
2017-12-10T15:16:48.090754: step 3181, loss 0.160329, acc 0.96875, prec 0.0691997, recall 0.893688
2017-12-10T15:16:48.283651: step 3182, loss 0.331028, acc 0.890625, prec 0.0692112, recall 0.893723
2017-12-10T15:16:48.484701: step 3183, loss 0.306193, acc 0.890625, prec 0.0691987, recall 0.893723
2017-12-10T15:16:48.678441: step 3184, loss 0.195456, acc 0.90625, prec 0.0691881, recall 0.893723
2017-12-10T15:16:48.876535: step 3185, loss 1.4109, acc 0.90625, prec 0.069227, recall 0.893497
2017-12-10T15:16:49.071902: step 3186, loss 0.24593, acc 0.890625, prec 0.0692146, recall 0.893497
2017-12-10T15:16:49.268842: step 3187, loss 0.395684, acc 0.9375, prec 0.0692553, recall 0.893568
2017-12-10T15:16:49.465867: step 3188, loss 0.317155, acc 0.84375, prec 0.0692375, recall 0.893568
2017-12-10T15:16:49.659084: step 3189, loss 0.31727, acc 0.875, prec 0.0692233, recall 0.893568
2017-12-10T15:16:49.854292: step 3190, loss 0.364665, acc 0.890625, prec 0.0692108, recall 0.893568
2017-12-10T15:16:50.046317: step 3191, loss 0.581779, acc 0.765625, prec 0.0691842, recall 0.893568
2017-12-10T15:16:50.241228: step 3192, loss 0.616265, acc 0.828125, prec 0.0691646, recall 0.893568
2017-12-10T15:16:50.431011: step 3193, loss 0.289346, acc 0.890625, prec 0.0691522, recall 0.893568
2017-12-10T15:16:50.626324: step 3194, loss 0.469336, acc 0.859375, prec 0.0691362, recall 0.893568
2017-12-10T15:16:50.819814: step 3195, loss 0.290223, acc 0.9375, prec 0.069153, recall 0.893603
2017-12-10T15:16:51.010319: step 3196, loss 0.495777, acc 0.875, prec 0.0691866, recall 0.893673
2017-12-10T15:16:51.204044: step 3197, loss 0.435789, acc 0.875, prec 0.0691724, recall 0.893673
2017-12-10T15:16:51.399743: step 3198, loss 0.238225, acc 0.90625, prec 0.0691618, recall 0.893673
2017-12-10T15:16:51.591497: step 3199, loss 0.191134, acc 0.90625, prec 0.0691511, recall 0.893673
2017-12-10T15:16:51.786766: step 3200, loss 0.277683, acc 0.875, prec 0.0691369, recall 0.893673
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_1/1512936356/checkpoints/model-3200

2017-12-10T15:16:53.104877: step 3201, loss 0.233932, acc 0.9375, prec 0.0691299, recall 0.893673
2017-12-10T15:16:53.298308: step 3202, loss 0.232315, acc 0.921875, prec 0.0691448, recall 0.893709
2017-12-10T15:16:53.490753: step 3203, loss 0.435717, acc 0.90625, prec 0.0691342, recall 0.893709
2017-12-10T15:16:53.688184: step 3204, loss 0.019494, acc 1, prec 0.0691581, recall 0.893744
2017-12-10T15:16:53.885836: step 3205, loss 0.233054, acc 0.921875, prec 0.0692446, recall 0.893884
2017-12-10T15:16:54.083249: step 3206, loss 0.0927572, acc 0.984375, prec 0.0692428, recall 0.893884
2017-12-10T15:16:54.279615: step 3207, loss 0.135952, acc 0.9375, prec 0.0692357, recall 0.893884
2017-12-10T15:16:54.473110: step 3208, loss 0.986764, acc 0.9375, prec 0.0692524, recall 0.893919
2017-12-10T15:16:54.670678: step 3209, loss 0.136072, acc 0.96875, prec 0.0692727, recall 0.893954
2017-12-10T15:16:54.866053: step 3210, loss 0.145932, acc 0.953125, prec 0.0692912, recall 0.893989
2017-12-10T15:16:55.058696: step 3211, loss 0.0197628, acc 1, prec 0.069315, recall 0.894024
2017-12-10T15:16:55.252100: step 3212, loss 0.103775, acc 0.96875, prec 0.0693353, recall 0.894059
2017-12-10T15:16:55.447562: step 3213, loss 0.257941, acc 0.96875, prec 0.0693318, recall 0.894059
2017-12-10T15:16:55.643062: step 3214, loss 0.668746, acc 0.96875, prec 0.069352, recall 0.894094
2017-12-10T15:16:55.841638: step 3215, loss 0.0124712, acc 1, prec 0.069352, recall 0.894094
2017-12-10T15:16:56.035965: step 3216, loss 0.217329, acc 0.9375, prec 0.0693449, recall 0.894094
2017-12-10T15:16:56.227558: step 3217, loss 0.0351045, acc 1, prec 0.0693687, recall 0.894129
2017-12-10T15:16:56.421272: step 3218, loss 0.100187, acc 0.96875, prec 0.0693652, recall 0.894129
2017-12-10T15:16:56.618710: step 3219, loss 0.320015, acc 0.96875, prec 0.0693855, recall 0.894164
2017-12-10T15:16:56.820121: step 3220, loss 0.0209938, acc 1, prec 0.0693855, recall 0.894164
2017-12-10T15:16:57.014411: step 3221, loss 0.202783, acc 0.9375, prec 0.069426, recall 0.894234
2017-12-10T15:16:57.210104: step 3222, loss 0.0755043, acc 0.96875, prec 0.0694224, recall 0.894234
2017-12-10T15:16:57.402583: step 3223, loss 0.137784, acc 0.953125, prec 0.0694171, recall 0.894234
2017-12-10T15:16:57.599779: step 3224, loss 3.25922, acc 0.96875, prec 0.0694153, recall 0.893939
2017-12-10T15:16:57.793401: step 3225, loss 0.449748, acc 0.953125, prec 0.0694338, recall 0.893974
2017-12-10T15:16:57.992873: step 3226, loss 0.296756, acc 0.96875, prec 0.069454, recall 0.894009
2017-12-10T15:16:58.189655: step 3227, loss 0.164775, acc 0.9375, prec 0.0695183, recall 0.894114
2017-12-10T15:16:58.379704: step 3228, loss 0.182318, acc 0.96875, prec 0.0695385, recall 0.894149
2017-12-10T15:16:58.571289: step 3229, loss 0.110228, acc 0.953125, prec 0.0695332, recall 0.894149
2017-12-10T15:16:58.766074: step 3230, loss 0.1297, acc 0.953125, prec 0.0695279, recall 0.894149
2017-12-10T15:16:58.960723: step 3231, loss 0.131191, acc 0.953125, prec 0.0695463, recall 0.894183
2017-12-10T15:16:59.159801: step 3232, loss 0.221591, acc 0.984375, prec 0.0695921, recall 0.894253
2017-12-10T15:16:59.361648: step 3233, loss 0.293678, acc 0.859375, prec 0.0695761, recall 0.894253
2017-12-10T15:16:59.555651: step 3234, loss 0.444566, acc 0.890625, prec 0.0696112, recall 0.894322
2017-12-10T15:16:59.749482: step 3235, loss 0.45989, acc 0.890625, prec 0.0696463, recall 0.894392
2017-12-10T15:16:59.942860: step 3236, loss 0.310767, acc 0.84375, prec 0.0696523, recall 0.894426
2017-12-10T15:17:00.138584: step 3237, loss 0.286243, acc 0.890625, prec 0.0696398, recall 0.894426
2017-12-10T15:17:00.330501: step 3238, loss 0.281834, acc 0.90625, prec 0.0697004, recall 0.89453
2017-12-10T15:17:00.523639: step 3239, loss 0.406972, acc 0.859375, prec 0.0696844, recall 0.89453
2017-12-10T15:17:00.724336: step 3240, loss 0.168302, acc 0.921875, prec 0.0696755, recall 0.89453
2017-12-10T15:17:00.916154: step 3241, loss 0.186346, acc 0.953125, prec 0.0696939, recall 0.894565
2017-12-10T15:17:01.107122: step 3242, loss 0.124748, acc 0.96875, prec 0.069809, recall 0.894737
2017-12-10T15:17:01.305211: step 3243, loss 0.114436, acc 0.984375, prec 0.0698072, recall 0.894737
2017-12-10T15:17:01.504832: step 3244, loss 0.102597, acc 0.953125, prec 0.0698256, recall 0.894771
2017-12-10T15:17:01.697602: step 3245, loss 0.126527, acc 0.96875, prec 0.069822, recall 0.894771
2017-12-10T15:17:01.888405: step 3246, loss 0.188035, acc 0.96875, prec 0.0698659, recall 0.89484
2017-12-10T15:17:02.080875: step 3247, loss 0.197666, acc 0.953125, prec 0.0698842, recall 0.894874
2017-12-10T15:17:02.273065: step 3248, loss 0.108364, acc 0.953125, prec 0.0699263, recall 0.894943
2017-12-10T15:17:02.470002: step 3249, loss 0.0615302, acc 0.96875, prec 0.0699228, recall 0.894943
2017-12-10T15:17:02.664085: step 3250, loss 0.193567, acc 0.9375, prec 0.0699156, recall 0.894943
2017-12-10T15:17:02.859900: step 3251, loss 0.267608, acc 0.90625, prec 0.0699049, recall 0.894943
2017-12-10T15:17:03.050784: step 3252, loss 0.108172, acc 0.96875, prec 0.0699014, recall 0.894943
2017-12-10T15:17:03.246443: step 3253, loss 0.485005, acc 0.9375, prec 0.0699417, recall 0.895011
2017-12-10T15:17:03.446899: step 3254, loss 0.127749, acc 0.96875, prec 0.0699618, recall 0.895046
2017-12-10T15:17:03.640732: step 3255, loss 0.0945261, acc 1, prec 0.0699855, recall 0.89508
2017-12-10T15:17:03.835659: step 3256, loss 0.0385909, acc 0.984375, prec 0.0699837, recall 0.89508
2017-12-10T15:17:04.030735: step 3257, loss 0.10329, acc 0.953125, prec 0.0699783, recall 0.89508
2017-12-10T15:17:04.226908: step 3258, loss 0.518462, acc 0.96875, prec 0.0699985, recall 0.895114
2017-12-10T15:17:04.423112: step 3259, loss 0.24906, acc 0.9375, prec 0.070015, recall 0.895148
2017-12-10T15:17:04.614527: step 3260, loss 0.144274, acc 0.9375, prec 0.0700079, recall 0.895148
2017-12-10T15:17:04.809281: step 3261, loss 0.0496325, acc 0.984375, prec 0.0700061, recall 0.895148
2017-12-10T15:17:05.007405: step 3262, loss 0.711697, acc 1, prec 0.0700298, recall 0.895182
2017-12-10T15:17:05.200995: step 3263, loss 0.0490531, acc 0.96875, prec 0.0700262, recall 0.895182
2017-12-10T15:17:05.393576: step 3264, loss 3.1758, acc 0.984375, prec 0.0700499, recall 0.894925
2017-12-10T15:17:05.591391: step 3265, loss 0.0998462, acc 0.96875, prec 0.0700463, recall 0.894925
2017-12-10T15:17:05.787291: step 3266, loss 0.182814, acc 0.953125, prec 0.0700647, recall 0.894959
2017-12-10T15:17:05.980986: step 3267, loss 0.123367, acc 0.984375, prec 0.0701102, recall 0.895028
2017-12-10T15:17:06.174320: step 3268, loss 0.159481, acc 0.921875, prec 0.070125, recall 0.895062
2017-12-10T15:17:06.365976: step 3269, loss 0.370378, acc 0.90625, prec 0.0701379, recall 0.895096
2017-12-10T15:17:06.561952: step 3270, loss 0.204634, acc 0.953125, prec 0.0701562, recall 0.89513
2017-12-10T15:17:06.753369: step 3271, loss 0.413575, acc 0.84375, prec 0.0701384, recall 0.89513
2017-12-10T15:17:06.942930: step 3272, loss 0.227303, acc 0.9375, prec 0.0702022, recall 0.895232
2017-12-10T15:17:07.136347: step 3273, loss 0.353523, acc 0.890625, prec 0.0702134, recall 0.895266
2017-12-10T15:17:07.332886: step 3274, loss 0.176798, acc 0.9375, prec 0.0702062, recall 0.895266
2017-12-10T15:17:07.524867: step 3275, loss 0.30748, acc 0.890625, prec 0.070241, recall 0.895334
2017-12-10T15:17:07.715855: step 3276, loss 0.42614, acc 0.90625, prec 0.0702303, recall 0.895334
2017-12-10T15:17:07.907087: step 3277, loss 0.16822, acc 0.9375, prec 0.0702468, recall 0.895368
2017-12-10T15:17:08.099862: step 3278, loss 0.380823, acc 0.890625, prec 0.0702579, recall 0.895402
2017-12-10T15:17:08.296372: step 3279, loss 0.175107, acc 0.953125, prec 0.0702526, recall 0.895402
2017-12-10T15:17:08.493653: step 3280, loss 0.15229, acc 0.9375, prec 0.0702454, recall 0.895402
2017-12-10T15:17:08.688419: step 3281, loss 0.113296, acc 0.9375, prec 0.0702619, recall 0.895435
2017-12-10T15:17:08.880578: step 3282, loss 0.338605, acc 0.90625, prec 0.0702512, recall 0.895435
2017-12-10T15:17:09.078242: step 3283, loss 0.203294, acc 0.953125, prec 0.0702694, recall 0.895469
2017-12-10T15:17:09.276525: step 3284, loss 0.0814285, acc 0.96875, prec 0.0702659, recall 0.895469
2017-12-10T15:17:09.467976: step 3285, loss 0.114806, acc 0.953125, prec 0.0702605, recall 0.895469
2017-12-10T15:17:09.662129: step 3286, loss 1.40344, acc 0.90625, prec 0.0703206, recall 0.895571
2017-12-10T15:17:09.856414: step 3287, loss 0.241736, acc 0.921875, prec 0.0703825, recall 0.895672
2017-12-10T15:17:10.051144: step 3288, loss 0.167783, acc 0.953125, prec 0.0703771, recall 0.895672
2017-12-10T15:17:10.242704: step 3289, loss 0.397031, acc 0.890625, prec 0.0703882, recall 0.895706
2017-12-10T15:17:10.439300: step 3290, loss 0.086596, acc 0.984375, prec 0.07041, recall 0.895739
2017-12-10T15:17:10.635423: step 3291, loss 1.29968, acc 0.96875, prec 0.0704318, recall 0.895484
2017-12-10T15:17:10.831082: step 3292, loss 0.318685, acc 0.96875, prec 0.0704754, recall 0.895551
2017-12-10T15:17:11.022847: step 3293, loss 0.228667, acc 0.921875, prec 0.0705136, recall 0.895619
2017-12-10T15:17:11.218082: step 3294, loss 0.15707, acc 0.9375, prec 0.0705301, recall 0.895652
2017-12-10T15:17:11.414789: step 3295, loss 0.126847, acc 0.953125, prec 0.0705718, recall 0.895719
2017-12-10T15:17:11.609828: step 3296, loss 0.323776, acc 0.921875, prec 0.0705629, recall 0.895719
2017-12-10T15:17:11.803382: step 3297, loss 0.138001, acc 0.953125, prec 0.0706282, recall 0.89582
2017-12-10T15:17:11.995453: step 3298, loss 0.173697, acc 0.953125, prec 0.0706464, recall 0.895853
2017-12-10T15:17:12.184829: step 3299, loss 0.481552, acc 0.9375, prec 0.0706863, recall 0.89592
2017-12-10T15:17:12.378669: step 3300, loss 0.325983, acc 0.921875, prec 0.070748, recall 0.896021
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_1/1512936356/checkpoints/model-3300

2017-12-10T15:17:13.669716: step 3301, loss 0.334053, acc 0.90625, prec 0.0707373, recall 0.896021
2017-12-10T15:17:13.861624: step 3302, loss 0.218391, acc 0.9375, prec 0.0707536, recall 0.896054
2017-12-10T15:17:14.052632: step 3303, loss 0.0523895, acc 0.96875, prec 0.0707501, recall 0.896054
2017-12-10T15:17:14.249623: step 3304, loss 1.62864, acc 0.90625, prec 0.0707411, recall 0.895766
2017-12-10T15:17:14.451222: step 3305, loss 0.129873, acc 0.9375, prec 0.0707339, recall 0.895766
2017-12-10T15:17:14.644187: step 3306, loss 0.13248, acc 0.953125, prec 0.0707286, recall 0.895766
2017-12-10T15:17:14.840456: step 3307, loss 0.137427, acc 0.953125, prec 0.0707232, recall 0.895766
2017-12-10T15:17:15.035996: step 3308, loss 0.258033, acc 0.90625, prec 0.0707124, recall 0.895766
2017-12-10T15:17:15.226267: step 3309, loss 0.24163, acc 0.9375, prec 0.0707288, recall 0.8958
2017-12-10T15:17:15.420002: step 3310, loss 0.200655, acc 0.921875, prec 0.0707199, recall 0.8958
2017-12-10T15:17:15.615269: step 3311, loss 0.233546, acc 0.9375, prec 0.0707362, recall 0.895833
2017-12-10T15:17:15.810973: step 3312, loss 0.178878, acc 0.9375, prec 0.0707291, recall 0.895833
2017-12-10T15:17:16.003702: step 3313, loss 0.64773, acc 0.953125, prec 0.0707472, recall 0.895867
2017-12-10T15:17:16.204307: step 3314, loss 0.0968422, acc 0.953125, prec 0.0707653, recall 0.8959
2017-12-10T15:17:16.398472: step 3315, loss 0.220924, acc 0.9375, prec 0.0707582, recall 0.8959
2017-12-10T15:17:16.594905: step 3316, loss 0.332796, acc 0.953125, prec 0.0708468, recall 0.896033
2017-12-10T15:17:16.788746: step 3317, loss 0.136464, acc 0.953125, prec 0.0708414, recall 0.896033
2017-12-10T15:17:16.981679: step 3318, loss 0.141399, acc 0.953125, prec 0.0709066, recall 0.896133
2017-12-10T15:17:17.172554: step 3319, loss 0.367222, acc 0.90625, prec 0.0709428, recall 0.896199
2017-12-10T15:17:17.365997: step 3320, loss 0.120879, acc 0.96875, prec 0.0709392, recall 0.896199
2017-12-10T15:17:17.559459: step 3321, loss 0.40448, acc 0.9375, prec 0.070979, recall 0.896266
2017-12-10T15:17:17.753026: step 3322, loss 0.134554, acc 0.9375, prec 0.0710188, recall 0.896332
2017-12-10T15:17:17.946672: step 3323, loss 0.239772, acc 0.9375, prec 0.0710116, recall 0.896332
2017-12-10T15:17:18.140360: step 3324, loss 0.528305, acc 0.921875, prec 0.0710496, recall 0.896398
2017-12-10T15:17:18.334073: step 3325, loss 0.449142, acc 0.953125, prec 0.0710911, recall 0.896464
2017-12-10T15:17:18.529482: step 3326, loss 0.279201, acc 0.859375, prec 0.0711219, recall 0.89653
2017-12-10T15:17:18.724180: step 3327, loss 0.291457, acc 0.921875, prec 0.0711129, recall 0.89653
2017-12-10T15:17:18.914877: step 3328, loss 0.0722118, acc 0.984375, prec 0.0711346, recall 0.896563
2017-12-10T15:17:19.108589: step 3329, loss 0.289504, acc 0.90625, prec 0.0711941, recall 0.896661
2017-12-10T15:17:19.302786: step 3330, loss 0.0644696, acc 0.96875, prec 0.0711906, recall 0.896661
2017-12-10T15:17:19.498816: step 3331, loss 0.0615723, acc 0.984375, prec 0.0712122, recall 0.896694
2017-12-10T15:17:19.694754: step 3332, loss 1.53589, acc 0.90625, prec 0.0712952, recall 0.896825
2017-12-10T15:17:19.888801: step 3333, loss 0.087401, acc 0.96875, prec 0.071315, recall 0.896858
2017-12-10T15:17:20.086579: step 3334, loss 0.4433, acc 0.921875, prec 0.071306, recall 0.896858
2017-12-10T15:17:20.279983: step 3335, loss 0.1658, acc 0.9375, prec 0.0712988, recall 0.896858
2017-12-10T15:17:20.471234: step 3336, loss 0.166984, acc 0.953125, prec 0.0712934, recall 0.896858
2017-12-10T15:17:20.663050: step 3337, loss 0.146113, acc 0.953125, prec 0.0713115, recall 0.896891
2017-12-10T15:17:20.852774: step 3338, loss 0.16626, acc 0.953125, prec 0.0713295, recall 0.896924
2017-12-10T15:17:21.049637: step 3339, loss 0.42296, acc 0.984375, prec 0.0713745, recall 0.896989
2017-12-10T15:17:21.244094: step 3340, loss 0.147889, acc 0.953125, prec 0.0713691, recall 0.896989
2017-12-10T15:17:21.436660: step 3341, loss 0.0739191, acc 0.984375, prec 0.0713673, recall 0.896989
2017-12-10T15:17:21.628956: step 3342, loss 0.251142, acc 0.890625, prec 0.0713547, recall 0.896989
2017-12-10T15:17:21.822060: step 3343, loss 0.328126, acc 0.921875, prec 0.0713691, recall 0.897022
2017-12-10T15:17:22.015511: step 3344, loss 0.178128, acc 0.953125, prec 0.0713638, recall 0.897022
2017-12-10T15:17:22.210180: step 3345, loss 0.448534, acc 0.875, prec 0.0713728, recall 0.897054
2017-12-10T15:17:22.399753: step 3346, loss 0.187893, acc 0.953125, prec 0.0713908, recall 0.897087
2017-12-10T15:17:22.590654: step 3347, loss 0.193059, acc 0.90625, prec 0.07138, recall 0.897087
2017-12-10T15:17:22.785552: step 3348, loss 0.187415, acc 0.921875, prec 0.071371, recall 0.897087
2017-12-10T15:17:22.980222: step 3349, loss 0.193934, acc 0.921875, prec 0.071362, recall 0.897087
2017-12-10T15:17:23.172554: step 3350, loss 0.357697, acc 0.90625, prec 0.0713746, recall 0.897119
2017-12-10T15:17:23.363697: step 3351, loss 0.348624, acc 0.890625, prec 0.0714088, recall 0.897184
2017-12-10T15:17:23.555303: step 3352, loss 0.115745, acc 0.953125, prec 0.0714034, recall 0.897184
2017-12-10T15:17:23.747156: step 3353, loss 0.1845, acc 0.9375, prec 0.071443, recall 0.897249
2017-12-10T15:17:23.940992: step 3354, loss 0.234553, acc 0.953125, prec 0.0714609, recall 0.897282
2017-12-10T15:17:24.141003: step 3355, loss 0.309798, acc 0.890625, prec 0.0714483, recall 0.897282
2017-12-10T15:17:24.336276: step 3356, loss 3.03024, acc 0.953125, prec 0.0714915, recall 0.897063
2017-12-10T15:17:24.535921: step 3357, loss 0.313886, acc 0.90625, prec 0.071504, recall 0.897096
2017-12-10T15:17:24.733971: step 3358, loss 0.24004, acc 0.9375, prec 0.0714969, recall 0.897096
2017-12-10T15:17:24.928587: step 3359, loss 0.0916208, acc 0.96875, prec 0.0715166, recall 0.897128
2017-12-10T15:17:25.122824: step 3360, loss 0.0879886, acc 0.96875, prec 0.0715364, recall 0.897161
2017-12-10T15:17:25.315252: step 3361, loss 0.19015, acc 0.953125, prec 0.071531, recall 0.897161
2017-12-10T15:17:25.507034: step 3362, loss 0.230495, acc 0.921875, prec 0.0715453, recall 0.897193
2017-12-10T15:17:25.702759: step 3363, loss 0.376474, acc 0.890625, prec 0.0715561, recall 0.897226
2017-12-10T15:17:25.891934: step 3364, loss 0.162784, acc 0.9375, prec 0.0715722, recall 0.897258
2017-12-10T15:17:26.086290: step 3365, loss 0.388831, acc 1, prec 0.0715956, recall 0.89729
2017-12-10T15:17:26.282243: step 3366, loss 0.261188, acc 0.9375, prec 0.071635, recall 0.897355
2017-12-10T15:17:26.474846: step 3367, loss 0.168382, acc 0.9375, prec 0.0716745, recall 0.89742
2017-12-10T15:17:26.670252: step 3368, loss 0.677811, acc 0.9375, prec 0.0716906, recall 0.897452
2017-12-10T15:17:26.866864: step 3369, loss 0.200654, acc 0.9375, prec 0.0716834, recall 0.897452
2017-12-10T15:17:27.062165: step 3370, loss 0.323851, acc 0.96875, prec 0.0717498, recall 0.897549
2017-12-10T15:17:27.256063: step 3371, loss 0.230559, acc 0.921875, prec 0.0717874, recall 0.897613
2017-12-10T15:17:27.451030: step 3372, loss 0.0356315, acc 1, prec 0.0717874, recall 0.897613
2017-12-10T15:17:27.645809: step 3373, loss 0.234011, acc 0.921875, prec 0.071825, recall 0.897677
2017-12-10T15:17:27.834250: step 3374, loss 0.301823, acc 0.9375, prec 0.0718178, recall 0.897677
2017-12-10T15:17:28.025624: step 3375, loss 0.376015, acc 0.921875, prec 0.0718554, recall 0.897742
2017-12-10T15:17:28.219410: step 3376, loss 0.801879, acc 0.859375, prec 0.071909, recall 0.897838
2017-12-10T15:17:28.415330: step 3377, loss 0.134452, acc 0.96875, prec 0.0719287, recall 0.89787
2017-12-10T15:17:28.617327: step 3378, loss 0.222088, acc 0.921875, prec 0.0719663, recall 0.897934
2017-12-10T15:17:28.806515: step 3379, loss 0.15832, acc 0.96875, prec 0.071986, recall 0.897966
2017-12-10T15:17:29.004072: step 3380, loss 0.232303, acc 0.96875, prec 0.0720289, recall 0.898029
2017-12-10T15:17:29.201481: step 3381, loss 0.858645, acc 0.828125, prec 0.0720323, recall 0.898061
2017-12-10T15:17:29.400602: step 3382, loss 0.131909, acc 0.90625, prec 0.0720215, recall 0.898061
2017-12-10T15:17:29.592761: step 3383, loss 0.245371, acc 0.9375, prec 0.0720142, recall 0.898061
2017-12-10T15:17:29.793155: step 3384, loss 0.137227, acc 0.953125, prec 0.0720088, recall 0.898061
2017-12-10T15:17:29.992622: step 3385, loss 0.148188, acc 0.90625, prec 0.0720445, recall 0.898125
2017-12-10T15:17:30.188586: step 3386, loss 0.321409, acc 0.9375, prec 0.0720606, recall 0.898157
2017-12-10T15:17:30.386104: step 3387, loss 0.351594, acc 0.890625, prec 0.0720479, recall 0.898157
2017-12-10T15:17:30.578662: step 3388, loss 0.506738, acc 0.84375, prec 0.0720531, recall 0.898189
2017-12-10T15:17:30.775950: step 3389, loss 0.286968, acc 0.90625, prec 0.0720423, recall 0.898189
2017-12-10T15:17:30.970748: step 3390, loss 0.4019, acc 0.9375, prec 0.0720351, recall 0.898189
2017-12-10T15:17:31.168677: step 3391, loss 0.223947, acc 0.921875, prec 0.0720725, recall 0.898252
2017-12-10T15:17:31.367483: step 3392, loss 0.103589, acc 0.96875, prec 0.0720689, recall 0.898252
2017-12-10T15:17:31.563634: step 3393, loss 0.118133, acc 0.9375, prec 0.0720617, recall 0.898252
2017-12-10T15:17:31.759021: step 3394, loss 0.0650317, acc 0.953125, prec 0.0720563, recall 0.898252
2017-12-10T15:17:31.949642: step 3395, loss 0.271822, acc 0.9375, prec 0.0720723, recall 0.898284
2017-12-10T15:17:32.141813: step 3396, loss 0.0899808, acc 0.953125, prec 0.0720669, recall 0.898284
2017-12-10T15:17:32.334170: step 3397, loss 0.368709, acc 0.921875, prec 0.0720579, recall 0.898284
2017-12-10T15:17:32.527902: step 3398, loss 0.214454, acc 0.96875, prec 0.0720775, recall 0.898316
2017-12-10T15:17:32.720875: step 3399, loss 0.0219972, acc 1, prec 0.0720775, recall 0.898316
2017-12-10T15:17:32.920240: step 3400, loss 0.0372371, acc 0.984375, prec 0.0720757, recall 0.898316
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_1/1512936356/checkpoints/model-3400

2017-12-10T15:17:34.187421: step 3401, loss 0.0600337, acc 0.953125, prec 0.0720935, recall 0.898347
2017-12-10T15:17:34.383021: step 3402, loss 1.58214, acc 0.953125, prec 0.0721363, recall 0.898131
2017-12-10T15:17:34.581472: step 3403, loss 0.187374, acc 0.9375, prec 0.0721291, recall 0.898131
2017-12-10T15:17:34.775306: step 3404, loss 0.0498343, acc 0.96875, prec 0.0721255, recall 0.898131
2017-12-10T15:17:34.974779: step 3405, loss 0.178066, acc 0.953125, prec 0.0721201, recall 0.898131
2017-12-10T15:17:35.175343: step 3406, loss 0.0530065, acc 0.96875, prec 0.0721397, recall 0.898163
2017-12-10T15:17:35.373401: step 3407, loss 0.121344, acc 0.96875, prec 0.0721593, recall 0.898194
2017-12-10T15:17:35.570764: step 3408, loss 0.212497, acc 0.9375, prec 0.0721753, recall 0.898226
2017-12-10T15:17:35.767896: step 3409, loss 0.201851, acc 0.96875, prec 0.0722181, recall 0.898289
2017-12-10T15:17:35.966070: step 3410, loss 0.180723, acc 0.953125, prec 0.072259, recall 0.898353
2017-12-10T15:17:36.162651: step 3411, loss 0.207028, acc 0.953125, prec 0.0723, recall 0.898416
2017-12-10T15:17:36.356845: step 3412, loss 0.157289, acc 0.9375, prec 0.0722928, recall 0.898416
2017-12-10T15:17:36.549441: step 3413, loss 0.122368, acc 0.984375, prec 0.0723142, recall 0.898447
2017-12-10T15:17:36.743538: step 3414, loss 2.07934, acc 0.921875, prec 0.0723069, recall 0.898168
2017-12-10T15:17:36.944393: step 3415, loss 0.307702, acc 0.90625, prec 0.0722961, recall 0.898168
2017-12-10T15:17:37.135415: step 3416, loss 0.140889, acc 0.9375, prec 0.0722889, recall 0.898168
2017-12-10T15:17:37.329721: step 3417, loss 0.333466, acc 0.90625, prec 0.072278, recall 0.898168
2017-12-10T15:17:37.521951: step 3418, loss 0.610703, acc 0.890625, prec 0.0722886, recall 0.8982
2017-12-10T15:17:37.717073: step 3419, loss 0.257879, acc 0.921875, prec 0.0723027, recall 0.898231
2017-12-10T15:17:37.914116: step 3420, loss 0.111506, acc 0.953125, prec 0.0722973, recall 0.898231
2017-12-10T15:17:38.105239: step 3421, loss 0.693951, acc 0.890625, prec 0.0723078, recall 0.898263
2017-12-10T15:17:38.300407: step 3422, loss 0.426621, acc 0.828125, prec 0.072288, recall 0.898263
2017-12-10T15:17:38.497744: step 3423, loss 0.199907, acc 0.9375, prec 0.0722807, recall 0.898263
2017-12-10T15:17:38.691735: step 3424, loss 0.246133, acc 0.953125, prec 0.0722753, recall 0.898263
2017-12-10T15:17:38.888671: step 3425, loss 0.407798, acc 0.9375, prec 0.0723144, recall 0.898326
2017-12-10T15:17:39.084823: step 3426, loss 0.246708, acc 0.9375, prec 0.0723303, recall 0.898358
2017-12-10T15:17:39.277413: step 3427, loss 0.323889, acc 0.953125, prec 0.0723481, recall 0.898389
2017-12-10T15:17:39.474722: step 3428, loss 0.150503, acc 0.953125, prec 0.0723658, recall 0.898421
2017-12-10T15:17:39.668319: step 3429, loss 0.488728, acc 0.9375, prec 0.072428, recall 0.898515
2017-12-10T15:17:39.864561: step 3430, loss 0.254536, acc 0.90625, prec 0.0724171, recall 0.898515
2017-12-10T15:17:40.058046: step 3431, loss 0.250353, acc 0.90625, prec 0.0724063, recall 0.898515
2017-12-10T15:17:40.249790: step 3432, loss 0.184447, acc 0.953125, prec 0.0724009, recall 0.898515
2017-12-10T15:17:40.443058: step 3433, loss 0.26512, acc 0.90625, prec 0.0723901, recall 0.898515
2017-12-10T15:17:40.638002: step 3434, loss 0.337423, acc 0.90625, prec 0.0723792, recall 0.898515
2017-12-10T15:17:40.834790: step 3435, loss 0.110627, acc 0.96875, prec 0.0723988, recall 0.898546
2017-12-10T15:17:41.029808: step 3436, loss 0.458585, acc 0.875, prec 0.0724305, recall 0.898609
2017-12-10T15:17:41.225311: step 3437, loss 0.379608, acc 0.9375, prec 0.0724233, recall 0.898609
2017-12-10T15:17:41.423338: step 3438, loss 0.264496, acc 0.921875, prec 0.0724143, recall 0.898609
2017-12-10T15:17:41.614422: step 3439, loss 0.0247322, acc 1, prec 0.0724374, recall 0.89864
2017-12-10T15:17:41.809035: step 3440, loss 0.197524, acc 0.953125, prec 0.0724551, recall 0.898672
2017-12-10T15:17:42.001725: step 3441, loss 0.193837, acc 1, prec 0.0724782, recall 0.898703
2017-12-10T15:17:42.199233: step 3442, loss 0.140059, acc 0.96875, prec 0.0724977, recall 0.898734
2017-12-10T15:17:42.392259: step 3443, loss 0.154368, acc 0.96875, prec 0.0725172, recall 0.898765
2017-12-10T15:17:42.583882: step 3444, loss 0.0454105, acc 0.96875, prec 0.0725136, recall 0.898765
2017-12-10T15:17:42.776363: step 3445, loss 0.161571, acc 0.96875, prec 0.07251, recall 0.898765
2017-12-10T15:17:42.972216: step 3446, loss 0.116744, acc 0.9375, prec 0.0725027, recall 0.898765
2017-12-10T15:17:43.170145: step 3447, loss 0.0736957, acc 1, prec 0.0725489, recall 0.898828
2017-12-10T15:17:43.364302: step 3448, loss 0.015317, acc 1, prec 0.0725489, recall 0.898828
2017-12-10T15:17:43.563110: step 3449, loss 1.01702, acc 0.953125, prec 0.0726128, recall 0.898921
2017-12-10T15:17:43.754725: step 3450, loss 0.393205, acc 0.953125, prec 0.0726073, recall 0.898921
2017-12-10T15:17:43.958819: step 3451, loss 0.179921, acc 0.921875, prec 0.0725983, recall 0.898921
2017-12-10T15:17:44.161390: step 3452, loss 0.0307346, acc 0.984375, prec 0.0726196, recall 0.898953
2017-12-10T15:17:44.361195: step 3453, loss 0.235683, acc 0.984375, prec 0.0726409, recall 0.898984
2017-12-10T15:17:44.556614: step 3454, loss 0.0507021, acc 0.984375, prec 0.0726621, recall 0.899015
2017-12-10T15:17:44.752518: step 3455, loss 0.4441, acc 0.921875, prec 0.0727454, recall 0.899139
2017-12-10T15:17:44.942478: step 3456, loss 0.921689, acc 0.9375, prec 0.0727843, recall 0.899201
2017-12-10T15:17:45.139245: step 3457, loss 0.168185, acc 0.9375, prec 0.0728001, recall 0.899232
2017-12-10T15:17:45.333543: step 3458, loss 0.175656, acc 0.9375, prec 0.0727928, recall 0.899232
2017-12-10T15:17:45.527713: step 3459, loss 0.0450539, acc 0.96875, prec 0.0727892, recall 0.899232
2017-12-10T15:17:45.724899: step 3460, loss 0.194918, acc 0.984375, prec 0.0728566, recall 0.899325
2017-12-10T15:17:45.918605: step 3461, loss 0.119787, acc 0.9375, prec 0.0728493, recall 0.899325
2017-12-10T15:17:46.111416: step 3462, loss 0.269759, acc 0.875, prec 0.0728579, recall 0.899356
2017-12-10T15:17:46.302034: step 3463, loss 0.490221, acc 0.90625, prec 0.0728931, recall 0.899417
2017-12-10T15:17:46.494296: step 3464, loss 0.327379, acc 0.90625, prec 0.0729283, recall 0.899479
2017-12-10T15:17:46.685391: step 3465, loss 0.288555, acc 0.921875, prec 0.0729193, recall 0.899479
2017-12-10T15:17:46.877230: step 3466, loss 0.20997, acc 0.921875, prec 0.0729332, recall 0.89951
2017-12-10T15:17:47.075522: step 3467, loss 0.175643, acc 0.96875, prec 0.0729296, recall 0.89951
2017-12-10T15:17:47.271356: step 3468, loss 0.269707, acc 0.921875, prec 0.0729205, recall 0.89951
2017-12-10T15:17:47.468079: step 3469, loss 0.409576, acc 0.875, prec 0.0729061, recall 0.89951
2017-12-10T15:17:47.662005: step 3470, loss 0.21918, acc 0.953125, prec 0.0729006, recall 0.89951
2017-12-10T15:17:47.853020: step 3471, loss 0.137628, acc 0.984375, prec 0.0728988, recall 0.89951
2017-12-10T15:17:48.047416: step 3472, loss 0.19968, acc 0.9375, prec 0.0728916, recall 0.89951
2017-12-10T15:17:48.243335: step 3473, loss 0.158845, acc 0.921875, prec 0.0729055, recall 0.899541
2017-12-10T15:17:48.438650: step 3474, loss 0.0874336, acc 0.96875, prec 0.0729249, recall 0.899571
2017-12-10T15:17:48.630529: step 3475, loss 0.246394, acc 0.921875, prec 0.0729389, recall 0.899602
2017-12-10T15:17:48.827454: step 3476, loss 0.0489609, acc 0.984375, prec 0.0729371, recall 0.899602
2017-12-10T15:17:49.015974: step 3477, loss 0.217578, acc 0.96875, prec 0.0729565, recall 0.899633
2017-12-10T15:17:49.211937: step 3478, loss 0.0389595, acc 0.984375, prec 0.0729547, recall 0.899633
2017-12-10T15:17:49.388399: step 3479, loss 0.0785437, acc 0.980392, prec 0.0729759, recall 0.899664
2017-12-10T15:17:49.591482: step 3480, loss 1.49645, acc 0.921875, prec 0.0730146, recall 0.89945
2017-12-10T15:17:49.788061: step 3481, loss 0.195706, acc 0.90625, prec 0.0730267, recall 0.899481
2017-12-10T15:17:49.981415: step 3482, loss 0.11235, acc 0.96875, prec 0.0730461, recall 0.899511
2017-12-10T15:17:50.181683: step 3483, loss 0.0632087, acc 0.96875, prec 0.0730425, recall 0.899511
2017-12-10T15:17:50.378483: step 3484, loss 0.175524, acc 0.953125, prec 0.0730371, recall 0.899511
2017-12-10T15:17:50.570100: step 3485, loss 0.0342766, acc 1, prec 0.0730371, recall 0.899511
2017-12-10T15:17:50.764836: step 3486, loss 0.204331, acc 0.9375, prec 0.0730298, recall 0.899511
2017-12-10T15:17:50.958074: step 3487, loss 0.301136, acc 0.921875, prec 0.0730437, recall 0.899542
2017-12-10T15:17:51.151881: step 3488, loss 0.853761, acc 0.953125, prec 0.0730843, recall 0.899603
2017-12-10T15:17:51.355006: step 3489, loss 0.0707813, acc 0.984375, prec 0.0731054, recall 0.899634
2017-12-10T15:17:51.551064: step 3490, loss 0.0469543, acc 0.984375, prec 0.0731266, recall 0.899665
2017-12-10T15:17:51.748869: step 3491, loss 0.270907, acc 0.953125, prec 0.0731441, recall 0.899695
2017-12-10T15:17:51.939888: step 3492, loss 0.192847, acc 0.953125, prec 0.0731387, recall 0.899695
2017-12-10T15:17:52.134948: step 3493, loss 0.0721126, acc 0.984375, prec 0.0731599, recall 0.899726
2017-12-10T15:17:52.328695: step 3494, loss 0.113281, acc 0.96875, prec 0.0731562, recall 0.899726
2017-12-10T15:17:52.524858: step 3495, loss 0.096279, acc 0.953125, prec 0.0731508, recall 0.899726
2017-12-10T15:17:52.720393: step 3496, loss 0.380171, acc 0.953125, prec 0.0731683, recall 0.899756
2017-12-10T15:17:52.913144: step 3497, loss 0.58166, acc 0.890625, prec 0.0731786, recall 0.899787
2017-12-10T15:17:53.108320: step 3498, loss 0.126071, acc 0.96875, prec 0.0731979, recall 0.899817
2017-12-10T15:17:53.303506: step 3499, loss 0.42224, acc 0.921875, prec 0.0732577, recall 0.899909
2017-12-10T15:17:53.499348: step 3500, loss 0.24634, acc 0.9375, prec 0.0732734, recall 0.899939
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_1/1512936356/checkpoints/model-3500

2017-12-10T15:17:54.827963: step 3501, loss 0.0469355, acc 0.984375, prec 0.0732945, recall 0.89997
2017-12-10T15:17:55.020956: step 3502, loss 0.155714, acc 0.984375, prec 0.0733386, recall 0.90003
2017-12-10T15:17:55.219364: step 3503, loss 0.361487, acc 0.9375, prec 0.0733543, recall 0.900061
2017-12-10T15:17:55.412682: step 3504, loss 0.220401, acc 0.9375, prec 0.0734158, recall 0.900152
2017-12-10T15:17:55.604536: step 3505, loss 0.146152, acc 0.953125, prec 0.0734104, recall 0.900152
2017-12-10T15:17:55.797851: step 3506, loss 0.153128, acc 0.953125, prec 0.0734049, recall 0.900152
2017-12-10T15:17:55.994605: step 3507, loss 0.116045, acc 0.953125, prec 0.0734224, recall 0.900182
2017-12-10T15:17:56.189345: step 3508, loss 0.0361671, acc 0.984375, prec 0.0734206, recall 0.900182
2017-12-10T15:17:56.383964: step 3509, loss 0.713335, acc 0.96875, prec 0.0734857, recall 0.900273
2017-12-10T15:17:56.580498: step 3510, loss 0.0317539, acc 1, prec 0.0734857, recall 0.900273
2017-12-10T15:17:56.777194: step 3511, loss 0.200858, acc 0.9375, prec 0.0734785, recall 0.900273
2017-12-10T15:17:56.970105: step 3512, loss 0.062574, acc 1, prec 0.0735014, recall 0.900303
2017-12-10T15:17:57.160684: step 3513, loss 0.171579, acc 0.96875, prec 0.0735436, recall 0.900363
2017-12-10T15:17:57.354132: step 3514, loss 0.160851, acc 0.953125, prec 0.0735381, recall 0.900363
2017-12-10T15:17:57.547774: step 3515, loss 0.118822, acc 0.96875, prec 0.0735345, recall 0.900363
2017-12-10T15:17:57.739091: step 3516, loss 0.0512004, acc 0.984375, prec 0.0735556, recall 0.900394
2017-12-10T15:17:57.933516: step 3517, loss 0.57515, acc 0.890625, prec 0.0735429, recall 0.900394
2017-12-10T15:17:58.129613: step 3518, loss 0.0527257, acc 0.984375, prec 0.073564, recall 0.900424
2017-12-10T15:17:58.328117: step 3519, loss 0.177927, acc 0.96875, prec 0.0735603, recall 0.900424
2017-12-10T15:17:58.523124: step 3520, loss 0.268903, acc 0.90625, prec 0.0736181, recall 0.900514
2017-12-10T15:17:58.719616: step 3521, loss 0.106737, acc 0.96875, prec 0.0736145, recall 0.900514
2017-12-10T15:17:58.914449: step 3522, loss 0.162462, acc 0.9375, prec 0.0736072, recall 0.900514
2017-12-10T15:17:59.110166: step 3523, loss 0.0978891, acc 0.9375, prec 0.0736457, recall 0.900574
2017-12-10T15:17:59.311185: step 3524, loss 0.160407, acc 0.984375, prec 0.0736668, recall 0.900604
2017-12-10T15:17:59.510205: step 3525, loss 0.157832, acc 0.9375, prec 0.0736595, recall 0.900604
2017-12-10T15:17:59.706758: step 3526, loss 0.0523252, acc 0.984375, prec 0.0736806, recall 0.900634
2017-12-10T15:17:59.900830: step 3527, loss 0.0489681, acc 0.96875, prec 0.0736769, recall 0.900634
2017-12-10T15:18:00.095930: step 3528, loss 0.0862998, acc 0.953125, prec 0.0737172, recall 0.900694
2017-12-10T15:18:00.285667: step 3529, loss 0.0649339, acc 0.96875, prec 0.0737136, recall 0.900694
2017-12-10T15:18:00.477830: step 3530, loss 0.47054, acc 0.953125, prec 0.073731, recall 0.900724
2017-12-10T15:18:00.677274: step 3531, loss 0.0937809, acc 0.96875, prec 0.0737731, recall 0.900784
2017-12-10T15:18:00.870095: step 3532, loss 0.0219519, acc 1, prec 0.073796, recall 0.900814
2017-12-10T15:18:01.063240: step 3533, loss 0.0299171, acc 0.984375, prec 0.0737942, recall 0.900814
2017-12-10T15:18:01.257139: step 3534, loss 0.23789, acc 0.96875, prec 0.0738134, recall 0.900844
2017-12-10T15:18:01.454091: step 3535, loss 0.0345664, acc 1, prec 0.0738591, recall 0.900904
2017-12-10T15:18:01.648695: step 3536, loss 0.0170272, acc 0.984375, prec 0.0738802, recall 0.900933
2017-12-10T15:18:01.847151: step 3537, loss 0.795076, acc 0.9375, prec 0.0739186, recall 0.900993
2017-12-10T15:18:02.044031: step 3538, loss 0.191926, acc 1, prec 0.0739415, recall 0.901023
2017-12-10T15:18:02.241030: step 3539, loss 0.0635347, acc 0.96875, prec 0.0739607, recall 0.901053
2017-12-10T15:18:02.436106: step 3540, loss 0.0247924, acc 1, prec 0.0740064, recall 0.901112
2017-12-10T15:18:02.632069: step 3541, loss 0.0917131, acc 0.9375, prec 0.0740448, recall 0.901172
2017-12-10T15:18:02.827411: step 3542, loss 0.102361, acc 0.953125, prec 0.0740393, recall 0.901172
2017-12-10T15:18:03.024724: step 3543, loss 0.528662, acc 0.953125, prec 0.0740796, recall 0.901231
2017-12-10T15:18:03.224738: step 3544, loss 0.376826, acc 0.9375, prec 0.0740951, recall 0.90126
2017-12-10T15:18:03.424402: step 3545, loss 0.846376, acc 0.953125, prec 0.0741125, recall 0.90129
2017-12-10T15:18:03.621776: step 3546, loss 0.294226, acc 0.953125, prec 0.0741298, recall 0.90132
2017-12-10T15:18:03.813500: step 3547, loss 0.0195211, acc 1, prec 0.0741983, recall 0.901408
2017-12-10T15:18:04.008038: step 3548, loss 0.0480553, acc 0.984375, prec 0.0742193, recall 0.901438
2017-12-10T15:18:04.205113: step 3549, loss 0.309908, acc 0.953125, prec 0.0742595, recall 0.901497
2017-12-10T15:18:04.400128: step 3550, loss 0.24356, acc 0.953125, prec 0.074254, recall 0.901497
2017-12-10T15:18:04.593816: step 3551, loss 0.187539, acc 0.921875, prec 0.0742905, recall 0.901556
2017-12-10T15:18:04.792353: step 3552, loss 0.159729, acc 0.9375, prec 0.0742832, recall 0.901556
2017-12-10T15:18:04.986645: step 3553, loss 0.0459923, acc 0.984375, prec 0.0743042, recall 0.901585
2017-12-10T15:18:05.178373: step 3554, loss 0.0613448, acc 0.96875, prec 0.0743005, recall 0.901585
2017-12-10T15:18:05.371077: step 3555, loss 0.415392, acc 0.9375, prec 0.0742932, recall 0.901585
2017-12-10T15:18:05.567507: step 3556, loss 0.0677606, acc 0.984375, prec 0.0743142, recall 0.901615
2017-12-10T15:18:05.762111: step 3557, loss 0.161, acc 0.953125, prec 0.0743771, recall 0.901703
2017-12-10T15:18:05.953650: step 3558, loss 0.435999, acc 0.921875, prec 0.0743908, recall 0.901732
2017-12-10T15:18:06.149590: step 3559, loss 0.161964, acc 0.96875, prec 0.0744327, recall 0.901791
2017-12-10T15:18:06.341741: step 3560, loss 0.303772, acc 0.90625, prec 0.0744901, recall 0.901879
2017-12-10T15:18:06.535833: step 3561, loss 0.227156, acc 0.953125, prec 0.074553, recall 0.901967
2017-12-10T15:18:06.730718: step 3562, loss 0.151221, acc 0.953125, prec 0.0745703, recall 0.901996
2017-12-10T15:18:06.927541: step 3563, loss 0.277739, acc 0.96875, prec 0.0745894, recall 0.902025
2017-12-10T15:18:07.121868: step 3564, loss 0.276881, acc 0.90625, prec 0.0745784, recall 0.902025
2017-12-10T15:18:07.316191: step 3565, loss 0.105083, acc 0.96875, prec 0.0746203, recall 0.902083
2017-12-10T15:18:07.512327: step 3566, loss 0.0444938, acc 1, prec 0.074643, recall 0.902112
2017-12-10T15:18:07.703100: step 3567, loss 0.211598, acc 0.953125, prec 0.0746831, recall 0.902171
2017-12-10T15:18:07.896057: step 3568, loss 0.0435039, acc 0.984375, prec 0.0746812, recall 0.902171
2017-12-10T15:18:08.093929: step 3569, loss 0.0541555, acc 0.984375, prec 0.0747022, recall 0.9022
2017-12-10T15:18:08.287780: step 3570, loss 0.194076, acc 0.953125, prec 0.0747194, recall 0.902229
2017-12-10T15:18:08.483262: step 3571, loss 0.105898, acc 0.96875, prec 0.0747158, recall 0.902229
2017-12-10T15:18:08.674805: step 3572, loss 0.267719, acc 0.90625, prec 0.0747275, recall 0.902258
2017-12-10T15:18:08.868723: step 3573, loss 0.0981741, acc 0.984375, prec 0.0747257, recall 0.902258
2017-12-10T15:18:09.057911: step 3574, loss 0.211171, acc 0.9375, prec 0.0747183, recall 0.902258
2017-12-10T15:18:09.256793: step 3575, loss 0.2814, acc 0.921875, prec 0.0747319, recall 0.902287
2017-12-10T15:18:09.453635: step 3576, loss 0.0102211, acc 1, prec 0.0747319, recall 0.902287
2017-12-10T15:18:09.644347: step 3577, loss 0.031537, acc 1, prec 0.0747319, recall 0.902287
2017-12-10T15:18:09.839979: step 3578, loss 2.29096, acc 0.953125, prec 0.0747282, recall 0.902019
2017-12-10T15:18:10.042744: step 3579, loss 0.00737559, acc 1, prec 0.0747282, recall 0.902019
2017-12-10T15:18:10.241564: step 3580, loss 0.141974, acc 0.96875, prec 0.0747473, recall 0.902048
2017-12-10T15:18:10.438106: step 3581, loss 0.044907, acc 0.984375, prec 0.0747454, recall 0.902048
2017-12-10T15:18:10.631613: step 3582, loss 0.951802, acc 1, prec 0.0748137, recall 0.902135
2017-12-10T15:18:10.829961: step 3583, loss 0.166031, acc 0.984375, prec 0.0748574, recall 0.902193
2017-12-10T15:18:11.025265: step 3584, loss 0.161842, acc 0.953125, prec 0.0748746, recall 0.902222
2017-12-10T15:18:11.218494: step 3585, loss 0.0768179, acc 0.953125, prec 0.0748691, recall 0.902222
2017-12-10T15:18:11.417285: step 3586, loss 2.90399, acc 0.9375, prec 0.0748635, recall 0.901955
2017-12-10T15:18:11.610296: step 3587, loss 0.165994, acc 0.953125, prec 0.0748808, recall 0.901984
2017-12-10T15:18:11.801890: step 3588, loss 0.438511, acc 0.828125, prec 0.074906, recall 0.902042
2017-12-10T15:18:11.994079: step 3589, loss 0.330563, acc 0.890625, prec 0.0749158, recall 0.902071
2017-12-10T15:18:12.191236: step 3590, loss 0.488162, acc 0.8125, prec 0.0749165, recall 0.9021
2017-12-10T15:18:12.385039: step 3591, loss 0.610336, acc 0.8125, prec 0.0749171, recall 0.902129
2017-12-10T15:18:12.573793: step 3592, loss 0.475957, acc 0.875, prec 0.0749024, recall 0.902129
2017-12-10T15:18:12.763765: step 3593, loss 0.590238, acc 0.796875, prec 0.0749466, recall 0.902216
2017-12-10T15:18:12.957302: step 3594, loss 1.07631, acc 0.6875, prec 0.0749099, recall 0.902216
2017-12-10T15:18:13.147446: step 3595, loss 0.796101, acc 0.796875, prec 0.0749087, recall 0.902245
2017-12-10T15:18:13.338684: step 3596, loss 1.12568, acc 0.875, prec 0.074962, recall 0.902331
2017-12-10T15:18:13.535698: step 3597, loss 0.760822, acc 0.765625, prec 0.0749571, recall 0.90236
2017-12-10T15:18:13.730489: step 3598, loss 0.907935, acc 0.75, prec 0.0749504, recall 0.902389
2017-12-10T15:18:13.920951: step 3599, loss 0.671309, acc 0.78125, prec 0.0749927, recall 0.902475
2017-12-10T15:18:14.114779: step 3600, loss 0.56727, acc 0.796875, prec 0.0749688, recall 0.902475
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_1/1512936356/checkpoints/model-3600

2017-12-10T15:18:15.288487: step 3601, loss 0.207031, acc 0.9375, prec 0.0749841, recall 0.902504
2017-12-10T15:18:15.486499: step 3602, loss 0.426201, acc 0.875, prec 0.0750147, recall 0.902561
2017-12-10T15:18:15.681524: step 3603, loss 0.416846, acc 0.90625, prec 0.0750037, recall 0.902561
2017-12-10T15:18:15.872817: step 3604, loss 0.308736, acc 0.875, prec 0.0750342, recall 0.902618
2017-12-10T15:18:16.062328: step 3605, loss 0.355066, acc 0.875, prec 0.0750196, recall 0.902618
2017-12-10T15:18:16.259623: step 3606, loss 0.219087, acc 0.953125, prec 0.0750593, recall 0.902676
2017-12-10T15:18:16.454228: step 3607, loss 0.398864, acc 0.890625, prec 0.0750464, recall 0.902676
2017-12-10T15:18:16.648413: step 3608, loss 0.0461437, acc 0.984375, prec 0.0750672, recall 0.902704
2017-12-10T15:18:16.840112: step 3609, loss 0.350566, acc 0.875, prec 0.0750978, recall 0.902761
2017-12-10T15:18:17.036889: step 3610, loss 0.273884, acc 0.90625, prec 0.0750867, recall 0.902761
2017-12-10T15:18:17.231800: step 3611, loss 0.403467, acc 0.90625, prec 0.0750983, recall 0.90279
2017-12-10T15:18:17.425330: step 3612, loss 0.079569, acc 0.953125, prec 0.0750928, recall 0.90279
2017-12-10T15:18:17.623572: step 3613, loss 0.0607034, acc 0.984375, prec 0.075091, recall 0.90279
2017-12-10T15:18:17.818248: step 3614, loss 0.0452675, acc 0.96875, prec 0.0750873, recall 0.90279
2017-12-10T15:18:18.011635: step 3615, loss 0.496237, acc 0.984375, prec 0.0751081, recall 0.902819
2017-12-10T15:18:18.207912: step 3616, loss 0.0709821, acc 0.953125, prec 0.0751026, recall 0.902819
2017-12-10T15:18:18.402122: step 3617, loss 0.118163, acc 0.96875, prec 0.0750989, recall 0.902819
2017-12-10T15:18:18.597721: step 3618, loss 0.106095, acc 0.953125, prec 0.0750934, recall 0.902819
2017-12-10T15:18:18.789126: step 3619, loss 0.068138, acc 0.96875, prec 0.0750897, recall 0.902819
2017-12-10T15:18:18.982834: step 3620, loss 0.0577882, acc 0.96875, prec 0.0750861, recall 0.902819
2017-12-10T15:18:19.179129: step 3621, loss 0.203034, acc 0.953125, prec 0.0750806, recall 0.902819
2017-12-10T15:18:19.378445: step 3622, loss 0.122236, acc 0.96875, prec 0.0750769, recall 0.902819
2017-12-10T15:18:19.570473: step 3623, loss 0.059764, acc 0.96875, prec 0.0750732, recall 0.902819
2017-12-10T15:18:19.766674: step 3624, loss 0.100047, acc 0.984375, prec 0.075094, recall 0.902847
2017-12-10T15:18:19.961204: step 3625, loss 0.00235464, acc 1, prec 0.075094, recall 0.902847
2017-12-10T15:18:20.151991: step 3626, loss 0.143183, acc 0.953125, prec 0.0750885, recall 0.902847
2017-12-10T15:18:20.349016: step 3627, loss 0.102275, acc 0.984375, prec 0.0751318, recall 0.902904
2017-12-10T15:18:20.545760: step 3628, loss 0.0110751, acc 1, prec 0.0751318, recall 0.902904
2017-12-10T15:18:20.740113: step 3629, loss 0.303597, acc 0.984375, prec 0.0751526, recall 0.902933
2017-12-10T15:18:20.940377: step 3630, loss 2.90949, acc 0.984375, prec 0.0751526, recall 0.902668
2017-12-10T15:18:21.138083: step 3631, loss 0.308829, acc 0.984375, prec 0.0751959, recall 0.902725
2017-12-10T15:18:21.332447: step 3632, loss 0.136554, acc 0.9375, prec 0.0751885, recall 0.902725
2017-12-10T15:18:21.527201: step 3633, loss 0.00665639, acc 1, prec 0.0751885, recall 0.902725
2017-12-10T15:18:21.717561: step 3634, loss 0.205548, acc 0.984375, prec 0.0752318, recall 0.902782
2017-12-10T15:18:21.913507: step 3635, loss 0.0791649, acc 0.984375, prec 0.07523, recall 0.902782
2017-12-10T15:18:22.107629: step 3636, loss 0.242982, acc 0.953125, prec 0.0752245, recall 0.902782
2017-12-10T15:18:22.301013: step 3637, loss 0.0469885, acc 0.984375, prec 0.0752226, recall 0.902782
2017-12-10T15:18:22.495325: step 3638, loss 0.173243, acc 0.96875, prec 0.0752415, recall 0.90281
2017-12-10T15:18:22.688603: step 3639, loss 0.16085, acc 0.9375, prec 0.0752567, recall 0.902839
2017-12-10T15:18:22.879265: step 3640, loss 0.170875, acc 0.96875, prec 0.0752756, recall 0.902867
2017-12-10T15:18:23.076379: step 3641, loss 0.0583903, acc 0.984375, prec 0.0752964, recall 0.902896
2017-12-10T15:18:23.270798: step 3642, loss 0.150458, acc 0.953125, prec 0.0753134, recall 0.902924
2017-12-10T15:18:23.464652: step 3643, loss 0.18804, acc 0.953125, prec 0.0753079, recall 0.902924
2017-12-10T15:18:23.658172: step 3644, loss 0.408135, acc 0.890625, prec 0.075295, recall 0.902924
2017-12-10T15:18:23.849991: step 3645, loss 0.145981, acc 0.953125, prec 0.0753121, recall 0.902952
2017-12-10T15:18:24.044264: step 3646, loss 0.251274, acc 0.921875, prec 0.0753029, recall 0.902952
2017-12-10T15:18:24.236998: step 3647, loss 0.0792465, acc 0.96875, prec 0.0752992, recall 0.902952
2017-12-10T15:18:24.430980: step 3648, loss 0.22252, acc 0.9375, prec 0.0753144, recall 0.902981
2017-12-10T15:18:24.623279: step 3649, loss 0.177835, acc 0.921875, prec 0.0753278, recall 0.903009
2017-12-10T15:18:24.814930: step 3650, loss 0.379189, acc 0.984375, prec 0.0753485, recall 0.903037
2017-12-10T15:18:25.010560: step 3651, loss 0.376517, acc 0.921875, prec 0.0754069, recall 0.903122
2017-12-10T15:18:25.203013: step 3652, loss 0.504469, acc 0.875, prec 0.0754147, recall 0.90315
2017-12-10T15:18:25.396732: step 3653, loss 0.342543, acc 0.890625, prec 0.0754244, recall 0.903179
2017-12-10T15:18:25.589371: step 3654, loss 0.0574301, acc 1, prec 0.0754469, recall 0.903207
2017-12-10T15:18:25.782217: step 3655, loss 0.318828, acc 0.9375, prec 0.0754621, recall 0.903235
2017-12-10T15:18:25.978067: step 3656, loss 0.313058, acc 0.921875, prec 0.0754979, recall 0.903292
2017-12-10T15:18:26.175840: step 3657, loss 0.141061, acc 0.96875, prec 0.0754942, recall 0.903292
2017-12-10T15:18:26.368263: step 3658, loss 0.147936, acc 0.953125, prec 0.0755337, recall 0.903348
2017-12-10T15:18:26.564411: step 3659, loss 0.0820325, acc 0.953125, prec 0.0755507, recall 0.903376
2017-12-10T15:18:26.758667: step 3660, loss 0.314041, acc 0.953125, prec 0.0755452, recall 0.903376
2017-12-10T15:18:26.952692: step 3661, loss 0.147269, acc 0.921875, prec 0.075536, recall 0.903376
2017-12-10T15:18:27.145493: step 3662, loss 0.159378, acc 0.96875, prec 0.0755323, recall 0.903376
2017-12-10T15:18:27.339805: step 3663, loss 0.047772, acc 0.96875, prec 0.0755736, recall 0.903432
2017-12-10T15:18:27.535724: step 3664, loss 0.334355, acc 0.96875, prec 0.0756149, recall 0.903488
2017-12-10T15:18:27.730532: step 3665, loss 0.062629, acc 0.96875, prec 0.0756337, recall 0.903516
2017-12-10T15:18:27.926237: step 3666, loss 0.197635, acc 0.9375, prec 0.0756489, recall 0.903544
2017-12-10T15:18:28.129919: step 3667, loss 0.0558995, acc 1, prec 0.0756713, recall 0.903572
2017-12-10T15:18:28.332138: step 3668, loss 0.0255923, acc 1, prec 0.0757388, recall 0.903656
2017-12-10T15:18:28.526365: step 3669, loss 0.114812, acc 0.96875, prec 0.0757351, recall 0.903656
2017-12-10T15:18:28.719991: step 3670, loss 0.0785687, acc 0.96875, prec 0.0757314, recall 0.903656
2017-12-10T15:18:28.921082: step 3671, loss 0.0211682, acc 1, prec 0.0757539, recall 0.903684
2017-12-10T15:18:29.119626: step 3672, loss 0.226181, acc 0.9375, prec 0.0757465, recall 0.903684
2017-12-10T15:18:29.322329: step 3673, loss 0.0367919, acc 0.984375, prec 0.0757447, recall 0.903684
2017-12-10T15:18:29.518396: step 3674, loss 0.00887249, acc 1, prec 0.0757447, recall 0.903684
2017-12-10T15:18:29.711644: step 3675, loss 0.132026, acc 0.953125, prec 0.0757616, recall 0.903712
2017-12-10T15:18:29.908528: step 3676, loss 0.0125257, acc 1, prec 0.0757616, recall 0.903712
2017-12-10T15:18:30.103177: step 3677, loss 0.0146222, acc 1, prec 0.0757616, recall 0.903712
2017-12-10T15:18:30.300427: step 3678, loss 0.127586, acc 0.953125, prec 0.0758235, recall 0.903796
2017-12-10T15:18:30.496986: step 3679, loss 0.0756565, acc 0.953125, prec 0.075818, recall 0.903796
2017-12-10T15:18:30.694502: step 3680, loss 0.0619168, acc 0.96875, prec 0.0758143, recall 0.903796
2017-12-10T15:18:30.891345: step 3681, loss 0.57892, acc 1, prec 0.0758368, recall 0.903824
2017-12-10T15:18:31.088304: step 3682, loss 0.029871, acc 0.984375, prec 0.0758574, recall 0.903852
2017-12-10T15:18:31.289627: step 3683, loss 0.0826777, acc 0.984375, prec 0.0758555, recall 0.903852
2017-12-10T15:18:31.484349: step 3684, loss 0.0180474, acc 1, prec 0.0758555, recall 0.903852
2017-12-10T15:18:31.682106: step 3685, loss 0.06235, acc 0.984375, prec 0.0758537, recall 0.903852
2017-12-10T15:18:31.876993: step 3686, loss 0.112106, acc 0.984375, prec 0.0758518, recall 0.903852
2017-12-10T15:18:32.072339: step 3687, loss 0.0734976, acc 0.96875, prec 0.0758482, recall 0.903852
2017-12-10T15:18:32.270429: step 3688, loss 2.17011, acc 0.96875, prec 0.0758688, recall 0.903618
2017-12-10T15:18:32.472368: step 3689, loss 0.152105, acc 0.96875, prec 0.0758875, recall 0.903646
2017-12-10T15:18:32.669777: step 3690, loss 0.742749, acc 1, prec 0.07591, recall 0.903674
2017-12-10T15:18:32.866976: step 3691, loss 0.080772, acc 0.984375, prec 0.0759306, recall 0.903702
2017-12-10T15:18:33.065398: step 3692, loss 0.196687, acc 0.921875, prec 0.0759663, recall 0.903757
2017-12-10T15:18:33.255867: step 3693, loss 0.273175, acc 0.9375, prec 0.0759589, recall 0.903757
2017-12-10T15:18:33.450541: step 3694, loss 0.114646, acc 0.984375, prec 0.0759795, recall 0.903785
2017-12-10T15:18:33.642693: step 3695, loss 0.172062, acc 0.9375, prec 0.0759721, recall 0.903785
2017-12-10T15:18:33.839313: step 3696, loss 0.0986504, acc 0.921875, prec 0.0759853, recall 0.903813
2017-12-10T15:18:34.033940: step 3697, loss 0.28242, acc 0.921875, prec 0.0759761, recall 0.903813
2017-12-10T15:18:34.229691: step 3698, loss 0.468552, acc 0.859375, prec 0.0760268, recall 0.903896
2017-12-10T15:18:34.423103: step 3699, loss 0.268993, acc 0.90625, prec 0.0760382, recall 0.903924
2017-12-10T15:18:34.614843: step 3700, loss 0.38502, acc 0.90625, prec 0.0760495, recall 0.903952
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_1/1512936356/checkpoints/model-3700

2017-12-10T15:18:35.819952: step 3701, loss 0.298983, acc 0.90625, prec 0.0760384, recall 0.903952
2017-12-10T15:18:36.014959: step 3702, loss 0.282697, acc 0.921875, prec 0.076074, recall 0.904007
2017-12-10T15:18:36.205179: step 3703, loss 0.25619, acc 0.890625, prec 0.0760835, recall 0.904035
2017-12-10T15:18:36.400345: step 3704, loss 0.274469, acc 0.90625, prec 0.0760725, recall 0.904035
2017-12-10T15:18:36.600323: step 3705, loss 0.233805, acc 0.921875, prec 0.0760856, recall 0.904062
2017-12-10T15:18:36.793916: step 3706, loss 0.231605, acc 0.9375, prec 0.0761007, recall 0.90409
2017-12-10T15:18:36.990347: step 3707, loss 0.385043, acc 0.921875, prec 0.0760914, recall 0.90409
2017-12-10T15:18:37.181819: step 3708, loss 0.223192, acc 0.921875, prec 0.0761046, recall 0.904117
2017-12-10T15:18:37.375267: step 3709, loss 0.144169, acc 0.9375, prec 0.0760972, recall 0.904117
2017-12-10T15:18:37.573424: step 3710, loss 0.225128, acc 0.9375, prec 0.0760899, recall 0.904117
2017-12-10T15:18:37.769263: step 3711, loss 0.279582, acc 0.921875, prec 0.0760806, recall 0.904117
2017-12-10T15:18:37.965187: step 3712, loss 0.625926, acc 0.90625, prec 0.0761591, recall 0.904228
2017-12-10T15:18:38.158802: step 3713, loss 0.148313, acc 0.96875, prec 0.0761778, recall 0.904255
2017-12-10T15:18:38.351615: step 3714, loss 0.0493975, acc 0.96875, prec 0.0761965, recall 0.904283
2017-12-10T15:18:38.542987: step 3715, loss 0.0932462, acc 0.953125, prec 0.0762133, recall 0.90431
2017-12-10T15:18:38.739825: step 3716, loss 0.152338, acc 0.984375, prec 0.0762115, recall 0.90431
2017-12-10T15:18:38.929747: step 3717, loss 0.218745, acc 0.9375, prec 0.0762265, recall 0.904338
2017-12-10T15:18:39.124407: step 3718, loss 0.163212, acc 0.9375, prec 0.0762414, recall 0.904365
2017-12-10T15:18:39.317580: step 3719, loss 0.0250049, acc 0.984375, prec 0.0762396, recall 0.904365
2017-12-10T15:18:39.517892: step 3720, loss 0.15268, acc 0.984375, prec 0.0762601, recall 0.904393
2017-12-10T15:18:39.718631: step 3721, loss 0.263634, acc 0.953125, prec 0.0762546, recall 0.904393
2017-12-10T15:18:39.911826: step 3722, loss 1.3304, acc 0.9375, prec 0.076249, recall 0.904133
2017-12-10T15:18:40.103709: step 3723, loss 0.1438, acc 0.96875, prec 0.0762677, recall 0.904161
2017-12-10T15:18:40.304244: step 3724, loss 0.960746, acc 0.984375, prec 0.0763553, recall 0.904271
2017-12-10T15:18:40.500400: step 3725, loss 0.0232572, acc 1, prec 0.0763553, recall 0.904271
2017-12-10T15:18:40.693593: step 3726, loss 0.124485, acc 0.953125, prec 0.0763497, recall 0.904271
2017-12-10T15:18:40.887231: step 3727, loss 0.10023, acc 0.9375, prec 0.0763423, recall 0.904271
2017-12-10T15:18:41.082588: step 3728, loss 0.404679, acc 0.9375, prec 0.0763796, recall 0.904325
2017-12-10T15:18:41.275878: step 3729, loss 0.113098, acc 0.9375, prec 0.0763946, recall 0.904353
2017-12-10T15:18:41.467474: step 3730, loss 0.17846, acc 0.953125, prec 0.0764114, recall 0.90438
2017-12-10T15:18:41.663831: step 3731, loss 0.390086, acc 0.875, prec 0.076419, recall 0.904408
2017-12-10T15:18:41.859776: step 3732, loss 0.146354, acc 0.9375, prec 0.0764786, recall 0.90449
2017-12-10T15:18:42.054849: step 3733, loss 0.282581, acc 0.90625, prec 0.0764675, recall 0.90449
2017-12-10T15:18:42.248685: step 3734, loss 0.24026, acc 0.921875, prec 0.0764805, recall 0.904517
2017-12-10T15:18:42.441415: step 3735, loss 0.375842, acc 0.859375, prec 0.0764639, recall 0.904517
2017-12-10T15:18:42.635345: step 3736, loss 0.216317, acc 0.890625, prec 0.076451, recall 0.904517
2017-12-10T15:18:42.828614: step 3737, loss 0.263578, acc 0.953125, prec 0.0765124, recall 0.904599
2017-12-10T15:18:43.020831: step 3738, loss 0.24719, acc 0.90625, prec 0.0765013, recall 0.904599
2017-12-10T15:18:43.220168: step 3739, loss 0.322902, acc 0.875, prec 0.0764865, recall 0.904599
2017-12-10T15:18:43.413959: step 3740, loss 0.315795, acc 0.9375, prec 0.0765014, recall 0.904626
2017-12-10T15:18:43.605979: step 3741, loss 0.322443, acc 0.875, prec 0.0764866, recall 0.904626
2017-12-10T15:18:43.803693: step 3742, loss 0.400164, acc 0.859375, prec 0.0765146, recall 0.90468
2017-12-10T15:18:44.001353: step 3743, loss 0.163466, acc 0.921875, prec 0.0765277, recall 0.904708
2017-12-10T15:18:44.200943: step 3744, loss 0.213351, acc 0.96875, prec 0.0765463, recall 0.904735
2017-12-10T15:18:44.404595: step 3745, loss 0.0869097, acc 0.953125, prec 0.076563, recall 0.904762
2017-12-10T15:18:44.605237: step 3746, loss 0.145705, acc 0.96875, prec 0.0766039, recall 0.904816
2017-12-10T15:18:44.800805: step 3747, loss 0.180075, acc 0.96875, prec 0.0766447, recall 0.90487
2017-12-10T15:18:44.992452: step 3748, loss 0.181883, acc 1, prec 0.0767115, recall 0.904952
2017-12-10T15:18:45.186590: step 3749, loss 0.337946, acc 0.890625, prec 0.0766986, recall 0.904952
2017-12-10T15:18:45.382869: step 3750, loss 0.0228812, acc 0.984375, prec 0.0766967, recall 0.904952
2017-12-10T15:18:45.577007: step 3751, loss 0.0572009, acc 0.96875, prec 0.076693, recall 0.904952
2017-12-10T15:18:45.774521: step 3752, loss 0.00905092, acc 1, prec 0.076693, recall 0.904952
2017-12-10T15:18:45.971518: step 3753, loss 0.977318, acc 0.984375, prec 0.0767135, recall 0.904979
2017-12-10T15:18:46.169869: step 3754, loss 0.0267932, acc 0.984375, prec 0.0767116, recall 0.904979
2017-12-10T15:18:46.366123: step 3755, loss 0.508616, acc 0.96875, prec 0.0767969, recall 0.905087
2017-12-10T15:18:46.561097: step 3756, loss 0.151957, acc 0.953125, prec 0.0768137, recall 0.905114
2017-12-10T15:18:46.757644: step 3757, loss 0.227401, acc 0.96875, prec 0.0768545, recall 0.905168
2017-12-10T15:18:46.958116: step 3758, loss 0.890114, acc 0.984375, prec 0.0769194, recall 0.905248
2017-12-10T15:18:47.156874: step 3759, loss 0.231025, acc 0.9375, prec 0.076912, recall 0.905248
2017-12-10T15:18:47.352762: step 3760, loss 0.254648, acc 0.984375, prec 0.0769546, recall 0.905302
2017-12-10T15:18:47.548152: step 3761, loss 0.216142, acc 0.90625, prec 0.0769435, recall 0.905302
2017-12-10T15:18:47.746892: step 3762, loss 0.11346, acc 0.953125, prec 0.0769601, recall 0.905329
2017-12-10T15:18:47.944347: step 3763, loss 0.159817, acc 0.953125, prec 0.0769991, recall 0.905382
2017-12-10T15:18:48.142754: step 3764, loss 1.24193, acc 0.921875, prec 0.0770139, recall 0.905153
2017-12-10T15:18:48.339967: step 3765, loss 0.169051, acc 0.9375, prec 0.0770065, recall 0.905153
2017-12-10T15:18:48.533533: step 3766, loss 0.38544, acc 0.9375, prec 0.0770435, recall 0.905207
2017-12-10T15:18:48.727610: step 3767, loss 0.389526, acc 0.859375, prec 0.0770712, recall 0.90526
2017-12-10T15:18:48.920264: step 3768, loss 0.534967, acc 0.828125, prec 0.0770731, recall 0.905287
2017-12-10T15:18:49.111518: step 3769, loss 0.526324, acc 0.84375, prec 0.0770767, recall 0.905314
2017-12-10T15:18:49.307801: step 3770, loss 0.393044, acc 0.890625, prec 0.0771081, recall 0.905367
2017-12-10T15:18:49.499819: step 3771, loss 0.533247, acc 0.859375, prec 0.0770914, recall 0.905367
2017-12-10T15:18:49.694157: step 3772, loss 0.511607, acc 0.890625, prec 0.0770785, recall 0.905367
2017-12-10T15:18:49.886975: step 3773, loss 0.327378, acc 0.875, prec 0.0770636, recall 0.905367
2017-12-10T15:18:50.083854: step 3774, loss 0.526536, acc 0.859375, prec 0.0770913, recall 0.905421
2017-12-10T15:18:50.277544: step 3775, loss 0.487767, acc 0.859375, prec 0.0770747, recall 0.905421
2017-12-10T15:18:50.472265: step 3776, loss 0.149811, acc 0.953125, prec 0.0771135, recall 0.905474
2017-12-10T15:18:50.668860: step 3777, loss 0.196678, acc 0.90625, prec 0.0771245, recall 0.905501
2017-12-10T15:18:50.863366: step 3778, loss 0.0972253, acc 0.9375, prec 0.0771393, recall 0.905527
2017-12-10T15:18:51.056682: step 3779, loss 0.277684, acc 0.953125, prec 0.0771337, recall 0.905527
2017-12-10T15:18:51.248883: step 3780, loss 0.261314, acc 0.9375, prec 0.0771485, recall 0.905554
2017-12-10T15:18:51.440465: step 3781, loss 0.13285, acc 0.984375, prec 0.0771466, recall 0.905554
2017-12-10T15:18:51.636010: step 3782, loss 0.0903151, acc 0.953125, prec 0.0771854, recall 0.905607
2017-12-10T15:18:51.830511: step 3783, loss 0.131804, acc 0.953125, prec 0.0771798, recall 0.905607
2017-12-10T15:18:52.027808: step 3784, loss 0.144722, acc 0.921875, prec 0.0771927, recall 0.905634
2017-12-10T15:18:52.223140: step 3785, loss 0.146216, acc 0.96875, prec 0.0772112, recall 0.90566
2017-12-10T15:18:52.422341: step 3786, loss 0.127783, acc 0.953125, prec 0.0772056, recall 0.90566
2017-12-10T15:18:52.615152: step 3787, loss 0.0328292, acc 0.984375, prec 0.0772038, recall 0.90566
2017-12-10T15:18:52.811269: step 3788, loss 0.129876, acc 0.953125, prec 0.0771982, recall 0.90566
2017-12-10T15:18:53.008936: step 3789, loss 0.0400563, acc 0.984375, prec 0.0772185, recall 0.905687
2017-12-10T15:18:53.206502: step 3790, loss 0.063519, acc 0.96875, prec 0.0772148, recall 0.905687
2017-12-10T15:18:53.408544: step 3791, loss 0.610585, acc 0.984375, prec 0.0772572, recall 0.90574
2017-12-10T15:18:53.601807: step 3792, loss 0.0805162, acc 0.984375, prec 0.0772554, recall 0.90574
2017-12-10T15:18:53.795563: step 3793, loss 0.544229, acc 0.984375, prec 0.0773421, recall 0.905846
2017-12-10T15:18:53.987502: step 3794, loss 3.68142, acc 0.953125, prec 0.0773605, recall 0.905618
2017-12-10T15:18:54.185773: step 3795, loss 0.125263, acc 0.953125, prec 0.077355, recall 0.905618
2017-12-10T15:18:54.383339: step 3796, loss 0.223109, acc 0.921875, prec 0.0773678, recall 0.905644
2017-12-10T15:18:54.577706: step 3797, loss 0.417421, acc 0.875, prec 0.077353, recall 0.905644
2017-12-10T15:18:54.774968: step 3798, loss 0.111231, acc 0.953125, prec 0.0773474, recall 0.905644
2017-12-10T15:18:54.965050: step 3799, loss 0.0735523, acc 0.984375, prec 0.0773898, recall 0.905697
2017-12-10T15:18:55.160333: step 3800, loss 0.167746, acc 0.9375, prec 0.0774045, recall 0.905724
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_1/1512936356/checkpoints/model-3800

2017-12-10T15:18:56.323779: step 3801, loss 0.193777, acc 0.953125, prec 0.0773989, recall 0.905724
2017-12-10T15:18:56.520021: step 3802, loss 0.254432, acc 0.90625, prec 0.0773878, recall 0.905724
2017-12-10T15:18:56.713755: step 3803, loss 0.189264, acc 0.921875, prec 0.0774228, recall 0.905777
2017-12-10T15:18:56.905548: step 3804, loss 0.278254, acc 0.90625, prec 0.0774558, recall 0.90583
2017-12-10T15:18:57.102361: step 3805, loss 0.261478, acc 0.90625, prec 0.0774668, recall 0.905856
2017-12-10T15:18:57.295887: step 3806, loss 0.445812, acc 0.890625, prec 0.0774538, recall 0.905856
2017-12-10T15:18:57.490925: step 3807, loss 0.372063, acc 0.859375, prec 0.0774371, recall 0.905856
2017-12-10T15:18:57.683661: step 3808, loss 0.246321, acc 0.90625, prec 0.077426, recall 0.905856
2017-12-10T15:18:57.877593: step 3809, loss 0.371161, acc 0.890625, prec 0.0774351, recall 0.905882
2017-12-10T15:18:58.074733: step 3810, loss 0.292399, acc 0.9375, prec 0.0774277, recall 0.905882
2017-12-10T15:18:58.267462: step 3811, loss 0.159434, acc 0.9375, prec 0.0774203, recall 0.905882
2017-12-10T15:18:58.466432: step 3812, loss 0.205789, acc 0.9375, prec 0.077457, recall 0.905935
2017-12-10T15:18:58.660239: step 3813, loss 0.103439, acc 0.96875, prec 0.0774533, recall 0.905935
2017-12-10T15:18:58.855108: step 3814, loss 0.0451018, acc 0.984375, prec 0.0774515, recall 0.905935
2017-12-10T15:18:59.053477: step 3815, loss 0.178458, acc 0.9375, prec 0.0774661, recall 0.905961
2017-12-10T15:18:59.252571: step 3816, loss 0.161451, acc 0.9375, prec 0.0774808, recall 0.905988
2017-12-10T15:18:59.449563: step 3817, loss 0.0195418, acc 1, prec 0.0774808, recall 0.905988
2017-12-10T15:18:59.651511: step 3818, loss 0.199298, acc 0.953125, prec 0.0774973, recall 0.906014
2017-12-10T15:18:59.846369: step 3819, loss 0.143397, acc 1, prec 0.0775194, recall 0.90604
2017-12-10T15:19:00.049486: step 3820, loss 0.31615, acc 0.96875, prec 0.0775598, recall 0.906093
2017-12-10T15:19:00.243714: step 3821, loss 0.0385672, acc 1, prec 0.0776039, recall 0.906145
2017-12-10T15:19:00.438497: step 3822, loss 0.233408, acc 0.9375, prec 0.0776186, recall 0.906171
2017-12-10T15:19:00.632310: step 3823, loss 0.231433, acc 0.96875, prec 0.0776369, recall 0.906198
2017-12-10T15:19:00.829576: step 3824, loss 0.302231, acc 0.921875, prec 0.0776497, recall 0.906224
2017-12-10T15:19:01.029226: step 3825, loss 0.0257989, acc 0.984375, prec 0.0776478, recall 0.906224
2017-12-10T15:19:01.227391: step 3826, loss 0.0948106, acc 0.96875, prec 0.0776662, recall 0.90625
2017-12-10T15:19:01.429997: step 3827, loss 0.160445, acc 0.921875, prec 0.077679, recall 0.906276
2017-12-10T15:19:01.633727: step 3828, loss 0.0731037, acc 0.984375, prec 0.0776991, recall 0.906302
2017-12-10T15:19:01.832208: step 3829, loss 0.072173, acc 0.984375, prec 0.0776973, recall 0.906302
2017-12-10T15:19:02.037168: step 3830, loss 0.242546, acc 0.984375, prec 0.0777175, recall 0.906328
2017-12-10T15:19:02.235357: step 3831, loss 0.312703, acc 0.953125, prec 0.0777119, recall 0.906328
2017-12-10T15:19:02.431583: step 3832, loss 0.103616, acc 0.984375, prec 0.0777541, recall 0.906381
2017-12-10T15:19:02.629934: step 3833, loss 0.0358375, acc 0.984375, prec 0.0777523, recall 0.906381
2017-12-10T15:19:02.823183: step 3834, loss 0.200566, acc 0.984375, prec 0.0777945, recall 0.906433
2017-12-10T15:19:03.021723: step 3835, loss 0.0885625, acc 0.96875, prec 0.0777908, recall 0.906433
2017-12-10T15:19:03.216673: step 3836, loss 0.0355094, acc 0.984375, prec 0.0777889, recall 0.906433
2017-12-10T15:19:03.413396: step 3837, loss 0.0610915, acc 0.96875, prec 0.0777852, recall 0.906433
2017-12-10T15:19:03.606740: step 3838, loss 0.00743167, acc 1, prec 0.0777852, recall 0.906433
2017-12-10T15:19:03.800677: step 3839, loss 1.29048, acc 0.984375, prec 0.0777852, recall 0.90618
2017-12-10T15:19:03.997853: step 3840, loss 0.0650482, acc 0.984375, prec 0.0778054, recall 0.906206
2017-12-10T15:19:04.192385: step 3841, loss 0.0616935, acc 0.984375, prec 0.0778035, recall 0.906206
2017-12-10T15:19:04.391403: step 3842, loss 0.12586, acc 1, prec 0.0778256, recall 0.906233
2017-12-10T15:19:04.588713: step 3843, loss 0.087124, acc 0.953125, prec 0.077842, recall 0.906259
2017-12-10T15:19:04.783609: step 3844, loss 0.261499, acc 0.9375, prec 0.0778346, recall 0.906259
2017-12-10T15:19:04.977246: step 3845, loss 0.113298, acc 0.953125, prec 0.077829, recall 0.906259
2017-12-10T15:19:05.169812: step 3846, loss 0.397051, acc 0.890625, prec 0.077816, recall 0.906259
2017-12-10T15:19:05.366535: step 3847, loss 0.426492, acc 0.921875, prec 0.0778067, recall 0.906259
2017-12-10T15:19:05.559929: step 3848, loss 0.0682519, acc 0.96875, prec 0.077803, recall 0.906259
2017-12-10T15:19:05.754070: step 3849, loss 0.107963, acc 0.953125, prec 0.0777974, recall 0.906259
2017-12-10T15:19:05.943718: step 3850, loss 0.16194, acc 0.9375, prec 0.07779, recall 0.906259
2017-12-10T15:19:06.142770: step 3851, loss 0.269074, acc 0.9375, prec 0.0778046, recall 0.906285
2017-12-10T15:19:06.336993: step 3852, loss 0.124353, acc 0.953125, prec 0.077821, recall 0.906311
2017-12-10T15:19:06.531868: step 3853, loss 0.129032, acc 0.953125, prec 0.0778375, recall 0.906337
2017-12-10T15:19:06.729432: step 3854, loss 0.255513, acc 0.953125, prec 0.0778979, recall 0.906415
2017-12-10T15:19:06.928891: step 3855, loss 0.180181, acc 0.96875, prec 0.0779382, recall 0.906467
2017-12-10T15:19:07.125256: step 3856, loss 0.0617283, acc 0.96875, prec 0.0779345, recall 0.906467
2017-12-10T15:19:07.322353: step 3857, loss 0.205269, acc 0.953125, prec 0.0779509, recall 0.906493
2017-12-10T15:19:07.515169: step 3858, loss 0.0901423, acc 0.953125, prec 0.0779453, recall 0.906493
2017-12-10T15:19:07.710780: step 3859, loss 0.232518, acc 0.984375, prec 0.0779655, recall 0.906519
2017-12-10T15:19:07.909788: step 3860, loss 1.71633, acc 0.953125, prec 0.0779837, recall 0.906293
2017-12-10T15:19:08.106837: step 3861, loss 0.158747, acc 0.953125, prec 0.0780001, recall 0.906319
2017-12-10T15:19:08.301975: step 3862, loss 0.219337, acc 0.96875, prec 0.0780404, recall 0.906371
2017-12-10T15:19:08.500884: step 3863, loss 0.0948907, acc 0.96875, prec 0.0780807, recall 0.906423
2017-12-10T15:19:08.695001: step 3864, loss 0.181067, acc 0.953125, prec 0.078119, recall 0.906475
2017-12-10T15:19:08.890982: step 3865, loss 0.393564, acc 0.90625, prec 0.0781298, recall 0.906501
2017-12-10T15:19:09.082628: step 3866, loss 0.305117, acc 0.921875, prec 0.0781425, recall 0.906527
2017-12-10T15:19:09.271976: step 3867, loss 0.108216, acc 0.953125, prec 0.0781589, recall 0.906552
2017-12-10T15:19:09.464342: step 3868, loss 0.221061, acc 0.9375, prec 0.0781514, recall 0.906552
2017-12-10T15:19:09.657072: step 3869, loss 0.0988124, acc 0.953125, prec 0.0781678, recall 0.906578
2017-12-10T15:19:09.848017: step 3870, loss 0.267824, acc 0.875, prec 0.0781749, recall 0.906604
2017-12-10T15:19:10.048930: step 3871, loss 0.13155, acc 0.953125, prec 0.0781693, recall 0.906604
2017-12-10T15:19:10.243343: step 3872, loss 0.0655425, acc 0.984375, prec 0.0782333, recall 0.906681
2017-12-10T15:19:10.437445: step 3873, loss 0.12829, acc 0.953125, prec 0.0782497, recall 0.906707
2017-12-10T15:19:10.629582: step 3874, loss 0.0718308, acc 0.984375, prec 0.0782478, recall 0.906707
2017-12-10T15:19:10.824585: step 3875, loss 0.201353, acc 0.953125, prec 0.0782642, recall 0.906733
2017-12-10T15:19:11.018836: step 3876, loss 0.2359, acc 0.9375, prec 0.0782787, recall 0.906759
2017-12-10T15:19:11.213167: step 3877, loss 0.447363, acc 0.875, prec 0.0783077, recall 0.90681
2017-12-10T15:19:11.410011: step 3878, loss 0.109947, acc 0.96875, prec 0.0783039, recall 0.90681
2017-12-10T15:19:11.608482: step 3879, loss 0.108441, acc 0.953125, prec 0.0782983, recall 0.90681
2017-12-10T15:19:11.799837: step 3880, loss 0.105207, acc 0.984375, prec 0.0783184, recall 0.906836
2017-12-10T15:19:11.993792: step 3881, loss 0.166717, acc 0.9375, prec 0.0783768, recall 0.906913
2017-12-10T15:19:12.186729: step 3882, loss 0.074046, acc 0.984375, prec 0.0783968, recall 0.906938
2017-12-10T15:19:12.380704: step 3883, loss 0.583016, acc 0.953125, prec 0.0784132, recall 0.906964
2017-12-10T15:19:12.578880: step 3884, loss 0.0774064, acc 0.984375, prec 0.0784552, recall 0.907015
2017-12-10T15:19:12.777978: step 3885, loss 0.117932, acc 0.96875, prec 0.0784514, recall 0.907015
2017-12-10T15:19:12.976995: step 3886, loss 0.171915, acc 0.984375, prec 0.0784715, recall 0.907041
2017-12-10T15:19:13.171257: step 3887, loss 0.167253, acc 0.9375, prec 0.078486, recall 0.907066
2017-12-10T15:19:13.365726: step 3888, loss 0.103234, acc 0.953125, prec 0.0785023, recall 0.907092
2017-12-10T15:19:13.560600: step 3889, loss 0.13066, acc 0.953125, prec 0.0785186, recall 0.907117
2017-12-10T15:19:13.761361: step 3890, loss 0.184168, acc 0.96875, prec 0.0785149, recall 0.907117
2017-12-10T15:19:13.958274: step 3891, loss 2.8011, acc 0.9375, prec 0.0785312, recall 0.906894
2017-12-10T15:19:14.164473: step 3892, loss 0.368819, acc 0.953125, prec 0.0785913, recall 0.90697
2017-12-10T15:19:14.370075: step 3893, loss 0.0446454, acc 1, prec 0.0785913, recall 0.90697
2017-12-10T15:19:14.561181: step 3894, loss 0.0902681, acc 0.984375, prec 0.0786332, recall 0.907021
2017-12-10T15:19:14.756936: step 3895, loss 0.211387, acc 0.9375, prec 0.0786696, recall 0.907072
2017-12-10T15:19:14.949405: step 3896, loss 0.447493, acc 0.921875, prec 0.0786821, recall 0.907098
2017-12-10T15:19:15.147826: step 3897, loss 0.326786, acc 0.90625, prec 0.0787366, recall 0.907174
2017-12-10T15:19:15.344711: step 3898, loss 0.850927, acc 0.953125, prec 0.0787529, recall 0.9072
2017-12-10T15:19:15.536919: step 3899, loss 0.240752, acc 0.90625, prec 0.0787635, recall 0.907225
2017-12-10T15:19:15.733298: step 3900, loss 0.356793, acc 0.890625, prec 0.0787504, recall 0.907225
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_1/1512936356/checkpoints/model-3900

2017-12-10T15:19:17.103553: step 3901, loss 0.102474, acc 0.96875, prec 0.0787686, recall 0.90725
2017-12-10T15:19:17.297096: step 3902, loss 0.181769, acc 0.96875, prec 0.0787648, recall 0.90725
2017-12-10T15:19:17.487881: step 3903, loss 0.833566, acc 0.84375, prec 0.0787461, recall 0.90725
2017-12-10T15:19:17.680851: step 3904, loss 0.247376, acc 0.875, prec 0.0787312, recall 0.90725
2017-12-10T15:19:17.880272: step 3905, loss 0.288185, acc 0.859375, prec 0.0787144, recall 0.90725
2017-12-10T15:19:18.082437: step 3906, loss 0.535506, acc 0.90625, prec 0.0787032, recall 0.90725
2017-12-10T15:19:18.275188: step 3907, loss 0.137978, acc 0.90625, prec 0.0787575, recall 0.907326
2017-12-10T15:19:18.473558: step 3908, loss 0.249289, acc 0.921875, prec 0.07877, recall 0.907352
2017-12-10T15:19:18.666150: step 3909, loss 0.425369, acc 0.90625, prec 0.0788025, recall 0.907402
2017-12-10T15:19:18.862728: step 3910, loss 0.278728, acc 0.921875, prec 0.0787932, recall 0.907402
2017-12-10T15:19:19.055472: step 3911, loss 0.186275, acc 0.921875, prec 0.0787839, recall 0.907402
2017-12-10T15:19:19.251343: step 3912, loss 0.295966, acc 0.921875, prec 0.0788182, recall 0.907453
2017-12-10T15:19:19.443991: step 3913, loss 0.217912, acc 0.984375, prec 0.07886, recall 0.907503
2017-12-10T15:19:19.638308: step 3914, loss 0.207519, acc 0.90625, prec 0.0788488, recall 0.907503
2017-12-10T15:19:19.834499: step 3915, loss 0.216655, acc 0.9375, prec 0.0788632, recall 0.907529
2017-12-10T15:19:20.030323: step 3916, loss 0.177764, acc 0.9375, prec 0.0788557, recall 0.907529
2017-12-10T15:19:20.225456: step 3917, loss 0.0493178, acc 1, prec 0.0788993, recall 0.907579
2017-12-10T15:19:20.420607: step 3918, loss 0.0677752, acc 0.953125, prec 0.0789156, recall 0.907604
2017-12-10T15:19:20.618391: step 3919, loss 0.0999751, acc 0.953125, prec 0.07891, recall 0.907604
2017-12-10T15:19:20.813574: step 3920, loss 0.015852, acc 1, prec 0.0789318, recall 0.907629
2017-12-10T15:19:21.008335: step 3921, loss 0.195953, acc 0.984375, prec 0.0790172, recall 0.90773
2017-12-10T15:19:21.203532: step 3922, loss 0.183104, acc 0.984375, prec 0.0790372, recall 0.907755
2017-12-10T15:19:21.400938: step 3923, loss 0.0789156, acc 0.984375, prec 0.0790571, recall 0.90778
2017-12-10T15:19:21.596314: step 3924, loss 0.666094, acc 1, prec 0.0791007, recall 0.90783
2017-12-10T15:19:21.797739: step 3925, loss 0.100305, acc 0.984375, prec 0.0791425, recall 0.90788
2017-12-10T15:19:21.991452: step 3926, loss 0.878412, acc 0.984375, prec 0.0791842, recall 0.90793
2017-12-10T15:19:22.194315: step 3927, loss 0.16666, acc 0.984375, prec 0.0792042, recall 0.907955
2017-12-10T15:19:22.391055: step 3928, loss 0.0199668, acc 1, prec 0.0792042, recall 0.907955
2017-12-10T15:19:22.586602: step 3929, loss 0.158146, acc 0.96875, prec 0.0792004, recall 0.907955
2017-12-10T15:19:22.780714: step 3930, loss 0.180417, acc 0.984375, prec 0.0792422, recall 0.908005
2017-12-10T15:19:22.979760: step 3931, loss 0.271792, acc 0.953125, prec 0.0792583, recall 0.90803
2017-12-10T15:19:23.176970: step 3932, loss 0.39221, acc 0.90625, prec 0.0792471, recall 0.90803
2017-12-10T15:19:23.368954: step 3933, loss 0.231693, acc 0.921875, prec 0.0792813, recall 0.90808
2017-12-10T15:19:23.565865: step 3934, loss 0.121351, acc 0.9375, prec 0.0792738, recall 0.90808
2017-12-10T15:19:23.758634: step 3935, loss 0.218466, acc 0.96875, prec 0.0792918, recall 0.908105
2017-12-10T15:19:23.954507: step 3936, loss 0.0409899, acc 0.984375, prec 0.0793117, recall 0.90813
2017-12-10T15:19:24.152820: step 3937, loss 0.175141, acc 0.9375, prec 0.079326, recall 0.908155
2017-12-10T15:19:24.347365: step 3938, loss 0.169388, acc 0.953125, prec 0.0793422, recall 0.90818
2017-12-10T15:19:24.546232: step 3939, loss 0.211144, acc 0.96875, prec 0.0793384, recall 0.90818
2017-12-10T15:19:24.742208: step 3940, loss 0.571516, acc 0.921875, prec 0.0793726, recall 0.90823
2017-12-10T15:19:24.936458: step 3941, loss 0.0635789, acc 0.984375, prec 0.079436, recall 0.908304
2017-12-10T15:19:25.131120: step 3942, loss 0.258316, acc 0.890625, prec 0.0794229, recall 0.908304
2017-12-10T15:19:25.330185: step 3943, loss 0.270935, acc 0.9375, prec 0.0794372, recall 0.908329
2017-12-10T15:19:25.528485: step 3944, loss 0.0184179, acc 1, prec 0.0794372, recall 0.908329
2017-12-10T15:19:25.721305: step 3945, loss 0.0766658, acc 0.984375, prec 0.0794788, recall 0.908378
2017-12-10T15:19:25.916528: step 3946, loss 0.0517686, acc 1, prec 0.0795223, recall 0.908428
2017-12-10T15:19:26.114549: step 3947, loss 0.141206, acc 0.953125, prec 0.0795602, recall 0.908477
2017-12-10T15:19:26.310121: step 3948, loss 0.770942, acc 0.984375, prec 0.0795801, recall 0.908502
2017-12-10T15:19:26.509364: step 3949, loss 2.3664, acc 0.953125, prec 0.0796199, recall 0.908306
2017-12-10T15:19:26.707199: step 3950, loss 0.896589, acc 0.9375, prec 0.0796776, recall 0.908381
2017-12-10T15:19:26.901425: step 3951, loss 0.212746, acc 0.96875, prec 0.0796738, recall 0.908381
2017-12-10T15:19:27.093181: step 3952, loss 1.00273, acc 0.890625, prec 0.0797042, recall 0.90843
2017-12-10T15:19:27.288897: step 3953, loss 0.615255, acc 0.78125, prec 0.0796778, recall 0.90843
2017-12-10T15:19:27.483746: step 3954, loss 0.667657, acc 0.765625, prec 0.079693, recall 0.908479
2017-12-10T15:19:27.675815: step 3955, loss 0.771839, acc 0.84375, prec 0.0796742, recall 0.908479
2017-12-10T15:19:27.870349: step 3956, loss 0.361574, acc 0.84375, prec 0.0796771, recall 0.908504
2017-12-10T15:19:28.065660: step 3957, loss 0.396994, acc 0.875, prec 0.0797055, recall 0.908553
2017-12-10T15:19:28.259423: step 3958, loss 0.505222, acc 0.859375, prec 0.0797103, recall 0.908578
2017-12-10T15:19:28.452092: step 3959, loss 0.285765, acc 0.90625, prec 0.0797642, recall 0.908651
2017-12-10T15:19:28.650976: step 3960, loss 0.619848, acc 0.8125, prec 0.0797416, recall 0.908651
2017-12-10T15:19:28.844259: step 3961, loss 0.598221, acc 0.828125, prec 0.0797426, recall 0.908676
2017-12-10T15:19:29.038472: step 3962, loss 0.634754, acc 0.828125, prec 0.0797653, recall 0.908725
2017-12-10T15:19:29.238067: step 3963, loss 0.59795, acc 0.84375, prec 0.0797682, recall 0.908749
2017-12-10T15:19:29.430523: step 3964, loss 0.718775, acc 0.828125, prec 0.0797692, recall 0.908774
2017-12-10T15:19:29.626809: step 3965, loss 0.93816, acc 0.765625, prec 0.0797627, recall 0.908798
2017-12-10T15:19:29.818458: step 3966, loss 0.645903, acc 0.875, prec 0.0797693, recall 0.908823
2017-12-10T15:19:30.009014: step 3967, loss 0.195622, acc 0.953125, prec 0.0797637, recall 0.908823
2017-12-10T15:19:30.203999: step 3968, loss 0.234327, acc 0.921875, prec 0.079776, recall 0.908847
2017-12-10T15:19:30.397829: step 3969, loss 0.30273, acc 0.890625, prec 0.0797845, recall 0.908872
2017-12-10T15:19:30.595817: step 3970, loss 0.309407, acc 0.96875, prec 0.0798024, recall 0.908896
2017-12-10T15:19:30.791651: step 3971, loss 0.155478, acc 0.9375, prec 0.0797949, recall 0.908896
2017-12-10T15:19:30.988355: step 3972, loss 0.0956024, acc 0.96875, prec 0.0798128, recall 0.90892
2017-12-10T15:19:31.192072: step 3973, loss 0.102095, acc 0.96875, prec 0.0798306, recall 0.908945
2017-12-10T15:19:31.387789: step 3974, loss 0.179423, acc 0.953125, prec 0.0798683, recall 0.908994
2017-12-10T15:19:31.589249: step 3975, loss 0.108844, acc 0.953125, prec 0.0798627, recall 0.908994
2017-12-10T15:19:31.765881: step 3976, loss 0.0797898, acc 0.980392, prec 0.0798608, recall 0.908994
2017-12-10T15:19:31.973095: step 3977, loss 2.62552, acc 0.953125, prec 0.0799219, recall 0.908824
2017-12-10T15:19:32.169495: step 3978, loss 0.139303, acc 0.984375, prec 0.0799201, recall 0.908824
2017-12-10T15:19:32.362971: step 3979, loss 2.25668, acc 0.96875, prec 0.0799614, recall 0.908629
2017-12-10T15:19:32.565934: step 3980, loss 0.045132, acc 0.96875, prec 0.0799577, recall 0.908629
2017-12-10T15:19:32.767351: step 3981, loss 0.0969559, acc 0.96875, prec 0.0799539, recall 0.908629
2017-12-10T15:19:32.959217: step 3982, loss 0.229431, acc 0.953125, prec 0.0800132, recall 0.908703
2017-12-10T15:19:33.152791: step 3983, loss 0.283541, acc 0.953125, prec 0.0800291, recall 0.908727
2017-12-10T15:19:33.343823: step 3984, loss 0.462655, acc 0.890625, prec 0.0800376, recall 0.908751
2017-12-10T15:19:33.537333: step 3985, loss 0.137744, acc 0.953125, prec 0.0800752, recall 0.9088
2017-12-10T15:19:33.730190: step 3986, loss 0.301399, acc 0.90625, prec 0.0801503, recall 0.908897
2017-12-10T15:19:33.923015: step 3987, loss 0.445058, acc 0.875, prec 0.0801353, recall 0.908897
2017-12-10T15:19:34.114428: step 3988, loss 0.397337, acc 0.875, prec 0.0801418, recall 0.908921
2017-12-10T15:19:34.311820: step 3989, loss 0.409859, acc 0.921875, prec 0.0801756, recall 0.90897
2017-12-10T15:19:34.504233: step 3990, loss 0.347766, acc 0.875, prec 0.0801606, recall 0.90897
2017-12-10T15:19:34.697405: step 3991, loss 0.317614, acc 0.875, prec 0.0801887, recall 0.909018
2017-12-10T15:19:34.889214: step 3992, loss 0.146979, acc 0.953125, prec 0.0802262, recall 0.909067
2017-12-10T15:19:35.079135: step 3993, loss 0.664017, acc 0.828125, prec 0.0802055, recall 0.909067
2017-12-10T15:19:35.270781: step 3994, loss 0.385193, acc 0.890625, prec 0.0801923, recall 0.909067
2017-12-10T15:19:35.463616: step 3995, loss 0.494902, acc 0.84375, prec 0.0802382, recall 0.909139
2017-12-10T15:19:35.656692: step 3996, loss 0.211883, acc 0.953125, prec 0.0803404, recall 0.90926
2017-12-10T15:19:35.849735: step 3997, loss 0.173587, acc 0.953125, prec 0.0803347, recall 0.90926
2017-12-10T15:19:36.041860: step 3998, loss 0.123447, acc 0.9375, prec 0.0803272, recall 0.90926
2017-12-10T15:19:36.236698: step 3999, loss 0.123455, acc 0.953125, prec 0.0803862, recall 0.909332
2017-12-10T15:19:36.432340: step 4000, loss 0.292807, acc 0.890625, prec 0.080373, recall 0.909332

Evaluation:
2017-12-10T15:19:40.992302: step 4000, loss 2.65363, acc 0.943202, prec 0.0812221, recall 0.896508

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_1/1512936356/checkpoints/model-4000

2017-12-10T15:19:42.292643: step 4001, loss 0.283261, acc 0.90625, prec 0.0812533, recall 0.89656
2017-12-10T15:19:42.486119: step 4002, loss 0.337595, acc 0.953125, prec 0.0812901, recall 0.896613
2017-12-10T15:19:42.680779: step 4003, loss 0.133224, acc 0.953125, prec 0.0812845, recall 0.896613
2017-12-10T15:19:42.874792: step 4004, loss 0.0693054, acc 0.984375, prec 0.081325, recall 0.896666
2017-12-10T15:19:43.071755: step 4005, loss 0.0530757, acc 0.96875, prec 0.0813213, recall 0.896666
2017-12-10T15:19:43.265731: step 4006, loss 0.553395, acc 0.953125, prec 0.081358, recall 0.896718
2017-12-10T15:19:43.465626: step 4007, loss 0.0776693, acc 0.984375, prec 0.0813774, recall 0.896745
2017-12-10T15:19:43.663087: step 4008, loss 0.0512467, acc 0.96875, prec 0.0813736, recall 0.896745
2017-12-10T15:19:43.858926: step 4009, loss 0.382082, acc 0.953125, prec 0.0814316, recall 0.896823
2017-12-10T15:19:44.057422: step 4010, loss 0.166119, acc 0.96875, prec 0.081449, recall 0.89685
2017-12-10T15:19:44.256410: step 4011, loss 0.0122438, acc 1, prec 0.0814702, recall 0.896876
2017-12-10T15:19:44.452459: step 4012, loss 0.14203, acc 0.96875, prec 0.0814876, recall 0.896902
2017-12-10T15:19:44.651650: step 4013, loss 0.0861085, acc 0.953125, prec 0.0815032, recall 0.896928
2017-12-10T15:19:44.848632: step 4014, loss 0.114865, acc 0.953125, prec 0.0814975, recall 0.896928
2017-12-10T15:19:45.044123: step 4015, loss 0.0746344, acc 0.96875, prec 0.081515, recall 0.896954
2017-12-10T15:19:45.240831: step 4016, loss 0.514663, acc 0.96875, prec 0.0815536, recall 0.897007
2017-12-10T15:19:45.436728: step 4017, loss 0.022168, acc 1, prec 0.0815748, recall 0.897033
2017-12-10T15:19:45.632917: step 4018, loss 0.219032, acc 0.9375, prec 0.0815884, recall 0.897059
2017-12-10T15:19:45.827124: step 4019, loss 0.270808, acc 0.96875, prec 0.081627, recall 0.897111
2017-12-10T15:19:46.020120: step 4020, loss 0.0128102, acc 1, prec 0.081627, recall 0.897111
2017-12-10T15:19:46.213306: step 4021, loss 0.337599, acc 0.953125, prec 0.0816214, recall 0.897111
2017-12-10T15:19:46.410580: step 4022, loss 0.264053, acc 0.921875, prec 0.0816331, recall 0.897137
2017-12-10T15:19:46.603572: step 4023, loss 0.229813, acc 0.9375, prec 0.0816468, recall 0.897163
2017-12-10T15:19:46.798972: step 4024, loss 0.0895836, acc 0.953125, prec 0.0816411, recall 0.897163
2017-12-10T15:19:46.997705: step 4025, loss 0.137646, acc 0.96875, prec 0.0816374, recall 0.897163
2017-12-10T15:19:47.191562: step 4026, loss 0.513892, acc 0.890625, prec 0.0816242, recall 0.897163
2017-12-10T15:19:47.389785: step 4027, loss 0.224079, acc 0.9375, prec 0.0816378, recall 0.897189
2017-12-10T15:19:47.584832: step 4028, loss 0.127715, acc 0.921875, prec 0.0816284, recall 0.897189
2017-12-10T15:19:47.778764: step 4029, loss 0.198844, acc 0.921875, prec 0.081619, recall 0.897189
2017-12-10T15:19:47.975924: step 4030, loss 0.0826357, acc 0.96875, prec 0.081721, recall 0.897319
2017-12-10T15:19:48.170277: step 4031, loss 4.07135, acc 0.921875, prec 0.0817135, recall 0.897092
2017-12-10T15:19:48.368434: step 4032, loss 0.014509, acc 1, prec 0.0817135, recall 0.897092
2017-12-10T15:19:48.565100: step 4033, loss 0.0857801, acc 0.96875, prec 0.0817309, recall 0.897118
2017-12-10T15:19:48.761398: step 4034, loss 0.152149, acc 0.953125, prec 0.0817252, recall 0.897118
2017-12-10T15:19:48.955059: step 4035, loss 0.12764, acc 0.9375, prec 0.0817389, recall 0.897144
2017-12-10T15:19:49.154231: step 4036, loss 0.175962, acc 0.9375, prec 0.0817525, recall 0.89717
2017-12-10T15:19:49.350101: step 4037, loss 0.266455, acc 0.890625, prec 0.0817604, recall 0.897196
2017-12-10T15:19:49.541347: step 4038, loss 0.404602, acc 0.921875, prec 0.081751, recall 0.897196
2017-12-10T15:19:49.735992: step 4039, loss 0.222014, acc 0.890625, prec 0.081759, recall 0.897222
2017-12-10T15:19:49.927989: step 4040, loss 0.748443, acc 0.859375, prec 0.081742, recall 0.897222
2017-12-10T15:19:50.122727: step 4041, loss 0.202354, acc 0.9375, prec 0.0817557, recall 0.897248
2017-12-10T15:19:50.314117: step 4042, loss 0.110772, acc 0.96875, prec 0.0817519, recall 0.897248
2017-12-10T15:19:50.513977: step 4043, loss 0.326247, acc 0.921875, prec 0.0817636, recall 0.897274
2017-12-10T15:19:50.706115: step 4044, loss 0.136901, acc 0.9375, prec 0.0818194, recall 0.897352
2017-12-10T15:19:50.902409: step 4045, loss 0.070555, acc 0.953125, prec 0.0818138, recall 0.897352
2017-12-10T15:19:51.103412: step 4046, loss 0.484009, acc 0.921875, prec 0.0818466, recall 0.897404
2017-12-10T15:19:51.297672: step 4047, loss 0.169518, acc 0.953125, prec 0.0818621, recall 0.897429
2017-12-10T15:19:51.495687: step 4048, loss 0.169239, acc 0.9375, prec 0.0818756, recall 0.897455
2017-12-10T15:19:51.688583: step 4049, loss 0.128758, acc 0.96875, prec 0.081893, recall 0.897481
2017-12-10T15:19:51.891144: step 4050, loss 0.203017, acc 0.921875, prec 0.0819047, recall 0.897507
2017-12-10T15:19:52.085360: step 4051, loss 0.0563833, acc 0.984375, prec 0.0819239, recall 0.897533
2017-12-10T15:19:52.279250: step 4052, loss 0.0597674, acc 0.984375, prec 0.081922, recall 0.897533
2017-12-10T15:19:52.471171: step 4053, loss 0.111424, acc 0.953125, prec 0.0819164, recall 0.897533
2017-12-10T15:19:52.663307: step 4054, loss 0.112019, acc 0.96875, prec 0.0819548, recall 0.897584
2017-12-10T15:19:52.853970: step 4055, loss 0.571811, acc 0.9375, prec 0.0819683, recall 0.89761
2017-12-10T15:19:53.049008: step 4056, loss 0.162377, acc 0.96875, prec 0.0820068, recall 0.897662
2017-12-10T15:19:53.244462: step 4057, loss 0.081471, acc 0.984375, prec 0.0820892, recall 0.897764
2017-12-10T15:19:53.443680: step 4058, loss 0.114391, acc 0.953125, prec 0.0821046, recall 0.89779
2017-12-10T15:19:53.635502: step 4059, loss 0.0995573, acc 0.96875, prec 0.0821009, recall 0.89779
2017-12-10T15:19:53.835056: step 4060, loss 0.09126, acc 0.96875, prec 0.0821182, recall 0.897816
2017-12-10T15:19:54.029518: step 4061, loss 0.0173113, acc 1, prec 0.0821182, recall 0.897816
2017-12-10T15:19:54.218644: step 4062, loss 0.164529, acc 0.9375, prec 0.0821106, recall 0.897816
2017-12-10T15:19:54.417640: step 4063, loss 0.106982, acc 0.9375, prec 0.0821242, recall 0.897841
2017-12-10T15:19:54.611147: step 4064, loss 0.049803, acc 0.96875, prec 0.0821204, recall 0.897841
2017-12-10T15:19:54.805739: step 4065, loss 0.0259817, acc 1, prec 0.0821204, recall 0.897841
2017-12-10T15:19:55.002122: step 4066, loss 0.3189, acc 0.921875, prec 0.082111, recall 0.897841
2017-12-10T15:19:55.199752: step 4067, loss 0.0780285, acc 0.96875, prec 0.0821072, recall 0.897841
2017-12-10T15:19:55.394068: step 4068, loss 0.0631146, acc 0.984375, prec 0.0821474, recall 0.897893
2017-12-10T15:19:55.589149: step 4069, loss 0.339667, acc 0.984375, prec 0.0821666, recall 0.897918
2017-12-10T15:19:55.788458: step 4070, loss 0.11695, acc 0.9375, prec 0.0821801, recall 0.897944
2017-12-10T15:19:55.983056: step 4071, loss 0.806554, acc 1, prec 0.0822012, recall 0.897969
2017-12-10T15:19:56.178723: step 4072, loss 0.0384671, acc 0.984375, prec 0.0821993, recall 0.897969
2017-12-10T15:19:56.379258: step 4073, loss 0.0270486, acc 0.984375, prec 0.0821974, recall 0.897969
2017-12-10T15:19:56.578110: step 4074, loss 0.358239, acc 0.9375, prec 0.082211, recall 0.897995
2017-12-10T15:19:56.774951: step 4075, loss 0.10318, acc 0.96875, prec 0.0822282, recall 0.898021
2017-12-10T15:19:56.971703: step 4076, loss 0.166711, acc 0.96875, prec 0.0822455, recall 0.898046
2017-12-10T15:19:57.172413: step 4077, loss 0.066429, acc 0.96875, prec 0.0822417, recall 0.898046
2017-12-10T15:19:57.366682: step 4078, loss 0.512629, acc 0.859375, prec 0.0822669, recall 0.898097
2017-12-10T15:19:57.566177: step 4079, loss 0.0727879, acc 0.953125, prec 0.0822612, recall 0.898097
2017-12-10T15:19:57.760816: step 4080, loss 0.143309, acc 0.90625, prec 0.0822499, recall 0.898097
2017-12-10T15:19:57.953509: step 4081, loss 0.0917416, acc 0.96875, prec 0.0822461, recall 0.898097
2017-12-10T15:19:58.149669: step 4082, loss 0.151696, acc 0.9375, prec 0.0822596, recall 0.898123
2017-12-10T15:19:58.345275: step 4083, loss 0.0723692, acc 0.984375, prec 0.0822788, recall 0.898148
2017-12-10T15:19:58.537045: step 4084, loss 0.168585, acc 0.90625, prec 0.0822675, recall 0.898148
2017-12-10T15:19:58.739732: step 4085, loss 0.195208, acc 0.90625, prec 0.0822561, recall 0.898148
2017-12-10T15:19:58.936587: step 4086, loss 0.0170682, acc 1, prec 0.0822982, recall 0.898199
2017-12-10T15:19:59.135044: step 4087, loss 0.123061, acc 0.96875, prec 0.0823155, recall 0.898225
2017-12-10T15:19:59.343457: step 4088, loss 0.0177537, acc 1, prec 0.0823365, recall 0.89825
2017-12-10T15:19:59.534733: step 4089, loss 0.0867935, acc 0.984375, prec 0.0823556, recall 0.898275
2017-12-10T15:19:59.730359: step 4090, loss 0.229439, acc 0.96875, prec 0.0823729, recall 0.898301
2017-12-10T15:19:59.926095: step 4091, loss 0.0479618, acc 0.984375, prec 0.082371, recall 0.898301
2017-12-10T15:20:00.116076: step 4092, loss 0.0405355, acc 0.984375, prec 0.0823691, recall 0.898301
2017-12-10T15:20:00.314370: step 4093, loss 0.0807634, acc 0.96875, prec 0.0823864, recall 0.898326
2017-12-10T15:20:00.511321: step 4094, loss 0.101918, acc 0.96875, prec 0.0823826, recall 0.898326
2017-12-10T15:20:00.710773: step 4095, loss 0.379686, acc 0.96875, prec 0.0824209, recall 0.898377
2017-12-10T15:20:00.906584: step 4096, loss 0.0247376, acc 0.984375, prec 0.082419, recall 0.898377
2017-12-10T15:20:01.106147: step 4097, loss 0.0113512, acc 1, prec 0.082419, recall 0.898377
2017-12-10T15:20:01.302201: step 4098, loss 0.0721293, acc 0.953125, prec 0.0824133, recall 0.898377
2017-12-10T15:20:01.509777: step 4099, loss 0.0334933, acc 0.984375, prec 0.0824114, recall 0.898377
2017-12-10T15:20:01.703824: step 4100, loss 0.0650526, acc 0.984375, prec 0.0824095, recall 0.898377
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_1/1512936356/checkpoints/model-4100

2017-12-10T15:20:03.031049: step 4101, loss 0.0993406, acc 0.984375, prec 0.0824287, recall 0.898402
2017-12-10T15:20:03.227743: step 4102, loss 0.726587, acc 1, prec 0.0824707, recall 0.898453
2017-12-10T15:20:03.425291: step 4103, loss 0.049579, acc 0.96875, prec 0.0824669, recall 0.898453
2017-12-10T15:20:03.621583: step 4104, loss 0.7847, acc 0.984375, prec 0.082486, recall 0.898478
2017-12-10T15:20:03.817783: step 4105, loss 0.162016, acc 0.96875, prec 0.0825033, recall 0.898504
2017-12-10T15:20:04.014351: step 4106, loss 0.265809, acc 0.921875, prec 0.0824938, recall 0.898504
2017-12-10T15:20:04.212361: step 4107, loss 0.191219, acc 0.9375, prec 0.0824863, recall 0.898504
2017-12-10T15:20:04.411270: step 4108, loss 0.117532, acc 0.953125, prec 0.0825226, recall 0.898554
2017-12-10T15:20:04.602068: step 4109, loss 0.296613, acc 0.90625, prec 0.0825113, recall 0.898554
2017-12-10T15:20:04.798088: step 4110, loss 0.281621, acc 0.890625, prec 0.08254, recall 0.898605
2017-12-10T15:20:04.992098: step 4111, loss 0.234705, acc 0.96875, prec 0.0825573, recall 0.89863
2017-12-10T15:20:05.185059: step 4112, loss 0.114025, acc 0.96875, prec 0.0825535, recall 0.89863
2017-12-10T15:20:05.382828: step 4113, loss 2.37154, acc 0.9375, prec 0.0825688, recall 0.898432
2017-12-10T15:20:05.583320: step 4114, loss 0.146295, acc 0.921875, prec 0.0825594, recall 0.898432
2017-12-10T15:20:05.776869: step 4115, loss 0.213676, acc 0.9375, prec 0.0825728, recall 0.898457
2017-12-10T15:20:05.967692: step 4116, loss 0.700699, acc 0.84375, prec 0.0825539, recall 0.898457
2017-12-10T15:20:06.161493: step 4117, loss 0.79564, acc 0.8125, prec 0.0825942, recall 0.898533
2017-12-10T15:20:06.354478: step 4118, loss 0.328967, acc 0.890625, prec 0.0826019, recall 0.898558
2017-12-10T15:20:06.550436: step 4119, loss 0.396182, acc 0.828125, prec 0.0825812, recall 0.898558
2017-12-10T15:20:06.744866: step 4120, loss 0.216677, acc 0.9375, prec 0.0825946, recall 0.898583
2017-12-10T15:20:06.941211: step 4121, loss 0.333804, acc 0.875, prec 0.0826214, recall 0.898634
2017-12-10T15:20:07.140735: step 4122, loss 0.218775, acc 0.9375, prec 0.0826139, recall 0.898634
2017-12-10T15:20:07.335736: step 4123, loss 0.283675, acc 0.921875, prec 0.0826254, recall 0.898659
2017-12-10T15:20:07.527428: step 4124, loss 0.310374, acc 0.921875, prec 0.0826159, recall 0.898659
2017-12-10T15:20:07.723179: step 4125, loss 0.336197, acc 0.828125, prec 0.0826371, recall 0.898709
2017-12-10T15:20:07.917548: step 4126, loss 0.353586, acc 0.921875, prec 0.0826277, recall 0.898709
2017-12-10T15:20:08.112220: step 4127, loss 0.0896314, acc 0.953125, prec 0.0826848, recall 0.898784
2017-12-10T15:20:08.311952: step 4128, loss 0.404885, acc 0.875, prec 0.0826906, recall 0.89881
2017-12-10T15:20:08.505167: step 4129, loss 0.35173, acc 0.890625, prec 0.0827193, recall 0.89886
2017-12-10T15:20:08.700347: step 4130, loss 0.327693, acc 0.875, prec 0.0827042, recall 0.89886
2017-12-10T15:20:08.896172: step 4131, loss 0.392756, acc 0.90625, prec 0.0827347, recall 0.89891
2017-12-10T15:20:09.094582: step 4132, loss 0.169507, acc 0.953125, prec 0.08275, recall 0.898935
2017-12-10T15:20:09.288970: step 4133, loss 0.175336, acc 0.96875, prec 0.082788, recall 0.898985
2017-12-10T15:20:09.485330: step 4134, loss 0.109915, acc 0.9375, prec 0.0828014, recall 0.89901
2017-12-10T15:20:09.679928: step 4135, loss 0.268793, acc 0.921875, prec 0.0828129, recall 0.899035
2017-12-10T15:20:09.872997: step 4136, loss 0.0768113, acc 0.984375, prec 0.0828319, recall 0.89906
2017-12-10T15:20:10.067481: step 4137, loss 0.0219064, acc 1, prec 0.0828319, recall 0.89906
2017-12-10T15:20:10.263878: step 4138, loss 0.0524103, acc 0.984375, prec 0.08283, recall 0.89906
2017-12-10T15:20:10.461106: step 4139, loss 0.0619587, acc 0.96875, prec 0.0828471, recall 0.899085
2017-12-10T15:20:10.655518: step 4140, loss 0.0995589, acc 0.921875, prec 0.0828377, recall 0.899085
2017-12-10T15:20:10.848499: step 4141, loss 0.124911, acc 0.953125, prec 0.082832, recall 0.899085
2017-12-10T15:20:11.043823: step 4142, loss 0.0140807, acc 1, prec 0.082832, recall 0.899085
2017-12-10T15:20:11.238381: step 4143, loss 0.0755789, acc 0.96875, prec 0.0828282, recall 0.899085
2017-12-10T15:20:11.436218: step 4144, loss 0.0831033, acc 1, prec 0.0828491, recall 0.89911
2017-12-10T15:20:11.635185: step 4145, loss 0.467943, acc 0.984375, prec 0.0828681, recall 0.899135
2017-12-10T15:20:11.836937: step 4146, loss 0.0213383, acc 1, prec 0.082889, recall 0.89916
2017-12-10T15:20:12.032727: step 4147, loss 0.0196222, acc 0.984375, prec 0.0828872, recall 0.89916
2017-12-10T15:20:12.227642: step 4148, loss 0.0370669, acc 0.984375, prec 0.0828853, recall 0.89916
2017-12-10T15:20:12.426791: step 4149, loss 1.69105, acc 0.984375, prec 0.0828853, recall 0.898937
2017-12-10T15:20:12.624766: step 4150, loss 0.12138, acc 0.96875, prec 0.0828815, recall 0.898937
2017-12-10T15:20:12.816961: step 4151, loss 0.0305048, acc 0.984375, prec 0.0828796, recall 0.898937
2017-12-10T15:20:13.014887: step 4152, loss 0.230149, acc 0.984375, prec 0.0828986, recall 0.898962
2017-12-10T15:20:13.213011: step 4153, loss 0.0815764, acc 0.96875, prec 0.0829157, recall 0.898987
2017-12-10T15:20:13.405875: step 4154, loss 0.247662, acc 0.984375, prec 0.0829556, recall 0.899037
2017-12-10T15:20:13.604272: step 4155, loss 0.24637, acc 0.984375, prec 0.0829955, recall 0.899087
2017-12-10T15:20:13.800016: step 4156, loss 0.0641184, acc 0.96875, prec 0.0829917, recall 0.899087
2017-12-10T15:20:13.996847: step 4157, loss 0.977824, acc 0.953125, prec 0.0830069, recall 0.899112
2017-12-10T15:20:14.203817: step 4158, loss 0.16686, acc 0.953125, prec 0.083043, recall 0.899162
2017-12-10T15:20:14.406794: step 4159, loss 0.351859, acc 0.921875, prec 0.0830336, recall 0.899162
2017-12-10T15:20:14.602012: step 4160, loss 0.335133, acc 0.90625, prec 0.0830431, recall 0.899187
2017-12-10T15:20:14.799068: step 4161, loss 0.167559, acc 0.921875, prec 0.0830963, recall 0.899261
2017-12-10T15:20:14.996114: step 4162, loss 0.24169, acc 0.921875, prec 0.0831494, recall 0.899335
2017-12-10T15:20:15.188198: step 4163, loss 0.459737, acc 0.859375, prec 0.0831741, recall 0.899385
2017-12-10T15:20:15.379599: step 4164, loss 0.149259, acc 0.953125, prec 0.0831893, recall 0.89941
2017-12-10T15:20:15.574585: step 4165, loss 0.223416, acc 0.90625, prec 0.0831779, recall 0.89941
2017-12-10T15:20:15.770774: step 4166, loss 0.406917, acc 0.84375, prec 0.083159, recall 0.89941
2017-12-10T15:20:15.968690: step 4167, loss 0.79084, acc 0.734375, prec 0.0831477, recall 0.899434
2017-12-10T15:20:16.162619: step 4168, loss 0.460317, acc 0.859375, prec 0.0831515, recall 0.899459
2017-12-10T15:20:16.354621: step 4169, loss 0.306175, acc 0.875, prec 0.0831572, recall 0.899484
2017-12-10T15:20:16.553231: step 4170, loss 0.15947, acc 0.9375, prec 0.0831705, recall 0.899509
2017-12-10T15:20:16.747665: step 4171, loss 0.24255, acc 0.9375, prec 0.0831838, recall 0.899533
2017-12-10T15:20:16.945101: step 4172, loss 0.683292, acc 0.96875, prec 0.0832008, recall 0.899558
2017-12-10T15:20:17.145054: step 4173, loss 0.230112, acc 0.875, prec 0.0832065, recall 0.899583
2017-12-10T15:20:17.340383: step 4174, loss 0.294902, acc 0.9375, prec 0.0832198, recall 0.899607
2017-12-10T15:20:17.535288: step 4175, loss 0.130934, acc 0.953125, prec 0.0832349, recall 0.899632
2017-12-10T15:20:17.730729: step 4176, loss 0.273049, acc 0.90625, prec 0.0832236, recall 0.899632
2017-12-10T15:20:17.922935: step 4177, loss 0.228334, acc 0.953125, prec 0.0832596, recall 0.899681
2017-12-10T15:20:18.125410: step 4178, loss 0.237083, acc 0.953125, prec 0.0833163, recall 0.899755
2017-12-10T15:20:18.317396: step 4179, loss 0.175664, acc 0.953125, prec 0.0833314, recall 0.899779
2017-12-10T15:20:18.512609: step 4180, loss 0.180629, acc 0.9375, prec 0.0833239, recall 0.899779
2017-12-10T15:20:18.707577: step 4181, loss 0.0385842, acc 0.984375, prec 0.083322, recall 0.899779
2017-12-10T15:20:18.903718: step 4182, loss 0.188288, acc 0.96875, prec 0.0833182, recall 0.899779
2017-12-10T15:20:19.096535: step 4183, loss 0.0751202, acc 0.96875, prec 0.0833352, recall 0.899804
2017-12-10T15:20:19.291578: step 4184, loss 0.285336, acc 0.953125, prec 0.0833296, recall 0.899804
2017-12-10T15:20:19.486452: step 4185, loss 2.46514, acc 0.953125, prec 0.0833674, recall 0.899633
2017-12-10T15:20:19.689578: step 4186, loss 0.188975, acc 0.96875, prec 0.0834052, recall 0.899682
2017-12-10T15:20:19.883853: step 4187, loss 0.057671, acc 0.984375, prec 0.0834241, recall 0.899706
2017-12-10T15:20:20.076212: step 4188, loss 0.186338, acc 0.9375, prec 0.0834165, recall 0.899706
2017-12-10T15:20:20.270946: step 4189, loss 0.153272, acc 0.9375, prec 0.0834089, recall 0.899706
2017-12-10T15:20:20.472802: step 4190, loss 0.163374, acc 0.953125, prec 0.0834033, recall 0.899706
2017-12-10T15:20:20.665678: step 4191, loss 0.0998374, acc 0.96875, prec 0.083441, recall 0.899755
2017-12-10T15:20:20.858194: step 4192, loss 0.103922, acc 0.953125, prec 0.0834354, recall 0.899755
2017-12-10T15:20:21.048112: step 4193, loss 0.224002, acc 0.921875, prec 0.0834259, recall 0.899755
2017-12-10T15:20:21.244431: step 4194, loss 0.100786, acc 0.9375, prec 0.0834599, recall 0.899804
2017-12-10T15:20:21.443229: step 4195, loss 0.166572, acc 0.9375, prec 0.0834731, recall 0.899829
2017-12-10T15:20:21.636298: step 4196, loss 0.10088, acc 0.953125, prec 0.0834674, recall 0.899829
2017-12-10T15:20:21.831803: step 4197, loss 0.219752, acc 0.921875, prec 0.083458, recall 0.899829
2017-12-10T15:20:22.026336: step 4198, loss 0.138832, acc 0.96875, prec 0.0834957, recall 0.899878
2017-12-10T15:20:22.224313: step 4199, loss 0.168975, acc 0.96875, prec 0.0834919, recall 0.899878
2017-12-10T15:20:22.416835: step 4200, loss 0.0908724, acc 0.96875, prec 0.0835089, recall 0.899902
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_1/1512936356/checkpoints/model-4200

2017-12-10T15:20:23.629563: step 4201, loss 0.0316185, acc 0.984375, prec 0.083507, recall 0.899902
2017-12-10T15:20:23.827748: step 4202, loss 0.190453, acc 0.96875, prec 0.0835448, recall 0.899951
2017-12-10T15:20:24.020900: step 4203, loss 0.0906143, acc 0.984375, prec 0.0835636, recall 0.899976
2017-12-10T15:20:24.219517: step 4204, loss 0.142734, acc 0.96875, prec 0.0836014, recall 0.900024
2017-12-10T15:20:24.415286: step 4205, loss 0.0237852, acc 1, prec 0.0836221, recall 0.900049
2017-12-10T15:20:24.612982: step 4206, loss 0.208846, acc 0.9375, prec 0.083656, recall 0.900097
2017-12-10T15:20:24.810816: step 4207, loss 1.72747, acc 0.953125, prec 0.0836523, recall 0.899878
2017-12-10T15:20:25.007511: step 4208, loss 0.108475, acc 0.984375, prec 0.0836711, recall 0.899903
2017-12-10T15:20:25.200317: step 4209, loss 0.0922386, acc 0.96875, prec 0.0836673, recall 0.899903
2017-12-10T15:20:25.399917: step 4210, loss 0.0476013, acc 0.96875, prec 0.0836635, recall 0.899903
2017-12-10T15:20:25.590587: step 4211, loss 0.0144804, acc 1, prec 0.0836635, recall 0.899903
2017-12-10T15:20:25.786268: step 4212, loss 0.0881454, acc 0.984375, prec 0.0836824, recall 0.899927
2017-12-10T15:20:25.984145: step 4213, loss 0.29211, acc 0.984375, prec 0.0837012, recall 0.899951
2017-12-10T15:20:26.179344: step 4214, loss 0.165974, acc 0.953125, prec 0.0837163, recall 0.899976
2017-12-10T15:20:26.372874: step 4215, loss 0.301281, acc 0.921875, prec 0.0837483, recall 0.900024
2017-12-10T15:20:26.568884: step 4216, loss 0.0975339, acc 0.953125, prec 0.0837426, recall 0.900024
2017-12-10T15:20:26.767555: step 4217, loss 0.187911, acc 0.9375, prec 0.0837558, recall 0.900049
2017-12-10T15:20:26.962481: step 4218, loss 0.18044, acc 0.96875, prec 0.0837727, recall 0.900073
2017-12-10T15:20:27.156833: step 4219, loss 0.216573, acc 0.953125, prec 0.08385, recall 0.90017
2017-12-10T15:20:27.354140: step 4220, loss 0.0666145, acc 0.96875, prec 0.0838462, recall 0.90017
2017-12-10T15:20:27.546095: step 4221, loss 0.103536, acc 0.953125, prec 0.0838612, recall 0.900194
2017-12-10T15:20:27.741993: step 4222, loss 0.233723, acc 0.921875, prec 0.0838932, recall 0.900243
2017-12-10T15:20:27.944387: step 4223, loss 0.117682, acc 0.953125, prec 0.0838875, recall 0.900243
2017-12-10T15:20:28.138114: step 4224, loss 0.0531269, acc 0.984375, prec 0.0838856, recall 0.900243
2017-12-10T15:20:28.330489: step 4225, loss 0.20551, acc 0.921875, prec 0.0838968, recall 0.900267
2017-12-10T15:20:28.529362: step 4226, loss 0.165084, acc 0.9375, prec 0.0838892, recall 0.900267
2017-12-10T15:20:28.724357: step 4227, loss 0.268184, acc 0.90625, prec 0.0838985, recall 0.900291
2017-12-10T15:20:28.914909: step 4228, loss 0.301354, acc 0.9375, prec 0.0839531, recall 0.900364
2017-12-10T15:20:29.107178: step 4229, loss 0.134762, acc 0.9375, prec 0.0839455, recall 0.900364
2017-12-10T15:20:29.323128: step 4230, loss 0.176978, acc 1, prec 0.0840076, recall 0.900436
2017-12-10T15:20:29.519767: step 4231, loss 0.351295, acc 0.953125, prec 0.0840226, recall 0.90046
2017-12-10T15:20:29.712476: step 4232, loss 0.404482, acc 0.9375, prec 0.0840357, recall 0.900484
2017-12-10T15:20:29.913782: step 4233, loss 0.243582, acc 0.96875, prec 0.0840526, recall 0.900508
2017-12-10T15:20:30.115713: step 4234, loss 0.0306319, acc 0.984375, prec 0.0840507, recall 0.900508
2017-12-10T15:20:30.316716: step 4235, loss 0.086271, acc 0.984375, prec 0.0840488, recall 0.900508
2017-12-10T15:20:30.506992: step 4236, loss 0.185843, acc 0.953125, prec 0.0840431, recall 0.900508
2017-12-10T15:20:30.698578: step 4237, loss 0.0723507, acc 0.96875, prec 0.08406, recall 0.900532
2017-12-10T15:20:30.895213: step 4238, loss 0.173545, acc 0.984375, prec 0.0840995, recall 0.900581
2017-12-10T15:20:31.089338: step 4239, loss 0.179747, acc 0.96875, prec 0.0840957, recall 0.900581
2017-12-10T15:20:31.285244: step 4240, loss 1.10503, acc 0.9375, prec 0.0841088, recall 0.900605
2017-12-10T15:20:31.483303: step 4241, loss 0.590541, acc 0.96875, prec 0.0841463, recall 0.900653
2017-12-10T15:20:31.686893: step 4242, loss 0.083651, acc 0.96875, prec 0.0841425, recall 0.900653
2017-12-10T15:20:31.880706: step 4243, loss 0.0667252, acc 0.984375, prec 0.0842027, recall 0.900725
2017-12-10T15:20:32.079728: step 4244, loss 0.270229, acc 0.953125, prec 0.0842383, recall 0.900773
2017-12-10T15:20:32.278310: step 4245, loss 0.435135, acc 1, prec 0.0842797, recall 0.90082
2017-12-10T15:20:32.469719: step 4246, loss 0.147499, acc 0.9375, prec 0.0842721, recall 0.90082
2017-12-10T15:20:32.661355: step 4247, loss 0.111417, acc 0.9375, prec 0.0842645, recall 0.90082
2017-12-10T15:20:32.859598: step 4248, loss 0.191874, acc 0.96875, prec 0.0842607, recall 0.90082
2017-12-10T15:20:33.050960: step 4249, loss 0.306372, acc 0.96875, prec 0.0842775, recall 0.900844
2017-12-10T15:20:33.252502: step 4250, loss 0.295522, acc 0.9375, prec 0.0842906, recall 0.900868
2017-12-10T15:20:33.445486: step 4251, loss 0.259134, acc 0.9375, prec 0.0843036, recall 0.900892
2017-12-10T15:20:33.641212: step 4252, loss 0.0597435, acc 0.96875, prec 0.0842998, recall 0.900892
2017-12-10T15:20:33.838704: step 4253, loss 0.145636, acc 0.9375, prec 0.0842922, recall 0.900892
2017-12-10T15:20:34.038082: step 4254, loss 0.0760428, acc 0.96875, prec 0.0843091, recall 0.900916
2017-12-10T15:20:34.230340: step 4255, loss 0.338833, acc 0.90625, prec 0.084339, recall 0.900964
2017-12-10T15:20:34.422979: step 4256, loss 0.27028, acc 0.96875, prec 0.0843558, recall 0.900988
2017-12-10T15:20:34.616808: step 4257, loss 0.17463, acc 0.96875, prec 0.0843933, recall 0.901035
2017-12-10T15:20:34.811116: step 4258, loss 0.0950065, acc 0.953125, prec 0.0843876, recall 0.901035
2017-12-10T15:20:35.005880: step 4259, loss 0.17374, acc 0.96875, prec 0.0844045, recall 0.901059
2017-12-10T15:20:35.201589: step 4260, loss 0.361177, acc 0.984375, prec 0.0844438, recall 0.901107
2017-12-10T15:20:35.398624: step 4261, loss 0.259299, acc 0.921875, prec 0.0844343, recall 0.901107
2017-12-10T15:20:35.599810: step 4262, loss 0.822657, acc 0.9375, prec 0.0844473, recall 0.901131
2017-12-10T15:20:35.796245: step 4263, loss 0.293662, acc 0.9375, prec 0.0844604, recall 0.901154
2017-12-10T15:20:35.989450: step 4264, loss 0.070415, acc 1, prec 0.084481, recall 0.901178
2017-12-10T15:20:36.187260: step 4265, loss 0.220452, acc 0.921875, prec 0.0844715, recall 0.901178
2017-12-10T15:20:36.378262: step 4266, loss 0.08978, acc 0.96875, prec 0.0844883, recall 0.901202
2017-12-10T15:20:36.573400: step 4267, loss 0.225575, acc 0.9375, prec 0.0845013, recall 0.901226
2017-12-10T15:20:36.769444: step 4268, loss 0.242676, acc 0.921875, prec 0.0844918, recall 0.901226
2017-12-10T15:20:36.979850: step 4269, loss 1.02991, acc 0.984375, prec 0.0845312, recall 0.901273
2017-12-10T15:20:37.180982: step 4270, loss 0.321036, acc 0.859375, prec 0.084514, recall 0.901273
2017-12-10T15:20:37.376565: step 4271, loss 0.211976, acc 0.953125, prec 0.0845495, recall 0.901321
2017-12-10T15:20:37.573432: step 4272, loss 0.27058, acc 0.890625, prec 0.0845568, recall 0.901344
2017-12-10T15:20:37.763415: step 4273, loss 0.382549, acc 0.890625, prec 0.0845435, recall 0.901344
2017-12-10T15:20:37.956656: step 4274, loss 0.403515, acc 0.84375, prec 0.0845245, recall 0.901344
2017-12-10T15:20:38.149526: step 4275, loss 0.276408, acc 0.90625, prec 0.0845337, recall 0.901368
2017-12-10T15:20:38.342890: step 4276, loss 0.134173, acc 0.9375, prec 0.0845467, recall 0.901392
2017-12-10T15:20:38.538374: step 4277, loss 0.268126, acc 0.921875, prec 0.0845577, recall 0.901415
2017-12-10T15:20:38.740763: step 4278, loss 0.163387, acc 0.9375, prec 0.0845913, recall 0.901462
2017-12-10T15:20:38.933550: step 4279, loss 0.133389, acc 0.953125, prec 0.0845856, recall 0.901462
2017-12-10T15:20:39.129567: step 4280, loss 0.183704, acc 0.9375, prec 0.0845986, recall 0.901486
2017-12-10T15:20:39.333262: step 4281, loss 0.15484, acc 0.96875, prec 0.0845948, recall 0.901486
2017-12-10T15:20:39.530871: step 4282, loss 0.142209, acc 0.953125, prec 0.0846303, recall 0.901533
2017-12-10T15:20:39.724354: step 4283, loss 0.0680147, acc 0.953125, prec 0.0846451, recall 0.901557
2017-12-10T15:20:39.920788: step 4284, loss 0.160183, acc 0.984375, prec 0.0846432, recall 0.901557
2017-12-10T15:20:40.114633: step 4285, loss 0.109684, acc 0.953125, prec 0.0846375, recall 0.901557
2017-12-10T15:20:40.309108: step 4286, loss 0.434592, acc 0.953125, prec 0.0846318, recall 0.901557
2017-12-10T15:20:40.502892: step 4287, loss 0.124575, acc 0.984375, prec 0.0846711, recall 0.901604
2017-12-10T15:20:40.697102: step 4288, loss 0.0793496, acc 0.96875, prec 0.0847084, recall 0.901651
2017-12-10T15:20:40.893500: step 4289, loss 0.128878, acc 0.953125, prec 0.0847233, recall 0.901675
2017-12-10T15:20:41.090650: step 4290, loss 0.0999904, acc 0.953125, prec 0.0847176, recall 0.901675
2017-12-10T15:20:41.287443: step 4291, loss 0.169001, acc 0.984375, prec 0.0847568, recall 0.901722
2017-12-10T15:20:41.486731: step 4292, loss 0.411165, acc 0.953125, prec 0.0847922, recall 0.901769
2017-12-10T15:20:41.679852: step 4293, loss 0.159945, acc 0.9375, prec 0.0847846, recall 0.901769
2017-12-10T15:20:41.874948: step 4294, loss 0.0765485, acc 0.96875, prec 0.0847808, recall 0.901769
2017-12-10T15:20:42.077075: step 4295, loss 0.0476975, acc 0.984375, prec 0.0847789, recall 0.901769
2017-12-10T15:20:42.272493: step 4296, loss 0.171564, acc 0.953125, prec 0.0847732, recall 0.901769
2017-12-10T15:20:42.468980: step 4297, loss 0.0870608, acc 0.96875, prec 0.0847899, recall 0.901792
2017-12-10T15:20:42.669511: step 4298, loss 0.128707, acc 0.953125, prec 0.0847842, recall 0.901792
2017-12-10T15:20:42.864527: step 4299, loss 0.0109774, acc 1, prec 0.0848048, recall 0.901816
2017-12-10T15:20:43.063932: step 4300, loss 6.62849, acc 0.96875, prec 0.0848253, recall 0.901408
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_1/1512936356/checkpoints/model-4300

2017-12-10T15:20:44.301410: step 4301, loss 0.37392, acc 0.96875, prec 0.0848626, recall 0.901456
2017-12-10T15:20:44.499466: step 4302, loss 0.111068, acc 0.984375, prec 0.0848813, recall 0.901479
2017-12-10T15:20:44.688762: step 4303, loss 0.145009, acc 0.9375, prec 0.0848737, recall 0.901479
2017-12-10T15:20:44.886372: step 4304, loss 0.0579976, acc 0.984375, prec 0.0848923, recall 0.901502
2017-12-10T15:20:45.079089: step 4305, loss 0.568024, acc 0.90625, prec 0.084922, recall 0.901549
2017-12-10T15:20:45.276397: step 4306, loss 0.147062, acc 0.953125, prec 0.0849573, recall 0.901596
2017-12-10T15:20:45.481683: step 4307, loss 0.427332, acc 0.859375, prec 0.0849607, recall 0.90162
2017-12-10T15:20:45.676328: step 4308, loss 0.229953, acc 0.9375, prec 0.0849531, recall 0.90162
2017-12-10T15:20:45.873339: step 4309, loss 0.34738, acc 0.859375, prec 0.0849975, recall 0.90169
2017-12-10T15:20:46.068712: step 4310, loss 0.499929, acc 0.890625, prec 0.0849842, recall 0.90169
2017-12-10T15:20:46.262827: step 4311, loss 0.141687, acc 0.9375, prec 0.0849766, recall 0.90169
2017-12-10T15:20:46.460683: step 4312, loss 0.534339, acc 0.875, prec 0.0849613, recall 0.90169
2017-12-10T15:20:46.653796: step 4313, loss 0.344788, acc 0.890625, prec 0.0849685, recall 0.901713
2017-12-10T15:20:46.849641: step 4314, loss 0.269568, acc 0.90625, prec 0.0849776, recall 0.901737
2017-12-10T15:20:47.038816: step 4315, loss 0.456762, acc 0.859375, prec 0.0850015, recall 0.901784
2017-12-10T15:20:47.232469: step 4316, loss 0.440265, acc 0.90625, prec 0.08499, recall 0.901784
2017-12-10T15:20:47.424752: step 4317, loss 0.209078, acc 0.9375, prec 0.0849824, recall 0.901784
2017-12-10T15:20:47.619746: step 4318, loss 0.177755, acc 0.953125, prec 0.0850382, recall 0.901854
2017-12-10T15:20:47.815217: step 4319, loss 0.447069, acc 0.875, prec 0.0850435, recall 0.901877
2017-12-10T15:20:48.011111: step 4320, loss 0.108707, acc 0.953125, prec 0.0850377, recall 0.901877
2017-12-10T15:20:48.207832: step 4321, loss 0.0803147, acc 0.953125, prec 0.0850525, recall 0.9019
2017-12-10T15:20:48.405640: step 4322, loss 0.347603, acc 0.90625, prec 0.0851026, recall 0.90197
2017-12-10T15:20:48.600694: step 4323, loss 0.277367, acc 0.9375, prec 0.085095, recall 0.90197
2017-12-10T15:20:48.795230: step 4324, loss 0.148733, acc 0.953125, prec 0.0851097, recall 0.901993
2017-12-10T15:20:48.998015: step 4325, loss 0.0571546, acc 0.953125, prec 0.085104, recall 0.901993
2017-12-10T15:20:49.193843: step 4326, loss 0.149799, acc 0.953125, prec 0.0851393, recall 0.90204
2017-12-10T15:20:49.388062: step 4327, loss 0.0337686, acc 1, prec 0.0851393, recall 0.90204
2017-12-10T15:20:49.585891: step 4328, loss 0.137095, acc 0.984375, prec 0.0851783, recall 0.902086
2017-12-10T15:20:49.782302: step 4329, loss 0.286175, acc 0.984375, prec 0.0852173, recall 0.902133
2017-12-10T15:20:49.989739: step 4330, loss 0.325328, acc 0.953125, prec 0.0852321, recall 0.902156
2017-12-10T15:20:50.191238: step 4331, loss 0.643286, acc 0.953125, prec 0.0852878, recall 0.902225
2017-12-10T15:20:50.387908: step 4332, loss 0.0279534, acc 0.984375, prec 0.0853064, recall 0.902249
2017-12-10T15:20:50.586504: step 4333, loss 0.272609, acc 0.984375, prec 0.0853249, recall 0.902272
2017-12-10T15:20:50.785556: step 4334, loss 0.151897, acc 0.953125, prec 0.0853397, recall 0.902295
2017-12-10T15:20:50.979540: step 4335, loss 0.0526649, acc 0.984375, prec 0.0853582, recall 0.902318
2017-12-10T15:20:51.175314: step 4336, loss 0.286692, acc 0.953125, prec 0.0853934, recall 0.902364
2017-12-10T15:20:51.371509: step 4337, loss 0.55681, acc 0.953125, prec 0.0854081, recall 0.902387
2017-12-10T15:20:51.567535: step 4338, loss 0.205149, acc 0.9375, prec 0.0854005, recall 0.902387
2017-12-10T15:20:51.772129: step 4339, loss 0.0799722, acc 0.96875, prec 0.0854376, recall 0.902433
2017-12-10T15:20:51.965802: step 4340, loss 0.213266, acc 0.953125, prec 0.0854319, recall 0.902433
2017-12-10T15:20:52.160976: step 4341, loss 0.273677, acc 0.921875, prec 0.0854223, recall 0.902433
2017-12-10T15:20:52.359915: step 4342, loss 0.0978364, acc 0.96875, prec 0.0854185, recall 0.902433
2017-12-10T15:20:52.558551: step 4343, loss 0.116103, acc 0.953125, prec 0.0854128, recall 0.902433
2017-12-10T15:20:52.754004: step 4344, loss 0.0940067, acc 0.984375, prec 0.0854108, recall 0.902433
2017-12-10T15:20:52.948015: step 4345, loss 0.168328, acc 0.984375, prec 0.0854498, recall 0.902479
2017-12-10T15:20:53.147056: step 4346, loss 0.184466, acc 0.921875, prec 0.0854403, recall 0.902479
2017-12-10T15:20:53.343120: step 4347, loss 0.366125, acc 0.9375, prec 0.0854326, recall 0.902479
2017-12-10T15:20:53.540565: step 4348, loss 0.219076, acc 0.984375, prec 0.0854512, recall 0.902502
2017-12-10T15:20:53.738900: step 4349, loss 0.189585, acc 0.953125, prec 0.0854863, recall 0.902548
2017-12-10T15:20:53.937611: step 4350, loss 0.167692, acc 0.984375, prec 0.0855049, recall 0.902571
2017-12-10T15:20:54.134741: step 4351, loss 0.0759661, acc 0.953125, prec 0.0854991, recall 0.902571
2017-12-10T15:20:54.334635: step 4352, loss 0.115633, acc 0.984375, prec 0.0855176, recall 0.902594
2017-12-10T15:20:54.530149: step 4353, loss 0.132364, acc 0.96875, prec 0.0855138, recall 0.902594
2017-12-10T15:20:54.723320: step 4354, loss 2.21111, acc 0.96875, prec 0.0855323, recall 0.902405
2017-12-10T15:20:54.924674: step 4355, loss 0.165717, acc 0.96875, prec 0.0855898, recall 0.902474
2017-12-10T15:20:55.115002: step 4356, loss 0.0503811, acc 0.96875, prec 0.085586, recall 0.902474
2017-12-10T15:20:55.314812: step 4357, loss 0.190277, acc 0.953125, prec 0.0855803, recall 0.902474
2017-12-10T15:20:55.509457: step 4358, loss 0.0687751, acc 0.984375, prec 0.0855783, recall 0.902474
2017-12-10T15:20:55.701508: step 4359, loss 0.0721304, acc 0.984375, prec 0.0855969, recall 0.902496
2017-12-10T15:20:55.899818: step 4360, loss 0.076421, acc 0.984375, prec 0.0856154, recall 0.902519
2017-12-10T15:20:56.093372: step 4361, loss 0.136939, acc 0.921875, prec 0.0856058, recall 0.902519
2017-12-10T15:20:56.291630: step 4362, loss 0.129117, acc 0.953125, prec 0.0856409, recall 0.902565
2017-12-10T15:20:56.487532: step 4363, loss 0.24009, acc 0.9375, prec 0.0856537, recall 0.902588
2017-12-10T15:20:56.680552: step 4364, loss 0.287882, acc 0.875, prec 0.0856384, recall 0.902588
2017-12-10T15:20:56.874567: step 4365, loss 0.210538, acc 0.90625, prec 0.0856269, recall 0.902588
2017-12-10T15:20:57.070146: step 4366, loss 0.280541, acc 0.890625, prec 0.0856135, recall 0.902588
2017-12-10T15:20:57.265070: step 4367, loss 0.195622, acc 0.96875, prec 0.0856097, recall 0.902588
2017-12-10T15:20:57.457214: step 4368, loss 0.314704, acc 0.90625, prec 0.0856187, recall 0.902611
2017-12-10T15:20:57.650836: step 4369, loss 0.148364, acc 0.953125, prec 0.0856333, recall 0.902634
2017-12-10T15:20:57.841233: step 4370, loss 0.550408, acc 0.984375, prec 0.0856518, recall 0.902657
2017-12-10T15:20:58.042698: step 4371, loss 0.0907709, acc 0.953125, prec 0.0856461, recall 0.902657
2017-12-10T15:20:58.243067: step 4372, loss 0.175182, acc 0.984375, prec 0.0857054, recall 0.902726
2017-12-10T15:20:58.439761: step 4373, loss 0.374015, acc 0.921875, prec 0.0857162, recall 0.902748
2017-12-10T15:20:58.639327: step 4374, loss 0.0849838, acc 0.96875, prec 0.0857328, recall 0.902771
2017-12-10T15:20:58.833983: step 4375, loss 0.174058, acc 0.9375, prec 0.0857659, recall 0.902817
2017-12-10T15:20:59.028064: step 4376, loss 0.132933, acc 0.921875, prec 0.0857563, recall 0.902817
2017-12-10T15:20:59.224097: step 4377, loss 0.386132, acc 0.9375, prec 0.0857691, recall 0.90284
2017-12-10T15:20:59.420849: step 4378, loss 0.321959, acc 0.96875, prec 0.0857856, recall 0.902862
2017-12-10T15:20:59.616171: step 4379, loss 0.190841, acc 0.953125, prec 0.0858003, recall 0.902885
2017-12-10T15:20:59.812654: step 4380, loss 0.356261, acc 0.9375, prec 0.085813, recall 0.902908
2017-12-10T15:21:00.005459: step 4381, loss 0.0521414, acc 0.984375, prec 0.0858315, recall 0.902931
2017-12-10T15:21:00.200233: step 4382, loss 0.0429188, acc 1, prec 0.0858518, recall 0.902954
2017-12-10T15:21:00.399849: step 4383, loss 0.243236, acc 0.9375, prec 0.0858646, recall 0.902976
2017-12-10T15:21:00.598243: step 4384, loss 0.291472, acc 0.9375, prec 0.0858569, recall 0.902976
2017-12-10T15:21:00.792973: step 4385, loss 0.129417, acc 0.953125, prec 0.0858715, recall 0.902999
2017-12-10T15:21:00.986250: step 4386, loss 1.68501, acc 0.90625, prec 0.0858823, recall 0.90281
2017-12-10T15:21:01.190421: step 4387, loss 0.11767, acc 0.9375, prec 0.0858747, recall 0.90281
2017-12-10T15:21:01.390115: step 4388, loss 0.305171, acc 0.921875, prec 0.0858651, recall 0.90281
2017-12-10T15:21:01.588232: step 4389, loss 0.107993, acc 0.953125, prec 0.0858594, recall 0.90281
2017-12-10T15:21:01.783295: step 4390, loss 3.05353, acc 0.953125, prec 0.0859166, recall 0.902667
2017-12-10T15:21:01.983527: step 4391, loss 0.513943, acc 0.9375, prec 0.0859497, recall 0.902713
2017-12-10T15:21:02.178581: step 4392, loss 0.235362, acc 0.9375, prec 0.085942, recall 0.902713
2017-12-10T15:21:02.372520: step 4393, loss 0.444616, acc 0.875, prec 0.0859267, recall 0.902713
2017-12-10T15:21:02.568504: step 4394, loss 0.433992, acc 0.890625, prec 0.085954, recall 0.902758
2017-12-10T15:21:02.768605: step 4395, loss 0.376153, acc 0.90625, prec 0.0859425, recall 0.902758
2017-12-10T15:21:02.962509: step 4396, loss 0.294826, acc 0.875, prec 0.0859883, recall 0.902826
2017-12-10T15:21:03.155632: step 4397, loss 0.429559, acc 0.828125, prec 0.0859672, recall 0.902826
2017-12-10T15:21:03.348728: step 4398, loss 0.383093, acc 0.890625, prec 0.0859538, recall 0.902826
2017-12-10T15:21:03.541869: step 4399, loss 0.219894, acc 0.921875, prec 0.0859443, recall 0.902826
2017-12-10T15:21:03.736671: step 4400, loss 0.357338, acc 0.921875, prec 0.0859957, recall 0.902894
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_1/1512936356/checkpoints/model-4400

2017-12-10T15:21:04.938997: step 4401, loss 0.432837, acc 0.875, prec 0.0859804, recall 0.902894
2017-12-10T15:21:05.128461: step 4402, loss 0.230707, acc 0.921875, prec 0.0859912, recall 0.902917
2017-12-10T15:21:05.324403: step 4403, loss 0.309146, acc 0.890625, prec 0.0859778, recall 0.902917
2017-12-10T15:21:05.525090: step 4404, loss 0.279824, acc 0.890625, prec 0.0859847, recall 0.90294
2017-12-10T15:21:05.720276: step 4405, loss 0.469444, acc 0.875, prec 0.08601, recall 0.902985
2017-12-10T15:21:05.913706: step 4406, loss 0.239928, acc 0.9375, prec 0.0860024, recall 0.902985
2017-12-10T15:21:06.110500: step 4407, loss 0.135876, acc 0.9375, prec 0.0859948, recall 0.902985
2017-12-10T15:21:06.302925: step 4408, loss 0.229073, acc 0.90625, prec 0.0860036, recall 0.903008
2017-12-10T15:21:06.503848: step 4409, loss 0.106726, acc 0.96875, prec 0.0859998, recall 0.903008
2017-12-10T15:21:06.701000: step 4410, loss 0.158204, acc 0.96875, prec 0.0860365, recall 0.903053
2017-12-10T15:21:06.897951: step 4411, loss 0.0816076, acc 0.953125, prec 0.0860714, recall 0.903098
2017-12-10T15:21:07.098437: step 4412, loss 0.0671939, acc 0.953125, prec 0.0861062, recall 0.903143
2017-12-10T15:21:07.295985: step 4413, loss 0.124558, acc 0.984375, prec 0.0861246, recall 0.903166
2017-12-10T15:21:07.499414: step 4414, loss 0.074826, acc 0.96875, prec 0.0861614, recall 0.903211
2017-12-10T15:21:07.697958: step 4415, loss 0.105727, acc 0.953125, prec 0.0861556, recall 0.903211
2017-12-10T15:21:07.895847: step 4416, loss 0.108125, acc 0.96875, prec 0.0861518, recall 0.903211
2017-12-10T15:21:08.088310: step 4417, loss 0.147274, acc 1, prec 0.0861924, recall 0.903256
2017-12-10T15:21:08.288317: step 4418, loss 0.336229, acc 0.984375, prec 0.0862107, recall 0.903278
2017-12-10T15:21:08.483471: step 4419, loss 0.24188, acc 0.921875, prec 0.0862012, recall 0.903278
2017-12-10T15:21:08.681879: step 4420, loss 0.0661035, acc 0.96875, prec 0.0861973, recall 0.903278
2017-12-10T15:21:08.882864: step 4421, loss 0.103286, acc 0.96875, prec 0.0861935, recall 0.903278
2017-12-10T15:21:09.082253: step 4422, loss 0.0558014, acc 0.984375, prec 0.0862119, recall 0.903301
2017-12-10T15:21:09.281696: step 4423, loss 0.0642184, acc 1, prec 0.0862524, recall 0.903346
2017-12-10T15:21:09.475470: step 4424, loss 0.0433887, acc 0.96875, prec 0.0862486, recall 0.903346
2017-12-10T15:21:09.670322: step 4425, loss 1.77359, acc 0.96875, prec 0.0862669, recall 0.903158
2017-12-10T15:21:09.873108: step 4426, loss 0.382718, acc 0.96875, prec 0.0863037, recall 0.903203
2017-12-10T15:21:10.076865: step 4427, loss 0.135827, acc 0.96875, prec 0.0862998, recall 0.903203
2017-12-10T15:21:10.273181: step 4428, loss 0.433708, acc 0.96875, prec 0.0863568, recall 0.903271
2017-12-10T15:21:10.470857: step 4429, loss 0.059227, acc 0.953125, prec 0.086351, recall 0.903271
2017-12-10T15:21:10.664981: step 4430, loss 0.255028, acc 0.921875, prec 0.0863415, recall 0.903271
2017-12-10T15:21:10.862803: step 4431, loss 0.272485, acc 0.984375, prec 0.0864206, recall 0.90336
2017-12-10T15:21:11.065558: step 4432, loss 0.311939, acc 0.90625, prec 0.0864293, recall 0.903383
2017-12-10T15:21:11.264570: step 4433, loss 0.157079, acc 0.9375, prec 0.0864419, recall 0.903405
2017-12-10T15:21:11.461637: step 4434, loss 0.420321, acc 0.96875, prec 0.0864583, recall 0.903428
2017-12-10T15:21:11.662423: step 4435, loss 0.146973, acc 0.953125, prec 0.0864526, recall 0.903428
2017-12-10T15:21:11.856489: step 4436, loss 0.0800058, acc 0.984375, prec 0.0864709, recall 0.90345
2017-12-10T15:21:12.048885: step 4437, loss 0.268242, acc 0.9375, prec 0.0865037, recall 0.903495
2017-12-10T15:21:12.249484: step 4438, loss 0.239938, acc 0.9375, prec 0.0865568, recall 0.903562
2017-12-10T15:21:12.444796: step 4439, loss 0.293385, acc 0.890625, prec 0.0865838, recall 0.903606
2017-12-10T15:21:12.640098: step 4440, loss 0.295046, acc 0.9375, prec 0.0866166, recall 0.903651
2017-12-10T15:21:12.840804: step 4441, loss 0.338875, acc 0.890625, prec 0.0866032, recall 0.903651
2017-12-10T15:21:13.043208: step 4442, loss 0.32648, acc 0.90625, prec 0.0866119, recall 0.903673
2017-12-10T15:21:13.237114: step 4443, loss 0.185189, acc 0.96875, prec 0.0866081, recall 0.903673
2017-12-10T15:21:13.435259: step 4444, loss 0.0791719, acc 0.984375, prec 0.0866264, recall 0.903695
2017-12-10T15:21:13.628639: step 4445, loss 0.203773, acc 0.953125, prec 0.0866611, recall 0.90374
2017-12-10T15:21:13.823762: step 4446, loss 0.0601805, acc 0.984375, prec 0.0866794, recall 0.903762
2017-12-10T15:21:14.024468: step 4447, loss 0.208841, acc 0.9375, prec 0.0866717, recall 0.903762
2017-12-10T15:21:14.224286: step 4448, loss 0.172109, acc 0.953125, prec 0.0867064, recall 0.903806
2017-12-10T15:21:14.425824: step 4449, loss 0.360744, acc 0.953125, prec 0.0867208, recall 0.903828
2017-12-10T15:21:14.623622: step 4450, loss 0.181475, acc 0.9375, prec 0.0867131, recall 0.903828
2017-12-10T15:21:14.822982: step 4451, loss 0.00403287, acc 1, prec 0.0867131, recall 0.903828
2017-12-10T15:21:15.019390: step 4452, loss 0.145932, acc 0.96875, prec 0.0867295, recall 0.903851
2017-12-10T15:21:15.215890: step 4453, loss 1.08648, acc 0.953125, prec 0.0867641, recall 0.903895
2017-12-10T15:21:15.418770: step 4454, loss 0.282636, acc 0.96875, prec 0.0867805, recall 0.903917
2017-12-10T15:21:15.614525: step 4455, loss 0.082662, acc 0.96875, prec 0.0867969, recall 0.903939
2017-12-10T15:21:15.805411: step 4456, loss 0.213015, acc 0.953125, prec 0.0867911, recall 0.903939
2017-12-10T15:21:16.001077: step 4457, loss 0.196346, acc 0.96875, prec 0.0868075, recall 0.903961
2017-12-10T15:21:16.194959: step 4458, loss 0.334882, acc 0.921875, prec 0.0867979, recall 0.903961
2017-12-10T15:21:16.392324: step 4459, loss 0.0719555, acc 0.9375, prec 0.0867902, recall 0.903961
2017-12-10T15:21:16.587545: step 4460, loss 0.325034, acc 0.890625, prec 0.0867768, recall 0.903961
2017-12-10T15:21:16.780429: step 4461, loss 0.288014, acc 0.9375, prec 0.0868095, recall 0.904006
2017-12-10T15:21:16.976506: step 4462, loss 0.0854022, acc 0.953125, prec 0.0868239, recall 0.904028
2017-12-10T15:21:17.171679: step 4463, loss 0.171481, acc 0.96875, prec 0.0868604, recall 0.904072
2017-12-10T15:21:17.368177: step 4464, loss 0.290514, acc 0.9375, prec 0.0868729, recall 0.904094
2017-12-10T15:21:17.570325: step 4465, loss 0.1534, acc 0.921875, prec 0.0868633, recall 0.904094
2017-12-10T15:21:17.765917: step 4466, loss 0.0495475, acc 0.96875, prec 0.0868595, recall 0.904094
2017-12-10T15:21:17.964048: step 4467, loss 0.354703, acc 0.921875, prec 0.0868499, recall 0.904094
2017-12-10T15:21:18.160090: step 4468, loss 0.562747, acc 0.953125, prec 0.0868643, recall 0.904116
2017-12-10T15:21:18.356125: step 4469, loss 0.0250248, acc 1, prec 0.0868845, recall 0.904138
2017-12-10T15:21:18.552862: step 4470, loss 0.178921, acc 0.9375, prec 0.0868768, recall 0.904138
2017-12-10T15:21:18.748929: step 4471, loss 0.13912, acc 0.953125, prec 0.0868711, recall 0.904138
2017-12-10T15:21:18.943568: step 4472, loss 0.132038, acc 0.984375, prec 0.0869296, recall 0.904204
2017-12-10T15:21:19.122062: step 4473, loss 0.184114, acc 0.980392, prec 0.0869479, recall 0.904226
2017-12-10T15:21:19.328073: step 4474, loss 0.0725754, acc 0.96875, prec 0.086944, recall 0.904226
2017-12-10T15:21:19.525250: step 4475, loss 0.251603, acc 0.96875, prec 0.0869604, recall 0.904248
2017-12-10T15:21:19.725150: step 4476, loss 0.116159, acc 1, prec 0.087041, recall 0.904336
2017-12-10T15:21:19.923246: step 4477, loss 0.229004, acc 0.953125, prec 0.0870756, recall 0.90438
2017-12-10T15:21:20.120285: step 4478, loss 0.205227, acc 0.953125, prec 0.0871101, recall 0.904424
2017-12-10T15:21:20.313603: step 4479, loss 0.3244, acc 0.9375, prec 0.0871024, recall 0.904424
2017-12-10T15:21:20.516589: step 4480, loss 0.0608704, acc 0.984375, prec 0.0871206, recall 0.904445
2017-12-10T15:21:20.716466: step 4481, loss 0.190006, acc 0.953125, prec 0.0871753, recall 0.904511
2017-12-10T15:21:20.913549: step 4482, loss 0.0381591, acc 1, prec 0.0872357, recall 0.904577
2017-12-10T15:21:21.108860: step 4483, loss 0.0525962, acc 0.984375, prec 0.087254, recall 0.904598
2017-12-10T15:21:21.308621: step 4484, loss 0.0349259, acc 1, prec 0.0872942, recall 0.904642
2017-12-10T15:21:21.503185: step 4485, loss 0.84476, acc 0.984375, prec 0.0873326, recall 0.904686
2017-12-10T15:21:21.702781: step 4486, loss 0.119352, acc 0.953125, prec 0.0873268, recall 0.904686
2017-12-10T15:21:21.898401: step 4487, loss 0.142153, acc 0.953125, prec 0.087321, recall 0.904686
2017-12-10T15:21:22.096040: step 4488, loss 0.14714, acc 0.953125, prec 0.0873152, recall 0.904686
2017-12-10T15:21:22.290032: step 4489, loss 0.102376, acc 0.953125, prec 0.0873497, recall 0.904729
2017-12-10T15:21:22.485930: step 4490, loss 0.0441586, acc 0.984375, prec 0.0873478, recall 0.904729
2017-12-10T15:21:22.677616: step 4491, loss 0.220508, acc 0.921875, prec 0.0873382, recall 0.904729
2017-12-10T15:21:22.874221: step 4492, loss 0.306859, acc 0.96875, prec 0.0873343, recall 0.904729
2017-12-10T15:21:23.069894: step 4493, loss 0.0589965, acc 0.984375, prec 0.0873324, recall 0.904729
2017-12-10T15:21:23.265602: step 4494, loss 0.0246233, acc 1, prec 0.0873726, recall 0.904773
2017-12-10T15:21:23.466042: step 4495, loss 0.0894026, acc 0.984375, prec 0.0873707, recall 0.904773
2017-12-10T15:21:23.660466: step 4496, loss 4.00838, acc 0.953125, prec 0.0873669, recall 0.904566
2017-12-10T15:21:23.859970: step 4497, loss 0.169065, acc 0.953125, prec 0.0873611, recall 0.904566
2017-12-10T15:21:24.053852: step 4498, loss 0.616933, acc 0.890625, prec 0.087408, recall 0.904632
2017-12-10T15:21:24.249019: step 4499, loss 0.145033, acc 0.953125, prec 0.0874022, recall 0.904632
2017-12-10T15:21:24.442962: step 4500, loss 0.189548, acc 0.9375, prec 0.0874146, recall 0.904653
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_1/1512936356/checkpoints/model-4500

2017-12-10T15:21:25.653545: step 4501, loss 0.19604, acc 0.9375, prec 0.0874069, recall 0.904653
2017-12-10T15:21:25.846924: step 4502, loss 0.275596, acc 0.921875, prec 0.0873973, recall 0.904653
2017-12-10T15:21:26.044358: step 4503, loss 0.214859, acc 0.921875, prec 0.0874479, recall 0.904718
2017-12-10T15:21:26.238239: step 4504, loss 0.572263, acc 0.84375, prec 0.0874488, recall 0.90474
2017-12-10T15:21:26.434272: step 4505, loss 0.435431, acc 0.828125, prec 0.0874477, recall 0.904762
2017-12-10T15:21:26.628916: step 4506, loss 0.36683, acc 0.90625, prec 0.0874562, recall 0.904784
2017-12-10T15:21:26.824724: step 4507, loss 0.295512, acc 0.890625, prec 0.0875231, recall 0.90487
2017-12-10T15:21:27.018370: step 4508, loss 0.196402, acc 0.9375, prec 0.0875355, recall 0.904892
2017-12-10T15:21:27.210791: step 4509, loss 0.574817, acc 0.78125, prec 0.0875286, recall 0.904914
2017-12-10T15:21:27.412324: step 4510, loss 0.388092, acc 0.90625, prec 0.0875371, recall 0.904935
2017-12-10T15:21:27.608369: step 4511, loss 0.299052, acc 0.890625, prec 0.0875236, recall 0.904935
2017-12-10T15:21:27.804019: step 4512, loss 0.470399, acc 0.828125, prec 0.0875827, recall 0.905022
2017-12-10T15:21:27.996723: step 4513, loss 0.346123, acc 0.890625, prec 0.0876094, recall 0.905065
2017-12-10T15:21:28.194805: step 4514, loss 0.184642, acc 0.953125, prec 0.0876237, recall 0.905086
2017-12-10T15:21:28.389912: step 4515, loss 0.201609, acc 0.953125, prec 0.0876379, recall 0.905108
2017-12-10T15:21:28.584977: step 4516, loss 0.0505413, acc 1, prec 0.087678, recall 0.905151
2017-12-10T15:21:28.780638: step 4517, loss 0.109954, acc 0.96875, prec 0.0876942, recall 0.905172
2017-12-10T15:21:28.977445: step 4518, loss 0.357521, acc 0.9375, prec 0.0877066, recall 0.905194
2017-12-10T15:21:29.181930: step 4519, loss 0.0583054, acc 0.984375, prec 0.0877447, recall 0.905237
2017-12-10T15:21:29.383303: step 4520, loss 0.044216, acc 0.96875, prec 0.0877609, recall 0.905258
2017-12-10T15:21:29.579085: step 4521, loss 0.152162, acc 0.96875, prec 0.0877571, recall 0.905258
2017-12-10T15:21:29.773140: step 4522, loss 0.0743464, acc 0.984375, prec 0.0877551, recall 0.905258
2017-12-10T15:21:29.971626: step 4523, loss 0.0433297, acc 0.984375, prec 0.0877532, recall 0.905258
2017-12-10T15:21:30.168680: step 4524, loss 0.407815, acc 1, prec 0.0877733, recall 0.90528
2017-12-10T15:21:30.369190: step 4525, loss 0.101408, acc 0.984375, prec 0.0877713, recall 0.90528
2017-12-10T15:21:30.570805: step 4526, loss 0.0438764, acc 0.96875, prec 0.0877675, recall 0.90528
2017-12-10T15:21:30.771825: step 4527, loss 0.0212448, acc 0.984375, prec 0.0877856, recall 0.905301
2017-12-10T15:21:30.972098: step 4528, loss 0.0907742, acc 1, prec 0.0878056, recall 0.905323
2017-12-10T15:21:31.175588: step 4529, loss 0.0954432, acc 0.984375, prec 0.0878237, recall 0.905344
2017-12-10T15:21:31.377379: step 4530, loss 0.0341867, acc 1, prec 0.0878438, recall 0.905366
2017-12-10T15:21:31.575233: step 4531, loss 0.12168, acc 0.96875, prec 0.0878399, recall 0.905366
2017-12-10T15:21:31.772179: step 4532, loss 0.0412477, acc 0.984375, prec 0.087838, recall 0.905366
2017-12-10T15:21:31.967293: step 4533, loss 0.0842255, acc 0.953125, prec 0.0878522, recall 0.905387
2017-12-10T15:21:32.160688: step 4534, loss 0.0466522, acc 0.984375, prec 0.0878503, recall 0.905387
2017-12-10T15:21:32.357422: step 4535, loss 0.0166347, acc 1, prec 0.0878703, recall 0.905408
2017-12-10T15:21:32.555438: step 4536, loss 0.00814943, acc 1, prec 0.0878904, recall 0.90543
2017-12-10T15:21:32.754181: step 4537, loss 0.522293, acc 0.984375, prec 0.0879085, recall 0.905451
2017-12-10T15:21:32.958382: step 4538, loss 0.167061, acc 0.96875, prec 0.0879046, recall 0.905451
2017-12-10T15:21:33.155392: step 4539, loss 0.0941871, acc 1, prec 0.0879447, recall 0.905494
2017-12-10T15:21:33.354310: step 4540, loss 0.0781089, acc 0.96875, prec 0.0879408, recall 0.905494
2017-12-10T15:21:33.549645: step 4541, loss 0.0709554, acc 1, prec 0.0879608, recall 0.905515
2017-12-10T15:21:33.750926: step 4542, loss 0.0336632, acc 0.984375, prec 0.0879789, recall 0.905537
2017-12-10T15:21:33.946410: step 4543, loss 0.0700295, acc 0.984375, prec 0.088017, recall 0.905579
2017-12-10T15:21:34.145643: step 4544, loss 0.136729, acc 0.96875, prec 0.0880132, recall 0.905579
2017-12-10T15:21:34.339681: step 4545, loss 0.149184, acc 0.953125, prec 0.0880274, recall 0.905601
2017-12-10T15:21:34.541211: step 4546, loss 0.0570352, acc 0.984375, prec 0.0880255, recall 0.905601
2017-12-10T15:21:34.737622: step 4547, loss 0.0199962, acc 1, prec 0.0880455, recall 0.905622
2017-12-10T15:21:34.931437: step 4548, loss 0.128366, acc 0.984375, prec 0.0880636, recall 0.905643
2017-12-10T15:21:35.131347: step 4549, loss 1.1935, acc 0.96875, prec 0.0880997, recall 0.905686
2017-12-10T15:21:35.336330: step 4550, loss 0.168799, acc 0.984375, prec 0.0881178, recall 0.905707
2017-12-10T15:21:35.534409: step 4551, loss 0.0978415, acc 0.96875, prec 0.088134, recall 0.905728
2017-12-10T15:21:35.729398: step 4552, loss 0.00876831, acc 1, prec 0.088154, recall 0.90575
2017-12-10T15:21:35.927830: step 4553, loss 0.162769, acc 0.96875, prec 0.0881501, recall 0.90575
2017-12-10T15:21:36.130258: step 4554, loss 0.478496, acc 0.9375, prec 0.0881624, recall 0.905771
2017-12-10T15:21:36.328299: step 4555, loss 0.0152524, acc 1, prec 0.0881624, recall 0.905771
2017-12-10T15:21:36.522974: step 4556, loss 0.146866, acc 0.953125, prec 0.0881766, recall 0.905792
2017-12-10T15:21:36.720971: step 4557, loss 0.0145962, acc 1, prec 0.0881766, recall 0.905792
2017-12-10T15:21:36.915445: step 4558, loss 0.142269, acc 0.953125, prec 0.0881908, recall 0.905813
2017-12-10T15:21:37.115557: step 4559, loss 0.123638, acc 0.96875, prec 0.0882469, recall 0.905877
2017-12-10T15:21:37.311893: step 4560, loss 0.361225, acc 0.96875, prec 0.088263, recall 0.905898
2017-12-10T15:21:37.507703: step 4561, loss 0.398779, acc 0.890625, prec 0.0882695, recall 0.905919
2017-12-10T15:21:37.701298: step 4562, loss 0.0611197, acc 0.984375, prec 0.0883075, recall 0.905962
2017-12-10T15:21:37.894991: step 4563, loss 0.0139815, acc 1, prec 0.0883275, recall 0.905983
2017-12-10T15:21:38.095642: step 4564, loss 0.12831, acc 0.953125, prec 0.0883217, recall 0.905983
2017-12-10T15:21:38.297671: step 4565, loss 1.51182, acc 0.984375, prec 0.0883417, recall 0.9058
2017-12-10T15:21:38.503138: step 4566, loss 0.0201557, acc 0.984375, prec 0.0883398, recall 0.9058
2017-12-10T15:21:38.695531: step 4567, loss 0.0858332, acc 0.96875, prec 0.0883559, recall 0.905822
2017-12-10T15:21:38.889816: step 4568, loss 0.149353, acc 0.96875, prec 0.088352, recall 0.905822
2017-12-10T15:21:39.082683: step 4569, loss 0.148385, acc 0.9375, prec 0.0883443, recall 0.905822
2017-12-10T15:21:39.278017: step 4570, loss 0.0865072, acc 0.96875, prec 0.0883404, recall 0.905822
2017-12-10T15:21:39.474261: step 4571, loss 0.295123, acc 0.890625, prec 0.0883268, recall 0.905822
2017-12-10T15:21:39.666149: step 4572, loss 0.451622, acc 0.890625, prec 0.0883333, recall 0.905843
2017-12-10T15:21:39.861997: step 4573, loss 0.321454, acc 0.90625, prec 0.0883816, recall 0.905906
2017-12-10T15:21:40.057238: step 4574, loss 0.122995, acc 0.953125, prec 0.0883758, recall 0.905906
2017-12-10T15:21:40.254277: step 4575, loss 0.165628, acc 0.9375, prec 0.088368, recall 0.905906
2017-12-10T15:21:40.452345: step 4576, loss 0.0506595, acc 0.984375, prec 0.0883661, recall 0.905906
2017-12-10T15:21:40.650304: step 4577, loss 0.401924, acc 0.90625, prec 0.0883545, recall 0.905906
2017-12-10T15:21:40.846773: step 4578, loss 0.167143, acc 0.96875, prec 0.0883506, recall 0.905906
2017-12-10T15:21:41.044580: step 4579, loss 0.148063, acc 0.9375, prec 0.0884027, recall 0.90597
2017-12-10T15:21:41.244258: step 4580, loss 0.0208526, acc 1, prec 0.0884227, recall 0.905991
2017-12-10T15:21:41.437877: step 4581, loss 0.250218, acc 0.90625, prec 0.0884311, recall 0.906012
2017-12-10T15:21:41.629963: step 4582, loss 1.16889, acc 0.953125, prec 0.0884671, recall 0.905851
2017-12-10T15:21:41.835516: step 4583, loss 0.0469769, acc 0.984375, prec 0.0884652, recall 0.905851
2017-12-10T15:21:42.035887: step 4584, loss 0.109095, acc 0.96875, prec 0.0884812, recall 0.905872
2017-12-10T15:21:42.231644: step 4585, loss 0.280568, acc 0.9375, prec 0.0884735, recall 0.905872
2017-12-10T15:21:42.423641: step 4586, loss 0.243049, acc 0.96875, prec 0.0884896, recall 0.905893
2017-12-10T15:21:42.620812: step 4587, loss 0.174071, acc 0.96875, prec 0.0884857, recall 0.905893
2017-12-10T15:21:42.815478: step 4588, loss 0.118417, acc 0.984375, prec 0.0885037, recall 0.905914
2017-12-10T15:21:43.009696: step 4589, loss 0.107487, acc 0.953125, prec 0.0885178, recall 0.905935
2017-12-10T15:21:43.210637: step 4590, loss 0.0927282, acc 0.984375, prec 0.0885558, recall 0.905977
2017-12-10T15:21:43.402541: step 4591, loss 0.119712, acc 0.9375, prec 0.0885879, recall 0.906019
2017-12-10T15:21:43.596800: step 4592, loss 0.119663, acc 0.96875, prec 0.0885841, recall 0.906019
2017-12-10T15:21:43.795801: step 4593, loss 0.0640256, acc 0.984375, prec 0.0886021, recall 0.90604
2017-12-10T15:21:43.995596: step 4594, loss 0.0510229, acc 0.96875, prec 0.088658, recall 0.906103
2017-12-10T15:21:44.194995: step 4595, loss 0.0627458, acc 0.984375, prec 0.088676, recall 0.906124
2017-12-10T15:21:44.403362: step 4596, loss 0.159956, acc 0.953125, prec 0.0886702, recall 0.906124
2017-12-10T15:21:44.600023: step 4597, loss 0.301666, acc 0.984375, prec 0.0886882, recall 0.906145
2017-12-10T15:21:44.797316: step 4598, loss 0.28228, acc 0.96875, prec 0.088764, recall 0.906229
2017-12-10T15:21:44.995281: step 4599, loss 0.111542, acc 0.953125, prec 0.0887582, recall 0.906229
2017-12-10T15:21:45.189175: step 4600, loss 0.191459, acc 0.96875, prec 0.0887941, recall 0.906271
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_1/1512936356/checkpoints/model-4600

2017-12-10T15:21:46.517342: step 4601, loss 0.0532545, acc 0.984375, prec 0.0888121, recall 0.906292
2017-12-10T15:21:46.711311: step 4602, loss 0.23395, acc 0.953125, prec 0.0888063, recall 0.906292
2017-12-10T15:21:46.906517: step 4603, loss 0.366168, acc 0.953125, prec 0.0888005, recall 0.906292
2017-12-10T15:21:47.103331: step 4604, loss 0.0622181, acc 0.96875, prec 0.0887966, recall 0.906292
2017-12-10T15:21:47.303452: step 4605, loss 1.24307, acc 0.9375, prec 0.0887908, recall 0.90609
2017-12-10T15:21:47.502730: step 4606, loss 0.0747036, acc 0.96875, prec 0.0888068, recall 0.906111
2017-12-10T15:21:47.697206: step 4607, loss 0.472401, acc 0.90625, prec 0.0887952, recall 0.906111
2017-12-10T15:21:47.889251: step 4608, loss 0.0299328, acc 1, prec 0.088835, recall 0.906152
2017-12-10T15:21:48.086107: step 4609, loss 0.201515, acc 0.953125, prec 0.0888292, recall 0.906152
2017-12-10T15:21:48.286469: step 4610, loss 0.315087, acc 0.9375, prec 0.0888413, recall 0.906173
2017-12-10T15:21:48.480913: step 4611, loss 0.202131, acc 0.9375, prec 0.0888534, recall 0.906194
2017-12-10T15:21:48.675610: step 4612, loss 0.206902, acc 0.9375, prec 0.0888855, recall 0.906236
2017-12-10T15:21:48.871191: step 4613, loss 0.174934, acc 0.9375, prec 0.0888976, recall 0.906257
2017-12-10T15:21:49.066140: step 4614, loss 0.37507, acc 0.9375, prec 0.0889297, recall 0.906299
2017-12-10T15:21:49.259171: step 4615, loss 0.310222, acc 0.96875, prec 0.0889656, recall 0.90634
2017-12-10T15:21:49.455098: step 4616, loss 0.199457, acc 0.9375, prec 0.0889777, recall 0.906361
2017-12-10T15:21:49.648945: step 4617, loss 0.171324, acc 0.9375, prec 0.0889898, recall 0.906382
2017-12-10T15:21:49.843087: step 4618, loss 0.114736, acc 0.953125, prec 0.0890238, recall 0.906424
2017-12-10T15:21:50.039865: step 4619, loss 0.0468389, acc 1, prec 0.0890635, recall 0.906465
2017-12-10T15:21:50.233529: step 4620, loss 0.403007, acc 0.953125, prec 0.0890975, recall 0.906507
2017-12-10T15:21:50.430309: step 4621, loss 0.240672, acc 0.953125, prec 0.0891115, recall 0.906528
2017-12-10T15:21:50.627478: step 4622, loss 0.221363, acc 0.953125, prec 0.0891852, recall 0.90661
2017-12-10T15:21:50.818942: step 4623, loss 0.0284911, acc 1, prec 0.0891852, recall 0.90661
2017-12-10T15:21:51.015515: step 4624, loss 0.0298894, acc 1, prec 0.0892249, recall 0.906652
2017-12-10T15:21:51.211933: step 4625, loss 0.0272722, acc 0.984375, prec 0.089223, recall 0.906652
2017-12-10T15:21:51.410016: step 4626, loss 0.328182, acc 0.90625, prec 0.0892113, recall 0.906652
2017-12-10T15:21:51.610506: step 4627, loss 0.402196, acc 0.953125, prec 0.0892253, recall 0.906673
2017-12-10T15:21:51.808836: step 4628, loss 0.0179086, acc 1, prec 0.0892452, recall 0.906693
2017-12-10T15:21:52.001400: step 4629, loss 0.142864, acc 0.9375, prec 0.0892771, recall 0.906735
2017-12-10T15:21:52.201133: step 4630, loss 0.377133, acc 0.9375, prec 0.0892694, recall 0.906735
2017-12-10T15:21:52.398444: step 4631, loss 0.188826, acc 0.9375, prec 0.0892616, recall 0.906735
2017-12-10T15:21:52.603395: step 4632, loss 0.238848, acc 0.921875, prec 0.0893114, recall 0.906797
2017-12-10T15:21:52.794978: step 4633, loss 0.0581915, acc 0.96875, prec 0.0893075, recall 0.906797
2017-12-10T15:21:52.990406: step 4634, loss 0.0965067, acc 0.953125, prec 0.0893017, recall 0.906797
2017-12-10T15:21:53.186705: step 4635, loss 0.147936, acc 0.96875, prec 0.0893176, recall 0.906817
2017-12-10T15:21:53.386690: step 4636, loss 0.605105, acc 0.96875, prec 0.0893931, recall 0.9069
2017-12-10T15:21:53.585478: step 4637, loss 0.0643434, acc 0.96875, prec 0.0894091, recall 0.90692
2017-12-10T15:21:53.780629: step 4638, loss 0.0186877, acc 1, prec 0.0894289, recall 0.906941
2017-12-10T15:21:53.980331: step 4639, loss 0.0468727, acc 0.984375, prec 0.0894468, recall 0.906961
2017-12-10T15:21:54.179031: step 4640, loss 0.00550775, acc 1, prec 0.0894468, recall 0.906961
2017-12-10T15:21:54.375793: step 4641, loss 0.0554277, acc 0.96875, prec 0.0894826, recall 0.907002
2017-12-10T15:21:54.577368: step 4642, loss 0.123937, acc 0.984375, prec 0.0895005, recall 0.907023
2017-12-10T15:21:54.775375: step 4643, loss 0.014527, acc 1, prec 0.0895005, recall 0.907023
2017-12-10T15:21:54.972794: step 4644, loss 0.135668, acc 0.984375, prec 0.0895184, recall 0.907044
2017-12-10T15:21:55.166167: step 4645, loss 0.174193, acc 0.984375, prec 0.0895363, recall 0.907064
2017-12-10T15:21:55.361505: step 4646, loss 0.192584, acc 0.96875, prec 0.0895522, recall 0.907085
2017-12-10T15:21:55.554703: step 4647, loss 0.072756, acc 0.96875, prec 0.0895682, recall 0.907105
2017-12-10T15:21:55.750641: step 4648, loss 0.0911847, acc 0.984375, prec 0.0896257, recall 0.907166
2017-12-10T15:21:55.947893: step 4649, loss 2.81615, acc 0.953125, prec 0.0897011, recall 0.907048
2017-12-10T15:21:56.143739: step 4650, loss 0.104627, acc 0.96875, prec 0.0897369, recall 0.907089
2017-12-10T15:21:56.341335: step 4651, loss 0.0797542, acc 0.96875, prec 0.089733, recall 0.907089
2017-12-10T15:21:56.534871: step 4652, loss 0.520018, acc 0.890625, prec 0.0897589, recall 0.90713
2017-12-10T15:21:56.733944: step 4653, loss 0.381813, acc 0.890625, prec 0.0897849, recall 0.907171
2017-12-10T15:21:56.928969: step 4654, loss 0.0425053, acc 1, prec 0.0898245, recall 0.907212
2017-12-10T15:21:57.124407: step 4655, loss 0.432002, acc 0.90625, prec 0.0898128, recall 0.907212
2017-12-10T15:21:57.320451: step 4656, loss 0.135215, acc 0.953125, prec 0.0898267, recall 0.907232
2017-12-10T15:21:57.522197: step 4657, loss 0.262351, acc 0.9375, prec 0.0898189, recall 0.907232
2017-12-10T15:21:57.712585: step 4658, loss 0.156725, acc 0.9375, prec 0.0898111, recall 0.907232
2017-12-10T15:21:57.908987: step 4659, loss 0.252222, acc 0.890625, prec 0.0897974, recall 0.907232
2017-12-10T15:21:58.101612: step 4660, loss 0.460465, acc 0.90625, prec 0.0898055, recall 0.907253
2017-12-10T15:21:58.297348: step 4661, loss 0.402603, acc 0.90625, prec 0.0898136, recall 0.907273
2017-12-10T15:21:58.492530: step 4662, loss 0.624134, acc 0.859375, prec 0.0898356, recall 0.907314
2017-12-10T15:21:58.687301: step 4663, loss 0.35296, acc 0.9375, prec 0.0898278, recall 0.907314
2017-12-10T15:21:58.879484: step 4664, loss 0.763614, acc 0.859375, prec 0.0898102, recall 0.907314
2017-12-10T15:21:59.072081: step 4665, loss 0.123525, acc 0.9375, prec 0.0898222, recall 0.907334
2017-12-10T15:21:59.279998: step 4666, loss 0.345189, acc 0.96875, prec 0.0898183, recall 0.907334
2017-12-10T15:21:59.477004: step 4667, loss 0.289449, acc 0.890625, prec 0.0898244, recall 0.907355
2017-12-10T15:21:59.674737: step 4668, loss 0.775746, acc 0.890625, prec 0.0898107, recall 0.907355
2017-12-10T15:21:59.868076: step 4669, loss 0.215777, acc 0.9375, prec 0.0898029, recall 0.907355
2017-12-10T15:22:00.059098: step 4670, loss 0.363893, acc 0.90625, prec 0.089811, recall 0.907375
2017-12-10T15:22:00.254478: step 4671, loss 0.166333, acc 0.890625, prec 0.0898171, recall 0.907395
2017-12-10T15:22:00.453576: step 4672, loss 0.133061, acc 0.984375, prec 0.0898152, recall 0.907395
2017-12-10T15:22:00.644069: step 4673, loss 0.204522, acc 0.9375, prec 0.0898074, recall 0.907395
2017-12-10T15:22:00.841732: step 4674, loss 0.0487395, acc 1, prec 0.0898271, recall 0.907416
2017-12-10T15:22:01.037789: step 4675, loss 0.151994, acc 0.953125, prec 0.089841, recall 0.907436
2017-12-10T15:22:01.237976: step 4676, loss 0.0833045, acc 0.96875, prec 0.0898569, recall 0.907456
2017-12-10T15:22:01.441085: step 4677, loss 0.0868866, acc 0.96875, prec 0.089853, recall 0.907456
2017-12-10T15:22:01.641605: step 4678, loss 0.147071, acc 0.953125, prec 0.0898471, recall 0.907456
2017-12-10T15:22:01.839361: step 4679, loss 0.039518, acc 0.96875, prec 0.0898432, recall 0.907456
2017-12-10T15:22:02.038460: step 4680, loss 0.0375844, acc 0.984375, prec 0.0898413, recall 0.907456
2017-12-10T15:22:02.232061: step 4681, loss 0.0726992, acc 0.96875, prec 0.0898374, recall 0.907456
2017-12-10T15:22:02.430477: step 4682, loss 0.361828, acc 1, prec 0.0898571, recall 0.907476
2017-12-10T15:22:02.625899: step 4683, loss 0.0485802, acc 0.984375, prec 0.0898552, recall 0.907476
2017-12-10T15:22:02.821214: step 4684, loss 0.102371, acc 0.953125, prec 0.0898889, recall 0.907517
2017-12-10T15:22:03.017117: step 4685, loss 0.0238915, acc 1, prec 0.0899284, recall 0.907557
2017-12-10T15:22:03.216210: step 4686, loss 0.166297, acc 1, prec 0.0899481, recall 0.907578
2017-12-10T15:22:03.412194: step 4687, loss 0.0698138, acc 0.984375, prec 0.0899462, recall 0.907578
2017-12-10T15:22:03.609480: step 4688, loss 0.078057, acc 0.96875, prec 0.0899423, recall 0.907578
2017-12-10T15:22:03.810996: step 4689, loss 0.0257466, acc 1, prec 0.0899423, recall 0.907578
2017-12-10T15:22:04.006018: step 4690, loss 0.00515186, acc 1, prec 0.089962, recall 0.907598
2017-12-10T15:22:04.201394: step 4691, loss 0.0100297, acc 1, prec 0.0899818, recall 0.907618
2017-12-10T15:22:04.403642: step 4692, loss 0.311271, acc 1, prec 0.090041, recall 0.907679
2017-12-10T15:22:04.599879: step 4693, loss 0.00461001, acc 1, prec 0.0900805, recall 0.907719
2017-12-10T15:22:04.794914: step 4694, loss 0.462768, acc 0.953125, prec 0.0900944, recall 0.907739
2017-12-10T15:22:04.991699: step 4695, loss 1.85985, acc 0.96875, prec 0.0900924, recall 0.907541
2017-12-10T15:22:05.189413: step 4696, loss 0.035627, acc 0.96875, prec 0.090128, recall 0.907581
2017-12-10T15:22:05.387454: step 4697, loss 0.0757568, acc 0.96875, prec 0.0901241, recall 0.907581
2017-12-10T15:22:05.584901: step 4698, loss 0.179506, acc 0.953125, prec 0.090138, recall 0.907602
2017-12-10T15:22:05.780736: step 4699, loss 0.0816557, acc 0.96875, prec 0.0901538, recall 0.907622
2017-12-10T15:22:05.976012: step 4700, loss 0.100076, acc 0.9375, prec 0.090146, recall 0.907622
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_1/1512936356/checkpoints/model-4700

2017-12-10T15:22:07.178567: step 4701, loss 0.153112, acc 0.953125, prec 0.0901598, recall 0.907642
2017-12-10T15:22:07.374002: step 4702, loss 0.146868, acc 0.953125, prec 0.090154, recall 0.907642
2017-12-10T15:22:07.568115: step 4703, loss 0.0345195, acc 0.984375, prec 0.0901915, recall 0.907682
2017-12-10T15:22:07.768481: step 4704, loss 0.620282, acc 0.890625, prec 0.0901778, recall 0.907682
2017-12-10T15:22:07.966750: step 4705, loss 0.119669, acc 0.953125, prec 0.0901719, recall 0.907682
2017-12-10T15:22:08.168685: step 4706, loss 0.265005, acc 0.9375, prec 0.0901838, recall 0.907702
2017-12-10T15:22:08.363377: step 4707, loss 0.247549, acc 0.875, prec 0.0901879, recall 0.907723
2017-12-10T15:22:08.554187: step 4708, loss 0.0460311, acc 0.984375, prec 0.0902254, recall 0.907763
2017-12-10T15:22:08.748102: step 4709, loss 0.179587, acc 0.9375, prec 0.0902373, recall 0.907783
2017-12-10T15:22:08.944344: step 4710, loss 0.203208, acc 0.9375, prec 0.0902492, recall 0.907803
2017-12-10T15:22:09.151248: step 4711, loss 0.216985, acc 0.953125, prec 0.0902433, recall 0.907803
2017-12-10T15:22:09.345409: step 4712, loss 0.15227, acc 0.90625, prec 0.0902513, recall 0.907823
2017-12-10T15:22:09.539376: step 4713, loss 0.225889, acc 0.9375, prec 0.0902435, recall 0.907823
2017-12-10T15:22:09.732201: step 4714, loss 0.173245, acc 0.9375, prec 0.0902357, recall 0.907823
2017-12-10T15:22:09.927551: step 4715, loss 0.257553, acc 0.9375, prec 0.0902278, recall 0.907823
2017-12-10T15:22:10.124868: step 4716, loss 0.0676311, acc 0.96875, prec 0.0902436, recall 0.907843
2017-12-10T15:22:10.321098: step 4717, loss 0.0813332, acc 0.96875, prec 0.0902397, recall 0.907843
2017-12-10T15:22:10.519721: step 4718, loss 0.0472172, acc 0.984375, prec 0.0902378, recall 0.907843
2017-12-10T15:22:10.717625: step 4719, loss 0.0197204, acc 0.984375, prec 0.0902358, recall 0.907843
2017-12-10T15:22:10.914622: step 4720, loss 0.165466, acc 0.953125, prec 0.0902694, recall 0.907883
2017-12-10T15:22:11.111642: step 4721, loss 0.0483838, acc 0.984375, prec 0.0902674, recall 0.907883
2017-12-10T15:22:11.310663: step 4722, loss 0.102189, acc 0.96875, prec 0.0902635, recall 0.907883
2017-12-10T15:22:11.503978: step 4723, loss 0.0846635, acc 0.984375, prec 0.0903009, recall 0.907923
2017-12-10T15:22:11.705275: step 4724, loss 0.501279, acc 0.921875, prec 0.0903305, recall 0.907963
2017-12-10T15:22:11.899575: step 4725, loss 0.00906275, acc 1, prec 0.0903896, recall 0.908023
2017-12-10T15:22:12.094800: step 4726, loss 0.0168429, acc 1, prec 0.0903896, recall 0.908023
2017-12-10T15:22:12.294076: step 4727, loss 7.47913, acc 0.96875, prec 0.0903877, recall 0.907826
2017-12-10T15:22:12.495917: step 4728, loss 0.0775993, acc 1, prec 0.0904073, recall 0.907846
2017-12-10T15:22:12.690155: step 4729, loss 0.14684, acc 0.96875, prec 0.0904034, recall 0.907846
2017-12-10T15:22:12.891825: step 4730, loss 0.211897, acc 0.9375, prec 0.0904153, recall 0.907866
2017-12-10T15:22:13.085084: step 4731, loss 0.239908, acc 0.9375, prec 0.0904271, recall 0.907886
2017-12-10T15:22:13.282367: step 4732, loss 0.872611, acc 0.875, prec 0.0904312, recall 0.907906
2017-12-10T15:22:13.473914: step 4733, loss 0.397029, acc 0.890625, prec 0.0904372, recall 0.907926
2017-12-10T15:22:13.665619: step 4734, loss 0.0625761, acc 0.984375, prec 0.0904352, recall 0.907926
2017-12-10T15:22:13.862128: step 4735, loss 0.178682, acc 0.921875, prec 0.0904254, recall 0.907926
2017-12-10T15:22:14.058447: step 4736, loss 0.227603, acc 0.953125, prec 0.0904392, recall 0.907946
2017-12-10T15:22:14.257265: step 4737, loss 0.308095, acc 0.90625, prec 0.0904275, recall 0.907946
2017-12-10T15:22:14.456591: step 4738, loss 0.291661, acc 0.90625, prec 0.0904354, recall 0.907966
2017-12-10T15:22:14.651463: step 4739, loss 0.450139, acc 0.84375, prec 0.0904355, recall 0.907986
2017-12-10T15:22:14.846203: step 4740, loss 0.444584, acc 0.84375, prec 0.090416, recall 0.907986
2017-12-10T15:22:15.044325: step 4741, loss 0.984668, acc 0.890625, prec 0.090422, recall 0.908006
2017-12-10T15:22:15.243746: step 4742, loss 0.555583, acc 0.921875, prec 0.0904319, recall 0.908026
2017-12-10T15:22:15.445371: step 4743, loss 0.218123, acc 0.90625, prec 0.0904398, recall 0.908046
2017-12-10T15:22:15.641845: step 4744, loss 0.292835, acc 0.921875, prec 0.0904497, recall 0.908066
2017-12-10T15:22:15.832015: step 4745, loss 0.303199, acc 0.90625, prec 0.0904772, recall 0.908106
2017-12-10T15:22:16.027904: step 4746, loss 0.533692, acc 0.859375, prec 0.0904793, recall 0.908126
2017-12-10T15:22:16.221567: step 4747, loss 0.477016, acc 0.859375, prec 0.0905206, recall 0.908185
2017-12-10T15:22:16.417348: step 4748, loss 0.458989, acc 0.90625, prec 0.0905481, recall 0.908225
2017-12-10T15:22:16.611346: step 4749, loss 0.211979, acc 0.890625, prec 0.0905344, recall 0.908225
2017-12-10T15:22:16.802441: step 4750, loss 0.134304, acc 0.921875, prec 0.0905247, recall 0.908225
2017-12-10T15:22:16.998965: step 4751, loss 0.565301, acc 0.921875, prec 0.0905542, recall 0.908265
2017-12-10T15:22:17.191747: step 4752, loss 0.284749, acc 0.9375, prec 0.0905856, recall 0.908305
2017-12-10T15:22:17.384358: step 4753, loss 0.0515809, acc 0.984375, prec 0.0906032, recall 0.908324
2017-12-10T15:22:17.575673: step 4754, loss 0.229365, acc 0.9375, prec 0.090615, recall 0.908344
2017-12-10T15:22:17.774245: step 4755, loss 0.397758, acc 0.9375, prec 0.0906072, recall 0.908344
2017-12-10T15:22:17.970806: step 4756, loss 0.231772, acc 0.9375, prec 0.0905994, recall 0.908344
2017-12-10T15:22:18.164296: step 4757, loss 0.063191, acc 0.984375, prec 0.0906171, recall 0.908364
2017-12-10T15:22:18.365238: step 4758, loss 0.102223, acc 0.96875, prec 0.0906524, recall 0.908404
2017-12-10T15:22:18.560533: step 4759, loss 0.156941, acc 0.953125, prec 0.0906661, recall 0.908423
2017-12-10T15:22:18.757548: step 4760, loss 0.358569, acc 1, prec 0.0906857, recall 0.908443
2017-12-10T15:22:18.957047: step 4761, loss 0.261199, acc 0.921875, prec 0.0906955, recall 0.908463
2017-12-10T15:22:19.155247: step 4762, loss 0.498422, acc 0.9375, prec 0.0907073, recall 0.908483
2017-12-10T15:22:19.351324: step 4763, loss 0.14973, acc 0.921875, prec 0.0906975, recall 0.908483
2017-12-10T15:22:19.548866: step 4764, loss 0.112586, acc 0.953125, prec 0.0906917, recall 0.908483
2017-12-10T15:22:19.740562: step 4765, loss 0.161556, acc 0.953125, prec 0.0906858, recall 0.908483
2017-12-10T15:22:19.937300: step 4766, loss 0.18879, acc 0.921875, prec 0.090676, recall 0.908483
2017-12-10T15:22:20.130964: step 4767, loss 0.187886, acc 0.953125, prec 0.0907093, recall 0.908522
2017-12-10T15:22:20.325465: step 4768, loss 0.240017, acc 0.9375, prec 0.0907211, recall 0.908542
2017-12-10T15:22:20.522963: step 4769, loss 1.77059, acc 0.953125, prec 0.0907172, recall 0.908346
2017-12-10T15:22:20.721828: step 4770, loss 0.0451266, acc 0.984375, prec 0.0907544, recall 0.908385
2017-12-10T15:22:20.916541: step 4771, loss 0.0593041, acc 0.984375, prec 0.090772, recall 0.908405
2017-12-10T15:22:21.111648: step 4772, loss 0.204046, acc 0.953125, prec 0.0907662, recall 0.908405
2017-12-10T15:22:21.312446: step 4773, loss 0.218641, acc 0.921875, prec 0.0907564, recall 0.908405
2017-12-10T15:22:21.508618: step 4774, loss 0.14833, acc 0.921875, prec 0.0907662, recall 0.908425
2017-12-10T15:22:21.701573: step 4775, loss 3.6787, acc 0.953125, prec 0.0907643, recall 0.908034
2017-12-10T15:22:21.901802: step 4776, loss 0.224367, acc 0.9375, prec 0.090776, recall 0.908053
2017-12-10T15:22:22.093565: step 4777, loss 0.0451595, acc 0.984375, prec 0.0907741, recall 0.908053
2017-12-10T15:22:22.292772: step 4778, loss 0.59911, acc 0.84375, prec 0.0907937, recall 0.908093
2017-12-10T15:22:22.491792: step 4779, loss 0.472307, acc 0.9375, prec 0.090825, recall 0.908133
2017-12-10T15:22:22.685973: step 4780, loss 0.676157, acc 0.8125, prec 0.0908015, recall 0.908133
2017-12-10T15:22:22.880749: step 4781, loss 0.705063, acc 0.859375, prec 0.0908035, recall 0.908152
2017-12-10T15:22:23.080329: step 4782, loss 0.394033, acc 0.859375, prec 0.0908055, recall 0.908172
2017-12-10T15:22:23.271261: step 4783, loss 0.975449, acc 0.765625, prec 0.0907762, recall 0.908172
2017-12-10T15:22:23.463561: step 4784, loss 0.592591, acc 0.828125, prec 0.0907938, recall 0.908212
2017-12-10T15:22:23.655899: step 4785, loss 0.821803, acc 0.75, prec 0.0907626, recall 0.908212
2017-12-10T15:22:23.853367: step 4786, loss 0.572712, acc 0.796875, prec 0.0907373, recall 0.908212
2017-12-10T15:22:24.048410: step 4787, loss 0.460245, acc 0.875, prec 0.0907412, recall 0.908231
2017-12-10T15:22:24.247402: step 4788, loss 0.341048, acc 0.890625, prec 0.0907666, recall 0.908271
2017-12-10T15:22:24.441555: step 4789, loss 0.690416, acc 0.859375, prec 0.0907491, recall 0.908271
2017-12-10T15:22:24.641681: step 4790, loss 0.292071, acc 0.859375, prec 0.0907706, recall 0.90831
2017-12-10T15:22:24.838724: step 4791, loss 0.446632, acc 0.890625, prec 0.0907765, recall 0.90833
2017-12-10T15:22:25.031974: step 4792, loss 0.360194, acc 0.875, prec 0.0907804, recall 0.908349
2017-12-10T15:22:25.226830: step 4793, loss 0.369126, acc 0.890625, prec 0.0907863, recall 0.908369
2017-12-10T15:22:25.422378: step 4794, loss 0.473283, acc 0.875, prec 0.0907707, recall 0.908369
2017-12-10T15:22:25.616656: step 4795, loss 0.406074, acc 0.875, prec 0.0907746, recall 0.908389
2017-12-10T15:22:25.807813: step 4796, loss 0.249374, acc 0.890625, prec 0.090761, recall 0.908389
2017-12-10T15:22:26.000751: step 4797, loss 0.486503, acc 0.890625, prec 0.0907669, recall 0.908408
2017-12-10T15:22:26.192082: step 4798, loss 0.385407, acc 0.96875, prec 0.090763, recall 0.908408
2017-12-10T15:22:26.389960: step 4799, loss 0.280328, acc 0.90625, prec 0.0907708, recall 0.908428
2017-12-10T15:22:26.589484: step 4800, loss 0.0942321, acc 0.984375, prec 0.0907688, recall 0.908428
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_1/1512936356/checkpoints/model-4800

2017-12-10T15:22:27.750993: step 4801, loss 0.00547523, acc 1, prec 0.0907688, recall 0.908428
2017-12-10T15:22:27.946665: step 4802, loss 0.0189049, acc 0.984375, prec 0.0907669, recall 0.908428
2017-12-10T15:22:28.140009: step 4803, loss 0.0230787, acc 1, prec 0.0908059, recall 0.908467
2017-12-10T15:22:28.340751: step 4804, loss 0.178736, acc 0.96875, prec 0.0908214, recall 0.908487
2017-12-10T15:22:28.539904: step 4805, loss 0.0126522, acc 1, prec 0.0908214, recall 0.908487
2017-12-10T15:22:28.736968: step 4806, loss 0.416077, acc 0.96875, prec 0.0908955, recall 0.908565
2017-12-10T15:22:28.939512: step 4807, loss 0.0203871, acc 1, prec 0.0908955, recall 0.908565
2017-12-10T15:22:29.143764: step 4808, loss 0.0294406, acc 0.984375, prec 0.0909325, recall 0.908604
2017-12-10T15:22:29.349387: step 4809, loss 0.0802704, acc 0.984375, prec 0.0909305, recall 0.908604
2017-12-10T15:22:29.549354: step 4810, loss 2.88461, acc 0.96875, prec 0.0909286, recall 0.90841
2017-12-10T15:22:29.751854: step 4811, loss 0.677307, acc 0.96875, prec 0.0909441, recall 0.90843
2017-12-10T15:22:29.954449: step 4812, loss 0.180383, acc 0.96875, prec 0.0909597, recall 0.908449
2017-12-10T15:22:30.147864: step 4813, loss 0.200366, acc 0.96875, prec 0.0909753, recall 0.908469
2017-12-10T15:22:30.342351: step 4814, loss 0.565155, acc 0.96875, prec 0.0910882, recall 0.908586
2017-12-10T15:22:30.541258: step 4815, loss 0.0681484, acc 0.96875, prec 0.0911037, recall 0.908606
2017-12-10T15:22:30.734020: step 4816, loss 0.132338, acc 0.96875, prec 0.0910998, recall 0.908606
2017-12-10T15:22:30.929014: step 4817, loss 0.097523, acc 0.96875, prec 0.0911154, recall 0.908625
2017-12-10T15:22:31.129136: step 4818, loss 0.340712, acc 0.953125, prec 0.0911095, recall 0.908625
2017-12-10T15:22:31.330512: step 4819, loss 0.446879, acc 0.9375, prec 0.0911017, recall 0.908625
2017-12-10T15:22:31.525877: step 4820, loss 0.154618, acc 0.890625, prec 0.0911075, recall 0.908645
2017-12-10T15:22:31.718078: step 4821, loss 0.463033, acc 0.890625, prec 0.0911133, recall 0.908664
2017-12-10T15:22:31.918518: step 4822, loss 0.265241, acc 0.9375, prec 0.0911833, recall 0.908742
2017-12-10T15:22:32.111628: step 4823, loss 0.152402, acc 0.9375, prec 0.0911755, recall 0.908742
2017-12-10T15:22:32.306367: step 4824, loss 0.107896, acc 0.953125, prec 0.0911891, recall 0.908761
2017-12-10T15:22:32.504357: step 4825, loss 0.260998, acc 0.9375, prec 0.0911813, recall 0.908761
2017-12-10T15:22:32.698386: step 4826, loss 0.215858, acc 0.953125, prec 0.0912338, recall 0.90882
2017-12-10T15:22:32.893613: step 4827, loss 0.390008, acc 0.921875, prec 0.0912629, recall 0.908859
2017-12-10T15:22:33.090509: step 4828, loss 0.199392, acc 0.953125, prec 0.091257, recall 0.908859
2017-12-10T15:22:33.286300: step 4829, loss 0.348798, acc 0.90625, prec 0.0912842, recall 0.908897
2017-12-10T15:22:33.484833: step 4830, loss 0.203497, acc 0.921875, prec 0.0912744, recall 0.908897
2017-12-10T15:22:33.679126: step 4831, loss 0.128815, acc 0.953125, prec 0.091288, recall 0.908917
2017-12-10T15:22:33.875209: step 4832, loss 0.257084, acc 0.921875, prec 0.0912977, recall 0.908936
2017-12-10T15:22:34.070234: step 4833, loss 0.293575, acc 0.859375, prec 0.0912995, recall 0.908956
2017-12-10T15:22:34.264750: step 4834, loss 0.178735, acc 0.9375, prec 0.0912917, recall 0.908956
2017-12-10T15:22:34.459875: step 4835, loss 0.131082, acc 0.96875, prec 0.0913849, recall 0.909052
2017-12-10T15:22:34.659374: step 4836, loss 0.21854, acc 0.890625, prec 0.0913712, recall 0.909052
2017-12-10T15:22:34.858064: step 4837, loss 0.0409356, acc 0.984375, prec 0.0913693, recall 0.909052
2017-12-10T15:22:35.051254: step 4838, loss 0.0493862, acc 1, prec 0.0913887, recall 0.909072
2017-12-10T15:22:35.244585: step 4839, loss 0.240897, acc 0.9375, prec 0.0913809, recall 0.909072
2017-12-10T15:22:35.443466: step 4840, loss 0.0615515, acc 0.984375, prec 0.0913983, recall 0.909091
2017-12-10T15:22:35.646450: step 4841, loss 0.0417677, acc 0.96875, prec 0.0913944, recall 0.909091
2017-12-10T15:22:35.845780: step 4842, loss 0.079592, acc 0.953125, prec 0.0913886, recall 0.909091
2017-12-10T15:22:36.041614: step 4843, loss 0.241977, acc 0.875, prec 0.0913729, recall 0.909091
2017-12-10T15:22:36.235225: step 4844, loss 0.0766296, acc 0.984375, prec 0.0913904, recall 0.90911
2017-12-10T15:22:36.436123: step 4845, loss 0.10675, acc 0.953125, prec 0.0914039, recall 0.90913
2017-12-10T15:22:36.637823: step 4846, loss 0.158094, acc 0.953125, prec 0.0914175, recall 0.909149
2017-12-10T15:22:36.832836: step 4847, loss 0.0471203, acc 0.984375, prec 0.0914155, recall 0.909149
2017-12-10T15:22:37.031559: step 4848, loss 0.244433, acc 1, prec 0.0914349, recall 0.909168
2017-12-10T15:22:37.231592: step 4849, loss 0.602662, acc 0.96875, prec 0.0914698, recall 0.909207
2017-12-10T15:22:37.431146: step 4850, loss 0.285927, acc 0.96875, prec 0.0914853, recall 0.909226
2017-12-10T15:22:37.631712: step 4851, loss 0.172243, acc 0.96875, prec 0.0915008, recall 0.909245
2017-12-10T15:22:37.825043: step 4852, loss 0.238945, acc 0.984375, prec 0.0915182, recall 0.909264
2017-12-10T15:22:38.027144: step 4853, loss 0.0706067, acc 0.953125, prec 0.0915123, recall 0.909264
2017-12-10T15:22:38.225337: step 4854, loss 3.54086, acc 0.984375, prec 0.0915511, recall 0.90911
2017-12-10T15:22:38.426662: step 4855, loss 0.0726824, acc 0.984375, prec 0.0915491, recall 0.90911
2017-12-10T15:22:38.628422: step 4856, loss 0.102695, acc 0.96875, prec 0.0915452, recall 0.90911
2017-12-10T15:22:38.820877: step 4857, loss 0.0236868, acc 1, prec 0.0915452, recall 0.90911
2017-12-10T15:22:39.018885: step 4858, loss 0.0667933, acc 0.96875, prec 0.0915607, recall 0.909129
2017-12-10T15:22:39.223426: step 4859, loss 0.230339, acc 0.9375, prec 0.0915723, recall 0.909149
2017-12-10T15:22:39.422801: step 4860, loss 0.221485, acc 0.953125, prec 0.0915664, recall 0.909149
2017-12-10T15:22:39.623632: step 4861, loss 0.259528, acc 0.953125, prec 0.0915606, recall 0.909149
2017-12-10T15:22:39.821108: step 4862, loss 0.284075, acc 0.890625, prec 0.0915856, recall 0.909187
2017-12-10T15:22:40.021884: step 4863, loss 0.0734713, acc 0.984375, prec 0.0916031, recall 0.909206
2017-12-10T15:22:40.218412: step 4864, loss 0.678442, acc 0.90625, prec 0.0916494, recall 0.909264
2017-12-10T15:22:40.417432: step 4865, loss 0.129074, acc 0.9375, prec 0.091661, recall 0.909283
2017-12-10T15:22:40.613504: step 4866, loss 0.10802, acc 0.96875, prec 0.0916764, recall 0.909302
2017-12-10T15:22:40.814440: step 4867, loss 0.386232, acc 0.90625, prec 0.0916647, recall 0.909302
2017-12-10T15:22:41.007223: step 4868, loss 0.212581, acc 0.953125, prec 0.0916782, recall 0.909321
2017-12-10T15:22:41.206948: step 4869, loss 0.226432, acc 0.953125, prec 0.0917111, recall 0.90936
2017-12-10T15:22:41.400072: step 4870, loss 0.409882, acc 0.90625, prec 0.0916993, recall 0.90936
2017-12-10T15:22:41.591997: step 4871, loss 0.472213, acc 0.921875, prec 0.0916896, recall 0.90936
2017-12-10T15:22:41.787840: step 4872, loss 0.10921, acc 0.9375, prec 0.0917205, recall 0.909398
2017-12-10T15:22:41.983235: step 4873, loss 0.259412, acc 0.90625, prec 0.0917474, recall 0.909436
2017-12-10T15:22:42.183517: step 4874, loss 0.180162, acc 0.953125, prec 0.0917609, recall 0.909455
2017-12-10T15:22:42.381777: step 4875, loss 0.0625908, acc 0.96875, prec 0.0917763, recall 0.909475
2017-12-10T15:22:42.574255: step 4876, loss 0.369218, acc 0.953125, prec 0.0917898, recall 0.909494
2017-12-10T15:22:42.767320: step 4877, loss 0.156581, acc 0.96875, prec 0.0918052, recall 0.909513
2017-12-10T15:22:42.960650: step 4878, loss 0.0353431, acc 0.984375, prec 0.0918226, recall 0.909532
2017-12-10T15:22:43.156508: step 4879, loss 0.0576662, acc 0.96875, prec 0.0918574, recall 0.90957
2017-12-10T15:22:43.351605: step 4880, loss 0.13883, acc 0.953125, prec 0.0918515, recall 0.90957
2017-12-10T15:22:43.543814: step 4881, loss 0.0719665, acc 0.953125, prec 0.0918456, recall 0.90957
2017-12-10T15:22:43.740446: step 4882, loss 0.994159, acc 0.96875, prec 0.0918997, recall 0.909627
2017-12-10T15:22:43.942571: step 4883, loss 0.20395, acc 0.921875, prec 0.0919286, recall 0.909665
2017-12-10T15:22:44.145652: step 4884, loss 0.0373762, acc 0.984375, prec 0.0919653, recall 0.909703
2017-12-10T15:22:44.347685: step 4885, loss 0.0106059, acc 1, prec 0.0919653, recall 0.909703
2017-12-10T15:22:44.540834: step 4886, loss 0.0953465, acc 0.96875, prec 0.0919614, recall 0.909703
2017-12-10T15:22:44.739556: step 4887, loss 0.0623353, acc 0.984375, prec 0.0919594, recall 0.909703
2017-12-10T15:22:44.938660: step 4888, loss 0.129699, acc 0.9375, prec 0.0919709, recall 0.909722
2017-12-10T15:22:45.138827: step 4889, loss 0.223884, acc 0.96875, prec 0.0919863, recall 0.909741
2017-12-10T15:22:45.339095: step 4890, loss 0.0625857, acc 0.953125, prec 0.0919804, recall 0.909741
2017-12-10T15:22:45.538184: step 4891, loss 0.523543, acc 0.90625, prec 0.091988, recall 0.90976
2017-12-10T15:22:45.736728: step 4892, loss 4.16434, acc 0.90625, prec 0.0919782, recall 0.909569
2017-12-10T15:22:45.933903: step 4893, loss 0.218316, acc 0.96875, prec 0.0919936, recall 0.909588
2017-12-10T15:22:46.131150: step 4894, loss 0.181743, acc 0.953125, prec 0.0919878, recall 0.909588
2017-12-10T15:22:46.328667: step 4895, loss 0.316207, acc 0.921875, prec 0.091978, recall 0.909588
2017-12-10T15:22:46.523747: step 4896, loss 0.13815, acc 0.9375, prec 0.0919702, recall 0.909588
2017-12-10T15:22:46.717936: step 4897, loss 0.98208, acc 0.859375, prec 0.0919719, recall 0.909607
2017-12-10T15:22:46.911915: step 4898, loss 0.431402, acc 0.890625, prec 0.0920161, recall 0.909664
2017-12-10T15:22:47.112072: step 4899, loss 0.980206, acc 0.796875, prec 0.0920485, recall 0.909721
2017-12-10T15:22:47.306424: step 4900, loss 0.50044, acc 0.90625, prec 0.0920368, recall 0.909721
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_1/1512936356/checkpoints/model-4900

2017-12-10T15:22:48.504750: step 4901, loss 0.697357, acc 0.859375, prec 0.0920578, recall 0.909759
2017-12-10T15:22:48.698028: step 4902, loss 0.440852, acc 0.859375, prec 0.0920402, recall 0.909759
2017-12-10T15:22:48.896304: step 4903, loss 0.627354, acc 0.828125, prec 0.0920187, recall 0.909759
2017-12-10T15:22:49.090717: step 4904, loss 0.558799, acc 0.796875, prec 0.0920511, recall 0.909815
2017-12-10T15:22:49.292496: step 4905, loss 0.334365, acc 0.90625, prec 0.0920779, recall 0.909853
2017-12-10T15:22:49.488024: step 4906, loss 0.386255, acc 0.890625, prec 0.0921028, recall 0.909891
2017-12-10T15:22:49.685864: step 4907, loss 0.175572, acc 0.921875, prec 0.0921122, recall 0.90991
2017-12-10T15:22:49.881664: step 4908, loss 0.40056, acc 0.859375, prec 0.0920947, recall 0.90991
2017-12-10T15:22:50.081861: step 4909, loss 0.149382, acc 0.9375, prec 0.0921061, recall 0.909929
2017-12-10T15:22:50.280313: step 4910, loss 0.0874128, acc 0.953125, prec 0.0921387, recall 0.909967
2017-12-10T15:22:50.473854: step 4911, loss 0.232801, acc 0.90625, prec 0.0921655, recall 0.910004
2017-12-10T15:22:50.668638: step 4912, loss 0.0562985, acc 0.96875, prec 0.0921616, recall 0.910004
2017-12-10T15:22:50.864629: step 4913, loss 0.178439, acc 0.96875, prec 0.0921577, recall 0.910004
2017-12-10T15:22:51.062644: step 4914, loss 0.180782, acc 0.921875, prec 0.0921672, recall 0.910023
2017-12-10T15:22:51.255870: step 4915, loss 0.0816015, acc 0.9375, prec 0.0921594, recall 0.910023
2017-12-10T15:22:51.454070: step 4916, loss 0.160316, acc 0.9375, prec 0.09219, recall 0.910061
2017-12-10T15:22:51.647194: step 4917, loss 0.102632, acc 0.96875, prec 0.0922053, recall 0.910079
2017-12-10T15:22:51.847647: step 4918, loss 0.0558723, acc 0.96875, prec 0.0922207, recall 0.910098
2017-12-10T15:22:52.047754: step 4919, loss 0.070666, acc 0.953125, prec 0.0922148, recall 0.910098
2017-12-10T15:22:52.244242: step 4920, loss 0.100327, acc 0.96875, prec 0.0922109, recall 0.910098
2017-12-10T15:22:52.448895: step 4921, loss 0.0521302, acc 0.984375, prec 0.0922282, recall 0.910117
2017-12-10T15:22:52.642343: step 4922, loss 0.0565524, acc 0.984375, prec 0.0922262, recall 0.910117
2017-12-10T15:22:52.837505: step 4923, loss 0.0271562, acc 0.984375, prec 0.0922243, recall 0.910117
2017-12-10T15:22:53.035538: step 4924, loss 0.266177, acc 0.9375, prec 0.0922165, recall 0.910117
2017-12-10T15:22:53.229313: step 4925, loss 0.113792, acc 0.984375, prec 0.0922145, recall 0.910117
2017-12-10T15:22:53.426720: step 4926, loss 0.105836, acc 0.953125, prec 0.0922279, recall 0.910136
2017-12-10T15:22:53.625541: step 4927, loss 0.43715, acc 0.984375, prec 0.0922644, recall 0.910173
2017-12-10T15:22:53.823338: step 4928, loss 0.0412104, acc 0.96875, prec 0.0922797, recall 0.910192
2017-12-10T15:22:54.018180: step 4929, loss 1.1582, acc 0.984375, prec 0.0922797, recall 0.910002
2017-12-10T15:22:54.225842: step 4930, loss 0.0641784, acc 0.96875, prec 0.0922758, recall 0.910002
2017-12-10T15:22:54.424701: step 4931, loss 0.00618888, acc 1, prec 0.092295, recall 0.910021
2017-12-10T15:22:54.619254: step 4932, loss 0.0645263, acc 0.96875, prec 0.0922911, recall 0.910021
2017-12-10T15:22:54.818137: step 4933, loss 0.0417367, acc 0.984375, prec 0.0922891, recall 0.910021
2017-12-10T15:22:55.010557: step 4934, loss 0.106523, acc 0.984375, prec 0.0923064, recall 0.91004
2017-12-10T15:22:55.210183: step 4935, loss 0.0975683, acc 0.984375, prec 0.0923429, recall 0.910077
2017-12-10T15:22:55.407275: step 4936, loss 0.035691, acc 0.96875, prec 0.092339, recall 0.910077
2017-12-10T15:22:55.604805: step 4937, loss 1.14529, acc 0.984375, prec 0.0924139, recall 0.910152
2017-12-10T15:22:55.808522: step 4938, loss 0.035917, acc 1, prec 0.0924331, recall 0.910171
2017-12-10T15:22:56.005259: step 4939, loss 0.112926, acc 0.9375, prec 0.0924252, recall 0.910171
2017-12-10T15:22:56.203156: step 4940, loss 0.0824395, acc 0.96875, prec 0.0924213, recall 0.910171
2017-12-10T15:22:56.402188: step 4941, loss 0.175098, acc 0.953125, prec 0.0924347, recall 0.91019
2017-12-10T15:22:56.602811: step 4942, loss 0.407327, acc 0.921875, prec 0.0924249, recall 0.91019
2017-12-10T15:22:56.801637: step 4943, loss 0.273043, acc 0.921875, prec 0.0924343, recall 0.910208
2017-12-10T15:22:56.999740: step 4944, loss 0.368793, acc 0.921875, prec 0.0924437, recall 0.910227
2017-12-10T15:22:57.200729: step 4945, loss 0.16152, acc 0.921875, prec 0.0924532, recall 0.910246
2017-12-10T15:22:57.401355: step 4946, loss 0.186701, acc 0.9375, prec 0.0924453, recall 0.910246
2017-12-10T15:22:57.595735: step 4947, loss 0.191447, acc 0.921875, prec 0.0924356, recall 0.910246
2017-12-10T15:22:57.793846: step 4948, loss 0.0788579, acc 0.984375, prec 0.0924336, recall 0.910246
2017-12-10T15:22:57.986565: step 4949, loss 0.209185, acc 0.953125, prec 0.0924277, recall 0.910246
2017-12-10T15:22:58.183334: step 4950, loss 0.279604, acc 0.90625, prec 0.0924352, recall 0.910264
2017-12-10T15:22:58.380036: step 4951, loss 0.344027, acc 0.875, prec 0.0924387, recall 0.910283
2017-12-10T15:22:58.573037: step 4952, loss 0.0915152, acc 0.9375, prec 0.0924309, recall 0.910283
2017-12-10T15:22:58.770134: step 4953, loss 0.130335, acc 0.953125, prec 0.0924251, recall 0.910283
2017-12-10T15:22:58.968183: step 4954, loss 0.135943, acc 0.953125, prec 0.0924576, recall 0.91032
2017-12-10T15:22:59.167128: step 4955, loss 0.46417, acc 0.859375, prec 0.0924592, recall 0.910339
2017-12-10T15:22:59.370587: step 4956, loss 0.130507, acc 0.96875, prec 0.0924744, recall 0.910358
2017-12-10T15:22:59.567829: step 4957, loss 0.136346, acc 0.953125, prec 0.0925069, recall 0.910395
2017-12-10T15:22:59.768046: step 4958, loss 0.354944, acc 0.953125, prec 0.0925394, recall 0.910432
2017-12-10T15:22:59.964218: step 4959, loss 0.127728, acc 0.953125, prec 0.0925527, recall 0.910451
2017-12-10T15:23:00.159901: step 4960, loss 0.168585, acc 0.953125, prec 0.0925852, recall 0.910488
2017-12-10T15:23:00.355429: step 4961, loss 0.0291427, acc 0.984375, prec 0.0926024, recall 0.910507
2017-12-10T15:23:00.557244: step 4962, loss 0.0256292, acc 0.984375, prec 0.0926004, recall 0.910507
2017-12-10T15:23:00.758458: step 4963, loss 0.0656278, acc 0.984375, prec 0.0926176, recall 0.910525
2017-12-10T15:23:00.954080: step 4964, loss 0.0361393, acc 0.984375, prec 0.0926348, recall 0.910544
2017-12-10T15:23:01.152456: step 4965, loss 0.0776903, acc 0.96875, prec 0.0926309, recall 0.910544
2017-12-10T15:23:01.349647: step 4966, loss 1.0426, acc 0.984375, prec 0.0926481, recall 0.910562
2017-12-10T15:23:01.552529: step 4967, loss 0.12788, acc 0.9375, prec 0.0926403, recall 0.910562
2017-12-10T15:23:01.750765: step 4968, loss 0.176941, acc 0.9375, prec 0.0926325, recall 0.910562
2017-12-10T15:23:01.947554: step 4969, loss 0.151095, acc 0.953125, prec 0.0926458, recall 0.910581
2017-12-10T15:23:02.119754: step 4970, loss 0.333248, acc 0.921569, prec 0.0926571, recall 0.910599
Training finished
Starting Fold: 2 => Train/Dev split: 31796/10598


 #################### 


SEQUENCE LENGTH 273
BATCH SIZE 64
EMBEDDING SIZE 128
FILTER SIZES [3, 4, 5]
NUMBER OF FILTERS 128
L2 REG LAMBDA 0.0
EPOCHS 10



RESULT DIR leave_pos_embedding_out_fold_2
VOCAB SIZE 33447
DROPOUT PROBABILITY 0.5



WORD EMBEDDING True
POSITION EMBEDDING True
POS EMBEDDING False


 #################### 


Writing to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_2/1512937383

Start training
2017-12-10T15:23:05.861954: step 1, loss 6.15114, acc 0.21875, prec 0, recall 0
2017-12-10T15:23:06.056875: step 2, loss 4.07976, acc 0.3125, prec 0.0106383, recall 0.5
2017-12-10T15:23:06.251334: step 3, loss 2.95645, acc 0.40625, prec 0.0223881, recall 0.75
2017-12-10T15:23:06.443446: step 4, loss 4.46939, acc 0.40625, prec 0.0175439, recall 0.6
2017-12-10T15:23:06.632735: step 5, loss 2.51786, acc 0.515625, prec 0.0197044, recall 0.666667
2017-12-10T15:23:06.828809: step 6, loss 2.45867, acc 0.484375, prec 0.0169492, recall 0.666667
2017-12-10T15:23:07.020021: step 7, loss 1.84146, acc 0.59375, prec 0.0190114, recall 0.714286
2017-12-10T15:23:07.218205: step 8, loss 23.8491, acc 0.6875, prec 0.0177936, recall 0.555556
2017-12-10T15:23:07.410227: step 9, loss 20.6868, acc 0.640625, prec 0.0197368, recall 0.545455
2017-12-10T15:23:07.603440: step 10, loss 1.74927, acc 0.578125, prec 0.024024, recall 0.615385
2017-12-10T15:23:07.795329: step 11, loss 6.90838, acc 0.59375, prec 0.0250696, recall 0.6
2017-12-10T15:23:07.997432: step 12, loss 1.64421, acc 0.59375, prec 0.0233766, recall 0.6
2017-12-10T15:23:08.191340: step 13, loss 1.32508, acc 0.59375, prec 0.0218978, recall 0.6
2017-12-10T15:23:08.384699: step 14, loss 7.02354, acc 0.484375, prec 0.0225225, recall 0.588235
2017-12-10T15:23:08.577354: step 15, loss 3.20559, acc 0.40625, prec 0.0227743, recall 0.611111
2017-12-10T15:23:08.771364: step 16, loss 3.42202, acc 0.34375, prec 0.0228137, recall 0.631579
2017-12-10T15:23:08.959857: step 17, loss 10.6863, acc 0.390625, prec 0.026455, recall 0.652174
2017-12-10T15:23:09.154910: step 18, loss 3.45119, acc 0.375, prec 0.0263158, recall 0.666667
2017-12-10T15:23:09.347640: step 19, loss 3.21948, acc 0.375, prec 0.0246914, recall 0.666667
2017-12-10T15:23:09.542713: step 20, loss 3.82212, acc 0.390625, prec 0.0232897, recall 0.666667
2017-12-10T15:23:09.732742: step 21, loss 3.45339, acc 0.296875, prec 0.0218579, recall 0.666667
2017-12-10T15:23:09.927259: step 22, loss 8.2485, acc 0.40625, prec 0.0208062, recall 0.64
2017-12-10T15:23:10.124525: step 23, loss 3.33135, acc 0.421875, prec 0.0198511, recall 0.64
2017-12-10T15:23:10.319256: step 24, loss 2.94106, acc 0.46875, prec 0.0213777, recall 0.666667
2017-12-10T15:23:10.512292: step 25, loss 2.15748, acc 0.515625, prec 0.0206186, recall 0.666667
2017-12-10T15:23:10.703579: step 26, loss 2.54612, acc 0.65625, prec 0.0244716, recall 0.709677
2017-12-10T15:23:10.902346: step 27, loss 1.8238, acc 0.625, prec 0.0238353, recall 0.709677
2017-12-10T15:23:11.095122: step 28, loss 1.49124, acc 0.65625, prec 0.0232804, recall 0.709677
2017-12-10T15:23:11.285881: step 29, loss 1.0965, acc 0.640625, prec 0.0227273, recall 0.709677
2017-12-10T15:23:11.480974: step 30, loss 1.49242, acc 0.703125, prec 0.0232794, recall 0.71875
2017-12-10T15:23:11.674140: step 31, loss 1.08964, acc 0.734375, prec 0.0238569, recall 0.727273
2017-12-10T15:23:11.873020: step 32, loss 3.50824, acc 0.90625, prec 0.0237389, recall 0.705882
2017-12-10T15:23:12.067373: step 33, loss 18.4754, acc 0.734375, prec 0.0243191, recall 0.694444
2017-12-10T15:23:12.261030: step 34, loss 41.781, acc 0.75, prec 0.0240154, recall 0.641026
2017-12-10T15:23:12.456797: step 35, loss 15.6849, acc 0.84375, prec 0.0238095, recall 0.625
2017-12-10T15:23:12.655392: step 36, loss 1.37134, acc 0.671875, prec 0.0233427, recall 0.625
2017-12-10T15:23:12.855558: step 37, loss 1.79677, acc 0.609375, prec 0.0228102, recall 0.625
2017-12-10T15:23:13.044686: step 38, loss 2.5968, acc 0.484375, prec 0.0221435, recall 0.625
2017-12-10T15:23:13.239325: step 39, loss 2.44949, acc 0.5, prec 0.0215332, recall 0.625
2017-12-10T15:23:13.435246: step 40, loss 18.7088, acc 0.40625, prec 0.0208681, recall 0.609756
2017-12-10T15:23:13.636385: step 41, loss 3.92301, acc 0.4375, prec 0.0218447, recall 0.627907
2017-12-10T15:23:13.829858: step 42, loss 3.70989, acc 0.359375, prec 0.0211433, recall 0.627907
2017-12-10T15:23:14.021729: step 43, loss 3.79544, acc 0.265625, prec 0.0211321, recall 0.636364
2017-12-10T15:23:14.217141: step 44, loss 3.48631, acc 0.4375, prec 0.0205731, recall 0.636364
2017-12-10T15:23:14.421708: step 45, loss 4.55616, acc 0.21875, prec 0.0198441, recall 0.636364
2017-12-10T15:23:14.615560: step 46, loss 3.07873, acc 0.40625, prec 0.0213499, recall 0.659574
2017-12-10T15:23:14.812312: step 47, loss 3.80968, acc 0.375, prec 0.0207775, recall 0.659574
2017-12-10T15:23:15.011688: step 48, loss 4.00051, acc 0.375, prec 0.020235, recall 0.659574
2017-12-10T15:23:15.202366: step 49, loss 2.50419, acc 0.484375, prec 0.0198083, recall 0.659574
2017-12-10T15:23:15.396803: step 50, loss 2.05475, acc 0.5, prec 0.0206379, recall 0.673469
2017-12-10T15:23:15.592510: step 51, loss 2.23219, acc 0.5, prec 0.020233, recall 0.673469
2017-12-10T15:23:15.787983: step 52, loss 3.31267, acc 0.78125, prec 0.0218579, recall 0.679245
2017-12-10T15:23:15.981791: step 53, loss 1.22242, acc 0.6875, prec 0.0215957, recall 0.679245
2017-12-10T15:23:16.178046: step 54, loss 6.05527, acc 0.65625, prec 0.0219065, recall 0.672727
2017-12-10T15:23:16.379005: step 55, loss 0.996509, acc 0.6875, prec 0.0216501, recall 0.672727
2017-12-10T15:23:16.577504: step 56, loss 0.998867, acc 0.734375, prec 0.0214368, recall 0.672727
2017-12-10T15:23:16.774138: step 57, loss 1.03277, acc 0.796875, prec 0.0218391, recall 0.678571
2017-12-10T15:23:16.965844: step 58, loss 0.669678, acc 0.78125, prec 0.0222222, recall 0.684211
2017-12-10T15:23:17.164280: step 59, loss 0.581796, acc 0.8125, prec 0.0220713, recall 0.684211
2017-12-10T15:23:17.356635: step 60, loss 2.16369, acc 0.71875, prec 0.0223964, recall 0.689655
2017-12-10T15:23:17.550059: step 61, loss 1.05097, acc 0.6875, prec 0.0221484, recall 0.689655
2017-12-10T15:23:17.746382: step 62, loss 22.2427, acc 0.8125, prec 0.0225523, recall 0.683333
2017-12-10T15:23:17.945100: step 63, loss 3.05697, acc 0.765625, prec 0.0229133, recall 0.677419
2017-12-10T15:23:18.141485: step 64, loss 2.78012, acc 0.71875, prec 0.0232307, recall 0.671875
2017-12-10T15:23:18.335744: step 65, loss 1.16969, acc 0.796875, prec 0.0235925, recall 0.676923
2017-12-10T15:23:18.531725: step 66, loss 2.39024, acc 0.609375, prec 0.0243129, recall 0.686567
2017-12-10T15:23:18.731794: step 67, loss 1.13297, acc 0.703125, prec 0.0245816, recall 0.691176
2017-12-10T15:23:18.927187: step 68, loss 4.83422, acc 0.71875, prec 0.024365, recall 0.681159
2017-12-10T15:23:19.118198: step 69, loss 4.24691, acc 0.4375, prec 0.0244151, recall 0.685714
2017-12-10T15:23:19.311691: step 70, loss 2.88743, acc 0.40625, prec 0.0244389, recall 0.690141
2017-12-10T15:23:19.506186: step 71, loss 3.07528, acc 0.4375, prec 0.0240078, recall 0.690141
2017-12-10T15:23:19.701324: step 72, loss 2.42846, acc 0.53125, prec 0.024602, recall 0.69863
2017-12-10T15:23:19.894847: step 73, loss 3.22336, acc 0.546875, prec 0.0247266, recall 0.702703
2017-12-10T15:23:20.093787: step 74, loss 3.07139, acc 0.5, prec 0.0252691, recall 0.710526
2017-12-10T15:23:20.286361: step 75, loss 3.23863, acc 0.5, prec 0.0253456, recall 0.714286
2017-12-10T15:23:20.479523: step 76, loss 3.08323, acc 0.359375, prec 0.0248756, recall 0.714286
2017-12-10T15:23:20.676620: step 77, loss 2.92045, acc 0.515625, prec 0.0249666, recall 0.717949
2017-12-10T15:23:20.867954: step 78, loss 2.7198, acc 0.53125, prec 0.024637, recall 0.717949
2017-12-10T15:23:21.062871: step 79, loss 6.70612, acc 0.484375, prec 0.024295, recall 0.708861
2017-12-10T15:23:21.258076: step 80, loss 1.40856, acc 0.65625, prec 0.0244845, recall 0.7125
2017-12-10T15:23:21.452084: step 81, loss 1.7222, acc 0.59375, prec 0.0242141, recall 0.7125
2017-12-10T15:23:21.646922: step 82, loss 1.67659, acc 0.671875, prec 0.024, recall 0.7125
2017-12-10T15:23:21.845236: step 83, loss 2.24482, acc 0.609375, prec 0.02375, recall 0.7125
2017-12-10T15:23:22.035095: step 84, loss 1.83305, acc 0.546875, prec 0.0234664, recall 0.7125
2017-12-10T15:23:22.232333: step 85, loss 1.05346, acc 0.703125, prec 0.0236831, recall 0.716049
2017-12-10T15:23:22.425004: step 86, loss 1.34529, acc 0.734375, prec 0.0239157, recall 0.719512
2017-12-10T15:23:22.618868: step 87, loss 0.351031, acc 0.890625, prec 0.023848, recall 0.719512
2017-12-10T15:23:22.811072: step 88, loss 0.316243, acc 0.90625, prec 0.0237903, recall 0.719512
2017-12-10T15:23:23.002846: step 89, loss 0.8177, acc 0.828125, prec 0.0236853, recall 0.719512
2017-12-10T15:23:23.193690: step 90, loss 17.9794, acc 0.875, prec 0.0236189, recall 0.710843
2017-12-10T15:23:23.387544: step 91, loss 13.3523, acc 0.90625, prec 0.0243513, recall 0.709302
2017-12-10T15:23:23.581539: step 92, loss 20.8864, acc 0.796875, prec 0.0242448, recall 0.693182
2017-12-10T15:23:23.775706: step 93, loss 1.12334, acc 0.765625, prec 0.0252565, recall 0.703297
2017-12-10T15:23:23.972836: step 94, loss 3.56019, acc 0.75, prec 0.0258722, recall 0.702128
2017-12-10T15:23:24.171598: step 95, loss 1.57555, acc 0.640625, prec 0.025641, recall 0.702128
2017-12-10T15:23:24.365406: step 96, loss 1.95848, acc 0.515625, prec 0.0253359, recall 0.702128
2017-12-10T15:23:24.559592: step 97, loss 2.28028, acc 0.5, prec 0.025398, recall 0.705263
2017-12-10T15:23:24.751702: step 98, loss 2.6733, acc 0.546875, prec 0.0258524, recall 0.71134
2017-12-10T15:23:24.943541: step 99, loss 2.76531, acc 0.453125, prec 0.026238, recall 0.717172
2017-12-10T15:23:25.139382: step 100, loss 2.71402, acc 0.515625, prec 0.0262966, recall 0.72
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_2/1512937383/checkpoints/model-100

2017-12-10T15:23:26.977322: step 101, loss 2.7787, acc 0.484375, prec 0.0263348, recall 0.722772
2017-12-10T15:23:27.175396: step 102, loss 2.9783, acc 0.546875, prec 0.0260621, recall 0.722772
2017-12-10T15:23:27.366266: step 103, loss 2.43745, acc 0.578125, prec 0.0258133, recall 0.722772
2017-12-10T15:23:27.560332: step 104, loss 1.78324, acc 0.59375, prec 0.0255781, recall 0.722772
2017-12-10T15:23:27.758713: step 105, loss 2.60681, acc 0.609375, prec 0.0260326, recall 0.728155
2017-12-10T15:23:27.951056: step 106, loss 1.94177, acc 0.546875, prec 0.0257732, recall 0.728155
2017-12-10T15:23:28.142046: step 107, loss 2.19973, acc 0.640625, prec 0.0265668, recall 0.735849
2017-12-10T15:23:28.337332: step 108, loss 1.5701, acc 0.671875, prec 0.0267072, recall 0.738318
2017-12-10T15:23:28.531781: step 109, loss 1.10179, acc 0.765625, prec 0.0268998, recall 0.740741
2017-12-10T15:23:28.727591: step 110, loss 0.960347, acc 0.765625, prec 0.0270903, recall 0.743119
2017-12-10T15:23:28.925721: step 111, loss 1.66808, acc 0.671875, prec 0.0272244, recall 0.745455
2017-12-10T15:23:29.119474: step 112, loss 1.03152, acc 0.734375, prec 0.0273927, recall 0.747748
2017-12-10T15:23:29.321633: step 113, loss 0.610566, acc 0.859375, prec 0.0276316, recall 0.75
2017-12-10T15:23:29.515939: step 114, loss 9.63543, acc 0.734375, prec 0.0274869, recall 0.743363
2017-12-10T15:23:29.710012: step 115, loss 1.35414, acc 0.859375, prec 0.0283572, recall 0.75
2017-12-10T15:23:29.909933: step 116, loss 1.05612, acc 0.78125, prec 0.0282284, recall 0.75
2017-12-10T15:23:30.102229: step 117, loss 14.1087, acc 0.78125, prec 0.0284238, recall 0.745763
2017-12-10T15:23:30.300003: step 118, loss 6.6652, acc 0.734375, prec 0.0285898, recall 0.741667
2017-12-10T15:23:30.495389: step 119, loss 0.772038, acc 0.8125, prec 0.02848, recall 0.741667
2017-12-10T15:23:30.685134: step 120, loss 1.10777, acc 0.796875, prec 0.028362, recall 0.741667
2017-12-10T15:23:30.880713: step 121, loss 1.58387, acc 0.65625, prec 0.0281646, recall 0.741667
2017-12-10T15:23:31.078170: step 122, loss 1.36575, acc 0.703125, prec 0.0283019, recall 0.743802
2017-12-10T15:23:31.280709: step 123, loss 6.10748, acc 0.6875, prec 0.0284375, recall 0.739837
2017-12-10T15:23:31.486005: step 124, loss 1.44588, acc 0.671875, prec 0.0282521, recall 0.739837
2017-12-10T15:23:31.677365: step 125, loss 1.32886, acc 0.65625, prec 0.0280604, recall 0.739837
2017-12-10T15:23:31.870941: step 126, loss 1.82552, acc 0.6875, prec 0.0281863, recall 0.741935
2017-12-10T15:23:32.067228: step 127, loss 0.821639, acc 0.75, prec 0.0280488, recall 0.741935
2017-12-10T15:23:32.258690: step 128, loss 1.6363, acc 0.640625, prec 0.0278535, recall 0.741935
2017-12-10T15:23:32.453123: step 129, loss 1.93348, acc 0.6875, prec 0.0282707, recall 0.746032
2017-12-10T15:23:32.646568: step 130, loss 1.48949, acc 0.6875, prec 0.0281016, recall 0.746032
2017-12-10T15:23:32.839627: step 131, loss 1.09302, acc 0.78125, prec 0.0285629, recall 0.75
2017-12-10T15:23:33.035332: step 132, loss 1.20969, acc 0.703125, prec 0.0284024, recall 0.75
2017-12-10T15:23:33.229806: step 133, loss 1.73934, acc 0.640625, prec 0.0282104, recall 0.75
2017-12-10T15:23:33.422701: step 134, loss 1.31275, acc 0.703125, prec 0.0286215, recall 0.753846
2017-12-10T15:23:33.619572: step 135, loss 9.51549, acc 0.6875, prec 0.0284635, recall 0.748092
2017-12-10T15:23:33.813175: step 136, loss 1.35537, acc 0.71875, prec 0.029157, recall 0.753731
2017-12-10T15:23:34.011491: step 137, loss 1.09797, acc 0.8125, prec 0.0293356, recall 0.755556
2017-12-10T15:23:34.202901: step 138, loss 1.85856, acc 0.625, prec 0.0296888, recall 0.759124
2017-12-10T15:23:34.399104: step 139, loss 1.71369, acc 0.640625, prec 0.0294952, recall 0.759124
2017-12-10T15:23:34.597489: step 140, loss 1.81146, acc 0.8125, prec 0.0299435, recall 0.76259
2017-12-10T15:23:34.799500: step 141, loss 1.47262, acc 0.6875, prec 0.0297753, recall 0.76259
2017-12-10T15:23:34.995706: step 142, loss 2.47591, acc 0.6875, prec 0.0298883, recall 0.758865
2017-12-10T15:23:35.187715: step 143, loss 1.06451, acc 0.71875, prec 0.0300083, recall 0.760563
2017-12-10T15:23:35.387635: step 144, loss 8.34373, acc 0.65625, prec 0.03037, recall 0.758621
2017-12-10T15:23:35.582486: step 145, loss 1.51561, acc 0.6875, prec 0.0302032, recall 0.758621
2017-12-10T15:23:35.773608: step 146, loss 1.90883, acc 0.71875, prec 0.0305844, recall 0.761905
2017-12-10T15:23:35.964239: step 147, loss 2.16078, acc 0.578125, prec 0.0303605, recall 0.761905
2017-12-10T15:23:36.156759: step 148, loss 2.4695, acc 0.46875, prec 0.0300833, recall 0.761905
2017-12-10T15:23:36.351935: step 149, loss 2.12507, acc 0.65625, prec 0.0299065, recall 0.761905
2017-12-10T15:23:36.546247: step 150, loss 2.29535, acc 0.546875, prec 0.0296767, recall 0.761905
2017-12-10T15:23:36.741491: step 151, loss 2.67581, acc 0.546875, prec 0.0297056, recall 0.763514
2017-12-10T15:23:36.937246: step 152, loss 2.28387, acc 0.5625, prec 0.0297417, recall 0.765101
2017-12-10T15:23:37.131691: step 153, loss 1.42861, acc 0.65625, prec 0.029572, recall 0.765101
2017-12-10T15:23:37.323791: step 154, loss 1.14044, acc 0.765625, prec 0.0294574, recall 0.765101
2017-12-10T15:23:37.518866: step 155, loss 1.05419, acc 0.78125, prec 0.0293512, recall 0.765101
2017-12-10T15:23:37.716386: step 156, loss 0.592373, acc 0.8125, prec 0.0292608, recall 0.765101
2017-12-10T15:23:37.912399: step 157, loss 0.968647, acc 0.828125, prec 0.0294268, recall 0.766667
2017-12-10T15:23:38.103588: step 158, loss 0.793165, acc 0.78125, prec 0.0293218, recall 0.766667
2017-12-10T15:23:38.302770: step 159, loss 0.628134, acc 0.8125, prec 0.0292323, recall 0.766667
2017-12-10T15:23:38.497079: step 160, loss 0.97868, acc 0.765625, prec 0.0291213, recall 0.766667
2017-12-10T15:23:38.692984: step 161, loss 3.32336, acc 0.890625, prec 0.0290771, recall 0.761589
2017-12-10T15:23:38.895271: step 162, loss 0.349875, acc 0.890625, prec 0.0290257, recall 0.761589
2017-12-10T15:23:39.093350: step 163, loss 0.499129, acc 0.890625, prec 0.0289746, recall 0.761589
2017-12-10T15:23:39.283926: step 164, loss 0.532807, acc 0.859375, prec 0.028909, recall 0.761589
2017-12-10T15:23:39.477991: step 165, loss 0.709624, acc 0.890625, prec 0.0291019, recall 0.763158
2017-12-10T15:23:39.668618: step 166, loss 0.75345, acc 0.78125, prec 0.029, recall 0.763158
2017-12-10T15:23:39.861789: step 167, loss 1.05068, acc 0.859375, prec 0.0289349, recall 0.763158
2017-12-10T15:23:40.051630: step 168, loss 8.15047, acc 0.875, prec 0.0288845, recall 0.75817
2017-12-10T15:23:40.249991: step 169, loss 2.90891, acc 0.859375, prec 0.028827, recall 0.753247
2017-12-10T15:23:40.446708: step 170, loss 0.509543, acc 0.828125, prec 0.0287485, recall 0.753247
2017-12-10T15:23:40.640417: step 171, loss 0.419768, acc 0.875, prec 0.0286916, recall 0.753247
2017-12-10T15:23:40.835434: step 172, loss 28.6149, acc 0.859375, prec 0.028642, recall 0.74359
2017-12-10T15:23:41.030600: step 173, loss 0.892297, acc 0.75, prec 0.0285293, recall 0.74359
2017-12-10T15:23:41.220583: step 174, loss 6.17658, acc 0.625, prec 0.0283688, recall 0.738854
2017-12-10T15:23:41.414446: step 175, loss 1.10425, acc 0.75, prec 0.0282582, recall 0.738854
2017-12-10T15:23:41.605523: step 176, loss 2.14167, acc 0.6875, prec 0.0285922, recall 0.742138
2017-12-10T15:23:41.799759: step 177, loss 2.02051, acc 0.640625, prec 0.0284337, recall 0.742138
2017-12-10T15:23:41.996380: step 178, loss 2.09164, acc 0.578125, prec 0.0282499, recall 0.742138
2017-12-10T15:23:42.191453: step 179, loss 3.02131, acc 0.53125, prec 0.0282795, recall 0.74375
2017-12-10T15:23:42.390664: step 180, loss 2.01935, acc 0.625, prec 0.0283487, recall 0.745342
2017-12-10T15:23:42.582892: step 181, loss 2.54426, acc 0.578125, prec 0.028169, recall 0.745342
2017-12-10T15:23:42.782789: step 182, loss 3.33186, acc 0.46875, prec 0.0283985, recall 0.748466
2017-12-10T15:23:42.981258: step 183, loss 15.1764, acc 0.484375, prec 0.0286374, recall 0.746988
2017-12-10T15:23:43.175943: step 184, loss 3.112, acc 0.53125, prec 0.0286632, recall 0.748503
2017-12-10T15:23:43.371043: step 185, loss 3.07392, acc 0.5625, prec 0.0284803, recall 0.748503
2017-12-10T15:23:43.561601: step 186, loss 4.72783, acc 0.359375, prec 0.0282167, recall 0.748503
2017-12-10T15:23:43.758263: step 187, loss 3.52257, acc 0.453125, prec 0.0279955, recall 0.748503
2017-12-10T15:23:43.956144: step 188, loss 2.71726, acc 0.59375, prec 0.0278334, recall 0.748503
2017-12-10T15:23:44.152803: step 189, loss 2.24672, acc 0.515625, prec 0.0276426, recall 0.748503
2017-12-10T15:23:44.350548: step 190, loss 3.37751, acc 0.46875, prec 0.0274363, recall 0.748503
2017-12-10T15:23:44.548318: step 191, loss 1.93177, acc 0.578125, prec 0.0272747, recall 0.748503
2017-12-10T15:23:44.738394: step 192, loss 1.70991, acc 0.609375, prec 0.0273378, recall 0.75
2017-12-10T15:23:44.933830: step 193, loss 1.79602, acc 0.703125, prec 0.0272256, recall 0.75
2017-12-10T15:23:45.128042: step 194, loss 18.306, acc 0.703125, prec 0.0273353, recall 0.74269
2017-12-10T15:23:45.318855: step 195, loss 1.47618, acc 0.734375, prec 0.0274443, recall 0.744186
2017-12-10T15:23:45.510737: step 196, loss 1.44483, acc 0.6875, prec 0.0273271, recall 0.744186
2017-12-10T15:23:45.705416: step 197, loss 1.38966, acc 0.78125, prec 0.0272456, recall 0.744186
2017-12-10T15:23:45.904973: step 198, loss 0.853094, acc 0.796875, prec 0.0271705, recall 0.744186
2017-12-10T15:23:46.096479: step 199, loss 0.752697, acc 0.828125, prec 0.0271072, recall 0.744186
2017-12-10T15:23:46.291143: step 200, loss 15.1358, acc 0.765625, prec 0.0274377, recall 0.742857
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_2/1512937383/checkpoints/model-200

2017-12-10T15:23:47.445895: step 201, loss 1.00902, acc 0.734375, prec 0.0277486, recall 0.745763
2017-12-10T15:23:47.642920: step 202, loss 7.11751, acc 0.78125, prec 0.027673, recall 0.741573
2017-12-10T15:23:47.836654: step 203, loss 1.14507, acc 0.6875, prec 0.0275574, recall 0.741573
2017-12-10T15:23:48.033809: step 204, loss 18.4764, acc 0.6875, prec 0.0274485, recall 0.73743
2017-12-10T15:23:48.226785: step 205, loss 1.43651, acc 0.78125, prec 0.0273689, recall 0.73743
2017-12-10T15:23:48.419632: step 206, loss 1.33212, acc 0.75, prec 0.0272784, recall 0.73743
2017-12-10T15:23:48.617204: step 207, loss 15.2166, acc 0.6875, prec 0.0273719, recall 0.734807
2017-12-10T15:23:48.817942: step 208, loss 2.8813, acc 0.65625, prec 0.0272541, recall 0.730769
2017-12-10T15:23:49.011853: step 209, loss 1.97784, acc 0.640625, prec 0.0271262, recall 0.730769
2017-12-10T15:23:49.206731: step 210, loss 2.0006, acc 0.59375, prec 0.0273778, recall 0.733696
2017-12-10T15:23:49.404466: step 211, loss 1.84133, acc 0.5625, prec 0.0274194, recall 0.735135
2017-12-10T15:23:49.594928: step 212, loss 3.2332, acc 0.578125, prec 0.0278557, recall 0.739362
2017-12-10T15:23:49.787356: step 213, loss 2.44054, acc 0.578125, prec 0.0278996, recall 0.740741
2017-12-10T15:23:49.980293: step 214, loss 2.70476, acc 0.4375, prec 0.0282776, recall 0.744792
2017-12-10T15:23:50.172107: step 215, loss 1.93858, acc 0.59375, prec 0.0283242, recall 0.746114
2017-12-10T15:23:50.368532: step 216, loss 2.6426, acc 0.46875, prec 0.028136, recall 0.746114
2017-12-10T15:23:50.565283: step 217, loss 1.87198, acc 0.609375, prec 0.0281882, recall 0.747423
2017-12-10T15:23:50.761255: step 218, loss 4.56969, acc 0.59375, prec 0.0284278, recall 0.746193
2017-12-10T15:23:50.956649: step 219, loss 2.00818, acc 0.546875, prec 0.0284561, recall 0.747475
2017-12-10T15:23:51.147632: step 220, loss 1.93791, acc 0.609375, prec 0.0283199, recall 0.747475
2017-12-10T15:23:51.338484: step 221, loss 1.83052, acc 0.625, prec 0.0283755, recall 0.748744
2017-12-10T15:23:51.531631: step 222, loss 2.01456, acc 0.59375, prec 0.0284199, recall 0.75
2017-12-10T15:23:51.724498: step 223, loss 4.26173, acc 0.671875, prec 0.0283126, recall 0.746269
2017-12-10T15:23:51.919410: step 224, loss 1.3328, acc 0.734375, prec 0.0285875, recall 0.748768
2017-12-10T15:23:52.120962: step 225, loss 1.83958, acc 0.578125, prec 0.0286249, recall 0.75
2017-12-10T15:23:52.314012: step 226, loss 5.37137, acc 0.640625, prec 0.0285075, recall 0.746341
2017-12-10T15:23:52.514601: step 227, loss 3.1113, acc 0.6875, prec 0.0291226, recall 0.751196
2017-12-10T15:23:52.715806: step 228, loss 11.5967, acc 0.625, prec 0.0289989, recall 0.747619
2017-12-10T15:23:52.907982: step 229, loss 3.37143, acc 0.5625, prec 0.0293848, recall 0.751174
2017-12-10T15:23:53.101114: step 230, loss 2.45142, acc 0.609375, prec 0.0294279, recall 0.752336
2017-12-10T15:23:53.292870: step 231, loss 3.26724, acc 0.578125, prec 0.0296364, recall 0.75463
2017-12-10T15:23:53.488100: step 232, loss 3.34676, acc 0.3125, prec 0.0295762, recall 0.75576
2017-12-10T15:23:53.685589: step 233, loss 3.64037, acc 0.453125, prec 0.0293907, recall 0.75576
2017-12-10T15:23:53.881376: step 234, loss 3.9477, acc 0.3125, prec 0.0291607, recall 0.75576
2017-12-10T15:23:54.073470: step 235, loss 4.966, acc 0.296875, prec 0.0289293, recall 0.75576
2017-12-10T15:23:54.270842: step 236, loss 3.37285, acc 0.328125, prec 0.0290515, recall 0.757991
2017-12-10T15:23:54.466724: step 237, loss 3.2175, acc 0.375, prec 0.0291869, recall 0.760181
2017-12-10T15:23:54.663728: step 238, loss 2.5687, acc 0.515625, prec 0.0297013, recall 0.764444
2017-12-10T15:23:54.857616: step 239, loss 2.90879, acc 0.484375, prec 0.0296996, recall 0.765487
2017-12-10T15:23:55.056232: step 240, loss 3.51989, acc 0.46875, prec 0.0296928, recall 0.76652
2017-12-10T15:23:55.245493: step 241, loss 2.19281, acc 0.515625, prec 0.0297013, recall 0.767544
2017-12-10T15:23:55.439063: step 242, loss 4.79382, acc 0.546875, prec 0.0297247, recall 0.765217
2017-12-10T15:23:55.632628: step 243, loss 1.6889, acc 0.609375, prec 0.0297629, recall 0.766234
2017-12-10T15:23:55.823361: step 244, loss 1.8357, acc 0.59375, prec 0.0297958, recall 0.767241
2017-12-10T15:23:56.015054: step 245, loss 2.53758, acc 0.671875, prec 0.0298532, recall 0.76824
2017-12-10T15:23:56.219652: step 246, loss 1.31038, acc 0.671875, prec 0.029749, recall 0.76824
2017-12-10T15:23:56.416947: step 247, loss 1.17286, acc 0.609375, prec 0.029626, recall 0.76824
2017-12-10T15:23:56.612795: step 248, loss 1.20091, acc 0.78125, prec 0.0297177, recall 0.769231
2017-12-10T15:23:56.808935: step 249, loss 0.643335, acc 0.8125, prec 0.0298188, recall 0.770213
2017-12-10T15:23:57.001180: step 250, loss 8.54106, acc 0.75, prec 0.0297453, recall 0.766949
2017-12-10T15:23:57.206711: step 251, loss 11.6928, acc 0.703125, prec 0.0296575, recall 0.763713
2017-12-10T15:23:57.401958: step 252, loss 1.62194, acc 0.8125, prec 0.029758, recall 0.764706
2017-12-10T15:23:57.599975: step 253, loss 4.55673, acc 0.796875, prec 0.0296997, recall 0.761506
2017-12-10T15:23:57.793449: step 254, loss 8.56431, acc 0.765625, prec 0.02979, recall 0.759336
2017-12-10T15:23:57.985935: step 255, loss 1.53036, acc 0.59375, prec 0.0298217, recall 0.760331
2017-12-10T15:23:58.185172: step 256, loss 1.19628, acc 0.6875, prec 0.0297254, recall 0.760331
2017-12-10T15:23:58.381723: step 257, loss 1.64894, acc 0.609375, prec 0.0296058, recall 0.760331
2017-12-10T15:23:58.587326: step 258, loss 2.35005, acc 0.484375, prec 0.0294494, recall 0.760331
2017-12-10T15:23:58.790319: step 259, loss 1.93496, acc 0.546875, prec 0.0293134, recall 0.760331
2017-12-10T15:23:58.987696: step 260, loss 2.19083, acc 0.46875, prec 0.0291554, recall 0.760331
2017-12-10T15:23:59.188975: step 261, loss 2.78235, acc 0.484375, prec 0.0291568, recall 0.761317
2017-12-10T15:23:59.390940: step 262, loss 2.1874, acc 0.515625, prec 0.0290151, recall 0.761317
2017-12-10T15:23:59.589959: step 263, loss 1.59248, acc 0.609375, prec 0.0290534, recall 0.762295
2017-12-10T15:23:59.783083: step 264, loss 1.54366, acc 0.625, prec 0.0289449, recall 0.762295
2017-12-10T15:23:59.976842: step 265, loss 2.68732, acc 0.484375, prec 0.0289474, recall 0.763265
2017-12-10T15:24:00.167552: step 266, loss 1.96987, acc 0.5625, prec 0.0289721, recall 0.764228
2017-12-10T15:24:00.361369: step 267, loss 1.67044, acc 0.59375, prec 0.0288565, recall 0.764228
2017-12-10T15:24:00.550880: step 268, loss 5.61114, acc 0.65625, prec 0.0287638, recall 0.761134
2017-12-10T15:24:00.745749: step 269, loss 4.36504, acc 0.609375, prec 0.0286585, recall 0.758065
2017-12-10T15:24:00.943790: step 270, loss 2.0992, acc 0.78125, prec 0.0286019, recall 0.75502
2017-12-10T15:24:01.142224: step 271, loss 1.09197, acc 0.71875, prec 0.0285237, recall 0.75502
2017-12-10T15:24:01.345028: step 272, loss 1.60148, acc 0.5625, prec 0.0284031, recall 0.75502
2017-12-10T15:24:01.542121: step 273, loss 1.34365, acc 0.734375, prec 0.0286231, recall 0.756972
2017-12-10T15:24:01.738130: step 274, loss 5.1497, acc 0.5, prec 0.0287813, recall 0.755906
2017-12-10T15:24:01.936551: step 275, loss 1.82081, acc 0.5, prec 0.0287888, recall 0.756863
2017-12-10T15:24:02.130176: step 276, loss 1.80221, acc 0.53125, prec 0.0286605, recall 0.756863
2017-12-10T15:24:02.323763: step 277, loss 12.8774, acc 0.65625, prec 0.0287152, recall 0.754864
2017-12-10T15:24:02.520797: step 278, loss 2.7147, acc 0.4375, prec 0.028706, recall 0.755814
2017-12-10T15:24:02.715331: step 279, loss 4.22649, acc 0.515625, prec 0.0285798, recall 0.752896
2017-12-10T15:24:02.911830: step 280, loss 4.48865, acc 0.53125, prec 0.0287423, recall 0.751908
2017-12-10T15:24:03.109780: step 281, loss 2.44934, acc 0.515625, prec 0.029036, recall 0.754717
2017-12-10T15:24:03.301769: step 282, loss 2.34121, acc 0.4375, prec 0.0290253, recall 0.755639
2017-12-10T15:24:03.491100: step 283, loss 4.03285, acc 0.421875, prec 0.0292893, recall 0.758364
2017-12-10T15:24:03.685142: step 284, loss 3.11221, acc 0.484375, prec 0.0291512, recall 0.758364
2017-12-10T15:24:03.879768: step 285, loss 6.24061, acc 0.390625, prec 0.0291317, recall 0.756458
2017-12-10T15:24:04.076059: step 286, loss 3.58451, acc 0.34375, prec 0.0293702, recall 0.759124
2017-12-10T15:24:04.267571: step 287, loss 4.25237, acc 0.3125, prec 0.0293251, recall 0.76
2017-12-10T15:24:04.462196: step 288, loss 3.0937, acc 0.390625, prec 0.029301, recall 0.76087
2017-12-10T15:24:04.656596: step 289, loss 2.61089, acc 0.390625, prec 0.0291424, recall 0.76087
2017-12-10T15:24:04.848089: step 290, loss 2.41274, acc 0.4375, prec 0.0292656, recall 0.76259
2017-12-10T15:24:05.045560: step 291, loss 3.15967, acc 0.421875, prec 0.0291169, recall 0.76259
2017-12-10T15:24:05.239729: step 292, loss 2.33155, acc 0.453125, prec 0.029243, recall 0.764286
2017-12-10T15:24:05.439004: step 293, loss 7.99352, acc 0.515625, prec 0.0292557, recall 0.762411
2017-12-10T15:24:05.630810: step 294, loss 1.53924, acc 0.546875, prec 0.0294038, recall 0.764085
2017-12-10T15:24:05.830029: step 295, loss 1.39529, acc 0.6875, prec 0.0293243, recall 0.764085
2017-12-10T15:24:06.024506: step 296, loss 5.34062, acc 0.5625, prec 0.0293484, recall 0.762238
2017-12-10T15:24:06.217487: step 297, loss 8.62721, acc 0.625, prec 0.0292578, recall 0.759582
2017-12-10T15:24:06.413215: step 298, loss 1.46742, acc 0.6875, prec 0.0291795, recall 0.759582
2017-12-10T15:24:06.606946: step 299, loss 1.74591, acc 0.609375, prec 0.0292117, recall 0.760417
2017-12-10T15:24:06.803403: step 300, loss 1.64521, acc 0.625, prec 0.0292475, recall 0.761246
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_2/1512937383/checkpoints/model-300

2017-12-10T15:24:08.116104: step 301, loss 1.66323, acc 0.515625, prec 0.029256, recall 0.762069
2017-12-10T15:24:08.310507: step 302, loss 2.26739, acc 0.5, prec 0.0292606, recall 0.762887
2017-12-10T15:24:08.505774: step 303, loss 1.9712, acc 0.671875, prec 0.029435, recall 0.764505
2017-12-10T15:24:08.698796: step 304, loss 1.66591, acc 0.59375, prec 0.0294618, recall 0.765306
2017-12-10T15:24:08.890659: step 305, loss 1.21931, acc 0.6875, prec 0.0295116, recall 0.766102
2017-12-10T15:24:09.084156: step 306, loss 1.40925, acc 0.625, prec 0.0294194, recall 0.766102
2017-12-10T15:24:09.277317: step 307, loss 1.06421, acc 0.6875, prec 0.029469, recall 0.766892
2017-12-10T15:24:09.473907: step 308, loss 3.82541, acc 0.703125, prec 0.029526, recall 0.765101
2017-12-10T15:24:09.671339: step 309, loss 1.50312, acc 0.6875, prec 0.0295751, recall 0.765886
2017-12-10T15:24:09.864204: step 310, loss 0.94615, acc 0.78125, prec 0.0295217, recall 0.765886
2017-12-10T15:24:10.059997: step 311, loss 1.1393, acc 0.75, prec 0.029461, recall 0.765886
2017-12-10T15:24:10.257867: step 312, loss 1.00387, acc 0.75, prec 0.0294004, recall 0.765886
2017-12-10T15:24:10.448316: step 313, loss 16.9289, acc 0.703125, prec 0.029457, recall 0.76412
2017-12-10T15:24:10.646952: step 314, loss 3.41177, acc 0.796875, prec 0.0294118, recall 0.761589
2017-12-10T15:24:10.843068: step 315, loss 1.72035, acc 0.71875, prec 0.029468, recall 0.762376
2017-12-10T15:24:11.036408: step 316, loss 2.04273, acc 0.578125, prec 0.0293669, recall 0.762376
2017-12-10T15:24:11.233077: step 317, loss 1.94388, acc 0.546875, prec 0.0295049, recall 0.763934
2017-12-10T15:24:11.423967: step 318, loss 1.71103, acc 0.59375, prec 0.0295305, recall 0.764706
2017-12-10T15:24:11.620189: step 319, loss 2.36535, acc 0.453125, prec 0.0294007, recall 0.764706
2017-12-10T15:24:11.813728: step 320, loss 1.88973, acc 0.53125, prec 0.0292903, recall 0.764706
2017-12-10T15:24:12.013219: step 321, loss 1.73828, acc 0.578125, prec 0.0294338, recall 0.766234
2017-12-10T15:24:12.209840: step 322, loss 1.74854, acc 0.5, prec 0.0294373, recall 0.76699
2017-12-10T15:24:12.403960: step 323, loss 2.30044, acc 0.453125, prec 0.0293099, recall 0.76699
2017-12-10T15:24:12.598131: step 324, loss 1.60209, acc 0.625, prec 0.0292232, recall 0.76699
2017-12-10T15:24:12.790077: step 325, loss 11.3965, acc 0.53125, prec 0.0292419, recall 0.762821
2017-12-10T15:24:12.989816: step 326, loss 1.38814, acc 0.609375, prec 0.0291524, recall 0.762821
2017-12-10T15:24:13.184239: step 327, loss 1.8022, acc 0.640625, prec 0.0290705, recall 0.762821
2017-12-10T15:24:13.375360: step 328, loss 3.77541, acc 0.578125, prec 0.0289784, recall 0.760383
2017-12-10T15:24:13.573452: step 329, loss 1.1139, acc 0.671875, prec 0.0289045, recall 0.760383
2017-12-10T15:24:13.768027: step 330, loss 1.77568, acc 0.625, prec 0.0288205, recall 0.760383
2017-12-10T15:24:13.966814: step 331, loss 8.6458, acc 0.546875, prec 0.0287231, recall 0.757962
2017-12-10T15:24:14.165060: step 332, loss 2.00686, acc 0.5625, prec 0.0287432, recall 0.75873
2017-12-10T15:24:14.362267: step 333, loss 1.43019, acc 0.609375, prec 0.0287735, recall 0.759494
2017-12-10T15:24:14.555700: step 334, loss 2.07475, acc 0.515625, prec 0.028667, recall 0.759494
2017-12-10T15:24:14.750419: step 335, loss 1.85222, acc 0.625, prec 0.028932, recall 0.761755
2017-12-10T15:24:14.945386: step 336, loss 2.52332, acc 0.484375, prec 0.0288188, recall 0.761755
2017-12-10T15:24:15.140033: step 337, loss 1.51796, acc 0.59375, prec 0.0287302, recall 0.761755
2017-12-10T15:24:15.334785: step 338, loss 4.26295, acc 0.71875, prec 0.0287872, recall 0.760125
2017-12-10T15:24:15.527393: step 339, loss 1.50226, acc 0.65625, prec 0.0288269, recall 0.76087
2017-12-10T15:24:15.717961: step 340, loss 1.09671, acc 0.65625, prec 0.0287525, recall 0.76087
2017-12-10T15:24:15.912139: step 341, loss 5.87596, acc 0.65625, prec 0.0286818, recall 0.758514
2017-12-10T15:24:16.110285: step 342, loss 1.56897, acc 0.6875, prec 0.0286148, recall 0.758514
2017-12-10T15:24:16.300519: step 343, loss 1.22523, acc 0.65625, prec 0.0286546, recall 0.759259
2017-12-10T15:24:16.491890: step 344, loss 1.36704, acc 0.578125, prec 0.0286776, recall 0.76
2017-12-10T15:24:16.684356: step 345, loss 0.799047, acc 0.75, prec 0.028737, recall 0.760736
2017-12-10T15:24:16.882020: step 346, loss 1.88366, acc 0.703125, prec 0.0288984, recall 0.762195
2017-12-10T15:24:17.075814: step 347, loss 1.21093, acc 0.703125, prec 0.0288351, recall 0.762195
2017-12-10T15:24:17.277437: step 348, loss 0.975412, acc 0.765625, prec 0.0288971, recall 0.762918
2017-12-10T15:24:17.472899: step 349, loss 1.16256, acc 0.703125, prec 0.0289456, recall 0.763636
2017-12-10T15:24:17.664779: step 350, loss 0.927368, acc 0.765625, prec 0.0290071, recall 0.76435
2017-12-10T15:24:17.860879: step 351, loss 2.86266, acc 0.75, prec 0.0291795, recall 0.763473
2017-12-10T15:24:18.057268: step 352, loss 1.2349, acc 0.703125, prec 0.0292271, recall 0.764179
2017-12-10T15:24:18.253635: step 353, loss 0.653191, acc 0.8125, prec 0.0291871, recall 0.764179
2017-12-10T15:24:18.448779: step 354, loss 1.0843, acc 0.796875, prec 0.0291439, recall 0.764179
2017-12-10T15:24:18.641782: step 355, loss 0.749985, acc 0.84375, prec 0.0291108, recall 0.764179
2017-12-10T15:24:18.834958: step 356, loss 1.26553, acc 0.765625, prec 0.0291714, recall 0.764881
2017-12-10T15:24:19.029374: step 357, loss 0.874051, acc 0.75, prec 0.0293385, recall 0.766272
2017-12-10T15:24:19.225219: step 358, loss 2.28195, acc 0.765625, prec 0.029292, recall 0.764012
2017-12-10T15:24:19.420700: step 359, loss 0.530717, acc 0.828125, prec 0.0292556, recall 0.764012
2017-12-10T15:24:19.618999: step 360, loss 12.4925, acc 0.828125, prec 0.0292226, recall 0.761765
2017-12-10T15:24:19.821866: step 361, loss 2.04342, acc 0.75, prec 0.0291732, recall 0.759531
2017-12-10T15:24:20.016170: step 362, loss 1.14315, acc 0.703125, prec 0.0291109, recall 0.759531
2017-12-10T15:24:20.209619: step 363, loss 0.820084, acc 0.8125, prec 0.0291807, recall 0.760234
2017-12-10T15:24:20.404375: step 364, loss 1.22617, acc 0.65625, prec 0.0291088, recall 0.760234
2017-12-10T15:24:20.599240: step 365, loss 1.20395, acc 0.6875, prec 0.0291522, recall 0.760933
2017-12-10T15:24:20.795588: step 366, loss 0.704709, acc 0.765625, prec 0.0291035, recall 0.760933
2017-12-10T15:24:20.998995: step 367, loss 1.44426, acc 0.671875, prec 0.0290355, recall 0.760933
2017-12-10T15:24:21.191869: step 368, loss 3.7105, acc 0.671875, prec 0.028971, recall 0.758721
2017-12-10T15:24:21.393250: step 369, loss 2.30694, acc 0.5625, prec 0.0289887, recall 0.75942
2017-12-10T15:24:21.589136: step 370, loss 1.15118, acc 0.6875, prec 0.0291391, recall 0.760807
2017-12-10T15:24:21.782107: step 371, loss 1.61459, acc 0.59375, prec 0.0290557, recall 0.760807
2017-12-10T15:24:21.976058: step 372, loss 5.05604, acc 0.59375, prec 0.028976, recall 0.758621
2017-12-10T15:24:22.170613: step 373, loss 2.24733, acc 0.53125, prec 0.0288809, recall 0.758621
2017-12-10T15:24:22.368368: step 374, loss 1.31176, acc 0.671875, prec 0.0288147, recall 0.758621
2017-12-10T15:24:22.564112: step 375, loss 1.31256, acc 0.609375, prec 0.028842, recall 0.759312
2017-12-10T15:24:22.760560: step 376, loss 1.34414, acc 0.65625, prec 0.0287731, recall 0.759312
2017-12-10T15:24:22.956030: step 377, loss 1.08304, acc 0.6875, prec 0.0287107, recall 0.759312
2017-12-10T15:24:23.147818: step 378, loss 1.6605, acc 0.65625, prec 0.0286425, recall 0.759312
2017-12-10T15:24:23.342368: step 379, loss 1.58083, acc 0.671875, prec 0.0285776, recall 0.759312
2017-12-10T15:24:23.535975: step 380, loss 1.50031, acc 0.71875, prec 0.0285222, recall 0.759312
2017-12-10T15:24:23.736808: step 381, loss 0.923782, acc 0.765625, prec 0.0284763, recall 0.759312
2017-12-10T15:24:23.940942: step 382, loss 0.585766, acc 0.84375, prec 0.0284457, recall 0.759312
2017-12-10T15:24:24.137010: step 383, loss 1.34621, acc 0.8125, prec 0.0285132, recall 0.76
2017-12-10T15:24:24.331055: step 384, loss 3.79381, acc 0.8125, prec 0.0284797, recall 0.757835
2017-12-10T15:24:24.528668: step 385, loss 0.537337, acc 0.8125, prec 0.0284431, recall 0.757835
2017-12-10T15:24:24.721879: step 386, loss 5.27413, acc 0.78125, prec 0.0284067, recall 0.753541
2017-12-10T15:24:24.916172: step 387, loss 0.657783, acc 0.78125, prec 0.0284679, recall 0.754237
2017-12-10T15:24:25.108679: step 388, loss 0.965109, acc 0.734375, prec 0.0285197, recall 0.75493
2017-12-10T15:24:25.309322: step 389, loss 7.47712, acc 0.578125, prec 0.0284441, recall 0.7507
2017-12-10T15:24:25.499896: step 390, loss 5.2381, acc 0.5625, prec 0.0283628, recall 0.748603
2017-12-10T15:24:25.691897: step 391, loss 2.01384, acc 0.5, prec 0.0283695, recall 0.749304
2017-12-10T15:24:25.881299: step 392, loss 2.15503, acc 0.515625, prec 0.0282771, recall 0.749304
2017-12-10T15:24:26.078102: step 393, loss 3.25951, acc 0.296875, prec 0.028144, recall 0.749304
2017-12-10T15:24:26.277137: step 394, loss 3.28371, acc 0.25, prec 0.0280033, recall 0.749304
2017-12-10T15:24:26.473469: step 395, loss 2.21364, acc 0.484375, prec 0.0280083, recall 0.75
2017-12-10T15:24:26.665681: step 396, loss 3.45893, acc 0.328125, prec 0.0278839, recall 0.75
2017-12-10T15:24:26.855699: step 397, loss 3.39927, acc 0.40625, prec 0.0278749, recall 0.750693
2017-12-10T15:24:27.051528: step 398, loss 2.27465, acc 0.453125, prec 0.0278746, recall 0.751381
2017-12-10T15:24:27.247334: step 399, loss 2.79575, acc 0.40625, prec 0.0278657, recall 0.752066
2017-12-10T15:24:27.438785: step 400, loss 2.52212, acc 0.453125, prec 0.0277665, recall 0.752066
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_2/1512937383/checkpoints/model-400

2017-12-10T15:24:28.712971: step 401, loss 2.15092, acc 0.59375, prec 0.0278905, recall 0.753425
2017-12-10T15:24:28.908418: step 402, loss 1.83958, acc 0.640625, prec 0.0279239, recall 0.754098
2017-12-10T15:24:29.107476: step 403, loss 1.48881, acc 0.71875, prec 0.0279713, recall 0.754768
2017-12-10T15:24:29.310151: step 404, loss 0.921785, acc 0.703125, prec 0.0279178, recall 0.754768
2017-12-10T15:24:29.499986: step 405, loss 1.45488, acc 0.71875, prec 0.0278672, recall 0.754768
2017-12-10T15:24:29.694796: step 406, loss 2.96958, acc 0.78125, prec 0.0279285, recall 0.753388
2017-12-10T15:24:29.890106: step 407, loss 0.915885, acc 0.859375, prec 0.0280983, recall 0.754717
2017-12-10T15:24:30.084542: step 408, loss 1.9195, acc 0.734375, prec 0.0280533, recall 0.752688
2017-12-10T15:24:30.280260: step 409, loss 4.38921, acc 0.78125, prec 0.0280168, recall 0.75067
2017-12-10T15:24:30.475662: step 410, loss 6.10102, acc 0.765625, prec 0.0281746, recall 0.748011
2017-12-10T15:24:30.672632: step 411, loss 9.98468, acc 0.671875, prec 0.0282154, recall 0.746702
2017-12-10T15:24:30.865520: step 412, loss 1.10731, acc 0.609375, prec 0.0282418, recall 0.747368
2017-12-10T15:24:31.061122: step 413, loss 1.45343, acc 0.625, prec 0.0281746, recall 0.747368
2017-12-10T15:24:31.259698: step 414, loss 1.51737, acc 0.546875, prec 0.0280938, recall 0.747368
2017-12-10T15:24:31.453068: step 415, loss 2.19837, acc 0.46875, prec 0.0279996, recall 0.747368
2017-12-10T15:24:31.650420: step 416, loss 2.83113, acc 0.390625, prec 0.0278924, recall 0.747368
2017-12-10T15:24:31.843845: step 417, loss 1.6823, acc 0.515625, prec 0.0279029, recall 0.748031
2017-12-10T15:24:32.043739: step 418, loss 2.82228, acc 0.59375, prec 0.0280219, recall 0.749347
2017-12-10T15:24:32.236808: step 419, loss 1.49028, acc 0.640625, prec 0.0281484, recall 0.750649
2017-12-10T15:24:32.431927: step 420, loss 2.86746, acc 0.46875, prec 0.0281499, recall 0.751295
2017-12-10T15:24:32.629350: step 421, loss 2.28988, acc 0.515625, prec 0.0282535, recall 0.752577
2017-12-10T15:24:32.820440: step 422, loss 1.92101, acc 0.515625, prec 0.0282628, recall 0.753213
2017-12-10T15:24:33.017433: step 423, loss 2.12995, acc 0.546875, prec 0.0281839, recall 0.753213
2017-12-10T15:24:33.215705: step 424, loss 1.55613, acc 0.625, prec 0.028119, recall 0.753213
2017-12-10T15:24:33.406453: step 425, loss 1.17227, acc 0.6875, prec 0.0280651, recall 0.753213
2017-12-10T15:24:33.604542: step 426, loss 5.89602, acc 0.578125, prec 0.0283668, recall 0.753807
2017-12-10T15:24:33.800532: step 427, loss 1.39315, acc 0.640625, prec 0.0283046, recall 0.753807
2017-12-10T15:24:33.991911: step 428, loss 10.8782, acc 0.59375, prec 0.0282373, recall 0.751899
2017-12-10T15:24:34.184620: step 429, loss 1.14528, acc 0.71875, prec 0.0281891, recall 0.751899
2017-12-10T15:24:34.376905: step 430, loss 1.09775, acc 0.734375, prec 0.0281437, recall 0.751899
2017-12-10T15:24:34.577740: step 431, loss 1.27622, acc 0.703125, prec 0.028185, recall 0.752525
2017-12-10T15:24:34.770029: step 432, loss 2.42216, acc 0.671875, prec 0.0282209, recall 0.753149
2017-12-10T15:24:34.963712: step 433, loss 0.946386, acc 0.625, prec 0.0282486, recall 0.753769
2017-12-10T15:24:35.156103: step 434, loss 1.51775, acc 0.625, prec 0.0281849, recall 0.753769
2017-12-10T15:24:35.349507: step 435, loss 0.963872, acc 0.71875, prec 0.0281373, recall 0.753769
2017-12-10T15:24:35.543696: step 436, loss 1.05295, acc 0.734375, prec 0.0281835, recall 0.754386
2017-12-10T15:24:35.735668: step 437, loss 1.75805, acc 0.75, prec 0.0283231, recall 0.755611
2017-12-10T15:24:35.930664: step 438, loss 0.799979, acc 0.75, prec 0.0283714, recall 0.756219
2017-12-10T15:24:36.128631: step 439, loss 0.960207, acc 0.6875, prec 0.0283186, recall 0.756219
2017-12-10T15:24:36.322938: step 440, loss 0.697633, acc 0.765625, prec 0.0283695, recall 0.756824
2017-12-10T15:24:36.514528: step 441, loss 1.90824, acc 0.71875, prec 0.0285025, recall 0.758025
2017-12-10T15:24:36.713817: step 442, loss 1.13117, acc 0.671875, prec 0.028447, recall 0.758025
2017-12-10T15:24:36.912305: step 443, loss 0.552924, acc 0.828125, prec 0.028508, recall 0.758621
2017-12-10T15:24:37.106763: step 444, loss 3.76206, acc 0.8125, prec 0.0285688, recall 0.757353
2017-12-10T15:24:37.305819: step 445, loss 0.760544, acc 0.765625, prec 0.0286189, recall 0.757946
2017-12-10T15:24:37.499191: step 446, loss 0.728995, acc 0.78125, prec 0.0286715, recall 0.758537
2017-12-10T15:24:37.692931: step 447, loss 4.86416, acc 0.8125, prec 0.0286425, recall 0.756691
2017-12-10T15:24:37.889303: step 448, loss 0.629805, acc 0.734375, prec 0.028687, recall 0.757282
2017-12-10T15:24:38.087858: step 449, loss 1.03191, acc 0.671875, prec 0.0286317, recall 0.757282
2017-12-10T15:24:38.284125: step 450, loss 0.850364, acc 0.734375, prec 0.0286761, recall 0.757869
2017-12-10T15:24:38.478690: step 451, loss 0.680144, acc 0.78125, prec 0.0286394, recall 0.757869
2017-12-10T15:24:38.672921: step 452, loss 1.82767, acc 0.625, prec 0.0286653, recall 0.758454
2017-12-10T15:24:38.868318: step 453, loss 0.833506, acc 0.75, prec 0.0286235, recall 0.758454
2017-12-10T15:24:39.062624: step 454, loss 1.03378, acc 0.703125, prec 0.028574, recall 0.758454
2017-12-10T15:24:39.261649: step 455, loss 0.95263, acc 0.765625, prec 0.0286234, recall 0.759036
2017-12-10T15:24:39.455295: step 456, loss 0.949654, acc 0.75, prec 0.0286699, recall 0.759615
2017-12-10T15:24:39.651360: step 457, loss 2.39896, acc 0.828125, prec 0.028732, recall 0.758373
2017-12-10T15:24:39.848122: step 458, loss 0.779361, acc 0.796875, prec 0.0287861, recall 0.75895
2017-12-10T15:24:40.045789: step 459, loss 4.7197, acc 0.71875, prec 0.0288322, recall 0.755924
2017-12-10T15:24:40.242124: step 460, loss 1.88394, acc 0.65625, prec 0.0289502, recall 0.757075
2017-12-10T15:24:40.434107: step 461, loss 1.3195, acc 0.71875, prec 0.0289033, recall 0.757075
2017-12-10T15:24:40.627030: step 462, loss 8.78254, acc 0.734375, prec 0.0288643, recall 0.753521
2017-12-10T15:24:40.820405: step 463, loss 1.30282, acc 0.640625, prec 0.0288919, recall 0.754098
2017-12-10T15:24:41.013838: step 464, loss 2.11379, acc 0.546875, prec 0.0289038, recall 0.754673
2017-12-10T15:24:41.208734: step 465, loss 2.01823, acc 0.484375, prec 0.0288187, recall 0.754673
2017-12-10T15:24:41.405529: step 466, loss 2.41332, acc 0.359375, prec 0.0288863, recall 0.755814
2017-12-10T15:24:41.598259: step 467, loss 3.17883, acc 0.296875, prec 0.0287712, recall 0.755814
2017-12-10T15:24:41.790491: step 468, loss 7.24836, acc 0.40625, prec 0.0288487, recall 0.755196
2017-12-10T15:24:41.983839: step 469, loss 3.03441, acc 0.453125, prec 0.0288453, recall 0.75576
2017-12-10T15:24:42.178047: step 470, loss 2.90241, acc 0.40625, prec 0.0288344, recall 0.756322
2017-12-10T15:24:42.370375: step 471, loss 7.6535, acc 0.484375, prec 0.0288386, recall 0.755149
2017-12-10T15:24:42.565503: step 472, loss 2.37416, acc 0.390625, prec 0.0288252, recall 0.755708
2017-12-10T15:24:42.761693: step 473, loss 2.81207, acc 0.453125, prec 0.0288219, recall 0.756264
2017-12-10T15:24:42.953647: step 474, loss 3.14092, acc 0.390625, prec 0.0288927, recall 0.75737
2017-12-10T15:24:43.147533: step 475, loss 3.27394, acc 0.34375, prec 0.0291229, recall 0.759551
2017-12-10T15:24:43.338857: step 476, loss 1.89911, acc 0.546875, prec 0.0293006, recall 0.761161
2017-12-10T15:24:43.533260: step 477, loss 1.67556, acc 0.609375, prec 0.0294042, recall 0.762222
2017-12-10T15:24:43.724230: step 478, loss 1.62882, acc 0.578125, prec 0.0294193, recall 0.762749
2017-12-10T15:24:43.918796: step 479, loss 1.83425, acc 0.578125, prec 0.0294343, recall 0.763274
2017-12-10T15:24:44.116260: step 480, loss 3.03039, acc 0.640625, prec 0.029542, recall 0.764317
2017-12-10T15:24:44.313745: step 481, loss 0.738616, acc 0.71875, prec 0.0295793, recall 0.764835
2017-12-10T15:24:44.506623: step 482, loss 1.46565, acc 0.65625, prec 0.0296064, recall 0.765351
2017-12-10T15:24:44.697289: step 483, loss 2.20575, acc 0.71875, prec 0.0295637, recall 0.763676
2017-12-10T15:24:44.895340: step 484, loss 1.08466, acc 0.703125, prec 0.0295162, recall 0.763676
2017-12-10T15:24:45.093343: step 485, loss 2.49827, acc 0.703125, prec 0.0294714, recall 0.762009
2017-12-10T15:24:45.289457: step 486, loss 1.53108, acc 0.765625, prec 0.0295159, recall 0.762527
2017-12-10T15:24:45.480763: step 487, loss 1.03468, acc 0.71875, prec 0.0295529, recall 0.763043
2017-12-10T15:24:45.672251: step 488, loss 1.19219, acc 0.671875, prec 0.0297454, recall 0.764579
2017-12-10T15:24:45.867058: step 489, loss 1.15851, acc 0.703125, prec 0.0297794, recall 0.765086
2017-12-10T15:24:46.058604: step 490, loss 1.10787, acc 0.734375, prec 0.0298182, recall 0.765591
2017-12-10T15:24:46.251001: step 491, loss 1.2892, acc 0.6875, prec 0.0298495, recall 0.766094
2017-12-10T15:24:46.441492: step 492, loss 13.9084, acc 0.71875, prec 0.0298096, recall 0.762821
2017-12-10T15:24:46.635777: step 493, loss 1.46614, acc 0.75, prec 0.0298507, recall 0.763326
2017-12-10T15:24:46.829400: step 494, loss 1.19127, acc 0.703125, prec 0.0298035, recall 0.763326
2017-12-10T15:24:47.024401: step 495, loss 0.975038, acc 0.734375, prec 0.0297614, recall 0.763326
2017-12-10T15:24:47.219801: step 496, loss 1.18561, acc 0.703125, prec 0.0297145, recall 0.763326
2017-12-10T15:24:47.389998: step 497, loss 9.5918, acc 0.730769, prec 0.0296824, recall 0.761702
2017-12-10T15:24:47.591262: step 498, loss 1.33908, acc 0.640625, prec 0.0297062, recall 0.762208
2017-12-10T15:24:47.785855: step 499, loss 1.45317, acc 0.625, prec 0.0296474, recall 0.762208
2017-12-10T15:24:47.977349: step 500, loss 1.09958, acc 0.625, prec 0.0296687, recall 0.762712
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_2/1512937383/checkpoints/model-500

2017-12-10T15:24:49.332840: step 501, loss 1.71161, acc 0.578125, prec 0.0296028, recall 0.762712
2017-12-10T15:24:49.523022: step 502, loss 1.39081, acc 0.546875, prec 0.0295324, recall 0.762712
2017-12-10T15:24:49.716369: step 503, loss 1.38335, acc 0.65625, prec 0.0297176, recall 0.764211
2017-12-10T15:24:49.909258: step 504, loss 1.70958, acc 0.59375, prec 0.0296544, recall 0.764211
2017-12-10T15:24:50.103528: step 505, loss 1.09439, acc 0.671875, prec 0.0296828, recall 0.764706
2017-12-10T15:24:50.310867: step 506, loss 1.11119, acc 0.703125, prec 0.0297159, recall 0.765199
2017-12-10T15:24:50.499315: step 507, loss 1.00214, acc 0.71875, prec 0.0296724, recall 0.765199
2017-12-10T15:24:50.696459: step 508, loss 1.04686, acc 0.703125, prec 0.0296266, recall 0.765199
2017-12-10T15:24:50.890683: step 509, loss 0.846917, acc 0.75, prec 0.0296669, recall 0.76569
2017-12-10T15:24:51.082610: step 510, loss 0.706341, acc 0.765625, prec 0.0297094, recall 0.76618
2017-12-10T15:24:51.277439: step 511, loss 0.531122, acc 0.8125, prec 0.029759, recall 0.766667
2017-12-10T15:24:51.467900: step 512, loss 0.68905, acc 0.78125, prec 0.0297254, recall 0.766667
2017-12-10T15:24:51.662691: step 513, loss 0.704139, acc 0.78125, prec 0.0298483, recall 0.767635
2017-12-10T15:24:51.853357: step 514, loss 0.575163, acc 0.8125, prec 0.0298195, recall 0.767635
2017-12-10T15:24:52.048352: step 515, loss 0.753189, acc 0.78125, prec 0.029864, recall 0.768116
2017-12-10T15:24:52.240500: step 516, loss 0.433106, acc 0.90625, prec 0.0298495, recall 0.768116
2017-12-10T15:24:52.435943: step 517, loss 10.5829, acc 0.890625, prec 0.0298351, recall 0.766529
2017-12-10T15:24:52.632579: step 518, loss 0.395709, acc 0.890625, prec 0.0298963, recall 0.76701
2017-12-10T15:24:52.827840: step 519, loss 0.509669, acc 0.828125, prec 0.0298699, recall 0.76701
2017-12-10T15:24:53.022402: step 520, loss 0.481572, acc 0.90625, prec 0.0299334, recall 0.76749
2017-12-10T15:24:53.216044: step 521, loss 0.480791, acc 0.8125, prec 0.0299046, recall 0.76749
2017-12-10T15:24:53.414381: step 522, loss 0.183667, acc 0.96875, prec 0.0300553, recall 0.768443
2017-12-10T15:24:53.611059: step 523, loss 0.176576, acc 0.953125, prec 0.0300481, recall 0.768443
2017-12-10T15:24:53.809376: step 524, loss 0.166715, acc 0.921875, prec 0.0301137, recall 0.768916
2017-12-10T15:24:54.005466: step 525, loss 0.51785, acc 0.875, prec 0.0301721, recall 0.769388
2017-12-10T15:24:54.196772: step 526, loss 2.66841, acc 0.890625, prec 0.0302352, recall 0.768293
2017-12-10T15:24:54.389359: step 527, loss 2.74101, acc 0.890625, prec 0.0302982, recall 0.767206
2017-12-10T15:24:54.580747: step 528, loss 0.666731, acc 0.890625, prec 0.0304362, recall 0.768145
2017-12-10T15:24:54.772759: step 529, loss 0.493653, acc 0.890625, prec 0.0304192, recall 0.768145
2017-12-10T15:24:54.963611: step 530, loss 0.579191, acc 0.875, prec 0.0305544, recall 0.769076
2017-12-10T15:24:55.161659: step 531, loss 1.59747, acc 0.859375, prec 0.030535, recall 0.767535
2017-12-10T15:24:55.360153: step 532, loss 2.46749, acc 0.71875, prec 0.0307227, recall 0.768924
2017-12-10T15:24:55.555375: step 533, loss 1.33304, acc 0.6875, prec 0.0307509, recall 0.769384
2017-12-10T15:24:55.750080: step 534, loss 1.54761, acc 0.609375, prec 0.0307668, recall 0.769841
2017-12-10T15:24:55.943760: step 535, loss 1.65409, acc 0.609375, prec 0.0307059, recall 0.769841
2017-12-10T15:24:56.135411: step 536, loss 5.32146, acc 0.75, prec 0.0307461, recall 0.768775
2017-12-10T15:24:56.327065: step 537, loss 1.52572, acc 0.640625, prec 0.0307668, recall 0.769231
2017-12-10T15:24:56.523910: step 538, loss 5.02916, acc 0.578125, prec 0.031009, recall 0.769531
2017-12-10T15:24:56.719710: step 539, loss 2.31679, acc 0.546875, prec 0.0310144, recall 0.76998
2017-12-10T15:24:56.914028: step 540, loss 2.13211, acc 0.53125, prec 0.0309416, recall 0.76998
2017-12-10T15:24:57.109048: step 541, loss 1.92054, acc 0.546875, prec 0.0309472, recall 0.770428
2017-12-10T15:24:57.306890: step 542, loss 2.07115, acc 0.5625, prec 0.0310307, recall 0.771318
2017-12-10T15:24:57.502256: step 543, loss 2.75517, acc 0.453125, prec 0.0309463, recall 0.771318
2017-12-10T15:24:57.694165: step 544, loss 3.00063, acc 0.53125, prec 0.0308743, recall 0.771318
2017-12-10T15:24:57.887335: step 545, loss 3.18839, acc 0.46875, prec 0.030943, recall 0.772201
2017-12-10T15:24:58.079470: step 546, loss 2.27351, acc 0.484375, prec 0.0310885, recall 0.773512
2017-12-10T15:24:58.270346: step 547, loss 2.43988, acc 0.59375, prec 0.0311754, recall 0.774379
2017-12-10T15:24:58.466394: step 548, loss 3.04818, acc 0.359375, prec 0.0310773, recall 0.774379
2017-12-10T15:24:58.660054: step 549, loss 2.63613, acc 0.46875, prec 0.0309965, recall 0.774379
2017-12-10T15:24:58.858927: step 550, loss 2.21393, acc 0.5, prec 0.0310687, recall 0.775238
2017-12-10T15:24:59.053042: step 551, loss 2.59662, acc 0.59375, prec 0.0311548, recall 0.776091
2017-12-10T15:24:59.253099: step 552, loss 1.13963, acc 0.65625, prec 0.03125, recall 0.776938
2017-12-10T15:24:59.447219: step 553, loss 2.95713, acc 0.625, prec 0.0311954, recall 0.775472
2017-12-10T15:24:59.645654: step 554, loss 1.21988, acc 0.65625, prec 0.0311434, recall 0.775472
2017-12-10T15:24:59.836911: step 555, loss 1.12513, acc 0.78125, prec 0.0311104, recall 0.775472
2017-12-10T15:25:00.029511: step 556, loss 1.19627, acc 0.625, prec 0.031054, recall 0.775472
2017-12-10T15:25:00.222546: step 557, loss 1.65008, acc 0.6875, prec 0.0310803, recall 0.775895
2017-12-10T15:25:00.412877: step 558, loss 0.831257, acc 0.78125, prec 0.0311205, recall 0.776316
2017-12-10T15:25:00.608071: step 559, loss 0.893839, acc 0.8125, prec 0.0313112, recall 0.77757
2017-12-10T15:25:00.804936: step 560, loss 0.652443, acc 0.8125, prec 0.0313557, recall 0.777985
2017-12-10T15:25:00.995230: step 561, loss 3.0142, acc 0.765625, prec 0.0313228, recall 0.776536
2017-12-10T15:25:01.202778: step 562, loss 0.51731, acc 0.84375, prec 0.0312993, recall 0.776536
2017-12-10T15:25:01.400194: step 563, loss 0.499972, acc 0.875, prec 0.0312805, recall 0.776536
2017-12-10T15:25:01.599135: step 564, loss 0.429593, acc 0.9375, prec 0.0312711, recall 0.776536
2017-12-10T15:25:01.793211: step 565, loss 0.392741, acc 0.859375, prec 0.0313226, recall 0.776952
2017-12-10T15:25:01.986332: step 566, loss 0.200873, acc 0.9375, prec 0.0313858, recall 0.777366
2017-12-10T15:25:02.178998: step 567, loss 0.908065, acc 0.859375, prec 0.0315096, recall 0.778189
2017-12-10T15:25:02.375742: step 568, loss 11.9252, acc 0.875, prec 0.0315679, recall 0.775735
2017-12-10T15:25:02.570373: step 569, loss 0.65134, acc 0.84375, prec 0.0316167, recall 0.776147
2017-12-10T15:25:02.763293: step 570, loss 0.487996, acc 0.8125, prec 0.0315884, recall 0.776147
2017-12-10T15:25:02.952835: step 571, loss 0.479088, acc 0.859375, prec 0.0316394, recall 0.776557
2017-12-10T15:25:03.152925: step 572, loss 0.342891, acc 0.90625, prec 0.0316975, recall 0.776965
2017-12-10T15:25:03.347210: step 573, loss 2.09824, acc 0.828125, prec 0.0316739, recall 0.775547
2017-12-10T15:25:03.545514: step 574, loss 0.909269, acc 0.703125, prec 0.0316291, recall 0.775547
2017-12-10T15:25:03.740216: step 575, loss 0.884698, acc 0.78125, prec 0.0316682, recall 0.775956
2017-12-10T15:25:03.931483: step 576, loss 0.643205, acc 0.8125, prec 0.0316399, recall 0.775956
2017-12-10T15:25:04.124700: step 577, loss 0.750771, acc 0.828125, prec 0.0318296, recall 0.777174
2017-12-10T15:25:04.316965: step 578, loss 0.672235, acc 0.765625, prec 0.0317943, recall 0.777174
2017-12-10T15:25:04.512859: step 579, loss 0.839955, acc 0.703125, prec 0.0318212, recall 0.777577
2017-12-10T15:25:04.704764: step 580, loss 0.838138, acc 0.765625, prec 0.0317859, recall 0.777577
2017-12-10T15:25:04.900430: step 581, loss 0.669602, acc 0.765625, prec 0.0318937, recall 0.778378
2017-12-10T15:25:05.094134: step 582, loss 0.944035, acc 0.75, prec 0.0318561, recall 0.778378
2017-12-10T15:25:05.291539: step 583, loss 0.833139, acc 0.734375, prec 0.0318875, recall 0.778777
2017-12-10T15:25:05.489075: step 584, loss 1.10989, acc 0.6875, prec 0.0318406, recall 0.778777
2017-12-10T15:25:05.686285: step 585, loss 0.689229, acc 0.796875, prec 0.0318102, recall 0.778777
2017-12-10T15:25:05.883405: step 586, loss 8.5107, acc 0.890625, prec 0.0318672, recall 0.777778
2017-12-10T15:25:06.082884: step 587, loss 3.58018, acc 0.78125, prec 0.0319079, recall 0.776786
2017-12-10T15:25:06.278679: step 588, loss 0.305538, acc 0.890625, prec 0.0318915, recall 0.776786
2017-12-10T15:25:06.468359: step 589, loss 1.20509, acc 0.703125, prec 0.031918, recall 0.777184
2017-12-10T15:25:06.661805: step 590, loss 0.987346, acc 0.734375, prec 0.0319491, recall 0.77758
2017-12-10T15:25:06.854367: step 591, loss 1.0297, acc 0.703125, prec 0.0319755, recall 0.777975
2017-12-10T15:25:07.046910: step 592, loss 0.611401, acc 0.8125, prec 0.0321593, recall 0.779152
2017-12-10T15:25:07.237744: step 593, loss 0.820952, acc 0.796875, prec 0.0321993, recall 0.779541
2017-12-10T15:25:07.434672: step 594, loss 1.48082, acc 0.71875, prec 0.032298, recall 0.780316
2017-12-10T15:25:07.632798: step 595, loss 1.93573, acc 0.640625, prec 0.0323143, recall 0.780702
2017-12-10T15:25:07.830598: step 596, loss 1.18354, acc 0.671875, prec 0.0322651, recall 0.780702
2017-12-10T15:25:08.028617: step 597, loss 1.04659, acc 0.671875, prec 0.032216, recall 0.780702
2017-12-10T15:25:08.221796: step 598, loss 0.923367, acc 0.6875, prec 0.0322394, recall 0.781086
2017-12-10T15:25:08.418497: step 599, loss 0.934257, acc 0.703125, prec 0.032265, recall 0.781469
2017-12-10T15:25:08.615653: step 600, loss 0.744562, acc 0.796875, prec 0.0323046, recall 0.78185
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_2/1512937383/checkpoints/model-600

2017-12-10T15:25:09.836147: step 601, loss 0.81468, acc 0.796875, prec 0.0322743, recall 0.78185
2017-12-10T15:25:10.028821: step 602, loss 0.804221, acc 0.78125, prec 0.0323115, recall 0.78223
2017-12-10T15:25:10.220644: step 603, loss 0.297184, acc 0.921875, prec 0.0322998, recall 0.78223
2017-12-10T15:25:10.416556: step 604, loss 0.420696, acc 0.828125, prec 0.0324829, recall 0.783362
2017-12-10T15:25:10.613655: step 605, loss 0.679463, acc 0.828125, prec 0.0325267, recall 0.783737
2017-12-10T15:25:10.810878: step 606, loss 0.436903, acc 0.875, prec 0.0325081, recall 0.783737
2017-12-10T15:25:11.004046: step 607, loss 5.78611, acc 0.859375, prec 0.0324918, recall 0.781034
2017-12-10T15:25:11.206928: step 608, loss 0.568277, acc 0.890625, prec 0.0324754, recall 0.781034
2017-12-10T15:25:11.404592: step 609, loss 0.569827, acc 0.828125, prec 0.0324499, recall 0.781034
2017-12-10T15:25:11.601677: step 610, loss 0.609596, acc 0.8125, prec 0.0324912, recall 0.781411
2017-12-10T15:25:11.797582: step 611, loss 0.665255, acc 0.8125, prec 0.0324634, recall 0.781411
2017-12-10T15:25:11.991810: step 612, loss 1.02863, acc 0.828125, prec 0.0325761, recall 0.782161
2017-12-10T15:25:12.188239: step 613, loss 2.49176, acc 0.765625, prec 0.0326816, recall 0.78157
2017-12-10T15:25:12.384576: step 614, loss 0.40382, acc 0.828125, prec 0.0327249, recall 0.781942
2017-12-10T15:25:12.581202: step 615, loss 0.770539, acc 0.765625, prec 0.03269, recall 0.781942
2017-12-10T15:25:12.773034: step 616, loss 0.742129, acc 0.796875, prec 0.0327974, recall 0.782683
2017-12-10T15:25:12.964283: step 617, loss 0.918432, acc 0.84375, prec 0.0329116, recall 0.783418
2017-12-10T15:25:13.155072: step 618, loss 1.37852, acc 0.765625, prec 0.0329452, recall 0.783784
2017-12-10T15:25:13.348244: step 619, loss 1.52636, acc 0.8125, prec 0.0329857, recall 0.784148
2017-12-10T15:25:13.543894: step 620, loss 1.21928, acc 0.734375, prec 0.033083, recall 0.784874
2017-12-10T15:25:13.740436: step 621, loss 1.06591, acc 0.75, prec 0.0330456, recall 0.784874
2017-12-10T15:25:13.934361: step 622, loss 2.72087, acc 0.59375, prec 0.0331897, recall 0.785953
2017-12-10T15:25:14.125547: step 623, loss 0.81705, acc 0.71875, prec 0.033284, recall 0.786667
2017-12-10T15:25:14.322555: step 624, loss 0.81181, acc 0.671875, prec 0.0332348, recall 0.786667
2017-12-10T15:25:14.511055: step 625, loss 1.23726, acc 0.6875, prec 0.033188, recall 0.786667
2017-12-10T15:25:14.701827: step 626, loss 2.15571, acc 0.734375, prec 0.0332186, recall 0.785714
2017-12-10T15:25:14.899218: step 627, loss 3.05297, acc 0.703125, prec 0.0331767, recall 0.784411
2017-12-10T15:25:15.098443: step 628, loss 2.53319, acc 0.671875, prec 0.0331979, recall 0.783471
2017-12-10T15:25:15.293534: step 629, loss 1.44575, acc 0.625, prec 0.0331422, recall 0.783471
2017-12-10T15:25:15.486275: step 630, loss 2.67086, acc 0.515625, prec 0.0330705, recall 0.783471
2017-12-10T15:25:15.675066: step 631, loss 1.9498, acc 0.625, prec 0.0330826, recall 0.783828
2017-12-10T15:25:15.869279: step 632, loss 1.5931, acc 0.65625, prec 0.0332337, recall 0.784893
2017-12-10T15:25:16.063486: step 633, loss 1.84453, acc 0.53125, prec 0.0331645, recall 0.784893
2017-12-10T15:25:16.255230: step 634, loss 1.73359, acc 0.625, prec 0.0331763, recall 0.785246
2017-12-10T15:25:16.453011: step 635, loss 1.51295, acc 0.640625, prec 0.0331236, recall 0.785246
2017-12-10T15:25:16.652505: step 636, loss 1.36112, acc 0.65625, prec 0.03314, recall 0.785597
2017-12-10T15:25:16.850393: step 637, loss 1.32557, acc 0.765625, prec 0.0331724, recall 0.785948
2017-12-10T15:25:17.045495: step 638, loss 0.874549, acc 0.75, prec 0.0331358, recall 0.785948
2017-12-10T15:25:17.240417: step 639, loss 1.34093, acc 0.703125, prec 0.0331591, recall 0.786297
2017-12-10T15:25:17.437382: step 640, loss 1.54483, acc 0.734375, prec 0.0331868, recall 0.786645
2017-12-10T15:25:17.634855: step 641, loss 1.09766, acc 0.734375, prec 0.033148, recall 0.786645
2017-12-10T15:25:17.828139: step 642, loss 0.573789, acc 0.78125, prec 0.0331825, recall 0.786992
2017-12-10T15:25:18.019680: step 643, loss 0.752887, acc 0.796875, prec 0.033153, recall 0.786992
2017-12-10T15:25:18.214174: step 644, loss 0.532964, acc 0.8125, prec 0.0331257, recall 0.786992
2017-12-10T15:25:18.406400: step 645, loss 0.585552, acc 0.875, prec 0.0331076, recall 0.786992
2017-12-10T15:25:18.599012: step 646, loss 0.227924, acc 0.921875, prec 0.0331624, recall 0.787338
2017-12-10T15:25:18.793103: step 647, loss 0.431895, acc 0.890625, prec 0.0332126, recall 0.787682
2017-12-10T15:25:18.982069: step 648, loss 1.27757, acc 0.90625, prec 0.0333311, recall 0.788368
2017-12-10T15:25:19.174220: step 649, loss 4.76937, acc 0.875, prec 0.0333151, recall 0.787097
2017-12-10T15:25:19.377652: step 650, loss 0.48192, acc 0.859375, prec 0.0333606, recall 0.78744
2017-12-10T15:25:19.571292: step 651, loss 1.61797, acc 0.90625, prec 0.0334152, recall 0.786517
2017-12-10T15:25:19.767092: step 652, loss 0.454555, acc 0.890625, prec 0.033531, recall 0.7872
2017-12-10T15:25:19.960404: step 653, loss 0.327876, acc 0.875, prec 0.0335127, recall 0.7872
2017-12-10T15:25:20.153214: step 654, loss 6.93734, acc 0.875, prec 0.0335625, recall 0.786284
2017-12-10T15:25:20.347692: step 655, loss 0.393002, acc 0.84375, prec 0.0335397, recall 0.786284
2017-12-10T15:25:20.543464: step 656, loss 0.716022, acc 0.765625, prec 0.0335055, recall 0.786284
2017-12-10T15:25:20.736802: step 657, loss 0.941423, acc 0.78125, prec 0.0334737, recall 0.786284
2017-12-10T15:25:20.932696: step 658, loss 0.67942, acc 0.765625, prec 0.0334396, recall 0.786284
2017-12-10T15:25:21.124680: step 659, loss 1.10635, acc 0.640625, prec 0.0333875, recall 0.786284
2017-12-10T15:25:21.321399: step 660, loss 1.38603, acc 0.65625, prec 0.0333378, recall 0.786284
2017-12-10T15:25:21.514768: step 661, loss 0.973216, acc 0.734375, prec 0.0332996, recall 0.786284
2017-12-10T15:25:21.708333: step 662, loss 1.8686, acc 0.71875, prec 0.0333895, recall 0.786963
2017-12-10T15:25:21.901254: step 663, loss 0.9347, acc 0.6875, prec 0.0333446, recall 0.786963
2017-12-10T15:25:22.093212: step 664, loss 0.861621, acc 0.75, prec 0.0333737, recall 0.787302
2017-12-10T15:25:22.286851: step 665, loss 0.606429, acc 0.828125, prec 0.033414, recall 0.787639
2017-12-10T15:25:22.477468: step 666, loss 1.2888, acc 0.8125, prec 0.0335169, recall 0.78831
2017-12-10T15:25:22.671750: step 667, loss 0.623668, acc 0.84375, prec 0.0334944, recall 0.78831
2017-12-10T15:25:22.865142: step 668, loss 0.765875, acc 0.796875, prec 0.03353, recall 0.788644
2017-12-10T15:25:23.057981: step 669, loss 2.52086, acc 0.703125, prec 0.0335543, recall 0.787736
2017-12-10T15:25:23.253200: step 670, loss 7.51023, acc 0.671875, prec 0.0335095, recall 0.786499
2017-12-10T15:25:23.450514: step 671, loss 0.912159, acc 0.703125, prec 0.0335315, recall 0.786834
2017-12-10T15:25:23.644677: step 672, loss 0.758948, acc 0.78125, prec 0.0335002, recall 0.786834
2017-12-10T15:25:23.842625: step 673, loss 1.44804, acc 0.6875, prec 0.0335843, recall 0.7875
2017-12-10T15:25:24.039624: step 674, loss 1.06578, acc 0.75, prec 0.0337415, recall 0.788491
2017-12-10T15:25:24.229385: step 675, loss 0.935096, acc 0.71875, prec 0.0337011, recall 0.788491
2017-12-10T15:25:24.419424: step 676, loss 2.54248, acc 0.609375, prec 0.0337094, recall 0.78882
2017-12-10T15:25:24.614532: step 677, loss 1.74317, acc 0.625, prec 0.0337198, recall 0.789147
2017-12-10T15:25:24.811827: step 678, loss 1.20262, acc 0.71875, prec 0.0336796, recall 0.789147
2017-12-10T15:25:25.005637: step 679, loss 0.894839, acc 0.75, prec 0.033644, recall 0.789147
2017-12-10T15:25:25.198290: step 680, loss 1.09716, acc 0.59375, prec 0.0335863, recall 0.789147
2017-12-10T15:25:25.396682: step 681, loss 1.67782, acc 0.546875, prec 0.0335221, recall 0.789147
2017-12-10T15:25:25.590851: step 682, loss 1.18065, acc 0.71875, prec 0.0334824, recall 0.789147
2017-12-10T15:25:25.784531: step 683, loss 11.7739, acc 0.75, prec 0.0334494, recall 0.787926
2017-12-10T15:25:25.984205: step 684, loss 1.16636, acc 0.640625, prec 0.0333989, recall 0.787926
2017-12-10T15:25:26.174734: step 685, loss 1.15693, acc 0.65625, prec 0.0334141, recall 0.788253
2017-12-10T15:25:26.367365: step 686, loss 1.22458, acc 0.65625, prec 0.033366, recall 0.788253
2017-12-10T15:25:26.557954: step 687, loss 1.09061, acc 0.65625, prec 0.0333181, recall 0.788253
2017-12-10T15:25:26.754013: step 688, loss 0.486051, acc 0.828125, prec 0.0332942, recall 0.788253
2017-12-10T15:25:26.946672: step 689, loss 0.910147, acc 0.734375, prec 0.0333833, recall 0.788906
2017-12-10T15:25:27.147305: step 690, loss 0.602969, acc 0.828125, prec 0.0333594, recall 0.788906
2017-12-10T15:25:27.337409: step 691, loss 1.33518, acc 0.765625, prec 0.0333897, recall 0.789231
2017-12-10T15:25:27.528598: step 692, loss 1.95859, acc 0.796875, prec 0.0334894, recall 0.788668
2017-12-10T15:25:27.722674: step 693, loss 0.548746, acc 0.828125, prec 0.0335283, recall 0.788991
2017-12-10T15:25:27.919646: step 694, loss 0.566342, acc 0.765625, prec 0.0334956, recall 0.788991
2017-12-10T15:25:28.109855: step 695, loss 0.57419, acc 0.78125, prec 0.0334652, recall 0.788991
2017-12-10T15:25:28.304946: step 696, loss 4.9932, acc 0.84375, prec 0.0334457, recall 0.787786
2017-12-10T15:25:28.505977: step 697, loss 0.543742, acc 0.875, prec 0.0335536, recall 0.788432
2017-12-10T15:25:28.700735: step 698, loss 0.628811, acc 0.859375, prec 0.033534, recall 0.788432
2017-12-10T15:25:28.901811: step 699, loss 0.452822, acc 0.828125, prec 0.0335102, recall 0.788432
2017-12-10T15:25:29.100531: step 700, loss 1.14727, acc 0.796875, prec 0.0335445, recall 0.788754
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_2/1512937383/checkpoints/model-700

2017-12-10T15:25:30.174463: step 701, loss 0.55936, acc 0.8125, prec 0.0335809, recall 0.789074
2017-12-10T15:25:30.368262: step 702, loss 0.474562, acc 0.8125, prec 0.0336172, recall 0.789394
2017-12-10T15:25:30.565186: step 703, loss 0.522524, acc 0.828125, prec 0.0335934, recall 0.789394
2017-12-10T15:25:30.763611: step 704, loss 0.702397, acc 0.828125, prec 0.0336319, recall 0.789713
2017-12-10T15:25:30.955635: step 705, loss 0.555064, acc 0.8125, prec 0.0336059, recall 0.789713
2017-12-10T15:25:31.152989: step 706, loss 0.436144, acc 0.90625, prec 0.0337794, recall 0.790663
2017-12-10T15:25:31.353419: step 707, loss 0.523824, acc 0.828125, prec 0.0338177, recall 0.790977
2017-12-10T15:25:31.564853: step 708, loss 0.327657, acc 0.875, prec 0.0338624, recall 0.791291
2017-12-10T15:25:31.758956: step 709, loss 7.76599, acc 0.875, prec 0.0339092, recall 0.790419
2017-12-10T15:25:31.957706: step 710, loss 0.43122, acc 0.875, prec 0.0340158, recall 0.791045
2017-12-10T15:25:32.155927: step 711, loss 1.82622, acc 0.875, prec 0.0341244, recall 0.79049
2017-12-10T15:25:32.355987: step 712, loss 0.957823, acc 0.8125, prec 0.0341601, recall 0.790801
2017-12-10T15:25:32.550607: step 713, loss 10.2595, acc 0.875, prec 0.0342685, recall 0.790251
2017-12-10T15:25:32.743308: step 714, loss 1.56619, acc 0.78125, prec 0.0343614, recall 0.790869
2017-12-10T15:25:32.935052: step 715, loss 0.919062, acc 0.734375, prec 0.0343858, recall 0.791176
2017-12-10T15:25:33.131857: step 716, loss 1.56825, acc 0.609375, prec 0.0343925, recall 0.791483
2017-12-10T15:25:33.328359: step 717, loss 0.961313, acc 0.65625, prec 0.0343443, recall 0.791483
2017-12-10T15:25:33.521926: step 718, loss 1.11799, acc 0.65625, prec 0.0342963, recall 0.791483
2017-12-10T15:25:33.716664: step 719, loss 1.36986, acc 0.625, prec 0.0343053, recall 0.791789
2017-12-10T15:25:33.912853: step 720, loss 1.24461, acc 0.609375, prec 0.0343122, recall 0.792094
2017-12-10T15:25:34.107258: step 721, loss 1.15409, acc 0.6875, prec 0.0342687, recall 0.792094
2017-12-10T15:25:34.303735: step 722, loss 4.77139, acc 0.53125, prec 0.0342669, recall 0.791241
2017-12-10T15:25:34.499848: step 723, loss 0.919377, acc 0.671875, prec 0.0342825, recall 0.791545
2017-12-10T15:25:34.694642: step 724, loss 2.19514, acc 0.625, prec 0.0343523, recall 0.792151
2017-12-10T15:25:34.892783: step 725, loss 1.0463, acc 0.6875, prec 0.0343699, recall 0.792453
2017-12-10T15:25:35.084001: step 726, loss 2.40652, acc 0.625, prec 0.0343202, recall 0.791304
2017-12-10T15:25:35.276623: step 727, loss 1.39495, acc 0.671875, prec 0.034275, recall 0.791304
2017-12-10T15:25:35.470451: step 728, loss 1.79299, acc 0.4375, prec 0.0341977, recall 0.791304
2017-12-10T15:25:35.666351: step 729, loss 1.23904, acc 0.71875, prec 0.0342196, recall 0.791606
2017-12-10T15:25:35.863683: step 730, loss 1.38959, acc 0.578125, prec 0.0342825, recall 0.792208
2017-12-10T15:25:36.062043: step 731, loss 1.09301, acc 0.671875, prec 0.034358, recall 0.792806
2017-12-10T15:25:36.254738: step 732, loss 1.2252, acc 0.625, prec 0.0343067, recall 0.792806
2017-12-10T15:25:36.449291: step 733, loss 2.71239, acc 0.640625, prec 0.0343198, recall 0.791966
2017-12-10T15:25:36.646542: step 734, loss 1.06194, acc 0.65625, prec 0.0343329, recall 0.792264
2017-12-10T15:25:36.841046: step 735, loss 1.19579, acc 0.671875, prec 0.0343481, recall 0.792561
2017-12-10T15:25:37.036273: step 736, loss 0.74703, acc 0.796875, prec 0.0343802, recall 0.792857
2017-12-10T15:25:37.227790: step 737, loss 0.947345, acc 0.796875, prec 0.0344123, recall 0.793153
2017-12-10T15:25:37.426868: step 738, loss 0.864848, acc 0.84375, prec 0.0345105, recall 0.793741
2017-12-10T15:25:37.619589: step 739, loss 0.548855, acc 0.796875, prec 0.0344828, recall 0.793741
2017-12-10T15:25:37.813258: step 740, loss 2.79508, acc 0.734375, prec 0.0345083, recall 0.792908
2017-12-10T15:25:38.012050: step 741, loss 0.754022, acc 0.765625, prec 0.0344764, recall 0.792908
2017-12-10T15:25:38.205562: step 742, loss 0.593115, acc 0.84375, prec 0.0344551, recall 0.792908
2017-12-10T15:25:38.401602: step 743, loss 0.783725, acc 0.703125, prec 0.0344148, recall 0.792908
2017-12-10T15:25:38.598793: step 744, loss 0.78128, acc 0.75, prec 0.034381, recall 0.792908
2017-12-10T15:25:38.793742: step 745, loss 1.08078, acc 0.75, prec 0.0344658, recall 0.793494
2017-12-10T15:25:38.991791: step 746, loss 0.745426, acc 0.8125, prec 0.034559, recall 0.794076
2017-12-10T15:25:39.187207: step 747, loss 1.8099, acc 0.78125, prec 0.0345906, recall 0.793249
2017-12-10T15:25:39.378335: step 748, loss 2.1315, acc 0.828125, prec 0.0345694, recall 0.792135
2017-12-10T15:25:39.575904: step 749, loss 0.78498, acc 0.765625, prec 0.0345377, recall 0.792135
2017-12-10T15:25:39.767129: step 750, loss 0.466778, acc 0.8125, prec 0.0345714, recall 0.792426
2017-12-10T15:25:39.961184: step 751, loss 0.814153, acc 0.765625, prec 0.0346577, recall 0.793007
2017-12-10T15:25:40.157666: step 752, loss 0.597206, acc 0.765625, prec 0.0346849, recall 0.793296
2017-12-10T15:25:40.349766: step 753, loss 0.72148, acc 0.78125, prec 0.0346553, recall 0.793296
2017-12-10T15:25:40.543269: step 754, loss 8.95323, acc 0.734375, prec 0.0346215, recall 0.79219
2017-12-10T15:25:40.738801: step 755, loss 1.06592, acc 0.75, prec 0.0345877, recall 0.79219
2017-12-10T15:25:40.933867: step 756, loss 0.52128, acc 0.84375, prec 0.0345667, recall 0.79219
2017-12-10T15:25:41.130788: step 757, loss 0.70828, acc 0.765625, prec 0.0345939, recall 0.792479
2017-12-10T15:25:41.324824: step 758, loss 0.563954, acc 0.78125, prec 0.0346231, recall 0.792768
2017-12-10T15:25:41.515477: step 759, loss 2.09159, acc 0.71875, prec 0.034646, recall 0.791956
2017-12-10T15:25:41.710931: step 760, loss 1.9319, acc 0.703125, prec 0.0347231, recall 0.792531
2017-12-10T15:25:41.905055: step 761, loss 0.830908, acc 0.71875, prec 0.0346852, recall 0.792531
2017-12-10T15:25:42.095980: step 762, loss 0.6415, acc 0.796875, prec 0.0347747, recall 0.793103
2017-12-10T15:25:42.289585: step 763, loss 1.121, acc 0.671875, prec 0.0347306, recall 0.793103
2017-12-10T15:25:42.484124: step 764, loss 1.14402, acc 0.703125, prec 0.0346908, recall 0.793103
2017-12-10T15:25:42.680614: step 765, loss 0.596702, acc 0.78125, prec 0.0346615, recall 0.793103
2017-12-10T15:25:42.881138: step 766, loss 1.2312, acc 0.71875, prec 0.0346821, recall 0.793388
2017-12-10T15:25:43.079281: step 767, loss 1.09978, acc 0.625, prec 0.0346901, recall 0.793673
2017-12-10T15:25:43.273704: step 768, loss 0.723745, acc 0.75, prec 0.0346567, recall 0.793673
2017-12-10T15:25:43.464622: step 769, loss 0.289312, acc 0.921875, prec 0.0346463, recall 0.793673
2017-12-10T15:25:43.658085: step 770, loss 0.839501, acc 0.71875, prec 0.0346089, recall 0.793673
2017-12-10T15:25:43.850868: step 771, loss 1.93735, acc 0.828125, prec 0.034646, recall 0.792867
2017-12-10T15:25:44.047008: step 772, loss 0.573106, acc 0.75, prec 0.0347285, recall 0.793434
2017-12-10T15:25:44.245774: step 773, loss 0.623852, acc 0.75, prec 0.034753, recall 0.793716
2017-12-10T15:25:44.440416: step 774, loss 3.87102, acc 0.765625, prec 0.0347239, recall 0.792633
2017-12-10T15:25:44.637356: step 775, loss 0.709268, acc 0.796875, prec 0.0346969, recall 0.792633
2017-12-10T15:25:44.830197: step 776, loss 3.11389, acc 0.796875, prec 0.0347873, recall 0.79212
2017-12-10T15:25:45.029615: step 777, loss 0.379553, acc 0.8125, prec 0.0347624, recall 0.79212
2017-12-10T15:25:45.221171: step 778, loss 0.555864, acc 0.796875, prec 0.0347355, recall 0.79212
2017-12-10T15:25:45.417099: step 779, loss 0.355448, acc 0.859375, prec 0.0347168, recall 0.79212
2017-12-10T15:25:45.610400: step 780, loss 0.773215, acc 0.671875, prec 0.0346735, recall 0.79212
2017-12-10T15:25:45.806970: step 781, loss 0.764852, acc 0.75, prec 0.0346979, recall 0.792402
2017-12-10T15:25:45.997439: step 782, loss 1.18669, acc 0.6875, prec 0.034714, recall 0.792683
2017-12-10T15:25:46.192799: step 783, loss 0.604096, acc 0.765625, prec 0.0346831, recall 0.792683
2017-12-10T15:25:46.383604: step 784, loss 0.573862, acc 0.8125, prec 0.0347156, recall 0.792963
2017-12-10T15:25:46.576681: step 785, loss 1.3466, acc 0.8125, prec 0.0347481, recall 0.793243
2017-12-10T15:25:46.774169: step 786, loss 0.428013, acc 0.8125, prec 0.0347235, recall 0.793243
2017-12-10T15:25:46.967043: step 787, loss 0.976787, acc 0.8125, prec 0.0347559, recall 0.793522
2017-12-10T15:25:47.160264: step 788, loss 0.68154, acc 0.796875, prec 0.0347292, recall 0.793522
2017-12-10T15:25:47.352441: step 789, loss 0.666866, acc 0.765625, prec 0.0346985, recall 0.793522
2017-12-10T15:25:47.546934: step 790, loss 0.377473, acc 0.875, prec 0.0346821, recall 0.793522
2017-12-10T15:25:47.743404: step 791, loss 1.74007, acc 0.78125, prec 0.0348262, recall 0.793289
2017-12-10T15:25:47.942974: step 792, loss 0.64713, acc 0.8125, prec 0.0348584, recall 0.793566
2017-12-10T15:25:48.136802: step 793, loss 1.78774, acc 0.859375, prec 0.0348988, recall 0.792781
2017-12-10T15:25:48.336456: step 794, loss 2.88737, acc 0.796875, prec 0.0349309, recall 0.792
2017-12-10T15:25:48.533036: step 795, loss 0.638738, acc 0.8125, prec 0.0349063, recall 0.792
2017-12-10T15:25:48.732236: step 796, loss 0.669161, acc 0.859375, prec 0.0349445, recall 0.792277
2017-12-10T15:25:48.928268: step 797, loss 0.787126, acc 0.796875, prec 0.0349178, recall 0.792277
2017-12-10T15:25:49.127968: step 798, loss 2.15399, acc 0.78125, prec 0.0350044, recall 0.791777
2017-12-10T15:25:49.322467: step 799, loss 1.12378, acc 0.625, prec 0.0349552, recall 0.791777
2017-12-10T15:25:49.517590: step 800, loss 1.17723, acc 0.6875, prec 0.0349143, recall 0.791777
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_2/1512937383/checkpoints/model-800

2017-12-10T15:25:50.703515: step 801, loss 0.88701, acc 0.671875, prec 0.0350406, recall 0.792602
2017-12-10T15:25:50.898790: step 802, loss 0.766447, acc 0.765625, prec 0.0350099, recall 0.792602
2017-12-10T15:25:51.091394: step 803, loss 0.978981, acc 0.671875, prec 0.0349671, recall 0.792602
2017-12-10T15:25:51.286504: step 804, loss 1.26722, acc 0.59375, prec 0.0349142, recall 0.792602
2017-12-10T15:25:51.480537: step 805, loss 1.55695, acc 0.640625, prec 0.0349236, recall 0.792876
2017-12-10T15:25:51.674489: step 806, loss 1.04792, acc 0.65625, prec 0.034879, recall 0.792876
2017-12-10T15:25:51.871665: step 807, loss 1.36084, acc 0.640625, prec 0.0348884, recall 0.793149
2017-12-10T15:25:52.062815: step 808, loss 0.9941, acc 0.671875, prec 0.034846, recall 0.793149
2017-12-10T15:25:52.266861: step 809, loss 0.716538, acc 0.765625, prec 0.0348158, recall 0.793149
2017-12-10T15:25:52.460006: step 810, loss 0.371795, acc 0.828125, prec 0.0347937, recall 0.793149
2017-12-10T15:25:52.654340: step 811, loss 1.71494, acc 0.78125, prec 0.0347675, recall 0.792105
2017-12-10T15:25:52.852737: step 812, loss 1.19465, acc 0.71875, prec 0.0348428, recall 0.792651
2017-12-10T15:25:53.046071: step 813, loss 0.523103, acc 0.84375, prec 0.0348784, recall 0.792923
2017-12-10T15:25:53.239348: step 814, loss 0.239573, acc 0.921875, prec 0.0348683, recall 0.792923
2017-12-10T15:25:53.441854: step 815, loss 0.398133, acc 0.890625, prec 0.0348542, recall 0.792923
2017-12-10T15:25:53.634111: step 816, loss 0.24449, acc 0.875, prec 0.0348382, recall 0.792923
2017-12-10T15:25:53.830616: step 817, loss 0.767559, acc 0.8125, prec 0.0348697, recall 0.793194
2017-12-10T15:25:54.023362: step 818, loss 1.06755, acc 0.875, prec 0.0349646, recall 0.793734
2017-12-10T15:25:54.224433: step 819, loss 0.481203, acc 0.921875, prec 0.0350101, recall 0.794003
2017-12-10T15:25:54.417170: step 820, loss 0.336007, acc 0.875, prec 0.034994, recall 0.794003
2017-12-10T15:25:54.610209: step 821, loss 0.399416, acc 0.859375, prec 0.0350313, recall 0.794271
2017-12-10T15:25:54.805399: step 822, loss 0.15746, acc 0.953125, prec 0.0350253, recall 0.794271
2017-12-10T15:25:54.996576: step 823, loss 0.211677, acc 0.9375, prec 0.0350172, recall 0.794271
2017-12-10T15:25:55.191302: step 824, loss 0.23707, acc 0.890625, prec 0.0350032, recall 0.794271
2017-12-10T15:25:55.382600: step 825, loss 4.9344, acc 0.84375, prec 0.0349851, recall 0.793238
2017-12-10T15:25:55.575224: step 826, loss 1.53967, acc 0.859375, prec 0.035133, recall 0.794041
2017-12-10T15:25:55.778594: step 827, loss 2.40565, acc 0.90625, prec 0.0351229, recall 0.793014
2017-12-10T15:25:55.978012: step 828, loss 0.204627, acc 0.890625, prec 0.0351641, recall 0.793282
2017-12-10T15:25:56.169566: step 829, loss 0.763542, acc 0.8125, prec 0.0351951, recall 0.793548
2017-12-10T15:25:56.363306: step 830, loss 0.841426, acc 0.796875, prec 0.035169, recall 0.793548
2017-12-10T15:25:56.556907: step 831, loss 0.79486, acc 0.78125, prec 0.0352511, recall 0.79408
2017-12-10T15:25:56.749711: step 832, loss 0.965458, acc 0.703125, prec 0.0352679, recall 0.794344
2017-12-10T15:25:56.941416: step 833, loss 0.540395, acc 0.796875, prec 0.0352418, recall 0.794344
2017-12-10T15:25:57.134353: step 834, loss 1.18025, acc 0.734375, prec 0.0352077, recall 0.794344
2017-12-10T15:25:57.331243: step 835, loss 0.624363, acc 0.8125, prec 0.0352385, recall 0.794608
2017-12-10T15:25:57.526827: step 836, loss 0.680351, acc 0.75, prec 0.0352065, recall 0.794608
2017-12-10T15:25:57.721476: step 837, loss 0.778549, acc 0.75, prec 0.0352293, recall 0.794872
2017-12-10T15:25:57.915870: step 838, loss 0.429506, acc 0.84375, prec 0.0352641, recall 0.795134
2017-12-10T15:25:58.112346: step 839, loss 0.551755, acc 0.765625, prec 0.035234, recall 0.795134
2017-12-10T15:25:58.307118: step 840, loss 0.787081, acc 0.890625, prec 0.0353295, recall 0.795658
2017-12-10T15:25:58.501035: step 841, loss 0.555581, acc 0.78125, prec 0.0353015, recall 0.795658
2017-12-10T15:25:58.692072: step 842, loss 1.51921, acc 0.8125, prec 0.0352795, recall 0.794643
2017-12-10T15:25:58.888036: step 843, loss 0.713461, acc 0.765625, prec 0.0352495, recall 0.794643
2017-12-10T15:25:59.082948: step 844, loss 3.63582, acc 0.84375, prec 0.0352861, recall 0.793893
2017-12-10T15:25:59.287236: step 845, loss 0.442109, acc 0.859375, prec 0.0353227, recall 0.794155
2017-12-10T15:25:59.483396: step 846, loss 6.21, acc 0.734375, prec 0.0352908, recall 0.793147
2017-12-10T15:25:59.676426: step 847, loss 0.59606, acc 0.859375, prec 0.0353817, recall 0.793671
2017-12-10T15:25:59.868651: step 848, loss 0.864782, acc 0.734375, prec 0.0353478, recall 0.793671
2017-12-10T15:26:00.063487: step 849, loss 0.786457, acc 0.75, prec 0.035316, recall 0.793671
2017-12-10T15:26:00.257405: step 850, loss 0.513173, acc 0.78125, prec 0.0353967, recall 0.794192
2017-12-10T15:26:00.457615: step 851, loss 1.0006, acc 0.6875, prec 0.0353569, recall 0.794192
2017-12-10T15:26:00.651448: step 852, loss 0.449082, acc 0.8125, prec 0.0353873, recall 0.794451
2017-12-10T15:26:00.850916: step 853, loss 1.33491, acc 0.71875, prec 0.035568, recall 0.795483
2017-12-10T15:26:01.048230: step 854, loss 3.16336, acc 0.78125, prec 0.0357043, recall 0.795256
2017-12-10T15:26:01.250854: step 855, loss 0.960713, acc 0.671875, prec 0.0358242, recall 0.79602
2017-12-10T15:26:01.445231: step 856, loss 1.30976, acc 0.609375, prec 0.0358281, recall 0.796273
2017-12-10T15:26:01.644369: step 857, loss 0.98946, acc 0.703125, prec 0.0358977, recall 0.796778
2017-12-10T15:26:01.839095: step 858, loss 3.38019, acc 0.625, prec 0.0359592, recall 0.796296
2017-12-10T15:26:02.034430: step 859, loss 0.671022, acc 0.71875, prec 0.0359231, recall 0.796296
2017-12-10T15:26:02.230204: step 860, loss 1.17284, acc 0.609375, prec 0.0359804, recall 0.796798
2017-12-10T15:26:02.426535: step 861, loss 1.03099, acc 0.59375, prec 0.0359285, recall 0.796798
2017-12-10T15:26:02.620909: step 862, loss 1.26718, acc 0.578125, prec 0.0359816, recall 0.797297
2017-12-10T15:26:02.821582: step 863, loss 0.793783, acc 0.765625, prec 0.0360585, recall 0.797794
2017-12-10T15:26:03.016581: step 864, loss 1.71093, acc 0.6875, prec 0.0361253, recall 0.798289
2017-12-10T15:26:03.217091: step 865, loss 1.12333, acc 0.625, prec 0.0360773, recall 0.798289
2017-12-10T15:26:03.409740: step 866, loss 0.743155, acc 0.75, prec 0.0360455, recall 0.798289
2017-12-10T15:26:03.602746: step 867, loss 0.770771, acc 0.734375, prec 0.0360649, recall 0.798535
2017-12-10T15:26:03.799919: step 868, loss 4.48551, acc 0.8125, prec 0.036043, recall 0.797561
2017-12-10T15:26:03.998300: step 869, loss 0.643744, acc 0.75, prec 0.0360112, recall 0.797561
2017-12-10T15:26:04.190152: step 870, loss 0.850696, acc 0.6875, prec 0.0360246, recall 0.797808
2017-12-10T15:26:04.385636: step 871, loss 0.538231, acc 0.78125, prec 0.0360499, recall 0.798054
2017-12-10T15:26:04.583634: step 872, loss 0.7607, acc 0.75, prec 0.036177, recall 0.798788
2017-12-10T15:26:04.778453: step 873, loss 0.820612, acc 0.6875, prec 0.0361373, recall 0.798788
2017-12-10T15:26:04.970059: step 874, loss 0.803077, acc 0.71875, prec 0.0361017, recall 0.798788
2017-12-10T15:26:05.165101: step 875, loss 0.507899, acc 0.796875, prec 0.0361287, recall 0.799031
2017-12-10T15:26:05.361144: step 876, loss 0.58661, acc 0.8125, prec 0.0361578, recall 0.799275
2017-12-10T15:26:05.551586: step 877, loss 0.415406, acc 0.875, prec 0.0361419, recall 0.799275
2017-12-10T15:26:05.748415: step 878, loss 1.64725, acc 0.8125, prec 0.0361709, recall 0.799517
2017-12-10T15:26:05.943375: step 879, loss 0.370594, acc 0.828125, prec 0.0361492, recall 0.799517
2017-12-10T15:26:06.139046: step 880, loss 0.417988, acc 0.875, prec 0.0361334, recall 0.799517
2017-12-10T15:26:06.337346: step 881, loss 1.9076, acc 0.875, prec 0.0361196, recall 0.798552
2017-12-10T15:26:06.533144: step 882, loss 0.294956, acc 0.859375, prec 0.0361019, recall 0.798552
2017-12-10T15:26:06.724285: step 883, loss 0.689646, acc 0.828125, prec 0.0361328, recall 0.798795
2017-12-10T15:26:06.921962: step 884, loss 0.258598, acc 0.90625, prec 0.0361209, recall 0.798795
2017-12-10T15:26:07.117375: step 885, loss 1.80084, acc 0.828125, prec 0.0361013, recall 0.797834
2017-12-10T15:26:07.317601: step 886, loss 0.499153, acc 0.859375, prec 0.0360836, recall 0.797834
2017-12-10T15:26:07.510117: step 887, loss 0.57202, acc 0.8125, prec 0.0361125, recall 0.798077
2017-12-10T15:26:07.706229: step 888, loss 11.7789, acc 0.859375, prec 0.0361492, recall 0.797362
2017-12-10T15:26:07.900838: step 889, loss 4.8703, acc 0.8125, prec 0.0362323, recall 0.796894
2017-12-10T15:26:08.093811: step 890, loss 0.531838, acc 0.796875, prec 0.0362067, recall 0.796894
2017-12-10T15:26:08.288276: step 891, loss 1.15244, acc 0.671875, prec 0.0361655, recall 0.796894
2017-12-10T15:26:08.482166: step 892, loss 0.813647, acc 0.703125, prec 0.0361805, recall 0.797136
2017-12-10T15:26:08.678594: step 893, loss 0.874471, acc 0.671875, prec 0.0361394, recall 0.797136
2017-12-10T15:26:08.870728: step 894, loss 0.881654, acc 0.671875, prec 0.0362546, recall 0.79786
2017-12-10T15:26:09.062847: step 895, loss 1.89063, acc 0.53125, prec 0.0362479, recall 0.7981
2017-12-10T15:26:09.259158: step 896, loss 1.63078, acc 0.53125, prec 0.0362412, recall 0.798339
2017-12-10T15:26:09.453371: step 897, loss 1.14281, acc 0.65625, prec 0.0362502, recall 0.798578
2017-12-10T15:26:09.648461: step 898, loss 1.31848, acc 0.5625, prec 0.0361957, recall 0.798578
2017-12-10T15:26:09.839345: step 899, loss 0.90556, acc 0.703125, prec 0.0362105, recall 0.798817
2017-12-10T15:26:10.038493: step 900, loss 1.15751, acc 0.6875, prec 0.0362233, recall 0.799054
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_2/1512937383/checkpoints/model-900

2017-12-10T15:26:11.200115: step 901, loss 1.06902, acc 0.65625, prec 0.0361807, recall 0.799054
2017-12-10T15:26:11.394708: step 902, loss 2.69345, acc 0.71875, prec 0.0361478, recall 0.798111
2017-12-10T15:26:11.591527: step 903, loss 1.07772, acc 0.703125, prec 0.0362141, recall 0.798587
2017-12-10T15:26:11.785484: step 904, loss 2.50662, acc 0.703125, prec 0.0361793, recall 0.797647
2017-12-10T15:26:11.982844: step 905, loss 0.738026, acc 0.765625, prec 0.0362017, recall 0.797885
2017-12-10T15:26:12.173725: step 906, loss 1.0339, acc 0.703125, prec 0.0362164, recall 0.798122
2017-12-10T15:26:12.369039: step 907, loss 0.725019, acc 0.765625, prec 0.0361875, recall 0.798122
2017-12-10T15:26:12.560263: step 908, loss 0.818832, acc 0.765625, prec 0.0362099, recall 0.798359
2017-12-10T15:26:12.753782: step 909, loss 0.910887, acc 0.65625, prec 0.0362188, recall 0.798595
2017-12-10T15:26:12.947763: step 910, loss 0.839907, acc 0.78125, prec 0.0361919, recall 0.798595
2017-12-10T15:26:13.140978: step 911, loss 0.839158, acc 0.734375, prec 0.0361593, recall 0.798595
2017-12-10T15:26:13.335802: step 912, loss 0.576486, acc 0.828125, prec 0.0361382, recall 0.798595
2017-12-10T15:26:13.531319: step 913, loss 0.578744, acc 0.90625, prec 0.0361778, recall 0.79883
2017-12-10T15:26:13.727554: step 914, loss 0.422656, acc 0.796875, prec 0.0362039, recall 0.799065
2017-12-10T15:26:13.920345: step 915, loss 0.763696, acc 0.78125, prec 0.0362281, recall 0.7993
2017-12-10T15:26:14.121406: step 916, loss 0.640132, acc 0.921875, prec 0.0363204, recall 0.799767
2017-12-10T15:26:14.321229: step 917, loss 0.276663, acc 0.859375, prec 0.0363031, recall 0.799767
2017-12-10T15:26:14.514725: step 918, loss 0.390096, acc 0.859375, prec 0.0362859, recall 0.799767
2017-12-10T15:26:14.711345: step 919, loss 0.302454, acc 0.890625, prec 0.0362724, recall 0.799767
2017-12-10T15:26:14.908988: step 920, loss 0.347972, acc 0.90625, prec 0.036261, recall 0.799767
2017-12-10T15:26:15.104609: step 921, loss 0.229672, acc 0.953125, prec 0.0363061, recall 0.8
2017-12-10T15:26:15.298529: step 922, loss 0.163727, acc 0.9375, prec 0.0362984, recall 0.8
2017-12-10T15:26:15.491699: step 923, loss 10.4653, acc 0.875, prec 0.0362888, recall 0.797219
2017-12-10T15:26:15.688484: step 924, loss 4.4499, acc 0.890625, prec 0.0362774, recall 0.796296
2017-12-10T15:26:15.884412: step 925, loss 9.63389, acc 0.765625, prec 0.0362525, recall 0.794457
2017-12-10T15:26:16.085518: step 926, loss 7.63672, acc 0.765625, prec 0.036378, recall 0.794253
2017-12-10T15:26:16.283860: step 927, loss 0.861132, acc 0.703125, prec 0.0363923, recall 0.794489
2017-12-10T15:26:16.480899: step 928, loss 1.08368, acc 0.625, prec 0.0363464, recall 0.794489
2017-12-10T15:26:16.675422: step 929, loss 1.73181, acc 0.5, prec 0.036336, recall 0.794725
2017-12-10T15:26:16.867622: step 930, loss 2.06229, acc 0.390625, prec 0.0363627, recall 0.795195
2017-12-10T15:26:17.064946: step 931, loss 3.18508, acc 0.234375, prec 0.0364206, recall 0.795895
2017-12-10T15:26:17.263555: step 932, loss 2.73082, acc 0.296875, prec 0.0363352, recall 0.795895
2017-12-10T15:26:17.459377: step 933, loss 3.4513, acc 0.28125, prec 0.0362484, recall 0.795895
2017-12-10T15:26:17.656507: step 934, loss 2.19893, acc 0.453125, prec 0.0361827, recall 0.795895
2017-12-10T15:26:17.851666: step 935, loss 3.28878, acc 0.328125, prec 0.0361521, recall 0.796128
2017-12-10T15:26:18.049821: step 936, loss 3.15214, acc 0.28125, prec 0.0361657, recall 0.796591
2017-12-10T15:26:18.244903: step 937, loss 2.6697, acc 0.390625, prec 0.0360931, recall 0.796591
2017-12-10T15:26:18.433490: step 938, loss 2.84745, acc 0.375, prec 0.0360189, recall 0.796591
2017-12-10T15:26:18.623168: step 939, loss 2.45819, acc 0.453125, prec 0.0360037, recall 0.796822
2017-12-10T15:26:18.823227: step 940, loss 1.98661, acc 0.53125, prec 0.0359977, recall 0.797052
2017-12-10T15:26:19.015495: step 941, loss 1.54607, acc 0.671875, prec 0.0359591, recall 0.797052
2017-12-10T15:26:19.213520: step 942, loss 1.02374, acc 0.671875, prec 0.0359205, recall 0.797052
2017-12-10T15:26:19.407401: step 943, loss 2.75117, acc 0.640625, prec 0.0359786, recall 0.79661
2017-12-10T15:26:19.599986: step 944, loss 0.96683, acc 0.734375, prec 0.0359965, recall 0.79684
2017-12-10T15:26:19.794244: step 945, loss 0.650204, acc 0.765625, prec 0.035969, recall 0.79684
2017-12-10T15:26:19.988650: step 946, loss 0.781059, acc 0.6875, prec 0.0360305, recall 0.797297
2017-12-10T15:26:20.185054: step 947, loss 0.690558, acc 0.796875, prec 0.0360557, recall 0.797525
2017-12-10T15:26:20.378635: step 948, loss 0.471457, acc 0.828125, prec 0.0360356, recall 0.797525
2017-12-10T15:26:20.571121: step 949, loss 1.71189, acc 0.765625, prec 0.036106, recall 0.79798
2017-12-10T15:26:20.763780: step 950, loss 0.574155, acc 0.859375, prec 0.0361385, recall 0.798206
2017-12-10T15:26:20.963086: step 951, loss 4.29365, acc 0.859375, prec 0.0361727, recall 0.797539
2017-12-10T15:26:21.158542: step 952, loss 0.58315, acc 0.78125, prec 0.0361959, recall 0.797765
2017-12-10T15:26:21.356170: step 953, loss 0.431238, acc 0.828125, prec 0.0362245, recall 0.797991
2017-12-10T15:26:21.550325: step 954, loss 3.42161, acc 0.890625, prec 0.0362135, recall 0.797101
2017-12-10T15:26:21.746033: step 955, loss 0.335697, acc 0.875, prec 0.0361989, recall 0.797101
2017-12-10T15:26:21.941226: step 956, loss 0.449951, acc 0.84375, prec 0.0361805, recall 0.797101
2017-12-10T15:26:22.133542: step 957, loss 3.66471, acc 0.84375, prec 0.0362128, recall 0.79644
2017-12-10T15:26:22.331868: step 958, loss 0.804342, acc 0.84375, prec 0.0362432, recall 0.796667
2017-12-10T15:26:22.530698: step 959, loss 0.541675, acc 0.78125, prec 0.036315, recall 0.797118
2017-12-10T15:26:22.723620: step 960, loss 2.02129, acc 0.75, prec 0.0362875, recall 0.796235
2017-12-10T15:26:22.920601: step 961, loss 0.618705, acc 0.796875, prec 0.0363123, recall 0.79646
2017-12-10T15:26:23.112997: step 962, loss 0.887838, acc 0.78125, prec 0.0363352, recall 0.796685
2017-12-10T15:26:23.303975: step 963, loss 1.051, acc 0.671875, prec 0.0362968, recall 0.796685
2017-12-10T15:26:23.498100: step 964, loss 1.03292, acc 0.703125, prec 0.0363591, recall 0.797133
2017-12-10T15:26:23.694456: step 965, loss 0.9947, acc 0.734375, prec 0.036328, recall 0.797133
2017-12-10T15:26:23.886416: step 966, loss 1.06332, acc 0.703125, prec 0.0362934, recall 0.797133
2017-12-10T15:26:24.082658: step 967, loss 2.19747, acc 0.671875, prec 0.0363053, recall 0.79648
2017-12-10T15:26:24.273843: step 968, loss 0.815245, acc 0.734375, prec 0.0363226, recall 0.796703
2017-12-10T15:26:24.471555: step 969, loss 0.423113, acc 0.78125, prec 0.0363454, recall 0.796926
2017-12-10T15:26:24.667712: step 970, loss 0.775609, acc 0.796875, prec 0.0363218, recall 0.796926
2017-12-10T15:26:24.860774: step 971, loss 0.926704, acc 0.75, prec 0.0363409, recall 0.797149
2017-12-10T15:26:25.055692: step 972, loss 8.12732, acc 0.8125, prec 0.0363209, recall 0.796276
2017-12-10T15:26:25.252804: step 973, loss 0.947685, acc 0.734375, prec 0.0362901, recall 0.796276
2017-12-10T15:26:25.443946: step 974, loss 1.27131, acc 0.84375, prec 0.0364162, recall 0.796943
2017-12-10T15:26:25.637234: step 975, loss 0.933317, acc 0.734375, prec 0.0363854, recall 0.796943
2017-12-10T15:26:25.829184: step 976, loss 0.50577, acc 0.859375, prec 0.0364651, recall 0.797386
2017-12-10T15:26:26.023821: step 977, loss 0.551814, acc 0.765625, prec 0.0364379, recall 0.797386
2017-12-10T15:26:26.219967: step 978, loss 0.799097, acc 0.75, prec 0.0365047, recall 0.797826
2017-12-10T15:26:26.415434: step 979, loss 0.860565, acc 0.78125, prec 0.0365272, recall 0.798046
2017-12-10T15:26:26.605412: step 980, loss 1.86877, acc 0.796875, prec 0.0365993, recall 0.798483
2017-12-10T15:26:26.798701: step 981, loss 1.12821, acc 0.734375, prec 0.0366162, recall 0.798701
2017-12-10T15:26:26.991858: step 982, loss 1.43157, acc 0.78125, prec 0.0366386, recall 0.798919
2017-12-10T15:26:27.188478: step 983, loss 0.705527, acc 0.796875, prec 0.0366627, recall 0.799136
2017-12-10T15:26:27.381489: step 984, loss 0.756113, acc 0.71875, prec 0.03663, recall 0.799136
2017-12-10T15:26:27.574952: step 985, loss 6.0146, acc 0.78125, prec 0.0366559, recall 0.797632
2017-12-10T15:26:27.770921: step 986, loss 3.08702, acc 0.796875, prec 0.0366342, recall 0.796774
2017-12-10T15:26:27.966738: step 987, loss 2.33595, acc 0.640625, prec 0.0367353, recall 0.797428
2017-12-10T15:26:28.160929: step 988, loss 0.94552, acc 0.640625, prec 0.0367411, recall 0.797645
2017-12-10T15:26:28.356234: step 989, loss 5.90444, acc 0.546875, prec 0.0367379, recall 0.797009
2017-12-10T15:26:28.548978: step 990, loss 1.53082, acc 0.59375, prec 0.0367857, recall 0.797441
2017-12-10T15:26:28.744885: step 991, loss 1.29043, acc 0.578125, prec 0.0367369, recall 0.797441
2017-12-10T15:26:28.938298: step 992, loss 2.15003, acc 0.53125, prec 0.0367773, recall 0.797872
2017-12-10T15:26:29.131405: step 993, loss 2.16279, acc 0.390625, prec 0.0367071, recall 0.797872
2017-12-10T15:26:29.309692: step 994, loss 1.52861, acc 0.461538, prec 0.0366569, recall 0.797872
2017-12-10T15:26:29.514067: step 995, loss 2.21795, acc 0.375, prec 0.0365854, recall 0.797872
2017-12-10T15:26:29.720282: step 996, loss 1.62164, acc 0.59375, prec 0.036586, recall 0.798087
2017-12-10T15:26:29.918653: step 997, loss 1.67233, acc 0.5625, prec 0.0366299, recall 0.798515
2017-12-10T15:26:30.109978: step 998, loss 2.08385, acc 0.46875, prec 0.0365694, recall 0.798515
2017-12-10T15:26:30.300772: step 999, loss 1.47205, acc 0.578125, prec 0.0366149, recall 0.798942
2017-12-10T15:26:30.493879: step 1000, loss 1.31498, acc 0.578125, prec 0.036707, recall 0.799578
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_2/1512937383/checkpoints/model-1000

2017-12-10T15:26:31.687307: step 1001, loss 0.993442, acc 0.640625, prec 0.0366662, recall 0.799578
2017-12-10T15:26:31.881491: step 1002, loss 1.11284, acc 0.65625, prec 0.0366738, recall 0.799789
2017-12-10T15:26:32.069983: step 1003, loss 0.749331, acc 0.734375, prec 0.0366902, recall 0.8
2017-12-10T15:26:32.264863: step 1004, loss 0.742829, acc 0.84375, prec 0.0368584, recall 0.800839
2017-12-10T15:26:32.456905: step 1005, loss 0.972205, acc 0.875, prec 0.036937, recall 0.801255
2017-12-10T15:26:32.655233: step 1006, loss 0.376374, acc 0.859375, prec 0.036921, recall 0.801255
2017-12-10T15:26:32.850702: step 1007, loss 0.444738, acc 0.875, prec 0.0369996, recall 0.80167
2017-12-10T15:26:33.044494: step 1008, loss 1.34195, acc 0.9375, prec 0.0369942, recall 0.800834
2017-12-10T15:26:33.240252: step 1009, loss 0.581256, acc 0.828125, prec 0.037021, recall 0.801042
2017-12-10T15:26:33.431802: step 1010, loss 0.322527, acc 0.890625, prec 0.0370549, recall 0.801249
2017-12-10T15:26:33.625489: step 1011, loss 5.56084, acc 0.890625, prec 0.0370442, recall 0.800416
2017-12-10T15:26:33.824967: step 1012, loss 0.293736, acc 0.90625, prec 0.0370335, recall 0.800416
2017-12-10T15:26:34.019764: step 1013, loss 1.33763, acc 0.875, prec 0.0372507, recall 0.801448
2017-12-10T15:26:34.213039: step 1014, loss 0.286093, acc 0.875, prec 0.0372363, recall 0.801448
2017-12-10T15:26:34.406928: step 1015, loss 0.594723, acc 0.875, prec 0.0372683, recall 0.801653
2017-12-10T15:26:34.608965: step 1016, loss 5.12433, acc 0.828125, prec 0.0372504, recall 0.800826
2017-12-10T15:26:34.801441: step 1017, loss 0.66175, acc 0.859375, prec 0.0373267, recall 0.801236
2017-12-10T15:26:34.995761: step 1018, loss 0.21638, acc 0.875, prec 0.0373124, recall 0.801236
2017-12-10T15:26:35.192982: step 1019, loss 0.24253, acc 0.921875, prec 0.0373034, recall 0.801236
2017-12-10T15:26:35.389014: step 1020, loss 3.6697, acc 0.734375, prec 0.0372748, recall 0.800412
2017-12-10T15:26:35.589962: step 1021, loss 0.638399, acc 0.765625, prec 0.0372941, recall 0.800617
2017-12-10T15:26:35.778332: step 1022, loss 0.574861, acc 0.78125, prec 0.0372692, recall 0.800617
2017-12-10T15:26:35.973830: step 1023, loss 0.761542, acc 0.78125, prec 0.0373363, recall 0.801026
2017-12-10T15:26:36.164884: step 1024, loss 0.477412, acc 0.84375, prec 0.0374104, recall 0.801433
2017-12-10T15:26:36.368507: step 1025, loss 1.10985, acc 0.6875, prec 0.0374206, recall 0.801636
2017-12-10T15:26:36.562696: step 1026, loss 0.94352, acc 0.6875, prec 0.037385, recall 0.801636
2017-12-10T15:26:36.752647: step 1027, loss 0.866302, acc 0.796875, prec 0.0373618, recall 0.801636
2017-12-10T15:26:36.945742: step 1028, loss 0.847096, acc 0.734375, prec 0.0373774, recall 0.801839
2017-12-10T15:26:37.137532: step 1029, loss 0.590278, acc 0.78125, prec 0.0374441, recall 0.802243
2017-12-10T15:26:37.332180: step 1030, loss 0.676586, acc 0.859375, prec 0.0374738, recall 0.802444
2017-12-10T15:26:37.523419: step 1031, loss 0.548915, acc 0.734375, prec 0.0374436, recall 0.802444
2017-12-10T15:26:37.717012: step 1032, loss 0.982175, acc 0.703125, prec 0.0374098, recall 0.802444
2017-12-10T15:26:37.908877: step 1033, loss 0.496874, acc 0.828125, prec 0.0373903, recall 0.802444
2017-12-10T15:26:38.099920: step 1034, loss 0.810762, acc 0.828125, prec 0.0374164, recall 0.802645
2017-12-10T15:26:38.293286: step 1035, loss 0.72278, acc 0.765625, prec 0.0373898, recall 0.802645
2017-12-10T15:26:38.491283: step 1036, loss 0.575239, acc 0.8125, prec 0.0374142, recall 0.802846
2017-12-10T15:26:38.685416: step 1037, loss 0.47136, acc 0.84375, prec 0.0373964, recall 0.802846
2017-12-10T15:26:38.886321: step 1038, loss 0.526751, acc 0.8125, prec 0.0374208, recall 0.803046
2017-12-10T15:26:39.082519: step 1039, loss 1.21615, acc 0.90625, prec 0.0375922, recall 0.803842
2017-12-10T15:26:39.273681: step 1040, loss 0.439854, acc 0.9375, prec 0.0376306, recall 0.80404
2017-12-10T15:26:39.471857: step 1041, loss 0.873474, acc 0.875, prec 0.0376618, recall 0.804238
2017-12-10T15:26:39.672867: step 1042, loss 0.393964, acc 0.9375, prec 0.0377457, recall 0.804632
2017-12-10T15:26:39.873278: step 1043, loss 0.373516, acc 0.90625, prec 0.0378258, recall 0.805025
2017-12-10T15:26:40.067801: step 1044, loss 4.10865, acc 0.84375, prec 0.0378116, recall 0.80341
2017-12-10T15:26:40.263012: step 1045, loss 0.522399, acc 0.921875, prec 0.037848, recall 0.803607
2017-12-10T15:26:40.464300: step 1046, loss 0.36831, acc 0.859375, prec 0.037832, recall 0.803607
2017-12-10T15:26:40.660053: step 1047, loss 0.565708, acc 0.828125, prec 0.0379031, recall 0.804
2017-12-10T15:26:40.853724: step 1048, loss 0.408809, acc 0.859375, prec 0.037887, recall 0.804
2017-12-10T15:26:41.050572: step 1049, loss 0.681942, acc 0.734375, prec 0.0379473, recall 0.804391
2017-12-10T15:26:41.244934: step 1050, loss 0.466145, acc 0.828125, prec 0.0379276, recall 0.804391
2017-12-10T15:26:41.441536: step 1051, loss 3.59253, acc 0.78125, prec 0.0379497, recall 0.803785
2017-12-10T15:26:41.637710: step 1052, loss 0.845812, acc 0.671875, prec 0.0379122, recall 0.803785
2017-12-10T15:26:41.833028: step 1053, loss 0.958616, acc 0.71875, prec 0.0379705, recall 0.804175
2017-12-10T15:26:42.027829: step 1054, loss 0.651248, acc 0.875, prec 0.0380917, recall 0.804757
2017-12-10T15:26:42.229685: step 1055, loss 0.785267, acc 0.796875, prec 0.0381135, recall 0.80495
2017-12-10T15:26:42.418524: step 1056, loss 0.654661, acc 0.765625, prec 0.0381318, recall 0.805143
2017-12-10T15:26:42.613698: step 1057, loss 0.339457, acc 0.859375, prec 0.0381608, recall 0.805336
2017-12-10T15:26:42.807135: step 1058, loss 1.62393, acc 0.78125, prec 0.0382258, recall 0.80572
2017-12-10T15:26:43.003432: step 1059, loss 0.43995, acc 0.828125, prec 0.0382511, recall 0.805911
2017-12-10T15:26:43.195862: step 1060, loss 0.309708, acc 0.84375, prec 0.0382332, recall 0.805911
2017-12-10T15:26:43.387852: step 1061, loss 0.581344, acc 0.828125, prec 0.0382136, recall 0.805911
2017-12-10T15:26:43.581545: step 1062, loss 0.638701, acc 0.734375, prec 0.0382282, recall 0.806102
2017-12-10T15:26:43.772357: step 1063, loss 0.878844, acc 0.75, prec 0.0383342, recall 0.806673
2017-12-10T15:26:43.965970: step 1064, loss 0.704639, acc 0.734375, prec 0.0383038, recall 0.806673
2017-12-10T15:26:44.161818: step 1065, loss 0.637779, acc 0.78125, prec 0.0383236, recall 0.806863
2017-12-10T15:26:44.361810: step 1066, loss 0.467565, acc 0.828125, prec 0.038304, recall 0.806863
2017-12-10T15:26:44.557735: step 1067, loss 0.63399, acc 0.84375, prec 0.0383309, recall 0.807052
2017-12-10T15:26:44.754792: step 1068, loss 0.382795, acc 0.890625, prec 0.0383185, recall 0.807052
2017-12-10T15:26:44.948482: step 1069, loss 0.895944, acc 0.75, prec 0.0383346, recall 0.807241
2017-12-10T15:26:45.142111: step 1070, loss 1.02629, acc 0.875, prec 0.0383651, recall 0.807429
2017-12-10T15:26:45.339874: step 1071, loss 0.292528, acc 0.890625, prec 0.0383526, recall 0.807429
2017-12-10T15:26:45.537392: step 1072, loss 0.459018, acc 0.875, prec 0.0383384, recall 0.807429
2017-12-10T15:26:45.735098: step 1073, loss 0.353653, acc 0.828125, prec 0.0383188, recall 0.807429
2017-12-10T15:26:45.933796: step 1074, loss 6.0765, acc 0.859375, prec 0.0383492, recall 0.806829
2017-12-10T15:26:46.134508: step 1075, loss 0.55536, acc 0.875, prec 0.0384241, recall 0.807205
2017-12-10T15:26:46.327777: step 1076, loss 0.628436, acc 0.8125, prec 0.0384473, recall 0.807393
2017-12-10T15:26:46.519741: step 1077, loss 0.223653, acc 0.90625, prec 0.0384366, recall 0.807393
2017-12-10T15:26:46.713259: step 1078, loss 0.255415, acc 0.890625, prec 0.0384687, recall 0.80758
2017-12-10T15:26:46.906073: step 1079, loss 0.245849, acc 0.84375, prec 0.0384509, recall 0.80758
2017-12-10T15:26:47.102350: step 1080, loss 0.360602, acc 0.875, prec 0.0384366, recall 0.80758
2017-12-10T15:26:47.294699: step 1081, loss 0.325668, acc 0.859375, prec 0.0385095, recall 0.807953
2017-12-10T15:26:47.488424: step 1082, loss 1.39635, acc 0.875, prec 0.0385397, recall 0.80814
2017-12-10T15:26:47.683187: step 1083, loss 0.542295, acc 0.828125, prec 0.0385646, recall 0.808325
2017-12-10T15:26:47.879197: step 1084, loss 0.388811, acc 0.84375, prec 0.0385468, recall 0.808325
2017-12-10T15:26:48.073573: step 1085, loss 0.505362, acc 0.84375, prec 0.0386177, recall 0.808696
2017-12-10T15:26:48.267316: step 1086, loss 0.580561, acc 0.78125, prec 0.0386814, recall 0.809065
2017-12-10T15:26:48.460866: step 1087, loss 0.311045, acc 0.828125, prec 0.0387061, recall 0.809249
2017-12-10T15:26:48.654957: step 1088, loss 0.643889, acc 0.796875, prec 0.0386829, recall 0.809249
2017-12-10T15:26:48.853615: step 1089, loss 0.19075, acc 0.9375, prec 0.0386758, recall 0.809249
2017-12-10T15:26:49.046837: step 1090, loss 0.446401, acc 0.875, prec 0.0386616, recall 0.809249
2017-12-10T15:26:49.241660: step 1091, loss 2.63737, acc 0.8125, prec 0.038642, recall 0.80847
2017-12-10T15:26:49.436178: step 1092, loss 1.27489, acc 0.859375, prec 0.038847, recall 0.809387
2017-12-10T15:26:49.636029: step 1093, loss 0.35381, acc 0.890625, prec 0.0388787, recall 0.809569
2017-12-10T15:26:49.829855: step 1094, loss 0.960363, acc 0.84375, prec 0.038905, recall 0.809751
2017-12-10T15:26:50.027877: step 1095, loss 0.631854, acc 0.78125, prec 0.0389241, recall 0.809933
2017-12-10T15:26:50.224469: step 1096, loss 0.616559, acc 0.8125, prec 0.0389467, recall 0.810115
2017-12-10T15:26:50.420147: step 1097, loss 0.373863, acc 0.859375, prec 0.0389747, recall 0.810296
2017-12-10T15:26:50.617696: step 1098, loss 0.253252, acc 0.890625, prec 0.0390503, recall 0.810656
2017-12-10T15:26:50.815145: step 1099, loss 2.23577, acc 0.75, prec 0.0390235, recall 0.809886
2017-12-10T15:26:51.011266: step 1100, loss 0.59514, acc 0.765625, prec 0.0389967, recall 0.809886
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_2/1512937383/checkpoints/model-1100

2017-12-10T15:26:52.863411: step 1101, loss 0.778124, acc 0.796875, prec 0.0389735, recall 0.809886
2017-12-10T15:26:53.054424: step 1102, loss 1.24734, acc 0.71875, prec 0.0389854, recall 0.810066
2017-12-10T15:26:53.254247: step 1103, loss 0.791958, acc 0.796875, prec 0.0390061, recall 0.810247
2017-12-10T15:26:53.451746: step 1104, loss 0.596958, acc 0.8125, prec 0.0390286, recall 0.810427
2017-12-10T15:26:53.648273: step 1105, loss 0.465483, acc 0.859375, prec 0.0390564, recall 0.810606
2017-12-10T15:26:53.857332: step 1106, loss 1.12543, acc 0.703125, prec 0.0391102, recall 0.810964
2017-12-10T15:26:54.049424: step 1107, loss 0.753319, acc 0.75, prec 0.0390817, recall 0.810964
2017-12-10T15:26:54.248635: step 1108, loss 0.588629, acc 0.796875, prec 0.0391023, recall 0.811143
2017-12-10T15:26:54.443077: step 1109, loss 0.554696, acc 0.78125, prec 0.0391211, recall 0.811321
2017-12-10T15:26:54.633463: step 1110, loss 0.396674, acc 0.84375, prec 0.0391034, recall 0.811321
2017-12-10T15:26:54.831227: step 1111, loss 2.34298, acc 0.84375, prec 0.039131, recall 0.810734
2017-12-10T15:26:55.031747: step 1112, loss 0.407888, acc 0.859375, prec 0.039115, recall 0.810734
2017-12-10T15:26:55.225718: step 1113, loss 0.571531, acc 0.71875, prec 0.0390831, recall 0.810734
2017-12-10T15:26:55.423020: step 1114, loss 0.469094, acc 0.796875, prec 0.03906, recall 0.810734
2017-12-10T15:26:55.619679: step 1115, loss 0.915856, acc 0.921875, prec 0.0390947, recall 0.810912
2017-12-10T15:26:55.815686: step 1116, loss 0.324856, acc 0.828125, prec 0.0391188, recall 0.81109
2017-12-10T15:26:56.011823: step 1117, loss 0.486327, acc 0.8125, prec 0.0391411, recall 0.811268
2017-12-10T15:26:56.212917: step 1118, loss 0.588696, acc 0.828125, prec 0.0391651, recall 0.811445
2017-12-10T15:26:56.411848: step 1119, loss 0.374437, acc 0.8125, prec 0.0391438, recall 0.811445
2017-12-10T15:26:56.613885: step 1120, loss 0.576848, acc 0.84375, prec 0.0391261, recall 0.811445
2017-12-10T15:26:56.811042: step 1121, loss 1.83704, acc 0.890625, prec 0.0391589, recall 0.810861
2017-12-10T15:26:57.010302: step 1122, loss 0.387284, acc 0.875, prec 0.0391448, recall 0.810861
2017-12-10T15:26:57.202680: step 1123, loss 0.194966, acc 0.953125, prec 0.0391829, recall 0.811038
2017-12-10T15:26:57.395469: step 1124, loss 0.493202, acc 0.8125, prec 0.0392051, recall 0.811215
2017-12-10T15:26:57.585358: step 1125, loss 0.302124, acc 0.90625, prec 0.0391944, recall 0.811215
2017-12-10T15:26:57.781393: step 1126, loss 0.287682, acc 0.90625, prec 0.0391838, recall 0.811215
2017-12-10T15:26:57.974513: step 1127, loss 0.179096, acc 0.9375, prec 0.0391767, recall 0.811215
2017-12-10T15:26:58.171361: step 1128, loss 0.16605, acc 0.9375, prec 0.0391697, recall 0.811215
2017-12-10T15:26:58.366085: step 1129, loss 0.333115, acc 0.875, prec 0.0391555, recall 0.811215
2017-12-10T15:26:58.555765: step 1130, loss 0.364352, acc 0.875, prec 0.0391414, recall 0.811215
2017-12-10T15:26:58.754715: step 1131, loss 1.47332, acc 0.921875, prec 0.0391344, recall 0.810458
2017-12-10T15:26:58.949201: step 1132, loss 0.168704, acc 0.9375, prec 0.0391273, recall 0.810458
2017-12-10T15:26:59.147836: step 1133, loss 0.196168, acc 0.890625, prec 0.0391583, recall 0.810634
2017-12-10T15:26:59.344957: step 1134, loss 0.235816, acc 0.90625, prec 0.039191, recall 0.810811
2017-12-10T15:26:59.539236: step 1135, loss 1.20618, acc 0.890625, prec 0.0393084, recall 0.811338
2017-12-10T15:26:59.733712: step 1136, loss 0.490141, acc 0.90625, prec 0.039341, recall 0.811513
2017-12-10T15:26:59.932946: step 1137, loss 0.826725, acc 0.875, prec 0.0393701, recall 0.811688
2017-12-10T15:27:00.126681: step 1138, loss 0.381747, acc 0.890625, prec 0.0394441, recall 0.812037
2017-12-10T15:27:00.324382: step 1139, loss 1.24213, acc 0.875, prec 0.0395595, recall 0.812558
2017-12-10T15:27:00.522411: step 1140, loss 0.839563, acc 0.90625, prec 0.0396351, recall 0.812903
2017-12-10T15:27:00.716631: step 1141, loss 0.402346, acc 0.875, prec 0.0396209, recall 0.812903
2017-12-10T15:27:00.913453: step 1142, loss 0.494373, acc 0.8125, prec 0.0395995, recall 0.812903
2017-12-10T15:27:01.105282: step 1143, loss 0.678995, acc 0.765625, prec 0.0395729, recall 0.812903
2017-12-10T15:27:01.305368: step 1144, loss 0.458128, acc 0.890625, prec 0.0396466, recall 0.813247
2017-12-10T15:27:01.507268: step 1145, loss 0.433883, acc 0.859375, prec 0.0396736, recall 0.813419
2017-12-10T15:27:01.704362: step 1146, loss 0.964518, acc 0.703125, prec 0.0396829, recall 0.81359
2017-12-10T15:27:01.898537: step 1147, loss 0.651302, acc 0.796875, prec 0.0397028, recall 0.813761
2017-12-10T15:27:02.094348: step 1148, loss 3.31297, acc 0.8125, prec 0.0396832, recall 0.813016
2017-12-10T15:27:02.290337: step 1149, loss 0.282163, acc 0.890625, prec 0.0396708, recall 0.813016
2017-12-10T15:27:02.484682: step 1150, loss 0.604919, acc 0.765625, prec 0.0396442, recall 0.813016
2017-12-10T15:27:02.680429: step 1151, loss 0.559533, acc 0.859375, prec 0.039757, recall 0.813528
2017-12-10T15:27:02.880018: step 1152, loss 0.458211, acc 0.8125, prec 0.0397357, recall 0.813528
2017-12-10T15:27:03.076768: step 1153, loss 0.50506, acc 0.8125, prec 0.0397144, recall 0.813528
2017-12-10T15:27:03.271570: step 1154, loss 0.69104, acc 0.78125, prec 0.0396896, recall 0.813528
2017-12-10T15:27:03.462954: step 1155, loss 0.567352, acc 0.828125, prec 0.039713, recall 0.813699
2017-12-10T15:27:03.655461: step 1156, loss 0.678214, acc 0.734375, prec 0.0396829, recall 0.813699
2017-12-10T15:27:03.851156: step 1157, loss 0.521001, acc 0.8125, prec 0.0396617, recall 0.813699
2017-12-10T15:27:04.044610: step 1158, loss 0.447068, acc 0.859375, prec 0.0396885, recall 0.813869
2017-12-10T15:27:04.239822: step 1159, loss 0.609391, acc 0.84375, prec 0.0397563, recall 0.814208
2017-12-10T15:27:04.432322: step 1160, loss 1.09813, acc 0.828125, prec 0.0398649, recall 0.814714
2017-12-10T15:27:04.626426: step 1161, loss 0.331787, acc 0.890625, prec 0.0399378, recall 0.81505
2017-12-10T15:27:04.826982: step 1162, loss 0.361629, acc 0.921875, prec 0.0399716, recall 0.815217
2017-12-10T15:27:05.021562: step 1163, loss 0.368161, acc 0.859375, prec 0.0399556, recall 0.815217
2017-12-10T15:27:05.217560: step 1164, loss 0.317773, acc 0.890625, prec 0.0399858, recall 0.815385
2017-12-10T15:27:05.409570: step 1165, loss 0.22492, acc 0.890625, prec 0.0399734, recall 0.815385
2017-12-10T15:27:05.602653: step 1166, loss 0.479707, acc 0.921875, prec 0.0400922, recall 0.815884
2017-12-10T15:27:05.808068: step 1167, loss 0.318406, acc 0.859375, prec 0.0400763, recall 0.815884
2017-12-10T15:27:06.001503: step 1168, loss 0.228897, acc 0.921875, prec 0.0400674, recall 0.815884
2017-12-10T15:27:06.198492: step 1169, loss 0.201016, acc 0.890625, prec 0.04014, recall 0.816216
2017-12-10T15:27:06.393654: step 1170, loss 0.343831, acc 0.890625, prec 0.0401276, recall 0.816216
2017-12-10T15:27:06.590899: step 1171, loss 0.21551, acc 0.90625, prec 0.0401169, recall 0.816216
2017-12-10T15:27:06.788618: step 1172, loss 0.26415, acc 0.96875, prec 0.0401558, recall 0.816382
2017-12-10T15:27:06.983145: step 1173, loss 2.88847, acc 0.9375, prec 0.040193, recall 0.815813
2017-12-10T15:27:07.176118: step 1174, loss 0.271736, acc 0.9375, prec 0.0402284, recall 0.815978
2017-12-10T15:27:07.377611: step 1175, loss 0.0977309, acc 0.96875, prec 0.0402248, recall 0.815978
2017-12-10T15:27:07.570151: step 1176, loss 6.55242, acc 0.921875, prec 0.0402195, recall 0.814516
2017-12-10T15:27:07.772352: step 1177, loss 0.232179, acc 0.875, prec 0.0402052, recall 0.814516
2017-12-10T15:27:07.970825: step 1178, loss 1.53599, acc 0.859375, prec 0.040191, recall 0.813787
2017-12-10T15:27:08.168559: step 1179, loss 1.07213, acc 0.921875, prec 0.0402245, recall 0.813953
2017-12-10T15:27:08.364194: step 1180, loss 0.629083, acc 0.78125, prec 0.0403269, recall 0.814451
2017-12-10T15:27:08.559006: step 1181, loss 0.691985, acc 0.71875, prec 0.0402948, recall 0.814451
2017-12-10T15:27:08.751550: step 1182, loss 0.939857, acc 0.828125, prec 0.0403176, recall 0.814617
2017-12-10T15:27:08.944498: step 1183, loss 0.755232, acc 0.734375, prec 0.0402874, recall 0.814617
2017-12-10T15:27:09.141510: step 1184, loss 0.886328, acc 0.71875, prec 0.0402977, recall 0.814782
2017-12-10T15:27:09.335748: step 1185, loss 0.891351, acc 0.671875, prec 0.0403449, recall 0.815111
2017-12-10T15:27:09.530348: step 1186, loss 0.895395, acc 0.6875, prec 0.0403095, recall 0.815111
2017-12-10T15:27:09.724011: step 1187, loss 0.974573, acc 0.65625, prec 0.0403127, recall 0.815275
2017-12-10T15:27:09.917404: step 1188, loss 1.24493, acc 0.65625, prec 0.0403159, recall 0.815439
2017-12-10T15:27:10.112820: step 1189, loss 0.755551, acc 0.8125, prec 0.0402946, recall 0.815439
2017-12-10T15:27:10.309466: step 1190, loss 0.689186, acc 0.765625, prec 0.0402682, recall 0.815439
2017-12-10T15:27:10.506948: step 1191, loss 0.718899, acc 0.8125, prec 0.040289, recall 0.815603
2017-12-10T15:27:10.701081: step 1192, loss 0.933246, acc 0.703125, prec 0.0403395, recall 0.815929
2017-12-10T15:27:10.897811: step 1193, loss 0.927609, acc 0.6875, prec 0.0403462, recall 0.816092
2017-12-10T15:27:11.093581: step 1194, loss 0.814436, acc 0.78125, prec 0.0403634, recall 0.816254
2017-12-10T15:27:11.289105: step 1195, loss 0.708715, acc 0.78125, prec 0.0403388, recall 0.816254
2017-12-10T15:27:11.486008: step 1196, loss 0.700715, acc 0.765625, prec 0.0403542, recall 0.816417
2017-12-10T15:27:11.684913: step 1197, loss 0.595818, acc 0.765625, prec 0.0403279, recall 0.816417
2017-12-10T15:27:11.877260: step 1198, loss 0.840909, acc 0.8125, prec 0.0403486, recall 0.816579
2017-12-10T15:27:12.070639: step 1199, loss 0.28847, acc 0.890625, prec 0.0403363, recall 0.816579
2017-12-10T15:27:12.263210: step 1200, loss 0.33724, acc 0.84375, prec 0.0403187, recall 0.816579
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_2/1512937383/checkpoints/model-1200

2017-12-10T15:27:14.164787: step 1201, loss 0.243209, acc 0.921875, prec 0.0403517, recall 0.81674
2017-12-10T15:27:14.363825: step 1202, loss 0.723793, acc 0.859375, prec 0.0403777, recall 0.816901
2017-12-10T15:27:14.556786: step 1203, loss 0.201456, acc 0.9375, prec 0.0403706, recall 0.816901
2017-12-10T15:27:14.755447: step 1204, loss 0.198389, acc 0.9375, prec 0.0404054, recall 0.817062
2017-12-10T15:27:14.949692: step 1205, loss 0.568745, acc 0.921875, prec 0.04048, recall 0.817384
2017-12-10T15:27:15.147798: step 1206, loss 1.23036, acc 0.953125, prec 0.0405165, recall 0.817544
2017-12-10T15:27:15.346526: step 1207, loss 1.269, acc 0.90625, prec 0.0405476, recall 0.817704
2017-12-10T15:27:15.545806: step 1208, loss 0.297235, acc 0.890625, prec 0.0405353, recall 0.817704
2017-12-10T15:27:15.737949: step 1209, loss 0.0741728, acc 0.984375, prec 0.0405335, recall 0.817704
2017-12-10T15:27:15.932202: step 1210, loss 0.229097, acc 0.875, prec 0.0405611, recall 0.817863
2017-12-10T15:27:16.126147: step 1211, loss 0.291997, acc 0.921875, prec 0.0405523, recall 0.817863
2017-12-10T15:27:16.320183: step 1212, loss 9.60499, acc 0.921875, prec 0.040547, recall 0.816434
2017-12-10T15:27:16.522558: step 1213, loss 0.154119, acc 0.90625, prec 0.0405364, recall 0.816434
2017-12-10T15:27:16.714471: step 1214, loss 0.40372, acc 0.828125, prec 0.0405171, recall 0.816434
2017-12-10T15:27:16.916360: step 1215, loss 0.601155, acc 0.796875, prec 0.0405774, recall 0.816754
2017-12-10T15:27:17.111485: step 1216, loss 0.334542, acc 0.921875, prec 0.0406102, recall 0.816914
2017-12-10T15:27:17.308758: step 1217, loss 0.538679, acc 0.84375, prec 0.0405926, recall 0.816914
2017-12-10T15:27:17.505602: step 1218, loss 0.361329, acc 0.859375, prec 0.0406184, recall 0.817073
2017-12-10T15:27:17.698072: step 1219, loss 0.789152, acc 0.796875, prec 0.040637, recall 0.817232
2017-12-10T15:27:17.889024: step 1220, loss 1.72279, acc 0.734375, prec 0.0406919, recall 0.81684
2017-12-10T15:27:18.087555: step 1221, loss 0.913692, acc 0.75, prec 0.0407052, recall 0.816999
2017-12-10T15:27:18.280648: step 1222, loss 0.546827, acc 0.734375, prec 0.0406753, recall 0.816999
2017-12-10T15:27:18.477563: step 1223, loss 1.37723, acc 0.640625, prec 0.0407591, recall 0.817474
2017-12-10T15:27:18.680075: step 1224, loss 0.970347, acc 0.703125, prec 0.0407257, recall 0.817474
2017-12-10T15:27:18.874654: step 1225, loss 0.881134, acc 0.703125, prec 0.0407337, recall 0.817632
2017-12-10T15:27:19.067066: step 1226, loss 0.645343, acc 0.8125, prec 0.040754, recall 0.817789
2017-12-10T15:27:19.258476: step 1227, loss 0.856074, acc 0.765625, prec 0.0408927, recall 0.818417
2017-12-10T15:27:19.458847: step 1228, loss 0.752489, acc 0.78125, prec 0.0409917, recall 0.818884
2017-12-10T15:27:19.652902: step 1229, loss 0.693974, acc 0.8125, prec 0.0410118, recall 0.819039
2017-12-10T15:27:19.845957: step 1230, loss 0.573387, acc 0.828125, prec 0.0409924, recall 0.819039
2017-12-10T15:27:20.044925: step 1231, loss 4.83244, acc 0.765625, prec 0.0410912, recall 0.818803
2017-12-10T15:27:20.240057: step 1232, loss 1.09959, acc 0.6875, prec 0.0411792, recall 0.819267
2017-12-10T15:27:20.432034: step 1233, loss 0.674326, acc 0.796875, prec 0.0411974, recall 0.819421
2017-12-10T15:27:20.629537: step 1234, loss 1.23317, acc 0.703125, prec 0.041287, recall 0.819881
2017-12-10T15:27:20.825539: step 1235, loss 0.691186, acc 0.765625, prec 0.0413015, recall 0.820034
2017-12-10T15:27:21.027550: step 1236, loss 0.593642, acc 0.78125, prec 0.0412768, recall 0.820034
2017-12-10T15:27:21.219350: step 1237, loss 0.896955, acc 0.703125, prec 0.0413661, recall 0.820491
2017-12-10T15:27:21.417679: step 1238, loss 0.561702, acc 0.75, prec 0.0413787, recall 0.820643
2017-12-10T15:27:21.616725: step 1239, loss 0.823131, acc 0.71875, prec 0.0414287, recall 0.820946
2017-12-10T15:27:21.810365: step 1240, loss 0.94985, acc 0.65625, prec 0.0414307, recall 0.821097
2017-12-10T15:27:22.009482: step 1241, loss 0.593973, acc 0.828125, prec 0.0414113, recall 0.821097
2017-12-10T15:27:22.207533: step 1242, loss 1.45844, acc 0.75, prec 0.0415054, recall 0.821549
2017-12-10T15:27:22.407008: step 1243, loss 0.674094, acc 0.75, prec 0.0414772, recall 0.821549
2017-12-10T15:27:22.605316: step 1244, loss 0.486225, acc 0.796875, prec 0.0415357, recall 0.821849
2017-12-10T15:27:22.798773: step 1245, loss 0.609402, acc 0.71875, prec 0.041504, recall 0.821849
2017-12-10T15:27:22.993086: step 1246, loss 4.97408, acc 0.8125, prec 0.0415659, recall 0.821459
2017-12-10T15:27:23.193606: step 1247, loss 0.510027, acc 0.8125, prec 0.0415448, recall 0.821459
2017-12-10T15:27:23.389734: step 1248, loss 0.427071, acc 0.8125, prec 0.0415237, recall 0.821459
2017-12-10T15:27:23.591768: step 1249, loss 0.400954, acc 0.828125, prec 0.0415449, recall 0.821608
2017-12-10T15:27:23.785627: step 1250, loss 0.41793, acc 0.859375, prec 0.0415291, recall 0.821608
2017-12-10T15:27:23.981005: step 1251, loss 0.305669, acc 0.875, prec 0.041515, recall 0.821608
2017-12-10T15:27:24.176547: step 1252, loss 0.528798, acc 0.765625, prec 0.0415292, recall 0.821757
2017-12-10T15:27:24.369992: step 1253, loss 0.248603, acc 0.90625, prec 0.0415187, recall 0.821757
2017-12-10T15:27:24.569355: step 1254, loss 0.697784, acc 0.8125, prec 0.0414976, recall 0.821757
2017-12-10T15:27:24.765732: step 1255, loss 0.309949, acc 0.859375, prec 0.0416033, recall 0.822204
2017-12-10T15:27:24.959478: step 1256, loss 0.411528, acc 0.828125, prec 0.041584, recall 0.822204
2017-12-10T15:27:25.154735: step 1257, loss 0.204262, acc 0.90625, prec 0.0415735, recall 0.822204
2017-12-10T15:27:25.345846: step 1258, loss 0.429901, acc 0.859375, prec 0.0415577, recall 0.822204
2017-12-10T15:27:25.547997: step 1259, loss 0.308809, acc 0.859375, prec 0.0415419, recall 0.822204
2017-12-10T15:27:25.744028: step 1260, loss 0.286124, acc 0.859375, prec 0.0415665, recall 0.822352
2017-12-10T15:27:25.939197: step 1261, loss 0.293193, acc 0.921875, prec 0.0415982, recall 0.8225
2017-12-10T15:27:26.140474: step 1262, loss 0.235087, acc 0.9375, prec 0.0415912, recall 0.8225
2017-12-10T15:27:26.338513: step 1263, loss 10.4911, acc 0.921875, prec 0.0415842, recall 0.821815
2017-12-10T15:27:26.536152: step 1264, loss 0.282455, acc 0.90625, prec 0.0415736, recall 0.821815
2017-12-10T15:27:26.732019: step 1265, loss 0.364034, acc 0.890625, prec 0.0416018, recall 0.821963
2017-12-10T15:27:26.928380: step 1266, loss 1.04616, acc 0.921875, prec 0.0416333, recall 0.822111
2017-12-10T15:27:27.125296: step 1267, loss 0.444454, acc 0.90625, prec 0.0416632, recall 0.822259
2017-12-10T15:27:27.323825: step 1268, loss 0.236163, acc 0.875, prec 0.0416491, recall 0.822259
2017-12-10T15:27:27.523126: step 1269, loss 0.112421, acc 0.953125, prec 0.0416439, recall 0.822259
2017-12-10T15:27:27.719304: step 1270, loss 1.11677, acc 0.890625, prec 0.0418331, recall 0.822994
2017-12-10T15:27:27.919741: step 1271, loss 0.140212, acc 0.96875, prec 0.0418698, recall 0.823141
2017-12-10T15:27:28.118524: step 1272, loss 1.44393, acc 0.90625, prec 0.0419801, recall 0.823578
2017-12-10T15:27:28.313524: step 1273, loss 0.378892, acc 0.90625, prec 0.04205, recall 0.823868
2017-12-10T15:27:28.507553: step 1274, loss 0.256777, acc 0.84375, prec 0.0420323, recall 0.823868
2017-12-10T15:27:28.704150: step 1275, loss 0.289681, acc 0.90625, prec 0.0421022, recall 0.824158
2017-12-10T15:27:28.903453: step 1276, loss 0.491112, acc 0.796875, prec 0.0421194, recall 0.824302
2017-12-10T15:27:29.103795: step 1277, loss 0.500006, acc 0.859375, prec 0.0421437, recall 0.824446
2017-12-10T15:27:29.304111: step 1278, loss 0.770223, acc 0.78125, prec 0.0421591, recall 0.82459
2017-12-10T15:27:29.503199: step 1279, loss 1.96284, acc 0.859375, prec 0.042145, recall 0.823915
2017-12-10T15:27:29.706739: step 1280, loss 0.939992, acc 0.78125, prec 0.0421202, recall 0.823915
2017-12-10T15:27:29.899032: step 1281, loss 0.348828, acc 0.84375, prec 0.0421026, recall 0.823915
2017-12-10T15:27:30.092947: step 1282, loss 2.02304, acc 0.8125, prec 0.0421233, recall 0.823385
2017-12-10T15:27:30.289513: step 1283, loss 0.562005, acc 0.859375, prec 0.0421475, recall 0.823529
2017-12-10T15:27:30.484336: step 1284, loss 0.58404, acc 0.796875, prec 0.0421646, recall 0.823673
2017-12-10T15:27:30.676309: step 1285, loss 0.574895, acc 0.75, prec 0.0421365, recall 0.823673
2017-12-10T15:27:30.872113: step 1286, loss 0.683329, acc 0.734375, prec 0.0421466, recall 0.823817
2017-12-10T15:27:31.066251: step 1287, loss 1.07985, acc 0.65625, prec 0.0421079, recall 0.823817
2017-12-10T15:27:31.263553: step 1288, loss 0.444841, acc 0.828125, prec 0.0420886, recall 0.823817
2017-12-10T15:27:31.462024: step 1289, loss 1.43644, acc 0.765625, prec 0.0421022, recall 0.823961
2017-12-10T15:27:31.661056: step 1290, loss 0.856254, acc 0.8125, prec 0.0421609, recall 0.824247
2017-12-10T15:27:31.857248: step 1291, loss 0.862821, acc 0.734375, prec 0.0421311, recall 0.824247
2017-12-10T15:27:32.057956: step 1292, loss 0.80399, acc 0.703125, prec 0.0421376, recall 0.82439
2017-12-10T15:27:32.250438: step 1293, loss 0.357829, acc 0.84375, prec 0.0421599, recall 0.824533
2017-12-10T15:27:32.442945: step 1294, loss 0.646093, acc 0.84375, prec 0.0421822, recall 0.824675
2017-12-10T15:27:32.639206: step 1295, loss 0.623278, acc 0.8125, prec 0.0421612, recall 0.824675
2017-12-10T15:27:32.833233: step 1296, loss 0.770845, acc 0.75, prec 0.0422126, recall 0.824959
2017-12-10T15:27:33.027875: step 1297, loss 0.7004, acc 0.828125, prec 0.0422331, recall 0.825101
2017-12-10T15:27:33.224038: step 1298, loss 0.570358, acc 0.8125, prec 0.0422518, recall 0.825243
2017-12-10T15:27:33.416444: step 1299, loss 0.562792, acc 0.84375, prec 0.0422739, recall 0.825384
2017-12-10T15:27:33.610970: step 1300, loss 2.37918, acc 0.90625, prec 0.0423048, recall 0.824859
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_2/1512937383/checkpoints/model-1300

2017-12-10T15:27:34.837412: step 1301, loss 0.836569, acc 0.78125, prec 0.0422803, recall 0.824859
2017-12-10T15:27:35.031196: step 1302, loss 0.291884, acc 0.890625, prec 0.0422681, recall 0.824859
2017-12-10T15:27:35.228534: step 1303, loss 0.218543, acc 0.890625, prec 0.0422558, recall 0.824859
2017-12-10T15:27:35.421321: step 1304, loss 0.150652, acc 0.921875, prec 0.0422471, recall 0.824859
2017-12-10T15:27:35.617584: step 1305, loss 0.752089, acc 0.9375, prec 0.0422797, recall 0.825
2017-12-10T15:27:35.819441: step 1306, loss 0.720658, acc 0.84375, prec 0.0423018, recall 0.825141
2017-12-10T15:27:36.015921: step 1307, loss 0.698411, acc 0.828125, prec 0.0423221, recall 0.825282
2017-12-10T15:27:36.213466: step 1308, loss 0.206985, acc 0.9375, prec 0.0423942, recall 0.825563
2017-12-10T15:27:36.406926: step 1309, loss 0.192264, acc 0.921875, prec 0.042425, recall 0.825703
2017-12-10T15:27:36.607072: step 1310, loss 0.261998, acc 0.921875, prec 0.0424162, recall 0.825703
2017-12-10T15:27:36.809315: step 1311, loss 0.213737, acc 0.90625, prec 0.0424057, recall 0.825703
2017-12-10T15:27:37.006743: step 1312, loss 0.450778, acc 0.796875, prec 0.042383, recall 0.825703
2017-12-10T15:27:37.201364: step 1313, loss 0.315706, acc 0.84375, prec 0.0423655, recall 0.825703
2017-12-10T15:27:37.398662: step 1314, loss 0.12416, acc 0.953125, prec 0.0423603, recall 0.825703
2017-12-10T15:27:37.597753: step 1315, loss 0.177291, acc 0.953125, prec 0.0423945, recall 0.825843
2017-12-10T15:27:37.795858: step 1316, loss 0.431377, acc 0.90625, prec 0.0424235, recall 0.825982
2017-12-10T15:27:37.990642: step 1317, loss 0.202536, acc 0.921875, prec 0.0424148, recall 0.825982
2017-12-10T15:27:38.190589: step 1318, loss 0.216682, acc 0.90625, prec 0.0424437, recall 0.826122
2017-12-10T15:27:38.386752: step 1319, loss 2.70583, acc 0.84375, prec 0.042428, recall 0.82546
2017-12-10T15:27:38.584252: step 1320, loss 1.68019, acc 0.921875, prec 0.0424998, recall 0.82508
2017-12-10T15:27:38.786603: step 1321, loss 0.276394, acc 0.890625, prec 0.0424876, recall 0.82508
2017-12-10T15:27:38.982908: step 1322, loss 0.927309, acc 0.875, prec 0.0425129, recall 0.825219
2017-12-10T15:27:39.178913: step 1323, loss 0.267881, acc 0.921875, prec 0.0425436, recall 0.825359
2017-12-10T15:27:39.377845: step 1324, loss 0.300969, acc 0.890625, prec 0.04261, recall 0.825637
2017-12-10T15:27:39.572592: step 1325, loss 0.39058, acc 0.828125, prec 0.0425908, recall 0.825637
2017-12-10T15:27:39.763499: step 1326, loss 0.273809, acc 0.828125, prec 0.0426108, recall 0.825776
2017-12-10T15:27:39.959046: step 1327, loss 0.535539, acc 0.796875, prec 0.0426274, recall 0.825914
2017-12-10T15:27:40.150916: step 1328, loss 0.545619, acc 0.828125, prec 0.0426082, recall 0.825914
2017-12-10T15:27:40.348136: step 1329, loss 0.791154, acc 0.921875, prec 0.0426779, recall 0.82619
2017-12-10T15:27:40.539921: step 1330, loss 0.617732, acc 0.8125, prec 0.0426569, recall 0.82619
2017-12-10T15:27:40.734685: step 1331, loss 0.272453, acc 0.890625, prec 0.0426447, recall 0.82619
2017-12-10T15:27:40.925352: step 1332, loss 0.39557, acc 0.859375, prec 0.0427074, recall 0.826466
2017-12-10T15:27:41.120772: step 1333, loss 1.07132, acc 0.84375, prec 0.0427683, recall 0.826741
2017-12-10T15:27:41.321279: step 1334, loss 0.74918, acc 0.796875, prec 0.0427847, recall 0.826877
2017-12-10T15:27:41.521624: step 1335, loss 2.31872, acc 0.78125, prec 0.0428011, recall 0.826361
2017-12-10T15:27:41.720114: step 1336, loss 0.579162, acc 0.84375, prec 0.0427836, recall 0.826361
2017-12-10T15:27:41.911344: step 1337, loss 1.00097, acc 0.796875, prec 0.0428391, recall 0.826635
2017-12-10T15:27:42.102928: step 1338, loss 0.37187, acc 0.78125, prec 0.0428146, recall 0.826635
2017-12-10T15:27:42.297952: step 1339, loss 0.454736, acc 0.84375, prec 0.0428752, recall 0.826908
2017-12-10T15:27:42.492699: step 1340, loss 0.413913, acc 0.859375, prec 0.0428595, recall 0.826908
2017-12-10T15:27:42.686281: step 1341, loss 0.641758, acc 0.796875, prec 0.0428368, recall 0.826908
2017-12-10T15:27:42.883689: step 1342, loss 0.447393, acc 0.859375, prec 0.0428211, recall 0.826908
2017-12-10T15:27:43.077637: step 1343, loss 0.408022, acc 0.875, prec 0.0428461, recall 0.827044
2017-12-10T15:27:43.270886: step 1344, loss 1.82301, acc 0.765625, prec 0.0428996, recall 0.826667
2017-12-10T15:27:43.467840: step 1345, loss 2.28623, acc 0.8125, prec 0.0429193, recall 0.826155
2017-12-10T15:27:43.667853: step 1346, loss 0.622804, acc 0.8125, prec 0.0428984, recall 0.826155
2017-12-10T15:27:43.860489: step 1347, loss 0.636101, acc 0.8125, prec 0.0429164, recall 0.826291
2017-12-10T15:27:44.060886: step 1348, loss 0.506938, acc 0.84375, prec 0.0428989, recall 0.826291
2017-12-10T15:27:44.262019: step 1349, loss 0.435206, acc 0.84375, prec 0.0429204, recall 0.826427
2017-12-10T15:27:44.456106: step 1350, loss 1.03621, acc 0.671875, prec 0.0428838, recall 0.826427
2017-12-10T15:27:44.653497: step 1351, loss 0.592011, acc 0.75, prec 0.042856, recall 0.826427
2017-12-10T15:27:44.846910: step 1352, loss 0.912299, acc 0.765625, prec 0.0429075, recall 0.826698
2017-12-10T15:27:45.042313: step 1353, loss 0.613505, acc 0.796875, prec 0.0428849, recall 0.826698
2017-12-10T15:27:45.236565: step 1354, loss 0.693619, acc 0.75, prec 0.0428571, recall 0.826698
2017-12-10T15:27:45.427540: step 1355, loss 0.698472, acc 0.828125, prec 0.0429542, recall 0.827103
2017-12-10T15:27:45.619508: step 1356, loss 0.500139, acc 0.8125, prec 0.0430108, recall 0.827372
2017-12-10T15:27:45.815328: step 1357, loss 0.170375, acc 0.9375, prec 0.0430038, recall 0.827372
2017-12-10T15:27:46.008148: step 1358, loss 0.325921, acc 0.796875, prec 0.0429812, recall 0.827372
2017-12-10T15:27:46.204534: step 1359, loss 0.269707, acc 0.9375, prec 0.0430129, recall 0.827506
2017-12-10T15:27:46.400110: step 1360, loss 5.61747, acc 0.890625, prec 0.0430411, recall 0.826998
2017-12-10T15:27:46.597448: step 1361, loss 0.212913, acc 0.9375, prec 0.0430342, recall 0.826998
2017-12-10T15:27:46.791528: step 1362, loss 0.282899, acc 0.859375, prec 0.0430186, recall 0.826998
2017-12-10T15:27:46.982261: step 1363, loss 0.144019, acc 0.953125, prec 0.0430134, recall 0.826998
2017-12-10T15:27:47.174385: step 1364, loss 0.510429, acc 0.875, prec 0.0430381, recall 0.827132
2017-12-10T15:27:47.368153: step 1365, loss 0.216466, acc 0.921875, prec 0.0430294, recall 0.827132
2017-12-10T15:27:47.566647: step 1366, loss 1.10288, acc 0.828125, prec 0.0430489, recall 0.827266
2017-12-10T15:27:47.763328: step 1367, loss 0.355943, acc 0.875, prec 0.043035, recall 0.827266
2017-12-10T15:27:47.960726: step 1368, loss 0.330406, acc 0.90625, prec 0.0430632, recall 0.827399
2017-12-10T15:27:48.153412: step 1369, loss 0.290258, acc 0.90625, prec 0.0430528, recall 0.827399
2017-12-10T15:27:48.351501: step 1370, loss 0.4677, acc 0.875, prec 0.0431159, recall 0.827666
2017-12-10T15:27:48.544204: step 1371, loss 0.788991, acc 0.875, prec 0.0432561, recall 0.828197
2017-12-10T15:27:48.741146: step 1372, loss 0.155652, acc 0.90625, prec 0.0432456, recall 0.828197
2017-12-10T15:27:48.937755: step 1373, loss 0.47903, acc 0.875, prec 0.0432702, recall 0.82833
2017-12-10T15:27:49.130018: step 1374, loss 5.51516, acc 0.84375, prec 0.043293, recall 0.827825
2017-12-10T15:27:49.331526: step 1375, loss 0.452856, acc 0.921875, prec 0.0433227, recall 0.827957
2017-12-10T15:27:49.528046: step 1376, loss 1.04142, acc 0.859375, prec 0.0434224, recall 0.828353
2017-12-10T15:27:49.724423: step 1377, loss 0.481979, acc 0.84375, prec 0.0434049, recall 0.828353
2017-12-10T15:27:49.921741: step 1378, loss 0.269979, acc 0.890625, prec 0.0433927, recall 0.828353
2017-12-10T15:27:50.118406: step 1379, loss 0.390895, acc 0.859375, prec 0.0433771, recall 0.828353
2017-12-10T15:27:50.311073: step 1380, loss 0.398214, acc 0.859375, prec 0.0433614, recall 0.828353
2017-12-10T15:27:50.502199: step 1381, loss 0.757434, acc 0.796875, prec 0.0434155, recall 0.828615
2017-12-10T15:27:50.698208: step 1382, loss 0.545436, acc 0.734375, prec 0.0434243, recall 0.828746
2017-12-10T15:27:50.891944: step 1383, loss 0.493174, acc 0.765625, prec 0.0433982, recall 0.828746
2017-12-10T15:27:51.092512: step 1384, loss 0.730821, acc 0.828125, prec 0.0434556, recall 0.829008
2017-12-10T15:27:51.289488: step 1385, loss 0.45376, acc 0.8125, prec 0.0434348, recall 0.829008
2017-12-10T15:27:51.486438: step 1386, loss 0.469938, acc 0.84375, prec 0.0434557, recall 0.829138
2017-12-10T15:27:51.679950: step 1387, loss 0.387103, acc 0.8125, prec 0.043473, recall 0.829268
2017-12-10T15:27:51.878516: step 1388, loss 0.549201, acc 0.828125, prec 0.0435304, recall 0.829528
2017-12-10T15:27:52.075061: step 1389, loss 0.515592, acc 0.78125, prec 0.043506, recall 0.829528
2017-12-10T15:27:52.270919: step 1390, loss 0.418699, acc 0.84375, prec 0.043565, recall 0.829787
2017-12-10T15:27:52.464063: step 1391, loss 0.401721, acc 0.828125, prec 0.0435459, recall 0.829787
2017-12-10T15:27:52.655521: step 1392, loss 0.664066, acc 0.890625, prec 0.0435719, recall 0.829916
2017-12-10T15:27:52.850617: step 1393, loss 0.997001, acc 0.859375, prec 0.0436325, recall 0.830174
2017-12-10T15:27:53.047765: step 1394, loss 0.671025, acc 0.90625, prec 0.0436982, recall 0.830432
2017-12-10T15:27:53.245266: step 1395, loss 0.237565, acc 0.90625, prec 0.0436878, recall 0.830432
2017-12-10T15:27:53.438805: step 1396, loss 2.87456, acc 0.90625, prec 0.0436791, recall 0.829803
2017-12-10T15:27:53.636771: step 1397, loss 0.371577, acc 0.859375, prec 0.0437396, recall 0.83006
2017-12-10T15:27:53.832749: step 1398, loss 0.649002, acc 0.796875, prec 0.043755, recall 0.830189
2017-12-10T15:27:54.025893: step 1399, loss 0.352765, acc 0.890625, prec 0.0437428, recall 0.830189
2017-12-10T15:27:54.219149: step 1400, loss 0.447791, acc 0.828125, prec 0.0437237, recall 0.830189
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_2/1512937383/checkpoints/model-1400

2017-12-10T15:27:55.376349: step 1401, loss 0.504889, acc 0.78125, prec 0.0437373, recall 0.830317
2017-12-10T15:27:55.579915: step 1402, loss 0.380652, acc 0.875, prec 0.0437614, recall 0.830445
2017-12-10T15:27:55.778554: step 1403, loss 1.68475, acc 0.78125, prec 0.043813, recall 0.8307
2017-12-10T15:27:55.972411: step 1404, loss 0.302464, acc 0.921875, prec 0.0438802, recall 0.830954
2017-12-10T15:27:56.169796: step 1405, loss 0.409653, acc 0.84375, prec 0.0439007, recall 0.831081
2017-12-10T15:27:56.368567: step 1406, loss 0.556232, acc 0.78125, prec 0.0438763, recall 0.831081
2017-12-10T15:27:56.567076: step 1407, loss 0.487735, acc 0.875, prec 0.0439382, recall 0.831334
2017-12-10T15:27:56.761204: step 1408, loss 0.358896, acc 0.890625, prec 0.0439639, recall 0.831461
2017-12-10T15:27:56.961423: step 1409, loss 0.515205, acc 0.84375, prec 0.0439843, recall 0.831587
2017-12-10T15:27:57.159998: step 1410, loss 0.537307, acc 0.828125, prec 0.0439652, recall 0.831587
2017-12-10T15:27:57.352543: step 1411, loss 0.309186, acc 0.921875, prec 0.0439943, recall 0.831713
2017-12-10T15:27:57.548197: step 1412, loss 0.394899, acc 0.859375, prec 0.0440543, recall 0.831964
2017-12-10T15:27:57.743039: step 1413, loss 0.22528, acc 0.875, prec 0.0440403, recall 0.831964
2017-12-10T15:27:57.939978: step 1414, loss 0.230604, acc 0.921875, prec 0.0440694, recall 0.83209
2017-12-10T15:27:58.134432: step 1415, loss 0.46402, acc 0.828125, prec 0.044088, recall 0.832215
2017-12-10T15:27:58.328280: step 1416, loss 2.25624, acc 0.8125, prec 0.0440689, recall 0.831595
2017-12-10T15:27:58.527592: step 1417, loss 0.639319, acc 0.890625, prec 0.0442076, recall 0.832095
2017-12-10T15:27:58.720132: step 1418, loss 0.824743, acc 0.921875, prec 0.0442366, recall 0.83222
2017-12-10T15:27:58.915132: step 1419, loss 0.516984, acc 0.84375, prec 0.0442946, recall 0.832469
2017-12-10T15:27:59.111188: step 1420, loss 0.340926, acc 0.84375, prec 0.0443148, recall 0.832593
2017-12-10T15:27:59.309223: step 1421, loss 0.283614, acc 0.921875, prec 0.044306, recall 0.832593
2017-12-10T15:27:59.502148: step 1422, loss 1.81719, acc 0.90625, prec 0.0442973, recall 0.831976
2017-12-10T15:27:59.699779: step 1423, loss 0.382088, acc 0.875, prec 0.044321, recall 0.832101
2017-12-10T15:27:59.892900: step 1424, loss 0.430016, acc 0.859375, prec 0.0443053, recall 0.832101
2017-12-10T15:28:00.094060: step 1425, loss 0.471813, acc 0.890625, prec 0.0443683, recall 0.832349
2017-12-10T15:28:00.288192: step 1426, loss 0.471695, acc 0.859375, prec 0.0443526, recall 0.832349
2017-12-10T15:28:00.485048: step 1427, loss 1.28899, acc 0.921875, prec 0.0444191, recall 0.832596
2017-12-10T15:28:00.679010: step 1428, loss 0.350318, acc 0.859375, prec 0.0444034, recall 0.832596
2017-12-10T15:28:00.879722: step 1429, loss 0.916898, acc 0.734375, prec 0.0444113, recall 0.832719
2017-12-10T15:28:01.074000: step 1430, loss 0.519811, acc 0.859375, prec 0.0444331, recall 0.832842
2017-12-10T15:28:01.276570: step 1431, loss 0.404237, acc 0.859375, prec 0.0444174, recall 0.832842
2017-12-10T15:28:01.477438: step 1432, loss 0.185407, acc 0.90625, prec 0.0444069, recall 0.832842
2017-12-10T15:28:01.682302: step 1433, loss 0.535526, acc 0.875, prec 0.0444305, recall 0.832965
2017-12-10T15:28:01.877610: step 1434, loss 1.33682, acc 0.828125, prec 0.0444488, recall 0.833088
2017-12-10T15:28:02.072908: step 1435, loss 1.02248, acc 0.828125, prec 0.0444671, recall 0.833211
2017-12-10T15:28:02.264384: step 1436, loss 2.25999, acc 0.890625, prec 0.0444941, recall 0.832722
2017-12-10T15:28:02.459565: step 1437, loss 0.589007, acc 0.8125, prec 0.0444732, recall 0.832722
2017-12-10T15:28:02.656065: step 1438, loss 0.4965, acc 0.8125, prec 0.0444897, recall 0.832845
2017-12-10T15:28:02.854207: step 1439, loss 0.679324, acc 0.734375, prec 0.0444975, recall 0.832967
2017-12-10T15:28:03.051450: step 1440, loss 1.06197, acc 0.6875, prec 0.0444627, recall 0.832967
2017-12-10T15:28:03.242696: step 1441, loss 0.482978, acc 0.828125, prec 0.0444809, recall 0.833089
2017-12-10T15:28:03.436368: step 1442, loss 0.921442, acc 0.75, prec 0.0444904, recall 0.833211
2017-12-10T15:28:03.633659: step 1443, loss 0.671101, acc 0.71875, prec 0.0444965, recall 0.833333
2017-12-10T15:28:03.827197: step 1444, loss 0.518533, acc 0.765625, prec 0.0444705, recall 0.833333
2017-12-10T15:28:04.023430: step 1445, loss 0.720571, acc 0.71875, prec 0.0444392, recall 0.833333
2017-12-10T15:28:04.217585: step 1446, loss 0.40177, acc 0.875, prec 0.0444254, recall 0.833333
2017-12-10T15:28:04.414620: step 1447, loss 0.491535, acc 0.84375, prec 0.0444081, recall 0.833333
2017-12-10T15:28:04.608033: step 1448, loss 1.36545, acc 0.78125, prec 0.0444583, recall 0.833577
2017-12-10T15:28:04.799604: step 1449, loss 0.464093, acc 0.890625, prec 0.0444834, recall 0.833698
2017-12-10T15:28:04.996091: step 1450, loss 0.874368, acc 0.8125, prec 0.0445369, recall 0.83394
2017-12-10T15:28:05.191791: step 1451, loss 0.661316, acc 0.796875, prec 0.0445887, recall 0.834182
2017-12-10T15:28:05.387768: step 1452, loss 0.606552, acc 0.84375, prec 0.0446085, recall 0.834302
2017-12-10T15:28:05.581274: step 1453, loss 0.426485, acc 0.828125, prec 0.0445895, recall 0.834302
2017-12-10T15:28:05.772334: step 1454, loss 0.24101, acc 0.890625, prec 0.0446144, recall 0.834423
2017-12-10T15:28:05.971638: step 1455, loss 0.399923, acc 0.859375, prec 0.0446359, recall 0.834543
2017-12-10T15:28:06.164845: step 1456, loss 0.611862, acc 0.828125, prec 0.0446539, recall 0.834663
2017-12-10T15:28:06.357467: step 1457, loss 0.495017, acc 0.796875, prec 0.0446314, recall 0.834663
2017-12-10T15:28:06.554888: step 1458, loss 0.324398, acc 0.921875, prec 0.0446598, recall 0.834783
2017-12-10T15:28:06.746824: step 1459, loss 1.47156, acc 0.859375, prec 0.0446813, recall 0.834902
2017-12-10T15:28:06.954571: step 1460, loss 5.30521, acc 0.890625, prec 0.0446709, recall 0.834298
2017-12-10T15:28:07.152400: step 1461, loss 0.320807, acc 0.90625, prec 0.0446975, recall 0.834418
2017-12-10T15:28:07.348282: step 1462, loss 0.370794, acc 0.921875, prec 0.0446888, recall 0.834418
2017-12-10T15:28:07.542965: step 1463, loss 0.46965, acc 0.859375, prec 0.0447472, recall 0.834657
2017-12-10T15:28:07.743276: step 1464, loss 0.509847, acc 0.859375, prec 0.0447317, recall 0.834657
2017-12-10T15:28:07.933105: step 1465, loss 0.510817, acc 0.890625, prec 0.0447565, recall 0.834776
2017-12-10T15:28:08.130000: step 1466, loss 0.305863, acc 0.90625, prec 0.044783, recall 0.834895
2017-12-10T15:28:08.324992: step 1467, loss 0.520585, acc 0.8125, prec 0.0447623, recall 0.834895
2017-12-10T15:28:08.520980: step 1468, loss 4.90108, acc 0.921875, prec 0.0448292, recall 0.834532
2017-12-10T15:28:08.722788: step 1469, loss 0.398032, acc 0.859375, prec 0.0448136, recall 0.834532
2017-12-10T15:28:08.919487: step 1470, loss 0.521538, acc 0.8125, prec 0.0448297, recall 0.834651
2017-12-10T15:28:09.115475: step 1471, loss 5.35368, acc 0.75, prec 0.0449161, recall 0.833811
2017-12-10T15:28:09.308334: step 1472, loss 0.495553, acc 0.828125, prec 0.0449707, recall 0.834049
2017-12-10T15:28:09.498445: step 1473, loss 0.616453, acc 0.796875, prec 0.0449482, recall 0.834049
2017-12-10T15:28:09.696881: step 1474, loss 1.05201, acc 0.703125, prec 0.0449153, recall 0.834049
2017-12-10T15:28:09.892092: step 1475, loss 1.05132, acc 0.640625, prec 0.0449123, recall 0.834167
2017-12-10T15:28:10.087896: step 1476, loss 1.40844, acc 0.515625, prec 0.0448587, recall 0.834167
2017-12-10T15:28:10.285097: step 1477, loss 1.47476, acc 0.515625, prec 0.044842, recall 0.834286
2017-12-10T15:28:10.482425: step 1478, loss 1.52244, acc 0.484375, prec 0.0447853, recall 0.834286
2017-12-10T15:28:10.680905: step 1479, loss 0.857197, acc 0.75, prec 0.0447578, recall 0.834286
2017-12-10T15:28:10.874145: step 1480, loss 0.894435, acc 0.65625, prec 0.0447201, recall 0.834286
2017-12-10T15:28:11.068394: step 1481, loss 0.919303, acc 0.734375, prec 0.044691, recall 0.834286
2017-12-10T15:28:11.263287: step 1482, loss 1.00496, acc 0.734375, prec 0.0446985, recall 0.834404
2017-12-10T15:28:11.458908: step 1483, loss 0.971451, acc 0.6875, prec 0.0447008, recall 0.834522
2017-12-10T15:28:11.649840: step 1484, loss 0.514322, acc 0.796875, prec 0.0447151, recall 0.83464
2017-12-10T15:28:11.847024: step 1485, loss 0.627065, acc 0.734375, prec 0.0446861, recall 0.83464
2017-12-10T15:28:12.044694: step 1486, loss 1.02091, acc 0.703125, prec 0.0446902, recall 0.834758
2017-12-10T15:28:12.243361: step 1487, loss 2.65697, acc 0.84375, prec 0.0447113, recall 0.834282
2017-12-10T15:28:12.437196: step 1488, loss 0.451841, acc 0.8125, prec 0.0447272, recall 0.834399
2017-12-10T15:28:12.633874: step 1489, loss 0.584665, acc 0.796875, prec 0.0447051, recall 0.834399
2017-12-10T15:28:12.830137: step 1490, loss 0.483983, acc 0.84375, prec 0.0447608, recall 0.834634
2017-12-10T15:28:13.005986: step 1491, loss 0.51836, acc 0.769231, prec 0.0447767, recall 0.834752
2017-12-10T15:28:13.205406: step 1492, loss 0.310562, acc 0.890625, prec 0.0448011, recall 0.834869
2017-12-10T15:28:13.399697: step 1493, loss 0.376741, acc 0.84375, prec 0.0448567, recall 0.835103
2017-12-10T15:28:13.596297: step 1494, loss 0.30352, acc 0.890625, prec 0.0448811, recall 0.835219
2017-12-10T15:28:13.789008: step 1495, loss 0.248922, acc 0.890625, prec 0.0448691, recall 0.835219
2017-12-10T15:28:13.982762: step 1496, loss 1.81371, acc 0.84375, prec 0.0448901, recall 0.834746
2017-12-10T15:28:14.184527: step 1497, loss 0.543837, acc 0.828125, prec 0.0449076, recall 0.834862
2017-12-10T15:28:14.387682: step 1498, loss 0.0750479, acc 0.984375, prec 0.0449059, recall 0.834862
2017-12-10T15:28:14.581290: step 1499, loss 0.169984, acc 0.9375, prec 0.044899, recall 0.834862
2017-12-10T15:28:14.779137: step 1500, loss 0.151286, acc 0.953125, prec 0.0448939, recall 0.834862
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_2/1512937383/checkpoints/model-1500

2017-12-10T15:28:16.304370: step 1501, loss 0.323326, acc 0.875, prec 0.0449165, recall 0.834979
2017-12-10T15:28:16.499142: step 1502, loss 0.444024, acc 0.90625, prec 0.0449425, recall 0.835095
2017-12-10T15:28:16.698822: step 1503, loss 0.20624, acc 0.90625, prec 0.0449323, recall 0.835095
2017-12-10T15:28:16.895935: step 1504, loss 0.714753, acc 0.875, prec 0.0449549, recall 0.835211
2017-12-10T15:28:17.094154: step 1505, loss 0.254728, acc 0.9375, prec 0.0449843, recall 0.835327
2017-12-10T15:28:17.291999: step 1506, loss 0.241479, acc 0.890625, prec 0.0450085, recall 0.835443
2017-12-10T15:28:17.488621: step 1507, loss 1.34007, acc 0.875, prec 0.0449966, recall 0.834856
2017-12-10T15:28:17.687316: step 1508, loss 3.78748, acc 0.921875, prec 0.0450259, recall 0.834386
2017-12-10T15:28:17.889239: step 1509, loss 0.241495, acc 0.921875, prec 0.0450174, recall 0.834386
2017-12-10T15:28:18.082850: step 1510, loss 0.475921, acc 0.921875, prec 0.0450812, recall 0.834618
2017-12-10T15:28:18.279536: step 1511, loss 0.290779, acc 0.921875, prec 0.0451449, recall 0.83485
2017-12-10T15:28:18.477605: step 1512, loss 0.437345, acc 0.890625, prec 0.0452052, recall 0.83508
2017-12-10T15:28:18.674307: step 1513, loss 0.360415, acc 0.875, prec 0.0451915, recall 0.83508
2017-12-10T15:28:18.871381: step 1514, loss 0.416694, acc 0.859375, prec 0.0452484, recall 0.835311
2017-12-10T15:28:19.067345: step 1515, loss 0.230116, acc 0.90625, prec 0.0452742, recall 0.835425
2017-12-10T15:28:19.263282: step 1516, loss 0.382285, acc 0.859375, prec 0.0453309, recall 0.835655
2017-12-10T15:28:19.454603: step 1517, loss 0.933289, acc 0.96875, prec 0.0453996, recall 0.835883
2017-12-10T15:28:19.653065: step 1518, loss 0.511521, acc 0.84375, prec 0.0454185, recall 0.835997
2017-12-10T15:28:19.845960: step 1519, loss 1.17888, acc 0.921875, prec 0.0455901, recall 0.836565
2017-12-10T15:28:20.043006: step 1520, loss 0.426693, acc 0.84375, prec 0.0455729, recall 0.836565
2017-12-10T15:28:20.239132: step 1521, loss 0.660722, acc 0.84375, prec 0.0455557, recall 0.836565
2017-12-10T15:28:20.432254: step 1522, loss 0.713778, acc 0.828125, prec 0.0455728, recall 0.836678
2017-12-10T15:28:20.627213: step 1523, loss 0.751444, acc 0.78125, prec 0.0455487, recall 0.836678
2017-12-10T15:28:20.831363: step 1524, loss 0.837921, acc 0.796875, prec 0.0456702, recall 0.837129
2017-12-10T15:28:21.029729: step 1525, loss 0.794062, acc 0.75, prec 0.0457145, recall 0.837354
2017-12-10T15:28:21.229385: step 1526, loss 0.247772, acc 0.90625, prec 0.0457401, recall 0.837466
2017-12-10T15:28:21.424574: step 1527, loss 0.611585, acc 0.84375, prec 0.0457229, recall 0.837466
2017-12-10T15:28:21.616002: step 1528, loss 0.307934, acc 0.859375, prec 0.0457074, recall 0.837466
2017-12-10T15:28:21.809869: step 1529, loss 0.645298, acc 0.796875, prec 0.0456851, recall 0.837466
2017-12-10T15:28:22.005704: step 1530, loss 0.813489, acc 0.78125, prec 0.0456969, recall 0.837577
2017-12-10T15:28:22.207774: step 1531, loss 0.464967, acc 0.828125, prec 0.0457855, recall 0.837912
2017-12-10T15:28:22.399858: step 1532, loss 0.319835, acc 0.890625, prec 0.0457735, recall 0.837912
2017-12-10T15:28:22.591481: step 1533, loss 0.447477, acc 0.84375, prec 0.0457563, recall 0.837912
2017-12-10T15:28:22.789320: step 1534, loss 0.340231, acc 0.890625, prec 0.0457443, recall 0.837912
2017-12-10T15:28:22.988119: step 1535, loss 0.241085, acc 0.9375, prec 0.0457732, recall 0.838023
2017-12-10T15:28:23.182410: step 1536, loss 0.274657, acc 0.90625, prec 0.0457987, recall 0.838134
2017-12-10T15:28:23.380615: step 1537, loss 0.251688, acc 0.9375, prec 0.0458276, recall 0.838245
2017-12-10T15:28:23.584864: step 1538, loss 0.205791, acc 0.9375, prec 0.0458922, recall 0.838467
2017-12-10T15:28:23.782257: step 1539, loss 0.143291, acc 0.9375, prec 0.0458853, recall 0.838467
2017-12-10T15:28:23.974341: step 1540, loss 3.13442, acc 0.9375, prec 0.0458802, recall 0.837893
2017-12-10T15:28:24.172575: step 1541, loss 1.74399, acc 0.90625, prec 0.0459787, recall 0.837653
2017-12-10T15:28:24.365189: step 1542, loss 0.166467, acc 0.921875, prec 0.0459701, recall 0.837653
2017-12-10T15:28:24.563617: step 1543, loss 0.458812, acc 0.921875, prec 0.0460329, recall 0.837875
2017-12-10T15:28:24.760577: step 1544, loss 0.396502, acc 0.859375, prec 0.0460174, recall 0.837875
2017-12-10T15:28:24.958703: step 1545, loss 0.380186, acc 0.875, prec 0.0460393, recall 0.837985
2017-12-10T15:28:25.157475: step 1546, loss 0.281531, acc 0.90625, prec 0.046029, recall 0.837985
2017-12-10T15:28:25.352441: step 1547, loss 0.341232, acc 0.90625, prec 0.04609, recall 0.838205
2017-12-10T15:28:25.549843: step 1548, loss 0.475323, acc 0.828125, prec 0.0461067, recall 0.838315
2017-12-10T15:28:25.743949: step 1549, loss 1.30512, acc 0.90625, prec 0.0461676, recall 0.838535
2017-12-10T15:28:25.945144: step 1550, loss 2.03495, acc 0.8125, prec 0.0461843, recall 0.838076
2017-12-10T15:28:26.148284: step 1551, loss 0.43848, acc 0.921875, prec 0.0462469, recall 0.838295
2017-12-10T15:28:26.346498: step 1552, loss 0.378284, acc 0.875, prec 0.0463042, recall 0.838513
2017-12-10T15:28:26.546325: step 1553, loss 0.622782, acc 0.796875, prec 0.0463174, recall 0.838623
2017-12-10T15:28:26.742502: step 1554, loss 0.611893, acc 0.8125, prec 0.0463677, recall 0.83884
2017-12-10T15:28:26.939950: step 1555, loss 0.351871, acc 0.90625, prec 0.0464284, recall 0.839057
2017-12-10T15:28:27.134447: step 1556, loss 0.562673, acc 0.84375, prec 0.0464467, recall 0.839166
2017-12-10T15:28:27.330120: step 1557, loss 0.639892, acc 0.828125, prec 0.0464631, recall 0.839274
2017-12-10T15:28:27.526316: step 1558, loss 0.713814, acc 0.75, prec 0.0464355, recall 0.839274
2017-12-10T15:28:27.724927: step 1559, loss 0.705033, acc 0.828125, prec 0.0464519, recall 0.839382
2017-12-10T15:28:27.920215: step 1560, loss 0.533158, acc 0.828125, prec 0.0464684, recall 0.83949
2017-12-10T15:28:28.120979: step 1561, loss 0.539671, acc 0.78125, prec 0.0464442, recall 0.83949
2017-12-10T15:28:28.317352: step 1562, loss 0.327308, acc 0.890625, prec 0.0464322, recall 0.83949
2017-12-10T15:28:28.515016: step 1563, loss 0.41664, acc 0.828125, prec 0.0464132, recall 0.83949
2017-12-10T15:28:28.707830: step 1564, loss 0.650013, acc 0.796875, prec 0.0464262, recall 0.839597
2017-12-10T15:28:28.904769: step 1565, loss 0.262215, acc 0.921875, prec 0.046453, recall 0.839705
2017-12-10T15:28:29.101545: step 1566, loss 0.539582, acc 0.78125, prec 0.0464642, recall 0.839812
2017-12-10T15:28:29.305211: step 1567, loss 0.305826, acc 0.921875, prec 0.0464909, recall 0.83992
2017-12-10T15:28:29.499781: step 1568, loss 0.253331, acc 0.890625, prec 0.0465142, recall 0.840027
2017-12-10T15:28:29.689292: step 1569, loss 0.175674, acc 0.9375, prec 0.0465073, recall 0.840027
2017-12-10T15:28:29.883532: step 1570, loss 0.173557, acc 0.9375, prec 0.0465004, recall 0.840027
2017-12-10T15:28:30.076157: step 1571, loss 0.15185, acc 0.9375, prec 0.0464935, recall 0.840027
2017-12-10T15:28:30.269638: step 1572, loss 0.0792035, acc 0.984375, prec 0.0464918, recall 0.840027
2017-12-10T15:28:30.469114: step 1573, loss 0.649951, acc 0.921875, prec 0.0465185, recall 0.840134
2017-12-10T15:28:30.665062: step 1574, loss 10.0771, acc 0.921875, prec 0.0465822, recall 0.839786
2017-12-10T15:28:30.863750: step 1575, loss 0.11703, acc 0.953125, prec 0.0466124, recall 0.839893
2017-12-10T15:28:31.060353: step 1576, loss 0.298639, acc 0.921875, prec 0.0466037, recall 0.839893
2017-12-10T15:28:31.257331: step 1577, loss 0.461696, acc 0.921875, prec 0.0466657, recall 0.840107
2017-12-10T15:28:31.460376: step 1578, loss 0.368818, acc 0.90625, prec 0.0466553, recall 0.840107
2017-12-10T15:28:31.660144: step 1579, loss 0.273044, acc 0.875, prec 0.0466768, recall 0.840213
2017-12-10T15:28:31.856658: step 1580, loss 0.0493068, acc 0.984375, prec 0.0467103, recall 0.840319
2017-12-10T15:28:32.051661: step 1581, loss 0.374993, acc 0.875, prec 0.0466965, recall 0.840319
2017-12-10T15:28:32.247935: step 1582, loss 0.118541, acc 0.953125, prec 0.0467266, recall 0.840426
2017-12-10T15:28:32.445995: step 1583, loss 0.240713, acc 0.921875, prec 0.0467531, recall 0.840532
2017-12-10T15:28:32.641703: step 1584, loss 1.08556, acc 0.859375, prec 0.0468433, recall 0.840849
2017-12-10T15:28:32.840402: step 1585, loss 0.421692, acc 0.875, prec 0.0468646, recall 0.840954
2017-12-10T15:28:33.038337: step 1586, loss 0.456703, acc 0.875, prec 0.0469211, recall 0.841165
2017-12-10T15:28:33.231266: step 1587, loss 0.2313, acc 0.921875, prec 0.0469125, recall 0.841165
2017-12-10T15:28:33.432130: step 1588, loss 0.262504, acc 0.921875, prec 0.0469038, recall 0.841165
2017-12-10T15:28:33.624618: step 1589, loss 0.244155, acc 0.890625, prec 0.046962, recall 0.841375
2017-12-10T15:28:33.820051: step 1590, loss 0.276216, acc 0.859375, prec 0.0469465, recall 0.841375
2017-12-10T15:28:34.022480: step 1591, loss 0.321652, acc 0.9375, prec 0.0470098, recall 0.841584
2017-12-10T15:28:34.214760: step 1592, loss 0.221243, acc 0.90625, prec 0.0469994, recall 0.841584
2017-12-10T15:28:34.412417: step 1593, loss 0.383214, acc 0.859375, prec 0.0469838, recall 0.841584
2017-12-10T15:28:34.604052: step 1594, loss 0.232702, acc 0.9375, prec 0.047012, recall 0.841689
2017-12-10T15:28:34.794773: step 1595, loss 0.485884, acc 0.8125, prec 0.0470263, recall 0.841793
2017-12-10T15:28:34.988157: step 1596, loss 0.134802, acc 0.9375, prec 0.0470194, recall 0.841793
2017-12-10T15:28:35.183623: step 1597, loss 0.281263, acc 0.953125, prec 0.0470142, recall 0.841793
2017-12-10T15:28:35.378522: step 1598, loss 0.719393, acc 0.9375, prec 0.0470424, recall 0.841897
2017-12-10T15:28:35.575634: step 1599, loss 1.36636, acc 0.9375, prec 0.0470372, recall 0.841343
2017-12-10T15:28:35.773710: step 1600, loss 0.541955, acc 0.90625, prec 0.0470619, recall 0.841447
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_2/1512937383/checkpoints/model-1600

2017-12-10T15:28:37.005923: step 1601, loss 0.769442, acc 0.890625, prec 0.047225, recall 0.841967
2017-12-10T15:28:37.206070: step 1602, loss 0.653901, acc 0.796875, prec 0.0472374, recall 0.842071
2017-12-10T15:28:37.400707: step 1603, loss 0.294703, acc 0.859375, prec 0.0472218, recall 0.842071
2017-12-10T15:28:37.598309: step 1604, loss 0.310168, acc 0.921875, prec 0.0472481, recall 0.842174
2017-12-10T15:28:37.799461: step 1605, loss 0.572883, acc 0.859375, prec 0.0473025, recall 0.842381
2017-12-10T15:28:37.996921: step 1606, loss 0.543622, acc 0.796875, prec 0.0473149, recall 0.842484
2017-12-10T15:28:38.193431: step 1607, loss 0.48502, acc 0.859375, prec 0.0473342, recall 0.842587
2017-12-10T15:28:38.389978: step 1608, loss 0.255172, acc 0.9375, prec 0.0473622, recall 0.842689
2017-12-10T15:28:38.589238: step 1609, loss 2.56274, acc 0.90625, prec 0.0474234, recall 0.842345
2017-12-10T15:28:38.785277: step 1610, loss 0.639683, acc 0.859375, prec 0.0474427, recall 0.842448
2017-12-10T15:28:38.981115: step 1611, loss 0.429251, acc 0.875, prec 0.0475335, recall 0.842755
2017-12-10T15:28:39.175767: step 1612, loss 0.231942, acc 0.921875, prec 0.0475248, recall 0.842755
2017-12-10T15:28:39.373397: step 1613, loss 0.383557, acc 0.875, prec 0.0475109, recall 0.842755
2017-12-10T15:28:39.575296: step 1614, loss 0.791129, acc 0.765625, prec 0.0475197, recall 0.842857
2017-12-10T15:28:39.773888: step 1615, loss 0.426858, acc 0.859375, prec 0.047504, recall 0.842857
2017-12-10T15:28:39.963073: step 1616, loss 0.475263, acc 0.828125, prec 0.0474849, recall 0.842857
2017-12-10T15:28:40.156015: step 1617, loss 0.361086, acc 0.875, prec 0.0475059, recall 0.842959
2017-12-10T15:28:40.347341: step 1618, loss 0.490481, acc 0.78125, prec 0.0474815, recall 0.842959
2017-12-10T15:28:40.537863: step 1619, loss 0.399299, acc 0.859375, prec 0.0475355, recall 0.843163
2017-12-10T15:28:40.732981: step 1620, loss 0.293073, acc 0.890625, prec 0.047593, recall 0.843366
2017-12-10T15:28:40.924913: step 1621, loss 0.914109, acc 0.890625, prec 0.0476851, recall 0.843669
2017-12-10T15:28:41.123986: step 1622, loss 0.325785, acc 0.921875, prec 0.0477459, recall 0.843871
2017-12-10T15:28:41.319873: step 1623, loss 0.248076, acc 0.921875, prec 0.047772, recall 0.843972
2017-12-10T15:28:41.513828: step 1624, loss 0.307464, acc 0.84375, prec 0.0477893, recall 0.844072
2017-12-10T15:28:41.709257: step 1625, loss 0.489964, acc 0.84375, prec 0.0478066, recall 0.844173
2017-12-10T15:28:41.905313: step 1626, loss 0.205819, acc 0.953125, prec 0.0478014, recall 0.844173
2017-12-10T15:28:42.104506: step 1627, loss 0.363144, acc 0.875, prec 0.0477874, recall 0.844173
2017-12-10T15:28:42.305837: step 1628, loss 0.155768, acc 0.9375, prec 0.0477805, recall 0.844173
2017-12-10T15:28:42.498242: step 1629, loss 0.412983, acc 0.890625, prec 0.047803, recall 0.844273
2017-12-10T15:28:42.693749: step 1630, loss 0.842888, acc 0.8125, prec 0.0478167, recall 0.844373
2017-12-10T15:28:42.885498: step 1631, loss 0.363106, acc 0.90625, prec 0.047841, recall 0.844473
2017-12-10T15:28:43.075253: step 1632, loss 0.860536, acc 0.890625, prec 0.0478981, recall 0.844673
2017-12-10T15:28:43.272613: step 1633, loss 0.228083, acc 0.9375, prec 0.0478911, recall 0.844673
2017-12-10T15:28:43.467873: step 1634, loss 0.2294, acc 0.984375, prec 0.0480279, recall 0.84507
2017-12-10T15:28:43.660888: step 1635, loss 0.115484, acc 0.953125, prec 0.0480227, recall 0.84507
2017-12-10T15:28:43.851424: step 1636, loss 0.219109, acc 0.921875, prec 0.048014, recall 0.84507
2017-12-10T15:28:44.042904: step 1637, loss 0.234756, acc 0.890625, prec 0.0480017, recall 0.84507
2017-12-10T15:28:44.243623: step 1638, loss 0.205672, acc 0.96875, prec 0.0480329, recall 0.84517
2017-12-10T15:28:44.446456: step 1639, loss 0.980171, acc 0.96875, prec 0.0481332, recall 0.845466
2017-12-10T15:28:44.647594: step 1640, loss 0.287017, acc 0.90625, prec 0.0481227, recall 0.845466
2017-12-10T15:28:44.840263: step 1641, loss 0.0575073, acc 0.984375, prec 0.048121, recall 0.845466
2017-12-10T15:28:45.039712: step 1642, loss 0.42313, acc 0.96875, prec 0.0481521, recall 0.845565
2017-12-10T15:28:45.242917: step 1643, loss 0.185192, acc 0.921875, prec 0.0481433, recall 0.845565
2017-12-10T15:28:45.442399: step 1644, loss 0.569199, acc 0.859375, prec 0.0481621, recall 0.845663
2017-12-10T15:28:45.639842: step 1645, loss 0.122535, acc 0.953125, prec 0.0481915, recall 0.845762
2017-12-10T15:28:45.834265: step 1646, loss 0.112248, acc 0.953125, prec 0.0481862, recall 0.845762
2017-12-10T15:28:46.031553: step 1647, loss 0.0930702, acc 0.96875, prec 0.0481827, recall 0.845762
2017-12-10T15:28:46.227408: step 1648, loss 0.158454, acc 0.953125, prec 0.048212, recall 0.84586
2017-12-10T15:28:46.422630: step 1649, loss 0.190862, acc 0.9375, prec 0.048205, recall 0.84586
2017-12-10T15:28:46.617107: step 1650, loss 0.0437136, acc 0.984375, prec 0.0482033, recall 0.84586
2017-12-10T15:28:46.815020: step 1651, loss 0.159488, acc 1, prec 0.0482378, recall 0.845958
2017-12-10T15:28:47.013738: step 1652, loss 0.17002, acc 0.90625, prec 0.0482273, recall 0.845958
2017-12-10T15:28:47.208467: step 1653, loss 0.116632, acc 0.953125, prec 0.0482221, recall 0.845958
2017-12-10T15:28:47.399794: step 1654, loss 0.344841, acc 0.875, prec 0.0482081, recall 0.845958
2017-12-10T15:28:47.602461: step 1655, loss 0.38473, acc 0.984375, prec 0.0482408, recall 0.846056
2017-12-10T15:28:47.801491: step 1656, loss 0.325736, acc 0.953125, prec 0.0483046, recall 0.846252
2017-12-10T15:28:48.000402: step 1657, loss 0.533397, acc 0.984375, prec 0.0483374, recall 0.846349
2017-12-10T15:28:48.205985: step 1658, loss 0.105031, acc 0.953125, prec 0.0483321, recall 0.846349
2017-12-10T15:28:48.403811: step 1659, loss 0.123832, acc 0.953125, prec 0.0483614, recall 0.846447
2017-12-10T15:28:48.599930: step 1660, loss 0.0822974, acc 0.984375, prec 0.0483596, recall 0.846447
2017-12-10T15:28:48.792277: step 1661, loss 0.157329, acc 0.921875, prec 0.0483509, recall 0.846447
2017-12-10T15:28:48.987200: step 1662, loss 0.142433, acc 0.9375, prec 0.0483783, recall 0.846544
2017-12-10T15:28:49.179471: step 1663, loss 0.32004, acc 0.9375, prec 0.0484058, recall 0.846641
2017-12-10T15:28:49.381263: step 1664, loss 0.210114, acc 0.9375, prec 0.0484677, recall 0.846835
2017-12-10T15:28:49.574796: step 1665, loss 0.194814, acc 0.90625, prec 0.0484572, recall 0.846835
2017-12-10T15:28:49.769289: step 1666, loss 0.496049, acc 0.96875, prec 0.0485226, recall 0.847029
2017-12-10T15:28:49.966317: step 1667, loss 0.15913, acc 0.90625, prec 0.0485809, recall 0.847222
2017-12-10T15:28:50.164896: step 1668, loss 0.412556, acc 0.90625, prec 0.0486048, recall 0.847319
2017-12-10T15:28:50.365292: step 1669, loss 0.209777, acc 0.9375, prec 0.0486322, recall 0.847415
2017-12-10T15:28:50.559535: step 1670, loss 0.263221, acc 0.9375, prec 0.048694, recall 0.847607
2017-12-10T15:28:50.751943: step 1671, loss 2.3598, acc 0.921875, prec 0.0487214, recall 0.84717
2017-12-10T15:28:50.945200: step 1672, loss 0.296326, acc 0.890625, prec 0.048709, recall 0.84717
2017-12-10T15:28:51.138679: step 1673, loss 0.361057, acc 0.890625, prec 0.0487311, recall 0.847266
2017-12-10T15:28:51.331200: step 1674, loss 0.300777, acc 0.890625, prec 0.0487188, recall 0.847266
2017-12-10T15:28:51.528724: step 1675, loss 0.358983, acc 0.859375, prec 0.0487029, recall 0.847266
2017-12-10T15:28:51.727175: step 1676, loss 0.478934, acc 0.921875, prec 0.0487285, recall 0.847362
2017-12-10T15:28:51.926867: step 1677, loss 3.47952, acc 0.9375, prec 0.0487576, recall 0.846926
2017-12-10T15:28:52.125211: step 1678, loss 0.547534, acc 0.8125, prec 0.0487708, recall 0.847022
2017-12-10T15:28:52.324050: step 1679, loss 0.453753, acc 0.828125, prec 0.0487858, recall 0.847118
2017-12-10T15:28:52.521451: step 1680, loss 0.622932, acc 0.78125, prec 0.0487611, recall 0.847118
2017-12-10T15:28:52.720226: step 1681, loss 0.543274, acc 0.78125, prec 0.0488051, recall 0.847309
2017-12-10T15:28:52.915794: step 1682, loss 0.557213, acc 0.859375, prec 0.0488236, recall 0.847405
2017-12-10T15:28:53.113179: step 1683, loss 1.05869, acc 0.71875, prec 0.0488262, recall 0.8475
2017-12-10T15:28:53.306879: step 1684, loss 0.52591, acc 0.765625, prec 0.0487998, recall 0.8475
2017-12-10T15:28:53.506580: step 1685, loss 0.550249, acc 0.796875, prec 0.048777, recall 0.8475
2017-12-10T15:28:53.700001: step 1686, loss 0.956702, acc 0.703125, prec 0.0487437, recall 0.8475
2017-12-10T15:28:53.893675: step 1687, loss 0.75465, acc 0.734375, prec 0.0487139, recall 0.8475
2017-12-10T15:28:54.091290: step 1688, loss 0.694004, acc 0.75, prec 0.0487542, recall 0.84769
2017-12-10T15:28:54.285511: step 1689, loss 0.858104, acc 0.875, prec 0.0488085, recall 0.84788
2017-12-10T15:28:54.478777: step 1690, loss 0.360558, acc 0.84375, prec 0.048791, recall 0.84788
2017-12-10T15:28:54.681273: step 1691, loss 0.931547, acc 0.890625, prec 0.0488129, recall 0.847975
2017-12-10T15:28:54.877576: step 1692, loss 0.337182, acc 0.859375, prec 0.0487971, recall 0.847975
2017-12-10T15:28:55.072471: step 1693, loss 0.526538, acc 0.859375, prec 0.0488155, recall 0.84807
2017-12-10T15:28:55.268657: step 1694, loss 0.260213, acc 0.90625, prec 0.048805, recall 0.84807
2017-12-10T15:28:55.463710: step 1695, loss 0.313071, acc 0.859375, prec 0.0488233, recall 0.848164
2017-12-10T15:28:55.655653: step 1696, loss 0.446865, acc 0.890625, prec 0.0488451, recall 0.848259
2017-12-10T15:28:55.847856: step 1697, loss 2.05572, acc 0.828125, prec 0.0488617, recall 0.847826
2017-12-10T15:28:56.047134: step 1698, loss 0.535834, acc 0.828125, prec 0.0488425, recall 0.847826
2017-12-10T15:28:56.240458: step 1699, loss 0.840229, acc 0.78125, prec 0.048852, recall 0.847921
2017-12-10T15:28:56.438769: step 1700, loss 0.348313, acc 0.890625, prec 0.0488398, recall 0.847921
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_2/1512937383/checkpoints/model-1700

2017-12-10T15:28:57.670964: step 1701, loss 0.255995, acc 0.90625, prec 0.0488293, recall 0.847921
2017-12-10T15:28:57.873184: step 1702, loss 0.519641, acc 0.921875, prec 0.0488546, recall 0.848015
2017-12-10T15:28:58.072255: step 1703, loss 0.158854, acc 0.953125, prec 0.0488493, recall 0.848015
2017-12-10T15:28:58.269685: step 1704, loss 0.105439, acc 0.96875, prec 0.0488458, recall 0.848015
2017-12-10T15:28:58.464466: step 1705, loss 0.4767, acc 0.875, prec 0.0488998, recall 0.848203
2017-12-10T15:28:58.661525: step 1706, loss 0.0676371, acc 0.96875, prec 0.0489303, recall 0.848297
2017-12-10T15:28:58.855079: step 1707, loss 0.334975, acc 0.90625, prec 0.0489878, recall 0.848485
2017-12-10T15:28:59.054272: step 1708, loss 0.930272, acc 0.953125, prec 0.0490165, recall 0.848579
2017-12-10T15:28:59.260556: step 1709, loss 1.67777, acc 0.875, prec 0.0490042, recall 0.848054
2017-12-10T15:28:59.463751: step 1710, loss 0.306942, acc 0.921875, prec 0.0490633, recall 0.848242
2017-12-10T15:28:59.663736: step 1711, loss 0.331135, acc 0.921875, prec 0.0490546, recall 0.848242
2017-12-10T15:28:59.863997: step 1712, loss 0.181905, acc 0.953125, prec 0.0490493, recall 0.848242
2017-12-10T15:29:00.058312: step 1713, loss 0.284059, acc 0.90625, prec 0.0490728, recall 0.848335
2017-12-10T15:29:00.254665: step 1714, loss 0.458496, acc 0.828125, prec 0.0490874, recall 0.848429
2017-12-10T15:29:00.452581: step 1715, loss 0.65237, acc 0.828125, prec 0.0491021, recall 0.848522
2017-12-10T15:29:00.647513: step 1716, loss 0.214928, acc 0.890625, prec 0.0490898, recall 0.848522
2017-12-10T15:29:00.843017: step 1717, loss 0.450807, acc 0.890625, prec 0.0491453, recall 0.848709
2017-12-10T15:29:01.037515: step 1718, loss 0.597024, acc 0.78125, prec 0.0491885, recall 0.848894
2017-12-10T15:29:01.238087: step 1719, loss 0.410614, acc 0.859375, prec 0.0491727, recall 0.848894
2017-12-10T15:29:01.439284: step 1720, loss 0.379594, acc 0.875, prec 0.0491588, recall 0.848894
2017-12-10T15:29:01.641144: step 1721, loss 0.600682, acc 0.921875, prec 0.0491838, recall 0.848987
2017-12-10T15:29:01.841523: step 1722, loss 0.218622, acc 0.921875, prec 0.0492427, recall 0.849172
2017-12-10T15:29:02.040008: step 1723, loss 1.08793, acc 0.84375, prec 0.0492928, recall 0.849357
2017-12-10T15:29:02.237470: step 1724, loss 0.229864, acc 0.921875, prec 0.0493178, recall 0.849449
2017-12-10T15:29:02.430313: step 1725, loss 0.222194, acc 0.921875, prec 0.0493428, recall 0.849541
2017-12-10T15:29:02.632818: step 1726, loss 0.78399, acc 0.90625, prec 0.0493661, recall 0.849633
2017-12-10T15:29:02.825967: step 1727, loss 0.298078, acc 0.890625, prec 0.0493875, recall 0.849725
2017-12-10T15:29:03.023780: step 1728, loss 0.259994, acc 0.9375, prec 0.049448, recall 0.849908
2017-12-10T15:29:03.219980: step 1729, loss 0.224447, acc 0.90625, prec 0.0494712, recall 0.85
2017-12-10T15:29:03.412526: step 1730, loss 0.252379, acc 0.90625, prec 0.0494607, recall 0.85
2017-12-10T15:29:03.607143: step 1731, loss 0.195626, acc 0.921875, prec 0.0495193, recall 0.850183
2017-12-10T15:29:03.805056: step 1732, loss 0.193195, acc 0.921875, prec 0.0495106, recall 0.850183
2017-12-10T15:29:04.005400: step 1733, loss 0.31456, acc 0.890625, prec 0.0494983, recall 0.850183
2017-12-10T15:29:04.205978: step 1734, loss 0.863279, acc 0.953125, prec 0.0495604, recall 0.850365
2017-12-10T15:29:04.404081: step 1735, loss 0.285136, acc 0.921875, prec 0.0495853, recall 0.850456
2017-12-10T15:29:04.602998: step 1736, loss 0.33742, acc 0.875, prec 0.0495713, recall 0.850456
2017-12-10T15:29:04.798055: step 1737, loss 0.29331, acc 0.921875, prec 0.0495961, recall 0.850547
2017-12-10T15:29:04.993194: step 1738, loss 0.118649, acc 0.953125, prec 0.0496582, recall 0.850728
2017-12-10T15:29:05.188967: step 1739, loss 0.479989, acc 0.875, prec 0.0496441, recall 0.850728
2017-12-10T15:29:05.380269: step 1740, loss 0.304767, acc 0.90625, prec 0.0496336, recall 0.850728
2017-12-10T15:29:05.572782: step 1741, loss 0.149844, acc 0.953125, prec 0.0496283, recall 0.850728
2017-12-10T15:29:05.765442: step 1742, loss 0.331374, acc 0.890625, prec 0.049616, recall 0.850728
2017-12-10T15:29:05.958309: step 1743, loss 0.146519, acc 0.96875, prec 0.0496461, recall 0.850819
2017-12-10T15:29:06.151597: step 1744, loss 9.87617, acc 0.890625, prec 0.0496356, recall 0.850303
2017-12-10T15:29:06.351412: step 1745, loss 0.109753, acc 0.96875, prec 0.0496657, recall 0.850394
2017-12-10T15:29:06.544230: step 1746, loss 0.532914, acc 0.90625, prec 0.0497224, recall 0.850575
2017-12-10T15:29:06.744142: step 1747, loss 0.120132, acc 0.953125, prec 0.0497507, recall 0.850665
2017-12-10T15:29:06.938084: step 1748, loss 0.161205, acc 0.953125, prec 0.0497454, recall 0.850665
2017-12-10T15:29:07.135942: step 1749, loss 0.360036, acc 0.875, prec 0.0497314, recall 0.850665
2017-12-10T15:29:07.331933: step 1750, loss 1.0177, acc 0.921875, prec 0.0497897, recall 0.850845
2017-12-10T15:29:07.527156: step 1751, loss 0.294735, acc 0.921875, prec 0.0498145, recall 0.850935
2017-12-10T15:29:07.719805: step 1752, loss 0.322407, acc 0.84375, prec 0.0498305, recall 0.851025
2017-12-10T15:29:07.912901: step 1753, loss 0.397197, acc 0.890625, prec 0.0498517, recall 0.851115
2017-12-10T15:29:08.107591: step 1754, loss 1.94143, acc 0.90625, prec 0.0498429, recall 0.850602
2017-12-10T15:29:08.307749: step 1755, loss 0.995034, acc 0.8125, prec 0.0498889, recall 0.850782
2017-12-10T15:29:08.503220: step 1756, loss 1.41461, acc 0.875, prec 0.0499101, recall 0.850361
2017-12-10T15:29:08.699529: step 1757, loss 0.51601, acc 0.859375, prec 0.0499277, recall 0.85045
2017-12-10T15:29:08.893037: step 1758, loss 0.649151, acc 0.8125, prec 0.050007, recall 0.850719
2017-12-10T15:29:09.090349: step 1759, loss 0.76941, acc 0.765625, prec 0.0499806, recall 0.850719
2017-12-10T15:29:09.284935: step 1760, loss 0.767914, acc 0.765625, prec 0.0499542, recall 0.850719
2017-12-10T15:29:09.481869: step 1761, loss 0.542446, acc 0.796875, prec 0.0499314, recall 0.850719
2017-12-10T15:29:09.676804: step 1762, loss 0.752845, acc 0.765625, prec 0.049905, recall 0.850719
2017-12-10T15:29:09.872390: step 1763, loss 0.559551, acc 0.828125, prec 0.0498857, recall 0.850719
2017-12-10T15:29:10.063031: step 1764, loss 0.587909, acc 0.78125, prec 0.0498612, recall 0.850719
2017-12-10T15:29:10.252774: step 1765, loss 0.554761, acc 0.796875, prec 0.0498384, recall 0.850719
2017-12-10T15:29:10.446426: step 1766, loss 0.675659, acc 0.796875, prec 0.049849, recall 0.850809
2017-12-10T15:29:10.642947: step 1767, loss 0.588829, acc 0.765625, prec 0.0498561, recall 0.850898
2017-12-10T15:29:10.835072: step 1768, loss 0.486531, acc 0.84375, prec 0.0498387, recall 0.850898
2017-12-10T15:29:11.031448: step 1769, loss 0.337724, acc 0.828125, prec 0.0498194, recall 0.850898
2017-12-10T15:29:11.224225: step 1770, loss 0.500438, acc 0.796875, prec 0.0498633, recall 0.851077
2017-12-10T15:29:11.419131: step 1771, loss 0.526364, acc 0.875, prec 0.0498494, recall 0.851077
2017-12-10T15:29:11.617214: step 1772, loss 0.316128, acc 0.84375, prec 0.0498319, recall 0.851077
2017-12-10T15:29:11.813400: step 1773, loss 0.356927, acc 0.859375, prec 0.0498495, recall 0.851166
2017-12-10T15:29:12.006013: step 1774, loss 0.197564, acc 0.921875, prec 0.0498407, recall 0.851166
2017-12-10T15:29:12.199970: step 1775, loss 0.242387, acc 0.9375, prec 0.0498338, recall 0.851166
2017-12-10T15:29:12.393939: step 1776, loss 1.44812, acc 0.890625, prec 0.049888, recall 0.851343
2017-12-10T15:29:12.590393: step 1777, loss 0.916495, acc 0.921875, prec 0.0499458, recall 0.851521
2017-12-10T15:29:12.790259: step 1778, loss 0.19877, acc 0.9375, prec 0.049972, recall 0.851609
2017-12-10T15:29:12.988336: step 1779, loss 0.494538, acc 0.890625, prec 0.0500262, recall 0.851786
2017-12-10T15:29:13.188605: step 1780, loss 0.0690706, acc 0.96875, prec 0.0500227, recall 0.851786
2017-12-10T15:29:13.385428: step 1781, loss 0.0976513, acc 0.984375, prec 0.050021, recall 0.851786
2017-12-10T15:29:13.584821: step 1782, loss 0.328447, acc 0.875, prec 0.0500402, recall 0.851874
2017-12-10T15:29:13.785593: step 1783, loss 0.336025, acc 0.890625, prec 0.0500611, recall 0.851962
2017-12-10T15:29:13.984466: step 1784, loss 1.86096, acc 0.90625, prec 0.0500524, recall 0.851456
2017-12-10T15:29:14.188634: step 1785, loss 0.227185, acc 0.9375, prec 0.0500786, recall 0.851544
2017-12-10T15:29:14.389084: step 1786, loss 0.282886, acc 0.9375, prec 0.0500716, recall 0.851544
2017-12-10T15:29:14.588029: step 1787, loss 1.56388, acc 0.90625, prec 0.0501292, recall 0.851215
2017-12-10T15:29:14.789503: step 1788, loss 0.249256, acc 0.921875, prec 0.0501536, recall 0.851303
2017-12-10T15:29:14.987090: step 1789, loss 0.436525, acc 0.875, prec 0.0501396, recall 0.851303
2017-12-10T15:29:15.183087: step 1790, loss 0.409658, acc 0.875, prec 0.0501256, recall 0.851303
2017-12-10T15:29:15.375056: step 1791, loss 0.871368, acc 0.828125, prec 0.0501726, recall 0.851479
2017-12-10T15:29:15.572011: step 1792, loss 0.457865, acc 0.78125, prec 0.0501481, recall 0.851479
2017-12-10T15:29:15.768124: step 1793, loss 0.256869, acc 0.953125, prec 0.0501429, recall 0.851479
2017-12-10T15:29:15.961710: step 1794, loss 0.312029, acc 0.875, prec 0.050162, recall 0.851567
2017-12-10T15:29:16.162076: step 1795, loss 1.12655, acc 0.859375, prec 0.0502124, recall 0.851742
2017-12-10T15:29:16.358917: step 1796, loss 0.240034, acc 0.890625, prec 0.0502332, recall 0.85183
2017-12-10T15:29:16.559901: step 1797, loss 0.25839, acc 0.9375, prec 0.0502593, recall 0.851917
2017-12-10T15:29:16.752193: step 1798, loss 0.224389, acc 0.90625, prec 0.0502488, recall 0.851917
2017-12-10T15:29:16.949340: step 1799, loss 0.845868, acc 0.921875, prec 0.0502731, recall 0.852005
2017-12-10T15:29:17.147894: step 1800, loss 0.408907, acc 0.875, prec 0.0503252, recall 0.852179
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_2/1512937383/checkpoints/model-1800

2017-12-10T15:29:18.438247: step 1801, loss 0.472513, acc 0.828125, prec 0.050339, recall 0.852266
2017-12-10T15:29:18.635682: step 1802, loss 0.820614, acc 0.921875, prec 0.0504952, recall 0.8527
2017-12-10T15:29:18.828473: step 1803, loss 1.05993, acc 0.875, prec 0.0505142, recall 0.852786
2017-12-10T15:29:19.020097: step 1804, loss 0.259363, acc 0.921875, prec 0.0505384, recall 0.852872
2017-12-10T15:29:19.214033: step 1805, loss 0.327989, acc 0.859375, prec 0.0505226, recall 0.852872
2017-12-10T15:29:19.409047: step 1806, loss 0.349034, acc 0.875, prec 0.0505415, recall 0.852958
2017-12-10T15:29:19.605035: step 1807, loss 0.573346, acc 0.796875, prec 0.0505187, recall 0.852958
2017-12-10T15:29:19.804364: step 1808, loss 0.401112, acc 0.875, prec 0.0505706, recall 0.85313
2017-12-10T15:29:20.001289: step 1809, loss 0.37505, acc 0.828125, prec 0.0505842, recall 0.853216
2017-12-10T15:29:20.194375: step 1810, loss 0.443914, acc 0.8125, prec 0.0505632, recall 0.853216
2017-12-10T15:29:20.396251: step 1811, loss 0.741938, acc 0.765625, prec 0.0505369, recall 0.853216
2017-12-10T15:29:20.587382: step 1812, loss 0.351877, acc 0.90625, prec 0.0505264, recall 0.853216
2017-12-10T15:29:20.780389: step 1813, loss 0.301108, acc 0.875, prec 0.0505453, recall 0.853302
2017-12-10T15:29:20.981716: step 1814, loss 0.411875, acc 0.828125, prec 0.0505589, recall 0.853388
2017-12-10T15:29:21.176407: step 1815, loss 0.611017, acc 0.875, prec 0.0505777, recall 0.853473
2017-12-10T15:29:21.372084: step 1816, loss 0.807226, acc 0.890625, prec 0.0505983, recall 0.853559
2017-12-10T15:29:21.570862: step 1817, loss 0.54825, acc 0.859375, prec 0.0506154, recall 0.853644
2017-12-10T15:29:21.767059: step 1818, loss 0.764434, acc 0.9375, prec 0.050674, recall 0.853815
2017-12-10T15:29:21.967331: step 1819, loss 0.407979, acc 0.859375, prec 0.0506911, recall 0.8539
2017-12-10T15:29:22.163269: step 1820, loss 0.256665, acc 0.921875, prec 0.0507151, recall 0.853985
2017-12-10T15:29:22.359327: step 1821, loss 0.211487, acc 0.921875, prec 0.0507064, recall 0.853985
2017-12-10T15:29:22.556142: step 1822, loss 0.308801, acc 0.890625, prec 0.0506941, recall 0.853985
2017-12-10T15:29:22.756033: step 1823, loss 0.200985, acc 0.9375, prec 0.0506871, recall 0.853985
2017-12-10T15:29:22.950455: step 1824, loss 0.254827, acc 0.90625, prec 0.0506766, recall 0.853985
2017-12-10T15:29:23.146693: step 1825, loss 0.356922, acc 0.90625, prec 0.0506989, recall 0.85407
2017-12-10T15:29:23.349624: step 1826, loss 0.0870506, acc 0.96875, prec 0.0507281, recall 0.854155
2017-12-10T15:29:23.548446: step 1827, loss 0.290145, acc 0.90625, prec 0.0507504, recall 0.854239
2017-12-10T15:29:23.743098: step 1828, loss 0.521401, acc 0.921875, prec 0.0508399, recall 0.854493
2017-12-10T15:29:23.946192: step 1829, loss 0.0654263, acc 0.984375, prec 0.0508381, recall 0.854493
2017-12-10T15:29:24.142529: step 1830, loss 0.289333, acc 0.9375, prec 0.0508638, recall 0.854577
2017-12-10T15:29:24.339256: step 1831, loss 1.41944, acc 0.953125, prec 0.0509567, recall 0.854829
2017-12-10T15:29:24.537585: step 1832, loss 0.207075, acc 0.96875, prec 0.0509859, recall 0.854913
2017-12-10T15:29:24.731684: step 1833, loss 1.06434, acc 0.90625, prec 0.0510735, recall 0.855164
2017-12-10T15:29:24.934110: step 1834, loss 0.535298, acc 0.9375, prec 0.0510992, recall 0.855248
2017-12-10T15:29:25.131929: step 1835, loss 0.203404, acc 0.953125, prec 0.0511266, recall 0.855331
2017-12-10T15:29:25.328205: step 1836, loss 0.306927, acc 0.890625, prec 0.0511469, recall 0.855415
2017-12-10T15:29:25.525647: step 1837, loss 0.861547, acc 0.859375, prec 0.0511638, recall 0.855498
2017-12-10T15:29:25.725523: step 1838, loss 0.350282, acc 0.890625, prec 0.0511841, recall 0.855581
2017-12-10T15:29:25.920136: step 1839, loss 0.229355, acc 0.90625, prec 0.0512062, recall 0.855664
2017-12-10T15:29:26.116514: step 1840, loss 0.287151, acc 0.859375, prec 0.0511903, recall 0.855664
2017-12-10T15:29:26.316488: step 1841, loss 0.517391, acc 0.84375, prec 0.0512053, recall 0.855747
2017-12-10T15:29:26.515218: step 1842, loss 0.324629, acc 0.90625, prec 0.0512274, recall 0.85583
2017-12-10T15:29:26.712830: step 1843, loss 0.228326, acc 0.890625, prec 0.0512151, recall 0.85583
2017-12-10T15:29:26.906455: step 1844, loss 0.510364, acc 0.84375, prec 0.0512627, recall 0.855995
2017-12-10T15:29:27.099600: step 1845, loss 0.445093, acc 0.859375, prec 0.0512468, recall 0.855995
2017-12-10T15:29:27.293984: step 1846, loss 0.771822, acc 0.828125, prec 0.0512275, recall 0.855995
2017-12-10T15:29:27.487785: step 1847, loss 0.574697, acc 0.828125, prec 0.0512081, recall 0.855995
2017-12-10T15:29:27.677533: step 1848, loss 0.430178, acc 0.84375, prec 0.0512557, recall 0.85616
2017-12-10T15:29:27.872284: step 1849, loss 0.507116, acc 0.78125, prec 0.0512311, recall 0.85616
2017-12-10T15:29:28.068416: step 1850, loss 0.413833, acc 0.859375, prec 0.0512478, recall 0.856243
2017-12-10T15:29:28.265867: step 1851, loss 2.34839, acc 0.921875, prec 0.0512733, recall 0.855835
2017-12-10T15:29:28.466308: step 1852, loss 0.292823, acc 0.921875, prec 0.051297, recall 0.855918
2017-12-10T15:29:28.658786: step 1853, loss 0.234312, acc 0.921875, prec 0.0512882, recall 0.855918
2017-12-10T15:29:28.854883: step 1854, loss 0.629145, acc 0.859375, prec 0.0513374, recall 0.856082
2017-12-10T15:29:29.050306: step 1855, loss 0.370933, acc 0.875, prec 0.0513558, recall 0.856164
2017-12-10T15:29:29.252536: step 1856, loss 0.301023, acc 0.90625, prec 0.0513777, recall 0.856246
2017-12-10T15:29:29.453334: step 1857, loss 0.235031, acc 0.890625, prec 0.0513654, recall 0.856246
2017-12-10T15:29:29.652825: step 1858, loss 0.184265, acc 0.953125, prec 0.0513926, recall 0.856328
2017-12-10T15:29:29.850294: step 1859, loss 0.526346, acc 0.9375, prec 0.0514829, recall 0.856574
2017-12-10T15:29:30.047296: step 1860, loss 0.5159, acc 0.953125, prec 0.0515101, recall 0.856655
2017-12-10T15:29:30.247909: step 1861, loss 0.256511, acc 0.890625, prec 0.0515302, recall 0.856737
2017-12-10T15:29:30.443733: step 1862, loss 0.228044, acc 0.953125, prec 0.0515573, recall 0.856818
2017-12-10T15:29:30.640490: step 1863, loss 0.979476, acc 0.890625, prec 0.0515774, recall 0.856899
2017-12-10T15:29:30.833338: step 1864, loss 0.336805, acc 0.890625, prec 0.0515975, recall 0.856981
2017-12-10T15:29:31.028444: step 1865, loss 0.390137, acc 0.859375, prec 0.0515816, recall 0.856981
2017-12-10T15:29:31.228658: step 1866, loss 0.496045, acc 0.796875, prec 0.0515587, recall 0.856981
2017-12-10T15:29:31.427698: step 1867, loss 0.377367, acc 0.921875, prec 0.0515499, recall 0.856981
2017-12-10T15:29:31.636319: step 1868, loss 0.357281, acc 0.890625, prec 0.0515376, recall 0.856981
2017-12-10T15:29:31.832891: step 1869, loss 0.517033, acc 0.921875, prec 0.0516583, recall 0.857305
2017-12-10T15:29:32.028477: step 1870, loss 0.263473, acc 0.890625, prec 0.0516459, recall 0.857305
2017-12-10T15:29:32.228083: step 1871, loss 0.297604, acc 0.921875, prec 0.0516695, recall 0.857385
2017-12-10T15:29:32.426094: step 1872, loss 0.179838, acc 0.921875, prec 0.0516606, recall 0.857385
2017-12-10T15:29:32.621171: step 1873, loss 0.28221, acc 0.90625, prec 0.0516824, recall 0.857466
2017-12-10T15:29:32.818182: step 1874, loss 0.125434, acc 0.953125, prec 0.0517418, recall 0.857627
2017-12-10T15:29:33.023455: step 1875, loss 0.415099, acc 0.84375, prec 0.0517564, recall 0.857708
2017-12-10T15:29:33.215229: step 1876, loss 0.580694, acc 0.859375, prec 0.0517729, recall 0.857788
2017-12-10T15:29:33.415000: step 1877, loss 0.314711, acc 0.96875, prec 0.0518016, recall 0.857868
2017-12-10T15:29:33.610171: step 1878, loss 0.256566, acc 0.921875, prec 0.0517928, recall 0.857868
2017-12-10T15:29:33.804060: step 1879, loss 0.133407, acc 0.9375, prec 0.0518503, recall 0.858028
2017-12-10T15:29:34.001411: step 1880, loss 0.131507, acc 0.984375, prec 0.0518486, recall 0.858028
2017-12-10T15:29:34.200783: step 1881, loss 0.259656, acc 0.921875, prec 0.051872, recall 0.858108
2017-12-10T15:29:34.396806: step 1882, loss 0.262807, acc 0.9375, prec 0.0518972, recall 0.858188
2017-12-10T15:29:34.588012: step 1883, loss 0.0861914, acc 0.953125, prec 0.0518919, recall 0.858188
2017-12-10T15:29:34.784897: step 1884, loss 0.339092, acc 0.953125, prec 0.0519189, recall 0.858268
2017-12-10T15:29:34.983929: step 1885, loss 3.5674, acc 0.953125, prec 0.0519154, recall 0.857785
2017-12-10T15:29:35.178807: step 1886, loss 0.0870751, acc 0.953125, prec 0.0519101, recall 0.857785
2017-12-10T15:29:35.374277: step 1887, loss 0.46268, acc 0.984375, prec 0.052005, recall 0.858025
2017-12-10T15:29:35.574236: step 1888, loss 0.206388, acc 0.890625, prec 0.0519927, recall 0.858025
2017-12-10T15:29:35.773675: step 1889, loss 1.49604, acc 0.890625, prec 0.0520447, recall 0.858184
2017-12-10T15:29:35.968801: step 1890, loss 0.222773, acc 0.90625, prec 0.0520663, recall 0.858263
2017-12-10T15:29:36.164109: step 1891, loss 0.303312, acc 0.875, prec 0.052181, recall 0.85858
2017-12-10T15:29:36.356272: step 1892, loss 0.521866, acc 0.875, prec 0.0522312, recall 0.858738
2017-12-10T15:29:36.550436: step 1893, loss 0.510497, acc 0.84375, prec 0.0522135, recall 0.858738
2017-12-10T15:29:36.744464: step 1894, loss 0.729365, acc 0.890625, prec 0.0522654, recall 0.858896
2017-12-10T15:29:36.941174: step 1895, loss 0.699263, acc 0.84375, prec 0.052312, recall 0.859053
2017-12-10T15:29:37.135839: step 1896, loss 0.572405, acc 0.78125, prec 0.0523193, recall 0.859131
2017-12-10T15:29:37.332441: step 1897, loss 0.511534, acc 0.8125, prec 0.0523301, recall 0.85921
2017-12-10T15:29:37.526760: step 1898, loss 0.811627, acc 0.75, prec 0.0523017, recall 0.85921
2017-12-10T15:29:37.723593: step 1899, loss 0.76693, acc 0.75, prec 0.0522734, recall 0.85921
2017-12-10T15:29:37.919807: step 1900, loss 0.272107, acc 0.84375, prec 0.0523199, recall 0.859366
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_2/1512937383/checkpoints/model-1900

2017-12-10T15:29:39.187621: step 1901, loss 0.486675, acc 0.8125, prec 0.0523628, recall 0.859522
2017-12-10T15:29:39.385730: step 1902, loss 0.368988, acc 0.875, prec 0.0523486, recall 0.859522
2017-12-10T15:29:39.583382: step 1903, loss 0.36409, acc 0.859375, prec 0.0523967, recall 0.859678
2017-12-10T15:29:39.777994: step 1904, loss 0.612395, acc 0.859375, prec 0.0524448, recall 0.859834
2017-12-10T15:29:39.973269: step 1905, loss 0.378403, acc 0.84375, prec 0.0524591, recall 0.859911
2017-12-10T15:29:40.165323: step 1906, loss 0.458209, acc 0.875, prec 0.052477, recall 0.859989
2017-12-10T15:29:40.360371: step 1907, loss 0.168823, acc 0.921875, prec 0.0524681, recall 0.859989
2017-12-10T15:29:40.556679: step 1908, loss 0.622691, acc 0.875, prec 0.0525499, recall 0.860221
2017-12-10T15:29:40.754174: step 1909, loss 0.279344, acc 0.875, prec 0.0525357, recall 0.860221
2017-12-10T15:29:40.945184: step 1910, loss 0.549793, acc 0.84375, prec 0.0525499, recall 0.860298
2017-12-10T15:29:41.140926: step 1911, loss 0.102604, acc 0.9375, prec 0.0525748, recall 0.860375
2017-12-10T15:29:41.334856: step 1912, loss 0.202904, acc 0.875, prec 0.0525606, recall 0.860375
2017-12-10T15:29:41.529052: step 1913, loss 0.0918487, acc 0.96875, prec 0.0525571, recall 0.860375
2017-12-10T15:29:41.724308: step 1914, loss 0.297154, acc 0.90625, prec 0.0525464, recall 0.860375
2017-12-10T15:29:41.922822: step 1915, loss 0.12081, acc 0.96875, prec 0.0525429, recall 0.860375
2017-12-10T15:29:42.123740: step 1916, loss 7.4253, acc 0.859375, prec 0.0525943, recall 0.859581
2017-12-10T15:29:42.319611: step 1917, loss 0.144983, acc 0.96875, prec 0.0525908, recall 0.859581
2017-12-10T15:29:42.519578: step 1918, loss 0.29124, acc 0.890625, prec 0.0526422, recall 0.859736
2017-12-10T15:29:42.715979: step 1919, loss 0.265573, acc 0.921875, prec 0.0526334, recall 0.859736
2017-12-10T15:29:42.913581: step 1920, loss 0.6098, acc 0.859375, prec 0.0526493, recall 0.859813
2017-12-10T15:29:43.107531: step 1921, loss 0.882727, acc 0.84375, prec 0.0526635, recall 0.85989
2017-12-10T15:29:43.303245: step 1922, loss 0.272742, acc 0.875, prec 0.0526493, recall 0.85989
2017-12-10T15:29:43.497440: step 1923, loss 0.304718, acc 0.84375, prec 0.0526316, recall 0.85989
2017-12-10T15:29:43.693664: step 1924, loss 0.733755, acc 0.796875, prec 0.0526404, recall 0.859967
2017-12-10T15:29:43.892801: step 1925, loss 0.254404, acc 0.890625, prec 0.0526599, recall 0.860044
2017-12-10T15:29:44.089298: step 1926, loss 0.526686, acc 0.8125, prec 0.0526386, recall 0.860044
2017-12-10T15:29:44.291838: step 1927, loss 0.574732, acc 0.75, prec 0.0526422, recall 0.860121
2017-12-10T15:29:44.491539: step 1928, loss 0.536513, acc 0.859375, prec 0.0526899, recall 0.860274
2017-12-10T15:29:44.686973: step 1929, loss 0.138029, acc 0.953125, prec 0.0526846, recall 0.860274
2017-12-10T15:29:44.883019: step 1930, loss 0.46842, acc 0.8125, prec 0.0526634, recall 0.860274
2017-12-10T15:29:45.080743: step 1931, loss 0.336837, acc 0.921875, prec 0.0526545, recall 0.860274
2017-12-10T15:29:45.280204: step 1932, loss 0.268805, acc 0.890625, prec 0.0526422, recall 0.860274
2017-12-10T15:29:45.477142: step 1933, loss 0.497662, acc 0.8125, prec 0.0526845, recall 0.860427
2017-12-10T15:29:45.672449: step 1934, loss 0.323632, acc 0.90625, prec 0.0527056, recall 0.860503
2017-12-10T15:29:45.876247: step 1935, loss 0.625444, acc 0.8125, prec 0.0527162, recall 0.86058
2017-12-10T15:29:46.079157: step 1936, loss 0.337803, acc 0.921875, prec 0.0527391, recall 0.860656
2017-12-10T15:29:46.270295: step 1937, loss 0.612858, acc 0.84375, prec 0.0527531, recall 0.860732
2017-12-10T15:29:46.472189: step 1938, loss 0.697928, acc 0.921875, prec 0.0528394, recall 0.86096
2017-12-10T15:29:46.666978: step 1939, loss 0.334196, acc 0.890625, prec 0.052827, recall 0.86096
2017-12-10T15:29:46.865691: step 1940, loss 0.472712, acc 0.8125, prec 0.0528058, recall 0.86096
2017-12-10T15:29:47.065984: step 1941, loss 2.05333, acc 0.921875, prec 0.0528938, recall 0.860718
2017-12-10T15:29:47.263432: step 1942, loss 0.273762, acc 0.90625, prec 0.0528832, recall 0.860718
2017-12-10T15:29:47.460148: step 1943, loss 0.370293, acc 0.875, prec 0.0529323, recall 0.86087
2017-12-10T15:29:47.655837: step 1944, loss 1.77215, acc 0.859375, prec 0.0529182, recall 0.860402
2017-12-10T15:29:47.853235: step 1945, loss 0.367323, acc 0.859375, prec 0.0529023, recall 0.860402
2017-12-10T15:29:48.043137: step 1946, loss 0.312043, acc 0.890625, prec 0.0528899, recall 0.860402
2017-12-10T15:29:48.241320: step 1947, loss 0.463391, acc 0.9375, prec 0.0529461, recall 0.860553
2017-12-10T15:29:48.436214: step 1948, loss 0.399844, acc 0.828125, prec 0.0529583, recall 0.860629
2017-12-10T15:29:48.632124: step 1949, loss 0.206638, acc 0.890625, prec 0.0529459, recall 0.860629
2017-12-10T15:29:48.829843: step 1950, loss 0.381981, acc 0.859375, prec 0.05293, recall 0.860629
2017-12-10T15:29:49.027316: step 1951, loss 0.414037, acc 0.890625, prec 0.0529176, recall 0.860629
2017-12-10T15:29:49.222229: step 1952, loss 0.285093, acc 0.90625, prec 0.0529702, recall 0.86078
2017-12-10T15:29:49.418295: step 1953, loss 0.429287, acc 0.78125, prec 0.052977, recall 0.860855
2017-12-10T15:29:49.612189: step 1954, loss 0.548441, acc 0.859375, prec 0.0529927, recall 0.860931
2017-12-10T15:29:49.815098: step 1955, loss 0.243072, acc 0.921875, prec 0.0530154, recall 0.861006
2017-12-10T15:29:50.008310: step 1956, loss 1.03197, acc 0.890625, prec 0.0530346, recall 0.861081
2017-12-10T15:29:50.205801: step 1957, loss 0.664298, acc 0.875, prec 0.0530835, recall 0.861231
2017-12-10T15:29:50.401866: step 1958, loss 0.553248, acc 0.828125, prec 0.0530956, recall 0.861306
2017-12-10T15:29:50.597192: step 1959, loss 0.294897, acc 0.859375, prec 0.0530797, recall 0.861306
2017-12-10T15:29:50.797518: step 1960, loss 0.408746, acc 0.9375, prec 0.0531041, recall 0.861381
2017-12-10T15:29:50.995695: step 1961, loss 0.442632, acc 0.890625, prec 0.0531232, recall 0.861456
2017-12-10T15:29:51.187838: step 1962, loss 0.3454, acc 0.890625, prec 0.0531109, recall 0.861456
2017-12-10T15:29:51.389194: step 1963, loss 3.2165, acc 0.859375, prec 0.0531597, recall 0.861141
2017-12-10T15:29:51.584290: step 1964, loss 0.37599, acc 0.890625, prec 0.0532102, recall 0.86129
2017-12-10T15:29:51.777590: step 1965, loss 1.67328, acc 0.953125, prec 0.0532678, recall 0.861439
2017-12-10T15:29:51.980516: step 1966, loss 0.648451, acc 0.796875, prec 0.0532762, recall 0.861514
2017-12-10T15:29:52.173847: step 1967, loss 0.556272, acc 0.796875, prec 0.0532533, recall 0.861514
2017-12-10T15:29:52.367383: step 1968, loss 1.06546, acc 0.875, prec 0.0532705, recall 0.861588
2017-12-10T15:29:52.563324: step 1969, loss 0.364276, acc 0.90625, prec 0.0532599, recall 0.861588
2017-12-10T15:29:52.761526: step 1970, loss 0.374114, acc 0.84375, prec 0.0532737, recall 0.861662
2017-12-10T15:29:52.956616: step 1971, loss 0.758374, acc 0.796875, prec 0.0532821, recall 0.861736
2017-12-10T15:29:53.150730: step 1972, loss 0.621056, acc 0.78125, prec 0.0532574, recall 0.861736
2017-12-10T15:29:53.346632: step 1973, loss 0.651524, acc 0.796875, prec 0.0533285, recall 0.861958
2017-12-10T15:29:53.548306: step 1974, loss 0.375039, acc 0.921875, prec 0.0533197, recall 0.861958
2017-12-10T15:29:53.748174: step 1975, loss 0.578158, acc 0.765625, prec 0.0532932, recall 0.861958
2017-12-10T15:29:53.947586: step 1976, loss 0.672461, acc 0.796875, prec 0.0533642, recall 0.862179
2017-12-10T15:29:54.147732: step 1977, loss 0.134233, acc 0.953125, prec 0.0534215, recall 0.862327
2017-12-10T15:29:54.346964: step 1978, loss 0.569498, acc 0.796875, prec 0.0534298, recall 0.8624
2017-12-10T15:29:54.539391: step 1979, loss 0.229988, acc 0.9375, prec 0.0534228, recall 0.8624
2017-12-10T15:29:54.743343: step 1980, loss 0.457329, acc 0.828125, prec 0.0534346, recall 0.862473
2017-12-10T15:29:54.935867: step 1981, loss 0.19936, acc 0.9375, prec 0.0534276, recall 0.862473
2017-12-10T15:29:55.133444: step 1982, loss 0.281722, acc 0.90625, prec 0.053417, recall 0.862473
2017-12-10T15:29:55.326838: step 1983, loss 2.34965, acc 0.875, prec 0.0534046, recall 0.862014
2017-12-10T15:29:55.528102: step 1984, loss 0.864042, acc 0.921875, prec 0.0534271, recall 0.862087
2017-12-10T15:29:55.725483: step 1985, loss 0.385992, acc 0.90625, prec 0.0534165, recall 0.862087
2017-12-10T15:29:55.923101: step 1986, loss 0.298217, acc 0.90625, prec 0.0534371, recall 0.862161
2017-12-10T15:29:56.120220: step 1987, loss 0.132709, acc 0.953125, prec 0.0534631, recall 0.862234
2017-12-10T15:29:56.296786: step 1988, loss 0.431205, acc 0.865385, prec 0.0534507, recall 0.862234
2017-12-10T15:29:56.500005: step 1989, loss 0.296565, acc 0.90625, prec 0.0534401, recall 0.862234
2017-12-10T15:29:56.696027: step 1990, loss 0.214703, acc 0.859375, prec 0.0534243, recall 0.862234
2017-12-10T15:29:56.897587: step 1991, loss 0.29774, acc 0.90625, prec 0.0534137, recall 0.862234
2017-12-10T15:29:57.090816: step 1992, loss 0.284173, acc 0.890625, prec 0.0534326, recall 0.862307
2017-12-10T15:29:57.285424: step 1993, loss 1.24666, acc 0.953125, prec 0.0534897, recall 0.862454
2017-12-10T15:29:57.484942: step 1994, loss 0.318856, acc 0.921875, prec 0.0535744, recall 0.862672
2017-12-10T15:29:57.682440: step 1995, loss 0.965673, acc 0.90625, prec 0.0535949, recall 0.862745
2017-12-10T15:29:57.876188: step 1996, loss 0.377301, acc 0.890625, prec 0.053676, recall 0.862963
2017-12-10T15:29:58.072789: step 1997, loss 0.174284, acc 0.953125, prec 0.0536707, recall 0.862963
2017-12-10T15:29:58.266448: step 1998, loss 0.236431, acc 0.9375, prec 0.0536948, recall 0.863035
2017-12-10T15:29:58.463267: step 1999, loss 0.322555, acc 0.890625, prec 0.0536824, recall 0.863035
2017-12-10T15:29:58.662356: step 2000, loss 0.236662, acc 0.875, prec 0.0536683, recall 0.863035
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_2/1512937383/checkpoints/model-2000

2017-12-10T15:29:59.907287: step 2001, loss 0.391186, acc 0.828125, prec 0.05368, recall 0.863108
2017-12-10T15:30:00.101360: step 2002, loss 0.329881, acc 0.890625, prec 0.0536677, recall 0.863108
2017-12-10T15:30:00.293658: step 2003, loss 0.419581, acc 0.90625, prec 0.0537504, recall 0.863325
2017-12-10T15:30:00.491135: step 2004, loss 0.178402, acc 0.9375, prec 0.0537744, recall 0.863397
2017-12-10T15:30:00.688390: step 2005, loss 0.135211, acc 0.953125, prec 0.0538002, recall 0.863469
2017-12-10T15:30:00.884878: step 2006, loss 0.256771, acc 0.90625, prec 0.0538517, recall 0.863612
2017-12-10T15:30:01.081415: step 2007, loss 0.228383, acc 0.90625, prec 0.0538411, recall 0.863612
2017-12-10T15:30:01.282546: step 2008, loss 0.198662, acc 0.9375, prec 0.053834, recall 0.863612
2017-12-10T15:30:01.480070: step 2009, loss 0.222748, acc 0.90625, prec 0.0538234, recall 0.863612
2017-12-10T15:30:01.679106: step 2010, loss 0.122552, acc 0.96875, prec 0.0538509, recall 0.863684
2017-12-10T15:30:01.877675: step 2011, loss 0.187785, acc 0.921875, prec 0.0538421, recall 0.863684
2017-12-10T15:30:02.075844: step 2012, loss 0.0941472, acc 0.953125, prec 0.0538368, recall 0.863684
2017-12-10T15:30:02.269610: step 2013, loss 0.184904, acc 0.953125, prec 0.0538936, recall 0.863828
2017-12-10T15:30:02.462923: step 2014, loss 0.255386, acc 0.9375, prec 0.0539486, recall 0.863971
2017-12-10T15:30:02.661886: step 2015, loss 3.35192, acc 0.90625, prec 0.0539397, recall 0.863517
2017-12-10T15:30:02.868507: step 2016, loss 0.1431, acc 0.9375, prec 0.0539327, recall 0.863517
2017-12-10T15:30:03.066489: step 2017, loss 0.101329, acc 1, prec 0.0539637, recall 0.863589
2017-12-10T15:30:03.261513: step 2018, loss 0.606887, acc 0.890625, prec 0.0539823, recall 0.86366
2017-12-10T15:30:03.457228: step 2019, loss 0.145717, acc 0.90625, prec 0.0539717, recall 0.86366
2017-12-10T15:30:03.651395: step 2020, loss 0.218909, acc 0.921875, prec 0.0539628, recall 0.86366
2017-12-10T15:30:03.849118: step 2021, loss 0.18054, acc 0.96875, prec 0.0540213, recall 0.863803
2017-12-10T15:30:04.047391: step 2022, loss 0.351, acc 0.890625, prec 0.0540399, recall 0.863874
2017-12-10T15:30:04.246353: step 2023, loss 0.363034, acc 0.890625, prec 0.0540894, recall 0.864017
2017-12-10T15:30:04.442245: step 2024, loss 0.187719, acc 0.9375, prec 0.0541443, recall 0.864159
2017-12-10T15:30:04.635936: step 2025, loss 0.193284, acc 0.9375, prec 0.0541372, recall 0.864159
2017-12-10T15:30:04.828579: step 2026, loss 0.203323, acc 0.9375, prec 0.0541611, recall 0.86423
2017-12-10T15:30:05.024221: step 2027, loss 0.164591, acc 0.921875, prec 0.0541832, recall 0.864301
2017-12-10T15:30:05.222166: step 2028, loss 0.452038, acc 0.859375, prec 0.0541672, recall 0.864301
2017-12-10T15:30:05.415956: step 2029, loss 0.16144, acc 0.953125, prec 0.0541619, recall 0.864301
2017-12-10T15:30:05.611687: step 2030, loss 0.550297, acc 0.953125, prec 0.0541875, recall 0.864371
2017-12-10T15:30:05.805632: step 2031, loss 0.296731, acc 0.90625, prec 0.0541769, recall 0.864371
2017-12-10T15:30:05.999816: step 2032, loss 0.109656, acc 0.921875, prec 0.054168, recall 0.864371
2017-12-10T15:30:06.195635: step 2033, loss 0.804026, acc 0.96875, prec 0.0542572, recall 0.864583
2017-12-10T15:30:06.392536: step 2034, loss 0.482774, acc 0.9375, prec 0.0543429, recall 0.864795
2017-12-10T15:30:06.591699: step 2035, loss 2.24483, acc 0.953125, prec 0.054492, recall 0.865145
2017-12-10T15:30:06.787185: step 2036, loss 0.144686, acc 0.953125, prec 0.0545484, recall 0.865285
2017-12-10T15:30:06.986272: step 2037, loss 0.379116, acc 0.875, prec 0.0545342, recall 0.865285
2017-12-10T15:30:07.184443: step 2038, loss 0.287918, acc 0.84375, prec 0.0545164, recall 0.865285
2017-12-10T15:30:07.383613: step 2039, loss 0.399172, acc 0.78125, prec 0.0544915, recall 0.865285
2017-12-10T15:30:07.575630: step 2040, loss 0.477612, acc 0.8125, prec 0.0544701, recall 0.865285
2017-12-10T15:30:07.769443: step 2041, loss 0.394673, acc 0.875, prec 0.0544559, recall 0.865285
2017-12-10T15:30:07.969638: step 2042, loss 0.472791, acc 0.828125, prec 0.0544672, recall 0.865355
2017-12-10T15:30:08.164991: step 2043, loss 0.291131, acc 0.859375, prec 0.0544512, recall 0.865355
2017-12-10T15:30:08.358889: step 2044, loss 0.3399, acc 0.890625, prec 0.0544388, recall 0.865355
2017-12-10T15:30:08.553670: step 2045, loss 0.336224, acc 0.875, prec 0.0544862, recall 0.865494
2017-12-10T15:30:08.748222: step 2046, loss 0.339689, acc 0.921875, prec 0.0545082, recall 0.865564
2017-12-10T15:30:08.942794: step 2047, loss 0.337751, acc 0.875, prec 0.054494, recall 0.865564
2017-12-10T15:30:09.136328: step 2048, loss 0.564266, acc 0.859375, prec 0.0545088, recall 0.865633
2017-12-10T15:30:09.330982: step 2049, loss 0.723539, acc 0.8125, prec 0.0545182, recall 0.865702
2017-12-10T15:30:09.525564: step 2050, loss 0.332231, acc 0.859375, prec 0.054533, recall 0.865772
2017-12-10T15:30:09.720197: step 2051, loss 0.458041, acc 0.8125, prec 0.0545118, recall 0.865772
2017-12-10T15:30:09.916767: step 2052, loss 0.394362, acc 0.890625, prec 0.0544994, recall 0.865772
2017-12-10T15:30:10.118344: step 2053, loss 0.508041, acc 0.953125, prec 0.0546169, recall 0.866048
2017-12-10T15:30:10.320341: step 2054, loss 0.187526, acc 0.921875, prec 0.0546081, recall 0.866048
2017-12-10T15:30:10.512999: step 2055, loss 0.162805, acc 0.9375, prec 0.054601, recall 0.866048
2017-12-10T15:30:10.711642: step 2056, loss 0.372058, acc 0.875, prec 0.0546175, recall 0.866117
2017-12-10T15:30:10.909199: step 2057, loss 0.194944, acc 0.921875, prec 0.0546393, recall 0.866186
2017-12-10T15:30:11.104624: step 2058, loss 0.275594, acc 0.953125, prec 0.0546647, recall 0.866255
2017-12-10T15:30:11.298350: step 2059, loss 1.24083, acc 0.953125, prec 0.0547207, recall 0.866393
2017-12-10T15:30:11.500108: step 2060, loss 0.128511, acc 0.96875, prec 0.0547172, recall 0.866393
2017-12-10T15:30:11.695351: step 2061, loss 1.18126, acc 0.96875, prec 0.054775, recall 0.86653
2017-12-10T15:30:11.892557: step 2062, loss 0.117688, acc 0.953125, prec 0.0548003, recall 0.866598
2017-12-10T15:30:12.089508: step 2063, loss 0.996165, acc 0.96875, prec 0.0548274, recall 0.866667
2017-12-10T15:30:12.289071: step 2064, loss 0.391788, acc 0.890625, prec 0.0548456, recall 0.866735
2017-12-10T15:30:12.484881: step 2065, loss 0.418608, acc 0.875, prec 0.054862, recall 0.866803
2017-12-10T15:30:12.682570: step 2066, loss 0.241228, acc 0.921875, prec 0.0548838, recall 0.866871
2017-12-10T15:30:12.877764: step 2067, loss 1.19541, acc 0.890625, prec 0.054902, recall 0.86694
2017-12-10T15:30:13.077643: step 2068, loss 0.0690798, acc 0.96875, prec 0.0549597, recall 0.867076
2017-12-10T15:30:13.276293: step 2069, loss 0.100449, acc 0.96875, prec 0.0549561, recall 0.867076
2017-12-10T15:30:13.472400: step 2070, loss 0.926983, acc 0.859375, prec 0.0550013, recall 0.867211
2017-12-10T15:30:13.668937: step 2071, loss 0.354176, acc 0.890625, prec 0.0549888, recall 0.867211
2017-12-10T15:30:13.865601: step 2072, loss 0.287724, acc 0.921875, prec 0.0550411, recall 0.867347
2017-12-10T15:30:14.062580: step 2073, loss 0.321137, acc 0.890625, prec 0.0550592, recall 0.867415
2017-12-10T15:30:14.267434: step 2074, loss 0.192943, acc 0.9375, prec 0.0550827, recall 0.867482
2017-12-10T15:30:14.470624: step 2075, loss 0.502014, acc 0.84375, prec 0.0550649, recall 0.867482
2017-12-10T15:30:14.662102: step 2076, loss 0.190652, acc 0.90625, prec 0.0550542, recall 0.867482
2017-12-10T15:30:14.862482: step 2077, loss 0.481827, acc 0.796875, prec 0.055031, recall 0.867482
2017-12-10T15:30:15.054323: step 2078, loss 0.390122, acc 0.84375, prec 0.0550743, recall 0.867617
2017-12-10T15:30:15.250859: step 2079, loss 0.474693, acc 0.828125, prec 0.0550548, recall 0.867617
2017-12-10T15:30:15.453538: step 2080, loss 0.266666, acc 0.890625, prec 0.0550423, recall 0.867617
2017-12-10T15:30:15.649173: step 2081, loss 0.169767, acc 0.9375, prec 0.0550962, recall 0.867752
2017-12-10T15:30:15.850158: step 2082, loss 0.150367, acc 0.9375, prec 0.0550891, recall 0.867752
2017-12-10T15:30:16.048832: step 2083, loss 0.367658, acc 0.875, prec 0.0550749, recall 0.867752
2017-12-10T15:30:16.251783: step 2084, loss 0.235723, acc 0.90625, prec 0.0550947, recall 0.867819
2017-12-10T15:30:16.445652: step 2085, loss 0.428725, acc 0.84375, prec 0.0551074, recall 0.867886
2017-12-10T15:30:16.640189: step 2086, loss 0.484442, acc 0.84375, prec 0.0551506, recall 0.86802
2017-12-10T15:30:16.834770: step 2087, loss 0.185255, acc 0.953125, prec 0.0551453, recall 0.86802
2017-12-10T15:30:17.030524: step 2088, loss 0.0485533, acc 0.984375, prec 0.0551435, recall 0.86802
2017-12-10T15:30:17.229011: step 2089, loss 0.480273, acc 0.9375, prec 0.0552278, recall 0.868221
2017-12-10T15:30:17.429898: step 2090, loss 0.453409, acc 0.921875, prec 0.0552493, recall 0.868288
2017-12-10T15:30:17.625631: step 2091, loss 0.208164, acc 0.984375, prec 0.0553085, recall 0.868421
2017-12-10T15:30:17.820847: step 2092, loss 0.157158, acc 0.9375, prec 0.0553318, recall 0.868488
2017-12-10T15:30:18.020396: step 2093, loss 0.177813, acc 0.9375, prec 0.0554159, recall 0.868687
2017-12-10T15:30:18.217720: step 2094, loss 0.237971, acc 0.9375, prec 0.0554088, recall 0.868687
2017-12-10T15:30:18.415222: step 2095, loss 0.378768, acc 0.953125, prec 0.0554947, recall 0.868886
2017-12-10T15:30:18.612690: step 2096, loss 0.280564, acc 0.953125, prec 0.0555502, recall 0.869018
2017-12-10T15:30:18.810626: step 2097, loss 1.00011, acc 0.90625, prec 0.0555699, recall 0.869084
2017-12-10T15:30:19.011355: step 2098, loss 0.122554, acc 0.9375, prec 0.0555627, recall 0.869084
2017-12-10T15:30:19.208007: step 2099, loss 0.137678, acc 0.953125, prec 0.0555877, recall 0.869149
2017-12-10T15:30:19.403588: step 2100, loss 0.258884, acc 0.953125, prec 0.0556736, recall 0.869347
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_2/1512937383/checkpoints/model-2100

2017-12-10T15:30:20.617133: step 2101, loss 0.121469, acc 0.921875, prec 0.0557254, recall 0.869478
2017-12-10T15:30:20.812773: step 2102, loss 0.700313, acc 0.984375, prec 0.055754, recall 0.869543
2017-12-10T15:30:21.014193: step 2103, loss 0.265461, acc 0.875, prec 0.05577, recall 0.869609
2017-12-10T15:30:21.214959: step 2104, loss 0.71313, acc 0.96875, prec 0.0557968, recall 0.869674
2017-12-10T15:30:21.415314: step 2105, loss 0.910106, acc 0.875, prec 0.0558735, recall 0.86987
2017-12-10T15:30:21.611971: step 2106, loss 0.382065, acc 0.875, prec 0.0558894, recall 0.869935
2017-12-10T15:30:21.811684: step 2107, loss 0.268566, acc 0.921875, prec 0.0558805, recall 0.869935
2017-12-10T15:30:22.004762: step 2108, loss 0.239777, acc 0.90625, prec 0.0558697, recall 0.869935
2017-12-10T15:30:22.204046: step 2109, loss 0.383139, acc 0.875, prec 0.0558857, recall 0.87
2017-12-10T15:30:22.402384: step 2110, loss 0.38566, acc 0.828125, prec 0.0558659, recall 0.87
2017-12-10T15:30:22.598422: step 2111, loss 0.594044, acc 0.890625, prec 0.055914, recall 0.87013
2017-12-10T15:30:22.800644: step 2112, loss 0.49705, acc 0.8125, prec 0.0558924, recall 0.87013
2017-12-10T15:30:22.998087: step 2113, loss 0.33013, acc 0.890625, prec 0.0559102, recall 0.870195
2017-12-10T15:30:23.194530: step 2114, loss 0.211853, acc 0.921875, prec 0.055992, recall 0.870389
2017-12-10T15:30:23.392397: step 2115, loss 0.270881, acc 0.921875, prec 0.0560133, recall 0.870453
2017-12-10T15:30:23.588717: step 2116, loss 0.180154, acc 0.90625, prec 0.0560026, recall 0.870453
2017-12-10T15:30:23.785059: step 2117, loss 0.267748, acc 0.90625, prec 0.0559918, recall 0.870453
2017-12-10T15:30:23.983782: step 2118, loss 0.315838, acc 0.890625, prec 0.0560095, recall 0.870518
2017-12-10T15:30:24.181242: step 2119, loss 0.187759, acc 0.921875, prec 0.0560005, recall 0.870518
2017-12-10T15:30:24.381379: step 2120, loss 0.205454, acc 0.921875, prec 0.0560218, recall 0.870582
2017-12-10T15:30:24.583517: step 2121, loss 0.526882, acc 0.921875, prec 0.0560733, recall 0.870711
2017-12-10T15:30:24.782606: step 2122, loss 0.371345, acc 0.921875, prec 0.0560945, recall 0.870775
2017-12-10T15:30:24.983203: step 2123, loss 0.0895546, acc 0.9375, prec 0.0560873, recall 0.870775
2017-12-10T15:30:25.180900: step 2124, loss 0.217893, acc 0.9375, prec 0.0561104, recall 0.87084
2017-12-10T15:30:25.374903: step 2125, loss 0.160739, acc 0.953125, prec 0.056105, recall 0.87084
2017-12-10T15:30:25.567405: step 2126, loss 0.395243, acc 0.859375, prec 0.0561492, recall 0.870968
2017-12-10T15:30:25.768166: step 2127, loss 0.303474, acc 0.9375, prec 0.0561722, recall 0.871032
2017-12-10T15:30:25.969927: step 2128, loss 0.046721, acc 1, prec 0.0561722, recall 0.871032
2017-12-10T15:30:26.165482: step 2129, loss 0.170367, acc 0.921875, prec 0.0561632, recall 0.871032
2017-12-10T15:30:26.361203: step 2130, loss 0.400639, acc 0.921875, prec 0.0562146, recall 0.87116
2017-12-10T15:30:26.561654: step 2131, loss 0.442481, acc 0.984375, prec 0.0563034, recall 0.871351
2017-12-10T15:30:26.758117: step 2132, loss 0.245096, acc 0.90625, prec 0.0563227, recall 0.871414
2017-12-10T15:30:26.954043: step 2133, loss 0.370317, acc 0.953125, prec 0.0563475, recall 0.871478
2017-12-10T15:30:27.149026: step 2134, loss 0.654034, acc 0.953125, prec 0.0563722, recall 0.871542
2017-12-10T15:30:27.346400: step 2135, loss 0.139252, acc 0.953125, prec 0.0563668, recall 0.871542
2017-12-10T15:30:27.540526: step 2136, loss 0.142187, acc 0.953125, prec 0.0564217, recall 0.871668
2017-12-10T15:30:27.734193: step 2137, loss 0.0853917, acc 1, prec 0.056482, recall 0.871795
2017-12-10T15:30:27.929823: step 2138, loss 0.179574, acc 0.953125, prec 0.0564766, recall 0.871795
2017-12-10T15:30:28.122982: step 2139, loss 0.183377, acc 0.921875, prec 0.0564676, recall 0.871795
2017-12-10T15:30:28.319860: step 2140, loss 0.205256, acc 0.921875, prec 0.0564586, recall 0.871795
2017-12-10T15:30:28.515095: step 2141, loss 0.3107, acc 0.921875, prec 0.0564797, recall 0.871858
2017-12-10T15:30:28.711326: step 2142, loss 0.116244, acc 0.984375, prec 0.056508, recall 0.871921
2017-12-10T15:30:28.909864: step 2143, loss 0.13977, acc 0.9375, prec 0.0565008, recall 0.871921
2017-12-10T15:30:29.103475: step 2144, loss 0.451185, acc 0.953125, prec 0.0565556, recall 0.872047
2017-12-10T15:30:29.304154: step 2145, loss 0.0407149, acc 1, prec 0.0566158, recall 0.872173
2017-12-10T15:30:29.497842: step 2146, loss 0.0317966, acc 1, prec 0.0566158, recall 0.872173
2017-12-10T15:30:29.696537: step 2147, loss 0.243835, acc 0.9375, prec 0.0566086, recall 0.872173
2017-12-10T15:30:29.897320: step 2148, loss 0.0607853, acc 0.984375, prec 0.0566068, recall 0.872173
2017-12-10T15:30:30.096602: step 2149, loss 0.293165, acc 0.921875, prec 0.0566279, recall 0.872236
2017-12-10T15:30:30.292089: step 2150, loss 0.626178, acc 0.984375, prec 0.0567163, recall 0.872424
2017-12-10T15:30:30.493418: step 2151, loss 1.00781, acc 0.953125, prec 0.0568011, recall 0.872611
2017-12-10T15:30:30.692697: step 2152, loss 0.20962, acc 0.921875, prec 0.0568222, recall 0.872674
2017-12-10T15:30:30.887655: step 2153, loss 0.169651, acc 0.953125, prec 0.0568167, recall 0.872674
2017-12-10T15:30:31.080310: step 2154, loss 0.115884, acc 0.953125, prec 0.0568113, recall 0.872674
2017-12-10T15:30:31.286427: step 2155, loss 0.447837, acc 0.875, prec 0.0568569, recall 0.872798
2017-12-10T15:30:31.485566: step 2156, loss 0.318177, acc 0.921875, prec 0.0568779, recall 0.872861
2017-12-10T15:30:31.684245: step 2157, loss 0.329247, acc 0.90625, prec 0.0568971, recall 0.872923
2017-12-10T15:30:31.879758: step 2158, loss 0.366034, acc 0.921875, prec 0.056888, recall 0.872923
2017-12-10T15:30:32.075990: step 2159, loss 0.107096, acc 0.9375, prec 0.0568808, recall 0.872923
2017-12-10T15:30:32.273545: step 2160, loss 0.0844005, acc 0.96875, prec 0.0568772, recall 0.872923
2017-12-10T15:30:32.470588: step 2161, loss 0.0657712, acc 0.96875, prec 0.0568735, recall 0.872923
2017-12-10T15:30:32.672636: step 2162, loss 0.188362, acc 0.921875, prec 0.0568645, recall 0.872923
2017-12-10T15:30:32.867282: step 2163, loss 0.513433, acc 0.859375, prec 0.0568482, recall 0.872923
2017-12-10T15:30:33.060220: step 2164, loss 0.242878, acc 0.921875, prec 0.0568392, recall 0.872923
2017-12-10T15:30:33.254241: step 2165, loss 1.28637, acc 0.90625, prec 0.0568883, recall 0.873047
2017-12-10T15:30:33.446890: step 2166, loss 0.140774, acc 0.9375, prec 0.0569111, recall 0.873109
2017-12-10T15:30:33.647870: step 2167, loss 0.300896, acc 0.890625, prec 0.0569884, recall 0.873294
2017-12-10T15:30:33.843370: step 2168, loss 0.133501, acc 0.96875, prec 0.0570148, recall 0.873356
2017-12-10T15:30:34.038534: step 2169, loss 0.155578, acc 0.953125, prec 0.0570093, recall 0.873356
2017-12-10T15:30:34.232495: step 2170, loss 0.277453, acc 0.90625, prec 0.0569984, recall 0.873356
2017-12-10T15:30:34.430727: step 2171, loss 0.309706, acc 0.875, prec 0.0570139, recall 0.873418
2017-12-10T15:30:34.624523: step 2172, loss 0.785418, acc 0.890625, prec 0.0570612, recall 0.873541
2017-12-10T15:30:34.824018: step 2173, loss 1.23461, acc 0.9375, prec 0.0570557, recall 0.873116
2017-12-10T15:30:35.025670: step 2174, loss 0.383421, acc 0.890625, prec 0.0571329, recall 0.873301
2017-12-10T15:30:35.227004: step 2175, loss 0.573035, acc 0.953125, prec 0.0571574, recall 0.873362
2017-12-10T15:30:35.426941: step 2176, loss 0.136583, acc 0.96875, prec 0.0572136, recall 0.873485
2017-12-10T15:30:35.623821: step 2177, loss 0.725257, acc 0.84375, prec 0.0572254, recall 0.873547
2017-12-10T15:30:35.818350: step 2178, loss 0.332952, acc 0.890625, prec 0.0572426, recall 0.873608
2017-12-10T15:30:36.014501: step 2179, loss 0.277955, acc 0.875, prec 0.0572281, recall 0.873608
2017-12-10T15:30:36.208966: step 2180, loss 0.379662, acc 0.859375, prec 0.0572117, recall 0.873608
2017-12-10T15:30:36.402851: step 2181, loss 0.281876, acc 0.875, prec 0.0572869, recall 0.873791
2017-12-10T15:30:36.603437: step 2182, loss 0.170688, acc 0.9375, prec 0.0573095, recall 0.873852
2017-12-10T15:30:36.801790: step 2183, loss 0.902815, acc 0.859375, prec 0.057323, recall 0.873913
2017-12-10T15:30:37.003333: step 2184, loss 0.47102, acc 0.8125, prec 0.0573311, recall 0.873974
2017-12-10T15:30:37.205112: step 2185, loss 0.30385, acc 0.875, prec 0.0573464, recall 0.874035
2017-12-10T15:30:37.400652: step 2186, loss 0.325949, acc 0.875, prec 0.0573617, recall 0.874095
2017-12-10T15:30:37.597743: step 2187, loss 0.395779, acc 0.875, prec 0.057377, recall 0.874156
2017-12-10T15:30:37.791810: step 2188, loss 0.339428, acc 0.875, prec 0.0573625, recall 0.874156
2017-12-10T15:30:37.987284: step 2189, loss 0.422427, acc 0.875, prec 0.0573778, recall 0.874217
2017-12-10T15:30:38.187871: step 2190, loss 0.155785, acc 0.9375, prec 0.0573706, recall 0.874217
2017-12-10T15:30:38.384299: step 2191, loss 0.239707, acc 0.953125, prec 0.0573949, recall 0.874277
2017-12-10T15:30:38.580861: step 2192, loss 0.283413, acc 0.921875, prec 0.0574157, recall 0.874338
2017-12-10T15:30:38.775333: step 2193, loss 0.19013, acc 0.90625, prec 0.0574346, recall 0.874398
2017-12-10T15:30:38.976610: step 2194, loss 0.428394, acc 0.84375, prec 0.0574164, recall 0.874398
2017-12-10T15:30:39.178407: step 2195, loss 0.24433, acc 0.90625, prec 0.0574055, recall 0.874398
2017-12-10T15:30:39.380924: step 2196, loss 0.974002, acc 0.90625, prec 0.0574542, recall 0.874519
2017-12-10T15:30:39.579930: step 2197, loss 0.117559, acc 0.953125, prec 0.0574488, recall 0.874519
2017-12-10T15:30:39.777302: step 2198, loss 0.136586, acc 0.9375, prec 0.0574415, recall 0.874519
2017-12-10T15:30:39.976173: step 2199, loss 0.89325, acc 0.921875, prec 0.0574622, recall 0.87458
2017-12-10T15:30:40.175539: step 2200, loss 0.105642, acc 0.984375, prec 0.0574604, recall 0.87458
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_2/1512937383/checkpoints/model-2200

2017-12-10T15:30:41.666026: step 2201, loss 0.31203, acc 0.890625, prec 0.0575072, recall 0.8747
2017-12-10T15:30:41.865875: step 2202, loss 0.144203, acc 0.9375, prec 0.0574999, recall 0.8747
2017-12-10T15:30:42.068295: step 2203, loss 0.18981, acc 0.90625, prec 0.057489, recall 0.8747
2017-12-10T15:30:42.267946: step 2204, loss 0.110024, acc 0.96875, prec 0.0575151, recall 0.87476
2017-12-10T15:30:42.465121: step 2205, loss 0.122857, acc 0.953125, prec 0.0575097, recall 0.87476
2017-12-10T15:30:42.662822: step 2206, loss 0.182181, acc 0.9375, prec 0.0575322, recall 0.87482
2017-12-10T15:30:42.858035: step 2207, loss 0.458517, acc 0.96875, prec 0.0575583, recall 0.87488
2017-12-10T15:30:43.060958: step 2208, loss 0.253617, acc 0.953125, prec 0.0575825, recall 0.87494
2017-12-10T15:30:43.262523: step 2209, loss 0.257207, acc 0.90625, prec 0.0575716, recall 0.87494
2017-12-10T15:30:43.464883: step 2210, loss 0.308436, acc 0.953125, prec 0.0576256, recall 0.87506
2017-12-10T15:30:43.661008: step 2211, loss 0.341782, acc 0.875, prec 0.0576111, recall 0.87506
2017-12-10T15:30:43.855524: step 2212, loss 0.211661, acc 0.96875, prec 0.0576372, recall 0.87512
2017-12-10T15:30:44.055643: step 2213, loss 0.0921257, acc 0.953125, prec 0.0576317, recall 0.87512
2017-12-10T15:30:44.257098: step 2214, loss 0.472704, acc 1, prec 0.0576911, recall 0.875239
2017-12-10T15:30:44.464462: step 2215, loss 0.167429, acc 0.953125, prec 0.0576856, recall 0.875239
2017-12-10T15:30:44.659589: step 2216, loss 0.167905, acc 0.953125, prec 0.0576802, recall 0.875239
2017-12-10T15:30:44.851265: step 2217, loss 0.199853, acc 0.9375, prec 0.0576729, recall 0.875239
2017-12-10T15:30:45.044450: step 2218, loss 0.122196, acc 0.96875, prec 0.0576693, recall 0.875239
2017-12-10T15:30:45.238803: step 2219, loss 0.114111, acc 0.96875, prec 0.0576657, recall 0.875239
2017-12-10T15:30:45.440911: step 2220, loss 0.220879, acc 0.953125, prec 0.0576899, recall 0.875299
2017-12-10T15:30:45.637321: step 2221, loss 0.129792, acc 0.96875, prec 0.0576863, recall 0.875299
2017-12-10T15:30:45.834212: step 2222, loss 0.38726, acc 0.96875, prec 0.057742, recall 0.875418
2017-12-10T15:30:46.032687: step 2223, loss 0.0772397, acc 0.96875, prec 0.057768, recall 0.875477
2017-12-10T15:30:46.237120: step 2224, loss 0.300405, acc 0.921875, prec 0.0577589, recall 0.875477
2017-12-10T15:30:46.435868: step 2225, loss 3.7428, acc 0.953125, prec 0.0577553, recall 0.87506
2017-12-10T15:30:46.637460: step 2226, loss 0.834609, acc 0.96875, prec 0.0578109, recall 0.875179
2017-12-10T15:30:46.842480: step 2227, loss 0.250927, acc 0.9375, prec 0.0578629, recall 0.875297
2017-12-10T15:30:47.039093: step 2228, loss 0.154411, acc 0.9375, prec 0.0578557, recall 0.875297
2017-12-10T15:30:47.243365: step 2229, loss 0.476994, acc 0.890625, prec 0.0579022, recall 0.875416
2017-12-10T15:30:47.433513: step 2230, loss 0.126644, acc 0.953125, prec 0.0578967, recall 0.875416
2017-12-10T15:30:47.630337: step 2231, loss 0.454429, acc 0.921875, prec 0.0579469, recall 0.875534
2017-12-10T15:30:47.828867: step 2232, loss 0.308802, acc 0.84375, prec 0.0579287, recall 0.875534
2017-12-10T15:30:48.025832: step 2233, loss 0.450582, acc 0.9375, prec 0.0579806, recall 0.875653
2017-12-10T15:30:48.223586: step 2234, loss 0.276393, acc 0.921875, prec 0.0580011, recall 0.875712
2017-12-10T15:30:48.418591: step 2235, loss 0.346574, acc 0.859375, prec 0.0579847, recall 0.875712
2017-12-10T15:30:48.613562: step 2236, loss 0.447255, acc 0.859375, prec 0.0579979, recall 0.875771
2017-12-10T15:30:48.806840: step 2237, loss 0.210783, acc 0.890625, prec 0.0580147, recall 0.875829
2017-12-10T15:30:49.002594: step 2238, loss 0.680462, acc 0.8125, prec 0.0580224, recall 0.875888
2017-12-10T15:30:49.198925: step 2239, loss 0.687628, acc 0.796875, prec 0.0580578, recall 0.876006
2017-12-10T15:30:49.393493: step 2240, loss 0.451012, acc 0.84375, prec 0.0580692, recall 0.876064
2017-12-10T15:30:49.591081: step 2241, loss 0.468077, acc 0.84375, prec 0.05811, recall 0.876181
2017-12-10T15:30:49.783863: step 2242, loss 0.388058, acc 0.875, prec 0.0582135, recall 0.876415
2017-12-10T15:30:49.983779: step 2243, loss 0.408098, acc 0.875, prec 0.0582284, recall 0.876473
2017-12-10T15:30:50.176417: step 2244, loss 0.938069, acc 0.859375, prec 0.0582415, recall 0.876532
2017-12-10T15:30:50.375491: step 2245, loss 0.444889, acc 0.875, prec 0.0582564, recall 0.87659
2017-12-10T15:30:50.569074: step 2246, loss 0.212283, acc 0.921875, prec 0.0582473, recall 0.87659
2017-12-10T15:30:50.767543: step 2247, loss 0.171257, acc 0.953125, prec 0.0583007, recall 0.876706
2017-12-10T15:30:50.964886: step 2248, loss 0.312148, acc 0.9375, prec 0.0582934, recall 0.876706
2017-12-10T15:30:51.165955: step 2249, loss 0.149084, acc 0.96875, prec 0.0583487, recall 0.876822
2017-12-10T15:30:51.365619: step 2250, loss 0.278548, acc 0.9375, prec 0.0584298, recall 0.876995
2017-12-10T15:30:51.561747: step 2251, loss 0.558962, acc 0.890625, prec 0.0584464, recall 0.877053
2017-12-10T15:30:51.763267: step 2252, loss 0.10114, acc 1, prec 0.0584759, recall 0.877111
2017-12-10T15:30:51.963231: step 2253, loss 0.166204, acc 0.9375, prec 0.058498, recall 0.877168
2017-12-10T15:30:52.161899: step 2254, loss 0.600804, acc 0.796875, prec 0.0585331, recall 0.877283
2017-12-10T15:30:52.355486: step 2255, loss 0.246459, acc 0.921875, prec 0.0585533, recall 0.877341
2017-12-10T15:30:52.553062: step 2256, loss 0.0879961, acc 0.953125, prec 0.0585773, recall 0.877398
2017-12-10T15:30:52.750530: step 2257, loss 0.167053, acc 0.9375, prec 0.05857, recall 0.877398
2017-12-10T15:30:52.940551: step 2258, loss 0.798086, acc 0.984375, prec 0.0586269, recall 0.877513
2017-12-10T15:30:53.136877: step 2259, loss 0.36791, acc 0.9375, prec 0.058649, recall 0.87757
2017-12-10T15:30:53.332943: step 2260, loss 0.163906, acc 0.921875, prec 0.0586399, recall 0.87757
2017-12-10T15:30:53.531469: step 2261, loss 0.156991, acc 0.96875, prec 0.0586656, recall 0.877627
2017-12-10T15:30:53.735041: step 2262, loss 0.134014, acc 0.921875, prec 0.0586858, recall 0.877684
2017-12-10T15:30:53.935852: step 2263, loss 0.418139, acc 0.9375, prec 0.0587372, recall 0.877798
2017-12-10T15:30:54.140005: step 2264, loss 1.40339, acc 0.90625, prec 0.0587575, recall 0.877446
2017-12-10T15:30:54.335502: step 2265, loss 0.508012, acc 0.859375, prec 0.0588584, recall 0.877674
2017-12-10T15:30:54.529397: step 2266, loss 0.120467, acc 0.96875, prec 0.0588841, recall 0.877731
2017-12-10T15:30:54.727002: step 2267, loss 0.246339, acc 0.828125, prec 0.0588639, recall 0.877731
2017-12-10T15:30:54.929474: step 2268, loss 0.306737, acc 0.890625, prec 0.0589097, recall 0.877845
2017-12-10T15:30:55.121794: step 2269, loss 0.295187, acc 0.890625, prec 0.0588969, recall 0.877845
2017-12-10T15:30:55.320840: step 2270, loss 0.241762, acc 0.953125, prec 0.0589207, recall 0.877902
2017-12-10T15:30:55.515740: step 2271, loss 0.334837, acc 0.859375, prec 0.0589335, recall 0.877958
2017-12-10T15:30:55.714105: step 2272, loss 0.12049, acc 0.953125, prec 0.058928, recall 0.877958
2017-12-10T15:30:55.912370: step 2273, loss 0.304538, acc 0.859375, prec 0.0589114, recall 0.877958
2017-12-10T15:30:56.105263: step 2274, loss 0.0973698, acc 0.96875, prec 0.0589078, recall 0.877958
2017-12-10T15:30:56.303958: step 2275, loss 0.144542, acc 0.984375, prec 0.0589352, recall 0.878015
2017-12-10T15:30:56.501606: step 2276, loss 0.309261, acc 0.921875, prec 0.0590139, recall 0.878184
2017-12-10T15:30:56.696280: step 2277, loss 0.309903, acc 0.984375, prec 0.0590414, recall 0.878241
2017-12-10T15:30:56.895154: step 2278, loss 0.135749, acc 0.96875, prec 0.0590377, recall 0.878241
2017-12-10T15:30:57.089437: step 2279, loss 0.131278, acc 0.96875, prec 0.059034, recall 0.878241
2017-12-10T15:30:57.285113: step 2280, loss 0.157282, acc 0.921875, prec 0.0590249, recall 0.878241
2017-12-10T15:30:57.482539: step 2281, loss 0.198431, acc 0.921875, prec 0.059045, recall 0.878297
2017-12-10T15:30:57.676741: step 2282, loss 0.325011, acc 0.90625, prec 0.0590632, recall 0.878353
2017-12-10T15:30:57.876498: step 2283, loss 1.75013, acc 0.96875, prec 0.0590906, recall 0.878004
2017-12-10T15:30:58.076659: step 2284, loss 0.330898, acc 0.9375, prec 0.0591418, recall 0.878116
2017-12-10T15:30:58.286492: step 2285, loss 0.682686, acc 0.875, prec 0.0591563, recall 0.878173
2017-12-10T15:30:58.481942: step 2286, loss 0.269411, acc 0.921875, prec 0.0591471, recall 0.878173
2017-12-10T15:30:58.678771: step 2287, loss 0.190505, acc 0.90625, prec 0.0591361, recall 0.878173
2017-12-10T15:30:58.875157: step 2288, loss 0.354988, acc 0.90625, prec 0.0591251, recall 0.878173
2017-12-10T15:30:59.072224: step 2289, loss 0.204227, acc 0.921875, prec 0.0591159, recall 0.878173
2017-12-10T15:30:59.277314: step 2290, loss 0.361638, acc 0.921875, prec 0.0591067, recall 0.878173
2017-12-10T15:30:59.474851: step 2291, loss 0.53409, acc 0.84375, prec 0.0591176, recall 0.878229
2017-12-10T15:30:59.671534: step 2292, loss 1.06593, acc 0.890625, prec 0.0592216, recall 0.878453
2017-12-10T15:30:59.872378: step 2293, loss 0.328305, acc 0.90625, prec 0.0592397, recall 0.878509
2017-12-10T15:31:00.063961: step 2294, loss 0.461641, acc 0.890625, prec 0.0592269, recall 0.878509
2017-12-10T15:31:00.261869: step 2295, loss 0.238004, acc 0.90625, prec 0.059245, recall 0.878565
2017-12-10T15:31:00.459239: step 2296, loss 0.673991, acc 0.859375, prec 0.0592285, recall 0.878565
2017-12-10T15:31:00.653025: step 2297, loss 0.144989, acc 0.953125, prec 0.059223, recall 0.878565
2017-12-10T15:31:00.844949: step 2298, loss 0.314952, acc 0.953125, prec 0.0592466, recall 0.878621
2017-12-10T15:31:01.040550: step 2299, loss 0.144884, acc 0.984375, prec 0.0592448, recall 0.878621
2017-12-10T15:31:01.242206: step 2300, loss 0.434049, acc 0.859375, prec 0.0592283, recall 0.878621
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_2/1512937383/checkpoints/model-2300

2017-12-10T15:31:02.396939: step 2301, loss 0.298134, acc 0.90625, prec 0.0592173, recall 0.878621
2017-12-10T15:31:02.593182: step 2302, loss 0.174248, acc 0.96875, prec 0.0592136, recall 0.878621
2017-12-10T15:31:02.791459: step 2303, loss 0.270473, acc 0.90625, prec 0.0592026, recall 0.878621
2017-12-10T15:31:02.990329: step 2304, loss 0.531602, acc 0.890625, prec 0.059248, recall 0.878732
2017-12-10T15:31:03.186325: step 2305, loss 0.172442, acc 0.9375, prec 0.0592407, recall 0.878732
2017-12-10T15:31:03.378351: step 2306, loss 0.165166, acc 0.921875, prec 0.0592315, recall 0.878732
2017-12-10T15:31:03.576332: step 2307, loss 0.245913, acc 0.953125, prec 0.0592551, recall 0.878788
2017-12-10T15:31:03.772787: step 2308, loss 0.193863, acc 0.953125, prec 0.0592496, recall 0.878788
2017-12-10T15:31:03.970225: step 2309, loss 0.271013, acc 0.953125, prec 0.0593024, recall 0.878899
2017-12-10T15:31:04.165226: step 2310, loss 0.373754, acc 0.890625, prec 0.0593768, recall 0.879066
2017-12-10T15:31:04.363399: step 2311, loss 0.449611, acc 1, prec 0.059435, recall 0.879176
2017-12-10T15:31:04.560048: step 2312, loss 0.234536, acc 0.984375, prec 0.0594914, recall 0.879287
2017-12-10T15:31:04.755702: step 2313, loss 0.263191, acc 0.9375, prec 0.0595422, recall 0.879397
2017-12-10T15:31:04.958039: step 2314, loss 0.262356, acc 0.921875, prec 0.0595621, recall 0.879452
2017-12-10T15:31:05.154883: step 2315, loss 1.14695, acc 0.875, prec 0.0595764, recall 0.879507
2017-12-10T15:31:05.353412: step 2316, loss 0.153561, acc 0.9375, prec 0.0595691, recall 0.879507
2017-12-10T15:31:05.552051: step 2317, loss 1.23758, acc 0.96875, prec 0.0596526, recall 0.879672
2017-12-10T15:31:05.756195: step 2318, loss 0.963641, acc 0.96875, prec 0.059707, recall 0.879781
2017-12-10T15:31:05.954558: step 2319, loss 0.148423, acc 0.953125, prec 0.0597015, recall 0.879781
2017-12-10T15:31:06.155388: step 2320, loss 0.374472, acc 0.859375, prec 0.0597139, recall 0.879836
2017-12-10T15:31:06.358034: step 2321, loss 0.20481, acc 0.921875, prec 0.0597047, recall 0.879836
2017-12-10T15:31:06.547782: step 2322, loss 0.216449, acc 0.875, prec 0.059719, recall 0.879891
2017-12-10T15:31:06.749493: step 2323, loss 0.40454, acc 0.875, prec 0.0597043, recall 0.879891
2017-12-10T15:31:06.942866: step 2324, loss 0.349068, acc 0.90625, prec 0.0598093, recall 0.880109
2017-12-10T15:31:07.135511: step 2325, loss 0.244611, acc 0.921875, prec 0.0598001, recall 0.880109
2017-12-10T15:31:07.326865: step 2326, loss 0.269589, acc 0.875, prec 0.0598433, recall 0.880218
2017-12-10T15:31:07.521143: step 2327, loss 0.17295, acc 0.9375, prec 0.0598649, recall 0.880272
2017-12-10T15:31:07.719109: step 2328, loss 0.350748, acc 0.9375, prec 0.0599155, recall 0.880381
2017-12-10T15:31:07.919503: step 2329, loss 0.12517, acc 0.9375, prec 0.0599371, recall 0.880435
2017-12-10T15:31:08.117636: step 2330, loss 0.385808, acc 0.828125, prec 0.0599458, recall 0.880489
2017-12-10T15:31:08.312848: step 2331, loss 0.446146, acc 0.875, prec 0.0599889, recall 0.880597
2017-12-10T15:31:08.506854: step 2332, loss 0.450765, acc 0.859375, prec 0.0600012, recall 0.880651
2017-12-10T15:31:08.705625: step 2333, loss 0.186946, acc 0.96875, prec 0.0600265, recall 0.880705
2017-12-10T15:31:08.904152: step 2334, loss 0.288846, acc 0.890625, prec 0.0600425, recall 0.880759
2017-12-10T15:31:09.102072: step 2335, loss 0.166635, acc 0.890625, prec 0.0600296, recall 0.880759
2017-12-10T15:31:09.293794: step 2336, loss 0.201295, acc 0.9375, prec 0.0600222, recall 0.880759
2017-12-10T15:31:09.492996: step 2337, loss 0.116032, acc 0.984375, prec 0.0600492, recall 0.880813
2017-12-10T15:31:09.691936: step 2338, loss 0.107858, acc 0.96875, prec 0.0600745, recall 0.880866
2017-12-10T15:31:09.889525: step 2339, loss 0.0454751, acc 0.984375, prec 0.0600726, recall 0.880866
2017-12-10T15:31:10.086690: step 2340, loss 0.118935, acc 0.96875, prec 0.0600689, recall 0.880866
2017-12-10T15:31:10.282094: step 2341, loss 0.891145, acc 0.953125, prec 0.0600923, recall 0.88092
2017-12-10T15:31:10.481049: step 2342, loss 0.0820349, acc 0.953125, prec 0.0601157, recall 0.880974
2017-12-10T15:31:10.677460: step 2343, loss 0.221001, acc 0.921875, prec 0.0601353, recall 0.881027
2017-12-10T15:31:10.877318: step 2344, loss 0.133646, acc 0.953125, prec 0.0601298, recall 0.881027
2017-12-10T15:31:11.074376: step 2345, loss 0.092728, acc 0.953125, prec 0.0601242, recall 0.881027
2017-12-10T15:31:11.272002: step 2346, loss 1.27146, acc 0.921875, prec 0.0601728, recall 0.881135
2017-12-10T15:31:11.470043: step 2347, loss 0.0908325, acc 1, prec 0.0602017, recall 0.881188
2017-12-10T15:31:11.669536: step 2348, loss 0.714696, acc 0.953125, prec 0.0602539, recall 0.881295
2017-12-10T15:31:11.870216: step 2349, loss 0.0875921, acc 0.96875, prec 0.0602502, recall 0.881295
2017-12-10T15:31:12.067977: step 2350, loss 0.529653, acc 0.96875, prec 0.0602754, recall 0.881348
2017-12-10T15:31:12.267913: step 2351, loss 0.324712, acc 0.921875, prec 0.060295, recall 0.881402
2017-12-10T15:31:12.465144: step 2352, loss 0.281966, acc 0.84375, prec 0.0603054, recall 0.881455
2017-12-10T15:31:12.660124: step 2353, loss 1.25494, acc 0.953125, prec 0.0603287, recall 0.881508
2017-12-10T15:31:12.863562: step 2354, loss 0.196841, acc 0.921875, prec 0.0603771, recall 0.881614
2017-12-10T15:31:13.058103: step 2355, loss 0.266868, acc 0.890625, prec 0.0603641, recall 0.881614
2017-12-10T15:31:13.254805: step 2356, loss 0.370314, acc 0.875, prec 0.0603493, recall 0.881614
2017-12-10T15:31:13.449367: step 2357, loss 0.894835, acc 0.96875, prec 0.0603745, recall 0.881667
2017-12-10T15:31:13.647534: step 2358, loss 0.461973, acc 0.875, prec 0.0603885, recall 0.88172
2017-12-10T15:31:13.846319: step 2359, loss 0.46687, acc 0.890625, prec 0.0604908, recall 0.881932
2017-12-10T15:31:14.044419: step 2360, loss 0.756665, acc 0.84375, prec 0.0605299, recall 0.882038
2017-12-10T15:31:14.257596: step 2361, loss 0.284814, acc 0.90625, prec 0.0605475, recall 0.88209
2017-12-10T15:31:14.456621: step 2362, loss 0.376289, acc 0.859375, prec 0.0605308, recall 0.88209
2017-12-10T15:31:14.651941: step 2363, loss 0.25905, acc 0.921875, prec 0.0606079, recall 0.882248
2017-12-10T15:31:14.850596: step 2364, loss 0.476791, acc 0.84375, prec 0.0606469, recall 0.882353
2017-12-10T15:31:15.043664: step 2365, loss 0.786745, acc 0.75, prec 0.060646, recall 0.882405
2017-12-10T15:31:15.239928: step 2366, loss 0.401849, acc 0.84375, prec 0.0606274, recall 0.882405
2017-12-10T15:31:15.437083: step 2367, loss 0.15804, acc 0.9375, prec 0.06062, recall 0.882405
2017-12-10T15:31:15.634878: step 2368, loss 0.232641, acc 0.90625, prec 0.0606088, recall 0.882405
2017-12-10T15:31:15.830523: step 2369, loss 0.521208, acc 0.796875, prec 0.0606135, recall 0.882458
2017-12-10T15:31:16.026483: step 2370, loss 0.316491, acc 0.875, prec 0.0605986, recall 0.882458
2017-12-10T15:31:16.223777: step 2371, loss 0.176523, acc 0.96875, prec 0.0606237, recall 0.88251
2017-12-10T15:31:16.424297: step 2372, loss 0.537228, acc 0.859375, prec 0.0606357, recall 0.882562
2017-12-10T15:31:16.620475: step 2373, loss 0.295777, acc 0.9375, prec 0.060657, recall 0.882614
2017-12-10T15:31:16.818930: step 2374, loss 0.36755, acc 0.84375, prec 0.0606958, recall 0.882719
2017-12-10T15:31:17.017891: step 2375, loss 0.251749, acc 0.921875, prec 0.0607153, recall 0.882771
2017-12-10T15:31:17.211943: step 2376, loss 0.317017, acc 0.875, prec 0.0607291, recall 0.882823
2017-12-10T15:31:17.406054: step 2377, loss 0.0918347, acc 0.984375, prec 0.0607559, recall 0.882875
2017-12-10T15:31:17.603094: step 2378, loss 0.587251, acc 0.9375, prec 0.0608059, recall 0.882979
2017-12-10T15:31:17.800486: step 2379, loss 0.260611, acc 0.921875, prec 0.0608252, recall 0.883031
2017-12-10T15:31:17.994638: step 2380, loss 0.0789042, acc 0.96875, prec 0.0608215, recall 0.883031
2017-12-10T15:31:18.194444: step 2381, loss 0.272835, acc 0.90625, prec 0.0608677, recall 0.883134
2017-12-10T15:31:18.390600: step 2382, loss 0.197314, acc 0.9375, prec 0.0609176, recall 0.883237
2017-12-10T15:31:18.591827: step 2383, loss 0.227617, acc 0.90625, prec 0.0609064, recall 0.883237
2017-12-10T15:31:18.793134: step 2384, loss 0.118364, acc 0.96875, prec 0.0609314, recall 0.883289
2017-12-10T15:31:18.987820: step 2385, loss 0.0490429, acc 0.984375, prec 0.0609295, recall 0.883289
2017-12-10T15:31:19.183786: step 2386, loss 0.243256, acc 0.953125, prec 0.0609239, recall 0.883289
2017-12-10T15:31:19.382621: step 2387, loss 0.120396, acc 0.96875, prec 0.0609488, recall 0.883341
2017-12-10T15:31:19.581865: step 2388, loss 0.0623449, acc 0.96875, prec 0.0609738, recall 0.883392
2017-12-10T15:31:19.780922: step 2389, loss 0.40188, acc 0.9375, prec 0.0609949, recall 0.883444
2017-12-10T15:31:19.977515: step 2390, loss 0.029049, acc 0.984375, prec 0.0609931, recall 0.883444
2017-12-10T15:31:20.173706: step 2391, loss 0.973303, acc 0.984375, prec 0.0610198, recall 0.883495
2017-12-10T15:31:20.373339: step 2392, loss 0.19712, acc 0.953125, prec 0.0610715, recall 0.883598
2017-12-10T15:31:20.571268: step 2393, loss 0.198942, acc 0.984375, prec 0.0610982, recall 0.883649
2017-12-10T15:31:20.774460: step 2394, loss 0.160986, acc 0.953125, prec 0.0610927, recall 0.883649
2017-12-10T15:31:20.969228: step 2395, loss 1.23156, acc 0.90625, prec 0.0611673, recall 0.883803
2017-12-10T15:31:21.168177: step 2396, loss 0.0390085, acc 0.984375, prec 0.0611654, recall 0.883803
2017-12-10T15:31:21.366852: step 2397, loss 0.271247, acc 0.96875, prec 0.0611903, recall 0.883854
2017-12-10T15:31:21.562629: step 2398, loss 0.110386, acc 0.96875, prec 0.0611866, recall 0.883854
2017-12-10T15:31:21.761314: step 2399, loss 0.175104, acc 0.9375, prec 0.0611791, recall 0.883854
2017-12-10T15:31:21.964337: step 2400, loss 0.312887, acc 0.890625, prec 0.0611947, recall 0.883905
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_2/1512937383/checkpoints/model-2400

2017-12-10T15:31:23.462555: step 2401, loss 0.298365, acc 0.90625, prec 0.0612121, recall 0.883956
2017-12-10T15:31:23.658272: step 2402, loss 0.685777, acc 0.90625, prec 0.0612295, recall 0.884007
2017-12-10T15:31:23.859451: step 2403, loss 0.402708, acc 0.921875, prec 0.0612201, recall 0.884007
2017-12-10T15:31:24.055338: step 2404, loss 0.257037, acc 0.921875, prec 0.0612394, recall 0.884058
2017-12-10T15:31:24.256039: step 2405, loss 0.216697, acc 0.921875, prec 0.0612586, recall 0.884109
2017-12-10T15:31:24.450310: step 2406, loss 0.166586, acc 0.953125, prec 0.061253, recall 0.884109
2017-12-10T15:31:24.646283: step 2407, loss 0.330662, acc 0.921875, prec 0.0612723, recall 0.88416
2017-12-10T15:31:24.845176: step 2408, loss 0.250886, acc 0.984375, prec 0.061299, recall 0.884211
2017-12-10T15:31:25.039479: step 2409, loss 0.300595, acc 0.9375, prec 0.0613486, recall 0.884312
2017-12-10T15:31:25.242496: step 2410, loss 0.23637, acc 0.984375, prec 0.0613752, recall 0.884363
2017-12-10T15:31:25.445247: step 2411, loss 0.202722, acc 0.90625, prec 0.0613641, recall 0.884363
2017-12-10T15:31:25.637757: step 2412, loss 0.18257, acc 0.90625, prec 0.0613529, recall 0.884363
2017-12-10T15:31:25.835367: step 2413, loss 0.265136, acc 0.9375, prec 0.0613454, recall 0.884363
2017-12-10T15:31:26.035669: step 2414, loss 0.310982, acc 0.921875, prec 0.0613646, recall 0.884413
2017-12-10T15:31:26.233920: step 2415, loss 0.120841, acc 0.9375, prec 0.0613571, recall 0.884413
2017-12-10T15:31:26.430227: step 2416, loss 0.223329, acc 0.921875, prec 0.0613478, recall 0.884413
2017-12-10T15:31:26.623790: step 2417, loss 0.391936, acc 0.921875, prec 0.0613385, recall 0.884413
2017-12-10T15:31:26.820139: step 2418, loss 0.279409, acc 0.875, prec 0.0613521, recall 0.884464
2017-12-10T15:31:27.015715: step 2419, loss 0.118825, acc 0.9375, prec 0.0613732, recall 0.884514
2017-12-10T15:31:27.208800: step 2420, loss 0.275141, acc 0.890625, prec 0.0613601, recall 0.884514
2017-12-10T15:31:27.406929: step 2421, loss 0.0498182, acc 0.953125, prec 0.0613545, recall 0.884514
2017-12-10T15:31:27.596328: step 2422, loss 0.261436, acc 0.921875, prec 0.0614022, recall 0.884615
2017-12-10T15:31:27.795502: step 2423, loss 3.75703, acc 0.9375, prec 0.0614535, recall 0.88433
2017-12-10T15:31:27.995881: step 2424, loss 0.0317023, acc 1, prec 0.0614535, recall 0.88433
2017-12-10T15:31:28.193391: step 2425, loss 0.0678575, acc 0.984375, prec 0.0614801, recall 0.88438
2017-12-10T15:31:28.390901: step 2426, loss 0.12836, acc 0.953125, prec 0.061503, recall 0.884431
2017-12-10T15:31:28.588792: step 2427, loss 0.199075, acc 0.921875, prec 0.0614937, recall 0.884431
2017-12-10T15:31:28.789905: step 2428, loss 0.212672, acc 0.9375, prec 0.0615147, recall 0.884481
2017-12-10T15:31:28.984332: step 2429, loss 0.214138, acc 0.9375, prec 0.0615641, recall 0.884582
2017-12-10T15:31:29.186224: step 2430, loss 0.331137, acc 0.96875, prec 0.0615888, recall 0.884632
2017-12-10T15:31:29.393190: step 2431, loss 0.241151, acc 0.875, prec 0.0616023, recall 0.884682
2017-12-10T15:31:29.597290: step 2432, loss 0.187895, acc 0.9375, prec 0.0615949, recall 0.884682
2017-12-10T15:31:29.796140: step 2433, loss 0.339275, acc 0.921875, prec 0.0616424, recall 0.884783
2017-12-10T15:31:29.997865: step 2434, loss 0.211587, acc 0.9375, prec 0.0616633, recall 0.884833
2017-12-10T15:31:30.195146: step 2435, loss 0.338637, acc 0.890625, prec 0.0617071, recall 0.884933
2017-12-10T15:31:30.393191: step 2436, loss 0.445307, acc 0.953125, prec 0.0617583, recall 0.885033
2017-12-10T15:31:30.590261: step 2437, loss 0.430972, acc 0.875, prec 0.0617717, recall 0.885082
2017-12-10T15:31:30.785226: step 2438, loss 0.165466, acc 0.921875, prec 0.0617624, recall 0.885082
2017-12-10T15:31:30.985813: step 2439, loss 0.194828, acc 0.953125, prec 0.0618136, recall 0.885182
2017-12-10T15:31:31.186502: step 2440, loss 0.229047, acc 0.921875, prec 0.061861, recall 0.885281
2017-12-10T15:31:31.388372: step 2441, loss 0.782419, acc 0.953125, prec 0.0619121, recall 0.885381
2017-12-10T15:31:31.597099: step 2442, loss 0.141085, acc 0.953125, prec 0.0619349, recall 0.88543
2017-12-10T15:31:31.794336: step 2443, loss 0.241696, acc 0.9375, prec 0.0619274, recall 0.88543
2017-12-10T15:31:31.991368: step 2444, loss 0.181851, acc 0.96875, prec 0.061952, recall 0.88548
2017-12-10T15:31:32.199402: step 2445, loss 0.585915, acc 0.84375, prec 0.0619616, recall 0.885529
2017-12-10T15:31:32.395306: step 2446, loss 0.164543, acc 0.921875, prec 0.0619523, recall 0.885529
2017-12-10T15:31:32.592344: step 2447, loss 0.0977019, acc 0.953125, prec 0.0619466, recall 0.885529
2017-12-10T15:31:32.787816: step 2448, loss 1.224, acc 0.875, prec 0.0619883, recall 0.885628
2017-12-10T15:31:32.985510: step 2449, loss 0.398461, acc 0.890625, prec 0.0620319, recall 0.885727
2017-12-10T15:31:33.184273: step 2450, loss 0.351009, acc 0.9375, prec 0.0620244, recall 0.885727
2017-12-10T15:31:33.384526: step 2451, loss 0.131785, acc 0.96875, prec 0.062049, recall 0.885776
2017-12-10T15:31:33.583670: step 2452, loss 0.562069, acc 0.9375, prec 0.0620698, recall 0.885825
2017-12-10T15:31:33.780233: step 2453, loss 0.148947, acc 0.921875, prec 0.0620604, recall 0.885825
2017-12-10T15:31:33.977504: step 2454, loss 0.875632, acc 0.875, prec 0.0620738, recall 0.885874
2017-12-10T15:31:34.176845: step 2455, loss 0.364249, acc 0.90625, prec 0.0620908, recall 0.885923
2017-12-10T15:31:34.374347: step 2456, loss 0.129887, acc 0.953125, prec 0.0620852, recall 0.885923
2017-12-10T15:31:34.569644: step 2457, loss 0.270034, acc 0.921875, prec 0.0621041, recall 0.885972
2017-12-10T15:31:34.767928: step 2458, loss 0.193976, acc 0.90625, prec 0.0621212, recall 0.886021
2017-12-10T15:31:34.961793: step 2459, loss 0.420616, acc 0.859375, prec 0.0621326, recall 0.88607
2017-12-10T15:31:35.158419: step 2460, loss 0.33203, acc 0.890625, prec 0.0621477, recall 0.886119
2017-12-10T15:31:35.357983: step 2461, loss 0.4936, acc 0.9375, prec 0.0621403, recall 0.886119
2017-12-10T15:31:35.560267: step 2462, loss 0.465262, acc 0.90625, prec 0.0621573, recall 0.886168
2017-12-10T15:31:35.758873: step 2463, loss 0.288968, acc 0.90625, prec 0.0622308, recall 0.886315
2017-12-10T15:31:35.954800: step 2464, loss 0.265015, acc 0.921875, prec 0.0622497, recall 0.886364
2017-12-10T15:31:36.147985: step 2465, loss 0.395701, acc 0.984375, prec 0.062276, recall 0.886412
2017-12-10T15:31:36.345659: step 2466, loss 0.262309, acc 0.890625, prec 0.0623194, recall 0.88651
2017-12-10T15:31:36.539276: step 2467, loss 0.127804, acc 0.953125, prec 0.062342, recall 0.886558
2017-12-10T15:31:36.740266: step 2468, loss 0.551014, acc 0.828125, prec 0.062406, recall 0.886704
2017-12-10T15:31:36.934721: step 2469, loss 0.0691162, acc 0.96875, prec 0.0624022, recall 0.886704
2017-12-10T15:31:37.136683: step 2470, loss 0.228832, acc 0.921875, prec 0.062421, recall 0.886752
2017-12-10T15:31:37.335842: step 2471, loss 0.0600375, acc 0.984375, prec 0.0624474, recall 0.886801
2017-12-10T15:31:37.529458: step 2472, loss 0.350952, acc 0.90625, prec 0.0624643, recall 0.886849
2017-12-10T15:31:37.725965: step 2473, loss 0.121276, acc 0.953125, prec 0.0624868, recall 0.886897
2017-12-10T15:31:37.925318: step 2474, loss 0.619638, acc 0.90625, prec 0.0625038, recall 0.886945
2017-12-10T15:31:38.126090: step 2475, loss 0.134344, acc 0.921875, prec 0.0625225, recall 0.886994
2017-12-10T15:31:38.324595: step 2476, loss 0.128639, acc 0.921875, prec 0.0625132, recall 0.886994
2017-12-10T15:31:38.520746: step 2477, loss 0.186063, acc 0.96875, prec 0.0625939, recall 0.887138
2017-12-10T15:31:38.717592: step 2478, loss 0.205932, acc 0.890625, prec 0.0625807, recall 0.887138
2017-12-10T15:31:38.916914: step 2479, loss 0.0800491, acc 0.984375, prec 0.0625789, recall 0.887138
2017-12-10T15:31:39.114138: step 2480, loss 0.103795, acc 0.953125, prec 0.0626014, recall 0.887186
2017-12-10T15:31:39.310196: step 2481, loss 0.0859297, acc 0.984375, prec 0.0625995, recall 0.887186
2017-12-10T15:31:39.511247: step 2482, loss 0.145863, acc 0.953125, prec 0.0625939, recall 0.887186
2017-12-10T15:31:39.712032: step 2483, loss 0.0878141, acc 0.953125, prec 0.0625882, recall 0.887186
2017-12-10T15:31:39.910653: step 2484, loss 0.129161, acc 0.953125, prec 0.0625826, recall 0.887186
2017-12-10T15:31:40.085962: step 2485, loss 0.39056, acc 0.923077, prec 0.0626032, recall 0.887234
2017-12-10T15:31:40.290607: step 2486, loss 0.221517, acc 0.96875, prec 0.0626276, recall 0.887282
2017-12-10T15:31:40.482638: step 2487, loss 1.17246, acc 0.984375, prec 0.062682, recall 0.887378
2017-12-10T15:31:40.684587: step 2488, loss 0.152508, acc 0.921875, prec 0.0626726, recall 0.887378
2017-12-10T15:31:40.882826: step 2489, loss 0.193516, acc 0.9375, prec 0.0626932, recall 0.887426
2017-12-10T15:31:41.082898: step 2490, loss 0.188476, acc 0.984375, prec 0.0627194, recall 0.887473
2017-12-10T15:31:41.277993: step 2491, loss 0.0975376, acc 0.984375, prec 0.0627176, recall 0.887473
2017-12-10T15:31:41.474039: step 2492, loss 0.145449, acc 0.9375, prec 0.0627663, recall 0.887569
2017-12-10T15:31:41.672625: step 2493, loss 0.0597776, acc 0.984375, prec 0.0627644, recall 0.887569
2017-12-10T15:31:41.864131: step 2494, loss 0.138203, acc 0.953125, prec 0.0627587, recall 0.887569
2017-12-10T15:31:42.059815: step 2495, loss 0.0722549, acc 0.96875, prec 0.0628112, recall 0.887664
2017-12-10T15:31:42.256962: step 2496, loss 0.26035, acc 0.96875, prec 0.0628074, recall 0.887664
2017-12-10T15:31:42.454768: step 2497, loss 0.0581307, acc 0.984375, prec 0.0628056, recall 0.887664
2017-12-10T15:31:42.650727: step 2498, loss 0.179577, acc 0.921875, prec 0.0628523, recall 0.887759
2017-12-10T15:31:42.852484: step 2499, loss 0.147534, acc 0.96875, prec 0.0628767, recall 0.887807
2017-12-10T15:31:43.046460: step 2500, loss 0.435856, acc 0.90625, prec 0.0628935, recall 0.887854
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_2/1512937383/checkpoints/model-2500

2017-12-10T15:31:44.365174: step 2501, loss 0.1504, acc 0.90625, prec 0.0629102, recall 0.887902
2017-12-10T15:31:44.563314: step 2502, loss 0.123349, acc 0.953125, prec 0.0629327, recall 0.887949
2017-12-10T15:31:44.765362: step 2503, loss 0.0715402, acc 0.96875, prec 0.0629289, recall 0.887949
2017-12-10T15:31:44.964121: step 2504, loss 0.063117, acc 1, prec 0.062957, recall 0.887997
2017-12-10T15:31:45.168841: step 2505, loss 0.193988, acc 0.9375, prec 0.0629775, recall 0.888044
2017-12-10T15:31:45.365928: step 2506, loss 0.10054, acc 0.9375, prec 0.06297, recall 0.888044
2017-12-10T15:31:45.570477: step 2507, loss 0.113399, acc 0.953125, prec 0.0629643, recall 0.888044
2017-12-10T15:31:45.762847: step 2508, loss 0.441482, acc 0.90625, prec 0.062981, recall 0.888091
2017-12-10T15:31:45.961243: step 2509, loss 0.116877, acc 0.9375, prec 0.0629735, recall 0.888091
2017-12-10T15:31:46.157646: step 2510, loss 0.0474427, acc 0.984375, prec 0.0629716, recall 0.888091
2017-12-10T15:31:46.353155: step 2511, loss 0.305471, acc 0.9375, prec 0.0630202, recall 0.888186
2017-12-10T15:31:46.554225: step 2512, loss 0.166162, acc 1, prec 0.0630763, recall 0.88828
2017-12-10T15:31:46.758791: step 2513, loss 0.0203783, acc 1, prec 0.0631043, recall 0.888327
2017-12-10T15:31:46.955457: step 2514, loss 0.454182, acc 0.953125, prec 0.0631267, recall 0.888374
2017-12-10T15:31:47.156966: step 2515, loss 0.189722, acc 0.9375, prec 0.0631191, recall 0.888374
2017-12-10T15:31:47.357135: step 2516, loss 0.135215, acc 0.9375, prec 0.0631677, recall 0.888468
2017-12-10T15:31:47.555245: step 2517, loss 0.0646334, acc 0.96875, prec 0.0631639, recall 0.888468
2017-12-10T15:31:47.749955: step 2518, loss 0.199716, acc 0.921875, prec 0.0631544, recall 0.888468
2017-12-10T15:31:47.947698: step 2519, loss 0.195491, acc 0.96875, prec 0.0632067, recall 0.888562
2017-12-10T15:31:48.147600: step 2520, loss 0.0372744, acc 0.984375, prec 0.0632328, recall 0.888609
2017-12-10T15:31:48.346816: step 2521, loss 0.0731991, acc 0.953125, prec 0.0632272, recall 0.888609
2017-12-10T15:31:48.542497: step 2522, loss 2.23715, acc 0.875, prec 0.0632139, recall 0.888235
2017-12-10T15:31:48.744345: step 2523, loss 0.264334, acc 0.953125, prec 0.0632923, recall 0.888376
2017-12-10T15:31:48.946828: step 2524, loss 0.160303, acc 0.953125, prec 0.0632866, recall 0.888376
2017-12-10T15:31:49.147718: step 2525, loss 0.19674, acc 0.953125, prec 0.0633089, recall 0.888423
2017-12-10T15:31:49.342984: step 2526, loss 0.404746, acc 0.921875, prec 0.0633554, recall 0.888516
2017-12-10T15:31:49.543431: step 2527, loss 0.146985, acc 0.9375, prec 0.0633479, recall 0.888516
2017-12-10T15:31:49.738980: step 2528, loss 0.593251, acc 0.921875, prec 0.0633944, recall 0.88861
2017-12-10T15:31:49.940548: step 2529, loss 0.167769, acc 0.9375, prec 0.0633868, recall 0.88861
2017-12-10T15:31:50.139226: step 2530, loss 0.118293, acc 0.9375, prec 0.0633792, recall 0.88861
2017-12-10T15:31:50.334096: step 2531, loss 0.26562, acc 0.96875, prec 0.0634314, recall 0.888703
2017-12-10T15:31:50.529595: step 2532, loss 0.376851, acc 0.890625, prec 0.0634181, recall 0.888703
2017-12-10T15:31:50.726984: step 2533, loss 0.177865, acc 0.9375, prec 0.0634665, recall 0.888796
2017-12-10T15:31:50.920402: step 2534, loss 0.371418, acc 0.921875, prec 0.063485, recall 0.888842
2017-12-10T15:31:51.116721: step 2535, loss 0.194832, acc 0.921875, prec 0.0634755, recall 0.888842
2017-12-10T15:31:51.314339: step 2536, loss 0.139916, acc 0.96875, prec 0.0634717, recall 0.888842
2017-12-10T15:31:51.509116: step 2537, loss 1.14684, acc 0.921875, prec 0.0635181, recall 0.888935
2017-12-10T15:31:51.707851: step 2538, loss 0.174095, acc 0.90625, prec 0.0635067, recall 0.888935
2017-12-10T15:31:51.908957: step 2539, loss 1.8631, acc 0.953125, prec 0.0635588, recall 0.888657
2017-12-10T15:31:52.108465: step 2540, loss 0.16865, acc 0.953125, prec 0.0635811, recall 0.888704
2017-12-10T15:31:52.303527: step 2541, loss 0.493843, acc 0.828125, prec 0.0635881, recall 0.88875
2017-12-10T15:31:52.505312: step 2542, loss 0.352569, acc 0.921875, prec 0.0636345, recall 0.888843
2017-12-10T15:31:52.699608: step 2543, loss 0.744188, acc 0.796875, prec 0.0636656, recall 0.888935
2017-12-10T15:31:52.895097: step 2544, loss 0.653382, acc 0.796875, prec 0.0636967, recall 0.889027
2017-12-10T15:31:53.088038: step 2545, loss 0.426382, acc 0.84375, prec 0.0637056, recall 0.889074
2017-12-10T15:31:53.280074: step 2546, loss 0.489827, acc 0.890625, prec 0.0636924, recall 0.889074
2017-12-10T15:31:53.481015: step 2547, loss 0.752154, acc 0.765625, prec 0.0636918, recall 0.88912
2017-12-10T15:31:53.678415: step 2548, loss 0.58004, acc 0.828125, prec 0.063671, recall 0.88912
2017-12-10T15:31:53.871347: step 2549, loss 0.403691, acc 0.859375, prec 0.0636818, recall 0.889166
2017-12-10T15:31:54.067369: step 2550, loss 0.347407, acc 0.890625, prec 0.0636685, recall 0.889166
2017-12-10T15:31:54.261679: step 2551, loss 0.445569, acc 0.828125, prec 0.0636477, recall 0.889166
2017-12-10T15:31:54.457754: step 2552, loss 0.101414, acc 0.953125, prec 0.0636977, recall 0.889258
2017-12-10T15:31:54.653050: step 2553, loss 0.319105, acc 0.953125, prec 0.0637198, recall 0.889304
2017-12-10T15:31:54.847415: step 2554, loss 0.14726, acc 0.9375, prec 0.0637401, recall 0.889349
2017-12-10T15:31:55.045365: step 2555, loss 0.101289, acc 0.953125, prec 0.0637344, recall 0.889349
2017-12-10T15:31:55.249772: step 2556, loss 1.08664, acc 0.9375, prec 0.0637824, recall 0.889441
2017-12-10T15:31:55.444789: step 2557, loss 0.391564, acc 0.875, prec 0.0638228, recall 0.889533
2017-12-10T15:31:55.639483: step 2558, loss 0.348455, acc 0.84375, prec 0.0638039, recall 0.889533
2017-12-10T15:31:55.835676: step 2559, loss 0.129216, acc 0.96875, prec 0.0638001, recall 0.889533
2017-12-10T15:31:56.032838: step 2560, loss 0.418497, acc 0.90625, prec 0.0638443, recall 0.889624
2017-12-10T15:31:56.229689: step 2561, loss 0.292978, acc 0.921875, prec 0.0638626, recall 0.889669
2017-12-10T15:31:56.427615: step 2562, loss 0.075313, acc 0.953125, prec 0.0638569, recall 0.889669
2017-12-10T15:31:56.633598: step 2563, loss 0.16113, acc 0.9375, prec 0.0638771, recall 0.889715
2017-12-10T15:31:56.826886: step 2564, loss 0.113237, acc 0.9375, prec 0.0638695, recall 0.889715
2017-12-10T15:31:57.024044: step 2565, loss 0.152557, acc 0.9375, prec 0.063862, recall 0.889715
2017-12-10T15:31:57.220796: step 2566, loss 0.529267, acc 0.953125, prec 0.0639673, recall 0.889897
2017-12-10T15:31:57.425647: step 2567, loss 0.0959351, acc 0.953125, prec 0.0640171, recall 0.889988
2017-12-10T15:31:57.627953: step 2568, loss 0.0523482, acc 0.984375, prec 0.0640152, recall 0.889988
2017-12-10T15:31:57.824167: step 2569, loss 0.116638, acc 0.96875, prec 0.0640114, recall 0.889988
2017-12-10T15:31:58.019897: step 2570, loss 0.23987, acc 0.96875, prec 0.0640353, recall 0.890033
2017-12-10T15:31:58.217908: step 2571, loss 0.0948166, acc 0.9375, prec 0.0640277, recall 0.890033
2017-12-10T15:31:58.413487: step 2572, loss 0.188265, acc 0.984375, prec 0.0640536, recall 0.890078
2017-12-10T15:31:58.606998: step 2573, loss 0.0349611, acc 1, prec 0.064109, recall 0.890169
2017-12-10T15:31:58.803579: step 2574, loss 0.122405, acc 0.96875, prec 0.0641329, recall 0.890214
2017-12-10T15:31:59.003033: step 2575, loss 2.02131, acc 0.953125, prec 0.0641291, recall 0.889848
2017-12-10T15:31:59.211313: step 2576, loss 1.61794, acc 0.96875, prec 0.0641272, recall 0.889482
2017-12-10T15:31:59.420617: step 2577, loss 0.0383811, acc 0.984375, prec 0.0641254, recall 0.889482
2017-12-10T15:31:59.620190: step 2578, loss 0.0467234, acc 0.984375, prec 0.0641512, recall 0.889528
2017-12-10T15:31:59.824877: step 2579, loss 0.235826, acc 0.921875, prec 0.0641971, recall 0.889618
2017-12-10T15:32:00.020957: step 2580, loss 0.368831, acc 0.90625, prec 0.0642688, recall 0.889754
2017-12-10T15:32:00.217759: step 2581, loss 0.100295, acc 0.9375, prec 0.0642889, recall 0.889799
2017-12-10T15:32:00.414010: step 2582, loss 0.281161, acc 0.90625, prec 0.0642775, recall 0.889799
2017-12-10T15:32:00.608825: step 2583, loss 0.275739, acc 0.9375, prec 0.0642699, recall 0.889799
2017-12-10T15:32:00.805559: step 2584, loss 0.236413, acc 0.953125, prec 0.0642918, recall 0.889844
2017-12-10T15:32:01.002339: step 2585, loss 0.180998, acc 0.921875, prec 0.0642823, recall 0.889844
2017-12-10T15:32:01.205961: step 2586, loss 0.184579, acc 0.9375, prec 0.0643024, recall 0.889889
2017-12-10T15:32:01.407796: step 2587, loss 0.27952, acc 0.921875, prec 0.0643206, recall 0.889935
2017-12-10T15:32:01.608719: step 2588, loss 0.609368, acc 0.953125, prec 0.0643702, recall 0.890025
2017-12-10T15:32:01.805638: step 2589, loss 0.134265, acc 0.9375, prec 0.0644179, recall 0.890114
2017-12-10T15:32:02.001943: step 2590, loss 0.21857, acc 0.953125, prec 0.0644398, recall 0.890159
2017-12-10T15:32:02.202349: step 2591, loss 0.147427, acc 0.890625, prec 0.0644265, recall 0.890159
2017-12-10T15:32:02.397572: step 2592, loss 0.331197, acc 0.875, prec 0.0644666, recall 0.890249
2017-12-10T15:32:02.596637: step 2593, loss 0.340983, acc 0.890625, prec 0.0644809, recall 0.890294
2017-12-10T15:32:02.791532: step 2594, loss 0.156122, acc 0.9375, prec 0.0645285, recall 0.890383
2017-12-10T15:32:02.987219: step 2595, loss 0.305426, acc 0.890625, prec 0.064598, recall 0.890517
2017-12-10T15:32:03.184323: step 2596, loss 0.306029, acc 0.890625, prec 0.0646123, recall 0.890561
2017-12-10T15:32:03.381224: step 2597, loss 0.233328, acc 0.953125, prec 0.0646618, recall 0.89065
2017-12-10T15:32:03.575415: step 2598, loss 0.0716072, acc 0.984375, prec 0.0646599, recall 0.89065
2017-12-10T15:32:03.774300: step 2599, loss 0.126861, acc 0.921875, prec 0.0646779, recall 0.890695
2017-12-10T15:32:03.966987: step 2600, loss 0.269867, acc 0.90625, prec 0.0646665, recall 0.890695
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_2/1512937383/checkpoints/model-2600

2017-12-10T15:32:06.641781: step 2601, loss 0.251169, acc 0.96875, prec 0.0646903, recall 0.890739
2017-12-10T15:32:06.839084: step 2602, loss 0.225855, acc 0.890625, prec 0.0647045, recall 0.890784
2017-12-10T15:32:07.033386: step 2603, loss 0.0981191, acc 0.921875, prec 0.0647225, recall 0.890828
2017-12-10T15:32:07.228950: step 2604, loss 0.111966, acc 0.953125, prec 0.064772, recall 0.890916
2017-12-10T15:32:07.425498: step 2605, loss 0.0752588, acc 0.96875, prec 0.0647681, recall 0.890916
2017-12-10T15:32:07.622221: step 2606, loss 0.126469, acc 0.953125, prec 0.0647624, recall 0.890916
2017-12-10T15:32:07.815454: step 2607, loss 0.0825379, acc 0.96875, prec 0.0647586, recall 0.890916
2017-12-10T15:32:08.012929: step 2608, loss 0.0493347, acc 0.96875, prec 0.0647548, recall 0.890916
2017-12-10T15:32:08.209538: step 2609, loss 0.056518, acc 0.984375, prec 0.0647529, recall 0.890916
2017-12-10T15:32:08.411980: step 2610, loss 0.0229678, acc 1, prec 0.0647529, recall 0.890916
2017-12-10T15:32:08.607442: step 2611, loss 0.2929, acc 0.9375, prec 0.0647728, recall 0.890961
2017-12-10T15:32:08.807443: step 2612, loss 0.259366, acc 0.9375, prec 0.0647927, recall 0.891005
2017-12-10T15:32:09.006221: step 2613, loss 4.66609, acc 0.9375, prec 0.0648145, recall 0.890688
2017-12-10T15:32:09.201777: step 2614, loss 0.204746, acc 0.96875, prec 0.0648383, recall 0.890732
2017-12-10T15:32:09.401615: step 2615, loss 0.116932, acc 0.953125, prec 0.0648601, recall 0.890777
2017-12-10T15:32:09.598480: step 2616, loss 0.218835, acc 0.921875, prec 0.0648781, recall 0.890821
2017-12-10T15:32:09.792964: step 2617, loss 0.464625, acc 0.96875, prec 0.0649018, recall 0.890865
2017-12-10T15:32:09.992081: step 2618, loss 0.0831653, acc 0.984375, prec 0.0649274, recall 0.890909
2017-12-10T15:32:10.185179: step 2619, loss 0.196842, acc 0.9375, prec 0.0650024, recall 0.891041
2017-12-10T15:32:10.381185: step 2620, loss 0.301699, acc 0.890625, prec 0.064989, recall 0.891041
2017-12-10T15:32:10.586081: step 2621, loss 0.36416, acc 0.921875, prec 0.0650069, recall 0.891085
2017-12-10T15:32:10.783704: step 2622, loss 0.192317, acc 0.9375, prec 0.0650268, recall 0.891129
2017-12-10T15:32:10.980100: step 2623, loss 0.176062, acc 0.90625, prec 0.0650428, recall 0.891173
2017-12-10T15:32:11.178643: step 2624, loss 0.936421, acc 0.859375, prec 0.0650531, recall 0.891217
2017-12-10T15:32:11.373064: step 2625, loss 0.476359, acc 0.875, prec 0.0650378, recall 0.891217
2017-12-10T15:32:11.569541: step 2626, loss 0.524229, acc 0.828125, prec 0.0650992, recall 0.891348
2017-12-10T15:32:11.762587: step 2627, loss 0.458603, acc 0.90625, prec 0.0651427, recall 0.891435
2017-12-10T15:32:11.959308: step 2628, loss 0.233943, acc 0.9375, prec 0.0651625, recall 0.891479
2017-12-10T15:32:12.155995: step 2629, loss 0.151979, acc 0.921875, prec 0.0651804, recall 0.891523
2017-12-10T15:32:12.353100: step 2630, loss 0.240165, acc 0.875, prec 0.0651925, recall 0.891566
2017-12-10T15:32:12.553223: step 2631, loss 0.290682, acc 0.921875, prec 0.0652104, recall 0.89161
2017-12-10T15:32:12.747004: step 2632, loss 0.265986, acc 0.90625, prec 0.0651989, recall 0.89161
2017-12-10T15:32:12.939420: step 2633, loss 0.184667, acc 0.921875, prec 0.0651893, recall 0.89161
2017-12-10T15:32:13.133891: step 2634, loss 0.413794, acc 0.84375, prec 0.0651976, recall 0.891653
2017-12-10T15:32:13.329292: step 2635, loss 0.0944568, acc 0.96875, prec 0.0651938, recall 0.891653
2017-12-10T15:32:13.522316: step 2636, loss 0.43182, acc 0.875, prec 0.0651785, recall 0.891653
2017-12-10T15:32:13.724596: step 2637, loss 0.109055, acc 0.953125, prec 0.0652002, recall 0.891697
2017-12-10T15:32:13.923599: step 2638, loss 0.0664604, acc 0.984375, prec 0.0652257, recall 0.89174
2017-12-10T15:32:14.122465: step 2639, loss 0.0981221, acc 0.984375, prec 0.0652238, recall 0.89174
2017-12-10T15:32:14.327052: step 2640, loss 0.166135, acc 0.96875, prec 0.0652199, recall 0.89174
2017-12-10T15:32:14.520417: step 2641, loss 0.157854, acc 0.984375, prec 0.065218, recall 0.89174
2017-12-10T15:32:14.714126: step 2642, loss 0.187662, acc 0.921875, prec 0.0652085, recall 0.89174
2017-12-10T15:32:14.910520: step 2643, loss 0.450022, acc 0.96875, prec 0.0652595, recall 0.891827
2017-12-10T15:32:15.110700: step 2644, loss 0.202013, acc 0.96875, prec 0.065283, recall 0.89187
2017-12-10T15:32:15.306006: step 2645, loss 0.0535723, acc 0.96875, prec 0.0653066, recall 0.891914
2017-12-10T15:32:15.500857: step 2646, loss 0.589682, acc 0.9375, prec 0.0653263, recall 0.891957
2017-12-10T15:32:15.702616: step 2647, loss 0.0966773, acc 0.9375, prec 0.0653187, recall 0.891957
2017-12-10T15:32:15.899221: step 2648, loss 0.392935, acc 0.9375, prec 0.0653384, recall 0.892
2017-12-10T15:32:16.091864: step 2649, loss 0.194421, acc 0.953125, prec 0.0653874, recall 0.892086
2017-12-10T15:32:16.292317: step 2650, loss 0.0818309, acc 0.96875, prec 0.0653836, recall 0.892086
2017-12-10T15:32:16.488221: step 2651, loss 0.102725, acc 1, prec 0.0654384, recall 0.892173
2017-12-10T15:32:16.683528: step 2652, loss 0.122063, acc 0.96875, prec 0.0654345, recall 0.892173
2017-12-10T15:32:16.880738: step 2653, loss 0.78742, acc 1, prec 0.0654893, recall 0.892259
2017-12-10T15:32:17.075494: step 2654, loss 0.181447, acc 0.984375, prec 0.0655147, recall 0.892302
2017-12-10T15:32:17.269176: step 2655, loss 0.338328, acc 0.96875, prec 0.0655382, recall 0.892344
2017-12-10T15:32:17.469699: step 2656, loss 0.0671341, acc 0.984375, prec 0.0655363, recall 0.892344
2017-12-10T15:32:17.666920: step 2657, loss 0.0475045, acc 0.984375, prec 0.0655344, recall 0.892344
2017-12-10T15:32:17.859662: step 2658, loss 0.194476, acc 0.953125, prec 0.065556, recall 0.892387
2017-12-10T15:32:18.057216: step 2659, loss 0.115453, acc 0.96875, prec 0.0655795, recall 0.89243
2017-12-10T15:32:18.255439: step 2660, loss 0.23287, acc 0.953125, prec 0.0656558, recall 0.892559
2017-12-10T15:32:18.451365: step 2661, loss 0.196337, acc 0.953125, prec 0.0657321, recall 0.892687
2017-12-10T15:32:18.647933: step 2662, loss 0.368446, acc 0.921875, prec 0.0657772, recall 0.892772
2017-12-10T15:32:18.842401: step 2663, loss 0.382961, acc 0.921875, prec 0.0658495, recall 0.8929
2017-12-10T15:32:19.036969: step 2664, loss 0.415621, acc 0.921875, prec 0.0658672, recall 0.892942
2017-12-10T15:32:19.236213: step 2665, loss 2.3521, acc 0.96875, prec 0.0658926, recall 0.892631
2017-12-10T15:32:19.434524: step 2666, loss 0.195676, acc 0.90625, prec 0.0659084, recall 0.892673
2017-12-10T15:32:19.631460: step 2667, loss 0.164563, acc 0.984375, prec 0.0659884, recall 0.892801
2017-12-10T15:32:19.829802: step 2668, loss 0.229613, acc 0.9375, prec 0.0660079, recall 0.892843
2017-12-10T15:32:20.023895: step 2669, loss 0.311393, acc 0.84375, prec 0.066016, recall 0.892885
2017-12-10T15:32:20.216800: step 2670, loss 0.470889, acc 0.84375, prec 0.0660785, recall 0.893012
2017-12-10T15:32:20.414282: step 2671, loss 0.271833, acc 0.90625, prec 0.0660942, recall 0.893054
2017-12-10T15:32:20.611618: step 2672, loss 0.473571, acc 0.828125, prec 0.0661003, recall 0.893097
2017-12-10T15:32:20.810110: step 2673, loss 0.373765, acc 0.890625, prec 0.0661413, recall 0.893181
2017-12-10T15:32:21.013913: step 2674, loss 0.568418, acc 0.828125, prec 0.0661745, recall 0.893265
2017-12-10T15:32:21.212598: step 2675, loss 0.387605, acc 0.875, prec 0.0661591, recall 0.893265
2017-12-10T15:32:21.407224: step 2676, loss 0.365269, acc 0.828125, prec 0.0661651, recall 0.893307
2017-12-10T15:32:21.600440: step 2677, loss 0.485118, acc 0.875, prec 0.0661497, recall 0.893307
2017-12-10T15:32:21.798045: step 2678, loss 0.284408, acc 0.890625, prec 0.0661906, recall 0.893391
2017-12-10T15:32:21.996170: step 2679, loss 0.434766, acc 0.859375, prec 0.0661733, recall 0.893391
2017-12-10T15:32:22.193445: step 2680, loss 0.233488, acc 0.921875, prec 0.0661908, recall 0.893433
2017-12-10T15:32:22.389631: step 2681, loss 0.290077, acc 0.890625, prec 0.0661773, recall 0.893433
2017-12-10T15:32:22.581953: step 2682, loss 0.28677, acc 0.859375, prec 0.0661872, recall 0.893475
2017-12-10T15:32:22.777951: step 2683, loss 0.76747, acc 0.875, prec 0.0661989, recall 0.893517
2017-12-10T15:32:22.974121: step 2684, loss 0.328805, acc 0.90625, prec 0.0662146, recall 0.893559
2017-12-10T15:32:23.173684: step 2685, loss 0.320913, acc 0.875, prec 0.0662535, recall 0.893642
2017-12-10T15:32:23.375918: step 2686, loss 0.157294, acc 0.921875, prec 0.066271, recall 0.893684
2017-12-10T15:32:23.577778: step 2687, loss 0.197642, acc 0.921875, prec 0.0662885, recall 0.893726
2017-12-10T15:32:23.775434: step 2688, loss 0.0684523, acc 0.984375, prec 0.0663138, recall 0.893767
2017-12-10T15:32:23.974942: step 2689, loss 0.258501, acc 0.921875, prec 0.0663313, recall 0.893809
2017-12-10T15:32:24.176615: step 2690, loss 0.231455, acc 0.953125, prec 0.0663526, recall 0.89385
2017-12-10T15:32:24.370951: step 2691, loss 0.0269193, acc 1, prec 0.0663798, recall 0.893892
2017-12-10T15:32:24.565577: step 2692, loss 0.203976, acc 0.9375, prec 0.0663721, recall 0.893892
2017-12-10T15:32:24.770583: step 2693, loss 0.0535943, acc 0.96875, prec 0.0663682, recall 0.893892
2017-12-10T15:32:24.967849: step 2694, loss 0.537128, acc 0.984375, prec 0.0664206, recall 0.893975
2017-12-10T15:32:25.170585: step 2695, loss 0.0509355, acc 1, prec 0.0664748, recall 0.894058
2017-12-10T15:32:25.364844: step 2696, loss 0.512084, acc 0.984375, prec 0.0665, recall 0.894099
2017-12-10T15:32:25.567077: step 2697, loss 0.0317445, acc 0.984375, prec 0.0664981, recall 0.894099
2017-12-10T15:32:25.763832: step 2698, loss 0.0372216, acc 1, prec 0.0665252, recall 0.894141
2017-12-10T15:32:25.961399: step 2699, loss 0.166516, acc 0.9375, prec 0.0665175, recall 0.894141
2017-12-10T15:32:26.158270: step 2700, loss 1.55128, acc 0.96875, prec 0.066595, recall 0.894265
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_2/1512937383/checkpoints/model-2700

2017-12-10T15:32:27.825470: step 2701, loss 0.0834745, acc 0.9375, prec 0.0665873, recall 0.894265
2017-12-10T15:32:28.020422: step 2702, loss 0.206643, acc 0.890625, prec 0.0666008, recall 0.894306
2017-12-10T15:32:28.216182: step 2703, loss 0.207045, acc 0.9375, prec 0.0666202, recall 0.894347
2017-12-10T15:32:28.413570: step 2704, loss 0.166787, acc 0.953125, prec 0.0666415, recall 0.894388
2017-12-10T15:32:28.610820: step 2705, loss 0.195655, acc 0.9375, prec 0.0666338, recall 0.894388
2017-12-10T15:32:28.809523: step 2706, loss 0.189593, acc 0.984375, prec 0.066686, recall 0.89447
2017-12-10T15:32:29.002166: step 2707, loss 0.127349, acc 0.96875, prec 0.0666822, recall 0.89447
2017-12-10T15:32:29.203525: step 2708, loss 0.201585, acc 0.921875, prec 0.0666725, recall 0.89447
2017-12-10T15:32:29.409213: step 2709, loss 0.375615, acc 0.90625, prec 0.0667692, recall 0.894635
2017-12-10T15:32:29.608981: step 2710, loss 0.225369, acc 0.890625, prec 0.0667827, recall 0.894675
2017-12-10T15:32:29.802309: step 2711, loss 0.336823, acc 0.890625, prec 0.0667692, recall 0.894675
2017-12-10T15:32:30.002135: step 2712, loss 0.331598, acc 0.859375, prec 0.0667517, recall 0.894675
2017-12-10T15:32:30.197397: step 2713, loss 0.440721, acc 0.90625, prec 0.0667942, recall 0.894757
2017-12-10T15:32:30.392686: step 2714, loss 0.428018, acc 0.875, prec 0.0668599, recall 0.89488
2017-12-10T15:32:30.589856: step 2715, loss 0.590106, acc 0.8125, prec 0.0668366, recall 0.89488
2017-12-10T15:32:30.782727: step 2716, loss 0.152746, acc 0.9375, prec 0.0668289, recall 0.89488
2017-12-10T15:32:30.977952: step 2717, loss 0.180511, acc 0.953125, prec 0.0668231, recall 0.89488
2017-12-10T15:32:31.179499: step 2718, loss 0.291305, acc 0.96875, prec 0.0668733, recall 0.894961
2017-12-10T15:32:31.383399: step 2719, loss 0.254929, acc 0.890625, prec 0.0669408, recall 0.895083
2017-12-10T15:32:31.588349: step 2720, loss 0.233023, acc 0.90625, prec 0.0669561, recall 0.895124
2017-12-10T15:32:31.783016: step 2721, loss 0.223236, acc 0.921875, prec 0.0669465, recall 0.895124
2017-12-10T15:32:31.979674: step 2722, loss 0.550052, acc 0.96875, prec 0.0669696, recall 0.895164
2017-12-10T15:32:32.175485: step 2723, loss 0.337637, acc 0.921875, prec 0.0669869, recall 0.895205
2017-12-10T15:32:32.377870: step 2724, loss 0.195152, acc 0.921875, prec 0.0669772, recall 0.895205
2017-12-10T15:32:32.573776: step 2725, loss 0.444825, acc 0.890625, prec 0.0669636, recall 0.895205
2017-12-10T15:32:32.775139: step 2726, loss 0.179422, acc 0.953125, prec 0.0669578, recall 0.895205
2017-12-10T15:32:32.972135: step 2727, loss 0.106288, acc 0.921875, prec 0.0669481, recall 0.895205
2017-12-10T15:32:33.171043: step 2728, loss 1.2302, acc 0.953125, prec 0.0669963, recall 0.895286
2017-12-10T15:32:33.373451: step 2729, loss 0.273489, acc 0.9375, prec 0.0670155, recall 0.895326
2017-12-10T15:32:33.569219: step 2730, loss 0.150496, acc 0.9375, prec 0.0670617, recall 0.895407
2017-12-10T15:32:33.769184: step 2731, loss 0.140853, acc 0.9375, prec 0.067054, recall 0.895407
2017-12-10T15:32:33.966958: step 2732, loss 0.224391, acc 0.96875, prec 0.067077, recall 0.895448
2017-12-10T15:32:34.164740: step 2733, loss 0.217744, acc 0.953125, prec 0.0670712, recall 0.895448
2017-12-10T15:32:34.365866: step 2734, loss 0.157674, acc 0.9375, prec 0.0670635, recall 0.895448
2017-12-10T15:32:34.565267: step 2735, loss 0.306018, acc 0.9375, prec 0.0670827, recall 0.895488
2017-12-10T15:32:34.762261: step 2736, loss 0.129932, acc 0.953125, prec 0.0671038, recall 0.895528
2017-12-10T15:32:34.952955: step 2737, loss 0.651869, acc 0.9375, prec 0.06715, recall 0.895609
2017-12-10T15:32:35.152099: step 2738, loss 0.25665, acc 0.953125, prec 0.0671711, recall 0.895649
2017-12-10T15:32:35.346296: step 2739, loss 0.0518578, acc 1, prec 0.0671711, recall 0.895649
2017-12-10T15:32:35.544811: step 2740, loss 0.138478, acc 0.921875, prec 0.0671614, recall 0.895649
2017-12-10T15:32:35.743884: step 2741, loss 0.247481, acc 0.859375, prec 0.0671439, recall 0.895649
2017-12-10T15:32:35.945614: step 2742, loss 2.46221, acc 0.921875, prec 0.0671631, recall 0.895344
2017-12-10T15:32:36.151182: step 2743, loss 0.0775445, acc 0.96875, prec 0.0671592, recall 0.895344
2017-12-10T15:32:36.349154: step 2744, loss 0.107834, acc 0.953125, prec 0.0672072, recall 0.895425
2017-12-10T15:32:36.543677: step 2745, loss 0.146193, acc 0.953125, prec 0.0672014, recall 0.895425
2017-12-10T15:32:36.736986: step 2746, loss 0.252206, acc 0.9375, prec 0.0671937, recall 0.895425
2017-12-10T15:32:36.932394: step 2747, loss 0.144971, acc 0.953125, prec 0.0671879, recall 0.895425
2017-12-10T15:32:37.129530: step 2748, loss 0.343867, acc 0.90625, prec 0.0671762, recall 0.895425
2017-12-10T15:32:37.323461: step 2749, loss 0.268071, acc 0.921875, prec 0.0671934, recall 0.895465
2017-12-10T15:32:37.514742: step 2750, loss 0.399395, acc 0.90625, prec 0.0672356, recall 0.895545
2017-12-10T15:32:37.714600: step 2751, loss 0.13367, acc 0.953125, prec 0.0672567, recall 0.895585
2017-12-10T15:32:37.914319: step 2752, loss 0.275527, acc 0.9375, prec 0.0673027, recall 0.895666
2017-12-10T15:32:38.109404: step 2753, loss 0.272621, acc 0.875, prec 0.0672872, recall 0.895666
2017-12-10T15:32:38.307195: step 2754, loss 0.27268, acc 0.9375, prec 0.0673063, recall 0.895706
2017-12-10T15:32:38.504935: step 2755, loss 0.113265, acc 0.96875, prec 0.0673562, recall 0.895785
2017-12-10T15:32:38.700943: step 2756, loss 0.138309, acc 0.953125, prec 0.0674041, recall 0.895865
2017-12-10T15:32:38.899097: step 2757, loss 0.355363, acc 0.890625, prec 0.0674711, recall 0.895985
2017-12-10T15:32:39.096845: step 2758, loss 0.198864, acc 0.953125, prec 0.0674921, recall 0.896024
2017-12-10T15:32:39.292342: step 2759, loss 0.177431, acc 0.984375, prec 0.067517, recall 0.896064
2017-12-10T15:32:39.493773: step 2760, loss 0.225234, acc 0.953125, prec 0.067538, recall 0.896104
2017-12-10T15:32:39.693655: step 2761, loss 0.119963, acc 0.9375, prec 0.0675302, recall 0.896104
2017-12-10T15:32:39.892526: step 2762, loss 0.0826163, acc 0.953125, prec 0.0675244, recall 0.896104
2017-12-10T15:32:40.087171: step 2763, loss 0.150867, acc 0.9375, prec 0.0675166, recall 0.896104
2017-12-10T15:32:40.282831: step 2764, loss 0.159187, acc 0.984375, prec 0.0675415, recall 0.896144
2017-12-10T15:32:40.483495: step 2765, loss 0.208159, acc 0.921875, prec 0.0675318, recall 0.896144
2017-12-10T15:32:40.681602: step 2766, loss 0.217258, acc 0.96875, prec 0.0675816, recall 0.896223
2017-12-10T15:32:40.879412: step 2767, loss 1.63495, acc 0.96875, prec 0.0676601, recall 0.896
2017-12-10T15:32:41.082269: step 2768, loss 0.172226, acc 0.9375, prec 0.0676791, recall 0.89604
2017-12-10T15:32:41.281535: step 2769, loss 0.133815, acc 0.9375, prec 0.067725, recall 0.896119
2017-12-10T15:32:41.480196: step 2770, loss 0.0289517, acc 1, prec 0.067725, recall 0.896119
2017-12-10T15:32:41.679125: step 2771, loss 0.188284, acc 0.921875, prec 0.0677152, recall 0.896119
2017-12-10T15:32:41.874327: step 2772, loss 0.389603, acc 0.875, prec 0.0677264, recall 0.896158
2017-12-10T15:32:42.072471: step 2773, loss 0.215189, acc 0.9375, prec 0.0677723, recall 0.896237
2017-12-10T15:32:42.267699: step 2774, loss 0.209387, acc 0.9375, prec 0.0677913, recall 0.896277
2017-12-10T15:32:42.463795: step 2775, loss 0.304144, acc 0.890625, prec 0.0677776, recall 0.896277
2017-12-10T15:32:42.660732: step 2776, loss 0.134687, acc 0.984375, prec 0.0678025, recall 0.896316
2017-12-10T15:32:42.857631: step 2777, loss 0.123089, acc 0.953125, prec 0.0678502, recall 0.896395
2017-12-10T15:32:43.057618: step 2778, loss 0.206483, acc 0.875, prec 0.0678346, recall 0.896395
2017-12-10T15:32:43.249648: step 2779, loss 0.428016, acc 0.875, prec 0.067819, recall 0.896395
2017-12-10T15:32:43.444213: step 2780, loss 1.08412, acc 0.921875, prec 0.067836, recall 0.896434
2017-12-10T15:32:43.642930: step 2781, loss 0.168392, acc 0.9375, prec 0.0678282, recall 0.896434
2017-12-10T15:32:43.840179: step 2782, loss 0.243032, acc 0.9375, prec 0.067874, recall 0.896513
2017-12-10T15:32:44.036470: step 2783, loss 0.20173, acc 0.90625, prec 0.067889, recall 0.896552
2017-12-10T15:32:44.234805: step 2784, loss 0.261632, acc 0.875, prec 0.0678734, recall 0.896552
2017-12-10T15:32:44.446114: step 2785, loss 0.22403, acc 0.953125, prec 0.0678943, recall 0.896591
2017-12-10T15:32:44.642785: step 2786, loss 0.356385, acc 0.859375, prec 0.0679035, recall 0.89663
2017-12-10T15:32:44.837445: step 2787, loss 0.153688, acc 0.921875, prec 0.0679472, recall 0.896708
2017-12-10T15:32:45.032794: step 2788, loss 0.264033, acc 0.9375, prec 0.0680196, recall 0.896825
2017-12-10T15:32:45.227463: step 2789, loss 0.396993, acc 0.921875, prec 0.0680366, recall 0.896864
2017-12-10T15:32:45.422696: step 2790, loss 0.190281, acc 0.921875, prec 0.0680268, recall 0.896864
2017-12-10T15:32:45.617999: step 2791, loss 0.302384, acc 0.890625, prec 0.0680132, recall 0.896864
2017-12-10T15:32:45.815095: step 2792, loss 0.0916693, acc 0.96875, prec 0.0680093, recall 0.896864
2017-12-10T15:32:46.017422: step 2793, loss 0.372964, acc 0.984375, prec 0.0680607, recall 0.896942
2017-12-10T15:32:46.215682: step 2794, loss 0.355676, acc 0.90625, prec 0.068049, recall 0.896942
2017-12-10T15:32:46.415485: step 2795, loss 0.225319, acc 0.9375, prec 0.0680412, recall 0.896942
2017-12-10T15:32:46.612009: step 2796, loss 0.0795095, acc 0.953125, prec 0.0680354, recall 0.896942
2017-12-10T15:32:46.816129: step 2797, loss 0.191568, acc 0.96875, prec 0.0680849, recall 0.89702
2017-12-10T15:32:47.010603: step 2798, loss 0.211861, acc 0.9375, prec 0.0681037, recall 0.897059
2017-12-10T15:32:47.201492: step 2799, loss 0.243848, acc 0.90625, prec 0.0681187, recall 0.897098
2017-12-10T15:32:47.391726: step 2800, loss 0.128515, acc 0.9375, prec 0.0681376, recall 0.897136
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_2/1512937383/checkpoints/model-2800

2017-12-10T15:32:48.729220: step 2801, loss 0.217495, acc 0.96875, prec 0.0681604, recall 0.897175
2017-12-10T15:32:48.929097: step 2802, loss 0.0388716, acc 1, prec 0.0681604, recall 0.897175
2017-12-10T15:32:49.127313: step 2803, loss 0.130862, acc 0.921875, prec 0.0681773, recall 0.897214
2017-12-10T15:32:49.321089: step 2804, loss 0.0908101, acc 0.96875, prec 0.0681734, recall 0.897214
2017-12-10T15:32:49.525724: step 2805, loss 2.80267, acc 0.984375, prec 0.0682, recall 0.896915
2017-12-10T15:32:49.728628: step 2806, loss 0.0857797, acc 0.96875, prec 0.0681961, recall 0.896915
2017-12-10T15:32:49.923534: step 2807, loss 0.101628, acc 0.96875, prec 0.0681922, recall 0.896915
2017-12-10T15:32:50.123204: step 2808, loss 0.0431154, acc 0.984375, prec 0.0681903, recall 0.896915
2017-12-10T15:32:50.319300: step 2809, loss 0.168751, acc 0.9375, prec 0.0681825, recall 0.896915
2017-12-10T15:32:50.517958: step 2810, loss 0.230462, acc 0.9375, prec 0.0682546, recall 0.897031
2017-12-10T15:32:50.711742: step 2811, loss 0.202841, acc 0.921875, prec 0.0683248, recall 0.897147
2017-12-10T15:32:50.908333: step 2812, loss 0.317911, acc 0.953125, prec 0.0683455, recall 0.897186
2017-12-10T15:32:51.100466: step 2813, loss 0.161434, acc 0.984375, prec 0.0683702, recall 0.897224
2017-12-10T15:32:51.298097: step 2814, loss 0.231047, acc 0.921875, prec 0.0683604, recall 0.897224
2017-12-10T15:32:51.496340: step 2815, loss 0.737806, acc 0.890625, prec 0.0684266, recall 0.89734
2017-12-10T15:32:51.693088: step 2816, loss 0.119267, acc 0.953125, prec 0.0684474, recall 0.897378
2017-12-10T15:32:51.888955: step 2817, loss 0.26754, acc 0.90625, prec 0.0684888, recall 0.897455
2017-12-10T15:32:52.089994: step 2818, loss 0.352767, acc 0.890625, prec 0.0685018, recall 0.897493
2017-12-10T15:32:52.285052: step 2819, loss 0.143777, acc 0.9375, prec 0.0685471, recall 0.89757
2017-12-10T15:32:52.486430: step 2820, loss 0.31378, acc 0.9375, prec 0.0685925, recall 0.897647
2017-12-10T15:32:52.687125: step 2821, loss 0.278084, acc 0.921875, prec 0.0685827, recall 0.897647
2017-12-10T15:32:52.884700: step 2822, loss 0.357186, acc 0.859375, prec 0.0685916, recall 0.897685
2017-12-10T15:32:53.084506: step 2823, loss 0.395555, acc 0.890625, prec 0.0685779, recall 0.897685
2017-12-10T15:32:53.279303: step 2824, loss 0.254397, acc 0.890625, prec 0.0685643, recall 0.897685
2017-12-10T15:32:53.470236: step 2825, loss 0.232208, acc 0.90625, prec 0.0685791, recall 0.897723
2017-12-10T15:32:53.667534: step 2826, loss 0.36458, acc 0.96875, prec 0.0686814, recall 0.897875
2017-12-10T15:32:53.865880: step 2827, loss 0.130647, acc 0.9375, prec 0.0687001, recall 0.897914
2017-12-10T15:32:54.069887: step 2828, loss 0.262263, acc 0.921875, prec 0.0686903, recall 0.897914
2017-12-10T15:32:54.266212: step 2829, loss 0.202597, acc 0.96875, prec 0.0686864, recall 0.897914
2017-12-10T15:32:54.460431: step 2830, loss 0.254988, acc 0.953125, prec 0.0687867, recall 0.898065
2017-12-10T15:32:54.659650: step 2831, loss 0.0715282, acc 0.96875, prec 0.0687828, recall 0.898065
2017-12-10T15:32:54.858056: step 2832, loss 2.08038, acc 0.9375, prec 0.0688034, recall 0.89777
2017-12-10T15:32:55.054223: step 2833, loss 0.684001, acc 0.921875, prec 0.0689262, recall 0.897959
2017-12-10T15:32:55.254539: step 2834, loss 0.204955, acc 0.96875, prec 0.0689488, recall 0.897997
2017-12-10T15:32:55.454591: step 2835, loss 0.215462, acc 0.90625, prec 0.0689636, recall 0.898035
2017-12-10T15:32:55.649564: step 2836, loss 0.100664, acc 0.953125, prec 0.0689842, recall 0.898073
2017-12-10T15:32:55.849993: step 2837, loss 0.409721, acc 0.90625, prec 0.0689989, recall 0.89811
2017-12-10T15:32:56.048361: step 2838, loss 0.298649, acc 0.921875, prec 0.0689891, recall 0.89811
2017-12-10T15:32:56.246231: step 2839, loss 0.587231, acc 0.828125, prec 0.068994, recall 0.898148
2017-12-10T15:32:56.444953: step 2840, loss 0.28917, acc 0.859375, prec 0.0690293, recall 0.898224
2017-12-10T15:32:56.645969: step 2841, loss 0.312294, acc 0.890625, prec 0.069042, recall 0.898261
2017-12-10T15:32:56.843075: step 2842, loss 0.453253, acc 0.828125, prec 0.0690998, recall 0.898374
2017-12-10T15:32:57.040967: step 2843, loss 0.351928, acc 0.90625, prec 0.0691409, recall 0.898449
2017-12-10T15:32:57.237803: step 2844, loss 0.352919, acc 0.890625, prec 0.0691536, recall 0.898487
2017-12-10T15:32:57.430602: step 2845, loss 0.405975, acc 0.9375, prec 0.0691458, recall 0.898487
2017-12-10T15:32:57.627422: step 2846, loss 0.1304, acc 0.96875, prec 0.0691418, recall 0.898487
2017-12-10T15:32:57.828387: step 2847, loss 0.210107, acc 0.90625, prec 0.0691301, recall 0.898487
2017-12-10T15:32:58.026085: step 2848, loss 0.409162, acc 0.859375, prec 0.0691388, recall 0.898524
2017-12-10T15:32:58.221245: step 2849, loss 0.133751, acc 0.921875, prec 0.0691554, recall 0.898561
2017-12-10T15:32:58.421617: step 2850, loss 0.133044, acc 0.921875, prec 0.0691456, recall 0.898561
2017-12-10T15:32:58.619091: step 2851, loss 0.160919, acc 0.9375, prec 0.0691378, recall 0.898561
2017-12-10T15:32:58.817399: step 2852, loss 0.185765, acc 0.984375, prec 0.0691622, recall 0.898599
2017-12-10T15:32:59.014924: step 2853, loss 0.218118, acc 0.96875, prec 0.0691847, recall 0.898636
2017-12-10T15:32:59.215187: step 2854, loss 0.72307, acc 0.953125, prec 0.0692316, recall 0.898711
2017-12-10T15:32:59.423648: step 2855, loss 0.0461429, acc 0.984375, prec 0.0692297, recall 0.898711
2017-12-10T15:32:59.622452: step 2856, loss 0.914608, acc 0.984375, prec 0.0692541, recall 0.898748
2017-12-10T15:32:59.820390: step 2857, loss 0.194257, acc 0.953125, prec 0.0692482, recall 0.898748
2017-12-10T15:33:00.016071: step 2858, loss 0.143533, acc 0.953125, prec 0.0692423, recall 0.898748
2017-12-10T15:33:00.208643: step 2859, loss 0.538145, acc 0.90625, prec 0.0692569, recall 0.898785
2017-12-10T15:33:00.404357: step 2860, loss 0.164449, acc 0.953125, prec 0.0692511, recall 0.898785
2017-12-10T15:33:00.599774: step 2861, loss 0.0749003, acc 0.96875, prec 0.0692735, recall 0.898823
2017-12-10T15:33:00.794724: step 2862, loss 0.331886, acc 0.890625, prec 0.0693125, recall 0.898897
2017-12-10T15:33:00.998038: step 2863, loss 0.221456, acc 0.953125, prec 0.0693066, recall 0.898897
2017-12-10T15:33:01.195669: step 2864, loss 2.0115, acc 0.875, prec 0.0693193, recall 0.898604
2017-12-10T15:33:01.400111: step 2865, loss 0.225295, acc 0.953125, prec 0.0693134, recall 0.898604
2017-12-10T15:33:01.599417: step 2866, loss 0.170191, acc 0.921875, prec 0.0693299, recall 0.898641
2017-12-10T15:33:01.794705: step 2867, loss 0.357173, acc 0.90625, prec 0.0693182, recall 0.898641
2017-12-10T15:33:01.987913: step 2868, loss 0.267457, acc 0.953125, prec 0.0693123, recall 0.898641
2017-12-10T15:33:02.184431: step 2869, loss 0.282386, acc 0.859375, prec 0.0692946, recall 0.898641
2017-12-10T15:33:02.378267: step 2870, loss 0.447936, acc 0.84375, prec 0.0693277, recall 0.898716
2017-12-10T15:33:02.575440: step 2871, loss 0.341611, acc 0.875, prec 0.069312, recall 0.898716
2017-12-10T15:33:02.772677: step 2872, loss 0.296737, acc 0.90625, prec 0.0693265, recall 0.898753
2017-12-10T15:33:02.971519: step 2873, loss 0.219229, acc 0.90625, prec 0.0693938, recall 0.898864
2017-12-10T15:33:03.165919: step 2874, loss 0.736145, acc 0.921875, prec 0.0694103, recall 0.898901
2017-12-10T15:33:03.362783: step 2875, loss 0.145994, acc 0.9375, prec 0.069455, recall 0.898975
2017-12-10T15:33:03.558207: step 2876, loss 0.326023, acc 0.890625, prec 0.0694676, recall 0.899012
2017-12-10T15:33:03.755969: step 2877, loss 0.55918, acc 0.828125, prec 0.069446, recall 0.899012
2017-12-10T15:33:03.950252: step 2878, loss 0.220786, acc 0.90625, prec 0.0694342, recall 0.899012
2017-12-10T15:33:04.146139: step 2879, loss 0.265831, acc 0.953125, prec 0.0694809, recall 0.899086
2017-12-10T15:33:04.341087: step 2880, loss 0.219958, acc 0.921875, prec 0.0694711, recall 0.899086
2017-12-10T15:33:04.538001: step 2881, loss 0.467213, acc 0.875, prec 0.0694817, recall 0.899123
2017-12-10T15:33:04.734577: step 2882, loss 0.885667, acc 0.859375, prec 0.0695166, recall 0.899197
2017-12-10T15:33:04.935069: step 2883, loss 0.92362, acc 0.953125, prec 0.0695895, recall 0.899307
2017-12-10T15:33:05.132223: step 2884, loss 0.247523, acc 0.9375, prec 0.0696342, recall 0.89938
2017-12-10T15:33:05.328796: step 2885, loss 0.401667, acc 0.90625, prec 0.0696749, recall 0.899454
2017-12-10T15:33:05.527016: step 2886, loss 0.31272, acc 0.9375, prec 0.069667, recall 0.899454
2017-12-10T15:33:05.726648: step 2887, loss 0.281372, acc 0.921875, prec 0.0696835, recall 0.89949
2017-12-10T15:33:05.924264: step 2888, loss 0.262815, acc 0.90625, prec 0.0696717, recall 0.89949
2017-12-10T15:33:06.119749: step 2889, loss 0.309679, acc 0.90625, prec 0.0696861, recall 0.899527
2017-12-10T15:33:06.318983: step 2890, loss 0.183657, acc 0.953125, prec 0.0696802, recall 0.899527
2017-12-10T15:33:06.511296: step 2891, loss 0.180967, acc 0.9375, prec 0.0696724, recall 0.899527
2017-12-10T15:33:06.713847: step 2892, loss 0.409226, acc 0.90625, prec 0.0696868, recall 0.899563
2017-12-10T15:33:06.911098: step 2893, loss 0.127673, acc 0.96875, prec 0.0696829, recall 0.899563
2017-12-10T15:33:07.113632: step 2894, loss 0.364696, acc 0.921875, prec 0.0696993, recall 0.8996
2017-12-10T15:33:07.314883: step 2895, loss 1.11127, acc 0.953125, prec 0.069772, recall 0.899709
2017-12-10T15:33:07.516429: step 2896, loss 0.341415, acc 0.875, prec 0.0697563, recall 0.899709
2017-12-10T15:33:07.715749: step 2897, loss 0.0638455, acc 0.984375, prec 0.0697543, recall 0.899709
2017-12-10T15:33:07.907966: step 2898, loss 1.34614, acc 0.875, prec 0.0697648, recall 0.899746
2017-12-10T15:33:08.108639: step 2899, loss 0.248272, acc 0.9375, prec 0.069757, recall 0.899746
2017-12-10T15:33:08.305250: step 2900, loss 0.299476, acc 0.90625, prec 0.0697714, recall 0.899782
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_2/1512937383/checkpoints/model-2900

2017-12-10T15:33:09.662879: step 2901, loss 0.194171, acc 0.953125, prec 0.0697655, recall 0.899782
2017-12-10T15:33:09.857803: step 2902, loss 0.351168, acc 0.890625, prec 0.0697517, recall 0.899782
2017-12-10T15:33:10.054182: step 2903, loss 0.367587, acc 0.875, prec 0.0697622, recall 0.899819
2017-12-10T15:33:10.250352: step 2904, loss 0.266345, acc 0.921875, prec 0.0697524, recall 0.899819
2017-12-10T15:33:10.450286: step 2905, loss 0.323695, acc 0.875, prec 0.0697367, recall 0.899819
2017-12-10T15:33:10.648738: step 2906, loss 0.284843, acc 0.90625, prec 0.0697511, recall 0.899855
2017-12-10T15:33:10.846601: step 2907, loss 0.220954, acc 0.921875, prec 0.0697674, recall 0.899891
2017-12-10T15:33:11.051959: step 2908, loss 0.240582, acc 0.890625, prec 0.0697537, recall 0.899891
2017-12-10T15:33:11.249919: step 2909, loss 0.135705, acc 0.96875, prec 0.0697759, recall 0.899927
2017-12-10T15:33:11.445839: step 2910, loss 0.0514388, acc 0.984375, prec 0.069774, recall 0.899927
2017-12-10T15:33:11.642117: step 2911, loss 0.62255, acc 0.9375, prec 0.0698184, recall 0.9
2017-12-10T15:33:11.841940: step 2912, loss 0.0842489, acc 0.96875, prec 0.0698145, recall 0.9
2017-12-10T15:33:12.039693: step 2913, loss 0.274951, acc 0.921875, prec 0.0698047, recall 0.9
2017-12-10T15:33:12.238666: step 2914, loss 0.157908, acc 0.953125, prec 0.0697988, recall 0.9
2017-12-10T15:33:12.437823: step 2915, loss 0.106206, acc 0.9375, prec 0.069791, recall 0.9
2017-12-10T15:33:12.632167: step 2916, loss 0.112869, acc 0.953125, prec 0.0697851, recall 0.9
2017-12-10T15:33:12.828568: step 2917, loss 0.0823425, acc 0.96875, prec 0.0698073, recall 0.900036
2017-12-10T15:33:13.021158: step 2918, loss 0.106004, acc 0.953125, prec 0.0698537, recall 0.900109
2017-12-10T15:33:13.218401: step 2919, loss 0.240126, acc 0.921875, prec 0.0698439, recall 0.900109
2017-12-10T15:33:13.415248: step 2920, loss 0.155313, acc 0.890625, prec 0.0698301, recall 0.900109
2017-12-10T15:33:13.612399: step 2921, loss 0.0252414, acc 1, prec 0.0698301, recall 0.900109
2017-12-10T15:33:13.808224: step 2922, loss 2.78378, acc 0.921875, prec 0.0698745, recall 0.899855
2017-12-10T15:33:14.007207: step 2923, loss 5.67087, acc 0.90625, prec 0.0698647, recall 0.89953
2017-12-10T15:33:14.210784: step 2924, loss 0.044978, acc 1, prec 0.0698647, recall 0.89953
2017-12-10T15:33:14.415971: step 2925, loss 0.0905017, acc 0.96875, prec 0.0698608, recall 0.89953
2017-12-10T15:33:14.615404: step 2926, loss 0.147501, acc 0.921875, prec 0.0698771, recall 0.899566
2017-12-10T15:33:14.807219: step 2927, loss 0.317501, acc 0.859375, prec 0.0698594, recall 0.899566
2017-12-10T15:33:15.003078: step 2928, loss 0.503754, acc 0.875, prec 0.0698438, recall 0.899566
2017-12-10T15:33:15.205709: step 2929, loss 0.720071, acc 0.765625, prec 0.0698665, recall 0.899639
2017-12-10T15:33:15.404297: step 2930, loss 0.297538, acc 0.890625, prec 0.069905, recall 0.899711
2017-12-10T15:33:15.597320: step 2931, loss 0.66409, acc 0.8125, prec 0.0699075, recall 0.899748
2017-12-10T15:33:15.794427: step 2932, loss 0.374321, acc 0.921875, prec 0.0699238, recall 0.899784
2017-12-10T15:33:15.987244: step 2933, loss 0.627252, acc 0.8125, prec 0.0699524, recall 0.899856
2017-12-10T15:33:16.185490: step 2934, loss 0.348825, acc 0.84375, prec 0.0699328, recall 0.899856
2017-12-10T15:33:16.381566: step 2935, loss 0.303044, acc 0.890625, prec 0.0699712, recall 0.899928
2017-12-10T15:33:16.577453: step 2936, loss 0.733283, acc 0.8125, prec 0.0699477, recall 0.899928
2017-12-10T15:33:16.776658: step 2937, loss 0.323269, acc 0.890625, prec 0.069986, recall 0.9
2017-12-10T15:33:16.966997: step 2938, loss 0.291585, acc 0.890625, prec 0.0699723, recall 0.9
2017-12-10T15:33:17.165460: step 2939, loss 0.328935, acc 0.921875, prec 0.0699625, recall 0.9
2017-12-10T15:33:17.367028: step 2940, loss 0.228003, acc 0.875, prec 0.0699469, recall 0.9
2017-12-10T15:33:17.565866: step 2941, loss 0.317605, acc 0.890625, prec 0.0699592, recall 0.900036
2017-12-10T15:33:17.766846: step 2942, loss 0.139651, acc 0.9375, prec 0.0699514, recall 0.900036
2017-12-10T15:33:17.962730: step 2943, loss 2.34937, acc 0.890625, prec 0.0699916, recall 0.899785
2017-12-10T15:33:18.170064: step 2944, loss 0.117495, acc 0.953125, prec 0.0700117, recall 0.89982
2017-12-10T15:33:18.367412: step 2945, loss 0.127304, acc 0.953125, prec 0.0700059, recall 0.89982
2017-12-10T15:33:18.567454: step 2946, loss 0.36698, acc 0.875, prec 0.0700162, recall 0.899856
2017-12-10T15:33:18.761363: step 2947, loss 0.514885, acc 0.890625, prec 0.0700025, recall 0.899856
2017-12-10T15:33:18.957217: step 2948, loss 0.15189, acc 0.953125, prec 0.0700226, recall 0.899892
2017-12-10T15:33:19.158652: step 2949, loss 0.391481, acc 0.953125, prec 0.0700427, recall 0.899928
2017-12-10T15:33:19.357504: step 2950, loss 0.302396, acc 0.90625, prec 0.070031, recall 0.899928
2017-12-10T15:33:19.560956: step 2951, loss 0.232794, acc 0.890625, prec 0.0700173, recall 0.899928
2017-12-10T15:33:19.762563: step 2952, loss 0.287533, acc 0.90625, prec 0.0700315, recall 0.899964
2017-12-10T15:33:19.960478: step 2953, loss 0.271278, acc 0.921875, prec 0.0700996, recall 0.900072
2017-12-10T15:33:20.161987: step 2954, loss 0.132432, acc 0.96875, prec 0.0701216, recall 0.900107
2017-12-10T15:33:20.359577: step 2955, loss 0.900637, acc 0.890625, prec 0.0701339, recall 0.900143
2017-12-10T15:33:20.555192: step 2956, loss 0.238722, acc 0.90625, prec 0.0701221, recall 0.900143
2017-12-10T15:33:20.747391: step 2957, loss 1.24677, acc 0.890625, prec 0.0701862, recall 0.90025
2017-12-10T15:33:20.947512: step 2958, loss 0.222527, acc 0.953125, prec 0.0702062, recall 0.900286
2017-12-10T15:33:21.141200: step 2959, loss 0.152136, acc 0.9375, prec 0.0702243, recall 0.900322
2017-12-10T15:33:21.337820: step 2960, loss 0.286675, acc 0.921875, prec 0.0702405, recall 0.900357
2017-12-10T15:33:21.539302: step 2961, loss 0.520788, acc 0.921875, prec 0.0702566, recall 0.900393
2017-12-10T15:33:21.739939: step 2962, loss 0.405614, acc 0.890625, prec 0.0702947, recall 0.900464
2017-12-10T15:33:21.940118: step 2963, loss 0.320782, acc 0.90625, prec 0.0702829, recall 0.900464
2017-12-10T15:33:22.138309: step 2964, loss 0.285409, acc 0.9375, prec 0.0703269, recall 0.900535
2017-12-10T15:33:22.337139: step 2965, loss 0.223054, acc 0.859375, prec 0.0703351, recall 0.90057
2017-12-10T15:33:22.541633: step 2966, loss 0.179577, acc 0.9375, prec 0.070379, recall 0.900641
2017-12-10T15:33:22.739925: step 2967, loss 0.155933, acc 0.9375, prec 0.0703971, recall 0.900676
2017-12-10T15:33:22.935218: step 2968, loss 0.081105, acc 0.953125, prec 0.0703912, recall 0.900676
2017-12-10T15:33:23.133555: step 2969, loss 0.127287, acc 0.96875, prec 0.0703873, recall 0.900676
2017-12-10T15:33:23.329361: step 2970, loss 0.174114, acc 0.9375, prec 0.0703794, recall 0.900676
2017-12-10T15:33:23.533108: step 2971, loss 0.103724, acc 0.953125, prec 0.0703736, recall 0.900676
2017-12-10T15:33:23.733633: step 2972, loss 0.0707738, acc 0.96875, prec 0.0703955, recall 0.900712
2017-12-10T15:33:23.931217: step 2973, loss 0.0763864, acc 0.984375, prec 0.0704453, recall 0.900782
2017-12-10T15:33:24.139011: step 2974, loss 0.0835722, acc 0.984375, prec 0.0704433, recall 0.900782
2017-12-10T15:33:24.332597: step 2975, loss 0.0772322, acc 0.96875, prec 0.0704394, recall 0.900782
2017-12-10T15:33:24.530564: step 2976, loss 0.0974264, acc 0.984375, prec 0.0704633, recall 0.900818
2017-12-10T15:33:24.730173: step 2977, loss 0.170633, acc 0.953125, prec 0.0704574, recall 0.900818
2017-12-10T15:33:24.927248: step 2978, loss 0.144337, acc 0.953125, prec 0.0704774, recall 0.900853
2017-12-10T15:33:25.122473: step 2979, loss 0.0417891, acc 0.984375, prec 0.0704754, recall 0.900853
2017-12-10T15:33:25.318962: step 2980, loss 0.277342, acc 0.90625, prec 0.0704895, recall 0.900888
2017-12-10T15:33:25.517212: step 2981, loss 0.22176, acc 0.953125, prec 0.0705353, recall 0.900958
2017-12-10T15:33:25.700093: step 2982, loss 0.546236, acc 1, prec 0.0706128, recall 0.901064
2017-12-10T15:33:25.909533: step 2983, loss 0.072688, acc 0.96875, prec 0.0706347, recall 0.901099
2017-12-10T15:33:26.112797: step 2984, loss 1.29368, acc 0.96875, prec 0.0706585, recall 0.900815
2017-12-10T15:33:26.313695: step 2985, loss 0.0159014, acc 1, prec 0.0706585, recall 0.900815
2017-12-10T15:33:26.509846: step 2986, loss 0.074647, acc 0.96875, prec 0.0706546, recall 0.900815
2017-12-10T15:33:26.706568: step 2987, loss 0.0849256, acc 0.96875, prec 0.0706765, recall 0.90085
2017-12-10T15:33:26.904090: step 2988, loss 0.118737, acc 0.96875, prec 0.0707242, recall 0.90092
2017-12-10T15:33:27.102785: step 2989, loss 0.326253, acc 0.890625, prec 0.0707104, recall 0.90092
2017-12-10T15:33:27.299821: step 2990, loss 0.157002, acc 0.953125, prec 0.0707304, recall 0.900955
2017-12-10T15:33:27.489536: step 2991, loss 0.159616, acc 0.921875, prec 0.0707205, recall 0.900955
2017-12-10T15:33:27.687888: step 2992, loss 0.138603, acc 0.984375, prec 0.0707702, recall 0.901025
2017-12-10T15:33:27.888878: step 2993, loss 0.0929271, acc 0.953125, prec 0.0707643, recall 0.901025
2017-12-10T15:33:28.083187: step 2994, loss 0.121454, acc 0.953125, prec 0.0707584, recall 0.901025
2017-12-10T15:33:28.280696: step 2995, loss 0.270305, acc 0.890625, prec 0.0707962, recall 0.901095
2017-12-10T15:33:28.478210: step 2996, loss 0.541858, acc 0.90625, prec 0.0708876, recall 0.901235
2017-12-10T15:33:28.678333: step 2997, loss 0.141131, acc 0.953125, prec 0.0709332, recall 0.901304
2017-12-10T15:33:28.877351: step 2998, loss 0.127639, acc 0.953125, prec 0.0709273, recall 0.901304
2017-12-10T15:33:29.076285: step 2999, loss 0.275891, acc 0.875, prec 0.0709116, recall 0.901304
2017-12-10T15:33:29.279508: step 3000, loss 0.12446, acc 0.96875, prec 0.0709592, recall 0.901374
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_2/1512937383/checkpoints/model-3000

2017-12-10T15:33:30.513914: step 3001, loss 0.140082, acc 0.96875, prec 0.0709552, recall 0.901374
2017-12-10T15:33:30.713620: step 3002, loss 0.0519796, acc 0.984375, prec 0.0709532, recall 0.901374
2017-12-10T15:33:30.910081: step 3003, loss 0.144709, acc 0.96875, prec 0.0709493, recall 0.901374
2017-12-10T15:33:31.114009: step 3004, loss 0.0592892, acc 0.984375, prec 0.0709473, recall 0.901374
2017-12-10T15:33:31.313428: step 3005, loss 0.238231, acc 1, prec 0.0709989, recall 0.901443
2017-12-10T15:33:31.515275: step 3006, loss 0.167746, acc 0.953125, prec 0.0710445, recall 0.901513
2017-12-10T15:33:31.714867: step 3007, loss 0.177505, acc 0.90625, prec 0.0710327, recall 0.901513
2017-12-10T15:33:31.915503: step 3008, loss 0.123049, acc 0.96875, prec 0.0710287, recall 0.901513
2017-12-10T15:33:32.112088: step 3009, loss 0.338824, acc 0.984375, prec 0.0710525, recall 0.901547
2017-12-10T15:33:32.309714: step 3010, loss 0.0995584, acc 0.984375, prec 0.0710763, recall 0.901582
2017-12-10T15:33:32.508554: step 3011, loss 0.0504223, acc 0.984375, prec 0.0710743, recall 0.901582
2017-12-10T15:33:32.706210: step 3012, loss 0.159241, acc 0.953125, prec 0.0711199, recall 0.901651
2017-12-10T15:33:32.904683: step 3013, loss 0.191209, acc 0.921875, prec 0.07111, recall 0.901651
2017-12-10T15:33:33.101918: step 3014, loss 0.0512522, acc 0.984375, prec 0.071108, recall 0.901651
2017-12-10T15:33:33.296855: step 3015, loss 0.785553, acc 0.96875, prec 0.0711555, recall 0.90172
2017-12-10T15:33:33.495959: step 3016, loss 0.096614, acc 0.96875, prec 0.0711516, recall 0.90172
2017-12-10T15:33:33.693224: step 3017, loss 0.0392392, acc 1, prec 0.0711773, recall 0.901754
2017-12-10T15:33:33.893419: step 3018, loss 0.157347, acc 0.9375, prec 0.0712209, recall 0.901823
2017-12-10T15:33:34.086900: step 3019, loss 0.217322, acc 0.921875, prec 0.0712882, recall 0.901926
2017-12-10T15:33:34.282828: step 3020, loss 0.141378, acc 0.9375, prec 0.071306, recall 0.901961
2017-12-10T15:33:34.485588: step 3021, loss 0.190376, acc 0.9375, prec 0.0713238, recall 0.901995
2017-12-10T15:33:34.680746: step 3022, loss 0.0915339, acc 0.96875, prec 0.0713455, recall 0.902029
2017-12-10T15:33:34.878212: step 3023, loss 0.208097, acc 0.953125, prec 0.0713396, recall 0.902029
2017-12-10T15:33:35.077931: step 3024, loss 0.244986, acc 1, prec 0.0713653, recall 0.902064
2017-12-10T15:33:35.280863: step 3025, loss 0.0441131, acc 1, prec 0.0713653, recall 0.902064
2017-12-10T15:33:35.479620: step 3026, loss 0.228921, acc 0.90625, prec 0.0713535, recall 0.902064
2017-12-10T15:33:35.673799: step 3027, loss 0.0985545, acc 0.96875, prec 0.0713752, recall 0.902098
2017-12-10T15:33:35.873349: step 3028, loss 0.491444, acc 0.953125, prec 0.0714464, recall 0.9022
2017-12-10T15:33:36.074545: step 3029, loss 0.0824393, acc 0.984375, prec 0.0714701, recall 0.902235
2017-12-10T15:33:36.273202: step 3030, loss 0.176437, acc 0.9375, prec 0.0714622, recall 0.902235
2017-12-10T15:33:36.467059: step 3031, loss 0.170789, acc 0.953125, prec 0.0714819, recall 0.902269
2017-12-10T15:33:36.666396: step 3032, loss 0.0836591, acc 0.9375, prec 0.071474, recall 0.902269
2017-12-10T15:33:36.867123: step 3033, loss 0.0829027, acc 0.984375, prec 0.071472, recall 0.902269
2017-12-10T15:33:37.059690: step 3034, loss 0.596815, acc 1, prec 0.071549, recall 0.902371
2017-12-10T15:33:37.264601: step 3035, loss 0.0910375, acc 0.96875, prec 0.0715707, recall 0.902405
2017-12-10T15:33:37.462826: step 3036, loss 0.26936, acc 0.9375, prec 0.0716398, recall 0.902507
2017-12-10T15:33:37.657251: step 3037, loss 0.0944177, acc 0.96875, prec 0.0716358, recall 0.902507
2017-12-10T15:33:37.853091: step 3038, loss 0.243959, acc 0.953125, prec 0.0716556, recall 0.902541
2017-12-10T15:33:38.053797: step 3039, loss 0.120584, acc 0.96875, prec 0.0716516, recall 0.902541
2017-12-10T15:33:38.249024: step 3040, loss 0.257041, acc 0.90625, prec 0.0716654, recall 0.902575
2017-12-10T15:33:38.447014: step 3041, loss 0.0934848, acc 0.96875, prec 0.0717127, recall 0.902643
2017-12-10T15:33:38.644207: step 3042, loss 0.0778079, acc 0.984375, prec 0.0717364, recall 0.902676
2017-12-10T15:33:38.846535: step 3043, loss 0.190738, acc 0.9375, prec 0.0717541, recall 0.90271
2017-12-10T15:33:39.042301: step 3044, loss 0.224796, acc 0.921875, prec 0.0717442, recall 0.90271
2017-12-10T15:33:39.241776: step 3045, loss 0.0384758, acc 0.984375, prec 0.0717422, recall 0.90271
2017-12-10T15:33:39.434377: step 3046, loss 0.242572, acc 0.9375, prec 0.0718368, recall 0.902845
2017-12-10T15:33:39.635670: step 3047, loss 0.153737, acc 0.953125, prec 0.0718308, recall 0.902845
2017-12-10T15:33:39.836017: step 3048, loss 0.152419, acc 0.90625, prec 0.0718189, recall 0.902845
2017-12-10T15:33:40.031944: step 3049, loss 0.0642985, acc 0.984375, prec 0.071817, recall 0.902845
2017-12-10T15:33:40.232177: step 3050, loss 0.0857462, acc 0.96875, prec 0.0718386, recall 0.902879
2017-12-10T15:33:40.434555: step 3051, loss 0.203705, acc 0.921875, prec 0.0718287, recall 0.902879
2017-12-10T15:33:40.630029: step 3052, loss 0.716179, acc 0.984375, prec 0.0718523, recall 0.902913
2017-12-10T15:33:40.835408: step 3053, loss 0.192992, acc 0.9375, prec 0.0718444, recall 0.902913
2017-12-10T15:33:41.037762: step 3054, loss 0.112727, acc 0.953125, prec 0.0718384, recall 0.902913
2017-12-10T15:33:41.235757: step 3055, loss 0.175467, acc 0.96875, prec 0.0718857, recall 0.90298
2017-12-10T15:33:41.434552: step 3056, loss 0.195582, acc 0.953125, prec 0.0719309, recall 0.903047
2017-12-10T15:33:41.638858: step 3057, loss 0.123318, acc 0.953125, prec 0.0719762, recall 0.903114
2017-12-10T15:33:41.836792: step 3058, loss 0.129387, acc 1, prec 0.0720274, recall 0.903181
2017-12-10T15:33:42.040215: step 3059, loss 0.0714091, acc 0.984375, prec 0.0720254, recall 0.903181
2017-12-10T15:33:42.235748: step 3060, loss 0.0153378, acc 1, prec 0.0720254, recall 0.903181
2017-12-10T15:33:42.431573: step 3061, loss 0.0914869, acc 0.96875, prec 0.0720214, recall 0.903181
2017-12-10T15:33:42.627174: step 3062, loss 0.0258436, acc 0.984375, prec 0.0720194, recall 0.903181
2017-12-10T15:33:42.822460: step 3063, loss 0.0984115, acc 0.984375, prec 0.0720174, recall 0.903181
2017-12-10T15:33:43.018818: step 3064, loss 0.307983, acc 0.921875, prec 0.0720075, recall 0.903181
2017-12-10T15:33:43.214089: step 3065, loss 0.127077, acc 0.953125, prec 0.0720271, recall 0.903215
2017-12-10T15:33:43.411748: step 3066, loss 5.06705, acc 0.96875, prec 0.0720507, recall 0.902936
2017-12-10T15:33:43.614444: step 3067, loss 1.05316, acc 0.984375, prec 0.0720743, recall 0.90297
2017-12-10T15:33:43.812678: step 3068, loss 0.166237, acc 0.9375, prec 0.0720664, recall 0.90297
2017-12-10T15:33:44.010826: step 3069, loss 0.111149, acc 0.96875, prec 0.0721135, recall 0.903037
2017-12-10T15:33:44.213181: step 3070, loss 0.296999, acc 0.890625, prec 0.0720996, recall 0.903037
2017-12-10T15:33:44.415600: step 3071, loss 0.483703, acc 0.796875, prec 0.0720738, recall 0.903037
2017-12-10T15:33:44.610068: step 3072, loss 0.203129, acc 0.921875, prec 0.0720639, recall 0.903037
2017-12-10T15:33:44.806561: step 3073, loss 0.354521, acc 0.84375, prec 0.0720696, recall 0.90307
2017-12-10T15:33:45.002841: step 3074, loss 0.567391, acc 0.859375, prec 0.0720773, recall 0.903103
2017-12-10T15:33:45.195241: step 3075, loss 0.294315, acc 0.921875, prec 0.0720674, recall 0.903103
2017-12-10T15:33:45.389033: step 3076, loss 0.631702, acc 0.84375, prec 0.0720475, recall 0.903103
2017-12-10T15:33:45.588443: step 3077, loss 0.206719, acc 0.890625, prec 0.0720337, recall 0.903103
2017-12-10T15:33:45.779919: step 3078, loss 0.194276, acc 0.90625, prec 0.0720473, recall 0.903137
2017-12-10T15:33:45.973782: step 3079, loss 0.323149, acc 0.859375, prec 0.072055, recall 0.90317
2017-12-10T15:33:46.176401: step 3080, loss 0.270884, acc 0.921875, prec 0.0720706, recall 0.903204
2017-12-10T15:33:46.372863: step 3081, loss 0.269034, acc 0.890625, prec 0.0721077, recall 0.90327
2017-12-10T15:33:46.574922: step 3082, loss 0.266335, acc 0.90625, prec 0.0720958, recall 0.90327
2017-12-10T15:33:46.770093: step 3083, loss 0.0849708, acc 0.96875, prec 0.0720919, recall 0.90327
2017-12-10T15:33:46.969928: step 3084, loss 0.132303, acc 0.96875, prec 0.0720879, recall 0.90327
2017-12-10T15:33:47.165661: step 3085, loss 0.324143, acc 0.921875, prec 0.0721035, recall 0.903304
2017-12-10T15:33:47.365995: step 3086, loss 0.235367, acc 0.921875, prec 0.0720936, recall 0.903304
2017-12-10T15:33:47.565387: step 3087, loss 0.0792697, acc 0.96875, prec 0.0720896, recall 0.903304
2017-12-10T15:33:47.761011: step 3088, loss 0.165723, acc 0.921875, prec 0.0720797, recall 0.903304
2017-12-10T15:33:47.958735: step 3089, loss 0.0529921, acc 0.984375, prec 0.0720778, recall 0.903304
2017-12-10T15:33:48.154556: step 3090, loss 0.271904, acc 0.921875, prec 0.0720933, recall 0.903337
2017-12-10T15:33:48.350646: step 3091, loss 0.0898905, acc 0.984375, prec 0.0721168, recall 0.90337
2017-12-10T15:33:48.546497: step 3092, loss 0.0145998, acc 1, prec 0.0721168, recall 0.90337
2017-12-10T15:33:48.743758: step 3093, loss 0.0704452, acc 0.953125, prec 0.0721109, recall 0.90337
2017-12-10T15:33:48.946291: step 3094, loss 0.120849, acc 0.953125, prec 0.072105, recall 0.90337
2017-12-10T15:33:49.146742: step 3095, loss 0.119314, acc 0.953125, prec 0.0721245, recall 0.903403
2017-12-10T15:33:49.347922: step 3096, loss 0.053669, acc 0.984375, prec 0.0721225, recall 0.903403
2017-12-10T15:33:49.544595: step 3097, loss 2.80642, acc 0.984375, prec 0.0721225, recall 0.903093
2017-12-10T15:33:49.750911: step 3098, loss 0.141774, acc 0.90625, prec 0.0721106, recall 0.903093
2017-12-10T15:33:49.949476: step 3099, loss 0.150632, acc 0.984375, prec 0.0721341, recall 0.903126
2017-12-10T15:33:50.149824: step 3100, loss 0.0446542, acc 1, prec 0.072185, recall 0.903193
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_2/1512937383/checkpoints/model-3100

2017-12-10T15:33:51.365918: step 3101, loss 0.068983, acc 0.984375, prec 0.0721831, recall 0.903193
2017-12-10T15:33:51.561082: step 3102, loss 0.0348244, acc 1, prec 0.0722085, recall 0.903226
2017-12-10T15:33:51.758994: step 3103, loss 0.0916454, acc 0.96875, prec 0.0722045, recall 0.903226
2017-12-10T15:33:51.958747: step 3104, loss 0.116939, acc 0.9375, prec 0.0722221, recall 0.903259
2017-12-10T15:33:52.155980: step 3105, loss 0.215223, acc 0.953125, prec 0.0722161, recall 0.903259
2017-12-10T15:33:52.356109: step 3106, loss 0.108037, acc 0.96875, prec 0.0722376, recall 0.903292
2017-12-10T15:33:52.556605: step 3107, loss 0.249525, acc 0.96875, prec 0.0722591, recall 0.903325
2017-12-10T15:33:52.754887: step 3108, loss 0.230964, acc 0.953125, prec 0.0723295, recall 0.903425
2017-12-10T15:33:52.949201: step 3109, loss 1.61472, acc 0.921875, prec 0.0723215, recall 0.903115
2017-12-10T15:33:53.144087: step 3110, loss 0.104075, acc 0.9375, prec 0.0723136, recall 0.903115
2017-12-10T15:33:53.340596: step 3111, loss 0.346102, acc 0.9375, prec 0.0723311, recall 0.903149
2017-12-10T15:33:53.535998: step 3112, loss 0.143532, acc 0.96875, prec 0.072378, recall 0.903215
2017-12-10T15:33:53.736215: step 3113, loss 0.326612, acc 0.90625, prec 0.0724423, recall 0.903314
2017-12-10T15:33:53.937303: step 3114, loss 0.328034, acc 0.890625, prec 0.0724792, recall 0.90338
2017-12-10T15:33:54.140038: step 3115, loss 0.314766, acc 0.921875, prec 0.0724693, recall 0.90338
2017-12-10T15:33:54.337793: step 3116, loss 0.228215, acc 0.90625, prec 0.0725082, recall 0.903446
2017-12-10T15:33:54.533769: step 3117, loss 0.231665, acc 0.90625, prec 0.0725217, recall 0.903479
2017-12-10T15:33:54.727248: step 3118, loss 0.197527, acc 0.921875, prec 0.0725118, recall 0.903479
2017-12-10T15:33:54.925542: step 3119, loss 0.320508, acc 0.890625, prec 0.0725233, recall 0.903512
2017-12-10T15:33:55.125932: step 3120, loss 0.177568, acc 0.9375, prec 0.0725153, recall 0.903512
2017-12-10T15:33:55.323884: step 3121, loss 0.282313, acc 0.890625, prec 0.0725268, recall 0.903545
2017-12-10T15:33:55.524035: step 3122, loss 0.235805, acc 0.90625, prec 0.0725403, recall 0.903578
2017-12-10T15:33:55.723369: step 3123, loss 0.308742, acc 0.921875, prec 0.0725811, recall 0.903643
2017-12-10T15:33:55.922045: step 3124, loss 0.206336, acc 0.96875, prec 0.0726025, recall 0.903676
2017-12-10T15:33:56.121121: step 3125, loss 0.153779, acc 0.953125, prec 0.0725965, recall 0.903676
2017-12-10T15:33:56.318403: step 3126, loss 0.178397, acc 0.953125, prec 0.0726159, recall 0.903709
2017-12-10T15:33:56.520381: step 3127, loss 0.158414, acc 0.9375, prec 0.0726587, recall 0.903774
2017-12-10T15:33:56.720886: step 3128, loss 0.145731, acc 0.9375, prec 0.0726507, recall 0.903774
2017-12-10T15:33:56.915796: step 3129, loss 0.131976, acc 0.9375, prec 0.0726428, recall 0.903774
2017-12-10T15:33:57.113440: step 3130, loss 0.167949, acc 0.9375, prec 0.0726349, recall 0.903774
2017-12-10T15:33:57.312497: step 3131, loss 0.121577, acc 0.953125, prec 0.0726289, recall 0.903774
2017-12-10T15:33:57.513987: step 3132, loss 0.326538, acc 0.921875, prec 0.072619, recall 0.903774
2017-12-10T15:33:57.711277: step 3133, loss 0.133729, acc 0.96875, prec 0.072615, recall 0.903774
2017-12-10T15:33:57.911513: step 3134, loss 10.2031, acc 0.953125, prec 0.0726364, recall 0.9035
2017-12-10T15:33:58.112174: step 3135, loss 0.250603, acc 0.984375, prec 0.0726851, recall 0.903565
2017-12-10T15:33:58.312691: step 3136, loss 0.13426, acc 0.953125, prec 0.0726791, recall 0.903565
2017-12-10T15:33:58.508978: step 3137, loss 0.0595989, acc 0.984375, prec 0.0727278, recall 0.903631
2017-12-10T15:33:58.714277: step 3138, loss 0.185303, acc 0.953125, prec 0.0727471, recall 0.903664
2017-12-10T15:33:58.910241: step 3139, loss 0.30804, acc 0.9375, prec 0.0727645, recall 0.903696
2017-12-10T15:33:59.106440: step 3140, loss 0.304492, acc 0.921875, prec 0.0727799, recall 0.903729
2017-12-10T15:33:59.315607: step 3141, loss 1.14611, acc 0.984375, prec 0.0728052, recall 0.903455
2017-12-10T15:33:59.517192: step 3142, loss 0.280569, acc 0.828125, prec 0.0727833, recall 0.903455
2017-12-10T15:33:59.715704: step 3143, loss 0.585384, acc 0.796875, prec 0.0727828, recall 0.903488
2017-12-10T15:33:59.908993: step 3144, loss 0.418125, acc 0.921875, prec 0.0728235, recall 0.903553
2017-12-10T15:34:00.101832: step 3145, loss 0.408945, acc 0.875, prec 0.0728582, recall 0.903619
2017-12-10T15:34:00.295760: step 3146, loss 0.434582, acc 0.8125, prec 0.0728343, recall 0.903619
2017-12-10T15:34:00.494315: step 3147, loss 0.360263, acc 0.859375, prec 0.0728417, recall 0.903651
2017-12-10T15:34:00.694221: step 3148, loss 0.362352, acc 0.84375, prec 0.0728471, recall 0.903684
2017-12-10T15:34:00.892241: step 3149, loss 0.476496, acc 0.9375, prec 0.0728645, recall 0.903716
2017-12-10T15:34:01.088592: step 3150, loss 0.368518, acc 0.84375, prec 0.0728446, recall 0.903716
2017-12-10T15:34:01.291910: step 3151, loss 0.642881, acc 0.78125, prec 0.0728421, recall 0.903749
2017-12-10T15:34:01.496075: step 3152, loss 0.551808, acc 0.875, prec 0.0728767, recall 0.903814
2017-12-10T15:34:01.691721: step 3153, loss 0.287568, acc 0.90625, prec 0.07289, recall 0.903846
2017-12-10T15:34:01.892613: step 3154, loss 0.426221, acc 0.859375, prec 0.0729478, recall 0.903943
2017-12-10T15:34:02.088449: step 3155, loss 0.297326, acc 0.9375, prec 0.0729651, recall 0.903976
2017-12-10T15:34:02.283957: step 3156, loss 0.174839, acc 0.953125, prec 0.0730096, recall 0.90404
2017-12-10T15:34:02.480582: step 3157, loss 0.209995, acc 0.9375, prec 0.0730772, recall 0.904137
2017-12-10T15:34:02.682759: step 3158, loss 0.277294, acc 0.953125, prec 0.0730965, recall 0.904169
2017-12-10T15:34:02.876386: step 3159, loss 0.091945, acc 0.96875, prec 0.0730925, recall 0.904169
2017-12-10T15:34:03.077716: step 3160, loss 0.111306, acc 0.96875, prec 0.0730885, recall 0.904169
2017-12-10T15:34:03.271601: step 3161, loss 0.143868, acc 0.921875, prec 0.0730786, recall 0.904169
2017-12-10T15:34:03.466518: step 3162, loss 0.0702664, acc 0.96875, prec 0.0730998, recall 0.904202
2017-12-10T15:34:03.660993: step 3163, loss 0.153891, acc 0.9375, prec 0.0730919, recall 0.904202
2017-12-10T15:34:03.857828: step 3164, loss 0.0989988, acc 0.96875, prec 0.0731131, recall 0.904234
2017-12-10T15:34:04.054536: step 3165, loss 0.0957514, acc 0.953125, prec 0.0731071, recall 0.904234
2017-12-10T15:34:04.249050: step 3166, loss 0.0466251, acc 1, prec 0.0731071, recall 0.904234
2017-12-10T15:34:04.444796: step 3167, loss 0.477769, acc 0.953125, prec 0.0731263, recall 0.904266
2017-12-10T15:34:04.648674: step 3168, loss 0.0911997, acc 0.953125, prec 0.0731204, recall 0.904266
2017-12-10T15:34:04.848119: step 3169, loss 0.0655411, acc 0.984375, prec 0.0731184, recall 0.904266
2017-12-10T15:34:05.046904: step 3170, loss 0.265153, acc 0.96875, prec 0.0731648, recall 0.90433
2017-12-10T15:34:05.243764: step 3171, loss 2.52299, acc 0.984375, prec 0.0732151, recall 0.904091
2017-12-10T15:34:05.442049: step 3172, loss 0.134792, acc 0.953125, prec 0.0732091, recall 0.904091
2017-12-10T15:34:05.642339: step 3173, loss 0.0749976, acc 0.984375, prec 0.0732323, recall 0.904123
2017-12-10T15:34:05.841400: step 3174, loss 0.193477, acc 0.9375, prec 0.0732747, recall 0.904188
2017-12-10T15:34:06.039958: step 3175, loss 0.148803, acc 0.953125, prec 0.0732687, recall 0.904188
2017-12-10T15:34:06.240536: step 3176, loss 0.187625, acc 0.953125, prec 0.0733131, recall 0.904252
2017-12-10T15:34:06.437101: step 3177, loss 0.0720304, acc 0.9375, prec 0.0733051, recall 0.904252
2017-12-10T15:34:06.635898: step 3178, loss 0.386051, acc 0.953125, prec 0.0733494, recall 0.904316
2017-12-10T15:34:06.833444: step 3179, loss 0.43367, acc 0.90625, prec 0.0733626, recall 0.904348
2017-12-10T15:34:07.031278: step 3180, loss 0.435757, acc 0.875, prec 0.073397, recall 0.904412
2017-12-10T15:34:07.226062: step 3181, loss 0.129602, acc 0.984375, prec 0.0734201, recall 0.904444
2017-12-10T15:34:07.421186: step 3182, loss 0.188645, acc 0.90625, prec 0.0734333, recall 0.904476
2017-12-10T15:34:07.619077: step 3183, loss 0.319123, acc 0.953125, prec 0.0734273, recall 0.904476
2017-12-10T15:34:07.813428: step 3184, loss 0.547919, acc 0.859375, prec 0.0734094, recall 0.904476
2017-12-10T15:34:08.014239: step 3185, loss 0.576751, acc 0.875, prec 0.0734437, recall 0.904539
2017-12-10T15:34:08.213413: step 3186, loss 0.170673, acc 0.90625, prec 0.0734569, recall 0.904571
2017-12-10T15:34:08.411849: step 3187, loss 0.617919, acc 0.890625, prec 0.0735434, recall 0.904698
2017-12-10T15:34:08.607073: step 3188, loss 0.176325, acc 0.9375, prec 0.0735856, recall 0.904762
2017-12-10T15:34:08.806541: step 3189, loss 0.217848, acc 0.953125, prec 0.0736047, recall 0.904794
2017-12-10T15:34:09.004913: step 3190, loss 0.34978, acc 0.875, prec 0.0736389, recall 0.904857
2017-12-10T15:34:09.204811: step 3191, loss 0.622723, acc 0.921875, prec 0.073654, recall 0.904889
2017-12-10T15:34:09.400665: step 3192, loss 0.186805, acc 0.9375, prec 0.0736711, recall 0.90492
2017-12-10T15:34:09.596288: step 3193, loss 0.24806, acc 0.875, prec 0.0736802, recall 0.904952
2017-12-10T15:34:09.790320: step 3194, loss 0.104298, acc 0.96875, prec 0.0736762, recall 0.904952
2017-12-10T15:34:09.986067: step 3195, loss 0.10947, acc 0.953125, prec 0.0736953, recall 0.904983
2017-12-10T15:34:10.185900: step 3196, loss 0.282187, acc 0.9375, prec 0.0737124, recall 0.905015
2017-12-10T15:34:10.384948: step 3197, loss 0.127612, acc 0.96875, prec 0.0737335, recall 0.905046
2017-12-10T15:34:10.585616: step 3198, loss 0.274663, acc 0.921875, prec 0.0737485, recall 0.905078
2017-12-10T15:34:10.784102: step 3199, loss 0.138505, acc 0.953125, prec 0.0737676, recall 0.905109
2017-12-10T15:34:10.985121: step 3200, loss 0.0770407, acc 0.96875, prec 0.0737636, recall 0.905109
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_2/1512937383/checkpoints/model-3200

2017-12-10T15:34:12.631870: step 3201, loss 0.088204, acc 0.96875, prec 0.0737596, recall 0.905109
2017-12-10T15:34:12.827734: step 3202, loss 0.2094, acc 0.921875, prec 0.0737497, recall 0.905109
2017-12-10T15:34:13.024265: step 3203, loss 0.0773658, acc 0.96875, prec 0.0737707, recall 0.905141
2017-12-10T15:34:13.221619: step 3204, loss 0.131892, acc 0.984375, prec 0.0737938, recall 0.905172
2017-12-10T15:34:13.416877: step 3205, loss 0.6061, acc 0.953125, prec 0.0738128, recall 0.905204
2017-12-10T15:34:13.615985: step 3206, loss 0.237214, acc 0.984375, prec 0.0738609, recall 0.905267
2017-12-10T15:34:13.818172: step 3207, loss 0.132268, acc 0.9375, prec 0.0738779, recall 0.905298
2017-12-10T15:34:14.018592: step 3208, loss 1.22676, acc 0.96875, prec 0.0739009, recall 0.90503
2017-12-10T15:34:14.221251: step 3209, loss 0.193682, acc 0.984375, prec 0.073999, recall 0.905155
2017-12-10T15:34:14.425345: step 3210, loss 0.421673, acc 0.921875, prec 0.074014, recall 0.905187
2017-12-10T15:34:14.626331: step 3211, loss 0.349481, acc 0.96875, prec 0.0740601, recall 0.905249
2017-12-10T15:34:14.822547: step 3212, loss 0.210151, acc 0.9375, prec 0.0740521, recall 0.905249
2017-12-10T15:34:15.022718: step 3213, loss 0.259428, acc 0.890625, prec 0.0740631, recall 0.905281
2017-12-10T15:34:15.220216: step 3214, loss 0.163016, acc 0.9375, prec 0.0740551, recall 0.905281
2017-12-10T15:34:15.415872: step 3215, loss 0.18326, acc 0.96875, prec 0.0741011, recall 0.905343
2017-12-10T15:34:15.616451: step 3216, loss 0.307794, acc 0.90625, prec 0.0741141, recall 0.905374
2017-12-10T15:34:15.813039: step 3217, loss 0.196692, acc 0.953125, prec 0.0741081, recall 0.905374
2017-12-10T15:34:16.011261: step 3218, loss 0.27063, acc 0.921875, prec 0.074123, recall 0.905405
2017-12-10T15:34:16.204912: step 3219, loss 0.205033, acc 0.953125, prec 0.074142, recall 0.905437
2017-12-10T15:34:16.404520: step 3220, loss 0.275427, acc 0.90625, prec 0.07413, recall 0.905437
2017-12-10T15:34:16.601152: step 3221, loss 2.61832, acc 0.875, prec 0.074166, recall 0.905201
2017-12-10T15:34:16.801666: step 3222, loss 0.29577, acc 0.875, prec 0.0741999, recall 0.905263
2017-12-10T15:34:16.994671: step 3223, loss 0.16586, acc 0.9375, prec 0.0742169, recall 0.905294
2017-12-10T15:34:17.189596: step 3224, loss 0.684601, acc 0.796875, prec 0.0741909, recall 0.905294
2017-12-10T15:34:17.393580: step 3225, loss 0.382402, acc 0.890625, prec 0.0742018, recall 0.905325
2017-12-10T15:34:17.590485: step 3226, loss 0.810582, acc 0.78125, prec 0.0741988, recall 0.905357
2017-12-10T15:34:17.791436: step 3227, loss 0.603518, acc 0.78125, prec 0.0741708, recall 0.905357
2017-12-10T15:34:17.984450: step 3228, loss 0.769089, acc 0.765625, prec 0.0741409, recall 0.905357
2017-12-10T15:34:18.178874: step 3229, loss 0.289462, acc 0.890625, prec 0.0742265, recall 0.905481
2017-12-10T15:34:18.376419: step 3230, loss 0.424416, acc 0.828125, prec 0.0742046, recall 0.905481
2017-12-10T15:34:18.572646: step 3231, loss 0.279512, acc 0.859375, prec 0.0741866, recall 0.905481
2017-12-10T15:34:18.769360: step 3232, loss 0.288572, acc 0.890625, prec 0.0741975, recall 0.905512
2017-12-10T15:34:18.965335: step 3233, loss 0.484443, acc 0.859375, prec 0.0741796, recall 0.905512
2017-12-10T15:34:19.161714: step 3234, loss 0.210421, acc 0.875, prec 0.0741636, recall 0.905512
2017-12-10T15:34:19.356908: step 3235, loss 0.324064, acc 0.90625, prec 0.0741766, recall 0.905543
2017-12-10T15:34:19.554416: step 3236, loss 0.277439, acc 0.90625, prec 0.0741646, recall 0.905543
2017-12-10T15:34:19.753207: step 3237, loss 0.291701, acc 0.9375, prec 0.0742064, recall 0.905605
2017-12-10T15:34:19.954936: step 3238, loss 0.231115, acc 0.921875, prec 0.0742461, recall 0.905667
2017-12-10T15:34:20.149538: step 3239, loss 0.114504, acc 0.9375, prec 0.0742382, recall 0.905667
2017-12-10T15:34:20.346057: step 3240, loss 0.0671412, acc 0.984375, prec 0.0742362, recall 0.905667
2017-12-10T15:34:20.540285: step 3241, loss 0.457507, acc 0.90625, prec 0.0742491, recall 0.905697
2017-12-10T15:34:20.739422: step 3242, loss 0.0698062, acc 0.96875, prec 0.0742699, recall 0.905728
2017-12-10T15:34:20.938856: step 3243, loss 0.0857652, acc 0.984375, prec 0.0742679, recall 0.905728
2017-12-10T15:34:21.136353: step 3244, loss 0.150356, acc 0.921875, prec 0.074258, recall 0.905728
2017-12-10T15:34:21.336432: step 3245, loss 0.320116, acc 0.953125, prec 0.0742768, recall 0.905759
2017-12-10T15:34:21.541763: step 3246, loss 0.0948299, acc 0.953125, prec 0.0742957, recall 0.90579
2017-12-10T15:34:21.738707: step 3247, loss 0.215473, acc 0.953125, prec 0.0743394, recall 0.905852
2017-12-10T15:34:21.940695: step 3248, loss 0.0779332, acc 0.953125, prec 0.0743334, recall 0.905852
2017-12-10T15:34:22.138541: step 3249, loss 1.87545, acc 0.96875, prec 0.0743811, recall 0.905617
2017-12-10T15:34:22.342182: step 3250, loss 0.214659, acc 0.984375, prec 0.0744039, recall 0.905648
2017-12-10T15:34:22.546636: step 3251, loss 0.364105, acc 0.96875, prec 0.0744247, recall 0.905679
2017-12-10T15:34:22.746698: step 3252, loss 0.284202, acc 0.9375, prec 0.0744416, recall 0.90571
2017-12-10T15:34:22.945646: step 3253, loss 0.133248, acc 0.96875, prec 0.0744872, recall 0.905771
2017-12-10T15:34:23.139725: step 3254, loss 0.194593, acc 0.984375, prec 0.0745596, recall 0.905863
2017-12-10T15:34:23.336676: step 3255, loss 0.0687798, acc 0.96875, prec 0.0745556, recall 0.905863
2017-12-10T15:34:23.535539: step 3256, loss 0.293157, acc 1, prec 0.0746301, recall 0.905955
2017-12-10T15:34:23.740608: step 3257, loss 0.176736, acc 0.953125, prec 0.0746489, recall 0.905986
2017-12-10T15:34:23.940615: step 3258, loss 0.181413, acc 0.9375, prec 0.0746409, recall 0.905986
2017-12-10T15:34:24.142492: step 3259, loss 0.288938, acc 0.890625, prec 0.0746269, recall 0.905986
2017-12-10T15:34:24.336555: step 3260, loss 0.213484, acc 0.953125, prec 0.0746209, recall 0.905986
2017-12-10T15:34:24.538989: step 3261, loss 0.112004, acc 0.953125, prec 0.0746397, recall 0.906016
2017-12-10T15:34:24.733224: step 3262, loss 0.227434, acc 0.890625, prec 0.0746505, recall 0.906047
2017-12-10T15:34:24.933105: step 3263, loss 0.161238, acc 0.953125, prec 0.0746445, recall 0.906047
2017-12-10T15:34:25.132645: step 3264, loss 0.100314, acc 0.953125, prec 0.0746385, recall 0.906047
2017-12-10T15:34:25.329854: step 3265, loss 0.10378, acc 0.9375, prec 0.0746305, recall 0.906047
2017-12-10T15:34:25.523432: step 3266, loss 0.0548888, acc 0.96875, prec 0.074676, recall 0.906108
2017-12-10T15:34:25.721690: step 3267, loss 0.150627, acc 0.953125, prec 0.0747196, recall 0.906169
2017-12-10T15:34:25.920579: step 3268, loss 0.182568, acc 0.953125, prec 0.0747631, recall 0.90623
2017-12-10T15:34:26.117452: step 3269, loss 0.196967, acc 0.9375, prec 0.0747799, recall 0.90626
2017-12-10T15:34:26.311770: step 3270, loss 0.0334488, acc 1, prec 0.0747799, recall 0.90626
2017-12-10T15:34:26.509799: step 3271, loss 0.168143, acc 0.921875, prec 0.0747699, recall 0.90626
2017-12-10T15:34:26.708109: step 3272, loss 0.750706, acc 0.9375, prec 0.0748114, recall 0.906321
2017-12-10T15:34:26.906575: step 3273, loss 0.160158, acc 0.96875, prec 0.0748074, recall 0.906321
2017-12-10T15:34:27.101262: step 3274, loss 0.0652145, acc 0.984375, prec 0.0748301, recall 0.906351
2017-12-10T15:34:27.294497: step 3275, loss 0.201789, acc 0.9375, prec 0.0748716, recall 0.906412
2017-12-10T15:34:27.493966: step 3276, loss 0.0989504, acc 0.9375, prec 0.0748883, recall 0.906442
2017-12-10T15:34:27.694699: step 3277, loss 0.074822, acc 0.984375, prec 0.0748863, recall 0.906442
2017-12-10T15:34:27.894042: step 3278, loss 0.0844708, acc 0.984375, prec 0.0749338, recall 0.906503
2017-12-10T15:34:28.091210: step 3279, loss 0.193511, acc 0.953125, prec 0.0749278, recall 0.906503
2017-12-10T15:34:28.291562: step 3280, loss 0.0933505, acc 1, prec 0.075002, recall 0.906593
2017-12-10T15:34:28.493127: step 3281, loss 1.1654, acc 0.921875, prec 0.0750909, recall 0.906714
2017-12-10T15:34:28.692713: step 3282, loss 0.147572, acc 0.9375, prec 0.0751323, recall 0.906774
2017-12-10T15:34:28.890125: step 3283, loss 0.176416, acc 0.96875, prec 0.075153, recall 0.906804
2017-12-10T15:34:29.094042: step 3284, loss 0.289441, acc 0.953125, prec 0.0751717, recall 0.906834
2017-12-10T15:34:29.297750: step 3285, loss 0.152733, acc 0.953125, prec 0.0752398, recall 0.906924
2017-12-10T15:34:29.499942: step 3286, loss 0.298211, acc 0.90625, prec 0.0752277, recall 0.906924
2017-12-10T15:34:29.693687: step 3287, loss 0.206529, acc 0.953125, prec 0.0752217, recall 0.906924
2017-12-10T15:34:29.897075: step 3288, loss 0.314628, acc 0.9375, prec 0.0752384, recall 0.906954
2017-12-10T15:34:30.093741: step 3289, loss 0.17211, acc 0.953125, prec 0.075257, recall 0.906984
2017-12-10T15:34:30.292827: step 3290, loss 0.0660858, acc 0.984375, prec 0.0752797, recall 0.907014
2017-12-10T15:34:30.488217: step 3291, loss 0.353836, acc 0.890625, prec 0.0752904, recall 0.907044
2017-12-10T15:34:30.688569: step 3292, loss 0.139585, acc 0.96875, prec 0.075311, recall 0.907074
2017-12-10T15:34:30.888945: step 3293, loss 0.153181, acc 0.9375, prec 0.075303, recall 0.907074
2017-12-10T15:34:31.089387: step 3294, loss 0.141035, acc 0.9375, prec 0.0753196, recall 0.907104
2017-12-10T15:34:31.292939: step 3295, loss 0.117768, acc 0.953125, prec 0.0753629, recall 0.907164
2017-12-10T15:34:31.495489: step 3296, loss 0.283108, acc 0.953125, prec 0.0754062, recall 0.907223
2017-12-10T15:34:31.693693: step 3297, loss 0.183799, acc 0.953125, prec 0.0754002, recall 0.907223
2017-12-10T15:34:31.891411: step 3298, loss 0.0455304, acc 0.984375, prec 0.0753982, recall 0.907223
2017-12-10T15:34:32.090310: step 3299, loss 0.128831, acc 0.953125, prec 0.0754662, recall 0.907312
2017-12-10T15:34:32.285573: step 3300, loss 2.68408, acc 0.921875, prec 0.0754828, recall 0.907051
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_2/1512937383/checkpoints/model-3300

2017-12-10T15:34:33.540559: step 3301, loss 0.135611, acc 0.953125, prec 0.075526, recall 0.907111
2017-12-10T15:34:33.738888: step 3302, loss 0.111156, acc 0.96875, prec 0.0755467, recall 0.907141
2017-12-10T15:34:33.936051: step 3303, loss 0.164169, acc 0.953125, prec 0.0755653, recall 0.90717
2017-12-10T15:34:34.138265: step 3304, loss 0.55219, acc 0.859375, prec 0.0755718, recall 0.9072
2017-12-10T15:34:34.334791: step 3305, loss 0.249618, acc 0.953125, prec 0.0755904, recall 0.90723
2017-12-10T15:34:34.534039: step 3306, loss 0.256004, acc 0.9375, prec 0.0755823, recall 0.90723
2017-12-10T15:34:34.728425: step 3307, loss 0.216647, acc 0.9375, prec 0.0756235, recall 0.907289
2017-12-10T15:34:34.923606: step 3308, loss 0.156371, acc 0.890625, prec 0.0756094, recall 0.907289
2017-12-10T15:34:35.120879: step 3309, loss 0.332676, acc 0.890625, prec 0.07562, recall 0.907319
2017-12-10T15:34:35.317856: step 3310, loss 0.384482, acc 0.921875, prec 0.0756591, recall 0.907378
2017-12-10T15:34:35.515913: step 3311, loss 0.439915, acc 0.859375, prec 0.0756656, recall 0.907407
2017-12-10T15:34:35.715067: step 3312, loss 0.196334, acc 0.953125, prec 0.0756596, recall 0.907407
2017-12-10T15:34:35.909592: step 3313, loss 0.428457, acc 0.90625, prec 0.0756721, recall 0.907437
2017-12-10T15:34:36.106297: step 3314, loss 0.168499, acc 0.9375, prec 0.0756886, recall 0.907466
2017-12-10T15:34:36.300333: step 3315, loss 0.180837, acc 0.9375, prec 0.0757052, recall 0.907496
2017-12-10T15:34:36.499714: step 3316, loss 0.0943124, acc 0.953125, prec 0.0757729, recall 0.907584
2017-12-10T15:34:36.699957: step 3317, loss 0.467346, acc 0.828125, prec 0.0757507, recall 0.907584
2017-12-10T15:34:36.898215: step 3318, loss 0.104894, acc 0.96875, prec 0.0757713, recall 0.907614
2017-12-10T15:34:37.094627: step 3319, loss 0.212272, acc 0.96875, prec 0.075841, recall 0.907702
2017-12-10T15:34:37.296315: step 3320, loss 0.0516731, acc 0.984375, prec 0.0758635, recall 0.907731
2017-12-10T15:34:37.493069: step 3321, loss 0.208855, acc 0.953125, prec 0.0758575, recall 0.907731
2017-12-10T15:34:37.690765: step 3322, loss 0.173763, acc 0.921875, prec 0.0758474, recall 0.907731
2017-12-10T15:34:37.888800: step 3323, loss 0.255372, acc 0.890625, prec 0.0758579, recall 0.907761
2017-12-10T15:34:38.084763: step 3324, loss 0.0656428, acc 0.96875, prec 0.0758538, recall 0.907761
2017-12-10T15:34:38.280245: step 3325, loss 0.0446071, acc 0.984375, prec 0.0758518, recall 0.907761
2017-12-10T15:34:38.481485: step 3326, loss 0.514358, acc 0.96875, prec 0.0759214, recall 0.907849
2017-12-10T15:34:38.684696: step 3327, loss 0.0960681, acc 1, prec 0.0760197, recall 0.907966
2017-12-10T15:34:38.882924: step 3328, loss 0.0745851, acc 0.96875, prec 0.0760156, recall 0.907966
2017-12-10T15:34:39.082561: step 3329, loss 0.0573425, acc 0.984375, prec 0.0760627, recall 0.908024
2017-12-10T15:34:39.278228: step 3330, loss 0.721825, acc 0.9375, prec 0.0761037, recall 0.908082
2017-12-10T15:34:39.482011: step 3331, loss 0.117656, acc 0.96875, prec 0.0760997, recall 0.908082
2017-12-10T15:34:39.681711: step 3332, loss 0.391843, acc 0.90625, prec 0.0760875, recall 0.908082
2017-12-10T15:34:39.884346: step 3333, loss 0.0748246, acc 0.96875, prec 0.076108, recall 0.908112
2017-12-10T15:34:40.082036: step 3334, loss 0.12672, acc 0.953125, prec 0.076102, recall 0.908112
2017-12-10T15:34:40.279409: step 3335, loss 4.11953, acc 0.953125, prec 0.0761225, recall 0.907853
2017-12-10T15:34:40.484032: step 3336, loss 0.0618271, acc 0.96875, prec 0.0761184, recall 0.907853
2017-12-10T15:34:40.682480: step 3337, loss 0.0314508, acc 1, prec 0.0761429, recall 0.907882
2017-12-10T15:34:40.879407: step 3338, loss 0.149959, acc 0.96875, prec 0.0761634, recall 0.907911
2017-12-10T15:34:41.074902: step 3339, loss 0.0879643, acc 0.96875, prec 0.0761594, recall 0.907911
2017-12-10T15:34:41.272705: step 3340, loss 0.352439, acc 0.90625, prec 0.0761963, recall 0.90797
2017-12-10T15:34:41.469101: step 3341, loss 0.322552, acc 0.875, prec 0.0762046, recall 0.907999
2017-12-10T15:34:41.668833: step 3342, loss 0.491171, acc 0.84375, prec 0.0762824, recall 0.908115
2017-12-10T15:34:41.866484: step 3343, loss 0.229426, acc 0.9375, prec 0.0762988, recall 0.908144
2017-12-10T15:34:42.059925: step 3344, loss 0.256299, acc 0.875, prec 0.0763071, recall 0.908173
2017-12-10T15:34:42.253287: step 3345, loss 0.568324, acc 0.890625, prec 0.0763419, recall 0.908231
2017-12-10T15:34:42.450359: step 3346, loss 0.336264, acc 0.90625, prec 0.0763298, recall 0.908231
2017-12-10T15:34:42.647195: step 3347, loss 0.193449, acc 0.90625, prec 0.0763177, recall 0.908231
2017-12-10T15:34:42.843464: step 3348, loss 0.257144, acc 0.875, prec 0.0763504, recall 0.908289
2017-12-10T15:34:43.036332: step 3349, loss 0.836842, acc 0.828125, prec 0.0763527, recall 0.908318
2017-12-10T15:34:43.234604: step 3350, loss 0.276169, acc 0.96875, prec 0.076422, recall 0.908404
2017-12-10T15:34:43.434936: step 3351, loss 0.161292, acc 0.96875, prec 0.0765158, recall 0.908519
2017-12-10T15:34:43.630676: step 3352, loss 0.358796, acc 0.84375, prec 0.0764955, recall 0.908519
2017-12-10T15:34:43.830795: step 3353, loss 0.167115, acc 0.9375, prec 0.0765607, recall 0.908606
2017-12-10T15:34:44.031754: step 3354, loss 0.238128, acc 0.890625, prec 0.0765954, recall 0.908663
2017-12-10T15:34:44.233066: step 3355, loss 0.184512, acc 0.9375, prec 0.0766117, recall 0.908692
2017-12-10T15:34:44.436425: step 3356, loss 0.0603976, acc 0.96875, prec 0.0766565, recall 0.908749
2017-12-10T15:34:44.640924: step 3357, loss 4.76881, acc 0.921875, prec 0.0766484, recall 0.908464
2017-12-10T15:34:44.839253: step 3358, loss 0.270767, acc 0.875, prec 0.076681, recall 0.908521
2017-12-10T15:34:45.037363: step 3359, loss 0.164324, acc 0.953125, prec 0.076675, recall 0.908521
2017-12-10T15:34:45.231311: step 3360, loss 0.195133, acc 0.921875, prec 0.0766648, recall 0.908521
2017-12-10T15:34:45.426335: step 3361, loss 0.496566, acc 0.890625, prec 0.0766506, recall 0.908521
2017-12-10T15:34:45.622403: step 3362, loss 0.397217, acc 0.859375, prec 0.0766324, recall 0.908521
2017-12-10T15:34:45.825703: step 3363, loss 0.250382, acc 0.921875, prec 0.0766711, recall 0.908579
2017-12-10T15:34:46.022263: step 3364, loss 0.166263, acc 0.96875, prec 0.0766914, recall 0.908607
2017-12-10T15:34:46.220556: step 3365, loss 0.082905, acc 0.984375, prec 0.0766894, recall 0.908607
2017-12-10T15:34:46.418718: step 3366, loss 0.279558, acc 0.921875, prec 0.0767524, recall 0.908693
2017-12-10T15:34:46.615928: step 3367, loss 0.461701, acc 0.828125, prec 0.0767545, recall 0.908721
2017-12-10T15:34:46.813198: step 3368, loss 0.573299, acc 0.859375, prec 0.0767363, recall 0.908721
2017-12-10T15:34:47.011325: step 3369, loss 0.324916, acc 0.875, prec 0.0767201, recall 0.908721
2017-12-10T15:34:47.206353: step 3370, loss 0.200175, acc 0.921875, prec 0.0767343, recall 0.90875
2017-12-10T15:34:47.403880: step 3371, loss 0.510681, acc 0.90625, prec 0.0767222, recall 0.90875
2017-12-10T15:34:47.598044: step 3372, loss 0.316725, acc 0.9375, prec 0.0767141, recall 0.90875
2017-12-10T15:34:47.790858: step 3373, loss 0.305136, acc 0.9375, prec 0.0767303, recall 0.908778
2017-12-10T15:34:47.988128: step 3374, loss 1.10618, acc 0.9375, prec 0.0767953, recall 0.908864
2017-12-10T15:34:48.187411: step 3375, loss 0.231427, acc 0.90625, prec 0.0768075, recall 0.908892
2017-12-10T15:34:48.389337: step 3376, loss 0.204592, acc 0.921875, prec 0.0767973, recall 0.908892
2017-12-10T15:34:48.585681: step 3377, loss 0.231442, acc 0.953125, prec 0.0768156, recall 0.908921
2017-12-10T15:34:48.778932: step 3378, loss 0.106638, acc 0.953125, prec 0.0768339, recall 0.908949
2017-12-10T15:34:48.974354: step 3379, loss 0.0922946, acc 0.9375, prec 0.0768258, recall 0.908949
2017-12-10T15:34:49.171461: step 3380, loss 0.120698, acc 0.96875, prec 0.0768217, recall 0.908949
2017-12-10T15:34:49.375309: step 3381, loss 0.0915204, acc 0.953125, prec 0.0768156, recall 0.908949
2017-12-10T15:34:49.571662: step 3382, loss 0.160928, acc 0.953125, prec 0.0768096, recall 0.908949
2017-12-10T15:34:49.765349: step 3383, loss 0.24671, acc 0.90625, prec 0.0768218, recall 0.908978
2017-12-10T15:34:49.964898: step 3384, loss 0.366244, acc 0.96875, prec 0.076842, recall 0.909006
2017-12-10T15:34:50.166860: step 3385, loss 0.185369, acc 0.9375, prec 0.0768582, recall 0.909034
2017-12-10T15:34:50.364139: step 3386, loss 0.221012, acc 0.921875, prec 0.0768724, recall 0.909063
2017-12-10T15:34:50.561046: step 3387, loss 0.267899, acc 0.90625, prec 0.0768846, recall 0.909091
2017-12-10T15:34:50.755743: step 3388, loss 1.45316, acc 0.90625, prec 0.0768745, recall 0.908808
2017-12-10T15:34:50.956668: step 3389, loss 0.123572, acc 0.984375, prec 0.0768967, recall 0.908836
2017-12-10T15:34:51.151319: step 3390, loss 0.0465008, acc 0.984375, prec 0.0768947, recall 0.908836
2017-12-10T15:34:51.348456: step 3391, loss 0.293303, acc 0.921875, prec 0.0769089, recall 0.908865
2017-12-10T15:34:51.549434: step 3392, loss 0.101977, acc 0.96875, prec 0.0769049, recall 0.908865
2017-12-10T15:34:51.743641: step 3393, loss 0.11966, acc 0.953125, prec 0.0769231, recall 0.908893
2017-12-10T15:34:51.942313: step 3394, loss 0.110032, acc 0.96875, prec 0.076919, recall 0.908893
2017-12-10T15:34:52.141944: step 3395, loss 0.26177, acc 0.953125, prec 0.0769615, recall 0.90895
2017-12-10T15:34:52.334043: step 3396, loss 0.518248, acc 0.921875, prec 0.077, recall 0.909006
2017-12-10T15:34:52.536004: step 3397, loss 0.184211, acc 0.921875, prec 0.0770141, recall 0.909034
2017-12-10T15:34:52.733616: step 3398, loss 0.321915, acc 0.921875, prec 0.0770525, recall 0.909091
2017-12-10T15:34:52.929078: step 3399, loss 0.981332, acc 0.953125, prec 0.077095, recall 0.909147
2017-12-10T15:34:53.128801: step 3400, loss 0.275921, acc 0.890625, prec 0.0771051, recall 0.909175
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_2/1512937383/checkpoints/model-3400

2017-12-10T15:34:54.357190: step 3401, loss 0.181848, acc 0.921875, prec 0.0770949, recall 0.909175
2017-12-10T15:34:54.551188: step 3402, loss 0.180375, acc 0.96875, prec 0.0770909, recall 0.909175
2017-12-10T15:34:54.746753: step 3403, loss 1.09452, acc 0.875, prec 0.0770989, recall 0.909204
2017-12-10T15:34:54.947448: step 3404, loss 0.162888, acc 0.96875, prec 0.0771191, recall 0.909232
2017-12-10T15:34:55.142361: step 3405, loss 0.1793, acc 0.921875, prec 0.077109, recall 0.909232
2017-12-10T15:34:55.333452: step 3406, loss 0.277479, acc 0.90625, prec 0.0771211, recall 0.90926
2017-12-10T15:34:55.536402: step 3407, loss 0.408197, acc 0.921875, prec 0.077111, recall 0.90926
2017-12-10T15:34:55.734429: step 3408, loss 0.243686, acc 0.90625, prec 0.0770988, recall 0.90926
2017-12-10T15:34:55.928964: step 3409, loss 0.192612, acc 0.90625, prec 0.0770867, recall 0.90926
2017-12-10T15:34:56.130793: step 3410, loss 0.0765402, acc 0.953125, prec 0.0771291, recall 0.909316
2017-12-10T15:34:56.328317: step 3411, loss 0.0620581, acc 0.96875, prec 0.077125, recall 0.909316
2017-12-10T15:34:56.524316: step 3412, loss 0.577301, acc 0.890625, prec 0.0771351, recall 0.909344
2017-12-10T15:34:56.724273: step 3413, loss 0.209943, acc 0.953125, prec 0.0771532, recall 0.909372
2017-12-10T15:34:56.924343: step 3414, loss 0.674433, acc 0.96875, prec 0.0771734, recall 0.9094
2017-12-10T15:34:57.131182: step 3415, loss 0.146547, acc 0.9375, prec 0.0771653, recall 0.9094
2017-12-10T15:34:57.325819: step 3416, loss 0.315604, acc 0.875, prec 0.0771733, recall 0.909428
2017-12-10T15:34:57.524040: step 3417, loss 0.216299, acc 0.921875, prec 0.0772116, recall 0.909484
2017-12-10T15:34:57.725347: step 3418, loss 0.199096, acc 0.921875, prec 0.0772014, recall 0.909484
2017-12-10T15:34:57.929278: step 3419, loss 0.205221, acc 0.9375, prec 0.0771933, recall 0.909484
2017-12-10T15:34:58.123673: step 3420, loss 0.525761, acc 0.890625, prec 0.0772034, recall 0.909512
2017-12-10T15:34:58.326057: step 3421, loss 0.216165, acc 0.921875, prec 0.0772174, recall 0.90954
2017-12-10T15:34:58.521876: step 3422, loss 0.320185, acc 0.890625, prec 0.0772033, recall 0.90954
2017-12-10T15:34:58.721539: step 3423, loss 0.119221, acc 0.953125, prec 0.0771972, recall 0.90954
2017-12-10T15:34:58.919864: step 3424, loss 0.284194, acc 0.921875, prec 0.0771871, recall 0.90954
2017-12-10T15:34:59.117922: step 3425, loss 0.0572586, acc 0.984375, prec 0.0771851, recall 0.90954
2017-12-10T15:34:59.322523: step 3426, loss 0.225365, acc 0.90625, prec 0.0771971, recall 0.909568
2017-12-10T15:34:59.520747: step 3427, loss 0.431207, acc 0.90625, prec 0.0772575, recall 0.909652
2017-12-10T15:34:59.716941: step 3428, loss 0.137305, acc 0.96875, prec 0.0773259, recall 0.909735
2017-12-10T15:34:59.917445: step 3429, loss 0.0173102, acc 1, prec 0.0773259, recall 0.909735
2017-12-10T15:35:00.115316: step 3430, loss 0.0782206, acc 0.984375, prec 0.0773481, recall 0.909763
2017-12-10T15:35:00.311585: step 3431, loss 0.0288494, acc 1, prec 0.0773481, recall 0.909763
2017-12-10T15:35:00.514261: step 3432, loss 0.164142, acc 0.953125, prec 0.0773662, recall 0.909791
2017-12-10T15:35:00.715263: step 3433, loss 1.91078, acc 0.890625, prec 0.077354, recall 0.909511
2017-12-10T15:35:00.914806: step 3434, loss 0.223926, acc 0.96875, prec 0.0773982, recall 0.909566
2017-12-10T15:35:01.109332: step 3435, loss 0.100899, acc 0.9375, prec 0.0774143, recall 0.909594
2017-12-10T15:35:01.317046: step 3436, loss 1.04995, acc 0.953125, prec 0.0774806, recall 0.909677
2017-12-10T15:35:01.523372: step 3437, loss 0.25525, acc 0.921875, prec 0.0774705, recall 0.909677
2017-12-10T15:35:01.730479: step 3438, loss 0.16937, acc 0.9375, prec 0.0774865, recall 0.909705
2017-12-10T15:35:01.928180: step 3439, loss 0.397018, acc 0.921875, prec 0.0775005, recall 0.909733
2017-12-10T15:35:02.124376: step 3440, loss 0.298254, acc 0.921875, prec 0.0775145, recall 0.909761
2017-12-10T15:35:02.318199: step 3441, loss 0.458951, acc 0.890625, prec 0.0775003, recall 0.909761
2017-12-10T15:35:02.512381: step 3442, loss 0.808561, acc 0.8125, prec 0.0775001, recall 0.909788
2017-12-10T15:35:02.705328: step 3443, loss 0.339899, acc 0.875, prec 0.0774839, recall 0.909788
2017-12-10T15:35:02.901749: step 3444, loss 0.315669, acc 0.875, prec 0.07754, recall 0.909871
2017-12-10T15:35:03.098756: step 3445, loss 0.37277, acc 0.859375, prec 0.07757, recall 0.909926
2017-12-10T15:35:03.300363: step 3446, loss 0.664902, acc 0.90625, prec 0.077606, recall 0.909982
2017-12-10T15:35:03.499619: step 3447, loss 0.311762, acc 0.9375, prec 0.0775979, recall 0.909982
2017-12-10T15:35:03.696298: step 3448, loss 0.246862, acc 0.90625, prec 0.0776098, recall 0.910009
2017-12-10T15:35:03.892266: step 3449, loss 0.491745, acc 0.859375, prec 0.0775916, recall 0.910009
2017-12-10T15:35:04.090516: step 3450, loss 0.274332, acc 0.90625, prec 0.0775795, recall 0.910009
2017-12-10T15:35:04.292231: step 3451, loss 0.191328, acc 0.953125, prec 0.0775734, recall 0.910009
2017-12-10T15:35:04.489856: step 3452, loss 0.244238, acc 0.890625, prec 0.0775592, recall 0.910009
2017-12-10T15:35:04.688785: step 3453, loss 0.151742, acc 0.96875, prec 0.0775552, recall 0.910009
2017-12-10T15:35:04.880797: step 3454, loss 0.433535, acc 0.921875, prec 0.0775691, recall 0.910037
2017-12-10T15:35:05.073278: step 3455, loss 0.185598, acc 0.921875, prec 0.0775831, recall 0.910064
2017-12-10T15:35:05.272737: step 3456, loss 0.185789, acc 0.984375, prec 0.0776291, recall 0.910119
2017-12-10T15:35:05.469722: step 3457, loss 0.0933269, acc 0.984375, prec 0.0776271, recall 0.910119
2017-12-10T15:35:05.672417: step 3458, loss 0.597491, acc 0.921875, prec 0.0776651, recall 0.910174
2017-12-10T15:35:05.872482: step 3459, loss 0.393708, acc 0.953125, prec 0.0776831, recall 0.910202
2017-12-10T15:35:06.077446: step 3460, loss 0.0402751, acc 0.984375, prec 0.077681, recall 0.910202
2017-12-10T15:35:06.272493: step 3461, loss 1.94593, acc 0.96875, prec 0.077679, recall 0.909924
2017-12-10T15:35:06.478938: step 3462, loss 0.675802, acc 0.953125, prec 0.077697, recall 0.909951
2017-12-10T15:35:06.679090: step 3463, loss 0.128842, acc 0.953125, prec 0.0776909, recall 0.909951
2017-12-10T15:35:06.875044: step 3464, loss 0.349159, acc 0.90625, prec 0.0777268, recall 0.910006
2017-12-10T15:35:07.074811: step 3465, loss 0.19821, acc 0.890625, prec 0.0777126, recall 0.910006
2017-12-10T15:35:07.275357: step 3466, loss 0.307037, acc 0.9375, prec 0.0777286, recall 0.910034
2017-12-10T15:35:07.471322: step 3467, loss 0.293606, acc 0.90625, prec 0.0777885, recall 0.910116
2017-12-10T15:35:07.666646: step 3468, loss 0.69726, acc 0.875, prec 0.0777723, recall 0.910116
2017-12-10T15:35:07.857203: step 3469, loss 0.344495, acc 0.921875, prec 0.0778102, recall 0.91017
2017-12-10T15:35:08.048981: step 3470, loss 0.512114, acc 0.796875, prec 0.0778079, recall 0.910198
2017-12-10T15:35:08.243521: step 3471, loss 0.294821, acc 0.90625, prec 0.0778197, recall 0.910225
2017-12-10T15:35:08.438594: step 3472, loss 1.24835, acc 0.890625, prec 0.0778295, recall 0.910253
2017-12-10T15:35:08.635837: step 3473, loss 0.210909, acc 0.890625, prec 0.0778393, recall 0.91028
2017-12-10T15:35:08.831126: step 3474, loss 0.28453, acc 0.890625, prec 0.0778252, recall 0.91028
2017-12-10T15:35:09.026508: step 3475, loss 0.21297, acc 0.90625, prec 0.077813, recall 0.91028
2017-12-10T15:35:09.225645: step 3476, loss 0.7001, acc 0.8125, prec 0.0777887, recall 0.91028
2017-12-10T15:35:09.422608: step 3477, loss 0.233523, acc 0.90625, prec 0.0777766, recall 0.91028
2017-12-10T15:35:09.620608: step 3478, loss 0.243511, acc 0.921875, prec 0.0777905, recall 0.910307
2017-12-10T15:35:09.801926: step 3479, loss 0.250619, acc 0.942308, prec 0.0778084, recall 0.910334
2017-12-10T15:35:10.001894: step 3480, loss 0.329456, acc 0.921875, prec 0.0777983, recall 0.910334
2017-12-10T15:35:10.198525: step 3481, loss 0.268637, acc 0.953125, prec 0.0777922, recall 0.910334
2017-12-10T15:35:10.399790: step 3482, loss 0.412868, acc 0.890625, prec 0.077802, recall 0.910362
2017-12-10T15:35:10.593828: step 3483, loss 1.32006, acc 0.9375, prec 0.077796, recall 0.910085
2017-12-10T15:35:10.799200: step 3484, loss 0.266145, acc 0.875, prec 0.0777798, recall 0.910085
2017-12-10T15:35:10.994950: step 3485, loss 0.363009, acc 0.9375, prec 0.0777957, recall 0.910112
2017-12-10T15:35:11.188331: step 3486, loss 0.575734, acc 0.953125, prec 0.0778375, recall 0.910167
2017-12-10T15:35:11.386189: step 3487, loss 0.297029, acc 0.890625, prec 0.0778473, recall 0.910194
2017-12-10T15:35:11.580092: step 3488, loss 0.276701, acc 0.921875, prec 0.077885, recall 0.910249
2017-12-10T15:35:11.775622: step 3489, loss 0.365522, acc 0.859375, prec 0.0778668, recall 0.910249
2017-12-10T15:35:11.975244: step 3490, loss 0.191025, acc 0.921875, prec 0.0778807, recall 0.910276
2017-12-10T15:35:12.175226: step 3491, loss 0.504072, acc 0.890625, prec 0.0779143, recall 0.91033
2017-12-10T15:35:12.380007: step 3492, loss 0.138872, acc 0.953125, prec 0.0779083, recall 0.91033
2017-12-10T15:35:12.577013: step 3493, loss 0.137403, acc 0.984375, prec 0.0779063, recall 0.91033
2017-12-10T15:35:12.770796: step 3494, loss 0.20496, acc 0.890625, prec 0.0778921, recall 0.91033
2017-12-10T15:35:12.966437: step 3495, loss 0.0930034, acc 0.96875, prec 0.077912, recall 0.910357
2017-12-10T15:35:13.164138: step 3496, loss 0.441268, acc 0.953125, prec 0.0779059, recall 0.910357
2017-12-10T15:35:13.362972: step 3497, loss 0.205149, acc 0.921875, prec 0.0779436, recall 0.910412
2017-12-10T15:35:13.561436: step 3498, loss 0.138371, acc 0.9375, prec 0.0779355, recall 0.910412
2017-12-10T15:35:13.761746: step 3499, loss 0.17227, acc 0.953125, prec 0.0779534, recall 0.910439
2017-12-10T15:35:13.956290: step 3500, loss 0.405398, acc 0.90625, prec 0.0779413, recall 0.910439
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_2/1512937383/checkpoints/model-3500

2017-12-10T15:35:15.371017: step 3501, loss 0.0533895, acc 1, prec 0.077989, recall 0.910493
2017-12-10T15:35:15.570057: step 3502, loss 0.355796, acc 0.875, prec 0.0779729, recall 0.910493
2017-12-10T15:35:15.770334: step 3503, loss 0.201835, acc 0.984375, prec 0.0779947, recall 0.91052
2017-12-10T15:35:15.973193: step 3504, loss 0.0782731, acc 0.984375, prec 0.0779927, recall 0.91052
2017-12-10T15:35:16.173802: step 3505, loss 0.0936208, acc 0.96875, prec 0.0779887, recall 0.91052
2017-12-10T15:35:16.372395: step 3506, loss 0.281039, acc 0.953125, prec 0.0780065, recall 0.910547
2017-12-10T15:35:16.574371: step 3507, loss 0.202717, acc 0.921875, prec 0.0780441, recall 0.910601
2017-12-10T15:35:16.776247: step 3508, loss 0.0914452, acc 0.984375, prec 0.0780898, recall 0.910655
2017-12-10T15:35:16.983709: step 3509, loss 0.666283, acc 0.984375, prec 0.0781355, recall 0.910709
2017-12-10T15:35:17.187252: step 3510, loss 1.8534, acc 0.9375, prec 0.0781533, recall 0.910461
2017-12-10T15:35:17.384865: step 3511, loss 0.186996, acc 0.9375, prec 0.0781452, recall 0.910461
2017-12-10T15:35:17.582946: step 3512, loss 0.0868137, acc 0.984375, prec 0.078167, recall 0.910488
2017-12-10T15:35:17.783650: step 3513, loss 0.440149, acc 0.90625, prec 0.0781549, recall 0.910488
2017-12-10T15:35:17.982629: step 3514, loss 0.432715, acc 0.921875, prec 0.0781925, recall 0.910542
2017-12-10T15:35:18.176599: step 3515, loss 0.162454, acc 0.9375, prec 0.0781844, recall 0.910542
2017-12-10T15:35:18.378219: step 3516, loss 0.282975, acc 0.953125, prec 0.0782737, recall 0.91065
2017-12-10T15:35:18.575458: step 3517, loss 0.291919, acc 0.953125, prec 0.0782914, recall 0.910677
2017-12-10T15:35:18.772320: step 3518, loss 0.0906985, acc 0.96875, prec 0.0782874, recall 0.910677
2017-12-10T15:35:18.968235: step 3519, loss 0.355929, acc 0.921875, prec 0.0783011, recall 0.910704
2017-12-10T15:35:19.162230: step 3520, loss 0.369324, acc 0.875, prec 0.0783087, recall 0.91073
2017-12-10T15:35:19.358034: step 3521, loss 0.123865, acc 0.96875, prec 0.0783285, recall 0.910757
2017-12-10T15:35:19.555747: step 3522, loss 0.190274, acc 0.9375, prec 0.0783204, recall 0.910757
2017-12-10T15:35:19.752494: step 3523, loss 0.373128, acc 0.9375, prec 0.0784076, recall 0.910864
2017-12-10T15:35:19.947919: step 3524, loss 0.38981, acc 0.890625, prec 0.0784648, recall 0.910945
2017-12-10T15:35:20.146526: step 3525, loss 0.0153454, acc 1, prec 0.0784886, recall 0.910971
2017-12-10T15:35:20.345237: step 3526, loss 0.167271, acc 0.953125, prec 0.0784825, recall 0.910971
2017-12-10T15:35:20.546551: step 3527, loss 0.126725, acc 0.96875, prec 0.0785023, recall 0.910998
2017-12-10T15:35:20.741366: step 3528, loss 0.200306, acc 0.9375, prec 0.0785417, recall 0.911051
2017-12-10T15:35:20.938225: step 3529, loss 0.182832, acc 0.921875, prec 0.0785316, recall 0.911051
2017-12-10T15:35:21.140383: step 3530, loss 0.174653, acc 0.90625, prec 0.0785432, recall 0.911078
2017-12-10T15:35:21.343014: step 3531, loss 0.223766, acc 0.921875, prec 0.0785806, recall 0.911131
2017-12-10T15:35:21.539689: step 3532, loss 0.206464, acc 0.953125, prec 0.0785746, recall 0.911131
2017-12-10T15:35:21.739063: step 3533, loss 0.282984, acc 0.9375, prec 0.0785902, recall 0.911158
2017-12-10T15:35:21.935003: step 3534, loss 0.0924356, acc 0.984375, prec 0.078612, recall 0.911184
2017-12-10T15:35:22.133172: step 3535, loss 0.107067, acc 0.921875, prec 0.0786018, recall 0.911184
2017-12-10T15:35:22.333022: step 3536, loss 0.199334, acc 0.953125, prec 0.0786433, recall 0.911237
2017-12-10T15:35:22.535508: step 3537, loss 0.792146, acc 0.984375, prec 0.0786888, recall 0.91129
2017-12-10T15:35:22.736952: step 3538, loss 0.149224, acc 0.96875, prec 0.0787322, recall 0.911343
2017-12-10T15:35:22.938807: step 3539, loss 0.115368, acc 0.953125, prec 0.0787499, recall 0.91137
2017-12-10T15:35:23.135849: step 3540, loss 0.0411039, acc 0.984375, prec 0.0787479, recall 0.91137
2017-12-10T15:35:23.331837: step 3541, loss 0.176066, acc 0.953125, prec 0.0787418, recall 0.91137
2017-12-10T15:35:23.532691: step 3542, loss 0.0433633, acc 0.96875, prec 0.0787377, recall 0.91137
2017-12-10T15:35:23.736482: step 3543, loss 0.229992, acc 0.921875, prec 0.0787276, recall 0.91137
2017-12-10T15:35:23.942788: step 3544, loss 0.102376, acc 0.984375, prec 0.0787968, recall 0.911449
2017-12-10T15:35:24.142909: step 3545, loss 0.163253, acc 0.953125, prec 0.0788144, recall 0.911475
2017-12-10T15:35:24.342292: step 3546, loss 0.106766, acc 0.953125, prec 0.0788083, recall 0.911475
2017-12-10T15:35:24.543064: step 3547, loss 0.849017, acc 0.953125, prec 0.078826, recall 0.911502
2017-12-10T15:35:24.744849: step 3548, loss 0.262605, acc 0.953125, prec 0.0788436, recall 0.911528
2017-12-10T15:35:24.942885: step 3549, loss 0.129955, acc 0.96875, prec 0.078887, recall 0.911581
2017-12-10T15:35:25.138367: step 3550, loss 0.0722747, acc 0.984375, prec 0.0789087, recall 0.911607
2017-12-10T15:35:25.335600: step 3551, loss 0.274271, acc 0.90625, prec 0.0789203, recall 0.911633
2017-12-10T15:35:25.532387: step 3552, loss 0.372934, acc 0.96875, prec 0.0789399, recall 0.91166
2017-12-10T15:35:25.730965: step 3553, loss 0.458039, acc 0.96875, prec 0.0789833, recall 0.911712
2017-12-10T15:35:25.933658: step 3554, loss 0.1848, acc 0.96875, prec 0.0789792, recall 0.911712
2017-12-10T15:35:26.143530: step 3555, loss 0.0629701, acc 0.984375, prec 0.0790009, recall 0.911738
2017-12-10T15:35:26.342770: step 3556, loss 0.06927, acc 0.96875, prec 0.0789968, recall 0.911738
2017-12-10T15:35:26.539100: step 3557, loss 0.189452, acc 0.890625, prec 0.0789826, recall 0.911738
2017-12-10T15:35:26.735124: step 3558, loss 0.164416, acc 0.96875, prec 0.0790497, recall 0.911817
2017-12-10T15:35:26.932321: step 3559, loss 0.0732961, acc 0.96875, prec 0.0790456, recall 0.911817
2017-12-10T15:35:27.130623: step 3560, loss 0.77611, acc 0.984375, prec 0.079091, recall 0.911869
2017-12-10T15:35:27.336142: step 3561, loss 0.338628, acc 0.921875, prec 0.0791282, recall 0.911922
2017-12-10T15:35:27.534502: step 3562, loss 0.154364, acc 0.984375, prec 0.0791498, recall 0.911948
2017-12-10T15:35:27.734480: step 3563, loss 0.079884, acc 0.96875, prec 0.0791695, recall 0.911974
2017-12-10T15:35:27.936974: step 3564, loss 0.211999, acc 0.9375, prec 0.0791613, recall 0.911974
2017-12-10T15:35:28.136242: step 3565, loss 0.130321, acc 0.9375, prec 0.0791769, recall 0.912
2017-12-10T15:35:28.333409: step 3566, loss 0.249396, acc 0.921875, prec 0.0791904, recall 0.912026
2017-12-10T15:35:28.528813: step 3567, loss 0.170009, acc 0.953125, prec 0.0791842, recall 0.912026
2017-12-10T15:35:28.726768: step 3568, loss 0.0874862, acc 0.953125, prec 0.0792018, recall 0.912052
2017-12-10T15:35:28.923209: step 3569, loss 0.128792, acc 0.9375, prec 0.079241, recall 0.912104
2017-12-10T15:35:29.118766: step 3570, loss 2.66963, acc 0.9375, prec 0.0792586, recall 0.91186
2017-12-10T15:35:29.327990: step 3571, loss 0.262407, acc 0.90625, prec 0.0792463, recall 0.91186
2017-12-10T15:35:29.527016: step 3572, loss 0.309786, acc 0.890625, prec 0.0792321, recall 0.91186
2017-12-10T15:35:29.723056: step 3573, loss 0.309985, acc 0.90625, prec 0.0792672, recall 0.911913
2017-12-10T15:35:29.916753: step 3574, loss 0.241862, acc 0.875, prec 0.0792509, recall 0.911913
2017-12-10T15:35:30.115118: step 3575, loss 0.106462, acc 0.953125, prec 0.0792448, recall 0.911913
2017-12-10T15:35:30.311963: step 3576, loss 0.20808, acc 0.9375, prec 0.079284, recall 0.911965
2017-12-10T15:35:30.510563: step 3577, loss 0.229786, acc 0.9375, prec 0.0792758, recall 0.911965
2017-12-10T15:35:30.707249: step 3578, loss 0.368383, acc 0.953125, prec 0.079317, recall 0.912017
2017-12-10T15:35:30.907998: step 3579, loss 0.289848, acc 0.921875, prec 0.0793068, recall 0.912017
2017-12-10T15:35:31.103590: step 3580, loss 0.37445, acc 0.921875, prec 0.0793203, recall 0.912042
2017-12-10T15:35:31.303555: step 3581, loss 0.386218, acc 0.875, prec 0.079304, recall 0.912042
2017-12-10T15:35:31.505807: step 3582, loss 0.132857, acc 0.9375, prec 0.0793195, recall 0.912068
2017-12-10T15:35:31.705877: step 3583, loss 1.0431, acc 0.890625, prec 0.0793761, recall 0.912146
2017-12-10T15:35:31.901643: step 3584, loss 0.617009, acc 0.859375, prec 0.0793578, recall 0.912146
2017-12-10T15:35:32.094369: step 3585, loss 0.262111, acc 0.890625, prec 0.0793907, recall 0.912198
2017-12-10T15:35:32.291324: step 3586, loss 0.218376, acc 0.953125, prec 0.0793846, recall 0.912198
2017-12-10T15:35:32.495256: step 3587, loss 0.0764266, acc 0.96875, prec 0.0793805, recall 0.912198
2017-12-10T15:35:32.691877: step 3588, loss 0.139866, acc 0.9375, prec 0.079396, recall 0.912224
2017-12-10T15:35:32.890843: step 3589, loss 0.977851, acc 0.984375, prec 0.0794648, recall 0.912301
2017-12-10T15:35:33.089510: step 3590, loss 0.270499, acc 0.90625, prec 0.0794761, recall 0.912327
2017-12-10T15:35:33.285982: step 3591, loss 0.969022, acc 0.90625, prec 0.0794875, recall 0.912353
2017-12-10T15:35:33.486611: step 3592, loss 0.0826911, acc 0.96875, prec 0.079507, recall 0.912379
2017-12-10T15:35:33.685475: step 3593, loss 0.125527, acc 0.953125, prec 0.0795009, recall 0.912379
2017-12-10T15:35:33.879404: step 3594, loss 0.33777, acc 0.875, prec 0.0795082, recall 0.912404
2017-12-10T15:35:34.076226: step 3595, loss 0.33409, acc 0.90625, prec 0.079496, recall 0.912404
2017-12-10T15:35:34.272457: step 3596, loss 0.254988, acc 0.890625, prec 0.0795053, recall 0.91243
2017-12-10T15:35:34.470673: step 3597, loss 0.294039, acc 0.90625, prec 0.0795167, recall 0.912456
2017-12-10T15:35:34.665734: step 3598, loss 0.373771, acc 0.890625, prec 0.0795495, recall 0.912507
2017-12-10T15:35:34.862336: step 3599, loss 0.385753, acc 0.890625, prec 0.0795588, recall 0.912533
2017-12-10T15:35:35.063033: step 3600, loss 0.395248, acc 0.90625, prec 0.0795466, recall 0.912533
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_2/1512937383/checkpoints/model-3600

2017-12-10T15:35:36.276760: step 3601, loss 0.097223, acc 0.953125, prec 0.0795405, recall 0.912533
2017-12-10T15:35:36.473001: step 3602, loss 0.263168, acc 0.9375, prec 0.0795559, recall 0.912559
2017-12-10T15:35:36.676031: step 3603, loss 0.239795, acc 0.9375, prec 0.0795478, recall 0.912559
2017-12-10T15:35:36.873116: step 3604, loss 0.101314, acc 0.984375, prec 0.0795457, recall 0.912559
2017-12-10T15:35:37.070898: step 3605, loss 0.122045, acc 0.953125, prec 0.0795632, recall 0.912584
2017-12-10T15:35:37.267023: step 3606, loss 0.204793, acc 0.953125, prec 0.0795571, recall 0.912584
2017-12-10T15:35:37.465534: step 3607, loss 0.194709, acc 0.953125, prec 0.079598, recall 0.912636
2017-12-10T15:35:37.660518: step 3608, loss 0.285948, acc 0.96875, prec 0.0796646, recall 0.912712
2017-12-10T15:35:37.861634: step 3609, loss 0.0912125, acc 0.96875, prec 0.0796605, recall 0.912712
2017-12-10T15:35:38.061005: step 3610, loss 0.023983, acc 1, prec 0.0796605, recall 0.912712
2017-12-10T15:35:38.263531: step 3611, loss 0.0988446, acc 0.96875, prec 0.079727, recall 0.912789
2017-12-10T15:35:38.463537: step 3612, loss 0.182333, acc 0.96875, prec 0.0797935, recall 0.912866
2017-12-10T15:35:38.665890: step 3613, loss 0.499052, acc 1, prec 0.079817, recall 0.912891
2017-12-10T15:35:38.873394: step 3614, loss 0.644881, acc 0.953125, prec 0.0798344, recall 0.912916
2017-12-10T15:35:39.072928: step 3615, loss 0.0226162, acc 0.984375, prec 0.0798324, recall 0.912916
2017-12-10T15:35:39.274273: step 3616, loss 0.0768493, acc 0.984375, prec 0.0798538, recall 0.912942
2017-12-10T15:35:39.473563: step 3617, loss 0.177519, acc 0.9375, prec 0.0798457, recall 0.912942
2017-12-10T15:35:39.671856: step 3618, loss 0.206175, acc 0.9375, prec 0.079861, recall 0.912967
2017-12-10T15:35:39.863315: step 3619, loss 0.175706, acc 0.96875, prec 0.0798569, recall 0.912967
2017-12-10T15:35:40.058053: step 3620, loss 0.0909081, acc 0.96875, prec 0.0799234, recall 0.913043
2017-12-10T15:35:40.256928: step 3621, loss 0.0348061, acc 1, prec 0.0799234, recall 0.913043
2017-12-10T15:35:40.460209: step 3622, loss 0.411619, acc 0.9375, prec 0.0799622, recall 0.913094
2017-12-10T15:35:40.658684: step 3623, loss 0.266741, acc 0.953125, prec 0.0799796, recall 0.91312
2017-12-10T15:35:40.859075: step 3624, loss 0.152162, acc 0.984375, prec 0.080001, recall 0.913145
2017-12-10T15:35:41.061375: step 3625, loss 0.133605, acc 0.953125, prec 0.0800184, recall 0.91317
2017-12-10T15:35:41.264189: step 3626, loss 0.067051, acc 0.984375, prec 0.0800633, recall 0.913221
2017-12-10T15:35:41.461442: step 3627, loss 0.119208, acc 0.953125, prec 0.0801276, recall 0.913296
2017-12-10T15:35:41.667771: step 3628, loss 0.0959777, acc 0.953125, prec 0.0801215, recall 0.913296
2017-12-10T15:35:41.868654: step 3629, loss 0.109994, acc 0.953125, prec 0.0801154, recall 0.913296
2017-12-10T15:35:42.066558: step 3630, loss 0.115249, acc 0.953125, prec 0.0801092, recall 0.913296
2017-12-10T15:35:42.262687: step 3631, loss 0.0923391, acc 0.96875, prec 0.0801051, recall 0.913296
2017-12-10T15:35:42.460340: step 3632, loss 0.181307, acc 0.953125, prec 0.0801225, recall 0.913322
2017-12-10T15:35:42.662542: step 3633, loss 0.0575658, acc 0.96875, prec 0.0801653, recall 0.913372
2017-12-10T15:35:42.860830: step 3634, loss 0.142386, acc 0.9375, prec 0.0801572, recall 0.913372
2017-12-10T15:35:43.056240: step 3635, loss 0.108198, acc 0.953125, prec 0.080151, recall 0.913372
2017-12-10T15:35:43.253533: step 3636, loss 0.403986, acc 0.9375, prec 0.0801428, recall 0.913372
2017-12-10T15:35:43.449843: step 3637, loss 0.182549, acc 0.953125, prec 0.0801602, recall 0.913397
2017-12-10T15:35:43.649776: step 3638, loss 0.0435799, acc 0.96875, prec 0.0801795, recall 0.913422
2017-12-10T15:35:43.853728: step 3639, loss 3.59628, acc 0.96875, prec 0.0802009, recall 0.913182
2017-12-10T15:35:44.061438: step 3640, loss 0.144761, acc 0.96875, prec 0.0801969, recall 0.913182
2017-12-10T15:35:44.263190: step 3641, loss 0.0664243, acc 0.96875, prec 0.0801928, recall 0.913182
2017-12-10T15:35:44.464508: step 3642, loss 0.133512, acc 0.953125, prec 0.0801866, recall 0.913182
2017-12-10T15:35:44.663475: step 3643, loss 0.102294, acc 0.96875, prec 0.080206, recall 0.913208
2017-12-10T15:35:44.861939: step 3644, loss 0.189388, acc 0.9375, prec 0.0801978, recall 0.913208
2017-12-10T15:35:45.058245: step 3645, loss 0.200436, acc 0.953125, prec 0.0801917, recall 0.913208
2017-12-10T15:35:45.254237: step 3646, loss 0.255839, acc 0.90625, prec 0.0801794, recall 0.913208
2017-12-10T15:35:45.452574: step 3647, loss 0.377835, acc 0.953125, prec 0.0802202, recall 0.913258
2017-12-10T15:35:45.650562: step 3648, loss 0.210854, acc 0.9375, prec 0.080212, recall 0.913258
2017-12-10T15:35:45.845409: step 3649, loss 0.64695, acc 0.921875, prec 0.0802486, recall 0.913308
2017-12-10T15:35:46.043690: step 3650, loss 0.0340473, acc 0.984375, prec 0.08027, recall 0.913333
2017-12-10T15:35:46.247808: step 3651, loss 0.15831, acc 0.953125, prec 0.0802639, recall 0.913333
2017-12-10T15:35:46.449269: step 3652, loss 0.430837, acc 0.859375, prec 0.0802923, recall 0.913384
2017-12-10T15:35:46.647328: step 3653, loss 0.261929, acc 0.9375, prec 0.0803076, recall 0.913409
2017-12-10T15:35:46.844671: step 3654, loss 0.10755, acc 0.953125, prec 0.0803483, recall 0.913459
2017-12-10T15:35:47.041911: step 3655, loss 0.203572, acc 0.921875, prec 0.0803381, recall 0.913459
2017-12-10T15:35:47.240265: step 3656, loss 0.171405, acc 0.96875, prec 0.080334, recall 0.913459
2017-12-10T15:35:47.435425: step 3657, loss 0.183725, acc 0.9375, prec 0.080396, recall 0.913534
2017-12-10T15:35:47.633123: step 3658, loss 0.438445, acc 0.875, prec 0.0803796, recall 0.913534
2017-12-10T15:35:47.828006: step 3659, loss 0.164897, acc 0.921875, prec 0.0803694, recall 0.913534
2017-12-10T15:35:48.026557: step 3660, loss 0.218154, acc 0.921875, prec 0.0803826, recall 0.913559
2017-12-10T15:35:48.221437: step 3661, loss 0.138079, acc 0.953125, prec 0.0803764, recall 0.913559
2017-12-10T15:35:48.417142: step 3662, loss 0.135308, acc 0.953125, prec 0.0803703, recall 0.913559
2017-12-10T15:35:48.615345: step 3663, loss 0.355402, acc 0.9375, prec 0.0803855, recall 0.913584
2017-12-10T15:35:48.816485: step 3664, loss 0.0540002, acc 0.96875, prec 0.0804048, recall 0.913609
2017-12-10T15:35:49.013515: step 3665, loss 0.20367, acc 0.96875, prec 0.0804007, recall 0.913609
2017-12-10T15:35:49.213557: step 3666, loss 0.0421753, acc 0.96875, prec 0.08042, recall 0.913634
2017-12-10T15:35:49.411006: step 3667, loss 0.0884767, acc 0.96875, prec 0.0804159, recall 0.913634
2017-12-10T15:35:49.604319: step 3668, loss 0.104003, acc 0.953125, prec 0.0804332, recall 0.913659
2017-12-10T15:35:49.803808: step 3669, loss 0.046085, acc 1, prec 0.0804566, recall 0.913684
2017-12-10T15:35:50.005445: step 3670, loss 0.929878, acc 0.9375, prec 0.0804717, recall 0.913709
2017-12-10T15:35:50.203780: step 3671, loss 0.0243394, acc 0.984375, prec 0.0804931, recall 0.913733
2017-12-10T15:35:50.403285: step 3672, loss 0.0315072, acc 0.984375, prec 0.0805144, recall 0.913758
2017-12-10T15:35:50.600874: step 3673, loss 0.0189913, acc 1, prec 0.0805144, recall 0.913758
2017-12-10T15:35:50.799451: step 3674, loss 0.0868416, acc 0.984375, prec 0.0805124, recall 0.913758
2017-12-10T15:35:50.996244: step 3675, loss 0.0340959, acc 0.984375, prec 0.0805337, recall 0.913783
2017-12-10T15:35:51.196158: step 3676, loss 0.035757, acc 0.96875, prec 0.0805296, recall 0.913783
2017-12-10T15:35:51.401529: step 3677, loss 4.30151, acc 0.90625, prec 0.0805448, recall 0.913282
2017-12-10T15:35:51.603829: step 3678, loss 0.0916861, acc 0.984375, prec 0.0805661, recall 0.913306
2017-12-10T15:35:51.805730: step 3679, loss 0.156034, acc 0.90625, prec 0.0805771, recall 0.913331
2017-12-10T15:35:52.002907: step 3680, loss 0.149526, acc 0.953125, prec 0.0806411, recall 0.913406
2017-12-10T15:35:52.198107: step 3681, loss 0.102636, acc 0.96875, prec 0.0806837, recall 0.913456
2017-12-10T15:35:52.395790: step 3682, loss 0.180815, acc 0.890625, prec 0.0806927, recall 0.913481
2017-12-10T15:35:52.591899: step 3683, loss 0.19272, acc 0.96875, prec 0.0806886, recall 0.913481
2017-12-10T15:35:52.792752: step 3684, loss 0.193243, acc 0.90625, prec 0.080723, recall 0.913531
2017-12-10T15:35:52.987889: step 3685, loss 0.300339, acc 0.90625, prec 0.0807573, recall 0.91358
2017-12-10T15:35:53.191251: step 3686, loss 0.375622, acc 0.859375, prec 0.0807389, recall 0.91358
2017-12-10T15:35:53.393319: step 3687, loss 0.400654, acc 0.890625, prec 0.0807712, recall 0.91363
2017-12-10T15:35:53.589294: step 3688, loss 0.443504, acc 0.859375, prec 0.0808227, recall 0.913704
2017-12-10T15:35:53.787192: step 3689, loss 0.43322, acc 0.90625, prec 0.0808104, recall 0.913704
2017-12-10T15:35:53.980995: step 3690, loss 0.575735, acc 0.875, prec 0.0808406, recall 0.913754
2017-12-10T15:35:54.174514: step 3691, loss 0.546998, acc 0.84375, prec 0.0808201, recall 0.913754
2017-12-10T15:35:54.376844: step 3692, loss 0.313461, acc 0.921875, prec 0.0808099, recall 0.913754
2017-12-10T15:35:54.570171: step 3693, loss 0.438754, acc 0.953125, prec 0.0808503, recall 0.913803
2017-12-10T15:35:54.773155: step 3694, loss 0.181843, acc 0.9375, prec 0.0808654, recall 0.913828
2017-12-10T15:35:54.967564: step 3695, loss 0.300797, acc 0.921875, prec 0.0808784, recall 0.913852
2017-12-10T15:35:55.162368: step 3696, loss 0.195018, acc 0.9375, prec 0.0808702, recall 0.913852
2017-12-10T15:35:55.362540: step 3697, loss 0.199166, acc 0.90625, prec 0.0809743, recall 0.913975
2017-12-10T15:35:55.560710: step 3698, loss 0.0769297, acc 0.96875, prec 0.0809702, recall 0.913975
2017-12-10T15:35:55.757730: step 3699, loss 0.728484, acc 0.953125, prec 0.0810804, recall 0.914098
2017-12-10T15:35:55.951772: step 3700, loss 0.129635, acc 0.9375, prec 0.0811187, recall 0.914147
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_2/1512937383/checkpoints/model-3700

2017-12-10T15:35:57.100423: step 3701, loss 0.187802, acc 0.984375, prec 0.0811399, recall 0.914172
2017-12-10T15:35:57.299129: step 3702, loss 0.347778, acc 0.90625, prec 0.0811276, recall 0.914172
2017-12-10T15:35:57.496630: step 3703, loss 0.268711, acc 0.890625, prec 0.0811365, recall 0.914196
2017-12-10T15:35:57.702037: step 3704, loss 0.200715, acc 0.9375, prec 0.0811515, recall 0.914221
2017-12-10T15:35:57.900387: step 3705, loss 0.1383, acc 0.96875, prec 0.0811939, recall 0.914269
2017-12-10T15:35:58.096483: step 3706, loss 0.185337, acc 0.921875, prec 0.0812069, recall 0.914294
2017-12-10T15:35:58.301470: step 3707, loss 0.159263, acc 0.953125, prec 0.0812007, recall 0.914294
2017-12-10T15:35:58.502270: step 3708, loss 0.333727, acc 0.921875, prec 0.0812137, recall 0.914318
2017-12-10T15:35:58.700388: step 3709, loss 0.197341, acc 0.96875, prec 0.081256, recall 0.914367
2017-12-10T15:35:58.897753: step 3710, loss 0.154066, acc 0.96875, prec 0.0812751, recall 0.914391
2017-12-10T15:35:59.096041: step 3711, loss 0.0834398, acc 0.96875, prec 0.0812942, recall 0.914416
2017-12-10T15:35:59.305558: step 3712, loss 0.0656905, acc 0.96875, prec 0.0812901, recall 0.914416
2017-12-10T15:35:59.504662: step 3713, loss 0.231195, acc 0.9375, prec 0.0812819, recall 0.914416
2017-12-10T15:35:59.704410: step 3714, loss 0.0173277, acc 1, prec 0.0812819, recall 0.914416
2017-12-10T15:35:59.901268: step 3715, loss 1.41864, acc 0.90625, prec 0.0812716, recall 0.914156
2017-12-10T15:36:00.104610: step 3716, loss 0.23947, acc 0.96875, prec 0.0812907, recall 0.91418
2017-12-10T15:36:00.302756: step 3717, loss 0.187024, acc 0.90625, prec 0.0813016, recall 0.914205
2017-12-10T15:36:00.502403: step 3718, loss 0.105268, acc 0.953125, prec 0.0812955, recall 0.914205
2017-12-10T15:36:00.697947: step 3719, loss 0.0985861, acc 0.953125, prec 0.0813357, recall 0.914253
2017-12-10T15:36:00.892240: step 3720, loss 0.152238, acc 0.96875, prec 0.0813316, recall 0.914253
2017-12-10T15:36:01.091653: step 3721, loss 0.314946, acc 0.9375, prec 0.0813698, recall 0.914302
2017-12-10T15:36:01.296280: step 3722, loss 0.131495, acc 0.96875, prec 0.0814353, recall 0.914375
2017-12-10T15:36:01.495804: step 3723, loss 0.324461, acc 0.953125, prec 0.0814523, recall 0.914399
2017-12-10T15:36:01.693236: step 3724, loss 0.278026, acc 0.9375, prec 0.0814905, recall 0.914448
2017-12-10T15:36:01.896472: step 3725, loss 0.190005, acc 0.921875, prec 0.0814802, recall 0.914448
2017-12-10T15:36:02.095619: step 3726, loss 0.291546, acc 0.921875, prec 0.0814931, recall 0.914472
2017-12-10T15:36:02.289613: step 3727, loss 0.182567, acc 0.953125, prec 0.0815101, recall 0.914496
2017-12-10T15:36:02.488128: step 3728, loss 0.199168, acc 0.953125, prec 0.0815503, recall 0.914544
2017-12-10T15:36:02.690764: step 3729, loss 0.477858, acc 0.9375, prec 0.0816115, recall 0.914617
2017-12-10T15:36:02.893543: step 3730, loss 0.531237, acc 0.921875, prec 0.0816476, recall 0.914665
2017-12-10T15:36:03.089314: step 3731, loss 0.0293258, acc 0.984375, prec 0.0816455, recall 0.914665
2017-12-10T15:36:03.285248: step 3732, loss 0.180907, acc 0.953125, prec 0.0816393, recall 0.914665
2017-12-10T15:36:03.486807: step 3733, loss 0.116219, acc 0.96875, prec 0.0816584, recall 0.914689
2017-12-10T15:36:03.684568: step 3734, loss 0.0551646, acc 0.984375, prec 0.0816563, recall 0.914689
2017-12-10T15:36:03.883328: step 3735, loss 0.297305, acc 0.953125, prec 0.0816965, recall 0.914737
2017-12-10T15:36:04.082838: step 3736, loss 1.7362, acc 0.96875, prec 0.0816944, recall 0.914479
2017-12-10T15:36:04.287489: step 3737, loss 0.661404, acc 0.953125, prec 0.0817345, recall 0.914527
2017-12-10T15:36:04.488301: step 3738, loss 0.0631197, acc 0.984375, prec 0.0817556, recall 0.914552
2017-12-10T15:36:04.684716: step 3739, loss 0.369865, acc 0.875, prec 0.0817623, recall 0.914576
2017-12-10T15:36:04.879003: step 3740, loss 0.501457, acc 0.875, prec 0.0817689, recall 0.9146
2017-12-10T15:36:05.079393: step 3741, loss 0.176098, acc 0.9375, prec 0.081807, recall 0.914648
2017-12-10T15:36:05.276808: step 3742, loss 0.0864433, acc 0.953125, prec 0.0818008, recall 0.914648
2017-12-10T15:36:05.478853: step 3743, loss 0.307317, acc 0.921875, prec 0.0817905, recall 0.914648
2017-12-10T15:36:05.678625: step 3744, loss 0.351235, acc 0.90625, prec 0.0817781, recall 0.914648
2017-12-10T15:36:05.876205: step 3745, loss 0.155064, acc 0.921875, prec 0.0817909, recall 0.914672
2017-12-10T15:36:06.071976: step 3746, loss 0.394301, acc 0.859375, prec 0.0817955, recall 0.914696
2017-12-10T15:36:06.265829: step 3747, loss 0.0556592, acc 1, prec 0.0818649, recall 0.914768
2017-12-10T15:36:06.465363: step 3748, loss 0.283985, acc 0.875, prec 0.0818946, recall 0.914816
2017-12-10T15:36:06.661611: step 3749, loss 0.412659, acc 0.90625, prec 0.0819053, recall 0.91484
2017-12-10T15:36:06.859873: step 3750, loss 0.437195, acc 0.90625, prec 0.081893, recall 0.91484
2017-12-10T15:36:07.061066: step 3751, loss 0.280986, acc 0.890625, prec 0.0819016, recall 0.914864
2017-12-10T15:36:07.255760: step 3752, loss 0.172227, acc 0.953125, prec 0.0819186, recall 0.914888
2017-12-10T15:36:07.455390: step 3753, loss 0.458148, acc 0.875, prec 0.0819021, recall 0.914888
2017-12-10T15:36:07.654524: step 3754, loss 0.146938, acc 0.953125, prec 0.081919, recall 0.914912
2017-12-10T15:36:07.854352: step 3755, loss 0.307685, acc 0.875, prec 0.0819025, recall 0.914912
2017-12-10T15:36:08.053254: step 3756, loss 0.132949, acc 0.96875, prec 0.0819445, recall 0.914959
2017-12-10T15:36:08.254661: step 3757, loss 0.0642953, acc 0.96875, prec 0.0819635, recall 0.914983
2017-12-10T15:36:08.450525: step 3758, loss 0.0100359, acc 1, prec 0.0820327, recall 0.915055
2017-12-10T15:36:08.645772: step 3759, loss 0.284861, acc 0.90625, prec 0.0820204, recall 0.915055
2017-12-10T15:36:08.844377: step 3760, loss 0.179855, acc 0.96875, prec 0.0820393, recall 0.915078
2017-12-10T15:36:09.047269: step 3761, loss 0.253888, acc 0.921875, prec 0.082029, recall 0.915078
2017-12-10T15:36:09.244809: step 3762, loss 0.109655, acc 0.953125, prec 0.0820228, recall 0.915078
2017-12-10T15:36:09.444974: step 3763, loss 0.561202, acc 0.96875, prec 0.0820879, recall 0.91515
2017-12-10T15:36:09.644585: step 3764, loss 0.0838291, acc 0.96875, prec 0.0820837, recall 0.91515
2017-12-10T15:36:09.846674: step 3765, loss 0.0734456, acc 0.953125, prec 0.0820776, recall 0.91515
2017-12-10T15:36:10.044395: step 3766, loss 0.0984754, acc 0.984375, prec 0.0820755, recall 0.91515
2017-12-10T15:36:10.247184: step 3767, loss 2.3946, acc 0.921875, prec 0.0821133, recall 0.914941
2017-12-10T15:36:10.449510: step 3768, loss 0.235804, acc 0.96875, prec 0.0821323, recall 0.914965
2017-12-10T15:36:10.648395: step 3769, loss 0.94306, acc 0.953125, prec 0.0821722, recall 0.915013
2017-12-10T15:36:10.849409: step 3770, loss 0.129166, acc 0.984375, prec 0.0821932, recall 0.915036
2017-12-10T15:36:11.044395: step 3771, loss 0.371972, acc 0.96875, prec 0.0822121, recall 0.91506
2017-12-10T15:36:11.247575: step 3772, loss 0.257504, acc 0.921875, prec 0.0822017, recall 0.91506
2017-12-10T15:36:11.449284: step 3773, loss 0.190553, acc 0.90625, prec 0.0821894, recall 0.91506
2017-12-10T15:36:11.643980: step 3774, loss 0.224464, acc 0.9375, prec 0.0821811, recall 0.91506
2017-12-10T15:36:11.838737: step 3775, loss 0.183175, acc 0.9375, prec 0.0821959, recall 0.915084
2017-12-10T15:36:12.035240: step 3776, loss 0.423481, acc 0.890625, prec 0.0821815, recall 0.915084
2017-12-10T15:36:12.232009: step 3777, loss 0.265051, acc 0.953125, prec 0.0821753, recall 0.915084
2017-12-10T15:36:12.434001: step 3778, loss 0.211086, acc 0.875, prec 0.0821588, recall 0.915084
2017-12-10T15:36:12.632967: step 3779, loss 0.556007, acc 0.828125, prec 0.0821361, recall 0.915084
2017-12-10T15:36:12.830385: step 3780, loss 0.194092, acc 0.90625, prec 0.0821698, recall 0.915131
2017-12-10T15:36:13.031603: step 3781, loss 0.396109, acc 0.90625, prec 0.0821574, recall 0.915131
2017-12-10T15:36:13.227678: step 3782, loss 0.304627, acc 0.953125, prec 0.0821513, recall 0.915131
2017-12-10T15:36:13.428558: step 3783, loss 0.34165, acc 0.9375, prec 0.082166, recall 0.915155
2017-12-10T15:36:13.626918: step 3784, loss 0.442687, acc 0.890625, prec 0.0821516, recall 0.915155
2017-12-10T15:36:13.828312: step 3785, loss 0.210237, acc 0.953125, prec 0.0821684, recall 0.915179
2017-12-10T15:36:14.036357: step 3786, loss 0.12227, acc 0.96875, prec 0.0821643, recall 0.915179
2017-12-10T15:36:14.238070: step 3787, loss 0.211271, acc 0.9375, prec 0.0821791, recall 0.915202
2017-12-10T15:36:14.445058: step 3788, loss 0.209674, acc 0.9375, prec 0.0821938, recall 0.915226
2017-12-10T15:36:14.642961: step 3789, loss 0.0116236, acc 1, prec 0.0822398, recall 0.915273
2017-12-10T15:36:14.842655: step 3790, loss 0.206524, acc 0.875, prec 0.0822233, recall 0.915273
2017-12-10T15:36:15.041805: step 3791, loss 0.916542, acc 0.984375, prec 0.0822672, recall 0.91532
2017-12-10T15:36:15.241705: step 3792, loss 0.0385722, acc 0.984375, prec 0.0822881, recall 0.915344
2017-12-10T15:36:15.443700: step 3793, loss 0.109382, acc 0.96875, prec 0.082307, recall 0.915367
2017-12-10T15:36:15.645322: step 3794, loss 0.320299, acc 0.90625, prec 0.0823176, recall 0.915391
2017-12-10T15:36:15.840795: step 3795, loss 0.221445, acc 0.921875, prec 0.0823073, recall 0.915391
2017-12-10T15:36:16.034356: step 3796, loss 0.315029, acc 0.890625, prec 0.0822929, recall 0.915391
2017-12-10T15:36:16.231599: step 3797, loss 0.127733, acc 0.953125, prec 0.0823326, recall 0.915438
2017-12-10T15:36:16.429515: step 3798, loss 0.306304, acc 0.9375, prec 0.0823244, recall 0.915438
2017-12-10T15:36:16.624859: step 3799, loss 0.085332, acc 0.953125, prec 0.0823182, recall 0.915438
2017-12-10T15:36:16.825690: step 3800, loss 0.142375, acc 0.953125, prec 0.0823579, recall 0.915485
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_2/1512937383/checkpoints/model-3800

2017-12-10T15:36:18.023572: step 3801, loss 0.0862024, acc 0.96875, prec 0.0823538, recall 0.915485
2017-12-10T15:36:18.223689: step 3802, loss 0.103908, acc 0.953125, prec 0.0823476, recall 0.915485
2017-12-10T15:36:18.428948: step 3803, loss 0.0266951, acc 1, prec 0.0823476, recall 0.915485
2017-12-10T15:36:18.626318: step 3804, loss 0.0691145, acc 0.96875, prec 0.0823435, recall 0.915485
2017-12-10T15:36:18.819136: step 3805, loss 0.0659801, acc 0.953125, prec 0.0823373, recall 0.915485
2017-12-10T15:36:19.018823: step 3806, loss 0.00684457, acc 1, prec 0.0823373, recall 0.915485
2017-12-10T15:36:19.220593: step 3807, loss 0.0848537, acc 1, prec 0.0823603, recall 0.915509
2017-12-10T15:36:19.418503: step 3808, loss 0.0764028, acc 0.984375, prec 0.0823812, recall 0.915532
2017-12-10T15:36:19.617664: step 3809, loss 2.32173, acc 0.953125, prec 0.0824, recall 0.915301
2017-12-10T15:36:19.822523: step 3810, loss 0.0988366, acc 0.96875, prec 0.0823959, recall 0.915301
2017-12-10T15:36:20.024795: step 3811, loss 0.189486, acc 0.953125, prec 0.0824126, recall 0.915325
2017-12-10T15:36:20.221766: step 3812, loss 0.19294, acc 0.921875, prec 0.0824253, recall 0.915348
2017-12-10T15:36:20.420245: step 3813, loss 0.168286, acc 0.9375, prec 0.082417, recall 0.915348
2017-12-10T15:36:20.617809: step 3814, loss 0.0646249, acc 0.984375, prec 0.082415, recall 0.915348
2017-12-10T15:36:20.815292: step 3815, loss 0.049039, acc 0.984375, prec 0.0824358, recall 0.915372
2017-12-10T15:36:21.019864: step 3816, loss 0.219769, acc 0.9375, prec 0.0824276, recall 0.915372
2017-12-10T15:36:21.214402: step 3817, loss 0.16805, acc 0.921875, prec 0.0824173, recall 0.915372
2017-12-10T15:36:21.410146: step 3818, loss 0.0701601, acc 0.96875, prec 0.0824132, recall 0.915372
2017-12-10T15:36:21.606599: step 3819, loss 0.154014, acc 0.9375, prec 0.0824508, recall 0.915419
2017-12-10T15:36:21.805991: step 3820, loss 0.594089, acc 0.9375, prec 0.0824655, recall 0.915442
2017-12-10T15:36:22.009383: step 3821, loss 0.122341, acc 0.96875, prec 0.0825072, recall 0.915489
2017-12-10T15:36:22.206177: step 3822, loss 0.359196, acc 0.9375, prec 0.0825677, recall 0.915559
2017-12-10T15:36:22.406005: step 3823, loss 0.1005, acc 0.96875, prec 0.0825864, recall 0.915583
2017-12-10T15:36:22.608219: step 3824, loss 0.0820456, acc 0.953125, prec 0.0825803, recall 0.915583
2017-12-10T15:36:22.810107: step 3825, loss 0.102692, acc 0.953125, prec 0.082597, recall 0.915606
2017-12-10T15:36:23.008680: step 3826, loss 0.147219, acc 0.9375, prec 0.0825887, recall 0.915606
2017-12-10T15:36:23.205010: step 3827, loss 0.406876, acc 0.859375, prec 0.0825702, recall 0.915606
2017-12-10T15:36:23.399464: step 3828, loss 0.27066, acc 0.90625, prec 0.0825807, recall 0.915629
2017-12-10T15:36:23.596059: step 3829, loss 0.336583, acc 0.921875, prec 0.0825704, recall 0.915629
2017-12-10T15:36:23.796694: step 3830, loss 0.223754, acc 0.921875, prec 0.0825601, recall 0.915629
2017-12-10T15:36:23.993224: step 3831, loss 0.133866, acc 0.921875, prec 0.0825498, recall 0.915629
2017-12-10T15:36:24.193993: step 3832, loss 0.0549265, acc 0.984375, prec 0.0825478, recall 0.915629
2017-12-10T15:36:24.392357: step 3833, loss 0.101094, acc 0.984375, prec 0.0825457, recall 0.915629
2017-12-10T15:36:24.593202: step 3834, loss 0.0765274, acc 0.984375, prec 0.0825436, recall 0.915629
2017-12-10T15:36:24.796392: step 3835, loss 0.163629, acc 0.96875, prec 0.0825624, recall 0.915653
2017-12-10T15:36:24.993891: step 3836, loss 0.17558, acc 0.953125, prec 0.082602, recall 0.915699
2017-12-10T15:36:25.190413: step 3837, loss 0.0538178, acc 0.984375, prec 0.0825999, recall 0.915699
2017-12-10T15:36:25.392922: step 3838, loss 0.0986739, acc 0.96875, prec 0.0825958, recall 0.915699
2017-12-10T15:36:25.592872: step 3839, loss 0.822721, acc 0.984375, prec 0.0826395, recall 0.915746
2017-12-10T15:36:25.790362: step 3840, loss 0.0843568, acc 0.96875, prec 0.0826811, recall 0.915792
2017-12-10T15:36:25.992307: step 3841, loss 0.131952, acc 0.984375, prec 0.082679, recall 0.915792
2017-12-10T15:36:26.193832: step 3842, loss 2.23904, acc 0.90625, prec 0.0826687, recall 0.91554
2017-12-10T15:36:26.396894: step 3843, loss 0.0262046, acc 0.984375, prec 0.0826895, recall 0.915563
2017-12-10T15:36:26.594781: step 3844, loss 0.0421996, acc 0.984375, prec 0.0826875, recall 0.915563
2017-12-10T15:36:26.798205: step 3845, loss 0.14806, acc 0.953125, prec 0.0826813, recall 0.915563
2017-12-10T15:36:26.994992: step 3846, loss 0.273985, acc 1, prec 0.082727, recall 0.915609
2017-12-10T15:36:27.191368: step 3847, loss 0.464896, acc 0.984375, prec 0.0827935, recall 0.915679
2017-12-10T15:36:27.390604: step 3848, loss 0.0728084, acc 0.984375, prec 0.0828143, recall 0.915702
2017-12-10T15:36:27.588854: step 3849, loss 0.041981, acc 1, prec 0.0828143, recall 0.915702
2017-12-10T15:36:27.787108: step 3850, loss 0.19134, acc 0.9375, prec 0.082806, recall 0.915702
2017-12-10T15:36:27.985792: step 3851, loss 0.251192, acc 0.90625, prec 0.0827937, recall 0.915702
2017-12-10T15:36:28.180420: step 3852, loss 0.176491, acc 0.9375, prec 0.0828539, recall 0.915772
2017-12-10T15:36:28.376881: step 3853, loss 0.154231, acc 0.96875, prec 0.0828727, recall 0.915795
2017-12-10T15:36:28.580616: step 3854, loss 0.480018, acc 0.859375, prec 0.0828541, recall 0.915795
2017-12-10T15:36:28.773257: step 3855, loss 0.551653, acc 0.875, prec 0.0828604, recall 0.915818
2017-12-10T15:36:28.970909: step 3856, loss 0.70288, acc 0.8125, prec 0.0828585, recall 0.915842
2017-12-10T15:36:29.167967: step 3857, loss 0.408565, acc 0.859375, prec 0.0828399, recall 0.915842
2017-12-10T15:36:29.372175: step 3858, loss 0.33772, acc 0.890625, prec 0.0828711, recall 0.915888
2017-12-10T15:36:29.570527: step 3859, loss 0.326451, acc 0.875, prec 0.0829003, recall 0.915934
2017-12-10T15:36:29.769483: step 3860, loss 0.2008, acc 0.921875, prec 0.0829128, recall 0.915957
2017-12-10T15:36:29.966009: step 3861, loss 0.518239, acc 0.890625, prec 0.0829211, recall 0.91598
2017-12-10T15:36:30.165423: step 3862, loss 0.18334, acc 0.953125, prec 0.0829377, recall 0.916003
2017-12-10T15:36:30.369783: step 3863, loss 0.243059, acc 0.90625, prec 0.0829254, recall 0.916003
2017-12-10T15:36:30.565776: step 3864, loss 0.225907, acc 0.9375, prec 0.0829627, recall 0.916049
2017-12-10T15:36:30.764747: step 3865, loss 0.295536, acc 0.90625, prec 0.0829503, recall 0.916049
2017-12-10T15:36:30.964298: step 3866, loss 0.194606, acc 0.921875, prec 0.0830084, recall 0.916118
2017-12-10T15:36:31.168661: step 3867, loss 0.122273, acc 0.984375, prec 0.0830519, recall 0.916164
2017-12-10T15:36:31.377056: step 3868, loss 0.110349, acc 0.921875, prec 0.0830415, recall 0.916164
2017-12-10T15:36:31.581531: step 3869, loss 0.385056, acc 0.921875, prec 0.083054, recall 0.916187
2017-12-10T15:36:31.782704: step 3870, loss 0.227803, acc 0.921875, prec 0.0830665, recall 0.91621
2017-12-10T15:36:31.976020: step 3871, loss 0.141964, acc 0.984375, prec 0.0831099, recall 0.916256
2017-12-10T15:36:32.175076: step 3872, loss 0.195978, acc 0.953125, prec 0.0831265, recall 0.916279
2017-12-10T15:36:32.377013: step 3873, loss 0.174886, acc 0.96875, prec 0.0831224, recall 0.916279
2017-12-10T15:36:32.572411: step 3874, loss 0.142192, acc 1, prec 0.0831451, recall 0.916302
2017-12-10T15:36:32.772622: step 3875, loss 0.434413, acc 0.984375, prec 0.0832341, recall 0.916393
2017-12-10T15:36:32.973015: step 3876, loss 1.12442, acc 0.953125, prec 0.0832961, recall 0.916462
2017-12-10T15:36:33.170565: step 3877, loss 0.127678, acc 0.9375, prec 0.0833106, recall 0.916485
2017-12-10T15:36:33.369373: step 3878, loss 0.865503, acc 0.953125, prec 0.0833499, recall 0.91653
2017-12-10T15:36:33.574927: step 3879, loss 0.113298, acc 0.9375, prec 0.0833871, recall 0.916576
2017-12-10T15:36:33.775855: step 3880, loss 0.109103, acc 0.953125, prec 0.0834036, recall 0.916598
2017-12-10T15:36:33.977804: step 3881, loss 0.165192, acc 0.9375, prec 0.0834408, recall 0.916644
2017-12-10T15:36:34.176314: step 3882, loss 0.267162, acc 0.953125, prec 0.08348, recall 0.916689
2017-12-10T15:36:34.375285: step 3883, loss 0.350662, acc 0.90625, prec 0.0834676, recall 0.916689
2017-12-10T15:36:34.576007: step 3884, loss 0.212103, acc 0.96875, prec 0.0835089, recall 0.916735
2017-12-10T15:36:34.774626: step 3885, loss 0.199716, acc 0.921875, prec 0.0834986, recall 0.916735
2017-12-10T15:36:34.974539: step 3886, loss 0.437979, acc 0.875, prec 0.0835047, recall 0.916757
2017-12-10T15:36:35.166562: step 3887, loss 0.177052, acc 0.9375, prec 0.0834964, recall 0.916757
2017-12-10T15:36:35.363834: step 3888, loss 0.279341, acc 0.953125, prec 0.0835129, recall 0.91678
2017-12-10T15:36:35.561904: step 3889, loss 0.25598, acc 0.9375, prec 0.0835274, recall 0.916803
2017-12-10T15:36:35.759513: step 3890, loss 0.0755917, acc 0.96875, prec 0.0835686, recall 0.916848
2017-12-10T15:36:35.953817: step 3891, loss 0.45619, acc 0.90625, prec 0.0835789, recall 0.91687
2017-12-10T15:36:36.149921: step 3892, loss 0.257534, acc 0.921875, prec 0.0836139, recall 0.916916
2017-12-10T15:36:36.346968: step 3893, loss 0.134693, acc 0.953125, prec 0.0836304, recall 0.916938
2017-12-10T15:36:36.548367: step 3894, loss 0.26561, acc 0.90625, prec 0.0836407, recall 0.916961
2017-12-10T15:36:36.742683: step 3895, loss 2.1988, acc 0.90625, prec 0.083653, recall 0.916734
2017-12-10T15:36:36.939534: step 3896, loss 0.195852, acc 0.984375, prec 0.0836736, recall 0.916757
2017-12-10T15:36:37.136573: step 3897, loss 0.159665, acc 0.921875, prec 0.0836859, recall 0.91678
2017-12-10T15:36:37.338013: step 3898, loss 0.299162, acc 0.921875, prec 0.0836756, recall 0.91678
2017-12-10T15:36:37.537520: step 3899, loss 0.0898178, acc 0.96875, prec 0.0836715, recall 0.91678
2017-12-10T15:36:37.737545: step 3900, loss 0.0978046, acc 0.953125, prec 0.0836879, recall 0.916802
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_2/1512937383/checkpoints/model-3900

2017-12-10T15:36:38.945720: step 3901, loss 0.139595, acc 0.9375, prec 0.0836796, recall 0.916802
2017-12-10T15:36:39.144387: step 3902, loss 0.150615, acc 0.96875, prec 0.0836982, recall 0.916825
2017-12-10T15:36:39.342904: step 3903, loss 0.243658, acc 0.90625, prec 0.0836857, recall 0.916825
2017-12-10T15:36:39.543605: step 3904, loss 0.222353, acc 0.9375, prec 0.0837228, recall 0.91687
2017-12-10T15:36:39.744876: step 3905, loss 0.273541, acc 0.921875, prec 0.0837124, recall 0.91687
2017-12-10T15:36:39.937668: step 3906, loss 0.141965, acc 0.9375, prec 0.0837041, recall 0.91687
2017-12-10T15:36:40.140945: step 3907, loss 0.0996559, acc 0.96875, prec 0.0837227, recall 0.916892
2017-12-10T15:36:40.340173: step 3908, loss 0.214585, acc 0.90625, prec 0.0837102, recall 0.916892
2017-12-10T15:36:40.535493: step 3909, loss 0.0771242, acc 0.984375, prec 0.0837308, recall 0.916915
2017-12-10T15:36:40.733041: step 3910, loss 0.32611, acc 0.921875, prec 0.0837884, recall 0.916982
2017-12-10T15:36:40.931311: step 3911, loss 1.008, acc 0.96875, prec 0.0838069, recall 0.917005
2017-12-10T15:36:41.133117: step 3912, loss 0.566998, acc 0.96875, prec 0.0838254, recall 0.917027
2017-12-10T15:36:41.336191: step 3913, loss 0.0839216, acc 0.953125, prec 0.0838418, recall 0.917049
2017-12-10T15:36:41.534391: step 3914, loss 0.273109, acc 0.953125, prec 0.0838582, recall 0.917072
2017-12-10T15:36:41.734156: step 3915, loss 0.125309, acc 0.9375, prec 0.0838499, recall 0.917072
2017-12-10T15:36:41.931374: step 3916, loss 0.219077, acc 0.90625, prec 0.0838375, recall 0.917072
2017-12-10T15:36:42.130391: step 3917, loss 0.247044, acc 0.90625, prec 0.0838251, recall 0.917072
2017-12-10T15:36:42.328262: step 3918, loss 0.289071, acc 0.890625, prec 0.0838332, recall 0.917094
2017-12-10T15:36:42.524226: step 3919, loss 0.161541, acc 0.921875, prec 0.0838455, recall 0.917117
2017-12-10T15:36:42.718707: step 3920, loss 0.294951, acc 0.921875, prec 0.0838578, recall 0.917139
2017-12-10T15:36:42.920592: step 3921, loss 0.153913, acc 0.9375, prec 0.0838947, recall 0.917184
2017-12-10T15:36:43.123530: step 3922, loss 0.411537, acc 0.90625, prec 0.0839049, recall 0.917206
2017-12-10T15:36:43.322369: step 3923, loss 0.23165, acc 0.96875, prec 0.0839233, recall 0.917228
2017-12-10T15:36:43.522103: step 3924, loss 0.182167, acc 0.96875, prec 0.0839192, recall 0.917228
2017-12-10T15:36:43.723403: step 3925, loss 0.0594082, acc 0.96875, prec 0.0839602, recall 0.917273
2017-12-10T15:36:43.920632: step 3926, loss 0.145406, acc 0.953125, prec 0.083954, recall 0.917273
2017-12-10T15:36:44.116622: step 3927, loss 0.100229, acc 0.9375, prec 0.0839683, recall 0.917295
2017-12-10T15:36:44.321563: step 3928, loss 0.210522, acc 0.90625, prec 0.0840011, recall 0.91734
2017-12-10T15:36:44.518512: step 3929, loss 0.56104, acc 0.890625, prec 0.0839866, recall 0.91734
2017-12-10T15:36:44.714396: step 3930, loss 0.053424, acc 0.96875, prec 0.084005, recall 0.917362
2017-12-10T15:36:44.911820: step 3931, loss 0.243217, acc 0.921875, prec 0.0839947, recall 0.917362
2017-12-10T15:36:45.106768: step 3932, loss 0.149755, acc 0.90625, prec 0.0840048, recall 0.917384
2017-12-10T15:36:45.302546: step 3933, loss 3.07609, acc 0.96875, prec 0.0840028, recall 0.917138
2017-12-10T15:36:45.506522: step 3934, loss 0.140366, acc 0.953125, prec 0.0840191, recall 0.91716
2017-12-10T15:36:45.708158: step 3935, loss 0.257666, acc 0.9375, prec 0.084056, recall 0.917204
2017-12-10T15:36:45.907990: step 3936, loss 0.36922, acc 0.9375, prec 0.0840928, recall 0.917249
2017-12-10T15:36:46.105981: step 3937, loss 0.349109, acc 0.890625, prec 0.0841009, recall 0.917271
2017-12-10T15:36:46.307252: step 3938, loss 0.727095, acc 0.921875, prec 0.0841131, recall 0.917293
2017-12-10T15:36:46.510569: step 3939, loss 0.447387, acc 0.859375, prec 0.084117, recall 0.917315
2017-12-10T15:36:46.703200: step 3940, loss 0.406222, acc 0.859375, prec 0.0840983, recall 0.917315
2017-12-10T15:36:46.902469: step 3941, loss 0.424542, acc 0.8125, prec 0.0840735, recall 0.917315
2017-12-10T15:36:47.103065: step 3942, loss 0.226299, acc 0.921875, prec 0.0840632, recall 0.917315
2017-12-10T15:36:47.296950: step 3943, loss 0.420767, acc 0.875, prec 0.0840692, recall 0.917338
2017-12-10T15:36:47.496136: step 3944, loss 0.245104, acc 0.953125, prec 0.084108, recall 0.917382
2017-12-10T15:36:47.693414: step 3945, loss 0.369323, acc 0.90625, prec 0.0841181, recall 0.917404
2017-12-10T15:36:47.888602: step 3946, loss 0.607522, acc 0.796875, prec 0.0841363, recall 0.917448
2017-12-10T15:36:48.085746: step 3947, loss 0.259874, acc 0.875, prec 0.0841422, recall 0.917471
2017-12-10T15:36:48.280080: step 3948, loss 0.109938, acc 0.953125, prec 0.084136, recall 0.917471
2017-12-10T15:36:48.475833: step 3949, loss 0.304579, acc 0.875, prec 0.0841195, recall 0.917471
2017-12-10T15:36:48.667962: step 3950, loss 0.510025, acc 0.875, prec 0.084103, recall 0.917471
2017-12-10T15:36:48.866906: step 3951, loss 0.937573, acc 0.890625, prec 0.084156, recall 0.917537
2017-12-10T15:36:49.064625: step 3952, loss 0.0649724, acc 0.96875, prec 0.0841518, recall 0.917537
2017-12-10T15:36:49.263198: step 3953, loss 0.306977, acc 0.90625, prec 0.0841395, recall 0.917537
2017-12-10T15:36:49.460425: step 3954, loss 0.652853, acc 0.84375, prec 0.0842087, recall 0.917625
2017-12-10T15:36:49.657377: step 3955, loss 0.0648257, acc 0.96875, prec 0.0842046, recall 0.917625
2017-12-10T15:36:49.853406: step 3956, loss 0.346524, acc 0.921875, prec 0.0841943, recall 0.917625
2017-12-10T15:36:50.050452: step 3957, loss 0.548896, acc 0.96875, prec 0.0842575, recall 0.917691
2017-12-10T15:36:50.252140: step 3958, loss 0.145062, acc 0.96875, prec 0.0842759, recall 0.917713
2017-12-10T15:36:50.449086: step 3959, loss 0.0992934, acc 0.953125, prec 0.0842697, recall 0.917713
2017-12-10T15:36:50.654120: step 3960, loss 0.645593, acc 0.984375, prec 0.0843125, recall 0.917757
2017-12-10T15:36:50.858285: step 3961, loss 0.206363, acc 0.96875, prec 0.0843084, recall 0.917757
2017-12-10T15:36:51.059667: step 3962, loss 0.534937, acc 0.953125, prec 0.0843246, recall 0.917779
2017-12-10T15:36:51.256812: step 3963, loss 0.124087, acc 0.921875, prec 0.0843143, recall 0.917779
2017-12-10T15:36:51.455977: step 3964, loss 0.221181, acc 0.90625, prec 0.0843244, recall 0.917801
2017-12-10T15:36:51.659333: step 3965, loss 0.530977, acc 0.953125, prec 0.0843855, recall 0.917867
2017-12-10T15:36:51.871280: step 3966, loss 1.61888, acc 0.859375, prec 0.084369, recall 0.917622
2017-12-10T15:36:52.076020: step 3967, loss 0.20674, acc 0.90625, prec 0.0844014, recall 0.917666
2017-12-10T15:36:52.277584: step 3968, loss 0.188948, acc 0.953125, prec 0.0843952, recall 0.917666
2017-12-10T15:36:52.476104: step 3969, loss 1.67928, acc 0.9375, prec 0.084389, recall 0.917421
2017-12-10T15:36:52.678675: step 3970, loss 0.193657, acc 0.890625, prec 0.0843745, recall 0.917421
2017-12-10T15:36:52.877012: step 3971, loss 0.385379, acc 0.875, prec 0.0844029, recall 0.917465
2017-12-10T15:36:53.073871: step 3972, loss 0.100379, acc 0.953125, prec 0.0844415, recall 0.917509
2017-12-10T15:36:53.278094: step 3973, loss 0.33259, acc 0.890625, prec 0.0844494, recall 0.917531
2017-12-10T15:36:53.476867: step 3974, loss 0.387497, acc 0.875, prec 0.0844329, recall 0.917531
2017-12-10T15:36:53.675615: step 3975, loss 0.0812866, acc 0.984375, prec 0.0844533, recall 0.917553
2017-12-10T15:36:53.862246: step 3976, loss 0.355088, acc 0.923077, prec 0.084445, recall 0.917553
2017-12-10T15:36:54.066491: step 3977, loss 0.29801, acc 0.875, prec 0.0844957, recall 0.917619
2017-12-10T15:36:54.263448: step 3978, loss 0.253962, acc 0.9375, prec 0.0845098, recall 0.917641
2017-12-10T15:36:54.465989: step 3979, loss 0.323139, acc 0.890625, prec 0.0845177, recall 0.917663
2017-12-10T15:36:54.663065: step 3980, loss 0.137237, acc 0.953125, prec 0.0845115, recall 0.917663
2017-12-10T15:36:54.870256: step 3981, loss 0.17103, acc 0.96875, prec 0.0845298, recall 0.917685
2017-12-10T15:36:55.072156: step 3982, loss 1.69383, acc 0.9375, prec 0.084546, recall 0.917463
2017-12-10T15:36:55.269435: step 3983, loss 0.370007, acc 0.890625, prec 0.0845539, recall 0.917485
2017-12-10T15:36:55.468431: step 3984, loss 0.274723, acc 0.9375, prec 0.084568, recall 0.917507
2017-12-10T15:36:55.663534: step 3985, loss 0.266609, acc 0.9375, prec 0.0846716, recall 0.917616
2017-12-10T15:36:55.863426: step 3986, loss 0.462135, acc 0.859375, prec 0.084653, recall 0.917616
2017-12-10T15:36:56.060411: step 3987, loss 0.451394, acc 0.90625, prec 0.0846629, recall 0.917638
2017-12-10T15:36:56.258726: step 3988, loss 0.549769, acc 0.859375, prec 0.0846667, recall 0.91766
2017-12-10T15:36:56.462020: step 3989, loss 0.303933, acc 0.90625, prec 0.0846543, recall 0.91766
2017-12-10T15:36:56.665157: step 3990, loss 0.223959, acc 0.921875, prec 0.084711, recall 0.917725
2017-12-10T15:36:56.861375: step 3991, loss 0.346089, acc 0.875, prec 0.0847168, recall 0.917747
2017-12-10T15:36:57.057728: step 3992, loss 0.357095, acc 0.9375, prec 0.0847532, recall 0.91779
2017-12-10T15:36:57.256336: step 3993, loss 0.113587, acc 0.96875, prec 0.0847714, recall 0.917812
2017-12-10T15:36:57.456971: step 3994, loss 0.385417, acc 0.90625, prec 0.084759, recall 0.917812
2017-12-10T15:36:57.656153: step 3995, loss 0.146529, acc 0.9375, prec 0.0847731, recall 0.917834
2017-12-10T15:36:57.855356: step 3996, loss 0.364311, acc 0.9375, prec 0.0847871, recall 0.917855
2017-12-10T15:36:58.049856: step 3997, loss 0.266687, acc 0.90625, prec 0.084797, recall 0.917877
2017-12-10T15:36:58.244870: step 3998, loss 0.10601, acc 0.96875, prec 0.0847929, recall 0.917877
2017-12-10T15:36:58.443738: step 3999, loss 0.198549, acc 0.953125, prec 0.084809, recall 0.917899
2017-12-10T15:36:58.640487: step 4000, loss 0.0925022, acc 0.984375, prec 0.0848069, recall 0.917899

Evaluation:
2017-12-10T15:37:03.206475: step 4000, loss 3.14085, acc 0.947443, prec 0.0851715, recall 0.898628

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_2/1512937383/checkpoints/model-4000

2017-12-10T15:37:04.533467: step 4001, loss 0.697415, acc 0.953125, prec 0.0852094, recall 0.89868
2017-12-10T15:37:04.734814: step 4002, loss 0.141017, acc 0.96875, prec 0.0852273, recall 0.898705
2017-12-10T15:37:04.932721: step 4003, loss 0.126031, acc 0.953125, prec 0.0852652, recall 0.898757
2017-12-10T15:37:05.131048: step 4004, loss 0.155468, acc 0.953125, prec 0.085281, recall 0.898782
2017-12-10T15:37:05.329068: step 4005, loss 0.847509, acc 0.9375, prec 0.0853168, recall 0.898834
2017-12-10T15:37:05.531022: step 4006, loss 0.128903, acc 0.984375, prec 0.0853368, recall 0.898859
2017-12-10T15:37:05.728947: step 4007, loss 0.172025, acc 0.953125, prec 0.0853527, recall 0.898885
2017-12-10T15:37:05.931043: step 4008, loss 0.177295, acc 0.96875, prec 0.0853485, recall 0.898885
2017-12-10T15:37:06.134492: step 4009, loss 0.87433, acc 0.984375, prec 0.0854125, recall 0.898962
2017-12-10T15:37:06.334941: step 4010, loss 0.0382163, acc 0.96875, prec 0.0854084, recall 0.898962
2017-12-10T15:37:06.538248: step 4011, loss 0.154393, acc 0.921875, prec 0.0854201, recall 0.898987
2017-12-10T15:37:06.740724: step 4012, loss 0.0883184, acc 0.953125, prec 0.085414, recall 0.898987
2017-12-10T15:37:06.941181: step 4013, loss 0.146984, acc 0.953125, prec 0.0854738, recall 0.899064
2017-12-10T15:37:07.140059: step 4014, loss 0.140031, acc 0.921875, prec 0.0854855, recall 0.89909
2017-12-10T15:37:07.335458: step 4015, loss 0.110873, acc 0.96875, prec 0.0854814, recall 0.89909
2017-12-10T15:37:07.529935: step 4016, loss 0.0111475, acc 1, prec 0.0854814, recall 0.89909
2017-12-10T15:37:07.727192: step 4017, loss 0.207399, acc 0.96875, prec 0.0855432, recall 0.899166
2017-12-10T15:37:07.921968: step 4018, loss 0.0927037, acc 0.984375, prec 0.0855632, recall 0.899191
2017-12-10T15:37:08.124999: step 4019, loss 0.164123, acc 0.9375, prec 0.0855549, recall 0.899191
2017-12-10T15:37:08.322452: step 4020, loss 0.306716, acc 0.90625, prec 0.0855426, recall 0.899191
2017-12-10T15:37:08.519872: step 4021, loss 0.291706, acc 0.96875, prec 0.0855605, recall 0.899217
2017-12-10T15:37:08.716735: step 4022, loss 0.0866953, acc 0.96875, prec 0.0855783, recall 0.899242
2017-12-10T15:37:08.915250: step 4023, loss 0.229474, acc 0.984375, prec 0.0856202, recall 0.899293
2017-12-10T15:37:09.112131: step 4024, loss 0.047816, acc 0.984375, prec 0.085706, recall 0.899395
2017-12-10T15:37:09.310688: step 4025, loss 0.0500068, acc 0.984375, prec 0.085726, recall 0.89942
2017-12-10T15:37:09.506785: step 4026, loss 0.108868, acc 0.96875, prec 0.0857218, recall 0.89942
2017-12-10T15:37:09.707588: step 4027, loss 0.202489, acc 0.921875, prec 0.0857335, recall 0.899446
2017-12-10T15:37:09.905523: step 4028, loss 0.0700995, acc 0.984375, prec 0.0857314, recall 0.899446
2017-12-10T15:37:10.105139: step 4029, loss 0.196698, acc 0.921875, prec 0.085787, recall 0.899522
2017-12-10T15:37:10.300941: step 4030, loss 0.0984956, acc 0.9375, prec 0.0858007, recall 0.899547
2017-12-10T15:37:10.499828: step 4031, loss 0.0661614, acc 0.96875, prec 0.0857966, recall 0.899547
2017-12-10T15:37:10.699682: step 4032, loss 0.109799, acc 0.96875, prec 0.0858144, recall 0.899572
2017-12-10T15:37:10.895128: step 4033, loss 0.0229343, acc 0.984375, prec 0.0858124, recall 0.899572
2017-12-10T15:37:11.093530: step 4034, loss 0.461104, acc 0.96875, prec 0.0858522, recall 0.899623
2017-12-10T15:37:11.295931: step 4035, loss 0.642195, acc 1, prec 0.0858961, recall 0.899673
2017-12-10T15:37:11.499490: step 4036, loss 0.288319, acc 0.953125, prec 0.0859118, recall 0.899698
2017-12-10T15:37:11.698665: step 4037, loss 0.0201396, acc 0.984375, prec 0.0859097, recall 0.899698
2017-12-10T15:37:11.894749: step 4038, loss 0.00623332, acc 1, prec 0.0859097, recall 0.899698
2017-12-10T15:37:12.094225: step 4039, loss 0.0695648, acc 0.984375, prec 0.0859296, recall 0.899724
2017-12-10T15:37:12.294478: step 4040, loss 0.19183, acc 0.9375, prec 0.0859214, recall 0.899724
2017-12-10T15:37:12.494095: step 4041, loss 0.0478007, acc 0.96875, prec 0.0859172, recall 0.899724
2017-12-10T15:37:12.696103: step 4042, loss 0.288354, acc 0.9375, prec 0.0859529, recall 0.899774
2017-12-10T15:37:12.893732: step 4043, loss 0.0188742, acc 1, prec 0.0859529, recall 0.899774
2017-12-10T15:37:13.089615: step 4044, loss 0.12759, acc 0.96875, prec 0.0859488, recall 0.899774
2017-12-10T15:37:13.286385: step 4045, loss 0.106729, acc 0.953125, prec 0.0860083, recall 0.899849
2017-12-10T15:37:13.488926: step 4046, loss 0.505634, acc 0.953125, prec 0.0860241, recall 0.899875
2017-12-10T15:37:13.689815: step 4047, loss 0.0856902, acc 0.96875, prec 0.08602, recall 0.899875
2017-12-10T15:37:13.892632: step 4048, loss 0.0313873, acc 0.984375, prec 0.0860179, recall 0.899875
2017-12-10T15:37:14.091845: step 4049, loss 0.0422861, acc 0.984375, prec 0.0860158, recall 0.899875
2017-12-10T15:37:14.291216: step 4050, loss 0.805266, acc 0.984375, prec 0.0860576, recall 0.899925
2017-12-10T15:37:14.491069: step 4051, loss 0.0834681, acc 0.984375, prec 0.0860555, recall 0.899925
2017-12-10T15:37:14.690538: step 4052, loss 0.457299, acc 0.90625, prec 0.0861308, recall 0.900025
2017-12-10T15:37:14.889585: step 4053, loss 0.276694, acc 0.9375, prec 0.0861445, recall 0.90005
2017-12-10T15:37:15.090933: step 4054, loss 0.155145, acc 0.9375, prec 0.0861581, recall 0.900075
2017-12-10T15:37:15.289570: step 4055, loss 0.228279, acc 0.953125, prec 0.0861957, recall 0.900125
2017-12-10T15:37:15.486540: step 4056, loss 0.233268, acc 0.953125, prec 0.0862333, recall 0.900175
2017-12-10T15:37:15.685791: step 4057, loss 0.135755, acc 0.9375, prec 0.086247, recall 0.9002
2017-12-10T15:37:15.884967: step 4058, loss 0.37616, acc 0.921875, prec 0.0862366, recall 0.9002
2017-12-10T15:37:16.084171: step 4059, loss 0.16038, acc 1, prec 0.0862804, recall 0.90025
2017-12-10T15:37:16.283341: step 4060, loss 0.249614, acc 0.921875, prec 0.086292, recall 0.900275
2017-12-10T15:37:16.479800: step 4061, loss 1.01502, acc 0.890625, prec 0.0863432, recall 0.90035
2017-12-10T15:37:16.685588: step 4062, loss 0.331815, acc 0.921875, prec 0.0863328, recall 0.90035
2017-12-10T15:37:16.880378: step 4063, loss 0.382771, acc 0.890625, prec 0.0863621, recall 0.900399
2017-12-10T15:37:17.077207: step 4064, loss 0.327512, acc 0.890625, prec 0.0863695, recall 0.900424
2017-12-10T15:37:17.274700: step 4065, loss 0.186706, acc 0.9375, prec 0.0864268, recall 0.900499
2017-12-10T15:37:17.469590: step 4066, loss 0.258323, acc 0.921875, prec 0.0864384, recall 0.900524
2017-12-10T15:37:17.663796: step 4067, loss 0.278532, acc 0.9375, prec 0.0864957, recall 0.900598
2017-12-10T15:37:17.860742: step 4068, loss 0.265549, acc 0.90625, prec 0.0864833, recall 0.900598
2017-12-10T15:37:18.063028: step 4069, loss 0.184332, acc 0.921875, prec 0.0864729, recall 0.900598
2017-12-10T15:37:18.259802: step 4070, loss 0.0650809, acc 0.984375, prec 0.0864927, recall 0.900623
2017-12-10T15:37:18.458010: step 4071, loss 0.25219, acc 0.984375, prec 0.0865343, recall 0.900672
2017-12-10T15:37:18.656665: step 4072, loss 0.127523, acc 0.953125, prec 0.0865718, recall 0.900722
2017-12-10T15:37:18.857377: step 4073, loss 0.197679, acc 0.9375, prec 0.0865854, recall 0.900746
2017-12-10T15:37:19.056071: step 4074, loss 0.516481, acc 0.953125, prec 0.086601, recall 0.900771
2017-12-10T15:37:19.256675: step 4075, loss 0.0348722, acc 1, prec 0.086601, recall 0.900771
2017-12-10T15:37:19.451279: step 4076, loss 0.178243, acc 0.96875, prec 0.0866405, recall 0.90082
2017-12-10T15:37:19.657462: step 4077, loss 0.0985922, acc 0.984375, prec 0.0866821, recall 0.90087
2017-12-10T15:37:19.853274: step 4078, loss 0.0489592, acc 0.953125, prec 0.0866977, recall 0.900894
2017-12-10T15:37:20.049157: step 4079, loss 0.0486368, acc 0.984375, prec 0.0867175, recall 0.900919
2017-12-10T15:37:20.243834: step 4080, loss 0.160456, acc 0.96875, prec 0.0867134, recall 0.900919
2017-12-10T15:37:20.443029: step 4081, loss 0.0703372, acc 0.953125, prec 0.0867071, recall 0.900919
2017-12-10T15:37:20.639220: step 4082, loss 0.0511439, acc 0.984375, prec 0.0867487, recall 0.900968
2017-12-10T15:37:20.842640: step 4083, loss 0.178095, acc 0.9375, prec 0.0867404, recall 0.900968
2017-12-10T15:37:21.038493: step 4084, loss 0.0345448, acc 0.984375, prec 0.0867383, recall 0.900968
2017-12-10T15:37:21.234819: step 4085, loss 0.0638604, acc 0.96875, prec 0.086756, recall 0.900993
2017-12-10T15:37:21.433496: step 4086, loss 0.101144, acc 0.96875, prec 0.0867737, recall 0.901017
2017-12-10T15:37:21.636007: step 4087, loss 0.0749368, acc 1, prec 0.0868391, recall 0.901091
2017-12-10T15:37:21.836234: step 4088, loss 0.00739846, acc 1, prec 0.086861, recall 0.901115
2017-12-10T15:37:22.034190: step 4089, loss 3.71514, acc 0.96875, prec 0.0868589, recall 0.900892
2017-12-10T15:37:22.243565: step 4090, loss 0.0167625, acc 1, prec 0.0868589, recall 0.900892
2017-12-10T15:37:22.443455: step 4091, loss 0.0323681, acc 0.984375, prec 0.0868568, recall 0.900892
2017-12-10T15:37:22.637987: step 4092, loss 0.163478, acc 0.96875, prec 0.0868745, recall 0.900917
2017-12-10T15:37:22.839062: step 4093, loss 0.0425748, acc 0.984375, prec 0.0869378, recall 0.90099
2017-12-10T15:37:23.038406: step 4094, loss 0.0621575, acc 0.953125, prec 0.0869316, recall 0.90099
2017-12-10T15:37:23.241135: step 4095, loss 0.0831106, acc 0.953125, prec 0.0869472, recall 0.901015
2017-12-10T15:37:23.438668: step 4096, loss 0.195154, acc 0.96875, prec 0.0869866, recall 0.901064
2017-12-10T15:37:23.639262: step 4097, loss 0.753078, acc 0.953125, prec 0.0870458, recall 0.901137
2017-12-10T15:37:23.841930: step 4098, loss 0.235905, acc 0.921875, prec 0.0870572, recall 0.901161
2017-12-10T15:37:24.039464: step 4099, loss 0.274838, acc 0.9375, prec 0.0870707, recall 0.901186
2017-12-10T15:37:24.238248: step 4100, loss 0.0272635, acc 1, prec 0.0870707, recall 0.901186
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_2/1512937383/checkpoints/model-4100

2017-12-10T15:37:25.573576: step 4101, loss 0.0716651, acc 0.96875, prec 0.0870665, recall 0.901186
2017-12-10T15:37:25.772917: step 4102, loss 0.252741, acc 0.90625, prec 0.0870976, recall 0.901235
2017-12-10T15:37:25.970644: step 4103, loss 0.170584, acc 0.953125, prec 0.0871349, recall 0.901283
2017-12-10T15:37:26.169546: step 4104, loss 0.176115, acc 0.953125, prec 0.0871723, recall 0.901332
2017-12-10T15:37:26.363192: step 4105, loss 0.457721, acc 0.890625, prec 0.0871795, recall 0.901356
2017-12-10T15:37:26.558872: step 4106, loss 0.47151, acc 0.875, prec 0.0871629, recall 0.901356
2017-12-10T15:37:26.758739: step 4107, loss 0.325962, acc 0.90625, prec 0.0871939, recall 0.901405
2017-12-10T15:37:26.960525: step 4108, loss 0.356482, acc 0.890625, prec 0.0872011, recall 0.901429
2017-12-10T15:37:27.158842: step 4109, loss 0.414553, acc 0.875, prec 0.0871845, recall 0.901429
2017-12-10T15:37:27.357806: step 4110, loss 0.352175, acc 0.875, prec 0.0871896, recall 0.901454
2017-12-10T15:37:27.553916: step 4111, loss 0.303611, acc 0.921875, prec 0.0871792, recall 0.901454
2017-12-10T15:37:27.750884: step 4112, loss 0.0492111, acc 0.984375, prec 0.0872424, recall 0.901526
2017-12-10T15:37:27.948659: step 4113, loss 0.0777692, acc 0.953125, prec 0.0872579, recall 0.901551
2017-12-10T15:37:28.145671: step 4114, loss 0.235126, acc 0.96875, prec 0.0872538, recall 0.901551
2017-12-10T15:37:28.339639: step 4115, loss 0.129802, acc 0.96875, prec 0.0872713, recall 0.901575
2017-12-10T15:37:28.535491: step 4116, loss 0.169196, acc 0.953125, prec 0.0872868, recall 0.901599
2017-12-10T15:37:28.740037: step 4117, loss 0.24509, acc 0.90625, prec 0.0872961, recall 0.901623
2017-12-10T15:37:28.938085: step 4118, loss 0.215297, acc 0.984375, prec 0.0873158, recall 0.901647
2017-12-10T15:37:29.137379: step 4119, loss 0.195885, acc 0.953125, prec 0.087353, recall 0.901696
2017-12-10T15:37:29.340399: step 4120, loss 0.12542, acc 0.953125, prec 0.0873467, recall 0.901696
2017-12-10T15:37:29.542286: step 4121, loss 0.0852482, acc 0.953125, prec 0.0873405, recall 0.901696
2017-12-10T15:37:29.740924: step 4122, loss 0.0828905, acc 0.96875, prec 0.0873363, recall 0.901696
2017-12-10T15:37:29.939675: step 4123, loss 0.0183202, acc 1, prec 0.0873581, recall 0.90172
2017-12-10T15:37:30.138305: step 4124, loss 0.0413511, acc 0.984375, prec 0.087356, recall 0.90172
2017-12-10T15:37:30.332843: step 4125, loss 0.16664, acc 0.96875, prec 0.0873736, recall 0.901744
2017-12-10T15:37:30.535012: step 4126, loss 0.0738457, acc 0.96875, prec 0.0873694, recall 0.901744
2017-12-10T15:37:30.736314: step 4127, loss 0.0224931, acc 0.984375, prec 0.0873673, recall 0.901744
2017-12-10T15:37:30.939688: step 4128, loss 0.0863032, acc 0.9375, prec 0.087359, recall 0.901744
2017-12-10T15:37:31.141906: step 4129, loss 0.00515604, acc 1, prec 0.087359, recall 0.901744
2017-12-10T15:37:31.344414: step 4130, loss 0.0538547, acc 0.984375, prec 0.0873569, recall 0.901744
2017-12-10T15:37:31.548454: step 4131, loss 0.743173, acc 0.984375, prec 0.0873983, recall 0.901792
2017-12-10T15:37:31.745849: step 4132, loss 0.00740793, acc 1, prec 0.0873983, recall 0.901792
2017-12-10T15:37:31.941196: step 4133, loss 0.0153659, acc 1, prec 0.0873983, recall 0.901792
2017-12-10T15:37:32.138393: step 4134, loss 0.0660241, acc 0.96875, prec 0.0874158, recall 0.901816
2017-12-10T15:37:32.336134: step 4135, loss 0.0252225, acc 1, prec 0.0874375, recall 0.901841
2017-12-10T15:37:32.537675: step 4136, loss 0.106766, acc 0.953125, prec 0.087453, recall 0.901865
2017-12-10T15:37:32.731877: step 4137, loss 0.122563, acc 0.953125, prec 0.0874468, recall 0.901865
2017-12-10T15:37:32.936450: step 4138, loss 0.218928, acc 1, prec 0.0875119, recall 0.901937
2017-12-10T15:37:33.134264: step 4139, loss 0.0537097, acc 1, prec 0.0875553, recall 0.901985
2017-12-10T15:37:33.333529: step 4140, loss 0.025241, acc 1, prec 0.0875553, recall 0.901985
2017-12-10T15:37:33.532530: step 4141, loss 1.21554, acc 0.953125, prec 0.0875511, recall 0.901764
2017-12-10T15:37:33.734345: step 4142, loss 0.126963, acc 0.984375, prec 0.0876142, recall 0.901836
2017-12-10T15:37:33.931500: step 4143, loss 0.33188, acc 0.953125, prec 0.087673, recall 0.901908
2017-12-10T15:37:34.131154: step 4144, loss 1.20569, acc 0.984375, prec 0.0876947, recall 0.901711
2017-12-10T15:37:34.335928: step 4145, loss 0.339476, acc 0.984375, prec 0.0877143, recall 0.901736
2017-12-10T15:37:34.533417: step 4146, loss 0.161377, acc 0.953125, prec 0.087708, recall 0.901736
2017-12-10T15:37:34.731437: step 4147, loss 0.249852, acc 0.90625, prec 0.0877172, recall 0.90176
2017-12-10T15:37:34.926590: step 4148, loss 0.0363703, acc 1, prec 0.0877172, recall 0.90176
2017-12-10T15:37:35.122770: step 4149, loss 0.203517, acc 0.9375, prec 0.0877089, recall 0.90176
2017-12-10T15:37:35.321466: step 4150, loss 0.318125, acc 0.90625, prec 0.0876964, recall 0.90176
2017-12-10T15:37:35.515886: step 4151, loss 0.487924, acc 0.890625, prec 0.0876818, recall 0.90176
2017-12-10T15:37:35.711785: step 4152, loss 0.594109, acc 0.890625, prec 0.0876672, recall 0.90176
2017-12-10T15:37:35.904886: step 4153, loss 0.366501, acc 0.875, prec 0.0876505, recall 0.90176
2017-12-10T15:37:36.104454: step 4154, loss 0.329679, acc 0.90625, prec 0.0876597, recall 0.901784
2017-12-10T15:37:36.302857: step 4155, loss 0.70726, acc 0.796875, prec 0.0876543, recall 0.901808
2017-12-10T15:37:36.499634: step 4156, loss 0.131352, acc 0.9375, prec 0.087646, recall 0.901808
2017-12-10T15:37:36.695318: step 4157, loss 0.440142, acc 0.859375, prec 0.0876489, recall 0.901832
2017-12-10T15:37:36.895033: step 4158, loss 2.78661, acc 0.84375, prec 0.0876302, recall 0.901611
2017-12-10T15:37:37.097631: step 4159, loss 0.209578, acc 0.890625, prec 0.0876157, recall 0.901611
2017-12-10T15:37:37.296581: step 4160, loss 0.095701, acc 0.96875, prec 0.0876548, recall 0.901659
2017-12-10T15:37:37.501342: step 4161, loss 0.193431, acc 0.953125, prec 0.0877135, recall 0.901731
2017-12-10T15:37:37.700890: step 4162, loss 0.675659, acc 0.890625, prec 0.0877205, recall 0.901755
2017-12-10T15:37:37.898814: step 4163, loss 0.335824, acc 0.890625, prec 0.0877276, recall 0.901779
2017-12-10T15:37:38.100538: step 4164, loss 0.14027, acc 0.953125, prec 0.087743, recall 0.901803
2017-12-10T15:37:38.299630: step 4165, loss 0.431067, acc 0.890625, prec 0.0877501, recall 0.901827
2017-12-10T15:37:38.497656: step 4166, loss 0.308095, acc 0.875, prec 0.0877334, recall 0.901827
2017-12-10T15:37:38.696606: step 4167, loss 0.150564, acc 0.953125, prec 0.0877488, recall 0.901851
2017-12-10T15:37:38.891907: step 4168, loss 0.379643, acc 0.9375, prec 0.0877621, recall 0.901875
2017-12-10T15:37:39.088955: step 4169, loss 0.642187, acc 0.859375, prec 0.087765, recall 0.901899
2017-12-10T15:37:39.279785: step 4170, loss 0.261319, acc 0.921875, prec 0.0877546, recall 0.901899
2017-12-10T15:37:39.477281: step 4171, loss 0.132539, acc 0.96875, prec 0.0877505, recall 0.901899
2017-12-10T15:37:39.673193: step 4172, loss 0.340202, acc 0.859375, prec 0.087775, recall 0.901946
2017-12-10T15:37:39.873887: step 4173, loss 0.0584517, acc 0.96875, prec 0.0877924, recall 0.90197
2017-12-10T15:37:40.070603: step 4174, loss 0.281573, acc 0.890625, prec 0.0877779, recall 0.90197
2017-12-10T15:37:40.269797: step 4175, loss 0.212659, acc 0.921875, prec 0.0877891, recall 0.901994
2017-12-10T15:37:40.470927: step 4176, loss 0.232173, acc 0.921875, prec 0.0877787, recall 0.901994
2017-12-10T15:37:40.671967: step 4177, loss 1.48597, acc 0.890625, prec 0.0877662, recall 0.901775
2017-12-10T15:37:40.869741: step 4178, loss 0.0718341, acc 0.984375, prec 0.0878073, recall 0.901823
2017-12-10T15:37:41.067101: step 4179, loss 0.207919, acc 0.9375, prec 0.0878637, recall 0.901894
2017-12-10T15:37:41.259968: step 4180, loss 0.323073, acc 0.90625, prec 0.0878728, recall 0.901918
2017-12-10T15:37:41.458127: step 4181, loss 0.270918, acc 0.96875, prec 0.087955, recall 0.902013
2017-12-10T15:37:41.658341: step 4182, loss 0.0498243, acc 0.984375, prec 0.0879529, recall 0.902013
2017-12-10T15:37:41.856559: step 4183, loss 0.136012, acc 0.953125, prec 0.0879682, recall 0.902037
2017-12-10T15:37:42.059287: step 4184, loss 1.06657, acc 0.953125, prec 0.0879835, recall 0.902061
2017-12-10T15:37:42.262359: step 4185, loss 0.0878782, acc 0.953125, prec 0.0879773, recall 0.902061
2017-12-10T15:37:42.458838: step 4186, loss 0.126046, acc 0.984375, prec 0.0879968, recall 0.902084
2017-12-10T15:37:42.654205: step 4187, loss 0.25565, acc 0.9375, prec 0.0879885, recall 0.902084
2017-12-10T15:37:42.851299: step 4188, loss 0.0552452, acc 0.96875, prec 0.0880059, recall 0.902108
2017-12-10T15:37:43.047235: step 4189, loss 0.250493, acc 0.9375, prec 0.0879975, recall 0.902108
2017-12-10T15:37:43.247782: step 4190, loss 0.303048, acc 0.9375, prec 0.0880108, recall 0.902132
2017-12-10T15:37:43.442169: step 4191, loss 0.113473, acc 0.9375, prec 0.0880025, recall 0.902132
2017-12-10T15:37:43.643426: step 4192, loss 0.212365, acc 0.90625, prec 0.0880331, recall 0.902179
2017-12-10T15:37:43.843058: step 4193, loss 0.17208, acc 0.984375, prec 0.088031, recall 0.902179
2017-12-10T15:37:44.044673: step 4194, loss 0.0115237, acc 1, prec 0.088031, recall 0.902179
2017-12-10T15:37:44.248147: step 4195, loss 0.12671, acc 0.9375, prec 0.0880658, recall 0.902227
2017-12-10T15:37:44.458132: step 4196, loss 0.0459915, acc 1, prec 0.0880873, recall 0.90225
2017-12-10T15:37:44.659300: step 4197, loss 0.054354, acc 1, prec 0.0881304, recall 0.902297
2017-12-10T15:37:44.859748: step 4198, loss 0.0518807, acc 0.984375, prec 0.0881498, recall 0.902321
2017-12-10T15:37:45.059617: step 4199, loss 0.732673, acc 0.90625, prec 0.0881589, recall 0.902345
2017-12-10T15:37:45.261194: step 4200, loss 0.133578, acc 0.953125, prec 0.0881526, recall 0.902345
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_2/1512937383/checkpoints/model-4200

2017-12-10T15:37:46.531039: step 4201, loss 0.211746, acc 0.96875, prec 0.08817, recall 0.902368
2017-12-10T15:37:46.732461: step 4202, loss 0.0522323, acc 0.96875, prec 0.0881658, recall 0.902368
2017-12-10T15:37:46.927582: step 4203, loss 0.519566, acc 0.90625, prec 0.0881749, recall 0.902392
2017-12-10T15:37:47.123055: step 4204, loss 0.191036, acc 0.9375, prec 0.0881881, recall 0.902415
2017-12-10T15:37:47.321105: step 4205, loss 0.274434, acc 0.921875, prec 0.0881777, recall 0.902415
2017-12-10T15:37:47.515914: step 4206, loss 0.259838, acc 0.96875, prec 0.0881735, recall 0.902415
2017-12-10T15:37:47.715138: step 4207, loss 0.68872, acc 0.984375, prec 0.0882145, recall 0.902463
2017-12-10T15:37:47.918629: step 4208, loss 0.0130856, acc 1, prec 0.088236, recall 0.902486
2017-12-10T15:37:48.117623: step 4209, loss 0.203324, acc 0.96875, prec 0.0882749, recall 0.902533
2017-12-10T15:37:48.316261: step 4210, loss 0.197467, acc 0.96875, prec 0.0882707, recall 0.902533
2017-12-10T15:37:48.519775: step 4211, loss 0.0924304, acc 0.96875, prec 0.088288, recall 0.902557
2017-12-10T15:37:48.721170: step 4212, loss 0.0812915, acc 0.953125, prec 0.0882818, recall 0.902557
2017-12-10T15:37:48.921739: step 4213, loss 0.0657815, acc 0.984375, prec 0.0882797, recall 0.902557
2017-12-10T15:37:49.120224: step 4214, loss 0.169136, acc 0.953125, prec 0.0883165, recall 0.902604
2017-12-10T15:37:49.318178: step 4215, loss 0.237148, acc 0.9375, prec 0.0883081, recall 0.902604
2017-12-10T15:37:49.521700: step 4216, loss 0.163789, acc 0.953125, prec 0.0883234, recall 0.902627
2017-12-10T15:37:49.720380: step 4217, loss 0.186895, acc 0.953125, prec 0.0883171, recall 0.902627
2017-12-10T15:37:49.916429: step 4218, loss 0.192014, acc 0.921875, prec 0.0883067, recall 0.902627
2017-12-10T15:37:50.114570: step 4219, loss 0.181826, acc 0.921875, prec 0.0882963, recall 0.902627
2017-12-10T15:37:50.312221: step 4220, loss 0.184523, acc 0.90625, prec 0.0883053, recall 0.902651
2017-12-10T15:37:50.507319: step 4221, loss 0.0614, acc 0.96875, prec 0.0883226, recall 0.902674
2017-12-10T15:37:50.712543: step 4222, loss 0.25934, acc 0.9375, prec 0.0883358, recall 0.902698
2017-12-10T15:37:50.905221: step 4223, loss 0.135847, acc 0.96875, prec 0.0883316, recall 0.902698
2017-12-10T15:37:51.103499: step 4224, loss 0.0271998, acc 1, prec 0.0883316, recall 0.902698
2017-12-10T15:37:51.298901: step 4225, loss 1.27405, acc 1, prec 0.0884176, recall 0.902791
2017-12-10T15:37:51.500098: step 4226, loss 0.107519, acc 0.953125, prec 0.0884113, recall 0.902791
2017-12-10T15:37:51.703485: step 4227, loss 0.160332, acc 0.96875, prec 0.0884072, recall 0.902791
2017-12-10T15:37:51.904694: step 4228, loss 0.1094, acc 0.9375, prec 0.0884203, recall 0.902815
2017-12-10T15:37:52.101909: step 4229, loss 0.00970038, acc 1, prec 0.0884203, recall 0.902815
2017-12-10T15:37:52.295986: step 4230, loss 0.085908, acc 0.96875, prec 0.0884161, recall 0.902815
2017-12-10T15:37:52.492272: step 4231, loss 0.172783, acc 0.953125, prec 0.0884099, recall 0.902815
2017-12-10T15:37:52.701351: step 4232, loss 0.0374909, acc 0.984375, prec 0.0884078, recall 0.902815
2017-12-10T15:37:52.904741: step 4233, loss 0.231002, acc 0.984375, prec 0.0884487, recall 0.902861
2017-12-10T15:37:53.105434: step 4234, loss 0.129912, acc 0.984375, prec 0.0884681, recall 0.902885
2017-12-10T15:37:53.307154: step 4235, loss 0.242896, acc 0.96875, prec 0.0884854, recall 0.902908
2017-12-10T15:37:53.509634: step 4236, loss 0.127184, acc 0.984375, prec 0.0885477, recall 0.902978
2017-12-10T15:37:53.711495: step 4237, loss 0.0378315, acc 0.96875, prec 0.088565, recall 0.903001
2017-12-10T15:37:53.908070: step 4238, loss 0.0598868, acc 1, prec 0.0886079, recall 0.903048
2017-12-10T15:37:54.103776: step 4239, loss 0.0995693, acc 0.953125, prec 0.0886016, recall 0.903048
2017-12-10T15:37:54.300460: step 4240, loss 0.0585307, acc 0.96875, prec 0.0886189, recall 0.903071
2017-12-10T15:37:54.498092: step 4241, loss 0.185049, acc 0.96875, prec 0.0886577, recall 0.903117
2017-12-10T15:37:54.699437: step 4242, loss 0.202641, acc 0.953125, prec 0.0886514, recall 0.903117
2017-12-10T15:37:54.901674: step 4243, loss 0.108115, acc 0.96875, prec 0.0886472, recall 0.903117
2017-12-10T15:37:55.099783: step 4244, loss 0.139602, acc 0.9375, prec 0.0886389, recall 0.903117
2017-12-10T15:37:55.297220: step 4245, loss 0.245301, acc 0.921875, prec 0.0886499, recall 0.903141
2017-12-10T15:37:55.495685: step 4246, loss 0.0759829, acc 0.984375, prec 0.0886693, recall 0.903164
2017-12-10T15:37:55.701315: step 4247, loss 0.0593323, acc 0.984375, prec 0.0886886, recall 0.903187
2017-12-10T15:37:55.897011: step 4248, loss 0.094233, acc 0.96875, prec 0.0886844, recall 0.903187
2017-12-10T15:37:56.097720: step 4249, loss 0.200841, acc 0.9375, prec 0.0886761, recall 0.903187
2017-12-10T15:37:56.295248: step 4250, loss 0.0991504, acc 0.96875, prec 0.0887148, recall 0.903234
2017-12-10T15:37:56.492959: step 4251, loss 0.276403, acc 0.96875, prec 0.0887321, recall 0.903257
2017-12-10T15:37:56.691305: step 4252, loss 0.0406847, acc 0.984375, prec 0.08873, recall 0.903257
2017-12-10T15:37:56.889369: step 4253, loss 0.154383, acc 0.96875, prec 0.0887687, recall 0.903303
2017-12-10T15:37:57.086558: step 4254, loss 0.0478519, acc 0.984375, prec 0.0887666, recall 0.903303
2017-12-10T15:37:57.288116: step 4255, loss 3.98406, acc 0.96875, prec 0.0887859, recall 0.90311
2017-12-10T15:37:57.489778: step 4256, loss 0.0621394, acc 0.96875, prec 0.0888246, recall 0.903156
2017-12-10T15:37:57.689292: step 4257, loss 0.0487439, acc 0.96875, prec 0.0888419, recall 0.90318
2017-12-10T15:37:57.888311: step 4258, loss 0.143075, acc 0.953125, prec 0.088857, recall 0.903203
2017-12-10T15:37:58.088298: step 4259, loss 0.380081, acc 0.90625, prec 0.0888445, recall 0.903203
2017-12-10T15:37:58.284498: step 4260, loss 0.0839049, acc 0.953125, prec 0.0888596, recall 0.903226
2017-12-10T15:37:58.482453: step 4261, loss 0.227199, acc 0.921875, prec 0.0888492, recall 0.903226
2017-12-10T15:37:58.676326: step 4262, loss 0.227233, acc 0.921875, prec 0.0888602, recall 0.903249
2017-12-10T15:37:58.875805: step 4263, loss 0.144814, acc 0.9375, prec 0.088916, recall 0.903318
2017-12-10T15:37:59.069847: step 4264, loss 0.63634, acc 0.84375, prec 0.0889166, recall 0.903341
2017-12-10T15:37:59.277809: step 4265, loss 0.310219, acc 0.875, prec 0.0889212, recall 0.903364
2017-12-10T15:37:59.477278: step 4266, loss 0.415273, acc 0.875, prec 0.0889045, recall 0.903364
2017-12-10T15:37:59.676043: step 4267, loss 0.0902611, acc 0.96875, prec 0.0889432, recall 0.90341
2017-12-10T15:37:59.871558: step 4268, loss 0.303377, acc 0.90625, prec 0.088952, recall 0.903434
2017-12-10T15:38:00.067105: step 4269, loss 0.174339, acc 0.953125, prec 0.0889671, recall 0.903457
2017-12-10T15:38:00.262947: step 4270, loss 0.311309, acc 0.890625, prec 0.0889953, recall 0.903503
2017-12-10T15:38:00.460815: step 4271, loss 0.323043, acc 0.90625, prec 0.0890255, recall 0.903548
2017-12-10T15:38:00.661766: step 4272, loss 0.702758, acc 0.859375, prec 0.0890281, recall 0.903571
2017-12-10T15:38:00.860214: step 4273, loss 0.32269, acc 0.84375, prec 0.0890499, recall 0.903617
2017-12-10T15:38:01.061785: step 4274, loss 0.0621341, acc 0.96875, prec 0.0890458, recall 0.903617
2017-12-10T15:38:01.264702: step 4275, loss 0.183486, acc 0.921875, prec 0.089078, recall 0.903663
2017-12-10T15:38:01.462223: step 4276, loss 0.373371, acc 0.90625, prec 0.0890869, recall 0.903686
2017-12-10T15:38:01.666971: step 4277, loss 0.353066, acc 0.90625, prec 0.089117, recall 0.903732
2017-12-10T15:38:01.866361: step 4278, loss 0.0805248, acc 0.953125, prec 0.0891108, recall 0.903732
2017-12-10T15:38:02.069287: step 4279, loss 0.141883, acc 0.96875, prec 0.0891493, recall 0.903778
2017-12-10T15:38:02.266622: step 4280, loss 0.352114, acc 1, prec 0.0891706, recall 0.9038
2017-12-10T15:38:02.467622: step 4281, loss 0.230408, acc 0.953125, prec 0.089207, recall 0.903846
2017-12-10T15:38:02.664338: step 4282, loss 0.0772026, acc 0.953125, prec 0.0892008, recall 0.903846
2017-12-10T15:38:02.860506: step 4283, loss 0.0324502, acc 0.984375, prec 0.0892414, recall 0.903892
2017-12-10T15:38:03.063207: step 4284, loss 0.39204, acc 0.9375, prec 0.0892543, recall 0.903915
2017-12-10T15:38:03.258625: step 4285, loss 0.201768, acc 0.90625, prec 0.0892631, recall 0.903937
2017-12-10T15:38:03.458306: step 4286, loss 0.0164016, acc 1, prec 0.0892845, recall 0.90396
2017-12-10T15:38:03.659607: step 4287, loss 0.115717, acc 0.953125, prec 0.0892782, recall 0.90396
2017-12-10T15:38:03.859354: step 4288, loss 0.0506902, acc 0.984375, prec 0.0892761, recall 0.90396
2017-12-10T15:38:04.057326: step 4289, loss 0.231108, acc 0.953125, prec 0.0892698, recall 0.90396
2017-12-10T15:38:04.256062: step 4290, loss 0.236229, acc 0.953125, prec 0.0892636, recall 0.90396
2017-12-10T15:38:04.454258: step 4291, loss 0.320393, acc 0.953125, prec 0.0892573, recall 0.90396
2017-12-10T15:38:04.651895: step 4292, loss 0.0486605, acc 0.984375, prec 0.0892765, recall 0.903983
2017-12-10T15:38:04.848598: step 4293, loss 0.0255098, acc 1, prec 0.0892765, recall 0.903983
2017-12-10T15:38:05.046133: step 4294, loss 1.08149, acc 1, prec 0.0892978, recall 0.904006
2017-12-10T15:38:05.247694: step 4295, loss 0.987347, acc 0.984375, prec 0.0893171, recall 0.904028
2017-12-10T15:38:05.446256: step 4296, loss 0.151735, acc 0.984375, prec 0.089315, recall 0.904028
2017-12-10T15:38:05.647374: step 4297, loss 0.22752, acc 0.96875, prec 0.0893321, recall 0.904051
2017-12-10T15:38:05.846641: step 4298, loss 0.0633548, acc 1, prec 0.0893748, recall 0.904097
2017-12-10T15:38:06.046785: step 4299, loss 0.0336923, acc 0.984375, prec 0.0894153, recall 0.904142
2017-12-10T15:38:06.244776: step 4300, loss 0.0714379, acc 0.984375, prec 0.0894132, recall 0.904142
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_2/1512937383/checkpoints/model-4300

2017-12-10T15:38:07.448977: step 4301, loss 0.227159, acc 0.953125, prec 0.0894282, recall 0.904165
2017-12-10T15:38:07.644365: step 4302, loss 0.216118, acc 0.96875, prec 0.0894454, recall 0.904187
2017-12-10T15:38:07.842284: step 4303, loss 0.0498369, acc 0.96875, prec 0.0894412, recall 0.904187
2017-12-10T15:38:08.042884: step 4304, loss 0.375568, acc 0.9375, prec 0.0894754, recall 0.904233
2017-12-10T15:38:08.236302: step 4305, loss 0.230266, acc 0.953125, prec 0.0894691, recall 0.904233
2017-12-10T15:38:08.436752: step 4306, loss 0.104912, acc 0.984375, prec 0.0894883, recall 0.904255
2017-12-10T15:38:08.634557: step 4307, loss 0.267226, acc 0.921875, prec 0.0894779, recall 0.904255
2017-12-10T15:38:08.830682: step 4308, loss 0.123521, acc 0.921875, prec 0.08951, recall 0.904301
2017-12-10T15:38:09.033543: step 4309, loss 0.34191, acc 0.9375, prec 0.0895229, recall 0.904323
2017-12-10T15:38:09.234651: step 4310, loss 0.218049, acc 0.921875, prec 0.089555, recall 0.904368
2017-12-10T15:38:09.434619: step 4311, loss 0.15798, acc 0.953125, prec 0.08957, recall 0.904391
2017-12-10T15:38:09.632656: step 4312, loss 0.361301, acc 0.90625, prec 0.0895575, recall 0.904391
2017-12-10T15:38:09.833596: step 4313, loss 0.132462, acc 0.953125, prec 0.0895512, recall 0.904391
2017-12-10T15:38:10.033394: step 4314, loss 0.158578, acc 0.953125, prec 0.0895662, recall 0.904414
2017-12-10T15:38:10.234327: step 4315, loss 0.170282, acc 0.9375, prec 0.0895578, recall 0.904414
2017-12-10T15:38:10.429044: step 4316, loss 0.157335, acc 0.984375, prec 0.0895983, recall 0.904459
2017-12-10T15:38:10.628386: step 4317, loss 0.1979, acc 0.9375, prec 0.0896112, recall 0.904481
2017-12-10T15:38:10.826818: step 4318, loss 0.0909531, acc 0.96875, prec 0.0896283, recall 0.904504
2017-12-10T15:38:11.024313: step 4319, loss 0.367903, acc 0.90625, prec 0.0896582, recall 0.904549
2017-12-10T15:38:11.227643: step 4320, loss 0.294571, acc 0.921875, prec 0.0896478, recall 0.904549
2017-12-10T15:38:11.421851: step 4321, loss 0.184971, acc 0.96875, prec 0.0897074, recall 0.904616
2017-12-10T15:38:11.623185: step 4322, loss 0.173838, acc 0.9375, prec 0.0897415, recall 0.904661
2017-12-10T15:38:11.827912: step 4323, loss 0.0262543, acc 0.984375, prec 0.0897607, recall 0.904683
2017-12-10T15:38:12.031170: step 4324, loss 0.196958, acc 0.9375, prec 0.0897948, recall 0.904728
2017-12-10T15:38:12.228121: step 4325, loss 0.250607, acc 0.953125, prec 0.0897885, recall 0.904728
2017-12-10T15:38:12.426008: step 4326, loss 0.11573, acc 0.96875, prec 0.0898055, recall 0.904751
2017-12-10T15:38:12.621116: step 4327, loss 0.0945045, acc 0.984375, prec 0.0898884, recall 0.90484
2017-12-10T15:38:12.823970: step 4328, loss 0.0341958, acc 0.984375, prec 0.0899076, recall 0.904863
2017-12-10T15:38:13.022871: step 4329, loss 0.811716, acc 0.96875, prec 0.0899883, recall 0.904952
2017-12-10T15:38:13.225591: step 4330, loss 0.0725699, acc 0.96875, prec 0.0899841, recall 0.904952
2017-12-10T15:38:13.419543: step 4331, loss 0.116714, acc 0.96875, prec 0.0900224, recall 0.904996
2017-12-10T15:38:13.615636: step 4332, loss 0.0854936, acc 0.953125, prec 0.0900373, recall 0.905019
2017-12-10T15:38:13.812560: step 4333, loss 0.0458011, acc 0.96875, prec 0.0900331, recall 0.905019
2017-12-10T15:38:14.014944: step 4334, loss 0.0420103, acc 1, prec 0.0900756, recall 0.905063
2017-12-10T15:38:14.217337: step 4335, loss 0.046207, acc 0.96875, prec 0.0900714, recall 0.905063
2017-12-10T15:38:14.418556: step 4336, loss 0.212237, acc 0.9375, prec 0.090063, recall 0.905063
2017-12-10T15:38:14.612015: step 4337, loss 0.157988, acc 0.953125, prec 0.0900567, recall 0.905063
2017-12-10T15:38:14.813619: step 4338, loss 0.151177, acc 0.9375, prec 0.0900483, recall 0.905063
2017-12-10T15:38:15.013794: step 4339, loss 0.0789047, acc 0.96875, prec 0.0900441, recall 0.905063
2017-12-10T15:38:15.214222: step 4340, loss 0.103017, acc 0.953125, prec 0.090059, recall 0.905086
2017-12-10T15:38:15.413801: step 4341, loss 0.175285, acc 0.9375, prec 0.0900718, recall 0.905108
2017-12-10T15:38:15.611365: step 4342, loss 0.0300953, acc 1, prec 0.0900718, recall 0.905108
2017-12-10T15:38:15.808062: step 4343, loss 0.157977, acc 0.96875, prec 0.0900888, recall 0.90513
2017-12-10T15:38:16.010293: step 4344, loss 0.42761, acc 0.984375, prec 0.0901079, recall 0.905152
2017-12-10T15:38:16.207583: step 4345, loss 0.075261, acc 0.953125, prec 0.0901016, recall 0.905152
2017-12-10T15:38:16.414329: step 4346, loss 0.0359623, acc 1, prec 0.0901016, recall 0.905152
2017-12-10T15:38:16.615312: step 4347, loss 0.198172, acc 0.9375, prec 0.0901357, recall 0.905197
2017-12-10T15:38:16.813137: step 4348, loss 0.0386946, acc 0.984375, prec 0.0901336, recall 0.905197
2017-12-10T15:38:17.008775: step 4349, loss 0.200874, acc 0.953125, prec 0.0901485, recall 0.905219
2017-12-10T15:38:17.209574: step 4350, loss 0.25792, acc 0.96875, prec 0.0901655, recall 0.905241
2017-12-10T15:38:17.410597: step 4351, loss 0.172655, acc 0.984375, prec 0.0902058, recall 0.905285
2017-12-10T15:38:17.613731: step 4352, loss 0.054822, acc 0.984375, prec 0.0902037, recall 0.905285
2017-12-10T15:38:17.811896: step 4353, loss 0.0141685, acc 1, prec 0.0902037, recall 0.905285
2017-12-10T15:38:18.010887: step 4354, loss 0.0255956, acc 1, prec 0.0902037, recall 0.905285
2017-12-10T15:38:18.213271: step 4355, loss 1.83525, acc 0.9375, prec 0.0902398, recall 0.905118
2017-12-10T15:38:18.412303: step 4356, loss 0.226685, acc 0.96875, prec 0.0902567, recall 0.90514
2017-12-10T15:38:18.610095: step 4357, loss 0.149405, acc 0.953125, prec 0.0902504, recall 0.90514
2017-12-10T15:38:18.809441: step 4358, loss 0.175671, acc 0.921875, prec 0.0902399, recall 0.90514
2017-12-10T15:38:19.011703: step 4359, loss 0.141709, acc 0.953125, prec 0.0902336, recall 0.90514
2017-12-10T15:38:19.206519: step 4360, loss 0.226006, acc 0.921875, prec 0.0902231, recall 0.90514
2017-12-10T15:38:19.401187: step 4361, loss 0.0999033, acc 0.921875, prec 0.0902126, recall 0.90514
2017-12-10T15:38:19.603964: step 4362, loss 0.176845, acc 0.90625, prec 0.0902212, recall 0.905162
2017-12-10T15:38:19.798853: step 4363, loss 0.454154, acc 0.875, prec 0.0902256, recall 0.905185
2017-12-10T15:38:19.997715: step 4364, loss 0.159533, acc 0.9375, prec 0.0902595, recall 0.905229
2017-12-10T15:38:20.191963: step 4365, loss 0.238309, acc 0.9375, prec 0.0902511, recall 0.905229
2017-12-10T15:38:20.390377: step 4366, loss 0.297717, acc 0.921875, prec 0.0902406, recall 0.905229
2017-12-10T15:38:20.591776: step 4367, loss 0.276083, acc 0.921875, prec 0.0902513, recall 0.905251
2017-12-10T15:38:20.786401: step 4368, loss 0.207665, acc 0.953125, prec 0.090245, recall 0.905251
2017-12-10T15:38:20.984599: step 4369, loss 0.161154, acc 0.921875, prec 0.0902345, recall 0.905251
2017-12-10T15:38:21.183521: step 4370, loss 0.157499, acc 0.953125, prec 0.0902494, recall 0.905273
2017-12-10T15:38:21.388301: step 4371, loss 0.0656514, acc 0.984375, prec 0.0902684, recall 0.905295
2017-12-10T15:38:21.591228: step 4372, loss 0.149106, acc 0.96875, prec 0.0903065, recall 0.905339
2017-12-10T15:38:21.792980: step 4373, loss 0.0621517, acc 0.984375, prec 0.0903256, recall 0.905361
2017-12-10T15:38:21.997300: step 4374, loss 0.170408, acc 0.953125, prec 0.0903193, recall 0.905361
2017-12-10T15:38:22.200584: step 4375, loss 0.164034, acc 0.96875, prec 0.0903362, recall 0.905383
2017-12-10T15:38:22.400331: step 4376, loss 0.0665728, acc 0.96875, prec 0.090332, recall 0.905383
2017-12-10T15:38:22.599020: step 4377, loss 0.265519, acc 0.921875, prec 0.0903215, recall 0.905383
2017-12-10T15:38:22.794487: step 4378, loss 0.149397, acc 0.921875, prec 0.0903533, recall 0.905427
2017-12-10T15:38:22.996023: step 4379, loss 0.0775215, acc 0.96875, prec 0.0903914, recall 0.905472
2017-12-10T15:38:23.192559: step 4380, loss 0.366449, acc 0.984375, prec 0.0904316, recall 0.905515
2017-12-10T15:38:23.388938: step 4381, loss 0.01072, acc 1, prec 0.0904316, recall 0.905515
2017-12-10T15:38:23.585017: step 4382, loss 0.386926, acc 0.9375, prec 0.0904443, recall 0.905537
2017-12-10T15:38:23.785820: step 4383, loss 0.0616388, acc 0.984375, prec 0.0904634, recall 0.905559
2017-12-10T15:38:23.987317: step 4384, loss 0.198313, acc 0.953125, prec 0.0904571, recall 0.905559
2017-12-10T15:38:24.192268: step 4385, loss 0.43287, acc 0.953125, prec 0.090493, recall 0.905603
2017-12-10T15:38:24.394309: step 4386, loss 0.343175, acc 0.953125, prec 0.0905501, recall 0.905669
2017-12-10T15:38:24.589496: step 4387, loss 0.0707855, acc 0.984375, prec 0.090548, recall 0.905669
2017-12-10T15:38:24.791190: step 4388, loss 0.12883, acc 0.96875, prec 0.0905649, recall 0.905691
2017-12-10T15:38:24.994139: step 4389, loss 0.0564604, acc 0.984375, prec 0.0905628, recall 0.905691
2017-12-10T15:38:25.190157: step 4390, loss 0.057632, acc 0.96875, prec 0.0906008, recall 0.905735
2017-12-10T15:38:25.391960: step 4391, loss 0.131333, acc 0.96875, prec 0.0905966, recall 0.905735
2017-12-10T15:38:25.594773: step 4392, loss 0.181741, acc 0.96875, prec 0.0905924, recall 0.905735
2017-12-10T15:38:25.796760: step 4393, loss 0.138316, acc 0.96875, prec 0.0905882, recall 0.905735
2017-12-10T15:38:25.997839: step 4394, loss 1.03244, acc 0.984375, prec 0.0906072, recall 0.905757
2017-12-10T15:38:26.199259: step 4395, loss 0.216774, acc 0.96875, prec 0.090603, recall 0.905757
2017-12-10T15:38:26.400219: step 4396, loss 0.0497566, acc 0.984375, prec 0.0906009, recall 0.905757
2017-12-10T15:38:26.597182: step 4397, loss 2.93211, acc 0.890625, prec 0.0906305, recall 0.90559
2017-12-10T15:38:26.799941: step 4398, loss 0.0988628, acc 0.96875, prec 0.0906263, recall 0.90559
2017-12-10T15:38:26.995323: step 4399, loss 0.212978, acc 0.90625, prec 0.0906348, recall 0.905612
2017-12-10T15:38:27.193314: step 4400, loss 0.138707, acc 0.953125, prec 0.0906496, recall 0.905634
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_2/1512937383/checkpoints/model-4400

2017-12-10T15:38:28.330566: step 4401, loss 0.297766, acc 0.90625, prec 0.0906792, recall 0.905678
2017-12-10T15:38:28.525562: step 4402, loss 0.366818, acc 0.921875, prec 0.0906897, recall 0.9057
2017-12-10T15:38:28.726070: step 4403, loss 0.234728, acc 0.921875, prec 0.0906792, recall 0.9057
2017-12-10T15:38:28.923579: step 4404, loss 0.623268, acc 0.8125, prec 0.090654, recall 0.9057
2017-12-10T15:38:29.123467: step 4405, loss 0.280168, acc 0.9375, prec 0.0906667, recall 0.905722
2017-12-10T15:38:29.327363: step 4406, loss 0.341064, acc 0.875, prec 0.090692, recall 0.905765
2017-12-10T15:38:29.527367: step 4407, loss 0.292533, acc 0.921875, prec 0.0907447, recall 0.905831
2017-12-10T15:38:29.723390: step 4408, loss 0.27922, acc 0.875, prec 0.0907701, recall 0.905874
2017-12-10T15:38:29.917871: step 4409, loss 0.153184, acc 0.9375, prec 0.0907827, recall 0.905896
2017-12-10T15:38:30.121436: step 4410, loss 0.204429, acc 0.9375, prec 0.0907954, recall 0.905918
2017-12-10T15:38:30.320914: step 4411, loss 0.140799, acc 0.953125, prec 0.0908101, recall 0.905939
2017-12-10T15:38:30.517342: step 4412, loss 0.353475, acc 0.9375, prec 0.0908438, recall 0.905983
2017-12-10T15:38:30.712108: step 4413, loss 0.14572, acc 0.953125, prec 0.0908586, recall 0.906005
2017-12-10T15:38:30.904795: step 4414, loss 0.394276, acc 0.84375, prec 0.0908375, recall 0.906005
2017-12-10T15:38:31.105674: step 4415, loss 0.150135, acc 0.9375, prec 0.0908712, recall 0.906048
2017-12-10T15:38:31.303997: step 4416, loss 0.151947, acc 0.953125, prec 0.0908859, recall 0.90607
2017-12-10T15:38:31.509500: step 4417, loss 0.233927, acc 0.921875, prec 0.0908754, recall 0.90607
2017-12-10T15:38:31.711485: step 4418, loss 0.271384, acc 0.9375, prec 0.0908881, recall 0.906091
2017-12-10T15:38:31.908745: step 4419, loss 0.237085, acc 0.921875, prec 0.0908775, recall 0.906091
2017-12-10T15:38:32.107272: step 4420, loss 0.216225, acc 0.9375, prec 0.0908691, recall 0.906091
2017-12-10T15:38:32.308397: step 4421, loss 0.149236, acc 0.953125, prec 0.0908628, recall 0.906091
2017-12-10T15:38:32.505516: step 4422, loss 0.204371, acc 0.984375, prec 0.0909028, recall 0.906135
2017-12-10T15:38:32.711635: step 4423, loss 0.233344, acc 0.953125, prec 0.0908965, recall 0.906135
2017-12-10T15:38:32.908705: step 4424, loss 0.100728, acc 0.96875, prec 0.0908923, recall 0.906135
2017-12-10T15:38:33.104230: step 4425, loss 0.101418, acc 0.96875, prec 0.0909301, recall 0.906178
2017-12-10T15:38:33.305413: step 4426, loss 0.123959, acc 0.984375, prec 0.090949, recall 0.9062
2017-12-10T15:38:33.505364: step 4427, loss 1.45282, acc 0.953125, prec 0.0909659, recall 0.906012
2017-12-10T15:38:33.715514: step 4428, loss 0.509668, acc 0.96875, prec 0.0909827, recall 0.906034
2017-12-10T15:38:33.918888: step 4429, loss 0.0579222, acc 0.96875, prec 0.0909785, recall 0.906034
2017-12-10T15:38:34.115307: step 4430, loss 0.0420047, acc 0.984375, prec 0.0910184, recall 0.906077
2017-12-10T15:38:34.315971: step 4431, loss 0.0310676, acc 1, prec 0.0910184, recall 0.906077
2017-12-10T15:38:34.518988: step 4432, loss 0.131211, acc 0.953125, prec 0.0910121, recall 0.906077
2017-12-10T15:38:34.717888: step 4433, loss 0.141885, acc 0.96875, prec 0.0910709, recall 0.906142
2017-12-10T15:38:34.915949: step 4434, loss 0.124389, acc 0.953125, prec 0.0910646, recall 0.906142
2017-12-10T15:38:35.115728: step 4435, loss 0.394802, acc 0.921875, prec 0.0910961, recall 0.906185
2017-12-10T15:38:35.313680: step 4436, loss 0.274487, acc 0.9375, prec 0.0911087, recall 0.906207
2017-12-10T15:38:35.514816: step 4437, loss 0.190344, acc 0.9375, prec 0.0911003, recall 0.906207
2017-12-10T15:38:35.712229: step 4438, loss 0.198273, acc 0.96875, prec 0.0911381, recall 0.90625
2017-12-10T15:38:35.915445: step 4439, loss 0.26987, acc 0.921875, prec 0.0911275, recall 0.90625
2017-12-10T15:38:36.114127: step 4440, loss 0.295563, acc 0.9375, prec 0.0911401, recall 0.906272
2017-12-10T15:38:36.313920: step 4441, loss 0.294532, acc 0.890625, prec 0.0911464, recall 0.906293
2017-12-10T15:38:36.507801: step 4442, loss 0.122164, acc 0.9375, prec 0.091138, recall 0.906293
2017-12-10T15:38:36.708651: step 4443, loss 0.0472144, acc 0.96875, prec 0.0911337, recall 0.906293
2017-12-10T15:38:36.906745: step 4444, loss 0.20999, acc 0.96875, prec 0.0911295, recall 0.906293
2017-12-10T15:38:37.108664: step 4445, loss 0.376534, acc 0.9375, prec 0.0911421, recall 0.906315
2017-12-10T15:38:37.307090: step 4446, loss 0.265613, acc 0.9375, prec 0.0911757, recall 0.906358
2017-12-10T15:38:37.505872: step 4447, loss 0.564004, acc 0.90625, prec 0.091184, recall 0.906379
2017-12-10T15:38:37.701390: step 4448, loss 0.125856, acc 0.953125, prec 0.0911777, recall 0.906379
2017-12-10T15:38:37.896587: step 4449, loss 0.0294864, acc 0.984375, prec 0.0912175, recall 0.906422
2017-12-10T15:38:38.097516: step 4450, loss 0.217483, acc 0.96875, prec 0.0912133, recall 0.906422
2017-12-10T15:38:38.297861: step 4451, loss 0.0887835, acc 0.984375, prec 0.0912322, recall 0.906443
2017-12-10T15:38:38.494464: step 4452, loss 0.352658, acc 0.984375, prec 0.091293, recall 0.906508
2017-12-10T15:38:38.704037: step 4453, loss 0.369594, acc 0.953125, prec 0.0912867, recall 0.906508
2017-12-10T15:38:38.904182: step 4454, loss 0.0419884, acc 0.984375, prec 0.0912846, recall 0.906508
2017-12-10T15:38:39.100869: step 4455, loss 0.137217, acc 0.984375, prec 0.0913034, recall 0.906529
2017-12-10T15:38:39.307921: step 4456, loss 0.0199234, acc 1, prec 0.0913034, recall 0.906529
2017-12-10T15:38:39.505365: step 4457, loss 0.399195, acc 0.9375, prec 0.091295, recall 0.906529
2017-12-10T15:38:39.705781: step 4458, loss 1.36416, acc 0.953125, prec 0.0912908, recall 0.906322
2017-12-10T15:38:39.907975: step 4459, loss 0.0853935, acc 0.984375, prec 0.0912887, recall 0.906322
2017-12-10T15:38:40.115864: step 4460, loss 0.0739316, acc 0.953125, prec 0.0913033, recall 0.906343
2017-12-10T15:38:40.310991: step 4461, loss 0.0505226, acc 0.984375, prec 0.0913012, recall 0.906343
2017-12-10T15:38:40.511899: step 4462, loss 0.0456298, acc 0.984375, prec 0.0912991, recall 0.906343
2017-12-10T15:38:40.709819: step 4463, loss 0.0824436, acc 0.96875, prec 0.0913368, recall 0.906386
2017-12-10T15:38:40.911129: step 4464, loss 0.196098, acc 0.96875, prec 0.0913326, recall 0.906386
2017-12-10T15:38:41.106975: step 4465, loss 0.181308, acc 0.9375, prec 0.0913242, recall 0.906386
2017-12-10T15:38:41.309868: step 4466, loss 0.0375447, acc 0.984375, prec 0.0913221, recall 0.906386
2017-12-10T15:38:41.505852: step 4467, loss 0.108162, acc 0.953125, prec 0.0913786, recall 0.90645
2017-12-10T15:38:41.701214: step 4468, loss 0.371649, acc 0.890625, prec 0.0914267, recall 0.906514
2017-12-10T15:38:41.896686: step 4469, loss 0.253991, acc 0.9375, prec 0.0914183, recall 0.906514
2017-12-10T15:38:42.091949: step 4470, loss 0.167416, acc 0.921875, prec 0.0914287, recall 0.906536
2017-12-10T15:38:42.290024: step 4471, loss 0.0159715, acc 1, prec 0.0914496, recall 0.906557
2017-12-10T15:38:42.492066: step 4472, loss 0.115164, acc 0.96875, prec 0.0914664, recall 0.906578
2017-12-10T15:38:42.670145: step 4473, loss 0.0848741, acc 0.980769, prec 0.0914643, recall 0.906578
2017-12-10T15:38:42.875263: step 4474, loss 0.0957394, acc 0.984375, prec 0.091525, recall 0.906642
2017-12-10T15:38:43.076265: step 4475, loss 0.129057, acc 0.9375, prec 0.0915375, recall 0.906664
2017-12-10T15:38:43.269989: step 4476, loss 0.112337, acc 0.96875, prec 0.0915542, recall 0.906685
2017-12-10T15:38:43.471633: step 4477, loss 0.120122, acc 0.953125, prec 0.0916106, recall 0.906749
2017-12-10T15:38:43.669798: step 4478, loss 0.202033, acc 0.9375, prec 0.0916022, recall 0.906749
2017-12-10T15:38:43.865462: step 4479, loss 0.0861021, acc 0.96875, prec 0.0916189, recall 0.90677
2017-12-10T15:38:44.065654: step 4480, loss 0.179144, acc 0.921875, prec 0.0916292, recall 0.906791
2017-12-10T15:38:44.277078: step 4481, loss 0.0871582, acc 0.96875, prec 0.0916459, recall 0.906812
2017-12-10T15:38:44.477293: step 4482, loss 0.0966291, acc 0.984375, prec 0.0916647, recall 0.906834
2017-12-10T15:38:44.675344: step 4483, loss 0.59254, acc 0.96875, prec 0.0917023, recall 0.906876
2017-12-10T15:38:44.877847: step 4484, loss 0.184997, acc 0.984375, prec 0.0917212, recall 0.906897
2017-12-10T15:38:45.079483: step 4485, loss 0.108086, acc 0.96875, prec 0.0917588, recall 0.90694
2017-12-10T15:38:45.278807: step 4486, loss 0.157195, acc 0.953125, prec 0.0917524, recall 0.90694
2017-12-10T15:38:45.478115: step 4487, loss 0.159798, acc 0.96875, prec 0.0917482, recall 0.90694
2017-12-10T15:38:45.681292: step 4488, loss 0.080708, acc 0.953125, prec 0.0917419, recall 0.90694
2017-12-10T15:38:45.886813: step 4489, loss 0.412164, acc 0.984375, prec 0.0917815, recall 0.906982
2017-12-10T15:38:46.094860: step 4490, loss 0.199516, acc 0.96875, prec 0.0917982, recall 0.907003
2017-12-10T15:38:46.294027: step 4491, loss 0.0956028, acc 0.96875, prec 0.0918567, recall 0.907067
2017-12-10T15:38:46.500147: step 4492, loss 0.546003, acc 0.90625, prec 0.0918649, recall 0.907088
2017-12-10T15:38:46.700892: step 4493, loss 0.0142519, acc 1, prec 0.0918649, recall 0.907088
2017-12-10T15:38:46.897902: step 4494, loss 0.132414, acc 0.96875, prec 0.0918607, recall 0.907088
2017-12-10T15:38:47.101594: step 4495, loss 0.0799011, acc 0.984375, prec 0.0919003, recall 0.90713
2017-12-10T15:38:47.303011: step 4496, loss 0.101174, acc 0.984375, prec 0.0918982, recall 0.90713
2017-12-10T15:38:47.503155: step 4497, loss 0.0227964, acc 1, prec 0.09194, recall 0.907172
2017-12-10T15:38:47.700853: step 4498, loss 0.0592918, acc 0.984375, prec 0.0919379, recall 0.907172
2017-12-10T15:38:47.898579: step 4499, loss 0.14892, acc 0.953125, prec 0.0919942, recall 0.907235
2017-12-10T15:38:48.098316: step 4500, loss 0.454487, acc 0.921875, prec 0.0920254, recall 0.907277
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_2/1512937383/checkpoints/model-4500

2017-12-10T15:38:49.282930: step 4501, loss 0.337997, acc 0.96875, prec 0.0920212, recall 0.907277
2017-12-10T15:38:49.482038: step 4502, loss 0.0157571, acc 1, prec 0.0920212, recall 0.907277
2017-12-10T15:38:49.684835: step 4503, loss 0.093848, acc 0.984375, prec 0.092019, recall 0.907277
2017-12-10T15:38:49.885680: step 4504, loss 0.0502072, acc 0.96875, prec 0.0920357, recall 0.907298
2017-12-10T15:38:50.088035: step 4505, loss 0.00757203, acc 1, prec 0.0920774, recall 0.90734
2017-12-10T15:38:50.287059: step 4506, loss 0.0906422, acc 0.96875, prec 0.0920941, recall 0.907361
2017-12-10T15:38:50.484885: step 4507, loss 0.0269813, acc 0.984375, prec 0.0921128, recall 0.907382
2017-12-10T15:38:50.686541: step 4508, loss 0.337895, acc 0.9375, prec 0.0921252, recall 0.907403
2017-12-10T15:38:50.882991: step 4509, loss 0.110778, acc 0.96875, prec 0.0921627, recall 0.907445
2017-12-10T15:38:51.085894: step 4510, loss 0.015633, acc 1, prec 0.0921836, recall 0.907466
2017-12-10T15:38:51.282558: step 4511, loss 0.0844767, acc 0.984375, prec 0.0922232, recall 0.907508
2017-12-10T15:38:51.483498: step 4512, loss 0.181159, acc 0.984375, prec 0.0922628, recall 0.90755
2017-12-10T15:38:51.683134: step 4513, loss 0.0638002, acc 1, prec 0.0922837, recall 0.907571
2017-12-10T15:38:51.881694: step 4514, loss 0.0252668, acc 0.984375, prec 0.0922815, recall 0.907571
2017-12-10T15:38:52.078552: step 4515, loss 0.521862, acc 0.953125, prec 0.0923377, recall 0.907633
2017-12-10T15:38:52.275691: step 4516, loss 0.0259846, acc 0.984375, prec 0.0923356, recall 0.907633
2017-12-10T15:38:52.472003: step 4517, loss 0.0875049, acc 0.984375, prec 0.0923543, recall 0.907654
2017-12-10T15:38:52.673862: step 4518, loss 2.20258, acc 0.984375, prec 0.0923543, recall 0.907449
2017-12-10T15:38:52.879468: step 4519, loss 0.218413, acc 0.96875, prec 0.092371, recall 0.90747
2017-12-10T15:38:53.082360: step 4520, loss 0.0672864, acc 0.953125, prec 0.0923646, recall 0.90747
2017-12-10T15:38:53.283866: step 4521, loss 0.0222461, acc 1, prec 0.0923646, recall 0.90747
2017-12-10T15:38:53.481793: step 4522, loss 0.200505, acc 0.9375, prec 0.0923769, recall 0.907491
2017-12-10T15:38:53.682236: step 4523, loss 0.263798, acc 0.921875, prec 0.0923663, recall 0.907491
2017-12-10T15:38:53.891015: step 4524, loss 0.048076, acc 0.96875, prec 0.0923829, recall 0.907512
2017-12-10T15:38:54.089147: step 4525, loss 0.12921, acc 0.96875, prec 0.0923787, recall 0.907512
2017-12-10T15:38:54.290751: step 4526, loss 0.306487, acc 0.90625, prec 0.0923868, recall 0.907533
2017-12-10T15:38:54.491668: step 4527, loss 0.273483, acc 0.890625, prec 0.092372, recall 0.907533
2017-12-10T15:38:54.694575: step 4528, loss 0.261252, acc 0.921875, prec 0.0923614, recall 0.907533
2017-12-10T15:38:54.901723: step 4529, loss 0.229991, acc 0.90625, prec 0.0923487, recall 0.907533
2017-12-10T15:38:55.093252: step 4530, loss 1.00808, acc 0.9375, prec 0.092361, recall 0.907554
2017-12-10T15:38:55.292176: step 4531, loss 0.23413, acc 0.9375, prec 0.0923525, recall 0.907554
2017-12-10T15:38:55.489936: step 4532, loss 0.247365, acc 0.921875, prec 0.0923419, recall 0.907554
2017-12-10T15:38:55.686154: step 4533, loss 0.149258, acc 0.953125, prec 0.0923564, recall 0.907574
2017-12-10T15:38:55.886412: step 4534, loss 0.283641, acc 0.953125, prec 0.0924125, recall 0.907637
2017-12-10T15:38:56.088061: step 4535, loss 0.280151, acc 0.9375, prec 0.0924248, recall 0.907658
2017-12-10T15:38:56.284261: step 4536, loss 0.208628, acc 0.96875, prec 0.092483, recall 0.90772
2017-12-10T15:38:56.489981: step 4537, loss 0.175956, acc 1, prec 0.0925246, recall 0.907762
2017-12-10T15:38:56.689670: step 4538, loss 0.526335, acc 0.890625, prec 0.0925306, recall 0.907782
2017-12-10T15:38:56.891352: step 4539, loss 0.374168, acc 0.9375, prec 0.0925429, recall 0.907803
2017-12-10T15:38:57.087316: step 4540, loss 0.0802413, acc 0.96875, prec 0.0925595, recall 0.907824
2017-12-10T15:38:57.286528: step 4541, loss 0.0940187, acc 0.953125, prec 0.0925531, recall 0.907824
2017-12-10T15:38:57.484080: step 4542, loss 0.157067, acc 0.96875, prec 0.0925697, recall 0.907844
2017-12-10T15:38:57.681799: step 4543, loss 0.300724, acc 0.90625, prec 0.0925777, recall 0.907865
2017-12-10T15:38:57.876517: step 4544, loss 0.131654, acc 0.9375, prec 0.0925693, recall 0.907865
2017-12-10T15:38:58.073897: step 4545, loss 0.140752, acc 0.953125, prec 0.0926045, recall 0.907907
2017-12-10T15:38:58.271988: step 4546, loss 0.233712, acc 0.921875, prec 0.0925939, recall 0.907907
2017-12-10T15:38:58.469395: step 4547, loss 0.0288222, acc 0.984375, prec 0.0926125, recall 0.907927
2017-12-10T15:38:58.665368: step 4548, loss 0.342911, acc 0.953125, prec 0.092627, recall 0.907948
2017-12-10T15:38:58.868605: step 4549, loss 0.0421637, acc 1, prec 0.0926477, recall 0.907969
2017-12-10T15:38:59.069578: step 4550, loss 0.0954151, acc 0.984375, prec 0.0926664, recall 0.907989
2017-12-10T15:38:59.271350: step 4551, loss 0.044783, acc 0.96875, prec 0.0926621, recall 0.907989
2017-12-10T15:38:59.475466: step 4552, loss 1.3606, acc 0.9375, prec 0.0926558, recall 0.907786
2017-12-10T15:38:59.677059: step 4553, loss 0.055081, acc 0.953125, prec 0.0926702, recall 0.907806
2017-12-10T15:38:59.874624: step 4554, loss 0.125381, acc 0.953125, prec 0.0926638, recall 0.907806
2017-12-10T15:39:00.076460: step 4555, loss 0.188455, acc 0.9375, prec 0.0926761, recall 0.907827
2017-12-10T15:39:00.273939: step 4556, loss 0.108587, acc 0.984375, prec 0.0926948, recall 0.907848
2017-12-10T15:39:00.472495: step 4557, loss 0.35133, acc 0.921875, prec 0.0927465, recall 0.907909
2017-12-10T15:39:00.668031: step 4558, loss 1.60083, acc 0.9375, prec 0.0927795, recall 0.907951
2017-12-10T15:39:00.871837: step 4559, loss 0.190293, acc 0.953125, prec 0.0927939, recall 0.907971
2017-12-10T15:39:01.074702: step 4560, loss 0.394824, acc 0.953125, prec 0.0928498, recall 0.908033
2017-12-10T15:39:01.280458: step 4561, loss 0.500727, acc 0.859375, prec 0.0928514, recall 0.908054
2017-12-10T15:39:01.483987: step 4562, loss 0.51907, acc 0.859375, prec 0.0928945, recall 0.908115
2017-12-10T15:39:01.683104: step 4563, loss 0.629696, acc 0.828125, prec 0.0928919, recall 0.908136
2017-12-10T15:39:01.877248: step 4564, loss 0.551696, acc 0.859375, prec 0.0928728, recall 0.908136
2017-12-10T15:39:02.072968: step 4565, loss 0.60243, acc 0.828125, prec 0.0928909, recall 0.908177
2017-12-10T15:39:02.269582: step 4566, loss 0.29197, acc 0.890625, prec 0.0928968, recall 0.908197
2017-12-10T15:39:02.465519: step 4567, loss 0.72636, acc 0.765625, prec 0.0928857, recall 0.908218
2017-12-10T15:39:02.665098: step 4568, loss 0.581537, acc 0.890625, prec 0.092933, recall 0.908279
2017-12-10T15:39:02.859324: step 4569, loss 0.600322, acc 0.84375, prec 0.0929532, recall 0.90832
2017-12-10T15:39:03.058219: step 4570, loss 0.481543, acc 0.8125, prec 0.0929484, recall 0.908341
2017-12-10T15:39:03.257628: step 4571, loss 0.643097, acc 0.8125, prec 0.0929437, recall 0.908361
2017-12-10T15:39:03.452493: step 4572, loss 0.352213, acc 0.890625, prec 0.0929288, recall 0.908361
2017-12-10T15:39:03.646998: step 4573, loss 0.317465, acc 0.921875, prec 0.0929389, recall 0.908382
2017-12-10T15:39:03.847518: step 4574, loss 0.212213, acc 0.90625, prec 0.0929469, recall 0.908402
2017-12-10T15:39:04.043396: step 4575, loss 0.631525, acc 0.890625, prec 0.0929734, recall 0.908443
2017-12-10T15:39:04.242388: step 4576, loss 0.45268, acc 0.875, prec 0.0929565, recall 0.908443
2017-12-10T15:39:04.440078: step 4577, loss 0.0633067, acc 0.984375, prec 0.0930164, recall 0.908504
2017-12-10T15:39:04.644192: step 4578, loss 0.0612272, acc 0.984375, prec 0.0930556, recall 0.908545
2017-12-10T15:39:04.842826: step 4579, loss 0.225957, acc 0.921875, prec 0.0930656, recall 0.908565
2017-12-10T15:39:05.041696: step 4580, loss 0.378268, acc 0.90625, prec 0.0930736, recall 0.908585
2017-12-10T15:39:05.247930: step 4581, loss 0.158264, acc 0.921875, prec 0.0931043, recall 0.908626
2017-12-10T15:39:05.448849: step 4582, loss 0.0686713, acc 0.96875, prec 0.0931207, recall 0.908646
2017-12-10T15:39:05.651180: step 4583, loss 0.426563, acc 0.90625, prec 0.093108, recall 0.908646
2017-12-10T15:39:05.849671: step 4584, loss 0.130444, acc 1, prec 0.0931287, recall 0.908667
2017-12-10T15:39:06.048641: step 4585, loss 0.492844, acc 0.9375, prec 0.0931615, recall 0.908707
2017-12-10T15:39:06.251370: step 4586, loss 0.164536, acc 0.953125, prec 0.0931964, recall 0.908748
2017-12-10T15:39:06.452499: step 4587, loss 0.0452343, acc 0.984375, prec 0.0932149, recall 0.908768
2017-12-10T15:39:06.661616: step 4588, loss 0.0617347, acc 0.984375, prec 0.0932335, recall 0.908788
2017-12-10T15:39:06.864074: step 4589, loss 0.761961, acc 0.96875, prec 0.0932499, recall 0.908809
2017-12-10T15:39:07.061983: step 4590, loss 0.00690764, acc 1, prec 0.0932499, recall 0.908809
2017-12-10T15:39:07.262146: step 4591, loss 0.00777316, acc 1, prec 0.0932499, recall 0.908809
2017-12-10T15:39:07.462807: step 4592, loss 0.0300866, acc 0.984375, prec 0.0932684, recall 0.908829
2017-12-10T15:39:07.662529: step 4593, loss 0.085915, acc 0.984375, prec 0.0932663, recall 0.908829
2017-12-10T15:39:07.859569: step 4594, loss 0.0104288, acc 1, prec 0.0932663, recall 0.908829
2017-12-10T15:39:08.061024: step 4595, loss 0.053213, acc 1, prec 0.0933075, recall 0.908869
2017-12-10T15:39:08.258691: step 4596, loss 0.142012, acc 0.921875, prec 0.0932969, recall 0.908869
2017-12-10T15:39:08.459499: step 4597, loss 0.0459497, acc 0.96875, prec 0.0932927, recall 0.908869
2017-12-10T15:39:08.657871: step 4598, loss 0.0425376, acc 0.984375, prec 0.0932905, recall 0.908869
2017-12-10T15:39:08.855566: step 4599, loss 0.0289107, acc 1, prec 0.0932905, recall 0.908869
2017-12-10T15:39:09.060230: step 4600, loss 0.26842, acc 0.953125, prec 0.0933048, recall 0.908889
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_2/1512937383/checkpoints/model-4600

2017-12-10T15:39:10.436232: step 4601, loss 0.102531, acc 0.984375, prec 0.0933027, recall 0.908889
2017-12-10T15:39:10.632636: step 4602, loss 0.0925288, acc 0.984375, prec 0.0933006, recall 0.908889
2017-12-10T15:39:10.837955: step 4603, loss 0.189853, acc 0.96875, prec 0.0933376, recall 0.90893
2017-12-10T15:39:11.040624: step 4604, loss 0.0752905, acc 0.984375, prec 0.0933561, recall 0.90895
2017-12-10T15:39:11.240088: step 4605, loss 0.24771, acc 1, prec 0.0933767, recall 0.90897
2017-12-10T15:39:11.440548: step 4606, loss 0.100418, acc 0.953125, prec 0.0933703, recall 0.90897
2017-12-10T15:39:11.640515: step 4607, loss 0.239907, acc 0.953125, prec 0.0934052, recall 0.90901
2017-12-10T15:39:11.844119: step 4608, loss 0.173073, acc 0.984375, prec 0.0934237, recall 0.909031
2017-12-10T15:39:12.045735: step 4609, loss 0.106494, acc 0.984375, prec 0.0934216, recall 0.909031
2017-12-10T15:39:12.245919: step 4610, loss 0.0871021, acc 0.984375, prec 0.0934195, recall 0.909031
2017-12-10T15:39:12.446485: step 4611, loss 0.00244727, acc 1, prec 0.0934401, recall 0.909051
2017-12-10T15:39:12.644019: step 4612, loss 0.00655881, acc 1, prec 0.0934401, recall 0.909051
2017-12-10T15:39:12.841858: step 4613, loss 0.0253877, acc 0.984375, prec 0.093438, recall 0.909051
2017-12-10T15:39:13.043506: step 4614, loss 0.693833, acc 0.9375, prec 0.0934913, recall 0.909111
2017-12-10T15:39:13.244792: step 4615, loss 2.39754, acc 0.984375, prec 0.0934913, recall 0.90891
2017-12-10T15:39:13.444778: step 4616, loss 0.0841638, acc 0.96875, prec 0.0934871, recall 0.90891
2017-12-10T15:39:13.642577: step 4617, loss 0.132404, acc 0.96875, prec 0.0934828, recall 0.90891
2017-12-10T15:39:13.839976: step 4618, loss 0.0627248, acc 0.984375, prec 0.0935013, recall 0.90893
2017-12-10T15:39:14.038717: step 4619, loss 0.102586, acc 0.96875, prec 0.093497, recall 0.90893
2017-12-10T15:39:14.237325: step 4620, loss 0.359256, acc 0.90625, prec 0.0934843, recall 0.90893
2017-12-10T15:39:14.439690: step 4621, loss 0.461403, acc 0.921875, prec 0.0934737, recall 0.90893
2017-12-10T15:39:14.633961: step 4622, loss 0.418322, acc 0.90625, prec 0.0934815, recall 0.90895
2017-12-10T15:39:14.835388: step 4623, loss 0.0426147, acc 0.984375, prec 0.0934794, recall 0.90895
2017-12-10T15:39:15.035837: step 4624, loss 0.407769, acc 0.875, prec 0.0934624, recall 0.90895
2017-12-10T15:39:15.237646: step 4625, loss 0.112639, acc 0.953125, prec 0.093456, recall 0.90895
2017-12-10T15:39:15.438896: step 4626, loss 0.22806, acc 0.921875, prec 0.093466, recall 0.90897
2017-12-10T15:39:15.638756: step 4627, loss 0.0540546, acc 0.96875, prec 0.0934618, recall 0.90897
2017-12-10T15:39:15.843780: step 4628, loss 0.254075, acc 0.921875, prec 0.0934923, recall 0.909011
2017-12-10T15:39:16.045814: step 4629, loss 0.39626, acc 0.90625, prec 0.0934796, recall 0.909011
2017-12-10T15:39:16.239355: step 4630, loss 0.218702, acc 0.953125, prec 0.0934938, recall 0.909031
2017-12-10T15:39:16.436816: step 4631, loss 0.114218, acc 0.9375, prec 0.0934853, recall 0.909031
2017-12-10T15:39:16.635216: step 4632, loss 0.0523222, acc 0.984375, prec 0.0935038, recall 0.909051
2017-12-10T15:39:16.832194: step 4633, loss 0.138237, acc 0.953125, prec 0.0934974, recall 0.909051
2017-12-10T15:39:17.035467: step 4634, loss 0.232813, acc 0.9375, prec 0.0934889, recall 0.909051
2017-12-10T15:39:17.234385: step 4635, loss 0.109733, acc 0.953125, prec 0.0935237, recall 0.909091
2017-12-10T15:39:17.435589: step 4636, loss 0.0750045, acc 0.96875, prec 0.0935606, recall 0.909131
2017-12-10T15:39:17.632607: step 4637, loss 0.022389, acc 0.984375, prec 0.0935996, recall 0.909171
2017-12-10T15:39:17.830503: step 4638, loss 0.10158, acc 0.9375, prec 0.0936323, recall 0.909211
2017-12-10T15:39:18.030357: step 4639, loss 0.331401, acc 0.953125, prec 0.0936259, recall 0.909211
2017-12-10T15:39:18.228838: step 4640, loss 0.0845776, acc 0.96875, prec 0.0936628, recall 0.909251
2017-12-10T15:39:18.426865: step 4641, loss 0.0755038, acc 0.96875, prec 0.0936585, recall 0.909251
2017-12-10T15:39:18.627085: step 4642, loss 2.22982, acc 0.984375, prec 0.0936997, recall 0.909091
2017-12-10T15:39:18.835243: step 4643, loss 0.00768856, acc 1, prec 0.0937613, recall 0.909151
2017-12-10T15:39:19.031449: step 4644, loss 0.33688, acc 0.9375, prec 0.093794, recall 0.909191
2017-12-10T15:39:19.236740: step 4645, loss 0.134021, acc 0.96875, prec 0.0938102, recall 0.909211
2017-12-10T15:39:19.433959: step 4646, loss 0.0844773, acc 0.96875, prec 0.0938265, recall 0.909231
2017-12-10T15:39:19.630246: step 4647, loss 0.208082, acc 0.9375, prec 0.0938386, recall 0.909251
2017-12-10T15:39:19.827970: step 4648, loss 0.198263, acc 0.953125, prec 0.0938322, recall 0.909251
2017-12-10T15:39:20.028517: step 4649, loss 0.0377133, acc 0.984375, prec 0.0938712, recall 0.909291
2017-12-10T15:39:20.227655: step 4650, loss 0.422787, acc 0.90625, prec 0.0938995, recall 0.90933
2017-12-10T15:39:20.428147: step 4651, loss 0.170077, acc 0.9375, prec 0.093932, recall 0.90937
2017-12-10T15:39:20.622833: step 4652, loss 0.139934, acc 0.9375, prec 0.0939441, recall 0.90939
2017-12-10T15:39:20.823417: step 4653, loss 0.245986, acc 0.921875, prec 0.093954, recall 0.90941
2017-12-10T15:39:21.024514: step 4654, loss 0.339267, acc 0.953125, prec 0.0939476, recall 0.90941
2017-12-10T15:39:21.221772: step 4655, loss 0.0465314, acc 0.984375, prec 0.0939454, recall 0.90941
2017-12-10T15:39:21.421061: step 4656, loss 0.0606394, acc 0.96875, prec 0.0939617, recall 0.90943
2017-12-10T15:39:21.625567: step 4657, loss 0.0475246, acc 1, prec 0.0939822, recall 0.90945
2017-12-10T15:39:21.823822: step 4658, loss 0.222945, acc 0.953125, prec 0.0939758, recall 0.90945
2017-12-10T15:39:22.025849: step 4659, loss 0.205515, acc 0.9375, prec 0.0939879, recall 0.90947
2017-12-10T15:39:22.227313: step 4660, loss 0.0591551, acc 0.96875, prec 0.0939836, recall 0.90947
2017-12-10T15:39:22.427626: step 4661, loss 0.176863, acc 0.96875, prec 0.0939793, recall 0.90947
2017-12-10T15:39:22.620145: step 4662, loss 0.124404, acc 0.953125, prec 0.0940345, recall 0.909529
2017-12-10T15:39:22.819368: step 4663, loss 0.219995, acc 0.90625, prec 0.0940628, recall 0.909569
2017-12-10T15:39:23.015942: step 4664, loss 0.121099, acc 0.96875, prec 0.0940585, recall 0.909569
2017-12-10T15:39:23.221555: step 4665, loss 0.0321212, acc 0.984375, prec 0.0940769, recall 0.909588
2017-12-10T15:39:23.415886: step 4666, loss 0.51689, acc 0.875, prec 0.0940599, recall 0.909588
2017-12-10T15:39:23.616578: step 4667, loss 0.0472547, acc 0.984375, prec 0.0940782, recall 0.909608
2017-12-10T15:39:23.819259: step 4668, loss 0.770307, acc 0.953125, prec 0.0940923, recall 0.909628
2017-12-10T15:39:24.021731: step 4669, loss 0.116351, acc 0.953125, prec 0.0941065, recall 0.909648
2017-12-10T15:39:24.216782: step 4670, loss 0.0513775, acc 0.984375, prec 0.0941043, recall 0.909648
2017-12-10T15:39:24.413391: step 4671, loss 0.0375596, acc 0.984375, prec 0.0941432, recall 0.909687
2017-12-10T15:39:24.612089: step 4672, loss 0.0624816, acc 0.96875, prec 0.0942004, recall 0.909747
2017-12-10T15:39:24.809842: step 4673, loss 0.0686507, acc 0.96875, prec 0.0942167, recall 0.909766
2017-12-10T15:39:25.011166: step 4674, loss 0.0188162, acc 0.984375, prec 0.0942145, recall 0.909766
2017-12-10T15:39:25.204949: step 4675, loss 0.00301721, acc 1, prec 0.0942145, recall 0.909766
2017-12-10T15:39:25.402088: step 4676, loss 0.119688, acc 0.953125, prec 0.0942286, recall 0.909786
2017-12-10T15:39:25.601128: step 4677, loss 0.0588332, acc 0.984375, prec 0.0942265, recall 0.909786
2017-12-10T15:39:25.801945: step 4678, loss 0.422008, acc 0.90625, prec 0.0942547, recall 0.909825
2017-12-10T15:39:26.000057: step 4679, loss 0.0585609, acc 0.96875, prec 0.0942709, recall 0.909845
2017-12-10T15:39:26.197753: step 4680, loss 0.913315, acc 0.96875, prec 0.0943076, recall 0.909884
2017-12-10T15:39:26.401051: step 4681, loss 0.0561596, acc 0.96875, prec 0.0943034, recall 0.909884
2017-12-10T15:39:26.601434: step 4682, loss 0.0372789, acc 0.984375, prec 0.0943217, recall 0.909904
2017-12-10T15:39:26.797299: step 4683, loss 0.11743, acc 0.953125, prec 0.0943153, recall 0.909904
2017-12-10T15:39:27.003095: step 4684, loss 0.108759, acc 0.984375, prec 0.0943336, recall 0.909924
2017-12-10T15:39:27.203056: step 4685, loss 0.0881005, acc 0.96875, prec 0.0943703, recall 0.909963
2017-12-10T15:39:27.409219: step 4686, loss 0.0966108, acc 0.96875, prec 0.0943661, recall 0.909963
2017-12-10T15:39:27.607177: step 4687, loss 0.14378, acc 0.9375, prec 0.0943985, recall 0.910002
2017-12-10T15:39:27.805023: step 4688, loss 0.0690464, acc 0.96875, prec 0.0944352, recall 0.910041
2017-12-10T15:39:27.999215: step 4689, loss 0.115434, acc 0.953125, prec 0.0944287, recall 0.910041
2017-12-10T15:39:28.200539: step 4690, loss 0.20386, acc 0.953125, prec 0.0944223, recall 0.910041
2017-12-10T15:39:28.397848: step 4691, loss 0.0301424, acc 0.984375, prec 0.0944202, recall 0.910041
2017-12-10T15:39:28.597430: step 4692, loss 0.700005, acc 0.953125, prec 0.0944752, recall 0.9101
2017-12-10T15:39:28.796497: step 4693, loss 0.176046, acc 0.96875, prec 0.0944914, recall 0.91012
2017-12-10T15:39:28.999052: step 4694, loss 0.149466, acc 0.984375, prec 0.0945302, recall 0.910159
2017-12-10T15:39:29.200797: step 4695, loss 0.326273, acc 0.9375, prec 0.0945216, recall 0.910159
2017-12-10T15:39:29.403797: step 4696, loss 0.268833, acc 0.953125, prec 0.0945357, recall 0.910178
2017-12-10T15:39:29.600398: step 4697, loss 0.108547, acc 0.953125, prec 0.0945293, recall 0.910178
2017-12-10T15:39:29.801219: step 4698, loss 0.076467, acc 0.984375, prec 0.0945476, recall 0.910198
2017-12-10T15:39:30.007122: step 4699, loss 0.0666031, acc 0.96875, prec 0.0945433, recall 0.910198
2017-12-10T15:39:30.206769: step 4700, loss 0.279754, acc 0.984375, prec 0.0945616, recall 0.910217
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_2/1512937383/checkpoints/model-4700

2017-12-10T15:39:31.417802: step 4701, loss 0.0499594, acc 0.984375, prec 0.0946413, recall 0.910295
2017-12-10T15:39:31.616621: step 4702, loss 0.0295735, acc 0.984375, prec 0.0946391, recall 0.910295
2017-12-10T15:39:31.814437: step 4703, loss 0.0771971, acc 0.984375, prec 0.0946575, recall 0.910315
2017-12-10T15:39:32.014526: step 4704, loss 0.139716, acc 0.953125, prec 0.0946715, recall 0.910334
2017-12-10T15:39:32.215649: step 4705, loss 0.25864, acc 0.9375, prec 0.0946629, recall 0.910334
2017-12-10T15:39:32.412380: step 4706, loss 0.149083, acc 0.953125, prec 0.0947178, recall 0.910393
2017-12-10T15:39:32.612644: step 4707, loss 0.171205, acc 0.9375, prec 0.0947501, recall 0.910432
2017-12-10T15:39:32.811039: step 4708, loss 0.486809, acc 0.9375, prec 0.0947824, recall 0.91047
2017-12-10T15:39:33.014329: step 4709, loss 0.044494, acc 0.984375, prec 0.0948007, recall 0.91049
2017-12-10T15:39:33.218178: step 4710, loss 0.0984612, acc 0.953125, prec 0.0948147, recall 0.910509
2017-12-10T15:39:33.419663: step 4711, loss 0.0129608, acc 1, prec 0.0948147, recall 0.910509
2017-12-10T15:39:33.614696: step 4712, loss 0.111584, acc 0.984375, prec 0.0948126, recall 0.910509
2017-12-10T15:39:33.815957: step 4713, loss 0.0412483, acc 1, prec 0.0948535, recall 0.910548
2017-12-10T15:39:34.014737: step 4714, loss 0.0163261, acc 0.984375, prec 0.0948513, recall 0.910548
2017-12-10T15:39:34.218770: step 4715, loss 0.0538823, acc 1, prec 0.0948717, recall 0.910567
2017-12-10T15:39:34.421361: step 4716, loss 0.0968999, acc 0.984375, prec 0.0948696, recall 0.910567
2017-12-10T15:39:34.617137: step 4717, loss 0.0253852, acc 0.984375, prec 0.0948675, recall 0.910567
2017-12-10T15:39:34.817323: step 4718, loss 0.226573, acc 0.953125, prec 0.094861, recall 0.910567
2017-12-10T15:39:35.027403: step 4719, loss 0.0397286, acc 0.96875, prec 0.0948772, recall 0.910587
2017-12-10T15:39:35.226678: step 4720, loss 0.0578152, acc 0.984375, prec 0.0948955, recall 0.910606
2017-12-10T15:39:35.430914: step 4721, loss 0.085426, acc 0.953125, prec 0.0949299, recall 0.910645
2017-12-10T15:39:35.630639: step 4722, loss 0.06999, acc 0.96875, prec 0.0949256, recall 0.910645
2017-12-10T15:39:35.828039: step 4723, loss 0.186294, acc 0.953125, prec 0.0949396, recall 0.910664
2017-12-10T15:39:36.030074: step 4724, loss 0.0823456, acc 1, prec 0.09496, recall 0.910683
2017-12-10T15:39:36.240355: step 4725, loss 0.0171049, acc 0.984375, prec 0.0949578, recall 0.910683
2017-12-10T15:39:36.435786: step 4726, loss 0.0253455, acc 0.984375, prec 0.0949761, recall 0.910703
2017-12-10T15:39:36.634397: step 4727, loss 0.845808, acc 0.984375, prec 0.0950148, recall 0.910741
2017-12-10T15:39:36.837600: step 4728, loss 0.0915573, acc 0.984375, prec 0.095033, recall 0.910761
2017-12-10T15:39:37.040260: step 4729, loss 0.118291, acc 1, prec 0.0950534, recall 0.91078
2017-12-10T15:39:37.249020: step 4730, loss 0.0831365, acc 0.96875, prec 0.0950492, recall 0.91078
2017-12-10T15:39:37.449282: step 4731, loss 0.0810976, acc 0.953125, prec 0.0950427, recall 0.91078
2017-12-10T15:39:37.650315: step 4732, loss 0.0175603, acc 1, prec 0.0950631, recall 0.910799
2017-12-10T15:39:37.848381: step 4733, loss 0.15182, acc 0.953125, prec 0.0950771, recall 0.910818
2017-12-10T15:39:38.046987: step 4734, loss 0.172481, acc 0.984375, prec 0.0950749, recall 0.910818
2017-12-10T15:39:38.254687: step 4735, loss 0.1375, acc 0.953125, prec 0.0950685, recall 0.910818
2017-12-10T15:39:38.458451: step 4736, loss 0.255817, acc 1, prec 0.0950889, recall 0.910838
2017-12-10T15:39:38.661702: step 4737, loss 0.0150663, acc 1, prec 0.0951093, recall 0.910857
2017-12-10T15:39:38.861423: step 4738, loss 0.0392848, acc 0.984375, prec 0.0951276, recall 0.910876
2017-12-10T15:39:39.065349: step 4739, loss 0.183915, acc 0.96875, prec 0.0951641, recall 0.910915
2017-12-10T15:39:39.264516: step 4740, loss 0.060299, acc 0.984375, prec 0.0951619, recall 0.910915
2017-12-10T15:39:39.469920: step 4741, loss 2.56852, acc 0.9375, prec 0.0951962, recall 0.910757
2017-12-10T15:39:39.676868: step 4742, loss 0.0931369, acc 0.953125, prec 0.0951898, recall 0.910757
2017-12-10T15:39:39.874822: step 4743, loss 0.150837, acc 0.953125, prec 0.0952038, recall 0.910776
2017-12-10T15:39:40.072938: step 4744, loss 0.144294, acc 0.9375, prec 0.0951952, recall 0.910776
2017-12-10T15:39:40.278379: step 4745, loss 0.112301, acc 0.9375, prec 0.0952274, recall 0.910814
2017-12-10T15:39:40.474429: step 4746, loss 0.0854856, acc 0.953125, prec 0.0952413, recall 0.910834
2017-12-10T15:39:40.669667: step 4747, loss 0.0958439, acc 0.953125, prec 0.0952553, recall 0.910853
2017-12-10T15:39:40.867049: step 4748, loss 0.130373, acc 0.96875, prec 0.0952713, recall 0.910872
2017-12-10T15:39:41.065021: step 4749, loss 0.325636, acc 0.9375, prec 0.0952628, recall 0.910872
2017-12-10T15:39:41.261785: step 4750, loss 0.190676, acc 0.9375, prec 0.0952745, recall 0.910891
2017-12-10T15:39:41.457991: step 4751, loss 0.0453673, acc 0.984375, prec 0.0952928, recall 0.91091
2017-12-10T15:39:41.655038: step 4752, loss 0.157812, acc 0.921875, prec 0.095282, recall 0.91091
2017-12-10T15:39:41.852109: step 4753, loss 0.260513, acc 0.953125, prec 0.0952756, recall 0.91091
2017-12-10T15:39:42.050290: step 4754, loss 0.316672, acc 0.9375, prec 0.0953078, recall 0.910949
2017-12-10T15:39:42.249488: step 4755, loss 0.441829, acc 0.890625, prec 0.0953538, recall 0.911006
2017-12-10T15:39:42.452489: step 4756, loss 0.127593, acc 0.953125, prec 0.0953474, recall 0.911006
2017-12-10T15:39:42.654346: step 4757, loss 0.0840919, acc 0.96875, prec 0.0953431, recall 0.911006
2017-12-10T15:39:42.852199: step 4758, loss 0.159063, acc 0.9375, prec 0.0953345, recall 0.911006
2017-12-10T15:39:43.055236: step 4759, loss 0.215299, acc 0.9375, prec 0.0953463, recall 0.911025
2017-12-10T15:39:43.253199: step 4760, loss 0.0849225, acc 0.96875, prec 0.0953623, recall 0.911044
2017-12-10T15:39:43.457621: step 4761, loss 0.23453, acc 0.96875, prec 0.0953784, recall 0.911063
2017-12-10T15:39:43.657124: step 4762, loss 0.0156137, acc 1, prec 0.0953784, recall 0.911063
2017-12-10T15:39:43.853272: step 4763, loss 0.0940749, acc 0.953125, prec 0.095372, recall 0.911063
2017-12-10T15:39:44.053910: step 4764, loss 0.264155, acc 0.96875, prec 0.0954084, recall 0.911102
2017-12-10T15:39:44.255011: step 4765, loss 0.420118, acc 0.921875, prec 0.095418, recall 0.911121
2017-12-10T15:39:44.456272: step 4766, loss 0.0286771, acc 1, prec 0.0954383, recall 0.91114
2017-12-10T15:39:44.658805: step 4767, loss 0.690737, acc 0.9375, prec 0.0954907, recall 0.911197
2017-12-10T15:39:44.859767: step 4768, loss 0.0308898, acc 0.984375, prec 0.0955089, recall 0.911216
2017-12-10T15:39:45.058638: step 4769, loss 0.240606, acc 0.9375, prec 0.095541, recall 0.911254
2017-12-10T15:39:45.255353: step 4770, loss 0.150709, acc 0.96875, prec 0.0955367, recall 0.911254
2017-12-10T15:39:45.461169: step 4771, loss 0.0329223, acc 0.984375, prec 0.0955955, recall 0.911311
2017-12-10T15:39:45.661701: step 4772, loss 0.0310484, acc 1, prec 0.0956158, recall 0.91133
2017-12-10T15:39:45.858894: step 4773, loss 0.111496, acc 1, prec 0.0956565, recall 0.911368
2017-12-10T15:39:46.063221: step 4774, loss 0.173434, acc 0.9375, prec 0.0956682, recall 0.911387
2017-12-10T15:39:46.267659: step 4775, loss 0.0767843, acc 0.953125, prec 0.0956617, recall 0.911387
2017-12-10T15:39:46.468955: step 4776, loss 0.0936918, acc 0.96875, prec 0.0957184, recall 0.911444
2017-12-10T15:39:46.664453: step 4777, loss 0.193215, acc 0.96875, prec 0.0957141, recall 0.911444
2017-12-10T15:39:46.865455: step 4778, loss 0.452389, acc 0.921875, prec 0.095744, recall 0.911482
2017-12-10T15:39:47.067459: step 4779, loss 0.169984, acc 0.96875, prec 0.0957397, recall 0.911482
2017-12-10T15:39:47.271273: step 4780, loss 0.128389, acc 0.953125, prec 0.0957332, recall 0.911482
2017-12-10T15:39:47.471111: step 4781, loss 0.136415, acc 0.96875, prec 0.0957695, recall 0.91152
2017-12-10T15:39:47.673502: step 4782, loss 0.0902021, acc 0.953125, prec 0.0957631, recall 0.91152
2017-12-10T15:39:47.878169: step 4783, loss 0.307443, acc 0.9375, prec 0.0957545, recall 0.91152
2017-12-10T15:39:48.075767: step 4784, loss 0.100963, acc 0.953125, prec 0.0957683, recall 0.911538
2017-12-10T15:39:48.276355: step 4785, loss 0.106525, acc 0.953125, prec 0.0957619, recall 0.911538
2017-12-10T15:39:48.479473: step 4786, loss 0.240399, acc 0.953125, prec 0.0957554, recall 0.911538
2017-12-10T15:39:48.676627: step 4787, loss 0.0186445, acc 1, prec 0.0957757, recall 0.911557
2017-12-10T15:39:48.876957: step 4788, loss 0.0263666, acc 0.984375, prec 0.0957736, recall 0.911557
2017-12-10T15:39:49.074187: step 4789, loss 0.842812, acc 0.984375, prec 0.095812, recall 0.911595
2017-12-10T15:39:49.279763: step 4790, loss 0.644439, acc 0.953125, prec 0.0958664, recall 0.911652
2017-12-10T15:39:49.482178: step 4791, loss 0.168637, acc 0.96875, prec 0.0958824, recall 0.911671
2017-12-10T15:39:49.680047: step 4792, loss 0.191293, acc 0.96875, prec 0.0958984, recall 0.911689
2017-12-10T15:39:49.880617: step 4793, loss 0.0454712, acc 0.984375, prec 0.0958963, recall 0.911689
2017-12-10T15:39:50.084024: step 4794, loss 0.194743, acc 0.984375, prec 0.0959144, recall 0.911708
2017-12-10T15:39:50.281556: step 4795, loss 0.0935806, acc 0.984375, prec 0.0959122, recall 0.911708
2017-12-10T15:39:50.484148: step 4796, loss 0.0907174, acc 0.984375, prec 0.0959101, recall 0.911708
2017-12-10T15:39:50.684839: step 4797, loss 0.0367721, acc 0.984375, prec 0.0959282, recall 0.911727
2017-12-10T15:39:50.881559: step 4798, loss 0.250427, acc 0.90625, prec 0.0959153, recall 0.911727
2017-12-10T15:39:51.081522: step 4799, loss 0.327702, acc 0.921875, prec 0.0959654, recall 0.911784
2017-12-10T15:39:51.283717: step 4800, loss 0.311018, acc 0.953125, prec 0.0959792, recall 0.911802
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_2/1512937383/checkpoints/model-4800

2017-12-10T15:39:52.457580: step 4801, loss 0.796243, acc 0.953125, prec 0.095993, recall 0.911821
2017-12-10T15:39:52.655609: step 4802, loss 0.0419955, acc 1, prec 0.0960335, recall 0.911859
2017-12-10T15:39:52.854612: step 4803, loss 0.427821, acc 0.953125, prec 0.0960271, recall 0.911859
2017-12-10T15:39:53.049432: step 4804, loss 0.0834334, acc 0.953125, prec 0.0960206, recall 0.911859
2017-12-10T15:39:53.250682: step 4805, loss 0.913014, acc 0.96875, prec 0.0960771, recall 0.911915
2017-12-10T15:39:53.452837: step 4806, loss 0.506945, acc 0.875, prec 0.0960801, recall 0.911934
2017-12-10T15:39:53.650341: step 4807, loss 0.307268, acc 0.921875, prec 0.0960896, recall 0.911952
2017-12-10T15:39:53.851213: step 4808, loss 0.128526, acc 0.953125, prec 0.0961237, recall 0.91199
2017-12-10T15:39:54.048234: step 4809, loss 0.183385, acc 0.890625, prec 0.0961086, recall 0.91199
2017-12-10T15:39:54.252997: step 4810, loss 0.283978, acc 0.890625, prec 0.0961138, recall 0.912009
2017-12-10T15:39:54.457295: step 4811, loss 0.298632, acc 0.890625, prec 0.0960987, recall 0.912009
2017-12-10T15:39:54.655113: step 4812, loss 0.240657, acc 0.9375, prec 0.0960901, recall 0.912009
2017-12-10T15:39:54.853752: step 4813, loss 0.387974, acc 0.890625, prec 0.0960953, recall 0.912027
2017-12-10T15:39:55.056347: step 4814, loss 0.343368, acc 0.9375, prec 0.0960867, recall 0.912027
2017-12-10T15:39:55.251240: step 4815, loss 0.210286, acc 0.921875, prec 0.0960759, recall 0.912027
2017-12-10T15:39:55.451130: step 4816, loss 0.537985, acc 0.921875, prec 0.0960652, recall 0.912027
2017-12-10T15:39:55.645402: step 4817, loss 0.304499, acc 0.90625, prec 0.0960927, recall 0.912065
2017-12-10T15:39:55.841043: step 4818, loss 0.094749, acc 0.96875, prec 0.0961289, recall 0.912102
2017-12-10T15:39:56.037826: step 4819, loss 0.401261, acc 0.90625, prec 0.0961564, recall 0.912139
2017-12-10T15:39:56.236376: step 4820, loss 0.35712, acc 0.9375, prec 0.0961883, recall 0.912176
2017-12-10T15:39:56.435990: step 4821, loss 0.334509, acc 0.875, prec 0.0962115, recall 0.912214
2017-12-10T15:39:56.634029: step 4822, loss 0.339737, acc 0.890625, prec 0.0961964, recall 0.912214
2017-12-10T15:39:56.829026: step 4823, loss 0.458914, acc 0.890625, prec 0.0962016, recall 0.912232
2017-12-10T15:39:57.025740: step 4824, loss 0.490676, acc 0.90625, prec 0.0962089, recall 0.912251
2017-12-10T15:39:57.228026: step 4825, loss 0.214752, acc 0.9375, prec 0.0962003, recall 0.912251
2017-12-10T15:39:57.427998: step 4826, loss 0.357879, acc 0.9375, prec 0.0961917, recall 0.912251
2017-12-10T15:39:57.626660: step 4827, loss 0.0163897, acc 1, prec 0.0962321, recall 0.912288
2017-12-10T15:39:57.828354: step 4828, loss 0.0111774, acc 1, prec 0.0962321, recall 0.912288
2017-12-10T15:39:58.025581: step 4829, loss 0.323654, acc 0.953125, prec 0.0962256, recall 0.912288
2017-12-10T15:39:58.224004: step 4830, loss 0.161271, acc 0.96875, prec 0.0962415, recall 0.912307
2017-12-10T15:39:58.420561: step 4831, loss 0.168461, acc 0.96875, prec 0.0962372, recall 0.912307
2017-12-10T15:39:58.620972: step 4832, loss 0.0187317, acc 1, prec 0.0962776, recall 0.912344
2017-12-10T15:39:58.815692: step 4833, loss 0.109823, acc 0.96875, prec 0.0962935, recall 0.912362
2017-12-10T15:39:59.021324: step 4834, loss 0.19157, acc 0.984375, prec 0.0963317, recall 0.912399
2017-12-10T15:39:59.222322: step 4835, loss 0.0340902, acc 0.984375, prec 0.0963296, recall 0.912399
2017-12-10T15:39:59.428945: step 4836, loss 0.337617, acc 0.96875, prec 0.0963656, recall 0.912437
2017-12-10T15:39:59.633760: step 4837, loss 0.0490445, acc 0.984375, prec 0.0963635, recall 0.912437
2017-12-10T15:39:59.834713: step 4838, loss 0.0493033, acc 0.984375, prec 0.0963613, recall 0.912437
2017-12-10T15:40:00.032193: step 4839, loss 0.279161, acc 1, prec 0.0963815, recall 0.912455
2017-12-10T15:40:00.238595: step 4840, loss 0.00449727, acc 1, prec 0.0964017, recall 0.912474
2017-12-10T15:40:00.434288: step 4841, loss 0.0989024, acc 0.953125, prec 0.0963952, recall 0.912474
2017-12-10T15:40:00.631949: step 4842, loss 0.0498355, acc 0.984375, prec 0.0963931, recall 0.912474
2017-12-10T15:40:00.836354: step 4843, loss 4.62897, acc 0.953125, prec 0.0963888, recall 0.912281
2017-12-10T15:40:01.046379: step 4844, loss 1.91691, acc 0.9375, prec 0.0964025, recall 0.912107
2017-12-10T15:40:01.258043: step 4845, loss 0.00560046, acc 1, prec 0.0964227, recall 0.912125
2017-12-10T15:40:01.463468: step 4846, loss 0.26858, acc 0.9375, prec 0.0964141, recall 0.912125
2017-12-10T15:40:01.667497: step 4847, loss 0.0170925, acc 1, prec 0.0964342, recall 0.912144
2017-12-10T15:40:01.868313: step 4848, loss 0.190359, acc 0.953125, prec 0.0964278, recall 0.912144
2017-12-10T15:40:02.068392: step 4849, loss 0.225746, acc 0.90625, prec 0.0964149, recall 0.912144
2017-12-10T15:40:02.261674: step 4850, loss 0.217325, acc 0.9375, prec 0.0964264, recall 0.912162
2017-12-10T15:40:02.460475: step 4851, loss 0.397689, acc 0.890625, prec 0.0964114, recall 0.912162
2017-12-10T15:40:02.659045: step 4852, loss 0.546883, acc 0.890625, prec 0.0963963, recall 0.912162
2017-12-10T15:40:02.862045: step 4853, loss 0.593971, acc 0.84375, prec 0.0963748, recall 0.912162
2017-12-10T15:40:03.059974: step 4854, loss 0.607446, acc 0.828125, prec 0.0963511, recall 0.912162
2017-12-10T15:40:03.255265: step 4855, loss 0.233447, acc 0.90625, prec 0.0963786, recall 0.912199
2017-12-10T15:40:03.454149: step 4856, loss 0.786472, acc 0.828125, prec 0.0963549, recall 0.912199
2017-12-10T15:40:03.647533: step 4857, loss 0.534735, acc 0.84375, prec 0.0963334, recall 0.912199
2017-12-10T15:40:03.842532: step 4858, loss 0.668079, acc 0.84375, prec 0.0963321, recall 0.912218
2017-12-10T15:40:04.040504: step 4859, loss 0.422205, acc 0.890625, prec 0.0963775, recall 0.912273
2017-12-10T15:40:04.238447: step 4860, loss 0.214109, acc 0.890625, prec 0.0963625, recall 0.912273
2017-12-10T15:40:04.432604: step 4861, loss 0.116624, acc 0.953125, prec 0.0964164, recall 0.912329
2017-12-10T15:40:04.636415: step 4862, loss 0.176662, acc 0.9375, prec 0.0964078, recall 0.912329
2017-12-10T15:40:04.835254: step 4863, loss 0.301989, acc 0.90625, prec 0.0963949, recall 0.912329
2017-12-10T15:40:05.033020: step 4864, loss 0.132246, acc 0.953125, prec 0.0963885, recall 0.912329
2017-12-10T15:40:05.239841: step 4865, loss 0.177186, acc 0.9375, prec 0.0964403, recall 0.912384
2017-12-10T15:40:05.439695: step 4866, loss 0.178246, acc 0.921875, prec 0.0964496, recall 0.912403
2017-12-10T15:40:05.639755: step 4867, loss 0.426821, acc 0.921875, prec 0.096459, recall 0.912421
2017-12-10T15:40:05.835532: step 4868, loss 0.108085, acc 0.96875, prec 0.0964748, recall 0.912439
2017-12-10T15:40:06.037876: step 4869, loss 0.149258, acc 0.953125, prec 0.0964684, recall 0.912439
2017-12-10T15:40:06.237248: step 4870, loss 0.293042, acc 0.890625, prec 0.0964534, recall 0.912439
2017-12-10T15:40:06.435142: step 4871, loss 0.161907, acc 0.953125, prec 0.096467, recall 0.912458
2017-12-10T15:40:06.633872: step 4872, loss 0.0853167, acc 0.96875, prec 0.0964627, recall 0.912458
2017-12-10T15:40:06.841122: step 4873, loss 0.0278851, acc 0.984375, prec 0.0964807, recall 0.912476
2017-12-10T15:40:07.046519: step 4874, loss 0.133182, acc 0.96875, prec 0.0964764, recall 0.912476
2017-12-10T15:40:07.248362: step 4875, loss 0.841059, acc 0.953125, prec 0.0965102, recall 0.912513
2017-12-10T15:40:07.453451: step 4876, loss 0.303405, acc 0.90625, prec 0.0964973, recall 0.912513
2017-12-10T15:40:07.652978: step 4877, loss 0.084057, acc 0.96875, prec 0.096493, recall 0.912513
2017-12-10T15:40:07.849209: step 4878, loss 0.130548, acc 0.9375, prec 0.0965246, recall 0.91255
2017-12-10T15:40:08.047766: step 4879, loss 0.05002, acc 0.984375, prec 0.0965224, recall 0.91255
2017-12-10T15:40:08.251514: step 4880, loss 0.594006, acc 0.96875, prec 0.0965583, recall 0.912587
2017-12-10T15:40:08.452828: step 4881, loss 0.15608, acc 0.984375, prec 0.0965562, recall 0.912587
2017-12-10T15:40:08.650967: step 4882, loss 0.0728231, acc 0.984375, prec 0.0965942, recall 0.912623
2017-12-10T15:40:08.851820: step 4883, loss 0.0126653, acc 1, prec 0.0965942, recall 0.912623
2017-12-10T15:40:09.054117: step 4884, loss 0.131601, acc 0.953125, prec 0.0966078, recall 0.912642
2017-12-10T15:40:09.253328: step 4885, loss 0.303339, acc 0.96875, prec 0.0966035, recall 0.912642
2017-12-10T15:40:09.449907: step 4886, loss 0.68477, acc 0.953125, prec 0.0965971, recall 0.912642
2017-12-10T15:40:09.647959: step 4887, loss 0.140004, acc 0.953125, prec 0.0966308, recall 0.912678
2017-12-10T15:40:09.841781: step 4888, loss 0.0944532, acc 0.96875, prec 0.0966466, recall 0.912697
2017-12-10T15:40:10.037608: step 4889, loss 0.00909873, acc 1, prec 0.0966667, recall 0.912715
2017-12-10T15:40:10.237062: step 4890, loss 0.333534, acc 0.921875, prec 0.0966559, recall 0.912715
2017-12-10T15:40:10.440064: step 4891, loss 0.071496, acc 0.984375, prec 0.0966739, recall 0.912733
2017-12-10T15:40:10.637614: step 4892, loss 0.877305, acc 0.953125, prec 0.0967276, recall 0.912788
2017-12-10T15:40:10.837062: step 4893, loss 0.161411, acc 0.953125, prec 0.0967412, recall 0.912807
2017-12-10T15:40:11.036910: step 4894, loss 0.103244, acc 0.921875, prec 0.0967505, recall 0.912825
2017-12-10T15:40:11.241369: step 4895, loss 0.296759, acc 0.96875, prec 0.0967463, recall 0.912825
2017-12-10T15:40:11.439707: step 4896, loss 0.00493105, acc 1, prec 0.0967463, recall 0.912825
2017-12-10T15:40:11.639444: step 4897, loss 0.0859574, acc 0.96875, prec 0.096762, recall 0.912843
2017-12-10T15:40:11.843124: step 4898, loss 0.0936593, acc 0.96875, prec 0.0967577, recall 0.912843
2017-12-10T15:40:12.042205: step 4899, loss 0.160087, acc 0.9375, prec 0.0967892, recall 0.91288
2017-12-10T15:40:12.241621: step 4900, loss 0.0620854, acc 0.96875, prec 0.096805, recall 0.912898
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_2/1512937383/checkpoints/model-4900

2017-12-10T15:40:13.381620: step 4901, loss 0.00740856, acc 1, prec 0.096805, recall 0.912898
2017-12-10T15:40:13.580261: step 4902, loss 0.296814, acc 0.96875, prec 0.0968408, recall 0.912934
2017-12-10T15:40:13.785646: step 4903, loss 0.304285, acc 0.953125, prec 0.0968744, recall 0.912971
2017-12-10T15:40:13.983720: step 4904, loss 0.0908432, acc 0.984375, prec 0.0968723, recall 0.912971
2017-12-10T15:40:14.183191: step 4905, loss 0.177769, acc 0.953125, prec 0.0969059, recall 0.913007
2017-12-10T15:40:14.395591: step 4906, loss 0.147223, acc 0.953125, prec 0.0969396, recall 0.913043
2017-12-10T15:40:14.595318: step 4907, loss 0.112444, acc 0.96875, prec 0.0969553, recall 0.913062
2017-12-10T15:40:14.792960: step 4908, loss 0.151079, acc 0.953125, prec 0.0969689, recall 0.91308
2017-12-10T15:40:14.993640: step 4909, loss 0.162486, acc 0.953125, prec 0.0969825, recall 0.913098
2017-12-10T15:40:15.195979: step 4910, loss 0.0238332, acc 0.984375, prec 0.0970404, recall 0.913152
2017-12-10T15:40:15.397155: step 4911, loss 0.668978, acc 0.890625, prec 0.0970654, recall 0.913189
2017-12-10T15:40:15.591970: step 4912, loss 0.0266179, acc 1, prec 0.0971055, recall 0.913225
2017-12-10T15:40:15.789061: step 4913, loss 0.726303, acc 0.890625, prec 0.0971304, recall 0.913261
2017-12-10T15:40:15.987430: step 4914, loss 0.172902, acc 0.96875, prec 0.0971662, recall 0.913297
2017-12-10T15:40:16.189616: step 4915, loss 0.0229405, acc 1, prec 0.0971662, recall 0.913297
2017-12-10T15:40:16.387650: step 4916, loss 0.0138863, acc 1, prec 0.0972062, recall 0.913333
2017-12-10T15:40:16.587836: step 4917, loss 0.144919, acc 0.921875, prec 0.0971954, recall 0.913333
2017-12-10T15:40:16.785742: step 4918, loss 0.172881, acc 1, prec 0.0972355, recall 0.913369
2017-12-10T15:40:16.983287: step 4919, loss 0.0334987, acc 1, prec 0.0972555, recall 0.913387
2017-12-10T15:40:17.187375: step 4920, loss 0.120751, acc 0.96875, prec 0.0972712, recall 0.913405
2017-12-10T15:40:17.386730: step 4921, loss 0.21288, acc 0.953125, prec 0.0972847, recall 0.913424
2017-12-10T15:40:17.586997: step 4922, loss 0.208531, acc 0.96875, prec 0.0973004, recall 0.913442
2017-12-10T15:40:17.786927: step 4923, loss 0.837445, acc 0.9375, prec 0.0973118, recall 0.91346
2017-12-10T15:40:17.989520: step 4924, loss 0.00744434, acc 1, prec 0.0973118, recall 0.91346
2017-12-10T15:40:18.188640: step 4925, loss 1.05921, acc 0.953125, prec 0.0973253, recall 0.913478
2017-12-10T15:40:18.394232: step 4926, loss 0.041993, acc 0.984375, prec 0.0973232, recall 0.913478
2017-12-10T15:40:18.591938: step 4927, loss 0.0692852, acc 0.984375, prec 0.097321, recall 0.913478
2017-12-10T15:40:18.793579: step 4928, loss 0.246176, acc 0.9375, prec 0.0973324, recall 0.913496
2017-12-10T15:40:18.996068: step 4929, loss 0.177682, acc 0.953125, prec 0.0973659, recall 0.913531
2017-12-10T15:40:19.193787: step 4930, loss 0.413476, acc 0.9375, prec 0.0973773, recall 0.913549
2017-12-10T15:40:19.400411: step 4931, loss 0.168757, acc 0.953125, prec 0.0973908, recall 0.913567
2017-12-10T15:40:19.600631: step 4932, loss 0.111106, acc 0.9375, prec 0.0973822, recall 0.913567
2017-12-10T15:40:19.801768: step 4933, loss 0.18582, acc 0.953125, prec 0.0973957, recall 0.913585
2017-12-10T15:40:20.002697: step 4934, loss 0.147188, acc 0.9375, prec 0.097427, recall 0.913621
2017-12-10T15:40:20.204313: step 4935, loss 0.213212, acc 0.90625, prec 0.0974141, recall 0.913621
2017-12-10T15:40:20.407347: step 4936, loss 0.477977, acc 0.9375, prec 0.0974454, recall 0.913657
2017-12-10T15:40:20.607737: step 4937, loss 0.158352, acc 0.953125, prec 0.0974589, recall 0.913675
2017-12-10T15:40:20.801553: step 4938, loss 0.081346, acc 0.96875, prec 0.0974746, recall 0.913693
2017-12-10T15:40:20.997142: step 4939, loss 0.12394, acc 0.96875, prec 0.0974903, recall 0.913711
2017-12-10T15:40:21.197195: step 4940, loss 0.249381, acc 0.90625, prec 0.0975173, recall 0.913747
2017-12-10T15:40:21.392402: step 4941, loss 0.0810654, acc 0.984375, prec 0.0975151, recall 0.913747
2017-12-10T15:40:21.589238: step 4942, loss 0.26363, acc 0.9375, prec 0.0975065, recall 0.913747
2017-12-10T15:40:21.784676: step 4943, loss 0.523686, acc 0.96875, prec 0.0975421, recall 0.913782
2017-12-10T15:40:21.990052: step 4944, loss 0.233815, acc 0.9375, prec 0.0975335, recall 0.913782
2017-12-10T15:40:22.186635: step 4945, loss 0.135384, acc 0.984375, prec 0.0975313, recall 0.913782
2017-12-10T15:40:22.385688: step 4946, loss 0.108437, acc 0.96875, prec 0.0975469, recall 0.9138
2017-12-10T15:40:22.587342: step 4947, loss 0.164552, acc 0.9375, prec 0.0975583, recall 0.913818
2017-12-10T15:40:22.788986: step 4948, loss 0.0763577, acc 0.96875, prec 0.0975739, recall 0.913836
2017-12-10T15:40:22.992128: step 4949, loss 0.428438, acc 0.9375, prec 0.0975653, recall 0.913836
2017-12-10T15:40:23.194088: step 4950, loss 0.246186, acc 0.96875, prec 0.0975809, recall 0.913854
2017-12-10T15:40:23.393310: step 4951, loss 0.234546, acc 0.953125, prec 0.0975944, recall 0.913872
2017-12-10T15:40:23.594731: step 4952, loss 0.0113827, acc 1, prec 0.0975944, recall 0.913872
2017-12-10T15:40:23.795167: step 4953, loss 0.138388, acc 0.984375, prec 0.0975922, recall 0.913872
2017-12-10T15:40:23.998112: step 4954, loss 0.0310847, acc 0.984375, prec 0.09761, recall 0.913889
2017-12-10T15:40:24.200515: step 4955, loss 0.0537013, acc 0.984375, prec 0.0976079, recall 0.913889
2017-12-10T15:40:24.403780: step 4956, loss 0.0582835, acc 0.96875, prec 0.0976036, recall 0.913889
2017-12-10T15:40:24.602983: step 4957, loss 0.0156519, acc 1, prec 0.0976235, recall 0.913907
2017-12-10T15:40:24.802887: step 4958, loss 0.0375672, acc 1, prec 0.0977033, recall 0.913979
2017-12-10T15:40:25.001649: step 4959, loss 0.0493117, acc 0.96875, prec 0.0977189, recall 0.913996
2017-12-10T15:40:25.196804: step 4960, loss 0.00765168, acc 1, prec 0.0977189, recall 0.913996
2017-12-10T15:40:25.398705: step 4961, loss 0.0755009, acc 0.984375, prec 0.0977567, recall 0.914032
2017-12-10T15:40:25.595385: step 4962, loss 0.182668, acc 0.96875, prec 0.0977723, recall 0.91405
2017-12-10T15:40:25.797358: step 4963, loss 0.00453567, acc 1, prec 0.0977723, recall 0.91405
2017-12-10T15:40:25.992482: step 4964, loss 3.75106, acc 0.953125, prec 0.0978078, recall 0.913896
2017-12-10T15:40:26.193763: step 4965, loss 0.132635, acc 0.984375, prec 0.0978455, recall 0.913932
2017-12-10T15:40:26.395680: step 4966, loss 0.0373722, acc 0.984375, prec 0.0978434, recall 0.913932
2017-12-10T15:40:26.604008: step 4967, loss 0.15362, acc 0.9375, prec 0.0978547, recall 0.91395
2017-12-10T15:40:26.800329: step 4968, loss 0.0500655, acc 0.984375, prec 0.0978525, recall 0.91395
2017-12-10T15:40:27.000214: step 4969, loss 0.0677147, acc 0.96875, prec 0.0978681, recall 0.913967
2017-12-10T15:40:27.182659: step 4970, loss 0.125071, acc 0.961538, prec 0.0978837, recall 0.913985
Training finished
Starting Fold: 3 => Train/Dev split: 31796/10598


 #################### 


SEQUENCE LENGTH 273
BATCH SIZE 64
EMBEDDING SIZE 128
FILTER SIZES [3, 4, 5]
NUMBER OF FILTERS 128
L2 REG LAMBDA 0.0
EPOCHS 10



RESULT DIR leave_pos_embedding_out_fold_3
VOCAB SIZE 33447
DROPOUT PROBABILITY 0.5



WORD EMBEDDING True
POSITION EMBEDDING True
POS EMBEDDING False


 #################### 


Writing to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_3/1512938428

Start training
2017-12-10T15:40:30.908909: step 1, loss 4.01075, acc 0.375, prec 0, recall 0
2017-12-10T15:40:31.102256: step 2, loss 1.61157, acc 0.546875, prec 0, recall 0
2017-12-10T15:40:31.301240: step 3, loss 17.4537, acc 0.875, prec 0, recall 0
2017-12-10T15:40:31.503574: step 4, loss 8.75223, acc 0.875, prec 0, recall 0
2017-12-10T15:40:31.701053: step 5, loss 0.444586, acc 0.890625, prec 0, recall 0
2017-12-10T15:40:31.899763: step 6, loss 0.360363, acc 0.84375, prec 0, recall 0
2017-12-10T15:40:32.095298: step 7, loss 3.84103, acc 0.734375, prec 0, recall 0
2017-12-10T15:40:32.291114: step 8, loss 8.06922, acc 0.765625, prec 0, recall 0
2017-12-10T15:40:32.486402: step 9, loss 12.0199, acc 0.734375, prec 0.00694444, recall 0.111111
2017-12-10T15:40:32.686110: step 10, loss 6.01153, acc 0.453125, prec 0.0111732, recall 0.181818
2017-12-10T15:40:32.881522: step 11, loss 2.08085, acc 0.453125, prec 0.00934579, recall 0.181818
2017-12-10T15:40:33.079353: step 12, loss 4.35828, acc 0.34375, prec 0.0078125, recall 0.181818
2017-12-10T15:40:33.273185: step 13, loss 4.76051, acc 0.28125, prec 0.0131579, recall 0.307692
2017-12-10T15:40:33.472176: step 14, loss 5.6605, acc 0.296875, prec 0.0114943, recall 0.285714
2017-12-10T15:40:33.670810: step 15, loss 5.60587, acc 0.234375, prec 0.0150376, recall 0.375
2017-12-10T15:40:33.864081: step 16, loss 5.64211, acc 0.21875, prec 0.0177384, recall 0.444444
2017-12-10T15:40:34.062958: step 17, loss 4.66116, acc 0.203125, prec 0.0198413, recall 0.5
2017-12-10T15:40:34.261643: step 18, loss 5.53571, acc 0.1875, prec 0.0197487, recall 0.52381
2017-12-10T15:40:34.455435: step 19, loss 10.6655, acc 0.21875, prec 0.0181518, recall 0.5
2017-12-10T15:40:34.652492: step 20, loss 4.39885, acc 0.203125, prec 0.0167428, recall 0.5
2017-12-10T15:40:34.848170: step 21, loss 4.68415, acc 0.234375, prec 0.0155807, recall 0.5
2017-12-10T15:40:35.047894: step 22, loss 6.74965, acc 0.3125, prec 0.016, recall 0.5
2017-12-10T15:40:35.241385: step 23, loss 4.14477, acc 0.375, prec 0.0151899, recall 0.5
2017-12-10T15:40:35.441819: step 24, loss 17.041, acc 0.375, prec 0.0144753, recall 0.48
2017-12-10T15:40:35.634553: step 25, loss 3.87628, acc 0.375, prec 0.013809, recall 0.48
2017-12-10T15:40:35.829812: step 26, loss 3.47458, acc 0.328125, prec 0.0131579, recall 0.48
2017-12-10T15:40:36.030158: step 27, loss 9.78247, acc 0.4375, prec 0.0126716, recall 0.461538
2017-12-10T15:40:36.226810: step 28, loss 2.40384, acc 0.53125, prec 0.0122825, recall 0.461538
2017-12-10T15:40:36.423759: step 29, loss 2.44317, acc 0.546875, prec 0.0119284, recall 0.461538
2017-12-10T15:40:36.616408: step 30, loss 4.72611, acc 0.46875, prec 0.0115496, recall 0.444444
2017-12-10T15:40:36.808286: step 31, loss 2.31325, acc 0.484375, prec 0.011194, recall 0.444444
2017-12-10T15:40:37.000330: step 32, loss 2.03516, acc 0.5, prec 0.0108696, recall 0.444444
2017-12-10T15:40:37.202066: step 33, loss 2.00433, acc 0.578125, prec 0.0106101, recall 0.444444
2017-12-10T15:40:37.395828: step 34, loss 1.46045, acc 0.546875, prec 0.0103448, recall 0.444444
2017-12-10T15:40:37.590332: step 35, loss 1.50969, acc 0.734375, prec 0.0101954, recall 0.444444
2017-12-10T15:40:37.785019: step 36, loss 7.05915, acc 0.6875, prec 0.0100334, recall 0.428571
2017-12-10T15:40:37.977458: step 37, loss 1.20596, acc 0.71875, prec 0.00988468, recall 0.428571
2017-12-10T15:40:38.173221: step 38, loss 0.944336, acc 0.671875, prec 0.0097166, recall 0.428571
2017-12-10T15:40:38.368999: step 39, loss 0.983226, acc 0.75, prec 0.00959233, recall 0.428571
2017-12-10T15:40:38.570879: step 40, loss 19.4618, acc 0.828125, prec 0.00951626, recall 0.413793
2017-12-10T15:40:38.769676: step 41, loss 1.11561, acc 0.703125, prec 0.009375, recall 0.413793
2017-12-10T15:40:38.968882: step 42, loss 1.0989, acc 0.734375, prec 0.00925212, recall 0.413793
2017-12-10T15:40:39.163336: step 43, loss 1.14495, acc 0.734375, prec 0.00913242, recall 0.413793
2017-12-10T15:40:39.358668: step 44, loss 0.495438, acc 0.828125, prec 0.0090566, recall 0.413793
2017-12-10T15:40:39.551788: step 45, loss 29.7663, acc 0.796875, prec 0.00898204, recall 0.387097
2017-12-10T15:40:39.746442: step 46, loss 3.172, acc 0.6875, prec 0.00885609, recall 0.375
2017-12-10T15:40:39.939553: step 47, loss 20.6282, acc 0.671875, prec 0.00945455, recall 0.371429
2017-12-10T15:40:40.138549: step 48, loss 1.35173, acc 0.78125, prec 0.0100719, recall 0.388889
2017-12-10T15:40:40.334692: step 49, loss 1.65162, acc 0.625, prec 0.00990099, recall 0.388889
2017-12-10T15:40:40.533092: step 50, loss 7.79134, acc 0.484375, prec 0.0103663, recall 0.394737
2017-12-10T15:40:40.734302: step 51, loss 3.70195, acc 0.375, prec 0.0114171, recall 0.425
2017-12-10T15:40:40.930096: step 52, loss 5.18041, acc 0.421875, prec 0.0137255, recall 0.477273
2017-12-10T15:40:41.124719: step 53, loss 5.07203, acc 0.28125, prec 0.0139505, recall 0.488889
2017-12-10T15:40:41.322598: step 54, loss 3.94375, acc 0.34375, prec 0.0154131, recall 0.520833
2017-12-10T15:40:41.521092: step 55, loss 5.09061, acc 0.203125, prec 0.0161194, recall 0.54
2017-12-10T15:40:41.717797: step 56, loss 5.99173, acc 0.109375, prec 0.0155889, recall 0.54
2017-12-10T15:40:41.911773: step 57, loss 6.65485, acc 0.15625, prec 0.0162192, recall 0.557692
2017-12-10T15:40:42.103774: step 58, loss 5.556, acc 0.15625, prec 0.0157438, recall 0.557692
2017-12-10T15:40:42.299005: step 59, loss 4.73272, acc 0.171875, prec 0.0153034, recall 0.557692
2017-12-10T15:40:42.495902: step 60, loss 5.10581, acc 0.234375, prec 0.0149177, recall 0.557692
2017-12-10T15:40:42.687552: step 61, loss 10.8998, acc 0.203125, prec 0.0145436, recall 0.54717
2017-12-10T15:40:42.880406: step 62, loss 3.8489, acc 0.390625, prec 0.0142646, recall 0.54717
2017-12-10T15:40:43.076268: step 63, loss 2.66333, acc 0.421875, prec 0.0144858, recall 0.555556
2017-12-10T15:40:43.278689: step 64, loss 3.01151, acc 0.375, prec 0.0142113, recall 0.555556
2017-12-10T15:40:43.476103: step 65, loss 2.44994, acc 0.640625, prec 0.0145199, recall 0.563636
2017-12-10T15:40:43.671841: step 66, loss 2.753, acc 0.453125, prec 0.0142857, recall 0.563636
2017-12-10T15:40:43.864621: step 67, loss 9.2143, acc 0.59375, prec 0.0154756, recall 0.566667
2017-12-10T15:40:44.063349: step 68, loss 1.99769, acc 0.5, prec 0.0152535, recall 0.566667
2017-12-10T15:40:44.258766: step 69, loss 2.74972, acc 0.515625, prec 0.0150442, recall 0.566667
2017-12-10T15:40:44.463748: step 70, loss 1.74815, acc 0.609375, prec 0.0148796, recall 0.566667
2017-12-10T15:40:44.658585: step 71, loss 2.82138, acc 0.4375, prec 0.0146489, recall 0.566667
2017-12-10T15:40:44.850048: step 72, loss 12.1015, acc 0.703125, prec 0.0149573, recall 0.564516
2017-12-10T15:40:45.042710: step 73, loss 0.948211, acc 0.734375, prec 0.0148494, recall 0.564516
2017-12-10T15:40:45.239590: step 74, loss 0.763299, acc 0.78125, prec 0.0147617, recall 0.564516
2017-12-10T15:40:45.436231: step 75, loss 1.16329, acc 0.71875, prec 0.0146505, recall 0.564516
2017-12-10T15:40:45.632857: step 76, loss 2.29893, acc 0.625, prec 0.014913, recall 0.571429
2017-12-10T15:40:45.828346: step 77, loss 1.16399, acc 0.71875, prec 0.0152076, recall 0.578125
2017-12-10T15:40:46.023661: step 78, loss 5.3601, acc 0.78125, prec 0.0155292, recall 0.575758
2017-12-10T15:40:46.218522: step 79, loss 11.2363, acc 0.671875, prec 0.0154096, recall 0.558824
2017-12-10T15:40:46.411707: step 80, loss 7.8578, acc 0.6875, prec 0.0152918, recall 0.550725
2017-12-10T15:40:46.604560: step 81, loss 8.25837, acc 0.65625, prec 0.0151636, recall 0.542857
2017-12-10T15:40:46.803117: step 82, loss 2.61463, acc 0.53125, prec 0.0149842, recall 0.542857
2017-12-10T15:40:47.003777: step 83, loss 2.70613, acc 0.453125, prec 0.0151633, recall 0.549296
2017-12-10T15:40:47.201381: step 84, loss 2.99013, acc 0.4375, prec 0.014954, recall 0.549296
2017-12-10T15:40:47.396470: step 85, loss 2.38795, acc 0.53125, prec 0.0151573, recall 0.555556
2017-12-10T15:40:47.591087: step 86, loss 3.73496, acc 0.328125, prec 0.0149142, recall 0.555556
2017-12-10T15:40:47.787511: step 87, loss 3.41284, acc 0.453125, prec 0.0150846, recall 0.561644
2017-12-10T15:40:47.981110: step 88, loss 4.13592, acc 0.390625, prec 0.0155854, recall 0.573333
2017-12-10T15:40:48.174058: step 89, loss 3.96528, acc 0.390625, prec 0.0160714, recall 0.584416
2017-12-10T15:40:48.366890: step 90, loss 4.31001, acc 0.390625, prec 0.0165435, recall 0.594937
2017-12-10T15:40:48.565561: step 91, loss 3.02869, acc 0.34375, prec 0.0163025, recall 0.594937
2017-12-10T15:40:48.758360: step 92, loss 2.82435, acc 0.40625, prec 0.0160904, recall 0.594937
2017-12-10T15:40:48.951295: step 93, loss 2.83498, acc 0.46875, prec 0.0162382, recall 0.6
2017-12-10T15:40:49.143457: step 94, loss 2.75514, acc 0.484375, prec 0.0160589, recall 0.6
2017-12-10T15:40:49.347235: step 95, loss 1.82012, acc 0.625, prec 0.015931, recall 0.6
2017-12-10T15:40:49.541742: step 96, loss 6.29392, acc 0.625, prec 0.0158103, recall 0.592593
2017-12-10T15:40:49.737054: step 97, loss 1.82795, acc 0.578125, prec 0.0163132, recall 0.60241
2017-12-10T15:40:49.935435: step 98, loss 1.67968, acc 0.640625, prec 0.0161917, recall 0.60241
2017-12-10T15:40:50.133145: step 99, loss 1.41677, acc 0.671875, prec 0.0160823, recall 0.60241
2017-12-10T15:40:50.326725: step 100, loss 0.852984, acc 0.8125, prec 0.0160205, recall 0.60241
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_3/1512938428/checkpoints/model-100

2017-12-10T15:40:51.598302: step 101, loss 0.694428, acc 0.8125, prec 0.0162731, recall 0.607143
2017-12-10T15:40:51.794819: step 102, loss 0.95538, acc 0.734375, prec 0.0164975, recall 0.611765
2017-12-10T15:40:51.994729: step 103, loss 10.3021, acc 0.6875, prec 0.0164038, recall 0.597701
2017-12-10T15:40:52.195815: step 104, loss 21.1991, acc 0.796875, prec 0.0163419, recall 0.590909
2017-12-10T15:40:52.394055: step 105, loss 0.743415, acc 0.8125, prec 0.0165884, recall 0.595506
2017-12-10T15:40:52.588839: step 106, loss 0.981083, acc 0.765625, prec 0.0165109, recall 0.595506
2017-12-10T15:40:52.783475: step 107, loss 1.59893, acc 0.6875, prec 0.0164087, recall 0.595506
2017-12-10T15:40:52.978581: step 108, loss 1.11506, acc 0.75, prec 0.0163278, recall 0.595506
2017-12-10T15:40:53.172071: step 109, loss 4.94526, acc 0.734375, prec 0.0171516, recall 0.602151
2017-12-10T15:40:53.368305: step 110, loss 1.25599, acc 0.65625, prec 0.0170368, recall 0.602151
2017-12-10T15:40:53.564336: step 111, loss 2.08318, acc 0.625, prec 0.0175068, recall 0.610526
2017-12-10T15:40:53.760403: step 112, loss 2.11074, acc 0.59375, prec 0.0173705, recall 0.610526
2017-12-10T15:40:53.953274: step 113, loss 3.82503, acc 0.515625, prec 0.0175074, recall 0.608247
2017-12-10T15:40:54.144078: step 114, loss 1.10128, acc 0.65625, prec 0.0173939, recall 0.608247
2017-12-10T15:40:54.336910: step 115, loss 1.46036, acc 0.640625, prec 0.0175644, recall 0.612245
2017-12-10T15:40:54.529803: step 116, loss 2.38161, acc 0.46875, prec 0.0173913, recall 0.612245
2017-12-10T15:40:54.728853: step 117, loss 2.05523, acc 0.5, prec 0.0175136, recall 0.616162
2017-12-10T15:40:54.924762: step 118, loss 6.72145, acc 0.5625, prec 0.0173789, recall 0.61
2017-12-10T15:40:55.122992: step 119, loss 2.33452, acc 0.6875, prec 0.0175588, recall 0.613861
2017-12-10T15:40:55.318781: step 120, loss 2.72394, acc 0.53125, prec 0.0182379, recall 0.625
2017-12-10T15:40:55.508673: step 121, loss 2.36783, acc 0.5625, prec 0.0180958, recall 0.625
2017-12-10T15:40:55.706752: step 122, loss 11.665, acc 0.515625, prec 0.0182169, recall 0.622642
2017-12-10T15:40:55.902188: step 123, loss 2.5122, acc 0.484375, prec 0.0180525, recall 0.622642
2017-12-10T15:40:56.098017: step 124, loss 2.31063, acc 0.5, prec 0.0178959, recall 0.622642
2017-12-10T15:40:56.291566: step 125, loss 2.64417, acc 0.609375, prec 0.0180398, recall 0.626168
2017-12-10T15:40:56.484379: step 126, loss 1.97719, acc 0.578125, prec 0.0186966, recall 0.636364
2017-12-10T15:40:56.684994: step 127, loss 1.82181, acc 0.65625, prec 0.0185874, recall 0.636364
2017-12-10T15:40:56.882482: step 128, loss 7.88374, acc 0.453125, prec 0.0184211, recall 0.630631
2017-12-10T15:40:57.079971: step 129, loss 2.0876, acc 0.609375, prec 0.0185572, recall 0.633929
2017-12-10T15:40:57.275963: step 130, loss 2.39434, acc 0.515625, prec 0.0184081, recall 0.633929
2017-12-10T15:40:57.469330: step 131, loss 1.83185, acc 0.53125, prec 0.018266, recall 0.633929
2017-12-10T15:40:57.664599: step 132, loss 1.98279, acc 0.578125, prec 0.0183908, recall 0.637168
2017-12-10T15:40:57.857881: step 133, loss 4.22854, acc 0.734375, prec 0.018316, recall 0.631579
2017-12-10T15:40:58.057328: step 134, loss 1.90518, acc 0.59375, prec 0.0184437, recall 0.634783
2017-12-10T15:40:58.254338: step 135, loss 2.5899, acc 0.703125, prec 0.018607, recall 0.632479
2017-12-10T15:40:58.451106: step 136, loss 2.05316, acc 0.609375, prec 0.0187359, recall 0.635593
2017-12-10T15:40:58.645799: step 137, loss 6.09604, acc 0.71875, prec 0.0196322, recall 0.642276
2017-12-10T15:40:58.846382: step 138, loss 5.77438, acc 0.625, prec 0.0197628, recall 0.64
2017-12-10T15:40:59.043540: step 139, loss 1.07711, acc 0.75, prec 0.0199262, recall 0.642857
2017-12-10T15:40:59.241231: step 140, loss 4.13975, acc 0.5, prec 0.0197754, recall 0.637795
2017-12-10T15:40:59.439823: step 141, loss 2.3151, acc 0.5625, prec 0.0198788, recall 0.640625
2017-12-10T15:40:59.635393: step 142, loss 3.32691, acc 0.5625, prec 0.0202166, recall 0.646154
2017-12-10T15:40:59.825524: step 143, loss 5.26038, acc 0.546875, prec 0.0203155, recall 0.643939
2017-12-10T15:41:00.019119: step 144, loss 2.93564, acc 0.453125, prec 0.0203791, recall 0.646617
2017-12-10T15:41:00.212156: step 145, loss 2.84361, acc 0.4375, prec 0.0204369, recall 0.649254
2017-12-10T15:41:00.408046: step 146, loss 4.18648, acc 0.296875, prec 0.0204508, recall 0.651852
2017-12-10T15:41:00.602613: step 147, loss 3.83758, acc 0.4375, prec 0.0202812, recall 0.651852
2017-12-10T15:41:00.799200: step 148, loss 3.27236, acc 0.390625, prec 0.0207715, recall 0.65942
2017-12-10T15:41:00.993033: step 149, loss 2.96207, acc 0.515625, prec 0.0206256, recall 0.65942
2017-12-10T15:41:01.189831: step 150, loss 1.87266, acc 0.640625, prec 0.0207394, recall 0.66187
2017-12-10T15:41:01.384181: step 151, loss 2.79401, acc 0.546875, prec 0.020824, recall 0.664286
2017-12-10T15:41:01.583715: step 152, loss 2.26022, acc 0.53125, prec 0.0206851, recall 0.664286
2017-12-10T15:41:01.776878: step 153, loss 2.18677, acc 0.59375, prec 0.0207827, recall 0.666667
2017-12-10T15:41:01.969435: step 154, loss 1.3935, acc 0.6875, prec 0.0209067, recall 0.669014
2017-12-10T15:41:02.164930: step 155, loss 9.2132, acc 0.609375, prec 0.0207968, recall 0.664336
2017-12-10T15:41:02.361729: step 156, loss 1.20648, acc 0.703125, prec 0.0207107, recall 0.664336
2017-12-10T15:41:02.551344: step 157, loss 1.19617, acc 0.75, prec 0.0208514, recall 0.666667
2017-12-10T15:41:02.744329: step 158, loss 7.42593, acc 0.8125, prec 0.0210139, recall 0.664384
2017-12-10T15:41:02.947856: step 159, loss 0.814553, acc 0.828125, prec 0.0218095, recall 0.673333
2017-12-10T15:41:03.142790: step 160, loss 1.2609, acc 0.703125, prec 0.0217204, recall 0.673333
2017-12-10T15:41:03.339227: step 161, loss 0.725033, acc 0.78125, prec 0.0216552, recall 0.673333
2017-12-10T15:41:03.539886: step 162, loss 9.13369, acc 0.6875, prec 0.0217763, recall 0.671053
2017-12-10T15:41:03.732624: step 163, loss 1.09238, acc 0.8125, prec 0.0219289, recall 0.673203
2017-12-10T15:41:03.929913: step 164, loss 1.29798, acc 0.765625, prec 0.0220666, recall 0.675325
2017-12-10T15:41:04.129652: step 165, loss 1.24658, acc 0.703125, prec 0.0221847, recall 0.677419
2017-12-10T15:41:04.328916: step 166, loss 1.21923, acc 0.703125, prec 0.022096, recall 0.677419
2017-12-10T15:41:04.529019: step 167, loss 1.41329, acc 0.703125, prec 0.022008, recall 0.677419
2017-12-10T15:41:04.722565: step 168, loss 12.9381, acc 0.734375, prec 0.0219344, recall 0.673077
2017-12-10T15:41:04.918420: step 169, loss 0.790909, acc 0.8125, prec 0.0218796, recall 0.673077
2017-12-10T15:41:05.109997: step 170, loss 4.32125, acc 0.78125, prec 0.0220237, recall 0.670886
2017-12-10T15:41:05.306610: step 171, loss 2.15931, acc 0.578125, prec 0.0219008, recall 0.670886
2017-12-10T15:41:05.511065: step 172, loss 1.0328, acc 0.703125, prec 0.0218152, recall 0.670886
2017-12-10T15:41:05.705324: step 173, loss 1.15192, acc 0.6875, prec 0.0219262, recall 0.672956
2017-12-10T15:41:05.898441: step 174, loss 0.850511, acc 0.8125, prec 0.0220723, recall 0.675
2017-12-10T15:41:06.096156: step 175, loss 1.81315, acc 0.65625, prec 0.0221725, recall 0.677019
2017-12-10T15:41:06.289794: step 176, loss 0.888496, acc 0.78125, prec 0.0221095, recall 0.677019
2017-12-10T15:41:06.484431: step 177, loss 2.12198, acc 0.765625, prec 0.0224378, recall 0.680982
2017-12-10T15:41:06.679739: step 178, loss 1.23687, acc 0.703125, prec 0.0225488, recall 0.682927
2017-12-10T15:41:06.879353: step 179, loss 3.30388, acc 0.6875, prec 0.0224629, recall 0.678788
2017-12-10T15:41:07.075745: step 180, loss 6.7846, acc 0.625, prec 0.0227499, recall 0.678571
2017-12-10T15:41:07.269643: step 181, loss 0.881195, acc 0.703125, prec 0.022664, recall 0.678571
2017-12-10T15:41:07.463766: step 182, loss 1.47219, acc 0.71875, prec 0.0227768, recall 0.680473
2017-12-10T15:41:07.656050: step 183, loss 1.44214, acc 0.65625, prec 0.022678, recall 0.680473
2017-12-10T15:41:07.849839: step 184, loss 1.20448, acc 0.625, prec 0.022763, recall 0.682353
2017-12-10T15:41:08.042798: step 185, loss 10.6034, acc 0.65625, prec 0.022674, recall 0.674419
2017-12-10T15:41:08.243672: step 186, loss 2.58801, acc 0.53125, prec 0.0225418, recall 0.674419
2017-12-10T15:41:08.434528: step 187, loss 2.25913, acc 0.625, prec 0.0228152, recall 0.678161
2017-12-10T15:41:08.628660: step 188, loss 2.88234, acc 0.453125, prec 0.0226618, recall 0.678161
2017-12-10T15:41:08.819209: step 189, loss 2.99652, acc 0.546875, prec 0.0227229, recall 0.68
2017-12-10T15:41:09.016890: step 190, loss 2.89198, acc 0.515625, prec 0.0225892, recall 0.68
2017-12-10T15:41:09.212219: step 191, loss 2.82988, acc 0.484375, prec 0.0224486, recall 0.68
2017-12-10T15:41:09.401746: step 192, loss 2.0941, acc 0.625, prec 0.022531, recall 0.681818
2017-12-10T15:41:09.600108: step 193, loss 4.8681, acc 0.640625, prec 0.0224383, recall 0.677966
2017-12-10T15:41:09.799353: step 194, loss 1.88531, acc 0.59375, prec 0.0225116, recall 0.679775
2017-12-10T15:41:09.997246: step 195, loss 2.21648, acc 0.59375, prec 0.0225842, recall 0.681564
2017-12-10T15:41:10.193318: step 196, loss 2.33654, acc 0.59375, prec 0.0224761, recall 0.681564
2017-12-10T15:41:10.392705: step 197, loss 1.29814, acc 0.703125, prec 0.0223977, recall 0.681564
2017-12-10T15:41:10.587352: step 198, loss 1.69291, acc 0.65625, prec 0.0223076, recall 0.681564
2017-12-10T15:41:10.782176: step 199, loss 1.11962, acc 0.765625, prec 0.022603, recall 0.685083
2017-12-10T15:41:10.971630: step 200, loss 2.09575, acc 0.546875, prec 0.0226613, recall 0.686813
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_3/1512938428/checkpoints/model-200

2017-12-10T15:41:12.226959: step 201, loss 1.0148, acc 0.765625, prec 0.0225999, recall 0.686813
2017-12-10T15:41:12.425374: step 202, loss 0.781167, acc 0.78125, prec 0.0227191, recall 0.688525
2017-12-10T15:41:12.619863: step 203, loss 22.7138, acc 0.796875, prec 0.02267, recall 0.684783
2017-12-10T15:41:12.813822: step 204, loss 5.13298, acc 0.8125, prec 0.0226252, recall 0.681081
2017-12-10T15:41:13.014273: step 205, loss 0.858105, acc 0.8125, prec 0.0225766, recall 0.681081
2017-12-10T15:41:13.211234: step 206, loss 2.01565, acc 0.8125, prec 0.0225322, recall 0.677419
2017-12-10T15:41:13.405415: step 207, loss 0.529105, acc 0.828125, prec 0.022488, recall 0.677419
2017-12-10T15:41:13.607639: step 208, loss 1.12031, acc 0.75, prec 0.0224239, recall 0.677419
2017-12-10T15:41:13.802862: step 209, loss 6.88316, acc 0.8125, prec 0.0223801, recall 0.673797
2017-12-10T15:41:14.000932: step 210, loss 1.84751, acc 0.65625, prec 0.0226388, recall 0.677249
2017-12-10T15:41:14.197797: step 211, loss 1.21795, acc 0.71875, prec 0.0227393, recall 0.678947
2017-12-10T15:41:14.397845: step 212, loss 1.28763, acc 0.734375, prec 0.0228431, recall 0.680628
2017-12-10T15:41:14.593051: step 213, loss 1.88548, acc 0.625, prec 0.0232599, recall 0.685567
2017-12-10T15:41:14.788353: step 214, loss 2.63611, acc 0.484375, prec 0.0231264, recall 0.685567
2017-12-10T15:41:14.983487: step 215, loss 1.51945, acc 0.65625, prec 0.0233766, recall 0.688776
2017-12-10T15:41:15.178898: step 216, loss 2.36576, acc 0.609375, prec 0.0234442, recall 0.690355
2017-12-10T15:41:15.372744: step 217, loss 1.9489, acc 0.703125, prec 0.0233677, recall 0.690355
2017-12-10T15:41:15.564801: step 218, loss 1.79661, acc 0.59375, prec 0.0237647, recall 0.695
2017-12-10T15:41:15.760740: step 219, loss 1.63264, acc 0.640625, prec 0.0236717, recall 0.695
2017-12-10T15:41:15.953666: step 220, loss 1.64183, acc 0.625, prec 0.0235753, recall 0.695
2017-12-10T15:41:16.155145: step 221, loss 1.49035, acc 0.703125, prec 0.0239946, recall 0.699507
2017-12-10T15:41:16.350850: step 222, loss 1.92109, acc 0.703125, prec 0.0244108, recall 0.703883
2017-12-10T15:41:16.555106: step 223, loss 1.94365, acc 0.671875, prec 0.0248156, recall 0.708134
2017-12-10T15:41:16.748377: step 224, loss 6.71439, acc 0.75, prec 0.0247533, recall 0.704762
2017-12-10T15:41:16.948427: step 225, loss 1.11654, acc 0.6875, prec 0.0246708, recall 0.704762
2017-12-10T15:41:17.152037: step 226, loss 7.29826, acc 0.6875, prec 0.0247549, recall 0.70283
2017-12-10T15:41:17.347186: step 227, loss 1.21497, acc 0.734375, prec 0.0246852, recall 0.70283
2017-12-10T15:41:17.542555: step 228, loss 1.37029, acc 0.640625, prec 0.0245915, recall 0.70283
2017-12-10T15:41:17.743228: step 229, loss 1.76396, acc 0.625, prec 0.0244945, recall 0.70283
2017-12-10T15:41:17.942236: step 230, loss 2.20486, acc 0.625, prec 0.024558, recall 0.704225
2017-12-10T15:41:18.136619: step 231, loss 0.84279, acc 0.8125, prec 0.0248285, recall 0.706977
2017-12-10T15:41:18.332051: step 232, loss 1.27534, acc 0.71875, prec 0.0247557, recall 0.706977
2017-12-10T15:41:18.530492: step 233, loss 1.06999, acc 0.6875, prec 0.0246753, recall 0.706977
2017-12-10T15:41:18.720634: step 234, loss 1.44493, acc 0.65625, prec 0.0245875, recall 0.706977
2017-12-10T15:41:18.915318: step 235, loss 3.10842, acc 0.65625, prec 0.0245043, recall 0.703704
2017-12-10T15:41:19.109007: step 236, loss 18.75, acc 0.640625, prec 0.0245744, recall 0.701835
2017-12-10T15:41:19.303343: step 237, loss 1.73296, acc 0.609375, prec 0.0244761, recall 0.701835
2017-12-10T15:41:19.495823: step 238, loss 1.33727, acc 0.734375, prec 0.0244097, recall 0.701835
2017-12-10T15:41:19.691984: step 239, loss 0.785825, acc 0.828125, prec 0.0245223, recall 0.703196
2017-12-10T15:41:19.891554: step 240, loss 1.48078, acc 0.703125, prec 0.0249127, recall 0.707207
2017-12-10T15:41:20.087438: step 241, loss 1.04577, acc 0.6875, prec 0.0248339, recall 0.707207
2017-12-10T15:41:20.285295: step 242, loss 1.56711, acc 0.609375, prec 0.0247361, recall 0.707207
2017-12-10T15:41:20.479475: step 243, loss 1.2419, acc 0.625, prec 0.024796, recall 0.70852
2017-12-10T15:41:20.672750: step 244, loss 1.45017, acc 0.734375, prec 0.0250352, recall 0.711111
2017-12-10T15:41:20.873707: step 245, loss 6.26343, acc 0.78125, prec 0.0249844, recall 0.707965
2017-12-10T15:41:21.079806: step 246, loss 11.652, acc 0.71875, prec 0.0249182, recall 0.704846
2017-12-10T15:41:21.277586: step 247, loss 1.25026, acc 0.765625, prec 0.0253145, recall 0.708696
2017-12-10T15:41:21.472067: step 248, loss 1.36291, acc 0.59375, prec 0.0255141, recall 0.711207
2017-12-10T15:41:21.674536: step 249, loss 1.24896, acc 0.65625, prec 0.0257279, recall 0.713675
2017-12-10T15:41:21.868747: step 250, loss 1.60654, acc 0.625, prec 0.0256332, recall 0.713675
2017-12-10T15:41:22.059866: step 251, loss 1.34593, acc 0.671875, prec 0.0255508, recall 0.713675
2017-12-10T15:41:22.254815: step 252, loss 0.723811, acc 0.796875, prec 0.0256489, recall 0.714894
2017-12-10T15:41:22.448875: step 253, loss 1.57648, acc 0.640625, prec 0.0255591, recall 0.714894
2017-12-10T15:41:22.645968: step 254, loss 1.33171, acc 0.78125, prec 0.0259484, recall 0.718487
2017-12-10T15:41:22.839512: step 255, loss 1.39957, acc 0.734375, prec 0.0260291, recall 0.719665
2017-12-10T15:41:23.041332: step 256, loss 1.58586, acc 0.625, prec 0.0259349, recall 0.719665
2017-12-10T15:41:23.235155: step 257, loss 1.32777, acc 0.71875, prec 0.0258647, recall 0.719665
2017-12-10T15:41:23.428707: step 258, loss 10.6965, acc 0.71875, prec 0.0259448, recall 0.717842
2017-12-10T15:41:23.625556: step 259, loss 1.47606, acc 0.71875, prec 0.0260206, recall 0.719008
2017-12-10T15:41:23.820348: step 260, loss 6.49061, acc 0.703125, prec 0.0263864, recall 0.719512
2017-12-10T15:41:24.016502: step 261, loss 2.05432, acc 0.5625, prec 0.0262767, recall 0.719512
2017-12-10T15:41:24.211467: step 262, loss 2.13528, acc 0.578125, prec 0.0261718, recall 0.719512
2017-12-10T15:41:24.408336: step 263, loss 1.38191, acc 0.578125, prec 0.0260677, recall 0.719512
2017-12-10T15:41:24.602739: step 264, loss 1.76075, acc 0.6875, prec 0.0261342, recall 0.720648
2017-12-10T15:41:24.795780: step 265, loss 1.54557, acc 0.59375, prec 0.0260348, recall 0.720648
2017-12-10T15:41:24.994925: step 266, loss 1.725, acc 0.515625, prec 0.0260591, recall 0.721774
2017-12-10T15:41:25.187135: step 267, loss 1.742, acc 0.5625, prec 0.0260945, recall 0.722892
2017-12-10T15:41:25.380211: step 268, loss 1.54771, acc 0.640625, prec 0.0261485, recall 0.724
2017-12-10T15:41:25.576252: step 269, loss 1.65552, acc 0.546875, prec 0.0260394, recall 0.724
2017-12-10T15:41:25.770907: step 270, loss 1.69895, acc 0.625, prec 0.0260895, recall 0.7251
2017-12-10T15:41:25.962062: step 271, loss 4.44591, acc 0.671875, prec 0.0261578, recall 0.720472
2017-12-10T15:41:26.158096: step 272, loss 1.45973, acc 0.625, prec 0.0260684, recall 0.720472
2017-12-10T15:41:26.356987: step 273, loss 1.59879, acc 0.75, prec 0.0262859, recall 0.722656
2017-12-10T15:41:26.553752: step 274, loss 1.11689, acc 0.65625, prec 0.026204, recall 0.722656
2017-12-10T15:41:26.749813: step 275, loss 1.23638, acc 0.734375, prec 0.026141, recall 0.722656
2017-12-10T15:41:26.941556: step 276, loss 1.8111, acc 0.59375, prec 0.0260453, recall 0.722656
2017-12-10T15:41:27.134126: step 277, loss 1.02915, acc 0.796875, prec 0.0259978, recall 0.722656
2017-12-10T15:41:27.327980: step 278, loss 0.875551, acc 0.765625, prec 0.0259431, recall 0.722656
2017-12-10T15:41:27.526827: step 279, loss 0.663576, acc 0.828125, prec 0.0259031, recall 0.722656
2017-12-10T15:41:27.721621: step 280, loss 1.04276, acc 0.765625, prec 0.0259849, recall 0.723735
2017-12-10T15:41:27.910549: step 281, loss 0.549353, acc 0.859375, prec 0.0259523, recall 0.723735
2017-12-10T15:41:28.109521: step 282, loss 1.49887, acc 0.84375, prec 0.0261875, recall 0.725869
2017-12-10T15:41:28.313599: step 283, loss 6.84061, acc 0.84375, prec 0.0261547, recall 0.723077
2017-12-10T15:41:28.511921: step 284, loss 0.479609, acc 0.828125, prec 0.0261147, recall 0.723077
2017-12-10T15:41:28.709857: step 285, loss 10.7282, acc 0.84375, prec 0.0262172, recall 0.721374
2017-12-10T15:41:28.910420: step 286, loss 0.893342, acc 0.765625, prec 0.0261628, recall 0.721374
2017-12-10T15:41:29.105476: step 287, loss 1.03515, acc 0.796875, prec 0.0261158, recall 0.721374
2017-12-10T15:41:29.304617: step 288, loss 1.41568, acc 0.671875, prec 0.0260402, recall 0.721374
2017-12-10T15:41:29.500720: step 289, loss 2.66272, acc 0.65625, prec 0.0260989, recall 0.719697
2017-12-10T15:41:29.698192: step 290, loss 0.853066, acc 0.8125, prec 0.026056, recall 0.719697
2017-12-10T15:41:29.891948: step 291, loss 9.42769, acc 0.671875, prec 0.0259847, recall 0.716981
2017-12-10T15:41:30.094131: step 292, loss 3.97952, acc 0.578125, prec 0.0258926, recall 0.714286
2017-12-10T15:41:30.290975: step 293, loss 1.85798, acc 0.53125, prec 0.0261837, recall 0.717472
2017-12-10T15:41:30.486018: step 294, loss 2.38297, acc 0.515625, prec 0.026074, recall 0.717472
2017-12-10T15:41:30.680966: step 295, loss 2.66612, acc 0.390625, prec 0.0259374, recall 0.717472
2017-12-10T15:41:30.873838: step 296, loss 2.83678, acc 0.453125, prec 0.0258159, recall 0.717472
2017-12-10T15:41:31.070699: step 297, loss 1.95926, acc 0.546875, prec 0.0257162, recall 0.717472
2017-12-10T15:41:31.274833: step 298, loss 2.65588, acc 0.375, prec 0.0255799, recall 0.717472
2017-12-10T15:41:31.470618: step 299, loss 2.26948, acc 0.546875, prec 0.0256106, recall 0.718518
2017-12-10T15:41:31.667746: step 300, loss 3.89225, acc 0.625, prec 0.025533, recall 0.715867
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_3/1512938428/checkpoints/model-300

2017-12-10T15:41:32.867752: step 301, loss 2.41623, acc 0.5, prec 0.025809, recall 0.718978
2017-12-10T15:41:33.067091: step 302, loss 2.2034, acc 0.484375, prec 0.025825, recall 0.72
2017-12-10T15:41:33.261094: step 303, loss 2.81709, acc 0.65625, prec 0.0258811, recall 0.718412
2017-12-10T15:41:33.455354: step 304, loss 1.52514, acc 0.71875, prec 0.0258207, recall 0.718412
2017-12-10T15:41:33.655916: step 305, loss 5.99585, acc 0.625, prec 0.0258699, recall 0.716846
2017-12-10T15:41:33.848597: step 306, loss 1.12191, acc 0.625, prec 0.0259154, recall 0.717857
2017-12-10T15:41:34.044288: step 307, loss 11.5908, acc 0.609375, prec 0.0258388, recall 0.712766
2017-12-10T15:41:34.241734: step 308, loss 1.72029, acc 0.609375, prec 0.0258808, recall 0.713781
2017-12-10T15:41:34.436507: step 309, loss 2.44704, acc 0.5625, prec 0.0259127, recall 0.714789
2017-12-10T15:41:34.631105: step 310, loss 2.47361, acc 0.40625, prec 0.0259113, recall 0.715789
2017-12-10T15:41:34.824924: step 311, loss 2.44357, acc 0.515625, prec 0.0261793, recall 0.71875
2017-12-10T15:41:35.020817: step 312, loss 2.51989, acc 0.53125, prec 0.0263257, recall 0.72069
2017-12-10T15:41:35.220752: step 313, loss 2.20539, acc 0.5, prec 0.02622, recall 0.72069
2017-12-10T15:41:35.416896: step 314, loss 2.00319, acc 0.578125, prec 0.0261315, recall 0.72069
2017-12-10T15:41:35.612770: step 315, loss 1.97689, acc 0.484375, prec 0.0260242, recall 0.72069
2017-12-10T15:41:35.808984: step 316, loss 2.18791, acc 0.5625, prec 0.0260546, recall 0.721649
2017-12-10T15:41:36.009479: step 317, loss 1.7039, acc 0.53125, prec 0.0261987, recall 0.723549
2017-12-10T15:41:36.209462: step 318, loss 2.61533, acc 0.53125, prec 0.0261019, recall 0.723549
2017-12-10T15:41:36.402575: step 319, loss 1.69157, acc 0.65625, prec 0.0262706, recall 0.725424
2017-12-10T15:41:36.595237: step 320, loss 1.05039, acc 0.6875, prec 0.0263255, recall 0.726351
2017-12-10T15:41:36.796588: step 321, loss 1.46956, acc 0.703125, prec 0.0263833, recall 0.727273
2017-12-10T15:41:36.991945: step 322, loss 0.857518, acc 0.8125, prec 0.0263447, recall 0.727273
2017-12-10T15:41:37.188746: step 323, loss 0.607664, acc 0.875, prec 0.026319, recall 0.727273
2017-12-10T15:41:37.383661: step 324, loss 1.3956, acc 0.875, prec 0.0264119, recall 0.728188
2017-12-10T15:41:37.579891: step 325, loss 0.690796, acc 0.828125, prec 0.0264949, recall 0.729097
2017-12-10T15:41:37.777093: step 326, loss 0.491154, acc 0.828125, prec 0.0264595, recall 0.729097
2017-12-10T15:41:37.974275: step 327, loss 0.371666, acc 0.90625, prec 0.0264403, recall 0.729097
2017-12-10T15:41:38.168488: step 328, loss 0.180872, acc 0.9375, prec 0.0265455, recall 0.73
2017-12-10T15:41:38.363010: step 329, loss 21.2843, acc 0.890625, prec 0.0265262, recall 0.727575
2017-12-10T15:41:38.560982: step 330, loss 4.58114, acc 0.921875, prec 0.0266312, recall 0.726073
2017-12-10T15:41:38.764294: step 331, loss 9.39006, acc 0.875, prec 0.0266086, recall 0.723684
2017-12-10T15:41:38.960631: step 332, loss 15.6356, acc 0.828125, prec 0.0265829, recall 0.716612
2017-12-10T15:41:39.155043: step 333, loss 4.2073, acc 0.8125, prec 0.0265476, recall 0.714286
2017-12-10T15:41:39.347811: step 334, loss 1.31119, acc 0.640625, prec 0.0264741, recall 0.714286
2017-12-10T15:41:39.545500: step 335, loss 2.43837, acc 0.5, prec 0.0264893, recall 0.71521
2017-12-10T15:41:39.742924: step 336, loss 3.24611, acc 0.421875, prec 0.0263723, recall 0.71521
2017-12-10T15:41:39.937205: step 337, loss 4.7871, acc 0.40625, prec 0.0264846, recall 0.717042
2017-12-10T15:41:40.135992: step 338, loss 3.62762, acc 0.359375, prec 0.0264713, recall 0.717949
2017-12-10T15:41:40.335768: step 339, loss 4.89151, acc 0.234375, prec 0.0263189, recall 0.717949
2017-12-10T15:41:40.533174: step 340, loss 4.64852, acc 0.25, prec 0.0261713, recall 0.717949
2017-12-10T15:41:40.728655: step 341, loss 4.45641, acc 0.265625, prec 0.0262546, recall 0.719745
2017-12-10T15:41:40.923734: step 342, loss 4.84195, acc 0.1875, prec 0.026097, recall 0.719745
2017-12-10T15:41:41.115697: step 343, loss 4.05985, acc 0.296875, prec 0.0261858, recall 0.721519
2017-12-10T15:41:41.312074: step 344, loss 4.26093, acc 0.359375, prec 0.0262857, recall 0.72327
2017-12-10T15:41:41.505742: step 345, loss 3.72754, acc 0.265625, prec 0.026256, recall 0.724138
2017-12-10T15:41:41.697074: step 346, loss 3.00616, acc 0.328125, prec 0.0261283, recall 0.724138
2017-12-10T15:41:41.887975: step 347, loss 2.54882, acc 0.5, prec 0.026034, recall 0.724138
2017-12-10T15:41:42.085294: step 348, loss 1.71457, acc 0.609375, prec 0.0261798, recall 0.725857
2017-12-10T15:41:42.279318: step 349, loss 2.05661, acc 0.625, prec 0.0262185, recall 0.726708
2017-12-10T15:41:42.469550: step 350, loss 1.23696, acc 0.703125, prec 0.0262717, recall 0.727554
2017-12-10T15:41:42.665725: step 351, loss 1.25504, acc 0.734375, prec 0.0262218, recall 0.727554
2017-12-10T15:41:42.858816: step 352, loss 0.696515, acc 0.78125, prec 0.0261809, recall 0.727554
2017-12-10T15:41:43.057448: step 353, loss 1.34526, acc 0.78125, prec 0.0262485, recall 0.728395
2017-12-10T15:41:43.255654: step 354, loss 0.589458, acc 0.796875, prec 0.0262106, recall 0.728395
2017-12-10T15:41:43.452353: step 355, loss 0.425729, acc 0.8125, prec 0.0261757, recall 0.728395
2017-12-10T15:41:43.642536: step 356, loss 0.141414, acc 0.921875, prec 0.0261612, recall 0.728395
2017-12-10T15:41:43.832794: step 357, loss 0.710596, acc 0.78125, prec 0.0262284, recall 0.729231
2017-12-10T15:41:44.029693: step 358, loss 0.323082, acc 0.875, prec 0.0262052, recall 0.729231
2017-12-10T15:41:44.232145: step 359, loss 0.132007, acc 0.96875, prec 0.0261994, recall 0.729231
2017-12-10T15:41:44.434636: step 360, loss 12.3954, acc 0.875, prec 0.0261821, recall 0.724771
2017-12-10T15:41:44.635135: step 361, loss 7.20515, acc 0.90625, prec 0.0262751, recall 0.723404
2017-12-10T15:41:44.833611: step 362, loss 0.290533, acc 0.890625, prec 0.0262548, recall 0.723404
2017-12-10T15:41:45.026974: step 363, loss 0.240931, acc 0.921875, prec 0.0262404, recall 0.723404
2017-12-10T15:41:45.225019: step 364, loss 0.422554, acc 0.921875, prec 0.0262259, recall 0.723404
2017-12-10T15:41:45.418780: step 365, loss 0.711396, acc 0.90625, prec 0.0263158, recall 0.724242
2017-12-10T15:41:45.614003: step 366, loss 0.465269, acc 0.90625, prec 0.0262984, recall 0.724242
2017-12-10T15:41:45.807728: step 367, loss 0.471902, acc 0.84375, prec 0.0262695, recall 0.724242
2017-12-10T15:41:46.005033: step 368, loss 0.449039, acc 0.875, prec 0.0263534, recall 0.725076
2017-12-10T15:41:46.196212: step 369, loss 8.62391, acc 0.796875, prec 0.0263216, recall 0.720721
2017-12-10T15:41:46.394670: step 370, loss 0.418095, acc 0.796875, prec 0.0262841, recall 0.720721
2017-12-10T15:41:46.589791: step 371, loss 0.776074, acc 0.796875, prec 0.0264597, recall 0.722388
2017-12-10T15:41:46.785207: step 372, loss 1.21966, acc 0.78125, prec 0.0265255, recall 0.723214
2017-12-10T15:41:46.982682: step 373, loss 0.788466, acc 0.828125, prec 0.0265998, recall 0.724036
2017-12-10T15:41:47.180968: step 374, loss 1.0574, acc 0.765625, prec 0.0265564, recall 0.724036
2017-12-10T15:41:47.379037: step 375, loss 1.15788, acc 0.671875, prec 0.0266015, recall 0.724852
2017-12-10T15:41:47.576300: step 376, loss 2.94423, acc 0.6875, prec 0.0267548, recall 0.726471
2017-12-10T15:41:47.770783: step 377, loss 4.16961, acc 0.5625, prec 0.0266767, recall 0.72434
2017-12-10T15:41:47.967226: step 378, loss 1.3678, acc 0.6875, prec 0.0266192, recall 0.72434
2017-12-10T15:41:48.159731: step 379, loss 1.59854, acc 0.703125, prec 0.0265649, recall 0.72434
2017-12-10T15:41:48.355010: step 380, loss 3.07689, acc 0.59375, prec 0.0266996, recall 0.725947
2017-12-10T15:41:48.551790: step 381, loss 2.07925, acc 0.5625, prec 0.0268277, recall 0.727536
2017-12-10T15:41:48.743954: step 382, loss 2.97035, acc 0.5, prec 0.0268399, recall 0.728324
2017-12-10T15:41:48.939634: step 383, loss 2.02059, acc 0.5625, prec 0.0267601, recall 0.728324
2017-12-10T15:41:49.138304: step 384, loss 3.09125, acc 0.59375, prec 0.0268925, recall 0.729885
2017-12-10T15:41:49.334480: step 385, loss 2.05144, acc 0.484375, prec 0.0267989, recall 0.729885
2017-12-10T15:41:49.533795: step 386, loss 4.28976, acc 0.625, prec 0.0268365, recall 0.728571
2017-12-10T15:41:49.734973: step 387, loss 1.79044, acc 0.625, prec 0.026871, recall 0.729345
2017-12-10T15:41:49.927784: step 388, loss 1.61257, acc 0.71875, prec 0.0269223, recall 0.730114
2017-12-10T15:41:50.122291: step 389, loss 7.67763, acc 0.4375, prec 0.0268239, recall 0.728045
2017-12-10T15:41:50.319504: step 390, loss 3.89923, acc 0.5, prec 0.0267374, recall 0.725989
2017-12-10T15:41:50.519332: step 391, loss 3.1337, acc 0.5, prec 0.0267496, recall 0.726761
2017-12-10T15:41:50.722114: step 392, loss 2.04232, acc 0.640625, prec 0.0268873, recall 0.728291
2017-12-10T15:41:50.920951: step 393, loss 2.43894, acc 0.546875, prec 0.0268069, recall 0.728291
2017-12-10T15:41:51.116078: step 394, loss 2.38606, acc 0.625, prec 0.0267407, recall 0.728291
2017-12-10T15:41:51.312172: step 395, loss 2.63259, acc 0.53125, prec 0.0266585, recall 0.728291
2017-12-10T15:41:51.504066: step 396, loss 2.07717, acc 0.578125, prec 0.0266844, recall 0.72905
2017-12-10T15:41:51.697267: step 397, loss 2.48076, acc 0.625, prec 0.0266191, recall 0.72905
2017-12-10T15:41:51.895667: step 398, loss 1.03802, acc 0.765625, prec 0.0267766, recall 0.730556
2017-12-10T15:41:52.088007: step 399, loss 2.41109, acc 0.5, prec 0.0266897, recall 0.730556
2017-12-10T15:41:52.279562: step 400, loss 1.48726, acc 0.6875, prec 0.0266356, recall 0.730556
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_3/1512938428/checkpoints/model-400

2017-12-10T15:41:53.434853: step 401, loss 14.3236, acc 0.75, prec 0.026792, recall 0.730028
2017-12-10T15:41:53.632482: step 402, loss 6.11249, acc 0.6875, prec 0.0267407, recall 0.728022
2017-12-10T15:41:53.830836: step 403, loss 9.41974, acc 0.71875, prec 0.0266976, recall 0.724044
2017-12-10T15:41:54.027531: step 404, loss 1.92486, acc 0.46875, prec 0.0266064, recall 0.724044
2017-12-10T15:41:54.225313: step 405, loss 1.54144, acc 0.53125, prec 0.0265265, recall 0.724044
2017-12-10T15:41:54.418441: step 406, loss 2.68553, acc 0.515625, prec 0.0264445, recall 0.724044
2017-12-10T15:41:54.612431: step 407, loss 1.83374, acc 0.578125, prec 0.0264703, recall 0.724796
2017-12-10T15:41:54.806258: step 408, loss 3.32418, acc 0.5, prec 0.0264855, recall 0.723577
2017-12-10T15:41:55.007072: step 409, loss 2.60411, acc 0.484375, prec 0.0264953, recall 0.724324
2017-12-10T15:41:55.202323: step 410, loss 2.54994, acc 0.515625, prec 0.0264143, recall 0.724324
2017-12-10T15:41:55.394035: step 411, loss 2.67479, acc 0.59375, prec 0.0267296, recall 0.727273
2017-12-10T15:41:55.586124: step 412, loss 2.67112, acc 0.421875, prec 0.0268233, recall 0.728723
2017-12-10T15:41:55.781560: step 413, loss 3.13607, acc 0.5, prec 0.0268345, recall 0.729443
2017-12-10T15:41:55.973825: step 414, loss 1.63031, acc 0.59375, prec 0.0267666, recall 0.729443
2017-12-10T15:41:56.171896: step 415, loss 1.91813, acc 0.5625, prec 0.0267883, recall 0.730159
2017-12-10T15:41:56.374628: step 416, loss 1.46799, acc 0.625, prec 0.0267261, recall 0.730159
2017-12-10T15:41:56.570905: step 417, loss 5.86613, acc 0.515625, prec 0.0268366, recall 0.729659
2017-12-10T15:41:56.771270: step 418, loss 1.36483, acc 0.65625, prec 0.0268734, recall 0.730366
2017-12-10T15:41:56.969875: step 419, loss 4.33267, acc 0.6875, prec 0.0269179, recall 0.729167
2017-12-10T15:41:57.167052: step 420, loss 1.88526, acc 0.515625, prec 0.0268379, recall 0.729167
2017-12-10T15:41:57.364251: step 421, loss 1.90125, acc 0.640625, prec 0.026872, recall 0.72987
2017-12-10T15:41:57.560534: step 422, loss 2.29453, acc 0.5, prec 0.0270682, recall 0.731959
2017-12-10T15:41:57.754124: step 423, loss 11.248, acc 0.703125, prec 0.027117, recall 0.7289
2017-12-10T15:41:57.950518: step 424, loss 2.03593, acc 0.53125, prec 0.0270398, recall 0.7289
2017-12-10T15:41:58.149910: step 425, loss 1.83708, acc 0.609375, prec 0.0269759, recall 0.7289
2017-12-10T15:41:58.349796: step 426, loss 1.91254, acc 0.5625, prec 0.0269964, recall 0.729592
2017-12-10T15:41:58.546528: step 427, loss 4.439, acc 0.609375, prec 0.0269354, recall 0.727735
2017-12-10T15:41:58.739663: step 428, loss 1.43388, acc 0.59375, prec 0.0268696, recall 0.727735
2017-12-10T15:41:58.930303: step 429, loss 2.8424, acc 0.546875, prec 0.0269789, recall 0.729114
2017-12-10T15:41:59.128267: step 430, loss 2.51163, acc 0.515625, prec 0.0269008, recall 0.729114
2017-12-10T15:41:59.327334: step 431, loss 1.76672, acc 0.65625, prec 0.0268456, recall 0.729114
2017-12-10T15:41:59.526201: step 432, loss 1.77358, acc 0.59375, prec 0.0269617, recall 0.730479
2017-12-10T15:41:59.718432: step 433, loss 3.62996, acc 0.625, prec 0.0270847, recall 0.73
2017-12-10T15:41:59.912824: step 434, loss 2.13483, acc 0.5, prec 0.0271845, recall 0.731343
2017-12-10T15:42:00.107534: step 435, loss 2.0604, acc 0.53125, prec 0.027199, recall 0.73201
2017-12-10T15:42:00.303339: step 436, loss 1.3842, acc 0.640625, prec 0.0272309, recall 0.732673
2017-12-10T15:42:00.495067: step 437, loss 1.1422, acc 0.65625, prec 0.0274438, recall 0.734644
2017-12-10T15:42:00.691622: step 438, loss 1.28455, acc 0.75, prec 0.0274035, recall 0.734644
2017-12-10T15:42:00.891538: step 439, loss 1.96714, acc 0.59375, prec 0.0273384, recall 0.734644
2017-12-10T15:42:01.090452: step 440, loss 1.77558, acc 0.5625, prec 0.0273573, recall 0.735294
2017-12-10T15:42:01.282922: step 441, loss 1.17496, acc 0.640625, prec 0.0273885, recall 0.735941
2017-12-10T15:42:01.482592: step 442, loss 1.08132, acc 0.765625, prec 0.0275279, recall 0.737226
2017-12-10T15:42:01.680859: step 443, loss 1.89746, acc 0.78125, prec 0.0274955, recall 0.735437
2017-12-10T15:42:01.875898: step 444, loss 3.82898, acc 0.8125, prec 0.0275562, recall 0.7343
2017-12-10T15:42:02.074798: step 445, loss 0.655172, acc 0.859375, prec 0.0275337, recall 0.7343
2017-12-10T15:42:02.273142: step 446, loss 8.98372, acc 0.75, prec 0.0275868, recall 0.731415
2017-12-10T15:42:02.474382: step 447, loss 2.98731, acc 0.6875, prec 0.0275395, recall 0.729665
2017-12-10T15:42:02.669302: step 448, loss 1.8421, acc 0.75, prec 0.0275875, recall 0.73031
2017-12-10T15:42:02.867035: step 449, loss 1.5645, acc 0.703125, prec 0.0275403, recall 0.73031
2017-12-10T15:42:03.061809: step 450, loss 2.0684, acc 0.59375, prec 0.027476, recall 0.73031
2017-12-10T15:42:03.257280: step 451, loss 1.28806, acc 0.65625, prec 0.0274218, recall 0.73031
2017-12-10T15:42:03.457170: step 452, loss 1.6526, acc 0.515625, prec 0.0273458, recall 0.73031
2017-12-10T15:42:03.659425: step 453, loss 1.72851, acc 0.59375, prec 0.0274559, recall 0.731591
2017-12-10T15:42:03.851505: step 454, loss 3.20793, acc 0.546875, prec 0.027474, recall 0.730496
2017-12-10T15:42:04.046748: step 455, loss 3.04677, acc 0.5, prec 0.027396, recall 0.730496
2017-12-10T15:42:04.239486: step 456, loss 2.28967, acc 0.46875, prec 0.0273137, recall 0.730496
2017-12-10T15:42:04.434106: step 457, loss 2.52331, acc 0.546875, prec 0.0274154, recall 0.731765
2017-12-10T15:42:04.625594: step 458, loss 2.84315, acc 0.46875, prec 0.0273335, recall 0.731765
2017-12-10T15:42:04.825952: step 459, loss 2.17626, acc 0.5625, prec 0.0273516, recall 0.732394
2017-12-10T15:42:05.023186: step 460, loss 7.8806, acc 0.625, prec 0.0273817, recall 0.731308
2017-12-10T15:42:05.223803: step 461, loss 1.78544, acc 0.5625, prec 0.0275694, recall 0.733179
2017-12-10T15:42:05.417488: step 462, loss 1.92909, acc 0.625, prec 0.0275964, recall 0.733796
2017-12-10T15:42:05.612456: step 463, loss 4.69928, acc 0.703125, prec 0.0275532, recall 0.732102
2017-12-10T15:42:05.804906: step 464, loss 2.2758, acc 0.484375, prec 0.0274744, recall 0.732102
2017-12-10T15:42:05.998369: step 465, loss 1.93728, acc 0.625, prec 0.0274174, recall 0.732102
2017-12-10T15:42:06.193845: step 466, loss 1.37509, acc 0.671875, prec 0.0273677, recall 0.732102
2017-12-10T15:42:06.391841: step 467, loss 1.57126, acc 0.609375, prec 0.0273088, recall 0.732102
2017-12-10T15:42:06.589244: step 468, loss 1.35373, acc 0.65625, prec 0.0272571, recall 0.732102
2017-12-10T15:42:06.797583: step 469, loss 1.5564, acc 0.71875, prec 0.0272985, recall 0.732719
2017-12-10T15:42:06.991815: step 470, loss 1.09169, acc 0.75, prec 0.027261, recall 0.732719
2017-12-10T15:42:07.187117: step 471, loss 1.57708, acc 0.640625, prec 0.0272074, recall 0.732719
2017-12-10T15:42:07.378550: step 472, loss 4.10619, acc 0.703125, prec 0.0273317, recall 0.732265
2017-12-10T15:42:07.577799: step 473, loss 0.871399, acc 0.734375, prec 0.0272921, recall 0.732265
2017-12-10T15:42:07.774481: step 474, loss 11.2929, acc 0.703125, prec 0.0272503, recall 0.730594
2017-12-10T15:42:07.972966: step 475, loss 1.27359, acc 0.71875, prec 0.0272913, recall 0.731207
2017-12-10T15:42:08.166621: step 476, loss 0.652656, acc 0.796875, prec 0.0272611, recall 0.731207
2017-12-10T15:42:08.363299: step 477, loss 1.6858, acc 0.65625, prec 0.0272928, recall 0.731818
2017-12-10T15:42:08.561809: step 478, loss 20.3907, acc 0.75, prec 0.0272581, recall 0.730159
2017-12-10T15:42:08.758195: step 479, loss 0.974503, acc 0.71875, prec 0.0272989, recall 0.730769
2017-12-10T15:42:08.954685: step 480, loss 1.30374, acc 0.71875, prec 0.0274215, recall 0.731982
2017-12-10T15:42:09.151562: step 481, loss 8.3677, acc 0.734375, prec 0.0273869, recall 0.7287
2017-12-10T15:42:09.350401: step 482, loss 15.2256, acc 0.671875, prec 0.027509, recall 0.725055
2017-12-10T15:42:09.544040: step 483, loss 1.90351, acc 0.65625, prec 0.0276215, recall 0.726269
2017-12-10T15:42:09.742058: step 484, loss 2.42354, acc 0.4375, prec 0.0275383, recall 0.726269
2017-12-10T15:42:09.938906: step 485, loss 2.69174, acc 0.40625, prec 0.0275321, recall 0.726872
2017-12-10T15:42:10.128840: step 486, loss 3.27198, acc 0.34375, prec 0.027436, recall 0.726872
2017-12-10T15:42:10.325267: step 487, loss 3.14574, acc 0.375, prec 0.027345, recall 0.726872
2017-12-10T15:42:10.520807: step 488, loss 4.10383, acc 0.1875, prec 0.0272277, recall 0.726872
2017-12-10T15:42:10.713948: step 489, loss 4.54787, acc 0.25, prec 0.0273601, recall 0.728665
2017-12-10T15:42:10.910260: step 490, loss 3.91913, acc 0.265625, prec 0.0272549, recall 0.728665
2017-12-10T15:42:11.106416: step 491, loss 3.81058, acc 0.34375, prec 0.0273202, recall 0.729847
2017-12-10T15:42:11.301667: step 492, loss 4.31349, acc 0.203125, prec 0.027207, recall 0.729847
2017-12-10T15:42:11.492390: step 493, loss 3.4958, acc 0.25, prec 0.0271801, recall 0.730435
2017-12-10T15:42:11.688383: step 494, loss 2.73563, acc 0.4375, prec 0.0271796, recall 0.731019
2017-12-10T15:42:11.887862: step 495, loss 2.75566, acc 0.390625, prec 0.0270944, recall 0.731019
2017-12-10T15:42:12.083989: step 496, loss 1.92791, acc 0.46875, prec 0.0270205, recall 0.731019
2017-12-10T15:42:12.262893: step 497, loss 1.17406, acc 0.673077, prec 0.0271395, recall 0.732181
2017-12-10T15:42:12.467448: step 498, loss 1.21401, acc 0.65625, prec 0.0271696, recall 0.732759
2017-12-10T15:42:12.664608: step 499, loss 1.22661, acc 0.703125, prec 0.027206, recall 0.733333
2017-12-10T15:42:12.859133: step 500, loss 5.31319, acc 0.734375, prec 0.0271713, recall 0.73176
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_3/1512938428/checkpoints/model-500

2017-12-10T15:42:14.049966: step 501, loss 3.82904, acc 0.734375, prec 0.0271367, recall 0.730193
2017-12-10T15:42:14.254381: step 502, loss 1.33848, acc 0.640625, prec 0.0270871, recall 0.730193
2017-12-10T15:42:14.455070: step 503, loss 0.715057, acc 0.828125, prec 0.0270635, recall 0.730193
2017-12-10T15:42:14.647512: step 504, loss 2.10704, acc 0.71875, prec 0.027102, recall 0.730769
2017-12-10T15:42:14.842118: step 505, loss 0.644072, acc 0.796875, prec 0.0270741, recall 0.730769
2017-12-10T15:42:15.036478: step 506, loss 0.673583, acc 0.796875, prec 0.027277, recall 0.732484
2017-12-10T15:42:15.238537: step 507, loss 1.21218, acc 0.703125, prec 0.0273897, recall 0.733615
2017-12-10T15:42:15.432285: step 508, loss 0.464525, acc 0.8125, prec 0.0273638, recall 0.733615
2017-12-10T15:42:15.630013: step 509, loss 0.786893, acc 0.828125, prec 0.0274167, recall 0.734177
2017-12-10T15:42:15.823620: step 510, loss 0.502387, acc 0.828125, prec 0.0273929, recall 0.734177
2017-12-10T15:42:16.023082: step 511, loss 0.748811, acc 0.78125, prec 0.0273628, recall 0.734177
2017-12-10T15:42:16.217598: step 512, loss 1.94078, acc 0.828125, prec 0.0273413, recall 0.732632
2017-12-10T15:42:16.414433: step 513, loss 0.57159, acc 0.8125, prec 0.0273919, recall 0.733193
2017-12-10T15:42:16.607121: step 514, loss 0.714492, acc 0.859375, prec 0.0273725, recall 0.733193
2017-12-10T15:42:16.802715: step 515, loss 0.539447, acc 0.84375, prec 0.0273511, recall 0.733193
2017-12-10T15:42:16.996667: step 516, loss 0.217113, acc 0.921875, prec 0.0273404, recall 0.733193
2017-12-10T15:42:17.188911: step 517, loss 14.0502, acc 0.9375, prec 0.0274101, recall 0.732218
2017-12-10T15:42:17.385416: step 518, loss 0.967594, acc 0.859375, prec 0.0276191, recall 0.733888
2017-12-10T15:42:17.582111: step 519, loss 0.858816, acc 0.828125, prec 0.0277474, recall 0.73499
2017-12-10T15:42:17.780575: step 520, loss 0.385072, acc 0.875, prec 0.027806, recall 0.735537
2017-12-10T15:42:17.979281: step 521, loss 0.270105, acc 0.859375, prec 0.0277865, recall 0.735537
2017-12-10T15:42:18.172801: step 522, loss 0.936112, acc 0.75, prec 0.0277518, recall 0.735537
2017-12-10T15:42:18.373491: step 523, loss 8.618, acc 0.75, prec 0.0277215, recall 0.73251
2017-12-10T15:42:18.572512: step 524, loss 0.461176, acc 0.796875, prec 0.0276935, recall 0.73251
2017-12-10T15:42:18.771620: step 525, loss 0.66661, acc 0.796875, prec 0.0276655, recall 0.73251
2017-12-10T15:42:18.968687: step 526, loss 7.69712, acc 0.71875, prec 0.0277799, recall 0.732106
2017-12-10T15:42:19.163522: step 527, loss 1.90394, acc 0.53125, prec 0.0277907, recall 0.732653
2017-12-10T15:42:19.359135: step 528, loss 1.64743, acc 0.6875, prec 0.0278229, recall 0.733198
2017-12-10T15:42:19.551759: step 529, loss 2.00104, acc 0.484375, prec 0.0277521, recall 0.733198
2017-12-10T15:42:19.744385: step 530, loss 1.58052, acc 0.609375, prec 0.0276987, recall 0.733198
2017-12-10T15:42:19.947671: step 531, loss 2.11934, acc 0.546875, prec 0.027637, recall 0.733198
2017-12-10T15:42:20.139275: step 532, loss 2.09785, acc 0.46875, prec 0.0275651, recall 0.733198
2017-12-10T15:42:20.334094: step 533, loss 1.59794, acc 0.5, prec 0.0274977, recall 0.733198
2017-12-10T15:42:20.524311: step 534, loss 1.67705, acc 0.515625, prec 0.0274328, recall 0.733198
2017-12-10T15:42:20.717879: step 535, loss 3.43247, acc 0.59375, prec 0.0274546, recall 0.732252
2017-12-10T15:42:20.916808: step 536, loss 2.39423, acc 0.484375, prec 0.0274596, recall 0.732794
2017-12-10T15:42:21.110835: step 537, loss 2.39039, acc 0.53125, prec 0.0273973, recall 0.732794
2017-12-10T15:42:21.305936: step 538, loss 1.76374, acc 0.5625, prec 0.0274128, recall 0.733333
2017-12-10T15:42:21.498594: step 539, loss 1.42478, acc 0.578125, prec 0.027357, recall 0.733333
2017-12-10T15:42:21.692984: step 540, loss 2.47001, acc 0.578125, prec 0.0275209, recall 0.73494
2017-12-10T15:42:21.893490: step 541, loss 0.945286, acc 0.71875, prec 0.0274837, recall 0.73494
2017-12-10T15:42:22.086262: step 542, loss 2.03739, acc 0.609375, prec 0.0275051, recall 0.735471
2017-12-10T15:42:22.283521: step 543, loss 0.864619, acc 0.78125, prec 0.0274762, recall 0.735471
2017-12-10T15:42:22.480940: step 544, loss 0.647666, acc 0.796875, prec 0.027595, recall 0.736527
2017-12-10T15:42:22.673369: step 545, loss 0.587031, acc 0.8125, prec 0.0275702, recall 0.736527
2017-12-10T15:42:22.867271: step 546, loss 0.79312, acc 0.703125, prec 0.0276762, recall 0.737575
2017-12-10T15:42:23.064492: step 547, loss 0.645088, acc 0.828125, prec 0.0276535, recall 0.737575
2017-12-10T15:42:23.258683: step 548, loss 0.841272, acc 0.75, prec 0.027693, recall 0.738095
2017-12-10T15:42:23.459261: step 549, loss 0.707548, acc 0.78125, prec 0.0276642, recall 0.738095
2017-12-10T15:42:23.655346: step 550, loss 0.874105, acc 0.75, prec 0.0276313, recall 0.738095
2017-12-10T15:42:23.849214: step 551, loss 1.73222, acc 0.875, prec 0.0276169, recall 0.736634
2017-12-10T15:42:24.047784: step 552, loss 0.172007, acc 0.9375, prec 0.0276087, recall 0.736634
2017-12-10T15:42:24.242542: step 553, loss 0.44233, acc 0.84375, prec 0.0275883, recall 0.736634
2017-12-10T15:42:24.442150: step 554, loss 0.50428, acc 0.875, prec 0.027644, recall 0.737154
2017-12-10T15:42:24.639625: step 555, loss 0.591499, acc 0.828125, prec 0.0276214, recall 0.737154
2017-12-10T15:42:24.832698: step 556, loss 0.454889, acc 0.890625, prec 0.0276071, recall 0.737154
2017-12-10T15:42:25.029042: step 557, loss 11.2345, acc 0.828125, prec 0.0275867, recall 0.7357
2017-12-10T15:42:25.227397: step 558, loss 0.334777, acc 0.828125, prec 0.0275643, recall 0.7357
2017-12-10T15:42:25.422473: step 559, loss 14.1785, acc 0.9375, prec 0.0276321, recall 0.733333
2017-12-10T15:42:25.619775: step 560, loss 14.6601, acc 0.84375, prec 0.0276157, recall 0.730469
2017-12-10T15:42:25.824752: step 561, loss 0.406289, acc 0.890625, prec 0.0276732, recall 0.730994
2017-12-10T15:42:26.020329: step 562, loss 3.34403, acc 0.703125, prec 0.0276365, recall 0.729572
2017-12-10T15:42:26.223048: step 563, loss 3.08003, acc 0.453125, prec 0.0275674, recall 0.728155
2017-12-10T15:42:26.421988: step 564, loss 1.38966, acc 0.578125, prec 0.0275128, recall 0.728155
2017-12-10T15:42:26.621229: step 565, loss 1.76484, acc 0.484375, prec 0.0274464, recall 0.728155
2017-12-10T15:42:26.813005: step 566, loss 2.11667, acc 0.40625, prec 0.0274413, recall 0.728682
2017-12-10T15:42:27.005983: step 567, loss 2.29088, acc 0.4375, prec 0.0274401, recall 0.729207
2017-12-10T15:42:27.202347: step 568, loss 3.14404, acc 0.34375, prec 0.0274271, recall 0.72973
2017-12-10T15:42:27.400589: step 569, loss 3.46725, acc 0.421875, prec 0.0274944, recall 0.730769
2017-12-10T15:42:27.596480: step 570, loss 3.1788, acc 0.375, prec 0.0275554, recall 0.731801
2017-12-10T15:42:27.787549: step 571, loss 3.59163, acc 0.328125, prec 0.0275401, recall 0.732314
2017-12-10T15:42:27.978452: step 572, loss 3.12872, acc 0.28125, prec 0.0275887, recall 0.733333
2017-12-10T15:42:28.174872: step 573, loss 3.18246, acc 0.28125, prec 0.027498, recall 0.733333
2017-12-10T15:42:28.370555: step 574, loss 1.92167, acc 0.546875, prec 0.0275798, recall 0.734345
2017-12-10T15:42:28.568194: step 575, loss 2.13033, acc 0.5625, prec 0.0275249, recall 0.734345
2017-12-10T15:42:28.763205: step 576, loss 2.22378, acc 0.4375, prec 0.0275236, recall 0.734848
2017-12-10T15:42:28.962213: step 577, loss 1.77198, acc 0.546875, prec 0.0274671, recall 0.734848
2017-12-10T15:42:29.156241: step 578, loss 1.4999, acc 0.5625, prec 0.0274127, recall 0.734848
2017-12-10T15:42:29.355248: step 579, loss 1.3779, acc 0.609375, prec 0.027433, recall 0.73535
2017-12-10T15:42:29.550092: step 580, loss 1.50615, acc 0.65625, prec 0.0273905, recall 0.73535
2017-12-10T15:42:29.748988: step 581, loss 0.654889, acc 0.703125, prec 0.0275591, recall 0.736842
2017-12-10T15:42:29.944589: step 582, loss 0.393745, acc 0.828125, prec 0.0275378, recall 0.736842
2017-12-10T15:42:30.141693: step 583, loss 0.639228, acc 0.765625, prec 0.0275088, recall 0.736842
2017-12-10T15:42:30.342186: step 584, loss 1.27017, acc 0.859375, prec 0.0275596, recall 0.737336
2017-12-10T15:42:30.543419: step 585, loss 3.1975, acc 0.90625, prec 0.0276181, recall 0.736449
2017-12-10T15:42:30.746577: step 586, loss 0.65308, acc 0.828125, prec 0.0275968, recall 0.736449
2017-12-10T15:42:30.937610: step 587, loss 0.635906, acc 0.828125, prec 0.0276436, recall 0.73694
2017-12-10T15:42:31.132632: step 588, loss 0.477619, acc 0.859375, prec 0.0276942, recall 0.73743
2017-12-10T15:42:31.331456: step 589, loss 2.27122, acc 0.875, prec 0.0276807, recall 0.736059
2017-12-10T15:42:31.535288: step 590, loss 0.543857, acc 0.828125, prec 0.0277273, recall 0.736549
2017-12-10T15:42:31.735582: step 591, loss 0.295464, acc 0.859375, prec 0.0277778, recall 0.737037
2017-12-10T15:42:31.929773: step 592, loss 0.479537, acc 0.875, prec 0.0278301, recall 0.737523
2017-12-10T15:42:32.122942: step 593, loss 0.356813, acc 0.90625, prec 0.0278862, recall 0.738007
2017-12-10T15:42:32.317694: step 594, loss 1.49021, acc 0.875, prec 0.0278726, recall 0.736648
2017-12-10T15:42:32.516144: step 595, loss 0.279517, acc 0.921875, prec 0.0279983, recall 0.737615
2017-12-10T15:42:32.715975: step 596, loss 14.558, acc 0.9375, prec 0.0279925, recall 0.736264
2017-12-10T15:42:32.915491: step 597, loss 0.430563, acc 0.859375, prec 0.0279749, recall 0.736264
2017-12-10T15:42:33.109497: step 598, loss 0.821801, acc 0.859375, prec 0.028025, recall 0.736746
2017-12-10T15:42:33.307651: step 599, loss 3.45694, acc 0.828125, prec 0.0280056, recall 0.735401
2017-12-10T15:42:33.502379: step 600, loss 0.684066, acc 0.78125, prec 0.0279783, recall 0.735401
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_3/1512938428/checkpoints/model-600

2017-12-10T15:42:34.776410: step 601, loss 3.35755, acc 0.84375, prec 0.0279609, recall 0.734062
2017-12-10T15:42:34.976268: step 602, loss 0.651397, acc 0.8125, prec 0.0279376, recall 0.734062
2017-12-10T15:42:35.171745: step 603, loss 6.85187, acc 0.703125, prec 0.0280374, recall 0.733696
2017-12-10T15:42:35.369411: step 604, loss 0.855926, acc 0.84375, prec 0.0281525, recall 0.734657
2017-12-10T15:42:35.566540: step 605, loss 2.65372, acc 0.65625, prec 0.0282459, recall 0.734291
2017-12-10T15:42:35.764631: step 606, loss 1.06984, acc 0.6875, prec 0.0282739, recall 0.734767
2017-12-10T15:42:35.963905: step 607, loss 5.52374, acc 0.5625, prec 0.0282214, recall 0.733453
2017-12-10T15:42:36.159438: step 608, loss 1.9449, acc 0.53125, prec 0.02823, recall 0.733929
2017-12-10T15:42:36.352558: step 609, loss 2.26043, acc 0.4375, prec 0.0282269, recall 0.734403
2017-12-10T15:42:36.547236: step 610, loss 2.57157, acc 0.4375, prec 0.0282239, recall 0.734875
2017-12-10T15:42:36.743560: step 611, loss 2.68836, acc 0.375, prec 0.0282132, recall 0.735346
2017-12-10T15:42:36.941217: step 612, loss 2.84236, acc 0.4375, prec 0.0283423, recall 0.736749
2017-12-10T15:42:37.135815: step 613, loss 2.58773, acc 0.5, prec 0.0283467, recall 0.737213
2017-12-10T15:42:37.329573: step 614, loss 2.44739, acc 0.4375, prec 0.0284091, recall 0.738137
2017-12-10T15:42:37.525721: step 615, loss 2.6241, acc 0.328125, prec 0.0283922, recall 0.738596
2017-12-10T15:42:37.721354: step 616, loss 2.20099, acc 0.5, prec 0.0283311, recall 0.738596
2017-12-10T15:42:37.919072: step 617, loss 2.09674, acc 0.453125, prec 0.0285254, recall 0.740418
2017-12-10T15:42:38.114982: step 618, loss 1.41522, acc 0.640625, prec 0.0286116, recall 0.741319
2017-12-10T15:42:38.313155: step 619, loss 2.1739, acc 0.46875, prec 0.0285466, recall 0.741319
2017-12-10T15:42:38.503411: step 620, loss 0.903354, acc 0.703125, prec 0.0285104, recall 0.741319
2017-12-10T15:42:38.699701: step 621, loss 1.3312, acc 0.625, prec 0.0284648, recall 0.741319
2017-12-10T15:42:38.891933: step 622, loss 0.614951, acc 0.765625, prec 0.0284363, recall 0.741319
2017-12-10T15:42:39.085879: step 623, loss 0.725584, acc 0.78125, prec 0.0284098, recall 0.741319
2017-12-10T15:42:39.277317: step 624, loss 0.562126, acc 0.796875, prec 0.0283853, recall 0.741319
2017-12-10T15:42:39.472528: step 625, loss 0.402343, acc 0.859375, prec 0.0283683, recall 0.741319
2017-12-10T15:42:39.664432: step 626, loss 0.158987, acc 0.921875, prec 0.0283589, recall 0.741319
2017-12-10T15:42:39.863234: step 627, loss 2.83375, acc 0.8125, prec 0.0284027, recall 0.740484
2017-12-10T15:42:40.057784: step 628, loss 0.162431, acc 0.921875, prec 0.0283933, recall 0.740484
2017-12-10T15:42:40.254512: step 629, loss 0.185478, acc 0.9375, prec 0.0285146, recall 0.741379
2017-12-10T15:42:40.448128: step 630, loss 3.76051, acc 0.8125, prec 0.0284938, recall 0.740103
2017-12-10T15:42:40.646312: step 631, loss 0.342723, acc 0.890625, prec 0.0284806, recall 0.740103
2017-12-10T15:42:40.847858: step 632, loss 0.979995, acc 0.890625, prec 0.0285317, recall 0.74055
2017-12-10T15:42:41.043686: step 633, loss 0.286643, acc 0.90625, prec 0.0285847, recall 0.740995
2017-12-10T15:42:41.239905: step 634, loss 8.75185, acc 0.796875, prec 0.0286281, recall 0.738908
2017-12-10T15:42:41.439662: step 635, loss 0.3524, acc 0.875, prec 0.0286772, recall 0.739353
2017-12-10T15:42:41.637655: step 636, loss 1.61574, acc 0.796875, prec 0.0287167, recall 0.739796
2017-12-10T15:42:41.830767: step 637, loss 0.550271, acc 0.765625, prec 0.0286883, recall 0.739796
2017-12-10T15:42:42.025470: step 638, loss 0.959474, acc 0.75, prec 0.028658, recall 0.739796
2017-12-10T15:42:42.222498: step 639, loss 1.05145, acc 0.734375, prec 0.028626, recall 0.739796
2017-12-10T15:42:42.420340: step 640, loss 1.18359, acc 0.671875, prec 0.0287779, recall 0.741117
2017-12-10T15:42:42.615349: step 641, loss 1.02396, acc 0.703125, prec 0.028742, recall 0.741117
2017-12-10T15:42:42.811622: step 642, loss 11.4321, acc 0.6875, prec 0.0287063, recall 0.739865
2017-12-10T15:42:43.006367: step 643, loss 3.72397, acc 0.6875, prec 0.0287977, recall 0.739496
2017-12-10T15:42:43.200139: step 644, loss 1.53279, acc 0.625, prec 0.028816, recall 0.739933
2017-12-10T15:42:43.390097: step 645, loss 1.73523, acc 0.578125, prec 0.0288919, recall 0.740803
2017-12-10T15:42:43.585092: step 646, loss 2.41411, acc 0.53125, prec 0.0288987, recall 0.741235
2017-12-10T15:42:43.779390: step 647, loss 1.47032, acc 0.5625, prec 0.0288462, recall 0.741235
2017-12-10T15:42:43.971737: step 648, loss 1.41685, acc 0.609375, prec 0.0288624, recall 0.741667
2017-12-10T15:42:44.163434: step 649, loss 1.91953, acc 0.5, prec 0.0289911, recall 0.742952
2017-12-10T15:42:44.364618: step 650, loss 1.10632, acc 0.6875, prec 0.0289537, recall 0.742952
2017-12-10T15:42:44.557665: step 651, loss 1.70802, acc 0.546875, prec 0.0288995, recall 0.742952
2017-12-10T15:42:44.752754: step 652, loss 1.44108, acc 0.59375, prec 0.0289136, recall 0.743378
2017-12-10T15:42:44.950834: step 653, loss 1.37229, acc 0.65625, prec 0.0289976, recall 0.744224
2017-12-10T15:42:45.145942: step 654, loss 1.44399, acc 0.671875, prec 0.0290832, recall 0.745066
2017-12-10T15:42:45.341610: step 655, loss 1.41003, acc 0.6875, prec 0.0291704, recall 0.745902
2017-12-10T15:42:45.538612: step 656, loss 0.999413, acc 0.671875, prec 0.0291312, recall 0.745902
2017-12-10T15:42:45.734299: step 657, loss 2.79964, acc 0.78125, prec 0.0291691, recall 0.745098
2017-12-10T15:42:45.929074: step 658, loss 0.755223, acc 0.78125, prec 0.029267, recall 0.745928
2017-12-10T15:42:46.124903: step 659, loss 2.72146, acc 0.734375, prec 0.0292372, recall 0.744715
2017-12-10T15:42:46.322433: step 660, loss 0.891249, acc 0.828125, prec 0.0292166, recall 0.744715
2017-12-10T15:42:46.515242: step 661, loss 0.712102, acc 0.75, prec 0.0291868, recall 0.744715
2017-12-10T15:42:46.714419: step 662, loss 0.771535, acc 0.796875, prec 0.0292245, recall 0.74513
2017-12-10T15:42:46.909829: step 663, loss 0.923531, acc 0.75, prec 0.0292565, recall 0.745543
2017-12-10T15:42:47.102761: step 664, loss 0.606202, acc 0.828125, prec 0.0292977, recall 0.745955
2017-12-10T15:42:47.297435: step 665, loss 0.512928, acc 0.765625, prec 0.0292698, recall 0.745955
2017-12-10T15:42:47.494548: step 666, loss 0.697612, acc 0.78125, prec 0.0292438, recall 0.745955
2017-12-10T15:42:47.691901: step 667, loss 0.442463, acc 0.890625, prec 0.0292309, recall 0.745955
2017-12-10T15:42:47.894723: step 668, loss 0.412719, acc 0.875, prec 0.0292776, recall 0.746365
2017-12-10T15:42:48.089882: step 669, loss 1.00628, acc 0.8125, prec 0.0293168, recall 0.746774
2017-12-10T15:42:48.288014: step 670, loss 2.00477, acc 0.84375, prec 0.0295457, recall 0.7472
2017-12-10T15:42:48.488917: step 671, loss 5.01783, acc 0.859375, prec 0.0296535, recall 0.746815
2017-12-10T15:42:48.686196: step 672, loss 0.294406, acc 0.90625, prec 0.0296423, recall 0.746815
2017-12-10T15:42:48.884327: step 673, loss 0.279573, acc 0.921875, prec 0.0296329, recall 0.746815
2017-12-10T15:42:49.081470: step 674, loss 1.17051, acc 0.671875, prec 0.0295936, recall 0.746815
2017-12-10T15:42:49.274665: step 675, loss 0.852044, acc 0.8125, prec 0.0295712, recall 0.746815
2017-12-10T15:42:49.469224: step 676, loss 0.847279, acc 0.71875, prec 0.0295377, recall 0.746815
2017-12-10T15:42:49.662247: step 677, loss 1.44663, acc 0.703125, prec 0.0295635, recall 0.747218
2017-12-10T15:42:49.860018: step 678, loss 0.272567, acc 0.859375, prec 0.0295467, recall 0.747218
2017-12-10T15:42:50.059925: step 679, loss 1.71286, acc 0.796875, prec 0.0295245, recall 0.746032
2017-12-10T15:42:50.257325: step 680, loss 1.20167, acc 0.671875, prec 0.0295465, recall 0.746434
2017-12-10T15:42:50.455058: step 681, loss 0.921955, acc 0.671875, prec 0.0295076, recall 0.746434
2017-12-10T15:42:50.651138: step 682, loss 0.908154, acc 0.875, prec 0.0296143, recall 0.747235
2017-12-10T15:42:50.849795: step 683, loss 1.07707, acc 0.734375, prec 0.0296435, recall 0.747634
2017-12-10T15:42:51.047914: step 684, loss 1.63423, acc 0.765625, prec 0.0296764, recall 0.748031
2017-12-10T15:42:51.248897: step 685, loss 0.813364, acc 0.78125, prec 0.029711, recall 0.748428
2017-12-10T15:42:51.444774: step 686, loss 0.72785, acc 0.828125, prec 0.0296906, recall 0.748428
2017-12-10T15:42:51.641530: step 687, loss 0.752589, acc 0.8125, prec 0.0296684, recall 0.748428
2017-12-10T15:42:51.833256: step 688, loss 0.584015, acc 0.8125, prec 0.0297067, recall 0.748823
2017-12-10T15:42:52.032567: step 689, loss 1.28544, acc 0.796875, prec 0.029743, recall 0.749216
2017-12-10T15:42:52.231494: step 690, loss 0.885106, acc 0.78125, prec 0.0297774, recall 0.749609
2017-12-10T15:42:52.427633: step 691, loss 0.91738, acc 0.78125, prec 0.0298721, recall 0.75039
2017-12-10T15:42:52.625613: step 692, loss 0.54516, acc 0.859375, prec 0.0298554, recall 0.75039
2017-12-10T15:42:52.827756: step 693, loss 0.652457, acc 0.8125, prec 0.0298332, recall 0.75039
2017-12-10T15:42:53.026532: step 694, loss 0.917366, acc 0.75, prec 0.0299839, recall 0.751553
2017-12-10T15:42:53.220883: step 695, loss 0.805243, acc 0.796875, prec 0.0299598, recall 0.751553
2017-12-10T15:42:53.415464: step 696, loss 4.7727, acc 0.859375, prec 0.0299449, recall 0.750388
2017-12-10T15:42:53.613431: step 697, loss 0.792196, acc 0.796875, prec 0.0299808, recall 0.750774
2017-12-10T15:42:53.811706: step 698, loss 0.545428, acc 0.828125, prec 0.0300204, recall 0.751159
2017-12-10T15:42:54.004723: step 699, loss 0.482339, acc 0.8125, prec 0.0299981, recall 0.751159
2017-12-10T15:42:54.206515: step 700, loss 8.90167, acc 0.6875, prec 0.029963, recall 0.75
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_3/1512938428/checkpoints/model-700

2017-12-10T15:42:55.524465: step 701, loss 0.507387, acc 0.8125, prec 0.0299409, recall 0.75
2017-12-10T15:42:55.717451: step 702, loss 0.869255, acc 0.703125, prec 0.0299655, recall 0.750385
2017-12-10T15:42:55.915661: step 703, loss 0.580821, acc 0.796875, prec 0.0299416, recall 0.750385
2017-12-10T15:42:56.109675: step 704, loss 1.42831, acc 0.734375, prec 0.0300295, recall 0.751152
2017-12-10T15:42:56.305928: step 705, loss 0.640045, acc 0.78125, prec 0.0300037, recall 0.751152
2017-12-10T15:42:56.503950: step 706, loss 1.80195, acc 0.796875, prec 0.0299816, recall 0.75
2017-12-10T15:42:56.698108: step 707, loss 0.564829, acc 0.828125, prec 0.0299614, recall 0.75
2017-12-10T15:42:56.895647: step 708, loss 0.563505, acc 0.8125, prec 0.0299394, recall 0.75
2017-12-10T15:42:57.088439: step 709, loss 0.839704, acc 0.765625, prec 0.0300306, recall 0.750765
2017-12-10T15:42:57.280303: step 710, loss 1.33447, acc 0.6875, prec 0.0300531, recall 0.751145
2017-12-10T15:42:57.481236: step 711, loss 0.743226, acc 0.765625, prec 0.0300848, recall 0.751524
2017-12-10T15:42:57.677648: step 712, loss 0.726745, acc 0.78125, prec 0.0300591, recall 0.751524
2017-12-10T15:42:57.877053: step 713, loss 1.43808, acc 0.734375, prec 0.0300871, recall 0.751903
2017-12-10T15:42:58.072884: step 714, loss 0.903531, acc 0.75, prec 0.0301168, recall 0.75228
2017-12-10T15:42:58.266502: step 715, loss 0.618346, acc 0.796875, prec 0.030152, recall 0.752656
2017-12-10T15:42:58.457189: step 716, loss 0.870088, acc 0.734375, prec 0.0301208, recall 0.752656
2017-12-10T15:42:58.655262: step 717, loss 0.47679, acc 0.828125, prec 0.0301596, recall 0.75303
2017-12-10T15:42:58.850609: step 718, loss 0.780959, acc 0.765625, prec 0.0301322, recall 0.75303
2017-12-10T15:42:59.045787: step 719, loss 0.520716, acc 0.828125, prec 0.0301121, recall 0.75303
2017-12-10T15:42:59.248901: step 720, loss 4.01182, acc 0.734375, prec 0.0301416, recall 0.752266
2017-12-10T15:42:59.453367: step 721, loss 0.747496, acc 0.734375, prec 0.0301106, recall 0.752266
2017-12-10T15:42:59.646706: step 722, loss 2.271, acc 0.65625, prec 0.0300725, recall 0.751131
2017-12-10T15:42:59.843629: step 723, loss 0.736797, acc 0.734375, prec 0.0301001, recall 0.751506
2017-12-10T15:43:00.039835: step 724, loss 0.580754, acc 0.796875, prec 0.0300765, recall 0.751506
2017-12-10T15:43:00.231772: step 725, loss 0.961753, acc 0.796875, prec 0.0301698, recall 0.752252
2017-12-10T15:43:00.426477: step 726, loss 0.855657, acc 0.84375, prec 0.03021, recall 0.752624
2017-12-10T15:43:00.621605: step 727, loss 0.996554, acc 0.796875, prec 0.0302447, recall 0.752994
2017-12-10T15:43:00.816225: step 728, loss 0.469867, acc 0.796875, prec 0.0303376, recall 0.753731
2017-12-10T15:43:01.011183: step 729, loss 0.530192, acc 0.828125, prec 0.0303176, recall 0.753731
2017-12-10T15:43:01.210968: step 730, loss 0.47177, acc 0.8125, prec 0.0303539, recall 0.754098
2017-12-10T15:43:01.411678: step 731, loss 6.30244, acc 0.765625, prec 0.0303285, recall 0.752976
2017-12-10T15:43:01.617200: step 732, loss 1.71937, acc 0.765625, prec 0.0303593, recall 0.753343
2017-12-10T15:43:01.814271: step 733, loss 1.80985, acc 0.734375, prec 0.0303864, recall 0.753709
2017-12-10T15:43:02.010808: step 734, loss 0.852714, acc 0.765625, prec 0.0304171, recall 0.754074
2017-12-10T15:43:02.209145: step 735, loss 0.888624, acc 0.71875, prec 0.0303844, recall 0.754074
2017-12-10T15:43:02.407814: step 736, loss 0.619469, acc 0.78125, prec 0.0304169, recall 0.754438
2017-12-10T15:43:02.605246: step 737, loss 15.6164, acc 0.671875, prec 0.0304402, recall 0.752577
2017-12-10T15:43:02.802796: step 738, loss 0.602443, acc 0.828125, prec 0.030478, recall 0.752941
2017-12-10T15:43:02.998088: step 739, loss 1.33901, acc 0.546875, prec 0.0304255, recall 0.752941
2017-12-10T15:43:03.190656: step 740, loss 1.43205, acc 0.53125, prec 0.0304289, recall 0.753304
2017-12-10T15:43:03.385524: step 741, loss 1.19974, acc 0.578125, prec 0.030495, recall 0.754026
2017-12-10T15:43:03.586477: step 742, loss 1.61572, acc 0.59375, prec 0.0304481, recall 0.754026
2017-12-10T15:43:03.778707: step 743, loss 1.4905, acc 0.546875, prec 0.0304533, recall 0.754386
2017-12-10T15:43:03.973892: step 744, loss 1.75457, acc 0.59375, prec 0.0304637, recall 0.754745
2017-12-10T15:43:04.174848: step 745, loss 1.47217, acc 0.578125, prec 0.0304153, recall 0.754745
2017-12-10T15:43:04.369484: step 746, loss 1.40913, acc 0.625, prec 0.0304864, recall 0.755459
2017-12-10T15:43:04.563600: step 747, loss 1.27278, acc 0.578125, prec 0.030495, recall 0.755814
2017-12-10T15:43:04.759701: step 748, loss 1.41467, acc 0.59375, prec 0.0305621, recall 0.756522
2017-12-10T15:43:04.960254: step 749, loss 1.1245, acc 0.625, prec 0.0305759, recall 0.756874
2017-12-10T15:43:05.156201: step 750, loss 2.85045, acc 0.6875, prec 0.0305419, recall 0.75578
2017-12-10T15:43:05.354713: step 751, loss 1.78493, acc 0.65625, prec 0.0306158, recall 0.756484
2017-12-10T15:43:05.549843: step 752, loss 1.9799, acc 0.734375, prec 0.0305873, recall 0.755396
2017-12-10T15:43:05.748988: step 753, loss 1.20228, acc 0.625, prec 0.030601, recall 0.755747
2017-12-10T15:43:05.948996: step 754, loss 6.56212, acc 0.546875, prec 0.0306638, recall 0.755365
2017-12-10T15:43:06.145747: step 755, loss 0.726664, acc 0.734375, prec 0.0306336, recall 0.755365
2017-12-10T15:43:06.341034: step 756, loss 1.0774, acc 0.625, prec 0.0306471, recall 0.755714
2017-12-10T15:43:06.542066: step 757, loss 1.14243, acc 0.640625, prec 0.0306063, recall 0.755714
2017-12-10T15:43:06.737145: step 758, loss 1.56458, acc 0.59375, prec 0.0306164, recall 0.756063
2017-12-10T15:43:06.931995: step 759, loss 3.60386, acc 0.671875, prec 0.0306929, recall 0.755682
2017-12-10T15:43:07.128853: step 760, loss 1.30693, acc 0.671875, prec 0.0308233, recall 0.756719
2017-12-10T15:43:07.320071: step 761, loss 3.56662, acc 0.6875, prec 0.0308454, recall 0.755994
2017-12-10T15:43:07.515924: step 762, loss 1.11006, acc 0.6875, prec 0.0308099, recall 0.755994
2017-12-10T15:43:07.710146: step 763, loss 1.04368, acc 0.6875, prec 0.0307745, recall 0.755994
2017-12-10T15:43:07.908626: step 764, loss 1.32397, acc 0.671875, prec 0.0308486, recall 0.756681
2017-12-10T15:43:08.100242: step 765, loss 1.32489, acc 0.625, prec 0.0308617, recall 0.757023
2017-12-10T15:43:08.299147: step 766, loss 1.09064, acc 0.65625, prec 0.0308229, recall 0.757023
2017-12-10T15:43:08.490929: step 767, loss 1.11558, acc 0.671875, prec 0.0308413, recall 0.757363
2017-12-10T15:43:08.685699: step 768, loss 1.01597, acc 0.6875, prec 0.0308614, recall 0.757703
2017-12-10T15:43:08.879871: step 769, loss 1.17184, acc 0.671875, prec 0.0308245, recall 0.757703
2017-12-10T15:43:09.071151: step 770, loss 1.09846, acc 0.6875, prec 0.0308445, recall 0.758042
2017-12-10T15:43:09.267874: step 771, loss 1.15409, acc 0.671875, prec 0.0308077, recall 0.758042
2017-12-10T15:43:09.462866: step 772, loss 0.784214, acc 0.765625, prec 0.0308365, recall 0.75838
2017-12-10T15:43:09.655394: step 773, loss 0.613416, acc 0.765625, prec 0.0308103, recall 0.75838
2017-12-10T15:43:09.857504: step 774, loss 0.778109, acc 0.75, prec 0.0308373, recall 0.758717
2017-12-10T15:43:10.058614: step 775, loss 0.611826, acc 0.8125, prec 0.0308163, recall 0.758717
2017-12-10T15:43:10.253747: step 776, loss 0.384472, acc 0.828125, prec 0.0307971, recall 0.758717
2017-12-10T15:43:10.455130: step 777, loss 3.69127, acc 0.859375, prec 0.0307832, recall 0.75766
2017-12-10T15:43:10.650297: step 778, loss 0.426275, acc 0.828125, prec 0.030764, recall 0.75766
2017-12-10T15:43:10.849042: step 779, loss 0.370296, acc 0.875, prec 0.0307501, recall 0.75766
2017-12-10T15:43:11.046957: step 780, loss 0.316636, acc 0.921875, prec 0.0307414, recall 0.75766
2017-12-10T15:43:11.240722: step 781, loss 0.681737, acc 0.921875, prec 0.0307875, recall 0.757997
2017-12-10T15:43:11.437062: step 782, loss 0.413518, acc 0.859375, prec 0.0307718, recall 0.757997
2017-12-10T15:43:11.636561: step 783, loss 0.431221, acc 0.890625, prec 0.0307597, recall 0.757997
2017-12-10T15:43:11.832315: step 784, loss 0.32633, acc 0.890625, prec 0.0308022, recall 0.758333
2017-12-10T15:43:12.026097: step 785, loss 0.511279, acc 0.84375, prec 0.0307848, recall 0.758333
2017-12-10T15:43:12.227042: step 786, loss 0.185726, acc 0.90625, prec 0.0307744, recall 0.758333
2017-12-10T15:43:12.426563: step 787, loss 2.71911, acc 0.875, prec 0.0307623, recall 0.757282
2017-12-10T15:43:12.622152: step 788, loss 0.267708, acc 0.890625, prec 0.0307502, recall 0.757282
2017-12-10T15:43:12.814409: step 789, loss 0.474027, acc 0.9375, prec 0.0308524, recall 0.757953
2017-12-10T15:43:13.013112: step 790, loss 1.86348, acc 0.828125, prec 0.030835, recall 0.756906
2017-12-10T15:43:13.214594: step 791, loss 2.76034, acc 0.828125, prec 0.0308194, recall 0.754821
2017-12-10T15:43:13.410889: step 792, loss 0.831644, acc 0.828125, prec 0.0309093, recall 0.755495
2017-12-10T15:43:13.605987: step 793, loss 0.593832, acc 0.78125, prec 0.030885, recall 0.755495
2017-12-10T15:43:13.798987: step 794, loss 0.621729, acc 0.828125, prec 0.0308659, recall 0.755495
2017-12-10T15:43:13.992055: step 795, loss 2.33744, acc 0.8125, prec 0.0308469, recall 0.754458
2017-12-10T15:43:14.188642: step 796, loss 0.455687, acc 0.859375, prec 0.0308313, recall 0.754458
2017-12-10T15:43:14.387316: step 797, loss 0.591856, acc 0.78125, prec 0.0308071, recall 0.754458
2017-12-10T15:43:14.583398: step 798, loss 0.934062, acc 0.734375, prec 0.0308321, recall 0.754795
2017-12-10T15:43:14.776053: step 799, loss 1.14002, acc 0.6875, prec 0.0309059, recall 0.755464
2017-12-10T15:43:14.969727: step 800, loss 1.21806, acc 0.703125, prec 0.0309273, recall 0.755798
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_3/1512938428/checkpoints/model-800

2017-12-10T15:43:16.516653: step 801, loss 0.666495, acc 0.796875, prec 0.0309048, recall 0.755798
2017-12-10T15:43:16.713674: step 802, loss 4.1225, acc 0.734375, prec 0.0308773, recall 0.754768
2017-12-10T15:43:16.911076: step 803, loss 0.739758, acc 0.71875, prec 0.0309003, recall 0.755102
2017-12-10T15:43:17.104269: step 804, loss 1.63535, acc 0.734375, prec 0.0310328, recall 0.756098
2017-12-10T15:43:17.298241: step 805, loss 1.93884, acc 0.8125, prec 0.0310138, recall 0.755074
2017-12-10T15:43:17.490779: step 806, loss 0.973846, acc 0.71875, prec 0.031198, recall 0.756393
2017-12-10T15:43:17.689598: step 807, loss 1.27734, acc 0.578125, prec 0.0311513, recall 0.756393
2017-12-10T15:43:17.880506: step 808, loss 1.96521, acc 0.546875, prec 0.0311549, recall 0.75672
2017-12-10T15:43:18.077960: step 809, loss 1.29426, acc 0.671875, prec 0.0311187, recall 0.75672
2017-12-10T15:43:18.272845: step 810, loss 1.23905, acc 0.65625, prec 0.0310809, recall 0.75672
2017-12-10T15:43:18.464976: step 811, loss 2.06568, acc 0.65625, prec 0.0311501, recall 0.757373
2017-12-10T15:43:18.663675: step 812, loss 1.41619, acc 0.640625, prec 0.031164, recall 0.757697
2017-12-10T15:43:18.858084: step 813, loss 5.26947, acc 0.625, prec 0.0311246, recall 0.756684
2017-12-10T15:43:19.063608: step 814, loss 1.44615, acc 0.578125, prec 0.0311848, recall 0.757333
2017-12-10T15:43:19.259709: step 815, loss 1.80655, acc 0.546875, prec 0.0311883, recall 0.757656
2017-12-10T15:43:19.463621: step 816, loss 1.07727, acc 0.671875, prec 0.0311525, recall 0.757656
2017-12-10T15:43:19.662635: step 817, loss 2.32142, acc 0.671875, prec 0.0312756, recall 0.758621
2017-12-10T15:43:19.857542: step 818, loss 1.54806, acc 0.578125, prec 0.0313353, recall 0.759259
2017-12-10T15:43:20.050890: step 819, loss 1.01304, acc 0.6875, prec 0.0313011, recall 0.759259
2017-12-10T15:43:20.242770: step 820, loss 1.00447, acc 0.65625, prec 0.0312636, recall 0.759259
2017-12-10T15:43:20.437466: step 821, loss 1.161, acc 0.640625, prec 0.0312245, recall 0.759259
2017-12-10T15:43:20.634475: step 822, loss 1.64622, acc 0.546875, prec 0.0312279, recall 0.759577
2017-12-10T15:43:20.828931: step 823, loss 0.682981, acc 0.78125, prec 0.0312042, recall 0.759577
2017-12-10T15:43:21.027337: step 824, loss 0.616162, acc 0.828125, prec 0.0311856, recall 0.759577
2017-12-10T15:43:21.221105: step 825, loss 0.52595, acc 0.796875, prec 0.0311636, recall 0.759577
2017-12-10T15:43:21.419060: step 826, loss 1.7205, acc 0.828125, prec 0.0312517, recall 0.759211
2017-12-10T15:43:21.613461: step 827, loss 0.8607, acc 0.796875, prec 0.0312821, recall 0.759527
2017-12-10T15:43:21.810743: step 828, loss 0.457781, acc 0.8125, prec 0.0313142, recall 0.759843
2017-12-10T15:43:22.006762: step 829, loss 0.819817, acc 0.734375, prec 0.0313378, recall 0.760157
2017-12-10T15:43:22.205130: step 830, loss 5.35276, acc 0.828125, prec 0.0313209, recall 0.759162
2017-12-10T15:43:22.401048: step 831, loss 0.431413, acc 0.84375, prec 0.0313563, recall 0.759477
2017-12-10T15:43:22.601576: step 832, loss 0.613972, acc 0.828125, prec 0.0313376, recall 0.759477
2017-12-10T15:43:22.796598: step 833, loss 0.246913, acc 0.921875, prec 0.0313814, recall 0.759791
2017-12-10T15:43:22.994252: step 834, loss 11.1403, acc 0.78125, prec 0.0314638, recall 0.759428
2017-12-10T15:43:23.192227: step 835, loss 0.51608, acc 0.8125, prec 0.0314435, recall 0.759428
2017-12-10T15:43:23.389468: step 836, loss 2.11562, acc 0.796875, prec 0.0314232, recall 0.758442
2017-12-10T15:43:23.591452: step 837, loss 0.571843, acc 0.78125, prec 0.0313995, recall 0.758442
2017-12-10T15:43:23.786837: step 838, loss 2.08102, acc 0.828125, prec 0.0313827, recall 0.757458
2017-12-10T15:43:23.985435: step 839, loss 0.92601, acc 0.75, prec 0.0314077, recall 0.757772
2017-12-10T15:43:24.184877: step 840, loss 0.823469, acc 0.703125, prec 0.0313757, recall 0.757772
2017-12-10T15:43:24.378224: step 841, loss 1.00755, acc 0.65625, prec 0.0313387, recall 0.757772
2017-12-10T15:43:24.571645: step 842, loss 1.14848, acc 0.78125, prec 0.0313671, recall 0.758085
2017-12-10T15:43:24.769661: step 843, loss 0.719647, acc 0.75, prec 0.0313921, recall 0.758398
2017-12-10T15:43:24.964479: step 844, loss 1.13176, acc 0.71875, prec 0.0314654, recall 0.759021
2017-12-10T15:43:25.162743: step 845, loss 1.08698, acc 0.71875, prec 0.0314868, recall 0.759331
2017-12-10T15:43:25.357746: step 846, loss 4.20485, acc 0.765625, prec 0.0314633, recall 0.758355
2017-12-10T15:43:25.555610: step 847, loss 0.988244, acc 0.703125, prec 0.0314315, recall 0.758355
2017-12-10T15:43:25.752390: step 848, loss 1.09848, acc 0.640625, prec 0.031393, recall 0.758355
2017-12-10T15:43:25.946234: step 849, loss 1.26736, acc 0.53125, prec 0.0315488, recall 0.759591
2017-12-10T15:43:26.144362: step 850, loss 0.886534, acc 0.796875, prec 0.0315784, recall 0.759898
2017-12-10T15:43:26.344389: step 851, loss 1.1256, acc 0.71875, prec 0.0316509, recall 0.76051
2017-12-10T15:43:26.537389: step 852, loss 1.21291, acc 0.625, prec 0.0316107, recall 0.76051
2017-12-10T15:43:26.729924: step 853, loss 0.576253, acc 0.765625, prec 0.0315856, recall 0.76051
2017-12-10T15:43:26.922321: step 854, loss 0.79154, acc 0.75, prec 0.0316101, recall 0.760814
2017-12-10T15:43:27.117549: step 855, loss 0.999587, acc 0.734375, prec 0.0316329, recall 0.761118
2017-12-10T15:43:27.309540: step 856, loss 0.969674, acc 0.65625, prec 0.0315962, recall 0.761118
2017-12-10T15:43:27.504866: step 857, loss 1.16283, acc 0.703125, prec 0.0316156, recall 0.761421
2017-12-10T15:43:27.700057: step 858, loss 0.50177, acc 0.8125, prec 0.0316466, recall 0.761724
2017-12-10T15:43:27.893709: step 859, loss 0.385532, acc 0.875, prec 0.0316332, recall 0.761724
2017-12-10T15:43:28.088136: step 860, loss 0.290154, acc 0.890625, prec 0.0316725, recall 0.762025
2017-12-10T15:43:28.280268: step 861, loss 0.374312, acc 0.84375, prec 0.0316559, recall 0.762025
2017-12-10T15:43:28.476530: step 862, loss 0.557334, acc 0.8125, prec 0.0316359, recall 0.762025
2017-12-10T15:43:28.671674: step 863, loss 9.38888, acc 0.875, prec 0.0316243, recall 0.761062
2017-12-10T15:43:28.871960: step 864, loss 0.471253, acc 0.875, prec 0.0317127, recall 0.761665
2017-12-10T15:43:29.067900: step 865, loss 0.248207, acc 0.90625, prec 0.0317027, recall 0.761665
2017-12-10T15:43:29.269564: step 866, loss 0.58575, acc 0.921875, prec 0.0317452, recall 0.761965
2017-12-10T15:43:29.475756: step 867, loss 1.99305, acc 0.859375, prec 0.0317319, recall 0.761006
2017-12-10T15:43:29.676586: step 868, loss 0.367261, acc 0.921875, prec 0.0318251, recall 0.761606
2017-12-10T15:43:29.874214: step 869, loss 0.559351, acc 0.828125, prec 0.0318067, recall 0.761606
2017-12-10T15:43:30.063782: step 870, loss 5.6338, acc 0.84375, prec 0.0318425, recall 0.760951
2017-12-10T15:43:30.265298: step 871, loss 2.72213, acc 0.875, prec 0.0319322, recall 0.760598
2017-12-10T15:43:30.460030: step 872, loss 4.65521, acc 0.78125, prec 0.0319104, recall 0.759651
2017-12-10T15:43:30.656620: step 873, loss 1.02661, acc 0.78125, prec 0.0320389, recall 0.760546
2017-12-10T15:43:30.852906: step 874, loss 0.841127, acc 0.71875, prec 0.0320593, recall 0.760843
2017-12-10T15:43:31.044323: step 875, loss 2.86198, acc 0.703125, prec 0.0320292, recall 0.759901
2017-12-10T15:43:31.247962: step 876, loss 1.59741, acc 0.703125, prec 0.0320479, recall 0.760198
2017-12-10T15:43:31.447654: step 877, loss 1.18754, acc 0.6875, prec 0.0321153, recall 0.760789
2017-12-10T15:43:31.644300: step 878, loss 1.72602, acc 0.546875, prec 0.0322178, recall 0.761671
2017-12-10T15:43:31.837267: step 879, loss 1.89647, acc 0.59375, prec 0.0322748, recall 0.762255
2017-12-10T15:43:32.038226: step 880, loss 1.85955, acc 0.5625, prec 0.0323283, recall 0.762836
2017-12-10T15:43:32.233714: step 881, loss 1.5387, acc 0.5625, prec 0.0323315, recall 0.763126
2017-12-10T15:43:32.432267: step 882, loss 1.18683, acc 0.640625, prec 0.0323431, recall 0.763415
2017-12-10T15:43:32.628369: step 883, loss 1.90327, acc 0.578125, prec 0.0324478, recall 0.764277
2017-12-10T15:43:32.825803: step 884, loss 1.50416, acc 0.59375, prec 0.0324542, recall 0.764563
2017-12-10T15:43:33.021292: step 885, loss 0.946366, acc 0.734375, prec 0.0324258, recall 0.764563
2017-12-10T15:43:33.213253: step 886, loss 1.57884, acc 0.609375, prec 0.0323841, recall 0.764563
2017-12-10T15:43:33.410613: step 887, loss 1.00139, acc 0.71875, prec 0.0324038, recall 0.764848
2017-12-10T15:43:33.605101: step 888, loss 1.03255, acc 0.6875, prec 0.0324699, recall 0.765417
2017-12-10T15:43:33.805624: step 889, loss 0.847247, acc 0.71875, prec 0.0324895, recall 0.7657
2017-12-10T15:43:34.003898: step 890, loss 1.2082, acc 0.6875, prec 0.0325553, recall 0.766265
2017-12-10T15:43:34.197968: step 891, loss 0.719232, acc 0.75, prec 0.0325286, recall 0.766265
2017-12-10T15:43:34.391073: step 892, loss 1.726, acc 0.71875, prec 0.0325976, recall 0.766827
2017-12-10T15:43:34.591764: step 893, loss 0.837689, acc 0.75, prec 0.0326203, recall 0.767107
2017-12-10T15:43:34.789234: step 894, loss 0.774068, acc 0.75, prec 0.0325937, recall 0.767107
2017-12-10T15:43:34.991585: step 895, loss 1.05382, acc 0.6875, prec 0.0326591, recall 0.767665
2017-12-10T15:43:35.191995: step 896, loss 0.919817, acc 0.734375, prec 0.0326308, recall 0.767665
2017-12-10T15:43:35.392134: step 897, loss 3.87986, acc 0.890625, prec 0.0326209, recall 0.766746
2017-12-10T15:43:35.589193: step 898, loss 0.750423, acc 0.84375, prec 0.0326535, recall 0.767025
2017-12-10T15:43:35.781928: step 899, loss 0.451606, acc 0.84375, prec 0.0327352, recall 0.76758
2017-12-10T15:43:35.980447: step 900, loss 0.527537, acc 0.828125, prec 0.0327661, recall 0.767857
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_3/1512938428/checkpoints/model-900

2017-12-10T15:43:37.280582: step 901, loss 0.509319, acc 0.875, prec 0.0328019, recall 0.768133
2017-12-10T15:43:37.485491: step 902, loss 4.25813, acc 0.796875, prec 0.032831, recall 0.767497
2017-12-10T15:43:37.677692: step 903, loss 0.440029, acc 0.84375, prec 0.0328143, recall 0.767497
2017-12-10T15:43:37.876493: step 904, loss 0.588212, acc 0.796875, prec 0.0328417, recall 0.767772
2017-12-10T15:43:38.070860: step 905, loss 1.65398, acc 0.71875, prec 0.0328608, recall 0.768047
2017-12-10T15:43:38.267856: step 906, loss 1.4313, acc 0.796875, prec 0.0328881, recall 0.768322
2017-12-10T15:43:38.465257: step 907, loss 1.28505, acc 0.8125, prec 0.0329659, recall 0.768868
2017-12-10T15:43:38.669846: step 908, loss 0.462723, acc 0.859375, prec 0.0330487, recall 0.769412
2017-12-10T15:43:38.863512: step 909, loss 0.70979, acc 0.703125, prec 0.0330658, recall 0.769683
2017-12-10T15:43:39.061173: step 910, loss 2.59742, acc 0.78125, prec 0.0331416, recall 0.769321
2017-12-10T15:43:39.253141: step 911, loss 0.535689, acc 0.796875, prec 0.0332174, recall 0.76986
2017-12-10T15:43:39.447978: step 912, loss 1.12143, acc 0.734375, prec 0.033189, recall 0.76986
2017-12-10T15:43:39.649453: step 913, loss 1.12273, acc 0.625, prec 0.0331975, recall 0.770128
2017-12-10T15:43:39.843800: step 914, loss 0.857441, acc 0.671875, prec 0.0332596, recall 0.770664
2017-12-10T15:43:40.039017: step 915, loss 0.821373, acc 0.71875, prec 0.0332296, recall 0.770664
2017-12-10T15:43:40.235373: step 916, loss 1.18216, acc 0.703125, prec 0.0331979, recall 0.770664
2017-12-10T15:43:40.433544: step 917, loss 0.718145, acc 0.71875, prec 0.0332164, recall 0.77093
2017-12-10T15:43:40.633651: step 918, loss 0.69949, acc 0.796875, prec 0.0332432, recall 0.771196
2017-12-10T15:43:40.827360: step 919, loss 1.90334, acc 0.59375, prec 0.0332483, recall 0.771462
2017-12-10T15:43:41.024521: step 920, loss 0.612694, acc 0.828125, prec 0.0332301, recall 0.771462
2017-12-10T15:43:41.228604: step 921, loss 0.522106, acc 0.84375, prec 0.0332135, recall 0.771462
2017-12-10T15:43:41.423544: step 922, loss 0.562514, acc 0.796875, prec 0.0331919, recall 0.771462
2017-12-10T15:43:41.617463: step 923, loss 0.658456, acc 0.8125, prec 0.0332203, recall 0.771727
2017-12-10T15:43:41.810458: step 924, loss 0.884713, acc 0.765625, prec 0.0332436, recall 0.771991
2017-12-10T15:43:42.008664: step 925, loss 1.0267, acc 0.71875, prec 0.033262, recall 0.772254
2017-12-10T15:43:42.201926: step 926, loss 2.65274, acc 0.84375, prec 0.0333914, recall 0.772152
2017-12-10T15:43:42.399566: step 927, loss 0.447223, acc 0.828125, prec 0.0334212, recall 0.772414
2017-12-10T15:43:42.594968: step 928, loss 1.38989, acc 0.859375, prec 0.0334543, recall 0.772675
2017-12-10T15:43:42.795753: step 929, loss 0.465606, acc 0.84375, prec 0.0334377, recall 0.772675
2017-12-10T15:43:42.987181: step 930, loss 0.404066, acc 0.90625, prec 0.0334277, recall 0.772675
2017-12-10T15:43:43.181050: step 931, loss 0.427552, acc 0.796875, prec 0.0334061, recall 0.772675
2017-12-10T15:43:43.374044: step 932, loss 0.612087, acc 0.84375, prec 0.0334855, recall 0.773196
2017-12-10T15:43:43.564549: step 933, loss 0.727007, acc 0.828125, prec 0.033563, recall 0.773714
2017-12-10T15:43:43.762552: step 934, loss 1.57137, acc 0.890625, prec 0.0336488, recall 0.773349
2017-12-10T15:43:43.961431: step 935, loss 0.48829, acc 0.890625, prec 0.033685, recall 0.773606
2017-12-10T15:43:44.152762: step 936, loss 0.376431, acc 0.90625, prec 0.033675, recall 0.773606
2017-12-10T15:43:44.354826: step 937, loss 0.751257, acc 0.828125, prec 0.0337045, recall 0.773864
2017-12-10T15:43:44.550738: step 938, loss 2.10733, acc 0.875, prec 0.0337407, recall 0.773243
2017-12-10T15:43:44.753132: step 939, loss 0.580207, acc 0.8125, prec 0.0337206, recall 0.773243
2017-12-10T15:43:44.948764: step 940, loss 7.68853, acc 0.8125, prec 0.0337023, recall 0.772367
2017-12-10T15:43:45.145649: step 941, loss 0.591239, acc 0.8125, prec 0.0336823, recall 0.772367
2017-12-10T15:43:45.343533: step 942, loss 2.12028, acc 0.75, prec 0.0336574, recall 0.771493
2017-12-10T15:43:45.534829: step 943, loss 0.8405, acc 0.78125, prec 0.0336342, recall 0.771493
2017-12-10T15:43:45.730060: step 944, loss 0.644947, acc 0.796875, prec 0.0336126, recall 0.771493
2017-12-10T15:43:45.924520: step 945, loss 0.643219, acc 0.734375, prec 0.0336796, recall 0.772009
2017-12-10T15:43:46.128553: step 946, loss 1.3863, acc 0.5, prec 0.0336267, recall 0.772009
2017-12-10T15:43:46.324126: step 947, loss 1.45861, acc 0.625, prec 0.033587, recall 0.772009
2017-12-10T15:43:46.517946: step 948, loss 1.49923, acc 0.546875, prec 0.0335393, recall 0.772009
2017-12-10T15:43:46.712962: step 949, loss 1.25751, acc 0.703125, prec 0.0335081, recall 0.772009
2017-12-10T15:43:46.910524: step 950, loss 1.1399, acc 0.65625, prec 0.0335193, recall 0.772266
2017-12-10T15:43:47.107645: step 951, loss 0.896543, acc 0.65625, prec 0.0335305, recall 0.772523
2017-12-10T15:43:47.302788: step 952, loss 1.19148, acc 0.59375, prec 0.0335351, recall 0.772778
2017-12-10T15:43:47.509499: step 953, loss 0.79434, acc 0.765625, prec 0.0335577, recall 0.773034
2017-12-10T15:43:47.703722: step 954, loss 1.55901, acc 0.828125, prec 0.0336339, recall 0.773543
2017-12-10T15:43:47.905780: step 955, loss 0.636447, acc 0.765625, prec 0.0336094, recall 0.773543
2017-12-10T15:43:48.098633: step 956, loss 0.754399, acc 0.765625, prec 0.0335848, recall 0.773543
2017-12-10T15:43:48.297198: step 957, loss 0.721157, acc 0.71875, prec 0.0336024, recall 0.773796
2017-12-10T15:43:48.502241: step 958, loss 0.486309, acc 0.828125, prec 0.0335844, recall 0.773796
2017-12-10T15:43:48.700004: step 959, loss 0.419034, acc 0.859375, prec 0.0335698, recall 0.773796
2017-12-10T15:43:48.893098: step 960, loss 1.79803, acc 0.828125, prec 0.0336473, recall 0.773438
2017-12-10T15:43:49.087183: step 961, loss 0.444006, acc 0.90625, prec 0.0337313, recall 0.773942
2017-12-10T15:43:49.279899: step 962, loss 0.388903, acc 0.84375, prec 0.033715, recall 0.773942
2017-12-10T15:43:49.479230: step 963, loss 7.82669, acc 0.84375, prec 0.0337504, recall 0.771619
2017-12-10T15:43:49.682172: step 964, loss 0.698264, acc 0.8125, prec 0.0337307, recall 0.771619
2017-12-10T15:43:49.875759: step 965, loss 0.791603, acc 0.796875, prec 0.0337095, recall 0.771619
2017-12-10T15:43:50.076678: step 966, loss 4.72907, acc 0.78125, prec 0.0336883, recall 0.770764
2017-12-10T15:43:50.274171: step 967, loss 0.571571, acc 0.78125, prec 0.0336655, recall 0.770764
2017-12-10T15:43:50.468998: step 968, loss 0.990032, acc 0.703125, prec 0.0336346, recall 0.770764
2017-12-10T15:43:50.666882: step 969, loss 1.05675, acc 0.640625, prec 0.0335972, recall 0.770764
2017-12-10T15:43:50.860805: step 970, loss 0.983705, acc 0.734375, prec 0.0337095, recall 0.771523
2017-12-10T15:43:51.048860: step 971, loss 1.52801, acc 0.546875, prec 0.0337555, recall 0.772026
2017-12-10T15:43:51.243989: step 972, loss 1.46338, acc 0.484375, prec 0.0337484, recall 0.772277
2017-12-10T15:43:51.437398: step 973, loss 1.05694, acc 0.6875, prec 0.033716, recall 0.772277
2017-12-10T15:43:51.632983: step 974, loss 1.10349, acc 0.625, prec 0.0336771, recall 0.772277
2017-12-10T15:43:51.825363: step 975, loss 0.872049, acc 0.6875, prec 0.0336912, recall 0.772527
2017-12-10T15:43:52.023157: step 976, loss 0.709624, acc 0.71875, prec 0.0337547, recall 0.773026
2017-12-10T15:43:52.216736: step 977, loss 1.31333, acc 0.75, prec 0.0337751, recall 0.773275
2017-12-10T15:43:52.415173: step 978, loss 0.518701, acc 0.765625, prec 0.0337508, recall 0.773275
2017-12-10T15:43:52.610510: step 979, loss 0.716001, acc 0.703125, prec 0.0337202, recall 0.773275
2017-12-10T15:43:52.807621: step 980, loss 0.642512, acc 0.78125, prec 0.0337438, recall 0.773523
2017-12-10T15:43:53.002108: step 981, loss 0.513723, acc 0.765625, prec 0.0337197, recall 0.773523
2017-12-10T15:43:53.202402: step 982, loss 3.65993, acc 0.734375, prec 0.0336939, recall 0.772678
2017-12-10T15:43:53.401302: step 983, loss 0.578686, acc 0.78125, prec 0.0337175, recall 0.772926
2017-12-10T15:43:53.596873: step 984, loss 1.91009, acc 0.78125, prec 0.0337426, recall 0.772331
2017-12-10T15:43:53.794568: step 985, loss 0.809323, acc 0.71875, prec 0.0337597, recall 0.772579
2017-12-10T15:43:53.994599: step 986, loss 0.593693, acc 0.8125, prec 0.0337864, recall 0.772826
2017-12-10T15:43:54.191322: step 987, loss 0.348204, acc 0.84375, prec 0.0337703, recall 0.772826
2017-12-10T15:43:54.388113: step 988, loss 0.520457, acc 0.828125, prec 0.0337985, recall 0.773073
2017-12-10T15:43:54.586069: step 989, loss 0.421339, acc 0.8125, prec 0.0338251, recall 0.773319
2017-12-10T15:43:54.779798: step 990, loss 0.614873, acc 0.78125, prec 0.0338027, recall 0.773319
2017-12-10T15:43:54.975314: step 991, loss 0.50195, acc 0.875, prec 0.0338357, recall 0.773564
2017-12-10T15:43:55.172167: step 992, loss 0.920458, acc 0.859375, prec 0.0339128, recall 0.774054
2017-12-10T15:43:55.373202: step 993, loss 3.39726, acc 0.828125, prec 0.0338967, recall 0.773218
2017-12-10T15:43:55.550516: step 994, loss 0.41858, acc 0.807692, prec 0.0338807, recall 0.773218
2017-12-10T15:43:55.748533: step 995, loss 0.30088, acc 0.875, prec 0.0338678, recall 0.773218
2017-12-10T15:43:55.948443: step 996, loss 0.392591, acc 0.78125, prec 0.0338454, recall 0.773218
2017-12-10T15:43:56.141074: step 997, loss 0.304557, acc 0.875, prec 0.0338326, recall 0.773218
2017-12-10T15:43:56.337849: step 998, loss 0.470194, acc 0.921875, prec 0.0339159, recall 0.773707
2017-12-10T15:43:56.532381: step 999, loss 0.320423, acc 0.90625, prec 0.0339063, recall 0.773707
2017-12-10T15:43:56.731118: step 1000, loss 0.277268, acc 0.90625, prec 0.0340335, recall 0.774436
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_3/1512938428/checkpoints/model-1000

2017-12-10T15:43:58.020195: step 1001, loss 0.381838, acc 0.859375, prec 0.0340191, recall 0.774436
2017-12-10T15:43:58.216728: step 1002, loss 0.842462, acc 0.875, prec 0.0340518, recall 0.774678
2017-12-10T15:43:58.409934: step 1003, loss 0.132807, acc 0.96875, prec 0.0340941, recall 0.77492
2017-12-10T15:43:58.600284: step 1004, loss 0.750457, acc 0.859375, prec 0.0341707, recall 0.775401
2017-12-10T15:43:58.799672: step 1005, loss 0.405768, acc 0.859375, prec 0.0341562, recall 0.775401
2017-12-10T15:43:58.997906: step 1006, loss 2.1758, acc 0.859375, prec 0.0341888, recall 0.774813
2017-12-10T15:43:59.197720: step 1007, loss 0.297011, acc 0.875, prec 0.034176, recall 0.774813
2017-12-10T15:43:59.404433: step 1008, loss 0.329149, acc 0.859375, prec 0.0342069, recall 0.775053
2017-12-10T15:43:59.601478: step 1009, loss 1.26431, acc 0.84375, prec 0.0342363, recall 0.775293
2017-12-10T15:43:59.802079: step 1010, loss 0.317675, acc 0.890625, prec 0.0342704, recall 0.775532
2017-12-10T15:43:59.998856: step 1011, loss 0.304372, acc 0.890625, prec 0.0343045, recall 0.77577
2017-12-10T15:44:00.196531: step 1012, loss 0.222428, acc 0.90625, prec 0.0343856, recall 0.776246
2017-12-10T15:44:00.394119: step 1013, loss 0.414591, acc 0.84375, prec 0.0343694, recall 0.776246
2017-12-10T15:44:00.591993: step 1014, loss 1.36371, acc 0.78125, prec 0.0343922, recall 0.776483
2017-12-10T15:44:00.786550: step 1015, loss 0.476856, acc 0.796875, prec 0.0343712, recall 0.776483
2017-12-10T15:44:00.982059: step 1016, loss 0.507414, acc 0.828125, prec 0.0343535, recall 0.776483
2017-12-10T15:44:01.184813: step 1017, loss 0.531763, acc 0.84375, prec 0.0343826, recall 0.77672
2017-12-10T15:44:01.384350: step 1018, loss 0.489405, acc 0.828125, prec 0.0344101, recall 0.776956
2017-12-10T15:44:01.584613: step 1019, loss 3.15347, acc 0.84375, prec 0.0343956, recall 0.776135
2017-12-10T15:44:01.783674: step 1020, loss 3.31761, acc 0.859375, prec 0.0344279, recall 0.775553
2017-12-10T15:44:01.986638: step 1021, loss 0.744967, acc 0.796875, prec 0.0344521, recall 0.775789
2017-12-10T15:44:02.180522: step 1022, loss 0.501729, acc 0.828125, prec 0.0344795, recall 0.776025
2017-12-10T15:44:02.374084: step 1023, loss 0.695335, acc 0.796875, prec 0.0345037, recall 0.77626
2017-12-10T15:44:02.576232: step 1024, loss 0.729128, acc 0.796875, prec 0.0345278, recall 0.776495
2017-12-10T15:44:02.769946: step 1025, loss 0.718852, acc 0.734375, prec 0.0345004, recall 0.776495
2017-12-10T15:44:02.969030: step 1026, loss 0.478717, acc 0.78125, prec 0.0345229, recall 0.77673
2017-12-10T15:44:03.163853: step 1027, loss 1.2128, acc 0.65625, prec 0.0344876, recall 0.77673
2017-12-10T15:44:03.365602: step 1028, loss 0.981658, acc 0.703125, prec 0.034502, recall 0.776963
2017-12-10T15:44:03.564022: step 1029, loss 0.90429, acc 0.625, prec 0.0344635, recall 0.776963
2017-12-10T15:44:03.758860: step 1030, loss 1.14138, acc 0.65625, prec 0.0344284, recall 0.776963
2017-12-10T15:44:03.955271: step 1031, loss 0.959572, acc 0.6875, prec 0.0343964, recall 0.776963
2017-12-10T15:44:04.153791: step 1032, loss 0.950657, acc 0.71875, prec 0.0344125, recall 0.777197
2017-12-10T15:44:04.349013: step 1033, loss 0.579878, acc 0.78125, prec 0.0344796, recall 0.777662
2017-12-10T15:44:04.545314: step 1034, loss 2.0949, acc 0.65625, prec 0.034623, recall 0.778586
2017-12-10T15:44:04.742414: step 1035, loss 0.590212, acc 0.8125, prec 0.034693, recall 0.779046
2017-12-10T15:44:04.937205: step 1036, loss 0.426052, acc 0.828125, prec 0.0346754, recall 0.779046
2017-12-10T15:44:05.135221: step 1037, loss 0.52487, acc 0.828125, prec 0.0347469, recall 0.779503
2017-12-10T15:44:05.331067: step 1038, loss 1.7258, acc 0.765625, prec 0.0348119, recall 0.779959
2017-12-10T15:44:05.527451: step 1039, loss 0.374229, acc 0.859375, prec 0.0348419, recall 0.780186
2017-12-10T15:44:05.725745: step 1040, loss 0.683619, acc 0.828125, prec 0.0348243, recall 0.780186
2017-12-10T15:44:05.922814: step 1041, loss 0.53074, acc 0.8125, prec 0.034805, recall 0.780186
2017-12-10T15:44:06.118363: step 1042, loss 0.497953, acc 0.84375, prec 0.034789, recall 0.780186
2017-12-10T15:44:06.312891: step 1043, loss 0.356979, acc 0.859375, prec 0.0348634, recall 0.780639
2017-12-10T15:44:06.506681: step 1044, loss 0.785869, acc 0.75, prec 0.0348378, recall 0.780639
2017-12-10T15:44:06.702057: step 1045, loss 0.417683, acc 0.8125, prec 0.0348629, recall 0.780864
2017-12-10T15:44:06.894746: step 1046, loss 0.555006, acc 0.8125, prec 0.0348437, recall 0.780864
2017-12-10T15:44:07.089879: step 1047, loss 0.314961, acc 0.859375, prec 0.0348736, recall 0.781089
2017-12-10T15:44:07.282296: step 1048, loss 0.222732, acc 0.921875, prec 0.0349099, recall 0.781314
2017-12-10T15:44:07.481969: step 1049, loss 0.372731, acc 0.84375, prec 0.0349381, recall 0.781538
2017-12-10T15:44:07.679487: step 1050, loss 0.394397, acc 0.90625, prec 0.0349727, recall 0.781762
2017-12-10T15:44:07.876619: step 1051, loss 0.34576, acc 0.859375, prec 0.0349583, recall 0.781762
2017-12-10T15:44:08.078929: step 1052, loss 0.38648, acc 0.9375, prec 0.0349961, recall 0.781986
2017-12-10T15:44:08.276788: step 1053, loss 0.35741, acc 0.9375, prec 0.0350339, recall 0.782209
2017-12-10T15:44:08.475999: step 1054, loss 0.12597, acc 0.96875, prec 0.0350307, recall 0.782209
2017-12-10T15:44:08.668485: step 1055, loss 0.139082, acc 0.9375, prec 0.0350243, recall 0.782209
2017-12-10T15:44:08.866649: step 1056, loss 0.47155, acc 0.921875, prec 0.0350604, recall 0.782431
2017-12-10T15:44:09.064793: step 1057, loss 0.306844, acc 0.921875, prec 0.0350524, recall 0.782431
2017-12-10T15:44:09.264217: step 1058, loss 0.0485097, acc 0.96875, prec 0.0350933, recall 0.782653
2017-12-10T15:44:09.461637: step 1059, loss 0.275344, acc 0.984375, prec 0.03518, recall 0.783096
2017-12-10T15:44:09.659071: step 1060, loss 0.159559, acc 0.9375, prec 0.0352618, recall 0.783537
2017-12-10T15:44:09.855444: step 1061, loss 0.444151, acc 0.953125, prec 0.0353011, recall 0.783756
2017-12-10T15:44:10.052989: step 1062, loss 0.0879084, acc 0.953125, prec 0.0352963, recall 0.783756
2017-12-10T15:44:10.249312: step 1063, loss 0.227841, acc 0.90625, prec 0.0352866, recall 0.783756
2017-12-10T15:44:10.444036: step 1064, loss 0.118261, acc 0.9375, prec 0.0352801, recall 0.783756
2017-12-10T15:44:10.642534: step 1065, loss 3.34343, acc 0.921875, prec 0.0352753, recall 0.782168
2017-12-10T15:44:10.836904: step 1066, loss 0.155395, acc 0.9375, prec 0.0353129, recall 0.782389
2017-12-10T15:44:11.030384: step 1067, loss 0.246065, acc 0.890625, prec 0.0353016, recall 0.782389
2017-12-10T15:44:11.234800: step 1068, loss 0.169463, acc 0.9375, prec 0.0353833, recall 0.782828
2017-12-10T15:44:11.433450: step 1069, loss 0.480266, acc 0.9375, prec 0.0354649, recall 0.783266
2017-12-10T15:44:11.632318: step 1070, loss 1.35722, acc 0.9375, prec 0.03546, recall 0.782477
2017-12-10T15:44:11.830451: step 1071, loss 0.34488, acc 0.859375, prec 0.0354455, recall 0.782477
2017-12-10T15:44:12.024427: step 1072, loss 0.137658, acc 0.9375, prec 0.035439, recall 0.782477
2017-12-10T15:44:12.221596: step 1073, loss 2.96982, acc 0.828125, prec 0.0354228, recall 0.78169
2017-12-10T15:44:12.419725: step 1074, loss 1.73783, acc 0.875, prec 0.0354115, recall 0.780905
2017-12-10T15:44:12.618868: step 1075, loss 0.810903, acc 0.75, prec 0.0353857, recall 0.780905
2017-12-10T15:44:12.816706: step 1076, loss 0.511808, acc 0.8125, prec 0.0353664, recall 0.780905
2017-12-10T15:44:13.011294: step 1077, loss 0.38333, acc 0.875, prec 0.0353974, recall 0.781124
2017-12-10T15:44:13.214924: step 1078, loss 0.806622, acc 0.796875, prec 0.0354204, recall 0.781344
2017-12-10T15:44:13.410248: step 1079, loss 0.740185, acc 0.734375, prec 0.035393, recall 0.781344
2017-12-10T15:44:13.605759: step 1080, loss 0.824549, acc 0.75, prec 0.0354549, recall 0.781782
2017-12-10T15:44:13.802183: step 1081, loss 0.75887, acc 0.75, prec 0.0354291, recall 0.781782
2017-12-10T15:44:13.997023: step 1082, loss 0.72853, acc 0.75, prec 0.0354472, recall 0.782
2017-12-10T15:44:14.192641: step 1083, loss 0.324629, acc 0.828125, prec 0.0354732, recall 0.782218
2017-12-10T15:44:14.397441: step 1084, loss 0.547728, acc 0.8125, prec 0.0354976, recall 0.782435
2017-12-10T15:44:14.591631: step 1085, loss 0.843854, acc 0.765625, prec 0.0355608, recall 0.782869
2017-12-10T15:44:14.787660: step 1086, loss 0.991402, acc 0.703125, prec 0.0355738, recall 0.783085
2017-12-10T15:44:14.982578: step 1087, loss 0.657304, acc 0.8125, prec 0.0355981, recall 0.7833
2017-12-10T15:44:15.176221: step 1088, loss 0.545523, acc 0.796875, prec 0.0355772, recall 0.7833
2017-12-10T15:44:15.371061: step 1089, loss 7.10882, acc 0.796875, prec 0.0356015, recall 0.782738
2017-12-10T15:44:15.569034: step 1090, loss 0.790002, acc 0.78125, prec 0.0356225, recall 0.782953
2017-12-10T15:44:15.763629: step 1091, loss 1.06569, acc 0.828125, prec 0.0356483, recall 0.783168
2017-12-10T15:44:15.964099: step 1092, loss 1.70945, acc 0.8125, prec 0.0357593, recall 0.78381
2017-12-10T15:44:16.162095: step 1093, loss 1.15524, acc 0.65625, prec 0.0357239, recall 0.78381
2017-12-10T15:44:16.358240: step 1094, loss 0.806776, acc 0.734375, prec 0.03574, recall 0.784024
2017-12-10T15:44:16.556008: step 1095, loss 1.16237, acc 0.6875, prec 0.0358378, recall 0.784661
2017-12-10T15:44:16.755333: step 1096, loss 0.580654, acc 0.71875, prec 0.0358088, recall 0.784661
2017-12-10T15:44:16.945934: step 1097, loss 0.873001, acc 0.71875, prec 0.0357799, recall 0.784661
2017-12-10T15:44:17.138749: step 1098, loss 0.607892, acc 0.796875, prec 0.0358023, recall 0.784872
2017-12-10T15:44:17.333133: step 1099, loss 0.351537, acc 0.84375, prec 0.0357863, recall 0.784872
2017-12-10T15:44:17.533523: step 1100, loss 0.563662, acc 0.765625, prec 0.0357622, recall 0.784872
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_3/1512938428/checkpoints/model-1100

2017-12-10T15:44:18.693001: step 1101, loss 0.601981, acc 0.8125, prec 0.0357862, recall 0.785083
2017-12-10T15:44:18.890358: step 1102, loss 0.779501, acc 0.765625, prec 0.0358053, recall 0.785294
2017-12-10T15:44:19.086933: step 1103, loss 0.503722, acc 0.828125, prec 0.0357877, recall 0.785294
2017-12-10T15:44:19.282434: step 1104, loss 4.14655, acc 0.796875, prec 0.0357685, recall 0.784525
2017-12-10T15:44:19.483145: step 1105, loss 2.53157, acc 0.859375, prec 0.0357988, recall 0.783969
2017-12-10T15:44:19.678774: step 1106, loss 1.59658, acc 0.75, prec 0.0358592, recall 0.78439
2017-12-10T15:44:19.875856: step 1107, loss 0.588579, acc 0.8125, prec 0.035926, recall 0.78481
2017-12-10T15:44:20.068027: step 1108, loss 0.399595, acc 0.859375, prec 0.0359975, recall 0.785228
2017-12-10T15:44:20.265225: step 1109, loss 0.641911, acc 0.828125, prec 0.0359799, recall 0.785228
2017-12-10T15:44:20.459586: step 1110, loss 0.677624, acc 0.78125, prec 0.0360004, recall 0.785437
2017-12-10T15:44:20.650142: step 1111, loss 0.725408, acc 0.78125, prec 0.0359779, recall 0.785437
2017-12-10T15:44:20.848356: step 1112, loss 0.562043, acc 0.703125, prec 0.0359904, recall 0.785645
2017-12-10T15:44:21.043852: step 1113, loss 0.760689, acc 0.765625, prec 0.0359664, recall 0.785645
2017-12-10T15:44:21.241261: step 1114, loss 0.828561, acc 0.765625, prec 0.0359853, recall 0.785853
2017-12-10T15:44:21.438146: step 1115, loss 1.13251, acc 0.78125, prec 0.0360484, recall 0.786267
2017-12-10T15:44:21.634394: step 1116, loss 0.871384, acc 0.75, prec 0.0361083, recall 0.78668
2017-12-10T15:44:21.828592: step 1117, loss 1.71625, acc 0.71875, prec 0.0361648, recall 0.787091
2017-12-10T15:44:22.028813: step 1118, loss 0.855465, acc 0.75, prec 0.0361819, recall 0.787295
2017-12-10T15:44:22.227581: step 1119, loss 0.513731, acc 0.875, prec 0.0362543, recall 0.787704
2017-12-10T15:44:22.426959: step 1120, loss 2.39714, acc 0.828125, prec 0.0362383, recall 0.786948
2017-12-10T15:44:22.621389: step 1121, loss 0.578059, acc 0.765625, prec 0.0362568, recall 0.787152
2017-12-10T15:44:22.818495: step 1122, loss 0.523186, acc 0.828125, prec 0.0362392, recall 0.787152
2017-12-10T15:44:23.016760: step 1123, loss 0.361095, acc 0.8125, prec 0.0362626, recall 0.787356
2017-12-10T15:44:23.208934: step 1124, loss 1.08357, acc 0.71875, prec 0.0362763, recall 0.78756
2017-12-10T15:44:23.401642: step 1125, loss 0.630016, acc 0.84375, prec 0.0363452, recall 0.787966
2017-12-10T15:44:23.599914: step 1126, loss 0.853159, acc 0.765625, prec 0.0363636, recall 0.788168
2017-12-10T15:44:23.796236: step 1127, loss 0.556882, acc 0.796875, prec 0.0364276, recall 0.788571
2017-12-10T15:44:23.998331: step 1128, loss 1.0044, acc 0.609375, prec 0.03643, recall 0.788773
2017-12-10T15:44:24.191320: step 1129, loss 0.713942, acc 0.8125, prec 0.0365377, recall 0.789374
2017-12-10T15:44:24.387020: step 1130, loss 0.573605, acc 0.828125, prec 0.0365201, recall 0.789374
2017-12-10T15:44:24.585487: step 1131, loss 0.25667, acc 0.90625, prec 0.0365104, recall 0.789374
2017-12-10T15:44:24.781566: step 1132, loss 0.401072, acc 0.828125, prec 0.0364928, recall 0.789374
2017-12-10T15:44:24.974526: step 1133, loss 0.443312, acc 0.84375, prec 0.0365191, recall 0.789573
2017-12-10T15:44:25.165747: step 1134, loss 0.29628, acc 0.859375, prec 0.0365891, recall 0.789972
2017-12-10T15:44:25.364839: step 1135, loss 0.240882, acc 0.9375, prec 0.0365827, recall 0.789972
2017-12-10T15:44:25.560042: step 1136, loss 4.92773, acc 0.890625, prec 0.0365731, recall 0.789225
2017-12-10T15:44:25.760591: step 1137, loss 2.03257, acc 0.921875, prec 0.036651, recall 0.788878
2017-12-10T15:44:25.960866: step 1138, loss 0.305566, acc 0.859375, prec 0.0366788, recall 0.789077
2017-12-10T15:44:26.153714: step 1139, loss 0.309488, acc 0.90625, prec 0.0367534, recall 0.789474
2017-12-10T15:44:26.348326: step 1140, loss 0.602394, acc 0.84375, prec 0.0367374, recall 0.789474
2017-12-10T15:44:26.551388: step 1141, loss 5.74751, acc 0.796875, prec 0.0367181, recall 0.788732
2017-12-10T15:44:26.755743: step 1142, loss 0.807045, acc 0.828125, prec 0.0367425, recall 0.788931
2017-12-10T15:44:26.951783: step 1143, loss 0.773775, acc 0.8125, prec 0.0367653, recall 0.789128
2017-12-10T15:44:27.145864: step 1144, loss 0.647331, acc 0.828125, prec 0.0367897, recall 0.789326
2017-12-10T15:44:27.341534: step 1145, loss 0.546278, acc 0.78125, prec 0.0368513, recall 0.78972
2017-12-10T15:44:27.533013: step 1146, loss 0.574468, acc 0.734375, prec 0.036824, recall 0.78972
2017-12-10T15:44:27.728539: step 1147, loss 0.625565, acc 0.734375, prec 0.0368806, recall 0.790112
2017-12-10T15:44:27.927058: step 1148, loss 0.624673, acc 0.703125, prec 0.0368501, recall 0.790112
2017-12-10T15:44:28.125915: step 1149, loss 0.668274, acc 0.75, prec 0.0368664, recall 0.790308
2017-12-10T15:44:28.320375: step 1150, loss 0.918283, acc 0.703125, prec 0.0368359, recall 0.790308
2017-12-10T15:44:28.514993: step 1151, loss 0.683227, acc 0.78125, prec 0.0368135, recall 0.790308
2017-12-10T15:44:28.711116: step 1152, loss 0.747736, acc 0.734375, prec 0.0367864, recall 0.790308
2017-12-10T15:44:28.904050: step 1153, loss 1.01874, acc 0.703125, prec 0.0368396, recall 0.790698
2017-12-10T15:44:29.097358: step 1154, loss 1.07563, acc 0.796875, prec 0.036944, recall 0.79128
2017-12-10T15:44:29.298364: step 1155, loss 3.25247, acc 0.75, prec 0.0369617, recall 0.790741
2017-12-10T15:44:29.498725: step 1156, loss 0.581823, acc 0.875, prec 0.0369906, recall 0.790934
2017-12-10T15:44:29.696590: step 1157, loss 0.675716, acc 0.765625, prec 0.0369666, recall 0.790934
2017-12-10T15:44:29.892614: step 1158, loss 0.395698, acc 0.859375, prec 0.0370771, recall 0.791513
2017-12-10T15:44:30.090562: step 1159, loss 0.730967, acc 0.703125, prec 0.0370882, recall 0.791705
2017-12-10T15:44:30.285752: step 1160, loss 0.936925, acc 0.6875, prec 0.0370562, recall 0.791705
2017-12-10T15:44:30.479793: step 1161, loss 0.865678, acc 0.6875, prec 0.0370243, recall 0.791705
2017-12-10T15:44:30.673819: step 1162, loss 0.526649, acc 0.84375, prec 0.0370083, recall 0.791705
2017-12-10T15:44:30.869868: step 1163, loss 0.592762, acc 0.75, prec 0.0369828, recall 0.791705
2017-12-10T15:44:31.070793: step 1164, loss 0.608569, acc 0.84375, prec 0.0370083, recall 0.791897
2017-12-10T15:44:31.271028: step 1165, loss 0.35725, acc 0.875, prec 0.0370785, recall 0.792279
2017-12-10T15:44:31.467711: step 1166, loss 0.500329, acc 0.84375, prec 0.0370625, recall 0.792279
2017-12-10T15:44:31.671225: step 1167, loss 0.398521, acc 0.859375, prec 0.0370896, recall 0.79247
2017-12-10T15:44:31.865732: step 1168, loss 0.635107, acc 0.875, prec 0.0370768, recall 0.79247
2017-12-10T15:44:32.058002: step 1169, loss 0.515437, acc 0.875, prec 0.0371054, recall 0.792661
2017-12-10T15:44:32.249212: step 1170, loss 0.391484, acc 0.859375, prec 0.0370911, recall 0.792661
2017-12-10T15:44:32.443174: step 1171, loss 1.45834, acc 0.875, prec 0.037161, recall 0.79304
2017-12-10T15:44:32.641428: step 1172, loss 0.312778, acc 0.9375, prec 0.0371546, recall 0.79304
2017-12-10T15:44:32.838308: step 1173, loss 0.634536, acc 0.90625, prec 0.0371864, recall 0.79323
2017-12-10T15:44:33.029328: step 1174, loss 0.270651, acc 0.875, prec 0.0371736, recall 0.79323
2017-12-10T15:44:33.226511: step 1175, loss 0.147565, acc 0.953125, prec 0.0371688, recall 0.79323
2017-12-10T15:44:33.419768: step 1176, loss 0.211448, acc 0.90625, prec 0.0371593, recall 0.79323
2017-12-10T15:44:33.615093: step 1177, loss 0.313889, acc 0.890625, prec 0.0371481, recall 0.79323
2017-12-10T15:44:33.812922: step 1178, loss 0.307367, acc 0.921875, prec 0.0372226, recall 0.793607
2017-12-10T15:44:34.012846: step 1179, loss 0.835131, acc 0.921875, prec 0.0372971, recall 0.793984
2017-12-10T15:44:34.205532: step 1180, loss 0.209047, acc 0.9375, prec 0.0372907, recall 0.793984
2017-12-10T15:44:34.400895: step 1181, loss 0.319913, acc 0.90625, prec 0.0373224, recall 0.794171
2017-12-10T15:44:34.595489: step 1182, loss 0.0838727, acc 0.96875, prec 0.0373604, recall 0.794358
2017-12-10T15:44:34.789749: step 1183, loss 0.108009, acc 0.9375, prec 0.037354, recall 0.794358
2017-12-10T15:44:34.987956: step 1184, loss 0.182011, acc 0.9375, prec 0.0373888, recall 0.794545
2017-12-10T15:44:35.184230: step 1185, loss 0.237976, acc 0.90625, prec 0.0374203, recall 0.794732
2017-12-10T15:44:35.380892: step 1186, loss 4.71761, acc 0.921875, prec 0.0374139, recall 0.794011
2017-12-10T15:44:35.583728: step 1187, loss 0.824149, acc 0.90625, prec 0.0374866, recall 0.794384
2017-12-10T15:44:35.783238: step 1188, loss 0.490685, acc 0.953125, prec 0.0375641, recall 0.794756
2017-12-10T15:44:35.979535: step 1189, loss 0.181921, acc 0.890625, prec 0.0375529, recall 0.794756
2017-12-10T15:44:36.175541: step 1190, loss 0.26075, acc 0.9375, prec 0.0375876, recall 0.794941
2017-12-10T15:44:36.372834: step 1191, loss 0.382234, acc 0.875, prec 0.0376158, recall 0.795126
2017-12-10T15:44:36.572435: step 1192, loss 9.96616, acc 0.859375, prec 0.0376046, recall 0.793694
2017-12-10T15:44:36.768252: step 1193, loss 0.404427, acc 0.828125, prec 0.037669, recall 0.794065
2017-12-10T15:44:36.962798: step 1194, loss 0.53129, acc 0.828125, prec 0.0376514, recall 0.794065
2017-12-10T15:44:37.165291: step 1195, loss 3.76682, acc 0.75, prec 0.0377503, recall 0.793907
2017-12-10T15:44:37.361936: step 1196, loss 0.57708, acc 0.734375, prec 0.037723, recall 0.793907
2017-12-10T15:44:37.559472: step 1197, loss 0.861146, acc 0.6875, prec 0.0376909, recall 0.793907
2017-12-10T15:44:37.752781: step 1198, loss 0.768442, acc 0.703125, prec 0.0376605, recall 0.793907
2017-12-10T15:44:37.948418: step 1199, loss 0.742473, acc 0.796875, prec 0.0376805, recall 0.794091
2017-12-10T15:44:38.144896: step 1200, loss 1.31566, acc 0.640625, prec 0.0376846, recall 0.794275
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_3/1512938428/checkpoints/model-1200

2017-12-10T15:44:39.343311: step 1201, loss 1.16946, acc 0.65625, prec 0.0376903, recall 0.794459
2017-12-10T15:44:39.541364: step 1202, loss 1.38784, acc 0.65625, prec 0.0377774, recall 0.795009
2017-12-10T15:44:39.739392: step 1203, loss 1.43847, acc 0.59375, prec 0.0377766, recall 0.795191
2017-12-10T15:44:39.934001: step 1204, loss 0.935614, acc 0.671875, prec 0.037743, recall 0.795191
2017-12-10T15:44:40.129316: step 1205, loss 0.866428, acc 0.671875, prec 0.0377096, recall 0.795191
2017-12-10T15:44:40.326980: step 1206, loss 0.876912, acc 0.71875, prec 0.0377215, recall 0.795374
2017-12-10T15:44:40.522707: step 1207, loss 1.16401, acc 0.71875, prec 0.0377335, recall 0.795556
2017-12-10T15:44:40.713791: step 1208, loss 0.777894, acc 0.703125, prec 0.0377438, recall 0.795737
2017-12-10T15:44:40.909661: step 1209, loss 0.86755, acc 0.71875, prec 0.0377152, recall 0.795737
2017-12-10T15:44:41.109425: step 1210, loss 0.799088, acc 0.71875, prec 0.0377271, recall 0.795918
2017-12-10T15:44:41.308427: step 1211, loss 0.622921, acc 0.78125, prec 0.0377049, recall 0.795918
2017-12-10T15:44:41.501673: step 1212, loss 0.85154, acc 0.65625, prec 0.0377105, recall 0.796099
2017-12-10T15:44:41.695265: step 1213, loss 1.32639, acc 0.78125, prec 0.0378095, recall 0.79664
2017-12-10T15:44:41.888267: step 1214, loss 5.66402, acc 0.796875, prec 0.0377905, recall 0.795936
2017-12-10T15:44:42.087434: step 1215, loss 0.323373, acc 0.90625, prec 0.0377809, recall 0.795936
2017-12-10T15:44:42.281529: step 1216, loss 0.383487, acc 0.859375, prec 0.0377667, recall 0.795936
2017-12-10T15:44:42.476949: step 1217, loss 0.818227, acc 0.875, prec 0.0377944, recall 0.796117
2017-12-10T15:44:42.681055: step 1218, loss 0.505646, acc 0.84375, prec 0.0377785, recall 0.796117
2017-12-10T15:44:42.878424: step 1219, loss 0.253916, acc 0.90625, prec 0.037769, recall 0.796117
2017-12-10T15:44:43.074251: step 1220, loss 1.33039, acc 0.78125, prec 0.0378274, recall 0.796476
2017-12-10T15:44:43.270478: step 1221, loss 0.658381, acc 0.78125, prec 0.0378053, recall 0.796476
2017-12-10T15:44:43.465555: step 1222, loss 0.204072, acc 0.921875, prec 0.0378778, recall 0.796834
2017-12-10T15:44:43.662574: step 1223, loss 0.603344, acc 0.796875, prec 0.0378975, recall 0.797012
2017-12-10T15:44:43.857581: step 1224, loss 2.29942, acc 0.78125, prec 0.0378769, recall 0.796313
2017-12-10T15:44:44.054759: step 1225, loss 0.716874, acc 0.84375, prec 0.0379012, recall 0.796491
2017-12-10T15:44:44.263180: step 1226, loss 0.631945, acc 0.78125, prec 0.0378791, recall 0.796491
2017-12-10T15:44:44.463216: step 1227, loss 0.722961, acc 0.765625, prec 0.0378955, recall 0.79667
2017-12-10T15:44:44.661131: step 1228, loss 0.350531, acc 0.8125, prec 0.0378766, recall 0.79667
2017-12-10T15:44:44.857074: step 1229, loss 1.13135, acc 0.859375, prec 0.0379425, recall 0.797025
2017-12-10T15:44:45.053483: step 1230, loss 0.645302, acc 0.84375, prec 0.0379267, recall 0.797025
2017-12-10T15:44:45.245657: step 1231, loss 0.60535, acc 0.890625, prec 0.0379557, recall 0.797203
2017-12-10T15:44:45.440285: step 1232, loss 0.572802, acc 0.765625, prec 0.0379721, recall 0.79738
2017-12-10T15:44:45.634790: step 1233, loss 1.3997, acc 0.78125, prec 0.0379899, recall 0.797557
2017-12-10T15:44:45.831659: step 1234, loss 0.333879, acc 0.90625, prec 0.0379805, recall 0.797557
2017-12-10T15:44:46.024098: step 1235, loss 0.422364, acc 0.890625, prec 0.0379694, recall 0.797557
2017-12-10T15:44:46.217592: step 1236, loss 1.00826, acc 0.8125, prec 0.0379905, recall 0.797733
2017-12-10T15:44:46.414339: step 1237, loss 0.544623, acc 0.859375, prec 0.0380561, recall 0.798085
2017-12-10T15:44:46.609295: step 1238, loss 0.342387, acc 0.875, prec 0.0380435, recall 0.798085
2017-12-10T15:44:46.805798: step 1239, loss 0.409689, acc 0.8125, prec 0.0380245, recall 0.798085
2017-12-10T15:44:47.006866: step 1240, loss 0.371574, acc 0.859375, prec 0.0380104, recall 0.798085
2017-12-10T15:44:47.203180: step 1241, loss 0.469196, acc 0.765625, prec 0.0379867, recall 0.798085
2017-12-10T15:44:47.401342: step 1242, loss 3.93627, acc 0.796875, prec 0.0380077, recall 0.797567
2017-12-10T15:44:47.604035: step 1243, loss 0.540565, acc 0.9375, prec 0.0380412, recall 0.797743
2017-12-10T15:44:47.801655: step 1244, loss 0.514, acc 0.765625, prec 0.0380972, recall 0.798094
2017-12-10T15:44:47.996710: step 1245, loss 0.465272, acc 0.84375, prec 0.0381212, recall 0.798268
2017-12-10T15:44:48.194948: step 1246, loss 0.92699, acc 0.78125, prec 0.0381787, recall 0.798617
2017-12-10T15:44:48.393600: step 1247, loss 0.513371, acc 0.84375, prec 0.0381629, recall 0.798617
2017-12-10T15:44:48.587572: step 1248, loss 0.614532, acc 0.734375, prec 0.0381758, recall 0.798791
2017-12-10T15:44:48.780922: step 1249, loss 0.497794, acc 0.84375, prec 0.0381997, recall 0.798965
2017-12-10T15:44:48.977330: step 1250, loss 0.626888, acc 0.78125, prec 0.0382966, recall 0.799484
2017-12-10T15:44:49.170592: step 1251, loss 0.51709, acc 0.8125, prec 0.0383173, recall 0.799656
2017-12-10T15:44:49.367276: step 1252, loss 0.706064, acc 0.78125, prec 0.0383348, recall 0.799828
2017-12-10T15:44:49.567367: step 1253, loss 4.54607, acc 0.75, prec 0.0383903, recall 0.799486
2017-12-10T15:44:49.773656: step 1254, loss 0.480576, acc 0.84375, prec 0.0383745, recall 0.799486
2017-12-10T15:44:49.970095: step 1255, loss 0.701823, acc 0.78125, prec 0.038392, recall 0.799658
2017-12-10T15:44:50.166364: step 1256, loss 0.762489, acc 0.890625, prec 0.0384995, recall 0.800171
2017-12-10T15:44:50.359393: step 1257, loss 0.531087, acc 0.78125, prec 0.0385168, recall 0.800341
2017-12-10T15:44:50.557522: step 1258, loss 0.497369, acc 0.8125, prec 0.0384978, recall 0.800341
2017-12-10T15:44:50.750990: step 1259, loss 0.649349, acc 0.859375, prec 0.0386414, recall 0.80102
2017-12-10T15:44:50.949735: step 1260, loss 0.491246, acc 0.796875, prec 0.0386602, recall 0.801189
2017-12-10T15:44:51.144138: step 1261, loss 1.61041, acc 0.765625, prec 0.0387152, recall 0.801527
2017-12-10T15:44:51.341752: step 1262, loss 0.415895, acc 0.84375, prec 0.0386994, recall 0.801527
2017-12-10T15:44:51.539298: step 1263, loss 0.655892, acc 0.796875, prec 0.0386788, recall 0.801527
2017-12-10T15:44:51.734099: step 1264, loss 0.261209, acc 0.875, prec 0.0386661, recall 0.801527
2017-12-10T15:44:51.927091: step 1265, loss 0.302324, acc 0.84375, prec 0.0386503, recall 0.801527
2017-12-10T15:44:52.125995: step 1266, loss 0.337291, acc 0.90625, prec 0.0386408, recall 0.801527
2017-12-10T15:44:52.321599: step 1267, loss 0.465846, acc 0.84375, prec 0.0386643, recall 0.801695
2017-12-10T15:44:52.519490: step 1268, loss 0.37453, acc 0.8125, prec 0.0386454, recall 0.801695
2017-12-10T15:44:52.716675: step 1269, loss 1.16561, acc 0.890625, prec 0.0386736, recall 0.801863
2017-12-10T15:44:52.915034: step 1270, loss 0.403873, acc 0.890625, prec 0.0386625, recall 0.801863
2017-12-10T15:44:53.111305: step 1271, loss 0.766406, acc 0.796875, prec 0.0386812, recall 0.80203
2017-12-10T15:44:53.303705: step 1272, loss 0.709135, acc 0.921875, prec 0.0387126, recall 0.802198
2017-12-10T15:44:53.506228: step 1273, loss 0.255123, acc 0.890625, prec 0.0387015, recall 0.802198
2017-12-10T15:44:53.704382: step 1274, loss 0.29137, acc 0.890625, prec 0.0386905, recall 0.802198
2017-12-10T15:44:53.902055: step 1275, loss 0.407064, acc 0.90625, prec 0.0387594, recall 0.802532
2017-12-10T15:44:54.096072: step 1276, loss 0.22882, acc 0.890625, prec 0.0387483, recall 0.802532
2017-12-10T15:44:54.296661: step 1277, loss 1.43615, acc 0.890625, prec 0.0388156, recall 0.802864
2017-12-10T15:44:54.496078: step 1278, loss 1.05092, acc 0.875, prec 0.0388421, recall 0.80303
2017-12-10T15:44:54.694049: step 1279, loss 2.12232, acc 0.796875, prec 0.0388622, recall 0.802521
2017-12-10T15:44:54.889114: step 1280, loss 1.46218, acc 0.859375, prec 0.0389653, recall 0.803018
2017-12-10T15:44:55.091319: step 1281, loss 0.496841, acc 0.8125, prec 0.0389853, recall 0.803183
2017-12-10T15:44:55.281481: step 1282, loss 0.468509, acc 0.78125, prec 0.0390022, recall 0.803347
2017-12-10T15:44:55.474124: step 1283, loss 0.575823, acc 0.75, prec 0.0389769, recall 0.803347
2017-12-10T15:44:55.672488: step 1284, loss 0.556827, acc 0.84375, prec 0.039, recall 0.803512
2017-12-10T15:44:55.870844: step 1285, loss 0.493372, acc 0.8125, prec 0.039059, recall 0.80384
2017-12-10T15:44:56.063170: step 1286, loss 0.761602, acc 0.75, prec 0.0390337, recall 0.80384
2017-12-10T15:44:56.259917: step 1287, loss 0.558363, acc 0.765625, prec 0.0390489, recall 0.804003
2017-12-10T15:44:56.459153: step 1288, loss 0.895293, acc 0.71875, prec 0.0390593, recall 0.804167
2017-12-10T15:44:56.657127: step 1289, loss 0.79141, acc 0.765625, prec 0.0390356, recall 0.804167
2017-12-10T15:44:56.853646: step 1290, loss 0.728188, acc 0.78125, prec 0.0390524, recall 0.80433
2017-12-10T15:44:57.049274: step 1291, loss 0.441725, acc 0.78125, prec 0.0390691, recall 0.804493
2017-12-10T15:44:57.240331: step 1292, loss 0.791982, acc 0.765625, prec 0.0390455, recall 0.804493
2017-12-10T15:44:57.434518: step 1293, loss 0.443199, acc 0.78125, prec 0.0390234, recall 0.804493
2017-12-10T15:44:57.626188: step 1294, loss 0.57385, acc 0.765625, prec 0.0389998, recall 0.804493
2017-12-10T15:44:57.817302: step 1295, loss 0.584849, acc 0.8125, prec 0.0390197, recall 0.804655
2017-12-10T15:44:58.010891: step 1296, loss 0.964959, acc 0.8125, prec 0.0391169, recall 0.805141
2017-12-10T15:44:58.204700: step 1297, loss 0.6561, acc 0.765625, prec 0.039132, recall 0.805302
2017-12-10T15:44:58.402659: step 1298, loss 0.568825, acc 0.78125, prec 0.0391873, recall 0.805624
2017-12-10T15:44:58.597980: step 1299, loss 0.472244, acc 0.859375, prec 0.0392117, recall 0.805785
2017-12-10T15:44:58.790307: step 1300, loss 0.436706, acc 0.9375, prec 0.0392441, recall 0.805946
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_3/1512938428/checkpoints/model-1300

2017-12-10T15:44:59.970969: step 1301, loss 1.46437, acc 0.875, prec 0.039233, recall 0.805281
2017-12-10T15:45:00.169527: step 1302, loss 0.182692, acc 0.921875, prec 0.0392638, recall 0.805441
2017-12-10T15:45:00.364804: step 1303, loss 0.23065, acc 0.90625, prec 0.0392543, recall 0.805441
2017-12-10T15:45:00.558885: step 1304, loss 0.539584, acc 0.796875, prec 0.0392338, recall 0.805441
2017-12-10T15:45:00.755812: step 1305, loss 0.323236, acc 0.859375, prec 0.0392196, recall 0.805441
2017-12-10T15:45:00.949723: step 1306, loss 0.208599, acc 0.9375, prec 0.0392519, recall 0.805601
2017-12-10T15:45:01.148601: step 1307, loss 0.13865, acc 0.984375, prec 0.0392503, recall 0.805601
2017-12-10T15:45:01.349259: step 1308, loss 0.316117, acc 0.921875, prec 0.0392424, recall 0.805601
2017-12-10T15:45:01.548228: step 1309, loss 0.517338, acc 0.828125, prec 0.0393022, recall 0.805921
2017-12-10T15:45:01.745081: step 1310, loss 0.450428, acc 0.828125, prec 0.0393234, recall 0.806081
2017-12-10T15:45:01.941746: step 1311, loss 0.316056, acc 0.875, prec 0.0393108, recall 0.806081
2017-12-10T15:45:02.135215: step 1312, loss 0.264605, acc 0.890625, prec 0.0392997, recall 0.806081
2017-12-10T15:45:02.328929: step 1313, loss 0.276768, acc 0.9375, prec 0.0393319, recall 0.80624
2017-12-10T15:45:02.523497: step 1314, loss 0.247274, acc 0.9375, prec 0.0393641, recall 0.806399
2017-12-10T15:45:02.718355: step 1315, loss 8.28679, acc 0.953125, prec 0.0394394, recall 0.805397
2017-12-10T15:45:02.915828: step 1316, loss 0.871968, acc 0.921875, prec 0.0395084, recall 0.805714
2017-12-10T15:45:03.109702: step 1317, loss 2.21437, acc 0.921875, prec 0.0396174, recall 0.805533
2017-12-10T15:45:03.311587: step 1318, loss 0.41445, acc 0.9375, prec 0.0396495, recall 0.805691
2017-12-10T15:45:03.509462: step 1319, loss 0.412629, acc 0.890625, prec 0.0396384, recall 0.805691
2017-12-10T15:45:03.707379: step 1320, loss 0.551545, acc 0.84375, prec 0.039661, recall 0.805849
2017-12-10T15:45:03.901961: step 1321, loss 0.626012, acc 0.734375, prec 0.0396724, recall 0.806006
2017-12-10T15:45:04.096906: step 1322, loss 0.973182, acc 0.65625, prec 0.0396376, recall 0.806006
2017-12-10T15:45:04.289696: step 1323, loss 0.966759, acc 0.6875, prec 0.0396059, recall 0.806006
2017-12-10T15:45:04.487606: step 1324, loss 0.586655, acc 0.796875, prec 0.0395854, recall 0.806006
2017-12-10T15:45:04.683009: step 1325, loss 1.0302, acc 0.734375, prec 0.0395586, recall 0.806006
2017-12-10T15:45:04.875439: step 1326, loss 1.06272, acc 0.59375, prec 0.0395559, recall 0.806164
2017-12-10T15:45:05.073113: step 1327, loss 0.899574, acc 0.6875, prec 0.0395626, recall 0.806321
2017-12-10T15:45:05.269500: step 1328, loss 0.455175, acc 0.796875, prec 0.0395422, recall 0.806321
2017-12-10T15:45:05.463550: step 1329, loss 0.832221, acc 0.765625, prec 0.0395186, recall 0.806321
2017-12-10T15:45:05.663111: step 1330, loss 5.58195, acc 0.765625, prec 0.0395364, recall 0.805174
2017-12-10T15:45:05.861788: step 1331, loss 0.774437, acc 0.703125, prec 0.0395066, recall 0.805174
2017-12-10T15:45:06.058493: step 1332, loss 1.35963, acc 0.640625, prec 0.0395086, recall 0.805331
2017-12-10T15:45:06.255137: step 1333, loss 1.21556, acc 0.671875, prec 0.0395138, recall 0.805488
2017-12-10T15:45:06.451752: step 1334, loss 1.397, acc 0.703125, prec 0.0395221, recall 0.805645
2017-12-10T15:45:06.651740: step 1335, loss 1.21248, acc 0.59375, prec 0.0394815, recall 0.805645
2017-12-10T15:45:06.844885: step 1336, loss 1.17155, acc 0.625, prec 0.0395199, recall 0.805958
2017-12-10T15:45:07.042335: step 1337, loss 1.30597, acc 0.625, prec 0.0395583, recall 0.80627
2017-12-10T15:45:07.232849: step 1338, loss 0.632339, acc 0.828125, prec 0.039579, recall 0.806426
2017-12-10T15:45:07.426551: step 1339, loss 0.54782, acc 0.796875, prec 0.0395587, recall 0.806426
2017-12-10T15:45:07.622397: step 1340, loss 0.469703, acc 0.8125, prec 0.03954, recall 0.806426
2017-12-10T15:45:07.819829: step 1341, loss 0.534809, acc 0.75, prec 0.0395151, recall 0.806426
2017-12-10T15:45:08.017588: step 1342, loss 0.866659, acc 0.859375, prec 0.0395389, recall 0.806581
2017-12-10T15:45:08.215271: step 1343, loss 0.783513, acc 0.875, prec 0.039602, recall 0.806891
2017-12-10T15:45:08.413131: step 1344, loss 0.409031, acc 0.828125, prec 0.0395849, recall 0.806891
2017-12-10T15:45:08.609717: step 1345, loss 0.529692, acc 0.828125, prec 0.0396055, recall 0.807046
2017-12-10T15:45:08.802693: step 1346, loss 0.873443, acc 0.671875, prec 0.0395729, recall 0.807046
2017-12-10T15:45:09.001355: step 1347, loss 0.582697, acc 0.75, prec 0.0395857, recall 0.8072
2017-12-10T15:45:09.195794: step 1348, loss 0.339074, acc 0.859375, prec 0.0395717, recall 0.8072
2017-12-10T15:45:09.396298: step 1349, loss 1.56644, acc 0.859375, prec 0.0396331, recall 0.807508
2017-12-10T15:45:09.596404: step 1350, loss 0.436904, acc 0.828125, prec 0.0396536, recall 0.807662
2017-12-10T15:45:09.791017: step 1351, loss 0.24827, acc 0.875, prec 0.0396412, recall 0.807662
2017-12-10T15:45:09.984313: step 1352, loss 0.241859, acc 0.921875, prec 0.0396334, recall 0.807662
2017-12-10T15:45:10.183170: step 1353, loss 0.150384, acc 0.921875, prec 0.0396257, recall 0.807662
2017-12-10T15:45:10.372383: step 1354, loss 1.51317, acc 0.8125, prec 0.0396446, recall 0.807815
2017-12-10T15:45:10.567226: step 1355, loss 1.40473, acc 0.859375, prec 0.0396698, recall 0.807325
2017-12-10T15:45:10.766498: step 1356, loss 0.36795, acc 0.921875, prec 0.0396996, recall 0.807478
2017-12-10T15:45:10.968643: step 1357, loss 0.437701, acc 0.875, prec 0.0396872, recall 0.807478
2017-12-10T15:45:11.168341: step 1358, loss 0.781925, acc 0.875, prec 0.0397499, recall 0.807784
2017-12-10T15:45:11.367531: step 1359, loss 0.202247, acc 0.9375, prec 0.0397812, recall 0.807936
2017-12-10T15:45:11.565042: step 1360, loss 0.290368, acc 0.890625, prec 0.0397703, recall 0.807936
2017-12-10T15:45:11.761413: step 1361, loss 0.141648, acc 0.953125, prec 0.0398406, recall 0.808241
2017-12-10T15:45:11.961075: step 1362, loss 2.92993, acc 0.890625, prec 0.0398313, recall 0.807601
2017-12-10T15:45:12.160836: step 1363, loss 0.523549, acc 0.875, prec 0.0398938, recall 0.807905
2017-12-10T15:45:12.359495: step 1364, loss 0.38973, acc 0.90625, prec 0.039922, recall 0.808057
2017-12-10T15:45:12.556784: step 1365, loss 0.305544, acc 0.875, prec 0.0399095, recall 0.808057
2017-12-10T15:45:12.757698: step 1366, loss 0.151708, acc 0.9375, prec 0.0399033, recall 0.808057
2017-12-10T15:45:12.950190: step 1367, loss 0.257087, acc 0.90625, prec 0.0399688, recall 0.80836
2017-12-10T15:45:13.146856: step 1368, loss 0.287707, acc 0.9375, prec 0.0399626, recall 0.80836
2017-12-10T15:45:13.345697: step 1369, loss 0.14251, acc 0.984375, prec 0.039961, recall 0.80836
2017-12-10T15:45:13.540646: step 1370, loss 0.558652, acc 0.78125, prec 0.0399766, recall 0.808511
2017-12-10T15:45:13.735794: step 1371, loss 0.23087, acc 0.890625, prec 0.0399657, recall 0.808511
2017-12-10T15:45:13.928991: step 1372, loss 0.281823, acc 0.875, prec 0.0399533, recall 0.808511
2017-12-10T15:45:14.127387: step 1373, loss 2.96194, acc 0.921875, prec 0.039947, recall 0.807874
2017-12-10T15:45:14.332944: step 1374, loss 0.210222, acc 0.921875, prec 0.0399766, recall 0.808025
2017-12-10T15:45:14.528033: step 1375, loss 0.312657, acc 0.90625, prec 0.0400047, recall 0.808176
2017-12-10T15:45:14.722673: step 1376, loss 1.049, acc 0.84375, prec 0.0400265, recall 0.808327
2017-12-10T15:45:14.921002: step 1377, loss 0.306037, acc 0.890625, prec 0.0400529, recall 0.808477
2017-12-10T15:45:15.112367: step 1378, loss 0.24632, acc 0.921875, prec 0.0400451, recall 0.808477
2017-12-10T15:45:15.311624: step 1379, loss 0.45165, acc 0.828125, prec 0.0400653, recall 0.808627
2017-12-10T15:45:15.505286: step 1380, loss 0.341837, acc 0.859375, prec 0.0400513, recall 0.808627
2017-12-10T15:45:15.698068: step 1381, loss 1.7798, acc 0.859375, prec 0.0400388, recall 0.807994
2017-12-10T15:45:15.888660: step 1382, loss 0.624117, acc 0.796875, prec 0.0400186, recall 0.807994
2017-12-10T15:45:16.087475: step 1383, loss 0.586069, acc 0.84375, prec 0.0400776, recall 0.808294
2017-12-10T15:45:16.286096: step 1384, loss 0.476139, acc 0.859375, prec 0.0401008, recall 0.808444
2017-12-10T15:45:16.479846: step 1385, loss 0.18969, acc 0.90625, prec 0.0400915, recall 0.808444
2017-12-10T15:45:16.675471: step 1386, loss 0.67186, acc 0.75, prec 0.0401038, recall 0.808594
2017-12-10T15:45:16.871179: step 1387, loss 0.450727, acc 0.8125, prec 0.0400852, recall 0.808594
2017-12-10T15:45:17.070668: step 1388, loss 1.26636, acc 0.796875, prec 0.0401022, recall 0.808743
2017-12-10T15:45:17.271124: step 1389, loss 0.336066, acc 0.84375, prec 0.0400867, recall 0.808743
2017-12-10T15:45:17.467473: step 1390, loss 0.429402, acc 0.828125, prec 0.0401067, recall 0.808892
2017-12-10T15:45:17.667025: step 1391, loss 0.410433, acc 0.8125, prec 0.0400881, recall 0.808892
2017-12-10T15:45:17.863442: step 1392, loss 2.43342, acc 0.875, prec 0.0402612, recall 0.809635
2017-12-10T15:45:18.066693: step 1393, loss 0.28419, acc 0.890625, prec 0.0402503, recall 0.809635
2017-12-10T15:45:18.258813: step 1394, loss 0.37054, acc 0.875, prec 0.0402379, recall 0.809635
2017-12-10T15:45:18.455716: step 1395, loss 0.389448, acc 0.828125, prec 0.0402208, recall 0.809635
2017-12-10T15:45:18.651478: step 1396, loss 0.415311, acc 0.859375, prec 0.0402809, recall 0.80993
2017-12-10T15:45:18.845150: step 1397, loss 0.482085, acc 0.828125, prec 0.0403008, recall 0.810078
2017-12-10T15:45:19.046120: step 1398, loss 0.56525, acc 0.78125, prec 0.0402791, recall 0.810078
2017-12-10T15:45:19.242193: step 1399, loss 0.364547, acc 0.859375, prec 0.0402651, recall 0.810078
2017-12-10T15:45:19.439886: step 1400, loss 0.453906, acc 0.75, prec 0.0402403, recall 0.810078
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_3/1512938428/checkpoints/model-1400

2017-12-10T15:45:20.616893: step 1401, loss 0.606433, acc 0.828125, prec 0.0402602, recall 0.810225
2017-12-10T15:45:20.811631: step 1402, loss 0.410834, acc 0.890625, prec 0.0403232, recall 0.810518
2017-12-10T15:45:21.010032: step 1403, loss 0.555174, acc 0.828125, prec 0.040343, recall 0.810665
2017-12-10T15:45:21.203384: step 1404, loss 0.315058, acc 0.875, prec 0.0403306, recall 0.810665
2017-12-10T15:45:21.399327: step 1405, loss 2.65318, acc 0.875, prec 0.0404304, recall 0.810478
2017-12-10T15:45:21.597297: step 1406, loss 0.203927, acc 0.9375, prec 0.0404611, recall 0.810624
2017-12-10T15:45:21.793936: step 1407, loss 0.429431, acc 0.84375, prec 0.0405193, recall 0.810915
2017-12-10T15:45:21.991584: step 1408, loss 0.319317, acc 0.921875, prec 0.0405852, recall 0.811205
2017-12-10T15:45:22.191679: step 1409, loss 0.344539, acc 0.859375, prec 0.0405711, recall 0.811205
2017-12-10T15:45:22.390108: step 1410, loss 1.01932, acc 0.796875, prec 0.0405877, recall 0.81135
2017-12-10T15:45:22.585638: step 1411, loss 0.985139, acc 0.859375, prec 0.0406473, recall 0.811639
2017-12-10T15:45:22.778479: step 1412, loss 0.268953, acc 0.9375, prec 0.0406411, recall 0.811639
2017-12-10T15:45:22.972526: step 1413, loss 0.359357, acc 0.875, prec 0.0407389, recall 0.81207
2017-12-10T15:45:23.166615: step 1414, loss 0.635467, acc 0.71875, prec 0.0407108, recall 0.81207
2017-12-10T15:45:23.361379: step 1415, loss 0.239841, acc 0.890625, prec 0.0407366, recall 0.812214
2017-12-10T15:45:23.557970: step 1416, loss 2.29732, acc 0.796875, prec 0.0407546, recall 0.811738
2017-12-10T15:45:23.755579: step 1417, loss 0.278411, acc 0.875, prec 0.0408155, recall 0.812024
2017-12-10T15:45:23.949905: step 1418, loss 0.539768, acc 0.859375, prec 0.0408382, recall 0.812167
2017-12-10T15:45:24.147139: step 1419, loss 0.30149, acc 0.890625, prec 0.0408272, recall 0.812167
2017-12-10T15:45:24.344008: step 1420, loss 0.795118, acc 0.75, prec 0.0408756, recall 0.812453
2017-12-10T15:45:24.537910: step 1421, loss 0.766632, acc 0.765625, prec 0.0409254, recall 0.812737
2017-12-10T15:45:24.730406: step 1422, loss 1.74554, acc 0.78125, prec 0.0409417, recall 0.812263
2017-12-10T15:45:24.930130: step 1423, loss 1.59438, acc 0.828125, prec 0.0409261, recall 0.811649
2017-12-10T15:45:25.127688: step 1424, loss 0.683564, acc 0.765625, prec 0.0409392, recall 0.811791
2017-12-10T15:45:25.320441: step 1425, loss 0.702533, acc 0.703125, prec 0.0409096, recall 0.811791
2017-12-10T15:45:25.515616: step 1426, loss 0.519151, acc 0.84375, prec 0.040894, recall 0.811791
2017-12-10T15:45:25.710803: step 1427, loss 0.512465, acc 0.875, prec 0.0409181, recall 0.811934
2017-12-10T15:45:25.904220: step 1428, loss 0.535539, acc 0.765625, prec 0.0408947, recall 0.811934
2017-12-10T15:45:26.102566: step 1429, loss 0.827866, acc 0.78125, prec 0.0409094, recall 0.812075
2017-12-10T15:45:26.301661: step 1430, loss 0.753421, acc 0.703125, prec 0.0408799, recall 0.812075
2017-12-10T15:45:26.497996: step 1431, loss 0.695893, acc 0.8125, prec 0.0409705, recall 0.8125
2017-12-10T15:45:26.690485: step 1432, loss 1.09166, acc 0.8125, prec 0.0410611, recall 0.812923
2017-12-10T15:45:26.884218: step 1433, loss 0.705273, acc 0.796875, prec 0.0410772, recall 0.813063
2017-12-10T15:45:27.075716: step 1434, loss 0.576092, acc 0.765625, prec 0.0410538, recall 0.813063
2017-12-10T15:45:27.269652: step 1435, loss 0.318861, acc 0.828125, prec 0.0410367, recall 0.813063
2017-12-10T15:45:27.464732: step 1436, loss 1.0455, acc 0.796875, prec 0.0410891, recall 0.813343
2017-12-10T15:45:27.662461: step 1437, loss 2.34844, acc 0.71875, prec 0.0410627, recall 0.812734
2017-12-10T15:45:27.862725: step 1438, loss 0.42027, acc 0.828125, prec 0.0410819, recall 0.812874
2017-12-10T15:45:28.060606: step 1439, loss 0.567328, acc 0.84375, prec 0.0411751, recall 0.813294
2017-12-10T15:45:28.258846: step 1440, loss 0.886127, acc 0.8125, prec 0.0412289, recall 0.813572
2017-12-10T15:45:28.451806: step 1441, loss 0.550959, acc 0.78125, prec 0.0413158, recall 0.813988
2017-12-10T15:45:28.645309: step 1442, loss 0.424185, acc 0.875, prec 0.0413395, recall 0.814126
2017-12-10T15:45:28.842000: step 1443, loss 0.437534, acc 0.828125, prec 0.0413223, recall 0.814126
2017-12-10T15:45:29.036619: step 1444, loss 0.868271, acc 0.796875, prec 0.0413382, recall 0.814264
2017-12-10T15:45:29.240700: step 1445, loss 1.2927, acc 0.8125, prec 0.0413557, recall 0.814402
2017-12-10T15:45:29.443402: step 1446, loss 0.349406, acc 0.828125, prec 0.0413385, recall 0.814402
2017-12-10T15:45:29.641122: step 1447, loss 0.382346, acc 0.828125, prec 0.0413214, recall 0.814402
2017-12-10T15:45:29.841088: step 1448, loss 2.83306, acc 0.84375, prec 0.0413435, recall 0.813936
2017-12-10T15:45:30.042890: step 1449, loss 0.381516, acc 0.828125, prec 0.0413264, recall 0.813936
2017-12-10T15:45:30.236209: step 1450, loss 0.897877, acc 0.84375, prec 0.0413829, recall 0.814212
2017-12-10T15:45:30.431233: step 1451, loss 0.343126, acc 0.890625, prec 0.0414442, recall 0.814486
2017-12-10T15:45:30.622329: step 1452, loss 0.917429, acc 0.828125, prec 0.041463, recall 0.814623
2017-12-10T15:45:30.819945: step 1453, loss 0.399435, acc 0.890625, prec 0.0414521, recall 0.814623
2017-12-10T15:45:31.015374: step 1454, loss 0.52687, acc 0.796875, prec 0.0415039, recall 0.814897
2017-12-10T15:45:31.217441: step 1455, loss 0.515399, acc 0.8125, prec 0.0414852, recall 0.814897
2017-12-10T15:45:31.411294: step 1456, loss 0.535886, acc 0.8125, prec 0.0414665, recall 0.814897
2017-12-10T15:45:31.614476: step 1457, loss 0.820665, acc 0.8125, prec 0.0414838, recall 0.815033
2017-12-10T15:45:31.802235: step 1458, loss 0.833018, acc 0.84375, prec 0.0415401, recall 0.815305
2017-12-10T15:45:31.994988: step 1459, loss 0.422105, acc 0.84375, prec 0.0415246, recall 0.815305
2017-12-10T15:45:32.188123: step 1460, loss 0.403104, acc 0.875, prec 0.041548, recall 0.815441
2017-12-10T15:45:32.384472: step 1461, loss 0.434348, acc 0.8125, prec 0.0415294, recall 0.815441
2017-12-10T15:45:32.576086: step 1462, loss 0.480862, acc 0.859375, prec 0.0415512, recall 0.815577
2017-12-10T15:45:32.771241: step 1463, loss 0.302659, acc 0.875, prec 0.0415747, recall 0.815712
2017-12-10T15:45:32.966851: step 1464, loss 4.0283, acc 0.859375, prec 0.0415622, recall 0.815114
2017-12-10T15:45:33.169137: step 1465, loss 0.438971, acc 0.84375, prec 0.0415825, recall 0.815249
2017-12-10T15:45:33.364680: step 1466, loss 0.581932, acc 0.78125, prec 0.0415966, recall 0.815385
2017-12-10T15:45:33.561646: step 1467, loss 2.06919, acc 0.828125, prec 0.041581, recall 0.814788
2017-12-10T15:45:33.757038: step 1468, loss 0.418456, acc 0.828125, prec 0.041564, recall 0.814788
2017-12-10T15:45:33.952542: step 1469, loss 0.322346, acc 0.84375, prec 0.0415842, recall 0.814923
2017-12-10T15:45:34.147369: step 1470, loss 0.408188, acc 0.859375, prec 0.0416775, recall 0.815328
2017-12-10T15:45:34.343285: step 1471, loss 0.475216, acc 0.828125, prec 0.0417319, recall 0.815598
2017-12-10T15:45:34.540317: step 1472, loss 0.507569, acc 0.796875, prec 0.0417117, recall 0.815598
2017-12-10T15:45:34.736791: step 1473, loss 0.287795, acc 0.890625, prec 0.0417008, recall 0.815598
2017-12-10T15:45:34.931929: step 1474, loss 0.672637, acc 0.8125, prec 0.0417536, recall 0.815866
2017-12-10T15:45:35.128577: step 1475, loss 0.352527, acc 0.890625, prec 0.0417427, recall 0.815866
2017-12-10T15:45:35.325244: step 1476, loss 0.499645, acc 0.828125, prec 0.0417256, recall 0.815866
2017-12-10T15:45:35.521034: step 1477, loss 1.36248, acc 0.703125, prec 0.0417674, recall 0.816134
2017-12-10T15:45:35.717078: step 1478, loss 0.406918, acc 0.890625, prec 0.0417922, recall 0.816267
2017-12-10T15:45:35.909527: step 1479, loss 0.393655, acc 0.890625, prec 0.0417813, recall 0.816267
2017-12-10T15:45:36.108276: step 1480, loss 0.516805, acc 0.78125, prec 0.0417595, recall 0.816267
2017-12-10T15:45:36.302299: step 1481, loss 1.82327, acc 0.90625, prec 0.0417874, recall 0.815809
2017-12-10T15:45:36.501730: step 1482, loss 0.55523, acc 0.890625, prec 0.0418477, recall 0.816075
2017-12-10T15:45:36.702438: step 1483, loss 0.37019, acc 0.890625, prec 0.0418368, recall 0.816075
2017-12-10T15:45:36.899020: step 1484, loss 0.31263, acc 0.90625, prec 0.0418986, recall 0.816341
2017-12-10T15:45:37.093689: step 1485, loss 0.411754, acc 0.90625, prec 0.0419248, recall 0.816474
2017-12-10T15:45:37.286805: step 1486, loss 0.456133, acc 0.828125, prec 0.0419077, recall 0.816474
2017-12-10T15:45:37.478297: step 1487, loss 2.04143, acc 0.828125, prec 0.0419632, recall 0.81615
2017-12-10T15:45:37.681984: step 1488, loss 1.35468, acc 0.875, prec 0.0419523, recall 0.815562
2017-12-10T15:45:37.887618: step 1489, loss 0.430019, acc 0.78125, prec 0.0419306, recall 0.815562
2017-12-10T15:45:38.080658: step 1490, loss 0.504949, acc 0.84375, prec 0.0419505, recall 0.815695
2017-12-10T15:45:38.257599: step 1491, loss 0.51601, acc 0.865385, prec 0.0419397, recall 0.815695
2017-12-10T15:45:38.462490: step 1492, loss 0.684177, acc 0.765625, prec 0.0419873, recall 0.81596
2017-12-10T15:45:38.664675: step 1493, loss 0.306713, acc 0.890625, prec 0.0420118, recall 0.816092
2017-12-10T15:45:38.860438: step 1494, loss 0.379662, acc 0.84375, prec 0.0420671, recall 0.816356
2017-12-10T15:45:39.061656: step 1495, loss 0.607796, acc 0.828125, prec 0.0420854, recall 0.816487
2017-12-10T15:45:39.257520: step 1496, loss 1.25637, acc 0.84375, prec 0.042176, recall 0.816881
2017-12-10T15:45:39.455807: step 1497, loss 0.573841, acc 0.8125, prec 0.0421927, recall 0.817012
2017-12-10T15:45:39.648115: step 1498, loss 0.350059, acc 0.859375, prec 0.042214, recall 0.817143
2017-12-10T15:45:39.846460: step 1499, loss 1.3632, acc 0.859375, prec 0.0422353, recall 0.817273
2017-12-10T15:45:40.041037: step 1500, loss 0.500665, acc 0.859375, prec 0.0422566, recall 0.817404
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_3/1512938428/checkpoints/model-1500

2017-12-10T15:45:41.234121: step 1501, loss 0.412471, acc 0.859375, prec 0.0422426, recall 0.817404
2017-12-10T15:45:41.430395: step 1502, loss 0.746178, acc 0.75, prec 0.0422177, recall 0.817404
2017-12-10T15:45:41.628915: step 1503, loss 0.499154, acc 0.828125, prec 0.0422006, recall 0.817404
2017-12-10T15:45:41.821797: step 1504, loss 0.619067, acc 0.78125, prec 0.0421789, recall 0.817404
2017-12-10T15:45:42.018731: step 1505, loss 0.163597, acc 0.921875, prec 0.0422064, recall 0.817534
2017-12-10T15:45:42.217002: step 1506, loss 0.520984, acc 0.859375, prec 0.0422276, recall 0.817664
2017-12-10T15:45:42.413423: step 1507, loss 1.04267, acc 0.8125, prec 0.0422442, recall 0.817794
2017-12-10T15:45:42.611553: step 1508, loss 0.278156, acc 0.859375, prec 0.0422654, recall 0.817923
2017-12-10T15:45:42.810608: step 1509, loss 0.550633, acc 0.875, prec 0.0423234, recall 0.818182
2017-12-10T15:45:43.005475: step 1510, loss 0.228239, acc 0.90625, prec 0.0423492, recall 0.818311
2017-12-10T15:45:43.198315: step 1511, loss 0.257237, acc 0.890625, prec 0.0423383, recall 0.818311
2017-12-10T15:45:43.398313: step 1512, loss 0.230579, acc 0.921875, prec 0.0423306, recall 0.818311
2017-12-10T15:45:43.590172: step 1513, loss 0.272152, acc 0.90625, prec 0.0423212, recall 0.818311
2017-12-10T15:45:43.784358: step 1514, loss 0.166548, acc 0.9375, prec 0.0423502, recall 0.81844
2017-12-10T15:45:43.979644: step 1515, loss 0.186626, acc 0.953125, prec 0.0424158, recall 0.818697
2017-12-10T15:45:44.175246: step 1516, loss 0.389255, acc 0.859375, prec 0.0424369, recall 0.818825
2017-12-10T15:45:44.378410: step 1517, loss 0.488415, acc 0.875, prec 0.0424947, recall 0.819081
2017-12-10T15:45:44.574835: step 1518, loss 0.136195, acc 0.9375, prec 0.0425236, recall 0.819209
2017-12-10T15:45:44.770996: step 1519, loss 0.337479, acc 0.84375, prec 0.0425431, recall 0.819337
2017-12-10T15:45:44.968559: step 1520, loss 0.262994, acc 0.90625, prec 0.0425688, recall 0.819464
2017-12-10T15:45:45.160212: step 1521, loss 0.14904, acc 0.90625, prec 0.0425594, recall 0.819464
2017-12-10T15:45:45.357274: step 1522, loss 1.20956, acc 0.953125, prec 0.0425898, recall 0.819591
2017-12-10T15:45:45.553508: step 1523, loss 0.0530552, acc 0.984375, prec 0.0425883, recall 0.819591
2017-12-10T15:45:45.755895: step 1524, loss 0.122934, acc 0.953125, prec 0.0426186, recall 0.819718
2017-12-10T15:45:45.951061: step 1525, loss 1.38879, acc 0.90625, prec 0.0426108, recall 0.819141
2017-12-10T15:45:46.154076: step 1526, loss 0.169855, acc 0.96875, prec 0.0426077, recall 0.819141
2017-12-10T15:45:46.357247: step 1527, loss 0.532915, acc 0.953125, prec 0.0426731, recall 0.819396
2017-12-10T15:45:46.551373: step 1528, loss 0.439421, acc 0.890625, prec 0.0426972, recall 0.819523
2017-12-10T15:45:46.747400: step 1529, loss 0.30819, acc 0.90625, prec 0.0427579, recall 0.819776
2017-12-10T15:45:46.943326: step 1530, loss 0.521101, acc 0.828125, prec 0.0427757, recall 0.819902
2017-12-10T15:45:47.137904: step 1531, loss 0.450954, acc 0.859375, prec 0.0427616, recall 0.819902
2017-12-10T15:45:47.330730: step 1532, loss 0.152451, acc 0.9375, prec 0.0427903, recall 0.820028
2017-12-10T15:45:47.524518: step 1533, loss 0.327537, acc 0.859375, prec 0.0427763, recall 0.820028
2017-12-10T15:45:47.714552: step 1534, loss 0.134549, acc 0.953125, prec 0.0427716, recall 0.820028
2017-12-10T15:45:47.911198: step 1535, loss 0.40184, acc 0.890625, prec 0.0428655, recall 0.820405
2017-12-10T15:45:48.104315: step 1536, loss 0.314473, acc 0.90625, prec 0.0428561, recall 0.820405
2017-12-10T15:45:48.298422: step 1537, loss 0.187142, acc 0.921875, prec 0.0428483, recall 0.820405
2017-12-10T15:45:48.495294: step 1538, loss 0.217495, acc 0.953125, prec 0.0428436, recall 0.820405
2017-12-10T15:45:48.691529: step 1539, loss 0.191475, acc 0.953125, prec 0.0428738, recall 0.820531
2017-12-10T15:45:48.887801: step 1540, loss 0.240088, acc 0.90625, prec 0.0428644, recall 0.820531
2017-12-10T15:45:49.085257: step 1541, loss 0.192456, acc 0.921875, prec 0.0428566, recall 0.820531
2017-12-10T15:45:49.282299: step 1542, loss 0.185353, acc 0.9375, prec 0.0428853, recall 0.820656
2017-12-10T15:45:49.478337: step 1543, loss 0.154918, acc 0.921875, prec 0.0428775, recall 0.820656
2017-12-10T15:45:49.669706: step 1544, loss 0.139302, acc 0.9375, prec 0.0428712, recall 0.820656
2017-12-10T15:45:49.864844: step 1545, loss 0.728106, acc 0.9375, prec 0.0429696, recall 0.821031
2017-12-10T15:45:50.066867: step 1546, loss 0.742183, acc 0.90625, prec 0.0430299, recall 0.82128
2017-12-10T15:45:50.262849: step 1547, loss 0.212935, acc 0.921875, prec 0.0430221, recall 0.82128
2017-12-10T15:45:50.457540: step 1548, loss 0.0973859, acc 0.984375, prec 0.0430205, recall 0.82128
2017-12-10T15:45:50.654463: step 1549, loss 0.178234, acc 0.953125, prec 0.0430507, recall 0.821404
2017-12-10T15:45:50.855723: step 1550, loss 0.401895, acc 0.953125, prec 0.0431157, recall 0.821652
2017-12-10T15:45:51.057317: step 1551, loss 0.493865, acc 0.9375, prec 0.0431443, recall 0.821775
2017-12-10T15:45:51.249686: step 1552, loss 0.178552, acc 0.90625, prec 0.0431348, recall 0.821775
2017-12-10T15:45:51.448980: step 1553, loss 0.197636, acc 0.890625, prec 0.0431238, recall 0.821775
2017-12-10T15:45:51.649262: step 1554, loss 2.02843, acc 0.921875, prec 0.0431872, recall 0.821453
2017-12-10T15:45:51.847646: step 1555, loss 0.168195, acc 0.9375, prec 0.0431809, recall 0.821453
2017-12-10T15:45:52.042358: step 1556, loss 0.331835, acc 0.90625, prec 0.0431715, recall 0.821453
2017-12-10T15:45:52.239313: step 1557, loss 3.15937, acc 0.953125, prec 0.0431683, recall 0.820885
2017-12-10T15:45:52.445884: step 1558, loss 0.31987, acc 0.828125, prec 0.0431859, recall 0.821009
2017-12-10T15:45:52.642494: step 1559, loss 0.622694, acc 0.875, prec 0.0432081, recall 0.821133
2017-12-10T15:45:52.839738: step 1560, loss 0.277184, acc 0.875, prec 0.0431955, recall 0.821133
2017-12-10T15:45:53.038154: step 1561, loss 0.454655, acc 0.890625, prec 0.0431845, recall 0.821133
2017-12-10T15:45:53.230089: step 1562, loss 0.940865, acc 0.828125, prec 0.043202, recall 0.821256
2017-12-10T15:45:53.426045: step 1563, loss 0.40734, acc 0.84375, prec 0.0432211, recall 0.821379
2017-12-10T15:45:53.626360: step 1564, loss 0.419815, acc 0.890625, prec 0.0432448, recall 0.821502
2017-12-10T15:45:53.820882: step 1565, loss 0.590327, acc 0.765625, prec 0.043256, recall 0.821625
2017-12-10T15:45:54.021506: step 1566, loss 0.425297, acc 0.8125, prec 0.0432718, recall 0.821748
2017-12-10T15:45:54.222097: step 1567, loss 0.383479, acc 0.921875, prec 0.0432987, recall 0.821871
2017-12-10T15:45:54.416065: step 1568, loss 0.506817, acc 0.796875, prec 0.0432783, recall 0.821871
2017-12-10T15:45:54.614636: step 1569, loss 0.830581, acc 0.703125, prec 0.0432831, recall 0.821993
2017-12-10T15:45:54.811121: step 1570, loss 0.313398, acc 0.859375, prec 0.0433037, recall 0.822115
2017-12-10T15:45:55.011612: step 1571, loss 0.374248, acc 0.828125, prec 0.0432864, recall 0.822115
2017-12-10T15:45:55.209158: step 1572, loss 0.304901, acc 0.828125, prec 0.0432692, recall 0.822115
2017-12-10T15:45:55.407438: step 1573, loss 0.434552, acc 0.84375, prec 0.0433227, recall 0.822359
2017-12-10T15:45:55.602317: step 1574, loss 0.392488, acc 0.859375, prec 0.0433086, recall 0.822359
2017-12-10T15:45:55.796830: step 1575, loss 0.47537, acc 0.859375, prec 0.0432946, recall 0.822359
2017-12-10T15:45:55.991754: step 1576, loss 0.902256, acc 0.859375, prec 0.043315, recall 0.822481
2017-12-10T15:45:56.186404: step 1577, loss 0.361317, acc 0.875, prec 0.0433371, recall 0.822603
2017-12-10T15:45:56.380840: step 1578, loss 0.21129, acc 0.859375, prec 0.043323, recall 0.822603
2017-12-10T15:45:56.579411: step 1579, loss 4.69613, acc 0.9375, prec 0.0433183, recall 0.82204
2017-12-10T15:45:56.774808: step 1580, loss 0.293867, acc 0.84375, prec 0.0433027, recall 0.82204
2017-12-10T15:45:56.969918: step 1581, loss 0.45203, acc 0.828125, prec 0.04332, recall 0.822161
2017-12-10T15:45:57.164878: step 1582, loss 0.330394, acc 0.953125, prec 0.0433498, recall 0.822283
2017-12-10T15:45:57.361830: step 1583, loss 0.381669, acc 0.90625, prec 0.0433749, recall 0.822404
2017-12-10T15:45:57.556441: step 1584, loss 0.279745, acc 0.859375, prec 0.0433608, recall 0.822404
2017-12-10T15:45:57.753834: step 1585, loss 0.293395, acc 0.921875, prec 0.043353, recall 0.822404
2017-12-10T15:45:57.948201: step 1586, loss 0.300935, acc 0.859375, prec 0.043339, recall 0.822404
2017-12-10T15:45:58.142559: step 1587, loss 0.200283, acc 0.890625, prec 0.0433281, recall 0.822404
2017-12-10T15:45:58.340782: step 1588, loss 0.474299, acc 0.875, prec 0.0433156, recall 0.822404
2017-12-10T15:45:58.539595: step 1589, loss 0.413256, acc 0.890625, prec 0.0433047, recall 0.822404
2017-12-10T15:45:58.733984: step 1590, loss 0.294376, acc 0.90625, prec 0.0433641, recall 0.822647
2017-12-10T15:45:58.930705: step 1591, loss 0.285709, acc 0.859375, prec 0.0433501, recall 0.822647
2017-12-10T15:45:59.128916: step 1592, loss 0.691348, acc 0.953125, prec 0.0434142, recall 0.822888
2017-12-10T15:45:59.337528: step 1593, loss 0.155608, acc 0.9375, prec 0.043408, recall 0.822888
2017-12-10T15:45:59.535889: step 1594, loss 0.388229, acc 0.921875, prec 0.0434345, recall 0.823009
2017-12-10T15:45:59.730972: step 1595, loss 0.195742, acc 0.9375, prec 0.0434283, recall 0.823009
2017-12-10T15:45:59.928574: step 1596, loss 2.43021, acc 0.953125, prec 0.0434595, recall 0.82257
2017-12-10T15:46:00.124397: step 1597, loss 0.238425, acc 0.890625, prec 0.0434486, recall 0.82257
2017-12-10T15:46:00.320020: step 1598, loss 1.60095, acc 0.875, prec 0.0435735, recall 0.823051
2017-12-10T15:46:00.517475: step 1599, loss 0.420673, acc 0.84375, prec 0.0435578, recall 0.823051
2017-12-10T15:46:00.710146: step 1600, loss 0.428788, acc 0.84375, prec 0.0436108, recall 0.82329
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_3/1512938428/checkpoints/model-1600

2017-12-10T15:46:02.113018: step 1601, loss 0.203667, acc 0.9375, prec 0.0436045, recall 0.82329
2017-12-10T15:46:02.308448: step 1602, loss 0.499889, acc 0.796875, prec 0.0435842, recall 0.82329
2017-12-10T15:46:02.501251: step 1603, loss 0.73194, acc 0.828125, prec 0.0436013, recall 0.82341
2017-12-10T15:46:02.697057: step 1604, loss 0.400555, acc 0.890625, prec 0.0436246, recall 0.823529
2017-12-10T15:46:02.897167: step 1605, loss 0.416696, acc 0.78125, prec 0.0436028, recall 0.823529
2017-12-10T15:46:03.091097: step 1606, loss 3.18937, acc 0.71875, prec 0.0436105, recall 0.823093
2017-12-10T15:46:03.288057: step 1607, loss 0.58508, acc 0.765625, prec 0.0436213, recall 0.823212
2017-12-10T15:46:03.484205: step 1608, loss 0.465346, acc 0.84375, prec 0.0436057, recall 0.823212
2017-12-10T15:46:03.677667: step 1609, loss 0.671714, acc 0.765625, prec 0.0436165, recall 0.823331
2017-12-10T15:46:03.876142: step 1610, loss 0.728566, acc 0.765625, prec 0.0436273, recall 0.82345
2017-12-10T15:46:04.075113: step 1611, loss 0.709184, acc 0.765625, prec 0.0436381, recall 0.823569
2017-12-10T15:46:04.266861: step 1612, loss 0.618661, acc 0.75, prec 0.0436132, recall 0.823569
2017-12-10T15:46:04.464230: step 1613, loss 1.03886, acc 0.765625, prec 0.0436239, recall 0.823688
2017-12-10T15:46:04.663707: step 1614, loss 0.825922, acc 0.734375, prec 0.0436316, recall 0.823806
2017-12-10T15:46:04.856645: step 1615, loss 0.539905, acc 0.828125, prec 0.0437166, recall 0.824161
2017-12-10T15:46:05.054529: step 1616, loss 0.258531, acc 0.84375, prec 0.0437351, recall 0.824279
2017-12-10T15:46:05.251317: step 1617, loss 0.768214, acc 0.734375, prec 0.0437427, recall 0.824397
2017-12-10T15:46:05.442925: step 1618, loss 0.41118, acc 0.84375, prec 0.0437611, recall 0.824514
2017-12-10T15:46:05.634253: step 1619, loss 0.237659, acc 0.90625, prec 0.0438197, recall 0.824749
2017-12-10T15:46:05.833074: step 1620, loss 0.456802, acc 0.796875, prec 0.0437995, recall 0.824749
2017-12-10T15:46:06.024735: step 1621, loss 0.200879, acc 0.9375, prec 0.0437933, recall 0.824749
2017-12-10T15:46:06.225219: step 1622, loss 0.399835, acc 0.890625, prec 0.0437824, recall 0.824749
2017-12-10T15:46:06.423598: step 1623, loss 0.495254, acc 0.875, prec 0.0438039, recall 0.824866
2017-12-10T15:46:06.620487: step 1624, loss 0.132912, acc 0.9375, prec 0.0437977, recall 0.824866
2017-12-10T15:46:06.817547: step 1625, loss 2.50139, acc 0.9375, prec 0.043793, recall 0.824315
2017-12-10T15:46:07.015640: step 1626, loss 1.03131, acc 0.96875, prec 0.0438578, recall 0.82455
2017-12-10T15:46:07.218001: step 1627, loss 1.58116, acc 0.921875, prec 0.0438855, recall 0.824117
2017-12-10T15:46:07.414064: step 1628, loss 0.218325, acc 0.921875, prec 0.0439116, recall 0.824234
2017-12-10T15:46:07.607359: step 1629, loss 0.533072, acc 0.84375, prec 0.0439299, recall 0.824351
2017-12-10T15:46:07.805321: step 1630, loss 0.283424, acc 0.90625, prec 0.0439884, recall 0.824585
2017-12-10T15:46:08.000206: step 1631, loss 0.418641, acc 0.90625, prec 0.043979, recall 0.824585
2017-12-10T15:46:08.195945: step 1632, loss 0.50667, acc 0.84375, prec 0.0439634, recall 0.824585
2017-12-10T15:46:08.398769: step 1633, loss 0.563068, acc 0.8125, prec 0.0439448, recall 0.824585
2017-12-10T15:46:08.596730: step 1634, loss 0.410815, acc 0.859375, prec 0.0439984, recall 0.824818
2017-12-10T15:46:08.793590: step 1635, loss 0.170749, acc 0.921875, prec 0.0439907, recall 0.824818
2017-12-10T15:46:08.992023: step 1636, loss 0.168327, acc 0.921875, prec 0.0439829, recall 0.824818
2017-12-10T15:46:09.186370: step 1637, loss 0.486048, acc 0.9375, prec 0.0440781, recall 0.825166
2017-12-10T15:46:09.381715: step 1638, loss 0.428484, acc 0.828125, prec 0.044061, recall 0.825166
2017-12-10T15:46:09.579570: step 1639, loss 0.363694, acc 0.875, prec 0.0440823, recall 0.825281
2017-12-10T15:46:09.778898: step 1640, loss 0.458503, acc 0.84375, prec 0.0440667, recall 0.825281
2017-12-10T15:46:09.978313: step 1641, loss 0.279954, acc 0.953125, prec 0.0441296, recall 0.825512
2017-12-10T15:46:10.175284: step 1642, loss 0.229701, acc 0.921875, prec 0.0441893, recall 0.825743
2017-12-10T15:46:10.369690: step 1643, loss 0.938886, acc 0.875, prec 0.0442106, recall 0.825858
2017-12-10T15:46:10.569170: step 1644, loss 0.287396, acc 0.9375, prec 0.0443056, recall 0.826201
2017-12-10T15:46:10.762931: step 1645, loss 0.265082, acc 0.859375, prec 0.0442915, recall 0.826201
2017-12-10T15:46:10.956983: step 1646, loss 0.628854, acc 0.8125, prec 0.0443065, recall 0.826316
2017-12-10T15:46:11.150749: step 1647, loss 0.364761, acc 0.90625, prec 0.0443645, recall 0.826544
2017-12-10T15:46:11.349093: step 1648, loss 1.16364, acc 0.921875, prec 0.0444241, recall 0.826772
2017-12-10T15:46:11.544565: step 1649, loss 0.468928, acc 0.828125, prec 0.0444405, recall 0.826885
2017-12-10T15:46:11.741036: step 1650, loss 0.178722, acc 0.953125, prec 0.0444358, recall 0.826885
2017-12-10T15:46:11.934461: step 1651, loss 1.24499, acc 0.90625, prec 0.0444601, recall 0.826999
2017-12-10T15:46:12.134448: step 1652, loss 0.318109, acc 0.875, prec 0.0444476, recall 0.826999
2017-12-10T15:46:12.332741: step 1653, loss 0.358977, acc 0.890625, prec 0.0445375, recall 0.827338
2017-12-10T15:46:12.531390: step 1654, loss 0.809488, acc 0.8125, prec 0.044586, recall 0.827564
2017-12-10T15:46:12.726628: step 1655, loss 0.283874, acc 0.90625, prec 0.0445766, recall 0.827564
2017-12-10T15:46:12.924581: step 1656, loss 1.57573, acc 0.828125, prec 0.0446601, recall 0.827901
2017-12-10T15:46:13.121826: step 1657, loss 0.520252, acc 0.859375, prec 0.044646, recall 0.827901
2017-12-10T15:46:13.315639: step 1658, loss 0.344129, acc 0.859375, prec 0.0446319, recall 0.827901
2017-12-10T15:46:13.515116: step 1659, loss 0.427813, acc 0.859375, prec 0.0446849, recall 0.828125
2017-12-10T15:46:13.707225: step 1660, loss 0.441719, acc 0.890625, prec 0.0447745, recall 0.82846
2017-12-10T15:46:13.905348: step 1661, loss 0.471741, acc 0.84375, prec 0.0447588, recall 0.82846
2017-12-10T15:46:14.102149: step 1662, loss 0.177089, acc 0.921875, prec 0.044751, recall 0.82846
2017-12-10T15:46:14.301535: step 1663, loss 3.04083, acc 0.890625, prec 0.0447416, recall 0.827922
2017-12-10T15:46:14.504729: step 1664, loss 0.337589, acc 0.859375, prec 0.0447944, recall 0.828145
2017-12-10T15:46:14.701748: step 1665, loss 0.279577, acc 0.890625, prec 0.0447834, recall 0.828145
2017-12-10T15:46:14.892971: step 1666, loss 0.910712, acc 0.828125, prec 0.0448331, recall 0.828368
2017-12-10T15:46:15.090415: step 1667, loss 0.292567, acc 0.890625, prec 0.0448221, recall 0.828368
2017-12-10T15:46:15.291585: step 1668, loss 2.56266, acc 0.84375, prec 0.044808, recall 0.827832
2017-12-10T15:46:15.487858: step 1669, loss 0.58275, acc 0.84375, prec 0.0448927, recall 0.828165
2017-12-10T15:46:15.680091: step 1670, loss 0.658609, acc 0.796875, prec 0.0449057, recall 0.828276
2017-12-10T15:46:15.878344: step 1671, loss 0.457661, acc 0.875, prec 0.0448931, recall 0.828276
2017-12-10T15:46:16.073401: step 1672, loss 0.847624, acc 0.671875, prec 0.0448935, recall 0.828387
2017-12-10T15:46:16.262974: step 1673, loss 0.715486, acc 0.78125, prec 0.0449383, recall 0.828608
2017-12-10T15:46:16.455060: step 1674, loss 0.926995, acc 0.6875, prec 0.0449069, recall 0.828608
2017-12-10T15:46:16.651198: step 1675, loss 0.546018, acc 0.796875, prec 0.0449199, recall 0.828719
2017-12-10T15:46:16.841414: step 1676, loss 0.512269, acc 0.75, prec 0.0449615, recall 0.828939
2017-12-10T15:46:17.048238: step 1677, loss 0.435454, acc 0.890625, prec 0.0449838, recall 0.829049
2017-12-10T15:46:17.241308: step 1678, loss 0.846825, acc 0.734375, prec 0.0449904, recall 0.829159
2017-12-10T15:46:17.440593: step 1679, loss 0.403623, acc 0.859375, prec 0.0450096, recall 0.829268
2017-12-10T15:46:17.636785: step 1680, loss 0.226005, acc 0.9375, prec 0.0450366, recall 0.829378
2017-12-10T15:46:17.833743: step 1681, loss 0.518912, acc 0.84375, prec 0.0450874, recall 0.829596
2017-12-10T15:46:18.030245: step 1682, loss 0.296883, acc 0.921875, prec 0.0451128, recall 0.829705
2017-12-10T15:46:18.231297: step 1683, loss 0.24073, acc 0.90625, prec 0.0451698, recall 0.829923
2017-12-10T15:46:18.427467: step 1684, loss 1.20979, acc 0.859375, prec 0.0452553, recall 0.830249
2017-12-10T15:46:18.619006: step 1685, loss 0.490783, acc 0.921875, prec 0.0453139, recall 0.830465
2017-12-10T15:46:18.816853: step 1686, loss 2.49499, acc 0.875, prec 0.045336, recall 0.830045
2017-12-10T15:46:19.023574: step 1687, loss 0.426311, acc 0.859375, prec 0.045355, recall 0.830153
2017-12-10T15:46:19.216909: step 1688, loss 0.202764, acc 0.90625, prec 0.0453456, recall 0.830153
2017-12-10T15:46:19.411347: step 1689, loss 0.404236, acc 0.90625, prec 0.0453693, recall 0.830261
2017-12-10T15:46:19.611055: step 1690, loss 2.51877, acc 0.890625, prec 0.0453598, recall 0.829733
2017-12-10T15:46:19.807532: step 1691, loss 0.280605, acc 0.875, prec 0.0453472, recall 0.829733
2017-12-10T15:46:20.002253: step 1692, loss 0.624503, acc 0.84375, prec 0.0453646, recall 0.829841
2017-12-10T15:46:20.204807: step 1693, loss 0.455874, acc 0.875, prec 0.0454514, recall 0.830165
2017-12-10T15:46:20.400095: step 1694, loss 0.524468, acc 0.875, prec 0.045505, recall 0.83038
2017-12-10T15:46:20.601125: step 1695, loss 0.525259, acc 0.84375, prec 0.0455223, recall 0.830487
2017-12-10T15:46:20.802042: step 1696, loss 0.657808, acc 0.828125, prec 0.0456373, recall 0.830915
2017-12-10T15:46:20.994631: step 1697, loss 0.459944, acc 0.859375, prec 0.045623, recall 0.830915
2017-12-10T15:46:21.193126: step 1698, loss 0.615384, acc 0.8125, prec 0.0456371, recall 0.831021
2017-12-10T15:46:21.391669: step 1699, loss 0.530686, acc 0.78125, prec 0.045648, recall 0.831128
2017-12-10T15:46:21.584441: step 1700, loss 0.520198, acc 0.734375, prec 0.0456872, recall 0.83134
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_3/1512938428/checkpoints/model-1700

2017-12-10T15:46:22.771683: step 1701, loss 0.628708, acc 0.78125, prec 0.0456981, recall 0.831447
2017-12-10T15:46:22.964655: step 1702, loss 0.356387, acc 0.921875, prec 0.0457562, recall 0.831658
2017-12-10T15:46:23.159923: step 1703, loss 1.64245, acc 0.796875, prec 0.0457686, recall 0.831764
2017-12-10T15:46:23.358518: step 1704, loss 0.386593, acc 0.8125, prec 0.0457496, recall 0.831764
2017-12-10T15:46:23.552727: step 1705, loss 0.265609, acc 0.90625, prec 0.0457731, recall 0.831869
2017-12-10T15:46:23.746960: step 1706, loss 1.87219, acc 0.8125, prec 0.0458215, recall 0.831559
2017-12-10T15:46:23.941316: step 1707, loss 0.8678, acc 0.734375, prec 0.0458276, recall 0.831665
2017-12-10T15:46:24.132541: step 1708, loss 0.403957, acc 0.859375, prec 0.0458134, recall 0.831665
2017-12-10T15:46:24.326755: step 1709, loss 0.601904, acc 0.875, prec 0.0458665, recall 0.831875
2017-12-10T15:46:24.523889: step 1710, loss 0.357346, acc 0.890625, prec 0.0458554, recall 0.831875
2017-12-10T15:46:24.720206: step 1711, loss 0.681599, acc 0.859375, prec 0.0459069, recall 0.832085
2017-12-10T15:46:24.918929: step 1712, loss 0.288425, acc 0.90625, prec 0.0459632, recall 0.832294
2017-12-10T15:46:25.112997: step 1713, loss 0.476178, acc 0.8125, prec 0.045977, recall 0.832399
2017-12-10T15:46:25.309298: step 1714, loss 0.487053, acc 0.8125, prec 0.045958, recall 0.832399
2017-12-10T15:46:25.508761: step 1715, loss 0.442275, acc 0.859375, prec 0.0459438, recall 0.832399
2017-12-10T15:46:25.698817: step 1716, loss 0.411533, acc 0.875, prec 0.0460296, recall 0.832711
2017-12-10T15:46:25.893011: step 1717, loss 0.426101, acc 0.84375, prec 0.0460137, recall 0.832711
2017-12-10T15:46:26.094514: step 1718, loss 0.898013, acc 0.890625, prec 0.0460355, recall 0.832815
2017-12-10T15:46:26.292667: step 1719, loss 1.9361, acc 0.90625, prec 0.0460603, recall 0.832402
2017-12-10T15:46:26.489399: step 1720, loss 0.630588, acc 0.921875, prec 0.0460852, recall 0.832506
2017-12-10T15:46:26.684355: step 1721, loss 0.397368, acc 0.90625, prec 0.0461084, recall 0.83261
2017-12-10T15:46:26.882023: step 1722, loss 0.699507, acc 0.875, prec 0.0461612, recall 0.832817
2017-12-10T15:46:27.078630: step 1723, loss 0.207644, acc 0.90625, prec 0.0461517, recall 0.832817
2017-12-10T15:46:27.271581: step 1724, loss 0.618362, acc 0.78125, prec 0.0461623, recall 0.832921
2017-12-10T15:46:27.467141: step 1725, loss 0.39125, acc 0.90625, prec 0.0461855, recall 0.833024
2017-12-10T15:46:27.667896: step 1726, loss 0.853462, acc 0.6875, prec 0.0461865, recall 0.833127
2017-12-10T15:46:27.865547: step 1727, loss 0.414084, acc 0.828125, prec 0.0462018, recall 0.83323
2017-12-10T15:46:28.062772: step 1728, loss 0.437507, acc 0.8125, prec 0.0462155, recall 0.833333
2017-12-10T15:46:28.262000: step 1729, loss 1.21964, acc 0.828125, prec 0.0462307, recall 0.833436
2017-12-10T15:46:28.461042: step 1730, loss 1.00185, acc 0.875, prec 0.0462833, recall 0.833641
2017-12-10T15:46:28.658072: step 1731, loss 0.331505, acc 0.859375, prec 0.0463017, recall 0.833744
2017-12-10T15:46:28.853496: step 1732, loss 0.376473, acc 0.859375, prec 0.0462874, recall 0.833744
2017-12-10T15:46:29.052302: step 1733, loss 0.370249, acc 0.828125, prec 0.04627, recall 0.833744
2017-12-10T15:46:29.253775: step 1734, loss 0.372863, acc 0.84375, prec 0.0463194, recall 0.833948
2017-12-10T15:46:29.457686: step 1735, loss 0.572272, acc 0.8125, prec 0.0463655, recall 0.834152
2017-12-10T15:46:29.652887: step 1736, loss 0.474084, acc 0.78125, prec 0.0463434, recall 0.834152
2017-12-10T15:46:29.849104: step 1737, loss 0.366141, acc 0.890625, prec 0.0463323, recall 0.834152
2017-12-10T15:46:30.048111: step 1738, loss 0.170831, acc 0.90625, prec 0.0463228, recall 0.834152
2017-12-10T15:46:30.242609: step 1739, loss 0.334167, acc 0.890625, prec 0.0463443, recall 0.834254
2017-12-10T15:46:30.437826: step 1740, loss 0.390262, acc 0.859375, prec 0.0463626, recall 0.834356
2017-12-10T15:46:30.633055: step 1741, loss 0.592445, acc 0.859375, prec 0.0463809, recall 0.834457
2017-12-10T15:46:30.829634: step 1742, loss 0.22136, acc 0.890625, prec 0.0463698, recall 0.834457
2017-12-10T15:46:31.023920: step 1743, loss 1.46621, acc 0.90625, prec 0.0464578, recall 0.834761
2017-12-10T15:46:31.226564: step 1744, loss 0.147352, acc 0.921875, prec 0.0464499, recall 0.834761
2017-12-10T15:46:31.423069: step 1745, loss 0.919165, acc 0.890625, prec 0.0464712, recall 0.834862
2017-12-10T15:46:31.625426: step 1746, loss 0.17685, acc 0.953125, prec 0.046499, recall 0.834963
2017-12-10T15:46:31.820328: step 1747, loss 0.439043, acc 0.859375, prec 0.0465172, recall 0.835064
2017-12-10T15:46:32.019538: step 1748, loss 0.265951, acc 0.90625, prec 0.0465401, recall 0.835165
2017-12-10T15:46:32.212692: step 1749, loss 0.65343, acc 0.921875, prec 0.0466295, recall 0.835466
2017-12-10T15:46:32.413548: step 1750, loss 0.519832, acc 0.890625, prec 0.0466832, recall 0.835666
2017-12-10T15:46:32.613632: step 1751, loss 0.242041, acc 0.953125, prec 0.0466785, recall 0.835666
2017-12-10T15:46:32.811689: step 1752, loss 0.786958, acc 0.9375, prec 0.0467369, recall 0.835866
2017-12-10T15:46:33.008355: step 1753, loss 0.19571, acc 0.9375, prec 0.0468277, recall 0.836165
2017-12-10T15:46:33.205713: step 1754, loss 2.6867, acc 0.90625, prec 0.0468522, recall 0.835758
2017-12-10T15:46:33.402031: step 1755, loss 0.315546, acc 0.90625, prec 0.046875, recall 0.835857
2017-12-10T15:46:33.600922: step 1756, loss 0.450476, acc 0.8125, prec 0.0468883, recall 0.835956
2017-12-10T15:46:33.796790: step 1757, loss 0.397756, acc 0.8125, prec 0.0468692, recall 0.835956
2017-12-10T15:46:33.995722: step 1758, loss 0.431105, acc 0.890625, prec 0.0469227, recall 0.836155
2017-12-10T15:46:34.189393: step 1759, loss 0.32785, acc 0.90625, prec 0.0470101, recall 0.836451
2017-12-10T15:46:34.382339: step 1760, loss 0.490947, acc 0.828125, prec 0.0470249, recall 0.83655
2017-12-10T15:46:34.578405: step 1761, loss 0.266701, acc 0.875, prec 0.0470122, recall 0.83655
2017-12-10T15:46:34.771390: step 1762, loss 0.27629, acc 0.890625, prec 0.0470333, recall 0.836649
2017-12-10T15:46:34.961765: step 1763, loss 0.623405, acc 0.796875, prec 0.0470126, recall 0.836649
2017-12-10T15:46:35.155536: step 1764, loss 0.817735, acc 0.796875, prec 0.0470242, recall 0.836747
2017-12-10T15:46:35.348181: step 1765, loss 0.418421, acc 0.90625, prec 0.0470146, recall 0.836747
2017-12-10T15:46:35.544520: step 1766, loss 0.204695, acc 0.9375, prec 0.0470083, recall 0.836747
2017-12-10T15:46:35.747924: step 1767, loss 0.217637, acc 0.90625, prec 0.047031, recall 0.836845
2017-12-10T15:46:35.944604: step 1768, loss 0.411364, acc 0.84375, prec 0.0470473, recall 0.836943
2017-12-10T15:46:36.140626: step 1769, loss 0.275851, acc 0.875, prec 0.0470346, recall 0.836943
2017-12-10T15:46:36.339703: step 1770, loss 0.332976, acc 0.859375, prec 0.0470525, recall 0.837041
2017-12-10T15:46:36.532076: step 1771, loss 2.25469, acc 0.859375, prec 0.0470719, recall 0.836637
2017-12-10T15:46:36.734855: step 1772, loss 0.262185, acc 0.890625, prec 0.047093, recall 0.836735
2017-12-10T15:46:36.926190: step 1773, loss 0.366812, acc 0.84375, prec 0.0471093, recall 0.836833
2017-12-10T15:46:37.124087: step 1774, loss 0.218984, acc 0.9375, prec 0.0471029, recall 0.836833
2017-12-10T15:46:37.315929: step 1775, loss 0.327439, acc 0.890625, prec 0.0470918, recall 0.836833
2017-12-10T15:46:37.508468: step 1776, loss 0.198851, acc 0.90625, prec 0.0471144, recall 0.83693
2017-12-10T15:46:37.700478: step 1777, loss 0.266065, acc 0.875, prec 0.047166, recall 0.837126
2017-12-10T15:46:37.894668: step 1778, loss 0.23826, acc 0.921875, prec 0.047158, recall 0.837126
2017-12-10T15:46:38.094879: step 1779, loss 0.415523, acc 0.90625, prec 0.0472128, recall 0.837321
2017-12-10T15:46:38.292686: step 1780, loss 0.258457, acc 0.875, prec 0.0472, recall 0.837321
2017-12-10T15:46:38.487515: step 1781, loss 0.321476, acc 0.921875, prec 0.0471921, recall 0.837321
2017-12-10T15:46:38.684666: step 1782, loss 0.101382, acc 0.96875, prec 0.0471889, recall 0.837321
2017-12-10T15:46:38.880775: step 1783, loss 0.17639, acc 0.921875, prec 0.0471809, recall 0.837321
2017-12-10T15:46:39.077616: step 1784, loss 0.101615, acc 0.953125, prec 0.0471762, recall 0.837321
2017-12-10T15:46:39.276632: step 1785, loss 0.139922, acc 0.9375, prec 0.0471698, recall 0.837321
2017-12-10T15:46:39.471234: step 1786, loss 0.12495, acc 0.9375, prec 0.0471635, recall 0.837321
2017-12-10T15:46:39.668412: step 1787, loss 0.242835, acc 0.890625, prec 0.0471523, recall 0.837321
2017-12-10T15:46:39.867687: step 1788, loss 0.091803, acc 0.96875, prec 0.0471492, recall 0.837321
2017-12-10T15:46:40.062791: step 1789, loss 0.143466, acc 0.921875, prec 0.0471412, recall 0.837321
2017-12-10T15:46:40.260658: step 1790, loss 0.196737, acc 0.921875, prec 0.0471333, recall 0.837321
2017-12-10T15:46:40.457444: step 1791, loss 0.123645, acc 0.96875, prec 0.0471622, recall 0.837418
2017-12-10T15:46:40.653408: step 1792, loss 0.0756807, acc 0.96875, prec 0.047159, recall 0.837418
2017-12-10T15:46:40.855620: step 1793, loss 0.080244, acc 0.96875, prec 0.0471558, recall 0.837418
2017-12-10T15:46:41.055653: step 1794, loss 0.573753, acc 0.96875, prec 0.0472168, recall 0.837612
2017-12-10T15:46:41.256086: step 1795, loss 0.0936406, acc 0.96875, prec 0.0472136, recall 0.837612
2017-12-10T15:46:41.451389: step 1796, loss 0.130129, acc 0.953125, prec 0.0472089, recall 0.837612
2017-12-10T15:46:41.644620: step 1797, loss 0.477213, acc 1, prec 0.0472409, recall 0.837709
2017-12-10T15:46:41.842018: step 1798, loss 0.0456537, acc 0.984375, prec 0.0472393, recall 0.837709
2017-12-10T15:46:42.040725: step 1799, loss 0.339485, acc 0.921875, prec 0.0472634, recall 0.837806
2017-12-10T15:46:42.239548: step 1800, loss 0.0746416, acc 0.96875, prec 0.0472603, recall 0.837806
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_3/1512938428/checkpoints/model-1800

2017-12-10T15:46:43.445383: step 1801, loss 1.17361, acc 0.9375, prec 0.0472859, recall 0.837902
2017-12-10T15:46:43.644132: step 1802, loss 0.711256, acc 0.9375, prec 0.0473436, recall 0.838095
2017-12-10T15:46:43.842871: step 1803, loss 1.95636, acc 0.984375, prec 0.0473757, recall 0.837693
2017-12-10T15:46:44.043311: step 1804, loss 0.227041, acc 0.953125, prec 0.0474349, recall 0.837886
2017-12-10T15:46:44.246806: step 1805, loss 0.133965, acc 0.96875, prec 0.0474318, recall 0.837886
2017-12-10T15:46:44.448894: step 1806, loss 0.512431, acc 0.890625, prec 0.0474526, recall 0.837982
2017-12-10T15:46:44.647561: step 1807, loss 0.235223, acc 0.90625, prec 0.047443, recall 0.837982
2017-12-10T15:46:44.840479: step 1808, loss 0.248861, acc 0.921875, prec 0.0474991, recall 0.838174
2017-12-10T15:46:45.037043: step 1809, loss 0.264935, acc 0.90625, prec 0.0475215, recall 0.83827
2017-12-10T15:46:45.232409: step 1810, loss 0.59023, acc 0.8125, prec 0.0475343, recall 0.838366
2017-12-10T15:46:45.433754: step 1811, loss 0.270588, acc 0.890625, prec 0.0475232, recall 0.838366
2017-12-10T15:46:45.632867: step 1812, loss 0.496927, acc 0.875, prec 0.0475424, recall 0.838462
2017-12-10T15:46:45.828523: step 1813, loss 0.554028, acc 0.859375, prec 0.047528, recall 0.838462
2017-12-10T15:46:46.028712: step 1814, loss 0.405206, acc 0.859375, prec 0.0475456, recall 0.838557
2017-12-10T15:46:46.221331: step 1815, loss 0.249618, acc 0.890625, prec 0.0475344, recall 0.838557
2017-12-10T15:46:46.413517: step 1816, loss 0.699013, acc 0.859375, prec 0.047552, recall 0.838652
2017-12-10T15:46:46.608109: step 1817, loss 0.265058, acc 0.859375, prec 0.0475377, recall 0.838652
2017-12-10T15:46:46.801246: step 1818, loss 0.327199, acc 0.84375, prec 0.0475218, recall 0.838652
2017-12-10T15:46:46.999962: step 1819, loss 0.380159, acc 0.84375, prec 0.0475377, recall 0.838748
2017-12-10T15:46:47.198287: step 1820, loss 0.405297, acc 0.90625, prec 0.0475601, recall 0.838843
2017-12-10T15:46:47.393577: step 1821, loss 0.324878, acc 0.875, prec 0.0475473, recall 0.838843
2017-12-10T15:46:47.596269: step 1822, loss 1.36905, acc 0.875, prec 0.0475983, recall 0.839033
2017-12-10T15:46:47.790420: step 1823, loss 1.06551, acc 0.84375, prec 0.0476143, recall 0.839128
2017-12-10T15:46:47.991766: step 1824, loss 0.335348, acc 0.875, prec 0.0476334, recall 0.839223
2017-12-10T15:46:48.187740: step 1825, loss 0.337018, acc 0.828125, prec 0.0476477, recall 0.839317
2017-12-10T15:46:48.383356: step 1826, loss 0.460383, acc 0.890625, prec 0.0477638, recall 0.839695
2017-12-10T15:46:48.577713: step 1827, loss 0.233156, acc 0.890625, prec 0.0477844, recall 0.839789
2017-12-10T15:46:48.772485: step 1828, loss 0.162249, acc 0.9375, prec 0.047778, recall 0.839789
2017-12-10T15:46:48.969880: step 1829, loss 0.259656, acc 0.921875, prec 0.0478654, recall 0.84007
2017-12-10T15:46:49.170719: step 1830, loss 1.52404, acc 0.890625, prec 0.0479495, recall 0.840351
2017-12-10T15:46:49.374169: step 1831, loss 0.345603, acc 0.875, prec 0.0479368, recall 0.840351
2017-12-10T15:46:49.574531: step 1832, loss 0.303537, acc 0.875, prec 0.047924, recall 0.840351
2017-12-10T15:46:49.771243: step 1833, loss 0.497217, acc 0.84375, prec 0.047908, recall 0.840351
2017-12-10T15:46:49.972306: step 1834, loss 0.408918, acc 0.90625, prec 0.0479301, recall 0.840444
2017-12-10T15:46:50.172634: step 1835, loss 0.286854, acc 0.84375, prec 0.0479459, recall 0.840537
2017-12-10T15:46:50.367470: step 1836, loss 0.307953, acc 0.890625, prec 0.0479664, recall 0.84063
2017-12-10T15:46:50.564124: step 1837, loss 0.158464, acc 0.953125, prec 0.0479933, recall 0.840723
2017-12-10T15:46:50.756004: step 1838, loss 0.332251, acc 0.859375, prec 0.0480107, recall 0.840816
2017-12-10T15:46:50.948916: step 1839, loss 0.346937, acc 0.875, prec 0.0479979, recall 0.840816
2017-12-10T15:46:51.148795: step 1840, loss 2.76121, acc 0.859375, prec 0.0480484, recall 0.840512
2017-12-10T15:46:51.349051: step 1841, loss 0.35973, acc 0.890625, prec 0.0480373, recall 0.840512
2017-12-10T15:46:51.546618: step 1842, loss 0.164102, acc 0.90625, prec 0.0480277, recall 0.840512
2017-12-10T15:46:51.740693: step 1843, loss 0.650916, acc 0.8125, prec 0.0480402, recall 0.840605
2017-12-10T15:46:51.935095: step 1844, loss 3.073, acc 0.828125, prec 0.0480875, recall 0.840302
2017-12-10T15:46:52.137621: step 1845, loss 0.511517, acc 0.796875, prec 0.0480667, recall 0.840302
2017-12-10T15:46:52.332035: step 1846, loss 0.394793, acc 0.828125, prec 0.0480491, recall 0.840302
2017-12-10T15:46:52.526373: step 1847, loss 1.06109, acc 0.703125, prec 0.048082, recall 0.840487
2017-12-10T15:46:52.720560: step 1848, loss 0.474935, acc 0.84375, prec 0.0480661, recall 0.840487
2017-12-10T15:46:52.918135: step 1849, loss 0.613085, acc 0.78125, prec 0.0480438, recall 0.840487
2017-12-10T15:46:53.113293: step 1850, loss 0.528117, acc 0.828125, prec 0.0480262, recall 0.840487
2017-12-10T15:46:53.310666: step 1851, loss 0.674625, acc 0.75, prec 0.0480323, recall 0.84058
2017-12-10T15:46:53.507473: step 1852, loss 0.872408, acc 0.703125, prec 0.0480336, recall 0.840672
2017-12-10T15:46:53.705233: step 1853, loss 0.764929, acc 0.78125, prec 0.0480429, recall 0.840764
2017-12-10T15:46:53.901360: step 1854, loss 1.00721, acc 0.765625, prec 0.048019, recall 0.840764
2017-12-10T15:46:54.096111: step 1855, loss 0.556816, acc 0.828125, prec 0.0480016, recall 0.840764
2017-12-10T15:46:54.292884: step 1856, loss 0.354925, acc 0.875, prec 0.0480518, recall 0.840949
2017-12-10T15:46:54.490291: step 1857, loss 0.522815, acc 0.875, prec 0.0481649, recall 0.841316
2017-12-10T15:46:54.695619: step 1858, loss 0.475809, acc 0.875, prec 0.0482151, recall 0.841499
2017-12-10T15:46:54.890844: step 1859, loss 0.586965, acc 0.78125, prec 0.0482242, recall 0.84159
2017-12-10T15:46:55.085643: step 1860, loss 0.456386, acc 0.8125, prec 0.0482365, recall 0.841681
2017-12-10T15:46:55.280947: step 1861, loss 0.491931, acc 0.890625, prec 0.0482568, recall 0.841772
2017-12-10T15:46:55.479137: step 1862, loss 0.396598, acc 0.921875, prec 0.0482802, recall 0.841863
2017-12-10T15:46:55.675839: step 1863, loss 0.208865, acc 0.921875, prec 0.0482722, recall 0.841863
2017-12-10T15:46:55.872204: step 1864, loss 0.308207, acc 0.875, prec 0.0482909, recall 0.841954
2017-12-10T15:46:56.067612: step 1865, loss 0.540806, acc 0.859375, prec 0.0483393, recall 0.842135
2017-12-10T15:46:56.264835: step 1866, loss 1.14011, acc 0.921875, prec 0.048394, recall 0.842317
2017-12-10T15:46:56.460426: step 1867, loss 0.52384, acc 0.859375, prec 0.0483797, recall 0.842317
2017-12-10T15:46:56.660592: step 1868, loss 0.312889, acc 0.890625, prec 0.0483685, recall 0.842317
2017-12-10T15:46:56.857534: step 1869, loss 1.14242, acc 0.90625, prec 0.0484216, recall 0.842497
2017-12-10T15:46:57.052710: step 1870, loss 0.275379, acc 0.953125, prec 0.0484795, recall 0.842677
2017-12-10T15:46:57.258152: step 1871, loss 0.341338, acc 0.890625, prec 0.0484683, recall 0.842677
2017-12-10T15:46:57.453863: step 1872, loss 0.272909, acc 0.90625, prec 0.0484587, recall 0.842677
2017-12-10T15:46:57.648471: step 1873, loss 0.395423, acc 0.90625, prec 0.0484492, recall 0.842677
2017-12-10T15:46:57.845606: step 1874, loss 0.222009, acc 0.921875, prec 0.0484412, recall 0.842677
2017-12-10T15:46:58.040003: step 1875, loss 0.692425, acc 0.90625, prec 0.0484942, recall 0.842857
2017-12-10T15:46:58.238275: step 1876, loss 0.274476, acc 0.859375, prec 0.0484799, recall 0.842857
2017-12-10T15:46:58.436265: step 1877, loss 0.183312, acc 0.921875, prec 0.0484719, recall 0.842857
2017-12-10T15:46:58.634247: step 1878, loss 0.0901607, acc 0.96875, prec 0.0485, recall 0.842947
2017-12-10T15:46:58.829141: step 1879, loss 0.241558, acc 0.875, prec 0.0484872, recall 0.842947
2017-12-10T15:46:59.028242: step 1880, loss 0.494306, acc 0.953125, prec 0.0486075, recall 0.843305
2017-12-10T15:46:59.227385: step 1881, loss 0.259977, acc 0.890625, prec 0.0485963, recall 0.843305
2017-12-10T15:46:59.432413: step 1882, loss 1.72136, acc 0.828125, prec 0.0486116, recall 0.842914
2017-12-10T15:46:59.625206: step 1883, loss 0.217278, acc 0.921875, prec 0.0486036, recall 0.842914
2017-12-10T15:46:59.818700: step 1884, loss 0.151556, acc 0.953125, prec 0.04863, recall 0.843003
2017-12-10T15:47:00.016192: step 1885, loss 1.23064, acc 0.921875, prec 0.0486533, recall 0.843093
2017-12-10T15:47:00.214143: step 1886, loss 0.468574, acc 0.875, prec 0.0486717, recall 0.843182
2017-12-10T15:47:00.413065: step 1887, loss 0.874172, acc 0.859375, prec 0.0487509, recall 0.843449
2017-12-10T15:47:00.612959: step 1888, loss 0.208934, acc 0.859375, prec 0.0487365, recall 0.843449
2017-12-10T15:47:00.811824: step 1889, loss 0.623666, acc 0.921875, prec 0.0487597, recall 0.843537
2017-12-10T15:47:01.009408: step 1890, loss 0.186324, acc 0.921875, prec 0.0487517, recall 0.843537
2017-12-10T15:47:01.212263: step 1891, loss 0.496321, acc 0.84375, prec 0.0487358, recall 0.843537
2017-12-10T15:47:01.417304: step 1892, loss 0.424606, acc 0.78125, prec 0.0487134, recall 0.843537
2017-12-10T15:47:01.616307: step 1893, loss 0.489606, acc 0.84375, prec 0.0487286, recall 0.843626
2017-12-10T15:47:01.813242: step 1894, loss 1.18002, acc 0.890625, prec 0.0488108, recall 0.843891
2017-12-10T15:47:02.018250: step 1895, loss 0.354708, acc 0.875, prec 0.0488603, recall 0.844068
2017-12-10T15:47:02.211252: step 1896, loss 0.311008, acc 0.84375, prec 0.0488754, recall 0.844156
2017-12-10T15:47:02.404543: step 1897, loss 0.598426, acc 0.75, prec 0.0488498, recall 0.844156
2017-12-10T15:47:02.596992: step 1898, loss 0.36458, acc 0.84375, prec 0.0488339, recall 0.844156
2017-12-10T15:47:02.792301: step 1899, loss 0.741485, acc 0.765625, prec 0.048841, recall 0.844244
2017-12-10T15:47:02.987172: step 1900, loss 0.516845, acc 0.796875, prec 0.0488203, recall 0.844244
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_3/1512938428/checkpoints/model-1900

2017-12-10T15:47:04.201499: step 1901, loss 0.574115, acc 0.828125, prec 0.0488338, recall 0.844332
2017-12-10T15:47:04.398464: step 1902, loss 1.2861, acc 0.828125, prec 0.0489093, recall 0.844595
2017-12-10T15:47:04.594899: step 1903, loss 0.268904, acc 0.90625, prec 0.0489308, recall 0.844682
2017-12-10T15:47:04.793984: step 1904, loss 0.626369, acc 0.8125, prec 0.0489116, recall 0.844682
2017-12-10T15:47:04.987151: step 1905, loss 0.451471, acc 0.84375, prec 0.0489267, recall 0.844769
2017-12-10T15:47:05.190820: step 1906, loss 0.390366, acc 0.90625, prec 0.0489171, recall 0.844769
2017-12-10T15:47:05.384268: step 1907, loss 0.180137, acc 0.90625, prec 0.0489385, recall 0.844857
2017-12-10T15:47:05.578558: step 1908, loss 0.438177, acc 0.859375, prec 0.0489551, recall 0.844944
2017-12-10T15:47:05.776033: step 1909, loss 0.194585, acc 0.921875, prec 0.0489472, recall 0.844944
2017-12-10T15:47:05.974629: step 1910, loss 0.197633, acc 0.921875, prec 0.0489392, recall 0.844944
2017-12-10T15:47:06.171657: step 1911, loss 0.936789, acc 0.921875, prec 0.0489622, recall 0.845031
2017-12-10T15:47:06.367720: step 1912, loss 0.338082, acc 0.890625, prec 0.048982, recall 0.845118
2017-12-10T15:47:06.565789: step 1913, loss 0.505791, acc 0.90625, prec 0.0490033, recall 0.845205
2017-12-10T15:47:06.764364: step 1914, loss 0.13139, acc 0.953125, prec 0.0489986, recall 0.845205
2017-12-10T15:47:06.962217: step 1915, loss 0.285938, acc 0.921875, prec 0.0490215, recall 0.845291
2017-12-10T15:47:07.161403: step 1916, loss 0.351038, acc 0.921875, prec 0.0490754, recall 0.845465
2017-12-10T15:47:07.359898: step 1917, loss 0.966355, acc 0.96875, prec 0.0491031, recall 0.845551
2017-12-10T15:47:07.560326: step 1918, loss 1.70685, acc 0.859375, prec 0.0490903, recall 0.845078
2017-12-10T15:47:07.766014: step 1919, loss 1.66351, acc 0.96875, prec 0.0491196, recall 0.844693
2017-12-10T15:47:07.958651: step 1920, loss 0.53381, acc 0.859375, prec 0.0491979, recall 0.844953
2017-12-10T15:47:08.153364: step 1921, loss 0.630345, acc 0.875, prec 0.049216, recall 0.845039
2017-12-10T15:47:08.350169: step 1922, loss 0.402841, acc 0.875, prec 0.0492649, recall 0.845212
2017-12-10T15:47:08.546665: step 1923, loss 0.245595, acc 0.90625, prec 0.0492862, recall 0.845298
2017-12-10T15:47:08.740921: step 1924, loss 0.709331, acc 0.859375, prec 0.049426, recall 0.845727
2017-12-10T15:47:08.938475: step 1925, loss 0.488466, acc 0.796875, prec 0.0494051, recall 0.845727
2017-12-10T15:47:09.141302: step 1926, loss 0.62997, acc 0.765625, prec 0.0493811, recall 0.845727
2017-12-10T15:47:09.332772: step 1927, loss 1.06685, acc 0.796875, prec 0.0493911, recall 0.845813
2017-12-10T15:47:09.535292: step 1928, loss 0.461293, acc 0.8125, prec 0.0494027, recall 0.845898
2017-12-10T15:47:09.731027: step 1929, loss 0.775333, acc 0.765625, prec 0.049471, recall 0.846154
2017-12-10T15:47:09.925020: step 1930, loss 0.652661, acc 0.8125, prec 0.0494825, recall 0.846239
2017-12-10T15:47:10.117631: step 1931, loss 1.56916, acc 0.75, prec 0.0494877, recall 0.846324
2017-12-10T15:47:10.318960: step 1932, loss 0.50742, acc 0.875, prec 0.0495363, recall 0.846494
2017-12-10T15:47:10.514020: step 1933, loss 0.749642, acc 0.78125, prec 0.0495446, recall 0.846578
2017-12-10T15:47:10.711731: step 1934, loss 0.413506, acc 0.859375, prec 0.0495302, recall 0.846578
2017-12-10T15:47:10.906576: step 1935, loss 0.634674, acc 0.828125, prec 0.0495126, recall 0.846578
2017-12-10T15:47:11.103706: step 1936, loss 1.73126, acc 0.765625, prec 0.0495193, recall 0.846663
2017-12-10T15:47:11.302474: step 1937, loss 0.567993, acc 0.796875, prec 0.0494986, recall 0.846663
2017-12-10T15:47:11.496226: step 1938, loss 0.46646, acc 0.921875, prec 0.0494906, recall 0.846663
2017-12-10T15:47:11.689115: step 1939, loss 0.655869, acc 0.78125, prec 0.0494989, recall 0.846748
2017-12-10T15:47:11.890395: step 1940, loss 0.69575, acc 0.796875, prec 0.0495088, recall 0.846832
2017-12-10T15:47:12.086022: step 1941, loss 0.257178, acc 0.90625, prec 0.0495604, recall 0.847001
2017-12-10T15:47:12.282622: step 1942, loss 0.327621, acc 0.84375, prec 0.0495751, recall 0.847085
2017-12-10T15:47:12.481697: step 1943, loss 0.267065, acc 0.890625, prec 0.0495945, recall 0.847169
2017-12-10T15:47:12.679591: step 1944, loss 0.35583, acc 0.859375, prec 0.0496107, recall 0.847253
2017-12-10T15:47:12.872024: step 1945, loss 0.331391, acc 0.828125, prec 0.0496237, recall 0.847337
2017-12-10T15:47:13.071446: step 1946, loss 0.807958, acc 0.890625, prec 0.0497042, recall 0.847588
2017-12-10T15:47:13.270570: step 1947, loss 0.227398, acc 0.921875, prec 0.0496962, recall 0.847588
2017-12-10T15:47:13.465415: step 1948, loss 0.187604, acc 0.953125, prec 0.0496914, recall 0.847588
2017-12-10T15:47:13.662348: step 1949, loss 0.287517, acc 0.890625, prec 0.0496803, recall 0.847588
2017-12-10T15:47:13.859452: step 1950, loss 0.28833, acc 0.921875, prec 0.0496723, recall 0.847588
2017-12-10T15:47:14.055848: step 1951, loss 0.168678, acc 0.921875, prec 0.0496643, recall 0.847588
2017-12-10T15:47:14.255659: step 1952, loss 0.325729, acc 0.953125, prec 0.04969, recall 0.847671
2017-12-10T15:47:14.462413: step 1953, loss 0.288593, acc 0.890625, prec 0.0496789, recall 0.847671
2017-12-10T15:47:14.652717: step 1954, loss 0.113784, acc 0.953125, prec 0.0496741, recall 0.847671
2017-12-10T15:47:14.850907: step 1955, loss 0.136176, acc 0.9375, prec 0.0496677, recall 0.847671
2017-12-10T15:47:15.048770: step 1956, loss 0.185725, acc 0.90625, prec 0.0496581, recall 0.847671
2017-12-10T15:47:15.247367: step 1957, loss 0.61602, acc 0.953125, prec 0.0497144, recall 0.847838
2017-12-10T15:47:15.449346: step 1958, loss 0.595725, acc 0.953125, prec 0.0497401, recall 0.847921
2017-12-10T15:47:15.649126: step 1959, loss 0.198561, acc 0.875, prec 0.0497578, recall 0.848004
2017-12-10T15:47:15.844750: step 1960, loss 0.0613261, acc 0.96875, prec 0.0497546, recall 0.848004
2017-12-10T15:47:16.039384: step 1961, loss 0.52693, acc 0.96875, prec 0.0497819, recall 0.848087
2017-12-10T15:47:16.240957: step 1962, loss 0.139074, acc 0.96875, prec 0.0498092, recall 0.84817
2017-12-10T15:47:16.442433: step 1963, loss 0.45076, acc 0.921875, prec 0.0498621, recall 0.848336
2017-12-10T15:47:16.642576: step 1964, loss 0.223463, acc 0.9375, prec 0.0498862, recall 0.848419
2017-12-10T15:47:16.838068: step 1965, loss 0.442794, acc 0.9375, prec 0.0499407, recall 0.848584
2017-12-10T15:47:17.036310: step 1966, loss 0.564902, acc 0.953125, prec 0.0499968, recall 0.848749
2017-12-10T15:47:17.240013: step 1967, loss 0.536617, acc 0.890625, prec 0.0500465, recall 0.848913
2017-12-10T15:47:17.436980: step 1968, loss 0.171199, acc 0.90625, prec 0.0500368, recall 0.848913
2017-12-10T15:47:17.631452: step 1969, loss 0.407342, acc 0.84375, prec 0.0500512, recall 0.848995
2017-12-10T15:47:17.830128: step 1970, loss 0.164569, acc 0.9375, prec 0.0500752, recall 0.849077
2017-12-10T15:47:18.032620: step 1971, loss 0.402245, acc 0.890625, prec 0.050064, recall 0.849077
2017-12-10T15:47:18.228486: step 1972, loss 0.080644, acc 0.984375, prec 0.0500624, recall 0.849077
2017-12-10T15:47:18.424498: step 1973, loss 0.217735, acc 0.875, prec 0.0500496, recall 0.849077
2017-12-10T15:47:18.621178: step 1974, loss 0.132441, acc 0.9375, prec 0.0500432, recall 0.849077
2017-12-10T15:47:18.819892: step 1975, loss 0.121903, acc 0.921875, prec 0.0500352, recall 0.849077
2017-12-10T15:47:19.014405: step 1976, loss 0.155516, acc 0.9375, prec 0.0500288, recall 0.849077
2017-12-10T15:47:19.210610: step 1977, loss 0.287966, acc 0.921875, prec 0.0500512, recall 0.849159
2017-12-10T15:47:19.406687: step 1978, loss 0.261443, acc 0.9375, prec 0.0500448, recall 0.849159
2017-12-10T15:47:19.604145: step 1979, loss 4.04147, acc 0.90625, prec 0.0500975, recall 0.848862
2017-12-10T15:47:19.804649: step 1980, loss 0.195938, acc 0.953125, prec 0.0501231, recall 0.848944
2017-12-10T15:47:20.000357: step 1981, loss 1.6929, acc 0.875, prec 0.0501119, recall 0.848485
2017-12-10T15:47:20.198519: step 1982, loss 0.216423, acc 0.875, prec 0.050099, recall 0.848485
2017-12-10T15:47:20.395047: step 1983, loss 0.463987, acc 0.828125, prec 0.0500814, recall 0.848485
2017-12-10T15:47:20.587713: step 1984, loss 0.516125, acc 0.890625, prec 0.0501309, recall 0.848649
2017-12-10T15:47:20.782620: step 1985, loss 0.501751, acc 0.78125, prec 0.0501085, recall 0.848649
2017-12-10T15:47:20.980752: step 1986, loss 0.468574, acc 0.796875, prec 0.0500877, recall 0.848649
2017-12-10T15:47:21.177127: step 1987, loss 0.982423, acc 0.828125, prec 0.0501005, recall 0.84873
2017-12-10T15:47:21.360771: step 1988, loss 0.415561, acc 0.884615, prec 0.0501212, recall 0.848812
2017-12-10T15:47:21.563986: step 1989, loss 0.407117, acc 0.84375, prec 0.0501657, recall 0.848975
2017-12-10T15:47:21.759292: step 1990, loss 0.663753, acc 0.78125, prec 0.0501434, recall 0.848975
2017-12-10T15:47:21.954885: step 1991, loss 2.05097, acc 0.8125, prec 0.0501258, recall 0.848518
2017-12-10T15:47:22.149075: step 1992, loss 0.970762, acc 0.90625, prec 0.0501465, recall 0.848599
2017-12-10T15:47:22.345224: step 1993, loss 0.410281, acc 0.828125, prec 0.0501289, recall 0.848599
2017-12-10T15:47:22.539729: step 1994, loss 0.455237, acc 0.875, prec 0.0501161, recall 0.848599
2017-12-10T15:47:22.735793: step 1995, loss 0.611126, acc 0.71875, prec 0.0501177, recall 0.848681
2017-12-10T15:47:22.934240: step 1996, loss 0.381268, acc 0.796875, prec 0.050097, recall 0.848681
2017-12-10T15:47:23.128260: step 1997, loss 0.369521, acc 0.84375, prec 0.050081, recall 0.848681
2017-12-10T15:47:23.324072: step 1998, loss 0.563382, acc 0.796875, prec 0.0500905, recall 0.848762
2017-12-10T15:47:23.520625: step 1999, loss 0.296119, acc 0.921875, prec 0.0502032, recall 0.849087
2017-12-10T15:47:23.713364: step 2000, loss 0.501304, acc 0.8125, prec 0.0502143, recall 0.849168
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_3/1512938428/checkpoints/model-2000

2017-12-10T15:47:24.936386: step 2001, loss 0.618837, acc 0.8125, prec 0.0501951, recall 0.849168
2017-12-10T15:47:25.131831: step 2002, loss 0.665353, acc 0.828125, prec 0.0501776, recall 0.849168
2017-12-10T15:47:25.327914: step 2003, loss 0.531098, acc 0.8125, prec 0.0501886, recall 0.849249
2017-12-10T15:47:25.525232: step 2004, loss 0.381131, acc 0.859375, prec 0.0502044, recall 0.84933
2017-12-10T15:47:25.721209: step 2005, loss 0.138053, acc 0.9375, prec 0.0501981, recall 0.84933
2017-12-10T15:47:25.918029: step 2006, loss 0.236136, acc 0.90625, prec 0.0502186, recall 0.84941
2017-12-10T15:47:26.119250: step 2007, loss 0.28943, acc 0.859375, prec 0.0502043, recall 0.84941
2017-12-10T15:47:26.310220: step 2008, loss 0.287349, acc 0.90625, prec 0.0501948, recall 0.84941
2017-12-10T15:47:26.505242: step 2009, loss 0.170554, acc 0.90625, prec 0.0502153, recall 0.849491
2017-12-10T15:47:26.702424: step 2010, loss 0.176405, acc 0.921875, prec 0.0502374, recall 0.849572
2017-12-10T15:47:26.901115: step 2011, loss 0.271987, acc 0.921875, prec 0.0502595, recall 0.849652
2017-12-10T15:47:27.102420: step 2012, loss 0.190526, acc 0.921875, prec 0.0502516, recall 0.849652
2017-12-10T15:47:27.299557: step 2013, loss 2.56928, acc 0.921875, prec 0.0502452, recall 0.849198
2017-12-10T15:47:27.498316: step 2014, loss 0.0454333, acc 0.984375, prec 0.0502436, recall 0.849198
2017-12-10T15:47:27.698343: step 2015, loss 0.272174, acc 0.96875, prec 0.0502705, recall 0.849278
2017-12-10T15:47:27.900521: step 2016, loss 0.0567532, acc 0.984375, prec 0.050329, recall 0.849439
2017-12-10T15:47:28.093141: step 2017, loss 0.144532, acc 0.96875, prec 0.0503258, recall 0.849439
2017-12-10T15:47:28.288917: step 2018, loss 0.2139, acc 0.9375, prec 0.0503194, recall 0.849439
2017-12-10T15:47:28.483286: step 2019, loss 0.0594993, acc 0.984375, prec 0.0503178, recall 0.849439
2017-12-10T15:47:28.680860: step 2020, loss 0.775851, acc 1, prec 0.050438, recall 0.84976
2017-12-10T15:47:28.876403: step 2021, loss 0.17564, acc 0.9375, prec 0.0504616, recall 0.84984
2017-12-10T15:47:29.078520: step 2022, loss 0.200356, acc 0.9375, prec 0.0504552, recall 0.84984
2017-12-10T15:47:29.282737: step 2023, loss 0.206307, acc 0.9375, prec 0.0504489, recall 0.84984
2017-12-10T15:47:29.478623: step 2024, loss 0.134097, acc 0.96875, prec 0.0504757, recall 0.84992
2017-12-10T15:47:29.678964: step 2025, loss 0.196671, acc 0.953125, prec 0.0505009, recall 0.85
2017-12-10T15:47:29.874544: step 2026, loss 0.198366, acc 0.9375, prec 0.0505245, recall 0.85008
2017-12-10T15:47:30.075516: step 2027, loss 0.379335, acc 0.921875, prec 0.0505465, recall 0.850159
2017-12-10T15:47:30.274189: step 2028, loss 0.617531, acc 0.953125, prec 0.0506017, recall 0.850318
2017-12-10T15:47:30.473455: step 2029, loss 1.02774, acc 0.921875, prec 0.0506537, recall 0.850477
2017-12-10T15:47:30.674767: step 2030, loss 0.142906, acc 0.921875, prec 0.0506757, recall 0.850556
2017-12-10T15:47:30.874319: step 2031, loss 0.474154, acc 0.890625, prec 0.0506944, recall 0.850636
2017-12-10T15:47:31.074193: step 2032, loss 0.120862, acc 0.96875, prec 0.0507212, recall 0.850715
2017-12-10T15:47:31.271252: step 2033, loss 2.3083, acc 0.96875, prec 0.0507795, recall 0.850423
2017-12-10T15:47:31.473453: step 2034, loss 0.1854, acc 0.953125, prec 0.0508047, recall 0.850502
2017-12-10T15:47:31.675182: step 2035, loss 0.206887, acc 0.953125, prec 0.0507999, recall 0.850502
2017-12-10T15:47:31.868815: step 2036, loss 0.189952, acc 0.921875, prec 0.0508218, recall 0.850581
2017-12-10T15:47:32.061487: step 2037, loss 0.225472, acc 0.90625, prec 0.0508122, recall 0.850581
2017-12-10T15:47:32.258141: step 2038, loss 0.342218, acc 0.890625, prec 0.0508309, recall 0.85066
2017-12-10T15:47:32.451119: step 2039, loss 0.532382, acc 0.796875, prec 0.05084, recall 0.850738
2017-12-10T15:47:32.645388: step 2040, loss 0.518141, acc 0.875, prec 0.050887, recall 0.850896
2017-12-10T15:47:32.844982: step 2041, loss 0.407351, acc 0.875, prec 0.0508742, recall 0.850896
2017-12-10T15:47:33.047145: step 2042, loss 0.427604, acc 0.859375, prec 0.0508597, recall 0.850896
2017-12-10T15:47:33.243010: step 2043, loss 0.429675, acc 0.859375, prec 0.0508453, recall 0.850896
2017-12-10T15:47:33.438524: step 2044, loss 0.423629, acc 0.84375, prec 0.0508592, recall 0.850974
2017-12-10T15:47:33.630423: step 2045, loss 0.580567, acc 0.796875, prec 0.0508384, recall 0.850974
2017-12-10T15:47:33.825354: step 2046, loss 0.411728, acc 0.84375, prec 0.0508523, recall 0.851053
2017-12-10T15:47:34.020754: step 2047, loss 0.314306, acc 0.875, prec 0.0508395, recall 0.851053
2017-12-10T15:47:34.213479: step 2048, loss 0.350955, acc 0.90625, prec 0.0508299, recall 0.851053
2017-12-10T15:47:34.407676: step 2049, loss 0.132392, acc 0.9375, prec 0.0508533, recall 0.851131
2017-12-10T15:47:34.602548: step 2050, loss 0.324687, acc 0.890625, prec 0.0508421, recall 0.851131
2017-12-10T15:47:34.796661: step 2051, loss 0.181482, acc 0.96875, prec 0.0508986, recall 0.851287
2017-12-10T15:47:34.992045: step 2052, loss 0.256862, acc 0.921875, prec 0.0508906, recall 0.851287
2017-12-10T15:47:35.190836: step 2053, loss 0.208076, acc 0.921875, prec 0.0508826, recall 0.851287
2017-12-10T15:47:35.384310: step 2054, loss 0.154862, acc 0.9375, prec 0.0508762, recall 0.851287
2017-12-10T15:47:35.582913: step 2055, loss 0.960523, acc 0.90625, prec 0.0509858, recall 0.851599
2017-12-10T15:47:35.783018: step 2056, loss 0.157467, acc 0.953125, prec 0.050981, recall 0.851599
2017-12-10T15:47:35.980449: step 2057, loss 0.157182, acc 0.96875, prec 0.0510076, recall 0.851677
2017-12-10T15:47:36.177349: step 2058, loss 0.141537, acc 0.96875, prec 0.051064, recall 0.851832
2017-12-10T15:47:36.368226: step 2059, loss 0.206317, acc 0.921875, prec 0.051056, recall 0.851832
2017-12-10T15:47:36.560856: step 2060, loss 0.0848037, acc 0.984375, prec 0.0510544, recall 0.851832
2017-12-10T15:47:36.759675: step 2061, loss 0.101883, acc 0.96875, prec 0.0510511, recall 0.851832
2017-12-10T15:47:36.955763: step 2062, loss 0.0166538, acc 1, prec 0.0510511, recall 0.851832
2017-12-10T15:47:37.154195: step 2063, loss 0.0290126, acc 1, prec 0.0510809, recall 0.85191
2017-12-10T15:47:37.346182: step 2064, loss 0.113637, acc 0.953125, prec 0.0510761, recall 0.85191
2017-12-10T15:47:37.542014: step 2065, loss 0.185018, acc 0.953125, prec 0.0511011, recall 0.851987
2017-12-10T15:47:37.740940: step 2066, loss 0.170169, acc 0.953125, prec 0.0510963, recall 0.851987
2017-12-10T15:47:37.936070: step 2067, loss 0.10729, acc 0.984375, prec 0.0510947, recall 0.851987
2017-12-10T15:47:38.137402: step 2068, loss 0.423155, acc 0.96875, prec 0.051151, recall 0.852142
2017-12-10T15:47:38.334051: step 2069, loss 2.44362, acc 0.953125, prec 0.0511478, recall 0.851697
2017-12-10T15:47:38.534569: step 2070, loss 0.19167, acc 0.984375, prec 0.0511759, recall 0.851775
2017-12-10T15:47:38.732867: step 2071, loss 0.346637, acc 0.875, prec 0.0511928, recall 0.851852
2017-12-10T15:47:38.935893: step 2072, loss 0.133854, acc 0.96875, prec 0.0512194, recall 0.851929
2017-12-10T15:47:39.131234: step 2073, loss 0.0887667, acc 0.96875, prec 0.0512756, recall 0.852083
2017-12-10T15:47:39.327647: step 2074, loss 0.0814661, acc 0.953125, prec 0.0512708, recall 0.852083
2017-12-10T15:47:39.525659: step 2075, loss 0.0796729, acc 0.96875, prec 0.0512676, recall 0.852083
2017-12-10T15:47:39.723860: step 2076, loss 0.171558, acc 0.953125, prec 0.0512628, recall 0.852083
2017-12-10T15:47:39.917635: step 2077, loss 0.250031, acc 0.953125, prec 0.0512579, recall 0.852083
2017-12-10T15:47:40.109991: step 2078, loss 0.266974, acc 0.90625, prec 0.0512483, recall 0.852083
2017-12-10T15:47:40.305370: step 2079, loss 0.366528, acc 0.921875, prec 0.05127, recall 0.85216
2017-12-10T15:47:40.500449: step 2080, loss 0.1784, acc 0.96875, prec 0.0512965, recall 0.852237
2017-12-10T15:47:40.695043: step 2081, loss 0.249243, acc 0.921875, prec 0.0513182, recall 0.852314
2017-12-10T15:47:40.888561: step 2082, loss 0.190339, acc 0.921875, prec 0.0513101, recall 0.852314
2017-12-10T15:47:41.080594: step 2083, loss 0.258021, acc 0.9375, prec 0.0513334, recall 0.852391
2017-12-10T15:47:41.277555: step 2084, loss 0.20593, acc 0.953125, prec 0.0514177, recall 0.852621
2017-12-10T15:47:41.469739: step 2085, loss 0.991099, acc 0.890625, prec 0.0514658, recall 0.852773
2017-12-10T15:47:41.661175: step 2086, loss 0.944421, acc 0.890625, prec 0.0515435, recall 0.853002
2017-12-10T15:47:41.857078: step 2087, loss 0.134233, acc 0.96875, prec 0.0515699, recall 0.853078
2017-12-10T15:47:42.053005: step 2088, loss 0.0963973, acc 0.953125, prec 0.0515651, recall 0.853078
2017-12-10T15:47:42.253105: step 2089, loss 0.221897, acc 0.921875, prec 0.0515867, recall 0.853154
2017-12-10T15:47:42.446668: step 2090, loss 0.0814419, acc 0.953125, prec 0.0516115, recall 0.85323
2017-12-10T15:47:42.640869: step 2091, loss 0.257108, acc 0.90625, prec 0.0516315, recall 0.853306
2017-12-10T15:47:42.835321: step 2092, loss 0.183974, acc 0.9375, prec 0.051625, recall 0.853306
2017-12-10T15:47:43.031731: step 2093, loss 0.134206, acc 0.953125, prec 0.0516202, recall 0.853306
2017-12-10T15:47:43.231806: step 2094, loss 0.173986, acc 0.953125, prec 0.0516153, recall 0.853306
2017-12-10T15:47:43.427278: step 2095, loss 0.265014, acc 0.96875, prec 0.0516714, recall 0.853457
2017-12-10T15:47:43.626362: step 2096, loss 0.093274, acc 0.96875, prec 0.0516681, recall 0.853457
2017-12-10T15:47:43.824279: step 2097, loss 0.153655, acc 0.984375, prec 0.0517258, recall 0.853608
2017-12-10T15:47:44.023439: step 2098, loss 0.134563, acc 0.953125, prec 0.0517505, recall 0.853684
2017-12-10T15:47:44.228308: step 2099, loss 0.175225, acc 0.96875, prec 0.0517769, recall 0.853759
2017-12-10T15:47:44.433487: step 2100, loss 0.128597, acc 0.953125, prec 0.0517721, recall 0.853759
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_3/1512938428/checkpoints/model-2100

2017-12-10T15:47:45.651475: step 2101, loss 0.123039, acc 0.953125, prec 0.0517968, recall 0.853834
2017-12-10T15:47:45.849947: step 2102, loss 0.348353, acc 0.828125, prec 0.0518086, recall 0.853909
2017-12-10T15:47:46.046700: step 2103, loss 0.276142, acc 0.9375, prec 0.0518317, recall 0.853985
2017-12-10T15:47:46.248888: step 2104, loss 0.206486, acc 0.96875, prec 0.0518581, recall 0.85406
2017-12-10T15:47:46.446174: step 2105, loss 0.353603, acc 0.953125, prec 0.0518828, recall 0.854135
2017-12-10T15:47:46.639446: step 2106, loss 0.114876, acc 0.953125, prec 0.051878, recall 0.854135
2017-12-10T15:47:46.834849: step 2107, loss 0.12978, acc 0.921875, prec 0.0518699, recall 0.854135
2017-12-10T15:47:47.031743: step 2108, loss 0.153088, acc 0.9375, prec 0.051893, recall 0.854209
2017-12-10T15:47:47.224837: step 2109, loss 0.0542781, acc 0.984375, prec 0.0519209, recall 0.854284
2017-12-10T15:47:47.424223: step 2110, loss 0.0526718, acc 0.984375, prec 0.0519193, recall 0.854284
2017-12-10T15:47:47.626062: step 2111, loss 0.0668201, acc 0.953125, prec 0.0519144, recall 0.854284
2017-12-10T15:47:47.824304: step 2112, loss 0.477642, acc 0.953125, prec 0.0519687, recall 0.854434
2017-12-10T15:47:48.021625: step 2113, loss 4.79562, acc 0.890625, prec 0.0520197, recall 0.853708
2017-12-10T15:47:48.220829: step 2114, loss 0.0252724, acc 1, prec 0.0520197, recall 0.853708
2017-12-10T15:47:48.419449: step 2115, loss 0.109879, acc 0.953125, prec 0.0520148, recall 0.853708
2017-12-10T15:47:48.614997: step 2116, loss 0.458308, acc 0.890625, prec 0.0520035, recall 0.853708
2017-12-10T15:47:48.807362: step 2117, loss 0.340064, acc 0.875, prec 0.0520201, recall 0.853783
2017-12-10T15:47:49.002856: step 2118, loss 0.290072, acc 0.90625, prec 0.0520103, recall 0.853783
2017-12-10T15:47:49.197056: step 2119, loss 0.461327, acc 0.84375, prec 0.0520237, recall 0.853858
2017-12-10T15:47:49.390688: step 2120, loss 0.17993, acc 0.90625, prec 0.0520139, recall 0.853858
2017-12-10T15:47:49.582832: step 2121, loss 0.213427, acc 0.921875, prec 0.0520354, recall 0.853933
2017-12-10T15:47:49.776702: step 2122, loss 0.307593, acc 0.890625, prec 0.052024, recall 0.853933
2017-12-10T15:47:49.973381: step 2123, loss 0.330839, acc 0.84375, prec 0.0520078, recall 0.853933
2017-12-10T15:47:50.167407: step 2124, loss 0.251747, acc 0.890625, prec 0.0519965, recall 0.853933
2017-12-10T15:47:50.360601: step 2125, loss 0.399054, acc 0.890625, prec 0.0520147, recall 0.854007
2017-12-10T15:47:50.554366: step 2126, loss 0.276515, acc 0.921875, prec 0.0520655, recall 0.854156
2017-12-10T15:47:50.754348: step 2127, loss 0.166204, acc 0.921875, prec 0.0520574, recall 0.854156
2017-12-10T15:47:50.947521: step 2128, loss 0.250978, acc 0.90625, prec 0.0520477, recall 0.854156
2017-12-10T15:47:51.141265: step 2129, loss 0.243376, acc 0.875, prec 0.0520348, recall 0.854156
2017-12-10T15:47:51.339094: step 2130, loss 0.202063, acc 0.90625, prec 0.0520251, recall 0.854156
2017-12-10T15:47:51.537119: step 2131, loss 0.356343, acc 0.90625, prec 0.0520448, recall 0.85423
2017-12-10T15:47:51.730735: step 2132, loss 0.267156, acc 0.90625, prec 0.052094, recall 0.854379
2017-12-10T15:47:51.929553: step 2133, loss 0.872274, acc 0.90625, prec 0.0521726, recall 0.854601
2017-12-10T15:47:52.122981: step 2134, loss 0.293301, acc 0.921875, prec 0.0522233, recall 0.854749
2017-12-10T15:47:52.319740: step 2135, loss 0.119998, acc 0.96875, prec 0.0522201, recall 0.854749
2017-12-10T15:47:52.514473: step 2136, loss 0.576714, acc 0.96875, prec 0.0522462, recall 0.854822
2017-12-10T15:47:52.707892: step 2137, loss 0.572092, acc 0.921875, prec 0.0523557, recall 0.855116
2017-12-10T15:47:52.907373: step 2138, loss 0.168154, acc 0.953125, prec 0.0523802, recall 0.85519
2017-12-10T15:47:53.101604: step 2139, loss 0.336475, acc 0.90625, prec 0.0523999, recall 0.855263
2017-12-10T15:47:53.299682: step 2140, loss 0.208161, acc 0.921875, prec 0.0524211, recall 0.855336
2017-12-10T15:47:53.501673: step 2141, loss 0.324255, acc 0.9375, prec 0.052444, recall 0.85541
2017-12-10T15:47:53.700035: step 2142, loss 0.198384, acc 0.96875, prec 0.0525288, recall 0.855628
2017-12-10T15:47:53.900453: step 2143, loss 0.105643, acc 0.953125, prec 0.0525239, recall 0.855628
2017-12-10T15:47:54.095528: step 2144, loss 0.153649, acc 0.921875, prec 0.0525452, recall 0.855701
2017-12-10T15:47:54.295081: step 2145, loss 0.135134, acc 0.9375, prec 0.0525386, recall 0.855701
2017-12-10T15:47:54.492990: step 2146, loss 2.18906, acc 0.90625, prec 0.0525599, recall 0.855343
2017-12-10T15:47:54.689017: step 2147, loss 0.0517385, acc 0.984375, prec 0.0525582, recall 0.855343
2017-12-10T15:47:54.886526: step 2148, loss 0.177719, acc 0.953125, prec 0.0525827, recall 0.855416
2017-12-10T15:47:55.079552: step 2149, loss 0.962349, acc 0.90625, prec 0.0526316, recall 0.855561
2017-12-10T15:47:55.279993: step 2150, loss 0.174992, acc 0.953125, prec 0.0526853, recall 0.855706
2017-12-10T15:47:55.483081: step 2151, loss 0.259947, acc 0.859375, prec 0.0527, recall 0.855779
2017-12-10T15:47:55.682143: step 2152, loss 2.60059, acc 0.90625, prec 0.0526918, recall 0.855349
2017-12-10T15:47:55.881132: step 2153, loss 0.18825, acc 0.921875, prec 0.0526837, recall 0.855349
2017-12-10T15:47:56.076367: step 2154, loss 0.503556, acc 0.859375, prec 0.052669, recall 0.855349
2017-12-10T15:47:56.271337: step 2155, loss 0.322845, acc 0.859375, prec 0.0526837, recall 0.855422
2017-12-10T15:47:56.466728: step 2156, loss 0.60476, acc 0.796875, prec 0.0526918, recall 0.855494
2017-12-10T15:47:56.663265: step 2157, loss 0.449519, acc 0.8125, prec 0.0526722, recall 0.855494
2017-12-10T15:47:56.856699: step 2158, loss 0.3959, acc 0.875, prec 0.0526592, recall 0.855494
2017-12-10T15:47:57.051345: step 2159, loss 0.322009, acc 0.875, prec 0.0527047, recall 0.855639
2017-12-10T15:47:57.247603: step 2160, loss 0.513085, acc 0.875, prec 0.0527209, recall 0.855711
2017-12-10T15:47:57.444603: step 2161, loss 0.4415, acc 0.859375, prec 0.0527355, recall 0.855784
2017-12-10T15:47:57.637943: step 2162, loss 0.495875, acc 0.875, prec 0.0527225, recall 0.855784
2017-12-10T15:47:57.831814: step 2163, loss 0.389058, acc 0.90625, prec 0.052742, recall 0.855856
2017-12-10T15:47:58.031236: step 2164, loss 0.490444, acc 0.84375, prec 0.0527841, recall 0.856
2017-12-10T15:47:58.223614: step 2165, loss 0.289505, acc 0.90625, prec 0.0528035, recall 0.856072
2017-12-10T15:47:58.422067: step 2166, loss 0.773252, acc 0.890625, prec 0.0528505, recall 0.856216
2017-12-10T15:47:58.619058: step 2167, loss 0.325028, acc 0.90625, prec 0.0528699, recall 0.856287
2017-12-10T15:47:58.816046: step 2168, loss 0.486347, acc 0.828125, prec 0.052852, recall 0.856287
2017-12-10T15:47:59.013483: step 2169, loss 0.364564, acc 0.859375, prec 0.0528666, recall 0.856359
2017-12-10T15:47:59.208203: step 2170, loss 0.520552, acc 0.796875, prec 0.0528454, recall 0.856359
2017-12-10T15:47:59.408618: step 2171, loss 0.38191, acc 0.875, prec 0.0528324, recall 0.856359
2017-12-10T15:47:59.604433: step 2172, loss 0.333179, acc 0.890625, prec 0.052821, recall 0.856359
2017-12-10T15:47:59.802057: step 2173, loss 0.276121, acc 0.875, prec 0.0528663, recall 0.856502
2017-12-10T15:47:59.999767: step 2174, loss 0.288733, acc 0.875, prec 0.0528533, recall 0.856502
2017-12-10T15:48:00.199627: step 2175, loss 0.262689, acc 0.921875, prec 0.0528743, recall 0.856574
2017-12-10T15:48:00.396829: step 2176, loss 0.0623073, acc 0.984375, prec 0.0528726, recall 0.856574
2017-12-10T15:48:00.601048: step 2177, loss 0.276763, acc 0.921875, prec 0.0528936, recall 0.856645
2017-12-10T15:48:00.796630: step 2178, loss 0.204336, acc 0.9375, prec 0.0528871, recall 0.856645
2017-12-10T15:48:00.993829: step 2179, loss 1.54145, acc 0.9375, prec 0.0529388, recall 0.856788
2017-12-10T15:48:01.193198: step 2180, loss 0.294221, acc 0.953125, prec 0.052963, recall 0.856859
2017-12-10T15:48:01.393104: step 2181, loss 0.225712, acc 0.96875, prec 0.0529889, recall 0.85693
2017-12-10T15:48:01.595530: step 2182, loss 0.424409, acc 0.90625, prec 0.0530082, recall 0.857001
2017-12-10T15:48:01.794858: step 2183, loss 0.175332, acc 0.984375, prec 0.0530357, recall 0.857072
2017-12-10T15:48:01.992321: step 2184, loss 0.192076, acc 0.9375, prec 0.0530291, recall 0.857072
2017-12-10T15:48:02.189782: step 2185, loss 2.37458, acc 0.921875, prec 0.0530226, recall 0.856647
2017-12-10T15:48:02.396077: step 2186, loss 0.089953, acc 0.96875, prec 0.0530484, recall 0.856718
2017-12-10T15:48:02.591422: step 2187, loss 1.18438, acc 0.953125, prec 0.0531017, recall 0.85686
2017-12-10T15:48:02.791910: step 2188, loss 0.27749, acc 0.96875, prec 0.0531566, recall 0.857001
2017-12-10T15:48:02.992097: step 2189, loss 0.338235, acc 0.90625, prec 0.0531758, recall 0.857072
2017-12-10T15:48:03.191615: step 2190, loss 0.293933, acc 0.890625, prec 0.0531934, recall 0.857143
2017-12-10T15:48:03.391512: step 2191, loss 0.174858, acc 0.9375, prec 0.053216, recall 0.857213
2017-12-10T15:48:03.590009: step 2192, loss 0.348915, acc 0.890625, prec 0.0532045, recall 0.857213
2017-12-10T15:48:03.783463: step 2193, loss 0.268815, acc 0.859375, prec 0.0531899, recall 0.857213
2017-12-10T15:48:03.979885: step 2194, loss 0.46452, acc 0.890625, prec 0.0532075, recall 0.857284
2017-12-10T15:48:04.172729: step 2195, loss 0.277522, acc 0.875, prec 0.0532234, recall 0.857354
2017-12-10T15:48:04.369465: step 2196, loss 0.827495, acc 0.765625, prec 0.053199, recall 0.857354
2017-12-10T15:48:04.568504: step 2197, loss 0.461688, acc 0.875, prec 0.053186, recall 0.857354
2017-12-10T15:48:04.762269: step 2198, loss 0.383679, acc 0.890625, prec 0.0531746, recall 0.857354
2017-12-10T15:48:04.957468: step 2199, loss 0.240015, acc 0.859375, prec 0.0531889, recall 0.857425
2017-12-10T15:48:05.156680: step 2200, loss 0.240832, acc 0.9375, prec 0.0532113, recall 0.857495
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_3/1512938428/checkpoints/model-2200

2017-12-10T15:48:06.320799: step 2201, loss 0.848355, acc 0.875, prec 0.0532273, recall 0.857565
2017-12-10T15:48:06.518425: step 2202, loss 0.136988, acc 0.953125, prec 0.0532803, recall 0.857706
2017-12-10T15:48:06.717732: step 2203, loss 0.307837, acc 0.875, prec 0.0533252, recall 0.857846
2017-12-10T15:48:06.915175: step 2204, loss 0.290929, acc 0.90625, prec 0.0533154, recall 0.857846
2017-12-10T15:48:07.112688: step 2205, loss 0.312749, acc 0.9375, prec 0.0534246, recall 0.858125
2017-12-10T15:48:07.306711: step 2206, loss 0.634896, acc 0.828125, prec 0.0534356, recall 0.858194
2017-12-10T15:48:07.505519: step 2207, loss 0.490178, acc 0.859375, prec 0.0534209, recall 0.858194
2017-12-10T15:48:07.700255: step 2208, loss 0.204684, acc 0.921875, prec 0.0534127, recall 0.858194
2017-12-10T15:48:07.897467: step 2209, loss 0.263622, acc 0.890625, prec 0.0534013, recall 0.858194
2017-12-10T15:48:08.092738: step 2210, loss 0.215293, acc 0.921875, prec 0.053451, recall 0.858333
2017-12-10T15:48:08.290815: step 2211, loss 0.381853, acc 0.875, prec 0.0534957, recall 0.858472
2017-12-10T15:48:08.484591: step 2212, loss 0.325678, acc 0.890625, prec 0.053542, recall 0.858611
2017-12-10T15:48:08.681367: step 2213, loss 0.361839, acc 0.9375, prec 0.0535932, recall 0.858749
2017-12-10T15:48:08.877702: step 2214, loss 0.0434013, acc 0.984375, prec 0.0535916, recall 0.858749
2017-12-10T15:48:09.079127: step 2215, loss 0.320049, acc 0.953125, prec 0.0536444, recall 0.858887
2017-12-10T15:48:09.279047: step 2216, loss 0.160509, acc 0.96875, prec 0.05367, recall 0.858956
2017-12-10T15:48:09.476121: step 2217, loss 0.322226, acc 0.9375, prec 0.0536634, recall 0.858956
2017-12-10T15:48:09.668874: step 2218, loss 0.341528, acc 0.9375, prec 0.0537146, recall 0.859093
2017-12-10T15:48:09.862992: step 2219, loss 0.0874869, acc 0.984375, prec 0.053713, recall 0.859093
2017-12-10T15:48:10.063844: step 2220, loss 0.628502, acc 0.953125, prec 0.0537657, recall 0.85923
2017-12-10T15:48:10.259406: step 2221, loss 0.109561, acc 0.953125, prec 0.0537897, recall 0.859299
2017-12-10T15:48:10.451870: step 2222, loss 0.0849502, acc 0.984375, prec 0.0538169, recall 0.859367
2017-12-10T15:48:10.651523: step 2223, loss 0.798149, acc 0.96875, prec 0.0538712, recall 0.859504
2017-12-10T15:48:10.850010: step 2224, loss 0.184866, acc 0.96875, prec 0.053868, recall 0.859504
2017-12-10T15:48:11.052794: step 2225, loss 0.0742386, acc 0.96875, prec 0.0538647, recall 0.859504
2017-12-10T15:48:11.249314: step 2226, loss 0.377324, acc 0.9375, prec 0.0538869, recall 0.859572
2017-12-10T15:48:11.447796: step 2227, loss 0.485321, acc 0.90625, prec 0.0539059, recall 0.859641
2017-12-10T15:48:11.651268: step 2228, loss 0.675332, acc 0.9375, prec 0.0539569, recall 0.859777
2017-12-10T15:48:11.853541: step 2229, loss 0.144714, acc 0.953125, prec 0.053952, recall 0.859777
2017-12-10T15:48:12.055620: step 2230, loss 0.147278, acc 0.953125, prec 0.0539471, recall 0.859777
2017-12-10T15:48:12.251437: step 2231, loss 0.269634, acc 0.953125, prec 0.0540286, recall 0.859981
2017-12-10T15:48:12.452058: step 2232, loss 0.800005, acc 0.984375, prec 0.0540845, recall 0.860116
2017-12-10T15:48:12.649993: step 2233, loss 0.417104, acc 0.890625, prec 0.054073, recall 0.860116
2017-12-10T15:48:12.849135: step 2234, loss 0.161704, acc 0.953125, prec 0.0540968, recall 0.860184
2017-12-10T15:48:13.053638: step 2235, loss 0.117273, acc 0.9375, prec 0.054119, recall 0.860251
2017-12-10T15:48:13.247542: step 2236, loss 0.42895, acc 0.875, prec 0.0541921, recall 0.860454
2017-12-10T15:48:13.440463: step 2237, loss 0.0952151, acc 0.96875, prec 0.0541888, recall 0.860454
2017-12-10T15:48:13.634818: step 2238, loss 0.607921, acc 0.875, prec 0.0542619, recall 0.860656
2017-12-10T15:48:13.832671: step 2239, loss 1.44358, acc 0.90625, prec 0.0542824, recall 0.860308
2017-12-10T15:48:14.035911: step 2240, loss 0.367347, acc 0.875, prec 0.054298, recall 0.860376
2017-12-10T15:48:14.240392: step 2241, loss 0.223178, acc 0.953125, prec 0.054293, recall 0.860376
2017-12-10T15:48:14.443387: step 2242, loss 0.201322, acc 0.953125, prec 0.0542881, recall 0.860376
2017-12-10T15:48:14.636840: step 2243, loss 0.311691, acc 0.90625, prec 0.0543356, recall 0.86051
2017-12-10T15:48:14.833622: step 2244, loss 0.387051, acc 0.859375, prec 0.0543495, recall 0.860577
2017-12-10T15:48:15.029440: step 2245, loss 0.312169, acc 0.90625, prec 0.0543396, recall 0.860577
2017-12-10T15:48:15.224012: step 2246, loss 0.401525, acc 0.84375, prec 0.0543518, recall 0.860644
2017-12-10T15:48:15.424355: step 2247, loss 0.238284, acc 0.9375, prec 0.0544026, recall 0.860778
2017-12-10T15:48:15.618902: step 2248, loss 0.254771, acc 0.90625, prec 0.0544214, recall 0.860845
2017-12-10T15:48:15.813299: step 2249, loss 0.2865, acc 0.90625, prec 0.0544115, recall 0.860845
2017-12-10T15:48:16.012989: step 2250, loss 0.342293, acc 0.890625, prec 0.0544286, recall 0.860911
2017-12-10T15:48:16.213534: step 2251, loss 0.146807, acc 0.9375, prec 0.0544506, recall 0.860978
2017-12-10T15:48:16.414675: step 2252, loss 0.291069, acc 0.875, prec 0.0544374, recall 0.860978
2017-12-10T15:48:16.611450: step 2253, loss 0.240256, acc 0.890625, prec 0.0544259, recall 0.860978
2017-12-10T15:48:16.810717: step 2254, loss 0.0758152, acc 0.96875, prec 0.0544226, recall 0.860978
2017-12-10T15:48:17.003733: step 2255, loss 0.145316, acc 0.9375, prec 0.0544446, recall 0.861045
2017-12-10T15:48:17.205741: step 2256, loss 0.308022, acc 0.984375, prec 0.0544716, recall 0.861111
2017-12-10T15:48:17.409622: step 2257, loss 1.73252, acc 0.921875, prec 0.0544937, recall 0.860766
2017-12-10T15:48:17.606635: step 2258, loss 0.0803803, acc 0.96875, prec 0.054519, recall 0.860832
2017-12-10T15:48:17.799295: step 2259, loss 0.217503, acc 0.90625, prec 0.0545377, recall 0.860899
2017-12-10T15:48:17.999698: step 2260, loss 0.562625, acc 0.9375, prec 0.0545884, recall 0.861032
2017-12-10T15:48:18.201835: step 2261, loss 0.170152, acc 0.984375, prec 0.054644, recall 0.861164
2017-12-10T15:48:18.406567: step 2262, loss 0.182615, acc 0.953125, prec 0.054639, recall 0.861164
2017-12-10T15:48:18.599671: step 2263, loss 0.313863, acc 0.890625, prec 0.0546561, recall 0.86123
2017-12-10T15:48:18.801009: step 2264, loss 2.46715, acc 0.90625, prec 0.0546478, recall 0.86082
2017-12-10T15:48:19.004768: step 2265, loss 0.212798, acc 0.984375, prec 0.0546747, recall 0.860886
2017-12-10T15:48:19.212564: step 2266, loss 0.182669, acc 0.953125, prec 0.054727, recall 0.861019
2017-12-10T15:48:19.412427: step 2267, loss 0.434142, acc 0.921875, prec 0.0548045, recall 0.861217
2017-12-10T15:48:19.607871: step 2268, loss 0.955236, acc 0.828125, prec 0.054872, recall 0.861414
2017-12-10T15:48:19.805549: step 2269, loss 0.311554, acc 0.890625, prec 0.0549461, recall 0.861611
2017-12-10T15:48:19.999724: step 2270, loss 0.399494, acc 0.84375, prec 0.0549295, recall 0.861611
2017-12-10T15:48:20.201131: step 2271, loss 0.407974, acc 0.84375, prec 0.0549129, recall 0.861611
2017-12-10T15:48:20.397544: step 2272, loss 0.429946, acc 0.796875, prec 0.0548913, recall 0.861611
2017-12-10T15:48:20.591441: step 2273, loss 0.339787, acc 0.890625, prec 0.0548797, recall 0.861611
2017-12-10T15:48:20.786151: step 2274, loss 1.78057, acc 0.828125, prec 0.0548917, recall 0.861269
2017-12-10T15:48:20.983431: step 2275, loss 0.372427, acc 0.921875, prec 0.0549404, recall 0.8614
2017-12-10T15:48:21.176490: step 2276, loss 2.19519, acc 0.75, prec 0.0549156, recall 0.860993
2017-12-10T15:48:21.381662: step 2277, loss 0.698087, acc 0.765625, prec 0.0549192, recall 0.861059
2017-12-10T15:48:21.577343: step 2278, loss 0.585226, acc 0.734375, prec 0.0549196, recall 0.861124
2017-12-10T15:48:21.769940: step 2279, loss 0.707438, acc 0.8125, prec 0.0549282, recall 0.86119
2017-12-10T15:48:21.967036: step 2280, loss 0.564752, acc 0.8125, prec 0.0549083, recall 0.86119
2017-12-10T15:48:22.160310: step 2281, loss 0.671711, acc 0.796875, prec 0.0549437, recall 0.861321
2017-12-10T15:48:22.361591: step 2282, loss 0.646859, acc 0.78125, prec 0.0549206, recall 0.861321
2017-12-10T15:48:22.556440: step 2283, loss 0.452205, acc 0.859375, prec 0.054991, recall 0.861517
2017-12-10T15:48:22.758194: step 2284, loss 0.346459, acc 0.859375, prec 0.0550045, recall 0.861582
2017-12-10T15:48:22.959153: step 2285, loss 0.615575, acc 0.84375, prec 0.0550164, recall 0.861647
2017-12-10T15:48:23.148790: step 2286, loss 0.38305, acc 0.84375, prec 0.0550282, recall 0.861712
2017-12-10T15:48:23.344558: step 2287, loss 0.562181, acc 0.828125, prec 0.0550101, recall 0.861712
2017-12-10T15:48:23.541918: step 2288, loss 0.553014, acc 0.828125, prec 0.0550203, recall 0.861777
2017-12-10T15:48:23.735362: step 2289, loss 0.648763, acc 0.78125, prec 0.0549971, recall 0.861777
2017-12-10T15:48:23.933865: step 2290, loss 1.15512, acc 0.828125, prec 0.0550073, recall 0.861842
2017-12-10T15:48:24.130073: step 2291, loss 0.220729, acc 0.921875, prec 0.0549991, recall 0.861842
2017-12-10T15:48:24.331076: step 2292, loss 0.594151, acc 0.859375, prec 0.0550409, recall 0.861972
2017-12-10T15:48:24.529869: step 2293, loss 0.241522, acc 0.875, prec 0.0550277, recall 0.861972
2017-12-10T15:48:24.730540: step 2294, loss 0.738223, acc 0.859375, prec 0.0550695, recall 0.862101
2017-12-10T15:48:24.933231: step 2295, loss 0.152548, acc 0.9375, prec 0.0550912, recall 0.862166
2017-12-10T15:48:25.129191: step 2296, loss 0.20063, acc 0.9375, prec 0.0550846, recall 0.862166
2017-12-10T15:48:25.326168: step 2297, loss 0.264844, acc 0.90625, prec 0.0550747, recall 0.862166
2017-12-10T15:48:25.522406: step 2298, loss 0.371773, acc 0.859375, prec 0.0550599, recall 0.862166
2017-12-10T15:48:25.718096: step 2299, loss 0.489195, acc 0.9375, prec 0.0550816, recall 0.862231
2017-12-10T15:48:25.916455: step 2300, loss 0.455711, acc 0.953125, prec 0.0551049, recall 0.862295
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_3/1512938428/checkpoints/model-2300

2017-12-10T15:48:27.217716: step 2301, loss 0.899084, acc 0.875, prec 0.0551483, recall 0.862424
2017-12-10T15:48:27.415749: step 2302, loss 0.161988, acc 0.921875, prec 0.05514, recall 0.862424
2017-12-10T15:48:27.611856: step 2303, loss 0.254315, acc 0.953125, prec 0.0551916, recall 0.862553
2017-12-10T15:48:27.810045: step 2304, loss 0.138743, acc 0.953125, prec 0.0552149, recall 0.862617
2017-12-10T15:48:28.011751: step 2305, loss 0.356624, acc 0.953125, prec 0.0552665, recall 0.862745
2017-12-10T15:48:28.204777: step 2306, loss 0.188188, acc 0.921875, prec 0.0552582, recall 0.862745
2017-12-10T15:48:28.402041: step 2307, loss 0.0383818, acc 0.984375, prec 0.0552565, recall 0.862745
2017-12-10T15:48:28.602843: step 2308, loss 0.221217, acc 0.9375, prec 0.0552782, recall 0.862809
2017-12-10T15:48:28.804443: step 2309, loss 0.14538, acc 0.96875, prec 0.0553031, recall 0.862873
2017-12-10T15:48:29.001795: step 2310, loss 0.180148, acc 0.90625, prec 0.0552932, recall 0.862873
2017-12-10T15:48:29.198358: step 2311, loss 0.571001, acc 0.890625, prec 0.0553099, recall 0.862937
2017-12-10T15:48:29.408228: step 2312, loss 0.0827928, acc 0.96875, prec 0.0553348, recall 0.863001
2017-12-10T15:48:29.602694: step 2313, loss 0.0851191, acc 0.953125, prec 0.0553298, recall 0.863001
2017-12-10T15:48:29.802695: step 2314, loss 1.82639, acc 0.890625, prec 0.0553199, recall 0.862599
2017-12-10T15:48:30.004826: step 2315, loss 0.225426, acc 0.921875, prec 0.0553117, recall 0.862599
2017-12-10T15:48:30.204157: step 2316, loss 3.9703, acc 0.9375, prec 0.0553067, recall 0.862197
2017-12-10T15:48:30.404968: step 2317, loss 0.307508, acc 0.953125, prec 0.0553299, recall 0.862262
2017-12-10T15:48:30.601670: step 2318, loss 0.164579, acc 0.90625, prec 0.05532, recall 0.862262
2017-12-10T15:48:30.800678: step 2319, loss 0.375706, acc 0.890625, prec 0.0553367, recall 0.862326
2017-12-10T15:48:30.999130: step 2320, loss 0.272514, acc 0.875, prec 0.0553517, recall 0.86239
2017-12-10T15:48:31.202693: step 2321, loss 0.783923, acc 0.9375, prec 0.0553732, recall 0.862454
2017-12-10T15:48:31.402303: step 2322, loss 0.901938, acc 0.765625, prec 0.0553485, recall 0.862454
2017-12-10T15:48:31.601191: step 2323, loss 0.488181, acc 0.828125, prec 0.0553303, recall 0.862454
2017-12-10T15:48:31.795845: step 2324, loss 0.453338, acc 0.78125, prec 0.0553072, recall 0.862454
2017-12-10T15:48:31.990671: step 2325, loss 0.560261, acc 0.8125, prec 0.0552875, recall 0.862454
2017-12-10T15:48:32.190048: step 2326, loss 0.969867, acc 0.765625, prec 0.055319, recall 0.862581
2017-12-10T15:48:32.385677: step 2327, loss 0.579555, acc 0.796875, prec 0.0552976, recall 0.862581
2017-12-10T15:48:32.580708: step 2328, loss 0.287425, acc 0.921875, prec 0.0552894, recall 0.862581
2017-12-10T15:48:32.777223: step 2329, loss 0.472779, acc 0.84375, prec 0.0553291, recall 0.862709
2017-12-10T15:48:32.971541: step 2330, loss 0.726362, acc 0.765625, prec 0.0553326, recall 0.862772
2017-12-10T15:48:33.164996: step 2331, loss 0.383606, acc 0.890625, prec 0.055321, recall 0.862772
2017-12-10T15:48:33.360013: step 2332, loss 0.356365, acc 0.875, prec 0.055336, recall 0.862836
2017-12-10T15:48:33.553034: step 2333, loss 0.494763, acc 0.828125, prec 0.0553459, recall 0.862899
2017-12-10T15:48:33.752511: step 2334, loss 0.375236, acc 0.875, prec 0.0553609, recall 0.862963
2017-12-10T15:48:33.951363: step 2335, loss 0.514174, acc 0.828125, prec 0.0553708, recall 0.863026
2017-12-10T15:48:34.146734: step 2336, loss 0.131579, acc 0.953125, prec 0.0553659, recall 0.863026
2017-12-10T15:48:34.344273: step 2337, loss 4.74611, acc 0.859375, prec 0.0553808, recall 0.862691
2017-12-10T15:48:34.546646: step 2338, loss 0.379434, acc 0.859375, prec 0.055366, recall 0.862691
2017-12-10T15:48:34.744124: step 2339, loss 0.371097, acc 0.90625, prec 0.0553842, recall 0.862754
2017-12-10T15:48:34.942116: step 2340, loss 0.183891, acc 0.90625, prec 0.0553743, recall 0.862754
2017-12-10T15:48:35.142767: step 2341, loss 0.202733, acc 0.921875, prec 0.0553661, recall 0.862754
2017-12-10T15:48:35.337546: step 2342, loss 0.412574, acc 0.96875, prec 0.0554748, recall 0.863007
2017-12-10T15:48:35.535156: step 2343, loss 0.415783, acc 0.875, prec 0.0554897, recall 0.863071
2017-12-10T15:48:35.734736: step 2344, loss 0.137903, acc 0.9375, prec 0.0554831, recall 0.863071
2017-12-10T15:48:35.927682: step 2345, loss 0.40156, acc 0.84375, prec 0.0555226, recall 0.863197
2017-12-10T15:48:36.123595: step 2346, loss 0.437102, acc 0.859375, prec 0.0555918, recall 0.863385
2017-12-10T15:48:36.322869: step 2347, loss 0.348255, acc 0.875, prec 0.0555786, recall 0.863385
2017-12-10T15:48:36.518568: step 2348, loss 0.264403, acc 0.921875, prec 0.0555983, recall 0.863448
2017-12-10T15:48:36.710754: step 2349, loss 0.157055, acc 0.953125, prec 0.0556213, recall 0.863511
2017-12-10T15:48:36.906862: step 2350, loss 0.990125, acc 0.90625, prec 0.0556674, recall 0.863636
2017-12-10T15:48:37.101645: step 2351, loss 0.452017, acc 0.875, prec 0.0556542, recall 0.863636
2017-12-10T15:48:37.302432: step 2352, loss 0.211237, acc 0.9375, prec 0.0556476, recall 0.863636
2017-12-10T15:48:37.494534: step 2353, loss 0.0579783, acc 0.984375, prec 0.055646, recall 0.863636
2017-12-10T15:48:37.693205: step 2354, loss 0.237941, acc 0.9375, prec 0.0556394, recall 0.863636
2017-12-10T15:48:37.892725: step 2355, loss 0.247125, acc 0.921875, prec 0.0556591, recall 0.863699
2017-12-10T15:48:38.093889: step 2356, loss 0.0862502, acc 0.96875, prec 0.0556558, recall 0.863699
2017-12-10T15:48:38.291619: step 2357, loss 0.275662, acc 0.953125, prec 0.0556788, recall 0.863761
2017-12-10T15:48:38.486118: step 2358, loss 5.68119, acc 0.921875, prec 0.0556722, recall 0.863365
2017-12-10T15:48:38.681721: step 2359, loss 0.164101, acc 0.953125, prec 0.0556952, recall 0.863428
2017-12-10T15:48:38.877070: step 2360, loss 0.248795, acc 0.921875, prec 0.0557148, recall 0.863491
2017-12-10T15:48:39.077454: step 2361, loss 0.467677, acc 0.9375, prec 0.0558199, recall 0.86374
2017-12-10T15:48:39.279007: step 2362, loss 0.283154, acc 0.96875, prec 0.0559281, recall 0.863989
2017-12-10T15:48:39.479989: step 2363, loss 0.231437, acc 0.921875, prec 0.0559757, recall 0.864113
2017-12-10T15:48:39.677310: step 2364, loss 0.507021, acc 0.859375, prec 0.0559887, recall 0.864175
2017-12-10T15:48:39.870885: step 2365, loss 0.662601, acc 0.796875, prec 0.0559672, recall 0.864175
2017-12-10T15:48:40.067626: step 2366, loss 0.387959, acc 0.921875, prec 0.0559868, recall 0.864237
2017-12-10T15:48:40.265800: step 2367, loss 0.475784, acc 0.859375, prec 0.0560276, recall 0.864361
2017-12-10T15:48:40.462414: step 2368, loss 0.363256, acc 0.890625, prec 0.056016, recall 0.864361
2017-12-10T15:48:40.661338: step 2369, loss 0.531764, acc 0.828125, prec 0.0559979, recall 0.864361
2017-12-10T15:48:40.853449: step 2370, loss 0.333211, acc 0.875, prec 0.0560125, recall 0.864422
2017-12-10T15:48:41.050801: step 2371, loss 0.244408, acc 0.875, prec 0.0559993, recall 0.864422
2017-12-10T15:48:41.244666: step 2372, loss 0.181796, acc 0.90625, prec 0.056045, recall 0.864545
2017-12-10T15:48:41.441824: step 2373, loss 0.842058, acc 0.859375, prec 0.056058, recall 0.864607
2017-12-10T15:48:41.644611: step 2374, loss 0.326399, acc 0.890625, prec 0.0560742, recall 0.864668
2017-12-10T15:48:41.838027: step 2375, loss 0.271623, acc 0.90625, prec 0.0561199, recall 0.864791
2017-12-10T15:48:42.039869: step 2376, loss 0.32502, acc 0.890625, prec 0.0561083, recall 0.864791
2017-12-10T15:48:42.238889: step 2377, loss 0.339854, acc 0.84375, prec 0.0561196, recall 0.864853
2017-12-10T15:48:42.434894: step 2378, loss 0.243488, acc 0.921875, prec 0.0561391, recall 0.864914
2017-12-10T15:48:42.631535: step 2379, loss 0.32254, acc 0.890625, prec 0.0561276, recall 0.864914
2017-12-10T15:48:42.833581: step 2380, loss 0.374625, acc 0.875, prec 0.0561143, recall 0.864914
2017-12-10T15:48:43.028484: step 2381, loss 0.553644, acc 0.921875, prec 0.0561616, recall 0.865036
2017-12-10T15:48:43.223011: step 2382, loss 0.21124, acc 0.90625, prec 0.0561517, recall 0.865036
2017-12-10T15:48:43.417894: step 2383, loss 0.183256, acc 0.953125, prec 0.0561467, recall 0.865036
2017-12-10T15:48:43.614435: step 2384, loss 0.118187, acc 0.9375, prec 0.0561401, recall 0.865036
2017-12-10T15:48:43.816367: step 2385, loss 0.207132, acc 0.921875, prec 0.0561874, recall 0.865158
2017-12-10T15:48:44.009519: step 2386, loss 3.04531, acc 0.953125, prec 0.0562395, recall 0.864889
2017-12-10T15:48:44.208663: step 2387, loss 0.250431, acc 0.90625, prec 0.0562573, recall 0.86495
2017-12-10T15:48:44.414463: step 2388, loss 0.136325, acc 0.9375, prec 0.0562507, recall 0.86495
2017-12-10T15:48:44.609136: step 2389, loss 0.281526, acc 0.90625, prec 0.056324, recall 0.865133
2017-12-10T15:48:44.802751: step 2390, loss 0.557334, acc 0.90625, prec 0.0563695, recall 0.865255
2017-12-10T15:48:45.001033: step 2391, loss 0.403148, acc 0.875, prec 0.0564116, recall 0.865376
2017-12-10T15:48:45.199723: step 2392, loss 0.354323, acc 0.9375, prec 0.0564881, recall 0.865558
2017-12-10T15:48:45.401275: step 2393, loss 0.214781, acc 0.9375, prec 0.0565091, recall 0.865618
2017-12-10T15:48:45.602118: step 2394, loss 0.356394, acc 0.875, prec 0.0565235, recall 0.865678
2017-12-10T15:48:45.797745: step 2395, loss 0.528767, acc 0.8125, prec 0.0565313, recall 0.865739
2017-12-10T15:48:45.994475: step 2396, loss 0.30902, acc 0.890625, prec 0.0565474, recall 0.865799
2017-12-10T15:48:46.189202: step 2397, loss 0.583843, acc 0.75, prec 0.0565208, recall 0.865799
2017-12-10T15:48:46.385239: step 2398, loss 0.321303, acc 0.859375, prec 0.0565336, recall 0.865859
2017-12-10T15:48:46.583757: step 2399, loss 0.475024, acc 0.875, prec 0.0565756, recall 0.865979
2017-12-10T15:48:46.779833: step 2400, loss 0.628044, acc 0.8125, prec 0.0565557, recall 0.865979
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_3/1512938428/checkpoints/model-2400

2017-12-10T15:48:48.008351: step 2401, loss 0.781059, acc 0.828125, prec 0.0565651, recall 0.866039
2017-12-10T15:48:48.205794: step 2402, loss 0.564966, acc 0.75, prec 0.0565662, recall 0.866099
2017-12-10T15:48:48.402323: step 2403, loss 0.25366, acc 0.921875, prec 0.0566959, recall 0.866399
2017-12-10T15:48:48.601591: step 2404, loss 0.518269, acc 0.875, prec 0.0567102, recall 0.866458
2017-12-10T15:48:48.806327: step 2405, loss 0.375604, acc 0.8125, prec 0.0567179, recall 0.866518
2017-12-10T15:48:49.001722: step 2406, loss 0.151617, acc 0.953125, prec 0.0567129, recall 0.866518
2017-12-10T15:48:49.197613: step 2407, loss 0.382406, acc 0.96875, prec 0.0567372, recall 0.866577
2017-12-10T15:48:49.397551: step 2408, loss 0.202502, acc 0.921875, prec 0.056784, recall 0.866696
2017-12-10T15:48:49.593041: step 2409, loss 0.279348, acc 0.921875, prec 0.0568308, recall 0.866815
2017-12-10T15:48:49.787205: step 2410, loss 0.151175, acc 0.9375, prec 0.0568242, recall 0.866815
2017-12-10T15:48:49.983908: step 2411, loss 0.629736, acc 0.859375, prec 0.0568368, recall 0.866874
2017-12-10T15:48:50.186011: step 2412, loss 0.649554, acc 0.859375, prec 0.0569044, recall 0.867052
2017-12-10T15:48:50.380638: step 2413, loss 0.0646081, acc 0.984375, prec 0.0569303, recall 0.867111
2017-12-10T15:48:50.579237: step 2414, loss 0.334467, acc 0.859375, prec 0.0569428, recall 0.86717
2017-12-10T15:48:50.775988: step 2415, loss 0.140785, acc 0.953125, prec 0.0569378, recall 0.86717
2017-12-10T15:48:50.972850: step 2416, loss 0.157374, acc 0.9375, prec 0.0569587, recall 0.867229
2017-12-10T15:48:51.171222: step 2417, loss 0.224667, acc 0.921875, prec 0.0569504, recall 0.867229
2017-12-10T15:48:51.367813: step 2418, loss 0.837592, acc 0.96875, prec 0.0569746, recall 0.867288
2017-12-10T15:48:51.566360: step 2419, loss 0.176835, acc 0.890625, prec 0.0569629, recall 0.867288
2017-12-10T15:48:51.763452: step 2420, loss 0.786325, acc 0.921875, prec 0.0570096, recall 0.867406
2017-12-10T15:48:51.960622: step 2421, loss 0.14992, acc 0.9375, prec 0.0570579, recall 0.867523
2017-12-10T15:48:52.157439: step 2422, loss 0.641534, acc 0.90625, prec 0.0571304, recall 0.867699
2017-12-10T15:48:52.352962: step 2423, loss 0.142014, acc 0.96875, prec 0.057182, recall 0.867816
2017-12-10T15:48:52.553343: step 2424, loss 0.170575, acc 0.984375, prec 0.0572352, recall 0.867933
2017-12-10T15:48:52.753252: step 2425, loss 0.275548, acc 0.9375, prec 0.0572286, recall 0.867933
2017-12-10T15:48:52.951936: step 2426, loss 0.205372, acc 0.90625, prec 0.0572186, recall 0.867933
2017-12-10T15:48:53.151827: step 2427, loss 0.302036, acc 0.90625, prec 0.057236, recall 0.867991
2017-12-10T15:48:53.351724: step 2428, loss 0.260162, acc 0.875, prec 0.0572776, recall 0.868108
2017-12-10T15:48:53.550277: step 2429, loss 0.249205, acc 0.90625, prec 0.057295, recall 0.868166
2017-12-10T15:48:53.746946: step 2430, loss 0.927349, acc 0.84375, prec 0.0573332, recall 0.868282
2017-12-10T15:48:53.944374: step 2431, loss 0.265784, acc 0.890625, prec 0.0573763, recall 0.868398
2017-12-10T15:48:54.136018: step 2432, loss 0.318746, acc 0.890625, prec 0.0573647, recall 0.868398
2017-12-10T15:48:54.333666: step 2433, loss 0.518431, acc 0.9375, prec 0.0573854, recall 0.868456
2017-12-10T15:48:54.535951: step 2434, loss 0.239331, acc 0.921875, prec 0.0574318, recall 0.868571
2017-12-10T15:48:54.732504: step 2435, loss 0.244763, acc 0.90625, prec 0.0574492, recall 0.868629
2017-12-10T15:48:54.930834: step 2436, loss 0.416184, acc 0.921875, prec 0.0574683, recall 0.868687
2017-12-10T15:48:55.128138: step 2437, loss 0.265983, acc 0.859375, prec 0.0574532, recall 0.868687
2017-12-10T15:48:55.320910: step 2438, loss 0.152729, acc 0.921875, prec 0.0574449, recall 0.868687
2017-12-10T15:48:55.517363: step 2439, loss 0.991924, acc 0.9375, prec 0.0575477, recall 0.868917
2017-12-10T15:48:55.721032: step 2440, loss 0.335313, acc 0.875, prec 0.0575343, recall 0.868917
2017-12-10T15:48:55.915627: step 2441, loss 0.308818, acc 0.875, prec 0.057521, recall 0.868917
2017-12-10T15:48:56.111669: step 2442, loss 0.139048, acc 0.9375, prec 0.0575143, recall 0.868917
2017-12-10T15:48:56.304942: step 2443, loss 0.130884, acc 0.9375, prec 0.0575076, recall 0.868917
2017-12-10T15:48:56.502540: step 2444, loss 0.219569, acc 0.9375, prec 0.0575009, recall 0.868917
2017-12-10T15:48:56.701340: step 2445, loss 0.269362, acc 0.921875, prec 0.0574926, recall 0.868917
2017-12-10T15:48:56.896191: step 2446, loss 0.197842, acc 0.984375, prec 0.0575456, recall 0.869032
2017-12-10T15:48:57.097215: step 2447, loss 0.297366, acc 0.890625, prec 0.0575339, recall 0.869032
2017-12-10T15:48:57.289753: step 2448, loss 0.108508, acc 0.96875, prec 0.0575306, recall 0.869032
2017-12-10T15:48:57.488614: step 2449, loss 0.691867, acc 0.984375, prec 0.0575836, recall 0.869147
2017-12-10T15:48:57.694815: step 2450, loss 1.29482, acc 0.890625, prec 0.0575992, recall 0.869204
2017-12-10T15:48:57.892448: step 2451, loss 1.48121, acc 0.859375, prec 0.0576132, recall 0.868881
2017-12-10T15:48:58.094021: step 2452, loss 0.200798, acc 0.875, prec 0.0575998, recall 0.868881
2017-12-10T15:48:58.292645: step 2453, loss 0.421063, acc 0.875, prec 0.0575865, recall 0.868881
2017-12-10T15:48:58.491606: step 2454, loss 0.491093, acc 0.859375, prec 0.0575987, recall 0.868938
2017-12-10T15:48:58.688174: step 2455, loss 0.33628, acc 0.84375, prec 0.0575821, recall 0.868938
2017-12-10T15:48:58.884884: step 2456, loss 0.168386, acc 0.921875, prec 0.0575737, recall 0.868938
2017-12-10T15:48:59.082150: step 2457, loss 0.571813, acc 0.859375, prec 0.057586, recall 0.868996
2017-12-10T15:48:59.282583: step 2458, loss 0.407573, acc 0.921875, prec 0.0576322, recall 0.86911
2017-12-10T15:48:59.485399: step 2459, loss 0.533679, acc 0.84375, prec 0.0576428, recall 0.869167
2017-12-10T15:48:59.681002: step 2460, loss 0.290138, acc 0.890625, prec 0.0576311, recall 0.869167
2017-12-10T15:48:59.876019: step 2461, loss 0.300579, acc 0.921875, prec 0.0576501, recall 0.869224
2017-12-10T15:49:00.073836: step 2462, loss 0.67568, acc 0.828125, prec 0.0576862, recall 0.869338
2017-12-10T15:49:00.272273: step 2463, loss 0.434601, acc 0.84375, prec 0.0576695, recall 0.869338
2017-12-10T15:49:00.469042: step 2464, loss 0.196189, acc 0.984375, prec 0.0576951, recall 0.869395
2017-12-10T15:49:00.667935: step 2465, loss 0.399936, acc 0.921875, prec 0.0577684, recall 0.869565
2017-12-10T15:49:00.863011: step 2466, loss 0.394731, acc 0.875, prec 0.0577551, recall 0.869565
2017-12-10T15:49:01.059071: step 2467, loss 0.431392, acc 0.90625, prec 0.0577451, recall 0.869565
2017-12-10T15:49:01.259785: step 2468, loss 0.237042, acc 0.90625, prec 0.0577351, recall 0.869565
2017-12-10T15:49:01.459715: step 2469, loss 0.0939357, acc 0.984375, prec 0.0577334, recall 0.869565
2017-12-10T15:49:01.662842: step 2470, loss 0.610311, acc 0.890625, prec 0.0577761, recall 0.869679
2017-12-10T15:49:01.866536: step 2471, loss 0.353038, acc 0.890625, prec 0.0577644, recall 0.869679
2017-12-10T15:49:02.067473: step 2472, loss 0.179543, acc 0.953125, prec 0.0577866, recall 0.869735
2017-12-10T15:49:02.264955: step 2473, loss 0.247039, acc 0.90625, prec 0.0577766, recall 0.869735
2017-12-10T15:49:02.461182: step 2474, loss 0.229794, acc 0.921875, prec 0.0577683, recall 0.869735
2017-12-10T15:49:02.654103: step 2475, loss 0.271532, acc 0.953125, prec 0.0578176, recall 0.869848
2017-12-10T15:49:02.852799: step 2476, loss 0.402468, acc 0.875, prec 0.0578043, recall 0.869848
2017-12-10T15:49:03.051593: step 2477, loss 0.0955673, acc 0.953125, prec 0.0577993, recall 0.869848
2017-12-10T15:49:03.252289: step 2478, loss 1.82921, acc 0.90625, prec 0.057791, recall 0.869471
2017-12-10T15:49:03.459091: step 2479, loss 1.8636, acc 0.953125, prec 0.0578691, recall 0.869264
2017-12-10T15:49:03.656566: step 2480, loss 0.131362, acc 0.921875, prec 0.0578608, recall 0.869264
2017-12-10T15:49:03.856391: step 2481, loss 0.145156, acc 0.921875, prec 0.0578524, recall 0.869264
2017-12-10T15:49:04.050490: step 2482, loss 0.482056, acc 0.859375, prec 0.0578646, recall 0.869321
2017-12-10T15:49:04.247826: step 2483, loss 0.188931, acc 0.9375, prec 0.057885, recall 0.869377
2017-12-10T15:49:04.446809: step 2484, loss 0.414764, acc 0.859375, prec 0.0578972, recall 0.869434
2017-12-10T15:49:04.630900: step 2485, loss 1.36826, acc 0.846154, prec 0.0579381, recall 0.869546
2017-12-10T15:49:04.840245: step 2486, loss 0.21701, acc 0.9375, prec 0.0579585, recall 0.869603
2017-12-10T15:49:05.035311: step 2487, loss 0.361323, acc 0.828125, prec 0.0579402, recall 0.869603
2017-12-10T15:49:05.232909: step 2488, loss 0.45968, acc 0.828125, prec 0.0579218, recall 0.869603
2017-12-10T15:49:05.431232: step 2489, loss 0.498524, acc 0.859375, prec 0.0579339, recall 0.869659
2017-12-10T15:49:05.630049: step 2490, loss 0.345254, acc 0.890625, prec 0.0579494, recall 0.869715
2017-12-10T15:49:05.832714: step 2491, loss 0.694346, acc 0.8125, prec 0.0579294, recall 0.869715
2017-12-10T15:49:06.026741: step 2492, loss 0.488702, acc 0.859375, prec 0.0579685, recall 0.869828
2017-12-10T15:49:06.231244: step 2493, loss 0.836182, acc 0.8125, prec 0.0579756, recall 0.869884
2017-12-10T15:49:06.425473: step 2494, loss 0.412684, acc 0.859375, prec 0.0579606, recall 0.869884
2017-12-10T15:49:06.623110: step 2495, loss 0.237659, acc 0.9375, prec 0.057954, recall 0.869884
2017-12-10T15:49:06.820841: step 2496, loss 0.623899, acc 0.78125, prec 0.0579307, recall 0.869884
2017-12-10T15:49:07.015831: step 2497, loss 0.183141, acc 0.921875, prec 0.0579494, recall 0.86994
2017-12-10T15:49:07.214804: step 2498, loss 0.173259, acc 0.921875, prec 0.0579681, recall 0.869996
2017-12-10T15:49:07.415059: step 2499, loss 0.310194, acc 0.90625, prec 0.0579581, recall 0.869996
2017-12-10T15:49:07.609585: step 2500, loss 0.52642, acc 0.875, prec 0.0580529, recall 0.870219
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_3/1512938428/checkpoints/model-2500

2017-12-10T15:49:08.938566: step 2501, loss 0.314966, acc 0.890625, prec 0.0580412, recall 0.870219
2017-12-10T15:49:09.140298: step 2502, loss 0.313273, acc 0.921875, prec 0.0580599, recall 0.870275
2017-12-10T15:49:09.341474: step 2503, loss 0.524436, acc 0.875, prec 0.0581275, recall 0.870442
2017-12-10T15:49:09.540884: step 2504, loss 0.102975, acc 0.9375, prec 0.0581748, recall 0.870553
2017-12-10T15:49:09.739625: step 2505, loss 0.199717, acc 0.96875, prec 0.0581715, recall 0.870553
2017-12-10T15:49:09.939382: step 2506, loss 0.555234, acc 0.9375, prec 0.0581918, recall 0.870608
2017-12-10T15:49:10.136640: step 2507, loss 0.0722937, acc 0.984375, prec 0.0581901, recall 0.870608
2017-12-10T15:49:10.338994: step 2508, loss 0.282682, acc 0.921875, prec 0.0582357, recall 0.870719
2017-12-10T15:49:10.540202: step 2509, loss 0.687561, acc 0.953125, prec 0.0582847, recall 0.87083
2017-12-10T15:49:10.738123: step 2510, loss 0.184737, acc 0.953125, prec 0.0583066, recall 0.870885
2017-12-10T15:49:10.934002: step 2511, loss 0.0942071, acc 0.953125, prec 0.0583016, recall 0.870885
2017-12-10T15:49:11.128274: step 2512, loss 0.238511, acc 0.9375, prec 0.0583219, recall 0.87094
2017-12-10T15:49:11.323120: step 2513, loss 0.134986, acc 0.984375, prec 0.0583472, recall 0.870995
2017-12-10T15:49:11.523580: step 2514, loss 0.132373, acc 0.96875, prec 0.0583438, recall 0.870995
2017-12-10T15:49:11.725862: step 2515, loss 0.0691241, acc 0.984375, prec 0.0583691, recall 0.87105
2017-12-10T15:49:11.923608: step 2516, loss 0.17151, acc 0.96875, prec 0.0583927, recall 0.871105
2017-12-10T15:49:12.124368: step 2517, loss 0.0321433, acc 1, prec 0.0584196, recall 0.87116
2017-12-10T15:49:12.324157: step 2518, loss 0.286231, acc 0.90625, prec 0.0584096, recall 0.87116
2017-12-10T15:49:12.525055: step 2519, loss 0.129438, acc 0.96875, prec 0.0584332, recall 0.871215
2017-12-10T15:49:12.721476: step 2520, loss 0.170716, acc 0.921875, prec 0.0584518, recall 0.87127
2017-12-10T15:49:12.919820: step 2521, loss 0.187972, acc 0.953125, prec 0.0584737, recall 0.871325
2017-12-10T15:49:13.120386: step 2522, loss 0.136337, acc 0.984375, prec 0.0584989, recall 0.87138
2017-12-10T15:49:13.317500: step 2523, loss 1.83556, acc 0.9375, prec 0.0584939, recall 0.871009
2017-12-10T15:49:13.516513: step 2524, loss 0.211532, acc 0.953125, prec 0.0585427, recall 0.871119
2017-12-10T15:49:13.713180: step 2525, loss 0.110103, acc 0.96875, prec 0.0585663, recall 0.871173
2017-12-10T15:49:13.911982: step 2526, loss 0.177527, acc 0.890625, prec 0.0585546, recall 0.871173
2017-12-10T15:49:14.115052: step 2527, loss 0.172363, acc 0.96875, prec 0.0585781, recall 0.871228
2017-12-10T15:49:14.321815: step 2528, loss 0.195322, acc 0.9375, prec 0.0585983, recall 0.871283
2017-12-10T15:49:14.524280: step 2529, loss 0.219409, acc 0.890625, prec 0.0585866, recall 0.871283
2017-12-10T15:49:14.717754: step 2530, loss 0.0833921, acc 0.96875, prec 0.0586102, recall 0.871338
2017-12-10T15:49:14.912904: step 2531, loss 0.116701, acc 0.984375, prec 0.0586623, recall 0.871447
2017-12-10T15:49:15.109388: step 2532, loss 3.13731, acc 0.890625, prec 0.0586791, recall 0.871132
2017-12-10T15:49:15.309101: step 2533, loss 0.205848, acc 0.921875, prec 0.0586707, recall 0.871132
2017-12-10T15:49:15.507540: step 2534, loss 0.304056, acc 0.890625, prec 0.058659, recall 0.871132
2017-12-10T15:49:15.699461: step 2535, loss 0.28762, acc 0.890625, prec 0.058701, recall 0.871241
2017-12-10T15:49:15.896121: step 2536, loss 0.175077, acc 0.921875, prec 0.0587195, recall 0.871296
2017-12-10T15:49:16.092282: step 2537, loss 0.340713, acc 0.921875, prec 0.0587379, recall 0.87135
2017-12-10T15:49:16.285942: step 2538, loss 0.173642, acc 0.9375, prec 0.0587312, recall 0.87135
2017-12-10T15:49:16.484845: step 2539, loss 0.262546, acc 0.859375, prec 0.058743, recall 0.871404
2017-12-10T15:49:16.680927: step 2540, loss 0.287492, acc 0.90625, prec 0.058733, recall 0.871404
2017-12-10T15:49:16.881104: step 2541, loss 0.156722, acc 0.921875, prec 0.0587514, recall 0.871459
2017-12-10T15:49:17.079710: step 2542, loss 0.136442, acc 0.953125, prec 0.0587464, recall 0.871459
2017-12-10T15:49:17.281627: step 2543, loss 0.48008, acc 0.859375, prec 0.0587313, recall 0.871459
2017-12-10T15:49:17.479739: step 2544, loss 0.311825, acc 0.890625, prec 0.0587464, recall 0.871513
2017-12-10T15:49:17.679974: step 2545, loss 0.512992, acc 0.859375, prec 0.058785, recall 0.871622
2017-12-10T15:49:17.883279: step 2546, loss 0.335037, acc 0.9375, prec 0.0588319, recall 0.87173
2017-12-10T15:49:18.093491: step 2547, loss 0.0687243, acc 0.96875, prec 0.0588286, recall 0.87173
2017-12-10T15:49:18.311533: step 2548, loss 1.0149, acc 0.9375, prec 0.0588754, recall 0.871838
2017-12-10T15:49:18.511014: step 2549, loss 0.157817, acc 0.921875, prec 0.0588939, recall 0.871892
2017-12-10T15:49:18.707666: step 2550, loss 0.124583, acc 0.953125, prec 0.0589156, recall 0.871946
2017-12-10T15:49:18.907885: step 2551, loss 0.127149, acc 0.953125, prec 0.0589374, recall 0.872
2017-12-10T15:49:19.118966: step 2552, loss 0.161327, acc 0.96875, prec 0.0589608, recall 0.872054
2017-12-10T15:49:19.319195: step 2553, loss 0.246141, acc 0.9375, prec 0.0589809, recall 0.872108
2017-12-10T15:49:19.515520: step 2554, loss 0.264189, acc 0.890625, prec 0.0589959, recall 0.872162
2017-12-10T15:49:19.715736: step 2555, loss 0.232865, acc 0.921875, prec 0.0589875, recall 0.872162
2017-12-10T15:49:19.914749: step 2556, loss 0.171667, acc 0.921875, prec 0.0589791, recall 0.872162
2017-12-10T15:49:20.109457: step 2557, loss 0.286083, acc 0.921875, prec 0.0589975, recall 0.872215
2017-12-10T15:49:20.303914: step 2558, loss 0.151409, acc 0.9375, prec 0.0590175, recall 0.872269
2017-12-10T15:49:20.503640: step 2559, loss 0.210417, acc 0.90625, prec 0.0590342, recall 0.872323
2017-12-10T15:49:20.731330: step 2560, loss 0.22381, acc 0.921875, prec 0.0590258, recall 0.872323
2017-12-10T15:49:20.941510: step 2561, loss 0.0300528, acc 0.984375, prec 0.0590241, recall 0.872323
2017-12-10T15:49:21.138542: step 2562, loss 0.115261, acc 0.96875, prec 0.0590475, recall 0.872376
2017-12-10T15:49:21.336796: step 2563, loss 0.324444, acc 0.9375, prec 0.0590943, recall 0.872483
2017-12-10T15:49:21.535708: step 2564, loss 0.716117, acc 0.9375, prec 0.0591143, recall 0.872537
2017-12-10T15:49:21.736015: step 2565, loss 0.234444, acc 0.921875, prec 0.0591326, recall 0.87259
2017-12-10T15:49:21.931458: step 2566, loss 0.231195, acc 0.921875, prec 0.0591776, recall 0.872697
2017-12-10T15:49:22.133393: step 2567, loss 0.107882, acc 0.96875, prec 0.059201, recall 0.87275
2017-12-10T15:49:22.336577: step 2568, loss 2.1445, acc 0.9375, prec 0.059196, recall 0.872385
2017-12-10T15:49:22.538559: step 2569, loss 0.0583789, acc 0.984375, prec 0.0592477, recall 0.872492
2017-12-10T15:49:22.736316: step 2570, loss 0.0849755, acc 0.953125, prec 0.0592694, recall 0.872545
2017-12-10T15:49:22.934299: step 2571, loss 0.0569652, acc 0.96875, prec 0.059266, recall 0.872545
2017-12-10T15:49:23.134196: step 2572, loss 0.173377, acc 0.9375, prec 0.0592593, recall 0.872545
2017-12-10T15:49:23.330635: step 2573, loss 0.0636459, acc 0.96875, prec 0.0592826, recall 0.872598
2017-12-10T15:49:23.526325: step 2574, loss 1.77382, acc 0.921875, prec 0.0593026, recall 0.872287
2017-12-10T15:49:23.724171: step 2575, loss 0.268894, acc 0.90625, prec 0.0593191, recall 0.87234
2017-12-10T15:49:23.921936: step 2576, loss 0.111212, acc 0.96875, prec 0.0593158, recall 0.87234
2017-12-10T15:49:24.119154: step 2577, loss 0.298659, acc 0.875, prec 0.059329, recall 0.872394
2017-12-10T15:49:24.317577: step 2578, loss 0.236741, acc 0.921875, prec 0.0593206, recall 0.872394
2017-12-10T15:49:24.509828: step 2579, loss 0.217245, acc 0.921875, prec 0.0593389, recall 0.872447
2017-12-10T15:49:24.709852: step 2580, loss 0.196258, acc 0.921875, prec 0.0593838, recall 0.872553
2017-12-10T15:49:24.900639: step 2581, loss 0.283747, acc 0.859375, prec 0.0593953, recall 0.872606
2017-12-10T15:49:25.097896: step 2582, loss 0.276949, acc 0.921875, prec 0.0593869, recall 0.872606
2017-12-10T15:49:25.297708: step 2583, loss 2.25566, acc 0.890625, prec 0.0594301, recall 0.872349
2017-12-10T15:49:25.499912: step 2584, loss 0.353058, acc 0.90625, prec 0.0594732, recall 0.872455
2017-12-10T15:49:25.697956: step 2585, loss 0.284215, acc 0.828125, prec 0.059508, recall 0.872561
2017-12-10T15:49:25.896395: step 2586, loss 0.536972, acc 0.859375, prec 0.0595194, recall 0.872614
2017-12-10T15:49:26.092939: step 2587, loss 0.354539, acc 0.875, prec 0.0595326, recall 0.872667
2017-12-10T15:49:26.290347: step 2588, loss 0.533306, acc 0.8125, prec 0.059539, recall 0.87272
2017-12-10T15:49:26.482958: step 2589, loss 0.287505, acc 0.859375, prec 0.0595504, recall 0.872772
2017-12-10T15:49:26.672809: step 2590, loss 0.493034, acc 0.875, prec 0.0595369, recall 0.872772
2017-12-10T15:49:26.869077: step 2591, loss 0.268129, acc 0.875, prec 0.0595235, recall 0.872772
2017-12-10T15:49:27.071041: step 2592, loss 0.576845, acc 0.84375, prec 0.0595598, recall 0.872878
2017-12-10T15:49:27.269026: step 2593, loss 0.481612, acc 0.8125, prec 0.0596193, recall 0.873036
2017-12-10T15:49:27.463678: step 2594, loss 0.320979, acc 0.921875, prec 0.0596109, recall 0.873036
2017-12-10T15:49:27.665794: step 2595, loss 0.437035, acc 0.890625, prec 0.0596256, recall 0.873088
2017-12-10T15:49:27.863561: step 2596, loss 0.254093, acc 0.890625, prec 0.0596404, recall 0.873141
2017-12-10T15:49:28.060773: step 2597, loss 0.415348, acc 0.890625, prec 0.0596552, recall 0.873193
2017-12-10T15:49:28.265275: step 2598, loss 0.183454, acc 0.9375, prec 0.059675, recall 0.873245
2017-12-10T15:49:28.465442: step 2599, loss 0.248073, acc 0.859375, prec 0.0596598, recall 0.873245
2017-12-10T15:49:28.662426: step 2600, loss 0.634892, acc 0.9375, prec 0.0598122, recall 0.873558
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_3/1512938428/checkpoints/model-2600

2017-12-10T15:49:29.960672: step 2601, loss 0.167864, acc 0.953125, prec 0.0598071, recall 0.873558
2017-12-10T15:49:30.155034: step 2602, loss 0.351051, acc 0.90625, prec 0.059797, recall 0.873558
2017-12-10T15:49:30.352603: step 2603, loss 0.167667, acc 0.953125, prec 0.059792, recall 0.873558
2017-12-10T15:49:30.549096: step 2604, loss 0.0507637, acc 0.96875, prec 0.0597886, recall 0.873558
2017-12-10T15:49:30.751463: step 2605, loss 0.168318, acc 0.9375, prec 0.0598083, recall 0.873611
2017-12-10T15:49:30.950561: step 2606, loss 0.0821485, acc 0.953125, prec 0.0598033, recall 0.873611
2017-12-10T15:49:31.155301: step 2607, loss 0.0774423, acc 0.96875, prec 0.0597999, recall 0.873611
2017-12-10T15:49:31.358766: step 2608, loss 0.179825, acc 0.953125, prec 0.0597949, recall 0.873611
2017-12-10T15:49:31.569894: step 2609, loss 0.915931, acc 0.984375, prec 0.0598197, recall 0.873663
2017-12-10T15:49:31.770464: step 2610, loss 0.0588209, acc 0.984375, prec 0.059871, recall 0.873766
2017-12-10T15:49:31.973363: step 2611, loss 3.11328, acc 0.96875, prec 0.0598958, recall 0.873459
2017-12-10T15:49:32.174066: step 2612, loss 0.463851, acc 0.953125, prec 0.0599437, recall 0.873563
2017-12-10T15:49:32.376766: step 2613, loss 0.074807, acc 0.984375, prec 0.059942, recall 0.873563
2017-12-10T15:49:32.579317: step 2614, loss 0.164282, acc 0.9375, prec 0.0599352, recall 0.873563
2017-12-10T15:49:32.777436: step 2615, loss 0.170692, acc 0.953125, prec 0.0599566, recall 0.873615
2017-12-10T15:49:32.975633: step 2616, loss 0.408527, acc 0.890625, prec 0.0599977, recall 0.873719
2017-12-10T15:49:33.171883: step 2617, loss 0.274849, acc 0.890625, prec 0.0599859, recall 0.873719
2017-12-10T15:49:33.370397: step 2618, loss 0.486849, acc 0.875, prec 0.0599724, recall 0.873719
2017-12-10T15:49:33.566699: step 2619, loss 0.254466, acc 0.90625, prec 0.0599623, recall 0.873719
2017-12-10T15:49:33.757875: step 2620, loss 0.447077, acc 0.8125, prec 0.0599685, recall 0.87377
2017-12-10T15:49:33.952363: step 2621, loss 0.399654, acc 0.90625, prec 0.0599584, recall 0.87377
2017-12-10T15:49:34.155476: step 2622, loss 0.127866, acc 0.953125, prec 0.0599798, recall 0.873822
2017-12-10T15:49:34.350667: step 2623, loss 0.471268, acc 0.828125, prec 0.0599612, recall 0.873822
2017-12-10T15:49:34.549084: step 2624, loss 0.441512, acc 0.859375, prec 0.0599989, recall 0.873926
2017-12-10T15:49:34.746005: step 2625, loss 0.47426, acc 0.875, prec 0.0600382, recall 0.874029
2017-12-10T15:49:34.941722: step 2626, loss 0.315329, acc 0.875, prec 0.0600511, recall 0.87408
2017-12-10T15:49:35.135879: step 2627, loss 0.190643, acc 0.953125, prec 0.0600989, recall 0.874183
2017-12-10T15:49:35.328133: step 2628, loss 0.414857, acc 0.859375, prec 0.0601101, recall 0.874234
2017-12-10T15:49:35.532388: step 2629, loss 0.197483, acc 0.9375, prec 0.0601561, recall 0.874337
2017-12-10T15:49:35.729218: step 2630, loss 0.186286, acc 0.9375, prec 0.0601493, recall 0.874337
2017-12-10T15:49:35.926829: step 2631, loss 0.204097, acc 0.9375, prec 0.0601426, recall 0.874337
2017-12-10T15:49:36.125120: step 2632, loss 0.385538, acc 0.875, prec 0.0601554, recall 0.874388
2017-12-10T15:49:36.320129: step 2633, loss 0.173259, acc 0.921875, prec 0.0601997, recall 0.874491
2017-12-10T15:49:36.522502: step 2634, loss 0.115689, acc 0.96875, prec 0.0602227, recall 0.874542
2017-12-10T15:49:36.722321: step 2635, loss 0.340951, acc 0.921875, prec 0.0602406, recall 0.874593
2017-12-10T15:49:36.923413: step 2636, loss 0.198938, acc 0.921875, prec 0.0602585, recall 0.874644
2017-12-10T15:49:37.120638: step 2637, loss 0.120337, acc 0.953125, prec 0.0602535, recall 0.874644
2017-12-10T15:49:37.317859: step 2638, loss 0.0575414, acc 0.984375, prec 0.0602518, recall 0.874644
2017-12-10T15:49:37.515916: step 2639, loss 0.25061, acc 1, prec 0.0602781, recall 0.874695
2017-12-10T15:49:37.720611: step 2640, loss 0.149131, acc 0.9375, prec 0.0602977, recall 0.874746
2017-12-10T15:49:37.917116: step 2641, loss 0.128907, acc 0.96875, prec 0.0603207, recall 0.874797
2017-12-10T15:49:38.116729: step 2642, loss 0.16504, acc 0.96875, prec 0.0603436, recall 0.874848
2017-12-10T15:49:38.318898: step 2643, loss 0.208187, acc 0.9375, prec 0.0603632, recall 0.874898
2017-12-10T15:49:38.515324: step 2644, loss 0.267209, acc 0.921875, prec 0.0603547, recall 0.874898
2017-12-10T15:49:38.711314: step 2645, loss 0.253422, acc 0.921875, prec 0.0603726, recall 0.874949
2017-12-10T15:49:38.906822: step 2646, loss 0.214099, acc 0.9375, prec 0.0603658, recall 0.874949
2017-12-10T15:49:39.106746: step 2647, loss 0.1548, acc 0.953125, prec 0.0603871, recall 0.875
2017-12-10T15:49:39.309963: step 2648, loss 0.0263753, acc 1, prec 0.0604134, recall 0.875051
2017-12-10T15:49:39.512758: step 2649, loss 0.47221, acc 0.921875, prec 0.0604313, recall 0.875101
2017-12-10T15:49:39.714599: step 2650, loss 0.158351, acc 0.984375, prec 0.0604559, recall 0.875152
2017-12-10T15:49:39.915032: step 2651, loss 0.0510432, acc 0.96875, prec 0.0604525, recall 0.875152
2017-12-10T15:49:40.113003: step 2652, loss 0.332511, acc 0.96875, prec 0.0605017, recall 0.875253
2017-12-10T15:49:40.315610: step 2653, loss 0.0354319, acc 0.984375, prec 0.0605, recall 0.875253
2017-12-10T15:49:40.518170: step 2654, loss 0.0328629, acc 0.984375, prec 0.0605246, recall 0.875304
2017-12-10T15:49:40.722350: step 2655, loss 1.06647, acc 0.953125, prec 0.0605458, recall 0.875354
2017-12-10T15:49:40.920943: step 2656, loss 0.266207, acc 0.96875, prec 0.0605687, recall 0.875405
2017-12-10T15:49:41.117967: step 2657, loss 0.3181, acc 0.9375, prec 0.0606145, recall 0.875505
2017-12-10T15:49:41.311490: step 2658, loss 0.0890814, acc 0.984375, prec 0.0606391, recall 0.875556
2017-12-10T15:49:41.520010: step 2659, loss 0.222465, acc 0.9375, prec 0.0606586, recall 0.875606
2017-12-10T15:49:41.718281: step 2660, loss 0.13286, acc 0.953125, prec 0.0606535, recall 0.875606
2017-12-10T15:49:41.917766: step 2661, loss 0.0590787, acc 0.96875, prec 0.0606764, recall 0.875656
2017-12-10T15:49:42.122943: step 2662, loss 0.561018, acc 0.984375, prec 0.0607273, recall 0.875756
2017-12-10T15:49:42.326967: step 2663, loss 0.19431, acc 0.9375, prec 0.0607205, recall 0.875756
2017-12-10T15:49:42.526925: step 2664, loss 0.140468, acc 0.96875, prec 0.0607171, recall 0.875756
2017-12-10T15:49:42.724315: step 2665, loss 0.305468, acc 0.96875, prec 0.06074, recall 0.875806
2017-12-10T15:49:42.950199: step 2666, loss 0.192603, acc 0.953125, prec 0.0607349, recall 0.875806
2017-12-10T15:49:43.151360: step 2667, loss 0.490843, acc 0.921875, prec 0.0607789, recall 0.875907
2017-12-10T15:49:43.353973: step 2668, loss 0.413206, acc 0.96875, prec 0.0608017, recall 0.875956
2017-12-10T15:49:43.549444: step 2669, loss 0.0316513, acc 0.984375, prec 0.0608, recall 0.875956
2017-12-10T15:49:43.749242: step 2670, loss 0.142082, acc 0.96875, prec 0.0608491, recall 0.876056
2017-12-10T15:49:43.947515: step 2671, loss 0.126044, acc 0.9375, prec 0.0608423, recall 0.876056
2017-12-10T15:49:44.155499: step 2672, loss 0.181845, acc 0.9375, prec 0.0608618, recall 0.876106
2017-12-10T15:49:44.358094: step 2673, loss 0.0834487, acc 0.984375, prec 0.0608863, recall 0.876156
2017-12-10T15:49:44.558128: step 2674, loss 0.147902, acc 0.953125, prec 0.0609337, recall 0.876256
2017-12-10T15:49:44.757074: step 2675, loss 0.501191, acc 0.9375, prec 0.0609531, recall 0.876305
2017-12-10T15:49:44.953410: step 2676, loss 0.118615, acc 0.953125, prec 0.0609742, recall 0.876355
2017-12-10T15:49:45.160111: step 2677, loss 0.106085, acc 0.96875, prec 0.0609971, recall 0.876405
2017-12-10T15:49:45.356654: step 2678, loss 0.207167, acc 0.953125, prec 0.0610182, recall 0.876454
2017-12-10T15:49:45.553590: step 2679, loss 0.361043, acc 0.9375, prec 0.0610638, recall 0.876553
2017-12-10T15:49:45.748423: step 2680, loss 0.169077, acc 0.921875, prec 0.0610553, recall 0.876553
2017-12-10T15:49:45.946525: step 2681, loss 0.650343, acc 0.9375, prec 0.0610747, recall 0.876603
2017-12-10T15:49:46.145548: step 2682, loss 0.152607, acc 0.953125, prec 0.0610958, recall 0.876652
2017-12-10T15:49:46.347185: step 2683, loss 2.26123, acc 0.890625, prec 0.0611886, recall 0.876849
2017-12-10T15:49:46.546690: step 2684, loss 0.293673, acc 0.921875, prec 0.0611801, recall 0.876849
2017-12-10T15:49:46.746816: step 2685, loss 0.242394, acc 0.921875, prec 0.0611977, recall 0.876898
2017-12-10T15:49:46.944048: step 2686, loss 0.255908, acc 0.9375, prec 0.0612695, recall 0.877046
2017-12-10T15:49:47.138929: step 2687, loss 0.409665, acc 0.90625, prec 0.0612854, recall 0.877095
2017-12-10T15:49:47.340928: step 2688, loss 0.270712, acc 0.90625, prec 0.0612751, recall 0.877095
2017-12-10T15:49:47.537848: step 2689, loss 0.148606, acc 0.9375, prec 0.0613206, recall 0.877193
2017-12-10T15:49:47.737090: step 2690, loss 0.361332, acc 0.859375, prec 0.0613052, recall 0.877193
2017-12-10T15:49:47.940810: step 2691, loss 0.208165, acc 0.890625, prec 0.0613194, recall 0.877242
2017-12-10T15:49:48.138672: step 2692, loss 0.573058, acc 0.859375, prec 0.0613302, recall 0.877291
2017-12-10T15:49:48.338813: step 2693, loss 0.376099, acc 0.890625, prec 0.0613183, recall 0.877291
2017-12-10T15:49:48.541049: step 2694, loss 0.349387, acc 0.875, prec 0.0613569, recall 0.877389
2017-12-10T15:49:48.739053: step 2695, loss 0.575356, acc 0.8125, prec 0.0613625, recall 0.877437
2017-12-10T15:49:48.933615: step 2696, loss 0.308493, acc 0.90625, prec 0.0614045, recall 0.877535
2017-12-10T15:49:49.132511: step 2697, loss 0.249422, acc 0.859375, prec 0.0614152, recall 0.877583
2017-12-10T15:49:49.330802: step 2698, loss 0.194731, acc 0.9375, prec 0.0614345, recall 0.877632
2017-12-10T15:49:49.527029: step 2699, loss 0.577885, acc 0.921875, prec 0.061452, recall 0.877681
2017-12-10T15:49:49.724149: step 2700, loss 0.372468, acc 0.828125, prec 0.0614333, recall 0.877681
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_3/1512938428/checkpoints/model-2700

2017-12-10T15:49:50.863211: step 2701, loss 0.389256, acc 0.875, prec 0.0614196, recall 0.877681
2017-12-10T15:49:51.058435: step 2702, loss 0.206022, acc 0.90625, prec 0.0614615, recall 0.877778
2017-12-10T15:49:51.255619: step 2703, loss 0.101502, acc 0.96875, prec 0.0615102, recall 0.877875
2017-12-10T15:49:51.455183: step 2704, loss 0.218257, acc 0.96875, prec 0.0615329, recall 0.877923
2017-12-10T15:49:51.652903: step 2705, loss 0.116668, acc 0.96875, prec 0.0615295, recall 0.877923
2017-12-10T15:49:51.849756: step 2706, loss 0.276429, acc 0.921875, prec 0.061547, recall 0.877971
2017-12-10T15:49:52.044020: step 2707, loss 0.390915, acc 0.96875, prec 0.0615957, recall 0.878068
2017-12-10T15:49:52.239874: step 2708, loss 0.208911, acc 0.9375, prec 0.0615889, recall 0.878068
2017-12-10T15:49:52.448741: step 2709, loss 0.0938153, acc 0.984375, prec 0.0615872, recall 0.878068
2017-12-10T15:49:52.648109: step 2710, loss 0.210918, acc 0.90625, prec 0.0616551, recall 0.878213
2017-12-10T15:49:52.842481: step 2711, loss 0.136543, acc 0.953125, prec 0.061702, recall 0.878309
2017-12-10T15:49:53.044360: step 2712, loss 0.158676, acc 0.96875, prec 0.0617246, recall 0.878357
2017-12-10T15:49:53.243410: step 2713, loss 0.0171439, acc 1, prec 0.0617246, recall 0.878357
2017-12-10T15:49:53.442000: step 2714, loss 0.127986, acc 0.953125, prec 0.0617455, recall 0.878405
2017-12-10T15:49:53.639424: step 2715, loss 0.461917, acc 0.953125, prec 0.0617664, recall 0.878453
2017-12-10T15:49:53.842799: step 2716, loss 0.109503, acc 1, prec 0.0618445, recall 0.878597
2017-12-10T15:49:54.036843: step 2717, loss 0.098793, acc 0.96875, prec 0.0618411, recall 0.878597
2017-12-10T15:49:54.232300: step 2718, loss 0.0632888, acc 0.96875, prec 0.0618377, recall 0.878597
2017-12-10T15:49:54.427863: step 2719, loss 0.0577108, acc 0.984375, prec 0.0618359, recall 0.878597
2017-12-10T15:49:54.625693: step 2720, loss 0.362354, acc 1, prec 0.061862, recall 0.878645
2017-12-10T15:49:54.825608: step 2721, loss 0.141315, acc 0.9375, prec 0.0618811, recall 0.878692
2017-12-10T15:49:55.020650: step 2722, loss 0.171255, acc 0.921875, prec 0.0618725, recall 0.878692
2017-12-10T15:49:55.220706: step 2723, loss 0.0788721, acc 0.984375, prec 0.0618708, recall 0.878692
2017-12-10T15:49:55.417268: step 2724, loss 1.20037, acc 1, prec 0.0619489, recall 0.878836
2017-12-10T15:49:55.618815: step 2725, loss 0.152012, acc 0.953125, prec 0.0619437, recall 0.878836
2017-12-10T15:49:55.816692: step 2726, loss 0.0623035, acc 0.96875, prec 0.0619403, recall 0.878836
2017-12-10T15:49:56.016183: step 2727, loss 0.171053, acc 0.96875, prec 0.0619629, recall 0.878883
2017-12-10T15:49:56.214164: step 2728, loss 0.174934, acc 0.984375, prec 0.0619871, recall 0.878931
2017-12-10T15:49:56.411775: step 2729, loss 0.183769, acc 0.9375, prec 0.0619803, recall 0.878931
2017-12-10T15:49:56.609530: step 2730, loss 0.25071, acc 0.90625, prec 0.061996, recall 0.878978
2017-12-10T15:49:56.804983: step 2731, loss 0.726558, acc 0.953125, prec 0.0620168, recall 0.879026
2017-12-10T15:49:57.005205: step 2732, loss 0.639819, acc 0.90625, prec 0.0620585, recall 0.879121
2017-12-10T15:49:57.229093: step 2733, loss 0.338983, acc 0.890625, prec 0.0620464, recall 0.879121
2017-12-10T15:49:57.431987: step 2734, loss 0.176663, acc 0.9375, prec 0.0620396, recall 0.879121
2017-12-10T15:49:57.634336: step 2735, loss 0.117041, acc 0.96875, prec 0.0620361, recall 0.879121
2017-12-10T15:49:57.829059: step 2736, loss 0.361902, acc 0.953125, prec 0.0620829, recall 0.879216
2017-12-10T15:49:58.033062: step 2737, loss 0.230228, acc 0.9375, prec 0.062102, recall 0.879263
2017-12-10T15:49:58.231761: step 2738, loss 0.440091, acc 0.921875, prec 0.0621453, recall 0.879358
2017-12-10T15:49:58.428820: step 2739, loss 0.375376, acc 0.859375, prec 0.0621299, recall 0.879358
2017-12-10T15:49:58.626325: step 2740, loss 0.253611, acc 0.953125, prec 0.0621506, recall 0.879405
2017-12-10T15:49:58.823542: step 2741, loss 0.295213, acc 0.890625, prec 0.0621386, recall 0.879405
2017-12-10T15:49:59.022099: step 2742, loss 0.335593, acc 0.90625, prec 0.0621542, recall 0.879452
2017-12-10T15:49:59.221906: step 2743, loss 0.23212, acc 0.921875, prec 0.0621456, recall 0.879452
2017-12-10T15:49:59.425341: step 2744, loss 0.204562, acc 0.890625, prec 0.0621595, recall 0.879499
2017-12-10T15:49:59.626299: step 2745, loss 0.116843, acc 0.9375, prec 0.0621527, recall 0.879499
2017-12-10T15:49:59.825448: step 2746, loss 0.290205, acc 0.921875, prec 0.0621959, recall 0.879593
2017-12-10T15:50:00.020322: step 2747, loss 0.128103, acc 0.921875, prec 0.0621873, recall 0.879593
2017-12-10T15:50:00.221208: step 2748, loss 0.0741896, acc 0.984375, prec 0.0622115, recall 0.87964
2017-12-10T15:50:00.426497: step 2749, loss 0.104556, acc 0.953125, prec 0.0622064, recall 0.87964
2017-12-10T15:50:00.636734: step 2750, loss 0.90538, acc 0.90625, prec 0.0622479, recall 0.879734
2017-12-10T15:50:00.841844: step 2751, loss 0.191957, acc 0.9375, prec 0.062241, recall 0.879734
2017-12-10T15:50:01.041833: step 2752, loss 0.291024, acc 0.953125, prec 0.0622618, recall 0.879781
2017-12-10T15:50:01.239466: step 2753, loss 0.222628, acc 0.90625, prec 0.0622514, recall 0.879781
2017-12-10T15:50:01.454826: step 2754, loss 0.218146, acc 0.984375, prec 0.0623274, recall 0.879922
2017-12-10T15:50:01.663077: step 2755, loss 0.29634, acc 0.953125, prec 0.0623481, recall 0.879969
2017-12-10T15:50:01.863073: step 2756, loss 0.100811, acc 0.953125, prec 0.062343, recall 0.879969
2017-12-10T15:50:02.065652: step 2757, loss 0.182515, acc 0.96875, prec 0.0623395, recall 0.879969
2017-12-10T15:50:02.273821: step 2758, loss 0.081525, acc 0.984375, prec 0.0623378, recall 0.879969
2017-12-10T15:50:02.471656: step 2759, loss 0.0465379, acc 0.984375, prec 0.0623361, recall 0.879969
2017-12-10T15:50:02.672478: step 2760, loss 0.157424, acc 0.984375, prec 0.0623861, recall 0.880062
2017-12-10T15:50:02.869039: step 2761, loss 0.151626, acc 0.953125, prec 0.062381, recall 0.880062
2017-12-10T15:50:03.069957: step 2762, loss 0.695417, acc 0.953125, prec 0.0624017, recall 0.880109
2017-12-10T15:50:03.272302: step 2763, loss 0.0661495, acc 0.953125, prec 0.0623965, recall 0.880109
2017-12-10T15:50:03.472014: step 2764, loss 0.232033, acc 0.9375, prec 0.0623896, recall 0.880109
2017-12-10T15:50:03.668014: step 2765, loss 0.276915, acc 0.984375, prec 0.0624138, recall 0.880156
2017-12-10T15:50:03.866686: step 2766, loss 0.502582, acc 0.953125, prec 0.0624603, recall 0.880249
2017-12-10T15:50:04.066692: step 2767, loss 0.782277, acc 0.953125, prec 0.0625328, recall 0.880388
2017-12-10T15:50:04.261157: step 2768, loss 0.0980142, acc 0.96875, prec 0.0625293, recall 0.880388
2017-12-10T15:50:04.464840: step 2769, loss 0.434282, acc 0.9375, prec 0.0625483, recall 0.880435
2017-12-10T15:50:04.661537: step 2770, loss 0.187358, acc 0.96875, prec 0.0625965, recall 0.880528
2017-12-10T15:50:04.860165: step 2771, loss 0.127911, acc 0.953125, prec 0.0626172, recall 0.880574
2017-12-10T15:50:05.065860: step 2772, loss 0.454536, acc 0.890625, prec 0.0626051, recall 0.880574
2017-12-10T15:50:05.266418: step 2773, loss 0.128082, acc 0.953125, prec 0.0626774, recall 0.880713
2017-12-10T15:50:05.464662: step 2774, loss 0.230859, acc 0.90625, prec 0.0626929, recall 0.880759
2017-12-10T15:50:05.659446: step 2775, loss 0.138406, acc 0.921875, prec 0.0626843, recall 0.880759
2017-12-10T15:50:05.860132: step 2776, loss 0.16375, acc 0.921875, prec 0.0626756, recall 0.880759
2017-12-10T15:50:06.057813: step 2777, loss 0.209799, acc 0.953125, prec 0.0626963, recall 0.880805
2017-12-10T15:50:06.250164: step 2778, loss 0.339235, acc 0.921875, prec 0.0627135, recall 0.880851
2017-12-10T15:50:06.445285: step 2779, loss 0.153245, acc 0.9375, prec 0.0627065, recall 0.880851
2017-12-10T15:50:06.643942: step 2780, loss 0.237965, acc 0.90625, prec 0.0627478, recall 0.880943
2017-12-10T15:50:06.839365: step 2781, loss 0.149652, acc 0.9375, prec 0.0627667, recall 0.880989
2017-12-10T15:50:07.035859: step 2782, loss 0.171192, acc 0.9375, prec 0.0628114, recall 0.881081
2017-12-10T15:50:07.232019: step 2783, loss 0.167114, acc 0.96875, prec 0.0628337, recall 0.881127
2017-12-10T15:50:07.427454: step 2784, loss 0.306139, acc 0.9375, prec 0.0628526, recall 0.881173
2017-12-10T15:50:07.628008: step 2785, loss 0.120109, acc 0.953125, prec 0.0628474, recall 0.881173
2017-12-10T15:50:07.830372: step 2786, loss 0.125706, acc 0.9375, prec 0.0628405, recall 0.881173
2017-12-10T15:50:08.029005: step 2787, loss 0.0301122, acc 1, prec 0.0628405, recall 0.881173
2017-12-10T15:50:08.225117: step 2788, loss 0.215734, acc 0.953125, prec 0.0628611, recall 0.881219
2017-12-10T15:50:08.423788: step 2789, loss 0.474366, acc 0.953125, prec 0.0629074, recall 0.88131
2017-12-10T15:50:08.627893: step 2790, loss 0.151436, acc 0.984375, prec 0.0629315, recall 0.881356
2017-12-10T15:50:08.825398: step 2791, loss 0.138169, acc 0.96875, prec 0.0629538, recall 0.881402
2017-12-10T15:50:09.028227: step 2792, loss 0.663028, acc 0.96875, prec 0.0629761, recall 0.881447
2017-12-10T15:50:09.231599: step 2793, loss 0.186764, acc 0.921875, prec 0.0629932, recall 0.881493
2017-12-10T15:50:09.434068: step 2794, loss 0.0647401, acc 0.984375, prec 0.0629915, recall 0.881493
2017-12-10T15:50:09.632447: step 2795, loss 0.0708079, acc 0.96875, prec 0.0630138, recall 0.881538
2017-12-10T15:50:09.830847: step 2796, loss 0.110117, acc 0.9375, prec 0.0630326, recall 0.881584
2017-12-10T15:50:10.026260: step 2797, loss 0.0796511, acc 0.984375, prec 0.0630309, recall 0.881584
2017-12-10T15:50:10.227655: step 2798, loss 0.172957, acc 0.921875, prec 0.0630222, recall 0.881584
2017-12-10T15:50:10.426690: step 2799, loss 0.905181, acc 0.984375, prec 0.063072, recall 0.881675
2017-12-10T15:50:10.630079: step 2800, loss 0.599076, acc 0.9375, prec 0.0631165, recall 0.881766
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_3/1512938428/checkpoints/model-2800

2017-12-10T15:50:11.829309: step 2801, loss 0.228338, acc 0.9375, prec 0.0631353, recall 0.881811
2017-12-10T15:50:12.024633: step 2802, loss 1.40217, acc 0.9375, prec 0.0631559, recall 0.881518
2017-12-10T15:50:12.223739: step 2803, loss 0.368874, acc 0.890625, prec 0.0631437, recall 0.881518
2017-12-10T15:50:12.426192: step 2804, loss 0.315878, acc 0.890625, prec 0.0631316, recall 0.881518
2017-12-10T15:50:12.627432: step 2805, loss 0.303171, acc 0.890625, prec 0.0631452, recall 0.881564
2017-12-10T15:50:12.824106: step 2806, loss 0.218519, acc 0.953125, prec 0.0631657, recall 0.881609
2017-12-10T15:50:13.024274: step 2807, loss 0.206197, acc 0.953125, prec 0.0631862, recall 0.881655
2017-12-10T15:50:13.223558: step 2808, loss 0.339893, acc 0.890625, prec 0.0632255, recall 0.881745
2017-12-10T15:50:13.421879: step 2809, loss 0.349719, acc 0.9375, prec 0.0632185, recall 0.881745
2017-12-10T15:50:13.616719: step 2810, loss 0.563111, acc 0.828125, prec 0.0631995, recall 0.881745
2017-12-10T15:50:13.818400: step 2811, loss 0.264223, acc 0.9375, prec 0.0631925, recall 0.881745
2017-12-10T15:50:14.018783: step 2812, loss 0.170863, acc 0.953125, prec 0.0631873, recall 0.881745
2017-12-10T15:50:14.219901: step 2813, loss 0.195494, acc 0.921875, prec 0.0631787, recall 0.881745
2017-12-10T15:50:14.423415: step 2814, loss 0.250407, acc 0.921875, prec 0.0631957, recall 0.88179
2017-12-10T15:50:14.619447: step 2815, loss 0.334077, acc 0.90625, prec 0.0631853, recall 0.88179
2017-12-10T15:50:14.820274: step 2816, loss 0.261429, acc 0.921875, prec 0.0631766, recall 0.88179
2017-12-10T15:50:15.016265: step 2817, loss 0.10499, acc 0.9375, prec 0.0632211, recall 0.881881
2017-12-10T15:50:15.216327: step 2818, loss 0.166303, acc 0.953125, prec 0.0632672, recall 0.881971
2017-12-10T15:50:15.414992: step 2819, loss 0.289467, acc 0.90625, prec 0.0632568, recall 0.881971
2017-12-10T15:50:15.615617: step 2820, loss 0.32622, acc 0.96875, prec 0.063279, recall 0.882016
2017-12-10T15:50:15.819824: step 2821, loss 0.287948, acc 0.96875, prec 0.0633269, recall 0.882106
2017-12-10T15:50:16.029373: step 2822, loss 0.23437, acc 0.921875, prec 0.0633182, recall 0.882106
2017-12-10T15:50:16.230344: step 2823, loss 0.143444, acc 0.9375, prec 0.0633112, recall 0.882106
2017-12-10T15:50:16.432086: step 2824, loss 0.311016, acc 0.984375, prec 0.0633352, recall 0.882151
2017-12-10T15:50:16.635580: step 2825, loss 0.0378887, acc 0.984375, prec 0.0633334, recall 0.882151
2017-12-10T15:50:16.845437: step 2826, loss 0.10135, acc 0.953125, prec 0.0633795, recall 0.882241
2017-12-10T15:50:17.045934: step 2827, loss 0.131103, acc 0.96875, prec 0.0634017, recall 0.882286
2017-12-10T15:50:17.243382: step 2828, loss 0.83053, acc 0.984375, prec 0.0635025, recall 0.882465
2017-12-10T15:50:17.450986: step 2829, loss 0.288919, acc 0.90625, prec 0.0635433, recall 0.882554
2017-12-10T15:50:17.648743: step 2830, loss 0.252714, acc 0.953125, prec 0.0635637, recall 0.882599
2017-12-10T15:50:17.848335: step 2831, loss 0.237493, acc 0.9375, prec 0.0635824, recall 0.882643
2017-12-10T15:50:18.042817: step 2832, loss 0.146163, acc 0.984375, prec 0.0636063, recall 0.882688
2017-12-10T15:50:18.272473: step 2833, loss 0.262916, acc 0.9375, prec 0.0636505, recall 0.882777
2017-12-10T15:50:18.472117: step 2834, loss 0.681936, acc 0.9375, prec 0.0636948, recall 0.882866
2017-12-10T15:50:18.676029: step 2835, loss 0.3624, acc 0.890625, prec 0.0637082, recall 0.88291
2017-12-10T15:50:18.868245: step 2836, loss 0.466076, acc 0.9375, prec 0.0637524, recall 0.882999
2017-12-10T15:50:19.064799: step 2837, loss 0.0584972, acc 0.984375, prec 0.0637763, recall 0.883043
2017-12-10T15:50:19.265606: step 2838, loss 0.372451, acc 0.90625, prec 0.063817, recall 0.883132
2017-12-10T15:50:19.465678: step 2839, loss 0.13932, acc 0.96875, prec 0.0638135, recall 0.883132
2017-12-10T15:50:19.661923: step 2840, loss 0.359564, acc 0.875, prec 0.0638251, recall 0.883176
2017-12-10T15:50:19.864537: step 2841, loss 0.452651, acc 0.890625, prec 0.0638129, recall 0.883176
2017-12-10T15:50:20.062690: step 2842, loss 0.118081, acc 0.953125, prec 0.0638077, recall 0.883176
2017-12-10T15:50:20.259176: step 2843, loss 0.404862, acc 0.9375, prec 0.0638519, recall 0.883264
2017-12-10T15:50:20.453227: step 2844, loss 0.10545, acc 0.921875, prec 0.0638432, recall 0.883264
2017-12-10T15:50:20.661022: step 2845, loss 0.234146, acc 0.90625, prec 0.0638327, recall 0.883264
2017-12-10T15:50:20.861069: step 2846, loss 0.193024, acc 0.90625, prec 0.0638222, recall 0.883264
2017-12-10T15:50:21.066036: step 2847, loss 0.172266, acc 0.96875, prec 0.0638699, recall 0.883352
2017-12-10T15:50:21.263364: step 2848, loss 0.0693389, acc 0.96875, prec 0.0638664, recall 0.883352
2017-12-10T15:50:21.459885: step 2849, loss 0.474443, acc 0.90625, prec 0.0638559, recall 0.883352
2017-12-10T15:50:21.659427: step 2850, loss 0.151639, acc 0.9375, prec 0.0638489, recall 0.883352
2017-12-10T15:50:21.856388: step 2851, loss 0.159018, acc 0.9375, prec 0.0638675, recall 0.883396
2017-12-10T15:50:22.054131: step 2852, loss 0.256227, acc 0.90625, prec 0.0638571, recall 0.883396
2017-12-10T15:50:22.249956: step 2853, loss 0.226353, acc 0.921875, prec 0.0638484, recall 0.883396
2017-12-10T15:50:22.449477: step 2854, loss 0.0493437, acc 1, prec 0.0638994, recall 0.883484
2017-12-10T15:50:22.651134: step 2855, loss 0.0884032, acc 0.96875, prec 0.0638959, recall 0.883484
2017-12-10T15:50:22.850759: step 2856, loss 0.650015, acc 1, prec 0.0639215, recall 0.883528
2017-12-10T15:50:23.049843: step 2857, loss 0.0289403, acc 0.984375, prec 0.0639197, recall 0.883528
2017-12-10T15:50:23.246646: step 2858, loss 0.0422196, acc 0.96875, prec 0.0639418, recall 0.883572
2017-12-10T15:50:23.440964: step 2859, loss 0.254945, acc 0.9375, prec 0.0639348, recall 0.883572
2017-12-10T15:50:23.640301: step 2860, loss 0.10242, acc 0.953125, prec 0.0639296, recall 0.883572
2017-12-10T15:50:23.837294: step 2861, loss 0.237629, acc 0.984375, prec 0.0639533, recall 0.883616
2017-12-10T15:50:24.038441: step 2862, loss 0.151763, acc 0.953125, prec 0.0639736, recall 0.88366
2017-12-10T15:50:24.236181: step 2863, loss 0.0819927, acc 0.953125, prec 0.0639684, recall 0.88366
2017-12-10T15:50:24.432210: step 2864, loss 0.27118, acc 0.921875, prec 0.0639852, recall 0.883703
2017-12-10T15:50:24.629356: step 2865, loss 0.20095, acc 0.9375, prec 0.0639782, recall 0.883703
2017-12-10T15:50:24.827698: step 2866, loss 0.218108, acc 0.953125, prec 0.063973, recall 0.883703
2017-12-10T15:50:25.025065: step 2867, loss 0.235807, acc 0.9375, prec 0.0639915, recall 0.883747
2017-12-10T15:50:25.219486: step 2868, loss 0.0745275, acc 0.984375, prec 0.0640153, recall 0.883791
2017-12-10T15:50:25.420008: step 2869, loss 0.0233526, acc 1, prec 0.0640408, recall 0.883835
2017-12-10T15:50:25.627406: step 2870, loss 0.190626, acc 0.9375, prec 0.0640338, recall 0.883835
2017-12-10T15:50:25.829406: step 2871, loss 0.476795, acc 0.96875, prec 0.0641068, recall 0.883965
2017-12-10T15:50:26.029708: step 2872, loss 0.0579506, acc 0.984375, prec 0.064105, recall 0.883965
2017-12-10T15:50:26.228076: step 2873, loss 0.0260896, acc 1, prec 0.0641305, recall 0.884009
2017-12-10T15:50:26.427961: step 2874, loss 0.100021, acc 0.9375, prec 0.0641235, recall 0.884009
2017-12-10T15:50:26.630572: step 2875, loss 1.526, acc 0.96875, prec 0.0641727, recall 0.883765
2017-12-10T15:50:26.831738: step 2876, loss 0.195691, acc 0.921875, prec 0.064164, recall 0.883765
2017-12-10T15:50:27.033258: step 2877, loss 1.07549, acc 0.9375, prec 0.0642589, recall 0.883939
2017-12-10T15:50:27.233313: step 2878, loss 0.204438, acc 0.90625, prec 0.0642993, recall 0.884025
2017-12-10T15:50:27.432007: step 2879, loss 1.5284, acc 0.90625, prec 0.0642906, recall 0.883695
2017-12-10T15:50:27.630975: step 2880, loss 0.397432, acc 0.90625, prec 0.0643055, recall 0.883738
2017-12-10T15:50:27.826013: step 2881, loss 0.189066, acc 0.9375, prec 0.0642985, recall 0.883738
2017-12-10T15:50:28.026752: step 2882, loss 0.447819, acc 0.84375, prec 0.0643065, recall 0.883782
2017-12-10T15:50:28.225568: step 2883, loss 0.514611, acc 0.890625, prec 0.0643451, recall 0.883869
2017-12-10T15:50:28.420783: step 2884, loss 0.564839, acc 0.796875, prec 0.0643224, recall 0.883869
2017-12-10T15:50:28.613425: step 2885, loss 0.221843, acc 0.9375, prec 0.0643408, recall 0.883912
2017-12-10T15:50:28.811130: step 2886, loss 0.499754, acc 0.84375, prec 0.0643996, recall 0.884042
2017-12-10T15:50:29.005216: step 2887, loss 0.418495, acc 0.890625, prec 0.0644128, recall 0.884085
2017-12-10T15:50:29.202561: step 2888, loss 0.522372, acc 0.8125, prec 0.0643918, recall 0.884085
2017-12-10T15:50:29.410480: step 2889, loss 0.481387, acc 0.84375, prec 0.0643997, recall 0.884128
2017-12-10T15:50:29.609654: step 2890, loss 0.25119, acc 0.90625, prec 0.0644146, recall 0.884171
2017-12-10T15:50:29.808780: step 2891, loss 0.865352, acc 0.78125, prec 0.0644409, recall 0.884258
2017-12-10T15:50:30.008158: step 2892, loss 0.670189, acc 0.8125, prec 0.0644706, recall 0.884344
2017-12-10T15:50:30.205049: step 2893, loss 0.408669, acc 0.875, prec 0.0644567, recall 0.884344
2017-12-10T15:50:30.402275: step 2894, loss 0.398871, acc 0.875, prec 0.064468, recall 0.884387
2017-12-10T15:50:30.599054: step 2895, loss 0.373603, acc 0.890625, prec 0.0644558, recall 0.884387
2017-12-10T15:50:30.794406: step 2896, loss 0.124948, acc 0.9375, prec 0.0644742, recall 0.88443
2017-12-10T15:50:30.993761: step 2897, loss 0.181314, acc 0.953125, prec 0.0644689, recall 0.88443
2017-12-10T15:50:31.193673: step 2898, loss 0.102731, acc 0.953125, prec 0.0644637, recall 0.88443
2017-12-10T15:50:31.392991: step 2899, loss 0.250084, acc 0.90625, prec 0.0644532, recall 0.88443
2017-12-10T15:50:31.597572: step 2900, loss 0.115959, acc 0.9375, prec 0.0644716, recall 0.884472
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_3/1512938428/checkpoints/model-2900

2017-12-10T15:50:32.763509: step 2901, loss 2.25325, acc 0.90625, prec 0.0644629, recall 0.884144
2017-12-10T15:50:32.962651: step 2902, loss 0.563111, acc 0.9375, prec 0.0644812, recall 0.884187
2017-12-10T15:50:33.162387: step 2903, loss 0.0551801, acc 0.984375, prec 0.0645048, recall 0.88423
2017-12-10T15:50:33.357519: step 2904, loss 0.228809, acc 0.90625, prec 0.0644943, recall 0.88423
2017-12-10T15:50:33.556148: step 2905, loss 0.42612, acc 0.890625, prec 0.0644821, recall 0.88423
2017-12-10T15:50:33.751293: step 2906, loss 0.328185, acc 0.90625, prec 0.0644969, recall 0.884273
2017-12-10T15:50:33.947111: step 2907, loss 0.0650412, acc 0.96875, prec 0.0645441, recall 0.884359
2017-12-10T15:50:34.143589: step 2908, loss 0.209173, acc 0.9375, prec 0.0645371, recall 0.884359
2017-12-10T15:50:34.339068: step 2909, loss 0.487656, acc 0.890625, prec 0.0645501, recall 0.884402
2017-12-10T15:50:34.533009: step 2910, loss 0.873258, acc 0.921875, prec 0.064592, recall 0.884487
2017-12-10T15:50:34.730571: step 2911, loss 0.249454, acc 0.90625, prec 0.0645815, recall 0.884487
2017-12-10T15:50:34.925863: step 2912, loss 0.367384, acc 0.921875, prec 0.0646487, recall 0.884615
2017-12-10T15:50:35.124969: step 2913, loss 0.271366, acc 0.890625, prec 0.0646617, recall 0.884658
2017-12-10T15:50:35.325597: step 2914, loss 0.28751, acc 0.890625, prec 0.0646747, recall 0.884701
2017-12-10T15:50:35.525167: step 2915, loss 0.279143, acc 0.9375, prec 0.064693, recall 0.884743
2017-12-10T15:50:35.723847: step 2916, loss 0.309869, acc 0.90625, prec 0.0646825, recall 0.884743
2017-12-10T15:50:35.924060: step 2917, loss 0.333606, acc 0.90625, prec 0.0646721, recall 0.884743
2017-12-10T15:50:36.119837: step 2918, loss 0.612375, acc 0.828125, prec 0.0646781, recall 0.884786
2017-12-10T15:50:36.317120: step 2919, loss 0.155401, acc 0.9375, prec 0.0646964, recall 0.884828
2017-12-10T15:50:36.519873: step 2920, loss 2.40559, acc 0.890625, prec 0.0646859, recall 0.884502
2017-12-10T15:50:36.719903: step 2921, loss 0.195635, acc 0.921875, prec 0.0646772, recall 0.884502
2017-12-10T15:50:36.920864: step 2922, loss 0.430837, acc 0.921875, prec 0.0646684, recall 0.884502
2017-12-10T15:50:37.116361: step 2923, loss 0.357221, acc 0.90625, prec 0.0647336, recall 0.88463
2017-12-10T15:50:37.309457: step 2924, loss 0.73045, acc 0.90625, prec 0.0647484, recall 0.884672
2017-12-10T15:50:37.503086: step 2925, loss 0.158243, acc 0.921875, prec 0.0647901, recall 0.884757
2017-12-10T15:50:37.701930: step 2926, loss 0.272252, acc 0.90625, prec 0.0648552, recall 0.884884
2017-12-10T15:50:37.897218: step 2927, loss 0.540104, acc 0.84375, prec 0.064863, recall 0.884926
2017-12-10T15:50:38.095208: step 2928, loss 1.2718, acc 0.890625, prec 0.0649011, recall 0.885011
2017-12-10T15:50:38.296859: step 2929, loss 0.379279, acc 0.84375, prec 0.0648836, recall 0.885011
2017-12-10T15:50:38.494145: step 2930, loss 0.354067, acc 0.921875, prec 0.0649253, recall 0.885095
2017-12-10T15:50:38.692952: step 2931, loss 0.187613, acc 0.921875, prec 0.0649165, recall 0.885095
2017-12-10T15:50:38.886759: step 2932, loss 4.71538, acc 0.8125, prec 0.0649225, recall 0.884813
2017-12-10T15:50:39.086137: step 2933, loss 0.384909, acc 0.859375, prec 0.0649319, recall 0.884855
2017-12-10T15:50:39.290950: step 2934, loss 0.483978, acc 0.8125, prec 0.064911, recall 0.884855
2017-12-10T15:50:39.486575: step 2935, loss 0.523604, acc 0.78125, prec 0.0648865, recall 0.884855
2017-12-10T15:50:39.687748: step 2936, loss 0.737515, acc 0.734375, prec 0.0648569, recall 0.884855
2017-12-10T15:50:39.885105: step 2937, loss 0.8031, acc 0.765625, prec 0.0648307, recall 0.884855
2017-12-10T15:50:40.096534: step 2938, loss 0.510303, acc 0.78125, prec 0.0648064, recall 0.884855
2017-12-10T15:50:40.298468: step 2939, loss 0.360821, acc 0.828125, prec 0.0648123, recall 0.884897
2017-12-10T15:50:40.495903: step 2940, loss 0.8131, acc 0.828125, prec 0.0647932, recall 0.884897
2017-12-10T15:50:40.694142: step 2941, loss 0.163833, acc 0.9375, prec 0.0647862, recall 0.884897
2017-12-10T15:50:40.883826: step 2942, loss 0.410182, acc 0.90625, prec 0.0648009, recall 0.88494
2017-12-10T15:50:41.094914: step 2943, loss 0.576861, acc 0.78125, prec 0.0648769, recall 0.885108
2017-12-10T15:50:41.293889: step 2944, loss 0.289909, acc 0.875, prec 0.064863, recall 0.885108
2017-12-10T15:50:41.493869: step 2945, loss 0.271319, acc 0.921875, prec 0.0648794, recall 0.88515
2017-12-10T15:50:41.688626: step 2946, loss 0.742104, acc 0.796875, prec 0.0648568, recall 0.88515
2017-12-10T15:50:41.886890: step 2947, loss 0.210259, acc 0.90625, prec 0.0648714, recall 0.885192
2017-12-10T15:50:42.084335: step 2948, loss 0.367275, acc 0.890625, prec 0.0648843, recall 0.885234
2017-12-10T15:50:42.277949: step 2949, loss 1.00935, acc 0.953125, prec 0.0649542, recall 0.88536
2017-12-10T15:50:42.498026: step 2950, loss 0.371647, acc 0.890625, prec 0.064942, recall 0.88536
2017-12-10T15:50:42.699764: step 2951, loss 1.33156, acc 0.921875, prec 0.0649584, recall 0.885401
2017-12-10T15:50:42.900293: step 2952, loss 0.291215, acc 0.90625, prec 0.064973, recall 0.885443
2017-12-10T15:50:43.096402: step 2953, loss 0.421983, acc 0.84375, prec 0.0650056, recall 0.885527
2017-12-10T15:50:43.292930: step 2954, loss 0.186341, acc 0.921875, prec 0.0649969, recall 0.885527
2017-12-10T15:50:43.492697: step 2955, loss 0.476985, acc 0.9375, prec 0.065015, recall 0.885568
2017-12-10T15:50:43.692392: step 2956, loss 0.341206, acc 0.921875, prec 0.0650063, recall 0.885568
2017-12-10T15:50:43.891641: step 2957, loss 0.468551, acc 0.984375, prec 0.0650796, recall 0.885693
2017-12-10T15:50:44.093658: step 2958, loss 4.09875, acc 0.953125, prec 0.0651011, recall 0.885413
2017-12-10T15:50:44.299412: step 2959, loss 0.780511, acc 0.921875, prec 0.0651924, recall 0.885579
2017-12-10T15:50:44.506506: step 2960, loss 0.200878, acc 0.921875, prec 0.0651837, recall 0.885579
2017-12-10T15:50:44.705705: step 2961, loss 0.145129, acc 0.953125, prec 0.0651785, recall 0.885579
2017-12-10T15:50:44.905607: step 2962, loss 0.349611, acc 0.90625, prec 0.065168, recall 0.885579
2017-12-10T15:50:45.108949: step 2963, loss 0.702513, acc 0.796875, prec 0.0651454, recall 0.885579
2017-12-10T15:50:45.304326: step 2964, loss 0.431013, acc 0.859375, prec 0.0651547, recall 0.885621
2017-12-10T15:50:45.499673: step 2965, loss 0.867686, acc 0.734375, prec 0.065175, recall 0.885704
2017-12-10T15:50:45.695984: step 2966, loss 0.46481, acc 0.859375, prec 0.0652093, recall 0.885787
2017-12-10T15:50:45.889478: step 2967, loss 0.432882, acc 0.859375, prec 0.0652435, recall 0.88587
2017-12-10T15:50:46.087197: step 2968, loss 0.485273, acc 0.78125, prec 0.0652191, recall 0.88587
2017-12-10T15:50:46.284467: step 2969, loss 0.68209, acc 0.78125, prec 0.0652197, recall 0.885911
2017-12-10T15:50:46.483234: step 2970, loss 0.326028, acc 0.90625, prec 0.0652591, recall 0.885993
2017-12-10T15:50:46.681145: step 2971, loss 0.71822, acc 0.875, prec 0.065295, recall 0.886076
2017-12-10T15:50:46.874547: step 2972, loss 0.54787, acc 0.859375, prec 0.0653043, recall 0.886117
2017-12-10T15:50:47.077471: step 2973, loss 0.448248, acc 0.859375, prec 0.0653135, recall 0.886158
2017-12-10T15:50:47.275440: step 2974, loss 0.50899, acc 0.859375, prec 0.0652979, recall 0.886158
2017-12-10T15:50:47.476884: step 2975, loss 0.285641, acc 0.90625, prec 0.0652874, recall 0.886158
2017-12-10T15:50:47.673461: step 2976, loss 0.236746, acc 0.9375, prec 0.0652805, recall 0.886158
2017-12-10T15:50:47.873764: step 2977, loss 0.200924, acc 0.90625, prec 0.0652701, recall 0.886158
2017-12-10T15:50:48.070581: step 2978, loss 0.145511, acc 0.953125, prec 0.0652897, recall 0.886199
2017-12-10T15:50:48.269833: step 2979, loss 2.15146, acc 0.890625, prec 0.065329, recall 0.885962
2017-12-10T15:50:48.470463: step 2980, loss 0.242136, acc 0.9375, prec 0.0654216, recall 0.886126
2017-12-10T15:50:48.664877: step 2981, loss 0.341287, acc 0.921875, prec 0.0654626, recall 0.886208
2017-12-10T15:50:48.844919: step 2982, loss 0.342099, acc 0.923077, prec 0.0654805, recall 0.886249
2017-12-10T15:50:49.059833: step 2983, loss 0.208099, acc 0.953125, prec 0.0655249, recall 0.886331
2017-12-10T15:50:49.258953: step 2984, loss 0.171848, acc 0.953125, prec 0.0655197, recall 0.886331
2017-12-10T15:50:49.462209: step 2985, loss 0.235755, acc 0.953125, prec 0.065589, recall 0.886453
2017-12-10T15:50:49.660831: step 2986, loss 0.112978, acc 0.96875, prec 0.0655855, recall 0.886453
2017-12-10T15:50:49.858729: step 2987, loss 0.249619, acc 0.875, prec 0.0655716, recall 0.886453
2017-12-10T15:50:50.059592: step 2988, loss 0.81287, acc 0.90625, prec 0.0656108, recall 0.886535
2017-12-10T15:50:50.260996: step 2989, loss 0.0390492, acc 0.984375, prec 0.0656091, recall 0.886535
2017-12-10T15:50:50.461925: step 2990, loss 0.233021, acc 0.921875, prec 0.0656003, recall 0.886535
2017-12-10T15:50:50.656574: step 2991, loss 0.372438, acc 0.921875, prec 0.0656413, recall 0.886616
2017-12-10T15:50:50.855357: step 2992, loss 0.16448, acc 0.9375, prec 0.0656343, recall 0.886616
2017-12-10T15:50:51.047770: step 2993, loss 0.171678, acc 0.953125, prec 0.0656539, recall 0.886657
2017-12-10T15:50:51.243458: step 2994, loss 0.141493, acc 0.953125, prec 0.0656735, recall 0.886698
2017-12-10T15:50:51.440639: step 2995, loss 0.119071, acc 0.953125, prec 0.0657179, recall 0.886779
2017-12-10T15:50:51.642933: step 2996, loss 0.358746, acc 0.921875, prec 0.0657091, recall 0.886779
2017-12-10T15:50:51.838691: step 2997, loss 0.168653, acc 0.921875, prec 0.0657004, recall 0.886779
2017-12-10T15:50:52.038287: step 2998, loss 0.833067, acc 0.984375, prec 0.0657235, recall 0.886819
2017-12-10T15:50:52.240897: step 2999, loss 0.125862, acc 0.90625, prec 0.065713, recall 0.886819
2017-12-10T15:50:52.443892: step 3000, loss 0.192115, acc 0.921875, prec 0.0657043, recall 0.886819
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_3/1512938428/checkpoints/model-3000

2017-12-10T15:50:53.800212: step 3001, loss 0.0790878, acc 0.96875, prec 0.0657008, recall 0.886819
2017-12-10T15:50:53.995184: step 3002, loss 0.12191, acc 0.96875, prec 0.0657221, recall 0.88686
2017-12-10T15:50:54.198246: step 3003, loss 0.28403, acc 0.90625, prec 0.0657364, recall 0.8869
2017-12-10T15:50:54.391471: step 3004, loss 0.117278, acc 0.96875, prec 0.0657825, recall 0.886981
2017-12-10T15:50:54.593203: step 3005, loss 0.382335, acc 0.890625, prec 0.0658198, recall 0.887062
2017-12-10T15:50:54.788124: step 3006, loss 0.287745, acc 0.953125, prec 0.0658394, recall 0.887103
2017-12-10T15:50:54.984848: step 3007, loss 0.186764, acc 0.953125, prec 0.0658341, recall 0.887103
2017-12-10T15:50:55.185778: step 3008, loss 0.16882, acc 0.96875, prec 0.0659049, recall 0.887223
2017-12-10T15:50:55.386723: step 3009, loss 0.0579798, acc 0.984375, prec 0.0659032, recall 0.887223
2017-12-10T15:50:55.583127: step 3010, loss 0.155331, acc 0.984375, prec 0.0659014, recall 0.887223
2017-12-10T15:50:55.780096: step 3011, loss 0.166315, acc 0.953125, prec 0.0658962, recall 0.887223
2017-12-10T15:50:55.979141: step 3012, loss 0.114782, acc 0.984375, prec 0.0658945, recall 0.887223
2017-12-10T15:50:56.176551: step 3013, loss 0.1302, acc 0.9375, prec 0.0659617, recall 0.887344
2017-12-10T15:50:56.369607: step 3014, loss 0.0529267, acc 1, prec 0.066036, recall 0.887464
2017-12-10T15:50:56.566077: step 3015, loss 0.125627, acc 1, prec 0.0660855, recall 0.887545
2017-12-10T15:50:56.766399: step 3016, loss 0.0381211, acc 0.984375, prec 0.0660837, recall 0.887545
2017-12-10T15:50:56.963662: step 3017, loss 0.112828, acc 0.921875, prec 0.0660997, recall 0.887585
2017-12-10T15:50:57.165632: step 3018, loss 0.104578, acc 0.953125, prec 0.0660945, recall 0.887585
2017-12-10T15:50:57.368679: step 3019, loss 0.331979, acc 0.96875, prec 0.0661404, recall 0.887664
2017-12-10T15:50:57.565691: step 3020, loss 0.13297, acc 0.96875, prec 0.0661369, recall 0.887664
2017-12-10T15:50:57.765234: step 3021, loss 0.187284, acc 0.921875, prec 0.0661529, recall 0.887704
2017-12-10T15:50:57.964125: step 3022, loss 0.124048, acc 0.984375, prec 0.0662006, recall 0.887784
2017-12-10T15:50:58.161791: step 3023, loss 0.0789094, acc 0.984375, prec 0.0661989, recall 0.887784
2017-12-10T15:50:58.361800: step 3024, loss 0.058497, acc 0.96875, prec 0.0661954, recall 0.887784
2017-12-10T15:50:58.559534: step 3025, loss 0.350885, acc 0.96875, prec 0.0662413, recall 0.887864
2017-12-10T15:50:58.765119: step 3026, loss 0.144201, acc 0.984375, prec 0.0662395, recall 0.887864
2017-12-10T15:50:58.961596: step 3027, loss 0.259204, acc 1, prec 0.0663137, recall 0.887983
2017-12-10T15:50:59.158726: step 3028, loss 0.766495, acc 0.953125, prec 0.0663579, recall 0.888062
2017-12-10T15:50:59.365225: step 3029, loss 0.46166, acc 0.9375, prec 0.0663755, recall 0.888102
2017-12-10T15:50:59.566657: step 3030, loss 0.064893, acc 0.953125, prec 0.0663703, recall 0.888102
2017-12-10T15:50:59.766592: step 3031, loss 0.262202, acc 0.90625, prec 0.0664091, recall 0.888181
2017-12-10T15:50:59.963470: step 3032, loss 0.043916, acc 1, prec 0.0664832, recall 0.8883
2017-12-10T15:51:00.163298: step 3033, loss 0.322907, acc 0.953125, prec 0.0665274, recall 0.888379
2017-12-10T15:51:00.362683: step 3034, loss 0.119714, acc 0.984375, prec 0.0665256, recall 0.888379
2017-12-10T15:51:00.562362: step 3035, loss 0.121601, acc 0.953125, prec 0.066545, recall 0.888418
2017-12-10T15:51:00.766751: step 3036, loss 0.254895, acc 0.953125, prec 0.0666138, recall 0.888536
2017-12-10T15:51:00.979002: step 3037, loss 0.196197, acc 0.953125, prec 0.0666085, recall 0.888536
2017-12-10T15:51:01.183472: step 3038, loss 0.157304, acc 0.921875, prec 0.0666244, recall 0.888575
2017-12-10T15:51:01.382102: step 3039, loss 0.262474, acc 0.90625, prec 0.0666138, recall 0.888575
2017-12-10T15:51:01.581121: step 3040, loss 0.0497069, acc 0.984375, prec 0.0666367, recall 0.888615
2017-12-10T15:51:01.775913: step 3041, loss 1.99649, acc 0.90625, prec 0.0666279, recall 0.888302
2017-12-10T15:51:01.995216: step 3042, loss 0.152858, acc 0.96875, prec 0.066649, recall 0.888341
2017-12-10T15:51:02.190756: step 3043, loss 0.390976, acc 0.90625, prec 0.0666631, recall 0.88838
2017-12-10T15:51:02.390571: step 3044, loss 0.40073, acc 0.90625, prec 0.0666772, recall 0.88842
2017-12-10T15:51:02.586686: step 3045, loss 0.488783, acc 0.921875, prec 0.0667177, recall 0.888498
2017-12-10T15:51:02.789460: step 3046, loss 0.284114, acc 0.890625, prec 0.0667054, recall 0.888498
2017-12-10T15:51:02.986366: step 3047, loss 0.311177, acc 0.90625, prec 0.0667195, recall 0.888537
2017-12-10T15:51:03.180756: step 3048, loss 0.186901, acc 0.9375, prec 0.0667124, recall 0.888537
2017-12-10T15:51:03.404467: step 3049, loss 0.345156, acc 0.875, prec 0.0666983, recall 0.888537
2017-12-10T15:51:03.607209: step 3050, loss 0.192842, acc 0.921875, prec 0.0667142, recall 0.888576
2017-12-10T15:51:03.804925: step 3051, loss 0.322759, acc 0.875, prec 0.066774, recall 0.888694
2017-12-10T15:51:04.003817: step 3052, loss 0.136851, acc 0.953125, prec 0.0667687, recall 0.888694
2017-12-10T15:51:04.202588: step 3053, loss 0.352731, acc 0.890625, prec 0.0667563, recall 0.888694
2017-12-10T15:51:04.400603: step 3054, loss 0.270216, acc 0.875, prec 0.0667423, recall 0.888694
2017-12-10T15:51:04.599338: step 3055, loss 0.16146, acc 0.921875, prec 0.0667581, recall 0.888733
2017-12-10T15:51:04.796753: step 3056, loss 0.0966445, acc 0.984375, prec 0.0667563, recall 0.888733
2017-12-10T15:51:04.999400: step 3057, loss 0.391797, acc 0.890625, prec 0.0667932, recall 0.888811
2017-12-10T15:51:05.196821: step 3058, loss 0.15607, acc 0.9375, prec 0.0667861, recall 0.888811
2017-12-10T15:51:05.392338: step 3059, loss 0.0940978, acc 0.96875, prec 0.0667826, recall 0.888811
2017-12-10T15:51:05.588651: step 3060, loss 0.215789, acc 0.875, prec 0.0667931, recall 0.88885
2017-12-10T15:51:05.785520: step 3061, loss 0.246989, acc 0.96875, prec 0.0668142, recall 0.888889
2017-12-10T15:51:05.983134: step 3062, loss 0.0779344, acc 0.953125, prec 0.0668089, recall 0.888889
2017-12-10T15:51:06.180261: step 3063, loss 0.200926, acc 0.9375, prec 0.0668019, recall 0.888889
2017-12-10T15:51:06.381039: step 3064, loss 1.48965, acc 0.96875, prec 0.0668247, recall 0.888616
2017-12-10T15:51:06.579871: step 3065, loss 0.192616, acc 0.9375, prec 0.0668422, recall 0.888655
2017-12-10T15:51:06.780500: step 3066, loss 0.327371, acc 0.984375, prec 0.0668651, recall 0.888694
2017-12-10T15:51:06.979745: step 3067, loss 0.147126, acc 0.953125, prec 0.0668843, recall 0.888733
2017-12-10T15:51:07.177264: step 3068, loss 0.0660259, acc 0.96875, prec 0.0668808, recall 0.888733
2017-12-10T15:51:07.374198: step 3069, loss 0.146476, acc 0.96875, prec 0.0669019, recall 0.888772
2017-12-10T15:51:07.576263: step 3070, loss 0.109503, acc 0.96875, prec 0.0668983, recall 0.888772
2017-12-10T15:51:07.773031: step 3071, loss 0.140366, acc 0.96875, prec 0.0668948, recall 0.888772
2017-12-10T15:51:07.970153: step 3072, loss 0.0535544, acc 0.96875, prec 0.0668913, recall 0.888772
2017-12-10T15:51:08.160660: step 3073, loss 0.937814, acc 0.96875, prec 0.0669369, recall 0.88885
2017-12-10T15:51:08.361689: step 3074, loss 0.290219, acc 0.921875, prec 0.0669527, recall 0.888889
2017-12-10T15:51:08.556914: step 3075, loss 0.251614, acc 0.921875, prec 0.0669438, recall 0.888889
2017-12-10T15:51:08.755137: step 3076, loss 0.0834567, acc 0.984375, prec 0.0669666, recall 0.888928
2017-12-10T15:51:08.955180: step 3077, loss 0.217082, acc 0.9375, prec 0.0669596, recall 0.888928
2017-12-10T15:51:09.158255: step 3078, loss 0.125944, acc 0.96875, prec 0.0670297, recall 0.889044
2017-12-10T15:51:09.355851: step 3079, loss 0.329089, acc 0.890625, prec 0.0670419, recall 0.889083
2017-12-10T15:51:09.551400: step 3080, loss 0.18466, acc 0.890625, prec 0.0670296, recall 0.889083
2017-12-10T15:51:09.750494: step 3081, loss 0.122475, acc 0.9375, prec 0.0670225, recall 0.889083
2017-12-10T15:51:09.949293: step 3082, loss 0.136192, acc 0.9375, prec 0.0670155, recall 0.889083
2017-12-10T15:51:10.147610: step 3083, loss 0.963199, acc 0.953125, prec 0.0670592, recall 0.88916
2017-12-10T15:51:10.347383: step 3084, loss 0.131821, acc 0.953125, prec 0.0670785, recall 0.889199
2017-12-10T15:51:10.542009: step 3085, loss 0.125015, acc 0.96875, prec 0.0670995, recall 0.889237
2017-12-10T15:51:10.739356: step 3086, loss 0.200287, acc 0.9375, prec 0.0671169, recall 0.889276
2017-12-10T15:51:10.932306: step 3087, loss 0.210318, acc 0.921875, prec 0.0671816, recall 0.889391
2017-12-10T15:51:11.130131: step 3088, loss 0.43253, acc 0.8125, prec 0.0672095, recall 0.889468
2017-12-10T15:51:11.324977: step 3089, loss 0.194581, acc 0.890625, prec 0.0671971, recall 0.889468
2017-12-10T15:51:11.521079: step 3090, loss 0.833994, acc 0.9375, prec 0.0672145, recall 0.889507
2017-12-10T15:51:11.719247: step 3091, loss 0.31612, acc 0.890625, prec 0.0672022, recall 0.889507
2017-12-10T15:51:11.916369: step 3092, loss 0.117464, acc 0.96875, prec 0.0671987, recall 0.889507
2017-12-10T15:51:12.113395: step 3093, loss 0.237917, acc 0.90625, prec 0.0672126, recall 0.889545
2017-12-10T15:51:12.312610: step 3094, loss 0.173485, acc 0.953125, prec 0.0672317, recall 0.889583
2017-12-10T15:51:12.509670: step 3095, loss 0.286513, acc 0.90625, prec 0.0672456, recall 0.889622
2017-12-10T15:51:12.713349: step 3096, loss 0.382491, acc 0.921875, prec 0.0672613, recall 0.88966
2017-12-10T15:51:12.908288: step 3097, loss 0.157033, acc 0.9375, prec 0.0672787, recall 0.889698
2017-12-10T15:51:13.103842: step 3098, loss 0.206618, acc 0.953125, prec 0.0672979, recall 0.889736
2017-12-10T15:51:13.302467: step 3099, loss 0.312335, acc 0.921875, prec 0.0673135, recall 0.889775
2017-12-10T15:51:13.499586: step 3100, loss 0.150002, acc 0.921875, prec 0.0673291, recall 0.889813
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_3/1512938428/checkpoints/model-3100

2017-12-10T15:51:14.868463: step 3101, loss 0.124584, acc 0.9375, prec 0.0673221, recall 0.889813
2017-12-10T15:51:15.067365: step 3102, loss 0.253265, acc 0.90625, prec 0.0673359, recall 0.889851
2017-12-10T15:51:15.263167: step 3103, loss 0.284109, acc 0.921875, prec 0.0673271, recall 0.889851
2017-12-10T15:51:15.464074: step 3104, loss 0.17104, acc 0.953125, prec 0.0673218, recall 0.889851
2017-12-10T15:51:15.661450: step 3105, loss 0.191157, acc 0.9375, prec 0.0673636, recall 0.889927
2017-12-10T15:51:15.857697: step 3106, loss 0.115168, acc 0.9375, prec 0.067381, recall 0.889965
2017-12-10T15:51:16.058331: step 3107, loss 0.0854006, acc 0.953125, prec 0.0674001, recall 0.890003
2017-12-10T15:51:16.252521: step 3108, loss 0.114512, acc 0.984375, prec 0.0673984, recall 0.890003
2017-12-10T15:51:16.446250: step 3109, loss 1.31628, acc 0.953125, prec 0.0674419, recall 0.890079
2017-12-10T15:51:16.645475: step 3110, loss 0.0978528, acc 0.96875, prec 0.0674384, recall 0.890079
2017-12-10T15:51:16.841393: step 3111, loss 0.192005, acc 0.9375, prec 0.0674557, recall 0.890117
2017-12-10T15:51:17.043012: step 3112, loss 0.372747, acc 0.9375, prec 0.0674731, recall 0.890155
2017-12-10T15:51:17.242188: step 3113, loss 0.0633782, acc 0.953125, prec 0.0674678, recall 0.890155
2017-12-10T15:51:17.439324: step 3114, loss 0.0616273, acc 0.96875, prec 0.0674887, recall 0.890193
2017-12-10T15:51:17.638720: step 3115, loss 0.0711655, acc 0.984375, prec 0.0675357, recall 0.890269
2017-12-10T15:51:17.839229: step 3116, loss 0.223268, acc 0.953125, prec 0.0675304, recall 0.890269
2017-12-10T15:51:18.033419: step 3117, loss 0.157836, acc 0.921875, prec 0.0675704, recall 0.890345
2017-12-10T15:51:18.229408: step 3118, loss 0.717531, acc 0.953125, prec 0.0676139, recall 0.89042
2017-12-10T15:51:18.431153: step 3119, loss 0.044211, acc 0.984375, prec 0.0676121, recall 0.89042
2017-12-10T15:51:18.628431: step 3120, loss 0.225067, acc 0.953125, prec 0.0676312, recall 0.890458
2017-12-10T15:51:18.830226: step 3121, loss 0.0926264, acc 0.9375, prec 0.0676729, recall 0.890534
2017-12-10T15:51:19.028581: step 3122, loss 0.0665314, acc 0.96875, prec 0.0676694, recall 0.890534
2017-12-10T15:51:19.228209: step 3123, loss 0.399343, acc 0.953125, prec 0.0677128, recall 0.890609
2017-12-10T15:51:19.424881: step 3124, loss 0.12521, acc 0.9375, prec 0.0677301, recall 0.890647
2017-12-10T15:51:19.619165: step 3125, loss 0.772927, acc 0.90625, prec 0.0677439, recall 0.890684
2017-12-10T15:51:19.818392: step 3126, loss 0.0718381, acc 0.96875, prec 0.0677403, recall 0.890684
2017-12-10T15:51:20.011240: step 3127, loss 0.107159, acc 0.9375, prec 0.0677333, recall 0.890684
2017-12-10T15:51:20.213658: step 3128, loss 0.255301, acc 0.9375, prec 0.0677505, recall 0.890722
2017-12-10T15:51:20.412738: step 3129, loss 0.249678, acc 0.953125, prec 0.0677696, recall 0.890759
2017-12-10T15:51:20.609794: step 3130, loss 0.23206, acc 0.921875, prec 0.0678338, recall 0.890872
2017-12-10T15:51:20.813530: step 3131, loss 0.0803492, acc 0.96875, prec 0.0678303, recall 0.890872
2017-12-10T15:51:21.010387: step 3132, loss 0.202455, acc 0.953125, prec 0.0678493, recall 0.890909
2017-12-10T15:51:21.212784: step 3133, loss 0.120823, acc 0.953125, prec 0.0678683, recall 0.890947
2017-12-10T15:51:21.407090: step 3134, loss 0.369194, acc 0.890625, prec 0.0678559, recall 0.890947
2017-12-10T15:51:21.608263: step 3135, loss 0.0836221, acc 0.96875, prec 0.0679011, recall 0.891021
2017-12-10T15:51:21.806253: step 3136, loss 0.145961, acc 0.9375, prec 0.067894, recall 0.891021
2017-12-10T15:51:22.003192: step 3137, loss 0.0528464, acc 0.984375, prec 0.0678922, recall 0.891021
2017-12-10T15:51:22.201209: step 3138, loss 0.159257, acc 0.96875, prec 0.0679373, recall 0.891096
2017-12-10T15:51:22.404446: step 3139, loss 0.0364317, acc 1, prec 0.0679373, recall 0.891096
2017-12-10T15:51:22.627088: step 3140, loss 0.0611308, acc 0.984375, prec 0.0679356, recall 0.891096
2017-12-10T15:51:22.825309: step 3141, loss 0.175234, acc 0.96875, prec 0.067932, recall 0.891096
2017-12-10T15:51:23.026106: step 3142, loss 0.263444, acc 0.9375, prec 0.0679493, recall 0.891133
2017-12-10T15:51:23.223881: step 3143, loss 0.0361927, acc 0.984375, prec 0.0679475, recall 0.891133
2017-12-10T15:51:23.421390: step 3144, loss 0.470653, acc 0.96875, prec 0.0679683, recall 0.89117
2017-12-10T15:51:23.623253: step 3145, loss 0.12664, acc 0.9375, prec 0.0680341, recall 0.891282
2017-12-10T15:51:23.821734: step 3146, loss 0.042826, acc 0.984375, prec 0.0680324, recall 0.891282
2017-12-10T15:51:24.021153: step 3147, loss 0.167638, acc 0.953125, prec 0.0681, recall 0.891393
2017-12-10T15:51:24.221592: step 3148, loss 1.65216, acc 0.96875, prec 0.0680982, recall 0.891089
2017-12-10T15:51:24.422399: step 3149, loss 0.278582, acc 0.921875, prec 0.0681136, recall 0.891126
2017-12-10T15:51:24.622314: step 3150, loss 0.0803719, acc 0.984375, prec 0.0681119, recall 0.891126
2017-12-10T15:51:24.819090: step 3151, loss 0.174001, acc 0.921875, prec 0.068103, recall 0.891126
2017-12-10T15:51:25.023864: step 3152, loss 0.200066, acc 0.953125, prec 0.0680977, recall 0.891126
2017-12-10T15:51:25.224722: step 3153, loss 0.0798105, acc 0.96875, prec 0.0680941, recall 0.891126
2017-12-10T15:51:25.423618: step 3154, loss 0.0480444, acc 0.984375, prec 0.0680923, recall 0.891126
2017-12-10T15:51:25.624073: step 3155, loss 0.094721, acc 0.984375, prec 0.0681148, recall 0.891163
2017-12-10T15:51:25.817146: step 3156, loss 0.123641, acc 0.96875, prec 0.0681113, recall 0.891163
2017-12-10T15:51:26.014898: step 3157, loss 0.0247081, acc 1, prec 0.0681113, recall 0.891163
2017-12-10T15:51:26.217764: step 3158, loss 0.251535, acc 0.9375, prec 0.0681528, recall 0.891238
2017-12-10T15:51:26.414254: step 3159, loss 0.113456, acc 0.984375, prec 0.068151, recall 0.891238
2017-12-10T15:51:26.614260: step 3160, loss 0.0678246, acc 0.96875, prec 0.0681717, recall 0.891275
2017-12-10T15:51:26.813471: step 3161, loss 0.0532455, acc 1, prec 0.068196, recall 0.891312
2017-12-10T15:51:27.034035: step 3162, loss 0.259273, acc 0.953125, prec 0.0682393, recall 0.891386
2017-12-10T15:51:27.232204: step 3163, loss 0.0945478, acc 0.953125, prec 0.0682339, recall 0.891386
2017-12-10T15:51:27.437881: step 3164, loss 0.0484633, acc 0.984375, prec 0.0682322, recall 0.891386
2017-12-10T15:51:27.635410: step 3165, loss 0.305172, acc 0.953125, prec 0.0682511, recall 0.891423
2017-12-10T15:51:27.833799: step 3166, loss 0.0151685, acc 1, prec 0.0682511, recall 0.891423
2017-12-10T15:51:28.047670: step 3167, loss 0.494787, acc 0.84375, prec 0.0682333, recall 0.891423
2017-12-10T15:51:28.245470: step 3168, loss 0.167164, acc 0.96875, prec 0.0682541, recall 0.89146
2017-12-10T15:51:28.444074: step 3169, loss 1.88441, acc 0.953125, prec 0.0682748, recall 0.891193
2017-12-10T15:51:28.642069: step 3170, loss 0.1338, acc 0.953125, prec 0.0682694, recall 0.891193
2017-12-10T15:51:28.839404: step 3171, loss 0.130292, acc 0.9375, prec 0.0682866, recall 0.89123
2017-12-10T15:51:29.051243: step 3172, loss 0.118105, acc 0.96875, prec 0.068283, recall 0.89123
2017-12-10T15:51:29.258420: step 3173, loss 0.946084, acc 0.953125, prec 0.068302, recall 0.891267
2017-12-10T15:51:29.465433: step 3174, loss 0.122705, acc 0.96875, prec 0.0683227, recall 0.891304
2017-12-10T15:51:29.660838: step 3175, loss 0.146541, acc 0.984375, prec 0.0683451, recall 0.891341
2017-12-10T15:51:29.859739: step 3176, loss 0.356097, acc 0.890625, prec 0.0683327, recall 0.891341
2017-12-10T15:51:30.056692: step 3177, loss 0.341216, acc 0.875, prec 0.0683185, recall 0.891341
2017-12-10T15:51:30.265354: step 3178, loss 0.46124, acc 0.90625, prec 0.068332, recall 0.891378
2017-12-10T15:51:30.461798: step 3179, loss 0.186596, acc 0.90625, prec 0.0683214, recall 0.891378
2017-12-10T15:51:30.655673: step 3180, loss 0.270558, acc 0.9375, prec 0.0683627, recall 0.891452
2017-12-10T15:51:30.855383: step 3181, loss 0.290422, acc 0.90625, prec 0.0683521, recall 0.891452
2017-12-10T15:51:31.055603: step 3182, loss 0.30226, acc 0.921875, prec 0.0683916, recall 0.891525
2017-12-10T15:51:31.261478: step 3183, loss 0.441682, acc 0.9375, prec 0.068433, recall 0.891599
2017-12-10T15:51:31.461158: step 3184, loss 0.129958, acc 0.9375, prec 0.0684743, recall 0.891672
2017-12-10T15:51:31.664919: step 3185, loss 0.155771, acc 0.9375, prec 0.0684672, recall 0.891672
2017-12-10T15:51:31.863225: step 3186, loss 0.331966, acc 0.90625, prec 0.0684807, recall 0.891709
2017-12-10T15:51:32.061489: step 3187, loss 0.387831, acc 0.859375, prec 0.0684647, recall 0.891709
2017-12-10T15:51:32.257818: step 3188, loss 0.443774, acc 0.921875, prec 0.06848, recall 0.891746
2017-12-10T15:51:32.461209: step 3189, loss 0.249347, acc 0.921875, prec 0.0684953, recall 0.891782
2017-12-10T15:51:32.659709: step 3190, loss 0.156262, acc 0.9375, prec 0.0685366, recall 0.891855
2017-12-10T15:51:32.861798: step 3191, loss 0.190108, acc 0.9375, prec 0.0685294, recall 0.891855
2017-12-10T15:51:33.061801: step 3192, loss 0.0607993, acc 0.984375, prec 0.0685277, recall 0.891855
2017-12-10T15:51:33.258602: step 3193, loss 0.102975, acc 0.9375, prec 0.0685205, recall 0.891855
2017-12-10T15:51:33.455731: step 3194, loss 2.52849, acc 0.953125, prec 0.0685653, recall 0.891627
2017-12-10T15:51:33.655662: step 3195, loss 0.12389, acc 0.96875, prec 0.068586, recall 0.891664
2017-12-10T15:51:33.854059: step 3196, loss 0.285109, acc 0.953125, prec 0.0686048, recall 0.8917
2017-12-10T15:51:34.055579: step 3197, loss 0.168264, acc 0.953125, prec 0.0685995, recall 0.8917
2017-12-10T15:51:34.257357: step 3198, loss 0.251735, acc 0.9375, prec 0.0685923, recall 0.8917
2017-12-10T15:51:34.452878: step 3199, loss 0.389371, acc 0.96875, prec 0.0686613, recall 0.89181
2017-12-10T15:51:34.653077: step 3200, loss 0.266358, acc 0.984375, prec 0.0687562, recall 0.891956
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_3/1512938428/checkpoints/model-3200

2017-12-10T15:51:35.879877: step 3201, loss 0.259176, acc 0.875, prec 0.0687661, recall 0.891992
2017-12-10T15:51:36.086362: step 3202, loss 0.517485, acc 0.875, prec 0.0688001, recall 0.892065
2017-12-10T15:51:36.281459: step 3203, loss 0.0967694, acc 0.9375, prec 0.0687929, recall 0.892065
2017-12-10T15:51:36.481565: step 3204, loss 0.259814, acc 0.90625, prec 0.0687822, recall 0.892065
2017-12-10T15:51:36.678009: step 3205, loss 0.478869, acc 0.859375, prec 0.0687662, recall 0.892065
2017-12-10T15:51:36.872283: step 3206, loss 0.224185, acc 0.96875, prec 0.068835, recall 0.892173
2017-12-10T15:51:37.074953: step 3207, loss 0.147149, acc 0.921875, prec 0.0688261, recall 0.892173
2017-12-10T15:51:37.271688: step 3208, loss 0.275491, acc 0.875, prec 0.0688119, recall 0.892173
2017-12-10T15:51:37.467640: step 3209, loss 0.212337, acc 0.921875, prec 0.0688271, recall 0.89221
2017-12-10T15:51:37.664212: step 3210, loss 0.180725, acc 0.9375, prec 0.0688199, recall 0.89221
2017-12-10T15:51:37.860930: step 3211, loss 0.349155, acc 0.890625, prec 0.0688316, recall 0.892246
2017-12-10T15:51:38.060613: step 3212, loss 0.161003, acc 0.921875, prec 0.0688227, recall 0.892246
2017-12-10T15:51:38.262858: step 3213, loss 0.0413062, acc 0.984375, prec 0.0688209, recall 0.892246
2017-12-10T15:51:38.465049: step 3214, loss 0.111781, acc 0.984375, prec 0.0688432, recall 0.892282
2017-12-10T15:51:38.663511: step 3215, loss 0.0791503, acc 0.96875, prec 0.0688637, recall 0.892318
2017-12-10T15:51:38.860970: step 3216, loss 0.422929, acc 0.90625, prec 0.0688772, recall 0.892354
2017-12-10T15:51:39.063688: step 3217, loss 0.103128, acc 0.96875, prec 0.0688977, recall 0.89239
2017-12-10T15:51:39.259976: step 3218, loss 0.172716, acc 0.953125, prec 0.0689164, recall 0.892426
2017-12-10T15:51:39.461668: step 3219, loss 0.156216, acc 0.96875, prec 0.0689129, recall 0.892426
2017-12-10T15:51:39.667833: step 3220, loss 0.0241377, acc 0.984375, prec 0.0689111, recall 0.892426
2017-12-10T15:51:39.867745: step 3221, loss 0.282256, acc 0.953125, prec 0.068978, recall 0.892534
2017-12-10T15:51:40.064581: step 3222, loss 0.0611298, acc 0.984375, prec 0.0689762, recall 0.892534
2017-12-10T15:51:40.265719: step 3223, loss 0.0889206, acc 0.96875, prec 0.0689727, recall 0.892534
2017-12-10T15:51:40.466100: step 3224, loss 0.0555411, acc 0.984375, prec 0.069019, recall 0.892606
2017-12-10T15:51:40.664225: step 3225, loss 4.53764, acc 0.90625, prec 0.0690101, recall 0.892308
2017-12-10T15:51:40.864440: step 3226, loss 0.167612, acc 0.953125, prec 0.0690288, recall 0.892344
2017-12-10T15:51:41.066357: step 3227, loss 0.527082, acc 0.90625, prec 0.0690422, recall 0.89238
2017-12-10T15:51:41.266929: step 3228, loss 0.165158, acc 0.921875, prec 0.0690573, recall 0.892416
2017-12-10T15:51:41.470305: step 3229, loss 0.0884088, acc 0.96875, prec 0.0690538, recall 0.892416
2017-12-10T15:51:41.672553: step 3230, loss 0.0501935, acc 0.96875, prec 0.0690743, recall 0.892452
2017-12-10T15:51:41.880112: step 3231, loss 0.100444, acc 0.96875, prec 0.0690707, recall 0.892452
2017-12-10T15:51:42.080155: step 3232, loss 0.264279, acc 0.9375, prec 0.0690876, recall 0.892487
2017-12-10T15:51:42.281486: step 3233, loss 0.212765, acc 0.90625, prec 0.0690769, recall 0.892487
2017-12-10T15:51:42.475910: step 3234, loss 0.23385, acc 0.921875, prec 0.069068, recall 0.892487
2017-12-10T15:51:42.674198: step 3235, loss 0.689528, acc 0.953125, prec 0.0691107, recall 0.892559
2017-12-10T15:51:42.876502: step 3236, loss 0.155483, acc 0.96875, prec 0.0691553, recall 0.892631
2017-12-10T15:51:43.080330: step 3237, loss 0.184205, acc 0.9375, prec 0.0691481, recall 0.892631
2017-12-10T15:51:43.278283: step 3238, loss 0.309521, acc 0.84375, prec 0.0691303, recall 0.892631
2017-12-10T15:51:43.476926: step 3239, loss 0.346965, acc 0.90625, prec 0.0691436, recall 0.892667
2017-12-10T15:51:43.673841: step 3240, loss 0.215847, acc 0.9375, prec 0.0691364, recall 0.892667
2017-12-10T15:51:43.896622: step 3241, loss 0.624032, acc 0.875, prec 0.0691702, recall 0.892738
2017-12-10T15:51:44.099797: step 3242, loss 0.378013, acc 0.828125, prec 0.0691746, recall 0.892774
2017-12-10T15:51:44.299673: step 3243, loss 0.297739, acc 0.859375, prec 0.0691585, recall 0.892774
2017-12-10T15:51:44.509081: step 3244, loss 0.286359, acc 0.90625, prec 0.0691478, recall 0.892774
2017-12-10T15:51:44.707225: step 3245, loss 0.203038, acc 0.90625, prec 0.0691611, recall 0.89281
2017-12-10T15:51:44.906820: step 3246, loss 0.0853424, acc 0.984375, prec 0.0691594, recall 0.89281
2017-12-10T15:51:45.106478: step 3247, loss 0.173619, acc 0.9375, prec 0.0691762, recall 0.892845
2017-12-10T15:51:45.305743: step 3248, loss 0.157063, acc 0.921875, prec 0.0691673, recall 0.892845
2017-12-10T15:51:45.501238: step 3249, loss 0.11678, acc 1, prec 0.0692153, recall 0.892917
2017-12-10T15:51:45.701346: step 3250, loss 0.140308, acc 0.953125, prec 0.0692339, recall 0.892952
2017-12-10T15:51:45.902028: step 3251, loss 0.221285, acc 0.921875, prec 0.069249, recall 0.892988
2017-12-10T15:51:46.098832: step 3252, loss 0.444923, acc 0.9375, prec 0.0692659, recall 0.893023
2017-12-10T15:51:46.302313: step 3253, loss 0.107057, acc 0.96875, prec 0.0692863, recall 0.893059
2017-12-10T15:51:46.503561: step 3254, loss 0.274149, acc 0.96875, prec 0.0693067, recall 0.893094
2017-12-10T15:51:46.699619: step 3255, loss 0.531056, acc 0.96875, prec 0.0693511, recall 0.893165
2017-12-10T15:51:46.899964: step 3256, loss 0.410343, acc 0.984375, prec 0.0693732, recall 0.893201
2017-12-10T15:51:47.103151: step 3257, loss 0.491956, acc 0.96875, prec 0.0694895, recall 0.893377
2017-12-10T15:51:47.301429: step 3258, loss 0.105827, acc 0.984375, prec 0.0694877, recall 0.893377
2017-12-10T15:51:47.499557: step 3259, loss 0.164495, acc 0.921875, prec 0.0694788, recall 0.893377
2017-12-10T15:51:47.696552: step 3260, loss 0.174988, acc 0.96875, prec 0.0694992, recall 0.893413
2017-12-10T15:51:47.898026: step 3261, loss 0.270821, acc 0.9375, prec 0.0695399, recall 0.893483
2017-12-10T15:51:48.114828: step 3262, loss 0.147783, acc 0.90625, prec 0.0695292, recall 0.893483
2017-12-10T15:51:48.313678: step 3263, loss 0.214109, acc 0.9375, prec 0.069522, recall 0.893483
2017-12-10T15:51:48.514759: step 3264, loss 0.288821, acc 0.953125, prec 0.0695645, recall 0.893554
2017-12-10T15:51:48.713048: step 3265, loss 0.188658, acc 0.96875, prec 0.0695849, recall 0.893589
2017-12-10T15:51:48.912527: step 3266, loss 0.171835, acc 0.96875, prec 0.0696292, recall 0.893659
2017-12-10T15:51:49.110333: step 3267, loss 0.272461, acc 0.890625, prec 0.0696406, recall 0.893694
2017-12-10T15:51:49.306685: step 3268, loss 0.326844, acc 0.9375, prec 0.0696334, recall 0.893694
2017-12-10T15:51:49.506756: step 3269, loss 0.789748, acc 0.921875, prec 0.0696723, recall 0.893764
2017-12-10T15:51:49.708998: step 3270, loss 0.0607761, acc 0.984375, prec 0.0696945, recall 0.893799
2017-12-10T15:51:49.906390: step 3271, loss 0.086631, acc 0.953125, prec 0.0696891, recall 0.893799
2017-12-10T15:51:50.099609: step 3272, loss 0.275821, acc 0.921875, prec 0.0697041, recall 0.893834
2017-12-10T15:51:50.297751: step 3273, loss 0.260001, acc 0.890625, prec 0.0697393, recall 0.893904
2017-12-10T15:51:50.498907: step 3274, loss 0.117424, acc 0.984375, prec 0.0697615, recall 0.893939
2017-12-10T15:51:50.696764: step 3275, loss 0.0693291, acc 0.96875, prec 0.0697818, recall 0.893974
2017-12-10T15:51:50.905369: step 3276, loss 0.241915, acc 0.890625, prec 0.069817, recall 0.894044
2017-12-10T15:51:51.102083: step 3277, loss 0.133385, acc 0.9375, prec 0.0698338, recall 0.894079
2017-12-10T15:51:51.301544: step 3278, loss 0.17198, acc 0.953125, prec 0.0698284, recall 0.894079
2017-12-10T15:51:51.499098: step 3279, loss 0.19646, acc 0.921875, prec 0.0698672, recall 0.894149
2017-12-10T15:51:51.698871: step 3280, loss 0.298658, acc 0.9375, prec 0.0699078, recall 0.894218
2017-12-10T15:51:51.896725: step 3281, loss 0.143637, acc 0.9375, prec 0.0699006, recall 0.894218
2017-12-10T15:51:52.095228: step 3282, loss 0.318027, acc 0.953125, prec 0.0699669, recall 0.894322
2017-12-10T15:51:52.300632: step 3283, loss 0.15896, acc 0.953125, prec 0.0699615, recall 0.894322
2017-12-10T15:51:52.498281: step 3284, loss 0.459702, acc 0.875, prec 0.0699471, recall 0.894322
2017-12-10T15:51:52.695585: step 3285, loss 0.108448, acc 0.96875, prec 0.0699435, recall 0.894322
2017-12-10T15:51:52.894512: step 3286, loss 0.0471058, acc 0.984375, prec 0.0699417, recall 0.894322
2017-12-10T15:51:53.090122: step 3287, loss 0.237555, acc 0.921875, prec 0.0699805, recall 0.894392
2017-12-10T15:51:53.294503: step 3288, loss 0.0361758, acc 1, prec 0.0699805, recall 0.894392
2017-12-10T15:51:53.495045: step 3289, loss 0.334853, acc 0.953125, prec 0.0700228, recall 0.894461
2017-12-10T15:51:53.697774: step 3290, loss 0.026502, acc 0.984375, prec 0.070021, recall 0.894461
2017-12-10T15:51:53.896929: step 3291, loss 0.828308, acc 0.984375, prec 0.070067, recall 0.89453
2017-12-10T15:51:54.102017: step 3292, loss 0.177554, acc 0.9375, prec 0.0700598, recall 0.89453
2017-12-10T15:51:54.299428: step 3293, loss 2.32696, acc 0.96875, prec 0.070058, recall 0.894237
2017-12-10T15:51:54.502566: step 3294, loss 0.101512, acc 0.96875, prec 0.0700544, recall 0.894237
2017-12-10T15:51:54.704421: step 3295, loss 0.562744, acc 0.9375, prec 0.0701187, recall 0.894341
2017-12-10T15:51:54.899749: step 3296, loss 2.01881, acc 0.921875, prec 0.0701592, recall 0.894118
2017-12-10T15:51:55.097112: step 3297, loss 0.208577, acc 0.9375, prec 0.0701997, recall 0.894187
2017-12-10T15:51:55.289971: step 3298, loss 0.169875, acc 0.953125, prec 0.0701943, recall 0.894187
2017-12-10T15:51:55.485746: step 3299, loss 0.588332, acc 0.828125, prec 0.070246, recall 0.89429
2017-12-10T15:51:55.682092: step 3300, loss 0.323609, acc 0.84375, prec 0.070228, recall 0.89429
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_3/1512938428/checkpoints/model-3300

2017-12-10T15:51:56.998141: step 3301, loss 0.637024, acc 0.765625, prec 0.070201, recall 0.89429
2017-12-10T15:51:57.195017: step 3302, loss 0.934637, acc 0.765625, prec 0.0701741, recall 0.89429
2017-12-10T15:51:57.393703: step 3303, loss 0.328044, acc 0.921875, prec 0.0701889, recall 0.894325
2017-12-10T15:51:57.592417: step 3304, loss 0.433854, acc 0.890625, prec 0.0701763, recall 0.894325
2017-12-10T15:51:57.788593: step 3305, loss 0.573909, acc 0.84375, prec 0.070206, recall 0.894394
2017-12-10T15:51:57.982020: step 3306, loss 0.614221, acc 0.796875, prec 0.0701826, recall 0.894394
2017-12-10T15:51:58.182699: step 3307, loss 0.901338, acc 0.8125, prec 0.0701849, recall 0.894428
2017-12-10T15:51:58.387774: step 3308, loss 0.343134, acc 0.875, prec 0.0701943, recall 0.894463
2017-12-10T15:51:58.582834: step 3309, loss 0.811957, acc 0.734375, prec 0.0701638, recall 0.894463
2017-12-10T15:51:58.780738: step 3310, loss 0.513484, acc 0.875, prec 0.0701494, recall 0.894463
2017-12-10T15:51:58.976537: step 3311, loss 0.456165, acc 0.84375, prec 0.0701553, recall 0.894497
2017-12-10T15:51:59.176437: step 3312, loss 0.273865, acc 0.875, prec 0.0701409, recall 0.894497
2017-12-10T15:51:59.378719: step 3313, loss 0.350005, acc 0.890625, prec 0.0701521, recall 0.894531
2017-12-10T15:51:59.582507: step 3314, loss 0.500457, acc 0.875, prec 0.0701378, recall 0.894531
2017-12-10T15:51:59.777076: step 3315, loss 0.440675, acc 0.859375, prec 0.0701217, recall 0.894531
2017-12-10T15:51:59.973578: step 3316, loss 0.254935, acc 0.90625, prec 0.0701822, recall 0.894634
2017-12-10T15:52:00.171029: step 3317, loss 0.401864, acc 0.90625, prec 0.0701951, recall 0.894668
2017-12-10T15:52:00.371033: step 3318, loss 0.528617, acc 0.890625, prec 0.0702063, recall 0.894703
2017-12-10T15:52:00.575102: step 3319, loss 0.301512, acc 0.96875, prec 0.0702739, recall 0.894805
2017-12-10T15:52:00.774674: step 3320, loss 0.229786, acc 0.96875, prec 0.0702703, recall 0.894805
2017-12-10T15:52:00.971602: step 3321, loss 0.114941, acc 0.96875, prec 0.0702904, recall 0.894839
2017-12-10T15:52:01.169865: step 3322, loss 0.201661, acc 0.9375, prec 0.0703306, recall 0.894908
2017-12-10T15:52:01.371604: step 3323, loss 0.106273, acc 0.9375, prec 0.0703471, recall 0.894942
2017-12-10T15:52:01.570282: step 3324, loss 0.150701, acc 0.984375, prec 0.0703927, recall 0.89501
2017-12-10T15:52:01.768997: step 3325, loss 0.0250414, acc 1, prec 0.0703927, recall 0.89501
2017-12-10T15:52:01.975130: step 3326, loss 0.0089365, acc 1, prec 0.0704164, recall 0.895044
2017-12-10T15:52:02.173522: step 3327, loss 0.11618, acc 0.96875, prec 0.0704365, recall 0.895078
2017-12-10T15:52:02.369164: step 3328, loss 0.194049, acc 0.984375, prec 0.0704584, recall 0.895112
2017-12-10T15:52:02.569391: step 3329, loss 0.464505, acc 0.984375, prec 0.070504, recall 0.89518
2017-12-10T15:52:02.768432: step 3330, loss 0.0552552, acc 0.984375, prec 0.0705259, recall 0.895213
2017-12-10T15:52:02.969242: step 3331, loss 0.472033, acc 0.984375, prec 0.0705478, recall 0.895247
2017-12-10T15:52:03.173411: step 3332, loss 0.173779, acc 0.953125, prec 0.0705661, recall 0.895281
2017-12-10T15:52:03.368018: step 3333, loss 0.147515, acc 0.96875, prec 0.0705861, recall 0.895315
2017-12-10T15:52:03.570573: step 3334, loss 0.0253065, acc 0.984375, prec 0.0705843, recall 0.895315
2017-12-10T15:52:03.766250: step 3335, loss 3.97196, acc 0.9375, prec 0.0706263, recall 0.895094
2017-12-10T15:52:03.970759: step 3336, loss 0.304268, acc 0.9375, prec 0.0706428, recall 0.895127
2017-12-10T15:52:04.168046: step 3337, loss 0.906828, acc 0.921875, prec 0.0707048, recall 0.895229
2017-12-10T15:52:04.368058: step 3338, loss 0.223218, acc 0.921875, prec 0.0707431, recall 0.895296
2017-12-10T15:52:04.587041: step 3339, loss 0.13777, acc 0.9375, prec 0.0707595, recall 0.89533
2017-12-10T15:52:04.792732: step 3340, loss 0.547995, acc 0.890625, prec 0.0707469, recall 0.89533
2017-12-10T15:52:04.992815: step 3341, loss 0.503163, acc 0.84375, prec 0.0707762, recall 0.895397
2017-12-10T15:52:05.191173: step 3342, loss 0.459538, acc 0.875, prec 0.07088, recall 0.895566
2017-12-10T15:52:05.384903: step 3343, loss 0.341678, acc 0.890625, prec 0.0709146, recall 0.895633
2017-12-10T15:52:05.580525: step 3344, loss 0.305484, acc 0.859375, prec 0.0708984, recall 0.895633
2017-12-10T15:52:05.776591: step 3345, loss 0.509966, acc 0.859375, prec 0.0709294, recall 0.8957
2017-12-10T15:52:05.973237: step 3346, loss 0.541765, acc 0.859375, prec 0.0709368, recall 0.895733
2017-12-10T15:52:06.171116: step 3347, loss 0.78294, acc 0.8125, prec 0.0709151, recall 0.895733
2017-12-10T15:52:06.369270: step 3348, loss 0.580057, acc 0.875, prec 0.0709007, recall 0.895733
2017-12-10T15:52:06.564659: step 3349, loss 0.396942, acc 0.859375, prec 0.0709081, recall 0.895766
2017-12-10T15:52:06.767597: step 3350, loss 0.707554, acc 0.796875, prec 0.0709319, recall 0.895833
2017-12-10T15:52:06.966350: step 3351, loss 0.38217, acc 0.890625, prec 0.0709429, recall 0.895867
2017-12-10T15:52:07.164134: step 3352, loss 0.389716, acc 0.859375, prec 0.0709267, recall 0.895867
2017-12-10T15:52:07.361225: step 3353, loss 0.429228, acc 0.84375, prec 0.0709322, recall 0.8959
2017-12-10T15:52:07.558732: step 3354, loss 0.272018, acc 0.921875, prec 0.0709704, recall 0.895967
2017-12-10T15:52:07.757779: step 3355, loss 0.291492, acc 0.875, prec 0.070956, recall 0.895967
2017-12-10T15:52:07.954281: step 3356, loss 0.137289, acc 0.984375, prec 0.0710013, recall 0.896033
2017-12-10T15:52:08.159728: step 3357, loss 0.146408, acc 0.921875, prec 0.0710158, recall 0.896067
2017-12-10T15:52:08.353924: step 3358, loss 0.256745, acc 0.890625, prec 0.0710032, recall 0.896067
2017-12-10T15:52:08.556458: step 3359, loss 0.158607, acc 0.953125, prec 0.0710214, recall 0.8961
2017-12-10T15:52:08.752704: step 3360, loss 0.137281, acc 0.96875, prec 0.0710178, recall 0.8961
2017-12-10T15:52:08.951273: step 3361, loss 0.0556896, acc 0.96875, prec 0.0710612, recall 0.896166
2017-12-10T15:52:09.150130: step 3362, loss 0.358899, acc 0.921875, prec 0.0710993, recall 0.896232
2017-12-10T15:52:09.349679: step 3363, loss 0.0655423, acc 0.96875, prec 0.0710957, recall 0.896232
2017-12-10T15:52:09.547035: step 3364, loss 0.107314, acc 0.953125, prec 0.0711373, recall 0.896299
2017-12-10T15:52:09.749478: step 3365, loss 0.421171, acc 0.96875, prec 0.0711808, recall 0.896365
2017-12-10T15:52:09.955455: step 3366, loss 0.350729, acc 0.953125, prec 0.0712224, recall 0.896431
2017-12-10T15:52:10.161499: step 3367, loss 0.508705, acc 0.9375, prec 0.0712622, recall 0.896497
2017-12-10T15:52:10.377257: step 3368, loss 0.380398, acc 0.921875, prec 0.0712767, recall 0.89653
2017-12-10T15:52:10.577731: step 3369, loss 0.221666, acc 0.96875, prec 0.0712731, recall 0.89653
2017-12-10T15:52:10.774246: step 3370, loss 0.371387, acc 0.953125, prec 0.0712912, recall 0.896563
2017-12-10T15:52:10.978124: step 3371, loss 1.54479, acc 0.9375, prec 0.0712858, recall 0.896277
2017-12-10T15:52:11.184348: step 3372, loss 0.0931727, acc 0.96875, prec 0.0713057, recall 0.89631
2017-12-10T15:52:11.391260: step 3373, loss 2.89754, acc 0.90625, prec 0.0713436, recall 0.896092
2017-12-10T15:52:11.594060: step 3374, loss 0.101192, acc 0.96875, prec 0.0713635, recall 0.896125
2017-12-10T15:52:11.790685: step 3375, loss 0.423823, acc 0.9375, prec 0.0713798, recall 0.896158
2017-12-10T15:52:11.991236: step 3376, loss 0.362299, acc 0.890625, prec 0.0713906, recall 0.89619
2017-12-10T15:52:12.187363: step 3377, loss 0.246654, acc 0.921875, prec 0.0713816, recall 0.89619
2017-12-10T15:52:12.387369: step 3378, loss 0.618448, acc 0.84375, prec 0.0713636, recall 0.89619
2017-12-10T15:52:12.585002: step 3379, loss 0.604039, acc 0.84375, prec 0.071369, recall 0.896223
2017-12-10T15:52:12.783721: step 3380, loss 0.564129, acc 0.84375, prec 0.071351, recall 0.896223
2017-12-10T15:52:12.981699: step 3381, loss 0.426232, acc 0.875, prec 0.0713366, recall 0.896223
2017-12-10T15:52:13.174025: step 3382, loss 0.601366, acc 0.828125, prec 0.0713402, recall 0.896256
2017-12-10T15:52:13.373744: step 3383, loss 0.16226, acc 0.921875, prec 0.0714015, recall 0.896355
2017-12-10T15:52:13.570212: step 3384, loss 0.441742, acc 0.859375, prec 0.0713853, recall 0.896355
2017-12-10T15:52:13.766568: step 3385, loss 0.388928, acc 0.921875, prec 0.0713997, recall 0.896388
2017-12-10T15:52:13.958552: step 3386, loss 0.527061, acc 0.84375, prec 0.0714286, recall 0.896453
2017-12-10T15:52:14.158258: step 3387, loss 0.355023, acc 0.890625, prec 0.0714628, recall 0.896519
2017-12-10T15:52:14.354566: step 3388, loss 0.641814, acc 0.828125, prec 0.071443, recall 0.896519
2017-12-10T15:52:14.560475: step 3389, loss 0.228518, acc 0.90625, prec 0.0714322, recall 0.896519
2017-12-10T15:52:14.753594: step 3390, loss 0.123512, acc 0.9375, prec 0.0714484, recall 0.896552
2017-12-10T15:52:14.951994: step 3391, loss 0.251245, acc 0.9375, prec 0.0714412, recall 0.896552
2017-12-10T15:52:15.151543: step 3392, loss 0.461609, acc 0.875, prec 0.071497, recall 0.89665
2017-12-10T15:52:15.355317: step 3393, loss 0.133197, acc 0.9375, prec 0.0715132, recall 0.896682
2017-12-10T15:52:15.561132: step 3394, loss 0.215451, acc 0.9375, prec 0.0715294, recall 0.896715
2017-12-10T15:52:15.758254: step 3395, loss 0.479715, acc 0.875, prec 0.0715149, recall 0.896715
2017-12-10T15:52:15.959443: step 3396, loss 0.254364, acc 0.9375, prec 0.0715077, recall 0.896715
2017-12-10T15:52:16.166828: step 3397, loss 0.164315, acc 0.953125, prec 0.0715023, recall 0.896715
2017-12-10T15:52:16.363562: step 3398, loss 0.315396, acc 0.90625, prec 0.0715617, recall 0.896813
2017-12-10T15:52:16.562893: step 3399, loss 0.113855, acc 0.96875, prec 0.0715581, recall 0.896813
2017-12-10T15:52:16.757971: step 3400, loss 0.999471, acc 0.953125, prec 0.071576, recall 0.896845
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_3/1512938428/checkpoints/model-3400

2017-12-10T15:52:18.065904: step 3401, loss 0.537557, acc 0.96875, prec 0.0715958, recall 0.896878
2017-12-10T15:52:18.267021: step 3402, loss 0.0661789, acc 0.984375, prec 0.071594, recall 0.896878
2017-12-10T15:52:18.470208: step 3403, loss 0.239904, acc 0.953125, prec 0.0716353, recall 0.896943
2017-12-10T15:52:18.670660: step 3404, loss 0.0558969, acc 0.984375, prec 0.0716335, recall 0.896943
2017-12-10T15:52:18.871964: step 3405, loss 0.367237, acc 0.921875, prec 0.0716712, recall 0.897008
2017-12-10T15:52:19.069366: step 3406, loss 0.514368, acc 0.921875, prec 0.0716856, recall 0.89704
2017-12-10T15:52:19.269952: step 3407, loss 0.351929, acc 0.9375, prec 0.0717251, recall 0.897105
2017-12-10T15:52:19.471675: step 3408, loss 0.0826923, acc 0.984375, prec 0.07177, recall 0.89717
2017-12-10T15:52:19.670572: step 3409, loss 0.0555388, acc 0.96875, prec 0.0717664, recall 0.89717
2017-12-10T15:52:19.872251: step 3410, loss 0.0605961, acc 0.984375, prec 0.0717646, recall 0.89717
2017-12-10T15:52:20.070881: step 3411, loss 0.0700671, acc 0.96875, prec 0.0717843, recall 0.897202
2017-12-10T15:52:20.269866: step 3412, loss 0.0847431, acc 0.984375, prec 0.0718058, recall 0.897234
2017-12-10T15:52:20.465617: step 3413, loss 0.253276, acc 0.984375, prec 0.0718274, recall 0.897267
2017-12-10T15:52:20.664943: step 3414, loss 0.073116, acc 0.96875, prec 0.0718238, recall 0.897267
2017-12-10T15:52:20.861837: step 3415, loss 0.620246, acc 0.9375, prec 0.0718399, recall 0.897299
2017-12-10T15:52:21.064118: step 3416, loss 0.334187, acc 0.953125, prec 0.0718345, recall 0.897299
2017-12-10T15:52:21.271066: step 3417, loss 0.020427, acc 1, prec 0.0718578, recall 0.897331
2017-12-10T15:52:21.474339: step 3418, loss 0.1094, acc 0.96875, prec 0.0718542, recall 0.897331
2017-12-10T15:52:21.673743: step 3419, loss 0.0448779, acc 0.984375, prec 0.071899, recall 0.897396
2017-12-10T15:52:21.875900: step 3420, loss 0.0587171, acc 0.96875, prec 0.0719188, recall 0.897428
2017-12-10T15:52:22.072991: step 3421, loss 0.0848401, acc 0.984375, prec 0.0719403, recall 0.89746
2017-12-10T15:52:22.272559: step 3422, loss 0.0722028, acc 0.984375, prec 0.0719385, recall 0.89746
2017-12-10T15:52:22.471048: step 3423, loss 1.41312, acc 0.921875, prec 0.0719546, recall 0.897211
2017-12-10T15:52:22.674170: step 3424, loss 0.175779, acc 0.953125, prec 0.0719491, recall 0.897211
2017-12-10T15:52:22.872401: step 3425, loss 0.271776, acc 0.90625, prec 0.0719616, recall 0.897243
2017-12-10T15:52:23.068199: step 3426, loss 0.109143, acc 0.953125, prec 0.0719562, recall 0.897243
2017-12-10T15:52:23.264154: step 3427, loss 0.112269, acc 0.9375, prec 0.0719723, recall 0.897275
2017-12-10T15:52:23.462717: step 3428, loss 0.262567, acc 0.921875, prec 0.0719865, recall 0.897307
2017-12-10T15:52:23.663375: step 3429, loss 0.164773, acc 0.9375, prec 0.0720026, recall 0.89734
2017-12-10T15:52:23.863674: step 3430, loss 0.195359, acc 0.9375, prec 0.0719954, recall 0.89734
2017-12-10T15:52:24.065050: step 3431, loss 0.135163, acc 0.953125, prec 0.0720133, recall 0.897372
2017-12-10T15:52:24.293803: step 3432, loss 0.758806, acc 0.984375, prec 0.0720581, recall 0.897436
2017-12-10T15:52:24.502760: step 3433, loss 0.101292, acc 0.890625, prec 0.0720454, recall 0.897436
2017-12-10T15:52:24.702919: step 3434, loss 0.2114, acc 0.9375, prec 0.0720382, recall 0.897436
2017-12-10T15:52:24.899653: step 3435, loss 0.192709, acc 0.9375, prec 0.0720309, recall 0.897436
2017-12-10T15:52:25.102821: step 3436, loss 0.27798, acc 0.921875, prec 0.0720219, recall 0.897436
2017-12-10T15:52:25.303406: step 3437, loss 0.409878, acc 0.953125, prec 0.0720397, recall 0.897468
2017-12-10T15:52:25.505025: step 3438, loss 0.306272, acc 0.875, prec 0.0720253, recall 0.897468
2017-12-10T15:52:25.707796: step 3439, loss 0.139468, acc 0.96875, prec 0.072045, recall 0.8975
2017-12-10T15:52:25.909343: step 3440, loss 0.0807831, acc 0.96875, prec 0.0720413, recall 0.8975
2017-12-10T15:52:26.109439: step 3441, loss 0.10423, acc 0.984375, prec 0.0720628, recall 0.897532
2017-12-10T15:52:26.311146: step 3442, loss 0.497787, acc 0.953125, prec 0.0721272, recall 0.897628
2017-12-10T15:52:26.514660: step 3443, loss 0.28838, acc 0.953125, prec 0.0721218, recall 0.897628
2017-12-10T15:52:26.716902: step 3444, loss 0.0978848, acc 0.9375, prec 0.0721378, recall 0.89766
2017-12-10T15:52:26.911669: step 3445, loss 0.266588, acc 0.875, prec 0.0721233, recall 0.89766
2017-12-10T15:52:27.112454: step 3446, loss 0.104963, acc 0.984375, prec 0.0721448, recall 0.897692
2017-12-10T15:52:27.308795: step 3447, loss 0.257257, acc 0.921875, prec 0.0721357, recall 0.897692
2017-12-10T15:52:27.510401: step 3448, loss 0.220747, acc 0.953125, prec 0.0721536, recall 0.897724
2017-12-10T15:52:27.728147: step 3449, loss 0.0762429, acc 0.96875, prec 0.0721732, recall 0.897756
2017-12-10T15:52:27.931084: step 3450, loss 0.168902, acc 0.921875, prec 0.0721874, recall 0.897787
2017-12-10T15:52:28.126314: step 3451, loss 0.056263, acc 0.984375, prec 0.0721856, recall 0.897787
2017-12-10T15:52:28.322980: step 3452, loss 0.176492, acc 0.9375, prec 0.0722016, recall 0.897819
2017-12-10T15:52:28.522602: step 3453, loss 0.974246, acc 0.96875, prec 0.0722677, recall 0.897915
2017-12-10T15:52:28.722881: step 3454, loss 0.289022, acc 0.921875, prec 0.0722587, recall 0.897915
2017-12-10T15:52:28.932149: step 3455, loss 0.213281, acc 0.953125, prec 0.0722532, recall 0.897915
2017-12-10T15:52:29.129525: step 3456, loss 0.241152, acc 0.96875, prec 0.0722496, recall 0.897915
2017-12-10T15:52:29.340827: step 3457, loss 1.28558, acc 0.921875, prec 0.0723335, recall 0.898042
2017-12-10T15:52:29.548760: step 3458, loss 0.15501, acc 0.953125, prec 0.0723513, recall 0.898073
2017-12-10T15:52:29.745332: step 3459, loss 0.17682, acc 0.921875, prec 0.0723422, recall 0.898073
2017-12-10T15:52:29.941317: step 3460, loss 0.205791, acc 0.953125, prec 0.0724065, recall 0.898168
2017-12-10T15:52:30.137585: step 3461, loss 0.497523, acc 0.796875, prec 0.0724061, recall 0.8982
2017-12-10T15:52:30.332792: step 3462, loss 0.159735, acc 0.921875, prec 0.0724435, recall 0.898263
2017-12-10T15:52:30.533774: step 3463, loss 0.33286, acc 0.890625, prec 0.0724772, recall 0.898326
2017-12-10T15:52:30.731565: step 3464, loss 0.284665, acc 0.9375, prec 0.0724699, recall 0.898326
2017-12-10T15:52:30.928740: step 3465, loss 0.207026, acc 0.890625, prec 0.0725036, recall 0.898389
2017-12-10T15:52:31.122235: step 3466, loss 0.304203, acc 0.875, prec 0.0724891, recall 0.898389
2017-12-10T15:52:31.331731: step 3467, loss 0.207586, acc 0.921875, prec 0.0724801, recall 0.898389
2017-12-10T15:52:31.531460: step 3468, loss 0.250789, acc 0.890625, prec 0.0724674, recall 0.898389
2017-12-10T15:52:31.731103: step 3469, loss 0.100255, acc 0.9375, prec 0.0724833, recall 0.898421
2017-12-10T15:52:31.928542: step 3470, loss 0.131529, acc 0.96875, prec 0.0725029, recall 0.898452
2017-12-10T15:52:32.124720: step 3471, loss 0.148727, acc 0.90625, prec 0.072492, recall 0.898452
2017-12-10T15:52:32.323138: step 3472, loss 1.05202, acc 0.9375, prec 0.0725311, recall 0.898515
2017-12-10T15:52:32.523999: step 3473, loss 0.386201, acc 0.859375, prec 0.072538, recall 0.898546
2017-12-10T15:52:32.847954: step 3474, loss 0.367054, acc 0.90625, prec 0.0725502, recall 0.898578
2017-12-10T15:52:33.044316: step 3475, loss 0.172253, acc 0.953125, prec 0.0725448, recall 0.898578
2017-12-10T15:52:33.243217: step 3476, loss 0.301516, acc 0.9375, prec 0.0726302, recall 0.898703
2017-12-10T15:52:33.444022: step 3477, loss 0.0702575, acc 0.96875, prec 0.0726265, recall 0.898703
2017-12-10T15:52:33.640287: step 3478, loss 0.136387, acc 1, prec 0.072696, recall 0.898797
2017-12-10T15:52:33.817744: step 3479, loss 0.197002, acc 0.923077, prec 0.0726887, recall 0.898797
2017-12-10T15:52:34.025798: step 3480, loss 0.106586, acc 0.984375, prec 0.0726869, recall 0.898797
2017-12-10T15:52:34.226256: step 3481, loss 0.20444, acc 0.953125, prec 0.0727046, recall 0.898828
2017-12-10T15:52:34.425461: step 3482, loss 0.183055, acc 0.96875, prec 0.0727472, recall 0.89889
2017-12-10T15:52:34.626886: step 3483, loss 0.127256, acc 0.9375, prec 0.07274, recall 0.89889
2017-12-10T15:52:34.823716: step 3484, loss 0.165988, acc 0.96875, prec 0.0727826, recall 0.898953
2017-12-10T15:52:35.023932: step 3485, loss 0.300776, acc 0.953125, prec 0.0728003, recall 0.898984
2017-12-10T15:52:35.224087: step 3486, loss 0.176897, acc 0.9375, prec 0.072793, recall 0.898984
2017-12-10T15:52:35.423143: step 3487, loss 0.0684355, acc 0.96875, prec 0.0728125, recall 0.899015
2017-12-10T15:52:35.620976: step 3488, loss 0.253223, acc 0.953125, prec 0.0728071, recall 0.899015
2017-12-10T15:52:35.820822: step 3489, loss 0.0770005, acc 0.984375, prec 0.0728515, recall 0.899077
2017-12-10T15:52:36.020533: step 3490, loss 0.102911, acc 0.953125, prec 0.072846, recall 0.899077
2017-12-10T15:52:36.217835: step 3491, loss 0.035201, acc 0.984375, prec 0.0728673, recall 0.899108
2017-12-10T15:52:36.419837: step 3492, loss 1.52061, acc 0.921875, prec 0.0729294, recall 0.898925
2017-12-10T15:52:36.624026: step 3493, loss 0.250876, acc 0.921875, prec 0.0729203, recall 0.898925
2017-12-10T15:52:36.820707: step 3494, loss 0.314867, acc 0.921875, prec 0.0729112, recall 0.898925
2017-12-10T15:52:37.017307: step 3495, loss 0.679661, acc 0.90625, prec 0.0729234, recall 0.898956
2017-12-10T15:52:37.215785: step 3496, loss 0.118344, acc 0.953125, prec 0.0729411, recall 0.898987
2017-12-10T15:52:37.417220: step 3497, loss 0.0856791, acc 0.953125, prec 0.0729587, recall 0.899018
2017-12-10T15:52:37.614838: step 3498, loss 0.0425965, acc 0.984375, prec 0.0730262, recall 0.899111
2017-12-10T15:52:37.814018: step 3499, loss 0.175721, acc 0.984375, prec 0.0730474, recall 0.899142
2017-12-10T15:52:38.009178: step 3500, loss 0.185008, acc 0.9375, prec 0.0730401, recall 0.899142
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_3/1512938428/checkpoints/model-3500

2017-12-10T15:52:39.420595: step 3501, loss 0.375462, acc 0.9375, prec 0.073056, recall 0.899173
2017-12-10T15:52:39.620420: step 3502, loss 0.0854668, acc 0.953125, prec 0.0730505, recall 0.899173
2017-12-10T15:52:39.820768: step 3503, loss 0.0721743, acc 0.96875, prec 0.073093, recall 0.899234
2017-12-10T15:52:40.015980: step 3504, loss 0.402038, acc 0.984375, prec 0.0731143, recall 0.899265
2017-12-10T15:52:40.216392: step 3505, loss 0.218485, acc 0.96875, prec 0.0731337, recall 0.899296
2017-12-10T15:52:40.413172: step 3506, loss 0.127501, acc 0.953125, prec 0.0731513, recall 0.899327
2017-12-10T15:52:40.609918: step 3507, loss 0.120193, acc 0.9375, prec 0.073144, recall 0.899327
2017-12-10T15:52:40.810095: step 3508, loss 0.0720356, acc 0.96875, prec 0.0731634, recall 0.899358
2017-12-10T15:52:41.005877: step 3509, loss 0.165552, acc 0.90625, prec 0.0731525, recall 0.899358
2017-12-10T15:52:41.201332: step 3510, loss 0.263282, acc 0.875, prec 0.073161, recall 0.899388
2017-12-10T15:52:41.398737: step 3511, loss 0.0371555, acc 1, prec 0.073161, recall 0.899388
2017-12-10T15:52:41.595895: step 3512, loss 0.674387, acc 0.9375, prec 0.0731999, recall 0.89945
2017-12-10T15:52:41.794014: step 3513, loss 0.0683558, acc 0.984375, prec 0.073198, recall 0.89945
2017-12-10T15:52:41.994459: step 3514, loss 0.209754, acc 0.90625, prec 0.0732332, recall 0.899511
2017-12-10T15:52:42.190091: step 3515, loss 0.0883821, acc 0.953125, prec 0.0732277, recall 0.899511
2017-12-10T15:52:42.384925: step 3516, loss 0.143522, acc 0.9375, prec 0.0732435, recall 0.899542
2017-12-10T15:52:42.586744: step 3517, loss 0.715535, acc 0.984375, prec 0.0732647, recall 0.899573
2017-12-10T15:52:42.788004: step 3518, loss 0.141913, acc 0.96875, prec 0.0732611, recall 0.899573
2017-12-10T15:52:42.987929: step 3519, loss 0.0759836, acc 0.96875, prec 0.0733035, recall 0.899634
2017-12-10T15:52:43.188872: step 3520, loss 0.0553342, acc 1, prec 0.0733035, recall 0.899634
2017-12-10T15:52:43.388694: step 3521, loss 0.223799, acc 0.90625, prec 0.0732926, recall 0.899634
2017-12-10T15:52:43.585970: step 3522, loss 0.201313, acc 0.9375, prec 0.0733083, recall 0.899665
2017-12-10T15:52:43.781401: step 3523, loss 0.134407, acc 0.953125, prec 0.0733259, recall 0.899695
2017-12-10T15:52:43.976311: step 3524, loss 0.0518641, acc 0.984375, prec 0.0733471, recall 0.899726
2017-12-10T15:52:44.175918: step 3525, loss 0.0694982, acc 0.984375, prec 0.0733683, recall 0.899756
2017-12-10T15:52:44.376434: step 3526, loss 0.0961011, acc 0.96875, prec 0.0733877, recall 0.899787
2017-12-10T15:52:44.578540: step 3527, loss 0.187335, acc 0.96875, prec 0.073407, recall 0.899817
2017-12-10T15:52:44.778892: step 3528, loss 0.288565, acc 0.921875, prec 0.0733979, recall 0.899817
2017-12-10T15:52:44.972895: step 3529, loss 0.0476174, acc 1, prec 0.0734439, recall 0.899878
2017-12-10T15:52:45.171094: step 3530, loss 0.193669, acc 0.953125, prec 0.0734615, recall 0.899909
2017-12-10T15:52:45.366856: step 3531, loss 0.0240803, acc 1, prec 0.0734615, recall 0.899909
2017-12-10T15:52:45.563790: step 3532, loss 0.0582213, acc 0.96875, prec 0.0734808, recall 0.899939
2017-12-10T15:52:45.755716: step 3533, loss 0.0107303, acc 1, prec 0.0734808, recall 0.899939
2017-12-10T15:52:45.953316: step 3534, loss 0.0224946, acc 1, prec 0.0735269, recall 0.9
2017-12-10T15:52:46.156168: step 3535, loss 0.0532829, acc 0.984375, prec 0.073571, recall 0.900061
2017-12-10T15:52:46.355402: step 3536, loss 0.205661, acc 0.90625, prec 0.0735601, recall 0.900061
2017-12-10T15:52:46.553477: step 3537, loss 0.979051, acc 0.9375, prec 0.0735988, recall 0.900121
2017-12-10T15:52:46.757813: step 3538, loss 0.157578, acc 0.921875, prec 0.0735896, recall 0.900121
2017-12-10T15:52:46.960780: step 3539, loss 0.195396, acc 0.96875, prec 0.073609, recall 0.900152
2017-12-10T15:52:47.155639: step 3540, loss 0.244202, acc 0.953125, prec 0.0736035, recall 0.900152
2017-12-10T15:52:47.356043: step 3541, loss 0.232027, acc 0.953125, prec 0.073621, recall 0.900182
2017-12-10T15:52:47.554304: step 3542, loss 1.3059, acc 0.953125, prec 0.0736173, recall 0.899909
2017-12-10T15:52:47.752399: step 3543, loss 0.0693122, acc 0.984375, prec 0.0736385, recall 0.899939
2017-12-10T15:52:47.953354: step 3544, loss 0.239525, acc 0.9375, prec 0.0736312, recall 0.899939
2017-12-10T15:52:48.147857: step 3545, loss 0.0856749, acc 0.984375, prec 0.0736523, recall 0.89997
2017-12-10T15:52:48.342543: step 3546, loss 0.0670514, acc 0.984375, prec 0.0736735, recall 0.9
2017-12-10T15:52:48.543377: step 3547, loss 0.0671324, acc 1, prec 0.0736735, recall 0.9
2017-12-10T15:52:48.744433: step 3548, loss 0.096637, acc 0.96875, prec 0.0736699, recall 0.9
2017-12-10T15:52:48.946836: step 3549, loss 0.0537483, acc 0.984375, prec 0.073691, recall 0.90003
2017-12-10T15:52:49.141609: step 3550, loss 0.0884182, acc 0.96875, prec 0.0736873, recall 0.90003
2017-12-10T15:52:49.337932: step 3551, loss 0.20764, acc 0.9375, prec 0.07368, recall 0.90003
2017-12-10T15:52:49.532780: step 3552, loss 0.16251, acc 0.9375, prec 0.0736957, recall 0.900061
2017-12-10T15:52:49.727455: step 3553, loss 0.210731, acc 0.9375, prec 0.0736884, recall 0.900061
2017-12-10T15:52:49.928934: step 3554, loss 0.14956, acc 0.984375, prec 0.0737095, recall 0.900091
2017-12-10T15:52:50.130460: step 3555, loss 0.166598, acc 0.96875, prec 0.0737059, recall 0.900091
2017-12-10T15:52:50.325291: step 3556, loss 0.0614541, acc 0.96875, prec 0.0737022, recall 0.900091
2017-12-10T15:52:50.521043: step 3557, loss 0.193314, acc 0.953125, prec 0.0737197, recall 0.900121
2017-12-10T15:52:50.716336: step 3558, loss 0.0426659, acc 0.984375, prec 0.0737408, recall 0.900151
2017-12-10T15:52:50.916919: step 3559, loss 0.180691, acc 0.953125, prec 0.0737353, recall 0.900151
2017-12-10T15:52:51.118817: step 3560, loss 0.251497, acc 0.953125, prec 0.0737299, recall 0.900151
2017-12-10T15:52:51.316014: step 3561, loss 0.0988359, acc 0.96875, prec 0.0737492, recall 0.900181
2017-12-10T15:52:51.518132: step 3562, loss 0.510787, acc 0.9375, prec 0.0738107, recall 0.900272
2017-12-10T15:52:51.714871: step 3563, loss 0.235779, acc 0.921875, prec 0.0738016, recall 0.900272
2017-12-10T15:52:51.912408: step 3564, loss 0.221257, acc 0.921875, prec 0.0738383, recall 0.900332
2017-12-10T15:52:52.112346: step 3565, loss 0.0637914, acc 0.984375, prec 0.0738365, recall 0.900332
2017-12-10T15:52:52.309481: step 3566, loss 0.370117, acc 0.890625, prec 0.0738237, recall 0.900332
2017-12-10T15:52:52.506407: step 3567, loss 0.288284, acc 0.953125, prec 0.0738411, recall 0.900362
2017-12-10T15:52:52.703354: step 3568, loss 0.293255, acc 0.953125, prec 0.0739044, recall 0.900452
2017-12-10T15:52:52.899837: step 3569, loss 0.0752555, acc 1, prec 0.0739503, recall 0.900513
2017-12-10T15:52:53.097772: step 3570, loss 0.16918, acc 0.953125, prec 0.0739906, recall 0.900572
2017-12-10T15:52:53.303223: step 3571, loss 0.233027, acc 0.921875, prec 0.0739815, recall 0.900572
2017-12-10T15:52:53.499610: step 3572, loss 0.0819935, acc 0.96875, prec 0.0739778, recall 0.900572
2017-12-10T15:52:53.699639: step 3573, loss 0.12822, acc 0.953125, prec 0.0739723, recall 0.900572
2017-12-10T15:52:53.898868: step 3574, loss 0.228836, acc 0.953125, prec 0.0739898, recall 0.900602
2017-12-10T15:52:54.095663: step 3575, loss 0.0406615, acc 0.96875, prec 0.0740319, recall 0.900662
2017-12-10T15:52:54.296542: step 3576, loss 0.146352, acc 0.921875, prec 0.0740457, recall 0.900692
2017-12-10T15:52:54.496583: step 3577, loss 0.215967, acc 0.921875, prec 0.0740365, recall 0.900692
2017-12-10T15:52:54.695973: step 3578, loss 0.0535363, acc 1, prec 0.0740823, recall 0.900752
2017-12-10T15:52:54.891998: step 3579, loss 0.101203, acc 0.96875, prec 0.0740787, recall 0.900752
2017-12-10T15:52:55.093480: step 3580, loss 0.21611, acc 0.921875, prec 0.0740924, recall 0.900782
2017-12-10T15:52:55.292485: step 3581, loss 0.0905186, acc 0.953125, prec 0.0741098, recall 0.900812
2017-12-10T15:52:55.489872: step 3582, loss 0.074283, acc 0.96875, prec 0.0741061, recall 0.900812
2017-12-10T15:52:55.691581: step 3583, loss 0.218914, acc 0.96875, prec 0.0741254, recall 0.900841
2017-12-10T15:52:55.888767: step 3584, loss 0.103729, acc 0.96875, prec 0.0741446, recall 0.900871
2017-12-10T15:52:56.090052: step 3585, loss 0.00692264, acc 1, prec 0.0741446, recall 0.900871
2017-12-10T15:52:56.290794: step 3586, loss 0.0181841, acc 1, prec 0.0741446, recall 0.900871
2017-12-10T15:52:56.489812: step 3587, loss 0.118573, acc 0.984375, prec 0.0741885, recall 0.900931
2017-12-10T15:52:56.686908: step 3588, loss 0.0250885, acc 0.984375, prec 0.0742325, recall 0.90099
2017-12-10T15:52:56.888441: step 3589, loss 0.0126963, acc 1, prec 0.0742553, recall 0.90102
2017-12-10T15:52:57.083351: step 3590, loss 0.0464877, acc 0.984375, prec 0.0742764, recall 0.901049
2017-12-10T15:52:57.283248: step 3591, loss 0.0784779, acc 0.96875, prec 0.0742727, recall 0.901049
2017-12-10T15:52:57.478363: step 3592, loss 0.0868484, acc 0.984375, prec 0.0742938, recall 0.901079
2017-12-10T15:52:57.673273: step 3593, loss 0.0317, acc 0.96875, prec 0.0742901, recall 0.901079
2017-12-10T15:52:57.866310: step 3594, loss 0.0956346, acc 0.984375, prec 0.074334, recall 0.901138
2017-12-10T15:52:58.066766: step 3595, loss 0.158936, acc 1, prec 0.0743798, recall 0.901198
2017-12-10T15:52:58.263149: step 3596, loss 0.199872, acc 0.96875, prec 0.0743989, recall 0.901227
2017-12-10T15:52:58.461476: step 3597, loss 0.425899, acc 0.96875, prec 0.0744181, recall 0.901257
2017-12-10T15:52:58.664705: step 3598, loss 0.136816, acc 0.96875, prec 0.0744145, recall 0.901257
2017-12-10T15:52:58.865560: step 3599, loss 0.0762862, acc 0.984375, prec 0.0744126, recall 0.901257
2017-12-10T15:52:59.065382: step 3600, loss 0.110626, acc 0.96875, prec 0.0744547, recall 0.901316
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_3/1512938428/checkpoints/model-3600

2017-12-10T15:53:00.347532: step 3601, loss 0.0606432, acc 0.96875, prec 0.0744739, recall 0.901345
2017-12-10T15:53:00.547560: step 3602, loss 0.0160691, acc 1, prec 0.0744739, recall 0.901345
2017-12-10T15:53:00.748328: step 3603, loss 0.0350361, acc 0.984375, prec 0.0744949, recall 0.901375
2017-12-10T15:53:00.945576: step 3604, loss 0.381757, acc 0.984375, prec 0.0745616, recall 0.901463
2017-12-10T15:53:01.141414: step 3605, loss 0.0114264, acc 1, prec 0.0745616, recall 0.901463
2017-12-10T15:53:01.346358: step 3606, loss 0.102984, acc 0.96875, prec 0.0745579, recall 0.901463
2017-12-10T15:53:01.546157: step 3607, loss 0.0287709, acc 1, prec 0.0745579, recall 0.901463
2017-12-10T15:53:01.741886: step 3608, loss 0.154964, acc 1, prec 0.0746036, recall 0.901522
2017-12-10T15:53:01.942206: step 3609, loss 0.0241362, acc 1, prec 0.0746036, recall 0.901522
2017-12-10T15:53:02.141372: step 3610, loss 2.74414, acc 0.953125, prec 0.0746228, recall 0.901282
2017-12-10T15:53:02.344399: step 3611, loss 0.0968508, acc 1, prec 0.0746457, recall 0.901312
2017-12-10T15:53:02.542887: step 3612, loss 3.97766, acc 0.96875, prec 0.0746667, recall 0.901073
2017-12-10T15:53:02.748659: step 3613, loss 0.435238, acc 0.96875, prec 0.0747544, recall 0.90119
2017-12-10T15:53:02.949838: step 3614, loss 0.343085, acc 0.921875, prec 0.074768, recall 0.90122
2017-12-10T15:53:03.147114: step 3615, loss 0.144027, acc 0.953125, prec 0.0747853, recall 0.901249
2017-12-10T15:53:03.343734: step 3616, loss 0.316013, acc 0.890625, prec 0.0747952, recall 0.901279
2017-12-10T15:53:03.545013: step 3617, loss 0.234719, acc 0.921875, prec 0.0748088, recall 0.901308
2017-12-10T15:53:03.740778: step 3618, loss 0.397918, acc 0.875, prec 0.074794, recall 0.901308
2017-12-10T15:53:03.937154: step 3619, loss 0.432522, acc 0.859375, prec 0.0747774, recall 0.901308
2017-12-10T15:53:04.133850: step 3620, loss 0.297891, acc 0.875, prec 0.0747627, recall 0.901308
2017-12-10T15:53:04.332999: step 3621, loss 0.555388, acc 0.8125, prec 0.0747406, recall 0.901308
2017-12-10T15:53:04.530565: step 3622, loss 0.855326, acc 0.796875, prec 0.0747622, recall 0.901367
2017-12-10T15:53:04.727319: step 3623, loss 0.675444, acc 0.765625, prec 0.0747346, recall 0.901367
2017-12-10T15:53:04.921118: step 3624, loss 0.399429, acc 0.859375, prec 0.074718, recall 0.901367
2017-12-10T15:53:05.118169: step 3625, loss 0.511862, acc 0.796875, prec 0.0746941, recall 0.901367
2017-12-10T15:53:05.314032: step 3626, loss 0.371786, acc 0.84375, prec 0.0746985, recall 0.901396
2017-12-10T15:53:05.510127: step 3627, loss 0.416521, acc 0.859375, prec 0.0747047, recall 0.901425
2017-12-10T15:53:05.705943: step 3628, loss 0.442687, acc 0.875, prec 0.07469, recall 0.901425
2017-12-10T15:53:05.906692: step 3629, loss 0.230246, acc 0.90625, prec 0.0747018, recall 0.901454
2017-12-10T15:53:06.099766: step 3630, loss 0.0981745, acc 0.984375, prec 0.0747454, recall 0.901513
2017-12-10T15:53:06.299363: step 3631, loss 0.856634, acc 0.78125, prec 0.0747425, recall 0.901542
2017-12-10T15:53:06.504746: step 3632, loss 0.176751, acc 0.9375, prec 0.0747579, recall 0.901571
2017-12-10T15:53:06.705356: step 3633, loss 0.378745, acc 0.953125, prec 0.0748206, recall 0.901659
2017-12-10T15:53:06.906090: step 3634, loss 0.238463, acc 0.953125, prec 0.074815, recall 0.901659
2017-12-10T15:53:07.106332: step 3635, loss 0.183773, acc 0.9375, prec 0.0748304, recall 0.901688
2017-12-10T15:53:07.298611: step 3636, loss 0.205333, acc 0.9375, prec 0.0748231, recall 0.901688
2017-12-10T15:53:07.494895: step 3637, loss 0.228876, acc 0.9375, prec 0.0748385, recall 0.901717
2017-12-10T15:53:07.697830: step 3638, loss 0.0668316, acc 0.96875, prec 0.0748575, recall 0.901746
2017-12-10T15:53:07.896740: step 3639, loss 0.0801958, acc 0.96875, prec 0.0748766, recall 0.901775
2017-12-10T15:53:08.098497: step 3640, loss 2.32886, acc 0.9375, prec 0.074871, recall 0.901508
2017-12-10T15:53:08.301892: step 3641, loss 0.285665, acc 0.921875, prec 0.0748846, recall 0.901538
2017-12-10T15:53:08.500087: step 3642, loss 1.24839, acc 0.9375, prec 0.0749226, recall 0.901596
2017-12-10T15:53:08.702796: step 3643, loss 0.0259002, acc 1, prec 0.0749226, recall 0.901596
2017-12-10T15:53:08.901905: step 3644, loss 0.306119, acc 0.9375, prec 0.074938, recall 0.901625
2017-12-10T15:53:09.103875: step 3645, loss 0.205148, acc 0.90625, prec 0.074927, recall 0.901625
2017-12-10T15:53:09.303885: step 3646, loss 0.135671, acc 0.953125, prec 0.0749896, recall 0.901712
2017-12-10T15:53:09.499828: step 3647, loss 0.146333, acc 0.921875, prec 0.0750031, recall 0.901741
2017-12-10T15:53:09.701316: step 3648, loss 0.136553, acc 0.9375, prec 0.0749957, recall 0.901741
2017-12-10T15:53:09.898387: step 3649, loss 0.136505, acc 0.90625, prec 0.0749847, recall 0.901741
2017-12-10T15:53:10.099691: step 3650, loss 0.0845305, acc 0.96875, prec 0.074981, recall 0.901741
2017-12-10T15:53:10.299754: step 3651, loss 0.280298, acc 0.890625, prec 0.0750362, recall 0.901828
2017-12-10T15:53:10.501355: step 3652, loss 0.345686, acc 0.859375, prec 0.0750196, recall 0.901828
2017-12-10T15:53:10.700054: step 3653, loss 0.263748, acc 0.90625, prec 0.0750313, recall 0.901857
2017-12-10T15:53:10.901874: step 3654, loss 0.34497, acc 0.890625, prec 0.0750637, recall 0.901915
2017-12-10T15:53:11.101472: step 3655, loss 0.318809, acc 0.90625, prec 0.0750754, recall 0.901943
2017-12-10T15:53:11.298789: step 3656, loss 0.304932, acc 0.921875, prec 0.0751115, recall 0.902001
2017-12-10T15:53:11.499426: step 3657, loss 0.208784, acc 0.90625, prec 0.0751005, recall 0.902001
2017-12-10T15:53:11.695237: step 3658, loss 0.0548397, acc 0.984375, prec 0.0750986, recall 0.902001
2017-12-10T15:53:11.890646: step 3659, loss 0.157059, acc 0.9375, prec 0.0750913, recall 0.902001
2017-12-10T15:53:12.089124: step 3660, loss 0.468551, acc 0.859375, prec 0.0750974, recall 0.90203
2017-12-10T15:53:12.287342: step 3661, loss 0.381131, acc 0.921875, prec 0.0751561, recall 0.902116
2017-12-10T15:53:12.489814: step 3662, loss 0.116692, acc 0.984375, prec 0.0751996, recall 0.902174
2017-12-10T15:53:12.688876: step 3663, loss 0.121682, acc 0.953125, prec 0.0752393, recall 0.902231
2017-12-10T15:53:12.892613: step 3664, loss 0.160392, acc 0.921875, prec 0.0752301, recall 0.902231
2017-12-10T15:53:13.088609: step 3665, loss 0.266413, acc 0.921875, prec 0.0752209, recall 0.902231
2017-12-10T15:53:13.287693: step 3666, loss 0.0114064, acc 1, prec 0.0752435, recall 0.90226
2017-12-10T15:53:13.488612: step 3667, loss 0.0926842, acc 0.96875, prec 0.0752851, recall 0.902317
2017-12-10T15:53:13.685829: step 3668, loss 0.0290014, acc 0.984375, prec 0.0752833, recall 0.902317
2017-12-10T15:53:13.887772: step 3669, loss 0.350709, acc 0.90625, prec 0.0752949, recall 0.902346
2017-12-10T15:53:14.084614: step 3670, loss 1.02452, acc 1, prec 0.0753175, recall 0.902375
2017-12-10T15:53:14.294929: step 3671, loss 0.167743, acc 0.96875, prec 0.0753138, recall 0.902375
2017-12-10T15:53:14.496497: step 3672, loss 0.102885, acc 0.96875, prec 0.0753327, recall 0.902403
2017-12-10T15:53:14.696220: step 3673, loss 0.234643, acc 0.984375, prec 0.0753761, recall 0.90246
2017-12-10T15:53:14.900804: step 3674, loss 0.043267, acc 0.984375, prec 0.0753969, recall 0.902489
2017-12-10T15:53:15.099122: step 3675, loss 0.0931614, acc 0.96875, prec 0.0753932, recall 0.902489
2017-12-10T15:53:15.294914: step 3676, loss 0.363865, acc 0.921875, prec 0.0754066, recall 0.902518
2017-12-10T15:53:15.493099: step 3677, loss 0.121501, acc 0.96875, prec 0.0754029, recall 0.902518
2017-12-10T15:53:15.693093: step 3678, loss 0.316776, acc 0.921875, prec 0.0754163, recall 0.902546
2017-12-10T15:53:15.886738: step 3679, loss 0.198783, acc 0.96875, prec 0.0754126, recall 0.902546
2017-12-10T15:53:16.082560: step 3680, loss 0.84591, acc 0.90625, prec 0.0754694, recall 0.902632
2017-12-10T15:53:16.283216: step 3681, loss 0.0993048, acc 0.96875, prec 0.0754657, recall 0.902632
2017-12-10T15:53:16.486489: step 3682, loss 0.194868, acc 0.96875, prec 0.075462, recall 0.902632
2017-12-10T15:53:16.687443: step 3683, loss 0.317376, acc 0.96875, prec 0.0754583, recall 0.902632
2017-12-10T15:53:16.888429: step 3684, loss 0.155412, acc 0.96875, prec 0.0754998, recall 0.902689
2017-12-10T15:53:17.081236: step 3685, loss 0.161183, acc 0.953125, prec 0.0755169, recall 0.902717
2017-12-10T15:53:17.278019: step 3686, loss 0.0351828, acc 0.984375, prec 0.075515, recall 0.902717
2017-12-10T15:53:17.480695: step 3687, loss 0.261995, acc 0.96875, prec 0.0755565, recall 0.902774
2017-12-10T15:53:17.680294: step 3688, loss 0.160005, acc 0.953125, prec 0.0755736, recall 0.902802
2017-12-10T15:53:17.882133: step 3689, loss 0.0283907, acc 0.984375, prec 0.0755943, recall 0.90283
2017-12-10T15:53:18.079351: step 3690, loss 0.0845024, acc 0.96875, prec 0.0756358, recall 0.902887
2017-12-10T15:53:18.284487: step 3691, loss 0.282308, acc 0.9375, prec 0.075651, recall 0.902915
2017-12-10T15:53:18.486982: step 3692, loss 0.0665911, acc 0.96875, prec 0.0756473, recall 0.902915
2017-12-10T15:53:18.682494: step 3693, loss 0.153389, acc 0.96875, prec 0.0756436, recall 0.902915
2017-12-10T15:53:18.878331: step 3694, loss 0.0200047, acc 1, prec 0.0756662, recall 0.902944
2017-12-10T15:53:19.077301: step 3695, loss 0.108574, acc 1, prec 0.0756887, recall 0.902972
2017-12-10T15:53:19.282637: step 3696, loss 0.0594971, acc 0.984375, prec 0.0757095, recall 0.903
2017-12-10T15:53:19.484068: step 3697, loss 0.0262613, acc 0.984375, prec 0.0757076, recall 0.903
2017-12-10T15:53:19.681510: step 3698, loss 0.176271, acc 0.953125, prec 0.0757472, recall 0.903057
2017-12-10T15:53:19.884288: step 3699, loss 0.237648, acc 0.96875, prec 0.0757661, recall 0.903085
2017-12-10T15:53:20.085678: step 3700, loss 0.0102281, acc 1, prec 0.0757661, recall 0.903085
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_3/1512938428/checkpoints/model-3700

2017-12-10T15:53:21.283909: step 3701, loss 0.0250083, acc 0.984375, prec 0.0757642, recall 0.903085
2017-12-10T15:53:21.487529: step 3702, loss 0.27294, acc 0.953125, prec 0.0757587, recall 0.903085
2017-12-10T15:53:21.688241: step 3703, loss 0.11852, acc 0.984375, prec 0.0757568, recall 0.903085
2017-12-10T15:53:21.892280: step 3704, loss 0.0837695, acc 0.984375, prec 0.0757776, recall 0.903113
2017-12-10T15:53:22.088689: step 3705, loss 0.0551037, acc 0.96875, prec 0.0757739, recall 0.903113
2017-12-10T15:53:22.288252: step 3706, loss 0.101581, acc 0.96875, prec 0.0757701, recall 0.903113
2017-12-10T15:53:22.488081: step 3707, loss 0.119767, acc 0.96875, prec 0.075789, recall 0.903141
2017-12-10T15:53:22.689216: step 3708, loss 0.071804, acc 0.96875, prec 0.0758079, recall 0.90317
2017-12-10T15:53:22.889357: step 3709, loss 0.0343218, acc 1, prec 0.0758304, recall 0.903198
2017-12-10T15:53:23.086685: step 3710, loss 0.0957727, acc 0.96875, prec 0.0758718, recall 0.903254
2017-12-10T15:53:23.286343: step 3711, loss 0.152552, acc 0.953125, prec 0.0758888, recall 0.903282
2017-12-10T15:53:23.486410: step 3712, loss 0.106408, acc 0.984375, prec 0.0759321, recall 0.903338
2017-12-10T15:53:23.689906: step 3713, loss 0.0413335, acc 0.984375, prec 0.0759302, recall 0.903338
2017-12-10T15:53:23.885002: step 3714, loss 0.183628, acc 0.984375, prec 0.0759509, recall 0.903366
2017-12-10T15:53:24.085918: step 3715, loss 0.381367, acc 1, prec 0.0759735, recall 0.903394
2017-12-10T15:53:24.288653: step 3716, loss 0.364269, acc 0.953125, prec 0.0759904, recall 0.903422
2017-12-10T15:53:24.487553: step 3717, loss 2.04611, acc 0.96875, prec 0.0760111, recall 0.903188
2017-12-10T15:53:24.691068: step 3718, loss 1.72447, acc 0.96875, prec 0.0760093, recall 0.902927
2017-12-10T15:53:24.897155: step 3719, loss 0.08496, acc 0.96875, prec 0.0760056, recall 0.902927
2017-12-10T15:53:25.101600: step 3720, loss 0.0633917, acc 0.984375, prec 0.0760262, recall 0.902955
2017-12-10T15:53:25.303518: step 3721, loss 0.306541, acc 0.921875, prec 0.0760846, recall 0.903039
2017-12-10T15:53:25.498370: step 3722, loss 0.304181, acc 0.875, prec 0.0761148, recall 0.903095
2017-12-10T15:53:25.695200: step 3723, loss 0.341615, acc 0.859375, prec 0.0760981, recall 0.903095
2017-12-10T15:53:25.889033: step 3724, loss 0.364338, acc 0.875, prec 0.0761058, recall 0.903123
2017-12-10T15:53:26.088097: step 3725, loss 0.515051, acc 0.859375, prec 0.0761116, recall 0.903151
2017-12-10T15:53:26.286763: step 3726, loss 0.348339, acc 0.890625, prec 0.0761886, recall 0.903263
2017-12-10T15:53:26.489212: step 3727, loss 0.214127, acc 0.90625, prec 0.0762225, recall 0.903319
2017-12-10T15:53:26.683050: step 3728, loss 0.596959, acc 0.84375, prec 0.0762714, recall 0.903403
2017-12-10T15:53:26.878419: step 3729, loss 0.337883, acc 0.828125, prec 0.076251, recall 0.903403
2017-12-10T15:53:27.078774: step 3730, loss 0.19468, acc 0.9375, prec 0.0762435, recall 0.903403
2017-12-10T15:53:27.278533: step 3731, loss 0.569836, acc 0.8125, prec 0.0762438, recall 0.90343
2017-12-10T15:53:27.479506: step 3732, loss 0.486522, acc 0.875, prec 0.0762514, recall 0.903458
2017-12-10T15:53:27.675765: step 3733, loss 0.309164, acc 0.875, prec 0.0762366, recall 0.903458
2017-12-10T15:53:27.867955: step 3734, loss 0.630373, acc 0.875, prec 0.0762217, recall 0.903458
2017-12-10T15:53:28.063872: step 3735, loss 0.419481, acc 0.828125, prec 0.0762014, recall 0.903458
2017-12-10T15:53:28.262074: step 3736, loss 0.115226, acc 0.9375, prec 0.0762388, recall 0.903514
2017-12-10T15:53:28.456038: step 3737, loss 0.262659, acc 0.921875, prec 0.0762745, recall 0.903569
2017-12-10T15:53:28.650624: step 3738, loss 0.434038, acc 0.875, prec 0.0762821, recall 0.903597
2017-12-10T15:53:28.845506: step 3739, loss 0.157559, acc 0.90625, prec 0.076271, recall 0.903597
2017-12-10T15:53:29.050367: step 3740, loss 0.14069, acc 0.9375, prec 0.0762636, recall 0.903597
2017-12-10T15:53:29.251328: step 3741, loss 0.10493, acc 0.96875, prec 0.0762823, recall 0.903625
2017-12-10T15:53:29.457519: step 3742, loss 0.0137726, acc 1, prec 0.0762823, recall 0.903625
2017-12-10T15:53:29.654305: step 3743, loss 0.141292, acc 0.953125, prec 0.0762992, recall 0.903653
2017-12-10T15:53:29.852459: step 3744, loss 0.0733324, acc 0.984375, prec 0.0762973, recall 0.903653
2017-12-10T15:53:30.054051: step 3745, loss 0.0327533, acc 0.984375, prec 0.0763403, recall 0.903708
2017-12-10T15:53:30.251668: step 3746, loss 0.0756451, acc 0.96875, prec 0.0763366, recall 0.903708
2017-12-10T15:53:30.451943: step 3747, loss 0.106688, acc 0.921875, prec 0.0763274, recall 0.903708
2017-12-10T15:53:30.653643: step 3748, loss 0.0534957, acc 0.984375, prec 0.0763479, recall 0.903736
2017-12-10T15:53:30.849340: step 3749, loss 3.20616, acc 0.984375, prec 0.0763479, recall 0.903476
2017-12-10T15:53:31.053897: step 3750, loss 0.456009, acc 0.9375, prec 0.0764302, recall 0.903587
2017-12-10T15:53:31.254825: step 3751, loss 0.0983891, acc 0.96875, prec 0.0764937, recall 0.90367
2017-12-10T15:53:31.459624: step 3752, loss 1.06798, acc 1, prec 0.0765161, recall 0.903697
2017-12-10T15:53:31.661513: step 3753, loss 0.111411, acc 0.984375, prec 0.0765143, recall 0.903697
2017-12-10T15:53:31.857567: step 3754, loss 0.119239, acc 0.96875, prec 0.0765106, recall 0.903697
2017-12-10T15:53:32.055594: step 3755, loss 0.0501606, acc 0.984375, prec 0.0765759, recall 0.90378
2017-12-10T15:53:32.255684: step 3756, loss 0.107749, acc 0.9375, prec 0.0765685, recall 0.90378
2017-12-10T15:53:32.456083: step 3757, loss 0.125537, acc 0.953125, prec 0.0765853, recall 0.903808
2017-12-10T15:53:32.654663: step 3758, loss 2.41587, acc 0.9375, prec 0.0766469, recall 0.903632
2017-12-10T15:53:32.849607: step 3759, loss 0.0449297, acc 0.984375, prec 0.0766451, recall 0.903632
2017-12-10T15:53:33.043565: step 3760, loss 0.339887, acc 0.921875, prec 0.0766582, recall 0.903659
2017-12-10T15:53:33.239810: step 3761, loss 0.230656, acc 0.921875, prec 0.0766937, recall 0.903714
2017-12-10T15:53:33.437158: step 3762, loss 0.390381, acc 0.8125, prec 0.0766714, recall 0.903714
2017-12-10T15:53:33.632235: step 3763, loss 0.462033, acc 0.90625, prec 0.0767273, recall 0.903797
2017-12-10T15:53:33.826996: step 3764, loss 0.63049, acc 0.734375, prec 0.0767181, recall 0.903824
2017-12-10T15:53:34.022487: step 3765, loss 0.483394, acc 0.859375, prec 0.0767237, recall 0.903852
2017-12-10T15:53:34.219506: step 3766, loss 0.951487, acc 0.765625, prec 0.0767182, recall 0.903879
2017-12-10T15:53:34.419054: step 3767, loss 0.66636, acc 0.859375, prec 0.0767462, recall 0.903934
2017-12-10T15:53:34.614008: step 3768, loss 0.590568, acc 0.78125, prec 0.0767426, recall 0.903961
2017-12-10T15:53:34.809532: step 3769, loss 0.589109, acc 0.828125, prec 0.0767445, recall 0.903989
2017-12-10T15:53:35.007588: step 3770, loss 0.432445, acc 0.84375, prec 0.0767259, recall 0.903989
2017-12-10T15:53:35.208107: step 3771, loss 0.210442, acc 0.921875, prec 0.0767613, recall 0.904043
2017-12-10T15:53:35.407119: step 3772, loss 0.171313, acc 0.921875, prec 0.0767743, recall 0.904071
2017-12-10T15:53:35.605480: step 3773, loss 0.222707, acc 0.890625, prec 0.0767613, recall 0.904071
2017-12-10T15:53:35.797426: step 3774, loss 0.212619, acc 0.953125, prec 0.0767558, recall 0.904071
2017-12-10T15:53:35.994509: step 3775, loss 0.192835, acc 0.9375, prec 0.0768153, recall 0.904152
2017-12-10T15:53:36.193221: step 3776, loss 0.288646, acc 0.921875, prec 0.0768729, recall 0.904234
2017-12-10T15:53:36.391915: step 3777, loss 0.141386, acc 0.9375, prec 0.0769101, recall 0.904289
2017-12-10T15:53:36.589337: step 3778, loss 0.413737, acc 0.859375, prec 0.0768934, recall 0.904289
2017-12-10T15:53:36.784888: step 3779, loss 1.71168, acc 0.96875, prec 0.0768915, recall 0.904032
2017-12-10T15:53:36.982547: step 3780, loss 0.088646, acc 0.984375, prec 0.0768896, recall 0.904032
2017-12-10T15:53:37.183658: step 3781, loss 0.188303, acc 0.96875, prec 0.0768859, recall 0.904032
2017-12-10T15:53:37.383302: step 3782, loss 0.318915, acc 0.96875, prec 0.0769045, recall 0.904059
2017-12-10T15:53:37.587132: step 3783, loss 0.122344, acc 0.9375, prec 0.0769194, recall 0.904086
2017-12-10T15:53:37.787144: step 3784, loss 0.142751, acc 0.9375, prec 0.0769342, recall 0.904113
2017-12-10T15:53:37.987708: step 3785, loss 0.0557189, acc 0.984375, prec 0.0769546, recall 0.904141
2017-12-10T15:53:38.185969: step 3786, loss 0.0138239, acc 1, prec 0.0769546, recall 0.904141
2017-12-10T15:53:38.383915: step 3787, loss 0.242042, acc 0.9375, prec 0.0769472, recall 0.904141
2017-12-10T15:53:38.578601: step 3788, loss 0.148594, acc 0.921875, prec 0.0769379, recall 0.904141
2017-12-10T15:53:38.775956: step 3789, loss 0.0429351, acc 0.96875, prec 0.0769342, recall 0.904141
2017-12-10T15:53:38.972461: step 3790, loss 0.10607, acc 0.96875, prec 0.076975, recall 0.904195
2017-12-10T15:53:39.170816: step 3791, loss 0.128207, acc 0.96875, prec 0.0769713, recall 0.904195
2017-12-10T15:53:39.366371: step 3792, loss 0.256214, acc 0.96875, prec 0.0769899, recall 0.904222
2017-12-10T15:53:39.564568: step 3793, loss 0.881181, acc 0.96875, prec 0.0770307, recall 0.904276
2017-12-10T15:53:39.764282: step 3794, loss 0.10957, acc 0.9375, prec 0.0770233, recall 0.904276
2017-12-10T15:53:39.963712: step 3795, loss 0.126363, acc 0.984375, prec 0.0770437, recall 0.904303
2017-12-10T15:53:40.162490: step 3796, loss 0.370901, acc 0.984375, prec 0.0771086, recall 0.904385
2017-12-10T15:53:40.360187: step 3797, loss 0.180004, acc 0.96875, prec 0.0771494, recall 0.904439
2017-12-10T15:53:40.560878: step 3798, loss 0.394887, acc 0.953125, prec 0.0771438, recall 0.904439
2017-12-10T15:53:40.753929: step 3799, loss 0.203009, acc 0.9375, prec 0.0771586, recall 0.904466
2017-12-10T15:53:40.956989: step 3800, loss 0.334771, acc 0.953125, prec 0.077153, recall 0.904466
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_3/1512938428/checkpoints/model-3800

2017-12-10T15:53:42.132115: step 3801, loss 0.308869, acc 0.90625, prec 0.0771419, recall 0.904466
2017-12-10T15:53:42.330026: step 3802, loss 0.131622, acc 0.9375, prec 0.0771345, recall 0.904466
2017-12-10T15:53:42.529390: step 3803, loss 0.128845, acc 0.9375, prec 0.0771715, recall 0.90452
2017-12-10T15:53:42.730604: step 3804, loss 0.258063, acc 0.921875, prec 0.0771844, recall 0.904547
2017-12-10T15:53:42.929171: step 3805, loss 0.165854, acc 0.953125, prec 0.0772011, recall 0.904574
2017-12-10T15:53:43.127048: step 3806, loss 0.0231108, acc 0.984375, prec 0.0771992, recall 0.904574
2017-12-10T15:53:43.322716: step 3807, loss 0.0600024, acc 0.984375, prec 0.0771974, recall 0.904574
2017-12-10T15:53:43.523267: step 3808, loss 0.0521801, acc 0.984375, prec 0.07724, recall 0.904628
2017-12-10T15:53:43.729785: step 3809, loss 0.717262, acc 0.9375, prec 0.077277, recall 0.904681
2017-12-10T15:53:43.930560: step 3810, loss 0.206271, acc 0.96875, prec 0.0772733, recall 0.904681
2017-12-10T15:53:44.132766: step 3811, loss 0.435913, acc 0.90625, prec 0.0772621, recall 0.904681
2017-12-10T15:53:44.331085: step 3812, loss 0.0438844, acc 0.984375, prec 0.0772602, recall 0.904681
2017-12-10T15:53:44.538382: step 3813, loss 0.18314, acc 0.953125, prec 0.0772769, recall 0.904708
2017-12-10T15:53:44.736183: step 3814, loss 0.195185, acc 0.921875, prec 0.0772898, recall 0.904735
2017-12-10T15:53:44.937763: step 3815, loss 0.021256, acc 1, prec 0.077312, recall 0.904762
2017-12-10T15:53:45.133847: step 3816, loss 0.0237781, acc 1, prec 0.077312, recall 0.904762
2017-12-10T15:53:45.333576: step 3817, loss 0.178935, acc 0.953125, prec 0.0773509, recall 0.904816
2017-12-10T15:53:45.533925: step 3818, loss 0.142198, acc 0.96875, prec 0.0773693, recall 0.904842
2017-12-10T15:53:45.732514: step 3819, loss 0.24335, acc 0.890625, prec 0.0773563, recall 0.904842
2017-12-10T15:53:45.926701: step 3820, loss 0.115017, acc 0.96875, prec 0.0773748, recall 0.904869
2017-12-10T15:53:46.125929: step 3821, loss 0.0492967, acc 0.984375, prec 0.0774395, recall 0.904949
2017-12-10T15:53:46.329544: step 3822, loss 0.253334, acc 0.96875, prec 0.077458, recall 0.904976
2017-12-10T15:53:46.532569: step 3823, loss 0.0360969, acc 1, prec 0.0774802, recall 0.905003
2017-12-10T15:53:46.725075: step 3824, loss 0.036551, acc 1, prec 0.0775024, recall 0.905029
2017-12-10T15:53:46.921722: step 3825, loss 0.0945319, acc 0.953125, prec 0.077519, recall 0.905056
2017-12-10T15:53:47.123005: step 3826, loss 0.177587, acc 0.96875, prec 0.0775375, recall 0.905083
2017-12-10T15:53:47.321609: step 3827, loss 0.00833829, acc 1, prec 0.0775375, recall 0.905083
2017-12-10T15:53:47.523776: step 3828, loss 0.073808, acc 0.96875, prec 0.0775781, recall 0.905136
2017-12-10T15:53:47.731680: step 3829, loss 0.0191995, acc 0.984375, prec 0.0775762, recall 0.905136
2017-12-10T15:53:47.929757: step 3830, loss 0.0313289, acc 0.984375, prec 0.0776188, recall 0.905189
2017-12-10T15:53:48.129221: step 3831, loss 0.0695718, acc 0.984375, prec 0.0776391, recall 0.905216
2017-12-10T15:53:48.331367: step 3832, loss 0.0169076, acc 0.984375, prec 0.0776372, recall 0.905216
2017-12-10T15:53:48.532311: step 3833, loss 0.0251008, acc 1, prec 0.0776594, recall 0.905243
2017-12-10T15:53:48.730447: step 3834, loss 0.146167, acc 0.96875, prec 0.0777, recall 0.905296
2017-12-10T15:53:48.933430: step 3835, loss 0.0738784, acc 0.984375, prec 0.0776982, recall 0.905296
2017-12-10T15:53:49.132639: step 3836, loss 0.301148, acc 0.953125, prec 0.0777369, recall 0.905349
2017-12-10T15:53:49.334670: step 3837, loss 0.140328, acc 0.984375, prec 0.077735, recall 0.905349
2017-12-10T15:53:49.537334: step 3838, loss 0.174559, acc 0.96875, prec 0.0777313, recall 0.905349
2017-12-10T15:53:49.744628: step 3839, loss 0.0320224, acc 0.984375, prec 0.0777294, recall 0.905349
2017-12-10T15:53:49.944128: step 3840, loss 0.0935348, acc 0.984375, prec 0.0777497, recall 0.905375
2017-12-10T15:53:50.142592: step 3841, loss 0.777086, acc 0.96875, prec 0.0778125, recall 0.905455
2017-12-10T15:53:50.344703: step 3842, loss 0.542597, acc 0.96875, prec 0.0778309, recall 0.905481
2017-12-10T15:53:50.545514: step 3843, loss 0.507611, acc 1, prec 0.0778753, recall 0.905534
2017-12-10T15:53:50.748198: step 3844, loss 0.0199493, acc 1, prec 0.0778753, recall 0.905534
2017-12-10T15:53:50.946287: step 3845, loss 0.055857, acc 0.96875, prec 0.0778715, recall 0.905534
2017-12-10T15:53:51.142735: step 3846, loss 0.0144153, acc 1, prec 0.0778937, recall 0.90556
2017-12-10T15:53:51.339601: step 3847, loss 0.34912, acc 0.953125, prec 0.0778881, recall 0.90556
2017-12-10T15:53:51.535954: step 3848, loss 2.32717, acc 0.953125, prec 0.0779065, recall 0.905334
2017-12-10T15:53:51.737387: step 3849, loss 0.192926, acc 0.9375, prec 0.0779211, recall 0.90536
2017-12-10T15:53:51.933133: step 3850, loss 0.363568, acc 0.875, prec 0.0779062, recall 0.90536
2017-12-10T15:53:52.129971: step 3851, loss 0.677572, acc 0.875, prec 0.0778912, recall 0.90536
2017-12-10T15:53:52.327436: step 3852, loss 0.443288, acc 0.890625, prec 0.0778781, recall 0.90536
2017-12-10T15:53:52.523935: step 3853, loss 0.558453, acc 0.796875, prec 0.0778538, recall 0.90536
2017-12-10T15:53:52.718670: step 3854, loss 0.370091, acc 0.875, prec 0.077861, recall 0.905387
2017-12-10T15:53:52.917477: step 3855, loss 0.60145, acc 0.78125, prec 0.077857, recall 0.905413
2017-12-10T15:53:53.119039: step 3856, loss 0.561504, acc 0.921875, prec 0.0778919, recall 0.905466
2017-12-10T15:53:53.313589: step 3857, loss 0.798997, acc 0.75, prec 0.0779062, recall 0.905518
2017-12-10T15:53:53.508469: step 3858, loss 0.691331, acc 0.78125, prec 0.0779022, recall 0.905545
2017-12-10T15:53:53.708345: step 3859, loss 0.586259, acc 0.8125, prec 0.0779018, recall 0.905571
2017-12-10T15:53:53.904733: step 3860, loss 0.798526, acc 0.78125, prec 0.0778757, recall 0.905571
2017-12-10T15:53:54.100158: step 3861, loss 0.333822, acc 0.921875, prec 0.0779106, recall 0.905624
2017-12-10T15:53:54.302644: step 3862, loss 0.458457, acc 0.84375, prec 0.0779361, recall 0.905676
2017-12-10T15:53:54.499646: step 3863, loss 0.181557, acc 0.921875, prec 0.0779709, recall 0.905729
2017-12-10T15:53:54.695341: step 3864, loss 0.46663, acc 0.921875, prec 0.0780057, recall 0.905781
2017-12-10T15:53:54.890812: step 3865, loss 0.247571, acc 0.890625, prec 0.0780147, recall 0.905807
2017-12-10T15:53:55.085325: step 3866, loss 0.219687, acc 0.953125, prec 0.0780091, recall 0.905807
2017-12-10T15:53:55.287622: step 3867, loss 0.124147, acc 0.9375, prec 0.0780016, recall 0.905807
2017-12-10T15:53:55.490127: step 3868, loss 0.076137, acc 0.96875, prec 0.07802, recall 0.905833
2017-12-10T15:53:55.688900: step 3869, loss 0.364678, acc 0.953125, prec 0.0780585, recall 0.905886
2017-12-10T15:53:55.890438: step 3870, loss 0.238652, acc 0.921875, prec 0.0780712, recall 0.905912
2017-12-10T15:53:56.083229: step 3871, loss 0.147406, acc 0.9375, prec 0.0781299, recall 0.90599
2017-12-10T15:53:56.276646: step 3872, loss 0.212431, acc 1, prec 0.0781519, recall 0.906016
2017-12-10T15:53:56.476763: step 3873, loss 0.0392832, acc 1, prec 0.078174, recall 0.906042
2017-12-10T15:53:56.673636: step 3874, loss 0.222985, acc 0.953125, prec 0.0781904, recall 0.906068
2017-12-10T15:53:56.871329: step 3875, loss 0.171823, acc 0.96875, prec 0.0781866, recall 0.906068
2017-12-10T15:53:57.068362: step 3876, loss 0.834581, acc 0.953125, prec 0.0782472, recall 0.906146
2017-12-10T15:53:57.270764: step 3877, loss 0.0810033, acc 0.984375, prec 0.0782453, recall 0.906146
2017-12-10T15:53:57.473790: step 3878, loss 0.387717, acc 0.9375, prec 0.0782819, recall 0.906198
2017-12-10T15:53:57.670141: step 3879, loss 0.185252, acc 0.921875, prec 0.0782725, recall 0.906198
2017-12-10T15:53:57.862702: step 3880, loss 0.35008, acc 0.90625, prec 0.0782833, recall 0.906224
2017-12-10T15:53:58.056628: step 3881, loss 0.142319, acc 0.953125, prec 0.0782997, recall 0.90625
2017-12-10T15:53:58.256244: step 3882, loss 0.20438, acc 0.953125, prec 0.0783161, recall 0.906276
2017-12-10T15:53:58.461245: step 3883, loss 0.292274, acc 0.921875, prec 0.0783288, recall 0.906302
2017-12-10T15:53:58.660417: step 3884, loss 0.0305892, acc 0.984375, prec 0.0783489, recall 0.906328
2017-12-10T15:53:58.860305: step 3885, loss 0.112539, acc 0.96875, prec 0.0783452, recall 0.906328
2017-12-10T15:53:59.068129: step 3886, loss 0.0591623, acc 0.96875, prec 0.0783415, recall 0.906328
2017-12-10T15:53:59.270471: step 3887, loss 0.0909098, acc 0.96875, prec 0.0783377, recall 0.906328
2017-12-10T15:53:59.479599: step 3888, loss 0.0815675, acc 0.96875, prec 0.078378, recall 0.906379
2017-12-10T15:53:59.676804: step 3889, loss 0.0718247, acc 0.96875, prec 0.0783742, recall 0.906379
2017-12-10T15:53:59.878690: step 3890, loss 0.00924484, acc 1, prec 0.0783742, recall 0.906379
2017-12-10T15:54:00.075291: step 3891, loss 0.0477824, acc 0.984375, prec 0.0783944, recall 0.906405
2017-12-10T15:54:00.270943: step 3892, loss 0.137449, acc 0.953125, prec 0.0783888, recall 0.906405
2017-12-10T15:54:00.477263: step 3893, loss 0.181066, acc 0.9375, prec 0.0783813, recall 0.906405
2017-12-10T15:54:00.674653: step 3894, loss 0.216387, acc 0.9375, prec 0.0784178, recall 0.906457
2017-12-10T15:54:00.876295: step 3895, loss 0.0504497, acc 0.984375, prec 0.0784379, recall 0.906483
2017-12-10T15:54:01.072774: step 3896, loss 0.0413722, acc 0.984375, prec 0.0784361, recall 0.906483
2017-12-10T15:54:01.272810: step 3897, loss 0.0430746, acc 0.96875, prec 0.0784323, recall 0.906483
2017-12-10T15:54:01.472739: step 3898, loss 0.0367764, acc 0.96875, prec 0.0784506, recall 0.906509
2017-12-10T15:54:01.672365: step 3899, loss 0.0701676, acc 0.953125, prec 0.0784889, recall 0.90656
2017-12-10T15:54:01.869054: step 3900, loss 0.0375093, acc 0.984375, prec 0.0784871, recall 0.90656
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_3/1512938428/checkpoints/model-3900

2017-12-10T15:54:03.235566: step 3901, loss 0.0892306, acc 0.984375, prec 0.0785072, recall 0.906586
2017-12-10T15:54:03.437026: step 3902, loss 0.497724, acc 1, prec 0.0785511, recall 0.906637
2017-12-10T15:54:03.640962: step 3903, loss 0.0587635, acc 0.96875, prec 0.0785694, recall 0.906663
2017-12-10T15:54:03.836194: step 3904, loss 0.0945142, acc 0.96875, prec 0.0785656, recall 0.906663
2017-12-10T15:54:04.031365: step 3905, loss 0.644931, acc 1, prec 0.0785876, recall 0.906689
2017-12-10T15:54:04.230700: step 3906, loss 0.270459, acc 0.921875, prec 0.0785782, recall 0.906689
2017-12-10T15:54:04.427806: step 3907, loss 0.0334665, acc 0.984375, prec 0.0786203, recall 0.90674
2017-12-10T15:54:04.627079: step 3908, loss 0.523532, acc 0.96875, prec 0.0787045, recall 0.906843
2017-12-10T15:54:04.830781: step 3909, loss 0.105736, acc 0.953125, prec 0.0787208, recall 0.906868
2017-12-10T15:54:05.027171: step 3910, loss 0.0832511, acc 0.96875, prec 0.078739, recall 0.906894
2017-12-10T15:54:05.225367: step 3911, loss 0.0799855, acc 0.984375, prec 0.0787591, recall 0.906919
2017-12-10T15:54:05.425416: step 3912, loss 0.134327, acc 0.96875, prec 0.0787554, recall 0.906919
2017-12-10T15:54:05.622617: step 3913, loss 0.114313, acc 0.953125, prec 0.0787937, recall 0.90697
2017-12-10T15:54:05.824331: step 3914, loss 0.0735298, acc 0.96875, prec 0.0788119, recall 0.906996
2017-12-10T15:54:06.025098: step 3915, loss 0.66396, acc 0.96875, prec 0.0788301, recall 0.907021
2017-12-10T15:54:06.225340: step 3916, loss 0.0613819, acc 0.96875, prec 0.0788702, recall 0.907072
2017-12-10T15:54:06.426829: step 3917, loss 1.94497, acc 0.96875, prec 0.0789122, recall 0.906875
2017-12-10T15:54:06.628181: step 3918, loss 0.173275, acc 0.9375, prec 0.0789047, recall 0.906875
2017-12-10T15:54:06.826871: step 3919, loss 0.285013, acc 0.90625, prec 0.0789593, recall 0.906951
2017-12-10T15:54:07.025802: step 3920, loss 0.398256, acc 0.90625, prec 0.078948, recall 0.906951
2017-12-10T15:54:07.221942: step 3921, loss 0.161978, acc 0.984375, prec 0.0789461, recall 0.906951
2017-12-10T15:54:07.418168: step 3922, loss 0.122076, acc 0.953125, prec 0.0789624, recall 0.906977
2017-12-10T15:54:07.618408: step 3923, loss 0.20848, acc 0.921875, prec 0.0789749, recall 0.907002
2017-12-10T15:54:07.813411: step 3924, loss 0.348021, acc 0.90625, prec 0.0789637, recall 0.907002
2017-12-10T15:54:08.014112: step 3925, loss 0.180675, acc 0.921875, prec 0.0789762, recall 0.907028
2017-12-10T15:54:08.211398: step 3926, loss 0.474978, acc 0.890625, prec 0.078985, recall 0.907053
2017-12-10T15:54:08.419957: step 3927, loss 0.24074, acc 0.859375, prec 0.078968, recall 0.907053
2017-12-10T15:54:08.614960: step 3928, loss 0.156354, acc 0.9375, prec 0.0789824, recall 0.907078
2017-12-10T15:54:08.813321: step 3929, loss 0.359883, acc 0.875, prec 0.0789674, recall 0.907078
2017-12-10T15:54:09.012992: step 3930, loss 0.0962906, acc 0.984375, prec 0.0789655, recall 0.907078
2017-12-10T15:54:09.208701: step 3931, loss 0.136198, acc 0.953125, prec 0.0789818, recall 0.907104
2017-12-10T15:54:09.405255: step 3932, loss 0.204116, acc 0.921875, prec 0.0789943, recall 0.907129
2017-12-10T15:54:09.603072: step 3933, loss 0.221763, acc 0.9375, prec 0.0789868, recall 0.907129
2017-12-10T15:54:09.805519: step 3934, loss 0.0980635, acc 0.953125, prec 0.0789812, recall 0.907129
2017-12-10T15:54:10.004736: step 3935, loss 0.338235, acc 0.921875, prec 0.0789718, recall 0.907129
2017-12-10T15:54:10.211378: step 3936, loss 0.345837, acc 0.921875, prec 0.0790062, recall 0.90718
2017-12-10T15:54:10.410219: step 3937, loss 0.499675, acc 0.890625, prec 0.078993, recall 0.90718
2017-12-10T15:54:10.611411: step 3938, loss 0.15919, acc 0.953125, prec 0.0790312, recall 0.907231
2017-12-10T15:54:10.809932: step 3939, loss 0.313697, acc 0.921875, prec 0.0790218, recall 0.907231
2017-12-10T15:54:11.002457: step 3940, loss 0.0887289, acc 0.984375, prec 0.0790418, recall 0.907256
2017-12-10T15:54:11.203228: step 3941, loss 0.0392958, acc 1, prec 0.0791075, recall 0.907332
2017-12-10T15:54:11.398144: step 3942, loss 0.0330874, acc 0.984375, prec 0.0791275, recall 0.907357
2017-12-10T15:54:11.600095: step 3943, loss 0.40542, acc 0.984375, prec 0.0791693, recall 0.907407
2017-12-10T15:54:11.802721: step 3944, loss 0.220419, acc 0.96875, prec 0.0791656, recall 0.907407
2017-12-10T15:54:12.002649: step 3945, loss 0.0644561, acc 0.96875, prec 0.0791837, recall 0.907433
2017-12-10T15:54:12.202267: step 3946, loss 0.0100479, acc 1, prec 0.0791837, recall 0.907433
2017-12-10T15:54:12.397364: step 3947, loss 0.0646949, acc 0.984375, prec 0.0792037, recall 0.907458
2017-12-10T15:54:12.599729: step 3948, loss 0.0250798, acc 1, prec 0.0792256, recall 0.907483
2017-12-10T15:54:12.799565: step 3949, loss 0.0222523, acc 1, prec 0.0792693, recall 0.907533
2017-12-10T15:54:12.997285: step 3950, loss 0.171975, acc 0.984375, prec 0.0793112, recall 0.907584
2017-12-10T15:54:13.201422: step 3951, loss 0.0182411, acc 0.984375, prec 0.0793093, recall 0.907584
2017-12-10T15:54:13.397934: step 3952, loss 0.16253, acc 0.96875, prec 0.0793055, recall 0.907584
2017-12-10T15:54:13.600648: step 3953, loss 0.0830594, acc 0.984375, prec 0.0793255, recall 0.907609
2017-12-10T15:54:13.799029: step 3954, loss 0.135811, acc 0.96875, prec 0.0793217, recall 0.907609
2017-12-10T15:54:14.008228: step 3955, loss 0.345829, acc 0.953125, prec 0.0793379, recall 0.907634
2017-12-10T15:54:14.208321: step 3956, loss 3.98542, acc 0.96875, prec 0.0793361, recall 0.907387
2017-12-10T15:54:14.419157: step 3957, loss 0.355361, acc 0.96875, prec 0.0793979, recall 0.907463
2017-12-10T15:54:14.620773: step 3958, loss 0.044882, acc 0.984375, prec 0.079396, recall 0.907463
2017-12-10T15:54:14.819292: step 3959, loss 0.246538, acc 0.9375, prec 0.0794322, recall 0.907513
2017-12-10T15:54:15.021262: step 3960, loss 0.264605, acc 0.90625, prec 0.0794427, recall 0.907538
2017-12-10T15:54:15.222650: step 3961, loss 0.38416, acc 0.921875, prec 0.0794551, recall 0.907563
2017-12-10T15:54:15.425940: step 3962, loss 0.165475, acc 0.90625, prec 0.0794438, recall 0.907563
2017-12-10T15:54:15.627781: step 3963, loss 0.169707, acc 0.921875, prec 0.0794344, recall 0.907563
2017-12-10T15:54:15.830473: step 3964, loss 0.126094, acc 0.9375, prec 0.0794268, recall 0.907563
2017-12-10T15:54:16.029600: step 3965, loss 0.511743, acc 0.890625, prec 0.0794355, recall 0.907588
2017-12-10T15:54:16.225332: step 3966, loss 0.258386, acc 0.9375, prec 0.0794498, recall 0.907613
2017-12-10T15:54:16.420893: step 3967, loss 0.465632, acc 0.890625, prec 0.0795239, recall 0.907713
2017-12-10T15:54:16.619140: step 3968, loss 0.359055, acc 0.921875, prec 0.0795363, recall 0.907738
2017-12-10T15:54:16.819629: step 3969, loss 0.12945, acc 0.921875, prec 0.0795269, recall 0.907738
2017-12-10T15:54:17.017286: step 3970, loss 0.303184, acc 0.921875, prec 0.0795174, recall 0.907738
2017-12-10T15:54:17.215317: step 3971, loss 0.637331, acc 0.875, prec 0.079546, recall 0.907788
2017-12-10T15:54:17.417464: step 3972, loss 0.357487, acc 0.90625, prec 0.0795565, recall 0.907813
2017-12-10T15:54:17.614086: step 3973, loss 0.241521, acc 0.9375, prec 0.079549, recall 0.907813
2017-12-10T15:54:17.806790: step 3974, loss 0.440509, acc 0.90625, prec 0.0796031, recall 0.907888
2017-12-10T15:54:18.003529: step 3975, loss 0.364373, acc 0.90625, prec 0.0795917, recall 0.907888
2017-12-10T15:54:18.182780: step 3976, loss 0.660439, acc 0.961538, prec 0.0796316, recall 0.907937
2017-12-10T15:54:18.400063: step 3977, loss 0.19506, acc 0.921875, prec 0.0796875, recall 0.908012
2017-12-10T15:54:18.599675: step 3978, loss 0.305839, acc 0.859375, prec 0.0796705, recall 0.908012
2017-12-10T15:54:18.796143: step 3979, loss 0.301803, acc 0.921875, prec 0.0797047, recall 0.908061
2017-12-10T15:54:18.993800: step 3980, loss 0.444918, acc 0.875, prec 0.0796896, recall 0.908061
2017-12-10T15:54:19.192391: step 3981, loss 0.104888, acc 0.96875, prec 0.0797511, recall 0.908136
2017-12-10T15:54:19.392310: step 3982, loss 0.166888, acc 0.9375, prec 0.0797436, recall 0.908136
2017-12-10T15:54:19.589829: step 3983, loss 0.110255, acc 0.9375, prec 0.0797578, recall 0.908161
2017-12-10T15:54:19.786205: step 3984, loss 0.175799, acc 0.9375, prec 0.0797502, recall 0.908161
2017-12-10T15:54:19.988596: step 3985, loss 0.0932568, acc 0.984375, prec 0.0797701, recall 0.908185
2017-12-10T15:54:20.190367: step 3986, loss 0.392163, acc 0.953125, prec 0.079808, recall 0.908235
2017-12-10T15:54:20.386342: step 3987, loss 0.189166, acc 0.921875, prec 0.0797985, recall 0.908235
2017-12-10T15:54:20.581367: step 3988, loss 0.412326, acc 0.921875, prec 0.0798326, recall 0.908284
2017-12-10T15:54:20.782693: step 3989, loss 0.252489, acc 0.90625, prec 0.0798648, recall 0.908333
2017-12-10T15:54:20.982054: step 3990, loss 0.0293569, acc 1, prec 0.0798865, recall 0.908358
2017-12-10T15:54:21.186408: step 3991, loss 0.192931, acc 0.921875, prec 0.0798989, recall 0.908383
2017-12-10T15:54:21.403766: step 3992, loss 0.271591, acc 0.890625, prec 0.0798856, recall 0.908383
2017-12-10T15:54:21.604554: step 3993, loss 0.0380283, acc 0.984375, prec 0.0799055, recall 0.908407
2017-12-10T15:54:21.801229: step 3994, loss 0.0487595, acc 0.984375, prec 0.0799253, recall 0.908432
2017-12-10T15:54:22.002250: step 3995, loss 0.182552, acc 0.9375, prec 0.0799613, recall 0.908481
2017-12-10T15:54:22.201329: step 3996, loss 0.0239608, acc 0.984375, prec 0.0800028, recall 0.90853
2017-12-10T15:54:22.404516: step 3997, loss 0.119544, acc 0.953125, prec 0.0799972, recall 0.90853
2017-12-10T15:54:22.606121: step 3998, loss 0.0771755, acc 0.96875, prec 0.0800368, recall 0.908579
2017-12-10T15:54:22.804515: step 3999, loss 0.117109, acc 0.96875, prec 0.0800548, recall 0.908604
2017-12-10T15:54:23.003203: step 4000, loss 0.0455947, acc 0.984375, prec 0.0800529, recall 0.908604

Evaluation:
2017-12-10T15:54:27.560551: step 4000, loss 4.44348, acc 0.966597, prec 0.0805067, recall 0.88317

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_3/1512938428/checkpoints/model-4000

2017-12-10T15:54:28.828063: step 4001, loss 0.203723, acc 0.984375, prec 0.0805911, recall 0.88329
2017-12-10T15:54:29.026982: step 4002, loss 0.0750643, acc 0.96875, prec 0.0806088, recall 0.883321
2017-12-10T15:54:29.240662: step 4003, loss 0.238012, acc 0.9375, prec 0.0806013, recall 0.883321
2017-12-10T15:54:29.451255: step 4004, loss 0.0254258, acc 0.984375, prec 0.0805994, recall 0.883321
2017-12-10T15:54:29.648796: step 4005, loss 0.0823931, acc 0.96875, prec 0.0806172, recall 0.88335
2017-12-10T15:54:29.846136: step 4006, loss 0.0947593, acc 0.984375, prec 0.0806368, recall 0.88338
2017-12-10T15:54:30.048704: step 4007, loss 1.16639, acc 0.96875, prec 0.080635, recall 0.883154
2017-12-10T15:54:30.246835: step 4008, loss 0.0346918, acc 1, prec 0.0806565, recall 0.883184
2017-12-10T15:54:30.444424: step 4009, loss 0.0100148, acc 1, prec 0.0806565, recall 0.883184
2017-12-10T15:54:30.645682: step 4010, loss 0.0461733, acc 0.984375, prec 0.0806546, recall 0.883184
2017-12-10T15:54:30.850139: step 4011, loss 0.259943, acc 0.984375, prec 0.0806527, recall 0.883184
2017-12-10T15:54:31.048458: step 4012, loss 0.111671, acc 0.96875, prec 0.0806705, recall 0.883214
2017-12-10T15:54:31.256035: step 4013, loss 0.0633861, acc 0.96875, prec 0.0806883, recall 0.883244
2017-12-10T15:54:31.458267: step 4014, loss 0.165563, acc 0.9375, prec 0.0806807, recall 0.883244
2017-12-10T15:54:31.656752: step 4015, loss 0.288418, acc 0.96875, prec 0.0806985, recall 0.883273
2017-12-10T15:54:31.853912: step 4016, loss 0.153783, acc 0.984375, prec 0.0807612, recall 0.883363
2017-12-10T15:54:32.052142: step 4017, loss 0.163641, acc 0.953125, prec 0.0807555, recall 0.883363
2017-12-10T15:54:32.254225: step 4018, loss 0.923056, acc 0.953125, prec 0.0807714, recall 0.883393
2017-12-10T15:54:32.459683: step 4019, loss 0.104967, acc 0.96875, prec 0.0807891, recall 0.883423
2017-12-10T15:54:32.655628: step 4020, loss 0.0586402, acc 0.96875, prec 0.0807854, recall 0.883423
2017-12-10T15:54:32.856697: step 4021, loss 0.156484, acc 0.984375, prec 0.080805, recall 0.883453
2017-12-10T15:54:33.054241: step 4022, loss 2.01092, acc 0.90625, prec 0.0808171, recall 0.883257
2017-12-10T15:54:33.253745: step 4023, loss 0.182911, acc 0.9375, prec 0.0808095, recall 0.883257
2017-12-10T15:54:33.450584: step 4024, loss 0.334477, acc 0.90625, prec 0.0808197, recall 0.883286
2017-12-10T15:54:33.645437: step 4025, loss 0.251255, acc 0.921875, prec 0.0808102, recall 0.883286
2017-12-10T15:54:33.840967: step 4026, loss 0.0804592, acc 0.953125, prec 0.0808476, recall 0.883346
2017-12-10T15:54:34.043208: step 4027, loss 0.218905, acc 0.90625, prec 0.0808793, recall 0.883406
2017-12-10T15:54:34.241256: step 4028, loss 0.106638, acc 0.953125, prec 0.0808736, recall 0.883406
2017-12-10T15:54:34.438461: step 4029, loss 0.216915, acc 0.921875, prec 0.0809286, recall 0.883495
2017-12-10T15:54:34.637083: step 4030, loss 0.576936, acc 0.921875, prec 0.0809407, recall 0.883525
2017-12-10T15:54:34.836141: step 4031, loss 0.24282, acc 0.921875, prec 0.0809527, recall 0.883555
2017-12-10T15:54:35.033966: step 4032, loss 1.16358, acc 0.84375, prec 0.0809983, recall 0.883644
2017-12-10T15:54:35.233806: step 4033, loss 0.543318, acc 0.859375, prec 0.0810027, recall 0.883673
2017-12-10T15:54:35.431342: step 4034, loss 0.400896, acc 0.859375, prec 0.0809857, recall 0.883673
2017-12-10T15:54:35.631873: step 4035, loss 0.671385, acc 0.828125, prec 0.0810078, recall 0.883733
2017-12-10T15:54:35.833552: step 4036, loss 0.380327, acc 0.828125, prec 0.080987, recall 0.883733
2017-12-10T15:54:36.029414: step 4037, loss 0.377729, acc 0.90625, prec 0.0809971, recall 0.883762
2017-12-10T15:54:36.230316: step 4038, loss 0.0731092, acc 0.96875, prec 0.0810148, recall 0.883792
2017-12-10T15:54:36.425742: step 4039, loss 0.411122, acc 0.9375, prec 0.0811145, recall 0.88394
2017-12-10T15:54:36.624175: step 4040, loss 0.43466, acc 0.90625, prec 0.0811246, recall 0.883969
2017-12-10T15:54:36.818340: step 4041, loss 0.123981, acc 0.96875, prec 0.0811208, recall 0.883969
2017-12-10T15:54:37.019336: step 4042, loss 0.406715, acc 0.890625, prec 0.0811076, recall 0.883969
2017-12-10T15:54:37.213125: step 4043, loss 0.32073, acc 0.9375, prec 0.0811215, recall 0.883999
2017-12-10T15:54:37.413110: step 4044, loss 0.321387, acc 0.953125, prec 0.0811372, recall 0.884028
2017-12-10T15:54:37.611860: step 4045, loss 0.137566, acc 0.953125, prec 0.0811315, recall 0.884028
2017-12-10T15:54:37.808404: step 4046, loss 0.028007, acc 0.984375, prec 0.0811511, recall 0.884058
2017-12-10T15:54:38.005861: step 4047, loss 0.179409, acc 0.921875, prec 0.0811631, recall 0.884087
2017-12-10T15:54:38.204550: step 4048, loss 0.145737, acc 0.953125, prec 0.0811574, recall 0.884087
2017-12-10T15:54:38.406169: step 4049, loss 0.0826392, acc 0.96875, prec 0.081175, recall 0.884117
2017-12-10T15:54:38.605256: step 4050, loss 0.0555563, acc 0.96875, prec 0.0811713, recall 0.884117
2017-12-10T15:54:38.801667: step 4051, loss 0.333041, acc 0.9375, prec 0.0811851, recall 0.884146
2017-12-10T15:54:39.000605: step 4052, loss 0.179825, acc 0.9375, prec 0.0811775, recall 0.884146
2017-12-10T15:54:39.205947: step 4053, loss 0.0406468, acc 0.984375, prec 0.0812185, recall 0.884205
2017-12-10T15:54:39.409740: step 4054, loss 0.737176, acc 1, prec 0.0812399, recall 0.884235
2017-12-10T15:54:39.614490: step 4055, loss 0.0136494, acc 1, prec 0.0812399, recall 0.884235
2017-12-10T15:54:39.815539: step 4056, loss 0.253033, acc 0.921875, prec 0.0812519, recall 0.884264
2017-12-10T15:54:40.019571: step 4057, loss 0.13201, acc 0.953125, prec 0.0812676, recall 0.884293
2017-12-10T15:54:40.224405: step 4058, loss 0.119113, acc 0.96875, prec 0.0812853, recall 0.884323
2017-12-10T15:54:40.426599: step 4059, loss 0.221302, acc 0.953125, prec 0.081301, recall 0.884352
2017-12-10T15:54:40.628022: step 4060, loss 0.0867445, acc 0.984375, prec 0.0813634, recall 0.88444
2017-12-10T15:54:40.827208: step 4061, loss 0.0465585, acc 1, prec 0.0813848, recall 0.884469
2017-12-10T15:54:41.025930: step 4062, loss 0.00909109, acc 1, prec 0.0814062, recall 0.884498
2017-12-10T15:54:41.219556: step 4063, loss 0.0269529, acc 0.984375, prec 0.0814257, recall 0.884528
2017-12-10T15:54:41.420392: step 4064, loss 0.0505554, acc 1, prec 0.0814685, recall 0.884586
2017-12-10T15:54:41.625128: step 4065, loss 0.176621, acc 0.96875, prec 0.0814861, recall 0.884615
2017-12-10T15:54:41.823470: step 4066, loss 0.464403, acc 0.96875, prec 0.0815037, recall 0.884645
2017-12-10T15:54:42.022177: step 4067, loss 0.559641, acc 0.96875, prec 0.0815428, recall 0.884703
2017-12-10T15:54:42.222403: step 4068, loss 0.0249303, acc 1, prec 0.0815856, recall 0.884761
2017-12-10T15:54:42.417519: step 4069, loss 0.640595, acc 0.96875, prec 0.081646, recall 0.884848
2017-12-10T15:54:42.622471: step 4070, loss 0.24374, acc 0.9375, prec 0.0816384, recall 0.884848
2017-12-10T15:54:42.823900: step 4071, loss 0.0773433, acc 0.984375, prec 0.0816365, recall 0.884848
2017-12-10T15:54:43.023608: step 4072, loss 0.111712, acc 0.96875, prec 0.081654, recall 0.884878
2017-12-10T15:54:43.224235: step 4073, loss 0.0602571, acc 0.96875, prec 0.081693, recall 0.884936
2017-12-10T15:54:43.429067: step 4074, loss 0.0818025, acc 0.96875, prec 0.0817534, recall 0.885023
2017-12-10T15:54:43.634032: step 4075, loss 0.378698, acc 0.9375, prec 0.0817458, recall 0.885023
2017-12-10T15:54:43.834564: step 4076, loss 0.290976, acc 0.9375, prec 0.0817595, recall 0.885052
2017-12-10T15:54:44.030890: step 4077, loss 0.0396633, acc 0.984375, prec 0.081779, recall 0.885081
2017-12-10T15:54:44.230315: step 4078, loss 0.370311, acc 0.875, prec 0.0817852, recall 0.88511
2017-12-10T15:54:44.438533: step 4079, loss 0.0592473, acc 0.984375, prec 0.0817833, recall 0.88511
2017-12-10T15:54:44.639514: step 4080, loss 0.126533, acc 0.96875, prec 0.0818008, recall 0.885139
2017-12-10T15:54:44.846211: step 4081, loss 0.0357631, acc 0.984375, prec 0.0817989, recall 0.885139
2017-12-10T15:54:45.048595: step 4082, loss 0.25191, acc 0.875, prec 0.0817837, recall 0.885139
2017-12-10T15:54:45.246516: step 4083, loss 0.0948053, acc 0.984375, prec 0.0818032, recall 0.885167
2017-12-10T15:54:45.442053: step 4084, loss 0.490313, acc 0.921875, prec 0.081815, recall 0.885196
2017-12-10T15:54:45.643356: step 4085, loss 0.148556, acc 0.96875, prec 0.0818112, recall 0.885196
2017-12-10T15:54:45.846577: step 4086, loss 0.154001, acc 0.96875, prec 0.0818074, recall 0.885196
2017-12-10T15:54:46.052957: step 4087, loss 0.224396, acc 0.953125, prec 0.0818017, recall 0.885196
2017-12-10T15:54:46.253346: step 4088, loss 0.154178, acc 0.921875, prec 0.0817922, recall 0.885196
2017-12-10T15:54:46.454405: step 4089, loss 0.14438, acc 0.984375, prec 0.0818116, recall 0.885225
2017-12-10T15:54:46.659820: step 4090, loss 0.0344817, acc 0.984375, prec 0.0818097, recall 0.885225
2017-12-10T15:54:46.862544: step 4091, loss 0.0666127, acc 0.96875, prec 0.0818059, recall 0.885225
2017-12-10T15:54:47.057913: step 4092, loss 0.122425, acc 0.96875, prec 0.0818235, recall 0.885254
2017-12-10T15:54:47.255019: step 4093, loss 0.0930448, acc 0.953125, prec 0.0818178, recall 0.885254
2017-12-10T15:54:47.453333: step 4094, loss 0.277204, acc 0.96875, prec 0.0818353, recall 0.885283
2017-12-10T15:54:47.650791: step 4095, loss 0.151311, acc 0.96875, prec 0.0818315, recall 0.885283
2017-12-10T15:54:47.854239: step 4096, loss 0.0425925, acc 0.984375, prec 0.0818509, recall 0.885312
2017-12-10T15:54:48.051278: step 4097, loss 0.279578, acc 0.96875, prec 0.0819112, recall 0.885398
2017-12-10T15:54:48.249209: step 4098, loss 0.0995367, acc 0.953125, prec 0.0819268, recall 0.885427
2017-12-10T15:54:48.452236: step 4099, loss 0.141766, acc 0.984375, prec 0.0819249, recall 0.885427
2017-12-10T15:54:48.658785: step 4100, loss 0.051898, acc 0.984375, prec 0.081923, recall 0.885427
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_3/1512938428/checkpoints/model-4100

2017-12-10T15:54:49.918408: step 4101, loss 0.0588809, acc 0.984375, prec 0.0819638, recall 0.885485
2017-12-10T15:54:50.117692: step 4102, loss 0.0346049, acc 0.984375, prec 0.0819619, recall 0.885485
2017-12-10T15:54:50.322961: step 4103, loss 0.0629642, acc 0.984375, prec 0.0819813, recall 0.885513
2017-12-10T15:54:50.522899: step 4104, loss 0.176058, acc 0.96875, prec 0.0820202, recall 0.885571
2017-12-10T15:54:50.721828: step 4105, loss 0.0296485, acc 1, prec 0.0820202, recall 0.885571
2017-12-10T15:54:50.919568: step 4106, loss 0.0541342, acc 0.984375, prec 0.0820396, recall 0.8856
2017-12-10T15:54:51.115587: step 4107, loss 0.0398892, acc 0.984375, prec 0.0820377, recall 0.8856
2017-12-10T15:54:51.311513: step 4108, loss 0.847456, acc 0.96875, prec 0.0820552, recall 0.885628
2017-12-10T15:54:51.512924: step 4109, loss 0.0891169, acc 0.984375, prec 0.0820533, recall 0.885628
2017-12-10T15:54:51.714355: step 4110, loss 0.0433992, acc 0.984375, prec 0.0820514, recall 0.885628
2017-12-10T15:54:51.913973: step 4111, loss 0.0205546, acc 1, prec 0.0820514, recall 0.885628
2017-12-10T15:54:52.119391: step 4112, loss 0.0810741, acc 0.96875, prec 0.0820476, recall 0.885628
2017-12-10T15:54:52.321769: step 4113, loss 0.0360295, acc 1, prec 0.0820689, recall 0.885657
2017-12-10T15:54:52.520558: step 4114, loss 0.286765, acc 0.953125, prec 0.0821272, recall 0.885743
2017-12-10T15:54:52.720873: step 4115, loss 0.342565, acc 0.96875, prec 0.0821447, recall 0.885772
2017-12-10T15:54:52.922526: step 4116, loss 0.0857044, acc 0.96875, prec 0.0821409, recall 0.885772
2017-12-10T15:54:53.120015: step 4117, loss 0.188148, acc 0.921875, prec 0.0821526, recall 0.8858
2017-12-10T15:54:53.318134: step 4118, loss 0.195745, acc 0.9375, prec 0.0821876, recall 0.885857
2017-12-10T15:54:53.516449: step 4119, loss 0.869979, acc 0.953125, prec 0.0822245, recall 0.885914
2017-12-10T15:54:53.717297: step 4120, loss 0.0479265, acc 0.984375, prec 0.0822439, recall 0.885943
2017-12-10T15:54:53.915617: step 4121, loss 0.215678, acc 0.96875, prec 0.082304, recall 0.886028
2017-12-10T15:54:54.114854: step 4122, loss 0.243463, acc 0.921875, prec 0.0822945, recall 0.886028
2017-12-10T15:54:54.313719: step 4123, loss 0.574697, acc 0.9375, prec 0.0823082, recall 0.886057
2017-12-10T15:54:54.514722: step 4124, loss 0.224352, acc 0.9375, prec 0.0823005, recall 0.886057
2017-12-10T15:54:54.718176: step 4125, loss 0.122429, acc 0.953125, prec 0.0822948, recall 0.886057
2017-12-10T15:54:54.918690: step 4126, loss 0.231781, acc 0.921875, prec 0.0822852, recall 0.886057
2017-12-10T15:54:55.118218: step 4127, loss 0.264532, acc 0.921875, prec 0.0823183, recall 0.886114
2017-12-10T15:54:55.315361: step 4128, loss 0.128446, acc 0.96875, prec 0.0823145, recall 0.886114
2017-12-10T15:54:55.509181: step 4129, loss 0.158406, acc 0.96875, prec 0.0823319, recall 0.886142
2017-12-10T15:54:55.703667: step 4130, loss 0.283388, acc 0.90625, prec 0.0823205, recall 0.886142
2017-12-10T15:54:55.900834: step 4131, loss 0.0543542, acc 0.984375, prec 0.0823398, recall 0.886171
2017-12-10T15:54:56.098920: step 4132, loss 0.0570825, acc 0.96875, prec 0.0823573, recall 0.886199
2017-12-10T15:54:56.296930: step 4133, loss 0.324726, acc 0.96875, prec 0.0823748, recall 0.886228
2017-12-10T15:54:56.497803: step 4134, loss 0.164794, acc 0.96875, prec 0.0823922, recall 0.886256
2017-12-10T15:54:56.699426: step 4135, loss 0.153672, acc 0.96875, prec 0.0824097, recall 0.886284
2017-12-10T15:54:56.899695: step 4136, loss 0.183977, acc 0.953125, prec 0.0824252, recall 0.886313
2017-12-10T15:54:57.096564: step 4137, loss 0.438204, acc 0.921875, prec 0.0824369, recall 0.886341
2017-12-10T15:54:57.295461: step 4138, loss 0.282824, acc 0.9375, prec 0.0824293, recall 0.886341
2017-12-10T15:54:57.495025: step 4139, loss 0.141169, acc 0.984375, prec 0.0824487, recall 0.886369
2017-12-10T15:54:57.696196: step 4140, loss 0.159709, acc 0.953125, prec 0.0824642, recall 0.886398
2017-12-10T15:54:57.891457: step 4141, loss 0.0163354, acc 1, prec 0.0824855, recall 0.886426
2017-12-10T15:54:58.090400: step 4142, loss 0.0314714, acc 0.984375, prec 0.0824835, recall 0.886426
2017-12-10T15:54:58.288692: step 4143, loss 2.14232, acc 0.96875, prec 0.0825029, recall 0.886234
2017-12-10T15:54:58.490338: step 4144, loss 0.124723, acc 0.953125, prec 0.0824972, recall 0.886234
2017-12-10T15:54:58.692498: step 4145, loss 0.484458, acc 0.984375, prec 0.0825378, recall 0.88629
2017-12-10T15:54:58.894121: step 4146, loss 0.29773, acc 0.9375, prec 0.0825514, recall 0.886318
2017-12-10T15:54:59.094808: step 4147, loss 0.28451, acc 0.875, prec 0.0825361, recall 0.886318
2017-12-10T15:54:59.301310: step 4148, loss 0.213289, acc 0.90625, prec 0.0825459, recall 0.886347
2017-12-10T15:54:59.504006: step 4149, loss 0.490945, acc 0.921875, prec 0.0826425, recall 0.886488
2017-12-10T15:54:59.705776: step 4150, loss 0.122753, acc 0.9375, prec 0.0826349, recall 0.886488
2017-12-10T15:54:59.902167: step 4151, loss 0.307778, acc 0.859375, prec 0.0826389, recall 0.886516
2017-12-10T15:55:00.100534: step 4152, loss 0.618812, acc 0.828125, prec 0.0826178, recall 0.886516
2017-12-10T15:55:00.298928: step 4153, loss 0.430481, acc 0.796875, prec 0.0826354, recall 0.886572
2017-12-10T15:55:00.503906: step 4154, loss 0.380465, acc 0.875, prec 0.0826414, recall 0.8866
2017-12-10T15:55:00.698731: step 4155, loss 0.451924, acc 0.921875, prec 0.082653, recall 0.886629
2017-12-10T15:55:00.895427: step 4156, loss 0.144648, acc 0.9375, prec 0.0826454, recall 0.886629
2017-12-10T15:55:01.090502: step 4157, loss 0.149869, acc 0.921875, prec 0.082657, recall 0.886657
2017-12-10T15:55:01.294830: step 4158, loss 0.124, acc 0.96875, prec 0.0826532, recall 0.886657
2017-12-10T15:55:01.497593: step 4159, loss 0.0777804, acc 0.984375, prec 0.0826513, recall 0.886657
2017-12-10T15:55:01.703807: step 4160, loss 0.191188, acc 0.953125, prec 0.0826456, recall 0.886657
2017-12-10T15:55:01.899741: step 4161, loss 0.236342, acc 0.9375, prec 0.0827016, recall 0.886741
2017-12-10T15:55:02.094425: step 4162, loss 0.177064, acc 0.9375, prec 0.0827151, recall 0.886769
2017-12-10T15:55:02.294273: step 4163, loss 0.236164, acc 0.9375, prec 0.0827075, recall 0.886769
2017-12-10T15:55:02.493229: step 4164, loss 0.313929, acc 0.875, prec 0.0827346, recall 0.886825
2017-12-10T15:55:02.696231: step 4165, loss 0.349661, acc 0.984375, prec 0.0827538, recall 0.886853
2017-12-10T15:55:02.896549: step 4166, loss 0.0804376, acc 0.984375, prec 0.0827519, recall 0.886853
2017-12-10T15:55:03.098691: step 4167, loss 0.137505, acc 0.96875, prec 0.0827693, recall 0.886881
2017-12-10T15:55:03.300831: step 4168, loss 1.85792, acc 0.9375, prec 0.0827636, recall 0.886662
2017-12-10T15:55:03.504075: step 4169, loss 0.0870384, acc 0.96875, prec 0.0827809, recall 0.88669
2017-12-10T15:55:03.701157: step 4170, loss 0.121945, acc 0.96875, prec 0.0827771, recall 0.88669
2017-12-10T15:55:03.898956: step 4171, loss 0.119224, acc 0.953125, prec 0.0827714, recall 0.88669
2017-12-10T15:55:04.097841: step 4172, loss 0.14014, acc 0.953125, prec 0.0827868, recall 0.886718
2017-12-10T15:55:04.291679: step 4173, loss 0.276066, acc 0.90625, prec 0.0827753, recall 0.886718
2017-12-10T15:55:04.493228: step 4174, loss 0.140026, acc 0.921875, prec 0.0827658, recall 0.886718
2017-12-10T15:55:04.688880: step 4175, loss 0.103098, acc 0.96875, prec 0.0827831, recall 0.886746
2017-12-10T15:55:04.888735: step 4176, loss 0.409406, acc 0.90625, prec 0.0827717, recall 0.886746
2017-12-10T15:55:05.086524: step 4177, loss 0.0619703, acc 0.984375, prec 0.0827909, recall 0.886774
2017-12-10T15:55:05.280052: step 4178, loss 0.16238, acc 0.953125, prec 0.0828064, recall 0.886802
2017-12-10T15:55:05.480614: step 4179, loss 0.19332, acc 0.953125, prec 0.0828218, recall 0.88683
2017-12-10T15:55:05.675119: step 4180, loss 0.152943, acc 0.96875, prec 0.082818, recall 0.88683
2017-12-10T15:55:05.874259: step 4181, loss 0.314195, acc 0.9375, prec 0.0828103, recall 0.88683
2017-12-10T15:55:06.073068: step 4182, loss 0.0724563, acc 0.96875, prec 0.0828065, recall 0.88683
2017-12-10T15:55:06.270631: step 4183, loss 0.185071, acc 0.9375, prec 0.0827989, recall 0.88683
2017-12-10T15:55:06.464671: step 4184, loss 0.166742, acc 0.96875, prec 0.0828585, recall 0.886914
2017-12-10T15:55:06.662193: step 4185, loss 0.206228, acc 0.953125, prec 0.0828739, recall 0.886941
2017-12-10T15:55:06.859612: step 4186, loss 0.210499, acc 0.921875, prec 0.0828644, recall 0.886941
2017-12-10T15:55:07.055411: step 4187, loss 0.029459, acc 0.984375, prec 0.0828625, recall 0.886941
2017-12-10T15:55:07.255636: step 4188, loss 0.153011, acc 0.984375, prec 0.0829029, recall 0.886997
2017-12-10T15:55:07.457363: step 4189, loss 0.0340499, acc 0.984375, prec 0.082901, recall 0.886997
2017-12-10T15:55:07.653052: step 4190, loss 0.130338, acc 0.96875, prec 0.0828971, recall 0.886997
2017-12-10T15:55:07.848596: step 4191, loss 0.157303, acc 0.953125, prec 0.0829125, recall 0.887025
2017-12-10T15:55:08.048552: step 4192, loss 0.0418858, acc 0.96875, prec 0.0829299, recall 0.887053
2017-12-10T15:55:08.244610: step 4193, loss 0.261562, acc 0.984375, prec 0.0829491, recall 0.887081
2017-12-10T15:55:08.443362: step 4194, loss 0.217044, acc 0.984375, prec 0.0829683, recall 0.887109
2017-12-10T15:55:08.644741: step 4195, loss 0.172771, acc 0.984375, prec 0.0829876, recall 0.887137
2017-12-10T15:55:08.846641: step 4196, loss 0.139409, acc 0.9375, prec 0.0829799, recall 0.887137
2017-12-10T15:55:09.047426: step 4197, loss 0.209231, acc 0.96875, prec 0.0829761, recall 0.887137
2017-12-10T15:55:09.249639: step 4198, loss 0.132025, acc 0.984375, prec 0.0829742, recall 0.887137
2017-12-10T15:55:09.445326: step 4199, loss 0.0101672, acc 1, prec 0.0829953, recall 0.887164
2017-12-10T15:55:09.640097: step 4200, loss 0.0145328, acc 1, prec 0.0830164, recall 0.887192
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_3/1512938428/checkpoints/model-4200

2017-12-10T15:55:10.969654: step 4201, loss 0.285473, acc 0.984375, prec 0.0830357, recall 0.88722
2017-12-10T15:55:11.170900: step 4202, loss 0.156334, acc 0.984375, prec 0.0830549, recall 0.887248
2017-12-10T15:55:11.371686: step 4203, loss 0.0853299, acc 0.96875, prec 0.0830722, recall 0.887275
2017-12-10T15:55:11.565886: step 4204, loss 0.248995, acc 0.953125, prec 0.0831087, recall 0.887331
2017-12-10T15:55:11.769403: step 4205, loss 0.238868, acc 0.953125, prec 0.0831241, recall 0.887359
2017-12-10T15:55:11.973611: step 4206, loss 0.0600926, acc 0.984375, prec 0.0831221, recall 0.887359
2017-12-10T15:55:12.173147: step 4207, loss 0.176343, acc 0.984375, prec 0.0831414, recall 0.887386
2017-12-10T15:55:12.373928: step 4208, loss 0.0596871, acc 0.96875, prec 0.0831375, recall 0.887386
2017-12-10T15:55:12.581172: step 4209, loss 0.133551, acc 0.96875, prec 0.0831759, recall 0.887442
2017-12-10T15:55:12.781394: step 4210, loss 0.235719, acc 0.984375, prec 0.0832374, recall 0.887525
2017-12-10T15:55:12.985557: step 4211, loss 0.0258976, acc 1, prec 0.0832374, recall 0.887525
2017-12-10T15:55:13.184464: step 4212, loss 0.290579, acc 0.953125, prec 0.0832527, recall 0.887552
2017-12-10T15:55:13.382361: step 4213, loss 0.0912455, acc 0.96875, prec 0.0832489, recall 0.887552
2017-12-10T15:55:13.581325: step 4214, loss 0.0373435, acc 0.984375, prec 0.0832681, recall 0.88758
2017-12-10T15:55:13.775706: step 4215, loss 0.0777202, acc 0.96875, prec 0.0832643, recall 0.88758
2017-12-10T15:55:13.970197: step 4216, loss 0.123884, acc 0.96875, prec 0.0832815, recall 0.887607
2017-12-10T15:55:14.175382: step 4217, loss 0.0861653, acc 0.953125, prec 0.0832758, recall 0.887607
2017-12-10T15:55:14.379914: step 4218, loss 2.55227, acc 0.9375, prec 0.08327, recall 0.88739
2017-12-10T15:55:14.587613: step 4219, loss 0.159931, acc 0.9375, prec 0.0832624, recall 0.88739
2017-12-10T15:55:14.791754: step 4220, loss 0.307576, acc 0.984375, prec 0.0832815, recall 0.887417
2017-12-10T15:55:14.993147: step 4221, loss 0.0930843, acc 0.953125, prec 0.0832758, recall 0.887417
2017-12-10T15:55:15.193805: step 4222, loss 0.296806, acc 0.953125, prec 0.0832911, recall 0.887445
2017-12-10T15:55:15.394545: step 4223, loss 0.29048, acc 0.9375, prec 0.0832835, recall 0.887445
2017-12-10T15:55:15.593395: step 4224, loss 0.4853, acc 0.875, prec 0.0833314, recall 0.887528
2017-12-10T15:55:15.792223: step 4225, loss 0.110857, acc 0.953125, prec 0.0833257, recall 0.887528
2017-12-10T15:55:15.986421: step 4226, loss 0.251307, acc 0.90625, prec 0.0833563, recall 0.887583
2017-12-10T15:55:16.186012: step 4227, loss 0.0420378, acc 0.984375, prec 0.0834387, recall 0.887693
2017-12-10T15:55:16.385608: step 4228, loss 0.111169, acc 0.921875, prec 0.0834502, recall 0.88772
2017-12-10T15:55:16.584020: step 4229, loss 0.100177, acc 0.953125, prec 0.0834445, recall 0.88772
2017-12-10T15:55:16.784919: step 4230, loss 0.134876, acc 0.953125, prec 0.0834387, recall 0.88772
2017-12-10T15:55:16.980747: step 4231, loss 0.398046, acc 0.890625, prec 0.0834464, recall 0.887748
2017-12-10T15:55:17.176380: step 4232, loss 0.183647, acc 0.9375, prec 0.0834597, recall 0.887775
2017-12-10T15:55:17.375381: step 4233, loss 0.360386, acc 0.90625, prec 0.0834482, recall 0.887775
2017-12-10T15:55:17.579562: step 4234, loss 0.403603, acc 0.875, prec 0.083475, recall 0.88783
2017-12-10T15:55:17.778211: step 4235, loss 0.511676, acc 0.921875, prec 0.0834865, recall 0.887857
2017-12-10T15:55:17.976895: step 4236, loss 0.102134, acc 0.953125, prec 0.0835018, recall 0.887885
2017-12-10T15:55:18.178195: step 4237, loss 0.0849962, acc 0.96875, prec 0.083498, recall 0.887885
2017-12-10T15:55:18.381809: step 4238, loss 0.247242, acc 0.90625, prec 0.0835285, recall 0.887939
2017-12-10T15:55:18.580384: step 4239, loss 0.0545621, acc 0.96875, prec 0.0835247, recall 0.887939
2017-12-10T15:55:18.778722: step 4240, loss 0.119724, acc 0.953125, prec 0.08354, recall 0.887967
2017-12-10T15:55:18.980308: step 4241, loss 0.197552, acc 0.953125, prec 0.0835342, recall 0.887967
2017-12-10T15:55:19.179122: step 4242, loss 0.178827, acc 0.90625, prec 0.0835227, recall 0.887967
2017-12-10T15:55:19.376978: step 4243, loss 0.135456, acc 0.96875, prec 0.0835189, recall 0.887967
2017-12-10T15:55:19.575668: step 4244, loss 0.212815, acc 0.9375, prec 0.0835112, recall 0.887967
2017-12-10T15:55:19.775100: step 4245, loss 0.0513882, acc 0.984375, prec 0.0835304, recall 0.887994
2017-12-10T15:55:19.973592: step 4246, loss 0.00405216, acc 1, prec 0.0835304, recall 0.887994
2017-12-10T15:55:20.173341: step 4247, loss 0.238188, acc 0.96875, prec 0.0835896, recall 0.888076
2017-12-10T15:55:20.371082: step 4248, loss 0.0278298, acc 1, prec 0.0836107, recall 0.888103
2017-12-10T15:55:20.567963: step 4249, loss 0.0303113, acc 0.96875, prec 0.0836068, recall 0.888103
2017-12-10T15:55:20.762518: step 4250, loss 0.0319643, acc 0.984375, prec 0.0836049, recall 0.888103
2017-12-10T15:55:20.962166: step 4251, loss 0.0844735, acc 0.984375, prec 0.083624, recall 0.888131
2017-12-10T15:55:21.157727: step 4252, loss 0.306587, acc 0.984375, prec 0.0836641, recall 0.888185
2017-12-10T15:55:21.362598: step 4253, loss 1.251, acc 0.953125, prec 0.0837215, recall 0.888267
2017-12-10T15:55:21.565273: step 4254, loss 0.0557843, acc 0.984375, prec 0.0837406, recall 0.888294
2017-12-10T15:55:21.767336: step 4255, loss 0.273948, acc 0.984375, prec 0.0838017, recall 0.888375
2017-12-10T15:55:21.969634: step 4256, loss 0.212101, acc 0.984375, prec 0.0838418, recall 0.88843
2017-12-10T15:55:22.164873: step 4257, loss 0.125058, acc 0.9375, prec 0.0838551, recall 0.888457
2017-12-10T15:55:22.362058: step 4258, loss 0.123922, acc 0.9375, prec 0.0838474, recall 0.888457
2017-12-10T15:55:22.565093: step 4259, loss 0.0584124, acc 0.984375, prec 0.0838875, recall 0.888511
2017-12-10T15:55:22.768951: step 4260, loss 0.153701, acc 0.9375, prec 0.0839219, recall 0.888565
2017-12-10T15:55:22.966909: step 4261, loss 0.284139, acc 0.953125, prec 0.0839371, recall 0.888592
2017-12-10T15:55:23.162702: step 4262, loss 0.283047, acc 0.890625, prec 0.0839656, recall 0.888646
2017-12-10T15:55:23.358618: step 4263, loss 0.0667239, acc 0.96875, prec 0.0840248, recall 0.888727
2017-12-10T15:55:23.554360: step 4264, loss 0.125188, acc 0.953125, prec 0.08404, recall 0.888754
2017-12-10T15:55:23.757603: step 4265, loss 0.282533, acc 0.921875, prec 0.0840303, recall 0.888754
2017-12-10T15:55:23.955580: step 4266, loss 0.940927, acc 0.9375, prec 0.0841066, recall 0.888862
2017-12-10T15:55:24.156310: step 4267, loss 0.145248, acc 0.90625, prec 0.084116, recall 0.888889
2017-12-10T15:55:24.353762: step 4268, loss 0.079423, acc 0.984375, prec 0.0841351, recall 0.888916
2017-12-10T15:55:24.553959: step 4269, loss 0.139489, acc 0.96875, prec 0.0841522, recall 0.888943
2017-12-10T15:55:24.756846: step 4270, loss 0.154701, acc 0.984375, prec 0.0841712, recall 0.88897
2017-12-10T15:55:24.956021: step 4271, loss 0.243718, acc 0.921875, prec 0.0841826, recall 0.888996
2017-12-10T15:55:25.155459: step 4272, loss 0.0478929, acc 0.96875, prec 0.0842416, recall 0.889077
2017-12-10T15:55:25.351647: step 4273, loss 0.209503, acc 0.921875, prec 0.0842529, recall 0.889104
2017-12-10T15:55:25.551288: step 4274, loss 0.276188, acc 0.9375, prec 0.0842452, recall 0.889104
2017-12-10T15:55:25.750766: step 4275, loss 0.228783, acc 0.9375, prec 0.0842585, recall 0.88913
2017-12-10T15:55:25.946473: step 4276, loss 0.339786, acc 0.84375, prec 0.0842392, recall 0.88913
2017-12-10T15:55:26.141887: step 4277, loss 0.258329, acc 0.96875, prec 0.0842563, recall 0.889157
2017-12-10T15:55:26.345185: step 4278, loss 0.154023, acc 0.96875, prec 0.0842524, recall 0.889157
2017-12-10T15:55:26.542807: step 4279, loss 0.108444, acc 0.984375, prec 0.0842924, recall 0.889211
2017-12-10T15:55:26.743266: step 4280, loss 0.193081, acc 0.9375, prec 0.0843057, recall 0.889237
2017-12-10T15:55:26.951595: step 4281, loss 2.22425, acc 0.921875, prec 0.0843189, recall 0.88905
2017-12-10T15:55:27.152484: step 4282, loss 0.42367, acc 0.890625, prec 0.0843263, recall 0.889076
2017-12-10T15:55:27.349143: step 4283, loss 0.253321, acc 0.9375, prec 0.0843396, recall 0.889103
2017-12-10T15:55:27.548244: step 4284, loss 0.0766975, acc 0.96875, prec 0.0843566, recall 0.88913
2017-12-10T15:55:27.747347: step 4285, loss 0.124288, acc 0.953125, prec 0.0843718, recall 0.889157
2017-12-10T15:55:27.944089: step 4286, loss 0.283264, acc 0.90625, prec 0.0843811, recall 0.889183
2017-12-10T15:55:28.145491: step 4287, loss 0.314744, acc 0.921875, prec 0.0843715, recall 0.889183
2017-12-10T15:55:28.345084: step 4288, loss 0.205473, acc 0.953125, prec 0.0843866, recall 0.88921
2017-12-10T15:55:28.545413: step 4289, loss 0.150839, acc 0.9375, prec 0.0843999, recall 0.889237
2017-12-10T15:55:28.744116: step 4290, loss 0.223886, acc 0.96875, prec 0.0844169, recall 0.889263
2017-12-10T15:55:28.951060: step 4291, loss 0.188983, acc 0.96875, prec 0.0844131, recall 0.889263
2017-12-10T15:55:29.151362: step 4292, loss 0.355566, acc 0.9375, prec 0.0844263, recall 0.88929
2017-12-10T15:55:29.353840: step 4293, loss 0.0845981, acc 0.96875, prec 0.0844433, recall 0.889317
2017-12-10T15:55:29.557883: step 4294, loss 0.148424, acc 0.96875, prec 0.0845022, recall 0.889396
2017-12-10T15:55:29.754683: step 4295, loss 0.0715703, acc 0.96875, prec 0.0844984, recall 0.889396
2017-12-10T15:55:29.951895: step 4296, loss 0.205808, acc 0.9375, prec 0.0844906, recall 0.889396
2017-12-10T15:55:30.157738: step 4297, loss 0.344616, acc 0.90625, prec 0.0845, recall 0.889423
2017-12-10T15:55:30.352842: step 4298, loss 0.421703, acc 0.9375, prec 0.0844922, recall 0.889423
2017-12-10T15:55:30.552840: step 4299, loss 0.10399, acc 0.96875, prec 0.0845093, recall 0.88945
2017-12-10T15:55:30.748572: step 4300, loss 0.311719, acc 0.953125, prec 0.0845244, recall 0.889476
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_3/1512938428/checkpoints/model-4300

2017-12-10T15:55:31.980907: step 4301, loss 0.145494, acc 0.953125, prec 0.0845186, recall 0.889476
2017-12-10T15:55:32.181247: step 4302, loss 0.36968, acc 0.953125, prec 0.0845128, recall 0.889476
2017-12-10T15:55:32.380145: step 4303, loss 0.275932, acc 0.921875, prec 0.0845241, recall 0.889503
2017-12-10T15:55:32.575502: step 4304, loss 0.1065, acc 0.953125, prec 0.0845392, recall 0.889529
2017-12-10T15:55:32.776208: step 4305, loss 0.237886, acc 0.96875, prec 0.0845562, recall 0.889556
2017-12-10T15:55:32.977351: step 4306, loss 0.285234, acc 0.953125, prec 0.0845713, recall 0.889582
2017-12-10T15:55:33.183402: step 4307, loss 0.337049, acc 0.984375, prec 0.0845903, recall 0.889609
2017-12-10T15:55:33.384949: step 4308, loss 0.0563342, acc 0.96875, prec 0.0846073, recall 0.889635
2017-12-10T15:55:33.583891: step 4309, loss 0.0924542, acc 0.96875, prec 0.0846243, recall 0.889662
2017-12-10T15:55:33.779885: step 4310, loss 0.00584027, acc 1, prec 0.0846243, recall 0.889662
2017-12-10T15:55:33.975874: step 4311, loss 0.0683043, acc 0.984375, prec 0.0846433, recall 0.889688
2017-12-10T15:55:34.177357: step 4312, loss 0.0411328, acc 0.984375, prec 0.0846831, recall 0.889741
2017-12-10T15:55:34.378857: step 4313, loss 0.187608, acc 0.96875, prec 0.0847001, recall 0.889768
2017-12-10T15:55:34.585047: step 4314, loss 1.19834, acc 0.953125, prec 0.0847778, recall 0.889873
2017-12-10T15:55:34.783908: step 4315, loss 0.0681366, acc 0.96875, prec 0.084774, recall 0.889873
2017-12-10T15:55:34.981910: step 4316, loss 0.154663, acc 0.96875, prec 0.0847701, recall 0.889873
2017-12-10T15:55:35.181922: step 4317, loss 0.112442, acc 0.953125, prec 0.0848061, recall 0.889926
2017-12-10T15:55:35.379636: step 4318, loss 0.0827386, acc 0.953125, prec 0.0848211, recall 0.889952
2017-12-10T15:55:35.581477: step 4319, loss 0.131388, acc 0.96875, prec 0.084859, recall 0.890005
2017-12-10T15:55:35.780214: step 4320, loss 0.172626, acc 0.90625, prec 0.0848474, recall 0.890005
2017-12-10T15:55:35.977246: step 4321, loss 0.160498, acc 0.96875, prec 0.0848644, recall 0.890031
2017-12-10T15:55:36.178726: step 4322, loss 0.100471, acc 0.9375, prec 0.0848984, recall 0.890084
2017-12-10T15:55:36.375668: step 4323, loss 0.185069, acc 0.953125, prec 0.0849343, recall 0.890136
2017-12-10T15:55:36.575358: step 4324, loss 0.132863, acc 0.953125, prec 0.0849493, recall 0.890162
2017-12-10T15:55:36.770505: step 4325, loss 0.03896, acc 1, prec 0.0849493, recall 0.890162
2017-12-10T15:55:36.963255: step 4326, loss 0.808743, acc 0.953125, prec 0.0849643, recall 0.890189
2017-12-10T15:55:37.168579: step 4327, loss 0.074268, acc 0.96875, prec 0.0849605, recall 0.890189
2017-12-10T15:55:37.368323: step 4328, loss 0.0425077, acc 0.984375, prec 0.0849585, recall 0.890189
2017-12-10T15:55:37.564814: step 4329, loss 0.0245674, acc 0.984375, prec 0.0849566, recall 0.890189
2017-12-10T15:55:37.766993: step 4330, loss 0.210659, acc 0.953125, prec 0.0849925, recall 0.890241
2017-12-10T15:55:37.959990: step 4331, loss 0.217487, acc 0.953125, prec 0.0850284, recall 0.890293
2017-12-10T15:55:38.159663: step 4332, loss 0.255459, acc 0.90625, prec 0.0850167, recall 0.890293
2017-12-10T15:55:38.360631: step 4333, loss 0.0570314, acc 0.96875, prec 0.0850129, recall 0.890293
2017-12-10T15:55:38.562660: step 4334, loss 0.0693036, acc 0.96875, prec 0.0850298, recall 0.89032
2017-12-10T15:55:38.762172: step 4335, loss 0.088551, acc 0.953125, prec 0.0850449, recall 0.890346
2017-12-10T15:55:38.958500: step 4336, loss 0.0539274, acc 0.96875, prec 0.085041, recall 0.890346
2017-12-10T15:55:39.159845: step 4337, loss 0.25485, acc 0.9375, prec 0.0850957, recall 0.890424
2017-12-10T15:55:39.361367: step 4338, loss 0.399153, acc 0.953125, prec 0.0850899, recall 0.890424
2017-12-10T15:55:39.560601: step 4339, loss 0.349292, acc 0.96875, prec 0.0851485, recall 0.890502
2017-12-10T15:55:39.759250: step 4340, loss 0.783185, acc 0.984375, prec 0.0851674, recall 0.890528
2017-12-10T15:55:39.962684: step 4341, loss 0.0971734, acc 0.953125, prec 0.0851616, recall 0.890528
2017-12-10T15:55:40.159466: step 4342, loss 0.0359987, acc 0.984375, prec 0.0852221, recall 0.890606
2017-12-10T15:55:40.358075: step 4343, loss 0.100274, acc 0.96875, prec 0.0852599, recall 0.890658
2017-12-10T15:55:40.562047: step 4344, loss 0.103086, acc 0.953125, prec 0.0852748, recall 0.890684
2017-12-10T15:55:40.762806: step 4345, loss 0.325076, acc 0.96875, prec 0.0852918, recall 0.89071
2017-12-10T15:55:40.961690: step 4346, loss 0.202697, acc 0.9375, prec 0.0853048, recall 0.890736
2017-12-10T15:55:41.160714: step 4347, loss 0.0755464, acc 0.96875, prec 0.0853009, recall 0.890736
2017-12-10T15:55:41.357786: step 4348, loss 0.0986653, acc 0.953125, prec 0.0852951, recall 0.890736
2017-12-10T15:55:41.557787: step 4349, loss 0.1591, acc 0.9375, prec 0.0853082, recall 0.890762
2017-12-10T15:55:41.756474: step 4350, loss 0.0628518, acc 0.984375, prec 0.085327, recall 0.890788
2017-12-10T15:55:41.955733: step 4351, loss 0.184552, acc 0.953125, prec 0.0853212, recall 0.890788
2017-12-10T15:55:42.151915: step 4352, loss 0.165665, acc 0.9375, prec 0.0853134, recall 0.890788
2017-12-10T15:55:42.352781: step 4353, loss 0.205513, acc 0.984375, prec 0.0853531, recall 0.89084
2017-12-10T15:55:42.552198: step 4354, loss 0.106233, acc 0.921875, prec 0.0853642, recall 0.890866
2017-12-10T15:55:42.753339: step 4355, loss 0.554398, acc 0.921875, prec 0.0853753, recall 0.890892
2017-12-10T15:55:42.951324: step 4356, loss 0.0691397, acc 0.984375, prec 0.0853733, recall 0.890892
2017-12-10T15:55:43.147201: step 4357, loss 0.151693, acc 0.953125, prec 0.0853675, recall 0.890892
2017-12-10T15:55:43.349377: step 4358, loss 0.148604, acc 0.984375, prec 0.0853864, recall 0.890918
2017-12-10T15:55:43.545019: step 4359, loss 0.026643, acc 0.984375, prec 0.0854052, recall 0.890944
2017-12-10T15:55:43.739909: step 4360, loss 0.0768366, acc 0.96875, prec 0.0854221, recall 0.890969
2017-12-10T15:55:43.941585: step 4361, loss 0.098047, acc 0.96875, prec 0.085439, recall 0.890995
2017-12-10T15:55:44.144737: step 4362, loss 0.0317551, acc 0.984375, prec 0.0854579, recall 0.891021
2017-12-10T15:55:44.346145: step 4363, loss 0.115021, acc 0.953125, prec 0.0854728, recall 0.891047
2017-12-10T15:55:44.550424: step 4364, loss 0.0536076, acc 0.984375, prec 0.0855124, recall 0.891098
2017-12-10T15:55:44.750359: step 4365, loss 0.0206824, acc 1, prec 0.0855332, recall 0.891124
2017-12-10T15:55:44.950634: step 4366, loss 0.607532, acc 0.96875, prec 0.0855501, recall 0.89115
2017-12-10T15:55:45.151033: step 4367, loss 0.0407874, acc 0.984375, prec 0.0855481, recall 0.89115
2017-12-10T15:55:45.349950: step 4368, loss 0.030959, acc 0.984375, prec 0.085567, recall 0.891176
2017-12-10T15:55:45.547737: step 4369, loss 0.24005, acc 0.953125, prec 0.0855611, recall 0.891176
2017-12-10T15:55:45.748793: step 4370, loss 0.129336, acc 0.953125, prec 0.0855553, recall 0.891176
2017-12-10T15:55:45.946920: step 4371, loss 0.0988388, acc 0.984375, prec 0.0855949, recall 0.891227
2017-12-10T15:55:46.144311: step 4372, loss 0.0192886, acc 1, prec 0.0856157, recall 0.891253
2017-12-10T15:55:46.344681: step 4373, loss 0.0823238, acc 0.984375, prec 0.0856137, recall 0.891253
2017-12-10T15:55:46.545834: step 4374, loss 0.9253, acc 0.984375, prec 0.0856533, recall 0.891304
2017-12-10T15:55:46.750433: step 4375, loss 0.29638, acc 0.984375, prec 0.0856929, recall 0.891356
2017-12-10T15:55:46.980917: step 4376, loss 0.0646818, acc 0.96875, prec 0.085689, recall 0.891356
2017-12-10T15:55:47.194225: step 4377, loss 0.195739, acc 0.953125, prec 0.0857247, recall 0.891407
2017-12-10T15:55:47.398134: step 4378, loss 0.216678, acc 0.96875, prec 0.0857623, recall 0.891458
2017-12-10T15:55:47.597562: step 4379, loss 0.478605, acc 0.9375, prec 0.0857752, recall 0.891484
2017-12-10T15:55:47.798844: step 4380, loss 0.226903, acc 0.953125, prec 0.0858109, recall 0.891535
2017-12-10T15:55:47.995731: step 4381, loss 0.0428229, acc 0.984375, prec 0.0858297, recall 0.891561
2017-12-10T15:55:48.198440: step 4382, loss 0.226545, acc 0.953125, prec 0.0858239, recall 0.891561
2017-12-10T15:55:48.396877: step 4383, loss 0.329778, acc 0.9375, prec 0.0858161, recall 0.891561
2017-12-10T15:55:48.600902: step 4384, loss 0.0433587, acc 0.984375, prec 0.0858141, recall 0.891561
2017-12-10T15:55:48.801324: step 4385, loss 0.017014, acc 1, prec 0.0858141, recall 0.891561
2017-12-10T15:55:48.999501: step 4386, loss 0.110538, acc 0.953125, prec 0.0858083, recall 0.891561
2017-12-10T15:55:49.201191: step 4387, loss 0.0480878, acc 0.984375, prec 0.0858063, recall 0.891561
2017-12-10T15:55:49.401411: step 4388, loss 0.0671126, acc 0.96875, prec 0.0858232, recall 0.891586
2017-12-10T15:55:49.598180: step 4389, loss 0.106038, acc 0.984375, prec 0.085842, recall 0.891612
2017-12-10T15:55:49.797122: step 4390, loss 0.118013, acc 0.953125, prec 0.0858569, recall 0.891637
2017-12-10T15:55:49.997189: step 4391, loss 0.194056, acc 0.953125, prec 0.085851, recall 0.891637
2017-12-10T15:55:50.191179: step 4392, loss 0.276043, acc 0.9375, prec 0.0858847, recall 0.891688
2017-12-10T15:55:50.389396: step 4393, loss 0.0929238, acc 0.96875, prec 0.0859015, recall 0.891714
2017-12-10T15:55:50.587803: step 4394, loss 0.166769, acc 0.984375, prec 0.085941, recall 0.891765
2017-12-10T15:55:50.791960: step 4395, loss 0.0222154, acc 1, prec 0.085941, recall 0.891765
2017-12-10T15:55:50.994690: step 4396, loss 0.0120457, acc 1, prec 0.0859618, recall 0.89179
2017-12-10T15:55:51.194326: step 4397, loss 0.118464, acc 0.9375, prec 0.085954, recall 0.89179
2017-12-10T15:55:51.394542: step 4398, loss 0.114675, acc 0.984375, prec 0.085952, recall 0.89179
2017-12-10T15:55:51.591064: step 4399, loss 0.0676756, acc 1, prec 0.0859935, recall 0.891841
2017-12-10T15:55:51.788289: step 4400, loss 0.6315, acc 0.921875, prec 0.0860252, recall 0.891892
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_3/1512938428/checkpoints/model-4400

2017-12-10T15:55:53.069289: step 4401, loss 0.125012, acc 0.9375, prec 0.0860174, recall 0.891892
2017-12-10T15:55:53.272344: step 4402, loss 0.219263, acc 0.984375, prec 0.0860568, recall 0.891943
2017-12-10T15:55:53.475254: step 4403, loss 0.0831671, acc 0.96875, prec 0.0860529, recall 0.891943
2017-12-10T15:55:53.676384: step 4404, loss 0.256369, acc 0.96875, prec 0.086049, recall 0.891943
2017-12-10T15:55:53.876444: step 4405, loss 0.0402167, acc 1, prec 0.0860698, recall 0.891968
2017-12-10T15:55:54.072410: step 4406, loss 0.0520988, acc 0.984375, prec 0.0860678, recall 0.891968
2017-12-10T15:55:54.272028: step 4407, loss 0.640212, acc 0.953125, prec 0.0861655, recall 0.892095
2017-12-10T15:55:54.472339: step 4408, loss 0.882359, acc 0.9375, prec 0.0861784, recall 0.89212
2017-12-10T15:55:54.673028: step 4409, loss 0.3788, acc 0.921875, prec 0.0861686, recall 0.89212
2017-12-10T15:55:54.877720: step 4410, loss 0.669002, acc 0.984375, prec 0.0861874, recall 0.892145
2017-12-10T15:55:55.082490: step 4411, loss 0.440457, acc 0.9375, prec 0.0861796, recall 0.892145
2017-12-10T15:55:55.283853: step 4412, loss 0.103806, acc 0.96875, prec 0.0861964, recall 0.892171
2017-12-10T15:55:55.488493: step 4413, loss 0.104718, acc 0.96875, prec 0.0862131, recall 0.892196
2017-12-10T15:55:55.687098: step 4414, loss 0.459261, acc 0.921875, prec 0.0862034, recall 0.892196
2017-12-10T15:55:55.887678: step 4415, loss 0.419155, acc 0.890625, prec 0.0861897, recall 0.892196
2017-12-10T15:55:56.082396: step 4416, loss 0.29116, acc 0.921875, prec 0.08618, recall 0.892196
2017-12-10T15:55:56.284904: step 4417, loss 0.172958, acc 0.9375, prec 0.0862135, recall 0.892246
2017-12-10T15:55:56.485056: step 4418, loss 0.263637, acc 0.953125, prec 0.0862284, recall 0.892272
2017-12-10T15:55:56.679707: step 4419, loss 0.309573, acc 0.921875, prec 0.0862186, recall 0.892272
2017-12-10T15:55:56.875231: step 4420, loss 0.265456, acc 0.921875, prec 0.0862089, recall 0.892272
2017-12-10T15:55:57.078802: step 4421, loss 0.437125, acc 0.890625, prec 0.0861952, recall 0.892272
2017-12-10T15:55:57.274923: step 4422, loss 0.234096, acc 0.9375, prec 0.0861874, recall 0.892272
2017-12-10T15:55:57.477066: step 4423, loss 0.347567, acc 0.921875, prec 0.086219, recall 0.892322
2017-12-10T15:55:57.678845: step 4424, loss 0.163594, acc 0.984375, prec 0.0862584, recall 0.892372
2017-12-10T15:55:57.879622: step 4425, loss 0.0296881, acc 1, prec 0.086341, recall 0.892473
2017-12-10T15:55:58.082719: step 4426, loss 0.0890726, acc 0.96875, prec 0.0863371, recall 0.892473
2017-12-10T15:55:58.279772: step 4427, loss 0.299009, acc 0.96875, prec 0.0863539, recall 0.892498
2017-12-10T15:55:58.477032: step 4428, loss 0.129956, acc 0.9375, prec 0.0863667, recall 0.892523
2017-12-10T15:55:58.677615: step 4429, loss 0.347669, acc 0.921875, prec 0.086357, recall 0.892523
2017-12-10T15:55:58.877702: step 4430, loss 0.0487728, acc 0.96875, prec 0.0863737, recall 0.892548
2017-12-10T15:55:59.075721: step 4431, loss 0.0146924, acc 1, prec 0.0863737, recall 0.892548
2017-12-10T15:55:59.281458: step 4432, loss 0.0956283, acc 0.953125, prec 0.0863678, recall 0.892548
2017-12-10T15:55:59.482157: step 4433, loss 1.22444, acc 0.9375, prec 0.086362, recall 0.89234
2017-12-10T15:55:59.686307: step 4434, loss 0.14899, acc 0.984375, prec 0.0863807, recall 0.892365
2017-12-10T15:55:59.888857: step 4435, loss 0.092122, acc 0.953125, prec 0.0863748, recall 0.892365
2017-12-10T15:56:00.085382: step 4436, loss 0.0842667, acc 0.984375, prec 0.0864348, recall 0.89244
2017-12-10T15:56:00.284907: step 4437, loss 0.125986, acc 0.953125, prec 0.086429, recall 0.89244
2017-12-10T15:56:00.480100: step 4438, loss 0.283258, acc 0.96875, prec 0.0865076, recall 0.892541
2017-12-10T15:56:00.682156: step 4439, loss 2.19201, acc 0.9375, prec 0.0865017, recall 0.892333
2017-12-10T15:56:00.888182: step 4440, loss 0.0620288, acc 0.984375, prec 0.0865204, recall 0.892358
2017-12-10T15:56:01.090640: step 4441, loss 0.381851, acc 0.9375, prec 0.0865332, recall 0.892383
2017-12-10T15:56:01.295910: step 4442, loss 0.307808, acc 0.890625, prec 0.0865196, recall 0.892383
2017-12-10T15:56:01.498230: step 4443, loss 0.0795609, acc 0.96875, prec 0.0865157, recall 0.892383
2017-12-10T15:56:01.709475: step 4444, loss 0.147931, acc 0.953125, prec 0.0865098, recall 0.892383
2017-12-10T15:56:01.905643: step 4445, loss 0.41588, acc 0.875, prec 0.0864942, recall 0.892383
2017-12-10T15:56:02.105297: step 4446, loss 0.291044, acc 0.890625, prec 0.0865218, recall 0.892433
2017-12-10T15:56:02.300777: step 4447, loss 0.112824, acc 0.96875, prec 0.0865385, recall 0.892458
2017-12-10T15:56:02.503119: step 4448, loss 0.316021, acc 0.890625, prec 0.0865454, recall 0.892483
2017-12-10T15:56:02.702344: step 4449, loss 0.432227, acc 0.859375, prec 0.0865278, recall 0.892483
2017-12-10T15:56:02.898381: step 4450, loss 0.56857, acc 0.84375, prec 0.0865289, recall 0.892508
2017-12-10T15:56:03.098542: step 4451, loss 0.153504, acc 0.953125, prec 0.0865437, recall 0.892533
2017-12-10T15:56:03.293829: step 4452, loss 0.315469, acc 0.890625, prec 0.0865712, recall 0.892583
2017-12-10T15:56:03.489974: step 4453, loss 0.311424, acc 0.890625, prec 0.0865781, recall 0.892608
2017-12-10T15:56:03.688574: step 4454, loss 0.0587452, acc 0.984375, prec 0.0865968, recall 0.892633
2017-12-10T15:56:03.888155: step 4455, loss 0.423769, acc 0.96875, prec 0.0865929, recall 0.892633
2017-12-10T15:56:04.082183: step 4456, loss 0.474497, acc 0.921875, prec 0.0866243, recall 0.892683
2017-12-10T15:56:04.284104: step 4457, loss 0.0624266, acc 0.96875, prec 0.0866204, recall 0.892683
2017-12-10T15:56:04.480898: step 4458, loss 0.470627, acc 0.890625, prec 0.0866067, recall 0.892683
2017-12-10T15:56:04.683087: step 4459, loss 0.461653, acc 0.921875, prec 0.0866175, recall 0.892708
2017-12-10T15:56:04.880520: step 4460, loss 0.16353, acc 0.9375, prec 0.0866509, recall 0.892758
2017-12-10T15:56:05.086097: step 4461, loss 0.199232, acc 0.921875, prec 0.0866411, recall 0.892758
2017-12-10T15:56:05.283568: step 4462, loss 0.339245, acc 0.9375, prec 0.0866539, recall 0.892783
2017-12-10T15:56:05.482007: step 4463, loss 0.0831587, acc 0.953125, prec 0.086648, recall 0.892783
2017-12-10T15:56:05.683364: step 4464, loss 0.0596744, acc 0.984375, prec 0.0866461, recall 0.892783
2017-12-10T15:56:05.887068: step 4465, loss 0.291955, acc 0.9375, prec 0.0867206, recall 0.892882
2017-12-10T15:56:06.086624: step 4466, loss 0.165117, acc 0.9375, prec 0.0867127, recall 0.892882
2017-12-10T15:56:06.285369: step 4467, loss 0.294132, acc 0.984375, prec 0.0867931, recall 0.892981
2017-12-10T15:56:06.483090: step 4468, loss 0.0726643, acc 0.984375, prec 0.0867911, recall 0.892981
2017-12-10T15:56:06.683415: step 4469, loss 0.0949377, acc 0.96875, prec 0.0867872, recall 0.892981
2017-12-10T15:56:06.886882: step 4470, loss 0.15516, acc 0.953125, prec 0.0868224, recall 0.893031
2017-12-10T15:56:07.084742: step 4471, loss 0.251143, acc 0.96875, prec 0.0868391, recall 0.893056
2017-12-10T15:56:07.295097: step 4472, loss 0.0973561, acc 0.96875, prec 0.0868557, recall 0.89308
2017-12-10T15:56:07.480336: step 4473, loss 2.33026, acc 0.923077, prec 0.0868499, recall 0.892874
2017-12-10T15:56:07.690392: step 4474, loss 0.0661938, acc 0.953125, prec 0.086844, recall 0.892874
2017-12-10T15:56:07.887499: step 4475, loss 0.111728, acc 0.984375, prec 0.0868626, recall 0.892898
2017-12-10T15:56:08.088283: step 4476, loss 0.216176, acc 0.953125, prec 0.0868773, recall 0.892923
2017-12-10T15:56:08.290415: step 4477, loss 0.105801, acc 0.984375, prec 0.0868753, recall 0.892923
2017-12-10T15:56:08.492289: step 4478, loss 0.222004, acc 0.953125, prec 0.0869105, recall 0.892973
2017-12-10T15:56:08.692330: step 4479, loss 0.0539448, acc 0.984375, prec 0.0869291, recall 0.892997
2017-12-10T15:56:08.890924: step 4480, loss 0.121208, acc 0.9375, prec 0.0869624, recall 0.893047
2017-12-10T15:56:09.087872: step 4481, loss 0.198311, acc 0.921875, prec 0.0869526, recall 0.893047
2017-12-10T15:56:09.288632: step 4482, loss 0.096614, acc 0.96875, prec 0.0869898, recall 0.893096
2017-12-10T15:56:09.485552: step 4483, loss 0.407022, acc 0.921875, prec 0.08698, recall 0.893096
2017-12-10T15:56:09.684383: step 4484, loss 0.500393, acc 0.90625, prec 0.0869888, recall 0.893121
2017-12-10T15:56:09.885924: step 4485, loss 0.336689, acc 0.890625, prec 0.0869751, recall 0.893121
2017-12-10T15:56:10.087182: step 4486, loss 0.342561, acc 0.90625, prec 0.0870044, recall 0.89317
2017-12-10T15:56:10.289185: step 4487, loss 0.0943334, acc 0.96875, prec 0.0870415, recall 0.89322
2017-12-10T15:56:10.486035: step 4488, loss 0.232818, acc 0.953125, prec 0.0870767, recall 0.893269
2017-12-10T15:56:10.684215: step 4489, loss 0.277366, acc 0.90625, prec 0.087065, recall 0.893269
2017-12-10T15:56:10.883788: step 4490, loss 0.255844, acc 0.9375, prec 0.0870982, recall 0.893318
2017-12-10T15:56:11.086842: step 4491, loss 0.290311, acc 0.875, prec 0.087144, recall 0.893392
2017-12-10T15:56:11.277841: step 4492, loss 0.148361, acc 0.9375, prec 0.0871567, recall 0.893416
2017-12-10T15:56:11.476869: step 4493, loss 0.102328, acc 0.96875, prec 0.0871528, recall 0.893416
2017-12-10T15:56:11.680838: step 4494, loss 0.282419, acc 0.9375, prec 0.0871449, recall 0.893416
2017-12-10T15:56:11.881889: step 4495, loss 0.220014, acc 0.921875, prec 0.0871762, recall 0.893465
2017-12-10T15:56:12.081915: step 4496, loss 0.293481, acc 0.921875, prec 0.0871869, recall 0.89349
2017-12-10T15:56:12.277524: step 4497, loss 0.175299, acc 0.9375, prec 0.08722, recall 0.893539
2017-12-10T15:56:12.475361: step 4498, loss 0.175752, acc 0.953125, prec 0.0872346, recall 0.893563
2017-12-10T15:56:12.675466: step 4499, loss 0.0149633, acc 1, prec 0.0872346, recall 0.893563
2017-12-10T15:56:12.869664: step 4500, loss 0.18098, acc 0.9375, prec 0.0872268, recall 0.893563
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_3/1512938428/checkpoints/model-4500

2017-12-10T15:56:14.052681: step 4501, loss 0.210787, acc 0.96875, prec 0.0872229, recall 0.893563
2017-12-10T15:56:14.259378: step 4502, loss 0.546193, acc 0.96875, prec 0.0872394, recall 0.893588
2017-12-10T15:56:14.473323: step 4503, loss 0.105453, acc 0.96875, prec 0.087256, recall 0.893612
2017-12-10T15:56:14.672704: step 4504, loss 0.700539, acc 0.96875, prec 0.0872726, recall 0.893637
2017-12-10T15:56:14.876392: step 4505, loss 0.0850073, acc 0.984375, prec 0.0872911, recall 0.893661
2017-12-10T15:56:15.075638: step 4506, loss 0.137519, acc 0.984375, prec 0.0873301, recall 0.89371
2017-12-10T15:56:15.271749: step 4507, loss 0.350692, acc 0.96875, prec 0.0873466, recall 0.893734
2017-12-10T15:56:15.473604: step 4508, loss 0.106833, acc 0.953125, prec 0.0873612, recall 0.893759
2017-12-10T15:56:15.671700: step 4509, loss 0.262046, acc 0.9375, prec 0.0873534, recall 0.893759
2017-12-10T15:56:15.872788: step 4510, loss 0.13366, acc 0.953125, prec 0.0873475, recall 0.893759
2017-12-10T15:56:16.078300: step 4511, loss 0.0732879, acc 0.984375, prec 0.087366, recall 0.893783
2017-12-10T15:56:16.277626: step 4512, loss 0.223603, acc 0.9375, prec 0.0873786, recall 0.893807
2017-12-10T15:56:16.473203: step 4513, loss 0.200438, acc 0.9375, prec 0.0873913, recall 0.893832
2017-12-10T15:56:16.675064: step 4514, loss 0.196733, acc 0.953125, prec 0.0874672, recall 0.893929
2017-12-10T15:56:16.874257: step 4515, loss 0.192281, acc 0.9375, prec 0.0875003, recall 0.893978
2017-12-10T15:56:17.076857: step 4516, loss 0.0429257, acc 0.984375, prec 0.0874983, recall 0.893978
2017-12-10T15:56:17.280812: step 4517, loss 0.122229, acc 0.984375, prec 0.0875168, recall 0.894002
2017-12-10T15:56:17.482051: step 4518, loss 0.180642, acc 0.953125, prec 0.0875109, recall 0.894002
2017-12-10T15:56:17.671992: step 4519, loss 0.239058, acc 0.9375, prec 0.0875235, recall 0.894026
2017-12-10T15:56:17.873975: step 4520, loss 0.163798, acc 0.984375, prec 0.0875625, recall 0.894075
2017-12-10T15:56:18.074852: step 4521, loss 0.046641, acc 0.984375, prec 0.0875809, recall 0.894099
2017-12-10T15:56:18.277144: step 4522, loss 0.00704679, acc 1, prec 0.0875809, recall 0.894099
2017-12-10T15:56:18.472360: step 4523, loss 0.14492, acc 0.984375, prec 0.0875994, recall 0.894123
2017-12-10T15:56:18.675134: step 4524, loss 0.159269, acc 0.96875, prec 0.0875955, recall 0.894123
2017-12-10T15:56:18.878648: step 4525, loss 0.0328486, acc 0.984375, prec 0.0875935, recall 0.894123
2017-12-10T15:56:19.077838: step 4526, loss 0.104261, acc 0.953125, prec 0.0876081, recall 0.894147
2017-12-10T15:56:19.276357: step 4527, loss 0.0194811, acc 1, prec 0.0876285, recall 0.894171
2017-12-10T15:56:19.476246: step 4528, loss 0.302271, acc 0.984375, prec 0.0876674, recall 0.89422
2017-12-10T15:56:19.676348: step 4529, loss 0.0968165, acc 0.96875, prec 0.0877044, recall 0.894268
2017-12-10T15:56:19.878638: step 4530, loss 0.0739609, acc 0.953125, prec 0.0877189, recall 0.894292
2017-12-10T15:56:20.076060: step 4531, loss 0.0101391, acc 1, prec 0.0877189, recall 0.894292
2017-12-10T15:56:20.276624: step 4532, loss 0.0657129, acc 0.96875, prec 0.0877558, recall 0.89434
2017-12-10T15:56:20.473596: step 4533, loss 0.087231, acc 0.9375, prec 0.0877684, recall 0.894365
2017-12-10T15:56:20.671223: step 4534, loss 0.114597, acc 0.984375, prec 0.0877664, recall 0.894365
2017-12-10T15:56:20.865711: step 4535, loss 0.254766, acc 0.953125, prec 0.0877605, recall 0.894365
2017-12-10T15:56:21.062381: step 4536, loss 0.0288842, acc 0.984375, prec 0.0877586, recall 0.894365
2017-12-10T15:56:21.261255: step 4537, loss 0.0213345, acc 0.984375, prec 0.087777, recall 0.894389
2017-12-10T15:56:21.461298: step 4538, loss 0.164507, acc 0.9375, prec 0.0877692, recall 0.894389
2017-12-10T15:56:21.658964: step 4539, loss 0.00549713, acc 1, prec 0.0877692, recall 0.894389
2017-12-10T15:56:21.859277: step 4540, loss 0.0347506, acc 0.984375, prec 0.087808, recall 0.894437
2017-12-10T15:56:22.055880: step 4541, loss 0.147514, acc 0.984375, prec 0.0878469, recall 0.894485
2017-12-10T15:56:22.254881: step 4542, loss 0.395809, acc 1, prec 0.0878877, recall 0.894533
2017-12-10T15:56:22.458429: step 4543, loss 0.190893, acc 0.984375, prec 0.0879266, recall 0.894581
2017-12-10T15:56:22.658587: step 4544, loss 0.0853976, acc 1, prec 0.087947, recall 0.894605
2017-12-10T15:56:22.860242: step 4545, loss 0.123943, acc 0.96875, prec 0.0879839, recall 0.894653
2017-12-10T15:56:23.063577: step 4546, loss 0.673099, acc 1, prec 0.0880043, recall 0.894677
2017-12-10T15:56:23.265659: step 4547, loss 0.110126, acc 0.953125, prec 0.0879984, recall 0.894677
2017-12-10T15:56:23.465251: step 4548, loss 0.0264264, acc 0.984375, prec 0.0879964, recall 0.894677
2017-12-10T15:56:23.667583: step 4549, loss 0.408878, acc 0.9375, prec 0.0879885, recall 0.894677
2017-12-10T15:56:23.867608: step 4550, loss 0.307961, acc 0.9375, prec 0.0880011, recall 0.894701
2017-12-10T15:56:24.065513: step 4551, loss 0.100974, acc 0.96875, prec 0.0880583, recall 0.894773
2017-12-10T15:56:24.268975: step 4552, loss 0.147085, acc 0.96875, prec 0.0880952, recall 0.894821
2017-12-10T15:56:24.469499: step 4553, loss 0.0845941, acc 0.96875, prec 0.0881116, recall 0.894844
2017-12-10T15:56:24.668642: step 4554, loss 0.102754, acc 0.96875, prec 0.0881485, recall 0.894892
2017-12-10T15:56:24.869330: step 4555, loss 0.1355, acc 0.9375, prec 0.0881406, recall 0.894892
2017-12-10T15:56:25.067798: step 4556, loss 0.846826, acc 0.90625, prec 0.0881695, recall 0.89494
2017-12-10T15:56:25.270752: step 4557, loss 0.0844726, acc 0.984375, prec 0.0881676, recall 0.89494
2017-12-10T15:56:25.468061: step 4558, loss 0.0899811, acc 0.953125, prec 0.0881617, recall 0.89494
2017-12-10T15:56:25.667050: step 4559, loss 0.155961, acc 0.96875, prec 0.0881781, recall 0.894964
2017-12-10T15:56:25.869842: step 4560, loss 0.159586, acc 0.9375, prec 0.0881906, recall 0.894988
2017-12-10T15:56:26.070396: step 4561, loss 0.350228, acc 0.9375, prec 0.0882031, recall 0.895011
2017-12-10T15:56:26.274705: step 4562, loss 0.0443526, acc 0.984375, prec 0.0882011, recall 0.895011
2017-12-10T15:56:26.467993: step 4563, loss 0.45874, acc 0.921875, prec 0.0882116, recall 0.895035
2017-12-10T15:56:26.671119: step 4564, loss 0.070857, acc 0.96875, prec 0.0882077, recall 0.895035
2017-12-10T15:56:26.869660: step 4565, loss 0.0376249, acc 0.984375, prec 0.0882465, recall 0.895083
2017-12-10T15:56:27.069397: step 4566, loss 0.233239, acc 0.921875, prec 0.0882366, recall 0.895083
2017-12-10T15:56:27.266531: step 4567, loss 0.171096, acc 0.953125, prec 0.0882307, recall 0.895083
2017-12-10T15:56:27.466004: step 4568, loss 0.209248, acc 0.96875, prec 0.0882675, recall 0.89513
2017-12-10T15:56:27.665678: step 4569, loss 0.183375, acc 0.96875, prec 0.0883246, recall 0.895201
2017-12-10T15:56:27.865864: step 4570, loss 0.178025, acc 0.9375, prec 0.0883167, recall 0.895201
2017-12-10T15:56:28.080579: step 4571, loss 0.188018, acc 0.984375, prec 0.0883351, recall 0.895225
2017-12-10T15:56:28.284221: step 4572, loss 0.122965, acc 0.984375, prec 0.0883739, recall 0.895273
2017-12-10T15:56:28.486700: step 4573, loss 0.0599448, acc 0.96875, prec 0.0884106, recall 0.89532
2017-12-10T15:56:28.689750: step 4574, loss 0.0456182, acc 0.984375, prec 0.088429, recall 0.895344
2017-12-10T15:56:28.888918: step 4575, loss 0.176872, acc 0.984375, prec 0.0884474, recall 0.895367
2017-12-10T15:56:29.085489: step 4576, loss 0.00447695, acc 1, prec 0.0884474, recall 0.895367
2017-12-10T15:56:29.288829: step 4577, loss 0.0215152, acc 0.984375, prec 0.0884657, recall 0.895391
2017-12-10T15:56:29.496259: step 4578, loss 0.167453, acc 0.96875, prec 0.0884821, recall 0.895415
2017-12-10T15:56:29.693899: step 4579, loss 0.079247, acc 0.953125, prec 0.0884762, recall 0.895415
2017-12-10T15:56:29.899490: step 4580, loss 0.0417378, acc 0.96875, prec 0.0884723, recall 0.895415
2017-12-10T15:56:30.102625: step 4581, loss 0.654021, acc 1, prec 0.0884926, recall 0.895438
2017-12-10T15:56:30.310104: step 4582, loss 0.115949, acc 0.96875, prec 0.0885497, recall 0.895509
2017-12-10T15:56:30.508941: step 4583, loss 0.910192, acc 0.96875, prec 0.0885661, recall 0.895532
2017-12-10T15:56:30.709648: step 4584, loss 0.0647707, acc 0.953125, prec 0.0885805, recall 0.895556
2017-12-10T15:56:30.905322: step 4585, loss 0.162754, acc 0.9375, prec 0.0885726, recall 0.895556
2017-12-10T15:56:31.101616: step 4586, loss 0.0904398, acc 0.984375, prec 0.0885909, recall 0.89558
2017-12-10T15:56:31.305047: step 4587, loss 0.0897718, acc 0.953125, prec 0.088585, recall 0.89558
2017-12-10T15:56:31.512367: step 4588, loss 0.135758, acc 0.953125, prec 0.0886197, recall 0.895627
2017-12-10T15:56:31.714767: step 4589, loss 0.117521, acc 0.9375, prec 0.0886118, recall 0.895627
2017-12-10T15:56:31.915760: step 4590, loss 0.149777, acc 0.9375, prec 0.0886039, recall 0.895627
2017-12-10T15:56:32.115687: step 4591, loss 0.104654, acc 0.96875, prec 0.0886203, recall 0.89565
2017-12-10T15:56:32.312638: step 4592, loss 0.0694863, acc 0.96875, prec 0.0886367, recall 0.895674
2017-12-10T15:56:32.513860: step 4593, loss 0.104241, acc 0.984375, prec 0.088655, recall 0.895697
2017-12-10T15:56:32.714975: step 4594, loss 0.314484, acc 0.953125, prec 0.0886491, recall 0.895697
2017-12-10T15:56:32.918702: step 4595, loss 0.0151083, acc 1, prec 0.0886491, recall 0.895697
2017-12-10T15:56:33.115607: step 4596, loss 0.132515, acc 0.96875, prec 0.0886451, recall 0.895697
2017-12-10T15:56:33.312139: step 4597, loss 0.230331, acc 0.953125, prec 0.0886798, recall 0.895744
2017-12-10T15:56:33.509065: step 4598, loss 0.115806, acc 0.953125, prec 0.0886942, recall 0.895768
2017-12-10T15:56:33.708943: step 4599, loss 0.441548, acc 0.890625, prec 0.0887413, recall 0.895838
2017-12-10T15:56:33.907398: step 4600, loss 0.345271, acc 0.90625, prec 0.0887498, recall 0.895861
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_3/1512938428/checkpoints/model-4600

2017-12-10T15:56:35.389139: step 4601, loss 0.206422, acc 0.9375, prec 0.0887621, recall 0.895885
2017-12-10T15:56:35.586308: step 4602, loss 0.047847, acc 0.984375, prec 0.0887805, recall 0.895908
2017-12-10T15:56:35.787709: step 4603, loss 0.271187, acc 0.9375, prec 0.0887726, recall 0.895908
2017-12-10T15:56:35.984068: step 4604, loss 1.15966, acc 0.953125, prec 0.0887869, recall 0.895932
2017-12-10T15:56:36.185457: step 4605, loss 0.0799955, acc 0.96875, prec 0.088783, recall 0.895932
2017-12-10T15:56:36.385744: step 4606, loss 0.281391, acc 0.953125, prec 0.088777, recall 0.895932
2017-12-10T15:56:36.586979: step 4607, loss 0.0461657, acc 0.984375, prec 0.0887751, recall 0.895932
2017-12-10T15:56:36.785798: step 4608, loss 0.195078, acc 0.953125, prec 0.0887894, recall 0.895955
2017-12-10T15:56:36.984482: step 4609, loss 0.0287439, acc 0.984375, prec 0.088828, recall 0.896002
2017-12-10T15:56:37.181630: step 4610, loss 0.246982, acc 0.9375, prec 0.0888607, recall 0.896048
2017-12-10T15:56:37.389083: step 4611, loss 0.00766719, acc 1, prec 0.088881, recall 0.896072
2017-12-10T15:56:37.587039: step 4612, loss 0.0127538, acc 1, prec 0.088881, recall 0.896072
2017-12-10T15:56:37.789285: step 4613, loss 0.0918795, acc 0.96875, prec 0.0888973, recall 0.896095
2017-12-10T15:56:37.990533: step 4614, loss 0.13931, acc 0.953125, prec 0.0889116, recall 0.896118
2017-12-10T15:56:38.192388: step 4615, loss 0.259103, acc 0.9375, prec 0.088924, recall 0.896142
2017-12-10T15:56:38.393024: step 4616, loss 0.0921225, acc 0.984375, prec 0.0889423, recall 0.896165
2017-12-10T15:56:38.588781: step 4617, loss 0.0635914, acc 0.96875, prec 0.0889789, recall 0.896212
2017-12-10T15:56:38.785637: step 4618, loss 0.0997824, acc 0.984375, prec 0.0889972, recall 0.896235
2017-12-10T15:56:38.989903: step 4619, loss 0.0776245, acc 0.96875, prec 0.0889932, recall 0.896235
2017-12-10T15:56:39.189017: step 4620, loss 0.0864281, acc 0.984375, prec 0.0890318, recall 0.896281
2017-12-10T15:56:39.391017: step 4621, loss 0.216221, acc 0.984375, prec 0.0890704, recall 0.896328
2017-12-10T15:56:39.591964: step 4622, loss 0.0747738, acc 0.96875, prec 0.0890867, recall 0.896351
2017-12-10T15:56:39.796685: step 4623, loss 0.0579262, acc 0.984375, prec 0.0891049, recall 0.896374
2017-12-10T15:56:39.991900: step 4624, loss 0.0030522, acc 1, prec 0.0891049, recall 0.896374
2017-12-10T15:56:40.220493: step 4625, loss 0.288593, acc 0.96875, prec 0.0891618, recall 0.896444
2017-12-10T15:56:40.424310: step 4626, loss 0.160069, acc 0.96875, prec 0.0891983, recall 0.89649
2017-12-10T15:56:40.621243: step 4627, loss 0.0148229, acc 1, prec 0.0892388, recall 0.896536
2017-12-10T15:56:40.822864: step 4628, loss 0.0170637, acc 1, prec 0.0892591, recall 0.896559
2017-12-10T15:56:41.027874: step 4629, loss 0.00797157, acc 1, prec 0.0893199, recall 0.896629
2017-12-10T15:56:41.227008: step 4630, loss 0.0109608, acc 1, prec 0.0893401, recall 0.896652
2017-12-10T15:56:41.429425: step 4631, loss 0.0402987, acc 0.984375, prec 0.0893381, recall 0.896652
2017-12-10T15:56:41.630290: step 4632, loss 0.115729, acc 0.984375, prec 0.0893361, recall 0.896652
2017-12-10T15:56:41.832024: step 4633, loss 0.103456, acc 0.96875, prec 0.0893322, recall 0.896652
2017-12-10T15:56:42.034436: step 4634, loss 0.204912, acc 0.921875, prec 0.0893425, recall 0.896675
2017-12-10T15:56:42.236342: step 4635, loss 0.0230405, acc 1, prec 0.0893425, recall 0.896675
2017-12-10T15:56:42.432338: step 4636, loss 0.00977453, acc 1, prec 0.0893425, recall 0.896675
2017-12-10T15:56:42.632953: step 4637, loss 0.016919, acc 1, prec 0.0893425, recall 0.896675
2017-12-10T15:56:42.827449: step 4638, loss 0.0278436, acc 1, prec 0.0893627, recall 0.896698
2017-12-10T15:56:43.026020: step 4639, loss 0.195675, acc 0.96875, prec 0.0893588, recall 0.896698
2017-12-10T15:56:43.228202: step 4640, loss 0.128769, acc 0.984375, prec 0.0893568, recall 0.896698
2017-12-10T15:56:43.430757: step 4641, loss 0.0392032, acc 0.96875, prec 0.0893528, recall 0.896698
2017-12-10T15:56:43.630981: step 4642, loss 0.0109582, acc 1, prec 0.0893528, recall 0.896698
2017-12-10T15:56:43.827391: step 4643, loss 0.0403816, acc 0.984375, prec 0.0893508, recall 0.896698
2017-12-10T15:56:44.026119: step 4644, loss 0.0117734, acc 1, prec 0.0893508, recall 0.896698
2017-12-10T15:56:44.231684: step 4645, loss 0.042989, acc 0.984375, prec 0.0893488, recall 0.896698
2017-12-10T15:56:44.438557: step 4646, loss 0.0708289, acc 0.984375, prec 0.0893671, recall 0.896721
2017-12-10T15:56:44.640101: step 4647, loss 0.118767, acc 0.953125, prec 0.0894016, recall 0.896767
2017-12-10T15:56:44.841831: step 4648, loss 0.247653, acc 0.96875, prec 0.0893976, recall 0.896767
2017-12-10T15:56:45.041226: step 4649, loss 0.135816, acc 0.984375, prec 0.0894159, recall 0.89679
2017-12-10T15:56:45.243115: step 4650, loss 0.0168964, acc 0.984375, prec 0.0894139, recall 0.89679
2017-12-10T15:56:45.442884: step 4651, loss 0.071995, acc 0.984375, prec 0.0894119, recall 0.89679
2017-12-10T15:56:45.643391: step 4652, loss 0.033191, acc 0.984375, prec 0.0894302, recall 0.896813
2017-12-10T15:56:45.844908: step 4653, loss 0.00886495, acc 1, prec 0.0894504, recall 0.896836
2017-12-10T15:56:46.050523: step 4654, loss 0.0232077, acc 1, prec 0.0894504, recall 0.896836
2017-12-10T15:56:46.252634: step 4655, loss 0.0437142, acc 0.984375, prec 0.0894687, recall 0.896859
2017-12-10T15:56:46.453216: step 4656, loss 0.581205, acc 0.96875, prec 0.0895051, recall 0.896905
2017-12-10T15:56:46.654458: step 4657, loss 3.11291, acc 0.953125, prec 0.0896023, recall 0.89682
2017-12-10T15:56:46.851041: step 4658, loss 0.427083, acc 1, prec 0.0896225, recall 0.896843
2017-12-10T15:56:47.054284: step 4659, loss 0.122003, acc 0.984375, prec 0.0896205, recall 0.896843
2017-12-10T15:56:47.255657: step 4660, loss 0.147745, acc 0.953125, prec 0.0896348, recall 0.896866
2017-12-10T15:56:47.460324: step 4661, loss 0.347249, acc 0.9375, prec 0.0896268, recall 0.896866
2017-12-10T15:56:47.663986: step 4662, loss 0.310266, acc 0.859375, prec 0.0896291, recall 0.896889
2017-12-10T15:56:47.864891: step 4663, loss 0.139395, acc 0.953125, prec 0.0896232, recall 0.896889
2017-12-10T15:56:48.068581: step 4664, loss 0.381636, acc 0.890625, prec 0.0896294, recall 0.896912
2017-12-10T15:56:48.266568: step 4665, loss 0.533585, acc 0.890625, prec 0.0896155, recall 0.896912
2017-12-10T15:56:48.463538: step 4666, loss 0.286849, acc 0.921875, prec 0.0896258, recall 0.896935
2017-12-10T15:56:48.659956: step 4667, loss 0.439187, acc 0.890625, prec 0.0896119, recall 0.896935
2017-12-10T15:56:48.854499: step 4668, loss 0.342437, acc 0.921875, prec 0.0896019, recall 0.896935
2017-12-10T15:56:49.053454: step 4669, loss 0.671765, acc 0.859375, prec 0.0896042, recall 0.896958
2017-12-10T15:56:49.251888: step 4670, loss 0.406492, acc 0.859375, prec 0.0895863, recall 0.896958
2017-12-10T15:56:49.454756: step 4671, loss 0.488948, acc 0.859375, prec 0.0895685, recall 0.896958
2017-12-10T15:56:49.651941: step 4672, loss 0.416401, acc 0.84375, prec 0.0895688, recall 0.89698
2017-12-10T15:56:49.845661: step 4673, loss 0.297438, acc 0.921875, prec 0.089579, recall 0.897003
2017-12-10T15:56:50.040197: step 4674, loss 0.337717, acc 0.90625, prec 0.0895671, recall 0.897003
2017-12-10T15:56:50.236604: step 4675, loss 0.202595, acc 0.9375, prec 0.0895592, recall 0.897003
2017-12-10T15:56:50.442782: step 4676, loss 0.766567, acc 0.859375, prec 0.0895615, recall 0.897026
2017-12-10T15:56:50.642873: step 4677, loss 0.250418, acc 0.953125, prec 0.0895555, recall 0.897026
2017-12-10T15:56:50.835722: step 4678, loss 0.262892, acc 0.9375, prec 0.0895678, recall 0.897049
2017-12-10T15:56:51.032582: step 4679, loss 0.190583, acc 0.9375, prec 0.0895598, recall 0.897049
2017-12-10T15:56:51.234515: step 4680, loss 0.0233039, acc 1, prec 0.0895598, recall 0.897049
2017-12-10T15:56:51.438010: step 4681, loss 0.0863561, acc 0.953125, prec 0.0895539, recall 0.897049
2017-12-10T15:56:51.638386: step 4682, loss 0.193844, acc 0.96875, prec 0.0895701, recall 0.897072
2017-12-10T15:56:51.841582: step 4683, loss 0.0848119, acc 0.953125, prec 0.0895641, recall 0.897072
2017-12-10T15:56:52.044898: step 4684, loss 0.0217706, acc 0.984375, prec 0.0896025, recall 0.897117
2017-12-10T15:56:52.243091: step 4685, loss 0.0591641, acc 0.96875, prec 0.0895985, recall 0.897117
2017-12-10T15:56:52.448592: step 4686, loss 0.0989327, acc 0.984375, prec 0.0896368, recall 0.897163
2017-12-10T15:56:52.653668: step 4687, loss 0.0376256, acc 0.984375, prec 0.0896349, recall 0.897163
2017-12-10T15:56:52.853037: step 4688, loss 0.0504836, acc 1, prec 0.0896752, recall 0.897209
2017-12-10T15:56:53.056514: step 4689, loss 0.0517264, acc 0.96875, prec 0.0896914, recall 0.897231
2017-12-10T15:56:53.263738: step 4690, loss 0.0647167, acc 0.96875, prec 0.0896874, recall 0.897231
2017-12-10T15:56:53.459702: step 4691, loss 0.13673, acc 0.953125, prec 0.0896814, recall 0.897231
2017-12-10T15:56:53.658172: step 4692, loss 0.00628186, acc 1, prec 0.0897016, recall 0.897254
2017-12-10T15:56:53.859972: step 4693, loss 0.169452, acc 0.96875, prec 0.0897178, recall 0.897277
2017-12-10T15:56:54.063675: step 4694, loss 0.0780151, acc 1, prec 0.0897379, recall 0.8973
2017-12-10T15:56:54.271055: step 4695, loss 0.0285066, acc 1, prec 0.0897782, recall 0.897345
2017-12-10T15:56:54.469762: step 4696, loss 0.101745, acc 0.96875, prec 0.0897944, recall 0.897368
2017-12-10T15:56:54.672331: step 4697, loss 0.0295182, acc 1, prec 0.0898347, recall 0.897413
2017-12-10T15:56:54.873929: step 4698, loss 0.00530419, acc 1, prec 0.089875, recall 0.897459
2017-12-10T15:56:55.072861: step 4699, loss 6.78705, acc 0.984375, prec 0.089875, recall 0.89726
2017-12-10T15:56:55.278422: step 4700, loss 0.0727158, acc 0.984375, prec 0.089873, recall 0.89726
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_3/1512938428/checkpoints/model-4700

2017-12-10T15:56:56.518521: step 4701, loss 0.129756, acc 0.984375, prec 0.0898911, recall 0.897283
2017-12-10T15:56:56.722641: step 4702, loss 0.088779, acc 0.96875, prec 0.0898871, recall 0.897283
2017-12-10T15:56:56.922526: step 4703, loss 0.199823, acc 0.953125, prec 0.0898812, recall 0.897283
2017-12-10T15:56:57.125855: step 4704, loss 0.419604, acc 0.96875, prec 0.0899779, recall 0.897396
2017-12-10T15:56:57.327611: step 4705, loss 0.129085, acc 0.953125, prec 0.089992, recall 0.897419
2017-12-10T15:56:57.526335: step 4706, loss 0.139734, acc 0.96875, prec 0.0900283, recall 0.897464
2017-12-10T15:56:57.720569: step 4707, loss 0.101864, acc 0.96875, prec 0.0900445, recall 0.897487
2017-12-10T15:56:57.926699: step 4708, loss 0.216938, acc 0.96875, prec 0.0900807, recall 0.897532
2017-12-10T15:56:58.129230: step 4709, loss 0.324876, acc 0.90625, prec 0.0900889, recall 0.897555
2017-12-10T15:56:58.323329: step 4710, loss 0.100232, acc 0.9375, prec 0.0900809, recall 0.897555
2017-12-10T15:56:58.523422: step 4711, loss 0.217769, acc 0.921875, prec 0.0900911, recall 0.897577
2017-12-10T15:56:58.719954: step 4712, loss 0.105325, acc 0.953125, prec 0.0901253, recall 0.897622
2017-12-10T15:56:58.924638: step 4713, loss 0.254713, acc 0.921875, prec 0.0901355, recall 0.897645
2017-12-10T15:56:59.124600: step 4714, loss 0.265109, acc 0.921875, prec 0.0901456, recall 0.897667
2017-12-10T15:56:59.332857: step 4715, loss 0.06675, acc 1, prec 0.0901657, recall 0.89769
2017-12-10T15:56:59.536102: step 4716, loss 0.277057, acc 0.890625, prec 0.0901518, recall 0.89769
2017-12-10T15:56:59.736352: step 4717, loss 0.161057, acc 0.9375, prec 0.0901438, recall 0.89769
2017-12-10T15:56:59.933951: step 4718, loss 0.182644, acc 0.9375, prec 0.0901761, recall 0.897735
2017-12-10T15:57:00.134016: step 4719, loss 0.293252, acc 0.90625, prec 0.0901641, recall 0.897735
2017-12-10T15:57:00.333387: step 4720, loss 0.105209, acc 0.953125, prec 0.0901782, recall 0.897757
2017-12-10T15:57:00.532826: step 4721, loss 0.486555, acc 0.890625, prec 0.0902045, recall 0.897802
2017-12-10T15:57:00.732617: step 4722, loss 0.0614996, acc 0.96875, prec 0.0902005, recall 0.897802
2017-12-10T15:57:00.932247: step 4723, loss 0.258007, acc 0.90625, prec 0.0902086, recall 0.897825
2017-12-10T15:57:01.133023: step 4724, loss 0.354716, acc 0.9375, prec 0.0902007, recall 0.897825
2017-12-10T15:57:01.337708: step 4725, loss 0.0623106, acc 0.96875, prec 0.0902369, recall 0.89787
2017-12-10T15:57:01.547757: step 4726, loss 0.0669118, acc 0.96875, prec 0.090273, recall 0.897914
2017-12-10T15:57:01.754427: step 4727, loss 0.225364, acc 0.953125, prec 0.0902871, recall 0.897937
2017-12-10T15:57:01.956815: step 4728, loss 0.148967, acc 0.953125, prec 0.0903012, recall 0.897959
2017-12-10T15:57:02.157284: step 4729, loss 0.0965489, acc 0.984375, prec 0.0903394, recall 0.898004
2017-12-10T15:57:02.363234: step 4730, loss 0.0540579, acc 0.96875, prec 0.0903755, recall 0.898049
2017-12-10T15:57:02.568962: step 4731, loss 0.877796, acc 0.953125, prec 0.0903896, recall 0.898071
2017-12-10T15:57:02.774760: step 4732, loss 0.00354624, acc 1, prec 0.0903896, recall 0.898071
2017-12-10T15:57:02.970682: step 4733, loss 0.104175, acc 0.953125, prec 0.0904238, recall 0.898116
2017-12-10T15:57:03.172434: step 4734, loss 0.0290488, acc 0.984375, prec 0.0904418, recall 0.898138
2017-12-10T15:57:03.372180: step 4735, loss 0.25966, acc 0.953125, prec 0.0904559, recall 0.89816
2017-12-10T15:57:03.577767: step 4736, loss 0.252531, acc 1, prec 0.090496, recall 0.898205
2017-12-10T15:57:03.777307: step 4737, loss 0.0803863, acc 0.9375, prec 0.0905081, recall 0.898227
2017-12-10T15:57:03.978692: step 4738, loss 0.0113763, acc 1, prec 0.0905081, recall 0.898227
2017-12-10T15:57:04.176023: step 4739, loss 0.02274, acc 0.984375, prec 0.0905061, recall 0.898227
2017-12-10T15:57:04.377904: step 4740, loss 0.168557, acc 0.96875, prec 0.0905623, recall 0.898294
2017-12-10T15:57:04.574151: step 4741, loss 0.106757, acc 0.984375, prec 0.0905804, recall 0.898316
2017-12-10T15:57:04.777304: step 4742, loss 0.00835424, acc 1, prec 0.0905804, recall 0.898316
2017-12-10T15:57:04.975663: step 4743, loss 0.116968, acc 0.96875, prec 0.0905764, recall 0.898316
2017-12-10T15:57:05.173602: step 4744, loss 0.144401, acc 0.96875, prec 0.0906125, recall 0.898361
2017-12-10T15:57:05.375220: step 4745, loss 0.319943, acc 0.984375, prec 0.0906506, recall 0.898405
2017-12-10T15:57:05.577492: step 4746, loss 0.19698, acc 0.96875, prec 0.0906466, recall 0.898405
2017-12-10T15:57:05.774900: step 4747, loss 0.117004, acc 0.984375, prec 0.0906446, recall 0.898405
2017-12-10T15:57:05.980928: step 4748, loss 2.91514, acc 0.953125, prec 0.0906406, recall 0.898209
2017-12-10T15:57:06.186915: step 4749, loss 0.159165, acc 0.984375, prec 0.0906787, recall 0.898253
2017-12-10T15:57:06.388990: step 4750, loss 0.0530263, acc 0.96875, prec 0.0906747, recall 0.898253
2017-12-10T15:57:06.586012: step 4751, loss 0.726712, acc 0.875, prec 0.0906787, recall 0.898275
2017-12-10T15:57:06.783767: step 4752, loss 0.0651192, acc 0.96875, prec 0.0906948, recall 0.898298
2017-12-10T15:57:06.985261: step 4753, loss 0.267244, acc 0.9375, prec 0.0907068, recall 0.89832
2017-12-10T15:57:07.188196: step 4754, loss 0.244682, acc 0.9375, prec 0.0907389, recall 0.898364
2017-12-10T15:57:07.391687: step 4755, loss 0.0645986, acc 0.953125, prec 0.0907529, recall 0.898386
2017-12-10T15:57:07.590355: step 4756, loss 0.351824, acc 0.859375, prec 0.0907349, recall 0.898386
2017-12-10T15:57:07.790853: step 4757, loss 0.302147, acc 0.90625, prec 0.0907429, recall 0.898409
2017-12-10T15:57:07.990333: step 4758, loss 0.20239, acc 0.953125, prec 0.090757, recall 0.898431
2017-12-10T15:57:08.187278: step 4759, loss 0.325907, acc 0.859375, prec 0.090799, recall 0.898497
2017-12-10T15:57:08.384383: step 4760, loss 0.500001, acc 0.875, prec 0.090783, recall 0.898497
2017-12-10T15:57:08.583618: step 4761, loss 0.215945, acc 0.90625, prec 0.0907711, recall 0.898497
2017-12-10T15:57:08.783897: step 4762, loss 0.152197, acc 0.921875, prec 0.0907811, recall 0.898519
2017-12-10T15:57:08.979300: step 4763, loss 0.363257, acc 0.90625, prec 0.0907891, recall 0.898541
2017-12-10T15:57:09.177793: step 4764, loss 0.250906, acc 0.9375, prec 0.0907811, recall 0.898541
2017-12-10T15:57:09.375631: step 4765, loss 0.135457, acc 0.9375, prec 0.0907931, recall 0.898563
2017-12-10T15:57:09.577540: step 4766, loss 0.073443, acc 0.96875, prec 0.0907891, recall 0.898563
2017-12-10T15:57:09.772938: step 4767, loss 0.195753, acc 0.953125, prec 0.0908231, recall 0.898607
2017-12-10T15:57:09.970322: step 4768, loss 0.251998, acc 0.984375, prec 0.0908411, recall 0.89863
2017-12-10T15:57:10.176739: step 4769, loss 0.260771, acc 0.921875, prec 0.0908711, recall 0.898674
2017-12-10T15:57:10.375355: step 4770, loss 0.284679, acc 0.9375, prec 0.0909031, recall 0.898718
2017-12-10T15:57:10.573948: step 4771, loss 0.0505492, acc 0.984375, prec 0.0909011, recall 0.898718
2017-12-10T15:57:10.775686: step 4772, loss 0.0492226, acc 0.984375, prec 0.0908991, recall 0.898718
2017-12-10T15:57:10.975369: step 4773, loss 0.118968, acc 0.953125, prec 0.0909531, recall 0.898784
2017-12-10T15:57:11.173558: step 4774, loss 0.115186, acc 0.953125, prec 0.090967, recall 0.898806
2017-12-10T15:57:11.371350: step 4775, loss 0.377721, acc 0.921875, prec 0.090997, recall 0.89885
2017-12-10T15:57:11.570234: step 4776, loss 0.111345, acc 0.953125, prec 0.090991, recall 0.89885
2017-12-10T15:57:11.774487: step 4777, loss 0.155596, acc 0.9375, prec 0.0910229, recall 0.898893
2017-12-10T15:57:11.972504: step 4778, loss 0.279308, acc 1, prec 0.0910828, recall 0.898959
2017-12-10T15:57:12.170832: step 4779, loss 0.0352606, acc 0.984375, prec 0.0910808, recall 0.898959
2017-12-10T15:57:12.368052: step 4780, loss 0.145075, acc 0.96875, prec 0.0910968, recall 0.898981
2017-12-10T15:57:12.574974: step 4781, loss 0.0690597, acc 0.984375, prec 0.0910948, recall 0.898981
2017-12-10T15:57:12.773788: step 4782, loss 0.0494924, acc 0.96875, prec 0.0910908, recall 0.898981
2017-12-10T15:57:12.973186: step 4783, loss 0.975689, acc 0.96875, prec 0.0911467, recall 0.899047
2017-12-10T15:57:13.172539: step 4784, loss 0.16783, acc 0.96875, prec 0.0911626, recall 0.899069
2017-12-10T15:57:13.369489: step 4785, loss 0.206796, acc 1, prec 0.0912225, recall 0.899134
2017-12-10T15:57:13.566635: step 4786, loss 0.166029, acc 0.953125, prec 0.0912165, recall 0.899134
2017-12-10T15:57:13.766025: step 4787, loss 0.0139149, acc 1, prec 0.0912365, recall 0.899156
2017-12-10T15:57:13.966268: step 4788, loss 0.0992312, acc 0.96875, prec 0.0912524, recall 0.899178
2017-12-10T15:57:14.166834: step 4789, loss 0.593624, acc 0.890625, prec 0.0912783, recall 0.899221
2017-12-10T15:57:14.373767: step 4790, loss 1.78583, acc 0.984375, prec 0.0912982, recall 0.899049
2017-12-10T15:57:14.585510: step 4791, loss 0.106901, acc 0.9375, prec 0.0913301, recall 0.899092
2017-12-10T15:57:14.782793: step 4792, loss 0.191397, acc 0.96875, prec 0.091346, recall 0.899114
2017-12-10T15:57:14.977311: step 4793, loss 0.201129, acc 0.984375, prec 0.091364, recall 0.899136
2017-12-10T15:57:15.181681: step 4794, loss 0.514685, acc 0.875, prec 0.0913479, recall 0.899136
2017-12-10T15:57:15.382900: step 4795, loss 0.282889, acc 0.9375, prec 0.0913399, recall 0.899136
2017-12-10T15:57:15.580667: step 4796, loss 0.168325, acc 0.9375, prec 0.0913319, recall 0.899136
2017-12-10T15:57:15.779995: step 4797, loss 0.389942, acc 0.890625, prec 0.0913378, recall 0.899158
2017-12-10T15:57:15.988035: step 4798, loss 0.255238, acc 0.921875, prec 0.0913477, recall 0.89918
2017-12-10T15:57:16.190880: step 4799, loss 0.207331, acc 0.9375, prec 0.0913796, recall 0.899223
2017-12-10T15:57:16.392281: step 4800, loss 0.122225, acc 0.921875, prec 0.0913696, recall 0.899223
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_3/1512938428/checkpoints/model-4800

2017-12-10T15:57:18.169652: step 4801, loss 0.560662, acc 0.890625, prec 0.0913755, recall 0.899245
2017-12-10T15:57:18.366570: step 4802, loss 0.497402, acc 0.890625, prec 0.0913614, recall 0.899245
2017-12-10T15:57:18.565235: step 4803, loss 0.256141, acc 0.921875, prec 0.0913913, recall 0.899288
2017-12-10T15:57:18.760099: step 4804, loss 0.338199, acc 0.90625, prec 0.0914191, recall 0.899332
2017-12-10T15:57:18.960098: step 4805, loss 0.331697, acc 0.953125, prec 0.091413, recall 0.899332
2017-12-10T15:57:19.157304: step 4806, loss 0.183669, acc 0.921875, prec 0.0914229, recall 0.899353
2017-12-10T15:57:19.356621: step 4807, loss 0.350501, acc 0.875, prec 0.0914069, recall 0.899353
2017-12-10T15:57:19.557116: step 4808, loss 0.286025, acc 0.890625, prec 0.0913929, recall 0.899353
2017-12-10T15:57:19.758150: step 4809, loss 0.192148, acc 0.953125, prec 0.0913869, recall 0.899353
2017-12-10T15:57:19.962656: step 4810, loss 0.180826, acc 0.96875, prec 0.0914227, recall 0.899397
2017-12-10T15:57:20.155723: step 4811, loss 0.0718471, acc 0.96875, prec 0.0914585, recall 0.89944
2017-12-10T15:57:20.356798: step 4812, loss 0.146953, acc 0.9375, prec 0.0914704, recall 0.899462
2017-12-10T15:57:20.558598: step 4813, loss 0.129153, acc 0.96875, prec 0.0914664, recall 0.899462
2017-12-10T15:57:20.761474: step 4814, loss 2.91646, acc 0.953125, prec 0.0915021, recall 0.899312
2017-12-10T15:57:20.965550: step 4815, loss 0.0535071, acc 0.96875, prec 0.091518, recall 0.899333
2017-12-10T15:57:21.165822: step 4816, loss 0.278641, acc 0.921875, prec 0.091508, recall 0.899333
2017-12-10T15:57:21.365091: step 4817, loss 0.0654817, acc 0.984375, prec 0.0915259, recall 0.899355
2017-12-10T15:57:21.564458: step 4818, loss 0.127449, acc 0.9375, prec 0.0915179, recall 0.899355
2017-12-10T15:57:21.767908: step 4819, loss 0.172429, acc 0.90625, prec 0.0915058, recall 0.899355
2017-12-10T15:57:21.970685: step 4820, loss 0.227592, acc 0.921875, prec 0.0915157, recall 0.899376
2017-12-10T15:57:22.166826: step 4821, loss 0.583487, acc 0.875, prec 0.0914997, recall 0.899376
2017-12-10T15:57:22.366537: step 4822, loss 0.0492082, acc 0.96875, prec 0.0915156, recall 0.899398
2017-12-10T15:57:22.570444: step 4823, loss 0.309416, acc 0.90625, prec 0.0915036, recall 0.899398
2017-12-10T15:57:22.767869: step 4824, loss 0.285638, acc 0.90625, prec 0.0915114, recall 0.89942
2017-12-10T15:57:22.966631: step 4825, loss 0.425986, acc 0.859375, prec 0.0915133, recall 0.899441
2017-12-10T15:57:23.163371: step 4826, loss 0.0795699, acc 0.984375, prec 0.0915113, recall 0.899441
2017-12-10T15:57:23.367482: step 4827, loss 0.25999, acc 0.953125, prec 0.0915251, recall 0.899463
2017-12-10T15:57:23.566637: step 4828, loss 0.265438, acc 0.953125, prec 0.0915191, recall 0.899463
2017-12-10T15:57:23.766326: step 4829, loss 0.107136, acc 0.953125, prec 0.0915131, recall 0.899463
2017-12-10T15:57:23.967726: step 4830, loss 0.304953, acc 0.890625, prec 0.0914991, recall 0.899463
2017-12-10T15:57:24.168844: step 4831, loss 0.398767, acc 0.90625, prec 0.0915467, recall 0.899528
2017-12-10T15:57:24.367222: step 4832, loss 0.124313, acc 0.953125, prec 0.0915605, recall 0.899549
2017-12-10T15:57:24.565559: step 4833, loss 0.56079, acc 0.9375, prec 0.0915922, recall 0.899592
2017-12-10T15:57:24.764249: step 4834, loss 0.25425, acc 0.921875, prec 0.091602, recall 0.899614
2017-12-10T15:57:24.964976: step 4835, loss 0.205307, acc 0.953125, prec 0.0915961, recall 0.899614
2017-12-10T15:57:25.167477: step 4836, loss 0.0947319, acc 0.953125, prec 0.0916099, recall 0.899635
2017-12-10T15:57:25.369044: step 4837, loss 0.115773, acc 0.953125, prec 0.0916237, recall 0.899657
2017-12-10T15:57:25.568547: step 4838, loss 0.161517, acc 0.9375, prec 0.0916157, recall 0.899657
2017-12-10T15:57:25.775460: step 4839, loss 0.0348695, acc 0.984375, prec 0.0916534, recall 0.8997
2017-12-10T15:57:25.981917: step 4840, loss 0.161458, acc 0.9375, prec 0.0916454, recall 0.8997
2017-12-10T15:57:26.181410: step 4841, loss 0.129421, acc 0.984375, prec 0.091683, recall 0.899743
2017-12-10T15:57:26.382810: step 4842, loss 0.112021, acc 0.96875, prec 0.091679, recall 0.899743
2017-12-10T15:57:26.583434: step 4843, loss 0.427054, acc 0.984375, prec 0.0917365, recall 0.899807
2017-12-10T15:57:26.786591: step 4844, loss 0.1229, acc 0.96875, prec 0.0917523, recall 0.899829
2017-12-10T15:57:26.983793: step 4845, loss 0.39149, acc 0.96875, prec 0.0918078, recall 0.899893
2017-12-10T15:57:27.186376: step 4846, loss 0.0597598, acc 0.96875, prec 0.0918236, recall 0.899914
2017-12-10T15:57:27.386867: step 4847, loss 0.104382, acc 0.9375, prec 0.0918354, recall 0.899936
2017-12-10T15:57:27.581949: step 4848, loss 0.105155, acc 0.96875, prec 0.0918512, recall 0.899957
2017-12-10T15:57:27.781198: step 4849, loss 0.0319457, acc 1, prec 0.0918512, recall 0.899957
2017-12-10T15:57:27.980533: step 4850, loss 0.182387, acc 0.96875, prec 0.091867, recall 0.899979
2017-12-10T15:57:28.180156: step 4851, loss 0.132815, acc 0.96875, prec 0.091863, recall 0.899979
2017-12-10T15:57:28.380396: step 4852, loss 0.047916, acc 0.984375, prec 0.0919006, recall 0.900021
2017-12-10T15:57:28.580051: step 4853, loss 0.111945, acc 0.953125, prec 0.0918946, recall 0.900021
2017-12-10T15:57:28.778794: step 4854, loss 0.0463603, acc 0.96875, prec 0.0918906, recall 0.900021
2017-12-10T15:57:28.976330: step 4855, loss 0.146648, acc 0.984375, prec 0.0919084, recall 0.900043
2017-12-10T15:57:29.174902: step 4856, loss 0.0613262, acc 0.96875, prec 0.0919044, recall 0.900043
2017-12-10T15:57:29.375260: step 4857, loss 0.125112, acc 0.96875, prec 0.0919004, recall 0.900043
2017-12-10T15:57:29.583404: step 4858, loss 0.0633433, acc 0.984375, prec 0.091938, recall 0.900085
2017-12-10T15:57:29.780049: step 4859, loss 0.122225, acc 0.96875, prec 0.0919538, recall 0.900107
2017-12-10T15:57:29.984429: step 4860, loss 0.0547159, acc 0.953125, prec 0.0919478, recall 0.900107
2017-12-10T15:57:30.181428: step 4861, loss 0.14481, acc 0.9375, prec 0.0919793, recall 0.900149
2017-12-10T15:57:30.385284: step 4862, loss 0.00591283, acc 1, prec 0.0919793, recall 0.900149
2017-12-10T15:57:30.582613: step 4863, loss 0.00788545, acc 1, prec 0.0919793, recall 0.900149
2017-12-10T15:57:30.783296: step 4864, loss 0.0268351, acc 1, prec 0.0919991, recall 0.900171
2017-12-10T15:57:30.983730: step 4865, loss 0.0157563, acc 1, prec 0.0920387, recall 0.900213
2017-12-10T15:57:31.195873: step 4866, loss 0.00372256, acc 1, prec 0.0920387, recall 0.900213
2017-12-10T15:57:31.403796: step 4867, loss 0.243069, acc 0.953125, prec 0.0920525, recall 0.900235
2017-12-10T15:57:31.605898: step 4868, loss 0.284359, acc 0.9375, prec 0.0920445, recall 0.900235
2017-12-10T15:57:31.805800: step 4869, loss 2.98233, acc 0.96875, prec 0.0920622, recall 0.900064
2017-12-10T15:57:32.006176: step 4870, loss 2.00579, acc 0.984375, prec 0.0920622, recall 0.899872
2017-12-10T15:57:32.215453: step 4871, loss 0.0749853, acc 0.96875, prec 0.0920582, recall 0.899872
2017-12-10T15:57:32.414782: step 4872, loss 0.036918, acc 1, prec 0.0920582, recall 0.899872
2017-12-10T15:57:32.613928: step 4873, loss 0.170049, acc 0.96875, prec 0.0920542, recall 0.899872
2017-12-10T15:57:32.809894: step 4874, loss 0.243084, acc 0.953125, prec 0.092068, recall 0.899894
2017-12-10T15:57:33.007741: step 4875, loss 0.207342, acc 0.953125, prec 0.0920818, recall 0.899915
2017-12-10T15:57:33.209075: step 4876, loss 0.258686, acc 0.859375, prec 0.092123, recall 0.899979
2017-12-10T15:57:33.411532: step 4877, loss 0.143307, acc 0.9375, prec 0.0921348, recall 0.9
2017-12-10T15:57:33.606132: step 4878, loss 0.618661, acc 0.828125, prec 0.0921325, recall 0.900021
2017-12-10T15:57:33.803805: step 4879, loss 0.304798, acc 0.921875, prec 0.092162, recall 0.900064
2017-12-10T15:57:34.005237: step 4880, loss 0.640625, acc 0.890625, prec 0.0921677, recall 0.900085
2017-12-10T15:57:34.205671: step 4881, loss 0.572437, acc 0.765625, prec 0.0921574, recall 0.900106
2017-12-10T15:57:34.408753: step 4882, loss 0.409525, acc 0.890625, prec 0.0922026, recall 0.90017
2017-12-10T15:57:34.604645: step 4883, loss 0.927379, acc 0.859375, prec 0.0922043, recall 0.900191
2017-12-10T15:57:34.809214: step 4884, loss 0.782669, acc 0.78125, prec 0.092196, recall 0.900212
2017-12-10T15:57:35.006140: step 4885, loss 0.288024, acc 0.90625, prec 0.0922234, recall 0.900255
2017-12-10T15:57:35.206834: step 4886, loss 0.664246, acc 0.796875, prec 0.0922171, recall 0.900276
2017-12-10T15:57:35.405361: step 4887, loss 0.240878, acc 0.953125, prec 0.0922111, recall 0.900276
2017-12-10T15:57:35.609517: step 4888, loss 0.142887, acc 0.9375, prec 0.0922031, recall 0.900276
2017-12-10T15:57:35.809274: step 4889, loss 0.602525, acc 0.890625, prec 0.0922088, recall 0.900297
2017-12-10T15:57:36.010628: step 4890, loss 0.34903, acc 0.9375, prec 0.0922205, recall 0.900318
2017-12-10T15:57:36.209683: step 4891, loss 0.350599, acc 0.921875, prec 0.0922499, recall 0.90036
2017-12-10T15:57:36.411140: step 4892, loss 0.190472, acc 0.953125, prec 0.0922439, recall 0.90036
2017-12-10T15:57:36.614608: step 4893, loss 0.21745, acc 0.9375, prec 0.0922556, recall 0.900382
2017-12-10T15:57:36.816790: step 4894, loss 0.158922, acc 0.921875, prec 0.092285, recall 0.900424
2017-12-10T15:57:37.016849: step 4895, loss 0.0407508, acc 0.96875, prec 0.0923007, recall 0.900445
2017-12-10T15:57:37.215673: step 4896, loss 0.0798817, acc 0.984375, prec 0.0922987, recall 0.900445
2017-12-10T15:57:37.414402: step 4897, loss 0.165262, acc 0.96875, prec 0.0922947, recall 0.900445
2017-12-10T15:57:37.622643: step 4898, loss 0.0216849, acc 0.984375, prec 0.0922927, recall 0.900445
2017-12-10T15:57:37.818918: step 4899, loss 0.2016, acc 0.953125, prec 0.0923064, recall 0.900466
2017-12-10T15:57:38.020790: step 4900, loss 0.211183, acc 0.953125, prec 0.0923003, recall 0.900466
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/leave_pos_embedding_out_fold_3/1512938428/checkpoints/model-4900

2017-12-10T15:57:39.290997: step 4901, loss 0.182978, acc 0.984375, prec 0.0922983, recall 0.900466
2017-12-10T15:57:39.492957: step 4902, loss 0.0387395, acc 0.984375, prec 0.092316, recall 0.900487
2017-12-10T15:57:39.696645: step 4903, loss 0.214158, acc 0.953125, prec 0.0923297, recall 0.900508
2017-12-10T15:57:39.897498: step 4904, loss 0.431923, acc 0.984375, prec 0.0923671, recall 0.90055
2017-12-10T15:57:40.101654: step 4905, loss 0.00847657, acc 1, prec 0.0923868, recall 0.900571
2017-12-10T15:57:40.302964: step 4906, loss 0.266032, acc 0.953125, prec 0.0924005, recall 0.900592
2017-12-10T15:57:40.510983: step 4907, loss 0.00298338, acc 1, prec 0.0924005, recall 0.900592
2017-12-10T15:57:40.712268: step 4908, loss 0.0594479, acc 0.984375, prec 0.0924182, recall 0.900613
2017-12-10T15:57:40.910184: step 4909, loss 1.62459, acc 1, prec 0.0924576, recall 0.900655
2017-12-10T15:57:41.114119: step 4910, loss 1.62232, acc 0.96875, prec 0.0924556, recall 0.900465
2017-12-10T15:57:41.318624: step 4911, loss 2.89614, acc 0.96875, prec 0.0924536, recall 0.900275
2017-12-10T15:57:41.523527: step 4912, loss 0.397607, acc 0.953125, prec 0.0924672, recall 0.900296
2017-12-10T15:57:41.723145: step 4913, loss 0.122114, acc 0.96875, prec 0.0924632, recall 0.900296
2017-12-10T15:57:41.921788: step 4914, loss 0.268637, acc 0.921875, prec 0.0924532, recall 0.900296
2017-12-10T15:57:42.122896: step 4915, loss 0.20286, acc 0.921875, prec 0.0924825, recall 0.900338
2017-12-10T15:57:42.325220: step 4916, loss 0.058541, acc 0.96875, prec 0.0924982, recall 0.900359
2017-12-10T15:57:42.528416: step 4917, loss 0.20382, acc 0.921875, prec 0.0925275, recall 0.900401
2017-12-10T15:57:42.729581: step 4918, loss 0.230295, acc 0.921875, prec 0.0925569, recall 0.900443
2017-12-10T15:57:42.924401: step 4919, loss 0.649819, acc 0.8125, prec 0.0925328, recall 0.900443
2017-12-10T15:57:43.120323: step 4920, loss 0.12741, acc 0.96875, prec 0.0925288, recall 0.900443
2017-12-10T15:57:43.316015: step 4921, loss 0.642927, acc 0.828125, prec 0.0925067, recall 0.900443
2017-12-10T15:57:43.519649: step 4922, loss 0.54816, acc 0.828125, prec 0.0924847, recall 0.900443
2017-12-10T15:57:43.722282: step 4923, loss 0.671892, acc 0.765625, prec 0.0924743, recall 0.900464
2017-12-10T15:57:43.919228: step 4924, loss 0.554913, acc 0.84375, prec 0.0924739, recall 0.900485
2017-12-10T15:57:44.113631: step 4925, loss 0.498449, acc 0.890625, prec 0.0924795, recall 0.900506
2017-12-10T15:57:44.317704: step 4926, loss 0.534928, acc 0.875, prec 0.0924635, recall 0.900506
2017-12-10T15:57:44.523894: step 4927, loss 0.285336, acc 0.90625, prec 0.0924515, recall 0.900506
2017-12-10T15:57:44.718063: step 4928, loss 0.718625, acc 0.8125, prec 0.0924275, recall 0.900506
2017-12-10T15:57:44.912670: step 4929, loss 0.811053, acc 0.75, prec 0.0924348, recall 0.900548
2017-12-10T15:57:45.112281: step 4930, loss 0.587307, acc 0.796875, prec 0.0924088, recall 0.900548
2017-12-10T15:57:45.314117: step 4931, loss 0.202696, acc 0.890625, prec 0.0924144, recall 0.900569
2017-12-10T15:57:45.512796: step 4932, loss 0.621299, acc 0.890625, prec 0.0924397, recall 0.900611
2017-12-10T15:57:45.711743: step 4933, loss 0.273072, acc 0.96875, prec 0.0924357, recall 0.900611
2017-12-10T15:57:45.913323: step 4934, loss 0.419612, acc 0.9375, prec 0.0924473, recall 0.900632
2017-12-10T15:57:46.114030: step 4935, loss 0.104425, acc 0.953125, prec 0.0924609, recall 0.900652
2017-12-10T15:57:46.311838: step 4936, loss 0.159448, acc 0.9375, prec 0.092453, recall 0.900652
2017-12-10T15:57:46.517969: step 4937, loss 0.397291, acc 0.96875, prec 0.092449, recall 0.900652
2017-12-10T15:57:46.713786: step 4938, loss 0.160798, acc 0.9375, prec 0.092441, recall 0.900652
2017-12-10T15:57:46.913553: step 4939, loss 0.108158, acc 0.953125, prec 0.0924546, recall 0.900673
2017-12-10T15:57:47.112184: step 4940, loss 0.102884, acc 0.96875, prec 0.0924506, recall 0.900673
2017-12-10T15:57:47.311090: step 4941, loss 0.0525663, acc 0.984375, prec 0.0924878, recall 0.900715
2017-12-10T15:57:47.512829: step 4942, loss 0.0589873, acc 0.96875, prec 0.0925034, recall 0.900736
2017-12-10T15:57:47.711421: step 4943, loss 3.07394, acc 0.96875, prec 0.0925014, recall 0.900547
2017-12-10T15:57:47.915948: step 4944, loss 0.351928, acc 0.921875, prec 0.092511, recall 0.900568
2017-12-10T15:57:48.115947: step 4945, loss 0.0185589, acc 1, prec 0.0925502, recall 0.900609
2017-12-10T15:57:48.318058: step 4946, loss 0.0248577, acc 1, prec 0.0925502, recall 0.900609
2017-12-10T15:57:48.518877: step 4947, loss 0.285336, acc 0.9375, prec 0.0925422, recall 0.900609
2017-12-10T15:57:48.727332: step 4948, loss 0.613082, acc 0.921875, prec 0.0925518, recall 0.90063
2017-12-10T15:57:48.930568: step 4949, loss 0.195171, acc 0.9375, prec 0.0925438, recall 0.90063
2017-12-10T15:57:49.134527: step 4950, loss 0.402972, acc 0.9375, prec 0.0925554, recall 0.900651
2017-12-10T15:57:49.337570: step 4951, loss 0.0573868, acc 0.984375, prec 0.0925534, recall 0.900651
2017-12-10T15:57:49.535418: step 4952, loss 0.0746057, acc 0.96875, prec 0.0925886, recall 0.900693
2017-12-10T15:57:49.734009: step 4953, loss 2.87928, acc 0.921875, prec 0.0925806, recall 0.900504
2017-12-10T15:57:49.937296: step 4954, loss 0.170844, acc 0.9375, prec 0.0925922, recall 0.900525
2017-12-10T15:57:50.139549: step 4955, loss 0.149042, acc 0.96875, prec 0.0926665, recall 0.900608
2017-12-10T15:57:50.337371: step 4956, loss 0.17682, acc 0.96875, prec 0.0926821, recall 0.900629
2017-12-10T15:57:50.541940: step 4957, loss 0.147196, acc 0.9375, prec 0.0926741, recall 0.900629
2017-12-10T15:57:50.743378: step 4958, loss 0.28663, acc 0.921875, prec 0.0926837, recall 0.90065
2017-12-10T15:57:50.940890: step 4959, loss 0.550619, acc 0.875, prec 0.0926872, recall 0.900671
2017-12-10T15:57:51.133516: step 4960, loss 0.141324, acc 0.953125, prec 0.0927204, recall 0.900712
2017-12-10T15:57:51.334137: step 4961, loss 0.639154, acc 0.828125, prec 0.0927179, recall 0.900733
2017-12-10T15:57:51.534395: step 4962, loss 0.276353, acc 0.90625, prec 0.0927451, recall 0.900775
2017-12-10T15:57:51.738629: step 4963, loss 0.486426, acc 0.890625, prec 0.0927311, recall 0.900775
2017-12-10T15:57:51.932316: step 4964, loss 0.566345, acc 0.84375, prec 0.0927306, recall 0.900795
2017-12-10T15:57:52.130918: step 4965, loss 0.420226, acc 0.90625, prec 0.0927187, recall 0.900795
2017-12-10T15:57:52.327088: step 4966, loss 0.263077, acc 0.921875, prec 0.0927087, recall 0.900795
2017-12-10T15:57:52.523588: step 4967, loss 0.543546, acc 0.890625, prec 0.0927142, recall 0.900816
2017-12-10T15:57:52.720684: step 4968, loss 0.456713, acc 0.921875, prec 0.0927238, recall 0.900837
2017-12-10T15:57:52.925909: step 4969, loss 0.357747, acc 0.90625, prec 0.0927509, recall 0.900878
2017-12-10T15:57:53.108301: step 4970, loss 0.287266, acc 0.923077, prec 0.0928015, recall 0.90094
Training finished
