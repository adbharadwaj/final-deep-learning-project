Vocabulary Size: 33447
encodedPathwayA = 8 encodedPathwayB = 53
Varying number of filter
Starting Experiment - num_filter_256 



Starting Fold: 0 => Train/Dev split: 31795/10599


 #################### 


SEQUENCE LENGTH 273
BATCH SIZE 64
EMBEDDING SIZE 128
FILTER SIZES [3, 4, 5]
NUMBER OF FILTERS 256
L2 REG LAMBDA 0.0
EPOCHS 20



RESULT DIR num_filter_256_fold_0
VOCAB SIZE 33447
DROPOUT PROBABILITY 0.5



WORD EMBEDDING True
POSITION EMBEDDING True
POS EMBEDDING True


 #################### 


Writing to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150

Start training
2017-12-10T02:32:32.882480: step 1, loss 1.95192, acc 0.5625, prec 0, recall 0
2017-12-10T02:32:33.146467: step 2, loss 0.0237107, acc 0.984375, prec 0, recall 0
2017-12-10T02:32:33.408683: step 3, loss 38.8853, acc 0.984375, prec 0, recall 0
2017-12-10T02:32:33.678857: step 4, loss 53.4303, acc 0.96875, prec 0, recall 0
2017-12-10T02:32:33.951093: step 5, loss 35.0624, acc 0.96875, prec 0, recall 0
2017-12-10T02:32:34.215755: step 6, loss 0.05845, acc 0.984375, prec 0, recall 0
2017-12-10T02:32:34.479556: step 7, loss 5.12406, acc 0.890625, prec 0, recall 0
2017-12-10T02:32:34.743972: step 8, loss 12.3423, acc 0.5625, prec 0, recall 0
2017-12-10T02:32:35.010000: step 9, loss 3.08938, acc 0.328125, prec 0, recall 0
2017-12-10T02:32:35.267199: step 10, loss 5.36783, acc 0.21875, prec 0, recall 0
2017-12-10T02:32:35.525502: step 11, loss 6.06535, acc 0.203125, prec 0.00480769, recall 0.125
2017-12-10T02:32:35.782475: step 12, loss 7.73831, acc 0.046875, prec 0.00371747, recall 0.125
2017-12-10T02:32:36.043482: step 13, loss 8.57843, acc 0.03125, prec 0.00900901, recall 0.3
2017-12-10T02:32:36.297557: step 14, loss 8.43394, acc 0.03125, prec 0.00759494, recall 0.3
2017-12-10T02:32:36.552250: step 15, loss 8.05834, acc 0.125, prec 0.00884956, recall 0.363636
2017-12-10T02:32:36.812638: step 16, loss 6.3344, acc 0.109375, prec 0.0117417, recall 0.461538
2017-12-10T02:32:37.067699: step 17, loss 7.90988, acc 0.296875, prec 0.0125899, recall 0.466667
2017-12-10T02:32:37.322553: step 18, loss 4.42975, acc 0.25, prec 0.0115894, recall 0.466667
2017-12-10T02:32:37.577012: step 19, loss 4.77698, acc 0.296875, prec 0.0107858, recall 0.466667
2017-12-10T02:32:37.832152: step 20, loss 2.24372, acc 0.5, prec 0.0117302, recall 0.5
2017-12-10T02:32:38.093151: step 21, loss 3.90191, acc 0.6875, prec 0.0142248, recall 0.526316
2017-12-10T02:32:38.358479: step 22, loss 1.9644, acc 0.640625, prec 0.0151307, recall 0.55
2017-12-10T02:32:38.628845: step 23, loss 0.718598, acc 0.78125, prec 0.0148448, recall 0.55
2017-12-10T02:32:38.889877: step 24, loss 1.03361, acc 0.71875, prec 0.0144928, recall 0.55
2017-12-10T02:32:39.150132: step 25, loss 0.779457, acc 0.828125, prec 0.0142857, recall 0.55
2017-12-10T02:32:39.413555: step 26, loss 0.636319, acc 0.796875, prec 0.0140485, recall 0.55
2017-12-10T02:32:39.678094: step 27, loss 9.75653, acc 0.953125, prec 0.0140127, recall 0.52381
2017-12-10T02:32:39.947604: step 28, loss 1.76642, acc 0.953125, prec 0.0139771, recall 0.5
2017-12-10T02:32:40.222246: step 29, loss 3.37825, acc 0.859375, prec 0.0138365, recall 0.478261
2017-12-10T02:32:40.493195: step 30, loss 0.558316, acc 0.84375, prec 0.0136646, recall 0.478261
2017-12-10T02:32:40.755882: step 31, loss 0.384157, acc 0.90625, prec 0.0135635, recall 0.478261
2017-12-10T02:32:41.022563: step 32, loss 0.48044, acc 0.78125, prec 0.0133333, recall 0.478261
2017-12-10T02:32:41.286117: step 33, loss 0.608409, acc 0.890625, prec 0.0132212, recall 0.478261
2017-12-10T02:32:41.555326: step 34, loss 6.4386, acc 0.78125, prec 0.0130178, recall 0.458333
2017-12-10T02:32:41.823952: step 35, loss 0.744551, acc 0.796875, prec 0.0128205, recall 0.458333
2017-12-10T02:32:42.079471: step 36, loss 1.67559, acc 0.671875, prec 0.0125142, recall 0.458333
2017-12-10T02:32:42.335476: step 37, loss 15.7375, acc 0.703125, prec 0.0122631, recall 0.44
2017-12-10T02:32:42.602833: step 38, loss 5.75993, acc 0.546875, prec 0.0118919, recall 0.423077
2017-12-10T02:32:42.866380: step 39, loss 14.1229, acc 0.484375, prec 0.0125392, recall 0.413793
2017-12-10T02:32:43.126964: step 40, loss 10.6218, acc 0.53125, prec 0.0131712, recall 0.419355
2017-12-10T02:32:43.386819: step 41, loss 4.5079, acc 0.375, prec 0.0126582, recall 0.419355
2017-12-10T02:32:43.642289: step 42, loss 5.38776, acc 0.1875, prec 0.013876, recall 0.454545
2017-12-10T02:32:43.904470: step 43, loss 6.81805, acc 0.203125, prec 0.0141218, recall 0.470588
2017-12-10T02:32:44.160134: step 44, loss 8.86449, acc 0.140625, prec 0.0142977, recall 0.485714
2017-12-10T02:32:44.418348: step 45, loss 6.77278, acc 0.140625, prec 0.0144578, recall 0.5
2017-12-10T02:32:44.675555: step 46, loss 8.22526, acc 0.109375, prec 0.0138249, recall 0.5
2017-12-10T02:32:44.943581: step 47, loss 7.66884, acc 0.171875, prec 0.0140118, recall 0.513514
2017-12-10T02:32:45.204048: step 48, loss 6.39472, acc 0.203125, prec 0.0142045, recall 0.526316
2017-12-10T02:32:45.458868: step 49, loss 5.34336, acc 0.28125, prec 0.0137552, recall 0.526316
2017-12-10T02:32:45.719091: step 50, loss 4.69901, acc 0.359375, prec 0.0146961, recall 0.55
2017-12-10T02:32:45.975417: step 51, loss 3.17054, acc 0.5, prec 0.0143885, recall 0.55
2017-12-10T02:32:46.241194: step 52, loss 2.18458, acc 0.5625, prec 0.0141297, recall 0.55
2017-12-10T02:32:46.502013: step 53, loss 3.11734, acc 0.546875, prec 0.0144928, recall 0.560976
2017-12-10T02:32:46.763538: step 54, loss 1.67532, acc 0.734375, prec 0.0149533, recall 0.571429
2017-12-10T02:32:47.024503: step 55, loss 0.798043, acc 0.765625, prec 0.0148148, recall 0.571429
2017-12-10T02:32:47.288470: step 56, loss 1.12914, acc 0.765625, prec 0.0146789, recall 0.571429
2017-12-10T02:32:47.548295: step 57, loss 3.42891, acc 0.8125, prec 0.0145808, recall 0.55814
2017-12-10T02:32:47.817831: step 58, loss 0.215179, acc 0.9375, prec 0.0145455, recall 0.55814
2017-12-10T02:32:48.084010: step 59, loss 30.6006, acc 0.78125, prec 0.0144491, recall 0.521739
2017-12-10T02:32:48.355340: step 60, loss 0.248564, acc 0.875, prec 0.0143799, recall 0.521739
2017-12-10T02:32:48.619147: step 61, loss 19.6573, acc 0.875, prec 0.0149165, recall 0.510204
2017-12-10T02:32:48.889904: step 62, loss 15.889, acc 0.65625, prec 0.0153212, recall 0.5
2017-12-10T02:32:49.152191: step 63, loss 2.83567, acc 0.640625, prec 0.0162602, recall 0.518519
2017-12-10T02:32:49.412555: step 64, loss 3.37583, acc 0.421875, prec 0.0164773, recall 0.527273
2017-12-10T02:32:49.675330: step 65, loss 4.55528, acc 0.421875, prec 0.016138, recall 0.527273
2017-12-10T02:32:49.936969: step 66, loss 4.58844, acc 0.296875, prec 0.0162778, recall 0.535714
2017-12-10T02:32:50.192135: step 67, loss 6.22763, acc 0.1875, prec 0.0163502, recall 0.54386
2017-12-10T02:32:50.450040: step 68, loss 5.58193, acc 0.25, prec 0.0164524, recall 0.551724
2017-12-10T02:32:50.722652: step 69, loss 5.73929, acc 0.25, prec 0.0160562, recall 0.551724
2017-12-10T02:32:50.982259: step 70, loss 9.51628, acc 0.140625, prec 0.0161133, recall 0.55
2017-12-10T02:32:51.248630: step 71, loss 6.2344, acc 0.125, prec 0.0156844, recall 0.55
2017-12-10T02:32:51.505228: step 72, loss 6.25864, acc 0.234375, prec 0.0153274, recall 0.55
2017-12-10T02:32:51.772362: step 73, loss 5.3188, acc 0.171875, prec 0.0149592, recall 0.55
2017-12-10T02:32:52.030477: step 74, loss 4.66338, acc 0.359375, prec 0.0155625, recall 0.564516
2017-12-10T02:32:52.290874: step 75, loss 3.53849, acc 0.34375, prec 0.0152772, recall 0.564516
2017-12-10T02:32:52.553836: step 76, loss 3.98312, acc 0.40625, prec 0.0154506, recall 0.571429
2017-12-10T02:32:52.814662: step 77, loss 3.01496, acc 0.515625, prec 0.0152478, recall 0.571429
2017-12-10T02:32:53.077804: step 78, loss 1.74674, acc 0.671875, prec 0.0155266, recall 0.578125
2017-12-10T02:32:53.337155: step 79, loss 20.2031, acc 0.796875, prec 0.0158598, recall 0.575758
2017-12-10T02:32:53.608529: step 80, loss 1.38545, acc 0.734375, prec 0.0161558, recall 0.58209
2017-12-10T02:32:53.876675: step 81, loss 4.52375, acc 0.75, prec 0.016056, recall 0.573529
2017-12-10T02:32:54.136654: step 82, loss 1.2236, acc 0.765625, prec 0.0163599, recall 0.57971
2017-12-10T02:32:54.402649: step 83, loss 3.5189, acc 0.671875, prec 0.0166261, recall 0.577465
2017-12-10T02:32:54.664856: step 84, loss 1.70836, acc 0.703125, prec 0.0168946, recall 0.583333
2017-12-10T02:32:54.930538: step 85, loss 18.6395, acc 0.671875, prec 0.0167598, recall 0.575342
2017-12-10T02:32:55.192216: step 86, loss 1.26978, acc 0.734375, prec 0.0170364, recall 0.581081
2017-12-10T02:32:55.458438: step 87, loss 2.90013, acc 0.5, prec 0.0168232, recall 0.581081
2017-12-10T02:32:55.717717: step 88, loss 2.02676, acc 0.578125, prec 0.0170279, recall 0.586667
2017-12-10T02:32:55.982188: step 89, loss 1.52058, acc 0.671875, prec 0.0176448, recall 0.597403
2017-12-10T02:32:56.241331: step 90, loss 3.1359, acc 0.59375, prec 0.0174772, recall 0.589744
2017-12-10T02:32:56.513988: step 91, loss 1.65016, acc 0.59375, prec 0.0173062, recall 0.589744
2017-12-10T02:32:56.776793: step 92, loss 1.68742, acc 0.703125, prec 0.0179171, recall 0.6
2017-12-10T02:32:57.040278: step 93, loss 2.6691, acc 0.65625, prec 0.0181347, recall 0.604938
2017-12-10T02:32:57.300873: step 94, loss 1.68249, acc 0.59375, prec 0.0179619, recall 0.604938
2017-12-10T02:32:57.568238: step 95, loss 1.97566, acc 0.5625, prec 0.0177794, recall 0.604938
2017-12-10T02:32:57.829102: step 96, loss 1.6785, acc 0.625, prec 0.0176259, recall 0.604938
2017-12-10T02:32:58.086936: step 97, loss 1.11718, acc 0.75, prec 0.017525, recall 0.604938
2017-12-10T02:32:58.348576: step 98, loss 1.29176, acc 0.703125, prec 0.0174068, recall 0.604938
2017-12-10T02:32:58.605619: step 99, loss 9.80214, acc 0.703125, prec 0.0172962, recall 0.597561
2017-12-10T02:32:58.865731: step 100, loss 4.41494, acc 0.796875, prec 0.0175685, recall 0.595238
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-100

2017-12-10T02:33:00.104487: step 101, loss 1.05567, acc 0.6875, prec 0.0174459, recall 0.595238
2017-12-10T02:33:00.373532: step 102, loss 9.52972, acc 0.734375, prec 0.0173491, recall 0.588235
2017-12-10T02:33:00.643214: step 103, loss 1.34586, acc 0.71875, prec 0.0172414, recall 0.588235
2017-12-10T02:33:00.909803: step 104, loss 2.42666, acc 0.5625, prec 0.0170765, recall 0.588235
2017-12-10T02:33:01.165616: step 105, loss 1.14853, acc 0.71875, prec 0.0173057, recall 0.593023
2017-12-10T02:33:01.421950: step 106, loss 1.54241, acc 0.65625, prec 0.0171775, recall 0.593023
2017-12-10T02:33:01.682048: step 107, loss 1.87964, acc 0.609375, prec 0.0170341, recall 0.593023
2017-12-10T02:33:01.939998: step 108, loss 1.99133, acc 0.640625, prec 0.0169042, recall 0.593023
2017-12-10T02:33:02.198377: step 109, loss 3.47581, acc 0.578125, prec 0.0167598, recall 0.586207
2017-12-10T02:33:02.463360: step 110, loss 2.71117, acc 0.515625, prec 0.0165908, recall 0.586207
2017-12-10T02:33:02.724978: step 111, loss 1.89565, acc 0.609375, prec 0.0164569, recall 0.586207
2017-12-10T02:33:02.983923: step 112, loss 1.74569, acc 0.703125, prec 0.016672, recall 0.590909
2017-12-10T02:33:03.245711: step 113, loss 11.269, acc 0.671875, prec 0.0171975, recall 0.586957
2017-12-10T02:33:03.508176: step 114, loss 1.72204, acc 0.625, prec 0.017067, recall 0.586957
2017-12-10T02:33:03.768170: step 115, loss 2.04699, acc 0.546875, prec 0.0172198, recall 0.591398
2017-12-10T02:33:04.032592: step 116, loss 3.63995, acc 0.671875, prec 0.0171126, recall 0.585106
2017-12-10T02:33:04.301132: step 117, loss 2.06041, acc 0.703125, prec 0.0170121, recall 0.585106
2017-12-10T02:33:04.563415: step 118, loss 2.74392, acc 0.421875, prec 0.0168196, recall 0.585106
2017-12-10T02:33:04.818876: step 119, loss 2.14344, acc 0.609375, prec 0.016692, recall 0.585106
2017-12-10T02:33:05.084876: step 120, loss 2.08218, acc 0.65625, prec 0.0165812, recall 0.585106
2017-12-10T02:33:05.344915: step 121, loss 2.27779, acc 0.640625, prec 0.0167614, recall 0.589474
2017-12-10T02:33:05.606186: step 122, loss 10.6517, acc 0.6875, prec 0.0169592, recall 0.587629
2017-12-10T02:33:05.873313: step 123, loss 1.55069, acc 0.75, prec 0.0174608, recall 0.59596
2017-12-10T02:33:06.141204: step 124, loss 2.20619, acc 0.671875, prec 0.0176419, recall 0.6
2017-12-10T02:33:06.404488: step 125, loss 1.5, acc 0.671875, prec 0.0175336, recall 0.6
2017-12-10T02:33:06.671772: step 126, loss 0.803949, acc 0.8125, prec 0.0174723, recall 0.6
2017-12-10T02:33:06.936488: step 127, loss 2.32447, acc 0.59375, prec 0.017625, recall 0.60396
2017-12-10T02:33:07.200256: step 128, loss 2.00138, acc 0.65625, prec 0.0177956, recall 0.607843
2017-12-10T02:33:07.458680: step 129, loss 1.79774, acc 0.640625, prec 0.0176789, recall 0.607843
2017-12-10T02:33:07.720625: step 130, loss 1.23836, acc 0.71875, prec 0.018424, recall 0.619048
2017-12-10T02:33:07.981008: step 131, loss 1.33934, acc 0.671875, prec 0.018315, recall 0.619048
2017-12-10T02:33:08.240731: step 132, loss 0.65275, acc 0.8125, prec 0.0185289, recall 0.622642
2017-12-10T02:33:08.498461: step 133, loss 1.13725, acc 0.828125, prec 0.0184719, recall 0.622642
2017-12-10T02:33:08.761672: step 134, loss 0.998863, acc 0.71875, prec 0.0183793, recall 0.622642
2017-12-10T02:33:09.027945: step 135, loss 0.560667, acc 0.828125, prec 0.0185956, recall 0.626168
2017-12-10T02:33:09.288820: step 136, loss 0.500526, acc 0.890625, prec 0.0188313, recall 0.62963
2017-12-10T02:33:09.548708: step 137, loss 2.88567, acc 0.890625, prec 0.0190713, recall 0.627273
2017-12-10T02:33:09.815964: step 138, loss 0.390398, acc 0.921875, prec 0.019045, recall 0.627273
2017-12-10T02:33:10.079189: step 139, loss 13.6054, acc 0.84375, prec 0.0192678, recall 0.625
2017-12-10T02:33:10.351000: step 140, loss 0.412972, acc 0.890625, prec 0.0192308, recall 0.625
2017-12-10T02:33:10.615773: step 141, loss 0.616204, acc 0.84375, prec 0.0191781, recall 0.625
2017-12-10T02:33:10.876689: step 142, loss 0.739001, acc 0.765625, prec 0.0190996, recall 0.625
2017-12-10T02:33:11.136758: step 143, loss 2.25545, acc 0.828125, prec 0.0190476, recall 0.619469
2017-12-10T02:33:11.406916: step 144, loss 0.869329, acc 0.75, prec 0.0189651, recall 0.619469
2017-12-10T02:33:11.668211: step 145, loss 32.1683, acc 0.78125, prec 0.0191633, recall 0.617391
2017-12-10T02:33:11.936127: step 146, loss 0.993669, acc 0.734375, prec 0.0196026, recall 0.623932
2017-12-10T02:33:12.197946: step 147, loss 16.1067, acc 0.640625, prec 0.0194875, recall 0.618644
2017-12-10T02:33:12.462866: step 148, loss 1.64935, acc 0.65625, prec 0.0193737, recall 0.618644
2017-12-10T02:33:12.722306: step 149, loss 2.81165, acc 0.578125, prec 0.0197524, recall 0.625
2017-12-10T02:33:12.984090: step 150, loss 6.70612, acc 0.390625, prec 0.0198123, recall 0.622951
2017-12-10T02:33:13.249523: step 151, loss 3.28119, acc 0.40625, prec 0.019618, recall 0.622951
2017-12-10T02:33:13.514783: step 152, loss 4.41235, acc 0.265625, prec 0.0198827, recall 0.629032
2017-12-10T02:33:13.772882: step 153, loss 4.38713, acc 0.234375, prec 0.0196375, recall 0.629032
2017-12-10T02:33:14.030280: step 154, loss 5.79239, acc 0.28125, prec 0.0194175, recall 0.624
2017-12-10T02:33:14.286714: step 155, loss 4.83123, acc 0.296875, prec 0.0194438, recall 0.626984
2017-12-10T02:33:14.543893: step 156, loss 6.4905, acc 0.3125, prec 0.0192401, recall 0.622047
2017-12-10T02:33:14.801266: step 157, loss 4.49273, acc 0.3125, prec 0.0190361, recall 0.622047
2017-12-10T02:33:15.056693: step 158, loss 4.73369, acc 0.25, prec 0.0188185, recall 0.622047
2017-12-10T02:33:15.319081: step 159, loss 9.25616, acc 0.359375, prec 0.0188724, recall 0.620155
2017-12-10T02:33:15.577397: step 160, loss 5.20999, acc 0.296875, prec 0.0186741, recall 0.620155
2017-12-10T02:33:15.841734: step 161, loss 4.15883, acc 0.359375, prec 0.0184971, recall 0.620155
2017-12-10T02:33:16.104003: step 162, loss 3.44641, acc 0.484375, prec 0.0185822, recall 0.623077
2017-12-10T02:33:16.368521: step 163, loss 3.88461, acc 0.5, prec 0.0186703, recall 0.625954
2017-12-10T02:33:16.631079: step 164, loss 3.87435, acc 0.390625, prec 0.018506, recall 0.625954
2017-12-10T02:33:16.891589: step 165, loss 2.56664, acc 0.484375, prec 0.0183692, recall 0.625954
2017-12-10T02:33:17.157690: step 166, loss 1.88306, acc 0.59375, prec 0.0182628, recall 0.625954
2017-12-10T02:33:17.420133: step 167, loss 2.18732, acc 0.5625, prec 0.0181496, recall 0.625954
2017-12-10T02:33:17.687138: step 168, loss 1.35546, acc 0.65625, prec 0.0180617, recall 0.625954
2017-12-10T02:33:17.955318: step 169, loss 1.10053, acc 0.671875, prec 0.0184089, recall 0.631579
2017-12-10T02:33:18.218478: step 170, loss 1.42413, acc 0.796875, prec 0.0183566, recall 0.631579
2017-12-10T02:33:18.482105: step 171, loss 18.5645, acc 0.796875, prec 0.0183086, recall 0.626866
2017-12-10T02:33:18.751604: step 172, loss 4.82834, acc 0.890625, prec 0.0184984, recall 0.625
2017-12-10T02:33:19.018127: step 173, loss 12.7063, acc 0.75, prec 0.0184422, recall 0.615942
2017-12-10T02:33:19.283154: step 174, loss 1.18749, acc 0.71875, prec 0.0183704, recall 0.615942
2017-12-10T02:33:19.543824: step 175, loss 0.883418, acc 0.78125, prec 0.0185265, recall 0.618705
2017-12-10T02:33:19.812596: step 176, loss 1.43063, acc 0.6875, prec 0.0186575, recall 0.621429
2017-12-10T02:33:20.069485: step 177, loss 1.48042, acc 0.671875, prec 0.0185739, recall 0.621429
2017-12-10T02:33:20.328047: step 178, loss 2.58502, acc 0.5625, prec 0.0186718, recall 0.624114
2017-12-10T02:33:20.586046: step 179, loss 2.14445, acc 0.5625, prec 0.0185615, recall 0.624114
2017-12-10T02:33:20.846857: step 180, loss 1.9958, acc 0.5625, prec 0.0184525, recall 0.624114
2017-12-10T02:33:21.111613: step 181, loss 19.6392, acc 0.546875, prec 0.0187539, recall 0.625
2017-12-10T02:33:21.373848: step 182, loss 2.20971, acc 0.546875, prec 0.0188445, recall 0.627586
2017-12-10T02:33:21.632545: step 183, loss 2.70913, acc 0.625, prec 0.0189534, recall 0.630137
2017-12-10T02:33:21.890790: step 184, loss 6.2074, acc 0.5, prec 0.0188332, recall 0.62585
2017-12-10T02:33:22.164278: step 185, loss 2.14011, acc 0.546875, prec 0.0193207, recall 0.633333
2017-12-10T02:33:22.426981: step 186, loss 3.23516, acc 0.5, prec 0.0191958, recall 0.633333
2017-12-10T02:33:22.695521: step 187, loss 7.60367, acc 0.421875, prec 0.0190572, recall 0.629139
2017-12-10T02:33:22.956866: step 188, loss 2.45059, acc 0.484375, prec 0.019518, recall 0.636364
2017-12-10T02:33:23.214946: step 189, loss 6.41921, acc 0.5625, prec 0.0196078, recall 0.634615
2017-12-10T02:33:23.480176: step 190, loss 3.44049, acc 0.546875, prec 0.0198819, recall 0.639241
2017-12-10T02:33:23.742568: step 191, loss 3.3601, acc 0.375, prec 0.019918, recall 0.641509
2017-12-10T02:33:24.001971: step 192, loss 3.93375, acc 0.34375, prec 0.019756, recall 0.641509
2017-12-10T02:33:24.268980: step 193, loss 2.64182, acc 0.515625, prec 0.0198268, recall 0.64375
2017-12-10T02:33:24.528688: step 194, loss 3.54581, acc 0.453125, prec 0.0196941, recall 0.64375
2017-12-10T02:33:24.788534: step 195, loss 14.6084, acc 0.46875, prec 0.0197568, recall 0.641975
2017-12-10T02:33:25.052326: step 196, loss 9.27773, acc 0.5, prec 0.0196412, recall 0.638037
2017-12-10T02:33:25.312360: step 197, loss 11.3085, acc 0.453125, prec 0.0195159, recall 0.634146
2017-12-10T02:33:25.577372: step 198, loss 3.86134, acc 0.328125, prec 0.0195422, recall 0.636364
2017-12-10T02:33:25.838435: step 199, loss 3.12522, acc 0.421875, prec 0.0194085, recall 0.636364
2017-12-10T02:33:26.108378: step 200, loss 4.19732, acc 0.375, prec 0.0198056, recall 0.642857
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-200

2017-12-10T02:33:27.588402: step 201, loss 3.79505, acc 0.5, prec 0.0196901, recall 0.642857
2017-12-10T02:33:27.853819: step 202, loss 3.15596, acc 0.5, prec 0.0197535, recall 0.64497
2017-12-10T02:33:28.109815: step 203, loss 5.09665, acc 0.5, prec 0.0198198, recall 0.643275
2017-12-10T02:33:28.368314: step 204, loss 3.02539, acc 0.4375, prec 0.0196921, recall 0.643275
2017-12-10T02:33:28.632320: step 205, loss 3.9092, acc 0.40625, prec 0.0199076, recall 0.647399
2017-12-10T02:33:28.891584: step 206, loss 2.82434, acc 0.5625, prec 0.0199823, recall 0.649425
2017-12-10T02:33:29.154402: step 207, loss 2.13898, acc 0.53125, prec 0.0200492, recall 0.651429
2017-12-10T02:33:29.414543: step 208, loss 1.89569, acc 0.546875, prec 0.0199475, recall 0.651429
2017-12-10T02:33:29.672057: step 209, loss 1.11555, acc 0.75, prec 0.0198918, recall 0.651429
2017-12-10T02:33:29.943660: step 210, loss 7.26411, acc 0.71875, prec 0.019833, recall 0.647727
2017-12-10T02:33:30.220053: step 211, loss 6.76233, acc 0.75, prec 0.0197848, recall 0.640449
2017-12-10T02:33:30.483984: step 212, loss 2.41236, acc 0.671875, prec 0.0200519, recall 0.644444
2017-12-10T02:33:30.755573: step 213, loss 1.42607, acc 0.734375, prec 0.0199931, recall 0.644444
2017-12-10T02:33:31.021446: step 214, loss 1.53715, acc 0.71875, prec 0.0200996, recall 0.646409
2017-12-10T02:33:31.290133: step 215, loss 1.88441, acc 0.640625, prec 0.0200205, recall 0.646409
2017-12-10T02:33:31.552198: step 216, loss 1.34658, acc 0.75, prec 0.0199659, recall 0.646409
2017-12-10T02:33:31.814533: step 217, loss 1.68274, acc 0.640625, prec 0.0198878, recall 0.646409
2017-12-10T02:33:32.072498: step 218, loss 1.47183, acc 0.734375, prec 0.0201627, recall 0.650273
2017-12-10T02:33:32.334483: step 219, loss 2.18414, acc 0.65625, prec 0.0200878, recall 0.650273
2017-12-10T02:33:32.606028: step 220, loss 1.75992, acc 0.671875, prec 0.0203464, recall 0.654054
2017-12-10T02:33:32.867799: step 221, loss 26.8035, acc 0.6875, prec 0.0204492, recall 0.648936
2017-12-10T02:33:33.137363: step 222, loss 2.81657, acc 0.5625, prec 0.0203537, recall 0.648936
2017-12-10T02:33:33.404321: step 223, loss 2.5598, acc 0.5625, prec 0.0204217, recall 0.650794
2017-12-10T02:33:33.672236: step 224, loss 9.12867, acc 0.546875, prec 0.0203272, recall 0.647368
2017-12-10T02:33:33.938022: step 225, loss 1.74436, acc 0.6875, prec 0.0204216, recall 0.649215
2017-12-10T02:33:34.202127: step 226, loss 2.31712, acc 0.515625, prec 0.0204784, recall 0.651042
2017-12-10T02:33:34.471857: step 227, loss 3.06751, acc 0.5625, prec 0.0203849, recall 0.651042
2017-12-10T02:33:34.736106: step 228, loss 2.95103, acc 0.484375, prec 0.0204346, recall 0.65285
2017-12-10T02:33:34.994891: step 229, loss 2.09896, acc 0.65625, prec 0.0206785, recall 0.65641
2017-12-10T02:33:35.260789: step 230, loss 2.77934, acc 0.609375, prec 0.0205953, recall 0.65641
2017-12-10T02:33:35.523718: step 231, loss 3.42766, acc 0.546875, prec 0.0206565, recall 0.658163
2017-12-10T02:33:35.788174: step 232, loss 2.18726, acc 0.59375, prec 0.020727, recall 0.659898
2017-12-10T02:33:36.055909: step 233, loss 1.74831, acc 0.65625, prec 0.0206546, recall 0.659898
2017-12-10T02:33:36.315310: step 234, loss 1.87056, acc 0.640625, prec 0.0205794, recall 0.659898
2017-12-10T02:33:36.577022: step 235, loss 2.2366, acc 0.625, prec 0.0206559, recall 0.661616
2017-12-10T02:33:36.839992: step 236, loss 8.07089, acc 0.703125, prec 0.0206007, recall 0.655
2017-12-10T02:33:37.101155: step 237, loss 22.4029, acc 0.734375, prec 0.0207026, recall 0.653465
2017-12-10T02:33:37.368277: step 238, loss 1.2028, acc 0.78125, prec 0.0209637, recall 0.656863
2017-12-10T02:33:37.633884: step 239, loss 1.30248, acc 0.703125, prec 0.0209016, recall 0.656863
2017-12-10T02:33:37.894908: step 240, loss 1.25809, acc 0.640625, prec 0.0208269, recall 0.656863
2017-12-10T02:33:38.159325: step 241, loss 1.51018, acc 0.71875, prec 0.0209205, recall 0.658537
2017-12-10T02:33:38.423064: step 242, loss 2.49294, acc 0.5625, prec 0.0211322, recall 0.661836
2017-12-10T02:33:38.681621: step 243, loss 1.91418, acc 0.6875, prec 0.0210672, recall 0.661836
2017-12-10T02:33:38.944642: step 244, loss 1.64656, acc 0.65625, prec 0.0209962, recall 0.661836
2017-12-10T02:33:39.210227: step 245, loss 1.2952, acc 0.765625, prec 0.020948, recall 0.661836
2017-12-10T02:33:39.474891: step 246, loss 1.91595, acc 0.671875, prec 0.020881, recall 0.661836
2017-12-10T02:33:39.741863: step 247, loss 0.765551, acc 0.828125, prec 0.020995, recall 0.663462
2017-12-10T02:33:40.012704: step 248, loss 0.79681, acc 0.78125, prec 0.021099, recall 0.665072
2017-12-10T02:33:40.281279: step 249, loss 0.946684, acc 0.828125, prec 0.0210638, recall 0.665072
2017-12-10T02:33:40.540387: step 250, loss 17.4536, acc 0.828125, prec 0.02118, recall 0.663507
2017-12-10T02:33:40.803577: step 251, loss 1.12879, acc 0.8125, prec 0.0214372, recall 0.666667
2017-12-10T02:33:41.068872: step 252, loss 2.03986, acc 0.796875, prec 0.0213984, recall 0.663551
2017-12-10T02:33:41.353683: step 253, loss 2.01251, acc 0.828125, prec 0.0213662, recall 0.660465
2017-12-10T02:33:41.621627: step 254, loss 4.32563, acc 0.734375, prec 0.0213149, recall 0.657407
2017-12-10T02:33:41.885417: step 255, loss 1.56519, acc 0.6875, prec 0.0213976, recall 0.658986
2017-12-10T02:33:42.147084: step 256, loss 1.46539, acc 0.71875, prec 0.0213401, recall 0.658986
2017-12-10T02:33:42.408141: step 257, loss 1.52293, acc 0.734375, prec 0.0214318, recall 0.66055
2017-12-10T02:33:42.669026: step 258, loss 1.93683, acc 0.578125, prec 0.021346, recall 0.66055
2017-12-10T02:33:42.936354: step 259, loss 1.70961, acc 0.75, prec 0.0215849, recall 0.663636
2017-12-10T02:33:43.195898: step 260, loss 1.09359, acc 0.703125, prec 0.0215244, recall 0.663636
2017-12-10T02:33:43.456070: step 261, loss 8.2382, acc 0.625, prec 0.0217391, recall 0.663677
2017-12-10T02:33:43.730585: step 262, loss 1.47581, acc 0.71875, prec 0.0216818, recall 0.663677
2017-12-10T02:33:43.992996: step 263, loss 2.53712, acc 0.5625, prec 0.0218786, recall 0.666667
2017-12-10T02:33:44.258271: step 264, loss 2.72144, acc 0.515625, prec 0.0217802, recall 0.666667
2017-12-10T02:33:44.526547: step 265, loss 2.86816, acc 0.515625, prec 0.0216826, recall 0.666667
2017-12-10T02:33:44.816605: step 266, loss 1.82198, acc 0.546875, prec 0.0217329, recall 0.668142
2017-12-10T02:33:45.085927: step 267, loss 2.82493, acc 0.546875, prec 0.0217827, recall 0.669604
2017-12-10T02:33:45.348065: step 268, loss 1.77314, acc 0.59375, prec 0.0217019, recall 0.669604
2017-12-10T02:33:45.616729: step 269, loss 8.74418, acc 0.59375, prec 0.0217639, recall 0.668122
2017-12-10T02:33:45.892861: step 270, loss 2.15941, acc 0.625, prec 0.0216898, recall 0.668122
2017-12-10T02:33:46.154798: step 271, loss 1.42146, acc 0.703125, prec 0.0219081, recall 0.670996
2017-12-10T02:33:46.423826: step 272, loss 1.12862, acc 0.6875, prec 0.0218464, recall 0.670996
2017-12-10T02:33:46.687960: step 273, loss 1.79753, acc 0.65625, prec 0.0219163, recall 0.672414
2017-12-10T02:33:46.958937: step 274, loss 2.84112, acc 0.765625, prec 0.0218732, recall 0.669528
2017-12-10T02:33:47.221067: step 275, loss 0.830345, acc 0.765625, prec 0.0218273, recall 0.669528
2017-12-10T02:33:47.492579: step 276, loss 1.10068, acc 0.703125, prec 0.0217695, recall 0.669528
2017-12-10T02:33:47.755143: step 277, loss 0.943081, acc 0.75, prec 0.0218572, recall 0.67094
2017-12-10T02:33:48.024122: step 278, loss 0.671553, acc 0.890625, prec 0.0218359, recall 0.67094
2017-12-10T02:33:48.296916: step 279, loss 0.63858, acc 0.859375, prec 0.0218086, recall 0.67094
2017-12-10T02:33:48.563918: step 280, loss 0.417318, acc 0.890625, prec 0.0217874, recall 0.67094
2017-12-10T02:33:48.830108: step 281, loss 0.772494, acc 0.75, prec 0.0217391, recall 0.67094
2017-12-10T02:33:49.092189: step 282, loss 0.584418, acc 0.796875, prec 0.0217001, recall 0.67094
2017-12-10T02:33:49.357505: step 283, loss 1.73128, acc 0.875, prec 0.0218142, recall 0.669492
2017-12-10T02:33:49.628423: step 284, loss 0.350913, acc 0.859375, prec 0.0217871, recall 0.669492
2017-12-10T02:33:49.889846: step 285, loss 0.348607, acc 0.90625, prec 0.0217691, recall 0.669492
2017-12-10T02:33:50.153099: step 286, loss 14.7343, acc 0.765625, prec 0.0217302, recall 0.663866
2017-12-10T02:33:50.426374: step 287, loss 0.972948, acc 0.765625, prec 0.0216854, recall 0.663866
2017-12-10T02:33:50.698352: step 288, loss 0.790599, acc 0.84375, prec 0.0219238, recall 0.666667
2017-12-10T02:33:50.969946: step 289, loss 6.29433, acc 0.6875, prec 0.0218669, recall 0.6639
2017-12-10T02:33:51.233256: step 290, loss 3.23768, acc 0.78125, prec 0.0218281, recall 0.661157
2017-12-10T02:33:51.495052: step 291, loss 1.19335, acc 0.703125, prec 0.0217717, recall 0.661157
2017-12-10T02:33:51.764184: step 292, loss 2.96058, acc 0.484375, prec 0.0218069, recall 0.662551
2017-12-10T02:33:52.034423: step 293, loss 3.47849, acc 0.390625, prec 0.0219558, recall 0.665306
2017-12-10T02:33:52.299539: step 294, loss 3.82041, acc 0.4375, prec 0.0218499, recall 0.665306
2017-12-10T02:33:52.559974: step 295, loss 3.29374, acc 0.359375, prec 0.0218608, recall 0.666667
2017-12-10T02:33:52.823835: step 296, loss 3.12676, acc 0.390625, prec 0.0217478, recall 0.666667
2017-12-10T02:33:53.086529: step 297, loss 4.904, acc 0.296875, prec 0.0218766, recall 0.669355
2017-12-10T02:33:53.350604: step 298, loss 2.85633, acc 0.46875, prec 0.0219074, recall 0.670683
2017-12-10T02:33:53.611572: step 299, loss 3.28424, acc 0.453125, prec 0.0221903, recall 0.674603
2017-12-10T02:33:53.880543: step 300, loss 2.67478, acc 0.515625, prec 0.022228, recall 0.675889

Evaluation:
2017-12-10T02:34:01.483641: step 300, loss 1.74948, acc 0.592697, prec 0.0262724, recall 0.764988

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-300

2017-12-10T02:34:02.778757: step 301, loss 1.96565, acc 0.578125, prec 0.0262942, recall 0.76555
2017-12-10T02:34:03.046975: step 302, loss 1.26098, acc 0.6875, prec 0.0263309, recall 0.76611
2017-12-10T02:34:03.307866: step 303, loss 1.4102, acc 0.703125, prec 0.0264494, recall 0.767221
2017-12-10T02:34:03.572594: step 304, loss 1.33177, acc 0.703125, prec 0.0264083, recall 0.767221
2017-12-10T02:34:03.831652: step 305, loss 0.591966, acc 0.8125, prec 0.0263824, recall 0.767221
2017-12-10T02:34:04.093768: step 306, loss 7.28302, acc 0.84375, prec 0.026363, recall 0.765403
2017-12-10T02:34:04.357441: step 307, loss 0.82924, acc 0.859375, prec 0.0265025, recall 0.766509
2017-12-10T02:34:04.625806: step 308, loss 0.160394, acc 0.96875, prec 0.0265775, recall 0.767059
2017-12-10T02:34:04.895870: step 309, loss 0.201131, acc 0.921875, prec 0.0265667, recall 0.767059
2017-12-10T02:34:05.156507: step 310, loss 0.369038, acc 0.890625, prec 0.0265516, recall 0.767059
2017-12-10T02:34:05.416147: step 311, loss 14.9276, acc 0.953125, prec 0.0265472, recall 0.765258
2017-12-10T02:34:05.683530: step 312, loss 17.7723, acc 0.859375, prec 0.0265321, recall 0.761682
2017-12-10T02:34:05.954787: step 313, loss 0.302078, acc 0.90625, prec 0.0265192, recall 0.761682
2017-12-10T02:34:06.248924: step 314, loss 1.99042, acc 0.796875, prec 0.0264933, recall 0.759907
2017-12-10T02:34:06.517444: step 315, loss 1.2749, acc 0.703125, prec 0.0264525, recall 0.759907
2017-12-10T02:34:06.783372: step 316, loss 11.2887, acc 0.609375, prec 0.0264032, recall 0.75638
2017-12-10T02:34:07.043386: step 317, loss 2.32949, acc 0.53125, prec 0.0263392, recall 0.75638
2017-12-10T02:34:07.299903: step 318, loss 2.52821, acc 0.515625, prec 0.0263518, recall 0.756944
2017-12-10T02:34:07.557607: step 319, loss 4.08957, acc 0.375, prec 0.0263454, recall 0.757506
2017-12-10T02:34:07.818117: step 320, loss 3.96811, acc 0.328125, prec 0.0262547, recall 0.757506
2017-12-10T02:34:08.078855: step 321, loss 4.68906, acc 0.328125, prec 0.0261646, recall 0.757506
2017-12-10T02:34:08.337906: step 322, loss 4.38433, acc 0.4375, prec 0.0261672, recall 0.758065
2017-12-10T02:34:08.596724: step 323, loss 3.03361, acc 0.4375, prec 0.0261697, recall 0.758621
2017-12-10T02:34:08.861373: step 324, loss 3.8033, acc 0.421875, prec 0.0260931, recall 0.758621
2017-12-10T02:34:09.119033: step 325, loss 2.42816, acc 0.515625, prec 0.0261062, recall 0.759174
2017-12-10T02:34:09.383454: step 326, loss 2.71276, acc 0.484375, prec 0.026115, recall 0.759725
2017-12-10T02:34:09.643371: step 327, loss 2.17867, acc 0.640625, prec 0.0260678, recall 0.759725
2017-12-10T02:34:09.901263: step 328, loss 1.77314, acc 0.609375, prec 0.0260931, recall 0.760274
2017-12-10T02:34:10.164498: step 329, loss 3.48526, acc 0.671875, prec 0.0262046, recall 0.759637
2017-12-10T02:34:10.431523: step 330, loss 0.995467, acc 0.703125, prec 0.0262418, recall 0.760181
2017-12-10T02:34:10.693659: step 331, loss 0.686386, acc 0.828125, prec 0.0262953, recall 0.760722
2017-12-10T02:34:10.955226: step 332, loss 1.30185, acc 0.6875, prec 0.0262543, recall 0.760722
2017-12-10T02:34:11.217868: step 333, loss 0.525628, acc 0.859375, prec 0.0263117, recall 0.761261
2017-12-10T02:34:11.487423: step 334, loss 3.82023, acc 0.921875, prec 0.0265308, recall 0.761161
2017-12-10T02:34:11.756727: step 335, loss 1.40385, acc 0.859375, prec 0.0265879, recall 0.761693
2017-12-10T02:34:12.023441: step 336, loss 0.521812, acc 0.84375, prec 0.0265672, recall 0.761693
2017-12-10T02:34:12.294428: step 337, loss 0.320718, acc 0.859375, prec 0.0265487, recall 0.761693
2017-12-10T02:34:12.555841: step 338, loss 0.944042, acc 0.8125, prec 0.0265995, recall 0.762222
2017-12-10T02:34:12.822577: step 339, loss 1.68873, acc 0.828125, prec 0.0266522, recall 0.762749
2017-12-10T02:34:13.088155: step 340, loss 0.438986, acc 0.828125, prec 0.0266295, recall 0.762749
2017-12-10T02:34:13.356036: step 341, loss 7.68761, acc 0.828125, prec 0.0267595, recall 0.762115
2017-12-10T02:34:13.639340: step 342, loss 0.441504, acc 0.859375, prec 0.0269665, recall 0.763676
2017-12-10T02:34:13.901134: step 343, loss 1.8141, acc 0.65625, prec 0.0269207, recall 0.763676
2017-12-10T02:34:14.162683: step 344, loss 0.98846, acc 0.8125, prec 0.0270458, recall 0.764706
2017-12-10T02:34:14.424036: step 345, loss 6.04352, acc 0.703125, prec 0.0270083, recall 0.763043
2017-12-10T02:34:14.683706: step 346, loss 1.21806, acc 0.6875, prec 0.0270416, recall 0.763557
2017-12-10T02:34:14.943573: step 347, loss 1.73876, acc 0.578125, prec 0.0269856, recall 0.763557
2017-12-10T02:34:15.216346: step 348, loss 1.77041, acc 0.625, prec 0.0271594, recall 0.765086
2017-12-10T02:34:15.483571: step 349, loss 3.69015, acc 0.578125, prec 0.0271054, recall 0.763441
2017-12-10T02:34:15.743826: step 350, loss 4.23041, acc 0.578125, prec 0.0271259, recall 0.762313
2017-12-10T02:34:16.008362: step 351, loss 2.88786, acc 0.453125, prec 0.0270537, recall 0.762313
2017-12-10T02:34:16.273926: step 352, loss 2.95356, acc 0.421875, prec 0.0270516, recall 0.762821
2017-12-10T02:34:16.539718: step 353, loss 4.08821, acc 0.375, prec 0.0269699, recall 0.762821
2017-12-10T02:34:16.796970: step 354, loss 3.6016, acc 0.515625, prec 0.0269802, recall 0.763326
2017-12-10T02:34:17.055401: step 355, loss 3.90993, acc 0.390625, prec 0.0269011, recall 0.763326
2017-12-10T02:34:17.322485: step 356, loss 4.60012, acc 0.46875, prec 0.0269804, recall 0.762712
2017-12-10T02:34:17.587112: step 357, loss 2.76688, acc 0.484375, prec 0.0270594, recall 0.763713
2017-12-10T02:34:17.854866: step 358, loss 3.13527, acc 0.515625, prec 0.0270694, recall 0.764211
2017-12-10T02:34:18.112335: step 359, loss 3.04606, acc 0.4375, prec 0.0270692, recall 0.764706
2017-12-10T02:34:18.380588: step 360, loss 1.73637, acc 0.640625, prec 0.027023, recall 0.764706
2017-12-10T02:34:18.644650: step 361, loss 1.07817, acc 0.71875, prec 0.026987, recall 0.764706
2017-12-10T02:34:18.909372: step 362, loss 1.54365, acc 0.765625, prec 0.0271731, recall 0.76618
2017-12-10T02:34:19.174693: step 363, loss 0.684829, acc 0.8125, prec 0.027149, recall 0.76618
2017-12-10T02:34:19.431077: step 364, loss 0.79, acc 0.8125, prec 0.0271249, recall 0.76618
2017-12-10T02:34:19.693810: step 365, loss 0.941494, acc 0.78125, prec 0.0271687, recall 0.766667
2017-12-10T02:34:19.961847: step 366, loss 0.59001, acc 0.84375, prec 0.0271487, recall 0.766667
2017-12-10T02:34:20.223381: step 367, loss 12.951, acc 0.90625, prec 0.0271406, recall 0.763485
2017-12-10T02:34:20.493356: step 368, loss 0.381573, acc 0.890625, prec 0.0271983, recall 0.763975
2017-12-10T02:34:20.759257: step 369, loss 0.397334, acc 0.875, prec 0.0271823, recall 0.763975
2017-12-10T02:34:21.027025: step 370, loss 8.93468, acc 0.859375, prec 0.0272379, recall 0.762887
2017-12-10T02:34:21.302899: step 371, loss 1.01289, acc 0.78125, prec 0.0272099, recall 0.762887
2017-12-10T02:34:21.566513: step 372, loss 19.2371, acc 0.6875, prec 0.0271739, recall 0.759754
2017-12-10T02:34:21.831457: step 373, loss 1.74129, acc 0.640625, prec 0.0271994, recall 0.760246
2017-12-10T02:34:22.096330: step 374, loss 3.08578, acc 0.625, prec 0.0271536, recall 0.758691
2017-12-10T02:34:22.354720: step 375, loss 2.6374, acc 0.5625, prec 0.0270981, recall 0.758691
2017-12-10T02:34:22.622016: step 376, loss 2.7407, acc 0.5625, prec 0.0271137, recall 0.759184
2017-12-10T02:34:22.880994: step 377, loss 2.6073, acc 0.578125, prec 0.0271312, recall 0.759674
2017-12-10T02:34:23.140155: step 378, loss 4.20872, acc 0.375, prec 0.0271231, recall 0.760163
2017-12-10T02:34:23.403231: step 379, loss 12.3093, acc 0.359375, prec 0.0270446, recall 0.758621
2017-12-10T02:34:23.663188: step 380, loss 3.47851, acc 0.40625, prec 0.0270407, recall 0.759109
2017-12-10T02:34:23.921050: step 381, loss 5.55096, acc 0.359375, prec 0.0269629, recall 0.757576
2017-12-10T02:34:24.186912: step 382, loss 4.53728, acc 0.359375, prec 0.0268836, recall 0.757576
2017-12-10T02:34:24.451112: step 383, loss 5.84609, acc 0.28125, prec 0.0269343, recall 0.758551
2017-12-10T02:34:24.714504: step 384, loss 4.46981, acc 0.3125, prec 0.0269885, recall 0.759519
2017-12-10T02:34:24.984638: step 385, loss 11.415, acc 0.421875, prec 0.0269195, recall 0.758
2017-12-10T02:34:25.243291: step 386, loss 4.01865, acc 0.421875, prec 0.0269868, recall 0.758964
2017-12-10T02:34:25.513092: step 387, loss 4.55698, acc 0.390625, prec 0.0270499, recall 0.759921
2017-12-10T02:34:25.784997: step 388, loss 3.0623, acc 0.5, prec 0.0270575, recall 0.760396
2017-12-10T02:34:26.047388: step 389, loss 8.02628, acc 0.390625, prec 0.0270536, recall 0.759369
2017-12-10T02:34:26.318833: step 390, loss 3.86025, acc 0.40625, prec 0.0271179, recall 0.760314
2017-12-10T02:34:26.591989: step 391, loss 3.3875, acc 0.375, prec 0.0271101, recall 0.760784
2017-12-10T02:34:26.865205: step 392, loss 10.9541, acc 0.515625, prec 0.0271212, recall 0.759766
2017-12-10T02:34:27.123397: step 393, loss 3.47338, acc 0.40625, prec 0.0271172, recall 0.760234
2017-12-10T02:34:27.380719: step 394, loss 1.89851, acc 0.609375, prec 0.0271377, recall 0.7607
2017-12-10T02:34:27.648816: step 395, loss 2.12429, acc 0.546875, prec 0.0270832, recall 0.7607
2017-12-10T02:34:27.906078: step 396, loss 1.67873, acc 0.65625, prec 0.027042, recall 0.7607
2017-12-10T02:34:28.174139: step 397, loss 2.48656, acc 0.65625, prec 0.0270009, recall 0.7607
2017-12-10T02:34:28.437183: step 398, loss 1.99265, acc 0.59375, prec 0.0270196, recall 0.761165
2017-12-10T02:34:28.697385: step 399, loss 5.01656, acc 0.703125, prec 0.0269861, recall 0.75969
2017-12-10T02:34:28.967789: step 400, loss 2.40272, acc 0.609375, prec 0.0269397, recall 0.75969
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-400

2017-12-10T02:34:30.255230: step 401, loss 0.699326, acc 0.8125, prec 0.0269843, recall 0.760155
2017-12-10T02:34:30.522935: step 402, loss 0.66717, acc 0.859375, prec 0.0271012, recall 0.761079
2017-12-10T02:34:30.785683: step 403, loss 6.31641, acc 0.8125, prec 0.0272808, recall 0.760994
2017-12-10T02:34:31.053434: step 404, loss 11.9793, acc 0.796875, prec 0.0274582, recall 0.760911
2017-12-10T02:34:31.323287: step 405, loss 1.51709, acc 0.640625, prec 0.0274151, recall 0.760911
2017-12-10T02:34:31.589775: step 406, loss 2.4035, acc 0.625, prec 0.0274365, recall 0.761364
2017-12-10T02:34:31.853310: step 407, loss 2.3769, acc 0.5625, prec 0.0273842, recall 0.761364
2017-12-10T02:34:32.114516: step 408, loss 2.1261, acc 0.625, prec 0.0275379, recall 0.762712
2017-12-10T02:34:32.387599: step 409, loss 13.0404, acc 0.625, prec 0.0274949, recall 0.761278
2017-12-10T02:34:32.656501: step 410, loss 2.26572, acc 0.5625, prec 0.0274427, recall 0.761278
2017-12-10T02:34:32.920833: step 411, loss 3.87183, acc 0.40625, prec 0.0273723, recall 0.761278
2017-12-10T02:34:33.182225: step 412, loss 3.17239, acc 0.53125, prec 0.0274481, recall 0.762172
2017-12-10T02:34:33.447076: step 413, loss 2.78607, acc 0.484375, prec 0.027518, recall 0.76306
2017-12-10T02:34:33.704836: step 414, loss 3.54573, acc 0.46875, prec 0.0274552, recall 0.76306
2017-12-10T02:34:33.973051: step 415, loss 2.54, acc 0.625, prec 0.0274762, recall 0.763501
2017-12-10T02:34:34.229239: step 416, loss 2.74684, acc 0.59375, prec 0.0274935, recall 0.763941
2017-12-10T02:34:34.492666: step 417, loss 26.6812, acc 0.515625, prec 0.0274384, recall 0.762523
2017-12-10T02:34:34.756918: step 418, loss 2.34341, acc 0.53125, prec 0.0273836, recall 0.762523
2017-12-10T02:34:35.022435: step 419, loss 1.01734, acc 0.734375, prec 0.0274173, recall 0.762963
2017-12-10T02:34:35.282903: step 420, loss 2.94466, acc 0.59375, prec 0.0274346, recall 0.763401
2017-12-10T02:34:35.545569: step 421, loss 1.84004, acc 0.625, prec 0.0275199, recall 0.764273
2017-12-10T02:34:35.801960: step 422, loss 1.15954, acc 0.734375, prec 0.0276177, recall 0.765138
2017-12-10T02:34:36.058806: step 423, loss 1.78993, acc 0.65625, prec 0.0276418, recall 0.765568
2017-12-10T02:34:36.331547: step 424, loss 8.61138, acc 0.765625, prec 0.027809, recall 0.765455
2017-12-10T02:34:36.602733: step 425, loss 1.20385, acc 0.6875, prec 0.0277723, recall 0.765455
2017-12-10T02:34:36.861031: step 426, loss 0.614385, acc 0.859375, prec 0.027884, recall 0.766304
2017-12-10T02:34:37.121530: step 427, loss 1.72381, acc 0.828125, prec 0.0279918, recall 0.767148
2017-12-10T02:34:37.384232: step 428, loss 4.7059, acc 0.828125, prec 0.0279734, recall 0.765766
2017-12-10T02:34:37.644676: step 429, loss 3.06995, acc 0.71875, prec 0.0279421, recall 0.764389
2017-12-10T02:34:37.913062: step 430, loss 1.72426, acc 0.6875, prec 0.0279693, recall 0.764812
2017-12-10T02:34:38.176675: step 431, loss 1.66366, acc 0.609375, prec 0.0279234, recall 0.764812
2017-12-10T02:34:38.435526: step 432, loss 5.14815, acc 0.59375, prec 0.0278778, recall 0.763441
2017-12-10T02:34:38.695666: step 433, loss 2.60175, acc 0.4375, prec 0.0279392, recall 0.764286
2017-12-10T02:34:38.956315: step 434, loss 3.66277, acc 0.484375, prec 0.0279424, recall 0.764706
2017-12-10T02:34:39.222751: step 435, loss 3.61276, acc 0.34375, prec 0.0280556, recall 0.765957
2017-12-10T02:34:39.480516: step 436, loss 2.42267, acc 0.53125, prec 0.028064, recall 0.766372
2017-12-10T02:34:39.740816: step 437, loss 2.3696, acc 0.5, prec 0.028006, recall 0.766372
2017-12-10T02:34:39.999717: step 438, loss 3.41477, acc 0.46875, prec 0.0280072, recall 0.766784
2017-12-10T02:34:40.272092: step 439, loss 2.76031, acc 0.453125, prec 0.0279441, recall 0.766784
2017-12-10T02:34:40.530524: step 440, loss 2.70801, acc 0.546875, prec 0.0279545, recall 0.767196
2017-12-10T02:34:40.802784: step 441, loss 1.25183, acc 0.703125, prec 0.0279828, recall 0.767606
2017-12-10T02:34:41.071272: step 442, loss 1.4169, acc 0.671875, prec 0.0279451, recall 0.767606
2017-12-10T02:34:41.339493: step 443, loss 1.15733, acc 0.78125, prec 0.0279201, recall 0.767606
2017-12-10T02:34:41.608144: step 444, loss 0.225244, acc 0.953125, prec 0.0280392, recall 0.768421
2017-12-10T02:34:41.875589: step 445, loss 0.391507, acc 0.890625, prec 0.0280888, recall 0.768827
2017-12-10T02:34:42.135813: step 446, loss 17.9094, acc 0.828125, prec 0.0280708, recall 0.767483
2017-12-10T02:34:42.403713: step 447, loss 4.95397, acc 0.921875, prec 0.0280637, recall 0.766143
2017-12-10T02:34:42.676391: step 448, loss 0.395427, acc 0.890625, prec 0.0280511, recall 0.766143
2017-12-10T02:34:42.938836: step 449, loss 0.780081, acc 0.828125, prec 0.0280935, recall 0.766551
2017-12-10T02:34:43.203313: step 450, loss 0.733995, acc 0.796875, prec 0.0280702, recall 0.766551
2017-12-10T02:34:43.461177: step 451, loss 0.524715, acc 0.875, prec 0.0280559, recall 0.766551
2017-12-10T02:34:43.722929: step 452, loss 6.27778, acc 0.765625, prec 0.0280928, recall 0.765625
2017-12-10T02:34:43.992859: step 453, loss 4.96923, acc 0.78125, prec 0.0280695, recall 0.764298
2017-12-10T02:34:44.260567: step 454, loss 1.0695, acc 0.703125, prec 0.0281592, recall 0.765112
2017-12-10T02:34:44.526691: step 455, loss 1.2886, acc 0.671875, prec 0.0281216, recall 0.765112
2017-12-10T02:34:44.796673: step 456, loss 1.31574, acc 0.65625, prec 0.0280824, recall 0.765112
2017-12-10T02:34:45.061602: step 457, loss 2.18041, acc 0.5625, prec 0.0280327, recall 0.765112
2017-12-10T02:34:45.324527: step 458, loss 1.45597, acc 0.65625, prec 0.0280551, recall 0.765517
2017-12-10T02:34:45.587396: step 459, loss 1.57995, acc 0.640625, prec 0.0280144, recall 0.765517
2017-12-10T02:34:45.846504: step 460, loss 2.26362, acc 0.515625, prec 0.0279597, recall 0.765517
2017-12-10T02:34:46.107650: step 461, loss 3.85829, acc 0.640625, prec 0.027921, recall 0.7642
2017-12-10T02:34:46.376418: step 462, loss 2.40855, acc 0.640625, prec 0.0279417, recall 0.764605
2017-12-10T02:34:46.644617: step 463, loss 1.72516, acc 0.5625, prec 0.0278927, recall 0.764605
2017-12-10T02:34:46.912433: step 464, loss 1.24565, acc 0.703125, prec 0.0279204, recall 0.765009
2017-12-10T02:34:47.178727: step 465, loss 3.98184, acc 0.65625, prec 0.0279445, recall 0.764103
2017-12-10T02:34:47.443387: step 466, loss 2.8632, acc 0.75, prec 0.0279183, recall 0.762799
2017-12-10T02:34:47.700506: step 467, loss 2.97147, acc 0.46875, prec 0.0278591, recall 0.762799
2017-12-10T02:34:47.959806: step 468, loss 1.59872, acc 0.625, prec 0.027878, recall 0.763203
2017-12-10T02:34:48.224443: step 469, loss 2.47343, acc 0.546875, prec 0.0278278, recall 0.763203
2017-12-10T02:34:48.486378: step 470, loss 2.34158, acc 0.609375, prec 0.027845, recall 0.763605
2017-12-10T02:34:48.752531: step 471, loss 3.66074, acc 0.5625, prec 0.027979, recall 0.763514
2017-12-10T02:34:49.016857: step 472, loss 1.78607, acc 0.59375, prec 0.0281142, recall 0.764706
2017-12-10T02:34:49.277601: step 473, loss 2.08111, acc 0.546875, prec 0.0280639, recall 0.764706
2017-12-10T02:34:49.544459: step 474, loss 2.21692, acc 0.625, prec 0.0280823, recall 0.765101
2017-12-10T02:34:49.812426: step 475, loss 1.68874, acc 0.578125, prec 0.0280357, recall 0.765101
2017-12-10T02:34:50.074663: step 476, loss 2.07264, acc 0.515625, prec 0.028042, recall 0.765494
2017-12-10T02:34:50.337718: step 477, loss 2.6505, acc 0.5625, prec 0.0281129, recall 0.766277
2017-12-10T02:34:50.599391: step 478, loss 1.81492, acc 0.65625, prec 0.028194, recall 0.767055
2017-12-10T02:34:50.870142: step 479, loss 1.24452, acc 0.71875, prec 0.028163, recall 0.767055
2017-12-10T02:34:51.129663: step 480, loss 8.56346, acc 0.71875, prec 0.0281931, recall 0.766169
2017-12-10T02:34:51.395787: step 481, loss 0.565167, acc 0.8125, prec 0.0281725, recall 0.766169
2017-12-10T02:34:51.667760: step 482, loss 11.7705, acc 0.6875, prec 0.028199, recall 0.765289
2017-12-10T02:34:51.928808: step 483, loss 1.49118, acc 0.71875, prec 0.0281682, recall 0.765289
2017-12-10T02:34:52.193786: step 484, loss 1.58859, acc 0.703125, prec 0.0281356, recall 0.765289
2017-12-10T02:34:52.453216: step 485, loss 1.35362, acc 0.65625, prec 0.0280981, recall 0.765289
2017-12-10T02:34:52.715439: step 486, loss 1.72071, acc 0.65625, prec 0.0280606, recall 0.765289
2017-12-10T02:34:52.973825: step 487, loss 8.83607, acc 0.640625, prec 0.0280821, recall 0.764415
2017-12-10T02:34:53.239075: step 488, loss 1.87868, acc 0.59375, prec 0.0280967, recall 0.764803
2017-12-10T02:34:53.510829: step 489, loss 1.66226, acc 0.578125, prec 0.0280509, recall 0.764803
2017-12-10T02:34:53.772797: step 490, loss 1.86887, acc 0.65625, prec 0.0280723, recall 0.765189
2017-12-10T02:34:54.037815: step 491, loss 1.59203, acc 0.625, prec 0.0280902, recall 0.765574
2017-12-10T02:34:54.305787: step 492, loss 3.01234, acc 0.578125, prec 0.028103, recall 0.765957
2017-12-10T02:34:54.571883: step 493, loss 1.67042, acc 0.640625, prec 0.0281226, recall 0.76634
2017-12-10T02:34:54.837118: step 494, loss 1.14536, acc 0.6875, prec 0.0280889, recall 0.76634
2017-12-10T02:34:55.102617: step 495, loss 3.82615, acc 0.671875, prec 0.0280553, recall 0.76509
2017-12-10T02:34:55.360609: step 496, loss 10.9665, acc 0.65625, prec 0.0280781, recall 0.764228
2017-12-10T02:34:55.587582: step 497, loss 1.45274, acc 0.705882, prec 0.028227, recall 0.765372
2017-12-10T02:34:55.858058: step 498, loss 1.65423, acc 0.640625, prec 0.028362, recall 0.766506
2017-12-10T02:34:56.116741: step 499, loss 1.80934, acc 0.5625, prec 0.0284304, recall 0.767255
2017-12-10T02:34:56.383688: step 500, loss 6.56913, acc 0.625, prec 0.0285069, recall 0.766773
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-500

2017-12-10T02:34:57.670764: step 501, loss 2.10404, acc 0.65625, prec 0.0285274, recall 0.767145
2017-12-10T02:34:57.937093: step 502, loss 2.40601, acc 0.53125, prec 0.0284767, recall 0.767145
2017-12-10T02:34:58.206344: step 503, loss 1.64147, acc 0.640625, prec 0.028438, recall 0.767145
2017-12-10T02:34:58.469223: step 504, loss 2.71917, acc 0.578125, prec 0.0285073, recall 0.767886
2017-12-10T02:34:58.725582: step 505, loss 9.42737, acc 0.640625, prec 0.0284704, recall 0.766667
2017-12-10T02:34:58.990571: step 506, loss 2.02103, acc 0.578125, prec 0.0284823, recall 0.767036
2017-12-10T02:34:59.252861: step 507, loss 2.4659, acc 0.546875, prec 0.0284338, recall 0.767036
2017-12-10T02:34:59.523674: step 508, loss 2.65921, acc 0.46875, prec 0.028491, recall 0.767772
2017-12-10T02:34:59.790073: step 509, loss 2.64356, acc 0.53125, prec 0.0285547, recall 0.768504
2017-12-10T02:35:00.057578: step 510, loss 1.74628, acc 0.625, prec 0.0286282, recall 0.769231
2017-12-10T02:35:00.327240: step 511, loss 1.41893, acc 0.609375, prec 0.0285864, recall 0.769231
2017-12-10T02:35:00.594231: step 512, loss 1.17659, acc 0.671875, prec 0.0285514, recall 0.769231
2017-12-10T02:35:00.859485: step 513, loss 2.2004, acc 0.65625, prec 0.028628, recall 0.769953
2017-12-10T02:35:01.126214: step 514, loss 1.12097, acc 0.703125, prec 0.0286528, recall 0.770312
2017-12-10T02:35:01.391925: step 515, loss 1.09443, acc 0.734375, prec 0.0286245, recall 0.770312
2017-12-10T02:35:01.660827: step 516, loss 1.1169, acc 0.84375, prec 0.0286643, recall 0.770671
2017-12-10T02:35:01.924100: step 517, loss 0.468126, acc 0.84375, prec 0.0286476, recall 0.770671
2017-12-10T02:35:02.187824: step 518, loss 0.810347, acc 0.8125, prec 0.0286277, recall 0.770671
2017-12-10T02:35:02.449822: step 519, loss 0.37082, acc 0.90625, prec 0.028674, recall 0.771028
2017-12-10T02:35:02.710233: step 520, loss 0.57013, acc 0.8125, prec 0.0287104, recall 0.771384
2017-12-10T02:35:02.968727: step 521, loss 11.3039, acc 0.953125, prec 0.0287632, recall 0.770543
2017-12-10T02:35:03.232177: step 522, loss 2.14176, acc 0.921875, prec 0.0287566, recall 0.76935
2017-12-10T02:35:03.493331: step 523, loss 0.423142, acc 0.84375, prec 0.02874, recall 0.76935
2017-12-10T02:35:03.759228: step 524, loss 0.396353, acc 0.84375, prec 0.0287233, recall 0.76935
2017-12-10T02:35:04.023463: step 525, loss 0.345747, acc 0.859375, prec 0.0287084, recall 0.76935
2017-12-10T02:35:04.289167: step 526, loss 0.480893, acc 0.828125, prec 0.0286902, recall 0.76935
2017-12-10T02:35:04.557633: step 527, loss 0.22726, acc 0.890625, prec 0.0287346, recall 0.769706
2017-12-10T02:35:04.822174: step 528, loss 0.34816, acc 0.921875, prec 0.0287824, recall 0.770062
2017-12-10T02:35:05.092051: step 529, loss 0.696804, acc 0.859375, prec 0.0287674, recall 0.770062
2017-12-10T02:35:05.360261: step 530, loss 0.657261, acc 0.890625, prec 0.0287558, recall 0.770062
2017-12-10T02:35:05.624510: step 531, loss 0.688086, acc 0.796875, prec 0.0287902, recall 0.770416
2017-12-10T02:35:05.885865: step 532, loss 0.426801, acc 0.890625, prec 0.0288904, recall 0.771121
2017-12-10T02:35:06.148727: step 533, loss 0.348699, acc 0.921875, prec 0.0288821, recall 0.771121
2017-12-10T02:35:06.422775: step 534, loss 0.642222, acc 0.78125, prec 0.0289147, recall 0.771472
2017-12-10T02:35:06.685849: step 535, loss 0.192118, acc 0.9375, prec 0.028908, recall 0.771472
2017-12-10T02:35:06.957574: step 536, loss 0.312048, acc 0.875, prec 0.0288948, recall 0.771472
2017-12-10T02:35:07.219682: step 537, loss 1.74058, acc 0.90625, prec 0.0288865, recall 0.770291
2017-12-10T02:35:07.492835: step 538, loss 0.193372, acc 0.953125, prec 0.0288815, recall 0.770291
2017-12-10T02:35:07.754144: step 539, loss 0.323935, acc 0.890625, prec 0.0289256, recall 0.770642
2017-12-10T02:35:08.021827: step 540, loss 4.94474, acc 0.921875, prec 0.0289747, recall 0.769817
2017-12-10T02:35:08.293299: step 541, loss 0.687851, acc 0.828125, prec 0.0289564, recall 0.769817
2017-12-10T02:35:08.564523: step 542, loss 0.439775, acc 0.859375, prec 0.0289415, recall 0.769817
2017-12-10T02:35:08.828450: step 543, loss 0.426623, acc 0.84375, prec 0.0289805, recall 0.770167
2017-12-10T02:35:09.085455: step 544, loss 0.50481, acc 0.828125, prec 0.0290179, recall 0.770517
2017-12-10T02:35:09.358272: step 545, loss 0.900951, acc 0.78125, prec 0.0290502, recall 0.770865
2017-12-10T02:35:09.612301: step 546, loss 0.835559, acc 0.796875, prec 0.0290286, recall 0.770865
2017-12-10T02:35:09.874058: step 547, loss 0.600829, acc 0.890625, prec 0.029017, recall 0.770865
2017-12-10T02:35:10.143396: step 548, loss 0.45288, acc 0.796875, prec 0.0289954, recall 0.770865
2017-12-10T02:35:10.405064: step 549, loss 8.59854, acc 0.828125, prec 0.0289789, recall 0.769697
2017-12-10T02:35:10.673683: step 550, loss 1.39711, acc 0.640625, prec 0.0289962, recall 0.770045
2017-12-10T02:35:10.937562: step 551, loss 1.23218, acc 0.75, prec 0.0290251, recall 0.770393
2017-12-10T02:35:11.198464: step 552, loss 1.23381, acc 0.71875, prec 0.0290506, recall 0.770739
2017-12-10T02:35:11.473669: step 553, loss 1.15988, acc 0.75, prec 0.0290242, recall 0.770739
2017-12-10T02:35:11.738698: step 554, loss 1.29095, acc 0.703125, prec 0.0289929, recall 0.770739
2017-12-10T02:35:12.002379: step 555, loss 0.801811, acc 0.828125, prec 0.0289748, recall 0.770739
2017-12-10T02:35:12.261377: step 556, loss 1.25191, acc 0.765625, prec 0.0290052, recall 0.771084
2017-12-10T02:35:12.525712: step 557, loss 4.77078, acc 0.734375, prec 0.0290339, recall 0.77027
2017-12-10T02:35:12.789240: step 558, loss 0.604009, acc 0.890625, prec 0.0290773, recall 0.770615
2017-12-10T02:35:13.050817: step 559, loss 0.844925, acc 0.78125, prec 0.0290543, recall 0.770615
2017-12-10T02:35:13.312938: step 560, loss 1.36154, acc 0.71875, prec 0.0290248, recall 0.770615
2017-12-10T02:35:13.576633: step 561, loss 1.48211, acc 0.671875, prec 0.0290452, recall 0.770958
2017-12-10T02:35:13.836221: step 562, loss 1.17018, acc 0.75, prec 0.0291831, recall 0.771982
2017-12-10T02:35:14.098247: step 563, loss 1.22868, acc 0.734375, prec 0.0292098, recall 0.772321
2017-12-10T02:35:14.365361: step 564, loss 2.83791, acc 0.78125, prec 0.0292431, recall 0.771513
2017-12-10T02:35:14.634944: step 565, loss 0.82167, acc 0.796875, prec 0.0292217, recall 0.771513
2017-12-10T02:35:14.899032: step 566, loss 0.770597, acc 0.765625, prec 0.0291971, recall 0.771513
2017-12-10T02:35:15.162726: step 567, loss 1.05555, acc 0.703125, prec 0.029166, recall 0.771513
2017-12-10T02:35:15.422322: step 568, loss 1.03484, acc 0.8125, prec 0.0291463, recall 0.771513
2017-12-10T02:35:15.691710: step 569, loss 0.874574, acc 0.75, prec 0.0291202, recall 0.771513
2017-12-10T02:35:15.962111: step 570, loss 0.787882, acc 0.75, prec 0.0292028, recall 0.772189
2017-12-10T02:35:16.229143: step 571, loss 0.740206, acc 0.765625, prec 0.0291783, recall 0.772189
2017-12-10T02:35:16.491625: step 572, loss 0.406098, acc 0.875, prec 0.0291653, recall 0.772189
2017-12-10T02:35:16.754638: step 573, loss 0.515638, acc 0.84375, prec 0.0292032, recall 0.772526
2017-12-10T02:35:17.014752: step 574, loss 3.09151, acc 0.8125, prec 0.0292394, recall 0.771723
2017-12-10T02:35:17.280962: step 575, loss 1.77904, acc 0.921875, prec 0.0293412, recall 0.771261
2017-12-10T02:35:17.549502: step 576, loss 0.68112, acc 0.796875, prec 0.0294822, recall 0.772263
2017-12-10T02:35:17.811905: step 577, loss 0.684987, acc 0.828125, prec 0.0294642, recall 0.772263
2017-12-10T02:35:18.078566: step 578, loss 0.421811, acc 0.890625, prec 0.0295067, recall 0.772595
2017-12-10T02:35:18.336669: step 579, loss 0.851089, acc 0.8125, prec 0.029541, recall 0.772926
2017-12-10T02:35:18.606404: step 580, loss 2.1233, acc 0.828125, prec 0.0295786, recall 0.772134
2017-12-10T02:35:18.870123: step 581, loss 0.765102, acc 0.796875, prec 0.0295572, recall 0.772134
2017-12-10T02:35:19.134055: step 582, loss 4.09663, acc 0.796875, prec 0.0295914, recall 0.771346
2017-12-10T02:35:19.400204: step 583, loss 1.12821, acc 0.796875, prec 0.0296239, recall 0.771676
2017-12-10T02:35:19.662475: step 584, loss 0.802677, acc 0.734375, prec 0.029596, recall 0.771676
2017-12-10T02:35:19.929684: step 585, loss 0.959817, acc 0.78125, prec 0.0296805, recall 0.772334
2017-12-10T02:35:20.193937: step 586, loss 8.6777, acc 0.703125, prec 0.0296526, recall 0.770115
2017-12-10T02:35:20.462047: step 587, loss 2.16732, acc 0.59375, prec 0.0297172, recall 0.770774
2017-12-10T02:35:20.725217: step 588, loss 1.76177, acc 0.609375, prec 0.0296762, recall 0.770774
2017-12-10T02:35:20.981910: step 589, loss 2.14923, acc 0.484375, prec 0.0296223, recall 0.770774
2017-12-10T02:35:21.241617: step 590, loss 3.02335, acc 0.46875, prec 0.0295669, recall 0.770774
2017-12-10T02:35:21.501301: step 591, loss 3.59621, acc 0.4375, prec 0.0295618, recall 0.771102
2017-12-10T02:35:21.757364: step 592, loss 2.95951, acc 0.546875, prec 0.0296742, recall 0.77208
2017-12-10T02:35:22.017576: step 593, loss 3.08402, acc 0.515625, prec 0.029624, recall 0.77208
2017-12-10T02:35:22.279504: step 594, loss 2.73304, acc 0.4375, prec 0.0295658, recall 0.77208
2017-12-10T02:35:22.541973: step 595, loss 3.50953, acc 0.515625, prec 0.0295687, recall 0.772404
2017-12-10T02:35:22.805256: step 596, loss 1.9066, acc 0.609375, prec 0.0295285, recall 0.772404
2017-12-10T02:35:23.062307: step 597, loss 1.47845, acc 0.71875, prec 0.0296051, recall 0.77305
2017-12-10T02:35:23.321519: step 598, loss 1.91048, acc 0.609375, prec 0.0295649, recall 0.77305
2017-12-10T02:35:23.582459: step 599, loss 1.30245, acc 0.65625, prec 0.0295297, recall 0.77305
2017-12-10T02:35:23.847089: step 600, loss 1.18243, acc 0.71875, prec 0.0295535, recall 0.773371

Evaluation:
2017-12-10T02:35:31.620597: step 600, loss 2.03349, acc 0.826021, prec 0.0326551, recall 0.765517

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-600

2017-12-10T02:35:32.915831: step 601, loss 5.80151, acc 0.765625, prec 0.0326327, recall 0.764638
2017-12-10T02:35:33.179822: step 602, loss 0.633022, acc 0.84375, prec 0.0327114, recall 0.765178
2017-12-10T02:35:33.443025: step 603, loss 1.0112, acc 0.828125, prec 0.0326938, recall 0.765178
2017-12-10T02:35:33.709319: step 604, loss 0.66071, acc 0.84375, prec 0.0326778, recall 0.765178
2017-12-10T02:35:33.977085: step 605, loss 3.6718, acc 0.875, prec 0.0326666, recall 0.764302
2017-12-10T02:35:34.238490: step 606, loss 0.585811, acc 0.859375, prec 0.0326523, recall 0.764302
2017-12-10T02:35:34.498808: step 607, loss 0.80525, acc 0.84375, prec 0.0326363, recall 0.764302
2017-12-10T02:35:34.761944: step 608, loss 18.9965, acc 0.875, prec 0.0326252, recall 0.763429
2017-12-10T02:35:35.034015: step 609, loss 0.574597, acc 0.828125, prec 0.0326076, recall 0.763429
2017-12-10T02:35:35.292546: step 610, loss 0.627594, acc 0.875, prec 0.0325949, recall 0.763429
2017-12-10T02:35:35.560856: step 611, loss 0.588167, acc 0.84375, prec 0.0326262, recall 0.763699
2017-12-10T02:35:35.817970: step 612, loss 0.978369, acc 0.8125, prec 0.0326543, recall 0.763968
2017-12-10T02:35:36.078229: step 613, loss 1.51842, acc 0.703125, prec 0.0326711, recall 0.764237
2017-12-10T02:35:36.344879: step 614, loss 6.43956, acc 0.859375, prec 0.0327055, recall 0.763636
2017-12-10T02:35:36.610674: step 615, loss 0.95584, acc 0.78125, prec 0.0327303, recall 0.763905
2017-12-10T02:35:36.879210: step 616, loss 1.7883, acc 0.640625, prec 0.0326937, recall 0.763905
2017-12-10T02:35:37.143139: step 617, loss 1.06359, acc 0.640625, prec 0.0326572, recall 0.763905
2017-12-10T02:35:37.398718: step 618, loss 1.57901, acc 0.6875, prec 0.0326724, recall 0.764172
2017-12-10T02:35:37.657583: step 619, loss 1.77825, acc 0.625, prec 0.0326345, recall 0.764172
2017-12-10T02:35:37.917431: step 620, loss 3.17814, acc 0.78125, prec 0.0327076, recall 0.763842
2017-12-10T02:35:38.182434: step 621, loss 0.814845, acc 0.796875, prec 0.0327338, recall 0.764108
2017-12-10T02:35:38.442355: step 622, loss 0.822526, acc 0.796875, prec 0.0327132, recall 0.764108
2017-12-10T02:35:38.703668: step 623, loss 1.31294, acc 0.796875, prec 0.0326927, recall 0.764108
2017-12-10T02:35:38.972674: step 624, loss 0.860802, acc 0.71875, prec 0.0326643, recall 0.764108
2017-12-10T02:35:39.230462: step 625, loss 1.79062, acc 0.71875, prec 0.0326359, recall 0.764108
2017-12-10T02:35:39.493269: step 626, loss 0.88231, acc 0.796875, prec 0.0327553, recall 0.764904
2017-12-10T02:35:39.758445: step 627, loss 0.692576, acc 0.828125, prec 0.032738, recall 0.764904
2017-12-10T02:35:40.022597: step 628, loss 1.0534, acc 0.796875, prec 0.032764, recall 0.765169
2017-12-10T02:35:40.291346: step 629, loss 0.626425, acc 0.828125, prec 0.0327467, recall 0.765169
2017-12-10T02:35:40.554966: step 630, loss 0.756952, acc 0.78125, prec 0.0328176, recall 0.765695
2017-12-10T02:35:40.821512: step 631, loss 16.7527, acc 0.796875, prec 0.0328451, recall 0.765101
2017-12-10T02:35:41.084379: step 632, loss 0.682258, acc 0.796875, prec 0.0328246, recall 0.765101
2017-12-10T02:35:41.349535: step 633, loss 0.676959, acc 0.84375, prec 0.0328553, recall 0.765363
2017-12-10T02:35:41.621498: step 634, loss 5.52157, acc 0.90625, prec 0.0328474, recall 0.764509
2017-12-10T02:35:41.886947: step 635, loss 0.602876, acc 0.859375, prec 0.0328332, recall 0.764509
2017-12-10T02:35:42.148383: step 636, loss 0.687834, acc 0.8125, prec 0.0328607, recall 0.764771
2017-12-10T02:35:42.409686: step 637, loss 0.813368, acc 0.78125, prec 0.0328387, recall 0.764771
2017-12-10T02:35:42.673469: step 638, loss 1.35976, acc 0.71875, prec 0.0328567, recall 0.765033
2017-12-10T02:35:42.934413: step 639, loss 0.434959, acc 0.859375, prec 0.0328888, recall 0.765295
2017-12-10T02:35:43.204386: step 640, loss 1.15643, acc 0.765625, prec 0.0329576, recall 0.765816
2017-12-10T02:35:43.462114: step 641, loss 0.710656, acc 0.828125, prec 0.0329864, recall 0.766075
2017-12-10T02:35:43.730765: step 642, loss 1.12442, acc 0.71875, prec 0.0329581, recall 0.766075
2017-12-10T02:35:43.998332: step 643, loss 0.826433, acc 0.796875, prec 0.0329838, recall 0.766334
2017-12-10T02:35:44.260682: step 644, loss 0.615029, acc 0.84375, prec 0.0330141, recall 0.766593
2017-12-10T02:35:44.520718: step 645, loss 0.476965, acc 0.859375, prec 0.033, recall 0.766593
2017-12-10T02:35:44.779407: step 646, loss 0.785893, acc 0.828125, prec 0.0330287, recall 0.766851
2017-12-10T02:35:45.046920: step 647, loss 2.88179, acc 0.890625, prec 0.0330653, recall 0.766262
2017-12-10T02:35:45.315904: step 648, loss 15.5613, acc 0.75, prec 0.0330417, recall 0.765419
2017-12-10T02:35:45.592028: step 649, loss 0.873589, acc 0.796875, prec 0.0330673, recall 0.765677
2017-12-10T02:35:45.854078: step 650, loss 0.933099, acc 0.828125, prec 0.03305, recall 0.765677
2017-12-10T02:35:46.117647: step 651, loss 0.762315, acc 0.796875, prec 0.0330755, recall 0.765934
2017-12-10T02:35:46.385703: step 652, loss 1.09044, acc 0.703125, prec 0.0330915, recall 0.766191
2017-12-10T02:35:46.654079: step 653, loss 1.04762, acc 0.765625, prec 0.0331138, recall 0.766447
2017-12-10T02:35:46.920586: step 654, loss 1.86953, acc 0.671875, prec 0.0331267, recall 0.766703
2017-12-10T02:35:47.188907: step 655, loss 2.91108, acc 0.65625, prec 0.0332293, recall 0.767467
2017-12-10T02:35:47.450394: step 656, loss 1.28373, acc 0.671875, prec 0.0333333, recall 0.768226
2017-12-10T02:35:47.716567: step 657, loss 3.49621, acc 0.75, prec 0.033401, recall 0.767896
2017-12-10T02:35:47.977116: step 658, loss 1.55591, acc 0.71875, prec 0.0336004, recall 0.769148
2017-12-10T02:35:48.242686: step 659, loss 1.72522, acc 0.671875, prec 0.0335672, recall 0.769148
2017-12-10T02:35:48.508067: step 660, loss 1.91289, acc 0.625, prec 0.0336202, recall 0.769645
2017-12-10T02:35:48.771980: step 661, loss 2.03415, acc 0.59375, prec 0.0336245, recall 0.769892
2017-12-10T02:35:49.032626: step 662, loss 2.43184, acc 0.53125, prec 0.0336225, recall 0.77014
2017-12-10T02:35:49.292692: step 663, loss 2.03697, acc 0.609375, prec 0.0336284, recall 0.770386
2017-12-10T02:35:49.551396: step 664, loss 13.3048, acc 0.578125, prec 0.0336327, recall 0.769807
2017-12-10T02:35:49.815095: step 665, loss 1.4014, acc 0.65625, prec 0.0336433, recall 0.770053
2017-12-10T02:35:50.078172: step 666, loss 1.58876, acc 0.640625, prec 0.0337425, recall 0.770789
2017-12-10T02:35:50.335472: step 667, loss 1.22037, acc 0.734375, prec 0.0337157, recall 0.770789
2017-12-10T02:35:50.595517: step 668, loss 1.48999, acc 0.703125, prec 0.0337309, recall 0.771033
2017-12-10T02:35:50.859795: step 669, loss 1.57269, acc 0.6875, prec 0.0336995, recall 0.771033
2017-12-10T02:35:51.116434: step 670, loss 1.57682, acc 0.671875, prec 0.0337115, recall 0.771277
2017-12-10T02:35:51.383084: step 671, loss 1.18839, acc 0.65625, prec 0.0336771, recall 0.771277
2017-12-10T02:35:51.640454: step 672, loss 1.15992, acc 0.75, prec 0.0336521, recall 0.771277
2017-12-10T02:35:51.900946: step 673, loss 0.740984, acc 0.828125, prec 0.0336797, recall 0.77152
2017-12-10T02:35:52.164402: step 674, loss 0.520246, acc 0.859375, prec 0.0338001, recall 0.772246
2017-12-10T02:35:52.434625: step 675, loss 0.357926, acc 0.859375, prec 0.033786, recall 0.772246
2017-12-10T02:35:52.693447: step 676, loss 8.52367, acc 0.90625, prec 0.0337781, recall 0.771429
2017-12-10T02:35:52.959289: step 677, loss 0.470108, acc 0.90625, prec 0.0337688, recall 0.771429
2017-12-10T02:35:53.219965: step 678, loss 7.74063, acc 0.859375, prec 0.0338457, recall 0.771097
2017-12-10T02:35:53.490226: step 679, loss 0.448967, acc 0.859375, prec 0.0338316, recall 0.771097
2017-12-10T02:35:53.750437: step 680, loss 5.47858, acc 0.75, prec 0.0338082, recall 0.770285
2017-12-10T02:35:54.013332: step 681, loss 1.04478, acc 0.765625, prec 0.033874, recall 0.770768
2017-12-10T02:35:54.274469: step 682, loss 1.16938, acc 0.671875, prec 0.0339304, recall 0.771249
2017-12-10T02:35:54.532644: step 683, loss 2.71502, acc 0.5, prec 0.0338803, recall 0.771249
2017-12-10T02:35:54.794285: step 684, loss 2.23652, acc 0.5, prec 0.0338304, recall 0.771249
2017-12-10T02:35:55.052670: step 685, loss 1.95531, acc 0.53125, prec 0.0337838, recall 0.771249
2017-12-10T02:35:55.312352: step 686, loss 2.91381, acc 0.359375, prec 0.0337202, recall 0.771249
2017-12-10T02:35:55.569052: step 687, loss 2.37957, acc 0.421875, prec 0.0336631, recall 0.771249
2017-12-10T02:35:55.824871: step 688, loss 2.8241, acc 0.515625, prec 0.0336596, recall 0.771488
2017-12-10T02:35:56.081784: step 689, loss 2.5504, acc 0.484375, prec 0.0336971, recall 0.771967
2017-12-10T02:35:56.346507: step 690, loss 1.93778, acc 0.4375, prec 0.0336418, recall 0.771967
2017-12-10T02:35:56.615858: step 691, loss 1.35145, acc 0.65625, prec 0.0336081, recall 0.771967
2017-12-10T02:35:56.878339: step 692, loss 1.7686, acc 0.578125, prec 0.0336547, recall 0.772443
2017-12-10T02:35:57.144213: step 693, loss 0.994792, acc 0.703125, prec 0.0336257, recall 0.772443
2017-12-10T02:35:57.405361: step 694, loss 0.905535, acc 0.859375, prec 0.0336558, recall 0.77268
2017-12-10T02:35:57.666019: step 695, loss 2.80182, acc 0.90625, prec 0.0336482, recall 0.771875
2017-12-10T02:35:57.931774: step 696, loss 0.4645, acc 0.890625, prec 0.0336813, recall 0.772112
2017-12-10T02:35:58.192294: step 697, loss 0.334479, acc 0.921875, prec 0.0336737, recall 0.772112
2017-12-10T02:35:58.452573: step 698, loss 0.242296, acc 0.90625, prec 0.0336645, recall 0.772112
2017-12-10T02:35:58.714213: step 699, loss 0.252253, acc 0.9375, prec 0.0337023, recall 0.772349
2017-12-10T02:35:58.976354: step 700, loss 0.244386, acc 0.90625, prec 0.0336931, recall 0.772349
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-700

2017-12-10T02:36:00.213579: step 701, loss 0.167428, acc 0.9375, prec 0.033687, recall 0.772349
2017-12-10T02:36:00.482254: step 702, loss 0.142115, acc 0.96875, prec 0.0336839, recall 0.772349
2017-12-10T02:36:00.747137: step 703, loss 0.315148, acc 0.90625, prec 0.0336748, recall 0.772349
2017-12-10T02:36:01.008277: step 704, loss 0.690227, acc 0.890625, prec 0.0336641, recall 0.772349
2017-12-10T02:36:01.270532: step 705, loss 0.0738212, acc 0.96875, prec 0.033661, recall 0.772349
2017-12-10T02:36:01.539403: step 706, loss 1.91699, acc 0.953125, prec 0.033658, recall 0.771547
2017-12-10T02:36:01.808685: step 707, loss 9.77673, acc 0.953125, prec 0.0336987, recall 0.770984
2017-12-10T02:36:02.081869: step 708, loss 0.0304195, acc 1, prec 0.0337425, recall 0.771222
2017-12-10T02:36:02.345180: step 709, loss 1.01744, acc 0.96875, prec 0.0337832, recall 0.771458
2017-12-10T02:36:02.613246: step 710, loss 4.21006, acc 0.828125, prec 0.0337679, recall 0.770661
2017-12-10T02:36:02.877053: step 711, loss 5.88513, acc 0.875, prec 0.0338009, recall 0.770103
2017-12-10T02:36:03.150063: step 712, loss 0.882439, acc 0.734375, prec 0.0337749, recall 0.770103
2017-12-10T02:36:03.415892: step 713, loss 1.012, acc 0.71875, prec 0.0337911, recall 0.77034
2017-12-10T02:36:03.675124: step 714, loss 2.24436, acc 0.484375, prec 0.0337408, recall 0.77034
2017-12-10T02:36:03.935735: step 715, loss 2.00309, acc 0.546875, prec 0.0336967, recall 0.77034
2017-12-10T02:36:04.203799: step 716, loss 3.10733, acc 0.4375, prec 0.0336422, recall 0.77034
2017-12-10T02:36:04.463623: step 717, loss 3.33132, acc 0.40625, prec 0.0337149, recall 0.771047
2017-12-10T02:36:04.720550: step 718, loss 1.90218, acc 0.53125, prec 0.0337129, recall 0.771282
2017-12-10T02:36:04.986660: step 719, loss 2.07023, acc 0.546875, prec 0.0336691, recall 0.771282
2017-12-10T02:36:05.247268: step 720, loss 1.63077, acc 0.625, prec 0.0336762, recall 0.771516
2017-12-10T02:36:05.515539: step 721, loss 2.12442, acc 0.5625, prec 0.0336341, recall 0.771516
2017-12-10T02:36:05.779047: step 722, loss 1.59551, acc 0.6875, prec 0.0336472, recall 0.77175
2017-12-10T02:36:06.039230: step 723, loss 1.24956, acc 0.671875, prec 0.0336157, recall 0.77175
2017-12-10T02:36:06.298307: step 724, loss 1.29445, acc 0.75, prec 0.0335917, recall 0.77175
2017-12-10T02:36:06.566421: step 725, loss 7.61968, acc 0.6875, prec 0.0336063, recall 0.771195
2017-12-10T02:36:06.827217: step 726, loss 0.791175, acc 0.765625, prec 0.0336269, recall 0.771429
2017-12-10T02:36:07.084987: step 727, loss 0.93378, acc 0.765625, prec 0.0336474, recall 0.771662
2017-12-10T02:36:07.349244: step 728, loss 1.17118, acc 0.796875, prec 0.0337139, recall 0.772126
2017-12-10T02:36:07.613571: step 729, loss 1.04605, acc 0.765625, prec 0.0337343, recall 0.772358
2017-12-10T02:36:07.886789: step 730, loss 0.530636, acc 0.890625, prec 0.0337667, recall 0.772589
2017-12-10T02:36:08.146950: step 731, loss 9.96092, acc 0.875, prec 0.0337562, recall 0.771805
2017-12-10T02:36:08.412798: step 732, loss 0.724377, acc 0.8125, prec 0.0337811, recall 0.772036
2017-12-10T02:36:08.672364: step 733, loss 1.13324, acc 0.71875, prec 0.033797, recall 0.772267
2017-12-10T02:36:08.933325: step 734, loss 0.988134, acc 0.8125, prec 0.033779, recall 0.772267
2017-12-10T02:36:09.195659: step 735, loss 0.506623, acc 0.90625, prec 0.0338555, recall 0.772727
2017-12-10T02:36:09.459227: step 736, loss 1.36626, acc 0.796875, prec 0.0339215, recall 0.773185
2017-12-10T02:36:09.726036: step 737, loss 0.882375, acc 0.8125, prec 0.0339036, recall 0.773185
2017-12-10T02:36:09.993965: step 738, loss 1.47811, acc 0.8125, prec 0.0339283, recall 0.773414
2017-12-10T02:36:10.264203: step 739, loss 0.582724, acc 0.828125, prec 0.0339544, recall 0.773642
2017-12-10T02:36:10.527072: step 740, loss 0.73754, acc 0.875, prec 0.0339851, recall 0.773869
2017-12-10T02:36:10.784543: step 741, loss 0.835137, acc 0.875, prec 0.0340157, recall 0.774096
2017-12-10T02:36:11.057073: step 742, loss 0.489797, acc 0.875, prec 0.0340037, recall 0.774096
2017-12-10T02:36:11.323171: step 743, loss 0.404835, acc 0.859375, prec 0.0340328, recall 0.774323
2017-12-10T02:36:11.589181: step 744, loss 6.56847, acc 0.8125, prec 0.0340589, recall 0.773774
2017-12-10T02:36:11.854193: step 745, loss 2.07193, acc 0.796875, prec 0.0340409, recall 0.773
2017-12-10T02:36:12.126498: step 746, loss 1.04512, acc 0.78125, prec 0.0340624, recall 0.773227
2017-12-10T02:36:12.388465: step 747, loss 1.18824, acc 0.734375, prec 0.0341219, recall 0.773679
2017-12-10T02:36:12.653191: step 748, loss 1.49415, acc 0.75, prec 0.0340979, recall 0.773679
2017-12-10T02:36:12.912829: step 749, loss 5.91836, acc 0.65625, prec 0.0340665, recall 0.772908
2017-12-10T02:36:13.183802: step 750, loss 2.21077, acc 0.71875, prec 0.0341667, recall 0.773585
2017-12-10T02:36:13.446685: step 751, loss 2.204, acc 0.609375, prec 0.0342139, recall 0.774034
2017-12-10T02:36:13.707773: step 752, loss 2.61202, acc 0.5, prec 0.0342082, recall 0.774257
2017-12-10T02:36:13.964338: step 753, loss 1.63194, acc 0.625, prec 0.0342146, recall 0.774481
2017-12-10T02:36:14.220933: step 754, loss 2.0384, acc 0.53125, prec 0.0342119, recall 0.774704
2017-12-10T02:36:14.485451: step 755, loss 2.02562, acc 0.546875, prec 0.0341687, recall 0.774704
2017-12-10T02:36:14.744449: step 756, loss 3.16711, acc 0.5, prec 0.0343312, recall 0.775811
2017-12-10T02:36:15.005513: step 757, loss 1.81514, acc 0.640625, prec 0.0343389, recall 0.776031
2017-12-10T02:36:15.272270: step 758, loss 1.71392, acc 0.609375, prec 0.0343435, recall 0.776251
2017-12-10T02:36:15.530158: step 759, loss 4.85392, acc 0.625, prec 0.0343511, recall 0.77571
2017-12-10T02:36:15.791071: step 760, loss 1.45658, acc 0.671875, prec 0.0343617, recall 0.77593
2017-12-10T02:36:16.052224: step 761, loss 1.42614, acc 0.671875, prec 0.0344141, recall 0.776367
2017-12-10T02:36:16.317545: step 762, loss 2.26289, acc 0.75, prec 0.0344738, recall 0.776803
2017-12-10T02:36:16.586683: step 763, loss 0.805608, acc 0.796875, prec 0.0345379, recall 0.777237
2017-12-10T02:36:16.847430: step 764, loss 0.797758, acc 0.71875, prec 0.0345528, recall 0.777454
2017-12-10T02:36:17.108107: step 765, loss 1.16723, acc 0.734375, prec 0.0345691, recall 0.77767
2017-12-10T02:36:17.370030: step 766, loss 0.51592, acc 0.8125, prec 0.0345512, recall 0.77767
2017-12-10T02:36:17.633177: step 767, loss 0.963724, acc 0.765625, prec 0.0345288, recall 0.77767
2017-12-10T02:36:17.892617: step 768, loss 0.39898, acc 0.890625, prec 0.0345184, recall 0.77767
2017-12-10T02:36:18.164211: step 769, loss 0.5025, acc 0.828125, prec 0.0345021, recall 0.77767
2017-12-10T02:36:18.424412: step 770, loss 0.147674, acc 0.890625, prec 0.0344917, recall 0.77767
2017-12-10T02:36:18.681948: step 771, loss 0.384088, acc 0.875, prec 0.0344798, recall 0.77767
2017-12-10T02:36:18.942925: step 772, loss 0.413626, acc 0.890625, prec 0.034511, recall 0.777886
2017-12-10T02:36:19.218851: step 773, loss 0.588924, acc 0.9375, prec 0.0345466, recall 0.778101
2017-12-10T02:36:19.486522: step 774, loss 0.462505, acc 0.859375, prec 0.0345332, recall 0.778101
2017-12-10T02:36:19.747861: step 775, loss 0.323777, acc 0.921875, prec 0.0346503, recall 0.778744
2017-12-10T02:36:20.010970: step 776, loss 0.28878, acc 0.90625, prec 0.0346413, recall 0.778744
2017-12-10T02:36:20.279446: step 777, loss 0.200374, acc 0.921875, prec 0.0346339, recall 0.778744
2017-12-10T02:36:20.546024: step 778, loss 0.125464, acc 1, prec 0.0346754, recall 0.778958
2017-12-10T02:36:20.808602: step 779, loss 26.8649, acc 0.921875, prec 0.0346724, recall 0.776708
2017-12-10T02:36:21.075309: step 780, loss 0.338646, acc 0.921875, prec 0.0347479, recall 0.777137
2017-12-10T02:36:21.345391: step 781, loss 1.80652, acc 0.890625, prec 0.0347389, recall 0.776392
2017-12-10T02:36:21.609622: step 782, loss 2.07874, acc 0.796875, prec 0.0347625, recall 0.775862
2017-12-10T02:36:21.876190: step 783, loss 0.826665, acc 0.765625, prec 0.0347815, recall 0.776077
2017-12-10T02:36:22.139673: step 784, loss 1.14266, acc 0.671875, prec 0.0347502, recall 0.776077
2017-12-10T02:36:22.406492: step 785, loss 1.85063, acc 0.578125, prec 0.03471, recall 0.776077
2017-12-10T02:36:22.660847: step 786, loss 2.25522, acc 0.515625, prec 0.0347053, recall 0.776291
2017-12-10T02:36:22.922758: step 787, loss 2.12938, acc 0.515625, prec 0.0346594, recall 0.776291
2017-12-10T02:36:23.182864: step 788, loss 3.86868, acc 0.40625, prec 0.0346444, recall 0.776504
2017-12-10T02:36:23.447041: step 789, loss 4.88384, acc 0.390625, prec 0.0345884, recall 0.775763
2017-12-10T02:36:23.709854: step 790, loss 3.54564, acc 0.296875, prec 0.0345223, recall 0.775763
2017-12-10T02:36:23.973734: step 791, loss 3.30472, acc 0.375, prec 0.0345047, recall 0.775977
2017-12-10T02:36:24.235152: step 792, loss 2.41473, acc 0.4375, prec 0.0344521, recall 0.775977
2017-12-10T02:36:24.491757: step 793, loss 2.74684, acc 0.4375, prec 0.0343997, recall 0.775977
2017-12-10T02:36:24.759544: step 794, loss 2.63872, acc 0.359375, prec 0.0343402, recall 0.775977
2017-12-10T02:36:25.024413: step 795, loss 10.0385, acc 0.578125, prec 0.034304, recall 0.7745
2017-12-10T02:36:25.290146: step 796, loss 1.87074, acc 0.5625, prec 0.0342636, recall 0.7745
2017-12-10T02:36:25.548570: step 797, loss 1.78714, acc 0.578125, prec 0.0342247, recall 0.7745
2017-12-10T02:36:25.815068: step 798, loss 0.683953, acc 0.75, prec 0.0342017, recall 0.7745
2017-12-10T02:36:26.083342: step 799, loss 1.4628, acc 0.546875, prec 0.0341601, recall 0.7745
2017-12-10T02:36:26.352227: step 800, loss 3.74654, acc 0.671875, prec 0.0341719, recall 0.773979
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-800

2017-12-10T02:36:27.795200: step 801, loss 1.47912, acc 0.65625, prec 0.0341809, recall 0.774194
2017-12-10T02:36:28.060149: step 802, loss 1.41776, acc 0.65625, prec 0.0341494, recall 0.774194
2017-12-10T02:36:28.324609: step 803, loss 1.73703, acc 0.609375, prec 0.0341541, recall 0.774408
2017-12-10T02:36:28.584728: step 804, loss 0.974985, acc 0.703125, prec 0.0341673, recall 0.774621
2017-12-10T02:36:28.845111: step 805, loss 0.772687, acc 0.6875, prec 0.0342194, recall 0.775047
2017-12-10T02:36:29.111777: step 806, loss 9.05053, acc 0.75, prec 0.0342383, recall 0.774528
2017-12-10T02:36:29.373783: step 807, loss 0.649362, acc 0.796875, prec 0.0342197, recall 0.774528
2017-12-10T02:36:29.635681: step 808, loss 1.06569, acc 0.640625, prec 0.0342272, recall 0.774741
2017-12-10T02:36:29.904133: step 809, loss 1.50773, acc 0.671875, prec 0.0342375, recall 0.774953
2017-12-10T02:36:30.170773: step 810, loss 1.49276, acc 0.65625, prec 0.0342463, recall 0.775165
2017-12-10T02:36:30.443681: step 811, loss 1.0944, acc 0.671875, prec 0.0342164, recall 0.775165
2017-12-10T02:36:30.702064: step 812, loss 0.954238, acc 0.765625, prec 0.0341951, recall 0.775165
2017-12-10T02:36:30.966592: step 813, loss 0.804606, acc 0.8125, prec 0.0342582, recall 0.775587
2017-12-10T02:36:31.240847: step 814, loss 0.94926, acc 0.734375, prec 0.0342341, recall 0.775587
2017-12-10T02:36:31.508274: step 815, loss 0.285048, acc 0.875, prec 0.0342628, recall 0.775797
2017-12-10T02:36:31.775733: step 816, loss 0.347622, acc 0.859375, prec 0.03429, recall 0.776007
2017-12-10T02:36:32.037734: step 817, loss 21.1239, acc 0.859375, prec 0.0343214, recall 0.774043
2017-12-10T02:36:32.309709: step 818, loss 1.77755, acc 0.78125, prec 0.0344614, recall 0.774884
2017-12-10T02:36:32.573657: step 819, loss 1.26848, acc 0.703125, prec 0.0344742, recall 0.775093
2017-12-10T02:36:32.835545: step 820, loss 1.72617, acc 0.5625, prec 0.0344344, recall 0.775093
2017-12-10T02:36:33.093093: step 821, loss 1.17242, acc 0.625, prec 0.0344799, recall 0.77551
2017-12-10T02:36:33.351518: step 822, loss 2.22159, acc 0.5, prec 0.0344742, recall 0.775718
2017-12-10T02:36:33.623971: step 823, loss 2.68325, acc 0.484375, prec 0.0345069, recall 0.776133
2017-12-10T02:36:33.886361: step 824, loss 2.78553, acc 0.5, prec 0.0344615, recall 0.776133
2017-12-10T02:36:34.146460: step 825, loss 2.9612, acc 0.453125, prec 0.0344912, recall 0.776547
2017-12-10T02:36:34.405004: step 826, loss 9.20562, acc 0.5, prec 0.034487, recall 0.776037
2017-12-10T02:36:34.666216: step 827, loss 2.19993, acc 0.453125, prec 0.0344376, recall 0.776037
2017-12-10T02:36:34.936053: step 828, loss 2.14742, acc 0.546875, prec 0.0344363, recall 0.776243
2017-12-10T02:36:35.196332: step 829, loss 3.0516, acc 0.5625, prec 0.0343983, recall 0.775529
2017-12-10T02:36:35.459484: step 830, loss 1.84804, acc 0.59375, prec 0.0343619, recall 0.775529
2017-12-10T02:36:35.724621: step 831, loss 1.64274, acc 0.59375, prec 0.0344041, recall 0.775941
2017-12-10T02:36:35.985394: step 832, loss 1.74096, acc 0.640625, prec 0.0343719, recall 0.775941
2017-12-10T02:36:36.254641: step 833, loss 1.95503, acc 0.671875, prec 0.0344211, recall 0.776352
2017-12-10T02:36:36.516300: step 834, loss 1.67055, acc 0.625, prec 0.0343876, recall 0.776352
2017-12-10T02:36:36.780893: step 835, loss 1.62148, acc 0.578125, prec 0.0343499, recall 0.776352
2017-12-10T02:36:37.041541: step 836, loss 0.889898, acc 0.75, prec 0.0343668, recall 0.776557
2017-12-10T02:36:37.309268: step 837, loss 7.84862, acc 0.828125, prec 0.034392, recall 0.776051
2017-12-10T02:36:37.568875: step 838, loss 1.73466, acc 0.703125, prec 0.0344437, recall 0.77646
2017-12-10T02:36:37.829502: step 839, loss 0.909725, acc 0.78125, prec 0.0344242, recall 0.77646
2017-12-10T02:36:38.088719: step 840, loss 0.845162, acc 0.796875, prec 0.0344061, recall 0.77646
2017-12-10T02:36:38.345237: step 841, loss 0.897274, acc 0.828125, prec 0.0344298, recall 0.776664
2017-12-10T02:36:38.608028: step 842, loss 0.907168, acc 0.8125, prec 0.0344131, recall 0.776664
2017-12-10T02:36:38.866798: step 843, loss 1.23413, acc 0.734375, prec 0.0343895, recall 0.776664
2017-12-10T02:36:39.124845: step 844, loss 1.58319, acc 0.640625, prec 0.0344355, recall 0.77707
2017-12-10T02:36:39.384898: step 845, loss 0.866364, acc 0.78125, prec 0.0344939, recall 0.777475
2017-12-10T02:36:39.649924: step 846, loss 0.646664, acc 0.828125, prec 0.0345564, recall 0.777879
2017-12-10T02:36:39.917173: step 847, loss 1.05369, acc 0.796875, prec 0.0346549, recall 0.778481
2017-12-10T02:36:40.176382: step 848, loss 4.92535, acc 0.8125, prec 0.0346395, recall 0.777778
2017-12-10T02:36:40.447624: step 849, loss 0.780389, acc 0.8125, prec 0.0346228, recall 0.777778
2017-12-10T02:36:40.719714: step 850, loss 5.54844, acc 0.8125, prec 0.0346089, recall 0.776375
2017-12-10T02:36:40.996071: step 851, loss 9.3851, acc 0.78125, prec 0.0345908, recall 0.775676
2017-12-10T02:36:41.259461: step 852, loss 6.44786, acc 0.640625, prec 0.0346378, recall 0.775382
2017-12-10T02:36:41.536819: step 853, loss 1.57046, acc 0.609375, prec 0.0346418, recall 0.775584
2017-12-10T02:36:41.800637: step 854, loss 1.98691, acc 0.578125, prec 0.0346816, recall 0.775986
2017-12-10T02:36:42.059882: step 855, loss 3.29517, acc 0.4375, prec 0.0346317, recall 0.775986
2017-12-10T02:36:42.333307: step 856, loss 2.95653, acc 0.453125, prec 0.0346218, recall 0.776186
2017-12-10T02:36:42.596174: step 857, loss 3.32238, acc 0.359375, prec 0.0346422, recall 0.776586
2017-12-10T02:36:42.856476: step 858, loss 3.71191, acc 0.40625, prec 0.0346282, recall 0.776786
2017-12-10T02:36:43.114592: step 859, loss 2.89494, acc 0.40625, prec 0.0345759, recall 0.776786
2017-12-10T02:36:43.380640: step 860, loss 3.59257, acc 0.421875, prec 0.0345252, recall 0.776786
2017-12-10T02:36:43.637624: step 861, loss 2.94364, acc 0.484375, prec 0.03448, recall 0.776786
2017-12-10T02:36:43.902511: step 862, loss 2.38656, acc 0.484375, prec 0.034435, recall 0.776786
2017-12-10T02:36:44.157097: step 863, loss 1.99498, acc 0.59375, prec 0.0344759, recall 0.777184
2017-12-10T02:36:44.414475: step 864, loss 1.9029, acc 0.609375, prec 0.0344419, recall 0.777184
2017-12-10T02:36:44.673137: step 865, loss 1.42688, acc 0.625, prec 0.0344093, recall 0.777184
2017-12-10T02:36:44.937693: step 866, loss 0.870852, acc 0.71875, prec 0.0343849, recall 0.777184
2017-12-10T02:36:45.202520: step 867, loss 0.958391, acc 0.796875, prec 0.0344053, recall 0.777382
2017-12-10T02:36:45.468990: step 868, loss 0.811179, acc 0.765625, prec 0.034423, recall 0.77758
2017-12-10T02:36:45.730025: step 869, loss 1.65136, acc 0.8125, prec 0.0344447, recall 0.777778
2017-12-10T02:36:45.989074: step 870, loss 0.490184, acc 0.90625, prec 0.0345886, recall 0.778565
2017-12-10T02:36:46.260435: step 871, loss 12.563, acc 0.84375, prec 0.0345791, recall 0.776502
2017-12-10T02:36:46.528828: step 872, loss 12.2227, acc 0.8125, prec 0.0345641, recall 0.775816
2017-12-10T02:36:46.792535: step 873, loss 0.725813, acc 0.8125, prec 0.0345478, recall 0.775816
2017-12-10T02:36:47.053438: step 874, loss 1.63949, acc 0.71875, prec 0.0345613, recall 0.776014
2017-12-10T02:36:47.319020: step 875, loss 1.90839, acc 0.609375, prec 0.0346789, recall 0.776801
2017-12-10T02:36:47.588037: step 876, loss 2.25561, acc 0.609375, prec 0.0347584, recall 0.777388
2017-12-10T02:36:47.854718: step 877, loss 2.00364, acc 0.59375, prec 0.0347608, recall 0.777583
2017-12-10T02:36:48.125820: step 878, loss 1.97535, acc 0.578125, prec 0.0347619, recall 0.777778
2017-12-10T02:36:48.383152: step 879, loss 1.94876, acc 0.578125, prec 0.0347252, recall 0.777778
2017-12-10T02:36:48.646707: step 880, loss 3.34738, acc 0.515625, prec 0.0347585, recall 0.778166
2017-12-10T02:36:48.906905: step 881, loss 2.10318, acc 0.5625, prec 0.0347582, recall 0.77836
2017-12-10T02:36:49.172745: step 882, loss 2.53988, acc 0.515625, prec 0.0347914, recall 0.778746
2017-12-10T02:36:49.435031: step 883, loss 2.67805, acc 0.5625, prec 0.0347911, recall 0.778938
2017-12-10T02:36:49.706117: step 884, loss 2.15262, acc 0.5, prec 0.0347478, recall 0.778938
2017-12-10T02:36:49.964252: step 885, loss 1.87485, acc 0.578125, prec 0.0347863, recall 0.779322
2017-12-10T02:36:50.227407: step 886, loss 1.37219, acc 0.6875, prec 0.0347594, recall 0.779322
2017-12-10T02:36:50.490337: step 887, loss 1.28249, acc 0.71875, prec 0.0347351, recall 0.779322
2017-12-10T02:36:50.752000: step 888, loss 1.52989, acc 0.640625, prec 0.0347789, recall 0.779705
2017-12-10T02:36:51.012797: step 889, loss 0.782797, acc 0.75, prec 0.0347574, recall 0.779705
2017-12-10T02:36:51.283017: step 890, loss 10.9764, acc 0.734375, prec 0.0347732, recall 0.779221
2017-12-10T02:36:51.547861: step 891, loss 0.769096, acc 0.796875, prec 0.034793, recall 0.779412
2017-12-10T02:36:51.810592: step 892, loss 1.13276, acc 0.828125, prec 0.0348528, recall 0.779793
2017-12-10T02:36:52.073996: step 893, loss 0.872814, acc 0.796875, prec 0.0348725, recall 0.779983
2017-12-10T02:36:52.331680: step 894, loss 0.414161, acc 0.8125, prec 0.034968, recall 0.780551
2017-12-10T02:36:52.594131: step 895, loss 0.784645, acc 0.796875, prec 0.0349877, recall 0.780739
2017-12-10T02:36:52.854551: step 896, loss 0.805909, acc 0.84375, prec 0.0350114, recall 0.780928
2017-12-10T02:36:53.117053: step 897, loss 0.578176, acc 0.84375, prec 0.0351093, recall 0.781491
2017-12-10T02:36:53.385748: step 898, loss 0.607785, acc 0.828125, prec 0.0351316, recall 0.781678
2017-12-10T02:36:53.651606: step 899, loss 0.553292, acc 0.796875, prec 0.035114, recall 0.781678
2017-12-10T02:36:53.914447: step 900, loss 0.354463, acc 0.875, prec 0.0351774, recall 0.782051

Evaluation:
2017-12-10T02:37:01.569055: step 900, loss 2.45852, acc 0.894424, prec 0.0374991, recall 0.763868

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-900

2017-12-10T02:37:02.821464: step 901, loss 0.735834, acc 0.8125, prec 0.0374825, recall 0.763868
2017-12-10T02:37:03.078934: step 902, loss 4.09961, acc 0.796875, prec 0.037466, recall 0.763296
2017-12-10T02:37:03.347946: step 903, loss 0.47454, acc 0.90625, prec 0.0374577, recall 0.763296
2017-12-10T02:37:03.604626: step 904, loss 0.289383, acc 0.90625, prec 0.0374848, recall 0.763473
2017-12-10T02:37:03.864658: step 905, loss 4.20211, acc 0.875, prec 0.0374752, recall 0.762902
2017-12-10T02:37:04.131659: step 906, loss 0.499638, acc 0.859375, prec 0.0375335, recall 0.763256
2017-12-10T02:37:04.393480: step 907, loss 0.448599, acc 0.859375, prec 0.0375211, recall 0.763256
2017-12-10T02:37:04.655807: step 908, loss 0.489165, acc 0.859375, prec 0.0375087, recall 0.763256
2017-12-10T02:37:04.928962: step 909, loss 7.61327, acc 0.828125, prec 0.037495, recall 0.762687
2017-12-10T02:37:05.194234: step 910, loss 0.644011, acc 0.8125, prec 0.0375138, recall 0.762864
2017-12-10T02:37:05.455166: step 911, loss 0.317084, acc 0.859375, prec 0.0375014, recall 0.762864
2017-12-10T02:37:05.719669: step 912, loss 0.844674, acc 0.828125, prec 0.0375215, recall 0.76304
2017-12-10T02:37:05.981375: step 913, loss 1.06884, acc 0.734375, prec 0.0375334, recall 0.763217
2017-12-10T02:37:06.239851: step 914, loss 1.44355, acc 0.6875, prec 0.0375059, recall 0.763217
2017-12-10T02:37:06.501696: step 915, loss 1.11128, acc 0.75, prec 0.037484, recall 0.763217
2017-12-10T02:37:06.762917: step 916, loss 1.46265, acc 0.65625, prec 0.0374539, recall 0.763217
2017-12-10T02:37:07.023016: step 917, loss 2.51095, acc 0.828125, prec 0.0374753, recall 0.762825
2017-12-10T02:37:07.289361: step 918, loss 1.51215, acc 0.765625, prec 0.03749, recall 0.763002
2017-12-10T02:37:07.554681: step 919, loss 0.747989, acc 0.828125, prec 0.0374749, recall 0.763002
2017-12-10T02:37:07.821446: step 920, loss 1.24302, acc 0.6875, prec 0.0374827, recall 0.763177
2017-12-10T02:37:08.086683: step 921, loss 0.555389, acc 0.875, prec 0.0374717, recall 0.763177
2017-12-10T02:37:08.351227: step 922, loss 1.21729, acc 0.75, prec 0.0374499, recall 0.763177
2017-12-10T02:37:08.613472: step 923, loss 1.45083, acc 0.703125, prec 0.037424, recall 0.763177
2017-12-10T02:37:08.874075: step 924, loss 0.900337, acc 0.734375, prec 0.0374009, recall 0.763177
2017-12-10T02:37:09.142167: step 925, loss 0.750013, acc 0.734375, prec 0.0373777, recall 0.763177
2017-12-10T02:37:09.403946: step 926, loss 1.12643, acc 0.765625, prec 0.0373574, recall 0.763177
2017-12-10T02:37:09.670697: step 927, loss 0.714969, acc 0.828125, prec 0.0373774, recall 0.763353
2017-12-10T02:37:09.931549: step 928, loss 0.773807, acc 0.890625, prec 0.0374728, recall 0.763879
2017-12-10T02:37:10.193807: step 929, loss 0.876878, acc 0.78125, prec 0.0374887, recall 0.764053
2017-12-10T02:37:10.453948: step 930, loss 0.585822, acc 0.796875, prec 0.0375059, recall 0.764228
2017-12-10T02:37:10.724506: step 931, loss 0.441711, acc 0.875, prec 0.037495, recall 0.764228
2017-12-10T02:37:10.987743: step 932, loss 0.730416, acc 0.859375, prec 0.0375177, recall 0.764402
2017-12-10T02:37:11.250777: step 933, loss 0.270627, acc 0.875, prec 0.0375068, recall 0.764402
2017-12-10T02:37:11.525564: step 934, loss 5.05878, acc 0.828125, prec 0.0375281, recall 0.764012
2017-12-10T02:37:11.789262: step 935, loss 2.848, acc 0.859375, prec 0.0375172, recall 0.763449
2017-12-10T02:37:12.056387: step 936, loss 0.47678, acc 0.859375, prec 0.037505, recall 0.763449
2017-12-10T02:37:12.323107: step 937, loss 0.477263, acc 0.8125, prec 0.0374887, recall 0.763449
2017-12-10T02:37:12.590120: step 938, loss 9.64485, acc 0.84375, prec 0.0375113, recall 0.763061
2017-12-10T02:37:12.855362: step 939, loss 0.591215, acc 0.78125, prec 0.0375271, recall 0.763235
2017-12-10T02:37:13.118665: step 940, loss 0.65869, acc 0.796875, prec 0.0375095, recall 0.763235
2017-12-10T02:37:13.387627: step 941, loss 0.945249, acc 0.78125, prec 0.0374905, recall 0.763235
2017-12-10T02:37:13.647392: step 942, loss 1.26781, acc 0.703125, prec 0.0374648, recall 0.763235
2017-12-10T02:37:13.908040: step 943, loss 1.52159, acc 0.75, prec 0.0374432, recall 0.763235
2017-12-10T02:37:14.168448: step 944, loss 1.02339, acc 0.75, prec 0.0374563, recall 0.763409
2017-12-10T02:37:14.425989: step 945, loss 1.43283, acc 0.640625, prec 0.0374946, recall 0.763756
2017-12-10T02:37:14.700207: step 946, loss 1.1504, acc 0.78125, prec 0.0375104, recall 0.76393
2017-12-10T02:37:14.960321: step 947, loss 1.30015, acc 0.671875, prec 0.037482, recall 0.76393
2017-12-10T02:37:15.219439: step 948, loss 1.55767, acc 0.734375, prec 0.0374937, recall 0.764103
2017-12-10T02:37:15.481505: step 949, loss 4.91154, acc 0.6875, prec 0.0375027, recall 0.763716
2017-12-10T02:37:15.743683: step 950, loss 1.54051, acc 0.734375, prec 0.0374798, recall 0.763716
2017-12-10T02:37:16.011877: step 951, loss 1.16923, acc 0.75, prec 0.0374928, recall 0.763889
2017-12-10T02:37:16.270852: step 952, loss 1.47365, acc 0.609375, prec 0.0374592, recall 0.763889
2017-12-10T02:37:16.535628: step 953, loss 1.14477, acc 0.65625, prec 0.0374642, recall 0.764061
2017-12-10T02:37:16.801623: step 954, loss 1.00108, acc 0.71875, prec 0.0374745, recall 0.764234
2017-12-10T02:37:17.063691: step 955, loss 1.07394, acc 0.609375, prec 0.037441, recall 0.764234
2017-12-10T02:37:17.334401: step 956, loss 0.880537, acc 0.703125, prec 0.0374156, recall 0.764234
2017-12-10T02:37:17.595353: step 957, loss 1.64963, acc 0.671875, prec 0.0373875, recall 0.764234
2017-12-10T02:37:17.853310: step 958, loss 1.18823, acc 0.71875, prec 0.0373979, recall 0.764406
2017-12-10T02:37:18.118344: step 959, loss 1.01571, acc 0.734375, prec 0.0373752, recall 0.764406
2017-12-10T02:37:18.382557: step 960, loss 0.617911, acc 0.828125, prec 0.0374292, recall 0.764749
2017-12-10T02:37:18.642954: step 961, loss 0.980008, acc 0.78125, prec 0.0374105, recall 0.764749
2017-12-10T02:37:18.903964: step 962, loss 0.380693, acc 0.84375, prec 0.0373972, recall 0.764749
2017-12-10T02:37:19.164582: step 963, loss 0.280126, acc 0.9375, prec 0.0374261, recall 0.76492
2017-12-10T02:37:19.428253: step 964, loss 5.93911, acc 0.921875, prec 0.0374208, recall 0.764364
2017-12-10T02:37:19.694880: step 965, loss 0.261405, acc 0.953125, prec 0.0374511, recall 0.764535
2017-12-10T02:37:19.953723: step 966, loss 0.382305, acc 0.890625, prec 0.037476, recall 0.764706
2017-12-10T02:37:20.217995: step 967, loss 0.262745, acc 0.9375, prec 0.0375734, recall 0.765217
2017-12-10T02:37:20.479044: step 968, loss 3.04843, acc 0.890625, prec 0.0375996, recall 0.764834
2017-12-10T02:37:20.749087: step 969, loss 12.7271, acc 0.9375, prec 0.0375969, recall 0.763728
2017-12-10T02:37:21.010727: step 970, loss 2.16893, acc 0.828125, prec 0.037652, recall 0.763518
2017-12-10T02:37:21.276517: step 971, loss 1.03505, acc 0.671875, prec 0.0376239, recall 0.763518
2017-12-10T02:37:21.535401: step 972, loss 1.136, acc 0.734375, prec 0.0376012, recall 0.763518
2017-12-10T02:37:21.805173: step 973, loss 0.824485, acc 0.78125, prec 0.0375825, recall 0.763518
2017-12-10T02:37:22.063194: step 974, loss 2.14577, acc 0.546875, prec 0.037578, recall 0.763689
2017-12-10T02:37:22.321131: step 975, loss 2.12957, acc 0.59375, prec 0.0376116, recall 0.764029
2017-12-10T02:37:22.581500: step 976, loss 2.69185, acc 0.5625, prec 0.0375743, recall 0.764029
2017-12-10T02:37:22.850236: step 977, loss 3.2471, acc 0.34375, prec 0.0375185, recall 0.764029
2017-12-10T02:37:23.112008: step 978, loss 2.82742, acc 0.453125, prec 0.0374722, recall 0.764029
2017-12-10T02:37:23.370972: step 979, loss 2.79725, acc 0.53125, prec 0.0374326, recall 0.764029
2017-12-10T02:37:23.629436: step 980, loss 2.75224, acc 0.5, prec 0.0373904, recall 0.764029
2017-12-10T02:37:23.889795: step 981, loss 1.79071, acc 0.59375, prec 0.0373901, recall 0.764198
2017-12-10T02:37:24.152976: step 982, loss 2.81243, acc 0.5, prec 0.0373819, recall 0.764368
2017-12-10T02:37:24.416339: step 983, loss 1.52637, acc 0.65625, prec 0.0373868, recall 0.764537
2017-12-10T02:37:24.674388: step 984, loss 1.49646, acc 0.65625, prec 0.0373579, recall 0.764537
2017-12-10T02:37:24.932225: step 985, loss 0.817495, acc 0.796875, prec 0.0373409, recall 0.764537
2017-12-10T02:37:25.201069: step 986, loss 0.868031, acc 0.671875, prec 0.0373809, recall 0.764875
2017-12-10T02:37:25.464286: step 987, loss 0.735931, acc 0.765625, prec 0.0373613, recall 0.764875
2017-12-10T02:37:25.734379: step 988, loss 1.20881, acc 0.84375, prec 0.0374493, recall 0.765379
2017-12-10T02:37:25.999141: step 989, loss 0.229654, acc 0.9375, prec 0.037444, recall 0.765379
2017-12-10T02:37:26.274623: step 990, loss 0.563572, acc 0.859375, prec 0.0374659, recall 0.765547
2017-12-10T02:37:26.548499: step 991, loss 0.346338, acc 0.9375, prec 0.0374607, recall 0.765547
2017-12-10T02:37:26.816616: step 992, loss 0.103463, acc 0.953125, prec 0.0374567, recall 0.765547
2017-12-10T02:37:27.076640: step 993, loss 0.19714, acc 0.96875, prec 0.0374541, recall 0.765547
2017-12-10T02:37:27.300166: step 994, loss 10.0104, acc 0.921569, prec 0.0374502, recall 0.765
2017-12-10T02:37:27.583174: step 995, loss 0.211103, acc 0.9375, prec 0.0374449, recall 0.765
2017-12-10T02:37:27.844276: step 996, loss 0.23016, acc 0.921875, prec 0.0374384, recall 0.765
2017-12-10T02:37:28.102282: step 997, loss 0.262497, acc 0.90625, prec 0.0374305, recall 0.765
2017-12-10T02:37:28.362216: step 998, loss 0.278634, acc 0.890625, prec 0.0374214, recall 0.765
2017-12-10T02:37:28.625730: step 999, loss 0.312661, acc 0.9375, prec 0.0374162, recall 0.765
2017-12-10T02:37:28.889625: step 1000, loss 0.487178, acc 0.875, prec 0.0374057, recall 0.765
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-1000

2017-12-10T02:37:30.168214: step 1001, loss 0.214696, acc 0.9375, prec 0.0374341, recall 0.765168
2017-12-10T02:37:30.430549: step 1002, loss 0.240314, acc 0.9375, prec 0.0374625, recall 0.765335
2017-12-10T02:37:30.691956: step 1003, loss 0.193378, acc 0.90625, prec 0.0374546, recall 0.765335
2017-12-10T02:37:30.952136: step 1004, loss 0.126563, acc 0.96875, prec 0.037452, recall 0.765335
2017-12-10T02:37:31.229837: step 1005, loss 0.257144, acc 0.9375, prec 0.0374468, recall 0.765335
2017-12-10T02:37:31.492145: step 1006, loss 0.139629, acc 0.953125, prec 0.0374429, recall 0.765335
2017-12-10T02:37:31.751384: step 1007, loss 0.160899, acc 0.9375, prec 0.0374376, recall 0.765335
2017-12-10T02:37:32.013345: step 1008, loss 3.17239, acc 0.953125, prec 0.037435, recall 0.76479
2017-12-10T02:37:32.996465: step 1009, loss 4.48197, acc 0.90625, prec 0.0374285, recall 0.764245
2017-12-10T02:37:33.368814: step 1010, loss 0.165596, acc 0.890625, prec 0.0374194, recall 0.764245
2017-12-10T02:37:34.140754: step 1011, loss 1.73962, acc 0.890625, prec 0.0374451, recall 0.763869
2017-12-10T02:37:34.422007: step 1012, loss 5.17151, acc 0.828125, prec 0.0374669, recall 0.762952
2017-12-10T02:37:34.708845: step 1013, loss 0.417247, acc 0.859375, prec 0.0374551, recall 0.762952
2017-12-10T02:37:34.967767: step 1014, loss 0.861966, acc 0.765625, prec 0.0375026, recall 0.763288
2017-12-10T02:37:35.229197: step 1015, loss 1.7007, acc 0.625, prec 0.0374713, recall 0.763288
2017-12-10T02:37:35.497320: step 1016, loss 2.39876, acc 0.453125, prec 0.0374257, recall 0.763288
2017-12-10T02:37:35.767143: step 1017, loss 1.28273, acc 0.625, prec 0.037428, recall 0.763456
2017-12-10T02:37:36.026532: step 1018, loss 1.95223, acc 0.5625, prec 0.037425, recall 0.763623
2017-12-10T02:37:36.291759: step 1019, loss 2.98184, acc 0.5, prec 0.0374169, recall 0.763791
2017-12-10T02:37:36.547790: step 1020, loss 2.29063, acc 0.5625, prec 0.0373806, recall 0.763791
2017-12-10T02:37:36.808122: step 1021, loss 3.63006, acc 0.3125, prec 0.037357, recall 0.763958
2017-12-10T02:37:37.064541: step 1022, loss 2.39449, acc 0.53125, prec 0.0373516, recall 0.764124
2017-12-10T02:37:37.319995: step 1023, loss 1.9848, acc 0.5625, prec 0.0373819, recall 0.764457
2017-12-10T02:37:37.579504: step 1024, loss 1.67847, acc 0.5625, prec 0.0373458, recall 0.764457
2017-12-10T02:37:37.845262: step 1025, loss 2.03565, acc 0.625, prec 0.037315, recall 0.764457
2017-12-10T02:37:38.104440: step 1026, loss 1.55421, acc 0.71875, prec 0.0372919, recall 0.764457
2017-12-10T02:37:38.366420: step 1027, loss 1.87192, acc 0.609375, prec 0.0372929, recall 0.764623
2017-12-10T02:37:38.624831: step 1028, loss 1.41338, acc 0.703125, prec 0.0373347, recall 0.764954
2017-12-10T02:37:38.888134: step 1029, loss 1.08532, acc 0.796875, prec 0.0373511, recall 0.76512
2017-12-10T02:37:39.150712: step 1030, loss 0.650542, acc 0.828125, prec 0.0374361, recall 0.765614
2017-12-10T02:37:39.419751: step 1031, loss 0.404392, acc 0.859375, prec 0.0374245, recall 0.765614
2017-12-10T02:37:39.679789: step 1032, loss 0.946854, acc 0.90625, prec 0.0374829, recall 0.765943
2017-12-10T02:37:39.941168: step 1033, loss 0.57758, acc 0.859375, prec 0.0374713, recall 0.765943
2017-12-10T02:37:40.203918: step 1034, loss 0.187145, acc 0.9375, prec 0.0374661, recall 0.765943
2017-12-10T02:37:40.466942: step 1035, loss 0.140046, acc 0.953125, prec 0.0374623, recall 0.765943
2017-12-10T02:37:40.731487: step 1036, loss 0.261079, acc 0.9375, prec 0.0374901, recall 0.766106
2017-12-10T02:37:40.994040: step 1037, loss 0.183783, acc 0.90625, prec 0.0374824, recall 0.766106
2017-12-10T02:37:41.259668: step 1038, loss 5.12445, acc 0.921875, prec 0.0374773, recall 0.76557
2017-12-10T02:37:41.537150: step 1039, loss 0.147266, acc 0.96875, prec 0.0374747, recall 0.76557
2017-12-10T02:37:41.797732: step 1040, loss 6.47258, acc 0.921875, prec 0.0374696, recall 0.765035
2017-12-10T02:37:42.063238: step 1041, loss 0.94612, acc 0.9375, prec 0.0375304, recall 0.765363
2017-12-10T02:37:42.327958: step 1042, loss 0.190948, acc 0.890625, prec 0.0375543, recall 0.765527
2017-12-10T02:37:42.595291: step 1043, loss 0.696729, acc 0.890625, prec 0.0375783, recall 0.76569
2017-12-10T02:37:42.864689: step 1044, loss 0.523787, acc 0.875, prec 0.037568, recall 0.76569
2017-12-10T02:37:43.132566: step 1045, loss 0.594847, acc 0.875, prec 0.0375906, recall 0.765854
2017-12-10T02:37:43.391577: step 1046, loss 0.535141, acc 0.796875, prec 0.0375739, recall 0.765854
2017-12-10T02:37:43.655067: step 1047, loss 0.562261, acc 0.796875, prec 0.0376559, recall 0.766342
2017-12-10T02:37:43.917512: step 1048, loss 1.04005, acc 0.765625, prec 0.0376695, recall 0.766505
2017-12-10T02:37:44.184998: step 1049, loss 0.920351, acc 0.8125, prec 0.0376869, recall 0.766667
2017-12-10T02:37:44.445545: step 1050, loss 0.999581, acc 0.796875, prec 0.0376702, recall 0.766667
2017-12-10T02:37:44.713580: step 1051, loss 0.613226, acc 0.796875, prec 0.0376535, recall 0.766667
2017-12-10T02:37:44.974069: step 1052, loss 0.630057, acc 0.859375, prec 0.0376747, recall 0.766829
2017-12-10T02:37:45.234678: step 1053, loss 0.965025, acc 0.8125, prec 0.0376593, recall 0.766829
2017-12-10T02:37:45.496981: step 1054, loss 4.60121, acc 0.875, prec 0.0376831, recall 0.766459
2017-12-10T02:37:45.762007: step 1055, loss 1.4738, acc 0.828125, prec 0.0377018, recall 0.766621
2017-12-10T02:37:46.027484: step 1056, loss 1.83936, acc 0.765625, prec 0.0377493, recall 0.766413
2017-12-10T02:37:46.290333: step 1057, loss 0.704719, acc 0.84375, prec 0.0377692, recall 0.766575
2017-12-10T02:37:46.550736: step 1058, loss 0.641832, acc 0.828125, prec 0.0377878, recall 0.766736
2017-12-10T02:37:46.815706: step 1059, loss 0.963108, acc 0.6875, prec 0.0377621, recall 0.766736
2017-12-10T02:37:47.087297: step 1060, loss 1.15239, acc 0.796875, prec 0.0377455, recall 0.766736
2017-12-10T02:37:47.354964: step 1061, loss 0.947122, acc 0.765625, prec 0.0377262, recall 0.766736
2017-12-10T02:37:47.616534: step 1062, loss 1.16176, acc 0.765625, prec 0.0377723, recall 0.767057
2017-12-10T02:37:47.880003: step 1063, loss 0.993927, acc 0.75, prec 0.0377518, recall 0.767057
2017-12-10T02:37:48.141486: step 1064, loss 1.25509, acc 0.6875, prec 0.0377589, recall 0.767218
2017-12-10T02:37:48.405182: step 1065, loss 1.29514, acc 0.671875, prec 0.037732, recall 0.767218
2017-12-10T02:37:48.660967: step 1066, loss 0.737532, acc 0.8125, prec 0.0377493, recall 0.767378
2017-12-10T02:37:48.929846: step 1067, loss 0.838321, acc 0.75, prec 0.0377614, recall 0.767538
2017-12-10T02:37:49.190263: step 1068, loss 0.727315, acc 0.859375, prec 0.037815, recall 0.767857
2017-12-10T02:37:49.456757: step 1069, loss 0.551991, acc 0.828125, prec 0.0378009, recall 0.767857
2017-12-10T02:37:49.719844: step 1070, loss 0.718154, acc 0.78125, prec 0.0378156, recall 0.768016
2017-12-10T02:37:49.980968: step 1071, loss 0.740172, acc 0.859375, prec 0.0378366, recall 0.768176
2017-12-10T02:37:50.244687: step 1072, loss 0.284935, acc 0.890625, prec 0.0378601, recall 0.768334
2017-12-10T02:37:50.509583: step 1073, loss 1.04145, acc 0.953125, prec 0.0379537, recall 0.76881
2017-12-10T02:37:50.769233: step 1074, loss 0.149375, acc 0.953125, prec 0.0379499, recall 0.76881
2017-12-10T02:37:51.031514: step 1075, loss 0.408878, acc 0.875, prec 0.0379396, recall 0.76881
2017-12-10T02:37:51.297321: step 1076, loss 3.54232, acc 0.828125, prec 0.0379281, recall 0.76776
2017-12-10T02:37:51.564154: step 1077, loss 0.35307, acc 0.875, prec 0.0379828, recall 0.768076
2017-12-10T02:37:51.822602: step 1078, loss 0.330844, acc 0.890625, prec 0.0380063, recall 0.768234
2017-12-10T02:37:52.091248: step 1079, loss 0.86906, acc 0.796875, prec 0.0380545, recall 0.76855
2017-12-10T02:37:52.357257: step 1080, loss 0.129121, acc 0.921875, prec 0.0380805, recall 0.768707
2017-12-10T02:37:52.618784: step 1081, loss 1.8144, acc 0.8125, prec 0.0380975, recall 0.768865
2017-12-10T02:37:52.881472: step 1082, loss 0.327319, acc 0.859375, prec 0.0380859, recall 0.768865
2017-12-10T02:37:53.140075: step 1083, loss 0.724435, acc 0.84375, prec 0.0380731, recall 0.768865
2017-12-10T02:37:53.403375: step 1084, loss 0.864338, acc 0.6875, prec 0.0380799, recall 0.769022
2017-12-10T02:37:53.667812: step 1085, loss 0.409622, acc 0.875, prec 0.0380696, recall 0.769022
2017-12-10T02:37:53.939729: step 1086, loss 0.419007, acc 0.859375, prec 0.0380904, recall 0.769179
2017-12-10T02:37:54.204640: step 1087, loss 0.532387, acc 0.859375, prec 0.0381436, recall 0.769492
2017-12-10T02:37:54.463433: step 1088, loss 0.704868, acc 0.828125, prec 0.0381941, recall 0.769804
2017-12-10T02:37:54.727065: step 1089, loss 0.849169, acc 0.8125, prec 0.038211, recall 0.769959
2017-12-10T02:37:54.988143: step 1090, loss 0.484538, acc 0.828125, prec 0.0382615, recall 0.77027
2017-12-10T02:37:55.246870: step 1091, loss 0.984891, acc 0.78125, prec 0.0382757, recall 0.770425
2017-12-10T02:37:55.517793: step 1092, loss 5.98893, acc 0.875, prec 0.0383313, recall 0.770216
2017-12-10T02:37:55.783731: step 1093, loss 0.338789, acc 0.9375, prec 0.0383584, recall 0.77037
2017-12-10T02:37:56.046573: step 1094, loss 0.805341, acc 0.875, prec 0.0384448, recall 0.770833
2017-12-10T02:37:56.317821: step 1095, loss 0.372143, acc 0.890625, prec 0.0385002, recall 0.771141
2017-12-10T02:37:56.582392: step 1096, loss 0.522269, acc 0.875, prec 0.0384899, recall 0.771141
2017-12-10T02:37:56.841836: step 1097, loss 0.471807, acc 0.875, prec 0.0384796, recall 0.771141
2017-12-10T02:37:57.107332: step 1098, loss 0.399876, acc 0.859375, prec 0.0385324, recall 0.771448
2017-12-10T02:37:57.366076: step 1099, loss 0.48633, acc 0.828125, prec 0.0385182, recall 0.771448
2017-12-10T02:37:57.622805: step 1100, loss 1.22341, acc 0.75, prec 0.038594, recall 0.771906
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-1100

2017-12-10T02:37:58.992723: step 1101, loss 0.474197, acc 0.828125, prec 0.038612, recall 0.772059
2017-12-10T02:37:59.251850: step 1102, loss 0.579048, acc 0.84375, prec 0.0386312, recall 0.772211
2017-12-10T02:37:59.511171: step 1103, loss 0.398226, acc 0.859375, prec 0.0386196, recall 0.772211
2017-12-10T02:37:59.780806: step 1104, loss 0.49652, acc 0.765625, prec 0.0386002, recall 0.772211
2017-12-10T02:38:00.048185: step 1105, loss 0.741916, acc 0.828125, prec 0.0386502, recall 0.772515
2017-12-10T02:38:00.325491: step 1106, loss 0.511868, acc 0.859375, prec 0.0387028, recall 0.772818
2017-12-10T02:38:00.584917: step 1107, loss 0.203451, acc 0.953125, prec 0.038731, recall 0.772969
2017-12-10T02:38:00.847244: step 1108, loss 0.5115, acc 0.84375, prec 0.0387181, recall 0.772969
2017-12-10T02:38:01.118600: step 1109, loss 0.206699, acc 0.875, prec 0.0387398, recall 0.77312
2017-12-10T02:38:01.385425: step 1110, loss 7.75255, acc 0.890625, prec 0.0387654, recall 0.772244
2017-12-10T02:38:01.646294: step 1111, loss 0.327341, acc 0.859375, prec 0.0387537, recall 0.772244
2017-12-10T02:38:01.906336: step 1112, loss 1.35351, acc 0.859375, prec 0.0388062, recall 0.772546
2017-12-10T02:38:02.168974: step 1113, loss 0.5657, acc 0.796875, prec 0.0387894, recall 0.772546
2017-12-10T02:38:02.429183: step 1114, loss 0.826644, acc 0.796875, prec 0.0388046, recall 0.772697
2017-12-10T02:38:02.688405: step 1115, loss 8.72427, acc 0.796875, prec 0.0388211, recall 0.772336
2017-12-10T02:38:02.951853: step 1116, loss 0.91223, acc 0.671875, prec 0.038794, recall 0.772336
2017-12-10T02:38:03.217623: step 1117, loss 1.52032, acc 0.609375, prec 0.0387617, recall 0.772336
2017-12-10T02:38:03.481584: step 1118, loss 1.01994, acc 0.65625, prec 0.0387334, recall 0.772336
2017-12-10T02:38:03.739765: step 1119, loss 1.26976, acc 0.640625, prec 0.0387358, recall 0.772487
2017-12-10T02:38:04.002229: step 1120, loss 1.50087, acc 0.609375, prec 0.0387037, recall 0.772487
2017-12-10T02:38:04.259133: step 1121, loss 1.11956, acc 0.71875, prec 0.0386806, recall 0.772487
2017-12-10T02:38:04.523823: step 1122, loss 1.11258, acc 0.671875, prec 0.0387174, recall 0.772787
2017-12-10T02:38:04.791000: step 1123, loss 1.34942, acc 0.640625, prec 0.0387197, recall 0.772937
2017-12-10T02:38:05.049926: step 1124, loss 1.27946, acc 0.65625, prec 0.0387233, recall 0.773087
2017-12-10T02:38:05.308961: step 1125, loss 0.981842, acc 0.734375, prec 0.0387333, recall 0.773237
2017-12-10T02:38:05.568050: step 1126, loss 1.2696, acc 0.71875, prec 0.038742, recall 0.773386
2017-12-10T02:38:05.830818: step 1127, loss 0.855875, acc 0.75, prec 0.0387533, recall 0.773535
2017-12-10T02:38:06.089488: step 1128, loss 0.543766, acc 0.796875, prec 0.0388001, recall 0.773833
2017-12-10T02:38:06.353641: step 1129, loss 0.596258, acc 0.78125, prec 0.0387822, recall 0.773833
2017-12-10T02:38:06.623080: step 1130, loss 0.345401, acc 0.890625, prec 0.0387732, recall 0.773833
2017-12-10T02:38:06.887411: step 1131, loss 0.269597, acc 0.921875, prec 0.0387668, recall 0.773833
2017-12-10T02:38:07.152801: step 1132, loss 0.391759, acc 0.84375, prec 0.0387541, recall 0.773833
2017-12-10T02:38:07.420565: step 1133, loss 0.907086, acc 0.875, prec 0.0388071, recall 0.77413
2017-12-10T02:38:07.685374: step 1134, loss 3.36738, acc 0.921875, prec 0.0388653, recall 0.773919
2017-12-10T02:38:07.947002: step 1135, loss 1.53619, acc 0.9375, prec 0.0388615, recall 0.773412
2017-12-10T02:38:08.209795: step 1136, loss 0.497756, acc 0.84375, prec 0.0388803, recall 0.77356
2017-12-10T02:38:08.470672: step 1137, loss 0.284632, acc 0.921875, prec 0.0389055, recall 0.773708
2017-12-10T02:38:08.736947: step 1138, loss 0.238198, acc 0.90625, prec 0.0388978, recall 0.773708
2017-12-10T02:38:09.003896: step 1139, loss 0.116167, acc 0.96875, prec 0.0388953, recall 0.773708
2017-12-10T02:38:09.272852: step 1140, loss 0.213853, acc 0.921875, prec 0.0389205, recall 0.773856
2017-12-10T02:38:09.536723: step 1141, loss 1.38182, acc 0.90625, prec 0.0389444, recall 0.774004
2017-12-10T02:38:09.801413: step 1142, loss 0.435582, acc 0.953125, prec 0.0389721, recall 0.774151
2017-12-10T02:38:10.065640: step 1143, loss 0.301158, acc 0.890625, prec 0.0389947, recall 0.774299
2017-12-10T02:38:10.340685: step 1144, loss 0.364322, acc 0.90625, prec 0.0390502, recall 0.774593
2017-12-10T02:38:10.605596: step 1145, loss 0.245346, acc 0.875, prec 0.0390399, recall 0.774593
2017-12-10T02:38:10.864511: step 1146, loss 0.531194, acc 0.859375, prec 0.0390599, recall 0.77474
2017-12-10T02:38:11.133269: step 1147, loss 0.189942, acc 0.9375, prec 0.0390863, recall 0.774886
2017-12-10T02:38:11.413261: step 1148, loss 0.266882, acc 0.859375, prec 0.0391063, recall 0.775033
2017-12-10T02:38:11.676178: step 1149, loss 0.383845, acc 0.84375, prec 0.039125, recall 0.775179
2017-12-10T02:38:11.938023: step 1150, loss 0.339615, acc 0.890625, prec 0.0391475, recall 0.775325
2017-12-10T02:38:12.196028: step 1151, loss 0.318376, acc 0.890625, prec 0.0391386, recall 0.775325
2017-12-10T02:38:12.456096: step 1152, loss 0.517792, acc 0.875, prec 0.0391283, recall 0.775325
2017-12-10T02:38:12.719235: step 1153, loss 0.429234, acc 0.875, prec 0.0391495, recall 0.77547
2017-12-10T02:38:12.979702: step 1154, loss 0.332316, acc 0.90625, prec 0.0391418, recall 0.77547
2017-12-10T02:38:13.246556: step 1155, loss 0.567794, acc 0.859375, prec 0.0391618, recall 0.775616
2017-12-10T02:38:13.518893: step 1156, loss 0.117157, acc 0.9375, prec 0.0391881, recall 0.775761
2017-12-10T02:38:13.783594: step 1157, loss 0.148791, acc 0.953125, prec 0.0392471, recall 0.776052
2017-12-10T02:38:14.052760: step 1158, loss 0.827775, acc 0.984375, prec 0.0393087, recall 0.776341
2017-12-10T02:38:14.319279: step 1159, loss 0.127193, acc 0.9375, prec 0.0393036, recall 0.776341
2017-12-10T02:38:14.581498: step 1160, loss 0.120113, acc 0.96875, prec 0.039301, recall 0.776341
2017-12-10T02:38:14.847555: step 1161, loss 0.183236, acc 0.953125, prec 0.0393286, recall 0.776486
2017-12-10T02:38:15.117327: step 1162, loss 0.225558, acc 0.921875, prec 0.0393222, recall 0.776486
2017-12-10T02:38:15.376645: step 1163, loss 0.941942, acc 0.90625, prec 0.0393773, recall 0.776774
2017-12-10T02:38:15.647097: step 1164, loss 0.0439963, acc 0.984375, prec 0.039376, recall 0.776774
2017-12-10T02:38:15.909250: step 1165, loss 0.208257, acc 0.90625, prec 0.0393997, recall 0.776918
2017-12-10T02:38:16.178755: step 1166, loss 0.217666, acc 0.9375, prec 0.0393945, recall 0.776918
2017-12-10T02:38:16.445015: step 1167, loss 0.227409, acc 0.90625, prec 0.0394182, recall 0.777062
2017-12-10T02:38:16.704617: step 1168, loss 2.88528, acc 0.90625, prec 0.0394118, recall 0.776561
2017-12-10T02:38:16.965869: step 1169, loss 0.70002, acc 0.953125, prec 0.0395021, recall 0.776992
2017-12-10T02:38:17.232755: step 1170, loss 0.359932, acc 0.875, prec 0.0394917, recall 0.776992
2017-12-10T02:38:17.499269: step 1171, loss 3.2531, acc 0.90625, prec 0.0395167, recall 0.776637
2017-12-10T02:38:17.771504: step 1172, loss 0.270939, acc 0.921875, prec 0.0395102, recall 0.776637
2017-12-10T02:38:18.037263: step 1173, loss 0.794774, acc 0.75, prec 0.0394896, recall 0.776637
2017-12-10T02:38:18.297737: step 1174, loss 0.601308, acc 0.859375, prec 0.039478, recall 0.776637
2017-12-10T02:38:18.556779: step 1175, loss 1.1407, acc 0.6875, prec 0.0394836, recall 0.77678
2017-12-10T02:38:18.825232: step 1176, loss 0.850525, acc 0.75, prec 0.0394943, recall 0.776923
2017-12-10T02:38:19.090765: step 1177, loss 0.635434, acc 0.78125, prec 0.0395388, recall 0.777209
2017-12-10T02:38:19.348280: step 1178, loss 0.928619, acc 0.734375, prec 0.0395482, recall 0.777351
2017-12-10T02:38:19.616238: step 1179, loss 0.647288, acc 0.796875, prec 0.0395315, recall 0.777351
2017-12-10T02:38:19.878450: step 1180, loss 0.884432, acc 0.75, prec 0.0395421, recall 0.777494
2017-12-10T02:38:20.144182: step 1181, loss 0.739024, acc 0.78125, prec 0.0395554, recall 0.777636
2017-12-10T02:38:20.414387: step 1182, loss 0.632465, acc 0.875, prec 0.0396075, recall 0.77792
2017-12-10T02:38:20.679246: step 1183, loss 1.6484, acc 0.828125, prec 0.0397181, recall 0.778485
2017-12-10T02:38:20.940587: step 1184, loss 0.884174, acc 0.671875, prec 0.039691, recall 0.778485
2017-12-10T02:38:21.201188: step 1185, loss 0.875798, acc 0.828125, prec 0.039708, recall 0.778626
2017-12-10T02:38:21.466438: step 1186, loss 0.753402, acc 0.8125, prec 0.0396926, recall 0.778626
2017-12-10T02:38:21.728495: step 1187, loss 0.447427, acc 0.875, prec 0.0396823, recall 0.778626
2017-12-10T02:38:21.989905: step 1188, loss 0.547904, acc 0.8125, prec 0.0397291, recall 0.778907
2017-12-10T02:38:22.253054: step 1189, loss 0.197123, acc 0.921875, prec 0.0397227, recall 0.778907
2017-12-10T02:38:22.527305: step 1190, loss 0.206252, acc 0.890625, prec 0.0397136, recall 0.778907
2017-12-10T02:38:22.788630: step 1191, loss 0.259902, acc 0.90625, prec 0.039737, recall 0.779048
2017-12-10T02:38:23.057452: step 1192, loss 0.208916, acc 0.9375, prec 0.039763, recall 0.779188
2017-12-10T02:38:23.325142: step 1193, loss 2.78372, acc 0.84375, prec 0.0397514, recall 0.778694
2017-12-10T02:38:23.591324: step 1194, loss 0.56721, acc 0.859375, prec 0.0397398, recall 0.778694
2017-12-10T02:38:23.856165: step 1195, loss 2.92182, acc 0.90625, prec 0.0397334, recall 0.7782
2017-12-10T02:38:24.119448: step 1196, loss 0.976848, acc 0.921875, prec 0.0397891, recall 0.778481
2017-12-10T02:38:24.385666: step 1197, loss 0.593341, acc 0.890625, prec 0.0398732, recall 0.778901
2017-12-10T02:38:24.652225: step 1198, loss 1.56332, acc 0.828125, prec 0.0398603, recall 0.778409
2017-12-10T02:38:24.920323: step 1199, loss 0.676787, acc 0.8125, prec 0.0398759, recall 0.778549
2017-12-10T02:38:25.182769: step 1200, loss 0.946632, acc 0.796875, prec 0.0399522, recall 0.778967

Evaluation:
2017-12-10T02:38:32.804963: step 1200, loss 1.33392, acc 0.762525, prec 0.0408789, recall 0.783676

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-1200

2017-12-10T02:38:34.190941: step 1201, loss 0.645759, acc 0.796875, prec 0.0408916, recall 0.783799
2017-12-10T02:38:34.460788: step 1202, loss 0.74019, acc 0.78125, prec 0.0409031, recall 0.783922
2017-12-10T02:38:34.723316: step 1203, loss 1.00129, acc 0.65625, prec 0.0409049, recall 0.784046
2017-12-10T02:38:34.982642: step 1204, loss 1.15028, acc 0.734375, prec 0.0409412, recall 0.784291
2017-12-10T02:38:35.243994: step 1205, loss 0.551956, acc 0.828125, prec 0.0409279, recall 0.784291
2017-12-10T02:38:35.506847: step 1206, loss 1.42775, acc 0.609375, prec 0.0408975, recall 0.784291
2017-12-10T02:38:35.769261: step 1207, loss 0.625181, acc 0.796875, prec 0.0409102, recall 0.784414
2017-12-10T02:38:36.029292: step 1208, loss 1.09658, acc 0.703125, prec 0.0409724, recall 0.784781
2017-12-10T02:38:36.293138: step 1209, loss 0.563044, acc 0.84375, prec 0.0409887, recall 0.784904
2017-12-10T02:38:36.560630: step 1210, loss 1.19306, acc 0.703125, prec 0.040994, recall 0.785026
2017-12-10T02:38:36.819442: step 1211, loss 0.708783, acc 0.828125, prec 0.0410091, recall 0.785147
2017-12-10T02:38:37.083440: step 1212, loss 0.587467, acc 0.8125, prec 0.0409945, recall 0.785147
2017-12-10T02:38:37.347888: step 1213, loss 0.427638, acc 0.90625, prec 0.0410724, recall 0.785512
2017-12-10T02:38:37.611708: step 1214, loss 0.789432, acc 0.78125, prec 0.0410837, recall 0.785634
2017-12-10T02:38:37.874093: step 1215, loss 0.472001, acc 0.84375, prec 0.0410716, recall 0.785634
2017-12-10T02:38:38.141481: step 1216, loss 0.26976, acc 0.921875, prec 0.0410939, recall 0.785755
2017-12-10T02:38:38.400443: step 1217, loss 0.405938, acc 0.875, prec 0.0410841, recall 0.785755
2017-12-10T02:38:38.662425: step 1218, loss 0.237604, acc 0.890625, prec 0.0410757, recall 0.785755
2017-12-10T02:38:38.928973: step 1219, loss 0.0805054, acc 0.96875, prec 0.0410732, recall 0.785755
2017-12-10T02:38:39.191278: step 1220, loss 0.385046, acc 0.90625, prec 0.0410659, recall 0.785755
2017-12-10T02:38:39.454441: step 1221, loss 10.772, acc 0.875, prec 0.0411141, recall 0.785553
2017-12-10T02:38:39.728785: step 1222, loss 0.139585, acc 0.96875, prec 0.0411117, recall 0.785553
2017-12-10T02:38:39.993688: step 1223, loss 0.0854024, acc 0.984375, prec 0.0411671, recall 0.785795
2017-12-10T02:38:40.257934: step 1224, loss 0.19015, acc 0.953125, prec 0.0411918, recall 0.785915
2017-12-10T02:38:40.519794: step 1225, loss 0.0975217, acc 0.953125, prec 0.0411881, recall 0.785915
2017-12-10T02:38:40.782719: step 1226, loss 0.221461, acc 0.921875, prec 0.041182, recall 0.785915
2017-12-10T02:38:41.051849: step 1227, loss 0.41483, acc 0.90625, prec 0.0411747, recall 0.785915
2017-12-10T02:38:41.317719: step 1228, loss 4.06294, acc 0.90625, prec 0.041197, recall 0.785594
2017-12-10T02:38:41.593445: step 1229, loss 3.44505, acc 0.828125, prec 0.0412131, recall 0.785273
2017-12-10T02:38:41.861046: step 1230, loss 0.548859, acc 0.828125, prec 0.0412563, recall 0.785514
2017-12-10T02:38:42.124570: step 1231, loss 1.50599, acc 0.859375, prec 0.0413019, recall 0.785754
2017-12-10T02:38:42.391112: step 1232, loss 1.32045, acc 0.65625, prec 0.0413033, recall 0.785874
2017-12-10T02:38:42.662338: step 1233, loss 1.54975, acc 0.609375, prec 0.0412729, recall 0.785874
2017-12-10T02:38:42.921504: step 1234, loss 1.83311, acc 0.5625, prec 0.0412953, recall 0.786114
2017-12-10T02:38:43.192029: step 1235, loss 1.69205, acc 0.625, prec 0.0413226, recall 0.786353
2017-12-10T02:38:43.448621: step 1236, loss 1.85465, acc 0.546875, prec 0.0413155, recall 0.786473
2017-12-10T02:38:43.703627: step 1237, loss 2.81974, acc 0.46875, prec 0.0412743, recall 0.786473
2017-12-10T02:38:43.966102: step 1238, loss 1.79417, acc 0.484375, prec 0.0412625, recall 0.786592
2017-12-10T02:38:44.232226: step 1239, loss 2.01034, acc 0.46875, prec 0.0412214, recall 0.786592
2017-12-10T02:38:44.498152: step 1240, loss 1.3912, acc 0.640625, prec 0.0412217, recall 0.786711
2017-12-10T02:38:44.758607: step 1241, loss 1.37441, acc 0.59375, prec 0.0412184, recall 0.78683
2017-12-10T02:38:45.019195: step 1242, loss 2.0072, acc 0.578125, prec 0.0412139, recall 0.786949
2017-12-10T02:38:45.284251: step 1243, loss 2.0659, acc 0.546875, prec 0.041179, recall 0.786949
2017-12-10T02:38:45.542128: step 1244, loss 1.11703, acc 0.71875, prec 0.0411574, recall 0.786949
2017-12-10T02:38:45.808199: step 1245, loss 0.955333, acc 0.71875, prec 0.0411358, recall 0.786949
2017-12-10T02:38:46.071623: step 1246, loss 0.805433, acc 0.78125, prec 0.041119, recall 0.786949
2017-12-10T02:38:46.333131: step 1247, loss 0.308465, acc 0.859375, prec 0.0411362, recall 0.787068
2017-12-10T02:38:46.592713: step 1248, loss 2.33666, acc 0.875, prec 0.0411557, recall 0.786748
2017-12-10T02:38:46.861926: step 1249, loss 0.419641, acc 0.90625, prec 0.0411485, recall 0.786748
2017-12-10T02:38:47.122753: step 1250, loss 0.335425, acc 0.859375, prec 0.0411378, recall 0.786748
2017-12-10T02:38:47.381822: step 1251, loss 0.501478, acc 0.796875, prec 0.0411222, recall 0.786748
2017-12-10T02:38:47.642571: step 1252, loss 0.179475, acc 0.953125, prec 0.0411186, recall 0.786748
2017-12-10T02:38:47.910243: step 1253, loss 0.109862, acc 0.9375, prec 0.0411417, recall 0.786867
2017-12-10T02:38:48.172993: step 1254, loss 0.380946, acc 0.921875, prec 0.0411636, recall 0.786986
2017-12-10T02:38:48.440904: step 1255, loss 0.173148, acc 0.953125, prec 0.0411879, recall 0.787104
2017-12-10T02:38:48.703044: step 1256, loss 0.212736, acc 0.96875, prec 0.0411855, recall 0.787104
2017-12-10T02:38:48.964613: step 1257, loss 0.152015, acc 0.96875, prec 0.041211, recall 0.787222
2017-12-10T02:38:49.225895: step 1258, loss 0.774917, acc 0.953125, prec 0.0412353, recall 0.78734
2017-12-10T02:38:49.495674: step 1259, loss 6.77987, acc 0.953125, prec 0.0412887, recall 0.78714
2017-12-10T02:38:49.764709: step 1260, loss 0.269114, acc 0.890625, prec 0.0413081, recall 0.787258
2017-12-10T02:38:50.025597: step 1261, loss 0.420473, acc 0.859375, prec 0.0413252, recall 0.787375
2017-12-10T02:38:50.287810: step 1262, loss 0.328972, acc 0.890625, prec 0.0414282, recall 0.787845
2017-12-10T02:38:50.551437: step 1263, loss 0.293064, acc 0.921875, prec 0.04145, recall 0.787962
2017-12-10T02:38:50.812693: step 1264, loss 0.953591, acc 0.796875, prec 0.0414344, recall 0.787962
2017-12-10T02:38:51.074648: step 1265, loss 0.662214, acc 0.875, prec 0.0414526, recall 0.788079
2017-12-10T02:38:51.334510: step 1266, loss 0.436888, acc 0.859375, prec 0.0414974, recall 0.788313
2017-12-10T02:38:51.594502: step 1267, loss 1.62463, acc 0.859375, prec 0.0415144, recall 0.78843
2017-12-10T02:38:51.860546: step 1268, loss 2.90891, acc 0.75, prec 0.0414963, recall 0.787996
2017-12-10T02:38:52.130229: step 1269, loss 0.776349, acc 0.796875, prec 0.0414807, recall 0.787996
2017-12-10T02:38:52.391379: step 1270, loss 0.600624, acc 0.796875, prec 0.0415206, recall 0.788229
2017-12-10T02:38:52.649806: step 1271, loss 1.18705, acc 0.703125, prec 0.0414977, recall 0.788229
2017-12-10T02:38:52.915725: step 1272, loss 1.24542, acc 0.6875, prec 0.0415292, recall 0.788462
2017-12-10T02:38:53.175185: step 1273, loss 1.80018, acc 0.765625, prec 0.0415944, recall 0.78881
2017-12-10T02:38:53.436751: step 1274, loss 1.1035, acc 0.78125, prec 0.0416606, recall 0.789157
2017-12-10T02:38:53.696123: step 1275, loss 0.989212, acc 0.796875, prec 0.0417281, recall 0.789502
2017-12-10T02:38:53.959953: step 1276, loss 1.84234, acc 0.625, prec 0.0417268, recall 0.789617
2017-12-10T02:38:54.225862: step 1277, loss 0.931428, acc 0.734375, prec 0.041734, recall 0.789732
2017-12-10T02:38:54.493120: step 1278, loss 0.786079, acc 0.796875, prec 0.041746, recall 0.789847
2017-12-10T02:38:54.750982: step 1279, loss 0.902067, acc 0.765625, prec 0.0418108, recall 0.790191
2017-12-10T02:38:55.015644: step 1280, loss 1.02104, acc 0.765625, prec 0.0418204, recall 0.790305
2017-12-10T02:38:55.276386: step 1281, loss 1.02187, acc 0.765625, prec 0.0418023, recall 0.790305
2017-12-10T02:38:55.538043: step 1282, loss 0.988109, acc 0.78125, prec 0.0417855, recall 0.790305
2017-12-10T02:38:55.805465: step 1283, loss 0.41408, acc 0.859375, prec 0.0418022, recall 0.790419
2017-12-10T02:38:56.069677: step 1284, loss 1.10714, acc 0.78125, prec 0.0417854, recall 0.790419
2017-12-10T02:38:56.335317: step 1285, loss 0.405266, acc 0.90625, prec 0.0418057, recall 0.790533
2017-12-10T02:38:56.601623: step 1286, loss 3.44825, acc 0.953125, prec 0.0418585, recall 0.790331
2017-12-10T02:38:56.871476: step 1287, loss 0.385141, acc 0.859375, prec 0.0418476, recall 0.790331
2017-12-10T02:38:57.130418: step 1288, loss 0.362261, acc 0.875, prec 0.041838, recall 0.790331
2017-12-10T02:38:57.393304: step 1289, loss 0.523295, acc 0.875, prec 0.0418559, recall 0.790445
2017-12-10T02:38:57.654665: step 1290, loss 0.582018, acc 0.765625, prec 0.0418379, recall 0.790445
2017-12-10T02:38:57.917675: step 1291, loss 0.556796, acc 0.8125, prec 0.041851, recall 0.790559
2017-12-10T02:38:58.186296: step 1292, loss 0.630956, acc 0.90625, prec 0.0418713, recall 0.790672
2017-12-10T02:38:58.449974: step 1293, loss 0.738263, acc 0.875, prec 0.0418892, recall 0.790786
2017-12-10T02:38:58.714165: step 1294, loss 0.28237, acc 0.921875, prec 0.0419382, recall 0.791012
2017-12-10T02:38:58.977266: step 1295, loss 0.413431, acc 0.890625, prec 0.0419297, recall 0.791012
2017-12-10T02:38:59.239719: step 1296, loss 0.190528, acc 0.890625, prec 0.0419213, recall 0.791012
2017-12-10T02:38:59.501222: step 1297, loss 3.8394, acc 0.953125, prec 0.0419464, recall 0.790698
2017-12-10T02:38:59.773490: step 1298, loss 0.480184, acc 0.9375, prec 0.042024, recall 0.791037
2017-12-10T02:39:00.049481: step 1299, loss 0.203148, acc 0.921875, prec 0.0420455, recall 0.791149
2017-12-10T02:39:00.320864: step 1300, loss 1.8125, acc 0.875, prec 0.042037, recall 0.790723
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-1300

2017-12-10T02:39:01.682354: step 1301, loss 0.254053, acc 0.921875, prec 0.0420585, recall 0.790836
2017-12-10T02:39:01.950115: step 1302, loss 0.716014, acc 0.875, prec 0.0421861, recall 0.791398
2017-12-10T02:39:02.214966: step 1303, loss 0.685875, acc 0.8125, prec 0.0422265, recall 0.791622
2017-12-10T02:39:02.480139: step 1304, loss 0.687462, acc 0.796875, prec 0.0422108, recall 0.791622
2017-12-10T02:39:02.745691: step 1305, loss 0.439484, acc 0.84375, prec 0.0421987, recall 0.791622
2017-12-10T02:39:03.004643: step 1306, loss 0.496743, acc 0.828125, prec 0.0422402, recall 0.791846
2017-12-10T02:39:03.264422: step 1307, loss 0.930334, acc 0.78125, prec 0.0422781, recall 0.792069
2017-12-10T02:39:03.525211: step 1308, loss 0.415434, acc 0.875, prec 0.0423232, recall 0.792291
2017-12-10T02:39:03.788299: step 1309, loss 0.974531, acc 0.765625, prec 0.0423051, recall 0.792291
2017-12-10T02:39:04.047127: step 1310, loss 0.63799, acc 0.796875, prec 0.0423167, recall 0.792402
2017-12-10T02:39:04.304961: step 1311, loss 0.339745, acc 0.859375, prec 0.0423332, recall 0.792513
2017-12-10T02:39:04.564537: step 1312, loss 0.628223, acc 0.78125, prec 0.0423163, recall 0.792513
2017-12-10T02:39:04.827656: step 1313, loss 0.417654, acc 0.890625, prec 0.0423078, recall 0.792513
2017-12-10T02:39:05.093204: step 1314, loss 0.452937, acc 0.796875, prec 0.0422921, recall 0.792513
2017-12-10T02:39:05.360722: step 1315, loss 0.688662, acc 0.84375, prec 0.04228, recall 0.792513
2017-12-10T02:39:05.620355: step 1316, loss 1.07707, acc 0.78125, prec 0.0422905, recall 0.792624
2017-12-10T02:39:05.880232: step 1317, loss 0.604008, acc 0.859375, prec 0.0422796, recall 0.792624
2017-12-10T02:39:06.149027: step 1318, loss 0.548794, acc 0.78125, prec 0.04229, recall 0.792735
2017-12-10T02:39:06.414319: step 1319, loss 0.450984, acc 0.859375, prec 0.0423338, recall 0.792956
2017-12-10T02:39:06.675309: step 1320, loss 0.382358, acc 0.84375, prec 0.042349, recall 0.793067
2017-12-10T02:39:06.939717: step 1321, loss 0.291693, acc 0.875, prec 0.0423666, recall 0.793177
2017-12-10T02:39:07.209281: step 1322, loss 14.0011, acc 0.859375, prec 0.042357, recall 0.792754
2017-12-10T02:39:07.477329: step 1323, loss 0.320148, acc 0.90625, prec 0.0423497, recall 0.792754
2017-12-10T02:39:07.741048: step 1324, loss 5.46401, acc 0.9375, prec 0.0423461, recall 0.792332
2017-12-10T02:39:08.008602: step 1325, loss 0.452276, acc 0.921875, prec 0.0423401, recall 0.792332
2017-12-10T02:39:08.277337: step 1326, loss 0.342544, acc 0.890625, prec 0.0423317, recall 0.792332
2017-12-10T02:39:08.535173: step 1327, loss 0.665939, acc 0.859375, prec 0.0424025, recall 0.792663
2017-12-10T02:39:08.805875: step 1328, loss 0.637439, acc 0.84375, prec 0.0424177, recall 0.792774
2017-12-10T02:39:09.070079: step 1329, loss 0.808401, acc 0.828125, prec 0.0424589, recall 0.792994
2017-12-10T02:39:09.336458: step 1330, loss 0.528894, acc 0.859375, prec 0.042448, recall 0.792994
2017-12-10T02:39:09.598772: step 1331, loss 0.710962, acc 0.75, prec 0.0424287, recall 0.792994
2017-12-10T02:39:09.857763: step 1332, loss 0.818901, acc 0.78125, prec 0.042439, recall 0.793103
2017-12-10T02:39:10.120638: step 1333, loss 0.948308, acc 0.703125, prec 0.0424433, recall 0.793213
2017-12-10T02:39:10.378645: step 1334, loss 0.576294, acc 0.78125, prec 0.0424265, recall 0.793213
2017-12-10T02:39:10.638582: step 1335, loss 0.792889, acc 0.8125, prec 0.0424392, recall 0.793323
2017-12-10T02:39:10.897847: step 1336, loss 0.707745, acc 0.828125, prec 0.042426, recall 0.793323
2017-12-10T02:39:11.160853: step 1337, loss 0.569152, acc 0.875, prec 0.0424163, recall 0.793323
2017-12-10T02:39:11.431413: step 1338, loss 0.713013, acc 0.78125, prec 0.0423995, recall 0.793323
2017-12-10T02:39:11.698286: step 1339, loss 0.489175, acc 0.875, prec 0.0423899, recall 0.793323
2017-12-10T02:39:11.969097: step 1340, loss 0.687066, acc 0.828125, prec 0.0423767, recall 0.793323
2017-12-10T02:39:12.227288: step 1341, loss 0.670937, acc 0.859375, prec 0.0424201, recall 0.793542
2017-12-10T02:39:12.490306: step 1342, loss 0.22958, acc 0.921875, prec 0.0424141, recall 0.793542
2017-12-10T02:39:12.752784: step 1343, loss 0.303358, acc 0.921875, prec 0.0424081, recall 0.793542
2017-12-10T02:39:13.021205: step 1344, loss 3.14629, acc 0.90625, prec 0.0424021, recall 0.793122
2017-12-10T02:39:13.290217: step 1345, loss 0.398542, acc 0.90625, prec 0.0423949, recall 0.793122
2017-12-10T02:39:13.559047: step 1346, loss 7.02173, acc 0.90625, prec 0.042416, recall 0.792812
2017-12-10T02:39:13.822031: step 1347, loss 0.406816, acc 0.90625, prec 0.0424359, recall 0.792921
2017-12-10T02:39:14.083988: step 1348, loss 1.27846, acc 0.90625, prec 0.0424828, recall 0.79314
2017-12-10T02:39:14.356798: step 1349, loss 0.458693, acc 0.84375, prec 0.0424708, recall 0.79314
2017-12-10T02:39:14.625860: step 1350, loss 0.623482, acc 0.859375, prec 0.04246, recall 0.79314
2017-12-10T02:39:14.889969: step 1351, loss 1.29351, acc 0.734375, prec 0.0425208, recall 0.793467
2017-12-10T02:39:15.156911: step 1352, loss 0.747175, acc 0.75, prec 0.0425016, recall 0.793467
2017-12-10T02:39:15.419421: step 1353, loss 0.710637, acc 0.8125, prec 0.0424872, recall 0.793467
2017-12-10T02:39:15.677447: step 1354, loss 1.11054, acc 0.734375, prec 0.0425208, recall 0.793684
2017-12-10T02:39:15.940411: step 1355, loss 0.562551, acc 0.84375, prec 0.0425088, recall 0.793684
2017-12-10T02:39:16.209372: step 1356, loss 0.708979, acc 0.734375, prec 0.0424884, recall 0.793684
2017-12-10T02:39:16.473508: step 1357, loss 0.769578, acc 0.75, prec 0.0424693, recall 0.793684
2017-12-10T02:39:16.732550: step 1358, loss 0.51662, acc 0.828125, prec 0.0424562, recall 0.793684
2017-12-10T02:39:16.995631: step 1359, loss 0.998568, acc 0.8125, prec 0.0424418, recall 0.793684
2017-12-10T02:39:17.256352: step 1360, loss 0.439691, acc 0.84375, prec 0.0424299, recall 0.793684
2017-12-10T02:39:17.515539: step 1361, loss 0.627082, acc 0.78125, prec 0.0424132, recall 0.793684
2017-12-10T02:39:17.780686: step 1362, loss 0.644169, acc 0.84375, prec 0.0424012, recall 0.793684
2017-12-10T02:39:18.048397: step 1363, loss 0.547975, acc 0.78125, prec 0.0423846, recall 0.793684
2017-12-10T02:39:18.305255: step 1364, loss 0.366502, acc 0.84375, prec 0.0423726, recall 0.793684
2017-12-10T02:39:18.561353: step 1365, loss 0.258878, acc 0.90625, prec 0.0423655, recall 0.793684
2017-12-10T02:39:18.828748: step 1366, loss 0.303829, acc 0.875, prec 0.042356, recall 0.793684
2017-12-10T02:39:19.092904: step 1367, loss 5.60527, acc 0.84375, prec 0.0423453, recall 0.793267
2017-12-10T02:39:19.355117: step 1368, loss 0.576617, acc 0.875, prec 0.0423895, recall 0.793484
2017-12-10T02:39:19.617495: step 1369, loss 0.355396, acc 0.90625, prec 0.0424361, recall 0.793701
2017-12-10T02:39:19.879015: step 1370, loss 0.43369, acc 0.921875, prec 0.0425108, recall 0.794025
2017-12-10T02:39:20.143963: step 1371, loss 0.182467, acc 0.921875, prec 0.0425317, recall 0.794133
2017-12-10T02:39:20.414702: step 1372, loss 3.03427, acc 0.875, prec 0.0425234, recall 0.793717
2017-12-10T02:39:20.688054: step 1373, loss 0.342799, acc 0.953125, prec 0.0425198, recall 0.793717
2017-12-10T02:39:20.949042: step 1374, loss 0.314241, acc 0.90625, prec 0.0425126, recall 0.793717
2017-12-10T02:39:21.213997: step 1375, loss 0.103672, acc 0.96875, prec 0.0425102, recall 0.793717
2017-12-10T02:39:21.475568: step 1376, loss 0.307354, acc 0.890625, prec 0.0425287, recall 0.793825
2017-12-10T02:39:21.737980: step 1377, loss 0.367562, acc 0.84375, prec 0.0425168, recall 0.793825
2017-12-10T02:39:21.998583: step 1378, loss 1.94938, acc 0.84375, prec 0.0425061, recall 0.79341
2017-12-10T02:39:22.261377: step 1379, loss 0.756063, acc 0.78125, prec 0.0424894, recall 0.79341
2017-12-10T02:39:22.529452: step 1380, loss 0.404904, acc 0.84375, prec 0.0425311, recall 0.793626
2017-12-10T02:39:22.796454: step 1381, loss 0.493429, acc 0.828125, prec 0.0425717, recall 0.793841
2017-12-10T02:39:23.058856: step 1382, loss 0.504743, acc 0.875, prec 0.0425621, recall 0.793841
2017-12-10T02:39:23.330683: step 1383, loss 0.487192, acc 0.828125, prec 0.0426026, recall 0.794056
2017-12-10T02:39:23.594747: step 1384, loss 0.438506, acc 0.8125, prec 0.0426954, recall 0.794485
2017-12-10T02:39:23.863658: step 1385, loss 0.591623, acc 0.796875, prec 0.0427334, recall 0.794699
2017-12-10T02:39:24.133399: step 1386, loss 0.54948, acc 0.828125, prec 0.0427202, recall 0.794699
2017-12-10T02:39:24.396437: step 1387, loss 0.74702, acc 0.828125, prec 0.0427338, recall 0.794805
2017-12-10T02:39:24.657255: step 1388, loss 0.546484, acc 0.828125, prec 0.0427475, recall 0.794912
2017-12-10T02:39:24.928247: step 1389, loss 0.980779, acc 0.75, prec 0.0427818, recall 0.795124
2017-12-10T02:39:25.196419: step 1390, loss 0.415169, acc 0.875, prec 0.0427722, recall 0.795124
2017-12-10T02:39:25.461026: step 1391, loss 0.441637, acc 0.828125, prec 0.0428392, recall 0.795443
2017-12-10T02:39:25.717519: step 1392, loss 0.253256, acc 0.90625, prec 0.0428587, recall 0.795549
2017-12-10T02:39:25.988000: step 1393, loss 0.261464, acc 0.890625, prec 0.0428771, recall 0.795654
2017-12-10T02:39:26.250357: step 1394, loss 0.402798, acc 0.9375, prec 0.0429256, recall 0.795866
2017-12-10T02:39:26.522051: step 1395, loss 0.486248, acc 0.84375, prec 0.0429403, recall 0.795971
2017-12-10T02:39:26.791522: step 1396, loss 0.132844, acc 0.96875, prec 0.0429913, recall 0.796182
2017-12-10T02:39:27.054466: step 1397, loss 0.13823, acc 0.921875, prec 0.0429853, recall 0.796182
2017-12-10T02:39:27.316276: step 1398, loss 0.312906, acc 0.90625, prec 0.0429781, recall 0.796182
2017-12-10T02:39:27.581114: step 1399, loss 6.846, acc 0.90625, prec 0.0429721, recall 0.795771
2017-12-10T02:39:27.844448: step 1400, loss 0.469725, acc 0.9375, prec 0.042994, recall 0.795876
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-1400

2017-12-10T02:39:29.134038: step 1401, loss 0.256997, acc 0.90625, prec 0.0429868, recall 0.795876
2017-12-10T02:39:29.400104: step 1402, loss 16.2885, acc 0.921875, prec 0.0430087, recall 0.795572
2017-12-10T02:39:29.667741: step 1403, loss 0.472518, acc 0.890625, prec 0.0430269, recall 0.795677
2017-12-10T02:39:29.928210: step 1404, loss 6.82877, acc 0.84375, prec 0.0430161, recall 0.795267
2017-12-10T02:39:30.203964: step 1405, loss 0.234891, acc 0.921875, prec 0.0430368, recall 0.795373
2017-12-10T02:39:30.474209: step 1406, loss 0.634785, acc 0.796875, prec 0.0430478, recall 0.795478
2017-12-10T02:39:30.736343: step 1407, loss 1.05513, acc 0.71875, prec 0.0430529, recall 0.795583
2017-12-10T02:39:30.993515: step 1408, loss 1.33644, acc 0.640625, prec 0.043052, recall 0.795688
2017-12-10T02:39:31.257874: step 1409, loss 1.9128, acc 0.53125, prec 0.0430161, recall 0.795688
2017-12-10T02:39:31.516191: step 1410, loss 1.54853, acc 0.671875, prec 0.0430442, recall 0.795897
2017-12-10T02:39:31.777985: step 1411, loss 1.53857, acc 0.6875, prec 0.0430733, recall 0.796107
2017-12-10T02:39:32.037321: step 1412, loss 1.47337, acc 0.734375, prec 0.0431061, recall 0.796315
2017-12-10T02:39:32.293987: step 1413, loss 1.58468, acc 0.625, prec 0.0431304, recall 0.796524
2017-12-10T02:39:32.558957: step 1414, loss 2.1214, acc 0.625, prec 0.0431018, recall 0.796524
2017-12-10T02:39:32.824456: step 1415, loss 1.89777, acc 0.609375, prec 0.0430984, recall 0.796627
2017-12-10T02:39:33.091875: step 1416, loss 1.28754, acc 0.71875, prec 0.043077, recall 0.796627
2017-12-10T02:39:33.356986: step 1417, loss 1.4622, acc 0.6875, prec 0.0430796, recall 0.796731
2017-12-10T02:39:33.621986: step 1418, loss 1.16781, acc 0.671875, prec 0.0431075, recall 0.796939
2017-12-10T02:39:33.881951: step 1419, loss 1.27549, acc 0.796875, prec 0.0431184, recall 0.797042
2017-12-10T02:39:34.142113: step 1420, loss 0.83405, acc 0.78125, prec 0.0431809, recall 0.797352
2017-12-10T02:39:34.412327: step 1421, loss 0.729121, acc 0.8125, prec 0.0431667, recall 0.797352
2017-12-10T02:39:34.672819: step 1422, loss 0.631021, acc 0.84375, prec 0.0431548, recall 0.797352
2017-12-10T02:39:34.934414: step 1423, loss 0.629154, acc 0.8125, prec 0.0431405, recall 0.797352
2017-12-10T02:39:35.194556: step 1424, loss 0.426195, acc 0.875, prec 0.043131, recall 0.797352
2017-12-10T02:39:35.458897: step 1425, loss 0.354166, acc 0.921875, prec 0.0431251, recall 0.797352
2017-12-10T02:39:35.728194: step 1426, loss 0.338175, acc 0.890625, prec 0.0431694, recall 0.797558
2017-12-10T02:39:35.993845: step 1427, loss 0.0964407, acc 0.953125, prec 0.0431659, recall 0.797558
2017-12-10T02:39:36.264674: step 1428, loss 0.366295, acc 0.953125, prec 0.0431623, recall 0.797558
2017-12-10T02:39:36.528925: step 1429, loss 0.389049, acc 0.953125, prec 0.0431851, recall 0.797661
2017-12-10T02:39:36.791367: step 1430, loss 0.189261, acc 0.921875, prec 0.0432055, recall 0.797764
2017-12-10T02:39:37.060361: step 1431, loss 0.255341, acc 0.96875, prec 0.0432294, recall 0.797867
2017-12-10T02:39:37.328067: step 1432, loss 0.134627, acc 0.96875, prec 0.0432534, recall 0.79797
2017-12-10T02:39:37.591968: step 1433, loss 0.252719, acc 0.921875, prec 0.0432474, recall 0.79797
2017-12-10T02:39:37.855484: step 1434, loss 4.72346, acc 0.96875, prec 0.0432725, recall 0.797667
2017-12-10T02:39:38.121868: step 1435, loss 2.97071, acc 0.953125, prec 0.0432702, recall 0.797263
2017-12-10T02:39:38.393081: step 1436, loss 0.249957, acc 0.90625, prec 0.043263, recall 0.797263
2017-12-10T02:39:38.653133: step 1437, loss 2.0568, acc 0.90625, prec 0.0432571, recall 0.796859
2017-12-10T02:39:38.926429: step 1438, loss 0.249162, acc 0.9375, prec 0.0432523, recall 0.796859
2017-12-10T02:39:39.192378: step 1439, loss 0.961859, acc 0.953125, prec 0.043275, recall 0.796962
2017-12-10T02:39:39.459001: step 1440, loss 0.872561, acc 0.765625, prec 0.0433098, recall 0.797167
2017-12-10T02:39:39.721727: step 1441, loss 1.21508, acc 0.859375, prec 0.0433516, recall 0.797372
2017-12-10T02:39:39.988733: step 1442, loss 1.02555, acc 0.703125, prec 0.0433553, recall 0.797475
2017-12-10T02:39:40.246192: step 1443, loss 0.704508, acc 0.828125, prec 0.0433422, recall 0.797475
2017-12-10T02:39:40.510948: step 1444, loss 0.725749, acc 0.75, prec 0.0433232, recall 0.797475
2017-12-10T02:39:40.773302: step 1445, loss 0.982619, acc 0.703125, prec 0.0433006, recall 0.797475
2017-12-10T02:39:41.037615: step 1446, loss 0.800468, acc 0.8125, prec 0.0432864, recall 0.797475
2017-12-10T02:39:41.314387: step 1447, loss 0.863702, acc 0.734375, prec 0.0432662, recall 0.797475
2017-12-10T02:39:41.584386: step 1448, loss 0.863034, acc 0.78125, prec 0.0432758, recall 0.797577
2017-12-10T02:39:41.843246: step 1449, loss 0.990969, acc 0.78125, prec 0.0432854, recall 0.797679
2017-12-10T02:39:42.109138: step 1450, loss 0.802532, acc 0.765625, prec 0.0432677, recall 0.797679
2017-12-10T02:39:42.372411: step 1451, loss 1.14547, acc 0.71875, prec 0.0432987, recall 0.797883
2017-12-10T02:39:42.642570: step 1452, loss 0.641582, acc 0.828125, prec 0.0433641, recall 0.798188
2017-12-10T02:39:42.902904: step 1453, loss 0.815597, acc 0.75, prec 0.0433452, recall 0.798188
2017-12-10T02:39:43.169306: step 1454, loss 0.788644, acc 0.84375, prec 0.0433333, recall 0.798188
2017-12-10T02:39:43.432519: step 1455, loss 1.15864, acc 0.765625, prec 0.0433417, recall 0.79829
2017-12-10T02:39:43.694871: step 1456, loss 1.02848, acc 0.765625, prec 0.043324, recall 0.79829
2017-12-10T02:39:43.953261: step 1457, loss 0.537566, acc 0.875, prec 0.0433145, recall 0.79829
2017-12-10T02:39:44.219365: step 1458, loss 0.285498, acc 0.890625, prec 0.0433062, recall 0.79829
2017-12-10T02:39:44.485709: step 1459, loss 0.439728, acc 0.859375, prec 0.0432956, recall 0.79829
2017-12-10T02:39:44.751629: step 1460, loss 0.397646, acc 0.9375, prec 0.0434213, recall 0.798796
2017-12-10T02:39:45.013943: step 1461, loss 0.163758, acc 0.921875, prec 0.0434154, recall 0.798796
2017-12-10T02:39:45.276629: step 1462, loss 3.26245, acc 0.921875, prec 0.0434368, recall 0.798496
2017-12-10T02:39:45.547620: step 1463, loss 3.7041, acc 0.890625, prec 0.0434297, recall 0.798096
2017-12-10T02:39:45.818502: step 1464, loss 0.0788436, acc 0.96875, prec 0.0434273, recall 0.798096
2017-12-10T02:39:46.087657: step 1465, loss 2.68594, acc 0.796875, prec 0.0434131, recall 0.797697
2017-12-10T02:39:46.349646: step 1466, loss 0.218248, acc 0.90625, prec 0.043406, recall 0.797697
2017-12-10T02:39:46.613396: step 1467, loss 0.578739, acc 0.796875, prec 0.0434427, recall 0.797899
2017-12-10T02:39:46.880035: step 1468, loss 1.18118, acc 0.734375, prec 0.0434226, recall 0.797899
2017-12-10T02:39:47.138728: step 1469, loss 3.38265, acc 0.796875, prec 0.0434084, recall 0.7975
2017-12-10T02:39:47.409687: step 1470, loss 0.758638, acc 0.796875, prec 0.0433931, recall 0.7975
2017-12-10T02:39:47.676908: step 1471, loss 1.41624, acc 0.703125, prec 0.0433967, recall 0.797601
2017-12-10T02:39:47.934105: step 1472, loss 1.23302, acc 0.671875, prec 0.0433979, recall 0.797702
2017-12-10T02:39:48.200510: step 1473, loss 1.51454, acc 0.703125, prec 0.0434015, recall 0.797803
2017-12-10T02:39:48.458502: step 1474, loss 1.67991, acc 0.625, prec 0.0433732, recall 0.797803
2017-12-10T02:39:48.719401: step 1475, loss 1.25802, acc 0.71875, prec 0.0433521, recall 0.797803
2017-12-10T02:39:48.981501: step 1476, loss 1.36396, acc 0.59375, prec 0.0433474, recall 0.797904
2017-12-10T02:39:49.247336: step 1477, loss 2.01306, acc 0.578125, prec 0.0433676, recall 0.798106
2017-12-10T02:39:49.506686: step 1478, loss 1.1566, acc 0.71875, prec 0.0433464, recall 0.798106
2017-12-10T02:39:49.767796: step 1479, loss 1.02503, acc 0.671875, prec 0.0433218, recall 0.798106
2017-12-10T02:39:50.028718: step 1480, loss 0.949341, acc 0.75, prec 0.0433289, recall 0.798206
2017-12-10T02:39:50.293492: step 1481, loss 0.952834, acc 0.734375, prec 0.043309, recall 0.798206
2017-12-10T02:39:50.556387: step 1482, loss 7.14223, acc 0.75, prec 0.0433173, recall 0.797909
2017-12-10T02:39:50.822136: step 1483, loss 0.847324, acc 0.84375, prec 0.0433314, recall 0.79801
2017-12-10T02:39:51.085221: step 1484, loss 0.993131, acc 0.75, prec 0.0433127, recall 0.79801
2017-12-10T02:39:51.345899: step 1485, loss 8.93966, acc 0.828125, prec 0.0433785, recall 0.797915
2017-12-10T02:39:51.610133: step 1486, loss 0.875486, acc 0.796875, prec 0.0433633, recall 0.797915
2017-12-10T02:39:51.870844: step 1487, loss 0.758134, acc 0.765625, prec 0.0433457, recall 0.797915
2017-12-10T02:39:52.128311: step 1488, loss 0.938388, acc 0.78125, prec 0.043381, recall 0.798115
2017-12-10T02:39:52.393344: step 1489, loss 1.21619, acc 0.734375, prec 0.0433869, recall 0.798215
2017-12-10T02:39:52.659366: step 1490, loss 0.758395, acc 0.78125, prec 0.0433963, recall 0.798315
2017-12-10T02:39:52.884768: step 1491, loss 0.728443, acc 0.784314, prec 0.0433834, recall 0.798315
2017-12-10T02:39:53.156889: step 1492, loss 0.518077, acc 0.78125, prec 0.0433671, recall 0.798315
2017-12-10T02:39:53.424352: step 1493, loss 1.0909, acc 0.8125, prec 0.0433788, recall 0.798415
2017-12-10T02:39:53.689203: step 1494, loss 0.747897, acc 0.828125, prec 0.0434432, recall 0.798714
2017-12-10T02:39:53.956156: step 1495, loss 0.836595, acc 0.828125, prec 0.0434303, recall 0.798714
2017-12-10T02:39:54.213046: step 1496, loss 0.875406, acc 0.8125, prec 0.0434677, recall 0.798913
2017-12-10T02:39:54.481992: step 1497, loss 0.36418, acc 0.890625, prec 0.0434596, recall 0.798913
2017-12-10T02:39:54.748383: step 1498, loss 0.439433, acc 0.84375, prec 0.0434993, recall 0.799112
2017-12-10T02:39:55.009761: step 1499, loss 0.580437, acc 0.859375, prec 0.0435402, recall 0.79931
2017-12-10T02:39:55.270267: step 1500, loss 1.86935, acc 0.90625, prec 0.04356, recall 0.799015

Evaluation:
2017-12-10T02:40:02.908055: step 1500, loss 1.95227, acc 0.873667, prec 0.0448816, recall 0.790337

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-1500

2017-12-10T02:40:04.256296: step 1501, loss 0.672073, acc 0.796875, prec 0.0448665, recall 0.790337
2017-12-10T02:40:04.521522: step 1502, loss 0.472752, acc 0.859375, prec 0.044856, recall 0.790337
2017-12-10T02:40:04.782097: step 1503, loss 0.175484, acc 0.921875, prec 0.0448502, recall 0.790337
2017-12-10T02:40:05.043724: step 1504, loss 0.181599, acc 0.921875, prec 0.0448691, recall 0.790433
2017-12-10T02:40:05.315438: step 1505, loss 0.41799, acc 0.875, prec 0.0448846, recall 0.790528
2017-12-10T02:40:05.576985: step 1506, loss 0.20219, acc 0.953125, prec 0.0449058, recall 0.790624
2017-12-10T02:40:05.845251: step 1507, loss 0.652299, acc 0.84375, prec 0.0448942, recall 0.790624
2017-12-10T02:40:06.107623: step 1508, loss 0.243551, acc 0.921875, prec 0.0448884, recall 0.790624
2017-12-10T02:40:06.368295: step 1509, loss 0.597602, acc 0.84375, prec 0.0449014, recall 0.790719
2017-12-10T02:40:06.631421: step 1510, loss 0.19109, acc 0.921875, prec 0.0449203, recall 0.790814
2017-12-10T02:40:06.890577: step 1511, loss 0.163336, acc 0.953125, prec 0.0449168, recall 0.790814
2017-12-10T02:40:07.148939: step 1512, loss 0.0896988, acc 0.953125, prec 0.0449134, recall 0.790814
2017-12-10T02:40:07.412952: step 1513, loss 0.0839562, acc 0.984375, prec 0.0449122, recall 0.790814
2017-12-10T02:40:07.671424: step 1514, loss 0.354478, acc 0.90625, prec 0.0449299, recall 0.790909
2017-12-10T02:40:07.936896: step 1515, loss 0.735331, acc 0.96875, prec 0.0449769, recall 0.791099
2017-12-10T02:40:08.207922: step 1516, loss 0.486245, acc 0.96875, prec 0.0450732, recall 0.791478
2017-12-10T02:40:08.477984: step 1517, loss 0.580256, acc 0.984375, prec 0.0451213, recall 0.791667
2017-12-10T02:40:08.742175: step 1518, loss 0.128003, acc 0.9375, prec 0.0451167, recall 0.791667
2017-12-10T02:40:09.006188: step 1519, loss 0.0959925, acc 0.96875, prec 0.0451143, recall 0.791667
2017-12-10T02:40:09.274034: step 1520, loss 6.38647, acc 0.953125, prec 0.0451132, recall 0.79095
2017-12-10T02:40:09.541681: step 1521, loss 0.214168, acc 0.953125, prec 0.0451097, recall 0.79095
2017-12-10T02:40:09.803182: step 1522, loss 0.16994, acc 0.9375, prec 0.0451297, recall 0.791045
2017-12-10T02:40:10.066136: step 1523, loss 0.484652, acc 0.875, prec 0.045145, recall 0.791139
2017-12-10T02:40:10.330639: step 1524, loss 0.491975, acc 0.921875, prec 0.0451392, recall 0.791139
2017-12-10T02:40:10.595679: step 1525, loss 0.361563, acc 0.875, prec 0.0451298, recall 0.791139
2017-12-10T02:40:10.855358: step 1526, loss 0.254345, acc 0.875, prec 0.0451205, recall 0.791139
2017-12-10T02:40:11.115577: step 1527, loss 0.89088, acc 0.8125, prec 0.045205, recall 0.791516
2017-12-10T02:40:11.380639: step 1528, loss 0.576583, acc 0.8125, prec 0.0452156, recall 0.79161
2017-12-10T02:40:11.649737: step 1529, loss 1.24237, acc 0.6875, prec 0.0451924, recall 0.79161
2017-12-10T02:40:11.914668: step 1530, loss 0.540181, acc 0.828125, prec 0.0451796, recall 0.79161
2017-12-10T02:40:12.173817: step 1531, loss 0.600099, acc 0.84375, prec 0.0452417, recall 0.791892
2017-12-10T02:40:12.443227: step 1532, loss 0.720561, acc 0.84375, prec 0.0452546, recall 0.791986
2017-12-10T02:40:12.704183: step 1533, loss 0.735843, acc 0.8125, prec 0.0453143, recall 0.792266
2017-12-10T02:40:12.965639: step 1534, loss 0.443704, acc 0.875, prec 0.0453049, recall 0.792266
2017-12-10T02:40:13.228896: step 1535, loss 0.678406, acc 0.828125, prec 0.0453412, recall 0.792453
2017-12-10T02:40:13.493611: step 1536, loss 0.384087, acc 0.875, prec 0.0453319, recall 0.792453
2017-12-10T02:40:13.765878: step 1537, loss 0.556546, acc 0.875, prec 0.0453471, recall 0.792546
2017-12-10T02:40:14.027802: step 1538, loss 0.39539, acc 0.890625, prec 0.045339, recall 0.792546
2017-12-10T02:40:14.299244: step 1539, loss 0.254846, acc 0.921875, prec 0.0453822, recall 0.792732
2017-12-10T02:40:14.559193: step 1540, loss 0.361944, acc 0.890625, prec 0.045374, recall 0.792732
2017-12-10T02:40:14.825472: step 1541, loss 1.1077, acc 0.953125, prec 0.0454195, recall 0.792918
2017-12-10T02:40:15.090852: step 1542, loss 0.0804574, acc 0.96875, prec 0.0454172, recall 0.792918
2017-12-10T02:40:15.359546: step 1543, loss 0.204115, acc 0.90625, prec 0.0454102, recall 0.792918
2017-12-10T02:40:15.627382: step 1544, loss 0.127665, acc 0.921875, prec 0.0454044, recall 0.792918
2017-12-10T02:40:15.897297: step 1545, loss 0.761806, acc 0.984375, prec 0.0454277, recall 0.793011
2017-12-10T02:40:16.166103: step 1546, loss 5.51095, acc 0.96875, prec 0.0454265, recall 0.792656
2017-12-10T02:40:16.439601: step 1547, loss 0.0876227, acc 0.96875, prec 0.0454242, recall 0.792656
2017-12-10T02:40:16.701025: step 1548, loss 0.241213, acc 0.9375, prec 0.045444, recall 0.792748
2017-12-10T02:40:16.967951: step 1549, loss 0.2641, acc 0.890625, prec 0.0454359, recall 0.792748
2017-12-10T02:40:17.239231: step 1550, loss 0.167357, acc 0.9375, prec 0.0454312, recall 0.792748
2017-12-10T02:40:17.505581: step 1551, loss 0.728101, acc 0.859375, prec 0.0454452, recall 0.792841
2017-12-10T02:40:17.769211: step 1552, loss 0.217577, acc 0.90625, prec 0.0454382, recall 0.792841
2017-12-10T02:40:18.035955: step 1553, loss 0.250677, acc 0.90625, prec 0.0454802, recall 0.793026
2017-12-10T02:40:18.300856: step 1554, loss 0.275395, acc 0.890625, prec 0.045472, recall 0.793026
2017-12-10T02:40:18.561185: step 1555, loss 0.525836, acc 0.8125, prec 0.045507, recall 0.793211
2017-12-10T02:40:18.827754: step 1556, loss 2.00485, acc 0.828125, prec 0.0454953, recall 0.792857
2017-12-10T02:40:19.089856: step 1557, loss 0.979098, acc 0.734375, prec 0.0454755, recall 0.792857
2017-12-10T02:40:19.349989: step 1558, loss 0.521669, acc 0.8125, prec 0.0454615, recall 0.792857
2017-12-10T02:40:19.612005: step 1559, loss 0.434321, acc 0.828125, prec 0.0454732, recall 0.79295
2017-12-10T02:40:19.886113: step 1560, loss 0.442782, acc 0.8125, prec 0.0454836, recall 0.793042
2017-12-10T02:40:20.153573: step 1561, loss 0.686629, acc 0.828125, prec 0.0455196, recall 0.793226
2017-12-10T02:40:20.413070: step 1562, loss 0.239808, acc 0.875, prec 0.0455347, recall 0.793319
2017-12-10T02:40:20.686891: step 1563, loss 0.351455, acc 0.84375, prec 0.0455719, recall 0.793502
2017-12-10T02:40:20.950955: step 1564, loss 0.432956, acc 0.859375, prec 0.0455614, recall 0.793502
2017-12-10T02:40:21.210892: step 1565, loss 0.694373, acc 0.84375, prec 0.0455741, recall 0.793594
2017-12-10T02:40:21.483730: step 1566, loss 0.496534, acc 0.890625, prec 0.0456391, recall 0.793869
2017-12-10T02:40:21.745371: step 1567, loss 0.591517, acc 0.859375, prec 0.045653, recall 0.793961
2017-12-10T02:40:22.010935: step 1568, loss 0.440676, acc 0.875, prec 0.0456437, recall 0.793961
2017-12-10T02:40:22.276222: step 1569, loss 0.496316, acc 0.8125, prec 0.0456784, recall 0.794144
2017-12-10T02:40:22.540007: step 1570, loss 0.419554, acc 0.84375, prec 0.0456911, recall 0.794235
2017-12-10T02:40:22.807383: step 1571, loss 0.488261, acc 0.90625, prec 0.0456841, recall 0.794235
2017-12-10T02:40:23.069434: step 1572, loss 0.476098, acc 0.890625, prec 0.045676, recall 0.794235
2017-12-10T02:40:23.333179: step 1573, loss 0.184947, acc 0.90625, prec 0.0456933, recall 0.794326
2017-12-10T02:40:23.594240: step 1574, loss 0.390471, acc 0.921875, prec 0.0457118, recall 0.794417
2017-12-10T02:40:23.869343: step 1575, loss 0.100704, acc 0.96875, prec 0.0457095, recall 0.794417
2017-12-10T02:40:24.136977: step 1576, loss 0.0551711, acc 0.96875, prec 0.0457315, recall 0.794508
2017-12-10T02:40:24.403119: step 1577, loss 0.174088, acc 0.96875, prec 0.0457291, recall 0.794508
2017-12-10T02:40:24.666404: step 1578, loss 0.210521, acc 0.921875, prec 0.0457233, recall 0.794508
2017-12-10T02:40:24.935419: step 1579, loss 4.77224, acc 0.859375, prec 0.0457383, recall 0.794248
2017-12-10T02:40:25.199842: step 1580, loss 0.474172, acc 0.953125, prec 0.0457591, recall 0.794339
2017-12-10T02:40:25.465909: step 1581, loss 1.85365, acc 0.90625, prec 0.0457533, recall 0.793988
2017-12-10T02:40:25.737494: step 1582, loss 0.165167, acc 0.921875, prec 0.0457475, recall 0.793988
2017-12-10T02:40:26.003720: step 1583, loss 0.231857, acc 0.9375, prec 0.0457428, recall 0.793988
2017-12-10T02:40:26.270324: step 1584, loss 0.171365, acc 0.90625, prec 0.0457358, recall 0.793988
2017-12-10T02:40:26.544037: step 1585, loss 0.166212, acc 0.953125, prec 0.0457323, recall 0.793988
2017-12-10T02:40:26.804992: step 1586, loss 0.342189, acc 0.890625, prec 0.0457242, recall 0.793988
2017-12-10T02:40:27.065284: step 1587, loss 0.595966, acc 0.84375, prec 0.0457125, recall 0.793988
2017-12-10T02:40:27.325399: step 1588, loss 0.193984, acc 0.953125, prec 0.0457333, recall 0.794079
2017-12-10T02:40:27.588054: step 1589, loss 0.295759, acc 0.890625, prec 0.0457252, recall 0.794079
2017-12-10T02:40:27.849396: step 1590, loss 0.666039, acc 0.796875, prec 0.0457101, recall 0.794079
2017-12-10T02:40:28.117194: step 1591, loss 0.477537, acc 0.875, prec 0.0457008, recall 0.794079
2017-12-10T02:40:28.380624: step 1592, loss 0.345201, acc 0.90625, prec 0.0457181, recall 0.79417
2017-12-10T02:40:28.644737: step 1593, loss 0.537123, acc 0.828125, prec 0.0457295, recall 0.794261
2017-12-10T02:40:28.905587: step 1594, loss 0.310088, acc 0.9375, prec 0.0457249, recall 0.794261
2017-12-10T02:40:29.172730: step 1595, loss 0.31106, acc 0.90625, prec 0.0457422, recall 0.794351
2017-12-10T02:40:29.437005: step 1596, loss 8.13477, acc 0.953125, prec 0.0457398, recall 0.794001
2017-12-10T02:40:29.716686: step 1597, loss 0.285283, acc 0.875, prec 0.045779, recall 0.794182
2017-12-10T02:40:29.983905: step 1598, loss 0.282236, acc 0.875, prec 0.0457697, recall 0.794182
2017-12-10T02:40:30.258020: step 1599, loss 4.24338, acc 0.84375, prec 0.0457593, recall 0.793833
2017-12-10T02:40:30.533060: step 1600, loss 0.124205, acc 0.953125, prec 0.0458042, recall 0.794014
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-1600

2017-12-10T02:40:31.940862: step 1601, loss 0.641293, acc 0.78125, prec 0.045788, recall 0.794014
2017-12-10T02:40:32.201552: step 1602, loss 4.05636, acc 0.828125, prec 0.0457763, recall 0.793665
2017-12-10T02:40:32.476191: step 1603, loss 0.482918, acc 0.828125, prec 0.045812, recall 0.793846
2017-12-10T02:40:32.742562: step 1604, loss 1.09278, acc 0.765625, prec 0.0458913, recall 0.794208
2017-12-10T02:40:33.007580: step 1605, loss 1.29412, acc 0.65625, prec 0.0458899, recall 0.794298
2017-12-10T02:40:33.268748: step 1606, loss 1.2217, acc 0.6875, prec 0.0458667, recall 0.794298
2017-12-10T02:40:33.534585: step 1607, loss 1.80347, acc 0.640625, prec 0.0458641, recall 0.794388
2017-12-10T02:40:33.798233: step 1608, loss 1.66782, acc 0.609375, prec 0.0458351, recall 0.794388
2017-12-10T02:40:34.061423: step 1609, loss 1.02911, acc 0.640625, prec 0.0458085, recall 0.794388
2017-12-10T02:40:34.324729: step 1610, loss 1.60085, acc 0.625, prec 0.0457807, recall 0.794388
2017-12-10T02:40:34.587650: step 1611, loss 1.39812, acc 0.671875, prec 0.0458528, recall 0.794748
2017-12-10T02:40:34.848502: step 1612, loss 0.778471, acc 0.71875, prec 0.045856, recall 0.794838
2017-12-10T02:40:35.108871: step 1613, loss 0.953373, acc 0.6875, prec 0.0458329, recall 0.794838
2017-12-10T02:40:35.369616: step 1614, loss 0.994858, acc 0.8125, prec 0.0458672, recall 0.795017
2017-12-10T02:40:35.638873: step 1615, loss 0.585094, acc 0.828125, prec 0.0458544, recall 0.795017
2017-12-10T02:40:35.899842: step 1616, loss 0.265819, acc 0.90625, prec 0.0458716, recall 0.795107
2017-12-10T02:40:36.162021: step 1617, loss 0.207128, acc 0.9375, prec 0.0458669, recall 0.795107
2017-12-10T02:40:36.433093: step 1618, loss 0.912973, acc 0.796875, prec 0.0459, recall 0.795286
2017-12-10T02:40:36.698815: step 1619, loss 0.227369, acc 0.9375, prec 0.0458954, recall 0.795286
2017-12-10T02:40:36.962490: step 1620, loss 2.81491, acc 0.84375, prec 0.045885, recall 0.794939
2017-12-10T02:40:37.227236: step 1621, loss 0.204444, acc 0.953125, prec 0.0458815, recall 0.794939
2017-12-10T02:40:37.489448: step 1622, loss 0.182059, acc 0.9375, prec 0.0459009, recall 0.795028
2017-12-10T02:40:37.750710: step 1623, loss 0.202298, acc 0.9375, prec 0.0459203, recall 0.795118
2017-12-10T02:40:38.010445: step 1624, loss 0.189443, acc 0.953125, prec 0.0459408, recall 0.795207
2017-12-10T02:40:38.270536: step 1625, loss 0.159962, acc 0.9375, prec 0.0460083, recall 0.795474
2017-12-10T02:40:38.531932: step 1626, loss 0.474079, acc 0.890625, prec 0.0460002, recall 0.795474
2017-12-10T02:40:38.797195: step 1627, loss 0.298478, acc 0.890625, prec 0.0460161, recall 0.795563
2017-12-10T02:40:39.058823: step 1628, loss 0.309832, acc 0.953125, prec 0.0460606, recall 0.795741
2017-12-10T02:40:39.324370: step 1629, loss 3.5869, acc 0.9375, prec 0.0460571, recall 0.795395
2017-12-10T02:40:39.586633: step 1630, loss 0.492265, acc 0.890625, prec 0.046097, recall 0.795573
2017-12-10T02:40:39.851843: step 1631, loss 0.752931, acc 0.984375, prec 0.0461198, recall 0.795662
2017-12-10T02:40:40.117278: step 1632, loss 0.116486, acc 0.953125, prec 0.0461403, recall 0.79575
2017-12-10T02:40:40.379843: step 1633, loss 0.342274, acc 0.90625, prec 0.0461573, recall 0.795839
2017-12-10T02:40:40.644715: step 1634, loss 0.287532, acc 0.859375, prec 0.0461709, recall 0.795927
2017-12-10T02:40:40.913311: step 1635, loss 0.609412, acc 0.859375, prec 0.0461604, recall 0.795927
2017-12-10T02:40:41.170964: step 1636, loss 0.250842, acc 0.890625, prec 0.0461523, recall 0.795927
2017-12-10T02:40:41.438892: step 1637, loss 0.49862, acc 0.84375, prec 0.0461407, recall 0.795927
2017-12-10T02:40:41.700739: step 1638, loss 0.455249, acc 0.875, prec 0.0461314, recall 0.795927
2017-12-10T02:40:41.960458: step 1639, loss 0.558235, acc 0.828125, prec 0.0461666, recall 0.796104
2017-12-10T02:40:42.218425: step 1640, loss 0.699435, acc 0.8125, prec 0.0461766, recall 0.796192
2017-12-10T02:40:42.475590: step 1641, loss 0.431007, acc 0.890625, prec 0.0462164, recall 0.796368
2017-12-10T02:40:42.739973: step 1642, loss 0.719413, acc 0.859375, prec 0.0462299, recall 0.796456
2017-12-10T02:40:42.995106: step 1643, loss 0.235774, acc 0.90625, prec 0.0462468, recall 0.796544
2017-12-10T02:40:43.265309: step 1644, loss 0.265377, acc 0.9375, prec 0.0462422, recall 0.796544
2017-12-10T02:40:43.529997: step 1645, loss 6.277, acc 0.859375, prec 0.0462568, recall 0.796288
2017-12-10T02:40:43.797418: step 1646, loss 3.50468, acc 0.796875, prec 0.0462429, recall 0.795945
2017-12-10T02:40:44.062635: step 1647, loss 2.05602, acc 0.75, prec 0.0462494, recall 0.79569
2017-12-10T02:40:44.327169: step 1648, loss 0.458741, acc 0.859375, prec 0.0462629, recall 0.795778
2017-12-10T02:40:44.594250: step 1649, loss 0.945106, acc 0.78125, prec 0.0462706, recall 0.795866
2017-12-10T02:40:44.857008: step 1650, loss 0.899673, acc 0.75, prec 0.046252, recall 0.795866
2017-12-10T02:40:45.122694: step 1651, loss 0.880442, acc 0.765625, prec 0.0462585, recall 0.795954
2017-12-10T02:40:45.384409: step 1652, loss 1.02554, acc 0.8125, prec 0.0462685, recall 0.796041
2017-12-10T02:40:45.648979: step 1653, loss 1.45442, acc 0.609375, prec 0.0462634, recall 0.796129
2017-12-10T02:40:45.912659: step 1654, loss 1.72109, acc 0.609375, prec 0.0462345, recall 0.796129
2017-12-10T02:40:46.177169: step 1655, loss 1.0149, acc 0.71875, prec 0.0462852, recall 0.796392
2017-12-10T02:40:46.440819: step 1656, loss 1.28368, acc 0.734375, prec 0.0462656, recall 0.796392
2017-12-10T02:40:46.706152: step 1657, loss 1.78846, acc 0.71875, prec 0.0462686, recall 0.796479
2017-12-10T02:40:46.963655: step 1658, loss 0.905697, acc 0.765625, prec 0.0462513, recall 0.796479
2017-12-10T02:40:47.223059: step 1659, loss 0.805529, acc 0.796875, prec 0.0462363, recall 0.796479
2017-12-10T02:40:47.486784: step 1660, loss 0.893485, acc 0.75, prec 0.0462179, recall 0.796479
2017-12-10T02:40:47.743589: step 1661, loss 0.500646, acc 0.859375, prec 0.0462313, recall 0.796567
2017-12-10T02:40:48.002929: step 1662, loss 0.783762, acc 0.765625, prec 0.046214, recall 0.796567
2017-12-10T02:40:48.265635: step 1663, loss 0.446845, acc 0.828125, prec 0.0462013, recall 0.796567
2017-12-10T02:40:48.531731: step 1664, loss 0.313115, acc 0.84375, prec 0.0462136, recall 0.796654
2017-12-10T02:40:48.796477: step 1665, loss 0.411031, acc 0.90625, prec 0.0462067, recall 0.796654
2017-12-10T02:40:49.060802: step 1666, loss 0.158831, acc 0.953125, prec 0.0462507, recall 0.796828
2017-12-10T02:40:49.322326: step 1667, loss 0.081142, acc 0.96875, prec 0.0462484, recall 0.796828
2017-12-10T02:40:49.588922: step 1668, loss 2.0351, acc 0.890625, prec 0.0462652, recall 0.796574
2017-12-10T02:40:49.856731: step 1669, loss 0.119918, acc 0.984375, prec 0.0462878, recall 0.796661
2017-12-10T02:40:50.122682: step 1670, loss 0.271797, acc 0.9375, prec 0.0463069, recall 0.796748
2017-12-10T02:40:50.389970: step 1671, loss 0.472153, acc 0.875, prec 0.0462977, recall 0.796748
2017-12-10T02:40:50.652863: step 1672, loss 0.0914531, acc 0.953125, prec 0.0462942, recall 0.796748
2017-12-10T02:40:50.922556: step 1673, loss 0.276246, acc 0.9375, prec 0.0462896, recall 0.796748
2017-12-10T02:40:51.185305: step 1674, loss 0.218507, acc 0.9375, prec 0.0463087, recall 0.796835
2017-12-10T02:40:51.448968: step 1675, loss 0.543189, acc 0.9375, prec 0.0463278, recall 0.796922
2017-12-10T02:40:51.719707: step 1676, loss 0.174755, acc 0.953125, prec 0.0463481, recall 0.797009
2017-12-10T02:40:51.981094: step 1677, loss 0.36753, acc 0.921875, prec 0.0463423, recall 0.797009
2017-12-10T02:40:52.246148: step 1678, loss 0.0384325, acc 0.96875, prec 0.04634, recall 0.797009
2017-12-10T02:40:52.513578: step 1679, loss 0.0980338, acc 0.953125, prec 0.0463366, recall 0.797009
2017-12-10T02:40:52.776841: step 1680, loss 1.29425, acc 0.921875, prec 0.0463556, recall 0.796755
2017-12-10T02:40:53.043349: step 1681, loss 0.183948, acc 0.921875, prec 0.0463736, recall 0.796842
2017-12-10T02:40:53.306777: step 1682, loss 0.319842, acc 0.90625, prec 0.0463903, recall 0.796928
2017-12-10T02:40:53.566347: step 1683, loss 0.0969087, acc 0.96875, prec 0.0464354, recall 0.797101
2017-12-10T02:40:53.828966: step 1684, loss 11.8779, acc 0.9375, prec 0.0464556, recall 0.796848
2017-12-10T02:40:54.101665: step 1685, loss 1.64991, acc 0.96875, prec 0.0465007, recall 0.797021
2017-12-10T02:40:54.378792: step 1686, loss 0.203907, acc 0.9375, prec 0.046496, recall 0.797021
2017-12-10T02:40:54.649378: step 1687, loss 0.503824, acc 0.890625, prec 0.046488, recall 0.797021
2017-12-10T02:40:54.915248: step 1688, loss 0.258294, acc 0.90625, prec 0.0465047, recall 0.797108
2017-12-10T02:40:55.180672: step 1689, loss 1.15857, acc 0.9375, prec 0.0465474, recall 0.79728
2017-12-10T02:40:55.447000: step 1690, loss 0.39216, acc 0.828125, prec 0.0465347, recall 0.79728
2017-12-10T02:40:55.720998: step 1691, loss 0.511046, acc 0.890625, prec 0.0465739, recall 0.797452
2017-12-10T02:40:55.982863: step 1692, loss 0.657066, acc 0.796875, prec 0.0465825, recall 0.797538
2017-12-10T02:40:56.245076: step 1693, loss 0.883818, acc 0.71875, prec 0.0465618, recall 0.797538
2017-12-10T02:40:56.523290: step 1694, loss 0.707116, acc 0.796875, prec 0.0466176, recall 0.797796
2017-12-10T02:40:56.798048: step 1695, loss 0.421379, acc 0.84375, prec 0.0466533, recall 0.797967
2017-12-10T02:40:57.056677: step 1696, loss 0.575469, acc 0.8125, prec 0.0466866, recall 0.798138
2017-12-10T02:40:57.314181: step 1697, loss 0.625403, acc 0.796875, prec 0.0467424, recall 0.798394
2017-12-10T02:40:57.580848: step 1698, loss 0.670235, acc 0.796875, prec 0.0467274, recall 0.798394
2017-12-10T02:40:57.846953: step 1699, loss 0.624496, acc 0.8125, prec 0.0467371, recall 0.798479
2017-12-10T02:40:58.108333: step 1700, loss 0.749319, acc 0.84375, prec 0.0467255, recall 0.798479
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-1700

2017-12-10T02:40:59.526132: step 1701, loss 0.753165, acc 0.71875, prec 0.0467283, recall 0.798564
2017-12-10T02:40:59.796928: step 1702, loss 0.80966, acc 0.78125, prec 0.0467121, recall 0.798564
2017-12-10T02:41:00.062874: step 1703, loss 0.896296, acc 0.765625, prec 0.0466948, recall 0.798564
2017-12-10T02:41:00.331785: step 1704, loss 4.20342, acc 0.828125, prec 0.0467068, recall 0.798312
2017-12-10T02:41:00.605527: step 1705, loss 0.332085, acc 0.84375, prec 0.0467188, recall 0.798397
2017-12-10T02:41:00.868265: step 1706, loss 0.457226, acc 0.828125, prec 0.0467297, recall 0.798482
2017-12-10T02:41:01.126479: step 1707, loss 0.36827, acc 0.859375, prec 0.0467898, recall 0.798737
2017-12-10T02:41:01.395851: step 1708, loss 0.678189, acc 0.84375, prec 0.0468253, recall 0.798906
2017-12-10T02:41:01.665924: step 1709, loss 0.611639, acc 0.75, prec 0.0468303, recall 0.798991
2017-12-10T02:41:01.935909: step 1710, loss 0.248368, acc 0.90625, prec 0.0468704, recall 0.79916
2017-12-10T02:41:02.206890: step 1711, loss 0.699127, acc 0.859375, prec 0.0469069, recall 0.799328
2017-12-10T02:41:02.473182: step 1712, loss 0.450871, acc 0.8125, prec 0.04694, recall 0.799497
2017-12-10T02:41:02.732856: step 1713, loss 0.45351, acc 0.859375, prec 0.0469296, recall 0.799497
2017-12-10T02:41:02.992665: step 1714, loss 0.289226, acc 0.859375, prec 0.0469192, recall 0.799497
2017-12-10T02:41:03.257342: step 1715, loss 2.73876, acc 0.796875, prec 0.0469054, recall 0.799161
2017-12-10T02:41:03.525486: step 1716, loss 0.480568, acc 0.890625, prec 0.0469442, recall 0.79933
2017-12-10T02:41:03.790593: step 1717, loss 0.251244, acc 0.890625, prec 0.0469361, recall 0.79933
2017-12-10T02:41:04.045717: step 1718, loss 0.510133, acc 0.859375, prec 0.0469726, recall 0.799498
2017-12-10T02:41:04.308944: step 1719, loss 0.559727, acc 0.859375, prec 0.0469622, recall 0.799498
2017-12-10T02:41:04.580815: step 1720, loss 0.228088, acc 0.921875, prec 0.0469564, recall 0.799498
2017-12-10T02:41:04.853224: step 1721, loss 0.202397, acc 0.9375, prec 0.0469518, recall 0.799498
2017-12-10T02:41:05.115478: step 1722, loss 0.624845, acc 0.84375, prec 0.0469871, recall 0.799665
2017-12-10T02:41:05.386508: step 1723, loss 0.42776, acc 0.84375, prec 0.0469756, recall 0.799665
2017-12-10T02:41:05.646280: step 1724, loss 0.772633, acc 0.875, prec 0.0470132, recall 0.799833
2017-12-10T02:41:05.910728: step 1725, loss 0.432397, acc 0.890625, prec 0.0470051, recall 0.799833
2017-12-10T02:41:06.172363: step 1726, loss 0.45273, acc 0.859375, prec 0.0469947, recall 0.799833
2017-12-10T02:41:06.433823: step 1727, loss 0.236622, acc 0.921875, prec 0.0470357, recall 0.8
2017-12-10T02:41:06.700447: step 1728, loss 0.224662, acc 0.953125, prec 0.0470556, recall 0.800083
2017-12-10T02:41:06.965258: step 1729, loss 0.511775, acc 0.796875, prec 0.0470874, recall 0.80025
2017-12-10T02:41:07.226769: step 1730, loss 0.238156, acc 0.9375, prec 0.0470828, recall 0.80025
2017-12-10T02:41:07.488111: step 1731, loss 0.964569, acc 0.921875, prec 0.0471238, recall 0.800417
2017-12-10T02:41:07.756212: step 1732, loss 1.14656, acc 0.921875, prec 0.0471414, recall 0.8005
2017-12-10T02:41:08.022458: step 1733, loss 1.9804, acc 0.9375, prec 0.0471613, recall 0.80025
2017-12-10T02:41:08.291769: step 1734, loss 0.489341, acc 0.921875, prec 0.0472022, recall 0.800416
2017-12-10T02:41:08.558069: step 1735, loss 1.27373, acc 0.9375, prec 0.0472443, recall 0.800582
2017-12-10T02:41:08.830889: step 1736, loss 0.484546, acc 0.828125, prec 0.0472549, recall 0.800664
2017-12-10T02:41:09.100867: step 1737, loss 0.151865, acc 0.953125, prec 0.0472514, recall 0.800664
2017-12-10T02:41:09.367737: step 1738, loss 0.318489, acc 0.90625, prec 0.0472445, recall 0.800664
2017-12-10T02:41:09.633989: step 1739, loss 0.91962, acc 0.765625, prec 0.0472505, recall 0.800747
2017-12-10T02:41:09.894653: step 1740, loss 0.724597, acc 0.75, prec 0.0472553, recall 0.80083
2017-12-10T02:41:10.163856: step 1741, loss 0.884288, acc 0.78125, prec 0.0472857, recall 0.800995
2017-12-10T02:41:10.424471: step 1742, loss 0.739174, acc 0.765625, prec 0.0472917, recall 0.801077
2017-12-10T02:41:10.688083: step 1743, loss 0.603198, acc 0.859375, prec 0.0472813, recall 0.801077
2017-12-10T02:41:10.951251: step 1744, loss 0.828408, acc 0.765625, prec 0.0472639, recall 0.801077
2017-12-10T02:41:11.214539: step 1745, loss 0.839224, acc 0.796875, prec 0.0473421, recall 0.801407
2017-12-10T02:41:11.487184: step 1746, loss 0.775724, acc 0.78125, prec 0.0473957, recall 0.801653
2017-12-10T02:41:11.753238: step 1747, loss 0.65626, acc 0.828125, prec 0.0473829, recall 0.801653
2017-12-10T02:41:12.019484: step 1748, loss 0.546926, acc 0.84375, prec 0.0473946, recall 0.801735
2017-12-10T02:41:12.285040: step 1749, loss 0.309801, acc 0.890625, prec 0.0473865, recall 0.801735
2017-12-10T02:41:12.546058: step 1750, loss 0.464286, acc 0.859375, prec 0.0473994, recall 0.801817
2017-12-10T02:41:12.816774: step 1751, loss 0.563895, acc 0.859375, prec 0.047389, recall 0.801817
2017-12-10T02:41:13.077303: step 1752, loss 0.399493, acc 0.890625, prec 0.0474274, recall 0.80198
2017-12-10T02:41:13.342192: step 1753, loss 0.134945, acc 0.953125, prec 0.0474239, recall 0.80198
2017-12-10T02:41:13.608446: step 1754, loss 0.616487, acc 0.84375, prec 0.0474356, recall 0.802062
2017-12-10T02:41:13.872437: step 1755, loss 0.0975613, acc 0.96875, prec 0.0474332, recall 0.802062
2017-12-10T02:41:14.141276: step 1756, loss 0.540434, acc 0.875, prec 0.0474472, recall 0.802143
2017-12-10T02:41:14.407702: step 1757, loss 0.207684, acc 0.890625, prec 0.0474623, recall 0.802225
2017-12-10T02:41:14.675499: step 1758, loss 2.60754, acc 0.953125, prec 0.0475065, recall 0.802058
2017-12-10T02:41:14.945609: step 1759, loss 0.027466, acc 1, prec 0.0475065, recall 0.802058
2017-12-10T02:41:15.213017: step 1760, loss 0.209564, acc 0.96875, prec 0.0475274, recall 0.802139
2017-12-10T02:41:15.475700: step 1761, loss 2.54767, acc 0.921875, prec 0.0475459, recall 0.801891
2017-12-10T02:41:15.750963: step 1762, loss 1.10811, acc 0.921875, prec 0.0475634, recall 0.801972
2017-12-10T02:41:16.014499: step 1763, loss 0.539499, acc 0.890625, prec 0.0475552, recall 0.801972
2017-12-10T02:41:16.283349: step 1764, loss 0.518707, acc 0.890625, prec 0.0475471, recall 0.801972
2017-12-10T02:41:16.549094: step 1765, loss 0.499296, acc 0.859375, prec 0.0475367, recall 0.801972
2017-12-10T02:41:16.812334: step 1766, loss 0.462984, acc 0.890625, prec 0.0475982, recall 0.802216
2017-12-10T02:41:17.076680: step 1767, loss 0.462066, acc 0.859375, prec 0.0475878, recall 0.802216
2017-12-10T02:41:17.351954: step 1768, loss 0.511067, acc 0.859375, prec 0.0476237, recall 0.802378
2017-12-10T02:41:17.612947: step 1769, loss 0.832173, acc 0.796875, prec 0.0476086, recall 0.802378
2017-12-10T02:41:17.878420: step 1770, loss 1.00764, acc 0.8125, prec 0.0476179, recall 0.802459
2017-12-10T02:41:18.139632: step 1771, loss 1.06665, acc 0.734375, prec 0.0475982, recall 0.802459
2017-12-10T02:41:18.434080: step 1772, loss 0.932442, acc 0.796875, prec 0.0476295, recall 0.802621
2017-12-10T02:41:18.707638: step 1773, loss 0.534232, acc 0.890625, prec 0.0476214, recall 0.802621
2017-12-10T02:41:18.964760: step 1774, loss 0.456654, acc 0.828125, prec 0.0476086, recall 0.802621
2017-12-10T02:41:19.241435: step 1775, loss 0.642872, acc 0.84375, prec 0.0475971, recall 0.802621
2017-12-10T02:41:19.508154: step 1776, loss 0.105904, acc 0.984375, prec 0.0476653, recall 0.802863
2017-12-10T02:41:19.775719: step 1777, loss 0.34101, acc 0.890625, prec 0.0476572, recall 0.802863
2017-12-10T02:41:20.039053: step 1778, loss 0.55621, acc 0.859375, prec 0.0476468, recall 0.802863
2017-12-10T02:41:20.299339: step 1779, loss 5.48048, acc 0.859375, prec 0.0477069, recall 0.802777
2017-12-10T02:41:20.571105: step 1780, loss 1.37821, acc 0.890625, prec 0.0477681, recall 0.803018
2017-12-10T02:41:20.833067: step 1781, loss 0.481937, acc 0.84375, prec 0.0477565, recall 0.803018
2017-12-10T02:41:21.099051: step 1782, loss 0.313527, acc 0.90625, prec 0.0477495, recall 0.803018
2017-12-10T02:41:21.364136: step 1783, loss 0.175621, acc 0.9375, prec 0.0477449, recall 0.803018
2017-12-10T02:41:21.629275: step 1784, loss 0.354311, acc 0.890625, prec 0.0477599, recall 0.803098
2017-12-10T02:41:21.895548: step 1785, loss 0.71464, acc 0.859375, prec 0.0477495, recall 0.803098
2017-12-10T02:41:22.158028: step 1786, loss 0.594176, acc 0.8125, prec 0.0477817, recall 0.803259
2017-12-10T02:41:22.421961: step 1787, loss 0.549854, acc 0.859375, prec 0.0477713, recall 0.803259
2017-12-10T02:41:22.683222: step 1788, loss 0.584952, acc 0.828125, prec 0.0477586, recall 0.803259
2017-12-10T02:41:22.952429: step 1789, loss 0.647624, acc 0.84375, prec 0.0478162, recall 0.803499
2017-12-10T02:41:23.220450: step 1790, loss 0.333042, acc 0.859375, prec 0.0478058, recall 0.803499
2017-12-10T02:41:23.491847: step 1791, loss 1.47601, acc 0.828125, prec 0.0478391, recall 0.803659
2017-12-10T02:41:23.755127: step 1792, loss 1.75982, acc 0.8125, prec 0.0478264, recall 0.803332
2017-12-10T02:41:24.018359: step 1793, loss 0.650237, acc 0.8125, prec 0.0478125, recall 0.803332
2017-12-10T02:41:24.280606: step 1794, loss 0.799663, acc 0.8125, prec 0.0477987, recall 0.803332
2017-12-10T02:41:24.540852: step 1795, loss 0.619154, acc 0.796875, prec 0.0478066, recall 0.803412
2017-12-10T02:41:24.805806: step 1796, loss 0.436946, acc 0.859375, prec 0.0477962, recall 0.803412
2017-12-10T02:41:25.061931: step 1797, loss 0.722156, acc 0.78125, prec 0.0478031, recall 0.803492
2017-12-10T02:41:25.326075: step 1798, loss 1.23209, acc 0.671875, prec 0.0477789, recall 0.803492
2017-12-10T02:41:25.589873: step 1799, loss 0.871477, acc 0.8125, prec 0.047765, recall 0.803492
2017-12-10T02:41:25.849153: step 1800, loss 0.974757, acc 0.78125, prec 0.0478178, recall 0.803731

Evaluation:
2017-12-10T02:41:33.427196: step 1800, loss 1.67877, acc 0.824606, prec 0.0485551, recall 0.801141

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-1800

2017-12-10T02:41:34.798012: step 1801, loss 0.554985, acc 0.859375, prec 0.048545, recall 0.801141
2017-12-10T02:41:35.061212: step 1802, loss 0.486787, acc 0.828125, prec 0.0485327, recall 0.801141
2017-12-10T02:41:35.334547: step 1803, loss 0.545068, acc 0.78125, prec 0.048539, recall 0.801216
2017-12-10T02:41:35.599949: step 1804, loss 0.602868, acc 0.828125, prec 0.0485486, recall 0.801292
2017-12-10T02:41:35.859644: step 1805, loss 0.784411, acc 0.875, prec 0.0486054, recall 0.801518
2017-12-10T02:41:36.123077: step 1806, loss 1.13843, acc 0.921875, prec 0.0486435, recall 0.801669
2017-12-10T02:41:36.396742: step 1807, loss 0.519477, acc 0.875, prec 0.0486346, recall 0.801669
2017-12-10T02:41:36.660346: step 1808, loss 0.460403, acc 0.890625, prec 0.0486486, recall 0.801744
2017-12-10T02:41:36.922973: step 1809, loss 0.191783, acc 0.921875, prec 0.0486649, recall 0.801819
2017-12-10T02:41:37.189333: step 1810, loss 0.193259, acc 0.9375, prec 0.0486605, recall 0.801819
2017-12-10T02:41:37.454789: step 1811, loss 0.36051, acc 0.890625, prec 0.0486745, recall 0.801894
2017-12-10T02:41:37.721647: step 1812, loss 0.366012, acc 0.90625, prec 0.0486897, recall 0.801969
2017-12-10T02:41:37.991442: step 1813, loss 0.187826, acc 0.9375, prec 0.0487289, recall 0.802119
2017-12-10T02:41:38.256308: step 1814, loss 0.205519, acc 0.96875, prec 0.0487485, recall 0.802194
2017-12-10T02:41:38.520849: step 1815, loss 0.388497, acc 0.890625, prec 0.0487407, recall 0.802194
2017-12-10T02:41:38.784242: step 1816, loss 0.308394, acc 0.890625, prec 0.0487329, recall 0.802194
2017-12-10T02:41:39.044647: step 1817, loss 1.73923, acc 0.9375, prec 0.0487295, recall 0.80189
2017-12-10T02:41:39.315835: step 1818, loss 0.114651, acc 1, prec 0.0487514, recall 0.801965
2017-12-10T02:41:39.585842: step 1819, loss 1.6932, acc 0.921875, prec 0.0487469, recall 0.801662
2017-12-10T02:41:39.858232: step 1820, loss 0.145015, acc 0.953125, prec 0.0487435, recall 0.801662
2017-12-10T02:41:40.120066: step 1821, loss 0.393384, acc 0.90625, prec 0.0487805, recall 0.801812
2017-12-10T02:41:40.387073: step 1822, loss 0.484279, acc 0.875, prec 0.0487715, recall 0.801812
2017-12-10T02:41:40.648965: step 1823, loss 0.994344, acc 0.921875, prec 0.0487878, recall 0.801887
2017-12-10T02:41:40.916721: step 1824, loss 0.517051, acc 0.828125, prec 0.0487973, recall 0.801962
2017-12-10T02:41:41.182464: step 1825, loss 6.52229, acc 0.859375, prec 0.0487883, recall 0.801659
2017-12-10T02:41:41.461068: step 1826, loss 0.718274, acc 0.859375, prec 0.0487783, recall 0.801659
2017-12-10T02:41:41.721614: step 1827, loss 0.746845, acc 0.796875, prec 0.0487637, recall 0.801659
2017-12-10T02:41:41.991041: step 1828, loss 0.903181, acc 0.765625, prec 0.0487687, recall 0.801734
2017-12-10T02:41:42.259664: step 1829, loss 1.17668, acc 0.6875, prec 0.0487464, recall 0.801734
2017-12-10T02:41:42.519986: step 1830, loss 0.962386, acc 0.75, prec 0.0487721, recall 0.801883
2017-12-10T02:41:42.785999: step 1831, loss 1.32673, acc 0.671875, prec 0.0487922, recall 0.802032
2017-12-10T02:41:43.047293: step 1832, loss 1.10526, acc 0.75, prec 0.0487961, recall 0.802107
2017-12-10T02:41:43.311083: step 1833, loss 1.75102, acc 0.671875, prec 0.0487944, recall 0.802181
2017-12-10T02:41:43.572322: step 1834, loss 1.19601, acc 0.6875, prec 0.0487721, recall 0.802181
2017-12-10T02:41:43.832757: step 1835, loss 1.28336, acc 0.828125, prec 0.0488033, recall 0.80233
2017-12-10T02:41:44.103871: step 1836, loss 0.64992, acc 0.828125, prec 0.0488128, recall 0.802404
2017-12-10T02:41:44.364425: step 1837, loss 0.764383, acc 0.828125, prec 0.048844, recall 0.802553
2017-12-10T02:41:44.625830: step 1838, loss 1.65972, acc 0.75, prec 0.0488913, recall 0.802775
2017-12-10T02:41:44.894148: step 1839, loss 0.433602, acc 0.890625, prec 0.0488835, recall 0.802775
2017-12-10T02:41:45.162547: step 1840, loss 0.430048, acc 0.875, prec 0.0488746, recall 0.802775
2017-12-10T02:41:45.425381: step 1841, loss 0.893701, acc 0.796875, prec 0.0488601, recall 0.802775
2017-12-10T02:41:45.688845: step 1842, loss 0.43113, acc 0.875, prec 0.0488512, recall 0.802775
2017-12-10T02:41:45.952559: step 1843, loss 0.527948, acc 0.84375, prec 0.04884, recall 0.802775
2017-12-10T02:41:46.216437: step 1844, loss 0.280502, acc 0.90625, prec 0.048855, recall 0.802849
2017-12-10T02:41:46.482172: step 1845, loss 0.212303, acc 0.90625, prec 0.0488483, recall 0.802849
2017-12-10T02:41:46.743068: step 1846, loss 0.503391, acc 0.828125, prec 0.0488795, recall 0.802996
2017-12-10T02:41:47.003938: step 1847, loss 0.183093, acc 0.890625, prec 0.0489367, recall 0.803217
2017-12-10T02:41:47.269158: step 1848, loss 0.44737, acc 0.890625, prec 0.0489289, recall 0.803217
2017-12-10T02:41:47.533474: step 1849, loss 0.0714468, acc 0.96875, prec 0.0489267, recall 0.803217
2017-12-10T02:41:47.796860: step 1850, loss 0.131794, acc 0.9375, prec 0.0489872, recall 0.803438
2017-12-10T02:41:48.069248: step 1851, loss 0.1376, acc 0.953125, prec 0.0490055, recall 0.803511
2017-12-10T02:41:48.332312: step 1852, loss 0.485135, acc 0.953125, prec 0.0490455, recall 0.803658
2017-12-10T02:41:48.595333: step 1853, loss 0.501382, acc 0.96875, prec 0.0490866, recall 0.803805
2017-12-10T02:41:48.859991: step 1854, loss 0.286933, acc 0.953125, prec 0.0490832, recall 0.803805
2017-12-10T02:41:49.125192: step 1855, loss 8.40481, acc 0.921875, prec 0.0491016, recall 0.803279
2017-12-10T02:41:49.389663: step 1856, loss 0.0789776, acc 0.953125, prec 0.0490982, recall 0.803279
2017-12-10T02:41:49.653032: step 1857, loss 0.361373, acc 0.859375, prec 0.0491098, recall 0.803352
2017-12-10T02:41:49.917020: step 1858, loss 0.908146, acc 0.796875, prec 0.0491169, recall 0.803425
2017-12-10T02:41:50.184042: step 1859, loss 0.486074, acc 0.8125, prec 0.0491035, recall 0.803425
2017-12-10T02:41:50.445744: step 1860, loss 0.321535, acc 0.875, prec 0.0491378, recall 0.803571
2017-12-10T02:41:50.718707: step 1861, loss 1.14522, acc 0.734375, prec 0.0491404, recall 0.803644
2017-12-10T02:41:50.984671: step 1862, loss 0.81604, acc 0.765625, prec 0.0491237, recall 0.803644
2017-12-10T02:41:51.241843: step 1863, loss 0.980182, acc 0.75, prec 0.0491274, recall 0.803717
2017-12-10T02:41:51.509540: step 1864, loss 0.865194, acc 0.71875, prec 0.0491289, recall 0.80379
2017-12-10T02:41:51.778906: step 1865, loss 1.47141, acc 0.625, prec 0.0491022, recall 0.80379
2017-12-10T02:41:52.038517: step 1866, loss 1.94308, acc 0.578125, prec 0.0490721, recall 0.80379
2017-12-10T02:41:52.303169: step 1867, loss 1.17544, acc 0.703125, prec 0.049051, recall 0.80379
2017-12-10T02:41:52.559995: step 1868, loss 1.38275, acc 0.734375, prec 0.0490536, recall 0.803863
2017-12-10T02:41:52.825389: step 1869, loss 1.06441, acc 0.765625, prec 0.0490585, recall 0.803936
2017-12-10T02:41:53.084011: step 1870, loss 0.978056, acc 0.765625, prec 0.049128, recall 0.804227
2017-12-10T02:41:53.349157: step 1871, loss 0.402678, acc 0.875, prec 0.0491191, recall 0.804227
2017-12-10T02:41:53.613738: step 1872, loss 0.39717, acc 0.859375, prec 0.0491091, recall 0.804227
2017-12-10T02:41:53.886432: step 1873, loss 0.730854, acc 0.859375, prec 0.0490991, recall 0.804227
2017-12-10T02:41:54.147478: step 1874, loss 0.487701, acc 0.859375, prec 0.0491106, recall 0.804299
2017-12-10T02:41:54.408239: step 1875, loss 0.154674, acc 0.90625, prec 0.0491039, recall 0.804299
2017-12-10T02:41:54.667558: step 1876, loss 0.224575, acc 0.90625, prec 0.0491188, recall 0.804372
2017-12-10T02:41:54.942014: step 1877, loss 0.493794, acc 0.875, prec 0.0491099, recall 0.804372
2017-12-10T02:41:55.204463: step 1878, loss 2.22204, acc 0.890625, prec 0.0491032, recall 0.804074
2017-12-10T02:41:55.472799: step 1879, loss 3.77798, acc 0.953125, prec 0.0491225, recall 0.803849
2017-12-10T02:41:55.736819: step 1880, loss 0.503089, acc 0.90625, prec 0.0491158, recall 0.803849
2017-12-10T02:41:56.008583: step 1881, loss 0.542189, acc 0.890625, prec 0.0491081, recall 0.803849
2017-12-10T02:41:56.268616: step 1882, loss 0.486371, acc 0.890625, prec 0.0491433, recall 0.803994
2017-12-10T02:41:56.546638: step 1883, loss 0.321984, acc 0.9375, prec 0.0491818, recall 0.804139
2017-12-10T02:41:56.810011: step 1884, loss 0.416615, acc 0.90625, prec 0.0491966, recall 0.804211
2017-12-10T02:41:57.077053: step 1885, loss 0.253236, acc 0.875, prec 0.0492092, recall 0.804284
2017-12-10T02:41:57.341833: step 1886, loss 0.310921, acc 0.890625, prec 0.0492444, recall 0.804428
2017-12-10T02:41:57.608223: step 1887, loss 0.23559, acc 0.90625, prec 0.0492377, recall 0.804428
2017-12-10T02:41:57.874897: step 1888, loss 0.25512, acc 0.96875, prec 0.049257, recall 0.8045
2017-12-10T02:41:58.145685: step 1889, loss 0.223678, acc 0.953125, prec 0.0492536, recall 0.8045
2017-12-10T02:41:58.410463: step 1890, loss 4.38413, acc 0.953125, prec 0.0492729, recall 0.804276
2017-12-10T02:41:58.677447: step 1891, loss 0.494521, acc 0.90625, prec 0.0492662, recall 0.804276
2017-12-10T02:41:58.937443: step 1892, loss 0.396855, acc 0.875, prec 0.0493002, recall 0.80442
2017-12-10T02:41:59.203636: step 1893, loss 1.79866, acc 0.875, prec 0.0493139, recall 0.804196
2017-12-10T02:41:59.467902: step 1894, loss 0.342945, acc 0.859375, prec 0.0493039, recall 0.804196
2017-12-10T02:41:59.735340: step 1895, loss 0.49244, acc 0.890625, prec 0.0492961, recall 0.804196
2017-12-10T02:42:00.001203: step 1896, loss 0.530129, acc 0.828125, prec 0.0493267, recall 0.80434
2017-12-10T02:42:00.265006: step 1897, loss 0.344351, acc 0.859375, prec 0.0493382, recall 0.804412
2017-12-10T02:42:00.525638: step 1898, loss 0.810957, acc 0.8125, prec 0.0493463, recall 0.804484
2017-12-10T02:42:00.782994: step 1899, loss 0.730903, acc 0.796875, prec 0.0493961, recall 0.804699
2017-12-10T02:42:01.044356: step 1900, loss 0.679218, acc 0.859375, prec 0.0494075, recall 0.804771
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-1900

2017-12-10T02:42:02.314712: step 1901, loss 0.760657, acc 0.8125, prec 0.0494369, recall 0.804914
2017-12-10T02:42:02.576513: step 1902, loss 0.344759, acc 0.90625, prec 0.0494303, recall 0.804914
2017-12-10T02:42:02.837048: step 1903, loss 0.80365, acc 0.8125, prec 0.0494383, recall 0.804985
2017-12-10T02:42:03.097565: step 1904, loss 0.602423, acc 0.78125, prec 0.0494441, recall 0.805057
2017-12-10T02:42:03.355930: step 1905, loss 0.334032, acc 0.890625, prec 0.0494363, recall 0.805057
2017-12-10T02:42:03.618106: step 1906, loss 0.384481, acc 0.84375, prec 0.0494466, recall 0.805128
2017-12-10T02:42:03.880685: step 1907, loss 1.40545, acc 0.8125, prec 0.049476, recall 0.805271
2017-12-10T02:42:04.149423: step 1908, loss 0.278381, acc 0.890625, prec 0.0494896, recall 0.805342
2017-12-10T02:42:04.410087: step 1909, loss 0.634052, acc 0.859375, prec 0.0494796, recall 0.805342
2017-12-10T02:42:04.673738: step 1910, loss 0.381867, acc 0.875, prec 0.0494707, recall 0.805342
2017-12-10T02:42:04.938479: step 1911, loss 0.391903, acc 0.9375, prec 0.0494662, recall 0.805342
2017-12-10T02:42:05.202588: step 1912, loss 0.537339, acc 0.890625, prec 0.0494585, recall 0.805342
2017-12-10T02:42:05.463915: step 1913, loss 11.1994, acc 0.859375, prec 0.0495136, recall 0.805261
2017-12-10T02:42:05.725998: step 1914, loss 0.116206, acc 0.96875, prec 0.0495328, recall 0.805332
2017-12-10T02:42:05.990983: step 1915, loss 0.234068, acc 0.9375, prec 0.0495283, recall 0.805332
2017-12-10T02:42:06.251726: step 1916, loss 0.376155, acc 0.875, prec 0.0495194, recall 0.805332
2017-12-10T02:42:06.511028: step 1917, loss 0.493128, acc 0.828125, prec 0.0495072, recall 0.805332
2017-12-10T02:42:06.770849: step 1918, loss 0.194129, acc 0.921875, prec 0.0495016, recall 0.805332
2017-12-10T02:42:07.033136: step 1919, loss 0.338308, acc 0.90625, prec 0.0495163, recall 0.805403
2017-12-10T02:42:07.291029: step 1920, loss 0.338919, acc 0.828125, prec 0.0495254, recall 0.805474
2017-12-10T02:42:07.550701: step 1921, loss 0.790704, acc 0.796875, prec 0.0495109, recall 0.805474
2017-12-10T02:42:07.817199: step 1922, loss 0.393434, acc 0.8125, prec 0.0495189, recall 0.805545
2017-12-10T02:42:08.076434: step 1923, loss 0.244301, acc 0.90625, prec 0.0495336, recall 0.805616
2017-12-10T02:42:08.336757: step 1924, loss 0.682303, acc 0.8125, prec 0.0495203, recall 0.805616
2017-12-10T02:42:08.604596: step 1925, loss 0.982953, acc 0.765625, prec 0.0495036, recall 0.805616
2017-12-10T02:42:08.867465: step 1926, loss 1.34309, acc 0.921875, prec 0.0495407, recall 0.805758
2017-12-10T02:42:09.131620: step 1927, loss 0.210518, acc 0.875, prec 0.0495318, recall 0.805758
2017-12-10T02:42:09.393361: step 1928, loss 0.28245, acc 0.875, prec 0.0495442, recall 0.805829
2017-12-10T02:42:09.655981: step 1929, loss 0.216262, acc 0.921875, prec 0.0495387, recall 0.805829
2017-12-10T02:42:09.914823: step 1930, loss 0.251356, acc 0.921875, prec 0.0495544, recall 0.8059
2017-12-10T02:42:10.183660: step 1931, loss 0.164513, acc 0.9375, prec 0.04955, recall 0.8059
2017-12-10T02:42:10.443174: step 1932, loss 0.410901, acc 0.875, prec 0.0495411, recall 0.8059
2017-12-10T02:42:10.702363: step 1933, loss 0.625502, acc 0.96875, prec 0.0495601, recall 0.80597
2017-12-10T02:42:10.965209: step 1934, loss 0.439759, acc 0.875, prec 0.0495513, recall 0.80597
2017-12-10T02:42:11.229758: step 1935, loss 0.181759, acc 0.953125, prec 0.0495479, recall 0.80597
2017-12-10T02:42:11.505862: step 1936, loss 0.152014, acc 0.9375, prec 0.0495435, recall 0.80597
2017-12-10T02:42:11.773633: step 1937, loss 0.0820339, acc 0.96875, prec 0.0495626, recall 0.806041
2017-12-10T02:42:12.033075: step 1938, loss 0.817904, acc 0.96875, prec 0.0496029, recall 0.806182
2017-12-10T02:42:12.302205: step 1939, loss 0.285718, acc 0.984375, prec 0.049623, recall 0.806252
2017-12-10T02:42:12.575178: step 1940, loss 0.287552, acc 0.890625, prec 0.0496152, recall 0.806252
2017-12-10T02:42:12.841840: step 1941, loss 0.135279, acc 0.96875, prec 0.049613, recall 0.806252
2017-12-10T02:42:13.107883: step 1942, loss 0.319507, acc 0.921875, prec 0.0496075, recall 0.806252
2017-12-10T02:42:13.369445: step 1943, loss 2.44233, acc 0.9375, prec 0.0496467, recall 0.8061
2017-12-10T02:42:13.631842: step 1944, loss 0.234757, acc 0.9375, prec 0.0496422, recall 0.8061
2017-12-10T02:42:13.890438: step 1945, loss 0.497741, acc 0.859375, prec 0.0496535, recall 0.806171
2017-12-10T02:42:14.148460: step 1946, loss 0.372746, acc 0.90625, prec 0.0496681, recall 0.806241
2017-12-10T02:42:14.413175: step 1947, loss 1.32298, acc 0.8125, prec 0.0497397, recall 0.806522
2017-12-10T02:42:14.670445: step 1948, loss 0.370515, acc 0.8125, prec 0.0497263, recall 0.806522
2017-12-10T02:42:14.929056: step 1949, loss 0.689439, acc 0.828125, prec 0.0497566, recall 0.806662
2017-12-10T02:42:15.191379: step 1950, loss 0.696278, acc 0.78125, prec 0.0497622, recall 0.806732
2017-12-10T02:42:15.453728: step 1951, loss 0.703866, acc 0.8125, prec 0.0497489, recall 0.806732
2017-12-10T02:42:15.707027: step 1952, loss 0.670119, acc 0.796875, prec 0.0497557, recall 0.806802
2017-12-10T02:42:15.974951: step 1953, loss 0.520239, acc 0.765625, prec 0.0497602, recall 0.806872
2017-12-10T02:42:16.239768: step 1954, loss 0.979984, acc 0.828125, prec 0.0497904, recall 0.807011
2017-12-10T02:42:16.498031: step 1955, loss 0.557786, acc 0.796875, prec 0.049776, recall 0.807011
2017-12-10T02:42:16.762677: step 1956, loss 0.840405, acc 0.796875, prec 0.0498251, recall 0.80722
2017-12-10T02:42:17.024738: step 1957, loss 0.622845, acc 0.75, prec 0.0498285, recall 0.80729
2017-12-10T02:42:17.292794: step 1958, loss 2.13541, acc 0.765625, prec 0.0498542, recall 0.807429
2017-12-10T02:42:17.557831: step 1959, loss 0.438501, acc 0.90625, prec 0.049911, recall 0.807637
2017-12-10T02:42:17.831364: step 1960, loss 0.806192, acc 0.796875, prec 0.0499177, recall 0.807706
2017-12-10T02:42:18.097980: step 1961, loss 0.650607, acc 0.8125, prec 0.0499043, recall 0.807706
2017-12-10T02:42:18.361458: step 1962, loss 0.726305, acc 0.8125, prec 0.0499121, recall 0.807775
2017-12-10T02:42:18.621196: step 1963, loss 0.981557, acc 0.78125, prec 0.0499177, recall 0.807845
2017-12-10T02:42:18.889402: step 1964, loss 0.465088, acc 0.859375, prec 0.0499077, recall 0.807845
2017-12-10T02:42:19.144091: step 1965, loss 0.409351, acc 0.859375, prec 0.0499189, recall 0.807914
2017-12-10T02:42:19.408646: step 1966, loss 0.451758, acc 0.828125, prec 0.0499067, recall 0.807914
2017-12-10T02:42:19.674487: step 1967, loss 0.265824, acc 0.921875, prec 0.0499222, recall 0.807983
2017-12-10T02:42:19.939023: step 1968, loss 0.473558, acc 0.84375, prec 0.0499323, recall 0.808052
2017-12-10T02:42:20.201238: step 1969, loss 0.642344, acc 0.8125, prec 0.04994, recall 0.808121
2017-12-10T02:42:20.461537: step 1970, loss 0.463047, acc 0.859375, prec 0.0499301, recall 0.808121
2017-12-10T02:42:20.723072: step 1971, loss 0.347347, acc 0.984375, prec 0.0499711, recall 0.808259
2017-12-10T02:42:20.990244: step 1972, loss 0.675817, acc 0.859375, prec 0.0500033, recall 0.808396
2017-12-10T02:42:21.253672: step 1973, loss 0.154809, acc 0.9375, prec 0.0499989, recall 0.808396
2017-12-10T02:42:21.515234: step 1974, loss 0.227304, acc 0.90625, prec 0.0499922, recall 0.808396
2017-12-10T02:42:21.785940: step 1975, loss 0.460944, acc 0.921875, prec 0.0499867, recall 0.808396
2017-12-10T02:42:22.044744: step 1976, loss 0.0809073, acc 0.96875, prec 0.0500055, recall 0.808465
2017-12-10T02:42:22.311222: step 1977, loss 0.166839, acc 0.90625, prec 0.0499989, recall 0.808465
2017-12-10T02:42:22.572475: step 1978, loss 0.0930496, acc 0.984375, prec 0.0499978, recall 0.808465
2017-12-10T02:42:22.834700: step 1979, loss 0.155665, acc 0.96875, prec 0.0500166, recall 0.808534
2017-12-10T02:42:23.105659: step 1980, loss 0.0193907, acc 1, prec 0.0500166, recall 0.808534
2017-12-10T02:42:23.367446: step 1981, loss 0.579479, acc 0.9375, prec 0.0500333, recall 0.808602
2017-12-10T02:42:23.632863: step 1982, loss 5.18328, acc 0.96875, prec 0.0500532, recall 0.808381
2017-12-10T02:42:23.900773: step 1983, loss 0.138006, acc 0.9375, prec 0.0500909, recall 0.808518
2017-12-10T02:42:24.171853: step 1984, loss 16.8158, acc 0.984375, prec 0.050112, recall 0.808298
2017-12-10T02:42:24.447237: step 1985, loss 0.0704421, acc 0.984375, prec 0.0501319, recall 0.808366
2017-12-10T02:42:24.705041: step 1986, loss 0.333688, acc 0.921875, prec 0.0501685, recall 0.808503
2017-12-10T02:42:24.971064: step 1987, loss 0.586691, acc 0.859375, prec 0.0501795, recall 0.808571
2017-12-10T02:42:25.194661: step 1988, loss 0.373423, acc 0.843137, prec 0.0501706, recall 0.808571
2017-12-10T02:42:25.462790: step 1989, loss 0.438892, acc 0.90625, prec 0.050164, recall 0.808571
2017-12-10T02:42:25.728830: step 1990, loss 0.507142, acc 0.8125, prec 0.0501717, recall 0.80864
2017-12-10T02:42:25.984282: step 1991, loss 0.314868, acc 0.890625, prec 0.0501639, recall 0.80864
2017-12-10T02:42:26.247303: step 1992, loss 0.695467, acc 0.765625, prec 0.0501683, recall 0.808708
2017-12-10T02:42:26.520452: step 1993, loss 0.614055, acc 0.796875, prec 0.0501538, recall 0.808708
2017-12-10T02:42:26.778962: step 1994, loss 0.498232, acc 0.859375, prec 0.0501438, recall 0.808708
2017-12-10T02:42:27.041761: step 1995, loss 1.00618, acc 0.765625, prec 0.0501272, recall 0.808708
2017-12-10T02:42:27.298694: step 1996, loss 0.582316, acc 0.796875, prec 0.0501338, recall 0.808776
2017-12-10T02:42:27.558559: step 1997, loss 0.756558, acc 0.796875, prec 0.0501194, recall 0.808776
2017-12-10T02:42:27.821597: step 1998, loss 0.532479, acc 0.859375, prec 0.0501514, recall 0.808913
2017-12-10T02:42:28.091048: step 1999, loss 0.522742, acc 0.84375, prec 0.0501403, recall 0.808913
2017-12-10T02:42:28.361878: step 2000, loss 0.428986, acc 0.84375, prec 0.0501712, recall 0.809049
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-2000

2017-12-10T02:42:29.571959: step 2001, loss 0.435299, acc 0.875, prec 0.0501833, recall 0.809117
2017-12-10T02:42:29.837655: step 2002, loss 0.334961, acc 0.96875, prec 0.0502231, recall 0.809253
2017-12-10T02:42:30.108469: step 2003, loss 0.376651, acc 0.875, prec 0.0502352, recall 0.809321
2017-12-10T02:42:30.381386: step 2004, loss 0.518649, acc 0.859375, prec 0.0502252, recall 0.809321
2017-12-10T02:42:30.641061: step 2005, loss 0.793253, acc 0.953125, prec 0.0502638, recall 0.809456
2017-12-10T02:42:30.910630: step 2006, loss 0.0941093, acc 0.96875, prec 0.0503245, recall 0.809659
2017-12-10T02:42:31.168933: step 2007, loss 0.579254, acc 0.84375, prec 0.0503134, recall 0.809659
2017-12-10T02:42:31.434350: step 2008, loss 0.159427, acc 0.953125, prec 0.050331, recall 0.809727
2017-12-10T02:42:31.699427: step 2009, loss 0.0898117, acc 0.96875, prec 0.0503916, recall 0.809929
2017-12-10T02:42:31.959753: step 2010, loss 0.336436, acc 0.890625, prec 0.0504048, recall 0.809996
2017-12-10T02:42:32.221918: step 2011, loss 0.248121, acc 0.9375, prec 0.0504213, recall 0.810064
2017-12-10T02:42:32.481088: step 2012, loss 0.0571364, acc 0.984375, prec 0.0504202, recall 0.810064
2017-12-10T02:42:32.747344: step 2013, loss 0.131372, acc 0.953125, prec 0.0504168, recall 0.810064
2017-12-10T02:42:33.014511: step 2014, loss 0.354279, acc 0.9375, prec 0.0504333, recall 0.810131
2017-12-10T02:42:33.274855: step 2015, loss 0.257184, acc 0.96875, prec 0.050473, recall 0.810265
2017-12-10T02:42:33.539003: step 2016, loss 0.4344, acc 0.953125, prec 0.0505324, recall 0.810467
2017-12-10T02:42:33.807436: step 2017, loss 0.468734, acc 0.96875, prec 0.050593, recall 0.810668
2017-12-10T02:42:34.077721: step 2018, loss 0.20313, acc 0.984375, prec 0.0506128, recall 0.810734
2017-12-10T02:42:34.338683: step 2019, loss 0.201727, acc 0.9375, prec 0.0506084, recall 0.810734
2017-12-10T02:42:34.604446: step 2020, loss 3.69605, acc 0.890625, prec 0.0506017, recall 0.810448
2017-12-10T02:42:34.868865: step 2021, loss 0.0717159, acc 0.96875, prec 0.0505994, recall 0.810448
2017-12-10T02:42:35.128023: step 2022, loss 0.555779, acc 0.859375, prec 0.0506103, recall 0.810515
2017-12-10T02:42:35.387124: step 2023, loss 0.156692, acc 0.9375, prec 0.0506059, recall 0.810515
2017-12-10T02:42:35.645920: step 2024, loss 0.350959, acc 0.890625, prec 0.0505981, recall 0.810515
2017-12-10T02:42:35.907793: step 2025, loss 0.393597, acc 0.921875, prec 0.0506343, recall 0.810649
2017-12-10T02:42:36.167678: step 2026, loss 0.233926, acc 0.90625, prec 0.0506276, recall 0.810649
2017-12-10T02:42:36.426759: step 2027, loss 0.228287, acc 0.890625, prec 0.0506407, recall 0.810716
2017-12-10T02:42:36.690877: step 2028, loss 0.30456, acc 0.921875, prec 0.0506351, recall 0.810716
2017-12-10T02:42:36.956290: step 2029, loss 0.275765, acc 0.90625, prec 0.0506494, recall 0.810782
2017-12-10T02:42:37.219991: step 2030, loss 0.311678, acc 0.875, prec 0.0506404, recall 0.810782
2017-12-10T02:42:37.483907: step 2031, loss 0.21596, acc 0.921875, prec 0.0506349, recall 0.810782
2017-12-10T02:42:37.742357: step 2032, loss 0.417476, acc 0.90625, prec 0.0506908, recall 0.810982
2017-12-10T02:42:38.005377: step 2033, loss 0.4733, acc 0.859375, prec 0.0507226, recall 0.811115
2017-12-10T02:42:38.267089: step 2034, loss 0.700658, acc 0.84375, prec 0.0507323, recall 0.811181
2017-12-10T02:42:38.529220: step 2035, loss 0.308168, acc 0.9375, prec 0.0507278, recall 0.811181
2017-12-10T02:42:38.793781: step 2036, loss 0.435472, acc 0.859375, prec 0.0507178, recall 0.811181
2017-12-10T02:42:39.054732: step 2037, loss 0.403603, acc 0.859375, prec 0.0507495, recall 0.811314
2017-12-10T02:42:39.313258: step 2038, loss 7.00341, acc 0.875, prec 0.0507417, recall 0.811029
2017-12-10T02:42:39.578978: step 2039, loss 0.435501, acc 0.875, prec 0.0507536, recall 0.811095
2017-12-10T02:42:39.842451: step 2040, loss 0.518444, acc 0.84375, prec 0.0507633, recall 0.811162
2017-12-10T02:42:40.103269: step 2041, loss 0.43146, acc 0.890625, prec 0.0507764, recall 0.811228
2017-12-10T02:42:40.363025: step 2042, loss 0.48958, acc 0.921875, prec 0.0508125, recall 0.81136
2017-12-10T02:42:40.623706: step 2043, loss 0.119499, acc 0.96875, prec 0.0508311, recall 0.811427
2017-12-10T02:42:40.880825: step 2044, loss 0.409693, acc 0.84375, prec 0.0508199, recall 0.811427
2017-12-10T02:42:41.142090: step 2045, loss 0.700178, acc 0.84375, prec 0.0508296, recall 0.811493
2017-12-10T02:42:41.412295: step 2046, loss 0.457689, acc 0.828125, prec 0.050859, recall 0.811625
2017-12-10T02:42:41.674781: step 2047, loss 0.52851, acc 0.859375, prec 0.0508489, recall 0.811625
2017-12-10T02:42:41.942009: step 2048, loss 0.324119, acc 0.921875, prec 0.0508434, recall 0.811625
2017-12-10T02:42:42.205831: step 2049, loss 0.887877, acc 0.78125, prec 0.0508486, recall 0.811691
2017-12-10T02:42:42.463745: step 2050, loss 0.387816, acc 0.875, prec 0.0508813, recall 0.811822
2017-12-10T02:42:42.723335: step 2051, loss 0.341756, acc 0.859375, prec 0.050892, recall 0.811888
2017-12-10T02:42:42.980556: step 2052, loss 0.672416, acc 0.84375, prec 0.0509017, recall 0.811954
2017-12-10T02:42:43.242060: step 2053, loss 0.515155, acc 0.875, prec 0.0509136, recall 0.81202
2017-12-10T02:42:43.504207: step 2054, loss 0.149505, acc 0.953125, prec 0.050931, recall 0.812085
2017-12-10T02:42:43.759904: step 2055, loss 0.350611, acc 0.90625, prec 0.0509243, recall 0.812085
2017-12-10T02:42:44.020707: step 2056, loss 0.261059, acc 0.90625, prec 0.0509176, recall 0.812085
2017-12-10T02:42:44.283265: step 2057, loss 0.140381, acc 0.953125, prec 0.0509558, recall 0.812216
2017-12-10T02:42:44.548990: step 2058, loss 3.25004, acc 0.921875, prec 0.0509514, recall 0.811933
2017-12-10T02:42:44.820494: step 2059, loss 0.183432, acc 0.9375, prec 0.0509469, recall 0.811933
2017-12-10T02:42:45.079313: step 2060, loss 0.927397, acc 0.953125, prec 0.0509643, recall 0.811999
2017-12-10T02:42:45.348504: step 2061, loss 0.199265, acc 0.9375, prec 0.0510014, recall 0.81213
2017-12-10T02:42:45.624057: step 2062, loss 0.253404, acc 0.90625, prec 0.0510155, recall 0.812195
2017-12-10T02:42:45.885207: step 2063, loss 0.284002, acc 0.90625, prec 0.0510088, recall 0.812195
2017-12-10T02:42:46.143381: step 2064, loss 0.164216, acc 0.953125, prec 0.0510677, recall 0.812391
2017-12-10T02:42:46.402154: step 2065, loss 0.412961, acc 0.859375, prec 0.0510577, recall 0.812391
2017-12-10T02:42:46.666388: step 2066, loss 0.445677, acc 0.890625, prec 0.0510706, recall 0.812456
2017-12-10T02:42:46.927466: step 2067, loss 0.517681, acc 0.84375, prec 0.0510595, recall 0.812456
2017-12-10T02:42:47.189396: step 2068, loss 0.294123, acc 0.90625, prec 0.0510735, recall 0.812522
2017-12-10T02:42:47.450789: step 2069, loss 0.368396, acc 0.875, prec 0.0510646, recall 0.812522
2017-12-10T02:42:47.713766: step 2070, loss 0.0801369, acc 0.984375, prec 0.0510635, recall 0.812522
2017-12-10T02:42:47.975323: step 2071, loss 0.11993, acc 0.9375, prec 0.0510797, recall 0.812587
2017-12-10T02:42:48.240107: step 2072, loss 0.0724064, acc 0.984375, prec 0.0510786, recall 0.812587
2017-12-10T02:42:48.500976: step 2073, loss 0.322707, acc 0.90625, prec 0.0510927, recall 0.812652
2017-12-10T02:42:48.760527: step 2074, loss 0.098091, acc 0.953125, prec 0.0510893, recall 0.812652
2017-12-10T02:42:49.027820: step 2075, loss 0.169722, acc 0.953125, prec 0.051086, recall 0.812652
2017-12-10T02:42:49.287945: step 2076, loss 0.176937, acc 0.953125, prec 0.0510826, recall 0.812652
2017-12-10T02:42:49.550568: step 2077, loss 0.14521, acc 0.984375, prec 0.0510815, recall 0.812652
2017-12-10T02:42:49.813702: step 2078, loss 0.12288, acc 0.9375, prec 0.051077, recall 0.812652
2017-12-10T02:42:50.072700: step 2079, loss 1.00409, acc 0.984375, prec 0.0510966, recall 0.812717
2017-12-10T02:42:50.341950: step 2080, loss 0.950468, acc 0.96875, prec 0.0511773, recall 0.812977
2017-12-10T02:42:50.612431: step 2081, loss 0.138542, acc 0.96875, prec 0.0511958, recall 0.813042
2017-12-10T02:42:50.874529: step 2082, loss 0.102958, acc 0.96875, prec 0.0511936, recall 0.813042
2017-12-10T02:42:51.133573: step 2083, loss 0.333987, acc 0.953125, prec 0.0512109, recall 0.813107
2017-12-10T02:42:51.396666: step 2084, loss 0.253103, acc 0.90625, prec 0.0512457, recall 0.813236
2017-12-10T02:42:51.663329: step 2085, loss 0.265339, acc 0.921875, prec 0.0512608, recall 0.813301
2017-12-10T02:42:51.935638: step 2086, loss 0.149955, acc 0.890625, prec 0.0512529, recall 0.813301
2017-12-10T02:42:52.197091: step 2087, loss 1.49201, acc 0.921875, prec 0.0512692, recall 0.813084
2017-12-10T02:42:52.466613: step 2088, loss 0.200384, acc 0.9375, prec 0.0512647, recall 0.813084
2017-12-10T02:42:52.729119: step 2089, loss 0.245515, acc 0.90625, prec 0.051258, recall 0.813084
2017-12-10T02:42:52.994616: step 2090, loss 0.225685, acc 0.90625, prec 0.0512513, recall 0.813084
2017-12-10T02:42:53.259729: step 2091, loss 0.55619, acc 0.828125, prec 0.0512597, recall 0.813149
2017-12-10T02:42:53.521781: step 2092, loss 0.246902, acc 0.921875, prec 0.0512748, recall 0.813213
2017-12-10T02:42:53.786747: step 2093, loss 0.629254, acc 0.859375, prec 0.0512647, recall 0.813213
2017-12-10T02:42:54.055888: step 2094, loss 0.71026, acc 0.828125, prec 0.0512731, recall 0.813278
2017-12-10T02:42:54.322503: step 2095, loss 0.534795, acc 0.828125, prec 0.0512815, recall 0.813343
2017-12-10T02:42:54.581375: step 2096, loss 0.601422, acc 0.78125, prec 0.0512865, recall 0.813407
2017-12-10T02:42:54.839960: step 2097, loss 0.210716, acc 0.90625, prec 0.0513005, recall 0.813471
2017-12-10T02:42:55.106027: step 2098, loss 0.561404, acc 0.890625, prec 0.051334, recall 0.8136
2017-12-10T02:42:55.378374: step 2099, loss 0.144003, acc 0.953125, prec 0.0513513, recall 0.813665
2017-12-10T02:42:55.639960: step 2100, loss 0.257263, acc 0.890625, prec 0.0513435, recall 0.813665

Evaluation:
2017-12-10T02:43:03.224047: step 2100, loss 1.99442, acc 0.916502, prec 0.0524496, recall 0.802417

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-2100

2017-12-10T02:43:04.613044: step 2101, loss 0.204345, acc 0.9375, prec 0.0524653, recall 0.802481
2017-12-10T02:43:04.877797: step 2102, loss 0.326973, acc 0.953125, prec 0.052462, recall 0.802481
2017-12-10T02:43:05.138488: step 2103, loss 0.548088, acc 0.90625, prec 0.0524957, recall 0.80261
2017-12-10T02:43:05.401555: step 2104, loss 0.268651, acc 0.90625, prec 0.0525092, recall 0.802674
2017-12-10T02:43:05.667118: step 2105, loss 0.101806, acc 0.984375, prec 0.0525283, recall 0.802739
2017-12-10T02:43:05.926485: step 2106, loss 0.141285, acc 0.953125, prec 0.0525249, recall 0.802739
2017-12-10T02:43:06.190244: step 2107, loss 0.070421, acc 0.984375, prec 0.052544, recall 0.802803
2017-12-10T02:43:06.452765: step 2108, loss 0.769186, acc 0.96875, prec 0.0525822, recall 0.802932
2017-12-10T02:43:06.725300: step 2109, loss 0.291565, acc 0.9375, prec 0.0526383, recall 0.803124
2017-12-10T02:43:06.993789: step 2110, loss 0.292783, acc 0.9375, prec 0.052654, recall 0.803188
2017-12-10T02:43:07.256177: step 2111, loss 0.162976, acc 0.9375, prec 0.0526697, recall 0.803252
2017-12-10T02:43:07.526891: step 2112, loss 0.259774, acc 0.921875, prec 0.0526843, recall 0.803316
2017-12-10T02:43:07.790848: step 2113, loss 0.108363, acc 0.953125, prec 0.0527011, recall 0.80338
2017-12-10T02:43:08.052703: step 2114, loss 0.11661, acc 0.953125, prec 0.0526978, recall 0.80338
2017-12-10T02:43:08.314184: step 2115, loss 0.0993638, acc 0.953125, prec 0.0527146, recall 0.803444
2017-12-10T02:43:08.572154: step 2116, loss 0.176781, acc 0.921875, prec 0.052709, recall 0.803444
2017-12-10T02:43:08.831640: step 2117, loss 0.340685, acc 0.875, prec 0.0527202, recall 0.803508
2017-12-10T02:43:09.095822: step 2118, loss 0.176683, acc 0.90625, prec 0.0527134, recall 0.803508
2017-12-10T02:43:09.358216: step 2119, loss 0.0984657, acc 0.96875, prec 0.0527314, recall 0.803571
2017-12-10T02:43:09.625597: step 2120, loss 3.07711, acc 0.921875, prec 0.0527269, recall 0.803311
2017-12-10T02:43:09.890014: step 2121, loss 0.321374, acc 0.96875, prec 0.052765, recall 0.803438
2017-12-10T02:43:10.161261: step 2122, loss 0.12968, acc 0.953125, prec 0.0527616, recall 0.803438
2017-12-10T02:43:10.422687: step 2123, loss 0.176137, acc 0.9375, prec 0.0527571, recall 0.803438
2017-12-10T02:43:10.686985: step 2124, loss 0.237008, acc 0.9375, prec 0.0527728, recall 0.803502
2017-12-10T02:43:10.953635: step 2125, loss 0.449529, acc 0.859375, prec 0.0527627, recall 0.803502
2017-12-10T02:43:11.215861: step 2126, loss 0.332905, acc 0.875, prec 0.0527739, recall 0.803566
2017-12-10T02:43:11.485521: step 2127, loss 0.530597, acc 0.84375, prec 0.0528634, recall 0.803883
2017-12-10T02:43:11.748775: step 2128, loss 0.485086, acc 0.84375, prec 0.0528723, recall 0.803947
2017-12-10T02:43:12.012816: step 2129, loss 0.285304, acc 0.9375, prec 0.0528678, recall 0.803947
2017-12-10T02:43:12.269363: step 2130, loss 0.223866, acc 0.953125, prec 0.0528645, recall 0.803947
2017-12-10T02:43:12.528839: step 2131, loss 0.220016, acc 0.9375, prec 0.05286, recall 0.803947
2017-12-10T02:43:12.797087: step 2132, loss 0.202521, acc 0.9375, prec 0.0528555, recall 0.803947
2017-12-10T02:43:13.059300: step 2133, loss 0.122811, acc 0.984375, prec 0.0528745, recall 0.80401
2017-12-10T02:43:13.320698: step 2134, loss 0.197616, acc 0.9375, prec 0.0529103, recall 0.804137
2017-12-10T02:43:13.581086: step 2135, loss 0.188367, acc 0.90625, prec 0.0529035, recall 0.804137
2017-12-10T02:43:13.843335: step 2136, loss 0.392581, acc 0.859375, prec 0.0529135, recall 0.8042
2017-12-10T02:43:14.113992: step 2137, loss 0.337893, acc 0.90625, prec 0.0529269, recall 0.804264
2017-12-10T02:43:14.371809: step 2138, loss 0.304938, acc 0.921875, prec 0.0529414, recall 0.804327
2017-12-10T02:43:14.641492: step 2139, loss 2.70722, acc 0.921875, prec 0.0529772, recall 0.804194
2017-12-10T02:43:14.905529: step 2140, loss 0.196352, acc 0.9375, prec 0.0529727, recall 0.804194
2017-12-10T02:43:15.167395: step 2141, loss 0.574765, acc 0.890625, prec 0.0530453, recall 0.804446
2017-12-10T02:43:15.427213: step 2142, loss 0.267237, acc 0.953125, prec 0.0530419, recall 0.804446
2017-12-10T02:43:15.690553: step 2143, loss 1.81061, acc 0.875, prec 0.0530541, recall 0.80425
2017-12-10T02:43:15.951631: step 2144, loss 0.336131, acc 0.890625, prec 0.0530462, recall 0.80425
2017-12-10T02:43:16.212928: step 2145, loss 0.55867, acc 0.890625, prec 0.0530383, recall 0.80425
2017-12-10T02:43:16.486223: step 2146, loss 0.515517, acc 0.875, prec 0.0530494, recall 0.804313
2017-12-10T02:43:16.749695: step 2147, loss 0.398621, acc 0.875, prec 0.0530404, recall 0.804313
2017-12-10T02:43:17.014475: step 2148, loss 0.33886, acc 0.90625, prec 0.0530337, recall 0.804313
2017-12-10T02:43:17.277629: step 2149, loss 0.538136, acc 0.84375, prec 0.0530626, recall 0.804439
2017-12-10T02:43:17.544195: step 2150, loss 0.589798, acc 0.859375, prec 0.0530525, recall 0.804439
2017-12-10T02:43:17.806270: step 2151, loss 1.67907, acc 0.796875, prec 0.0530981, recall 0.804627
2017-12-10T02:43:18.074271: step 2152, loss 0.666044, acc 0.875, prec 0.0531493, recall 0.804815
2017-12-10T02:43:18.332753: step 2153, loss 0.955564, acc 0.78125, prec 0.0531335, recall 0.804815
2017-12-10T02:43:18.603292: step 2154, loss 0.604437, acc 0.828125, prec 0.0531813, recall 0.805003
2017-12-10T02:43:18.863563: step 2155, loss 0.530951, acc 0.875, prec 0.0531924, recall 0.805066
2017-12-10T02:43:19.122737: step 2156, loss 0.460905, acc 0.859375, prec 0.0531823, recall 0.805066
2017-12-10T02:43:19.383004: step 2157, loss 0.610364, acc 0.859375, prec 0.0531922, recall 0.805128
2017-12-10T02:43:19.641398: step 2158, loss 0.86019, acc 0.796875, prec 0.0532176, recall 0.805253
2017-12-10T02:43:19.902650: step 2159, loss 0.728361, acc 0.859375, prec 0.0532676, recall 0.80544
2017-12-10T02:43:20.157172: step 2160, loss 0.395547, acc 0.875, prec 0.0532586, recall 0.80544
2017-12-10T02:43:20.424699: step 2161, loss 0.820623, acc 0.828125, prec 0.0532862, recall 0.805564
2017-12-10T02:43:20.690801: step 2162, loss 0.29731, acc 0.890625, prec 0.0532783, recall 0.805564
2017-12-10T02:43:20.954607: step 2163, loss 0.357738, acc 0.828125, prec 0.053326, recall 0.805751
2017-12-10T02:43:21.211999: step 2164, loss 0.494222, acc 0.859375, prec 0.0533759, recall 0.805937
2017-12-10T02:43:21.473459: step 2165, loss 0.133133, acc 0.953125, prec 0.0533725, recall 0.805937
2017-12-10T02:43:21.735494: step 2166, loss 0.296839, acc 0.90625, prec 0.0533657, recall 0.805937
2017-12-10T02:43:21.996188: step 2167, loss 0.330263, acc 0.90625, prec 0.053359, recall 0.805937
2017-12-10T02:43:22.262909: step 2168, loss 0.247676, acc 0.921875, prec 0.0533733, recall 0.805999
2017-12-10T02:43:22.518770: step 2169, loss 0.129407, acc 0.953125, prec 0.05341, recall 0.806122
2017-12-10T02:43:22.781955: step 2170, loss 0.113378, acc 0.953125, prec 0.0534266, recall 0.806184
2017-12-10T02:43:23.048301: step 2171, loss 0.288492, acc 0.90625, prec 0.0534598, recall 0.806308
2017-12-10T02:43:23.317783: step 2172, loss 0.109666, acc 0.953125, prec 0.0534764, recall 0.806369
2017-12-10T02:43:23.581068: step 2173, loss 1.52719, acc 0.953125, prec 0.0535141, recall 0.806236
2017-12-10T02:43:23.846689: step 2174, loss 0.0522265, acc 0.96875, prec 0.0535118, recall 0.806236
2017-12-10T02:43:24.106519: step 2175, loss 0.168311, acc 0.96875, prec 0.0535096, recall 0.806236
2017-12-10T02:43:24.365341: step 2176, loss 0.0681618, acc 0.96875, prec 0.0535473, recall 0.806359
2017-12-10T02:43:24.634405: step 2177, loss 0.254745, acc 0.921875, prec 0.0535416, recall 0.806359
2017-12-10T02:43:24.897570: step 2178, loss 0.0723998, acc 0.984375, prec 0.0535405, recall 0.806359
2017-12-10T02:43:25.160245: step 2179, loss 0.177362, acc 0.9375, prec 0.053556, recall 0.806421
2017-12-10T02:43:25.423183: step 2180, loss 0.114071, acc 0.921875, prec 0.0535503, recall 0.806421
2017-12-10T02:43:25.683895: step 2181, loss 0.118367, acc 0.96875, prec 0.0535481, recall 0.806421
2017-12-10T02:43:25.947386: step 2182, loss 0.213246, acc 0.9375, prec 0.0535835, recall 0.806544
2017-12-10T02:43:26.210919: step 2183, loss 0.163158, acc 0.96875, prec 0.0536012, recall 0.806605
2017-12-10T02:43:26.480782: step 2184, loss 1.26591, acc 0.953125, prec 0.0536189, recall 0.806411
2017-12-10T02:43:26.743991: step 2185, loss 0.294232, acc 0.921875, prec 0.0536132, recall 0.806411
2017-12-10T02:43:27.011079: step 2186, loss 0.230182, acc 0.953125, prec 0.0536298, recall 0.806472
2017-12-10T02:43:27.272134: step 2187, loss 0.264853, acc 0.921875, prec 0.0536242, recall 0.806472
2017-12-10T02:43:27.536678: step 2188, loss 0.149326, acc 0.953125, prec 0.0536607, recall 0.806595
2017-12-10T02:43:27.802436: step 2189, loss 0.177818, acc 0.96875, prec 0.0536584, recall 0.806595
2017-12-10T02:43:28.063780: step 2190, loss 0.235228, acc 0.890625, prec 0.0536505, recall 0.806595
2017-12-10T02:43:28.337632: step 2191, loss 2.18662, acc 0.9375, prec 0.053687, recall 0.806462
2017-12-10T02:43:28.610154: step 2192, loss 0.131359, acc 0.9375, prec 0.0536825, recall 0.806462
2017-12-10T02:43:28.876193: step 2193, loss 0.337067, acc 0.90625, prec 0.0536957, recall 0.806523
2017-12-10T02:43:29.140985: step 2194, loss 0.306491, acc 0.859375, prec 0.0537054, recall 0.806584
2017-12-10T02:43:29.409666: step 2195, loss 0.574509, acc 0.859375, prec 0.0536952, recall 0.806584
2017-12-10T02:43:29.669827: step 2196, loss 0.46787, acc 0.921875, prec 0.0537095, recall 0.806646
2017-12-10T02:43:29.932449: step 2197, loss 0.280863, acc 0.921875, prec 0.0537238, recall 0.806707
2017-12-10T02:43:30.203618: step 2198, loss 0.283295, acc 0.90625, prec 0.053717, recall 0.806707
2017-12-10T02:43:30.475049: step 2199, loss 0.298261, acc 0.90625, prec 0.0537301, recall 0.806768
2017-12-10T02:43:30.742701: step 2200, loss 0.912252, acc 0.859375, prec 0.0537598, recall 0.80689
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-2200

2017-12-10T02:43:32.275551: step 2201, loss 1.38135, acc 0.875, prec 0.0537519, recall 0.806635
2017-12-10T02:43:32.545382: step 2202, loss 1.96479, acc 0.921875, prec 0.0537474, recall 0.80638
2017-12-10T02:43:32.810877: step 2203, loss 0.130909, acc 0.9375, prec 0.0537827, recall 0.806503
2017-12-10T02:43:33.072549: step 2204, loss 0.491503, acc 0.84375, prec 0.0537714, recall 0.806503
2017-12-10T02:43:33.338808: step 2205, loss 0.706178, acc 0.859375, prec 0.053801, recall 0.806625
2017-12-10T02:43:33.598238: step 2206, loss 0.480809, acc 0.8125, prec 0.0537874, recall 0.806625
2017-12-10T02:43:33.860721: step 2207, loss 0.908071, acc 0.65625, prec 0.0537625, recall 0.806625
2017-12-10T02:43:34.128458: step 2208, loss 0.610393, acc 0.875, prec 0.0538132, recall 0.806807
2017-12-10T02:43:34.392330: step 2209, loss 0.610675, acc 0.890625, prec 0.0538053, recall 0.806807
2017-12-10T02:43:34.657326: step 2210, loss 0.376287, acc 0.90625, prec 0.0537985, recall 0.806807
2017-12-10T02:43:34.917112: step 2211, loss 0.491132, acc 0.828125, prec 0.0538059, recall 0.806868
2017-12-10T02:43:35.180028: step 2212, loss 0.485827, acc 0.84375, prec 0.0538145, recall 0.806929
2017-12-10T02:43:35.442097: step 2213, loss 0.440324, acc 0.921875, prec 0.0538287, recall 0.80699
2017-12-10T02:43:35.701556: step 2214, loss 0.108769, acc 0.9375, prec 0.0538639, recall 0.807111
2017-12-10T02:43:35.967378: step 2215, loss 2.39584, acc 0.890625, prec 0.0538571, recall 0.806858
2017-12-10T02:43:36.234891: step 2216, loss 0.157194, acc 0.953125, prec 0.0538935, recall 0.806979
2017-12-10T02:43:36.494362: step 2217, loss 0.227767, acc 0.90625, prec 0.0538867, recall 0.806979
2017-12-10T02:43:36.758019: step 2218, loss 0.547281, acc 0.84375, prec 0.0538754, recall 0.806979
2017-12-10T02:43:37.019685: step 2219, loss 0.263824, acc 0.921875, prec 0.0538697, recall 0.806979
2017-12-10T02:43:37.283503: step 2220, loss 0.840149, acc 0.828125, prec 0.053897, recall 0.8071
2017-12-10T02:43:37.544300: step 2221, loss 0.219698, acc 0.890625, prec 0.0539089, recall 0.807161
2017-12-10T02:43:37.803423: step 2222, loss 0.296402, acc 0.890625, prec 0.0539605, recall 0.807342
2017-12-10T02:43:38.061235: step 2223, loss 0.301755, acc 0.875, prec 0.0539515, recall 0.807342
2017-12-10T02:43:38.322490: step 2224, loss 0.335268, acc 0.921875, prec 0.0540053, recall 0.807523
2017-12-10T02:43:38.587064: step 2225, loss 0.532207, acc 0.859375, prec 0.054015, recall 0.807584
2017-12-10T02:43:38.858322: step 2226, loss 0.207078, acc 0.921875, prec 0.0540291, recall 0.807644
2017-12-10T02:43:39.116932: step 2227, loss 0.126652, acc 0.921875, prec 0.0540433, recall 0.807704
2017-12-10T02:43:39.379737: step 2228, loss 0.191127, acc 0.9375, prec 0.0540586, recall 0.807765
2017-12-10T02:43:39.640868: step 2229, loss 0.218027, acc 0.90625, prec 0.0540518, recall 0.807765
2017-12-10T02:43:39.912644: step 2230, loss 0.202911, acc 0.9375, prec 0.0540473, recall 0.807765
2017-12-10T02:43:40.175080: step 2231, loss 0.138988, acc 0.953125, prec 0.0540637, recall 0.807825
2017-12-10T02:43:40.446776: step 2232, loss 0.276017, acc 0.953125, prec 0.0540801, recall 0.807885
2017-12-10T02:43:40.715468: step 2233, loss 0.302147, acc 0.90625, prec 0.0541129, recall 0.808005
2017-12-10T02:43:40.981883: step 2234, loss 0.118225, acc 0.984375, prec 0.0541118, recall 0.808005
2017-12-10T02:43:41.249184: step 2235, loss 0.199265, acc 0.921875, prec 0.0541061, recall 0.808005
2017-12-10T02:43:41.520993: step 2236, loss 1.54133, acc 0.9375, prec 0.0541225, recall 0.807813
2017-12-10T02:43:41.789025: step 2237, loss 0.10874, acc 0.9375, prec 0.054118, recall 0.807813
2017-12-10T02:43:42.049690: step 2238, loss 0.0998894, acc 0.984375, prec 0.0541367, recall 0.807873
2017-12-10T02:43:42.319526: step 2239, loss 4.66462, acc 0.9375, prec 0.0541531, recall 0.80768
2017-12-10T02:43:42.587212: step 2240, loss 0.149493, acc 0.9375, prec 0.0541485, recall 0.80768
2017-12-10T02:43:42.854047: step 2241, loss 0.360243, acc 0.921875, prec 0.0541429, recall 0.80768
2017-12-10T02:43:43.114758: step 2242, loss 0.16885, acc 0.9375, prec 0.0541383, recall 0.80768
2017-12-10T02:43:43.374328: step 2243, loss 0.210933, acc 0.921875, prec 0.0541327, recall 0.80768
2017-12-10T02:43:43.635890: step 2244, loss 0.520841, acc 0.84375, prec 0.0541213, recall 0.80768
2017-12-10T02:43:43.899118: step 2245, loss 0.479504, acc 0.875, prec 0.0541321, recall 0.80774
2017-12-10T02:43:44.166077: step 2246, loss 0.450837, acc 0.84375, prec 0.0541405, recall 0.8078
2017-12-10T02:43:44.426611: step 2247, loss 0.930602, acc 0.859375, prec 0.0542094, recall 0.80804
2017-12-10T02:43:44.696668: step 2248, loss 0.287376, acc 0.921875, prec 0.0542038, recall 0.80804
2017-12-10T02:43:44.958853: step 2249, loss 0.591587, acc 0.8125, prec 0.0542297, recall 0.808159
2017-12-10T02:43:45.220194: step 2250, loss 0.449279, acc 0.859375, prec 0.0542393, recall 0.808219
2017-12-10T02:43:45.477656: step 2251, loss 0.137547, acc 0.96875, prec 0.054237, recall 0.808219
2017-12-10T02:43:45.745586: step 2252, loss 0.599247, acc 0.84375, prec 0.0542257, recall 0.808219
2017-12-10T02:43:46.007533: step 2253, loss 0.243628, acc 0.90625, prec 0.0542189, recall 0.808219
2017-12-10T02:43:46.270278: step 2254, loss 0.308932, acc 0.90625, prec 0.0542318, recall 0.808279
2017-12-10T02:43:46.533068: step 2255, loss 0.316223, acc 0.859375, prec 0.0542216, recall 0.808279
2017-12-10T02:43:46.794746: step 2256, loss 0.285119, acc 0.9375, prec 0.0542764, recall 0.808458
2017-12-10T02:43:47.058774: step 2257, loss 0.238026, acc 0.921875, prec 0.0542707, recall 0.808458
2017-12-10T02:43:47.317578: step 2258, loss 0.136018, acc 0.9375, prec 0.0542662, recall 0.808458
2017-12-10T02:43:47.589583: step 2259, loss 0.201686, acc 0.921875, prec 0.0543197, recall 0.808636
2017-12-10T02:43:47.855627: step 2260, loss 0.168596, acc 0.9375, prec 0.0543349, recall 0.808696
2017-12-10T02:43:48.117458: step 2261, loss 0.242751, acc 0.921875, prec 0.0543292, recall 0.808696
2017-12-10T02:43:48.380126: step 2262, loss 0.0723753, acc 0.953125, prec 0.0543258, recall 0.808696
2017-12-10T02:43:48.645288: step 2263, loss 0.115443, acc 0.96875, prec 0.0543433, recall 0.808755
2017-12-10T02:43:48.906758: step 2264, loss 1.04669, acc 0.953125, prec 0.0543793, recall 0.808874
2017-12-10T02:43:49.169732: step 2265, loss 0.0663409, acc 0.96875, prec 0.0543771, recall 0.808874
2017-12-10T02:43:49.438987: step 2266, loss 2.21142, acc 0.96875, prec 0.0543957, recall 0.808682
2017-12-10T02:43:49.703207: step 2267, loss 0.142746, acc 0.953125, prec 0.054412, recall 0.808741
2017-12-10T02:43:49.974363: step 2268, loss 0.158922, acc 0.953125, prec 0.0544086, recall 0.808741
2017-12-10T02:43:50.239137: step 2269, loss 0.405471, acc 0.953125, prec 0.0544446, recall 0.80886
2017-12-10T02:43:50.504878: step 2270, loss 1.93974, acc 0.9375, prec 0.0544609, recall 0.808669
2017-12-10T02:43:50.772522: step 2271, loss 8.04031, acc 0.953125, prec 0.0544598, recall 0.808168
2017-12-10T02:43:51.039896: step 2272, loss 0.483237, acc 0.875, prec 0.0544507, recall 0.808168
2017-12-10T02:43:51.306046: step 2273, loss 0.82674, acc 0.796875, prec 0.0544754, recall 0.808287
2017-12-10T02:43:51.567848: step 2274, loss 1.0607, acc 0.640625, prec 0.0544886, recall 0.808405
2017-12-10T02:43:51.823171: step 2275, loss 0.711991, acc 0.75, prec 0.0544705, recall 0.808405
2017-12-10T02:43:52.089840: step 2276, loss 1.86825, acc 0.453125, prec 0.0544505, recall 0.808465
2017-12-10T02:43:52.355600: step 2277, loss 2.25333, acc 0.53125, prec 0.0544559, recall 0.808583
2017-12-10T02:43:52.614409: step 2278, loss 2.09558, acc 0.5, prec 0.0544589, recall 0.808701
2017-12-10T02:43:52.873745: step 2279, loss 2.042, acc 0.484375, prec 0.0544413, recall 0.80876
2017-12-10T02:43:53.135518: step 2280, loss 2.15781, acc 0.5, prec 0.0544444, recall 0.808878
2017-12-10T02:43:53.394634: step 2281, loss 1.86008, acc 0.46875, prec 0.054406, recall 0.808878
2017-12-10T02:43:53.656429: step 2282, loss 2.04236, acc 0.546875, prec 0.0543929, recall 0.808937
2017-12-10T02:43:53.916479: step 2283, loss 1.34578, acc 0.640625, prec 0.0544061, recall 0.809054
2017-12-10T02:43:54.178564: step 2284, loss 1.11654, acc 0.6875, prec 0.0544032, recall 0.809113
2017-12-10T02:43:54.439010: step 2285, loss 0.595007, acc 0.78125, prec 0.054407, recall 0.809172
2017-12-10T02:43:54.700455: step 2286, loss 0.747929, acc 0.71875, prec 0.0544063, recall 0.809231
2017-12-10T02:43:54.968893: step 2287, loss 0.640925, acc 0.796875, prec 0.0544112, recall 0.809289
2017-12-10T02:43:55.231809: step 2288, loss 0.76143, acc 0.859375, prec 0.0544206, recall 0.809348
2017-12-10T02:43:55.505398: step 2289, loss 0.214682, acc 0.953125, prec 0.0544368, recall 0.809407
2017-12-10T02:43:55.766929: step 2290, loss 2.36715, acc 0.890625, prec 0.0544496, recall 0.809217
2017-12-10T02:43:56.032535: step 2291, loss 0.41738, acc 0.890625, prec 0.0544417, recall 0.809217
2017-12-10T02:43:56.296130: step 2292, loss 1.2071, acc 0.953125, prec 0.054459, recall 0.809027
2017-12-10T02:43:56.570319: step 2293, loss 0.142199, acc 0.9375, prec 0.0544741, recall 0.809085
2017-12-10T02:43:56.828903: step 2294, loss 0.156964, acc 0.96875, prec 0.0544718, recall 0.809085
2017-12-10T02:43:57.088515: step 2295, loss 0.139595, acc 0.96875, prec 0.0544891, recall 0.809144
2017-12-10T02:43:57.351651: step 2296, loss 0.335038, acc 0.859375, prec 0.054518, recall 0.809261
2017-12-10T02:43:57.615756: step 2297, loss 0.42351, acc 0.890625, prec 0.0545297, recall 0.809319
2017-12-10T02:43:57.879050: step 2298, loss 0.394847, acc 0.921875, prec 0.054524, recall 0.809319
2017-12-10T02:43:58.139345: step 2299, loss 0.713634, acc 0.890625, prec 0.0545552, recall 0.809436
2017-12-10T02:43:58.402772: step 2300, loss 0.210486, acc 0.90625, prec 0.0545485, recall 0.809436
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-2300

2017-12-10T02:43:59.672269: step 2301, loss 0.375629, acc 0.9375, prec 0.0545635, recall 0.809495
2017-12-10T02:43:59.938538: step 2302, loss 0.161498, acc 0.953125, prec 0.0545796, recall 0.809553
2017-12-10T02:44:00.205578: step 2303, loss 0.266178, acc 0.9375, prec 0.0545946, recall 0.809611
2017-12-10T02:44:00.472970: step 2304, loss 0.337375, acc 0.890625, prec 0.0545867, recall 0.809611
2017-12-10T02:44:00.735200: step 2305, loss 0.144691, acc 0.9375, prec 0.0546212, recall 0.809728
2017-12-10T02:44:00.996816: step 2306, loss 0.053149, acc 0.984375, prec 0.0546201, recall 0.809728
2017-12-10T02:44:01.265389: step 2307, loss 0.293217, acc 0.90625, prec 0.0546134, recall 0.809728
2017-12-10T02:44:01.531359: step 2308, loss 0.447468, acc 0.875, prec 0.0546433, recall 0.809844
2017-12-10T02:44:01.792408: step 2309, loss 0.076175, acc 0.96875, prec 0.0546606, recall 0.809902
2017-12-10T02:44:02.057215: step 2310, loss 0.201291, acc 0.984375, prec 0.0546595, recall 0.809902
2017-12-10T02:44:02.323682: step 2311, loss 0.221271, acc 0.921875, prec 0.0546538, recall 0.809902
2017-12-10T02:44:02.584801: step 2312, loss 0.326193, acc 0.9375, prec 0.0546688, recall 0.80996
2017-12-10T02:44:02.844101: step 2313, loss 5.3881, acc 0.953125, prec 0.0546666, recall 0.809713
2017-12-10T02:44:03.110199: step 2314, loss 0.178301, acc 0.953125, prec 0.0547022, recall 0.809829
2017-12-10T02:44:03.373900: step 2315, loss 0.341951, acc 0.9375, prec 0.0547171, recall 0.809887
2017-12-10T02:44:03.641661: step 2316, loss 0.0521534, acc 0.96875, prec 0.0547344, recall 0.809945
2017-12-10T02:44:03.902419: step 2317, loss 0.217705, acc 0.890625, prec 0.054746, recall 0.810003
2017-12-10T02:44:04.163695: step 2318, loss 0.344324, acc 0.90625, prec 0.0547392, recall 0.810003
2017-12-10T02:44:04.424843: step 2319, loss 0.213648, acc 0.9375, prec 0.0547347, recall 0.810003
2017-12-10T02:44:04.687263: step 2320, loss 0.53707, acc 0.953125, prec 0.0547508, recall 0.810061
2017-12-10T02:44:04.949058: step 2321, loss 0.200028, acc 0.984375, prec 0.0547691, recall 0.810119
2017-12-10T02:44:05.210081: step 2322, loss 0.255495, acc 0.890625, prec 0.0547612, recall 0.810119
2017-12-10T02:44:05.469923: step 2323, loss 0.260942, acc 0.875, prec 0.0548301, recall 0.81035
2017-12-10T02:44:05.744620: step 2324, loss 0.20193, acc 0.90625, prec 0.0548233, recall 0.81035
2017-12-10T02:44:06.008606: step 2325, loss 0.332953, acc 0.9375, prec 0.0548382, recall 0.810408
2017-12-10T02:44:06.272791: step 2326, loss 0.155947, acc 0.96875, prec 0.054836, recall 0.810408
2017-12-10T02:44:06.534063: step 2327, loss 0.47024, acc 0.84375, prec 0.0548247, recall 0.810408
2017-12-10T02:44:06.793855: step 2328, loss 0.0708197, acc 0.96875, prec 0.0548224, recall 0.810408
2017-12-10T02:44:07.056502: step 2329, loss 0.284971, acc 0.953125, prec 0.0549163, recall 0.810696
2017-12-10T02:44:07.320882: step 2330, loss 0.213828, acc 0.921875, prec 0.0549301, recall 0.810753
2017-12-10T02:44:07.583798: step 2331, loss 0.659508, acc 0.9375, prec 0.0549451, recall 0.810811
2017-12-10T02:44:07.850510: step 2332, loss 0.155009, acc 0.921875, prec 0.0549394, recall 0.810811
2017-12-10T02:44:08.122546: step 2333, loss 0.159487, acc 0.984375, prec 0.0549772, recall 0.810926
2017-12-10T02:44:08.392624: step 2334, loss 0.234833, acc 0.953125, prec 0.0549932, recall 0.810983
2017-12-10T02:44:08.656359: step 2335, loss 0.121365, acc 0.953125, prec 0.0550287, recall 0.811098
2017-12-10T02:44:08.920703: step 2336, loss 0.346191, acc 0.96875, prec 0.0550459, recall 0.811155
2017-12-10T02:44:09.185903: step 2337, loss 0.0923237, acc 0.9375, prec 0.0550608, recall 0.811212
2017-12-10T02:44:09.442455: step 2338, loss 0.167564, acc 0.984375, prec 0.0550791, recall 0.811269
2017-12-10T02:44:09.711024: step 2339, loss 0.218249, acc 0.90625, prec 0.0550723, recall 0.811269
2017-12-10T02:44:09.978901: step 2340, loss 0.148295, acc 0.984375, prec 0.05511, recall 0.811384
2017-12-10T02:44:10.238966: step 2341, loss 0.16344, acc 0.953125, prec 0.0551066, recall 0.811384
2017-12-10T02:44:10.511469: step 2342, loss 0.222239, acc 0.9375, prec 0.0551021, recall 0.811384
2017-12-10T02:44:10.770034: step 2343, loss 0.24477, acc 0.9375, prec 0.055117, recall 0.811441
2017-12-10T02:44:11.039820: step 2344, loss 0.237454, acc 0.90625, prec 0.0551102, recall 0.811441
2017-12-10T02:44:11.310852: step 2345, loss 0.18657, acc 0.953125, prec 0.0551068, recall 0.811441
2017-12-10T02:44:11.584080: step 2346, loss 0.862232, acc 1, prec 0.0551262, recall 0.811498
2017-12-10T02:44:11.846418: step 2347, loss 0.121081, acc 0.96875, prec 0.0551434, recall 0.811555
2017-12-10T02:44:12.112811: step 2348, loss 0.0733137, acc 0.984375, prec 0.0551422, recall 0.811555
2017-12-10T02:44:12.375265: step 2349, loss 0.392862, acc 0.96875, prec 0.0551788, recall 0.811669
2017-12-10T02:44:12.644601: step 2350, loss 0.154513, acc 0.953125, prec 0.0552142, recall 0.811782
2017-12-10T02:44:12.909381: step 2351, loss 0.116862, acc 0.953125, prec 0.0552302, recall 0.811839
2017-12-10T02:44:13.179312: step 2352, loss 0.191304, acc 0.9375, prec 0.0552451, recall 0.811896
2017-12-10T02:44:13.440812: step 2353, loss 0.34629, acc 0.9375, prec 0.0552406, recall 0.811896
2017-12-10T02:44:13.700154: step 2354, loss 0.272548, acc 0.9375, prec 0.0552748, recall 0.81201
2017-12-10T02:44:13.966480: step 2355, loss 0.298724, acc 0.9375, prec 0.0552703, recall 0.81201
2017-12-10T02:44:14.228211: step 2356, loss 0.0783268, acc 0.96875, prec 0.055268, recall 0.81201
2017-12-10T02:44:14.495928: step 2357, loss 0.0699362, acc 0.96875, prec 0.0552658, recall 0.81201
2017-12-10T02:44:14.762289: step 2358, loss 0.144231, acc 0.9375, prec 0.0552806, recall 0.812066
2017-12-10T02:44:15.024874: step 2359, loss 0.294496, acc 0.921875, prec 0.0552749, recall 0.812066
2017-12-10T02:44:15.286780: step 2360, loss 0.0899775, acc 0.953125, prec 0.0552715, recall 0.812066
2017-12-10T02:44:15.549376: step 2361, loss 0.123949, acc 0.96875, prec 0.0553081, recall 0.81218
2017-12-10T02:44:15.815583: step 2362, loss 0.32795, acc 1, prec 0.0553274, recall 0.812236
2017-12-10T02:44:16.080111: step 2363, loss 2.05778, acc 0.953125, prec 0.0553252, recall 0.811992
2017-12-10T02:44:16.344552: step 2364, loss 0.382847, acc 0.9375, prec 0.05534, recall 0.812048
2017-12-10T02:44:16.614427: step 2365, loss 4.12631, acc 0.9375, prec 0.055356, recall 0.81186
2017-12-10T02:44:16.887262: step 2366, loss 0.227092, acc 0.921875, prec 0.0553503, recall 0.81186
2017-12-10T02:44:17.152801: step 2367, loss 0.356899, acc 0.921875, prec 0.0553446, recall 0.81186
2017-12-10T02:44:17.416132: step 2368, loss 0.271587, acc 0.859375, prec 0.0553732, recall 0.811974
2017-12-10T02:44:17.675420: step 2369, loss 0.116915, acc 0.96875, prec 0.0553903, recall 0.81203
2017-12-10T02:44:17.937709: step 2370, loss 0.655106, acc 0.84375, prec 0.0554371, recall 0.8122
2017-12-10T02:44:18.205794: step 2371, loss 0.573231, acc 0.84375, prec 0.0554451, recall 0.812256
2017-12-10T02:44:18.465996: step 2372, loss 0.324698, acc 0.921875, prec 0.0554781, recall 0.812369
2017-12-10T02:44:18.732672: step 2373, loss 0.331656, acc 0.859375, prec 0.0554679, recall 0.812369
2017-12-10T02:44:18.997898: step 2374, loss 0.462106, acc 0.875, prec 0.0554588, recall 0.812369
2017-12-10T02:44:19.258976: step 2375, loss 0.731998, acc 0.859375, prec 0.0554873, recall 0.812481
2017-12-10T02:44:19.523695: step 2376, loss 0.596041, acc 0.765625, prec 0.0555089, recall 0.812594
2017-12-10T02:44:19.782810: step 2377, loss 0.49863, acc 0.859375, prec 0.0554987, recall 0.812594
2017-12-10T02:44:20.046363: step 2378, loss 0.587609, acc 0.796875, prec 0.0555032, recall 0.81265
2017-12-10T02:44:20.314095: step 2379, loss 0.517278, acc 0.859375, prec 0.0555123, recall 0.812706
2017-12-10T02:44:20.584261: step 2380, loss 0.476526, acc 0.828125, prec 0.0554998, recall 0.812706
2017-12-10T02:44:20.851736: step 2381, loss 0.457308, acc 0.9375, prec 0.0555146, recall 0.812762
2017-12-10T02:44:21.113140: step 2382, loss 0.720749, acc 0.8125, prec 0.0555203, recall 0.812818
2017-12-10T02:44:21.378469: step 2383, loss 0.25982, acc 0.890625, prec 0.0555317, recall 0.812874
2017-12-10T02:44:21.637168: step 2384, loss 0.278025, acc 0.890625, prec 0.0555237, recall 0.812874
2017-12-10T02:44:21.895320: step 2385, loss 0.16808, acc 0.953125, prec 0.0555976, recall 0.813098
2017-12-10T02:44:22.157366: step 2386, loss 0.392056, acc 0.859375, prec 0.0556067, recall 0.813154
2017-12-10T02:44:22.417779: step 2387, loss 0.143457, acc 0.9375, prec 0.0556407, recall 0.813266
2017-12-10T02:44:22.678182: step 2388, loss 0.0797035, acc 0.96875, prec 0.0556771, recall 0.813377
2017-12-10T02:44:22.942149: step 2389, loss 0.120753, acc 0.953125, prec 0.0556736, recall 0.813377
2017-12-10T02:44:23.204501: step 2390, loss 0.641002, acc 0.890625, prec 0.0557043, recall 0.813488
2017-12-10T02:44:23.476139: step 2391, loss 0.16236, acc 0.9375, prec 0.0556997, recall 0.813488
2017-12-10T02:44:23.735024: step 2392, loss 0.118105, acc 0.96875, prec 0.0556974, recall 0.813488
2017-12-10T02:44:23.999070: step 2393, loss 0.0471918, acc 0.984375, prec 0.0556963, recall 0.813488
2017-12-10T02:44:24.258073: step 2394, loss 0.222395, acc 0.953125, prec 0.0556929, recall 0.813488
2017-12-10T02:44:24.532216: step 2395, loss 0.168602, acc 0.921875, prec 0.0557065, recall 0.813544
2017-12-10T02:44:24.793484: step 2396, loss 0.10999, acc 0.96875, prec 0.0557235, recall 0.8136
2017-12-10T02:44:25.059220: step 2397, loss 0.189179, acc 0.9375, prec 0.055719, recall 0.8136
2017-12-10T02:44:25.322066: step 2398, loss 0.192295, acc 0.953125, prec 0.0557348, recall 0.813655
2017-12-10T02:44:25.585493: step 2399, loss 0.0123326, acc 1, prec 0.0557541, recall 0.813711
2017-12-10T02:44:25.849898: step 2400, loss 1.32137, acc 0.96875, prec 0.0557723, recall 0.813524

Evaluation:
2017-12-10T02:44:33.467794: step 2400, loss 4.23763, acc 0.965468, prec 0.0564092, recall 0.789264

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-2400

2017-12-10T02:44:34.715297: step 2401, loss 0.137717, acc 0.984375, prec 0.0564081, recall 0.789264
2017-12-10T02:44:34.977216: step 2402, loss 0.340828, acc 0.921875, prec 0.0564406, recall 0.789384
2017-12-10T02:44:35.242760: step 2403, loss 0.164193, acc 0.953125, prec 0.0564564, recall 0.789444
2017-12-10T02:44:35.503395: step 2404, loss 0.118652, acc 0.953125, prec 0.0564529, recall 0.789444
2017-12-10T02:44:35.771498: step 2405, loss 0.0989655, acc 0.9375, prec 0.0564675, recall 0.789504
2017-12-10T02:44:36.040027: step 2406, loss 0.0634458, acc 0.96875, prec 0.0564652, recall 0.789504
2017-12-10T02:44:36.306784: step 2407, loss 0.108021, acc 0.96875, prec 0.0564629, recall 0.789504
2017-12-10T02:44:36.572610: step 2408, loss 0.039742, acc 1, prec 0.0564629, recall 0.789504
2017-12-10T02:44:36.837281: step 2409, loss 0.177398, acc 0.9375, prec 0.0564583, recall 0.789504
2017-12-10T02:44:37.107285: step 2410, loss 0.107512, acc 0.9375, prec 0.0564537, recall 0.789504
2017-12-10T02:44:37.367642: step 2411, loss 1.90155, acc 0.921875, prec 0.0564492, recall 0.78928
2017-12-10T02:44:37.632677: step 2412, loss 0.202245, acc 0.90625, prec 0.0564614, recall 0.789339
2017-12-10T02:44:37.896974: step 2413, loss 0.2061, acc 0.96875, prec 0.0564783, recall 0.789399
2017-12-10T02:44:38.164670: step 2414, loss 0.212515, acc 0.921875, prec 0.0564917, recall 0.789459
2017-12-10T02:44:38.429471: step 2415, loss 1.23419, acc 0.9375, prec 0.0565074, recall 0.789295
2017-12-10T02:44:38.693727: step 2416, loss 0.563874, acc 0.859375, prec 0.0564971, recall 0.789295
2017-12-10T02:44:38.961888: step 2417, loss 0.135532, acc 0.921875, prec 0.0564913, recall 0.789295
2017-12-10T02:44:39.221959: step 2418, loss 0.205405, acc 0.90625, prec 0.0565036, recall 0.789355
2017-12-10T02:44:39.479852: step 2419, loss 0.345623, acc 0.828125, prec 0.056491, recall 0.789355
2017-12-10T02:44:39.744551: step 2420, loss 0.341905, acc 0.875, prec 0.056501, recall 0.789414
2017-12-10T02:44:40.013006: step 2421, loss 0.455658, acc 0.84375, prec 0.0564895, recall 0.789414
2017-12-10T02:44:40.278629: step 2422, loss 0.40762, acc 0.84375, prec 0.0564781, recall 0.789414
2017-12-10T02:44:40.541047: step 2423, loss 0.350478, acc 0.875, prec 0.056488, recall 0.789474
2017-12-10T02:44:40.802585: step 2424, loss 0.382491, acc 0.859375, prec 0.0564777, recall 0.789474
2017-12-10T02:44:41.070732: step 2425, loss 0.25174, acc 0.90625, prec 0.0564709, recall 0.789474
2017-12-10T02:44:41.332317: step 2426, loss 0.210623, acc 0.953125, prec 0.0564865, recall 0.789533
2017-12-10T02:44:41.600572: step 2427, loss 0.381865, acc 0.90625, prec 0.0564988, recall 0.789593
2017-12-10T02:44:41.859742: step 2428, loss 0.270363, acc 0.90625, prec 0.0564919, recall 0.789593
2017-12-10T02:44:42.120966: step 2429, loss 0.295679, acc 0.90625, prec 0.0564851, recall 0.789593
2017-12-10T02:44:42.389900: step 2430, loss 0.240711, acc 0.953125, prec 0.0565389, recall 0.789771
2017-12-10T02:44:42.656844: step 2431, loss 0.197164, acc 0.953125, prec 0.0565545, recall 0.789831
2017-12-10T02:44:42.928252: step 2432, loss 0.0454226, acc 1, prec 0.0565927, recall 0.789949
2017-12-10T02:44:43.197302: step 2433, loss 0.563357, acc 0.890625, prec 0.0566038, recall 0.790008
2017-12-10T02:44:43.461022: step 2434, loss 0.388549, acc 0.921875, prec 0.0566171, recall 0.790068
2017-12-10T02:44:43.722842: step 2435, loss 0.0589995, acc 0.96875, prec 0.0566148, recall 0.790068
2017-12-10T02:44:43.987276: step 2436, loss 0.236176, acc 0.90625, prec 0.0566461, recall 0.790186
2017-12-10T02:44:44.246813: step 2437, loss 0.12214, acc 0.921875, prec 0.0566404, recall 0.790186
2017-12-10T02:44:44.512767: step 2438, loss 0.0916787, acc 0.96875, prec 0.0566381, recall 0.790186
2017-12-10T02:44:44.782366: step 2439, loss 0.340574, acc 0.953125, prec 0.0566537, recall 0.790245
2017-12-10T02:44:45.042285: step 2440, loss 0.0683688, acc 0.953125, prec 0.0566503, recall 0.790245
2017-12-10T02:44:45.307146: step 2441, loss 0.0215866, acc 0.984375, prec 0.0566492, recall 0.790245
2017-12-10T02:44:45.565961: step 2442, loss 0.0866556, acc 0.984375, prec 0.0566671, recall 0.790304
2017-12-10T02:44:45.831507: step 2443, loss 1.6393, acc 0.953125, prec 0.0566648, recall 0.790082
2017-12-10T02:44:46.104091: step 2444, loss 0.044792, acc 0.984375, prec 0.0566636, recall 0.790082
2017-12-10T02:44:46.363422: step 2445, loss 0.0664544, acc 0.953125, prec 0.0566602, recall 0.790082
2017-12-10T02:44:46.629207: step 2446, loss 0.974934, acc 0.984375, prec 0.0567162, recall 0.790259
2017-12-10T02:44:46.894809: step 2447, loss 0.644988, acc 0.96875, prec 0.056733, recall 0.790318
2017-12-10T02:44:47.160764: step 2448, loss 0.253239, acc 0.921875, prec 0.0567273, recall 0.790318
2017-12-10T02:44:47.431075: step 2449, loss 0.104642, acc 0.9375, prec 0.0567227, recall 0.790318
2017-12-10T02:44:47.693481: step 2450, loss 0.119909, acc 0.953125, prec 0.0567193, recall 0.790318
2017-12-10T02:44:47.960377: step 2451, loss 0.299114, acc 0.90625, prec 0.0567314, recall 0.790377
2017-12-10T02:44:48.224866: step 2452, loss 0.214885, acc 0.921875, prec 0.0567638, recall 0.790495
2017-12-10T02:44:48.487453: step 2453, loss 0.461319, acc 0.890625, prec 0.0567558, recall 0.790495
2017-12-10T02:44:48.759388: step 2454, loss 0.305767, acc 0.90625, prec 0.0567679, recall 0.790554
2017-12-10T02:44:49.026490: step 2455, loss 0.365541, acc 0.875, prec 0.0567778, recall 0.790613
2017-12-10T02:44:49.289301: step 2456, loss 0.591681, acc 0.796875, prec 0.056801, recall 0.79073
2017-12-10T02:44:49.553940: step 2457, loss 0.34302, acc 0.875, prec 0.0567918, recall 0.79073
2017-12-10T02:44:49.814881: step 2458, loss 0.330424, acc 0.890625, prec 0.0568218, recall 0.790848
2017-12-10T02:44:50.073569: step 2459, loss 0.0730999, acc 0.96875, prec 0.0568196, recall 0.790848
2017-12-10T02:44:50.330796: step 2460, loss 0.574355, acc 0.921875, prec 0.0568328, recall 0.790907
2017-12-10T02:44:50.598622: step 2461, loss 0.205253, acc 0.921875, prec 0.0568461, recall 0.790965
2017-12-10T02:44:50.857674: step 2462, loss 0.226217, acc 0.890625, prec 0.0568571, recall 0.791024
2017-12-10T02:44:51.122531: step 2463, loss 0.467572, acc 0.90625, prec 0.0568883, recall 0.791141
2017-12-10T02:44:51.384012: step 2464, loss 0.228918, acc 0.921875, prec 0.0568825, recall 0.791141
2017-12-10T02:44:51.653765: step 2465, loss 0.176583, acc 0.921875, prec 0.0568768, recall 0.791141
2017-12-10T02:44:51.915167: step 2466, loss 0.178918, acc 0.9375, prec 0.0569102, recall 0.791258
2017-12-10T02:44:52.178884: step 2467, loss 0.095601, acc 0.96875, prec 0.056946, recall 0.791375
2017-12-10T02:44:52.441274: step 2468, loss 0.140508, acc 0.9375, prec 0.0569414, recall 0.791375
2017-12-10T02:44:52.706877: step 2469, loss 0.339753, acc 0.90625, prec 0.0569725, recall 0.791492
2017-12-10T02:44:52.966157: step 2470, loss 0.101985, acc 0.96875, prec 0.0569702, recall 0.791492
2017-12-10T02:44:53.226939: step 2471, loss 0.528533, acc 0.921875, prec 0.0569834, recall 0.79155
2017-12-10T02:44:53.498499: step 2472, loss 1.53644, acc 0.953125, prec 0.0569811, recall 0.791329
2017-12-10T02:44:53.771749: step 2473, loss 0.139052, acc 0.9375, prec 0.0569766, recall 0.791329
2017-12-10T02:44:54.040063: step 2474, loss 0.187059, acc 0.953125, prec 0.0569731, recall 0.791329
2017-12-10T02:44:54.301128: step 2475, loss 0.105809, acc 0.96875, prec 0.0569708, recall 0.791329
2017-12-10T02:44:54.563701: step 2476, loss 0.186329, acc 0.953125, prec 0.0569674, recall 0.791329
2017-12-10T02:44:54.825181: step 2477, loss 0.347371, acc 0.90625, prec 0.0569605, recall 0.791329
2017-12-10T02:44:55.085750: step 2478, loss 0.234476, acc 0.953125, prec 0.056976, recall 0.791387
2017-12-10T02:44:55.355326: step 2479, loss 0.51236, acc 0.921875, prec 0.0570083, recall 0.791504
2017-12-10T02:44:55.619058: step 2480, loss 0.287212, acc 0.953125, prec 0.0570238, recall 0.791562
2017-12-10T02:44:55.880469: step 2481, loss 0.176215, acc 0.9375, prec 0.0570572, recall 0.791678
2017-12-10T02:44:56.144249: step 2482, loss 0.251326, acc 0.921875, prec 0.0570514, recall 0.791678
2017-12-10T02:44:56.416086: step 2483, loss 0.597526, acc 0.9375, prec 0.0570658, recall 0.791736
2017-12-10T02:44:56.677528: step 2484, loss 0.14051, acc 0.9375, prec 0.0570612, recall 0.791736
2017-12-10T02:44:56.911047: step 2485, loss 0.191392, acc 0.941176, prec 0.0570578, recall 0.791736
2017-12-10T02:44:57.181533: step 2486, loss 0.0806373, acc 0.953125, prec 0.0570543, recall 0.791736
2017-12-10T02:44:57.441110: step 2487, loss 0.537296, acc 0.90625, prec 0.0570854, recall 0.791853
2017-12-10T02:44:57.708485: step 2488, loss 0.285254, acc 0.96875, prec 0.0571021, recall 0.791911
2017-12-10T02:44:57.972549: step 2489, loss 0.242858, acc 0.9375, prec 0.0570975, recall 0.791911
2017-12-10T02:44:58.231043: step 2490, loss 0.241722, acc 0.90625, prec 0.0570906, recall 0.791911
2017-12-10T02:44:58.493112: step 2491, loss 0.357406, acc 0.90625, prec 0.0571026, recall 0.791969
2017-12-10T02:44:58.754791: step 2492, loss 0.151317, acc 0.921875, prec 0.0570969, recall 0.791969
2017-12-10T02:44:59.023349: step 2493, loss 0.184793, acc 0.9375, prec 0.0570923, recall 0.791969
2017-12-10T02:44:59.284679: step 2494, loss 0.272921, acc 0.875, prec 0.0571021, recall 0.792027
2017-12-10T02:44:59.544587: step 2495, loss 0.325944, acc 0.90625, prec 0.0570952, recall 0.792027
2017-12-10T02:44:59.802874: step 2496, loss 0.209665, acc 0.921875, prec 0.0570895, recall 0.792027
2017-12-10T02:45:00.074072: step 2497, loss 0.225748, acc 0.9375, prec 0.0571038, recall 0.792085
2017-12-10T02:45:00.343177: step 2498, loss 1.61317, acc 0.9375, prec 0.0571004, recall 0.791864
2017-12-10T02:45:00.614746: step 2499, loss 0.448049, acc 0.921875, prec 0.0571136, recall 0.791922
2017-12-10T02:45:00.884396: step 2500, loss 0.368837, acc 0.984375, prec 0.0571693, recall 0.792096
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-2500

2017-12-10T02:45:02.184069: step 2501, loss 0.177326, acc 0.96875, prec 0.0571859, recall 0.792154
2017-12-10T02:45:02.446200: step 2502, loss 0.159137, acc 0.96875, prec 0.0572215, recall 0.792269
2017-12-10T02:45:02.707413: step 2503, loss 0.226549, acc 0.921875, prec 0.0572347, recall 0.792327
2017-12-10T02:45:02.968853: step 2504, loss 0.0664409, acc 0.984375, prec 0.0572524, recall 0.792385
2017-12-10T02:45:03.228518: step 2505, loss 0.173966, acc 0.96875, prec 0.0572501, recall 0.792385
2017-12-10T02:45:03.486551: step 2506, loss 0.174297, acc 0.90625, prec 0.0573, recall 0.792558
2017-12-10T02:45:03.747780: step 2507, loss 0.425781, acc 0.875, prec 0.0572908, recall 0.792558
2017-12-10T02:45:04.016015: step 2508, loss 0.134723, acc 0.9375, prec 0.0572862, recall 0.792558
2017-12-10T02:45:04.278615: step 2509, loss 0.0979136, acc 0.96875, prec 0.0573218, recall 0.792673
2017-12-10T02:45:04.546642: step 2510, loss 0.126906, acc 0.921875, prec 0.057316, recall 0.792673
2017-12-10T02:45:04.807548: step 2511, loss 0.138448, acc 0.9375, prec 0.0573114, recall 0.792673
2017-12-10T02:45:05.076034: step 2512, loss 0.45428, acc 0.953125, prec 0.0573458, recall 0.792788
2017-12-10T02:45:05.340869: step 2513, loss 0.158535, acc 0.96875, prec 0.0573435, recall 0.792788
2017-12-10T02:45:05.606390: step 2514, loss 0.180782, acc 0.921875, prec 0.0573756, recall 0.792903
2017-12-10T02:45:05.865216: step 2515, loss 0.180136, acc 0.9375, prec 0.0573899, recall 0.79296
2017-12-10T02:45:06.129623: step 2516, loss 0.215564, acc 0.96875, prec 0.0574065, recall 0.793017
2017-12-10T02:45:06.392603: step 2517, loss 0.17188, acc 0.984375, prec 0.0574431, recall 0.793132
2017-12-10T02:45:06.652090: step 2518, loss 0.320379, acc 0.921875, prec 0.0574563, recall 0.793189
2017-12-10T02:45:06.912385: step 2519, loss 0.387925, acc 0.921875, prec 0.0574694, recall 0.793247
2017-12-10T02:45:07.178423: step 2520, loss 0.176578, acc 0.953125, prec 0.057466, recall 0.793247
2017-12-10T02:45:07.448345: step 2521, loss 0.25542, acc 0.90625, prec 0.0574779, recall 0.793304
2017-12-10T02:45:07.710350: step 2522, loss 0.0974609, acc 0.96875, prec 0.0575134, recall 0.793418
2017-12-10T02:45:07.971737: step 2523, loss 0.0714754, acc 0.984375, prec 0.0575123, recall 0.793418
2017-12-10T02:45:08.234404: step 2524, loss 0.191931, acc 0.984375, prec 0.05753, recall 0.793475
2017-12-10T02:45:08.497808: step 2525, loss 0.0491141, acc 0.953125, prec 0.0575266, recall 0.793475
2017-12-10T02:45:08.760858: step 2526, loss 0.0070405, acc 1, prec 0.0575266, recall 0.793475
2017-12-10T02:45:09.028948: step 2527, loss 0.154415, acc 0.96875, prec 0.0575431, recall 0.793532
2017-12-10T02:45:09.293594: step 2528, loss 0.256931, acc 0.9375, prec 0.0575385, recall 0.793532
2017-12-10T02:45:09.556260: step 2529, loss 0.0175647, acc 1, prec 0.0575574, recall 0.793589
2017-12-10T02:45:09.826926: step 2530, loss 0.0894153, acc 1, prec 0.0575763, recall 0.793646
2017-12-10T02:45:10.091410: step 2531, loss 0.161854, acc 0.9375, prec 0.0575717, recall 0.793646
2017-12-10T02:45:10.361881: step 2532, loss 0.186703, acc 0.921875, prec 0.0575659, recall 0.793646
2017-12-10T02:45:10.623766: step 2533, loss 0.227904, acc 0.921875, prec 0.057579, recall 0.793703
2017-12-10T02:45:10.891013: step 2534, loss 5.11856, acc 0.984375, prec 0.057579, recall 0.793484
2017-12-10T02:45:11.165381: step 2535, loss 0.570135, acc 0.96875, prec 0.0575956, recall 0.793541
2017-12-10T02:45:11.447795: step 2536, loss 0.172959, acc 0.9375, prec 0.057591, recall 0.793541
2017-12-10T02:45:11.714895: step 2537, loss 0.0626157, acc 0.96875, prec 0.0576076, recall 0.793598
2017-12-10T02:45:11.971686: step 2538, loss 0.258974, acc 0.921875, prec 0.0576207, recall 0.793655
2017-12-10T02:45:12.241107: step 2539, loss 0.0393898, acc 0.984375, prec 0.0576195, recall 0.793655
2017-12-10T02:45:12.506441: step 2540, loss 0.145454, acc 0.96875, prec 0.0576172, recall 0.793655
2017-12-10T02:45:12.774829: step 2541, loss 0.238112, acc 0.921875, prec 0.0576303, recall 0.793712
2017-12-10T02:45:13.036142: step 2542, loss 0.706878, acc 0.859375, prec 0.0576199, recall 0.793712
2017-12-10T02:45:13.301115: step 2543, loss 0.222852, acc 0.9375, prec 0.057653, recall 0.793826
2017-12-10T02:45:13.560675: step 2544, loss 0.445815, acc 0.875, prec 0.0576627, recall 0.793883
2017-12-10T02:45:13.822350: step 2545, loss 0.363686, acc 0.90625, prec 0.0576746, recall 0.793939
2017-12-10T02:45:14.088810: step 2546, loss 0.559022, acc 0.875, prec 0.0576654, recall 0.793939
2017-12-10T02:45:14.348266: step 2547, loss 0.360994, acc 0.828125, prec 0.0576527, recall 0.793939
2017-12-10T02:45:14.610958: step 2548, loss 0.406611, acc 0.890625, prec 0.0576823, recall 0.794053
2017-12-10T02:45:14.874352: step 2549, loss 0.0543901, acc 0.984375, prec 0.0577188, recall 0.794166
2017-12-10T02:45:15.134205: step 2550, loss 0.322303, acc 0.921875, prec 0.0577508, recall 0.794279
2017-12-10T02:45:15.393614: step 2551, loss 0.39886, acc 0.90625, prec 0.0577438, recall 0.794279
2017-12-10T02:45:15.660229: step 2552, loss 0.353437, acc 0.890625, prec 0.0577734, recall 0.794393
2017-12-10T02:45:15.923776: step 2553, loss 0.53569, acc 0.8125, prec 0.0577784, recall 0.794449
2017-12-10T02:45:16.182264: step 2554, loss 0.65688, acc 0.859375, prec 0.057768, recall 0.794449
2017-12-10T02:45:16.449400: step 2555, loss 0.337665, acc 0.90625, prec 0.0577611, recall 0.794449
2017-12-10T02:45:16.712989: step 2556, loss 0.145741, acc 0.953125, prec 0.0577764, recall 0.794505
2017-12-10T02:45:16.974165: step 2557, loss 0.063837, acc 0.984375, prec 0.0577753, recall 0.794505
2017-12-10T02:45:17.239738: step 2558, loss 1.50853, acc 0.921875, prec 0.0578083, recall 0.7944
2017-12-10T02:45:17.506634: step 2559, loss 0.328378, acc 0.9375, prec 0.0578225, recall 0.794457
2017-12-10T02:45:17.774182: step 2560, loss 0.216271, acc 0.953125, prec 0.0578567, recall 0.794569
2017-12-10T02:45:18.041097: step 2561, loss 0.526528, acc 0.90625, prec 0.0578498, recall 0.794569
2017-12-10T02:45:18.311502: step 2562, loss 0.289674, acc 0.890625, prec 0.0578793, recall 0.794682
2017-12-10T02:45:18.570626: step 2563, loss 3.05917, acc 0.890625, prec 0.0578735, recall 0.794247
2017-12-10T02:45:18.835555: step 2564, loss 0.594992, acc 0.859375, prec 0.0578631, recall 0.794247
2017-12-10T02:45:19.098589: step 2565, loss 0.321311, acc 0.890625, prec 0.0578738, recall 0.794303
2017-12-10T02:45:19.359480: step 2566, loss 0.348507, acc 0.875, prec 0.0579022, recall 0.794416
2017-12-10T02:45:19.622270: step 2567, loss 0.506145, acc 0.875, prec 0.0579117, recall 0.794472
2017-12-10T02:45:19.888194: step 2568, loss 0.461894, acc 0.859375, prec 0.0579201, recall 0.794528
2017-12-10T02:45:20.151200: step 2569, loss 0.379351, acc 0.84375, prec 0.0579274, recall 0.794584
2017-12-10T02:45:20.413753: step 2570, loss 0.358588, acc 0.859375, prec 0.057917, recall 0.794584
2017-12-10T02:45:20.670914: step 2571, loss 0.741311, acc 0.890625, prec 0.0579277, recall 0.79464
2017-12-10T02:45:20.926042: step 2572, loss 0.274609, acc 0.859375, prec 0.0579361, recall 0.794697
2017-12-10T02:45:21.193939: step 2573, loss 0.483391, acc 0.875, prec 0.0579268, recall 0.794697
2017-12-10T02:45:21.455607: step 2574, loss 0.565322, acc 0.875, prec 0.0579176, recall 0.794697
2017-12-10T02:45:21.715304: step 2575, loss 0.580912, acc 0.84375, prec 0.0579061, recall 0.794697
2017-12-10T02:45:21.975562: step 2576, loss 0.534772, acc 0.828125, prec 0.0579121, recall 0.794753
2017-12-10T02:45:22.239590: step 2577, loss 0.422166, acc 0.875, prec 0.0579404, recall 0.794865
2017-12-10T02:45:22.507190: step 2578, loss 0.413054, acc 0.890625, prec 0.0579511, recall 0.794921
2017-12-10T02:45:22.767316: step 2579, loss 0.515399, acc 0.828125, prec 0.0579759, recall 0.795033
2017-12-10T02:45:23.030647: step 2580, loss 0.257619, acc 0.90625, prec 0.0579877, recall 0.795089
2017-12-10T02:45:23.290944: step 2581, loss 0.34934, acc 0.90625, prec 0.0579996, recall 0.795145
2017-12-10T02:45:23.559367: step 2582, loss 0.178696, acc 0.953125, prec 0.0580336, recall 0.795256
2017-12-10T02:45:23.824515: step 2583, loss 0.123029, acc 0.96875, prec 0.05805, recall 0.795312
2017-12-10T02:45:24.089712: step 2584, loss 0.0517806, acc 1, prec 0.05805, recall 0.795312
2017-12-10T02:45:24.349267: step 2585, loss 4.86819, acc 0.96875, prec 0.0580489, recall 0.795095
2017-12-10T02:45:24.617718: step 2586, loss 0.221456, acc 0.9375, prec 0.0580442, recall 0.795095
2017-12-10T02:45:24.884509: step 2587, loss 0.355426, acc 0.953125, prec 0.0580595, recall 0.795151
2017-12-10T02:45:25.147906: step 2588, loss 0.0581408, acc 0.953125, prec 0.058056, recall 0.795151
2017-12-10T02:45:25.404664: step 2589, loss 0.1339, acc 0.953125, prec 0.0580526, recall 0.795151
2017-12-10T02:45:25.672588: step 2590, loss 0.370547, acc 0.921875, prec 0.0580655, recall 0.795207
2017-12-10T02:45:25.936740: step 2591, loss 0.0298576, acc 1, prec 0.058103, recall 0.795318
2017-12-10T02:45:26.207901: step 2592, loss 0.285279, acc 0.90625, prec 0.0581335, recall 0.79543
2017-12-10T02:45:26.484533: step 2593, loss 0.0788793, acc 0.96875, prec 0.0581499, recall 0.795485
2017-12-10T02:45:26.753587: step 2594, loss 0.189241, acc 0.96875, prec 0.0581476, recall 0.795485
2017-12-10T02:45:27.011862: step 2595, loss 0.268047, acc 0.9375, prec 0.0581617, recall 0.795541
2017-12-10T02:45:27.273106: step 2596, loss 1.26248, acc 0.96875, prec 0.0581793, recall 0.79538
2017-12-10T02:45:27.543949: step 2597, loss 0.221164, acc 0.9375, prec 0.0581934, recall 0.795436
2017-12-10T02:45:27.803148: step 2598, loss 0.161324, acc 0.96875, prec 0.0582472, recall 0.795603
2017-12-10T02:45:28.062978: step 2599, loss 0.187172, acc 0.953125, prec 0.0582812, recall 0.795713
2017-12-10T02:45:28.321994: step 2600, loss 0.133653, acc 0.9375, prec 0.0582765, recall 0.795713
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-2600

2017-12-10T02:45:29.589859: step 2601, loss 0.194784, acc 0.921875, prec 0.0582895, recall 0.795769
2017-12-10T02:45:29.856716: step 2602, loss 0.407376, acc 0.921875, prec 0.0583024, recall 0.795824
2017-12-10T02:45:30.125615: step 2603, loss 0.460397, acc 0.875, prec 0.0583118, recall 0.79588
2017-12-10T02:45:30.399034: step 2604, loss 0.544033, acc 0.921875, prec 0.0583621, recall 0.796045
2017-12-10T02:45:30.670885: step 2605, loss 0.114054, acc 0.921875, prec 0.0583563, recall 0.796045
2017-12-10T02:45:30.933940: step 2606, loss 1.19791, acc 0.921875, prec 0.0583692, recall 0.796101
2017-12-10T02:45:31.194032: step 2607, loss 0.267309, acc 0.890625, prec 0.0583985, recall 0.796211
2017-12-10T02:45:31.451960: step 2608, loss 0.46201, acc 0.84375, prec 0.0583869, recall 0.796211
2017-12-10T02:45:31.718549: step 2609, loss 0.423342, acc 0.84375, prec 0.0583753, recall 0.796211
2017-12-10T02:45:31.978981: step 2610, loss 0.0690113, acc 0.96875, prec 0.058373, recall 0.796211
2017-12-10T02:45:32.244795: step 2611, loss 0.462497, acc 0.84375, prec 0.0583614, recall 0.796211
2017-12-10T02:45:32.505317: step 2612, loss 0.202072, acc 0.890625, prec 0.058372, recall 0.796266
2017-12-10T02:45:32.768342: step 2613, loss 0.352841, acc 0.890625, prec 0.0583826, recall 0.796321
2017-12-10T02:45:33.025799: step 2614, loss 0.292756, acc 0.921875, prec 0.0583768, recall 0.796321
2017-12-10T02:45:33.295676: step 2615, loss 1.8497, acc 0.890625, prec 0.0583698, recall 0.796106
2017-12-10T02:45:33.556287: step 2616, loss 0.260746, acc 0.9375, prec 0.0583839, recall 0.796161
2017-12-10T02:45:33.816929: step 2617, loss 0.476493, acc 0.859375, prec 0.0584108, recall 0.796271
2017-12-10T02:45:34.077641: step 2618, loss 0.16718, acc 0.9375, prec 0.0584621, recall 0.796436
2017-12-10T02:45:34.348269: step 2619, loss 0.192203, acc 0.921875, prec 0.058475, recall 0.796491
2017-12-10T02:45:34.611047: step 2620, loss 0.292714, acc 0.921875, prec 0.0584692, recall 0.796491
2017-12-10T02:45:34.878307: step 2621, loss 0.425457, acc 0.859375, prec 0.0584588, recall 0.796491
2017-12-10T02:45:35.139174: step 2622, loss 0.189442, acc 0.953125, prec 0.0584553, recall 0.796491
2017-12-10T02:45:35.398606: step 2623, loss 0.398304, acc 0.921875, prec 0.0584495, recall 0.796491
2017-12-10T02:45:35.665883: step 2624, loss 0.266196, acc 0.90625, prec 0.0584426, recall 0.796491
2017-12-10T02:45:35.923590: step 2625, loss 0.297913, acc 0.953125, prec 0.0584391, recall 0.796491
2017-12-10T02:45:36.189375: step 2626, loss 0.165113, acc 0.921875, prec 0.0584333, recall 0.796491
2017-12-10T02:45:36.453639: step 2627, loss 0.314067, acc 0.9375, prec 0.0584846, recall 0.796656
2017-12-10T02:45:36.718667: step 2628, loss 0.217258, acc 0.9375, prec 0.0584986, recall 0.796711
2017-12-10T02:45:36.985346: step 2629, loss 0.319017, acc 0.90625, prec 0.0585103, recall 0.796766
2017-12-10T02:45:37.247741: step 2630, loss 0.107549, acc 0.96875, prec 0.0585266, recall 0.79682
2017-12-10T02:45:37.508391: step 2631, loss 0.0333312, acc 1, prec 0.0585266, recall 0.79682
2017-12-10T02:45:37.772102: step 2632, loss 0.119928, acc 0.96875, prec 0.058543, recall 0.796875
2017-12-10T02:45:38.032679: step 2633, loss 0.202374, acc 0.953125, prec 0.0585954, recall 0.797039
2017-12-10T02:45:38.299710: step 2634, loss 9.46378, acc 0.9375, prec 0.0585919, recall 0.796825
2017-12-10T02:45:38.567635: step 2635, loss 0.0319691, acc 0.984375, prec 0.0585907, recall 0.796825
2017-12-10T02:45:38.828674: step 2636, loss 0.270723, acc 0.921875, prec 0.0585849, recall 0.796825
2017-12-10T02:45:39.098146: step 2637, loss 0.308271, acc 0.890625, prec 0.0585954, recall 0.796879
2017-12-10T02:45:39.364816: step 2638, loss 0.308494, acc 0.921875, prec 0.0585897, recall 0.796879
2017-12-10T02:45:39.625767: step 2639, loss 0.200491, acc 0.953125, prec 0.0586048, recall 0.796934
2017-12-10T02:45:39.889185: step 2640, loss 0.350285, acc 0.890625, prec 0.0585967, recall 0.796934
2017-12-10T02:45:40.153657: step 2641, loss 0.435441, acc 0.90625, prec 0.0586083, recall 0.796988
2017-12-10T02:45:40.415792: step 2642, loss 0.122644, acc 0.9375, prec 0.0586223, recall 0.797043
2017-12-10T02:45:40.680477: step 2643, loss 0.445484, acc 0.890625, prec 0.0586328, recall 0.797098
2017-12-10T02:45:40.945184: step 2644, loss 0.330725, acc 0.953125, prec 0.0586293, recall 0.797098
2017-12-10T02:45:41.204400: step 2645, loss 0.275903, acc 0.90625, prec 0.058641, recall 0.797152
2017-12-10T02:45:41.476094: step 2646, loss 0.175819, acc 0.921875, prec 0.0586352, recall 0.797152
2017-12-10T02:45:41.738771: step 2647, loss 0.0942765, acc 0.953125, prec 0.0586317, recall 0.797152
2017-12-10T02:45:42.008215: step 2648, loss 0.355553, acc 0.90625, prec 0.0586434, recall 0.797207
2017-12-10T02:45:42.275870: step 2649, loss 1.1358, acc 0.921875, prec 0.0586562, recall 0.797261
2017-12-10T02:45:42.543571: step 2650, loss 0.14654, acc 0.9375, prec 0.0586515, recall 0.797261
2017-12-10T02:45:42.803761: step 2651, loss 0.244667, acc 0.9375, prec 0.0586655, recall 0.797315
2017-12-10T02:45:43.070033: step 2652, loss 1.78281, acc 0.890625, prec 0.0586771, recall 0.797156
2017-12-10T02:45:43.334036: step 2653, loss 0.483677, acc 0.90625, prec 0.0587074, recall 0.797265
2017-12-10T02:45:43.592392: step 2654, loss 0.386891, acc 0.890625, prec 0.0587364, recall 0.797373
2017-12-10T02:45:43.855346: step 2655, loss 0.570662, acc 0.828125, prec 0.0587237, recall 0.797373
2017-12-10T02:45:44.116869: step 2656, loss 0.3945, acc 0.921875, prec 0.0587179, recall 0.797373
2017-12-10T02:45:44.380496: step 2657, loss 0.420959, acc 0.84375, prec 0.0587063, recall 0.797373
2017-12-10T02:45:44.637911: step 2658, loss 0.474128, acc 0.859375, prec 0.0586959, recall 0.797373
2017-12-10T02:45:44.903638: step 2659, loss 0.404279, acc 0.90625, prec 0.0587261, recall 0.797482
2017-12-10T02:45:45.167277: step 2660, loss 0.357184, acc 0.859375, prec 0.0587156, recall 0.797482
2017-12-10T02:45:45.427952: step 2661, loss 0.301204, acc 0.921875, prec 0.0587284, recall 0.797536
2017-12-10T02:45:45.685913: step 2662, loss 0.820708, acc 0.8125, prec 0.0587145, recall 0.797536
2017-12-10T02:45:45.952210: step 2663, loss 1.13321, acc 0.765625, prec 0.0587157, recall 0.79759
2017-12-10T02:45:46.218235: step 2664, loss 0.49417, acc 0.84375, prec 0.0587041, recall 0.79759
2017-12-10T02:45:46.486655: step 2665, loss 1.05498, acc 0.90625, prec 0.0587343, recall 0.797699
2017-12-10T02:45:46.753048: step 2666, loss 0.793627, acc 0.875, prec 0.0587621, recall 0.797807
2017-12-10T02:45:47.027644: step 2667, loss 0.323996, acc 0.890625, prec 0.0587726, recall 0.797861
2017-12-10T02:45:47.293265: step 2668, loss 0.449775, acc 0.96875, prec 0.0587888, recall 0.797915
2017-12-10T02:45:47.555004: step 2669, loss 0.373043, acc 0.9375, prec 0.0587841, recall 0.797915
2017-12-10T02:45:47.815964: step 2670, loss 0.339893, acc 0.90625, prec 0.0588143, recall 0.798023
2017-12-10T02:45:48.077167: step 2671, loss 0.445077, acc 0.875, prec 0.058805, recall 0.798023
2017-12-10T02:45:48.340439: step 2672, loss 0.618326, acc 0.765625, prec 0.0588247, recall 0.798131
2017-12-10T02:45:48.606154: step 2673, loss 0.23941, acc 0.921875, prec 0.0588189, recall 0.798131
2017-12-10T02:45:48.873520: step 2674, loss 0.520882, acc 0.859375, prec 0.0588455, recall 0.798239
2017-12-10T02:45:49.136379: step 2675, loss 0.228583, acc 0.9375, prec 0.0589335, recall 0.798507
2017-12-10T02:45:49.399572: step 2676, loss 0.242906, acc 0.921875, prec 0.0589462, recall 0.798561
2017-12-10T02:45:49.663878: step 2677, loss 0.260068, acc 0.90625, prec 0.0589392, recall 0.798561
2017-12-10T02:45:49.924355: step 2678, loss 0.115173, acc 0.96875, prec 0.0589369, recall 0.798561
2017-12-10T02:45:50.191299: step 2679, loss 0.359391, acc 0.921875, prec 0.0589311, recall 0.798561
2017-12-10T02:45:50.454590: step 2680, loss 0.121324, acc 0.9375, prec 0.0589265, recall 0.798561
2017-12-10T02:45:50.715734: step 2681, loss 0.429458, acc 0.9375, prec 0.0589588, recall 0.798668
2017-12-10T02:45:50.981860: step 2682, loss 0.186678, acc 0.921875, prec 0.058953, recall 0.798668
2017-12-10T02:45:51.255645: step 2683, loss 0.359929, acc 0.96875, prec 0.0589692, recall 0.798722
2017-12-10T02:45:51.532651: step 2684, loss 0.185274, acc 0.953125, prec 0.0589657, recall 0.798722
2017-12-10T02:45:51.805615: step 2685, loss 0.943924, acc 1, prec 0.0590397, recall 0.798936
2017-12-10T02:45:52.085672: step 2686, loss 0.0423411, acc 0.984375, prec 0.0590571, recall 0.79899
2017-12-10T02:45:52.350612: step 2687, loss 0.106337, acc 0.953125, prec 0.0590536, recall 0.79899
2017-12-10T02:45:52.611692: step 2688, loss 0.153558, acc 0.9375, prec 0.0590489, recall 0.79899
2017-12-10T02:45:52.874984: step 2689, loss 0.372686, acc 0.96875, prec 0.0590651, recall 0.799043
2017-12-10T02:45:53.140448: step 2690, loss 0.253649, acc 0.90625, prec 0.0591136, recall 0.799203
2017-12-10T02:45:53.409056: step 2691, loss 0.362552, acc 0.90625, prec 0.0591251, recall 0.799257
2017-12-10T02:45:53.672187: step 2692, loss 0.0899575, acc 1, prec 0.0591436, recall 0.79931
2017-12-10T02:45:53.936912: step 2693, loss 0.121695, acc 0.953125, prec 0.0591401, recall 0.79931
2017-12-10T02:45:54.202114: step 2694, loss 0.236008, acc 0.9375, prec 0.0591539, recall 0.799363
2017-12-10T02:45:54.469422: step 2695, loss 0.230192, acc 0.921875, prec 0.0591481, recall 0.799363
2017-12-10T02:45:54.736345: step 2696, loss 0.104537, acc 0.953125, prec 0.0591631, recall 0.799416
2017-12-10T02:45:54.996533: step 2697, loss 0.0234397, acc 1, prec 0.0591631, recall 0.799416
2017-12-10T02:45:55.261348: step 2698, loss 0.59501, acc 0.890625, prec 0.0591919, recall 0.799523
2017-12-10T02:45:55.522612: step 2699, loss 0.286169, acc 0.953125, prec 0.0592069, recall 0.799576
2017-12-10T02:45:55.791892: step 2700, loss 0.130909, acc 0.953125, prec 0.0592034, recall 0.799576

Evaluation:
2017-12-10T02:46:03.385141: step 2700, loss 2.95139, acc 0.944334, prec 0.0600656, recall 0.786331

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-2700

2017-12-10T02:46:04.636389: step 2701, loss 1.42576, acc 0.96875, prec 0.0600827, recall 0.786186
2017-12-10T02:46:04.902940: step 2702, loss 0.160997, acc 0.953125, prec 0.0600974, recall 0.78624
2017-12-10T02:46:05.174434: step 2703, loss 0.225932, acc 0.9375, prec 0.0600927, recall 0.78624
2017-12-10T02:46:05.437633: step 2704, loss 0.292145, acc 0.9375, prec 0.0600881, recall 0.78624
2017-12-10T02:46:05.705169: step 2705, loss 0.135377, acc 0.953125, prec 0.0600846, recall 0.78624
2017-12-10T02:46:05.966565: step 2706, loss 0.366132, acc 0.921875, prec 0.0600788, recall 0.78624
2017-12-10T02:46:06.236782: step 2707, loss 0.411654, acc 0.921875, prec 0.0601094, recall 0.786349
2017-12-10T02:46:06.500509: step 2708, loss 0.301181, acc 0.921875, prec 0.06014, recall 0.786457
2017-12-10T02:46:06.764049: step 2709, loss 0.0615297, acc 0.96875, prec 0.0601559, recall 0.786511
2017-12-10T02:46:07.034388: step 2710, loss 0.0792359, acc 0.96875, prec 0.0601718, recall 0.786565
2017-12-10T02:46:07.302293: step 2711, loss 0.485153, acc 0.90625, prec 0.0601648, recall 0.786565
2017-12-10T02:46:07.569210: step 2712, loss 0.45009, acc 0.9375, prec 0.0601784, recall 0.786619
2017-12-10T02:46:07.838943: step 2713, loss 0.188794, acc 0.921875, prec 0.0601725, recall 0.786619
2017-12-10T02:46:08.100014: step 2714, loss 0.117525, acc 0.953125, prec 0.0601873, recall 0.786673
2017-12-10T02:46:08.361974: step 2715, loss 0.124501, acc 0.953125, prec 0.0601837, recall 0.786673
2017-12-10T02:46:08.627063: step 2716, loss 0.110206, acc 0.96875, prec 0.0602178, recall 0.786781
2017-12-10T02:46:08.892881: step 2717, loss 0.0909345, acc 0.984375, prec 0.0602167, recall 0.786781
2017-12-10T02:46:09.155482: step 2718, loss 0.403773, acc 0.9375, prec 0.0602484, recall 0.786889
2017-12-10T02:46:09.417671: step 2719, loss 0.0560395, acc 0.96875, prec 0.0602461, recall 0.786889
2017-12-10T02:46:09.688344: step 2720, loss 0.181845, acc 0.96875, prec 0.0602438, recall 0.786889
2017-12-10T02:46:09.955065: step 2721, loss 0.332652, acc 0.96875, prec 0.0602596, recall 0.786943
2017-12-10T02:46:10.222757: step 2722, loss 0.2405, acc 0.9375, prec 0.0602732, recall 0.786997
2017-12-10T02:46:10.484651: step 2723, loss 0.140578, acc 0.953125, prec 0.0602879, recall 0.787051
2017-12-10T02:46:10.755585: step 2724, loss 0.191541, acc 0.96875, prec 0.0602855, recall 0.787051
2017-12-10T02:46:11.017855: step 2725, loss 0.0796108, acc 0.96875, prec 0.0602832, recall 0.787051
2017-12-10T02:46:11.285969: step 2726, loss 0.232495, acc 0.984375, prec 0.0603002, recall 0.787105
2017-12-10T02:46:11.558792: step 2727, loss 0.119965, acc 0.953125, prec 0.0603149, recall 0.787159
2017-12-10T02:46:11.824018: step 2728, loss 0.00731376, acc 1, prec 0.0603149, recall 0.787159
2017-12-10T02:46:12.088806: step 2729, loss 0.092619, acc 0.953125, prec 0.0603478, recall 0.787266
2017-12-10T02:46:12.349433: step 2730, loss 0.139461, acc 0.953125, prec 0.0603443, recall 0.787266
2017-12-10T02:46:12.608895: step 2731, loss 0.181929, acc 0.9375, prec 0.0603579, recall 0.78732
2017-12-10T02:46:12.878431: step 2732, loss 0.0382847, acc 0.984375, prec 0.0603931, recall 0.787427
2017-12-10T02:46:13.143847: step 2733, loss 0.0171633, acc 1, prec 0.0603931, recall 0.787427
2017-12-10T02:46:13.403427: step 2734, loss 0.713191, acc 0.953125, prec 0.0604441, recall 0.787588
2017-12-10T02:46:13.668242: step 2735, loss 3.10064, acc 0.953125, prec 0.0604418, recall 0.78739
2017-12-10T02:46:13.941331: step 2736, loss 0.0545219, acc 0.96875, prec 0.0604395, recall 0.78739
2017-12-10T02:46:14.206070: step 2737, loss 0.111505, acc 0.953125, prec 0.0604359, recall 0.78739
2017-12-10T02:46:14.470490: step 2738, loss 0.0812842, acc 0.96875, prec 0.0604336, recall 0.78739
2017-12-10T02:46:14.744389: step 2739, loss 0.106059, acc 0.96875, prec 0.0604495, recall 0.787443
2017-12-10T02:46:15.005678: step 2740, loss 0.139652, acc 0.953125, prec 0.0604641, recall 0.787497
2017-12-10T02:46:15.268274: step 2741, loss 0.0728602, acc 0.96875, prec 0.06048, recall 0.78755
2017-12-10T02:46:15.531303: step 2742, loss 0.172523, acc 0.9375, prec 0.0604753, recall 0.78755
2017-12-10T02:46:15.794557: step 2743, loss 0.239608, acc 0.9375, prec 0.0604706, recall 0.78755
2017-12-10T02:46:16.057014: step 2744, loss 0.157965, acc 0.921875, prec 0.0604648, recall 0.78755
2017-12-10T02:46:16.318120: step 2745, loss 0.310197, acc 0.921875, prec 0.0604953, recall 0.787657
2017-12-10T02:46:16.579887: step 2746, loss 0.189968, acc 0.921875, prec 0.0605076, recall 0.787711
2017-12-10T02:46:16.849504: step 2747, loss 0.288926, acc 0.96875, prec 0.0605598, recall 0.787871
2017-12-10T02:46:17.110755: step 2748, loss 0.0435994, acc 0.96875, prec 0.0605574, recall 0.787871
2017-12-10T02:46:17.371555: step 2749, loss 0.134738, acc 0.9375, prec 0.0605709, recall 0.787925
2017-12-10T02:46:17.640451: step 2750, loss 0.119904, acc 0.96875, prec 0.0605867, recall 0.787978
2017-12-10T02:46:17.902547: step 2751, loss 0.0449596, acc 0.96875, prec 0.0605844, recall 0.787978
2017-12-10T02:46:18.160898: step 2752, loss 0.0627266, acc 0.96875, prec 0.0606365, recall 0.788138
2017-12-10T02:46:18.423529: step 2753, loss 0.328479, acc 0.96875, prec 0.0606523, recall 0.788191
2017-12-10T02:46:18.683971: step 2754, loss 0.263456, acc 0.953125, prec 0.060667, recall 0.788244
2017-12-10T02:46:18.946227: step 2755, loss 0.0951679, acc 0.984375, prec 0.0607021, recall 0.78835
2017-12-10T02:46:19.209095: step 2756, loss 0.250769, acc 0.875, prec 0.0606927, recall 0.78835
2017-12-10T02:46:19.478006: step 2757, loss 0.163284, acc 0.96875, prec 0.0606904, recall 0.78835
2017-12-10T02:46:19.738689: step 2758, loss 0.0177865, acc 1, prec 0.0607086, recall 0.788404
2017-12-10T02:46:20.002666: step 2759, loss 0.147667, acc 0.953125, prec 0.060705, recall 0.788404
2017-12-10T02:46:20.271128: step 2760, loss 0.222705, acc 0.921875, prec 0.0606992, recall 0.788404
2017-12-10T02:46:20.530726: step 2761, loss 0.33395, acc 0.984375, prec 0.0607161, recall 0.788457
2017-12-10T02:46:20.792016: step 2762, loss 0.24602, acc 0.96875, prec 0.0607501, recall 0.788563
2017-12-10T02:46:21.054258: step 2763, loss 0.0851196, acc 0.984375, prec 0.0607671, recall 0.788616
2017-12-10T02:46:21.323960: step 2764, loss 0.763613, acc 0.953125, prec 0.0607998, recall 0.788722
2017-12-10T02:46:21.586432: step 2765, loss 0.272744, acc 0.953125, prec 0.0608326, recall 0.788828
2017-12-10T02:46:21.846077: step 2766, loss 0.101193, acc 0.953125, prec 0.0608472, recall 0.788881
2017-12-10T02:46:22.106239: step 2767, loss 0.164108, acc 0.9375, prec 0.0608425, recall 0.788881
2017-12-10T02:46:22.375259: step 2768, loss 0.173817, acc 0.953125, prec 0.060839, recall 0.788881
2017-12-10T02:46:22.638770: step 2769, loss 0.210563, acc 0.90625, prec 0.0608319, recall 0.788881
2017-12-10T02:46:22.903814: step 2770, loss 0.121428, acc 0.96875, prec 0.0608296, recall 0.788881
2017-12-10T02:46:23.168261: step 2771, loss 0.784263, acc 0.9375, prec 0.0608612, recall 0.788986
2017-12-10T02:46:23.429906: step 2772, loss 0.250353, acc 0.9375, prec 0.0608746, recall 0.789039
2017-12-10T02:46:23.692686: step 2773, loss 0.241639, acc 0.953125, prec 0.0608892, recall 0.789092
2017-12-10T02:46:23.956767: step 2774, loss 0.454562, acc 0.90625, prec 0.0608822, recall 0.789092
2017-12-10T02:46:24.231076: step 2775, loss 0.32409, acc 0.9375, prec 0.0608956, recall 0.789145
2017-12-10T02:46:24.496235: step 2776, loss 0.182921, acc 0.953125, prec 0.0608921, recall 0.789145
2017-12-10T02:46:24.762743: step 2777, loss 0.101179, acc 0.96875, prec 0.0608897, recall 0.789145
2017-12-10T02:46:25.021626: step 2778, loss 0.0897541, acc 0.953125, prec 0.0609043, recall 0.789197
2017-12-10T02:46:25.283420: step 2779, loss 0.312791, acc 0.9375, prec 0.0608996, recall 0.789197
2017-12-10T02:46:25.549782: step 2780, loss 0.152606, acc 0.96875, prec 0.0609154, recall 0.78925
2017-12-10T02:46:25.812339: step 2781, loss 0.100922, acc 1, prec 0.0609335, recall 0.789303
2017-12-10T02:46:26.076993: step 2782, loss 0.074208, acc 0.953125, prec 0.06093, recall 0.789303
2017-12-10T02:46:26.352236: step 2783, loss 0.367382, acc 1, prec 0.0609662, recall 0.789408
2017-12-10T02:46:26.626932: step 2784, loss 4.47106, acc 0.953125, prec 0.060982, recall 0.789263
2017-12-10T02:46:26.891520: step 2785, loss 0.21679, acc 0.921875, prec 0.0609761, recall 0.789263
2017-12-10T02:46:27.153807: step 2786, loss 0.933699, acc 0.859375, prec 0.0610017, recall 0.789369
2017-12-10T02:46:27.416260: step 2787, loss 0.116453, acc 0.9375, prec 0.0610151, recall 0.789421
2017-12-10T02:46:27.679468: step 2788, loss 0.520953, acc 0.90625, prec 0.0610262, recall 0.789474
2017-12-10T02:46:27.940374: step 2789, loss 0.308022, acc 0.90625, prec 0.0610372, recall 0.789526
2017-12-10T02:46:28.197030: step 2790, loss 0.549995, acc 0.890625, prec 0.0610471, recall 0.789579
2017-12-10T02:46:28.458466: step 2791, loss 0.767922, acc 0.84375, prec 0.0610534, recall 0.789631
2017-12-10T02:46:28.722079: step 2792, loss 0.333489, acc 0.875, prec 0.061044, recall 0.789631
2017-12-10T02:46:28.987651: step 2793, loss 0.899431, acc 0.6875, prec 0.0610566, recall 0.789736
2017-12-10T02:46:29.250281: step 2794, loss 0.435974, acc 0.890625, prec 0.0610665, recall 0.789788
2017-12-10T02:46:29.516796: step 2795, loss 0.453129, acc 0.828125, prec 0.0610716, recall 0.789841
2017-12-10T02:46:29.779274: step 2796, loss 0.503009, acc 0.859375, prec 0.0610791, recall 0.789893
2017-12-10T02:46:30.049120: step 2797, loss 0.399903, acc 0.875, prec 0.0610878, recall 0.789945
2017-12-10T02:46:30.316098: step 2798, loss 0.350743, acc 0.84375, prec 0.0610941, recall 0.789998
2017-12-10T02:46:30.574287: step 2799, loss 0.457457, acc 0.859375, prec 0.0610835, recall 0.789998
2017-12-10T02:46:30.838428: step 2800, loss 1.99897, acc 0.875, prec 0.0610934, recall 0.789853
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-2800

2017-12-10T02:46:32.107111: step 2801, loss 0.310653, acc 0.890625, prec 0.0611393, recall 0.79001
2017-12-10T02:46:32.371525: step 2802, loss 0.240455, acc 0.953125, prec 0.0611358, recall 0.79001
2017-12-10T02:46:32.633929: step 2803, loss 0.279448, acc 0.921875, prec 0.061148, recall 0.790062
2017-12-10T02:46:32.891922: step 2804, loss 0.320773, acc 0.859375, prec 0.0611554, recall 0.790114
2017-12-10T02:46:33.166441: step 2805, loss 0.28188, acc 0.921875, prec 0.0611496, recall 0.790114
2017-12-10T02:46:33.426348: step 2806, loss 0.570196, acc 0.875, prec 0.0611402, recall 0.790114
2017-12-10T02:46:33.690886: step 2807, loss 0.500003, acc 0.859375, prec 0.0611476, recall 0.790166
2017-12-10T02:46:33.952280: step 2808, loss 0.222591, acc 0.9375, prec 0.061161, recall 0.790218
2017-12-10T02:46:34.216816: step 2809, loss 0.194863, acc 0.96875, prec 0.0611586, recall 0.790218
2017-12-10T02:46:34.478309: step 2810, loss 0.122022, acc 0.96875, prec 0.0611743, recall 0.790271
2017-12-10T02:46:34.738708: step 2811, loss 0.354965, acc 0.90625, prec 0.0611672, recall 0.790271
2017-12-10T02:46:35.000686: step 2812, loss 0.0609733, acc 0.984375, prec 0.0612021, recall 0.790375
2017-12-10T02:46:35.261280: step 2813, loss 0.281986, acc 0.921875, prec 0.0612143, recall 0.790427
2017-12-10T02:46:35.531407: step 2814, loss 0.381791, acc 0.921875, prec 0.0612445, recall 0.790531
2017-12-10T02:46:35.805516: step 2815, loss 0.15071, acc 0.921875, prec 0.0612386, recall 0.790531
2017-12-10T02:46:36.074822: step 2816, loss 0.0299418, acc 1, prec 0.0612566, recall 0.790582
2017-12-10T02:46:36.335600: step 2817, loss 0.0384886, acc 0.984375, prec 0.0612915, recall 0.790686
2017-12-10T02:46:36.598922: step 2818, loss 0.0792902, acc 0.96875, prec 0.0612891, recall 0.790686
2017-12-10T02:46:36.869253: step 2819, loss 0.129163, acc 0.984375, prec 0.061306, recall 0.790738
2017-12-10T02:46:37.131931: step 2820, loss 0.0821578, acc 0.96875, prec 0.0613036, recall 0.790738
2017-12-10T02:46:37.405700: step 2821, loss 0.0829125, acc 0.984375, prec 0.0613205, recall 0.79079
2017-12-10T02:46:37.669202: step 2822, loss 0.118398, acc 0.953125, prec 0.061317, recall 0.79079
2017-12-10T02:46:37.935253: step 2823, loss 0.622441, acc 0.96875, prec 0.0613687, recall 0.790945
2017-12-10T02:46:38.202927: step 2824, loss 0.0603776, acc 0.984375, prec 0.0614215, recall 0.7911
2017-12-10T02:46:38.463804: step 2825, loss 0.282282, acc 0.953125, prec 0.061418, recall 0.7911
2017-12-10T02:46:38.733756: step 2826, loss 0.0529099, acc 0.96875, prec 0.0614156, recall 0.7911
2017-12-10T02:46:39.000465: step 2827, loss 0.0867555, acc 0.9375, prec 0.0614469, recall 0.791203
2017-12-10T02:46:39.271011: step 2828, loss 0.00733486, acc 1, prec 0.061465, recall 0.791255
2017-12-10T02:46:39.533342: step 2829, loss 0.19012, acc 0.984375, prec 0.0614998, recall 0.791358
2017-12-10T02:46:39.797083: step 2830, loss 2.15583, acc 0.96875, prec 0.0615166, recall 0.791214
2017-12-10T02:46:40.059742: step 2831, loss 2.08433, acc 0.96875, prec 0.0615334, recall 0.791071
2017-12-10T02:46:40.322733: step 2832, loss 0.276242, acc 0.953125, prec 0.0615659, recall 0.791174
2017-12-10T02:46:40.586973: step 2833, loss 0.342308, acc 0.890625, prec 0.0615756, recall 0.791225
2017-12-10T02:46:40.856545: step 2834, loss 0.400116, acc 0.90625, prec 0.0615686, recall 0.791225
2017-12-10T02:46:41.120528: step 2835, loss 0.41303, acc 0.875, prec 0.0615591, recall 0.791225
2017-12-10T02:46:41.395033: step 2836, loss 0.585971, acc 0.859375, prec 0.0615485, recall 0.791225
2017-12-10T02:46:41.660317: step 2837, loss 0.359185, acc 0.875, prec 0.061557, recall 0.791277
2017-12-10T02:46:41.917158: step 2838, loss 0.455144, acc 0.875, prec 0.0615656, recall 0.791328
2017-12-10T02:46:42.182728: step 2839, loss 0.690203, acc 0.765625, prec 0.0615839, recall 0.791431
2017-12-10T02:46:42.441515: step 2840, loss 0.456097, acc 0.859375, prec 0.0615912, recall 0.791482
2017-12-10T02:46:42.706871: step 2841, loss 0.904227, acc 0.765625, prec 0.0615735, recall 0.791482
2017-12-10T02:46:42.966214: step 2842, loss 0.59642, acc 0.828125, prec 0.0615965, recall 0.791585
2017-12-10T02:46:43.226450: step 2843, loss 0.62519, acc 0.828125, prec 0.0616015, recall 0.791636
2017-12-10T02:46:43.484694: step 2844, loss 0.581113, acc 0.875, prec 0.061628, recall 0.791738
2017-12-10T02:46:43.743724: step 2845, loss 0.525857, acc 0.84375, prec 0.0616521, recall 0.791841
2017-12-10T02:46:44.005983: step 2846, loss 0.568481, acc 0.8125, prec 0.0616379, recall 0.791841
2017-12-10T02:46:44.267584: step 2847, loss 0.452736, acc 0.9375, prec 0.0616332, recall 0.791841
2017-12-10T02:46:44.529467: step 2848, loss 0.47828, acc 0.828125, prec 0.0616382, recall 0.791892
2017-12-10T02:46:44.801298: step 2849, loss 0.40502, acc 0.921875, prec 0.0616861, recall 0.792045
2017-12-10T02:46:45.066785: step 2850, loss 0.597389, acc 0.859375, prec 0.0616755, recall 0.792045
2017-12-10T02:46:45.325401: step 2851, loss 0.335452, acc 0.890625, prec 0.0616673, recall 0.792045
2017-12-10T02:46:45.586674: step 2852, loss 0.268946, acc 0.90625, prec 0.0616602, recall 0.792045
2017-12-10T02:46:45.856844: step 2853, loss 0.142028, acc 0.96875, prec 0.0616937, recall 0.792147
2017-12-10T02:46:46.119496: step 2854, loss 0.339986, acc 1, prec 0.0617116, recall 0.792198
2017-12-10T02:46:46.384102: step 2855, loss 0.262523, acc 0.96875, prec 0.0617631, recall 0.792351
2017-12-10T02:46:46.644654: step 2856, loss 1.20232, acc 0.921875, prec 0.061811, recall 0.792504
2017-12-10T02:46:46.909540: step 2857, loss 0.565304, acc 0.9375, prec 0.0618242, recall 0.792554
2017-12-10T02:46:47.180355: step 2858, loss 0.10582, acc 0.953125, prec 0.0618206, recall 0.792554
2017-12-10T02:46:47.443686: step 2859, loss 1.92086, acc 0.921875, prec 0.0618338, recall 0.792411
2017-12-10T02:46:47.710197: step 2860, loss 0.29771, acc 0.875, prec 0.0618244, recall 0.792411
2017-12-10T02:46:47.973334: step 2861, loss 0.325772, acc 0.9375, prec 0.0618196, recall 0.792411
2017-12-10T02:46:48.235852: step 2862, loss 0.566024, acc 0.859375, prec 0.0618448, recall 0.792513
2017-12-10T02:46:48.500578: step 2863, loss 0.681841, acc 0.90625, prec 0.0618736, recall 0.792614
2017-12-10T02:46:48.760941: step 2864, loss 0.303504, acc 0.84375, prec 0.0618618, recall 0.792614
2017-12-10T02:46:49.026300: step 2865, loss 0.356349, acc 0.9375, prec 0.061875, recall 0.792665
2017-12-10T02:46:49.286834: step 2866, loss 0.230017, acc 0.921875, prec 0.061887, recall 0.792716
2017-12-10T02:46:49.548644: step 2867, loss 0.246656, acc 0.9375, prec 0.061918, recall 0.792817
2017-12-10T02:46:49.812654: step 2868, loss 0.446674, acc 0.828125, prec 0.061905, recall 0.792817
2017-12-10T02:46:50.075901: step 2869, loss 0.598088, acc 0.875, prec 0.0618956, recall 0.792817
2017-12-10T02:46:50.337721: step 2870, loss 0.450689, acc 0.84375, prec 0.0618838, recall 0.792817
2017-12-10T02:46:50.600574: step 2871, loss 0.301443, acc 0.96875, prec 0.0618993, recall 0.792868
2017-12-10T02:46:50.863665: step 2872, loss 0.67089, acc 0.84375, prec 0.0618875, recall 0.792868
2017-12-10T02:46:51.129811: step 2873, loss 0.860674, acc 0.84375, prec 0.0618757, recall 0.792868
2017-12-10T02:46:51.390106: step 2874, loss 0.204805, acc 0.9375, prec 0.061871, recall 0.792868
2017-12-10T02:46:51.659192: step 2875, loss 0.379041, acc 0.890625, prec 0.0618806, recall 0.792918
2017-12-10T02:46:51.925989: step 2876, loss 0.248109, acc 0.90625, prec 0.0618735, recall 0.792918
2017-12-10T02:46:52.185207: step 2877, loss 0.32908, acc 0.890625, prec 0.0618653, recall 0.792918
2017-12-10T02:46:52.456934: step 2878, loss 0.301233, acc 0.9375, prec 0.0618963, recall 0.793019
2017-12-10T02:46:52.719450: step 2879, loss 0.129642, acc 0.953125, prec 0.0618928, recall 0.793019
2017-12-10T02:46:52.986105: step 2880, loss 0.116245, acc 0.953125, prec 0.0619071, recall 0.79307
2017-12-10T02:46:53.254386: step 2881, loss 0.342149, acc 0.890625, prec 0.0618989, recall 0.79307
2017-12-10T02:46:53.520609: step 2882, loss 0.28988, acc 0.9375, prec 0.0618942, recall 0.79307
2017-12-10T02:46:53.787859: step 2883, loss 0.0936086, acc 0.96875, prec 0.0619097, recall 0.79312
2017-12-10T02:46:54.056171: step 2884, loss 0.0705932, acc 0.96875, prec 0.061943, recall 0.793221
2017-12-10T02:46:54.324329: step 2885, loss 0.131869, acc 0.953125, prec 0.0619573, recall 0.793272
2017-12-10T02:46:54.591891: step 2886, loss 1.17372, acc 0.9375, prec 0.0619884, recall 0.793372
2017-12-10T02:46:54.861768: step 2887, loss 0.453336, acc 0.96875, prec 0.0620217, recall 0.793473
2017-12-10T02:46:55.132833: step 2888, loss 0.189368, acc 0.953125, prec 0.062036, recall 0.793523
2017-12-10T02:46:55.396609: step 2889, loss 0.249093, acc 0.9375, prec 0.062067, recall 0.793624
2017-12-10T02:46:55.661026: step 2890, loss 0.050398, acc 0.96875, prec 0.0620646, recall 0.793624
2017-12-10T02:46:55.934130: step 2891, loss 0.0568729, acc 0.96875, prec 0.0620623, recall 0.793624
2017-12-10T02:46:56.197278: step 2892, loss 0.305164, acc 0.96875, prec 0.0620956, recall 0.793724
2017-12-10T02:46:56.475289: step 2893, loss 0.190116, acc 0.9375, prec 0.0621087, recall 0.793774
2017-12-10T02:46:56.733121: step 2894, loss 0.203902, acc 0.96875, prec 0.0621421, recall 0.793875
2017-12-10T02:46:56.999839: step 2895, loss 0.768176, acc 0.984375, prec 0.0621944, recall 0.794025
2017-12-10T02:46:57.272493: step 2896, loss 0.175553, acc 0.9375, prec 0.0622075, recall 0.794075
2017-12-10T02:46:57.540996: step 2897, loss 0.0768546, acc 0.96875, prec 0.0622051, recall 0.794075
2017-12-10T02:46:57.810526: step 2898, loss 0.207466, acc 0.9375, prec 0.0622182, recall 0.794125
2017-12-10T02:46:58.077155: step 2899, loss 0.0657231, acc 0.984375, prec 0.0622171, recall 0.794125
2017-12-10T02:46:58.349029: step 2900, loss 0.226475, acc 0.9375, prec 0.062248, recall 0.794225
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-2900

2017-12-10T02:46:59.618931: step 2901, loss 0.0517207, acc 0.984375, prec 0.0622468, recall 0.794225
2017-12-10T02:46:59.886322: step 2902, loss 1.53794, acc 0.90625, prec 0.0622587, recall 0.794082
2017-12-10T02:47:00.166660: step 2903, loss 0.205658, acc 0.921875, prec 0.0622706, recall 0.794132
2017-12-10T02:47:00.439124: step 2904, loss 0.640093, acc 0.859375, prec 0.06226, recall 0.794132
2017-12-10T02:47:00.695444: step 2905, loss 0.183555, acc 0.953125, prec 0.0622564, recall 0.794132
2017-12-10T02:47:00.960878: step 2906, loss 0.162716, acc 0.9375, prec 0.0622517, recall 0.794132
2017-12-10T02:47:01.220776: step 2907, loss 0.172654, acc 0.9375, prec 0.0622648, recall 0.794182
2017-12-10T02:47:01.487654: step 2908, loss 0.297633, acc 0.875, prec 0.0622553, recall 0.794182
2017-12-10T02:47:01.747783: step 2909, loss 0.880494, acc 0.875, prec 0.0622637, recall 0.794232
2017-12-10T02:47:02.013794: step 2910, loss 0.352179, acc 0.890625, prec 0.0622732, recall 0.794282
2017-12-10T02:47:02.278855: step 2911, loss 0.3044, acc 0.859375, prec 0.0622626, recall 0.794282
2017-12-10T02:47:02.545269: step 2912, loss 0.685353, acc 0.859375, prec 0.0623054, recall 0.794431
2017-12-10T02:47:02.804033: step 2913, loss 0.25922, acc 0.921875, prec 0.0623172, recall 0.794481
2017-12-10T02:47:03.066520: step 2914, loss 0.118743, acc 0.921875, prec 0.0623113, recall 0.794481
2017-12-10T02:47:03.332323: step 2915, loss 0.343532, acc 0.9375, prec 0.0623422, recall 0.79458
2017-12-10T02:47:03.600024: step 2916, loss 0.318128, acc 0.859375, prec 0.0623315, recall 0.79458
2017-12-10T02:47:03.868619: step 2917, loss 0.314331, acc 0.90625, prec 0.0623422, recall 0.79463
2017-12-10T02:47:04.130478: step 2918, loss 0.466512, acc 0.90625, prec 0.0623707, recall 0.794729
2017-12-10T02:47:04.396418: step 2919, loss 0.20378, acc 0.953125, prec 0.062385, recall 0.794779
2017-12-10T02:47:04.667147: step 2920, loss 0.150866, acc 0.921875, prec 0.0623791, recall 0.794779
2017-12-10T02:47:04.931542: step 2921, loss 0.241043, acc 0.96875, prec 0.0623945, recall 0.794828
2017-12-10T02:47:05.196795: step 2922, loss 0.470825, acc 0.890625, prec 0.0624218, recall 0.794928
2017-12-10T02:47:05.456107: step 2923, loss 0.0848351, acc 0.953125, prec 0.062436, recall 0.794977
2017-12-10T02:47:05.718437: step 2924, loss 0.192913, acc 0.953125, prec 0.0625036, recall 0.795175
2017-12-10T02:47:05.977573: step 2925, loss 0.106975, acc 0.9375, prec 0.0624988, recall 0.795175
2017-12-10T02:47:06.239325: step 2926, loss 0.234563, acc 0.9375, prec 0.0624941, recall 0.795175
2017-12-10T02:47:06.500121: step 2927, loss 0.0341687, acc 1, prec 0.0625118, recall 0.795224
2017-12-10T02:47:06.770072: step 2928, loss 0.0364647, acc 0.96875, prec 0.0625095, recall 0.795224
2017-12-10T02:47:07.030721: step 2929, loss 0.109939, acc 0.9375, prec 0.0625047, recall 0.795224
2017-12-10T02:47:07.293272: step 2930, loss 2.02581, acc 0.9375, prec 0.0625545, recall 0.795181
2017-12-10T02:47:07.566600: step 2931, loss 0.0781857, acc 0.953125, prec 0.0625509, recall 0.795181
2017-12-10T02:47:07.832179: step 2932, loss 2.07431, acc 0.890625, prec 0.0625794, recall 0.795088
2017-12-10T02:47:08.097484: step 2933, loss 0.139245, acc 0.96875, prec 0.0625948, recall 0.795137
2017-12-10T02:47:08.363785: step 2934, loss 0.166786, acc 0.921875, prec 0.0625888, recall 0.795137
2017-12-10T02:47:08.632336: step 2935, loss 0.72841, acc 0.890625, prec 0.062616, recall 0.795236
2017-12-10T02:47:08.894345: step 2936, loss 0.281338, acc 0.90625, prec 0.0626089, recall 0.795236
2017-12-10T02:47:09.160604: step 2937, loss 0.580587, acc 0.859375, prec 0.0625983, recall 0.795236
2017-12-10T02:47:09.424852: step 2938, loss 0.372371, acc 0.90625, prec 0.0625911, recall 0.795236
2017-12-10T02:47:09.684320: step 2939, loss 0.568971, acc 0.84375, prec 0.062597, recall 0.795285
2017-12-10T02:47:09.950892: step 2940, loss 0.459979, acc 0.890625, prec 0.0626065, recall 0.795334
2017-12-10T02:47:10.215671: step 2941, loss 0.496004, acc 0.890625, prec 0.0626337, recall 0.795433
2017-12-10T02:47:10.478309: step 2942, loss 0.544567, acc 0.84375, prec 0.0626396, recall 0.795482
2017-12-10T02:47:10.752974: step 2943, loss 0.415742, acc 0.859375, prec 0.0626289, recall 0.795482
2017-12-10T02:47:11.013302: step 2944, loss 0.390824, acc 0.875, prec 0.0626549, recall 0.79558
2017-12-10T02:47:11.288801: step 2945, loss 0.643282, acc 0.8125, prec 0.0626407, recall 0.79558
2017-12-10T02:47:11.566527: step 2946, loss 0.550441, acc 0.90625, prec 0.0626513, recall 0.795629
2017-12-10T02:47:11.835359: step 2947, loss 0.358859, acc 0.890625, prec 0.0626607, recall 0.795678
2017-12-10T02:47:12.101043: step 2948, loss 0.345461, acc 0.921875, prec 0.0626902, recall 0.795776
2017-12-10T02:47:12.363346: step 2949, loss 0.371271, acc 0.890625, prec 0.0626819, recall 0.795776
2017-12-10T02:47:12.623636: step 2950, loss 0.240485, acc 0.953125, prec 0.0626784, recall 0.795776
2017-12-10T02:47:12.883404: step 2951, loss 0.131161, acc 0.96875, prec 0.0627114, recall 0.795874
2017-12-10T02:47:13.150707: step 2952, loss 0.0830048, acc 0.953125, prec 0.0627079, recall 0.795874
2017-12-10T02:47:13.419211: step 2953, loss 4.79265, acc 0.953125, prec 0.0627409, recall 0.795781
2017-12-10T02:47:13.681619: step 2954, loss 0.494076, acc 0.890625, prec 0.0627327, recall 0.795781
2017-12-10T02:47:13.944351: step 2955, loss 0.264967, acc 0.921875, prec 0.0627444, recall 0.79583
2017-12-10T02:47:14.207514: step 2956, loss 0.367532, acc 0.90625, prec 0.062755, recall 0.795879
2017-12-10T02:47:14.472833: step 2957, loss 0.125738, acc 0.9375, prec 0.0627503, recall 0.795879
2017-12-10T02:47:14.732025: step 2958, loss 0.253945, acc 0.9375, prec 0.0627632, recall 0.795928
2017-12-10T02:47:14.989776: step 2959, loss 0.330699, acc 0.921875, prec 0.0627927, recall 0.796026
2017-12-10T02:47:15.256075: step 2960, loss 0.218133, acc 0.890625, prec 0.0627844, recall 0.796026
2017-12-10T02:47:15.524510: step 2961, loss 0.365447, acc 0.84375, prec 0.062808, recall 0.796123
2017-12-10T02:47:15.786913: step 2962, loss 0.850359, acc 0.875, prec 0.0628162, recall 0.796172
2017-12-10T02:47:16.049964: step 2963, loss 0.279549, acc 0.921875, prec 0.0628102, recall 0.796172
2017-12-10T02:47:16.315218: step 2964, loss 2.09934, acc 0.921875, prec 0.0628055, recall 0.795982
2017-12-10T02:47:16.582469: step 2965, loss 0.219033, acc 0.90625, prec 0.0628161, recall 0.796031
2017-12-10T02:47:16.842026: step 2966, loss 0.159172, acc 0.953125, prec 0.0628125, recall 0.796031
2017-12-10T02:47:17.105074: step 2967, loss 0.181558, acc 0.9375, prec 0.0628078, recall 0.796031
2017-12-10T02:47:18.079157: step 2968, loss 0.480211, acc 0.921875, prec 0.0628195, recall 0.796079
2017-12-10T02:47:18.455170: step 2969, loss 0.419509, acc 0.875, prec 0.0628277, recall 0.796128
2017-12-10T02:47:18.719590: step 2970, loss 0.624006, acc 0.875, prec 0.0628182, recall 0.796128
2017-12-10T02:47:19.477766: step 2971, loss 2.17674, acc 0.875, prec 0.0628276, recall 0.795987
2017-12-10T02:47:20.270117: step 2972, loss 0.600552, acc 0.8125, prec 0.0628134, recall 0.795987
2017-12-10T02:47:20.993169: step 2973, loss 0.548836, acc 0.890625, prec 0.0628581, recall 0.796133
2017-12-10T02:47:21.718390: step 2974, loss 0.392756, acc 0.890625, prec 0.0628675, recall 0.796181
2017-12-10T02:47:22.487646: step 2975, loss 0.441505, acc 0.859375, prec 0.0628745, recall 0.79623
2017-12-10T02:47:22.844297: step 2976, loss 0.457721, acc 0.859375, prec 0.0628638, recall 0.79623
2017-12-10T02:47:23.112411: step 2977, loss 0.334814, acc 0.890625, prec 0.0628908, recall 0.796327
2017-12-10T02:47:23.381204: step 2978, loss 0.707176, acc 0.796875, prec 0.0629107, recall 0.796424
2017-12-10T02:47:23.644362: step 2979, loss 0.266765, acc 0.90625, prec 0.0629213, recall 0.796473
2017-12-10T02:47:23.900755: step 2980, loss 0.351602, acc 0.921875, prec 0.062933, recall 0.796521
2017-12-10T02:47:24.167933: step 2981, loss 0.178081, acc 0.90625, prec 0.0629788, recall 0.796667
2017-12-10T02:47:24.395192: step 2982, loss 0.293141, acc 0.921569, prec 0.062974, recall 0.796667
2017-12-10T02:47:24.676417: step 2983, loss 0.147494, acc 0.953125, prec 0.0630058, recall 0.796763
2017-12-10T02:47:24.944016: step 2984, loss 0.271877, acc 0.921875, prec 0.0629998, recall 0.796763
2017-12-10T02:47:25.214970: step 2985, loss 0.0888286, acc 0.984375, prec 0.0629986, recall 0.796763
2017-12-10T02:47:25.479985: step 2986, loss 0.162376, acc 0.9375, prec 0.0629939, recall 0.796763
2017-12-10T02:47:25.739822: step 2987, loss 0.315714, acc 0.921875, prec 0.0630056, recall 0.796812
2017-12-10T02:47:26.004841: step 2988, loss 0.124387, acc 0.953125, prec 0.063002, recall 0.796812
2017-12-10T02:47:26.262913: step 2989, loss 0.172655, acc 0.9375, prec 0.0630149, recall 0.79686
2017-12-10T02:47:26.539615: step 2990, loss 0.133584, acc 0.953125, prec 0.0630466, recall 0.796957
2017-12-10T02:47:26.805544: step 2991, loss 0.0816662, acc 0.953125, prec 0.0630607, recall 0.797005
2017-12-10T02:47:27.070373: step 2992, loss 0.0495615, acc 0.984375, prec 0.0630771, recall 0.797053
2017-12-10T02:47:27.336049: step 2993, loss 0.0331959, acc 0.984375, prec 0.0631112, recall 0.79715
2017-12-10T02:47:27.598839: step 2994, loss 0.345928, acc 0.9375, prec 0.0631417, recall 0.797246
2017-12-10T02:47:27.868239: step 2995, loss 0.0758241, acc 0.953125, prec 0.0631381, recall 0.797246
2017-12-10T02:47:28.132987: step 2996, loss 5.17395, acc 0.9375, prec 0.0631345, recall 0.797057
2017-12-10T02:47:28.407668: step 2997, loss 0.104776, acc 0.953125, prec 0.0631838, recall 0.797201
2017-12-10T02:47:28.680089: step 2998, loss 0.0775673, acc 0.984375, prec 0.0631826, recall 0.797201
2017-12-10T02:47:28.948392: step 2999, loss 0.0965655, acc 0.984375, prec 0.0631991, recall 0.797249
2017-12-10T02:47:29.209677: step 3000, loss 0.174107, acc 0.9375, prec 0.0632295, recall 0.797345

Evaluation:
2017-12-10T02:47:36.838387: step 3000, loss 3.03765, acc 0.941032, prec 0.0640349, recall 0.786448

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-3000

2017-12-10T02:47:38.389730: step 3001, loss 0.321801, acc 0.90625, prec 0.0640452, recall 0.786496
2017-12-10T02:47:38.651189: step 3002, loss 0.107979, acc 0.96875, prec 0.0640428, recall 0.786496
2017-12-10T02:47:38.917531: step 3003, loss 0.260175, acc 0.9375, prec 0.0640902, recall 0.786642
2017-12-10T02:47:39.184842: step 3004, loss 0.214025, acc 0.9375, prec 0.0640854, recall 0.786642
2017-12-10T02:47:39.449914: step 3005, loss 0.329084, acc 0.921875, prec 0.0640969, recall 0.786691
2017-12-10T02:47:39.708732: step 3006, loss 0.177937, acc 0.921875, prec 0.0640909, recall 0.786691
2017-12-10T02:47:39.970923: step 3007, loss 0.160837, acc 0.96875, prec 0.0641059, recall 0.78674
2017-12-10T02:47:40.230555: step 3008, loss 0.167908, acc 0.9375, prec 0.0641185, recall 0.786788
2017-12-10T02:47:40.496114: step 3009, loss 0.201567, acc 0.90625, prec 0.0641114, recall 0.786788
2017-12-10T02:47:40.758885: step 3010, loss 0.839049, acc 0.921875, prec 0.0641228, recall 0.786837
2017-12-10T02:47:41.026452: step 3011, loss 0.197368, acc 0.90625, prec 0.0641157, recall 0.786837
2017-12-10T02:47:41.290615: step 3012, loss 0.334175, acc 0.953125, prec 0.0641121, recall 0.786837
2017-12-10T02:47:41.564193: step 3013, loss 0.230043, acc 0.921875, prec 0.0641409, recall 0.786934
2017-12-10T02:47:41.824121: step 3014, loss 0.442309, acc 0.90625, prec 0.0641337, recall 0.786934
2017-12-10T02:47:42.082971: step 3015, loss 0.153941, acc 0.96875, prec 0.0641313, recall 0.786934
2017-12-10T02:47:42.347837: step 3016, loss 0.158617, acc 0.953125, prec 0.0641278, recall 0.786934
2017-12-10T02:47:42.611913: step 3017, loss 0.290083, acc 0.90625, prec 0.064138, recall 0.786982
2017-12-10T02:47:42.879940: step 3018, loss 0.0527958, acc 0.984375, prec 0.0641368, recall 0.786982
2017-12-10T02:47:43.140395: step 3019, loss 0.0428646, acc 1, prec 0.0641368, recall 0.786982
2017-12-10T02:47:43.408765: step 3020, loss 0.429035, acc 0.859375, prec 0.0641435, recall 0.787031
2017-12-10T02:47:43.675742: step 3021, loss 0.12547, acc 0.96875, prec 0.0641411, recall 0.787031
2017-12-10T02:47:43.935146: step 3022, loss 0.130128, acc 0.953125, prec 0.0641375, recall 0.787031
2017-12-10T02:47:44.202431: step 3023, loss 0.0994054, acc 0.96875, prec 0.0641698, recall 0.787128
2017-12-10T02:47:44.466059: step 3024, loss 0.345612, acc 0.9375, prec 0.0641998, recall 0.787224
2017-12-10T02:47:44.737422: step 3025, loss 0.078145, acc 0.96875, prec 0.0641974, recall 0.787224
2017-12-10T02:47:45.003688: step 3026, loss 0.0984102, acc 0.953125, prec 0.0641938, recall 0.787224
2017-12-10T02:47:45.266419: step 3027, loss 0.419931, acc 0.9375, prec 0.0641891, recall 0.787224
2017-12-10T02:47:45.528724: step 3028, loss 0.20374, acc 0.953125, prec 0.0642028, recall 0.787273
2017-12-10T02:47:45.795256: step 3029, loss 0.238545, acc 0.96875, prec 0.0642005, recall 0.787273
2017-12-10T02:47:46.057660: step 3030, loss 0.553384, acc 0.984375, prec 0.0642513, recall 0.787418
2017-12-10T02:47:46.329984: step 3031, loss 0.0581497, acc 0.96875, prec 0.0642489, recall 0.787418
2017-12-10T02:47:46.596742: step 3032, loss 0.311239, acc 0.90625, prec 0.0642591, recall 0.787466
2017-12-10T02:47:46.864528: step 3033, loss 0.0087581, acc 1, prec 0.0642764, recall 0.787514
2017-12-10T02:47:47.133998: step 3034, loss 0.135783, acc 0.953125, prec 0.0642729, recall 0.787514
2017-12-10T02:47:47.398322: step 3035, loss 3.80082, acc 0.9375, prec 0.0642866, recall 0.787384
2017-12-10T02:47:47.663386: step 3036, loss 0.175362, acc 0.953125, prec 0.0643004, recall 0.787432
2017-12-10T02:47:47.933820: step 3037, loss 0.286227, acc 0.921875, prec 0.0642944, recall 0.787432
2017-12-10T02:47:48.202436: step 3038, loss 0.0745869, acc 0.96875, prec 0.0642921, recall 0.787432
2017-12-10T02:47:48.470869: step 3039, loss 0.463472, acc 0.890625, prec 0.0642837, recall 0.787432
2017-12-10T02:47:48.740632: step 3040, loss 0.279899, acc 0.90625, prec 0.0642939, recall 0.78748
2017-12-10T02:47:49.003771: step 3041, loss 0.45134, acc 0.84375, prec 0.064282, recall 0.78748
2017-12-10T02:47:49.263639: step 3042, loss 0.337925, acc 0.890625, prec 0.064291, recall 0.787528
2017-12-10T02:47:49.525206: step 3043, loss 0.615258, acc 0.828125, prec 0.0642952, recall 0.787576
2017-12-10T02:47:49.785209: step 3044, loss 0.38101, acc 0.90625, prec 0.0643227, recall 0.787673
2017-12-10T02:47:50.062752: step 3045, loss 0.189273, acc 0.921875, prec 0.0643341, recall 0.787721
2017-12-10T02:47:50.322668: step 3046, loss 0.254968, acc 0.921875, prec 0.0643281, recall 0.787721
2017-12-10T02:47:50.595559: step 3047, loss 0.37767, acc 0.859375, prec 0.0643174, recall 0.787721
2017-12-10T02:47:50.857035: step 3048, loss 0.64836, acc 0.8125, prec 0.0643378, recall 0.787817
2017-12-10T02:47:51.126255: step 3049, loss 0.477497, acc 0.84375, prec 0.0643259, recall 0.787817
2017-12-10T02:47:51.388285: step 3050, loss 0.214574, acc 0.953125, prec 0.0643223, recall 0.787817
2017-12-10T02:47:51.652248: step 3051, loss 0.599939, acc 0.921875, prec 0.0643336, recall 0.787865
2017-12-10T02:47:51.916125: step 3052, loss 0.0645191, acc 0.984375, prec 0.0643498, recall 0.787913
2017-12-10T02:47:52.177057: step 3053, loss 0.307507, acc 0.90625, prec 0.0643772, recall 0.788009
2017-12-10T02:47:52.444060: step 3054, loss 0.467285, acc 0.859375, prec 0.0643838, recall 0.788057
2017-12-10T02:47:52.712021: step 3055, loss 0.623368, acc 0.859375, prec 0.0643731, recall 0.788057
2017-12-10T02:47:52.981246: step 3056, loss 0.159482, acc 0.953125, prec 0.0643695, recall 0.788057
2017-12-10T02:47:53.240965: step 3057, loss 0.080205, acc 0.96875, prec 0.0644017, recall 0.788153
2017-12-10T02:47:53.501093: step 3058, loss 0.183938, acc 0.96875, prec 0.0644339, recall 0.788249
2017-12-10T02:47:53.776692: step 3059, loss 0.0288902, acc 0.984375, prec 0.06445, recall 0.788296
2017-12-10T02:47:54.042599: step 3060, loss 0.322557, acc 0.921875, prec 0.0644613, recall 0.788344
2017-12-10T02:47:54.307659: step 3061, loss 0.181001, acc 0.953125, prec 0.0644577, recall 0.788344
2017-12-10T02:47:54.576674: step 3062, loss 2.26787, acc 0.921875, prec 0.0644703, recall 0.788214
2017-12-10T02:47:54.840949: step 3063, loss 2.12731, acc 0.96875, prec 0.0644863, recall 0.788084
2017-12-10T02:47:55.111224: step 3064, loss 0.167234, acc 0.953125, prec 0.0644828, recall 0.788084
2017-12-10T02:47:55.377372: step 3065, loss 0.211429, acc 0.9375, prec 0.064478, recall 0.788084
2017-12-10T02:47:55.639077: step 3066, loss 2.23421, acc 0.875, prec 0.064487, recall 0.787954
2017-12-10T02:47:55.903574: step 3067, loss 0.591821, acc 0.828125, prec 0.0644739, recall 0.787954
2017-12-10T02:47:56.168563: step 3068, loss 0.311058, acc 0.921875, prec 0.0644679, recall 0.787954
2017-12-10T02:47:56.437383: step 3069, loss 0.795515, acc 0.828125, prec 0.0644893, recall 0.78805
2017-12-10T02:47:56.707332: step 3070, loss 0.536969, acc 0.734375, prec 0.0644691, recall 0.78805
2017-12-10T02:47:56.972780: step 3071, loss 0.710454, acc 0.828125, prec 0.0644733, recall 0.788097
2017-12-10T02:47:57.232079: step 3072, loss 1.08821, acc 0.6875, prec 0.0644668, recall 0.788145
2017-12-10T02:47:57.491436: step 3073, loss 0.901735, acc 0.734375, prec 0.0644811, recall 0.788241
2017-12-10T02:47:57.752364: step 3074, loss 0.337794, acc 0.90625, prec 0.0644912, recall 0.788288
2017-12-10T02:47:58.015670: step 3075, loss 0.895513, acc 0.8125, prec 0.0644941, recall 0.788336
2017-12-10T02:47:58.284656: step 3076, loss 0.530149, acc 0.890625, prec 0.0645031, recall 0.788384
2017-12-10T02:47:58.546254: step 3077, loss 0.620925, acc 0.78125, prec 0.0644864, recall 0.788384
2017-12-10T02:47:58.805604: step 3078, loss 0.939142, acc 0.828125, prec 0.0644906, recall 0.788431
2017-12-10T02:47:59.075717: step 3079, loss 0.989663, acc 0.796875, prec 0.0645096, recall 0.788526
2017-12-10T02:47:59.330837: step 3080, loss 0.306934, acc 0.875, prec 0.0645173, recall 0.788574
2017-12-10T02:47:59.598014: step 3081, loss 0.345003, acc 0.90625, prec 0.0645446, recall 0.788669
2017-12-10T02:47:59.860127: step 3082, loss 0.599306, acc 0.84375, prec 0.06455, recall 0.788717
2017-12-10T02:48:00.125664: step 3083, loss 0.206721, acc 0.90625, prec 0.0645772, recall 0.788812
2017-12-10T02:48:00.391157: step 3084, loss 0.374715, acc 0.875, prec 0.0645677, recall 0.788812
2017-12-10T02:48:00.651708: step 3085, loss 0.346309, acc 0.90625, prec 0.0645606, recall 0.788812
2017-12-10T02:48:00.914087: step 3086, loss 0.20036, acc 0.9375, prec 0.0645559, recall 0.788812
2017-12-10T02:48:01.175770: step 3087, loss 0.295787, acc 0.921875, prec 0.0645843, recall 0.788906
2017-12-10T02:48:01.440186: step 3088, loss 0.0518334, acc 0.953125, prec 0.0645808, recall 0.788906
2017-12-10T02:48:01.701915: step 3089, loss 0.0748546, acc 0.984375, prec 0.0645968, recall 0.788954
2017-12-10T02:48:01.965395: step 3090, loss 0.285456, acc 0.953125, prec 0.0646104, recall 0.789001
2017-12-10T02:48:02.230029: step 3091, loss 0.160533, acc 0.96875, prec 0.064608, recall 0.789001
2017-12-10T02:48:02.491326: step 3092, loss 0.0318029, acc 0.984375, prec 0.064624, recall 0.789048
2017-12-10T02:48:02.751536: step 3093, loss 0.0236386, acc 0.984375, prec 0.0646228, recall 0.789048
2017-12-10T02:48:03.020665: step 3094, loss 0.0414232, acc 0.984375, prec 0.0646217, recall 0.789048
2017-12-10T02:48:03.282438: step 3095, loss 0.0223779, acc 0.984375, prec 0.0646205, recall 0.789048
2017-12-10T02:48:03.542737: step 3096, loss 0.117204, acc 0.96875, prec 0.0646181, recall 0.789048
2017-12-10T02:48:03.806718: step 3097, loss 0.0849441, acc 1, prec 0.0646697, recall 0.78919
2017-12-10T02:48:04.073492: step 3098, loss 0.709051, acc 1, prec 0.0646869, recall 0.789238
2017-12-10T02:48:04.341033: step 3099, loss 0.10205, acc 0.984375, prec 0.0647029, recall 0.789285
2017-12-10T02:48:04.603537: step 3100, loss 0.00154638, acc 1, prec 0.06472, recall 0.789332
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-3100

2017-12-10T02:48:06.536328: step 3101, loss 0.101535, acc 0.984375, prec 0.0647532, recall 0.789427
2017-12-10T02:48:06.802392: step 3102, loss 0.00616596, acc 1, prec 0.0647876, recall 0.789521
2017-12-10T02:48:07.067618: step 3103, loss 0.0683975, acc 0.96875, prec 0.0647852, recall 0.789521
2017-12-10T02:48:07.338670: step 3104, loss 0.0015045, acc 1, prec 0.0647852, recall 0.789521
2017-12-10T02:48:07.599696: step 3105, loss 0.142589, acc 0.96875, prec 0.0648, recall 0.789568
2017-12-10T02:48:07.871669: step 3106, loss 0.0587107, acc 0.984375, prec 0.0647988, recall 0.789568
2017-12-10T02:48:08.133770: step 3107, loss 1.0804, acc 1, prec 0.064816, recall 0.789615
2017-12-10T02:48:08.408515: step 3108, loss 0.0679049, acc 0.96875, prec 0.0648136, recall 0.789615
2017-12-10T02:48:08.676050: step 3109, loss 0.847784, acc 0.953125, prec 0.0648616, recall 0.789756
2017-12-10T02:48:08.947026: step 3110, loss 0.436822, acc 1, prec 0.0649131, recall 0.789897
2017-12-10T02:48:09.218096: step 3111, loss 0.103992, acc 0.953125, prec 0.0649267, recall 0.789944
2017-12-10T02:48:09.488548: step 3112, loss 0.0978872, acc 1, prec 0.0649782, recall 0.790085
2017-12-10T02:48:09.748819: step 3113, loss 0.142096, acc 0.953125, prec 0.0649747, recall 0.790085
2017-12-10T02:48:10.009252: step 3114, loss 0.0894084, acc 0.953125, prec 0.0649711, recall 0.790085
2017-12-10T02:48:10.271360: step 3115, loss 0.151078, acc 0.953125, prec 0.0649675, recall 0.790085
2017-12-10T02:48:10.537039: step 3116, loss 0.483582, acc 0.875, prec 0.064958, recall 0.790085
2017-12-10T02:48:10.801405: step 3117, loss 0.204282, acc 0.9375, prec 0.0649532, recall 0.790085
2017-12-10T02:48:11.066239: step 3118, loss 0.241313, acc 0.875, prec 0.0649608, recall 0.790132
2017-12-10T02:48:11.327482: step 3119, loss 0.543907, acc 0.9375, prec 0.0649732, recall 0.790179
2017-12-10T02:48:11.603240: step 3120, loss 0.796669, acc 0.796875, prec 0.0649749, recall 0.790225
2017-12-10T02:48:11.863109: step 3121, loss 0.412555, acc 0.890625, prec 0.0650008, recall 0.790319
2017-12-10T02:48:12.126575: step 3122, loss 0.233748, acc 0.9375, prec 0.0649961, recall 0.790319
2017-12-10T02:48:12.390253: step 3123, loss 0.277242, acc 0.9375, prec 0.0650084, recall 0.790366
2017-12-10T02:48:12.649265: step 3124, loss 0.191119, acc 0.9375, prec 0.0650037, recall 0.790366
2017-12-10T02:48:12.912401: step 3125, loss 0.479977, acc 0.859375, prec 0.0649929, recall 0.790366
2017-12-10T02:48:13.170867: step 3126, loss 0.117886, acc 0.9375, prec 0.0650053, recall 0.790412
2017-12-10T02:48:13.438183: step 3127, loss 0.533511, acc 0.859375, prec 0.0649946, recall 0.790412
2017-12-10T02:48:13.701667: step 3128, loss 0.189377, acc 0.953125, prec 0.0650253, recall 0.790506
2017-12-10T02:48:13.972214: step 3129, loss 0.448955, acc 0.84375, prec 0.0650305, recall 0.790553
2017-12-10T02:48:14.234636: step 3130, loss 0.0658546, acc 0.96875, prec 0.0650624, recall 0.790646
2017-12-10T02:48:14.506122: step 3131, loss 0.229731, acc 0.9375, prec 0.0651262, recall 0.790832
2017-12-10T02:48:14.776191: step 3132, loss 0.0977876, acc 0.9375, prec 0.0651214, recall 0.790832
2017-12-10T02:48:15.046390: step 3133, loss 1.33968, acc 0.953125, prec 0.0651533, recall 0.790749
2017-12-10T02:48:15.309964: step 3134, loss 0.0807026, acc 0.96875, prec 0.0651509, recall 0.790749
2017-12-10T02:48:15.576420: step 3135, loss 0.145628, acc 0.9375, prec 0.0651632, recall 0.790796
2017-12-10T02:48:15.845522: step 3136, loss 0.296231, acc 0.890625, prec 0.065172, recall 0.790842
2017-12-10T02:48:16.108377: step 3137, loss 0.187022, acc 0.9375, prec 0.0651843, recall 0.790889
2017-12-10T02:48:16.379598: step 3138, loss 0.201152, acc 0.96875, prec 0.0652162, recall 0.790982
2017-12-10T02:48:16.651856: step 3139, loss 0.285653, acc 0.921875, prec 0.0652102, recall 0.790982
2017-12-10T02:48:16.914378: step 3140, loss 0.202456, acc 0.921875, prec 0.0652043, recall 0.790982
2017-12-10T02:48:17.175433: step 3141, loss 0.210359, acc 0.953125, prec 0.0652007, recall 0.790982
2017-12-10T02:48:17.438230: step 3142, loss 0.192128, acc 0.9375, prec 0.065213, recall 0.791028
2017-12-10T02:48:17.703324: step 3143, loss 0.0309457, acc 0.984375, prec 0.0652289, recall 0.791075
2017-12-10T02:48:17.964619: step 3144, loss 0.309584, acc 0.90625, prec 0.0652218, recall 0.791075
2017-12-10T02:48:18.226436: step 3145, loss 1.75528, acc 0.953125, prec 0.0652194, recall 0.790899
2017-12-10T02:48:18.497698: step 3146, loss 0.145994, acc 0.953125, prec 0.0652158, recall 0.790899
2017-12-10T02:48:18.762563: step 3147, loss 0.374884, acc 0.921875, prec 0.0652269, recall 0.790945
2017-12-10T02:48:19.024985: step 3148, loss 0.173361, acc 0.921875, prec 0.0652381, recall 0.790992
2017-12-10T02:48:19.291196: step 3149, loss 0.492835, acc 0.859375, prec 0.0652615, recall 0.791084
2017-12-10T02:48:19.548660: step 3150, loss 0.31198, acc 0.921875, prec 0.0653069, recall 0.791223
2017-12-10T02:48:19.811638: step 3151, loss 0.394062, acc 0.90625, prec 0.0653339, recall 0.791316
2017-12-10T02:48:20.082065: step 3152, loss 0.262383, acc 0.890625, prec 0.0653255, recall 0.791316
2017-12-10T02:48:20.342701: step 3153, loss 0.282118, acc 0.875, prec 0.065316, recall 0.791316
2017-12-10T02:48:20.604580: step 3154, loss 0.388824, acc 0.875, prec 0.0653406, recall 0.791408
2017-12-10T02:48:20.863182: step 3155, loss 0.546833, acc 0.875, prec 0.0653823, recall 0.791547
2017-12-10T02:48:21.119493: step 3156, loss 0.883332, acc 0.875, prec 0.0653727, recall 0.791547
2017-12-10T02:48:21.379485: step 3157, loss 0.241907, acc 0.90625, prec 0.0653656, recall 0.791547
2017-12-10T02:48:21.642787: step 3158, loss 0.513149, acc 0.890625, prec 0.0653572, recall 0.791547
2017-12-10T02:48:21.912986: step 3159, loss 0.494765, acc 0.921875, prec 0.0653854, recall 0.791639
2017-12-10T02:48:22.171379: step 3160, loss 0.333694, acc 0.890625, prec 0.065377, recall 0.791639
2017-12-10T02:48:22.432137: step 3161, loss 0.106061, acc 0.96875, prec 0.0653746, recall 0.791639
2017-12-10T02:48:22.692264: step 3162, loss 0.243523, acc 0.921875, prec 0.0653857, recall 0.791685
2017-12-10T02:48:22.962372: step 3163, loss 0.235898, acc 0.953125, prec 0.0653992, recall 0.791731
2017-12-10T02:48:23.234228: step 3164, loss 0.207738, acc 0.953125, prec 0.0654127, recall 0.791777
2017-12-10T02:48:23.501722: step 3165, loss 0.137867, acc 0.953125, prec 0.0654262, recall 0.791823
2017-12-10T02:48:23.766342: step 3166, loss 0.102718, acc 0.96875, prec 0.0654579, recall 0.791915
2017-12-10T02:48:24.029925: step 3167, loss 0.0370527, acc 0.984375, prec 0.0654567, recall 0.791915
2017-12-10T02:48:24.289876: step 3168, loss 0.0464657, acc 0.984375, prec 0.0654555, recall 0.791915
2017-12-10T02:48:24.550172: step 3169, loss 0.320108, acc 0.9375, prec 0.0654678, recall 0.791961
2017-12-10T02:48:24.812839: step 3170, loss 0.226992, acc 0.96875, prec 0.0654825, recall 0.792007
2017-12-10T02:48:25.076137: step 3171, loss 0.828301, acc 0.96875, prec 0.0655142, recall 0.792099
2017-12-10T02:48:25.342266: step 3172, loss 0.033067, acc 1, prec 0.0655142, recall 0.792099
2017-12-10T02:48:25.610311: step 3173, loss 0.0375078, acc 1, prec 0.0655483, recall 0.792191
2017-12-10T02:48:25.875378: step 3174, loss 0.110016, acc 0.96875, prec 0.0655459, recall 0.792191
2017-12-10T02:48:26.136849: step 3175, loss 0.284037, acc 0.96875, prec 0.0655777, recall 0.792282
2017-12-10T02:48:26.410539: step 3176, loss 0.152286, acc 0.984375, prec 0.0655935, recall 0.792328
2017-12-10T02:48:26.686840: step 3177, loss 0.459079, acc 1, prec 0.0656276, recall 0.79242
2017-12-10T02:48:26.960882: step 3178, loss 0.12062, acc 0.96875, prec 0.0656252, recall 0.79242
2017-12-10T02:48:27.224075: step 3179, loss 7.709, acc 0.96875, prec 0.0657093, recall 0.792474
2017-12-10T02:48:27.485580: step 3180, loss 2.57155, acc 0.890625, prec 0.0657191, recall 0.792345
2017-12-10T02:48:27.753806: step 3181, loss 0.245285, acc 0.875, prec 0.0657266, recall 0.792391
2017-12-10T02:48:28.016252: step 3182, loss 0.693606, acc 0.828125, prec 0.0657134, recall 0.792391
2017-12-10T02:48:28.277715: step 3183, loss 0.290104, acc 0.890625, prec 0.065705, recall 0.792391
2017-12-10T02:48:28.539992: step 3184, loss 0.429884, acc 0.859375, prec 0.0656942, recall 0.792391
2017-12-10T02:48:28.804764: step 3185, loss 1.58173, acc 0.671875, prec 0.0656691, recall 0.792391
2017-12-10T02:48:29.064447: step 3186, loss 1.05276, acc 0.78125, prec 0.0657034, recall 0.792527
2017-12-10T02:48:29.326938: step 3187, loss 1.53368, acc 0.65625, prec 0.0656941, recall 0.792573
2017-12-10T02:48:29.588446: step 3188, loss 1.02548, acc 0.734375, prec 0.0656908, recall 0.792619
2017-12-10T02:48:29.853082: step 3189, loss 0.847688, acc 0.78125, prec 0.065691, recall 0.792664
2017-12-10T02:48:30.132961: step 3190, loss 1.44182, acc 0.6875, prec 0.0657011, recall 0.792755
2017-12-10T02:48:30.400604: step 3191, loss 1.05031, acc 0.78125, prec 0.0656844, recall 0.792755
2017-12-10T02:48:30.660127: step 3192, loss 1.24759, acc 0.78125, prec 0.0657187, recall 0.792892
2017-12-10T02:48:30.922309: step 3193, loss 1.08286, acc 0.65625, prec 0.0656924, recall 0.792892
2017-12-10T02:48:31.179883: step 3194, loss 0.589591, acc 0.828125, prec 0.0656792, recall 0.792892
2017-12-10T02:48:31.439961: step 3195, loss 0.937888, acc 0.8125, prec 0.0656989, recall 0.792982
2017-12-10T02:48:31.708038: step 3196, loss 0.673794, acc 0.796875, prec 0.0657003, recall 0.793028
2017-12-10T02:48:31.985642: step 3197, loss 0.462392, acc 0.859375, prec 0.0657066, recall 0.793073
2017-12-10T02:48:32.251584: step 3198, loss 0.63528, acc 0.859375, prec 0.0657128, recall 0.793119
2017-12-10T02:48:32.520685: step 3199, loss 0.342472, acc 0.953125, prec 0.0657092, recall 0.793119
2017-12-10T02:48:32.785768: step 3200, loss 0.192149, acc 0.953125, prec 0.0657056, recall 0.793119
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-3200

2017-12-10T02:48:34.094208: step 3201, loss 0.0852397, acc 0.984375, prec 0.0657044, recall 0.793119
2017-12-10T02:48:34.365824: step 3202, loss 0.205647, acc 0.953125, prec 0.0657178, recall 0.793164
2017-12-10T02:48:34.630931: step 3203, loss 0.143075, acc 0.953125, prec 0.0657142, recall 0.793164
2017-12-10T02:48:34.909697: step 3204, loss 4.18363, acc 0.96875, prec 0.065713, recall 0.79299
2017-12-10T02:48:35.183613: step 3205, loss 2.60368, acc 0.953125, prec 0.0657107, recall 0.792816
2017-12-10T02:48:35.445727: step 3206, loss 0.273995, acc 0.953125, prec 0.065741, recall 0.792907
2017-12-10T02:48:35.709022: step 3207, loss 0.480946, acc 0.96875, prec 0.0657556, recall 0.792952
2017-12-10T02:48:35.971278: step 3208, loss 0.0379423, acc 0.984375, prec 0.0657544, recall 0.792952
2017-12-10T02:48:36.242091: step 3209, loss 0.389539, acc 0.921875, prec 0.0657654, recall 0.792998
2017-12-10T02:48:36.503766: step 3210, loss 0.221346, acc 0.90625, prec 0.0657751, recall 0.793043
2017-12-10T02:48:36.763380: step 3211, loss 0.528671, acc 0.875, prec 0.0657656, recall 0.793043
2017-12-10T02:48:37.031035: step 3212, loss 0.727732, acc 0.8125, prec 0.0657682, recall 0.793088
2017-12-10T02:48:37.296512: step 3213, loss 0.430681, acc 0.8125, prec 0.0657539, recall 0.793088
2017-12-10T02:48:37.563362: step 3214, loss 0.440009, acc 0.8125, prec 0.0657735, recall 0.793179
2017-12-10T02:48:37.835970: step 3215, loss 0.439858, acc 0.890625, prec 0.0657651, recall 0.793179
2017-12-10T02:48:38.104309: step 3216, loss 0.199362, acc 0.9375, prec 0.0657604, recall 0.793179
2017-12-10T02:48:38.366610: step 3217, loss 0.664358, acc 0.84375, prec 0.0657654, recall 0.793224
2017-12-10T02:48:38.631947: step 3218, loss 0.843901, acc 0.8125, prec 0.065768, recall 0.793269
2017-12-10T02:48:38.897777: step 3219, loss 0.740155, acc 0.828125, prec 0.0657549, recall 0.793269
2017-12-10T02:48:39.174225: step 3220, loss 0.319629, acc 0.921875, prec 0.0657659, recall 0.793314
2017-12-10T02:48:39.434969: step 3221, loss 0.579301, acc 0.859375, prec 0.0657552, recall 0.793314
2017-12-10T02:48:39.705154: step 3222, loss 0.163023, acc 0.921875, prec 0.065783, recall 0.793405
2017-12-10T02:48:39.973096: step 3223, loss 0.293335, acc 0.890625, prec 0.0657747, recall 0.793405
2017-12-10T02:48:40.235495: step 3224, loss 0.120813, acc 0.984375, prec 0.0657904, recall 0.79345
2017-12-10T02:48:40.497330: step 3225, loss 0.146696, acc 0.96875, prec 0.065805, recall 0.793495
2017-12-10T02:48:40.759693: step 3226, loss 0.35744, acc 0.90625, prec 0.0657978, recall 0.793495
2017-12-10T02:48:41.025208: step 3227, loss 0.274164, acc 0.953125, prec 0.0658281, recall 0.793585
2017-12-10T02:48:41.299818: step 3228, loss 0.0676367, acc 0.96875, prec 0.0658426, recall 0.79363
2017-12-10T02:48:41.583324: step 3229, loss 0.0571008, acc 0.96875, prec 0.0658571, recall 0.793675
2017-12-10T02:48:41.848183: step 3230, loss 0.266559, acc 0.90625, prec 0.0658669, recall 0.79372
2017-12-10T02:48:42.109236: step 3231, loss 0.122097, acc 0.953125, prec 0.0658971, recall 0.79381
2017-12-10T02:48:42.368950: step 3232, loss 0.0355748, acc 0.984375, prec 0.0658959, recall 0.79381
2017-12-10T02:48:42.633413: step 3233, loss 0.769547, acc 0.984375, prec 0.0659116, recall 0.793855
2017-12-10T02:48:42.900842: step 3234, loss 0.197676, acc 0.9375, prec 0.0659068, recall 0.793855
2017-12-10T02:48:43.169618: step 3235, loss 0.0570108, acc 0.984375, prec 0.0659225, recall 0.7939
2017-12-10T02:48:43.432239: step 3236, loss 0.659936, acc 1, prec 0.0659732, recall 0.794034
2017-12-10T02:48:43.701337: step 3237, loss 0.00841809, acc 1, prec 0.0659732, recall 0.794034
2017-12-10T02:48:43.958345: step 3238, loss 0.0250978, acc 0.984375, prec 0.065972, recall 0.794034
2017-12-10T02:48:44.225613: step 3239, loss 0.273436, acc 0.9375, prec 0.0660011, recall 0.794124
2017-12-10T02:48:44.488135: step 3240, loss 0.0218426, acc 1, prec 0.0660348, recall 0.794214
2017-12-10T02:48:44.750198: step 3241, loss 0.118298, acc 0.953125, prec 0.0660481, recall 0.794258
2017-12-10T02:48:45.018484: step 3242, loss 0.0584342, acc 0.96875, prec 0.0660458, recall 0.794258
2017-12-10T02:48:45.280539: step 3243, loss 0.173137, acc 0.953125, prec 0.066076, recall 0.794348
2017-12-10T02:48:45.547473: step 3244, loss 0.180356, acc 0.953125, prec 0.0661061, recall 0.794437
2017-12-10T02:48:45.817879: step 3245, loss 0.278148, acc 0.90625, prec 0.066099, recall 0.794437
2017-12-10T02:48:46.077512: step 3246, loss 0.05981, acc 0.984375, prec 0.0660978, recall 0.794437
2017-12-10T02:48:46.342650: step 3247, loss 0.313425, acc 0.9375, prec 0.0661099, recall 0.794482
2017-12-10T02:48:46.609477: step 3248, loss 0.279903, acc 0.875, prec 0.0661341, recall 0.794571
2017-12-10T02:48:46.871227: step 3249, loss 0.190767, acc 0.953125, prec 0.0661305, recall 0.794571
2017-12-10T02:48:47.141221: step 3250, loss 0.283155, acc 0.96875, prec 0.0661619, recall 0.79466
2017-12-10T02:48:47.406843: step 3251, loss 0.150723, acc 0.96875, prec 0.0661932, recall 0.794749
2017-12-10T02:48:47.672556: step 3252, loss 0.0652942, acc 0.984375, prec 0.066192, recall 0.794749
2017-12-10T02:48:47.934885: step 3253, loss 0.31028, acc 0.9375, prec 0.0662379, recall 0.794883
2017-12-10T02:48:48.196200: step 3254, loss 1.41222, acc 0.96875, prec 0.0662535, recall 0.794755
2017-12-10T02:48:48.464937: step 3255, loss 0.280607, acc 0.921875, prec 0.0662475, recall 0.794755
2017-12-10T02:48:48.730100: step 3256, loss 0.0830551, acc 0.96875, prec 0.0662451, recall 0.794755
2017-12-10T02:48:48.998116: step 3257, loss 0.093241, acc 0.984375, prec 0.0662439, recall 0.794755
2017-12-10T02:48:49.264821: step 3258, loss 0.192773, acc 0.90625, prec 0.0662368, recall 0.794755
2017-12-10T02:48:49.539361: step 3259, loss 0.134653, acc 0.953125, prec 0.0662669, recall 0.794844
2017-12-10T02:48:49.812725: step 3260, loss 0.124693, acc 0.96875, prec 0.0662982, recall 0.794933
2017-12-10T02:48:50.106866: step 3261, loss 0.240441, acc 0.9375, prec 0.0663103, recall 0.794977
2017-12-10T02:48:50.385556: step 3262, loss 0.592556, acc 0.953125, prec 0.0663404, recall 0.795066
2017-12-10T02:48:50.656258: step 3263, loss 0.619294, acc 0.890625, prec 0.0663489, recall 0.79511
2017-12-10T02:48:50.926854: step 3264, loss 0.491158, acc 0.84375, prec 0.0663538, recall 0.795155
2017-12-10T02:48:51.195543: step 3265, loss 1.67364, acc 0.90625, prec 0.0663972, recall 0.795287
2017-12-10T02:48:51.470520: step 3266, loss 0.501403, acc 0.890625, prec 0.0664056, recall 0.795332
2017-12-10T02:48:51.732213: step 3267, loss 0.225165, acc 0.921875, prec 0.0664165, recall 0.795376
2017-12-10T02:48:52.000125: step 3268, loss 0.681023, acc 0.875, prec 0.0664237, recall 0.79542
2017-12-10T02:48:52.271954: step 3269, loss 0.472018, acc 0.796875, prec 0.0664081, recall 0.79542
2017-12-10T02:48:52.537359: step 3270, loss 0.306979, acc 0.90625, prec 0.0664178, recall 0.795464
2017-12-10T02:48:52.806730: step 3271, loss 0.304276, acc 0.921875, prec 0.0664286, recall 0.795509
2017-12-10T02:48:53.070610: step 3272, loss 0.524801, acc 0.875, prec 0.0664191, recall 0.795509
2017-12-10T02:48:53.331645: step 3273, loss 0.269435, acc 0.875, prec 0.0664095, recall 0.795509
2017-12-10T02:48:53.593504: step 3274, loss 0.702695, acc 0.859375, prec 0.0663987, recall 0.795509
2017-12-10T02:48:53.861398: step 3275, loss 0.487844, acc 0.828125, prec 0.0663856, recall 0.795509
2017-12-10T02:48:54.129567: step 3276, loss 0.222767, acc 0.90625, prec 0.0663784, recall 0.795509
2017-12-10T02:48:54.391787: step 3277, loss 0.458914, acc 0.9375, prec 0.0664241, recall 0.795641
2017-12-10T02:48:54.663545: step 3278, loss 0.257971, acc 0.921875, prec 0.0664181, recall 0.795641
2017-12-10T02:48:54.926820: step 3279, loss 0.0528658, acc 0.984375, prec 0.0664169, recall 0.795641
2017-12-10T02:48:55.187475: step 3280, loss 0.0384403, acc 0.984375, prec 0.0664493, recall 0.795729
2017-12-10T02:48:55.456871: step 3281, loss 0.103447, acc 0.9375, prec 0.0664445, recall 0.795729
2017-12-10T02:48:55.726746: step 3282, loss 0.155446, acc 0.953125, prec 0.0664409, recall 0.795729
2017-12-10T02:48:55.991058: step 3283, loss 0.0583115, acc 1, prec 0.0664577, recall 0.795773
2017-12-10T02:48:56.259406: step 3284, loss 0.328954, acc 0.96875, prec 0.0664722, recall 0.795817
2017-12-10T02:48:56.541155: step 3285, loss 0.0167301, acc 1, prec 0.0664722, recall 0.795817
2017-12-10T02:48:56.803559: step 3286, loss 0.047485, acc 0.984375, prec 0.0664878, recall 0.795861
2017-12-10T02:48:57.071735: step 3287, loss 0.0142787, acc 1, prec 0.0664878, recall 0.795861
2017-12-10T02:48:57.337559: step 3288, loss 0.276537, acc 0.921875, prec 0.0664818, recall 0.795861
2017-12-10T02:48:57.604449: step 3289, loss 2.11717, acc 0.9375, prec 0.066495, recall 0.795734
2017-12-10T02:48:57.872339: step 3290, loss 0.0140746, acc 0.984375, prec 0.0664938, recall 0.795734
2017-12-10T02:48:58.133919: step 3291, loss 0.171474, acc 0.984375, prec 0.0665094, recall 0.795778
2017-12-10T02:48:58.397825: step 3292, loss 0.0832027, acc 0.984375, prec 0.0665082, recall 0.795778
2017-12-10T02:48:58.664701: step 3293, loss 0.91791, acc 0.90625, prec 0.0665347, recall 0.795866
2017-12-10T02:48:58.934331: step 3294, loss 0.289515, acc 0.9375, prec 0.0665299, recall 0.795866
2017-12-10T02:48:59.194113: step 3295, loss 0.174542, acc 0.921875, prec 0.0665239, recall 0.795866
2017-12-10T02:48:59.458878: step 3296, loss 0.0932597, acc 0.96875, prec 0.0665383, recall 0.79591
2017-12-10T02:48:59.726527: step 3297, loss 0.371567, acc 0.890625, prec 0.0665299, recall 0.79591
2017-12-10T02:48:59.996888: step 3298, loss 0.161995, acc 0.921875, prec 0.0665239, recall 0.79591
2017-12-10T02:49:00.267514: step 3299, loss 0.428307, acc 0.859375, prec 0.0665467, recall 0.795997
2017-12-10T02:49:00.538856: step 3300, loss 0.147713, acc 0.96875, prec 0.0665779, recall 0.796085

Evaluation:
2017-12-10T02:49:08.086409: step 3300, loss 1.98577, acc 0.900557, prec 0.0671535, recall 0.790983

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-3300

2017-12-10T02:49:09.431769: step 3301, loss 0.348452, acc 0.890625, prec 0.0671617, recall 0.791026
2017-12-10T02:49:09.691531: step 3302, loss 0.328787, acc 0.890625, prec 0.0671863, recall 0.791113
2017-12-10T02:49:09.953178: step 3303, loss 0.186798, acc 0.921875, prec 0.0671968, recall 0.791156
2017-12-10T02:49:10.216478: step 3304, loss 0.211615, acc 0.921875, prec 0.0671909, recall 0.791156
2017-12-10T02:49:10.482861: step 3305, loss 0.421299, acc 0.890625, prec 0.067199, recall 0.7912
2017-12-10T02:49:10.747059: step 3306, loss 0.39192, acc 0.875, prec 0.067206, recall 0.791243
2017-12-10T02:49:11.016874: step 3307, loss 0.125114, acc 0.96875, prec 0.0672036, recall 0.791243
2017-12-10T02:49:11.284171: step 3308, loss 0.0532117, acc 1, prec 0.0672036, recall 0.791243
2017-12-10T02:49:11.557910: step 3309, loss 0.0575681, acc 0.96875, prec 0.0672177, recall 0.791286
2017-12-10T02:49:11.827988: step 3310, loss 0.137292, acc 0.9375, prec 0.067213, recall 0.791286
2017-12-10T02:49:12.090001: step 3311, loss 0.347547, acc 0.953125, prec 0.0672752, recall 0.791459
2017-12-10T02:49:12.353478: step 3312, loss 0.114106, acc 0.9375, prec 0.0673033, recall 0.791546
2017-12-10T02:49:12.616483: step 3313, loss 0.722913, acc 0.921875, prec 0.0673302, recall 0.791632
2017-12-10T02:49:12.888288: step 3314, loss 0.0565735, acc 0.96875, prec 0.0673278, recall 0.791632
2017-12-10T02:49:13.158356: step 3315, loss 0.308528, acc 0.875, prec 0.0673348, recall 0.791675
2017-12-10T02:49:13.421729: step 3316, loss 0.161739, acc 0.984375, prec 0.06735, recall 0.791718
2017-12-10T02:49:13.690044: step 3317, loss 0.096819, acc 0.96875, prec 0.0673477, recall 0.791718
2017-12-10T02:49:13.952060: step 3318, loss 0.236084, acc 0.953125, prec 0.0673605, recall 0.791762
2017-12-10T02:49:14.223788: step 3319, loss 0.0168434, acc 1, prec 0.0673934, recall 0.791848
2017-12-10T02:49:14.491116: step 3320, loss 0.217396, acc 0.96875, prec 0.0674238, recall 0.791934
2017-12-10T02:49:14.751171: step 3321, loss 0.102492, acc 0.96875, prec 0.0674215, recall 0.791934
2017-12-10T02:49:15.016918: step 3322, loss 0.0547513, acc 0.96875, prec 0.0674191, recall 0.791934
2017-12-10T02:49:15.281312: step 3323, loss 0.361388, acc 0.953125, prec 0.0674484, recall 0.79202
2017-12-10T02:49:15.555464: step 3324, loss 0.0278872, acc 1, prec 0.0674484, recall 0.79202
2017-12-10T02:49:15.824678: step 3325, loss 0.224547, acc 0.9375, prec 0.0674436, recall 0.79202
2017-12-10T02:49:16.097302: step 3326, loss 0.418246, acc 0.90625, prec 0.0674365, recall 0.79202
2017-12-10T02:49:16.365926: step 3327, loss 0.169472, acc 0.953125, prec 0.0674329, recall 0.79202
2017-12-10T02:49:16.633115: step 3328, loss 0.37117, acc 0.953125, prec 0.0674622, recall 0.792106
2017-12-10T02:49:16.896736: step 3329, loss 0.101367, acc 0.96875, prec 0.0674598, recall 0.792106
2017-12-10T02:49:17.162202: step 3330, loss 0.0722217, acc 0.984375, prec 0.0674915, recall 0.792192
2017-12-10T02:49:17.437054: step 3331, loss 0.173445, acc 0.953125, prec 0.0674879, recall 0.792192
2017-12-10T02:49:17.701213: step 3332, loss 0.503025, acc 1, prec 0.0675043, recall 0.792235
2017-12-10T02:49:17.975838: step 3333, loss 0.0988404, acc 0.96875, prec 0.0675183, recall 0.792278
2017-12-10T02:49:18.236057: step 3334, loss 0.29909, acc 0.96875, prec 0.0675488, recall 0.792363
2017-12-10T02:49:18.506476: step 3335, loss 0.196678, acc 0.921875, prec 0.0675756, recall 0.792449
2017-12-10T02:49:18.766323: step 3336, loss 0.0535781, acc 0.984375, prec 0.0675745, recall 0.792449
2017-12-10T02:49:19.030440: step 3337, loss 0.25444, acc 0.953125, prec 0.0676037, recall 0.792535
2017-12-10T02:49:19.294795: step 3338, loss 0.0342903, acc 1, prec 0.0676365, recall 0.79262
2017-12-10T02:49:19.560536: step 3339, loss 0.246732, acc 0.890625, prec 0.0676446, recall 0.792663
2017-12-10T02:49:19.821604: step 3340, loss 0.654585, acc 0.96875, prec 0.0676586, recall 0.792706
2017-12-10T02:49:20.094293: step 3341, loss 0.125321, acc 0.984375, prec 0.0676738, recall 0.792748
2017-12-10T02:49:20.361128: step 3342, loss 0.127844, acc 0.96875, prec 0.0676714, recall 0.792748
2017-12-10T02:49:20.628414: step 3343, loss 0.166834, acc 0.953125, prec 0.067717, recall 0.792876
2017-12-10T02:49:20.902880: step 3344, loss 0.0921175, acc 0.953125, prec 0.0677135, recall 0.792876
2017-12-10T02:49:21.176211: step 3345, loss 0.277375, acc 0.90625, prec 0.0677063, recall 0.792876
2017-12-10T02:49:21.441672: step 3346, loss 0.256539, acc 0.96875, prec 0.0677367, recall 0.792962
2017-12-10T02:49:21.714493: step 3347, loss 0.588106, acc 0.84375, prec 0.0677248, recall 0.792962
2017-12-10T02:49:21.977555: step 3348, loss 0.0637139, acc 0.96875, prec 0.0677224, recall 0.792962
2017-12-10T02:49:22.248451: step 3349, loss 0.261242, acc 0.921875, prec 0.0677329, recall 0.793004
2017-12-10T02:49:22.515673: step 3350, loss 1.21062, acc 0.9375, prec 0.0677609, recall 0.793089
2017-12-10T02:49:22.780626: step 3351, loss 0.0592565, acc 0.96875, prec 0.0677585, recall 0.793089
2017-12-10T02:49:23.048994: step 3352, loss 0.258055, acc 0.90625, prec 0.0677677, recall 0.793132
2017-12-10T02:49:23.311719: step 3353, loss 0.163684, acc 0.953125, prec 0.0677642, recall 0.793132
2017-12-10T02:49:23.583395: step 3354, loss 0.212086, acc 0.921875, prec 0.0677582, recall 0.793132
2017-12-10T02:49:23.848855: step 3355, loss 0.204335, acc 0.9375, prec 0.0677862, recall 0.793217
2017-12-10T02:49:24.111898: step 3356, loss 0.0891992, acc 0.953125, prec 0.0677826, recall 0.793217
2017-12-10T02:49:24.377127: step 3357, loss 0.162005, acc 0.953125, prec 0.0677954, recall 0.793259
2017-12-10T02:49:24.640358: step 3358, loss 0.495554, acc 0.875, prec 0.0678023, recall 0.793302
2017-12-10T02:49:24.901105: step 3359, loss 0.230146, acc 0.9375, prec 0.0677975, recall 0.793302
2017-12-10T02:49:25.167140: step 3360, loss 0.14022, acc 0.96875, prec 0.0678115, recall 0.793344
2017-12-10T02:49:25.428204: step 3361, loss 0.189497, acc 0.953125, prec 0.0678243, recall 0.793387
2017-12-10T02:49:25.692258: step 3362, loss 0.147975, acc 0.953125, prec 0.0678371, recall 0.793429
2017-12-10T02:49:25.966497: step 3363, loss 0.229307, acc 0.9375, prec 0.067865, recall 0.793514
2017-12-10T02:49:26.239400: step 3364, loss 0.0531908, acc 0.984375, prec 0.0678802, recall 0.793556
2017-12-10T02:49:26.510537: step 3365, loss 0.137051, acc 0.953125, prec 0.0678766, recall 0.793556
2017-12-10T02:49:26.774759: step 3366, loss 0.166388, acc 0.953125, prec 0.0678894, recall 0.793599
2017-12-10T02:49:27.037690: step 3367, loss 0.278333, acc 0.9375, prec 0.0678847, recall 0.793599
2017-12-10T02:49:27.297410: step 3368, loss 0.359781, acc 1, prec 0.067901, recall 0.793641
2017-12-10T02:49:27.561270: step 3369, loss 0.165201, acc 0.953125, prec 0.0679138, recall 0.793683
2017-12-10T02:49:27.834783: step 3370, loss 0.215056, acc 0.953125, prec 0.0679266, recall 0.793726
2017-12-10T02:49:28.108246: step 3371, loss 0.120334, acc 0.96875, prec 0.0679406, recall 0.793768
2017-12-10T02:49:28.374422: step 3372, loss 0.00770727, acc 1, prec 0.0679896, recall 0.793895
2017-12-10T02:49:28.635067: step 3373, loss 0.183493, acc 0.984375, prec 0.0680048, recall 0.793937
2017-12-10T02:49:28.902938: step 3374, loss 0.0768792, acc 0.96875, prec 0.0680024, recall 0.793937
2017-12-10T02:49:29.163684: step 3375, loss 0.0446904, acc 0.96875, prec 0.0680164, recall 0.793979
2017-12-10T02:49:29.437794: step 3376, loss 0.0194969, acc 1, prec 0.0680164, recall 0.793979
2017-12-10T02:49:29.709713: step 3377, loss 0.267264, acc 0.953125, prec 0.0680291, recall 0.794021
2017-12-10T02:49:29.976831: step 3378, loss 0.24557, acc 0.96875, prec 0.0680431, recall 0.794063
2017-12-10T02:49:30.252894: step 3379, loss 0.0237123, acc 0.984375, prec 0.0680419, recall 0.794063
2017-12-10T02:49:30.517686: step 3380, loss 0.0202244, acc 1, prec 0.0680419, recall 0.794063
2017-12-10T02:49:30.780261: step 3381, loss 0.259536, acc 0.96875, prec 0.0680558, recall 0.794106
2017-12-10T02:49:31.042600: step 3382, loss 0.123515, acc 0.96875, prec 0.0680535, recall 0.794106
2017-12-10T02:49:31.306711: step 3383, loss 0.337697, acc 0.984375, prec 0.0680686, recall 0.794148
2017-12-10T02:49:31.578697: step 3384, loss 0.295655, acc 0.921875, prec 0.0680626, recall 0.794148
2017-12-10T02:49:31.847769: step 3385, loss 0.0463097, acc 1, prec 0.0680626, recall 0.794148
2017-12-10T02:49:32.115541: step 3386, loss 0.193351, acc 0.984375, prec 0.0680778, recall 0.79419
2017-12-10T02:49:32.378414: step 3387, loss 0.24656, acc 1, prec 0.0681268, recall 0.794316
2017-12-10T02:49:32.646237: step 3388, loss 0.470261, acc 0.984375, prec 0.0681583, recall 0.7944
2017-12-10T02:49:32.921032: step 3389, loss 0.103627, acc 0.96875, prec 0.0681723, recall 0.794442
2017-12-10T02:49:33.189993: step 3390, loss 0.0511878, acc 0.984375, prec 0.0681874, recall 0.794484
2017-12-10T02:49:33.453797: step 3391, loss 0.199315, acc 0.96875, prec 0.0682013, recall 0.794526
2017-12-10T02:49:33.716593: step 3392, loss 0.0862728, acc 0.953125, prec 0.0681978, recall 0.794526
2017-12-10T02:49:33.982983: step 3393, loss 0.120164, acc 0.96875, prec 0.0682607, recall 0.794694
2017-12-10T02:49:34.250452: step 3394, loss 0.499756, acc 0.953125, prec 0.0682734, recall 0.794736
2017-12-10T02:49:34.522271: step 3395, loss 0.230126, acc 0.921875, prec 0.0682838, recall 0.794778
2017-12-10T02:49:34.799120: step 3396, loss 0.240332, acc 0.953125, prec 0.0682965, recall 0.794819
2017-12-10T02:49:35.065718: step 3397, loss 0.0435233, acc 0.96875, prec 0.0682941, recall 0.794819
2017-12-10T02:49:35.333393: step 3398, loss 0.200399, acc 0.9375, prec 0.0682893, recall 0.794819
2017-12-10T02:49:35.602035: step 3399, loss 0.809052, acc 0.9375, prec 0.0683172, recall 0.794903
2017-12-10T02:49:35.872567: step 3400, loss 1.14003, acc 0.9375, prec 0.0683287, recall 0.794945
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-3400

2017-12-10T02:49:37.156463: step 3401, loss 0.738064, acc 0.875, prec 0.0683518, recall 0.795029
2017-12-10T02:49:37.419854: step 3402, loss 0.408899, acc 0.875, prec 0.0683586, recall 0.79507
2017-12-10T02:49:37.678723: step 3403, loss 0.505367, acc 0.90625, prec 0.0684003, recall 0.795195
2017-12-10T02:49:37.945405: step 3404, loss 0.607243, acc 0.84375, prec 0.0684047, recall 0.795237
2017-12-10T02:49:38.208636: step 3405, loss 0.39891, acc 0.890625, prec 0.0683963, recall 0.795237
2017-12-10T02:49:38.474259: step 3406, loss 0.295555, acc 0.9375, prec 0.0684078, recall 0.795279
2017-12-10T02:49:38.741026: step 3407, loss 0.47273, acc 0.875, prec 0.0684145, recall 0.79532
2017-12-10T02:49:39.001196: step 3408, loss 0.757061, acc 0.765625, prec 0.0684129, recall 0.795362
2017-12-10T02:49:39.270677: step 3409, loss 0.718723, acc 0.859375, prec 0.0684021, recall 0.795362
2017-12-10T02:49:39.534870: step 3410, loss 0.682315, acc 0.84375, prec 0.0684227, recall 0.795445
2017-12-10T02:49:39.795948: step 3411, loss 0.291691, acc 0.890625, prec 0.0684306, recall 0.795487
2017-12-10T02:49:40.057662: step 3412, loss 0.742783, acc 0.921875, prec 0.0684572, recall 0.79557
2017-12-10T02:49:40.321635: step 3413, loss 0.318703, acc 0.921875, prec 0.0684512, recall 0.79557
2017-12-10T02:49:40.584729: step 3414, loss 0.467191, acc 0.859375, prec 0.0684405, recall 0.79557
2017-12-10T02:49:40.847670: step 3415, loss 0.747945, acc 0.8125, prec 0.0684261, recall 0.79557
2017-12-10T02:49:41.112559: step 3416, loss 0.449805, acc 0.890625, prec 0.068434, recall 0.795612
2017-12-10T02:49:41.374504: step 3417, loss 0.302029, acc 0.890625, prec 0.0684419, recall 0.795653
2017-12-10T02:49:41.645232: step 3418, loss 0.129128, acc 0.953125, prec 0.0684383, recall 0.795653
2017-12-10T02:49:41.915942: step 3419, loss 1.00746, acc 0.9375, prec 0.0684661, recall 0.795736
2017-12-10T02:49:42.185835: step 3420, loss 0.113288, acc 0.984375, prec 0.0684812, recall 0.795777
2017-12-10T02:49:42.453746: step 3421, loss 0.427956, acc 0.890625, prec 0.0684728, recall 0.795777
2017-12-10T02:49:42.725192: step 3422, loss 0.101088, acc 0.96875, prec 0.0684867, recall 0.795819
2017-12-10T02:49:42.997924: step 3423, loss 0.145908, acc 0.9375, prec 0.0684819, recall 0.795819
2017-12-10T02:49:43.258808: step 3424, loss 0.387118, acc 0.890625, prec 0.0684735, recall 0.795819
2017-12-10T02:49:43.523938: step 3425, loss 0.109683, acc 0.953125, prec 0.0684862, recall 0.79586
2017-12-10T02:49:43.791981: step 3426, loss 0.177506, acc 0.96875, prec 0.0685326, recall 0.795985
2017-12-10T02:49:44.061824: step 3427, loss 0.130468, acc 0.96875, prec 0.0685302, recall 0.795985
2017-12-10T02:49:44.331742: step 3428, loss 0.216921, acc 0.90625, prec 0.068523, recall 0.795985
2017-12-10T02:49:44.598287: step 3429, loss 0.461234, acc 0.890625, prec 0.0685472, recall 0.796067
2017-12-10T02:49:44.860076: step 3430, loss 0.275165, acc 0.96875, prec 0.0685448, recall 0.796067
2017-12-10T02:49:45.130963: step 3431, loss 0.277748, acc 0.96875, prec 0.0685587, recall 0.796109
2017-12-10T02:49:45.392959: step 3432, loss 0.226468, acc 0.953125, prec 0.0685551, recall 0.796109
2017-12-10T02:49:45.659446: step 3433, loss 0.116394, acc 0.921875, prec 0.0685653, recall 0.79615
2017-12-10T02:49:45.922587: step 3434, loss 0.0733307, acc 0.96875, prec 0.068563, recall 0.79615
2017-12-10T02:49:46.186755: step 3435, loss 0.0333319, acc 0.984375, prec 0.068578, recall 0.796191
2017-12-10T02:49:46.454100: step 3436, loss 0.117734, acc 0.9375, prec 0.0685895, recall 0.796233
2017-12-10T02:49:46.718269: step 3437, loss 0.241978, acc 0.953125, prec 0.0685859, recall 0.796233
2017-12-10T02:49:46.984243: step 3438, loss 0.174339, acc 0.953125, prec 0.0685823, recall 0.796233
2017-12-10T02:49:47.259206: step 3439, loss 0.0794059, acc 0.984375, prec 0.0685973, recall 0.796274
2017-12-10T02:49:47.520972: step 3440, loss 0.0529192, acc 0.984375, prec 0.0686124, recall 0.796315
2017-12-10T02:49:47.782684: step 3441, loss 0.10164, acc 0.96875, prec 0.0686425, recall 0.796398
2017-12-10T02:49:48.052644: step 3442, loss 0.0124959, acc 1, prec 0.0686425, recall 0.796398
2017-12-10T02:49:48.313851: step 3443, loss 0.371461, acc 0.984375, prec 0.0686575, recall 0.796439
2017-12-10T02:49:48.582968: step 3444, loss 0.0200536, acc 1, prec 0.0686575, recall 0.796439
2017-12-10T02:49:48.840918: step 3445, loss 0.0582052, acc 1, prec 0.0687225, recall 0.796603
2017-12-10T02:49:49.101882: step 3446, loss 0.000269996, acc 1, prec 0.0687225, recall 0.796603
2017-12-10T02:49:49.370318: step 3447, loss 5.22187, acc 0.9375, prec 0.0687689, recall 0.796405
2017-12-10T02:49:49.641587: step 3448, loss 2.01182, acc 0.953125, prec 0.0687827, recall 0.796285
2017-12-10T02:49:49.910189: step 3449, loss 0.176624, acc 0.9375, prec 0.0687941, recall 0.796326
2017-12-10T02:49:50.183193: step 3450, loss 0.362727, acc 0.875, prec 0.0687845, recall 0.796326
2017-12-10T02:49:50.453009: step 3451, loss 0.75362, acc 0.796875, prec 0.068769, recall 0.796326
2017-12-10T02:49:50.717522: step 3452, loss 0.598593, acc 0.828125, prec 0.0687558, recall 0.796326
2017-12-10T02:49:50.976496: step 3453, loss 1.30387, acc 0.765625, prec 0.068754, recall 0.796367
2017-12-10T02:49:51.244861: step 3454, loss 0.864366, acc 0.75, prec 0.0687673, recall 0.796449
2017-12-10T02:49:51.505489: step 3455, loss 1.13216, acc 0.765625, prec 0.0687493, recall 0.796449
2017-12-10T02:49:51.770787: step 3456, loss 1.8871, acc 0.5625, prec 0.0687321, recall 0.796491
2017-12-10T02:49:52.029213: step 3457, loss 1.5343, acc 0.609375, prec 0.0687508, recall 0.796614
2017-12-10T02:49:52.297467: step 3458, loss 1.5172, acc 0.625, prec 0.0687221, recall 0.796614
2017-12-10T02:49:52.564592: step 3459, loss 1.22006, acc 0.6875, prec 0.0686982, recall 0.796614
2017-12-10T02:49:52.829520: step 3460, loss 0.756144, acc 0.78125, prec 0.0686976, recall 0.796655
2017-12-10T02:49:53.090422: step 3461, loss 1.25462, acc 0.71875, prec 0.0687085, recall 0.796736
2017-12-10T02:49:53.360770: step 3462, loss 1.28563, acc 0.671875, prec 0.0686835, recall 0.796736
2017-12-10T02:49:53.623579: step 3463, loss 0.47348, acc 0.84375, prec 0.0687201, recall 0.796859
2017-12-10T02:49:53.891859: step 3464, loss 0.167673, acc 0.921875, prec 0.0687141, recall 0.796859
2017-12-10T02:49:54.149823: step 3465, loss 0.405109, acc 0.890625, prec 0.0687219, recall 0.7969
2017-12-10T02:49:54.411153: step 3466, loss 0.194235, acc 0.9375, prec 0.0687495, recall 0.796982
2017-12-10T02:49:54.672587: step 3467, loss 0.341213, acc 0.890625, prec 0.0687411, recall 0.796982
2017-12-10T02:49:54.939906: step 3468, loss 0.258129, acc 0.921875, prec 0.0687513, recall 0.797023
2017-12-10T02:49:55.203004: step 3469, loss 0.0989963, acc 0.96875, prec 0.0687489, recall 0.797023
2017-12-10T02:49:55.469457: step 3470, loss 0.981426, acc 0.96875, prec 0.0687788, recall 0.797104
2017-12-10T02:49:55.738005: step 3471, loss 4.9346, acc 0.9375, prec 0.0687765, recall 0.796784
2017-12-10T02:49:56.014418: step 3472, loss 0.215191, acc 0.9375, prec 0.0687717, recall 0.796784
2017-12-10T02:49:56.276811: step 3473, loss 0.424294, acc 0.875, prec 0.0687621, recall 0.796784
2017-12-10T02:49:56.557198: step 3474, loss 0.408819, acc 0.90625, prec 0.0687711, recall 0.796825
2017-12-10T02:49:56.820184: step 3475, loss 0.168327, acc 0.921875, prec 0.0687813, recall 0.796866
2017-12-10T02:49:57.085963: step 3476, loss 0.465194, acc 0.859375, prec 0.0687867, recall 0.796906
2017-12-10T02:49:57.346600: step 3477, loss 0.288312, acc 0.921875, prec 0.0688131, recall 0.796988
2017-12-10T02:49:57.611518: step 3478, loss 0.627948, acc 0.859375, prec 0.0688346, recall 0.797069
2017-12-10T02:49:57.834830: step 3479, loss 1.11607, acc 0.764706, prec 0.0688203, recall 0.797069
2017-12-10T02:49:58.112416: step 3480, loss 0.505961, acc 0.875, prec 0.0688269, recall 0.79711
2017-12-10T02:49:58.379721: step 3481, loss 0.779135, acc 0.78125, prec 0.0688263, recall 0.797151
2017-12-10T02:49:58.640533: step 3482, loss 0.82358, acc 0.828125, prec 0.0688294, recall 0.797192
2017-12-10T02:49:58.900622: step 3483, loss 0.721351, acc 0.796875, prec 0.0688139, recall 0.797192
2017-12-10T02:49:59.161481: step 3484, loss 0.303781, acc 0.921875, prec 0.0688079, recall 0.797192
2017-12-10T02:49:59.433419: step 3485, loss 0.488541, acc 0.859375, prec 0.0687972, recall 0.797192
2017-12-10T02:49:59.698169: step 3486, loss 0.488682, acc 0.890625, prec 0.068805, recall 0.797232
2017-12-10T02:49:59.969845: step 3487, loss 0.175016, acc 0.921875, prec 0.0688312, recall 0.797314
2017-12-10T02:50:00.241144: step 3488, loss 0.808737, acc 0.84375, prec 0.0688516, recall 0.797395
2017-12-10T02:50:00.511067: step 3489, loss 0.309765, acc 0.921875, prec 0.0688456, recall 0.797395
2017-12-10T02:50:00.769566: step 3490, loss 0.258717, acc 0.890625, prec 0.0688534, recall 0.797435
2017-12-10T02:50:01.041797: step 3491, loss 0.411309, acc 0.90625, prec 0.0688623, recall 0.797476
2017-12-10T02:50:01.302236: step 3492, loss 0.173163, acc 0.9375, prec 0.0688737, recall 0.797517
2017-12-10T02:50:01.560651: step 3493, loss 0.286703, acc 0.953125, prec 0.0688862, recall 0.797557
2017-12-10T02:50:01.834822: step 3494, loss 0.115716, acc 0.984375, prec 0.068885, recall 0.797557
2017-12-10T02:50:02.098877: step 3495, loss 0.0469092, acc 1, prec 0.0689011, recall 0.797598
2017-12-10T02:50:02.367304: step 3496, loss 0.062822, acc 0.984375, prec 0.0688999, recall 0.797598
2017-12-10T02:50:02.627292: step 3497, loss 0.256362, acc 0.96875, prec 0.0689297, recall 0.797679
2017-12-10T02:50:02.886172: step 3498, loss 0.15216, acc 0.953125, prec 0.0689262, recall 0.797679
2017-12-10T02:50:03.160742: step 3499, loss 0.0743523, acc 0.984375, prec 0.0689411, recall 0.797719
2017-12-10T02:50:03.434933: step 3500, loss 0.231351, acc 0.96875, prec 0.0689387, recall 0.797719
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-3500

2017-12-10T02:50:04.735416: step 3501, loss 0.0767615, acc 0.984375, prec 0.0689375, recall 0.797719
2017-12-10T02:50:05.000972: step 3502, loss 0.0730094, acc 0.96875, prec 0.0689512, recall 0.79776
2017-12-10T02:50:05.262819: step 3503, loss 0.0139359, acc 1, prec 0.0689512, recall 0.79776
2017-12-10T02:50:05.529269: step 3504, loss 0.0731406, acc 0.96875, prec 0.0689488, recall 0.79776
2017-12-10T02:50:05.792428: step 3505, loss 0.347181, acc 0.984375, prec 0.0689959, recall 0.797881
2017-12-10T02:50:06.055317: step 3506, loss 0.0289939, acc 0.984375, prec 0.0690108, recall 0.797921
2017-12-10T02:50:06.320293: step 3507, loss 0.883352, acc 0.96875, prec 0.0690406, recall 0.798002
2017-12-10T02:50:06.590251: step 3508, loss 0.00482973, acc 1, prec 0.0690567, recall 0.798042
2017-12-10T02:50:06.851981: step 3509, loss 0.160224, acc 0.984375, prec 0.0690716, recall 0.798083
2017-12-10T02:50:07.115452: step 3510, loss 0.0403654, acc 0.984375, prec 0.0690704, recall 0.798083
2017-12-10T02:50:07.376647: step 3511, loss 0.0334747, acc 0.984375, prec 0.0690692, recall 0.798083
2017-12-10T02:50:07.640895: step 3512, loss 0.0730558, acc 0.984375, prec 0.0690841, recall 0.798123
2017-12-10T02:50:07.905635: step 3513, loss 0.0625299, acc 0.96875, prec 0.0690817, recall 0.798123
2017-12-10T02:50:08.176022: step 3514, loss 0.41898, acc 0.984375, prec 0.0691127, recall 0.798204
2017-12-10T02:50:08.445588: step 3515, loss 4.15492, acc 0.90625, prec 0.0691228, recall 0.798085
2017-12-10T02:50:08.718693: step 3516, loss 0.415228, acc 0.96875, prec 0.0692009, recall 0.798286
2017-12-10T02:50:08.981086: step 3517, loss 0.0573948, acc 1, prec 0.069233, recall 0.798366
2017-12-10T02:50:09.250441: step 3518, loss 0.203898, acc 0.96875, prec 0.0692628, recall 0.798447
2017-12-10T02:50:09.513482: step 3519, loss 0.184526, acc 0.96875, prec 0.0692765, recall 0.798487
2017-12-10T02:50:09.783775: step 3520, loss 0.49155, acc 0.921875, prec 0.0693187, recall 0.798607
2017-12-10T02:50:10.054589: step 3521, loss 0.163966, acc 0.90625, prec 0.0693276, recall 0.798647
2017-12-10T02:50:10.323265: step 3522, loss 0.500839, acc 0.875, prec 0.0693663, recall 0.798767
2017-12-10T02:50:10.585582: step 3523, loss 0.542832, acc 0.84375, prec 0.0693704, recall 0.798807
2017-12-10T02:50:10.847495: step 3524, loss 0.665994, acc 0.8125, prec 0.069372, recall 0.798847
2017-12-10T02:50:11.110150: step 3525, loss 0.463612, acc 0.90625, prec 0.0693649, recall 0.798847
2017-12-10T02:50:11.379157: step 3526, loss 0.314093, acc 0.90625, prec 0.0693577, recall 0.798847
2017-12-10T02:50:11.644875: step 3527, loss 0.255536, acc 0.890625, prec 0.0693493, recall 0.798847
2017-12-10T02:50:11.910007: step 3528, loss 0.250272, acc 0.859375, prec 0.0693385, recall 0.798847
2017-12-10T02:50:12.173498: step 3529, loss 0.511683, acc 0.859375, prec 0.0693278, recall 0.798847
2017-12-10T02:50:12.432928: step 3530, loss 0.378004, acc 0.859375, prec 0.0693331, recall 0.798887
2017-12-10T02:50:12.700850: step 3531, loss 0.407146, acc 0.890625, prec 0.0693247, recall 0.798887
2017-12-10T02:50:12.965000: step 3532, loss 0.494986, acc 0.84375, prec 0.0693288, recall 0.798927
2017-12-10T02:50:13.231928: step 3533, loss 0.608345, acc 0.875, prec 0.0693513, recall 0.799007
2017-12-10T02:50:13.494057: step 3534, loss 0.377161, acc 0.921875, prec 0.0693774, recall 0.799087
2017-12-10T02:50:13.759970: step 3535, loss 0.240622, acc 0.921875, prec 0.0693875, recall 0.799127
2017-12-10T02:50:14.024517: step 3536, loss 0.382096, acc 0.9375, prec 0.0694468, recall 0.799286
2017-12-10T02:50:14.287791: step 3537, loss 0.23419, acc 0.90625, prec 0.0694557, recall 0.799326
2017-12-10T02:50:14.552301: step 3538, loss 0.23916, acc 0.96875, prec 0.0694854, recall 0.799405
2017-12-10T02:50:14.814684: step 3539, loss 0.0511099, acc 0.984375, prec 0.0694842, recall 0.799405
2017-12-10T02:50:15.074251: step 3540, loss 0.0908175, acc 0.953125, prec 0.0694806, recall 0.799405
2017-12-10T02:50:15.341518: step 3541, loss 0.352362, acc 0.96875, prec 0.0694942, recall 0.799445
2017-12-10T02:50:15.604514: step 3542, loss 0.161151, acc 0.96875, prec 0.0694918, recall 0.799445
2017-12-10T02:50:15.870848: step 3543, loss 0.248157, acc 0.953125, prec 0.0694882, recall 0.799445
2017-12-10T02:50:16.135530: step 3544, loss 0.546632, acc 0.96875, prec 0.0695179, recall 0.799525
2017-12-10T02:50:16.404489: step 3545, loss 0.797133, acc 0.9375, prec 0.0695291, recall 0.799564
2017-12-10T02:50:16.670403: step 3546, loss 0.047007, acc 0.984375, prec 0.0695279, recall 0.799564
2017-12-10T02:50:16.933863: step 3547, loss 0.189534, acc 0.96875, prec 0.0695576, recall 0.799644
2017-12-10T02:50:17.196988: step 3548, loss 0.338268, acc 0.9375, prec 0.0695528, recall 0.799644
2017-12-10T02:50:17.456382: step 3549, loss 5.17414, acc 0.9375, prec 0.0695812, recall 0.799565
2017-12-10T02:50:17.724198: step 3550, loss 0.219766, acc 0.953125, prec 0.0695776, recall 0.799565
2017-12-10T02:50:17.986404: step 3551, loss 0.162858, acc 0.953125, prec 0.0695741, recall 0.799565
2017-12-10T02:50:18.252476: step 3552, loss 0.144865, acc 0.953125, prec 0.0695705, recall 0.799565
2017-12-10T02:50:18.521414: step 3553, loss 0.102254, acc 0.953125, prec 0.0695669, recall 0.799565
2017-12-10T02:50:18.789093: step 3554, loss 0.407883, acc 0.90625, prec 0.0695917, recall 0.799644
2017-12-10T02:50:19.048727: step 3555, loss 0.238627, acc 0.890625, prec 0.0695993, recall 0.799684
2017-12-10T02:50:19.313096: step 3556, loss 0.219995, acc 0.921875, prec 0.0695933, recall 0.799684
2017-12-10T02:50:19.572221: step 3557, loss 0.546374, acc 0.890625, prec 0.069601, recall 0.799723
2017-12-10T02:50:19.843120: step 3558, loss 0.2722, acc 0.890625, prec 0.0695926, recall 0.799723
2017-12-10T02:50:20.106735: step 3559, loss 0.420887, acc 0.828125, prec 0.0695794, recall 0.799723
2017-12-10T02:50:20.367822: step 3560, loss 0.735013, acc 0.921875, prec 0.0695734, recall 0.799723
2017-12-10T02:50:20.635100: step 3561, loss 0.29799, acc 0.953125, prec 0.0695859, recall 0.799763
2017-12-10T02:50:20.905234: step 3562, loss 0.228996, acc 0.90625, prec 0.0695947, recall 0.799802
2017-12-10T02:50:21.179398: step 3563, loss 0.246404, acc 0.921875, prec 0.0696047, recall 0.799842
2017-12-10T02:50:21.449724: step 3564, loss 0.552091, acc 0.90625, prec 0.0696135, recall 0.799881
2017-12-10T02:50:21.712556: step 3565, loss 0.452207, acc 0.890625, prec 0.0696211, recall 0.799921
2017-12-10T02:50:21.980851: step 3566, loss 0.362504, acc 0.890625, prec 0.0696127, recall 0.799921
2017-12-10T02:50:22.245426: step 3567, loss 0.379414, acc 0.921875, prec 0.0696387, recall 0.8
2017-12-10T02:50:22.509938: step 3568, loss 0.147372, acc 0.9375, prec 0.069634, recall 0.8
2017-12-10T02:50:22.772636: step 3569, loss 0.50596, acc 0.890625, prec 0.0696256, recall 0.8
2017-12-10T02:50:23.032630: step 3570, loss 0.137208, acc 0.9375, prec 0.0696368, recall 0.800039
2017-12-10T02:50:23.296139: step 3571, loss 0.13804, acc 0.953125, prec 0.0696492, recall 0.800079
2017-12-10T02:50:23.558157: step 3572, loss 0.151434, acc 0.9375, prec 0.0696444, recall 0.800079
2017-12-10T02:50:23.827244: step 3573, loss 0.320784, acc 0.921875, prec 0.0696544, recall 0.800118
2017-12-10T02:50:24.085386: step 3574, loss 1.92306, acc 0.953125, prec 0.069652, recall 0.799961
2017-12-10T02:50:24.358113: step 3575, loss 0.191826, acc 0.9375, prec 0.0696472, recall 0.799961
2017-12-10T02:50:24.630532: step 3576, loss 0.0490454, acc 0.984375, prec 0.069646, recall 0.799961
2017-12-10T02:50:24.893083: step 3577, loss 0.315387, acc 0.953125, prec 0.0696744, recall 0.800039
2017-12-10T02:50:25.161640: step 3578, loss 2.78158, acc 0.9375, prec 0.0696708, recall 0.799882
2017-12-10T02:50:25.434206: step 3579, loss 0.100904, acc 0.984375, prec 0.0696856, recall 0.799921
2017-12-10T02:50:25.698548: step 3580, loss 0.163711, acc 0.9375, prec 0.0696968, recall 0.799961
2017-12-10T02:50:25.967802: step 3581, loss 0.335657, acc 0.921875, prec 0.0697227, recall 0.800039
2017-12-10T02:50:26.230932: step 3582, loss 0.194902, acc 0.921875, prec 0.0697487, recall 0.800118
2017-12-10T02:50:26.513554: step 3583, loss 0.546315, acc 0.84375, prec 0.0697367, recall 0.800118
2017-12-10T02:50:26.777961: step 3584, loss 0.347224, acc 0.921875, prec 0.0697467, recall 0.800157
2017-12-10T02:50:27.040320: step 3585, loss 0.344357, acc 0.921875, prec 0.0697407, recall 0.800157
2017-12-10T02:50:27.305134: step 3586, loss 0.156476, acc 0.9375, prec 0.0697519, recall 0.800197
2017-12-10T02:50:27.567393: step 3587, loss 0.586379, acc 0.828125, prec 0.0697547, recall 0.800236
2017-12-10T02:50:27.838031: step 3588, loss 0.560502, acc 0.875, prec 0.0697451, recall 0.800236
2017-12-10T02:50:28.103101: step 3589, loss 0.724842, acc 0.859375, prec 0.0697343, recall 0.800236
2017-12-10T02:50:28.372279: step 3590, loss 0.35535, acc 0.890625, prec 0.069726, recall 0.800236
2017-12-10T02:50:28.636979: step 3591, loss 0.308189, acc 0.859375, prec 0.0697471, recall 0.800315
2017-12-10T02:50:28.895889: step 3592, loss 0.20668, acc 0.953125, prec 0.0697435, recall 0.800315
2017-12-10T02:50:29.157605: step 3593, loss 0.0989173, acc 0.96875, prec 0.069773, recall 0.800393
2017-12-10T02:50:29.426045: step 3594, loss 0.343804, acc 0.890625, prec 0.0697806, recall 0.800433
2017-12-10T02:50:29.686175: step 3595, loss 0.31523, acc 0.90625, prec 0.0697894, recall 0.800472
2017-12-10T02:50:29.953524: step 3596, loss 0.510032, acc 0.90625, prec 0.0698141, recall 0.80055
2017-12-10T02:50:30.223095: step 3597, loss 0.169787, acc 0.890625, prec 0.0698057, recall 0.80055
2017-12-10T02:50:30.491179: step 3598, loss 0.0752, acc 0.96875, prec 0.0698033, recall 0.80055
2017-12-10T02:50:30.752635: step 3599, loss 0.095956, acc 0.96875, prec 0.0698009, recall 0.80055
2017-12-10T02:50:31.020593: step 3600, loss 0.0478554, acc 0.96875, prec 0.0698304, recall 0.800629

Evaluation:
2017-12-10T02:50:38.619038: step 3600, loss 4.19204, acc 0.952448, prec 0.0704357, recall 0.788773

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-3600

2017-12-10T02:50:40.189030: step 3601, loss 0.192647, acc 0.953125, prec 0.0704321, recall 0.788773
2017-12-10T02:50:40.451625: step 3602, loss 0.0544492, acc 0.96875, prec 0.0704455, recall 0.788813
2017-12-10T02:50:40.720605: step 3603, loss 0.161296, acc 0.984375, prec 0.0704759, recall 0.788893
2017-12-10T02:50:40.984985: step 3604, loss 0.11402, acc 0.96875, prec 0.0704735, recall 0.788893
2017-12-10T02:50:41.258540: step 3605, loss 0.158498, acc 0.984375, prec 0.0704881, recall 0.788933
2017-12-10T02:50:41.530718: step 3606, loss 0.154031, acc 0.96875, prec 0.0704857, recall 0.788933
2017-12-10T02:50:41.798712: step 3607, loss 0.818636, acc 0.953125, prec 0.0705137, recall 0.789014
2017-12-10T02:50:42.073323: step 3608, loss 0.00720889, acc 1, prec 0.0705295, recall 0.789054
2017-12-10T02:50:42.336422: step 3609, loss 2.32857, acc 0.9375, prec 0.0705575, recall 0.788984
2017-12-10T02:50:42.612705: step 3610, loss 0.120424, acc 1, prec 0.0705732, recall 0.789024
2017-12-10T02:50:42.879085: step 3611, loss 0.0295021, acc 1, prec 0.0705732, recall 0.789024
2017-12-10T02:50:43.143363: step 3612, loss 0.105448, acc 0.984375, prec 0.0706194, recall 0.789144
2017-12-10T02:50:43.414051: step 3613, loss 0.202616, acc 0.96875, prec 0.070617, recall 0.789144
2017-12-10T02:50:43.673577: step 3614, loss 0.24087, acc 0.921875, prec 0.0706268, recall 0.789184
2017-12-10T02:50:43.939474: step 3615, loss 0.0963735, acc 0.953125, prec 0.0706232, recall 0.789184
2017-12-10T02:50:44.204107: step 3616, loss 0.182959, acc 0.953125, prec 0.0706196, recall 0.789184
2017-12-10T02:50:44.473908: step 3617, loss 0.157458, acc 0.96875, prec 0.0706172, recall 0.789184
2017-12-10T02:50:44.732617: step 3618, loss 0.0816097, acc 0.9375, prec 0.0706124, recall 0.789184
2017-12-10T02:50:44.994294: step 3619, loss 0.256782, acc 0.9375, prec 0.0706392, recall 0.789264
2017-12-10T02:50:45.256525: step 3620, loss 0.148833, acc 0.96875, prec 0.0706368, recall 0.789264
2017-12-10T02:50:45.518536: step 3621, loss 0.180865, acc 0.953125, prec 0.0706647, recall 0.789344
2017-12-10T02:50:45.786378: step 3622, loss 0.40006, acc 0.875, prec 0.0706867, recall 0.789424
2017-12-10T02:50:46.049419: step 3623, loss 0.0109947, acc 1, prec 0.0706867, recall 0.789424
2017-12-10T02:50:46.315973: step 3624, loss 0.179645, acc 0.9375, prec 0.0707292, recall 0.789543
2017-12-10T02:50:46.582864: step 3625, loss 0.202735, acc 0.9375, prec 0.0707244, recall 0.789543
2017-12-10T02:50:46.849686: step 3626, loss 0.220149, acc 0.9375, prec 0.0707511, recall 0.789623
2017-12-10T02:50:47.117307: step 3627, loss 0.140222, acc 0.953125, prec 0.0707633, recall 0.789663
2017-12-10T02:50:47.419482: step 3628, loss 0.112559, acc 0.953125, prec 0.0707597, recall 0.789663
2017-12-10T02:50:47.691641: step 3629, loss 0.193299, acc 0.953125, prec 0.0707876, recall 0.789743
2017-12-10T02:50:47.954958: step 3630, loss 0.469014, acc 0.9375, prec 0.0707828, recall 0.789743
2017-12-10T02:50:48.229026: step 3631, loss 0.159135, acc 0.953125, prec 0.0708107, recall 0.789822
2017-12-10T02:50:48.495521: step 3632, loss 0.299099, acc 0.921875, prec 0.0708205, recall 0.789862
2017-12-10T02:50:48.757858: step 3633, loss 0.0719556, acc 0.953125, prec 0.0708169, recall 0.789862
2017-12-10T02:50:49.024124: step 3634, loss 0.267549, acc 0.96875, prec 0.0708145, recall 0.789862
2017-12-10T02:50:49.290544: step 3635, loss 0.052755, acc 0.96875, prec 0.0708121, recall 0.789862
2017-12-10T02:50:49.559974: step 3636, loss 0.0734836, acc 0.984375, prec 0.0708266, recall 0.789902
2017-12-10T02:50:49.827039: step 3637, loss 0.0404993, acc 0.984375, prec 0.0708412, recall 0.789941
2017-12-10T02:50:50.092031: step 3638, loss 0.041234, acc 0.984375, prec 0.07084, recall 0.789941
2017-12-10T02:50:50.358208: step 3639, loss 0.0623196, acc 0.96875, prec 0.0708533, recall 0.789981
2017-12-10T02:50:50.624554: step 3640, loss 0.0498069, acc 0.953125, prec 0.0708497, recall 0.789981
2017-12-10T02:50:50.890243: step 3641, loss 0.103009, acc 0.953125, prec 0.0708461, recall 0.789981
2017-12-10T02:50:51.154552: step 3642, loss 0.185267, acc 1, prec 0.0708776, recall 0.79006
2017-12-10T02:50:51.417614: step 3643, loss 0.0150744, acc 1, prec 0.0708934, recall 0.7901
2017-12-10T02:50:51.684983: step 3644, loss 0.309895, acc 0.984375, prec 0.0709079, recall 0.79014
2017-12-10T02:50:51.954295: step 3645, loss 0.345211, acc 0.953125, prec 0.0709358, recall 0.790219
2017-12-10T02:50:52.224426: step 3646, loss 0.109531, acc 0.96875, prec 0.0709492, recall 0.790259
2017-12-10T02:50:52.485228: step 3647, loss 0.174917, acc 0.96875, prec 0.0709625, recall 0.790298
2017-12-10T02:50:52.749085: step 3648, loss 0.372693, acc 0.953125, prec 0.0709746, recall 0.790338
2017-12-10T02:50:53.019658: step 3649, loss 0.0701665, acc 0.96875, prec 0.0709722, recall 0.790338
2017-12-10T02:50:53.295193: step 3650, loss 0.0854451, acc 0.984375, prec 0.0710025, recall 0.790417
2017-12-10T02:50:53.556331: step 3651, loss 0.0167214, acc 1, prec 0.0710182, recall 0.790456
2017-12-10T02:50:53.818197: step 3652, loss 0.175653, acc 0.96875, prec 0.0710473, recall 0.790535
2017-12-10T02:50:54.081873: step 3653, loss 0.779245, acc 0.984375, prec 0.0711091, recall 0.790693
2017-12-10T02:50:54.348862: step 3654, loss 0.0575785, acc 0.96875, prec 0.0711224, recall 0.790733
2017-12-10T02:50:54.621781: step 3655, loss 0.152611, acc 1, prec 0.0711539, recall 0.790812
2017-12-10T02:50:54.893265: step 3656, loss 1.29825, acc 0.953125, prec 0.0711515, recall 0.790663
2017-12-10T02:50:55.164081: step 3657, loss 0.122643, acc 0.953125, prec 0.0711479, recall 0.790663
2017-12-10T02:50:55.436001: step 3658, loss 0.357665, acc 0.921875, prec 0.0711733, recall 0.790741
2017-12-10T02:50:55.707639: step 3659, loss 0.215521, acc 0.9375, prec 0.0711842, recall 0.790781
2017-12-10T02:50:55.981885: step 3660, loss 0.329292, acc 0.890625, prec 0.0712072, recall 0.79086
2017-12-10T02:50:56.247959: step 3661, loss 0.383096, acc 0.890625, prec 0.0712145, recall 0.790899
2017-12-10T02:50:56.520661: step 3662, loss 0.456944, acc 0.859375, prec 0.0712037, recall 0.790899
2017-12-10T02:50:56.781292: step 3663, loss 0.105376, acc 0.953125, prec 0.0712315, recall 0.790977
2017-12-10T02:50:57.050656: step 3664, loss 0.667603, acc 0.828125, prec 0.0712339, recall 0.791017
2017-12-10T02:50:57.310952: step 3665, loss 0.487992, acc 0.921875, prec 0.0712279, recall 0.791017
2017-12-10T02:50:57.574599: step 3666, loss 0.664405, acc 0.859375, prec 0.0712642, recall 0.791134
2017-12-10T02:50:57.836269: step 3667, loss 0.355859, acc 0.84375, prec 0.0712522, recall 0.791134
2017-12-10T02:50:58.103779: step 3668, loss 0.0700857, acc 0.96875, prec 0.0712655, recall 0.791174
2017-12-10T02:50:58.368193: step 3669, loss 0.454241, acc 0.859375, prec 0.0712546, recall 0.791174
2017-12-10T02:50:58.638563: step 3670, loss 0.14802, acc 0.9375, prec 0.0712812, recall 0.791252
2017-12-10T02:50:58.905472: step 3671, loss 0.247052, acc 0.953125, prec 0.0712933, recall 0.791291
2017-12-10T02:50:59.168436: step 3672, loss 0.479475, acc 0.921875, prec 0.0713187, recall 0.79137
2017-12-10T02:50:59.428646: step 3673, loss 0.427591, acc 0.921875, prec 0.071344, recall 0.791448
2017-12-10T02:50:59.691531: step 3674, loss 0.438406, acc 0.9375, prec 0.0713392, recall 0.791448
2017-12-10T02:50:59.956878: step 3675, loss 0.103287, acc 0.9375, prec 0.0713344, recall 0.791448
2017-12-10T02:51:00.228408: step 3676, loss 0.119798, acc 0.96875, prec 0.0713477, recall 0.791487
2017-12-10T02:51:00.504816: step 3677, loss 0.32146, acc 0.9375, prec 0.0713742, recall 0.791565
2017-12-10T02:51:00.767870: step 3678, loss 0.139468, acc 0.984375, prec 0.071373, recall 0.791565
2017-12-10T02:51:01.035750: step 3679, loss 0.0749546, acc 0.984375, prec 0.0713875, recall 0.791604
2017-12-10T02:51:01.306128: step 3680, loss 0.237852, acc 0.96875, prec 0.0714322, recall 0.791721
2017-12-10T02:51:01.573120: step 3681, loss 0.12075, acc 0.96875, prec 0.0714298, recall 0.791721
2017-12-10T02:51:01.834211: step 3682, loss 0.0962398, acc 0.953125, prec 0.0714419, recall 0.79176
2017-12-10T02:51:02.103195: step 3683, loss 0.0902634, acc 0.96875, prec 0.0714551, recall 0.791799
2017-12-10T02:51:02.365265: step 3684, loss 0.117993, acc 0.984375, prec 0.0714853, recall 0.791877
2017-12-10T02:51:02.626133: step 3685, loss 0.170823, acc 0.984375, prec 0.0714998, recall 0.791916
2017-12-10T02:51:02.897203: step 3686, loss 0.161953, acc 0.9375, prec 0.0714949, recall 0.791916
2017-12-10T02:51:03.170262: step 3687, loss 0.0211925, acc 0.984375, prec 0.0715094, recall 0.791955
2017-12-10T02:51:03.440053: step 3688, loss 0.095943, acc 0.953125, prec 0.0715058, recall 0.791955
2017-12-10T02:51:03.701631: step 3689, loss 0.121046, acc 0.984375, prec 0.0715046, recall 0.791955
2017-12-10T02:51:03.964357: step 3690, loss 0.0317959, acc 0.984375, prec 0.0715034, recall 0.791955
2017-12-10T02:51:04.232068: step 3691, loss 0.0501341, acc 1, prec 0.0715347, recall 0.792033
2017-12-10T02:51:04.505527: step 3692, loss 0.0691929, acc 0.953125, prec 0.0715311, recall 0.792033
2017-12-10T02:51:04.768415: step 3693, loss 0.00224636, acc 1, prec 0.0715625, recall 0.792111
2017-12-10T02:51:05.029656: step 3694, loss 0.0264493, acc 0.984375, prec 0.071577, recall 0.79215
2017-12-10T02:51:05.291289: step 3695, loss 0.249954, acc 0.984375, prec 0.0716071, recall 0.792227
2017-12-10T02:51:05.565565: step 3696, loss 2.34365, acc 0.953125, prec 0.0716047, recall 0.792079
2017-12-10T02:51:05.833671: step 3697, loss 0.0845578, acc 0.96875, prec 0.0716023, recall 0.792079
2017-12-10T02:51:06.095955: step 3698, loss 0.134603, acc 0.953125, prec 0.0715986, recall 0.792079
2017-12-10T02:51:06.361266: step 3699, loss 0.115178, acc 0.984375, prec 0.0716131, recall 0.792118
2017-12-10T02:51:06.630580: step 3700, loss 0.184178, acc 0.953125, prec 0.0716252, recall 0.792157
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-3700

2017-12-10T02:51:07.905187: step 3701, loss 0.1839, acc 0.953125, prec 0.0716215, recall 0.792157
2017-12-10T02:51:08.168131: step 3702, loss 0.0819259, acc 0.953125, prec 0.0716179, recall 0.792157
2017-12-10T02:51:08.435015: step 3703, loss 0.10431, acc 0.953125, prec 0.0716143, recall 0.792157
2017-12-10T02:51:08.697773: step 3704, loss 0.0276548, acc 1, prec 0.0716143, recall 0.792157
2017-12-10T02:51:08.958984: step 3705, loss 0.47244, acc 0.84375, prec 0.0716179, recall 0.792196
2017-12-10T02:51:09.233072: step 3706, loss 0.0950911, acc 0.9375, prec 0.071613, recall 0.792196
2017-12-10T02:51:09.498339: step 3707, loss 0.192155, acc 0.953125, prec 0.0716251, recall 0.792234
2017-12-10T02:51:09.763668: step 3708, loss 0.0684146, acc 0.96875, prec 0.0716226, recall 0.792234
2017-12-10T02:51:10.036682: step 3709, loss 0.101161, acc 0.953125, prec 0.071619, recall 0.792234
2017-12-10T02:51:10.311302: step 3710, loss 0.232119, acc 0.9375, prec 0.0716299, recall 0.792273
2017-12-10T02:51:10.580520: step 3711, loss 0.127864, acc 0.96875, prec 0.0716431, recall 0.792312
2017-12-10T02:51:10.843621: step 3712, loss 0.178327, acc 0.953125, prec 0.0716708, recall 0.792389
2017-12-10T02:51:11.108527: step 3713, loss 0.25127, acc 0.90625, prec 0.0716792, recall 0.792428
2017-12-10T02:51:11.377124: step 3714, loss 0.086016, acc 0.953125, prec 0.0716756, recall 0.792428
2017-12-10T02:51:11.650793: step 3715, loss 0.0141249, acc 1, prec 0.0716756, recall 0.792428
2017-12-10T02:51:11.915735: step 3716, loss 0.567631, acc 0.96875, prec 0.0716888, recall 0.792467
2017-12-10T02:51:12.180098: step 3717, loss 0.120058, acc 0.96875, prec 0.0716864, recall 0.792467
2017-12-10T02:51:12.447969: step 3718, loss 0.0141391, acc 1, prec 0.0717021, recall 0.792506
2017-12-10T02:51:12.711813: step 3719, loss 0.171855, acc 1, prec 0.071749, recall 0.792622
2017-12-10T02:51:12.974008: step 3720, loss 0.0367666, acc 0.984375, prec 0.0717478, recall 0.792622
2017-12-10T02:51:13.245810: step 3721, loss 0.341772, acc 0.921875, prec 0.0717574, recall 0.79266
2017-12-10T02:51:13.508534: step 3722, loss 0.36426, acc 0.9375, prec 0.0717682, recall 0.792699
2017-12-10T02:51:13.768883: step 3723, loss 0.280158, acc 0.90625, prec 0.0717766, recall 0.792737
2017-12-10T02:51:14.030128: step 3724, loss 0.0303789, acc 1, prec 0.0717923, recall 0.792776
2017-12-10T02:51:14.289544: step 3725, loss 5.99633, acc 0.96875, prec 0.0717911, recall 0.792628
2017-12-10T02:51:14.555363: step 3726, loss 0.0565065, acc 0.96875, prec 0.0717886, recall 0.792628
2017-12-10T02:51:14.831488: step 3727, loss 0.1328, acc 0.9375, prec 0.0717838, recall 0.792628
2017-12-10T02:51:15.093589: step 3728, loss 0.273926, acc 0.953125, prec 0.0717802, recall 0.792628
2017-12-10T02:51:15.363099: step 3729, loss 0.335797, acc 0.890625, prec 0.0717873, recall 0.792667
2017-12-10T02:51:15.626544: step 3730, loss 0.171933, acc 0.9375, prec 0.0717982, recall 0.792706
2017-12-10T02:51:15.888549: step 3731, loss 0.245474, acc 0.921875, prec 0.0718077, recall 0.792744
2017-12-10T02:51:16.155868: step 3732, loss 0.389315, acc 0.859375, prec 0.0717969, recall 0.792744
2017-12-10T02:51:16.428698: step 3733, loss 0.344622, acc 0.828125, prec 0.0717836, recall 0.792744
2017-12-10T02:51:16.694368: step 3734, loss 0.425143, acc 0.921875, prec 0.0717931, recall 0.792783
2017-12-10T02:51:16.961731: step 3735, loss 0.454386, acc 0.859375, prec 0.0718292, recall 0.792898
2017-12-10T02:51:17.230454: step 3736, loss 0.207628, acc 0.9375, prec 0.0718243, recall 0.792898
2017-12-10T02:51:17.496439: step 3737, loss 0.532887, acc 0.890625, prec 0.0718471, recall 0.792975
2017-12-10T02:51:17.759445: step 3738, loss 0.396411, acc 0.890625, prec 0.0718386, recall 0.792975
2017-12-10T02:51:18.021229: step 3739, loss 0.774148, acc 0.90625, prec 0.071847, recall 0.793014
2017-12-10T02:51:18.289249: step 3740, loss 0.431723, acc 0.9375, prec 0.071889, recall 0.793129
2017-12-10T02:51:18.556383: step 3741, loss 0.299705, acc 0.921875, prec 0.071883, recall 0.793129
2017-12-10T02:51:18.818002: step 3742, loss 0.273279, acc 0.953125, prec 0.071895, recall 0.793167
2017-12-10T02:51:19.082151: step 3743, loss 0.779495, acc 0.859375, prec 0.0718841, recall 0.793167
2017-12-10T02:51:19.342651: step 3744, loss 0.155402, acc 0.9375, prec 0.0718949, recall 0.793206
2017-12-10T02:51:19.606967: step 3745, loss 0.0853043, acc 0.953125, prec 0.0719381, recall 0.793321
2017-12-10T02:51:19.874010: step 3746, loss 0.152427, acc 0.953125, prec 0.0719501, recall 0.793359
2017-12-10T02:51:20.138327: step 3747, loss 0.077329, acc 0.984375, prec 0.0719489, recall 0.793359
2017-12-10T02:51:20.402520: step 3748, loss 0.0415877, acc 0.984375, prec 0.0719789, recall 0.793436
2017-12-10T02:51:20.666247: step 3749, loss 0.154166, acc 0.96875, prec 0.0720077, recall 0.793513
2017-12-10T02:51:20.939957: step 3750, loss 0.102374, acc 0.96875, prec 0.0720052, recall 0.793513
2017-12-10T02:51:21.209631: step 3751, loss 0.0264969, acc 1, prec 0.0720209, recall 0.793551
2017-12-10T02:51:21.487803: step 3752, loss 0.018092, acc 1, prec 0.0720209, recall 0.793551
2017-12-10T02:51:21.754899: step 3753, loss 0.0901149, acc 0.9375, prec 0.0720472, recall 0.793627
2017-12-10T02:51:22.029938: step 3754, loss 0.129756, acc 0.96875, prec 0.0720448, recall 0.793627
2017-12-10T02:51:22.292634: step 3755, loss 0.0759332, acc 0.984375, prec 0.0720436, recall 0.793627
2017-12-10T02:51:22.561020: step 3756, loss 0.0124341, acc 1, prec 0.0720748, recall 0.793704
2017-12-10T02:51:22.826325: step 3757, loss 0.641969, acc 0.96875, prec 0.0721036, recall 0.79378
2017-12-10T02:51:23.095198: step 3758, loss 0.0726228, acc 0.953125, prec 0.0720999, recall 0.79378
2017-12-10T02:51:23.359816: step 3759, loss 0.00992023, acc 1, prec 0.0721155, recall 0.793818
2017-12-10T02:51:23.624715: step 3760, loss 0.0153793, acc 1, prec 0.0721468, recall 0.793895
2017-12-10T02:51:23.881555: step 3761, loss 0.130127, acc 0.96875, prec 0.0721443, recall 0.793895
2017-12-10T02:51:24.147793: step 3762, loss 0.0384238, acc 0.984375, prec 0.0721431, recall 0.793895
2017-12-10T02:51:24.410698: step 3763, loss 0.0959238, acc 0.984375, prec 0.0721575, recall 0.793933
2017-12-10T02:51:24.683126: step 3764, loss 6.67441, acc 0.953125, prec 0.0721551, recall 0.793786
2017-12-10T02:51:24.950994: step 3765, loss 0.0639262, acc 0.96875, prec 0.0721526, recall 0.793786
2017-12-10T02:51:25.217789: step 3766, loss 0.136356, acc 0.953125, prec 0.0721646, recall 0.793824
2017-12-10T02:51:25.480306: step 3767, loss 0.121809, acc 0.9375, prec 0.0721753, recall 0.793862
2017-12-10T02:51:25.741804: step 3768, loss 0.373642, acc 0.90625, prec 0.0721837, recall 0.7939
2017-12-10T02:51:26.007340: step 3769, loss 0.390923, acc 0.890625, prec 0.0721908, recall 0.793938
2017-12-10T02:51:26.269144: step 3770, loss 0.597079, acc 0.90625, prec 0.0721835, recall 0.793938
2017-12-10T02:51:26.545910: step 3771, loss 0.183565, acc 0.953125, prec 0.0721798, recall 0.793938
2017-12-10T02:51:26.808902: step 3772, loss 0.20741, acc 0.953125, prec 0.0722541, recall 0.794129
2017-12-10T02:51:27.074616: step 3773, loss 0.508689, acc 0.90625, prec 0.0722469, recall 0.794129
2017-12-10T02:51:27.343479: step 3774, loss 0.176235, acc 0.9375, prec 0.0722888, recall 0.794243
2017-12-10T02:51:27.608192: step 3775, loss 0.443096, acc 0.8125, prec 0.0722742, recall 0.794243
2017-12-10T02:51:27.872843: step 3776, loss 0.212288, acc 0.90625, prec 0.0722669, recall 0.794243
2017-12-10T02:51:28.139949: step 3777, loss 0.335571, acc 0.875, prec 0.0722728, recall 0.79428
2017-12-10T02:51:28.404697: step 3778, loss 0.580916, acc 0.859375, prec 0.0722619, recall 0.79428
2017-12-10T02:51:28.668067: step 3779, loss 0.354936, acc 0.859375, prec 0.0722665, recall 0.794318
2017-12-10T02:51:28.940119: step 3780, loss 0.318896, acc 0.890625, prec 0.0722736, recall 0.794356
2017-12-10T02:51:29.202092: step 3781, loss 0.382718, acc 0.953125, prec 0.0723011, recall 0.794432
2017-12-10T02:51:29.464355: step 3782, loss 0.384116, acc 0.859375, prec 0.0722902, recall 0.794432
2017-12-10T02:51:29.733274: step 3783, loss 0.778879, acc 0.921875, prec 0.0722997, recall 0.79447
2017-12-10T02:51:30.007102: step 3784, loss 0.0777461, acc 0.953125, prec 0.072296, recall 0.79447
2017-12-10T02:51:30.281441: step 3785, loss 0.161548, acc 0.96875, prec 0.0723558, recall 0.794621
2017-12-10T02:51:30.545436: step 3786, loss 0.1937, acc 0.9375, prec 0.0723665, recall 0.794659
2017-12-10T02:51:30.808839: step 3787, loss 0.186227, acc 0.9375, prec 0.0723772, recall 0.794697
2017-12-10T02:51:31.074967: step 3788, loss 0.0853017, acc 0.953125, prec 0.0723736, recall 0.794697
2017-12-10T02:51:31.340145: step 3789, loss 0.298939, acc 0.953125, prec 0.07237, recall 0.794697
2017-12-10T02:51:31.605084: step 3790, loss 0.0499118, acc 0.984375, prec 0.0723687, recall 0.794697
2017-12-10T02:51:31.867439: step 3791, loss 0.395438, acc 0.921875, prec 0.0723627, recall 0.794697
2017-12-10T02:51:32.140935: step 3792, loss 0.143277, acc 0.9375, prec 0.0723889, recall 0.794773
2017-12-10T02:51:32.403022: step 3793, loss 0.193477, acc 0.953125, prec 0.0723853, recall 0.794773
2017-12-10T02:51:32.667179: step 3794, loss 0.131247, acc 0.984375, prec 0.0724307, recall 0.794886
2017-12-10T02:51:32.940321: step 3795, loss 0.155546, acc 0.984375, prec 0.0724606, recall 0.794961
2017-12-10T02:51:33.207636: step 3796, loss 0.126316, acc 1, prec 0.0725072, recall 0.795074
2017-12-10T02:51:33.474888: step 3797, loss 0.102575, acc 0.96875, prec 0.0725048, recall 0.795074
2017-12-10T02:51:33.742827: step 3798, loss 0.188481, acc 1, prec 0.0725204, recall 0.795112
2017-12-10T02:51:34.010863: step 3799, loss 0.168972, acc 0.984375, prec 0.0725347, recall 0.79515
2017-12-10T02:51:34.277323: step 3800, loss 1.60039, acc 0.96875, prec 0.0725335, recall 0.795004
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-3800

2017-12-10T02:51:35.611167: step 3801, loss 0.109343, acc 0.984375, prec 0.0725633, recall 0.795079
2017-12-10T02:51:35.877157: step 3802, loss 0.207472, acc 0.953125, prec 0.0725597, recall 0.795079
2017-12-10T02:51:36.141662: step 3803, loss 0.614307, acc 0.953125, prec 0.072556, recall 0.795079
2017-12-10T02:51:36.405456: step 3804, loss 0.0180586, acc 1, prec 0.072556, recall 0.795079
2017-12-10T02:51:36.670488: step 3805, loss 0.10529, acc 0.953125, prec 0.0725679, recall 0.795117
2017-12-10T02:51:36.937438: step 3806, loss 0.176449, acc 0.953125, prec 0.0725643, recall 0.795117
2017-12-10T02:51:37.199154: step 3807, loss 0.123445, acc 1, prec 0.0725798, recall 0.795154
2017-12-10T02:51:37.464436: step 3808, loss 1.73388, acc 0.9375, prec 0.0726228, recall 0.795121
2017-12-10T02:51:37.728201: step 3809, loss 0.166078, acc 0.96875, prec 0.0726204, recall 0.795121
2017-12-10T02:51:37.990218: step 3810, loss 0.01888, acc 1, prec 0.0726204, recall 0.795121
2017-12-10T02:51:38.255239: step 3811, loss 0.095671, acc 0.96875, prec 0.0726179, recall 0.795121
2017-12-10T02:51:38.525441: step 3812, loss 0.233727, acc 0.890625, prec 0.0726405, recall 0.795196
2017-12-10T02:51:38.791694: step 3813, loss 0.222916, acc 0.921875, prec 0.0726499, recall 0.795234
2017-12-10T02:51:39.055595: step 3814, loss 0.183717, acc 0.921875, prec 0.0726749, recall 0.795309
2017-12-10T02:51:39.318821: step 3815, loss 0.262941, acc 0.9375, prec 0.07267, recall 0.795309
2017-12-10T02:51:39.583242: step 3816, loss 0.444078, acc 0.890625, prec 0.0726615, recall 0.795309
2017-12-10T02:51:39.854729: step 3817, loss 0.582208, acc 0.84375, prec 0.0726494, recall 0.795309
2017-12-10T02:51:40.123024: step 3818, loss 0.416475, acc 0.8125, prec 0.0726348, recall 0.795309
2017-12-10T02:51:40.388363: step 3819, loss 0.226584, acc 0.890625, prec 0.0726263, recall 0.795309
2017-12-10T02:51:40.656195: step 3820, loss 0.350854, acc 0.90625, prec 0.0726655, recall 0.795421
2017-12-10T02:51:40.932071: step 3821, loss 0.569639, acc 0.84375, prec 0.0726534, recall 0.795421
2017-12-10T02:51:41.195263: step 3822, loss 0.356605, acc 0.921875, prec 0.0726628, recall 0.795459
2017-12-10T02:51:41.469401: step 3823, loss 0.279959, acc 0.953125, prec 0.0726591, recall 0.795459
2017-12-10T02:51:41.740313: step 3824, loss 0.230527, acc 0.953125, prec 0.072671, recall 0.795496
2017-12-10T02:51:42.005233: step 3825, loss 0.125405, acc 0.953125, prec 0.0726984, recall 0.795571
2017-12-10T02:51:42.266650: step 3826, loss 0.3797, acc 0.9375, prec 0.072709, recall 0.795608
2017-12-10T02:51:42.531299: step 3827, loss 0.389939, acc 0.90625, prec 0.0727172, recall 0.795646
2017-12-10T02:51:42.793875: step 3828, loss 0.116135, acc 0.953125, prec 0.0727291, recall 0.795683
2017-12-10T02:51:43.058223: step 3829, loss 0.225481, acc 0.9375, prec 0.0727242, recall 0.795683
2017-12-10T02:51:43.322091: step 3830, loss 0.0879369, acc 0.9375, prec 0.0727194, recall 0.795683
2017-12-10T02:51:43.596409: step 3831, loss 0.133735, acc 0.953125, prec 0.0727157, recall 0.795683
2017-12-10T02:51:43.860644: step 3832, loss 0.145378, acc 0.96875, prec 0.0727288, recall 0.795721
2017-12-10T02:51:44.127846: step 3833, loss 0.481234, acc 0.90625, prec 0.0727215, recall 0.795721
2017-12-10T02:51:44.390511: step 3834, loss 0.197942, acc 0.921875, prec 0.0727309, recall 0.795758
2017-12-10T02:51:44.662802: step 3835, loss 0.116579, acc 0.96875, prec 0.0727285, recall 0.795758
2017-12-10T02:51:44.926916: step 3836, loss 0.843694, acc 0.96875, prec 0.0727416, recall 0.795795
2017-12-10T02:51:45.196903: step 3837, loss 1.19706, acc 0.96875, prec 0.0727403, recall 0.79565
2017-12-10T02:51:45.462196: step 3838, loss 0.13183, acc 0.953125, prec 0.0727522, recall 0.795687
2017-12-10T02:51:45.723865: step 3839, loss 0.0296576, acc 0.984375, prec 0.072782, recall 0.795762
2017-12-10T02:51:45.998704: step 3840, loss 0.126331, acc 0.96875, prec 0.072795, recall 0.795799
2017-12-10T02:51:46.262225: step 3841, loss 0.0764816, acc 0.984375, prec 0.0728248, recall 0.795874
2017-12-10T02:51:46.531992: step 3842, loss 0.039157, acc 0.984375, prec 0.0728545, recall 0.795948
2017-12-10T02:51:46.799847: step 3843, loss 0.255442, acc 0.9375, prec 0.0728652, recall 0.795985
2017-12-10T02:51:47.071005: step 3844, loss 0.168073, acc 0.9375, prec 0.0728913, recall 0.79606
2017-12-10T02:51:47.336486: step 3845, loss 0.242546, acc 0.953125, prec 0.0729186, recall 0.796134
2017-12-10T02:51:47.602719: step 3846, loss 0.441264, acc 0.90625, prec 0.0729268, recall 0.796171
2017-12-10T02:51:47.868301: step 3847, loss 0.172192, acc 0.9375, prec 0.0729219, recall 0.796171
2017-12-10T02:51:48.129400: step 3848, loss 0.21235, acc 0.890625, prec 0.0729134, recall 0.796171
2017-12-10T02:51:48.389320: step 3849, loss 0.230514, acc 0.9375, prec 0.0729085, recall 0.796171
2017-12-10T02:51:48.655057: step 3850, loss 0.286225, acc 0.921875, prec 0.0729024, recall 0.796171
2017-12-10T02:51:48.914873: step 3851, loss 0.115525, acc 0.96875, prec 0.0729, recall 0.796171
2017-12-10T02:51:49.186568: step 3852, loss 0.183642, acc 0.953125, prec 0.0729427, recall 0.796283
2017-12-10T02:51:49.451860: step 3853, loss 0.410473, acc 0.890625, prec 0.0729342, recall 0.796283
2017-12-10T02:51:49.713054: step 3854, loss 0.127362, acc 0.984375, prec 0.072933, recall 0.796283
2017-12-10T02:51:49.976339: step 3855, loss 0.125674, acc 0.984375, prec 0.0729318, recall 0.796283
2017-12-10T02:51:50.238953: step 3856, loss 0.393194, acc 0.90625, prec 0.0729245, recall 0.796283
2017-12-10T02:51:50.498334: step 3857, loss 0.343844, acc 0.96875, prec 0.0729375, recall 0.79632
2017-12-10T02:51:50.764178: step 3858, loss 0.212155, acc 0.953125, prec 0.0729493, recall 0.796357
2017-12-10T02:51:51.028064: step 3859, loss 0.0102993, acc 1, prec 0.0729648, recall 0.796394
2017-12-10T02:51:51.290870: step 3860, loss 0.260072, acc 0.921875, prec 0.0729587, recall 0.796394
2017-12-10T02:51:51.559075: step 3861, loss 0.0912438, acc 0.9375, prec 0.0729539, recall 0.796394
2017-12-10T02:51:51.827849: step 3862, loss 0.161411, acc 0.921875, prec 0.0729632, recall 0.796431
2017-12-10T02:51:52.099550: step 3863, loss 0.170887, acc 0.953125, prec 0.0729596, recall 0.796431
2017-12-10T02:51:52.365423: step 3864, loss 0.0743075, acc 0.984375, prec 0.0729738, recall 0.796468
2017-12-10T02:51:52.625450: step 3865, loss 0.105534, acc 0.984375, prec 0.0729881, recall 0.796505
2017-12-10T02:51:52.890689: step 3866, loss 0.126308, acc 0.96875, prec 0.0729856, recall 0.796505
2017-12-10T02:51:53.152781: step 3867, loss 0.184547, acc 0.9375, prec 0.0729962, recall 0.796542
2017-12-10T02:51:53.416088: step 3868, loss 0.110416, acc 0.96875, prec 0.0729938, recall 0.796542
2017-12-10T02:51:53.676194: step 3869, loss 0.0276223, acc 0.984375, prec 0.0729926, recall 0.796542
2017-12-10T02:51:53.949618: step 3870, loss 0.0150198, acc 1, prec 0.0730235, recall 0.796616
2017-12-10T02:51:54.214050: step 3871, loss 0.116587, acc 0.984375, prec 0.0730377, recall 0.796653
2017-12-10T02:51:54.489559: step 3872, loss 0.012015, acc 1, prec 0.0730377, recall 0.796653
2017-12-10T02:51:54.755639: step 3873, loss 0.0416137, acc 0.984375, prec 0.073052, recall 0.79669
2017-12-10T02:51:55.020734: step 3874, loss 0.0151272, acc 1, prec 0.0730829, recall 0.796764
2017-12-10T02:51:55.285527: step 3875, loss 0.0671641, acc 0.96875, prec 0.0730805, recall 0.796764
2017-12-10T02:51:55.551731: step 3876, loss 0.179078, acc 0.953125, prec 0.0730768, recall 0.796764
2017-12-10T02:51:55.822630: step 3877, loss 0.0276784, acc 0.984375, prec 0.073091, recall 0.796801
2017-12-10T02:51:56.091270: step 3878, loss 0.198411, acc 0.984375, prec 0.0730898, recall 0.796801
2017-12-10T02:51:56.366492: step 3879, loss 2.16696, acc 0.984375, prec 0.0730898, recall 0.796656
2017-12-10T02:51:56.648239: step 3880, loss 2.19372, acc 0.984375, prec 0.0731053, recall 0.796549
2017-12-10T02:51:56.912738: step 3881, loss 6.26265, acc 0.96875, prec 0.073104, recall 0.796404
2017-12-10T02:51:57.182977: step 3882, loss 0.164265, acc 0.96875, prec 0.0731016, recall 0.796404
2017-12-10T02:51:57.452342: step 3883, loss 0.0789356, acc 0.96875, prec 0.0731146, recall 0.796441
2017-12-10T02:51:57.710739: step 3884, loss 0.322559, acc 0.90625, prec 0.0731073, recall 0.796441
2017-12-10T02:51:57.975348: step 3885, loss 0.28437, acc 0.90625, prec 0.0731, recall 0.796441
2017-12-10T02:51:58.236933: step 3886, loss 0.473088, acc 0.890625, prec 0.0730915, recall 0.796441
2017-12-10T02:51:58.500208: step 3887, loss 0.948951, acc 0.78125, prec 0.0731053, recall 0.796515
2017-12-10T02:51:58.763139: step 3888, loss 1.07176, acc 0.6875, prec 0.073081, recall 0.796515
2017-12-10T02:51:59.021151: step 3889, loss 0.928805, acc 0.8125, prec 0.0730664, recall 0.796515
2017-12-10T02:51:59.282107: step 3890, loss 1.13121, acc 0.765625, prec 0.0730944, recall 0.796626
2017-12-10T02:51:59.546639: step 3891, loss 0.788765, acc 0.828125, prec 0.0731119, recall 0.796699
2017-12-10T02:51:59.817825: step 3892, loss 0.778187, acc 0.8125, prec 0.0731127, recall 0.796736
2017-12-10T02:52:00.080690: step 3893, loss 0.700238, acc 0.78125, prec 0.0730957, recall 0.796736
2017-12-10T02:52:00.353913: step 3894, loss 0.875826, acc 0.796875, prec 0.0731261, recall 0.796847
2017-12-10T02:52:00.625703: step 3895, loss 0.834246, acc 0.859375, prec 0.0731614, recall 0.796957
2017-12-10T02:52:00.887976: step 3896, loss 1.16489, acc 0.796875, prec 0.073161, recall 0.796994
2017-12-10T02:52:01.153869: step 3897, loss 0.592587, acc 0.84375, prec 0.0731488, recall 0.796994
2017-12-10T02:52:01.422455: step 3898, loss 0.984065, acc 0.765625, prec 0.0731306, recall 0.796994
2017-12-10T02:52:01.680016: step 3899, loss 0.406585, acc 0.890625, prec 0.0731529, recall 0.797067
2017-12-10T02:52:01.940934: step 3900, loss 0.30422, acc 0.90625, prec 0.0731456, recall 0.797067

Evaluation:
2017-12-10T02:52:09.622390: step 3900, loss 2.18153, acc 0.899991, prec 0.0736779, recall 0.794304

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-3900

2017-12-10T02:52:10.982763: step 3901, loss 0.329333, acc 0.9375, prec 0.0736731, recall 0.794304
2017-12-10T02:52:11.246793: step 3902, loss 0.31698, acc 0.921875, prec 0.073667, recall 0.794304
2017-12-10T02:52:11.524281: step 3903, loss 0.276998, acc 0.9375, prec 0.0736773, recall 0.79434
2017-12-10T02:52:11.789451: step 3904, loss 0.238782, acc 0.953125, prec 0.0736737, recall 0.79434
2017-12-10T02:52:12.053770: step 3905, loss 0.209421, acc 0.9375, prec 0.0736991, recall 0.794412
2017-12-10T02:52:12.316755: step 3906, loss 0.295277, acc 0.953125, prec 0.0737106, recall 0.794448
2017-12-10T02:52:12.583231: step 3907, loss 0.019613, acc 1, prec 0.0737257, recall 0.794484
2017-12-10T02:52:12.850762: step 3908, loss 1.18932, acc 0.9375, prec 0.073736, recall 0.794521
2017-12-10T02:52:13.119825: step 3909, loss 0.139921, acc 0.9375, prec 0.0737312, recall 0.794521
2017-12-10T02:52:13.386632: step 3910, loss 0.129411, acc 0.96875, prec 0.0737288, recall 0.794521
2017-12-10T02:52:13.647589: step 3911, loss 8.59688, acc 0.921875, prec 0.073724, recall 0.794381
2017-12-10T02:52:13.924954: step 3912, loss 0.530978, acc 0.9375, prec 0.0737192, recall 0.794381
2017-12-10T02:52:14.186788: step 3913, loss 0.384384, acc 0.9375, prec 0.0737295, recall 0.794417
2017-12-10T02:52:14.447001: step 3914, loss 0.148662, acc 0.96875, prec 0.0737422, recall 0.794453
2017-12-10T02:52:14.710095: step 3915, loss 0.192797, acc 0.9375, prec 0.0737374, recall 0.794453
2017-12-10T02:52:14.976606: step 3916, loss 0.110374, acc 0.96875, prec 0.0737652, recall 0.794525
2017-12-10T02:52:15.239384: step 3917, loss 0.181949, acc 0.921875, prec 0.0737591, recall 0.794525
2017-12-10T02:52:15.501931: step 3918, loss 0.274388, acc 0.953125, prec 0.0737555, recall 0.794525
2017-12-10T02:52:15.769855: step 3919, loss 0.125679, acc 0.953125, prec 0.0737519, recall 0.794525
2017-12-10T02:52:16.033724: step 3920, loss 0.206457, acc 0.953125, prec 0.0737634, recall 0.794561
2017-12-10T02:52:16.294091: step 3921, loss 0.476586, acc 0.90625, prec 0.0737864, recall 0.794633
2017-12-10T02:52:16.556308: step 3922, loss 0.201914, acc 0.921875, prec 0.0737955, recall 0.794669
2017-12-10T02:52:16.816835: step 3923, loss 0.189283, acc 0.921875, prec 0.0738045, recall 0.794705
2017-12-10T02:52:17.089575: step 3924, loss 0.122842, acc 0.9375, prec 0.0738299, recall 0.794777
2017-12-10T02:52:17.350338: step 3925, loss 0.149907, acc 0.953125, prec 0.0738715, recall 0.794885
2017-12-10T02:52:17.612711: step 3926, loss 0.126418, acc 0.96875, prec 0.0738842, recall 0.794921
2017-12-10T02:52:17.874531: step 3927, loss 0.142815, acc 0.953125, prec 0.0738956, recall 0.794957
2017-12-10T02:52:18.134550: step 3928, loss 0.462789, acc 0.890625, prec 0.0739324, recall 0.795065
2017-12-10T02:52:18.398567: step 3929, loss 0.242563, acc 0.9375, prec 0.0739276, recall 0.795065
2017-12-10T02:52:18.667517: step 3930, loss 0.219104, acc 0.953125, prec 0.0739391, recall 0.795101
2017-12-10T02:52:18.931871: step 3931, loss 0.652445, acc 0.921875, prec 0.0739632, recall 0.795172
2017-12-10T02:52:19.204050: step 3932, loss 0.167734, acc 0.9375, prec 0.0739584, recall 0.795172
2017-12-10T02:52:19.470416: step 3933, loss 0.126658, acc 0.96875, prec 0.073971, recall 0.795208
2017-12-10T02:52:19.736226: step 3934, loss 0.77907, acc 0.953125, prec 0.0739825, recall 0.795244
2017-12-10T02:52:20.001332: step 3935, loss 0.224285, acc 0.953125, prec 0.073994, recall 0.79528
2017-12-10T02:52:20.262278: step 3936, loss 0.0353479, acc 0.984375, prec 0.0740078, recall 0.795316
2017-12-10T02:52:20.527606: step 3937, loss 0.160538, acc 0.9375, prec 0.0740181, recall 0.795351
2017-12-10T02:52:20.787658: step 3938, loss 0.0920875, acc 0.953125, prec 0.0740295, recall 0.795387
2017-12-10T02:52:21.052185: step 3939, loss 0.238601, acc 0.984375, prec 0.0740434, recall 0.795423
2017-12-10T02:52:21.322405: step 3940, loss 0.372332, acc 0.921875, prec 0.0740373, recall 0.795423
2017-12-10T02:52:21.583971: step 3941, loss 0.394688, acc 0.9375, prec 0.0740476, recall 0.795458
2017-12-10T02:52:21.846743: step 3942, loss 0.686894, acc 0.875, prec 0.0740379, recall 0.795458
2017-12-10T02:52:22.109632: step 3943, loss 0.198118, acc 0.953125, prec 0.0740343, recall 0.795458
2017-12-10T02:52:22.374602: step 3944, loss 0.34083, acc 0.9375, prec 0.0740897, recall 0.795601
2017-12-10T02:52:22.648900: step 3945, loss 0.481514, acc 0.890625, prec 0.0740813, recall 0.795601
2017-12-10T02:52:22.912720: step 3946, loss 2.10141, acc 0.953125, prec 0.0740789, recall 0.795462
2017-12-10T02:52:23.190489: step 3947, loss 0.11262, acc 0.96875, prec 0.0741066, recall 0.795534
2017-12-10T02:52:23.459110: step 3948, loss 0.324515, acc 0.9375, prec 0.0741469, recall 0.795641
2017-12-10T02:52:23.732956: step 3949, loss 0.349721, acc 0.90625, prec 0.0741848, recall 0.795748
2017-12-10T02:52:23.998394: step 3950, loss 0.323912, acc 0.921875, prec 0.0741788, recall 0.795748
2017-12-10T02:52:24.263357: step 3951, loss 0.205139, acc 0.921875, prec 0.0741727, recall 0.795748
2017-12-10T02:52:24.525373: step 3952, loss 0.163291, acc 0.953125, prec 0.0741691, recall 0.795748
2017-12-10T02:52:24.793655: step 3953, loss 0.119584, acc 0.953125, prec 0.0741806, recall 0.795783
2017-12-10T02:52:25.057153: step 3954, loss 0.5195, acc 0.90625, prec 0.0742034, recall 0.795854
2017-12-10T02:52:25.316512: step 3955, loss 0.0960708, acc 0.96875, prec 0.074201, recall 0.795854
2017-12-10T02:52:25.582208: step 3956, loss 0.370667, acc 0.921875, prec 0.0742401, recall 0.795961
2017-12-10T02:52:25.841760: step 3957, loss 0.378974, acc 0.890625, prec 0.0742467, recall 0.795997
2017-12-10T02:52:26.108700: step 3958, loss 0.424372, acc 0.921875, prec 0.0742406, recall 0.795997
2017-12-10T02:52:26.378706: step 3959, loss 0.294258, acc 0.9375, prec 0.0742358, recall 0.795997
2017-12-10T02:52:26.654418: step 3960, loss 0.159494, acc 0.9375, prec 0.074231, recall 0.795997
2017-12-10T02:52:26.923498: step 3961, loss 0.20915, acc 0.9375, prec 0.0742262, recall 0.795997
2017-12-10T02:52:27.198532: step 3962, loss 0.108516, acc 0.953125, prec 0.0742376, recall 0.796032
2017-12-10T02:52:27.466669: step 3963, loss 0.185298, acc 0.953125, prec 0.074234, recall 0.796032
2017-12-10T02:52:27.736469: step 3964, loss 0.130257, acc 0.96875, prec 0.0742466, recall 0.796068
2017-12-10T02:52:28.000484: step 3965, loss 0.320855, acc 0.890625, prec 0.0742532, recall 0.796103
2017-12-10T02:52:28.265514: step 3966, loss 0.156194, acc 0.984375, prec 0.074312, recall 0.796245
2017-12-10T02:52:28.529017: step 3967, loss 0.0848184, acc 0.953125, prec 0.0743234, recall 0.79628
2017-12-10T02:52:28.791879: step 3968, loss 0.0615707, acc 0.953125, prec 0.0743198, recall 0.79628
2017-12-10T02:52:29.062205: step 3969, loss 0.156187, acc 0.96875, prec 0.0743475, recall 0.796351
2017-12-10T02:52:29.327670: step 3970, loss 0.179555, acc 0.953125, prec 0.0743589, recall 0.796386
2017-12-10T02:52:29.595523: step 3971, loss 0.0819834, acc 0.96875, prec 0.0743714, recall 0.796422
2017-12-10T02:52:29.869378: step 3972, loss 0.0748526, acc 0.96875, prec 0.074369, recall 0.796422
2017-12-10T02:52:30.132325: step 3973, loss 0.120193, acc 0.96875, prec 0.0743967, recall 0.796492
2017-12-10T02:52:30.398687: step 3974, loss 0.313767, acc 0.984375, prec 0.0744255, recall 0.796563
2017-12-10T02:52:30.667196: step 3975, loss 0.0353675, acc 1, prec 0.0744555, recall 0.796634
2017-12-10T02:52:30.893550: step 3976, loss 1.33679, acc 0.960784, prec 0.0744681, recall 0.796669
2017-12-10T02:52:31.167045: step 3977, loss 0.035563, acc 0.984375, prec 0.0744669, recall 0.796669
2017-12-10T02:52:31.430370: step 3978, loss 0.0179041, acc 1, prec 0.0744669, recall 0.796669
2017-12-10T02:52:31.691590: step 3979, loss 0.096483, acc 0.921875, prec 0.0744608, recall 0.796669
2017-12-10T02:52:31.956127: step 3980, loss 0.281126, acc 0.9375, prec 0.074471, recall 0.796704
2017-12-10T02:52:32.228144: step 3981, loss 0.202517, acc 0.9375, prec 0.0745262, recall 0.796845
2017-12-10T02:52:32.492072: step 3982, loss 0.0759501, acc 0.96875, prec 0.0745238, recall 0.796845
2017-12-10T02:52:32.752515: step 3983, loss 0.0590528, acc 0.984375, prec 0.0745226, recall 0.796845
2017-12-10T02:52:33.017722: step 3984, loss 0.132157, acc 0.9375, prec 0.0745328, recall 0.79688
2017-12-10T02:52:33.281057: step 3985, loss 0.527718, acc 0.90625, prec 0.0745405, recall 0.796916
2017-12-10T02:52:33.549003: step 3986, loss 0.0708815, acc 0.96875, prec 0.0745381, recall 0.796916
2017-12-10T02:52:33.814625: step 3987, loss 0.125947, acc 0.96875, prec 0.0745657, recall 0.796986
2017-12-10T02:52:34.086904: step 3988, loss 0.279857, acc 0.9375, prec 0.0745608, recall 0.796986
2017-12-10T02:52:34.346140: step 3989, loss 0.0347464, acc 1, prec 0.0745908, recall 0.797056
2017-12-10T02:52:34.613519: step 3990, loss 0.271121, acc 0.921875, prec 0.0745848, recall 0.797056
2017-12-10T02:52:34.875671: step 3991, loss 0.194302, acc 0.9375, prec 0.0745949, recall 0.797091
2017-12-10T02:52:35.143442: step 3992, loss 0.0905157, acc 0.984375, prec 0.0745937, recall 0.797091
2017-12-10T02:52:35.411491: step 3993, loss 0.430552, acc 0.921875, prec 0.0745877, recall 0.797091
2017-12-10T02:52:35.673353: step 3994, loss 0.0373032, acc 0.984375, prec 0.0746015, recall 0.797127
2017-12-10T02:52:35.948859: step 3995, loss 0.0888189, acc 0.953125, prec 0.0745979, recall 0.797127
2017-12-10T02:52:36.211980: step 3996, loss 0.0482501, acc 0.96875, prec 0.0746104, recall 0.797162
2017-12-10T02:52:36.474891: step 3997, loss 0.199855, acc 0.9375, prec 0.0746655, recall 0.797302
2017-12-10T02:52:36.745707: step 3998, loss 0.124754, acc 0.984375, prec 0.0746643, recall 0.797302
2017-12-10T02:52:37.008453: step 3999, loss 0.0506483, acc 0.96875, prec 0.0746769, recall 0.797337
2017-12-10T02:52:37.277336: step 4000, loss 0.105775, acc 0.9375, prec 0.0746721, recall 0.797337
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-4000

2017-12-10T02:52:38.630296: step 4001, loss 0.154456, acc 0.984375, prec 0.0746709, recall 0.797337
2017-12-10T02:52:38.898112: step 4002, loss 0.0879401, acc 0.96875, prec 0.0746684, recall 0.797337
2017-12-10T02:52:39.169736: step 4003, loss 0.114124, acc 0.953125, prec 0.0746798, recall 0.797372
2017-12-10T02:52:39.431644: step 4004, loss 0.0723, acc 0.953125, prec 0.0746762, recall 0.797372
2017-12-10T02:52:39.694839: step 4005, loss 0.0141141, acc 0.984375, prec 0.0747049, recall 0.797442
2017-12-10T02:52:39.956213: step 4006, loss 0.0904268, acc 1, prec 0.0747349, recall 0.797512
2017-12-10T02:52:40.224489: step 4007, loss 0.00604583, acc 1, prec 0.0747648, recall 0.797582
2017-12-10T02:52:40.494173: step 4008, loss 0.827227, acc 1, prec 0.0747798, recall 0.797617
2017-12-10T02:52:40.760055: step 4009, loss 0.114587, acc 1, prec 0.0748098, recall 0.797687
2017-12-10T02:52:41.026078: step 4010, loss 0.169673, acc 0.96875, prec 0.0748223, recall 0.797722
2017-12-10T02:52:41.293993: step 4011, loss 0.109449, acc 0.96875, prec 0.0748199, recall 0.797722
2017-12-10T02:52:41.576141: step 4012, loss 0.0937769, acc 0.953125, prec 0.0748612, recall 0.797826
2017-12-10T02:52:41.838285: step 4013, loss 0.0852617, acc 0.953125, prec 0.0748576, recall 0.797826
2017-12-10T02:52:42.102224: step 4014, loss 0.121773, acc 0.96875, prec 0.0748851, recall 0.797896
2017-12-10T02:52:42.377071: step 4015, loss 0.265182, acc 0.9375, prec 0.0748802, recall 0.797896
2017-12-10T02:52:42.645269: step 4016, loss 0.0497625, acc 0.984375, prec 0.074894, recall 0.797931
2017-12-10T02:52:42.909137: step 4017, loss 0.0490988, acc 0.984375, prec 0.0749227, recall 0.798001
2017-12-10T02:52:43.172573: step 4018, loss 0.114978, acc 0.953125, prec 0.0749191, recall 0.798001
2017-12-10T02:52:43.437234: step 4019, loss 0.153551, acc 0.96875, prec 0.0749167, recall 0.798001
2017-12-10T02:52:43.707796: step 4020, loss 0.22638, acc 0.96875, prec 0.0749292, recall 0.798036
2017-12-10T02:52:43.969301: step 4021, loss 0.0561673, acc 0.984375, prec 0.074928, recall 0.798036
2017-12-10T02:52:44.235408: step 4022, loss 0.0188399, acc 0.984375, prec 0.0749268, recall 0.798036
2017-12-10T02:52:44.507499: step 4023, loss 0.0636287, acc 1, prec 0.0749567, recall 0.798105
2017-12-10T02:52:44.772257: step 4024, loss 0.00981315, acc 1, prec 0.0749717, recall 0.79814
2017-12-10T02:52:45.038465: step 4025, loss 0.249123, acc 0.96875, prec 0.0749992, recall 0.798209
2017-12-10T02:52:45.300907: step 4026, loss 0.0132787, acc 0.984375, prec 0.074998, recall 0.798209
2017-12-10T02:52:45.566613: step 4027, loss 0.260009, acc 0.96875, prec 0.0750255, recall 0.798279
2017-12-10T02:52:45.831072: step 4028, loss 0.0322047, acc 1, prec 0.0750404, recall 0.798314
2017-12-10T02:52:46.098014: step 4029, loss 0.0265056, acc 1, prec 0.0750704, recall 0.798383
2017-12-10T02:52:46.366212: step 4030, loss 0.00537903, acc 1, prec 0.0750853, recall 0.798418
2017-12-10T02:52:46.629883: step 4031, loss 0.0176279, acc 1, prec 0.0751003, recall 0.798452
2017-12-10T02:52:46.899110: step 4032, loss 0.177492, acc 0.96875, prec 0.0750979, recall 0.798452
2017-12-10T02:52:47.163270: step 4033, loss 0.0505071, acc 0.984375, prec 0.0750966, recall 0.798452
2017-12-10T02:52:47.432519: step 4034, loss 0.0490708, acc 0.984375, prec 0.0751253, recall 0.798522
2017-12-10T02:52:47.698445: step 4035, loss 0.528463, acc 0.9375, prec 0.0751354, recall 0.798556
2017-12-10T02:52:47.967767: step 4036, loss 0.0427512, acc 0.984375, prec 0.0751342, recall 0.798556
2017-12-10T02:52:48.232240: step 4037, loss 0.0710545, acc 0.96875, prec 0.0751318, recall 0.798556
2017-12-10T02:52:48.502415: step 4038, loss 0.141618, acc 0.984375, prec 0.0751605, recall 0.798625
2017-12-10T02:52:48.770673: step 4039, loss 0.243176, acc 0.9375, prec 0.0751556, recall 0.798625
2017-12-10T02:52:49.043683: step 4040, loss 0.216542, acc 0.96875, prec 0.0751682, recall 0.79866
2017-12-10T02:52:49.312221: step 4041, loss 0.0078361, acc 1, prec 0.0751682, recall 0.79866
2017-12-10T02:52:49.572133: step 4042, loss 0.0116317, acc 1, prec 0.0751682, recall 0.79866
2017-12-10T02:52:49.834386: step 4043, loss 0.00192492, acc 1, prec 0.0751682, recall 0.79866
2017-12-10T02:52:50.095462: step 4044, loss 0.00552422, acc 1, prec 0.0751981, recall 0.798729
2017-12-10T02:52:50.362038: step 4045, loss 0.0278285, acc 0.984375, prec 0.0751968, recall 0.798729
2017-12-10T02:52:50.624269: step 4046, loss 0.118631, acc 0.96875, prec 0.0752243, recall 0.798798
2017-12-10T02:52:50.894268: step 4047, loss 0.0809948, acc 0.96875, prec 0.0752219, recall 0.798798
2017-12-10T02:52:51.158200: step 4048, loss 0.0148285, acc 1, prec 0.0752219, recall 0.798798
2017-12-10T02:52:51.427448: step 4049, loss 2.19697, acc 0.953125, prec 0.0752494, recall 0.79873
2017-12-10T02:52:51.693328: step 4050, loss 0.0984525, acc 0.96875, prec 0.0752469, recall 0.79873
2017-12-10T02:52:51.958821: step 4051, loss 0.0174457, acc 0.984375, prec 0.0752457, recall 0.79873
2017-12-10T02:52:52.224873: step 4052, loss 0.118203, acc 0.96875, prec 0.0752433, recall 0.79873
2017-12-10T02:52:52.488659: step 4053, loss 0.02587, acc 0.984375, prec 0.075257, recall 0.798765
2017-12-10T02:52:52.760573: step 4054, loss 0.0256418, acc 0.984375, prec 0.0752707, recall 0.798799
2017-12-10T02:52:53.020939: step 4055, loss 0.220988, acc 0.921875, prec 0.0752646, recall 0.798799
2017-12-10T02:52:53.287132: step 4056, loss 0.138938, acc 0.96875, prec 0.0752622, recall 0.798799
2017-12-10T02:52:53.551965: step 4057, loss 0.138609, acc 0.9375, prec 0.0752573, recall 0.798799
2017-12-10T02:52:53.819634: step 4058, loss 0.40565, acc 0.953125, prec 0.0752686, recall 0.798834
2017-12-10T02:52:54.088094: step 4059, loss 0.242887, acc 0.9375, prec 0.0753086, recall 0.798937
2017-12-10T02:52:54.352662: step 4060, loss 0.276773, acc 0.921875, prec 0.0753324, recall 0.799006
2017-12-10T02:52:54.614529: step 4061, loss 0.330661, acc 0.890625, prec 0.0753537, recall 0.799075
2017-12-10T02:52:54.877992: step 4062, loss 0.154529, acc 0.9375, prec 0.0753489, recall 0.799075
2017-12-10T02:52:55.143854: step 4063, loss 0.273066, acc 0.859375, prec 0.0753529, recall 0.799109
2017-12-10T02:52:55.405815: step 4064, loss 0.123081, acc 0.9375, prec 0.075348, recall 0.799109
2017-12-10T02:52:55.673207: step 4065, loss 0.636557, acc 0.890625, prec 0.0753544, recall 0.799144
2017-12-10T02:52:55.942426: step 4066, loss 0.246266, acc 0.9375, prec 0.0753645, recall 0.799178
2017-12-10T02:52:56.214350: step 4067, loss 0.128553, acc 0.96875, prec 0.075377, recall 0.799213
2017-12-10T02:52:56.480541: step 4068, loss 0.144572, acc 0.953125, prec 0.0753733, recall 0.799213
2017-12-10T02:52:56.744545: step 4069, loss 0.0911809, acc 0.96875, prec 0.0753858, recall 0.799247
2017-12-10T02:52:57.012780: step 4070, loss 0.204231, acc 0.953125, prec 0.0753971, recall 0.799281
2017-12-10T02:52:57.279282: step 4071, loss 0.179549, acc 0.921875, prec 0.0754059, recall 0.799316
2017-12-10T02:52:57.543919: step 4072, loss 0.0913128, acc 0.96875, prec 0.0754184, recall 0.79935
2017-12-10T02:52:57.809952: step 4073, loss 0.135989, acc 0.984375, prec 0.0754321, recall 0.799384
2017-12-10T02:52:58.070696: step 4074, loss 0.167992, acc 0.9375, prec 0.0754571, recall 0.799453
2017-12-10T02:52:58.346553: step 4075, loss 0.152172, acc 0.953125, prec 0.0754982, recall 0.799556
2017-12-10T02:52:58.613341: step 4076, loss 0.0310569, acc 0.984375, prec 0.0755119, recall 0.79959
2017-12-10T02:52:58.881690: step 4077, loss 0.536201, acc 0.9375, prec 0.0755368, recall 0.799658
2017-12-10T02:52:59.147158: step 4078, loss 0.140017, acc 0.96875, prec 0.0755493, recall 0.799693
2017-12-10T02:52:59.415358: step 4079, loss 0.0590634, acc 0.96875, prec 0.0755618, recall 0.799727
2017-12-10T02:52:59.683650: step 4080, loss 0.0251723, acc 0.984375, prec 0.0755606, recall 0.799727
2017-12-10T02:52:59.952818: step 4081, loss 0.115062, acc 0.984375, prec 0.0755594, recall 0.799727
2017-12-10T02:53:00.223149: step 4082, loss 0.0186212, acc 1, prec 0.0755892, recall 0.799795
2017-12-10T02:53:00.491374: step 4083, loss 0.29919, acc 0.9375, prec 0.0755992, recall 0.799829
2017-12-10T02:53:00.759565: step 4084, loss 0.0130585, acc 1, prec 0.0756141, recall 0.799864
2017-12-10T02:53:01.024004: step 4085, loss 0.167791, acc 0.96875, prec 0.0756117, recall 0.799864
2017-12-10T02:53:01.288418: step 4086, loss 3.93397, acc 0.96875, prec 0.0756403, recall 0.799795
2017-12-10T02:53:01.559063: step 4087, loss 0.166099, acc 0.96875, prec 0.0756527, recall 0.799829
2017-12-10T02:53:01.821338: step 4088, loss 0.0982276, acc 0.953125, prec 0.0756491, recall 0.799829
2017-12-10T02:53:02.091186: step 4089, loss 0.0279348, acc 0.984375, prec 0.0756479, recall 0.799829
2017-12-10T02:53:02.360496: step 4090, loss 0.129555, acc 0.9375, prec 0.0756728, recall 0.799898
2017-12-10T02:53:02.632063: step 4091, loss 0.22024, acc 0.984375, prec 0.0757163, recall 0.8
2017-12-10T02:53:02.900254: step 4092, loss 0.167451, acc 0.953125, prec 0.0757126, recall 0.8
2017-12-10T02:53:03.161414: step 4093, loss 0.420048, acc 0.9375, prec 0.0757524, recall 0.800102
2017-12-10T02:53:03.432718: step 4094, loss 0.282611, acc 0.9375, prec 0.0757625, recall 0.800136
2017-12-10T02:53:03.696656: step 4095, loss 0.067936, acc 0.96875, prec 0.07576, recall 0.800136
2017-12-10T02:53:03.957425: step 4096, loss 0.096274, acc 0.96875, prec 0.0757874, recall 0.800204
2017-12-10T02:53:04.225433: step 4097, loss 0.102693, acc 0.984375, prec 0.075801, recall 0.800238
2017-12-10T02:53:04.487501: step 4098, loss 0.378218, acc 0.921875, prec 0.0758098, recall 0.800272
2017-12-10T02:53:04.756278: step 4099, loss 0.433978, acc 0.890625, prec 0.0758311, recall 0.80034
2017-12-10T02:53:05.016953: step 4100, loss 0.107921, acc 0.921875, prec 0.0758547, recall 0.800408
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-4100

2017-12-10T02:53:06.292835: step 4101, loss 0.23371, acc 0.9375, prec 0.0758647, recall 0.800442
2017-12-10T02:53:06.556268: step 4102, loss 0.170828, acc 0.953125, prec 0.0758908, recall 0.80051
2017-12-10T02:53:06.822704: step 4103, loss 0.157664, acc 0.90625, prec 0.0758984, recall 0.800544
2017-12-10T02:53:07.093132: step 4104, loss 0.203768, acc 0.96875, prec 0.075896, recall 0.800544
2017-12-10T02:53:07.363040: step 4105, loss 0.234822, acc 0.9375, prec 0.0759059, recall 0.800578
2017-12-10T02:53:07.628423: step 4106, loss 0.187836, acc 0.921875, prec 0.0758998, recall 0.800578
2017-12-10T02:53:07.897810: step 4107, loss 0.347651, acc 0.9375, prec 0.0759098, recall 0.800611
2017-12-10T02:53:08.158556: step 4108, loss 0.102295, acc 0.953125, prec 0.075921, recall 0.800645
2017-12-10T02:53:08.419736: step 4109, loss 0.314972, acc 0.921875, prec 0.0759149, recall 0.800645
2017-12-10T02:53:08.681594: step 4110, loss 0.0150011, acc 1, prec 0.0759149, recall 0.800645
2017-12-10T02:53:08.941859: step 4111, loss 0.0414173, acc 0.96875, prec 0.0759571, recall 0.800747
2017-12-10T02:53:09.210385: step 4112, loss 0.0283752, acc 0.984375, prec 0.0759708, recall 0.800781
2017-12-10T02:53:09.478770: step 4113, loss 0.0127632, acc 1, prec 0.0759708, recall 0.800781
2017-12-10T02:53:09.748549: step 4114, loss 0.0787489, acc 0.96875, prec 0.0759683, recall 0.800781
2017-12-10T02:53:10.013425: step 4115, loss 0.11549, acc 0.96875, prec 0.0759659, recall 0.800781
2017-12-10T02:53:10.274314: step 4116, loss 0.0186425, acc 0.984375, prec 0.0759647, recall 0.800781
2017-12-10T02:53:10.536509: step 4117, loss 0.0286368, acc 1, prec 0.0759647, recall 0.800781
2017-12-10T02:53:10.795673: step 4118, loss 0.0059061, acc 1, prec 0.0759647, recall 0.800781
2017-12-10T02:53:11.062881: step 4119, loss 0.180461, acc 0.96875, prec 0.0759771, recall 0.800814
2017-12-10T02:53:11.338203: step 4120, loss 0.0902921, acc 0.984375, prec 0.0759759, recall 0.800814
2017-12-10T02:53:11.618583: step 4121, loss 0.0867198, acc 0.984375, prec 0.0760044, recall 0.800882
2017-12-10T02:53:11.884994: step 4122, loss 0.0678161, acc 0.984375, prec 0.076018, recall 0.800916
2017-12-10T02:53:12.147844: step 4123, loss 0.0181521, acc 1, prec 0.0760478, recall 0.800983
2017-12-10T02:53:12.421611: step 4124, loss 0.0270456, acc 1, prec 0.0760626, recall 0.801017
2017-12-10T02:53:12.688704: step 4125, loss 4.19437, acc 0.96875, prec 0.0760614, recall 0.800881
2017-12-10T02:53:12.958102: step 4126, loss 0.118814, acc 0.953125, prec 0.0760726, recall 0.800915
2017-12-10T02:53:13.229056: step 4127, loss 0.534703, acc 0.984375, prec 0.0760863, recall 0.800949
2017-12-10T02:53:13.500345: step 4128, loss 0.0737758, acc 0.96875, prec 0.0760838, recall 0.800949
2017-12-10T02:53:13.763180: step 4129, loss 0.111194, acc 0.96875, prec 0.0760962, recall 0.800982
2017-12-10T02:53:14.032129: step 4130, loss 0.0267852, acc 0.984375, prec 0.0761099, recall 0.801016
2017-12-10T02:53:14.295290: step 4131, loss 0.324101, acc 0.875, prec 0.0761298, recall 0.801083
2017-12-10T02:53:14.561032: step 4132, loss 0.0563085, acc 0.96875, prec 0.0761422, recall 0.801117
2017-12-10T02:53:14.832033: step 4133, loss 0.272938, acc 0.890625, prec 0.0761336, recall 0.801117
2017-12-10T02:53:15.093631: step 4134, loss 0.444484, acc 0.859375, prec 0.0761226, recall 0.801117
2017-12-10T02:53:15.358382: step 4135, loss 0.529473, acc 0.875, prec 0.0761425, recall 0.801184
2017-12-10T02:53:15.620927: step 4136, loss 0.167496, acc 0.921875, prec 0.0761513, recall 0.801218
2017-12-10T02:53:15.882767: step 4137, loss 0.192421, acc 0.921875, prec 0.0761749, recall 0.801285
2017-12-10T02:53:16.147277: step 4138, loss 0.392126, acc 0.921875, prec 0.0761836, recall 0.801319
2017-12-10T02:53:16.409025: step 4139, loss 0.406543, acc 0.9375, prec 0.0761935, recall 0.801353
2017-12-10T02:53:16.672669: step 4140, loss 0.109056, acc 0.953125, prec 0.0762196, recall 0.80142
2017-12-10T02:53:16.935907: step 4141, loss 0.302586, acc 0.90625, prec 0.0762271, recall 0.801453
2017-12-10T02:53:17.204026: step 4142, loss 0.306309, acc 0.9375, prec 0.076237, recall 0.801487
2017-12-10T02:53:17.464257: step 4143, loss 0.25587, acc 0.921875, prec 0.0762309, recall 0.801487
2017-12-10T02:53:17.724469: step 4144, loss 0.341228, acc 0.890625, prec 0.0762371, recall 0.80152
2017-12-10T02:53:17.986204: step 4145, loss 0.0647382, acc 0.984375, prec 0.0762508, recall 0.801554
2017-12-10T02:53:18.254669: step 4146, loss 0.171658, acc 0.9375, prec 0.0762607, recall 0.801587
2017-12-10T02:53:18.518065: step 4147, loss 0.391075, acc 0.90625, prec 0.0762682, recall 0.801621
2017-12-10T02:53:18.780932: step 4148, loss 0.075388, acc 0.96875, prec 0.0762657, recall 0.801621
2017-12-10T02:53:19.048397: step 4149, loss 0.0722294, acc 0.96875, prec 0.0762633, recall 0.801621
2017-12-10T02:53:19.312524: step 4150, loss 0.134124, acc 0.9375, prec 0.0762584, recall 0.801621
2017-12-10T02:53:19.576811: step 4151, loss 0.0458214, acc 0.984375, prec 0.0762572, recall 0.801621
2017-12-10T02:53:19.838650: step 4152, loss 0.241877, acc 0.953125, prec 0.0762683, recall 0.801654
2017-12-10T02:53:20.112153: step 4153, loss 0.275507, acc 0.953125, prec 0.0763092, recall 0.801755
2017-12-10T02:53:20.381764: step 4154, loss 0.00697632, acc 1, prec 0.0763092, recall 0.801755
2017-12-10T02:53:20.647051: step 4155, loss 0.00230459, acc 1, prec 0.076324, recall 0.801788
2017-12-10T02:53:20.906421: step 4156, loss 0.05667, acc 0.96875, prec 0.0763215, recall 0.801788
2017-12-10T02:53:21.174495: step 4157, loss 0.0586633, acc 0.984375, prec 0.0763351, recall 0.801822
2017-12-10T02:53:21.445943: step 4158, loss 0.0341517, acc 0.984375, prec 0.0763636, recall 0.801888
2017-12-10T02:53:21.705429: step 4159, loss 0.0186958, acc 0.984375, prec 0.0763772, recall 0.801922
2017-12-10T02:53:21.971736: step 4160, loss 0.0740649, acc 1, prec 0.076392, recall 0.801955
2017-12-10T02:53:22.238649: step 4161, loss 0.134774, acc 1, prec 0.0764068, recall 0.801989
2017-12-10T02:53:22.511858: step 4162, loss 0.00417917, acc 1, prec 0.0764068, recall 0.801989
2017-12-10T02:53:22.774970: step 4163, loss 0.212043, acc 0.984375, prec 0.0764353, recall 0.802055
2017-12-10T02:53:23.044784: step 4164, loss 0.0517263, acc 0.96875, prec 0.0764328, recall 0.802055
2017-12-10T02:53:23.308219: step 4165, loss 0.861687, acc 0.9375, prec 0.0764724, recall 0.802155
2017-12-10T02:53:23.583575: step 4166, loss 0.247396, acc 1, prec 0.0764872, recall 0.802189
2017-12-10T02:53:23.845529: step 4167, loss 0.01968, acc 0.984375, prec 0.076486, recall 0.802189
2017-12-10T02:53:24.110531: step 4168, loss 0.395913, acc 0.984375, prec 0.0765144, recall 0.802255
2017-12-10T02:53:24.381630: step 4169, loss 0.00784315, acc 1, prec 0.0765292, recall 0.802288
2017-12-10T02:53:24.644705: step 4170, loss 0.0057179, acc 1, prec 0.0765292, recall 0.802288
2017-12-10T02:53:24.905945: step 4171, loss 0.216816, acc 0.953125, prec 0.0765404, recall 0.802322
2017-12-10T02:53:25.173593: step 4172, loss 4.00456, acc 0.921875, prec 0.0765354, recall 0.802187
2017-12-10T02:53:25.442301: step 4173, loss 0.19276, acc 0.953125, prec 0.0765614, recall 0.802253
2017-12-10T02:53:25.701872: step 4174, loss 0.240777, acc 0.9375, prec 0.0765713, recall 0.802287
2017-12-10T02:53:25.970247: step 4175, loss 0.177079, acc 0.921875, prec 0.07658, recall 0.80232
2017-12-10T02:53:26.231167: step 4176, loss 0.202294, acc 0.921875, prec 0.0765738, recall 0.80232
2017-12-10T02:53:26.511027: step 4177, loss 0.324786, acc 0.921875, prec 0.0765973, recall 0.802386
2017-12-10T02:53:26.776374: step 4178, loss 0.252523, acc 0.9375, prec 0.076622, recall 0.802453
2017-12-10T02:53:27.051055: step 4179, loss 0.545416, acc 0.90625, prec 0.0766295, recall 0.802486
2017-12-10T02:53:27.324571: step 4180, loss 0.631884, acc 0.875, prec 0.0766196, recall 0.802486
2017-12-10T02:53:27.589695: step 4181, loss 0.506369, acc 0.859375, prec 0.0766086, recall 0.802486
2017-12-10T02:53:27.853295: step 4182, loss 0.262563, acc 0.921875, prec 0.0766172, recall 0.802519
2017-12-10T02:53:28.113640: step 4183, loss 0.476474, acc 0.890625, prec 0.0766234, recall 0.802552
2017-12-10T02:53:28.377744: step 4184, loss 0.378755, acc 0.90625, prec 0.0766309, recall 0.802585
2017-12-10T02:53:28.645824: step 4185, loss 0.583137, acc 0.859375, prec 0.0766198, recall 0.802585
2017-12-10T02:53:28.910730: step 4186, loss 0.324393, acc 0.90625, prec 0.076642, recall 0.802651
2017-12-10T02:53:29.175586: step 4187, loss 0.482736, acc 0.875, prec 0.0766322, recall 0.802651
2017-12-10T02:53:29.441967: step 4188, loss 0.385701, acc 0.890625, prec 0.076668, recall 0.802751
2017-12-10T02:53:29.711239: step 4189, loss 0.197667, acc 0.953125, prec 0.0766643, recall 0.802751
2017-12-10T02:53:29.978568: step 4190, loss 0.0859324, acc 0.953125, prec 0.0766754, recall 0.802784
2017-12-10T02:53:30.247681: step 4191, loss 0.295187, acc 0.90625, prec 0.0766828, recall 0.802817
2017-12-10T02:53:30.524142: step 4192, loss 0.368118, acc 0.9375, prec 0.0766779, recall 0.802817
2017-12-10T02:53:30.792187: step 4193, loss 0.624964, acc 0.90625, prec 0.0767149, recall 0.802916
2017-12-10T02:53:31.055269: step 4194, loss 0.218517, acc 0.921875, prec 0.0767383, recall 0.802982
2017-12-10T02:53:31.317520: step 4195, loss 0.467829, acc 0.9375, prec 0.0767334, recall 0.802982
2017-12-10T02:53:31.588356: step 4196, loss 0.189675, acc 0.9375, prec 0.0767581, recall 0.803048
2017-12-10T02:53:31.851327: step 4197, loss 0.357499, acc 0.953125, prec 0.0767692, recall 0.803081
2017-12-10T02:53:32.117714: step 4198, loss 0.0560904, acc 0.984375, prec 0.0767679, recall 0.803081
2017-12-10T02:53:32.381195: step 4199, loss 0.0124001, acc 1, prec 0.0767827, recall 0.803114
2017-12-10T02:53:32.646937: step 4200, loss 0.0978729, acc 0.96875, prec 0.0768098, recall 0.80318

Evaluation:
2017-12-10T02:53:40.325712: step 4200, loss 5.25039, acc 0.960468, prec 0.077333, recall 0.791823

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-4200

2017-12-10T02:53:41.724042: step 4201, loss 0.134916, acc 0.96875, prec 0.0773306, recall 0.791823
2017-12-10T02:53:41.985599: step 4202, loss 0.572812, acc 0.953125, prec 0.0773269, recall 0.791823
2017-12-10T02:53:42.256656: step 4203, loss 0.29188, acc 0.953125, prec 0.0773379, recall 0.791857
2017-12-10T02:53:42.525252: step 4204, loss 0.267281, acc 0.96875, prec 0.0773354, recall 0.791857
2017-12-10T02:53:42.790743: step 4205, loss 0.0416769, acc 0.984375, prec 0.0773489, recall 0.791891
2017-12-10T02:53:43.051888: step 4206, loss 0.0664572, acc 0.984375, prec 0.0773623, recall 0.791924
2017-12-10T02:53:43.314230: step 4207, loss 2.85335, acc 0.984375, prec 0.077377, recall 0.791829
2017-12-10T02:53:43.582430: step 4208, loss 0.0536119, acc 0.96875, prec 0.0773745, recall 0.791829
2017-12-10T02:53:43.846858: step 4209, loss 0.203568, acc 0.984375, prec 0.077432, recall 0.791965
2017-12-10T02:53:44.115673: step 4210, loss 0.172191, acc 0.96875, prec 0.0774442, recall 0.791999
2017-12-10T02:53:44.376623: step 4211, loss 0.050028, acc 0.984375, prec 0.0774723, recall 0.792066
2017-12-10T02:53:44.644994: step 4212, loss 0.0475923, acc 0.984375, prec 0.0774857, recall 0.7921
2017-12-10T02:53:44.916974: step 4213, loss 0.217366, acc 0.984375, prec 0.0774992, recall 0.792134
2017-12-10T02:53:45.185758: step 4214, loss 0.158596, acc 0.96875, prec 0.0775114, recall 0.792168
2017-12-10T02:53:45.445590: step 4215, loss 0.255847, acc 0.921875, prec 0.0775052, recall 0.792168
2017-12-10T02:53:45.708839: step 4216, loss 0.188426, acc 0.9375, prec 0.0775003, recall 0.792168
2017-12-10T02:53:45.973311: step 4217, loss 0.126868, acc 0.953125, prec 0.0775259, recall 0.792235
2017-12-10T02:53:46.240335: step 4218, loss 0.129745, acc 0.984375, prec 0.0775393, recall 0.792269
2017-12-10T02:53:46.505680: step 4219, loss 0.259805, acc 0.953125, prec 0.0775503, recall 0.792303
2017-12-10T02:53:46.771853: step 4220, loss 0.269124, acc 0.953125, prec 0.0775466, recall 0.792303
2017-12-10T02:53:47.036465: step 4221, loss 1.87513, acc 0.90625, prec 0.0775404, recall 0.792174
2017-12-10T02:53:47.306428: step 4222, loss 0.489996, acc 0.890625, prec 0.0775318, recall 0.792174
2017-12-10T02:53:47.564698: step 4223, loss 0.505177, acc 0.859375, prec 0.0775354, recall 0.792208
2017-12-10T02:53:47.833991: step 4224, loss 0.304503, acc 0.9375, prec 0.0775451, recall 0.792242
2017-12-10T02:53:48.100511: step 4225, loss 0.209513, acc 0.953125, prec 0.0775414, recall 0.792242
2017-12-10T02:53:48.372587: step 4226, loss 0.466288, acc 0.921875, prec 0.0775499, recall 0.792275
2017-12-10T02:53:48.643465: step 4227, loss 0.169201, acc 0.953125, prec 0.0775462, recall 0.792275
2017-12-10T02:53:48.915984: step 4228, loss 0.0439094, acc 0.984375, prec 0.0775596, recall 0.792309
2017-12-10T02:53:49.178308: step 4229, loss 0.27811, acc 0.921875, prec 0.0775681, recall 0.792343
2017-12-10T02:53:49.443973: step 4230, loss 0.579216, acc 0.8125, prec 0.0775826, recall 0.79241
2017-12-10T02:53:49.708054: step 4231, loss 0.288758, acc 0.890625, prec 0.077574, recall 0.79241
2017-12-10T02:53:49.970120: step 4232, loss 0.179852, acc 0.96875, prec 0.0775862, recall 0.792444
2017-12-10T02:53:50.237565: step 4233, loss 0.277537, acc 0.90625, prec 0.0776374, recall 0.792578
2017-12-10T02:53:50.505208: step 4234, loss 0.23457, acc 0.953125, prec 0.0776483, recall 0.792612
2017-12-10T02:53:50.771305: step 4235, loss 0.402227, acc 0.90625, prec 0.0776556, recall 0.792645
2017-12-10T02:53:51.031687: step 4236, loss 0.22207, acc 0.9375, prec 0.0776653, recall 0.792679
2017-12-10T02:53:51.297705: step 4237, loss 0.0333848, acc 0.984375, prec 0.077664, recall 0.792679
2017-12-10T02:53:51.557833: step 4238, loss 0.290034, acc 0.9375, prec 0.0776591, recall 0.792679
2017-12-10T02:53:51.825483: step 4239, loss 0.274771, acc 0.9375, prec 0.0776835, recall 0.792746
2017-12-10T02:53:52.095070: step 4240, loss 0.0839597, acc 0.953125, prec 0.0776798, recall 0.792746
2017-12-10T02:53:52.362190: step 4241, loss 0.241341, acc 0.9375, prec 0.0776748, recall 0.792746
2017-12-10T02:53:52.626903: step 4242, loss 0.0260518, acc 0.984375, prec 0.0776882, recall 0.79278
2017-12-10T02:53:52.888394: step 4243, loss 0.11024, acc 0.984375, prec 0.0777163, recall 0.792847
2017-12-10T02:53:53.156437: step 4244, loss 1.96555, acc 0.953125, prec 0.0777431, recall 0.792786
2017-12-10T02:53:53.425317: step 4245, loss 0.574349, acc 0.875, prec 0.0777478, recall 0.792819
2017-12-10T02:53:53.693132: step 4246, loss 0.0672639, acc 0.984375, prec 0.0777612, recall 0.792853
2017-12-10T02:53:53.954382: step 4247, loss 0.0746339, acc 0.953125, prec 0.0777575, recall 0.792853
2017-12-10T02:53:54.216312: step 4248, loss 0.371762, acc 0.921875, prec 0.077766, recall 0.792886
2017-12-10T02:53:54.482322: step 4249, loss 0.550356, acc 0.921875, prec 0.0777598, recall 0.792886
2017-12-10T02:53:54.746283: step 4250, loss 0.504554, acc 0.921875, prec 0.0777536, recall 0.792886
2017-12-10T02:53:55.006619: step 4251, loss 0.164858, acc 0.96875, prec 0.0777658, recall 0.79292
2017-12-10T02:53:55.271521: step 4252, loss 0.117415, acc 0.953125, prec 0.0777621, recall 0.79292
2017-12-10T02:53:55.539969: step 4253, loss 0.050593, acc 0.984375, prec 0.0777755, recall 0.792953
2017-12-10T02:53:55.810753: step 4254, loss 0.483714, acc 0.921875, prec 0.0777693, recall 0.792953
2017-12-10T02:53:56.078378: step 4255, loss 0.144313, acc 0.9375, prec 0.0777644, recall 0.792953
2017-12-10T02:53:56.341687: step 4256, loss 0.169388, acc 0.953125, prec 0.0777753, recall 0.792986
2017-12-10T02:53:56.619271: step 4257, loss 0.0665983, acc 0.96875, prec 0.0777728, recall 0.792986
2017-12-10T02:53:56.879740: step 4258, loss 0.293104, acc 0.921875, prec 0.0777667, recall 0.792986
2017-12-10T02:53:57.148393: step 4259, loss 0.363609, acc 0.9375, prec 0.0777618, recall 0.792986
2017-12-10T02:53:57.414716: step 4260, loss 0.118955, acc 0.9375, prec 0.0777568, recall 0.792986
2017-12-10T02:53:57.678448: step 4261, loss 0.0742707, acc 1, prec 0.0777861, recall 0.793053
2017-12-10T02:53:57.943557: step 4262, loss 0.310893, acc 0.9375, prec 0.0777957, recall 0.793087
2017-12-10T02:53:58.218187: step 4263, loss 0.0546715, acc 0.984375, prec 0.0778237, recall 0.793154
2017-12-10T02:53:58.478682: step 4264, loss 0.0925048, acc 0.984375, prec 0.0778371, recall 0.793187
2017-12-10T02:53:58.743674: step 4265, loss 0.384787, acc 0.953125, prec 0.077848, recall 0.79322
2017-12-10T02:53:59.007741: step 4266, loss 0.293397, acc 0.953125, prec 0.0778735, recall 0.793287
2017-12-10T02:53:59.272589: step 4267, loss 0.102774, acc 0.96875, prec 0.0779003, recall 0.793354
2017-12-10T02:53:59.546397: step 4268, loss 0.110852, acc 0.984375, prec 0.077899, recall 0.793354
2017-12-10T02:53:59.814671: step 4269, loss 0.160369, acc 0.9375, prec 0.0779087, recall 0.793387
2017-12-10T02:54:00.102823: step 4270, loss 0.185328, acc 0.953125, prec 0.077905, recall 0.793387
2017-12-10T02:54:00.374698: step 4271, loss 0.0185405, acc 1, prec 0.0779196, recall 0.79342
2017-12-10T02:54:00.640756: step 4272, loss 3.54, acc 0.9375, prec 0.0779305, recall 0.793326
2017-12-10T02:54:00.910421: step 4273, loss 0.0548787, acc 0.984375, prec 0.0779731, recall 0.793426
2017-12-10T02:54:01.172692: step 4274, loss 0.288757, acc 0.90625, prec 0.0779803, recall 0.793459
2017-12-10T02:54:01.435879: step 4275, loss 0.109326, acc 0.96875, prec 0.0779778, recall 0.793459
2017-12-10T02:54:01.704899: step 4276, loss 0.581211, acc 0.890625, prec 0.0779838, recall 0.793492
2017-12-10T02:54:01.971599: step 4277, loss 0.283515, acc 0.90625, prec 0.0779909, recall 0.793526
2017-12-10T02:54:02.240273: step 4278, loss 0.521923, acc 0.84375, prec 0.0779786, recall 0.793526
2017-12-10T02:54:02.500615: step 4279, loss 0.319653, acc 0.890625, prec 0.0779846, recall 0.793559
2017-12-10T02:54:02.773491: step 4280, loss 0.441274, acc 0.828125, prec 0.077971, recall 0.793559
2017-12-10T02:54:03.035977: step 4281, loss 0.437275, acc 0.859375, prec 0.0779599, recall 0.793559
2017-12-10T02:54:03.306037: step 4282, loss 0.192522, acc 0.953125, prec 0.0779854, recall 0.793625
2017-12-10T02:54:03.575537: step 4283, loss 0.270897, acc 0.890625, prec 0.0779767, recall 0.793625
2017-12-10T02:54:03.842105: step 4284, loss 0.19347, acc 0.953125, prec 0.077973, recall 0.793625
2017-12-10T02:54:04.106015: step 4285, loss 0.185819, acc 0.9375, prec 0.0779827, recall 0.793658
2017-12-10T02:54:04.375397: step 4286, loss 0.600642, acc 0.859375, prec 0.0779716, recall 0.793658
2017-12-10T02:54:04.648256: step 4287, loss 0.635483, acc 0.890625, prec 0.0779629, recall 0.793658
2017-12-10T02:54:04.922931: step 4288, loss 0.273586, acc 0.9375, prec 0.0779872, recall 0.793725
2017-12-10T02:54:05.184372: step 4289, loss 0.519441, acc 0.9375, prec 0.0779968, recall 0.793758
2017-12-10T02:54:05.447074: step 4290, loss 0.347637, acc 0.921875, prec 0.0779906, recall 0.793758
2017-12-10T02:54:05.714092: step 4291, loss 0.348682, acc 0.96875, prec 0.0779882, recall 0.793758
2017-12-10T02:54:05.981400: step 4292, loss 0.164977, acc 0.984375, prec 0.0779869, recall 0.793758
2017-12-10T02:54:06.254729: step 4293, loss 0.182056, acc 0.96875, prec 0.0779991, recall 0.793791
2017-12-10T02:54:06.515920: step 4294, loss 0.0135666, acc 1, prec 0.0779991, recall 0.793791
2017-12-10T02:54:06.789586: step 4295, loss 0.130201, acc 0.9375, prec 0.0779941, recall 0.793791
2017-12-10T02:54:07.060514: step 4296, loss 0.242955, acc 1, prec 0.0780378, recall 0.793891
2017-12-10T02:54:07.327460: step 4297, loss 0.729033, acc 0.96875, prec 0.0780499, recall 0.793924
2017-12-10T02:54:07.599979: step 4298, loss 0.0707294, acc 0.984375, prec 0.0780487, recall 0.793924
2017-12-10T02:54:07.874526: step 4299, loss 0.0420546, acc 0.96875, prec 0.0780754, recall 0.79399
2017-12-10T02:54:08.139748: step 4300, loss 0.126128, acc 0.96875, prec 0.0780729, recall 0.79399
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-4300

2017-12-10T02:54:09.413362: step 4301, loss 0.0494863, acc 0.96875, prec 0.0781141, recall 0.794089
2017-12-10T02:54:09.678081: step 4302, loss 0.0256616, acc 0.984375, prec 0.0781275, recall 0.794122
2017-12-10T02:54:09.940317: step 4303, loss 0.0132947, acc 1, prec 0.0781712, recall 0.794222
2017-12-10T02:54:10.200983: step 4304, loss 0.626468, acc 0.90625, prec 0.0781638, recall 0.794222
2017-12-10T02:54:10.472532: step 4305, loss 0.12515, acc 0.96875, prec 0.0781758, recall 0.794255
2017-12-10T02:54:10.737731: step 4306, loss 0.209981, acc 0.953125, prec 0.0781867, recall 0.794288
2017-12-10T02:54:11.000093: step 4307, loss 0.205559, acc 0.9375, prec 0.0781818, recall 0.794288
2017-12-10T02:54:11.270548: step 4308, loss 0.133198, acc 0.96875, prec 0.0782084, recall 0.794354
2017-12-10T02:54:11.544001: step 4309, loss 0.00529787, acc 1, prec 0.0782521, recall 0.794452
2017-12-10T02:54:11.813621: step 4310, loss 0.0845916, acc 0.96875, prec 0.0782496, recall 0.794452
2017-12-10T02:54:12.075095: step 4311, loss 0.153989, acc 0.984375, prec 0.0782484, recall 0.794452
2017-12-10T02:54:12.341915: step 4312, loss 0.155558, acc 0.953125, prec 0.0782738, recall 0.794518
2017-12-10T02:54:12.601957: step 4313, loss 0.139157, acc 0.984375, prec 0.0782725, recall 0.794518
2017-12-10T02:54:12.867202: step 4314, loss 0.0347892, acc 0.984375, prec 0.0782713, recall 0.794518
2017-12-10T02:54:13.139045: step 4315, loss 0.0648537, acc 0.984375, prec 0.0782701, recall 0.794518
2017-12-10T02:54:13.403814: step 4316, loss 0.214525, acc 0.984375, prec 0.0783125, recall 0.794617
2017-12-10T02:54:13.668464: step 4317, loss 0.0246046, acc 0.984375, prec 0.0783113, recall 0.794617
2017-12-10T02:54:13.937494: step 4318, loss 0.238136, acc 0.9375, prec 0.0783209, recall 0.79465
2017-12-10T02:54:14.204437: step 4319, loss 0.880207, acc 0.96875, prec 0.0783475, recall 0.794716
2017-12-10T02:54:14.470505: step 4320, loss 0.169614, acc 0.984375, prec 0.0783608, recall 0.794749
2017-12-10T02:54:14.737679: step 4321, loss 0.202168, acc 0.96875, prec 0.0783583, recall 0.794749
2017-12-10T02:54:15.000815: step 4322, loss 0.0604554, acc 0.984375, prec 0.0783571, recall 0.794749
2017-12-10T02:54:15.263728: step 4323, loss 6.07508, acc 0.953125, prec 0.0783692, recall 0.794654
2017-12-10T02:54:15.532530: step 4324, loss 1.85589, acc 0.953125, prec 0.0783812, recall 0.79456
2017-12-10T02:54:15.808554: step 4325, loss 0.161117, acc 0.953125, prec 0.0784066, recall 0.794626
2017-12-10T02:54:16.072622: step 4326, loss 0.300794, acc 0.921875, prec 0.0784004, recall 0.794626
2017-12-10T02:54:16.343360: step 4327, loss 0.743288, acc 0.828125, prec 0.0783868, recall 0.794626
2017-12-10T02:54:16.603272: step 4328, loss 0.350342, acc 0.890625, prec 0.0783927, recall 0.794659
2017-12-10T02:54:16.873144: step 4329, loss 0.922399, acc 0.75, prec 0.0783875, recall 0.794691
2017-12-10T02:54:17.136338: step 4330, loss 0.619941, acc 0.890625, prec 0.0783933, recall 0.794724
2017-12-10T02:54:17.397750: step 4331, loss 0.463398, acc 0.859375, prec 0.0783822, recall 0.794724
2017-12-10T02:54:17.667268: step 4332, loss 0.219791, acc 0.921875, prec 0.0784051, recall 0.79479
2017-12-10T02:54:17.937376: step 4333, loss 0.57577, acc 0.828125, prec 0.0784351, recall 0.794888
2017-12-10T02:54:18.199908: step 4334, loss 0.738316, acc 0.828125, prec 0.078436, recall 0.794921
2017-12-10T02:54:18.460923: step 4335, loss 0.727898, acc 0.796875, prec 0.0784345, recall 0.794954
2017-12-10T02:54:18.729930: step 4336, loss 0.750809, acc 0.796875, prec 0.0784329, recall 0.794986
2017-12-10T02:54:18.995512: step 4337, loss 0.796515, acc 0.875, prec 0.078423, recall 0.794986
2017-12-10T02:54:19.257320: step 4338, loss 0.498854, acc 0.890625, prec 0.0784289, recall 0.795019
2017-12-10T02:54:19.519044: step 4339, loss 0.34104, acc 0.90625, prec 0.0784505, recall 0.795085
2017-12-10T02:54:19.781998: step 4340, loss 0.19519, acc 0.984375, prec 0.0784638, recall 0.795117
2017-12-10T02:54:20.044296: step 4341, loss 0.413686, acc 0.890625, prec 0.0784551, recall 0.795117
2017-12-10T02:54:20.312743: step 4342, loss 0.0527732, acc 1, prec 0.0784697, recall 0.79515
2017-12-10T02:54:20.583066: step 4343, loss 0.216636, acc 0.9375, prec 0.0784792, recall 0.795183
2017-12-10T02:54:20.848897: step 4344, loss 0.41001, acc 0.921875, prec 0.0785021, recall 0.795248
2017-12-10T02:54:21.115105: step 4345, loss 0.365081, acc 0.90625, prec 0.0784946, recall 0.795248
2017-12-10T02:54:21.376806: step 4346, loss 0.381983, acc 0.90625, prec 0.0785307, recall 0.795346
2017-12-10T02:54:21.637327: step 4347, loss 0.776995, acc 0.96875, prec 0.0785573, recall 0.795411
2017-12-10T02:54:21.902906: step 4348, loss 0.303665, acc 0.953125, prec 0.0785826, recall 0.795476
2017-12-10T02:54:22.166075: step 4349, loss 0.262544, acc 0.953125, prec 0.0785933, recall 0.795509
2017-12-10T02:54:22.428660: step 4350, loss 0.104387, acc 0.96875, prec 0.0785909, recall 0.795509
2017-12-10T02:54:22.698627: step 4351, loss 0.158084, acc 0.9375, prec 0.0786149, recall 0.795574
2017-12-10T02:54:22.961814: step 4352, loss 0.176492, acc 0.984375, prec 0.0786137, recall 0.795574
2017-12-10T02:54:23.230480: step 4353, loss 0.186223, acc 0.96875, prec 0.0786257, recall 0.795606
2017-12-10T02:54:23.498611: step 4354, loss 0.0408016, acc 0.984375, prec 0.0786245, recall 0.795606
2017-12-10T02:54:23.771126: step 4355, loss 0.0852918, acc 0.953125, prec 0.0786208, recall 0.795606
2017-12-10T02:54:24.041378: step 4356, loss 0.097098, acc 0.984375, prec 0.078634, recall 0.795639
2017-12-10T02:54:24.305473: step 4357, loss 0.0564987, acc 0.984375, prec 0.0786473, recall 0.795672
2017-12-10T02:54:24.570675: step 4358, loss 0.00930482, acc 1, prec 0.0786473, recall 0.795672
2017-12-10T02:54:24.832485: step 4359, loss 0.0734127, acc 0.984375, prec 0.078646, recall 0.795672
2017-12-10T02:54:25.098565: step 4360, loss 0.0545029, acc 0.984375, prec 0.0786448, recall 0.795672
2017-12-10T02:54:25.363755: step 4361, loss 0.0457988, acc 0.984375, prec 0.0786436, recall 0.795672
2017-12-10T02:54:25.632329: step 4362, loss 0.0247111, acc 0.984375, prec 0.0786423, recall 0.795672
2017-12-10T02:54:25.893845: step 4363, loss 0.253609, acc 0.921875, prec 0.0786361, recall 0.795672
2017-12-10T02:54:26.155261: step 4364, loss 0.0461602, acc 0.984375, prec 0.0786349, recall 0.795672
2017-12-10T02:54:26.425510: step 4365, loss 1.41504, acc 0.953125, prec 0.0786324, recall 0.795545
2017-12-10T02:54:26.696841: step 4366, loss 0.192511, acc 0.953125, prec 0.0786287, recall 0.795545
2017-12-10T02:54:26.960118: step 4367, loss 1.88623, acc 0.96875, prec 0.0786275, recall 0.795418
2017-12-10T02:54:27.230323: step 4368, loss 0.104071, acc 0.96875, prec 0.078654, recall 0.795483
2017-12-10T02:54:27.494202: step 4369, loss 0.173542, acc 0.9375, prec 0.078649, recall 0.795483
2017-12-10T02:54:27.763499: step 4370, loss 0.679421, acc 0.9375, prec 0.0786586, recall 0.795516
2017-12-10T02:54:28.026428: step 4371, loss 0.0997558, acc 0.953125, prec 0.0786693, recall 0.795548
2017-12-10T02:54:28.297244: step 4372, loss 0.424085, acc 0.890625, prec 0.0786607, recall 0.795548
2017-12-10T02:54:28.563947: step 4373, loss 0.141692, acc 0.9375, prec 0.0786557, recall 0.795548
2017-12-10T02:54:28.827777: step 4374, loss 0.322739, acc 0.921875, prec 0.0786785, recall 0.795613
2017-12-10T02:54:29.091357: step 4375, loss 0.227582, acc 0.890625, prec 0.0786988, recall 0.795678
2017-12-10T02:54:29.351224: step 4376, loss 0.387035, acc 0.9375, prec 0.0786939, recall 0.795678
2017-12-10T02:54:29.617951: step 4377, loss 0.6568, acc 0.859375, prec 0.0786828, recall 0.795678
2017-12-10T02:54:29.883398: step 4378, loss 0.72002, acc 0.828125, prec 0.0786836, recall 0.795711
2017-12-10T02:54:30.153246: step 4379, loss 0.350683, acc 0.921875, prec 0.0786919, recall 0.795743
2017-12-10T02:54:30.426445: step 4380, loss 0.402262, acc 0.890625, prec 0.0786977, recall 0.795776
2017-12-10T02:54:30.693018: step 4381, loss 0.183244, acc 0.96875, prec 0.0786953, recall 0.795776
2017-12-10T02:54:30.957205: step 4382, loss 0.537945, acc 0.828125, prec 0.0786817, recall 0.795776
2017-12-10T02:54:31.219204: step 4383, loss 0.543115, acc 0.9375, prec 0.0786767, recall 0.795776
2017-12-10T02:54:31.485510: step 4384, loss 2.2483, acc 0.90625, prec 0.078685, recall 0.795682
2017-12-10T02:54:31.751547: step 4385, loss 0.0641851, acc 0.984375, prec 0.0786983, recall 0.795714
2017-12-10T02:54:32.013240: step 4386, loss 0.249419, acc 0.921875, prec 0.078721, recall 0.795779
2017-12-10T02:54:32.280357: step 4387, loss 0.264818, acc 0.921875, prec 0.0787293, recall 0.795812
2017-12-10T02:54:32.542828: step 4388, loss 0.509363, acc 0.875, prec 0.0787194, recall 0.795812
2017-12-10T02:54:32.811179: step 4389, loss 0.343491, acc 0.890625, prec 0.0787252, recall 0.795844
2017-12-10T02:54:33.077756: step 4390, loss 0.456492, acc 0.90625, prec 0.0787323, recall 0.795876
2017-12-10T02:54:33.338931: step 4391, loss 0.239929, acc 0.9375, prec 0.0787418, recall 0.795909
2017-12-10T02:54:33.600972: step 4392, loss 0.40467, acc 0.96875, prec 0.0787537, recall 0.795941
2017-12-10T02:54:33.876078: step 4393, loss 0.41825, acc 0.921875, prec 0.078762, recall 0.795973
2017-12-10T02:54:34.136689: step 4394, loss 0.16501, acc 0.953125, prec 0.0787728, recall 0.796006
2017-12-10T02:54:34.406529: step 4395, loss 0.265134, acc 0.9375, prec 0.0787823, recall 0.796038
2017-12-10T02:54:34.669185: step 4396, loss 0.138582, acc 0.953125, prec 0.078793, recall 0.79607
2017-12-10T02:54:34.932141: step 4397, loss 0.452688, acc 0.890625, prec 0.0787844, recall 0.79607
2017-12-10T02:54:35.200795: step 4398, loss 0.224885, acc 0.921875, prec 0.0788215, recall 0.796167
2017-12-10T02:54:35.466150: step 4399, loss 0.199146, acc 0.921875, prec 0.0788298, recall 0.7962
2017-12-10T02:54:35.737596: step 4400, loss 0.141491, acc 0.96875, prec 0.0788418, recall 0.796232
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-4400

2017-12-10T02:54:37.134312: step 4401, loss 0.317214, acc 0.921875, prec 0.0788356, recall 0.796232
2017-12-10T02:54:37.399796: step 4402, loss 0.381895, acc 0.9375, prec 0.0788451, recall 0.796264
2017-12-10T02:54:37.666336: step 4403, loss 0.269845, acc 0.9375, prec 0.0788546, recall 0.796296
2017-12-10T02:54:37.925189: step 4404, loss 0.147815, acc 0.921875, prec 0.0788628, recall 0.796329
2017-12-10T02:54:38.189243: step 4405, loss 0.179023, acc 0.9375, prec 0.0788723, recall 0.796361
2017-12-10T02:54:38.454898: step 4406, loss 0.09475, acc 0.953125, prec 0.0788975, recall 0.796425
2017-12-10T02:54:38.726470: step 4407, loss 0.262523, acc 0.953125, prec 0.0788938, recall 0.796425
2017-12-10T02:54:38.992405: step 4408, loss 0.0087836, acc 1, prec 0.0788938, recall 0.796425
2017-12-10T02:54:39.262595: step 4409, loss 0.244711, acc 0.984375, prec 0.078907, recall 0.796457
2017-12-10T02:54:39.535496: step 4410, loss 1.83581, acc 0.96875, prec 0.0789635, recall 0.79646
2017-12-10T02:54:39.804028: step 4411, loss 0.0726911, acc 0.984375, prec 0.0789766, recall 0.796492
2017-12-10T02:54:40.063321: step 4412, loss 0.186844, acc 0.953125, prec 0.0789729, recall 0.796492
2017-12-10T02:54:40.333201: step 4413, loss 0.0319666, acc 0.984375, prec 0.0789717, recall 0.796492
2017-12-10T02:54:40.600066: step 4414, loss 0.0948341, acc 0.921875, prec 0.0789944, recall 0.796557
2017-12-10T02:54:40.863556: step 4415, loss 0.246919, acc 0.90625, prec 0.0789869, recall 0.796557
2017-12-10T02:54:41.129398: step 4416, loss 0.197826, acc 0.9375, prec 0.078982, recall 0.796557
2017-12-10T02:54:41.410104: step 4417, loss 0.0881329, acc 0.953125, prec 0.0789783, recall 0.796557
2017-12-10T02:54:41.680694: step 4418, loss 0.479163, acc 0.90625, prec 0.0789853, recall 0.796589
2017-12-10T02:54:41.949330: step 4419, loss 0.1742, acc 0.9375, prec 0.0789803, recall 0.796589
2017-12-10T02:54:42.216600: step 4420, loss 0.302544, acc 0.9375, prec 0.0790331, recall 0.796717
2017-12-10T02:54:42.482487: step 4421, loss 0.212165, acc 0.921875, prec 0.0790269, recall 0.796717
2017-12-10T02:54:42.745973: step 4422, loss 0.0995227, acc 0.9375, prec 0.0790363, recall 0.796749
2017-12-10T02:54:43.011507: step 4423, loss 0.0354805, acc 0.984375, prec 0.0790495, recall 0.796781
2017-12-10T02:54:43.273589: step 4424, loss 0.223109, acc 0.953125, prec 0.0790602, recall 0.796813
2017-12-10T02:54:43.543560: step 4425, loss 0.124946, acc 0.953125, prec 0.0790565, recall 0.796813
2017-12-10T02:54:43.807819: step 4426, loss 0.110986, acc 0.96875, prec 0.0790685, recall 0.796845
2017-12-10T02:54:44.070454: step 4427, loss 0.233207, acc 0.9375, prec 0.0790635, recall 0.796845
2017-12-10T02:54:44.341542: step 4428, loss 0.08196, acc 0.96875, prec 0.079061, recall 0.796845
2017-12-10T02:54:44.605415: step 4429, loss 0.0863539, acc 0.96875, prec 0.0790586, recall 0.796845
2017-12-10T02:54:44.870256: step 4430, loss 0.333721, acc 0.90625, prec 0.0790655, recall 0.796877
2017-12-10T02:54:45.139490: step 4431, loss 0.12233, acc 0.96875, prec 0.0790775, recall 0.79691
2017-12-10T02:54:45.408277: step 4432, loss 0.0115544, acc 1, prec 0.0790775, recall 0.79691
2017-12-10T02:54:45.687502: step 4433, loss 0.255298, acc 0.96875, prec 0.0790894, recall 0.796942
2017-12-10T02:54:45.956144: step 4434, loss 0.0142041, acc 1, prec 0.0790894, recall 0.796942
2017-12-10T02:54:46.228687: step 4435, loss 0.153279, acc 0.96875, prec 0.0790869, recall 0.796942
2017-12-10T02:54:46.494341: step 4436, loss 0.0425576, acc 0.984375, prec 0.0790857, recall 0.796942
2017-12-10T02:54:46.765687: step 4437, loss 0.00673238, acc 1, prec 0.0790857, recall 0.796942
2017-12-10T02:54:47.036660: step 4438, loss 0.0322021, acc 0.984375, prec 0.0791133, recall 0.797006
2017-12-10T02:54:47.299458: step 4439, loss 0.0111724, acc 1, prec 0.0791133, recall 0.797006
2017-12-10T02:54:47.565732: step 4440, loss 0.277993, acc 0.953125, prec 0.079124, recall 0.797037
2017-12-10T02:54:47.829971: step 4441, loss 0.0243643, acc 0.984375, prec 0.0791371, recall 0.797069
2017-12-10T02:54:48.099994: step 4442, loss 0.141223, acc 0.984375, prec 0.0791647, recall 0.797133
2017-12-10T02:54:48.370581: step 4443, loss 0.0866439, acc 0.96875, prec 0.0791766, recall 0.797165
2017-12-10T02:54:48.633464: step 4444, loss 0.00256081, acc 1, prec 0.0791766, recall 0.797165
2017-12-10T02:54:48.895938: step 4445, loss 0.0156838, acc 1, prec 0.0791766, recall 0.797165
2017-12-10T02:54:49.168008: step 4446, loss 0.0196382, acc 1, prec 0.079191, recall 0.797197
2017-12-10T02:54:49.438459: step 4447, loss 0.00869641, acc 1, prec 0.0792054, recall 0.797229
2017-12-10T02:54:49.698696: step 4448, loss 2.16917, acc 0.984375, prec 0.0792342, recall 0.797168
2017-12-10T02:54:49.968573: step 4449, loss 0.0890369, acc 0.984375, prec 0.0792474, recall 0.797199
2017-12-10T02:54:50.237698: step 4450, loss 0.237118, acc 0.96875, prec 0.0792737, recall 0.797263
2017-12-10T02:54:50.510970: step 4451, loss 0.055269, acc 0.96875, prec 0.0792712, recall 0.797263
2017-12-10T02:54:50.780229: step 4452, loss 0.261401, acc 0.921875, prec 0.0792938, recall 0.797327
2017-12-10T02:54:51.045133: step 4453, loss 0.286506, acc 0.921875, prec 0.0793308, recall 0.797423
2017-12-10T02:54:51.310193: step 4454, loss 0.222752, acc 0.90625, prec 0.0793234, recall 0.797423
2017-12-10T02:54:51.575415: step 4455, loss 0.396248, acc 0.890625, prec 0.0793291, recall 0.797454
2017-12-10T02:54:51.841988: step 4456, loss 0.383294, acc 0.890625, prec 0.0793348, recall 0.797486
2017-12-10T02:54:52.115349: step 4457, loss 0.859002, acc 0.9375, prec 0.0793586, recall 0.79755
2017-12-10T02:54:52.390464: step 4458, loss 0.192106, acc 0.90625, prec 0.07938, recall 0.797613
2017-12-10T02:54:52.656954: step 4459, loss 0.262026, acc 0.921875, prec 0.0793738, recall 0.797613
2017-12-10T02:54:52.920894: step 4460, loss 0.25412, acc 0.859375, prec 0.0794057, recall 0.797709
2017-12-10T02:54:53.182937: step 4461, loss 0.507259, acc 0.90625, prec 0.0793983, recall 0.797709
2017-12-10T02:54:53.448083: step 4462, loss 0.437734, acc 0.875, prec 0.0793884, recall 0.797709
2017-12-10T02:54:53.706994: step 4463, loss 0.438598, acc 0.875, prec 0.0793785, recall 0.797709
2017-12-10T02:54:53.979941: step 4464, loss 0.466178, acc 0.828125, prec 0.0793792, recall 0.79774
2017-12-10T02:54:54.253425: step 4465, loss 0.163005, acc 0.953125, prec 0.0793755, recall 0.79774
2017-12-10T02:54:54.520663: step 4466, loss 0.39594, acc 0.90625, prec 0.0793824, recall 0.797772
2017-12-10T02:54:54.791384: step 4467, loss 0.950403, acc 0.8125, prec 0.0793676, recall 0.797772
2017-12-10T02:54:55.059048: step 4468, loss 0.098651, acc 0.953125, prec 0.0793638, recall 0.797772
2017-12-10T02:54:55.322796: step 4469, loss 0.324615, acc 0.921875, prec 0.0793576, recall 0.797772
2017-12-10T02:54:55.589720: step 4470, loss 0.174372, acc 0.96875, prec 0.0794126, recall 0.797899
2017-12-10T02:54:55.852915: step 4471, loss 0.137448, acc 0.90625, prec 0.0794339, recall 0.797962
2017-12-10T02:54:56.117635: step 4472, loss 0.139587, acc 0.953125, prec 0.0794446, recall 0.797994
2017-12-10T02:54:56.348227: step 4473, loss 0.287945, acc 0.960784, prec 0.0794565, recall 0.798026
2017-12-10T02:54:56.630054: step 4474, loss 0.0906455, acc 0.96875, prec 0.0794683, recall 0.798057
2017-12-10T02:54:56.897341: step 4475, loss 1.86126, acc 0.9375, prec 0.0794646, recall 0.797932
2017-12-10T02:54:57.163059: step 4476, loss 0.0488304, acc 0.984375, prec 0.0794634, recall 0.797932
2017-12-10T02:54:57.435854: step 4477, loss 0.190167, acc 0.953125, prec 0.079474, recall 0.797964
2017-12-10T02:54:57.700728: step 4478, loss 0.190749, acc 0.953125, prec 0.0794703, recall 0.797964
2017-12-10T02:54:57.965296: step 4479, loss 0.391549, acc 0.921875, prec 0.0794641, recall 0.797964
2017-12-10T02:54:58.231220: step 4480, loss 0.295741, acc 0.9375, prec 0.0794591, recall 0.797964
2017-12-10T02:54:58.498181: step 4481, loss 0.298152, acc 0.921875, prec 0.079453, recall 0.797964
2017-12-10T02:54:58.758861: step 4482, loss 0.338201, acc 0.9375, prec 0.079448, recall 0.797964
2017-12-10T02:54:59.023299: step 4483, loss 0.104728, acc 0.953125, prec 0.0794443, recall 0.797964
2017-12-10T02:54:59.285956: step 4484, loss 0.251515, acc 0.859375, prec 0.0794331, recall 0.797964
2017-12-10T02:54:59.556452: step 4485, loss 0.137915, acc 0.9375, prec 0.0794282, recall 0.797964
2017-12-10T02:54:59.821822: step 4486, loss 0.0594373, acc 0.96875, prec 0.0794257, recall 0.797964
2017-12-10T02:55:00.090935: step 4487, loss 0.468995, acc 0.9375, prec 0.0794351, recall 0.797996
2017-12-10T02:55:00.362863: step 4488, loss 0.0590967, acc 0.984375, prec 0.0794482, recall 0.798027
2017-12-10T02:55:00.629235: step 4489, loss 0.182529, acc 0.953125, prec 0.0794445, recall 0.798027
2017-12-10T02:55:00.892236: step 4490, loss 0.0813711, acc 0.96875, prec 0.0794707, recall 0.79809
2017-12-10T02:55:01.158559: step 4491, loss 0.108608, acc 0.96875, prec 0.0794682, recall 0.79809
2017-12-10T02:55:01.431318: step 4492, loss 0.512558, acc 0.953125, prec 0.0794789, recall 0.798122
2017-12-10T02:55:01.707794: step 4493, loss 0.178546, acc 0.9375, prec 0.0794739, recall 0.798122
2017-12-10T02:55:01.978019: step 4494, loss 0.18148, acc 0.984375, prec 0.079487, recall 0.798154
2017-12-10T02:55:02.237253: step 4495, loss 0.262935, acc 0.96875, prec 0.0794845, recall 0.798154
2017-12-10T02:55:02.500231: step 4496, loss 0.0174678, acc 1, prec 0.0795132, recall 0.798217
2017-12-10T02:55:02.760113: step 4497, loss 0.0743301, acc 0.96875, prec 0.0795251, recall 0.798248
2017-12-10T02:55:03.022611: step 4498, loss 0.828579, acc 0.96875, prec 0.0795513, recall 0.798311
2017-12-10T02:55:03.293073: step 4499, loss 0.298269, acc 0.9375, prec 0.0795463, recall 0.798311
2017-12-10T02:55:03.555474: step 4500, loss 0.192875, acc 0.9375, prec 0.0795414, recall 0.798311

Evaluation:
2017-12-10T02:55:11.274916: step 4500, loss 4.65502, acc 0.953769, prec 0.0800247, recall 0.78872

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-4500

2017-12-10T02:55:12.633882: step 4501, loss 0.0794467, acc 0.984375, prec 0.0800235, recall 0.78872
2017-12-10T02:55:12.896815: step 4502, loss 0.110246, acc 0.96875, prec 0.080021, recall 0.78872
2017-12-10T02:55:13.163308: step 4503, loss 0.0765087, acc 0.953125, prec 0.0800316, recall 0.788752
2017-12-10T02:55:13.428319: step 4504, loss 0.0089945, acc 1, prec 0.08006, recall 0.788816
2017-12-10T02:55:13.690439: step 4505, loss 0.117232, acc 0.9375, prec 0.0800551, recall 0.788816
2017-12-10T02:55:13.960749: step 4506, loss 0.259142, acc 0.9375, prec 0.0800501, recall 0.788816
2017-12-10T02:55:14.238121: step 4507, loss 0.520231, acc 0.984375, prec 0.0800631, recall 0.788848
2017-12-10T02:55:14.506151: step 4508, loss 0.261031, acc 0.953125, prec 0.0800594, recall 0.788848
2017-12-10T02:55:14.769264: step 4509, loss 0.0716654, acc 0.96875, prec 0.0800569, recall 0.788848
2017-12-10T02:55:15.034671: step 4510, loss 0.524567, acc 0.984375, prec 0.0800699, recall 0.78888
2017-12-10T02:55:15.306852: step 4511, loss 0.116851, acc 0.953125, prec 0.0800804, recall 0.788913
2017-12-10T02:55:15.582672: step 4512, loss 0.170951, acc 0.96875, prec 0.0800921, recall 0.788945
2017-12-10T02:55:15.848524: step 4513, loss 0.956395, acc 0.90625, prec 0.0800989, recall 0.788977
2017-12-10T02:55:16.112606: step 4514, loss 0.131754, acc 0.9375, prec 0.0801224, recall 0.789041
2017-12-10T02:55:16.376680: step 4515, loss 0.163217, acc 0.921875, prec 0.0801447, recall 0.789105
2017-12-10T02:55:16.639971: step 4516, loss 0.121301, acc 0.96875, prec 0.0801848, recall 0.789201
2017-12-10T02:55:16.911940: step 4517, loss 0.342426, acc 0.921875, prec 0.0801928, recall 0.789234
2017-12-10T02:55:17.173448: step 4518, loss 0.0919718, acc 0.984375, prec 0.0802058, recall 0.789266
2017-12-10T02:55:17.440476: step 4519, loss 0.125708, acc 0.953125, prec 0.0802163, recall 0.789298
2017-12-10T02:55:17.705286: step 4520, loss 0.526576, acc 0.890625, prec 0.0802076, recall 0.789298
2017-12-10T02:55:17.970490: step 4521, loss 0.242823, acc 0.9375, prec 0.0802027, recall 0.789298
2017-12-10T02:55:18.242812: step 4522, loss 0.149152, acc 0.96875, prec 0.0802286, recall 0.789362
2017-12-10T02:55:18.511921: step 4523, loss 0.235022, acc 0.9375, prec 0.0802236, recall 0.789362
2017-12-10T02:55:18.776991: step 4524, loss 0.0300932, acc 0.984375, prec 0.0802224, recall 0.789362
2017-12-10T02:55:19.046999: step 4525, loss 0.167861, acc 0.953125, prec 0.0802187, recall 0.789362
2017-12-10T02:55:19.319713: step 4526, loss 0.380919, acc 0.953125, prec 0.0802292, recall 0.789394
2017-12-10T02:55:19.581721: step 4527, loss 0.164134, acc 0.984375, prec 0.0802564, recall 0.789458
2017-12-10T02:55:19.847279: step 4528, loss 0.307538, acc 0.90625, prec 0.0802631, recall 0.78949
2017-12-10T02:55:20.112542: step 4529, loss 0.297594, acc 0.9375, prec 0.0803008, recall 0.789586
2017-12-10T02:55:20.383889: step 4530, loss 0.0635513, acc 0.96875, prec 0.0803125, recall 0.789617
2017-12-10T02:55:20.649898: step 4531, loss 0.316611, acc 0.953125, prec 0.0803514, recall 0.789713
2017-12-10T02:55:20.913671: step 4532, loss 0.0139311, acc 1, prec 0.0803514, recall 0.789713
2017-12-10T02:55:21.186058: step 4533, loss 0.151304, acc 0.953125, prec 0.0803476, recall 0.789713
2017-12-10T02:55:21.448697: step 4534, loss 0.224164, acc 0.96875, prec 0.0803452, recall 0.789713
2017-12-10T02:55:21.724000: step 4535, loss 0.228827, acc 0.953125, prec 0.0803556, recall 0.789745
2017-12-10T02:55:21.994694: step 4536, loss 0.0584698, acc 0.984375, prec 0.0803686, recall 0.789777
2017-12-10T02:55:22.270939: step 4537, loss 0.130699, acc 0.953125, prec 0.0804074, recall 0.789873
2017-12-10T02:55:22.540261: step 4538, loss 0.144954, acc 0.96875, prec 0.0804333, recall 0.789936
2017-12-10T02:55:22.807673: step 4539, loss 0.0494424, acc 0.96875, prec 0.0804309, recall 0.789936
2017-12-10T02:55:23.081816: step 4540, loss 0.14974, acc 0.9375, prec 0.0804543, recall 0.79
2017-12-10T02:55:23.346418: step 4541, loss 0.103553, acc 0.9375, prec 0.0804493, recall 0.79
2017-12-10T02:55:23.611833: step 4542, loss 0.0311734, acc 0.984375, prec 0.0804481, recall 0.79
2017-12-10T02:55:23.889600: step 4543, loss 0.724878, acc 0.96875, prec 0.080474, recall 0.790064
2017-12-10T02:55:24.158931: step 4544, loss 0.0306074, acc 0.984375, prec 0.0805011, recall 0.790127
2017-12-10T02:55:24.418502: step 4545, loss 0.0263788, acc 0.984375, prec 0.0804998, recall 0.790127
2017-12-10T02:55:24.684156: step 4546, loss 0.0912029, acc 0.984375, prec 0.0804986, recall 0.790127
2017-12-10T02:55:24.947261: step 4547, loss 0.162427, acc 0.96875, prec 0.0805103, recall 0.790159
2017-12-10T02:55:25.216819: step 4548, loss 0.136828, acc 0.953125, prec 0.0805066, recall 0.790159
2017-12-10T02:55:25.479500: step 4549, loss 0.0803701, acc 0.96875, prec 0.0805041, recall 0.790159
2017-12-10T02:55:25.739460: step 4550, loss 0.0027903, acc 1, prec 0.0805041, recall 0.790159
2017-12-10T02:55:26.008474: step 4551, loss 0.169372, acc 0.953125, prec 0.0805004, recall 0.790159
2017-12-10T02:55:26.276157: step 4552, loss 0.0328836, acc 0.984375, prec 0.0805133, recall 0.790191
2017-12-10T02:55:26.548845: step 4553, loss 0.055833, acc 0.96875, prec 0.0805108, recall 0.790191
2017-12-10T02:55:26.818544: step 4554, loss 0.0578349, acc 0.984375, prec 0.0805521, recall 0.790286
2017-12-10T02:55:27.080235: step 4555, loss 0.0493774, acc 0.984375, prec 0.0805651, recall 0.790318
2017-12-10T02:55:27.342190: step 4556, loss 0.147893, acc 0.984375, prec 0.080578, recall 0.790349
2017-12-10T02:55:27.614780: step 4557, loss 0.0087502, acc 1, prec 0.080578, recall 0.790349
2017-12-10T02:55:27.880529: step 4558, loss 0.0607879, acc 0.984375, prec 0.0806051, recall 0.790413
2017-12-10T02:55:28.147028: step 4559, loss 0.00796034, acc 1, prec 0.0806476, recall 0.790508
2017-12-10T02:55:28.412751: step 4560, loss 0.0157364, acc 1, prec 0.0806476, recall 0.790508
2017-12-10T02:55:28.672272: step 4561, loss 0.0516828, acc 0.96875, prec 0.0806452, recall 0.790508
2017-12-10T02:55:28.942944: step 4562, loss 0.0840038, acc 0.953125, prec 0.0806414, recall 0.790508
2017-12-10T02:55:29.216560: step 4563, loss 0.0942426, acc 1, prec 0.0806556, recall 0.79054
2017-12-10T02:55:29.486571: step 4564, loss 0.113869, acc 0.96875, prec 0.0806815, recall 0.790603
2017-12-10T02:55:29.748658: step 4565, loss 0.173037, acc 0.96875, prec 0.080679, recall 0.790603
2017-12-10T02:55:30.014233: step 4566, loss 0.0412603, acc 0.984375, prec 0.0807061, recall 0.790666
2017-12-10T02:55:30.282493: step 4567, loss 0.00205045, acc 1, prec 0.0807203, recall 0.790698
2017-12-10T02:55:30.555222: step 4568, loss 3.72054, acc 0.96875, prec 0.080719, recall 0.790578
2017-12-10T02:55:30.824594: step 4569, loss 0.0300351, acc 0.984375, prec 0.0807178, recall 0.790578
2017-12-10T02:55:31.089986: step 4570, loss 0.205311, acc 0.953125, prec 0.0807282, recall 0.79061
2017-12-10T02:55:31.359429: step 4571, loss 0.24205, acc 0.953125, prec 0.0807528, recall 0.790673
2017-12-10T02:55:31.622090: step 4572, loss 0.0362899, acc 0.984375, prec 0.0807657, recall 0.790705
2017-12-10T02:55:31.893958: step 4573, loss 0.193659, acc 0.96875, prec 0.0807916, recall 0.790768
2017-12-10T02:55:32.155383: step 4574, loss 0.11649, acc 0.96875, prec 0.0807891, recall 0.790768
2017-12-10T02:55:32.422037: step 4575, loss 0.267642, acc 0.953125, prec 0.0807854, recall 0.790768
2017-12-10T02:55:32.686063: step 4576, loss 0.191823, acc 0.9375, prec 0.0807945, recall 0.790799
2017-12-10T02:55:32.948041: step 4577, loss 0.171094, acc 0.953125, prec 0.0808191, recall 0.790863
2017-12-10T02:55:33.218027: step 4578, loss 0.402067, acc 0.9375, prec 0.0808425, recall 0.790926
2017-12-10T02:55:33.483718: step 4579, loss 0.0957288, acc 0.984375, prec 0.0808412, recall 0.790926
2017-12-10T02:55:33.745494: step 4580, loss 0.524416, acc 0.921875, prec 0.080835, recall 0.790926
2017-12-10T02:55:34.007864: step 4581, loss 0.201811, acc 0.921875, prec 0.0808571, recall 0.790989
2017-12-10T02:55:34.278739: step 4582, loss 0.144516, acc 0.953125, prec 0.0808675, recall 0.79102
2017-12-10T02:55:34.549110: step 4583, loss 0.370127, acc 0.921875, prec 0.0808754, recall 0.791052
2017-12-10T02:55:35.525854: step 4584, loss 0.249394, acc 0.953125, prec 0.0809, recall 0.791114
2017-12-10T02:55:35.886957: step 4585, loss 0.188055, acc 0.953125, prec 0.0809104, recall 0.791146
2017-12-10T02:55:36.150413: step 4586, loss 0.15907, acc 0.96875, prec 0.0809221, recall 0.791177
2017-12-10T02:55:36.538732: step 4587, loss 0.103972, acc 0.984375, prec 0.0809209, recall 0.791177
2017-12-10T02:55:37.337931: step 4588, loss 0.0820313, acc 0.984375, prec 0.0809338, recall 0.791209
2017-12-10T02:55:38.090987: step 4589, loss 0.0508546, acc 0.96875, prec 0.0809596, recall 0.791272
2017-12-10T02:55:38.853907: step 4590, loss 0.00874496, acc 1, prec 0.0809596, recall 0.791272
2017-12-10T02:55:39.582433: step 4591, loss 0.595478, acc 0.953125, prec 0.0809841, recall 0.791334
2017-12-10T02:55:40.305699: step 4592, loss 0.0808213, acc 0.953125, prec 0.0809804, recall 0.791334
2017-12-10T02:55:41.426794: step 4593, loss 0.116803, acc 0.96875, prec 0.0809779, recall 0.791334
2017-12-10T02:55:41.832403: step 4594, loss 0.042217, acc 0.984375, prec 0.0809908, recall 0.791366
2017-12-10T02:55:42.117537: step 4595, loss 0.0245042, acc 1, prec 0.0809908, recall 0.791366
2017-12-10T02:55:42.397870: step 4596, loss 0.338853, acc 0.9375, prec 0.081, recall 0.791397
2017-12-10T02:55:42.672079: step 4597, loss 0.00378002, acc 1, prec 0.0810141, recall 0.791429
2017-12-10T02:55:42.942322: step 4598, loss 0.0580207, acc 0.984375, prec 0.081027, recall 0.79146
2017-12-10T02:55:43.217892: step 4599, loss 0.0438936, acc 1, prec 0.0810411, recall 0.791491
2017-12-10T02:55:43.488638: step 4600, loss 0.223627, acc 0.96875, prec 0.0810669, recall 0.791554
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-4600

2017-12-10T02:55:44.892982: step 4601, loss 0.418919, acc 0.96875, prec 0.0810644, recall 0.791554
2017-12-10T02:55:45.166047: step 4602, loss 0.204018, acc 0.96875, prec 0.0810619, recall 0.791554
2017-12-10T02:55:45.441918: step 4603, loss 0.0747619, acc 0.984375, prec 0.081089, recall 0.791617
2017-12-10T02:55:45.715459: step 4604, loss 0.0485462, acc 0.984375, prec 0.0811019, recall 0.791648
2017-12-10T02:55:45.980701: step 4605, loss 0.0228704, acc 0.984375, prec 0.0811431, recall 0.791742
2017-12-10T02:55:46.246263: step 4606, loss 0.147004, acc 0.96875, prec 0.0811406, recall 0.791742
2017-12-10T02:55:46.528887: step 4607, loss 0.100506, acc 0.984375, prec 0.0811534, recall 0.791773
2017-12-10T02:55:46.807526: step 4608, loss 0.00753779, acc 1, prec 0.0811676, recall 0.791804
2017-12-10T02:55:47.074368: step 4609, loss 0.130854, acc 0.953125, prec 0.081178, recall 0.791835
2017-12-10T02:55:47.343518: step 4610, loss 0.11253, acc 0.953125, prec 0.0811884, recall 0.791867
2017-12-10T02:55:47.615314: step 4611, loss 0.188021, acc 0.96875, prec 0.0812141, recall 0.791929
2017-12-10T02:55:47.880723: step 4612, loss 0.00643804, acc 1, prec 0.0812283, recall 0.79196
2017-12-10T02:55:48.145391: step 4613, loss 0.0533386, acc 0.984375, prec 0.0812553, recall 0.792023
2017-12-10T02:55:48.418037: step 4614, loss 0.256306, acc 0.984375, prec 0.0812823, recall 0.792085
2017-12-10T02:55:48.682214: step 4615, loss 0.126945, acc 0.96875, prec 0.0812798, recall 0.792085
2017-12-10T02:55:48.950382: step 4616, loss 0.0292016, acc 0.984375, prec 0.0813209, recall 0.792179
2017-12-10T02:55:49.216590: step 4617, loss 0.206377, acc 0.984375, prec 0.0813621, recall 0.792272
2017-12-10T02:55:49.482029: step 4618, loss 0.151102, acc 0.984375, prec 0.0813891, recall 0.792334
2017-12-10T02:55:49.746228: step 4619, loss 0.0199136, acc 1, prec 0.0814315, recall 0.792427
2017-12-10T02:55:50.011258: step 4620, loss 5.74787, acc 0.96875, prec 0.0814302, recall 0.792309
2017-12-10T02:55:50.281801: step 4621, loss 0.0275654, acc 0.984375, prec 0.081429, recall 0.792309
2017-12-10T02:55:50.549402: step 4622, loss 0.0246878, acc 0.984375, prec 0.081456, recall 0.792371
2017-12-10T02:55:50.813883: step 4623, loss 0.0420492, acc 0.984375, prec 0.081483, recall 0.792433
2017-12-10T02:55:51.077445: step 4624, loss 0.384205, acc 0.921875, prec 0.0814908, recall 0.792464
2017-12-10T02:55:51.338691: step 4625, loss 0.2323, acc 0.921875, prec 0.0814846, recall 0.792464
2017-12-10T02:55:51.610031: step 4626, loss 0.262869, acc 0.890625, prec 0.0814899, recall 0.792495
2017-12-10T02:55:51.888970: step 4627, loss 0.138229, acc 0.9375, prec 0.0815273, recall 0.792588
2017-12-10T02:55:52.158348: step 4628, loss 0.475389, acc 0.875, prec 0.0815596, recall 0.792681
2017-12-10T02:55:52.420687: step 4629, loss 0.365709, acc 0.859375, prec 0.0815483, recall 0.792681
2017-12-10T02:55:52.684314: step 4630, loss 0.48217, acc 0.859375, prec 0.081537, recall 0.792681
2017-12-10T02:55:52.947580: step 4631, loss 0.198105, acc 0.953125, prec 0.0815756, recall 0.792774
2017-12-10T02:55:53.212490: step 4632, loss 0.578247, acc 0.828125, prec 0.0815759, recall 0.792805
2017-12-10T02:55:53.475915: step 4633, loss 0.256814, acc 0.921875, prec 0.0815696, recall 0.792805
2017-12-10T02:55:53.737988: step 4634, loss 0.213349, acc 0.875, prec 0.0815878, recall 0.792867
2017-12-10T02:55:54.000116: step 4635, loss 0.144528, acc 0.953125, prec 0.0816405, recall 0.79299
2017-12-10T02:55:54.264252: step 4636, loss 0.161755, acc 0.921875, prec 0.0816483, recall 0.793021
2017-12-10T02:55:54.526723: step 4637, loss 0.146472, acc 0.9375, prec 0.0816574, recall 0.793052
2017-12-10T02:55:54.789897: step 4638, loss 0.198819, acc 0.953125, prec 0.0816536, recall 0.793052
2017-12-10T02:55:55.051292: step 4639, loss 0.0944068, acc 0.953125, prec 0.081664, recall 0.793083
2017-12-10T02:55:55.322183: step 4640, loss 0.22848, acc 0.9375, prec 0.0816731, recall 0.793114
2017-12-10T02:55:55.588195: step 4641, loss 0.982388, acc 0.9375, prec 0.0816962, recall 0.793175
2017-12-10T02:55:55.858389: step 4642, loss 0.132662, acc 0.96875, prec 0.0817078, recall 0.793206
2017-12-10T02:55:56.122388: step 4643, loss 0.140435, acc 0.953125, prec 0.0817322, recall 0.793268
2017-12-10T02:55:56.392803: step 4644, loss 0.0548676, acc 0.984375, prec 0.0817451, recall 0.793299
2017-12-10T02:55:56.663329: step 4645, loss 0.128646, acc 0.984375, prec 0.0817438, recall 0.793299
2017-12-10T02:55:56.925784: step 4646, loss 0.120902, acc 0.96875, prec 0.0817413, recall 0.793299
2017-12-10T02:55:57.192448: step 4647, loss 0.246787, acc 0.9375, prec 0.0817363, recall 0.793299
2017-12-10T02:55:57.454770: step 4648, loss 0.145205, acc 0.953125, prec 0.0817325, recall 0.793299
2017-12-10T02:55:57.715372: step 4649, loss 0.136356, acc 0.953125, prec 0.0817288, recall 0.793299
2017-12-10T02:55:57.983256: step 4650, loss 0.060149, acc 0.96875, prec 0.0817263, recall 0.793299
2017-12-10T02:55:58.251891: step 4651, loss 0.201479, acc 0.9375, prec 0.0817353, recall 0.793329
2017-12-10T02:55:58.523701: step 4652, loss 0.0760087, acc 0.96875, prec 0.0817469, recall 0.79336
2017-12-10T02:55:58.797874: step 4653, loss 0.110299, acc 0.953125, prec 0.0817713, recall 0.793422
2017-12-10T02:55:59.068742: step 4654, loss 0.10782, acc 0.953125, prec 0.0817816, recall 0.793452
2017-12-10T02:55:59.331574: step 4655, loss 0.0832762, acc 0.984375, prec 0.0818086, recall 0.793514
2017-12-10T02:55:59.597243: step 4656, loss 0.0464557, acc 0.984375, prec 0.0818214, recall 0.793545
2017-12-10T02:55:59.867331: step 4657, loss 0.0504274, acc 0.984375, prec 0.0818342, recall 0.793575
2017-12-10T02:56:00.144175: step 4658, loss 0.0164732, acc 1, prec 0.0818483, recall 0.793606
2017-12-10T02:56:00.411407: step 4659, loss 0.134604, acc 0.984375, prec 0.081847, recall 0.793606
2017-12-10T02:56:00.682662: step 4660, loss 0.633167, acc 0.921875, prec 0.081883, recall 0.793698
2017-12-10T02:56:00.945573: step 4661, loss 0.0440815, acc 0.984375, prec 0.0818817, recall 0.793698
2017-12-10T02:56:01.208702: step 4662, loss 0.161599, acc 0.96875, prec 0.0819074, recall 0.793759
2017-12-10T02:56:01.475858: step 4663, loss 0.072576, acc 0.96875, prec 0.0819049, recall 0.793759
2017-12-10T02:56:01.746981: step 4664, loss 1.03823, acc 0.96875, prec 0.0819164, recall 0.79379
2017-12-10T02:56:02.018672: step 4665, loss 0.0536881, acc 0.96875, prec 0.0819139, recall 0.79379
2017-12-10T02:56:02.282696: step 4666, loss 0.780835, acc 0.9375, prec 0.0819089, recall 0.79379
2017-12-10T02:56:02.543933: step 4667, loss 0.263613, acc 0.96875, prec 0.0819345, recall 0.793851
2017-12-10T02:56:02.807480: step 4668, loss 0.110583, acc 0.96875, prec 0.0819461, recall 0.793882
2017-12-10T02:56:03.083305: step 4669, loss 0.175417, acc 0.96875, prec 0.0819577, recall 0.793912
2017-12-10T02:56:03.347625: step 4670, loss 0.115695, acc 0.96875, prec 0.0819551, recall 0.793912
2017-12-10T02:56:03.612923: step 4671, loss 0.216634, acc 0.984375, prec 0.0819539, recall 0.793912
2017-12-10T02:56:03.879326: step 4672, loss 0.222958, acc 0.921875, prec 0.0819758, recall 0.793974
2017-12-10T02:56:04.139397: step 4673, loss 0.238369, acc 0.9375, prec 0.0819848, recall 0.794004
2017-12-10T02:56:04.403514: step 4674, loss 0.177145, acc 0.96875, prec 0.0819823, recall 0.794004
2017-12-10T02:56:04.664022: step 4675, loss 0.697958, acc 0.96875, prec 0.082022, recall 0.794096
2017-12-10T02:56:04.939283: step 4676, loss 0.352877, acc 0.921875, prec 0.0820157, recall 0.794096
2017-12-10T02:56:05.208494: step 4677, loss 0.0452827, acc 0.984375, prec 0.0820426, recall 0.794157
2017-12-10T02:56:05.475608: step 4678, loss 0.0870524, acc 0.984375, prec 0.0820413, recall 0.794157
2017-12-10T02:56:05.738237: step 4679, loss 0.264108, acc 0.9375, prec 0.0820363, recall 0.794157
2017-12-10T02:56:05.999935: step 4680, loss 0.114936, acc 0.96875, prec 0.0820478, recall 0.794187
2017-12-10T02:56:06.262100: step 4681, loss 0.275306, acc 0.953125, prec 0.0820862, recall 0.794279
2017-12-10T02:56:06.526237: step 4682, loss 0.213585, acc 0.96875, prec 0.0821259, recall 0.79437
2017-12-10T02:56:06.788757: step 4683, loss 0.0592394, acc 0.96875, prec 0.0821515, recall 0.794431
2017-12-10T02:56:07.052742: step 4684, loss 0.425556, acc 0.953125, prec 0.0821477, recall 0.794431
2017-12-10T02:56:07.318057: step 4685, loss 0.0329616, acc 0.984375, prec 0.0821465, recall 0.794431
2017-12-10T02:56:07.584412: step 4686, loss 0.0447247, acc 0.96875, prec 0.0821439, recall 0.794431
2017-12-10T02:56:07.847224: step 4687, loss 0.243255, acc 0.953125, prec 0.0821402, recall 0.794431
2017-12-10T02:56:08.111386: step 4688, loss 0.122426, acc 0.984375, prec 0.082153, recall 0.794462
2017-12-10T02:56:08.389188: step 4689, loss 0.0918847, acc 0.984375, prec 0.0821658, recall 0.794492
2017-12-10T02:56:08.650265: step 4690, loss 0.10671, acc 0.984375, prec 0.0821645, recall 0.794492
2017-12-10T02:56:08.913700: step 4691, loss 0.0442473, acc 1, prec 0.0822067, recall 0.794583
2017-12-10T02:56:09.175208: step 4692, loss 0.123467, acc 0.953125, prec 0.0822169, recall 0.794614
2017-12-10T02:56:09.442504: step 4693, loss 0.128634, acc 1, prec 0.082231, recall 0.794644
2017-12-10T02:56:09.707579: step 4694, loss 0.0972683, acc 0.96875, prec 0.0822285, recall 0.794644
2017-12-10T02:56:09.972439: step 4695, loss 0.0518715, acc 1, prec 0.0822566, recall 0.794705
2017-12-10T02:56:10.236660: step 4696, loss 0.396966, acc 0.9375, prec 0.0822515, recall 0.794705
2017-12-10T02:56:10.508799: step 4697, loss 0.159995, acc 0.984375, prec 0.0822784, recall 0.794766
2017-12-10T02:56:10.781693: step 4698, loss 0.182095, acc 0.96875, prec 0.0822899, recall 0.794796
2017-12-10T02:56:11.046040: step 4699, loss 0.800607, acc 0.953125, prec 0.0823002, recall 0.794826
2017-12-10T02:56:11.313420: step 4700, loss 0.00347086, acc 1, prec 0.0823002, recall 0.794826
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-4700

2017-12-10T02:56:12.664651: step 4701, loss 0.0751088, acc 0.96875, prec 0.0823257, recall 0.794887
2017-12-10T02:56:12.938713: step 4702, loss 0.0405265, acc 0.984375, prec 0.0823245, recall 0.794887
2017-12-10T02:56:13.209571: step 4703, loss 0.880124, acc 0.9375, prec 0.0823335, recall 0.794917
2017-12-10T02:56:13.483914: step 4704, loss 0.396315, acc 0.90625, prec 0.08234, recall 0.794948
2017-12-10T02:56:13.747288: step 4705, loss 0.0915936, acc 0.96875, prec 0.0823655, recall 0.795008
2017-12-10T02:56:14.014978: step 4706, loss 0.110734, acc 0.953125, prec 0.0823618, recall 0.795008
2017-12-10T02:56:14.277329: step 4707, loss 0.0723568, acc 0.984375, prec 0.0823745, recall 0.795038
2017-12-10T02:56:14.543863: step 4708, loss 1.58229, acc 0.9375, prec 0.0823848, recall 0.794951
2017-12-10T02:56:14.819264: step 4709, loss 0.373891, acc 0.9375, prec 0.0823798, recall 0.794951
2017-12-10T02:56:15.077108: step 4710, loss 0.043024, acc 0.984375, prec 0.0823785, recall 0.794951
2017-12-10T02:56:15.345500: step 4711, loss 0.876007, acc 0.984375, prec 0.0823913, recall 0.794982
2017-12-10T02:56:15.612774: step 4712, loss 0.198544, acc 0.9375, prec 0.0824003, recall 0.795012
2017-12-10T02:56:15.874259: step 4713, loss 0.102798, acc 0.9375, prec 0.0824233, recall 0.795072
2017-12-10T02:56:16.139625: step 4714, loss 0.383583, acc 0.9375, prec 0.0824183, recall 0.795072
2017-12-10T02:56:16.414062: step 4715, loss 0.238359, acc 0.921875, prec 0.082412, recall 0.795072
2017-12-10T02:56:16.681542: step 4716, loss 0.358276, acc 0.90625, prec 0.0824184, recall 0.795103
2017-12-10T02:56:16.943824: step 4717, loss 0.560633, acc 0.875, prec 0.0824083, recall 0.795103
2017-12-10T02:56:17.206680: step 4718, loss 0.527792, acc 0.890625, prec 0.0823995, recall 0.795103
2017-12-10T02:56:17.472338: step 4719, loss 0.568253, acc 0.890625, prec 0.0824188, recall 0.795163
2017-12-10T02:56:17.736259: step 4720, loss 0.0512411, acc 0.984375, prec 0.0824175, recall 0.795163
2017-12-10T02:56:18.002243: step 4721, loss 0.257615, acc 0.9375, prec 0.0824125, recall 0.795163
2017-12-10T02:56:18.268744: step 4722, loss 0.364331, acc 0.890625, prec 0.0824177, recall 0.795193
2017-12-10T02:56:18.528639: step 4723, loss 0.261506, acc 0.90625, prec 0.0824241, recall 0.795223
2017-12-10T02:56:18.792454: step 4724, loss 0.28768, acc 0.921875, prec 0.0824459, recall 0.795284
2017-12-10T02:56:19.058441: step 4725, loss 0.319167, acc 0.921875, prec 0.0824676, recall 0.795344
2017-12-10T02:56:19.325762: step 4726, loss 0.222362, acc 0.953125, prec 0.0824919, recall 0.795404
2017-12-10T02:56:19.593809: step 4727, loss 0.200277, acc 0.90625, prec 0.0824983, recall 0.795434
2017-12-10T02:56:19.856583: step 4728, loss 0.039007, acc 0.96875, prec 0.0825098, recall 0.795465
2017-12-10T02:56:20.124861: step 4729, loss 0.0827484, acc 0.96875, prec 0.0825073, recall 0.795465
2017-12-10T02:56:20.386454: step 4730, loss 0.0319921, acc 0.984375, prec 0.08252, recall 0.795495
2017-12-10T02:56:20.655051: step 4731, loss 0.805147, acc 0.984375, prec 0.0825608, recall 0.795585
2017-12-10T02:56:20.922987: step 4732, loss 0.295519, acc 0.90625, prec 0.0825533, recall 0.795585
2017-12-10T02:56:21.191896: step 4733, loss 0.11792, acc 0.96875, prec 0.0825647, recall 0.795615
2017-12-10T02:56:21.452205: step 4734, loss 0.811307, acc 0.96875, prec 0.0825762, recall 0.795645
2017-12-10T02:56:21.719831: step 4735, loss 0.048237, acc 0.984375, prec 0.082575, recall 0.795645
2017-12-10T02:56:21.986222: step 4736, loss 0.213819, acc 0.953125, prec 0.0825852, recall 0.795675
2017-12-10T02:56:22.252503: step 4737, loss 0.306395, acc 0.90625, prec 0.0826056, recall 0.795735
2017-12-10T02:56:22.516769: step 4738, loss 0.417344, acc 0.921875, prec 0.0826133, recall 0.795765
2017-12-10T02:56:22.776676: step 4739, loss 0.0952908, acc 0.96875, prec 0.0826388, recall 0.795825
2017-12-10T02:56:23.042143: step 4740, loss 0.304431, acc 0.9375, prec 0.0826338, recall 0.795825
2017-12-10T02:56:23.305207: step 4741, loss 0.181279, acc 0.9375, prec 0.0826287, recall 0.795825
2017-12-10T02:56:23.580284: step 4742, loss 0.0765394, acc 0.96875, prec 0.0826262, recall 0.795825
2017-12-10T02:56:23.842832: step 4743, loss 0.1501, acc 0.9375, prec 0.0826212, recall 0.795825
2017-12-10T02:56:24.108724: step 4744, loss 0.436726, acc 0.90625, prec 0.0826136, recall 0.795825
2017-12-10T02:56:24.380028: step 4745, loss 0.359548, acc 0.90625, prec 0.082606, recall 0.795825
2017-12-10T02:56:24.646342: step 4746, loss 0.387971, acc 0.890625, prec 0.0826252, recall 0.795885
2017-12-10T02:56:24.913019: step 4747, loss 0.368859, acc 0.9375, prec 0.0826202, recall 0.795885
2017-12-10T02:56:25.177470: step 4748, loss 0.106285, acc 0.921875, prec 0.0826139, recall 0.795885
2017-12-10T02:56:25.439349: step 4749, loss 0.498574, acc 0.890625, prec 0.082605, recall 0.795885
2017-12-10T02:56:25.707032: step 4750, loss 0.699975, acc 0.953125, prec 0.0826153, recall 0.795915
2017-12-10T02:56:25.980581: step 4751, loss 0.287747, acc 0.953125, prec 0.0826115, recall 0.795915
2017-12-10T02:56:26.241990: step 4752, loss 0.296318, acc 0.953125, prec 0.0826497, recall 0.796005
2017-12-10T02:56:26.524518: step 4753, loss 0.0224466, acc 1, prec 0.0826497, recall 0.796005
2017-12-10T02:56:26.786020: step 4754, loss 0.0860122, acc 0.96875, prec 0.0826611, recall 0.796035
2017-12-10T02:56:27.059310: step 4755, loss 0.0521276, acc 0.984375, prec 0.0826739, recall 0.796065
2017-12-10T02:56:27.330413: step 4756, loss 0.0935048, acc 0.953125, prec 0.0826701, recall 0.796065
2017-12-10T02:56:27.592343: step 4757, loss 0.15974, acc 0.953125, prec 0.0826803, recall 0.796095
2017-12-10T02:56:27.856338: step 4758, loss 0.0277725, acc 0.984375, prec 0.082679, recall 0.796095
2017-12-10T02:56:28.125299: step 4759, loss 0.148974, acc 0.984375, prec 0.0827057, recall 0.796155
2017-12-10T02:56:28.398060: step 4760, loss 0.00459492, acc 1, prec 0.0827057, recall 0.796155
2017-12-10T02:56:28.667341: step 4761, loss 0.0165745, acc 1, prec 0.0827057, recall 0.796155
2017-12-10T02:56:28.933290: step 4762, loss 0.0571809, acc 0.96875, prec 0.0827312, recall 0.796215
2017-12-10T02:56:29.211766: step 4763, loss 0.300089, acc 0.984375, prec 0.0827299, recall 0.796215
2017-12-10T02:56:29.479371: step 4764, loss 0.133237, acc 0.96875, prec 0.0827414, recall 0.796245
2017-12-10T02:56:29.744709: step 4765, loss 0.0453364, acc 0.984375, prec 0.0827541, recall 0.796275
2017-12-10T02:56:30.017726: step 4766, loss 0.0314898, acc 0.984375, prec 0.0827528, recall 0.796275
2017-12-10T02:56:30.286090: step 4767, loss 0.0362614, acc 0.984375, prec 0.0827516, recall 0.796275
2017-12-10T02:56:30.557177: step 4768, loss 0.141013, acc 1, prec 0.0827795, recall 0.796334
2017-12-10T02:56:30.824999: step 4769, loss 0.0615092, acc 0.984375, prec 0.0827783, recall 0.796334
2017-12-10T02:56:31.098015: step 4770, loss 0.573199, acc 0.96875, prec 0.0828037, recall 0.796394
2017-12-10T02:56:31.369431: step 4771, loss 0.956825, acc 0.953125, prec 0.0828139, recall 0.796424
2017-12-10T02:56:31.636116: step 4772, loss 0.0411025, acc 0.984375, prec 0.0828266, recall 0.796454
2017-12-10T02:56:31.904112: step 4773, loss 0.145973, acc 0.96875, prec 0.0828381, recall 0.796484
2017-12-10T02:56:32.169831: step 4774, loss 0.155932, acc 0.953125, prec 0.0828622, recall 0.796543
2017-12-10T02:56:32.442881: step 4775, loss 0.344132, acc 0.890625, prec 0.0828674, recall 0.796573
2017-12-10T02:56:32.704656: step 4776, loss 0.41416, acc 0.921875, prec 0.082875, recall 0.796603
2017-12-10T02:56:32.970320: step 4777, loss 0.624326, acc 0.921875, prec 0.0828967, recall 0.796662
2017-12-10T02:56:33.240754: step 4778, loss 0.482104, acc 0.890625, prec 0.0829018, recall 0.796692
2017-12-10T02:56:33.503742: step 4779, loss 0.329101, acc 0.890625, prec 0.0829209, recall 0.796752
2017-12-10T02:56:33.765377: step 4780, loss 0.188441, acc 0.96875, prec 0.0829323, recall 0.796781
2017-12-10T02:56:34.032115: step 4781, loss 0.174116, acc 0.953125, prec 0.0829285, recall 0.796781
2017-12-10T02:56:34.297025: step 4782, loss 0.286122, acc 0.9375, prec 0.0829374, recall 0.796811
2017-12-10T02:56:34.562655: step 4783, loss 0.365045, acc 0.90625, prec 0.0829438, recall 0.796841
2017-12-10T02:56:34.831888: step 4784, loss 0.303119, acc 0.96875, prec 0.0829832, recall 0.79693
2017-12-10T02:56:35.089742: step 4785, loss 0.284512, acc 0.953125, prec 0.0829794, recall 0.79693
2017-12-10T02:56:35.353601: step 4786, loss 0.180096, acc 0.9375, prec 0.0830162, recall 0.797019
2017-12-10T02:56:35.618185: step 4787, loss 0.2636, acc 0.90625, prec 0.0830226, recall 0.797049
2017-12-10T02:56:35.880795: step 4788, loss 0.0506987, acc 0.984375, prec 0.0830353, recall 0.797078
2017-12-10T02:56:36.148874: step 4789, loss 0.117239, acc 0.953125, prec 0.0830315, recall 0.797078
2017-12-10T02:56:36.417064: step 4790, loss 0.0996634, acc 0.96875, prec 0.0830569, recall 0.797137
2017-12-10T02:56:36.688415: step 4791, loss 0.229927, acc 0.953125, prec 0.0830949, recall 0.797226
2017-12-10T02:56:36.956250: step 4792, loss 0.117196, acc 0.984375, prec 0.0830937, recall 0.797226
2017-12-10T02:56:37.225140: step 4793, loss 0.00911389, acc 1, prec 0.0831355, recall 0.797315
2017-12-10T02:56:37.499098: step 4794, loss 0.0636654, acc 0.96875, prec 0.083133, recall 0.797315
2017-12-10T02:56:37.762957: step 4795, loss 0.0114888, acc 1, prec 0.083147, recall 0.797345
2017-12-10T02:56:38.031705: step 4796, loss 0.0145787, acc 1, prec 0.0831749, recall 0.797404
2017-12-10T02:56:38.293973: step 4797, loss 0.147623, acc 0.953125, prec 0.083185, recall 0.797433
2017-12-10T02:56:38.561222: step 4798, loss 0.194452, acc 0.953125, prec 0.0831952, recall 0.797463
2017-12-10T02:56:38.825152: step 4799, loss 0.287693, acc 0.96875, prec 0.0831926, recall 0.797463
2017-12-10T02:56:39.087785: step 4800, loss 0.086975, acc 0.96875, prec 0.0831901, recall 0.797463

Evaluation:
2017-12-10T02:56:46.755605: step 4800, loss 5.37362, acc 0.957826, prec 0.0836659, recall 0.788237

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-4800

2017-12-10T02:56:48.032788: step 4801, loss 0.0457982, acc 0.984375, prec 0.0836646, recall 0.788237
2017-12-10T02:56:48.302021: step 4802, loss 0.278052, acc 0.9375, prec 0.0836734, recall 0.788267
2017-12-10T02:56:48.574058: step 4803, loss 0.101935, acc 0.984375, prec 0.0836721, recall 0.788267
2017-12-10T02:56:48.840186: step 4804, loss 0.0691539, acc 0.984375, prec 0.0836709, recall 0.788267
2017-12-10T02:56:49.102158: step 4805, loss 0.0970522, acc 0.96875, prec 0.0836822, recall 0.788297
2017-12-10T02:56:49.370721: step 4806, loss 0.000514658, acc 1, prec 0.083696, recall 0.788327
2017-12-10T02:56:49.634557: step 4807, loss 0.12418, acc 0.984375, prec 0.0836948, recall 0.788327
2017-12-10T02:56:49.900262: step 4808, loss 0.0325133, acc 0.984375, prec 0.0836935, recall 0.788327
2017-12-10T02:56:50.163998: step 4809, loss 0.0204791, acc 0.984375, prec 0.0836923, recall 0.788327
2017-12-10T02:56:50.426262: step 4810, loss 0.045559, acc 0.984375, prec 0.083691, recall 0.788327
2017-12-10T02:56:50.698450: step 4811, loss 0.0553433, acc 0.984375, prec 0.0836897, recall 0.788327
2017-12-10T02:56:50.961428: step 4812, loss 0.293026, acc 0.96875, prec 0.0837149, recall 0.788388
2017-12-10T02:56:51.233322: step 4813, loss 0.0805296, acc 0.953125, prec 0.0837111, recall 0.788388
2017-12-10T02:56:51.494770: step 4814, loss 0.00100559, acc 1, prec 0.0837111, recall 0.788388
2017-12-10T02:56:51.762910: step 4815, loss 0.0315744, acc 0.984375, prec 0.0837098, recall 0.788388
2017-12-10T02:56:52.028853: step 4816, loss 0.0835384, acc 0.984375, prec 0.0837224, recall 0.788418
2017-12-10T02:56:52.299207: step 4817, loss 0.712356, acc 0.984375, prec 0.083735, recall 0.788448
2017-12-10T02:56:52.569964: step 4818, loss 0.190354, acc 0.953125, prec 0.083745, recall 0.788478
2017-12-10T02:56:52.831446: step 4819, loss 0.364755, acc 0.984375, prec 0.0837576, recall 0.788508
2017-12-10T02:56:53.103360: step 4820, loss 0.161319, acc 0.96875, prec 0.0837689, recall 0.788538
2017-12-10T02:56:53.372852: step 4821, loss 0.0813118, acc 0.96875, prec 0.0837664, recall 0.788538
2017-12-10T02:56:53.635545: step 4822, loss 0.0689112, acc 0.984375, prec 0.0837651, recall 0.788538
2017-12-10T02:56:53.899598: step 4823, loss 0.164463, acc 0.96875, prec 0.0837626, recall 0.788538
2017-12-10T02:56:54.162840: step 4824, loss 0.311686, acc 0.953125, prec 0.0837726, recall 0.788568
2017-12-10T02:56:54.433528: step 4825, loss 0.043499, acc 0.984375, prec 0.083799, recall 0.788628
2017-12-10T02:56:54.698828: step 4826, loss 0.0943404, acc 0.96875, prec 0.0837965, recall 0.788628
2017-12-10T02:56:54.971993: step 4827, loss 0.131961, acc 0.96875, prec 0.083794, recall 0.788628
2017-12-10T02:56:55.233978: step 4828, loss 0.190877, acc 0.9375, prec 0.0838028, recall 0.788658
2017-12-10T02:56:55.493302: step 4829, loss 0.00182564, acc 1, prec 0.0838028, recall 0.788658
2017-12-10T02:56:55.756264: step 4830, loss 0.133636, acc 1, prec 0.0838304, recall 0.788718
2017-12-10T02:56:56.026226: step 4831, loss 0.0906557, acc 0.96875, prec 0.0838279, recall 0.788718
2017-12-10T02:56:56.289169: step 4832, loss 0.189826, acc 0.984375, prec 0.0838405, recall 0.788748
2017-12-10T02:56:56.570551: step 4833, loss 0.0161626, acc 1, prec 0.0838681, recall 0.788808
2017-12-10T02:56:56.833341: step 4834, loss 0.120179, acc 0.984375, prec 0.0838807, recall 0.788838
2017-12-10T02:56:57.095757: step 4835, loss 0.0328517, acc 1, prec 0.0838945, recall 0.788868
2017-12-10T02:56:57.360793: step 4836, loss 0.0873398, acc 0.96875, prec 0.083892, recall 0.788868
2017-12-10T02:56:57.624052: step 4837, loss 0.105188, acc 0.984375, prec 0.0838907, recall 0.788868
2017-12-10T02:56:57.885052: step 4838, loss 0.00934688, acc 1, prec 0.0838907, recall 0.788868
2017-12-10T02:56:58.144965: step 4839, loss 0.0800979, acc 0.96875, prec 0.083902, recall 0.788898
2017-12-10T02:56:58.409196: step 4840, loss 0.166713, acc 0.953125, prec 0.0839121, recall 0.788928
2017-12-10T02:56:58.673076: step 4841, loss 0.263038, acc 0.984375, prec 0.0839385, recall 0.788988
2017-12-10T02:56:58.943548: step 4842, loss 0.0869518, acc 0.984375, prec 0.083951, recall 0.789018
2017-12-10T02:56:59.215563: step 4843, loss 0.0161665, acc 1, prec 0.083951, recall 0.789018
2017-12-10T02:56:59.481956: step 4844, loss 0.11117, acc 0.984375, prec 0.0839912, recall 0.789108
2017-12-10T02:56:59.745427: step 4845, loss 0.000546065, acc 1, prec 0.0840051, recall 0.789138
2017-12-10T02:57:00.006854: step 4846, loss 0.0322005, acc 1, prec 0.0840189, recall 0.789168
2017-12-10T02:57:00.277107: step 4847, loss 0.0099572, acc 1, prec 0.0840189, recall 0.789168
2017-12-10T02:57:00.549081: step 4848, loss 0.151747, acc 0.96875, prec 0.0840302, recall 0.789198
2017-12-10T02:57:00.828913: step 4849, loss 0.0480918, acc 0.96875, prec 0.0840415, recall 0.789227
2017-12-10T02:57:01.091353: step 4850, loss 0.0938637, acc 0.984375, prec 0.0840402, recall 0.789227
2017-12-10T02:57:01.353373: step 4851, loss 0.211651, acc 0.984375, prec 0.0840666, recall 0.789287
2017-12-10T02:57:01.619600: step 4852, loss 0.0550962, acc 0.96875, prec 0.0840641, recall 0.789287
2017-12-10T02:57:01.880618: step 4853, loss 0.0905559, acc 0.984375, prec 0.0840766, recall 0.789317
2017-12-10T02:57:02.144806: step 4854, loss 0.0144751, acc 1, prec 0.0840904, recall 0.789347
2017-12-10T02:57:02.410264: step 4855, loss 0.0518015, acc 0.984375, prec 0.0841168, recall 0.789407
2017-12-10T02:57:02.683339: step 4856, loss 0.25424, acc 0.9375, prec 0.0841117, recall 0.789407
2017-12-10T02:57:02.957548: step 4857, loss 0.118332, acc 0.953125, prec 0.0841217, recall 0.789436
2017-12-10T02:57:03.222051: step 4858, loss 0.236149, acc 0.953125, prec 0.0841317, recall 0.789466
2017-12-10T02:57:03.498704: step 4859, loss 0.0866904, acc 0.984375, prec 0.0841305, recall 0.789466
2017-12-10T02:57:03.764733: step 4860, loss 0.0202552, acc 0.984375, prec 0.0841568, recall 0.789526
2017-12-10T02:57:04.030699: step 4861, loss 0.0380285, acc 0.984375, prec 0.0841832, recall 0.789585
2017-12-10T02:57:04.295298: step 4862, loss 0.0876529, acc 0.953125, prec 0.0841794, recall 0.789585
2017-12-10T02:57:04.563930: step 4863, loss 0.00817663, acc 1, prec 0.0841794, recall 0.789585
2017-12-10T02:57:04.839604: step 4864, loss 0.0889871, acc 0.953125, prec 0.0841756, recall 0.789585
2017-12-10T02:57:05.816846: step 4865, loss 0.354766, acc 0.953125, prec 0.0841718, recall 0.789585
2017-12-10T02:57:06.169171: step 4866, loss 0.0708162, acc 0.984375, prec 0.0841981, recall 0.789645
2017-12-10T02:57:06.438965: step 4867, loss 0.133023, acc 0.96875, prec 0.0841956, recall 0.789645
2017-12-10T02:57:06.702355: step 4868, loss 0.203058, acc 0.9375, prec 0.0841905, recall 0.789645
2017-12-10T02:57:07.433723: step 4869, loss 0.00987168, acc 1, prec 0.0842181, recall 0.789704
2017-12-10T02:57:08.130844: step 4870, loss 0.184785, acc 0.984375, prec 0.0842169, recall 0.789704
2017-12-10T02:57:08.895147: step 4871, loss 0.0102988, acc 1, prec 0.0842307, recall 0.789734
2017-12-10T02:57:09.333054: step 4872, loss 0.199439, acc 0.953125, prec 0.0842407, recall 0.789764
2017-12-10T02:57:09.622639: step 4873, loss 0.00312432, acc 1, prec 0.0842683, recall 0.789823
2017-12-10T02:57:09.886749: step 4874, loss 0.00572291, acc 1, prec 0.0842821, recall 0.789853
2017-12-10T02:57:10.159218: step 4875, loss 0.209532, acc 0.984375, prec 0.0842808, recall 0.789853
2017-12-10T02:57:10.422003: step 4876, loss 0.125802, acc 0.96875, prec 0.0842783, recall 0.789853
2017-12-10T02:57:10.696580: step 4877, loss 0.096855, acc 1, prec 0.0842921, recall 0.789883
2017-12-10T02:57:10.967096: step 4878, loss 0.8296, acc 0.984375, prec 0.0843046, recall 0.789912
2017-12-10T02:57:11.233077: step 4879, loss 0.181212, acc 0.953125, prec 0.0843146, recall 0.789942
2017-12-10T02:57:11.507721: step 4880, loss 1.29977, acc 0.953125, prec 0.0843259, recall 0.78986
2017-12-10T02:57:11.781708: step 4881, loss 0.0844989, acc 0.984375, prec 0.0843522, recall 0.78992
2017-12-10T02:57:12.057267: step 4882, loss 0.410642, acc 0.953125, prec 0.0843484, recall 0.78992
2017-12-10T02:57:12.325824: step 4883, loss 0.182001, acc 0.984375, prec 0.084361, recall 0.789949
2017-12-10T02:57:12.592950: step 4884, loss 0.0427509, acc 0.96875, prec 0.0843722, recall 0.789979
2017-12-10T02:57:12.869455: step 4885, loss 0.68116, acc 0.921875, prec 0.0843797, recall 0.790008
2017-12-10T02:57:13.139200: step 4886, loss 0.0865321, acc 0.953125, prec 0.0843896, recall 0.790038
2017-12-10T02:57:13.403555: step 4887, loss 0.46545, acc 0.9375, prec 0.0843984, recall 0.790068
2017-12-10T02:57:13.662065: step 4888, loss 0.0502563, acc 0.984375, prec 0.0843971, recall 0.790068
2017-12-10T02:57:13.926453: step 4889, loss 0.187699, acc 0.90625, prec 0.0844309, recall 0.790157
2017-12-10T02:57:14.191778: step 4890, loss 0.28964, acc 0.921875, prec 0.0844245, recall 0.790157
2017-12-10T02:57:14.461938: step 4891, loss 0.752754, acc 0.84375, prec 0.0844669, recall 0.790275
2017-12-10T02:57:14.729677: step 4892, loss 0.0533284, acc 0.984375, prec 0.0844795, recall 0.790304
2017-12-10T02:57:14.993800: step 4893, loss 0.205207, acc 0.9375, prec 0.0844882, recall 0.790334
2017-12-10T02:57:15.254743: step 4894, loss 0.556534, acc 0.921875, prec 0.0844818, recall 0.790334
2017-12-10T02:57:15.517529: step 4895, loss 0.0866929, acc 0.96875, prec 0.0844793, recall 0.790334
2017-12-10T02:57:15.784561: step 4896, loss 0.0654977, acc 0.984375, prec 0.0845056, recall 0.790393
2017-12-10T02:57:16.061145: step 4897, loss 0.215007, acc 0.9375, prec 0.0845143, recall 0.790423
2017-12-10T02:57:16.334774: step 4898, loss 0.129386, acc 0.96875, prec 0.0845117, recall 0.790423
2017-12-10T02:57:16.601302: step 4899, loss 0.653271, acc 0.890625, prec 0.0845028, recall 0.790423
2017-12-10T02:57:16.867031: step 4900, loss 0.161479, acc 0.921875, prec 0.0845102, recall 0.790452
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-4900

2017-12-10T02:57:18.149618: step 4901, loss 0.113311, acc 0.953125, prec 0.0845064, recall 0.790452
2017-12-10T02:57:18.419037: step 4902, loss 0.253229, acc 0.984375, prec 0.0845051, recall 0.790452
2017-12-10T02:57:18.684120: step 4903, loss 0.153141, acc 0.984375, prec 0.0845176, recall 0.790482
2017-12-10T02:57:18.962284: step 4904, loss 0.29137, acc 0.96875, prec 0.0845289, recall 0.790511
2017-12-10T02:57:19.222387: step 4905, loss 0.12312, acc 0.984375, prec 0.0845414, recall 0.790541
2017-12-10T02:57:19.492400: step 4906, loss 0.00724181, acc 1, prec 0.0845552, recall 0.79057
2017-12-10T02:57:19.754563: step 4907, loss 0.139649, acc 0.984375, prec 0.0845539, recall 0.79057
2017-12-10T02:57:20.019257: step 4908, loss 0.100819, acc 0.984375, prec 0.0845526, recall 0.79057
2017-12-10T02:57:20.283093: step 4909, loss 0.133334, acc 0.953125, prec 0.0845626, recall 0.790599
2017-12-10T02:57:20.547257: step 4910, loss 0.0871355, acc 0.984375, prec 0.0845751, recall 0.790629
2017-12-10T02:57:20.821816: step 4911, loss 0.106857, acc 0.984375, prec 0.0846014, recall 0.790688
2017-12-10T02:57:21.096713: step 4912, loss 0.0937456, acc 0.96875, prec 0.0845988, recall 0.790688
2017-12-10T02:57:21.366129: step 4913, loss 0.137471, acc 0.96875, prec 0.0845963, recall 0.790688
2017-12-10T02:57:21.630302: step 4914, loss 0.208916, acc 0.96875, prec 0.0846488, recall 0.790806
2017-12-10T02:57:21.898013: step 4915, loss 0.234525, acc 0.96875, prec 0.0846601, recall 0.790835
2017-12-10T02:57:22.166140: step 4916, loss 0.00766212, acc 1, prec 0.0846601, recall 0.790835
2017-12-10T02:57:22.428091: step 4917, loss 0.152444, acc 0.984375, prec 0.0846588, recall 0.790835
2017-12-10T02:57:22.691709: step 4918, loss 0.105498, acc 0.953125, prec 0.084655, recall 0.790835
2017-12-10T02:57:22.956460: step 4919, loss 0.0240158, acc 1, prec 0.0846687, recall 0.790864
2017-12-10T02:57:23.220990: step 4920, loss 0.176063, acc 0.9375, prec 0.0846636, recall 0.790864
2017-12-10T02:57:23.488140: step 4921, loss 7.29348, acc 0.984375, prec 0.0846912, recall 0.790812
2017-12-10T02:57:23.766050: step 4922, loss 0.406028, acc 0.9375, prec 0.0847136, recall 0.790871
2017-12-10T02:57:24.041928: step 4923, loss 0.0262256, acc 0.984375, prec 0.0847261, recall 0.7909
2017-12-10T02:57:24.311287: step 4924, loss 0.0934793, acc 0.9375, prec 0.0847348, recall 0.790929
2017-12-10T02:57:24.576726: step 4925, loss 0.577553, acc 0.890625, prec 0.0847396, recall 0.790959
2017-12-10T02:57:24.841597: step 4926, loss 0.475406, acc 0.90625, prec 0.084732, recall 0.790959
2017-12-10T02:57:25.103495: step 4927, loss 0.209172, acc 0.953125, prec 0.0847832, recall 0.791076
2017-12-10T02:57:25.372946: step 4928, loss 0.109662, acc 0.96875, prec 0.0848082, recall 0.791135
2017-12-10T02:57:25.632421: step 4929, loss 0.116415, acc 0.953125, prec 0.0848044, recall 0.791135
2017-12-10T02:57:25.905003: step 4930, loss 0.18748, acc 0.953125, prec 0.0848281, recall 0.791193
2017-12-10T02:57:26.175820: step 4931, loss 0.845754, acc 0.78125, prec 0.084824, recall 0.791223
2017-12-10T02:57:26.441632: step 4932, loss 0.476887, acc 0.828125, prec 0.0848237, recall 0.791252
2017-12-10T02:57:26.711112: step 4933, loss 0.668841, acc 0.90625, prec 0.0848161, recall 0.791252
2017-12-10T02:57:26.976979: step 4934, loss 0.236722, acc 0.890625, prec 0.0848071, recall 0.791252
2017-12-10T02:57:27.239695: step 4935, loss 0.56154, acc 0.84375, prec 0.0847944, recall 0.791252
2017-12-10T02:57:27.504803: step 4936, loss 0.479431, acc 0.90625, prec 0.0847868, recall 0.791252
2017-12-10T02:57:27.773285: step 4937, loss 0.575788, acc 0.890625, prec 0.0847778, recall 0.791252
2017-12-10T02:57:28.038628: step 4938, loss 0.314622, acc 0.921875, prec 0.084799, recall 0.79131
2017-12-10T02:57:28.299458: step 4939, loss 0.493532, acc 0.953125, prec 0.0847951, recall 0.79131
2017-12-10T02:57:28.563146: step 4940, loss 0.0124999, acc 1, prec 0.0847951, recall 0.79131
2017-12-10T02:57:28.825839: step 4941, loss 0.195979, acc 0.96875, prec 0.0847926, recall 0.79131
2017-12-10T02:57:29.089726: step 4942, loss 0.139644, acc 0.96875, prec 0.0847901, recall 0.79131
2017-12-10T02:57:29.357331: step 4943, loss 0.153889, acc 0.9375, prec 0.0847987, recall 0.79134
2017-12-10T02:57:29.617316: step 4944, loss 0.244481, acc 0.96875, prec 0.0847962, recall 0.79134
2017-12-10T02:57:29.883686: step 4945, loss 0.194425, acc 0.96875, prec 0.0847936, recall 0.79134
2017-12-10T02:57:30.160082: step 4946, loss 0.0212625, acc 1, prec 0.0848348, recall 0.791427
2017-12-10T02:57:30.428622: step 4947, loss 0.174933, acc 0.96875, prec 0.084846, recall 0.791457
2017-12-10T02:57:30.696723: step 4948, loss 0.390996, acc 1, prec 0.0848598, recall 0.791486
2017-12-10T02:57:30.962553: step 4949, loss 0.0404266, acc 0.96875, prec 0.084871, recall 0.791515
2017-12-10T02:57:31.233749: step 4950, loss 0.00656334, acc 1, prec 0.084871, recall 0.791515
2017-12-10T02:57:31.496950: step 4951, loss 0.354706, acc 0.953125, prec 0.0848809, recall 0.791544
2017-12-10T02:57:31.758025: step 4952, loss 0.00271932, acc 1, prec 0.0848809, recall 0.791544
2017-12-10T02:57:32.020130: step 4953, loss 0.142707, acc 0.90625, prec 0.0849007, recall 0.791602
2017-12-10T02:57:32.286409: step 4954, loss 0.101559, acc 0.96875, prec 0.0848982, recall 0.791602
2017-12-10T02:57:32.550566: step 4955, loss 0.381296, acc 0.984375, prec 0.0849656, recall 0.791748
2017-12-10T02:57:32.822856: step 4956, loss 0.0762135, acc 0.984375, prec 0.0849643, recall 0.791748
2017-12-10T02:57:33.083749: step 4957, loss 0.425435, acc 0.96875, prec 0.0850029, recall 0.791836
2017-12-10T02:57:33.346611: step 4958, loss 0.016674, acc 1, prec 0.0850167, recall 0.791865
2017-12-10T02:57:33.608360: step 4959, loss 0.088583, acc 0.984375, prec 0.0850154, recall 0.791865
2017-12-10T02:57:33.875522: step 4960, loss 0.0598847, acc 0.96875, prec 0.0850403, recall 0.791923
2017-12-10T02:57:34.139669: step 4961, loss 0.0974733, acc 0.96875, prec 0.0850515, recall 0.791952
2017-12-10T02:57:34.403916: step 4962, loss 0.00788006, acc 1, prec 0.0850515, recall 0.791952
2017-12-10T02:57:34.668752: step 4963, loss 0.153521, acc 0.953125, prec 0.0850476, recall 0.791952
2017-12-10T02:57:34.932749: step 4964, loss 0.0989159, acc 0.96875, prec 0.0850588, recall 0.791981
2017-12-10T02:57:35.198126: step 4965, loss 0.0403237, acc 0.984375, prec 0.0850575, recall 0.791981
2017-12-10T02:57:35.463131: step 4966, loss 0.109988, acc 0.984375, prec 0.08507, recall 0.79201
2017-12-10T02:57:35.725735: step 4967, loss 0.0842535, acc 0.96875, prec 0.0850674, recall 0.79201
2017-12-10T02:57:35.994101: step 4968, loss 0.12456, acc 0.984375, prec 0.0851073, recall 0.792097
2017-12-10T02:57:36.266609: step 4969, loss 0.110749, acc 0.953125, prec 0.0851035, recall 0.792097
2017-12-10T02:57:36.493735: step 4970, loss 0.0101222, acc 1, prec 0.085131, recall 0.792155
2017-12-10T02:57:36.764303: step 4971, loss 0.0348351, acc 0.984375, prec 0.0851434, recall 0.792184
2017-12-10T02:57:37.028788: step 4972, loss 0.0516158, acc 0.984375, prec 0.0851559, recall 0.792213
2017-12-10T02:57:37.297692: step 4973, loss 0.048726, acc 0.96875, prec 0.0851533, recall 0.792213
2017-12-10T02:57:37.559298: step 4974, loss 0.301967, acc 0.984375, prec 0.0851657, recall 0.792242
2017-12-10T02:57:37.824780: step 4975, loss 0.13876, acc 0.984375, prec 0.0851919, recall 0.7923
2017-12-10T02:57:38.091729: step 4976, loss 0.323408, acc 0.96875, prec 0.0852442, recall 0.792416
2017-12-10T02:57:38.362840: step 4977, loss 1.33736, acc 0.96875, prec 0.0852691, recall 0.792474
2017-12-10T02:57:38.633053: step 4978, loss 0.0752679, acc 0.984375, prec 0.0852816, recall 0.792503
2017-12-10T02:57:38.893209: step 4979, loss 0.0415785, acc 0.96875, prec 0.0852927, recall 0.792532
2017-12-10T02:57:39.161635: step 4980, loss 0.197737, acc 0.90625, prec 0.0852987, recall 0.792561
2017-12-10T02:57:39.427496: step 4981, loss 0.429089, acc 0.9375, prec 0.0853073, recall 0.792589
2017-12-10T02:57:39.695656: step 4982, loss 0.257126, acc 0.96875, prec 0.0853322, recall 0.792647
2017-12-10T02:57:39.961304: step 4983, loss 0.536295, acc 0.9375, prec 0.0853271, recall 0.792647
2017-12-10T02:57:40.229512: step 4984, loss 0.136456, acc 0.96875, prec 0.0853382, recall 0.792676
2017-12-10T02:57:40.500011: step 4985, loss 0.435801, acc 0.921875, prec 0.0853319, recall 0.792676
2017-12-10T02:57:40.761874: step 4986, loss 0.169989, acc 0.953125, prec 0.085328, recall 0.792676
2017-12-10T02:57:41.024685: step 4987, loss 0.147179, acc 0.984375, prec 0.0853404, recall 0.792705
2017-12-10T02:57:41.291775: step 4988, loss 0.0834848, acc 0.984375, prec 0.0853529, recall 0.792734
2017-12-10T02:57:41.565469: step 4989, loss 0.21514, acc 0.953125, prec 0.0853765, recall 0.792792
2017-12-10T02:57:41.831581: step 4990, loss 0.276115, acc 0.9375, prec 0.085385, recall 0.79282
2017-12-10T02:57:42.097009: step 4991, loss 0.167595, acc 0.953125, prec 0.0853812, recall 0.79282
2017-12-10T02:57:42.365815: step 4992, loss 0.0652872, acc 0.984375, prec 0.0853936, recall 0.792849
2017-12-10T02:57:42.636510: step 4993, loss 0.132904, acc 0.953125, prec 0.0854172, recall 0.792907
2017-12-10T02:57:42.898374: step 4994, loss 0.0889105, acc 0.96875, prec 0.085442, recall 0.792964
2017-12-10T02:57:43.159379: step 4995, loss 0.00666538, acc 1, prec 0.085442, recall 0.792964
2017-12-10T02:57:43.425167: step 4996, loss 0.0340844, acc 0.984375, prec 0.0854545, recall 0.792993
2017-12-10T02:57:43.684642: step 4997, loss 0.0203918, acc 0.984375, prec 0.0854669, recall 0.793022
2017-12-10T02:57:43.945817: step 4998, loss 0.150749, acc 0.953125, prec 0.0854904, recall 0.793079
2017-12-10T02:57:44.209930: step 4999, loss 0.746299, acc 0.953125, prec 0.0854866, recall 0.793079
2017-12-10T02:57:44.469992: step 5000, loss 0.0183288, acc 0.984375, prec 0.0854853, recall 0.793079
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-5000

2017-12-10T02:57:45.886657: step 5001, loss 0.766161, acc 0.96875, prec 0.0855102, recall 0.793137
2017-12-10T02:57:46.159671: step 5002, loss 0.202011, acc 0.953125, prec 0.0855063, recall 0.793137
2017-12-10T02:57:46.423507: step 5003, loss 0.00431877, acc 1, prec 0.08552, recall 0.793166
2017-12-10T02:57:46.695996: step 5004, loss 0.0843439, acc 0.953125, prec 0.0855299, recall 0.793194
2017-12-10T02:57:46.966848: step 5005, loss 0.096398, acc 0.953125, prec 0.0855534, recall 0.793252
2017-12-10T02:57:47.234671: step 5006, loss 0.0440179, acc 0.96875, prec 0.0855508, recall 0.793252
2017-12-10T02:57:47.506283: step 5007, loss 0.0958438, acc 0.953125, prec 0.085547, recall 0.793252
2017-12-10T02:57:47.774973: step 5008, loss 0.0103063, acc 1, prec 0.0855607, recall 0.793281
2017-12-10T02:57:48.044761: step 5009, loss 0.0409551, acc 0.984375, prec 0.0855594, recall 0.793281
2017-12-10T02:57:48.307398: step 5010, loss 0.0288021, acc 0.984375, prec 0.0855855, recall 0.793338
2017-12-10T02:57:48.573445: step 5011, loss 0.0178706, acc 1, prec 0.0855992, recall 0.793367
2017-12-10T02:57:48.833705: step 5012, loss 0.0155472, acc 1, prec 0.0856266, recall 0.793424
2017-12-10T02:57:49.105591: step 5013, loss 0.124066, acc 0.953125, prec 0.0856364, recall 0.793453
2017-12-10T02:57:49.365326: step 5014, loss 0.0321982, acc 0.984375, prec 0.0856352, recall 0.793453
2017-12-10T02:57:49.626583: step 5015, loss 0.159511, acc 0.953125, prec 0.085645, recall 0.793481
2017-12-10T02:57:49.895643: step 5016, loss 0.052816, acc 0.984375, prec 0.0856711, recall 0.793539
2017-12-10T02:57:50.165802: step 5017, loss 0.00591727, acc 1, prec 0.0856711, recall 0.793539
2017-12-10T02:57:50.427483: step 5018, loss 0.246372, acc 0.984375, prec 0.0856835, recall 0.793567
2017-12-10T02:57:50.693146: step 5019, loss 0.245293, acc 0.9375, prec 0.085692, recall 0.793596
2017-12-10T02:57:50.957973: step 5020, loss 0.175229, acc 0.96875, prec 0.0857032, recall 0.793624
2017-12-10T02:57:51.218221: step 5021, loss 0.180753, acc 0.96875, prec 0.0857143, recall 0.793653
2017-12-10T02:57:51.486141: step 5022, loss 0.253704, acc 0.953125, prec 0.0857104, recall 0.793653
2017-12-10T02:57:51.753364: step 5023, loss 0.000910572, acc 1, prec 0.0857241, recall 0.793682
2017-12-10T02:57:52.015560: step 5024, loss 0.0172793, acc 0.984375, prec 0.0857228, recall 0.793682
2017-12-10T02:57:52.281784: step 5025, loss 0.00693376, acc 1, prec 0.0857365, recall 0.79371
2017-12-10T02:57:52.545402: step 5026, loss 0.0747425, acc 0.96875, prec 0.085734, recall 0.79371
2017-12-10T02:57:52.814074: step 5027, loss 0.00460549, acc 1, prec 0.0857476, recall 0.793739
2017-12-10T02:57:53.084469: step 5028, loss 1.51806, acc 0.984375, prec 0.0857476, recall 0.793629
2017-12-10T02:57:53.350041: step 5029, loss 0.0563437, acc 0.96875, prec 0.0857724, recall 0.793686
2017-12-10T02:57:53.617472: step 5030, loss 0.0146953, acc 0.984375, prec 0.0857712, recall 0.793686
2017-12-10T02:57:53.884063: step 5031, loss 0.0266957, acc 0.984375, prec 0.0857835, recall 0.793715
2017-12-10T02:57:54.145906: step 5032, loss 0.0579911, acc 0.96875, prec 0.0857947, recall 0.793743
2017-12-10T02:57:54.412699: step 5033, loss 0.10057, acc 0.96875, prec 0.0858058, recall 0.793772
2017-12-10T02:57:54.677018: step 5034, loss 0.205827, acc 0.984375, prec 0.0858182, recall 0.7938
2017-12-10T02:57:54.939223: step 5035, loss 0.0413183, acc 0.984375, prec 0.0858169, recall 0.7938
2017-12-10T02:57:55.199326: step 5036, loss 0.179804, acc 0.921875, prec 0.0858241, recall 0.793829
2017-12-10T02:57:55.462549: step 5037, loss 0.047691, acc 0.96875, prec 0.0858216, recall 0.793829
2017-12-10T02:57:55.722749: step 5038, loss 0.227199, acc 0.921875, prec 0.0858151, recall 0.793829
2017-12-10T02:57:55.987348: step 5039, loss 0.282386, acc 0.953125, prec 0.0858113, recall 0.793829
2017-12-10T02:57:56.259439: step 5040, loss 3.0793, acc 0.875, prec 0.085816, recall 0.793747
2017-12-10T02:57:56.540455: step 5041, loss 0.190665, acc 0.921875, prec 0.0858096, recall 0.793747
2017-12-10T02:57:56.809130: step 5042, loss 0.271689, acc 0.921875, prec 0.0858168, recall 0.793776
2017-12-10T02:57:57.071569: step 5043, loss 0.248886, acc 0.9375, prec 0.0858254, recall 0.793804
2017-12-10T02:57:57.342240: step 5044, loss 0.175663, acc 0.953125, prec 0.0858352, recall 0.793833
2017-12-10T02:57:57.612330: step 5045, loss 0.053487, acc 0.984375, prec 0.0858339, recall 0.793833
2017-12-10T02:57:57.915250: step 5046, loss 0.312735, acc 0.921875, prec 0.0858411, recall 0.793861
2017-12-10T02:57:58.196431: step 5047, loss 0.195712, acc 0.921875, prec 0.0858757, recall 0.793947
2017-12-10T02:57:58.459998: step 5048, loss 0.339234, acc 0.90625, prec 0.0858817, recall 0.793975
2017-12-10T02:57:58.725131: step 5049, loss 0.585237, acc 0.90625, prec 0.085874, recall 0.793975
2017-12-10T02:57:58.994498: step 5050, loss 0.264815, acc 0.875, prec 0.085891, recall 0.794032
2017-12-10T02:57:59.262103: step 5051, loss 0.440486, acc 0.90625, prec 0.085897, recall 0.794061
2017-12-10T02:57:59.524099: step 5052, loss 0.319638, acc 0.96875, prec 0.0859081, recall 0.794089
2017-12-10T02:57:59.788925: step 5053, loss 0.504976, acc 0.921875, prec 0.0859426, recall 0.794174
2017-12-10T02:58:00.069934: step 5054, loss 0.353843, acc 0.9375, prec 0.0860058, recall 0.794316
2017-12-10T02:58:00.351189: step 5055, loss 0.183184, acc 0.9375, prec 0.0860006, recall 0.794316
2017-12-10T02:58:00.620272: step 5056, loss 0.338945, acc 0.875, prec 0.086004, recall 0.794345
2017-12-10T02:58:00.887374: step 5057, loss 0.0437259, acc 0.984375, prec 0.0860164, recall 0.794373
2017-12-10T02:58:01.162548: step 5058, loss 0.068936, acc 0.96875, prec 0.0860274, recall 0.794402
2017-12-10T02:58:01.430587: step 5059, loss 0.00622552, acc 1, prec 0.0860411, recall 0.79443
2017-12-10T02:58:01.693671: step 5060, loss 0.0461075, acc 0.953125, prec 0.0860509, recall 0.794458
2017-12-10T02:58:01.958774: step 5061, loss 0.0422714, acc 1, prec 0.0860782, recall 0.794515
2017-12-10T02:58:02.223017: step 5062, loss 0.0813199, acc 0.9375, prec 0.086073, recall 0.794515
2017-12-10T02:58:02.483996: step 5063, loss 0.105646, acc 0.96875, prec 0.0860705, recall 0.794515
2017-12-10T02:58:02.751508: step 5064, loss 0.233151, acc 0.9375, prec 0.0860653, recall 0.794515
2017-12-10T02:58:03.025848: step 5065, loss 0.0380553, acc 0.984375, prec 0.086064, recall 0.794515
2017-12-10T02:58:03.289367: step 5066, loss 0.024553, acc 0.984375, prec 0.0860628, recall 0.794515
2017-12-10T02:58:03.556640: step 5067, loss 0.0527017, acc 0.96875, prec 0.0860602, recall 0.794515
2017-12-10T02:58:03.822120: step 5068, loss 0.00207237, acc 1, prec 0.0860602, recall 0.794515
2017-12-10T02:58:04.083840: step 5069, loss 0.0956367, acc 0.96875, prec 0.0860576, recall 0.794515
2017-12-10T02:58:04.348951: step 5070, loss 0.113681, acc 0.96875, prec 0.0860687, recall 0.794543
2017-12-10T02:58:04.612205: step 5071, loss 0.647486, acc 0.96875, prec 0.0861207, recall 0.794656
2017-12-10T02:58:04.885021: step 5072, loss 0.019305, acc 1, prec 0.0861616, recall 0.794741
2017-12-10T02:58:05.149535: step 5073, loss 0.0778608, acc 0.984375, prec 0.086174, recall 0.794769
2017-12-10T02:58:05.412289: step 5074, loss 0.00491905, acc 1, prec 0.086174, recall 0.794769
2017-12-10T02:58:05.679367: step 5075, loss 0.00930472, acc 1, prec 0.086174, recall 0.794769
2017-12-10T02:58:05.957605: step 5076, loss 0.00673318, acc 1, prec 0.086174, recall 0.794769
2017-12-10T02:58:06.223564: step 5077, loss 0.0159554, acc 1, prec 0.0862149, recall 0.794854
2017-12-10T02:58:06.489659: step 5078, loss 0.0125418, acc 0.984375, prec 0.0862136, recall 0.794854
2017-12-10T02:58:06.765462: step 5079, loss 0.160074, acc 1, prec 0.0862409, recall 0.794911
2017-12-10T02:58:07.034479: step 5080, loss 0.0532981, acc 0.984375, prec 0.0862396, recall 0.794911
2017-12-10T02:58:07.299755: step 5081, loss 0.268215, acc 0.96875, prec 0.0862506, recall 0.794939
2017-12-10T02:58:07.563765: step 5082, loss 0.047994, acc 1, prec 0.0862643, recall 0.794967
2017-12-10T02:58:07.830141: step 5083, loss 0.200874, acc 0.9375, prec 0.0862728, recall 0.794995
2017-12-10T02:58:08.099954: step 5084, loss 0.0224399, acc 0.984375, prec 0.0862851, recall 0.795023
2017-12-10T02:58:08.369463: step 5085, loss 0.0245858, acc 0.984375, prec 0.0863111, recall 0.79508
2017-12-10T02:58:08.640896: step 5086, loss 0.0804412, acc 0.96875, prec 0.0863085, recall 0.79508
2017-12-10T02:58:08.905322: step 5087, loss 0.0785781, acc 0.984375, prec 0.0863208, recall 0.795108
2017-12-10T02:58:09.168022: step 5088, loss 0.0852823, acc 0.984375, prec 0.0863196, recall 0.795108
2017-12-10T02:58:09.436629: step 5089, loss 0.165842, acc 0.96875, prec 0.0863306, recall 0.795136
2017-12-10T02:58:09.702854: step 5090, loss 0.0774184, acc 0.984375, prec 0.086343, recall 0.795164
2017-12-10T02:58:09.967248: step 5091, loss 0.187441, acc 1, prec 0.0863566, recall 0.795192
2017-12-10T02:58:10.235242: step 5092, loss 0.0216056, acc 1, prec 0.0863702, recall 0.79522
2017-12-10T02:58:10.497827: step 5093, loss 0.0922229, acc 1, prec 0.0864111, recall 0.795305
2017-12-10T02:58:10.763181: step 5094, loss 0.0311019, acc 0.984375, prec 0.0864234, recall 0.795333
2017-12-10T02:58:11.035511: step 5095, loss 0.0128804, acc 1, prec 0.0864234, recall 0.795333
2017-12-10T02:58:11.300957: step 5096, loss 0.00642493, acc 1, prec 0.0864371, recall 0.795361
2017-12-10T02:58:11.576004: step 5097, loss 0.116545, acc 0.984375, prec 0.086463, recall 0.795417
2017-12-10T02:58:11.841470: step 5098, loss 0.153113, acc 0.921875, prec 0.0864702, recall 0.795445
2017-12-10T02:58:12.106835: step 5099, loss 0.00840382, acc 1, prec 0.0864702, recall 0.795445
2017-12-10T02:58:12.368421: step 5100, loss 0.182561, acc 0.953125, prec 0.0864663, recall 0.795445

Evaluation:
2017-12-10T02:58:19.905924: step 5100, loss 6.25424, acc 0.96377, prec 0.0868468, recall 0.785187

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-5100

2017-12-10T02:58:21.267506: step 5101, loss 0.00809293, acc 1, prec 0.0868468, recall 0.785187
2017-12-10T02:58:21.533191: step 5102, loss 0.151992, acc 0.984375, prec 0.0868591, recall 0.785216
2017-12-10T02:58:21.804611: step 5103, loss 0.0687257, acc 0.984375, prec 0.0868714, recall 0.785245
2017-12-10T02:58:22.068270: step 5104, loss 0.00521868, acc 1, prec 0.0868849, recall 0.785274
2017-12-10T02:58:22.344736: step 5105, loss 0.109453, acc 0.984375, prec 0.0869107, recall 0.785331
2017-12-10T02:58:22.610910: step 5106, loss 0.00461514, acc 1, prec 0.0869243, recall 0.78536
2017-12-10T02:58:22.880006: step 5107, loss 0.0154115, acc 1, prec 0.0869378, recall 0.785389
2017-12-10T02:58:23.141406: step 5108, loss 0.197098, acc 0.984375, prec 0.0869636, recall 0.785446
2017-12-10T02:58:23.413887: step 5109, loss 0.184676, acc 0.96875, prec 0.086961, recall 0.785446
2017-12-10T02:58:23.683166: step 5110, loss 0.00613777, acc 1, prec 0.0869881, recall 0.785504
2017-12-10T02:58:23.952481: step 5111, loss 0.00763421, acc 1, prec 0.0869881, recall 0.785504
2017-12-10T02:58:24.216181: step 5112, loss 0.307436, acc 0.984375, prec 0.0870681, recall 0.785676
2017-12-10T02:58:24.482085: step 5113, loss 0.027327, acc 1, prec 0.0870816, recall 0.785705
2017-12-10T02:58:24.755607: step 5114, loss 0.143573, acc 0.984375, prec 0.0870804, recall 0.785705
2017-12-10T02:58:25.018593: step 5115, loss 0.0385504, acc 0.984375, prec 0.0870791, recall 0.785705
2017-12-10T02:58:25.289518: step 5116, loss 0.0868219, acc 0.984375, prec 0.0870778, recall 0.785705
2017-12-10T02:58:25.554751: step 5117, loss 0.00749875, acc 1, prec 0.0870778, recall 0.785705
2017-12-10T02:58:25.820273: step 5118, loss 0.289162, acc 0.96875, prec 0.0870887, recall 0.785733
2017-12-10T02:58:26.084640: step 5119, loss 0.0649914, acc 0.984375, prec 0.0870874, recall 0.785733
2017-12-10T02:58:26.360263: step 5120, loss 0.00363747, acc 1, prec 0.0870874, recall 0.785733
2017-12-10T02:58:26.635011: step 5121, loss 0.294526, acc 0.9375, prec 0.0870958, recall 0.785762
2017-12-10T02:58:26.899576: step 5122, loss 0.0193574, acc 1, prec 0.0871094, recall 0.785791
2017-12-10T02:58:27.160414: step 5123, loss 0.21916, acc 0.96875, prec 0.0871068, recall 0.785791
2017-12-10T02:58:27.434680: step 5124, loss 0.0150812, acc 0.984375, prec 0.0871055, recall 0.785791
2017-12-10T02:58:27.703964: step 5125, loss 0.0121789, acc 1, prec 0.0871326, recall 0.785848
2017-12-10T02:58:27.962692: step 5126, loss 0.0818736, acc 0.984375, prec 0.0871313, recall 0.785848
2017-12-10T02:58:28.228649: step 5127, loss 0.160401, acc 0.984375, prec 0.0871435, recall 0.785877
2017-12-10T02:58:28.497334: step 5128, loss 0.0310225, acc 0.984375, prec 0.0871422, recall 0.785877
2017-12-10T02:58:28.758745: step 5129, loss 0.23302, acc 0.953125, prec 0.0871519, recall 0.785905
2017-12-10T02:58:29.020192: step 5130, loss 0.0577771, acc 0.984375, prec 0.0871506, recall 0.785905
2017-12-10T02:58:29.288065: step 5131, loss 4.93534, acc 0.9375, prec 0.0871603, recall 0.785829
2017-12-10T02:58:29.556654: step 5132, loss 0.0661565, acc 0.96875, prec 0.0871847, recall 0.785886
2017-12-10T02:58:29.831694: step 5133, loss 0.116954, acc 0.953125, prec 0.0871809, recall 0.785886
2017-12-10T02:58:30.103392: step 5134, loss 0.0756425, acc 0.953125, prec 0.087177, recall 0.785886
2017-12-10T02:58:30.382186: step 5135, loss 0.310845, acc 0.921875, prec 0.087184, recall 0.785915
2017-12-10T02:58:30.649283: step 5136, loss 0.159825, acc 0.9375, prec 0.0872059, recall 0.785972
2017-12-10T02:58:30.911420: step 5137, loss 0.512816, acc 0.921875, prec 0.087213, recall 0.786001
2017-12-10T02:58:31.176872: step 5138, loss 0.140137, acc 0.953125, prec 0.0872227, recall 0.786029
2017-12-10T02:58:31.449425: step 5139, loss 0.0453261, acc 0.96875, prec 0.0872336, recall 0.786058
2017-12-10T02:58:31.716472: step 5140, loss 0.353794, acc 0.921875, prec 0.0872271, recall 0.786058
2017-12-10T02:58:31.982933: step 5141, loss 0.699377, acc 0.828125, prec 0.0872129, recall 0.786058
2017-12-10T02:58:32.256652: step 5142, loss 0.486718, acc 0.875, prec 0.0872026, recall 0.786058
2017-12-10T02:58:32.521550: step 5143, loss 0.464479, acc 0.90625, prec 0.0871948, recall 0.786058
2017-12-10T02:58:32.787987: step 5144, loss 0.285105, acc 0.890625, prec 0.0871993, recall 0.786086
2017-12-10T02:58:33.054463: step 5145, loss 0.157101, acc 0.9375, prec 0.0872347, recall 0.786172
2017-12-10T02:58:33.317375: step 5146, loss 0.199529, acc 0.921875, prec 0.0872418, recall 0.7862
2017-12-10T02:58:33.588636: step 5147, loss 0.475821, acc 0.96875, prec 0.0872392, recall 0.7862
2017-12-10T02:58:33.864490: step 5148, loss 0.137764, acc 0.9375, prec 0.0872475, recall 0.786229
2017-12-10T02:58:34.128229: step 5149, loss 0.195799, acc 0.953125, prec 0.0872842, recall 0.786315
2017-12-10T02:58:34.392086: step 5150, loss 0.111443, acc 0.984375, prec 0.0872829, recall 0.786315
2017-12-10T02:58:34.663851: step 5151, loss 0.518578, acc 0.90625, prec 0.0872887, recall 0.786343
2017-12-10T02:58:34.931385: step 5152, loss 0.0495484, acc 0.984375, prec 0.0872874, recall 0.786343
2017-12-10T02:58:35.197929: step 5153, loss 0.209795, acc 0.96875, prec 0.0872848, recall 0.786343
2017-12-10T02:58:35.469973: step 5154, loss 0.101837, acc 0.953125, prec 0.0872809, recall 0.786343
2017-12-10T02:58:35.735628: step 5155, loss 0.0813056, acc 0.984375, prec 0.0872931, recall 0.786372
2017-12-10T02:58:35.999572: step 5156, loss 0.0792344, acc 0.96875, prec 0.0872905, recall 0.786372
2017-12-10T02:58:36.265565: step 5157, loss 0.558009, acc 0.953125, prec 0.0873272, recall 0.786457
2017-12-10T02:58:36.529961: step 5158, loss 0.0785208, acc 0.96875, prec 0.0873381, recall 0.786485
2017-12-10T02:58:36.798663: step 5159, loss 0.0191991, acc 0.984375, prec 0.0873368, recall 0.786485
2017-12-10T02:58:37.067171: step 5160, loss 0.00234375, acc 1, prec 0.0873638, recall 0.786542
2017-12-10T02:58:37.330422: step 5161, loss 0.0979394, acc 0.9375, prec 0.0873857, recall 0.786599
2017-12-10T02:58:37.603180: step 5162, loss 0.230741, acc 0.96875, prec 0.0873831, recall 0.786599
2017-12-10T02:58:37.871190: step 5163, loss 0.203236, acc 0.96875, prec 0.087394, recall 0.786628
2017-12-10T02:58:38.138184: step 5164, loss 0.185401, acc 0.96875, prec 0.0873914, recall 0.786628
2017-12-10T02:58:38.405990: step 5165, loss 0.0314261, acc 0.984375, prec 0.0873901, recall 0.786628
2017-12-10T02:58:38.669934: step 5166, loss 0.272567, acc 0.96875, prec 0.0873875, recall 0.786628
2017-12-10T02:58:38.944918: step 5167, loss 0.172405, acc 0.984375, prec 0.0873998, recall 0.786656
2017-12-10T02:58:39.208519: step 5168, loss 0.000682323, acc 1, prec 0.0873998, recall 0.786656
2017-12-10T02:58:39.470033: step 5169, loss 0.02695, acc 0.984375, prec 0.0873985, recall 0.786656
2017-12-10T02:58:39.732321: step 5170, loss 0.000407895, acc 1, prec 0.0873985, recall 0.786656
2017-12-10T02:58:39.993247: step 5171, loss 0.53201, acc 0.984375, prec 0.0874377, recall 0.786741
2017-12-10T02:58:40.258521: step 5172, loss 0.513308, acc 0.984375, prec 0.0874499, recall 0.78677
2017-12-10T02:58:40.521763: step 5173, loss 0.147958, acc 0.984375, prec 0.0874486, recall 0.78677
2017-12-10T02:58:40.794838: step 5174, loss 0.00739947, acc 1, prec 0.0874621, recall 0.786798
2017-12-10T02:58:41.057481: step 5175, loss 0.21568, acc 0.984375, prec 0.0874743, recall 0.786826
2017-12-10T02:58:41.332548: step 5176, loss 0.0349533, acc 1, prec 0.0874878, recall 0.786855
2017-12-10T02:58:41.612085: step 5177, loss 0.0651704, acc 0.96875, prec 0.0874852, recall 0.786855
2017-12-10T02:58:41.875693: step 5178, loss 0.179424, acc 0.953125, prec 0.0874813, recall 0.786855
2017-12-10T02:58:42.143914: step 5179, loss 0.0654687, acc 0.96875, prec 0.0874787, recall 0.786855
2017-12-10T02:58:42.411029: step 5180, loss 0.346294, acc 0.96875, prec 0.0874896, recall 0.786883
2017-12-10T02:58:42.686246: step 5181, loss 0.082645, acc 0.953125, prec 0.0874993, recall 0.786911
2017-12-10T02:58:42.949493: step 5182, loss 0.00219691, acc 1, prec 0.0874993, recall 0.786911
2017-12-10T02:58:43.209695: step 5183, loss 0.00410236, acc 1, prec 0.0874993, recall 0.786911
2017-12-10T02:58:43.483991: step 5184, loss 0.102993, acc 0.96875, prec 0.0874967, recall 0.786911
2017-12-10T02:58:43.744968: step 5185, loss 0.104232, acc 0.953125, prec 0.0874928, recall 0.786911
2017-12-10T02:58:44.013169: step 5186, loss 0.0738228, acc 0.984375, prec 0.0874915, recall 0.786911
2017-12-10T02:58:44.277750: step 5187, loss 0.0228505, acc 1, prec 0.0875185, recall 0.786968
2017-12-10T02:58:44.541297: step 5188, loss 0.0651972, acc 0.984375, prec 0.0875442, recall 0.787025
2017-12-10T02:58:44.806782: step 5189, loss 0.115358, acc 0.984375, prec 0.0875564, recall 0.787053
2017-12-10T02:58:45.074982: step 5190, loss 0.03645, acc 0.984375, prec 0.0875686, recall 0.787081
2017-12-10T02:58:45.339554: step 5191, loss 0.0438891, acc 0.984375, prec 0.0875673, recall 0.787081
2017-12-10T02:58:45.605859: step 5192, loss 0.0180829, acc 0.984375, prec 0.087566, recall 0.787081
2017-12-10T02:58:45.873720: step 5193, loss 0.0107844, acc 1, prec 0.0875795, recall 0.78711
2017-12-10T02:58:46.141109: step 5194, loss 0.109997, acc 0.96875, prec 0.0876039, recall 0.787166
2017-12-10T02:58:46.412877: step 5195, loss 0.158435, acc 0.96875, prec 0.0876148, recall 0.787194
2017-12-10T02:58:46.682677: step 5196, loss 0.187695, acc 0.984375, prec 0.0876135, recall 0.787194
2017-12-10T02:58:46.951774: step 5197, loss 1.22057, acc 0.96875, prec 0.0876122, recall 0.78709
2017-12-10T02:58:47.231645: step 5198, loss 0.0831638, acc 0.953125, prec 0.0876218, recall 0.787118
2017-12-10T02:58:47.498714: step 5199, loss 0.0828386, acc 0.984375, prec 0.0876205, recall 0.787118
2017-12-10T02:58:47.766223: step 5200, loss 0.0422318, acc 0.984375, prec 0.0876192, recall 0.787118
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-5200

2017-12-10T02:58:49.035868: step 5201, loss 0.105711, acc 0.984375, prec 0.0876179, recall 0.787118
2017-12-10T02:58:49.305114: step 5202, loss 0.29152, acc 0.9375, prec 0.0876262, recall 0.787146
2017-12-10T02:58:49.567666: step 5203, loss 0.2948, acc 0.9375, prec 0.087621, recall 0.787146
2017-12-10T02:58:49.835886: step 5204, loss 0.604304, acc 0.921875, prec 0.087628, recall 0.787175
2017-12-10T02:58:50.098502: step 5205, loss 0.0951791, acc 0.96875, prec 0.0876389, recall 0.787203
2017-12-10T02:58:50.365295: step 5206, loss 0.593535, acc 0.90625, prec 0.0876446, recall 0.787231
2017-12-10T02:58:50.627074: step 5207, loss 0.0636765, acc 0.96875, prec 0.087669, recall 0.787288
2017-12-10T02:58:50.897231: step 5208, loss 0.175344, acc 0.9375, prec 0.0876773, recall 0.787316
2017-12-10T02:58:51.157493: step 5209, loss 0.318608, acc 0.953125, prec 0.0876869, recall 0.787344
2017-12-10T02:58:51.424667: step 5210, loss 0.256334, acc 0.9375, prec 0.0876952, recall 0.787372
2017-12-10T02:58:51.689816: step 5211, loss 0.115093, acc 0.984375, prec 0.0877074, recall 0.787401
2017-12-10T02:58:51.951275: step 5212, loss 0.133365, acc 0.953125, prec 0.0877035, recall 0.787401
2017-12-10T02:58:52.219062: step 5213, loss 0.239661, acc 0.96875, prec 0.0877279, recall 0.787457
2017-12-10T02:58:52.484405: step 5214, loss 0.225399, acc 0.9375, prec 0.0877227, recall 0.787457
2017-12-10T02:58:52.755107: step 5215, loss 0.280536, acc 0.96875, prec 0.0877336, recall 0.787485
2017-12-10T02:58:53.018192: step 5216, loss 0.277814, acc 0.96875, prec 0.0877444, recall 0.787513
2017-12-10T02:58:53.276352: step 5217, loss 0.172842, acc 0.953125, prec 0.087754, recall 0.787541
2017-12-10T02:58:53.534871: step 5218, loss 0.0123818, acc 1, prec 0.087754, recall 0.787541
2017-12-10T02:58:53.804388: step 5219, loss 0.0682553, acc 0.984375, prec 0.0877527, recall 0.787541
2017-12-10T02:58:54.070401: step 5220, loss 0.0316302, acc 1, prec 0.0877931, recall 0.787626
2017-12-10T02:58:54.338440: step 5221, loss 0.00168306, acc 1, prec 0.0877931, recall 0.787626
2017-12-10T02:58:54.605184: step 5222, loss 0.154992, acc 0.96875, prec 0.087804, recall 0.787654
2017-12-10T02:58:54.873488: step 5223, loss 0.0113712, acc 1, prec 0.087804, recall 0.787654
2017-12-10T02:58:55.135305: step 5224, loss 0.135954, acc 1, prec 0.0878175, recall 0.787682
2017-12-10T02:58:55.401057: step 5225, loss 0.243433, acc 0.96875, prec 0.0878418, recall 0.787738
2017-12-10T02:58:55.668957: step 5226, loss 0.0826275, acc 0.96875, prec 0.0878527, recall 0.787766
2017-12-10T02:58:55.936828: step 5227, loss 0.0499062, acc 0.984375, prec 0.0878514, recall 0.787766
2017-12-10T02:58:56.200141: step 5228, loss 0.0133083, acc 1, prec 0.0878514, recall 0.787766
2017-12-10T02:58:56.471192: step 5229, loss 0.141596, acc 0.984375, prec 0.087877, recall 0.787823
2017-12-10T02:58:56.740395: step 5230, loss 0.0301062, acc 1, prec 0.087904, recall 0.787879
2017-12-10T02:58:57.012376: step 5231, loss 0.00122345, acc 1, prec 0.087904, recall 0.787879
2017-12-10T02:58:57.281498: step 5232, loss 0.130618, acc 0.96875, prec 0.0879148, recall 0.787907
2017-12-10T02:58:57.545790: step 5233, loss 0.205817, acc 0.96875, prec 0.0879392, recall 0.787963
2017-12-10T02:58:57.811748: step 5234, loss 0.046603, acc 1, prec 0.0879526, recall 0.787991
2017-12-10T02:58:58.077256: step 5235, loss 0.0183678, acc 0.984375, prec 0.0879513, recall 0.787991
2017-12-10T02:58:58.342498: step 5236, loss 0.00699879, acc 1, prec 0.0879513, recall 0.787991
2017-12-10T02:58:58.606339: step 5237, loss 0.0518657, acc 0.984375, prec 0.0879635, recall 0.788019
2017-12-10T02:58:58.872694: step 5238, loss 3.73377, acc 0.96875, prec 0.0880026, recall 0.787999
2017-12-10T02:58:59.142428: step 5239, loss 0.0328932, acc 1, prec 0.0880161, recall 0.788027
2017-12-10T02:58:59.409945: step 5240, loss 0.522327, acc 0.96875, prec 0.0880269, recall 0.788055
2017-12-10T02:58:59.678751: step 5241, loss 0.067127, acc 0.96875, prec 0.0880378, recall 0.788083
2017-12-10T02:58:59.952972: step 5242, loss 0.163229, acc 0.96875, prec 0.088089, recall 0.788195
2017-12-10T02:59:00.235413: step 5243, loss 0.16215, acc 0.9375, prec 0.0880838, recall 0.788195
2017-12-10T02:59:00.509148: step 5244, loss 0.272457, acc 0.90625, prec 0.0881029, recall 0.788251
2017-12-10T02:59:00.772906: step 5245, loss 0.536807, acc 0.953125, prec 0.0881125, recall 0.788279
2017-12-10T02:59:01.035843: step 5246, loss 0.211226, acc 0.921875, prec 0.088106, recall 0.788279
2017-12-10T02:59:01.301424: step 5247, loss 0.68464, acc 0.859375, prec 0.0881077, recall 0.788307
2017-12-10T02:59:01.573728: step 5248, loss 1.12601, acc 0.859375, prec 0.0881229, recall 0.788363
2017-12-10T02:59:01.833011: step 5249, loss 0.388671, acc 0.90625, prec 0.0881151, recall 0.788363
2017-12-10T02:59:02.101429: step 5250, loss 0.192422, acc 0.921875, prec 0.0881087, recall 0.788363
2017-12-10T02:59:02.372470: step 5251, loss 0.510337, acc 0.90625, prec 0.0881009, recall 0.788363
2017-12-10T02:59:02.637598: step 5252, loss 0.850729, acc 0.796875, prec 0.088084, recall 0.788363
2017-12-10T02:59:02.903845: step 5253, loss 0.168202, acc 0.90625, prec 0.0880896, recall 0.788391
2017-12-10T02:59:03.167817: step 5254, loss 0.453681, acc 0.875, prec 0.0880792, recall 0.788391
2017-12-10T02:59:03.430572: step 5255, loss 0.320392, acc 0.921875, prec 0.0880996, recall 0.788446
2017-12-10T02:59:03.698415: step 5256, loss 0.282887, acc 0.890625, prec 0.0880905, recall 0.788446
2017-12-10T02:59:03.971848: step 5257, loss 0.254554, acc 0.921875, prec 0.0880975, recall 0.788474
2017-12-10T02:59:04.237322: step 5258, loss 0.161912, acc 0.9375, prec 0.0881057, recall 0.788502
2017-12-10T02:59:04.501272: step 5259, loss 0.221532, acc 0.9375, prec 0.0881005, recall 0.788502
2017-12-10T02:59:04.773634: step 5260, loss 0.0273246, acc 0.984375, prec 0.0881127, recall 0.78853
2017-12-10T02:59:05.037883: step 5261, loss 0.10072, acc 0.953125, prec 0.0881088, recall 0.78853
2017-12-10T02:59:05.306658: step 5262, loss 0.147033, acc 0.9375, prec 0.0881305, recall 0.788586
2017-12-10T02:59:05.584256: step 5263, loss 0.0313844, acc 0.984375, prec 0.0881426, recall 0.788614
2017-12-10T02:59:05.850122: step 5264, loss 1.44723, acc 0.96875, prec 0.0881547, recall 0.788538
2017-12-10T02:59:06.118670: step 5265, loss 0.0147995, acc 1, prec 0.0881681, recall 0.788565
2017-12-10T02:59:06.381592: step 5266, loss 0.0527663, acc 0.96875, prec 0.0881656, recall 0.788565
2017-12-10T02:59:06.647211: step 5267, loss 0.02299, acc 0.984375, prec 0.0881642, recall 0.788565
2017-12-10T02:59:06.916337: step 5268, loss 0.268721, acc 0.96875, prec 0.0881751, recall 0.788593
2017-12-10T02:59:07.185617: step 5269, loss 0.0505835, acc 0.96875, prec 0.0881725, recall 0.788593
2017-12-10T02:59:07.448149: step 5270, loss 0.164752, acc 0.953125, prec 0.088182, recall 0.788621
2017-12-10T02:59:07.711592: step 5271, loss 0.0943438, acc 0.953125, prec 0.0881916, recall 0.788649
2017-12-10T02:59:07.976434: step 5272, loss 0.0297203, acc 0.984375, prec 0.0881903, recall 0.788649
2017-12-10T02:59:08.242022: step 5273, loss 0.0085967, acc 1, prec 0.0882037, recall 0.788677
2017-12-10T02:59:08.513264: step 5274, loss 0.0656022, acc 0.984375, prec 0.0882158, recall 0.788705
2017-12-10T02:59:08.785109: step 5275, loss 0.0961356, acc 0.953125, prec 0.0882656, recall 0.788816
2017-12-10T02:59:09.049723: step 5276, loss 0.201042, acc 0.953125, prec 0.0882617, recall 0.788816
2017-12-10T02:59:09.323262: step 5277, loss 0.0689545, acc 0.984375, prec 0.0882738, recall 0.788844
2017-12-10T02:59:09.594103: step 5278, loss 0.236639, acc 0.953125, prec 0.0882699, recall 0.788844
2017-12-10T02:59:09.857445: step 5279, loss 0.0132226, acc 1, prec 0.0882834, recall 0.788871
2017-12-10T02:59:10.123285: step 5280, loss 0.0871582, acc 0.96875, prec 0.0882808, recall 0.788871
2017-12-10T02:59:10.389324: step 5281, loss 0.496802, acc 0.9375, prec 0.0882756, recall 0.788871
2017-12-10T02:59:10.650522: step 5282, loss 0.198648, acc 0.9375, prec 0.0882838, recall 0.788899
2017-12-10T02:59:10.917950: step 5283, loss 0.202434, acc 0.9375, prec 0.088292, recall 0.788927
2017-12-10T02:59:11.184484: step 5284, loss 0.00210937, acc 1, prec 0.0883188, recall 0.788982
2017-12-10T02:59:11.459577: step 5285, loss 0.00353128, acc 1, prec 0.0883188, recall 0.788982
2017-12-10T02:59:11.730293: step 5286, loss 0.00169298, acc 1, prec 0.0883323, recall 0.78901
2017-12-10T02:59:11.995318: step 5287, loss 0.37301, acc 0.96875, prec 0.0883431, recall 0.789038
2017-12-10T02:59:12.266780: step 5288, loss 0.357885, acc 1, prec 0.0883699, recall 0.789093
2017-12-10T02:59:12.537889: step 5289, loss 0.0947236, acc 0.984375, prec 0.0884089, recall 0.789176
2017-12-10T02:59:12.814043: step 5290, loss 0.322963, acc 0.953125, prec 0.0884049, recall 0.789176
2017-12-10T02:59:13.081894: step 5291, loss 0.0957597, acc 0.96875, prec 0.088456, recall 0.789287
2017-12-10T02:59:13.351329: step 5292, loss 0.0386435, acc 0.984375, prec 0.0884547, recall 0.789287
2017-12-10T02:59:13.617148: step 5293, loss 0.102379, acc 0.984375, prec 0.0884802, recall 0.789342
2017-12-10T02:59:13.882662: step 5294, loss 0.386498, acc 0.953125, prec 0.0884897, recall 0.78937
2017-12-10T02:59:14.151634: step 5295, loss 0.0196701, acc 1, prec 0.0885031, recall 0.789398
2017-12-10T02:59:14.416473: step 5296, loss 0.161695, acc 0.9375, prec 0.0885113, recall 0.789425
2017-12-10T02:59:14.690292: step 5297, loss 0.365571, acc 0.96875, prec 0.0885355, recall 0.789481
2017-12-10T02:59:14.959499: step 5298, loss 0.0787066, acc 0.96875, prec 0.0885463, recall 0.789508
2017-12-10T02:59:15.225015: step 5299, loss 0.0811769, acc 0.953125, prec 0.0885692, recall 0.789563
2017-12-10T02:59:15.496710: step 5300, loss 0.0390299, acc 0.984375, prec 0.0885679, recall 0.789563
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-5300

2017-12-10T02:59:16.932625: step 5301, loss 0.301407, acc 0.953125, prec 0.088564, recall 0.789563
2017-12-10T02:59:17.201299: step 5302, loss 0.113918, acc 0.984375, prec 0.0885627, recall 0.789563
2017-12-10T02:59:17.465826: step 5303, loss 0.194085, acc 0.9375, prec 0.0885575, recall 0.789563
2017-12-10T02:59:17.735598: step 5304, loss 0.00274543, acc 1, prec 0.0885575, recall 0.789563
2017-12-10T02:59:18.005191: step 5305, loss 0.0152565, acc 1, prec 0.0885575, recall 0.789563
2017-12-10T02:59:18.278882: step 5306, loss 0.101064, acc 0.96875, prec 0.0885817, recall 0.789619
2017-12-10T02:59:18.551448: step 5307, loss 0.28812, acc 0.96875, prec 0.0885925, recall 0.789646
2017-12-10T02:59:18.818824: step 5308, loss 0.00540362, acc 1, prec 0.0886059, recall 0.789674
2017-12-10T02:59:19.082529: step 5309, loss 0.19574, acc 0.984375, prec 0.0886448, recall 0.789756
2017-12-10T02:59:19.345214: step 5310, loss 0.0523694, acc 1, prec 0.0886582, recall 0.789784
2017-12-10T02:59:19.617811: step 5311, loss 0.0745972, acc 1, prec 0.0886716, recall 0.789811
2017-12-10T02:59:19.884326: step 5312, loss 1.47706, acc 0.953125, prec 0.0886824, recall 0.789736
2017-12-10T02:59:20.150829: step 5313, loss 0.00384278, acc 1, prec 0.0887092, recall 0.789791
2017-12-10T02:59:20.411233: step 5314, loss 0.0869053, acc 0.984375, prec 0.0887749, recall 0.789928
2017-12-10T02:59:20.675315: step 5315, loss 0.356674, acc 0.96875, prec 0.0887723, recall 0.789928
2017-12-10T02:59:20.942628: step 5316, loss 0.067586, acc 0.984375, prec 0.0887844, recall 0.789956
2017-12-10T02:59:21.204745: step 5317, loss 0.300443, acc 0.953125, prec 0.0887804, recall 0.789956
2017-12-10T02:59:21.469632: step 5318, loss 0.25172, acc 0.921875, prec 0.0887873, recall 0.789983
2017-12-10T02:59:21.742477: step 5319, loss 0.368718, acc 0.9375, prec 0.0887955, recall 0.79001
2017-12-10T02:59:22.006188: step 5320, loss 0.314887, acc 0.921875, prec 0.088789, recall 0.79001
2017-12-10T02:59:22.274326: step 5321, loss 0.146967, acc 0.984375, prec 0.0887877, recall 0.79001
2017-12-10T02:59:22.543883: step 5322, loss 0.633938, acc 0.9375, prec 0.0887824, recall 0.79001
2017-12-10T02:59:22.811962: step 5323, loss 0.304797, acc 0.921875, prec 0.0887893, recall 0.790038
2017-12-10T02:59:23.076815: step 5324, loss 0.0874221, acc 0.953125, prec 0.0888122, recall 0.790093
2017-12-10T02:59:23.349124: step 5325, loss 0.166784, acc 0.984375, prec 0.0888109, recall 0.790093
2017-12-10T02:59:23.616972: step 5326, loss 0.041842, acc 0.984375, prec 0.0888096, recall 0.790093
2017-12-10T02:59:23.895122: step 5327, loss 0.411396, acc 0.890625, prec 0.0888138, recall 0.79012
2017-12-10T02:59:24.162315: step 5328, loss 0.425861, acc 0.90625, prec 0.088806, recall 0.79012
2017-12-10T02:59:24.431281: step 5329, loss 0.2852, acc 0.96875, prec 0.0888168, recall 0.790148
2017-12-10T02:59:24.701413: step 5330, loss 0.144823, acc 0.96875, prec 0.0888543, recall 0.79023
2017-12-10T02:59:24.985344: step 5331, loss 0.539954, acc 0.921875, prec 0.0888879, recall 0.790312
2017-12-10T02:59:25.248896: step 5332, loss 0.426561, acc 0.9375, prec 0.0889228, recall 0.790394
2017-12-10T02:59:25.521313: step 5333, loss 0.019352, acc 0.984375, prec 0.0889349, recall 0.790421
2017-12-10T02:59:25.788394: step 5334, loss 0.165456, acc 0.96875, prec 0.0889323, recall 0.790421
2017-12-10T02:59:26.055518: step 5335, loss 0.443851, acc 0.96875, prec 0.088943, recall 0.790449
2017-12-10T02:59:26.322879: step 5336, loss 0.195606, acc 0.9375, prec 0.0889646, recall 0.790504
2017-12-10T02:59:26.596282: step 5337, loss 1.15642, acc 0.96875, prec 0.0889753, recall 0.790531
2017-12-10T02:59:26.866776: step 5338, loss 0.238739, acc 0.921875, prec 0.0889822, recall 0.790558
2017-12-10T02:59:27.133262: step 5339, loss 0.114982, acc 0.96875, prec 0.0889796, recall 0.790558
2017-12-10T02:59:27.398254: step 5340, loss 0.104381, acc 0.984375, prec 0.0889783, recall 0.790558
2017-12-10T02:59:27.668303: step 5341, loss 0.169212, acc 0.921875, prec 0.0889851, recall 0.790585
2017-12-10T02:59:27.941003: step 5342, loss 0.183514, acc 0.953125, prec 0.0889946, recall 0.790613
2017-12-10T02:59:28.204438: step 5343, loss 0.129685, acc 0.96875, prec 0.0890187, recall 0.790667
2017-12-10T02:59:28.470281: step 5344, loss 0.0978436, acc 0.96875, prec 0.0890294, recall 0.790695
2017-12-10T02:59:28.735219: step 5345, loss 0.161534, acc 0.984375, prec 0.0890415, recall 0.790722
2017-12-10T02:59:28.998404: step 5346, loss 0.305598, acc 0.953125, prec 0.0890376, recall 0.790722
2017-12-10T02:59:29.264706: step 5347, loss 0.356493, acc 0.875, prec 0.0890405, recall 0.790749
2017-12-10T02:59:29.530032: step 5348, loss 0.151488, acc 0.96875, prec 0.0890379, recall 0.790749
2017-12-10T02:59:29.799677: step 5349, loss 0.143259, acc 0.96875, prec 0.0890353, recall 0.790749
2017-12-10T02:59:30.074236: step 5350, loss 0.325109, acc 0.953125, prec 0.0890447, recall 0.790776
2017-12-10T02:59:30.347119: step 5351, loss 0.275607, acc 0.90625, prec 0.0890369, recall 0.790776
2017-12-10T02:59:30.621097: step 5352, loss 0.0151322, acc 1, prec 0.0890369, recall 0.790776
2017-12-10T02:59:30.883310: step 5353, loss 0.0704819, acc 0.984375, prec 0.0890356, recall 0.790776
2017-12-10T02:59:31.146493: step 5354, loss 0.0862294, acc 0.984375, prec 0.0890343, recall 0.790776
2017-12-10T02:59:31.413924: step 5355, loss 0.129426, acc 0.96875, prec 0.0890317, recall 0.790776
2017-12-10T02:59:31.678781: step 5356, loss 0.0313399, acc 1, prec 0.0890317, recall 0.790776
2017-12-10T02:59:31.947117: step 5357, loss 0.225688, acc 0.96875, prec 0.0890424, recall 0.790804
2017-12-10T02:59:32.214879: step 5358, loss 0.0124128, acc 1, prec 0.0890558, recall 0.790831
2017-12-10T02:59:32.477880: step 5359, loss 0.101027, acc 0.953125, prec 0.0890518, recall 0.790831
2017-12-10T02:59:32.741385: step 5360, loss 0.121665, acc 0.984375, prec 0.0890505, recall 0.790831
2017-12-10T02:59:33.009675: step 5361, loss 0.0577464, acc 0.984375, prec 0.0890626, recall 0.790858
2017-12-10T02:59:33.272016: step 5362, loss 0.172639, acc 0.984375, prec 0.0890746, recall 0.790885
2017-12-10T02:59:33.547502: step 5363, loss 0.00164289, acc 1, prec 0.0890746, recall 0.790885
2017-12-10T02:59:33.813202: step 5364, loss 0.0469268, acc 0.953125, prec 0.0890707, recall 0.790885
2017-12-10T02:59:34.074088: step 5365, loss 0.153961, acc 0.96875, prec 0.0890815, recall 0.790913
2017-12-10T02:59:34.339349: step 5366, loss 0.0406035, acc 0.984375, prec 0.0890935, recall 0.79094
2017-12-10T02:59:34.605480: step 5367, loss 0.0830601, acc 0.96875, prec 0.0891043, recall 0.790967
2017-12-10T02:59:34.885054: step 5368, loss 0.311537, acc 0.96875, prec 0.089115, recall 0.790994
2017-12-10T02:59:35.161128: step 5369, loss 0.0600214, acc 0.984375, prec 0.0891271, recall 0.791021
2017-12-10T02:59:35.429858: step 5370, loss 0.0934531, acc 1, prec 0.0891538, recall 0.791076
2017-12-10T02:59:35.700461: step 5371, loss 0.0282003, acc 0.984375, prec 0.0891658, recall 0.791103
2017-12-10T02:59:36.001604: step 5372, loss 0.398635, acc 0.953125, prec 0.0892019, recall 0.791184
2017-12-10T02:59:36.282256: step 5373, loss 0.048231, acc 1, prec 0.0892153, recall 0.791212
2017-12-10T02:59:36.550346: step 5374, loss 0.510528, acc 0.953125, prec 0.0892514, recall 0.791293
2017-12-10T02:59:36.821280: step 5375, loss 0.139885, acc 0.96875, prec 0.0892755, recall 0.791347
2017-12-10T02:59:37.085845: step 5376, loss 0.142146, acc 0.984375, prec 0.0892875, recall 0.791374
2017-12-10T02:59:37.362004: step 5377, loss 0.00556579, acc 1, prec 0.0892875, recall 0.791374
2017-12-10T02:59:37.630719: step 5378, loss 0.0198177, acc 1, prec 0.0893142, recall 0.791429
2017-12-10T02:59:37.904863: step 5379, loss 0.128937, acc 0.984375, prec 0.0893263, recall 0.791456
2017-12-10T02:59:38.171272: step 5380, loss 0.0328486, acc 0.984375, prec 0.0893383, recall 0.791483
2017-12-10T02:59:38.437720: step 5381, loss 0.0961474, acc 0.953125, prec 0.0893477, recall 0.79151
2017-12-10T02:59:38.701342: step 5382, loss 0.241428, acc 0.9375, prec 0.0893558, recall 0.791537
2017-12-10T02:59:38.970793: step 5383, loss 0.273731, acc 0.953125, prec 0.0893519, recall 0.791537
2017-12-10T02:59:39.239329: step 5384, loss 0.0841989, acc 0.96875, prec 0.0893626, recall 0.791564
2017-12-10T02:59:39.505396: step 5385, loss 0.330556, acc 0.984375, prec 0.0894014, recall 0.791645
2017-12-10T02:59:39.772616: step 5386, loss 0.704671, acc 0.96875, prec 0.0894121, recall 0.791672
2017-12-10T02:59:40.038077: step 5387, loss 0.159979, acc 0.953125, prec 0.0894215, recall 0.791699
2017-12-10T02:59:40.300826: step 5388, loss 0.00980652, acc 1, prec 0.0894348, recall 0.791726
2017-12-10T02:59:40.563978: step 5389, loss 0.0196361, acc 0.984375, prec 0.0894335, recall 0.791726
2017-12-10T02:59:40.828316: step 5390, loss 0.19227, acc 0.984375, prec 0.0894455, recall 0.791753
2017-12-10T02:59:41.094642: step 5391, loss 0.558362, acc 0.96875, prec 0.0894829, recall 0.791834
2017-12-10T02:59:41.361389: step 5392, loss 0.0142151, acc 1, prec 0.0895096, recall 0.791888
2017-12-10T02:59:41.639212: step 5393, loss 0.00868066, acc 1, prec 0.0895096, recall 0.791888
2017-12-10T02:59:41.901464: step 5394, loss 0.312089, acc 0.96875, prec 0.0895337, recall 0.791942
2017-12-10T02:59:42.160502: step 5395, loss 0.251954, acc 0.953125, prec 0.0895431, recall 0.791969
2017-12-10T02:59:42.428108: step 5396, loss 0.0629, acc 0.984375, prec 0.0895417, recall 0.791969
2017-12-10T02:59:42.696399: step 5397, loss 0.0254224, acc 0.984375, prec 0.0895404, recall 0.791969
2017-12-10T02:59:42.970774: step 5398, loss 0.0587291, acc 0.984375, prec 0.0895658, recall 0.792023
2017-12-10T02:59:43.234655: step 5399, loss 0.0006814, acc 1, prec 0.0895658, recall 0.792023
2017-12-10T02:59:43.495209: step 5400, loss 0.00582635, acc 1, prec 0.0895658, recall 0.792023

Evaluation:
2017-12-10T02:59:51.101114: step 5400, loss 7.51172, acc 0.970941, prec 0.0899167, recall 0.781258

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-5400

2017-12-10T02:59:52.790363: step 5401, loss 0.0120647, acc 1, prec 0.0899299, recall 0.781286
2017-12-10T02:59:53.061987: step 5402, loss 0.091023, acc 0.984375, prec 0.0899419, recall 0.781313
2017-12-10T02:59:53.328068: step 5403, loss 0.00658686, acc 1, prec 0.0899419, recall 0.781313
2017-12-10T02:59:53.597215: step 5404, loss 0.00824205, acc 1, prec 0.0899685, recall 0.781369
2017-12-10T02:59:53.859681: step 5405, loss 0.265334, acc 0.96875, prec 0.0899924, recall 0.781424
2017-12-10T02:59:54.127582: step 5406, loss 0.00367761, acc 1, prec 0.0899924, recall 0.781424
2017-12-10T02:59:54.402367: step 5407, loss 0.00526574, acc 1, prec 0.0899924, recall 0.781424
2017-12-10T02:59:54.668667: step 5408, loss 0.0253116, acc 0.984375, prec 0.0899911, recall 0.781424
2017-12-10T02:59:54.943957: step 5409, loss 0.0847877, acc 0.984375, prec 0.0900031, recall 0.781452
2017-12-10T02:59:55.213379: step 5410, loss 0.83957, acc 0.984375, prec 0.0900283, recall 0.781507
2017-12-10T02:59:55.483294: step 5411, loss 0.495872, acc 0.953125, prec 0.0900244, recall 0.781507
2017-12-10T02:59:55.750923: step 5412, loss 0.345146, acc 0.953125, prec 0.090047, recall 0.781563
2017-12-10T02:59:56.014805: step 5413, loss 0.249632, acc 0.96875, prec 0.0900576, recall 0.78159
2017-12-10T02:59:56.281263: step 5414, loss 0.0561195, acc 0.984375, prec 0.0900563, recall 0.78159
2017-12-10T02:59:56.558705: step 5415, loss 0.0630269, acc 0.96875, prec 0.0900537, recall 0.78159
2017-12-10T02:59:56.830881: step 5416, loss 0.0770531, acc 0.984375, prec 0.0900524, recall 0.78159
2017-12-10T02:59:57.096413: step 5417, loss 0.00321834, acc 1, prec 0.0900524, recall 0.78159
2017-12-10T02:59:57.362029: step 5418, loss 0.100128, acc 0.9375, prec 0.0900471, recall 0.78159
2017-12-10T02:59:57.633947: step 5419, loss 0.106863, acc 0.984375, prec 0.0900458, recall 0.78159
2017-12-10T02:59:57.906707: step 5420, loss 0.0554083, acc 0.984375, prec 0.0900578, recall 0.781618
2017-12-10T02:59:58.182268: step 5421, loss 0.212611, acc 0.984375, prec 0.0900564, recall 0.781618
2017-12-10T02:59:58.448083: step 5422, loss 0.25952, acc 0.96875, prec 0.0900804, recall 0.781673
2017-12-10T02:59:58.713264: step 5423, loss 0.20969, acc 0.953125, prec 0.0900897, recall 0.781701
2017-12-10T02:59:58.983783: step 5424, loss 0.0526854, acc 0.96875, prec 0.0901136, recall 0.781756
2017-12-10T02:59:59.263203: step 5425, loss 0.0109261, acc 1, prec 0.0901136, recall 0.781756
2017-12-10T02:59:59.536771: step 5426, loss 0.0664425, acc 0.984375, prec 0.0901388, recall 0.781811
2017-12-10T02:59:59.807885: step 5427, loss 0.0626979, acc 0.984375, prec 0.0901508, recall 0.781839
2017-12-10T03:00:00.075379: step 5428, loss 0.030043, acc 0.984375, prec 0.090176, recall 0.781894
2017-12-10T03:00:00.345775: step 5429, loss 0.0764227, acc 0.984375, prec 0.0901747, recall 0.781894
2017-12-10T03:00:00.611931: step 5430, loss 0.247087, acc 0.953125, prec 0.0901707, recall 0.781894
2017-12-10T03:00:00.881977: step 5431, loss 0.0171832, acc 0.984375, prec 0.0901827, recall 0.781922
2017-12-10T03:00:01.147503: step 5432, loss 0.0391852, acc 0.984375, prec 0.0901814, recall 0.781922
2017-12-10T03:00:01.415514: step 5433, loss 0.42182, acc 0.984375, prec 0.0902066, recall 0.781977
2017-12-10T03:00:01.685184: step 5434, loss 0.0826436, acc 0.984375, prec 0.0902186, recall 0.782004
2017-12-10T03:00:01.958952: step 5435, loss 0.103622, acc 0.96875, prec 0.0902159, recall 0.782004
2017-12-10T03:00:02.223878: step 5436, loss 0.335041, acc 0.953125, prec 0.090212, recall 0.782004
2017-12-10T03:00:02.487771: step 5437, loss 0.0682127, acc 0.984375, prec 0.0902107, recall 0.782004
2017-12-10T03:00:02.752984: step 5438, loss 0.364075, acc 0.96875, prec 0.0902213, recall 0.782032
2017-12-10T03:00:03.020935: step 5439, loss 0.118171, acc 0.984375, prec 0.0902598, recall 0.782114
2017-12-10T03:00:03.293693: step 5440, loss 0.163469, acc 0.96875, prec 0.0902571, recall 0.782114
2017-12-10T03:00:03.562687: step 5441, loss 0.153971, acc 0.96875, prec 0.0902943, recall 0.782197
2017-12-10T03:00:03.826218: step 5442, loss 0.0885401, acc 0.984375, prec 0.0903327, recall 0.782279
2017-12-10T03:00:04.090256: step 5443, loss 0.309182, acc 0.96875, prec 0.0903301, recall 0.782279
2017-12-10T03:00:04.354516: step 5444, loss 0.158957, acc 0.984375, prec 0.0903288, recall 0.782279
2017-12-10T03:00:04.618851: step 5445, loss 0.00888746, acc 1, prec 0.0903288, recall 0.782279
2017-12-10T03:00:04.883692: step 5446, loss 0.0679502, acc 0.96875, prec 0.0903262, recall 0.782279
2017-12-10T03:00:05.156512: step 5447, loss 0.0252063, acc 0.984375, prec 0.0903381, recall 0.782307
2017-12-10T03:00:05.426600: step 5448, loss 0.264848, acc 0.9375, prec 0.0903328, recall 0.782307
2017-12-10T03:00:05.690972: step 5449, loss 0.14654, acc 0.984375, prec 0.0903448, recall 0.782334
2017-12-10T03:00:05.957270: step 5450, loss 0.0274871, acc 1, prec 0.0903713, recall 0.782389
2017-12-10T03:00:06.224497: step 5451, loss 0.140773, acc 0.9375, prec 0.090366, recall 0.782389
2017-12-10T03:00:06.490566: step 5452, loss 0.0597133, acc 0.96875, prec 0.0903634, recall 0.782389
2017-12-10T03:00:06.764147: step 5453, loss 0.109227, acc 0.96875, prec 0.090374, recall 0.782417
2017-12-10T03:00:07.040216: step 5454, loss 0.100192, acc 0.96875, prec 0.0903714, recall 0.782417
2017-12-10T03:00:07.302154: step 5455, loss 0.0444547, acc 1, prec 0.0903846, recall 0.782444
2017-12-10T03:00:07.577866: step 5456, loss 0.0456076, acc 1, prec 0.0904111, recall 0.782499
2017-12-10T03:00:07.845748: step 5457, loss 0.0497362, acc 0.984375, prec 0.0904231, recall 0.782526
2017-12-10T03:00:08.111982: step 5458, loss 0.0153758, acc 0.984375, prec 0.090435, recall 0.782554
2017-12-10T03:00:08.377480: step 5459, loss 0.00107839, acc 1, prec 0.090435, recall 0.782554
2017-12-10T03:00:08.640210: step 5460, loss 0.0235007, acc 0.984375, prec 0.0904734, recall 0.782636
2017-12-10T03:00:08.907732: step 5461, loss 0.0101535, acc 1, prec 0.0904867, recall 0.782663
2017-12-10T03:00:09.179455: step 5462, loss 0.00198657, acc 1, prec 0.0905264, recall 0.782746
2017-12-10T03:00:09.444917: step 5463, loss 0.129798, acc 0.984375, prec 0.0905251, recall 0.782746
2017-12-10T03:00:09.719316: step 5464, loss 0.0637713, acc 0.984375, prec 0.0905238, recall 0.782746
2017-12-10T03:00:09.997013: step 5465, loss 0.106837, acc 0.984375, prec 0.0905489, recall 0.7828
2017-12-10T03:00:10.265770: step 5466, loss 0.574849, acc 1, prec 0.0905622, recall 0.782828
2017-12-10T03:00:10.499396: step 5467, loss 0.43018, acc 0.960784, prec 0.0905993, recall 0.78291
2017-12-10T03:00:10.786511: step 5468, loss 0.0122176, acc 1, prec 0.0906258, recall 0.782964
2017-12-10T03:00:11.056062: step 5469, loss 0.00659502, acc 1, prec 0.090639, recall 0.782992
2017-12-10T03:00:11.328095: step 5470, loss 0.0185854, acc 1, prec 0.0906523, recall 0.783019
2017-12-10T03:00:11.609833: step 5471, loss 0.0486607, acc 0.984375, prec 0.0906774, recall 0.783073
2017-12-10T03:00:11.876770: step 5472, loss 0.138115, acc 0.96875, prec 0.090688, recall 0.783101
2017-12-10T03:00:12.143529: step 5473, loss 0.00802792, acc 1, prec 0.0907145, recall 0.783155
2017-12-10T03:00:12.417142: step 5474, loss 0.11918, acc 0.984375, prec 0.0907132, recall 0.783155
2017-12-10T03:00:12.694007: step 5475, loss 0.0143264, acc 1, prec 0.0907132, recall 0.783155
2017-12-10T03:00:12.960573: step 5476, loss 0.00140875, acc 1, prec 0.0907132, recall 0.783155
2017-12-10T03:00:13.224976: step 5477, loss 0.203623, acc 0.921875, prec 0.0907066, recall 0.783155
2017-12-10T03:00:13.492282: step 5478, loss 0.18442, acc 0.953125, prec 0.0907159, recall 0.783183
2017-12-10T03:00:13.757039: step 5479, loss 0.255402, acc 0.984375, prec 0.0907145, recall 0.783183
2017-12-10T03:00:14.030271: step 5480, loss 0.10517, acc 0.953125, prec 0.0907106, recall 0.783183
2017-12-10T03:00:14.291366: step 5481, loss 0.0867435, acc 0.96875, prec 0.0907079, recall 0.783183
2017-12-10T03:00:14.555058: step 5482, loss 0.355423, acc 0.953125, prec 0.090704, recall 0.783183
2017-12-10T03:00:14.819298: step 5483, loss 0.0768965, acc 0.96875, prec 0.0907013, recall 0.783183
2017-12-10T03:00:15.086787: step 5484, loss 0.0293034, acc 0.984375, prec 0.0907, recall 0.783183
2017-12-10T03:00:15.347624: step 5485, loss 0.15941, acc 0.921875, prec 0.0907066, recall 0.78321
2017-12-10T03:00:15.611354: step 5486, loss 0.189058, acc 0.9375, prec 0.0907146, recall 0.783237
2017-12-10T03:00:15.885074: step 5487, loss 0.411527, acc 0.9375, prec 0.0907358, recall 0.783291
2017-12-10T03:00:16.150339: step 5488, loss 0.217849, acc 0.9375, prec 0.0907305, recall 0.783291
2017-12-10T03:00:16.414521: step 5489, loss 0.0174952, acc 1, prec 0.0907702, recall 0.783373
2017-12-10T03:00:16.676279: step 5490, loss 0.0621301, acc 0.953125, prec 0.0907795, recall 0.7834
2017-12-10T03:00:16.948340: step 5491, loss 0.0528649, acc 0.953125, prec 0.0907755, recall 0.7834
2017-12-10T03:00:17.217136: step 5492, loss 0.00610194, acc 1, prec 0.0907755, recall 0.7834
2017-12-10T03:00:17.478700: step 5493, loss 0.0167009, acc 1, prec 0.0907755, recall 0.7834
2017-12-10T03:00:17.739828: step 5494, loss 0.189121, acc 0.96875, prec 0.0907729, recall 0.7834
2017-12-10T03:00:18.012325: step 5495, loss 0.0563074, acc 0.984375, prec 0.0907715, recall 0.7834
2017-12-10T03:00:18.278503: step 5496, loss 0.0690578, acc 0.984375, prec 0.0907834, recall 0.783427
2017-12-10T03:00:18.541454: step 5497, loss 0.192294, acc 0.9375, prec 0.0907914, recall 0.783455
2017-12-10T03:00:18.814545: step 5498, loss 0.166085, acc 0.984375, prec 0.0907901, recall 0.783455
2017-12-10T03:00:19.085474: step 5499, loss 0.47669, acc 1, prec 0.0908165, recall 0.783509
2017-12-10T03:00:19.367866: step 5500, loss 0.173888, acc 0.96875, prec 0.0908139, recall 0.783509
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-5500

2017-12-10T03:00:20.761684: step 5501, loss 0.0838594, acc 0.984375, prec 0.0908126, recall 0.783509
2017-12-10T03:00:21.030199: step 5502, loss 0.0750324, acc 0.984375, prec 0.0908377, recall 0.783563
2017-12-10T03:00:21.296549: step 5503, loss 0.0627586, acc 0.96875, prec 0.0908483, recall 0.78359
2017-12-10T03:00:21.559763: step 5504, loss 0.0248208, acc 0.984375, prec 0.0908602, recall 0.783618
2017-12-10T03:00:21.832546: step 5505, loss 0.0111543, acc 1, prec 0.0908602, recall 0.783618
2017-12-10T03:00:22.099562: step 5506, loss 0.0176961, acc 1, prec 0.0908866, recall 0.783672
2017-12-10T03:00:22.370652: step 5507, loss 0.0394599, acc 0.984375, prec 0.0908985, recall 0.783699
2017-12-10T03:00:22.634211: step 5508, loss 0.0448371, acc 0.96875, prec 0.0909223, recall 0.783753
2017-12-10T03:00:22.899624: step 5509, loss 0.0122733, acc 0.984375, prec 0.090921, recall 0.783753
2017-12-10T03:00:23.161277: step 5510, loss 0.0432793, acc 0.96875, prec 0.0909183, recall 0.783753
2017-12-10T03:00:23.430560: step 5511, loss 0.0324081, acc 0.984375, prec 0.090917, recall 0.783753
2017-12-10T03:00:23.697316: step 5512, loss 0.257251, acc 0.984375, prec 0.0909157, recall 0.783753
2017-12-10T03:00:23.972985: step 5513, loss 0.00583044, acc 1, prec 0.0909157, recall 0.783753
2017-12-10T03:00:24.238238: step 5514, loss 0.242897, acc 0.984375, prec 0.090954, recall 0.783835
2017-12-10T03:00:24.503769: step 5515, loss 0.00294611, acc 1, prec 0.090954, recall 0.783835
2017-12-10T03:00:24.767238: step 5516, loss 0.0367961, acc 0.984375, prec 0.0909527, recall 0.783835
2017-12-10T03:00:25.033386: step 5517, loss 1.45296, acc 0.96875, prec 0.0909778, recall 0.783791
2017-12-10T03:00:25.312024: step 5518, loss 0.188401, acc 0.984375, prec 0.0909897, recall 0.783818
2017-12-10T03:00:25.582807: step 5519, loss 0.0476696, acc 0.96875, prec 0.0909871, recall 0.783818
2017-12-10T03:00:25.857293: step 5520, loss 0.0875643, acc 0.96875, prec 0.0910109, recall 0.783872
2017-12-10T03:00:26.126483: step 5521, loss 0.0920532, acc 0.984375, prec 0.091036, recall 0.783926
2017-12-10T03:00:26.406002: step 5522, loss 0.206696, acc 0.953125, prec 0.091032, recall 0.783926
2017-12-10T03:00:26.686017: step 5523, loss 0.156212, acc 0.9375, prec 0.0910399, recall 0.783953
2017-12-10T03:00:26.951504: step 5524, loss 0.516871, acc 0.9375, prec 0.091061, recall 0.784007
2017-12-10T03:00:27.224299: step 5525, loss 0.175038, acc 0.984375, prec 0.0910729, recall 0.784034
2017-12-10T03:00:27.488408: step 5526, loss 0.236438, acc 0.953125, prec 0.0910822, recall 0.784061
2017-12-10T03:00:27.754928: step 5527, loss 0.306763, acc 0.921875, prec 0.091102, recall 0.784115
2017-12-10T03:00:28.020803: step 5528, loss 0.124238, acc 0.953125, prec 0.0911112, recall 0.784142
2017-12-10T03:00:28.285415: step 5529, loss 0.285205, acc 0.9375, prec 0.0911191, recall 0.784169
2017-12-10T03:00:28.558278: step 5530, loss 0.379277, acc 0.90625, prec 0.0911244, recall 0.784196
2017-12-10T03:00:28.828533: step 5531, loss 0.179767, acc 0.953125, prec 0.0911336, recall 0.784223
2017-12-10T03:00:29.093295: step 5532, loss 0.211383, acc 0.9375, prec 0.0911415, recall 0.78425
2017-12-10T03:00:29.358727: step 5533, loss 0.409499, acc 0.9375, prec 0.0911494, recall 0.784277
2017-12-10T03:00:29.630554: step 5534, loss 0.901474, acc 0.953125, prec 0.0911455, recall 0.784277
2017-12-10T03:00:29.896104: step 5535, loss 0.0835951, acc 0.953125, prec 0.0911547, recall 0.784304
2017-12-10T03:00:30.173185: step 5536, loss 0.058072, acc 0.984375, prec 0.0911666, recall 0.784331
2017-12-10T03:00:30.448696: step 5537, loss 0.0317697, acc 0.984375, prec 0.0911784, recall 0.784358
2017-12-10T03:00:30.713105: step 5538, loss 0.180083, acc 0.953125, prec 0.0912141, recall 0.784439
2017-12-10T03:00:30.992619: step 5539, loss 0.0579243, acc 0.96875, prec 0.0912114, recall 0.784439
2017-12-10T03:00:31.256124: step 5540, loss 0.041687, acc 0.984375, prec 0.0912233, recall 0.784466
2017-12-10T03:00:31.526957: step 5541, loss 0.0801866, acc 0.96875, prec 0.0912338, recall 0.784492
2017-12-10T03:00:31.797164: step 5542, loss 0.0482219, acc 0.984375, prec 0.0912325, recall 0.784492
2017-12-10T03:00:32.069360: step 5543, loss 0.124603, acc 0.953125, prec 0.0912417, recall 0.784519
2017-12-10T03:00:32.336329: step 5544, loss 0.0952782, acc 0.96875, prec 0.0912391, recall 0.784519
2017-12-10T03:00:32.600082: step 5545, loss 0.0913541, acc 0.953125, prec 0.0912615, recall 0.784573
2017-12-10T03:00:32.861747: step 5546, loss 0.00475944, acc 1, prec 0.0912747, recall 0.7846
2017-12-10T03:00:33.123369: step 5547, loss 0.00953553, acc 1, prec 0.0912879, recall 0.784627
2017-12-10T03:00:33.396853: step 5548, loss 0.0307506, acc 0.984375, prec 0.0912865, recall 0.784627
2017-12-10T03:00:33.658178: step 5549, loss 0.00423267, acc 1, prec 0.0912997, recall 0.784654
2017-12-10T03:00:33.920544: step 5550, loss 0.0267714, acc 0.984375, prec 0.0912984, recall 0.784654
2017-12-10T03:00:34.182022: step 5551, loss 0.0516749, acc 0.984375, prec 0.0913103, recall 0.784681
2017-12-10T03:00:34.452701: step 5552, loss 0.0207718, acc 1, prec 0.0913367, recall 0.784734
2017-12-10T03:00:34.732787: step 5553, loss 0.0125923, acc 1, prec 0.091363, recall 0.784788
2017-12-10T03:00:35.004264: step 5554, loss 0.568959, acc 0.984375, prec 0.0913881, recall 0.784842
2017-12-10T03:00:35.272316: step 5555, loss 0.228382, acc 0.96875, prec 0.0913986, recall 0.784868
2017-12-10T03:00:35.546648: step 5556, loss 0.00423583, acc 1, prec 0.0913986, recall 0.784868
2017-12-10T03:00:35.808766: step 5557, loss 0.0217136, acc 0.984375, prec 0.0913973, recall 0.784868
2017-12-10T03:00:36.084153: step 5558, loss 0.15967, acc 0.984375, prec 0.0914223, recall 0.784922
2017-12-10T03:00:36.352027: step 5559, loss 0.0124552, acc 1, prec 0.0914223, recall 0.784922
2017-12-10T03:00:36.619179: step 5560, loss 0.00501103, acc 1, prec 0.0914355, recall 0.784949
2017-12-10T03:00:36.889626: step 5561, loss 0.0592342, acc 0.96875, prec 0.0914329, recall 0.784949
2017-12-10T03:00:37.160046: step 5562, loss 0.256164, acc 0.96875, prec 0.0914566, recall 0.785002
2017-12-10T03:00:37.434937: step 5563, loss 0.159565, acc 0.984375, prec 0.0914816, recall 0.785056
2017-12-10T03:00:37.701599: step 5564, loss 0.0115924, acc 1, prec 0.0914816, recall 0.785056
2017-12-10T03:00:37.970861: step 5565, loss 0.215028, acc 0.953125, prec 0.0914777, recall 0.785056
2017-12-10T03:00:38.241028: step 5566, loss 0.00989003, acc 1, prec 0.0914777, recall 0.785056
2017-12-10T03:00:38.504471: step 5567, loss 0.0839731, acc 0.96875, prec 0.0914882, recall 0.785083
2017-12-10T03:00:38.777557: step 5568, loss 0.0669211, acc 0.96875, prec 0.0914855, recall 0.785083
2017-12-10T03:00:39.039125: step 5569, loss 0.103011, acc 0.984375, prec 0.0915106, recall 0.785136
2017-12-10T03:00:39.303422: step 5570, loss 0.0016062, acc 1, prec 0.0915238, recall 0.785163
2017-12-10T03:00:39.571631: step 5571, loss 0.128601, acc 0.96875, prec 0.0915343, recall 0.78519
2017-12-10T03:00:39.846328: step 5572, loss 0.00199699, acc 1, prec 0.0915343, recall 0.78519
2017-12-10T03:00:40.112133: step 5573, loss 0.049852, acc 0.984375, prec 0.0915593, recall 0.785243
2017-12-10T03:00:40.377760: step 5574, loss 0.0383816, acc 0.984375, prec 0.091558, recall 0.785243
2017-12-10T03:00:40.645785: step 5575, loss 0.158086, acc 0.953125, prec 0.091554, recall 0.785243
2017-12-10T03:00:40.920753: step 5576, loss 0.0106244, acc 1, prec 0.0915935, recall 0.785323
2017-12-10T03:00:41.185564: step 5577, loss 0.203788, acc 0.96875, prec 0.0915909, recall 0.785323
2017-12-10T03:00:41.458928: step 5578, loss 0.000699618, acc 1, prec 0.0915909, recall 0.785323
2017-12-10T03:00:41.727218: step 5579, loss 0.125331, acc 0.984375, prec 0.0915895, recall 0.785323
2017-12-10T03:00:41.989864: step 5580, loss 0.0422925, acc 0.984375, prec 0.0916014, recall 0.78535
2017-12-10T03:00:42.262502: step 5581, loss 0.0760977, acc 0.984375, prec 0.0916001, recall 0.78535
2017-12-10T03:00:42.533963: step 5582, loss 0.0131445, acc 1, prec 0.0916001, recall 0.78535
2017-12-10T03:00:42.806896: step 5583, loss 0.00299107, acc 1, prec 0.0916264, recall 0.785403
2017-12-10T03:00:43.074669: step 5584, loss 0.0194797, acc 0.984375, prec 0.0916383, recall 0.78543
2017-12-10T03:00:43.345320: step 5585, loss 8.86099, acc 0.953125, prec 0.0916488, recall 0.785359
2017-12-10T03:00:43.619940: step 5586, loss 0.0319131, acc 0.984375, prec 0.0916606, recall 0.785386
2017-12-10T03:00:43.881114: step 5587, loss 0.359284, acc 0.984375, prec 0.0916725, recall 0.785413
2017-12-10T03:00:44.145700: step 5588, loss 0.0705659, acc 0.96875, prec 0.0916698, recall 0.785413
2017-12-10T03:00:44.417845: step 5589, loss 0.0710386, acc 0.96875, prec 0.0916672, recall 0.785413
2017-12-10T03:00:44.687804: step 5590, loss 0.295119, acc 0.9375, prec 0.091675, recall 0.785439
2017-12-10T03:00:44.953401: step 5591, loss 0.55426, acc 0.9375, prec 0.091696, recall 0.785492
2017-12-10T03:00:45.220553: step 5592, loss 0.163499, acc 0.96875, prec 0.0917197, recall 0.785546
2017-12-10T03:00:45.486174: step 5593, loss 0.335315, acc 0.90625, prec 0.0917117, recall 0.785546
2017-12-10T03:00:45.758842: step 5594, loss 0.321806, acc 0.90625, prec 0.0917301, recall 0.785599
2017-12-10T03:00:46.029581: step 5595, loss 0.339844, acc 0.890625, prec 0.0917471, recall 0.785652
2017-12-10T03:00:46.298806: step 5596, loss 0.280587, acc 0.96875, prec 0.0917708, recall 0.785705
2017-12-10T03:00:46.561126: step 5597, loss 0.229365, acc 0.921875, prec 0.0917773, recall 0.785732
2017-12-10T03:00:46.825317: step 5598, loss 0.185543, acc 0.953125, prec 0.0917733, recall 0.785732
2017-12-10T03:00:47.094332: step 5599, loss 0.306434, acc 0.953125, prec 0.0917693, recall 0.785732
2017-12-10T03:00:47.368886: step 5600, loss 0.215791, acc 0.921875, prec 0.0917758, recall 0.785759
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-5600

2017-12-10T03:00:48.759186: step 5601, loss 0.729435, acc 0.890625, prec 0.0917665, recall 0.785759
2017-12-10T03:00:49.024360: step 5602, loss 0.126356, acc 0.953125, prec 0.0917888, recall 0.785812
2017-12-10T03:00:49.290911: step 5603, loss 0.224855, acc 0.96875, prec 0.0917993, recall 0.785838
2017-12-10T03:00:49.562044: step 5604, loss 0.106184, acc 0.953125, prec 0.0918217, recall 0.785891
2017-12-10T03:00:49.824365: step 5605, loss 0.138077, acc 0.953125, prec 0.0918308, recall 0.785918
2017-12-10T03:00:50.096632: step 5606, loss 0.304041, acc 0.921875, prec 0.0918505, recall 0.785971
2017-12-10T03:00:50.362664: step 5607, loss 0.436171, acc 0.921875, prec 0.091857, recall 0.785998
2017-12-10T03:00:50.630111: step 5608, loss 1.39595, acc 0.9375, prec 0.0918648, recall 0.786024
2017-12-10T03:00:50.891816: step 5609, loss 0.707327, acc 0.953125, prec 0.0919003, recall 0.786104
2017-12-10T03:00:51.168270: step 5610, loss 0.477513, acc 0.984375, prec 0.0919647, recall 0.786236
2017-12-10T03:00:51.441870: step 5611, loss 0.345157, acc 0.9375, prec 0.0919593, recall 0.786236
2017-12-10T03:00:51.709293: step 5612, loss 0.424596, acc 0.921875, prec 0.0919658, recall 0.786262
2017-12-10T03:00:51.972548: step 5613, loss 0.0636639, acc 0.96875, prec 0.0919895, recall 0.786315
2017-12-10T03:00:52.233372: step 5614, loss 0.208997, acc 0.921875, prec 0.0919828, recall 0.786315
2017-12-10T03:00:52.505884: step 5615, loss 0.180845, acc 0.953125, prec 0.0920051, recall 0.786368
2017-12-10T03:00:52.769363: step 5616, loss 0.176969, acc 0.96875, prec 0.0920156, recall 0.786395
2017-12-10T03:00:53.047137: step 5617, loss 0.18709, acc 0.9375, prec 0.0920234, recall 0.786421
2017-12-10T03:00:53.316052: step 5618, loss 0.379195, acc 0.953125, prec 0.0920325, recall 0.786447
2017-12-10T03:00:53.589365: step 5619, loss 0.104132, acc 0.984375, prec 0.0920312, recall 0.786447
2017-12-10T03:00:53.856588: step 5620, loss 0.276166, acc 0.953125, prec 0.0920403, recall 0.786474
2017-12-10T03:00:54.122559: step 5621, loss 0.279941, acc 0.953125, prec 0.0920626, recall 0.786527
2017-12-10T03:00:54.389912: step 5622, loss 0.0665429, acc 0.96875, prec 0.09206, recall 0.786527
2017-12-10T03:00:54.657460: step 5623, loss 0.0811193, acc 0.984375, prec 0.0920586, recall 0.786527
2017-12-10T03:00:54.925991: step 5624, loss 0.245633, acc 0.96875, prec 0.092056, recall 0.786527
2017-12-10T03:00:55.193158: step 5625, loss 0.434889, acc 0.9375, prec 0.0920769, recall 0.786579
2017-12-10T03:00:55.458573: step 5626, loss 0.256467, acc 0.984375, prec 0.0921018, recall 0.786632
2017-12-10T03:00:55.722467: step 5627, loss 0.0162706, acc 0.984375, prec 0.0921005, recall 0.786632
2017-12-10T03:00:55.994401: step 5628, loss 0.0767562, acc 0.984375, prec 0.0920992, recall 0.786632
2017-12-10T03:00:56.262150: step 5629, loss 0.131909, acc 0.953125, prec 0.0920952, recall 0.786632
2017-12-10T03:00:56.538358: step 5630, loss 0.0954452, acc 0.953125, prec 0.0920912, recall 0.786632
2017-12-10T03:00:56.805804: step 5631, loss 0.0188559, acc 0.984375, prec 0.0920898, recall 0.786632
2017-12-10T03:00:57.074868: step 5632, loss 0.286385, acc 0.96875, prec 0.0920872, recall 0.786632
2017-12-10T03:00:57.342667: step 5633, loss 0.0909295, acc 0.96875, prec 0.0920977, recall 0.786658
2017-12-10T03:00:57.610058: step 5634, loss 0.000263247, acc 1, prec 0.0921239, recall 0.786711
2017-12-10T03:00:57.879831: step 5635, loss 0.00232758, acc 1, prec 0.0921239, recall 0.786711
2017-12-10T03:00:58.146131: step 5636, loss 0.243037, acc 0.96875, prec 0.0921475, recall 0.786764
2017-12-10T03:00:58.414512: step 5637, loss 0.0194322, acc 1, prec 0.0921606, recall 0.78679
2017-12-10T03:00:58.683364: step 5638, loss 0.0150824, acc 0.984375, prec 0.0921593, recall 0.78679
2017-12-10T03:00:58.946981: step 5639, loss 0.024812, acc 0.984375, prec 0.092158, recall 0.78679
2017-12-10T03:00:59.211079: step 5640, loss 0.248862, acc 1, prec 0.0921711, recall 0.786816
2017-12-10T03:00:59.474826: step 5641, loss 0.000920898, acc 1, prec 0.0921711, recall 0.786816
2017-12-10T03:00:59.740143: step 5642, loss 0.0780012, acc 0.96875, prec 0.0922078, recall 0.786895
2017-12-10T03:01:00.012844: step 5643, loss 0.000426773, acc 1, prec 0.0922078, recall 0.786895
2017-12-10T03:01:00.289526: step 5644, loss 0.00962015, acc 1, prec 0.0922078, recall 0.786895
2017-12-10T03:01:00.557835: step 5645, loss 0.185972, acc 0.96875, prec 0.0922183, recall 0.786922
2017-12-10T03:01:00.830277: step 5646, loss 0.00733552, acc 1, prec 0.0922314, recall 0.786948
2017-12-10T03:01:01.095530: step 5647, loss 0.055758, acc 1, prec 0.0922445, recall 0.786974
2017-12-10T03:01:01.366934: step 5648, loss 0.00223327, acc 1, prec 0.0922445, recall 0.786974
2017-12-10T03:01:01.637989: step 5649, loss 0.00584372, acc 1, prec 0.0922576, recall 0.787
2017-12-10T03:01:01.909373: step 5650, loss 0.0066698, acc 1, prec 0.0922839, recall 0.787053
2017-12-10T03:01:02.176894: step 5651, loss 0.000794867, acc 1, prec 0.092297, recall 0.787079
2017-12-10T03:01:02.441252: step 5652, loss 0.100838, acc 0.984375, prec 0.0922957, recall 0.787079
2017-12-10T03:01:02.705791: step 5653, loss 0.00072535, acc 1, prec 0.0922957, recall 0.787079
2017-12-10T03:01:02.967622: step 5654, loss 0.00215442, acc 1, prec 0.0922957, recall 0.787079
2017-12-10T03:01:03.233801: step 5655, loss 0.0376736, acc 0.984375, prec 0.0923337, recall 0.787158
2017-12-10T03:01:03.500606: step 5656, loss 0.0229479, acc 0.984375, prec 0.0923455, recall 0.787184
2017-12-10T03:01:03.766914: step 5657, loss 0.057975, acc 0.984375, prec 0.0923704, recall 0.787237
2017-12-10T03:01:04.028907: step 5658, loss 0.00463141, acc 1, prec 0.0923704, recall 0.787237
2017-12-10T03:01:04.296823: step 5659, loss 0.0245598, acc 0.984375, prec 0.0923822, recall 0.787263
2017-12-10T03:01:04.560429: step 5660, loss 0.0282135, acc 0.984375, prec 0.092394, recall 0.787289
2017-12-10T03:01:04.826831: step 5661, loss 0.0230788, acc 1, prec 0.0924202, recall 0.787341
2017-12-10T03:01:05.102178: step 5662, loss 0.0326579, acc 0.96875, prec 0.0924175, recall 0.787341
2017-12-10T03:01:05.373974: step 5663, loss 0.00522103, acc 1, prec 0.0924175, recall 0.787341
2017-12-10T03:01:05.644987: step 5664, loss 0.00128463, acc 1, prec 0.0924175, recall 0.787341
2017-12-10T03:01:05.914472: step 5665, loss 0.00127363, acc 1, prec 0.0924175, recall 0.787341
2017-12-10T03:01:06.182688: step 5666, loss 0.666756, acc 0.96875, prec 0.0924411, recall 0.787394
2017-12-10T03:01:06.453930: step 5667, loss 0.20391, acc 0.96875, prec 0.0924516, recall 0.78742
2017-12-10T03:01:06.723174: step 5668, loss 0.00517517, acc 1, prec 0.0924647, recall 0.787446
2017-12-10T03:01:06.985805: step 5669, loss 0.0056833, acc 1, prec 0.0924778, recall 0.787472
2017-12-10T03:01:07.258330: step 5670, loss 0.0729501, acc 0.96875, prec 0.0924882, recall 0.787498
2017-12-10T03:01:07.523402: step 5671, loss 1.6985, acc 0.984375, prec 0.0924882, recall 0.787402
2017-12-10T03:01:07.799240: step 5672, loss 0.123614, acc 0.96875, prec 0.0924987, recall 0.787428
2017-12-10T03:01:08.067102: step 5673, loss 0.0920648, acc 0.984375, prec 0.0925104, recall 0.787454
2017-12-10T03:01:08.330947: step 5674, loss 0.339221, acc 0.9375, prec 0.0925313, recall 0.787506
2017-12-10T03:01:08.606534: step 5675, loss 0.160394, acc 0.96875, prec 0.0925549, recall 0.787558
2017-12-10T03:01:08.870505: step 5676, loss 0.18576, acc 0.953125, prec 0.092564, recall 0.787584
2017-12-10T03:01:09.131095: step 5677, loss 0.078468, acc 0.96875, prec 0.0925744, recall 0.787611
2017-12-10T03:01:09.398354: step 5678, loss 0.315768, acc 0.953125, prec 0.0925835, recall 0.787637
2017-12-10T03:01:09.661824: step 5679, loss 0.429896, acc 0.9375, prec 0.0926044, recall 0.787689
2017-12-10T03:01:09.934322: step 5680, loss 0.233163, acc 0.96875, prec 0.092641, recall 0.787767
2017-12-10T03:01:10.204131: step 5681, loss 0.313071, acc 0.953125, prec 0.0926763, recall 0.787845
2017-12-10T03:01:10.463411: step 5682, loss 0.0621047, acc 0.953125, prec 0.0926854, recall 0.787871
2017-12-10T03:01:10.731634: step 5683, loss 0.304176, acc 0.90625, prec 0.0926774, recall 0.787871
2017-12-10T03:01:10.996851: step 5684, loss 0.359621, acc 0.890625, prec 0.0926811, recall 0.787897
2017-12-10T03:01:11.263972: step 5685, loss 0.536281, acc 0.890625, prec 0.0926848, recall 0.787923
2017-12-10T03:01:11.540334: step 5686, loss 0.182699, acc 0.921875, prec 0.0926781, recall 0.787923
2017-12-10T03:01:11.807921: step 5687, loss 0.752635, acc 0.9375, prec 0.0927121, recall 0.788001
2017-12-10T03:01:12.071447: step 5688, loss 0.267865, acc 0.921875, prec 0.0927185, recall 0.788027
2017-12-10T03:01:12.334052: step 5689, loss 0.331913, acc 0.890625, prec 0.0927222, recall 0.788053
2017-12-10T03:01:12.596496: step 5690, loss 0.351343, acc 0.890625, prec 0.092739, recall 0.788105
2017-12-10T03:01:12.863998: step 5691, loss 0.509147, acc 0.890625, prec 0.0927297, recall 0.788105
2017-12-10T03:01:13.129130: step 5692, loss 0.465578, acc 0.9375, prec 0.0927505, recall 0.788157
2017-12-10T03:01:13.391314: step 5693, loss 0.226047, acc 0.96875, prec 0.092774, recall 0.788209
2017-12-10T03:01:13.663496: step 5694, loss 0.0910164, acc 0.984375, prec 0.0928119, recall 0.788287
2017-12-10T03:01:13.937593: step 5695, loss 0.0957383, acc 0.984375, prec 0.0928237, recall 0.788313
2017-12-10T03:01:14.200382: step 5696, loss 0.0590232, acc 0.96875, prec 0.0928602, recall 0.788391
2017-12-10T03:01:14.466570: step 5697, loss 0.0133989, acc 1, prec 0.0928864, recall 0.788443
2017-12-10T03:01:14.728679: step 5698, loss 0.0587867, acc 0.96875, prec 0.0928968, recall 0.788469
2017-12-10T03:01:15.001829: step 5699, loss 0.0748757, acc 0.984375, prec 0.0929347, recall 0.788546
2017-12-10T03:01:15.266837: step 5700, loss 0.0733254, acc 0.984375, prec 0.0929726, recall 0.788624

Evaluation:
2017-12-10T03:01:22.750423: step 5700, loss 5.98301, acc 0.958392, prec 0.0933853, recall 0.781149

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-5700

2017-12-10T03:01:24.034063: step 5701, loss 0.0310992, acc 0.984375, prec 0.093384, recall 0.781149
2017-12-10T03:01:24.303409: step 5702, loss 0.160626, acc 0.96875, prec 0.0933943, recall 0.781175
2017-12-10T03:01:24.570672: step 5703, loss 0.00937563, acc 1, prec 0.0934073, recall 0.781201
2017-12-10T03:01:24.841853: step 5704, loss 1.4437, acc 0.96875, prec 0.093419, recall 0.781134
2017-12-10T03:01:25.118994: step 5705, loss 0.122506, acc 0.96875, prec 0.0934163, recall 0.781134
2017-12-10T03:01:25.392650: step 5706, loss 0.219759, acc 0.96875, prec 0.0934526, recall 0.781213
2017-12-10T03:01:25.664125: step 5707, loss 0.385635, acc 0.984375, prec 0.0934642, recall 0.781239
2017-12-10T03:01:25.932261: step 5708, loss 0.153023, acc 0.953125, prec 0.0934602, recall 0.781239
2017-12-10T03:01:26.201676: step 5709, loss 0.28718, acc 0.953125, prec 0.0934562, recall 0.781239
2017-12-10T03:01:26.477725: step 5710, loss 0.150447, acc 0.953125, prec 0.0934652, recall 0.781265
2017-12-10T03:01:26.745226: step 5711, loss 0.274908, acc 0.953125, prec 0.0934612, recall 0.781265
2017-12-10T03:01:27.014072: step 5712, loss 0.834452, acc 0.828125, prec 0.0934464, recall 0.781265
2017-12-10T03:01:27.283491: step 5713, loss 0.37146, acc 0.90625, prec 0.0934644, recall 0.781317
2017-12-10T03:01:27.558597: step 5714, loss 0.141709, acc 0.953125, prec 0.0934604, recall 0.781317
2017-12-10T03:01:27.818954: step 5715, loss 0.177605, acc 0.9375, prec 0.093468, recall 0.781344
2017-12-10T03:01:28.083597: step 5716, loss 0.536588, acc 0.96875, prec 0.0934783, recall 0.78137
2017-12-10T03:01:28.355401: step 5717, loss 0.0371702, acc 0.984375, prec 0.0935029, recall 0.781422
2017-12-10T03:01:28.618156: step 5718, loss 0.519692, acc 0.90625, prec 0.0934949, recall 0.781422
2017-12-10T03:01:28.884512: step 5719, loss 0.435957, acc 0.953125, prec 0.0935298, recall 0.781501
2017-12-10T03:01:29.150822: step 5720, loss 0.0772548, acc 0.96875, prec 0.0935271, recall 0.781501
2017-12-10T03:01:29.417628: step 5721, loss 0.354118, acc 0.875, prec 0.0935164, recall 0.781501
2017-12-10T03:01:29.685291: step 5722, loss 0.356411, acc 0.9375, prec 0.0935111, recall 0.781501
2017-12-10T03:01:29.956081: step 5723, loss 0.196635, acc 0.96875, prec 0.0935084, recall 0.781501
2017-12-10T03:01:30.227784: step 5724, loss 0.0619277, acc 0.984375, prec 0.09352, recall 0.781527
2017-12-10T03:01:30.496551: step 5725, loss 0.611676, acc 0.984375, prec 0.0935317, recall 0.781553
2017-12-10T03:01:30.763578: step 5726, loss 0.0572135, acc 0.984375, prec 0.0935303, recall 0.781553
2017-12-10T03:01:31.029599: step 5727, loss 0.288087, acc 0.96875, prec 0.0935277, recall 0.781553
2017-12-10T03:01:31.301041: step 5728, loss 0.202631, acc 0.96875, prec 0.093525, recall 0.781553
2017-12-10T03:01:31.566516: step 5729, loss 0.169467, acc 0.96875, prec 0.0935353, recall 0.781579
2017-12-10T03:01:31.833336: step 5730, loss 0.00210048, acc 1, prec 0.0935482, recall 0.781605
2017-12-10T03:01:32.093206: step 5731, loss 0.0778497, acc 0.96875, prec 0.0935715, recall 0.781657
2017-12-10T03:01:32.357977: step 5732, loss 0.0251874, acc 0.984375, prec 0.0935961, recall 0.781709
2017-12-10T03:01:32.625535: step 5733, loss 0.0112521, acc 1, prec 0.0936091, recall 0.781736
2017-12-10T03:01:32.890042: step 5734, loss 0.00153033, acc 1, prec 0.0936221, recall 0.781762
2017-12-10T03:01:33.160725: step 5735, loss 2.25027, acc 0.96875, prec 0.0936337, recall 0.781694
2017-12-10T03:01:33.433669: step 5736, loss 0.434899, acc 0.9375, prec 0.0936283, recall 0.781694
2017-12-10T03:01:33.707308: step 5737, loss 0.290696, acc 0.984375, prec 0.0936529, recall 0.781747
2017-12-10T03:01:33.982809: step 5738, loss 0.00822251, acc 1, prec 0.0936659, recall 0.781773
2017-12-10T03:01:34.250094: step 5739, loss 0.233276, acc 0.96875, prec 0.0936632, recall 0.781773
2017-12-10T03:01:34.518484: step 5740, loss 0.338201, acc 0.9375, prec 0.0936709, recall 0.781799
2017-12-10T03:01:34.783763: step 5741, loss 0.191104, acc 0.953125, prec 0.0936928, recall 0.781851
2017-12-10T03:01:35.049636: step 5742, loss 0.0111164, acc 1, prec 0.0936928, recall 0.781851
2017-12-10T03:01:35.313902: step 5743, loss 0.0396415, acc 0.96875, prec 0.0937031, recall 0.781877
2017-12-10T03:01:35.583428: step 5744, loss 0.24511, acc 0.9375, prec 0.0936977, recall 0.781877
2017-12-10T03:01:35.857780: step 5745, loss 0.344619, acc 0.921875, prec 0.093691, recall 0.781877
2017-12-10T03:01:36.133190: step 5746, loss 0.0767904, acc 0.984375, prec 0.0937156, recall 0.781929
2017-12-10T03:01:36.404576: step 5747, loss 0.431551, acc 0.9375, prec 0.0937232, recall 0.781955
2017-12-10T03:01:36.667913: step 5748, loss 0.737465, acc 0.921875, prec 0.0937165, recall 0.781955
2017-12-10T03:01:36.934583: step 5749, loss 0.0884171, acc 0.96875, prec 0.0937397, recall 0.782007
2017-12-10T03:01:37.202233: step 5750, loss 0.0734296, acc 0.96875, prec 0.09375, recall 0.782033
2017-12-10T03:01:37.473004: step 5751, loss 0.276957, acc 0.953125, prec 0.093746, recall 0.782033
2017-12-10T03:01:37.739264: step 5752, loss 0.154846, acc 0.953125, prec 0.093742, recall 0.782033
2017-12-10T03:01:38.010996: step 5753, loss 0.0289874, acc 0.984375, prec 0.0937406, recall 0.782033
2017-12-10T03:01:38.275137: step 5754, loss 0.184873, acc 0.953125, prec 0.0937496, recall 0.782059
2017-12-10T03:01:38.543543: step 5755, loss 0.262654, acc 0.9375, prec 0.0937572, recall 0.782085
2017-12-10T03:01:38.816611: step 5756, loss 0.0402748, acc 0.984375, prec 0.0937817, recall 0.782137
2017-12-10T03:01:39.085482: step 5757, loss 0.132186, acc 0.96875, prec 0.093779, recall 0.782137
2017-12-10T03:01:39.356535: step 5758, loss 0.111082, acc 0.984375, prec 0.0937907, recall 0.782163
2017-12-10T03:01:39.618483: step 5759, loss 0.268376, acc 0.96875, prec 0.0938009, recall 0.782189
2017-12-10T03:01:39.894539: step 5760, loss 0.0388642, acc 0.984375, prec 0.0938125, recall 0.782215
2017-12-10T03:01:40.164022: step 5761, loss 0.102983, acc 0.984375, prec 0.0938242, recall 0.782241
2017-12-10T03:01:40.439616: step 5762, loss 0.247407, acc 0.953125, prec 0.093859, recall 0.782319
2017-12-10T03:01:40.704262: step 5763, loss 0.00738157, acc 1, prec 0.0938849, recall 0.78237
2017-12-10T03:01:40.972631: step 5764, loss 0.245167, acc 0.96875, prec 0.0938952, recall 0.782396
2017-12-10T03:01:41.241752: step 5765, loss 0.0195229, acc 1, prec 0.093934, recall 0.782474
2017-12-10T03:01:41.517226: step 5766, loss 0.0257032, acc 0.984375, prec 0.0939327, recall 0.782474
2017-12-10T03:01:41.785031: step 5767, loss 0.184637, acc 0.96875, prec 0.0939429, recall 0.7825
2017-12-10T03:01:42.048525: step 5768, loss 0.250945, acc 0.953125, prec 0.0939389, recall 0.7825
2017-12-10T03:01:43.037566: step 5769, loss 0.0440212, acc 0.96875, prec 0.0939362, recall 0.7825
2017-12-10T03:01:43.396492: step 5770, loss 0.00322585, acc 1, prec 0.0939362, recall 0.7825
2017-12-10T03:01:43.663734: step 5771, loss 2.56141, acc 0.96875, prec 0.0939349, recall 0.782407
2017-12-10T03:01:44.377617: step 5772, loss 0.0661219, acc 0.984375, prec 0.0939465, recall 0.782433
2017-12-10T03:01:45.120039: step 5773, loss 0.047543, acc 0.984375, prec 0.0939452, recall 0.782433
2017-12-10T03:01:45.854180: step 5774, loss 0.158082, acc 0.984375, prec 0.0939438, recall 0.782433
2017-12-10T03:01:46.574025: step 5775, loss 0.000858907, acc 1, prec 0.0939568, recall 0.782459
2017-12-10T03:01:47.559025: step 5776, loss 0.0762682, acc 0.96875, prec 0.09398, recall 0.78251
2017-12-10T03:01:47.908424: step 5777, loss 0.283313, acc 0.96875, prec 0.0939902, recall 0.782536
2017-12-10T03:01:48.245347: step 5778, loss 0.00399459, acc 1, prec 0.0939902, recall 0.782536
2017-12-10T03:01:48.527906: step 5779, loss 0.252373, acc 0.9375, prec 0.0940107, recall 0.782588
2017-12-10T03:01:48.808742: step 5780, loss 0.56892, acc 0.96875, prec 0.0940339, recall 0.78264
2017-12-10T03:01:49.076695: step 5781, loss 0.167161, acc 0.984375, prec 0.0940844, recall 0.782743
2017-12-10T03:01:49.352192: step 5782, loss 0.0958405, acc 0.984375, prec 0.0941089, recall 0.782795
2017-12-10T03:01:49.628720: step 5783, loss 0.0928706, acc 0.96875, prec 0.0941062, recall 0.782795
2017-12-10T03:01:49.904585: step 5784, loss 0.0854235, acc 0.96875, prec 0.0941165, recall 0.78282
2017-12-10T03:01:50.180966: step 5785, loss 0.233383, acc 0.953125, prec 0.0941124, recall 0.78282
2017-12-10T03:01:50.443494: step 5786, loss 0.0396209, acc 0.984375, prec 0.0941111, recall 0.78282
2017-12-10T03:01:50.706306: step 5787, loss 0.312613, acc 0.953125, prec 0.0941071, recall 0.78282
2017-12-10T03:01:50.977993: step 5788, loss 0.09053, acc 0.96875, prec 0.0941044, recall 0.78282
2017-12-10T03:01:51.245980: step 5789, loss 0.0325695, acc 0.984375, prec 0.094103, recall 0.78282
2017-12-10T03:01:51.509911: step 5790, loss 0.0373533, acc 0.984375, prec 0.0941146, recall 0.782846
2017-12-10T03:01:51.774981: step 5791, loss 0.332678, acc 0.9375, prec 0.0941092, recall 0.782846
2017-12-10T03:01:52.038440: step 5792, loss 0.126685, acc 0.9375, prec 0.0941039, recall 0.782846
2017-12-10T03:01:52.301584: step 5793, loss 0.430464, acc 0.96875, prec 0.0941271, recall 0.782898
2017-12-10T03:01:52.570059: step 5794, loss 0.00633613, acc 1, prec 0.09414, recall 0.782924
2017-12-10T03:01:52.831948: step 5795, loss 0.0884209, acc 0.953125, prec 0.094136, recall 0.782924
2017-12-10T03:01:53.098410: step 5796, loss 0.187196, acc 0.953125, prec 0.0941449, recall 0.782949
2017-12-10T03:01:53.358962: step 5797, loss 0.218383, acc 0.953125, prec 0.0941667, recall 0.783001
2017-12-10T03:01:53.632607: step 5798, loss 0.0205513, acc 0.984375, prec 0.0941653, recall 0.783001
2017-12-10T03:01:53.895956: step 5799, loss 0.281009, acc 0.953125, prec 0.0941742, recall 0.783027
2017-12-10T03:01:54.161297: step 5800, loss 0.331774, acc 0.96875, prec 0.0941716, recall 0.783027
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-5800

2017-12-10T03:01:55.474234: step 5801, loss 0.147679, acc 0.96875, prec 0.0941818, recall 0.783052
2017-12-10T03:01:55.736954: step 5802, loss 0.0864085, acc 0.96875, prec 0.0941791, recall 0.783052
2017-12-10T03:01:56.006000: step 5803, loss 0.259314, acc 0.984375, prec 0.0941907, recall 0.783078
2017-12-10T03:01:56.274961: step 5804, loss 0.0339828, acc 0.984375, prec 0.0942023, recall 0.783104
2017-12-10T03:01:56.545881: step 5805, loss 0.00347563, acc 1, prec 0.0942152, recall 0.78313
2017-12-10T03:01:56.819514: step 5806, loss 0.0423547, acc 0.984375, prec 0.0942397, recall 0.783181
2017-12-10T03:01:57.087692: step 5807, loss 0.0185689, acc 0.984375, prec 0.0942642, recall 0.783233
2017-12-10T03:01:57.355975: step 5808, loss 0.00223574, acc 1, prec 0.0942642, recall 0.783233
2017-12-10T03:01:57.621423: step 5809, loss 0.04984, acc 0.984375, prec 0.0942758, recall 0.783258
2017-12-10T03:01:57.891578: step 5810, loss 0.861811, acc 1, prec 0.0943017, recall 0.78331
2017-12-10T03:01:58.167758: step 5811, loss 6.91709, acc 0.984375, prec 0.0943146, recall 0.783242
2017-12-10T03:01:58.438150: step 5812, loss 0.0173346, acc 0.984375, prec 0.0943262, recall 0.783268
2017-12-10T03:01:58.704593: step 5813, loss 0.0773739, acc 0.96875, prec 0.0943235, recall 0.783268
2017-12-10T03:01:58.970247: step 5814, loss 0.342342, acc 0.953125, prec 0.0943582, recall 0.783345
2017-12-10T03:01:59.242088: step 5815, loss 0.114156, acc 0.96875, prec 0.0943813, recall 0.783396
2017-12-10T03:01:59.512452: step 5816, loss 0.150301, acc 0.96875, prec 0.0943787, recall 0.783396
2017-12-10T03:01:59.779333: step 5817, loss 0.570996, acc 0.890625, prec 0.0943822, recall 0.783422
2017-12-10T03:02:00.063906: step 5818, loss 0.828537, acc 0.875, prec 0.0943714, recall 0.783422
2017-12-10T03:02:00.339227: step 5819, loss 1.03907, acc 0.8125, prec 0.0943681, recall 0.783448
2017-12-10T03:02:00.607032: step 5820, loss 0.75769, acc 0.875, prec 0.0943703, recall 0.783473
2017-12-10T03:02:00.868183: step 5821, loss 0.750588, acc 0.8125, prec 0.0943671, recall 0.783499
2017-12-10T03:02:01.140573: step 5822, loss 0.617136, acc 0.859375, prec 0.094355, recall 0.783499
2017-12-10T03:02:01.406211: step 5823, loss 0.759285, acc 0.859375, prec 0.0943558, recall 0.783525
2017-12-10T03:02:01.671205: step 5824, loss 0.461465, acc 0.84375, prec 0.0943423, recall 0.783525
2017-12-10T03:02:01.937056: step 5825, loss 0.878179, acc 0.796875, prec 0.0943377, recall 0.78355
2017-12-10T03:02:02.203522: step 5826, loss 0.627134, acc 0.859375, prec 0.0943386, recall 0.783576
2017-12-10T03:02:02.476533: step 5827, loss 0.373922, acc 0.890625, prec 0.094342, recall 0.783602
2017-12-10T03:02:02.739924: step 5828, loss 0.172894, acc 0.96875, prec 0.0943523, recall 0.783627
2017-12-10T03:02:03.016501: step 5829, loss 0.415144, acc 0.859375, prec 0.094366, recall 0.783678
2017-12-10T03:02:03.291859: step 5830, loss 0.164396, acc 0.9375, prec 0.0943606, recall 0.783678
2017-12-10T03:02:03.570080: step 5831, loss 0.276071, acc 0.953125, prec 0.0943566, recall 0.783678
2017-12-10T03:02:03.846914: step 5832, loss 0.105025, acc 0.953125, prec 0.0943525, recall 0.783678
2017-12-10T03:02:04.117436: step 5833, loss 0.264569, acc 0.953125, prec 0.0943485, recall 0.783678
2017-12-10T03:02:04.391719: step 5834, loss 0.255413, acc 0.921875, prec 0.0943547, recall 0.783704
2017-12-10T03:02:04.657825: step 5835, loss 0.144949, acc 0.96875, prec 0.094352, recall 0.783704
2017-12-10T03:02:04.919899: step 5836, loss 0.11124, acc 0.984375, prec 0.0943635, recall 0.783729
2017-12-10T03:02:05.179496: step 5837, loss 0.0456593, acc 0.984375, prec 0.0943622, recall 0.783729
2017-12-10T03:02:05.442974: step 5838, loss 0.627013, acc 1, prec 0.0943751, recall 0.783755
2017-12-10T03:02:05.706416: step 5839, loss 0.0275468, acc 0.984375, prec 0.0943866, recall 0.783781
2017-12-10T03:02:05.970060: step 5840, loss 0.546451, acc 0.953125, prec 0.0944084, recall 0.783832
2017-12-10T03:02:06.237042: step 5841, loss 0.170341, acc 0.953125, prec 0.0944172, recall 0.783857
2017-12-10T03:02:06.506965: step 5842, loss 0.0706818, acc 0.984375, prec 0.0944288, recall 0.783883
2017-12-10T03:02:06.772208: step 5843, loss 0.0193161, acc 1, prec 0.0944417, recall 0.783908
2017-12-10T03:02:07.040756: step 5844, loss 0.0131424, acc 1, prec 0.0944546, recall 0.783934
2017-12-10T03:02:07.305628: step 5845, loss 0.544398, acc 0.953125, prec 0.0944634, recall 0.783959
2017-12-10T03:02:07.567956: step 5846, loss 0.904242, acc 0.921875, prec 0.0944696, recall 0.783985
2017-12-10T03:02:07.843857: step 5847, loss 0.0493624, acc 0.984375, prec 0.0944811, recall 0.78401
2017-12-10T03:02:08.117501: step 5848, loss 0.309275, acc 0.96875, prec 0.0945042, recall 0.784061
2017-12-10T03:02:08.388137: step 5849, loss 0.0530694, acc 0.984375, prec 0.0945158, recall 0.784087
2017-12-10T03:02:08.654227: step 5850, loss 0.00445724, acc 1, prec 0.0945286, recall 0.784112
2017-12-10T03:02:08.924483: step 5851, loss 0.0772583, acc 0.984375, prec 0.0945273, recall 0.784112
2017-12-10T03:02:09.193862: step 5852, loss 0.0106737, acc 1, prec 0.0945402, recall 0.784138
2017-12-10T03:02:09.455933: step 5853, loss 0.00533224, acc 1, prec 0.0945531, recall 0.784163
2017-12-10T03:02:09.721236: step 5854, loss 0.212463, acc 0.953125, prec 0.094549, recall 0.784163
2017-12-10T03:02:09.986175: step 5855, loss 0.128566, acc 0.96875, prec 0.0945592, recall 0.784189
2017-12-10T03:02:10.249914: step 5856, loss 0.358317, acc 0.9375, prec 0.0945796, recall 0.78424
2017-12-10T03:02:10.521538: step 5857, loss 0.0873911, acc 0.984375, prec 0.0945911, recall 0.784265
2017-12-10T03:02:10.784665: step 5858, loss 0.340373, acc 0.96875, prec 0.0945884, recall 0.784265
2017-12-10T03:02:11.054481: step 5859, loss 0.061772, acc 0.96875, prec 0.0946115, recall 0.784316
2017-12-10T03:02:11.327617: step 5860, loss 0.10161, acc 0.953125, prec 0.0946075, recall 0.784316
2017-12-10T03:02:11.605946: step 5861, loss 0.115594, acc 0.96875, prec 0.0946177, recall 0.784341
2017-12-10T03:02:11.868484: step 5862, loss 0.251606, acc 0.9375, prec 0.0946252, recall 0.784367
2017-12-10T03:02:12.140344: step 5863, loss 0.0775706, acc 0.953125, prec 0.0946211, recall 0.784367
2017-12-10T03:02:12.407061: step 5864, loss 0.10722, acc 0.96875, prec 0.0946184, recall 0.784367
2017-12-10T03:02:12.678100: step 5865, loss 0.265411, acc 0.9375, prec 0.0946259, recall 0.784392
2017-12-10T03:02:12.943003: step 5866, loss 0.153127, acc 0.953125, prec 0.0946219, recall 0.784392
2017-12-10T03:02:13.213061: step 5867, loss 0.522987, acc 0.953125, prec 0.0946307, recall 0.784418
2017-12-10T03:02:13.482912: step 5868, loss 0.245256, acc 0.9375, prec 0.0946382, recall 0.784443
2017-12-10T03:02:13.750209: step 5869, loss 0.00796078, acc 1, prec 0.094664, recall 0.784494
2017-12-10T03:02:14.013502: step 5870, loss 0.0405646, acc 0.984375, prec 0.0946755, recall 0.784519
2017-12-10T03:02:14.281770: step 5871, loss 0.221884, acc 0.921875, prec 0.0946688, recall 0.784519
2017-12-10T03:02:14.555691: step 5872, loss 0.0558013, acc 0.984375, prec 0.0946803, recall 0.784545
2017-12-10T03:02:14.816617: step 5873, loss 0.0443775, acc 0.984375, prec 0.0946789, recall 0.784545
2017-12-10T03:02:15.084181: step 5874, loss 0.0613349, acc 0.96875, prec 0.0946762, recall 0.784545
2017-12-10T03:02:15.357073: step 5875, loss 0.00318724, acc 1, prec 0.094702, recall 0.784595
2017-12-10T03:02:15.620941: step 5876, loss 0.0814072, acc 0.984375, prec 0.0947264, recall 0.784646
2017-12-10T03:02:15.886498: step 5877, loss 0.000769465, acc 1, prec 0.0947264, recall 0.784646
2017-12-10T03:02:16.158058: step 5878, loss 1.04018, acc 0.984375, prec 0.0947508, recall 0.784697
2017-12-10T03:02:16.434502: step 5879, loss 0.000683961, acc 1, prec 0.0947508, recall 0.784697
2017-12-10T03:02:16.704263: step 5880, loss 0.358997, acc 0.96875, prec 0.0947609, recall 0.784722
2017-12-10T03:02:16.970208: step 5881, loss 0.0033325, acc 1, prec 0.0947867, recall 0.784773
2017-12-10T03:02:17.240320: step 5882, loss 0.0961092, acc 0.9375, prec 0.0947813, recall 0.784773
2017-12-10T03:02:17.509177: step 5883, loss 0.0856656, acc 0.953125, prec 0.0947901, recall 0.784798
2017-12-10T03:02:17.773312: step 5884, loss 0.0153647, acc 1, prec 0.0947901, recall 0.784798
2017-12-10T03:02:18.043211: step 5885, loss 0.0554362, acc 0.96875, prec 0.0948131, recall 0.784849
2017-12-10T03:02:18.314530: step 5886, loss 0.14614, acc 0.96875, prec 0.0948233, recall 0.784874
2017-12-10T03:02:18.584589: step 5887, loss 0.152441, acc 0.96875, prec 0.0948206, recall 0.784874
2017-12-10T03:02:18.850244: step 5888, loss 0.21936, acc 0.96875, prec 0.0948179, recall 0.784874
2017-12-10T03:02:19.123060: step 5889, loss 0.426254, acc 0.984375, prec 0.0948294, recall 0.784899
2017-12-10T03:02:19.397296: step 5890, loss 0.0757239, acc 0.96875, prec 0.0948267, recall 0.784899
2017-12-10T03:02:19.664596: step 5891, loss 0.0300022, acc 0.984375, prec 0.0948254, recall 0.784899
2017-12-10T03:02:19.927835: step 5892, loss 0.0468814, acc 0.984375, prec 0.094824, recall 0.784899
2017-12-10T03:02:20.197363: step 5893, loss 0.149941, acc 0.953125, prec 0.0948329, recall 0.784925
2017-12-10T03:02:20.464980: step 5894, loss 0.050197, acc 0.984375, prec 0.0948315, recall 0.784925
2017-12-10T03:02:20.729840: step 5895, loss 0.157378, acc 0.984375, prec 0.0948302, recall 0.784925
2017-12-10T03:02:20.996719: step 5896, loss 0.0677244, acc 0.984375, prec 0.0948545, recall 0.784975
2017-12-10T03:02:21.266193: step 5897, loss 0.00467936, acc 1, prec 0.0948802, recall 0.785026
2017-12-10T03:02:21.532554: step 5898, loss 0.116914, acc 0.984375, prec 0.0948918, recall 0.785051
2017-12-10T03:02:21.795880: step 5899, loss 0.1802, acc 0.984375, prec 0.0949161, recall 0.785102
2017-12-10T03:02:22.061861: step 5900, loss 0.259372, acc 0.9375, prec 0.0949364, recall 0.785152
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-5900

2017-12-10T03:02:23.478878: step 5901, loss 0.0114644, acc 1, prec 0.0949364, recall 0.785152
2017-12-10T03:02:23.748455: step 5902, loss 0.148584, acc 0.96875, prec 0.0949337, recall 0.785152
2017-12-10T03:02:24.013529: step 5903, loss 0.432177, acc 0.9375, prec 0.0949412, recall 0.785177
2017-12-10T03:02:24.276855: step 5904, loss 0.120404, acc 0.953125, prec 0.0949372, recall 0.785177
2017-12-10T03:02:24.551484: step 5905, loss 1.09519, acc 0.96875, prec 0.0949602, recall 0.785228
2017-12-10T03:02:24.827825: step 5906, loss 0.028189, acc 0.984375, prec 0.0949588, recall 0.785228
2017-12-10T03:02:25.090150: step 5907, loss 0.00736367, acc 1, prec 0.0949717, recall 0.785253
2017-12-10T03:02:25.353589: step 5908, loss 0.336908, acc 0.921875, prec 0.0949778, recall 0.785278
2017-12-10T03:02:25.620274: step 5909, loss 0.100158, acc 0.984375, prec 0.0949764, recall 0.785278
2017-12-10T03:02:25.881669: step 5910, loss 0.176482, acc 0.921875, prec 0.0949825, recall 0.785303
2017-12-10T03:02:26.146661: step 5911, loss 0.189176, acc 0.96875, prec 0.0949798, recall 0.785303
2017-12-10T03:02:26.417307: step 5912, loss 0.105725, acc 0.9375, prec 0.0950001, recall 0.785354
2017-12-10T03:02:26.700107: step 5913, loss 0.428136, acc 0.9375, prec 0.0949947, recall 0.785354
2017-12-10T03:02:26.964559: step 5914, loss 0.0159254, acc 1, prec 0.0950204, recall 0.785404
2017-12-10T03:02:27.232318: step 5915, loss 0.150532, acc 0.96875, prec 0.0950306, recall 0.785429
2017-12-10T03:02:27.507145: step 5916, loss 0.199475, acc 0.9375, prec 0.095038, recall 0.785455
2017-12-10T03:02:27.770662: step 5917, loss 0.144421, acc 0.953125, prec 0.0950725, recall 0.78553
2017-12-10T03:02:28.039469: step 5918, loss 0.393254, acc 0.9375, prec 0.0950671, recall 0.78553
2017-12-10T03:02:28.304429: step 5919, loss 0.201189, acc 0.96875, prec 0.0950644, recall 0.78553
2017-12-10T03:02:28.571415: step 5920, loss 0.161824, acc 0.984375, prec 0.0950631, recall 0.78553
2017-12-10T03:02:28.831204: step 5921, loss 0.166975, acc 0.890625, prec 0.0950536, recall 0.78553
2017-12-10T03:02:29.103326: step 5922, loss 0.094035, acc 0.96875, prec 0.0950638, recall 0.785555
2017-12-10T03:02:29.367187: step 5923, loss 0.302765, acc 0.953125, prec 0.0950726, recall 0.78558
2017-12-10T03:02:29.634015: step 5924, loss 0.058132, acc 0.984375, prec 0.0950712, recall 0.78558
2017-12-10T03:02:29.899255: step 5925, loss 0.0174843, acc 1, prec 0.0950841, recall 0.785605
2017-12-10T03:02:30.169309: step 5926, loss 0.118225, acc 0.96875, prec 0.0950814, recall 0.785605
2017-12-10T03:02:30.439966: step 5927, loss 0.104338, acc 0.984375, prec 0.09508, recall 0.785605
2017-12-10T03:02:30.712039: step 5928, loss 0.109311, acc 0.96875, prec 0.0950902, recall 0.785631
2017-12-10T03:02:30.975823: step 5929, loss 0.0155664, acc 0.984375, prec 0.0950888, recall 0.785631
2017-12-10T03:02:31.240290: step 5930, loss 0.12095, acc 0.96875, prec 0.0951118, recall 0.785681
2017-12-10T03:02:31.509397: step 5931, loss 0.00463651, acc 1, prec 0.0951246, recall 0.785706
2017-12-10T03:02:31.775238: step 5932, loss 0.106454, acc 0.984375, prec 0.0951361, recall 0.785731
2017-12-10T03:02:32.047731: step 5933, loss 0.0649309, acc 0.984375, prec 0.0951604, recall 0.785781
2017-12-10T03:02:32.322203: step 5934, loss 0.0430954, acc 0.984375, prec 0.0951591, recall 0.785781
2017-12-10T03:02:32.596804: step 5935, loss 0.461627, acc 0.953125, prec 0.0951679, recall 0.785806
2017-12-10T03:02:32.863481: step 5936, loss 0.000367931, acc 1, prec 0.0951679, recall 0.785806
2017-12-10T03:02:33.845842: step 5937, loss 0.000472457, acc 1, prec 0.0951807, recall 0.785831
2017-12-10T03:02:34.293911: step 5938, loss 0.0272054, acc 0.984375, prec 0.0951793, recall 0.785831
2017-12-10T03:02:35.205293: step 5939, loss 0.00142522, acc 1, prec 0.0951922, recall 0.785856
2017-12-10T03:02:35.512539: step 5940, loss 1.84925, acc 0.953125, prec 0.0951895, recall 0.785764
2017-12-10T03:02:35.826932: step 5941, loss 0.00367079, acc 1, prec 0.0952023, recall 0.78579
2017-12-10T03:02:36.124828: step 5942, loss 0.00162354, acc 1, prec 0.0952023, recall 0.78579
2017-12-10T03:02:36.411098: step 5943, loss 0.00352886, acc 1, prec 0.0952151, recall 0.785815
2017-12-10T03:02:36.688747: step 5944, loss 0.0184283, acc 1, prec 0.095228, recall 0.78584
2017-12-10T03:02:36.967514: step 5945, loss 0.106391, acc 0.96875, prec 0.0952381, recall 0.785865
2017-12-10T03:02:37.237971: step 5946, loss 0.0718178, acc 0.984375, prec 0.0952496, recall 0.78589
2017-12-10T03:02:37.516842: step 5947, loss 0.208916, acc 0.96875, prec 0.0952597, recall 0.785915
2017-12-10T03:02:37.798758: step 5948, loss 0.0952254, acc 0.953125, prec 0.0952557, recall 0.785915
2017-12-10T03:02:38.080425: step 5949, loss 0.305127, acc 0.9375, prec 0.0952502, recall 0.785915
2017-12-10T03:02:38.350476: step 5950, loss 0.0549917, acc 0.96875, prec 0.0952475, recall 0.785915
2017-12-10T03:02:38.612942: step 5951, loss 0.0212053, acc 1, prec 0.0952732, recall 0.785965
2017-12-10T03:02:38.876909: step 5952, loss 0.321611, acc 0.96875, prec 0.0952833, recall 0.78599
2017-12-10T03:02:39.141710: step 5953, loss 0.0369198, acc 0.984375, prec 0.095282, recall 0.78599
2017-12-10T03:02:39.402489: step 5954, loss 0.107373, acc 0.953125, prec 0.0953036, recall 0.78604
2017-12-10T03:02:39.666891: step 5955, loss 0.373707, acc 0.9375, prec 0.0952982, recall 0.78604
2017-12-10T03:02:39.931056: step 5956, loss 0.231257, acc 0.9375, prec 0.0953184, recall 0.78609
2017-12-10T03:02:40.199779: step 5957, loss 0.0643053, acc 0.984375, prec 0.0953299, recall 0.786115
2017-12-10T03:02:40.466338: step 5958, loss 0.347716, acc 0.90625, prec 0.0953218, recall 0.786115
2017-12-10T03:02:40.730773: step 5959, loss 0.224869, acc 0.984375, prec 0.0953332, recall 0.78614
2017-12-10T03:02:40.995179: step 5960, loss 0.0337326, acc 0.984375, prec 0.0953447, recall 0.786165
2017-12-10T03:02:41.257383: step 5961, loss 0.0239664, acc 1, prec 0.095396, recall 0.786265
2017-12-10T03:02:41.533087: step 5962, loss 0.00465262, acc 1, prec 0.0954088, recall 0.78629
2017-12-10T03:02:41.799750: step 5963, loss 0.110089, acc 0.984375, prec 0.0954203, recall 0.786315
2017-12-10T03:02:42.027861: step 5964, loss 0.0107136, acc 1, prec 0.0954203, recall 0.786315
2017-12-10T03:02:42.301999: step 5965, loss 0.810765, acc 0.984375, prec 0.0954702, recall 0.786415
2017-12-10T03:02:42.568087: step 5966, loss 0.0359616, acc 0.984375, prec 0.0954817, recall 0.786439
2017-12-10T03:02:42.836841: step 5967, loss 0.0829924, acc 0.984375, prec 0.0955059, recall 0.786489
2017-12-10T03:02:43.109521: step 5968, loss 0.0603108, acc 0.984375, prec 0.0955174, recall 0.786514
2017-12-10T03:02:43.373071: step 5969, loss 0.0938417, acc 0.96875, prec 0.0955147, recall 0.786514
2017-12-10T03:02:43.646480: step 5970, loss 0.456488, acc 0.9375, prec 0.0955349, recall 0.786564
2017-12-10T03:02:43.917906: step 5971, loss 0.0438485, acc 0.96875, prec 0.0955322, recall 0.786564
2017-12-10T03:02:44.179566: step 5972, loss 0.10983, acc 0.96875, prec 0.0955423, recall 0.786589
2017-12-10T03:02:44.446323: step 5973, loss 0.136095, acc 0.9375, prec 0.0955497, recall 0.786614
2017-12-10T03:02:44.707417: step 5974, loss 0.256634, acc 0.921875, prec 0.0955429, recall 0.786614
2017-12-10T03:02:44.977872: step 5975, loss 0.361277, acc 0.90625, prec 0.0955604, recall 0.786664
2017-12-10T03:02:45.239431: step 5976, loss 0.161352, acc 0.96875, prec 0.0955577, recall 0.786664
2017-12-10T03:02:45.504600: step 5977, loss 0.0270655, acc 0.984375, prec 0.0955692, recall 0.786688
2017-12-10T03:02:45.774432: step 5978, loss 0.00788785, acc 1, prec 0.0955692, recall 0.786688
2017-12-10T03:02:46.036329: step 5979, loss 0.0185794, acc 0.984375, prec 0.0955806, recall 0.786713
2017-12-10T03:02:46.303612: step 5980, loss 0.135218, acc 0.984375, prec 0.0956049, recall 0.786763
2017-12-10T03:02:46.570625: step 5981, loss 0.196942, acc 0.953125, prec 0.0956264, recall 0.786813
2017-12-10T03:02:46.836127: step 5982, loss 0.155425, acc 0.984375, prec 0.0956251, recall 0.786813
2017-12-10T03:02:47.104709: step 5983, loss 0.129442, acc 0.984375, prec 0.0956365, recall 0.786838
2017-12-10T03:02:47.369282: step 5984, loss 0.0455475, acc 0.984375, prec 0.095648, recall 0.786862
2017-12-10T03:02:47.634992: step 5985, loss 0.00247754, acc 1, prec 0.095648, recall 0.786862
2017-12-10T03:02:47.899741: step 5986, loss 0.270237, acc 0.9375, prec 0.0956426, recall 0.786862
2017-12-10T03:02:48.166557: step 5987, loss 0.459243, acc 0.953125, prec 0.0956385, recall 0.786862
2017-12-10T03:02:48.429136: step 5988, loss 0.0768509, acc 0.984375, prec 0.09565, recall 0.786887
2017-12-10T03:02:48.694422: step 5989, loss 0.0711235, acc 0.96875, prec 0.0956473, recall 0.786887
2017-12-10T03:02:48.960300: step 5990, loss 0.0805731, acc 1, prec 0.0956729, recall 0.786937
2017-12-10T03:02:49.228299: step 5991, loss 0.0188967, acc 1, prec 0.0956857, recall 0.786962
2017-12-10T03:02:49.494828: step 5992, loss 0.262613, acc 0.953125, prec 0.0956944, recall 0.786986
2017-12-10T03:02:49.758457: step 5993, loss 0.0027796, acc 1, prec 0.0956944, recall 0.786986
2017-12-10T03:02:50.022001: step 5994, loss 0.951488, acc 0.921875, prec 0.0956876, recall 0.786986
2017-12-10T03:02:50.285661: step 5995, loss 0.0238247, acc 0.984375, prec 0.0956991, recall 0.787011
2017-12-10T03:02:50.549946: step 5996, loss 0.0450432, acc 0.96875, prec 0.0956964, recall 0.787011
2017-12-10T03:02:50.817406: step 5997, loss 0.0105147, acc 1, prec 0.0956964, recall 0.787011
2017-12-10T03:02:51.079474: step 5998, loss 0.105153, acc 0.984375, prec 0.0957078, recall 0.787036
2017-12-10T03:02:51.352209: step 5999, loss 0.0208605, acc 0.984375, prec 0.0957064, recall 0.787036
2017-12-10T03:02:51.625479: step 6000, loss 0.0368382, acc 0.984375, prec 0.0957051, recall 0.787036

Evaluation:
2017-12-10T03:02:59.250021: step 6000, loss 9.27644, acc 0.970186, prec 0.0960245, recall 0.777549

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-6000

2017-12-10T03:03:00.508119: step 6001, loss 0.0828183, acc 0.96875, prec 0.0960345, recall 0.777575
2017-12-10T03:03:00.771454: step 6002, loss 0.0140454, acc 1, prec 0.0960473, recall 0.7776
2017-12-10T03:03:01.045982: step 6003, loss 0.00481131, acc 1, prec 0.09606, recall 0.777626
2017-12-10T03:03:01.313059: step 6004, loss 0.00276341, acc 1, prec 0.0960728, recall 0.777651
2017-12-10T03:03:01.573819: step 6005, loss 0.000527542, acc 1, prec 0.0960728, recall 0.777651
2017-12-10T03:03:01.839484: step 6006, loss 0.0188969, acc 0.984375, prec 0.0960842, recall 0.777676
2017-12-10T03:03:02.113770: step 6007, loss 0.00642228, acc 1, prec 0.0960969, recall 0.777702
2017-12-10T03:03:02.379605: step 6008, loss 8.51297e-05, acc 1, prec 0.0960969, recall 0.777702
2017-12-10T03:03:02.646394: step 6009, loss 0.15806, acc 0.96875, prec 0.0960942, recall 0.777702
2017-12-10T03:03:02.914482: step 6010, loss 0.0744596, acc 0.984375, prec 0.0961183, recall 0.777752
2017-12-10T03:03:03.182100: step 6011, loss 0.179986, acc 1, prec 0.0961566, recall 0.777828
2017-12-10T03:03:03.447054: step 6012, loss 0.00254599, acc 1, prec 0.0961693, recall 0.777854
2017-12-10T03:03:03.711055: step 6013, loss 0.000127673, acc 1, prec 0.0961693, recall 0.777854
2017-12-10T03:03:03.972344: step 6014, loss 0.0202015, acc 0.984375, prec 0.0961679, recall 0.777854
2017-12-10T03:03:04.246610: step 6015, loss 0.0738045, acc 0.984375, prec 0.0961793, recall 0.777879
2017-12-10T03:03:04.512279: step 6016, loss 0.0268959, acc 1, prec 0.0961921, recall 0.777904
2017-12-10T03:03:04.776857: step 6017, loss 0.0134592, acc 1, prec 0.0962048, recall 0.77793
2017-12-10T03:03:05.048421: step 6018, loss 1.55827, acc 0.984375, prec 0.0962048, recall 0.777841
2017-12-10T03:03:05.322756: step 6019, loss 0.0762044, acc 0.984375, prec 0.0962162, recall 0.777866
2017-12-10T03:03:05.586243: step 6020, loss 0.0515604, acc 0.96875, prec 0.0962135, recall 0.777866
2017-12-10T03:03:05.849738: step 6021, loss 0.0815613, acc 0.984375, prec 0.0962121, recall 0.777866
2017-12-10T03:03:06.123381: step 6022, loss 0.0352182, acc 0.984375, prec 0.0962235, recall 0.777892
2017-12-10T03:03:06.386996: step 6023, loss 0.115717, acc 0.953125, prec 0.0962322, recall 0.777917
2017-12-10T03:03:06.653820: step 6024, loss 0.252618, acc 0.9375, prec 0.0962268, recall 0.777917
2017-12-10T03:03:06.916624: step 6025, loss 0.371774, acc 0.890625, prec 0.0962173, recall 0.777917
2017-12-10T03:03:07.191389: step 6026, loss 0.198625, acc 0.953125, prec 0.0962132, recall 0.777917
2017-12-10T03:03:07.462453: step 6027, loss 0.0758139, acc 0.984375, prec 0.0962118, recall 0.777917
2017-12-10T03:03:07.729466: step 6028, loss 0.196355, acc 0.953125, prec 0.0962205, recall 0.777942
2017-12-10T03:03:07.999408: step 6029, loss 0.701325, acc 0.90625, prec 0.0962124, recall 0.777942
2017-12-10T03:03:08.267187: step 6030, loss 0.343888, acc 0.875, prec 0.096227, recall 0.777993
2017-12-10T03:03:08.527126: step 6031, loss 0.635271, acc 0.859375, prec 0.0962275, recall 0.778018
2017-12-10T03:03:08.793280: step 6032, loss 0.449689, acc 0.9375, prec 0.0962348, recall 0.778044
2017-12-10T03:03:09.065484: step 6033, loss 0.129715, acc 0.953125, prec 0.0962562, recall 0.778094
2017-12-10T03:03:09.332899: step 6034, loss 0.537741, acc 0.90625, prec 0.0962481, recall 0.778094
2017-12-10T03:03:09.599396: step 6035, loss 0.460894, acc 0.921875, prec 0.0962541, recall 0.778119
2017-12-10T03:03:09.863913: step 6036, loss 0.532698, acc 0.859375, prec 0.0962419, recall 0.778119
2017-12-10T03:03:10.130471: step 6037, loss 0.26754, acc 0.9375, prec 0.0962619, recall 0.77817
2017-12-10T03:03:10.396979: step 6038, loss 0.151403, acc 0.9375, prec 0.0962819, recall 0.77822
2017-12-10T03:03:10.661745: step 6039, loss 0.0771304, acc 0.984375, prec 0.0962806, recall 0.77822
2017-12-10T03:03:10.929638: step 6040, loss 0.00393704, acc 1, prec 0.0962806, recall 0.77822
2017-12-10T03:03:11.189469: step 6041, loss 0.283834, acc 0.96875, prec 0.0962778, recall 0.77822
2017-12-10T03:03:11.459367: step 6042, loss 0.0248869, acc 0.984375, prec 0.0962765, recall 0.77822
2017-12-10T03:03:11.731568: step 6043, loss 0.086165, acc 0.96875, prec 0.0962865, recall 0.778246
2017-12-10T03:03:12.002891: step 6044, loss 0.084252, acc 0.96875, prec 0.0962838, recall 0.778246
2017-12-10T03:03:12.268553: step 6045, loss 0.00711083, acc 1, prec 0.0962838, recall 0.778246
2017-12-10T03:03:12.534657: step 6046, loss 0.0785864, acc 0.96875, prec 0.0962811, recall 0.778246
2017-12-10T03:03:12.799785: step 6047, loss 0.0107596, acc 1, prec 0.0962811, recall 0.778246
2017-12-10T03:03:13.064895: step 6048, loss 0.140652, acc 0.96875, prec 0.0963038, recall 0.778296
2017-12-10T03:03:13.326916: step 6049, loss 0.148516, acc 0.96875, prec 0.0963011, recall 0.778296
2017-12-10T03:03:13.598694: step 6050, loss 0.0470735, acc 1, prec 0.0963265, recall 0.778346
2017-12-10T03:03:13.867361: step 6051, loss 5.79463, acc 0.984375, prec 0.0963265, recall 0.778258
2017-12-10T03:03:14.148110: step 6052, loss 0.00489421, acc 1, prec 0.096352, recall 0.778308
2017-12-10T03:03:14.414023: step 6053, loss 0.0780045, acc 0.984375, prec 0.0963506, recall 0.778308
2017-12-10T03:03:14.678668: step 6054, loss 0.00283417, acc 1, prec 0.096376, recall 0.778359
2017-12-10T03:03:14.948668: step 6055, loss 0.243319, acc 0.96875, prec 0.0963861, recall 0.778384
2017-12-10T03:03:15.216741: step 6056, loss 0.112528, acc 0.9375, prec 0.0963806, recall 0.778384
2017-12-10T03:03:15.490320: step 6057, loss 0.0563783, acc 0.96875, prec 0.0963779, recall 0.778384
2017-12-10T03:03:15.756545: step 6058, loss 0.0762776, acc 0.953125, prec 0.0963866, recall 0.778409
2017-12-10T03:03:16.024401: step 6059, loss 0.168638, acc 0.953125, prec 0.0963825, recall 0.778409
2017-12-10T03:03:16.292928: step 6060, loss 0.433194, acc 0.9375, prec 0.0964152, recall 0.778485
2017-12-10T03:03:16.570458: step 6061, loss 0.887974, acc 0.875, prec 0.0964171, recall 0.77851
2017-12-10T03:03:16.831613: step 6062, loss 0.747339, acc 0.859375, prec 0.0964049, recall 0.77851
2017-12-10T03:03:17.101281: step 6063, loss 0.336645, acc 0.9375, prec 0.0964121, recall 0.778535
2017-12-10T03:03:17.368693: step 6064, loss 0.342133, acc 0.9375, prec 0.0964194, recall 0.77856
2017-12-10T03:03:17.632515: step 6065, loss 0.36448, acc 0.96875, prec 0.0964421, recall 0.77861
2017-12-10T03:03:17.897221: step 6066, loss 0.0532537, acc 0.96875, prec 0.0964394, recall 0.77861
2017-12-10T03:03:18.164113: step 6067, loss 0.115529, acc 0.96875, prec 0.0964367, recall 0.77861
2017-12-10T03:03:18.434650: step 6068, loss 0.171301, acc 0.96875, prec 0.096434, recall 0.77861
2017-12-10T03:03:18.699433: step 6069, loss 0.127365, acc 0.96875, prec 0.0964313, recall 0.77861
2017-12-10T03:03:18.970491: step 6070, loss 0.0997166, acc 0.953125, prec 0.0964272, recall 0.77861
2017-12-10T03:03:19.242166: step 6071, loss 0.2376, acc 0.890625, prec 0.0964304, recall 0.778636
2017-12-10T03:03:19.510482: step 6072, loss 0.193815, acc 0.953125, prec 0.0964391, recall 0.778661
2017-12-10T03:03:19.774301: step 6073, loss 0.0249799, acc 0.984375, prec 0.0964758, recall 0.778736
2017-12-10T03:03:20.041243: step 6074, loss 0.00990386, acc 1, prec 0.0964758, recall 0.778736
2017-12-10T03:03:20.306242: step 6075, loss 0.116368, acc 0.9375, prec 0.0964704, recall 0.778736
2017-12-10T03:03:20.572002: step 6076, loss 0.00414783, acc 1, prec 0.0964704, recall 0.778736
2017-12-10T03:03:20.840791: step 6077, loss 0.00918542, acc 1, prec 0.0964704, recall 0.778736
2017-12-10T03:03:21.104225: step 6078, loss 0.135923, acc 0.984375, prec 0.0964944, recall 0.778786
2017-12-10T03:03:21.379029: step 6079, loss 0.367092, acc 0.953125, prec 0.0965285, recall 0.778861
2017-12-10T03:03:21.645919: step 6080, loss 0.134275, acc 0.96875, prec 0.0965258, recall 0.778861
2017-12-10T03:03:21.908920: step 6081, loss 0.0126017, acc 1, prec 0.0965384, recall 0.778886
2017-12-10T03:03:22.184115: step 6082, loss 0.000451967, acc 1, prec 0.0965384, recall 0.778886
2017-12-10T03:03:22.444706: step 6083, loss 0.110115, acc 0.984375, prec 0.0965498, recall 0.778912
2017-12-10T03:03:22.708807: step 6084, loss 0.117893, acc 0.984375, prec 0.0965484, recall 0.778912
2017-12-10T03:03:22.979732: step 6085, loss 0.0035996, acc 1, prec 0.0965611, recall 0.778937
2017-12-10T03:03:23.243441: step 6086, loss 0.0925439, acc 0.984375, prec 0.0965979, recall 0.779012
2017-12-10T03:03:23.524002: step 6087, loss 0.0229757, acc 1, prec 0.0966232, recall 0.779062
2017-12-10T03:03:23.791332: step 6088, loss 0.437398, acc 0.953125, prec 0.0966319, recall 0.779087
2017-12-10T03:03:24.055330: step 6089, loss 0.0288868, acc 0.984375, prec 0.0966432, recall 0.779112
2017-12-10T03:03:24.320973: step 6090, loss 0.00196872, acc 1, prec 0.0966432, recall 0.779112
2017-12-10T03:03:24.586993: step 6091, loss 0.0290774, acc 0.984375, prec 0.0966545, recall 0.779137
2017-12-10T03:03:24.848658: step 6092, loss 6.00034e-05, acc 1, prec 0.0966799, recall 0.779187
2017-12-10T03:03:25.107950: step 6093, loss 0.00651046, acc 1, prec 0.0966926, recall 0.779212
2017-12-10T03:03:25.381589: step 6094, loss 0.00270653, acc 1, prec 0.0966926, recall 0.779212
2017-12-10T03:03:25.642697: step 6095, loss 0.0355864, acc 0.984375, prec 0.0966913, recall 0.779212
2017-12-10T03:03:25.907878: step 6096, loss 0.00175917, acc 1, prec 0.0967166, recall 0.779262
2017-12-10T03:03:26.166195: step 6097, loss 0.0599175, acc 0.96875, prec 0.0967139, recall 0.779262
2017-12-10T03:03:26.438525: step 6098, loss 0.11653, acc 0.984375, prec 0.0967379, recall 0.779312
2017-12-10T03:03:26.707886: step 6099, loss 10.6728, acc 0.984375, prec 0.0967506, recall 0.779249
2017-12-10T03:03:26.979100: step 6100, loss 0.311133, acc 0.96875, prec 0.0967606, recall 0.779274
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-6100

2017-12-10T03:03:28.262804: step 6101, loss 0.102418, acc 0.984375, prec 0.0967846, recall 0.779324
2017-12-10T03:03:28.529379: step 6102, loss 0.00282296, acc 1, prec 0.0967846, recall 0.779324
2017-12-10T03:03:28.800738: step 6103, loss 0.138777, acc 0.96875, prec 0.0967819, recall 0.779324
2017-12-10T03:03:29.065619: step 6104, loss 0.689642, acc 0.953125, prec 0.0968032, recall 0.779374
2017-12-10T03:03:29.329331: step 6105, loss 0.0209323, acc 1, prec 0.0968032, recall 0.779374
2017-12-10T03:03:29.601345: step 6106, loss 0.843373, acc 0.890625, prec 0.096819, recall 0.779423
2017-12-10T03:03:29.867556: step 6107, loss 0.334442, acc 0.9375, prec 0.0968263, recall 0.779448
2017-12-10T03:03:30.141961: step 6108, loss 0.299586, acc 0.921875, prec 0.0968195, recall 0.779448
2017-12-10T03:03:30.410825: step 6109, loss 0.245765, acc 0.953125, prec 0.0968535, recall 0.779523
2017-12-10T03:03:30.678299: step 6110, loss 0.356185, acc 0.921875, prec 0.0968593, recall 0.779548
2017-12-10T03:03:30.940076: step 6111, loss 0.66026, acc 0.90625, prec 0.0968639, recall 0.779573
2017-12-10T03:03:31.200736: step 6112, loss 0.103969, acc 0.96875, prec 0.0968738, recall 0.779598
2017-12-10T03:03:31.470241: step 6113, loss 0.0638263, acc 0.953125, prec 0.0968951, recall 0.779648
2017-12-10T03:03:31.731980: step 6114, loss 0.840537, acc 0.875, prec 0.0969096, recall 0.779697
2017-12-10T03:03:32.008644: step 6115, loss 0.832165, acc 0.890625, prec 0.0969, recall 0.779697
2017-12-10T03:03:32.278227: step 6116, loss 0.374179, acc 0.9375, prec 0.0969073, recall 0.779722
2017-12-10T03:03:32.551904: step 6117, loss 0.138227, acc 0.953125, prec 0.0969159, recall 0.779747
2017-12-10T03:03:32.826259: step 6118, loss 0.182373, acc 0.96875, prec 0.0969258, recall 0.779772
2017-12-10T03:03:33.090278: step 6119, loss 0.10654, acc 0.984375, prec 0.0969245, recall 0.779772
2017-12-10T03:03:33.361243: step 6120, loss 0.165235, acc 0.96875, prec 0.0969217, recall 0.779772
2017-12-10T03:03:33.628800: step 6121, loss 0.00226828, acc 1, prec 0.0969217, recall 0.779772
2017-12-10T03:03:33.894799: step 6122, loss 0.249477, acc 0.9375, prec 0.0969163, recall 0.779772
2017-12-10T03:03:34.168567: step 6123, loss 0.162679, acc 0.984375, prec 0.0969149, recall 0.779772
2017-12-10T03:03:34.437355: step 6124, loss 0.0858671, acc 0.984375, prec 0.0969516, recall 0.779847
2017-12-10T03:03:34.703992: step 6125, loss 0.0843388, acc 0.96875, prec 0.0969489, recall 0.779847
2017-12-10T03:03:34.973932: step 6126, loss 0.569748, acc 0.9375, prec 0.0969561, recall 0.779871
2017-12-10T03:03:35.238370: step 6127, loss 0.0984837, acc 0.96875, prec 0.0969787, recall 0.779921
2017-12-10T03:03:35.506591: step 6128, loss 0.0138169, acc 1, prec 0.0970167, recall 0.779996
2017-12-10T03:03:35.770198: step 6129, loss 0.0408149, acc 0.984375, prec 0.0970153, recall 0.779996
2017-12-10T03:03:36.029279: step 6130, loss 0.0111313, acc 1, prec 0.0970153, recall 0.779996
2017-12-10T03:03:36.298985: step 6131, loss 0.0431321, acc 0.984375, prec 0.0970266, recall 0.78002
2017-12-10T03:03:36.565285: step 6132, loss 0.862363, acc 1, prec 0.0970393, recall 0.780045
2017-12-10T03:03:36.838064: step 6133, loss 0.409466, acc 0.984375, prec 0.0970506, recall 0.78007
2017-12-10T03:03:37.110938: step 6134, loss 0.182917, acc 0.984375, prec 0.0970619, recall 0.780095
2017-12-10T03:03:37.381385: step 6135, loss 0.0455947, acc 0.96875, prec 0.0970592, recall 0.780095
2017-12-10T03:03:37.648711: step 6136, loss 0.00094766, acc 1, prec 0.0970592, recall 0.780095
2017-12-10T03:03:37.911006: step 6137, loss 0.177775, acc 0.953125, prec 0.0970551, recall 0.780095
2017-12-10T03:03:38.174171: step 6138, loss 0.177891, acc 0.96875, prec 0.0970651, recall 0.780119
2017-12-10T03:03:38.443014: step 6139, loss 0.185401, acc 0.9375, prec 0.0970723, recall 0.780144
2017-12-10T03:03:38.713420: step 6140, loss 0.127452, acc 0.96875, prec 0.0970695, recall 0.780144
2017-12-10T03:03:38.985114: step 6141, loss 0.0518921, acc 0.96875, prec 0.0970921, recall 0.780194
2017-12-10T03:03:39.260252: step 6142, loss 0.0666014, acc 0.96875, prec 0.0970894, recall 0.780194
2017-12-10T03:03:39.521478: step 6143, loss 0.138266, acc 0.984375, prec 0.0970881, recall 0.780194
2017-12-10T03:03:39.791359: step 6144, loss 0.571608, acc 0.921875, prec 0.0971066, recall 0.780243
2017-12-10T03:03:40.059201: step 6145, loss 0.0168334, acc 1, prec 0.0971319, recall 0.780293
2017-12-10T03:03:40.327892: step 6146, loss 0.0178668, acc 1, prec 0.0971319, recall 0.780293
2017-12-10T03:03:40.594314: step 6147, loss 0.2124, acc 0.921875, prec 0.0971504, recall 0.780342
2017-12-10T03:03:40.857181: step 6148, loss 0.0368357, acc 0.984375, prec 0.097149, recall 0.780342
2017-12-10T03:03:41.126417: step 6149, loss 0.17577, acc 0.96875, prec 0.097159, recall 0.780367
2017-12-10T03:03:41.397672: step 6150, loss 0.0375139, acc 0.96875, prec 0.0971689, recall 0.780392
2017-12-10T03:03:41.674527: step 6151, loss 0.0189935, acc 0.984375, prec 0.0971928, recall 0.780441
2017-12-10T03:03:41.937365: step 6152, loss 0.277357, acc 0.9375, prec 0.0972, recall 0.780466
2017-12-10T03:03:42.203252: step 6153, loss 0.124719, acc 0.96875, prec 0.0971973, recall 0.780466
2017-12-10T03:03:42.465653: step 6154, loss 0.474815, acc 0.9375, prec 0.0972045, recall 0.780491
2017-12-10T03:03:42.730287: step 6155, loss 0.0352737, acc 0.984375, prec 0.0972032, recall 0.780491
2017-12-10T03:03:42.998915: step 6156, loss 0.0101392, acc 1, prec 0.0972032, recall 0.780491
2017-12-10T03:03:43.266251: step 6157, loss 0.375631, acc 0.96875, prec 0.0972131, recall 0.780515
2017-12-10T03:03:43.536007: step 6158, loss 0.26289, acc 0.953125, prec 0.0972216, recall 0.78054
2017-12-10T03:03:43.805230: step 6159, loss 0.131274, acc 0.984375, prec 0.0972329, recall 0.780565
2017-12-10T03:03:44.069555: step 6160, loss 0.0187327, acc 0.984375, prec 0.0972569, recall 0.780614
2017-12-10T03:03:44.334507: step 6161, loss 0.119981, acc 0.96875, prec 0.0973047, recall 0.780713
2017-12-10T03:03:44.597072: step 6162, loss 0.123953, acc 0.96875, prec 0.0973146, recall 0.780737
2017-12-10T03:03:44.863616: step 6163, loss 0.158177, acc 0.953125, prec 0.0973106, recall 0.780737
2017-12-10T03:03:45.132809: step 6164, loss 0.115893, acc 0.984375, prec 0.0973092, recall 0.780737
2017-12-10T03:03:45.404661: step 6165, loss 0.222194, acc 0.984375, prec 0.0973205, recall 0.780762
2017-12-10T03:03:45.677187: step 6166, loss 0.00417783, acc 1, prec 0.0973205, recall 0.780762
2017-12-10T03:03:45.943097: step 6167, loss 0.0630109, acc 0.96875, prec 0.097343, recall 0.780811
2017-12-10T03:03:46.208024: step 6168, loss 0.234734, acc 0.96875, prec 0.0973403, recall 0.780811
2017-12-10T03:03:46.474261: step 6169, loss 0.0108078, acc 1, prec 0.0973403, recall 0.780811
2017-12-10T03:03:46.741347: step 6170, loss 0.167405, acc 0.96875, prec 0.0973629, recall 0.78086
2017-12-10T03:03:47.005022: step 6171, loss 4.27245, acc 0.96875, prec 0.0973741, recall 0.780797
2017-12-10T03:03:47.275501: step 6172, loss 0.0519335, acc 0.984375, prec 0.0973728, recall 0.780797
2017-12-10T03:03:47.540309: step 6173, loss 0.108211, acc 0.953125, prec 0.0973687, recall 0.780797
2017-12-10T03:03:47.806506: step 6174, loss 0.171998, acc 0.953125, prec 0.0973772, recall 0.780822
2017-12-10T03:03:48.075593: step 6175, loss 0.0349376, acc 0.984375, prec 0.0973885, recall 0.780847
2017-12-10T03:03:48.348682: step 6176, loss 0.20011, acc 0.96875, prec 0.0973858, recall 0.780847
2017-12-10T03:03:48.618365: step 6177, loss 0.396303, acc 0.9375, prec 0.097393, recall 0.780871
2017-12-10T03:03:48.880009: step 6178, loss 0.3124, acc 0.9375, prec 0.0974001, recall 0.780896
2017-12-10T03:03:49.142227: step 6179, loss 0.196596, acc 0.953125, prec 0.0973961, recall 0.780896
2017-12-10T03:03:49.414961: step 6180, loss 0.363867, acc 0.890625, prec 0.0973865, recall 0.780896
2017-12-10T03:03:49.693916: step 6181, loss 0.253222, acc 0.9375, prec 0.0974063, recall 0.780945
2017-12-10T03:03:49.958792: step 6182, loss 0.377955, acc 0.890625, prec 0.097422, recall 0.780994
2017-12-10T03:03:50.222365: step 6183, loss 0.378511, acc 0.9375, prec 0.0974292, recall 0.781019
2017-12-10T03:03:50.488480: step 6184, loss 0.48353, acc 0.953125, prec 0.0974378, recall 0.781043
2017-12-10T03:03:50.750409: step 6185, loss 0.674874, acc 0.921875, prec 0.0974436, recall 0.781068
2017-12-10T03:03:51.019500: step 6186, loss 0.25406, acc 0.953125, prec 0.0974647, recall 0.781117
2017-12-10T03:03:51.287052: step 6187, loss 0.17788, acc 0.9375, prec 0.0974719, recall 0.781141
2017-12-10T03:03:51.558805: step 6188, loss 0.223943, acc 0.953125, prec 0.0974678, recall 0.781141
2017-12-10T03:03:51.821811: step 6189, loss 0.0745292, acc 0.96875, prec 0.0974777, recall 0.781166
2017-12-10T03:03:52.087687: step 6190, loss 0.00715542, acc 1, prec 0.0974777, recall 0.781166
2017-12-10T03:03:52.354454: step 6191, loss 0.345098, acc 0.921875, prec 0.0974709, recall 0.781166
2017-12-10T03:03:52.624383: step 6192, loss 0.256104, acc 0.921875, prec 0.0974893, recall 0.781215
2017-12-10T03:03:52.895310: step 6193, loss 0.0739479, acc 0.953125, prec 0.0975231, recall 0.781289
2017-12-10T03:03:53.157127: step 6194, loss 0.410653, acc 0.921875, prec 0.0975289, recall 0.781313
2017-12-10T03:03:53.428954: step 6195, loss 0.0730488, acc 0.96875, prec 0.0975388, recall 0.781337
2017-12-10T03:03:53.689314: step 6196, loss 0.0240854, acc 1, prec 0.0975388, recall 0.781337
2017-12-10T03:03:53.957684: step 6197, loss 0.400685, acc 0.953125, prec 0.0975347, recall 0.781337
2017-12-10T03:03:54.221712: step 6198, loss 0.0056194, acc 1, prec 0.0975347, recall 0.781337
2017-12-10T03:03:54.484056: step 6199, loss 0.529148, acc 0.9375, prec 0.0975419, recall 0.781362
2017-12-10T03:03:54.745441: step 6200, loss 0.173241, acc 0.96875, prec 0.0975391, recall 0.781362
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-6200

2017-12-10T03:03:56.108833: step 6201, loss 0.0102478, acc 1, prec 0.0975391, recall 0.781362
2017-12-10T03:03:56.373046: step 6202, loss 0.106708, acc 0.984375, prec 0.0975378, recall 0.781362
2017-12-10T03:03:56.651499: step 6203, loss 0.0245977, acc 0.984375, prec 0.0975364, recall 0.781362
2017-12-10T03:03:56.916569: step 6204, loss 0.308119, acc 0.96875, prec 0.0975337, recall 0.781362
2017-12-10T03:03:57.187847: step 6205, loss 0.105963, acc 0.984375, prec 0.0975449, recall 0.781386
2017-12-10T03:03:57.452942: step 6206, loss 0.0280722, acc 0.984375, prec 0.0975436, recall 0.781386
2017-12-10T03:03:57.722826: step 6207, loss 0.140388, acc 0.984375, prec 0.0975422, recall 0.781386
2017-12-10T03:03:57.984916: step 6208, loss 0.0398086, acc 0.984375, prec 0.0975535, recall 0.781411
2017-12-10T03:03:58.253517: step 6209, loss 0.00448842, acc 1, prec 0.0975535, recall 0.781411
2017-12-10T03:03:58.518417: step 6210, loss 0.284761, acc 0.984375, prec 0.0975521, recall 0.781411
2017-12-10T03:03:58.781021: step 6211, loss 0.101037, acc 0.984375, prec 0.0975507, recall 0.781411
2017-12-10T03:03:59.050643: step 6212, loss 2.47905, acc 0.9375, prec 0.0975593, recall 0.781348
2017-12-10T03:03:59.322026: step 6213, loss 0.19812, acc 0.984375, prec 0.0975705, recall 0.781372
2017-12-10T03:03:59.591060: step 6214, loss 0.151811, acc 1, prec 0.0976084, recall 0.781446
2017-12-10T03:03:59.855113: step 6215, loss 0.354271, acc 0.953125, prec 0.0976547, recall 0.781544
2017-12-10T03:04:00.127064: step 6216, loss 0.0955538, acc 0.984375, prec 0.097666, recall 0.781568
2017-12-10T03:04:00.402437: step 6217, loss 0.00289508, acc 1, prec 0.097666, recall 0.781568
2017-12-10T03:04:00.666093: step 6218, loss 1.29381, acc 0.9375, prec 0.0976857, recall 0.781617
2017-12-10T03:04:00.933303: step 6219, loss 0.0626057, acc 0.984375, prec 0.097697, recall 0.781641
2017-12-10T03:04:01.199983: step 6220, loss 0.366186, acc 0.96875, prec 0.0977069, recall 0.781666
2017-12-10T03:04:01.462838: step 6221, loss 0.285509, acc 0.96875, prec 0.0977167, recall 0.78169
2017-12-10T03:04:01.732992: step 6222, loss 0.309473, acc 0.875, prec 0.097731, recall 0.781739
2017-12-10T03:04:02.008536: step 6223, loss 0.27999, acc 0.953125, prec 0.0977269, recall 0.781739
2017-12-10T03:04:02.278061: step 6224, loss 0.439624, acc 0.90625, prec 0.0977187, recall 0.781739
2017-12-10T03:04:02.543685: step 6225, loss 0.616243, acc 0.90625, prec 0.0977357, recall 0.781788
2017-12-10T03:04:02.807378: step 6226, loss 0.555434, acc 0.859375, prec 0.0977361, recall 0.781812
2017-12-10T03:04:03.082022: step 6227, loss 0.258948, acc 0.953125, prec 0.097732, recall 0.781812
2017-12-10T03:04:03.351475: step 6228, loss 0.625125, acc 0.890625, prec 0.0977602, recall 0.781885
2017-12-10T03:04:03.617488: step 6229, loss 0.89607, acc 0.8125, prec 0.0977564, recall 0.78191
2017-12-10T03:04:03.884688: step 6230, loss 0.364368, acc 0.90625, prec 0.0977734, recall 0.781958
2017-12-10T03:04:04.146629: step 6231, loss 0.23899, acc 0.90625, prec 0.0977778, recall 0.781983
2017-12-10T03:04:05.128494: step 6232, loss 0.580109, acc 0.875, prec 0.0977669, recall 0.781983
2017-12-10T03:04:05.504478: step 6233, loss 0.259565, acc 0.9375, prec 0.0977992, recall 0.782056
2017-12-10T03:04:05.778789: step 6234, loss 0.0725648, acc 0.96875, prec 0.0978217, recall 0.782104
2017-12-10T03:04:06.159701: step 6235, loss 0.0822543, acc 0.96875, prec 0.0978315, recall 0.782129
2017-12-10T03:04:06.921026: step 6236, loss 0.0234756, acc 0.984375, prec 0.0978428, recall 0.782153
2017-12-10T03:04:07.655516: step 6237, loss 0.0909788, acc 0.953125, prec 0.0978387, recall 0.782153
2017-12-10T03:04:08.366765: step 6238, loss 0.470303, acc 0.9375, prec 0.0978332, recall 0.782153
2017-12-10T03:04:09.097836: step 6239, loss 0.335426, acc 0.90625, prec 0.097825, recall 0.782153
2017-12-10T03:04:09.856813: step 6240, loss 0.0212854, acc 1, prec 0.097825, recall 0.782153
2017-12-10T03:04:10.570274: step 6241, loss 0.0203962, acc 0.984375, prec 0.0978488, recall 0.782201
2017-12-10T03:04:11.304995: step 6242, loss 0.474496, acc 0.953125, prec 0.0978447, recall 0.782201
2017-12-10T03:04:12.030016: step 6243, loss 0.143454, acc 0.984375, prec 0.097856, recall 0.782226
2017-12-10T03:04:12.783591: step 6244, loss 1.20665, acc 0.984375, prec 0.0978811, recall 0.782187
2017-12-10T03:04:13.514165: step 6245, loss 0.0247888, acc 0.984375, prec 0.0978798, recall 0.782187
2017-12-10T03:04:14.235896: step 6246, loss 0.203342, acc 0.96875, prec 0.0978896, recall 0.782211
2017-12-10T03:04:14.981806: step 6247, loss 0.165167, acc 0.96875, prec 0.0978995, recall 0.782236
2017-12-10T03:04:15.714370: step 6248, loss 0.0836838, acc 0.984375, prec 0.0979107, recall 0.78226
2017-12-10T03:04:16.433322: step 6249, loss 0.102858, acc 0.984375, prec 0.0979219, recall 0.782284
2017-12-10T03:04:17.183575: step 6250, loss 0.0418906, acc 0.984375, prec 0.0979205, recall 0.782284
2017-12-10T03:04:17.942518: step 6251, loss 0.00175173, acc 1, prec 0.0979331, recall 0.782308
2017-12-10T03:04:18.747897: step 6252, loss 0.0140899, acc 1, prec 0.0979457, recall 0.782333
2017-12-10T03:04:19.473506: step 6253, loss 0.0329439, acc 1, prec 0.0979834, recall 0.782405
2017-12-10T03:04:19.760497: step 6254, loss 0.0315103, acc 0.984375, prec 0.0979946, recall 0.78243
2017-12-10T03:04:20.039478: step 6255, loss 0.0470297, acc 0.984375, prec 0.0980059, recall 0.782454
2017-12-10T03:04:20.317627: step 6256, loss 0.182628, acc 0.96875, prec 0.0980031, recall 0.782454
2017-12-10T03:04:20.603037: step 6257, loss 0.16953, acc 0.96875, prec 0.0980004, recall 0.782454
2017-12-10T03:04:20.881315: step 6258, loss 0.202374, acc 0.96875, prec 0.0980102, recall 0.782478
2017-12-10T03:04:21.168799: step 6259, loss 0.15795, acc 0.953125, prec 0.0980061, recall 0.782478
2017-12-10T03:04:21.450791: step 6260, loss 0.182082, acc 0.96875, prec 0.0980034, recall 0.782478
2017-12-10T03:04:21.718825: step 6261, loss 0.144922, acc 0.953125, prec 0.0980119, recall 0.782502
2017-12-10T03:04:21.983565: step 6262, loss 0.0915244, acc 0.96875, prec 0.0980091, recall 0.782502
2017-12-10T03:04:22.250087: step 6263, loss 0.165288, acc 0.953125, prec 0.0980176, recall 0.782526
2017-12-10T03:04:22.522413: step 6264, loss 0.147911, acc 0.984375, prec 0.0980163, recall 0.782526
2017-12-10T03:04:23.484955: step 6265, loss 0.0471172, acc 0.984375, prec 0.09804, recall 0.782575
2017-12-10T03:04:23.844747: step 6266, loss 0.894294, acc 1, prec 0.0980526, recall 0.782599
2017-12-10T03:04:24.138810: step 6267, loss 0.0411295, acc 0.984375, prec 0.0980512, recall 0.782599
2017-12-10T03:04:24.431112: step 6268, loss 0.120317, acc 0.96875, prec 0.0980611, recall 0.782623
2017-12-10T03:04:24.700716: step 6269, loss 0.225101, acc 0.984375, prec 0.0980597, recall 0.782623
2017-12-10T03:04:24.971848: step 6270, loss 0.144946, acc 0.96875, prec 0.0980821, recall 0.782672
2017-12-10T03:04:25.240354: step 6271, loss 0.36333, acc 0.953125, prec 0.0981032, recall 0.78272
2017-12-10T03:04:25.509662: step 6272, loss 0.193403, acc 0.953125, prec 0.0980991, recall 0.78272
2017-12-10T03:04:25.779883: step 6273, loss 0.0382349, acc 0.984375, prec 0.0981228, recall 0.782768
2017-12-10T03:04:26.045357: step 6274, loss 0.102268, acc 0.953125, prec 0.0981187, recall 0.782768
2017-12-10T03:04:26.320257: step 6275, loss 0.0340412, acc 0.984375, prec 0.0981299, recall 0.782792
2017-12-10T03:04:26.602144: step 6276, loss 0.0337294, acc 0.984375, prec 0.0981286, recall 0.782792
2017-12-10T03:04:26.868364: step 6277, loss 0.0434814, acc 0.96875, prec 0.0981384, recall 0.782816
2017-12-10T03:04:27.135212: step 6278, loss 0.501011, acc 0.953125, prec 0.0981343, recall 0.782816
2017-12-10T03:04:27.402638: step 6279, loss 0.186284, acc 0.96875, prec 0.0981441, recall 0.782841
2017-12-10T03:04:27.667766: step 6280, loss 0.20708, acc 0.96875, prec 0.098154, recall 0.782865
2017-12-10T03:04:27.944639: step 6281, loss 0.0409241, acc 0.984375, prec 0.0981777, recall 0.782913
2017-12-10T03:04:28.216099: step 6282, loss 0.0265938, acc 0.984375, prec 0.0981763, recall 0.782913
2017-12-10T03:04:28.484623: step 6283, loss 0.133839, acc 0.984375, prec 0.0981875, recall 0.782937
2017-12-10T03:04:28.755189: step 6284, loss 0.0649474, acc 0.984375, prec 0.0981862, recall 0.782937
2017-12-10T03:04:29.030320: step 6285, loss 0.0803033, acc 0.953125, prec 0.0981946, recall 0.782961
2017-12-10T03:04:29.301463: step 6286, loss 0.0293206, acc 0.96875, prec 0.0982045, recall 0.782985
2017-12-10T03:04:29.562639: step 6287, loss 0.0152821, acc 1, prec 0.0982045, recall 0.782985
2017-12-10T03:04:29.829285: step 6288, loss 0.0398415, acc 0.984375, prec 0.0982282, recall 0.783034
2017-12-10T03:04:30.099586: step 6289, loss 0.0473268, acc 0.984375, prec 0.098252, recall 0.783082
2017-12-10T03:04:30.370589: step 6290, loss 0.041803, acc 0.96875, prec 0.0982618, recall 0.783106
2017-12-10T03:04:30.643026: step 6291, loss 0.0603633, acc 0.96875, prec 0.0982716, recall 0.78313
2017-12-10T03:04:30.913259: step 6292, loss 0.0526532, acc 0.96875, prec 0.0982689, recall 0.78313
2017-12-10T03:04:31.182962: step 6293, loss 0.299345, acc 0.953125, prec 0.0982773, recall 0.783154
2017-12-10T03:04:31.453678: step 6294, loss 0.0243303, acc 0.984375, prec 0.098276, recall 0.783154
2017-12-10T03:04:31.717012: step 6295, loss 0.0128529, acc 0.984375, prec 0.0982746, recall 0.783154
2017-12-10T03:04:31.983377: step 6296, loss 0.146741, acc 0.984375, prec 0.0982858, recall 0.783178
2017-12-10T03:04:32.252476: step 6297, loss 0.213625, acc 0.984375, prec 0.0983095, recall 0.783226
2017-12-10T03:04:32.531325: step 6298, loss 1.2266, acc 0.953125, prec 0.0983193, recall 0.783163
2017-12-10T03:04:32.800951: step 6299, loss 0.311459, acc 0.953125, prec 0.0983152, recall 0.783163
2017-12-10T03:04:33.072722: step 6300, loss 5.22304, acc 0.9375, prec 0.0983362, recall 0.783125

Evaluation:
2017-12-10T03:04:40.658976: step 6300, loss 6.65654, acc 0.949995, prec 0.0985717, recall 0.776326

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-6300

2017-12-10T03:04:41.943871: step 6301, loss 0.00808489, acc 1, prec 0.0985717, recall 0.776326
2017-12-10T03:04:42.214373: step 6302, loss 0.178535, acc 0.9375, prec 0.0985662, recall 0.776326
2017-12-10T03:04:42.479984: step 6303, loss 0.16923, acc 0.953125, prec 0.0985871, recall 0.776375
2017-12-10T03:04:42.745014: step 6304, loss 0.140619, acc 0.953125, prec 0.0985954, recall 0.776399
2017-12-10T03:04:43.011274: step 6305, loss 0.425128, acc 0.875, prec 0.0986095, recall 0.776448
2017-12-10T03:04:43.273821: step 6306, loss 0.475172, acc 0.859375, prec 0.0986096, recall 0.776472
2017-12-10T03:04:43.538666: step 6307, loss 0.228934, acc 0.9375, prec 0.0986416, recall 0.776545
2017-12-10T03:04:43.800191: step 6308, loss 0.647164, acc 0.875, prec 0.0986307, recall 0.776545
2017-12-10T03:04:44.062078: step 6309, loss 0.915726, acc 0.734375, prec 0.0986573, recall 0.776642
2017-12-10T03:04:44.326896: step 6310, loss 1.21392, acc 0.828125, prec 0.0986548, recall 0.776666
2017-12-10T03:04:44.589783: step 6311, loss 0.819649, acc 0.75, prec 0.0986454, recall 0.776691
2017-12-10T03:04:44.852255: step 6312, loss 0.934397, acc 0.796875, prec 0.0986277, recall 0.776691
2017-12-10T03:04:45.123070: step 6313, loss 0.381669, acc 0.875, prec 0.0986168, recall 0.776691
2017-12-10T03:04:45.389030: step 6314, loss 0.541244, acc 0.859375, prec 0.0986046, recall 0.776691
2017-12-10T03:04:45.657148: step 6315, loss 0.97543, acc 0.90625, prec 0.0986089, recall 0.776715
2017-12-10T03:04:45.929420: step 6316, loss 0.349734, acc 0.96875, prec 0.0986434, recall 0.776788
2017-12-10T03:04:46.199900: step 6317, loss 0.241112, acc 0.9375, prec 0.0986504, recall 0.776812
2017-12-10T03:04:46.465029: step 6318, loss 0.18008, acc 0.953125, prec 0.0986712, recall 0.77686
2017-12-10T03:04:46.731856: step 6319, loss 0.0245443, acc 0.984375, prec 0.0986947, recall 0.776909
2017-12-10T03:04:46.998858: step 6320, loss 0.0795826, acc 0.96875, prec 0.0987044, recall 0.776933
2017-12-10T03:04:47.263834: step 6321, loss 0.0471371, acc 0.96875, prec 0.0987017, recall 0.776933
2017-12-10T03:04:47.526953: step 6322, loss 0.0514639, acc 0.984375, prec 0.0987004, recall 0.776933
2017-12-10T03:04:47.795828: step 6323, loss 0.056075, acc 0.96875, prec 0.0987101, recall 0.776957
2017-12-10T03:04:48.070132: step 6324, loss 0.834875, acc 0.96875, prec 0.0987198, recall 0.776982
2017-12-10T03:04:48.340597: step 6325, loss 0.56525, acc 0.984375, prec 0.0987309, recall 0.777006
2017-12-10T03:04:48.610565: step 6326, loss 1.69829, acc 0.921875, prec 0.0987378, recall 0.776946
2017-12-10T03:04:48.881642: step 6327, loss 0.132147, acc 0.96875, prec 0.0987476, recall 0.77697
2017-12-10T03:04:49.146085: step 6328, loss 0.107052, acc 0.953125, prec 0.0987683, recall 0.777018
2017-12-10T03:04:49.415857: step 6329, loss 0.214431, acc 0.9375, prec 0.0987877, recall 0.777067
2017-12-10T03:04:49.689275: step 6330, loss 0.282064, acc 0.953125, prec 0.0988085, recall 0.777115
2017-12-10T03:04:49.954800: step 6331, loss 0.485796, acc 0.890625, prec 0.0988114, recall 0.777139
2017-12-10T03:04:50.221515: step 6332, loss 0.218882, acc 0.90625, prec 0.0988281, recall 0.777187
2017-12-10T03:04:50.491556: step 6333, loss 0.142491, acc 0.921875, prec 0.0988213, recall 0.777187
2017-12-10T03:04:50.762379: step 6334, loss 0.292332, acc 0.953125, prec 0.0988296, recall 0.777212
2017-12-10T03:04:51.023055: step 6335, loss 0.452234, acc 0.890625, prec 0.0988325, recall 0.777236
2017-12-10T03:04:51.290375: step 6336, loss 0.10124, acc 0.953125, prec 0.0988284, recall 0.777236
2017-12-10T03:04:51.553337: step 6337, loss 0.863407, acc 0.78125, prec 0.0988093, recall 0.777236
2017-12-10T03:04:51.815807: step 6338, loss 0.401906, acc 0.921875, prec 0.0988149, recall 0.77726
2017-12-10T03:04:52.079635: step 6339, loss 0.495275, acc 0.890625, prec 0.0988178, recall 0.777284
2017-12-10T03:04:52.345000: step 6340, loss 0.477021, acc 0.9375, prec 0.0988124, recall 0.777284
2017-12-10T03:04:52.615750: step 6341, loss 0.214148, acc 0.953125, prec 0.0988083, recall 0.777284
2017-12-10T03:04:52.881773: step 6342, loss 0.254565, acc 0.953125, prec 0.0988042, recall 0.777284
2017-12-10T03:04:53.151878: step 6343, loss 0.0865474, acc 0.96875, prec 0.0988139, recall 0.777308
2017-12-10T03:04:53.423039: step 6344, loss 0.257463, acc 0.953125, prec 0.0988222, recall 0.777332
2017-12-10T03:04:53.687081: step 6345, loss 0.0956272, acc 0.953125, prec 0.0988306, recall 0.777356
2017-12-10T03:04:53.959825: step 6346, loss 0.0552195, acc 0.96875, prec 0.0988527, recall 0.777405
2017-12-10T03:04:54.221930: step 6347, loss 0.0427072, acc 0.984375, prec 0.0988513, recall 0.777405
2017-12-10T03:04:54.492265: step 6348, loss 0.389543, acc 0.953125, prec 0.0988596, recall 0.777429
2017-12-10T03:04:54.761464: step 6349, loss 0.239448, acc 0.96875, prec 0.0988693, recall 0.777453
2017-12-10T03:04:55.026476: step 6350, loss 0.791567, acc 0.953125, prec 0.0988652, recall 0.777453
2017-12-10T03:04:55.298649: step 6351, loss 0.0135247, acc 1, prec 0.0988776, recall 0.777477
2017-12-10T03:04:55.560563: step 6352, loss 0.102546, acc 0.96875, prec 0.0988749, recall 0.777477
2017-12-10T03:04:55.828924: step 6353, loss 0.16697, acc 0.96875, prec 0.0989218, recall 0.777573
2017-12-10T03:04:56.095465: step 6354, loss 0.00711121, acc 1, prec 0.0989342, recall 0.777597
2017-12-10T03:04:56.359081: step 6355, loss 0.0429778, acc 1, prec 0.0989839, recall 0.777694
2017-12-10T03:04:56.631342: step 6356, loss 0.0282247, acc 0.984375, prec 0.0989949, recall 0.777718
2017-12-10T03:04:56.893851: step 6357, loss 0.00159217, acc 1, prec 0.0990073, recall 0.777742
2017-12-10T03:04:57.157548: step 6358, loss 0.0892524, acc 0.984375, prec 0.0990059, recall 0.777742
2017-12-10T03:04:57.419641: step 6359, loss 0.108052, acc 1, prec 0.0990432, recall 0.777814
2017-12-10T03:04:57.693704: step 6360, loss 0.0112972, acc 1, prec 0.0990432, recall 0.777814
2017-12-10T03:04:57.956328: step 6361, loss 0.0206908, acc 1, prec 0.099068, recall 0.777862
2017-12-10T03:04:58.228953: step 6362, loss 0.268001, acc 0.984375, prec 0.0990914, recall 0.77791
2017-12-10T03:04:58.496830: step 6363, loss 0.209816, acc 0.953125, prec 0.0990873, recall 0.77791
2017-12-10T03:04:58.763417: step 6364, loss 0.0625878, acc 1, prec 0.0990997, recall 0.777934
2017-12-10T03:04:59.026176: step 6365, loss 0.00152823, acc 1, prec 0.0991121, recall 0.777958
2017-12-10T03:04:59.293134: step 6366, loss 0.000708942, acc 1, prec 0.0991121, recall 0.777958
2017-12-10T03:04:59.552956: step 6367, loss 0.0753557, acc 0.984375, prec 0.0991108, recall 0.777958
2017-12-10T03:04:59.811690: step 6368, loss 0.0355096, acc 0.96875, prec 0.0991204, recall 0.777982
2017-12-10T03:05:00.083115: step 6369, loss 0.0568708, acc 0.984375, prec 0.0991191, recall 0.777982
2017-12-10T03:05:00.366469: step 6370, loss 0.343186, acc 1, prec 0.0991687, recall 0.778078
2017-12-10T03:05:00.630144: step 6371, loss 0.181375, acc 0.984375, prec 0.0991673, recall 0.778078
2017-12-10T03:05:00.903540: step 6372, loss 0.00232386, acc 1, prec 0.0991673, recall 0.778078
2017-12-10T03:05:01.175840: step 6373, loss 0.00868788, acc 1, prec 0.0991797, recall 0.778102
2017-12-10T03:05:01.437630: step 6374, loss 0.113001, acc 1, prec 0.0992169, recall 0.778174
2017-12-10T03:05:01.705110: step 6375, loss 2.776, acc 0.96875, prec 0.0992155, recall 0.77809
2017-12-10T03:05:01.976559: step 6376, loss 0.0970575, acc 0.96875, prec 0.0992252, recall 0.778114
2017-12-10T03:05:02.243112: step 6377, loss 0.01047, acc 1, prec 0.09925, recall 0.778161
2017-12-10T03:05:02.508460: step 6378, loss 0.0771671, acc 0.96875, prec 0.0992596, recall 0.778185
2017-12-10T03:05:02.783408: step 6379, loss 0.203614, acc 0.953125, prec 0.0992679, recall 0.778209
2017-12-10T03:05:03.056105: step 6380, loss 0.0130825, acc 1, prec 0.0992679, recall 0.778209
2017-12-10T03:05:03.318788: step 6381, loss 0.13382, acc 0.953125, prec 0.0992762, recall 0.778233
2017-12-10T03:05:03.589017: step 6382, loss 0.154127, acc 0.953125, prec 0.0992721, recall 0.778233
2017-12-10T03:05:03.859088: step 6383, loss 0.074185, acc 0.984375, prec 0.0992956, recall 0.778281
2017-12-10T03:05:04.124467: step 6384, loss 0.173411, acc 0.953125, prec 0.0993286, recall 0.778353
2017-12-10T03:05:04.395445: step 6385, loss 0.119985, acc 0.984375, prec 0.0993397, recall 0.778377
2017-12-10T03:05:04.657786: step 6386, loss 0.403437, acc 0.953125, prec 0.0993603, recall 0.778424
2017-12-10T03:05:04.917111: step 6387, loss 0.0378513, acc 0.984375, prec 0.0993714, recall 0.778448
2017-12-10T03:05:05.179514: step 6388, loss 0.484371, acc 0.96875, prec 0.099381, recall 0.778472
2017-12-10T03:05:05.453596: step 6389, loss 0.2548, acc 0.953125, prec 0.0993769, recall 0.778472
2017-12-10T03:05:05.723423: step 6390, loss 0.506114, acc 0.953125, prec 0.0993852, recall 0.778496
2017-12-10T03:05:05.990177: step 6391, loss 0.185765, acc 0.953125, prec 0.0993811, recall 0.778496
2017-12-10T03:05:06.264809: step 6392, loss 0.205779, acc 0.953125, prec 0.0994142, recall 0.778568
2017-12-10T03:05:06.529701: step 6393, loss 0.412466, acc 0.90625, prec 0.0994183, recall 0.778591
2017-12-10T03:05:06.802759: step 6394, loss 0.151876, acc 0.9375, prec 0.0994253, recall 0.778615
2017-12-10T03:05:07.068500: step 6395, loss 0.087727, acc 0.953125, prec 0.0994335, recall 0.778639
2017-12-10T03:05:07.343178: step 6396, loss 0.212317, acc 0.984375, prec 0.0994322, recall 0.778639
2017-12-10T03:05:07.615930: step 6397, loss 0.0951935, acc 0.984375, prec 0.0994308, recall 0.778639
2017-12-10T03:05:07.878980: step 6398, loss 0.217342, acc 0.96875, prec 0.0994281, recall 0.778639
2017-12-10T03:05:08.145546: step 6399, loss 0.215296, acc 0.984375, prec 0.0994515, recall 0.778687
2017-12-10T03:05:08.419809: step 6400, loss 0.14519, acc 0.96875, prec 0.0994487, recall 0.778687
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-6400

2017-12-10T03:05:09.733360: step 6401, loss 0.00111268, acc 1, prec 0.0994859, recall 0.778758
2017-12-10T03:05:10.007800: step 6402, loss 0.0293114, acc 0.984375, prec 0.0994845, recall 0.778758
2017-12-10T03:05:10.268570: step 6403, loss 0.131575, acc 0.984375, prec 0.0994831, recall 0.778758
2017-12-10T03:05:10.533827: step 6404, loss 0.00886871, acc 1, prec 0.0994831, recall 0.778758
2017-12-10T03:05:10.795560: step 6405, loss 0.557829, acc 0.9375, prec 0.0994777, recall 0.778758
2017-12-10T03:05:11.059435: step 6406, loss 4.20865, acc 0.953125, prec 0.0994873, recall 0.778698
2017-12-10T03:05:11.331760: step 6407, loss 0.187162, acc 0.9375, prec 0.0994818, recall 0.778698
2017-12-10T03:05:11.612672: step 6408, loss 0.380953, acc 0.953125, prec 0.0994777, recall 0.778698
2017-12-10T03:05:11.885663: step 6409, loss 0.00293372, acc 1, prec 0.0994901, recall 0.778722
2017-12-10T03:05:12.145832: step 6410, loss 0.353432, acc 0.921875, prec 0.0994833, recall 0.778722
2017-12-10T03:05:12.416745: step 6411, loss 0.9701, acc 0.921875, prec 0.0995012, recall 0.77877
2017-12-10T03:05:12.686785: step 6412, loss 0.151917, acc 0.953125, prec 0.0994971, recall 0.77877
2017-12-10T03:05:12.957571: step 6413, loss 0.555944, acc 0.890625, prec 0.0994999, recall 0.778793
2017-12-10T03:05:13.227376: step 6414, loss 0.358507, acc 0.90625, prec 0.0995041, recall 0.778817
2017-12-10T03:05:13.494670: step 6415, loss 0.531579, acc 0.921875, prec 0.0994972, recall 0.778817
2017-12-10T03:05:13.763705: step 6416, loss 1.05902, acc 0.890625, prec 0.0995371, recall 0.778912
2017-12-10T03:05:14.028688: step 6417, loss 0.219318, acc 0.953125, prec 0.0995454, recall 0.778936
2017-12-10T03:05:14.291830: step 6418, loss 0.393582, acc 0.921875, prec 0.0995509, recall 0.77896
2017-12-10T03:05:14.564151: step 6419, loss 0.180388, acc 0.953125, prec 0.0995592, recall 0.778984
2017-12-10T03:05:14.830328: step 6420, loss 0.703941, acc 0.875, prec 0.0995483, recall 0.778984
2017-12-10T03:05:15.094195: step 6421, loss 0.318257, acc 0.953125, prec 0.0995442, recall 0.778984
2017-12-10T03:05:15.357167: step 6422, loss 0.197289, acc 0.9375, prec 0.0995387, recall 0.778984
2017-12-10T03:05:15.624690: step 6423, loss 0.0145746, acc 1, prec 0.0995511, recall 0.779007
2017-12-10T03:05:15.887516: step 6424, loss 0.656148, acc 0.890625, prec 0.0995662, recall 0.779055
2017-12-10T03:05:16.156459: step 6425, loss 0.652524, acc 0.953125, prec 0.0995621, recall 0.779055
2017-12-10T03:05:16.430339: step 6426, loss 0.0768032, acc 0.96875, prec 0.0996088, recall 0.77915
2017-12-10T03:05:16.699956: step 6427, loss 0.228714, acc 0.9375, prec 0.0996157, recall 0.779173
2017-12-10T03:05:16.967204: step 6428, loss 0.114736, acc 0.96875, prec 0.0996253, recall 0.779197
2017-12-10T03:05:17.235416: step 6429, loss 0.083201, acc 0.984375, prec 0.099624, recall 0.779197
2017-12-10T03:05:17.497885: step 6430, loss 0.403847, acc 0.921875, prec 0.0996418, recall 0.779244
2017-12-10T03:05:17.761520: step 6431, loss 0.0624588, acc 0.984375, prec 0.0996528, recall 0.779268
2017-12-10T03:05:18.026935: step 6432, loss 0.0927842, acc 0.984375, prec 0.0996515, recall 0.779268
2017-12-10T03:05:18.300220: step 6433, loss 0.085734, acc 0.953125, prec 0.0996597, recall 0.779292
2017-12-10T03:05:18.570347: step 6434, loss 0.34949, acc 0.96875, prec 0.099657, recall 0.779292
2017-12-10T03:05:18.850473: step 6435, loss 0.121607, acc 0.984375, prec 0.0996556, recall 0.779292
2017-12-10T03:05:19.113044: step 6436, loss 0.200346, acc 0.953125, prec 0.0996515, recall 0.779292
2017-12-10T03:05:19.383302: step 6437, loss 0.307488, acc 0.96875, prec 0.0996611, recall 0.779316
2017-12-10T03:05:19.653618: step 6438, loss 3.10421, acc 0.9375, prec 0.099657, recall 0.779232
2017-12-10T03:05:19.924851: step 6439, loss 0.0236904, acc 0.984375, prec 0.0996804, recall 0.779279
2017-12-10T03:05:20.198170: step 6440, loss 0.0992711, acc 0.96875, prec 0.0996776, recall 0.779279
2017-12-10T03:05:20.463268: step 6441, loss 0.0180568, acc 1, prec 0.0996776, recall 0.779279
2017-12-10T03:05:20.727155: step 6442, loss 0.0204927, acc 0.984375, prec 0.099701, recall 0.779327
2017-12-10T03:05:20.989910: step 6443, loss 1.8183, acc 0.9375, prec 0.0997092, recall 0.779267
2017-12-10T03:05:21.253616: step 6444, loss 0.46431, acc 0.9375, prec 0.0997284, recall 0.779314
2017-12-10T03:05:21.515712: step 6445, loss 0.423414, acc 0.9375, prec 0.099723, recall 0.779314
2017-12-10T03:05:21.775694: step 6446, loss 0.280606, acc 0.9375, prec 0.0997175, recall 0.779314
2017-12-10T03:05:22.040575: step 6447, loss 0.25694, acc 0.9375, prec 0.0997244, recall 0.779338
2017-12-10T03:05:22.310263: step 6448, loss 0.348464, acc 0.953125, prec 0.0997449, recall 0.779385
2017-12-10T03:05:22.575310: step 6449, loss 0.619488, acc 0.890625, prec 0.0997724, recall 0.779456
2017-12-10T03:05:22.841629: step 6450, loss 0.755824, acc 0.859375, prec 0.0997601, recall 0.779456
2017-12-10T03:05:23.119888: step 6451, loss 1.01488, acc 0.875, prec 0.0997615, recall 0.77948
2017-12-10T03:05:23.382847: step 6452, loss 0.0911755, acc 0.9375, prec 0.0997684, recall 0.779503
2017-12-10T03:05:23.655271: step 6453, loss 0.200998, acc 0.9375, prec 0.0997629, recall 0.779503
2017-12-10T03:05:23.921214: step 6454, loss 0.372601, acc 0.875, prec 0.0997766, recall 0.77955
2017-12-10T03:05:24.187983: step 6455, loss 0.511878, acc 0.921875, prec 0.0997945, recall 0.779598
2017-12-10T03:05:24.464548: step 6456, loss 0.243551, acc 0.953125, prec 0.0998027, recall 0.779621
2017-12-10T03:05:24.735390: step 6457, loss 0.156722, acc 0.9375, prec 0.0998096, recall 0.779645
2017-12-10T03:05:25.001624: step 6458, loss 0.0722568, acc 0.984375, prec 0.0998205, recall 0.779668
2017-12-10T03:05:25.268044: step 6459, loss 0.348214, acc 0.921875, prec 0.099826, recall 0.779692
2017-12-10T03:05:25.532431: step 6460, loss 0.466232, acc 0.953125, prec 0.0998219, recall 0.779692
2017-12-10T03:05:25.768852: step 6461, loss 0.443743, acc 0.941176, prec 0.0998178, recall 0.779692
2017-12-10T03:05:26.044284: step 6462, loss 0.07231, acc 0.953125, prec 0.0998137, recall 0.779692
2017-12-10T03:05:26.309228: step 6463, loss 0.14539, acc 0.984375, prec 0.0998123, recall 0.779692
2017-12-10T03:05:26.585221: step 6464, loss 0.0126032, acc 1, prec 0.0998247, recall 0.779715
2017-12-10T03:05:26.848239: step 6465, loss 0.0370111, acc 0.96875, prec 0.0998219, recall 0.779715
2017-12-10T03:05:27.125263: step 6466, loss 0.0113334, acc 1, prec 0.0998219, recall 0.779715
2017-12-10T03:05:27.392390: step 6467, loss 0.0508967, acc 0.984375, prec 0.0998329, recall 0.779739
2017-12-10T03:05:27.653424: step 6468, loss 0.17092, acc 0.984375, prec 0.0998685, recall 0.77981
2017-12-10T03:05:27.925625: step 6469, loss 0.509399, acc 0.984375, prec 0.0998672, recall 0.77981
2017-12-10T03:05:28.194388: step 6470, loss 0.214957, acc 0.953125, prec 0.0998877, recall 0.779857
2017-12-10T03:05:28.459292: step 6471, loss 0.0925947, acc 0.984375, prec 0.0998987, recall 0.77988
2017-12-10T03:05:28.727327: step 6472, loss 0.0931033, acc 0.96875, prec 0.0999083, recall 0.779904
2017-12-10T03:05:28.996426: step 6473, loss 0.258373, acc 0.9375, prec 0.0999151, recall 0.779927
2017-12-10T03:05:29.261103: step 6474, loss 0.185787, acc 0.953125, prec 0.099911, recall 0.779927
2017-12-10T03:05:29.524611: step 6475, loss 0.12709, acc 0.984375, prec 0.0999096, recall 0.779927
2017-12-10T03:05:29.790470: step 6476, loss 0.0148276, acc 1, prec 0.0999096, recall 0.779927
2017-12-10T03:05:30.057612: step 6477, loss 0.0315102, acc 0.984375, prec 0.0999206, recall 0.779951
2017-12-10T03:05:30.328081: step 6478, loss 0.0194086, acc 1, prec 0.0999206, recall 0.779951
2017-12-10T03:05:30.602560: step 6479, loss 4.12688, acc 0.984375, prec 0.0999576, recall 0.779938
2017-12-10T03:05:30.879927: step 6480, loss 0.253099, acc 0.953125, prec 0.0999535, recall 0.779938
2017-12-10T03:05:31.146045: step 6481, loss 0.388338, acc 0.9375, prec 0.0999849, recall 0.780009
2017-12-10T03:05:31.420856: step 6482, loss 0.0437239, acc 0.953125, prec 0.0999808, recall 0.780009
2017-12-10T03:05:31.687774: step 6483, loss 0.43858, acc 0.953125, prec 0.099989, recall 0.780032
2017-12-10T03:05:31.955794: step 6484, loss 0.203646, acc 0.96875, prec 0.0999863, recall 0.780032
2017-12-10T03:05:32.219886: step 6485, loss 0.170749, acc 0.96875, prec 0.0999959, recall 0.780056
2017-12-10T03:05:32.487296: step 6486, loss 0.226393, acc 0.9375, prec 0.100027, recall 0.780126
2017-12-10T03:05:32.752166: step 6487, loss 0.332888, acc 0.9375, prec 0.100022, recall 0.780126
2017-12-10T03:05:33.017010: step 6488, loss 0.28809, acc 0.921875, prec 0.100015, recall 0.780126
2017-12-10T03:05:33.292132: step 6489, loss 0.32455, acc 0.9375, prec 0.100022, recall 0.780149
2017-12-10T03:05:33.555829: step 6490, loss 0.29197, acc 0.921875, prec 0.100015, recall 0.780149
2017-12-10T03:05:33.833253: step 6491, loss 0.416123, acc 0.96875, prec 0.100012, recall 0.780149
2017-12-10T03:05:34.096057: step 6492, loss 0.247107, acc 0.921875, prec 0.100005, recall 0.780149
2017-12-10T03:05:34.361802: step 6493, loss 0.251866, acc 0.921875, prec 0.100011, recall 0.780173
2017-12-10T03:05:34.629731: step 6494, loss 0.812201, acc 0.875, prec 0.1, recall 0.780173
2017-12-10T03:05:34.900625: step 6495, loss 0.265443, acc 0.90625, prec 0.0999918, recall 0.780173
2017-12-10T03:05:35.163252: step 6496, loss 0.144325, acc 0.953125, prec 0.1, recall 0.780196
2017-12-10T03:05:35.428947: step 6497, loss 0.263919, acc 0.90625, prec 0.0999918, recall 0.780196
2017-12-10T03:05:35.698128: step 6498, loss 0.130995, acc 0.953125, prec 0.1, recall 0.78022
2017-12-10T03:05:35.967101: step 6499, loss 0.377915, acc 0.96875, prec 0.10001, recall 0.780243
2017-12-10T03:05:36.233035: step 6500, loss 0.28667, acc 0.875, prec 0.0999986, recall 0.780243
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-6500

2017-12-10T03:05:37.529069: step 6501, loss 0.504584, acc 0.9375, prec 0.100005, recall 0.780267
2017-12-10T03:05:37.796656: step 6502, loss 0.606626, acc 0.9375, prec 0.1, recall 0.780267
2017-12-10T03:05:38.063329: step 6503, loss 0.274301, acc 0.96875, prec 0.100022, recall 0.780314
2017-12-10T03:05:38.331012: step 6504, loss 0.00135564, acc 1, prec 0.100022, recall 0.780314
2017-12-10T03:05:38.598136: step 6505, loss 0.413694, acc 0.9375, prec 0.100029, recall 0.780337
2017-12-10T03:05:38.876852: step 6506, loss 0.123912, acc 0.953125, prec 0.100037, recall 0.78036
2017-12-10T03:05:39.139283: step 6507, loss 0.498554, acc 0.984375, prec 0.100048, recall 0.780384
2017-12-10T03:05:39.407717: step 6508, loss 0.0697656, acc 0.984375, prec 0.100059, recall 0.780407
2017-12-10T03:05:39.673728: step 6509, loss 0.0511748, acc 0.96875, prec 0.100068, recall 0.780431
2017-12-10T03:05:39.936794: step 6510, loss 0.327617, acc 0.96875, prec 0.10009, recall 0.780477
2017-12-10T03:05:40.200444: step 6511, loss 0.0642274, acc 0.984375, prec 0.100101, recall 0.780501
2017-12-10T03:05:40.464143: step 6512, loss 0.028782, acc 1, prec 0.100113, recall 0.780524
2017-12-10T03:05:40.733522: step 6513, loss 0.0271508, acc 0.984375, prec 0.100112, recall 0.780524
2017-12-10T03:05:40.998604: step 6514, loss 0.00135361, acc 1, prec 0.100112, recall 0.780524
2017-12-10T03:05:41.272306: step 6515, loss 0.0063544, acc 1, prec 0.100124, recall 0.780548
2017-12-10T03:05:41.545410: step 6516, loss 0.0156993, acc 1, prec 0.100137, recall 0.780571
2017-12-10T03:05:41.816691: step 6517, loss 0.0298277, acc 0.984375, prec 0.100135, recall 0.780571
2017-12-10T03:05:42.089583: step 6518, loss 0.438453, acc 0.953125, prec 0.100168, recall 0.780641
2017-12-10T03:05:42.357052: step 6519, loss 0.138331, acc 0.96875, prec 0.100178, recall 0.780664
2017-12-10T03:05:42.626065: step 6520, loss 0.0282418, acc 1, prec 0.10019, recall 0.780688
2017-12-10T03:05:42.887736: step 6521, loss 0.160217, acc 0.96875, prec 0.100199, recall 0.780711
2017-12-10T03:05:43.154092: step 6522, loss 0.115384, acc 0.984375, prec 0.10021, recall 0.780734
2017-12-10T03:05:43.416466: step 6523, loss 0.266128, acc 0.96875, prec 0.10022, recall 0.780758
2017-12-10T03:05:43.697015: step 6524, loss 0.177976, acc 0.96875, prec 0.100254, recall 0.780828
2017-12-10T03:05:43.960032: step 6525, loss 0.0160216, acc 0.984375, prec 0.100253, recall 0.780828
2017-12-10T03:05:44.224843: step 6526, loss 0.0463697, acc 0.984375, prec 0.100264, recall 0.780851
2017-12-10T03:05:44.497767: step 6527, loss 0.0241429, acc 0.984375, prec 0.100287, recall 0.780898
2017-12-10T03:05:44.771247: step 6528, loss 0.128277, acc 0.96875, prec 0.100284, recall 0.780898
2017-12-10T03:05:45.043256: step 6529, loss 0.154594, acc 0.953125, prec 0.10028, recall 0.780898
2017-12-10T03:05:45.309298: step 6530, loss 0.09718, acc 0.984375, prec 0.100279, recall 0.780898
2017-12-10T03:05:45.570914: step 6531, loss 0.0119326, acc 1, prec 0.100279, recall 0.780898
2017-12-10T03:05:45.837627: step 6532, loss 0.130048, acc 0.984375, prec 0.100277, recall 0.780898
2017-12-10T03:05:46.106467: step 6533, loss 0.00584773, acc 1, prec 0.10029, recall 0.780921
2017-12-10T03:05:46.376033: step 6534, loss 0.122166, acc 0.953125, prec 0.100285, recall 0.780921
2017-12-10T03:05:46.640514: step 6535, loss 0.019969, acc 0.984375, prec 0.100296, recall 0.780944
2017-12-10T03:05:46.908382: step 6536, loss 0.00213396, acc 1, prec 0.100309, recall 0.780968
2017-12-10T03:05:47.172617: step 6537, loss 0.00997117, acc 1, prec 0.100358, recall 0.781061
2017-12-10T03:05:47.436688: step 6538, loss 0.149532, acc 1, prec 0.10037, recall 0.781084
2017-12-10T03:05:47.712907: step 6539, loss 0.0798047, acc 1, prec 0.100395, recall 0.78113
2017-12-10T03:05:47.983571: step 6540, loss 0.0610079, acc 0.984375, prec 0.100406, recall 0.781154
2017-12-10T03:05:48.261302: step 6541, loss 3.71482e-05, acc 1, prec 0.100406, recall 0.781154
2017-12-10T03:05:48.518452: step 6542, loss 0.0238696, acc 0.984375, prec 0.100404, recall 0.781154
2017-12-10T03:05:48.783170: step 6543, loss 0.0234103, acc 0.984375, prec 0.100403, recall 0.781154
2017-12-10T03:05:49.045988: step 6544, loss 0.133114, acc 0.96875, prec 0.1004, recall 0.781154
2017-12-10T03:05:49.310074: step 6545, loss 2.99647e-05, acc 1, prec 0.100412, recall 0.781177
2017-12-10T03:05:49.569722: step 6546, loss 0.115413, acc 0.984375, prec 0.100423, recall 0.7812
2017-12-10T03:05:49.843674: step 6547, loss 0.430997, acc 0.96875, prec 0.100433, recall 0.781223
2017-12-10T03:05:50.120819: step 6548, loss 0.0666524, acc 0.984375, prec 0.100444, recall 0.781247
2017-12-10T03:05:50.385092: step 6549, loss 0.142473, acc 0.96875, prec 0.100453, recall 0.78127
2017-12-10T03:05:50.651435: step 6550, loss 0.0271306, acc 0.984375, prec 0.100452, recall 0.78127
2017-12-10T03:05:50.930647: step 6551, loss 0.137873, acc 0.984375, prec 0.100451, recall 0.78127
2017-12-10T03:05:51.202189: step 6552, loss 0.162141, acc 1, prec 0.100463, recall 0.781293
2017-12-10T03:05:51.469959: step 6553, loss 0.0275109, acc 0.984375, prec 0.100474, recall 0.781316
2017-12-10T03:05:51.737263: step 6554, loss 2.95044e-05, acc 1, prec 0.100474, recall 0.781316
2017-12-10T03:05:51.999360: step 6555, loss 0.036744, acc 1, prec 0.100486, recall 0.78134
2017-12-10T03:05:52.263855: step 6556, loss 0.232308, acc 0.96875, prec 0.100508, recall 0.781386
2017-12-10T03:05:52.526661: step 6557, loss 0.00386271, acc 1, prec 0.100508, recall 0.781386
2017-12-10T03:05:52.789681: step 6558, loss 0.375354, acc 0.953125, prec 0.100504, recall 0.781386
2017-12-10T03:05:53.056853: step 6559, loss 0.044189, acc 0.984375, prec 0.100502, recall 0.781386
2017-12-10T03:05:53.321024: step 6560, loss 0.152183, acc 0.984375, prec 0.100513, recall 0.781409
2017-12-10T03:05:53.588636: step 6561, loss 0.000320378, acc 1, prec 0.100513, recall 0.781409
2017-12-10T03:05:53.849473: step 6562, loss 0.0566176, acc 0.984375, prec 0.100512, recall 0.781409
2017-12-10T03:05:54.115222: step 6563, loss 0.000281071, acc 1, prec 0.100536, recall 0.781456
2017-12-10T03:05:54.392074: step 6564, loss 0.136854, acc 0.984375, prec 0.100547, recall 0.781479
2017-12-10T03:05:54.674564: step 6565, loss 0.0631792, acc 0.984375, prec 0.100546, recall 0.781479
2017-12-10T03:05:54.945252: step 6566, loss 0.214537, acc 0.984375, prec 0.100581, recall 0.781548
2017-12-10T03:05:55.212860: step 6567, loss 0.224165, acc 0.984375, prec 0.100592, recall 0.781571
2017-12-10T03:05:55.479775: step 6568, loss 0.0888667, acc 0.984375, prec 0.100591, recall 0.781571
2017-12-10T03:05:55.750636: step 6569, loss 0.0667548, acc 0.984375, prec 0.100602, recall 0.781595
2017-12-10T03:05:56.015826: step 6570, loss 0.0251675, acc 0.984375, prec 0.1006, recall 0.781595
2017-12-10T03:05:56.290576: step 6571, loss 0.263168, acc 0.953125, prec 0.100596, recall 0.781595
2017-12-10T03:05:56.570058: step 6572, loss 1.51839, acc 0.984375, prec 0.100609, recall 0.781535
2017-12-10T03:05:56.838419: step 6573, loss 0.0115335, acc 1, prec 0.100621, recall 0.781558
2017-12-10T03:05:57.111632: step 6574, loss 0.0904664, acc 1, prec 0.100633, recall 0.781581
2017-12-10T03:05:57.388371: step 6575, loss 0.000397468, acc 1, prec 0.100633, recall 0.781581
2017-12-10T03:05:57.654946: step 6576, loss 0.102103, acc 0.96875, prec 0.100643, recall 0.781604
2017-12-10T03:05:57.925536: step 6577, loss 0.0981518, acc 0.9375, prec 0.100649, recall 0.781627
2017-12-10T03:05:58.202234: step 6578, loss 0.347707, acc 0.921875, prec 0.100667, recall 0.781674
2017-12-10T03:05:58.471855: step 6579, loss 0.373425, acc 0.953125, prec 0.100663, recall 0.781674
2017-12-10T03:05:58.740215: step 6580, loss 0.260286, acc 0.9375, prec 0.10067, recall 0.781697
2017-12-10T03:05:59.012041: step 6581, loss 0.0577563, acc 0.96875, prec 0.100667, recall 0.781697
2017-12-10T03:05:59.282419: step 6582, loss 0.354368, acc 0.9375, prec 0.100698, recall 0.781766
2017-12-10T03:05:59.548765: step 6583, loss 0.260061, acc 0.890625, prec 0.100701, recall 0.781789
2017-12-10T03:05:59.815307: step 6584, loss 0.188314, acc 0.921875, prec 0.100694, recall 0.781789
2017-12-10T03:06:00.095428: step 6585, loss 0.710665, acc 0.921875, prec 0.1007, recall 0.781812
2017-12-10T03:06:00.363389: step 6586, loss 0.304166, acc 0.875, prec 0.100701, recall 0.781835
2017-12-10T03:06:00.630575: step 6587, loss 0.141879, acc 0.953125, prec 0.100721, recall 0.781882
2017-12-10T03:06:00.906682: step 6588, loss 0.422754, acc 0.9375, prec 0.100728, recall 0.781905
2017-12-10T03:06:01.171016: step 6589, loss 0.518184, acc 0.875, prec 0.100741, recall 0.781951
2017-12-10T03:06:02.149938: step 6590, loss 0.0706398, acc 0.96875, prec 0.100739, recall 0.781951
2017-12-10T03:06:02.523634: step 6591, loss 0.173566, acc 0.96875, prec 0.100761, recall 0.781997
2017-12-10T03:06:02.831735: step 6592, loss 0.367152, acc 0.921875, prec 0.100778, recall 0.782043
2017-12-10T03:06:03.548242: step 6593, loss 0.236676, acc 0.9375, prec 0.100773, recall 0.782043
2017-12-10T03:06:04.285022: step 6594, loss 0.116298, acc 0.984375, prec 0.100784, recall 0.782066
2017-12-10T03:06:04.991951: step 6595, loss 0.00613168, acc 1, prec 0.100808, recall 0.782112
2017-12-10T03:06:05.716294: step 6596, loss 0.0181636, acc 1, prec 0.100808, recall 0.782112
2017-12-10T03:06:06.411546: step 6597, loss 0.231386, acc 0.953125, prec 0.100816, recall 0.782135
2017-12-10T03:06:07.153655: step 6598, loss 0.165547, acc 0.953125, prec 0.100812, recall 0.782135
2017-12-10T03:06:07.901019: step 6599, loss 0.617159, acc 0.953125, prec 0.100808, recall 0.782135
2017-12-10T03:06:08.946558: step 6600, loss 4.69412, acc 0.984375, prec 0.100808, recall 0.782053

Evaluation:
2017-12-10T03:06:16.589945: step 6600, loss 8.0424, acc 0.954524, prec 0.101141, recall 0.776104

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-6600

2017-12-10T03:06:17.880096: step 6601, loss 0.21158, acc 0.984375, prec 0.101152, recall 0.776127
2017-12-10T03:06:18.144333: step 6602, loss 0.0274912, acc 0.984375, prec 0.101175, recall 0.776174
2017-12-10T03:06:18.415089: step 6603, loss 0.283084, acc 0.953125, prec 0.101171, recall 0.776174
2017-12-10T03:06:18.677483: step 6604, loss 0.00979036, acc 1, prec 0.101195, recall 0.77622
2017-12-10T03:06:18.946703: step 6605, loss 0.439083, acc 0.921875, prec 0.101189, recall 0.77622
2017-12-10T03:06:19.213650: step 6606, loss 0.559369, acc 0.921875, prec 0.101194, recall 0.776243
2017-12-10T03:06:19.481279: step 6607, loss 0.405668, acc 0.96875, prec 0.101215, recall 0.77629
2017-12-10T03:06:19.748576: step 6608, loss 0.309532, acc 0.953125, prec 0.101224, recall 0.776313
2017-12-10T03:06:20.016097: step 6609, loss 0.478748, acc 0.921875, prec 0.101229, recall 0.776336
2017-12-10T03:06:20.286091: step 6610, loss 0.28004, acc 0.9375, prec 0.101248, recall 0.776383
2017-12-10T03:06:20.551646: step 6611, loss 0.254426, acc 0.953125, prec 0.101244, recall 0.776383
2017-12-10T03:06:20.830873: step 6612, loss 0.18858, acc 0.96875, prec 0.101253, recall 0.776406
2017-12-10T03:06:21.099556: step 6613, loss 0.366419, acc 0.953125, prec 0.101249, recall 0.776406
2017-12-10T03:06:21.366092: step 6614, loss 0.417847, acc 0.921875, prec 0.101254, recall 0.776429
2017-12-10T03:06:21.631317: step 6615, loss 0.328753, acc 0.890625, prec 0.101245, recall 0.776429
2017-12-10T03:06:21.901820: step 6616, loss 0.186363, acc 0.9375, prec 0.101251, recall 0.776452
2017-12-10T03:06:22.167559: step 6617, loss 0.0529972, acc 0.984375, prec 0.101262, recall 0.776475
2017-12-10T03:06:22.433882: step 6618, loss 0.153613, acc 0.953125, prec 0.101282, recall 0.776522
2017-12-10T03:06:22.705603: step 6619, loss 0.246721, acc 0.984375, prec 0.101305, recall 0.776568
2017-12-10T03:06:22.974915: step 6620, loss 0.0293874, acc 1, prec 0.101317, recall 0.776591
2017-12-10T03:06:23.236856: step 6621, loss 0.23928, acc 0.953125, prec 0.101325, recall 0.776614
2017-12-10T03:06:23.501722: step 6622, loss 0.206401, acc 0.984375, prec 0.101324, recall 0.776614
2017-12-10T03:06:23.768717: step 6623, loss 0.255302, acc 0.953125, prec 0.101344, recall 0.776661
2017-12-10T03:06:24.030011: step 6624, loss 0.0921213, acc 0.96875, prec 0.101366, recall 0.776707
2017-12-10T03:06:24.294046: step 6625, loss 0.136137, acc 0.984375, prec 0.101364, recall 0.776707
2017-12-10T03:06:24.557415: step 6626, loss 0.164983, acc 0.96875, prec 0.101374, recall 0.77673
2017-12-10T03:06:24.830370: step 6627, loss 0.379675, acc 0.953125, prec 0.10137, recall 0.77673
2017-12-10T03:06:25.090555: step 6628, loss 0.0448652, acc 0.984375, prec 0.10138, recall 0.776753
2017-12-10T03:06:25.356958: step 6629, loss 0.131958, acc 0.984375, prec 0.101416, recall 0.776823
2017-12-10T03:06:25.618910: step 6630, loss 0.0839974, acc 0.96875, prec 0.101413, recall 0.776823
2017-12-10T03:06:25.883767: step 6631, loss 0.0536373, acc 0.96875, prec 0.10141, recall 0.776823
2017-12-10T03:06:26.143576: step 6632, loss 0.180464, acc 0.96875, prec 0.101419, recall 0.776846
2017-12-10T03:06:26.413868: step 6633, loss 0.090782, acc 0.984375, prec 0.10143, recall 0.776869
2017-12-10T03:06:26.697375: step 6634, loss 0.059242, acc 1, prec 0.101442, recall 0.776892
2017-12-10T03:06:26.960548: step 6635, loss 1.97171, acc 0.9375, prec 0.10145, recall 0.776835
2017-12-10T03:06:27.231266: step 6636, loss 0.0862636, acc 0.953125, prec 0.101458, recall 0.776858
2017-12-10T03:06:27.504718: step 6637, loss 0.120217, acc 0.96875, prec 0.101456, recall 0.776858
2017-12-10T03:06:27.774465: step 6638, loss 7.00997, acc 0.96875, prec 0.101479, recall 0.776824
2017-12-10T03:06:28.052104: step 6639, loss 0.0517857, acc 0.984375, prec 0.101477, recall 0.776824
2017-12-10T03:06:28.324396: step 6640, loss 0.238876, acc 0.953125, prec 0.101485, recall 0.776847
2017-12-10T03:06:28.593990: step 6641, loss 1.18979, acc 0.921875, prec 0.101491, recall 0.77687
2017-12-10T03:06:28.860506: step 6642, loss 1.0871, acc 0.828125, prec 0.101476, recall 0.77687
2017-12-10T03:06:29.126520: step 6643, loss 0.699189, acc 0.890625, prec 0.101514, recall 0.776962
2017-12-10T03:06:29.392400: step 6644, loss 0.396888, acc 0.921875, prec 0.10152, recall 0.776985
2017-12-10T03:06:29.661505: step 6645, loss 0.398338, acc 0.921875, prec 0.101525, recall 0.777008
2017-12-10T03:06:29.929772: step 6646, loss 0.824537, acc 0.8125, prec 0.101509, recall 0.777008
2017-12-10T03:06:30.195994: step 6647, loss 1.69826, acc 0.75, prec 0.101499, recall 0.777031
2017-12-10T03:06:30.462762: step 6648, loss 1.082, acc 0.828125, prec 0.101484, recall 0.777031
2017-12-10T03:06:30.724481: step 6649, loss 1.28889, acc 0.765625, prec 0.101463, recall 0.777031
2017-12-10T03:06:30.994008: step 6650, loss 1.25887, acc 0.796875, prec 0.101457, recall 0.777054
2017-12-10T03:06:31.259985: step 6651, loss 1.2011, acc 0.84375, prec 0.101444, recall 0.777054
2017-12-10T03:06:31.527406: step 6652, loss 1.2672, acc 0.890625, prec 0.101458, recall 0.7771
2017-12-10T03:06:31.792640: step 6653, loss 0.637104, acc 0.875, prec 0.101472, recall 0.777146
2017-12-10T03:06:32.057946: step 6654, loss 0.896011, acc 0.828125, prec 0.101481, recall 0.777192
2017-12-10T03:06:32.322804: step 6655, loss 0.863993, acc 0.90625, prec 0.101509, recall 0.777261
2017-12-10T03:06:32.591843: step 6656, loss 0.435887, acc 0.9375, prec 0.101528, recall 0.777307
2017-12-10T03:06:32.855489: step 6657, loss 0.40627, acc 0.921875, prec 0.101521, recall 0.777307
2017-12-10T03:06:33.116594: step 6658, loss 0.271726, acc 0.9375, prec 0.10154, recall 0.777353
2017-12-10T03:06:33.382185: step 6659, loss 0.00702271, acc 1, prec 0.101564, recall 0.777399
2017-12-10T03:06:33.643097: step 6660, loss 0.0472674, acc 0.984375, prec 0.101587, recall 0.777445
2017-12-10T03:06:33.913827: step 6661, loss 0.77374, acc 0.953125, prec 0.101595, recall 0.777468
2017-12-10T03:06:34.179295: step 6662, loss 0.0180024, acc 1, prec 0.101595, recall 0.777468
2017-12-10T03:06:34.439631: step 6663, loss 0.15229, acc 0.984375, prec 0.101593, recall 0.777468
2017-12-10T03:06:34.702766: step 6664, loss 0.0465567, acc 0.984375, prec 0.101592, recall 0.777468
2017-12-10T03:06:34.965032: step 6665, loss 0.000443685, acc 1, prec 0.101604, recall 0.777491
2017-12-10T03:06:35.226083: step 6666, loss 0.0253608, acc 0.984375, prec 0.101615, recall 0.777514
2017-12-10T03:06:35.496425: step 6667, loss 0.0226155, acc 1, prec 0.101627, recall 0.777537
2017-12-10T03:06:35.758558: step 6668, loss 0.1562, acc 0.953125, prec 0.101623, recall 0.777537
2017-12-10T03:06:36.021343: step 6669, loss 17.1205, acc 0.96875, prec 0.101634, recall 0.77748
2017-12-10T03:06:36.291110: step 6670, loss 0.0452406, acc 0.984375, prec 0.101644, recall 0.777503
2017-12-10T03:06:36.558695: step 6671, loss 0.0354077, acc 0.96875, prec 0.101654, recall 0.777526
2017-12-10T03:06:36.824812: step 6672, loss 0.202695, acc 0.984375, prec 0.101665, recall 0.777549
2017-12-10T03:06:37.091283: step 6673, loss 0.310803, acc 0.953125, prec 0.101673, recall 0.777572
2017-12-10T03:06:37.367747: step 6674, loss 0.214283, acc 0.953125, prec 0.101681, recall 0.777595
2017-12-10T03:06:37.636715: step 6675, loss 0.0297964, acc 0.984375, prec 0.101679, recall 0.777595
2017-12-10T03:06:37.900302: step 6676, loss 0.684789, acc 0.875, prec 0.101705, recall 0.777663
2017-12-10T03:06:38.164486: step 6677, loss 0.189473, acc 0.921875, prec 0.101698, recall 0.777663
2017-12-10T03:06:38.428543: step 6678, loss 0.334015, acc 0.90625, prec 0.101702, recall 0.777686
2017-12-10T03:06:38.694329: step 6679, loss 0.0828597, acc 0.953125, prec 0.101697, recall 0.777686
2017-12-10T03:06:38.959339: step 6680, loss 0.188674, acc 0.96875, prec 0.101695, recall 0.777686
2017-12-10T03:06:39.225031: step 6681, loss 0.297397, acc 0.9375, prec 0.101701, recall 0.777709
2017-12-10T03:06:39.496232: step 6682, loss 0.104717, acc 0.96875, prec 0.101723, recall 0.777755
2017-12-10T03:06:39.758059: step 6683, loss 0.392681, acc 0.953125, prec 0.101743, recall 0.777801
2017-12-10T03:06:40.025264: step 6684, loss 0.388277, acc 0.9375, prec 0.101749, recall 0.777824
2017-12-10T03:06:40.288679: step 6685, loss 0.218619, acc 0.953125, prec 0.101757, recall 0.777846
2017-12-10T03:06:40.555788: step 6686, loss 0.20179, acc 0.921875, prec 0.101787, recall 0.777915
2017-12-10T03:06:40.820104: step 6687, loss 0.139986, acc 0.953125, prec 0.101783, recall 0.777915
2017-12-10T03:06:41.094179: step 6688, loss 0.44893, acc 0.953125, prec 0.101791, recall 0.777938
2017-12-10T03:06:41.370302: step 6689, loss 0.242868, acc 0.953125, prec 0.101823, recall 0.778006
2017-12-10T03:06:41.646967: step 6690, loss 0.0877524, acc 0.96875, prec 0.10182, recall 0.778006
2017-12-10T03:06:41.913747: step 6691, loss 0.342662, acc 0.9375, prec 0.101827, recall 0.778029
2017-12-10T03:06:42.177177: step 6692, loss 0.00365783, acc 1, prec 0.101827, recall 0.778029
2017-12-10T03:06:42.456671: step 6693, loss 0.200843, acc 0.953125, prec 0.101847, recall 0.778075
2017-12-10T03:06:42.725535: step 6694, loss 0.630489, acc 0.953125, prec 0.101867, recall 0.778121
2017-12-10T03:06:42.992948: step 6695, loss 0.0704183, acc 0.984375, prec 0.101866, recall 0.778121
2017-12-10T03:06:43.253402: step 6696, loss 0.0437364, acc 0.984375, prec 0.101864, recall 0.778121
2017-12-10T03:06:43.526859: step 6697, loss 0.0962084, acc 0.96875, prec 0.101886, recall 0.778166
2017-12-10T03:06:43.789941: step 6698, loss 0.00966628, acc 1, prec 0.101886, recall 0.778166
2017-12-10T03:06:44.049935: step 6699, loss 0.351553, acc 0.96875, prec 0.101895, recall 0.778189
2017-12-10T03:06:44.317775: step 6700, loss 0.0369472, acc 0.984375, prec 0.101894, recall 0.778189
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-6700

2017-12-10T03:06:45.713824: step 6701, loss 0.0195832, acc 0.984375, prec 0.101904, recall 0.778212
2017-12-10T03:06:45.980457: step 6702, loss 0.0443129, acc 0.984375, prec 0.101903, recall 0.778212
2017-12-10T03:06:46.247411: step 6703, loss 0.344839, acc 0.96875, prec 0.101924, recall 0.778257
2017-12-10T03:06:46.511638: step 6704, loss 0.0758075, acc 0.953125, prec 0.101932, recall 0.77828
2017-12-10T03:06:46.777739: step 6705, loss 0.412515, acc 0.953125, prec 0.101928, recall 0.77828
2017-12-10T03:06:47.042110: step 6706, loss 0.108835, acc 0.96875, prec 0.101974, recall 0.778371
2017-12-10T03:06:47.303914: step 6707, loss 0.190435, acc 0.953125, prec 0.101982, recall 0.778394
2017-12-10T03:06:47.573736: step 6708, loss 0.154316, acc 1, prec 0.101994, recall 0.778417
2017-12-10T03:06:47.847207: step 6709, loss 0.130116, acc 0.96875, prec 0.102003, recall 0.778439
2017-12-10T03:06:48.106097: step 6710, loss 0.21778, acc 0.96875, prec 0.102, recall 0.778439
2017-12-10T03:06:48.368254: step 6711, loss 0.0041157, acc 1, prec 0.102, recall 0.778439
2017-12-10T03:06:48.630254: step 6712, loss 0.0195782, acc 0.984375, prec 0.101999, recall 0.778439
2017-12-10T03:06:48.898286: step 6713, loss 0.0935753, acc 0.96875, prec 0.102008, recall 0.778462
2017-12-10T03:06:49.159842: step 6714, loss 0.0018446, acc 1, prec 0.102008, recall 0.778462
2017-12-10T03:06:49.416442: step 6715, loss 0.102838, acc 0.984375, prec 0.102019, recall 0.778485
2017-12-10T03:06:49.682267: step 6716, loss 0.000635462, acc 1, prec 0.102031, recall 0.778508
2017-12-10T03:06:49.948483: step 6717, loss 0.0146854, acc 1, prec 0.102055, recall 0.778553
2017-12-10T03:06:50.216573: step 6718, loss 0.405055, acc 0.953125, prec 0.102051, recall 0.778553
2017-12-10T03:06:50.486840: step 6719, loss 2.1398, acc 0.984375, prec 0.102051, recall 0.778473
2017-12-10T03:06:50.758350: step 6720, loss 0.000955718, acc 1, prec 0.102051, recall 0.778473
2017-12-10T03:06:51.024304: step 6721, loss 0.010078, acc 1, prec 0.102063, recall 0.778496
2017-12-10T03:06:51.290600: step 6722, loss 0.103662, acc 0.953125, prec 0.102095, recall 0.778564
2017-12-10T03:06:51.557079: step 6723, loss 0.20388, acc 0.96875, prec 0.102105, recall 0.778587
2017-12-10T03:06:51.825519: step 6724, loss 0.118669, acc 0.984375, prec 0.102115, recall 0.77861
2017-12-10T03:06:52.093225: step 6725, loss 0.70851, acc 0.9375, prec 0.102122, recall 0.778632
2017-12-10T03:06:52.357175: step 6726, loss 0.0239382, acc 0.984375, prec 0.102133, recall 0.778655
2017-12-10T03:06:52.625629: step 6727, loss 0.014994, acc 1, prec 0.102157, recall 0.7787
2017-12-10T03:06:52.895795: step 6728, loss 0.03615, acc 0.984375, prec 0.102168, recall 0.778723
2017-12-10T03:06:53.170027: step 6729, loss 0.176888, acc 0.953125, prec 0.1022, recall 0.778791
2017-12-10T03:06:53.436594: step 6730, loss 0.00182848, acc 1, prec 0.102212, recall 0.778814
2017-12-10T03:06:53.704205: step 6731, loss 0.0386277, acc 0.984375, prec 0.102222, recall 0.778836
2017-12-10T03:06:53.969710: step 6732, loss 0.13693, acc 0.96875, prec 0.102232, recall 0.778859
2017-12-10T03:06:54.235431: step 6733, loss 2.83103, acc 0.953125, prec 0.102253, recall 0.778825
2017-12-10T03:06:54.506042: step 6734, loss 0.676773, acc 0.96875, prec 0.10225, recall 0.778825
2017-12-10T03:06:54.770323: step 6735, loss 0.344507, acc 0.90625, prec 0.102278, recall 0.778892
2017-12-10T03:06:55.035265: step 6736, loss 0.0703744, acc 0.96875, prec 0.102288, recall 0.778915
2017-12-10T03:06:55.308866: step 6737, loss 0.316242, acc 0.921875, prec 0.102293, recall 0.778938
2017-12-10T03:06:55.582104: step 6738, loss 0.451067, acc 0.921875, prec 0.102286, recall 0.778938
2017-12-10T03:06:55.846318: step 6739, loss 0.811362, acc 0.90625, prec 0.102278, recall 0.778938
2017-12-10T03:06:56.116419: step 6740, loss 0.349457, acc 0.9375, prec 0.102272, recall 0.778938
2017-12-10T03:06:56.386716: step 6741, loss 0.311749, acc 0.921875, prec 0.102265, recall 0.778938
2017-12-10T03:06:56.659310: step 6742, loss 1.19731, acc 0.90625, prec 0.102257, recall 0.778938
2017-12-10T03:06:56.924072: step 6743, loss 0.610666, acc 0.890625, prec 0.10226, recall 0.77896
2017-12-10T03:06:57.189750: step 6744, loss 0.217913, acc 0.9375, prec 0.102254, recall 0.77896
2017-12-10T03:06:57.456712: step 6745, loss 0.28009, acc 0.9375, prec 0.102261, recall 0.778983
2017-12-10T03:06:57.724400: step 6746, loss 0.320734, acc 0.953125, prec 0.102281, recall 0.779028
2017-12-10T03:06:57.987873: step 6747, loss 0.841461, acc 0.921875, prec 0.102286, recall 0.779051
2017-12-10T03:06:58.249449: step 6748, loss 0.221707, acc 0.953125, prec 0.102294, recall 0.779073
2017-12-10T03:06:58.509345: step 6749, loss 0.54471, acc 0.921875, prec 0.102299, recall 0.779096
2017-12-10T03:06:58.779952: step 6750, loss 0.14999, acc 0.953125, prec 0.102319, recall 0.779141
2017-12-10T03:06:59.046228: step 6751, loss 0.469128, acc 0.96875, prec 0.10234, recall 0.779186
2017-12-10T03:06:59.311413: step 6752, loss 0.389093, acc 0.953125, prec 0.10236, recall 0.779231
2017-12-10T03:06:59.570197: step 6753, loss 0.473047, acc 0.9375, prec 0.102355, recall 0.779231
2017-12-10T03:06:59.832911: step 6754, loss 0.0736682, acc 0.96875, prec 0.102364, recall 0.779254
2017-12-10T03:07:00.111077: step 6755, loss 0.679823, acc 0.921875, prec 0.102381, recall 0.779299
2017-12-10T03:07:00.386512: step 6756, loss 0.431372, acc 0.96875, prec 0.102391, recall 0.779322
2017-12-10T03:07:00.648319: step 6757, loss 0.0292987, acc 1, prec 0.102415, recall 0.779367
2017-12-10T03:07:00.912759: step 6758, loss 0.339093, acc 0.9375, prec 0.102421, recall 0.779389
2017-12-10T03:07:01.177445: step 6759, loss 0.00213709, acc 1, prec 0.102421, recall 0.779389
2017-12-10T03:07:01.447361: step 6760, loss 1.0398, acc 0.9375, prec 0.102452, recall 0.779457
2017-12-10T03:07:01.718415: step 6761, loss 0.101598, acc 0.96875, prec 0.102461, recall 0.779479
2017-12-10T03:07:01.979580: step 6762, loss 0.0682741, acc 0.96875, prec 0.102458, recall 0.779479
2017-12-10T03:07:02.241673: step 6763, loss 0.0208022, acc 0.984375, prec 0.102457, recall 0.779479
2017-12-10T03:07:02.509898: step 6764, loss 0.0301055, acc 0.984375, prec 0.102456, recall 0.779479
2017-12-10T03:07:02.773680: step 6765, loss 0.161012, acc 0.953125, prec 0.102452, recall 0.779479
2017-12-10T03:07:03.040530: step 6766, loss 0.308953, acc 0.96875, prec 0.102473, recall 0.779524
2017-12-10T03:07:03.304237: step 6767, loss 0.33881, acc 0.96875, prec 0.102482, recall 0.779547
2017-12-10T03:07:03.570645: step 6768, loss 4.28289, acc 0.890625, prec 0.102474, recall 0.779467
2017-12-10T03:07:03.839510: step 6769, loss 0.529326, acc 0.921875, prec 0.102479, recall 0.77949
2017-12-10T03:07:04.108638: step 6770, loss 0.0552284, acc 0.984375, prec 0.102478, recall 0.77949
2017-12-10T03:07:04.377113: step 6771, loss 0.57217, acc 0.9375, prec 0.102484, recall 0.779512
2017-12-10T03:07:04.645756: step 6772, loss 0.56445, acc 0.890625, prec 0.102511, recall 0.77958
2017-12-10T03:07:04.913949: step 6773, loss 0.559761, acc 0.890625, prec 0.102513, recall 0.779602
2017-12-10T03:07:05.178214: step 6774, loss 0.383852, acc 0.90625, prec 0.102505, recall 0.779602
2017-12-10T03:07:05.440090: step 6775, loss 0.364024, acc 0.921875, prec 0.102498, recall 0.779602
2017-12-10T03:07:05.703607: step 6776, loss 0.349634, acc 0.921875, prec 0.102515, recall 0.779647
2017-12-10T03:07:05.965638: step 6777, loss 1.18597, acc 0.84375, prec 0.102514, recall 0.77967
2017-12-10T03:07:06.235797: step 6778, loss 0.493905, acc 0.84375, prec 0.1025, recall 0.77967
2017-12-10T03:07:06.502389: step 6779, loss 0.407639, acc 0.921875, prec 0.102493, recall 0.77967
2017-12-10T03:07:06.763072: step 6780, loss 0.226576, acc 0.921875, prec 0.102522, recall 0.779737
2017-12-10T03:07:07.034826: step 6781, loss 0.554867, acc 0.9375, prec 0.102517, recall 0.779737
2017-12-10T03:07:07.297122: step 6782, loss 0.551899, acc 0.90625, prec 0.102545, recall 0.779804
2017-12-10T03:07:07.564244: step 6783, loss 0.88726, acc 0.859375, prec 0.102532, recall 0.779804
2017-12-10T03:07:07.828232: step 6784, loss 0.396199, acc 0.921875, prec 0.102549, recall 0.779849
2017-12-10T03:07:08.096169: step 6785, loss 0.28992, acc 0.9375, prec 0.102568, recall 0.779894
2017-12-10T03:07:08.360636: step 6786, loss 0.198742, acc 0.96875, prec 0.102589, recall 0.779939
2017-12-10T03:07:08.627779: step 6787, loss 0.88108, acc 0.90625, prec 0.102593, recall 0.779961
2017-12-10T03:07:08.900119: step 6788, loss 0.110143, acc 0.953125, prec 0.102613, recall 0.780006
2017-12-10T03:07:09.162667: step 6789, loss 0.582015, acc 0.9375, prec 0.102619, recall 0.780029
2017-12-10T03:07:09.428350: step 6790, loss 0.152092, acc 0.953125, prec 0.102615, recall 0.780029
2017-12-10T03:07:09.691312: step 6791, loss 0.0328859, acc 0.984375, prec 0.102626, recall 0.780051
2017-12-10T03:07:09.959322: step 6792, loss 0.0593089, acc 0.984375, prec 0.102625, recall 0.780051
2017-12-10T03:07:10.223740: step 6793, loss 0.48758, acc 0.9375, prec 0.102655, recall 0.780118
2017-12-10T03:07:10.489307: step 6794, loss 0.0731823, acc 0.953125, prec 0.102651, recall 0.780118
2017-12-10T03:07:10.754860: step 6795, loss 0.281279, acc 0.953125, prec 0.102647, recall 0.780118
2017-12-10T03:07:11.018846: step 6796, loss 0.0035615, acc 1, prec 0.102647, recall 0.780118
2017-12-10T03:07:11.288879: step 6797, loss 0.260495, acc 0.984375, prec 0.102657, recall 0.78014
2017-12-10T03:07:11.561379: step 6798, loss 0.00543725, acc 1, prec 0.102657, recall 0.78014
2017-12-10T03:07:11.829119: step 6799, loss 0.00229285, acc 1, prec 0.102657, recall 0.78014
2017-12-10T03:07:12.095750: step 6800, loss 0.172404, acc 0.984375, prec 0.10268, recall 0.780185
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-6800

2017-12-10T03:07:13.424155: step 6801, loss 0.0793388, acc 0.984375, prec 0.102679, recall 0.780185
2017-12-10T03:07:13.689594: step 6802, loss 0.237159, acc 0.96875, prec 0.102688, recall 0.780208
2017-12-10T03:07:13.955787: step 6803, loss 4.0873, acc 0.953125, prec 0.102697, recall 0.78015
2017-12-10T03:07:14.233829: step 6804, loss 9.03545e-05, acc 1, prec 0.102697, recall 0.78015
2017-12-10T03:07:14.489998: step 6805, loss 0.814471, acc 0.9375, prec 0.102716, recall 0.780195
2017-12-10T03:07:14.757477: step 6806, loss 0.0862671, acc 0.984375, prec 0.102726, recall 0.780218
2017-12-10T03:07:15.029876: step 6807, loss 0.142101, acc 0.96875, prec 0.102736, recall 0.78024
2017-12-10T03:07:15.291816: step 6808, loss 0.138101, acc 0.953125, prec 0.102744, recall 0.780262
2017-12-10T03:07:15.558530: step 6809, loss 0.224081, acc 0.953125, prec 0.102739, recall 0.780262
2017-12-10T03:07:15.825028: step 6810, loss 0.0190821, acc 0.984375, prec 0.102762, recall 0.780307
2017-12-10T03:07:16.094881: step 6811, loss 0.120175, acc 0.953125, prec 0.102782, recall 0.780352
2017-12-10T03:07:16.366063: step 6812, loss 0.864338, acc 0.921875, prec 0.102775, recall 0.780352
2017-12-10T03:07:16.637173: step 6813, loss 0.0956952, acc 0.96875, prec 0.102784, recall 0.780374
2017-12-10T03:07:16.907301: step 6814, loss 0.376827, acc 0.9375, prec 0.102803, recall 0.780418
2017-12-10T03:07:17.172144: step 6815, loss 0.501574, acc 0.875, prec 0.102816, recall 0.780463
2017-12-10T03:07:17.434054: step 6816, loss 0.879505, acc 0.921875, prec 0.102809, recall 0.780463
2017-12-10T03:07:17.697672: step 6817, loss 0.452228, acc 0.921875, prec 0.102826, recall 0.780508
2017-12-10T03:07:17.959510: step 6818, loss 0.52327, acc 0.9375, prec 0.102833, recall 0.78053
2017-12-10T03:07:18.225178: step 6819, loss 0.567494, acc 0.90625, prec 0.102824, recall 0.78053
2017-12-10T03:07:18.492167: step 6820, loss 0.219487, acc 0.96875, prec 0.102834, recall 0.780552
2017-12-10T03:07:18.760243: step 6821, loss 0.316042, acc 0.9375, prec 0.102864, recall 0.780619
2017-12-10T03:07:19.029973: step 6822, loss 0.200582, acc 0.96875, prec 0.102861, recall 0.780619
2017-12-10T03:07:19.294027: step 6823, loss 0.493737, acc 0.953125, prec 0.102857, recall 0.780619
2017-12-10T03:07:19.560530: step 6824, loss 0.805936, acc 0.9375, prec 0.102864, recall 0.780641
2017-12-10T03:07:19.825176: step 6825, loss 0.354856, acc 0.953125, prec 0.10286, recall 0.780641
2017-12-10T03:07:20.088383: step 6826, loss 0.164518, acc 0.953125, prec 0.102867, recall 0.780663
2017-12-10T03:07:20.350211: step 6827, loss 0.0334952, acc 0.984375, prec 0.102866, recall 0.780663
2017-12-10T03:07:20.620013: step 6828, loss 0.46773, acc 0.90625, prec 0.102858, recall 0.780663
2017-12-10T03:07:20.883711: step 6829, loss 0.262011, acc 0.96875, prec 0.102855, recall 0.780663
2017-12-10T03:07:21.145385: step 6830, loss 0.0241206, acc 1, prec 0.102855, recall 0.780663
2017-12-10T03:07:21.409313: step 6831, loss 0.213739, acc 0.953125, prec 0.102851, recall 0.780663
2017-12-10T03:07:21.676117: step 6832, loss 0.191229, acc 0.953125, prec 0.102859, recall 0.780686
2017-12-10T03:07:21.946541: step 6833, loss 0.28139, acc 0.96875, prec 0.10288, recall 0.78073
2017-12-10T03:07:22.213060: step 6834, loss 0.0915004, acc 0.984375, prec 0.102915, recall 0.780797
2017-12-10T03:07:22.474206: step 6835, loss 0.308837, acc 0.953125, prec 0.102923, recall 0.780819
2017-12-10T03:07:22.738035: step 6836, loss 0.102541, acc 0.96875, prec 0.10292, recall 0.780819
2017-12-10T03:07:23.000676: step 6837, loss 0.038448, acc 0.984375, prec 0.102918, recall 0.780819
2017-12-10T03:07:23.265557: step 6838, loss 0.0460715, acc 0.984375, prec 0.102917, recall 0.780819
2017-12-10T03:07:23.532217: step 6839, loss 0.0138351, acc 0.984375, prec 0.102916, recall 0.780819
2017-12-10T03:07:23.795753: step 6840, loss 0.00759779, acc 1, prec 0.102928, recall 0.780841
2017-12-10T03:07:24.063519: step 6841, loss 0.12208, acc 0.984375, prec 0.102926, recall 0.780841
2017-12-10T03:07:24.332302: step 6842, loss 0.161049, acc 0.96875, prec 0.102923, recall 0.780841
2017-12-10T03:07:24.596484: step 6843, loss 0.0223098, acc 0.984375, prec 0.102934, recall 0.780864
2017-12-10T03:07:24.865145: step 6844, loss 0.170025, acc 0.953125, prec 0.10293, recall 0.780864
2017-12-10T03:07:25.131089: step 6845, loss 1.62053, acc 0.96875, prec 0.102929, recall 0.780784
2017-12-10T03:07:25.400373: step 6846, loss 0.0788937, acc 0.96875, prec 0.102926, recall 0.780784
2017-12-10T03:07:25.664426: step 6847, loss 1.55318, acc 0.953125, prec 0.102935, recall 0.780728
2017-12-10T03:07:25.936440: step 6848, loss 0.103028, acc 0.953125, prec 0.102931, recall 0.780728
2017-12-10T03:07:26.208255: step 6849, loss 0.29648, acc 0.921875, prec 0.102924, recall 0.780728
2017-12-10T03:07:26.477681: step 6850, loss 0.206462, acc 1, prec 0.102936, recall 0.78075
2017-12-10T03:07:26.756720: step 6851, loss 0.0560696, acc 0.984375, prec 0.102935, recall 0.78075
2017-12-10T03:07:27.019731: step 6852, loss 0.198108, acc 0.9375, prec 0.102929, recall 0.78075
2017-12-10T03:07:27.283558: step 6853, loss 0.560285, acc 0.921875, prec 0.102922, recall 0.78075
2017-12-10T03:07:27.550097: step 6854, loss 0.252959, acc 0.96875, prec 0.10292, recall 0.78075
2017-12-10T03:07:27.815178: step 6855, loss 0.16721, acc 0.96875, prec 0.102917, recall 0.78075
2017-12-10T03:07:28.085831: step 6856, loss 0.200878, acc 0.9375, prec 0.102911, recall 0.78075
2017-12-10T03:07:28.352459: step 6857, loss 0.075925, acc 0.96875, prec 0.102921, recall 0.780772
2017-12-10T03:07:28.623422: step 6858, loss 0.376865, acc 0.921875, prec 0.102914, recall 0.780772
2017-12-10T03:07:28.884457: step 6859, loss 0.212517, acc 0.96875, prec 0.102923, recall 0.780794
2017-12-10T03:07:29.154589: step 6860, loss 0.0617865, acc 0.96875, prec 0.102932, recall 0.780816
2017-12-10T03:07:29.425026: step 6861, loss 0.211188, acc 0.953125, prec 0.102928, recall 0.780816
2017-12-10T03:07:29.688922: step 6862, loss 0.412763, acc 0.9375, prec 0.102935, recall 0.780839
2017-12-10T03:07:29.953859: step 6863, loss 0.573476, acc 0.921875, prec 0.102952, recall 0.780883
2017-12-10T03:07:30.222425: step 6864, loss 0.236947, acc 0.921875, prec 0.102945, recall 0.780883
2017-12-10T03:07:30.493538: step 6865, loss 0.385115, acc 0.953125, prec 0.102965, recall 0.780927
2017-12-10T03:07:30.758767: step 6866, loss 0.349656, acc 0.953125, prec 0.10296, recall 0.780927
2017-12-10T03:07:31.027696: step 6867, loss 0.599501, acc 0.90625, prec 0.102964, recall 0.780949
2017-12-10T03:07:31.300810: step 6868, loss 0.133579, acc 0.96875, prec 0.102973, recall 0.780972
2017-12-10T03:07:31.562746: step 6869, loss 0.675768, acc 0.890625, prec 0.102964, recall 0.780972
2017-12-10T03:07:31.839781: step 6870, loss 0.317648, acc 0.953125, prec 0.102984, recall 0.781016
2017-12-10T03:07:32.104318: step 6871, loss 0.0759565, acc 0.96875, prec 0.102981, recall 0.781016
2017-12-10T03:07:32.375237: step 6872, loss 0.334805, acc 0.953125, prec 0.102977, recall 0.781016
2017-12-10T03:07:32.643188: step 6873, loss 0.128139, acc 0.984375, prec 0.102987, recall 0.781038
2017-12-10T03:07:32.910339: step 6874, loss 0.0835294, acc 0.984375, prec 0.102998, recall 0.78106
2017-12-10T03:07:33.171089: step 6875, loss 0.000195821, acc 1, prec 0.102998, recall 0.78106
2017-12-10T03:07:33.430018: step 6876, loss 0.240861, acc 0.984375, prec 0.10302, recall 0.781105
2017-12-10T03:07:33.698050: step 6877, loss 0.0800301, acc 0.96875, prec 0.103042, recall 0.781149
2017-12-10T03:07:33.966596: step 6878, loss 0.0585663, acc 1, prec 0.103054, recall 0.781171
2017-12-10T03:07:34.239326: step 6879, loss 0.220535, acc 0.953125, prec 0.103073, recall 0.781215
2017-12-10T03:07:34.499317: step 6880, loss 0.00650917, acc 1, prec 0.103085, recall 0.781237
2017-12-10T03:07:34.764666: step 6881, loss 0.0644216, acc 0.984375, prec 0.103108, recall 0.781282
2017-12-10T03:07:35.028937: step 6882, loss 1.8719, acc 0.953125, prec 0.103129, recall 0.781247
2017-12-10T03:07:35.299798: step 6883, loss 0.141715, acc 0.96875, prec 0.103126, recall 0.781247
2017-12-10T03:07:35.574000: step 6884, loss 0.0217239, acc 0.984375, prec 0.103125, recall 0.781247
2017-12-10T03:07:35.851126: step 6885, loss 0.198232, acc 0.96875, prec 0.103134, recall 0.781269
2017-12-10T03:07:36.112385: step 6886, loss 0.022793, acc 0.984375, prec 0.103133, recall 0.781269
2017-12-10T03:07:36.376563: step 6887, loss 0.0994704, acc 0.984375, prec 0.103143, recall 0.781291
2017-12-10T03:07:36.646993: step 6888, loss 0.130574, acc 0.984375, prec 0.103142, recall 0.781291
2017-12-10T03:07:36.914486: step 6889, loss 0.269187, acc 0.9375, prec 0.103148, recall 0.781313
2017-12-10T03:07:37.180395: step 6890, loss 0.440777, acc 0.953125, prec 0.103144, recall 0.781313
2017-12-10T03:07:37.454271: step 6891, loss 0.432737, acc 0.96875, prec 0.103154, recall 0.781335
2017-12-10T03:07:37.720334: step 6892, loss 0.295868, acc 0.96875, prec 0.103163, recall 0.781357
2017-12-10T03:07:37.988196: step 6893, loss 0.255258, acc 0.953125, prec 0.103171, recall 0.781379
2017-12-10T03:07:38.258841: step 6894, loss 0.0727441, acc 0.953125, prec 0.10319, recall 0.781424
2017-12-10T03:07:38.525459: step 6895, loss 0.897343, acc 0.9375, prec 0.103197, recall 0.781446
2017-12-10T03:07:38.795803: step 6896, loss 0.0704619, acc 0.984375, prec 0.103207, recall 0.781468
2017-12-10T03:07:39.060251: step 6897, loss 0.348663, acc 0.921875, prec 0.103212, recall 0.78149
2017-12-10T03:07:39.323583: step 6898, loss 0.100056, acc 0.96875, prec 0.103222, recall 0.781512
2017-12-10T03:07:39.586704: step 6899, loss 0.248208, acc 0.921875, prec 0.103215, recall 0.781512
2017-12-10T03:07:39.849592: step 6900, loss 0.124148, acc 0.953125, prec 0.103247, recall 0.781578

Evaluation:
2017-12-10T03:07:47.463890: step 6900, loss 5.79572, acc 0.927257, prec 0.103362, recall 0.777789

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-6900

2017-12-10T03:07:49.073235: step 6901, loss 0.488305, acc 0.875, prec 0.103375, recall 0.777833
2017-12-10T03:07:49.340989: step 6902, loss 0.288379, acc 0.96875, prec 0.103396, recall 0.777877
2017-12-10T03:07:49.609525: step 6903, loss 0.141979, acc 0.953125, prec 0.103391, recall 0.777877
2017-12-10T03:07:49.872661: step 6904, loss 0.764455, acc 0.921875, prec 0.103385, recall 0.777877
2017-12-10T03:07:50.141059: step 6905, loss 0.27705, acc 0.9375, prec 0.103379, recall 0.777877
2017-12-10T03:07:50.411605: step 6906, loss 0.572564, acc 0.9375, prec 0.103397, recall 0.777921
2017-12-10T03:07:50.685234: step 6907, loss 0.0274641, acc 0.984375, prec 0.103443, recall 0.778009
2017-12-10T03:07:50.948795: step 6908, loss 0.111762, acc 0.96875, prec 0.103441, recall 0.778009
2017-12-10T03:07:51.218522: step 6909, loss 0.140429, acc 0.953125, prec 0.103448, recall 0.778031
2017-12-10T03:07:51.486640: step 6910, loss 0.319605, acc 0.90625, prec 0.103464, recall 0.778075
2017-12-10T03:07:51.751020: step 6911, loss 0.217723, acc 0.953125, prec 0.10346, recall 0.778075
2017-12-10T03:07:52.016495: step 6912, loss 0.153572, acc 0.96875, prec 0.103457, recall 0.778075
2017-12-10T03:07:52.283291: step 6913, loss 0.470805, acc 0.9375, prec 0.103451, recall 0.778075
2017-12-10T03:07:52.545094: step 6914, loss 0.178619, acc 0.96875, prec 0.103461, recall 0.778097
2017-12-10T03:07:52.808043: step 6915, loss 0.237662, acc 0.984375, prec 0.103459, recall 0.778097
2017-12-10T03:07:53.078449: step 6916, loss 0.336143, acc 0.953125, prec 0.103479, recall 0.778141
2017-12-10T03:07:53.340968: step 6917, loss 0.414913, acc 0.96875, prec 0.103476, recall 0.778141
2017-12-10T03:07:53.618537: step 6918, loss 0.0726694, acc 0.984375, prec 0.103475, recall 0.778141
2017-12-10T03:07:53.891381: step 6919, loss 0.0483676, acc 0.984375, prec 0.103473, recall 0.778141
2017-12-10T03:07:54.154983: step 6920, loss 0.100102, acc 0.984375, prec 0.103472, recall 0.778141
2017-12-10T03:07:54.428120: step 6921, loss 0.000479686, acc 1, prec 0.103472, recall 0.778141
2017-12-10T03:07:54.692678: step 6922, loss 0.372083, acc 0.9375, prec 0.103466, recall 0.778141
2017-12-10T03:07:54.952709: step 6923, loss 1.22485, acc 0.96875, prec 0.103477, recall 0.778086
2017-12-10T03:07:55.219259: step 6924, loss 0.537227, acc 0.953125, prec 0.103473, recall 0.778086
2017-12-10T03:07:55.483655: step 6925, loss 0.240718, acc 0.9375, prec 0.103479, recall 0.778108
2017-12-10T03:07:55.745137: step 6926, loss 0.0522661, acc 0.984375, prec 0.103501, recall 0.778152
2017-12-10T03:07:56.013587: step 6927, loss 0.262272, acc 0.953125, prec 0.103497, recall 0.778152
2017-12-10T03:07:56.282141: step 6928, loss 0.0245752, acc 0.984375, prec 0.103508, recall 0.778174
2017-12-10T03:07:56.561622: step 6929, loss 0.363845, acc 0.9375, prec 0.103514, recall 0.778196
2017-12-10T03:07:56.827410: step 6930, loss 0.136081, acc 0.96875, prec 0.103523, recall 0.778218
2017-12-10T03:07:57.091265: step 6931, loss 0.312677, acc 0.953125, prec 0.103519, recall 0.778218
2017-12-10T03:07:57.352454: step 6932, loss 0.190622, acc 0.953125, prec 0.103515, recall 0.778218
2017-12-10T03:07:57.624183: step 6933, loss 0.442914, acc 0.96875, prec 0.10356, recall 0.778306
2017-12-10T03:07:57.891782: step 6934, loss 0.0345837, acc 0.984375, prec 0.10357, recall 0.778328
2017-12-10T03:07:58.159104: step 6935, loss 1.57738, acc 0.96875, prec 0.103592, recall 0.778294
2017-12-10T03:07:58.429321: step 6936, loss 0.042991, acc 0.984375, prec 0.103603, recall 0.778316
2017-12-10T03:07:58.701796: step 6937, loss 0.114886, acc 0.984375, prec 0.103613, recall 0.778338
2017-12-10T03:07:58.976112: step 6938, loss 0.00792929, acc 1, prec 0.103625, recall 0.77836
2017-12-10T03:07:59.239983: step 6939, loss 0.249753, acc 0.953125, prec 0.103633, recall 0.778382
2017-12-10T03:07:59.504983: step 6940, loss 0.347833, acc 0.953125, prec 0.103629, recall 0.778382
2017-12-10T03:07:59.767279: step 6941, loss 0.0650653, acc 0.953125, prec 0.103636, recall 0.778404
2017-12-10T03:08:00.039956: step 6942, loss 0.0242909, acc 0.984375, prec 0.103635, recall 0.778404
2017-12-10T03:08:00.315263: step 6943, loss 0.196995, acc 0.953125, prec 0.103654, recall 0.778448
2017-12-10T03:08:00.584190: step 6944, loss 0.301048, acc 0.9375, prec 0.103661, recall 0.77847
2017-12-10T03:08:00.849210: step 6945, loss 0.0955702, acc 0.9375, prec 0.103655, recall 0.77847
2017-12-10T03:08:01.114600: step 6946, loss 0.36491, acc 0.921875, prec 0.10366, recall 0.778492
2017-12-10T03:08:01.379315: step 6947, loss 0.118446, acc 0.96875, prec 0.103669, recall 0.778514
2017-12-10T03:08:01.657654: step 6948, loss 0.0770909, acc 0.984375, prec 0.10368, recall 0.778535
2017-12-10T03:08:01.920668: step 6949, loss 0.581299, acc 0.921875, prec 0.103673, recall 0.778535
2017-12-10T03:08:02.183614: step 6950, loss 0.46661, acc 0.921875, prec 0.103666, recall 0.778535
2017-12-10T03:08:02.447060: step 6951, loss 0.0944112, acc 0.96875, prec 0.103675, recall 0.778557
2017-12-10T03:08:02.718501: step 6952, loss 0.440177, acc 0.921875, prec 0.103692, recall 0.778601
2017-12-10T03:08:02.990377: step 6953, loss 0.368333, acc 0.9375, prec 0.10371, recall 0.778645
2017-12-10T03:08:03.252280: step 6954, loss 0.548828, acc 0.90625, prec 0.103702, recall 0.778645
2017-12-10T03:08:03.518636: step 6955, loss 0.018983, acc 1, prec 0.103725, recall 0.778689
2017-12-10T03:08:03.779811: step 6956, loss 0.140887, acc 0.921875, prec 0.103719, recall 0.778689
2017-12-10T03:08:04.052232: step 6957, loss 0.145256, acc 0.953125, prec 0.103726, recall 0.77871
2017-12-10T03:08:04.277188: step 6958, loss 0.000489773, acc 1, prec 0.103738, recall 0.778732
2017-12-10T03:08:04.547824: step 6959, loss 0.58907, acc 0.921875, prec 0.103743, recall 0.778754
2017-12-10T03:08:04.810393: step 6960, loss 0.0948122, acc 0.984375, prec 0.103765, recall 0.778798
2017-12-10T03:08:05.072506: step 6961, loss 0.0723818, acc 0.96875, prec 0.103786, recall 0.778841
2017-12-10T03:08:05.342836: step 6962, loss 0.00462521, acc 1, prec 0.10381, recall 0.778885
2017-12-10T03:08:05.607034: step 6963, loss 0.230448, acc 0.984375, prec 0.103832, recall 0.778929
2017-12-10T03:08:05.883111: step 6964, loss 0.121917, acc 0.953125, prec 0.103828, recall 0.778929
2017-12-10T03:08:06.148766: step 6965, loss 0.194043, acc 0.984375, prec 0.103826, recall 0.778929
2017-12-10T03:08:06.417211: step 6966, loss 0.138374, acc 0.984375, prec 0.103849, recall 0.778972
2017-12-10T03:08:06.679402: step 6967, loss 0.144877, acc 0.953125, prec 0.103856, recall 0.778994
2017-12-10T03:08:06.941239: step 6968, loss 0.0361302, acc 0.984375, prec 0.103867, recall 0.779016
2017-12-10T03:08:07.207496: step 6969, loss 0.011071, acc 1, prec 0.103879, recall 0.779038
2017-12-10T03:08:07.476170: step 6970, loss 0.0188307, acc 0.984375, prec 0.103877, recall 0.779038
2017-12-10T03:08:07.736872: step 6971, loss 0.0423373, acc 0.984375, prec 0.103876, recall 0.779038
2017-12-10T03:08:08.000575: step 6972, loss 0.0284039, acc 1, prec 0.103923, recall 0.779125
2017-12-10T03:08:08.271370: step 6973, loss 0.000649468, acc 1, prec 0.103935, recall 0.779147
2017-12-10T03:08:08.529598: step 6974, loss 0.0113278, acc 1, prec 0.103935, recall 0.779147
2017-12-10T03:08:08.789883: step 6975, loss 2.71231, acc 0.96875, prec 0.103945, recall 0.779092
2017-12-10T03:08:09.059540: step 6976, loss 0.300538, acc 0.96875, prec 0.103954, recall 0.779113
2017-12-10T03:08:09.322462: step 6977, loss 0.15371, acc 0.984375, prec 0.103953, recall 0.779113
2017-12-10T03:08:09.586958: step 6978, loss 0.022744, acc 0.984375, prec 0.103951, recall 0.779113
2017-12-10T03:08:09.846798: step 6979, loss 0.0135771, acc 1, prec 0.103975, recall 0.779157
2017-12-10T03:08:10.113357: step 6980, loss 0.0573774, acc 0.984375, prec 0.103974, recall 0.779157
2017-12-10T03:08:10.380019: step 6981, loss 0.173551, acc 0.953125, prec 0.103993, recall 0.7792
2017-12-10T03:08:10.650503: step 6982, loss 0.00502435, acc 1, prec 0.103993, recall 0.7792
2017-12-10T03:08:10.919306: step 6983, loss 0.16438, acc 0.953125, prec 0.104001, recall 0.779222
2017-12-10T03:08:11.184710: step 6984, loss 0.132015, acc 0.953125, prec 0.103997, recall 0.779222
2017-12-10T03:08:11.452929: step 6985, loss 0.141596, acc 0.953125, prec 0.103993, recall 0.779222
2017-12-10T03:08:11.733848: step 6986, loss 0.0686802, acc 0.96875, prec 0.104002, recall 0.779244
2017-12-10T03:08:12.008660: step 6987, loss 0.0391869, acc 0.96875, prec 0.104011, recall 0.779266
2017-12-10T03:08:12.275392: step 6988, loss 0.434358, acc 0.890625, prec 0.104001, recall 0.779266
2017-12-10T03:08:12.543682: step 6989, loss 0.193236, acc 0.953125, prec 0.10402, recall 0.779309
2017-12-10T03:08:12.809008: step 6990, loss 0.139738, acc 0.96875, prec 0.10403, recall 0.779331
2017-12-10T03:08:13.073738: step 6991, loss 0.497263, acc 0.953125, prec 0.104061, recall 0.779396
2017-12-10T03:08:13.335028: step 6992, loss 0.219926, acc 0.9375, prec 0.104055, recall 0.779396
2017-12-10T03:08:13.599501: step 6993, loss 0.00407218, acc 1, prec 0.104055, recall 0.779396
2017-12-10T03:08:13.863472: step 6994, loss 0.367466, acc 0.921875, prec 0.104084, recall 0.779461
2017-12-10T03:08:14.129289: step 6995, loss 0.198236, acc 0.96875, prec 0.104081, recall 0.779461
2017-12-10T03:08:14.393935: step 6996, loss 0.248003, acc 0.953125, prec 0.104112, recall 0.779526
2017-12-10T03:08:14.665413: step 6997, loss 0.357265, acc 0.9375, prec 0.104107, recall 0.779526
2017-12-10T03:08:14.937706: step 6998, loss 0.0622482, acc 0.984375, prec 0.104117, recall 0.779548
2017-12-10T03:08:15.210929: step 6999, loss 0.0293519, acc 0.96875, prec 0.104126, recall 0.779569
2017-12-10T03:08:15.477710: step 7000, loss 0.0326088, acc 0.984375, prec 0.104137, recall 0.779591
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-7000

2017-12-10T03:08:17.003324: step 7001, loss 0.334163, acc 0.953125, prec 0.104168, recall 0.779656
2017-12-10T03:08:17.275545: step 7002, loss 0.00138084, acc 1, prec 0.10418, recall 0.779678
2017-12-10T03:08:17.538626: step 7003, loss 0.0603396, acc 0.96875, prec 0.104189, recall 0.779699
2017-12-10T03:08:17.806450: step 7004, loss 0.575176, acc 0.984375, prec 0.104187, recall 0.779699
2017-12-10T03:08:18.072366: step 7005, loss 0.00681203, acc 1, prec 0.104187, recall 0.779699
2017-12-10T03:08:18.338620: step 7006, loss 0.00430598, acc 1, prec 0.104199, recall 0.779721
2017-12-10T03:08:18.599364: step 7007, loss 0.226642, acc 0.984375, prec 0.104209, recall 0.779743
2017-12-10T03:08:18.869864: step 7008, loss 0.0994578, acc 0.984375, prec 0.104231, recall 0.779786
2017-12-10T03:08:19.134448: step 7009, loss 0.0330601, acc 0.984375, prec 0.10423, recall 0.779786
2017-12-10T03:08:19.405524: step 7010, loss 0.00170795, acc 1, prec 0.10423, recall 0.779786
2017-12-10T03:08:19.668347: step 7011, loss 0.58026, acc 0.953125, prec 0.104226, recall 0.779786
2017-12-10T03:08:19.931087: step 7012, loss 0.179163, acc 0.953125, prec 0.104222, recall 0.779786
2017-12-10T03:08:20.201507: step 7013, loss 0.264704, acc 0.9375, prec 0.104216, recall 0.779786
2017-12-10T03:08:20.466542: step 7014, loss 0.013187, acc 1, prec 0.10424, recall 0.779829
2017-12-10T03:08:20.732060: step 7015, loss 1.85509, acc 0.984375, prec 0.10424, recall 0.779753
2017-12-10T03:08:21.006069: step 7016, loss 0.0185305, acc 0.984375, prec 0.104274, recall 0.779817
2017-12-10T03:08:21.279364: step 7017, loss 0.626264, acc 0.921875, prec 0.104267, recall 0.779817
2017-12-10T03:08:21.549582: step 7018, loss 0.0166998, acc 0.984375, prec 0.104266, recall 0.779817
2017-12-10T03:08:21.818618: step 7019, loss 0.797977, acc 0.890625, prec 0.104268, recall 0.779839
2017-12-10T03:08:22.090119: step 7020, loss 0.0143817, acc 1, prec 0.104315, recall 0.779925
2017-12-10T03:08:22.369490: step 7021, loss 0.0628336, acc 0.984375, prec 0.104313, recall 0.779925
2017-12-10T03:08:22.636760: step 7022, loss 0.36776, acc 0.9375, prec 0.104343, recall 0.77999
2017-12-10T03:08:22.906652: step 7023, loss 0.226482, acc 0.96875, prec 0.104341, recall 0.77999
2017-12-10T03:08:23.181977: step 7024, loss 0.211501, acc 0.9375, prec 0.104347, recall 0.780012
2017-12-10T03:08:23.450735: step 7025, loss 0.267082, acc 0.9375, prec 0.104353, recall 0.780033
2017-12-10T03:08:23.726005: step 7026, loss 0.348199, acc 0.9375, prec 0.104359, recall 0.780055
2017-12-10T03:08:23.990373: step 7027, loss 0.32446, acc 0.984375, prec 0.104393, recall 0.78012
2017-12-10T03:08:24.254773: step 7028, loss 0.0931729, acc 1, prec 0.10444, recall 0.780206
2017-12-10T03:08:24.517398: step 7029, loss 0.175926, acc 0.953125, prec 0.10446, recall 0.780249
2017-12-10T03:08:24.786033: step 7030, loss 0.121031, acc 0.953125, prec 0.104479, recall 0.780292
2017-12-10T03:08:25.050829: step 7031, loss 0.279728, acc 0.921875, prec 0.104507, recall 0.780356
2017-12-10T03:08:25.313153: step 7032, loss 0.171544, acc 0.96875, prec 0.104516, recall 0.780378
2017-12-10T03:08:25.574375: step 7033, loss 0.686775, acc 0.890625, prec 0.104507, recall 0.780378
2017-12-10T03:08:25.837212: step 7034, loss 0.516679, acc 0.96875, prec 0.104528, recall 0.780421
2017-12-10T03:08:26.104670: step 7035, loss 0.0827023, acc 0.96875, prec 0.104548, recall 0.780464
2017-12-10T03:08:26.372588: step 7036, loss 0.123823, acc 0.96875, prec 0.104546, recall 0.780464
2017-12-10T03:08:26.653191: step 7037, loss 0.285572, acc 0.96875, prec 0.104555, recall 0.780485
2017-12-10T03:08:26.918844: step 7038, loss 1.31888, acc 0.84375, prec 0.104553, recall 0.780507
2017-12-10T03:08:27.186989: step 7039, loss 0.12772, acc 0.96875, prec 0.104562, recall 0.780528
2017-12-10T03:08:27.453488: step 7040, loss 0.390698, acc 0.953125, prec 0.104581, recall 0.780571
2017-12-10T03:08:27.722225: step 7041, loss 0.123511, acc 0.984375, prec 0.10458, recall 0.780571
2017-12-10T03:08:27.982310: step 7042, loss 0.241903, acc 0.953125, prec 0.104587, recall 0.780593
2017-12-10T03:08:28.252869: step 7043, loss 0.112848, acc 0.953125, prec 0.104583, recall 0.780593
2017-12-10T03:08:28.522017: step 7044, loss 0.0020113, acc 1, prec 0.104583, recall 0.780593
2017-12-10T03:08:28.783211: step 7045, loss 0.0282696, acc 0.984375, prec 0.104593, recall 0.780614
2017-12-10T03:08:29.048808: step 7046, loss 0.0330566, acc 0.984375, prec 0.104616, recall 0.780657
2017-12-10T03:08:29.317476: step 7047, loss 0.143153, acc 0.984375, prec 0.104626, recall 0.780679
2017-12-10T03:08:29.581872: step 7048, loss 0.138128, acc 0.984375, prec 0.104624, recall 0.780679
2017-12-10T03:08:29.848563: step 7049, loss 0.10869, acc 0.96875, prec 0.104633, recall 0.7807
2017-12-10T03:08:30.123783: step 7050, loss 0.355484, acc 0.96875, prec 0.104642, recall 0.780721
2017-12-10T03:08:30.392804: step 7051, loss 0.0578112, acc 0.984375, prec 0.104653, recall 0.780743
2017-12-10T03:08:30.660228: step 7052, loss 0.00380856, acc 1, prec 0.104665, recall 0.780764
2017-12-10T03:08:30.925102: step 7053, loss 0.0988873, acc 0.953125, prec 0.104684, recall 0.780807
2017-12-10T03:08:31.192468: step 7054, loss 1.4572e-05, acc 1, prec 0.104684, recall 0.780807
2017-12-10T03:08:31.457414: step 7055, loss 3.77878, acc 0.984375, prec 0.104684, recall 0.780731
2017-12-10T03:08:31.736152: step 7056, loss 0.299865, acc 0.96875, prec 0.104693, recall 0.780752
2017-12-10T03:08:32.002127: step 7057, loss 0.236418, acc 0.953125, prec 0.104689, recall 0.780752
2017-12-10T03:08:32.265860: step 7058, loss 0.260122, acc 0.96875, prec 0.104698, recall 0.780774
2017-12-10T03:08:32.531438: step 7059, loss 0.00497778, acc 1, prec 0.104721, recall 0.780817
2017-12-10T03:08:32.804776: step 7060, loss 0.115325, acc 0.96875, prec 0.104718, recall 0.780817
2017-12-10T03:08:33.065913: step 7061, loss 1.10645, acc 0.84375, prec 0.104705, recall 0.780817
2017-12-10T03:08:33.333634: step 7062, loss 0.29254, acc 0.953125, prec 0.104701, recall 0.780817
2017-12-10T03:08:33.605879: step 7063, loss 0.393873, acc 0.90625, prec 0.104716, recall 0.780859
2017-12-10T03:08:33.879943: step 7064, loss 0.734813, acc 0.90625, prec 0.104719, recall 0.780881
2017-12-10T03:08:34.154613: step 7065, loss 0.295794, acc 0.9375, prec 0.104726, recall 0.780902
2017-12-10T03:08:34.427902: step 7066, loss 0.373888, acc 0.953125, prec 0.104733, recall 0.780924
2017-12-10T03:08:34.698559: step 7067, loss 0.57911, acc 0.90625, prec 0.104748, recall 0.780966
2017-12-10T03:08:34.981476: step 7068, loss 0.0647547, acc 0.953125, prec 0.104744, recall 0.780966
2017-12-10T03:08:35.252936: step 7069, loss 0.456555, acc 0.9375, prec 0.104739, recall 0.780966
2017-12-10T03:08:35.516933: step 7070, loss 0.413309, acc 0.96875, prec 0.104736, recall 0.780966
2017-12-10T03:08:35.784032: step 7071, loss 0.117465, acc 0.953125, prec 0.104732, recall 0.780966
2017-12-10T03:08:36.047711: step 7072, loss 0.142856, acc 0.953125, prec 0.104728, recall 0.780966
2017-12-10T03:08:36.319284: step 7073, loss 0.0597254, acc 0.96875, prec 0.104725, recall 0.780966
2017-12-10T03:08:36.586311: step 7074, loss 0.127681, acc 0.96875, prec 0.104722, recall 0.780966
2017-12-10T03:08:36.851605: step 7075, loss 0.208145, acc 0.953125, prec 0.10473, recall 0.780988
2017-12-10T03:08:37.117089: step 7076, loss 0.351133, acc 0.921875, prec 0.104723, recall 0.780988
2017-12-10T03:08:37.385801: step 7077, loss 0.117112, acc 0.96875, prec 0.10472, recall 0.780988
2017-12-10T03:08:37.652172: step 7078, loss 0.141391, acc 0.953125, prec 0.104716, recall 0.780988
2017-12-10T03:08:37.913914: step 7079, loss 0.346678, acc 0.9375, prec 0.104711, recall 0.780988
2017-12-10T03:08:38.179235: step 7080, loss 0.0561203, acc 0.984375, prec 0.104709, recall 0.780988
2017-12-10T03:08:38.444670: step 7081, loss 0.00144103, acc 1, prec 0.104709, recall 0.780988
2017-12-10T03:08:38.712314: step 7082, loss 0.257223, acc 0.953125, prec 0.104717, recall 0.781009
2017-12-10T03:08:38.974552: step 7083, loss 0.189939, acc 0.953125, prec 0.104713, recall 0.781009
2017-12-10T03:08:39.240435: step 7084, loss 0.877988, acc 0.9375, prec 0.104719, recall 0.78103
2017-12-10T03:08:39.505220: step 7085, loss 0.234787, acc 0.96875, prec 0.104728, recall 0.781052
2017-12-10T03:08:39.770685: step 7086, loss 0.261889, acc 0.96875, prec 0.104737, recall 0.781073
2017-12-10T03:08:40.047307: step 7087, loss 0.916266, acc 0.9375, prec 0.104743, recall 0.781095
2017-12-10T03:08:40.313231: step 7088, loss 0.0677345, acc 0.984375, prec 0.104742, recall 0.781095
2017-12-10T03:08:40.577138: step 7089, loss 0.17798, acc 0.953125, prec 0.10475, recall 0.781116
2017-12-10T03:08:40.839990: step 7090, loss 0.000991501, acc 1, prec 0.10475, recall 0.781116
2017-12-10T03:08:41.103516: step 7091, loss 0.0133105, acc 0.984375, prec 0.104748, recall 0.781116
2017-12-10T03:08:41.369887: step 7092, loss 0.258394, acc 0.96875, prec 0.104757, recall 0.781137
2017-12-10T03:08:41.647811: step 7093, loss 0.293083, acc 0.953125, prec 0.104753, recall 0.781137
2017-12-10T03:08:41.914848: step 7094, loss 0.014953, acc 1, prec 0.104765, recall 0.781159
2017-12-10T03:08:42.181958: step 7095, loss 0.00402441, acc 1, prec 0.104776, recall 0.78118
2017-12-10T03:08:42.447796: step 7096, loss 4.55234, acc 0.953125, prec 0.104787, recall 0.781049
2017-12-10T03:08:42.714271: step 7097, loss 0.0687677, acc 0.96875, prec 0.104784, recall 0.781049
2017-12-10T03:08:42.976327: step 7098, loss 0.0654985, acc 0.96875, prec 0.104793, recall 0.78107
2017-12-10T03:08:43.243254: step 7099, loss 0.292724, acc 0.953125, prec 0.104789, recall 0.78107
2017-12-10T03:08:43.511225: step 7100, loss 0.226545, acc 0.984375, prec 0.104811, recall 0.781113
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-7100

2017-12-10T03:08:44.803816: step 7101, loss 0.2705, acc 0.90625, prec 0.104803, recall 0.781113
2017-12-10T03:08:45.071039: step 7102, loss 2.31942, acc 0.765625, prec 0.104782, recall 0.781113
2017-12-10T03:08:45.336893: step 7103, loss 0.724997, acc 0.875, prec 0.104783, recall 0.781134
2017-12-10T03:08:45.600978: step 7104, loss 0.445217, acc 0.90625, prec 0.104786, recall 0.781156
2017-12-10T03:08:45.867292: step 7105, loss 0.999196, acc 0.828125, prec 0.104783, recall 0.781177
2017-12-10T03:08:46.130347: step 7106, loss 1.23239, acc 0.796875, prec 0.104765, recall 0.781177
2017-12-10T03:08:46.398093: step 7107, loss 0.686466, acc 0.890625, prec 0.104756, recall 0.781177
2017-12-10T03:08:46.674702: step 7108, loss 1.72557, acc 0.765625, prec 0.104747, recall 0.781198
2017-12-10T03:08:46.937071: step 7109, loss 0.984837, acc 0.828125, prec 0.104743, recall 0.78122
2017-12-10T03:08:47.201831: step 7110, loss 1.22483, acc 0.828125, prec 0.104728, recall 0.78122
2017-12-10T03:08:47.472109: step 7111, loss 0.863787, acc 0.859375, prec 0.104716, recall 0.78122
2017-12-10T03:08:47.750209: step 7112, loss 1.33861, acc 0.84375, prec 0.104702, recall 0.78122
2017-12-10T03:08:48.018630: step 7113, loss 1.05105, acc 0.859375, prec 0.10469, recall 0.78122
2017-12-10T03:08:48.285855: step 7114, loss 0.260344, acc 0.921875, prec 0.104695, recall 0.781241
2017-12-10T03:08:48.557249: step 7115, loss 1.36854, acc 0.84375, prec 0.104705, recall 0.781283
2017-12-10T03:08:48.824995: step 7116, loss 1.06055, acc 0.875, prec 0.104717, recall 0.781326
2017-12-10T03:08:49.089814: step 7117, loss 0.293747, acc 0.921875, prec 0.104722, recall 0.781347
2017-12-10T03:08:49.360295: step 7118, loss 0.643089, acc 0.890625, prec 0.104712, recall 0.781347
2017-12-10T03:08:49.626736: step 7119, loss 0.55245, acc 0.953125, prec 0.104708, recall 0.781347
2017-12-10T03:08:49.893384: step 7120, loss 0.215858, acc 0.953125, prec 0.104704, recall 0.781347
2017-12-10T03:08:50.156879: step 7121, loss 0.229902, acc 0.96875, prec 0.104702, recall 0.781347
2017-12-10T03:08:50.429619: step 7122, loss 0.189147, acc 0.96875, prec 0.104699, recall 0.781347
2017-12-10T03:08:50.693758: step 7123, loss 0.0766429, acc 0.984375, prec 0.104697, recall 0.781347
2017-12-10T03:08:50.965792: step 7124, loss 0.249951, acc 0.96875, prec 0.104695, recall 0.781347
2017-12-10T03:08:51.224488: step 7125, loss 0.142142, acc 0.953125, prec 0.104702, recall 0.781369
2017-12-10T03:08:51.492532: step 7126, loss 0.365169, acc 0.96875, prec 0.104723, recall 0.781411
2017-12-10T03:08:51.757519: step 7127, loss 0.0319766, acc 1, prec 0.104746, recall 0.781454
2017-12-10T03:08:52.019604: step 7128, loss 0.440509, acc 0.96875, prec 0.104755, recall 0.781475
2017-12-10T03:08:52.283340: step 7129, loss 0.294746, acc 0.984375, prec 0.104766, recall 0.781496
2017-12-10T03:08:52.547229: step 7130, loss 0.0416531, acc 0.984375, prec 0.104764, recall 0.781496
2017-12-10T03:08:52.809704: step 7131, loss 0.41259, acc 1, prec 0.104787, recall 0.781539
2017-12-10T03:08:53.076420: step 7132, loss 0.333188, acc 0.984375, prec 0.104821, recall 0.781603
2017-12-10T03:08:53.342190: step 7133, loss 0.087795, acc 0.984375, prec 0.10482, recall 0.781603
2017-12-10T03:08:53.608902: step 7134, loss 0.165264, acc 0.96875, prec 0.104829, recall 0.781624
2017-12-10T03:08:53.881695: step 7135, loss 0.237323, acc 0.953125, prec 0.104825, recall 0.781624
2017-12-10T03:08:54.143440: step 7136, loss 0.0606773, acc 0.984375, prec 0.104823, recall 0.781624
2017-12-10T03:08:54.404462: step 7137, loss 0.328016, acc 0.9375, prec 0.104818, recall 0.781624
2017-12-10T03:08:54.670809: step 7138, loss 0.000942496, acc 1, prec 0.104818, recall 0.781624
2017-12-10T03:08:54.931052: step 7139, loss 0.00355096, acc 1, prec 0.104829, recall 0.781645
2017-12-10T03:08:55.196540: step 7140, loss 0.0140524, acc 0.984375, prec 0.104828, recall 0.781645
2017-12-10T03:08:55.459559: step 7141, loss 0.00247328, acc 1, prec 0.10484, recall 0.781666
2017-12-10T03:08:55.728501: step 7142, loss 0.0301308, acc 0.984375, prec 0.104838, recall 0.781666
2017-12-10T03:08:55.995302: step 7143, loss 0.0011415, acc 1, prec 0.104838, recall 0.781666
2017-12-10T03:08:56.264702: step 7144, loss 0.89245, acc 1, prec 0.10485, recall 0.781687
2017-12-10T03:08:56.540630: step 7145, loss 0.0500135, acc 1, prec 0.104862, recall 0.781709
2017-12-10T03:08:56.810711: step 7146, loss 0.00395933, acc 1, prec 0.104862, recall 0.781709
2017-12-10T03:08:57.078859: step 7147, loss 3.81612, acc 0.984375, prec 0.104873, recall 0.781654
2017-12-10T03:08:57.351439: step 7148, loss 0.352584, acc 0.953125, prec 0.104893, recall 0.781696
2017-12-10T03:08:57.615709: step 7149, loss 0.107352, acc 0.96875, prec 0.10489, recall 0.781696
2017-12-10T03:08:57.883262: step 7150, loss 0.587482, acc 0.96875, prec 0.104899, recall 0.781717
2017-12-10T03:08:58.152393: step 7151, loss 0.174738, acc 0.984375, prec 0.104897, recall 0.781717
2017-12-10T03:08:58.418760: step 7152, loss 0.108517, acc 0.96875, prec 0.104906, recall 0.781739
2017-12-10T03:08:58.689324: step 7153, loss 0.134739, acc 0.9375, prec 0.104901, recall 0.781739
2017-12-10T03:08:58.962213: step 7154, loss 0.569327, acc 0.890625, prec 0.104903, recall 0.78176
2017-12-10T03:08:59.236985: step 7155, loss 0.0999343, acc 0.96875, prec 0.1049, recall 0.78176
2017-12-10T03:08:59.505272: step 7156, loss 0.253013, acc 0.9375, prec 0.104895, recall 0.78176
2017-12-10T03:08:59.776074: step 7157, loss 0.420106, acc 0.9375, prec 0.104924, recall 0.781823
2017-12-10T03:09:00.056114: step 7158, loss 0.725295, acc 0.875, prec 0.104913, recall 0.781823
2017-12-10T03:09:00.327324: step 7159, loss 0.476754, acc 0.90625, prec 0.104917, recall 0.781845
2017-12-10T03:09:00.594199: step 7160, loss 1.38126, acc 0.84375, prec 0.104915, recall 0.781866
2017-12-10T03:09:00.864601: step 7161, loss 0.672155, acc 0.890625, prec 0.104905, recall 0.781866
2017-12-10T03:09:01.137745: step 7162, loss 0.194694, acc 0.953125, prec 0.104925, recall 0.781908
2017-12-10T03:09:01.411250: step 7163, loss 0.480708, acc 0.9375, prec 0.104942, recall 0.781951
2017-12-10T03:09:01.675981: step 7164, loss 0.60954, acc 0.875, prec 0.104955, recall 0.781993
2017-12-10T03:09:01.936873: step 7165, loss 0.26207, acc 0.9375, prec 0.104949, recall 0.781993
2017-12-10T03:09:02.198514: step 7166, loss 0.332229, acc 0.9375, prec 0.104955, recall 0.782014
2017-12-10T03:09:02.460546: step 7167, loss 0.157052, acc 0.9375, prec 0.104973, recall 0.782056
2017-12-10T03:09:02.732201: step 7168, loss 0.197647, acc 0.953125, prec 0.104981, recall 0.782077
2017-12-10T03:09:02.997935: step 7169, loss 0.196412, acc 0.953125, prec 0.105012, recall 0.782141
2017-12-10T03:09:03.264894: step 7170, loss 0.0439317, acc 0.96875, prec 0.105009, recall 0.782141
2017-12-10T03:09:03.536428: step 7171, loss 0.431044, acc 0.921875, prec 0.105025, recall 0.782183
2017-12-10T03:09:03.803370: step 7172, loss 0.00995396, acc 1, prec 0.105037, recall 0.782204
2017-12-10T03:09:04.075941: step 7173, loss 0.145239, acc 0.984375, prec 0.105047, recall 0.782225
2017-12-10T03:09:04.344406: step 7174, loss 0.0012498, acc 1, prec 0.105059, recall 0.782246
2017-12-10T03:09:04.608940: step 7175, loss 0.119584, acc 0.9375, prec 0.105088, recall 0.78231
2017-12-10T03:09:04.874441: step 7176, loss 0.0918382, acc 0.984375, prec 0.10511, recall 0.782352
2017-12-10T03:09:05.142967: step 7177, loss 0.0242123, acc 0.984375, prec 0.105121, recall 0.782373
2017-12-10T03:09:05.417025: step 7178, loss 0.3101, acc 0.984375, prec 0.105131, recall 0.782394
2017-12-10T03:09:05.686037: step 7179, loss 0.0240405, acc 0.984375, prec 0.10513, recall 0.782394
2017-12-10T03:09:05.946619: step 7180, loss 0.33155, acc 0.96875, prec 0.10515, recall 0.782436
2017-12-10T03:09:06.216598: step 7181, loss 0.00477134, acc 1, prec 0.10515, recall 0.782436
2017-12-10T03:09:06.482346: step 7182, loss 0.0359925, acc 0.984375, prec 0.105149, recall 0.782436
2017-12-10T03:09:06.746029: step 7183, loss 0.371985, acc 0.984375, prec 0.105147, recall 0.782436
2017-12-10T03:09:07.012115: step 7184, loss 0.27754, acc 0.96875, prec 0.105145, recall 0.782436
2017-12-10T03:09:07.276891: step 7185, loss 0.0174146, acc 1, prec 0.105156, recall 0.782457
2017-12-10T03:09:07.546136: step 7186, loss 0.0375315, acc 0.96875, prec 0.105165, recall 0.782478
2017-12-10T03:09:07.813944: step 7187, loss 0.115508, acc 0.984375, prec 0.105176, recall 0.782499
2017-12-10T03:09:08.081982: step 7188, loss 7.86056, acc 0.984375, prec 0.105199, recall 0.782466
2017-12-10T03:09:08.348920: step 7189, loss 0.0361907, acc 0.984375, prec 0.105221, recall 0.782508
2017-12-10T03:09:08.613140: step 7190, loss 0.166175, acc 0.96875, prec 0.105241, recall 0.78255
2017-12-10T03:09:08.882757: step 7191, loss 0.607576, acc 0.984375, prec 0.105252, recall 0.782571
2017-12-10T03:09:09.145322: step 7192, loss 0.188406, acc 0.96875, prec 0.10526, recall 0.782592
2017-12-10T03:09:09.410635: step 7193, loss 0.508642, acc 0.953125, prec 0.105268, recall 0.782613
2017-12-10T03:09:09.681260: step 7194, loss 0.503616, acc 0.9375, prec 0.105286, recall 0.782655
2017-12-10T03:09:09.955199: step 7195, loss 0.16655, acc 0.953125, prec 0.105317, recall 0.782718
2017-12-10T03:09:10.224475: step 7196, loss 0.470874, acc 0.96875, prec 0.105325, recall 0.782739
2017-12-10T03:09:10.491451: step 7197, loss 1.26883, acc 0.90625, prec 0.105317, recall 0.782739
2017-12-10T03:09:10.757714: step 7198, loss 0.312407, acc 0.90625, prec 0.105309, recall 0.782739
2017-12-10T03:09:11.020645: step 7199, loss 0.237719, acc 0.953125, prec 0.105305, recall 0.782739
2017-12-10T03:09:11.285195: step 7200, loss 0.0229095, acc 1, prec 0.105305, recall 0.782739

Evaluation:
2017-12-10T03:09:18.862737: step 7200, loss 6.91289, acc 0.929805, prec 0.105443, recall 0.779184

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-7200

2017-12-10T03:09:20.197752: step 7201, loss 0.164537, acc 0.96875, prec 0.105452, recall 0.779205
2017-12-10T03:09:20.462513: step 7202, loss 0.260926, acc 0.96875, prec 0.105461, recall 0.779226
2017-12-10T03:09:20.721547: step 7203, loss 0.312196, acc 0.984375, prec 0.10546, recall 0.779226
2017-12-10T03:09:20.993954: step 7204, loss 0.426518, acc 0.96875, prec 0.105457, recall 0.779226
2017-12-10T03:09:21.264389: step 7205, loss 0.286237, acc 0.96875, prec 0.105454, recall 0.779226
2017-12-10T03:09:21.539175: step 7206, loss 0.196557, acc 0.984375, prec 0.105453, recall 0.779226
2017-12-10T03:09:21.814770: step 7207, loss 0.114757, acc 0.96875, prec 0.105462, recall 0.779247
2017-12-10T03:09:22.082280: step 7208, loss 6.85715, acc 0.953125, prec 0.10547, recall 0.779194
2017-12-10T03:09:22.350906: step 7209, loss 0.559489, acc 0.953125, prec 0.105478, recall 0.779215
2017-12-10T03:09:22.620505: step 7210, loss 0.370657, acc 0.953125, prec 0.105474, recall 0.779215
2017-12-10T03:09:22.883141: step 7211, loss 0.510277, acc 0.921875, prec 0.10549, recall 0.779257
2017-12-10T03:09:23.144654: step 7212, loss 0.40998, acc 0.875, prec 0.105502, recall 0.779299
2017-12-10T03:09:23.409103: step 7213, loss 1.43468, acc 0.8125, prec 0.105486, recall 0.779299
2017-12-10T03:09:23.673902: step 7214, loss 0.91275, acc 0.828125, prec 0.105483, recall 0.77932
2017-12-10T03:09:23.939972: step 7215, loss 1.34086, acc 0.875, prec 0.105472, recall 0.77932
2017-12-10T03:09:24.200323: step 7216, loss 0.782938, acc 0.859375, prec 0.105459, recall 0.77932
2017-12-10T03:09:24.472036: step 7217, loss 1.25388, acc 0.84375, prec 0.105457, recall 0.779341
2017-12-10T03:09:24.742014: step 7218, loss 1.10891, acc 0.84375, prec 0.105444, recall 0.779341
2017-12-10T03:09:25.011949: step 7219, loss 0.285616, acc 0.90625, prec 0.105436, recall 0.779341
2017-12-10T03:09:25.285800: step 7220, loss 0.15633, acc 0.953125, prec 0.105432, recall 0.779341
2017-12-10T03:09:25.550551: step 7221, loss 0.0678982, acc 0.984375, prec 0.105442, recall 0.779361
2017-12-10T03:09:25.813691: step 7222, loss 0.487055, acc 0.890625, prec 0.105444, recall 0.779382
2017-12-10T03:09:26.074981: step 7223, loss 0.895261, acc 0.828125, prec 0.10544, recall 0.779403
2017-12-10T03:09:26.352607: step 7224, loss 0.0354655, acc 0.984375, prec 0.105439, recall 0.779403
2017-12-10T03:09:26.625328: step 7225, loss 0.356175, acc 0.953125, prec 0.105446, recall 0.779424
2017-12-10T03:09:26.887255: step 7226, loss 0.509621, acc 0.9375, prec 0.105464, recall 0.779466
2017-12-10T03:09:27.152265: step 7227, loss 0.867377, acc 0.953125, prec 0.105494, recall 0.779529
2017-12-10T03:09:27.422373: step 7228, loss 0.138292, acc 0.953125, prec 0.10549, recall 0.779529
2017-12-10T03:09:27.688038: step 7229, loss 0.270237, acc 0.96875, prec 0.105499, recall 0.77955
2017-12-10T03:09:27.952725: step 7230, loss 0.135447, acc 0.953125, prec 0.105518, recall 0.779592
2017-12-10T03:09:28.216359: step 7231, loss 0.173261, acc 0.9375, prec 0.105524, recall 0.779613
2017-12-10T03:09:28.483029: step 7232, loss 0.186992, acc 0.9375, prec 0.10553, recall 0.779634
2017-12-10T03:09:28.751757: step 7233, loss 0.0358415, acc 0.984375, prec 0.105529, recall 0.779634
2017-12-10T03:09:29.016119: step 7234, loss 0.00405728, acc 1, prec 0.10554, recall 0.779655
2017-12-10T03:09:29.276562: step 7235, loss 0.0230934, acc 0.984375, prec 0.105539, recall 0.779655
2017-12-10T03:09:29.542842: step 7236, loss 0.152233, acc 0.984375, prec 0.105549, recall 0.779675
2017-12-10T03:09:29.809628: step 7237, loss 0.157271, acc 0.9375, prec 0.105544, recall 0.779675
2017-12-10T03:09:30.077913: step 7238, loss 0.0362363, acc 0.984375, prec 0.105542, recall 0.779675
2017-12-10T03:09:30.346456: step 7239, loss 0.000360258, acc 1, prec 0.105542, recall 0.779675
2017-12-10T03:09:30.608172: step 7240, loss 3.34593, acc 0.953125, prec 0.10554, recall 0.779602
2017-12-10T03:09:30.875442: step 7241, loss 0.25238, acc 0.953125, prec 0.105559, recall 0.779643
2017-12-10T03:09:31.138866: step 7242, loss 0.260098, acc 0.96875, prec 0.105567, recall 0.779664
2017-12-10T03:09:31.411708: step 7243, loss 0.393519, acc 0.96875, prec 0.105565, recall 0.779664
2017-12-10T03:09:31.672805: step 7244, loss 0.0794412, acc 0.96875, prec 0.105585, recall 0.779706
2017-12-10T03:09:31.935822: step 7245, loss 0.0901525, acc 0.953125, prec 0.105592, recall 0.779727
2017-12-10T03:09:32.195898: step 7246, loss 0.299749, acc 0.953125, prec 0.105588, recall 0.779727
2017-12-10T03:09:32.459696: step 7247, loss 0.0198163, acc 1, prec 0.105611, recall 0.779769
2017-12-10T03:09:32.723705: step 7248, loss 0.738088, acc 0.921875, prec 0.105616, recall 0.77979
2017-12-10T03:09:32.984718: step 7249, loss 0.0139268, acc 1, prec 0.105627, recall 0.77981
2017-12-10T03:09:33.252992: step 7250, loss 0.253874, acc 0.9375, prec 0.105622, recall 0.77981
2017-12-10T03:09:33.524358: step 7251, loss 0.152121, acc 0.9375, prec 0.105651, recall 0.779873
2017-12-10T03:09:33.790725: step 7252, loss 0.16008, acc 0.96875, prec 0.10566, recall 0.779894
2017-12-10T03:09:34.059018: step 7253, loss 0.127619, acc 0.96875, prec 0.105669, recall 0.779915
2017-12-10T03:09:34.324825: step 7254, loss 0.203299, acc 0.984375, prec 0.105667, recall 0.779915
2017-12-10T03:09:34.589554: step 7255, loss 1.14162, acc 0.921875, prec 0.10566, recall 0.779915
2017-12-10T03:09:34.854551: step 7256, loss 0.0794148, acc 0.96875, prec 0.105669, recall 0.779936
2017-12-10T03:09:35.118266: step 7257, loss 0.0184099, acc 1, prec 0.105669, recall 0.779936
2017-12-10T03:09:35.385309: step 7258, loss 0.710598, acc 0.9375, prec 0.105664, recall 0.779936
2017-12-10T03:09:35.649423: step 7259, loss 0.0929438, acc 0.984375, prec 0.105674, recall 0.779956
2017-12-10T03:09:35.911374: step 7260, loss 0.0842291, acc 0.96875, prec 0.105683, recall 0.779977
2017-12-10T03:09:36.185971: step 7261, loss 0.156261, acc 0.984375, prec 0.105681, recall 0.779977
2017-12-10T03:09:36.450392: step 7262, loss 0.571861, acc 0.890625, prec 0.105672, recall 0.779977
2017-12-10T03:09:36.719153: step 7263, loss 0.446397, acc 0.9375, prec 0.105678, recall 0.779998
2017-12-10T03:09:36.985052: step 7264, loss 0.0322751, acc 0.984375, prec 0.105699, recall 0.78004
2017-12-10T03:09:37.251694: step 7265, loss 0.485986, acc 0.953125, prec 0.105707, recall 0.780061
2017-12-10T03:09:37.524731: step 7266, loss 0.0057948, acc 1, prec 0.10573, recall 0.780102
2017-12-10T03:09:37.793691: step 7267, loss 0.108229, acc 0.984375, prec 0.105728, recall 0.780102
2017-12-10T03:09:38.063110: step 7268, loss 0.678456, acc 0.953125, prec 0.105747, recall 0.780144
2017-12-10T03:09:38.335751: step 7269, loss 0.124196, acc 0.921875, prec 0.10574, recall 0.780144
2017-12-10T03:09:38.599787: step 7270, loss 3.45021, acc 0.953125, prec 0.105772, recall 0.780132
2017-12-10T03:09:38.866169: step 7271, loss 0.279231, acc 0.96875, prec 0.105792, recall 0.780174
2017-12-10T03:09:39.136109: step 7272, loss 0.544866, acc 0.96875, prec 0.10579, recall 0.780174
2017-12-10T03:09:39.398861: step 7273, loss 0.166474, acc 0.96875, prec 0.10581, recall 0.780216
2017-12-10T03:09:39.664421: step 7274, loss 0.353899, acc 0.921875, prec 0.105803, recall 0.780216
2017-12-10T03:09:39.937693: step 7275, loss 0.498871, acc 0.921875, prec 0.105796, recall 0.780216
2017-12-10T03:09:40.205643: step 7276, loss 0.349096, acc 0.9375, prec 0.105791, recall 0.780216
2017-12-10T03:09:40.476940: step 7277, loss 0.785948, acc 0.84375, prec 0.1058, recall 0.780257
2017-12-10T03:09:40.743655: step 7278, loss 1.01749, acc 0.875, prec 0.105801, recall 0.780278
2017-12-10T03:09:41.012673: step 7279, loss 0.164227, acc 0.9375, prec 0.105807, recall 0.780299
2017-12-10T03:09:41.286025: step 7280, loss 0.337233, acc 0.921875, prec 0.105812, recall 0.78032
2017-12-10T03:09:41.565868: step 7281, loss 0.680083, acc 0.90625, prec 0.105815, recall 0.78034
2017-12-10T03:09:41.827617: step 7282, loss 0.778481, acc 0.890625, prec 0.105817, recall 0.780361
2017-12-10T03:09:42.093809: step 7283, loss 0.208623, acc 0.9375, prec 0.105834, recall 0.780403
2017-12-10T03:09:42.366370: step 7284, loss 1.40889, acc 0.859375, prec 0.105822, recall 0.780403
2017-12-10T03:09:42.636818: step 7285, loss 0.643101, acc 0.875, prec 0.105823, recall 0.780423
2017-12-10T03:09:42.903979: step 7286, loss 1.11611, acc 0.8125, prec 0.105818, recall 0.780444
2017-12-10T03:09:43.172018: step 7287, loss 0.643625, acc 0.921875, prec 0.105823, recall 0.780465
2017-12-10T03:09:43.444991: step 7288, loss 0.685101, acc 0.953125, prec 0.105819, recall 0.780465
2017-12-10T03:09:43.709966: step 7289, loss 0.95298, acc 0.890625, prec 0.105809, recall 0.780465
2017-12-10T03:09:43.971894: step 7290, loss 0.244299, acc 0.890625, prec 0.1058, recall 0.780465
2017-12-10T03:09:44.234068: step 7291, loss 0.543034, acc 0.96875, prec 0.105808, recall 0.780486
2017-12-10T03:09:44.496650: step 7292, loss 0.117513, acc 0.96875, prec 0.105829, recall 0.780527
2017-12-10T03:09:44.761779: step 7293, loss 0.444984, acc 0.9375, prec 0.105823, recall 0.780527
2017-12-10T03:09:45.023349: step 7294, loss 0.461839, acc 0.984375, prec 0.105868, recall 0.78061
2017-12-10T03:09:45.290492: step 7295, loss 0.389846, acc 0.96875, prec 0.105876, recall 0.780631
2017-12-10T03:09:45.556531: step 7296, loss 0.0183125, acc 0.984375, prec 0.105909, recall 0.780693
2017-12-10T03:09:45.826062: step 7297, loss 0.0356934, acc 0.96875, prec 0.105918, recall 0.780713
2017-12-10T03:09:46.086851: step 7298, loss 0.0565153, acc 0.96875, prec 0.105915, recall 0.780713
2017-12-10T03:09:46.355549: step 7299, loss 0.122138, acc 0.984375, prec 0.105948, recall 0.780775
2017-12-10T03:09:46.619114: step 7300, loss 0.22286, acc 0.984375, prec 0.105958, recall 0.780796
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-7300

2017-12-10T03:09:47.943825: step 7301, loss 0.00742294, acc 1, prec 0.105958, recall 0.780796
2017-12-10T03:09:48.207505: step 7302, loss 0.0206792, acc 1, prec 0.10597, recall 0.780817
2017-12-10T03:09:48.468897: step 7303, loss 0.678004, acc 0.9375, prec 0.105964, recall 0.780817
2017-12-10T03:09:48.738493: step 7304, loss 0.101336, acc 0.984375, prec 0.105963, recall 0.780817
2017-12-10T03:09:49.015924: step 7305, loss 6.91785e-05, acc 1, prec 0.105963, recall 0.780817
2017-12-10T03:09:49.277489: step 7306, loss 0.00116069, acc 1, prec 0.105975, recall 0.780837
2017-12-10T03:09:49.545565: step 7307, loss 0.0127514, acc 0.984375, prec 0.105973, recall 0.780837
2017-12-10T03:09:49.806797: step 7308, loss 0.00038033, acc 1, prec 0.105973, recall 0.780837
2017-12-10T03:09:50.070389: step 7309, loss 2.08748, acc 0.984375, prec 0.105973, recall 0.780764
2017-12-10T03:09:50.337010: step 7310, loss 0.321153, acc 0.9375, prec 0.105979, recall 0.780784
2017-12-10T03:09:50.602594: step 7311, loss 0.252948, acc 0.953125, prec 0.106021, recall 0.780867
2017-12-10T03:09:50.872361: step 7312, loss 0.0365932, acc 0.984375, prec 0.10602, recall 0.780867
2017-12-10T03:09:51.148533: step 7313, loss 0.189729, acc 0.984375, prec 0.106018, recall 0.780867
2017-12-10T03:09:51.416867: step 7314, loss 0.447711, acc 0.984375, prec 0.10604, recall 0.780908
2017-12-10T03:09:51.686631: step 7315, loss 0.00346186, acc 1, prec 0.106051, recall 0.780929
2017-12-10T03:09:51.952757: step 7316, loss 0.127086, acc 0.96875, prec 0.106071, recall 0.78097
2017-12-10T03:09:52.217277: step 7317, loss 0.0742155, acc 0.96875, prec 0.10608, recall 0.780991
2017-12-10T03:09:52.476989: step 7318, loss 0.379249, acc 0.984375, prec 0.10609, recall 0.781012
2017-12-10T03:09:52.746572: step 7319, loss 0.276801, acc 0.921875, prec 0.106095, recall 0.781032
2017-12-10T03:09:53.014372: step 7320, loss 0.444958, acc 0.96875, prec 0.106103, recall 0.781053
2017-12-10T03:09:53.275520: step 7321, loss 0.264258, acc 0.9375, prec 0.106109, recall 0.781073
2017-12-10T03:09:53.541823: step 7322, loss 0.092707, acc 0.96875, prec 0.106118, recall 0.781094
2017-12-10T03:09:53.801233: step 7323, loss 0.622084, acc 0.90625, prec 0.10611, recall 0.781094
2017-12-10T03:09:54.064390: step 7324, loss 0.0402051, acc 0.96875, prec 0.106107, recall 0.781094
2017-12-10T03:09:54.332347: step 7325, loss 0.32051, acc 0.953125, prec 0.106103, recall 0.781094
2017-12-10T03:09:54.596593: step 7326, loss 0.383824, acc 0.890625, prec 0.106105, recall 0.781115
2017-12-10T03:09:54.857404: step 7327, loss 0.383843, acc 0.953125, prec 0.106113, recall 0.781135
2017-12-10T03:09:55.127518: step 7328, loss 0.0335074, acc 0.984375, prec 0.106123, recall 0.781156
2017-12-10T03:09:55.399754: step 7329, loss 0.0378512, acc 0.984375, prec 0.106144, recall 0.781197
2017-12-10T03:09:55.662645: step 7330, loss 0.133529, acc 0.96875, prec 0.106141, recall 0.781197
2017-12-10T03:09:55.927625: step 7331, loss 0.0231253, acc 0.984375, prec 0.10614, recall 0.781197
2017-12-10T03:09:56.190693: step 7332, loss 0.17553, acc 0.953125, prec 0.106147, recall 0.781218
2017-12-10T03:09:56.458282: step 7333, loss 0.0576467, acc 0.96875, prec 0.106145, recall 0.781218
2017-12-10T03:09:56.730140: step 7334, loss 0.288908, acc 0.96875, prec 0.106142, recall 0.781218
2017-12-10T03:09:57.002619: step 7335, loss 0.586497, acc 0.953125, prec 0.106138, recall 0.781218
2017-12-10T03:09:57.269667: step 7336, loss 0.188502, acc 0.96875, prec 0.106135, recall 0.781218
2017-12-10T03:09:57.540785: step 7337, loss 0.298725, acc 0.96875, prec 0.106132, recall 0.781218
2017-12-10T03:09:57.806723: step 7338, loss 0.00201703, acc 1, prec 0.106132, recall 0.781218
2017-12-10T03:09:58.067915: step 7339, loss 0.00883153, acc 1, prec 0.106132, recall 0.781218
2017-12-10T03:09:58.333644: step 7340, loss 0.00296836, acc 1, prec 0.106144, recall 0.781238
2017-12-10T03:09:58.601669: step 7341, loss 0.00485387, acc 1, prec 0.106167, recall 0.781279
2017-12-10T03:09:58.865951: step 7342, loss 9.5828, acc 0.96875, prec 0.106177, recall 0.781227
2017-12-10T03:09:59.132407: step 7343, loss 2.00403e-05, acc 1, prec 0.106177, recall 0.781227
2017-12-10T03:09:59.397947: step 7344, loss 2.4571, acc 0.953125, prec 0.106197, recall 0.781194
2017-12-10T03:09:59.671895: step 7345, loss 0.414924, acc 0.953125, prec 0.106193, recall 0.781194
2017-12-10T03:09:59.951480: step 7346, loss 0.0866802, acc 0.96875, prec 0.10619, recall 0.781194
2017-12-10T03:10:00.232992: step 7347, loss 0.917501, acc 0.890625, prec 0.106181, recall 0.781194
2017-12-10T03:10:00.499210: step 7348, loss 1.32772, acc 0.796875, prec 0.106163, recall 0.781194
2017-12-10T03:10:00.766226: step 7349, loss 0.50175, acc 0.921875, prec 0.106168, recall 0.781215
2017-12-10T03:10:01.034462: step 7350, loss 0.798017, acc 0.859375, prec 0.106167, recall 0.781235
2017-12-10T03:10:01.304677: step 7351, loss 1.09256, acc 0.8125, prec 0.106162, recall 0.781256
2017-12-10T03:10:01.568853: step 7352, loss 0.941915, acc 0.828125, prec 0.106147, recall 0.781256
2017-12-10T03:10:01.833701: step 7353, loss 0.974527, acc 0.765625, prec 0.106138, recall 0.781276
2017-12-10T03:10:02.094420: step 7354, loss 2.25084, acc 0.734375, prec 0.106115, recall 0.781276
2017-12-10T03:10:02.359473: step 7355, loss 1.40579, acc 0.796875, prec 0.106109, recall 0.781297
2017-12-10T03:10:02.633864: step 7356, loss 1.03853, acc 0.8125, prec 0.106104, recall 0.781318
2017-12-10T03:10:02.902523: step 7357, loss 1.2539, acc 0.84375, prec 0.106091, recall 0.781318
2017-12-10T03:10:03.166627: step 7358, loss 1.74468, acc 0.796875, prec 0.106073, recall 0.781318
2017-12-10T03:10:03.433376: step 7359, loss 2.02865, acc 0.78125, prec 0.106065, recall 0.781338
2017-12-10T03:10:03.698090: step 7360, loss 0.843782, acc 0.828125, prec 0.106062, recall 0.781359
2017-12-10T03:10:03.967336: step 7361, loss 1.0609, acc 0.84375, prec 0.106048, recall 0.781359
2017-12-10T03:10:04.229297: step 7362, loss 0.57382, acc 0.921875, prec 0.106053, recall 0.781379
2017-12-10T03:10:04.491758: step 7363, loss 1.12835, acc 0.921875, prec 0.106046, recall 0.781379
2017-12-10T03:10:04.759099: step 7364, loss 0.632875, acc 0.921875, prec 0.106051, recall 0.7814
2017-12-10T03:10:05.027461: step 7365, loss 0.806997, acc 0.890625, prec 0.106053, recall 0.78142
2017-12-10T03:10:05.290267: step 7366, loss 1.14849, acc 0.9375, prec 0.106047, recall 0.78142
2017-12-10T03:10:05.556785: step 7367, loss 0.24894, acc 0.96875, prec 0.106045, recall 0.78142
2017-12-10T03:10:05.821223: step 7368, loss 0.347847, acc 0.921875, prec 0.106038, recall 0.78142
2017-12-10T03:10:06.091093: step 7369, loss 0.119446, acc 0.96875, prec 0.106035, recall 0.78142
2017-12-10T03:10:06.354533: step 7370, loss 0.303406, acc 0.953125, prec 0.1061, recall 0.781543
2017-12-10T03:10:06.620498: step 7371, loss 0.439001, acc 0.953125, prec 0.106107, recall 0.781564
2017-12-10T03:10:06.888418: step 7372, loss 0.0355671, acc 0.984375, prec 0.106117, recall 0.781584
2017-12-10T03:10:07.162115: step 7373, loss 0.111934, acc 0.96875, prec 0.106114, recall 0.781584
2017-12-10T03:10:07.431644: step 7374, loss 6.19071e-06, acc 1, prec 0.106114, recall 0.781584
2017-12-10T03:10:07.692875: step 7375, loss 0.109711, acc 0.984375, prec 0.106124, recall 0.781605
2017-12-10T03:10:07.958698: step 7376, loss 1.02436, acc 0.953125, prec 0.106132, recall 0.781625
2017-12-10T03:10:08.227149: step 7377, loss 0.130775, acc 0.953125, prec 0.106128, recall 0.781625
2017-12-10T03:10:08.490093: step 7378, loss 0.0779674, acc 0.984375, prec 0.106126, recall 0.781625
2017-12-10T03:10:08.753317: step 7379, loss 0.156202, acc 0.984375, prec 0.106136, recall 0.781646
2017-12-10T03:10:09.017365: step 7380, loss 0.249354, acc 0.984375, prec 0.106146, recall 0.781666
2017-12-10T03:10:09.279815: step 7381, loss 0.00459336, acc 1, prec 0.106146, recall 0.781666
2017-12-10T03:10:09.544723: step 7382, loss 0.210419, acc 0.984375, prec 0.106179, recall 0.781728
2017-12-10T03:10:09.809218: step 7383, loss 0.00462568, acc 1, prec 0.106179, recall 0.781728
2017-12-10T03:10:10.072043: step 7384, loss 0.000677396, acc 1, prec 0.106191, recall 0.781748
2017-12-10T03:10:10.341344: step 7385, loss 0.214839, acc 0.984375, prec 0.106201, recall 0.781769
2017-12-10T03:10:10.605352: step 7386, loss 0.140405, acc 0.96875, prec 0.106198, recall 0.781769
2017-12-10T03:10:10.872149: step 7387, loss 3.54627, acc 0.984375, prec 0.106209, recall 0.781716
2017-12-10T03:10:11.141790: step 7388, loss 1.28723, acc 0.96875, prec 0.106241, recall 0.781777
2017-12-10T03:10:11.407032: step 7389, loss 12.0713, acc 0.953125, prec 0.106251, recall 0.781651
2017-12-10T03:10:11.686151: step 7390, loss 0.204565, acc 0.96875, prec 0.106259, recall 0.781672
2017-12-10T03:10:11.952370: step 7391, loss 0.42044, acc 0.9375, prec 0.106265, recall 0.781692
2017-12-10T03:10:12.220649: step 7392, loss 0.213637, acc 0.9375, prec 0.10626, recall 0.781692
2017-12-10T03:10:12.481737: step 7393, loss 0.156658, acc 0.953125, prec 0.106256, recall 0.781692
2017-12-10T03:10:12.747869: step 7394, loss 0.422595, acc 0.9375, prec 0.106262, recall 0.781713
2017-12-10T03:10:13.013715: step 7395, loss 0.846836, acc 0.828125, prec 0.106247, recall 0.781713
2017-12-10T03:10:13.280357: step 7396, loss 0.744781, acc 0.890625, prec 0.106249, recall 0.781733
2017-12-10T03:10:13.547133: step 7397, loss 1.73594, acc 0.78125, prec 0.106253, recall 0.781774
2017-12-10T03:10:13.809577: step 7398, loss 1.30811, acc 0.71875, prec 0.106274, recall 0.781856
2017-12-10T03:10:14.074791: step 7399, loss 2.98434, acc 0.640625, prec 0.106266, recall 0.781896
2017-12-10T03:10:14.337483: step 7400, loss 1.37328, acc 0.78125, prec 0.106247, recall 0.781896
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-7400

2017-12-10T03:10:15.582898: step 7401, loss 2.89875, acc 0.6875, prec 0.106242, recall 0.781937
2017-12-10T03:10:15.849848: step 7402, loss 1.3309, acc 0.796875, prec 0.106236, recall 0.781958
2017-12-10T03:10:16.116369: step 7403, loss 2.04127, acc 0.734375, prec 0.106213, recall 0.781958
2017-12-10T03:10:16.383514: step 7404, loss 0.715311, acc 0.875, prec 0.106225, recall 0.781999
2017-12-10T03:10:16.654395: step 7405, loss 0.973978, acc 0.78125, prec 0.106218, recall 0.782019
2017-12-10T03:10:16.926760: step 7406, loss 1.17861, acc 0.84375, prec 0.106227, recall 0.78206
2017-12-10T03:10:17.194477: step 7407, loss 1.51305, acc 0.6875, prec 0.1062, recall 0.78206
2017-12-10T03:10:17.457780: step 7408, loss 1.01815, acc 0.8125, prec 0.106195, recall 0.78208
2017-12-10T03:10:17.720348: step 7409, loss 1.67602, acc 0.890625, prec 0.10622, recall 0.782141
2017-12-10T03:10:17.990028: step 7410, loss 0.51251, acc 0.875, prec 0.10622, recall 0.782162
2017-12-10T03:10:18.255721: step 7411, loss 0.405104, acc 0.90625, prec 0.106235, recall 0.782202
2017-12-10T03:10:18.523233: step 7412, loss 0.776357, acc 0.90625, prec 0.106238, recall 0.782223
2017-12-10T03:10:18.788428: step 7413, loss 0.196288, acc 0.9375, prec 0.106267, recall 0.782284
2017-12-10T03:10:19.053094: step 7414, loss 0.452383, acc 0.9375, prec 0.106261, recall 0.782284
2017-12-10T03:10:19.323258: step 7415, loss 0.661265, acc 0.921875, prec 0.106277, recall 0.782324
2017-12-10T03:10:19.589586: step 7416, loss 0.0538408, acc 0.984375, prec 0.106287, recall 0.782345
2017-12-10T03:10:19.850050: step 7417, loss 0.166279, acc 0.953125, prec 0.106283, recall 0.782345
2017-12-10T03:10:20.112486: step 7418, loss 0.00337777, acc 1, prec 0.106306, recall 0.782385
2017-12-10T03:10:20.372243: step 7419, loss 0.197925, acc 0.953125, prec 0.106302, recall 0.782385
2017-12-10T03:10:20.637117: step 7420, loss 0.101079, acc 0.96875, prec 0.106299, recall 0.782385
2017-12-10T03:10:20.901775: step 7421, loss 0.0646799, acc 0.984375, prec 0.106298, recall 0.782385
2017-12-10T03:10:21.173441: step 7422, loss 0.00814329, acc 1, prec 0.106298, recall 0.782385
2017-12-10T03:10:21.443667: step 7423, loss 0.261448, acc 0.9375, prec 0.106304, recall 0.782406
2017-12-10T03:10:21.711857: step 7424, loss 0.00418639, acc 1, prec 0.106326, recall 0.782446
2017-12-10T03:10:21.982355: step 7425, loss 2.30032, acc 0.96875, prec 0.106336, recall 0.782394
2017-12-10T03:10:22.250413: step 7426, loss 0.00294131, acc 1, prec 0.106359, recall 0.782434
2017-12-10T03:10:22.522975: step 7427, loss 0.0747957, acc 0.984375, prec 0.106369, recall 0.782454
2017-12-10T03:10:22.794537: step 7428, loss 0.491319, acc 0.953125, prec 0.106388, recall 0.782495
2017-12-10T03:10:23.067382: step 7429, loss 0.032358, acc 0.984375, prec 0.106386, recall 0.782495
2017-12-10T03:10:23.338923: step 7430, loss 0.298624, acc 0.9375, prec 0.106415, recall 0.782556
2017-12-10T03:10:23.602126: step 7431, loss 0.788449, acc 0.953125, prec 0.106433, recall 0.782597
2017-12-10T03:10:23.869418: step 7432, loss 0.17497, acc 0.96875, prec 0.106442, recall 0.782617
2017-12-10T03:10:24.135857: step 7433, loss 0.00641034, acc 1, prec 0.106453, recall 0.782637
2017-12-10T03:10:24.400550: step 7434, loss 0.0330167, acc 0.984375, prec 0.106463, recall 0.782657
2017-12-10T03:10:24.666578: step 7435, loss 0.980441, acc 0.953125, prec 0.106459, recall 0.782657
2017-12-10T03:10:24.932081: step 7436, loss 0.148091, acc 0.953125, prec 0.106455, recall 0.782657
2017-12-10T03:10:25.197314: step 7437, loss 0.00714719, acc 1, prec 0.106455, recall 0.782657
2017-12-10T03:10:25.460878: step 7438, loss 0.0895687, acc 0.953125, prec 0.106451, recall 0.782657
2017-12-10T03:10:25.730709: step 7439, loss 0.179002, acc 0.984375, prec 0.106507, recall 0.782759
2017-12-10T03:10:25.993387: step 7440, loss 1.81497, acc 0.953125, prec 0.106538, recall 0.782746
2017-12-10T03:10:26.262971: step 7441, loss 0.204707, acc 0.921875, prec 0.106542, recall 0.782767
2017-12-10T03:10:26.539443: step 7442, loss 0.207945, acc 0.953125, prec 0.106538, recall 0.782767
2017-12-10T03:10:26.804448: step 7443, loss 0.624065, acc 0.90625, prec 0.10653, recall 0.782767
2017-12-10T03:10:27.069032: step 7444, loss 0.74221, acc 0.90625, prec 0.106522, recall 0.782767
2017-12-10T03:10:27.346430: step 7445, loss 0.335237, acc 0.9375, prec 0.106539, recall 0.782807
2017-12-10T03:10:27.610476: step 7446, loss 0.0217999, acc 1, prec 0.106539, recall 0.782807
2017-12-10T03:10:27.883238: step 7447, loss 0.640938, acc 0.921875, prec 0.106533, recall 0.782807
2017-12-10T03:10:28.145371: step 7448, loss 0.529096, acc 0.96875, prec 0.10653, recall 0.782807
2017-12-10T03:10:28.407269: step 7449, loss 0.718833, acc 0.921875, prec 0.106523, recall 0.782807
2017-12-10T03:10:28.676239: step 7450, loss 0.287869, acc 0.921875, prec 0.106539, recall 0.782848
2017-12-10T03:10:28.938907: step 7451, loss 0.256442, acc 0.953125, prec 0.106558, recall 0.782888
2017-12-10T03:10:29.203834: step 7452, loss 0.968099, acc 0.875, prec 0.106547, recall 0.782888
2017-12-10T03:10:29.477662: step 7453, loss 0.330294, acc 0.9375, prec 0.106553, recall 0.782908
2017-12-10T03:10:29.747924: step 7454, loss 0.806182, acc 0.90625, prec 0.106567, recall 0.782949
2017-12-10T03:10:29.979567: step 7455, loss 0.560345, acc 0.941176, prec 0.106586, recall 0.782989
2017-12-10T03:10:30.258678: step 7456, loss 0.732227, acc 0.921875, prec 0.106579, recall 0.782989
2017-12-10T03:10:30.535071: step 7457, loss 0.373632, acc 0.953125, prec 0.106586, recall 0.783009
2017-12-10T03:10:30.805499: step 7458, loss 0.194354, acc 0.984375, prec 0.106585, recall 0.783009
2017-12-10T03:10:31.078926: step 7459, loss 0.00124242, acc 1, prec 0.106608, recall 0.78305
2017-12-10T03:10:31.339973: step 7460, loss 0.19421, acc 0.9375, prec 0.106614, recall 0.78307
2017-12-10T03:10:31.614299: step 7461, loss 0.0082099, acc 1, prec 0.106648, recall 0.78313
2017-12-10T03:10:31.875562: step 7462, loss 0.186907, acc 0.96875, prec 0.106668, recall 0.783171
2017-12-10T03:10:32.147841: step 7463, loss 0.143847, acc 0.953125, prec 0.106686, recall 0.783211
2017-12-10T03:10:32.417497: step 7464, loss 0.0648125, acc 1, prec 0.106697, recall 0.783231
2017-12-10T03:10:32.684851: step 7465, loss 0.0131553, acc 1, prec 0.10672, recall 0.783271
2017-12-10T03:10:32.951263: step 7466, loss 0.0841303, acc 0.96875, prec 0.106717, recall 0.783271
2017-12-10T03:10:33.211921: step 7467, loss 0.534128, acc 0.953125, prec 0.106713, recall 0.783271
2017-12-10T03:10:33.478614: step 7468, loss 0.0817, acc 0.96875, prec 0.106722, recall 0.783292
2017-12-10T03:10:33.743007: step 7469, loss 0.43855, acc 0.9375, prec 0.106739, recall 0.783332
2017-12-10T03:10:34.006376: step 7470, loss 0.673442, acc 0.953125, prec 0.106735, recall 0.783332
2017-12-10T03:10:34.279499: step 7471, loss 0.034168, acc 0.984375, prec 0.106734, recall 0.783332
2017-12-10T03:10:34.542558: step 7472, loss 0.0758116, acc 0.984375, prec 0.106766, recall 0.783392
2017-12-10T03:10:34.812981: step 7473, loss 0.00185534, acc 1, prec 0.106778, recall 0.783412
2017-12-10T03:10:35.076709: step 7474, loss 0.0224577, acc 0.984375, prec 0.106788, recall 0.783432
2017-12-10T03:10:35.343237: step 7475, loss 0.158119, acc 0.96875, prec 0.106785, recall 0.783432
2017-12-10T03:10:35.608690: step 7476, loss 0.000401886, acc 1, prec 0.106796, recall 0.783453
2017-12-10T03:10:35.879750: step 7477, loss 0.215209, acc 0.984375, prec 0.106795, recall 0.783453
2017-12-10T03:10:36.154751: step 7478, loss 0.00451636, acc 1, prec 0.106795, recall 0.783453
2017-12-10T03:10:36.416590: step 7479, loss 0.153256, acc 0.96875, prec 0.106792, recall 0.783453
2017-12-10T03:10:36.685110: step 7480, loss 0.0974737, acc 0.984375, prec 0.106802, recall 0.783473
2017-12-10T03:10:36.952792: step 7481, loss 0.0100214, acc 1, prec 0.106802, recall 0.783473
2017-12-10T03:10:37.216902: step 7482, loss 3.62671, acc 0.96875, prec 0.106823, recall 0.78344
2017-12-10T03:10:37.488662: step 7483, loss 0.0103052, acc 1, prec 0.106823, recall 0.78344
2017-12-10T03:10:37.753220: step 7484, loss 0.160458, acc 0.96875, prec 0.106821, recall 0.78344
2017-12-10T03:10:38.016189: step 7485, loss 0.823909, acc 0.953125, prec 0.106839, recall 0.78348
2017-12-10T03:10:38.284776: step 7486, loss 0.345717, acc 0.984375, prec 0.106838, recall 0.78348
2017-12-10T03:10:38.556947: step 7487, loss 0.0346914, acc 0.984375, prec 0.106836, recall 0.78348
2017-12-10T03:10:38.822680: step 7488, loss 0.274803, acc 0.96875, prec 0.106834, recall 0.78348
2017-12-10T03:10:39.093168: step 7489, loss 0.269405, acc 0.96875, prec 0.106865, recall 0.783541
2017-12-10T03:10:39.362369: step 7490, loss 0.0875753, acc 0.984375, prec 0.106897, recall 0.783601
2017-12-10T03:10:39.633413: step 7491, loss 0.0538299, acc 0.96875, prec 0.106895, recall 0.783601
2017-12-10T03:10:39.900654: step 7492, loss 0.35748, acc 0.953125, prec 0.106891, recall 0.783601
2017-12-10T03:10:40.171325: step 7493, loss 0.321884, acc 0.96875, prec 0.106933, recall 0.783681
2017-12-10T03:10:40.433417: step 7494, loss 0.281926, acc 0.9375, prec 0.106973, recall 0.783761
2017-12-10T03:10:40.700039: step 7495, loss 0.192521, acc 0.984375, prec 0.106972, recall 0.783761
2017-12-10T03:10:40.969599: step 7496, loss 0.203696, acc 0.953125, prec 0.106968, recall 0.783761
2017-12-10T03:10:41.239053: step 7497, loss 0.43653, acc 0.9375, prec 0.106973, recall 0.783781
2017-12-10T03:10:41.507429: step 7498, loss 0.33734, acc 0.953125, prec 0.106981, recall 0.783801
2017-12-10T03:10:41.776295: step 7499, loss 0.672971, acc 0.9375, prec 0.107009, recall 0.783861
2017-12-10T03:10:42.041404: step 7500, loss 0.013653, acc 0.984375, prec 0.10703, recall 0.783901

Evaluation:
2017-12-10T03:10:49.796471: step 7500, loss 9.44593, acc 0.944146, prec 0.107254, recall 0.779562

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-7500

2017-12-10T03:10:51.095396: step 7501, loss 0.269966, acc 0.984375, prec 0.107253, recall 0.779562
2017-12-10T03:10:51.359251: step 7502, loss 0.284778, acc 0.96875, prec 0.107262, recall 0.779582
2017-12-10T03:10:51.623606: step 7503, loss 0.0505071, acc 0.984375, prec 0.107272, recall 0.779602
2017-12-10T03:10:51.895823: step 7504, loss 0.284625, acc 0.9375, prec 0.107277, recall 0.779622
2017-12-10T03:10:52.154535: step 7505, loss 0.0915338, acc 0.96875, prec 0.107297, recall 0.779663
2017-12-10T03:10:52.423241: step 7506, loss 0.0526603, acc 0.96875, prec 0.107294, recall 0.779663
2017-12-10T03:10:52.697791: step 7507, loss 0.318788, acc 0.984375, prec 0.107293, recall 0.779663
2017-12-10T03:10:52.961973: step 7508, loss 0.234738, acc 0.953125, prec 0.1073, recall 0.779683
2017-12-10T03:10:53.228525: step 7509, loss 0.264744, acc 0.984375, prec 0.10731, recall 0.779703
2017-12-10T03:10:53.488767: step 7510, loss 0.0308279, acc 0.984375, prec 0.10732, recall 0.779723
2017-12-10T03:10:53.758902: step 7511, loss 0.00295933, acc 1, prec 0.10732, recall 0.779723
2017-12-10T03:10:54.018443: step 7512, loss 0.23764, acc 0.96875, prec 0.107328, recall 0.779743
2017-12-10T03:10:54.287080: step 7513, loss 0.394857, acc 0.96875, prec 0.107337, recall 0.779763
2017-12-10T03:10:54.552102: step 7514, loss 0.0474207, acc 0.984375, prec 0.107336, recall 0.779763
2017-12-10T03:10:54.825032: step 7515, loss 0.0341246, acc 0.984375, prec 0.107334, recall 0.779763
2017-12-10T03:10:55.097802: step 7516, loss 0.222161, acc 0.96875, prec 0.107332, recall 0.779763
2017-12-10T03:10:55.358155: step 7517, loss 0.0953171, acc 0.984375, prec 0.10733, recall 0.779763
2017-12-10T03:10:55.628728: step 7518, loss 0.00129528, acc 1, prec 0.10733, recall 0.779763
2017-12-10T03:10:55.891312: step 7519, loss 0.00290656, acc 1, prec 0.10733, recall 0.779763
2017-12-10T03:10:56.154603: step 7520, loss 0.577484, acc 0.921875, prec 0.107346, recall 0.779803
2017-12-10T03:10:56.419346: step 7521, loss 0.431997, acc 0.96875, prec 0.107343, recall 0.779803
2017-12-10T03:10:56.693775: step 7522, loss 0.00672129, acc 1, prec 0.107354, recall 0.779823
2017-12-10T03:10:56.959786: step 7523, loss 0.510078, acc 0.9375, prec 0.107349, recall 0.779823
2017-12-10T03:10:57.224307: step 7524, loss 0.543654, acc 0.953125, prec 0.107345, recall 0.779823
2017-12-10T03:10:57.488242: step 7525, loss 0.0876852, acc 0.96875, prec 0.107387, recall 0.779903
2017-12-10T03:10:57.754503: step 7526, loss 0.0378778, acc 0.96875, prec 0.107384, recall 0.779903
2017-12-10T03:10:58.016133: step 7527, loss 0.0788287, acc 0.984375, prec 0.107394, recall 0.779923
2017-12-10T03:10:58.281756: step 7528, loss 0.19844, acc 0.96875, prec 0.107414, recall 0.779964
2017-12-10T03:10:58.548072: step 7529, loss 3.94344, acc 0.96875, prec 0.107413, recall 0.779893
2017-12-10T03:10:58.814648: step 7530, loss 0.24176, acc 0.984375, prec 0.107411, recall 0.779893
2017-12-10T03:10:59.076180: step 7531, loss 0.141597, acc 0.984375, prec 0.107421, recall 0.779913
2017-12-10T03:10:59.342464: step 7532, loss 0.00303279, acc 1, prec 0.107443, recall 0.779953
2017-12-10T03:10:59.600892: step 7533, loss 0.323904, acc 0.96875, prec 0.107452, recall 0.779973
2017-12-10T03:10:59.862693: step 7534, loss 0.267342, acc 0.953125, prec 0.107448, recall 0.779973
2017-12-10T03:11:00.129142: step 7535, loss 0.323825, acc 0.984375, prec 0.107458, recall 0.779993
2017-12-10T03:11:00.403905: step 7536, loss 0.502434, acc 0.9375, prec 0.107452, recall 0.779993
2017-12-10T03:11:00.666076: step 7537, loss 0.333885, acc 0.953125, prec 0.107448, recall 0.779993
2017-12-10T03:11:00.937486: step 7538, loss 1.03747, acc 0.921875, prec 0.107453, recall 0.780013
2017-12-10T03:11:01.201468: step 7539, loss 0.298175, acc 0.96875, prec 0.10745, recall 0.780013
2017-12-10T03:11:01.472713: step 7540, loss 0.0794937, acc 0.984375, prec 0.107449, recall 0.780013
2017-12-10T03:11:01.737717: step 7541, loss 0.0846851, acc 0.96875, prec 0.107468, recall 0.780053
2017-12-10T03:11:02.003776: step 7542, loss 0.249624, acc 0.96875, prec 0.107488, recall 0.780093
2017-12-10T03:11:02.273915: step 7543, loss 0.430636, acc 0.953125, prec 0.107495, recall 0.780113
2017-12-10T03:11:02.544129: step 7544, loss 0.336584, acc 0.96875, prec 0.107526, recall 0.780173
2017-12-10T03:11:02.809682: step 7545, loss 0.20273, acc 0.96875, prec 0.107546, recall 0.780213
2017-12-10T03:11:03.081522: step 7546, loss 0.808384, acc 0.90625, prec 0.107549, recall 0.780233
2017-12-10T03:11:03.346863: step 7547, loss 0.726998, acc 0.9375, prec 0.107566, recall 0.780273
2017-12-10T03:11:03.611779: step 7548, loss 0.118731, acc 0.96875, prec 0.107585, recall 0.780313
2017-12-10T03:11:03.879308: step 7549, loss 0.407198, acc 0.953125, prec 0.107593, recall 0.780333
2017-12-10T03:11:04.149551: step 7550, loss 0.14414, acc 0.96875, prec 0.10759, recall 0.780333
2017-12-10T03:11:04.414018: step 7551, loss 0.116911, acc 0.96875, prec 0.107598, recall 0.780353
2017-12-10T03:11:04.683450: step 7552, loss 0.893133, acc 0.9375, prec 0.107604, recall 0.780373
2017-12-10T03:11:04.948457: step 7553, loss 0.604758, acc 0.953125, prec 0.107611, recall 0.780393
2017-12-10T03:11:05.215320: step 7554, loss 0.223639, acc 0.953125, prec 0.107607, recall 0.780393
2017-12-10T03:11:05.480642: step 7555, loss 0.00866299, acc 1, prec 0.107618, recall 0.780412
2017-12-10T03:11:05.755259: step 7556, loss 0.196736, acc 0.921875, prec 0.107612, recall 0.780412
2017-12-10T03:11:06.017514: step 7557, loss 0.0229139, acc 0.984375, prec 0.10761, recall 0.780412
2017-12-10T03:11:06.278253: step 7558, loss 0.449616, acc 0.953125, prec 0.107618, recall 0.780432
2017-12-10T03:11:06.544183: step 7559, loss 0.0773939, acc 0.984375, prec 0.107616, recall 0.780432
2017-12-10T03:11:06.808462: step 7560, loss 0.296454, acc 0.984375, prec 0.107615, recall 0.780432
2017-12-10T03:11:07.074237: step 7561, loss 0.0889323, acc 0.984375, prec 0.107613, recall 0.780432
2017-12-10T03:11:07.341625: step 7562, loss 0.16021, acc 0.96875, prec 0.107611, recall 0.780432
2017-12-10T03:11:07.608866: step 7563, loss 0.00650315, acc 1, prec 0.107622, recall 0.780452
2017-12-10T03:11:07.871887: step 7564, loss 0.150033, acc 0.984375, prec 0.107621, recall 0.780452
2017-12-10T03:11:08.133924: step 7565, loss 0.00184933, acc 1, prec 0.107632, recall 0.780472
2017-12-10T03:11:08.397191: step 7566, loss 0.722356, acc 0.984375, prec 0.107642, recall 0.780492
2017-12-10T03:11:08.664196: step 7567, loss 0.0036617, acc 1, prec 0.107653, recall 0.780512
2017-12-10T03:11:08.926772: step 7568, loss 0.022734, acc 0.984375, prec 0.107674, recall 0.780552
2017-12-10T03:11:09.190308: step 7569, loss 0.00356192, acc 1, prec 0.107674, recall 0.780552
2017-12-10T03:11:09.453666: step 7570, loss 0.245459, acc 0.984375, prec 0.107695, recall 0.780592
2017-12-10T03:11:09.724356: step 7571, loss 0.00472475, acc 1, prec 0.107706, recall 0.780612
2017-12-10T03:11:09.982854: step 7572, loss 0.00184033, acc 1, prec 0.107706, recall 0.780612
2017-12-10T03:11:10.248672: step 7573, loss 0.00564885, acc 1, prec 0.107717, recall 0.780632
2017-12-10T03:11:10.515925: step 7574, loss 0.0362794, acc 1, prec 0.107728, recall 0.780652
2017-12-10T03:11:10.781929: step 7575, loss 0.105581, acc 0.984375, prec 0.107727, recall 0.780652
2017-12-10T03:11:11.042879: step 7576, loss 0.12401, acc 0.984375, prec 0.107726, recall 0.780652
2017-12-10T03:11:11.308493: step 7577, loss 0.0287488, acc 0.984375, prec 0.107735, recall 0.780671
2017-12-10T03:11:11.584728: step 7578, loss 0.0368621, acc 0.96875, prec 0.107733, recall 0.780671
2017-12-10T03:11:11.855337: step 7579, loss 0.0047129, acc 1, prec 0.107733, recall 0.780671
2017-12-10T03:11:12.116531: step 7580, loss 0.0118729, acc 1, prec 0.107733, recall 0.780671
2017-12-10T03:11:12.378191: step 7581, loss 0.00250057, acc 1, prec 0.107744, recall 0.780691
2017-12-10T03:11:12.642357: step 7582, loss 0.0042645, acc 1, prec 0.107766, recall 0.780731
2017-12-10T03:11:12.907976: step 7583, loss 0.457927, acc 0.984375, prec 0.107765, recall 0.780731
2017-12-10T03:11:13.172901: step 7584, loss 0.00157025, acc 1, prec 0.107787, recall 0.780771
2017-12-10T03:11:13.448889: step 7585, loss 0.00177986, acc 1, prec 0.107798, recall 0.780791
2017-12-10T03:11:13.705434: step 7586, loss 0.217967, acc 0.984375, prec 0.107797, recall 0.780791
2017-12-10T03:11:13.970206: step 7587, loss 0.0225549, acc 0.984375, prec 0.107796, recall 0.780791
2017-12-10T03:11:14.233560: step 7588, loss 0.0020594, acc 1, prec 0.107796, recall 0.780791
2017-12-10T03:11:14.496070: step 7589, loss 0.415071, acc 0.96875, prec 0.107804, recall 0.780811
2017-12-10T03:11:14.757381: step 7590, loss 2.56777e-05, acc 1, prec 0.107804, recall 0.780811
2017-12-10T03:11:15.019602: step 7591, loss 0.132992, acc 0.984375, prec 0.107803, recall 0.780811
2017-12-10T03:11:15.288347: step 7592, loss 0.321038, acc 1, prec 0.107814, recall 0.780831
2017-12-10T03:11:15.557800: step 7593, loss 4.60066, acc 0.984375, prec 0.107836, recall 0.7808
2017-12-10T03:11:15.826137: step 7594, loss 0.0851915, acc 0.984375, prec 0.107835, recall 0.7808
2017-12-10T03:11:16.095613: step 7595, loss 0.454557, acc 0.984375, prec 0.107856, recall 0.780839
2017-12-10T03:11:16.369407: step 7596, loss 0.69674, acc 0.953125, prec 0.107863, recall 0.780859
2017-12-10T03:11:16.628438: step 7597, loss 0.789305, acc 0.921875, prec 0.107856, recall 0.780859
2017-12-10T03:11:16.895132: step 7598, loss 0.303162, acc 0.96875, prec 0.107865, recall 0.780879
2017-12-10T03:11:17.156738: step 7599, loss 0.777582, acc 0.921875, prec 0.107858, recall 0.780879
2017-12-10T03:11:17.427446: step 7600, loss 0.769511, acc 0.890625, prec 0.107849, recall 0.780879
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-7600

2017-12-10T03:11:18.784135: step 7601, loss 0.692521, acc 0.921875, prec 0.107887, recall 0.780958
2017-12-10T03:11:19.050576: step 7602, loss 0.502332, acc 0.90625, prec 0.10789, recall 0.780978
2017-12-10T03:11:19.316089: step 7603, loss 0.699487, acc 0.90625, prec 0.107893, recall 0.780998
2017-12-10T03:11:19.579840: step 7604, loss 2.0504, acc 0.9375, prec 0.107922, recall 0.780987
2017-12-10T03:11:19.849759: step 7605, loss 0.778582, acc 0.90625, prec 0.107914, recall 0.780987
2017-12-10T03:11:20.110972: step 7606, loss 0.41399, acc 0.875, prec 0.107903, recall 0.780987
2017-12-10T03:11:20.380239: step 7607, loss 0.570938, acc 0.875, prec 0.107892, recall 0.780987
2017-12-10T03:11:20.646668: step 7608, loss 1.0641, acc 0.84375, prec 0.107879, recall 0.780987
2017-12-10T03:11:20.907897: step 7609, loss 0.780164, acc 0.828125, prec 0.107864, recall 0.780987
2017-12-10T03:11:21.179063: step 7610, loss 1.07449, acc 0.875, prec 0.107864, recall 0.781007
2017-12-10T03:11:21.441994: step 7611, loss 1.14155, acc 0.734375, prec 0.107864, recall 0.781046
2017-12-10T03:11:21.707362: step 7612, loss 0.525817, acc 0.875, prec 0.107853, recall 0.781046
2017-12-10T03:11:21.972596: step 7613, loss 0.962201, acc 0.859375, prec 0.107841, recall 0.781046
2017-12-10T03:11:22.239517: step 7614, loss 0.689027, acc 0.875, prec 0.107864, recall 0.781106
2017-12-10T03:11:22.500331: step 7615, loss 0.273999, acc 0.921875, prec 0.107857, recall 0.781106
2017-12-10T03:11:22.774043: step 7616, loss 0.235494, acc 0.953125, prec 0.107875, recall 0.781145
2017-12-10T03:11:23.041244: step 7617, loss 0.205985, acc 0.875, prec 0.107864, recall 0.781145
2017-12-10T03:11:23.307417: step 7618, loss 0.36295, acc 0.953125, prec 0.107883, recall 0.781185
2017-12-10T03:11:23.572899: step 7619, loss 0.0970809, acc 0.96875, prec 0.10788, recall 0.781185
2017-12-10T03:11:23.838691: step 7620, loss 0.318005, acc 0.96875, prec 0.107888, recall 0.781205
2017-12-10T03:11:24.106598: step 7621, loss 0.438558, acc 0.921875, prec 0.107882, recall 0.781205
2017-12-10T03:11:24.366478: step 7622, loss 0.312267, acc 0.953125, prec 0.107889, recall 0.781225
2017-12-10T03:11:24.630743: step 7623, loss 0.102319, acc 0.96875, prec 0.107897, recall 0.781244
2017-12-10T03:11:24.897350: step 7624, loss 0.468615, acc 0.90625, prec 0.107889, recall 0.781244
2017-12-10T03:11:25.160152: step 7625, loss 0.0874704, acc 0.984375, prec 0.107888, recall 0.781244
2017-12-10T03:11:25.431206: step 7626, loss 0.107239, acc 0.96875, prec 0.107896, recall 0.781264
2017-12-10T03:11:25.705331: step 7627, loss 0.416145, acc 0.9375, prec 0.107891, recall 0.781264
2017-12-10T03:11:25.971985: step 7628, loss 0.000704597, acc 1, prec 0.107891, recall 0.781264
2017-12-10T03:11:26.233281: step 7629, loss 0.889351, acc 0.96875, prec 0.107888, recall 0.781264
2017-12-10T03:11:26.500647: step 7630, loss 0.355402, acc 0.953125, prec 0.107895, recall 0.781284
2017-12-10T03:11:26.773196: step 7631, loss 0.548989, acc 0.96875, prec 0.107904, recall 0.781304
2017-12-10T03:11:27.039134: step 7632, loss 0.222588, acc 0.953125, prec 0.1079, recall 0.781304
2017-12-10T03:11:27.301769: step 7633, loss 0.22676, acc 0.96875, prec 0.107897, recall 0.781304
2017-12-10T03:11:27.567503: step 7634, loss 0.254617, acc 0.953125, prec 0.107904, recall 0.781323
2017-12-10T03:11:27.832903: step 7635, loss 0.00241166, acc 1, prec 0.107915, recall 0.781343
2017-12-10T03:11:28.098353: step 7636, loss 0.746303, acc 0.96875, prec 0.107935, recall 0.781383
2017-12-10T03:11:28.364360: step 7637, loss 0.121315, acc 0.96875, prec 0.107954, recall 0.781422
2017-12-10T03:11:28.631092: step 7638, loss 0.0651377, acc 0.96875, prec 0.107974, recall 0.781462
2017-12-10T03:11:28.897351: step 7639, loss 0.000154638, acc 1, prec 0.107985, recall 0.781482
2017-12-10T03:11:29.159710: step 7640, loss 0.00145495, acc 1, prec 0.107985, recall 0.781482
2017-12-10T03:11:29.424945: step 7641, loss 0.175496, acc 0.984375, prec 0.107984, recall 0.781482
2017-12-10T03:11:29.695330: step 7642, loss 8.88701, acc 0.953125, prec 0.107992, recall 0.781431
2017-12-10T03:11:29.973622: step 7643, loss 0.190703, acc 0.96875, prec 0.107989, recall 0.781431
2017-12-10T03:11:30.245429: step 7644, loss 7.10317e-06, acc 1, prec 0.108001, recall 0.78145
2017-12-10T03:11:30.514681: step 7645, loss 0.218184, acc 0.96875, prec 0.107998, recall 0.78145
2017-12-10T03:11:30.784208: step 7646, loss 0.301098, acc 0.984375, prec 0.107996, recall 0.78145
2017-12-10T03:11:31.053717: step 7647, loss 1.12293, acc 0.90625, prec 0.107988, recall 0.78145
2017-12-10T03:11:31.320760: step 7648, loss 0.580258, acc 0.96875, prec 0.107986, recall 0.78145
2017-12-10T03:11:31.589616: step 7649, loss 0.417726, acc 0.984375, prec 0.107984, recall 0.78145
2017-12-10T03:11:31.860388: step 7650, loss 0.167496, acc 0.984375, prec 0.107983, recall 0.78145
2017-12-10T03:11:32.125145: step 7651, loss 0.160018, acc 0.953125, prec 0.10799, recall 0.78147
2017-12-10T03:11:32.389037: step 7652, loss 0.240678, acc 0.96875, prec 0.107999, recall 0.78149
2017-12-10T03:11:32.653807: step 7653, loss 0.20517, acc 0.984375, prec 0.108008, recall 0.78151
2017-12-10T03:11:32.929324: step 7654, loss 0.553467, acc 0.953125, prec 0.108015, recall 0.781529
2017-12-10T03:11:33.194350: step 7655, loss 0.646846, acc 0.9375, prec 0.10801, recall 0.781529
2017-12-10T03:11:33.454904: step 7656, loss 1.02153, acc 0.9375, prec 0.108005, recall 0.781529
2017-12-10T03:11:33.715706: step 7657, loss 0.234465, acc 0.96875, prec 0.108002, recall 0.781529
2017-12-10T03:11:33.979399: step 7658, loss 0.531009, acc 0.9375, prec 0.107997, recall 0.781529
2017-12-10T03:11:34.243971: step 7659, loss 0.243368, acc 0.9375, prec 0.108002, recall 0.781549
2017-12-10T03:11:34.508350: step 7660, loss 0.225828, acc 0.953125, prec 0.10802, recall 0.781588
2017-12-10T03:11:34.775328: step 7661, loss 0.376938, acc 0.96875, prec 0.108051, recall 0.781648
2017-12-10T03:11:35.042010: step 7662, loss 0.356379, acc 0.953125, prec 0.108058, recall 0.781667
2017-12-10T03:11:35.308968: step 7663, loss 0.504809, acc 0.90625, prec 0.108061, recall 0.781687
2017-12-10T03:11:35.577036: step 7664, loss 0.000399512, acc 1, prec 0.108061, recall 0.781687
2017-12-10T03:11:35.834240: step 7665, loss 3.73475, acc 0.9375, prec 0.108079, recall 0.781656
2017-12-10T03:11:36.100479: step 7666, loss 0.466682, acc 0.921875, prec 0.108084, recall 0.781676
2017-12-10T03:11:36.364184: step 7667, loss 0.401964, acc 0.953125, prec 0.108091, recall 0.781695
2017-12-10T03:11:36.637754: step 7668, loss 0.805416, acc 0.921875, prec 0.108084, recall 0.781695
2017-12-10T03:11:36.905663: step 7669, loss 0.113311, acc 0.96875, prec 0.108093, recall 0.781715
2017-12-10T03:11:37.179074: step 7670, loss 0.432065, acc 0.90625, prec 0.108096, recall 0.781735
2017-12-10T03:11:37.449428: step 7671, loss 0.288761, acc 0.90625, prec 0.108099, recall 0.781754
2017-12-10T03:11:37.711341: step 7672, loss 0.504188, acc 0.890625, prec 0.108089, recall 0.781754
2017-12-10T03:11:37.970976: step 7673, loss 1.04689, acc 0.90625, prec 0.108081, recall 0.781754
2017-12-10T03:11:38.236449: step 7674, loss 0.802146, acc 0.890625, prec 0.108083, recall 0.781774
2017-12-10T03:11:38.505416: step 7675, loss 1.26646, acc 0.84375, prec 0.108069, recall 0.781774
2017-12-10T03:11:38.770241: step 7676, loss 1.14036, acc 0.90625, prec 0.108072, recall 0.781794
2017-12-10T03:11:39.032921: step 7677, loss 0.889589, acc 0.890625, prec 0.108063, recall 0.781794
2017-12-10T03:11:39.297612: step 7678, loss 0.651791, acc 0.9375, prec 0.108069, recall 0.781813
2017-12-10T03:11:39.563200: step 7679, loss 0.358796, acc 0.921875, prec 0.108062, recall 0.781813
2017-12-10T03:11:39.828852: step 7680, loss 0.217347, acc 0.96875, prec 0.108059, recall 0.781813
2017-12-10T03:11:40.099345: step 7681, loss 0.607497, acc 0.9375, prec 0.108076, recall 0.781853
2017-12-10T03:11:40.371649: step 7682, loss 0.0119025, acc 1, prec 0.108087, recall 0.781872
2017-12-10T03:11:40.633379: step 7683, loss 0.17056, acc 0.984375, prec 0.108097, recall 0.781892
2017-12-10T03:11:40.898633: step 7684, loss 0.000523479, acc 1, prec 0.108108, recall 0.781912
2017-12-10T03:11:41.166322: step 7685, loss 0.530577, acc 0.921875, prec 0.108135, recall 0.78197
2017-12-10T03:11:41.431888: step 7686, loss 0.799617, acc 0.96875, prec 0.108165, recall 0.782029
2017-12-10T03:11:41.715760: step 7687, loss 0.0138585, acc 0.984375, prec 0.108164, recall 0.782029
2017-12-10T03:11:41.984707: step 7688, loss 0.625152, acc 0.9375, prec 0.108159, recall 0.782029
2017-12-10T03:11:42.245809: step 7689, loss 0.124848, acc 0.953125, prec 0.108155, recall 0.782029
2017-12-10T03:11:42.515410: step 7690, loss 0.0707386, acc 0.96875, prec 0.108152, recall 0.782029
2017-12-10T03:11:42.776381: step 7691, loss 0.120114, acc 0.984375, prec 0.108151, recall 0.782029
2017-12-10T03:11:43.048728: step 7692, loss 0.414371, acc 0.96875, prec 0.108159, recall 0.782049
2017-12-10T03:11:43.312266: step 7693, loss 0.192802, acc 0.984375, prec 0.108169, recall 0.782069
2017-12-10T03:11:43.579074: step 7694, loss 0.506512, acc 0.90625, prec 0.108183, recall 0.782108
2017-12-10T03:11:43.846648: step 7695, loss 0.621096, acc 0.9375, prec 0.1082, recall 0.782147
2017-12-10T03:11:44.116859: step 7696, loss 0.144371, acc 0.96875, prec 0.108208, recall 0.782167
2017-12-10T03:11:44.380296: step 7697, loss 0.100235, acc 0.984375, prec 0.108218, recall 0.782186
2017-12-10T03:11:44.657977: step 7698, loss 0.219216, acc 0.96875, prec 0.108215, recall 0.782186
2017-12-10T03:11:44.926847: step 7699, loss 0.0142199, acc 0.984375, prec 0.108214, recall 0.782186
2017-12-10T03:11:45.193306: step 7700, loss 0.000192973, acc 1, prec 0.108225, recall 0.782206
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-7700

2017-12-10T03:11:46.411178: step 7701, loss 0.339409, acc 0.953125, prec 0.108221, recall 0.782206
2017-12-10T03:11:46.673638: step 7702, loss 0.627476, acc 0.953125, prec 0.108228, recall 0.782225
2017-12-10T03:11:46.939303: step 7703, loss 0.337384, acc 0.984375, prec 0.108238, recall 0.782245
2017-12-10T03:11:47.209257: step 7704, loss 1.46314, acc 0.953125, prec 0.108245, recall 0.782265
2017-12-10T03:11:47.477511: step 7705, loss 0.000943245, acc 1, prec 0.108245, recall 0.782265
2017-12-10T03:11:47.741507: step 7706, loss 0.102591, acc 0.984375, prec 0.108243, recall 0.782265
2017-12-10T03:11:48.008481: step 7707, loss 0.0836657, acc 0.96875, prec 0.108252, recall 0.782284
2017-12-10T03:11:48.273033: step 7708, loss 0.362248, acc 0.96875, prec 0.108249, recall 0.782284
2017-12-10T03:11:48.539713: step 7709, loss 0.000114992, acc 1, prec 0.108249, recall 0.782284
2017-12-10T03:11:48.796938: step 7710, loss 0.186868, acc 0.96875, prec 0.108246, recall 0.782284
2017-12-10T03:11:49.067046: step 7711, loss 0.0209264, acc 1, prec 0.108257, recall 0.782304
2017-12-10T03:11:49.331197: step 7712, loss 0.00664261, acc 1, prec 0.108269, recall 0.782323
2017-12-10T03:11:49.593728: step 7713, loss 0.00535674, acc 1, prec 0.10828, recall 0.782343
2017-12-10T03:11:49.858794: step 7714, loss 0.0204805, acc 0.984375, prec 0.108289, recall 0.782362
2017-12-10T03:11:50.122597: step 7715, loss 0.00316779, acc 1, prec 0.1083, recall 0.782382
2017-12-10T03:11:50.387680: step 7716, loss 0.311868, acc 0.96875, prec 0.108298, recall 0.782382
2017-12-10T03:11:50.652513: step 7717, loss 0.092889, acc 0.984375, prec 0.108308, recall 0.782402
2017-12-10T03:11:50.917317: step 7718, loss 0.0418196, acc 0.984375, prec 0.108306, recall 0.782402
2017-12-10T03:11:51.184042: step 7719, loss 0.227344, acc 0.96875, prec 0.108326, recall 0.782441
2017-12-10T03:11:51.447377: step 7720, loss 0.106681, acc 0.984375, prec 0.108358, recall 0.782499
2017-12-10T03:11:51.715905: step 7721, loss 0.325955, acc 0.984375, prec 0.108378, recall 0.782538
2017-12-10T03:11:51.985087: step 7722, loss 0.213098, acc 0.96875, prec 0.108387, recall 0.782558
2017-12-10T03:11:52.252307: step 7723, loss 0.183981, acc 0.9375, prec 0.108393, recall 0.782577
2017-12-10T03:11:52.516480: step 7724, loss 0.0965946, acc 0.984375, prec 0.108391, recall 0.782577
2017-12-10T03:11:52.775128: step 7725, loss 0.133659, acc 0.984375, prec 0.108412, recall 0.782616
2017-12-10T03:11:53.042651: step 7726, loss 0.339356, acc 0.9375, prec 0.108429, recall 0.782656
2017-12-10T03:11:53.305472: step 7727, loss 0.220087, acc 0.96875, prec 0.108459, recall 0.782714
2017-12-10T03:11:53.571800: step 7728, loss 0.00404723, acc 1, prec 0.108459, recall 0.782714
2017-12-10T03:11:53.845519: step 7729, loss 0.152394, acc 0.96875, prec 0.108468, recall 0.782734
2017-12-10T03:11:54.111251: step 7730, loss 0.311504, acc 0.96875, prec 0.108476, recall 0.782753
2017-12-10T03:11:54.375351: step 7731, loss 0.0213093, acc 0.984375, prec 0.108486, recall 0.782773
2017-12-10T03:11:54.639031: step 7732, loss 0.1538, acc 0.96875, prec 0.108483, recall 0.782773
2017-12-10T03:11:54.904205: step 7733, loss 4.74929e-06, acc 1, prec 0.108483, recall 0.782773
2017-12-10T03:11:55.165993: step 7734, loss 0.867747, acc 0.96875, prec 0.10848, recall 0.782773
2017-12-10T03:11:55.427748: step 7735, loss 0.00114378, acc 1, prec 0.108492, recall 0.782792
2017-12-10T03:11:55.689354: step 7736, loss 0.000132018, acc 1, prec 0.108503, recall 0.782812
2017-12-10T03:11:55.952649: step 7737, loss 0.00121137, acc 1, prec 0.108503, recall 0.782812
2017-12-10T03:11:56.213097: step 7738, loss 0.133085, acc 0.96875, prec 0.108544, recall 0.782889
2017-12-10T03:11:56.483201: step 7739, loss 0.26493, acc 0.984375, prec 0.108554, recall 0.782909
2017-12-10T03:11:56.750003: step 7740, loss 0.0703982, acc 0.96875, prec 0.108551, recall 0.782909
2017-12-10T03:11:57.019647: step 7741, loss 5.94187e-05, acc 1, prec 0.108551, recall 0.782909
2017-12-10T03:11:57.283197: step 7742, loss 0.0777922, acc 0.96875, prec 0.10856, recall 0.782928
2017-12-10T03:11:57.558686: step 7743, loss 0.259468, acc 0.96875, prec 0.108568, recall 0.782948
2017-12-10T03:11:57.820628: step 7744, loss 2.23305, acc 0.984375, prec 0.108579, recall 0.782897
2017-12-10T03:11:58.087585: step 7745, loss 8.1798e-05, acc 1, prec 0.108579, recall 0.782897
2017-12-10T03:11:58.349551: step 7746, loss 0.17596, acc 0.984375, prec 0.1086, recall 0.782936
2017-12-10T03:11:58.617524: step 7747, loss 0.249051, acc 0.984375, prec 0.108599, recall 0.782936
2017-12-10T03:11:58.884719: step 7748, loss 0.0628151, acc 0.984375, prec 0.108608, recall 0.782955
2017-12-10T03:11:59.146082: step 7749, loss 0.014702, acc 0.984375, prec 0.108607, recall 0.782955
2017-12-10T03:11:59.412290: step 7750, loss 0.145361, acc 0.984375, prec 0.108606, recall 0.782955
2017-12-10T03:11:59.675464: step 7751, loss 0.700649, acc 0.984375, prec 0.108615, recall 0.782975
2017-12-10T03:11:59.937047: step 7752, loss 0.292398, acc 0.953125, prec 0.108611, recall 0.782975
2017-12-10T03:12:00.208431: step 7753, loss 0.281536, acc 0.96875, prec 0.108642, recall 0.783033
2017-12-10T03:12:00.477790: step 7754, loss 0.711828, acc 0.875, prec 0.108653, recall 0.783072
2017-12-10T03:12:00.748880: step 7755, loss 0.412215, acc 0.953125, prec 0.10866, recall 0.783092
2017-12-10T03:12:01.021323: step 7756, loss 0.0515454, acc 0.984375, prec 0.10867, recall 0.783111
2017-12-10T03:12:01.294997: step 7757, loss 0.425059, acc 0.96875, prec 0.108678, recall 0.78313
2017-12-10T03:12:01.556468: step 7758, loss 0.48793, acc 0.921875, prec 0.108683, recall 0.78315
2017-12-10T03:12:01.825977: step 7759, loss 0.560897, acc 0.953125, prec 0.108712, recall 0.783208
2017-12-10T03:12:02.098672: step 7760, loss 0.0234096, acc 1, prec 0.108723, recall 0.783227
2017-12-10T03:12:02.368886: step 7761, loss 0.0326615, acc 0.984375, prec 0.108733, recall 0.783247
2017-12-10T03:12:02.639583: step 7762, loss 0.214246, acc 0.953125, prec 0.108729, recall 0.783247
2017-12-10T03:12:02.901646: step 7763, loss 0.0969458, acc 0.984375, prec 0.108727, recall 0.783247
2017-12-10T03:12:03.165102: step 7764, loss 0.112836, acc 0.953125, prec 0.108723, recall 0.783247
2017-12-10T03:12:03.433787: step 7765, loss 0.120005, acc 0.96875, prec 0.10872, recall 0.783247
2017-12-10T03:12:03.708827: step 7766, loss 0.319516, acc 0.9375, prec 0.108715, recall 0.783247
2017-12-10T03:12:03.980804: step 7767, loss 0.032042, acc 0.984375, prec 0.108714, recall 0.783247
2017-12-10T03:12:04.244329: step 7768, loss 0.593015, acc 0.984375, prec 0.108712, recall 0.783247
2017-12-10T03:12:04.510023: step 7769, loss 0.294509, acc 0.953125, prec 0.10873, recall 0.783286
2017-12-10T03:12:04.774608: step 7770, loss 0.00365793, acc 1, prec 0.10873, recall 0.783286
2017-12-10T03:12:05.042249: step 7771, loss 0.865408, acc 0.96875, prec 0.108739, recall 0.783305
2017-12-10T03:12:05.301131: step 7772, loss 3.04593, acc 0.953125, prec 0.108758, recall 0.783274
2017-12-10T03:12:05.568371: step 7773, loss 0.00793793, acc 1, prec 0.108758, recall 0.783274
2017-12-10T03:12:05.827148: step 7774, loss 0.379762, acc 0.96875, prec 0.108778, recall 0.783312
2017-12-10T03:12:06.088466: step 7775, loss 0.0411031, acc 0.984375, prec 0.108799, recall 0.783351
2017-12-10T03:12:06.356256: step 7776, loss 0.236277, acc 0.9375, prec 0.108793, recall 0.783351
2017-12-10T03:12:06.617791: step 7777, loss 0.0537003, acc 0.984375, prec 0.108803, recall 0.783371
2017-12-10T03:12:06.887125: step 7778, loss 0.116267, acc 0.953125, prec 0.108799, recall 0.783371
2017-12-10T03:12:07.147768: step 7779, loss 0.594752, acc 0.9375, prec 0.108793, recall 0.783371
2017-12-10T03:12:07.407254: step 7780, loss 0.472812, acc 0.90625, prec 0.108785, recall 0.783371
2017-12-10T03:12:07.668873: step 7781, loss 0.67202, acc 0.921875, prec 0.108801, recall 0.783409
2017-12-10T03:12:07.935145: step 7782, loss 0.162178, acc 0.96875, prec 0.108798, recall 0.783409
2017-12-10T03:12:08.201646: step 7783, loss 0.220935, acc 0.953125, prec 0.108794, recall 0.783409
2017-12-10T03:12:08.473520: step 7784, loss 1.7123, acc 0.921875, prec 0.108831, recall 0.783487
2017-12-10T03:12:08.738759: step 7785, loss 0.120768, acc 0.96875, prec 0.108851, recall 0.783525
2017-12-10T03:12:09.000765: step 7786, loss 0.0300563, acc 0.984375, prec 0.108849, recall 0.783525
2017-12-10T03:12:09.266572: step 7787, loss 0.190141, acc 0.953125, prec 0.108879, recall 0.783583
2017-12-10T03:12:09.534937: step 7788, loss 0.291261, acc 0.953125, prec 0.108875, recall 0.783583
2017-12-10T03:12:09.805580: step 7789, loss 0.540772, acc 0.953125, prec 0.10887, recall 0.783583
2017-12-10T03:12:10.074178: step 7790, loss 0.408725, acc 0.953125, prec 0.108866, recall 0.783583
2017-12-10T03:12:10.343832: step 7791, loss 0.40025, acc 0.984375, prec 0.108876, recall 0.783603
2017-12-10T03:12:10.607439: step 7792, loss 0.0511563, acc 0.984375, prec 0.108886, recall 0.783622
2017-12-10T03:12:10.881541: step 7793, loss 0.020897, acc 0.984375, prec 0.108896, recall 0.783641
2017-12-10T03:12:11.146029: step 7794, loss 0.89829, acc 0.921875, prec 0.1089, recall 0.783661
2017-12-10T03:12:11.407463: step 7795, loss 0.00253591, acc 1, prec 0.1089, recall 0.783661
2017-12-10T03:12:11.683060: step 7796, loss 0.126414, acc 0.984375, prec 0.10891, recall 0.78368
2017-12-10T03:12:11.951861: step 7797, loss 0.0682214, acc 0.96875, prec 0.108907, recall 0.78368
2017-12-10T03:12:12.220391: step 7798, loss 0.0628379, acc 0.984375, prec 0.108905, recall 0.78368
2017-12-10T03:12:12.480715: step 7799, loss 0.684311, acc 0.984375, prec 0.108915, recall 0.783699
2017-12-10T03:12:12.746508: step 7800, loss 0.267609, acc 0.953125, prec 0.108933, recall 0.783738

Evaluation:
2017-12-10T03:12:20.352263: step 7800, loss 13.8711, acc 0.958392, prec 0.109101, recall 0.777358

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-7800

2017-12-10T03:12:21.789957: step 7801, loss 0.367461, acc 0.96875, prec 0.109121, recall 0.777397
2017-12-10T03:12:22.053631: step 7802, loss 0.166046, acc 0.9375, prec 0.109126, recall 0.777416
2017-12-10T03:12:22.319043: step 7803, loss 0.000194944, acc 1, prec 0.109126, recall 0.777416
2017-12-10T03:12:22.582577: step 7804, loss 0.208452, acc 0.984375, prec 0.109136, recall 0.777436
2017-12-10T03:12:22.857531: step 7805, loss 0.2965, acc 0.953125, prec 0.109143, recall 0.777455
2017-12-10T03:12:23.122205: step 7806, loss 0.00429823, acc 1, prec 0.109143, recall 0.777455
2017-12-10T03:12:23.384058: step 7807, loss 0.0668211, acc 0.984375, prec 0.109142, recall 0.777455
2017-12-10T03:12:23.651059: step 7808, loss 0.0889792, acc 0.96875, prec 0.109139, recall 0.777455
2017-12-10T03:12:23.915668: step 7809, loss 0.51323, acc 0.96875, prec 0.109158, recall 0.777494
2017-12-10T03:12:24.184533: step 7810, loss 0.156528, acc 0.96875, prec 0.109167, recall 0.777514
2017-12-10T03:12:24.455835: step 7811, loss 0.0163495, acc 0.984375, prec 0.109187, recall 0.777553
2017-12-10T03:12:24.719762: step 7812, loss 0.0263909, acc 1, prec 0.109198, recall 0.777573
2017-12-10T03:12:24.993855: step 7813, loss 0.0314216, acc 0.984375, prec 0.109208, recall 0.777592
2017-12-10T03:12:25.262333: step 7814, loss 0.874046, acc 0.984375, prec 0.109206, recall 0.777592
2017-12-10T03:12:25.527254: step 7815, loss 0.370032, acc 0.9375, prec 0.109201, recall 0.777592
2017-12-10T03:12:25.797184: step 7816, loss 0.000799202, acc 1, prec 0.109223, recall 0.777631
2017-12-10T03:12:26.056504: step 7817, loss 0.131629, acc 0.984375, prec 0.109255, recall 0.77769
2017-12-10T03:12:26.326127: step 7818, loss 0.0153597, acc 0.984375, prec 0.109253, recall 0.77769
2017-12-10T03:12:26.598056: step 7819, loss 0.000123158, acc 1, prec 0.109253, recall 0.77769
2017-12-10T03:12:26.863466: step 7820, loss 8.27524, acc 0.96875, prec 0.109252, recall 0.777622
2017-12-10T03:12:27.128705: step 7821, loss 0.00205842, acc 1, prec 0.109252, recall 0.777622
2017-12-10T03:12:27.399833: step 7822, loss 0.0662736, acc 0.984375, prec 0.109262, recall 0.777641
2017-12-10T03:12:27.660395: step 7823, loss 0.373305, acc 0.96875, prec 0.109259, recall 0.777641
2017-12-10T03:12:27.925854: step 7824, loss 0.87897, acc 0.96875, prec 0.109278, recall 0.77768
2017-12-10T03:12:28.188608: step 7825, loss 0.327886, acc 0.953125, prec 0.109285, recall 0.7777
2017-12-10T03:12:28.464149: step 7826, loss 0.321302, acc 0.96875, prec 0.109282, recall 0.7777
2017-12-10T03:12:28.729753: step 7827, loss 0.667314, acc 0.921875, prec 0.109298, recall 0.777739
2017-12-10T03:12:28.996762: step 7828, loss 0.539881, acc 0.90625, prec 0.109301, recall 0.777758
2017-12-10T03:12:29.260442: step 7829, loss 0.20279, acc 0.953125, prec 0.109341, recall 0.777836
2017-12-10T03:12:29.531215: step 7830, loss 0.58993, acc 0.9375, prec 0.109346, recall 0.777856
2017-12-10T03:12:29.796773: step 7831, loss 0.449454, acc 0.953125, prec 0.109375, recall 0.777914
2017-12-10T03:12:30.069020: step 7832, loss 0.571671, acc 0.890625, prec 0.109377, recall 0.777934
2017-12-10T03:12:30.337640: step 7833, loss 1.86234, acc 0.875, prec 0.109377, recall 0.777953
2017-12-10T03:12:30.604667: step 7834, loss 0.287787, acc 0.953125, prec 0.109373, recall 0.777953
2017-12-10T03:12:30.868604: step 7835, loss 0.568595, acc 0.9375, prec 0.109378, recall 0.777973
2017-12-10T03:12:31.130226: step 7836, loss 0.410471, acc 0.953125, prec 0.109385, recall 0.777992
2017-12-10T03:12:31.396099: step 7837, loss 0.411798, acc 0.953125, prec 0.109381, recall 0.777992
2017-12-10T03:12:31.664400: step 7838, loss 0.634173, acc 0.953125, prec 0.109377, recall 0.777992
2017-12-10T03:12:31.924966: step 7839, loss 0.309023, acc 0.9375, prec 0.109405, recall 0.77805
2017-12-10T03:12:32.191312: step 7840, loss 0.445732, acc 0.953125, prec 0.109412, recall 0.77807
2017-12-10T03:12:32.466299: step 7841, loss 0.193511, acc 0.9375, prec 0.109428, recall 0.778109
2017-12-10T03:12:32.733128: step 7842, loss 0.338064, acc 0.921875, prec 0.109421, recall 0.778109
2017-12-10T03:12:33.003273: step 7843, loss 0.303885, acc 0.953125, prec 0.109417, recall 0.778109
2017-12-10T03:12:33.274219: step 7844, loss 0.227776, acc 0.96875, prec 0.109415, recall 0.778109
2017-12-10T03:12:33.539551: step 7845, loss 0.0933901, acc 0.984375, prec 0.109413, recall 0.778109
2017-12-10T03:12:33.815128: step 7846, loss 0.00452368, acc 1, prec 0.109424, recall 0.778128
2017-12-10T03:12:34.075801: step 7847, loss 0.427328, acc 0.953125, prec 0.109431, recall 0.778148
2017-12-10T03:12:34.350384: step 7848, loss 0.205286, acc 0.953125, prec 0.109438, recall 0.778167
2017-12-10T03:12:34.623109: step 7849, loss 0.0802909, acc 0.984375, prec 0.109437, recall 0.778167
2017-12-10T03:12:34.896835: step 7850, loss 0.227025, acc 0.96875, prec 0.109445, recall 0.778187
2017-12-10T03:12:35.166360: step 7851, loss 0.163669, acc 0.953125, prec 0.109463, recall 0.778225
2017-12-10T03:12:35.432713: step 7852, loss 0.0192681, acc 0.984375, prec 0.109473, recall 0.778245
2017-12-10T03:12:35.698442: step 7853, loss 0.0648865, acc 0.984375, prec 0.109471, recall 0.778245
2017-12-10T03:12:35.961736: step 7854, loss 0.0271318, acc 0.984375, prec 0.10947, recall 0.778245
2017-12-10T03:12:36.220185: step 7855, loss 0.128347, acc 0.984375, prec 0.109512, recall 0.778323
2017-12-10T03:12:36.486676: step 7856, loss 0.968177, acc 0.96875, prec 0.109532, recall 0.778361
2017-12-10T03:12:36.756668: step 7857, loss 0.224774, acc 0.953125, prec 0.109528, recall 0.778361
2017-12-10T03:12:37.022301: step 7858, loss 1.51878e-05, acc 1, prec 0.109539, recall 0.778381
2017-12-10T03:12:37.286595: step 7859, loss 0.319517, acc 0.96875, prec 0.109547, recall 0.7784
2017-12-10T03:12:37.547719: step 7860, loss 0.00967258, acc 1, prec 0.109558, recall 0.77842
2017-12-10T03:12:37.818550: step 7861, loss 0.253875, acc 0.96875, prec 0.109577, recall 0.778458
2017-12-10T03:12:38.087030: step 7862, loss 0.820862, acc 0.96875, prec 0.109585, recall 0.778478
2017-12-10T03:12:38.351991: step 7863, loss 8.67974e-07, acc 1, prec 0.109585, recall 0.778478
2017-12-10T03:12:38.614608: step 7864, loss 4.2243, acc 0.984375, prec 0.109585, recall 0.77841
2017-12-10T03:12:38.882649: step 7865, loss 0.032829, acc 0.984375, prec 0.109606, recall 0.778448
2017-12-10T03:12:39.151216: step 7866, loss 6.20171, acc 0.96875, prec 0.109605, recall 0.77838
2017-12-10T03:12:39.423278: step 7867, loss 0.192685, acc 0.96875, prec 0.109613, recall 0.7784
2017-12-10T03:12:39.687816: step 7868, loss 0.0713662, acc 0.984375, prec 0.109622, recall 0.778419
2017-12-10T03:12:39.948716: step 7869, loss 0.119712, acc 0.96875, prec 0.109631, recall 0.778438
2017-12-10T03:12:40.215465: step 7870, loss 0.484463, acc 0.96875, prec 0.109661, recall 0.778497
2017-12-10T03:12:40.482515: step 7871, loss 0.386706, acc 0.953125, prec 0.109668, recall 0.778516
2017-12-10T03:12:40.753466: step 7872, loss 0.448745, acc 0.9375, prec 0.109673, recall 0.778535
2017-12-10T03:12:41.019740: step 7873, loss 0.456674, acc 0.953125, prec 0.109713, recall 0.778613
2017-12-10T03:12:41.297725: step 7874, loss 0.067359, acc 0.96875, prec 0.109732, recall 0.778651
2017-12-10T03:12:41.574435: step 7875, loss 0.24949, acc 0.9375, prec 0.109749, recall 0.77869
2017-12-10T03:12:41.845897: step 7876, loss 0.603875, acc 0.84375, prec 0.109735, recall 0.77869
2017-12-10T03:12:42.109772: step 7877, loss 0.0545482, acc 0.984375, prec 0.109756, recall 0.778729
2017-12-10T03:12:42.373993: step 7878, loss 0.75037, acc 0.953125, prec 0.109785, recall 0.778787
2017-12-10T03:12:42.639897: step 7879, loss 1.58718, acc 0.75, prec 0.109763, recall 0.778787
2017-12-10T03:12:42.908146: step 7880, loss 0.228081, acc 0.9375, prec 0.109758, recall 0.778787
2017-12-10T03:12:43.174592: step 7881, loss 0.767372, acc 0.890625, prec 0.109781, recall 0.778844
2017-12-10T03:12:43.439351: step 7882, loss 0.416194, acc 0.875, prec 0.109792, recall 0.778883
2017-12-10T03:12:43.704535: step 7883, loss 0.368657, acc 0.953125, prec 0.109788, recall 0.778883
2017-12-10T03:12:43.970299: step 7884, loss 1.25315, acc 0.921875, prec 0.109825, recall 0.77896
2017-12-10T03:12:44.238916: step 7885, loss 0.914068, acc 0.9375, prec 0.10982, recall 0.77896
2017-12-10T03:12:44.506410: step 7886, loss 0.742322, acc 0.921875, prec 0.109813, recall 0.77896
2017-12-10T03:12:44.774334: step 7887, loss 0.436548, acc 0.953125, prec 0.10982, recall 0.778979
2017-12-10T03:12:45.044742: step 7888, loss 0.337286, acc 0.953125, prec 0.109816, recall 0.778979
2017-12-10T03:12:45.306461: step 7889, loss 0.128671, acc 0.96875, prec 0.109824, recall 0.778999
2017-12-10T03:12:45.570657: step 7890, loss 0.35806, acc 0.9375, prec 0.10983, recall 0.779018
2017-12-10T03:12:45.834538: step 7891, loss 0.104913, acc 0.984375, prec 0.109828, recall 0.779018
2017-12-10T03:12:46.098114: step 7892, loss 0.123788, acc 0.984375, prec 0.109838, recall 0.779037
2017-12-10T03:12:46.363996: step 7893, loss 0.209565, acc 0.953125, prec 0.109845, recall 0.779057
2017-12-10T03:12:46.637002: step 7894, loss 0.0174615, acc 0.984375, prec 0.109865, recall 0.779095
2017-12-10T03:12:46.903456: step 7895, loss 0.256503, acc 0.953125, prec 0.109883, recall 0.779134
2017-12-10T03:12:47.168537: step 7896, loss 1.22927, acc 0.96875, prec 0.109882, recall 0.779066
2017-12-10T03:12:47.444150: step 7897, loss 0.0895721, acc 0.96875, prec 0.109879, recall 0.779066
2017-12-10T03:12:47.712937: step 7898, loss 1.10884, acc 0.9375, prec 0.109874, recall 0.779066
2017-12-10T03:12:47.982719: step 7899, loss 0.306971, acc 0.96875, prec 0.109904, recall 0.779123
2017-12-10T03:12:48.253127: step 7900, loss 0.0967308, acc 0.953125, prec 0.109911, recall 0.779143
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-7900

2017-12-10T03:12:49.500498: step 7901, loss 0.232334, acc 0.96875, prec 0.109908, recall 0.779143
2017-12-10T03:12:49.766665: step 7902, loss 0.199002, acc 0.96875, prec 0.109938, recall 0.7792
2017-12-10T03:12:50.032617: step 7903, loss 0.0815756, acc 0.96875, prec 0.109979, recall 0.779277
2017-12-10T03:12:50.301702: step 7904, loss 0.324001, acc 0.953125, prec 0.109986, recall 0.779297
2017-12-10T03:12:50.563760: step 7905, loss 0.103688, acc 0.984375, prec 0.109985, recall 0.779297
2017-12-10T03:12:50.829753: step 7906, loss 0.0690126, acc 0.96875, prec 0.110015, recall 0.779354
2017-12-10T03:12:51.093075: step 7907, loss 0.11243, acc 0.953125, prec 0.110022, recall 0.779373
2017-12-10T03:12:51.359502: step 7908, loss 0.377457, acc 0.953125, prec 0.110018, recall 0.779373
2017-12-10T03:12:51.630581: step 7909, loss 0.000621976, acc 1, prec 0.110018, recall 0.779373
2017-12-10T03:12:51.893312: step 7910, loss 0.157428, acc 0.96875, prec 0.110015, recall 0.779373
2017-12-10T03:12:52.160542: step 7911, loss 0.53934, acc 0.953125, prec 0.110011, recall 0.779373
2017-12-10T03:12:52.426018: step 7912, loss 0.0252591, acc 0.984375, prec 0.110021, recall 0.779393
2017-12-10T03:12:52.696061: step 7913, loss 0.167024, acc 0.984375, prec 0.11003, recall 0.779412
2017-12-10T03:12:52.957702: step 7914, loss 0.139426, acc 0.953125, prec 0.110048, recall 0.77945
2017-12-10T03:12:53.222208: step 7915, loss 0.764154, acc 0.90625, prec 0.11004, recall 0.77945
2017-12-10T03:12:53.488084: step 7916, loss 0.244311, acc 0.96875, prec 0.11007, recall 0.779508
2017-12-10T03:12:53.749090: step 7917, loss 0.192728, acc 0.96875, prec 0.110078, recall 0.779527
2017-12-10T03:12:54.014483: step 7918, loss 0.269802, acc 0.984375, prec 0.110077, recall 0.779527
2017-12-10T03:12:54.284797: step 7919, loss 0.0422313, acc 0.984375, prec 0.110086, recall 0.779546
2017-12-10T03:12:54.548659: step 7920, loss 0.088937, acc 0.96875, prec 0.110116, recall 0.779604
2017-12-10T03:12:54.820945: step 7921, loss 0.196407, acc 0.984375, prec 0.110137, recall 0.779642
2017-12-10T03:12:55.085622: step 7922, loss 0.0868304, acc 0.984375, prec 0.110136, recall 0.779642
2017-12-10T03:12:55.355345: step 7923, loss 0.000314759, acc 1, prec 0.110136, recall 0.779642
2017-12-10T03:12:55.617508: step 7924, loss 0.0213776, acc 1, prec 0.110136, recall 0.779642
2017-12-10T03:12:55.884045: step 7925, loss 0.0237457, acc 0.984375, prec 0.110134, recall 0.779642
2017-12-10T03:12:56.147571: step 7926, loss 0.444791, acc 0.9375, prec 0.110129, recall 0.779642
2017-12-10T03:12:56.418599: step 7927, loss 0.0273233, acc 0.984375, prec 0.110138, recall 0.779661
2017-12-10T03:12:56.693506: step 7928, loss 0.0218998, acc 0.984375, prec 0.110137, recall 0.779661
2017-12-10T03:12:56.953458: step 7929, loss 0.549325, acc 0.96875, prec 0.110156, recall 0.779699
2017-12-10T03:12:57.218047: step 7930, loss 0.0568696, acc 0.984375, prec 0.110155, recall 0.779699
2017-12-10T03:12:57.482020: step 7931, loss 0.0426788, acc 0.984375, prec 0.110153, recall 0.779699
2017-12-10T03:12:57.750184: step 7932, loss 0.00334885, acc 1, prec 0.110153, recall 0.779699
2017-12-10T03:12:58.017202: step 7933, loss 0.964472, acc 0.984375, prec 0.110152, recall 0.779699
2017-12-10T03:12:58.279939: step 7934, loss 0.0579904, acc 0.96875, prec 0.110149, recall 0.779699
2017-12-10T03:12:58.543988: step 7935, loss 0.0140059, acc 1, prec 0.11016, recall 0.779718
2017-12-10T03:12:58.812282: step 7936, loss 0.424087, acc 0.953125, prec 0.110167, recall 0.779738
2017-12-10T03:12:59.076840: step 7937, loss 0.882951, acc 0.96875, prec 0.110186, recall 0.779776
2017-12-10T03:12:59.340743: step 7938, loss 0.000829461, acc 1, prec 0.110186, recall 0.779776
2017-12-10T03:12:59.600361: step 7939, loss 0.0266129, acc 0.984375, prec 0.110185, recall 0.779776
2017-12-10T03:12:59.860922: step 7940, loss 0.669459, acc 0.96875, prec 0.110182, recall 0.779776
2017-12-10T03:13:00.135496: step 7941, loss 0.030453, acc 0.984375, prec 0.110181, recall 0.779776
2017-12-10T03:13:00.407326: step 7942, loss 0.015406, acc 1, prec 0.110203, recall 0.779814
2017-12-10T03:13:00.671909: step 7943, loss 0.451926, acc 0.96875, prec 0.110211, recall 0.779833
2017-12-10T03:13:00.934054: step 7944, loss 0.646479, acc 0.96875, prec 0.110263, recall 0.779929
2017-12-10T03:13:01.203535: step 7945, loss 0.13315, acc 0.96875, prec 0.11026, recall 0.779929
2017-12-10T03:13:01.479017: step 7946, loss 0.0202421, acc 0.984375, prec 0.110259, recall 0.779929
2017-12-10T03:13:01.741911: step 7947, loss 0.16496, acc 0.984375, prec 0.110257, recall 0.779929
2017-12-10T03:13:02.010792: step 7948, loss 0.0112653, acc 1, prec 0.110257, recall 0.779929
2017-12-10T03:13:02.274337: step 7949, loss 0.00489087, acc 1, prec 0.110301, recall 0.780005
2017-12-10T03:13:02.535803: step 7950, loss 0.313415, acc 0.984375, prec 0.110311, recall 0.780024
2017-12-10T03:13:02.805869: step 7951, loss 0.0899613, acc 0.96875, prec 0.110341, recall 0.780082
2017-12-10T03:13:03.031612: step 7952, loss 0.000477965, acc 1, prec 0.110352, recall 0.780101
2017-12-10T03:13:03.301341: step 7953, loss 0.0129325, acc 0.984375, prec 0.110361, recall 0.78012
2017-12-10T03:13:03.573028: step 7954, loss 0.328085, acc 0.96875, prec 0.110359, recall 0.78012
2017-12-10T03:13:03.840322: step 7955, loss 0.280221, acc 0.953125, prec 0.110365, recall 0.780139
2017-12-10T03:13:04.104345: step 7956, loss 7.49782e-05, acc 1, prec 0.110376, recall 0.780158
2017-12-10T03:13:04.368927: step 7957, loss 0.167691, acc 0.984375, prec 0.110419, recall 0.780234
2017-12-10T03:13:04.636303: step 7958, loss 0.00252066, acc 1, prec 0.110451, recall 0.780291
2017-12-10T03:13:04.893678: step 7959, loss 1.97252, acc 0.96875, prec 0.110472, recall 0.780262
2017-12-10T03:13:05.162814: step 7960, loss 0.461791, acc 0.96875, prec 0.11048, recall 0.780281
2017-12-10T03:13:05.428996: step 7961, loss 0.0861276, acc 0.96875, prec 0.110488, recall 0.7803
2017-12-10T03:13:05.693079: step 7962, loss 0.04668, acc 0.96875, prec 0.110507, recall 0.780338
2017-12-10T03:13:05.957144: step 7963, loss 1.08713, acc 0.953125, prec 0.110503, recall 0.780338
2017-12-10T03:13:06.220665: step 7964, loss 0.0228523, acc 0.984375, prec 0.110524, recall 0.780376
2017-12-10T03:13:06.484311: step 7965, loss 0.220424, acc 0.984375, prec 0.110522, recall 0.780376
2017-12-10T03:13:06.752058: step 7966, loss 0.11088, acc 0.96875, prec 0.11052, recall 0.780376
2017-12-10T03:13:07.019957: step 7967, loss 0.0104683, acc 1, prec 0.11052, recall 0.780376
2017-12-10T03:13:07.287486: step 7968, loss 0.0571369, acc 0.984375, prec 0.110529, recall 0.780395
2017-12-10T03:13:07.554151: step 7969, loss 0.00102321, acc 1, prec 0.110551, recall 0.780433
2017-12-10T03:13:07.825714: step 7970, loss 0.22574, acc 0.984375, prec 0.110571, recall 0.780471
2017-12-10T03:13:08.089110: step 7971, loss 0.337758, acc 0.9375, prec 0.110577, recall 0.78049
2017-12-10T03:13:08.354110: step 7972, loss 0.137754, acc 0.96875, prec 0.110585, recall 0.780509
2017-12-10T03:13:08.616810: step 7973, loss 0.0994682, acc 0.96875, prec 0.110593, recall 0.780528
2017-12-10T03:13:08.880889: step 7974, loss 0.840407, acc 0.9375, prec 0.110599, recall 0.780547
2017-12-10T03:13:09.145129: step 7975, loss 0.116644, acc 0.984375, prec 0.110597, recall 0.780547
2017-12-10T03:13:09.409587: step 7976, loss 0.118505, acc 0.984375, prec 0.110596, recall 0.780547
2017-12-10T03:13:09.675504: step 7977, loss 0.671751, acc 0.9375, prec 0.110602, recall 0.780566
2017-12-10T03:13:09.939519: step 7978, loss 0.112553, acc 0.96875, prec 0.110632, recall 0.780623
2017-12-10T03:13:10.209613: step 7979, loss 0.0313871, acc 0.984375, prec 0.11063, recall 0.780623
2017-12-10T03:13:10.474073: step 7980, loss 0.369142, acc 0.96875, prec 0.110638, recall 0.780642
2017-12-10T03:13:10.741614: step 7981, loss 0.00159261, acc 1, prec 0.110638, recall 0.780642
2017-12-10T03:13:11.006035: step 7982, loss 0.175317, acc 0.953125, prec 0.110634, recall 0.780642
2017-12-10T03:13:11.269711: step 7983, loss 0.162516, acc 0.96875, prec 0.110643, recall 0.780661
2017-12-10T03:13:11.539178: step 7984, loss 0.0200936, acc 0.984375, prec 0.110641, recall 0.780661
2017-12-10T03:13:11.812833: step 7985, loss 0.0770556, acc 1, prec 0.110674, recall 0.780718
2017-12-10T03:13:12.077549: step 7986, loss 0.362035, acc 0.953125, prec 0.110713, recall 0.780793
2017-12-10T03:13:12.342205: step 7987, loss 0.0549103, acc 0.984375, prec 0.110745, recall 0.78085
2017-12-10T03:13:12.605974: step 7988, loss 0.1556, acc 0.984375, prec 0.110743, recall 0.78085
2017-12-10T03:13:12.872153: step 7989, loss 0.178819, acc 0.984375, prec 0.110753, recall 0.780869
2017-12-10T03:13:13.137983: step 7990, loss 0.693656, acc 0.9375, prec 0.110758, recall 0.780888
2017-12-10T03:13:13.401643: step 7991, loss 0.24696, acc 0.96875, prec 0.110756, recall 0.780888
2017-12-10T03:13:13.664006: step 7992, loss 3.04731, acc 0.96875, prec 0.110787, recall 0.780878
2017-12-10T03:13:13.936684: step 7993, loss 0.196663, acc 0.96875, prec 0.110795, recall 0.780896
2017-12-10T03:13:14.201803: step 7994, loss 0.00487359, acc 1, prec 0.110795, recall 0.780896
2017-12-10T03:13:14.472029: step 7995, loss 0.0904453, acc 0.96875, prec 0.110793, recall 0.780896
2017-12-10T03:13:14.739624: step 7996, loss 0.116791, acc 0.953125, prec 0.110799, recall 0.780915
2017-12-10T03:13:15.001233: step 7997, loss 0.483906, acc 0.953125, prec 0.110795, recall 0.780915
2017-12-10T03:13:15.270528: step 7998, loss 0.266928, acc 0.953125, prec 0.110791, recall 0.780915
2017-12-10T03:13:15.537656: step 7999, loss 0.522966, acc 0.96875, prec 0.110799, recall 0.780934
2017-12-10T03:13:15.808315: step 8000, loss 0.326921, acc 0.984375, prec 0.110809, recall 0.780953
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-8000

2017-12-10T03:13:17.072442: step 8001, loss 0.508064, acc 0.9375, prec 0.110814, recall 0.780972
2017-12-10T03:13:17.339057: step 8002, loss 0.469077, acc 0.921875, prec 0.110818, recall 0.780991
2017-12-10T03:13:17.604411: step 8003, loss 0.462831, acc 0.90625, prec 0.110821, recall 0.78101
2017-12-10T03:13:17.880600: step 8004, loss 1.30123, acc 0.953125, prec 0.110828, recall 0.781029
2017-12-10T03:13:18.146680: step 8005, loss 0.325604, acc 0.9375, prec 0.110866, recall 0.781104
2017-12-10T03:13:18.412865: step 8006, loss 0.163452, acc 0.953125, prec 0.110895, recall 0.781161
2017-12-10T03:13:18.675117: step 8007, loss 0.201943, acc 0.96875, prec 0.110892, recall 0.781161
2017-12-10T03:13:18.938714: step 8008, loss 0.159985, acc 0.953125, prec 0.11091, recall 0.781199
2017-12-10T03:13:19.212068: step 8009, loss 0.0405645, acc 0.984375, prec 0.110908, recall 0.781199
2017-12-10T03:13:19.476828: step 8010, loss 3.09866, acc 0.96875, prec 0.110907, recall 0.781131
2017-12-10T03:13:19.744487: step 8011, loss 0.398535, acc 0.9375, prec 0.110902, recall 0.781131
2017-12-10T03:13:20.005518: step 8012, loss 0.196121, acc 0.9375, prec 0.110907, recall 0.78115
2017-12-10T03:13:20.268270: step 8013, loss 0.447087, acc 0.921875, prec 0.1109, recall 0.78115
2017-12-10T03:13:20.536330: step 8014, loss 0.498086, acc 0.953125, prec 0.110918, recall 0.781188
2017-12-10T03:13:20.800448: step 8015, loss 0.357753, acc 0.953125, prec 0.110914, recall 0.781188
2017-12-10T03:13:21.065728: step 8016, loss 0.580784, acc 0.90625, prec 0.110917, recall 0.781207
2017-12-10T03:13:21.330128: step 8017, loss 0.573179, acc 0.921875, prec 0.110921, recall 0.781226
2017-12-10T03:13:21.593971: step 8018, loss 0.463941, acc 0.9375, prec 0.110926, recall 0.781245
2017-12-10T03:13:21.857757: step 8019, loss 1.06223, acc 0.859375, prec 0.110925, recall 0.781263
2017-12-10T03:13:22.120115: step 8020, loss 0.721132, acc 0.9375, prec 0.11093, recall 0.781282
2017-12-10T03:13:22.387146: step 8021, loss 0.166179, acc 0.9375, prec 0.110925, recall 0.781282
2017-12-10T03:13:22.658289: step 8022, loss 0.666795, acc 0.90625, prec 0.110917, recall 0.781282
2017-12-10T03:13:22.921879: step 8023, loss 0.401066, acc 0.90625, prec 0.110919, recall 0.781301
2017-12-10T03:13:23.182169: step 8024, loss 1.14039, acc 0.875, prec 0.11093, recall 0.781339
2017-12-10T03:13:23.450100: step 8025, loss 0.630539, acc 0.859375, prec 0.110951, recall 0.781395
2017-12-10T03:13:23.717383: step 8026, loss 0.471923, acc 0.953125, prec 0.110958, recall 0.781414
2017-12-10T03:13:23.983235: step 8027, loss 0.348991, acc 0.875, prec 0.110947, recall 0.781414
2017-12-10T03:13:24.251881: step 8028, loss 0.159447, acc 0.921875, prec 0.110962, recall 0.781452
2017-12-10T03:13:24.514170: step 8029, loss 0.117717, acc 0.96875, prec 0.11097, recall 0.781471
2017-12-10T03:13:24.782310: step 8030, loss 0.127884, acc 0.984375, prec 0.110979, recall 0.781489
2017-12-10T03:13:25.045879: step 8031, loss 0.25614, acc 0.953125, prec 0.110975, recall 0.781489
2017-12-10T03:13:25.311418: step 8032, loss 0.154288, acc 0.953125, prec 0.110971, recall 0.781489
2017-12-10T03:13:25.576056: step 8033, loss 0.00506753, acc 1, prec 0.110982, recall 0.781508
2017-12-10T03:13:25.845760: step 8034, loss 0.300918, acc 0.9375, prec 0.110998, recall 0.781546
2017-12-10T03:13:26.118507: step 8035, loss 0.169414, acc 0.96875, prec 0.110996, recall 0.781546
2017-12-10T03:13:26.379263: step 8036, loss 0.00381716, acc 1, prec 0.110996, recall 0.781546
2017-12-10T03:13:26.658231: step 8037, loss 0.11681, acc 0.953125, prec 0.110992, recall 0.781546
2017-12-10T03:13:26.921126: step 8038, loss 0.36674, acc 0.984375, prec 0.11099, recall 0.781546
2017-12-10T03:13:27.192254: step 8039, loss 0.415746, acc 0.953125, prec 0.111019, recall 0.781602
2017-12-10T03:13:27.456127: step 8040, loss 0.171281, acc 0.984375, prec 0.111028, recall 0.781621
2017-12-10T03:13:27.720177: step 8041, loss 0.477678, acc 0.953125, prec 0.111046, recall 0.781659
2017-12-10T03:13:27.983791: step 8042, loss 0.0518566, acc 0.984375, prec 0.111077, recall 0.781715
2017-12-10T03:13:28.249416: step 8043, loss 0.351307, acc 0.96875, prec 0.111096, recall 0.781753
2017-12-10T03:13:28.510279: step 8044, loss 0.283946, acc 0.953125, prec 0.111092, recall 0.781753
2017-12-10T03:13:28.773628: step 8045, loss 1.09509e-05, acc 1, prec 0.111092, recall 0.781753
2017-12-10T03:13:29.034867: step 8046, loss 3.68044e-06, acc 1, prec 0.111092, recall 0.781753
2017-12-10T03:13:29.299632: step 8047, loss 1.00104, acc 0.96875, prec 0.1111, recall 0.781771
2017-12-10T03:13:29.561806: step 8048, loss 0.219203, acc 1, prec 0.111133, recall 0.781828
2017-12-10T03:13:29.835229: step 8049, loss 0.226269, acc 0.953125, prec 0.111129, recall 0.781828
2017-12-10T03:13:30.107493: step 8050, loss 0.694336, acc 0.984375, prec 0.11116, recall 0.781884
2017-12-10T03:13:30.378555: step 8051, loss 0.332863, acc 0.984375, prec 0.111169, recall 0.781903
2017-12-10T03:13:30.649664: step 8052, loss 0.133977, acc 0.984375, prec 0.111179, recall 0.781921
2017-12-10T03:13:30.915118: step 8053, loss 0.27721, acc 0.96875, prec 0.111187, recall 0.78194
2017-12-10T03:13:31.187381: step 8054, loss 0.0385199, acc 0.984375, prec 0.111197, recall 0.781959
2017-12-10T03:13:31.459766: step 8055, loss 1.02115e-05, acc 1, prec 0.111218, recall 0.781996
2017-12-10T03:13:31.721419: step 8056, loss 0.357112, acc 0.984375, prec 0.111217, recall 0.781996
2017-12-10T03:13:31.988590: step 8057, loss 0.373654, acc 0.96875, prec 0.111247, recall 0.782052
2017-12-10T03:13:32.253349: step 8058, loss 0.312657, acc 0.984375, prec 0.111245, recall 0.782052
2017-12-10T03:13:32.519419: step 8059, loss 0.0044273, acc 1, prec 0.111245, recall 0.782052
2017-12-10T03:13:32.782561: step 8060, loss 0.28227, acc 0.96875, prec 0.111254, recall 0.782071
2017-12-10T03:13:33.047439: step 8061, loss 0.217837, acc 0.96875, prec 0.111251, recall 0.782071
2017-12-10T03:13:33.307299: step 8062, loss 0.615378, acc 0.9375, prec 0.111256, recall 0.78209
2017-12-10T03:13:33.577158: step 8063, loss 0.157864, acc 0.984375, prec 0.111255, recall 0.78209
2017-12-10T03:13:33.852488: step 8064, loss 0.0795327, acc 0.96875, prec 0.111263, recall 0.782109
2017-12-10T03:13:34.116011: step 8065, loss 0.479413, acc 0.984375, prec 0.111273, recall 0.782127
2017-12-10T03:13:34.380777: step 8066, loss 0.0655533, acc 0.984375, prec 0.111271, recall 0.782127
2017-12-10T03:13:34.645025: step 8067, loss 0.0253497, acc 0.984375, prec 0.11127, recall 0.782127
2017-12-10T03:13:34.905893: step 8068, loss 0.000927749, acc 1, prec 0.11127, recall 0.782127
2017-12-10T03:13:35.167284: step 8069, loss 3.99444e-05, acc 1, prec 0.11127, recall 0.782127
2017-12-10T03:13:35.434538: step 8070, loss 1.13291, acc 0.984375, prec 0.11127, recall 0.78206
2017-12-10T03:13:35.702183: step 8071, loss 0.000177885, acc 1, prec 0.11127, recall 0.78206
2017-12-10T03:13:35.964987: step 8072, loss 0.176318, acc 0.984375, prec 0.111269, recall 0.78206
2017-12-10T03:13:36.227395: step 8073, loss 0.0230065, acc 0.984375, prec 0.111278, recall 0.782079
2017-12-10T03:13:36.487853: step 8074, loss 0.0772853, acc 0.96875, prec 0.111275, recall 0.782079
2017-12-10T03:13:36.752091: step 8075, loss 0.185168, acc 0.96875, prec 0.111273, recall 0.782079
2017-12-10T03:13:37.023524: step 8076, loss 0.384223, acc 0.953125, prec 0.11129, recall 0.782116
2017-12-10T03:13:37.296014: step 8077, loss 0.000890545, acc 1, prec 0.111301, recall 0.782135
2017-12-10T03:13:37.558354: step 8078, loss 0.34424, acc 0.984375, prec 0.111311, recall 0.782154
2017-12-10T03:13:37.823334: step 8079, loss 0.00253601, acc 1, prec 0.111321, recall 0.782172
2017-12-10T03:13:38.089980: step 8080, loss 0.0407734, acc 0.984375, prec 0.111331, recall 0.782191
2017-12-10T03:13:38.357296: step 8081, loss 0.0020974, acc 1, prec 0.111331, recall 0.782191
2017-12-10T03:13:38.626274: step 8082, loss 0.0683831, acc 0.984375, prec 0.11134, recall 0.78221
2017-12-10T03:13:38.893914: step 8083, loss 0.0955659, acc 0.984375, prec 0.111339, recall 0.78221
2017-12-10T03:13:39.157857: step 8084, loss 0.399229, acc 0.953125, prec 0.111346, recall 0.782228
2017-12-10T03:13:39.431053: step 8085, loss 0.00339187, acc 1, prec 0.111357, recall 0.782247
2017-12-10T03:13:39.692736: step 8086, loss 0.0183679, acc 0.984375, prec 0.111366, recall 0.782266
2017-12-10T03:13:39.955662: step 8087, loss 0.309884, acc 0.984375, prec 0.111365, recall 0.782266
2017-12-10T03:13:40.223304: step 8088, loss 0.260069, acc 0.96875, prec 0.111373, recall 0.782284
2017-12-10T03:13:40.495496: step 8089, loss 0.223471, acc 0.96875, prec 0.111381, recall 0.782303
2017-12-10T03:13:40.764551: step 8090, loss 0.927138, acc 0.96875, prec 0.111378, recall 0.782303
2017-12-10T03:13:41.040683: step 8091, loss 0.543485, acc 0.96875, prec 0.111408, recall 0.782359
2017-12-10T03:13:41.316076: step 8092, loss 0.0225734, acc 0.984375, prec 0.111428, recall 0.782396
2017-12-10T03:13:41.584636: step 8093, loss 1.04335, acc 0.953125, prec 0.111468, recall 0.782471
2017-12-10T03:13:41.847965: step 8094, loss 0.230287, acc 0.96875, prec 0.111487, recall 0.782508
2017-12-10T03:13:42.107948: step 8095, loss 0.0442007, acc 0.984375, prec 0.111507, recall 0.782545
2017-12-10T03:13:42.375390: step 8096, loss 0.116941, acc 0.9375, prec 0.111502, recall 0.782545
2017-12-10T03:13:42.648764: step 8097, loss 0.325846, acc 0.953125, prec 0.111498, recall 0.782545
2017-12-10T03:13:42.909126: step 8098, loss 8.07077e-06, acc 1, prec 0.111519, recall 0.782583
2017-12-10T03:13:43.171682: step 8099, loss 0.0670078, acc 0.984375, prec 0.111529, recall 0.782601
2017-12-10T03:13:43.440673: step 8100, loss 0.701389, acc 0.984375, prec 0.111538, recall 0.78262

Evaluation:
2017-12-10T03:13:51.068349: step 8100, loss 14.9094, acc 0.961034, prec 0.111701, recall 0.776258

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-8100

2017-12-10T03:13:52.335388: step 8101, loss 0.0599961, acc 0.984375, prec 0.11171, recall 0.776277
2017-12-10T03:13:52.598339: step 8102, loss 0.000724118, acc 1, prec 0.111721, recall 0.776296
2017-12-10T03:13:52.860661: step 8103, loss 0.0169019, acc 1, prec 0.111743, recall 0.776334
2017-12-10T03:13:53.125440: step 8104, loss 0.0520495, acc 0.984375, prec 0.111741, recall 0.776334
2017-12-10T03:13:53.391560: step 8105, loss 0.291659, acc 0.96875, prec 0.11175, recall 0.776352
2017-12-10T03:13:53.657276: step 8106, loss 0.000228338, acc 1, prec 0.11175, recall 0.776352
2017-12-10T03:13:53.916919: step 8107, loss 0.00194732, acc 1, prec 0.111782, recall 0.776409
2017-12-10T03:13:54.182319: step 8108, loss 5.22385e-05, acc 1, prec 0.111804, recall 0.776447
2017-12-10T03:13:54.439349: step 8109, loss 0.0592357, acc 0.984375, prec 0.111824, recall 0.776484
2017-12-10T03:13:54.703703: step 8110, loss 0.06533, acc 0.984375, prec 0.111833, recall 0.776503
2017-12-10T03:13:54.976336: step 8111, loss 0.279158, acc 0.96875, prec 0.11183, recall 0.776503
2017-12-10T03:13:55.247761: step 8112, loss 4.17391e-06, acc 1, prec 0.11183, recall 0.776503
2017-12-10T03:13:55.512714: step 8113, loss 0.00550633, acc 1, prec 0.111841, recall 0.776522
2017-12-10T03:13:55.778837: step 8114, loss 0.179932, acc 0.953125, prec 0.111848, recall 0.776541
2017-12-10T03:13:56.039599: step 8115, loss 0.17448, acc 0.984375, prec 0.111857, recall 0.77656
2017-12-10T03:13:56.301895: step 8116, loss 0.0344206, acc 0.984375, prec 0.111867, recall 0.776579
2017-12-10T03:13:56.571233: step 8117, loss 0.000285306, acc 1, prec 0.111867, recall 0.776579
2017-12-10T03:13:56.829869: step 8118, loss 0.204699, acc 0.96875, prec 0.111875, recall 0.776598
2017-12-10T03:13:57.105530: step 8119, loss 0.0428007, acc 0.984375, prec 0.111874, recall 0.776598
2017-12-10T03:13:57.368032: step 8120, loss 9.62876e-06, acc 1, prec 0.111874, recall 0.776598
2017-12-10T03:13:57.625583: step 8121, loss 0.000145767, acc 1, prec 0.111884, recall 0.776616
2017-12-10T03:13:57.884923: step 8122, loss 2.26908, acc 0.953125, prec 0.111892, recall 0.77657
2017-12-10T03:13:58.150402: step 8123, loss 0.299974, acc 0.96875, prec 0.11189, recall 0.77657
2017-12-10T03:13:58.417532: step 8124, loss 0.143743, acc 0.984375, prec 0.111899, recall 0.776589
2017-12-10T03:13:58.693369: step 8125, loss 0.778839, acc 0.953125, prec 0.111895, recall 0.776589
2017-12-10T03:13:58.957088: step 8126, loss 0.118826, acc 0.984375, prec 0.111915, recall 0.776626
2017-12-10T03:13:59.225458: step 8127, loss 0.259113, acc 0.9375, prec 0.111931, recall 0.776664
2017-12-10T03:13:59.490996: step 8128, loss 0.0857069, acc 0.96875, prec 0.111929, recall 0.776664
2017-12-10T03:13:59.759517: step 8129, loss 0.319864, acc 0.90625, prec 0.11192, recall 0.776664
2017-12-10T03:14:00.032267: step 8130, loss 0.107476, acc 0.984375, prec 0.11193, recall 0.776683
2017-12-10T03:14:00.314124: step 8131, loss 0.176626, acc 0.96875, prec 0.111938, recall 0.776702
2017-12-10T03:14:00.593650: step 8132, loss 0.883533, acc 0.96875, prec 0.111957, recall 0.776739
2017-12-10T03:14:00.857146: step 8133, loss 0.84462, acc 0.953125, prec 0.111963, recall 0.776758
2017-12-10T03:14:01.123362: step 8134, loss 0.588989, acc 0.90625, prec 0.111966, recall 0.776777
2017-12-10T03:14:01.392648: step 8135, loss 0.140562, acc 0.984375, prec 0.111965, recall 0.776777
2017-12-10T03:14:01.660478: step 8136, loss 0.305936, acc 0.953125, prec 0.111971, recall 0.776796
2017-12-10T03:14:01.925894: step 8137, loss 0.197367, acc 0.984375, prec 0.11197, recall 0.776796
2017-12-10T03:14:02.199416: step 8138, loss 0.412973, acc 0.9375, prec 0.111975, recall 0.776814
2017-12-10T03:14:02.466680: step 8139, loss 0.130556, acc 0.96875, prec 0.111983, recall 0.776833
2017-12-10T03:14:02.730967: step 8140, loss 0.240655, acc 0.984375, prec 0.111982, recall 0.776833
2017-12-10T03:14:03.004895: step 8141, loss 0.122525, acc 0.984375, prec 0.111992, recall 0.776852
2017-12-10T03:14:03.272050: step 8142, loss 0.25368, acc 0.9375, prec 0.111997, recall 0.776871
2017-12-10T03:14:03.532898: step 8143, loss 0.00121001, acc 1, prec 0.112008, recall 0.776889
2017-12-10T03:14:03.793653: step 8144, loss 0.512678, acc 0.9375, prec 0.112013, recall 0.776908
2017-12-10T03:14:04.055272: step 8145, loss 0.114248, acc 0.984375, prec 0.112012, recall 0.776908
2017-12-10T03:14:04.318203: step 8146, loss 0.0913295, acc 0.96875, prec 0.112009, recall 0.776908
2017-12-10T03:14:04.583863: step 8147, loss 0.122327, acc 0.96875, prec 0.112006, recall 0.776908
2017-12-10T03:14:04.853676: step 8148, loss 0.262728, acc 0.953125, prec 0.112002, recall 0.776908
2017-12-10T03:14:05.121309: step 8149, loss 0.289026, acc 0.9375, prec 0.112007, recall 0.776927
2017-12-10T03:14:05.392822: step 8150, loss 0.23533, acc 0.953125, prec 0.112014, recall 0.776946
2017-12-10T03:14:05.654661: step 8151, loss 0.210274, acc 0.96875, prec 0.112022, recall 0.776964
2017-12-10T03:14:05.923587: step 8152, loss 0.445614, acc 0.96875, prec 0.112052, recall 0.777021
2017-12-10T03:14:06.185619: step 8153, loss 0.279612, acc 0.953125, prec 0.112059, recall 0.77704
2017-12-10T03:14:06.452538: step 8154, loss 0.0797482, acc 0.96875, prec 0.112056, recall 0.77704
2017-12-10T03:14:06.717231: step 8155, loss 0.15008, acc 0.984375, prec 0.112065, recall 0.777058
2017-12-10T03:14:06.979810: step 8156, loss 0.000435951, acc 1, prec 0.112065, recall 0.777058
2017-12-10T03:14:07.243758: step 8157, loss 0.17455, acc 0.96875, prec 0.112073, recall 0.777077
2017-12-10T03:14:07.512107: step 8158, loss 0.0236514, acc 0.984375, prec 0.112083, recall 0.777096
2017-12-10T03:14:07.776472: step 8159, loss 0.0615021, acc 0.984375, prec 0.112081, recall 0.777096
2017-12-10T03:14:08.042146: step 8160, loss 4.9749e-05, acc 1, prec 0.112081, recall 0.777096
2017-12-10T03:14:08.302645: step 8161, loss 0.264122, acc 0.984375, prec 0.112091, recall 0.777115
2017-12-10T03:14:08.566389: step 8162, loss 0.000128377, acc 1, prec 0.112101, recall 0.777133
2017-12-10T03:14:08.829988: step 8163, loss 1.34411, acc 0.984375, prec 0.112111, recall 0.777152
2017-12-10T03:14:09.098913: step 8164, loss 4.67683e-06, acc 1, prec 0.112111, recall 0.777152
2017-12-10T03:14:09.357739: step 8165, loss 0.245858, acc 0.96875, prec 0.11213, recall 0.777189
2017-12-10T03:14:09.620552: step 8166, loss 0.02029, acc 0.984375, prec 0.112139, recall 0.777208
2017-12-10T03:14:09.889209: step 8167, loss 0.172262, acc 0.984375, prec 0.112149, recall 0.777227
2017-12-10T03:14:10.154068: step 8168, loss 0.234265, acc 0.953125, prec 0.112155, recall 0.777246
2017-12-10T03:14:10.420666: step 8169, loss 0.116846, acc 0.984375, prec 0.112186, recall 0.777302
2017-12-10T03:14:10.683638: step 8170, loss 0.000153568, acc 1, prec 0.112197, recall 0.77732
2017-12-10T03:14:10.947729: step 8171, loss 0.0799785, acc 0.984375, prec 0.112206, recall 0.777339
2017-12-10T03:14:11.214478: step 8172, loss 0.00280002, acc 1, prec 0.112217, recall 0.777358
2017-12-10T03:14:11.480250: step 8173, loss 0.281702, acc 0.96875, prec 0.112214, recall 0.777358
2017-12-10T03:14:11.756292: step 8174, loss 0.113253, acc 0.984375, prec 0.112224, recall 0.777377
2017-12-10T03:14:12.021616: step 8175, loss 0.014126, acc 0.984375, prec 0.112233, recall 0.777395
2017-12-10T03:14:12.285721: step 8176, loss 0.149788, acc 0.96875, prec 0.11223, recall 0.777395
2017-12-10T03:14:12.554238: step 8177, loss 0.144125, acc 0.96875, prec 0.112238, recall 0.777414
2017-12-10T03:14:12.822098: step 8178, loss 0.57081, acc 0.984375, prec 0.112248, recall 0.777433
2017-12-10T03:14:13.086176: step 8179, loss 0.0014774, acc 1, prec 0.112248, recall 0.777433
2017-12-10T03:14:13.348006: step 8180, loss 0.0401609, acc 0.984375, prec 0.112247, recall 0.777433
2017-12-10T03:14:13.614883: step 8181, loss 0.23693, acc 0.953125, prec 0.112253, recall 0.777451
2017-12-10T03:14:13.883992: step 8182, loss 0.2296, acc 0.984375, prec 0.112263, recall 0.77747
2017-12-10T03:14:14.150513: step 8183, loss 0.327084, acc 0.984375, prec 0.112261, recall 0.77747
2017-12-10T03:14:14.417406: step 8184, loss 4.96898, acc 0.953125, prec 0.112259, recall 0.777405
2017-12-10T03:14:14.686504: step 8185, loss 0.257946, acc 1, prec 0.11228, recall 0.777442
2017-12-10T03:14:14.963122: step 8186, loss 1.16171, acc 0.96875, prec 0.112299, recall 0.777479
2017-12-10T03:14:15.232382: step 8187, loss 0.0875357, acc 0.953125, prec 0.112305, recall 0.777498
2017-12-10T03:14:15.503769: step 8188, loss 0.00349868, acc 1, prec 0.112305, recall 0.777498
2017-12-10T03:14:15.774641: step 8189, loss 0.157201, acc 0.96875, prec 0.112324, recall 0.777535
2017-12-10T03:14:16.037515: step 8190, loss 0.91327, acc 0.90625, prec 0.112316, recall 0.777535
2017-12-10T03:14:16.301545: step 8191, loss 0.823422, acc 0.9375, prec 0.112311, recall 0.777535
2017-12-10T03:14:16.567670: step 8192, loss 0.426779, acc 0.953125, prec 0.112307, recall 0.777535
2017-12-10T03:14:16.836453: step 8193, loss 0.60871, acc 0.96875, prec 0.112325, recall 0.777573
2017-12-10T03:14:17.109414: step 8194, loss 0.842146, acc 0.953125, prec 0.112321, recall 0.777573
2017-12-10T03:14:17.377962: step 8195, loss 0.161456, acc 0.9375, prec 0.112316, recall 0.777573
2017-12-10T03:14:17.644862: step 8196, loss 0.0681901, acc 0.984375, prec 0.112325, recall 0.777591
2017-12-10T03:14:17.909361: step 8197, loss 0.274142, acc 0.9375, prec 0.11232, recall 0.777591
2017-12-10T03:14:18.175052: step 8198, loss 0.654373, acc 0.9375, prec 0.112314, recall 0.777591
2017-12-10T03:14:18.440141: step 8199, loss 0.431743, acc 0.953125, prec 0.112321, recall 0.77761
2017-12-10T03:14:18.703880: step 8200, loss 0.136812, acc 0.953125, prec 0.112317, recall 0.77761
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-8200

2017-12-10T03:14:19.944523: step 8201, loss 0.20155, acc 0.984375, prec 0.112326, recall 0.777629
2017-12-10T03:14:20.214942: step 8202, loss 0.879997, acc 0.890625, prec 0.112338, recall 0.777666
2017-12-10T03:14:20.482172: step 8203, loss 0.0685631, acc 0.984375, prec 0.112358, recall 0.777703
2017-12-10T03:14:20.747743: step 8204, loss 0.0577261, acc 0.96875, prec 0.112377, recall 0.777741
2017-12-10T03:14:21.019984: step 8205, loss 0.718036, acc 0.953125, prec 0.112395, recall 0.777778
2017-12-10T03:14:21.284592: step 8206, loss 0.531801, acc 0.921875, prec 0.112409, recall 0.777815
2017-12-10T03:14:21.548341: step 8207, loss 0.236355, acc 0.96875, prec 0.112407, recall 0.777815
2017-12-10T03:14:21.813086: step 8208, loss 0.0704712, acc 0.96875, prec 0.112404, recall 0.777815
2017-12-10T03:14:22.079265: step 8209, loss 0.25081, acc 0.96875, prec 0.112401, recall 0.777815
2017-12-10T03:14:22.349492: step 8210, loss 0.294649, acc 0.953125, prec 0.112397, recall 0.777815
2017-12-10T03:14:22.614025: step 8211, loss 0.3077, acc 0.96875, prec 0.112394, recall 0.777815
2017-12-10T03:14:22.881325: step 8212, loss 0.326744, acc 0.9375, prec 0.1124, recall 0.777834
2017-12-10T03:14:23.145060: step 8213, loss 0.0117287, acc 1, prec 0.1124, recall 0.777834
2017-12-10T03:14:23.408725: step 8214, loss 0.188837, acc 0.96875, prec 0.112418, recall 0.777871
2017-12-10T03:14:23.674389: step 8215, loss 0.953176, acc 0.90625, prec 0.112421, recall 0.777889
2017-12-10T03:14:23.938096: step 8216, loss 0.069781, acc 0.984375, prec 0.11242, recall 0.777889
2017-12-10T03:14:24.199674: step 8217, loss 0.349603, acc 0.96875, prec 0.112428, recall 0.777908
2017-12-10T03:14:24.463574: step 8218, loss 0.000898483, acc 1, prec 0.11246, recall 0.777964
2017-12-10T03:14:24.731941: step 8219, loss 0.396553, acc 0.984375, prec 0.112469, recall 0.777982
2017-12-10T03:14:24.991804: step 8220, loss 0.614556, acc 0.9375, prec 0.112464, recall 0.777982
2017-12-10T03:14:25.261282: step 8221, loss 0.0842769, acc 0.984375, prec 0.112462, recall 0.777982
2017-12-10T03:14:25.529012: step 8222, loss 0.0839868, acc 0.96875, prec 0.112481, recall 0.77802
2017-12-10T03:14:25.802219: step 8223, loss 0.125242, acc 0.953125, prec 0.112499, recall 0.778057
2017-12-10T03:14:26.064575: step 8224, loss 0.0105909, acc 1, prec 0.112499, recall 0.778057
2017-12-10T03:14:26.322552: step 8225, loss 0.000372723, acc 1, prec 0.112499, recall 0.778057
2017-12-10T03:14:26.593485: step 8226, loss 0.000682596, acc 1, prec 0.11252, recall 0.778094
2017-12-10T03:14:26.862638: step 8227, loss 0.000591721, acc 1, prec 0.112531, recall 0.778112
2017-12-10T03:14:27.128093: step 8228, loss 0.00723802, acc 1, prec 0.112531, recall 0.778112
2017-12-10T03:14:27.394448: step 8229, loss 0.541859, acc 1, prec 0.112552, recall 0.77815
2017-12-10T03:14:27.669108: step 8230, loss 0.0168322, acc 0.984375, prec 0.112551, recall 0.77815
2017-12-10T03:14:27.930880: step 8231, loss 0.00137825, acc 1, prec 0.112572, recall 0.778187
2017-12-10T03:14:28.196474: step 8232, loss 0.062361, acc 0.984375, prec 0.112582, recall 0.778205
2017-12-10T03:14:28.459997: step 8233, loss 0.243228, acc 0.984375, prec 0.11258, recall 0.778205
2017-12-10T03:14:28.725412: step 8234, loss 0.072141, acc 0.984375, prec 0.112601, recall 0.778242
2017-12-10T03:14:28.995273: step 8235, loss 0.00189211, acc 1, prec 0.112601, recall 0.778242
2017-12-10T03:14:29.257121: step 8236, loss 0.204168, acc 0.96875, prec 0.112598, recall 0.778242
2017-12-10T03:14:29.520037: step 8237, loss 0.00940008, acc 1, prec 0.112609, recall 0.778261
2017-12-10T03:14:29.783638: step 8238, loss 0.103471, acc 0.984375, prec 0.112607, recall 0.778261
2017-12-10T03:14:30.045813: step 8239, loss 2.38072e-05, acc 1, prec 0.112607, recall 0.778261
2017-12-10T03:14:30.308643: step 8240, loss 0.575694, acc 0.984375, prec 0.112606, recall 0.778261
2017-12-10T03:14:30.588331: step 8241, loss 0.0174377, acc 1, prec 0.112627, recall 0.778298
2017-12-10T03:14:30.850958: step 8242, loss 0.0277602, acc 0.984375, prec 0.112626, recall 0.778298
2017-12-10T03:14:31.112524: step 8243, loss 0.064129, acc 0.984375, prec 0.112635, recall 0.778316
2017-12-10T03:14:31.380291: step 8244, loss 0.273566, acc 0.984375, prec 0.112655, recall 0.778354
2017-12-10T03:14:31.639461: step 8245, loss 0.00103178, acc 1, prec 0.112666, recall 0.778372
2017-12-10T03:14:31.898013: step 8246, loss 0.00261496, acc 1, prec 0.112688, recall 0.778409
2017-12-10T03:14:32.169825: step 8247, loss 0.288214, acc 0.984375, prec 0.112697, recall 0.778428
2017-12-10T03:14:32.431540: step 8248, loss 0.000681007, acc 1, prec 0.112708, recall 0.778446
2017-12-10T03:14:32.690324: step 8249, loss 0.255318, acc 0.984375, prec 0.112717, recall 0.778465
2017-12-10T03:14:32.955004: step 8250, loss 0.00169301, acc 1, prec 0.112728, recall 0.778483
2017-12-10T03:14:33.216827: step 8251, loss 0.125246, acc 0.984375, prec 0.112726, recall 0.778483
2017-12-10T03:14:33.487190: step 8252, loss 3.07247, acc 0.984375, prec 0.112737, recall 0.778437
2017-12-10T03:14:33.751891: step 8253, loss 0.0972824, acc 1, prec 0.112759, recall 0.778474
2017-12-10T03:14:34.019421: step 8254, loss 0.0190653, acc 0.984375, prec 0.112768, recall 0.778492
2017-12-10T03:14:34.287093: step 8255, loss 6.46426e-05, acc 1, prec 0.112768, recall 0.778492
2017-12-10T03:14:34.545897: step 8256, loss 0.0436645, acc 0.984375, prec 0.112767, recall 0.778492
2017-12-10T03:14:34.808739: step 8257, loss 0.0307854, acc 0.984375, prec 0.112776, recall 0.778511
2017-12-10T03:14:35.076894: step 8258, loss 0.0240094, acc 0.984375, prec 0.112775, recall 0.778511
2017-12-10T03:14:35.343884: step 8259, loss 0.360459, acc 0.921875, prec 0.112789, recall 0.778548
2017-12-10T03:14:35.607087: step 8260, loss 0.381093, acc 0.9375, prec 0.112784, recall 0.778548
2017-12-10T03:14:35.878429: step 8261, loss 0.0537643, acc 0.984375, prec 0.112783, recall 0.778548
2017-12-10T03:14:36.144970: step 8262, loss 0.580149, acc 0.890625, prec 0.112773, recall 0.778548
2017-12-10T03:14:36.408669: step 8263, loss 0.874718, acc 0.90625, prec 0.112775, recall 0.778566
2017-12-10T03:14:36.676822: step 8264, loss 0.447747, acc 0.953125, prec 0.112771, recall 0.778566
2017-12-10T03:14:36.941083: step 8265, loss 0.270468, acc 0.9375, prec 0.112787, recall 0.778603
2017-12-10T03:14:37.204200: step 8266, loss 1.86214, acc 0.875, prec 0.112787, recall 0.778621
2017-12-10T03:14:37.478321: step 8267, loss 0.229121, acc 0.96875, prec 0.112795, recall 0.77864
2017-12-10T03:14:37.739731: step 8268, loss 0.149245, acc 0.953125, prec 0.112813, recall 0.778677
2017-12-10T03:14:38.004407: step 8269, loss 0.0890121, acc 0.953125, prec 0.112819, recall 0.778695
2017-12-10T03:14:38.275134: step 8270, loss 0.327116, acc 0.96875, prec 0.112838, recall 0.778732
2017-12-10T03:14:38.538765: step 8271, loss 0.591213, acc 0.9375, prec 0.112843, recall 0.778751
2017-12-10T03:14:38.801517: step 8272, loss 0.525866, acc 0.9375, prec 0.112838, recall 0.778751
2017-12-10T03:14:39.074670: step 8273, loss 0.0311344, acc 0.984375, prec 0.112869, recall 0.778806
2017-12-10T03:14:39.334061: step 8274, loss 0.422153, acc 0.96875, prec 0.112877, recall 0.778825
2017-12-10T03:14:39.598250: step 8275, loss 0.0596405, acc 0.96875, prec 0.112874, recall 0.778825
2017-12-10T03:14:39.862220: step 8276, loss 0.00277788, acc 1, prec 0.112885, recall 0.778843
2017-12-10T03:14:40.123543: step 8277, loss 0.00124923, acc 1, prec 0.112895, recall 0.778861
2017-12-10T03:14:40.386674: step 8278, loss 1.09812, acc 0.921875, prec 0.112888, recall 0.778861
2017-12-10T03:14:40.648591: step 8279, loss 0.689432, acc 0.96875, prec 0.112886, recall 0.778861
2017-12-10T03:14:40.919552: step 8280, loss 0.167165, acc 0.96875, prec 0.112883, recall 0.778861
2017-12-10T03:14:41.187548: step 8281, loss 1.12884, acc 0.984375, prec 0.112904, recall 0.778833
2017-12-10T03:14:41.458964: step 8282, loss 0.143904, acc 0.953125, prec 0.112911, recall 0.778852
2017-12-10T03:14:41.732992: step 8283, loss 13.4501, acc 0.953125, prec 0.112919, recall 0.778805
2017-12-10T03:14:42.001509: step 8284, loss 0.85069, acc 0.953125, prec 0.112915, recall 0.778805
2017-12-10T03:14:42.266698: step 8285, loss 1.092, acc 0.9375, prec 0.112909, recall 0.778805
2017-12-10T03:14:42.528471: step 8286, loss 0.966255, acc 0.84375, prec 0.112907, recall 0.778824
2017-12-10T03:14:42.794570: step 8287, loss 0.176723, acc 0.9375, prec 0.112933, recall 0.778879
2017-12-10T03:14:43.059844: step 8288, loss 1.23341, acc 0.890625, prec 0.112924, recall 0.778879
2017-12-10T03:14:43.331338: step 8289, loss 0.649356, acc 0.921875, prec 0.112917, recall 0.778879
2017-12-10T03:14:43.599681: step 8290, loss 0.789461, acc 0.90625, prec 0.112909, recall 0.778879
2017-12-10T03:14:43.874837: step 8291, loss 0.393872, acc 0.890625, prec 0.11291, recall 0.778897
2017-12-10T03:14:44.143541: step 8292, loss 1.21842, acc 0.84375, prec 0.112907, recall 0.778916
2017-12-10T03:14:44.418982: step 8293, loss 2.95854, acc 0.71875, prec 0.112893, recall 0.778934
2017-12-10T03:14:44.692061: step 8294, loss 0.95057, acc 0.90625, prec 0.112896, recall 0.778953
2017-12-10T03:14:44.965393: step 8295, loss 0.807067, acc 0.875, prec 0.112885, recall 0.778953
2017-12-10T03:14:45.233084: step 8296, loss 0.804056, acc 0.90625, prec 0.112898, recall 0.778989
2017-12-10T03:14:45.500895: step 8297, loss 0.310014, acc 0.921875, prec 0.112913, recall 0.779026
2017-12-10T03:14:45.767864: step 8298, loss 1.85255, acc 0.828125, prec 0.112898, recall 0.779026
2017-12-10T03:14:46.037058: step 8299, loss 1.07714, acc 0.875, prec 0.112887, recall 0.779026
2017-12-10T03:14:46.301850: step 8300, loss 0.962976, acc 0.875, prec 0.112876, recall 0.779026
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-8300

2017-12-10T03:14:47.667425: step 8301, loss 0.326282, acc 0.9375, prec 0.112881, recall 0.779045
2017-12-10T03:14:47.935904: step 8302, loss 1.2859, acc 0.953125, prec 0.112888, recall 0.779063
2017-12-10T03:14:48.204621: step 8303, loss 0.632737, acc 0.90625, prec 0.11289, recall 0.779081
2017-12-10T03:14:48.472476: step 8304, loss 0.470062, acc 0.921875, prec 0.112905, recall 0.779118
2017-12-10T03:14:48.737312: step 8305, loss 0.406386, acc 0.921875, prec 0.112909, recall 0.779137
2017-12-10T03:14:49.001458: step 8306, loss 0.715967, acc 0.921875, prec 0.112913, recall 0.779155
2017-12-10T03:14:49.274709: step 8307, loss 1.02534, acc 0.890625, prec 0.112914, recall 0.779173
2017-12-10T03:14:49.540850: step 8308, loss 0.133307, acc 0.96875, prec 0.112932, recall 0.77921
2017-12-10T03:14:49.805583: step 8309, loss 0.109217, acc 0.984375, prec 0.112942, recall 0.779228
2017-12-10T03:14:50.076798: step 8310, loss 0.0520879, acc 0.96875, prec 0.112939, recall 0.779228
2017-12-10T03:14:50.342961: step 8311, loss 0.1922, acc 0.96875, prec 0.112936, recall 0.779228
2017-12-10T03:14:50.608372: step 8312, loss 2.47891e-05, acc 1, prec 0.112936, recall 0.779228
2017-12-10T03:14:50.865501: step 8313, loss 0.515461, acc 0.9375, prec 0.112931, recall 0.779228
2017-12-10T03:14:51.134194: step 8314, loss 0.00841003, acc 1, prec 0.112931, recall 0.779228
2017-12-10T03:14:51.394794: step 8315, loss 0.0320503, acc 0.984375, prec 0.112929, recall 0.779228
2017-12-10T03:14:51.665305: step 8316, loss 0.472817, acc 0.96875, prec 0.112937, recall 0.779247
2017-12-10T03:14:51.924317: step 8317, loss 10.7846, acc 0.953125, prec 0.112957, recall 0.779154
2017-12-10T03:14:52.198227: step 8318, loss 0.293386, acc 0.96875, prec 0.112965, recall 0.779172
2017-12-10T03:14:52.463155: step 8319, loss 0.0562551, acc 0.984375, prec 0.112964, recall 0.779172
2017-12-10T03:14:52.729663: step 8320, loss 0.0594199, acc 0.984375, prec 0.112973, recall 0.779191
2017-12-10T03:14:52.993781: step 8321, loss 0.256586, acc 0.90625, prec 0.112997, recall 0.779246
2017-12-10T03:14:53.265390: step 8322, loss 0.977337, acc 0.890625, prec 0.112988, recall 0.779246
2017-12-10T03:14:53.527732: step 8323, loss 1.29788, acc 0.890625, prec 0.113, recall 0.779282
2017-12-10T03:14:53.792058: step 8324, loss 0.408725, acc 0.921875, prec 0.113003, recall 0.779301
2017-12-10T03:14:54.062253: step 8325, loss 0.493583, acc 0.921875, prec 0.113007, recall 0.779319
2017-12-10T03:14:54.330819: step 8326, loss 0.775156, acc 0.828125, prec 0.112992, recall 0.779319
2017-12-10T03:14:54.597493: step 8327, loss 1.11869, acc 0.921875, prec 0.112986, recall 0.779319
2017-12-10T03:14:54.862771: step 8328, loss 0.262217, acc 0.953125, prec 0.113003, recall 0.779356
2017-12-10T03:14:55.125715: step 8329, loss 0.512118, acc 0.9375, prec 0.113008, recall 0.779374
2017-12-10T03:14:55.388795: step 8330, loss 1.48902, acc 0.890625, prec 0.113009, recall 0.779392
2017-12-10T03:14:55.655851: step 8331, loss 0.380429, acc 0.9375, prec 0.113004, recall 0.779392
2017-12-10T03:14:55.923146: step 8332, loss 0.391514, acc 0.90625, prec 0.113017, recall 0.779429
2017-12-10T03:14:56.195384: step 8333, loss 0.54825, acc 0.90625, prec 0.11303, recall 0.779465
2017-12-10T03:14:56.464006: step 8334, loss 0.315272, acc 0.9375, prec 0.113025, recall 0.779465
2017-12-10T03:14:56.741457: step 8335, loss 0.197796, acc 0.953125, prec 0.113021, recall 0.779465
2017-12-10T03:14:57.006105: step 8336, loss 0.659092, acc 0.921875, prec 0.113025, recall 0.779484
2017-12-10T03:14:57.277389: step 8337, loss 0.210206, acc 0.953125, prec 0.113031, recall 0.779502
2017-12-10T03:14:57.545783: step 8338, loss 1.10161, acc 0.875, prec 0.11302, recall 0.779502
2017-12-10T03:14:57.819406: step 8339, loss 0.87776, acc 0.90625, prec 0.113012, recall 0.779502
2017-12-10T03:14:58.086302: step 8340, loss 0.717954, acc 0.890625, prec 0.113003, recall 0.779502
2017-12-10T03:14:58.359819: step 8341, loss 0.794816, acc 0.875, prec 0.113002, recall 0.77952
2017-12-10T03:14:58.635134: step 8342, loss 0.179702, acc 0.9375, prec 0.113008, recall 0.779539
2017-12-10T03:14:58.903068: step 8343, loss 0.606492, acc 0.90625, prec 0.11301, recall 0.779557
2017-12-10T03:14:59.167485: step 8344, loss 0.198574, acc 0.9375, prec 0.113015, recall 0.779575
2017-12-10T03:14:59.437704: step 8345, loss 0.0297027, acc 0.984375, prec 0.113014, recall 0.779575
2017-12-10T03:14:59.701128: step 8346, loss 0.767641, acc 0.9375, prec 0.113009, recall 0.779575
2017-12-10T03:14:59.970325: step 8347, loss 0.0773325, acc 0.984375, prec 0.113007, recall 0.779575
2017-12-10T03:15:00.246983: step 8348, loss 0.062464, acc 0.984375, prec 0.113027, recall 0.779612
2017-12-10T03:15:00.516414: step 8349, loss 0.944837, acc 0.953125, prec 0.113023, recall 0.779612
2017-12-10T03:15:00.781569: step 8350, loss 0.46398, acc 0.9375, prec 0.113018, recall 0.779612
2017-12-10T03:15:01.054516: step 8351, loss 2.61827, acc 0.953125, prec 0.113026, recall 0.779565
2017-12-10T03:15:01.321655: step 8352, loss 0.00072027, acc 1, prec 0.113026, recall 0.779565
2017-12-10T03:15:01.584551: step 8353, loss 0.285741, acc 0.984375, prec 0.113046, recall 0.779602
2017-12-10T03:15:01.847174: step 8354, loss 0.140813, acc 0.984375, prec 0.113055, recall 0.77962
2017-12-10T03:15:02.114341: step 8355, loss 1.91701, acc 0.953125, prec 0.113052, recall 0.779556
2017-12-10T03:15:02.384793: step 8356, loss 0.0715327, acc 0.984375, prec 0.113051, recall 0.779556
2017-12-10T03:15:02.643709: step 8357, loss 0.0950418, acc 0.96875, prec 0.113059, recall 0.779574
2017-12-10T03:15:02.908287: step 8358, loss 0.148126, acc 0.96875, prec 0.113056, recall 0.779574
2017-12-10T03:15:03.184206: step 8359, loss 0.310202, acc 0.953125, prec 0.113073, recall 0.77961
2017-12-10T03:15:03.446775: step 8360, loss 0.373955, acc 0.90625, prec 0.113065, recall 0.77961
2017-12-10T03:15:03.707751: step 8361, loss 1.41466, acc 0.890625, prec 0.113056, recall 0.77961
2017-12-10T03:15:03.968527: step 8362, loss 0.470563, acc 0.90625, prec 0.113047, recall 0.77961
2017-12-10T03:15:04.234849: step 8363, loss 0.13148, acc 0.9375, prec 0.113042, recall 0.77961
2017-12-10T03:15:04.498941: step 8364, loss 0.918683, acc 0.828125, prec 0.113027, recall 0.77961
2017-12-10T03:15:04.761658: step 8365, loss 0.601208, acc 0.890625, prec 0.113028, recall 0.779629
2017-12-10T03:15:05.023965: step 8366, loss 1.38135, acc 0.890625, prec 0.11304, recall 0.779665
2017-12-10T03:15:05.284485: step 8367, loss 0.824022, acc 0.90625, prec 0.113053, recall 0.779702
2017-12-10T03:15:05.549119: step 8368, loss 0.159, acc 0.9375, prec 0.113048, recall 0.779702
2017-12-10T03:15:05.821409: step 8369, loss 0.198092, acc 0.96875, prec 0.113045, recall 0.779702
2017-12-10T03:15:06.087152: step 8370, loss 1.80546, acc 0.890625, prec 0.113046, recall 0.77972
2017-12-10T03:15:06.351708: step 8371, loss 0.869873, acc 0.90625, prec 0.113038, recall 0.77972
2017-12-10T03:15:06.615670: step 8372, loss 0.391272, acc 0.9375, prec 0.113033, recall 0.77972
2017-12-10T03:15:06.877795: step 8373, loss 0.919861, acc 0.90625, prec 0.113035, recall 0.779738
2017-12-10T03:15:07.143565: step 8374, loss 0.278639, acc 0.9375, prec 0.113051, recall 0.779775
2017-12-10T03:15:07.407459: step 8375, loss 0.116112, acc 0.96875, prec 0.113059, recall 0.779793
2017-12-10T03:15:07.675680: step 8376, loss 1.11363, acc 0.9375, prec 0.113054, recall 0.779793
2017-12-10T03:15:07.946121: step 8377, loss 0.593918, acc 0.953125, prec 0.11306, recall 0.779811
2017-12-10T03:15:08.212363: step 8378, loss 0.498575, acc 0.953125, prec 0.113056, recall 0.779811
2017-12-10T03:15:08.473858: step 8379, loss 0.113756, acc 0.96875, prec 0.113064, recall 0.779829
2017-12-10T03:15:08.741275: step 8380, loss 0.373785, acc 0.9375, prec 0.113069, recall 0.779848
2017-12-10T03:15:09.005676: step 8381, loss 0.390958, acc 0.953125, prec 0.113076, recall 0.779866
2017-12-10T03:15:09.265375: step 8382, loss 0.0217047, acc 0.984375, prec 0.113074, recall 0.779866
2017-12-10T03:15:09.529825: step 8383, loss 0.0096601, acc 1, prec 0.113085, recall 0.779884
2017-12-10T03:15:09.791707: step 8384, loss 0.150769, acc 0.953125, prec 0.113092, recall 0.779902
2017-12-10T03:15:10.058359: step 8385, loss 0.158181, acc 0.984375, prec 0.113101, recall 0.779921
2017-12-10T03:15:10.325364: step 8386, loss 0.00230771, acc 1, prec 0.113112, recall 0.779939
2017-12-10T03:15:10.596180: step 8387, loss 0.61741, acc 0.984375, prec 0.113142, recall 0.779993
2017-12-10T03:15:10.866787: step 8388, loss 0.0381596, acc 0.984375, prec 0.113151, recall 0.780012
2017-12-10T03:15:11.131795: step 8389, loss 0.0192446, acc 0.984375, prec 0.11315, recall 0.780012
2017-12-10T03:15:11.395529: step 8390, loss 0.024471, acc 0.984375, prec 0.113159, recall 0.78003
2017-12-10T03:15:11.666976: step 8391, loss 0.469575, acc 0.96875, prec 0.113157, recall 0.78003
2017-12-10T03:15:11.933366: step 8392, loss 0.0513226, acc 0.984375, prec 0.113155, recall 0.78003
2017-12-10T03:15:12.204241: step 8393, loss 0.0152521, acc 0.984375, prec 0.113154, recall 0.78003
2017-12-10T03:15:12.461742: step 8394, loss 0.0667407, acc 1, prec 0.113175, recall 0.780066
2017-12-10T03:15:12.730112: step 8395, loss 0.0562849, acc 0.984375, prec 0.113184, recall 0.780084
2017-12-10T03:15:12.990029: step 8396, loss 0.0660466, acc 0.96875, prec 0.113203, recall 0.780121
2017-12-10T03:15:13.255080: step 8397, loss 0.0671677, acc 0.984375, prec 0.113223, recall 0.780157
2017-12-10T03:15:13.517671: step 8398, loss 2.46179, acc 0.953125, prec 0.113242, recall 0.780129
2017-12-10T03:15:13.783363: step 8399, loss 7.90567, acc 0.984375, prec 0.113252, recall 0.780083
2017-12-10T03:15:14.055233: step 8400, loss 0.00240775, acc 1, prec 0.113263, recall 0.780101

Evaluation:
2017-12-10T03:15:21.738472: step 8400, loss 14.6659, acc 0.951599, prec 0.113409, recall 0.775132

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-8400

2017-12-10T03:15:23.154513: step 8401, loss 0.377733, acc 0.953125, prec 0.113427, recall 0.775169
2017-12-10T03:15:23.416358: step 8402, loss 0.0211975, acc 0.984375, prec 0.113457, recall 0.775224
2017-12-10T03:15:23.681886: step 8403, loss 0.357811, acc 0.9375, prec 0.113462, recall 0.775242
2017-12-10T03:15:23.947384: step 8404, loss 0.564003, acc 0.890625, prec 0.113463, recall 0.775261
2017-12-10T03:15:24.215688: step 8405, loss 0.776424, acc 0.953125, prec 0.11347, recall 0.775279
2017-12-10T03:15:24.488327: step 8406, loss 0.718254, acc 0.96875, prec 0.113467, recall 0.775279
2017-12-10T03:15:24.751658: step 8407, loss 1.26071, acc 0.796875, prec 0.11346, recall 0.775297
2017-12-10T03:15:25.013755: step 8408, loss 0.431345, acc 0.953125, prec 0.113488, recall 0.775352
2017-12-10T03:15:25.282069: step 8409, loss 0.588155, acc 0.90625, prec 0.113479, recall 0.775352
2017-12-10T03:15:25.546877: step 8410, loss 1.04472, acc 0.859375, prec 0.113488, recall 0.775389
2017-12-10T03:15:25.809847: step 8411, loss 0.85984, acc 0.890625, prec 0.1135, recall 0.775425
2017-12-10T03:15:26.077920: step 8412, loss 0.184466, acc 0.953125, prec 0.113496, recall 0.775425
2017-12-10T03:15:26.345329: step 8413, loss 0.828403, acc 0.890625, prec 0.113487, recall 0.775425
2017-12-10T03:15:26.620862: step 8414, loss 0.769833, acc 0.90625, prec 0.113489, recall 0.775444
2017-12-10T03:15:26.883733: step 8415, loss 1.28166, acc 0.9375, prec 0.113494, recall 0.775462
2017-12-10T03:15:27.148860: step 8416, loss 0.155966, acc 0.953125, prec 0.113501, recall 0.77548
2017-12-10T03:15:27.413657: step 8417, loss 0.483458, acc 0.9375, prec 0.113506, recall 0.775499
2017-12-10T03:15:27.680727: step 8418, loss 0.511173, acc 0.9375, prec 0.113522, recall 0.775535
2017-12-10T03:15:27.946849: step 8419, loss 1.28718, acc 0.890625, prec 0.113523, recall 0.775553
2017-12-10T03:15:28.210963: step 8420, loss 0.690906, acc 0.90625, prec 0.113525, recall 0.775572
2017-12-10T03:15:28.484048: step 8421, loss 0.295743, acc 0.953125, prec 0.113521, recall 0.775572
2017-12-10T03:15:28.749797: step 8422, loss 0.67751, acc 0.875, prec 0.11351, recall 0.775572
2017-12-10T03:15:29.011956: step 8423, loss 0.477965, acc 0.90625, prec 0.113502, recall 0.775572
2017-12-10T03:15:29.274264: step 8424, loss 0.79144, acc 0.90625, prec 0.113505, recall 0.77559
2017-12-10T03:15:29.541144: step 8425, loss 0.204712, acc 0.953125, prec 0.113522, recall 0.775626
2017-12-10T03:15:29.802655: step 8426, loss 0.817803, acc 0.921875, prec 0.113515, recall 0.775626
2017-12-10T03:15:30.070997: step 8427, loss 0.357001, acc 0.921875, prec 0.113519, recall 0.775645
2017-12-10T03:15:30.346826: step 8428, loss 0.485715, acc 0.921875, prec 0.113512, recall 0.775645
2017-12-10T03:15:30.611203: step 8429, loss 0.694585, acc 0.90625, prec 0.113514, recall 0.775663
2017-12-10T03:15:30.877227: step 8430, loss 0.686408, acc 0.9375, prec 0.11353, recall 0.775699
2017-12-10T03:15:31.146133: step 8431, loss 0.241528, acc 0.953125, prec 0.113526, recall 0.775699
2017-12-10T03:15:31.411104: step 8432, loss 1.0306, acc 0.984375, prec 0.113546, recall 0.775736
2017-12-10T03:15:31.687691: step 8433, loss 0.535682, acc 0.921875, prec 0.113539, recall 0.775736
2017-12-10T03:15:31.951805: step 8434, loss 0.656198, acc 0.96875, prec 0.113547, recall 0.775754
2017-12-10T03:15:32.217218: step 8435, loss 0.000320772, acc 1, prec 0.113568, recall 0.775791
2017-12-10T03:15:32.480608: step 8436, loss 0.241694, acc 0.953125, prec 0.113574, recall 0.775809
2017-12-10T03:15:32.753772: step 8437, loss 0.245495, acc 0.96875, prec 0.113572, recall 0.775809
2017-12-10T03:15:33.018430: step 8438, loss 0.0336743, acc 0.984375, prec 0.113581, recall 0.775827
2017-12-10T03:15:33.291836: step 8439, loss 0.0262112, acc 0.984375, prec 0.113601, recall 0.775863
2017-12-10T03:15:33.561290: step 8440, loss 0.00333555, acc 1, prec 0.113611, recall 0.775882
2017-12-10T03:15:33.822266: step 8441, loss 0.35515, acc 0.96875, prec 0.113609, recall 0.775882
2017-12-10T03:15:34.088561: step 8442, loss 0.153757, acc 0.96875, prec 0.113606, recall 0.775882
2017-12-10T03:15:34.352067: step 8443, loss 0.235165, acc 0.953125, prec 0.113612, recall 0.7759
2017-12-10T03:15:34.623158: step 8444, loss 0.00472157, acc 1, prec 0.113612, recall 0.7759
2017-12-10T03:15:34.886540: step 8445, loss 0.0273338, acc 0.984375, prec 0.113611, recall 0.7759
2017-12-10T03:15:35.147722: step 8446, loss 0.0108526, acc 1, prec 0.113621, recall 0.775918
2017-12-10T03:15:35.415062: step 8447, loss 0.549016, acc 0.984375, prec 0.11362, recall 0.775918
2017-12-10T03:15:35.689992: step 8448, loss 3.51183, acc 0.96875, prec 0.113629, recall 0.775873
2017-12-10T03:15:35.924205: step 8449, loss 0.264336, acc 0.960784, prec 0.113627, recall 0.775873
2017-12-10T03:15:36.195756: step 8450, loss 0.0105819, acc 1, prec 0.113627, recall 0.775873
2017-12-10T03:15:36.461071: step 8451, loss 0.178911, acc 0.953125, prec 0.113623, recall 0.775873
2017-12-10T03:15:36.724242: step 8452, loss 0.480819, acc 0.96875, prec 0.11363, recall 0.775891
2017-12-10T03:15:36.991141: step 8453, loss 0.8529, acc 0.890625, prec 0.113642, recall 0.775928
2017-12-10T03:15:37.248444: step 8454, loss 0.0459809, acc 0.984375, prec 0.113641, recall 0.775928
2017-12-10T03:15:37.516193: step 8455, loss 0.226239, acc 0.9375, prec 0.113656, recall 0.775964
2017-12-10T03:15:37.783495: step 8456, loss 0.81719, acc 0.90625, prec 0.113648, recall 0.775964
2017-12-10T03:15:38.049082: step 8457, loss 0.461068, acc 0.9375, prec 0.113643, recall 0.775964
2017-12-10T03:15:38.308217: step 8458, loss 0.711279, acc 0.890625, prec 0.113644, recall 0.775982
2017-12-10T03:15:38.571516: step 8459, loss 0.0103385, acc 1, prec 0.113644, recall 0.775982
2017-12-10T03:15:38.838050: step 8460, loss 0.316595, acc 0.953125, prec 0.113671, recall 0.776037
2017-12-10T03:15:39.109750: step 8461, loss 0.0951954, acc 0.953125, prec 0.113667, recall 0.776037
2017-12-10T03:15:39.382227: step 8462, loss 0.279427, acc 0.96875, prec 0.113675, recall 0.776055
2017-12-10T03:15:39.647603: step 8463, loss 0.479753, acc 0.921875, prec 0.11369, recall 0.776092
2017-12-10T03:15:39.912514: step 8464, loss 0.37816, acc 0.921875, prec 0.113683, recall 0.776092
2017-12-10T03:15:40.181740: step 8465, loss 1.21243, acc 0.90625, prec 0.113685, recall 0.77611
2017-12-10T03:15:40.446113: step 8466, loss 0.113654, acc 0.96875, prec 0.113683, recall 0.77611
2017-12-10T03:15:40.713988: step 8467, loss 0.960886, acc 0.921875, prec 0.113686, recall 0.776128
2017-12-10T03:15:40.976792: step 8468, loss 0.0013497, acc 1, prec 0.113686, recall 0.776128
2017-12-10T03:15:41.250813: step 8469, loss 0.147818, acc 0.96875, prec 0.113684, recall 0.776128
2017-12-10T03:15:41.522453: step 8470, loss 0.453776, acc 0.953125, prec 0.113711, recall 0.776182
2017-12-10T03:15:41.788829: step 8471, loss 0.145719, acc 0.9375, prec 0.113706, recall 0.776182
2017-12-10T03:15:42.051864: step 8472, loss 0.396842, acc 0.9375, prec 0.1137, recall 0.776182
2017-12-10T03:15:42.313523: step 8473, loss 0.370269, acc 0.96875, prec 0.113708, recall 0.776201
2017-12-10T03:15:42.583675: step 8474, loss 0.581367, acc 0.953125, prec 0.113704, recall 0.776201
2017-12-10T03:15:42.846673: step 8475, loss 0.0845288, acc 0.984375, prec 0.113713, recall 0.776219
2017-12-10T03:15:43.116598: step 8476, loss 0.0897277, acc 0.96875, prec 0.113721, recall 0.776237
2017-12-10T03:15:43.379797: step 8477, loss 0.0755539, acc 0.984375, prec 0.11372, recall 0.776237
2017-12-10T03:15:43.645325: step 8478, loss 0.480799, acc 0.9375, prec 0.113725, recall 0.776255
2017-12-10T03:15:43.913274: step 8479, loss 0.196763, acc 0.96875, prec 0.113733, recall 0.776273
2017-12-10T03:15:44.174972: step 8480, loss 0.0002769, acc 1, prec 0.113754, recall 0.776309
2017-12-10T03:15:44.435564: step 8481, loss 0.17756, acc 0.953125, prec 0.11375, recall 0.776309
2017-12-10T03:15:44.708442: step 8482, loss 0.154007, acc 0.96875, prec 0.113747, recall 0.776309
2017-12-10T03:15:44.974848: step 8483, loss 0.41585, acc 0.96875, prec 0.113755, recall 0.776328
2017-12-10T03:15:45.241582: step 8484, loss 0.0487306, acc 0.984375, prec 0.113754, recall 0.776328
2017-12-10T03:15:45.506951: step 8485, loss 0.0365367, acc 0.984375, prec 0.113763, recall 0.776346
2017-12-10T03:15:45.771079: step 8486, loss 0.453696, acc 0.96875, prec 0.11376, recall 0.776346
2017-12-10T03:15:46.034721: step 8487, loss 0.504715, acc 0.984375, prec 0.113759, recall 0.776346
2017-12-10T03:15:46.295997: step 8488, loss 0.439097, acc 0.96875, prec 0.113756, recall 0.776346
2017-12-10T03:15:46.570322: step 8489, loss 0.227561, acc 0.96875, prec 0.113764, recall 0.776364
2017-12-10T03:15:46.836508: step 8490, loss 0.00521225, acc 1, prec 0.113785, recall 0.7764
2017-12-10T03:15:47.101653: step 8491, loss 1.36698, acc 0.921875, prec 0.113778, recall 0.7764
2017-12-10T03:15:47.367799: step 8492, loss 0.0104769, acc 1, prec 0.113778, recall 0.7764
2017-12-10T03:15:47.631381: step 8493, loss 0.451247, acc 0.984375, prec 0.113787, recall 0.776418
2017-12-10T03:15:47.895808: step 8494, loss 0.228707, acc 0.984375, prec 0.113786, recall 0.776418
2017-12-10T03:15:48.159072: step 8495, loss 0.0207283, acc 1, prec 0.113796, recall 0.776436
2017-12-10T03:15:48.421229: step 8496, loss 0.0920498, acc 0.96875, prec 0.113794, recall 0.776436
2017-12-10T03:15:48.682936: step 8497, loss 0.278123, acc 0.984375, prec 0.113792, recall 0.776436
2017-12-10T03:15:48.948485: step 8498, loss 0.0534142, acc 1, prec 0.113803, recall 0.776454
2017-12-10T03:15:49.215188: step 8499, loss 0.0208497, acc 0.984375, prec 0.113802, recall 0.776454
2017-12-10T03:15:49.493071: step 8500, loss 0.613159, acc 0.984375, prec 0.113811, recall 0.776473
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-8500

2017-12-10T03:15:50.742807: step 8501, loss 0.344722, acc 0.984375, prec 0.113809, recall 0.776473
2017-12-10T03:15:51.012323: step 8502, loss 0.377032, acc 1, prec 0.11383, recall 0.776509
2017-12-10T03:15:51.281379: step 8503, loss 9.24181e-05, acc 1, prec 0.11383, recall 0.776509
2017-12-10T03:15:51.546682: step 8504, loss 0.00172052, acc 1, prec 0.113851, recall 0.776545
2017-12-10T03:15:51.805062: step 8505, loss 0.0705438, acc 0.984375, prec 0.113892, recall 0.776617
2017-12-10T03:15:52.071642: step 8506, loss 5.75549e-07, acc 1, prec 0.113913, recall 0.776653
2017-12-10T03:15:52.331084: step 8507, loss 4.62841e-06, acc 1, prec 0.113913, recall 0.776653
2017-12-10T03:15:52.591294: step 8508, loss 0.00111227, acc 1, prec 0.113924, recall 0.776672
2017-12-10T03:15:52.856704: step 8509, loss 0.00494518, acc 1, prec 0.113924, recall 0.776672
2017-12-10T03:15:53.120933: step 8510, loss 0.0075173, acc 1, prec 0.113934, recall 0.77669
2017-12-10T03:15:53.386590: step 8511, loss 1.11383e-06, acc 1, prec 0.113934, recall 0.77669
2017-12-10T03:15:53.650478: step 8512, loss 5.13855e-05, acc 1, prec 0.113934, recall 0.77669
2017-12-10T03:15:53.911026: step 8513, loss 14.9466, acc 0.984375, prec 0.113955, recall 0.776663
2017-12-10T03:15:54.173532: step 8514, loss 0.00800941, acc 1, prec 0.113966, recall 0.776681
2017-12-10T03:15:54.436518: step 8515, loss 0.00755752, acc 1, prec 0.113966, recall 0.776681
2017-12-10T03:15:54.698664: step 8516, loss 0.0011638, acc 1, prec 0.113966, recall 0.776681
2017-12-10T03:15:54.965928: step 8517, loss 0.165101, acc 0.96875, prec 0.113963, recall 0.776681
2017-12-10T03:15:55.238766: step 8518, loss 0.274432, acc 0.953125, prec 0.11398, recall 0.776717
2017-12-10T03:15:55.500383: step 8519, loss 0.160463, acc 0.96875, prec 0.113988, recall 0.776735
2017-12-10T03:15:55.776610: step 8520, loss 0.895474, acc 0.921875, prec 0.114002, recall 0.776771
2017-12-10T03:15:56.044782: step 8521, loss 1.25441, acc 0.921875, prec 0.114016, recall 0.776807
2017-12-10T03:15:56.311381: step 8522, loss 0.570919, acc 0.921875, prec 0.11401, recall 0.776807
2017-12-10T03:15:56.583110: step 8523, loss 0.227735, acc 0.984375, prec 0.114019, recall 0.776825
2017-12-10T03:15:56.848665: step 8524, loss 0.0968651, acc 0.96875, prec 0.114016, recall 0.776825
2017-12-10T03:15:57.120594: step 8525, loss 0.0164565, acc 1, prec 0.114016, recall 0.776825
2017-12-10T03:15:57.384033: step 8526, loss 0.720771, acc 0.90625, prec 0.114019, recall 0.776843
2017-12-10T03:15:57.652310: step 8527, loss 0.606468, acc 0.921875, prec 0.114012, recall 0.776843
2017-12-10T03:15:57.917364: step 8528, loss 0.445994, acc 0.96875, prec 0.11402, recall 0.776861
2017-12-10T03:15:58.186078: step 8529, loss 0.437007, acc 0.953125, prec 0.114037, recall 0.776898
2017-12-10T03:15:58.446423: step 8530, loss 0.302599, acc 0.953125, prec 0.114043, recall 0.776916
2017-12-10T03:15:58.716348: step 8531, loss 0.358082, acc 0.890625, prec 0.114044, recall 0.776934
2017-12-10T03:15:58.979427: step 8532, loss 0.535718, acc 0.9375, prec 0.11406, recall 0.77697
2017-12-10T03:15:59.250471: step 8533, loss 1.1621, acc 0.859375, prec 0.114068, recall 0.777006
2017-12-10T03:15:59.512319: step 8534, loss 0.783487, acc 0.9375, prec 0.114074, recall 0.777024
2017-12-10T03:15:59.773525: step 8535, loss 0.0139741, acc 0.984375, prec 0.114072, recall 0.777024
2017-12-10T03:16:00.045800: step 8536, loss 0.61868, acc 0.921875, prec 0.114065, recall 0.777024
2017-12-10T03:16:00.313618: step 8537, loss 0.222034, acc 0.96875, prec 0.114063, recall 0.777024
2017-12-10T03:16:00.581406: step 8538, loss 0.0191061, acc 0.984375, prec 0.114072, recall 0.777042
2017-12-10T03:16:00.846190: step 8539, loss 0.0497327, acc 0.96875, prec 0.11409, recall 0.777078
2017-12-10T03:16:01.113066: step 8540, loss 0.207017, acc 0.9375, prec 0.114085, recall 0.777078
2017-12-10T03:16:01.381458: step 8541, loss 0.000214606, acc 1, prec 0.114085, recall 0.777078
2017-12-10T03:16:01.643376: step 8542, loss 0.0119764, acc 1, prec 0.114095, recall 0.777096
2017-12-10T03:16:01.910820: step 8543, loss 0.396776, acc 0.9375, prec 0.11409, recall 0.777096
2017-12-10T03:16:02.183611: step 8544, loss 0.011732, acc 0.984375, prec 0.114089, recall 0.777096
2017-12-10T03:16:02.453149: step 8545, loss 0.20376, acc 0.953125, prec 0.114084, recall 0.777096
2017-12-10T03:16:02.720075: step 8546, loss 0.260469, acc 0.953125, prec 0.114101, recall 0.777132
2017-12-10T03:16:02.984946: step 8547, loss 0.00435985, acc 1, prec 0.114122, recall 0.777168
2017-12-10T03:16:03.248319: step 8548, loss 0.164119, acc 0.96875, prec 0.11412, recall 0.777168
2017-12-10T03:16:03.515421: step 8549, loss 0.221792, acc 0.953125, prec 0.114126, recall 0.777186
2017-12-10T03:16:03.778544: step 8550, loss 0.254037, acc 0.953125, prec 0.114133, recall 0.777204
2017-12-10T03:16:04.048479: step 8551, loss 0.29422, acc 0.953125, prec 0.114129, recall 0.777204
2017-12-10T03:16:04.313977: step 8552, loss 0.122133, acc 0.96875, prec 0.114136, recall 0.777222
2017-12-10T03:16:04.579369: step 8553, loss 0.0971196, acc 0.96875, prec 0.114155, recall 0.777258
2017-12-10T03:16:04.844269: step 8554, loss 0.000914241, acc 1, prec 0.114155, recall 0.777258
2017-12-10T03:16:05.105583: step 8555, loss 0.00207676, acc 1, prec 0.114176, recall 0.777294
2017-12-10T03:16:05.373157: step 8556, loss 0.000112681, acc 1, prec 0.114197, recall 0.77733
2017-12-10T03:16:05.634550: step 8557, loss 0.483674, acc 0.953125, prec 0.114203, recall 0.777348
2017-12-10T03:16:05.899087: step 8558, loss 0.000100707, acc 1, prec 0.114214, recall 0.777366
2017-12-10T03:16:06.160640: step 8559, loss 0.0315968, acc 0.984375, prec 0.114233, recall 0.777401
2017-12-10T03:16:06.421029: step 8560, loss 0.00271449, acc 1, prec 0.114254, recall 0.777437
2017-12-10T03:16:06.686261: step 8561, loss 0.0014156, acc 1, prec 0.114265, recall 0.777455
2017-12-10T03:16:06.959890: step 8562, loss 0.0317488, acc 0.984375, prec 0.114295, recall 0.777509
2017-12-10T03:16:07.224356: step 8563, loss 0.00211167, acc 1, prec 0.114305, recall 0.777527
2017-12-10T03:16:07.481605: step 8564, loss 0.163379, acc 0.96875, prec 0.114303, recall 0.777527
2017-12-10T03:16:07.745185: step 8565, loss 0.212401, acc 0.984375, prec 0.114301, recall 0.777527
2017-12-10T03:16:08.011057: step 8566, loss 0.000318762, acc 1, prec 0.114301, recall 0.777527
2017-12-10T03:16:08.273796: step 8567, loss 1.50582e-05, acc 1, prec 0.114301, recall 0.777527
2017-12-10T03:16:08.531011: step 8568, loss 0.09348, acc 0.96875, prec 0.114299, recall 0.777527
2017-12-10T03:16:08.802487: step 8569, loss 4.1455, acc 0.984375, prec 0.114309, recall 0.777482
2017-12-10T03:16:09.071115: step 8570, loss 0.00538564, acc 1, prec 0.114372, recall 0.77759
2017-12-10T03:16:09.332385: step 8571, loss 0.22782, acc 0.96875, prec 0.11438, recall 0.777608
2017-12-10T03:16:09.599560: step 8572, loss 0.0245338, acc 0.984375, prec 0.11441, recall 0.777661
2017-12-10T03:16:09.866658: step 8573, loss 0.0954466, acc 0.984375, prec 0.114419, recall 0.777679
2017-12-10T03:16:10.130250: step 8574, loss 0.185368, acc 0.96875, prec 0.114427, recall 0.777697
2017-12-10T03:16:10.396477: step 8575, loss 0.00254107, acc 1, prec 0.114448, recall 0.777733
2017-12-10T03:16:10.667752: step 8576, loss 0.61971, acc 0.984375, prec 0.114446, recall 0.777733
2017-12-10T03:16:10.935753: step 8577, loss 0.701617, acc 0.984375, prec 0.114456, recall 0.777751
2017-12-10T03:16:11.198399: step 8578, loss 0.647471, acc 0.953125, prec 0.114483, recall 0.777805
2017-12-10T03:16:11.463720: step 8579, loss 0.385858, acc 0.96875, prec 0.11448, recall 0.777805
2017-12-10T03:16:11.738034: step 8580, loss 0.238358, acc 0.984375, prec 0.1145, recall 0.77784
2017-12-10T03:16:12.005874: step 8581, loss 0.0315783, acc 1, prec 0.11451, recall 0.777858
2017-12-10T03:16:12.267311: step 8582, loss 0.77993, acc 0.921875, prec 0.114514, recall 0.777876
2017-12-10T03:16:12.528083: step 8583, loss 0.267004, acc 0.921875, prec 0.114518, recall 0.777894
2017-12-10T03:16:12.800482: step 8584, loss 0.0770248, acc 0.984375, prec 0.114548, recall 0.777948
2017-12-10T03:16:13.071575: step 8585, loss 0.265566, acc 0.953125, prec 0.114554, recall 0.777965
2017-12-10T03:16:13.342442: step 8586, loss 0.923043, acc 0.90625, prec 0.114546, recall 0.777965
2017-12-10T03:16:13.608289: step 8587, loss 0.0756277, acc 0.96875, prec 0.114554, recall 0.777983
2017-12-10T03:16:13.873982: step 8588, loss 0.654294, acc 0.953125, prec 0.114581, recall 0.778037
2017-12-10T03:16:14.145124: step 8589, loss 0.207377, acc 0.96875, prec 0.1146, recall 0.778072
2017-12-10T03:16:14.410682: step 8590, loss 0.254443, acc 0.90625, prec 0.114591, recall 0.778072
2017-12-10T03:16:14.671479: step 8591, loss 0.0372968, acc 0.96875, prec 0.114589, recall 0.778072
2017-12-10T03:16:14.940907: step 8592, loss 0.346155, acc 0.96875, prec 0.114597, recall 0.77809
2017-12-10T03:16:15.207316: step 8593, loss 0.375522, acc 0.875, prec 0.114596, recall 0.778108
2017-12-10T03:16:15.475735: step 8594, loss 0.668596, acc 0.90625, prec 0.114598, recall 0.778126
2017-12-10T03:16:15.741754: step 8595, loss 0.0771842, acc 0.96875, prec 0.114606, recall 0.778144
2017-12-10T03:16:16.004808: step 8596, loss 0.00192794, acc 1, prec 0.114617, recall 0.778162
2017-12-10T03:16:16.269236: step 8597, loss 0.500081, acc 0.953125, prec 0.114613, recall 0.778162
2017-12-10T03:16:16.538647: step 8598, loss 0.155481, acc 0.953125, prec 0.11463, recall 0.778197
2017-12-10T03:16:16.799174: step 8599, loss 0.93101, acc 0.9375, prec 0.114624, recall 0.778197
2017-12-10T03:16:17.061202: step 8600, loss 0.000249657, acc 1, prec 0.114624, recall 0.778197
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-8600

2017-12-10T03:16:18.355449: step 8601, loss 0.00220803, acc 1, prec 0.114645, recall 0.778233
2017-12-10T03:16:18.617175: step 8602, loss 0.250939, acc 0.96875, prec 0.114642, recall 0.778233
2017-12-10T03:16:18.879111: step 8603, loss 0.791487, acc 0.96875, prec 0.11465, recall 0.778251
2017-12-10T03:16:19.152046: step 8604, loss 0.064579, acc 0.984375, prec 0.114659, recall 0.778269
2017-12-10T03:16:19.416241: step 8605, loss 0.633907, acc 0.984375, prec 0.114658, recall 0.778269
2017-12-10T03:16:19.684367: step 8606, loss 0.0504318, acc 0.984375, prec 0.114677, recall 0.778304
2017-12-10T03:16:19.950985: step 8607, loss 0.35625, acc 1, prec 0.114719, recall 0.778375
2017-12-10T03:16:20.214402: step 8608, loss 0.189623, acc 0.984375, prec 0.114728, recall 0.778393
2017-12-10T03:16:20.483459: step 8609, loss 2.43238, acc 0.9375, prec 0.114735, recall 0.778348
2017-12-10T03:16:20.762216: step 8610, loss 0.264201, acc 0.984375, prec 0.114734, recall 0.778348
2017-12-10T03:16:21.030622: step 8611, loss 0.114163, acc 0.96875, prec 0.114731, recall 0.778348
2017-12-10T03:16:21.302054: step 8612, loss 0.2259, acc 0.96875, prec 0.114739, recall 0.778366
2017-12-10T03:16:21.566475: step 8613, loss 0.0106159, acc 1, prec 0.11476, recall 0.778402
2017-12-10T03:16:21.833377: step 8614, loss 0.0939217, acc 0.984375, prec 0.114779, recall 0.778437
2017-12-10T03:16:22.098438: step 8615, loss 0.103302, acc 0.984375, prec 0.114788, recall 0.778455
2017-12-10T03:16:22.368713: step 8616, loss 0.517538, acc 0.96875, prec 0.114796, recall 0.778473
2017-12-10T03:16:22.637496: step 8617, loss 0.312467, acc 0.96875, prec 0.114804, recall 0.778491
2017-12-10T03:16:22.903659: step 8618, loss 0.605685, acc 0.953125, prec 0.114821, recall 0.778526
2017-12-10T03:16:23.169924: step 8619, loss 0.635857, acc 0.921875, prec 0.114824, recall 0.778544
2017-12-10T03:16:23.432419: step 8620, loss 0.375559, acc 0.984375, prec 0.114833, recall 0.778562
2017-12-10T03:16:23.698662: step 8621, loss 0.0809146, acc 0.96875, prec 0.114831, recall 0.778562
2017-12-10T03:16:23.961049: step 8622, loss 1.21678, acc 0.921875, prec 0.114824, recall 0.778562
2017-12-10T03:16:24.236669: step 8623, loss 0.0657042, acc 0.984375, prec 0.114833, recall 0.778579
2017-12-10T03:16:24.504474: step 8624, loss 0.0616029, acc 0.984375, prec 0.114842, recall 0.778597
2017-12-10T03:16:24.772556: step 8625, loss 0.0580802, acc 0.96875, prec 0.11485, recall 0.778615
2017-12-10T03:16:25.040048: step 8626, loss 0.24955, acc 0.96875, prec 0.114858, recall 0.778633
2017-12-10T03:16:25.301167: step 8627, loss 0.0179386, acc 0.984375, prec 0.114856, recall 0.778633
2017-12-10T03:16:25.564530: step 8628, loss 0.683157, acc 0.96875, prec 0.114874, recall 0.778668
2017-12-10T03:16:25.833692: step 8629, loss 0.0830104, acc 0.953125, prec 0.114881, recall 0.778686
2017-12-10T03:16:26.096259: step 8630, loss 0.352326, acc 0.984375, prec 0.11489, recall 0.778704
2017-12-10T03:16:26.370729: step 8631, loss 0.0704486, acc 0.953125, prec 0.114896, recall 0.778721
2017-12-10T03:16:26.643805: step 8632, loss 0.0740244, acc 0.96875, prec 0.114894, recall 0.778721
2017-12-10T03:16:26.907893: step 8633, loss 0.578579, acc 0.953125, prec 0.1149, recall 0.778739
2017-12-10T03:16:27.170231: step 8634, loss 0.0491942, acc 0.984375, prec 0.114899, recall 0.778739
2017-12-10T03:16:27.432524: step 8635, loss 0.0728856, acc 0.984375, prec 0.114897, recall 0.778739
2017-12-10T03:16:27.693394: step 8636, loss 0.368739, acc 0.984375, prec 0.114906, recall 0.778757
2017-12-10T03:16:27.961670: step 8637, loss 0.818619, acc 0.984375, prec 0.114905, recall 0.778757
2017-12-10T03:16:28.223250: step 8638, loss 0.0795619, acc 0.984375, prec 0.114935, recall 0.77881
2017-12-10T03:16:28.484803: step 8639, loss 0.0257591, acc 0.984375, prec 0.114955, recall 0.778845
2017-12-10T03:16:28.758473: step 8640, loss 0.0468265, acc 0.96875, prec 0.114952, recall 0.778845
2017-12-10T03:16:29.022575: step 8641, loss 0.0285822, acc 1, prec 0.114973, recall 0.778881
2017-12-10T03:16:29.281550: step 8642, loss 0.622081, acc 0.9375, prec 0.114988, recall 0.778916
2017-12-10T03:16:29.548848: step 8643, loss 0.0387585, acc 0.984375, prec 0.115008, recall 0.778952
2017-12-10T03:16:29.818967: step 8644, loss 0.00469457, acc 1, prec 0.115008, recall 0.778952
2017-12-10T03:16:30.088850: step 8645, loss 0.327874, acc 0.953125, prec 0.115004, recall 0.778952
2017-12-10T03:16:30.357356: step 8646, loss 0.323155, acc 0.96875, prec 0.115001, recall 0.778952
2017-12-10T03:16:30.626958: step 8647, loss 0.200329, acc 0.96875, prec 0.115009, recall 0.778969
2017-12-10T03:16:30.890547: step 8648, loss 0.700499, acc 0.96875, prec 0.115017, recall 0.778987
2017-12-10T03:16:31.160289: step 8649, loss 16.5889, acc 0.96875, prec 0.115026, recall 0.778942
2017-12-10T03:16:31.425569: step 8650, loss 0.946971, acc 0.921875, prec 0.115019, recall 0.778942
2017-12-10T03:16:31.684571: step 8651, loss 0.616616, acc 0.953125, prec 0.115025, recall 0.77896
2017-12-10T03:16:31.949104: step 8652, loss 0.642742, acc 0.9375, prec 0.11502, recall 0.77896
2017-12-10T03:16:32.220131: step 8653, loss 0.439468, acc 0.96875, prec 0.115017, recall 0.77896
2017-12-10T03:16:32.484889: step 8654, loss 0.077174, acc 0.953125, prec 0.115023, recall 0.778978
2017-12-10T03:16:32.746975: step 8655, loss 0.283816, acc 0.9375, prec 0.115039, recall 0.779013
2017-12-10T03:16:33.015455: step 8656, loss 0.666918, acc 0.875, prec 0.115049, recall 0.779048
2017-12-10T03:16:33.281069: step 8657, loss 0.309503, acc 0.9375, prec 0.115054, recall 0.779066
2017-12-10T03:16:33.547960: step 8658, loss 0.243778, acc 0.953125, prec 0.115071, recall 0.779101
2017-12-10T03:16:33.810695: step 8659, loss 0.11368, acc 0.96875, prec 0.115089, recall 0.779137
2017-12-10T03:16:34.073944: step 8660, loss 1.32619, acc 0.921875, prec 0.115103, recall 0.779172
2017-12-10T03:16:34.342656: step 8661, loss 0.420811, acc 0.90625, prec 0.115105, recall 0.77919
2017-12-10T03:16:34.605737: step 8662, loss 1.39252, acc 0.828125, prec 0.115101, recall 0.779207
2017-12-10T03:16:34.869544: step 8663, loss 1.27682, acc 0.8125, prec 0.115095, recall 0.779225
2017-12-10T03:16:35.148043: step 8664, loss 0.99954, acc 0.84375, prec 0.115102, recall 0.77926
2017-12-10T03:16:35.414382: step 8665, loss 0.287302, acc 0.921875, prec 0.115096, recall 0.77926
2017-12-10T03:16:35.689454: step 8666, loss 0.814815, acc 0.875, prec 0.115085, recall 0.77926
2017-12-10T03:16:35.954821: step 8667, loss 1.2722, acc 0.875, prec 0.115095, recall 0.779295
2017-12-10T03:16:36.219382: step 8668, loss 0.475997, acc 0.953125, prec 0.115091, recall 0.779295
2017-12-10T03:16:36.484402: step 8669, loss 0.408181, acc 0.90625, prec 0.115093, recall 0.779313
2017-12-10T03:16:36.747615: step 8670, loss 0.586121, acc 0.9375, prec 0.115087, recall 0.779313
2017-12-10T03:16:37.012024: step 8671, loss 0.439025, acc 0.90625, prec 0.115079, recall 0.779313
2017-12-10T03:16:37.276858: step 8672, loss 0.439187, acc 0.921875, prec 0.115083, recall 0.779331
2017-12-10T03:16:37.541898: step 8673, loss 0.115124, acc 0.984375, prec 0.115082, recall 0.779331
2017-12-10T03:16:37.809211: step 8674, loss 0.140497, acc 0.96875, prec 0.115089, recall 0.779348
2017-12-10T03:16:38.073901: step 8675, loss 0.275155, acc 0.9375, prec 0.115084, recall 0.779348
2017-12-10T03:16:38.340066: step 8676, loss 0.0687124, acc 0.984375, prec 0.115124, recall 0.779419
2017-12-10T03:16:38.601653: step 8677, loss 6.1741e-05, acc 1, prec 0.115145, recall 0.779454
2017-12-10T03:16:38.869259: step 8678, loss 0.0267958, acc 0.984375, prec 0.115154, recall 0.779472
2017-12-10T03:16:39.136893: step 8679, loss 0.100483, acc 0.96875, prec 0.115162, recall 0.779489
2017-12-10T03:16:39.405436: step 8680, loss 0.137269, acc 0.96875, prec 0.115159, recall 0.779489
2017-12-10T03:16:39.675487: step 8681, loss 0.312427, acc 0.953125, prec 0.115166, recall 0.779507
2017-12-10T03:16:39.937723: step 8682, loss 0.0909164, acc 0.984375, prec 0.115196, recall 0.77956
2017-12-10T03:16:40.209122: step 8683, loss 0.0289795, acc 0.984375, prec 0.115225, recall 0.779612
2017-12-10T03:16:40.481123: step 8684, loss 0.00278945, acc 1, prec 0.115225, recall 0.779612
2017-12-10T03:16:40.749678: step 8685, loss 0.0772291, acc 0.984375, prec 0.115224, recall 0.779612
2017-12-10T03:16:41.014998: step 8686, loss 0.99137, acc 0.96875, prec 0.115232, recall 0.77963
2017-12-10T03:16:41.282904: step 8687, loss 0.134334, acc 0.96875, prec 0.115229, recall 0.77963
2017-12-10T03:16:41.561274: step 8688, loss 0.00736223, acc 1, prec 0.115229, recall 0.77963
2017-12-10T03:16:41.826982: step 8689, loss 0.116625, acc 0.984375, prec 0.115228, recall 0.77963
2017-12-10T03:16:42.092576: step 8690, loss 0.0002778, acc 1, prec 0.115238, recall 0.779648
2017-12-10T03:16:42.354432: step 8691, loss 5.57989e-05, acc 1, prec 0.115249, recall 0.779665
2017-12-10T03:16:42.609067: step 8692, loss 0.0195529, acc 0.984375, prec 0.115247, recall 0.779665
2017-12-10T03:16:42.871293: step 8693, loss 3.65473e-05, acc 1, prec 0.115268, recall 0.7797
2017-12-10T03:16:43.129088: step 8694, loss 0.00291373, acc 1, prec 0.115268, recall 0.7797
2017-12-10T03:16:43.388676: step 8695, loss 4.75535e-05, acc 1, prec 0.115268, recall 0.7797
2017-12-10T03:16:43.648925: step 8696, loss 0.264119, acc 0.953125, prec 0.115274, recall 0.779718
2017-12-10T03:16:43.916361: step 8697, loss 0.260701, acc 0.984375, prec 0.115273, recall 0.779718
2017-12-10T03:16:44.177416: step 8698, loss 0.00252712, acc 1, prec 0.115273, recall 0.779718
2017-12-10T03:16:44.440093: step 8699, loss 0.309781, acc 0.984375, prec 0.115272, recall 0.779718
2017-12-10T03:16:44.707925: step 8700, loss 0.481067, acc 1, prec 0.115282, recall 0.779735

Evaluation:
2017-12-10T03:16:52.324812: step 8700, loss 21.804, acc 0.971978, prec 0.115491, recall 0.773056

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-8700

2017-12-10T03:16:53.588557: step 8701, loss 0.0826325, acc 0.984375, prec 0.1155, recall 0.773074
2017-12-10T03:16:53.851337: step 8702, loss 10.1498, acc 0.984375, prec 0.11551, recall 0.773031
2017-12-10T03:16:54.115145: step 8703, loss 0.198185, acc 0.96875, prec 0.115508, recall 0.773031
2017-12-10T03:16:54.375301: step 8704, loss 0.000418393, acc 1, prec 0.115518, recall 0.773049
2017-12-10T03:16:54.640829: step 8705, loss 0.623676, acc 0.984375, prec 0.115517, recall 0.773049
2017-12-10T03:16:54.912272: step 8706, loss 0.00553459, acc 1, prec 0.115517, recall 0.773049
2017-12-10T03:16:55.177151: step 8707, loss 0.684615, acc 0.953125, prec 0.115533, recall 0.773085
2017-12-10T03:16:55.451189: step 8708, loss 0.373743, acc 0.953125, prec 0.11554, recall 0.773103
2017-12-10T03:16:55.719706: step 8709, loss 0.782402, acc 0.921875, prec 0.115533, recall 0.773103
2017-12-10T03:16:55.984665: step 8710, loss 0.437733, acc 0.953125, prec 0.11556, recall 0.773156
2017-12-10T03:16:56.252557: step 8711, loss 0.758654, acc 0.921875, prec 0.115553, recall 0.773156
2017-12-10T03:16:56.521923: step 8712, loss 0.229362, acc 0.953125, prec 0.11556, recall 0.773174
2017-12-10T03:16:56.795067: step 8713, loss 0.121888, acc 0.984375, prec 0.115558, recall 0.773174
2017-12-10T03:16:57.057774: step 8714, loss 0.646746, acc 0.9375, prec 0.115563, recall 0.773192
2017-12-10T03:16:57.329175: step 8715, loss 0.272853, acc 0.90625, prec 0.115555, recall 0.773192
2017-12-10T03:16:57.593845: step 8716, loss 0.155077, acc 0.96875, prec 0.115552, recall 0.773192
2017-12-10T03:16:57.858372: step 8717, loss 0.530962, acc 0.96875, prec 0.11555, recall 0.773192
2017-12-10T03:16:58.123924: step 8718, loss 0.986491, acc 0.90625, prec 0.115562, recall 0.773227
2017-12-10T03:16:58.395862: step 8719, loss 0.163627, acc 0.953125, prec 0.115569, recall 0.773245
2017-12-10T03:16:58.673936: step 8720, loss 0.0657289, acc 0.984375, prec 0.115609, recall 0.773317
2017-12-10T03:16:58.943377: step 8721, loss 0.197344, acc 0.984375, prec 0.115618, recall 0.773334
2017-12-10T03:16:59.214542: step 8722, loss 3.22798, acc 0.953125, prec 0.115615, recall 0.773274
2017-12-10T03:16:59.483800: step 8723, loss 0.949774, acc 0.953125, prec 0.115611, recall 0.773274
2017-12-10T03:16:59.752869: step 8724, loss 0.145656, acc 0.96875, prec 0.115619, recall 0.773291
2017-12-10T03:17:00.022342: step 8725, loss 0.656432, acc 0.921875, prec 0.115622, recall 0.773309
2017-12-10T03:17:00.290469: step 8726, loss 1.49358, acc 0.875, prec 0.115622, recall 0.773327
2017-12-10T03:17:00.562757: step 8727, loss 0.242972, acc 0.9375, prec 0.115637, recall 0.773363
2017-12-10T03:17:00.830692: step 8728, loss 0.169177, acc 0.9375, prec 0.115652, recall 0.773398
2017-12-10T03:17:01.102404: step 8729, loss 0.104911, acc 0.953125, prec 0.115669, recall 0.773434
2017-12-10T03:17:01.376186: step 8730, loss 0.474195, acc 0.96875, prec 0.115666, recall 0.773434
2017-12-10T03:17:01.639939: step 8731, loss 0.513605, acc 0.890625, prec 0.115657, recall 0.773434
2017-12-10T03:17:01.906130: step 8732, loss 0.456128, acc 0.9375, prec 0.115662, recall 0.773452
2017-12-10T03:17:02.171828: step 8733, loss 0.118291, acc 0.953125, prec 0.115658, recall 0.773452
2017-12-10T03:17:02.436569: step 8734, loss 1.14777, acc 0.90625, prec 0.11567, recall 0.773487
2017-12-10T03:17:02.706010: step 8735, loss 1.034, acc 0.921875, prec 0.115664, recall 0.773487
2017-12-10T03:17:02.972960: step 8736, loss 0.610949, acc 0.953125, prec 0.11566, recall 0.773487
2017-12-10T03:17:03.239470: step 8737, loss 0.92332, acc 0.921875, prec 0.115663, recall 0.773505
2017-12-10T03:17:03.499969: step 8738, loss 0.36964, acc 0.90625, prec 0.115655, recall 0.773505
2017-12-10T03:17:03.767399: step 8739, loss 0.419051, acc 0.9375, prec 0.11566, recall 0.773523
2017-12-10T03:17:04.032940: step 8740, loss 0.216058, acc 0.9375, prec 0.115665, recall 0.77354
2017-12-10T03:17:04.297993: step 8741, loss 0.296158, acc 0.9375, prec 0.11568, recall 0.773576
2017-12-10T03:17:04.563323: step 8742, loss 0.456529, acc 0.953125, prec 0.115686, recall 0.773594
2017-12-10T03:17:04.830068: step 8743, loss 0.366954, acc 0.96875, prec 0.115684, recall 0.773594
2017-12-10T03:17:05.101970: step 8744, loss 0.309261, acc 0.96875, prec 0.115691, recall 0.773612
2017-12-10T03:17:05.372718: step 8745, loss 0.445795, acc 0.953125, prec 0.115698, recall 0.773629
2017-12-10T03:17:05.631393: step 8746, loss 0.126613, acc 0.953125, prec 0.115694, recall 0.773629
2017-12-10T03:17:05.893603: step 8747, loss 0.201963, acc 0.96875, prec 0.115701, recall 0.773647
2017-12-10T03:17:06.156150: step 8748, loss 0.0837561, acc 0.984375, prec 0.115721, recall 0.773683
2017-12-10T03:17:06.417644: step 8749, loss 0.800666, acc 0.953125, prec 0.115717, recall 0.773683
2017-12-10T03:17:06.686052: step 8750, loss 0.131752, acc 0.953125, prec 0.115713, recall 0.773683
2017-12-10T03:17:06.958117: step 8751, loss 0.00126554, acc 1, prec 0.115723, recall 0.7737
2017-12-10T03:17:07.229369: step 8752, loss 0.641965, acc 0.984375, prec 0.115753, recall 0.773754
2017-12-10T03:17:07.505257: step 8753, loss 0.351359, acc 0.953125, prec 0.11578, recall 0.773807
2017-12-10T03:17:07.767593: step 8754, loss 0.675698, acc 0.984375, prec 0.115778, recall 0.773807
2017-12-10T03:17:08.038993: step 8755, loss 0.0210445, acc 0.984375, prec 0.115798, recall 0.773842
2017-12-10T03:17:08.302943: step 8756, loss 0.662616, acc 0.953125, prec 0.115825, recall 0.773895
2017-12-10T03:17:08.568287: step 8757, loss 0.579883, acc 0.984375, prec 0.115844, recall 0.773931
2017-12-10T03:17:08.833172: step 8758, loss 0.0105539, acc 1, prec 0.115844, recall 0.773931
2017-12-10T03:17:09.097629: step 8759, loss 1.57986, acc 0.96875, prec 0.115853, recall 0.773888
2017-12-10T03:17:09.368930: step 8760, loss 0.0343502, acc 0.984375, prec 0.115852, recall 0.773888
2017-12-10T03:17:09.634992: step 8761, loss 0.0431493, acc 0.984375, prec 0.115861, recall 0.773906
2017-12-10T03:17:09.897310: step 8762, loss 0.163799, acc 0.984375, prec 0.11587, recall 0.773923
2017-12-10T03:17:10.161885: step 8763, loss 0.0020167, acc 1, prec 0.11587, recall 0.773923
2017-12-10T03:17:10.420351: step 8764, loss 0.0867096, acc 0.984375, prec 0.115868, recall 0.773923
2017-12-10T03:17:10.690221: step 8765, loss 0.387052, acc 0.96875, prec 0.115876, recall 0.773941
2017-12-10T03:17:10.952643: step 8766, loss 0.136404, acc 0.984375, prec 0.115885, recall 0.773959
2017-12-10T03:17:11.219888: step 8767, loss 0.502598, acc 0.953125, prec 0.115891, recall 0.773976
2017-12-10T03:17:11.489276: step 8768, loss 0.574897, acc 0.9375, prec 0.115896, recall 0.773994
2017-12-10T03:17:11.759710: step 8769, loss 0.294328, acc 0.96875, prec 0.115904, recall 0.774012
2017-12-10T03:17:12.024443: step 8770, loss 0.188744, acc 0.953125, prec 0.11591, recall 0.774029
2017-12-10T03:17:12.294025: step 8771, loss 0.00734103, acc 1, prec 0.11591, recall 0.774029
2017-12-10T03:17:12.555179: step 8772, loss 0.315551, acc 0.953125, prec 0.115917, recall 0.774047
2017-12-10T03:17:12.820302: step 8773, loss 0.0499772, acc 0.984375, prec 0.115936, recall 0.774082
2017-12-10T03:17:13.085952: step 8774, loss 0.433905, acc 0.9375, prec 0.115941, recall 0.7741
2017-12-10T03:17:13.354745: step 8775, loss 0.130185, acc 0.96875, prec 0.115938, recall 0.7741
2017-12-10T03:17:13.619622: step 8776, loss 0.221005, acc 0.984375, prec 0.115937, recall 0.7741
2017-12-10T03:17:13.880417: step 8777, loss 0.323979, acc 0.96875, prec 0.115955, recall 0.774136
2017-12-10T03:17:14.144600: step 8778, loss 0.151024, acc 0.96875, prec 0.115962, recall 0.774153
2017-12-10T03:17:14.405428: step 8779, loss 0.124714, acc 0.953125, prec 0.115969, recall 0.774171
2017-12-10T03:17:14.670693: step 8780, loss 0.691699, acc 0.9375, prec 0.115974, recall 0.774189
2017-12-10T03:17:14.933040: step 8781, loss 0.0852127, acc 0.96875, prec 0.116012, recall 0.774259
2017-12-10T03:17:15.192935: step 8782, loss 0.0444774, acc 0.984375, prec 0.116032, recall 0.774294
2017-12-10T03:17:15.456540: step 8783, loss 0.246289, acc 0.96875, prec 0.116029, recall 0.774294
2017-12-10T03:17:15.722875: step 8784, loss 0.540821, acc 0.953125, prec 0.116046, recall 0.77433
2017-12-10T03:17:15.984620: step 8785, loss 0.00585149, acc 1, prec 0.116046, recall 0.77433
2017-12-10T03:17:16.248638: step 8786, loss 0.379544, acc 0.96875, prec 0.116064, recall 0.774365
2017-12-10T03:17:16.517760: step 8787, loss 0.00011027, acc 1, prec 0.116074, recall 0.774383
2017-12-10T03:17:16.779126: step 8788, loss 0.279777, acc 0.953125, prec 0.116091, recall 0.774418
2017-12-10T03:17:17.041659: step 8789, loss 0.236773, acc 0.953125, prec 0.116086, recall 0.774418
2017-12-10T03:17:17.305397: step 8790, loss 0.123275, acc 0.96875, prec 0.116104, recall 0.774453
2017-12-10T03:17:17.573317: step 8791, loss 0.0358649, acc 0.984375, prec 0.116103, recall 0.774453
2017-12-10T03:17:17.840901: step 8792, loss 0.14294, acc 0.984375, prec 0.116112, recall 0.774471
2017-12-10T03:17:18.104717: step 8793, loss 0.123232, acc 0.984375, prec 0.116121, recall 0.774488
2017-12-10T03:17:18.370662: step 8794, loss 0.109472, acc 0.984375, prec 0.11613, recall 0.774506
2017-12-10T03:17:18.641150: step 8795, loss 0.0996058, acc 0.984375, prec 0.116139, recall 0.774524
2017-12-10T03:17:18.903061: step 8796, loss 0.0010275, acc 1, prec 0.116139, recall 0.774524
2017-12-10T03:17:19.163736: step 8797, loss 0.111041, acc 0.96875, prec 0.116167, recall 0.774576
2017-12-10T03:17:19.429405: step 8798, loss 0.034384, acc 1, prec 0.116188, recall 0.774612
2017-12-10T03:17:19.700997: step 8799, loss 0.64011, acc 0.953125, prec 0.116194, recall 0.774629
2017-12-10T03:17:19.959713: step 8800, loss 0.180405, acc 0.984375, prec 0.116203, recall 0.774647
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-8800

2017-12-10T03:17:21.327699: step 8801, loss 1.76015e-06, acc 1, prec 0.116203, recall 0.774647
2017-12-10T03:17:21.583619: step 8802, loss 5.563e-06, acc 1, prec 0.116203, recall 0.774647
2017-12-10T03:17:21.842886: step 8803, loss 0.337461, acc 0.984375, prec 0.116202, recall 0.774647
2017-12-10T03:17:22.102497: step 8804, loss 0.000835017, acc 1, prec 0.116202, recall 0.774647
2017-12-10T03:17:22.367866: step 8805, loss 0.127104, acc 0.984375, prec 0.116201, recall 0.774647
2017-12-10T03:17:22.630050: step 8806, loss 0.000881922, acc 1, prec 0.116201, recall 0.774647
2017-12-10T03:17:22.890716: step 8807, loss 0.0749735, acc 0.984375, prec 0.116199, recall 0.774647
2017-12-10T03:17:23.151279: step 8808, loss 0.0574407, acc 0.984375, prec 0.116229, recall 0.7747
2017-12-10T03:17:23.415123: step 8809, loss 1.16962e-05, acc 1, prec 0.116229, recall 0.7747
2017-12-10T03:17:23.677126: step 8810, loss 0.247296, acc 0.9375, prec 0.116234, recall 0.774717
2017-12-10T03:17:23.946126: step 8811, loss 0.00139186, acc 1, prec 0.116244, recall 0.774735
2017-12-10T03:17:24.930311: step 8812, loss 0.147856, acc 0.96875, prec 0.116262, recall 0.77477
2017-12-10T03:17:25.278082: step 8813, loss 9.17152e-06, acc 1, prec 0.116283, recall 0.774805
2017-12-10T03:17:25.536630: step 8814, loss 0.000278807, acc 1, prec 0.116283, recall 0.774805
2017-12-10T03:17:26.091889: step 8815, loss 9.36048e-05, acc 1, prec 0.116293, recall 0.774823
2017-12-10T03:17:26.848353: step 8816, loss 0.013136, acc 0.984375, prec 0.116292, recall 0.774823
2017-12-10T03:17:27.600420: step 8817, loss 1.30385e-08, acc 1, prec 0.116292, recall 0.774823
2017-12-10T03:17:28.306322: step 8818, loss 0.000996787, acc 1, prec 0.116302, recall 0.77484
2017-12-10T03:17:29.039214: step 8819, loss 0.28896, acc 0.984375, prec 0.116301, recall 0.77484
2017-12-10T03:17:29.777963: step 8820, loss 0.0436519, acc 0.984375, prec 0.11632, recall 0.774875
2017-12-10T03:17:30.499332: step 8821, loss 2.99303, acc 0.984375, prec 0.116331, recall 0.774832
2017-12-10T03:17:31.242385: step 8822, loss 0.333755, acc 0.984375, prec 0.11635, recall 0.774867
2017-12-10T03:17:31.975059: step 8823, loss 0.107832, acc 0.984375, prec 0.116359, recall 0.774885
2017-12-10T03:17:32.863198: step 8824, loss 0.755348, acc 0.984375, prec 0.116378, recall 0.77492
2017-12-10T03:17:33.210022: step 8825, loss 0.21478, acc 0.984375, prec 0.116387, recall 0.774938
2017-12-10T03:17:33.497082: step 8826, loss 0.100541, acc 0.984375, prec 0.116396, recall 0.774955
2017-12-10T03:17:33.780131: step 8827, loss 1.13815, acc 0.96875, prec 0.116435, recall 0.775025
2017-12-10T03:17:34.072990: step 8828, loss 0.137697, acc 0.9375, prec 0.116429, recall 0.775025
2017-12-10T03:17:34.369677: step 8829, loss 0.175179, acc 0.953125, prec 0.116436, recall 0.775043
2017-12-10T03:17:34.652351: step 8830, loss 0.420939, acc 0.953125, prec 0.116431, recall 0.775043
2017-12-10T03:17:34.922137: step 8831, loss 0.198062, acc 0.953125, prec 0.116427, recall 0.775043
2017-12-10T03:17:35.190914: step 8832, loss 0.0510188, acc 0.984375, prec 0.116426, recall 0.775043
2017-12-10T03:17:35.454585: step 8833, loss 0.0463114, acc 0.984375, prec 0.116425, recall 0.775043
2017-12-10T03:17:35.718328: step 8834, loss 0.385005, acc 0.953125, prec 0.116421, recall 0.775043
2017-12-10T03:17:35.992693: step 8835, loss 0.65601, acc 0.859375, prec 0.116408, recall 0.775043
2017-12-10T03:17:36.259414: step 8836, loss 0.464599, acc 0.921875, prec 0.116412, recall 0.77506
2017-12-10T03:17:36.528912: step 8837, loss 0.399292, acc 0.953125, prec 0.116408, recall 0.77506
2017-12-10T03:17:36.791959: step 8838, loss 0.244282, acc 0.96875, prec 0.116415, recall 0.775078
2017-12-10T03:17:37.064005: step 8839, loss 0.538512, acc 0.953125, prec 0.116411, recall 0.775078
2017-12-10T03:17:37.329129: step 8840, loss 0.113248, acc 0.96875, prec 0.116409, recall 0.775078
2017-12-10T03:17:37.590709: step 8841, loss 0.676809, acc 0.90625, prec 0.116411, recall 0.775095
2017-12-10T03:17:37.849147: step 8842, loss 0.576191, acc 0.9375, prec 0.116416, recall 0.775113
2017-12-10T03:17:38.113062: step 8843, loss 0.226204, acc 0.96875, prec 0.116413, recall 0.775113
2017-12-10T03:17:38.375724: step 8844, loss 1.06842, acc 0.96875, prec 0.11641, recall 0.775113
2017-12-10T03:17:38.643803: step 8845, loss 0.380243, acc 0.96875, prec 0.116418, recall 0.77513
2017-12-10T03:17:38.914484: step 8846, loss 0.584205, acc 0.921875, prec 0.116421, recall 0.775148
2017-12-10T03:17:39.177472: step 8847, loss 1.31333, acc 0.96875, prec 0.11645, recall 0.7752
2017-12-10T03:17:39.440562: step 8848, loss 0.38693, acc 0.9375, prec 0.116465, recall 0.775235
2017-12-10T03:17:39.714709: step 8849, loss 0.21094, acc 0.9375, prec 0.11648, recall 0.77527
2017-12-10T03:17:39.979954: step 8850, loss 0.535567, acc 0.984375, prec 0.116489, recall 0.775288
2017-12-10T03:17:40.242545: step 8851, loss 0.00301975, acc 1, prec 0.116489, recall 0.775288
2017-12-10T03:17:40.510863: step 8852, loss 0.907995, acc 0.921875, prec 0.116482, recall 0.775288
2017-12-10T03:17:40.774610: step 8853, loss 0.0124627, acc 0.984375, prec 0.116501, recall 0.775323
2017-12-10T03:17:41.039759: step 8854, loss 0.0478979, acc 0.953125, prec 0.116497, recall 0.775323
2017-12-10T03:17:41.306730: step 8855, loss 1.17606, acc 0.9375, prec 0.116492, recall 0.775323
2017-12-10T03:17:41.586765: step 8856, loss 0.433623, acc 0.96875, prec 0.1165, recall 0.77534
2017-12-10T03:17:41.852564: step 8857, loss 0.285391, acc 0.96875, prec 0.116507, recall 0.775358
2017-12-10T03:17:42.115885: step 8858, loss 0.130755, acc 0.96875, prec 0.116504, recall 0.775358
2017-12-10T03:17:42.375914: step 8859, loss 0.197045, acc 0.96875, prec 0.116502, recall 0.775358
2017-12-10T03:17:42.643250: step 8860, loss 0.277193, acc 0.984375, prec 0.116511, recall 0.775375
2017-12-10T03:17:42.908702: step 8861, loss 0.112857, acc 0.96875, prec 0.116529, recall 0.77541
2017-12-10T03:17:43.171660: step 8862, loss 0.00131356, acc 1, prec 0.116539, recall 0.775428
2017-12-10T03:17:43.437647: step 8863, loss 0.35997, acc 0.984375, prec 0.116538, recall 0.775428
2017-12-10T03:17:43.703489: step 8864, loss 0.0737931, acc 0.953125, prec 0.116544, recall 0.775445
2017-12-10T03:17:43.971224: step 8865, loss 0.208793, acc 0.96875, prec 0.116551, recall 0.775463
2017-12-10T03:17:44.234854: step 8866, loss 0.723115, acc 0.921875, prec 0.116565, recall 0.775497
2017-12-10T03:17:44.498890: step 8867, loss 0.000120829, acc 1, prec 0.116576, recall 0.775515
2017-12-10T03:17:44.756681: step 8868, loss 0.00613198, acc 1, prec 0.116576, recall 0.775515
2017-12-10T03:17:45.021776: step 8869, loss 0.902449, acc 0.953125, prec 0.116571, recall 0.775515
2017-12-10T03:17:45.290863: step 8870, loss 0.100605, acc 0.984375, prec 0.11658, recall 0.775532
2017-12-10T03:17:45.557732: step 8871, loss 0.0196023, acc 1, prec 0.116591, recall 0.77555
2017-12-10T03:17:45.829951: step 8872, loss 0.044871, acc 0.984375, prec 0.116589, recall 0.77555
2017-12-10T03:17:46.100601: step 8873, loss 0.39574, acc 0.984375, prec 0.116588, recall 0.77555
2017-12-10T03:17:46.372623: step 8874, loss 0.103366, acc 0.984375, prec 0.116587, recall 0.77555
2017-12-10T03:17:46.645856: step 8875, loss 0.000289847, acc 1, prec 0.116587, recall 0.77555
2017-12-10T03:17:46.910688: step 8876, loss 0.0718736, acc 0.984375, prec 0.116596, recall 0.775567
2017-12-10T03:17:47.177154: step 8877, loss 0.0132288, acc 1, prec 0.116606, recall 0.775585
2017-12-10T03:17:47.443986: step 8878, loss 0.0426313, acc 0.96875, prec 0.116603, recall 0.775585
2017-12-10T03:17:47.707635: step 8879, loss 0.0302511, acc 0.984375, prec 0.116602, recall 0.775585
2017-12-10T03:17:47.976204: step 8880, loss 0.00725297, acc 1, prec 0.116602, recall 0.775585
2017-12-10T03:17:48.240669: step 8881, loss 4.8943e-06, acc 1, prec 0.116612, recall 0.775602
2017-12-10T03:17:48.500652: step 8882, loss 0.00102719, acc 1, prec 0.116622, recall 0.77562
2017-12-10T03:17:48.768940: step 8883, loss 0.480752, acc 0.921875, prec 0.116626, recall 0.775637
2017-12-10T03:17:49.033042: step 8884, loss 0.271978, acc 0.953125, prec 0.116643, recall 0.775672
2017-12-10T03:17:49.302881: step 8885, loss 0.129955, acc 0.984375, prec 0.116662, recall 0.775707
2017-12-10T03:17:49.567586: step 8886, loss 0.0164084, acc 0.984375, prec 0.116681, recall 0.775742
2017-12-10T03:17:49.833593: step 8887, loss 0.0619486, acc 0.984375, prec 0.11669, recall 0.775759
2017-12-10T03:17:50.096493: step 8888, loss 0.234181, acc 0.984375, prec 0.116699, recall 0.775776
2017-12-10T03:17:50.356671: step 8889, loss 0.199222, acc 0.984375, prec 0.116698, recall 0.775776
2017-12-10T03:17:50.621448: step 8890, loss 0.117192, acc 0.984375, prec 0.116696, recall 0.775776
2017-12-10T03:17:50.887401: step 8891, loss 0.404936, acc 1, prec 0.116707, recall 0.775794
2017-12-10T03:17:51.154087: step 8892, loss 3.61815, acc 0.953125, prec 0.116724, recall 0.775768
2017-12-10T03:17:51.422750: step 8893, loss 1.11761, acc 0.921875, prec 0.116718, recall 0.775768
2017-12-10T03:17:51.685791: step 8894, loss 0.00073462, acc 1, prec 0.116728, recall 0.775786
2017-12-10T03:17:51.954912: step 8895, loss 7.42491e-05, acc 1, prec 0.116738, recall 0.775803
2017-12-10T03:17:52.217381: step 8896, loss 0.380363, acc 0.953125, prec 0.116734, recall 0.775803
2017-12-10T03:17:52.482098: step 8897, loss 0.0995818, acc 0.96875, prec 0.116731, recall 0.775803
2017-12-10T03:17:52.744506: step 8898, loss 0.525559, acc 0.953125, prec 0.116738, recall 0.775821
2017-12-10T03:17:53.006625: step 8899, loss 0.0212186, acc 0.984375, prec 0.116736, recall 0.775821
2017-12-10T03:17:53.275309: step 8900, loss 0.0297115, acc 0.984375, prec 0.116756, recall 0.775855
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-8900

2017-12-10T03:17:54.566075: step 8901, loss 0.391318, acc 0.984375, prec 0.116765, recall 0.775873
2017-12-10T03:17:54.830807: step 8902, loss 0.380926, acc 0.953125, prec 0.116771, recall 0.77589
2017-12-10T03:17:55.095792: step 8903, loss 0.388258, acc 0.953125, prec 0.116767, recall 0.77589
2017-12-10T03:17:55.361464: step 8904, loss 0.601196, acc 0.90625, prec 0.116769, recall 0.775908
2017-12-10T03:17:55.623127: step 8905, loss 0.779751, acc 0.9375, prec 0.116784, recall 0.775942
2017-12-10T03:17:55.883283: step 8906, loss 0.540889, acc 0.9375, prec 0.116779, recall 0.775942
2017-12-10T03:17:56.155328: step 8907, loss 0.122147, acc 0.953125, prec 0.116785, recall 0.77596
2017-12-10T03:17:56.425055: step 8908, loss 0.26094, acc 0.96875, prec 0.116782, recall 0.77596
2017-12-10T03:17:56.700953: step 8909, loss 0.296198, acc 0.96875, prec 0.116779, recall 0.77596
2017-12-10T03:17:56.964176: step 8910, loss 0.753876, acc 0.9375, prec 0.116774, recall 0.77596
2017-12-10T03:17:57.229710: step 8911, loss 0.467034, acc 0.90625, prec 0.116786, recall 0.775994
2017-12-10T03:17:57.501011: step 8912, loss 0.569699, acc 0.9375, prec 0.116791, recall 0.776012
2017-12-10T03:17:57.766664: step 8913, loss 0.260432, acc 0.953125, prec 0.116787, recall 0.776012
2017-12-10T03:17:58.030735: step 8914, loss 0.799111, acc 0.875, prec 0.116786, recall 0.776029
2017-12-10T03:17:58.292858: step 8915, loss 0.220218, acc 0.96875, prec 0.116804, recall 0.776064
2017-12-10T03:17:58.561656: step 8916, loss 0.467668, acc 0.984375, prec 0.116834, recall 0.776116
2017-12-10T03:17:58.827926: step 8917, loss 0.127046, acc 0.96875, prec 0.116831, recall 0.776116
2017-12-10T03:17:59.093517: step 8918, loss 0.545529, acc 0.96875, prec 0.116839, recall 0.776133
2017-12-10T03:17:59.356682: step 8919, loss 0.61656, acc 0.90625, prec 0.116831, recall 0.776133
2017-12-10T03:17:59.621177: step 8920, loss 0.208713, acc 0.96875, prec 0.116838, recall 0.776151
2017-12-10T03:17:59.886741: step 8921, loss 0.360723, acc 0.953125, prec 0.116834, recall 0.776151
2017-12-10T03:18:00.165281: step 8922, loss 0.0561216, acc 0.984375, prec 0.116843, recall 0.776168
2017-12-10T03:18:00.442356: step 8923, loss 2.45745, acc 0.96875, prec 0.116852, recall 0.776125
2017-12-10T03:18:00.715898: step 8924, loss 1.82031, acc 0.953125, prec 0.116849, recall 0.776065
2017-12-10T03:18:00.988886: step 8925, loss 1.10773, acc 0.90625, prec 0.116851, recall 0.776082
2017-12-10T03:18:01.264406: step 8926, loss 0.340812, acc 0.953125, prec 0.116858, recall 0.7761
2017-12-10T03:18:01.528082: step 8927, loss 0.532495, acc 0.890625, prec 0.116848, recall 0.7761
2017-12-10T03:18:01.790816: step 8928, loss 0.122286, acc 0.96875, prec 0.116866, recall 0.776134
2017-12-10T03:18:02.061229: step 8929, loss 0.365765, acc 0.953125, prec 0.116862, recall 0.776134
2017-12-10T03:18:02.332547: step 8930, loss 0.756561, acc 0.84375, prec 0.116858, recall 0.776152
2017-12-10T03:18:02.596529: step 8931, loss 0.373699, acc 0.9375, prec 0.116863, recall 0.776169
2017-12-10T03:18:02.864446: step 8932, loss 1.44359, acc 0.828125, prec 0.116879, recall 0.776221
2017-12-10T03:18:03.128321: step 8933, loss 0.79737, acc 0.90625, prec 0.116881, recall 0.776238
2017-12-10T03:18:03.391859: step 8934, loss 1.28865, acc 0.859375, prec 0.116879, recall 0.776256
2017-12-10T03:18:03.653697: step 8935, loss 1.96339, acc 0.78125, prec 0.116881, recall 0.77629
2017-12-10T03:18:03.916077: step 8936, loss 1.24397, acc 0.90625, prec 0.116873, recall 0.77629
2017-12-10T03:18:04.181168: step 8937, loss 0.96564, acc 0.875, prec 0.116882, recall 0.776325
2017-12-10T03:18:04.448004: step 8938, loss 0.311708, acc 0.953125, prec 0.116878, recall 0.776325
2017-12-10T03:18:04.709636: step 8939, loss 1.57878, acc 0.90625, prec 0.11687, recall 0.776325
2017-12-10T03:18:04.975050: step 8940, loss 0.493832, acc 0.96875, prec 0.116878, recall 0.776342
2017-12-10T03:18:05.237015: step 8941, loss 1.0203, acc 0.9375, prec 0.116883, recall 0.77636
2017-12-10T03:18:05.509239: step 8942, loss 0.609471, acc 0.9375, prec 0.116877, recall 0.77636
2017-12-10T03:18:05.771770: step 8943, loss 0.224568, acc 0.96875, prec 0.116874, recall 0.77636
2017-12-10T03:18:06.035438: step 8944, loss 0.657754, acc 0.953125, prec 0.116881, recall 0.776377
2017-12-10T03:18:06.299715: step 8945, loss 0.265862, acc 0.9375, prec 0.116875, recall 0.776377
2017-12-10T03:18:06.531788: step 8946, loss 0.0921792, acc 0.941176, prec 0.116871, recall 0.776377
2017-12-10T03:18:06.809123: step 8947, loss 0.225365, acc 0.96875, prec 0.116868, recall 0.776377
2017-12-10T03:18:07.069889: step 8948, loss 0.0896321, acc 0.984375, prec 0.116867, recall 0.776377
2017-12-10T03:18:07.340443: step 8949, loss 0.181803, acc 0.96875, prec 0.116864, recall 0.776377
2017-12-10T03:18:07.600097: step 8950, loss 0.880379, acc 0.953125, prec 0.11686, recall 0.776377
2017-12-10T03:18:07.868500: step 8951, loss 0.404178, acc 0.953125, prec 0.116877, recall 0.776411
2017-12-10T03:18:08.132643: step 8952, loss 0.0972001, acc 0.984375, prec 0.116896, recall 0.776446
2017-12-10T03:18:08.404910: step 8953, loss 0.195557, acc 0.953125, prec 0.116892, recall 0.776446
2017-12-10T03:18:09.383827: step 8954, loss 7.52304e-05, acc 1, prec 0.116892, recall 0.776446
2017-12-10T03:18:09.749739: step 8955, loss 0.874773, acc 0.921875, prec 0.116895, recall 0.776463
2017-12-10T03:18:10.014446: step 8956, loss 0.537691, acc 0.984375, prec 0.116904, recall 0.776481
2017-12-10T03:18:10.731369: step 8957, loss 0.0233984, acc 0.984375, prec 0.116903, recall 0.776481
2017-12-10T03:18:11.779136: step 8958, loss 0.0157335, acc 0.984375, prec 0.116912, recall 0.776498
2017-12-10T03:18:12.129130: step 8959, loss 0.534545, acc 0.96875, prec 0.11693, recall 0.776532
2017-12-10T03:18:12.411142: step 8960, loss 0.0933577, acc 0.984375, prec 0.116949, recall 0.776567
2017-12-10T03:18:12.693816: step 8961, loss 0.00264288, acc 1, prec 0.116949, recall 0.776567
2017-12-10T03:18:12.978715: step 8962, loss 0.0957204, acc 0.984375, prec 0.116958, recall 0.776584
2017-12-10T03:18:13.265534: step 8963, loss 0.025589, acc 0.984375, prec 0.116956, recall 0.776584
2017-12-10T03:18:13.548104: step 8964, loss 0.00124095, acc 1, prec 0.116956, recall 0.776584
2017-12-10T03:18:13.817873: step 8965, loss 0.211234, acc 0.984375, prec 0.116975, recall 0.776619
2017-12-10T03:18:14.088417: step 8966, loss 3.96861e-05, acc 1, prec 0.116996, recall 0.776653
2017-12-10T03:18:14.359868: step 8967, loss 0.302212, acc 0.96875, prec 0.117004, recall 0.776671
2017-12-10T03:18:14.621879: step 8968, loss 0.556458, acc 0.96875, prec 0.117001, recall 0.776671
2017-12-10T03:18:14.887287: step 8969, loss 0.00588282, acc 1, prec 0.117011, recall 0.776688
2017-12-10T03:18:15.154714: step 8970, loss 1.2555e-05, acc 1, prec 0.117021, recall 0.776705
2017-12-10T03:18:15.415168: step 8971, loss 3.22393e-06, acc 1, prec 0.117021, recall 0.776705
2017-12-10T03:18:15.680698: step 8972, loss 0.3078, acc 0.984375, prec 0.117041, recall 0.77674
2017-12-10T03:18:15.943683: step 8973, loss 5.99762e-07, acc 1, prec 0.117041, recall 0.77674
2017-12-10T03:18:16.199456: step 8974, loss 0.000108271, acc 1, prec 0.117041, recall 0.77674
2017-12-10T03:18:16.464456: step 8975, loss 0.0684852, acc 0.984375, prec 0.11705, recall 0.776757
2017-12-10T03:18:16.728918: step 8976, loss 5.09762e-06, acc 1, prec 0.11706, recall 0.776774
2017-12-10T03:18:16.988660: step 8977, loss 0.52287, acc 0.96875, prec 0.117067, recall 0.776791
2017-12-10T03:18:17.251574: step 8978, loss 0.126273, acc 0.96875, prec 0.117065, recall 0.776791
2017-12-10T03:18:17.523151: step 8979, loss 0.111629, acc 0.984375, prec 0.117084, recall 0.776826
2017-12-10T03:18:17.790743: step 8980, loss 0.00978072, acc 1, prec 0.117084, recall 0.776826
2017-12-10T03:18:18.054274: step 8981, loss 5.35742, acc 0.984375, prec 0.117084, recall 0.776766
2017-12-10T03:18:18.317004: step 8982, loss 0.714876, acc 0.96875, prec 0.117102, recall 0.7768
2017-12-10T03:18:18.581996: step 8983, loss 0.322393, acc 0.953125, prec 0.117098, recall 0.7768
2017-12-10T03:18:18.842934: step 8984, loss 1.695, acc 0.96875, prec 0.117106, recall 0.776757
2017-12-10T03:18:19.115629: step 8985, loss 0.506152, acc 0.9375, prec 0.117101, recall 0.776757
2017-12-10T03:18:19.380037: step 8986, loss 0.296782, acc 0.953125, prec 0.117148, recall 0.776844
2017-12-10T03:18:19.657356: step 8987, loss 0.00879613, acc 1, prec 0.117159, recall 0.776861
2017-12-10T03:18:19.923514: step 8988, loss 0.320467, acc 0.9375, prec 0.117163, recall 0.776878
2017-12-10T03:18:20.187887: step 8989, loss 0.828113, acc 0.921875, prec 0.117187, recall 0.77693
2017-12-10T03:18:20.448559: step 8990, loss 0.701346, acc 0.921875, prec 0.117191, recall 0.776947
2017-12-10T03:18:20.721850: step 8991, loss 0.711818, acc 0.9375, prec 0.117196, recall 0.776964
2017-12-10T03:18:20.987418: step 8992, loss 0.169513, acc 0.953125, prec 0.117192, recall 0.776964
2017-12-10T03:18:21.255771: step 8993, loss 0.69118, acc 0.921875, prec 0.117205, recall 0.776998
2017-12-10T03:18:21.525289: step 8994, loss 0.629398, acc 0.9375, prec 0.1172, recall 0.776998
2017-12-10T03:18:21.792208: step 8995, loss 0.67379, acc 0.921875, prec 0.117213, recall 0.777033
2017-12-10T03:18:22.058376: step 8996, loss 0.355649, acc 0.921875, prec 0.117237, recall 0.777084
2017-12-10T03:18:22.324979: step 8997, loss 1.37404, acc 0.921875, prec 0.117251, recall 0.777119
2017-12-10T03:18:22.587287: step 8998, loss 1.20868, acc 0.859375, prec 0.117239, recall 0.777119
2017-12-10T03:18:22.849155: step 8999, loss 0.397856, acc 0.875, prec 0.117249, recall 0.777153
2017-12-10T03:18:23.116759: step 9000, loss 0.664146, acc 0.9375, prec 0.117274, recall 0.777204

Evaluation:
2017-12-10T03:18:30.795813: step 9000, loss 10.3111, acc 0.912539, prec 0.117011, recall 0.774127

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-9000

2017-12-10T03:18:32.053338: step 9001, loss 0.938655, acc 0.859375, prec 0.117019, recall 0.774162
2017-12-10T03:18:32.316507: step 9002, loss 0.0286634, acc 0.984375, prec 0.117018, recall 0.774162
2017-12-10T03:18:32.583883: step 9003, loss 0.124162, acc 0.953125, prec 0.117024, recall 0.774179
2017-12-10T03:18:32.848711: step 9004, loss 0.435995, acc 0.96875, prec 0.117021, recall 0.774179
2017-12-10T03:18:33.118558: step 9005, loss 0.104783, acc 0.953125, prec 0.117027, recall 0.774196
2017-12-10T03:18:33.394574: step 9006, loss 0.722191, acc 0.9375, prec 0.117022, recall 0.774196
2017-12-10T03:18:33.658031: step 9007, loss 0.270435, acc 0.96875, prec 0.117019, recall 0.774196
2017-12-10T03:18:33.925432: step 9008, loss 0.618295, acc 0.953125, prec 0.117035, recall 0.77423
2017-12-10T03:18:34.187307: step 9009, loss 0.8993, acc 0.953125, prec 0.117041, recall 0.774247
2017-12-10T03:18:34.452078: step 9010, loss 0.000885091, acc 1, prec 0.117052, recall 0.774265
2017-12-10T03:18:34.718083: step 9011, loss 0.0226975, acc 0.984375, prec 0.117071, recall 0.774299
2017-12-10T03:18:34.990981: step 9012, loss 0.180878, acc 0.96875, prec 0.117078, recall 0.774316
2017-12-10T03:18:35.970634: step 9013, loss 0.0100794, acc 1, prec 0.117078, recall 0.774316
2017-12-10T03:18:36.335771: step 9014, loss 0.336859, acc 0.96875, prec 0.117085, recall 0.774333
2017-12-10T03:18:36.706030: step 9015, loss 0.000146596, acc 1, prec 0.117085, recall 0.774333
2017-12-10T03:18:37.417433: step 9016, loss 0.0240746, acc 0.984375, prec 0.117084, recall 0.774333
2017-12-10T03:18:38.135608: step 9017, loss 0.029622, acc 0.984375, prec 0.117083, recall 0.774333
2017-12-10T03:18:38.908724: step 9018, loss 6.4359e-05, acc 1, prec 0.117083, recall 0.774333
2017-12-10T03:18:39.645744: step 9019, loss 0.0972499, acc 0.984375, prec 0.117081, recall 0.774333
2017-12-10T03:18:40.396315: step 9020, loss 0.000252342, acc 1, prec 0.117102, recall 0.774368
2017-12-10T03:18:41.135554: step 9021, loss 0.0264282, acc 0.984375, prec 0.117131, recall 0.774419
2017-12-10T03:18:41.896527: step 9022, loss 0.000245274, acc 1, prec 0.117131, recall 0.774419
2017-12-10T03:18:42.615241: step 9023, loss 0.00592861, acc 1, prec 0.117141, recall 0.774436
2017-12-10T03:18:43.370762: step 9024, loss 0.11944, acc 0.984375, prec 0.11715, recall 0.774453
2017-12-10T03:18:44.102193: step 9025, loss 3.49715, acc 0.984375, prec 0.11716, recall 0.774412
2017-12-10T03:18:44.841836: step 9026, loss 0.0458288, acc 0.984375, prec 0.117169, recall 0.774429
2017-12-10T03:18:45.598122: step 9027, loss 0.0449655, acc 0.984375, prec 0.117177, recall 0.774446
2017-12-10T03:18:46.350968: step 9028, loss 0.0100388, acc 1, prec 0.117177, recall 0.774446
2017-12-10T03:18:47.063230: step 9029, loss 0.0934219, acc 0.96875, prec 0.117195, recall 0.77448
2017-12-10T03:18:47.778722: step 9030, loss 1.15456, acc 0.953125, prec 0.117201, recall 0.774497
2017-12-10T03:18:48.525087: step 9031, loss 0.667621, acc 0.90625, prec 0.117203, recall 0.774514
2017-12-10T03:18:49.264063: step 9032, loss 0.158736, acc 0.953125, prec 0.117199, recall 0.774514
2017-12-10T03:18:49.972317: step 9033, loss 0.654571, acc 0.9375, prec 0.117204, recall 0.774531
2017-12-10T03:18:50.721963: step 9034, loss 1.13911, acc 0.90625, prec 0.117206, recall 0.774548
2017-12-10T03:18:51.437298: step 9035, loss 0.402946, acc 0.890625, prec 0.117227, recall 0.7746
2017-12-10T03:18:52.145942: step 9036, loss 0.634083, acc 0.9375, prec 0.117222, recall 0.7746
2017-12-10T03:18:52.837703: step 9037, loss 1.68691, acc 0.921875, prec 0.117235, recall 0.774634
2017-12-10T03:18:53.645347: step 9038, loss 0.274232, acc 0.96875, prec 0.117232, recall 0.774634
2017-12-10T03:18:54.140772: step 9039, loss 0.0281813, acc 0.984375, prec 0.117241, recall 0.774651
2017-12-10T03:18:54.417505: step 9040, loss 0.267162, acc 0.921875, prec 0.117255, recall 0.774685
2017-12-10T03:18:54.687272: step 9041, loss 0.504526, acc 0.921875, prec 0.117248, recall 0.774685
2017-12-10T03:18:54.952932: step 9042, loss 0.391404, acc 0.96875, prec 0.117266, recall 0.774719
2017-12-10T03:18:55.215779: step 9043, loss 1.10227, acc 0.90625, prec 0.117258, recall 0.774719
2017-12-10T03:18:55.481274: step 9044, loss 0.419381, acc 0.9375, prec 0.117252, recall 0.774719
2017-12-10T03:18:55.744855: step 9045, loss 0.08182, acc 0.96875, prec 0.117249, recall 0.774719
2017-12-10T03:18:56.013222: step 9046, loss 0.0866415, acc 0.96875, prec 0.117247, recall 0.774719
2017-12-10T03:18:56.283213: step 9047, loss 0.363538, acc 0.984375, prec 0.117256, recall 0.774737
2017-12-10T03:18:56.558625: step 9048, loss 0.367775, acc 0.96875, prec 0.117253, recall 0.774737
2017-12-10T03:18:56.828055: step 9049, loss 0.126375, acc 0.96875, prec 0.11725, recall 0.774737
2017-12-10T03:18:57.092319: step 9050, loss 0.00830502, acc 1, prec 0.11726, recall 0.774754
2017-12-10T03:18:57.357835: step 9051, loss 0.168917, acc 0.96875, prec 0.117258, recall 0.774754
2017-12-10T03:18:58.338990: step 9052, loss 0.243168, acc 0.953125, prec 0.117264, recall 0.774771
2017-12-10T03:18:58.706389: step 9053, loss 0.655507, acc 0.96875, prec 0.117281, recall 0.774805
2017-12-10T03:18:58.968877: step 9054, loss 0.0135694, acc 1, prec 0.117302, recall 0.774839
2017-12-10T03:18:59.732364: step 9055, loss 0.101639, acc 0.984375, prec 0.1173, recall 0.774839
2017-12-10T03:19:00.491135: step 9056, loss 0.0803843, acc 0.96875, prec 0.117297, recall 0.774839
2017-12-10T03:19:01.236481: step 9057, loss 1.54902, acc 0.96875, prec 0.117316, recall 0.774814
2017-12-10T03:19:01.984404: step 9058, loss 0.198019, acc 0.96875, prec 0.117334, recall 0.774848
2017-12-10T03:19:02.715713: step 9059, loss 0.00211725, acc 1, prec 0.117344, recall 0.774866
2017-12-10T03:19:04.110413: step 9060, loss 0.1022, acc 0.984375, prec 0.117363, recall 0.7749
2017-12-10T03:19:04.480498: step 9061, loss 0.0738375, acc 0.984375, prec 0.117372, recall 0.774917
2017-12-10T03:19:04.771379: step 9062, loss 0.102159, acc 0.984375, prec 0.117381, recall 0.774934
2017-12-10T03:19:05.053048: step 9063, loss 1.75176, acc 0.9375, prec 0.117375, recall 0.774934
2017-12-10T03:19:05.313895: step 9064, loss 1.28805, acc 0.921875, prec 0.117389, recall 0.774968
2017-12-10T03:19:05.576381: step 9065, loss 0.0323475, acc 0.984375, prec 0.117387, recall 0.774968
2017-12-10T03:19:05.849532: step 9066, loss 0.0158105, acc 0.984375, prec 0.117386, recall 0.774968
2017-12-10T03:19:06.117458: step 9067, loss 0.267295, acc 0.96875, prec 0.117383, recall 0.774968
2017-12-10T03:19:06.385924: step 9068, loss 0.600533, acc 0.96875, prec 0.117391, recall 0.774985
2017-12-10T03:19:06.655351: step 9069, loss 1.02706, acc 0.96875, prec 0.117418, recall 0.775036
2017-12-10T03:19:06.931300: step 9070, loss 0.341727, acc 0.96875, prec 0.117416, recall 0.775036
2017-12-10T03:19:07.194434: step 9071, loss 0.00626132, acc 1, prec 0.117426, recall 0.775053
2017-12-10T03:19:07.454873: step 9072, loss 0.186255, acc 0.96875, prec 0.117423, recall 0.775053
2017-12-10T03:19:07.723782: step 9073, loss 0.69167, acc 0.953125, prec 0.11746, recall 0.775121
2017-12-10T03:19:07.983366: step 9074, loss 0.634326, acc 0.953125, prec 0.117466, recall 0.775138
2017-12-10T03:19:08.251895: step 9075, loss 0.21966, acc 0.984375, prec 0.117474, recall 0.775155
2017-12-10T03:19:08.522229: step 9076, loss 0.188947, acc 0.984375, prec 0.117473, recall 0.775155
2017-12-10T03:19:08.795201: step 9077, loss 0.000374509, acc 1, prec 0.117473, recall 0.775155
2017-12-10T03:19:09.061415: step 9078, loss 0.000528224, acc 1, prec 0.117473, recall 0.775155
2017-12-10T03:19:09.321946: step 9079, loss 1.39962, acc 0.953125, prec 0.117469, recall 0.775155
2017-12-10T03:19:09.592795: step 9080, loss 0.0172971, acc 0.984375, prec 0.117468, recall 0.775155
2017-12-10T03:19:09.857571: step 9081, loss 0.168266, acc 0.953125, prec 0.117464, recall 0.775155
2017-12-10T03:19:10.124918: step 9082, loss 0.157885, acc 0.96875, prec 0.117461, recall 0.775155
2017-12-10T03:19:10.387019: step 9083, loss 0.123044, acc 0.984375, prec 0.11749, recall 0.775206
2017-12-10T03:19:10.650374: step 9084, loss 0.274824, acc 0.96875, prec 0.117497, recall 0.775223
2017-12-10T03:19:10.913314: step 9085, loss 0.0219593, acc 0.984375, prec 0.117526, recall 0.775274
2017-12-10T03:19:11.185318: step 9086, loss 0.0227678, acc 0.984375, prec 0.117535, recall 0.775291
2017-12-10T03:19:11.453082: step 9087, loss 0.0953295, acc 0.96875, prec 0.117543, recall 0.775308
2017-12-10T03:19:11.736054: step 9088, loss 0.199387, acc 0.984375, prec 0.117551, recall 0.775325
2017-12-10T03:19:12.011449: step 9089, loss 0.348365, acc 0.984375, prec 0.11756, recall 0.775342
2017-12-10T03:19:12.271815: step 9090, loss 0.0706003, acc 0.984375, prec 0.117569, recall 0.775359
2017-12-10T03:19:12.534940: step 9091, loss 1.87588, acc 0.984375, prec 0.117589, recall 0.775334
2017-12-10T03:19:12.800015: step 9092, loss 0.56045, acc 0.984375, prec 0.117588, recall 0.775334
2017-12-10T03:19:13.071535: step 9093, loss 0.352379, acc 0.9375, prec 0.117582, recall 0.775334
2017-12-10T03:19:13.333894: step 9094, loss 0.413062, acc 0.953125, prec 0.117588, recall 0.775351
2017-12-10T03:19:13.604413: step 9095, loss 1.13757, acc 0.953125, prec 0.117594, recall 0.775368
2017-12-10T03:19:13.878860: step 9096, loss 1.38571, acc 0.890625, prec 0.117605, recall 0.775402
2017-12-10T03:19:14.140829: step 9097, loss 0.689099, acc 0.953125, prec 0.117601, recall 0.775402
2017-12-10T03:19:14.412733: step 9098, loss 0.000558034, acc 1, prec 0.117621, recall 0.775436
2017-12-10T03:19:14.674997: step 9099, loss 0.733529, acc 0.9375, prec 0.117636, recall 0.77547
2017-12-10T03:19:14.936678: step 9100, loss 0.18844, acc 0.96875, prec 0.117644, recall 0.775487
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-9100

2017-12-10T03:19:16.296912: step 9101, loss 0.39064, acc 0.9375, prec 0.117648, recall 0.775504
2017-12-10T03:19:16.566312: step 9102, loss 0.0253101, acc 0.984375, prec 0.117657, recall 0.775521
2017-12-10T03:19:16.827804: step 9103, loss 0.0374604, acc 0.984375, prec 0.117676, recall 0.775555
2017-12-10T03:19:17.103555: step 9104, loss 0.533565, acc 0.9375, prec 0.117671, recall 0.775555
2017-12-10T03:19:17.375152: step 9105, loss 0.47663, acc 0.953125, prec 0.117697, recall 0.775606
2017-12-10T03:19:17.639066: step 9106, loss 0.00861533, acc 1, prec 0.117697, recall 0.775606
2017-12-10T03:19:17.904499: step 9107, loss 0.0119158, acc 0.984375, prec 0.117696, recall 0.775606
2017-12-10T03:19:18.166991: step 9108, loss 0.182489, acc 0.96875, prec 0.117703, recall 0.775623
2017-12-10T03:19:18.433823: step 9109, loss 0.419011, acc 0.984375, prec 0.117702, recall 0.775623
2017-12-10T03:19:18.694664: step 9110, loss 0.372041, acc 0.984375, prec 0.11772, recall 0.775657
2017-12-10T03:19:18.965967: step 9111, loss 0.0351842, acc 0.984375, prec 0.117749, recall 0.775707
2017-12-10T03:19:19.230789: step 9112, loss 0.211366, acc 0.96875, prec 0.117757, recall 0.775724
2017-12-10T03:19:19.497591: step 9113, loss 0.771967, acc 0.9375, prec 0.117751, recall 0.775724
2017-12-10T03:19:19.761628: step 9114, loss 0.0791506, acc 0.984375, prec 0.11777, recall 0.775758
2017-12-10T03:19:20.027520: step 9115, loss 0.532268, acc 0.96875, prec 0.117788, recall 0.775792
2017-12-10T03:19:20.295698: step 9116, loss 0.917084, acc 0.921875, prec 0.117781, recall 0.775792
2017-12-10T03:19:20.566455: step 9117, loss 0.105488, acc 0.953125, prec 0.117777, recall 0.775792
2017-12-10T03:19:20.828796: step 9118, loss 0.723645, acc 0.953125, prec 0.117773, recall 0.775792
2017-12-10T03:19:21.096144: step 9119, loss 0.900175, acc 0.90625, prec 0.117765, recall 0.775792
2017-12-10T03:19:21.355434: step 9120, loss 0.0446163, acc 0.984375, prec 0.117784, recall 0.775826
2017-12-10T03:19:21.619598: step 9121, loss 0.0347622, acc 0.984375, prec 0.117782, recall 0.775826
2017-12-10T03:19:21.881278: step 9122, loss 0.00792541, acc 1, prec 0.117782, recall 0.775826
2017-12-10T03:19:22.151382: step 9123, loss 0.417048, acc 0.984375, prec 0.117781, recall 0.775826
2017-12-10T03:19:22.421419: step 9124, loss 0.0145375, acc 0.984375, prec 0.11779, recall 0.775843
2017-12-10T03:19:22.684899: step 9125, loss 0.141354, acc 0.984375, prec 0.117799, recall 0.775859
2017-12-10T03:19:22.947644: step 9126, loss 0.0865515, acc 0.984375, prec 0.117797, recall 0.775859
2017-12-10T03:19:23.210547: step 9127, loss 0.0108423, acc 1, prec 0.117797, recall 0.775859
2017-12-10T03:19:23.469995: step 9128, loss 0.209249, acc 0.9375, prec 0.117792, recall 0.775859
2017-12-10T03:19:23.735551: step 9129, loss 0.398219, acc 0.953125, prec 0.117808, recall 0.775893
2017-12-10T03:19:24.001771: step 9130, loss 0.136027, acc 0.953125, prec 0.117804, recall 0.775893
2017-12-10T03:19:24.265337: step 9131, loss 0.248777, acc 0.96875, prec 0.117811, recall 0.77591
2017-12-10T03:19:24.522536: step 9132, loss 0.613338, acc 0.984375, prec 0.11782, recall 0.775927
2017-12-10T03:19:24.785206: step 9133, loss 0.000783232, acc 1, prec 0.11782, recall 0.775927
2017-12-10T03:19:25.046282: step 9134, loss 0.416153, acc 0.9375, prec 0.117825, recall 0.775944
2017-12-10T03:19:25.306707: step 9135, loss 0.408987, acc 0.96875, prec 0.117842, recall 0.775978
2017-12-10T03:19:25.579813: step 9136, loss 0.0131396, acc 1, prec 0.117842, recall 0.775978
2017-12-10T03:19:25.848914: step 9137, loss 0.00130328, acc 1, prec 0.117852, recall 0.775995
2017-12-10T03:19:26.114093: step 9138, loss 0.000124124, acc 1, prec 0.117852, recall 0.775995
2017-12-10T03:19:26.373387: step 9139, loss 0.000330157, acc 1, prec 0.117873, recall 0.776028
2017-12-10T03:19:26.645677: step 9140, loss 2.27242e-07, acc 1, prec 0.117873, recall 0.776028
2017-12-10T03:19:26.908333: step 9141, loss 0.0355724, acc 0.96875, prec 0.11788, recall 0.776045
2017-12-10T03:19:27.169511: step 9142, loss 0.283036, acc 0.984375, prec 0.117879, recall 0.776045
2017-12-10T03:19:27.432998: step 9143, loss 0.00900791, acc 1, prec 0.117889, recall 0.776062
2017-12-10T03:19:27.698791: step 9144, loss 0.490222, acc 0.984375, prec 0.117897, recall 0.776079
2017-12-10T03:19:27.966440: step 9145, loss 0.00108168, acc 1, prec 0.117897, recall 0.776079
2017-12-10T03:19:28.225228: step 9146, loss 24.2728, acc 0.984375, prec 0.117928, recall 0.776071
2017-12-10T03:19:28.494967: step 9147, loss 0.000226005, acc 1, prec 0.117928, recall 0.776071
2017-12-10T03:19:28.754474: step 9148, loss 0.215671, acc 0.96875, prec 0.117925, recall 0.776071
2017-12-10T03:19:29.021533: step 9149, loss 0.364887, acc 0.984375, prec 0.117924, recall 0.776071
2017-12-10T03:19:29.289921: step 9150, loss 3.95242, acc 0.953125, prec 0.117951, recall 0.776063
2017-12-10T03:19:29.558290: step 9151, loss 0.206162, acc 0.953125, prec 0.117957, recall 0.77608
2017-12-10T03:19:29.822021: step 9152, loss 0.283794, acc 0.96875, prec 0.117965, recall 0.776097
2017-12-10T03:19:30.092908: step 9153, loss 0.3815, acc 0.953125, prec 0.117961, recall 0.776097
2017-12-10T03:19:30.357744: step 9154, loss 0.10036, acc 0.984375, prec 0.11799, recall 0.776147
2017-12-10T03:19:30.625396: step 9155, loss 0.0093444, acc 1, prec 0.11799, recall 0.776147
2017-12-10T03:19:30.890928: step 9156, loss 1.57912, acc 0.859375, prec 0.117998, recall 0.776181
2017-12-10T03:19:31.155871: step 9157, loss 1.98798, acc 0.90625, prec 0.117989, recall 0.776181
2017-12-10T03:19:31.419784: step 9158, loss 0.335589, acc 0.953125, prec 0.117996, recall 0.776198
2017-12-10T03:19:31.684750: step 9159, loss 1.14599, acc 0.875, prec 0.117995, recall 0.776215
2017-12-10T03:19:31.948048: step 9160, loss 0.617113, acc 0.90625, prec 0.117987, recall 0.776215
2017-12-10T03:19:32.220025: step 9161, loss 0.931606, acc 0.84375, prec 0.117993, recall 0.776249
2017-12-10T03:19:32.490453: step 9162, loss 1.2091, acc 0.84375, prec 0.11799, recall 0.776265
2017-12-10T03:19:32.754550: step 9163, loss 0.174824, acc 0.953125, prec 0.118006, recall 0.776299
2017-12-10T03:19:33.024301: step 9164, loss 0.897568, acc 0.875, prec 0.117995, recall 0.776299
2017-12-10T03:19:33.291224: step 9165, loss 0.808313, acc 0.890625, prec 0.117996, recall 0.776316
2017-12-10T03:19:33.558903: step 9166, loss 0.697241, acc 0.921875, prec 0.117999, recall 0.776333
2017-12-10T03:19:33.826721: step 9167, loss 0.379259, acc 0.890625, prec 0.118, recall 0.776349
2017-12-10T03:19:34.087793: step 9168, loss 0.329228, acc 0.9375, prec 0.118005, recall 0.776366
2017-12-10T03:19:34.359892: step 9169, loss 0.209517, acc 0.96875, prec 0.118022, recall 0.7764
2017-12-10T03:19:34.633502: step 9170, loss 0.429124, acc 0.953125, prec 0.118028, recall 0.776417
2017-12-10T03:19:34.892803: step 9171, loss 0.238023, acc 0.9375, prec 0.118053, recall 0.776467
2017-12-10T03:19:35.159111: step 9172, loss 0.758364, acc 0.9375, prec 0.118048, recall 0.776467
2017-12-10T03:19:35.426400: step 9173, loss 0.389648, acc 0.9375, prec 0.118062, recall 0.776501
2017-12-10T03:19:35.686131: step 9174, loss 0.157922, acc 0.953125, prec 0.118078, recall 0.776534
2017-12-10T03:19:35.947875: step 9175, loss 0.681219, acc 0.9375, prec 0.118103, recall 0.776585
2017-12-10T03:19:36.211742: step 9176, loss 0.393448, acc 0.953125, prec 0.118099, recall 0.776585
2017-12-10T03:19:36.481802: step 9177, loss 0.465904, acc 0.953125, prec 0.118115, recall 0.776618
2017-12-10T03:19:36.758253: step 9178, loss 0.150595, acc 0.984375, prec 0.118114, recall 0.776618
2017-12-10T03:19:37.023973: step 9179, loss 0.33789, acc 0.984375, prec 0.118123, recall 0.776635
2017-12-10T03:19:37.295962: step 9180, loss 0.378541, acc 0.96875, prec 0.11812, recall 0.776635
2017-12-10T03:19:37.561419: step 9181, loss 0.59091, acc 0.96875, prec 0.118117, recall 0.776635
2017-12-10T03:19:37.823681: step 9182, loss 0.963373, acc 0.9375, prec 0.118122, recall 0.776652
2017-12-10T03:19:38.087127: step 9183, loss 0.0498796, acc 0.984375, prec 0.118121, recall 0.776652
2017-12-10T03:19:38.357543: step 9184, loss 0.104775, acc 0.953125, prec 0.118137, recall 0.776685
2017-12-10T03:19:38.622635: step 9185, loss 0.551465, acc 0.953125, prec 0.118153, recall 0.776719
2017-12-10T03:19:38.893712: step 9186, loss 0.750417, acc 0.953125, prec 0.118159, recall 0.776735
2017-12-10T03:19:39.164186: step 9187, loss 0.401187, acc 0.96875, prec 0.118166, recall 0.776752
2017-12-10T03:19:39.426758: step 9188, loss 0.664765, acc 0.953125, prec 0.118172, recall 0.776769
2017-12-10T03:19:39.692045: step 9189, loss 0.376981, acc 0.96875, prec 0.118169, recall 0.776769
2017-12-10T03:19:39.957040: step 9190, loss 0.713122, acc 0.96875, prec 0.118167, recall 0.776769
2017-12-10T03:19:40.221796: step 9191, loss 0.0558438, acc 0.96875, prec 0.118174, recall 0.776786
2017-12-10T03:19:40.483637: step 9192, loss 0.709256, acc 0.953125, prec 0.11818, recall 0.776802
2017-12-10T03:19:40.751534: step 9193, loss 0.00113105, acc 1, prec 0.11819, recall 0.776819
2017-12-10T03:19:41.010149: step 9194, loss 0.0200231, acc 0.984375, prec 0.118209, recall 0.776853
2017-12-10T03:19:41.278904: step 9195, loss 0.360706, acc 0.984375, prec 0.118218, recall 0.776869
2017-12-10T03:19:41.547766: step 9196, loss 0.111591, acc 0.96875, prec 0.118215, recall 0.776869
2017-12-10T03:19:41.828600: step 9197, loss 0.275588, acc 0.96875, prec 0.118232, recall 0.776903
2017-12-10T03:19:42.091105: step 9198, loss 3.05276e-06, acc 1, prec 0.118232, recall 0.776903
2017-12-10T03:19:42.350226: step 9199, loss 0.323469, acc 0.96875, prec 0.11824, recall 0.77692
2017-12-10T03:19:42.616212: step 9200, loss 2.92162e-05, acc 1, prec 0.11826, recall 0.776953
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-9200

2017-12-10T03:19:44.018733: step 9201, loss 0.0709545, acc 0.984375, prec 0.118269, recall 0.77697
2017-12-10T03:19:44.286110: step 9202, loss 0.156987, acc 0.96875, prec 0.118266, recall 0.77697
2017-12-10T03:19:44.555632: step 9203, loss 0.004344, acc 1, prec 0.118266, recall 0.77697
2017-12-10T03:19:44.821096: step 9204, loss 0.588867, acc 0.953125, prec 0.118272, recall 0.776986
2017-12-10T03:19:45.092740: step 9205, loss 0.000551024, acc 1, prec 0.118282, recall 0.777003
2017-12-10T03:19:45.350846: step 9206, loss 0.000102362, acc 1, prec 0.118282, recall 0.777003
2017-12-10T03:19:45.613025: step 9207, loss 0.00464252, acc 1, prec 0.118282, recall 0.777003
2017-12-10T03:19:45.875886: step 9208, loss 0.449793, acc 0.984375, prec 0.118301, recall 0.777037
2017-12-10T03:19:46.151552: step 9209, loss 4.85195e-06, acc 1, prec 0.118301, recall 0.777037
2017-12-10T03:19:46.406769: step 9210, loss 0.310477, acc 0.96875, prec 0.118298, recall 0.777037
2017-12-10T03:19:46.672877: step 9211, loss 0.0258795, acc 0.984375, prec 0.118297, recall 0.777037
2017-12-10T03:19:46.938494: step 9212, loss 0.001687, acc 1, prec 0.118297, recall 0.777037
2017-12-10T03:19:47.201111: step 9213, loss 0.0717386, acc 0.984375, prec 0.118305, recall 0.777053
2017-12-10T03:19:47.473142: step 9214, loss 0.00979716, acc 1, prec 0.118305, recall 0.777053
2017-12-10T03:19:47.737709: step 9215, loss 0.69074, acc 0.96875, prec 0.118303, recall 0.777053
2017-12-10T03:19:48.003467: step 9216, loss 0.412752, acc 0.96875, prec 0.1183, recall 0.777053
2017-12-10T03:19:48.285766: step 9217, loss 0.0953414, acc 0.96875, prec 0.118307, recall 0.77707
2017-12-10T03:19:48.551519: step 9218, loss 0.202075, acc 0.96875, prec 0.118315, recall 0.777087
2017-12-10T03:19:48.812759: step 9219, loss 1.49024, acc 0.984375, prec 0.118315, recall 0.777029
2017-12-10T03:19:49.072286: step 9220, loss 0.163055, acc 0.96875, prec 0.118312, recall 0.777029
2017-12-10T03:19:49.333412: step 9221, loss 0.178623, acc 0.984375, prec 0.118311, recall 0.777029
2017-12-10T03:19:49.600443: step 9222, loss 0.00909039, acc 1, prec 0.118321, recall 0.777045
2017-12-10T03:19:49.867962: step 9223, loss 0.167123, acc 0.953125, prec 0.118327, recall 0.777062
2017-12-10T03:19:50.132989: step 9224, loss 0.551918, acc 0.921875, prec 0.11832, recall 0.777062
2017-12-10T03:19:50.396884: step 9225, loss 0.417657, acc 0.953125, prec 0.118336, recall 0.777095
2017-12-10T03:19:50.659151: step 9226, loss 0.058186, acc 0.984375, prec 0.118345, recall 0.777112
2017-12-10T03:19:50.919394: step 9227, loss 0.0888406, acc 0.984375, prec 0.118354, recall 0.777129
2017-12-10T03:19:51.185300: step 9228, loss 0.0435938, acc 0.96875, prec 0.118381, recall 0.777179
2017-12-10T03:19:51.445214: step 9229, loss 0.0637194, acc 0.984375, prec 0.11839, recall 0.777195
2017-12-10T03:19:51.710965: step 9230, loss 0.372791, acc 0.9375, prec 0.118394, recall 0.777212
2017-12-10T03:19:51.974825: step 9231, loss 0.607421, acc 0.96875, prec 0.118392, recall 0.777212
2017-12-10T03:19:52.246675: step 9232, loss 0.148131, acc 0.984375, prec 0.11841, recall 0.777246
2017-12-10T03:19:52.514604: step 9233, loss 0.0140532, acc 0.984375, prec 0.118409, recall 0.777246
2017-12-10T03:19:52.784865: step 9234, loss 0.162327, acc 0.953125, prec 0.118405, recall 0.777246
2017-12-10T03:19:53.051169: step 9235, loss 0.309129, acc 0.96875, prec 0.118412, recall 0.777262
2017-12-10T03:19:53.315173: step 9236, loss 0.406757, acc 0.953125, prec 0.118438, recall 0.777312
2017-12-10T03:19:53.592261: step 9237, loss 0.0028283, acc 1, prec 0.118459, recall 0.777345
2017-12-10T03:19:53.858243: step 9238, loss 0.0153677, acc 0.984375, prec 0.118457, recall 0.777345
2017-12-10T03:19:54.134138: step 9239, loss 0.141974, acc 0.984375, prec 0.118456, recall 0.777345
2017-12-10T03:19:54.399111: step 9240, loss 0.278459, acc 0.953125, prec 0.118472, recall 0.777379
2017-12-10T03:19:54.664039: step 9241, loss 0.182374, acc 0.96875, prec 0.118479, recall 0.777395
2017-12-10T03:19:54.928896: step 9242, loss 0.00386862, acc 1, prec 0.118489, recall 0.777412
2017-12-10T03:19:55.191003: step 9243, loss 0.328058, acc 0.96875, prec 0.118487, recall 0.777412
2017-12-10T03:19:55.459300: step 9244, loss 0.359824, acc 0.96875, prec 0.118484, recall 0.777412
2017-12-10T03:19:55.730072: step 9245, loss 0.70223, acc 0.96875, prec 0.118491, recall 0.777429
2017-12-10T03:19:55.997582: step 9246, loss 0.0164364, acc 0.984375, prec 0.11851, recall 0.777462
2017-12-10T03:19:56.262288: step 9247, loss 4.27818, acc 0.984375, prec 0.11852, recall 0.777421
2017-12-10T03:19:56.530400: step 9248, loss 0.037674, acc 0.984375, prec 0.118529, recall 0.777437
2017-12-10T03:19:56.809185: step 9249, loss 0.712631, acc 0.953125, prec 0.118535, recall 0.777454
2017-12-10T03:19:57.072224: step 9250, loss 0.000126101, acc 1, prec 0.118545, recall 0.77747
2017-12-10T03:19:57.329859: step 9251, loss 0.110509, acc 0.96875, prec 0.118542, recall 0.77747
2017-12-10T03:19:57.599053: step 9252, loss 0.585209, acc 0.90625, prec 0.118544, recall 0.777487
2017-12-10T03:19:57.860079: step 9253, loss 0.764525, acc 0.890625, prec 0.118535, recall 0.777487
2017-12-10T03:19:58.130071: step 9254, loss 1.53605, acc 0.953125, prec 0.118571, recall 0.777554
2017-12-10T03:19:58.396894: step 9255, loss 0.793279, acc 0.9375, prec 0.118565, recall 0.777554
2017-12-10T03:19:58.663668: step 9256, loss 1.70858, acc 0.875, prec 0.118554, recall 0.777554
2017-12-10T03:19:58.932053: step 9257, loss 0.187857, acc 0.9375, prec 0.118559, recall 0.77757
2017-12-10T03:19:59.204999: step 9258, loss 0.532928, acc 0.953125, prec 0.118585, recall 0.77762
2017-12-10T03:19:59.471636: step 9259, loss 0.516605, acc 0.953125, prec 0.118601, recall 0.777653
2017-12-10T03:19:59.742573: step 9260, loss 6.46545, acc 0.90625, prec 0.118604, recall 0.777612
2017-12-10T03:20:00.013905: step 9261, loss 0.606706, acc 0.9375, prec 0.118609, recall 0.777628
2017-12-10T03:20:00.286613: step 9262, loss 1.22343, acc 0.828125, prec 0.118594, recall 0.777628
2017-12-10T03:20:00.561026: step 9263, loss 0.191728, acc 0.953125, prec 0.11859, recall 0.777628
2017-12-10T03:20:00.822514: step 9264, loss 1.34932, acc 0.90625, prec 0.118582, recall 0.777628
2017-12-10T03:20:01.089327: step 9265, loss 1.90844, acc 0.8125, prec 0.118566, recall 0.777628
2017-12-10T03:20:01.353380: step 9266, loss 0.644739, acc 0.953125, prec 0.118572, recall 0.777645
2017-12-10T03:20:01.621436: step 9267, loss 0.387505, acc 0.90625, prec 0.118564, recall 0.777645
2017-12-10T03:20:01.886070: step 9268, loss 0.701061, acc 0.953125, prec 0.11856, recall 0.777645
2017-12-10T03:20:02.152389: step 9269, loss 1.33154, acc 0.859375, prec 0.118548, recall 0.777645
2017-12-10T03:20:02.415474: step 9270, loss 1.15057, acc 0.828125, prec 0.118543, recall 0.777662
2017-12-10T03:20:02.681241: step 9271, loss 0.38805, acc 0.921875, prec 0.118536, recall 0.777662
2017-12-10T03:20:02.956154: step 9272, loss 1.57843, acc 0.875, prec 0.118525, recall 0.777662
2017-12-10T03:20:03.223297: step 9273, loss 0.973782, acc 0.890625, prec 0.118516, recall 0.777662
2017-12-10T03:20:03.488641: step 9274, loss 0.578995, acc 0.921875, prec 0.118509, recall 0.777662
2017-12-10T03:20:03.754992: step 9275, loss 1.39236, acc 0.875, prec 0.118498, recall 0.777662
2017-12-10T03:20:04.016168: step 9276, loss 1.02039, acc 0.921875, prec 0.118492, recall 0.777662
2017-12-10T03:20:04.281388: step 9277, loss 0.363839, acc 0.921875, prec 0.118495, recall 0.777678
2017-12-10T03:20:04.543377: step 9278, loss 0.231819, acc 0.9375, prec 0.118499, recall 0.777695
2017-12-10T03:20:04.807924: step 9279, loss 1.65093, acc 0.90625, prec 0.118501, recall 0.777711
2017-12-10T03:20:05.077327: step 9280, loss 0.238681, acc 0.953125, prec 0.118507, recall 0.777728
2017-12-10T03:20:05.346720: step 9281, loss 0.479817, acc 0.9375, prec 0.118512, recall 0.777745
2017-12-10T03:20:05.620635: step 9282, loss 0.939226, acc 0.953125, prec 0.118508, recall 0.777745
2017-12-10T03:20:05.885381: step 9283, loss 0.473305, acc 0.96875, prec 0.118525, recall 0.777778
2017-12-10T03:20:06.155456: step 9284, loss 0.513396, acc 0.921875, prec 0.118529, recall 0.777794
2017-12-10T03:20:06.417800: step 9285, loss 0.0245627, acc 0.984375, prec 0.118547, recall 0.777828
2017-12-10T03:20:06.686152: step 9286, loss 0.430829, acc 0.921875, prec 0.118551, recall 0.777844
2017-12-10T03:20:06.950225: step 9287, loss 0.106427, acc 0.96875, prec 0.118548, recall 0.777844
2017-12-10T03:20:07.221986: step 9288, loss 0.364755, acc 0.984375, prec 0.118547, recall 0.777844
2017-12-10T03:20:07.490269: step 9289, loss 0.000724355, acc 1, prec 0.118567, recall 0.777877
2017-12-10T03:20:07.752785: step 9290, loss 0.173492, acc 0.984375, prec 0.118565, recall 0.777877
2017-12-10T03:20:08.014118: step 9291, loss 0.147507, acc 0.984375, prec 0.118574, recall 0.777894
2017-12-10T03:20:08.276552: step 9292, loss 0.000428307, acc 1, prec 0.118574, recall 0.777894
2017-12-10T03:20:08.537061: step 9293, loss 0.0295749, acc 0.984375, prec 0.118573, recall 0.777894
2017-12-10T03:20:08.801608: step 9294, loss 7.64126e-06, acc 1, prec 0.118583, recall 0.77791
2017-12-10T03:20:09.062651: step 9295, loss 0.134632, acc 0.984375, prec 0.118581, recall 0.77791
2017-12-10T03:20:09.330511: step 9296, loss 0.357418, acc 0.984375, prec 0.11859, recall 0.777927
2017-12-10T03:20:09.590525: step 9297, loss 0.395335, acc 0.96875, prec 0.118597, recall 0.777943
2017-12-10T03:20:09.849932: step 9298, loss 0.000237421, acc 1, prec 0.118607, recall 0.77796
2017-12-10T03:20:10.109091: step 9299, loss 4.60206e-05, acc 1, prec 0.118607, recall 0.77796
2017-12-10T03:20:10.372815: step 9300, loss 0.0924205, acc 0.984375, prec 0.118606, recall 0.77796

Evaluation:
2017-12-10T03:20:17.966232: step 9300, loss 22.7537, acc 0.969808, prec 0.118785, recall 0.771877

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-9300

2017-12-10T03:20:19.231189: step 9301, loss 0.215894, acc 0.984375, prec 0.118794, recall 0.771894
2017-12-10T03:20:19.496970: step 9302, loss 0.644574, acc 0.96875, prec 0.118811, recall 0.771927
2017-12-10T03:20:19.759950: step 9303, loss 0.652742, acc 0.953125, prec 0.118817, recall 0.771944
2017-12-10T03:20:20.026881: step 9304, loss 0.11123, acc 1, prec 0.118857, recall 0.772011
2017-12-10T03:20:20.304570: step 9305, loss 1.58969, acc 0.984375, prec 0.118857, recall 0.771954
2017-12-10T03:20:20.572856: step 9306, loss 5.03072e-06, acc 1, prec 0.118867, recall 0.771971
2017-12-10T03:20:20.831052: step 9307, loss 0.00452345, acc 1, prec 0.118867, recall 0.771971
2017-12-10T03:20:21.096208: step 9308, loss 0.47226, acc 0.984375, prec 0.118886, recall 0.772005
2017-12-10T03:20:21.362558: step 9309, loss 0.288079, acc 0.953125, prec 0.118892, recall 0.772021
2017-12-10T03:20:21.632805: step 9310, loss 0.0226373, acc 0.984375, prec 0.1189, recall 0.772038
2017-12-10T03:20:21.897337: step 9311, loss 0.205266, acc 0.984375, prec 0.118909, recall 0.772055
2017-12-10T03:20:22.163513: step 9312, loss 0.000258563, acc 1, prec 0.118919, recall 0.772072
2017-12-10T03:20:22.426676: step 9313, loss 0.528579, acc 0.9375, prec 0.118933, recall 0.772105
2017-12-10T03:20:22.695644: step 9314, loss 0.0105787, acc 1, prec 0.118933, recall 0.772105
2017-12-10T03:20:22.968782: step 9315, loss 0.00669725, acc 1, prec 0.118943, recall 0.772122
2017-12-10T03:20:23.239979: step 9316, loss 0.549296, acc 0.96875, prec 0.118951, recall 0.772139
2017-12-10T03:20:23.507390: step 9317, loss 0.301, acc 0.96875, prec 0.118978, recall 0.772189
2017-12-10T03:20:23.772416: step 9318, loss 10.1992, acc 0.96875, prec 0.118987, recall 0.772149
2017-12-10T03:20:24.047165: step 9319, loss 0.0617188, acc 0.96875, prec 0.119014, recall 0.772199
2017-12-10T03:20:24.311573: step 9320, loss 0.138613, acc 0.953125, prec 0.11901, recall 0.772199
2017-12-10T03:20:24.582982: step 9321, loss 0.00294185, acc 1, prec 0.11901, recall 0.772199
2017-12-10T03:20:24.849363: step 9322, loss 0.281952, acc 0.921875, prec 0.119013, recall 0.772216
2017-12-10T03:20:25.115807: step 9323, loss 0.531715, acc 0.953125, prec 0.119029, recall 0.77225
2017-12-10T03:20:25.389754: step 9324, loss 0.496165, acc 0.9375, prec 0.119024, recall 0.77225
2017-12-10T03:20:25.654240: step 9325, loss 0.385055, acc 0.953125, prec 0.11904, recall 0.772283
2017-12-10T03:20:25.923852: step 9326, loss 0.527882, acc 0.9375, prec 0.119044, recall 0.7723
2017-12-10T03:20:26.196389: step 9327, loss 0.127063, acc 0.96875, prec 0.119041, recall 0.7723
2017-12-10T03:20:26.459907: step 9328, loss 0.602724, acc 0.9375, prec 0.119036, recall 0.7723
2017-12-10T03:20:26.740069: step 9329, loss 1.47163, acc 0.953125, prec 0.119042, recall 0.772317
2017-12-10T03:20:27.001257: step 9330, loss 0.089964, acc 0.96875, prec 0.119039, recall 0.772317
2017-12-10T03:20:27.271825: step 9331, loss 1.11261, acc 0.90625, prec 0.119031, recall 0.772317
2017-12-10T03:20:27.540064: step 9332, loss 1.03112, acc 0.90625, prec 0.119033, recall 0.772333
2017-12-10T03:20:27.805406: step 9333, loss 0.490494, acc 0.9375, prec 0.119038, recall 0.77235
2017-12-10T03:20:28.070744: step 9334, loss 0.467874, acc 0.921875, prec 0.119051, recall 0.772383
2017-12-10T03:20:28.331493: step 9335, loss 0.210196, acc 0.984375, prec 0.119089, recall 0.77245
2017-12-10T03:20:28.596719: step 9336, loss 0.242182, acc 0.953125, prec 0.119095, recall 0.772467
2017-12-10T03:20:28.870450: step 9337, loss 1.28041, acc 0.921875, prec 0.119109, recall 0.7725
2017-12-10T03:20:29.130727: step 9338, loss 0.969796, acc 0.890625, prec 0.119099, recall 0.7725
2017-12-10T03:20:29.400098: step 9339, loss 0.751951, acc 0.9375, prec 0.119114, recall 0.772534
2017-12-10T03:20:29.673948: step 9340, loss 0.186154, acc 0.96875, prec 0.119121, recall 0.77255
2017-12-10T03:20:29.942626: step 9341, loss 0.298912, acc 0.96875, prec 0.119128, recall 0.772567
2017-12-10T03:20:30.211908: step 9342, loss 1.33055, acc 0.921875, prec 0.119131, recall 0.772584
2017-12-10T03:20:30.483267: step 9343, loss 0.157941, acc 0.953125, prec 0.119127, recall 0.772584
2017-12-10T03:20:30.752898: step 9344, loss 0.208784, acc 0.953125, prec 0.119123, recall 0.772584
2017-12-10T03:20:31.017749: step 9345, loss 0.173203, acc 0.96875, prec 0.119131, recall 0.772601
2017-12-10T03:20:31.282279: step 9346, loss 0.0227154, acc 0.984375, prec 0.119129, recall 0.772601
2017-12-10T03:20:31.542391: step 9347, loss 0.00867351, acc 1, prec 0.119129, recall 0.772601
2017-12-10T03:20:31.805364: step 9348, loss 0.583174, acc 0.96875, prec 0.119156, recall 0.772651
2017-12-10T03:20:32.070667: step 9349, loss 0.623558, acc 0.984375, prec 0.119165, recall 0.772667
2017-12-10T03:20:32.339001: step 9350, loss 0.18435, acc 0.96875, prec 0.119172, recall 0.772684
2017-12-10T03:20:32.603239: step 9351, loss 0.580617, acc 0.96875, prec 0.11918, recall 0.772701
2017-12-10T03:20:32.874004: step 9352, loss 1.01718, acc 0.9375, prec 0.119184, recall 0.772717
2017-12-10T03:20:33.137241: step 9353, loss 0.0354201, acc 0.984375, prec 0.119193, recall 0.772734
2017-12-10T03:20:33.405246: step 9354, loss 0.306762, acc 0.921875, prec 0.119196, recall 0.772751
2017-12-10T03:20:33.670709: step 9355, loss 0.113528, acc 0.96875, prec 0.119193, recall 0.772751
2017-12-10T03:20:33.932501: step 9356, loss 0.445283, acc 0.96875, prec 0.119191, recall 0.772751
2017-12-10T03:20:34.193890: step 9357, loss 0.0866352, acc 0.984375, prec 0.119199, recall 0.772767
2017-12-10T03:20:34.457966: step 9358, loss 0.0598926, acc 0.984375, prec 0.119218, recall 0.772801
2017-12-10T03:20:34.720852: step 9359, loss 0.634129, acc 0.984375, prec 0.119216, recall 0.772801
2017-12-10T03:20:34.985879: step 9360, loss 0.145888, acc 0.984375, prec 0.119215, recall 0.772801
2017-12-10T03:20:35.249666: step 9361, loss 1.7324, acc 0.96875, prec 0.119242, recall 0.772851
2017-12-10T03:20:35.518674: step 9362, loss 0.00637425, acc 1, prec 0.119242, recall 0.772851
2017-12-10T03:20:35.791927: step 9363, loss 0.26803, acc 0.96875, prec 0.11925, recall 0.772867
2017-12-10T03:20:36.056204: step 9364, loss 0.680987, acc 0.96875, prec 0.119247, recall 0.772867
2017-12-10T03:20:36.317451: step 9365, loss 0.295679, acc 0.984375, prec 0.119275, recall 0.772917
2017-12-10T03:20:36.576782: step 9366, loss 1.15014, acc 0.953125, prec 0.119281, recall 0.772934
2017-12-10T03:20:36.841723: step 9367, loss 0.00184449, acc 1, prec 0.119291, recall 0.77295
2017-12-10T03:20:37.105062: step 9368, loss 0.692091, acc 0.9375, prec 0.119296, recall 0.772967
2017-12-10T03:20:37.367918: step 9369, loss 0.000894015, acc 1, prec 0.119296, recall 0.772967
2017-12-10T03:20:37.623842: step 9370, loss 1.27724, acc 0.953125, prec 0.119312, recall 0.773
2017-12-10T03:20:37.891801: step 9371, loss 0.541943, acc 0.96875, prec 0.119329, recall 0.773034
2017-12-10T03:20:38.151789: step 9372, loss 0.00149729, acc 1, prec 0.119329, recall 0.773034
2017-12-10T03:20:38.413333: step 9373, loss 0.290547, acc 0.984375, prec 0.119328, recall 0.773034
2017-12-10T03:20:38.676371: step 9374, loss 0.568407, acc 0.9375, prec 0.119322, recall 0.773034
2017-12-10T03:20:38.935041: step 9375, loss 0.426902, acc 0.9375, prec 0.119317, recall 0.773034
2017-12-10T03:20:39.204922: step 9376, loss 0.284263, acc 0.9375, prec 0.119341, recall 0.773083
2017-12-10T03:20:39.470234: step 9377, loss 0.178507, acc 0.984375, prec 0.11935, recall 0.7731
2017-12-10T03:20:39.731215: step 9378, loss 0.0991694, acc 0.96875, prec 0.119347, recall 0.7731
2017-12-10T03:20:39.994274: step 9379, loss 0.657683, acc 0.953125, prec 0.119373, recall 0.77315
2017-12-10T03:20:40.255283: step 9380, loss 0.0104259, acc 1, prec 0.119373, recall 0.77315
2017-12-10T03:20:40.524491: step 9381, loss 0.547101, acc 0.9375, prec 0.119368, recall 0.77315
2017-12-10T03:20:40.794755: step 9382, loss 3.00453, acc 0.96875, prec 0.119376, recall 0.77311
2017-12-10T03:20:41.063157: step 9383, loss 0.0647109, acc 0.984375, prec 0.119385, recall 0.773126
2017-12-10T03:20:41.331432: step 9384, loss 0.0829991, acc 0.984375, prec 0.119383, recall 0.773126
2017-12-10T03:20:41.598862: step 9385, loss 1.66036, acc 0.890625, prec 0.119384, recall 0.773143
2017-12-10T03:20:41.864810: step 9386, loss 0.251964, acc 0.96875, prec 0.119391, recall 0.77316
2017-12-10T03:20:42.139619: step 9387, loss 0.401083, acc 0.921875, prec 0.119384, recall 0.77316
2017-12-10T03:20:42.403805: step 9388, loss 0.0234002, acc 0.984375, prec 0.119383, recall 0.77316
2017-12-10T03:20:42.672399: step 9389, loss 2.96517, acc 0.90625, prec 0.119406, recall 0.773153
2017-12-10T03:20:42.937535: step 9390, loss 1.07307, acc 0.828125, prec 0.119431, recall 0.773219
2017-12-10T03:20:43.202196: step 9391, loss 0.595348, acc 0.921875, prec 0.119424, recall 0.773219
2017-12-10T03:20:43.464360: step 9392, loss 1.93371, acc 0.84375, prec 0.119421, recall 0.773236
2017-12-10T03:20:43.726540: step 9393, loss 1.00113, acc 0.859375, prec 0.119409, recall 0.773236
2017-12-10T03:20:43.989933: step 9394, loss 0.39035, acc 0.890625, prec 0.119399, recall 0.773236
2017-12-10T03:20:44.257366: step 9395, loss 0.271406, acc 0.9375, prec 0.119404, recall 0.773252
2017-12-10T03:20:44.520249: step 9396, loss 0.586268, acc 0.875, prec 0.119403, recall 0.773269
2017-12-10T03:20:44.788936: step 9397, loss 2.09744, acc 0.734375, prec 0.11938, recall 0.773269
2017-12-10T03:20:45.055930: step 9398, loss 1.37953, acc 0.890625, prec 0.119381, recall 0.773286
2017-12-10T03:20:45.320061: step 9399, loss 0.841089, acc 0.890625, prec 0.119381, recall 0.773302
2017-12-10T03:20:45.586816: step 9400, loss 0.803679, acc 0.875, prec 0.11939, recall 0.773335
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-9400

2017-12-10T03:20:46.861252: step 9401, loss 1.01766, acc 0.875, prec 0.119379, recall 0.773335
2017-12-10T03:20:47.125709: step 9402, loss 0.230827, acc 0.953125, prec 0.119395, recall 0.773368
2017-12-10T03:20:47.394284: step 9403, loss 0.552318, acc 0.921875, prec 0.119389, recall 0.773368
2017-12-10T03:20:47.661193: step 9404, loss 0.962959, acc 0.875, prec 0.119388, recall 0.773385
2017-12-10T03:20:47.930246: step 9405, loss 0.218226, acc 0.90625, prec 0.11938, recall 0.773385
2017-12-10T03:20:48.191158: step 9406, loss 0.142941, acc 0.953125, prec 0.119376, recall 0.773385
2017-12-10T03:20:48.453198: step 9407, loss 0.123569, acc 0.9375, prec 0.11938, recall 0.773402
2017-12-10T03:20:48.722850: step 9408, loss 0.408151, acc 0.96875, prec 0.119387, recall 0.773418
2017-12-10T03:20:48.986243: step 9409, loss 0.0370653, acc 0.984375, prec 0.119386, recall 0.773418
2017-12-10T03:20:49.253594: step 9410, loss 0.334111, acc 0.953125, prec 0.119392, recall 0.773435
2017-12-10T03:20:49.519242: step 9411, loss 0.78334, acc 0.90625, prec 0.119394, recall 0.773451
2017-12-10T03:20:49.785206: step 9412, loss 0.589401, acc 0.953125, prec 0.1194, recall 0.773468
2017-12-10T03:20:50.050298: step 9413, loss 0.105084, acc 0.96875, prec 0.119407, recall 0.773484
2017-12-10T03:20:50.317390: step 9414, loss 0.103082, acc 0.984375, prec 0.119415, recall 0.773501
2017-12-10T03:20:50.578664: step 9415, loss 0.547391, acc 0.9375, prec 0.11943, recall 0.773534
2017-12-10T03:20:50.840149: step 9416, loss 0.000211159, acc 1, prec 0.11944, recall 0.77355
2017-12-10T03:20:51.108850: step 9417, loss 0.742091, acc 0.921875, prec 0.119433, recall 0.77355
2017-12-10T03:20:51.368954: step 9418, loss 0.157534, acc 0.984375, prec 0.119432, recall 0.77355
2017-12-10T03:20:51.632120: step 9419, loss 0.0807902, acc 0.984375, prec 0.11944, recall 0.773567
2017-12-10T03:20:51.893091: step 9420, loss 0.718552, acc 0.953125, prec 0.119436, recall 0.773567
2017-12-10T03:20:52.166672: step 9421, loss 0.667988, acc 0.953125, prec 0.119432, recall 0.773567
2017-12-10T03:20:52.431790: step 9422, loss 0.000359507, acc 1, prec 0.119432, recall 0.773567
2017-12-10T03:20:52.688686: step 9423, loss 0.133888, acc 0.96875, prec 0.11943, recall 0.773567
2017-12-10T03:20:52.957931: step 9424, loss 0.578815, acc 0.96875, prec 0.119437, recall 0.773584
2017-12-10T03:20:53.218586: step 9425, loss 0.0291614, acc 0.984375, prec 0.119435, recall 0.773584
2017-12-10T03:20:53.481540: step 9426, loss 0.000839932, acc 1, prec 0.119435, recall 0.773584
2017-12-10T03:20:53.744181: step 9427, loss 1.46274, acc 0.953125, prec 0.119441, recall 0.7736
2017-12-10T03:20:54.015664: step 9428, loss 0.000475192, acc 1, prec 0.119441, recall 0.7736
2017-12-10T03:20:54.275688: step 9429, loss 0.00377666, acc 1, prec 0.119441, recall 0.7736
2017-12-10T03:20:54.544261: step 9430, loss 0.0605118, acc 0.984375, prec 0.11945, recall 0.773617
2017-12-10T03:20:54.808321: step 9431, loss 0.00159986, acc 1, prec 0.11945, recall 0.773617
2017-12-10T03:20:55.067516: step 9432, loss 0.00513154, acc 1, prec 0.11945, recall 0.773617
2017-12-10T03:20:55.328981: step 9433, loss 1.15877, acc 0.984375, prec 0.11946, recall 0.773577
2017-12-10T03:20:55.593107: step 9434, loss 0.00115127, acc 1, prec 0.11949, recall 0.773626
2017-12-10T03:20:55.859144: step 9435, loss 0.00279319, acc 1, prec 0.11949, recall 0.773626
2017-12-10T03:20:56.121612: step 9436, loss 0.000365579, acc 1, prec 0.1195, recall 0.773643
2017-12-10T03:20:56.382210: step 9437, loss 0.187245, acc 0.984375, prec 0.119498, recall 0.773643
2017-12-10T03:20:56.665857: step 9438, loss 0.593968, acc 0.9375, prec 0.119493, recall 0.773643
2017-12-10T03:20:56.931870: step 9439, loss 0.00739933, acc 1, prec 0.119513, recall 0.773676
2017-12-10T03:20:57.195555: step 9440, loss 0.684702, acc 0.96875, prec 0.11953, recall 0.773709
2017-12-10T03:20:57.456502: step 9441, loss 0.0120598, acc 0.984375, prec 0.119538, recall 0.773725
2017-12-10T03:20:57.721917: step 9442, loss 0.143376, acc 0.96875, prec 0.119546, recall 0.773742
2017-12-10T03:20:57.952622: step 9443, loss 0.000991663, acc 1, prec 0.119546, recall 0.773742
2017-12-10T03:20:58.216943: step 9444, loss 0.112368, acc 0.984375, prec 0.119544, recall 0.773742
2017-12-10T03:20:58.485657: step 9445, loss 2.26186, acc 0.953125, prec 0.119552, recall 0.773702
2017-12-10T03:20:58.755158: step 9446, loss 0.0351481, acc 0.984375, prec 0.11955, recall 0.773702
2017-12-10T03:20:59.018112: step 9447, loss 0.0528892, acc 0.984375, prec 0.119549, recall 0.773702
2017-12-10T03:20:59.284036: step 9448, loss 0.269481, acc 0.96875, prec 0.119546, recall 0.773702
2017-12-10T03:20:59.548141: step 9449, loss 0.0742931, acc 0.96875, prec 0.119543, recall 0.773702
2017-12-10T03:20:59.817190: step 9450, loss 0.145079, acc 0.953125, prec 0.119539, recall 0.773702
2017-12-10T03:21:00.091770: step 9451, loss 0.589948, acc 0.96875, prec 0.119537, recall 0.773702
2017-12-10T03:21:00.365825: step 9452, loss 0.0266788, acc 0.984375, prec 0.119535, recall 0.773702
2017-12-10T03:21:00.628767: step 9453, loss 0.109599, acc 0.984375, prec 0.119544, recall 0.773718
2017-12-10T03:21:00.895876: step 9454, loss 0.479133, acc 0.953125, prec 0.11955, recall 0.773735
2017-12-10T03:21:01.161583: step 9455, loss 0.0321149, acc 0.984375, prec 0.119568, recall 0.773768
2017-12-10T03:21:01.429213: step 9456, loss 0.36191, acc 0.96875, prec 0.119566, recall 0.773768
2017-12-10T03:21:01.696040: step 9457, loss 0.213008, acc 0.9375, prec 0.11956, recall 0.773768
2017-12-10T03:21:01.958653: step 9458, loss 0.106776, acc 0.984375, prec 0.119569, recall 0.773784
2017-12-10T03:21:02.228291: step 9459, loss 0.294391, acc 0.953125, prec 0.119575, recall 0.773801
2017-12-10T03:21:02.492807: step 9460, loss 0.286103, acc 0.96875, prec 0.119582, recall 0.773817
2017-12-10T03:21:02.753426: step 9461, loss 0.520383, acc 0.953125, prec 0.119578, recall 0.773817
2017-12-10T03:21:03.017366: step 9462, loss 6.0322, acc 0.9375, prec 0.119594, recall 0.773794
2017-12-10T03:21:03.285579: step 9463, loss 0.0611992, acc 0.984375, prec 0.119602, recall 0.77381
2017-12-10T03:21:03.546554: step 9464, loss 0.179769, acc 0.96875, prec 0.119609, recall 0.773827
2017-12-10T03:21:03.810616: step 9465, loss 0.0587535, acc 0.96875, prec 0.119607, recall 0.773827
2017-12-10T03:21:04.069704: step 9466, loss 0.33375, acc 0.921875, prec 0.1196, recall 0.773827
2017-12-10T03:21:04.339055: step 9467, loss 0.888899, acc 0.9375, prec 0.119595, recall 0.773827
2017-12-10T03:21:04.612005: step 9468, loss 1.11478, acc 0.890625, prec 0.119595, recall 0.773843
2017-12-10T03:21:04.889557: step 9469, loss 0.394876, acc 0.921875, prec 0.119598, recall 0.77386
2017-12-10T03:21:05.154565: step 9470, loss 0.491587, acc 0.9375, prec 0.119603, recall 0.773876
2017-12-10T03:21:05.428627: step 9471, loss 0.308426, acc 0.96875, prec 0.11961, recall 0.773893
2017-12-10T03:21:05.692903: step 9472, loss 0.484475, acc 0.90625, prec 0.119622, recall 0.773926
2017-12-10T03:21:05.954094: step 9473, loss 0.238713, acc 0.90625, prec 0.119614, recall 0.773926
2017-12-10T03:21:06.217454: step 9474, loss 0.331471, acc 0.9375, prec 0.119608, recall 0.773926
2017-12-10T03:21:06.482847: step 9475, loss 0.908189, acc 0.890625, prec 0.119599, recall 0.773926
2017-12-10T03:21:06.751284: step 9476, loss 0.702202, acc 0.953125, prec 0.119615, recall 0.773959
2017-12-10T03:21:07.012032: step 9477, loss 0.567156, acc 0.9375, prec 0.119629, recall 0.773992
2017-12-10T03:21:07.272208: step 9478, loss 0.611178, acc 0.921875, prec 0.119622, recall 0.773992
2017-12-10T03:21:07.534893: step 9479, loss 0.454002, acc 0.921875, prec 0.119616, recall 0.773992
2017-12-10T03:21:07.804211: step 9480, loss 0.650311, acc 0.921875, prec 0.119609, recall 0.773992
2017-12-10T03:21:08.072372: step 9481, loss 0.885362, acc 0.9375, prec 0.119613, recall 0.774008
2017-12-10T03:21:08.338593: step 9482, loss 1.31001, acc 0.9375, prec 0.119608, recall 0.774008
2017-12-10T03:21:08.610456: step 9483, loss 0.367669, acc 0.9375, prec 0.119613, recall 0.774024
2017-12-10T03:21:08.872946: step 9484, loss 0.0661455, acc 0.96875, prec 0.11963, recall 0.774057
2017-12-10T03:21:09.150416: step 9485, loss 0.746705, acc 0.984375, prec 0.119638, recall 0.774074
2017-12-10T03:21:09.416446: step 9486, loss 0.324144, acc 0.9375, prec 0.119653, recall 0.774107
2017-12-10T03:21:09.681577: step 9487, loss 0.494939, acc 0.96875, prec 0.11967, recall 0.77414
2017-12-10T03:21:09.949446: step 9488, loss 0.00102172, acc 1, prec 0.11969, recall 0.774172
2017-12-10T03:21:10.214748: step 9489, loss 0.0987519, acc 0.96875, prec 0.119707, recall 0.774205
2017-12-10T03:21:10.483202: step 9490, loss 0.496261, acc 0.984375, prec 0.119725, recall 0.774238
2017-12-10T03:21:10.746290: step 9491, loss 0.143218, acc 0.984375, prec 0.119734, recall 0.774255
2017-12-10T03:21:11.008927: step 9492, loss 0.0316158, acc 0.984375, prec 0.119752, recall 0.774287
2017-12-10T03:21:11.269775: step 9493, loss 0.580958, acc 0.953125, prec 0.119748, recall 0.774287
2017-12-10T03:21:11.539236: step 9494, loss 0.370062, acc 0.921875, prec 0.119771, recall 0.774337
2017-12-10T03:21:11.815242: step 9495, loss 0.559489, acc 0.984375, prec 0.119799, recall 0.774386
2017-12-10T03:21:12.083331: step 9496, loss 2.02301e-05, acc 1, prec 0.119819, recall 0.774419
2017-12-10T03:21:12.348289: step 9497, loss 0.771985, acc 0.953125, prec 0.119825, recall 0.774435
2017-12-10T03:21:12.611784: step 9498, loss 0.0291575, acc 0.984375, prec 0.119834, recall 0.774451
2017-12-10T03:21:12.882937: step 9499, loss 0.526112, acc 0.953125, prec 0.119839, recall 0.774468
2017-12-10T03:21:13.152227: step 9500, loss 0.681698, acc 0.953125, prec 0.119845, recall 0.774484
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-9500

2017-12-10T03:21:14.544768: step 9501, loss 0.000270523, acc 1, prec 0.119845, recall 0.774484
2017-12-10T03:21:14.807249: step 9502, loss 0.052657, acc 0.984375, prec 0.119844, recall 0.774484
2017-12-10T03:21:15.062644: step 9503, loss 0.0543847, acc 0.984375, prec 0.119843, recall 0.774484
2017-12-10T03:21:15.332729: step 9504, loss 0.0467842, acc 0.984375, prec 0.119851, recall 0.774501
2017-12-10T03:21:15.596833: step 9505, loss 0.421506, acc 0.96875, prec 0.119858, recall 0.774517
2017-12-10T03:21:15.863332: step 9506, loss 0.471959, acc 0.984375, prec 0.119877, recall 0.77455
2017-12-10T03:21:16.123232: step 9507, loss 0.574241, acc 0.9375, prec 0.119871, recall 0.77455
2017-12-10T03:21:16.386650: step 9508, loss 0.112714, acc 0.953125, prec 0.119877, recall 0.774566
2017-12-10T03:21:16.651649: step 9509, loss 0.393654, acc 0.96875, prec 0.119894, recall 0.774599
2017-12-10T03:21:16.917446: step 9510, loss 0.313663, acc 0.984375, prec 0.119893, recall 0.774599
2017-12-10T03:21:17.183500: step 9511, loss 0.0208639, acc 0.984375, prec 0.119902, recall 0.774615
2017-12-10T03:21:17.448058: step 9512, loss 0.474352, acc 0.96875, prec 0.119899, recall 0.774615
2017-12-10T03:21:17.705620: step 9513, loss 5.73426e-05, acc 1, prec 0.119909, recall 0.774632
2017-12-10T03:21:17.972370: step 9514, loss 0.153302, acc 0.984375, prec 0.119907, recall 0.774632
2017-12-10T03:21:18.237731: step 9515, loss 0.188054, acc 0.984375, prec 0.119926, recall 0.774664
2017-12-10T03:21:18.503407: step 9516, loss 0.0155003, acc 0.984375, prec 0.119944, recall 0.774697
2017-12-10T03:21:18.774365: step 9517, loss 0.219202, acc 0.984375, prec 0.119953, recall 0.774713
2017-12-10T03:21:19.035150: step 9518, loss 0.740762, acc 0.96875, prec 0.11996, recall 0.77473
2017-12-10T03:21:19.300533: step 9519, loss 0.00253795, acc 1, prec 0.11998, recall 0.774762
2017-12-10T03:21:19.566587: step 9520, loss 0.000648703, acc 1, prec 0.11998, recall 0.774762
2017-12-10T03:21:19.826083: step 9521, loss 4.71706, acc 0.96875, prec 0.119998, recall 0.774739
2017-12-10T03:21:20.088319: step 9522, loss 0.44246, acc 0.953125, prec 0.119994, recall 0.774739
2017-12-10T03:21:20.354881: step 9523, loss 0.638089, acc 0.984375, prec 0.120003, recall 0.774755
2017-12-10T03:21:20.619317: step 9524, loss 0.0025994, acc 1, prec 0.120013, recall 0.774772
2017-12-10T03:21:20.878098: step 9525, loss 0.280035, acc 0.96875, prec 0.12001, recall 0.774772
2017-12-10T03:21:21.140497: step 9526, loss 0.329498, acc 0.96875, prec 0.120017, recall 0.774788
2017-12-10T03:21:21.405422: step 9527, loss 0.104263, acc 0.984375, prec 0.120026, recall 0.774804
2017-12-10T03:21:21.675682: step 9528, loss 0.032769, acc 0.984375, prec 0.120034, recall 0.774821
2017-12-10T03:21:21.939938: step 9529, loss 0.0796003, acc 0.984375, prec 0.120053, recall 0.774853
2017-12-10T03:21:22.211981: step 9530, loss 0.73772, acc 0.984375, prec 0.120061, recall 0.77487
2017-12-10T03:21:22.476657: step 9531, loss 0.131995, acc 0.984375, prec 0.12007, recall 0.774886
2017-12-10T03:21:22.737916: step 9532, loss 0.00186965, acc 1, prec 0.12008, recall 0.774902
2017-12-10T03:21:23.007592: step 9533, loss 0.322034, acc 0.953125, prec 0.120095, recall 0.774935
2017-12-10T03:21:23.271284: step 9534, loss 1.77586, acc 0.90625, prec 0.120097, recall 0.774951
2017-12-10T03:21:23.543363: step 9535, loss 0.353564, acc 0.953125, prec 0.120093, recall 0.774951
2017-12-10T03:21:23.807407: step 9536, loss 0.365298, acc 0.953125, prec 0.120089, recall 0.774951
2017-12-10T03:21:24.074273: step 9537, loss 0.758851, acc 0.953125, prec 0.120105, recall 0.774984
2017-12-10T03:21:24.338874: step 9538, loss 0.342752, acc 0.953125, prec 0.120101, recall 0.774984
2017-12-10T03:21:24.605986: step 9539, loss 0.651072, acc 0.96875, prec 0.120098, recall 0.774984
2017-12-10T03:21:24.869496: step 9540, loss 0.098199, acc 0.984375, prec 0.120097, recall 0.774984
2017-12-10T03:21:25.139171: step 9541, loss 0.101619, acc 0.96875, prec 0.120114, recall 0.775016
2017-12-10T03:21:25.404929: step 9542, loss 0.2056, acc 0.953125, prec 0.120139, recall 0.775065
2017-12-10T03:21:25.669342: step 9543, loss 0.00178861, acc 1, prec 0.120139, recall 0.775065
2017-12-10T03:21:25.933225: step 9544, loss 0.101519, acc 0.984375, prec 0.120148, recall 0.775082
2017-12-10T03:21:26.198080: step 9545, loss 0.476414, acc 0.96875, prec 0.120145, recall 0.775082
2017-12-10T03:21:26.459409: step 9546, loss 0.113128, acc 0.984375, prec 0.120173, recall 0.77513
2017-12-10T03:21:26.728835: step 9547, loss 0.317437, acc 0.96875, prec 0.12019, recall 0.775163
2017-12-10T03:21:26.986734: step 9548, loss 16.0492, acc 0.921875, prec 0.120185, recall 0.775107
2017-12-10T03:21:27.251652: step 9549, loss 0.128741, acc 0.984375, prec 0.120194, recall 0.775123
2017-12-10T03:21:27.509710: step 9550, loss 0.237405, acc 0.953125, prec 0.12019, recall 0.775123
2017-12-10T03:21:27.769805: step 9551, loss 0.343245, acc 0.9375, prec 0.120184, recall 0.775123
2017-12-10T03:21:28.037695: step 9552, loss 0.872821, acc 0.9375, prec 0.120198, recall 0.775156
2017-12-10T03:21:28.298388: step 9553, loss 0.532518, acc 0.984375, prec 0.120197, recall 0.775156
2017-12-10T03:21:28.562306: step 9554, loss 0.781115, acc 0.90625, prec 0.120199, recall 0.775172
2017-12-10T03:21:28.819826: step 9555, loss 1.12484, acc 0.875, prec 0.120188, recall 0.775172
2017-12-10T03:21:29.080005: step 9556, loss 0.338347, acc 0.921875, prec 0.120181, recall 0.775172
2017-12-10T03:21:29.343307: step 9557, loss 0.888196, acc 0.890625, prec 0.120182, recall 0.775188
2017-12-10T03:21:29.608796: step 9558, loss 0.86474, acc 0.90625, prec 0.120184, recall 0.775204
2017-12-10T03:21:29.871004: step 9559, loss 1.7029, acc 0.921875, prec 0.120187, recall 0.775221
2017-12-10T03:21:30.137688: step 9560, loss 1.29311, acc 0.859375, prec 0.120184, recall 0.775237
2017-12-10T03:21:30.410222: step 9561, loss 0.439167, acc 0.9375, prec 0.120189, recall 0.775253
2017-12-10T03:21:30.675435: step 9562, loss 1.02932, acc 0.890625, prec 0.120199, recall 0.775286
2017-12-10T03:21:30.936441: step 9563, loss 0.605942, acc 0.953125, prec 0.120205, recall 0.775302
2017-12-10T03:21:31.198422: step 9564, loss 0.247293, acc 0.9375, prec 0.1202, recall 0.775302
2017-12-10T03:21:31.472378: step 9565, loss 0.383819, acc 0.96875, prec 0.120197, recall 0.775302
2017-12-10T03:21:31.742953: step 9566, loss 0.0952434, acc 0.953125, prec 0.120203, recall 0.775318
2017-12-10T03:21:32.005487: step 9567, loss 0.62487, acc 0.9375, prec 0.120217, recall 0.775351
2017-12-10T03:21:32.274907: step 9568, loss 0.962736, acc 0.9375, prec 0.120222, recall 0.775367
2017-12-10T03:21:32.549102: step 9569, loss 0.191712, acc 0.953125, prec 0.120218, recall 0.775367
2017-12-10T03:21:32.810098: step 9570, loss 1.10477, acc 0.921875, prec 0.120211, recall 0.775367
2017-12-10T03:21:33.072578: step 9571, loss 0.538126, acc 0.9375, prec 0.120205, recall 0.775367
2017-12-10T03:21:33.337371: step 9572, loss 0.263472, acc 0.96875, prec 0.120213, recall 0.775383
2017-12-10T03:21:33.611777: step 9573, loss 0.152559, acc 0.953125, prec 0.120218, recall 0.7754
2017-12-10T03:21:33.878433: step 9574, loss 0.325733, acc 0.984375, prec 0.120247, recall 0.775448
2017-12-10T03:21:34.153702: step 9575, loss 0.000171551, acc 1, prec 0.120266, recall 0.775481
2017-12-10T03:21:34.414470: step 9576, loss 0.226754, acc 0.984375, prec 0.120265, recall 0.775481
2017-12-10T03:21:34.675962: step 9577, loss 0.126303, acc 0.984375, prec 0.120264, recall 0.775481
2017-12-10T03:21:34.937004: step 9578, loss 0.117997, acc 0.96875, prec 0.120271, recall 0.775497
2017-12-10T03:21:35.205407: step 9579, loss 0.50504, acc 0.984375, prec 0.120289, recall 0.775529
2017-12-10T03:21:35.482281: step 9580, loss 0.105955, acc 0.984375, prec 0.120317, recall 0.775578
2017-12-10T03:21:35.745118: step 9581, loss 0.216164, acc 0.96875, prec 0.120315, recall 0.775578
2017-12-10T03:21:36.014245: step 9582, loss 0.223008, acc 0.96875, prec 0.120332, recall 0.77561
2017-12-10T03:21:36.274278: step 9583, loss 0.80991, acc 0.9375, prec 0.120336, recall 0.775627
2017-12-10T03:21:36.536134: step 9584, loss 0.0563934, acc 0.96875, prec 0.120343, recall 0.775643
2017-12-10T03:21:36.796141: step 9585, loss 3.43278e-05, acc 1, prec 0.120343, recall 0.775643
2017-12-10T03:21:37.057707: step 9586, loss 0.472952, acc 0.984375, prec 0.120352, recall 0.775659
2017-12-10T03:21:37.320473: step 9587, loss 0.223517, acc 0.984375, prec 0.12037, recall 0.775692
2017-12-10T03:21:37.581452: step 9588, loss 0.277813, acc 0.984375, prec 0.120369, recall 0.775692
2017-12-10T03:21:37.846137: step 9589, loss 0.0246558, acc 0.984375, prec 0.120387, recall 0.775724
2017-12-10T03:21:38.112001: step 9590, loss 0.00762668, acc 1, prec 0.120397, recall 0.77574
2017-12-10T03:21:38.379120: step 9591, loss 1.13545, acc 0.953125, prec 0.120393, recall 0.77574
2017-12-10T03:21:38.643603: step 9592, loss 0.318524, acc 0.984375, prec 0.120392, recall 0.77574
2017-12-10T03:21:38.909258: step 9593, loss 0.248982, acc 0.984375, prec 0.1204, recall 0.775756
2017-12-10T03:21:39.168726: step 9594, loss 0.0207845, acc 0.984375, prec 0.120399, recall 0.775756
2017-12-10T03:21:39.434595: step 9595, loss 0.000849308, acc 1, prec 0.120419, recall 0.775789
2017-12-10T03:21:39.693945: step 9596, loss 4.88534e-06, acc 1, prec 0.120428, recall 0.775805
2017-12-10T03:21:39.954080: step 9597, loss 0.436131, acc 0.96875, prec 0.120426, recall 0.775805
2017-12-10T03:21:40.213555: step 9598, loss 0.00358962, acc 1, prec 0.120445, recall 0.775837
2017-12-10T03:21:40.473103: step 9599, loss 1.31531e-05, acc 1, prec 0.120445, recall 0.775837
2017-12-10T03:21:40.729560: step 9600, loss 0.0135384, acc 0.984375, prec 0.120464, recall 0.77587

Evaluation:
2017-12-10T03:21:48.313258: step 9600, loss 24.1823, acc 0.971035, prec 0.120645, recall 0.769933

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-9600

2017-12-10T03:21:49.530101: step 9601, loss 0.0525844, acc 0.984375, prec 0.120643, recall 0.769933
2017-12-10T03:21:49.796025: step 9602, loss 0.00712477, acc 1, prec 0.120643, recall 0.769933
2017-12-10T03:21:50.062749: step 9603, loss 0.0959532, acc 0.96875, prec 0.120641, recall 0.769933
2017-12-10T03:21:50.326232: step 9604, loss 0.087868, acc 0.96875, prec 0.120667, recall 0.769982
2017-12-10T03:21:50.591419: step 9605, loss 0.757039, acc 0.96875, prec 0.120684, recall 0.770015
2017-12-10T03:21:50.855587: step 9606, loss 0.00109463, acc 1, prec 0.120704, recall 0.770048
2017-12-10T03:21:51.117241: step 9607, loss 0.00398775, acc 1, prec 0.120724, recall 0.770081
2017-12-10T03:21:51.379146: step 9608, loss 0.0355329, acc 0.984375, prec 0.120722, recall 0.770081
2017-12-10T03:21:51.644588: step 9609, loss 0.666499, acc 0.96875, prec 0.12072, recall 0.770081
2017-12-10T03:21:51.905624: step 9610, loss 0.341312, acc 0.984375, prec 0.120738, recall 0.770113
2017-12-10T03:21:52.171285: step 9611, loss 0.359446, acc 0.984375, prec 0.120746, recall 0.77013
2017-12-10T03:21:52.436801: step 9612, loss 0.209749, acc 1, prec 0.120756, recall 0.770146
2017-12-10T03:21:52.695912: step 9613, loss 4.64416, acc 0.984375, prec 0.120756, recall 0.770091
2017-12-10T03:21:52.960577: step 9614, loss 0.00821295, acc 1, prec 0.120766, recall 0.770108
2017-12-10T03:21:53.227652: step 9615, loss 4.35237, acc 0.984375, prec 0.120766, recall 0.770053
2017-12-10T03:21:53.493462: step 9616, loss 0.250268, acc 0.96875, prec 0.120763, recall 0.770053
2017-12-10T03:21:53.753938: step 9617, loss 0.451221, acc 0.984375, prec 0.120782, recall 0.770085
2017-12-10T03:21:54.023186: step 9618, loss 0.879093, acc 0.953125, prec 0.120807, recall 0.770135
2017-12-10T03:21:54.285688: step 9619, loss 0.550636, acc 0.921875, prec 0.1208, recall 0.770135
2017-12-10T03:21:54.563339: step 9620, loss 0.550774, acc 0.953125, prec 0.120796, recall 0.770135
2017-12-10T03:21:54.829945: step 9621, loss 0.160281, acc 0.96875, prec 0.120803, recall 0.770151
2017-12-10T03:21:55.096170: step 9622, loss 0.260781, acc 0.953125, prec 0.120809, recall 0.770167
2017-12-10T03:21:55.359277: step 9623, loss 0.305449, acc 0.96875, prec 0.120816, recall 0.770184
2017-12-10T03:21:55.630075: step 9624, loss 0.710073, acc 0.890625, prec 0.120807, recall 0.770184
2017-12-10T03:21:55.892382: step 9625, loss 0.777466, acc 0.921875, prec 0.12081, recall 0.7702
2017-12-10T03:21:56.159571: step 9626, loss 0.956027, acc 0.9375, prec 0.120804, recall 0.7702
2017-12-10T03:21:56.433576: step 9627, loss 1.06593, acc 0.890625, prec 0.120805, recall 0.770216
2017-12-10T03:21:56.710753: step 9628, loss 0.936692, acc 0.890625, prec 0.120805, recall 0.770233
2017-12-10T03:21:56.971937: step 9629, loss 0.917641, acc 0.890625, prec 0.120796, recall 0.770233
2017-12-10T03:21:57.235728: step 9630, loss 0.710914, acc 0.90625, prec 0.120788, recall 0.770233
2017-12-10T03:21:57.502825: step 9631, loss 1.69956, acc 0.859375, prec 0.120785, recall 0.770249
2017-12-10T03:21:57.764178: step 9632, loss 1.65675, acc 0.875, prec 0.120794, recall 0.770282
2017-12-10T03:21:58.030176: step 9633, loss 1.39023, acc 0.90625, prec 0.120815, recall 0.770331
2017-12-10T03:21:58.293514: step 9634, loss 1.45259, acc 0.875, prec 0.120805, recall 0.770331
2017-12-10T03:21:58.559031: step 9635, loss 0.519524, acc 0.9375, prec 0.120839, recall 0.770396
2017-12-10T03:21:58.828398: step 9636, loss 0.48314, acc 0.921875, prec 0.120842, recall 0.770413
2017-12-10T03:21:59.090103: step 9637, loss 0.530913, acc 0.90625, prec 0.120863, recall 0.770461
2017-12-10T03:21:59.355581: step 9638, loss 0.563813, acc 0.96875, prec 0.12087, recall 0.770478
2017-12-10T03:21:59.621524: step 9639, loss 0.255191, acc 0.96875, prec 0.120867, recall 0.770478
2017-12-10T03:21:59.887787: step 9640, loss 0.541707, acc 0.90625, prec 0.120869, recall 0.770494
2017-12-10T03:22:00.163478: step 9641, loss 0.308949, acc 0.984375, prec 0.120878, recall 0.77051
2017-12-10T03:22:00.435813: step 9642, loss 0.339407, acc 0.9375, prec 0.120882, recall 0.770527
2017-12-10T03:22:00.705403: step 9643, loss 0.324551, acc 0.9375, prec 0.120886, recall 0.770543
2017-12-10T03:22:00.973907: step 9644, loss 0.165414, acc 0.984375, prec 0.120895, recall 0.770559
2017-12-10T03:22:01.236622: step 9645, loss 0.124654, acc 0.984375, prec 0.120893, recall 0.770559
2017-12-10T03:22:01.499455: step 9646, loss 0.487111, acc 0.953125, prec 0.120899, recall 0.770576
2017-12-10T03:22:01.761047: step 9647, loss 0.383283, acc 0.9375, prec 0.120894, recall 0.770576
2017-12-10T03:22:02.030509: step 9648, loss 0.509878, acc 0.96875, prec 0.120891, recall 0.770576
2017-12-10T03:22:02.296326: step 9649, loss 0.554764, acc 0.96875, prec 0.120888, recall 0.770576
2017-12-10T03:22:02.562224: step 9650, loss 0.091413, acc 0.984375, prec 0.120907, recall 0.770608
2017-12-10T03:22:02.830349: step 9651, loss 0.018287, acc 0.984375, prec 0.120915, recall 0.770625
2017-12-10T03:22:03.095923: step 9652, loss 0.0705778, acc 0.984375, prec 0.120914, recall 0.770625
2017-12-10T03:22:03.359326: step 9653, loss 0.324694, acc 0.96875, prec 0.120921, recall 0.770641
2017-12-10T03:22:03.629331: step 9654, loss 0.00388352, acc 1, prec 0.12094, recall 0.770674
2017-12-10T03:22:03.896169: step 9655, loss 0.258621, acc 0.984375, prec 0.120949, recall 0.77069
2017-12-10T03:22:04.160362: step 9656, loss 1.2422, acc 0.90625, prec 0.120951, recall 0.770706
2017-12-10T03:22:04.426356: step 9657, loss 0.000719737, acc 1, prec 0.12096, recall 0.770722
2017-12-10T03:22:04.688611: step 9658, loss 0.224073, acc 0.984375, prec 0.120969, recall 0.770739
2017-12-10T03:22:04.954154: step 9659, loss 0.000964858, acc 1, prec 0.120998, recall 0.770787
2017-12-10T03:22:05.214614: step 9660, loss 0.058043, acc 0.984375, prec 0.120997, recall 0.770787
2017-12-10T03:22:05.481859: step 9661, loss 0.0367733, acc 0.96875, prec 0.121014, recall 0.77082
2017-12-10T03:22:05.746106: step 9662, loss 0.00102093, acc 1, prec 0.121024, recall 0.770836
2017-12-10T03:22:06.009921: step 9663, loss 0.000748024, acc 1, prec 0.121033, recall 0.770853
2017-12-10T03:22:06.270765: step 9664, loss 0.553547, acc 0.96875, prec 0.12105, recall 0.770885
2017-12-10T03:22:06.537492: step 9665, loss 0.210852, acc 0.984375, prec 0.121069, recall 0.770918
2017-12-10T03:22:06.799255: step 9666, loss 0.000598748, acc 1, prec 0.121069, recall 0.770918
2017-12-10T03:22:07.058427: step 9667, loss 0.000372727, acc 1, prec 0.121069, recall 0.770918
2017-12-10T03:22:07.317341: step 9668, loss 0.00651081, acc 1, prec 0.121069, recall 0.770918
2017-12-10T03:22:07.577881: step 9669, loss 0.0611271, acc 0.984375, prec 0.121087, recall 0.77095
2017-12-10T03:22:07.852762: step 9670, loss 0.199825, acc 0.984375, prec 0.121085, recall 0.77095
2017-12-10T03:22:08.112657: step 9671, loss 0.0012003, acc 1, prec 0.121095, recall 0.770966
2017-12-10T03:22:08.373675: step 9672, loss 4.26521, acc 0.96875, prec 0.121094, recall 0.770912
2017-12-10T03:22:08.642865: step 9673, loss 0.0638127, acc 0.96875, prec 0.121101, recall 0.770928
2017-12-10T03:22:08.905545: step 9674, loss 0.306806, acc 0.96875, prec 0.121108, recall 0.770944
2017-12-10T03:22:09.170029: step 9675, loss 0.0432952, acc 0.96875, prec 0.121105, recall 0.770944
2017-12-10T03:22:09.437939: step 9676, loss 0.000897908, acc 1, prec 0.121125, recall 0.770977
2017-12-10T03:22:09.696821: step 9677, loss 0.000270543, acc 1, prec 0.121125, recall 0.770977
2017-12-10T03:22:09.966754: step 9678, loss 0.688492, acc 0.953125, prec 0.121121, recall 0.770977
2017-12-10T03:22:10.238691: step 9679, loss 0.869123, acc 0.96875, prec 0.121128, recall 0.770993
2017-12-10T03:22:10.507132: step 9680, loss 0.891283, acc 0.96875, prec 0.121125, recall 0.770993
2017-12-10T03:22:10.778645: step 9681, loss 0.703834, acc 0.953125, prec 0.121121, recall 0.770993
2017-12-10T03:22:11.048650: step 9682, loss 0.195111, acc 0.953125, prec 0.121127, recall 0.771009
2017-12-10T03:22:11.313375: step 9683, loss 0.307213, acc 0.984375, prec 0.121145, recall 0.771042
2017-12-10T03:22:11.587772: step 9684, loss 0.836254, acc 0.9375, prec 0.12115, recall 0.771058
2017-12-10T03:22:11.852547: step 9685, loss 0.147769, acc 0.96875, prec 0.121147, recall 0.771058
2017-12-10T03:22:12.116914: step 9686, loss 0.0874721, acc 0.984375, prec 0.121146, recall 0.771058
2017-12-10T03:22:12.386513: step 9687, loss 0.0354316, acc 0.96875, prec 0.121153, recall 0.771074
2017-12-10T03:22:12.652003: step 9688, loss 0.195795, acc 0.9375, prec 0.121157, recall 0.77109
2017-12-10T03:22:12.919691: step 9689, loss 0.0497753, acc 0.984375, prec 0.121175, recall 0.771123
2017-12-10T03:22:13.182416: step 9690, loss 0.216152, acc 0.96875, prec 0.121173, recall 0.771123
2017-12-10T03:22:13.452053: step 9691, loss 0.360345, acc 0.953125, prec 0.121188, recall 0.771155
2017-12-10T03:22:13.719121: step 9692, loss 0.777926, acc 0.9375, prec 0.121183, recall 0.771155
2017-12-10T03:22:13.988349: step 9693, loss 0.273681, acc 0.953125, prec 0.121188, recall 0.771171
2017-12-10T03:22:14.256793: step 9694, loss 0.406708, acc 0.96875, prec 0.121196, recall 0.771188
2017-12-10T03:22:14.520862: step 9695, loss 0.451834, acc 0.96875, prec 0.121203, recall 0.771204
2017-12-10T03:22:14.787779: step 9696, loss 0.406208, acc 0.9375, prec 0.121207, recall 0.77122
2017-12-10T03:22:15.060194: step 9697, loss 0.332414, acc 0.9375, prec 0.121202, recall 0.77122
2017-12-10T03:22:15.324562: step 9698, loss 0.679622, acc 0.921875, prec 0.121214, recall 0.771252
2017-12-10T03:22:15.587614: step 9699, loss 0.673747, acc 0.921875, prec 0.121218, recall 0.771269
2017-12-10T03:22:15.849367: step 9700, loss 0.183399, acc 0.984375, prec 0.121226, recall 0.771285
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-9700

2017-12-10T03:22:17.197303: step 9701, loss 0.0555642, acc 0.984375, prec 0.121244, recall 0.771317
2017-12-10T03:22:17.459731: step 9702, loss 0.00090555, acc 1, prec 0.121244, recall 0.771317
2017-12-10T03:22:17.729469: step 9703, loss 0.0165752, acc 0.984375, prec 0.121243, recall 0.771317
2017-12-10T03:22:17.991143: step 9704, loss 0.515134, acc 0.96875, prec 0.12124, recall 0.771317
2017-12-10T03:22:18.262111: step 9705, loss 0.796278, acc 0.984375, prec 0.121249, recall 0.771333
2017-12-10T03:22:18.527313: step 9706, loss 0.281852, acc 0.96875, prec 0.121246, recall 0.771333
2017-12-10T03:22:18.791337: step 9707, loss 0.196137, acc 0.984375, prec 0.121254, recall 0.77135
2017-12-10T03:22:19.055067: step 9708, loss 1.0622, acc 0.9375, prec 0.121249, recall 0.77135
2017-12-10T03:22:19.316624: step 9709, loss 0.037854, acc 0.984375, prec 0.121257, recall 0.771366
2017-12-10T03:22:19.575292: step 9710, loss 0.00205949, acc 1, prec 0.121277, recall 0.771398
2017-12-10T03:22:19.834294: step 9711, loss 0.109675, acc 0.984375, prec 0.121276, recall 0.771398
2017-12-10T03:22:20.097755: step 9712, loss 0.00998544, acc 1, prec 0.121285, recall 0.771414
2017-12-10T03:22:20.357233: step 9713, loss 0.588453, acc 0.953125, prec 0.121281, recall 0.771414
2017-12-10T03:22:20.618232: step 9714, loss 0.21708, acc 0.984375, prec 0.12129, recall 0.771431
2017-12-10T03:22:20.886401: step 9715, loss 0.0976743, acc 0.96875, prec 0.121287, recall 0.771431
2017-12-10T03:22:21.156886: step 9716, loss 0.215519, acc 0.9375, prec 0.121282, recall 0.771431
2017-12-10T03:22:21.418361: step 9717, loss 1.05493e-05, acc 1, prec 0.121282, recall 0.771431
2017-12-10T03:22:21.675027: step 9718, loss 0.479151, acc 0.984375, prec 0.1213, recall 0.771463
2017-12-10T03:22:21.936522: step 9719, loss 0.0245168, acc 0.984375, prec 0.121308, recall 0.771479
2017-12-10T03:22:22.197284: step 9720, loss 5.05521e-06, acc 1, prec 0.121328, recall 0.771511
2017-12-10T03:22:22.452933: step 9721, loss 0.000509006, acc 1, prec 0.121328, recall 0.771511
2017-12-10T03:22:22.722074: step 9722, loss 0.0647786, acc 0.984375, prec 0.121336, recall 0.771528
2017-12-10T03:22:22.980425: step 9723, loss 0, acc 1, prec 0.121336, recall 0.771528
2017-12-10T03:22:23.224608: step 9724, loss 3.12349e-06, acc 1, prec 0.121336, recall 0.771528
2017-12-10T03:22:23.480586: step 9725, loss 0.771806, acc 0.984375, prec 0.121335, recall 0.771528
2017-12-10T03:22:23.744287: step 9726, loss 2.07875, acc 0.96875, prec 0.121334, recall 0.771473
2017-12-10T03:22:24.010842: step 9727, loss 0.461468, acc 0.953125, prec 0.121339, recall 0.771489
2017-12-10T03:22:24.271807: step 9728, loss 0.0189703, acc 0.984375, prec 0.121338, recall 0.771489
2017-12-10T03:22:24.530635: step 9729, loss 0.00908615, acc 1, prec 0.121357, recall 0.771522
2017-12-10T03:22:24.794263: step 9730, loss 0.397744, acc 0.953125, prec 0.121353, recall 0.771522
2017-12-10T03:22:25.058668: step 9731, loss 0.127329, acc 0.984375, prec 0.121372, recall 0.771554
2017-12-10T03:22:25.323271: step 9732, loss 0.337158, acc 0.9375, prec 0.121366, recall 0.771554
2017-12-10T03:22:25.582981: step 9733, loss 0.125684, acc 0.953125, prec 0.121362, recall 0.771554
2017-12-10T03:22:25.854496: step 9734, loss 0.0443271, acc 0.984375, prec 0.121361, recall 0.771554
2017-12-10T03:22:26.118021: step 9735, loss 0.838788, acc 0.96875, prec 0.121368, recall 0.77157
2017-12-10T03:22:26.382428: step 9736, loss 0.641683, acc 0.9375, prec 0.121372, recall 0.771586
2017-12-10T03:22:26.656695: step 9737, loss 2.00748, acc 0.9375, prec 0.121368, recall 0.771532
2017-12-10T03:22:26.927306: step 9738, loss 0.186224, acc 0.984375, prec 0.121377, recall 0.771548
2017-12-10T03:22:27.193012: step 9739, loss 0.765083, acc 0.9375, prec 0.121381, recall 0.771564
2017-12-10T03:22:27.461671: step 9740, loss 1.09374, acc 0.953125, prec 0.121377, recall 0.771564
2017-12-10T03:22:27.724791: step 9741, loss 0.263912, acc 0.953125, prec 0.121383, recall 0.77158
2017-12-10T03:22:27.985079: step 9742, loss 0.00998231, acc 1, prec 0.121402, recall 0.771612
2017-12-10T03:22:28.247188: step 9743, loss 0.350325, acc 0.953125, prec 0.121418, recall 0.771645
2017-12-10T03:22:28.508897: step 9744, loss 0.434919, acc 0.9375, prec 0.121432, recall 0.771677
2017-12-10T03:22:28.779768: step 9745, loss 0.267433, acc 0.9375, prec 0.121446, recall 0.771709
2017-12-10T03:22:29.051832: step 9746, loss 0.000323598, acc 1, prec 0.121456, recall 0.771725
2017-12-10T03:22:29.308700: step 9747, loss 0.251502, acc 0.9375, prec 0.12146, recall 0.771741
2017-12-10T03:22:29.580582: step 9748, loss 1.1396, acc 0.90625, prec 0.121452, recall 0.771741
2017-12-10T03:22:29.845033: step 9749, loss 0.318972, acc 0.96875, prec 0.121469, recall 0.771774
2017-12-10T03:22:30.117800: step 9750, loss 0.0542471, acc 0.96875, prec 0.121486, recall 0.771806
2017-12-10T03:22:30.389104: step 9751, loss 0.367367, acc 0.9375, prec 0.12148, recall 0.771806
2017-12-10T03:22:30.660114: step 9752, loss 0.458943, acc 0.953125, prec 0.121486, recall 0.771822
2017-12-10T03:22:30.929687: step 9753, loss 1.20526, acc 0.96875, prec 0.121493, recall 0.771838
2017-12-10T03:22:31.194866: step 9754, loss 0.546301, acc 0.953125, prec 0.121489, recall 0.771838
2017-12-10T03:22:31.461938: step 9755, loss 0.937432, acc 0.9375, prec 0.121493, recall 0.771854
2017-12-10T03:22:31.731513: step 9756, loss 0.208433, acc 0.953125, prec 0.121499, recall 0.77187
2017-12-10T03:22:31.993296: step 9757, loss 0.386831, acc 0.953125, prec 0.121495, recall 0.77187
2017-12-10T03:22:32.252412: step 9758, loss 1.35117, acc 0.9375, prec 0.12149, recall 0.77187
2017-12-10T03:22:32.516728: step 9759, loss 0.459292, acc 0.953125, prec 0.121505, recall 0.771903
2017-12-10T03:22:32.779117: step 9760, loss 0.358178, acc 0.984375, prec 0.121504, recall 0.771903
2017-12-10T03:22:33.043289: step 9761, loss 0.815937, acc 0.96875, prec 0.121511, recall 0.771919
2017-12-10T03:22:33.304037: step 9762, loss 0.0210736, acc 0.984375, prec 0.121548, recall 0.771983
2017-12-10T03:22:33.576897: step 9763, loss 0.0956712, acc 0.96875, prec 0.121556, recall 0.771999
2017-12-10T03:22:33.850751: step 9764, loss 0.656916, acc 0.9375, prec 0.12157, recall 0.772031
2017-12-10T03:22:34.114264: step 9765, loss 0.64696, acc 0.984375, prec 0.121578, recall 0.772047
2017-12-10T03:22:34.376452: step 9766, loss 0.196888, acc 0.953125, prec 0.121584, recall 0.772063
2017-12-10T03:22:34.638439: step 9767, loss 0.00100339, acc 1, prec 0.121584, recall 0.772063
2017-12-10T03:22:34.898416: step 9768, loss 0.0137043, acc 0.984375, prec 0.121582, recall 0.772063
2017-12-10T03:22:35.163897: step 9769, loss 0.113323, acc 0.96875, prec 0.12158, recall 0.772063
2017-12-10T03:22:35.427633: step 9770, loss 0.637395, acc 0.9375, prec 0.121574, recall 0.772063
2017-12-10T03:22:35.693596: step 9771, loss 0.264333, acc 0.96875, prec 0.121581, recall 0.77208
2017-12-10T03:22:35.957148: step 9772, loss 0.252068, acc 0.96875, prec 0.121588, recall 0.772096
2017-12-10T03:22:36.223969: step 9773, loss 0.000268194, acc 1, prec 0.121608, recall 0.772128
2017-12-10T03:22:36.480890: step 9774, loss 9.61489e-06, acc 1, prec 0.121618, recall 0.772144
2017-12-10T03:22:36.735685: step 9775, loss 9.43789e-05, acc 1, prec 0.121618, recall 0.772144
2017-12-10T03:22:36.999041: step 9776, loss 0.00212651, acc 1, prec 0.121627, recall 0.77216
2017-12-10T03:22:37.262342: step 9777, loss 0.000273809, acc 1, prec 0.121627, recall 0.77216
2017-12-10T03:22:37.517941: step 9778, loss 0.117345, acc 0.984375, prec 0.121626, recall 0.77216
2017-12-10T03:22:37.787633: step 9779, loss 0.0014125, acc 1, prec 0.121626, recall 0.77216
2017-12-10T03:22:38.055984: step 9780, loss 0.00183188, acc 1, prec 0.121636, recall 0.772176
2017-12-10T03:22:38.318407: step 9781, loss 0.447312, acc 0.96875, prec 0.121643, recall 0.772192
2017-12-10T03:22:38.585231: step 9782, loss 0.129076, acc 0.984375, prec 0.121642, recall 0.772192
2017-12-10T03:22:38.850420: step 9783, loss 8.04167e-05, acc 1, prec 0.121642, recall 0.772192
2017-12-10T03:22:39.115251: step 9784, loss 0.352186, acc 0.96875, prec 0.121649, recall 0.772208
2017-12-10T03:22:39.379711: step 9785, loss 0.869834, acc 0.984375, prec 0.121657, recall 0.772224
2017-12-10T03:22:39.651286: step 9786, loss 0.000506571, acc 1, prec 0.121667, recall 0.77224
2017-12-10T03:22:39.911257: step 9787, loss 1.17204, acc 0.953125, prec 0.121682, recall 0.772272
2017-12-10T03:22:40.179358: step 9788, loss 0.00712998, acc 1, prec 0.121682, recall 0.772272
2017-12-10T03:22:40.444269: step 9789, loss 0.228943, acc 0.96875, prec 0.121689, recall 0.772288
2017-12-10T03:22:40.706945: step 9790, loss 0.0125792, acc 0.984375, prec 0.121688, recall 0.772288
2017-12-10T03:22:40.974912: step 9791, loss 2.17763, acc 0.9375, prec 0.121703, recall 0.772266
2017-12-10T03:22:41.242141: step 9792, loss 0.207842, acc 0.96875, prec 0.121701, recall 0.772266
2017-12-10T03:22:41.507056: step 9793, loss 0.367911, acc 0.96875, prec 0.121698, recall 0.772266
2017-12-10T03:22:41.783804: step 9794, loss 0.928077, acc 0.953125, prec 0.121723, recall 0.772314
2017-12-10T03:22:42.044564: step 9795, loss 0.53203, acc 0.96875, prec 0.121721, recall 0.772314
2017-12-10T03:22:42.307863: step 9796, loss 0.50089, acc 0.96875, prec 0.121747, recall 0.772362
2017-12-10T03:22:42.568099: step 9797, loss 0.390987, acc 0.953125, prec 0.121753, recall 0.772378
2017-12-10T03:22:42.835721: step 9798, loss 0.563731, acc 0.9375, prec 0.121747, recall 0.772378
2017-12-10T03:22:43.107958: step 9799, loss 0.26271, acc 0.9375, prec 0.121752, recall 0.772394
2017-12-10T03:22:43.369223: step 9800, loss 0.489942, acc 0.9375, prec 0.121766, recall 0.772426
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-9800

2017-12-10T03:22:44.764207: step 9801, loss 0.588192, acc 0.953125, prec 0.121791, recall 0.772474
2017-12-10T03:22:45.025471: step 9802, loss 0.808041, acc 0.921875, prec 0.121784, recall 0.772474
2017-12-10T03:22:45.290946: step 9803, loss 0.310903, acc 0.96875, prec 0.121781, recall 0.772474
2017-12-10T03:22:45.562110: step 9804, loss 0.156752, acc 0.984375, prec 0.1218, recall 0.772507
2017-12-10T03:22:45.827765: step 9805, loss 0.981612, acc 0.921875, prec 0.121812, recall 0.772539
2017-12-10T03:22:46.092288: step 9806, loss 0.546687, acc 0.953125, prec 0.121818, recall 0.772555
2017-12-10T03:22:46.359322: step 9807, loss 0.359769, acc 0.953125, prec 0.121843, recall 0.772603
2017-12-10T03:22:46.623787: step 9808, loss 0.430291, acc 0.96875, prec 0.12185, recall 0.772619
2017-12-10T03:22:46.885842: step 9809, loss 1.1314, acc 0.953125, prec 0.121856, recall 0.772635
2017-12-10T03:22:47.149383: step 9810, loss 0.576997, acc 0.9375, prec 0.121851, recall 0.772635
2017-12-10T03:22:47.416000: step 9811, loss 0.161358, acc 0.96875, prec 0.121858, recall 0.772651
2017-12-10T03:22:47.688506: step 9812, loss 0.405107, acc 0.953125, prec 0.121854, recall 0.772651
2017-12-10T03:22:47.952745: step 9813, loss 0.00400787, acc 1, prec 0.121863, recall 0.772667
2017-12-10T03:22:48.224458: step 9814, loss 0.804549, acc 0.953125, prec 0.121879, recall 0.772699
2017-12-10T03:22:48.489114: step 9815, loss 0.216359, acc 0.96875, prec 0.121876, recall 0.772699
2017-12-10T03:22:48.755915: step 9816, loss 0.0113976, acc 0.984375, prec 0.121875, recall 0.772699
2017-12-10T03:22:49.019133: step 9817, loss 0.0949837, acc 0.984375, prec 0.121873, recall 0.772699
2017-12-10T03:22:49.286360: step 9818, loss 0.337044, acc 0.953125, prec 0.121869, recall 0.772699
2017-12-10T03:22:49.552282: step 9819, loss 0.367728, acc 0.953125, prec 0.121875, recall 0.772714
2017-12-10T03:22:49.824599: step 9820, loss 0.167528, acc 0.984375, prec 0.121883, recall 0.77273
2017-12-10T03:22:50.094442: step 9821, loss 0.000423497, acc 1, prec 0.121913, recall 0.772778
2017-12-10T03:22:50.356839: step 9822, loss 0.478286, acc 0.9375, prec 0.121907, recall 0.772778
2017-12-10T03:22:50.622682: step 9823, loss 0.457879, acc 0.953125, prec 0.121903, recall 0.772778
2017-12-10T03:22:50.890487: step 9824, loss 0.0437019, acc 0.984375, prec 0.121902, recall 0.772778
2017-12-10T03:22:51.151645: step 9825, loss 7.34819e-05, acc 1, prec 0.121911, recall 0.772794
2017-12-10T03:22:51.409682: step 9826, loss 1.92183e-05, acc 1, prec 0.121911, recall 0.772794
2017-12-10T03:22:51.669118: step 9827, loss 0.304374, acc 0.96875, prec 0.121909, recall 0.772794
2017-12-10T03:22:51.932078: step 9828, loss 0.127156, acc 0.96875, prec 0.121916, recall 0.77281
2017-12-10T03:22:52.197934: step 9829, loss 0.0789734, acc 0.984375, prec 0.121934, recall 0.772842
2017-12-10T03:22:52.459515: step 9830, loss 7.22259e-06, acc 1, prec 0.121944, recall 0.772858
2017-12-10T03:22:52.717636: step 9831, loss 1.58325e-07, acc 1, prec 0.121944, recall 0.772858
2017-12-10T03:22:52.975468: step 9832, loss 0.365036, acc 0.984375, prec 0.121952, recall 0.772874
2017-12-10T03:22:53.240865: step 9833, loss 0.0789981, acc 0.984375, prec 0.12196, recall 0.77289
2017-12-10T03:22:53.507902: step 9834, loss 4.39253, acc 0.96875, prec 0.121979, recall 0.772868
2017-12-10T03:22:53.778944: step 9835, loss 0.473658, acc 0.984375, prec 0.122006, recall 0.772916
2017-12-10T03:22:54.048834: step 9836, loss 0.698948, acc 0.96875, prec 0.122013, recall 0.772932
2017-12-10T03:22:54.313610: step 9837, loss 0.00164665, acc 1, prec 0.122023, recall 0.772948
2017-12-10T03:22:54.576977: step 9838, loss 0.254661, acc 0.984375, prec 0.122032, recall 0.772963
2017-12-10T03:22:54.839890: step 9839, loss 0.000991623, acc 1, prec 0.122041, recall 0.772979
2017-12-10T03:22:55.109330: step 9840, loss 0.972206, acc 0.96875, prec 0.122048, recall 0.772995
2017-12-10T03:22:55.372502: step 9841, loss 0.0215564, acc 0.984375, prec 0.122047, recall 0.772995
2017-12-10T03:22:55.635756: step 9842, loss 4.49706e-05, acc 1, prec 0.122057, recall 0.773011
2017-12-10T03:22:55.898018: step 9843, loss 0.062831, acc 0.96875, prec 0.122054, recall 0.773011
2017-12-10T03:22:56.158199: step 9844, loss 0.00482678, acc 1, prec 0.122054, recall 0.773011
2017-12-10T03:22:56.416328: step 9845, loss 0.537862, acc 0.984375, prec 0.122062, recall 0.773027
2017-12-10T03:22:56.685409: step 9846, loss 0.978114, acc 0.921875, prec 0.122056, recall 0.773027
2017-12-10T03:22:56.950811: step 9847, loss 0.443785, acc 0.953125, prec 0.122061, recall 0.773043
2017-12-10T03:22:57.212703: step 9848, loss 0.718296, acc 0.984375, prec 0.12206, recall 0.773043
2017-12-10T03:22:57.474866: step 9849, loss 0.183584, acc 0.96875, prec 0.122057, recall 0.773043
2017-12-10T03:22:57.737764: step 9850, loss 0.0433491, acc 0.984375, prec 0.122066, recall 0.773059
2017-12-10T03:22:58.003036: step 9851, loss 0.581097, acc 0.984375, prec 0.122074, recall 0.773075
2017-12-10T03:22:58.265764: step 9852, loss 9.46785e-05, acc 1, prec 0.122093, recall 0.773107
2017-12-10T03:22:58.525754: step 9853, loss 0.549875, acc 0.96875, prec 0.12211, recall 0.773139
2017-12-10T03:22:58.786653: step 9854, loss 0.533258, acc 0.984375, prec 0.122128, recall 0.773171
2017-12-10T03:22:59.050256: step 9855, loss 2.22904, acc 0.96875, prec 0.122137, recall 0.773132
2017-12-10T03:22:59.322898: step 9856, loss 0.169994, acc 0.953125, prec 0.122133, recall 0.773132
2017-12-10T03:22:59.584547: step 9857, loss 0.166156, acc 0.984375, prec 0.122151, recall 0.773164
2017-12-10T03:22:59.860677: step 9858, loss 0.818543, acc 0.96875, prec 0.122158, recall 0.77318
2017-12-10T03:23:00.129988: step 9859, loss 0.390712, acc 0.96875, prec 0.122165, recall 0.773196
2017-12-10T03:23:00.403949: step 9860, loss 0.960586, acc 0.921875, prec 0.122158, recall 0.773196
2017-12-10T03:23:00.672781: step 9861, loss 0.205148, acc 0.921875, prec 0.122161, recall 0.773212
2017-12-10T03:23:00.939090: step 9862, loss 0.000343971, acc 1, prec 0.122161, recall 0.773212
2017-12-10T03:23:01.198439: step 9863, loss 0.130839, acc 0.96875, prec 0.122187, recall 0.773259
2017-12-10T03:23:01.465432: step 9864, loss 0.268421, acc 0.96875, prec 0.122185, recall 0.773259
2017-12-10T03:23:01.728983: step 9865, loss 1.10229, acc 0.90625, prec 0.122186, recall 0.773275
2017-12-10T03:23:01.993104: step 9866, loss 0.00855585, acc 1, prec 0.122206, recall 0.773307
2017-12-10T03:23:02.259286: step 9867, loss 0.788396, acc 0.9375, prec 0.1222, recall 0.773307
2017-12-10T03:23:02.523078: step 9868, loss 0.680195, acc 0.953125, prec 0.122196, recall 0.773307
2017-12-10T03:23:02.786965: step 9869, loss 0.36364, acc 0.96875, prec 0.122203, recall 0.773323
2017-12-10T03:23:03.049867: step 9870, loss 0.27382, acc 0.9375, prec 0.122208, recall 0.773339
2017-12-10T03:23:03.313872: step 9871, loss 0.139434, acc 0.96875, prec 0.122205, recall 0.773339
2017-12-10T03:23:03.579847: step 9872, loss 0.406708, acc 0.9375, prec 0.122199, recall 0.773339
2017-12-10T03:23:03.847295: step 9873, loss 0.00552293, acc 1, prec 0.122209, recall 0.773355
2017-12-10T03:23:04.109243: step 9874, loss 0.356261, acc 0.96875, prec 0.122216, recall 0.773371
2017-12-10T03:23:04.378034: step 9875, loss 1.18017, acc 0.96875, prec 0.122213, recall 0.773371
2017-12-10T03:23:04.646949: step 9876, loss 0.495514, acc 0.921875, prec 0.122216, recall 0.773387
2017-12-10T03:23:04.910292: step 9877, loss 0.0344759, acc 0.984375, prec 0.122215, recall 0.773387
2017-12-10T03:23:05.186135: step 9878, loss 0.820984, acc 0.953125, prec 0.122211, recall 0.773387
2017-12-10T03:23:05.448108: step 9879, loss 0.000261103, acc 1, prec 0.122221, recall 0.773402
2017-12-10T03:23:05.704747: step 9880, loss 0.0590258, acc 0.96875, prec 0.122237, recall 0.773434
2017-12-10T03:23:05.973107: step 9881, loss 0.511451, acc 0.953125, prec 0.122233, recall 0.773434
2017-12-10T03:23:06.239689: step 9882, loss 0.456272, acc 0.96875, prec 0.122279, recall 0.773514
2017-12-10T03:23:06.499902: step 9883, loss 0.157367, acc 0.984375, prec 0.122288, recall 0.773529
2017-12-10T03:23:06.770492: step 9884, loss 19.7474, acc 0.953125, prec 0.122285, recall 0.773475
2017-12-10T03:23:07.041437: step 9885, loss 0.0845064, acc 0.96875, prec 0.122311, recall 0.773523
2017-12-10T03:23:07.309280: step 9886, loss 0.643092, acc 0.921875, prec 0.122305, recall 0.773523
2017-12-10T03:23:07.579073: step 9887, loss 0.742122, acc 0.953125, prec 0.12231, recall 0.773539
2017-12-10T03:23:07.843552: step 9888, loss 0.0470129, acc 0.984375, prec 0.122309, recall 0.773539
2017-12-10T03:23:08.104909: step 9889, loss 0.126875, acc 0.96875, prec 0.122326, recall 0.77357
2017-12-10T03:23:08.366933: step 9890, loss 0.337782, acc 0.953125, prec 0.122322, recall 0.77357
2017-12-10T03:23:08.631845: step 9891, loss 0.77533, acc 0.921875, prec 0.122315, recall 0.77357
2017-12-10T03:23:08.894872: step 9892, loss 0.470284, acc 0.921875, prec 0.122318, recall 0.773586
2017-12-10T03:23:09.158647: step 9893, loss 0.678264, acc 0.9375, prec 0.122332, recall 0.773618
2017-12-10T03:23:09.435651: step 9894, loss 0.450652, acc 0.921875, prec 0.122335, recall 0.773634
2017-12-10T03:23:09.706431: step 9895, loss 0.588552, acc 0.9375, prec 0.122349, recall 0.773665
2017-12-10T03:23:09.973576: step 9896, loss 0.627532, acc 0.921875, prec 0.122342, recall 0.773665
2017-12-10T03:23:10.236262: step 9897, loss 1.52502, acc 0.84375, prec 0.122338, recall 0.773681
2017-12-10T03:23:10.502299: step 9898, loss 1.37567, acc 0.921875, prec 0.12236, recall 0.773729
2017-12-10T03:23:10.763863: step 9899, loss 0.316672, acc 0.953125, prec 0.122366, recall 0.773745
2017-12-10T03:23:11.030182: step 9900, loss 0.793724, acc 0.9375, prec 0.12238, recall 0.773776

Evaluation:
2017-12-10T03:23:18.545911: step 9900, loss 11.4382, acc 0.926597, prec 0.12226, recall 0.770879

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_0/1512891150/checkpoints/model-9900

2017-12-10T03:23:19.869582: step 9901, loss 0.597119, acc 0.9375, prec 0.122255, recall 0.770879
2017-12-10T03:23:20.128354: step 9902, loss 0.130454, acc 0.953125, prec 0.122251, recall 0.770879
2017-12-10T03:23:20.397448: step 9903, loss 0.321416, acc 0.9375, prec 0.122255, recall 0.770895
2017-12-10T03:23:20.659919: step 9904, loss 0.314787, acc 0.953125, prec 0.122251, recall 0.770895
2017-12-10T03:23:20.926129: step 9905, loss 0.133242, acc 0.96875, prec 0.122258, recall 0.770911
2017-12-10T03:23:21.196828: step 9906, loss 0.636597, acc 0.96875, prec 0.122274, recall 0.770943
2017-12-10T03:23:21.461634: step 9907, loss 0.447697, acc 0.921875, prec 0.122277, recall 0.770959
2017-12-10T03:23:21.722142: step 9908, loss 0.467277, acc 0.96875, prec 0.122275, recall 0.770959
2017-12-10T03:23:21.992557: step 9909, loss 0.274384, acc 0.9375, prec 0.122269, recall 0.770959
2017-12-10T03:23:22.256932: step 9910, loss 0.048087, acc 0.953125, prec 0.122275, recall 0.770974
2017-12-10T03:23:22.525239: step 9911, loss 0.393853, acc 0.9375, prec 0.122279, recall 0.77099
2017-12-10T03:23:22.798651: step 9912, loss 0.621432, acc 0.96875, prec 0.122277, recall 0.77099
2017-12-10T03:23:23.062838: step 9913, loss 0.196835, acc 0.953125, prec 0.122273, recall 0.77099
2017-12-10T03:23:23.327094: step 9914, loss 0.7097, acc 0.9375, prec 0.122286, recall 0.771022
2017-12-10T03:23:23.588589: step 9915, loss 0.115284, acc 0.984375, prec 0.122285, recall 0.771022
2017-12-10T03:23:23.854851: step 9916, loss 0.19091, acc 0.984375, prec 0.122293, recall 0.771038
2017-12-10T03:23:24.118513: step 9917, loss 0.0170619, acc 0.984375, prec 0.122292, recall 0.771038
2017-12-10T03:23:24.377728: step 9918, loss 1.38084, acc 0.921875, prec 0.122295, recall 0.771054
2017-12-10T03:23:24.645462: step 9919, loss 0.661955, acc 0.984375, prec 0.122303, recall 0.771069
2017-12-10T03:23:24.909577: step 9920, loss 0.0142788, acc 0.984375, prec 0.122302, recall 0.771069
2017-12-10T03:23:25.171935: step 9921, loss 0.00052193, acc 1, prec 0.122302, recall 0.771069
2017-12-10T03:23:25.434078: step 9922, loss 6.27703e-07, acc 1, prec 0.122302, recall 0.771069
2017-12-10T03:23:25.694419: step 9923, loss 0.0294431, acc 0.984375, prec 0.12231, recall 0.771085
2017-12-10T03:23:25.962641: step 9924, loss 0.553075, acc 0.984375, prec 0.122309, recall 0.771085
2017-12-10T03:23:26.227196: step 9925, loss 0.000678005, acc 1, prec 0.122318, recall 0.771101
2017-12-10T03:23:26.491625: step 9926, loss 0.0526766, acc 0.984375, prec 0.122336, recall 0.771133
2017-12-10T03:23:26.767815: step 9927, loss 0.0393982, acc 0.984375, prec 0.122345, recall 0.771148
2017-12-10T03:23:27.035786: step 9928, loss 0.0799711, acc 0.96875, prec 0.122342, recall 0.771148
2017-12-10T03:23:27.300126: step 9929, loss 0.00761452, acc 1, prec 0.122351, recall 0.771164
2017-12-10T03:23:27.564204: step 9930, loss 0.00151175, acc 1, prec 0.122361, recall 0.77118
2017-12-10T03:23:27.833556: step 9931, loss 0.441282, acc 0.953125, prec 0.122376, recall 0.771212
2017-12-10T03:23:28.093013: step 9932, loss 1.379, acc 0.96875, prec 0.122383, recall 0.771227
2017-12-10T03:23:28.351440: step 9933, loss 0.0405624, acc 0.984375, prec 0.122382, recall 0.771227
2017-12-10T03:23:28.616137: step 9934, loss 0.0616278, acc 1, prec 0.122401, recall 0.771259
2017-12-10T03:23:28.875023: step 9935, loss 30.0727, acc 0.953125, prec 0.122398, recall 0.771206
2017-12-10T03:23:29.135255: step 9936, loss 0.060685, acc 0.984375, prec 0.122397, recall 0.771206
2017-12-10T03:23:29.401080: step 9937, loss 1.71727e-06, acc 1, prec 0.122416, recall 0.771237
2017-12-10T03:23:29.659620: step 9938, loss 0.352175, acc 0.96875, prec 0.122414, recall 0.771237
2017-12-10T03:23:29.929059: step 9939, loss 0.116564, acc 0.984375, prec 0.122422, recall 0.771253
2017-12-10T03:23:30.165707: step 9940, loss 2.49574, acc 0.941176, prec 0.122418, recall 0.771253
Training finished
Starting Fold: 1 => Train/Dev split: 31795/10599


 #################### 


SEQUENCE LENGTH 273
BATCH SIZE 64
EMBEDDING SIZE 128
FILTER SIZES [3, 4, 5]
NUMBER OF FILTERS 256
L2 REG LAMBDA 0.0
EPOCHS 20



RESULT DIR num_filter_256_fold_1
VOCAB SIZE 33447
DROPOUT PROBABILITY 0.5



WORD EMBEDDING True
POSITION EMBEDDING True
POS EMBEDDING True


 #################### 


Writing to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211

Start training
2017-12-10T03:23:33.910303: step 1, loss 4.36981, acc 0.28125, prec 0.0212766, recall 1
2017-12-10T03:23:34.171882: step 2, loss 1.66516, acc 0.5625, prec 0.0133333, recall 1
2017-12-10T03:23:34.432118: step 3, loss 0.39298, acc 0.875, prec 0.0120482, recall 1
2017-12-10T03:23:34.704373: step 4, loss 11.8255, acc 0.9375, prec 0.0116279, recall 0.5
2017-12-10T03:23:34.970592: step 5, loss 16.4365, acc 0.921875, prec 0.0111111, recall 0.333333
2017-12-10T03:23:35.235946: step 6, loss 19.0051, acc 0.9375, prec 0.0108696, recall 0.2
2017-12-10T03:23:35.499099: step 7, loss 0.362275, acc 0.890625, prec 0.02, recall 0.333333
2017-12-10T03:23:35.756922: step 8, loss 1.04722, acc 0.765625, prec 0.0173913, recall 0.333333
2017-12-10T03:23:36.030494: step 9, loss 0.99592, acc 0.703125, prec 0.0222222, recall 0.428571
2017-12-10T03:23:36.295416: step 10, loss 1.56467, acc 0.703125, prec 0.0258065, recall 0.5
2017-12-10T03:23:36.556054: step 11, loss 1.38337, acc 0.65625, prec 0.0225989, recall 0.5
2017-12-10T03:23:36.816965: step 12, loss 3.5221, acc 0.484375, prec 0.0191388, recall 0.444444
2017-12-10T03:23:37.083120: step 13, loss 2.01585, acc 0.625, prec 0.0171674, recall 0.444444
2017-12-10T03:23:37.344503: step 14, loss 2.26748, acc 0.453125, prec 0.0149254, recall 0.444444
2017-12-10T03:23:37.603251: step 15, loss 1.79259, acc 0.5625, prec 0.0135135, recall 0.444444
2017-12-10T03:23:37.862446: step 16, loss 1.4847, acc 0.625, prec 0.0155763, recall 0.5
2017-12-10T03:23:38.122223: step 17, loss 2.088, acc 0.625, prec 0.0144928, recall 0.5
2017-12-10T03:23:38.383077: step 18, loss 0.780775, acc 0.78125, prec 0.0139276, recall 0.5
2017-12-10T03:23:38.641564: step 19, loss 35.7784, acc 0.734375, prec 0.013369, recall 0.416667
2017-12-10T03:23:38.901625: step 20, loss 0.777449, acc 0.8125, prec 0.0129534, recall 0.416667
2017-12-10T03:23:39.167496: step 21, loss 2.69679, acc 0.703125, prec 0.0123762, recall 0.384615
2017-12-10T03:23:39.432726: step 22, loss 1.50313, acc 0.6875, prec 0.0164319, recall 0.466667
2017-12-10T03:23:39.695090: step 23, loss 1.52088, acc 0.6875, prec 0.0156951, recall 0.466667
2017-12-10T03:23:39.952250: step 24, loss 2.38707, acc 0.515625, prec 0.0167364, recall 0.5
2017-12-10T03:23:40.214462: step 25, loss 2.36127, acc 0.609375, prec 0.0159046, recall 0.5
2017-12-10T03:23:40.477979: step 26, loss 2.28977, acc 0.5625, prec 0.0150659, recall 0.5
2017-12-10T03:23:40.739660: step 27, loss 6.46938, acc 0.65625, prec 0.0144928, recall 0.470588
2017-12-10T03:23:41.008353: step 28, loss 1.8995, acc 0.625, prec 0.0155979, recall 0.5
2017-12-10T03:23:41.271520: step 29, loss 2.40283, acc 0.578125, prec 0.0181518, recall 0.55
2017-12-10T03:23:41.538165: step 30, loss 5.61717, acc 0.65625, prec 0.0175439, recall 0.52381
2017-12-10T03:23:41.818552: step 31, loss 2.08964, acc 0.515625, prec 0.0167173, recall 0.52381
2017-12-10T03:23:42.077538: step 32, loss 2.11413, acc 0.5625, prec 0.016035, recall 0.52381
2017-12-10T03:23:42.336482: step 33, loss 1.72589, acc 0.640625, prec 0.0155148, recall 0.52381
2017-12-10T03:23:42.601887: step 34, loss 15.1618, acc 0.703125, prec 0.0164835, recall 0.521739
2017-12-10T03:23:42.870763: step 35, loss 1.4113, acc 0.640625, prec 0.0172872, recall 0.541667
2017-12-10T03:23:43.130680: step 36, loss 1.37059, acc 0.734375, prec 0.0181818, recall 0.56
2017-12-10T03:23:43.393266: step 37, loss 1.77955, acc 0.640625, prec 0.0176545, recall 0.56
2017-12-10T03:23:43.654535: step 38, loss 0.899323, acc 0.796875, prec 0.0173697, recall 0.56
2017-12-10T03:23:43.914265: step 39, loss 13.6758, acc 0.71875, prec 0.0170109, recall 0.538462
2017-12-10T03:23:44.180608: step 40, loss 1.41196, acc 0.640625, prec 0.0165485, recall 0.538462
2017-12-10T03:23:44.445855: step 41, loss 2.3848, acc 0.609375, prec 0.0160735, recall 0.538462
2017-12-10T03:23:44.704275: step 42, loss 1.28504, acc 0.71875, prec 0.0168539, recall 0.555556
2017-12-10T03:23:44.970746: step 43, loss 1.32031, acc 0.703125, prec 0.0175824, recall 0.571429
2017-12-10T03:23:45.228837: step 44, loss 6.19413, acc 0.625, prec 0.017149, recall 0.551724
2017-12-10T03:23:45.490783: step 45, loss 1.50707, acc 0.625, prec 0.0167189, recall 0.551724
2017-12-10T03:23:45.756872: step 46, loss 2.28166, acc 0.609375, prec 0.0182927, recall 0.580645
2017-12-10T03:23:46.018186: step 47, loss 6.982, acc 0.640625, prec 0.0178926, recall 0.5625
2017-12-10T03:23:46.277756: step 48, loss 1.58798, acc 0.578125, prec 0.0183752, recall 0.575758
2017-12-10T03:23:46.543185: step 49, loss 2.08857, acc 0.578125, prec 0.0179076, recall 0.575758
2017-12-10T03:23:46.817265: step 50, loss 1.70946, acc 0.578125, prec 0.0183655, recall 0.588235
2017-12-10T03:23:47.079246: step 51, loss 2.25838, acc 0.59375, prec 0.0179372, recall 0.588235
2017-12-10T03:23:47.341515: step 52, loss 1.14931, acc 0.75, prec 0.0194175, recall 0.611111
2017-12-10T03:23:47.611859: step 53, loss 11.0853, acc 0.609375, prec 0.019879, recall 0.589744
2017-12-10T03:23:47.875857: step 54, loss 15.0249, acc 0.59375, prec 0.0202874, recall 0.585366
2017-12-10T03:23:48.135823: step 55, loss 2.7475, acc 0.4375, prec 0.0204918, recall 0.595238
2017-12-10T03:23:48.395708: step 56, loss 3.53978, acc 0.453125, prec 0.0199203, recall 0.595238
2017-12-10T03:23:48.651283: step 57, loss 4.67057, acc 0.390625, prec 0.0208333, recall 0.613636
2017-12-10T03:23:48.909916: step 58, loss 3.97917, acc 0.328125, prec 0.0201643, recall 0.613636
2017-12-10T03:23:49.168905: step 59, loss 12.8655, acc 0.28125, prec 0.0195087, recall 0.6
2017-12-10T03:23:49.430315: step 60, loss 5.04739, acc 0.328125, prec 0.0196078, recall 0.608696
2017-12-10T03:23:49.690426: step 61, loss 4.94915, acc 0.25, prec 0.0196344, recall 0.617021
2017-12-10T03:23:49.950321: step 62, loss 4.19394, acc 0.296875, prec 0.019698, recall 0.625
2017-12-10T03:23:50.206535: step 63, loss 3.92284, acc 0.375, prec 0.0191939, recall 0.625
2017-12-10T03:23:50.466143: step 64, loss 4.00287, acc 0.40625, prec 0.0199626, recall 0.64
2017-12-10T03:23:50.724925: step 65, loss 3.14448, acc 0.375, prec 0.020073, recall 0.647059
2017-12-10T03:23:50.990948: step 66, loss 1.72038, acc 0.5625, prec 0.0203228, recall 0.653846
2017-12-10T03:23:51.253055: step 67, loss 1.6535, acc 0.703125, prec 0.0206734, recall 0.660377
2017-12-10T03:23:51.511042: step 68, loss 12.4022, acc 0.703125, prec 0.0215995, recall 0.660714
2017-12-10T03:23:51.772223: step 69, loss 1.11203, acc 0.78125, prec 0.0214244, recall 0.660714
2017-12-10T03:23:52.044547: step 70, loss 1.08999, acc 0.640625, prec 0.0211429, recall 0.660714
2017-12-10T03:23:52.308858: step 71, loss 10.189, acc 0.796875, prec 0.0209989, recall 0.649123
2017-12-10T03:23:52.574156: step 72, loss 18.5072, acc 0.765625, prec 0.0208333, recall 0.637931
2017-12-10T03:23:52.842022: step 73, loss 17.6337, acc 0.703125, prec 0.0211699, recall 0.633333
2017-12-10T03:23:53.100833: step 74, loss 2.02797, acc 0.59375, prec 0.021405, recall 0.639344
2017-12-10T03:23:53.368225: step 75, loss 3.89895, acc 0.4375, prec 0.0215169, recall 0.645161
2017-12-10T03:23:53.630679: step 76, loss 6.40712, acc 0.53125, prec 0.0217046, recall 0.640625
2017-12-10T03:23:53.892866: step 77, loss 6.63949, acc 0.375, prec 0.0212656, recall 0.630769
2017-12-10T03:23:54.154408: step 78, loss 5.27125, acc 0.234375, prec 0.0212336, recall 0.636364
2017-12-10T03:23:54.414547: step 79, loss 6.02314, acc 0.078125, prec 0.0210991, recall 0.641791
2017-12-10T03:23:54.680444: step 80, loss 5.94587, acc 0.25, prec 0.0215517, recall 0.652174
2017-12-10T03:23:54.937348: step 81, loss 13.1078, acc 0.140625, prec 0.0210084, recall 0.642857
2017-12-10T03:23:55.203329: step 82, loss 7.76596, acc 0.21875, prec 0.0205292, recall 0.642857
2017-12-10T03:23:55.459954: step 83, loss 6.07728, acc 0.171875, prec 0.0200445, recall 0.642857
2017-12-10T03:23:55.724682: step 84, loss 5.58193, acc 0.296875, prec 0.0209333, recall 0.657534
2017-12-10T03:23:55.989028: step 85, loss 4.81814, acc 0.328125, prec 0.0205479, recall 0.657534
2017-12-10T03:23:56.248997: step 86, loss 4.11495, acc 0.3125, prec 0.0201681, recall 0.657534
2017-12-10T03:23:56.517196: step 87, loss 3.52324, acc 0.375, prec 0.0198347, recall 0.657534
2017-12-10T03:23:56.793536: step 88, loss 3.6723, acc 0.4375, prec 0.019943, recall 0.662162
2017-12-10T03:23:57.058127: step 89, loss 1.9395, acc 0.578125, prec 0.0201207, recall 0.666667
2017-12-10T03:23:57.322061: step 90, loss 1.88501, acc 0.671875, prec 0.0207337, recall 0.675325
2017-12-10T03:23:57.591572: step 91, loss 0.921675, acc 0.8125, prec 0.0206349, recall 0.675325
2017-12-10T03:23:57.854405: step 92, loss 0.914649, acc 0.765625, prec 0.0205128, recall 0.675325
2017-12-10T03:23:58.116039: step 93, loss 5.29219, acc 0.859375, prec 0.0204483, recall 0.666667
2017-12-10T03:23:58.381047: step 94, loss 0.383899, acc 0.890625, prec 0.0203922, recall 0.666667
2017-12-10T03:23:58.641000: step 95, loss 18.6096, acc 0.8125, prec 0.0203046, recall 0.658228
2017-12-10T03:23:58.908331: step 96, loss 10.6362, acc 0.9375, prec 0.0202808, recall 0.65
2017-12-10T03:23:59.172992: step 97, loss 0.437672, acc 0.875, prec 0.0202177, recall 0.65
2017-12-10T03:23:59.439295: step 98, loss 0.854362, acc 0.796875, prec 0.0201161, recall 0.65
2017-12-10T03:23:59.700144: step 99, loss 0.527373, acc 0.875, prec 0.020054, recall 0.65
2017-12-10T03:23:59.961493: step 100, loss 29.6219, acc 0.796875, prec 0.0199616, recall 0.641975
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-100

2017-12-10T03:24:01.341182: step 101, loss 0.579037, acc 0.796875, prec 0.0198625, recall 0.641975
2017-12-10T03:24:01.604321: step 102, loss 16.5081, acc 0.765625, prec 0.0197568, recall 0.634146
2017-12-10T03:24:01.863565: step 103, loss 1.67167, acc 0.6875, prec 0.0203466, recall 0.642857
2017-12-10T03:24:02.125773: step 104, loss 22.3903, acc 0.578125, prec 0.0205224, recall 0.632184
2017-12-10T03:24:02.390120: step 105, loss 3.09059, acc 0.4375, prec 0.0202504, recall 0.632184
2017-12-10T03:24:02.646148: step 106, loss 4.87106, acc 0.234375, prec 0.0202458, recall 0.636364
2017-12-10T03:24:02.904850: step 107, loss 4.11712, acc 0.234375, prec 0.0198934, recall 0.636364
2017-12-10T03:24:03.160604: step 108, loss 5.82433, acc 0.25, prec 0.0202443, recall 0.644444
2017-12-10T03:24:03.420132: step 109, loss 7.05359, acc 0.1875, prec 0.0198834, recall 0.644444
2017-12-10T03:24:03.678813: step 110, loss 7.26631, acc 0.140625, prec 0.0195155, recall 0.644444
2017-12-10T03:24:03.940746: step 111, loss 5.09798, acc 0.296875, prec 0.0192244, recall 0.644444
2017-12-10T03:24:04.196024: step 112, loss 4.2087, acc 0.28125, prec 0.0192559, recall 0.648352
2017-12-10T03:24:04.455107: step 113, loss 2.85085, acc 0.421875, prec 0.0193424, recall 0.652174
2017-12-10T03:24:04.712299: step 114, loss 3.05246, acc 0.421875, prec 0.0194268, recall 0.655914
2017-12-10T03:24:04.970873: step 115, loss 3.22299, acc 0.515625, prec 0.0201638, recall 0.666667
2017-12-10T03:24:05.236515: step 116, loss 1.45163, acc 0.703125, prec 0.0203507, recall 0.670103
2017-12-10T03:24:05.500577: step 117, loss 14.8648, acc 0.734375, prec 0.0202492, recall 0.663265
2017-12-10T03:24:05.769098: step 118, loss 11.9321, acc 0.796875, prec 0.0204778, recall 0.66
2017-12-10T03:24:06.032180: step 119, loss 1.59514, acc 0.703125, prec 0.0203578, recall 0.66
2017-12-10T03:24:06.299725: step 120, loss 1.20315, acc 0.75, prec 0.0202578, recall 0.66
2017-12-10T03:24:06.566953: step 121, loss 1.41212, acc 0.6875, prec 0.0201342, recall 0.66
2017-12-10T03:24:06.826804: step 122, loss 0.940471, acc 0.71875, prec 0.0203215, recall 0.663366
2017-12-10T03:24:07.087989: step 123, loss 1.36404, acc 0.734375, prec 0.0208082, recall 0.669903
2017-12-10T03:24:07.347588: step 124, loss 1.01939, acc 0.828125, prec 0.0207394, recall 0.669903
2017-12-10T03:24:07.613297: step 125, loss 0.991742, acc 0.828125, prec 0.0212575, recall 0.67619
2017-12-10T03:24:07.881270: step 126, loss 1.215, acc 0.703125, prec 0.0211372, recall 0.67619
2017-12-10T03:24:08.153041: step 127, loss 0.689003, acc 0.78125, prec 0.0210495, recall 0.67619
2017-12-10T03:24:08.425322: step 128, loss 12.869, acc 0.78125, prec 0.0218353, recall 0.678899
2017-12-10T03:24:08.690267: step 129, loss 0.995437, acc 0.75, prec 0.0217327, recall 0.678899
2017-12-10T03:24:08.958238: step 130, loss 0.969967, acc 0.78125, prec 0.0216438, recall 0.678899
2017-12-10T03:24:09.222384: step 131, loss 1.88725, acc 0.65625, prec 0.0215054, recall 0.678899
2017-12-10T03:24:09.478781: step 132, loss 1.91995, acc 0.5625, prec 0.0213318, recall 0.678899
2017-12-10T03:24:09.745650: step 133, loss 1.69921, acc 0.6875, prec 0.0212095, recall 0.678899
2017-12-10T03:24:10.011982: step 134, loss 2.48301, acc 0.640625, prec 0.0210706, recall 0.678899
2017-12-10T03:24:10.284948: step 135, loss 2.60991, acc 0.6875, prec 0.0209572, recall 0.672727
2017-12-10T03:24:10.551410: step 136, loss 0.865196, acc 0.78125, prec 0.0208745, recall 0.672727
2017-12-10T03:24:10.811303: step 137, loss 21.2393, acc 0.71875, prec 0.0210497, recall 0.669643
2017-12-10T03:24:11.080293: step 138, loss 5.08098, acc 0.765625, prec 0.0215204, recall 0.663793
2017-12-10T03:24:11.351903: step 139, loss 1.68879, acc 0.65625, prec 0.0213889, recall 0.663793
2017-12-10T03:24:11.623468: step 140, loss 2.02421, acc 0.5625, prec 0.0212238, recall 0.663793
2017-12-10T03:24:11.890369: step 141, loss 1.70579, acc 0.640625, prec 0.0210901, recall 0.663793
2017-12-10T03:24:12.156660: step 142, loss 3.13471, acc 0.46875, prec 0.0208955, recall 0.663793
2017-12-10T03:24:12.424276: step 143, loss 3.02607, acc 0.484375, prec 0.0212366, recall 0.669492
2017-12-10T03:24:12.688288: step 144, loss 2.55027, acc 0.546875, prec 0.0213333, recall 0.672269
2017-12-10T03:24:12.948997: step 145, loss 2.0806, acc 0.5625, prec 0.0214342, recall 0.675
2017-12-10T03:24:13.214406: step 146, loss 2.12872, acc 0.546875, prec 0.021528, recall 0.677686
2017-12-10T03:24:13.475394: step 147, loss 2.55449, acc 0.59375, prec 0.0216371, recall 0.680328
2017-12-10T03:24:13.741693: step 148, loss 1.06646, acc 0.71875, prec 0.0215361, recall 0.680328
2017-12-10T03:24:14.011804: step 149, loss 11.7652, acc 0.671875, prec 0.0214249, recall 0.674797
2017-12-10T03:24:14.278971: step 150, loss 0.95178, acc 0.828125, prec 0.0213642, recall 0.674797
2017-12-10T03:24:14.541678: step 151, loss 0.754613, acc 0.8125, prec 0.0212984, recall 0.674797
2017-12-10T03:24:14.805561: step 152, loss 1.81609, acc 0.671875, prec 0.021434, recall 0.677419
2017-12-10T03:24:15.072109: step 153, loss 0.929792, acc 0.71875, prec 0.021336, recall 0.677419
2017-12-10T03:24:15.342310: step 154, loss 6.09089, acc 0.703125, prec 0.0212389, recall 0.672
2017-12-10T03:24:15.607495: step 155, loss 1.19524, acc 0.71875, prec 0.0211427, recall 0.672
2017-12-10T03:24:15.878090: step 156, loss 1.35317, acc 0.78125, prec 0.0210685, recall 0.672
2017-12-10T03:24:16.142473: step 157, loss 0.787251, acc 0.8125, prec 0.0210053, recall 0.672
2017-12-10T03:24:16.412121: step 158, loss 1.09638, acc 0.703125, prec 0.0209059, recall 0.672
2017-12-10T03:24:16.673673: step 159, loss 1.21714, acc 0.71875, prec 0.0208127, recall 0.672
2017-12-10T03:24:16.940476: step 160, loss 0.67162, acc 0.84375, prec 0.0207612, recall 0.672
2017-12-10T03:24:17.203537: step 161, loss 1.2105, acc 0.765625, prec 0.0206846, recall 0.672
2017-12-10T03:24:17.474523: step 162, loss 1.14573, acc 0.8125, prec 0.0206236, recall 0.672
2017-12-10T03:24:17.735356: step 163, loss 0.494709, acc 0.875, prec 0.0205832, recall 0.672
2017-12-10T03:24:18.000990: step 164, loss 0.37306, acc 0.875, prec 0.0205429, recall 0.672
2017-12-10T03:24:18.268251: step 165, loss 0.413576, acc 0.859375, prec 0.0204978, recall 0.672
2017-12-10T03:24:18.534157: step 166, loss 0.357461, acc 0.90625, prec 0.0204678, recall 0.672
2017-12-10T03:24:18.803533: step 167, loss 8.39847, acc 0.875, prec 0.020438, recall 0.661417
2017-12-10T03:24:19.069512: step 168, loss 0.508617, acc 0.859375, prec 0.0203933, recall 0.661417
2017-12-10T03:24:19.332841: step 169, loss 0.387075, acc 0.890625, prec 0.0203587, recall 0.661417
2017-12-10T03:24:19.593345: step 170, loss 2.0826, acc 0.859375, prec 0.0205562, recall 0.658915
2017-12-10T03:24:19.863252: step 171, loss 2.08676, acc 0.84375, prec 0.0207479, recall 0.656489
2017-12-10T03:24:20.132915: step 172, loss 0.80136, acc 0.828125, prec 0.0209286, recall 0.659091
2017-12-10T03:24:20.407450: step 173, loss 0.570975, acc 0.828125, prec 0.0208733, recall 0.659091
2017-12-10T03:24:20.668403: step 174, loss 1.19402, acc 0.78125, prec 0.0208034, recall 0.659091
2017-12-10T03:24:20.924842: step 175, loss 8.92037, acc 0.75, prec 0.0209624, recall 0.656716
2017-12-10T03:24:21.188804: step 176, loss 2.5409, acc 0.59375, prec 0.0215283, recall 0.664234
2017-12-10T03:24:21.454475: step 177, loss 2.1628, acc 0.625, prec 0.0214067, recall 0.664234
2017-12-10T03:24:21.715113: step 178, loss 2.74982, acc 0.5, prec 0.0217036, recall 0.669065
2017-12-10T03:24:21.987748: step 179, loss 3.02992, acc 0.390625, prec 0.0215079, recall 0.669065
2017-12-10T03:24:22.246506: step 180, loss 3.53114, acc 0.390625, prec 0.0215399, recall 0.671429
2017-12-10T03:24:22.508041: step 181, loss 2.93828, acc 0.53125, prec 0.021838, recall 0.676056
2017-12-10T03:24:22.770245: step 182, loss 3.99577, acc 0.53125, prec 0.0221319, recall 0.680556
2017-12-10T03:24:23.033345: step 183, loss 3.43282, acc 0.453125, prec 0.0219583, recall 0.680556
2017-12-10T03:24:23.297579: step 184, loss 4.17755, acc 0.53125, prec 0.0222469, recall 0.684932
2017-12-10T03:24:23.561098: step 185, loss 9.4493, acc 0.5, prec 0.0223106, recall 0.682432
2017-12-10T03:24:23.821920: step 186, loss 2.68805, acc 0.59375, prec 0.0223979, recall 0.684564
2017-12-10T03:24:24.083652: step 187, loss 2.93312, acc 0.546875, prec 0.0224695, recall 0.686667
2017-12-10T03:24:24.348371: step 188, loss 1.9187, acc 0.703125, prec 0.0228013, recall 0.690789
2017-12-10T03:24:24.617451: step 189, loss 7.53128, acc 0.59375, prec 0.0226831, recall 0.681818
2017-12-10T03:24:24.881418: step 190, loss 1.57826, acc 0.65625, prec 0.0227859, recall 0.683871
2017-12-10T03:24:25.147145: step 191, loss 2.57206, acc 0.625, prec 0.0230868, recall 0.687898
2017-12-10T03:24:25.409952: step 192, loss 2.90809, acc 0.546875, prec 0.0229446, recall 0.687898
2017-12-10T03:24:25.667887: step 193, loss 3.41506, acc 0.515625, prec 0.0234128, recall 0.69375
2017-12-10T03:24:25.927349: step 194, loss 1.51937, acc 0.65625, prec 0.0233046, recall 0.69375
2017-12-10T03:24:26.190907: step 195, loss 4.46824, acc 0.578125, prec 0.023382, recall 0.691358
2017-12-10T03:24:26.450758: step 196, loss 1.82275, acc 0.640625, prec 0.0232703, recall 0.691358
2017-12-10T03:24:26.729511: step 197, loss 2.1674, acc 0.578125, prec 0.0231405, recall 0.691358
2017-12-10T03:24:26.992013: step 198, loss 1.25696, acc 0.765625, prec 0.0232702, recall 0.693252
2017-12-10T03:24:27.259575: step 199, loss 4.47618, acc 0.59375, prec 0.023151, recall 0.689024
2017-12-10T03:24:27.519618: step 200, loss 1.9827, acc 0.625, prec 0.0232369, recall 0.690909
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-200

2017-12-10T03:24:28.803346: step 201, loss 2.36095, acc 0.578125, prec 0.0231097, recall 0.690909
2017-12-10T03:24:29.066999: step 202, loss 6.27124, acc 0.546875, prec 0.0231761, recall 0.688623
2017-12-10T03:24:29.332690: step 203, loss 3.09151, acc 0.546875, prec 0.0230415, recall 0.688623
2017-12-10T03:24:29.591877: step 204, loss 1.40168, acc 0.765625, prec 0.0229724, recall 0.688623
2017-12-10T03:24:29.848440: step 205, loss 11.3126, acc 0.53125, prec 0.0230387, recall 0.682353
2017-12-10T03:24:30.121598: step 206, loss 3.63813, acc 0.421875, prec 0.0228707, recall 0.682353
2017-12-10T03:24:30.389408: step 207, loss 2.99462, acc 0.421875, prec 0.022705, recall 0.682353
2017-12-10T03:24:30.655350: step 208, loss 3.14753, acc 0.515625, prec 0.0225681, recall 0.682353
2017-12-10T03:24:30.919580: step 209, loss 3.89147, acc 0.5, prec 0.0228063, recall 0.686047
2017-12-10T03:24:31.195119: step 210, loss 4.44429, acc 0.453125, prec 0.0228407, recall 0.687861
2017-12-10T03:24:31.452992: step 211, loss 5.08869, acc 0.46875, prec 0.0230696, recall 0.6875
2017-12-10T03:24:31.712525: step 212, loss 3.99987, acc 0.390625, prec 0.0228993, recall 0.6875
2017-12-10T03:24:31.972683: step 213, loss 5.84154, acc 0.46875, prec 0.0227572, recall 0.683616
2017-12-10T03:24:32.231387: step 214, loss 3.9912, acc 0.3125, prec 0.0225704, recall 0.683616
2017-12-10T03:24:32.494613: step 215, loss 3.31557, acc 0.4375, prec 0.0224199, recall 0.683616
2017-12-10T03:24:32.760425: step 216, loss 4.09818, acc 0.359375, prec 0.0226103, recall 0.687151
2017-12-10T03:24:33.032798: step 217, loss 3.34564, acc 0.453125, prec 0.0230011, recall 0.692308
2017-12-10T03:24:33.292615: step 218, loss 2.95635, acc 0.5, prec 0.0230448, recall 0.693989
2017-12-10T03:24:33.556600: step 219, loss 1.85093, acc 0.671875, prec 0.0231339, recall 0.695652
2017-12-10T03:24:33.817844: step 220, loss 1.61544, acc 0.71875, prec 0.0230589, recall 0.695652
2017-12-10T03:24:34.076125: step 221, loss 1.87875, acc 0.703125, prec 0.0233309, recall 0.698925
2017-12-10T03:24:34.341200: step 222, loss 13.4983, acc 0.71875, prec 0.02326, recall 0.695187
2017-12-10T03:24:34.609469: step 223, loss 1.28099, acc 0.6875, prec 0.023177, recall 0.695187
2017-12-10T03:24:34.869641: step 224, loss 1.2819, acc 0.71875, prec 0.0231029, recall 0.695187
2017-12-10T03:24:35.143449: step 225, loss 0.76663, acc 0.859375, prec 0.0234126, recall 0.698413
2017-12-10T03:24:35.407866: step 226, loss 0.642063, acc 0.84375, prec 0.0233711, recall 0.698413
2017-12-10T03:24:35.671686: step 227, loss 0.640251, acc 0.859375, prec 0.0233339, recall 0.698413
2017-12-10T03:24:35.935758: step 228, loss 24.1544, acc 0.875, prec 0.0234816, recall 0.692708
2017-12-10T03:24:36.205828: step 229, loss 22.6055, acc 0.75, prec 0.0234196, recall 0.689119
2017-12-10T03:24:36.473660: step 230, loss 1.52082, acc 0.703125, prec 0.0233415, recall 0.689119
2017-12-10T03:24:36.731568: step 231, loss 2.26458, acc 0.671875, prec 0.0234266, recall 0.690722
2017-12-10T03:24:36.997730: step 232, loss 3.27023, acc 0.515625, prec 0.0233003, recall 0.690722
2017-12-10T03:24:37.270556: step 233, loss 2.98142, acc 0.5625, prec 0.0233564, recall 0.692308
2017-12-10T03:24:37.538133: step 234, loss 3.88799, acc 0.40625, prec 0.0232038, recall 0.692308
2017-12-10T03:24:37.798462: step 235, loss 2.84131, acc 0.53125, prec 0.0232518, recall 0.693878
2017-12-10T03:24:38.059017: step 236, loss 2.40991, acc 0.5, prec 0.0234574, recall 0.69697
2017-12-10T03:24:38.326674: step 237, loss 2.96471, acc 0.546875, prec 0.0233424, recall 0.69697
2017-12-10T03:24:38.587801: step 238, loss 3.12926, acc 0.4375, prec 0.0236935, recall 0.701493
2017-12-10T03:24:38.850555: step 239, loss 3.26961, acc 0.421875, prec 0.0235471, recall 0.701493
2017-12-10T03:24:39.108372: step 240, loss 2.42168, acc 0.59375, prec 0.0236076, recall 0.70297
2017-12-10T03:24:39.372977: step 241, loss 2.50812, acc 0.546875, prec 0.0234944, recall 0.70297
2017-12-10T03:24:39.641273: step 242, loss 1.34588, acc 0.71875, prec 0.0237467, recall 0.705882
2017-12-10T03:24:39.908526: step 243, loss 1.50185, acc 0.75, prec 0.0238448, recall 0.707317
2017-12-10T03:24:40.178372: step 244, loss 1.10628, acc 0.8125, prec 0.0237978, recall 0.707317
2017-12-10T03:24:40.446011: step 245, loss 3.02076, acc 0.84375, prec 0.0239227, recall 0.705314
2017-12-10T03:24:40.715444: step 246, loss 0.178662, acc 0.921875, prec 0.0239031, recall 0.705314
2017-12-10T03:24:40.985684: step 247, loss 3.69375, acc 0.921875, prec 0.0240471, recall 0.703349
2017-12-10T03:24:41.252299: step 248, loss 46.1981, acc 0.796875, prec 0.0240078, recall 0.693396
2017-12-10T03:24:41.524386: step 249, loss 0.969735, acc 0.78125, prec 0.0239531, recall 0.693396
2017-12-10T03:24:41.795128: step 250, loss 4.44972, acc 0.65625, prec 0.0238714, recall 0.690141
2017-12-10T03:24:42.062616: step 251, loss 2.433, acc 0.625, prec 0.0237787, recall 0.690141
2017-12-10T03:24:42.330681: step 252, loss 2.5777, acc 0.546875, prec 0.0242961, recall 0.695853
2017-12-10T03:24:42.591385: step 253, loss 4.73521, acc 0.296875, prec 0.0244331, recall 0.69863
2017-12-10T03:24:42.852013: step 254, loss 4.45895, acc 0.3125, prec 0.0242626, recall 0.69863
2017-12-10T03:24:43.113196: step 255, loss 5.15805, acc 0.390625, prec 0.024421, recall 0.701357
2017-12-10T03:24:43.373422: step 256, loss 3.5788, acc 0.359375, prec 0.0247223, recall 0.705357
2017-12-10T03:24:43.629569: step 257, loss 4.89033, acc 0.40625, prec 0.0245761, recall 0.705357
2017-12-10T03:24:43.890862: step 258, loss 5.27867, acc 0.34375, prec 0.0245674, recall 0.706667
2017-12-10T03:24:44.151596: step 259, loss 4.12269, acc 0.40625, prec 0.0245738, recall 0.707965
2017-12-10T03:24:44.415771: step 260, loss 3.68752, acc 0.46875, prec 0.0244461, recall 0.707965
2017-12-10T03:24:44.675959: step 261, loss 2.65123, acc 0.484375, prec 0.0243235, recall 0.707965
2017-12-10T03:24:44.938318: step 262, loss 2.1269, acc 0.625, prec 0.0242351, recall 0.707965
2017-12-10T03:24:45.202064: step 263, loss 2.02063, acc 0.609375, prec 0.0241437, recall 0.707965
2017-12-10T03:24:45.469350: step 264, loss 1.50902, acc 0.71875, prec 0.0240783, recall 0.707965
2017-12-10T03:24:45.742653: step 265, loss 3.08477, acc 0.84375, prec 0.0240457, recall 0.704846
2017-12-10T03:24:46.008696: step 266, loss 0.523796, acc 0.84375, prec 0.0240096, recall 0.704846
2017-12-10T03:24:46.282351: step 267, loss 1.31665, acc 0.75, prec 0.0240982, recall 0.70614
2017-12-10T03:24:46.547831: step 268, loss 0.645852, acc 0.875, prec 0.0246526, recall 0.711207
2017-12-10T03:24:46.817887: step 269, loss 0.20381, acc 0.90625, prec 0.0246305, recall 0.711207
2017-12-10T03:24:47.079707: step 270, loss 0.458965, acc 0.890625, prec 0.0247503, recall 0.712446
2017-12-10T03:24:47.337977: step 271, loss 12.7989, acc 0.90625, prec 0.0248771, recall 0.710638
2017-12-10T03:24:47.611576: step 272, loss 8.42255, acc 0.90625, prec 0.0250037, recall 0.708861
2017-12-10T03:24:47.887700: step 273, loss 0.406656, acc 0.9375, prec 0.0251338, recall 0.710084
2017-12-10T03:24:48.151088: step 274, loss 13.9992, acc 0.796875, prec 0.0250891, recall 0.707113
2017-12-10T03:24:48.423283: step 275, loss 1.28849, acc 0.75, prec 0.0250296, recall 0.707113
2017-12-10T03:24:48.688986: step 276, loss 0.54767, acc 0.859375, prec 0.0251405, recall 0.708333
2017-12-10T03:24:48.952506: step 277, loss 1.1449, acc 0.828125, prec 0.0250997, recall 0.708333
2017-12-10T03:24:49.216079: step 278, loss 1.59526, acc 0.671875, prec 0.0251656, recall 0.709544
2017-12-10T03:24:49.484144: step 279, loss 20.8196, acc 0.703125, prec 0.0251028, recall 0.703704
2017-12-10T03:24:49.761794: step 280, loss 2.37198, acc 0.625, prec 0.0251572, recall 0.704918
2017-12-10T03:24:50.027940: step 281, loss 2.84684, acc 0.5625, prec 0.0250546, recall 0.704918
2017-12-10T03:24:50.289278: step 282, loss 14.334, acc 0.5, prec 0.024942, recall 0.702041
2017-12-10T03:24:50.557513: step 283, loss 4.41538, acc 0.46875, prec 0.025101, recall 0.704453
2017-12-10T03:24:50.820969: step 284, loss 4.22091, acc 0.484375, prec 0.025122, recall 0.705645
2017-12-10T03:24:51.083446: step 285, loss 4.21341, acc 0.34375, prec 0.0251106, recall 0.706827
2017-12-10T03:24:51.345790: step 286, loss 4.82925, acc 0.375, prec 0.0249681, recall 0.706827
2017-12-10T03:24:51.604591: step 287, loss 4.34023, acc 0.390625, prec 0.0248307, recall 0.706827
2017-12-10T03:24:51.869097: step 288, loss 4.10147, acc 0.375, prec 0.0248282, recall 0.708
2017-12-10T03:24:52.128625: step 289, loss 3.64018, acc 0.40625, prec 0.0248326, recall 0.709163
2017-12-10T03:24:52.389381: step 290, loss 4.37652, acc 0.421875, prec 0.0249757, recall 0.711462
2017-12-10T03:24:52.658510: step 291, loss 3.73851, acc 0.453125, prec 0.0249896, recall 0.712598
2017-12-10T03:24:52.923808: step 292, loss 4.08048, acc 0.46875, prec 0.0250069, recall 0.713726
2017-12-10T03:24:53.192737: step 293, loss 3.04661, acc 0.546875, prec 0.0250411, recall 0.714844
2017-12-10T03:24:53.453197: step 294, loss 3.14949, acc 0.71875, prec 0.0249829, recall 0.712062
2017-12-10T03:24:53.727669: step 295, loss 7.44088, acc 0.71875, prec 0.0250579, recall 0.710425
2017-12-10T03:24:53.999090: step 296, loss 1.24143, acc 0.703125, prec 0.0251256, recall 0.711538
2017-12-10T03:24:54.263703: step 297, loss 0.744057, acc 0.8125, prec 0.0252169, recall 0.712644
2017-12-10T03:24:54.526469: step 298, loss 1.05076, acc 0.8125, prec 0.025176, recall 0.712644
2017-12-10T03:24:54.788329: step 299, loss 4.4507, acc 0.828125, prec 0.0252737, recall 0.711027
2017-12-10T03:24:55.054726: step 300, loss 10.2474, acc 0.625, prec 0.0251954, recall 0.708333

Evaluation:
2017-12-10T03:25:02.694420: step 300, loss 2.14602, acc 0.800642, prec 0.0311199, recall 0.720482

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-300

2017-12-10T03:25:03.988109: step 301, loss 1.35052, acc 0.71875, prec 0.0311624, recall 0.721154
2017-12-10T03:25:04.250940: step 302, loss 2.30962, acc 0.8125, prec 0.0311268, recall 0.719424
2017-12-10T03:25:04.519368: step 303, loss 1.4883, acc 0.6875, prec 0.0311626, recall 0.720096
2017-12-10T03:25:04.782116: step 304, loss 2.14823, acc 0.703125, prec 0.0312016, recall 0.720764
2017-12-10T03:25:05.046258: step 305, loss 1.63317, acc 0.75, prec 0.0311501, recall 0.720764
2017-12-10T03:25:05.314927: step 306, loss 2.74652, acc 0.59375, prec 0.0312661, recall 0.72209
2017-12-10T03:25:05.584574: step 307, loss 3.42947, acc 0.5625, prec 0.0312756, recall 0.722749
2017-12-10T03:25:05.855220: step 308, loss 7.31943, acc 0.5, prec 0.0311765, recall 0.72104
2017-12-10T03:25:06.124963: step 309, loss 2.7041, acc 0.609375, prec 0.0311958, recall 0.721698
2017-12-10T03:25:06.387021: step 310, loss 3.01758, acc 0.5625, prec 0.031107, recall 0.721698
2017-12-10T03:25:06.649254: step 311, loss 4.48973, acc 0.515625, prec 0.0310125, recall 0.72
2017-12-10T03:25:06.919074: step 312, loss 2.2598, acc 0.515625, prec 0.0310132, recall 0.720657
2017-12-10T03:25:07.185357: step 313, loss 2.90821, acc 0.59375, prec 0.030932, recall 0.720657
2017-12-10T03:25:07.446335: step 314, loss 2.52748, acc 0.625, prec 0.0310522, recall 0.721963
2017-12-10T03:25:07.713996: step 315, loss 2.33229, acc 0.515625, prec 0.0311498, recall 0.723256
2017-12-10T03:25:07.972838: step 316, loss 5.69682, acc 0.625, prec 0.0311751, recall 0.722222
2017-12-10T03:25:08.236364: step 317, loss 2.12264, acc 0.59375, prec 0.0310943, recall 0.722222
2017-12-10T03:25:08.499013: step 318, loss 1.81414, acc 0.71875, prec 0.0312314, recall 0.723502
2017-12-10T03:25:08.772525: step 319, loss 1.95125, acc 0.625, prec 0.0312531, recall 0.724138
2017-12-10T03:25:09.037281: step 320, loss 2.52726, acc 0.609375, prec 0.0312716, recall 0.724771
2017-12-10T03:25:09.303187: step 321, loss 16.3784, acc 0.671875, prec 0.0312099, recall 0.723112
2017-12-10T03:25:09.576022: step 322, loss 1.77496, acc 0.734375, prec 0.031635, recall 0.726244
2017-12-10T03:25:09.836245: step 323, loss 2.01628, acc 0.59375, prec 0.0316493, recall 0.726862
2017-12-10T03:25:10.103657: step 324, loss 2.74474, acc 0.65625, prec 0.0315841, recall 0.725225
2017-12-10T03:25:10.375572: step 325, loss 2.60027, acc 0.59375, prec 0.0315038, recall 0.725225
2017-12-10T03:25:10.637250: step 326, loss 1.31583, acc 0.71875, prec 0.031543, recall 0.725843
2017-12-10T03:25:10.899721: step 327, loss 1.47424, acc 0.65625, prec 0.0315697, recall 0.726457
2017-12-10T03:25:11.167382: step 328, loss 2.37484, acc 0.765625, prec 0.0318062, recall 0.728285
2017-12-10T03:25:11.440938: step 329, loss 3.68672, acc 0.703125, prec 0.0319386, recall 0.727876
2017-12-10T03:25:11.718792: step 330, loss 1.54908, acc 0.703125, prec 0.0318798, recall 0.727876
2017-12-10T03:25:11.993396: step 331, loss 1.50407, acc 0.65625, prec 0.031812, recall 0.727876
2017-12-10T03:25:12.255824: step 332, loss 4.49211, acc 0.65625, prec 0.0317476, recall 0.726269
2017-12-10T03:25:12.524762: step 333, loss 2.84936, acc 0.59375, prec 0.0318545, recall 0.727473
2017-12-10T03:25:12.791575: step 334, loss 3.61692, acc 0.640625, prec 0.0317872, recall 0.725877
2017-12-10T03:25:13.052258: step 335, loss 5.79073, acc 0.46875, prec 0.0316868, recall 0.724289
2017-12-10T03:25:13.314378: step 336, loss 3.56777, acc 0.484375, prec 0.031587, recall 0.724289
2017-12-10T03:25:13.575264: step 337, loss 2.98264, acc 0.484375, prec 0.0315799, recall 0.724891
2017-12-10T03:25:13.843135: step 338, loss 3.21674, acc 0.484375, prec 0.0314811, recall 0.724891
2017-12-10T03:25:14.107320: step 339, loss 11.5729, acc 0.484375, prec 0.031752, recall 0.725702
2017-12-10T03:25:14.370751: step 340, loss 3.75047, acc 0.421875, prec 0.0317326, recall 0.726293
2017-12-10T03:25:14.643436: step 341, loss 4.16074, acc 0.40625, prec 0.0316194, recall 0.726293
2017-12-10T03:25:14.908114: step 342, loss 3.48014, acc 0.421875, prec 0.0316006, recall 0.726882
2017-12-10T03:25:15.173940: step 343, loss 3.70202, acc 0.421875, prec 0.0314917, recall 0.726882
2017-12-10T03:25:15.439658: step 344, loss 3.97303, acc 0.359375, prec 0.0313718, recall 0.726882
2017-12-10T03:25:15.699931: step 345, loss 2.90444, acc 0.421875, prec 0.0313541, recall 0.727468
2017-12-10T03:25:15.965333: step 346, loss 2.79574, acc 0.421875, prec 0.0314257, recall 0.728632
2017-12-10T03:25:16.224098: step 347, loss 2.41463, acc 0.609375, prec 0.0314425, recall 0.729211
2017-12-10T03:25:16.488537: step 348, loss 1.62655, acc 0.640625, prec 0.0313761, recall 0.729211
2017-12-10T03:25:16.751034: step 349, loss 0.633226, acc 0.796875, prec 0.0314275, recall 0.729787
2017-12-10T03:25:17.021531: step 350, loss 1.12807, acc 0.75, prec 0.0313815, recall 0.729787
2017-12-10T03:25:17.285860: step 351, loss 11.4263, acc 0.765625, prec 0.0314299, recall 0.728814
2017-12-10T03:25:17.551372: step 352, loss 0.208514, acc 0.90625, prec 0.0315011, recall 0.729387
2017-12-10T03:25:17.810829: step 353, loss 2.23173, acc 0.75, prec 0.0316317, recall 0.730526
2017-12-10T03:25:18.081942: step 354, loss 5.36172, acc 0.890625, prec 0.0316144, recall 0.728992
2017-12-10T03:25:18.349091: step 355, loss 1.34798, acc 0.671875, prec 0.0315541, recall 0.728992
2017-12-10T03:25:18.619962: step 356, loss 1.0913, acc 0.703125, prec 0.0314996, recall 0.728992
2017-12-10T03:25:18.881190: step 357, loss 0.895541, acc 0.8125, prec 0.0315532, recall 0.72956
2017-12-10T03:25:19.155404: step 358, loss 1.0077, acc 0.828125, prec 0.0315217, recall 0.72956
2017-12-10T03:25:19.422747: step 359, loss 1.89418, acc 0.671875, prec 0.0315495, recall 0.730126
2017-12-10T03:25:19.685752: step 360, loss 1.77684, acc 0.671875, prec 0.0314897, recall 0.730126
2017-12-10T03:25:19.945003: step 361, loss 1.18012, acc 0.65625, prec 0.0314273, recall 0.730126
2017-12-10T03:25:20.214777: step 362, loss 7.12036, acc 0.703125, prec 0.0315506, recall 0.72973
2017-12-10T03:25:20.479609: step 363, loss 1.06324, acc 0.734375, prec 0.0315024, recall 0.72973
2017-12-10T03:25:20.744320: step 364, loss 1.56132, acc 0.65625, prec 0.0315271, recall 0.73029
2017-12-10T03:25:21.014813: step 365, loss 1.5063, acc 0.6875, prec 0.0315573, recall 0.730849
2017-12-10T03:25:21.280234: step 366, loss 1.70657, acc 0.640625, prec 0.0314925, recall 0.730849
2017-12-10T03:25:21.543712: step 367, loss 21.8801, acc 0.671875, prec 0.0315283, recall 0.726899
2017-12-10T03:25:21.811107: step 368, loss 2.18267, acc 0.5625, prec 0.031622, recall 0.728016
2017-12-10T03:25:22.074618: step 369, loss 3.04026, acc 0.421875, prec 0.0316898, recall 0.729124
2017-12-10T03:25:22.331099: step 370, loss 3.76996, acc 0.40625, prec 0.0315836, recall 0.729124
2017-12-10T03:25:22.593843: step 371, loss 3.65973, acc 0.375, prec 0.0315577, recall 0.729675
2017-12-10T03:25:22.853980: step 372, loss 3.01109, acc 0.421875, prec 0.0316251, recall 0.730769
2017-12-10T03:25:23.129645: step 373, loss 3.97926, acc 0.34375, prec 0.0317627, recall 0.732394
2017-12-10T03:25:23.396466: step 374, loss 4.12676, acc 0.375, prec 0.0319889, recall 0.734531
2017-12-10T03:25:23.664864: step 375, loss 4.3208, acc 0.375, prec 0.0319619, recall 0.73506
2017-12-10T03:25:23.943468: step 376, loss 2.84987, acc 0.546875, prec 0.0321327, recall 0.736634
2017-12-10T03:25:24.207843: step 377, loss 2.94519, acc 0.453125, prec 0.0320358, recall 0.736634
2017-12-10T03:25:24.469012: step 378, loss 3.39427, acc 0.46875, prec 0.0320254, recall 0.737154
2017-12-10T03:25:24.728233: step 379, loss 1.729, acc 0.625, prec 0.0320425, recall 0.737673
2017-12-10T03:25:24.989463: step 380, loss 1.43016, acc 0.671875, prec 0.0319849, recall 0.737673
2017-12-10T03:25:25.252488: step 381, loss 0.826058, acc 0.765625, prec 0.031944, recall 0.737673
2017-12-10T03:25:25.520906: step 382, loss 5.00582, acc 0.796875, prec 0.0319939, recall 0.736739
2017-12-10T03:25:25.787512: step 383, loss 5.92276, acc 0.828125, prec 0.0319666, recall 0.735294
2017-12-10T03:25:26.058948: step 384, loss 0.445246, acc 0.828125, prec 0.0319366, recall 0.735294
2017-12-10T03:25:26.333593: step 385, loss 1.15153, acc 0.75, prec 0.0318932, recall 0.735294
2017-12-10T03:25:26.605529: step 386, loss 2.32213, acc 0.78125, prec 0.031858, recall 0.733855
2017-12-10T03:25:26.876345: step 387, loss 0.545885, acc 0.859375, prec 0.0318336, recall 0.733855
2017-12-10T03:25:27.144568: step 388, loss 18.9893, acc 0.84375, prec 0.0318093, recall 0.732422
2017-12-10T03:25:27.419927: step 389, loss 10.0115, acc 0.734375, prec 0.0319302, recall 0.732039
2017-12-10T03:25:27.685508: step 390, loss 1.54136, acc 0.65625, prec 0.0318708, recall 0.732039
2017-12-10T03:25:27.944812: step 391, loss 4.46354, acc 0.609375, prec 0.0318063, recall 0.73062
2017-12-10T03:25:28.212476: step 392, loss 3.03475, acc 0.65625, prec 0.0318289, recall 0.731141
2017-12-10T03:25:28.478361: step 393, loss 2.39971, acc 0.5625, prec 0.031754, recall 0.731141
2017-12-10T03:25:28.744371: step 394, loss 2.88912, acc 0.46875, prec 0.0316636, recall 0.731141
2017-12-10T03:25:29.006453: step 395, loss 3.46441, acc 0.453125, prec 0.0318136, recall 0.732692
2017-12-10T03:25:29.270885: step 396, loss 3.58257, acc 0.375, prec 0.0318689, recall 0.733716
2017-12-10T03:25:29.530864: step 397, loss 4.08351, acc 0.328125, prec 0.0317552, recall 0.733716
2017-12-10T03:25:29.798280: step 398, loss 4.48876, acc 0.34375, prec 0.031805, recall 0.734733
2017-12-10T03:25:30.063951: step 399, loss 7.95336, acc 0.421875, prec 0.0317905, recall 0.73384
2017-12-10T03:25:30.340316: step 400, loss 8.40279, acc 0.296875, prec 0.0316757, recall 0.732448
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-400

2017-12-10T03:25:31.536474: step 401, loss 4.71309, acc 0.328125, prec 0.0318018, recall 0.733962
2017-12-10T03:25:31.799478: step 402, loss 4.66459, acc 0.3125, prec 0.0316878, recall 0.733962
2017-12-10T03:25:32.064456: step 403, loss 3.19753, acc 0.40625, prec 0.0317473, recall 0.734962
2017-12-10T03:25:32.326404: step 404, loss 3.809, acc 0.375, prec 0.0317229, recall 0.73546
2017-12-10T03:25:32.587131: step 405, loss 3.37504, acc 0.375, prec 0.0316987, recall 0.735955
2017-12-10T03:25:32.849940: step 406, loss 2.66325, acc 0.484375, prec 0.0317703, recall 0.73694
2017-12-10T03:25:33.118710: step 407, loss 2.57305, acc 0.484375, prec 0.0316862, recall 0.73694
2017-12-10T03:25:33.378042: step 408, loss 2.78568, acc 0.671875, prec 0.0316354, recall 0.735568
2017-12-10T03:25:33.644459: step 409, loss 1.68776, acc 0.640625, prec 0.0317321, recall 0.736549
2017-12-10T03:25:33.907317: step 410, loss 8.86362, acc 0.6875, prec 0.0317612, recall 0.735675
2017-12-10T03:25:34.170745: step 411, loss 1.43836, acc 0.578125, prec 0.0316929, recall 0.735675
2017-12-10T03:25:34.431677: step 412, loss 2.38537, acc 0.6875, prec 0.0317965, recall 0.736648
2017-12-10T03:25:34.696515: step 413, loss 1.46321, acc 0.75, prec 0.0317561, recall 0.736648
2017-12-10T03:25:34.961291: step 414, loss 1.11343, acc 0.6875, prec 0.0317825, recall 0.737132
2017-12-10T03:25:35.229855: step 415, loss 1.31964, acc 0.734375, prec 0.0318164, recall 0.737615
2017-12-10T03:25:35.495232: step 416, loss 0.90798, acc 0.6875, prec 0.0318426, recall 0.738095
2017-12-10T03:25:35.757698: step 417, loss 1.06886, acc 0.796875, prec 0.0318863, recall 0.738574
2017-12-10T03:25:36.019222: step 418, loss 0.957225, acc 0.796875, prec 0.03193, recall 0.739051
2017-12-10T03:25:36.286731: step 419, loss 9.48746, acc 0.6875, prec 0.0319584, recall 0.738182
2017-12-10T03:25:36.549620: step 420, loss 0.799899, acc 0.78125, prec 0.0319233, recall 0.738182
2017-12-10T03:25:36.821215: step 421, loss 3.23302, acc 0.890625, prec 0.0319843, recall 0.737319
2017-12-10T03:25:37.085191: step 422, loss 1.33672, acc 0.6875, prec 0.0319341, recall 0.737319
2017-12-10T03:25:37.352805: step 423, loss 1.0568, acc 0.78125, prec 0.031899, recall 0.737319
2017-12-10T03:25:37.620677: step 424, loss 0.989012, acc 0.734375, prec 0.0318566, recall 0.737319
2017-12-10T03:25:37.892103: step 425, loss 1.0609, acc 0.75, prec 0.0318924, recall 0.737794
2017-12-10T03:25:38.151496: step 426, loss 0.957643, acc 0.75, prec 0.0318526, recall 0.737794
2017-12-10T03:25:38.413727: step 427, loss 1.60351, acc 0.671875, prec 0.0319514, recall 0.738739
2017-12-10T03:25:38.677480: step 428, loss 0.580521, acc 0.8125, prec 0.0319969, recall 0.739209
2017-12-10T03:25:38.939090: step 429, loss 4.54687, acc 0.84375, prec 0.0319745, recall 0.737881
2017-12-10T03:25:39.210705: step 430, loss 18.7655, acc 0.703125, prec 0.0320802, recall 0.7375
2017-12-10T03:25:39.479757: step 431, loss 1.61557, acc 0.640625, prec 0.0320229, recall 0.7375
2017-12-10T03:25:39.746939: step 432, loss 1.44862, acc 0.71875, prec 0.0319783, recall 0.7375
2017-12-10T03:25:40.007166: step 433, loss 1.72997, acc 0.578125, prec 0.0319116, recall 0.7375
2017-12-10T03:25:40.273245: step 434, loss 2.37936, acc 0.4375, prec 0.0319723, recall 0.738434
2017-12-10T03:25:40.533561: step 435, loss 2.49169, acc 0.46875, prec 0.0319631, recall 0.738899
2017-12-10T03:25:40.797934: step 436, loss 2.17948, acc 0.546875, prec 0.0318921, recall 0.738899
2017-12-10T03:25:41.060064: step 437, loss 3.83148, acc 0.453125, prec 0.0318832, recall 0.738053
2017-12-10T03:25:41.322348: step 438, loss 3.1821, acc 0.6875, prec 0.0319108, recall 0.737213
2017-12-10T03:25:41.597528: step 439, loss 1.91944, acc 0.53125, prec 0.0318379, recall 0.737213
2017-12-10T03:25:41.868406: step 440, loss 1.96635, acc 0.53125, prec 0.0317653, recall 0.737213
2017-12-10T03:25:42.131626: step 441, loss 1.72836, acc 0.625, prec 0.0317809, recall 0.737676
2017-12-10T03:25:42.395277: step 442, loss 2.36566, acc 0.5, prec 0.031704, recall 0.737676
2017-12-10T03:25:42.660427: step 443, loss 8.43109, acc 0.5625, prec 0.0317856, recall 0.737303
2017-12-10T03:25:42.924416: step 444, loss 5.50193, acc 0.546875, prec 0.0317185, recall 0.736014
2017-12-10T03:25:43.184670: step 445, loss 11.324, acc 0.46875, prec 0.0317126, recall 0.735192
2017-12-10T03:25:43.448933: step 446, loss 1.61682, acc 0.609375, prec 0.0316532, recall 0.735192
2017-12-10T03:25:43.707590: step 447, loss 3.08539, acc 0.546875, prec 0.0317294, recall 0.736111
2017-12-10T03:25:43.968622: step 448, loss 2.74824, acc 0.515625, prec 0.031656, recall 0.736111
2017-12-10T03:25:44.231645: step 449, loss 3.86069, acc 0.359375, prec 0.0315594, recall 0.736111
2017-12-10T03:25:44.490313: step 450, loss 3.09994, acc 0.4375, prec 0.031475, recall 0.736111
2017-12-10T03:25:44.751375: step 451, loss 2.86247, acc 0.359375, prec 0.0313795, recall 0.736111
2017-12-10T03:25:45.017383: step 452, loss 2.2494, acc 0.59375, prec 0.0314623, recall 0.737024
2017-12-10T03:25:45.280185: step 453, loss 1.96998, acc 0.546875, prec 0.0313951, recall 0.737024
2017-12-10T03:25:45.547329: step 454, loss 2.20187, acc 0.59375, prec 0.0314063, recall 0.737478
2017-12-10T03:25:45.811041: step 455, loss 1.04327, acc 0.703125, prec 0.0313625, recall 0.737478
2017-12-10T03:25:46.075391: step 456, loss 1.56914, acc 0.671875, prec 0.0313142, recall 0.737478
2017-12-10T03:25:46.336480: step 457, loss 1.03713, acc 0.671875, prec 0.031266, recall 0.737478
2017-12-10T03:25:46.603544: step 458, loss 9.25447, acc 0.765625, prec 0.0313757, recall 0.737113
2017-12-10T03:25:46.871286: step 459, loss 1.23034, acc 0.75, prec 0.0314098, recall 0.737564
2017-12-10T03:25:47.139857: step 460, loss 4.68809, acc 0.703125, prec 0.0314392, recall 0.736752
2017-12-10T03:25:47.405997: step 461, loss 0.934663, acc 0.75, prec 0.0314026, recall 0.736752
2017-12-10T03:25:47.669583: step 462, loss 1.0253, acc 0.734375, prec 0.0313637, recall 0.736752
2017-12-10T03:25:47.935971: step 463, loss 0.987606, acc 0.78125, prec 0.0314022, recall 0.737201
2017-12-10T03:25:48.206682: step 464, loss 1.49884, acc 0.765625, prec 0.0314383, recall 0.737649
2017-12-10T03:25:48.473210: step 465, loss 0.971394, acc 0.703125, prec 0.031395, recall 0.737649
2017-12-10T03:25:48.738980: step 466, loss 0.925607, acc 0.765625, prec 0.0313609, recall 0.737649
2017-12-10T03:25:49.007426: step 467, loss 11.8016, acc 0.84375, prec 0.0314807, recall 0.737288
2017-12-10T03:25:49.273570: step 468, loss 0.652608, acc 0.84375, prec 0.031598, recall 0.738176
2017-12-10T03:25:49.548772: step 469, loss 0.506575, acc 0.796875, prec 0.0315683, recall 0.738176
2017-12-10T03:25:49.812171: step 470, loss 6.15443, acc 0.75, prec 0.0315341, recall 0.736931
2017-12-10T03:25:50.082663: step 471, loss 1.23884, acc 0.75, prec 0.0315676, recall 0.737374
2017-12-10T03:25:50.347434: step 472, loss 1.19062, acc 0.703125, prec 0.0315244, recall 0.737374
2017-12-10T03:25:50.620030: step 473, loss 1.39513, acc 0.75, prec 0.0315578, recall 0.737815
2017-12-10T03:25:50.885593: step 474, loss 1.38636, acc 0.703125, prec 0.0315147, recall 0.737815
2017-12-10T03:25:51.154123: step 475, loss 4.27337, acc 0.625, prec 0.0314628, recall 0.736577
2017-12-10T03:25:51.425824: step 476, loss 1.67933, acc 0.53125, prec 0.0313953, recall 0.736577
2017-12-10T03:25:51.688267: step 477, loss 1.4047, acc 0.671875, prec 0.0313482, recall 0.736577
2017-12-10T03:25:51.949366: step 478, loss 2.18887, acc 0.578125, prec 0.0312879, recall 0.736577
2017-12-10T03:25:52.208782: step 479, loss 1.94812, acc 0.5, prec 0.0312167, recall 0.736577
2017-12-10T03:25:52.472725: step 480, loss 2.58819, acc 0.484375, prec 0.0313497, recall 0.737897
2017-12-10T03:25:52.735204: step 481, loss 1.17673, acc 0.65625, prec 0.0313695, recall 0.738333
2017-12-10T03:25:52.997586: step 482, loss 2.76197, acc 0.5625, prec 0.0314443, recall 0.739203
2017-12-10T03:25:53.259966: step 483, loss 7.13305, acc 0.65625, prec 0.0313977, recall 0.737977
2017-12-10T03:25:53.524853: step 484, loss 1.10957, acc 0.703125, prec 0.0313557, recall 0.737977
2017-12-10T03:25:53.790869: step 485, loss 5.35578, acc 0.578125, prec 0.0312984, recall 0.736755
2017-12-10T03:25:54.052924: step 486, loss 5.46779, acc 0.734375, prec 0.0313312, recall 0.735974
2017-12-10T03:25:54.320036: step 487, loss 3.04371, acc 0.625, prec 0.0314143, recall 0.736842
2017-12-10T03:25:54.581349: step 488, loss 2.88676, acc 0.40625, prec 0.0313986, recall 0.737274
2017-12-10T03:25:54.840916: step 489, loss 2.86687, acc 0.5, prec 0.0313961, recall 0.737705
2017-12-10T03:25:55.104617: step 490, loss 2.72038, acc 0.46875, prec 0.0313892, recall 0.738134
2017-12-10T03:25:55.363787: step 491, loss 3.28635, acc 0.40625, prec 0.0313064, recall 0.738134
2017-12-10T03:25:55.627499: step 492, loss 3.59866, acc 0.4375, prec 0.0314296, recall 0.739414
2017-12-10T03:25:55.887226: step 493, loss 2.70208, acc 0.515625, prec 0.0313623, recall 0.739414
2017-12-10T03:25:56.146529: step 494, loss 2.28206, acc 0.53125, prec 0.0312974, recall 0.739414
2017-12-10T03:25:56.407614: step 495, loss 3.34591, acc 0.515625, prec 0.0312973, recall 0.739837
2017-12-10T03:25:56.680676: step 496, loss 2.24624, acc 0.578125, prec 0.0313058, recall 0.74026
2017-12-10T03:25:56.908820: step 497, loss 21.1692, acc 0.529412, prec 0.0313228, recall 0.739482
2017-12-10T03:25:57.180833: step 498, loss 1.64783, acc 0.625, prec 0.0313377, recall 0.739903
2017-12-10T03:25:57.454033: step 499, loss 1.97344, acc 0.5625, prec 0.0312777, recall 0.739903
2017-12-10T03:25:57.723292: step 500, loss 1.99333, acc 0.578125, prec 0.0312862, recall 0.740323
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-500

2017-12-10T03:25:59.175229: step 501, loss 1.53542, acc 0.609375, prec 0.0312989, recall 0.740741
2017-12-10T03:25:59.436870: step 502, loss 1.54243, acc 0.65625, prec 0.0312521, recall 0.740741
2017-12-10T03:25:59.700426: step 503, loss 1.2154, acc 0.6875, prec 0.0312097, recall 0.740741
2017-12-10T03:25:59.971625: step 504, loss 1.15551, acc 0.703125, prec 0.0313008, recall 0.741573
2017-12-10T03:26:00.240396: step 505, loss 0.577014, acc 0.84375, prec 0.0312796, recall 0.741573
2017-12-10T03:26:00.508276: step 506, loss 1.07048, acc 0.765625, prec 0.0312479, recall 0.741573
2017-12-10T03:26:00.769181: step 507, loss 0.526882, acc 0.765625, prec 0.0312817, recall 0.741987
2017-12-10T03:26:01.033926: step 508, loss 0.405051, acc 0.890625, prec 0.0314631, recall 0.743222
2017-12-10T03:26:01.297566: step 509, loss 0.816411, acc 0.765625, prec 0.0314313, recall 0.743222
2017-12-10T03:26:01.557082: step 510, loss 9.72604, acc 0.921875, prec 0.0314902, recall 0.74127
2017-12-10T03:26:01.830412: step 511, loss 7.36616, acc 0.8125, prec 0.0315343, recall 0.739336
2017-12-10T03:26:02.101740: step 512, loss 7.4156, acc 0.796875, prec 0.031574, recall 0.738583
2017-12-10T03:26:02.370419: step 513, loss 0.985727, acc 0.65625, prec 0.0315924, recall 0.738994
2017-12-10T03:26:02.634861: step 514, loss 1.79076, acc 0.625, prec 0.0315415, recall 0.738994
2017-12-10T03:26:02.897029: step 515, loss 2.78227, acc 0.421875, prec 0.0314634, recall 0.738994
2017-12-10T03:26:03.880912: step 516, loss 2.46161, acc 0.5625, prec 0.0314692, recall 0.739403
2017-12-10T03:26:04.235812: step 517, loss 3.36389, acc 0.390625, prec 0.0315165, recall 0.740219
2017-12-10T03:26:04.964087: step 518, loss 3.94764, acc 0.375, prec 0.0314971, recall 0.740625
2017-12-10T03:26:05.704584: step 519, loss 4.03466, acc 0.328125, prec 0.0314715, recall 0.74103
2017-12-10T03:26:06.461782: step 520, loss 3.45632, acc 0.453125, prec 0.0313987, recall 0.74103
2017-12-10T03:26:07.588720: step 521, loss 4.26757, acc 0.359375, prec 0.0313139, recall 0.74103
2017-12-10T03:26:07.934521: step 522, loss 4.38951, acc 0.5, prec 0.0313137, recall 0.74028
2017-12-10T03:26:08.281702: step 523, loss 2.76634, acc 0.46875, prec 0.0312438, recall 0.74028
2017-12-10T03:26:08.562795: step 524, loss 2.12334, acc 0.609375, prec 0.0311927, recall 0.74028
2017-12-10T03:26:08.840496: step 525, loss 3.09491, acc 0.4375, prec 0.0311826, recall 0.740683
2017-12-10T03:26:09.104134: step 526, loss 2.07667, acc 0.59375, prec 0.0311297, recall 0.740683
2017-12-10T03:26:09.374070: step 527, loss 1.92112, acc 0.640625, prec 0.0311462, recall 0.741085
2017-12-10T03:26:09.636382: step 528, loss 1.41386, acc 0.703125, prec 0.0311076, recall 0.741085
2017-12-10T03:26:09.912626: step 529, loss 1.24678, acc 0.734375, prec 0.0310733, recall 0.741085
2017-12-10T03:26:10.176391: step 530, loss 1.20304, acc 0.734375, prec 0.0311019, recall 0.741486
2017-12-10T03:26:10.444252: step 531, loss 0.706809, acc 0.78125, prec 0.0310736, recall 0.741486
2017-12-10T03:26:10.716519: step 532, loss 0.506614, acc 0.8125, prec 0.0310495, recall 0.741486
2017-12-10T03:26:10.976376: step 533, loss 0.491708, acc 0.859375, prec 0.0310314, recall 0.741486
2017-12-10T03:26:11.249309: step 534, loss 0.279584, acc 0.9375, prec 0.0310861, recall 0.741886
2017-12-10T03:26:11.523987: step 535, loss 0.134501, acc 0.9375, prec 0.031078, recall 0.741886
2017-12-10T03:26:11.803368: step 536, loss 0.705817, acc 0.953125, prec 0.0312601, recall 0.743077
2017-12-10T03:26:12.069652: step 537, loss 0.177261, acc 0.921875, prec 0.0313127, recall 0.743472
2017-12-10T03:26:12.332751: step 538, loss 0.195275, acc 0.953125, prec 0.0313066, recall 0.743472
2017-12-10T03:26:12.598547: step 539, loss 0.136416, acc 0.921875, prec 0.0312965, recall 0.743472
2017-12-10T03:26:12.869710: step 540, loss 37.1866, acc 0.953125, prec 0.0312965, recall 0.740061
2017-12-10T03:26:13.144687: step 541, loss 0.463009, acc 0.953125, prec 0.031353, recall 0.740458
2017-12-10T03:26:13.422586: step 542, loss 17.0298, acc 0.890625, prec 0.0313429, recall 0.738204
2017-12-10T03:26:13.693049: step 543, loss 1.7133, acc 0.796875, prec 0.0313791, recall 0.738602
2017-12-10T03:26:13.963205: step 544, loss 0.722772, acc 0.8125, prec 0.0313548, recall 0.738602
2017-12-10T03:26:14.228073: step 545, loss 1.57341, acc 0.703125, prec 0.0313165, recall 0.738602
2017-12-10T03:26:14.497681: step 546, loss 2.11925, acc 0.59375, prec 0.0312641, recall 0.738602
2017-12-10T03:26:14.758565: step 547, loss 3.79659, acc 0.390625, prec 0.0311858, recall 0.738602
2017-12-10T03:26:15.017771: step 548, loss 3.06731, acc 0.421875, prec 0.031236, recall 0.739394
2017-12-10T03:26:15.279608: step 549, loss 3.75668, acc 0.390625, prec 0.0312201, recall 0.739788
2017-12-10T03:26:15.538512: step 550, loss 3.64565, acc 0.484375, prec 0.0312162, recall 0.740181
2017-12-10T03:26:15.797644: step 551, loss 4.67177, acc 0.375, prec 0.0312599, recall 0.740964
2017-12-10T03:26:16.063937: step 552, loss 3.80449, acc 0.359375, prec 0.0311787, recall 0.740964
2017-12-10T03:26:16.330389: step 553, loss 4.42156, acc 0.328125, prec 0.031094, recall 0.740964
2017-12-10T03:26:16.594469: step 554, loss 3.55782, acc 0.453125, prec 0.0310254, recall 0.740964
2017-12-10T03:26:16.852314: step 555, loss 3.20101, acc 0.40625, prec 0.0309512, recall 0.740964
2017-12-10T03:26:17.118599: step 556, loss 2.3155, acc 0.484375, prec 0.0309479, recall 0.741353
2017-12-10T03:26:17.377735: step 557, loss 2.55038, acc 0.578125, prec 0.0310777, recall 0.742515
2017-12-10T03:26:17.644927: step 558, loss 2.3254, acc 0.640625, prec 0.0310936, recall 0.7429
2017-12-10T03:26:17.909896: step 559, loss 1.35053, acc 0.703125, prec 0.0310567, recall 0.7429
2017-12-10T03:26:18.176202: step 560, loss 3.13644, acc 0.8125, prec 0.0311563, recall 0.74256
2017-12-10T03:26:18.450553: step 561, loss 0.561582, acc 0.765625, prec 0.0311272, recall 0.74256
2017-12-10T03:26:18.719444: step 562, loss 1.69273, acc 0.703125, prec 0.0310903, recall 0.74256
2017-12-10T03:26:18.990424: step 563, loss 1.33117, acc 0.765625, prec 0.0311216, recall 0.742942
2017-12-10T03:26:19.251750: step 564, loss 0.826955, acc 0.8125, prec 0.0312189, recall 0.743704
2017-12-10T03:26:19.513136: step 565, loss 0.886644, acc 0.8125, prec 0.0311956, recall 0.743704
2017-12-10T03:26:19.775373: step 566, loss 0.59317, acc 0.859375, prec 0.0311782, recall 0.743704
2017-12-10T03:26:20.040473: step 567, loss 0.525179, acc 0.84375, prec 0.0311588, recall 0.743704
2017-12-10T03:26:20.301553: step 568, loss 0.564799, acc 0.890625, prec 0.0312054, recall 0.744083
2017-12-10T03:26:20.562602: step 569, loss 0.645722, acc 0.84375, prec 0.0311861, recall 0.744083
2017-12-10T03:26:20.829319: step 570, loss 14.1688, acc 0.90625, prec 0.0311764, recall 0.742984
2017-12-10T03:26:21.094439: step 571, loss 4.58981, acc 0.859375, prec 0.0311609, recall 0.741888
2017-12-10T03:26:21.366002: step 572, loss 0.83384, acc 0.84375, prec 0.0312016, recall 0.742268
2017-12-10T03:26:21.628910: step 573, loss 0.666877, acc 0.875, prec 0.0312461, recall 0.742647
2017-12-10T03:26:21.896353: step 574, loss 0.245364, acc 0.921875, prec 0.0312964, recall 0.743025
2017-12-10T03:26:22.160826: step 575, loss 16.1538, acc 0.890625, prec 0.0312848, recall 0.741935
2017-12-10T03:26:22.425592: step 576, loss 0.811794, acc 0.71875, prec 0.0313098, recall 0.742313
2017-12-10T03:26:22.690325: step 577, loss 1.16397, acc 0.78125, prec 0.0313425, recall 0.74269
2017-12-10T03:26:22.964493: step 578, loss 0.957959, acc 0.734375, prec 0.0314291, recall 0.74344
2017-12-10T03:26:23.224482: step 579, loss 1.52212, acc 0.703125, prec 0.031452, recall 0.743814
2017-12-10T03:26:23.487728: step 580, loss 1.55729, acc 0.703125, prec 0.0314152, recall 0.743814
2017-12-10T03:26:23.750078: step 581, loss 1.4569, acc 0.765625, prec 0.0315647, recall 0.744928
2017-12-10T03:26:24.011397: step 582, loss 1.33151, acc 0.640625, prec 0.031639, recall 0.745665
2017-12-10T03:26:24.268394: step 583, loss 6.19791, acc 0.6875, prec 0.0316022, recall 0.744589
2017-12-10T03:26:24.532389: step 584, loss 1.6691, acc 0.609375, prec 0.0315538, recall 0.744589
2017-12-10T03:26:24.793576: step 585, loss 2.28969, acc 0.625, prec 0.0316259, recall 0.745324
2017-12-10T03:26:25.057905: step 586, loss 1.56623, acc 0.625, prec 0.0316977, recall 0.746055
2017-12-10T03:26:25.322452: step 587, loss 1.99775, acc 0.625, prec 0.0318281, recall 0.747143
2017-12-10T03:26:25.590757: step 588, loss 1.45562, acc 0.625, prec 0.0318994, recall 0.747863
2017-12-10T03:26:25.848522: step 589, loss 1.15474, acc 0.734375, prec 0.031984, recall 0.74858
2017-12-10T03:26:26.116236: step 590, loss 7.73937, acc 0.546875, prec 0.032047, recall 0.748232
2017-12-10T03:26:26.380566: step 591, loss 0.970263, acc 0.6875, prec 0.0320082, recall 0.748232
2017-12-10T03:26:26.651641: step 592, loss 1.98164, acc 0.5625, prec 0.0319541, recall 0.748232
2017-12-10T03:26:26.913892: step 593, loss 1.66805, acc 0.65625, prec 0.0319701, recall 0.748588
2017-12-10T03:26:27.182176: step 594, loss 1.40101, acc 0.671875, prec 0.031988, recall 0.748942
2017-12-10T03:26:27.443967: step 595, loss 1.05162, acc 0.734375, prec 0.0320135, recall 0.749296
2017-12-10T03:26:27.708870: step 596, loss 1.78415, acc 0.625, prec 0.0320255, recall 0.749648
2017-12-10T03:26:27.972436: step 597, loss 0.932266, acc 0.859375, prec 0.0320663, recall 0.75
2017-12-10T03:26:28.237492: step 598, loss 0.824184, acc 0.765625, prec 0.0320374, recall 0.75
2017-12-10T03:26:28.509501: step 599, loss 0.386417, acc 0.859375, prec 0.0320201, recall 0.75
2017-12-10T03:26:28.776242: step 600, loss 8.97991, acc 0.8125, prec 0.032057, recall 0.7493

Evaluation:
2017-12-10T03:26:36.331805: step 600, loss 1.88741, acc 0.864704, prec 0.0348927, recall 0.732948

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-600

2017-12-10T03:26:37.547254: step 601, loss 10.2135, acc 0.796875, prec 0.0348716, recall 0.731257
2017-12-10T03:26:37.810122: step 602, loss 4.03119, acc 0.71875, prec 0.034839, recall 0.730415
2017-12-10T03:26:38.072723: step 603, loss 0.997286, acc 0.71875, prec 0.0348046, recall 0.730415
2017-12-10T03:26:38.336746: step 604, loss 1.63142, acc 0.578125, prec 0.0347531, recall 0.730415
2017-12-10T03:26:38.600262: step 605, loss 2.54432, acc 0.5625, prec 0.0348055, recall 0.731034
2017-12-10T03:26:38.863536: step 606, loss 2.30935, acc 0.578125, prec 0.0347541, recall 0.731034
2017-12-10T03:26:39.129923: step 607, loss 1.63869, acc 0.546875, prec 0.0346991, recall 0.731034
2017-12-10T03:26:39.392718: step 608, loss 3.07727, acc 0.484375, prec 0.0346893, recall 0.731343
2017-12-10T03:26:39.652886: step 609, loss 1.76543, acc 0.578125, prec 0.0346909, recall 0.731651
2017-12-10T03:26:39.918001: step 610, loss 2.72652, acc 0.640625, prec 0.0347, recall 0.731959
2017-12-10T03:26:40.184315: step 611, loss 2.56363, acc 0.421875, prec 0.0346304, recall 0.731959
2017-12-10T03:26:40.446862: step 612, loss 1.85437, acc 0.59375, prec 0.0345817, recall 0.731959
2017-12-10T03:26:40.710949: step 613, loss 1.52314, acc 0.59375, prec 0.0345331, recall 0.731959
2017-12-10T03:26:40.972259: step 614, loss 0.946072, acc 0.6875, prec 0.0345479, recall 0.732265
2017-12-10T03:26:41.233242: step 615, loss 1.61976, acc 0.703125, prec 0.0346687, recall 0.733181
2017-12-10T03:26:41.502052: step 616, loss 0.68955, acc 0.78125, prec 0.0348505, recall 0.734393
2017-12-10T03:26:41.774697: step 617, loss 4.1614, acc 0.609375, prec 0.0348575, recall 0.733862
2017-12-10T03:26:42.047105: step 618, loss 8.45738, acc 0.796875, prec 0.0348868, recall 0.733333
2017-12-10T03:26:42.314809: step 619, loss 1.0177, acc 0.765625, prec 0.0348587, recall 0.733333
2017-12-10T03:26:42.576267: step 620, loss 1.0145, acc 0.71875, prec 0.0348251, recall 0.733333
2017-12-10T03:26:42.839599: step 621, loss 1.19523, acc 0.640625, prec 0.0348339, recall 0.733634
2017-12-10T03:26:43.105240: step 622, loss 4.26906, acc 0.703125, prec 0.034852, recall 0.733108
2017-12-10T03:26:43.371581: step 623, loss 1.24573, acc 0.6875, prec 0.0348147, recall 0.733108
2017-12-10T03:26:43.633688: step 624, loss 1.19468, acc 0.71875, prec 0.0347812, recall 0.733108
2017-12-10T03:26:43.894579: step 625, loss 1.71203, acc 0.640625, prec 0.0347385, recall 0.733108
2017-12-10T03:26:44.153195: step 626, loss 0.73444, acc 0.765625, prec 0.0347107, recall 0.733108
2017-12-10T03:26:44.416342: step 627, loss 3.08232, acc 0.71875, prec 0.0347821, recall 0.732884
2017-12-10T03:26:44.681388: step 628, loss 1.33053, acc 0.671875, prec 0.0347433, recall 0.732884
2017-12-10T03:26:44.945528: step 629, loss 1.72845, acc 0.59375, prec 0.0347979, recall 0.733483
2017-12-10T03:26:45.205678: step 630, loss 1.88874, acc 0.671875, prec 0.0348103, recall 0.733781
2017-12-10T03:26:45.467151: step 631, loss 0.877683, acc 0.765625, prec 0.0347826, recall 0.733781
2017-12-10T03:26:45.731646: step 632, loss 0.944476, acc 0.703125, prec 0.0347476, recall 0.733781
2017-12-10T03:26:45.996626: step 633, loss 1.58266, acc 0.6875, prec 0.0347108, recall 0.733781
2017-12-10T03:26:46.262644: step 634, loss 1.33081, acc 0.78125, prec 0.0347362, recall 0.734078
2017-12-10T03:26:46.526384: step 635, loss 0.826311, acc 0.78125, prec 0.0347105, recall 0.734078
2017-12-10T03:26:46.792794: step 636, loss 0.585633, acc 0.859375, prec 0.0348469, recall 0.734967
2017-12-10T03:26:47.056434: step 637, loss 1.65322, acc 0.71875, prec 0.0348647, recall 0.735261
2017-12-10T03:26:47.318658: step 638, loss 0.66067, acc 0.828125, prec 0.0348445, recall 0.735261
2017-12-10T03:26:47.584794: step 639, loss 0.420141, acc 0.84375, prec 0.034877, recall 0.735556
2017-12-10T03:26:47.843764: step 640, loss 1.70405, acc 0.828125, prec 0.0349076, recall 0.735849
2017-12-10T03:26:48.109742: step 641, loss 0.640963, acc 0.8125, prec 0.0348856, recall 0.735849
2017-12-10T03:26:48.376905: step 642, loss 6.87742, acc 0.875, prec 0.0349235, recall 0.735327
2017-12-10T03:26:48.654576: step 643, loss 0.753454, acc 0.875, prec 0.0349595, recall 0.735619
2017-12-10T03:26:48.927487: step 644, loss 0.792445, acc 0.828125, prec 0.03499, recall 0.735912
2017-12-10T03:26:49.187896: step 645, loss 0.883624, acc 0.828125, prec 0.0350205, recall 0.736203
2017-12-10T03:26:49.458778: step 646, loss 0.593021, acc 0.84375, prec 0.0350021, recall 0.736203
2017-12-10T03:26:49.719404: step 647, loss 0.854769, acc 0.734375, prec 0.0349709, recall 0.736203
2017-12-10T03:26:49.986543: step 648, loss 1.381, acc 0.796875, prec 0.0350482, recall 0.736784
2017-12-10T03:26:50.250655: step 649, loss 4.40132, acc 0.828125, prec 0.0350298, recall 0.735974
2017-12-10T03:26:50.511455: step 650, loss 0.525116, acc 0.796875, prec 0.035006, recall 0.735974
2017-12-10T03:26:50.779117: step 651, loss 0.918191, acc 0.734375, prec 0.0350254, recall 0.736264
2017-12-10T03:26:51.039238: step 652, loss 1.46564, acc 0.734375, prec 0.0351454, recall 0.73713
2017-12-10T03:26:51.315133: step 653, loss 1.07795, acc 0.734375, prec 0.0351646, recall 0.737418
2017-12-10T03:26:51.589279: step 654, loss 0.909874, acc 0.75, prec 0.0351353, recall 0.737418
2017-12-10T03:26:51.853628: step 655, loss 1.89144, acc 0.59375, prec 0.0351379, recall 0.737705
2017-12-10T03:26:52.117605: step 656, loss 0.980439, acc 0.78125, prec 0.0351625, recall 0.737991
2017-12-10T03:26:52.387333: step 657, loss 0.85538, acc 0.78125, prec 0.035137, recall 0.737991
2017-12-10T03:26:52.647627: step 658, loss 1.27183, acc 0.703125, prec 0.0351023, recall 0.737991
2017-12-10T03:26:52.911516: step 659, loss 1.22349, acc 0.734375, prec 0.0351214, recall 0.738277
2017-12-10T03:26:53.174106: step 660, loss 0.639013, acc 0.828125, prec 0.0352014, recall 0.738847
2017-12-10T03:26:53.447417: step 661, loss 0.813537, acc 0.765625, prec 0.0351741, recall 0.738847
2017-12-10T03:26:53.714935: step 662, loss 0.229134, acc 0.90625, prec 0.0351631, recall 0.738847
2017-12-10T03:26:53.976481: step 663, loss 0.63524, acc 0.796875, prec 0.0351395, recall 0.738847
2017-12-10T03:26:54.240911: step 664, loss 0.57515, acc 0.859375, prec 0.035173, recall 0.73913
2017-12-10T03:26:54.502235: step 665, loss 0.372948, acc 0.890625, prec 0.0351603, recall 0.73913
2017-12-10T03:26:54.769735: step 666, loss 4.07005, acc 0.875, prec 0.0351476, recall 0.738328
2017-12-10T03:26:55.033374: step 667, loss 1.21729, acc 0.9375, prec 0.03524, recall 0.738895
2017-12-10T03:26:55.302575: step 668, loss 0.2724, acc 0.828125, prec 0.03522, recall 0.738895
2017-12-10T03:26:55.568598: step 669, loss 0.18457, acc 0.96875, prec 0.0352164, recall 0.738895
2017-12-10T03:26:55.829172: step 670, loss 0.346236, acc 0.875, prec 0.0352018, recall 0.738895
2017-12-10T03:26:56.101131: step 671, loss 0.401409, acc 0.875, prec 0.0352371, recall 0.739177
2017-12-10T03:26:56.361803: step 672, loss 0.259358, acc 0.921875, prec 0.0352777, recall 0.739459
2017-12-10T03:26:56.631965: step 673, loss 1.60683, acc 0.84375, prec 0.0352614, recall 0.738661
2017-12-10T03:26:56.902035: step 674, loss 0.392972, acc 0.90625, prec 0.0352505, recall 0.738661
2017-12-10T03:26:57.168879: step 675, loss 0.427853, acc 0.8125, prec 0.0352287, recall 0.738661
2017-12-10T03:26:57.431811: step 676, loss 8.68043, acc 0.8125, prec 0.0352105, recall 0.737069
2017-12-10T03:26:57.704698: step 677, loss 0.48471, acc 0.875, prec 0.035196, recall 0.737069
2017-12-10T03:26:57.971351: step 678, loss 0.847478, acc 0.890625, prec 0.0352826, recall 0.737634
2017-12-10T03:26:58.241145: step 679, loss 2.68766, acc 0.796875, prec 0.0354096, recall 0.737687
2017-12-10T03:26:58.507097: step 680, loss 1.31686, acc 0.6875, prec 0.0354228, recall 0.737968
2017-12-10T03:26:58.774249: step 681, loss 1.23764, acc 0.6875, prec 0.0354359, recall 0.738248
2017-12-10T03:26:59.037403: step 682, loss 4.05328, acc 0.625, prec 0.0354436, recall 0.73774
2017-12-10T03:26:59.300496: step 683, loss 2.40636, acc 0.546875, prec 0.0354403, recall 0.738019
2017-12-10T03:26:59.567105: step 684, loss 2.20491, acc 0.484375, prec 0.0353806, recall 0.738019
2017-12-10T03:26:59.839131: step 685, loss 2.38389, acc 0.484375, prec 0.0353211, recall 0.738019
2017-12-10T03:27:00.104999: step 686, loss 3.91009, acc 0.359375, prec 0.0352965, recall 0.738298
2017-12-10T03:27:00.366553: step 687, loss 3.37273, acc 0.390625, prec 0.0352266, recall 0.738298
2017-12-10T03:27:00.628458: step 688, loss 3.25834, acc 0.4375, prec 0.0351624, recall 0.738298
2017-12-10T03:27:00.898082: step 689, loss 2.20004, acc 0.5, prec 0.0351055, recall 0.738298
2017-12-10T03:27:01.161512: step 690, loss 1.84853, acc 0.609375, prec 0.0351099, recall 0.738576
2017-12-10T03:27:01.423458: step 691, loss 2.58587, acc 0.5625, prec 0.035109, recall 0.738854
2017-12-10T03:27:01.684289: step 692, loss 1.40839, acc 0.59375, prec 0.035063, recall 0.738854
2017-12-10T03:27:01.946617: step 693, loss 1.96205, acc 0.59375, prec 0.0350657, recall 0.73913
2017-12-10T03:27:02.209049: step 694, loss 1.23746, acc 0.671875, prec 0.0350771, recall 0.739407
2017-12-10T03:27:02.478774: step 695, loss 1.25751, acc 0.75, prec 0.0351459, recall 0.739958
2017-12-10T03:27:02.746447: step 696, loss 0.462078, acc 0.828125, prec 0.0352233, recall 0.740506
2017-12-10T03:27:03.009841: step 697, loss 0.547687, acc 0.875, prec 0.0352575, recall 0.74078
2017-12-10T03:27:03.275723: step 698, loss 0.219926, acc 0.890625, prec 0.0353419, recall 0.741325
2017-12-10T03:27:03.536836: step 699, loss 0.162914, acc 0.9375, prec 0.0353348, recall 0.741325
2017-12-10T03:27:03.808610: step 700, loss 0.128943, acc 0.953125, prec 0.0353778, recall 0.741597
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-700

2017-12-10T03:27:05.293097: step 701, loss 5.23159, acc 0.9375, prec 0.0353725, recall 0.740818
2017-12-10T03:27:05.561026: step 702, loss 1.85827, acc 0.953125, prec 0.035369, recall 0.740042
2017-12-10T03:27:05.834813: step 703, loss 0.171333, acc 0.953125, prec 0.0353637, recall 0.740042
2017-12-10T03:27:06.097567: step 704, loss 0.216349, acc 0.921875, prec 0.0353548, recall 0.740042
2017-12-10T03:27:06.362346: step 705, loss 0.257444, acc 0.9375, prec 0.0353477, recall 0.740042
2017-12-10T03:27:06.632432: step 706, loss 1.08691, acc 0.921875, prec 0.0354354, recall 0.740586
2017-12-10T03:27:06.894664: step 707, loss 0.843701, acc 0.90625, prec 0.0356178, recall 0.741667
2017-12-10T03:27:07.175693: step 708, loss 0.774241, acc 0.875, prec 0.0356518, recall 0.741935
2017-12-10T03:27:07.448948: step 709, loss 0.917936, acc 0.828125, prec 0.0357286, recall 0.742471
2017-12-10T03:27:07.714311: step 710, loss 0.536522, acc 0.96875, prec 0.0358213, recall 0.743005
2017-12-10T03:27:07.983587: step 711, loss 0.81877, acc 0.953125, prec 0.0359123, recall 0.743537
2017-12-10T03:27:08.256321: step 712, loss 0.528517, acc 0.875, prec 0.0359461, recall 0.743802
2017-12-10T03:27:08.519175: step 713, loss 0.411704, acc 0.828125, prec 0.0359745, recall 0.744066
2017-12-10T03:27:08.777266: step 714, loss 0.509182, acc 0.84375, prec 0.0360046, recall 0.74433
2017-12-10T03:27:09.040766: step 715, loss 0.584209, acc 0.8125, prec 0.0359831, recall 0.74433
2017-12-10T03:27:09.305442: step 716, loss 1.72982, acc 0.75, prec 0.0360024, recall 0.744593
2017-12-10T03:27:09.579686: step 717, loss 0.641807, acc 0.765625, prec 0.0359755, recall 0.744593
2017-12-10T03:27:09.843950: step 718, loss 0.921339, acc 0.734375, prec 0.0359451, recall 0.744593
2017-12-10T03:27:10.108354: step 719, loss 0.826844, acc 0.84375, prec 0.0359273, recall 0.744593
2017-12-10T03:27:10.374228: step 720, loss 0.976551, acc 0.796875, prec 0.0359998, recall 0.745118
2017-12-10T03:27:10.638335: step 721, loss 0.753512, acc 0.828125, prec 0.0359802, recall 0.745118
2017-12-10T03:27:10.902092: step 722, loss 0.780466, acc 0.78125, prec 0.036003, recall 0.74538
2017-12-10T03:27:11.168218: step 723, loss 0.806665, acc 0.8125, prec 0.0359816, recall 0.74538
2017-12-10T03:27:11.435653: step 724, loss 1.14548, acc 0.84375, prec 0.0360115, recall 0.745641
2017-12-10T03:27:11.713865: step 725, loss 0.922395, acc 0.734375, prec 0.0360766, recall 0.746162
2017-12-10T03:27:11.988544: step 726, loss 0.530359, acc 0.859375, prec 0.0360605, recall 0.746162
2017-12-10T03:27:12.250791: step 727, loss 0.442858, acc 0.828125, prec 0.0360409, recall 0.746162
2017-12-10T03:27:12.511313: step 728, loss 0.471119, acc 0.84375, prec 0.0360231, recall 0.746162
2017-12-10T03:27:12.781021: step 729, loss 0.325824, acc 0.890625, prec 0.0360583, recall 0.746421
2017-12-10T03:27:13.051563: step 730, loss 0.416292, acc 0.859375, prec 0.0360423, recall 0.746421
2017-12-10T03:27:13.317172: step 731, loss 0.479204, acc 0.859375, prec 0.0360738, recall 0.74668
2017-12-10T03:27:13.589967: step 732, loss 3.71209, acc 0.859375, prec 0.0360596, recall 0.745918
2017-12-10T03:27:13.859072: step 733, loss 1.89064, acc 0.859375, prec 0.0360454, recall 0.745158
2017-12-10T03:27:14.124935: step 734, loss 8.36773, acc 0.90625, prec 0.036084, recall 0.744659
2017-12-10T03:27:14.394477: step 735, loss 1.25891, acc 0.8125, prec 0.0361102, recall 0.744919
2017-12-10T03:27:14.657698: step 736, loss 0.520473, acc 0.78125, prec 0.0360853, recall 0.744919
2017-12-10T03:27:14.916639: step 737, loss 0.84657, acc 0.78125, prec 0.0360604, recall 0.744919
2017-12-10T03:27:15.179231: step 738, loss 18.0825, acc 0.796875, prec 0.0360883, recall 0.743668
2017-12-10T03:27:15.446054: step 739, loss 2.03248, acc 0.65625, prec 0.0360967, recall 0.743927
2017-12-10T03:27:15.712579: step 740, loss 2.23318, acc 0.5625, prec 0.0360471, recall 0.743927
2017-12-10T03:27:15.976073: step 741, loss 2.1006, acc 0.5625, prec 0.0360449, recall 0.744186
2017-12-10T03:27:16.237627: step 742, loss 3.42189, acc 0.421875, prec 0.0359797, recall 0.744186
2017-12-10T03:27:16.496669: step 743, loss 3.8915, acc 0.375, prec 0.0359565, recall 0.744444
2017-12-10T03:27:16.760538: step 744, loss 3.02574, acc 0.4375, prec 0.0360812, recall 0.745473
2017-12-10T03:27:17.022563: step 745, loss 2.09807, acc 0.546875, prec 0.0360772, recall 0.745729
2017-12-10T03:27:17.278594: step 746, loss 2.75268, acc 0.53125, prec 0.0360247, recall 0.745729
2017-12-10T03:27:17.535372: step 747, loss 3.03365, acc 0.578125, prec 0.036071, recall 0.746239
2017-12-10T03:27:17.802969: step 748, loss 2.51077, acc 0.515625, prec 0.0360168, recall 0.746239
2017-12-10T03:27:18.064826: step 749, loss 2.12536, acc 0.578125, prec 0.0359698, recall 0.746239
2017-12-10T03:27:18.324015: step 750, loss 1.77608, acc 0.578125, prec 0.0359229, recall 0.746239
2017-12-10T03:27:18.587364: step 751, loss 1.04613, acc 0.75, prec 0.0359417, recall 0.746493
2017-12-10T03:27:18.845573: step 752, loss 1.88923, acc 0.640625, prec 0.0359019, recall 0.746493
2017-12-10T03:27:19.109099: step 753, loss 1.8192, acc 0.640625, prec 0.0359085, recall 0.746747
2017-12-10T03:27:19.370518: step 754, loss 1.23124, acc 0.71875, prec 0.0358775, recall 0.746747
2017-12-10T03:27:19.641321: step 755, loss 0.864062, acc 0.765625, prec 0.0358516, recall 0.746747
2017-12-10T03:27:19.905051: step 756, loss 1.15813, acc 0.796875, prec 0.0359218, recall 0.747253
2017-12-10T03:27:20.170584: step 757, loss 1.14853, acc 0.78125, prec 0.0358977, recall 0.747253
2017-12-10T03:27:20.430382: step 758, loss 0.386824, acc 0.90625, prec 0.0358873, recall 0.747253
2017-12-10T03:27:20.694534: step 759, loss 0.424843, acc 0.890625, prec 0.0358753, recall 0.747253
2017-12-10T03:27:20.954138: step 760, loss 0.193747, acc 0.9375, prec 0.0358684, recall 0.747253
2017-12-10T03:27:21.218540: step 761, loss 3.65395, acc 0.96875, prec 0.0358667, recall 0.746507
2017-12-10T03:27:21.490723: step 762, loss 26.932, acc 0.921875, prec 0.0358633, recall 0.744279
2017-12-10T03:27:21.761258: step 763, loss 0.235912, acc 0.953125, prec 0.0359043, recall 0.744533
2017-12-10T03:27:22.032425: step 764, loss 0.338141, acc 0.90625, prec 0.0359402, recall 0.744787
2017-12-10T03:27:22.295085: step 765, loss 2.83736, acc 0.875, prec 0.0360667, recall 0.744807
2017-12-10T03:27:22.567487: step 766, loss 0.808374, acc 0.796875, prec 0.0361365, recall 0.745311
2017-12-10T03:27:22.831106: step 767, loss 7.24063, acc 0.65625, prec 0.0361002, recall 0.744576
2017-12-10T03:27:23.097215: step 768, loss 2.74196, acc 0.5625, prec 0.036098, recall 0.744828
2017-12-10T03:27:23.358715: step 769, loss 1.97088, acc 0.578125, prec 0.0360975, recall 0.745079
2017-12-10T03:27:23.623196: step 770, loss 2.61441, acc 0.578125, prec 0.0360511, recall 0.745079
2017-12-10T03:27:23.886523: step 771, loss 3.39575, acc 0.4375, prec 0.0359894, recall 0.745079
2017-12-10T03:27:24.142327: step 772, loss 3.11944, acc 0.515625, prec 0.0359822, recall 0.745329
2017-12-10T03:27:24.404674: step 773, loss 4.01652, acc 0.46875, prec 0.0359242, recall 0.745329
2017-12-10T03:27:24.669523: step 774, loss 3.11523, acc 0.546875, prec 0.0358749, recall 0.745329
2017-12-10T03:27:24.929986: step 775, loss 2.65271, acc 0.5, prec 0.0359573, recall 0.746078
2017-12-10T03:27:25.190375: step 776, loss 2.86582, acc 0.4375, prec 0.0358962, recall 0.746078
2017-12-10T03:27:25.453252: step 777, loss 3.51523, acc 0.484375, prec 0.0359766, recall 0.746823
2017-12-10T03:27:25.725172: step 778, loss 1.88079, acc 0.53125, prec 0.0360619, recall 0.747563
2017-12-10T03:27:25.982601: step 779, loss 2.3569, acc 0.578125, prec 0.0360614, recall 0.747809
2017-12-10T03:27:26.253861: step 780, loss 1.84244, acc 0.609375, prec 0.0361095, recall 0.748299
2017-12-10T03:27:26.523863: step 781, loss 1.09788, acc 0.796875, prec 0.0360875, recall 0.748299
2017-12-10T03:27:26.805560: step 782, loss 1.23897, acc 0.75, prec 0.0360605, recall 0.748299
2017-12-10T03:27:27.069472: step 783, loss 1.03407, acc 0.75, prec 0.0360335, recall 0.748299
2017-12-10T03:27:27.334978: step 784, loss 0.960308, acc 0.78125, prec 0.0360099, recall 0.748299
2017-12-10T03:27:27.593864: step 785, loss 0.299713, acc 0.921875, prec 0.0360466, recall 0.748544
2017-12-10T03:27:27.858325: step 786, loss 0.831205, acc 0.828125, prec 0.0360731, recall 0.748788
2017-12-10T03:27:28.123585: step 787, loss 0.298695, acc 0.875, prec 0.0361046, recall 0.749031
2017-12-10T03:27:28.389521: step 788, loss 1.06273, acc 0.890625, prec 0.0361378, recall 0.749274
2017-12-10T03:27:28.653796: step 789, loss 0.0994553, acc 0.9375, prec 0.0361311, recall 0.749274
2017-12-10T03:27:28.919960: step 790, loss 0.136419, acc 0.984375, prec 0.0362194, recall 0.749758
2017-12-10T03:27:29.180414: step 791, loss 4.80822, acc 0.875, prec 0.0362542, recall 0.748555
2017-12-10T03:27:29.452134: step 792, loss 0.566823, acc 0.921875, prec 0.0362907, recall 0.748797
2017-12-10T03:27:29.712534: step 793, loss 7.96772, acc 0.875, prec 0.0362805, recall 0.747358
2017-12-10T03:27:29.982930: step 794, loss 0.481341, acc 0.859375, prec 0.0363102, recall 0.747601
2017-12-10T03:27:30.252389: step 795, loss 0.550694, acc 0.84375, prec 0.0362933, recall 0.747601
2017-12-10T03:27:30.523604: step 796, loss 0.598172, acc 0.765625, prec 0.036268, recall 0.747601
2017-12-10T03:27:30.787835: step 797, loss 0.806421, acc 0.75, prec 0.0362858, recall 0.747843
2017-12-10T03:27:31.049911: step 798, loss 0.788262, acc 0.78125, prec 0.0363518, recall 0.748325
2017-12-10T03:27:31.315010: step 799, loss 1.52535, acc 0.703125, prec 0.0364092, recall 0.748806
2017-12-10T03:27:31.576915: step 800, loss 1.51265, acc 0.71875, prec 0.0363788, recall 0.748806
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-800

2017-12-10T03:27:32.831457: step 801, loss 0.670889, acc 0.828125, prec 0.0363603, recall 0.748806
2017-12-10T03:27:33.089153: step 802, loss 1.48737, acc 0.640625, prec 0.0363215, recall 0.748806
2017-12-10T03:27:33.360415: step 803, loss 1.6033, acc 0.703125, prec 0.0364234, recall 0.749524
2017-12-10T03:27:33.621603: step 804, loss 0.80749, acc 0.828125, prec 0.0364494, recall 0.749762
2017-12-10T03:27:33.881788: step 805, loss 2.11083, acc 0.703125, prec 0.036551, recall 0.750474
2017-12-10T03:27:34.147908: step 806, loss 1.24974, acc 0.6875, prec 0.0365172, recall 0.750474
2017-12-10T03:27:34.408280: step 807, loss 1.47011, acc 0.6875, prec 0.0364836, recall 0.750474
2017-12-10T03:27:34.674119: step 808, loss 2.13072, acc 0.703125, prec 0.036496, recall 0.750711
2017-12-10T03:27:34.937939: step 809, loss 3.2838, acc 0.625, prec 0.0364574, recall 0.75
2017-12-10T03:27:35.206530: step 810, loss 11.613, acc 0.71875, prec 0.0365175, recall 0.749764
2017-12-10T03:27:35.476097: step 811, loss 2.49329, acc 0.609375, prec 0.0365641, recall 0.750236
2017-12-10T03:27:35.734737: step 812, loss 2.28959, acc 0.59375, prec 0.0365647, recall 0.750471
2017-12-10T03:27:35.994650: step 813, loss 2.06762, acc 0.6875, prec 0.0366195, recall 0.75094
2017-12-10T03:27:36.250697: step 814, loss 1.81237, acc 0.609375, prec 0.0366658, recall 0.751407
2017-12-10T03:27:36.511736: step 815, loss 2.17616, acc 0.5625, prec 0.0366188, recall 0.751407
2017-12-10T03:27:36.773513: step 816, loss 2.37298, acc 0.6875, prec 0.0367174, recall 0.752105
2017-12-10T03:27:37.042511: step 817, loss 2.09757, acc 0.5625, prec 0.0367583, recall 0.752568
2017-12-10T03:27:37.304874: step 818, loss 1.55145, acc 0.671875, prec 0.0368109, recall 0.753029
2017-12-10T03:27:37.566319: step 819, loss 2.00871, acc 0.625, prec 0.0368146, recall 0.753259
2017-12-10T03:27:37.832139: step 820, loss 5.75654, acc 0.609375, prec 0.0367744, recall 0.752558
2017-12-10T03:27:38.099795: step 821, loss 1.41949, acc 0.703125, prec 0.0368302, recall 0.753018
2017-12-10T03:27:38.361596: step 822, loss 1.45765, acc 0.734375, prec 0.0368017, recall 0.753018
2017-12-10T03:27:38.627069: step 823, loss 0.843972, acc 0.765625, prec 0.0369077, recall 0.753704
2017-12-10T03:27:38.889431: step 824, loss 1.65705, acc 0.625, prec 0.0368676, recall 0.753704
2017-12-10T03:27:39.148864: step 825, loss 4.19969, acc 0.71875, prec 0.0368392, recall 0.753006
2017-12-10T03:27:39.418867: step 826, loss 1.64401, acc 0.703125, prec 0.0368511, recall 0.753235
2017-12-10T03:27:39.685983: step 827, loss 0.988669, acc 0.75, prec 0.0369115, recall 0.75369
2017-12-10T03:27:39.948248: step 828, loss 1.06513, acc 0.640625, prec 0.0369167, recall 0.753917
2017-12-10T03:27:40.215107: step 829, loss 0.582402, acc 0.8125, prec 0.0369401, recall 0.754144
2017-12-10T03:27:40.483496: step 830, loss 1.25714, acc 0.734375, prec 0.0369986, recall 0.754596
2017-12-10T03:27:40.746432: step 831, loss 1.28439, acc 0.640625, prec 0.0370037, recall 0.754821
2017-12-10T03:27:41.004936: step 832, loss 3.87052, acc 0.71875, prec 0.0370187, recall 0.754354
2017-12-10T03:27:41.269396: step 833, loss 0.780841, acc 0.765625, prec 0.0369938, recall 0.754354
2017-12-10T03:27:41.532608: step 834, loss 1.71476, acc 0.8125, prec 0.0369755, recall 0.753663
2017-12-10T03:27:41.810797: step 835, loss 0.573777, acc 0.796875, prec 0.0369539, recall 0.753663
2017-12-10T03:27:42.070849: step 836, loss 1.0093, acc 0.75, prec 0.0369706, recall 0.753888
2017-12-10T03:27:42.333815: step 837, loss 1.37563, acc 0.65625, prec 0.0369773, recall 0.754113
2017-12-10T03:27:42.600602: step 838, loss 1.10913, acc 0.75, prec 0.037037, recall 0.754562
2017-12-10T03:27:42.864241: step 839, loss 0.37762, acc 0.890625, prec 0.0370685, recall 0.754786
2017-12-10T03:27:43.130811: step 840, loss 1.09464, acc 0.765625, prec 0.0370867, recall 0.755009
2017-12-10T03:27:43.393984: step 841, loss 0.780649, acc 0.734375, prec 0.0370586, recall 0.755009
2017-12-10T03:27:43.656913: step 842, loss 0.602334, acc 0.828125, prec 0.0370403, recall 0.755009
2017-12-10T03:27:43.918643: step 843, loss 0.903385, acc 0.75, prec 0.0370999, recall 0.755455
2017-12-10T03:27:44.183957: step 844, loss 1.89633, acc 0.78125, prec 0.0371626, recall 0.755898
2017-12-10T03:27:44.449545: step 845, loss 0.378715, acc 0.859375, prec 0.0371906, recall 0.75612
2017-12-10T03:27:44.711938: step 846, loss 4.00904, acc 0.796875, prec 0.0371707, recall 0.755435
2017-12-10T03:27:44.979638: step 847, loss 0.415894, acc 0.796875, prec 0.0371921, recall 0.755656
2017-12-10T03:27:45.243849: step 848, loss 0.534078, acc 0.828125, prec 0.0371739, recall 0.755656
2017-12-10T03:27:45.511652: step 849, loss 0.76597, acc 0.796875, prec 0.0371524, recall 0.755656
2017-12-10T03:27:45.784960: step 850, loss 0.511873, acc 0.859375, prec 0.0371375, recall 0.755656
2017-12-10T03:27:46.049376: step 851, loss 0.949344, acc 0.75, prec 0.0371111, recall 0.755656
2017-12-10T03:27:46.315485: step 852, loss 0.518696, acc 0.84375, prec 0.0370946, recall 0.755656
2017-12-10T03:27:46.575825: step 853, loss 5.00884, acc 0.859375, prec 0.0370814, recall 0.754973
2017-12-10T03:27:46.848176: step 854, loss 0.44427, acc 0.84375, prec 0.0371077, recall 0.755194
2017-12-10T03:27:47.128540: step 855, loss 0.896648, acc 0.859375, prec 0.0370929, recall 0.755194
2017-12-10T03:27:47.390211: step 856, loss 0.609573, acc 0.859375, prec 0.0370781, recall 0.755194
2017-12-10T03:27:47.655008: step 857, loss 0.452608, acc 0.890625, prec 0.0370666, recall 0.755194
2017-12-10T03:27:47.920813: step 858, loss 0.326651, acc 0.875, prec 0.0370535, recall 0.755194
2017-12-10T03:27:48.190503: step 859, loss 0.490742, acc 0.859375, prec 0.0370813, recall 0.755415
2017-12-10T03:27:48.462999: step 860, loss 0.67485, acc 0.734375, prec 0.0370534, recall 0.755415
2017-12-10T03:27:48.728736: step 861, loss 0.515797, acc 0.875, prec 0.0370403, recall 0.755415
2017-12-10T03:27:48.995843: step 862, loss 0.369383, acc 0.84375, prec 0.0370239, recall 0.755415
2017-12-10T03:27:49.267465: step 863, loss 5.60421, acc 0.90625, prec 0.0370583, recall 0.754955
2017-12-10T03:27:49.532886: step 864, loss 0.654933, acc 0.796875, prec 0.037037, recall 0.754955
2017-12-10T03:27:49.797721: step 865, loss 0.706658, acc 0.875, prec 0.0370665, recall 0.755176
2017-12-10T03:27:50.069196: step 866, loss 3.09259, acc 0.8125, prec 0.037091, recall 0.754717
2017-12-10T03:27:50.337361: step 867, loss 1.02172, acc 0.75, prec 0.0371073, recall 0.754937
2017-12-10T03:27:50.601859: step 868, loss 0.561714, acc 0.8125, prec 0.0370877, recall 0.754937
2017-12-10T03:27:50.878546: step 869, loss 1.24803, acc 0.765625, prec 0.037148, recall 0.755376
2017-12-10T03:27:51.138014: step 870, loss 1.19062, acc 0.6875, prec 0.0371153, recall 0.755376
2017-12-10T03:27:51.398180: step 871, loss 1.46709, acc 0.578125, prec 0.0370712, recall 0.755376
2017-12-10T03:27:51.660373: step 872, loss 1.21367, acc 0.703125, prec 0.0370403, recall 0.755376
2017-12-10T03:27:51.918103: step 873, loss 2.03161, acc 0.71875, prec 0.0370533, recall 0.755595
2017-12-10T03:27:52.182269: step 874, loss 0.862406, acc 0.765625, prec 0.0370289, recall 0.755595
2017-12-10T03:27:52.453474: step 875, loss 1.14032, acc 0.765625, prec 0.0370468, recall 0.755814
2017-12-10T03:27:52.724593: step 876, loss 1.0837, acc 0.71875, prec 0.0370176, recall 0.755814
2017-12-10T03:27:52.991584: step 877, loss 2.41024, acc 0.640625, prec 0.0370241, recall 0.755357
2017-12-10T03:27:53.262623: step 878, loss 8.62278, acc 0.796875, prec 0.0370468, recall 0.754902
2017-12-10T03:27:53.529285: step 879, loss 1.28971, acc 0.6875, prec 0.0370565, recall 0.75512
2017-12-10T03:27:53.797312: step 880, loss 1.42931, acc 0.71875, prec 0.0370273, recall 0.75512
2017-12-10T03:27:54.061045: step 881, loss 1.3945, acc 0.625, prec 0.0370726, recall 0.755556
2017-12-10T03:27:54.326514: step 882, loss 1.26671, acc 0.71875, prec 0.0370435, recall 0.755556
2017-12-10T03:27:54.590685: step 883, loss 1.7532, acc 0.65625, prec 0.0370499, recall 0.755773
2017-12-10T03:27:54.853991: step 884, loss 1.68396, acc 0.625, prec 0.037095, recall 0.756206
2017-12-10T03:27:55.121910: step 885, loss 3.11785, acc 0.71875, prec 0.0370676, recall 0.755536
2017-12-10T03:27:55.392803: step 886, loss 0.788793, acc 0.8125, prec 0.0371319, recall 0.755968
2017-12-10T03:27:55.654851: step 887, loss 1.05493, acc 0.75, prec 0.0371897, recall 0.756399
2017-12-10T03:27:55.919075: step 888, loss 1.02942, acc 0.671875, prec 0.0372393, recall 0.756828
2017-12-10T03:27:56.190118: step 889, loss 0.805723, acc 0.828125, prec 0.0372216, recall 0.756828
2017-12-10T03:27:56.454035: step 890, loss 0.619234, acc 0.8125, prec 0.0372439, recall 0.757042
2017-12-10T03:27:56.725721: step 891, loss 0.635347, acc 0.84375, prec 0.0373528, recall 0.757682
2017-12-10T03:27:56.987491: step 892, loss 1.98432, acc 0.75, prec 0.0374103, recall 0.758107
2017-12-10T03:27:57.259836: step 893, loss 1.26394, acc 0.6875, prec 0.0373779, recall 0.758107
2017-12-10T03:27:57.520415: step 894, loss 6.59392, acc 0.796875, prec 0.0373586, recall 0.757443
2017-12-10T03:27:57.783379: step 895, loss 2.94558, acc 0.78125, prec 0.0373376, recall 0.75678
2017-12-10T03:27:58.044173: step 896, loss 4.45315, acc 0.78125, prec 0.0373997, recall 0.756545
2017-12-10T03:27:58.310527: step 897, loss 0.922233, acc 0.765625, prec 0.0374585, recall 0.756969
2017-12-10T03:27:58.575242: step 898, loss 1.44558, acc 0.65625, prec 0.0375059, recall 0.757391
2017-12-10T03:27:58.838043: step 899, loss 1.88127, acc 0.5625, prec 0.0375022, recall 0.757602
2017-12-10T03:27:59.100214: step 900, loss 1.31139, acc 0.640625, prec 0.0375064, recall 0.757812

Evaluation:
2017-12-10T03:28:06.636200: step 900, loss 1.81937, acc 0.558826, prec 0.0363054, recall 0.782809

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-900

2017-12-10T03:28:07.996490: step 901, loss 2.60089, acc 0.40625, prec 0.0362564, recall 0.782809
2017-12-10T03:28:08.264395: step 902, loss 1.94904, acc 0.625, prec 0.0362597, recall 0.782975
2017-12-10T03:28:08.529093: step 903, loss 1.69874, acc 0.578125, prec 0.0362249, recall 0.782975
2017-12-10T03:28:08.792762: step 904, loss 1.59868, acc 0.59375, prec 0.0361916, recall 0.782975
2017-12-10T03:28:09.055751: step 905, loss 1.65464, acc 0.625, prec 0.0361949, recall 0.783142
2017-12-10T03:28:09.314296: step 906, loss 2.02454, acc 0.578125, prec 0.0361603, recall 0.783142
2017-12-10T03:28:09.574836: step 907, loss 1.73678, acc 0.671875, prec 0.0361676, recall 0.783308
2017-12-10T03:28:09.842269: step 908, loss 3.87341, acc 0.6875, prec 0.0361433, recall 0.782708
2017-12-10T03:28:10.112158: step 909, loss 1.29564, acc 0.609375, prec 0.0361454, recall 0.782875
2017-12-10T03:28:10.372676: step 910, loss 1.0404, acc 0.703125, prec 0.0361552, recall 0.78304
2017-12-10T03:28:10.644081: step 911, loss 1.35826, acc 0.6875, prec 0.0361637, recall 0.783206
2017-12-10T03:28:10.902672: step 912, loss 0.767379, acc 0.71875, prec 0.0361408, recall 0.783206
2017-12-10T03:28:11.163610: step 913, loss 0.857273, acc 0.765625, prec 0.0361556, recall 0.783371
2017-12-10T03:28:11.426849: step 914, loss 0.58971, acc 0.78125, prec 0.0361717, recall 0.783537
2017-12-10T03:28:11.707284: step 915, loss 0.618285, acc 0.828125, prec 0.0361916, recall 0.783701
2017-12-10T03:28:11.976886: step 916, loss 0.345423, acc 0.890625, prec 0.0361827, recall 0.783701
2017-12-10T03:28:12.245808: step 917, loss 0.356968, acc 0.890625, prec 0.0362077, recall 0.783866
2017-12-10T03:28:12.515165: step 918, loss 0.329677, acc 0.890625, prec 0.0361988, recall 0.783866
2017-12-10T03:28:12.775195: step 919, loss 0.572274, acc 0.859375, prec 0.0362212, recall 0.78403
2017-12-10T03:28:13.036893: step 920, loss 0.275631, acc 0.9375, prec 0.03625, recall 0.784195
2017-12-10T03:28:13.309497: step 921, loss 0.172729, acc 0.953125, prec 0.0362461, recall 0.784195
2017-12-10T03:28:13.576238: step 922, loss 3.2489, acc 0.9375, prec 0.0362423, recall 0.783599
2017-12-10T03:28:13.845074: step 923, loss 0.249305, acc 0.96875, prec 0.0362736, recall 0.783763
2017-12-10T03:28:14.109558: step 924, loss 0.132791, acc 0.984375, prec 0.0362723, recall 0.783763
2017-12-10T03:28:14.372477: step 925, loss 5.45739, acc 0.921875, prec 0.0362672, recall 0.783169
2017-12-10T03:28:14.642361: step 926, loss 0.188443, acc 0.921875, prec 0.0362609, recall 0.783169
2017-12-10T03:28:14.906455: step 927, loss 4.55476, acc 0.96875, prec 0.0362934, recall 0.78274
2017-12-10T03:28:15.172238: step 928, loss 0.454827, acc 0.921875, prec 0.0363209, recall 0.782905
2017-12-10T03:28:15.441416: step 929, loss 0.67044, acc 0.8125, prec 0.0363394, recall 0.783069
2017-12-10T03:28:15.701710: step 930, loss 2.09103, acc 0.859375, prec 0.0363292, recall 0.782477
2017-12-10T03:28:15.957507: step 931, loss 0.78674, acc 0.796875, prec 0.0363127, recall 0.782477
2017-12-10T03:28:16.220510: step 932, loss 1.28372, acc 0.703125, prec 0.0363222, recall 0.782642
2017-12-10T03:28:16.484629: step 933, loss 1.22272, acc 0.75, prec 0.0363019, recall 0.782642
2017-12-10T03:28:16.747358: step 934, loss 1.74944, acc 0.609375, prec 0.0362702, recall 0.782642
2017-12-10T03:28:17.008055: step 935, loss 2.09844, acc 0.5625, prec 0.0362683, recall 0.782805
2017-12-10T03:28:17.269355: step 936, loss 1.79587, acc 0.609375, prec 0.0362703, recall 0.782969
2017-12-10T03:28:17.529528: step 937, loss 1.64683, acc 0.625, prec 0.0362736, recall 0.783133
2017-12-10T03:28:17.802152: step 938, loss 1.46868, acc 0.65625, prec 0.0363129, recall 0.783459
2017-12-10T03:28:18.067202: step 939, loss 1.35349, acc 0.75, prec 0.0363598, recall 0.783784
2017-12-10T03:28:18.333693: step 940, loss 0.827987, acc 0.75, prec 0.0363396, recall 0.783784
2017-12-10T03:28:18.604997: step 941, loss 4.35913, acc 0.734375, prec 0.0363194, recall 0.783196
2017-12-10T03:28:18.869257: step 942, loss 2.03011, acc 0.65625, prec 0.0363586, recall 0.783521
2017-12-10T03:28:19.130805: step 943, loss 4.78531, acc 0.640625, prec 0.0363977, recall 0.783259
2017-12-10T03:28:19.394606: step 944, loss 5.38282, acc 0.625, prec 0.0364021, recall 0.782836
2017-12-10T03:28:19.664290: step 945, loss 1.57747, acc 0.59375, prec 0.0364027, recall 0.782998
2017-12-10T03:28:19.926572: step 946, loss 1.37313, acc 0.59375, prec 0.0364367, recall 0.783321
2017-12-10T03:28:20.195045: step 947, loss 2.17111, acc 0.546875, prec 0.0364668, recall 0.783643
2017-12-10T03:28:20.453647: step 948, loss 2.28007, acc 0.5, prec 0.0364598, recall 0.783804
2017-12-10T03:28:20.718235: step 949, loss 2.23218, acc 0.46875, prec 0.036417, recall 0.783804
2017-12-10T03:28:20.978742: step 950, loss 1.51812, acc 0.65625, prec 0.0364226, recall 0.783964
2017-12-10T03:28:21.242825: step 951, loss 1.55833, acc 0.625, prec 0.0364257, recall 0.784125
2017-12-10T03:28:21.510713: step 952, loss 1.9259, acc 0.65625, prec 0.0364644, recall 0.784444
2017-12-10T03:28:21.779687: step 953, loss 1.79147, acc 0.6875, prec 0.0364725, recall 0.784604
2017-12-10T03:28:22.043944: step 954, loss 1.668, acc 0.5625, prec 0.0364374, recall 0.784604
2017-12-10T03:28:22.311123: step 955, loss 1.09745, acc 0.71875, prec 0.036448, recall 0.784763
2017-12-10T03:28:22.583482: step 956, loss 0.876348, acc 0.78125, prec 0.0364304, recall 0.784763
2017-12-10T03:28:22.847839: step 957, loss 0.726697, acc 0.859375, prec 0.0364192, recall 0.784763
2017-12-10T03:28:23.114129: step 958, loss 0.727422, acc 0.8125, prec 0.0364042, recall 0.784763
2017-12-10T03:28:23.373321: step 959, loss 0.511134, acc 0.859375, prec 0.036459, recall 0.785081
2017-12-10T03:28:23.648628: step 960, loss 0.442555, acc 0.859375, prec 0.0364478, recall 0.785081
2017-12-10T03:28:23.916226: step 961, loss 1.17744, acc 0.921875, prec 0.0365076, recall 0.785398
2017-12-10T03:28:24.190680: step 962, loss 0.412252, acc 0.859375, prec 0.0365294, recall 0.785556
2017-12-10T03:28:24.461337: step 963, loss 0.305589, acc 0.9375, prec 0.0365904, recall 0.785872
2017-12-10T03:28:24.726905: step 964, loss 0.244063, acc 0.921875, prec 0.0365841, recall 0.785872
2017-12-10T03:28:24.991700: step 965, loss 0.21671, acc 0.953125, prec 0.0366133, recall 0.786029
2017-12-10T03:28:25.254880: step 966, loss 0.943282, acc 0.84375, prec 0.0366338, recall 0.786187
2017-12-10T03:28:25.523436: step 967, loss 0.355064, acc 0.953125, prec 0.036696, recall 0.7865
2017-12-10T03:28:25.786443: step 968, loss 5.49315, acc 0.875, prec 0.0367531, recall 0.786237
2017-12-10T03:28:26.051697: step 969, loss 0.583946, acc 0.90625, prec 0.0367785, recall 0.786394
2017-12-10T03:28:26.316373: step 970, loss 9.58976, acc 0.875, prec 0.0367697, recall 0.785819
2017-12-10T03:28:26.591914: step 971, loss 0.902795, acc 0.890625, prec 0.0367939, recall 0.785975
2017-12-10T03:28:26.861923: step 972, loss 1.09633, acc 0.765625, prec 0.0368079, recall 0.786131
2017-12-10T03:28:27.128112: step 973, loss 0.897757, acc 0.78125, prec 0.0368232, recall 0.786287
2017-12-10T03:28:27.392764: step 974, loss 1.10223, acc 0.765625, prec 0.036903, recall 0.786754
2017-12-10T03:28:27.656593: step 975, loss 1.02329, acc 0.6875, prec 0.0368778, recall 0.786754
2017-12-10T03:28:27.920297: step 976, loss 0.898265, acc 0.671875, prec 0.0368514, recall 0.786754
2017-12-10T03:28:28.184770: step 977, loss 1.31549, acc 0.734375, prec 0.0368301, recall 0.786754
2017-12-10T03:28:28.444594: step 978, loss 1.18091, acc 0.65625, prec 0.0368681, recall 0.787064
2017-12-10T03:28:28.709085: step 979, loss 1.21508, acc 0.578125, prec 0.0368342, recall 0.787064
2017-12-10T03:28:28.967499: step 980, loss 1.21281, acc 0.640625, prec 0.0368054, recall 0.787064
2017-12-10T03:28:29.238937: step 981, loss 1.60948, acc 0.671875, prec 0.0368119, recall 0.787219
2017-12-10T03:28:29.504347: step 982, loss 1.1474, acc 0.59375, prec 0.0368121, recall 0.787373
2017-12-10T03:28:29.767926: step 983, loss 1.31527, acc 0.734375, prec 0.0368235, recall 0.787527
2017-12-10T03:28:30.031528: step 984, loss 0.805, acc 0.703125, prec 0.0367998, recall 0.787527
2017-12-10T03:28:30.303968: step 985, loss 0.711546, acc 0.8125, prec 0.0368175, recall 0.787681
2017-12-10T03:28:30.570760: step 986, loss 1.00498, acc 0.765625, prec 0.0367988, recall 0.787681
2017-12-10T03:28:30.831462: step 987, loss 0.778536, acc 0.8125, prec 0.0368816, recall 0.788142
2017-12-10T03:28:31.092887: step 988, loss 0.515158, acc 0.796875, prec 0.0368654, recall 0.788142
2017-12-10T03:28:31.362813: step 989, loss 22.2302, acc 0.828125, prec 0.0368542, recall 0.787004
2017-12-10T03:28:31.644055: step 990, loss 0.820546, acc 0.796875, prec 0.036838, recall 0.787004
2017-12-10T03:28:31.911392: step 991, loss 0.657367, acc 0.8125, prec 0.0368231, recall 0.787004
2017-12-10T03:28:32.179591: step 992, loss 0.724102, acc 0.84375, prec 0.0368106, recall 0.787004
2017-12-10T03:28:32.447199: step 993, loss 0.41195, acc 0.875, prec 0.0368007, recall 0.787004
2017-12-10T03:28:32.672129: step 994, loss 2.28524, acc 0.882353, prec 0.036827, recall 0.78659
2017-12-10T03:28:32.945222: step 995, loss 0.762553, acc 0.796875, prec 0.0368758, recall 0.786897
2017-12-10T03:28:33.209981: step 996, loss 4.2557, acc 0.734375, prec 0.0368884, recall 0.786485
2017-12-10T03:28:33.472883: step 997, loss 0.71717, acc 0.8125, prec 0.036906, recall 0.786638
2017-12-10T03:28:33.740492: step 998, loss 0.977144, acc 0.765625, prec 0.0369522, recall 0.786944
2017-12-10T03:28:34.010995: step 999, loss 1.57682, acc 0.71875, prec 0.0369946, recall 0.787249
2017-12-10T03:28:34.276944: step 1000, loss 1.28408, acc 0.6875, prec 0.0369698, recall 0.787249
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-1000

2017-12-10T03:28:35.743938: step 1001, loss 0.909707, acc 0.734375, prec 0.0370134, recall 0.787554
2017-12-10T03:28:36.017142: step 1002, loss 1.89686, acc 0.578125, prec 0.0369798, recall 0.787554
2017-12-10T03:28:36.282130: step 1003, loss 1.65005, acc 0.71875, prec 0.0370544, recall 0.788009
2017-12-10T03:28:36.545859: step 1004, loss 1.46331, acc 0.609375, prec 0.0370557, recall 0.78816
2017-12-10T03:28:36.808443: step 1005, loss 1.42084, acc 0.578125, prec 0.0370221, recall 0.78816
2017-12-10T03:28:37.069081: step 1006, loss 1.11164, acc 0.6875, prec 0.0370296, recall 0.788311
2017-12-10T03:28:37.329822: step 1007, loss 0.839758, acc 0.765625, prec 0.037011, recall 0.788311
2017-12-10T03:28:37.596941: step 1008, loss 1.99766, acc 0.703125, prec 0.0370209, recall 0.7879
2017-12-10T03:28:37.864618: step 1009, loss 0.743983, acc 0.765625, prec 0.0370024, recall 0.7879
2017-12-10T03:28:38.139825: step 1010, loss 3.74952, acc 0.796875, prec 0.0369875, recall 0.78734
2017-12-10T03:28:38.409792: step 1011, loss 1.09985, acc 0.71875, prec 0.0369975, recall 0.787491
2017-12-10T03:28:38.671569: step 1012, loss 1.03112, acc 0.75, prec 0.0369777, recall 0.787491
2017-12-10T03:28:38.938489: step 1013, loss 0.81835, acc 0.78125, prec 0.0369604, recall 0.787491
2017-12-10T03:28:39.207362: step 1014, loss 0.949355, acc 0.75, prec 0.0369728, recall 0.787642
2017-12-10T03:28:39.469011: step 1015, loss 0.999458, acc 0.703125, prec 0.0370136, recall 0.787943
2017-12-10T03:28:39.737003: step 1016, loss 1.2321, acc 0.703125, prec 0.0370864, recall 0.788393
2017-12-10T03:28:40.006749: step 1017, loss 0.740626, acc 0.796875, prec 0.0370703, recall 0.788393
2017-12-10T03:28:40.272353: step 1018, loss 0.311349, acc 0.890625, prec 0.0371257, recall 0.788693
2017-12-10T03:28:40.531360: step 1019, loss 0.405523, acc 0.828125, prec 0.0371122, recall 0.788693
2017-12-10T03:28:40.794401: step 1020, loss 0.822355, acc 0.890625, prec 0.0372316, recall 0.789288
2017-12-10T03:28:41.057930: step 1021, loss 0.289297, acc 0.90625, prec 0.0372241, recall 0.789288
2017-12-10T03:28:41.332442: step 1022, loss 0.534686, acc 0.828125, prec 0.0372105, recall 0.789288
2017-12-10T03:28:41.601577: step 1023, loss 0.577936, acc 0.859375, prec 0.0372314, recall 0.789437
2017-12-10T03:28:41.874564: step 1024, loss 0.347026, acc 0.90625, prec 0.037224, recall 0.789437
2017-12-10T03:28:42.137303: step 1025, loss 2.10915, acc 0.921875, prec 0.037251, recall 0.78903
2017-12-10T03:28:42.402026: step 1026, loss 3.83018, acc 0.828125, prec 0.0372386, recall 0.788475
2017-12-10T03:28:42.677063: step 1027, loss 0.218084, acc 0.90625, prec 0.0372312, recall 0.788475
2017-12-10T03:28:42.942069: step 1028, loss 0.594401, acc 0.828125, prec 0.0372176, recall 0.788475
2017-12-10T03:28:43.204383: step 1029, loss 0.496141, acc 0.8125, prec 0.0372347, recall 0.788624
2017-12-10T03:28:43.463371: step 1030, loss 0.568438, acc 0.828125, prec 0.0372212, recall 0.788624
2017-12-10T03:28:43.723542: step 1031, loss 0.61361, acc 0.78125, prec 0.0372039, recall 0.788624
2017-12-10T03:28:43.983024: step 1032, loss 0.294841, acc 0.875, prec 0.0371941, recall 0.788624
2017-12-10T03:28:44.255092: step 1033, loss 0.616788, acc 0.828125, prec 0.0371805, recall 0.788624
2017-12-10T03:28:44.517197: step 1034, loss 0.470295, acc 0.90625, prec 0.0371731, recall 0.788624
2017-12-10T03:28:44.778268: step 1035, loss 0.720819, acc 0.8125, prec 0.0371902, recall 0.788772
2017-12-10T03:28:45.051092: step 1036, loss 0.992522, acc 0.921875, prec 0.0372159, recall 0.78892
2017-12-10T03:28:45.318279: step 1037, loss 1.15228, acc 0.890625, prec 0.0373028, recall 0.789363
2017-12-10T03:28:45.587107: step 1038, loss 0.622046, acc 0.84375, prec 0.0372905, recall 0.789363
2017-12-10T03:28:45.846600: step 1039, loss 0.781865, acc 0.828125, prec 0.0373087, recall 0.78951
2017-12-10T03:28:46.109475: step 1040, loss 0.630338, acc 0.859375, prec 0.0372977, recall 0.78951
2017-12-10T03:28:46.373784: step 1041, loss 0.6425, acc 0.828125, prec 0.0372841, recall 0.78951
2017-12-10T03:28:46.637581: step 1042, loss 0.646566, acc 0.796875, prec 0.0373317, recall 0.789804
2017-12-10T03:28:46.901616: step 1043, loss 2.61521, acc 0.84375, prec 0.0374159, recall 0.789694
2017-12-10T03:28:47.168717: step 1044, loss 0.388306, acc 0.890625, prec 0.0374072, recall 0.789694
2017-12-10T03:28:47.430810: step 1045, loss 1.20488, acc 0.796875, prec 0.0375181, recall 0.790278
2017-12-10T03:28:47.697086: step 1046, loss 1.06368, acc 0.734375, prec 0.0375288, recall 0.790423
2017-12-10T03:28:47.965864: step 1047, loss 0.771218, acc 0.71875, prec 0.0375066, recall 0.790423
2017-12-10T03:28:48.235856: step 1048, loss 0.89312, acc 0.796875, prec 0.0374905, recall 0.790423
2017-12-10T03:28:48.499270: step 1049, loss 1.05457, acc 0.71875, prec 0.0375, recall 0.790569
2017-12-10T03:28:48.766444: step 1050, loss 1.13314, acc 0.734375, prec 0.037479, recall 0.790569
2017-12-10T03:28:49.028362: step 1051, loss 0.689663, acc 0.71875, prec 0.0374885, recall 0.790714
2017-12-10T03:28:49.290556: step 1052, loss 2.07027, acc 0.703125, prec 0.0374967, recall 0.790859
2017-12-10T03:28:49.556792: step 1053, loss 0.680222, acc 0.78125, prec 0.0374795, recall 0.790859
2017-12-10T03:28:49.816847: step 1054, loss 0.897319, acc 0.84375, prec 0.0374988, recall 0.791003
2017-12-10T03:28:50.080060: step 1055, loss 0.9957, acc 0.765625, prec 0.0374803, recall 0.791003
2017-12-10T03:28:50.341472: step 1056, loss 0.460309, acc 0.8125, prec 0.0374656, recall 0.791003
2017-12-10T03:28:50.599121: step 1057, loss 0.457197, acc 0.84375, prec 0.0374533, recall 0.791003
2017-12-10T03:28:50.859586: step 1058, loss 0.165129, acc 0.921875, prec 0.0374472, recall 0.791003
2017-12-10T03:28:51.127754: step 1059, loss 0.480838, acc 0.8125, prec 0.0374325, recall 0.791003
2017-12-10T03:28:51.393032: step 1060, loss 0.396417, acc 0.875, prec 0.0374226, recall 0.791003
2017-12-10T03:28:51.656319: step 1061, loss 0.155459, acc 0.90625, prec 0.0374153, recall 0.791003
2017-12-10T03:28:51.914913: step 1062, loss 1.80416, acc 0.90625, prec 0.0374092, recall 0.790456
2017-12-10T03:28:52.184341: step 1063, loss 0.489328, acc 0.875, prec 0.0374309, recall 0.790601
2017-12-10T03:28:52.453584: step 1064, loss 0.316134, acc 0.90625, prec 0.037518, recall 0.791034
2017-12-10T03:28:52.718862: step 1065, loss 0.488782, acc 0.890625, prec 0.0376038, recall 0.791466
2017-12-10T03:28:52.980352: step 1066, loss 1.54307, acc 0.828125, prec 0.0376859, recall 0.791352
2017-12-10T03:28:53.253203: step 1067, loss 0.187749, acc 0.90625, prec 0.03771, recall 0.791495
2017-12-10T03:28:53.515132: step 1068, loss 0.356767, acc 0.90625, prec 0.0377026, recall 0.791495
2017-12-10T03:28:53.777641: step 1069, loss 1.78114, acc 0.9375, prec 0.0377303, recall 0.791096
2017-12-10T03:28:54.042553: step 1070, loss 0.42141, acc 0.875, prec 0.0377519, recall 0.791239
2017-12-10T03:28:54.308239: step 1071, loss 0.600099, acc 0.890625, prec 0.0378375, recall 0.791667
2017-12-10T03:28:54.571352: step 1072, loss 0.306913, acc 0.875, prec 0.0378276, recall 0.791667
2017-12-10T03:28:54.831108: step 1073, loss 4.4084, acc 0.859375, prec 0.0378177, recall 0.791126
2017-12-10T03:28:55.099396: step 1074, loss 0.743756, acc 0.78125, prec 0.0378005, recall 0.791126
2017-12-10T03:28:55.363153: step 1075, loss 0.527932, acc 0.84375, prec 0.0378195, recall 0.791269
2017-12-10T03:28:55.632309: step 1076, loss 0.59803, acc 0.875, prec 0.0378096, recall 0.791269
2017-12-10T03:28:55.896185: step 1077, loss 1.16667, acc 0.75, prec 0.0378213, recall 0.791411
2017-12-10T03:28:56.169134: step 1078, loss 0.956166, acc 0.71875, prec 0.0377991, recall 0.791411
2017-12-10T03:28:56.432394: step 1079, loss 0.853744, acc 0.796875, prec 0.0378144, recall 0.791553
2017-12-10T03:28:56.704497: step 1080, loss 0.829238, acc 0.8125, prec 0.0377997, recall 0.791553
2017-12-10T03:28:56.968256: step 1081, loss 0.361717, acc 0.875, prec 0.0378211, recall 0.791695
2017-12-10T03:28:57.230577: step 1082, loss 0.811184, acc 0.78125, prec 0.0378352, recall 0.791837
2017-12-10T03:28:57.495361: step 1083, loss 0.857674, acc 0.828125, prec 0.0378529, recall 0.791978
2017-12-10T03:28:57.759258: step 1084, loss 0.597081, acc 0.796875, prec 0.037837, recall 0.791978
2017-12-10T03:28:58.017342: step 1085, loss 0.753598, acc 0.828125, prec 0.0378859, recall 0.792261
2017-12-10T03:28:58.279656: step 1086, loss 0.497296, acc 0.890625, prec 0.0378773, recall 0.792261
2017-12-10T03:28:58.538562: step 1087, loss 4.53754, acc 0.796875, prec 0.0378626, recall 0.791723
2017-12-10T03:28:58.804628: step 1088, loss 0.49488, acc 0.859375, prec 0.0378515, recall 0.791723
2017-12-10T03:28:59.068223: step 1089, loss 6.2, acc 0.78125, prec 0.0378668, recall 0.791328
2017-12-10T03:28:59.334644: step 1090, loss 0.356647, acc 0.875, prec 0.0378881, recall 0.791469
2017-12-10T03:28:59.597485: step 1091, loss 0.796264, acc 0.796875, prec 0.0379345, recall 0.791751
2017-12-10T03:28:59.866699: step 1092, loss 0.51709, acc 0.828125, prec 0.0379833, recall 0.792032
2017-12-10T03:29:00.136370: step 1093, loss 0.874501, acc 0.734375, prec 0.0379624, recall 0.792032
2017-12-10T03:29:00.403979: step 1094, loss 0.593368, acc 0.828125, prec 0.0380111, recall 0.792313
2017-12-10T03:29:00.669809: step 1095, loss 1.5941, acc 0.734375, prec 0.0380213, recall 0.792453
2017-12-10T03:29:00.934273: step 1096, loss 0.928915, acc 0.75, prec 0.0380328, recall 0.792593
2017-12-10T03:29:01.196904: step 1097, loss 0.620127, acc 0.796875, prec 0.0380789, recall 0.792872
2017-12-10T03:29:01.456226: step 1098, loss 0.595327, acc 0.84375, prec 0.0380666, recall 0.792872
2017-12-10T03:29:01.714249: step 1099, loss 2.78295, acc 0.71875, prec 0.0381066, recall 0.79315
2017-12-10T03:29:01.979008: step 1100, loss 0.831042, acc 0.84375, prec 0.0381564, recall 0.793427
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-1100

2017-12-10T03:29:03.273342: step 1101, loss 0.867888, acc 0.734375, prec 0.0381355, recall 0.793427
2017-12-10T03:29:03.533056: step 1102, loss 1.01685, acc 0.765625, prec 0.038148, recall 0.793566
2017-12-10T03:29:03.793747: step 1103, loss 0.882337, acc 0.671875, prec 0.0381842, recall 0.793842
2017-12-10T03:29:04.053534: step 1104, loss 0.88019, acc 0.734375, prec 0.0381942, recall 0.79398
2017-12-10T03:29:04.316405: step 1105, loss 1.1102, acc 0.734375, prec 0.0381733, recall 0.79398
2017-12-10T03:29:04.585964: step 1106, loss 7.61608, acc 0.71875, prec 0.0381525, recall 0.793449
2017-12-10T03:29:04.854148: step 1107, loss 1.28577, acc 0.765625, prec 0.0381341, recall 0.793449
2017-12-10T03:29:05.114461: step 1108, loss 0.792157, acc 0.765625, prec 0.0381466, recall 0.793587
2017-12-10T03:29:05.378760: step 1109, loss 1.47925, acc 0.734375, prec 0.0382493, recall 0.794137
2017-12-10T03:29:05.645827: step 1110, loss 0.712814, acc 0.859375, prec 0.0382382, recall 0.794137
2017-12-10T03:29:05.915694: step 1111, loss 0.821913, acc 0.796875, prec 0.0382223, recall 0.794137
2017-12-10T03:29:06.184681: step 1112, loss 0.874687, acc 0.796875, prec 0.0382372, recall 0.794274
2017-12-10T03:29:06.445483: step 1113, loss 0.741698, acc 0.765625, prec 0.0382188, recall 0.794274
2017-12-10T03:29:06.704968: step 1114, loss 1.22879, acc 0.828125, prec 0.0382361, recall 0.794411
2017-12-10T03:29:06.972537: step 1115, loss 0.666048, acc 0.828125, prec 0.0382227, recall 0.794411
2017-12-10T03:29:07.245390: step 1116, loss 0.541979, acc 0.890625, prec 0.0382449, recall 0.794548
2017-12-10T03:29:07.508655: step 1117, loss 0.508307, acc 0.875, prec 0.0382966, recall 0.794821
2017-12-10T03:29:07.771407: step 1118, loss 0.302912, acc 0.875, prec 0.0383176, recall 0.794957
2017-12-10T03:29:08.036146: step 1119, loss 0.569923, acc 0.84375, prec 0.0383054, recall 0.794957
2017-12-10T03:29:08.301693: step 1120, loss 2.32874, acc 0.859375, prec 0.0382956, recall 0.79443
2017-12-10T03:29:08.566314: step 1121, loss 0.461803, acc 0.875, prec 0.0383472, recall 0.794702
2017-12-10T03:29:08.830737: step 1122, loss 0.565167, acc 0.8125, prec 0.0383633, recall 0.794838
2017-12-10T03:29:09.092840: step 1123, loss 0.39744, acc 0.84375, prec 0.038351, recall 0.794838
2017-12-10T03:29:09.361278: step 1124, loss 0.169147, acc 0.921875, prec 0.0383449, recall 0.794838
2017-12-10T03:29:09.626333: step 1125, loss 0.728354, acc 0.875, prec 0.0383658, recall 0.794974
2017-12-10T03:29:09.889942: step 1126, loss 0.192477, acc 0.90625, prec 0.0383584, recall 0.794974
2017-12-10T03:29:10.157716: step 1127, loss 0.390758, acc 0.828125, prec 0.038345, recall 0.794974
2017-12-10T03:29:10.421892: step 1128, loss 3.86435, acc 0.890625, prec 0.0383683, recall 0.794584
2017-12-10T03:29:10.693404: step 1129, loss 0.945438, acc 0.90625, prec 0.038453, recall 0.79499
2017-12-10T03:29:10.965936: step 1130, loss 0.120927, acc 0.96875, prec 0.0384812, recall 0.795125
2017-12-10T03:29:11.234684: step 1131, loss 0.417384, acc 0.859375, prec 0.0384701, recall 0.795125
2017-12-10T03:29:11.495614: step 1132, loss 0.299254, acc 0.90625, prec 0.0384934, recall 0.79526
2017-12-10T03:29:11.765553: step 1133, loss 0.883425, acc 0.765625, prec 0.038475, recall 0.79526
2017-12-10T03:29:12.025042: step 1134, loss 0.772907, acc 0.796875, prec 0.0384591, recall 0.79526
2017-12-10T03:29:12.301733: step 1135, loss 0.896588, acc 0.765625, prec 0.0384407, recall 0.79526
2017-12-10T03:29:12.562535: step 1136, loss 0.328904, acc 0.890625, prec 0.0384934, recall 0.795529
2017-12-10T03:29:12.823809: step 1137, loss 1.19226, acc 0.671875, prec 0.0384982, recall 0.795664
2017-12-10T03:29:13.083087: step 1138, loss 0.645335, acc 0.875, prec 0.0384884, recall 0.795664
2017-12-10T03:29:13.347702: step 1139, loss 0.292754, acc 0.953125, prec 0.0384848, recall 0.795664
2017-12-10T03:29:13.610938: step 1140, loss 5.30453, acc 0.765625, prec 0.0384676, recall 0.795141
2017-12-10T03:29:13.877730: step 1141, loss 0.387683, acc 0.875, prec 0.0384884, recall 0.795276
2017-12-10T03:29:14.143534: step 1142, loss 0.34816, acc 0.875, prec 0.0384786, recall 0.795276
2017-12-10T03:29:14.408593: step 1143, loss 0.459711, acc 0.84375, prec 0.0385275, recall 0.795544
2017-12-10T03:29:14.681590: step 1144, loss 0.955133, acc 0.796875, prec 0.0385116, recall 0.795544
2017-12-10T03:29:14.941845: step 1145, loss 0.620481, acc 0.84375, prec 0.0385603, recall 0.795812
2017-12-10T03:29:15.207452: step 1146, loss 0.888066, acc 0.859375, prec 0.0386408, recall 0.796212
2017-12-10T03:29:15.471597: step 1147, loss 0.679299, acc 0.859375, prec 0.0386297, recall 0.796212
2017-12-10T03:29:15.731822: step 1148, loss 0.328922, acc 0.875, prec 0.0387418, recall 0.796743
2017-12-10T03:29:15.996385: step 1149, loss 0.322672, acc 0.890625, prec 0.0387636, recall 0.796875
2017-12-10T03:29:16.267624: step 1150, loss 0.843729, acc 0.71875, prec 0.038772, recall 0.797007
2017-12-10T03:29:16.532376: step 1151, loss 0.723103, acc 0.8125, prec 0.0388181, recall 0.797271
2017-12-10T03:29:16.798583: step 1152, loss 0.858815, acc 0.765625, prec 0.03883, recall 0.797403
2017-12-10T03:29:17.062507: step 1153, loss 0.657256, acc 0.8125, prec 0.0388457, recall 0.797534
2017-12-10T03:29:17.330892: step 1154, loss 0.178049, acc 0.921875, prec 0.0388699, recall 0.797665
2017-12-10T03:29:17.594885: step 1155, loss 0.263666, acc 0.921875, prec 0.0388638, recall 0.797665
2017-12-10T03:29:17.861863: step 1156, loss 0.268347, acc 0.90625, prec 0.0388868, recall 0.797796
2017-12-10T03:29:18.127179: step 1157, loss 0.257601, acc 0.890625, prec 0.0389389, recall 0.798058
2017-12-10T03:29:18.393388: step 1158, loss 0.121674, acc 0.953125, prec 0.0389655, recall 0.798189
2017-12-10T03:29:18.663442: step 1159, loss 0.367993, acc 0.859375, prec 0.0389848, recall 0.798319
2017-12-10T03:29:18.925641: step 1160, loss 0.203021, acc 0.9375, prec 0.0389799, recall 0.798319
2017-12-10T03:29:19.186823: step 1161, loss 2.97037, acc 0.90625, prec 0.0390041, recall 0.797934
2017-12-10T03:29:19.460414: step 1162, loss 0.819718, acc 0.9375, prec 0.0390295, recall 0.798065
2017-12-10T03:29:19.726035: step 1163, loss 0.171125, acc 0.90625, prec 0.0390221, recall 0.798065
2017-12-10T03:29:19.990002: step 1164, loss 0.854951, acc 0.859375, prec 0.0390413, recall 0.798195
2017-12-10T03:29:20.249781: step 1165, loss 0.241166, acc 0.9375, prec 0.0390667, recall 0.798325
2017-12-10T03:29:20.517993: step 1166, loss 0.997829, acc 0.90625, prec 0.0390896, recall 0.798455
2017-12-10T03:29:20.791297: step 1167, loss 2.84319, acc 0.875, prec 0.0391113, recall 0.798071
2017-12-10T03:29:21.056153: step 1168, loss 0.697783, acc 0.828125, prec 0.039128, recall 0.7982
2017-12-10T03:29:21.323470: step 1169, loss 0.549141, acc 0.78125, prec 0.0391107, recall 0.7982
2017-12-10T03:29:21.586450: step 1170, loss 0.485602, acc 0.828125, prec 0.0391274, recall 0.79833
2017-12-10T03:29:21.847992: step 1171, loss 0.827828, acc 0.8125, prec 0.0391126, recall 0.79833
2017-12-10T03:29:22.111301: step 1172, loss 0.872725, acc 0.78125, prec 0.0391559, recall 0.798589
2017-12-10T03:29:22.376806: step 1173, loss 0.704942, acc 0.796875, prec 0.0391399, recall 0.798589
2017-12-10T03:29:22.641971: step 1174, loss 1.0795, acc 0.703125, prec 0.0391165, recall 0.798589
2017-12-10T03:29:22.907501: step 1175, loss 0.778345, acc 0.796875, prec 0.0391307, recall 0.798718
2017-12-10T03:29:23.174262: step 1176, loss 0.715986, acc 0.75, prec 0.0391111, recall 0.798718
2017-12-10T03:29:23.434094: step 1177, loss 0.433332, acc 0.84375, prec 0.0390988, recall 0.798718
2017-12-10T03:29:23.698933: step 1178, loss 0.733588, acc 0.734375, prec 0.0391081, recall 0.798847
2017-12-10T03:29:23.965524: step 1179, loss 0.916544, acc 0.734375, prec 0.0391475, recall 0.799104
2017-12-10T03:29:24.230716: step 1180, loss 0.898769, acc 0.734375, prec 0.0391266, recall 0.799104
2017-12-10T03:29:24.499539: step 1181, loss 1.22494, acc 0.84375, prec 0.0391445, recall 0.799233
2017-12-10T03:29:24.768854: step 1182, loss 1.28829, acc 0.78125, prec 0.0391875, recall 0.799489
2017-12-10T03:29:25.046517: step 1183, loss 0.702132, acc 0.8125, prec 0.0391727, recall 0.799489
2017-12-10T03:29:25.306952: step 1184, loss 3.7736, acc 0.78125, prec 0.0391869, recall 0.799107
2017-12-10T03:29:25.580818: step 1185, loss 0.648723, acc 0.859375, prec 0.0392359, recall 0.799363
2017-12-10T03:29:25.842660: step 1186, loss 0.625037, acc 0.828125, prec 0.0392224, recall 0.799363
2017-12-10T03:29:26.109309: step 1187, loss 0.689373, acc 0.796875, prec 0.0392365, recall 0.799491
2017-12-10T03:29:26.369736: step 1188, loss 0.591363, acc 0.859375, prec 0.0392555, recall 0.799618
2017-12-10T03:29:26.647224: step 1189, loss 0.549578, acc 0.90625, prec 0.0392781, recall 0.799746
2017-12-10T03:29:26.918421: step 1190, loss 0.918244, acc 0.796875, prec 0.0393222, recall 0.8
2017-12-10T03:29:27.184111: step 1191, loss 0.570877, acc 0.796875, prec 0.0393062, recall 0.8
2017-12-10T03:29:27.451762: step 1192, loss 0.462301, acc 0.859375, prec 0.0392952, recall 0.8
2017-12-10T03:29:27.719780: step 1193, loss 4.59405, acc 0.796875, prec 0.0392805, recall 0.799492
2017-12-10T03:29:27.986758: step 1194, loss 0.811024, acc 0.75, prec 0.0392609, recall 0.799492
2017-12-10T03:29:28.259830: step 1195, loss 0.676703, acc 0.796875, prec 0.039245, recall 0.799492
2017-12-10T03:29:28.520973: step 1196, loss 0.587095, acc 0.875, prec 0.0392651, recall 0.79962
2017-12-10T03:29:28.795947: step 1197, loss 0.478653, acc 0.8125, prec 0.0392505, recall 0.79962
2017-12-10T03:29:29.070498: step 1198, loss 0.463578, acc 0.875, prec 0.0392407, recall 0.79962
2017-12-10T03:29:29.339300: step 1199, loss 0.487089, acc 0.90625, prec 0.0392633, recall 0.799747
2017-12-10T03:29:29.601615: step 1200, loss 0.651635, acc 0.828125, prec 0.0392498, recall 0.799747

Evaluation:
2017-12-10T03:29:37.090555: step 1200, loss 1.3687, acc 0.848476, prec 0.0406667, recall 0.795836

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-1200

2017-12-10T03:29:38.288317: step 1201, loss 0.461439, acc 0.84375, prec 0.0406547, recall 0.795836
2017-12-10T03:29:38.558420: step 1202, loss 0.51769, acc 0.84375, prec 0.0406994, recall 0.796072
2017-12-10T03:29:38.822191: step 1203, loss 0.548791, acc 0.859375, prec 0.0406886, recall 0.796072
2017-12-10T03:29:39.084498: step 1204, loss 0.710359, acc 0.84375, prec 0.0407049, recall 0.796189
2017-12-10T03:29:39.348537: step 1205, loss 0.37426, acc 0.890625, prec 0.0406965, recall 0.796189
2017-12-10T03:29:39.614296: step 1206, loss 0.470296, acc 0.796875, prec 0.0406809, recall 0.796189
2017-12-10T03:29:39.876835: step 1207, loss 0.288873, acc 0.890625, prec 0.0406725, recall 0.796189
2017-12-10T03:29:40.141756: step 1208, loss 0.181678, acc 0.921875, prec 0.0406665, recall 0.796189
2017-12-10T03:29:40.413302: step 1209, loss 0.246435, acc 0.890625, prec 0.0406864, recall 0.796307
2017-12-10T03:29:40.673486: step 1210, loss 0.906114, acc 0.921875, prec 0.0407369, recall 0.796542
2017-12-10T03:29:40.939688: step 1211, loss 0.316205, acc 0.921875, prec 0.0407875, recall 0.796776
2017-12-10T03:29:41.200583: step 1212, loss 0.371593, acc 0.921875, prec 0.0408097, recall 0.796893
2017-12-10T03:29:41.460049: step 1213, loss 0.0711252, acc 0.984375, prec 0.0408368, recall 0.79701
2017-12-10T03:29:41.734399: step 1214, loss 7.47237, acc 0.90625, prec 0.0408602, recall 0.796211
2017-12-10T03:29:42.009416: step 1215, loss 0.455271, acc 0.890625, prec 0.04088, recall 0.796328
2017-12-10T03:29:42.275677: step 1216, loss 0.177875, acc 0.9375, prec 0.0409035, recall 0.796445
2017-12-10T03:29:42.536933: step 1217, loss 0.430686, acc 0.859375, prec 0.0409209, recall 0.796562
2017-12-10T03:29:42.797361: step 1218, loss 1.39452, acc 0.84375, prec 0.0409935, recall 0.796911
2017-12-10T03:29:43.062962: step 1219, loss 0.702824, acc 0.796875, prec 0.041006, recall 0.797027
2017-12-10T03:29:43.326475: step 1220, loss 0.523484, acc 0.8125, prec 0.0409916, recall 0.797027
2017-12-10T03:29:43.592559: step 1221, loss 0.720981, acc 0.828125, prec 0.0409783, recall 0.797027
2017-12-10T03:29:43.854865: step 1222, loss 1.04265, acc 0.75, prec 0.0409872, recall 0.797143
2017-12-10T03:29:44.127313: step 1223, loss 1.00354, acc 0.75, prec 0.0410243, recall 0.797374
2017-12-10T03:29:44.396082: step 1224, loss 0.475441, acc 0.84375, prec 0.0410685, recall 0.797605
2017-12-10T03:29:44.660279: step 1225, loss 0.758577, acc 0.703125, prec 0.0410738, recall 0.797721
2017-12-10T03:29:44.920731: step 1226, loss 1.52003, acc 0.796875, prec 0.0411144, recall 0.797951
2017-12-10T03:29:45.197330: step 1227, loss 2.86367, acc 0.78125, prec 0.0411268, recall 0.797612
2017-12-10T03:29:45.469444: step 1228, loss 0.878428, acc 0.78125, prec 0.041138, recall 0.797727
2017-12-10T03:29:45.735211: step 1229, loss 0.82162, acc 0.765625, prec 0.04112, recall 0.797727
2017-12-10T03:29:46.005672: step 1230, loss 0.973493, acc 0.734375, prec 0.0410995, recall 0.797727
2017-12-10T03:29:46.263393: step 1231, loss 0.587428, acc 0.796875, prec 0.0410839, recall 0.797727
2017-12-10T03:29:46.522102: step 1232, loss 1.22713, acc 0.703125, prec 0.0410891, recall 0.797842
2017-12-10T03:29:46.783793: step 1233, loss 0.998626, acc 0.734375, prec 0.0410967, recall 0.797957
2017-12-10T03:29:47.043133: step 1234, loss 1.02092, acc 0.796875, prec 0.0410811, recall 0.797957
2017-12-10T03:29:47.307534: step 1235, loss 1.07199, acc 0.71875, prec 0.0411155, recall 0.798186
2017-12-10T03:29:47.569948: step 1236, loss 0.631978, acc 0.796875, prec 0.0410999, recall 0.798186
2017-12-10T03:29:47.843386: step 1237, loss 0.482883, acc 0.84375, prec 0.0410879, recall 0.798186
2017-12-10T03:29:48.106055: step 1238, loss 0.246051, acc 0.875, prec 0.0410783, recall 0.798186
2017-12-10T03:29:48.373870: step 1239, loss 0.680666, acc 0.796875, prec 0.0411187, recall 0.798414
2017-12-10T03:29:48.638568: step 1240, loss 0.687375, acc 0.828125, prec 0.0411055, recall 0.798414
2017-12-10T03:29:48.901305: step 1241, loss 1.77753, acc 0.8125, prec 0.0410923, recall 0.797963
2017-12-10T03:29:49.162916: step 1242, loss 1.08331, acc 0.828125, prec 0.041135, recall 0.798191
2017-12-10T03:29:49.432292: step 1243, loss 3.70217, acc 0.890625, prec 0.0411557, recall 0.797854
2017-12-10T03:29:49.711953: step 1244, loss 0.397196, acc 0.859375, prec 0.0411729, recall 0.797968
2017-12-10T03:29:49.974460: step 1245, loss 0.465739, acc 0.828125, prec 0.0412155, recall 0.798196
2017-12-10T03:29:50.236535: step 1246, loss 0.296329, acc 0.921875, prec 0.0412653, recall 0.798423
2017-12-10T03:29:50.501323: step 1247, loss 0.50641, acc 0.875, prec 0.0412557, recall 0.798423
2017-12-10T03:29:50.766623: step 1248, loss 1.29653, acc 0.859375, prec 0.0413564, recall 0.798876
2017-12-10T03:29:51.047463: step 1249, loss 2.36066, acc 0.828125, prec 0.0413723, recall 0.798541
2017-12-10T03:29:51.313308: step 1250, loss 0.593477, acc 0.8125, prec 0.0414136, recall 0.798767
2017-12-10T03:29:51.577614: step 1251, loss 0.736062, acc 0.796875, prec 0.0414258, recall 0.79888
2017-12-10T03:29:51.839186: step 1252, loss 1.55827, acc 0.65625, prec 0.0414272, recall 0.798992
2017-12-10T03:29:52.105245: step 1253, loss 0.578653, acc 0.8125, prec 0.0414127, recall 0.798992
2017-12-10T03:29:52.371098: step 1254, loss 1.06869, acc 0.65625, prec 0.0414141, recall 0.799105
2017-12-10T03:29:52.637217: step 1255, loss 1.03623, acc 0.734375, prec 0.0414493, recall 0.799329
2017-12-10T03:29:52.896102: step 1256, loss 0.93467, acc 0.71875, prec 0.0414277, recall 0.799329
2017-12-10T03:29:53.165371: step 1257, loss 1.09385, acc 0.75, prec 0.041464, recall 0.799553
2017-12-10T03:29:53.430135: step 1258, loss 5.52431, acc 0.8125, prec 0.0414785, recall 0.799219
2017-12-10T03:29:53.699044: step 1259, loss 0.967454, acc 0.734375, prec 0.0414859, recall 0.799331
2017-12-10T03:29:53.963143: step 1260, loss 0.441444, acc 0.875, prec 0.041504, recall 0.799443
2017-12-10T03:29:54.240688: step 1261, loss 0.805655, acc 0.75, prec 0.0415402, recall 0.799666
2017-12-10T03:29:54.503937: step 1262, loss 0.906812, acc 0.796875, prec 0.0415523, recall 0.799778
2017-12-10T03:29:54.768753: step 1263, loss 1.06756, acc 0.796875, prec 0.0416197, recall 0.800111
2017-12-10T03:29:55.035320: step 1264, loss 0.483491, acc 0.890625, prec 0.041639, recall 0.800222
2017-12-10T03:29:55.303353: step 1265, loss 0.721858, acc 0.859375, prec 0.0417112, recall 0.800554
2017-12-10T03:29:55.575265: step 1266, loss 0.6122, acc 0.765625, prec 0.0416931, recall 0.800554
2017-12-10T03:29:55.838220: step 1267, loss 6.39476, acc 0.859375, prec 0.0417388, recall 0.800332
2017-12-10T03:29:56.112377: step 1268, loss 0.533449, acc 0.8125, prec 0.0417243, recall 0.800332
2017-12-10T03:29:56.381396: step 1269, loss 0.850106, acc 0.703125, prec 0.0417291, recall 0.800442
2017-12-10T03:29:56.651808: step 1270, loss 0.987502, acc 0.78125, prec 0.0417399, recall 0.800552
2017-12-10T03:29:56.919077: step 1271, loss 1.10488, acc 0.734375, prec 0.0417746, recall 0.800773
2017-12-10T03:29:57.178408: step 1272, loss 1.09062, acc 0.78125, prec 0.0418129, recall 0.800992
2017-12-10T03:29:57.443933: step 1273, loss 0.950716, acc 0.8125, prec 0.0417985, recall 0.800992
2017-12-10T03:29:57.718400: step 1274, loss 1.52707, acc 0.65625, prec 0.0417721, recall 0.800992
2017-12-10T03:29:57.976413: step 1275, loss 1.21189, acc 0.59375, prec 0.0417409, recall 0.800992
2017-12-10T03:29:58.238820: step 1276, loss 1.03855, acc 0.75, prec 0.0417217, recall 0.800992
2017-12-10T03:29:58.507211: step 1277, loss 0.981489, acc 0.703125, prec 0.0417265, recall 0.801102
2017-12-10T03:29:58.775210: step 1278, loss 0.967089, acc 0.734375, prec 0.0417336, recall 0.801211
2017-12-10T03:29:59.040086: step 1279, loss 1.08359, acc 0.78125, prec 0.0417718, recall 0.80143
2017-12-10T03:29:59.303139: step 1280, loss 0.665591, acc 0.859375, prec 0.0418159, recall 0.801648
2017-12-10T03:29:59.571125: step 1281, loss 0.525254, acc 0.78125, prec 0.0418266, recall 0.801757
2017-12-10T03:29:59.842516: step 1282, loss 0.546814, acc 0.8125, prec 0.0418122, recall 0.801757
2017-12-10T03:30:00.116680: step 1283, loss 0.383589, acc 0.890625, prec 0.0418039, recall 0.801757
2017-12-10T03:30:00.387755: step 1284, loss 0.295056, acc 0.875, prec 0.0418491, recall 0.801975
2017-12-10T03:30:00.656015: step 1285, loss 0.317499, acc 0.890625, prec 0.0418682, recall 0.802083
2017-12-10T03:30:00.921499: step 1286, loss 0.0987408, acc 0.953125, prec 0.041892, recall 0.802192
2017-12-10T03:30:01.188256: step 1287, loss 0.481232, acc 0.9375, prec 0.0419146, recall 0.8023
2017-12-10T03:30:01.454208: step 1288, loss 0.21734, acc 0.90625, prec 0.0419074, recall 0.8023
2017-12-10T03:30:01.730007: step 1289, loss 0.160508, acc 0.9375, prec 0.04193, recall 0.802408
2017-12-10T03:30:01.997043: step 1290, loss 7.64969, acc 0.890625, prec 0.0419502, recall 0.802078
2017-12-10T03:30:02.266496: step 1291, loss 0.119591, acc 0.984375, prec 0.041949, recall 0.802078
2017-12-10T03:30:02.532309: step 1292, loss 0.274223, acc 0.90625, prec 0.0419692, recall 0.802186
2017-12-10T03:30:02.794500: step 1293, loss 0.243301, acc 0.921875, prec 0.0419632, recall 0.802186
2017-12-10T03:30:03.058973: step 1294, loss 0.182665, acc 0.9375, prec 0.0419858, recall 0.802294
2017-12-10T03:30:03.319899: step 1295, loss 0.318557, acc 0.921875, prec 0.0419798, recall 0.802294
2017-12-10T03:30:03.589561: step 1296, loss 0.284641, acc 0.90625, prec 0.042, recall 0.802402
2017-12-10T03:30:03.852905: step 1297, loss 0.253038, acc 0.921875, prec 0.041994, recall 0.802402
2017-12-10T03:30:04.116892: step 1298, loss 0.861891, acc 0.953125, prec 0.0420451, recall 0.802617
2017-12-10T03:30:04.383831: step 1299, loss 3.3409, acc 0.859375, prec 0.0420355, recall 0.80218
2017-12-10T03:30:04.648080: step 1300, loss 2.46631, acc 0.828125, prec 0.0420509, recall 0.801851
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-1300

2017-12-10T03:30:05.933724: step 1301, loss 0.325294, acc 0.90625, prec 0.0420437, recall 0.801851
2017-12-10T03:30:06.196310: step 1302, loss 0.417507, acc 0.859375, prec 0.0420602, recall 0.801959
2017-12-10T03:30:06.462955: step 1303, loss 0.938858, acc 0.828125, prec 0.042129, recall 0.802281
2017-12-10T03:30:06.721137: step 1304, loss 0.829128, acc 0.78125, prec 0.0421941, recall 0.802603
2017-12-10T03:30:06.980868: step 1305, loss 0.986371, acc 0.6875, prec 0.04217, recall 0.802603
2017-12-10T03:30:07.238596: step 1306, loss 0.958818, acc 0.75, prec 0.0421781, recall 0.80271
2017-12-10T03:30:07.507825: step 1307, loss 0.86126, acc 0.75, prec 0.0421589, recall 0.80271
2017-12-10T03:30:07.771207: step 1308, loss 1.32213, acc 0.671875, prec 0.0421337, recall 0.80271
2017-12-10T03:30:08.034630: step 1309, loss 0.685004, acc 0.75, prec 0.0421145, recall 0.80271
2017-12-10T03:30:08.292462: step 1310, loss 1.24207, acc 0.75, prec 0.0421226, recall 0.802817
2017-12-10T03:30:08.556231: step 1311, loss 0.982019, acc 0.71875, prec 0.0421283, recall 0.802924
2017-12-10T03:30:08.819590: step 1312, loss 0.97407, acc 0.6875, prec 0.0421316, recall 0.80303
2017-12-10T03:30:09.082686: step 1313, loss 0.48234, acc 0.828125, prec 0.0421728, recall 0.803243
2017-12-10T03:30:09.346869: step 1314, loss 0.786289, acc 0.765625, prec 0.042182, recall 0.80335
2017-12-10T03:30:09.611873: step 1315, loss 1.67682, acc 0.859375, prec 0.0421996, recall 0.803022
2017-12-10T03:30:09.878828: step 1316, loss 0.796071, acc 0.796875, prec 0.042184, recall 0.803022
2017-12-10T03:30:10.138487: step 1317, loss 0.379021, acc 0.90625, prec 0.0421769, recall 0.803022
2017-12-10T03:30:10.409189: step 1318, loss 0.592858, acc 0.78125, prec 0.0421873, recall 0.803128
2017-12-10T03:30:10.672653: step 1319, loss 0.381243, acc 0.90625, prec 0.0421801, recall 0.803128
2017-12-10T03:30:10.930502: step 1320, loss 0.268594, acc 0.890625, prec 0.0421717, recall 0.803128
2017-12-10T03:30:11.198863: step 1321, loss 0.718468, acc 0.78125, prec 0.0422093, recall 0.80334
2017-12-10T03:30:11.463268: step 1322, loss 0.309853, acc 0.9375, prec 0.0422316, recall 0.803446
2017-12-10T03:30:11.736618: step 1323, loss 0.190685, acc 0.9375, prec 0.042281, recall 0.803658
2017-12-10T03:30:11.999966: step 1324, loss 0.335208, acc 0.890625, prec 0.0422997, recall 0.803763
2017-12-10T03:30:12.264744: step 1325, loss 0.179245, acc 0.921875, prec 0.0422938, recall 0.803763
2017-12-10T03:30:12.525322: step 1326, loss 0.685013, acc 0.984375, prec 0.0423197, recall 0.803869
2017-12-10T03:30:12.794751: step 1327, loss 18.1066, acc 0.9375, prec 0.0423432, recall 0.803543
2017-12-10T03:30:13.064569: step 1328, loss 0.215327, acc 0.9375, prec 0.0423384, recall 0.803543
2017-12-10T03:30:13.323199: step 1329, loss 0.103535, acc 0.953125, prec 0.0423348, recall 0.803543
2017-12-10T03:30:13.592570: step 1330, loss 0.171649, acc 0.921875, prec 0.0423288, recall 0.803543
2017-12-10T03:30:13.859737: step 1331, loss 0.263484, acc 0.875, prec 0.0423463, recall 0.803648
2017-12-10T03:30:14.124741: step 1332, loss 0.464244, acc 0.875, prec 0.0423908, recall 0.803859
2017-12-10T03:30:14.390731: step 1333, loss 0.553823, acc 0.859375, prec 0.0424071, recall 0.803964
2017-12-10T03:30:14.650228: step 1334, loss 0.507468, acc 0.84375, prec 0.0424222, recall 0.804069
2017-12-10T03:30:14.917715: step 1335, loss 0.426394, acc 0.890625, prec 0.0424138, recall 0.804069
2017-12-10T03:30:15.191708: step 1336, loss 0.445051, acc 0.9375, prec 0.0424631, recall 0.804278
2017-12-10T03:30:15.451015: step 1337, loss 0.426715, acc 0.875, prec 0.0424805, recall 0.804383
2017-12-10T03:30:15.712005: step 1338, loss 0.728189, acc 0.796875, prec 0.042519, recall 0.804592
2017-12-10T03:30:15.974383: step 1339, loss 1.36987, acc 0.875, prec 0.0425106, recall 0.804162
2017-12-10T03:30:16.239902: step 1340, loss 0.616654, acc 0.8125, prec 0.0424962, recall 0.804162
2017-12-10T03:30:16.506877: step 1341, loss 0.313277, acc 0.859375, prec 0.0424854, recall 0.804162
2017-12-10T03:30:16.772196: step 1342, loss 0.531899, acc 0.859375, prec 0.0424746, recall 0.804162
2017-12-10T03:30:17.033157: step 1343, loss 0.416258, acc 0.859375, prec 0.0424908, recall 0.804267
2017-12-10T03:30:17.309292: step 1344, loss 0.997899, acc 0.8125, prec 0.0425035, recall 0.804371
2017-12-10T03:30:17.569135: step 1345, loss 5.80301, acc 0.890625, prec 0.0425232, recall 0.804047
2017-12-10T03:30:17.841636: step 1346, loss 0.596775, acc 0.859375, prec 0.0425664, recall 0.804255
2017-12-10T03:30:18.117319: step 1347, loss 2.87847, acc 0.828125, prec 0.0425544, recall 0.803828
2017-12-10T03:30:18.382200: step 1348, loss 0.835252, acc 0.796875, prec 0.0425927, recall 0.804036
2017-12-10T03:30:18.648037: step 1349, loss 0.533724, acc 0.875, prec 0.04261, recall 0.80414
2017-12-10T03:30:18.919713: step 1350, loss 0.611042, acc 0.8125, prec 0.0426226, recall 0.804244
2017-12-10T03:30:19.183352: step 1351, loss 1.32186, acc 0.6875, prec 0.0425986, recall 0.804244
2017-12-10T03:30:19.445825: step 1352, loss 0.858241, acc 0.75, prec 0.0426064, recall 0.804348
2017-12-10T03:30:19.710030: step 1353, loss 1.49298, acc 0.65625, prec 0.0425801, recall 0.804348
2017-12-10T03:30:19.968785: step 1354, loss 1.2802, acc 0.765625, prec 0.0426696, recall 0.804762
2017-12-10T03:30:20.235055: step 1355, loss 0.902476, acc 0.75, prec 0.0426773, recall 0.804865
2017-12-10T03:30:20.504263: step 1356, loss 1.47098, acc 0.6875, prec 0.0426534, recall 0.804865
2017-12-10T03:30:20.775027: step 1357, loss 1.16809, acc 0.8125, prec 0.0427463, recall 0.805277
2017-12-10T03:30:21.042062: step 1358, loss 1.61713, acc 0.703125, prec 0.0427236, recall 0.805277
2017-12-10T03:30:21.299939: step 1359, loss 1.07537, acc 0.734375, prec 0.0427568, recall 0.805482
2017-12-10T03:30:21.573013: step 1360, loss 0.687929, acc 0.75, prec 0.0427644, recall 0.805585
2017-12-10T03:30:21.836321: step 1361, loss 0.862762, acc 0.75, prec 0.0427453, recall 0.805585
2017-12-10T03:30:22.095137: step 1362, loss 3.69979, acc 0.71875, prec 0.042725, recall 0.805161
2017-12-10T03:30:22.362543: step 1363, loss 0.66068, acc 0.8125, prec 0.0427374, recall 0.805263
2017-12-10T03:30:22.632494: step 1364, loss 0.368218, acc 0.8125, prec 0.0428033, recall 0.80557
2017-12-10T03:30:22.899940: step 1365, loss 1.92964, acc 0.875, prec 0.0428484, recall 0.805351
2017-12-10T03:30:23.171689: step 1366, loss 0.730426, acc 0.8125, prec 0.042834, recall 0.805351
2017-12-10T03:30:23.436671: step 1367, loss 0.599761, acc 0.796875, prec 0.0428452, recall 0.805454
2017-12-10T03:30:23.709043: step 1368, loss 0.570685, acc 0.859375, prec 0.0428878, recall 0.805657
2017-12-10T03:30:23.976863: step 1369, loss 0.657635, acc 0.84375, prec 0.0429292, recall 0.805861
2017-12-10T03:30:24.243399: step 1370, loss 0.668355, acc 0.796875, prec 0.0429137, recall 0.805861
2017-12-10T03:30:24.507975: step 1371, loss 0.598018, acc 0.875, prec 0.0429574, recall 0.806064
2017-12-10T03:30:24.778878: step 1372, loss 0.520767, acc 0.84375, prec 0.0429721, recall 0.806165
2017-12-10T03:30:25.051892: step 1373, loss 0.506742, acc 0.90625, prec 0.0430182, recall 0.806367
2017-12-10T03:30:25.313286: step 1374, loss 1.72534, acc 0.84375, prec 0.0430075, recall 0.805947
2017-12-10T03:30:25.582169: step 1375, loss 0.596536, acc 0.875, prec 0.0430511, recall 0.806149
2017-12-10T03:30:25.854687: step 1376, loss 0.557909, acc 0.84375, prec 0.0430924, recall 0.806351
2017-12-10T03:30:26.116536: step 1377, loss 0.665794, acc 0.875, prec 0.0431361, recall 0.806552
2017-12-10T03:30:26.385057: step 1378, loss 0.340584, acc 0.90625, prec 0.0431555, recall 0.806653
2017-12-10T03:30:26.657643: step 1379, loss 0.382932, acc 0.859375, prec 0.0431447, recall 0.806653
2017-12-10T03:30:26.929624: step 1380, loss 2.34938, acc 0.796875, prec 0.0431569, recall 0.806334
2017-12-10T03:30:27.197889: step 1381, loss 0.440407, acc 0.90625, prec 0.0431497, recall 0.806334
2017-12-10T03:30:27.468857: step 1382, loss 0.501597, acc 0.84375, prec 0.0431377, recall 0.806334
2017-12-10T03:30:27.740838: step 1383, loss 0.507897, acc 0.84375, prec 0.0431257, recall 0.806334
2017-12-10T03:30:28.004863: step 1384, loss 0.501587, acc 0.796875, prec 0.0431367, recall 0.806435
2017-12-10T03:30:28.264922: step 1385, loss 0.808183, acc 0.796875, prec 0.0431477, recall 0.806535
2017-12-10T03:30:28.541287: step 1386, loss 1.22779, acc 0.78125, prec 0.043184, recall 0.806736
2017-12-10T03:30:28.821868: step 1387, loss 1.15905, acc 0.796875, prec 0.043195, recall 0.806836
2017-12-10T03:30:29.081929: step 1388, loss 0.959172, acc 0.84375, prec 0.0432626, recall 0.807135
2017-12-10T03:30:29.347786: step 1389, loss 0.799209, acc 0.78125, prec 0.0432723, recall 0.807235
2017-12-10T03:30:29.612631: step 1390, loss 0.951015, acc 0.75, prec 0.0432531, recall 0.807235
2017-12-10T03:30:29.880356: step 1391, loss 0.777819, acc 0.78125, prec 0.0432628, recall 0.807335
2017-12-10T03:30:30.156497: step 1392, loss 0.314873, acc 0.921875, prec 0.0432569, recall 0.807335
2017-12-10T03:30:30.423472: step 1393, loss 0.497414, acc 0.8125, prec 0.0432425, recall 0.807335
2017-12-10T03:30:30.690785: step 1394, loss 0.221582, acc 0.9375, prec 0.0432377, recall 0.807335
2017-12-10T03:30:30.950866: step 1395, loss 0.635771, acc 0.859375, prec 0.0432269, recall 0.807335
2017-12-10T03:30:31.211964: step 1396, loss 0.437368, acc 0.859375, prec 0.0432162, recall 0.807335
2017-12-10T03:30:31.475878: step 1397, loss 0.187679, acc 0.9375, prec 0.0432114, recall 0.807335
2017-12-10T03:30:31.739471: step 1398, loss 0.236199, acc 0.875, prec 0.0432019, recall 0.807335
2017-12-10T03:30:31.999781: step 1399, loss 4.44207, acc 0.875, prec 0.0432728, recall 0.807216
2017-12-10T03:30:32.273401: step 1400, loss 0.194295, acc 0.921875, prec 0.0432668, recall 0.807216
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-1400

2017-12-10T03:30:33.509480: step 1401, loss 0.370214, acc 0.859375, prec 0.0432825, recall 0.807316
2017-12-10T03:30:33.780639: step 1402, loss 0.293784, acc 0.84375, prec 0.0432706, recall 0.807316
2017-12-10T03:30:34.045191: step 1403, loss 0.316204, acc 0.9375, prec 0.043345, recall 0.807613
2017-12-10T03:30:34.311769: step 1404, loss 0.108573, acc 0.953125, prec 0.0433414, recall 0.807613
2017-12-10T03:30:34.575178: step 1405, loss 0.415087, acc 0.921875, prec 0.0433354, recall 0.807613
2017-12-10T03:30:34.839625: step 1406, loss 0.849479, acc 0.875, prec 0.0433523, recall 0.807712
2017-12-10T03:30:35.103895: step 1407, loss 0.177927, acc 0.90625, prec 0.0433451, recall 0.807712
2017-12-10T03:30:35.376640: step 1408, loss 0.708998, acc 0.953125, prec 0.0433679, recall 0.807811
2017-12-10T03:30:35.645549: step 1409, loss 1.73742, acc 0.859375, prec 0.0433583, recall 0.807396
2017-12-10T03:30:35.916981: step 1410, loss 0.685262, acc 0.859375, prec 0.0434003, recall 0.807594
2017-12-10T03:30:36.177191: step 1411, loss 0.161947, acc 0.921875, prec 0.0433944, recall 0.807594
2017-12-10T03:30:36.443977: step 1412, loss 3.88254, acc 0.875, prec 0.0434387, recall 0.807377
2017-12-10T03:30:36.712805: step 1413, loss 0.5734, acc 0.859375, prec 0.0434543, recall 0.807476
2017-12-10T03:30:36.979432: step 1414, loss 0.805247, acc 0.78125, prec 0.0434639, recall 0.807574
2017-12-10T03:30:37.250491: step 1415, loss 0.583143, acc 0.828125, prec 0.0434507, recall 0.807574
2017-12-10T03:30:37.519118: step 1416, loss 1.12197, acc 0.765625, prec 0.0434328, recall 0.807574
2017-12-10T03:30:37.783975: step 1417, loss 0.988665, acc 0.765625, prec 0.0434675, recall 0.807771
2017-12-10T03:30:38.043132: step 1418, loss 0.757599, acc 0.75, prec 0.0434484, recall 0.807771
2017-12-10T03:30:38.303260: step 1419, loss 0.982419, acc 0.703125, prec 0.0434783, recall 0.807967
2017-12-10T03:30:38.565930: step 1420, loss 0.999726, acc 0.6875, prec 0.0434544, recall 0.807967
2017-12-10T03:30:38.832721: step 1421, loss 1.06317, acc 0.703125, prec 0.043458, recall 0.808065
2017-12-10T03:30:39.094275: step 1422, loss 1.17139, acc 0.734375, prec 0.0434639, recall 0.808163
2017-12-10T03:30:39.355147: step 1423, loss 0.925631, acc 0.6875, prec 0.0434401, recall 0.808163
2017-12-10T03:30:39.618774: step 1424, loss 0.745715, acc 0.796875, prec 0.0434246, recall 0.808163
2017-12-10T03:30:39.881937: step 1425, loss 0.842128, acc 0.703125, prec 0.0434282, recall 0.808261
2017-12-10T03:30:40.138320: step 1426, loss 0.466728, acc 0.84375, prec 0.0434163, recall 0.808261
2017-12-10T03:30:40.409072: step 1427, loss 0.483254, acc 0.8125, prec 0.0434021, recall 0.808261
2017-12-10T03:30:40.670669: step 1428, loss 0.387673, acc 0.875, prec 0.0434187, recall 0.808359
2017-12-10T03:30:40.930353: step 1429, loss 0.375356, acc 0.828125, prec 0.0434057, recall 0.808359
2017-12-10T03:30:41.189408: step 1430, loss 0.859052, acc 0.828125, prec 0.043445, recall 0.808554
2017-12-10T03:30:41.462411: step 1431, loss 0.35763, acc 0.90625, prec 0.043464, recall 0.808651
2017-12-10T03:30:41.739694: step 1432, loss 0.166283, acc 0.9375, prec 0.0434854, recall 0.808749
2017-12-10T03:30:42.000491: step 1433, loss 0.191476, acc 0.921875, prec 0.0434795, recall 0.808749
2017-12-10T03:30:42.265375: step 1434, loss 0.104255, acc 0.96875, prec 0.0434771, recall 0.808749
2017-12-10T03:30:42.530670: step 1435, loss 0.544942, acc 0.953125, prec 0.0434997, recall 0.808846
2017-12-10T03:30:42.809131: step 1436, loss 5.15872, acc 0.875, prec 0.0435448, recall 0.808219
2017-12-10T03:30:43.076927: step 1437, loss 0.158753, acc 0.953125, prec 0.0435412, recall 0.808219
2017-12-10T03:30:43.350904: step 1438, loss 0.183685, acc 0.984375, prec 0.0435923, recall 0.808414
2017-12-10T03:30:43.625305: step 1439, loss 0.217202, acc 0.921875, prec 0.0435864, recall 0.808414
2017-12-10T03:30:43.897545: step 1440, loss 0.207965, acc 0.90625, prec 0.0435792, recall 0.808414
2017-12-10T03:30:44.165908: step 1441, loss 0.378064, acc 0.890625, prec 0.043597, recall 0.808511
2017-12-10T03:30:44.425611: step 1442, loss 0.373179, acc 0.90625, prec 0.0435899, recall 0.808511
2017-12-10T03:30:44.688599: step 1443, loss 0.323648, acc 0.90625, prec 0.0435827, recall 0.808511
2017-12-10T03:30:44.954796: step 1444, loss 0.279712, acc 0.90625, prec 0.0435756, recall 0.808511
2017-12-10T03:30:45.221630: step 1445, loss 0.405427, acc 0.875, prec 0.0435661, recall 0.808511
2017-12-10T03:30:45.496234: step 1446, loss 0.254971, acc 0.9375, prec 0.0435613, recall 0.808511
2017-12-10T03:30:45.757362: step 1447, loss 6.343, acc 0.875, prec 0.043553, recall 0.808101
2017-12-10T03:30:46.025605: step 1448, loss 1.53173, acc 0.828125, prec 0.0435411, recall 0.807692
2017-12-10T03:30:46.295874: step 1449, loss 0.658101, acc 0.8125, prec 0.0435269, recall 0.807692
2017-12-10T03:30:46.558760: step 1450, loss 0.73173, acc 0.703125, prec 0.0435043, recall 0.807692
2017-12-10T03:30:46.822421: step 1451, loss 0.932231, acc 0.6875, prec 0.0435067, recall 0.80779
2017-12-10T03:30:47.082694: step 1452, loss 0.964637, acc 0.71875, prec 0.0434854, recall 0.80779
2017-12-10T03:30:47.346532: step 1453, loss 1.14415, acc 0.703125, prec 0.0434629, recall 0.80779
2017-12-10T03:30:47.607924: step 1454, loss 1.26068, acc 0.59375, prec 0.0434321, recall 0.80779
2017-12-10T03:30:47.874613: step 1455, loss 1.20567, acc 0.71875, prec 0.0434109, recall 0.80779
2017-12-10T03:30:48.136376: step 1456, loss 4.85818, acc 0.625, prec 0.0433838, recall 0.807381
2017-12-10T03:30:48.406831: step 1457, loss 0.962341, acc 0.703125, prec 0.0433874, recall 0.807479
2017-12-10T03:30:48.668419: step 1458, loss 0.648789, acc 0.78125, prec 0.0433709, recall 0.807479
2017-12-10T03:30:48.932701: step 1459, loss 0.947858, acc 0.65625, prec 0.0433709, recall 0.807576
2017-12-10T03:30:49.197046: step 1460, loss 1.68285, acc 0.71875, prec 0.0433757, recall 0.807673
2017-12-10T03:30:49.459305: step 1461, loss 1.03379, acc 0.71875, prec 0.0433546, recall 0.807673
2017-12-10T03:30:49.732699: step 1462, loss 0.951174, acc 0.671875, prec 0.0433558, recall 0.80777
2017-12-10T03:30:50.002234: step 1463, loss 1.06522, acc 0.71875, prec 0.0433606, recall 0.807867
2017-12-10T03:30:50.269985: step 1464, loss 0.673197, acc 0.8125, prec 0.0433465, recall 0.807867
2017-12-10T03:30:50.535704: step 1465, loss 0.562312, acc 0.78125, prec 0.0433301, recall 0.807867
2017-12-10T03:30:50.797564: step 1466, loss 0.877183, acc 0.78125, prec 0.0433137, recall 0.807867
2017-12-10T03:30:51.061228: step 1467, loss 1.16882, acc 0.828125, prec 0.0433784, recall 0.808157
2017-12-10T03:30:51.330250: step 1468, loss 0.81984, acc 0.8125, prec 0.0433643, recall 0.808157
2017-12-10T03:30:51.595836: step 1469, loss 1.25333, acc 0.8125, prec 0.0433761, recall 0.808254
2017-12-10T03:30:51.861573: step 1470, loss 0.465091, acc 0.8125, prec 0.0433879, recall 0.80835
2017-12-10T03:30:52.129545: step 1471, loss 0.220767, acc 0.953125, prec 0.0433844, recall 0.80835
2017-12-10T03:30:52.396256: step 1472, loss 0.0552561, acc 0.984375, prec 0.0433832, recall 0.80835
2017-12-10T03:30:52.663915: step 1473, loss 0.149043, acc 0.96875, prec 0.0433808, recall 0.80835
2017-12-10T03:30:52.923438: step 1474, loss 0.19439, acc 0.921875, prec 0.043375, recall 0.80835
2017-12-10T03:30:53.188899: step 1475, loss 0.461365, acc 0.84375, prec 0.0433633, recall 0.80835
2017-12-10T03:30:53.464259: step 1476, loss 0.306427, acc 0.875, prec 0.0433539, recall 0.80835
2017-12-10T03:30:53.733408: step 1477, loss 0.220112, acc 0.921875, prec 0.0433739, recall 0.808446
2017-12-10T03:30:53.997749: step 1478, loss 2.69845, acc 0.921875, prec 0.043395, recall 0.808137
2017-12-10T03:30:54.266650: step 1479, loss 2.40346, acc 0.984375, prec 0.0434208, recall 0.807827
2017-12-10T03:30:54.533846: step 1480, loss 0.119524, acc 0.96875, prec 0.0434185, recall 0.807827
2017-12-10T03:30:54.797856: step 1481, loss 11.398, acc 0.84375, prec 0.0434091, recall 0.807018
2017-12-10T03:30:55.062079: step 1482, loss 0.285943, acc 0.890625, prec 0.0434267, recall 0.807114
2017-12-10T03:30:55.324394: step 1483, loss 1.60091, acc 0.734375, prec 0.0434583, recall 0.807307
2017-12-10T03:30:55.590721: step 1484, loss 1.23877, acc 0.625, prec 0.0434303, recall 0.807307
2017-12-10T03:30:55.855457: step 1485, loss 1.54903, acc 0.59375, prec 0.0434256, recall 0.807404
2017-12-10T03:30:56.117639: step 1486, loss 1.37709, acc 0.703125, prec 0.0434549, recall 0.807596
2017-12-10T03:30:56.380148: step 1487, loss 2.32282, acc 0.484375, prec 0.0434421, recall 0.807692
2017-12-10T03:30:56.648111: step 1488, loss 1.56483, acc 0.609375, prec 0.0434129, recall 0.807692
2017-12-10T03:30:56.915718: step 1489, loss 2.76977, acc 0.40625, prec 0.0433686, recall 0.807692
2017-12-10T03:30:57.187917: step 1490, loss 2.60471, acc 0.453125, prec 0.0433792, recall 0.807884
2017-12-10T03:30:57.412571: step 1491, loss 2.0662, acc 0.431373, prec 0.0433712, recall 0.80798
2017-12-10T03:30:57.686517: step 1492, loss 1.47656, acc 0.53125, prec 0.0433364, recall 0.80798
2017-12-10T03:30:57.945512: step 1493, loss 1.526, acc 0.59375, prec 0.0433574, recall 0.808171
2017-12-10T03:30:58.219774: step 1494, loss 1.60423, acc 0.578125, prec 0.0433517, recall 0.808267
2017-12-10T03:30:58.480984: step 1495, loss 1.36016, acc 0.625, prec 0.043375, recall 0.808458
2017-12-10T03:30:58.738271: step 1496, loss 0.640075, acc 0.796875, prec 0.0433855, recall 0.808553
2017-12-10T03:30:58.999785: step 1497, loss 0.70716, acc 0.796875, prec 0.0433704, recall 0.808553
2017-12-10T03:30:59.268598: step 1498, loss 0.456869, acc 0.890625, prec 0.0434133, recall 0.808743
2017-12-10T03:30:59.529389: step 1499, loss 0.547476, acc 0.859375, prec 0.0434029, recall 0.808743
2017-12-10T03:30:59.796791: step 1500, loss 0.362885, acc 0.859375, prec 0.0433925, recall 0.808743

Evaluation:
2017-12-10T03:31:07.496768: step 1500, loss 2.14908, acc 0.929144, prec 0.0447399, recall 0.79159

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-1500

2017-12-10T03:31:08.876675: step 1501, loss 0.320326, acc 0.90625, prec 0.0447329, recall 0.79159
2017-12-10T03:31:09.142953: step 1502, loss 0.203182, acc 0.921875, prec 0.044727, recall 0.79159
2017-12-10T03:31:09.411450: step 1503, loss 6.47568, acc 0.9375, prec 0.0447235, recall 0.791224
2017-12-10T03:31:09.677862: step 1504, loss 0.158069, acc 0.9375, prec 0.0447438, recall 0.79132
2017-12-10T03:31:09.943938: step 1505, loss 0.952819, acc 0.953125, prec 0.0447901, recall 0.791513
2017-12-10T03:31:10.211243: step 1506, loss 0.171441, acc 0.9375, prec 0.0448104, recall 0.791609
2017-12-10T03:31:10.477866: step 1507, loss 0.574048, acc 0.9375, prec 0.0448556, recall 0.791801
2017-12-10T03:31:10.748651: step 1508, loss 0.356542, acc 0.921875, prec 0.0448497, recall 0.791801
2017-12-10T03:31:11.008819: step 1509, loss 0.30123, acc 0.90625, prec 0.0448427, recall 0.791801
2017-12-10T03:31:11.267680: step 1510, loss 0.15109, acc 0.90625, prec 0.0448357, recall 0.791801
2017-12-10T03:31:11.539830: step 1511, loss 0.575976, acc 0.921875, prec 0.0448547, recall 0.791897
2017-12-10T03:31:11.815444: step 1512, loss 5.8496, acc 0.9375, prec 0.0448512, recall 0.791532
2017-12-10T03:31:12.079357: step 1513, loss 3.54333, acc 0.8125, prec 0.0448644, recall 0.790901
2017-12-10T03:31:12.344649: step 1514, loss 0.638379, acc 0.78125, prec 0.0448979, recall 0.791093
2017-12-10T03:31:12.606421: step 1515, loss 0.670199, acc 0.796875, prec 0.0448826, recall 0.791093
2017-12-10T03:31:12.872959: step 1516, loss 1.00372, acc 0.671875, prec 0.0449078, recall 0.791284
2017-12-10T03:31:13.136453: step 1517, loss 0.93581, acc 0.765625, prec 0.0449152, recall 0.79138
2017-12-10T03:31:13.402451: step 1518, loss 1.24879, acc 0.65625, prec 0.0448895, recall 0.79138
2017-12-10T03:31:13.665141: step 1519, loss 1.30634, acc 0.6875, prec 0.0448661, recall 0.79138
2017-12-10T03:31:13.930534: step 1520, loss 1.27085, acc 0.65625, prec 0.0448405, recall 0.79138
2017-12-10T03:31:14.196632: step 1521, loss 0.987629, acc 0.65625, prec 0.0448149, recall 0.79138
2017-12-10T03:31:14.454685: step 1522, loss 1.19207, acc 0.65625, prec 0.0448141, recall 0.791476
2017-12-10T03:31:14.717511: step 1523, loss 1.81217, acc 0.640625, prec 0.0448121, recall 0.791571
2017-12-10T03:31:14.981135: step 1524, loss 1.52357, acc 0.609375, prec 0.0448078, recall 0.791667
2017-12-10T03:31:15.244322: step 1525, loss 1.26368, acc 0.625, prec 0.0448295, recall 0.791857
2017-12-10T03:31:15.504574: step 1526, loss 0.775785, acc 0.765625, prec 0.0448121, recall 0.791857
2017-12-10T03:31:15.765345: step 1527, loss 0.942551, acc 0.75, prec 0.0448182, recall 0.791952
2017-12-10T03:31:16.029531: step 1528, loss 0.485398, acc 0.828125, prec 0.0448302, recall 0.792048
2017-12-10T03:31:16.291098: step 1529, loss 0.271169, acc 0.875, prec 0.0448456, recall 0.792143
2017-12-10T03:31:16.553852: step 1530, loss 0.428303, acc 0.90625, prec 0.0449127, recall 0.792427
2017-12-10T03:31:16.817985: step 1531, loss 0.124675, acc 0.96875, prec 0.0449351, recall 0.792522
2017-12-10T03:31:17.082381: step 1532, loss 0.357828, acc 0.828125, prec 0.0449223, recall 0.792522
2017-12-10T03:31:17.349072: step 1533, loss 0.383264, acc 0.921875, prec 0.0449165, recall 0.792522
2017-12-10T03:31:17.611527: step 1534, loss 0.0648758, acc 0.984375, prec 0.04494, recall 0.792616
2017-12-10T03:31:17.876329: step 1535, loss 0.07795, acc 0.984375, prec 0.0449882, recall 0.792805
2017-12-10T03:31:18.151317: step 1536, loss 2.8902, acc 0.953125, prec 0.0449859, recall 0.792444
2017-12-10T03:31:18.420262: step 1537, loss 0.114906, acc 0.96875, prec 0.0449836, recall 0.792444
2017-12-10T03:31:18.683337: step 1538, loss 0.589155, acc 1, prec 0.0450083, recall 0.792539
2017-12-10T03:31:18.948357: step 1539, loss 0.242014, acc 0.953125, prec 0.0450295, recall 0.792633
2017-12-10T03:31:19.226635: step 1540, loss 0.106655, acc 0.953125, prec 0.045026, recall 0.792633
2017-12-10T03:31:19.488222: step 1541, loss 0.343997, acc 0.953125, prec 0.0450718, recall 0.792821
2017-12-10T03:31:19.759665: step 1542, loss 0.0165024, acc 1, prec 0.0450718, recall 0.792821
2017-12-10T03:31:20.024465: step 1543, loss 2.35483, acc 0.953125, prec 0.0450941, recall 0.792556
2017-12-10T03:31:20.296098: step 1544, loss 0.33912, acc 0.90625, prec 0.0451118, recall 0.79265
2017-12-10T03:31:20.561535: step 1545, loss 0.276108, acc 0.921875, prec 0.0451306, recall 0.792744
2017-12-10T03:31:20.829456: step 1546, loss 0.582792, acc 0.8125, prec 0.0451167, recall 0.792744
2017-12-10T03:31:21.090577: step 1547, loss 0.345, acc 0.859375, prec 0.0451062, recall 0.792744
2017-12-10T03:31:21.356599: step 1548, loss 0.139173, acc 0.9375, prec 0.0451015, recall 0.792744
2017-12-10T03:31:21.620711: step 1549, loss 0.928992, acc 0.796875, prec 0.045111, recall 0.792838
2017-12-10T03:31:21.883432: step 1550, loss 0.300245, acc 0.921875, prec 0.0451298, recall 0.792932
2017-12-10T03:31:22.142956: step 1551, loss 0.225195, acc 0.875, prec 0.0451452, recall 0.793025
2017-12-10T03:31:22.411308: step 1552, loss 0.508238, acc 0.890625, prec 0.0452108, recall 0.793306
2017-12-10T03:31:22.674869: step 1553, loss 0.447065, acc 0.84375, prec 0.0451992, recall 0.793306
2017-12-10T03:31:22.937897: step 1554, loss 0.347122, acc 0.84375, prec 0.0452121, recall 0.7934
2017-12-10T03:31:23.209480: step 1555, loss 0.403957, acc 0.921875, prec 0.0452309, recall 0.793493
2017-12-10T03:31:23.480912: step 1556, loss 0.273174, acc 0.90625, prec 0.0452485, recall 0.793586
2017-12-10T03:31:23.748206: step 1557, loss 0.68149, acc 0.8125, prec 0.0452837, recall 0.793773
2017-12-10T03:31:24.024939: step 1558, loss 3.95311, acc 0.90625, prec 0.0452779, recall 0.793415
2017-12-10T03:31:24.294206: step 1559, loss 0.395959, acc 0.890625, prec 0.0452943, recall 0.793508
2017-12-10T03:31:24.557601: step 1560, loss 0.368621, acc 0.859375, prec 0.0452838, recall 0.793508
2017-12-10T03:31:24.823004: step 1561, loss 0.933728, acc 0.875, prec 0.045299, recall 0.793601
2017-12-10T03:31:25.086780: step 1562, loss 0.713789, acc 0.8125, prec 0.0452851, recall 0.793601
2017-12-10T03:31:25.347714: step 1563, loss 0.46392, acc 0.828125, prec 0.0452722, recall 0.793601
2017-12-10T03:31:25.615243: step 1564, loss 2.11989, acc 0.859375, prec 0.0452875, recall 0.793336
2017-12-10T03:31:25.883799: step 1565, loss 0.346089, acc 0.90625, prec 0.0453296, recall 0.793522
2017-12-10T03:31:26.156639: step 1566, loss 0.650435, acc 0.78125, prec 0.0453133, recall 0.793522
2017-12-10T03:31:26.427893: step 1567, loss 0.656609, acc 0.796875, prec 0.0452981, recall 0.793522
2017-12-10T03:31:26.702940: step 1568, loss 1.04348, acc 0.703125, prec 0.0453006, recall 0.793615
2017-12-10T03:31:26.976771: step 1569, loss 0.564636, acc 0.859375, prec 0.0453391, recall 0.793801
2017-12-10T03:31:27.239938: step 1570, loss 1.16086, acc 0.703125, prec 0.0453415, recall 0.793893
2017-12-10T03:31:27.502616: step 1571, loss 1.11016, acc 0.703125, prec 0.0453439, recall 0.793986
2017-12-10T03:31:27.775993: step 1572, loss 0.585957, acc 0.796875, prec 0.0454021, recall 0.794263
2017-12-10T03:31:28.045645: step 1573, loss 0.50515, acc 0.84375, prec 0.0454394, recall 0.794447
2017-12-10T03:31:28.308555: step 1574, loss 1.09862, acc 0.859375, prec 0.0454534, recall 0.794539
2017-12-10T03:31:28.569982: step 1575, loss 0.554137, acc 0.828125, prec 0.0454406, recall 0.794539
2017-12-10T03:31:28.829467: step 1576, loss 0.692912, acc 0.796875, prec 0.0454499, recall 0.794631
2017-12-10T03:31:29.095687: step 1577, loss 0.527655, acc 0.796875, prec 0.0454836, recall 0.794814
2017-12-10T03:31:29.357913: step 1578, loss 1.47121, acc 0.828125, prec 0.0454952, recall 0.794906
2017-12-10T03:31:29.629417: step 1579, loss 0.287978, acc 0.921875, prec 0.0455138, recall 0.794998
2017-12-10T03:31:29.902364: step 1580, loss 0.500954, acc 0.8125, prec 0.0455243, recall 0.795089
2017-12-10T03:31:30.164093: step 1581, loss 0.136305, acc 0.9375, prec 0.0455196, recall 0.795089
2017-12-10T03:31:30.436676: step 1582, loss 0.23576, acc 0.953125, prec 0.0455649, recall 0.795272
2017-12-10T03:31:30.704751: step 1583, loss 0.78804, acc 0.859375, prec 0.0455788, recall 0.795363
2017-12-10T03:31:30.977368: step 1584, loss 0.257363, acc 0.875, prec 0.0455939, recall 0.795455
2017-12-10T03:31:31.247005: step 1585, loss 0.343495, acc 0.90625, prec 0.0456356, recall 0.795637
2017-12-10T03:31:31.514969: step 1586, loss 0.112219, acc 0.9375, prec 0.045631, recall 0.795637
2017-12-10T03:31:31.778659: step 1587, loss 1.4794, acc 0.890625, prec 0.045624, recall 0.795283
2017-12-10T03:31:32.048463: step 1588, loss 1.48354, acc 0.9375, prec 0.0457168, recall 0.795646
2017-12-10T03:31:32.322579: step 1589, loss 0.303147, acc 0.921875, prec 0.0457109, recall 0.795646
2017-12-10T03:31:32.585449: step 1590, loss 0.258968, acc 0.90625, prec 0.0457039, recall 0.795646
2017-12-10T03:31:32.843908: step 1591, loss 0.439934, acc 0.875, prec 0.0457433, recall 0.795828
2017-12-10T03:31:33.110834: step 1592, loss 0.403512, acc 0.8125, prec 0.0457536, recall 0.795918
2017-12-10T03:31:33.372188: step 1593, loss 0.413662, acc 0.90625, prec 0.0457466, recall 0.795918
2017-12-10T03:31:33.645868: step 1594, loss 0.612991, acc 0.8125, prec 0.045757, recall 0.796009
2017-12-10T03:31:33.906533: step 1595, loss 1.19675, acc 0.796875, prec 0.0458148, recall 0.79628
2017-12-10T03:31:34.176366: step 1596, loss 0.56524, acc 0.8125, prec 0.0458494, recall 0.79646
2017-12-10T03:31:34.441844: step 1597, loss 0.515699, acc 0.84375, prec 0.045862, recall 0.79655
2017-12-10T03:31:34.710444: step 1598, loss 0.383439, acc 0.828125, prec 0.0458491, recall 0.79655
2017-12-10T03:31:34.972609: step 1599, loss 0.679731, acc 0.84375, prec 0.0458618, recall 0.79664
2017-12-10T03:31:35.234721: step 1600, loss 1.47525, acc 0.796875, prec 0.0458709, recall 0.79673
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-1600

2017-12-10T03:31:36.515507: step 1601, loss 0.408365, acc 0.875, prec 0.0458858, recall 0.79682
2017-12-10T03:31:36.782802: step 1602, loss 0.315566, acc 0.90625, prec 0.0458788, recall 0.79682
2017-12-10T03:31:37.051829: step 1603, loss 0.410828, acc 0.84375, prec 0.0458671, recall 0.79682
2017-12-10T03:31:37.312308: step 1604, loss 0.777176, acc 0.734375, prec 0.0458473, recall 0.79682
2017-12-10T03:31:37.579501: step 1605, loss 0.409658, acc 0.859375, prec 0.0458368, recall 0.79682
2017-12-10T03:31:37.838150: step 1606, loss 0.246757, acc 0.890625, prec 0.0458772, recall 0.796999
2017-12-10T03:31:38.100885: step 1607, loss 0.467564, acc 0.875, prec 0.0458678, recall 0.796999
2017-12-10T03:31:38.358480: step 1608, loss 0.134218, acc 0.953125, prec 0.0458643, recall 0.796999
2017-12-10T03:31:38.628445: step 1609, loss 0.112597, acc 0.9375, prec 0.0458597, recall 0.796999
2017-12-10T03:31:38.898054: step 1610, loss 1.60367, acc 0.84375, prec 0.0458734, recall 0.796737
2017-12-10T03:31:39.160077: step 1611, loss 0.415772, acc 0.953125, prec 0.0458699, recall 0.796737
2017-12-10T03:31:39.424795: step 1612, loss 0.321966, acc 0.9375, prec 0.0458895, recall 0.796827
2017-12-10T03:31:39.687792: step 1613, loss 0.411953, acc 0.796875, prec 0.0458744, recall 0.796827
2017-12-10T03:31:39.956768: step 1614, loss 0.500707, acc 0.828125, prec 0.0458616, recall 0.796827
2017-12-10T03:31:40.228761: step 1615, loss 0.319879, acc 0.875, prec 0.0458764, recall 0.796916
2017-12-10T03:31:40.492174: step 1616, loss 0.307264, acc 0.90625, prec 0.0458695, recall 0.796916
2017-12-10T03:31:40.760529: step 1617, loss 0.333507, acc 0.921875, prec 0.0458637, recall 0.796916
2017-12-10T03:31:41.026445: step 1618, loss 0.564746, acc 0.84375, prec 0.045852, recall 0.796916
2017-12-10T03:31:41.290171: step 1619, loss 1.3892, acc 0.953125, prec 0.0458739, recall 0.796655
2017-12-10T03:31:41.560947: step 1620, loss 0.398357, acc 0.90625, prec 0.0458911, recall 0.796744
2017-12-10T03:31:41.839431: step 1621, loss 0.195678, acc 0.921875, prec 0.0458853, recall 0.796744
2017-12-10T03:31:42.101194: step 1622, loss 0.125166, acc 0.96875, prec 0.0458829, recall 0.796744
2017-12-10T03:31:42.365511: step 1623, loss 0.191538, acc 0.921875, prec 0.0459013, recall 0.796834
2017-12-10T03:31:42.632586: step 1624, loss 0.182065, acc 0.921875, prec 0.0459438, recall 0.797012
2017-12-10T03:31:42.896731: step 1625, loss 0.141342, acc 0.96875, prec 0.0459415, recall 0.797012
2017-12-10T03:31:43.169211: step 1626, loss 0.0913659, acc 0.96875, prec 0.0459392, recall 0.797012
2017-12-10T03:31:43.441179: step 1627, loss 0.260664, acc 0.890625, prec 0.045931, recall 0.797012
2017-12-10T03:31:43.705508: step 1628, loss 0.32784, acc 0.9375, prec 0.0459264, recall 0.797012
2017-12-10T03:31:43.969927: step 1629, loss 0.231827, acc 0.9375, prec 0.0459217, recall 0.797012
2017-12-10T03:31:44.235930: step 1630, loss 0.455954, acc 0.875, prec 0.0459366, recall 0.797101
2017-12-10T03:31:44.506641: step 1631, loss 0.299374, acc 0.890625, prec 0.0459284, recall 0.797101
2017-12-10T03:31:44.770828: step 1632, loss 0.0929135, acc 0.953125, prec 0.045925, recall 0.797101
2017-12-10T03:31:45.032707: step 1633, loss 2.13859, acc 0.921875, prec 0.0459444, recall 0.796841
2017-12-10T03:31:45.296664: step 1634, loss 0.091085, acc 0.953125, prec 0.045941, recall 0.796841
2017-12-10T03:31:45.563983: step 1635, loss 0.129849, acc 0.953125, prec 0.0459375, recall 0.796841
2017-12-10T03:31:45.831851: step 1636, loss 0.209449, acc 0.9375, prec 0.045957, recall 0.79693
2017-12-10T03:31:46.097471: step 1637, loss 1.49011, acc 0.9375, prec 0.0459776, recall 0.79667
2017-12-10T03:31:46.365831: step 1638, loss 0.164313, acc 0.96875, prec 0.0459753, recall 0.79667
2017-12-10T03:31:46.632525: step 1639, loss 0.303439, acc 0.921875, prec 0.0459936, recall 0.796759
2017-12-10T03:31:46.894524: step 1640, loss 0.218875, acc 0.9375, prec 0.0459889, recall 0.796759
2017-12-10T03:31:47.167465: step 1641, loss 0.254242, acc 0.9375, prec 0.0459843, recall 0.796759
2017-12-10T03:31:47.442084: step 1642, loss 0.476549, acc 0.890625, prec 0.0460003, recall 0.796848
2017-12-10T03:31:47.707061: step 1643, loss 0.45061, acc 0.875, prec 0.045991, recall 0.796848
2017-12-10T03:31:47.970484: step 1644, loss 0.227757, acc 0.90625, prec 0.045984, recall 0.796848
2017-12-10T03:31:48.240743: step 1645, loss 0.170384, acc 0.9375, prec 0.0459793, recall 0.796848
2017-12-10T03:31:48.502316: step 1646, loss 0.354999, acc 0.9375, prec 0.0459747, recall 0.796848
2017-12-10T03:31:48.772673: step 1647, loss 0.202145, acc 0.921875, prec 0.045993, recall 0.796937
2017-12-10T03:31:49.041231: step 1648, loss 0.403736, acc 0.859375, prec 0.0459825, recall 0.796937
2017-12-10T03:31:49.305607: step 1649, loss 0.272124, acc 0.921875, prec 0.0459767, recall 0.796937
2017-12-10T03:31:49.582488: step 1650, loss 0.41127, acc 0.890625, prec 0.0459686, recall 0.796937
2017-12-10T03:31:49.845567: step 1651, loss 0.556173, acc 0.90625, prec 0.0460339, recall 0.797203
2017-12-10T03:31:50.109862: step 1652, loss 0.115681, acc 0.953125, prec 0.0460545, recall 0.797291
2017-12-10T03:31:50.371278: step 1653, loss 0.130482, acc 0.953125, prec 0.046051, recall 0.797291
2017-12-10T03:31:50.641228: step 1654, loss 1.7897, acc 0.96875, prec 0.0461449, recall 0.797645
2017-12-10T03:31:50.913122: step 1655, loss 0.130154, acc 0.984375, prec 0.0461438, recall 0.797645
2017-12-10T03:31:51.177878: step 1656, loss 1.05518, acc 0.921875, prec 0.0462582, recall 0.798085
2017-12-10T03:31:51.450809: step 1657, loss 0.342504, acc 0.890625, prec 0.0462982, recall 0.798261
2017-12-10T03:31:51.713561: step 1658, loss 0.250692, acc 0.890625, prec 0.04629, recall 0.798261
2017-12-10T03:31:51.987087: step 1659, loss 0.455437, acc 0.84375, prec 0.0462783, recall 0.798261
2017-12-10T03:31:52.254544: step 1660, loss 0.215768, acc 0.953125, prec 0.0462989, recall 0.798349
2017-12-10T03:31:52.520390: step 1661, loss 0.641589, acc 0.8125, prec 0.0463089, recall 0.798436
2017-12-10T03:31:52.782874: step 1662, loss 0.638767, acc 0.78125, prec 0.0462926, recall 0.798436
2017-12-10T03:31:53.046342: step 1663, loss 0.182089, acc 0.875, prec 0.0462832, recall 0.798436
2017-12-10T03:31:53.318720: step 1664, loss 0.632623, acc 0.8125, prec 0.0463173, recall 0.798611
2017-12-10T03:31:53.585026: step 1665, loss 0.275636, acc 0.875, prec 0.0463079, recall 0.798611
2017-12-10T03:31:53.857709: step 1666, loss 0.589509, acc 0.828125, prec 0.0462951, recall 0.798611
2017-12-10T03:31:54.129005: step 1667, loss 0.487254, acc 0.875, prec 0.0463338, recall 0.798786
2017-12-10T03:31:54.390429: step 1668, loss 0.356559, acc 0.859375, prec 0.0463233, recall 0.798786
2017-12-10T03:31:54.655504: step 1669, loss 0.304629, acc 0.921875, prec 0.0463175, recall 0.798786
2017-12-10T03:31:54.918696: step 1670, loss 0.186982, acc 0.90625, prec 0.0463345, recall 0.798873
2017-12-10T03:31:55.179897: step 1671, loss 1.6191, acc 0.90625, prec 0.0463286, recall 0.798527
2017-12-10T03:31:55.449451: step 1672, loss 0.18548, acc 0.953125, prec 0.0463252, recall 0.798527
2017-12-10T03:31:55.713915: step 1673, loss 0.333061, acc 0.859375, prec 0.0463147, recall 0.798527
2017-12-10T03:31:55.982411: step 1674, loss 0.461364, acc 0.875, prec 0.0463533, recall 0.798701
2017-12-10T03:31:56.244444: step 1675, loss 0.190788, acc 0.9375, prec 0.0463486, recall 0.798701
2017-12-10T03:31:56.507090: step 1676, loss 0.510165, acc 0.9375, prec 0.0463919, recall 0.798875
2017-12-10T03:31:56.781475: step 1677, loss 0.224007, acc 0.921875, prec 0.0463861, recall 0.798875
2017-12-10T03:31:57.042422: step 1678, loss 0.253049, acc 0.9375, prec 0.0464053, recall 0.798962
2017-12-10T03:31:57.306070: step 1679, loss 0.0989252, acc 0.984375, prec 0.0464281, recall 0.799049
2017-12-10T03:31:57.571720: step 1680, loss 0.220412, acc 0.921875, prec 0.0464702, recall 0.799223
2017-12-10T03:31:57.840324: step 1681, loss 0.122555, acc 0.9375, prec 0.0464894, recall 0.799309
2017-12-10T03:31:58.105619: step 1682, loss 0.286477, acc 0.890625, prec 0.0464813, recall 0.799309
2017-12-10T03:31:58.364053: step 1683, loss 0.441579, acc 0.828125, prec 0.0464924, recall 0.799396
2017-12-10T03:31:58.627379: step 1684, loss 0.262992, acc 0.921875, prec 0.0465344, recall 0.799569
2017-12-10T03:31:58.891692: step 1685, loss 4.25361, acc 0.875, prec 0.0465501, recall 0.799311
2017-12-10T03:31:59.162350: step 1686, loss 0.20003, acc 0.890625, prec 0.0465659, recall 0.799397
2017-12-10T03:31:59.424490: step 1687, loss 0.873526, acc 0.953125, prec 0.0465863, recall 0.799484
2017-12-10T03:31:59.690776: step 1688, loss 0.344599, acc 0.90625, prec 0.0465793, recall 0.799484
2017-12-10T03:31:59.955106: step 1689, loss 0.709437, acc 0.8125, prec 0.046613, recall 0.799656
2017-12-10T03:32:00.226821: step 1690, loss 0.269323, acc 0.890625, prec 0.0466287, recall 0.799742
2017-12-10T03:32:00.494291: step 1691, loss 0.289593, acc 0.859375, prec 0.0466182, recall 0.799742
2017-12-10T03:32:00.758429: step 1692, loss 0.26259, acc 0.890625, prec 0.0466101, recall 0.799742
2017-12-10T03:32:01.019201: step 1693, loss 0.365891, acc 0.890625, prec 0.0466258, recall 0.799828
2017-12-10T03:32:01.280634: step 1694, loss 1.4016, acc 0.796875, prec 0.0466583, recall 0.8
2017-12-10T03:32:01.545852: step 1695, loss 1.21254, acc 0.703125, prec 0.04666, recall 0.800086
2017-12-10T03:32:01.809186: step 1696, loss 0.883272, acc 0.84375, prec 0.0466483, recall 0.800086
2017-12-10T03:32:02.077526: step 1697, loss 0.318389, acc 0.875, prec 0.046639, recall 0.800086
2017-12-10T03:32:02.339229: step 1698, loss 0.477961, acc 0.828125, prec 0.04665, recall 0.800172
2017-12-10T03:32:02.605774: step 1699, loss 0.711443, acc 0.859375, prec 0.0466633, recall 0.800257
2017-12-10T03:32:02.873561: step 1700, loss 0.472905, acc 0.84375, prec 0.0466755, recall 0.800343
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-1700

2017-12-10T03:32:04.195835: step 1701, loss 0.597539, acc 0.875, prec 0.04669, recall 0.800428
2017-12-10T03:32:04.461252: step 1702, loss 0.544007, acc 0.828125, prec 0.0467248, recall 0.800599
2017-12-10T03:32:04.725071: step 1703, loss 0.556785, acc 0.8125, prec 0.0467108, recall 0.800599
2017-12-10T03:32:04.982474: step 1704, loss 0.346475, acc 0.921875, prec 0.0467049, recall 0.800599
2017-12-10T03:32:05.242915: step 1705, loss 0.228312, acc 0.890625, prec 0.0466968, recall 0.800599
2017-12-10T03:32:05.505680: step 1706, loss 0.227213, acc 0.90625, prec 0.0466898, recall 0.800599
2017-12-10T03:32:05.766249: step 1707, loss 0.143899, acc 0.9375, prec 0.0467089, recall 0.800684
2017-12-10T03:32:06.029507: step 1708, loss 0.470216, acc 0.90625, prec 0.0467495, recall 0.800855
2017-12-10T03:32:06.290190: step 1709, loss 0.351551, acc 0.84375, prec 0.0467378, recall 0.800855
2017-12-10T03:32:06.554510: step 1710, loss 0.211439, acc 0.890625, prec 0.0467772, recall 0.801025
2017-12-10T03:32:06.816276: step 1711, loss 1.03761, acc 0.875, prec 0.0467916, recall 0.80111
2017-12-10T03:32:07.084896: step 1712, loss 0.255384, acc 0.9375, prec 0.0468583, recall 0.801364
2017-12-10T03:32:07.355610: step 1713, loss 0.202124, acc 0.90625, prec 0.0468512, recall 0.801364
2017-12-10T03:32:07.615679: step 1714, loss 0.143535, acc 0.96875, prec 0.0468489, recall 0.801364
2017-12-10T03:32:07.874146: step 1715, loss 0.191566, acc 0.90625, prec 0.0468657, recall 0.801449
2017-12-10T03:32:08.136840: step 1716, loss 0.178987, acc 0.953125, prec 0.0468859, recall 0.801533
2017-12-10T03:32:08.402902: step 1717, loss 0.328371, acc 0.953125, prec 0.0469061, recall 0.801618
2017-12-10T03:32:08.667936: step 1718, loss 0.472564, acc 0.921875, prec 0.046924, recall 0.801702
2017-12-10T03:32:08.932120: step 1719, loss 0.365139, acc 0.921875, prec 0.0469419, recall 0.801786
2017-12-10T03:32:09.200358: step 1720, loss 0.996105, acc 0.9375, prec 0.046961, recall 0.801871
2017-12-10T03:32:09.465837: step 1721, loss 0.458743, acc 0.859375, prec 0.0469742, recall 0.801955
2017-12-10T03:32:09.731838: step 1722, loss 0.0803417, acc 0.96875, prec 0.0469718, recall 0.801955
2017-12-10T03:32:09.996522: step 1723, loss 0.0489874, acc 0.984375, prec 0.0470181, recall 0.802123
2017-12-10T03:32:10.262651: step 1724, loss 0.396841, acc 0.921875, prec 0.0470123, recall 0.802123
2017-12-10T03:32:10.532760: step 1725, loss 0.940035, acc 0.921875, prec 0.0470301, recall 0.802207
2017-12-10T03:32:10.794869: step 1726, loss 0.589842, acc 0.953125, prec 0.047074, recall 0.802375
2017-12-10T03:32:11.068288: step 1727, loss 0.129248, acc 0.9375, prec 0.0470931, recall 0.802459
2017-12-10T03:32:11.331178: step 1728, loss 2.31542, acc 0.90625, prec 0.0470872, recall 0.802119
2017-12-10T03:32:11.602733: step 1729, loss 0.566015, acc 0.84375, prec 0.0470755, recall 0.802119
2017-12-10T03:32:11.868890: step 1730, loss 0.539694, acc 0.859375, prec 0.047065, recall 0.802119
2017-12-10T03:32:12.131489: step 1731, loss 1.01571, acc 0.8125, prec 0.0470746, recall 0.802202
2017-12-10T03:32:12.395308: step 1732, loss 0.386181, acc 0.8125, prec 0.0470843, recall 0.802286
2017-12-10T03:32:12.661970: step 1733, loss 0.341947, acc 0.84375, prec 0.0471199, recall 0.802453
2017-12-10T03:32:12.920798: step 1734, loss 0.505467, acc 0.8125, prec 0.0471295, recall 0.802537
2017-12-10T03:32:13.185986: step 1735, loss 0.986501, acc 0.703125, prec 0.0471546, recall 0.802704
2017-12-10T03:32:13.457239: step 1736, loss 1.40839, acc 0.90625, prec 0.0471487, recall 0.802365
2017-12-10T03:32:13.722344: step 1737, loss 0.685087, acc 0.765625, prec 0.0471312, recall 0.802365
2017-12-10T03:32:13.987565: step 1738, loss 0.447138, acc 0.828125, prec 0.0471183, recall 0.802365
2017-12-10T03:32:14.250242: step 1739, loss 0.490011, acc 0.859375, prec 0.0471078, recall 0.802365
2017-12-10T03:32:14.511862: step 1740, loss 0.714396, acc 0.8125, prec 0.0471174, recall 0.802448
2017-12-10T03:32:14.777320: step 1741, loss 0.563799, acc 0.796875, prec 0.0471259, recall 0.802532
2017-12-10T03:32:15.040897: step 1742, loss 0.699488, acc 0.703125, prec 0.0471273, recall 0.802615
2017-12-10T03:32:15.304696: step 1743, loss 0.45904, acc 0.84375, prec 0.0471156, recall 0.802615
2017-12-10T03:32:15.568322: step 1744, loss 0.80588, acc 0.875, prec 0.0471299, recall 0.802698
2017-12-10T03:32:15.839023: step 1745, loss 0.289341, acc 0.90625, prec 0.0471465, recall 0.802781
2017-12-10T03:32:16.104068: step 1746, loss 0.479419, acc 0.890625, prec 0.0471619, recall 0.802864
2017-12-10T03:32:16.373566: step 1747, loss 0.475213, acc 0.859375, prec 0.0471514, recall 0.802864
2017-12-10T03:32:16.641765: step 1748, loss 1.94512, acc 0.90625, prec 0.0472162, recall 0.802775
2017-12-10T03:32:16.904864: step 1749, loss 0.153432, acc 0.953125, prec 0.0472363, recall 0.802858
2017-12-10T03:32:17.174960: step 1750, loss 4.41452, acc 0.9375, prec 0.0472799, recall 0.802687
2017-12-10T03:32:17.442250: step 1751, loss 0.384575, acc 0.90625, prec 0.0472729, recall 0.802687
2017-12-10T03:32:17.704080: step 1752, loss 0.320141, acc 0.921875, prec 0.0473377, recall 0.802935
2017-12-10T03:32:17.972635: step 1753, loss 0.240216, acc 0.90625, prec 0.0473307, recall 0.802935
2017-12-10T03:32:18.236004: step 1754, loss 2.56799, acc 0.921875, prec 0.0473496, recall 0.802681
2017-12-10T03:32:18.503086: step 1755, loss 0.246447, acc 0.921875, prec 0.0473437, recall 0.802681
2017-12-10T03:32:18.761696: step 1756, loss 0.576334, acc 0.859375, prec 0.0474038, recall 0.802929
2017-12-10T03:32:19.025563: step 1757, loss 0.560377, acc 0.875, prec 0.0474415, recall 0.803094
2017-12-10T03:32:19.292076: step 1758, loss 0.302541, acc 0.875, prec 0.0474321, recall 0.803094
2017-12-10T03:32:19.555920: step 1759, loss 0.86796, acc 0.796875, prec 0.0474169, recall 0.803094
2017-12-10T03:32:19.824001: step 1760, loss 0.95946, acc 0.8125, prec 0.0474263, recall 0.803176
2017-12-10T03:32:20.088488: step 1761, loss 0.716742, acc 0.75, prec 0.0474076, recall 0.803176
2017-12-10T03:32:20.347668: step 1762, loss 0.768537, acc 0.796875, prec 0.0474159, recall 0.803258
2017-12-10T03:32:20.613094: step 1763, loss 0.43362, acc 0.875, prec 0.0474535, recall 0.803422
2017-12-10T03:32:20.879279: step 1764, loss 0.431196, acc 0.828125, prec 0.0474407, recall 0.803422
2017-12-10T03:32:21.144023: step 1765, loss 1.94428, acc 0.78125, prec 0.0474712, recall 0.803586
2017-12-10T03:32:21.405475: step 1766, loss 0.540146, acc 0.8125, prec 0.0474807, recall 0.803668
2017-12-10T03:32:21.671599: step 1767, loss 0.792541, acc 0.75, prec 0.047462, recall 0.803668
2017-12-10T03:32:21.938478: step 1768, loss 0.544858, acc 0.828125, prec 0.047496, recall 0.803832
2017-12-10T03:32:22.203584: step 1769, loss 0.403099, acc 0.828125, prec 0.0474831, recall 0.803832
2017-12-10T03:32:22.463410: step 1770, loss 0.711489, acc 0.75, prec 0.0474879, recall 0.803913
2017-12-10T03:32:22.728916: step 1771, loss 0.520847, acc 0.859375, prec 0.0475008, recall 0.803995
2017-12-10T03:32:23.004596: step 1772, loss 0.236098, acc 0.921875, prec 0.0475652, recall 0.804239
2017-12-10T03:32:23.268734: step 1773, loss 0.344295, acc 0.90625, prec 0.0475816, recall 0.804321
2017-12-10T03:32:23.534289: step 1774, loss 0.724222, acc 0.875, prec 0.0475956, recall 0.804402
2017-12-10T03:32:23.806625: step 1775, loss 0.256784, acc 0.90625, prec 0.0476354, recall 0.804564
2017-12-10T03:32:24.073470: step 1776, loss 0.431203, acc 0.84375, prec 0.0476471, recall 0.804645
2017-12-10T03:32:24.339649: step 1777, loss 0.32103, acc 0.859375, prec 0.04766, recall 0.804726
2017-12-10T03:32:24.611268: step 1778, loss 0.333635, acc 0.890625, prec 0.0476518, recall 0.804726
2017-12-10T03:32:24.873695: step 1779, loss 1.13575, acc 0.890625, prec 0.047667, recall 0.804807
2017-12-10T03:32:25.142303: step 1780, loss 0.0466605, acc 0.984375, prec 0.0476892, recall 0.804888
2017-12-10T03:32:25.401376: step 1781, loss 0.498346, acc 0.890625, prec 0.0477277, recall 0.80505
2017-12-10T03:32:25.666434: step 1782, loss 1.89415, acc 0.953125, prec 0.0477254, recall 0.804717
2017-12-10T03:32:25.941311: step 1783, loss 0.143988, acc 0.953125, prec 0.0477452, recall 0.804797
2017-12-10T03:32:26.207806: step 1784, loss 0.227011, acc 0.9375, prec 0.0477405, recall 0.804797
2017-12-10T03:32:26.470368: step 1785, loss 0.143378, acc 0.953125, prec 0.0477604, recall 0.804878
2017-12-10T03:32:26.748363: step 1786, loss 0.356453, acc 0.890625, prec 0.0477755, recall 0.804959
2017-12-10T03:32:27.013956: step 1787, loss 0.932413, acc 0.859375, prec 0.0477883, recall 0.805039
2017-12-10T03:32:27.284232: step 1788, loss 0.622352, acc 0.890625, prec 0.0477801, recall 0.805039
2017-12-10T03:32:27.546521: step 1789, loss 0.122202, acc 0.953125, prec 0.0477766, recall 0.805039
2017-12-10T03:32:27.811769: step 1790, loss 0.270666, acc 0.9375, prec 0.0477719, recall 0.805039
2017-12-10T03:32:28.076641: step 1791, loss 0.408195, acc 0.90625, prec 0.0477883, recall 0.80512
2017-12-10T03:32:28.340333: step 1792, loss 0.192527, acc 0.953125, prec 0.0478081, recall 0.8052
2017-12-10T03:32:28.606569: step 1793, loss 0.425376, acc 0.84375, prec 0.0477964, recall 0.8052
2017-12-10T03:32:28.870571: step 1794, loss 0.199136, acc 0.9375, prec 0.047815, recall 0.805281
2017-12-10T03:32:29.133192: step 1795, loss 0.464031, acc 0.859375, prec 0.0478278, recall 0.805361
2017-12-10T03:32:29.396902: step 1796, loss 0.359018, acc 0.90625, prec 0.0478441, recall 0.805441
2017-12-10T03:32:29.655199: step 1797, loss 0.213193, acc 0.9375, prec 0.0478394, recall 0.805441
2017-12-10T03:32:29.921451: step 1798, loss 0.564062, acc 0.859375, prec 0.0478522, recall 0.805521
2017-12-10T03:32:30.185704: step 1799, loss 0.170736, acc 0.9375, prec 0.0478475, recall 0.805521
2017-12-10T03:32:30.456130: step 1800, loss 0.20055, acc 0.9375, prec 0.0478661, recall 0.805601

Evaluation:
2017-12-10T03:32:38.134375: step 1800, loss 2.67767, acc 0.945278, prec 0.0490288, recall 0.787902

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-1800

2017-12-10T03:32:39.398817: step 1801, loss 0.347188, acc 0.9375, prec 0.04907, recall 0.788067
2017-12-10T03:32:39.661555: step 1802, loss 0.117807, acc 0.921875, prec 0.0490641, recall 0.788067
2017-12-10T03:32:39.925124: step 1803, loss 0.151713, acc 0.9375, prec 0.0490593, recall 0.788067
2017-12-10T03:32:40.189936: step 1804, loss 0.0808344, acc 0.953125, prec 0.0490787, recall 0.788149
2017-12-10T03:32:40.456979: step 1805, loss 0.463914, acc 0.984375, prec 0.0491234, recall 0.788313
2017-12-10T03:32:40.724358: step 1806, loss 0.209649, acc 0.953125, prec 0.0491428, recall 0.788395
2017-12-10T03:32:40.993788: step 1807, loss 0.192512, acc 0.9375, prec 0.049138, recall 0.788395
2017-12-10T03:32:41.259134: step 1808, loss 0.310551, acc 0.921875, prec 0.0491321, recall 0.788395
2017-12-10T03:32:41.530534: step 1809, loss 0.136728, acc 0.9375, prec 0.0491732, recall 0.788558
2017-12-10T03:32:41.804377: step 1810, loss 0.198916, acc 0.9375, prec 0.0491914, recall 0.78864
2017-12-10T03:32:42.074237: step 1811, loss 0.44217, acc 0.953125, prec 0.0492107, recall 0.788722
2017-12-10T03:32:42.335675: step 1812, loss 3.6829, acc 0.953125, prec 0.0492554, recall 0.788276
2017-12-10T03:32:42.599512: step 1813, loss 0.149492, acc 0.953125, prec 0.0492518, recall 0.788276
2017-12-10T03:32:42.864544: step 1814, loss 0.273492, acc 0.96875, prec 0.0492953, recall 0.788439
2017-12-10T03:32:43.127743: step 1815, loss 0.796457, acc 0.875, prec 0.0493316, recall 0.788602
2017-12-10T03:32:43.397273: step 1816, loss 0.512261, acc 0.828125, prec 0.0493185, recall 0.788602
2017-12-10T03:32:43.662112: step 1817, loss 0.482784, acc 0.828125, prec 0.0493054, recall 0.788602
2017-12-10T03:32:43.924957: step 1818, loss 0.26368, acc 0.90625, prec 0.0493441, recall 0.788765
2017-12-10T03:32:44.196709: step 1819, loss 0.392585, acc 0.875, prec 0.0493803, recall 0.788927
2017-12-10T03:32:44.468769: step 1820, loss 0.728216, acc 0.8125, prec 0.049389, recall 0.789008
2017-12-10T03:32:44.735402: step 1821, loss 0.674186, acc 0.8125, prec 0.0493747, recall 0.789008
2017-12-10T03:32:44.995553: step 1822, loss 0.507268, acc 0.875, prec 0.0493881, recall 0.78909
2017-12-10T03:32:45.268279: step 1823, loss 0.884682, acc 0.828125, prec 0.0493979, recall 0.789171
2017-12-10T03:32:45.532713: step 1824, loss 0.339066, acc 0.84375, prec 0.0494088, recall 0.789251
2017-12-10T03:32:45.802693: step 1825, loss 0.401846, acc 0.890625, prec 0.0494005, recall 0.789251
2017-12-10T03:32:46.069271: step 1826, loss 0.256741, acc 0.890625, prec 0.0494607, recall 0.789494
2017-12-10T03:32:46.340405: step 1827, loss 0.388195, acc 0.84375, prec 0.0494717, recall 0.789575
2017-12-10T03:32:46.616154: step 1828, loss 6.47607, acc 0.90625, prec 0.0495114, recall 0.789433
2017-12-10T03:32:46.883137: step 1829, loss 0.454042, acc 0.796875, prec 0.0494959, recall 0.789433
2017-12-10T03:32:47.148060: step 1830, loss 0.669625, acc 0.890625, prec 0.049556, recall 0.789675
2017-12-10T03:32:47.413670: step 1831, loss 0.273484, acc 0.875, prec 0.0495921, recall 0.789836
2017-12-10T03:32:47.676811: step 1832, loss 0.541155, acc 0.828125, prec 0.0496018, recall 0.789916
2017-12-10T03:32:47.942541: step 1833, loss 0.500502, acc 0.859375, prec 0.0496139, recall 0.789996
2017-12-10T03:32:48.202158: step 1834, loss 0.463902, acc 0.859375, prec 0.0496488, recall 0.790156
2017-12-10T03:32:48.466842: step 1835, loss 0.826953, acc 0.828125, prec 0.0496585, recall 0.790236
2017-12-10T03:32:48.736806: step 1836, loss 0.408011, acc 0.84375, prec 0.0496694, recall 0.790316
2017-12-10T03:32:49.001462: step 1837, loss 0.628692, acc 0.796875, prec 0.0496766, recall 0.790396
2017-12-10T03:32:49.270852: step 1838, loss 0.268898, acc 0.90625, prec 0.0497378, recall 0.790636
2017-12-10T03:32:49.542621: step 1839, loss 0.977001, acc 0.75, prec 0.0497415, recall 0.790715
2017-12-10T03:32:49.806566: step 1840, loss 1.83366, acc 0.875, prec 0.0497331, recall 0.790415
2017-12-10T03:32:50.078303: step 1841, loss 0.432194, acc 0.890625, prec 0.0497248, recall 0.790415
2017-12-10T03:32:50.344371: step 1842, loss 0.526858, acc 0.875, prec 0.049738, recall 0.790494
2017-12-10T03:32:50.612519: step 1843, loss 0.516658, acc 0.828125, prec 0.0497477, recall 0.790574
2017-12-10T03:32:50.875275: step 1844, loss 0.301556, acc 0.921875, prec 0.0497645, recall 0.790653
2017-12-10T03:32:51.143528: step 1845, loss 0.491865, acc 0.84375, prec 0.0497526, recall 0.790653
2017-12-10T03:32:51.415386: step 1846, loss 0.294205, acc 0.875, prec 0.049743, recall 0.790653
2017-12-10T03:32:51.677342: step 1847, loss 4.52486, acc 0.859375, prec 0.0497335, recall 0.790353
2017-12-10T03:32:51.946260: step 1848, loss 0.430334, acc 0.8125, prec 0.049742, recall 0.790433
2017-12-10T03:32:52.211708: step 1849, loss 0.488375, acc 0.84375, prec 0.0497528, recall 0.790512
2017-12-10T03:32:52.483971: step 1850, loss 0.580262, acc 0.875, prec 0.049766, recall 0.790592
2017-12-10T03:32:52.752265: step 1851, loss 0.352087, acc 0.890625, prec 0.0497577, recall 0.790592
2017-12-10T03:32:53.013092: step 1852, loss 0.208538, acc 0.890625, prec 0.049772, recall 0.790671
2017-12-10T03:32:53.275486: step 1853, loss 0.375209, acc 0.859375, prec 0.0497613, recall 0.790671
2017-12-10T03:32:53.538933: step 1854, loss 0.331561, acc 0.890625, prec 0.049753, recall 0.790671
2017-12-10T03:32:53.803414: step 1855, loss 0.398037, acc 0.859375, prec 0.0497423, recall 0.790671
2017-12-10T03:32:54.064908: step 1856, loss 0.457443, acc 0.875, prec 0.0497328, recall 0.790671
2017-12-10T03:32:54.331088: step 1857, loss 0.673388, acc 0.796875, prec 0.0497628, recall 0.79083
2017-12-10T03:32:54.591966: step 1858, loss 0.430713, acc 0.90625, prec 0.0498236, recall 0.791067
2017-12-10T03:32:54.856028: step 1859, loss 0.275465, acc 0.9375, prec 0.0498641, recall 0.791225
2017-12-10T03:32:55.120795: step 1860, loss 0.578351, acc 0.828125, prec 0.0498511, recall 0.791225
2017-12-10T03:32:55.380587: step 1861, loss 0.349321, acc 0.890625, prec 0.0498654, recall 0.791304
2017-12-10T03:32:55.642722: step 1862, loss 3.37487, acc 0.9375, prec 0.0498618, recall 0.791005
2017-12-10T03:32:55.915125: step 1863, loss 5.47483, acc 0.859375, prec 0.049875, recall 0.790785
2017-12-10T03:32:56.182928: step 1864, loss 0.285305, acc 0.921875, prec 0.0499143, recall 0.790943
2017-12-10T03:32:56.454172: step 1865, loss 7.28042, acc 0.90625, prec 0.0499762, recall 0.790882
2017-12-10T03:32:56.732557: step 1866, loss 0.638002, acc 0.8125, prec 0.0499619, recall 0.790882
2017-12-10T03:32:56.995106: step 1867, loss 1.32014, acc 0.6875, prec 0.0499381, recall 0.790882
2017-12-10T03:32:57.261120: step 1868, loss 1.16041, acc 0.671875, prec 0.0499584, recall 0.791039
2017-12-10T03:32:57.523833: step 1869, loss 0.907097, acc 0.65625, prec 0.0499548, recall 0.791118
2017-12-10T03:32:57.786427: step 1870, loss 0.937081, acc 0.6875, prec 0.0499762, recall 0.791275
2017-12-10T03:32:58.047729: step 1871, loss 1.65555, acc 0.484375, prec 0.0499371, recall 0.791275
2017-12-10T03:32:58.309421: step 1872, loss 1.41765, acc 0.625, prec 0.0499087, recall 0.791275
2017-12-10T03:32:58.575758: step 1873, loss 1.3478, acc 0.65625, prec 0.0499052, recall 0.791353
2017-12-10T03:32:58.840765: step 1874, loss 1.15965, acc 0.65625, prec 0.0499017, recall 0.791432
2017-12-10T03:32:59.101445: step 1875, loss 1.15653, acc 0.703125, prec 0.0498792, recall 0.791432
2017-12-10T03:32:59.365825: step 1876, loss 0.73351, acc 0.75, prec 0.0498828, recall 0.79151
2017-12-10T03:32:59.624885: step 1877, loss 1.68991, acc 0.578125, prec 0.0498734, recall 0.791588
2017-12-10T03:32:59.890919: step 1878, loss 0.958171, acc 0.734375, prec 0.0498534, recall 0.791588
2017-12-10T03:33:00.169097: step 1879, loss 0.881362, acc 0.671875, prec 0.0498511, recall 0.791667
2017-12-10T03:33:00.439087: step 1880, loss 0.955271, acc 0.75, prec 0.049922, recall 0.791979
2017-12-10T03:33:00.707799: step 1881, loss 2.77697, acc 0.859375, prec 0.0499126, recall 0.791682
2017-12-10T03:33:00.980077: step 1882, loss 0.813546, acc 0.875, prec 0.049948, recall 0.791838
2017-12-10T03:33:01.242023: step 1883, loss 2.86164, acc 0.890625, prec 0.0499634, recall 0.79162
2017-12-10T03:33:01.506941: step 1884, loss 0.592598, acc 0.828125, prec 0.0499504, recall 0.79162
2017-12-10T03:33:01.779817: step 1885, loss 0.755291, acc 0.921875, prec 0.0499894, recall 0.791776
2017-12-10T03:33:02.041932: step 1886, loss 0.699146, acc 0.84375, prec 0.05, recall 0.791853
2017-12-10T03:33:02.299945: step 1887, loss 0.418207, acc 0.84375, prec 0.0499882, recall 0.791853
2017-12-10T03:33:02.568479: step 1888, loss 0.497783, acc 0.84375, prec 0.0499988, recall 0.791931
2017-12-10T03:33:02.838302: step 1889, loss 0.689938, acc 0.796875, prec 0.0500283, recall 0.792087
2017-12-10T03:33:03.102074: step 1890, loss 0.867749, acc 0.765625, prec 0.050033, recall 0.792164
2017-12-10T03:33:03.363409: step 1891, loss 0.374521, acc 0.859375, prec 0.0500224, recall 0.792164
2017-12-10T03:33:03.624339: step 1892, loss 0.682414, acc 0.796875, prec 0.0500294, recall 0.792242
2017-12-10T03:33:03.890887: step 1893, loss 0.654704, acc 0.796875, prec 0.0500365, recall 0.792319
2017-12-10T03:33:04.155634: step 1894, loss 0.770252, acc 0.734375, prec 0.0500388, recall 0.792397
2017-12-10T03:33:04.420155: step 1895, loss 0.3382, acc 0.859375, prec 0.0500282, recall 0.792397
2017-12-10T03:33:04.684036: step 1896, loss 0.547048, acc 0.796875, prec 0.0500576, recall 0.792551
2017-12-10T03:33:04.950737: step 1897, loss 0.459043, acc 0.890625, prec 0.0500494, recall 0.792551
2017-12-10T03:33:05.211635: step 1898, loss 0.406011, acc 0.859375, prec 0.0500835, recall 0.792706
2017-12-10T03:33:05.476079: step 1899, loss 0.30474, acc 0.90625, prec 0.0500764, recall 0.792706
2017-12-10T03:33:05.739312: step 1900, loss 0.160789, acc 0.921875, prec 0.0500705, recall 0.792706
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-1900

2017-12-10T03:33:07.277723: step 1901, loss 0.551715, acc 0.953125, prec 0.0501116, recall 0.79286
2017-12-10T03:33:07.540388: step 1902, loss 0.214479, acc 0.9375, prec 0.0501069, recall 0.79286
2017-12-10T03:33:07.805473: step 1903, loss 2.07182, acc 0.890625, prec 0.0500999, recall 0.792565
2017-12-10T03:33:08.068126: step 1904, loss 0.103812, acc 0.953125, prec 0.050141, recall 0.792719
2017-12-10T03:33:08.332587: step 1905, loss 0.23468, acc 0.890625, prec 0.0501327, recall 0.792719
2017-12-10T03:33:08.595847: step 1906, loss 0.168678, acc 0.90625, prec 0.050148, recall 0.792796
2017-12-10T03:33:08.855345: step 1907, loss 5.42105, acc 0.890625, prec 0.0501632, recall 0.792579
2017-12-10T03:33:09.118470: step 1908, loss 0.0869616, acc 0.96875, prec 0.0501609, recall 0.792579
2017-12-10T03:33:09.386208: step 1909, loss 0.435548, acc 0.875, prec 0.0501737, recall 0.792656
2017-12-10T03:33:09.646345: step 1910, loss 1.02032, acc 0.828125, prec 0.0502054, recall 0.792809
2017-12-10T03:33:09.908487: step 1911, loss 0.481277, acc 0.828125, prec 0.0501924, recall 0.792809
2017-12-10T03:33:10.170214: step 1912, loss 0.376594, acc 0.890625, prec 0.0501842, recall 0.792809
2017-12-10T03:33:10.432235: step 1913, loss 0.745096, acc 0.734375, prec 0.0501864, recall 0.792886
2017-12-10T03:33:10.694561: step 1914, loss 0.558341, acc 0.859375, prec 0.0501758, recall 0.792886
2017-12-10T03:33:10.958423: step 1915, loss 0.475765, acc 0.84375, prec 0.0501641, recall 0.792886
2017-12-10T03:33:11.221383: step 1916, loss 0.84133, acc 0.8125, prec 0.0502168, recall 0.793116
2017-12-10T03:33:11.485259: step 1917, loss 0.55339, acc 0.828125, prec 0.0502261, recall 0.793193
2017-12-10T03:33:11.755697: step 1918, loss 0.521548, acc 0.8125, prec 0.0502564, recall 0.793346
2017-12-10T03:33:12.027674: step 1919, loss 0.368281, acc 0.890625, prec 0.0502927, recall 0.793498
2017-12-10T03:33:12.296299: step 1920, loss 0.559726, acc 0.8125, prec 0.0503008, recall 0.793575
2017-12-10T03:33:12.563946: step 1921, loss 0.441152, acc 0.875, prec 0.0503802, recall 0.793879
2017-12-10T03:33:12.824611: step 1922, loss 0.372504, acc 0.875, prec 0.0504153, recall 0.794031
2017-12-10T03:33:13.084337: step 1923, loss 0.15095, acc 0.953125, prec 0.0504339, recall 0.794107
2017-12-10T03:33:13.344311: step 1924, loss 0.829531, acc 0.78125, prec 0.0504174, recall 0.794107
2017-12-10T03:33:13.607034: step 1925, loss 0.125709, acc 0.953125, prec 0.0504139, recall 0.794107
2017-12-10T03:33:13.866620: step 1926, loss 2.33629, acc 0.953125, prec 0.0504337, recall 0.79389
2017-12-10T03:33:14.138303: step 1927, loss 0.27624, acc 0.921875, prec 0.05045, recall 0.793966
2017-12-10T03:33:14.400961: step 1928, loss 0.438399, acc 0.890625, prec 0.050464, recall 0.794042
2017-12-10T03:33:14.662142: step 1929, loss 0.197221, acc 0.90625, prec 0.0504569, recall 0.794042
2017-12-10T03:33:14.922836: step 1930, loss 0.793048, acc 0.859375, prec 0.0504685, recall 0.794118
2017-12-10T03:33:15.185537: step 1931, loss 0.294284, acc 0.890625, prec 0.0505046, recall 0.794269
2017-12-10T03:33:15.445931: step 1932, loss 0.353887, acc 0.921875, prec 0.0504987, recall 0.794269
2017-12-10T03:33:15.706818: step 1933, loss 0.329097, acc 0.9375, prec 0.050494, recall 0.794269
2017-12-10T03:33:15.968504: step 1934, loss 0.553045, acc 0.859375, prec 0.0505055, recall 0.794344
2017-12-10T03:33:16.239990: step 1935, loss 0.20371, acc 0.90625, prec 0.0504984, recall 0.794344
2017-12-10T03:33:16.505097: step 1936, loss 1.99823, acc 0.921875, prec 0.0505159, recall 0.794128
2017-12-10T03:33:16.777262: step 1937, loss 0.194089, acc 0.953125, prec 0.0505345, recall 0.794204
2017-12-10T03:33:17.041322: step 1938, loss 0.641185, acc 0.890625, prec 0.0505484, recall 0.794279
2017-12-10T03:33:17.304750: step 1939, loss 0.399, acc 0.859375, prec 0.0505378, recall 0.794279
2017-12-10T03:33:17.567926: step 1940, loss 0.205986, acc 0.9375, prec 0.0505552, recall 0.794355
2017-12-10T03:33:17.841697: step 1941, loss 0.632616, acc 0.828125, prec 0.0505423, recall 0.794355
2017-12-10T03:33:18.108940: step 1942, loss 0.466619, acc 0.828125, prec 0.0505293, recall 0.794355
2017-12-10T03:33:18.373018: step 1943, loss 0.285338, acc 0.890625, prec 0.0505653, recall 0.794505
2017-12-10T03:33:18.632906: step 1944, loss 0.365774, acc 0.875, prec 0.0506002, recall 0.794656
2017-12-10T03:33:18.900617: step 1945, loss 0.590537, acc 0.84375, prec 0.0505884, recall 0.794656
2017-12-10T03:33:19.170864: step 1946, loss 0.462283, acc 0.921875, prec 0.0506046, recall 0.794731
2017-12-10T03:33:19.436198: step 1947, loss 0.524527, acc 0.9375, prec 0.0506883, recall 0.795031
2017-12-10T03:33:19.701724: step 1948, loss 0.32401, acc 0.84375, prec 0.0506765, recall 0.795031
2017-12-10T03:33:19.972550: step 1949, loss 0.399049, acc 0.828125, prec 0.0506636, recall 0.795031
2017-12-10T03:33:20.239316: step 1950, loss 0.281664, acc 0.9375, prec 0.0506588, recall 0.795031
2017-12-10T03:33:20.510376: step 1951, loss 0.564456, acc 0.953125, prec 0.0506774, recall 0.795106
2017-12-10T03:33:20.772888: step 1952, loss 0.838247, acc 0.890625, prec 0.0506912, recall 0.795181
2017-12-10T03:33:21.038287: step 1953, loss 0.221188, acc 0.921875, prec 0.0506853, recall 0.795181
2017-12-10T03:33:21.308343: step 1954, loss 0.154218, acc 0.9375, prec 0.0507027, recall 0.795255
2017-12-10T03:33:21.575454: step 1955, loss 0.322034, acc 0.9375, prec 0.0507201, recall 0.79533
2017-12-10T03:33:21.841333: step 1956, loss 0.424876, acc 0.90625, prec 0.050713, recall 0.79533
2017-12-10T03:33:22.104156: step 1957, loss 0.218024, acc 0.921875, prec 0.0507071, recall 0.79533
2017-12-10T03:33:22.362953: step 1958, loss 0.23683, acc 0.890625, prec 0.0506989, recall 0.79533
2017-12-10T03:33:22.629424: step 1959, loss 0.112212, acc 0.984375, prec 0.0507197, recall 0.795405
2017-12-10T03:33:22.892577: step 1960, loss 0.59916, acc 0.875, prec 0.0507545, recall 0.795554
2017-12-10T03:33:23.156784: step 1961, loss 0.0658157, acc 0.984375, prec 0.0507533, recall 0.795554
2017-12-10T03:33:23.428950: step 1962, loss 0.337068, acc 0.9375, prec 0.0507927, recall 0.795703
2017-12-10T03:33:23.686901: step 1963, loss 0.116478, acc 0.953125, prec 0.0507892, recall 0.795703
2017-12-10T03:33:23.947100: step 1964, loss 0.222326, acc 0.921875, prec 0.0508053, recall 0.795777
2017-12-10T03:33:24.215675: step 1965, loss 0.185046, acc 0.9375, prec 0.0508447, recall 0.795926
2017-12-10T03:33:24.483443: step 1966, loss 0.123777, acc 0.984375, prec 0.0508656, recall 0.796
2017-12-10T03:33:24.745148: step 1967, loss 3.10052, acc 0.90625, prec 0.0508817, recall 0.795785
2017-12-10T03:33:25.010300: step 1968, loss 0.126463, acc 0.9375, prec 0.050877, recall 0.795785
2017-12-10T03:33:25.276603: step 1969, loss 0.159757, acc 0.9375, prec 0.0508943, recall 0.795859
2017-12-10T03:33:25.541940: step 1970, loss 0.35153, acc 0.9375, prec 0.0509116, recall 0.795933
2017-12-10T03:33:25.808990: step 1971, loss 0.212468, acc 0.953125, prec 0.0509522, recall 0.796081
2017-12-10T03:33:26.078363: step 1972, loss 6.32822, acc 0.921875, prec 0.0509474, recall 0.795793
2017-12-10T03:33:26.347314: step 1973, loss 0.351778, acc 0.875, prec 0.050938, recall 0.795793
2017-12-10T03:33:26.611834: step 1974, loss 6.60907, acc 0.8125, prec 0.050969, recall 0.795652
2017-12-10T03:33:26.891203: step 1975, loss 0.69929, acc 0.875, prec 0.0510036, recall 0.7958
2017-12-10T03:33:27.154463: step 1976, loss 0.399341, acc 0.890625, prec 0.0510173, recall 0.795874
2017-12-10T03:33:27.415519: step 1977, loss 0.730543, acc 0.828125, prec 0.0510263, recall 0.795948
2017-12-10T03:33:27.686896: step 1978, loss 1.06213, acc 0.640625, prec 0.0510431, recall 0.796095
2017-12-10T03:33:27.947472: step 1979, loss 1.22169, acc 0.703125, prec 0.0510206, recall 0.796095
2017-12-10T03:33:28.217797: step 1980, loss 0.738437, acc 0.71875, prec 0.0509994, recall 0.796095
2017-12-10T03:33:28.479252: step 1981, loss 1.31512, acc 0.671875, prec 0.0510185, recall 0.796243
2017-12-10T03:33:28.740485: step 1982, loss 1.45968, acc 0.546875, prec 0.0509843, recall 0.796243
2017-12-10T03:33:29.001649: step 1983, loss 1.47034, acc 0.6875, prec 0.0509607, recall 0.796243
2017-12-10T03:33:29.262223: step 1984, loss 1.22646, acc 0.65625, prec 0.0509787, recall 0.79639
2017-12-10T03:33:29.528056: step 1985, loss 0.781833, acc 0.640625, prec 0.0509735, recall 0.796463
2017-12-10T03:33:29.786681: step 1986, loss 1.12803, acc 0.703125, prec 0.0509511, recall 0.796463
2017-12-10T03:33:30.054591: step 1987, loss 0.449376, acc 0.859375, prec 0.0509625, recall 0.796537
2017-12-10T03:33:30.289514: step 1988, loss 1.10686, acc 0.705882, prec 0.0509886, recall 0.796683
2017-12-10T03:33:30.566596: step 1989, loss 0.260866, acc 0.875, prec 0.0509792, recall 0.796683
2017-12-10T03:33:30.834215: step 1990, loss 0.393346, acc 0.875, prec 0.0509698, recall 0.796683
2017-12-10T03:33:31.094680: step 1991, loss 0.365923, acc 0.859375, prec 0.051003, recall 0.79683
2017-12-10T03:33:31.362495: step 1992, loss 0.494299, acc 0.828125, prec 0.0510119, recall 0.796903
2017-12-10T03:33:31.627675: step 1993, loss 3.24537, acc 0.859375, prec 0.0510244, recall 0.796689
2017-12-10T03:33:31.893845: step 1994, loss 0.206248, acc 0.921875, prec 0.0510185, recall 0.796689
2017-12-10T03:33:32.161911: step 1995, loss 0.316675, acc 0.90625, prec 0.0510333, recall 0.796763
2017-12-10T03:33:32.420066: step 1996, loss 0.207714, acc 0.953125, prec 0.0510517, recall 0.796836
2017-12-10T03:33:32.684502: step 1997, loss 0.151109, acc 0.921875, prec 0.0510458, recall 0.796836
2017-12-10T03:33:32.953994: step 1998, loss 0.185374, acc 0.921875, prec 0.0510836, recall 0.796982
2017-12-10T03:33:33.215622: step 1999, loss 0.262068, acc 0.890625, prec 0.0510754, recall 0.796982
2017-12-10T03:33:33.482658: step 2000, loss 0.517081, acc 0.96875, prec 0.0510949, recall 0.797055
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-2000

2017-12-10T03:33:34.852101: step 2001, loss 2.05862, acc 0.890625, prec 0.0511534, recall 0.796987
2017-12-10T03:33:35.121690: step 2002, loss 0.200689, acc 0.921875, prec 0.0511693, recall 0.79706
2017-12-10T03:33:35.390422: step 2003, loss 0.365551, acc 0.9375, prec 0.0511864, recall 0.797133
2017-12-10T03:33:35.656325: step 2004, loss 6.28796, acc 0.859375, prec 0.0511989, recall 0.79692
2017-12-10T03:33:35.922851: step 2005, loss 1.88079, acc 0.828125, prec 0.0512307, recall 0.79678
2017-12-10T03:33:36.188448: step 2006, loss 0.540751, acc 0.875, prec 0.0512213, recall 0.79678
2017-12-10T03:33:36.452809: step 2007, loss 1.18657, acc 0.671875, prec 0.0512184, recall 0.796853
2017-12-10T03:33:36.716185: step 2008, loss 0.741488, acc 0.796875, prec 0.0512467, recall 0.796998
2017-12-10T03:33:36.987319: step 2009, loss 1.18368, acc 0.6875, prec 0.0512232, recall 0.796998
2017-12-10T03:33:37.246809: step 2010, loss 0.883165, acc 0.78125, prec 0.0512285, recall 0.79707
2017-12-10T03:33:37.517094: step 2011, loss 0.990427, acc 0.6875, prec 0.051205, recall 0.79707
2017-12-10T03:33:37.776151: step 2012, loss 1.30876, acc 0.703125, prec 0.0512262, recall 0.797215
2017-12-10T03:33:38.041691: step 2013, loss 1.50925, acc 0.671875, prec 0.0512015, recall 0.797215
2017-12-10T03:33:38.301820: step 2014, loss 1.49167, acc 0.609375, prec 0.0512374, recall 0.797432
2017-12-10T03:33:38.562893: step 2015, loss 1.29267, acc 0.671875, prec 0.0512128, recall 0.797432
2017-12-10T03:33:38.827121: step 2016, loss 1.43578, acc 0.59375, prec 0.051204, recall 0.797504
2017-12-10T03:33:39.088918: step 2017, loss 1.35646, acc 0.703125, prec 0.0512034, recall 0.797577
2017-12-10T03:33:39.352272: step 2018, loss 0.814627, acc 0.796875, prec 0.0512316, recall 0.797721
2017-12-10T03:33:39.614082: step 2019, loss 0.629598, acc 0.75, prec 0.0512346, recall 0.797793
2017-12-10T03:33:39.883700: step 2020, loss 0.653645, acc 0.734375, prec 0.0512363, recall 0.797865
2017-12-10T03:33:40.143251: step 2021, loss 0.572597, acc 0.875, prec 0.0512487, recall 0.797937
2017-12-10T03:33:40.403576: step 2022, loss 3.60325, acc 0.796875, prec 0.0512563, recall 0.797725
2017-12-10T03:33:40.671653: step 2023, loss 0.720866, acc 0.84375, prec 0.0512879, recall 0.797869
2017-12-10T03:33:40.931296: step 2024, loss 0.886663, acc 0.828125, prec 0.0513183, recall 0.798012
2017-12-10T03:33:41.195651: step 2025, loss 0.334235, acc 0.875, prec 0.0513306, recall 0.798084
2017-12-10T03:33:41.464006: step 2026, loss 0.164935, acc 0.96875, prec 0.0513499, recall 0.798155
2017-12-10T03:33:41.741801: step 2027, loss 0.48367, acc 0.890625, prec 0.051385, recall 0.798298
2017-12-10T03:33:42.003452: step 2028, loss 1.2885, acc 0.875, prec 0.0514406, recall 0.798513
2017-12-10T03:33:42.274141: step 2029, loss 0.242253, acc 0.90625, prec 0.0514335, recall 0.798513
2017-12-10T03:33:42.539673: step 2030, loss 0.374119, acc 0.890625, prec 0.0514686, recall 0.798655
2017-12-10T03:33:42.805251: step 2031, loss 0.203413, acc 0.9375, prec 0.0514855, recall 0.798727
2017-12-10T03:33:43.078236: step 2032, loss 0.590161, acc 0.890625, prec 0.0515205, recall 0.798869
2017-12-10T03:33:43.352736: step 2033, loss 0.374705, acc 0.890625, prec 0.0515339, recall 0.79894
2017-12-10T03:33:43.622439: step 2034, loss 0.473284, acc 0.90625, prec 0.0515269, recall 0.79894
2017-12-10T03:33:43.886880: step 2035, loss 0.263548, acc 0.921875, prec 0.051521, recall 0.79894
2017-12-10T03:33:44.146320: step 2036, loss 0.250185, acc 0.921875, prec 0.0515152, recall 0.79894
2017-12-10T03:33:44.420473: step 2037, loss 0.118514, acc 0.90625, prec 0.0515297, recall 0.799011
2017-12-10T03:33:44.684628: step 2038, loss 0.26719, acc 0.90625, prec 0.0515443, recall 0.799082
2017-12-10T03:33:44.946189: step 2039, loss 0.0675702, acc 0.953125, prec 0.0515408, recall 0.799082
2017-12-10T03:33:45.216257: step 2040, loss 0.152782, acc 0.9375, prec 0.0515577, recall 0.799153
2017-12-10T03:33:45.483571: step 2041, loss 1.27579, acc 0.890625, prec 0.0515926, recall 0.799295
2017-12-10T03:33:45.749631: step 2042, loss 1.21172, acc 0.96875, prec 0.0516131, recall 0.799084
2017-12-10T03:33:46.019536: step 2043, loss 0.102701, acc 0.953125, prec 0.0516311, recall 0.799154
2017-12-10T03:33:46.288656: step 2044, loss 0.282803, acc 0.90625, prec 0.0516457, recall 0.799225
2017-12-10T03:33:46.556224: step 2045, loss 8.90057, acc 0.921875, prec 0.0516625, recall 0.799014
2017-12-10T03:33:46.822846: step 2046, loss 0.531133, acc 0.859375, prec 0.0516735, recall 0.799085
2017-12-10T03:33:47.087514: step 2047, loss 0.440177, acc 0.875, prec 0.0516641, recall 0.799085
2017-12-10T03:33:47.355216: step 2048, loss 0.296502, acc 0.984375, prec 0.0517061, recall 0.799226
2017-12-10T03:33:47.626859: step 2049, loss 4.13357, acc 0.90625, prec 0.0517002, recall 0.798946
2017-12-10T03:33:47.892023: step 2050, loss 0.679231, acc 0.765625, prec 0.0517041, recall 0.799016
2017-12-10T03:33:48.156182: step 2051, loss 0.227121, acc 0.9375, prec 0.051721, recall 0.799087
2017-12-10T03:33:48.419880: step 2052, loss 0.475403, acc 0.828125, prec 0.0517296, recall 0.799157
2017-12-10T03:33:48.682372: step 2053, loss 0.708203, acc 0.796875, prec 0.0517143, recall 0.799157
2017-12-10T03:33:48.949001: step 2054, loss 1.02981, acc 0.65625, prec 0.0516885, recall 0.799157
2017-12-10T03:33:49.209990: step 2055, loss 0.667531, acc 0.8125, prec 0.0516744, recall 0.799157
2017-12-10T03:33:49.472040: step 2056, loss 0.954549, acc 0.75, prec 0.0516557, recall 0.799157
2017-12-10T03:33:49.733766: step 2057, loss 1.24171, acc 0.78125, prec 0.0516823, recall 0.799298
2017-12-10T03:33:50.001694: step 2058, loss 0.897134, acc 0.75, prec 0.0516635, recall 0.799298
2017-12-10T03:33:50.263405: step 2059, loss 0.589455, acc 0.796875, prec 0.0516698, recall 0.799369
2017-12-10T03:33:50.525742: step 2060, loss 1.09918, acc 0.78125, prec 0.0516749, recall 0.799439
2017-12-10T03:33:50.797622: step 2061, loss 0.567047, acc 0.859375, prec 0.0517073, recall 0.79958
2017-12-10T03:33:51.056451: step 2062, loss 1.35923, acc 0.671875, prec 0.0517042, recall 0.79965
2017-12-10T03:33:51.318432: step 2063, loss 0.526489, acc 0.84375, prec 0.051714, recall 0.79972
2017-12-10T03:33:51.585201: step 2064, loss 0.656464, acc 0.90625, prec 0.0517284, recall 0.79979
2017-12-10T03:33:51.847150: step 2065, loss 0.360118, acc 0.796875, prec 0.0517347, recall 0.79986
2017-12-10T03:33:52.110032: step 2066, loss 0.697321, acc 0.859375, prec 0.051767, recall 0.8
2017-12-10T03:33:52.371053: step 2067, loss 0.348248, acc 0.90625, prec 0.05176, recall 0.8
2017-12-10T03:33:52.640267: step 2068, loss 0.348008, acc 0.875, prec 0.0517507, recall 0.8
2017-12-10T03:33:52.902092: step 2069, loss 0.113213, acc 0.953125, prec 0.0517471, recall 0.8
2017-12-10T03:33:53.168357: step 2070, loss 0.172331, acc 0.90625, prec 0.0517616, recall 0.80007
2017-12-10T03:33:53.432785: step 2071, loss 0.295514, acc 0.890625, prec 0.0517534, recall 0.80007
2017-12-10T03:33:53.700015: step 2072, loss 0.216598, acc 0.9375, prec 0.0517701, recall 0.80014
2017-12-10T03:33:53.971005: step 2073, loss 0.14493, acc 0.96875, prec 0.0517678, recall 0.80014
2017-12-10T03:33:54.236825: step 2074, loss 0.113024, acc 0.96875, prec 0.0517869, recall 0.80021
2017-12-10T03:33:54.510488: step 2075, loss 0.0999718, acc 0.9375, prec 0.0517822, recall 0.80021
2017-12-10T03:33:54.769062: step 2076, loss 3.01598, acc 0.90625, prec 0.0517764, recall 0.79993
2017-12-10T03:33:55.035748: step 2077, loss 0.891023, acc 0.984375, prec 0.051818, recall 0.80007
2017-12-10T03:33:55.305670: step 2078, loss 0.119362, acc 0.953125, prec 0.0518145, recall 0.80007
2017-12-10T03:33:55.575907: step 2079, loss 1.18155, acc 0.90625, prec 0.0518289, recall 0.80014
2017-12-10T03:33:55.844718: step 2080, loss 0.204345, acc 0.9375, prec 0.0518671, recall 0.800279
2017-12-10T03:33:56.109085: step 2081, loss 0.080578, acc 0.96875, prec 0.0518647, recall 0.800279
2017-12-10T03:33:56.372801: step 2082, loss 0.274545, acc 0.875, prec 0.0518768, recall 0.800348
2017-12-10T03:33:56.654571: step 2083, loss 0.332756, acc 0.890625, prec 0.0519114, recall 0.800487
2017-12-10T03:33:56.922365: step 2084, loss 0.236394, acc 0.90625, prec 0.0519044, recall 0.800487
2017-12-10T03:33:57.189937: step 2085, loss 0.130785, acc 0.953125, prec 0.0519008, recall 0.800487
2017-12-10T03:33:57.453434: step 2086, loss 0.62541, acc 0.890625, prec 0.051914, recall 0.800557
2017-12-10T03:33:57.726408: step 2087, loss 0.337685, acc 0.859375, prec 0.0519035, recall 0.800557
2017-12-10T03:33:57.990516: step 2088, loss 0.304447, acc 0.921875, prec 0.051919, recall 0.800626
2017-12-10T03:33:58.257122: step 2089, loss 0.394825, acc 0.859375, prec 0.0519513, recall 0.800765
2017-12-10T03:33:58.520478: step 2090, loss 0.340452, acc 0.9375, prec 0.0519466, recall 0.800765
2017-12-10T03:33:58.782862: step 2091, loss 0.846526, acc 0.96875, prec 0.0519656, recall 0.800834
2017-12-10T03:33:59.053985: step 2092, loss 0.187644, acc 0.9375, prec 0.0519823, recall 0.800903
2017-12-10T03:33:59.315411: step 2093, loss 1.3922, acc 0.875, prec 0.0519943, recall 0.800973
2017-12-10T03:33:59.583970: step 2094, loss 0.137872, acc 0.9375, prec 0.052011, recall 0.801042
2017-12-10T03:33:59.851405: step 2095, loss 0.282133, acc 0.90625, prec 0.052004, recall 0.801042
2017-12-10T03:34:00.124949: step 2096, loss 0.536824, acc 0.84375, prec 0.0519922, recall 0.801042
2017-12-10T03:34:00.396295: step 2097, loss 0.442932, acc 0.90625, prec 0.0519852, recall 0.801042
2017-12-10T03:34:00.666475: step 2098, loss 0.245628, acc 0.90625, prec 0.0519782, recall 0.801042
2017-12-10T03:34:00.929470: step 2099, loss 0.492147, acc 0.859375, prec 0.051989, recall 0.801111
2017-12-10T03:34:01.187843: step 2100, loss 0.432897, acc 0.859375, prec 0.0519785, recall 0.801111

Evaluation:
2017-12-10T03:34:09.019814: step 2100, loss 1.51568, acc 0.885933, prec 0.0528148, recall 0.795515

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-2100

2017-12-10T03:34:10.316108: step 2101, loss 0.433248, acc 0.875, prec 0.0528056, recall 0.795515
2017-12-10T03:34:10.580180: step 2102, loss 0.531156, acc 0.875, prec 0.0527963, recall 0.795515
2017-12-10T03:34:10.846138: step 2103, loss 0.139263, acc 0.953125, prec 0.0528136, recall 0.795582
2017-12-10T03:34:11.118625: step 2104, loss 1.04965, acc 0.921875, prec 0.0528285, recall 0.795649
2017-12-10T03:34:11.387968: step 2105, loss 0.395098, acc 0.90625, prec 0.0528423, recall 0.795717
2017-12-10T03:34:11.659922: step 2106, loss 0.22626, acc 0.96875, prec 0.0528607, recall 0.795784
2017-12-10T03:34:11.922784: step 2107, loss 0.48877, acc 0.84375, prec 0.0529113, recall 0.795986
2017-12-10T03:34:12.191781: step 2108, loss 0.214639, acc 0.921875, prec 0.0529055, recall 0.795986
2017-12-10T03:34:12.462659: step 2109, loss 0.617596, acc 0.875, prec 0.0528963, recall 0.795986
2017-12-10T03:34:12.722490: step 2110, loss 0.38414, acc 0.84375, prec 0.0529468, recall 0.796187
2017-12-10T03:34:12.987134: step 2111, loss 1.02207, acc 0.890625, prec 0.0529594, recall 0.796254
2017-12-10T03:34:13.248287: step 2112, loss 0.77185, acc 0.796875, prec 0.0529651, recall 0.796321
2017-12-10T03:34:13.512364: step 2113, loss 0.485465, acc 0.90625, prec 0.0529788, recall 0.796387
2017-12-10T03:34:13.774649: step 2114, loss 0.158749, acc 0.921875, prec 0.052973, recall 0.796387
2017-12-10T03:34:14.034779: step 2115, loss 1.75473, acc 0.921875, prec 0.0530098, recall 0.79626
2017-12-10T03:34:14.303996: step 2116, loss 0.329109, acc 0.890625, prec 0.0530224, recall 0.796327
2017-12-10T03:34:14.565669: step 2117, loss 0.138401, acc 0.96875, prec 0.0530407, recall 0.796393
2017-12-10T03:34:14.827374: step 2118, loss 0.4359, acc 0.859375, prec 0.0530303, recall 0.796393
2017-12-10T03:34:15.086413: step 2119, loss 0.412899, acc 0.875, prec 0.0530624, recall 0.796527
2017-12-10T03:34:15.346698: step 2120, loss 0.531982, acc 0.890625, prec 0.0530749, recall 0.796593
2017-12-10T03:34:15.609882: step 2121, loss 0.275746, acc 0.890625, prec 0.0531082, recall 0.796727
2017-12-10T03:34:15.868407: step 2122, loss 0.240955, acc 0.90625, prec 0.0531012, recall 0.796727
2017-12-10T03:34:16.139129: step 2123, loss 0.305869, acc 0.890625, prec 0.0531138, recall 0.796793
2017-12-10T03:34:16.401599: step 2124, loss 0.229142, acc 0.90625, prec 0.0531275, recall 0.79686
2017-12-10T03:34:16.665533: step 2125, loss 0.247787, acc 0.921875, prec 0.053163, recall 0.796992
2017-12-10T03:34:16.926953: step 2126, loss 0.628961, acc 0.875, prec 0.0531743, recall 0.797059
2017-12-10T03:34:17.196052: step 2127, loss 0.353024, acc 0.921875, prec 0.0531892, recall 0.797125
2017-12-10T03:34:17.456457: step 2128, loss 0.0900407, acc 0.953125, prec 0.0531857, recall 0.797125
2017-12-10T03:34:17.727088: step 2129, loss 0.097688, acc 0.9375, prec 0.0531811, recall 0.797125
2017-12-10T03:34:17.992304: step 2130, loss 0.481892, acc 0.875, prec 0.0531718, recall 0.797125
2017-12-10T03:34:18.254681: step 2131, loss 0.17975, acc 0.921875, prec 0.053166, recall 0.797125
2017-12-10T03:34:18.515510: step 2132, loss 1.62271, acc 0.921875, prec 0.0531614, recall 0.796865
2017-12-10T03:34:18.777987: step 2133, loss 1.97765, acc 0.921875, prec 0.053198, recall 0.796737
2017-12-10T03:34:19.044294: step 2134, loss 0.432292, acc 0.859375, prec 0.0531875, recall 0.796737
2017-12-10T03:34:19.310089: step 2135, loss 0.141732, acc 0.953125, prec 0.0531841, recall 0.796737
2017-12-10T03:34:19.579745: step 2136, loss 3.94375, acc 0.9375, prec 0.0532631, recall 0.796743
2017-12-10T03:34:19.844739: step 2137, loss 0.203282, acc 0.921875, prec 0.0532573, recall 0.796743
2017-12-10T03:34:20.112094: step 2138, loss 0.426595, acc 0.890625, prec 0.0532492, recall 0.796743
2017-12-10T03:34:20.376580: step 2139, loss 1.01028, acc 0.859375, prec 0.0532799, recall 0.796875
2017-12-10T03:34:20.650699: step 2140, loss 0.573001, acc 0.8125, prec 0.0533072, recall 0.797007
2017-12-10T03:34:20.916872: step 2141, loss 0.825627, acc 0.6875, prec 0.0533046, recall 0.797073
2017-12-10T03:34:21.184819: step 2142, loss 0.896785, acc 0.703125, prec 0.0533238, recall 0.797205
2017-12-10T03:34:21.450433: step 2143, loss 0.691175, acc 0.75, prec 0.0533464, recall 0.797337
2017-12-10T03:34:21.710439: step 2144, loss 0.811935, acc 0.6875, prec 0.0533438, recall 0.797403
2017-12-10T03:34:21.981407: step 2145, loss 1.29126, acc 0.625, prec 0.0533365, recall 0.797468
2017-12-10T03:34:22.247730: step 2146, loss 1.04072, acc 0.765625, prec 0.0533602, recall 0.7976
2017-12-10T03:34:22.515976: step 2147, loss 1.72894, acc 0.59375, prec 0.0533507, recall 0.797665
2017-12-10T03:34:22.782048: step 2148, loss 0.588242, acc 0.78125, prec 0.0533345, recall 0.797665
2017-12-10T03:34:23.044606: step 2149, loss 0.488272, acc 0.890625, prec 0.0533469, recall 0.797731
2017-12-10T03:34:23.306345: step 2150, loss 0.689214, acc 0.828125, prec 0.0533547, recall 0.797796
2017-12-10T03:34:23.570751: step 2151, loss 1.10128, acc 0.765625, prec 0.0533989, recall 0.797993
2017-12-10T03:34:23.828059: step 2152, loss 0.453088, acc 0.796875, prec 0.0534044, recall 0.798058
2017-12-10T03:34:24.092401: step 2153, loss 0.453396, acc 0.859375, prec 0.053394, recall 0.798058
2017-12-10T03:34:24.361344: step 2154, loss 0.570205, acc 0.8125, prec 0.0534006, recall 0.798124
2017-12-10T03:34:24.623839: step 2155, loss 0.290177, acc 0.875, prec 0.0533913, recall 0.798124
2017-12-10T03:34:24.882770: step 2156, loss 0.319438, acc 0.90625, prec 0.0534049, recall 0.798189
2017-12-10T03:34:25.152411: step 2157, loss 0.185209, acc 0.953125, prec 0.0534219, recall 0.798254
2017-12-10T03:34:25.422903: step 2158, loss 1.77716, acc 0.921875, prec 0.0534378, recall 0.798061
2017-12-10T03:34:25.690648: step 2159, loss 0.291331, acc 0.921875, prec 0.0534729, recall 0.798192
2017-12-10T03:34:25.961947: step 2160, loss 0.223, acc 0.953125, prec 0.0534899, recall 0.798257
2017-12-10T03:34:26.238507: step 2161, loss 2.72673, acc 0.921875, prec 0.0534853, recall 0.797999
2017-12-10T03:34:26.514752: step 2162, loss 0.361372, acc 0.953125, prec 0.0535432, recall 0.798195
2017-12-10T03:34:26.790111: step 2163, loss 0.415562, acc 0.890625, prec 0.0535351, recall 0.798195
2017-12-10T03:34:27.052724: step 2164, loss 0.382624, acc 0.84375, prec 0.0535236, recall 0.798195
2017-12-10T03:34:27.320044: step 2165, loss 0.306194, acc 0.9375, prec 0.0535599, recall 0.798325
2017-12-10T03:34:27.584079: step 2166, loss 0.472707, acc 0.921875, prec 0.0535541, recall 0.798325
2017-12-10T03:34:27.843493: step 2167, loss 4.51949, acc 0.875, prec 0.053546, recall 0.798068
2017-12-10T03:34:28.111383: step 2168, loss 0.390811, acc 0.875, prec 0.0535367, recall 0.798068
2017-12-10T03:34:28.376863: step 2169, loss 0.925389, acc 0.890625, prec 0.0535491, recall 0.798133
2017-12-10T03:34:28.641565: step 2170, loss 0.515142, acc 0.84375, prec 0.0535784, recall 0.798263
2017-12-10T03:34:28.905511: step 2171, loss 0.65573, acc 0.765625, prec 0.053561, recall 0.798263
2017-12-10T03:34:29.171847: step 2172, loss 0.493065, acc 0.90625, prec 0.0535745, recall 0.798327
2017-12-10T03:34:29.443886: step 2173, loss 0.678124, acc 0.8125, prec 0.0535811, recall 0.798392
2017-12-10T03:34:29.708786: step 2174, loss 0.795787, acc 0.796875, prec 0.053566, recall 0.798392
2017-12-10T03:34:29.977310: step 2175, loss 1.10003, acc 0.703125, prec 0.0535849, recall 0.798522
2017-12-10T03:34:30.245435: step 2176, loss 0.832413, acc 0.734375, prec 0.0535653, recall 0.798522
2017-12-10T03:34:30.511008: step 2177, loss 0.819348, acc 0.65625, prec 0.0535603, recall 0.798587
2017-12-10T03:34:30.780430: step 2178, loss 0.802462, acc 0.65625, prec 0.0535349, recall 0.798587
2017-12-10T03:34:31.036304: step 2179, loss 0.346304, acc 0.84375, prec 0.0535437, recall 0.798651
2017-12-10T03:34:31.303499: step 2180, loss 0.475923, acc 0.84375, prec 0.053573, recall 0.798781
2017-12-10T03:34:31.567256: step 2181, loss 0.423186, acc 0.921875, prec 0.0535672, recall 0.798781
2017-12-10T03:34:31.833060: step 2182, loss 0.38118, acc 0.890625, prec 0.0535795, recall 0.798845
2017-12-10T03:34:32.100057: step 2183, loss 0.321707, acc 0.890625, prec 0.0535918, recall 0.79891
2017-12-10T03:34:32.364747: step 2184, loss 0.286198, acc 0.890625, prec 0.0536041, recall 0.798974
2017-12-10T03:34:32.625340: step 2185, loss 0.331306, acc 0.90625, prec 0.0536379, recall 0.799103
2017-12-10T03:34:32.901942: step 2186, loss 0.194408, acc 0.96875, prec 0.0536559, recall 0.799167
2017-12-10T03:34:33.164208: step 2187, loss 0.129068, acc 0.984375, prec 0.0536548, recall 0.799167
2017-12-10T03:34:33.434198: step 2188, loss 0.0867609, acc 0.953125, prec 0.0536513, recall 0.799167
2017-12-10T03:34:33.693788: step 2189, loss 0.25566, acc 0.921875, prec 0.0536455, recall 0.799167
2017-12-10T03:34:33.960696: step 2190, loss 0.077298, acc 0.96875, prec 0.0536432, recall 0.799167
2017-12-10T03:34:34.225005: step 2191, loss 2.7939, acc 0.90625, prec 0.0536375, recall 0.798911
2017-12-10T03:34:34.498829: step 2192, loss 0.110956, acc 0.9375, prec 0.0536532, recall 0.798976
2017-12-10T03:34:34.764240: step 2193, loss 0.128543, acc 0.953125, prec 0.0537107, recall 0.799169
2017-12-10T03:34:35.028091: step 2194, loss 0.250691, acc 0.9375, prec 0.0537265, recall 0.799233
2017-12-10T03:34:35.293464: step 2195, loss 0.342794, acc 0.9375, prec 0.0537625, recall 0.799361
2017-12-10T03:34:35.555698: step 2196, loss 0.0780128, acc 0.96875, prec 0.0538009, recall 0.799489
2017-12-10T03:34:35.820729: step 2197, loss 3.162, acc 0.984375, prec 0.0538212, recall 0.799298
2017-12-10T03:34:36.087499: step 2198, loss 0.131722, acc 0.9375, prec 0.0538166, recall 0.799298
2017-12-10T03:34:36.360289: step 2199, loss 1.9925, acc 0.96875, prec 0.0538357, recall 0.799107
2017-12-10T03:34:36.625586: step 2200, loss 0.245814, acc 0.90625, prec 0.0538288, recall 0.799107
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-2200

2017-12-10T03:34:38.007975: step 2201, loss 0.596319, acc 0.84375, prec 0.0538376, recall 0.799171
2017-12-10T03:34:38.272974: step 2202, loss 0.43446, acc 0.828125, prec 0.0538248, recall 0.799171
2017-12-10T03:34:38.545922: step 2203, loss 0.71411, acc 0.828125, prec 0.0538528, recall 0.799299
2017-12-10T03:34:38.811007: step 2204, loss 0.70898, acc 0.765625, prec 0.0538354, recall 0.799299
2017-12-10T03:34:39.081658: step 2205, loss 0.940037, acc 0.734375, prec 0.0538767, recall 0.799491
2017-12-10T03:34:39.344171: step 2206, loss 0.696267, acc 0.734375, prec 0.053857, recall 0.799491
2017-12-10T03:34:39.609519: step 2207, loss 0.927446, acc 0.71875, prec 0.0538565, recall 0.799555
2017-12-10T03:34:39.871294: step 2208, loss 0.887608, acc 0.734375, prec 0.0538369, recall 0.799555
2017-12-10T03:34:40.134684: step 2209, loss 0.822023, acc 0.71875, prec 0.0538162, recall 0.799555
2017-12-10T03:34:40.393454: step 2210, loss 1.10753, acc 0.765625, prec 0.0538394, recall 0.799682
2017-12-10T03:34:40.657578: step 2211, loss 0.523984, acc 0.828125, prec 0.0538267, recall 0.799682
2017-12-10T03:34:40.920822: step 2212, loss 0.841844, acc 0.75, prec 0.0538083, recall 0.799682
2017-12-10T03:34:41.183093: step 2213, loss 0.86723, acc 0.8125, prec 0.0537945, recall 0.799682
2017-12-10T03:34:41.446382: step 2214, loss 0.856385, acc 0.8125, prec 0.0537807, recall 0.799682
2017-12-10T03:34:41.718990: step 2215, loss 0.446719, acc 0.828125, prec 0.053768, recall 0.799682
2017-12-10T03:34:41.986593: step 2216, loss 0.606379, acc 0.796875, prec 0.0537531, recall 0.799682
2017-12-10T03:34:42.247098: step 2217, loss 0.245192, acc 0.90625, prec 0.0537462, recall 0.799682
2017-12-10T03:34:42.509232: step 2218, loss 0.201153, acc 0.90625, prec 0.0537798, recall 0.799809
2017-12-10T03:34:42.774933: step 2219, loss 0.194888, acc 0.9375, prec 0.0537954, recall 0.799873
2017-12-10T03:34:43.042369: step 2220, loss 0.166721, acc 0.90625, prec 0.0537885, recall 0.799873
2017-12-10T03:34:43.306042: step 2221, loss 0.116148, acc 0.953125, prec 0.0538255, recall 0.8
2017-12-10T03:34:43.577158: step 2222, loss 0.0290738, acc 0.984375, prec 0.0538243, recall 0.8
2017-12-10T03:34:43.842039: step 2223, loss 0.174187, acc 0.984375, prec 0.0538232, recall 0.8
2017-12-10T03:34:44.109259: step 2224, loss 0.0720314, acc 0.96875, prec 0.0538411, recall 0.800063
2017-12-10T03:34:44.374503: step 2225, loss 9.24086, acc 0.984375, prec 0.0538411, recall 0.79981
2017-12-10T03:34:44.641269: step 2226, loss 0.0652309, acc 0.96875, prec 0.0538388, recall 0.79981
2017-12-10T03:34:44.911818: step 2227, loss 0.0842634, acc 0.96875, prec 0.0538365, recall 0.79981
2017-12-10T03:34:45.177650: step 2228, loss 0.15146, acc 0.953125, prec 0.053833, recall 0.79981
2017-12-10T03:34:45.444087: step 2229, loss 0.0598264, acc 1, prec 0.0538532, recall 0.799873
2017-12-10T03:34:45.713793: step 2230, loss 0.172234, acc 0.9375, prec 0.0538688, recall 0.799937
2017-12-10T03:34:45.984024: step 2231, loss 0.118509, acc 0.96875, prec 0.0538867, recall 0.8
2017-12-10T03:34:46.254143: step 2232, loss 0.358281, acc 0.9375, prec 0.0539023, recall 0.800063
2017-12-10T03:34:46.520931: step 2233, loss 0.140158, acc 0.96875, prec 0.0539, recall 0.800063
2017-12-10T03:34:46.780067: step 2234, loss 0.154017, acc 0.953125, prec 0.0539168, recall 0.800127
2017-12-10T03:34:47.044154: step 2235, loss 0.107446, acc 0.96875, prec 0.0539145, recall 0.800127
2017-12-10T03:34:47.308716: step 2236, loss 0.342167, acc 0.890625, prec 0.0539064, recall 0.800127
2017-12-10T03:34:47.569772: step 2237, loss 0.334545, acc 0.953125, prec 0.0539231, recall 0.80019
2017-12-10T03:34:47.838142: step 2238, loss 0.390977, acc 0.9375, prec 0.0539589, recall 0.800316
2017-12-10T03:34:48.107154: step 2239, loss 0.414006, acc 0.875, prec 0.0539699, recall 0.80038
2017-12-10T03:34:48.373145: step 2240, loss 0.415352, acc 0.890625, prec 0.053982, recall 0.800443
2017-12-10T03:34:48.637307: step 2241, loss 1.94899, acc 0.890625, prec 0.0539751, recall 0.80019
2017-12-10T03:34:48.907039: step 2242, loss 0.28733, acc 0.921875, prec 0.0539895, recall 0.800253
2017-12-10T03:34:49.175288: step 2243, loss 0.251876, acc 0.90625, prec 0.0540028, recall 0.800316
2017-12-10T03:34:49.439739: step 2244, loss 0.207794, acc 0.9375, prec 0.0540183, recall 0.800379
2017-12-10T03:34:49.699489: step 2245, loss 0.414484, acc 0.921875, prec 0.0540126, recall 0.800379
2017-12-10T03:34:49.967638: step 2246, loss 0.280644, acc 0.90625, prec 0.0540057, recall 0.800379
2017-12-10T03:34:50.232090: step 2247, loss 0.585516, acc 0.859375, prec 0.0540155, recall 0.800442
2017-12-10T03:34:50.494802: step 2248, loss 0.230923, acc 0.859375, prec 0.0540253, recall 0.800505
2017-12-10T03:34:50.757385: step 2249, loss 0.372966, acc 0.90625, prec 0.0540385, recall 0.800568
2017-12-10T03:34:51.029396: step 2250, loss 0.300865, acc 0.890625, prec 0.0540506, recall 0.800631
2017-12-10T03:34:51.290318: step 2251, loss 0.389667, acc 0.828125, prec 0.0540581, recall 0.800694
2017-12-10T03:34:51.555724: step 2252, loss 0.704513, acc 0.9375, prec 0.0540736, recall 0.800757
2017-12-10T03:34:51.825102: step 2253, loss 0.66841, acc 0.921875, prec 0.054088, recall 0.800819
2017-12-10T03:34:52.093212: step 2254, loss 0.326416, acc 0.90625, prec 0.0540811, recall 0.800819
2017-12-10T03:34:52.355734: step 2255, loss 0.0820552, acc 0.984375, prec 0.0541001, recall 0.800882
2017-12-10T03:34:52.622519: step 2256, loss 0.227017, acc 0.9375, prec 0.0540955, recall 0.800882
2017-12-10T03:34:52.894514: step 2257, loss 0.120793, acc 0.96875, prec 0.0540932, recall 0.800882
2017-12-10T03:34:53.164499: step 2258, loss 0.448208, acc 0.890625, prec 0.0541052, recall 0.800945
2017-12-10T03:34:53.426713: step 2259, loss 0.179635, acc 0.90625, prec 0.0541184, recall 0.801008
2017-12-10T03:34:53.697233: step 2260, loss 0.570829, acc 0.8125, prec 0.0541046, recall 0.801008
2017-12-10T03:34:53.965744: step 2261, loss 1.53965, acc 0.9375, prec 0.0541012, recall 0.800755
2017-12-10T03:34:54.232846: step 2262, loss 0.164575, acc 0.921875, prec 0.0540954, recall 0.800755
2017-12-10T03:34:54.493762: step 2263, loss 0.209194, acc 1, prec 0.0541759, recall 0.801006
2017-12-10T03:34:54.757328: step 2264, loss 0.164655, acc 0.921875, prec 0.0541701, recall 0.801006
2017-12-10T03:34:55.019917: step 2265, loss 1.03421, acc 0.9375, prec 0.0542258, recall 0.801193
2017-12-10T03:34:55.283376: step 2266, loss 0.645426, acc 0.828125, prec 0.0542534, recall 0.801318
2017-12-10T03:34:55.545673: step 2267, loss 0.372661, acc 0.921875, prec 0.0542677, recall 0.801381
2017-12-10T03:34:55.807420: step 2268, loss 2.25441, acc 0.890625, prec 0.0543009, recall 0.801254
2017-12-10T03:34:56.069079: step 2269, loss 0.489089, acc 0.828125, prec 0.0543083, recall 0.801316
2017-12-10T03:34:56.330578: step 2270, loss 0.39819, acc 0.90625, prec 0.0543215, recall 0.801378
2017-12-10T03:34:56.602926: step 2271, loss 0.48897, acc 0.890625, prec 0.0543335, recall 0.801441
2017-12-10T03:34:56.882643: step 2272, loss 0.279892, acc 0.890625, prec 0.0543455, recall 0.801503
2017-12-10T03:34:57.142854: step 2273, loss 0.915575, acc 0.75, prec 0.0543873, recall 0.801689
2017-12-10T03:34:57.405879: step 2274, loss 0.634971, acc 0.78125, prec 0.0543912, recall 0.801751
2017-12-10T03:34:57.665061: step 2275, loss 0.83648, acc 0.8125, prec 0.0543773, recall 0.801751
2017-12-10T03:34:57.929393: step 2276, loss 0.847457, acc 0.8125, prec 0.0544036, recall 0.801875
2017-12-10T03:34:58.190078: step 2277, loss 0.617822, acc 0.78125, prec 0.0544075, recall 0.801937
2017-12-10T03:34:58.458323: step 2278, loss 1.0366, acc 0.90625, prec 0.0544607, recall 0.802122
2017-12-10T03:34:58.728059: step 2279, loss 0.463954, acc 0.859375, prec 0.0544703, recall 0.802184
2017-12-10T03:34:58.988515: step 2280, loss 0.553497, acc 0.84375, prec 0.0545189, recall 0.802369
2017-12-10T03:34:59.254983: step 2281, loss 0.329702, acc 0.875, prec 0.0545096, recall 0.802369
2017-12-10T03:34:59.514279: step 2282, loss 0.658422, acc 0.8125, prec 0.0544958, recall 0.802369
2017-12-10T03:34:59.776995: step 2283, loss 0.597689, acc 0.84375, prec 0.0545643, recall 0.802615
2017-12-10T03:35:00.039952: step 2284, loss 0.406029, acc 0.921875, prec 0.0545785, recall 0.802677
2017-12-10T03:35:00.318722: step 2285, loss 0.78303, acc 0.828125, prec 0.0545658, recall 0.802677
2017-12-10T03:35:00.584328: step 2286, loss 0.588613, acc 0.890625, prec 0.0545578, recall 0.802677
2017-12-10T03:35:00.846251: step 2287, loss 0.256436, acc 0.921875, prec 0.054552, recall 0.802677
2017-12-10T03:35:01.118776: step 2288, loss 0.120849, acc 0.9375, prec 0.0545674, recall 0.802738
2017-12-10T03:35:01.382461: step 2289, loss 0.344938, acc 0.921875, prec 0.0546016, recall 0.802861
2017-12-10T03:35:01.645032: step 2290, loss 0.148593, acc 0.921875, prec 0.0545958, recall 0.802861
2017-12-10T03:35:01.908683: step 2291, loss 0.142476, acc 0.953125, prec 0.0545924, recall 0.802861
2017-12-10T03:35:02.177042: step 2292, loss 0.236539, acc 0.921875, prec 0.0545866, recall 0.802861
2017-12-10T03:35:02.440241: step 2293, loss 0.213318, acc 0.96875, prec 0.0546442, recall 0.803044
2017-12-10T03:35:02.709376: step 2294, loss 0.832053, acc 0.921875, prec 0.0546784, recall 0.803167
2017-12-10T03:35:02.978839: step 2295, loss 0.155745, acc 0.953125, prec 0.0547149, recall 0.803289
2017-12-10T03:35:03.247702: step 2296, loss 0.0378598, acc 1, prec 0.0547149, recall 0.803289
2017-12-10T03:35:03.518642: step 2297, loss 0.170994, acc 0.9375, prec 0.0547103, recall 0.803289
2017-12-10T03:35:03.784091: step 2298, loss 0.288676, acc 0.890625, prec 0.0547222, recall 0.80335
2017-12-10T03:35:04.049290: step 2299, loss 0.128715, acc 0.953125, prec 0.0547586, recall 0.803472
2017-12-10T03:35:04.316737: step 2300, loss 0.550122, acc 0.984375, prec 0.0547774, recall 0.803533
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-2300

2017-12-10T03:35:05.616215: step 2301, loss 0.0932277, acc 0.96875, prec 0.0547951, recall 0.803594
2017-12-10T03:35:05.881709: step 2302, loss 2.36194, acc 0.984375, prec 0.054835, recall 0.803466
2017-12-10T03:35:06.155536: step 2303, loss 0.505194, acc 0.921875, prec 0.0548492, recall 0.803527
2017-12-10T03:35:06.418382: step 2304, loss 0.26644, acc 0.921875, prec 0.0548434, recall 0.803527
2017-12-10T03:35:06.682341: step 2305, loss 0.169956, acc 0.9375, prec 0.0548388, recall 0.803527
2017-12-10T03:35:06.940337: step 2306, loss 0.207633, acc 0.90625, prec 0.0548518, recall 0.803588
2017-12-10T03:35:07.202320: step 2307, loss 0.372879, acc 0.921875, prec 0.054846, recall 0.803588
2017-12-10T03:35:07.470922: step 2308, loss 0.484186, acc 0.90625, prec 0.054839, recall 0.803588
2017-12-10T03:35:07.732860: step 2309, loss 0.128077, acc 0.9375, prec 0.0548344, recall 0.803588
2017-12-10T03:35:08.002514: step 2310, loss 0.204178, acc 0.921875, prec 0.0548685, recall 0.803709
2017-12-10T03:35:08.270056: step 2311, loss 0.501804, acc 0.875, prec 0.0548792, recall 0.80377
2017-12-10T03:35:08.532027: step 2312, loss 1.07755, acc 0.859375, prec 0.0548887, recall 0.803831
2017-12-10T03:35:08.801893: step 2313, loss 0.0721448, acc 0.96875, prec 0.0548864, recall 0.803831
2017-12-10T03:35:09.065565: step 2314, loss 0.626012, acc 0.875, prec 0.054917, recall 0.803952
2017-12-10T03:35:09.338049: step 2315, loss 0.672346, acc 0.875, prec 0.0549277, recall 0.804012
2017-12-10T03:35:09.600837: step 2316, loss 0.330422, acc 0.890625, prec 0.0549395, recall 0.804073
2017-12-10T03:35:09.871407: step 2317, loss 0.339059, acc 0.90625, prec 0.0549525, recall 0.804133
2017-12-10T03:35:10.136095: step 2318, loss 0.549268, acc 0.875, prec 0.0549631, recall 0.804194
2017-12-10T03:35:10.395609: step 2319, loss 0.404578, acc 0.875, prec 0.0549539, recall 0.804194
2017-12-10T03:35:10.660342: step 2320, loss 0.401564, acc 0.890625, prec 0.0549657, recall 0.804254
2017-12-10T03:35:10.931902: step 2321, loss 0.372517, acc 0.84375, prec 0.054974, recall 0.804314
2017-12-10T03:35:11.195212: step 2322, loss 0.363164, acc 0.890625, prec 0.0549858, recall 0.804375
2017-12-10T03:35:11.458259: step 2323, loss 0.148606, acc 0.9375, prec 0.0549812, recall 0.804375
2017-12-10T03:35:11.731720: step 2324, loss 0.389317, acc 0.90625, prec 0.0549941, recall 0.804435
2017-12-10T03:35:11.995108: step 2325, loss 0.504948, acc 0.84375, prec 0.0549825, recall 0.804435
2017-12-10T03:35:12.258035: step 2326, loss 0.328777, acc 0.890625, prec 0.0549744, recall 0.804435
2017-12-10T03:35:12.523804: step 2327, loss 0.294345, acc 0.90625, prec 0.0549874, recall 0.804495
2017-12-10T03:35:12.797029: step 2328, loss 0.369394, acc 0.890625, prec 0.0549793, recall 0.804495
2017-12-10T03:35:13.062793: step 2329, loss 0.231427, acc 0.96875, prec 0.0549968, recall 0.804555
2017-12-10T03:35:13.325341: step 2330, loss 0.489675, acc 0.859375, prec 0.0549864, recall 0.804555
2017-12-10T03:35:13.589790: step 2331, loss 0.0581717, acc 0.984375, prec 0.0550052, recall 0.804615
2017-12-10T03:35:13.855817: step 2332, loss 0.0281541, acc 0.984375, prec 0.055004, recall 0.804615
2017-12-10T03:35:14.116477: step 2333, loss 0.0419261, acc 0.984375, prec 0.0550028, recall 0.804615
2017-12-10T03:35:14.381351: step 2334, loss 0.0613243, acc 0.96875, prec 0.0550204, recall 0.804675
2017-12-10T03:35:14.656737: step 2335, loss 2.93189, acc 0.984375, prec 0.0550204, recall 0.804428
2017-12-10T03:35:14.921616: step 2336, loss 0.0557041, acc 0.984375, prec 0.0550192, recall 0.804428
2017-12-10T03:35:15.186205: step 2337, loss 0.17042, acc 1, prec 0.055059, recall 0.804548
2017-12-10T03:35:15.456606: step 2338, loss 0.14846, acc 0.96875, prec 0.0550765, recall 0.804608
2017-12-10T03:35:15.718284: step 2339, loss 0.265037, acc 0.921875, prec 0.0550708, recall 0.804608
2017-12-10T03:35:15.982712: step 2340, loss 0.0978384, acc 0.984375, prec 0.0550696, recall 0.804608
2017-12-10T03:35:16.250857: step 2341, loss 0.207429, acc 0.953125, prec 0.0550661, recall 0.804608
2017-12-10T03:35:16.511618: step 2342, loss 0.348381, acc 0.90625, prec 0.0550592, recall 0.804608
2017-12-10T03:35:16.773631: step 2343, loss 0.09338, acc 0.9375, prec 0.0550744, recall 0.804668
2017-12-10T03:35:17.040944: step 2344, loss 0.453258, acc 0.890625, prec 0.0550862, recall 0.804728
2017-12-10T03:35:17.306837: step 2345, loss 0.321159, acc 0.90625, prec 0.0551189, recall 0.804848
2017-12-10T03:35:17.570714: step 2346, loss 0.105168, acc 0.953125, prec 0.0551155, recall 0.804848
2017-12-10T03:35:17.835587: step 2347, loss 0.662539, acc 0.984375, prec 0.0551342, recall 0.804908
2017-12-10T03:35:18.109287: step 2348, loss 0.121496, acc 0.96875, prec 0.0551318, recall 0.804908
2017-12-10T03:35:18.373307: step 2349, loss 0.333565, acc 0.921875, prec 0.0551657, recall 0.805028
2017-12-10T03:35:18.634554: step 2350, loss 0.650217, acc 0.9375, prec 0.0552008, recall 0.805147
2017-12-10T03:35:18.903763: step 2351, loss 0.103242, acc 0.96875, prec 0.0551985, recall 0.805147
2017-12-10T03:35:19.165428: step 2352, loss 0.117252, acc 0.953125, prec 0.0552347, recall 0.805266
2017-12-10T03:35:19.429996: step 2353, loss 0.30913, acc 0.921875, prec 0.0552487, recall 0.805326
2017-12-10T03:35:19.693452: step 2354, loss 0.0668458, acc 0.953125, prec 0.0552453, recall 0.805326
2017-12-10T03:35:19.963785: step 2355, loss 0.146137, acc 0.96875, prec 0.0552628, recall 0.805386
2017-12-10T03:35:20.230399: step 2356, loss 0.242121, acc 0.921875, prec 0.055257, recall 0.805386
2017-12-10T03:35:20.493793: step 2357, loss 0.186803, acc 0.90625, prec 0.0552897, recall 0.805505
2017-12-10T03:35:20.760608: step 2358, loss 0.19125, acc 0.9375, prec 0.055285, recall 0.805505
2017-12-10T03:35:21.028620: step 2359, loss 0.141235, acc 0.96875, prec 0.0552827, recall 0.805505
2017-12-10T03:35:21.300376: step 2360, loss 0.263742, acc 0.90625, prec 0.0552956, recall 0.805564
2017-12-10T03:35:21.571071: step 2361, loss 0.114647, acc 0.984375, prec 0.0552944, recall 0.805564
2017-12-10T03:35:21.840992: step 2362, loss 0.133371, acc 0.953125, prec 0.0553108, recall 0.805623
2017-12-10T03:35:22.115807: step 2363, loss 0.317442, acc 0.953125, prec 0.0553271, recall 0.805683
2017-12-10T03:35:22.382065: step 2364, loss 0.566928, acc 0.984375, prec 0.0553854, recall 0.805861
2017-12-10T03:35:22.652911: step 2365, loss 0.256393, acc 0.9375, prec 0.0554402, recall 0.806038
2017-12-10T03:35:22.922568: step 2366, loss 0.115221, acc 0.9375, prec 0.0554355, recall 0.806038
2017-12-10T03:35:23.196316: step 2367, loss 0.285656, acc 0.9375, prec 0.0554507, recall 0.806098
2017-12-10T03:35:23.462404: step 2368, loss 0.230215, acc 0.953125, prec 0.0555462, recall 0.806393
2017-12-10T03:35:23.729086: step 2369, loss 0.548, acc 0.9375, prec 0.0555812, recall 0.806511
2017-12-10T03:35:23.999143: step 2370, loss 0.291212, acc 0.953125, prec 0.0555777, recall 0.806511
2017-12-10T03:35:24.263071: step 2371, loss 0.0350773, acc 0.984375, prec 0.0555765, recall 0.806511
2017-12-10T03:35:24.525587: step 2372, loss 0.78209, acc 0.953125, prec 0.0556126, recall 0.806628
2017-12-10T03:35:24.793336: step 2373, loss 0.610222, acc 0.921875, prec 0.0556464, recall 0.806746
2017-12-10T03:35:25.065907: step 2374, loss 0.213905, acc 0.9375, prec 0.0556615, recall 0.806804
2017-12-10T03:35:25.328983: step 2375, loss 0.112611, acc 0.96875, prec 0.0556592, recall 0.806804
2017-12-10T03:35:25.590875: step 2376, loss 0.262269, acc 0.921875, prec 0.0556731, recall 0.806863
2017-12-10T03:35:25.852560: step 2377, loss 0.0824789, acc 0.96875, prec 0.0556906, recall 0.806922
2017-12-10T03:35:26.115364: step 2378, loss 0.243388, acc 0.875, prec 0.055701, recall 0.80698
2017-12-10T03:35:26.378959: step 2379, loss 0.28924, acc 0.90625, prec 0.055694, recall 0.80698
2017-12-10T03:35:26.642929: step 2380, loss 0.352424, acc 0.921875, prec 0.0556882, recall 0.80698
2017-12-10T03:35:26.913145: step 2381, loss 0.275248, acc 0.953125, prec 0.0556847, recall 0.80698
2017-12-10T03:35:27.170371: step 2382, loss 0.0263231, acc 1, prec 0.0556847, recall 0.80698
2017-12-10T03:35:27.439616: step 2383, loss 0.300875, acc 0.890625, prec 0.0556765, recall 0.80698
2017-12-10T03:35:27.707656: step 2384, loss 0.307555, acc 0.890625, prec 0.0556881, recall 0.807039
2017-12-10T03:35:27.976376: step 2385, loss 0.434953, acc 0.90625, prec 0.0557404, recall 0.807214
2017-12-10T03:35:28.240148: step 2386, loss 0.262926, acc 0.90625, prec 0.0557532, recall 0.807273
2017-12-10T03:35:28.513427: step 2387, loss 1.93629, acc 0.953125, prec 0.0557904, recall 0.807145
2017-12-10T03:35:28.780204: step 2388, loss 0.197021, acc 0.921875, prec 0.0557846, recall 0.807145
2017-12-10T03:35:29.041345: step 2389, loss 0.21191, acc 0.90625, prec 0.0557776, recall 0.807145
2017-12-10T03:35:29.308720: step 2390, loss 0.250211, acc 0.953125, prec 0.0557741, recall 0.807145
2017-12-10T03:35:29.572061: step 2391, loss 0.442818, acc 0.90625, prec 0.0557671, recall 0.807145
2017-12-10T03:35:29.831752: step 2392, loss 0.456632, acc 0.890625, prec 0.0557984, recall 0.807262
2017-12-10T03:35:30.097818: step 2393, loss 0.254298, acc 0.921875, prec 0.0558123, recall 0.80732
2017-12-10T03:35:30.369267: step 2394, loss 0.103209, acc 0.953125, prec 0.0558088, recall 0.80732
2017-12-10T03:35:30.639041: step 2395, loss 0.0800478, acc 0.96875, prec 0.0558065, recall 0.80732
2017-12-10T03:35:30.914533: step 2396, loss 0.298277, acc 0.90625, prec 0.0558192, recall 0.807378
2017-12-10T03:35:31.177211: step 2397, loss 0.337988, acc 0.90625, prec 0.0558122, recall 0.807378
2017-12-10T03:35:31.448541: step 2398, loss 0.631062, acc 0.78125, prec 0.0558156, recall 0.807437
2017-12-10T03:35:31.711336: step 2399, loss 0.317768, acc 0.90625, prec 0.0558283, recall 0.807495
2017-12-10T03:35:31.971912: step 2400, loss 0.0601282, acc 0.984375, prec 0.0558469, recall 0.807553

Evaluation:
2017-12-10T03:35:39.539948: step 2400, loss 1.96549, acc 0.923672, prec 0.0567704, recall 0.798902

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-2400

2017-12-10T03:35:40.819848: step 2401, loss 0.334701, acc 0.890625, prec 0.0567816, recall 0.79896
2017-12-10T03:35:41.089211: step 2402, loss 0.467943, acc 0.890625, prec 0.0568121, recall 0.799076
2017-12-10T03:35:41.364122: step 2403, loss 0.295229, acc 0.90625, prec 0.0568051, recall 0.799076
2017-12-10T03:35:41.634689: step 2404, loss 0.177292, acc 0.953125, prec 0.0568403, recall 0.799192
2017-12-10T03:35:41.909287: step 2405, loss 0.118119, acc 0.9375, prec 0.0568357, recall 0.799192
2017-12-10T03:35:42.173453: step 2406, loss 0.179378, acc 1, prec 0.0568744, recall 0.799308
2017-12-10T03:35:42.440330: step 2407, loss 1.69037, acc 0.9375, prec 0.0568902, recall 0.799135
2017-12-10T03:35:42.705819: step 2408, loss 0.202112, acc 0.921875, prec 0.0568844, recall 0.799135
2017-12-10T03:35:42.974922: step 2409, loss 0.743049, acc 0.9375, prec 0.0569378, recall 0.799309
2017-12-10T03:35:43.243213: step 2410, loss 0.100527, acc 0.96875, prec 0.0569354, recall 0.799309
2017-12-10T03:35:43.506328: step 2411, loss 0.472518, acc 0.875, prec 0.0569261, recall 0.799309
2017-12-10T03:35:43.773041: step 2412, loss 0.676933, acc 0.9375, prec 0.0569601, recall 0.799424
2017-12-10T03:35:44.040024: step 2413, loss 0.115405, acc 0.953125, prec 0.0569566, recall 0.799424
2017-12-10T03:35:44.313951: step 2414, loss 0.478679, acc 0.828125, prec 0.0569824, recall 0.79954
2017-12-10T03:35:44.585001: step 2415, loss 0.321016, acc 0.875, prec 0.0569731, recall 0.79954
2017-12-10T03:35:44.843939: step 2416, loss 0.321373, acc 0.875, prec 0.0569637, recall 0.79954
2017-12-10T03:35:45.112245: step 2417, loss 0.351319, acc 0.875, prec 0.0569737, recall 0.799597
2017-12-10T03:35:45.369654: step 2418, loss 0.892888, acc 0.8125, prec 0.056979, recall 0.799655
2017-12-10T03:35:45.633371: step 2419, loss 0.914977, acc 0.828125, prec 0.0569855, recall 0.799713
2017-12-10T03:35:45.895803: step 2420, loss 0.322579, acc 0.875, prec 0.0569762, recall 0.799713
2017-12-10T03:35:46.165019: step 2421, loss 0.329658, acc 0.921875, prec 0.0569703, recall 0.799713
2017-12-10T03:35:46.426555: step 2422, loss 0.463433, acc 0.828125, prec 0.0569768, recall 0.79977
2017-12-10T03:35:46.688504: step 2423, loss 0.376338, acc 0.875, prec 0.0569868, recall 0.799828
2017-12-10T03:35:46.950428: step 2424, loss 0.159873, acc 0.9375, prec 0.0569821, recall 0.799828
2017-12-10T03:35:47.208304: step 2425, loss 1.39413, acc 0.84375, prec 0.0569716, recall 0.799598
2017-12-10T03:35:47.472216: step 2426, loss 0.617622, acc 0.84375, prec 0.0569985, recall 0.799713
2017-12-10T03:35:47.736845: step 2427, loss 0.333295, acc 0.890625, prec 0.0570289, recall 0.799828
2017-12-10T03:35:48.011438: step 2428, loss 0.467202, acc 0.875, prec 0.0570582, recall 0.799943
2017-12-10T03:35:48.283609: step 2429, loss 1.95706, acc 0.90625, prec 0.0570909, recall 0.799828
2017-12-10T03:35:48.553921: step 2430, loss 3.69144, acc 0.875, prec 0.0571213, recall 0.799714
2017-12-10T03:35:48.818569: step 2431, loss 1.62169, acc 0.890625, prec 0.0571142, recall 0.799485
2017-12-10T03:35:49.092062: step 2432, loss 0.95765, acc 0.8125, prec 0.0571195, recall 0.799542
2017-12-10T03:35:49.360895: step 2433, loss 0.427378, acc 0.890625, prec 0.0571113, recall 0.799542
2017-12-10T03:35:49.625144: step 2434, loss 0.846356, acc 0.828125, prec 0.0571178, recall 0.7996
2017-12-10T03:35:49.892776: step 2435, loss 1.24668, acc 0.671875, prec 0.0570933, recall 0.7996
2017-12-10T03:35:50.154786: step 2436, loss 0.495713, acc 0.796875, prec 0.0570974, recall 0.799657
2017-12-10T03:35:50.423902: step 2437, loss 1.27988, acc 0.671875, prec 0.0570729, recall 0.799657
2017-12-10T03:35:50.687502: step 2438, loss 1.41887, acc 0.703125, prec 0.0570508, recall 0.799657
2017-12-10T03:35:50.950838: step 2439, loss 1.56264, acc 0.59375, prec 0.0570206, recall 0.799657
2017-12-10T03:35:51.210550: step 2440, loss 1.14343, acc 0.703125, prec 0.0570369, recall 0.799771
2017-12-10T03:35:51.478653: step 2441, loss 1.38861, acc 0.703125, prec 0.0570149, recall 0.799771
2017-12-10T03:35:51.736549: step 2442, loss 0.856333, acc 0.703125, prec 0.057012, recall 0.799829
2017-12-10T03:35:51.996043: step 2443, loss 0.850641, acc 0.75, prec 0.0570126, recall 0.799886
2017-12-10T03:35:52.261309: step 2444, loss 0.344693, acc 0.921875, prec 0.057026, recall 0.799943
2017-12-10T03:35:52.525685: step 2445, loss 0.685646, acc 0.84375, prec 0.0570336, recall 0.8
2017-12-10T03:35:52.791545: step 2446, loss 0.327874, acc 0.9375, prec 0.0570481, recall 0.800057
2017-12-10T03:35:53.062387: step 2447, loss 0.490988, acc 0.859375, prec 0.0570569, recall 0.800114
2017-12-10T03:35:53.333020: step 2448, loss 0.206113, acc 0.9375, prec 0.0570714, recall 0.800171
2017-12-10T03:35:53.598312: step 2449, loss 1.1236, acc 0.890625, prec 0.0570825, recall 0.800228
2017-12-10T03:35:53.869450: step 2450, loss 0.301894, acc 0.9375, prec 0.057097, recall 0.800285
2017-12-10T03:35:54.137794: step 2451, loss 0.112249, acc 0.9375, prec 0.0570923, recall 0.800285
2017-12-10T03:35:54.408399: step 2452, loss 0.138369, acc 0.9375, prec 0.0570877, recall 0.800285
2017-12-10T03:35:54.673018: step 2453, loss 0.114682, acc 0.984375, prec 0.0570865, recall 0.800285
2017-12-10T03:35:54.932251: step 2454, loss 0.347391, acc 0.921875, prec 0.0570999, recall 0.800342
2017-12-10T03:35:55.195838: step 2455, loss 0.0992923, acc 0.9375, prec 0.0570953, recall 0.800342
2017-12-10T03:35:55.464534: step 2456, loss 0.370987, acc 0.921875, prec 0.0570895, recall 0.800342
2017-12-10T03:35:55.731655: step 2457, loss 0.209066, acc 0.96875, prec 0.0570871, recall 0.800342
2017-12-10T03:35:55.996807: step 2458, loss 0.15203, acc 0.9375, prec 0.0571016, recall 0.800399
2017-12-10T03:35:56.261015: step 2459, loss 0.354223, acc 0.921875, prec 0.0571342, recall 0.800512
2017-12-10T03:35:56.530016: step 2460, loss 0.0541021, acc 0.96875, prec 0.057151, recall 0.800569
2017-12-10T03:35:56.808720: step 2461, loss 0.0652109, acc 0.96875, prec 0.0571487, recall 0.800569
2017-12-10T03:35:57.073562: step 2462, loss 0.245929, acc 0.9375, prec 0.057144, recall 0.800569
2017-12-10T03:35:57.337960: step 2463, loss 0.13738, acc 0.96875, prec 0.0571417, recall 0.800569
2017-12-10T03:35:57.601767: step 2464, loss 0.10754, acc 1, prec 0.05718, recall 0.800682
2017-12-10T03:35:57.867913: step 2465, loss 0.162202, acc 0.96875, prec 0.0571777, recall 0.800682
2017-12-10T03:35:58.129424: step 2466, loss 0.979809, acc 1, prec 0.057216, recall 0.800796
2017-12-10T03:35:58.404097: step 2467, loss 0.0803286, acc 0.96875, prec 0.0572136, recall 0.800796
2017-12-10T03:35:58.666867: step 2468, loss 1.32351, acc 0.96875, prec 0.0572316, recall 0.800625
2017-12-10T03:35:58.935552: step 2469, loss 0.0736615, acc 0.96875, prec 0.0572293, recall 0.800625
2017-12-10T03:35:59.200581: step 2470, loss 0.163875, acc 0.953125, prec 0.0572641, recall 0.800738
2017-12-10T03:35:59.468278: step 2471, loss 0.0921977, acc 0.96875, prec 0.0572809, recall 0.800795
2017-12-10T03:35:59.734816: step 2472, loss 3.22245, acc 0.96875, prec 0.057318, recall 0.80068
2017-12-10T03:36:00.008773: step 2473, loss 0.191657, acc 0.921875, prec 0.0573696, recall 0.80085
2017-12-10T03:36:00.283129: step 2474, loss 0.493371, acc 0.921875, prec 0.0573637, recall 0.80085
2017-12-10T03:36:00.553339: step 2475, loss 0.798507, acc 0.953125, prec 0.0573794, recall 0.800906
2017-12-10T03:36:00.820709: step 2476, loss 0.672321, acc 0.859375, prec 0.057388, recall 0.800963
2017-12-10T03:36:01.085394: step 2477, loss 0.442247, acc 0.859375, prec 0.0573967, recall 0.801019
2017-12-10T03:36:01.343561: step 2478, loss 0.59086, acc 0.828125, prec 0.0574412, recall 0.801188
2017-12-10T03:36:01.608036: step 2479, loss 0.336179, acc 0.84375, prec 0.0574296, recall 0.801188
2017-12-10T03:36:01.879820: step 2480, loss 0.543065, acc 0.8125, prec 0.0574538, recall 0.8013
2017-12-10T03:36:02.141025: step 2481, loss 0.196405, acc 0.90625, prec 0.057485, recall 0.801412
2017-12-10T03:36:02.400990: step 2482, loss 0.56073, acc 0.734375, prec 0.0574652, recall 0.801412
2017-12-10T03:36:02.665066: step 2483, loss 0.977684, acc 0.703125, prec 0.0575004, recall 0.801581
2017-12-10T03:36:02.932473: step 2484, loss 0.728373, acc 0.8125, prec 0.0574864, recall 0.801581
2017-12-10T03:36:03.163931: step 2485, loss 0.472206, acc 0.823529, prec 0.0574759, recall 0.801581
2017-12-10T03:36:03.434617: step 2486, loss 0.303574, acc 0.890625, prec 0.0574868, recall 0.801637
2017-12-10T03:36:03.699187: step 2487, loss 0.372726, acc 0.84375, prec 0.0574943, recall 0.801693
2017-12-10T03:36:03.963502: step 2488, loss 0.333871, acc 0.90625, prec 0.0575064, recall 0.801748
2017-12-10T03:36:04.232645: step 2489, loss 0.34742, acc 0.921875, prec 0.0575577, recall 0.801916
2017-12-10T03:36:04.497208: step 2490, loss 0.150063, acc 0.921875, prec 0.0575519, recall 0.801916
2017-12-10T03:36:04.761508: step 2491, loss 0.703957, acc 0.84375, prec 0.0575784, recall 0.802028
2017-12-10T03:36:05.027221: step 2492, loss 0.347599, acc 0.9375, prec 0.0575737, recall 0.802028
2017-12-10T03:36:05.289599: step 2493, loss 0.310474, acc 0.90625, prec 0.0575858, recall 0.802083
2017-12-10T03:36:05.555016: step 2494, loss 0.270974, acc 0.984375, prec 0.0576227, recall 0.802195
2017-12-10T03:36:05.821585: step 2495, loss 0.172665, acc 0.921875, prec 0.0576169, recall 0.802195
2017-12-10T03:36:06.085552: step 2496, loss 0.221549, acc 0.9375, prec 0.0576694, recall 0.802362
2017-12-10T03:36:06.351384: step 2497, loss 0.319465, acc 0.96875, prec 0.0576861, recall 0.802417
2017-12-10T03:36:06.616141: step 2498, loss 0.128883, acc 0.984375, prec 0.057704, recall 0.802473
2017-12-10T03:36:06.877543: step 2499, loss 0.0266341, acc 0.984375, prec 0.0577218, recall 0.802528
2017-12-10T03:36:07.140237: step 2500, loss 0.921908, acc 0.953125, prec 0.0578135, recall 0.802805
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-2500

2017-12-10T03:36:08.486902: step 2501, loss 0.127654, acc 0.984375, prec 0.0578314, recall 0.80286
2017-12-10T03:36:08.763036: step 2502, loss 0.0275865, acc 0.96875, prec 0.057829, recall 0.80286
2017-12-10T03:36:09.023203: step 2503, loss 0.538824, acc 0.921875, prec 0.0578422, recall 0.802916
2017-12-10T03:36:09.284569: step 2504, loss 0.115124, acc 0.953125, prec 0.0578577, recall 0.802971
2017-12-10T03:36:09.546954: step 2505, loss 0.143481, acc 0.96875, prec 0.0579315, recall 0.803191
2017-12-10T03:36:09.811487: step 2506, loss 0.036198, acc 0.984375, prec 0.0579303, recall 0.803191
2017-12-10T03:36:10.072498: step 2507, loss 0.0483155, acc 0.984375, prec 0.0579292, recall 0.803191
2017-12-10T03:36:10.345515: step 2508, loss 0.125255, acc 0.953125, prec 0.0579637, recall 0.803302
2017-12-10T03:36:10.617081: step 2509, loss 0.119568, acc 0.96875, prec 0.0579614, recall 0.803302
2017-12-10T03:36:10.879075: step 2510, loss 0.179214, acc 0.953125, prec 0.0579769, recall 0.803357
2017-12-10T03:36:11.148637: step 2511, loss 0.307223, acc 0.9375, prec 0.0580102, recall 0.803467
2017-12-10T03:36:11.410596: step 2512, loss 0.364888, acc 0.890625, prec 0.058021, recall 0.803522
2017-12-10T03:36:11.689944: step 2513, loss 0.136293, acc 0.953125, prec 0.0580365, recall 0.803576
2017-12-10T03:36:11.956282: step 2514, loss 2.59468, acc 0.953125, prec 0.0580912, recall 0.803517
2017-12-10T03:36:12.232125: step 2515, loss 0.28651, acc 0.921875, prec 0.0580853, recall 0.803517
2017-12-10T03:36:12.499273: step 2516, loss 0.199459, acc 0.96875, prec 0.05814, recall 0.803681
2017-12-10T03:36:12.765610: step 2517, loss 0.162275, acc 0.921875, prec 0.0581341, recall 0.803681
2017-12-10T03:36:13.025148: step 2518, loss 0.1019, acc 0.96875, prec 0.0581698, recall 0.80379
2017-12-10T03:36:13.291226: step 2519, loss 0.308732, acc 0.9375, prec 0.0581841, recall 0.803845
2017-12-10T03:36:13.560229: step 2520, loss 0.189903, acc 0.921875, prec 0.0581972, recall 0.8039
2017-12-10T03:36:13.829777: step 2521, loss 0.255738, acc 0.890625, prec 0.058208, recall 0.803954
2017-12-10T03:36:14.098758: step 2522, loss 0.439322, acc 0.875, prec 0.0581986, recall 0.803954
2017-12-10T03:36:14.361479: step 2523, loss 0.36947, acc 0.890625, prec 0.0582094, recall 0.804009
2017-12-10T03:36:14.627028: step 2524, loss 0.694896, acc 0.828125, prec 0.0582155, recall 0.804063
2017-12-10T03:36:14.894483: step 2525, loss 0.54402, acc 0.828125, prec 0.0582025, recall 0.804063
2017-12-10T03:36:15.158634: step 2526, loss 0.331158, acc 0.890625, prec 0.0581943, recall 0.804063
2017-12-10T03:36:15.420634: step 2527, loss 0.916318, acc 0.796875, prec 0.058217, recall 0.804172
2017-12-10T03:36:15.687106: step 2528, loss 0.175176, acc 0.953125, prec 0.0582514, recall 0.804281
2017-12-10T03:36:15.951698: step 2529, loss 0.579544, acc 0.84375, prec 0.0582587, recall 0.804336
2017-12-10T03:36:16.217493: step 2530, loss 0.411138, acc 0.890625, prec 0.0582884, recall 0.804444
2017-12-10T03:36:16.484026: step 2531, loss 0.106836, acc 0.9375, prec 0.0583026, recall 0.804499
2017-12-10T03:36:16.743496: step 2532, loss 0.118177, acc 0.9375, prec 0.058298, recall 0.804499
2017-12-10T03:36:17.009094: step 2533, loss 0.506098, acc 0.90625, prec 0.0582909, recall 0.804499
2017-12-10T03:36:17.278799: step 2534, loss 0.0902093, acc 0.96875, prec 0.0582886, recall 0.804499
2017-12-10T03:36:17.537354: step 2535, loss 0.347233, acc 0.9375, prec 0.0582839, recall 0.804499
2017-12-10T03:36:17.805467: step 2536, loss 0.0784416, acc 0.96875, prec 0.0583005, recall 0.804553
2017-12-10T03:36:18.080368: step 2537, loss 0.251037, acc 0.921875, prec 0.0582946, recall 0.804553
2017-12-10T03:36:18.352479: step 2538, loss 6.97689, acc 0.90625, prec 0.0583077, recall 0.804384
2017-12-10T03:36:18.617726: step 2539, loss 0.0513866, acc 0.96875, prec 0.0583053, recall 0.804384
2017-12-10T03:36:18.882751: step 2540, loss 0.0352445, acc 0.984375, prec 0.058342, recall 0.804493
2017-12-10T03:36:19.149350: step 2541, loss 0.119981, acc 0.96875, prec 0.0583586, recall 0.804547
2017-12-10T03:36:19.411133: step 2542, loss 0.328524, acc 0.90625, prec 0.0583516, recall 0.804547
2017-12-10T03:36:19.687730: step 2543, loss 0.277869, acc 0.9375, prec 0.0583658, recall 0.804601
2017-12-10T03:36:19.956315: step 2544, loss 0.262999, acc 0.953125, prec 0.0583812, recall 0.804655
2017-12-10T03:36:20.216501: step 2545, loss 0.1779, acc 0.9375, prec 0.0583955, recall 0.804709
2017-12-10T03:36:20.482020: step 2546, loss 0.147406, acc 0.9375, prec 0.0583908, recall 0.804709
2017-12-10T03:36:20.755047: step 2547, loss 0.124762, acc 0.953125, prec 0.0583873, recall 0.804709
2017-12-10T03:36:21.024791: step 2548, loss 0.226001, acc 0.921875, prec 0.0583814, recall 0.804709
2017-12-10T03:36:21.286636: step 2549, loss 0.379063, acc 0.890625, prec 0.0583921, recall 0.804763
2017-12-10T03:36:21.550904: step 2550, loss 0.146618, acc 0.953125, prec 0.0583886, recall 0.804763
2017-12-10T03:36:21.819842: step 2551, loss 0.317747, acc 0.9375, prec 0.0584028, recall 0.804817
2017-12-10T03:36:22.092422: step 2552, loss 0.356237, acc 0.921875, prec 0.0584348, recall 0.804925
2017-12-10T03:36:22.354861: step 2553, loss 0.310942, acc 0.875, prec 0.0584254, recall 0.804925
2017-12-10T03:36:22.620138: step 2554, loss 0.337515, acc 0.984375, prec 0.058462, recall 0.805033
2017-12-10T03:36:22.894293: step 2555, loss 0.124297, acc 0.96875, prec 0.0584975, recall 0.805141
2017-12-10T03:36:23.166138: step 2556, loss 0.744243, acc 0.953125, prec 0.0585129, recall 0.805195
2017-12-10T03:36:23.442097: step 2557, loss 0.021721, acc 1, prec 0.0585507, recall 0.805302
2017-12-10T03:36:23.704786: step 2558, loss 0.221052, acc 0.921875, prec 0.0586015, recall 0.805464
2017-12-10T03:36:23.981016: step 2559, loss 0.20822, acc 0.953125, prec 0.058598, recall 0.805464
2017-12-10T03:36:24.244740: step 2560, loss 0.132916, acc 0.96875, prec 0.0586145, recall 0.805517
2017-12-10T03:36:24.510578: step 2561, loss 0.23994, acc 0.953125, prec 0.0586299, recall 0.805571
2017-12-10T03:36:24.776812: step 2562, loss 0.0880084, acc 0.96875, prec 0.0586464, recall 0.805624
2017-12-10T03:36:25.046361: step 2563, loss 0.354502, acc 0.90625, prec 0.0586583, recall 0.805678
2017-12-10T03:36:25.309764: step 2564, loss 0.139079, acc 0.953125, prec 0.0586547, recall 0.805678
2017-12-10T03:36:25.568676: step 2565, loss 0.237537, acc 0.96875, prec 0.0586524, recall 0.805678
2017-12-10T03:36:25.829762: step 2566, loss 0.266917, acc 0.921875, prec 0.0586843, recall 0.805785
2017-12-10T03:36:26.095774: step 2567, loss 0.139445, acc 0.9375, prec 0.0586796, recall 0.805785
2017-12-10T03:36:26.359993: step 2568, loss 0.0262729, acc 1, prec 0.0586796, recall 0.805785
2017-12-10T03:36:26.630557: step 2569, loss 5.2874, acc 0.96875, prec 0.058735, recall 0.805724
2017-12-10T03:36:26.898144: step 2570, loss 0.206428, acc 0.984375, prec 0.0587716, recall 0.805831
2017-12-10T03:36:27.163760: step 2571, loss 0.299241, acc 0.96875, prec 0.058807, recall 0.805937
2017-12-10T03:36:27.427388: step 2572, loss 0.408492, acc 0.875, prec 0.0588165, recall 0.805991
2017-12-10T03:36:27.687318: step 2573, loss 0.417747, acc 0.875, prec 0.0588259, recall 0.806044
2017-12-10T03:36:27.949205: step 2574, loss 0.176204, acc 0.921875, prec 0.05882, recall 0.806044
2017-12-10T03:36:28.206460: step 2575, loss 0.624595, acc 0.8125, prec 0.0588058, recall 0.806044
2017-12-10T03:36:28.473230: step 2576, loss 0.280111, acc 0.859375, prec 0.058833, recall 0.80615
2017-12-10T03:36:28.738170: step 2577, loss 0.511033, acc 0.859375, prec 0.0588412, recall 0.806204
2017-12-10T03:36:28.996382: step 2578, loss 0.692237, acc 0.765625, prec 0.0588235, recall 0.806204
2017-12-10T03:36:29.264794: step 2579, loss 0.454597, acc 0.8125, prec 0.0588282, recall 0.806257
2017-12-10T03:36:29.531166: step 2580, loss 0.283117, acc 0.875, prec 0.0588188, recall 0.806257
2017-12-10T03:36:29.794260: step 2581, loss 0.507976, acc 0.8125, prec 0.0588235, recall 0.80631
2017-12-10T03:36:30.067660: step 2582, loss 0.507747, acc 0.8125, prec 0.0588094, recall 0.80631
2017-12-10T03:36:30.334867: step 2583, loss 0.604952, acc 0.796875, prec 0.0588318, recall 0.806416
2017-12-10T03:36:30.604234: step 2584, loss 0.509517, acc 0.796875, prec 0.0588353, recall 0.806469
2017-12-10T03:36:30.872128: step 2585, loss 0.251049, acc 0.890625, prec 0.0588647, recall 0.806575
2017-12-10T03:36:31.135376: step 2586, loss 0.292586, acc 0.890625, prec 0.0588753, recall 0.806628
2017-12-10T03:36:31.402255: step 2587, loss 0.223533, acc 0.90625, prec 0.0588682, recall 0.806628
2017-12-10T03:36:31.663026: step 2588, loss 0.353238, acc 0.921875, prec 0.0588623, recall 0.806628
2017-12-10T03:36:31.926433: step 2589, loss 0.256509, acc 0.90625, prec 0.0588553, recall 0.806628
2017-12-10T03:36:32.188599: step 2590, loss 0.234002, acc 0.9375, prec 0.0588694, recall 0.806681
2017-12-10T03:36:32.452552: step 2591, loss 0.0877453, acc 0.96875, prec 0.0588858, recall 0.806734
2017-12-10T03:36:32.725502: step 2592, loss 0.0652891, acc 0.984375, prec 0.0588846, recall 0.806734
2017-12-10T03:36:32.991381: step 2593, loss 0.12042, acc 0.96875, prec 0.0589011, recall 0.806787
2017-12-10T03:36:33.262678: step 2594, loss 0.0680509, acc 0.984375, prec 0.0589187, recall 0.80684
2017-12-10T03:36:33.539153: step 2595, loss 2.20902, acc 0.921875, prec 0.058914, recall 0.806619
2017-12-10T03:36:33.814478: step 2596, loss 0.256003, acc 0.96875, prec 0.0589305, recall 0.806672
2017-12-10T03:36:34.081296: step 2597, loss 0.598234, acc 0.984375, prec 0.0589481, recall 0.806725
2017-12-10T03:36:34.348481: step 2598, loss 0.0409315, acc 0.984375, prec 0.0589469, recall 0.806725
2017-12-10T03:36:34.619712: step 2599, loss 0.128435, acc 0.953125, prec 0.0589434, recall 0.806725
2017-12-10T03:36:34.887170: step 2600, loss 0.134451, acc 0.9375, prec 0.0589575, recall 0.806778
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-2600

2017-12-10T03:36:36.185917: step 2601, loss 0.0585621, acc 0.96875, prec 0.0589927, recall 0.806883
2017-12-10T03:36:36.446776: step 2602, loss 0.122644, acc 0.953125, prec 0.0589892, recall 0.806883
2017-12-10T03:36:36.707058: step 2603, loss 0.321018, acc 0.875, prec 0.0589985, recall 0.806936
2017-12-10T03:36:36.972664: step 2604, loss 0.3388, acc 0.90625, prec 0.0589915, recall 0.806936
2017-12-10T03:36:37.236646: step 2605, loss 0.0457843, acc 0.984375, prec 0.0590278, recall 0.807041
2017-12-10T03:36:37.499891: step 2606, loss 0.452191, acc 0.90625, prec 0.0590583, recall 0.807147
2017-12-10T03:36:37.767787: step 2607, loss 0.110033, acc 0.9375, prec 0.0590724, recall 0.807199
2017-12-10T03:36:38.034606: step 2608, loss 0.0540722, acc 0.96875, prec 0.05907, recall 0.807199
2017-12-10T03:36:38.300552: step 2609, loss 0.213575, acc 0.9375, prec 0.0591217, recall 0.807357
2017-12-10T03:36:38.568760: step 2610, loss 0.210192, acc 0.9375, prec 0.0591169, recall 0.807357
2017-12-10T03:36:38.841489: step 2611, loss 0.135588, acc 0.96875, prec 0.0591146, recall 0.807357
2017-12-10T03:36:39.114410: step 2612, loss 0.197512, acc 0.9375, prec 0.0591286, recall 0.807409
2017-12-10T03:36:39.392775: step 2613, loss 0.118781, acc 0.984375, prec 0.0591274, recall 0.807409
2017-12-10T03:36:39.659356: step 2614, loss 0.0963442, acc 0.96875, prec 0.0591251, recall 0.807409
2017-12-10T03:36:39.930829: step 2615, loss 0.0941924, acc 0.953125, prec 0.0591216, recall 0.807409
2017-12-10T03:36:40.195298: step 2616, loss 0.492593, acc 0.96875, prec 0.059138, recall 0.807462
2017-12-10T03:36:40.464034: step 2617, loss 0.106131, acc 0.953125, prec 0.0591532, recall 0.807514
2017-12-10T03:36:40.730890: step 2618, loss 0.250309, acc 0.953125, prec 0.0591684, recall 0.807567
2017-12-10T03:36:40.995988: step 2619, loss 1.06897, acc 0.96875, prec 0.0591848, recall 0.807619
2017-12-10T03:36:41.264702: step 2620, loss 0.0655268, acc 0.953125, prec 0.0591813, recall 0.807619
2017-12-10T03:36:41.537312: step 2621, loss 0.203736, acc 0.96875, prec 0.0591977, recall 0.807671
2017-12-10T03:36:41.813129: step 2622, loss 0.118024, acc 0.96875, prec 0.0592141, recall 0.807724
2017-12-10T03:36:42.078019: step 2623, loss 0.0675579, acc 0.96875, prec 0.0592117, recall 0.807724
2017-12-10T03:36:42.341786: step 2624, loss 0.0718208, acc 0.984375, prec 0.0592293, recall 0.807776
2017-12-10T03:36:42.603870: step 2625, loss 0.440597, acc 0.84375, prec 0.059255, recall 0.80788
2017-12-10T03:36:42.870582: step 2626, loss 0.291059, acc 0.890625, prec 0.0592655, recall 0.807933
2017-12-10T03:36:43.135058: step 2627, loss 0.443141, acc 0.9375, prec 0.0592607, recall 0.807933
2017-12-10T03:36:43.399603: step 2628, loss 0.386622, acc 0.890625, prec 0.05929, recall 0.808037
2017-12-10T03:36:43.662531: step 2629, loss 0.313485, acc 0.875, prec 0.0592805, recall 0.808037
2017-12-10T03:36:43.926976: step 2630, loss 0.281881, acc 0.9375, prec 0.0592758, recall 0.808037
2017-12-10T03:36:44.188821: step 2631, loss 0.282327, acc 0.921875, prec 0.0592699, recall 0.808037
2017-12-10T03:36:44.452535: step 2632, loss 0.396183, acc 0.921875, prec 0.0593014, recall 0.808141
2017-12-10T03:36:44.724951: step 2633, loss 0.0481073, acc 0.984375, prec 0.059319, recall 0.808193
2017-12-10T03:36:44.992680: step 2634, loss 0.827364, acc 0.8125, prec 0.059361, recall 0.808349
2017-12-10T03:36:45.258212: step 2635, loss 0.144746, acc 0.921875, prec 0.0594113, recall 0.808505
2017-12-10T03:36:45.522734: step 2636, loss 0.244935, acc 0.90625, prec 0.0594042, recall 0.808505
2017-12-10T03:36:45.784329: step 2637, loss 0.202471, acc 0.90625, prec 0.0594158, recall 0.808557
2017-12-10T03:36:46.050017: step 2638, loss 0.168761, acc 0.96875, prec 0.0594321, recall 0.808609
2017-12-10T03:36:46.314817: step 2639, loss 0.0417605, acc 1, prec 0.0594509, recall 0.80866
2017-12-10T03:36:46.576454: step 2640, loss 0.201816, acc 0.90625, prec 0.0594625, recall 0.808712
2017-12-10T03:36:46.843561: step 2641, loss 0.148461, acc 0.953125, prec 0.0594589, recall 0.808712
2017-12-10T03:36:47.109396: step 2642, loss 0.657842, acc 0.890625, prec 0.0594881, recall 0.808816
2017-12-10T03:36:47.378221: step 2643, loss 0.943071, acc 0.96875, prec 0.0595418, recall 0.808971
2017-12-10T03:36:47.645747: step 2644, loss 0.294042, acc 0.96875, prec 0.0595581, recall 0.809022
2017-12-10T03:36:47.906876: step 2645, loss 0.116033, acc 0.9375, prec 0.0595534, recall 0.809022
2017-12-10T03:36:48.174799: step 2646, loss 0.189307, acc 0.90625, prec 0.059565, recall 0.809074
2017-12-10T03:36:48.437114: step 2647, loss 0.257752, acc 0.953125, prec 0.0595614, recall 0.809074
2017-12-10T03:36:48.701413: step 2648, loss 0.251076, acc 0.96875, prec 0.0595778, recall 0.809125
2017-12-10T03:36:48.965715: step 2649, loss 0.479584, acc 0.953125, prec 0.059649, recall 0.809331
2017-12-10T03:36:49.229130: step 2650, loss 0.188137, acc 0.9375, prec 0.0596629, recall 0.809383
2017-12-10T03:36:49.491246: step 2651, loss 3.26783, acc 0.890625, prec 0.0596745, recall 0.809216
2017-12-10T03:36:49.762216: step 2652, loss 0.309923, acc 0.890625, prec 0.0596849, recall 0.809267
2017-12-10T03:36:50.029213: step 2653, loss 0.34128, acc 0.921875, prec 0.059679, recall 0.809267
2017-12-10T03:36:50.290648: step 2654, loss 0.338625, acc 0.90625, prec 0.0596905, recall 0.809319
2017-12-10T03:36:50.550044: step 2655, loss 0.147928, acc 0.9375, prec 0.0597045, recall 0.80937
2017-12-10T03:36:50.809230: step 2656, loss 0.340666, acc 0.875, prec 0.0597136, recall 0.809421
2017-12-10T03:36:51.071425: step 2657, loss 0.484837, acc 0.828125, prec 0.0597006, recall 0.809421
2017-12-10T03:36:51.340167: step 2658, loss 0.332363, acc 0.890625, prec 0.0597296, recall 0.809524
2017-12-10T03:36:51.609614: step 2659, loss 0.304037, acc 0.90625, prec 0.0597412, recall 0.809575
2017-12-10T03:36:51.881851: step 2660, loss 0.203139, acc 0.90625, prec 0.0597527, recall 0.809626
2017-12-10T03:36:52.147582: step 2661, loss 0.283018, acc 0.890625, prec 0.0597444, recall 0.809626
2017-12-10T03:36:52.413578: step 2662, loss 0.401464, acc 0.921875, prec 0.0597758, recall 0.809729
2017-12-10T03:36:52.678506: step 2663, loss 0.255162, acc 0.90625, prec 0.0597687, recall 0.809729
2017-12-10T03:36:52.948557: step 2664, loss 0.291727, acc 0.875, prec 0.0597592, recall 0.809729
2017-12-10T03:36:53.210194: step 2665, loss 0.153958, acc 0.921875, prec 0.0597906, recall 0.809831
2017-12-10T03:36:53.470602: step 2666, loss 0.296878, acc 0.90625, prec 0.0598208, recall 0.809933
2017-12-10T03:36:53.742516: step 2667, loss 0.196548, acc 0.953125, prec 0.0598358, recall 0.809984
2017-12-10T03:36:54.010404: step 2668, loss 0.428696, acc 0.9375, prec 0.0598684, recall 0.810086
2017-12-10T03:36:54.274300: step 2669, loss 0.269849, acc 0.953125, prec 0.0599021, recall 0.810188
2017-12-10T03:36:54.545819: step 2670, loss 0.143894, acc 0.96875, prec 0.0598997, recall 0.810188
2017-12-10T03:36:54.811222: step 2671, loss 0.56945, acc 0.90625, prec 0.0598926, recall 0.810188
2017-12-10T03:36:55.078891: step 2672, loss 0.169763, acc 0.9375, prec 0.0599251, recall 0.810289
2017-12-10T03:36:55.341184: step 2673, loss 0.209997, acc 0.921875, prec 0.0599192, recall 0.810289
2017-12-10T03:36:55.610301: step 2674, loss 0.151051, acc 0.96875, prec 0.0599354, recall 0.81034
2017-12-10T03:36:55.879644: step 2675, loss 0.435241, acc 0.890625, prec 0.0599643, recall 0.810442
2017-12-10T03:36:56.145774: step 2676, loss 0.307378, acc 0.96875, prec 0.0599992, recall 0.810543
2017-12-10T03:36:56.406303: step 2677, loss 0.445276, acc 0.9375, prec 0.0600131, recall 0.810594
2017-12-10T03:36:56.679371: step 2678, loss 0.181862, acc 0.953125, prec 0.0600095, recall 0.810594
2017-12-10T03:36:56.948679: step 2679, loss 0.0613519, acc 0.984375, prec 0.0600269, recall 0.810645
2017-12-10T03:36:57.219325: step 2680, loss 0.0774687, acc 0.984375, prec 0.0600257, recall 0.810645
2017-12-10T03:36:57.481838: step 2681, loss 0.0795751, acc 0.984375, prec 0.0600246, recall 0.810645
2017-12-10T03:36:57.747704: step 2682, loss 1.47768, acc 0.96875, prec 0.060042, recall 0.810479
2017-12-10T03:36:58.019199: step 2683, loss 0.202862, acc 0.96875, prec 0.0600582, recall 0.810529
2017-12-10T03:36:58.284230: step 2684, loss 0.055696, acc 0.96875, prec 0.0600558, recall 0.810529
2017-12-10T03:36:58.548775: step 2685, loss 0.13132, acc 0.9375, prec 0.0600883, recall 0.81063
2017-12-10T03:36:58.811484: step 2686, loss 0.15556, acc 0.9375, prec 0.0601021, recall 0.810681
2017-12-10T03:36:59.075014: step 2687, loss 0.186033, acc 0.96875, prec 0.0600998, recall 0.810681
2017-12-10T03:36:59.343521: step 2688, loss 0.270253, acc 0.953125, prec 0.0601148, recall 0.810731
2017-12-10T03:36:59.604996: step 2689, loss 0.213743, acc 0.9375, prec 0.0601286, recall 0.810782
2017-12-10T03:36:59.870437: step 2690, loss 0.183708, acc 0.9375, prec 0.0601425, recall 0.810832
2017-12-10T03:37:00.141704: step 2691, loss 0.272157, acc 0.875, prec 0.0601516, recall 0.810883
2017-12-10T03:37:00.416682: step 2692, loss 0.0947229, acc 0.9375, prec 0.060184, recall 0.810984
2017-12-10T03:37:00.689460: step 2693, loss 0.238934, acc 0.9375, prec 0.0601792, recall 0.810984
2017-12-10T03:37:00.961155: step 2694, loss 0.143575, acc 0.953125, prec 0.0602128, recall 0.811084
2017-12-10T03:37:01.222474: step 2695, loss 1.02576, acc 0.9375, prec 0.0602267, recall 0.811135
2017-12-10T03:37:01.491435: step 2696, loss 0.16442, acc 0.9375, prec 0.0602219, recall 0.811135
2017-12-10T03:37:01.762783: step 2697, loss 0.127636, acc 0.9375, prec 0.0602357, recall 0.811185
2017-12-10T03:37:02.031265: step 2698, loss 0.613489, acc 0.984375, prec 0.0602717, recall 0.811286
2017-12-10T03:37:02.304728: step 2699, loss 0.416434, acc 0.84375, prec 0.0602784, recall 0.811336
2017-12-10T03:37:02.576344: step 2700, loss 0.290716, acc 0.921875, prec 0.0603096, recall 0.811436

Evaluation:
2017-12-10T03:37:10.185717: step 2700, loss 1.56973, acc 0.886876, prec 0.0609053, recall 0.807466

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-2700

2017-12-10T03:37:11.479180: step 2701, loss 0.574397, acc 0.828125, prec 0.0609105, recall 0.807515
2017-12-10T03:37:11.753620: step 2702, loss 0.343681, acc 0.90625, prec 0.0609034, recall 0.807515
2017-12-10T03:37:12.017379: step 2703, loss 0.426685, acc 0.921875, prec 0.0608976, recall 0.807515
2017-12-10T03:37:12.280971: step 2704, loss 0.295387, acc 0.875, prec 0.0609063, recall 0.807565
2017-12-10T03:37:12.543839: step 2705, loss 0.390247, acc 0.84375, prec 0.0609126, recall 0.807614
2017-12-10T03:37:12.806316: step 2706, loss 0.247608, acc 0.90625, prec 0.0609056, recall 0.807614
2017-12-10T03:37:13.066661: step 2707, loss 0.247169, acc 0.90625, prec 0.0609166, recall 0.807663
2017-12-10T03:37:13.325208: step 2708, loss 0.548118, acc 0.796875, prec 0.0609014, recall 0.807663
2017-12-10T03:37:13.592468: step 2709, loss 0.0888189, acc 0.96875, prec 0.0609171, recall 0.807712
2017-12-10T03:37:13.854546: step 2710, loss 0.285854, acc 0.84375, prec 0.0609416, recall 0.80781
2017-12-10T03:37:14.130823: step 2711, loss 0.224031, acc 0.9375, prec 0.0609549, recall 0.807859
2017-12-10T03:37:14.401366: step 2712, loss 0.305625, acc 0.90625, prec 0.0609841, recall 0.807957
2017-12-10T03:37:14.663924: step 2713, loss 0.159718, acc 0.9375, prec 0.0609974, recall 0.808006
2017-12-10T03:37:14.927766: step 2714, loss 0.155017, acc 0.953125, prec 0.0609939, recall 0.808006
2017-12-10T03:37:15.193206: step 2715, loss 0.117518, acc 0.96875, prec 0.0610096, recall 0.808055
2017-12-10T03:37:15.457493: step 2716, loss 0.157224, acc 0.890625, prec 0.0610014, recall 0.808055
2017-12-10T03:37:15.729541: step 2717, loss 0.0511701, acc 0.984375, prec 0.0610183, recall 0.808104
2017-12-10T03:37:15.994784: step 2718, loss 0.0453821, acc 0.984375, prec 0.0610352, recall 0.808153
2017-12-10T03:37:16.255368: step 2719, loss 0.185008, acc 0.984375, prec 0.0610702, recall 0.808251
2017-12-10T03:37:16.531593: step 2720, loss 0.0731161, acc 0.984375, prec 0.061069, recall 0.808251
2017-12-10T03:37:16.799026: step 2721, loss 0.321841, acc 0.921875, prec 0.0610631, recall 0.808251
2017-12-10T03:37:17.066773: step 2722, loss 0.105586, acc 0.984375, prec 0.06108, recall 0.808299
2017-12-10T03:37:17.338657: step 2723, loss 0.0490751, acc 0.984375, prec 0.0610788, recall 0.808299
2017-12-10T03:37:17.602762: step 2724, loss 0.043136, acc 0.984375, prec 0.0610957, recall 0.808348
2017-12-10T03:37:17.868854: step 2725, loss 0.0473909, acc 0.984375, prec 0.0610945, recall 0.808348
2017-12-10T03:37:18.136044: step 2726, loss 0.130041, acc 0.9375, prec 0.0610898, recall 0.808348
2017-12-10T03:37:18.404442: step 2727, loss 0.556157, acc 0.96875, prec 0.0611056, recall 0.808397
2017-12-10T03:37:18.673502: step 2728, loss 5.18963, acc 0.96875, prec 0.0611044, recall 0.808191
2017-12-10T03:37:18.955188: step 2729, loss 0.116805, acc 0.9375, prec 0.0611358, recall 0.808289
2017-12-10T03:37:19.219857: step 2730, loss 0.0289055, acc 1, prec 0.0611538, recall 0.808338
2017-12-10T03:37:19.497605: step 2731, loss 0.145049, acc 0.953125, prec 0.0611503, recall 0.808338
2017-12-10T03:37:19.761511: step 2732, loss 0.111386, acc 0.953125, prec 0.0611648, recall 0.808386
2017-12-10T03:37:20.030017: step 2733, loss 0.355834, acc 0.90625, prec 0.0611578, recall 0.808386
2017-12-10T03:37:20.306606: step 2734, loss 0.196723, acc 0.90625, prec 0.0611507, recall 0.808386
2017-12-10T03:37:20.571394: step 2735, loss 0.18502, acc 0.921875, prec 0.0611449, recall 0.808386
2017-12-10T03:37:20.840735: step 2736, loss 0.456273, acc 0.90625, prec 0.0611558, recall 0.808435
2017-12-10T03:37:21.108809: step 2737, loss 0.23306, acc 0.921875, prec 0.061168, recall 0.808484
2017-12-10T03:37:21.381609: step 2738, loss 0.332533, acc 0.90625, prec 0.061179, recall 0.808532
2017-12-10T03:37:21.641318: step 2739, loss 0.228657, acc 0.90625, prec 0.0611719, recall 0.808532
2017-12-10T03:37:21.905526: step 2740, loss 0.227878, acc 0.9375, prec 0.0611672, recall 0.808532
2017-12-10T03:37:22.171226: step 2741, loss 0.517173, acc 0.84375, prec 0.0611555, recall 0.808532
2017-12-10T03:37:22.435218: step 2742, loss 0.520721, acc 0.90625, prec 0.0611485, recall 0.808532
2017-12-10T03:37:22.697954: step 2743, loss 0.355689, acc 0.890625, prec 0.0611763, recall 0.808629
2017-12-10T03:37:22.960481: step 2744, loss 1.75681, acc 0.9375, prec 0.0611728, recall 0.808424
2017-12-10T03:37:23.227387: step 2745, loss 0.421039, acc 0.90625, prec 0.0611837, recall 0.808473
2017-12-10T03:37:23.484745: step 2746, loss 0.164473, acc 0.9375, prec 0.061179, recall 0.808473
2017-12-10T03:37:23.753854: step 2747, loss 0.265745, acc 0.90625, prec 0.061208, recall 0.80857
2017-12-10T03:37:24.017495: step 2748, loss 0.359341, acc 0.890625, prec 0.0612178, recall 0.808618
2017-12-10T03:37:24.284712: step 2749, loss 0.177653, acc 0.953125, prec 0.0612143, recall 0.808618
2017-12-10T03:37:24.548794: step 2750, loss 0.410587, acc 0.875, prec 0.0612049, recall 0.808618
2017-12-10T03:37:24.807364: step 2751, loss 0.386395, acc 0.875, prec 0.0611955, recall 0.808618
2017-12-10T03:37:25.069883: step 2752, loss 0.248805, acc 0.9375, prec 0.0611908, recall 0.808618
2017-12-10T03:37:25.332442: step 2753, loss 0.28221, acc 0.921875, prec 0.061221, recall 0.808715
2017-12-10T03:37:25.597212: step 2754, loss 0.459983, acc 0.921875, prec 0.0612331, recall 0.808764
2017-12-10T03:37:25.863283: step 2755, loss 0.394661, acc 0.9375, prec 0.0612644, recall 0.808861
2017-12-10T03:37:26.125048: step 2756, loss 0.26258, acc 0.9375, prec 0.0612597, recall 0.808861
2017-12-10T03:37:26.393215: step 2757, loss 1.50401, acc 0.96875, prec 0.0612585, recall 0.808656
2017-12-10T03:37:26.666663: step 2758, loss 0.116271, acc 0.953125, prec 0.061255, recall 0.808656
2017-12-10T03:37:26.933300: step 2759, loss 0.12669, acc 0.953125, prec 0.0612515, recall 0.808656
2017-12-10T03:37:27.197376: step 2760, loss 0.303022, acc 0.90625, prec 0.0612444, recall 0.808656
2017-12-10T03:37:27.461915: step 2761, loss 0.292953, acc 0.875, prec 0.061253, recall 0.808704
2017-12-10T03:37:27.727103: step 2762, loss 0.399977, acc 0.921875, prec 0.0613011, recall 0.80885
2017-12-10T03:37:27.989946: step 2763, loss 0.0601179, acc 0.984375, prec 0.0613, recall 0.80885
2017-12-10T03:37:28.251897: step 2764, loss 0.213329, acc 0.90625, prec 0.0613109, recall 0.808898
2017-12-10T03:37:28.518590: step 2765, loss 4.53435, acc 0.875, prec 0.0613027, recall 0.808693
2017-12-10T03:37:28.789531: step 2766, loss 0.362155, acc 0.875, prec 0.0613292, recall 0.80879
2017-12-10T03:37:29.054368: step 2767, loss 0.263339, acc 0.90625, prec 0.0613222, recall 0.80879
2017-12-10T03:37:29.314382: step 2768, loss 0.726135, acc 0.796875, prec 0.0613249, recall 0.808838
2017-12-10T03:37:29.574943: step 2769, loss 0.256734, acc 0.875, prec 0.0613155, recall 0.808838
2017-12-10T03:37:29.839189: step 2770, loss 0.453655, acc 0.875, prec 0.0613061, recall 0.808838
2017-12-10T03:37:30.107411: step 2771, loss 0.27695, acc 0.890625, prec 0.0613518, recall 0.808983
2017-12-10T03:37:30.377737: step 2772, loss 0.450715, acc 0.9375, prec 0.0613471, recall 0.808983
2017-12-10T03:37:30.642736: step 2773, loss 0.366121, acc 0.859375, prec 0.0613365, recall 0.808983
2017-12-10T03:37:30.914102: step 2774, loss 0.514562, acc 0.859375, prec 0.0613439, recall 0.809031
2017-12-10T03:37:31.174497: step 2775, loss 0.412983, acc 0.859375, prec 0.0613334, recall 0.809031
2017-12-10T03:37:31.437422: step 2776, loss 0.384459, acc 0.890625, prec 0.0613431, recall 0.809079
2017-12-10T03:37:31.700683: step 2777, loss 0.348214, acc 0.890625, prec 0.0613529, recall 0.809128
2017-12-10T03:37:31.968743: step 2778, loss 0.510714, acc 0.84375, prec 0.0613411, recall 0.809128
2017-12-10T03:37:32.229587: step 2779, loss 0.75685, acc 0.921875, prec 0.061407, recall 0.80932
2017-12-10T03:37:32.495127: step 2780, loss 0.202211, acc 0.953125, prec 0.0614394, recall 0.809416
2017-12-10T03:37:32.768805: step 2781, loss 0.523492, acc 0.859375, prec 0.0614288, recall 0.809416
2017-12-10T03:37:33.038269: step 2782, loss 0.199663, acc 0.921875, prec 0.0614229, recall 0.809416
2017-12-10T03:37:33.299070: step 2783, loss 0.402724, acc 0.890625, prec 0.0614506, recall 0.809512
2017-12-10T03:37:33.567319: step 2784, loss 0.804822, acc 0.84375, prec 0.0614747, recall 0.809608
2017-12-10T03:37:33.827758: step 2785, loss 0.287125, acc 0.9375, prec 0.0615238, recall 0.809751
2017-12-10T03:37:34.099500: step 2786, loss 0.421245, acc 0.96875, prec 0.0615393, recall 0.809799
2017-12-10T03:37:34.367617: step 2787, loss 0.201741, acc 0.953125, prec 0.0615537, recall 0.809847
2017-12-10T03:37:34.637179: step 2788, loss 0.219873, acc 0.890625, prec 0.0615634, recall 0.809895
2017-12-10T03:37:34.905119: step 2789, loss 0.158531, acc 0.953125, prec 0.0615599, recall 0.809895
2017-12-10T03:37:35.169298: step 2790, loss 0.203026, acc 0.953125, prec 0.0615564, recall 0.809895
2017-12-10T03:37:35.443655: step 2791, loss 0.123701, acc 0.96875, prec 0.0615898, recall 0.80999
2017-12-10T03:37:35.707663: step 2792, loss 0.229509, acc 0.96875, prec 0.0616591, recall 0.810181
2017-12-10T03:37:35.974907: step 2793, loss 0.311328, acc 0.90625, prec 0.06167, recall 0.810228
2017-12-10T03:37:36.236497: step 2794, loss 0.164488, acc 0.953125, prec 0.0617022, recall 0.810323
2017-12-10T03:37:36.499876: step 2795, loss 0.184772, acc 0.921875, prec 0.0617322, recall 0.810418
2017-12-10T03:37:36.765941: step 2796, loss 0.11155, acc 0.953125, prec 0.0617286, recall 0.810418
2017-12-10T03:37:37.030869: step 2797, loss 0.0516065, acc 0.984375, prec 0.0617632, recall 0.810513
2017-12-10T03:37:37.293995: step 2798, loss 0.115621, acc 0.953125, prec 0.0617597, recall 0.810513
2017-12-10T03:37:37.568194: step 2799, loss 0.0577854, acc 0.96875, prec 0.0617753, recall 0.810561
2017-12-10T03:37:37.829912: step 2800, loss 0.386586, acc 1, prec 0.0617931, recall 0.810608
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-2800

2017-12-10T03:37:39.172616: step 2801, loss 0.0850317, acc 0.96875, prec 0.0618087, recall 0.810655
2017-12-10T03:37:39.446239: step 2802, loss 0.342244, acc 0.984375, prec 0.0618254, recall 0.810703
2017-12-10T03:37:39.711999: step 2803, loss 0.163422, acc 0.921875, prec 0.0618553, recall 0.810797
2017-12-10T03:37:39.978760: step 2804, loss 0.120414, acc 1, prec 0.0618911, recall 0.810892
2017-12-10T03:37:40.242522: step 2805, loss 0.0292741, acc 1, prec 0.0618911, recall 0.810892
2017-12-10T03:37:40.504138: step 2806, loss 1.30993, acc 1, prec 0.0619447, recall 0.811033
2017-12-10T03:37:40.769930: step 2807, loss 0.051402, acc 1, prec 0.0619626, recall 0.811081
2017-12-10T03:37:41.047748: step 2808, loss 0.83196, acc 0.96875, prec 0.0619781, recall 0.811128
2017-12-10T03:37:41.317822: step 2809, loss 0.187416, acc 0.96875, prec 0.0619758, recall 0.811128
2017-12-10T03:37:41.578318: step 2810, loss 0.382276, acc 0.921875, prec 0.0619877, recall 0.811175
2017-12-10T03:37:41.854894: step 2811, loss 0.553778, acc 0.875, prec 0.0619783, recall 0.811175
2017-12-10T03:37:42.122689: step 2812, loss 0.561209, acc 0.859375, prec 0.0619676, recall 0.811175
2017-12-10T03:37:42.384180: step 2813, loss 0.510471, acc 0.828125, prec 0.0619904, recall 0.811269
2017-12-10T03:37:42.647317: step 2814, loss 0.338681, acc 0.875, prec 0.061981, recall 0.811269
2017-12-10T03:37:42.909779: step 2815, loss 0.289302, acc 0.890625, prec 0.0619906, recall 0.811316
2017-12-10T03:37:43.176966: step 2816, loss 0.416286, acc 0.828125, prec 0.0620133, recall 0.81141
2017-12-10T03:37:43.439463: step 2817, loss 0.43255, acc 0.890625, prec 0.0620229, recall 0.811457
2017-12-10T03:37:43.706280: step 2818, loss 0.583376, acc 0.8125, prec 0.0620266, recall 0.811504
2017-12-10T03:37:43.972297: step 2819, loss 0.44564, acc 0.890625, prec 0.0620362, recall 0.811551
2017-12-10T03:37:44.237084: step 2820, loss 0.418706, acc 0.84375, prec 0.06206, recall 0.811645
2017-12-10T03:37:44.503963: step 2821, loss 0.370802, acc 0.890625, prec 0.0620696, recall 0.811692
2017-12-10T03:37:44.768543: step 2822, loss 0.202403, acc 0.921875, prec 0.0620637, recall 0.811692
2017-12-10T03:37:45.031715: step 2823, loss 0.460506, acc 0.984375, prec 0.0620804, recall 0.811738
2017-12-10T03:37:45.304067: step 2824, loss 0.583678, acc 0.859375, prec 0.0620876, recall 0.811785
2017-12-10T03:37:45.567826: step 2825, loss 0.123209, acc 0.984375, prec 0.0620864, recall 0.811785
2017-12-10T03:37:45.842743: step 2826, loss 0.274319, acc 0.875, prec 0.0620948, recall 0.811832
2017-12-10T03:37:46.108679: step 2827, loss 0.175351, acc 0.921875, prec 0.0620889, recall 0.811832
2017-12-10T03:37:46.373045: step 2828, loss 0.0743619, acc 0.96875, prec 0.0621044, recall 0.811879
2017-12-10T03:37:46.640270: step 2829, loss 0.0990629, acc 0.953125, prec 0.0621008, recall 0.811879
2017-12-10T03:37:46.907600: step 2830, loss 6.55671, acc 0.9375, prec 0.0621151, recall 0.811724
2017-12-10T03:37:47.181446: step 2831, loss 0.0581841, acc 0.984375, prec 0.0621139, recall 0.811724
2017-12-10T03:37:47.446095: step 2832, loss 0.206782, acc 0.921875, prec 0.062108, recall 0.811724
2017-12-10T03:37:47.708911: step 2833, loss 0.374734, acc 0.859375, prec 0.0621509, recall 0.811864
2017-12-10T03:37:47.974220: step 2834, loss 0.313295, acc 0.90625, prec 0.0621438, recall 0.811864
2017-12-10T03:37:48.239128: step 2835, loss 0.144672, acc 0.953125, prec 0.0621402, recall 0.811864
2017-12-10T03:37:48.507370: step 2836, loss 0.318505, acc 0.921875, prec 0.0621343, recall 0.811864
2017-12-10T03:37:48.773751: step 2837, loss 0.197685, acc 0.90625, prec 0.0621451, recall 0.811911
2017-12-10T03:37:49.038420: step 2838, loss 0.483973, acc 0.859375, prec 0.0621523, recall 0.811957
2017-12-10T03:37:49.300359: step 2839, loss 0.531868, acc 0.828125, prec 0.0621393, recall 0.811957
2017-12-10T03:37:49.564603: step 2840, loss 0.434617, acc 0.890625, prec 0.0621666, recall 0.812051
2017-12-10T03:37:49.827944: step 2841, loss 0.366689, acc 0.828125, prec 0.0621536, recall 0.812051
2017-12-10T03:37:50.089432: step 2842, loss 0.131447, acc 0.953125, prec 0.0621679, recall 0.812097
2017-12-10T03:37:50.357678: step 2843, loss 0.288736, acc 0.890625, prec 0.0622308, recall 0.812283
2017-12-10T03:37:50.619639: step 2844, loss 0.552416, acc 0.890625, prec 0.0622403, recall 0.81233
2017-12-10T03:37:50.887286: step 2845, loss 0.596027, acc 0.890625, prec 0.0622321, recall 0.81233
2017-12-10T03:37:51.153813: step 2846, loss 0.447043, acc 0.890625, prec 0.0622416, recall 0.812376
2017-12-10T03:37:51.418015: step 2847, loss 0.223899, acc 0.890625, prec 0.0622333, recall 0.812376
2017-12-10T03:37:51.683468: step 2848, loss 0.268719, acc 0.921875, prec 0.0622452, recall 0.812423
2017-12-10T03:37:51.948096: step 2849, loss 0.245393, acc 0.9375, prec 0.0622583, recall 0.812469
2017-12-10T03:37:52.215940: step 2850, loss 0.0982381, acc 0.96875, prec 0.0622559, recall 0.812469
2017-12-10T03:37:52.481922: step 2851, loss 0.235158, acc 0.90625, prec 0.0623199, recall 0.812654
2017-12-10T03:37:52.742892: step 2852, loss 1.11971, acc 0.90625, prec 0.0623306, recall 0.812701
2017-12-10T03:37:53.017613: step 2853, loss 1.53336, acc 0.953125, prec 0.0623283, recall 0.8125
2017-12-10T03:37:53.282817: step 2854, loss 0.130785, acc 0.921875, prec 0.0623757, recall 0.812639
2017-12-10T03:37:53.546268: step 2855, loss 0.296638, acc 0.90625, prec 0.0623686, recall 0.812639
2017-12-10T03:37:53.814703: step 2856, loss 0.013418, acc 1, prec 0.0623863, recall 0.812685
2017-12-10T03:37:54.075948: step 2857, loss 0.732474, acc 0.921875, prec 0.0624159, recall 0.812778
2017-12-10T03:37:54.340947: step 2858, loss 0.232744, acc 0.9375, prec 0.062429, recall 0.812824
2017-12-10T03:37:54.605427: step 2859, loss 0.140984, acc 0.96875, prec 0.0624444, recall 0.81287
2017-12-10T03:37:54.875610: step 2860, loss 0.901528, acc 0.953125, prec 0.0624586, recall 0.812916
2017-12-10T03:37:55.142213: step 2861, loss 0.145725, acc 0.9375, prec 0.0624716, recall 0.812962
2017-12-10T03:37:55.408050: step 2862, loss 0.458164, acc 0.875, prec 0.0624799, recall 0.813008
2017-12-10T03:37:55.668002: step 2863, loss 0.586831, acc 0.890625, prec 0.0624716, recall 0.813008
2017-12-10T03:37:55.940921: step 2864, loss 0.412905, acc 0.953125, prec 0.0625035, recall 0.8131
2017-12-10T03:37:56.211715: step 2865, loss 0.138079, acc 0.9375, prec 0.0624988, recall 0.8131
2017-12-10T03:37:56.473641: step 2866, loss 0.184464, acc 0.9375, prec 0.0624941, recall 0.8131
2017-12-10T03:37:56.746975: step 2867, loss 0.372167, acc 0.9375, prec 0.0625071, recall 0.813146
2017-12-10T03:37:57.009453: step 2868, loss 0.44727, acc 0.90625, prec 0.0625177, recall 0.813192
2017-12-10T03:37:57.280776: step 2869, loss 0.251628, acc 0.90625, prec 0.0625106, recall 0.813192
2017-12-10T03:37:57.545910: step 2870, loss 0.153959, acc 0.9375, prec 0.0625236, recall 0.813238
2017-12-10T03:37:57.817432: step 2871, loss 0.314402, acc 0.890625, prec 0.0625154, recall 0.813238
2017-12-10T03:37:58.080466: step 2872, loss 0.086456, acc 0.9375, prec 0.0625106, recall 0.813238
2017-12-10T03:37:58.345025: step 2873, loss 9.42161, acc 0.96875, prec 0.0625272, recall 0.813084
2017-12-10T03:37:58.616784: step 2874, loss 0.242229, acc 0.90625, prec 0.0625378, recall 0.81313
2017-12-10T03:37:58.878208: step 2875, loss 0.334296, acc 0.921875, prec 0.0625496, recall 0.813176
2017-12-10T03:37:59.141065: step 2876, loss 0.372034, acc 0.84375, prec 0.0625378, recall 0.813176
2017-12-10T03:37:59.400814: step 2877, loss 0.366044, acc 0.90625, prec 0.0625307, recall 0.813176
2017-12-10T03:37:59.666930: step 2878, loss 0.600445, acc 0.859375, prec 0.0625378, recall 0.813222
2017-12-10T03:37:59.931190: step 2879, loss 0.380736, acc 0.90625, prec 0.0625307, recall 0.813222
2017-12-10T03:38:00.198928: step 2880, loss 0.406448, acc 0.859375, prec 0.0625378, recall 0.813268
2017-12-10T03:38:00.474685: step 2881, loss 0.244904, acc 0.921875, prec 0.0625319, recall 0.813268
2017-12-10T03:38:00.738581: step 2882, loss 0.436921, acc 0.875, prec 0.0625224, recall 0.813268
2017-12-10T03:38:01.001345: step 2883, loss 0.303759, acc 0.890625, prec 0.0625496, recall 0.813359
2017-12-10T03:38:01.264935: step 2884, loss 0.514856, acc 0.90625, prec 0.0625602, recall 0.813405
2017-12-10T03:38:01.528119: step 2885, loss 0.315737, acc 0.890625, prec 0.0625519, recall 0.813405
2017-12-10T03:38:01.799644: step 2886, loss 0.641508, acc 0.875, prec 0.0625602, recall 0.813451
2017-12-10T03:38:02.060009: step 2887, loss 0.413977, acc 0.875, prec 0.0626038, recall 0.813588
2017-12-10T03:38:02.324641: step 2888, loss 0.150757, acc 0.90625, prec 0.0625967, recall 0.813588
2017-12-10T03:38:02.586549: step 2889, loss 0.204406, acc 0.953125, prec 0.0626109, recall 0.813634
2017-12-10T03:38:02.853101: step 2890, loss 0.139739, acc 0.953125, prec 0.0626073, recall 0.813634
2017-12-10T03:38:03.113741: step 2891, loss 1.26691, acc 0.9375, prec 0.0626203, recall 0.81368
2017-12-10T03:38:03.382999: step 2892, loss 0.250503, acc 0.90625, prec 0.0626132, recall 0.81368
2017-12-10T03:38:03.653056: step 2893, loss 0.337963, acc 0.90625, prec 0.0626238, recall 0.813725
2017-12-10T03:38:03.919726: step 2894, loss 0.308495, acc 0.890625, prec 0.0626155, recall 0.813725
2017-12-10T03:38:04.188996: step 2895, loss 0.157306, acc 0.953125, prec 0.0626473, recall 0.813817
2017-12-10T03:38:04.457148: step 2896, loss 0.392446, acc 0.90625, prec 0.0626402, recall 0.813817
2017-12-10T03:38:04.730404: step 2897, loss 0.311991, acc 0.9375, prec 0.0626709, recall 0.813908
2017-12-10T03:38:04.993606: step 2898, loss 0.58468, acc 0.90625, prec 0.0626991, recall 0.813999
2017-12-10T03:38:05.261547: step 2899, loss 0.348465, acc 0.953125, prec 0.0627132, recall 0.814045
2017-12-10T03:38:05.525469: step 2900, loss 0.142972, acc 0.9375, prec 0.0627438, recall 0.814135
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-2900

2017-12-10T03:38:06.937492: step 2901, loss 0.125177, acc 0.9375, prec 0.0627391, recall 0.814135
2017-12-10T03:38:07.212179: step 2902, loss 0.422623, acc 0.96875, prec 0.0627721, recall 0.814226
2017-12-10T03:38:07.478547: step 2903, loss 0.154342, acc 0.96875, prec 0.0627697, recall 0.814226
2017-12-10T03:38:07.742108: step 2904, loss 0.117866, acc 0.953125, prec 0.0627838, recall 0.814272
2017-12-10T03:38:08.006770: step 2905, loss 0.564001, acc 0.921875, prec 0.0627956, recall 0.814317
2017-12-10T03:38:08.277970: step 2906, loss 0.0910462, acc 0.96875, prec 0.0627932, recall 0.814317
2017-12-10T03:38:08.542513: step 2907, loss 0.40226, acc 0.890625, prec 0.0628026, recall 0.814362
2017-12-10T03:38:08.809402: step 2908, loss 0.344075, acc 0.90625, prec 0.0627955, recall 0.814362
2017-12-10T03:38:09.078102: step 2909, loss 0.181969, acc 0.90625, prec 0.062806, recall 0.814408
2017-12-10T03:38:09.345600: step 2910, loss 1.29222, acc 0.96875, prec 0.0628225, recall 0.814254
2017-12-10T03:38:09.621127: step 2911, loss 0.0145553, acc 1, prec 0.0628401, recall 0.8143
2017-12-10T03:38:09.883191: step 2912, loss 0.391458, acc 0.9375, prec 0.0628531, recall 0.814345
2017-12-10T03:38:10.154954: step 2913, loss 0.12104, acc 0.96875, prec 0.0629036, recall 0.814481
2017-12-10T03:38:10.415984: step 2914, loss 0.523603, acc 0.90625, prec 0.0629142, recall 0.814526
2017-12-10T03:38:10.684348: step 2915, loss 0.0655307, acc 0.96875, prec 0.0629294, recall 0.814571
2017-12-10T03:38:10.950256: step 2916, loss 0.331984, acc 0.953125, prec 0.0629259, recall 0.814571
2017-12-10T03:38:11.213040: step 2917, loss 0.251131, acc 0.921875, prec 0.06292, recall 0.814571
2017-12-10T03:38:11.471974: step 2918, loss 0.241008, acc 0.921875, prec 0.0629317, recall 0.814616
2017-12-10T03:38:11.750461: step 2919, loss 0.394078, acc 0.9375, prec 0.0629269, recall 0.814616
2017-12-10T03:38:12.012333: step 2920, loss 0.524963, acc 0.921875, prec 0.0629915, recall 0.814797
2017-12-10T03:38:12.281315: step 2921, loss 0.197195, acc 0.9375, prec 0.0629868, recall 0.814797
2017-12-10T03:38:12.549782: step 2922, loss 0.188665, acc 0.96875, prec 0.063002, recall 0.814842
2017-12-10T03:38:12.815349: step 2923, loss 0.140827, acc 0.953125, prec 0.0630161, recall 0.814887
2017-12-10T03:38:13.077705: step 2924, loss 0.0370077, acc 0.96875, prec 0.0630137, recall 0.814887
2017-12-10T03:38:13.344448: step 2925, loss 0.110575, acc 0.9375, prec 0.063009, recall 0.814887
2017-12-10T03:38:13.607291: step 2926, loss 0.14049, acc 0.9375, prec 0.0630043, recall 0.814887
2017-12-10T03:38:13.871979: step 2927, loss 0.476422, acc 0.9375, prec 0.0630172, recall 0.814932
2017-12-10T03:38:14.136682: step 2928, loss 0.376943, acc 0.9375, prec 0.0630653, recall 0.815067
2017-12-10T03:38:14.411507: step 2929, loss 0.175307, acc 0.9375, prec 0.0630605, recall 0.815067
2017-12-10T03:38:14.673541: step 2930, loss 0.215183, acc 0.96875, prec 0.0630934, recall 0.815157
2017-12-10T03:38:14.935987: step 2931, loss 0.033337, acc 1, prec 0.063111, recall 0.815202
2017-12-10T03:38:15.198062: step 2932, loss 0.0664236, acc 0.96875, prec 0.0631086, recall 0.815202
2017-12-10T03:38:15.466644: step 2933, loss 0.0461044, acc 0.984375, prec 0.063125, recall 0.815246
2017-12-10T03:38:15.732073: step 2934, loss 0.749091, acc 0.96875, prec 0.0631579, recall 0.815336
2017-12-10T03:38:16.002076: step 2935, loss 0.0421691, acc 0.984375, prec 0.0631567, recall 0.815336
2017-12-10T03:38:16.264172: step 2936, loss 0.204647, acc 0.96875, prec 0.0631719, recall 0.815381
2017-12-10T03:38:16.529875: step 2937, loss 0.0647396, acc 0.96875, prec 0.0631872, recall 0.815426
2017-12-10T03:38:16.802856: step 2938, loss 0.0539545, acc 0.96875, prec 0.0631848, recall 0.815426
2017-12-10T03:38:17.067100: step 2939, loss 0.148663, acc 0.953125, prec 0.0631812, recall 0.815426
2017-12-10T03:38:17.337257: step 2940, loss 0.0139229, acc 1, prec 0.0631812, recall 0.815426
2017-12-10T03:38:17.603183: step 2941, loss 0.0203909, acc 0.984375, prec 0.0631801, recall 0.815426
2017-12-10T03:38:17.868249: step 2942, loss 0.785476, acc 0.921875, prec 0.0631917, recall 0.81547
2017-12-10T03:38:18.150849: step 2943, loss 0.0961365, acc 0.9375, prec 0.0632046, recall 0.815515
2017-12-10T03:38:18.412582: step 2944, loss 3.70121, acc 0.921875, prec 0.0632174, recall 0.815362
2017-12-10T03:38:18.684794: step 2945, loss 0.131676, acc 0.953125, prec 0.0632315, recall 0.815407
2017-12-10T03:38:18.952087: step 2946, loss 0.481786, acc 0.890625, prec 0.0632231, recall 0.815407
2017-12-10T03:38:19.218551: step 2947, loss 0.321025, acc 0.90625, prec 0.063216, recall 0.815407
2017-12-10T03:38:19.487474: step 2948, loss 0.409333, acc 0.875, prec 0.0632065, recall 0.815407
2017-12-10T03:38:19.755353: step 2949, loss 0.259763, acc 0.9375, prec 0.063237, recall 0.815496
2017-12-10T03:38:20.022008: step 2950, loss 0.479662, acc 0.84375, prec 0.0632251, recall 0.815496
2017-12-10T03:38:20.292507: step 2951, loss 0.492431, acc 0.859375, prec 0.0632496, recall 0.815586
2017-12-10T03:38:20.552884: step 2952, loss 0.569271, acc 0.90625, prec 0.0632952, recall 0.815719
2017-12-10T03:38:20.817297: step 2953, loss 0.457796, acc 0.859375, prec 0.0633021, recall 0.815764
2017-12-10T03:38:21.083825: step 2954, loss 0.279517, acc 0.890625, prec 0.0633113, recall 0.815809
2017-12-10T03:38:21.348673: step 2955, loss 0.561765, acc 0.796875, prec 0.0633135, recall 0.815853
2017-12-10T03:38:21.617998: step 2956, loss 0.226174, acc 0.890625, prec 0.0633403, recall 0.815942
2017-12-10T03:38:21.884786: step 2957, loss 0.475781, acc 0.8125, prec 0.0633436, recall 0.815986
2017-12-10T03:38:22.157026: step 2958, loss 0.252013, acc 0.953125, prec 0.0633751, recall 0.816075
2017-12-10T03:38:22.424579: step 2959, loss 0.302936, acc 0.875, prec 0.0633832, recall 0.81612
2017-12-10T03:38:22.689503: step 2960, loss 0.47447, acc 0.828125, prec 0.0633701, recall 0.81612
2017-12-10T03:38:22.952243: step 2961, loss 0.380888, acc 0.90625, prec 0.063363, recall 0.81612
2017-12-10T03:38:23.216166: step 2962, loss 0.487211, acc 0.890625, prec 0.0634073, recall 0.816253
2017-12-10T03:38:23.480416: step 2963, loss 0.20863, acc 0.9375, prec 0.0634201, recall 0.816297
2017-12-10T03:38:23.745886: step 2964, loss 0.18788, acc 0.953125, prec 0.0634341, recall 0.816341
2017-12-10T03:38:24.010643: step 2965, loss 0.0821189, acc 0.96875, prec 0.0634317, recall 0.816341
2017-12-10T03:38:24.276044: step 2966, loss 0.128956, acc 0.953125, prec 0.0634282, recall 0.816341
2017-12-10T03:38:24.541395: step 2967, loss 0.119632, acc 0.9375, prec 0.0634234, recall 0.816341
2017-12-10T03:38:24.807205: step 2968, loss 0.0701127, acc 0.96875, prec 0.0634386, recall 0.816386
2017-12-10T03:38:25.076045: step 2969, loss 0.254571, acc 0.953125, prec 0.0634525, recall 0.81643
2017-12-10T03:38:25.340950: step 2970, loss 0.104658, acc 0.96875, prec 0.0634677, recall 0.816474
2017-12-10T03:38:25.607878: step 2971, loss 0.121788, acc 0.96875, prec 0.0634653, recall 0.816474
2017-12-10T03:38:25.871993: step 2972, loss 0.101099, acc 0.984375, prec 0.0634641, recall 0.816474
2017-12-10T03:38:26.132793: step 2973, loss 0.208417, acc 0.96875, prec 0.0634618, recall 0.816474
2017-12-10T03:38:26.396753: step 2974, loss 0.220689, acc 0.96875, prec 0.0634944, recall 0.816562
2017-12-10T03:38:26.679158: step 2975, loss 3.05322, acc 0.9375, prec 0.0635259, recall 0.816454
2017-12-10T03:38:26.951270: step 2976, loss 0.0871308, acc 0.984375, prec 0.0635423, recall 0.816498
2017-12-10T03:38:27.212761: step 2977, loss 0.719824, acc 0.984375, prec 0.0635586, recall 0.816542
2017-12-10T03:38:27.478256: step 2978, loss 0.0456031, acc 0.984375, prec 0.0635749, recall 0.816587
2017-12-10T03:38:27.748561: step 2979, loss 0.140418, acc 0.96875, prec 0.0635901, recall 0.816631
2017-12-10T03:38:28.010228: step 2980, loss 0.0952408, acc 0.9375, prec 0.0635853, recall 0.816631
2017-12-10T03:38:28.271599: step 2981, loss 0.214569, acc 0.96875, prec 0.0635829, recall 0.816631
2017-12-10T03:38:28.495888: step 2982, loss 0.320689, acc 0.901961, prec 0.063577, recall 0.816631
2017-12-10T03:38:28.768467: step 2983, loss 0.534484, acc 0.78125, prec 0.0635779, recall 0.816675
2017-12-10T03:38:29.031632: step 2984, loss 0.386891, acc 0.828125, prec 0.0635998, recall 0.816763
2017-12-10T03:38:29.306707: step 2985, loss 0.347268, acc 0.875, prec 0.0636078, recall 0.816807
2017-12-10T03:38:29.570902: step 2986, loss 0.213341, acc 0.90625, prec 0.0636357, recall 0.816895
2017-12-10T03:38:29.847763: step 2987, loss 0.339599, acc 0.875, prec 0.0636262, recall 0.816895
2017-12-10T03:38:30.125178: step 2988, loss 0.479678, acc 0.875, prec 0.0636167, recall 0.816895
2017-12-10T03:38:30.397297: step 2989, loss 0.174391, acc 0.890625, prec 0.0636083, recall 0.816895
2017-12-10T03:38:30.663839: step 2990, loss 0.362116, acc 0.875, prec 0.0636163, recall 0.816939
2017-12-10T03:38:30.937185: step 2991, loss 0.233694, acc 0.9375, prec 0.0636116, recall 0.816939
2017-12-10T03:38:31.202653: step 2992, loss 0.133727, acc 0.953125, prec 0.0636255, recall 0.816983
2017-12-10T03:38:31.466706: step 2993, loss 0.251871, acc 0.90625, prec 0.0636184, recall 0.816983
2017-12-10T03:38:31.730104: step 2994, loss 0.241569, acc 0.921875, prec 0.0636124, recall 0.816983
2017-12-10T03:38:31.993827: step 2995, loss 0.197266, acc 0.921875, prec 0.0636065, recall 0.816983
2017-12-10T03:38:32.261827: step 2996, loss 0.0437426, acc 0.984375, prec 0.0636053, recall 0.816983
2017-12-10T03:38:32.522271: step 2997, loss 0.0406941, acc 0.984375, prec 0.0636041, recall 0.816983
2017-12-10T03:38:32.784710: step 2998, loss 0.13499, acc 0.9375, prec 0.0636343, recall 0.81707
2017-12-10T03:38:33.056113: step 2999, loss 0.121093, acc 0.96875, prec 0.063632, recall 0.81707
2017-12-10T03:38:33.322875: step 3000, loss 0.203453, acc 0.9375, prec 0.0636272, recall 0.81707

Evaluation:
2017-12-10T03:38:41.012246: step 3000, loss 3.71428, acc 0.963204, prec 0.0641547, recall 0.800093

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-3000

2017-12-10T03:38:42.255655: step 3001, loss 0.836524, acc 0.921875, prec 0.0641834, recall 0.800185
2017-12-10T03:38:42.519298: step 3002, loss 0.189297, acc 0.9375, prec 0.0642481, recall 0.80037
2017-12-10T03:38:42.782270: step 3003, loss 0.0453984, acc 0.96875, prec 0.0642457, recall 0.80037
2017-12-10T03:38:43.053071: step 3004, loss 0.0732729, acc 0.984375, prec 0.0642792, recall 0.800462
2017-12-10T03:38:43.317054: step 3005, loss 0.200377, acc 1, prec 0.0642966, recall 0.800508
2017-12-10T03:38:43.586189: step 3006, loss 0.0583915, acc 0.96875, prec 0.0642942, recall 0.800508
2017-12-10T03:38:43.852462: step 3007, loss 0.135599, acc 1, prec 0.0643115, recall 0.800554
2017-12-10T03:38:44.117214: step 3008, loss 0.0685098, acc 0.984375, prec 0.0643103, recall 0.800554
2017-12-10T03:38:44.376563: step 3009, loss 0.200682, acc 0.953125, prec 0.0643241, recall 0.8006
2017-12-10T03:38:44.636454: step 3010, loss 0.133302, acc 0.953125, prec 0.0643379, recall 0.800646
2017-12-10T03:38:44.899885: step 3011, loss 0.841388, acc 0.9375, prec 0.0643678, recall 0.800738
2017-12-10T03:38:45.170534: step 3012, loss 0.0435718, acc 0.984375, prec 0.0643666, recall 0.800738
2017-12-10T03:38:45.436781: step 3013, loss 0.135848, acc 0.96875, prec 0.0643816, recall 0.800784
2017-12-10T03:38:45.702179: step 3014, loss 0.115999, acc 0.953125, prec 0.064378, recall 0.800784
2017-12-10T03:38:45.966185: step 3015, loss 0.0793804, acc 0.9375, prec 0.0643732, recall 0.800784
2017-12-10T03:38:46.231345: step 3016, loss 0.173772, acc 0.953125, prec 0.064387, recall 0.80083
2017-12-10T03:38:46.509587: step 3017, loss 0.179185, acc 0.984375, prec 0.0644031, recall 0.800876
2017-12-10T03:38:46.771708: step 3018, loss 0.0493561, acc 0.96875, prec 0.0644007, recall 0.800876
2017-12-10T03:38:47.031071: step 3019, loss 0.0432574, acc 0.984375, prec 0.0644342, recall 0.800968
2017-12-10T03:38:47.295601: step 3020, loss 0.167192, acc 0.953125, prec 0.0644653, recall 0.801059
2017-12-10T03:38:47.566700: step 3021, loss 0.212099, acc 0.953125, prec 0.0644617, recall 0.801059
2017-12-10T03:38:47.827414: step 3022, loss 0.0606611, acc 0.96875, prec 0.064494, recall 0.801151
2017-12-10T03:38:48.097175: step 3023, loss 0.15576, acc 0.9375, prec 0.0644892, recall 0.801151
2017-12-10T03:38:48.371956: step 3024, loss 0.738813, acc 0.96875, prec 0.0645042, recall 0.801197
2017-12-10T03:38:48.639054: step 3025, loss 0.112615, acc 0.953125, prec 0.0645179, recall 0.801242
2017-12-10T03:38:48.912783: step 3026, loss 0.0589837, acc 0.96875, prec 0.0645155, recall 0.801242
2017-12-10T03:38:49.177416: step 3027, loss 0.234778, acc 0.921875, prec 0.0645442, recall 0.801334
2017-12-10T03:38:49.451154: step 3028, loss 0.175222, acc 0.953125, prec 0.0645753, recall 0.801425
2017-12-10T03:38:49.715635: step 3029, loss 0.292435, acc 0.953125, prec 0.0646063, recall 0.801516
2017-12-10T03:38:49.985045: step 3030, loss 0.118696, acc 0.96875, prec 0.0646213, recall 0.801562
2017-12-10T03:38:50.251282: step 3031, loss 0.114952, acc 0.984375, prec 0.0646201, recall 0.801562
2017-12-10T03:38:50.517370: step 3032, loss 0.300437, acc 0.921875, prec 0.0646314, recall 0.801607
2017-12-10T03:38:50.780614: step 3033, loss 0.238432, acc 0.984375, prec 0.0646475, recall 0.801653
2017-12-10T03:38:51.054372: step 3034, loss 0.0823847, acc 0.96875, prec 0.0646451, recall 0.801653
2017-12-10T03:38:51.322134: step 3035, loss 0.171665, acc 0.9375, prec 0.0646403, recall 0.801653
2017-12-10T03:38:51.591018: step 3036, loss 0.204964, acc 0.9375, prec 0.0646355, recall 0.801653
2017-12-10T03:38:51.863183: step 3037, loss 0.0403319, acc 0.984375, prec 0.0646517, recall 0.801698
2017-12-10T03:38:52.127007: step 3038, loss 0.336202, acc 0.9375, prec 0.0646815, recall 0.801789
2017-12-10T03:38:52.392831: step 3039, loss 0.236549, acc 0.953125, prec 0.0646952, recall 0.801835
2017-12-10T03:38:52.659915: step 3040, loss 0.604184, acc 0.953125, prec 0.0647262, recall 0.801926
2017-12-10T03:38:52.927818: step 3041, loss 0.279877, acc 0.921875, prec 0.0647203, recall 0.801926
2017-12-10T03:38:53.199911: step 3042, loss 0.0279199, acc 1, prec 0.0647549, recall 0.802016
2017-12-10T03:38:53.469057: step 3043, loss 0.072971, acc 0.984375, prec 0.0647537, recall 0.802016
2017-12-10T03:38:53.741566: step 3044, loss 0.110088, acc 0.9375, prec 0.0647489, recall 0.802016
2017-12-10T03:38:54.005642: step 3045, loss 0.29522, acc 0.9375, prec 0.0647441, recall 0.802016
2017-12-10T03:38:54.272249: step 3046, loss 0.0489063, acc 0.984375, prec 0.0647429, recall 0.802016
2017-12-10T03:38:54.537077: step 3047, loss 3.12435, acc 0.953125, prec 0.0647578, recall 0.801878
2017-12-10T03:38:54.812373: step 3048, loss 0.0546836, acc 0.984375, prec 0.0647566, recall 0.801878
2017-12-10T03:38:55.074371: step 3049, loss 0.0665546, acc 0.984375, prec 0.0647554, recall 0.801878
2017-12-10T03:38:55.338759: step 3050, loss 0.200685, acc 0.921875, prec 0.0647667, recall 0.801924
2017-12-10T03:38:55.600871: step 3051, loss 0.277087, acc 0.921875, prec 0.0647607, recall 0.801924
2017-12-10T03:38:55.869651: step 3052, loss 0.369469, acc 0.921875, prec 0.0647547, recall 0.801924
2017-12-10T03:38:56.140442: step 3053, loss 0.228739, acc 0.921875, prec 0.0647487, recall 0.801924
2017-12-10T03:38:56.408556: step 3054, loss 0.194, acc 0.921875, prec 0.06476, recall 0.801969
2017-12-10T03:38:56.682238: step 3055, loss 2.78168, acc 0.921875, prec 0.0647553, recall 0.801785
2017-12-10T03:38:56.952262: step 3056, loss 0.308578, acc 0.8125, prec 0.0647582, recall 0.801831
2017-12-10T03:38:57.221601: step 3057, loss 0.76378, acc 0.796875, prec 0.0647599, recall 0.801876
2017-12-10T03:38:57.478198: step 3058, loss 0.26171, acc 0.90625, prec 0.0647873, recall 0.801967
2017-12-10T03:38:57.748109: step 3059, loss 0.452575, acc 0.828125, prec 0.0647741, recall 0.801967
2017-12-10T03:38:58.013962: step 3060, loss 0.877987, acc 0.8125, prec 0.0647598, recall 0.801967
2017-12-10T03:38:58.284345: step 3061, loss 0.413841, acc 0.78125, prec 0.0647603, recall 0.802012
2017-12-10T03:38:58.549267: step 3062, loss 0.650969, acc 0.8125, prec 0.0647632, recall 0.802057
2017-12-10T03:38:58.811169: step 3063, loss 0.529079, acc 0.84375, prec 0.0647685, recall 0.802102
2017-12-10T03:38:59.079962: step 3064, loss 0.938761, acc 0.765625, prec 0.0647506, recall 0.802102
2017-12-10T03:38:59.338544: step 3065, loss 0.213062, acc 0.890625, prec 0.0647767, recall 0.802193
2017-12-10T03:38:59.612037: step 3066, loss 0.742467, acc 0.859375, prec 0.0647832, recall 0.802238
2017-12-10T03:38:59.874685: step 3067, loss 0.540975, acc 0.875, prec 0.0647737, recall 0.802238
2017-12-10T03:39:00.145017: step 3068, loss 0.46555, acc 0.890625, prec 0.0647826, recall 0.802283
2017-12-10T03:39:00.414779: step 3069, loss 0.933823, acc 0.8125, prec 0.0647682, recall 0.802283
2017-12-10T03:39:00.676586: step 3070, loss 0.111305, acc 0.953125, prec 0.0647646, recall 0.802283
2017-12-10T03:39:00.954014: step 3071, loss 0.129422, acc 0.9375, prec 0.0647771, recall 0.802328
2017-12-10T03:39:01.224622: step 3072, loss 0.473533, acc 0.921875, prec 0.0648228, recall 0.802464
2017-12-10T03:39:01.492696: step 3073, loss 0.095068, acc 0.96875, prec 0.0648204, recall 0.802464
2017-12-10T03:39:01.758259: step 3074, loss 2.05126, acc 0.953125, prec 0.0648181, recall 0.80228
2017-12-10T03:39:02.035233: step 3075, loss 0.184531, acc 0.953125, prec 0.0648145, recall 0.80228
2017-12-10T03:39:02.305572: step 3076, loss 0.18459, acc 0.9375, prec 0.0648442, recall 0.802371
2017-12-10T03:39:02.566588: step 3077, loss 0.0981672, acc 0.953125, prec 0.0648406, recall 0.802371
2017-12-10T03:39:02.839368: step 3078, loss 0.342509, acc 0.9375, prec 0.0648875, recall 0.802506
2017-12-10T03:39:03.106076: step 3079, loss 0.260806, acc 0.984375, prec 0.0649379, recall 0.802641
2017-12-10T03:39:03.374005: step 3080, loss 0.283108, acc 0.90625, prec 0.0649308, recall 0.802641
2017-12-10T03:39:03.636406: step 3081, loss 0.0156637, acc 1, prec 0.0649652, recall 0.80273
2017-12-10T03:39:03.900362: step 3082, loss 0.147576, acc 0.9375, prec 0.0649776, recall 0.802775
2017-12-10T03:39:04.162577: step 3083, loss 0.185977, acc 0.953125, prec 0.064974, recall 0.802775
2017-12-10T03:39:04.429562: step 3084, loss 0.158522, acc 0.953125, prec 0.0649877, recall 0.80282
2017-12-10T03:39:04.696724: step 3085, loss 0.0301852, acc 0.984375, prec 0.0650037, recall 0.802865
2017-12-10T03:39:04.964836: step 3086, loss 1.6053, acc 0.90625, prec 0.0649977, recall 0.802682
2017-12-10T03:39:05.233382: step 3087, loss 0.527434, acc 0.90625, prec 0.0649905, recall 0.802682
2017-12-10T03:39:05.509001: step 3088, loss 0.412083, acc 0.921875, prec 0.0649845, recall 0.802682
2017-12-10T03:39:05.775916: step 3089, loss 0.139877, acc 0.921875, prec 0.0649786, recall 0.802682
2017-12-10T03:39:06.051549: step 3090, loss 0.60133, acc 0.90625, prec 0.065023, recall 0.802817
2017-12-10T03:39:06.315630: step 3091, loss 0.235916, acc 0.953125, prec 0.0650538, recall 0.802906
2017-12-10T03:39:06.578271: step 3092, loss 0.191047, acc 0.9375, prec 0.0651006, recall 0.803041
2017-12-10T03:39:06.842500: step 3093, loss 0.585903, acc 0.890625, prec 0.0650922, recall 0.803041
2017-12-10T03:39:07.110338: step 3094, loss 0.298126, acc 0.90625, prec 0.0651022, recall 0.803085
2017-12-10T03:39:07.372148: step 3095, loss 0.225293, acc 0.90625, prec 0.0651123, recall 0.80313
2017-12-10T03:39:07.633885: step 3096, loss 0.165054, acc 0.90625, prec 0.0651051, recall 0.80313
2017-12-10T03:39:07.898361: step 3097, loss 0.177529, acc 0.921875, prec 0.0650991, recall 0.80313
2017-12-10T03:39:08.164416: step 3098, loss 0.200498, acc 0.921875, prec 0.0651103, recall 0.803175
2017-12-10T03:39:08.425814: step 3099, loss 0.305261, acc 0.890625, prec 0.0651363, recall 0.803264
2017-12-10T03:39:08.691171: step 3100, loss 0.300707, acc 0.9375, prec 0.0651487, recall 0.803308
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-3100

2017-12-10T03:39:09.953836: step 3101, loss 0.115126, acc 0.96875, prec 0.0651635, recall 0.803353
2017-12-10T03:39:10.222094: step 3102, loss 0.174339, acc 0.953125, prec 0.065177, recall 0.803398
2017-12-10T03:39:10.491529: step 3103, loss 0.18236, acc 0.9375, prec 0.0651723, recall 0.803398
2017-12-10T03:39:10.761934: step 3104, loss 0.34896, acc 0.90625, prec 0.0651651, recall 0.803398
2017-12-10T03:39:11.023529: step 3105, loss 0.116754, acc 0.953125, prec 0.0651615, recall 0.803398
2017-12-10T03:39:11.285353: step 3106, loss 0.22441, acc 0.9375, prec 0.0651739, recall 0.803442
2017-12-10T03:39:11.549873: step 3107, loss 0.223384, acc 0.953125, prec 0.0651703, recall 0.803442
2017-12-10T03:39:11.826963: step 3108, loss 0.176033, acc 0.953125, prec 0.065201, recall 0.803531
2017-12-10T03:39:12.092364: step 3109, loss 0.137087, acc 0.953125, prec 0.0651974, recall 0.803531
2017-12-10T03:39:12.355314: step 3110, loss 0.378647, acc 0.96875, prec 0.0652294, recall 0.80362
2017-12-10T03:39:12.626720: step 3111, loss 0.38116, acc 1, prec 0.0652809, recall 0.803753
2017-12-10T03:39:12.893179: step 3112, loss 1.93367, acc 0.953125, prec 0.0653471, recall 0.803749
2017-12-10T03:39:13.161902: step 3113, loss 0.236572, acc 0.96875, prec 0.065379, recall 0.803837
2017-12-10T03:39:13.426749: step 3114, loss 0.0729664, acc 0.96875, prec 0.0653766, recall 0.803837
2017-12-10T03:39:13.696433: step 3115, loss 0.0943501, acc 0.96875, prec 0.0653742, recall 0.803837
2017-12-10T03:39:13.963866: step 3116, loss 0.0955595, acc 0.9375, prec 0.0654038, recall 0.803926
2017-12-10T03:39:14.230821: step 3117, loss 0.28746, acc 0.9375, prec 0.0654161, recall 0.80397
2017-12-10T03:39:14.495489: step 3118, loss 0.0622031, acc 0.984375, prec 0.0654321, recall 0.804014
2017-12-10T03:39:14.773062: step 3119, loss 0.260613, acc 0.921875, prec 0.0654261, recall 0.804014
2017-12-10T03:39:15.035262: step 3120, loss 0.330803, acc 0.875, prec 0.0654164, recall 0.804014
2017-12-10T03:39:15.307380: step 3121, loss 0.434098, acc 0.890625, prec 0.0654423, recall 0.804103
2017-12-10T03:39:15.576941: step 3122, loss 0.191065, acc 0.9375, prec 0.0654547, recall 0.804147
2017-12-10T03:39:15.845792: step 3123, loss 0.176748, acc 0.9375, prec 0.065467, recall 0.804191
2017-12-10T03:39:16.107759: step 3124, loss 0.4566, acc 0.921875, prec 0.0654782, recall 0.804235
2017-12-10T03:39:16.379340: step 3125, loss 0.247512, acc 0.921875, prec 0.0654721, recall 0.804235
2017-12-10T03:39:16.650386: step 3126, loss 0.252986, acc 0.9375, prec 0.0654845, recall 0.804279
2017-12-10T03:39:16.911973: step 3127, loss 0.370752, acc 0.96875, prec 0.0654821, recall 0.804279
2017-12-10T03:39:17.186868: step 3128, loss 0.345701, acc 0.921875, prec 0.0655103, recall 0.804367
2017-12-10T03:39:17.462982: step 3129, loss 0.317715, acc 0.890625, prec 0.0655019, recall 0.804367
2017-12-10T03:39:17.727419: step 3130, loss 0.0812952, acc 0.96875, prec 0.0655167, recall 0.804411
2017-12-10T03:39:17.999106: step 3131, loss 0.189934, acc 0.9375, prec 0.065529, recall 0.804455
2017-12-10T03:39:18.266074: step 3132, loss 0.204432, acc 0.921875, prec 0.065523, recall 0.804455
2017-12-10T03:39:18.528239: step 3133, loss 0.299526, acc 0.890625, prec 0.0655146, recall 0.804455
2017-12-10T03:39:18.789361: step 3134, loss 0.234414, acc 0.9375, prec 0.0655098, recall 0.804455
2017-12-10T03:39:19.064910: step 3135, loss 0.225531, acc 0.953125, prec 0.0655233, recall 0.804499
2017-12-10T03:39:19.331314: step 3136, loss 0.0826546, acc 0.953125, prec 0.0655711, recall 0.804631
2017-12-10T03:39:19.592032: step 3137, loss 0.189368, acc 0.953125, prec 0.0655675, recall 0.804631
2017-12-10T03:39:19.856655: step 3138, loss 0.035944, acc 0.984375, prec 0.0655663, recall 0.804631
2017-12-10T03:39:20.130033: step 3139, loss 5.86931, acc 0.96875, prec 0.0655822, recall 0.804494
2017-12-10T03:39:20.394512: step 3140, loss 1.13029, acc 0.96875, prec 0.0656311, recall 0.804626
2017-12-10T03:39:20.658307: step 3141, loss 0.160101, acc 0.953125, prec 0.0656275, recall 0.804626
2017-12-10T03:39:20.934318: step 3142, loss 0.144645, acc 0.953125, prec 0.0656581, recall 0.804714
2017-12-10T03:39:21.204948: step 3143, loss 0.542523, acc 0.90625, prec 0.0656851, recall 0.804801
2017-12-10T03:39:21.467033: step 3144, loss 0.202455, acc 0.953125, prec 0.0656815, recall 0.804801
2017-12-10T03:39:21.729466: step 3145, loss 0.279949, acc 0.9375, prec 0.0656767, recall 0.804801
2017-12-10T03:39:21.994713: step 3146, loss 0.831312, acc 0.859375, prec 0.065683, recall 0.804845
2017-12-10T03:39:22.261651: step 3147, loss 0.493431, acc 0.890625, prec 0.0657088, recall 0.804933
2017-12-10T03:39:22.522595: step 3148, loss 0.194568, acc 0.953125, prec 0.0657223, recall 0.804976
2017-12-10T03:39:22.789227: step 3149, loss 0.491088, acc 0.828125, prec 0.0657091, recall 0.804976
2017-12-10T03:39:23.062031: step 3150, loss 0.264568, acc 0.921875, prec 0.065703, recall 0.804976
2017-12-10T03:39:23.323341: step 3151, loss 0.745648, acc 0.78125, prec 0.0656862, recall 0.804976
2017-12-10T03:39:23.582802: step 3152, loss 0.353827, acc 0.90625, prec 0.0656961, recall 0.80502
2017-12-10T03:39:23.842318: step 3153, loss 0.41342, acc 0.875, prec 0.0657377, recall 0.805151
2017-12-10T03:39:24.109466: step 3154, loss 0.351155, acc 0.875, prec 0.0657281, recall 0.805151
2017-12-10T03:39:24.370644: step 3155, loss 0.375351, acc 0.921875, prec 0.0657392, recall 0.805195
2017-12-10T03:39:24.630196: step 3156, loss 0.407065, acc 0.875, prec 0.0657467, recall 0.805238
2017-12-10T03:39:24.897429: step 3157, loss 0.0828859, acc 0.984375, prec 0.0657625, recall 0.805282
2017-12-10T03:39:25.161113: step 3158, loss 0.460047, acc 0.90625, prec 0.0657895, recall 0.805369
2017-12-10T03:39:25.426593: step 3159, loss 0.337249, acc 0.921875, prec 0.0657835, recall 0.805369
2017-12-10T03:39:25.686575: step 3160, loss 0.405846, acc 0.890625, prec 0.0658092, recall 0.805456
2017-12-10T03:39:25.946722: step 3161, loss 0.116123, acc 0.953125, prec 0.0658056, recall 0.805456
2017-12-10T03:39:26.210733: step 3162, loss 0.0959495, acc 0.96875, prec 0.0658202, recall 0.8055
2017-12-10T03:39:26.483914: step 3163, loss 0.156787, acc 0.953125, prec 0.0658166, recall 0.8055
2017-12-10T03:39:26.761264: step 3164, loss 0.622836, acc 0.90625, prec 0.0658265, recall 0.805543
2017-12-10T03:39:27.027378: step 3165, loss 0.073754, acc 0.984375, prec 0.0658253, recall 0.805543
2017-12-10T03:39:27.293403: step 3166, loss 0.178091, acc 0.953125, prec 0.0658729, recall 0.805673
2017-12-10T03:39:27.556850: step 3167, loss 0.339699, acc 0.984375, prec 0.0658887, recall 0.805717
2017-12-10T03:39:27.822002: step 3168, loss 0.0446623, acc 0.984375, prec 0.0659046, recall 0.80576
2017-12-10T03:39:28.089413: step 3169, loss 0.553726, acc 0.96875, prec 0.0659192, recall 0.805804
2017-12-10T03:39:28.354509: step 3170, loss 0.15601, acc 0.953125, prec 0.0659327, recall 0.805847
2017-12-10T03:39:28.616841: step 3171, loss 0.109965, acc 0.953125, prec 0.0659461, recall 0.80589
2017-12-10T03:39:28.882932: step 3172, loss 0.161245, acc 0.953125, prec 0.0659425, recall 0.80589
2017-12-10T03:39:29.151351: step 3173, loss 0.102996, acc 0.953125, prec 0.0659389, recall 0.80589
2017-12-10T03:39:29.421273: step 3174, loss 1.59066, acc 0.921875, prec 0.0659341, recall 0.80571
2017-12-10T03:39:29.685444: step 3175, loss 0.307659, acc 0.9375, prec 0.0659292, recall 0.80571
2017-12-10T03:39:29.951604: step 3176, loss 0.222566, acc 0.921875, prec 0.0659744, recall 0.80584
2017-12-10T03:39:30.227552: step 3177, loss 0.108618, acc 0.9375, prec 0.0660207, recall 0.80597
2017-12-10T03:39:30.493893: step 3178, loss 0.237238, acc 0.9375, prec 0.0660329, recall 0.806013
2017-12-10T03:39:30.753915: step 3179, loss 0.245933, acc 0.921875, prec 0.0660439, recall 0.806057
2017-12-10T03:39:31.022503: step 3180, loss 0.222755, acc 0.9375, prec 0.0660561, recall 0.8061
2017-12-10T03:39:31.285361: step 3181, loss 0.371895, acc 0.875, prec 0.0660635, recall 0.806143
2017-12-10T03:39:31.551209: step 3182, loss 0.211217, acc 0.9375, prec 0.0660587, recall 0.806143
2017-12-10T03:39:31.818955: step 3183, loss 0.105617, acc 0.953125, prec 0.0660892, recall 0.806229
2017-12-10T03:39:32.084496: step 3184, loss 0.163209, acc 0.9375, prec 0.0661014, recall 0.806272
2017-12-10T03:39:32.349144: step 3185, loss 0.376292, acc 0.90625, prec 0.0660942, recall 0.806272
2017-12-10T03:39:32.612089: step 3186, loss 0.25915, acc 0.953125, prec 0.0661416, recall 0.806401
2017-12-10T03:39:32.879421: step 3187, loss 0.0826719, acc 0.9375, prec 0.0661368, recall 0.806401
2017-12-10T03:39:33.142571: step 3188, loss 0.577454, acc 0.84375, prec 0.0661247, recall 0.806401
2017-12-10T03:39:33.409734: step 3189, loss 0.0648808, acc 0.96875, prec 0.0661223, recall 0.806401
2017-12-10T03:39:33.679265: step 3190, loss 0.377008, acc 0.90625, prec 0.0661151, recall 0.806401
2017-12-10T03:39:33.945106: step 3191, loss 5.76717, acc 0.921875, prec 0.0661443, recall 0.806308
2017-12-10T03:39:34.228479: step 3192, loss 0.213073, acc 0.921875, prec 0.0661383, recall 0.806308
2017-12-10T03:39:34.498173: step 3193, loss 0.361446, acc 0.859375, prec 0.0661274, recall 0.806308
2017-12-10T03:39:34.765698: step 3194, loss 0.409082, acc 0.875, prec 0.0661178, recall 0.806308
2017-12-10T03:39:35.038347: step 3195, loss 0.400614, acc 0.9375, prec 0.066113, recall 0.806308
2017-12-10T03:39:35.300585: step 3196, loss 0.125843, acc 0.953125, prec 0.0661264, recall 0.806351
2017-12-10T03:39:35.569210: step 3197, loss 0.257047, acc 0.9375, prec 0.0661386, recall 0.806394
2017-12-10T03:39:35.829423: step 3198, loss 0.227706, acc 0.921875, prec 0.0661496, recall 0.806437
2017-12-10T03:39:36.088696: step 3199, loss 0.170779, acc 0.90625, prec 0.0661593, recall 0.80648
2017-12-10T03:39:36.353555: step 3200, loss 0.918953, acc 0.875, prec 0.0661837, recall 0.806566
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-3200

2017-12-10T03:39:37.669037: step 3201, loss 0.396011, acc 0.9375, prec 0.0662299, recall 0.806695
2017-12-10T03:39:37.931895: step 3202, loss 0.311261, acc 0.9375, prec 0.066259, recall 0.80678
2017-12-10T03:39:38.194335: step 3203, loss 0.0896276, acc 0.953125, prec 0.0662724, recall 0.806823
2017-12-10T03:39:38.459972: step 3204, loss 0.401545, acc 0.90625, prec 0.0662652, recall 0.806823
2017-12-10T03:39:38.724403: step 3205, loss 0.918897, acc 0.828125, prec 0.0662689, recall 0.806866
2017-12-10T03:39:38.986819: step 3206, loss 0.466821, acc 0.84375, prec 0.0662568, recall 0.806866
2017-12-10T03:39:39.250132: step 3207, loss 0.434625, acc 0.90625, prec 0.0662666, recall 0.806909
2017-12-10T03:39:39.525893: step 3208, loss 0.342701, acc 0.9375, prec 0.0662618, recall 0.806909
2017-12-10T03:39:39.792347: step 3209, loss 0.442406, acc 0.890625, prec 0.0662533, recall 0.806909
2017-12-10T03:39:40.060964: step 3210, loss 0.294603, acc 0.9375, prec 0.0662655, recall 0.806952
2017-12-10T03:39:40.327981: step 3211, loss 0.245974, acc 0.953125, prec 0.0662958, recall 0.807037
2017-12-10T03:39:40.591567: step 3212, loss 0.162679, acc 0.9375, prec 0.066291, recall 0.807037
2017-12-10T03:39:40.853902: step 3213, loss 0.170611, acc 0.921875, prec 0.066302, recall 0.80708
2017-12-10T03:39:41.125704: step 3214, loss 0.63627, acc 0.984375, prec 0.0663856, recall 0.807293
2017-12-10T03:39:41.390881: step 3215, loss 0.18851, acc 0.96875, prec 0.0663832, recall 0.807293
2017-12-10T03:39:41.661432: step 3216, loss 0.0889309, acc 0.9375, prec 0.0663784, recall 0.807293
2017-12-10T03:39:41.934068: step 3217, loss 0.0640141, acc 0.984375, prec 0.0663771, recall 0.807293
2017-12-10T03:39:42.198005: step 3218, loss 0.119372, acc 0.96875, prec 0.0663917, recall 0.807335
2017-12-10T03:39:42.470736: step 3219, loss 0.0839991, acc 0.96875, prec 0.0664062, recall 0.807378
2017-12-10T03:39:42.735468: step 3220, loss 0.146517, acc 0.953125, prec 0.0664026, recall 0.807378
2017-12-10T03:39:43.002608: step 3221, loss 0.316974, acc 0.96875, prec 0.0664172, recall 0.80742
2017-12-10T03:39:43.272249: step 3222, loss 0.836455, acc 0.921875, prec 0.0664451, recall 0.807506
2017-12-10T03:39:43.541518: step 3223, loss 3.106, acc 0.90625, prec 0.066456, recall 0.80737
2017-12-10T03:39:43.813640: step 3224, loss 2.05601, acc 0.9375, prec 0.0664524, recall 0.807192
2017-12-10T03:39:44.086717: step 3225, loss 0.157886, acc 0.953125, prec 0.0664826, recall 0.807277
2017-12-10T03:39:44.358433: step 3226, loss 0.4961, acc 0.890625, prec 0.066542, recall 0.807447
2017-12-10T03:39:44.620626: step 3227, loss 0.384386, acc 0.921875, prec 0.0665868, recall 0.807574
2017-12-10T03:39:44.881380: step 3228, loss 0.476126, acc 0.921875, prec 0.0665977, recall 0.807616
2017-12-10T03:39:45.139583: step 3229, loss 0.882125, acc 0.84375, prec 0.0666025, recall 0.807658
2017-12-10T03:39:45.403119: step 3230, loss 0.73176, acc 0.875, prec 0.0665929, recall 0.807658
2017-12-10T03:39:45.676642: step 3231, loss 0.289485, acc 0.921875, prec 0.0666546, recall 0.807828
2017-12-10T03:39:45.939232: step 3232, loss 0.428179, acc 0.890625, prec 0.066663, recall 0.80787
2017-12-10T03:39:46.206393: step 3233, loss 0.552209, acc 0.890625, prec 0.0666715, recall 0.807912
2017-12-10T03:39:46.466624: step 3234, loss 0.881158, acc 0.796875, prec 0.0666558, recall 0.807912
2017-12-10T03:39:46.729598: step 3235, loss 0.837863, acc 0.875, prec 0.066663, recall 0.807954
2017-12-10T03:39:46.999623: step 3236, loss 0.969587, acc 0.78125, prec 0.06668, recall 0.808039
2017-12-10T03:39:47.261532: step 3237, loss 0.503517, acc 0.875, prec 0.0666703, recall 0.808039
2017-12-10T03:39:47.525143: step 3238, loss 0.386235, acc 0.84375, prec 0.0666751, recall 0.808081
2017-12-10T03:39:47.794054: step 3239, loss 0.311831, acc 0.90625, prec 0.0666679, recall 0.808081
2017-12-10T03:39:48.064660: step 3240, loss 0.382778, acc 0.90625, prec 0.0666606, recall 0.808081
2017-12-10T03:39:48.334636: step 3241, loss 0.23094, acc 0.890625, prec 0.0666691, recall 0.808123
2017-12-10T03:39:48.596812: step 3242, loss 0.249102, acc 0.890625, prec 0.0666775, recall 0.808165
2017-12-10T03:39:48.872302: step 3243, loss 0.150845, acc 0.953125, prec 0.0666739, recall 0.808165
2017-12-10T03:39:49.145049: step 3244, loss 0.387208, acc 0.921875, prec 0.0666848, recall 0.808207
2017-12-10T03:39:49.412007: step 3245, loss 0.175091, acc 0.921875, prec 0.0666787, recall 0.808207
2017-12-10T03:39:49.679126: step 3246, loss 0.214914, acc 0.9375, prec 0.0667246, recall 0.808333
2017-12-10T03:39:49.942964: step 3247, loss 0.257939, acc 0.90625, prec 0.0667342, recall 0.808375
2017-12-10T03:39:50.208400: step 3248, loss 0.066185, acc 0.96875, prec 0.0667487, recall 0.808417
2017-12-10T03:39:50.480433: step 3249, loss 0.158223, acc 0.96875, prec 0.0667632, recall 0.808459
2017-12-10T03:39:50.745690: step 3250, loss 0.950146, acc 0.984375, prec 0.0668464, recall 0.808669
2017-12-10T03:39:51.011223: step 3251, loss 2.56491, acc 0.96875, prec 0.0669127, recall 0.808659
2017-12-10T03:39:51.281285: step 3252, loss 0.0546989, acc 0.984375, prec 0.0669284, recall 0.808701
2017-12-10T03:39:51.544931: step 3253, loss 0.153433, acc 0.9375, prec 0.0669405, recall 0.808743
2017-12-10T03:39:51.817029: step 3254, loss 0.320075, acc 0.953125, prec 0.0669368, recall 0.808743
2017-12-10T03:39:52.080321: step 3255, loss 0.127882, acc 0.953125, prec 0.066967, recall 0.808827
2017-12-10T03:39:52.347002: step 3256, loss 0.245736, acc 0.96875, prec 0.0670152, recall 0.808952
2017-12-10T03:39:52.619900: step 3257, loss 0.159721, acc 0.921875, prec 0.0670091, recall 0.808952
2017-12-10T03:39:52.885255: step 3258, loss 0.160891, acc 0.9375, prec 0.0670042, recall 0.808952
2017-12-10T03:39:53.158174: step 3259, loss 0.289724, acc 0.953125, prec 0.0670512, recall 0.809077
2017-12-10T03:39:53.422815: step 3260, loss 0.134555, acc 0.953125, prec 0.0670645, recall 0.809119
2017-12-10T03:39:53.688974: step 3261, loss 0.374446, acc 0.9375, prec 0.0670765, recall 0.80916
2017-12-10T03:39:53.950279: step 3262, loss 0.669346, acc 0.875, prec 0.0670836, recall 0.809202
2017-12-10T03:39:54.218440: step 3263, loss 0.270493, acc 0.875, prec 0.0670739, recall 0.809202
2017-12-10T03:39:54.486748: step 3264, loss 0.365036, acc 0.890625, prec 0.0670823, recall 0.809244
2017-12-10T03:39:54.758391: step 3265, loss 0.631537, acc 0.828125, prec 0.0671364, recall 0.80941
2017-12-10T03:39:55.028379: step 3266, loss 0.302216, acc 0.890625, prec 0.0671616, recall 0.809493
2017-12-10T03:39:55.299743: step 3267, loss 1.4485, acc 0.875, prec 0.0672025, recall 0.809617
2017-12-10T03:39:55.567921: step 3268, loss 0.529454, acc 0.859375, prec 0.0671915, recall 0.809617
2017-12-10T03:39:55.833709: step 3269, loss 0.181744, acc 0.953125, prec 0.0671879, recall 0.809617
2017-12-10T03:39:56.100709: step 3270, loss 0.297397, acc 0.9375, prec 0.067183, recall 0.809617
2017-12-10T03:39:56.361009: step 3271, loss 0.610514, acc 0.875, prec 0.0671733, recall 0.809617
2017-12-10T03:39:56.635314: step 3272, loss 0.54697, acc 0.859375, prec 0.0671793, recall 0.809658
2017-12-10T03:39:56.912509: step 3273, loss 0.236719, acc 0.90625, prec 0.0671888, recall 0.8097
2017-12-10T03:39:57.175804: step 3274, loss 0.219561, acc 0.921875, prec 0.0671996, recall 0.809741
2017-12-10T03:39:57.444173: step 3275, loss 0.675081, acc 0.90625, prec 0.0671923, recall 0.809741
2017-12-10T03:39:57.706024: step 3276, loss 0.510609, acc 0.90625, prec 0.0672355, recall 0.809865
2017-12-10T03:39:57.973472: step 3277, loss 0.203057, acc 0.9375, prec 0.0672475, recall 0.809907
2017-12-10T03:39:58.242389: step 3278, loss 0.102907, acc 0.953125, prec 0.0672439, recall 0.809907
2017-12-10T03:39:58.514429: step 3279, loss 0.279034, acc 0.90625, prec 0.0672534, recall 0.809948
2017-12-10T03:39:58.781809: step 3280, loss 0.241898, acc 0.921875, prec 0.0672642, recall 0.809989
2017-12-10T03:39:59.051103: step 3281, loss 0.143869, acc 0.953125, prec 0.0672773, recall 0.81003
2017-12-10T03:39:59.314659: step 3282, loss 0.262306, acc 0.953125, prec 0.0672905, recall 0.810072
2017-12-10T03:39:59.588651: step 3283, loss 0.0684819, acc 0.96875, prec 0.0672881, recall 0.810072
2017-12-10T03:39:59.855591: step 3284, loss 0.552569, acc 0.96875, prec 0.0673193, recall 0.810154
2017-12-10T03:40:00.129340: step 3285, loss 0.218854, acc 0.9375, prec 0.0673145, recall 0.810154
2017-12-10T03:40:00.412422: step 3286, loss 0.910788, acc 0.9375, prec 0.0673264, recall 0.810195
2017-12-10T03:40:00.675779: step 3287, loss 0.144811, acc 0.953125, prec 0.0673396, recall 0.810236
2017-12-10T03:40:00.938085: step 3288, loss 0.03316, acc 1, prec 0.0673396, recall 0.810236
2017-12-10T03:40:01.198829: step 3289, loss 0.0908439, acc 0.953125, prec 0.0673359, recall 0.810236
2017-12-10T03:40:01.467095: step 3290, loss 0.0351215, acc 1, prec 0.0673527, recall 0.810278
2017-12-10T03:40:01.732622: step 3291, loss 0.124515, acc 0.9375, prec 0.0673983, recall 0.810401
2017-12-10T03:40:01.995577: step 3292, loss 0.131137, acc 0.953125, prec 0.0673947, recall 0.810401
2017-12-10T03:40:02.259452: step 3293, loss 0.294338, acc 0.96875, prec 0.0674259, recall 0.810483
2017-12-10T03:40:02.524129: step 3294, loss 0.220913, acc 0.953125, prec 0.0674558, recall 0.810565
2017-12-10T03:40:02.794376: step 3295, loss 0.09225, acc 0.953125, prec 0.067469, recall 0.810606
2017-12-10T03:40:03.055920: step 3296, loss 0.304018, acc 0.890625, prec 0.0674605, recall 0.810606
2017-12-10T03:40:03.325114: step 3297, loss 0.147905, acc 0.953125, prec 0.0674568, recall 0.810606
2017-12-10T03:40:03.588506: step 3298, loss 0.155281, acc 0.953125, prec 0.06747, recall 0.810647
2017-12-10T03:40:03.850853: step 3299, loss 0.267003, acc 0.9375, prec 0.0674819, recall 0.810688
2017-12-10T03:40:04.114555: step 3300, loss 0.284386, acc 0.921875, prec 0.0674758, recall 0.810688

Evaluation:
2017-12-10T03:40:11.715966: step 3300, loss 3.0269, acc 0.948391, prec 0.0680793, recall 0.799707

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-3300

2017-12-10T03:40:12.988891: step 3301, loss 0.104726, acc 0.984375, prec 0.0680947, recall 0.799749
2017-12-10T03:40:13.258135: step 3302, loss 0.0455042, acc 1, prec 0.0681113, recall 0.799791
2017-12-10T03:40:13.522821: step 3303, loss 0.202663, acc 0.9375, prec 0.0681064, recall 0.799791
2017-12-10T03:40:13.793141: step 3304, loss 0.0610541, acc 0.984375, prec 0.0681052, recall 0.799791
2017-12-10T03:40:14.057454: step 3305, loss 0.23529, acc 0.9375, prec 0.068117, recall 0.799833
2017-12-10T03:40:14.321681: step 3306, loss 0.4108, acc 0.96875, prec 0.0681145, recall 0.799833
2017-12-10T03:40:14.581757: step 3307, loss 0.0864889, acc 0.984375, prec 0.0681133, recall 0.799833
2017-12-10T03:40:14.862456: step 3308, loss 0.117275, acc 0.984375, prec 0.0681287, recall 0.799874
2017-12-10T03:40:15.133296: step 3309, loss 0.228238, acc 0.90625, prec 0.0681381, recall 0.799916
2017-12-10T03:40:15.396556: step 3310, loss 0.0617686, acc 0.96875, prec 0.0681356, recall 0.799916
2017-12-10T03:40:15.663308: step 3311, loss 0.181491, acc 0.96875, prec 0.0681498, recall 0.799958
2017-12-10T03:40:15.928699: step 3312, loss 0.0903753, acc 1, prec 0.0681664, recall 0.8
2017-12-10T03:40:16.197535: step 3313, loss 0.106021, acc 0.96875, prec 0.068164, recall 0.8
2017-12-10T03:40:16.462890: step 3314, loss 0.138413, acc 0.9375, prec 0.0681757, recall 0.800042
2017-12-10T03:40:16.725932: step 3315, loss 0.0416703, acc 0.984375, prec 0.0681745, recall 0.800042
2017-12-10T03:40:16.990599: step 3316, loss 0.110484, acc 0.953125, prec 0.0681709, recall 0.800042
2017-12-10T03:40:17.256857: step 3317, loss 0.0123498, acc 1, prec 0.0681875, recall 0.800084
2017-12-10T03:40:17.518736: step 3318, loss 0.0906591, acc 0.984375, prec 0.0681863, recall 0.800084
2017-12-10T03:40:17.785074: step 3319, loss 0.0854228, acc 0.9375, prec 0.068198, recall 0.800125
2017-12-10T03:40:18.050687: step 3320, loss 0.149116, acc 0.984375, prec 0.0682134, recall 0.800167
2017-12-10T03:40:18.312608: step 3321, loss 0.0862633, acc 0.96875, prec 0.068211, recall 0.800167
2017-12-10T03:40:18.580792: step 3322, loss 0.0152066, acc 1, prec 0.068211, recall 0.800167
2017-12-10T03:40:18.845283: step 3323, loss 0.342729, acc 0.96875, prec 0.0682251, recall 0.800209
2017-12-10T03:40:19.113193: step 3324, loss 0.076379, acc 0.96875, prec 0.0682227, recall 0.800209
2017-12-10T03:40:19.378935: step 3325, loss 0.190949, acc 0.96875, prec 0.0682535, recall 0.800292
2017-12-10T03:40:19.647253: step 3326, loss 0.0334137, acc 0.984375, prec 0.0682523, recall 0.800292
2017-12-10T03:40:19.909857: step 3327, loss 0.0336193, acc 0.984375, prec 0.0682511, recall 0.800292
2017-12-10T03:40:20.184317: step 3328, loss 4.90792, acc 0.96875, prec 0.0682996, recall 0.80025
2017-12-10T03:40:20.454871: step 3329, loss 0.0905292, acc 0.96875, prec 0.0682972, recall 0.80025
2017-12-10T03:40:20.721413: step 3330, loss 0.236843, acc 0.953125, prec 0.0683267, recall 0.800334
2017-12-10T03:40:20.984833: step 3331, loss 0.135973, acc 0.96875, prec 0.0683243, recall 0.800334
2017-12-10T03:40:21.251690: step 3332, loss 0.281082, acc 0.953125, prec 0.0683373, recall 0.800375
2017-12-10T03:40:21.521773: step 3333, loss 0.0736461, acc 0.96875, prec 0.0683348, recall 0.800375
2017-12-10T03:40:21.800887: step 3334, loss 0.311341, acc 0.90625, prec 0.0683275, recall 0.800375
2017-12-10T03:40:22.064963: step 3335, loss 0.509646, acc 0.9375, prec 0.0683392, recall 0.800417
2017-12-10T03:40:22.334709: step 3336, loss 0.123008, acc 0.90625, prec 0.0683485, recall 0.800459
2017-12-10T03:40:22.607476: step 3337, loss 0.277434, acc 0.921875, prec 0.0683424, recall 0.800459
2017-12-10T03:40:22.878010: step 3338, loss 0.237441, acc 0.953125, prec 0.0683388, recall 0.800459
2017-12-10T03:40:23.145941: step 3339, loss 0.739207, acc 0.796875, prec 0.0683396, recall 0.8005
2017-12-10T03:40:23.415254: step 3340, loss 0.167153, acc 0.90625, prec 0.0683654, recall 0.800583
2017-12-10T03:40:23.683242: step 3341, loss 0.488681, acc 0.953125, prec 0.0683949, recall 0.800667
2017-12-10T03:40:23.949642: step 3342, loss 0.259991, acc 0.9375, prec 0.0683901, recall 0.800667
2017-12-10T03:40:24.213252: step 3343, loss 0.482669, acc 0.859375, prec 0.0683957, recall 0.800708
2017-12-10T03:40:24.478892: step 3344, loss 0.392737, acc 0.859375, prec 0.0684013, recall 0.80075
2017-12-10T03:40:24.747567: step 3345, loss 0.318415, acc 0.90625, prec 0.068394, recall 0.80075
2017-12-10T03:40:25.014508: step 3346, loss 0.448724, acc 0.84375, prec 0.0683984, recall 0.800791
2017-12-10T03:40:25.280261: step 3347, loss 0.18335, acc 0.90625, prec 0.0684077, recall 0.800832
2017-12-10T03:40:25.549575: step 3348, loss 0.247549, acc 0.90625, prec 0.0684004, recall 0.800832
2017-12-10T03:40:25.813706: step 3349, loss 0.274796, acc 0.90625, prec 0.0683931, recall 0.800832
2017-12-10T03:40:26.086968: step 3350, loss 0.0928608, acc 0.953125, prec 0.068406, recall 0.800874
2017-12-10T03:40:26.356618: step 3351, loss 0.296598, acc 0.890625, prec 0.068414, recall 0.800915
2017-12-10T03:40:26.623892: step 3352, loss 0.36141, acc 0.890625, prec 0.0684055, recall 0.800915
2017-12-10T03:40:26.896264: step 3353, loss 0.246655, acc 0.9375, prec 0.0684007, recall 0.800915
2017-12-10T03:40:27.160826: step 3354, loss 0.053737, acc 0.96875, prec 0.0683982, recall 0.800915
2017-12-10T03:40:27.425577: step 3355, loss 0.933396, acc 0.9375, prec 0.068443, recall 0.80104
2017-12-10T03:40:27.701802: step 3356, loss 0.13628, acc 0.96875, prec 0.0684737, recall 0.801122
2017-12-10T03:40:27.972976: step 3357, loss 0.15558, acc 0.9375, prec 0.0684688, recall 0.801122
2017-12-10T03:40:28.236300: step 3358, loss 5.00458, acc 0.953125, prec 0.0684829, recall 0.800997
2017-12-10T03:40:28.505909: step 3359, loss 0.0593982, acc 0.96875, prec 0.068497, recall 0.801038
2017-12-10T03:40:28.771691: step 3360, loss 0.466445, acc 0.9375, prec 0.0685087, recall 0.80108
2017-12-10T03:40:29.044692: step 3361, loss 0.425832, acc 0.875, prec 0.0685155, recall 0.801121
2017-12-10T03:40:29.307729: step 3362, loss 0.217017, acc 0.90625, prec 0.0685082, recall 0.801121
2017-12-10T03:40:29.573250: step 3363, loss 0.545155, acc 0.875, prec 0.0685316, recall 0.801204
2017-12-10T03:40:29.842984: step 3364, loss 0.516909, acc 0.859375, prec 0.0685206, recall 0.801204
2017-12-10T03:40:30.114061: step 3365, loss 0.453457, acc 0.921875, prec 0.0685476, recall 0.801286
2017-12-10T03:40:30.390900: step 3366, loss 0.571536, acc 0.875, prec 0.0685379, recall 0.801286
2017-12-10T03:40:30.660384: step 3367, loss 0.275271, acc 0.875, prec 0.0685612, recall 0.801368
2017-12-10T03:40:30.931324: step 3368, loss 0.471332, acc 0.859375, prec 0.0685833, recall 0.801451
2017-12-10T03:40:31.199297: step 3369, loss 0.438654, acc 0.890625, prec 0.0686078, recall 0.801533
2017-12-10T03:40:31.464140: step 3370, loss 0.483424, acc 0.796875, prec 0.068592, recall 0.801533
2017-12-10T03:40:31.725406: step 3371, loss 0.415472, acc 0.90625, prec 0.0686012, recall 0.801574
2017-12-10T03:40:31.990116: step 3372, loss 0.278683, acc 0.90625, prec 0.0686434, recall 0.801697
2017-12-10T03:40:32.252212: step 3373, loss 0.286531, acc 0.90625, prec 0.0686526, recall 0.801738
2017-12-10T03:40:32.522305: step 3374, loss 0.217808, acc 0.90625, prec 0.0686618, recall 0.801779
2017-12-10T03:40:32.789708: step 3375, loss 0.349875, acc 0.9375, prec 0.06869, recall 0.801861
2017-12-10T03:40:33.053832: step 3376, loss 0.138642, acc 0.953125, prec 0.0687193, recall 0.801943
2017-12-10T03:40:33.318053: step 3377, loss 0.101091, acc 0.96875, prec 0.0687334, recall 0.801984
2017-12-10T03:40:33.583920: step 3378, loss 0.141514, acc 0.953125, prec 0.0687462, recall 0.802025
2017-12-10T03:40:33.849365: step 3379, loss 1.19871, acc 0.96875, prec 0.0687768, recall 0.802107
2017-12-10T03:40:34.115848: step 3380, loss 0.102348, acc 0.96875, prec 0.0687908, recall 0.802148
2017-12-10T03:40:34.382578: step 3381, loss 0.270804, acc 0.90625, prec 0.0687835, recall 0.802148
2017-12-10T03:40:34.658195: step 3382, loss 0.0766808, acc 0.984375, prec 0.0687988, recall 0.802189
2017-12-10T03:40:34.921605: step 3383, loss 0.181037, acc 0.9375, prec 0.0687939, recall 0.802189
2017-12-10T03:40:35.188660: step 3384, loss 0.14439, acc 0.9375, prec 0.0688385, recall 0.802311
2017-12-10T03:40:35.452833: step 3385, loss 0.385292, acc 0.9375, prec 0.0688501, recall 0.802352
2017-12-10T03:40:35.721744: step 3386, loss 0.156822, acc 0.9375, prec 0.0688453, recall 0.802352
2017-12-10T03:40:35.986517: step 3387, loss 0.0533023, acc 0.96875, prec 0.0688593, recall 0.802393
2017-12-10T03:40:36.257908: step 3388, loss 0.102921, acc 0.96875, prec 0.0688569, recall 0.802393
2017-12-10T03:40:36.526329: step 3389, loss 0.0594975, acc 0.984375, prec 0.0688721, recall 0.802433
2017-12-10T03:40:36.798319: step 3390, loss 0.082303, acc 0.984375, prec 0.0688874, recall 0.802474
2017-12-10T03:40:37.062874: step 3391, loss 0.219353, acc 0.953125, prec 0.0688837, recall 0.802474
2017-12-10T03:40:37.327177: step 3392, loss 0.0860516, acc 0.96875, prec 0.0689143, recall 0.802556
2017-12-10T03:40:37.605496: step 3393, loss 0.146605, acc 0.96875, prec 0.0689118, recall 0.802556
2017-12-10T03:40:37.869912: step 3394, loss 0.256917, acc 0.96875, prec 0.0689588, recall 0.802678
2017-12-10T03:40:38.142372: step 3395, loss 0.169139, acc 0.96875, prec 0.0689728, recall 0.802718
2017-12-10T03:40:38.406600: step 3396, loss 0.0339815, acc 1, prec 0.0689893, recall 0.802759
2017-12-10T03:40:38.676174: step 3397, loss 0.0805501, acc 0.984375, prec 0.0690046, recall 0.8028
2017-12-10T03:40:38.944215: step 3398, loss 0.0229514, acc 1, prec 0.069021, recall 0.80284
2017-12-10T03:40:39.213910: step 3399, loss 0.147128, acc 0.96875, prec 0.0690351, recall 0.802881
2017-12-10T03:40:39.486208: step 3400, loss 0.148346, acc 0.9375, prec 0.0690302, recall 0.802881
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-3400

2017-12-10T03:40:40.740899: step 3401, loss 0.260291, acc 0.9375, prec 0.0690418, recall 0.802921
2017-12-10T03:40:41.006301: step 3402, loss 0.0615379, acc 0.984375, prec 0.0690405, recall 0.802921
2017-12-10T03:40:41.270659: step 3403, loss 0.0378511, acc 0.984375, prec 0.0690393, recall 0.802921
2017-12-10T03:40:41.538778: step 3404, loss 0.111709, acc 0.984375, prec 0.0690546, recall 0.802962
2017-12-10T03:40:41.811235: step 3405, loss 0.0484232, acc 0.984375, prec 0.0690533, recall 0.802962
2017-12-10T03:40:42.080559: step 3406, loss 0.0240618, acc 1, prec 0.0690698, recall 0.803002
2017-12-10T03:40:42.339636: step 3407, loss 0.282546, acc 0.953125, prec 0.0690661, recall 0.803002
2017-12-10T03:40:42.612063: step 3408, loss 3.72089, acc 0.9375, prec 0.0691119, recall 0.802959
2017-12-10T03:40:42.883781: step 3409, loss 0.204739, acc 0.9375, prec 0.0691399, recall 0.80304
2017-12-10T03:40:43.160340: step 3410, loss 0.174698, acc 0.953125, prec 0.0691692, recall 0.80312
2017-12-10T03:40:43.427209: step 3411, loss 0.108177, acc 1, prec 0.0691856, recall 0.803161
2017-12-10T03:40:43.693057: step 3412, loss 0.210488, acc 0.9375, prec 0.0691807, recall 0.803161
2017-12-10T03:40:43.959042: step 3413, loss 0.539925, acc 0.9375, prec 0.0692087, recall 0.803242
2017-12-10T03:40:44.230411: step 3414, loss 0.12793, acc 0.9375, prec 0.0692038, recall 0.803242
2017-12-10T03:40:44.495737: step 3415, loss 0.272621, acc 0.96875, prec 0.0692179, recall 0.803282
2017-12-10T03:40:44.771451: step 3416, loss 0.107038, acc 0.9375, prec 0.069213, recall 0.803282
2017-12-10T03:40:45.039931: step 3417, loss 0.919102, acc 0.90625, prec 0.069255, recall 0.803403
2017-12-10T03:40:45.308567: step 3418, loss 0.340243, acc 0.890625, prec 0.0692628, recall 0.803443
2017-12-10T03:40:45.567404: step 3419, loss 0.654109, acc 0.828125, prec 0.0692658, recall 0.803484
2017-12-10T03:40:45.834833: step 3420, loss 0.512124, acc 0.84375, prec 0.0692536, recall 0.803484
2017-12-10T03:40:46.101554: step 3421, loss 0.253823, acc 0.921875, prec 0.0692639, recall 0.803524
2017-12-10T03:40:46.368589: step 3422, loss 0.217583, acc 0.90625, prec 0.069273, recall 0.803564
2017-12-10T03:40:46.634995: step 3423, loss 0.333914, acc 0.890625, prec 0.0692644, recall 0.803564
2017-12-10T03:40:46.900218: step 3424, loss 0.21933, acc 0.875, prec 0.0692875, recall 0.803645
2017-12-10T03:40:47.162879: step 3425, loss 0.278386, acc 0.890625, prec 0.0693282, recall 0.803765
2017-12-10T03:40:47.432359: step 3426, loss 0.317551, acc 0.890625, prec 0.0693525, recall 0.803845
2017-12-10T03:40:47.698792: step 3427, loss 0.376769, acc 0.890625, prec 0.069344, recall 0.803845
2017-12-10T03:40:47.973158: step 3428, loss 0.460087, acc 0.890625, prec 0.0693354, recall 0.803845
2017-12-10T03:40:48.233381: step 3429, loss 0.318408, acc 0.859375, prec 0.0693244, recall 0.803845
2017-12-10T03:40:48.504760: step 3430, loss 0.124916, acc 0.9375, prec 0.0693852, recall 0.804006
2017-12-10T03:40:48.769710: step 3431, loss 0.157221, acc 0.96875, prec 0.0693827, recall 0.804006
2017-12-10T03:40:49.031870: step 3432, loss 0.389752, acc 0.953125, prec 0.0694119, recall 0.804086
2017-12-10T03:40:49.309127: step 3433, loss 0.220315, acc 0.921875, prec 0.0694057, recall 0.804086
2017-12-10T03:40:49.577891: step 3434, loss 0.258367, acc 0.9375, prec 0.0694173, recall 0.804126
2017-12-10T03:40:49.842600: step 3435, loss 0.230666, acc 0.90625, prec 0.0694099, recall 0.804126
2017-12-10T03:40:50.102552: step 3436, loss 0.159981, acc 0.984375, prec 0.0694415, recall 0.804206
2017-12-10T03:40:50.367131: step 3437, loss 0.0755538, acc 0.984375, prec 0.0694567, recall 0.804246
2017-12-10T03:40:50.637154: step 3438, loss 0.0555852, acc 0.984375, prec 0.0694555, recall 0.804246
2017-12-10T03:40:50.903329: step 3439, loss 0.0356023, acc 0.984375, prec 0.0694542, recall 0.804246
2017-12-10T03:40:51.169535: step 3440, loss 0.332434, acc 0.953125, prec 0.0694998, recall 0.804366
2017-12-10T03:40:51.435431: step 3441, loss 0.140328, acc 0.9375, prec 0.0694949, recall 0.804366
2017-12-10T03:40:51.699095: step 3442, loss 1.13801, acc 0.96875, prec 0.0695088, recall 0.804405
2017-12-10T03:40:51.968658: step 3443, loss 0.149147, acc 0.96875, prec 0.0695064, recall 0.804405
2017-12-10T03:40:52.238828: step 3444, loss 0.14426, acc 0.96875, prec 0.0695203, recall 0.804445
2017-12-10T03:40:52.505167: step 3445, loss 0.0445142, acc 0.96875, prec 0.0695179, recall 0.804445
2017-12-10T03:40:52.765309: step 3446, loss 0.0267734, acc 0.984375, prec 0.069533, recall 0.804485
2017-12-10T03:40:53.032007: step 3447, loss 0.0892576, acc 0.984375, prec 0.0695482, recall 0.804525
2017-12-10T03:40:53.297009: step 3448, loss 0.0919674, acc 0.953125, prec 0.0695609, recall 0.804565
2017-12-10T03:40:53.561206: step 3449, loss 0.0924715, acc 0.953125, prec 0.0695572, recall 0.804565
2017-12-10T03:40:53.824811: step 3450, loss 0.0579688, acc 0.984375, prec 0.0696052, recall 0.804684
2017-12-10T03:40:54.094505: step 3451, loss 0.0544869, acc 0.984375, prec 0.0696204, recall 0.804724
2017-12-10T03:40:54.359587: step 3452, loss 0.033736, acc 0.984375, prec 0.0696191, recall 0.804724
2017-12-10T03:40:54.623489: step 3453, loss 0.156032, acc 0.953125, prec 0.0696155, recall 0.804724
2017-12-10T03:40:54.888778: step 3454, loss 0.0456237, acc 0.96875, prec 0.069613, recall 0.804724
2017-12-10T03:40:55.160478: step 3455, loss 0.262505, acc 0.984375, prec 0.0696282, recall 0.804764
2017-12-10T03:40:55.425760: step 3456, loss 0.00819171, acc 1, prec 0.0696282, recall 0.804764
2017-12-10T03:40:55.694402: step 3457, loss 0.0758983, acc 0.984375, prec 0.0696269, recall 0.804764
2017-12-10T03:40:55.965564: step 3458, loss 0.0202836, acc 1, prec 0.0696269, recall 0.804764
2017-12-10T03:40:56.232374: step 3459, loss 0.221461, acc 0.890625, prec 0.0696184, recall 0.804764
2017-12-10T03:40:56.493845: step 3460, loss 0.0898735, acc 0.96875, prec 0.0696323, recall 0.804804
2017-12-10T03:40:56.768014: step 3461, loss 0.240893, acc 0.921875, prec 0.0696425, recall 0.804843
2017-12-10T03:40:57.040575: step 3462, loss 0.033969, acc 0.984375, prec 0.0696577, recall 0.804883
2017-12-10T03:40:57.305345: step 3463, loss 0.0312167, acc 0.984375, prec 0.0696565, recall 0.804883
2017-12-10T03:40:57.575273: step 3464, loss 0.0882445, acc 0.96875, prec 0.0696704, recall 0.804923
2017-12-10T03:40:57.844886: step 3465, loss 0.0219424, acc 1, prec 0.0696704, recall 0.804923
2017-12-10T03:40:58.106879: step 3466, loss 0.0166491, acc 0.984375, prec 0.0696692, recall 0.804923
2017-12-10T03:40:58.371113: step 3467, loss 0.0625786, acc 0.953125, prec 0.0696819, recall 0.804962
2017-12-10T03:40:58.643783: step 3468, loss 0.279436, acc 0.96875, prec 0.0696958, recall 0.805002
2017-12-10T03:40:58.913316: step 3469, loss 0.167921, acc 0.953125, prec 0.0696921, recall 0.805002
2017-12-10T03:40:59.180722: step 3470, loss 0.134898, acc 0.984375, prec 0.0697236, recall 0.805081
2017-12-10T03:40:59.447714: step 3471, loss 0.0491229, acc 0.984375, prec 0.0697224, recall 0.805081
2017-12-10T03:40:59.710099: step 3472, loss 4.53931, acc 0.953125, prec 0.06972, recall 0.804918
2017-12-10T03:40:59.982569: step 3473, loss 6.68249, acc 0.953125, prec 0.0697175, recall 0.804754
2017-12-10T03:41:00.257289: step 3474, loss 0.309332, acc 0.90625, prec 0.0697265, recall 0.804794
2017-12-10T03:41:00.528509: step 3475, loss 0.583629, acc 0.875, prec 0.0697494, recall 0.804873
2017-12-10T03:41:00.792596: step 3476, loss 0.374711, acc 0.84375, prec 0.0697535, recall 0.804913
2017-12-10T03:41:01.059673: step 3477, loss 0.375609, acc 0.828125, prec 0.06974, recall 0.804913
2017-12-10T03:41:01.327290: step 3478, loss 0.655055, acc 0.734375, prec 0.0697355, recall 0.804952
2017-12-10T03:41:01.558061: step 3479, loss 1.24893, acc 0.72549, prec 0.0697674, recall 0.805071
2017-12-10T03:41:01.829497: step 3480, loss 1.0702, acc 0.734375, prec 0.0697793, recall 0.80515
2017-12-10T03:41:02.101333: step 3481, loss 1.43328, acc 0.6875, prec 0.0697548, recall 0.80515
2017-12-10T03:41:02.370135: step 3482, loss 1.31568, acc 0.71875, prec 0.0697327, recall 0.80515
2017-12-10T03:41:02.636138: step 3483, loss 2.13364, acc 0.609375, prec 0.0697021, recall 0.80515
2017-12-10T03:41:02.894458: step 3484, loss 0.544074, acc 0.875, prec 0.0696923, recall 0.80515
2017-12-10T03:41:03.159563: step 3485, loss 1.13094, acc 0.78125, prec 0.0696915, recall 0.80519
2017-12-10T03:41:03.413335: step 3486, loss 1.21875, acc 0.6875, prec 0.0696834, recall 0.805229
2017-12-10T03:41:03.678274: step 3487, loss 0.529755, acc 0.84375, prec 0.0696875, recall 0.805268
2017-12-10T03:41:03.940053: step 3488, loss 0.476704, acc 0.890625, prec 0.0696953, recall 0.805308
2017-12-10T03:41:04.198757: step 3489, loss 0.659242, acc 0.875, prec 0.0697018, recall 0.805347
2017-12-10T03:41:04.471859: step 3490, loss 0.659565, acc 0.859375, prec 0.0696908, recall 0.805347
2017-12-10T03:41:04.740055: step 3491, loss 0.287688, acc 0.890625, prec 0.0696986, recall 0.805387
2017-12-10T03:41:05.004803: step 3492, loss 0.133282, acc 0.921875, prec 0.0696925, recall 0.805387
2017-12-10T03:41:05.267156: step 3493, loss 0.0995244, acc 0.953125, prec 0.0697051, recall 0.805426
2017-12-10T03:41:05.535336: step 3494, loss 0.444141, acc 0.890625, prec 0.0697291, recall 0.805505
2017-12-10T03:41:05.799643: step 3495, loss 0.163099, acc 0.890625, prec 0.0697206, recall 0.805505
2017-12-10T03:41:06.071710: step 3496, loss 0.217175, acc 0.953125, prec 0.0697169, recall 0.805505
2017-12-10T03:41:06.335428: step 3497, loss 0.210584, acc 0.953125, prec 0.0697133, recall 0.805505
2017-12-10T03:41:06.604939: step 3498, loss 0.344849, acc 0.953125, prec 0.0697422, recall 0.805584
2017-12-10T03:41:06.866722: step 3499, loss 0.145571, acc 0.96875, prec 0.0697397, recall 0.805584
2017-12-10T03:41:07.131596: step 3500, loss 0.0964924, acc 0.96875, prec 0.0697536, recall 0.805623
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-3500

2017-12-10T03:41:08.593256: step 3501, loss 0.0808734, acc 0.96875, prec 0.0697674, recall 0.805662
2017-12-10T03:41:08.853245: step 3502, loss 5.09734, acc 0.953125, prec 0.0697813, recall 0.805539
2017-12-10T03:41:09.129501: step 3503, loss 0.0244529, acc 0.984375, prec 0.0697801, recall 0.805539
2017-12-10T03:41:09.393701: step 3504, loss 0.00686461, acc 1, prec 0.0697801, recall 0.805539
2017-12-10T03:41:09.657551: step 3505, loss 0.00976654, acc 1, prec 0.0697964, recall 0.805578
2017-12-10T03:41:09.921065: step 3506, loss 0.0341836, acc 1, prec 0.0697964, recall 0.805578
2017-12-10T03:41:10.189116: step 3507, loss 0.191441, acc 0.953125, prec 0.0697927, recall 0.805578
2017-12-10T03:41:10.458029: step 3508, loss 1.38374, acc 0.953125, prec 0.0698228, recall 0.805494
2017-12-10T03:41:10.721316: step 3509, loss 0.132435, acc 0.953125, prec 0.0698354, recall 0.805533
2017-12-10T03:41:10.985979: step 3510, loss 0.188581, acc 0.9375, prec 0.0698468, recall 0.805572
2017-12-10T03:41:11.252425: step 3511, loss 0.2257, acc 0.9375, prec 0.0698419, recall 0.805572
2017-12-10T03:41:11.516997: step 3512, loss 0.0996389, acc 0.953125, prec 0.0698546, recall 0.805612
2017-12-10T03:41:11.787941: step 3513, loss 0.439321, acc 0.875, prec 0.0698448, recall 0.805612
2017-12-10T03:41:12.050451: step 3514, loss 0.367686, acc 0.90625, prec 0.0698537, recall 0.805651
2017-12-10T03:41:12.312999: step 3515, loss 0.368944, acc 0.890625, prec 0.0698777, recall 0.805729
2017-12-10T03:41:12.574379: step 3516, loss 0.242182, acc 0.921875, prec 0.0698716, recall 0.805729
2017-12-10T03:41:12.837532: step 3517, loss 0.273421, acc 0.875, prec 0.0698618, recall 0.805729
2017-12-10T03:41:13.104228: step 3518, loss 0.0795789, acc 0.96875, prec 0.0698756, recall 0.805768
2017-12-10T03:41:13.373812: step 3519, loss 0.3055, acc 0.921875, prec 0.0699021, recall 0.805847
2017-12-10T03:41:13.638784: step 3520, loss 0.179628, acc 0.953125, prec 0.0698984, recall 0.805847
2017-12-10T03:41:13.905652: step 3521, loss 0.206857, acc 0.953125, prec 0.069911, recall 0.805886
2017-12-10T03:41:14.170204: step 3522, loss 0.108028, acc 0.96875, prec 0.0699086, recall 0.805886
2017-12-10T03:41:14.438415: step 3523, loss 0.106774, acc 0.953125, prec 0.0699049, recall 0.805886
2017-12-10T03:41:14.706085: step 3524, loss 0.179661, acc 0.90625, prec 0.0699138, recall 0.805925
2017-12-10T03:41:14.973548: step 3525, loss 0.206503, acc 0.984375, prec 0.0699288, recall 0.805964
2017-12-10T03:41:15.242906: step 3526, loss 0.0777217, acc 0.9375, prec 0.0699402, recall 0.806003
2017-12-10T03:41:15.517954: step 3527, loss 0.26707, acc 0.984375, prec 0.0699553, recall 0.806042
2017-12-10T03:41:15.792477: step 3528, loss 0.184671, acc 0.953125, prec 0.0699516, recall 0.806042
2017-12-10T03:41:16.058028: step 3529, loss 0.683872, acc 0.9375, prec 0.0699792, recall 0.80612
2017-12-10T03:41:16.335330: step 3530, loss 0.0878799, acc 0.953125, prec 0.0700243, recall 0.806237
2017-12-10T03:41:16.606111: step 3531, loss 0.0939773, acc 0.9375, prec 0.0700194, recall 0.806237
2017-12-10T03:41:16.872771: step 3532, loss 0.155958, acc 0.90625, prec 0.0700121, recall 0.806237
2017-12-10T03:41:17.138743: step 3533, loss 0.206966, acc 0.953125, prec 0.0700084, recall 0.806237
2017-12-10T03:41:17.406682: step 3534, loss 0.0578783, acc 0.953125, prec 0.070021, recall 0.806276
2017-12-10T03:41:17.676670: step 3535, loss 0.187841, acc 0.953125, prec 0.0701148, recall 0.80651
2017-12-10T03:41:17.943919: step 3536, loss 0.339097, acc 0.984375, prec 0.070146, recall 0.806588
2017-12-10T03:41:18.206646: step 3537, loss 0.236611, acc 0.9375, prec 0.0701574, recall 0.806626
2017-12-10T03:41:18.475574: step 3538, loss 0.120309, acc 0.921875, prec 0.0701512, recall 0.806626
2017-12-10T03:41:18.741535: step 3539, loss 0.250979, acc 0.96875, prec 0.0701813, recall 0.806704
2017-12-10T03:41:19.014195: step 3540, loss 0.0448094, acc 0.984375, prec 0.0702125, recall 0.806782
2017-12-10T03:41:19.284982: step 3541, loss 1.62439, acc 0.953125, prec 0.0702738, recall 0.806937
2017-12-10T03:41:19.558705: step 3542, loss 0.0979601, acc 0.96875, prec 0.0703038, recall 0.807014
2017-12-10T03:41:19.826560: step 3543, loss 0.264905, acc 0.890625, prec 0.0702952, recall 0.807014
2017-12-10T03:41:20.097920: step 3544, loss 0.0363485, acc 0.984375, prec 0.070294, recall 0.807014
2017-12-10T03:41:20.367998: step 3545, loss 0.347292, acc 0.90625, prec 0.0702866, recall 0.807014
2017-12-10T03:41:20.629948: step 3546, loss 0.452382, acc 0.84375, prec 0.0702743, recall 0.807014
2017-12-10T03:41:20.899738: step 3547, loss 0.187907, acc 0.984375, prec 0.0703055, recall 0.807091
2017-12-10T03:41:21.166937: step 3548, loss 0.194021, acc 0.890625, prec 0.0703294, recall 0.807169
2017-12-10T03:41:21.430254: step 3549, loss 0.283683, acc 0.90625, prec 0.0703545, recall 0.807246
2017-12-10T03:41:21.694955: step 3550, loss 0.136459, acc 0.9375, prec 0.0703658, recall 0.807284
2017-12-10T03:41:21.962950: step 3551, loss 0.151883, acc 0.9375, prec 0.0703609, recall 0.807284
2017-12-10T03:41:22.227576: step 3552, loss 0.277199, acc 0.921875, prec 0.0703547, recall 0.807284
2017-12-10T03:41:22.498474: step 3553, loss 0.256346, acc 0.875, prec 0.0703449, recall 0.807284
2017-12-10T03:41:22.764116: step 3554, loss 0.194912, acc 0.921875, prec 0.0703388, recall 0.807284
2017-12-10T03:41:23.030721: step 3555, loss 0.270709, acc 0.921875, prec 0.0703489, recall 0.807323
2017-12-10T03:41:23.294911: step 3556, loss 0.657708, acc 0.9375, prec 0.0703764, recall 0.8074
2017-12-10T03:41:23.566949: step 3557, loss 0.13671, acc 0.984375, prec 0.0703751, recall 0.8074
2017-12-10T03:41:23.832866: step 3558, loss 0.0916152, acc 0.96875, prec 0.0703889, recall 0.807438
2017-12-10T03:41:24.105458: step 3559, loss 0.0658677, acc 0.984375, prec 0.0704201, recall 0.807516
2017-12-10T03:41:24.367149: step 3560, loss 0.0467066, acc 0.984375, prec 0.0704513, recall 0.807592
2017-12-10T03:41:24.637118: step 3561, loss 0.137958, acc 0.953125, prec 0.0704476, recall 0.807592
2017-12-10T03:41:24.898908: step 3562, loss 0.0973273, acc 0.953125, prec 0.0704439, recall 0.807592
2017-12-10T03:41:25.173835: step 3563, loss 0.00582761, acc 1, prec 0.0704601, recall 0.807631
2017-12-10T03:41:25.445822: step 3564, loss 0.0507367, acc 0.984375, prec 0.0704589, recall 0.807631
2017-12-10T03:41:25.711392: step 3565, loss 0.366622, acc 0.96875, prec 0.0704726, recall 0.807669
2017-12-10T03:41:25.977566: step 3566, loss 0.329127, acc 0.984375, prec 0.0705038, recall 0.807746
2017-12-10T03:41:26.241335: step 3567, loss 0.182551, acc 0.984375, prec 0.0705025, recall 0.807746
2017-12-10T03:41:26.507689: step 3568, loss 0.215382, acc 0.96875, prec 0.0705325, recall 0.807823
2017-12-10T03:41:26.790973: step 3569, loss 0.153699, acc 0.96875, prec 0.07053, recall 0.807823
2017-12-10T03:41:27.053205: step 3570, loss 0.206364, acc 0.9375, prec 0.0705413, recall 0.807861
2017-12-10T03:41:27.329887: step 3571, loss 0.244858, acc 0.9375, prec 0.0705526, recall 0.807899
2017-12-10T03:41:27.595396: step 3572, loss 0.0943278, acc 0.96875, prec 0.0705501, recall 0.807899
2017-12-10T03:41:27.860935: step 3573, loss 0.381176, acc 0.953125, prec 0.0705464, recall 0.807899
2017-12-10T03:41:28.127971: step 3574, loss 0.304141, acc 0.921875, prec 0.0705403, recall 0.807899
2017-12-10T03:41:28.395823: step 3575, loss 0.0198205, acc 1, prec 0.0705565, recall 0.807938
2017-12-10T03:41:28.660660: step 3576, loss 0.166251, acc 0.96875, prec 0.070554, recall 0.807938
2017-12-10T03:41:28.938891: step 3577, loss 0.0458008, acc 0.984375, prec 0.070569, recall 0.807976
2017-12-10T03:41:29.208323: step 3578, loss 0.00649648, acc 1, prec 0.070569, recall 0.807976
2017-12-10T03:41:29.475193: step 3579, loss 1.73311, acc 0.96875, prec 0.0705989, recall 0.808053
2017-12-10T03:41:29.742989: step 3580, loss 0.105807, acc 0.96875, prec 0.0705964, recall 0.808053
2017-12-10T03:41:30.017872: step 3581, loss 0.189675, acc 0.953125, prec 0.0705927, recall 0.808053
2017-12-10T03:41:30.292076: step 3582, loss 0.239262, acc 0.96875, prec 0.0706227, recall 0.808129
2017-12-10T03:41:30.566928: step 3583, loss 0.263739, acc 0.953125, prec 0.0706351, recall 0.808167
2017-12-10T03:41:30.838861: step 3584, loss 2.69222, acc 0.921875, prec 0.0706302, recall 0.808006
2017-12-10T03:41:31.113478: step 3585, loss 0.333259, acc 0.921875, prec 0.0706403, recall 0.808045
2017-12-10T03:41:31.382290: step 3586, loss 0.294693, acc 0.90625, prec 0.0706491, recall 0.808083
2017-12-10T03:41:31.647846: step 3587, loss 0.452276, acc 0.890625, prec 0.0706566, recall 0.808121
2017-12-10T03:41:31.920472: step 3588, loss 0.485361, acc 0.84375, prec 0.0706767, recall 0.808197
2017-12-10T03:41:32.180195: step 3589, loss 0.738958, acc 0.796875, prec 0.070693, recall 0.808274
2017-12-10T03:41:32.451716: step 3590, loss 0.637167, acc 0.875, prec 0.0706993, recall 0.808312
2017-12-10T03:41:32.714848: step 3591, loss 0.597809, acc 0.90625, prec 0.070692, recall 0.808312
2017-12-10T03:41:32.979205: step 3592, loss 0.673009, acc 0.828125, prec 0.0707108, recall 0.808388
2017-12-10T03:41:33.240151: step 3593, loss 0.547376, acc 0.796875, prec 0.0706948, recall 0.808388
2017-12-10T03:41:33.504749: step 3594, loss 0.547272, acc 0.859375, prec 0.0707322, recall 0.808502
2017-12-10T03:41:33.773045: step 3595, loss 0.785413, acc 0.75, prec 0.0707287, recall 0.80854
2017-12-10T03:41:34.039045: step 3596, loss 0.681569, acc 0.859375, prec 0.0707337, recall 0.808578
2017-12-10T03:41:34.311664: step 3597, loss 0.512006, acc 0.828125, prec 0.0707202, recall 0.808578
2017-12-10T03:41:34.583227: step 3598, loss 0.715848, acc 0.875, prec 0.0707265, recall 0.808616
2017-12-10T03:41:34.851482: step 3599, loss 0.589978, acc 0.859375, prec 0.0707316, recall 0.808654
2017-12-10T03:41:35.117430: step 3600, loss 0.110665, acc 0.953125, prec 0.0707441, recall 0.808692

Evaluation:
2017-12-10T03:41:42.779494: step 3600, loss 2.09079, acc 0.917162, prec 0.0712589, recall 0.803468

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-3600

2017-12-10T03:41:44.075600: step 3601, loss 0.377357, acc 0.890625, prec 0.0712663, recall 0.803506
2017-12-10T03:41:44.337546: step 3602, loss 0.368767, acc 0.9375, prec 0.0712614, recall 0.803506
2017-12-10T03:41:44.604456: step 3603, loss 0.277744, acc 0.90625, prec 0.0712699, recall 0.803544
2017-12-10T03:41:44.869740: step 3604, loss 0.1801, acc 0.9375, prec 0.0712809, recall 0.803582
2017-12-10T03:41:45.133128: step 3605, loss 0.293936, acc 0.921875, prec 0.0712907, recall 0.80362
2017-12-10T03:41:45.395276: step 3606, loss 0.151284, acc 0.9375, prec 0.0713017, recall 0.803657
2017-12-10T03:41:45.656504: step 3607, loss 1.74793, acc 0.9375, prec 0.071298, recall 0.803503
2017-12-10T03:41:45.922638: step 3608, loss 0.0541474, acc 0.984375, prec 0.0713286, recall 0.803578
2017-12-10T03:41:46.186344: step 3609, loss 0.0162839, acc 1, prec 0.0713286, recall 0.803578
2017-12-10T03:41:46.455791: step 3610, loss 0.23632, acc 0.9375, prec 0.0713871, recall 0.803729
2017-12-10T03:41:46.724759: step 3611, loss 0.145631, acc 0.96875, prec 0.0713847, recall 0.803729
2017-12-10T03:41:46.993653: step 3612, loss 0.0742868, acc 0.96875, prec 0.0714139, recall 0.803805
2017-12-10T03:41:47.265312: step 3613, loss 0.138696, acc 0.96875, prec 0.0714591, recall 0.803918
2017-12-10T03:41:47.536248: step 3614, loss 0.157859, acc 0.953125, prec 0.0714554, recall 0.803918
2017-12-10T03:41:47.800186: step 3615, loss 0.165588, acc 0.96875, prec 0.0714847, recall 0.803993
2017-12-10T03:41:48.072443: step 3616, loss 0.107419, acc 0.96875, prec 0.0715298, recall 0.804106
2017-12-10T03:41:48.338033: step 3617, loss 0.240074, acc 0.96875, prec 0.0715432, recall 0.804143
2017-12-10T03:41:48.608316: step 3618, loss 0.142671, acc 0.984375, prec 0.0715895, recall 0.804256
2017-12-10T03:41:48.875477: step 3619, loss 0.0595011, acc 0.96875, prec 0.0716029, recall 0.804294
2017-12-10T03:41:49.140976: step 3620, loss 6.5763, acc 0.953125, prec 0.0716163, recall 0.804177
2017-12-10T03:41:49.416261: step 3621, loss 0.721117, acc 0.9375, prec 0.0716431, recall 0.804252
2017-12-10T03:41:49.681043: step 3622, loss 0.544708, acc 0.9375, prec 0.0716699, recall 0.804327
2017-12-10T03:41:49.950127: step 3623, loss 0.228131, acc 0.9375, prec 0.0716966, recall 0.804402
2017-12-10T03:41:50.207458: step 3624, loss 0.134295, acc 0.953125, prec 0.071693, recall 0.804402
2017-12-10T03:41:50.473864: step 3625, loss 0.153376, acc 0.953125, prec 0.0716893, recall 0.804402
2017-12-10T03:41:50.741020: step 3626, loss 0.557695, acc 0.859375, prec 0.0716783, recall 0.804402
2017-12-10T03:41:51.007766: step 3627, loss 0.623036, acc 0.859375, prec 0.0716673, recall 0.804402
2017-12-10T03:41:51.275820: step 3628, loss 0.632609, acc 0.859375, prec 0.0716721, recall 0.804439
2017-12-10T03:41:51.539720: step 3629, loss 0.338571, acc 0.90625, prec 0.0716806, recall 0.804477
2017-12-10T03:41:51.807655: step 3630, loss 0.657606, acc 0.890625, prec 0.0717037, recall 0.804552
2017-12-10T03:41:52.072696: step 3631, loss 0.860172, acc 0.875, prec 0.0716939, recall 0.804552
2017-12-10T03:41:52.338675: step 3632, loss 0.192548, acc 0.9375, prec 0.0717207, recall 0.804626
2017-12-10T03:41:52.602553: step 3633, loss 0.31088, acc 0.890625, prec 0.0717121, recall 0.804626
2017-12-10T03:41:52.868080: step 3634, loss 0.449414, acc 0.890625, prec 0.0717194, recall 0.804664
2017-12-10T03:41:53.131922: step 3635, loss 0.38313, acc 0.875, prec 0.0717254, recall 0.804701
2017-12-10T03:41:53.402581: step 3636, loss 0.667763, acc 0.796875, prec 0.0717096, recall 0.804701
2017-12-10T03:41:53.670573: step 3637, loss 0.403127, acc 0.890625, prec 0.071701, recall 0.804701
2017-12-10T03:41:53.935069: step 3638, loss 0.370194, acc 0.921875, prec 0.0716949, recall 0.804701
2017-12-10T03:41:54.206136: step 3639, loss 0.104494, acc 0.96875, prec 0.0717083, recall 0.804738
2017-12-10T03:41:54.468588: step 3640, loss 0.387271, acc 0.90625, prec 0.0717167, recall 0.804776
2017-12-10T03:41:54.737343: step 3641, loss 0.596198, acc 0.9375, prec 0.0717435, recall 0.80485
2017-12-10T03:41:55.009555: step 3642, loss 0.351299, acc 0.890625, prec 0.0717349, recall 0.80485
2017-12-10T03:41:55.278976: step 3643, loss 0.247731, acc 0.921875, prec 0.0717604, recall 0.804925
2017-12-10T03:41:55.548749: step 3644, loss 0.226822, acc 0.90625, prec 0.0717531, recall 0.804925
2017-12-10T03:41:56.536008: step 3645, loss 0.217904, acc 0.96875, prec 0.0717822, recall 0.804999
2017-12-10T03:41:56.902689: step 3646, loss 1.63776, acc 0.96875, prec 0.0717968, recall 0.804883
2017-12-10T03:41:57.179944: step 3647, loss 0.314305, acc 0.921875, prec 0.0718065, recall 0.80492
2017-12-10T03:41:57.861215: step 3648, loss 0.0536901, acc 0.984375, prec 0.0718526, recall 0.805031
2017-12-10T03:41:58.583787: step 3649, loss 0.200145, acc 0.953125, prec 0.0718647, recall 0.805069
2017-12-10T03:41:59.287696: step 3650, loss 0.165454, acc 0.9375, prec 0.0718756, recall 0.805106
2017-12-10T03:42:00.371439: step 3651, loss 0.146442, acc 0.984375, prec 0.0718744, recall 0.805106
2017-12-10T03:42:00.769649: step 3652, loss 0.456726, acc 0.90625, prec 0.0718986, recall 0.80518
2017-12-10T03:42:01.049065: step 3653, loss 0.129296, acc 0.953125, prec 0.071895, recall 0.80518
2017-12-10T03:42:01.332197: step 3654, loss 0.18303, acc 0.96875, prec 0.0719083, recall 0.805217
2017-12-10T03:42:01.615228: step 3655, loss 0.143484, acc 0.96875, prec 0.0719059, recall 0.805217
2017-12-10T03:42:01.899007: step 3656, loss 0.168559, acc 0.9375, prec 0.071901, recall 0.805217
2017-12-10T03:42:02.192494: step 3657, loss 0.229954, acc 0.96875, prec 0.0718985, recall 0.805217
2017-12-10T03:42:02.461003: step 3658, loss 0.0830671, acc 0.96875, prec 0.0718961, recall 0.805217
2017-12-10T03:42:02.722994: step 3659, loss 1.80027, acc 0.96875, prec 0.0719106, recall 0.805101
2017-12-10T03:42:02.991635: step 3660, loss 0.843266, acc 0.953125, prec 0.0719228, recall 0.805138
2017-12-10T03:42:03.272213: step 3661, loss 0.106632, acc 0.953125, prec 0.0719349, recall 0.805175
2017-12-10T03:42:03.534424: step 3662, loss 0.30543, acc 0.921875, prec 0.0719288, recall 0.805175
2017-12-10T03:42:03.801273: step 3663, loss 0.422156, acc 0.90625, prec 0.0719687, recall 0.805286
2017-12-10T03:42:04.080806: step 3664, loss 0.0857476, acc 0.953125, prec 0.0719651, recall 0.805286
2017-12-10T03:42:04.353934: step 3665, loss 0.156924, acc 0.9375, prec 0.0719602, recall 0.805286
2017-12-10T03:42:04.622612: step 3666, loss 0.186885, acc 0.90625, prec 0.0719686, recall 0.805323
2017-12-10T03:42:04.890215: step 3667, loss 0.295038, acc 0.890625, prec 0.0719758, recall 0.80536
2017-12-10T03:42:05.157456: step 3668, loss 0.270112, acc 0.90625, prec 0.0719685, recall 0.80536
2017-12-10T03:42:05.432485: step 3669, loss 0.347195, acc 0.90625, prec 0.0719769, recall 0.805397
2017-12-10T03:42:05.701248: step 3670, loss 0.171454, acc 0.9375, prec 0.071972, recall 0.805397
2017-12-10T03:42:05.970223: step 3671, loss 0.34642, acc 0.875, prec 0.0719622, recall 0.805397
2017-12-10T03:42:06.235318: step 3672, loss 0.290591, acc 0.9375, prec 0.0720046, recall 0.805508
2017-12-10T03:42:06.503026: step 3673, loss 0.147888, acc 0.9375, prec 0.0719997, recall 0.805508
2017-12-10T03:42:06.779008: step 3674, loss 0.413203, acc 0.921875, prec 0.0719936, recall 0.805508
2017-12-10T03:42:07.048609: step 3675, loss 0.0139209, acc 1, prec 0.0720094, recall 0.805545
2017-12-10T03:42:07.313203: step 3676, loss 0.44712, acc 0.890625, prec 0.0720481, recall 0.805656
2017-12-10T03:42:07.580514: step 3677, loss 0.733148, acc 0.84375, prec 0.0720516, recall 0.805693
2017-12-10T03:42:07.847216: step 3678, loss 0.114003, acc 0.953125, prec 0.0720479, recall 0.805693
2017-12-10T03:42:08.112722: step 3679, loss 0.0368827, acc 1, prec 0.0720794, recall 0.805766
2017-12-10T03:42:08.373305: step 3680, loss 0.0711494, acc 0.96875, prec 0.0720927, recall 0.805803
2017-12-10T03:42:08.639525: step 3681, loss 0.14149, acc 0.921875, prec 0.0720866, recall 0.805803
2017-12-10T03:42:08.909048: step 3682, loss 0.133877, acc 0.984375, prec 0.0721169, recall 0.805877
2017-12-10T03:42:09.179345: step 3683, loss 0.147657, acc 0.984375, prec 0.0721314, recall 0.805914
2017-12-10T03:42:09.451761: step 3684, loss 0.104548, acc 0.984375, prec 0.0721301, recall 0.805914
2017-12-10T03:42:09.722750: step 3685, loss 0.112981, acc 0.984375, prec 0.0721761, recall 0.806024
2017-12-10T03:42:09.989508: step 3686, loss 0.308098, acc 0.921875, prec 0.07217, recall 0.806024
2017-12-10T03:42:10.265614: step 3687, loss 0.158979, acc 0.953125, prec 0.0721663, recall 0.806024
2017-12-10T03:42:10.530025: step 3688, loss 0.0429065, acc 0.984375, prec 0.0721651, recall 0.806024
2017-12-10T03:42:10.792687: step 3689, loss 0.00449554, acc 1, prec 0.0721651, recall 0.806024
2017-12-10T03:42:11.055302: step 3690, loss 0.103625, acc 0.953125, prec 0.0721929, recall 0.806097
2017-12-10T03:42:11.319288: step 3691, loss 0.127047, acc 0.96875, prec 0.0721905, recall 0.806097
2017-12-10T03:42:11.590694: step 3692, loss 0.0663326, acc 0.96875, prec 0.072188, recall 0.806097
2017-12-10T03:42:11.872014: step 3693, loss 0.00408597, acc 1, prec 0.072188, recall 0.806097
2017-12-10T03:42:12.135549: step 3694, loss 0.0435251, acc 0.984375, prec 0.0721868, recall 0.806097
2017-12-10T03:42:12.411182: step 3695, loss 0.00825007, acc 1, prec 0.0721868, recall 0.806097
2017-12-10T03:42:12.677085: step 3696, loss 0.00751004, acc 1, prec 0.0722025, recall 0.806134
2017-12-10T03:42:12.942452: step 3697, loss 4.42502, acc 0.984375, prec 0.0722025, recall 0.805981
2017-12-10T03:42:13.207314: step 3698, loss 0.00146223, acc 1, prec 0.0722183, recall 0.806018
2017-12-10T03:42:13.478313: step 3699, loss 0.065029, acc 0.96875, prec 0.0722315, recall 0.806055
2017-12-10T03:42:13.746017: step 3700, loss 0.117705, acc 0.96875, prec 0.0722448, recall 0.806092
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-3700

2017-12-10T03:42:15.016516: step 3701, loss 0.074013, acc 0.984375, prec 0.0722593, recall 0.806128
2017-12-10T03:42:15.281233: step 3702, loss 0.122781, acc 0.953125, prec 0.0722557, recall 0.806128
2017-12-10T03:42:15.549016: step 3703, loss 0.0551456, acc 0.96875, prec 0.0722532, recall 0.806128
2017-12-10T03:42:15.815244: step 3704, loss 0.0757749, acc 0.984375, prec 0.0722677, recall 0.806165
2017-12-10T03:42:16.085921: step 3705, loss 0.192674, acc 0.953125, prec 0.0722798, recall 0.806202
2017-12-10T03:42:16.360667: step 3706, loss 0.201483, acc 0.953125, prec 0.0722761, recall 0.806202
2017-12-10T03:42:16.624408: step 3707, loss 0.147941, acc 0.96875, prec 0.0722736, recall 0.806202
2017-12-10T03:42:16.890693: step 3708, loss 0.237585, acc 0.921875, prec 0.072299, recall 0.806275
2017-12-10T03:42:17.158695: step 3709, loss 0.441569, acc 0.90625, prec 0.0723073, recall 0.806311
2017-12-10T03:42:17.427661: step 3710, loss 0.0717539, acc 0.96875, prec 0.0723363, recall 0.806385
2017-12-10T03:42:17.690083: step 3711, loss 0.0837397, acc 0.953125, prec 0.0723326, recall 0.806385
2017-12-10T03:42:17.962537: step 3712, loss 0.181664, acc 0.9375, prec 0.0723277, recall 0.806385
2017-12-10T03:42:18.225471: step 3713, loss 0.121301, acc 0.9375, prec 0.0723386, recall 0.806421
2017-12-10T03:42:18.483941: step 3714, loss 0.132957, acc 0.96875, prec 0.0723832, recall 0.806531
2017-12-10T03:42:18.746801: step 3715, loss 0.127701, acc 0.9375, prec 0.0723941, recall 0.806567
2017-12-10T03:42:19.014414: step 3716, loss 0.330672, acc 0.921875, prec 0.0724036, recall 0.806604
2017-12-10T03:42:19.280115: step 3717, loss 0.133155, acc 0.96875, prec 0.0724169, recall 0.80664
2017-12-10T03:42:19.541524: step 3718, loss 0.0306561, acc 0.984375, prec 0.0724157, recall 0.80664
2017-12-10T03:42:19.804467: step 3719, loss 0.390111, acc 0.953125, prec 0.0724434, recall 0.806713
2017-12-10T03:42:20.066696: step 3720, loss 0.113253, acc 0.96875, prec 0.0724567, recall 0.80675
2017-12-10T03:42:20.335695: step 3721, loss 0.138829, acc 0.96875, prec 0.0724542, recall 0.80675
2017-12-10T03:42:20.604735: step 3722, loss 0.169433, acc 0.953125, prec 0.0724505, recall 0.80675
2017-12-10T03:42:20.867597: step 3723, loss 0.19369, acc 0.953125, prec 0.0724939, recall 0.806859
2017-12-10T03:42:21.134221: step 3724, loss 2.57335, acc 0.9375, prec 0.0724903, recall 0.806707
2017-12-10T03:42:21.407751: step 3725, loss 0.0741847, acc 0.96875, prec 0.0724878, recall 0.806707
2017-12-10T03:42:21.678571: step 3726, loss 0.253178, acc 0.921875, prec 0.0724974, recall 0.806743
2017-12-10T03:42:21.941428: step 3727, loss 0.19615, acc 0.9375, prec 0.0725082, recall 0.80678
2017-12-10T03:42:22.210914: step 3728, loss 0.213542, acc 0.953125, prec 0.0725202, recall 0.806816
2017-12-10T03:42:22.479795: step 3729, loss 0.124908, acc 0.984375, prec 0.072519, recall 0.806816
2017-12-10T03:42:22.749506: step 3730, loss 0.43593, acc 0.875, prec 0.0725091, recall 0.806816
2017-12-10T03:42:23.018759: step 3731, loss 0.623841, acc 0.84375, prec 0.0725126, recall 0.806852
2017-12-10T03:42:23.287471: step 3732, loss 0.329054, acc 0.90625, prec 0.0725052, recall 0.806852
2017-12-10T03:42:23.555534: step 3733, loss 0.517883, acc 0.828125, prec 0.0725074, recall 0.806889
2017-12-10T03:42:23.817804: step 3734, loss 0.130827, acc 0.9375, prec 0.0725496, recall 0.806998
2017-12-10T03:42:24.088661: step 3735, loss 0.170633, acc 0.921875, prec 0.0725591, recall 0.807034
2017-12-10T03:42:24.350894: step 3736, loss 0.155294, acc 0.953125, prec 0.0725554, recall 0.807034
2017-12-10T03:42:24.616505: step 3737, loss 0.158155, acc 0.9375, prec 0.0725662, recall 0.80707
2017-12-10T03:42:24.889633: step 3738, loss 0.158154, acc 0.921875, prec 0.0725757, recall 0.807107
2017-12-10T03:42:25.159987: step 3739, loss 0.299735, acc 0.890625, prec 0.0725671, recall 0.807107
2017-12-10T03:42:25.424765: step 3740, loss 0.334895, acc 0.890625, prec 0.0725586, recall 0.807107
2017-12-10T03:42:25.693023: step 3741, loss 0.34772, acc 0.921875, prec 0.0725681, recall 0.807143
2017-12-10T03:42:25.958920: step 3742, loss 0.165771, acc 0.9375, prec 0.0725789, recall 0.807179
2017-12-10T03:42:26.219893: step 3743, loss 0.142044, acc 0.984375, prec 0.0725776, recall 0.807179
2017-12-10T03:42:26.486482: step 3744, loss 0.119685, acc 0.953125, prec 0.0725896, recall 0.807215
2017-12-10T03:42:26.759945: step 3745, loss 0.330656, acc 0.953125, prec 0.0726016, recall 0.807252
2017-12-10T03:42:27.028914: step 3746, loss 0.379178, acc 0.9375, prec 0.0725967, recall 0.807252
2017-12-10T03:42:27.294121: step 3747, loss 0.0757676, acc 0.953125, prec 0.072593, recall 0.807252
2017-12-10T03:42:27.559112: step 3748, loss 0.098046, acc 0.984375, prec 0.0726075, recall 0.807288
2017-12-10T03:42:27.825566: step 3749, loss 0.1937, acc 0.96875, prec 0.0726207, recall 0.807324
2017-12-10T03:42:28.088837: step 3750, loss 0.0436869, acc 0.984375, prec 0.0726351, recall 0.80736
2017-12-10T03:42:28.353375: step 3751, loss 0.0845561, acc 0.953125, prec 0.0726628, recall 0.807432
2017-12-10T03:42:28.619424: step 3752, loss 0.0464095, acc 0.984375, prec 0.0726616, recall 0.807432
2017-12-10T03:42:28.884291: step 3753, loss 0.120183, acc 0.984375, prec 0.0726916, recall 0.807505
2017-12-10T03:42:29.154514: step 3754, loss 0.372512, acc 0.96875, prec 0.0727049, recall 0.807541
2017-12-10T03:42:29.421521: step 3755, loss 0.0026533, acc 1, prec 0.0727049, recall 0.807541
2017-12-10T03:42:29.686558: step 3756, loss 0.84099, acc 0.96875, prec 0.0727181, recall 0.807577
2017-12-10T03:42:29.958179: step 3757, loss 0.0832591, acc 0.984375, prec 0.0727325, recall 0.807613
2017-12-10T03:42:30.234660: step 3758, loss 0.0288004, acc 0.984375, prec 0.0727469, recall 0.807649
2017-12-10T03:42:30.508818: step 3759, loss 0.19019, acc 0.984375, prec 0.0727457, recall 0.807649
2017-12-10T03:42:30.776685: step 3760, loss 0.258462, acc 0.96875, prec 0.0727589, recall 0.807685
2017-12-10T03:42:31.048590: step 3761, loss 0.0425264, acc 0.984375, prec 0.0727733, recall 0.807721
2017-12-10T03:42:31.319216: step 3762, loss 0.186208, acc 0.9375, prec 0.0727841, recall 0.807757
2017-12-10T03:42:31.581182: step 3763, loss 0.128419, acc 0.953125, prec 0.0728273, recall 0.807865
2017-12-10T03:42:31.843022: step 3764, loss 0.0577924, acc 0.96875, prec 0.0728562, recall 0.807937
2017-12-10T03:42:32.106083: step 3765, loss 0.313523, acc 0.9375, prec 0.0728513, recall 0.807937
2017-12-10T03:42:32.371946: step 3766, loss 3.22644, acc 0.890625, prec 0.0728439, recall 0.807786
2017-12-10T03:42:32.646238: step 3767, loss 0.174944, acc 0.9375, prec 0.0728859, recall 0.807894
2017-12-10T03:42:32.910773: step 3768, loss 0.210483, acc 0.953125, prec 0.0729135, recall 0.807966
2017-12-10T03:42:33.176413: step 3769, loss 0.230087, acc 0.9375, prec 0.0729086, recall 0.807966
2017-12-10T03:42:33.446416: step 3770, loss 0.420384, acc 0.921875, prec 0.0729181, recall 0.808002
2017-12-10T03:42:33.708526: step 3771, loss 0.67295, acc 0.84375, prec 0.0729058, recall 0.808002
2017-12-10T03:42:33.970506: step 3772, loss 0.332565, acc 0.90625, prec 0.0728984, recall 0.808002
2017-12-10T03:42:34.237523: step 3773, loss 0.177052, acc 0.9375, prec 0.0729091, recall 0.808037
2017-12-10T03:42:34.497450: step 3774, loss 0.306507, acc 0.875, prec 0.0728993, recall 0.808037
2017-12-10T03:42:34.771880: step 3775, loss 0.487972, acc 0.875, prec 0.0729207, recall 0.808109
2017-12-10T03:42:35.037852: step 3776, loss 0.220697, acc 0.90625, prec 0.0729446, recall 0.808181
2017-12-10T03:42:35.312789: step 3777, loss 0.33238, acc 0.875, prec 0.0729504, recall 0.808217
2017-12-10T03:42:35.578770: step 3778, loss 0.106759, acc 0.921875, prec 0.0729599, recall 0.808252
2017-12-10T03:42:35.847200: step 3779, loss 0.0837633, acc 0.984375, prec 0.0729586, recall 0.808252
2017-12-10T03:42:36.112034: step 3780, loss 0.146494, acc 0.984375, prec 0.072973, recall 0.808288
2017-12-10T03:42:36.377821: step 3781, loss 0.0808958, acc 0.953125, prec 0.0729693, recall 0.808288
2017-12-10T03:42:36.654125: step 3782, loss 0.0783942, acc 0.96875, prec 0.0729669, recall 0.808288
2017-12-10T03:42:36.927730: step 3783, loss 0.0837812, acc 0.96875, prec 0.0729957, recall 0.80836
2017-12-10T03:42:37.194187: step 3784, loss 0.0174292, acc 0.984375, prec 0.0729944, recall 0.80836
2017-12-10T03:42:37.468351: step 3785, loss 0.103249, acc 0.96875, prec 0.072992, recall 0.80836
2017-12-10T03:42:37.742103: step 3786, loss 0.0143855, acc 1, prec 0.072992, recall 0.80836
2017-12-10T03:42:38.009478: step 3787, loss 0.346303, acc 0.90625, prec 0.0730002, recall 0.808396
2017-12-10T03:42:38.274493: step 3788, loss 0.382739, acc 0.96875, prec 0.0730134, recall 0.808431
2017-12-10T03:42:38.542182: step 3789, loss 0.0288412, acc 0.96875, prec 0.0730265, recall 0.808467
2017-12-10T03:42:38.818782: step 3790, loss 0.0829716, acc 0.984375, prec 0.0730409, recall 0.808503
2017-12-10T03:42:39.085142: step 3791, loss 0.0197576, acc 1, prec 0.0730409, recall 0.808503
2017-12-10T03:42:39.355450: step 3792, loss 0.0562277, acc 0.96875, prec 0.0730541, recall 0.808538
2017-12-10T03:42:39.619486: step 3793, loss 0.0291526, acc 0.984375, prec 0.0730528, recall 0.808538
2017-12-10T03:42:39.881744: step 3794, loss 0.0301844, acc 0.984375, prec 0.0730516, recall 0.808538
2017-12-10T03:42:40.151254: step 3795, loss 0.0977716, acc 0.984375, prec 0.073066, recall 0.808574
2017-12-10T03:42:40.419365: step 3796, loss 0.0476043, acc 0.96875, prec 0.0730791, recall 0.80861
2017-12-10T03:42:40.684643: step 3797, loss 0.0327884, acc 0.984375, prec 0.0730779, recall 0.80861
2017-12-10T03:42:40.959363: step 3798, loss 0.167492, acc 0.96875, prec 0.073091, recall 0.808645
2017-12-10T03:42:41.223383: step 3799, loss 0.0720295, acc 0.953125, prec 0.0730873, recall 0.808645
2017-12-10T03:42:41.491264: step 3800, loss 0.00706448, acc 1, prec 0.073103, recall 0.808681
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-3800

2017-12-10T03:42:42.702905: step 3801, loss 5.71646, acc 0.953125, prec 0.0731486, recall 0.808487
2017-12-10T03:42:42.968056: step 3802, loss 0.604359, acc 0.96875, prec 0.0731617, recall 0.808523
2017-12-10T03:42:43.238327: step 3803, loss 0.282751, acc 0.90625, prec 0.0732011, recall 0.808629
2017-12-10T03:42:43.504242: step 3804, loss 0.316488, acc 0.890625, prec 0.0732237, recall 0.808701
2017-12-10T03:42:43.777244: step 3805, loss 1.07672, acc 0.765625, prec 0.0732052, recall 0.808701
2017-12-10T03:42:44.045782: step 3806, loss 0.62464, acc 0.859375, prec 0.0731941, recall 0.808701
2017-12-10T03:42:44.306911: step 3807, loss 0.483339, acc 0.8125, prec 0.0731793, recall 0.808701
2017-12-10T03:42:44.566760: step 3808, loss 0.77056, acc 0.78125, prec 0.0731777, recall 0.808736
2017-12-10T03:42:44.829703: step 3809, loss 0.64944, acc 0.75, prec 0.0731736, recall 0.808772
2017-12-10T03:42:45.097373: step 3810, loss 0.768431, acc 0.765625, prec 0.0731551, recall 0.808772
2017-12-10T03:42:45.365441: step 3811, loss 0.835868, acc 0.796875, prec 0.0731547, recall 0.808807
2017-12-10T03:42:45.627309: step 3812, loss 1.37268, acc 0.671875, prec 0.0731445, recall 0.808843
2017-12-10T03:42:45.890659: step 3813, loss 0.929477, acc 0.75, prec 0.0731404, recall 0.808878
2017-12-10T03:42:46.155981: step 3814, loss 0.654062, acc 0.828125, prec 0.0731269, recall 0.808878
2017-12-10T03:42:46.427155: step 3815, loss 0.617404, acc 0.8125, prec 0.0731277, recall 0.808914
2017-12-10T03:42:46.704379: step 3816, loss 0.665459, acc 0.796875, prec 0.0731585, recall 0.80902
2017-12-10T03:42:46.975974: step 3817, loss 0.29962, acc 0.875, prec 0.0732108, recall 0.809162
2017-12-10T03:42:47.243620: step 3818, loss 0.232128, acc 0.90625, prec 0.073219, recall 0.809197
2017-12-10T03:42:47.506272: step 3819, loss 0.35946, acc 0.875, prec 0.0732558, recall 0.809303
2017-12-10T03:42:47.774824: step 3820, loss 0.553663, acc 0.90625, prec 0.073264, recall 0.809339
2017-12-10T03:42:48.042169: step 3821, loss 0.171668, acc 0.9375, prec 0.0732746, recall 0.809374
2017-12-10T03:42:48.313133: step 3822, loss 0.293879, acc 0.921875, prec 0.073284, recall 0.809409
2017-12-10T03:42:48.575304: step 3823, loss 0.0719765, acc 0.96875, prec 0.0732816, recall 0.809409
2017-12-10T03:42:48.844233: step 3824, loss 0.398064, acc 0.921875, prec 0.0732754, recall 0.809409
2017-12-10T03:42:49.111956: step 3825, loss 0.0929559, acc 0.953125, prec 0.0732717, recall 0.809409
2017-12-10T03:42:49.382441: step 3826, loss 0.0429163, acc 0.96875, prec 0.0732693, recall 0.809409
2017-12-10T03:42:49.647686: step 3827, loss 0.175926, acc 0.96875, prec 0.0732824, recall 0.809444
2017-12-10T03:42:49.918579: step 3828, loss 0.0626419, acc 0.96875, prec 0.073311, recall 0.809515
2017-12-10T03:42:50.182422: step 3829, loss 3.6728, acc 0.953125, prec 0.0733241, recall 0.8094
2017-12-10T03:42:50.451687: step 3830, loss 0.154439, acc 0.96875, prec 0.0733371, recall 0.809436
2017-12-10T03:42:50.718881: step 3831, loss 0.0766182, acc 0.96875, prec 0.0733347, recall 0.809436
2017-12-10T03:42:50.996131: step 3832, loss 0.228676, acc 0.953125, prec 0.0733465, recall 0.809471
2017-12-10T03:42:51.258637: step 3833, loss 0.118958, acc 0.953125, prec 0.0733428, recall 0.809471
2017-12-10T03:42:51.524602: step 3834, loss 0.115352, acc 0.953125, prec 0.0733547, recall 0.809506
2017-12-10T03:42:51.788677: step 3835, loss 0.614232, acc 0.921875, prec 0.0733796, recall 0.809577
2017-12-10T03:42:52.058328: step 3836, loss 0.355552, acc 0.921875, prec 0.0734045, recall 0.809647
2017-12-10T03:42:52.329752: step 3837, loss 0.305884, acc 0.90625, prec 0.0734282, recall 0.809717
2017-12-10T03:42:52.613873: step 3838, loss 0.415805, acc 0.9375, prec 0.0734232, recall 0.809717
2017-12-10T03:42:52.878184: step 3839, loss 0.102631, acc 0.96875, prec 0.0734518, recall 0.809788
2017-12-10T03:42:53.149767: step 3840, loss 0.355957, acc 0.90625, prec 0.0734755, recall 0.809858
2017-12-10T03:42:53.411113: step 3841, loss 0.283951, acc 0.9375, prec 0.0734705, recall 0.809858
2017-12-10T03:42:53.680865: step 3842, loss 0.372198, acc 0.953125, prec 0.0734824, recall 0.809893
2017-12-10T03:42:53.951950: step 3843, loss 0.195758, acc 0.953125, prec 0.0734787, recall 0.809893
2017-12-10T03:42:54.219318: step 3844, loss 0.362359, acc 0.90625, prec 0.0734713, recall 0.809893
2017-12-10T03:42:54.488903: step 3845, loss 0.945709, acc 0.875, prec 0.073477, recall 0.809928
2017-12-10T03:42:54.754928: step 3846, loss 0.203524, acc 0.90625, prec 0.0734696, recall 0.809928
2017-12-10T03:42:55.021966: step 3847, loss 0.293193, acc 0.921875, prec 0.07351, recall 0.810033
2017-12-10T03:42:55.285033: step 3848, loss 0.270507, acc 0.9375, prec 0.073505, recall 0.810033
2017-12-10T03:42:55.545171: step 3849, loss 0.634274, acc 0.859375, prec 0.073494, recall 0.810033
2017-12-10T03:42:55.808079: step 3850, loss 0.461381, acc 0.890625, prec 0.0734854, recall 0.810033
2017-12-10T03:42:56.078380: step 3851, loss 0.57053, acc 0.921875, prec 0.0734947, recall 0.810068
2017-12-10T03:42:56.343850: step 3852, loss 0.109822, acc 0.953125, prec 0.073491, recall 0.810068
2017-12-10T03:42:56.615261: step 3853, loss 0.0357507, acc 0.984375, prec 0.0734898, recall 0.810068
2017-12-10T03:42:56.894543: step 3854, loss 0.0995355, acc 0.984375, prec 0.0734886, recall 0.810068
2017-12-10T03:42:57.169721: step 3855, loss 0.0537956, acc 0.96875, prec 0.0734861, recall 0.810068
2017-12-10T03:42:57.435357: step 3856, loss 0.176714, acc 0.921875, prec 0.07348, recall 0.810068
2017-12-10T03:42:57.698964: step 3857, loss 0.0776745, acc 0.96875, prec 0.073493, recall 0.810103
2017-12-10T03:42:57.971698: step 3858, loss 1.37525, acc 0.953125, prec 0.0735215, recall 0.810024
2017-12-10T03:42:58.242289: step 3859, loss 0.217186, acc 0.96875, prec 0.0735346, recall 0.810059
2017-12-10T03:42:58.509403: step 3860, loss 5.08491, acc 0.96875, prec 0.0735333, recall 0.80991
2017-12-10T03:42:58.780710: step 3861, loss 0.537975, acc 1, prec 0.0735643, recall 0.80998
2017-12-10T03:42:59.048136: step 3862, loss 0.154801, acc 0.9375, prec 0.0735749, recall 0.810015
2017-12-10T03:42:59.313564: step 3863, loss 0.295188, acc 0.90625, prec 0.0735985, recall 0.810085
2017-12-10T03:42:59.579835: step 3864, loss 0.254488, acc 0.890625, prec 0.0736054, recall 0.81012
2017-12-10T03:42:59.848685: step 3865, loss 0.379272, acc 0.921875, prec 0.0736147, recall 0.810154
2017-12-10T03:43:00.129208: step 3866, loss 0.746003, acc 0.875, prec 0.0736358, recall 0.810224
2017-12-10T03:43:00.406856: step 3867, loss 0.355777, acc 0.875, prec 0.073626, recall 0.810224
2017-12-10T03:43:00.674113: step 3868, loss 0.436635, acc 0.875, prec 0.0736471, recall 0.810294
2017-12-10T03:43:00.937594: step 3869, loss 0.391712, acc 0.859375, prec 0.073636, recall 0.810294
2017-12-10T03:43:01.206258: step 3870, loss 0.770724, acc 0.78125, prec 0.0736343, recall 0.810329
2017-12-10T03:43:01.468536: step 3871, loss 0.671266, acc 0.796875, prec 0.0736492, recall 0.810399
2017-12-10T03:43:01.732138: step 3872, loss 0.466689, acc 0.890625, prec 0.0736716, recall 0.810468
2017-12-10T03:43:01.994538: step 3873, loss 0.337679, acc 0.84375, prec 0.0736593, recall 0.810468
2017-12-10T03:43:02.266825: step 3874, loss 0.800775, acc 0.84375, prec 0.073647, recall 0.810468
2017-12-10T03:43:02.532580: step 3875, loss 0.353801, acc 0.875, prec 0.0736526, recall 0.810503
2017-12-10T03:43:02.795813: step 3876, loss 0.196694, acc 0.953125, prec 0.0736489, recall 0.810503
2017-12-10T03:43:03.067987: step 3877, loss 0.114878, acc 0.96875, prec 0.0736464, recall 0.810503
2017-12-10T03:43:03.335524: step 3878, loss 0.253698, acc 0.9375, prec 0.073657, recall 0.810538
2017-12-10T03:43:03.599631: step 3879, loss 0.173072, acc 0.9375, prec 0.0736984, recall 0.810642
2017-12-10T03:43:03.868878: step 3880, loss 0.0340414, acc 0.984375, prec 0.0736972, recall 0.810642
2017-12-10T03:43:04.130088: step 3881, loss 0.0891912, acc 0.953125, prec 0.0737244, recall 0.810712
2017-12-10T03:43:04.396214: step 3882, loss 0.0692432, acc 0.96875, prec 0.0737374, recall 0.810746
2017-12-10T03:43:04.663008: step 3883, loss 0.285929, acc 0.984375, prec 0.0737516, recall 0.810781
2017-12-10T03:43:04.939480: step 3884, loss 0.320819, acc 0.921875, prec 0.0737609, recall 0.810816
2017-12-10T03:43:05.203643: step 3885, loss 0.0588556, acc 0.984375, prec 0.0737906, recall 0.810885
2017-12-10T03:43:05.472929: step 3886, loss 0.140848, acc 0.96875, prec 0.073819, recall 0.810954
2017-12-10T03:43:05.754152: step 3887, loss 0.107298, acc 0.96875, prec 0.073832, recall 0.810989
2017-12-10T03:43:06.021760: step 3888, loss 0.0535955, acc 0.984375, prec 0.0738462, recall 0.811024
2017-12-10T03:43:06.288007: step 3889, loss 0.0637337, acc 0.984375, prec 0.0738759, recall 0.811093
2017-12-10T03:43:06.553032: step 3890, loss 0.0355517, acc 0.984375, prec 0.0738746, recall 0.811093
2017-12-10T03:43:06.814557: step 3891, loss 0.0504233, acc 0.984375, prec 0.0738734, recall 0.811093
2017-12-10T03:43:07.085681: step 3892, loss 3.78752, acc 0.96875, prec 0.0738876, recall 0.810979
2017-12-10T03:43:07.351947: step 3893, loss 0.676908, acc 0.96875, prec 0.0739006, recall 0.811014
2017-12-10T03:43:07.618702: step 3894, loss 0.457639, acc 0.9375, prec 0.0739111, recall 0.811048
2017-12-10T03:43:07.889749: step 3895, loss 0.141415, acc 0.953125, prec 0.0739228, recall 0.811083
2017-12-10T03:43:08.168212: step 3896, loss 0.209867, acc 0.921875, prec 0.0739167, recall 0.811083
2017-12-10T03:43:08.435653: step 3897, loss 0.520647, acc 0.84375, prec 0.0739044, recall 0.811083
2017-12-10T03:43:08.704469: step 3898, loss 0.481135, acc 0.859375, prec 0.0738933, recall 0.811083
2017-12-10T03:43:08.970375: step 3899, loss 0.83558, acc 0.78125, prec 0.073876, recall 0.811083
2017-12-10T03:43:09.239227: step 3900, loss 0.395198, acc 0.859375, prec 0.0739112, recall 0.811186

Evaluation:
2017-12-10T03:43:16.889742: step 3900, loss 1.79369, acc 0.855741, prec 0.073835, recall 0.809676

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-3900

2017-12-10T03:43:18.099027: step 3901, loss 0.543475, acc 0.828125, prec 0.0738368, recall 0.80971
2017-12-10T03:43:18.366188: step 3902, loss 0.201061, acc 0.90625, prec 0.0738296, recall 0.80971
2017-12-10T03:43:18.630739: step 3903, loss 0.935807, acc 0.875, prec 0.0738201, recall 0.80971
2017-12-10T03:43:18.900233: step 3904, loss 0.932905, acc 0.84375, prec 0.0738231, recall 0.809744
2017-12-10T03:43:19.165877: step 3905, loss 0.263627, acc 0.921875, prec 0.0738321, recall 0.809778
2017-12-10T03:43:19.430646: step 3906, loss 0.586514, acc 0.890625, prec 0.0738388, recall 0.809812
2017-12-10T03:43:19.700860: step 3907, loss 0.771842, acc 0.859375, prec 0.073828, recall 0.809812
2017-12-10T03:43:19.962763: step 3908, loss 0.394569, acc 0.890625, prec 0.0738196, recall 0.809812
2017-12-10T03:43:20.228266: step 3909, loss 0.106363, acc 0.953125, prec 0.0738461, recall 0.809879
2017-12-10T03:43:20.491327: step 3910, loss 0.282575, acc 0.9375, prec 0.0738413, recall 0.809879
2017-12-10T03:43:20.750787: step 3911, loss 0.0493812, acc 0.984375, prec 0.0738551, recall 0.809913
2017-12-10T03:43:21.023928: step 3912, loss 0.0547838, acc 0.984375, prec 0.0738689, recall 0.809947
2017-12-10T03:43:21.289321: step 3913, loss 0.127955, acc 0.96875, prec 0.0738815, recall 0.80998
2017-12-10T03:43:21.560215: step 3914, loss 0.85415, acc 0.953125, prec 0.0739079, recall 0.810048
2017-12-10T03:43:21.827019: step 3915, loss 0.0630668, acc 0.984375, prec 0.0739067, recall 0.810048
2017-12-10T03:43:22.087764: step 3916, loss 0.554947, acc 0.9375, prec 0.0739469, recall 0.810149
2017-12-10T03:43:22.359316: step 3917, loss 1.77409, acc 0.9375, prec 0.0739433, recall 0.810005
2017-12-10T03:43:22.627255: step 3918, loss 0.137562, acc 0.921875, prec 0.0739673, recall 0.810073
2017-12-10T03:43:22.895961: step 3919, loss 4.37401, acc 0.859375, prec 0.0740027, recall 0.81003
2017-12-10T03:43:23.170402: step 3920, loss 0.362744, acc 0.875, prec 0.0739931, recall 0.81003
2017-12-10T03:43:23.434797: step 3921, loss 0.314577, acc 0.890625, prec 0.0740147, recall 0.810097
2017-12-10T03:43:23.708178: step 3922, loss 0.447117, acc 0.84375, prec 0.0740177, recall 0.810131
2017-12-10T03:43:23.974071: step 3923, loss 0.773881, acc 0.8125, prec 0.0740333, recall 0.810198
2017-12-10T03:43:24.248234: step 3924, loss 0.585819, acc 0.84375, prec 0.0740214, recall 0.810198
2017-12-10T03:43:24.518626: step 3925, loss 0.914752, acc 0.765625, prec 0.0740034, recall 0.810198
2017-12-10T03:43:24.782101: step 3926, loss 0.835271, acc 0.78125, prec 0.0740166, recall 0.810265
2017-12-10T03:43:25.045405: step 3927, loss 0.567425, acc 0.8125, prec 0.0740022, recall 0.810265
2017-12-10T03:43:25.310650: step 3928, loss 0.895768, acc 0.6875, prec 0.0739783, recall 0.810265
2017-12-10T03:43:25.574339: step 3929, loss 0.654864, acc 0.859375, prec 0.0739825, recall 0.810299
2017-12-10T03:43:25.834308: step 3930, loss 0.371652, acc 0.890625, prec 0.0739742, recall 0.810299
2017-12-10T03:43:26.099349: step 3931, loss 0.545006, acc 0.875, prec 0.0739945, recall 0.810366
2017-12-10T03:43:26.361224: step 3932, loss 0.508724, acc 0.859375, prec 0.0739838, recall 0.810366
2017-12-10T03:43:26.635927: step 3933, loss 0.627759, acc 0.859375, prec 0.073973, recall 0.810366
2017-12-10T03:43:26.908272: step 3934, loss 0.463591, acc 0.921875, prec 0.073967, recall 0.810366
2017-12-10T03:43:27.177628: step 3935, loss 0.0809912, acc 0.953125, prec 0.0739784, recall 0.8104
2017-12-10T03:43:27.446188: step 3936, loss 0.07019, acc 0.96875, prec 0.073976, recall 0.8104
2017-12-10T03:43:27.706011: step 3937, loss 0.15338, acc 0.953125, prec 0.0740023, recall 0.810467
2017-12-10T03:43:27.975751: step 3938, loss 0.684615, acc 0.921875, prec 0.0739964, recall 0.810467
2017-12-10T03:43:28.239117: step 3939, loss 0.206278, acc 0.921875, prec 0.0739904, recall 0.810467
2017-12-10T03:43:28.503079: step 3940, loss 0.180019, acc 0.953125, prec 0.0740316, recall 0.810567
2017-12-10T03:43:28.767370: step 3941, loss 0.121823, acc 0.984375, prec 0.0740454, recall 0.810601
2017-12-10T03:43:29.033015: step 3942, loss 0.341512, acc 0.9375, prec 0.0740705, recall 0.810668
2017-12-10T03:43:29.302070: step 3943, loss 0.255977, acc 0.953125, prec 0.0740669, recall 0.810668
2017-12-10T03:43:29.569920: step 3944, loss 0.014152, acc 1, prec 0.0740968, recall 0.810734
2017-12-10T03:43:29.833712: step 3945, loss 2.15359, acc 0.953125, prec 0.0740944, recall 0.810591
2017-12-10T03:43:30.118441: step 3946, loss 0.00807482, acc 1, prec 0.0740944, recall 0.810591
2017-12-10T03:43:30.389379: step 3947, loss 0.0929439, acc 0.953125, prec 0.0741207, recall 0.810658
2017-12-10T03:43:30.659470: step 3948, loss 0.0331652, acc 0.984375, prec 0.0741195, recall 0.810658
2017-12-10T03:43:30.926892: step 3949, loss 0.432084, acc 0.984375, prec 0.0741631, recall 0.810758
2017-12-10T03:43:31.194336: step 3950, loss 0.159668, acc 0.921875, prec 0.0741721, recall 0.810792
2017-12-10T03:43:31.460843: step 3951, loss 0.118481, acc 0.953125, prec 0.0741834, recall 0.810825
2017-12-10T03:43:31.730235: step 3952, loss 0.0269291, acc 1, prec 0.0741834, recall 0.810825
2017-12-10T03:43:31.997866: step 3953, loss 0.434969, acc 0.921875, prec 0.0742073, recall 0.810892
2017-12-10T03:43:32.264941: step 3954, loss 0.194844, acc 0.953125, prec 0.0742634, recall 0.811025
2017-12-10T03:43:32.539228: step 3955, loss 0.220283, acc 0.90625, prec 0.0742562, recall 0.811025
2017-12-10T03:43:32.803984: step 3956, loss 0.474014, acc 0.875, prec 0.0742616, recall 0.811058
2017-12-10T03:43:33.077679: step 3957, loss 0.169367, acc 0.921875, prec 0.0742705, recall 0.811092
2017-12-10T03:43:33.342510: step 3958, loss 0.105188, acc 0.96875, prec 0.074283, recall 0.811125
2017-12-10T03:43:33.610556: step 3959, loss 0.290807, acc 0.921875, prec 0.0742771, recall 0.811125
2017-12-10T03:43:33.877497: step 3960, loss 0.262924, acc 0.921875, prec 0.074286, recall 0.811158
2017-12-10T03:43:34.141332: step 3961, loss 0.0302493, acc 0.984375, prec 0.0742997, recall 0.811191
2017-12-10T03:43:34.406751: step 3962, loss 0.021067, acc 1, prec 0.0743146, recall 0.811224
2017-12-10T03:43:34.679053: step 3963, loss 0.0622262, acc 0.984375, prec 0.0743433, recall 0.811291
2017-12-10T03:43:34.950877: step 3964, loss 0.0594159, acc 0.984375, prec 0.074357, recall 0.811324
2017-12-10T03:43:35.211284: step 3965, loss 0.212778, acc 0.96875, prec 0.0743695, recall 0.811357
2017-12-10T03:43:35.488528: step 3966, loss 0.299606, acc 0.921875, prec 0.0743933, recall 0.811424
2017-12-10T03:43:35.755275: step 3967, loss 0.1103, acc 0.984375, prec 0.0744071, recall 0.811457
2017-12-10T03:43:36.019048: step 3968, loss 10.7416, acc 0.9375, prec 0.0744035, recall 0.811314
2017-12-10T03:43:36.287945: step 3969, loss 0.089035, acc 0.984375, prec 0.0744023, recall 0.811314
2017-12-10T03:43:36.550732: step 3970, loss 0.474816, acc 0.921875, prec 0.0743963, recall 0.811314
2017-12-10T03:43:36.816953: step 3971, loss 0.231398, acc 0.953125, prec 0.0743927, recall 0.811314
2017-12-10T03:43:37.079558: step 3972, loss 0.295884, acc 0.875, prec 0.0744129, recall 0.81138
2017-12-10T03:43:37.344704: step 3973, loss 0.260541, acc 0.9375, prec 0.0744081, recall 0.81138
2017-12-10T03:43:37.604092: step 3974, loss 0.204173, acc 0.90625, prec 0.0744456, recall 0.81148
2017-12-10T03:43:37.870545: step 3975, loss 0.181117, acc 0.953125, prec 0.0744719, recall 0.811546
2017-12-10T03:43:38.101630: step 3976, loss 1.31262, acc 0.823529, prec 0.0744611, recall 0.811546
2017-12-10T03:43:38.372208: step 3977, loss 0.591468, acc 0.84375, prec 0.0744491, recall 0.811546
2017-12-10T03:43:38.635851: step 3978, loss 0.455173, acc 0.890625, prec 0.0744556, recall 0.811579
2017-12-10T03:43:38.906538: step 3979, loss 0.231825, acc 0.890625, prec 0.0744472, recall 0.811579
2017-12-10T03:43:39.172336: step 3980, loss 0.385327, acc 0.859375, prec 0.0744513, recall 0.811612
2017-12-10T03:43:39.437399: step 3981, loss 0.228034, acc 0.90625, prec 0.074459, recall 0.811645
2017-12-10T03:43:39.708121: step 3982, loss 0.12967, acc 0.9375, prec 0.0744542, recall 0.811645
2017-12-10T03:43:39.977974: step 3983, loss 0.335743, acc 0.875, prec 0.0744744, recall 0.811711
2017-12-10T03:43:40.238797: step 3984, loss 0.14637, acc 0.984375, prec 0.0744732, recall 0.811711
2017-12-10T03:43:40.501314: step 3985, loss 0.541096, acc 0.921875, prec 0.0744672, recall 0.811711
2017-12-10T03:43:40.767639: step 3986, loss 0.197025, acc 0.90625, prec 0.07446, recall 0.811711
2017-12-10T03:43:41.033171: step 3987, loss 0.0702264, acc 0.984375, prec 0.0744737, recall 0.811744
2017-12-10T03:43:41.304880: step 3988, loss 0.242589, acc 0.90625, prec 0.0744963, recall 0.81181
2017-12-10T03:43:41.572776: step 3989, loss 0.197395, acc 0.96875, prec 0.0745088, recall 0.811843
2017-12-10T03:43:41.851277: step 3990, loss 0.257001, acc 0.90625, prec 0.0745462, recall 0.811942
2017-12-10T03:43:42.117292: step 3991, loss 0.142606, acc 0.9375, prec 0.0745414, recall 0.811942
2017-12-10T03:43:42.380277: step 3992, loss 0.339202, acc 0.9375, prec 0.0745367, recall 0.811942
2017-12-10T03:43:42.648073: step 3993, loss 0.0634608, acc 0.96875, prec 0.074564, recall 0.812008
2017-12-10T03:43:42.911509: step 3994, loss 0.0658993, acc 0.984375, prec 0.0745628, recall 0.812008
2017-12-10T03:43:43.179285: step 3995, loss 0.267162, acc 0.96875, prec 0.0745753, recall 0.812041
2017-12-10T03:43:43.453909: step 3996, loss 2.52752, acc 0.96875, prec 0.0745741, recall 0.811899
2017-12-10T03:43:43.729049: step 3997, loss 0.12693, acc 0.96875, prec 0.0746014, recall 0.811964
2017-12-10T03:43:44.002912: step 3998, loss 0.00443445, acc 1, prec 0.0746163, recall 0.811997
2017-12-10T03:43:44.260776: step 3999, loss 0.276631, acc 0.953125, prec 0.0746425, recall 0.812063
2017-12-10T03:43:44.528319: step 4000, loss 0.121369, acc 0.9375, prec 0.0746525, recall 0.812096
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-4000

2017-12-10T03:43:45.840384: step 4001, loss 0.469236, acc 0.96875, prec 0.0746799, recall 0.812161
2017-12-10T03:43:46.107825: step 4002, loss 0.0767857, acc 0.96875, prec 0.0747072, recall 0.812227
2017-12-10T03:43:46.377265: step 4003, loss 0.0245824, acc 1, prec 0.0747072, recall 0.812227
2017-12-10T03:43:46.649495: step 4004, loss 0.223044, acc 0.96875, prec 0.0747345, recall 0.812293
2017-12-10T03:43:46.915756: step 4005, loss 0.439351, acc 0.90625, prec 0.0747422, recall 0.812325
2017-12-10T03:43:47.182474: step 4006, loss 0.100174, acc 0.96875, prec 0.0747546, recall 0.812358
2017-12-10T03:43:47.446650: step 4007, loss 0.28851, acc 0.9375, prec 0.0747498, recall 0.812358
2017-12-10T03:43:47.714303: step 4008, loss 0.417342, acc 0.890625, prec 0.0747563, recall 0.812391
2017-12-10T03:43:47.985379: step 4009, loss 0.34476, acc 0.890625, prec 0.0747776, recall 0.812456
2017-12-10T03:43:48.253665: step 4010, loss 0.44403, acc 0.890625, prec 0.0747692, recall 0.812456
2017-12-10T03:43:48.517801: step 4011, loss 0.192805, acc 0.90625, prec 0.074762, recall 0.812456
2017-12-10T03:43:48.779696: step 4012, loss 0.200374, acc 0.9375, prec 0.0747572, recall 0.812456
2017-12-10T03:43:49.046745: step 4013, loss 0.147733, acc 0.9375, prec 0.0747821, recall 0.812522
2017-12-10T03:43:49.316202: step 4014, loss 0.218215, acc 0.953125, prec 0.0747785, recall 0.812522
2017-12-10T03:43:49.586075: step 4015, loss 0.122313, acc 0.953125, prec 0.0747749, recall 0.812522
2017-12-10T03:43:49.859789: step 4016, loss 0.109044, acc 0.96875, prec 0.0747874, recall 0.812554
2017-12-10T03:43:50.125521: step 4017, loss 0.0133697, acc 1, prec 0.0747874, recall 0.812554
2017-12-10T03:43:50.395955: step 4018, loss 4.34511, acc 0.953125, prec 0.0747849, recall 0.812413
2017-12-10T03:43:50.664002: step 4019, loss 0.0925127, acc 0.96875, prec 0.0747826, recall 0.812413
2017-12-10T03:43:50.936368: step 4020, loss 0.246621, acc 0.921875, prec 0.0747914, recall 0.812446
2017-12-10T03:43:51.201105: step 4021, loss 0.263397, acc 0.921875, prec 0.0748151, recall 0.812511
2017-12-10T03:43:51.465978: step 4022, loss 0.472086, acc 0.84375, prec 0.0748179, recall 0.812544
2017-12-10T03:43:51.728922: step 4023, loss 0.527494, acc 0.90625, prec 0.0748404, recall 0.812609
2017-12-10T03:43:52.002104: step 4024, loss 0.253217, acc 0.921875, prec 0.0748492, recall 0.812642
2017-12-10T03:43:52.274907: step 4025, loss 0.152313, acc 0.953125, prec 0.0748456, recall 0.812642
2017-12-10T03:43:52.541467: step 4026, loss 0.436673, acc 0.8125, prec 0.0748312, recall 0.812642
2017-12-10T03:43:52.809737: step 4027, loss 0.208804, acc 0.921875, prec 0.0748846, recall 0.812772
2017-12-10T03:43:53.086423: step 4028, loss 0.22261, acc 0.90625, prec 0.074907, recall 0.812837
2017-12-10T03:43:53.350040: step 4029, loss 0.250798, acc 0.9375, prec 0.0749022, recall 0.812837
2017-12-10T03:43:53.617845: step 4030, loss 0.131164, acc 0.9375, prec 0.0748974, recall 0.812837
2017-12-10T03:43:53.884447: step 4031, loss 0.136126, acc 0.953125, prec 0.0748938, recall 0.812837
2017-12-10T03:43:54.145518: step 4032, loss 0.265473, acc 0.9375, prec 0.0749335, recall 0.812935
2017-12-10T03:43:54.411946: step 4033, loss 0.55809, acc 0.921875, prec 0.0749275, recall 0.812935
2017-12-10T03:43:54.686862: step 4034, loss 0.325166, acc 0.90625, prec 0.0749351, recall 0.812967
2017-12-10T03:43:54.955836: step 4035, loss 0.257813, acc 0.90625, prec 0.0749427, recall 0.813
2017-12-10T03:43:55.226345: step 4036, loss 0.12729, acc 0.96875, prec 0.07497, recall 0.813065
2017-12-10T03:43:55.499103: step 4037, loss 0.0391449, acc 1, prec 0.0749996, recall 0.81313
2017-12-10T03:43:55.763825: step 4038, loss 0.25132, acc 0.9375, prec 0.0750096, recall 0.813162
2017-12-10T03:43:56.031762: step 4039, loss 0.211979, acc 0.921875, prec 0.0750332, recall 0.813227
2017-12-10T03:43:56.296917: step 4040, loss 0.389168, acc 0.96875, prec 0.0750605, recall 0.813292
2017-12-10T03:43:56.568099: step 4041, loss 0.11886, acc 0.9375, prec 0.0750705, recall 0.813324
2017-12-10T03:43:56.841540: step 4042, loss 0.0558148, acc 0.96875, prec 0.0750829, recall 0.813356
2017-12-10T03:43:57.102947: step 4043, loss 0.117844, acc 0.921875, prec 0.0751065, recall 0.813421
2017-12-10T03:43:57.366216: step 4044, loss 0.110274, acc 0.96875, prec 0.0751337, recall 0.813486
2017-12-10T03:43:57.633930: step 4045, loss 0.0377321, acc 1, prec 0.0751633, recall 0.813551
2017-12-10T03:43:57.905107: step 4046, loss 0.0411476, acc 1, prec 0.0751929, recall 0.813615
2017-12-10T03:43:58.171427: step 4047, loss 1.0836, acc 1, prec 0.0752077, recall 0.813647
2017-12-10T03:43:58.438025: step 4048, loss 0.0174207, acc 1, prec 0.0752521, recall 0.813744
2017-12-10T03:43:58.702363: step 4049, loss 0.0456985, acc 0.984375, prec 0.0752509, recall 0.813744
2017-12-10T03:43:58.970240: step 4050, loss 0.11132, acc 0.96875, prec 0.0752633, recall 0.813776
2017-12-10T03:43:59.236614: step 4051, loss 0.103147, acc 0.953125, prec 0.0753041, recall 0.813873
2017-12-10T03:43:59.504855: step 4052, loss 0.0202187, acc 0.984375, prec 0.0753177, recall 0.813905
2017-12-10T03:43:59.772127: step 4053, loss 0.714508, acc 0.953125, prec 0.0753289, recall 0.813937
2017-12-10T03:44:00.041299: step 4054, loss 0.39851, acc 0.875, prec 0.0753192, recall 0.813937
2017-12-10T03:44:00.319014: step 4055, loss 0.438485, acc 0.90625, prec 0.0753564, recall 0.814034
2017-12-10T03:44:00.595305: step 4056, loss 0.0554828, acc 0.984375, prec 0.07537, recall 0.814066
2017-12-10T03:44:00.867519: step 4057, loss 0.0353274, acc 0.984375, prec 0.0753983, recall 0.81413
2017-12-10T03:44:01.136495: step 4058, loss 0.249482, acc 0.953125, prec 0.0753947, recall 0.81413
2017-12-10T03:44:01.404914: step 4059, loss 0.416061, acc 0.953125, prec 0.0753911, recall 0.81413
2017-12-10T03:44:01.671280: step 4060, loss 0.217495, acc 0.890625, prec 0.0754123, recall 0.814194
2017-12-10T03:44:01.931633: step 4061, loss 0.0866148, acc 0.96875, prec 0.0754098, recall 0.814194
2017-12-10T03:44:02.200354: step 4062, loss 0.0918267, acc 0.953125, prec 0.0754358, recall 0.814259
2017-12-10T03:44:02.464647: step 4063, loss 0.193202, acc 0.921875, prec 0.0754593, recall 0.814323
2017-12-10T03:44:02.725996: step 4064, loss 0.103976, acc 0.96875, prec 0.0754717, recall 0.814355
2017-12-10T03:44:02.996868: step 4065, loss 0.262407, acc 0.90625, prec 0.0754792, recall 0.814387
2017-12-10T03:44:03.260272: step 4066, loss 0.0994218, acc 0.9375, prec 0.0754744, recall 0.814387
2017-12-10T03:44:03.520364: step 4067, loss 0.543321, acc 0.921875, prec 0.0754832, recall 0.814419
2017-12-10T03:44:03.795347: step 4068, loss 0.142958, acc 0.96875, prec 0.0754955, recall 0.814451
2017-12-10T03:44:04.058131: step 4069, loss 0.0718464, acc 0.96875, prec 0.0755079, recall 0.814483
2017-12-10T03:44:04.319510: step 4070, loss 0.629621, acc 0.96875, prec 0.0755202, recall 0.814515
2017-12-10T03:44:04.586246: step 4071, loss 0.0704271, acc 0.96875, prec 0.0755326, recall 0.814547
2017-12-10T03:44:04.850382: step 4072, loss 0.249574, acc 0.921875, prec 0.0755413, recall 0.814579
2017-12-10T03:44:05.122140: step 4073, loss 0.0690227, acc 0.96875, prec 0.0755537, recall 0.814611
2017-12-10T03:44:05.387467: step 4074, loss 0.0123109, acc 1, prec 0.0755537, recall 0.814611
2017-12-10T03:44:05.651601: step 4075, loss 0.122854, acc 0.96875, prec 0.0755661, recall 0.814643
2017-12-10T03:44:05.925594: step 4076, loss 2.32825, acc 0.953125, prec 0.0755784, recall 0.814534
2017-12-10T03:44:06.188832: step 4077, loss 0.178344, acc 0.953125, prec 0.0755748, recall 0.814534
2017-12-10T03:44:06.451875: step 4078, loss 0.23394, acc 0.953125, prec 0.0755712, recall 0.814534
2017-12-10T03:44:06.720215: step 4079, loss 0.166772, acc 0.953125, prec 0.0755823, recall 0.814566
2017-12-10T03:44:06.995222: step 4080, loss 0.218008, acc 0.921875, prec 0.0755911, recall 0.814598
2017-12-10T03:44:07.268729: step 4081, loss 0.154047, acc 0.96875, prec 0.0756034, recall 0.81463
2017-12-10T03:44:07.534908: step 4082, loss 0.243807, acc 0.953125, prec 0.0756441, recall 0.814726
2017-12-10T03:44:07.802489: step 4083, loss 0.181342, acc 0.9375, prec 0.0756688, recall 0.814789
2017-12-10T03:44:08.072245: step 4084, loss 0.290825, acc 0.9375, prec 0.0756787, recall 0.814821
2017-12-10T03:44:08.333911: step 4085, loss 0.0921896, acc 0.984375, prec 0.0756923, recall 0.814853
2017-12-10T03:44:08.599485: step 4086, loss 0.151185, acc 0.921875, prec 0.0756862, recall 0.814853
2017-12-10T03:44:08.864514: step 4087, loss 0.123366, acc 0.953125, prec 0.0756973, recall 0.814885
2017-12-10T03:44:09.126484: step 4088, loss 0.271761, acc 0.921875, prec 0.075706, recall 0.814917
2017-12-10T03:44:09.392783: step 4089, loss 0.312989, acc 0.90625, prec 0.0757283, recall 0.81498
2017-12-10T03:44:09.655955: step 4090, loss 0.255675, acc 0.953125, prec 0.0757542, recall 0.815044
2017-12-10T03:44:09.919626: step 4091, loss 0.332046, acc 0.96875, prec 0.0757813, recall 0.815107
2017-12-10T03:44:10.187392: step 4092, loss 0.147393, acc 0.953125, prec 0.0757924, recall 0.815139
2017-12-10T03:44:10.454821: step 4093, loss 0.121551, acc 0.921875, prec 0.0758158, recall 0.815202
2017-12-10T03:44:10.715431: step 4094, loss 0.108975, acc 0.953125, prec 0.0758122, recall 0.815202
2017-12-10T03:44:10.980308: step 4095, loss 0.234335, acc 0.921875, prec 0.0758209, recall 0.815234
2017-12-10T03:44:11.255769: step 4096, loss 0.102391, acc 0.984375, prec 0.0758345, recall 0.815266
2017-12-10T03:44:11.516407: step 4097, loss 0.187529, acc 0.953125, prec 0.0758456, recall 0.815298
2017-12-10T03:44:11.792644: step 4098, loss 0.16405, acc 0.96875, prec 0.0758431, recall 0.815298
2017-12-10T03:44:12.065464: step 4099, loss 0.0651396, acc 0.953125, prec 0.0758395, recall 0.815298
2017-12-10T03:44:12.334459: step 4100, loss 0.145721, acc 0.9375, prec 0.0758789, recall 0.815393
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-4100

2017-12-10T03:44:13.657957: step 4101, loss 0.201615, acc 0.9375, prec 0.0758888, recall 0.815424
2017-12-10T03:44:13.925271: step 4102, loss 0.121385, acc 0.953125, prec 0.0758852, recall 0.815424
2017-12-10T03:44:14.195013: step 4103, loss 0.110446, acc 0.96875, prec 0.0758827, recall 0.815424
2017-12-10T03:44:14.464078: step 4104, loss 0.851503, acc 0.921875, prec 0.0759062, recall 0.815487
2017-12-10T03:44:14.728748: step 4105, loss 0.255421, acc 0.953125, prec 0.0759173, recall 0.815519
2017-12-10T03:44:14.995967: step 4106, loss 0.0492343, acc 0.984375, prec 0.0759308, recall 0.815551
2017-12-10T03:44:15.258666: step 4107, loss 0.00995761, acc 1, prec 0.0759455, recall 0.815582
2017-12-10T03:44:15.522367: step 4108, loss 0.182186, acc 0.96875, prec 0.0759431, recall 0.815582
2017-12-10T03:44:15.784597: step 4109, loss 0.208944, acc 0.953125, prec 0.0759984, recall 0.815708
2017-12-10T03:44:16.047833: step 4110, loss 0.198488, acc 0.953125, prec 0.0760095, recall 0.81574
2017-12-10T03:44:16.313306: step 4111, loss 0.261268, acc 0.953125, prec 0.0760206, recall 0.815771
2017-12-10T03:44:16.580600: step 4112, loss 0.447097, acc 0.984375, prec 0.0760488, recall 0.815834
2017-12-10T03:44:16.851602: step 4113, loss 0.0539023, acc 0.984375, prec 0.0760476, recall 0.815834
2017-12-10T03:44:17.122076: step 4114, loss 1.15511, acc 0.953125, prec 0.0760734, recall 0.815897
2017-12-10T03:44:17.385187: step 4115, loss 0.0979302, acc 0.9375, prec 0.076098, recall 0.81596
2017-12-10T03:44:17.656592: step 4116, loss 0.155054, acc 0.921875, prec 0.076092, recall 0.81596
2017-12-10T03:44:17.923654: step 4117, loss 0.0284739, acc 0.984375, prec 0.0761055, recall 0.815992
2017-12-10T03:44:18.190407: step 4118, loss 0.308928, acc 0.9375, prec 0.0761006, recall 0.815992
2017-12-10T03:44:18.456728: step 4119, loss 0.138998, acc 0.984375, prec 0.0761436, recall 0.816086
2017-12-10T03:44:18.719224: step 4120, loss 0.0402204, acc 0.96875, prec 0.0761559, recall 0.816117
2017-12-10T03:44:18.986385: step 4121, loss 0.114372, acc 0.953125, prec 0.0761522, recall 0.816117
2017-12-10T03:44:19.266573: step 4122, loss 0.00607015, acc 1, prec 0.076167, recall 0.816149
2017-12-10T03:44:19.529903: step 4123, loss 0.146706, acc 0.953125, prec 0.076178, recall 0.81618
2017-12-10T03:44:19.796978: step 4124, loss 0.210343, acc 0.921875, prec 0.0762014, recall 0.816243
2017-12-10T03:44:20.061288: step 4125, loss 0.237922, acc 0.953125, prec 0.0762125, recall 0.816274
2017-12-10T03:44:20.325083: step 4126, loss 0.0442196, acc 0.984375, prec 0.076226, recall 0.816306
2017-12-10T03:44:20.593132: step 4127, loss 0.0492313, acc 0.984375, prec 0.0762248, recall 0.816306
2017-12-10T03:44:20.858418: step 4128, loss 0.268665, acc 0.9375, prec 0.0762199, recall 0.816306
2017-12-10T03:44:21.125313: step 4129, loss 0.0882936, acc 0.96875, prec 0.0762322, recall 0.816337
2017-12-10T03:44:21.390665: step 4130, loss 0.117229, acc 0.96875, prec 0.0762298, recall 0.816337
2017-12-10T03:44:21.656948: step 4131, loss 0.100482, acc 0.9375, prec 0.0762543, recall 0.8164
2017-12-10T03:44:21.920741: step 4132, loss 0.186029, acc 0.96875, prec 0.0762519, recall 0.8164
2017-12-10T03:44:22.190603: step 4133, loss 0.149474, acc 0.96875, prec 0.0762495, recall 0.8164
2017-12-10T03:44:22.461270: step 4134, loss 0.0618153, acc 0.96875, prec 0.0762617, recall 0.816431
2017-12-10T03:44:22.726129: step 4135, loss 0.0992347, acc 0.953125, prec 0.0762728, recall 0.816462
2017-12-10T03:44:22.992443: step 4136, loss 0.0391412, acc 0.984375, prec 0.0762716, recall 0.816462
2017-12-10T03:44:23.256083: step 4137, loss 0.0831099, acc 0.953125, prec 0.076268, recall 0.816462
2017-12-10T03:44:23.526820: step 4138, loss 0.0906643, acc 0.984375, prec 0.0762814, recall 0.816493
2017-12-10T03:44:23.797523: step 4139, loss 0.77251, acc 0.96875, prec 0.0762937, recall 0.816525
2017-12-10T03:44:24.065562: step 4140, loss 0.00580051, acc 1, prec 0.0762937, recall 0.816525
2017-12-10T03:44:24.329719: step 4141, loss 0.0143595, acc 1, prec 0.0762937, recall 0.816525
2017-12-10T03:44:24.600115: step 4142, loss 0.160315, acc 0.953125, prec 0.0763195, recall 0.816587
2017-12-10T03:44:24.865241: step 4143, loss 0.133219, acc 0.984375, prec 0.0763477, recall 0.81665
2017-12-10T03:44:25.139081: step 4144, loss 0.064764, acc 1, prec 0.0763771, recall 0.816712
2017-12-10T03:44:25.405440: step 4145, loss 0.167208, acc 0.96875, prec 0.0763746, recall 0.816712
2017-12-10T03:44:25.678853: step 4146, loss 0.0493069, acc 0.984375, prec 0.0763881, recall 0.816743
2017-12-10T03:44:25.944701: step 4147, loss 0.155641, acc 0.953125, prec 0.0764286, recall 0.816837
2017-12-10T03:44:26.209262: step 4148, loss 0.119366, acc 0.984375, prec 0.076442, recall 0.816868
2017-12-10T03:44:26.477888: step 4149, loss 6.54264, acc 0.953125, prec 0.0764543, recall 0.81676
2017-12-10T03:44:26.756123: step 4150, loss 0.143856, acc 0.953125, prec 0.07648, recall 0.816822
2017-12-10T03:44:27.032659: step 4151, loss 0.0673978, acc 0.984375, prec 0.0764788, recall 0.816822
2017-12-10T03:44:27.299286: step 4152, loss 0.358325, acc 0.921875, prec 0.0765021, recall 0.816885
2017-12-10T03:44:27.566037: step 4153, loss 0.137427, acc 0.9375, prec 0.0764973, recall 0.816885
2017-12-10T03:44:27.831217: step 4154, loss 0.330367, acc 0.921875, prec 0.0765059, recall 0.816916
2017-12-10T03:44:28.097950: step 4155, loss 0.520568, acc 0.828125, prec 0.0765072, recall 0.816947
2017-12-10T03:44:28.362512: step 4156, loss 0.105157, acc 0.953125, prec 0.0765035, recall 0.816947
2017-12-10T03:44:28.629971: step 4157, loss 0.201898, acc 0.9375, prec 0.0764987, recall 0.816947
2017-12-10T03:44:28.897589: step 4158, loss 0.621973, acc 0.84375, prec 0.0765012, recall 0.816978
2017-12-10T03:44:29.169021: step 4159, loss 0.245537, acc 0.875, prec 0.0765208, recall 0.81704
2017-12-10T03:44:29.435352: step 4160, loss 0.211858, acc 0.921875, prec 0.0765147, recall 0.81704
2017-12-10T03:44:29.705489: step 4161, loss 0.505436, acc 0.90625, prec 0.0765074, recall 0.81704
2017-12-10T03:44:29.969087: step 4162, loss 0.18268, acc 0.9375, prec 0.0765319, recall 0.817102
2017-12-10T03:44:30.246316: step 4163, loss 0.427961, acc 0.90625, prec 0.0765246, recall 0.817102
2017-12-10T03:44:30.518624: step 4164, loss 0.461279, acc 0.9375, prec 0.0765198, recall 0.817102
2017-12-10T03:44:30.786060: step 4165, loss 0.0873577, acc 0.984375, prec 0.0765625, recall 0.817195
2017-12-10T03:44:31.048828: step 4166, loss 0.166242, acc 0.9375, prec 0.076587, recall 0.817257
2017-12-10T03:44:31.311624: step 4167, loss 0.0525514, acc 0.96875, prec 0.0765846, recall 0.817257
2017-12-10T03:44:31.575700: step 4168, loss 0.10445, acc 0.953125, prec 0.0765809, recall 0.817257
2017-12-10T03:44:31.841096: step 4169, loss 0.198927, acc 0.953125, prec 0.0765773, recall 0.817257
2017-12-10T03:44:32.107794: step 4170, loss 1.1386, acc 0.9375, prec 0.0766018, recall 0.817319
2017-12-10T03:44:32.377436: step 4171, loss 0.13591, acc 0.9375, prec 0.0766116, recall 0.81735
2017-12-10T03:44:32.649616: step 4172, loss 0.428649, acc 0.90625, prec 0.0766043, recall 0.81735
2017-12-10T03:44:32.915167: step 4173, loss 0.079118, acc 0.984375, prec 0.0766177, recall 0.817381
2017-12-10T03:44:33.181714: step 4174, loss 0.0292996, acc 0.984375, prec 0.0766312, recall 0.817412
2017-12-10T03:44:33.449456: step 4175, loss 0.305908, acc 0.890625, prec 0.0766226, recall 0.817412
2017-12-10T03:44:33.715660: step 4176, loss 0.199711, acc 0.96875, prec 0.0766349, recall 0.817443
2017-12-10T03:44:33.985044: step 4177, loss 1.00506, acc 0.90625, prec 0.0766422, recall 0.817474
2017-12-10T03:44:34.255266: step 4178, loss 0.385202, acc 0.90625, prec 0.0766496, recall 0.817505
2017-12-10T03:44:34.524000: step 4179, loss 0.0227821, acc 1, prec 0.0766642, recall 0.817536
2017-12-10T03:44:34.788821: step 4180, loss 0.325484, acc 0.921875, prec 0.0766582, recall 0.817536
2017-12-10T03:44:35.053814: step 4181, loss 0.332416, acc 0.921875, prec 0.0766521, recall 0.817536
2017-12-10T03:44:35.322004: step 4182, loss 0.357086, acc 0.90625, prec 0.0766594, recall 0.817566
2017-12-10T03:44:35.592800: step 4183, loss 0.619659, acc 0.90625, prec 0.0766668, recall 0.817597
2017-12-10T03:44:35.854190: step 4184, loss 0.174702, acc 0.96875, prec 0.0766936, recall 0.817659
2017-12-10T03:44:36.121453: step 4185, loss 0.172701, acc 0.953125, prec 0.0767046, recall 0.81769
2017-12-10T03:44:36.382449: step 4186, loss 0.802936, acc 0.921875, prec 0.0767278, recall 0.817751
2017-12-10T03:44:36.647218: step 4187, loss 0.085848, acc 0.96875, prec 0.0767547, recall 0.817813
2017-12-10T03:44:36.917353: step 4188, loss 0.356581, acc 0.9375, prec 0.0767498, recall 0.817813
2017-12-10T03:44:37.182992: step 4189, loss 0.101024, acc 1, prec 0.0767645, recall 0.817844
2017-12-10T03:44:37.453277: step 4190, loss 0.198002, acc 0.984375, prec 0.0767633, recall 0.817844
2017-12-10T03:44:37.713930: step 4191, loss 0.65061, acc 0.921875, prec 0.0767572, recall 0.817844
2017-12-10T03:44:37.981412: step 4192, loss 0.0149869, acc 1, prec 0.0767572, recall 0.817844
2017-12-10T03:44:38.242116: step 4193, loss 4.40634, acc 0.921875, prec 0.0767669, recall 0.817737
2017-12-10T03:44:38.511181: step 4194, loss 0.0715175, acc 0.953125, prec 0.0767779, recall 0.817767
2017-12-10T03:44:38.774042: step 4195, loss 0.309159, acc 0.921875, prec 0.0767865, recall 0.817798
2017-12-10T03:44:39.038094: step 4196, loss 0.274177, acc 0.859375, prec 0.0768048, recall 0.81786
2017-12-10T03:44:39.302808: step 4197, loss 0.0254839, acc 0.984375, prec 0.0768036, recall 0.81786
2017-12-10T03:44:39.566953: step 4198, loss 0.107558, acc 0.9375, prec 0.0767987, recall 0.81786
2017-12-10T03:44:39.834401: step 4199, loss 0.27389, acc 0.921875, prec 0.0768219, recall 0.817921
2017-12-10T03:44:40.106490: step 4200, loss 0.375833, acc 0.90625, prec 0.0768438, recall 0.817982

Evaluation:
2017-12-10T03:44:47.761548: step 4200, loss 2.22313, acc 0.911595, prec 0.0772087, recall 0.81395

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-4200

2017-12-10T03:44:49.024983: step 4201, loss 0.308144, acc 0.90625, prec 0.0772015, recall 0.81395
2017-12-10T03:44:49.292308: step 4202, loss 0.643509, acc 0.9375, prec 0.0771967, recall 0.81395
2017-12-10T03:44:49.562322: step 4203, loss 0.121358, acc 0.953125, prec 0.0772075, recall 0.81398
2017-12-10T03:44:49.828452: step 4204, loss 0.239505, acc 0.890625, prec 0.0772135, recall 0.814011
2017-12-10T03:44:50.100377: step 4205, loss 0.0814953, acc 0.984375, prec 0.077241, recall 0.814072
2017-12-10T03:44:50.365008: step 4206, loss 0.309588, acc 0.96875, prec 0.0772962, recall 0.814194
2017-12-10T03:44:50.628284: step 4207, loss 0.374582, acc 0.890625, prec 0.0772878, recall 0.814194
2017-12-10T03:44:50.893541: step 4208, loss 0.147578, acc 0.953125, prec 0.0772985, recall 0.814225
2017-12-10T03:44:51.163110: step 4209, loss 0.576967, acc 0.875, prec 0.0773177, recall 0.814286
2017-12-10T03:44:51.440682: step 4210, loss 0.199354, acc 0.953125, prec 0.0773284, recall 0.814316
2017-12-10T03:44:51.710424: step 4211, loss 0.212411, acc 0.921875, prec 0.0773512, recall 0.814377
2017-12-10T03:44:51.977349: step 4212, loss 0.042789, acc 0.984375, prec 0.0773787, recall 0.814438
2017-12-10T03:44:52.242592: step 4213, loss 0.417852, acc 0.90625, prec 0.0773859, recall 0.814469
2017-12-10T03:44:52.515853: step 4214, loss 0.230929, acc 0.953125, prec 0.077411, recall 0.814529
2017-12-10T03:44:52.791664: step 4215, loss 0.103442, acc 0.984375, prec 0.0774242, recall 0.81456
2017-12-10T03:44:53.055573: step 4216, loss 0.131675, acc 0.953125, prec 0.0774349, recall 0.81459
2017-12-10T03:44:53.320814: step 4217, loss 0.240015, acc 0.953125, prec 0.0774313, recall 0.81459
2017-12-10T03:44:53.597959: step 4218, loss 0.39146, acc 0.953125, prec 0.0774565, recall 0.814651
2017-12-10T03:44:53.863632: step 4219, loss 0.10677, acc 0.953125, prec 0.0774528, recall 0.814651
2017-12-10T03:44:54.126435: step 4220, loss 0.0229711, acc 0.984375, prec 0.0774516, recall 0.814651
2017-12-10T03:44:54.391517: step 4221, loss 0.16168, acc 0.96875, prec 0.0774636, recall 0.814681
2017-12-10T03:44:54.655483: step 4222, loss 0.0523443, acc 0.96875, prec 0.0774612, recall 0.814681
2017-12-10T03:44:54.920590: step 4223, loss 0.155256, acc 0.984375, prec 0.07746, recall 0.814681
2017-12-10T03:44:55.196598: step 4224, loss 0.0954315, acc 0.953125, prec 0.0774707, recall 0.814712
2017-12-10T03:44:55.459745: step 4225, loss 0.0629876, acc 0.96875, prec 0.0774827, recall 0.814742
2017-12-10T03:44:55.724355: step 4226, loss 1.53599, acc 0.9375, prec 0.077479, recall 0.814609
2017-12-10T03:44:55.989221: step 4227, loss 0.029463, acc 0.984375, prec 0.0774778, recall 0.814609
2017-12-10T03:44:56.260560: step 4228, loss 0.348392, acc 0.921875, prec 0.0774862, recall 0.814639
2017-12-10T03:44:56.527688: step 4229, loss 0.0699632, acc 0.96875, prec 0.0774981, recall 0.814669
2017-12-10T03:44:56.808375: step 4230, loss 0.0437391, acc 0.984375, prec 0.0774969, recall 0.814669
2017-12-10T03:44:57.076559: step 4231, loss 0.121064, acc 0.984375, prec 0.0775245, recall 0.81473
2017-12-10T03:44:57.345666: step 4232, loss 0.0571326, acc 0.96875, prec 0.077522, recall 0.81473
2017-12-10T03:44:57.608710: step 4233, loss 0.119394, acc 0.96875, prec 0.0775196, recall 0.81473
2017-12-10T03:44:57.878131: step 4234, loss 0.287073, acc 0.96875, prec 0.0775172, recall 0.81473
2017-12-10T03:44:58.143726: step 4235, loss 0.0572621, acc 0.96875, prec 0.0775292, recall 0.81476
2017-12-10T03:44:58.410802: step 4236, loss 0.370385, acc 0.96875, prec 0.0775267, recall 0.81476
2017-12-10T03:44:58.676067: step 4237, loss 0.0669487, acc 0.984375, prec 0.0775255, recall 0.81476
2017-12-10T03:44:58.942133: step 4238, loss 0.377875, acc 0.921875, prec 0.0775195, recall 0.81476
2017-12-10T03:44:59.208970: step 4239, loss 0.00875563, acc 1, prec 0.0775195, recall 0.81476
2017-12-10T03:44:59.475049: step 4240, loss 4.62548, acc 0.921875, prec 0.077529, recall 0.814657
2017-12-10T03:44:59.749840: step 4241, loss 0.199747, acc 0.9375, prec 0.0775242, recall 0.814657
2017-12-10T03:45:00.016392: step 4242, loss 0.122496, acc 0.96875, prec 0.0775218, recall 0.814657
2017-12-10T03:45:00.296508: step 4243, loss 0.147868, acc 0.9375, prec 0.077517, recall 0.814657
2017-12-10T03:45:00.563766: step 4244, loss 0.077889, acc 0.96875, prec 0.0775146, recall 0.814657
2017-12-10T03:45:00.838648: step 4245, loss 0.198268, acc 0.9375, prec 0.0775097, recall 0.814657
2017-12-10T03:45:01.108630: step 4246, loss 0.394064, acc 0.859375, prec 0.0774989, recall 0.814657
2017-12-10T03:45:01.378535: step 4247, loss 0.214494, acc 0.90625, prec 0.077506, recall 0.814688
2017-12-10T03:45:01.647897: step 4248, loss 0.465411, acc 0.859375, prec 0.0774951, recall 0.814688
2017-12-10T03:45:01.918480: step 4249, loss 0.373234, acc 0.921875, prec 0.0775178, recall 0.814748
2017-12-10T03:45:02.180605: step 4250, loss 0.176202, acc 0.96875, prec 0.0775298, recall 0.814779
2017-12-10T03:45:02.453684: step 4251, loss 0.513472, acc 0.890625, prec 0.07755, recall 0.814839
2017-12-10T03:45:02.716955: step 4252, loss 2.9191, acc 0.796875, prec 0.0775355, recall 0.814706
2017-12-10T03:45:02.977136: step 4253, loss 0.504133, acc 0.84375, prec 0.0775378, recall 0.814736
2017-12-10T03:45:03.240612: step 4254, loss 0.161184, acc 0.96875, prec 0.0775497, recall 0.814766
2017-12-10T03:45:03.509893: step 4255, loss 0.447459, acc 0.90625, prec 0.0775569, recall 0.814797
2017-12-10T03:45:03.774694: step 4256, loss 0.412474, acc 0.875, prec 0.0775472, recall 0.814797
2017-12-10T03:45:04.038381: step 4257, loss 0.243952, acc 0.90625, prec 0.0775687, recall 0.814857
2017-12-10T03:45:04.302705: step 4258, loss 0.765283, acc 0.875, prec 0.0775733, recall 0.814887
2017-12-10T03:45:04.568898: step 4259, loss 0.0706014, acc 0.96875, prec 0.0775853, recall 0.814918
2017-12-10T03:45:04.833275: step 4260, loss 0.341007, acc 0.921875, prec 0.0775792, recall 0.814918
2017-12-10T03:45:05.105629: step 4261, loss 0.439724, acc 0.859375, prec 0.0775684, recall 0.814918
2017-12-10T03:45:05.368318: step 4262, loss 0.350397, acc 0.875, prec 0.0775731, recall 0.814948
2017-12-10T03:45:05.630117: step 4263, loss 0.350256, acc 0.90625, prec 0.0775802, recall 0.814978
2017-12-10T03:45:05.897572: step 4264, loss 0.128726, acc 0.90625, prec 0.0775729, recall 0.814978
2017-12-10T03:45:06.164015: step 4265, loss 0.204914, acc 0.9375, prec 0.0775825, recall 0.815008
2017-12-10T03:45:06.430992: step 4266, loss 0.383425, acc 0.90625, prec 0.0776182, recall 0.815099
2017-12-10T03:45:06.704391: step 4267, loss 0.349125, acc 0.921875, prec 0.0776551, recall 0.815189
2017-12-10T03:45:06.975288: step 4268, loss 0.19547, acc 0.9375, prec 0.0776789, recall 0.815249
2017-12-10T03:45:07.239072: step 4269, loss 0.631724, acc 0.890625, prec 0.0776848, recall 0.815279
2017-12-10T03:45:07.508483: step 4270, loss 1.13062, acc 0.9375, prec 0.0777086, recall 0.81534
2017-12-10T03:45:07.774642: step 4271, loss 0.453645, acc 0.9375, prec 0.0777181, recall 0.81537
2017-12-10T03:45:08.038536: step 4272, loss 0.111742, acc 0.953125, prec 0.0777288, recall 0.8154
2017-12-10T03:45:08.305765: step 4273, loss 0.0947526, acc 0.953125, prec 0.0777395, recall 0.81543
2017-12-10T03:45:08.571591: step 4274, loss 0.331517, acc 0.984375, prec 0.0777669, recall 0.81549
2017-12-10T03:45:08.838986: step 4275, loss 0.191635, acc 0.96875, prec 0.0777645, recall 0.81549
2017-12-10T03:45:09.107090: step 4276, loss 0.310841, acc 0.890625, prec 0.0777561, recall 0.81549
2017-12-10T03:45:09.369869: step 4277, loss 0.273733, acc 0.984375, prec 0.0777549, recall 0.81549
2017-12-10T03:45:09.637194: step 4278, loss 0.0966236, acc 0.984375, prec 0.0777536, recall 0.81549
2017-12-10T03:45:09.906952: step 4279, loss 0.231104, acc 0.953125, prec 0.0777643, recall 0.81552
2017-12-10T03:45:10.171275: step 4280, loss 0.268506, acc 0.953125, prec 0.077775, recall 0.81555
2017-12-10T03:45:10.440587: step 4281, loss 0.0371404, acc 0.984375, prec 0.0777738, recall 0.81555
2017-12-10T03:45:10.701483: step 4282, loss 0.3674, acc 0.890625, prec 0.0777797, recall 0.81558
2017-12-10T03:45:10.967523: step 4283, loss 0.0266024, acc 1, prec 0.0777797, recall 0.81558
2017-12-10T03:45:11.231185: step 4284, loss 0.07258, acc 0.984375, prec 0.0777928, recall 0.81561
2017-12-10T03:45:11.504847: step 4285, loss 0.160922, acc 0.96875, prec 0.0778047, recall 0.81564
2017-12-10T03:45:11.778862: step 4286, loss 0.0910078, acc 0.984375, prec 0.0778035, recall 0.81564
2017-12-10T03:45:12.052194: step 4287, loss 0.437516, acc 0.984375, prec 0.0778308, recall 0.8157
2017-12-10T03:45:12.323415: step 4288, loss 0.00457161, acc 1, prec 0.0778308, recall 0.8157
2017-12-10T03:45:12.584843: step 4289, loss 0.175787, acc 0.953125, prec 0.0778272, recall 0.8157
2017-12-10T03:45:12.848340: step 4290, loss 0.215349, acc 0.96875, prec 0.0778534, recall 0.81576
2017-12-10T03:45:13.122337: step 4291, loss 0.333314, acc 0.984375, prec 0.0778808, recall 0.815819
2017-12-10T03:45:13.387067: step 4292, loss 0.137623, acc 0.96875, prec 0.0778784, recall 0.815819
2017-12-10T03:45:13.652007: step 4293, loss 0.111521, acc 0.953125, prec 0.0778748, recall 0.815819
2017-12-10T03:45:13.924697: step 4294, loss 0.0890554, acc 0.984375, prec 0.0779021, recall 0.815879
2017-12-10T03:45:14.189229: step 4295, loss 0.115405, acc 0.953125, prec 0.0778985, recall 0.815879
2017-12-10T03:45:14.461796: step 4296, loss 0.224184, acc 0.953125, prec 0.0779235, recall 0.815939
2017-12-10T03:45:14.728522: step 4297, loss 0.136708, acc 0.984375, prec 0.0779366, recall 0.815969
2017-12-10T03:45:15.009210: step 4298, loss 0.104559, acc 0.96875, prec 0.0779342, recall 0.815969
2017-12-10T03:45:15.272935: step 4299, loss 0.15312, acc 0.9375, prec 0.0779293, recall 0.815969
2017-12-10T03:45:15.540086: step 4300, loss 0.219064, acc 0.953125, prec 0.07794, recall 0.815999
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-4300

2017-12-10T03:45:16.804349: step 4301, loss 0.0125942, acc 1, prec 0.07794, recall 0.815999
2017-12-10T03:45:17.073379: step 4302, loss 0.387621, acc 0.96875, prec 0.0779519, recall 0.816029
2017-12-10T03:45:17.343862: step 4303, loss 0.412932, acc 0.90625, prec 0.0779589, recall 0.816058
2017-12-10T03:45:17.611444: step 4304, loss 0.0494989, acc 0.96875, prec 0.0779565, recall 0.816058
2017-12-10T03:45:17.880873: step 4305, loss 0.0649853, acc 0.984375, prec 0.0779696, recall 0.816088
2017-12-10T03:45:18.152682: step 4306, loss 0.149468, acc 0.984375, prec 0.0779684, recall 0.816088
2017-12-10T03:45:18.419037: step 4307, loss 0.5582, acc 0.984375, prec 0.0779814, recall 0.816118
2017-12-10T03:45:18.682877: step 4308, loss 0.0520845, acc 0.96875, prec 0.077979, recall 0.816118
2017-12-10T03:45:18.951459: step 4309, loss 0.152555, acc 0.96875, prec 0.0779909, recall 0.816148
2017-12-10T03:45:19.220935: step 4310, loss 0.13222, acc 0.953125, prec 0.0779873, recall 0.816148
2017-12-10T03:45:19.487753: step 4311, loss 0.21187, acc 0.953125, prec 0.0779979, recall 0.816178
2017-12-10T03:45:19.760424: step 4312, loss 0.206884, acc 0.9375, prec 0.0779931, recall 0.816178
2017-12-10T03:45:20.028281: step 4313, loss 0.277516, acc 0.921875, prec 0.0779871, recall 0.816178
2017-12-10T03:45:20.294810: step 4314, loss 0.174893, acc 0.9375, prec 0.0779822, recall 0.816178
2017-12-10T03:45:20.553720: step 4315, loss 0.141466, acc 0.953125, prec 0.0779786, recall 0.816178
2017-12-10T03:45:20.824273: step 4316, loss 0.0244113, acc 0.984375, prec 0.0780059, recall 0.816237
2017-12-10T03:45:21.100761: step 4317, loss 0.374602, acc 0.953125, prec 0.0780166, recall 0.816267
2017-12-10T03:45:21.370177: step 4318, loss 0.0360679, acc 0.984375, prec 0.0780582, recall 0.816356
2017-12-10T03:45:21.636954: step 4319, loss 0.231707, acc 0.9375, prec 0.0780819, recall 0.816416
2017-12-10T03:45:21.900836: step 4320, loss 0.122018, acc 0.953125, prec 0.0780926, recall 0.816445
2017-12-10T03:45:22.168028: step 4321, loss 2.26594, acc 0.96875, prec 0.0781199, recall 0.816373
2017-12-10T03:45:22.438736: step 4322, loss 0.0315029, acc 0.984375, prec 0.0781187, recall 0.816373
2017-12-10T03:45:22.707713: step 4323, loss 0.435394, acc 0.953125, prec 0.0781294, recall 0.816402
2017-12-10T03:45:22.975167: step 4324, loss 0.109795, acc 0.96875, prec 0.0781412, recall 0.816432
2017-12-10T03:45:23.246097: step 4325, loss 0.0131847, acc 1, prec 0.0781555, recall 0.816462
2017-12-10T03:45:23.513357: step 4326, loss 0.152732, acc 0.921875, prec 0.0781494, recall 0.816462
2017-12-10T03:45:23.780154: step 4327, loss 0.171763, acc 0.96875, prec 0.078147, recall 0.816462
2017-12-10T03:45:24.047155: step 4328, loss 0.292764, acc 0.921875, prec 0.0781552, recall 0.816491
2017-12-10T03:45:24.318164: step 4329, loss 0.316443, acc 0.9375, prec 0.0781647, recall 0.816521
2017-12-10T03:45:24.592623: step 4330, loss 0.137111, acc 0.953125, prec 0.078161, recall 0.816521
2017-12-10T03:45:24.868661: step 4331, loss 0.48, acc 0.90625, prec 0.0781538, recall 0.816521
2017-12-10T03:45:25.137751: step 4332, loss 0.12026, acc 0.96875, prec 0.0781514, recall 0.816521
2017-12-10T03:45:25.406428: step 4333, loss 0.127814, acc 0.9375, prec 0.0781465, recall 0.816521
2017-12-10T03:45:25.670765: step 4334, loss 0.408984, acc 0.9375, prec 0.0781559, recall 0.816551
2017-12-10T03:45:25.932245: step 4335, loss 0.199862, acc 0.9375, prec 0.0781654, recall 0.81658
2017-12-10T03:45:26.193928: step 4336, loss 0.475219, acc 0.9375, prec 0.0781605, recall 0.81658
2017-12-10T03:45:26.459420: step 4337, loss 0.0487059, acc 0.984375, prec 0.0781593, recall 0.81658
2017-12-10T03:45:26.728108: step 4338, loss 0.0575127, acc 0.984375, prec 0.0781724, recall 0.81661
2017-12-10T03:45:26.994486: step 4339, loss 0.0124297, acc 1, prec 0.0781866, recall 0.81664
2017-12-10T03:45:27.261970: step 4340, loss 0.10197, acc 0.984375, prec 0.0781997, recall 0.816669
2017-12-10T03:45:27.528325: step 4341, loss 0.0230283, acc 0.984375, prec 0.0781985, recall 0.816669
2017-12-10T03:45:27.794047: step 4342, loss 3.83502, acc 0.953125, prec 0.0782103, recall 0.816567
2017-12-10T03:45:28.060412: step 4343, loss 0.053506, acc 0.96875, prec 0.0782221, recall 0.816597
2017-12-10T03:45:28.332174: step 4344, loss 0.146094, acc 0.953125, prec 0.0782328, recall 0.816626
2017-12-10T03:45:28.595781: step 4345, loss 0.395276, acc 0.953125, prec 0.0782576, recall 0.816685
2017-12-10T03:45:28.861343: step 4346, loss 0.203722, acc 0.96875, prec 0.0782695, recall 0.816715
2017-12-10T03:45:29.125752: step 4347, loss 0.301565, acc 0.921875, prec 0.0782777, recall 0.816745
2017-12-10T03:45:29.390006: step 4348, loss 0.0509755, acc 0.96875, prec 0.0783038, recall 0.816804
2017-12-10T03:45:29.650572: step 4349, loss 0.465627, acc 0.953125, prec 0.0783001, recall 0.816804
2017-12-10T03:45:29.912270: step 4350, loss 0.671636, acc 0.953125, prec 0.0782965, recall 0.816804
2017-12-10T03:45:30.189095: step 4351, loss 0.144763, acc 0.9375, prec 0.0782916, recall 0.816804
2017-12-10T03:45:30.457625: step 4352, loss 0.360305, acc 0.921875, prec 0.0782856, recall 0.816804
2017-12-10T03:45:30.719471: step 4353, loss 0.370303, acc 0.90625, prec 0.0783068, recall 0.816863
2017-12-10T03:45:30.989369: step 4354, loss 0.251344, acc 0.9375, prec 0.078302, recall 0.816863
2017-12-10T03:45:31.252008: step 4355, loss 0.0803021, acc 0.96875, prec 0.0783138, recall 0.816892
2017-12-10T03:45:31.525480: step 4356, loss 0.44051, acc 0.953125, prec 0.0783387, recall 0.816951
2017-12-10T03:45:31.796146: step 4357, loss 0.0656633, acc 0.96875, prec 0.0783505, recall 0.816981
2017-12-10T03:45:32.060187: step 4358, loss 0.249007, acc 0.90625, prec 0.0783717, recall 0.81704
2017-12-10T03:45:32.330643: step 4359, loss 0.102196, acc 0.953125, prec 0.0783823, recall 0.817069
2017-12-10T03:45:32.598668: step 4360, loss 0.300171, acc 0.96875, prec 0.0783941, recall 0.817099
2017-12-10T03:45:32.863195: step 4361, loss 0.389119, acc 0.96875, prec 0.0784629, recall 0.817246
2017-12-10T03:45:33.128021: step 4362, loss 0.15841, acc 0.953125, prec 0.0784877, recall 0.817305
2017-12-10T03:45:33.391541: step 4363, loss 0.447577, acc 0.875, prec 0.0784922, recall 0.817334
2017-12-10T03:45:33.656568: step 4364, loss 0.0776379, acc 0.984375, prec 0.0785195, recall 0.817393
2017-12-10T03:45:33.929770: step 4365, loss 0.233817, acc 0.921875, prec 0.0785419, recall 0.817451
2017-12-10T03:45:34.199168: step 4366, loss 0.0769353, acc 0.96875, prec 0.0785537, recall 0.817481
2017-12-10T03:45:34.458628: step 4367, loss 0.305787, acc 0.890625, prec 0.0785594, recall 0.81751
2017-12-10T03:45:34.726762: step 4368, loss 0.0755752, acc 0.953125, prec 0.07857, recall 0.817539
2017-12-10T03:45:34.989740: step 4369, loss 1.09068, acc 0.984375, prec 0.0786114, recall 0.817627
2017-12-10T03:45:35.256868: step 4370, loss 0.351275, acc 0.953125, prec 0.078622, recall 0.817657
2017-12-10T03:45:35.520758: step 4371, loss 0.277393, acc 0.9375, prec 0.0786456, recall 0.817715
2017-12-10T03:45:35.782394: step 4372, loss 0.348271, acc 0.859375, prec 0.0786489, recall 0.817744
2017-12-10T03:45:36.051206: step 4373, loss 0.118495, acc 0.96875, prec 0.0786465, recall 0.817744
2017-12-10T03:45:36.314155: step 4374, loss 0.400242, acc 0.953125, prec 0.0786997, recall 0.817861
2017-12-10T03:45:36.587608: step 4375, loss 0.184729, acc 0.9375, prec 0.0787091, recall 0.81789
2017-12-10T03:45:36.857211: step 4376, loss 0.386949, acc 0.9375, prec 0.0787042, recall 0.81789
2017-12-10T03:45:37.121828: step 4377, loss 0.286343, acc 0.921875, prec 0.0786981, recall 0.81789
2017-12-10T03:45:37.392657: step 4378, loss 0.0351657, acc 0.984375, prec 0.0786969, recall 0.81789
2017-12-10T03:45:37.659318: step 4379, loss 0.350204, acc 0.84375, prec 0.0786848, recall 0.81789
2017-12-10T03:45:37.921796: step 4380, loss 1.81969, acc 0.890625, prec 0.0786917, recall 0.817788
2017-12-10T03:45:38.192928: step 4381, loss 0.162056, acc 0.96875, prec 0.0787035, recall 0.817818
2017-12-10T03:45:38.462063: step 4382, loss 0.368829, acc 0.890625, prec 0.0787376, recall 0.817905
2017-12-10T03:45:38.726757: step 4383, loss 0.445355, acc 0.890625, prec 0.0787291, recall 0.817905
2017-12-10T03:45:38.991866: step 4384, loss 0.279856, acc 0.890625, prec 0.0787206, recall 0.817905
2017-12-10T03:45:39.259637: step 4385, loss 0.826489, acc 0.78125, prec 0.078732, recall 0.817963
2017-12-10T03:45:39.521641: step 4386, loss 0.881767, acc 0.796875, prec 0.0787446, recall 0.818022
2017-12-10T03:45:39.793015: step 4387, loss 0.643717, acc 0.890625, prec 0.0787362, recall 0.818022
2017-12-10T03:45:40.060713: step 4388, loss 0.359509, acc 0.90625, prec 0.0787714, recall 0.818109
2017-12-10T03:45:40.322626: step 4389, loss 0.520363, acc 0.90625, prec 0.0787642, recall 0.818109
2017-12-10T03:45:40.586541: step 4390, loss 0.413952, acc 0.90625, prec 0.0787569, recall 0.818109
2017-12-10T03:45:40.853874: step 4391, loss 0.93414, acc 0.90625, prec 0.078778, recall 0.818167
2017-12-10T03:45:41.116978: step 4392, loss 0.592641, acc 0.859375, prec 0.0787954, recall 0.818225
2017-12-10T03:45:41.389572: step 4393, loss 0.651028, acc 0.921875, prec 0.0787894, recall 0.818225
2017-12-10T03:45:41.657423: step 4394, loss 0.719874, acc 0.9375, prec 0.0787987, recall 0.818254
2017-12-10T03:45:41.926850: step 4395, loss 0.604183, acc 0.875, prec 0.078789, recall 0.818254
2017-12-10T03:45:42.195140: step 4396, loss 0.241665, acc 0.890625, prec 0.0787805, recall 0.818254
2017-12-10T03:45:42.472092: step 4397, loss 0.722532, acc 0.875, prec 0.0787992, recall 0.818313
2017-12-10T03:45:42.738274: step 4398, loss 0.446851, acc 0.921875, prec 0.0788073, recall 0.818342
2017-12-10T03:45:43.002835: step 4399, loss 0.39885, acc 0.921875, prec 0.0788154, recall 0.818371
2017-12-10T03:45:43.270981: step 4400, loss 0.442338, acc 0.921875, prec 0.0788093, recall 0.818371
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-4400

2017-12-10T03:45:44.592954: step 4401, loss 0.0232639, acc 1, prec 0.0788093, recall 0.818371
2017-12-10T03:45:44.861967: step 4402, loss 0.179332, acc 0.921875, prec 0.0788033, recall 0.818371
2017-12-10T03:45:45.126816: step 4403, loss 0.140678, acc 0.96875, prec 0.078815, recall 0.8184
2017-12-10T03:45:45.391970: step 4404, loss 0.0260425, acc 1, prec 0.0788292, recall 0.818429
2017-12-10T03:45:45.656193: step 4405, loss 0.451673, acc 0.953125, prec 0.0788539, recall 0.818487
2017-12-10T03:45:45.920668: step 4406, loss 0.359974, acc 0.90625, prec 0.0788608, recall 0.818516
2017-12-10T03:45:46.187063: step 4407, loss 0.467368, acc 0.90625, prec 0.0788677, recall 0.818545
2017-12-10T03:45:46.454499: step 4408, loss 0.178543, acc 0.953125, prec 0.0789348, recall 0.818689
2017-12-10T03:45:46.725875: step 4409, loss 0.298799, acc 1, prec 0.078949, recall 0.818718
2017-12-10T03:45:46.990077: step 4410, loss 0.121451, acc 0.96875, prec 0.0789466, recall 0.818718
2017-12-10T03:45:47.252808: step 4411, loss 0.230139, acc 0.9375, prec 0.0789417, recall 0.818718
2017-12-10T03:45:47.527365: step 4412, loss 0.0373901, acc 0.984375, prec 0.0789547, recall 0.818747
2017-12-10T03:45:47.791732: step 4413, loss 0.273182, acc 0.953125, prec 0.078951, recall 0.818747
2017-12-10T03:45:48.058650: step 4414, loss 0.100505, acc 0.953125, prec 0.0789474, recall 0.818747
2017-12-10T03:45:48.323415: step 4415, loss 0.0809426, acc 0.984375, prec 0.0789462, recall 0.818747
2017-12-10T03:45:48.590415: step 4416, loss 0.340282, acc 0.921875, prec 0.0789542, recall 0.818776
2017-12-10T03:45:48.863600: step 4417, loss 0.202463, acc 0.953125, prec 0.0789648, recall 0.818805
2017-12-10T03:45:49.133256: step 4418, loss 0.0118902, acc 1, prec 0.0789789, recall 0.818834
2017-12-10T03:45:49.403693: step 4419, loss 0.000839714, acc 1, prec 0.0789789, recall 0.818834
2017-12-10T03:45:49.679277: step 4420, loss 0.0645251, acc 0.96875, prec 0.0789906, recall 0.818862
2017-12-10T03:45:49.948385: step 4421, loss 0.0679461, acc 0.96875, prec 0.0790165, recall 0.81892
2017-12-10T03:45:50.220819: step 4422, loss 0.254344, acc 0.953125, prec 0.0790553, recall 0.819007
2017-12-10T03:45:50.503198: step 4423, loss 2.1056, acc 0.984375, prec 0.0790553, recall 0.818876
2017-12-10T03:45:50.768097: step 4424, loss 0.0221048, acc 0.984375, prec 0.0790541, recall 0.818876
2017-12-10T03:45:51.035953: step 4425, loss 0.0388948, acc 0.984375, prec 0.0790529, recall 0.818876
2017-12-10T03:45:51.303605: step 4426, loss 0.0972963, acc 1, prec 0.0790671, recall 0.818905
2017-12-10T03:45:51.574933: step 4427, loss 0.001892, acc 1, prec 0.0790671, recall 0.818905
2017-12-10T03:45:51.840748: step 4428, loss 0.101285, acc 0.96875, prec 0.0790788, recall 0.818934
2017-12-10T03:45:52.109418: step 4429, loss 0.126585, acc 0.96875, prec 0.0790905, recall 0.818963
2017-12-10T03:45:52.383816: step 4430, loss 0.103722, acc 0.984375, prec 0.0791176, recall 0.81902
2017-12-10T03:45:52.649874: step 4431, loss 0.191802, acc 0.984375, prec 0.0791305, recall 0.819049
2017-12-10T03:45:52.925911: step 4432, loss 0.411342, acc 0.96875, prec 0.0791281, recall 0.819049
2017-12-10T03:45:53.190617: step 4433, loss 0.075021, acc 0.96875, prec 0.0791256, recall 0.819049
2017-12-10T03:45:53.463784: step 4434, loss 0.0346869, acc 1, prec 0.0791398, recall 0.819078
2017-12-10T03:45:53.734017: step 4435, loss 0.067632, acc 0.96875, prec 0.0791374, recall 0.819078
2017-12-10T03:45:54.015487: step 4436, loss 0.0526941, acc 0.984375, prec 0.0791361, recall 0.819078
2017-12-10T03:45:54.282239: step 4437, loss 0.1478, acc 0.96875, prec 0.0791337, recall 0.819078
2017-12-10T03:45:54.551370: step 4438, loss 0.123951, acc 0.953125, prec 0.0791301, recall 0.819078
2017-12-10T03:45:54.817138: step 4439, loss 0.161459, acc 0.984375, prec 0.079143, recall 0.819107
2017-12-10T03:45:55.085910: step 4440, loss 0.223327, acc 0.921875, prec 0.0791511, recall 0.819135
2017-12-10T03:45:55.349550: step 4441, loss 0.0388953, acc 0.984375, prec 0.0791498, recall 0.819135
2017-12-10T03:45:55.620280: step 4442, loss 0.0304237, acc 0.96875, prec 0.0791474, recall 0.819135
2017-12-10T03:45:55.881801: step 4443, loss 0.24673, acc 0.9375, prec 0.0791708, recall 0.819193
2017-12-10T03:45:56.149790: step 4444, loss 0.0722999, acc 0.96875, prec 0.0791684, recall 0.819193
2017-12-10T03:45:56.416783: step 4445, loss 0.471226, acc 0.953125, prec 0.0791789, recall 0.819222
2017-12-10T03:45:56.697663: step 4446, loss 0.0721599, acc 0.96875, prec 0.0791906, recall 0.81925
2017-12-10T03:45:56.965980: step 4447, loss 0.969796, acc 0.953125, prec 0.0792152, recall 0.819308
2017-12-10T03:45:57.242416: step 4448, loss 5.17708, acc 0.921875, prec 0.0792245, recall 0.819206
2017-12-10T03:45:57.514004: step 4449, loss 0.107061, acc 1, prec 0.0792669, recall 0.819292
2017-12-10T03:45:57.788788: step 4450, loss 0.485684, acc 0.90625, prec 0.0792737, recall 0.819321
2017-12-10T03:45:58.058809: step 4451, loss 0.258704, acc 0.921875, prec 0.0792676, recall 0.819321
2017-12-10T03:45:58.333777: step 4452, loss 0.682423, acc 0.765625, prec 0.0792635, recall 0.81935
2017-12-10T03:45:58.600467: step 4453, loss 0.374688, acc 0.859375, prec 0.0792526, recall 0.81935
2017-12-10T03:45:58.874572: step 4454, loss 0.556384, acc 0.890625, prec 0.0792441, recall 0.81935
2017-12-10T03:45:59.140524: step 4455, loss 1.64392, acc 0.703125, prec 0.0792492, recall 0.819407
2017-12-10T03:45:59.414881: step 4456, loss 0.717127, acc 0.78125, prec 0.0792463, recall 0.819436
2017-12-10T03:45:59.679803: step 4457, loss 1.00268, acc 0.78125, prec 0.0792434, recall 0.819464
2017-12-10T03:45:59.942477: step 4458, loss 0.619103, acc 0.84375, prec 0.0792313, recall 0.819464
2017-12-10T03:46:00.214737: step 4459, loss 0.526881, acc 0.84375, prec 0.0792191, recall 0.819464
2017-12-10T03:46:00.481655: step 4460, loss 0.34596, acc 0.875, prec 0.0792377, recall 0.819521
2017-12-10T03:46:00.754355: step 4461, loss 0.717227, acc 0.765625, prec 0.0792194, recall 0.819521
2017-12-10T03:46:01.017625: step 4462, loss 0.175944, acc 0.921875, prec 0.0792416, recall 0.819579
2017-12-10T03:46:01.283568: step 4463, loss 0.326428, acc 0.90625, prec 0.0792343, recall 0.819579
2017-12-10T03:46:01.560017: step 4464, loss 0.860822, acc 0.828125, prec 0.079221, recall 0.819579
2017-12-10T03:46:01.826152: step 4465, loss 0.267162, acc 0.921875, prec 0.0792149, recall 0.819579
2017-12-10T03:46:02.088146: step 4466, loss 0.113614, acc 0.96875, prec 0.0792125, recall 0.819579
2017-12-10T03:46:02.357764: step 4467, loss 0.0641022, acc 0.984375, prec 0.0792395, recall 0.819636
2017-12-10T03:46:02.620801: step 4468, loss 0.150785, acc 0.9375, prec 0.0792628, recall 0.819693
2017-12-10T03:46:02.881121: step 4469, loss 0.0265028, acc 0.984375, prec 0.0792616, recall 0.819693
2017-12-10T03:46:03.146033: step 4470, loss 0.0928656, acc 0.96875, prec 0.0792591, recall 0.819693
2017-12-10T03:46:03.414976: step 4471, loss 0.0193775, acc 1, prec 0.0792591, recall 0.819693
2017-12-10T03:46:03.683770: step 4472, loss 0.126824, acc 0.984375, prec 0.0792579, recall 0.819693
2017-12-10T03:46:03.922494: step 4473, loss 0.066662, acc 0.980392, prec 0.0792567, recall 0.819693
2017-12-10T03:46:04.201383: step 4474, loss 0.188242, acc 0.96875, prec 0.0792684, recall 0.819721
2017-12-10T03:46:04.466891: step 4475, loss 0.044628, acc 0.984375, prec 0.0792813, recall 0.81975
2017-12-10T03:46:04.729149: step 4476, loss 0.00386308, acc 1, prec 0.0792954, recall 0.819779
2017-12-10T03:46:04.994354: step 4477, loss 0.0454615, acc 0.96875, prec 0.0792929, recall 0.819779
2017-12-10T03:46:05.264808: step 4478, loss 0.0988879, acc 0.953125, prec 0.0792893, recall 0.819779
2017-12-10T03:46:05.531167: step 4479, loss 0.113955, acc 0.96875, prec 0.0792869, recall 0.819779
2017-12-10T03:46:05.809829: step 4480, loss 0.0949983, acc 0.984375, prec 0.0792857, recall 0.819779
2017-12-10T03:46:06.086080: step 4481, loss 0.00148407, acc 1, prec 0.0792857, recall 0.819779
2017-12-10T03:46:06.360792: step 4482, loss 0.112376, acc 0.984375, prec 0.0792985, recall 0.819807
2017-12-10T03:46:06.636707: step 4483, loss 0.0398194, acc 0.96875, prec 0.0792961, recall 0.819807
2017-12-10T03:46:06.901695: step 4484, loss 1.65326, acc 0.984375, prec 0.0793102, recall 0.819706
2017-12-10T03:46:07.169457: step 4485, loss 0.00142456, acc 1, prec 0.0793102, recall 0.819706
2017-12-10T03:46:07.446873: step 4486, loss 0.0831778, acc 0.96875, prec 0.0793078, recall 0.819706
2017-12-10T03:46:07.714126: step 4487, loss 0.00694066, acc 1, prec 0.0793218, recall 0.819734
2017-12-10T03:46:07.984015: step 4488, loss 0.00413618, acc 1, prec 0.0793218, recall 0.819734
2017-12-10T03:46:08.251368: step 4489, loss 0.0337444, acc 0.984375, prec 0.0793206, recall 0.819734
2017-12-10T03:46:08.518493: step 4490, loss 0.804608, acc 1, prec 0.0793347, recall 0.819763
2017-12-10T03:46:08.788386: step 4491, loss 1.67396, acc 0.984375, prec 0.079377, recall 0.819719
2017-12-10T03:46:09.058375: step 4492, loss 0.248834, acc 0.984375, prec 0.0793898, recall 0.819747
2017-12-10T03:46:09.323120: step 4493, loss 0.34296, acc 0.9375, prec 0.079385, recall 0.819747
2017-12-10T03:46:09.597986: step 4494, loss 0.0725183, acc 0.953125, prec 0.0794095, recall 0.819804
2017-12-10T03:46:09.860786: step 4495, loss 0.0332452, acc 0.984375, prec 0.0794083, recall 0.819804
2017-12-10T03:46:10.130036: step 4496, loss 0.0156331, acc 1, prec 0.0794083, recall 0.819804
2017-12-10T03:46:10.393939: step 4497, loss 0.0918913, acc 0.953125, prec 0.0794469, recall 0.819889
2017-12-10T03:46:10.658965: step 4498, loss 0.18173, acc 0.953125, prec 0.0794714, recall 0.819946
2017-12-10T03:46:10.922004: step 4499, loss 0.212436, acc 0.921875, prec 0.0794653, recall 0.819946
2017-12-10T03:46:11.192133: step 4500, loss 0.165749, acc 0.96875, prec 0.079477, recall 0.819975

Evaluation:
2017-12-10T03:46:18.789213: step 4500, loss 2.85019, acc 0.918672, prec 0.0797157, recall 0.814147

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-4500

2017-12-10T03:46:20.066714: step 4501, loss 0.224333, acc 0.96875, prec 0.0797272, recall 0.814176
2017-12-10T03:46:20.331297: step 4502, loss 0.136826, acc 0.9375, prec 0.0797502, recall 0.814233
2017-12-10T03:46:20.606462: step 4503, loss 0.295356, acc 0.9375, prec 0.0797453, recall 0.814233
2017-12-10T03:46:20.873174: step 4504, loss 0.305569, acc 0.890625, prec 0.0797369, recall 0.814233
2017-12-10T03:46:21.145945: step 4505, loss 0.198965, acc 0.921875, prec 0.0797309, recall 0.814233
2017-12-10T03:46:21.416231: step 4506, loss 0.655477, acc 0.84375, prec 0.0797466, recall 0.81429
2017-12-10T03:46:21.686055: step 4507, loss 0.588181, acc 0.828125, prec 0.0797334, recall 0.81429
2017-12-10T03:46:21.959875: step 4508, loss 0.235903, acc 0.921875, prec 0.0797413, recall 0.814319
2017-12-10T03:46:22.229721: step 4509, loss 0.268985, acc 0.953125, prec 0.0797516, recall 0.814347
2017-12-10T03:46:22.494820: step 4510, loss 0.432207, acc 0.921875, prec 0.0797594, recall 0.814376
2017-12-10T03:46:22.767014: step 4511, loss 0.166979, acc 0.953125, prec 0.0797697, recall 0.814404
2017-12-10T03:46:23.035076: step 4512, loss 0.141868, acc 0.953125, prec 0.0797799, recall 0.814433
2017-12-10T03:46:23.303543: step 4513, loss 0.191561, acc 0.9375, prec 0.079789, recall 0.814462
2017-12-10T03:46:23.577622: step 4514, loss 0.0216231, acc 1, prec 0.0798029, recall 0.81449
2017-12-10T03:46:23.843258: step 4515, loss 0.330608, acc 0.90625, prec 0.0797957, recall 0.81449
2017-12-10T03:46:24.121088: step 4516, loss 0.152061, acc 0.953125, prec 0.0798059, recall 0.814519
2017-12-10T03:46:24.392423: step 4517, loss 0.197761, acc 0.984375, prec 0.0798186, recall 0.814547
2017-12-10T03:46:24.662815: step 4518, loss 0.0582617, acc 1, prec 0.0798463, recall 0.814604
2017-12-10T03:46:24.931734: step 4519, loss 0.0357113, acc 1, prec 0.079874, recall 0.814661
2017-12-10T03:46:25.194184: step 4520, loss 0.916899, acc 1, prec 0.0799018, recall 0.814718
2017-12-10T03:46:25.466459: step 4521, loss 0.160832, acc 0.984375, prec 0.0799421, recall 0.814803
2017-12-10T03:46:25.743248: step 4522, loss 0.0105756, acc 1, prec 0.079956, recall 0.814832
2017-12-10T03:46:26.009299: step 4523, loss 0.0776396, acc 0.96875, prec 0.0799536, recall 0.814832
2017-12-10T03:46:26.278202: step 4524, loss 0.191585, acc 0.984375, prec 0.0799801, recall 0.814889
2017-12-10T03:46:26.558060: step 4525, loss 0.298298, acc 0.953125, prec 0.0799765, recall 0.814889
2017-12-10T03:46:26.840296: step 4526, loss 2.07353, acc 0.96875, prec 0.0800169, recall 0.814849
2017-12-10T03:46:27.106230: step 4527, loss 0.246081, acc 0.953125, prec 0.0800271, recall 0.814877
2017-12-10T03:46:27.383428: step 4528, loss 0.0889287, acc 0.953125, prec 0.0800235, recall 0.814877
2017-12-10T03:46:27.646291: step 4529, loss 0.602584, acc 0.9375, prec 0.0800325, recall 0.814906
2017-12-10T03:46:27.911979: step 4530, loss 0.336654, acc 0.890625, prec 0.0800241, recall 0.814906
2017-12-10T03:46:28.181432: step 4531, loss 0.215597, acc 0.921875, prec 0.0800319, recall 0.814934
2017-12-10T03:46:28.453958: step 4532, loss 0.574675, acc 0.859375, prec 0.0800211, recall 0.814934
2017-12-10T03:46:28.727848: step 4533, loss 0.0710596, acc 0.953125, prec 0.0800175, recall 0.814934
2017-12-10T03:46:28.998948: step 4534, loss 0.346468, acc 0.921875, prec 0.0800253, recall 0.814962
2017-12-10T03:46:29.272057: step 4535, loss 0.614627, acc 0.859375, prec 0.0800837, recall 0.815104
2017-12-10T03:46:29.535744: step 4536, loss 0.270186, acc 0.9375, prec 0.0800789, recall 0.815104
2017-12-10T03:46:29.805338: step 4537, loss 0.334175, acc 0.921875, prec 0.0800728, recall 0.815104
2017-12-10T03:46:30.073293: step 4538, loss 0.719402, acc 0.890625, prec 0.0801059, recall 0.815189
2017-12-10T03:46:30.354841: step 4539, loss 0.437629, acc 0.875, prec 0.0801101, recall 0.815217
2017-12-10T03:46:30.629789: step 4540, loss 0.0961565, acc 0.953125, prec 0.0801203, recall 0.815246
2017-12-10T03:46:30.900376: step 4541, loss 0.322842, acc 0.9375, prec 0.0801155, recall 0.815246
2017-12-10T03:46:31.171891: step 4542, loss 0.0998464, acc 0.984375, prec 0.0801282, recall 0.815274
2017-12-10T03:46:31.444911: step 4543, loss 0.296699, acc 0.921875, prec 0.0801498, recall 0.815331
2017-12-10T03:46:31.713863: step 4544, loss 0.375657, acc 0.953125, prec 0.0801462, recall 0.815331
2017-12-10T03:46:31.984902: step 4545, loss 0.167288, acc 0.96875, prec 0.0801576, recall 0.815359
2017-12-10T03:46:32.251654: step 4546, loss 0.3022, acc 0.90625, prec 0.0801504, recall 0.815359
2017-12-10T03:46:32.524442: step 4547, loss 0.278828, acc 0.953125, prec 0.0801468, recall 0.815359
2017-12-10T03:46:32.791588: step 4548, loss 0.0318642, acc 1, prec 0.0801468, recall 0.815359
2017-12-10T03:46:33.057742: step 4549, loss 0.24341, acc 0.9375, prec 0.0801834, recall 0.815443
2017-12-10T03:46:33.327059: step 4550, loss 0.337303, acc 0.90625, prec 0.0801762, recall 0.815443
2017-12-10T03:46:33.592853: step 4551, loss 0.0557753, acc 0.96875, prec 0.0802153, recall 0.815528
2017-12-10T03:46:33.859980: step 4552, loss 0.179415, acc 0.953125, prec 0.0802255, recall 0.815556
2017-12-10T03:46:34.134964: step 4553, loss 2.71008, acc 0.984375, prec 0.0802393, recall 0.81546
2017-12-10T03:46:34.409287: step 4554, loss 0.179098, acc 0.953125, prec 0.0802357, recall 0.81546
2017-12-10T03:46:34.675211: step 4555, loss 0.0733965, acc 0.9375, prec 0.0802447, recall 0.815488
2017-12-10T03:46:34.949602: step 4556, loss 0.350757, acc 0.921875, prec 0.0802387, recall 0.815488
2017-12-10T03:46:35.213946: step 4557, loss 0.290439, acc 0.96875, prec 0.0802362, recall 0.815488
2017-12-10T03:46:35.478382: step 4558, loss 0.225429, acc 0.9375, prec 0.0802314, recall 0.815488
2017-12-10T03:46:35.742733: step 4559, loss 0.336715, acc 0.921875, prec 0.0802254, recall 0.815488
2017-12-10T03:46:36.007213: step 4560, loss 0.100567, acc 0.984375, prec 0.0802242, recall 0.815488
2017-12-10T03:46:36.272481: step 4561, loss 0.176672, acc 0.9375, prec 0.0802194, recall 0.815488
2017-12-10T03:46:36.537238: step 4562, loss 0.0740701, acc 0.96875, prec 0.0802584, recall 0.815573
2017-12-10T03:46:36.804914: step 4563, loss 0.149449, acc 0.9375, prec 0.0802536, recall 0.815573
2017-12-10T03:46:37.074981: step 4564, loss 0.501514, acc 0.921875, prec 0.0802614, recall 0.815601
2017-12-10T03:46:37.350113: step 4565, loss 0.376913, acc 0.953125, prec 0.0802716, recall 0.815629
2017-12-10T03:46:37.620367: step 4566, loss 0.120517, acc 0.921875, prec 0.0802656, recall 0.815629
2017-12-10T03:46:37.889149: step 4567, loss 0.399874, acc 0.90625, prec 0.0802721, recall 0.815657
2017-12-10T03:46:38.154708: step 4568, loss 0.300156, acc 0.984375, prec 0.0802847, recall 0.815685
2017-12-10T03:46:38.417963: step 4569, loss 0.177088, acc 0.9375, prec 0.0802937, recall 0.815713
2017-12-10T03:46:38.688868: step 4570, loss 0.18185, acc 0.953125, prec 0.0803039, recall 0.815741
2017-12-10T03:46:38.957590: step 4571, loss 0.162977, acc 0.984375, prec 0.0803027, recall 0.815741
2017-12-10T03:46:39.227283: step 4572, loss 0.129747, acc 0.9375, prec 0.0802979, recall 0.815741
2017-12-10T03:46:39.490969: step 4573, loss 0.327698, acc 0.96875, prec 0.0803093, recall 0.815769
2017-12-10T03:46:39.761078: step 4574, loss 0.0640293, acc 0.96875, prec 0.0803069, recall 0.815769
2017-12-10T03:46:40.028096: step 4575, loss 0.0656812, acc 0.953125, prec 0.0803033, recall 0.815769
2017-12-10T03:46:40.297472: step 4576, loss 0.194525, acc 0.96875, prec 0.0803147, recall 0.815798
2017-12-10T03:46:40.563508: step 4577, loss 0.171903, acc 0.96875, prec 0.0803122, recall 0.815798
2017-12-10T03:46:40.828463: step 4578, loss 0.0807452, acc 0.984375, prec 0.0803248, recall 0.815826
2017-12-10T03:46:41.088746: step 4579, loss 0.0621805, acc 0.984375, prec 0.0803512, recall 0.815882
2017-12-10T03:46:41.365204: step 4580, loss 0.0774513, acc 0.984375, prec 0.08035, recall 0.815882
2017-12-10T03:46:41.641306: step 4581, loss 0.000737853, acc 1, prec 0.08035, recall 0.815882
2017-12-10T03:46:41.917583: step 4582, loss 0.0700693, acc 0.96875, prec 0.0803614, recall 0.81591
2017-12-10T03:46:42.185195: step 4583, loss 0.057089, acc 0.984375, prec 0.0803602, recall 0.81591
2017-12-10T03:46:42.452542: step 4584, loss 0.258965, acc 0.96875, prec 0.0803578, recall 0.81591
2017-12-10T03:46:42.721815: step 4585, loss 0.308073, acc 1, prec 0.0803992, recall 0.815994
2017-12-10T03:46:42.987338: step 4586, loss 0.24685, acc 0.953125, prec 0.0804094, recall 0.816022
2017-12-10T03:46:43.255978: step 4587, loss 0.37585, acc 1, prec 0.0804508, recall 0.816106
2017-12-10T03:46:43.527289: step 4588, loss 0.28745, acc 1, prec 0.080506, recall 0.816218
2017-12-10T03:46:43.805336: step 4589, loss 0.119738, acc 0.984375, prec 0.0805324, recall 0.816274
2017-12-10T03:46:44.076940: step 4590, loss 0.201151, acc 0.984375, prec 0.080545, recall 0.816302
2017-12-10T03:46:44.352644: step 4591, loss 0.134911, acc 0.96875, prec 0.0805701, recall 0.816358
2017-12-10T03:46:44.616370: step 4592, loss 0.0643927, acc 0.953125, prec 0.0805941, recall 0.816413
2017-12-10T03:46:44.882185: step 4593, loss 0.0155025, acc 0.984375, prec 0.0805929, recall 0.816413
2017-12-10T03:46:45.152980: step 4594, loss 0.192477, acc 0.953125, prec 0.0806031, recall 0.816441
2017-12-10T03:46:45.423613: step 4595, loss 0.197058, acc 0.921875, prec 0.0806246, recall 0.816497
2017-12-10T03:46:45.690607: step 4596, loss 0.101037, acc 0.9375, prec 0.0806336, recall 0.816525
2017-12-10T03:46:45.965603: step 4597, loss 1.75517, acc 0.984375, prec 0.0806611, recall 0.816457
2017-12-10T03:46:46.240021: step 4598, loss 0.375196, acc 0.953125, prec 0.0806713, recall 0.816485
2017-12-10T03:46:46.507028: step 4599, loss 0.151112, acc 0.96875, prec 0.0806827, recall 0.816512
2017-12-10T03:46:46.770525: step 4600, loss 0.202033, acc 0.921875, prec 0.0806766, recall 0.816512
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-4600

2017-12-10T03:46:48.092515: step 4601, loss 0.269004, acc 0.953125, prec 0.0806868, recall 0.81654
2017-12-10T03:46:48.366442: step 4602, loss 0.26619, acc 0.9375, prec 0.0806957, recall 0.816568
2017-12-10T03:46:48.631139: step 4603, loss 0.164084, acc 0.921875, prec 0.0806897, recall 0.816568
2017-12-10T03:46:48.900371: step 4604, loss 0.168472, acc 0.96875, prec 0.080701, recall 0.816596
2017-12-10T03:46:49.174718: step 4605, loss 0.159688, acc 0.96875, prec 0.0807124, recall 0.816624
2017-12-10T03:46:49.447690: step 4606, loss 0.379253, acc 0.9375, prec 0.0807075, recall 0.816624
2017-12-10T03:46:49.713854: step 4607, loss 0.133547, acc 0.96875, prec 0.0807327, recall 0.816679
2017-12-10T03:46:49.981388: step 4608, loss 0.116173, acc 0.953125, prec 0.0807842, recall 0.81679
2017-12-10T03:46:50.251521: step 4609, loss 0.166839, acc 0.953125, prec 0.0808081, recall 0.816846
2017-12-10T03:46:50.527201: step 4610, loss 0.189531, acc 0.921875, prec 0.0808296, recall 0.816901
2017-12-10T03:46:50.796877: step 4611, loss 0.566937, acc 0.875, prec 0.0808474, recall 0.816957
2017-12-10T03:46:51.059980: step 4612, loss 0.0928541, acc 0.953125, prec 0.0808438, recall 0.816957
2017-12-10T03:46:51.325064: step 4613, loss 0.161372, acc 0.96875, prec 0.0808551, recall 0.816985
2017-12-10T03:46:51.593119: step 4614, loss 0.0967369, acc 0.953125, prec 0.0808653, recall 0.817012
2017-12-10T03:46:51.861078: step 4615, loss 0.213546, acc 0.9375, prec 0.080888, recall 0.817068
2017-12-10T03:46:52.127086: step 4616, loss 0.27329, acc 0.921875, prec 0.0808957, recall 0.817095
2017-12-10T03:46:52.399460: step 4617, loss 0.0873354, acc 0.953125, prec 0.0809196, recall 0.817151
2017-12-10T03:46:52.667463: step 4618, loss 0.00441218, acc 1, prec 0.0809196, recall 0.817151
2017-12-10T03:46:52.937658: step 4619, loss 0.0202021, acc 1, prec 0.0809471, recall 0.817206
2017-12-10T03:46:53.201595: step 4620, loss 0.197921, acc 0.96875, prec 0.0809722, recall 0.817261
2017-12-10T03:46:53.477227: step 4621, loss 0.116509, acc 0.953125, prec 0.0809961, recall 0.817316
2017-12-10T03:46:53.744371: step 4622, loss 0.0447309, acc 0.984375, prec 0.0810086, recall 0.817344
2017-12-10T03:46:54.010364: step 4623, loss 0.143496, acc 0.953125, prec 0.081005, recall 0.817344
2017-12-10T03:46:54.275525: step 4624, loss 0.0569017, acc 0.984375, prec 0.0810175, recall 0.817372
2017-12-10T03:46:54.550589: step 4625, loss 0.196597, acc 0.96875, prec 0.0810151, recall 0.817372
2017-12-10T03:46:54.825021: step 4626, loss 0.0507427, acc 1, prec 0.0810289, recall 0.817399
2017-12-10T03:46:55.095929: step 4627, loss 0.180954, acc 0.984375, prec 0.0810552, recall 0.817454
2017-12-10T03:46:55.365601: step 4628, loss 0.101737, acc 0.984375, prec 0.0810677, recall 0.817482
2017-12-10T03:46:55.631944: step 4629, loss 0.297478, acc 0.921875, prec 0.0810754, recall 0.817509
2017-12-10T03:46:55.901895: step 4630, loss 0.105904, acc 1, prec 0.0811029, recall 0.817564
2017-12-10T03:46:56.172183: step 4631, loss 0.030304, acc 0.984375, prec 0.0811155, recall 0.817592
2017-12-10T03:46:56.434689: step 4632, loss 0.058451, acc 0.984375, prec 0.0811143, recall 0.817592
2017-12-10T03:46:56.707154: step 4633, loss 0.154274, acc 0.96875, prec 0.0811393, recall 0.817647
2017-12-10T03:46:56.981117: step 4634, loss 0.406756, acc 0.984375, prec 0.0811656, recall 0.817702
2017-12-10T03:46:57.247867: step 4635, loss 0.907376, acc 0.984375, prec 0.0812057, recall 0.817784
2017-12-10T03:46:57.520235: step 4636, loss 0.0184684, acc 1, prec 0.0812057, recall 0.817784
2017-12-10T03:46:57.789534: step 4637, loss 0.0298418, acc 1, prec 0.0812332, recall 0.817839
2017-12-10T03:46:58.067411: step 4638, loss 0.0121805, acc 1, prec 0.0812469, recall 0.817867
2017-12-10T03:46:58.333188: step 4639, loss 0.0388348, acc 0.984375, prec 0.0812457, recall 0.817867
2017-12-10T03:46:58.599180: step 4640, loss 0.0327099, acc 0.984375, prec 0.0812445, recall 0.817867
2017-12-10T03:46:58.869056: step 4641, loss 0.170907, acc 0.9375, prec 0.0812396, recall 0.817867
2017-12-10T03:46:59.136088: step 4642, loss 0.217009, acc 0.953125, prec 0.0812772, recall 0.817949
2017-12-10T03:46:59.406352: step 4643, loss 0.129702, acc 0.96875, prec 0.0812748, recall 0.817949
2017-12-10T03:46:59.677519: step 4644, loss 0.263814, acc 0.921875, prec 0.0812687, recall 0.817949
2017-12-10T03:46:59.943052: step 4645, loss 0.852888, acc 0.90625, prec 0.0812752, recall 0.817977
2017-12-10T03:47:00.208155: step 4646, loss 0.106168, acc 0.953125, prec 0.0812852, recall 0.818004
2017-12-10T03:47:00.477964: step 4647, loss 0.0806931, acc 0.953125, prec 0.0812816, recall 0.818004
2017-12-10T03:47:00.744692: step 4648, loss 0.104411, acc 0.96875, prec 0.0812929, recall 0.818031
2017-12-10T03:47:01.014542: step 4649, loss 0.163945, acc 0.96875, prec 0.081318, recall 0.818086
2017-12-10T03:47:01.282962: step 4650, loss 0.284189, acc 0.921875, prec 0.0813256, recall 0.818113
2017-12-10T03:47:01.554103: step 4651, loss 0.284418, acc 0.921875, prec 0.0813195, recall 0.818113
2017-12-10T03:47:01.825093: step 4652, loss 0.406503, acc 0.953125, prec 0.0813434, recall 0.818168
2017-12-10T03:47:02.091728: step 4653, loss 0.0291775, acc 0.984375, prec 0.0813421, recall 0.818168
2017-12-10T03:47:02.362209: step 4654, loss 0.500737, acc 0.96875, prec 0.0813534, recall 0.818195
2017-12-10T03:47:02.634457: step 4655, loss 0.290794, acc 0.90625, prec 0.0813599, recall 0.818223
2017-12-10T03:47:02.903719: step 4656, loss 0.197201, acc 0.96875, prec 0.0813574, recall 0.818223
2017-12-10T03:47:03.172238: step 4657, loss 0.145819, acc 0.96875, prec 0.081355, recall 0.818223
2017-12-10T03:47:03.439978: step 4658, loss 0.0503759, acc 0.96875, prec 0.0813526, recall 0.818223
2017-12-10T03:47:03.705109: step 4659, loss 0.0421511, acc 0.984375, prec 0.0813651, recall 0.81825
2017-12-10T03:47:03.972026: step 4660, loss 0.0195566, acc 1, prec 0.0813651, recall 0.81825
2017-12-10T03:47:04.238689: step 4661, loss 0.246441, acc 0.953125, prec 0.0813615, recall 0.81825
2017-12-10T03:47:04.512136: step 4662, loss 0.0284238, acc 0.984375, prec 0.081374, recall 0.818277
2017-12-10T03:47:04.778199: step 4663, loss 0.0448492, acc 0.984375, prec 0.0813728, recall 0.818277
2017-12-10T03:47:05.041066: step 4664, loss 0.12118, acc 0.96875, prec 0.0813841, recall 0.818305
2017-12-10T03:47:05.303909: step 4665, loss 0.142429, acc 0.984375, prec 0.0813966, recall 0.818332
2017-12-10T03:47:05.569064: step 4666, loss 0.0777533, acc 0.96875, prec 0.0814353, recall 0.818414
2017-12-10T03:47:05.833664: step 4667, loss 0.0283332, acc 0.984375, prec 0.0814341, recall 0.818414
2017-12-10T03:47:06.103908: step 4668, loss 1.32308, acc 0.96875, prec 0.0814329, recall 0.818291
2017-12-10T03:47:06.379687: step 4669, loss 0.476308, acc 0.96875, prec 0.0814305, recall 0.818291
2017-12-10T03:47:06.644739: step 4670, loss 0.149645, acc 0.96875, prec 0.0814555, recall 0.818346
2017-12-10T03:47:06.916678: step 4671, loss 0.56419, acc 0.953125, prec 0.081493, recall 0.818427
2017-12-10T03:47:07.185943: step 4672, loss 0.176918, acc 0.96875, prec 0.0814906, recall 0.818427
2017-12-10T03:47:07.450881: step 4673, loss 0.0465209, acc 0.96875, prec 0.0814881, recall 0.818427
2017-12-10T03:47:07.713629: step 4674, loss 0.17324, acc 0.9375, prec 0.0815244, recall 0.818509
2017-12-10T03:47:07.975045: step 4675, loss 0.421634, acc 0.890625, prec 0.0815159, recall 0.818509
2017-12-10T03:47:08.249854: step 4676, loss 0.089794, acc 0.984375, prec 0.0815147, recall 0.818509
2017-12-10T03:47:08.517910: step 4677, loss 0.0438692, acc 0.984375, prec 0.0815272, recall 0.818536
2017-12-10T03:47:08.792210: step 4678, loss 0.112885, acc 0.953125, prec 0.081551, recall 0.818591
2017-12-10T03:47:09.055538: step 4679, loss 0.496406, acc 0.953125, prec 0.0815473, recall 0.818591
2017-12-10T03:47:09.326662: step 4680, loss 0.235331, acc 0.96875, prec 0.0815586, recall 0.818618
2017-12-10T03:47:09.593894: step 4681, loss 0.279305, acc 0.9375, prec 0.0815537, recall 0.818618
2017-12-10T03:47:09.866286: step 4682, loss 0.250875, acc 0.953125, prec 0.0815775, recall 0.818672
2017-12-10T03:47:10.140115: step 4683, loss 0.283928, acc 0.890625, prec 0.0815964, recall 0.818727
2017-12-10T03:47:10.400672: step 4684, loss 0.305601, acc 0.921875, prec 0.0815903, recall 0.818727
2017-12-10T03:47:10.668921: step 4685, loss 0.16453, acc 0.96875, prec 0.0816016, recall 0.818754
2017-12-10T03:47:10.939695: step 4686, loss 0.244045, acc 0.96875, prec 0.0816128, recall 0.818781
2017-12-10T03:47:11.203230: step 4687, loss 0.0811536, acc 0.96875, prec 0.0816515, recall 0.818862
2017-12-10T03:47:11.468662: step 4688, loss 0.011786, acc 1, prec 0.0816515, recall 0.818862
2017-12-10T03:47:11.738533: step 4689, loss 0.132637, acc 0.96875, prec 0.0816491, recall 0.818862
2017-12-10T03:47:12.008507: step 4690, loss 0.125719, acc 0.96875, prec 0.0816467, recall 0.818862
2017-12-10T03:47:12.282995: step 4691, loss 0.0693998, acc 0.984375, prec 0.0816592, recall 0.818889
2017-12-10T03:47:12.551574: step 4692, loss 0.0213509, acc 1, prec 0.0816866, recall 0.818944
2017-12-10T03:47:12.815835: step 4693, loss 0.177154, acc 0.96875, prec 0.0817115, recall 0.818998
2017-12-10T03:47:13.080601: step 4694, loss 0.0205864, acc 0.984375, prec 0.0817377, recall 0.819052
2017-12-10T03:47:13.352847: step 4695, loss 0.140418, acc 0.953125, prec 0.0817615, recall 0.819106
2017-12-10T03:47:13.620718: step 4696, loss 0.309827, acc 0.9375, prec 0.0817566, recall 0.819106
2017-12-10T03:47:13.893090: step 4697, loss 0.0677049, acc 0.984375, prec 0.0817691, recall 0.819133
2017-12-10T03:47:14.164315: step 4698, loss 0.0136566, acc 1, prec 0.0817691, recall 0.819133
2017-12-10T03:47:14.431855: step 4699, loss 0.0169533, acc 1, prec 0.0817828, recall 0.81916
2017-12-10T03:47:14.701384: step 4700, loss 0.192147, acc 0.984375, prec 0.081809, recall 0.819214
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-4700

2017-12-10T03:47:15.966117: step 4701, loss 0.130088, acc 0.96875, prec 0.0818339, recall 0.819268
2017-12-10T03:47:16.236096: step 4702, loss 0.00894578, acc 1, prec 0.0818339, recall 0.819268
2017-12-10T03:47:16.503302: step 4703, loss 1.98358, acc 0.953125, prec 0.0818315, recall 0.819146
2017-12-10T03:47:16.772036: step 4704, loss 0.0328997, acc 0.984375, prec 0.0818303, recall 0.819146
2017-12-10T03:47:17.039209: step 4705, loss 0.157966, acc 0.96875, prec 0.0818552, recall 0.8192
2017-12-10T03:47:17.307786: step 4706, loss 0.0678358, acc 0.984375, prec 0.0818677, recall 0.819227
2017-12-10T03:47:17.574396: step 4707, loss 4.81857, acc 0.921875, prec 0.0818628, recall 0.819104
2017-12-10T03:47:17.849188: step 4708, loss 0.0527335, acc 0.96875, prec 0.0818741, recall 0.819131
2017-12-10T03:47:18.117148: step 4709, loss 0.242189, acc 0.953125, prec 0.0818841, recall 0.819158
2017-12-10T03:47:18.385250: step 4710, loss 0.446995, acc 0.90625, prec 0.0818904, recall 0.819185
2017-12-10T03:47:18.647841: step 4711, loss 0.472398, acc 0.90625, prec 0.0818968, recall 0.819212
2017-12-10T03:47:18.913323: step 4712, loss 1.03448, acc 0.8125, prec 0.0818822, recall 0.819212
2017-12-10T03:47:19.187821: step 4713, loss 0.65445, acc 0.890625, prec 0.0819147, recall 0.819293
2017-12-10T03:47:19.452714: step 4714, loss 0.447293, acc 0.8125, prec 0.0819137, recall 0.81932
2017-12-10T03:47:19.717710: step 4715, loss 0.89335, acc 0.8125, prec 0.0819127, recall 0.819347
2017-12-10T03:47:19.985665: step 4716, loss 0.298194, acc 0.890625, prec 0.0819316, recall 0.819401
2017-12-10T03:47:20.247878: step 4717, loss 0.667909, acc 0.890625, prec 0.081923, recall 0.819401
2017-12-10T03:47:20.511709: step 4718, loss 0.672104, acc 0.84375, prec 0.0819245, recall 0.819428
2017-12-10T03:47:20.778790: step 4719, loss 0.48269, acc 0.890625, prec 0.0819433, recall 0.819482
2017-12-10T03:47:21.042831: step 4720, loss 0.654162, acc 0.796875, prec 0.0819411, recall 0.819509
2017-12-10T03:47:21.312103: step 4721, loss 0.481876, acc 0.890625, prec 0.0819462, recall 0.819535
2017-12-10T03:47:21.580103: step 4722, loss 0.491981, acc 0.859375, prec 0.0819352, recall 0.819535
2017-12-10T03:47:21.842994: step 4723, loss 0.344749, acc 0.890625, prec 0.0819814, recall 0.819643
2017-12-10T03:47:22.107578: step 4724, loss 0.17007, acc 0.953125, prec 0.082005, recall 0.819697
2017-12-10T03:47:22.371750: step 4725, loss 0.469018, acc 0.90625, prec 0.0820114, recall 0.819723
2017-12-10T03:47:22.637613: step 4726, loss 0.290046, acc 0.921875, prec 0.0820189, recall 0.81975
2017-12-10T03:47:22.903008: step 4727, loss 0.204158, acc 0.921875, prec 0.0820265, recall 0.819777
2017-12-10T03:47:23.169562: step 4728, loss 0.0154464, acc 1, prec 0.0820265, recall 0.819777
2017-12-10T03:47:23.436377: step 4729, loss 0.134954, acc 0.953125, prec 0.0820638, recall 0.819857
2017-12-10T03:47:23.711833: step 4730, loss 0.115729, acc 0.9375, prec 0.0820862, recall 0.819911
2017-12-10T03:47:23.982499: step 4731, loss 0.0565956, acc 0.96875, prec 0.0821111, recall 0.819964
2017-12-10T03:47:24.247338: step 4732, loss 0.311388, acc 0.96875, prec 0.0821086, recall 0.819964
2017-12-10T03:47:24.511118: step 4733, loss 0.0329527, acc 0.984375, prec 0.0821074, recall 0.819964
2017-12-10T03:47:24.774006: step 4734, loss 0.114826, acc 0.984375, prec 0.0821062, recall 0.819964
2017-12-10T03:47:25.036319: step 4735, loss 0.108179, acc 0.96875, prec 0.0821174, recall 0.819991
2017-12-10T03:47:25.305771: step 4736, loss 0.480888, acc 0.984375, prec 0.0821435, recall 0.820045
2017-12-10T03:47:25.572665: step 4737, loss 0.063817, acc 0.984375, prec 0.0821423, recall 0.820045
2017-12-10T03:47:25.841086: step 4738, loss 0.0178511, acc 0.984375, prec 0.082141, recall 0.820045
2017-12-10T03:47:26.113293: step 4739, loss 2.88874, acc 0.984375, prec 0.0821547, recall 0.81995
2017-12-10T03:47:26.382092: step 4740, loss 0.10557, acc 0.984375, prec 0.0821808, recall 0.820003
2017-12-10T03:47:26.651788: step 4741, loss 0.07761, acc 0.984375, prec 0.0821796, recall 0.820003
2017-12-10T03:47:26.925865: step 4742, loss 0.0825499, acc 0.953125, prec 0.0822032, recall 0.820056
2017-12-10T03:47:27.196505: step 4743, loss 0.0197524, acc 0.984375, prec 0.082202, recall 0.820056
2017-12-10T03:47:27.466760: step 4744, loss 0.123946, acc 0.953125, prec 0.0821983, recall 0.820056
2017-12-10T03:47:27.729831: step 4745, loss 0.158789, acc 0.921875, prec 0.0822058, recall 0.820083
2017-12-10T03:47:27.993738: step 4746, loss 0.0499654, acc 0.984375, prec 0.0822183, recall 0.82011
2017-12-10T03:47:28.261696: step 4747, loss 0.372603, acc 0.921875, prec 0.0822258, recall 0.820136
2017-12-10T03:47:28.527024: step 4748, loss 0.10998, acc 0.953125, prec 0.0822221, recall 0.820136
2017-12-10T03:47:28.794507: step 4749, loss 0.321336, acc 0.9375, prec 0.0822309, recall 0.820163
2017-12-10T03:47:29.059685: step 4750, loss 0.0699683, acc 0.953125, prec 0.0822408, recall 0.82019
2017-12-10T03:47:29.337109: step 4751, loss 0.181818, acc 0.953125, prec 0.0822508, recall 0.820216
2017-12-10T03:47:29.611523: step 4752, loss 0.133499, acc 0.953125, prec 0.0822472, recall 0.820216
2017-12-10T03:47:29.878284: step 4753, loss 0.305528, acc 0.890625, prec 0.0822386, recall 0.820216
2017-12-10T03:47:30.146228: step 4754, loss 0.519227, acc 0.90625, prec 0.0822313, recall 0.820216
2017-12-10T03:47:30.412969: step 4755, loss 0.345198, acc 0.9375, prec 0.08224, recall 0.820243
2017-12-10T03:47:30.681092: step 4756, loss 0.199965, acc 0.953125, prec 0.0822364, recall 0.820243
2017-12-10T03:47:30.943438: step 4757, loss 0.0161299, acc 1, prec 0.0822364, recall 0.820243
2017-12-10T03:47:31.210956: step 4758, loss 0.195343, acc 0.9375, prec 0.0822587, recall 0.820296
2017-12-10T03:47:31.474232: step 4759, loss 0.258789, acc 0.890625, prec 0.0822502, recall 0.820296
2017-12-10T03:47:31.739775: step 4760, loss 0.256035, acc 0.953125, prec 0.0822601, recall 0.820323
2017-12-10T03:47:32.015193: step 4761, loss 0.222962, acc 0.953125, prec 0.0822837, recall 0.820376
2017-12-10T03:47:32.282565: step 4762, loss 0.198591, acc 0.96875, prec 0.0822949, recall 0.820403
2017-12-10T03:47:32.546299: step 4763, loss 0.0109043, acc 1, prec 0.0822949, recall 0.820403
2017-12-10T03:47:32.810524: step 4764, loss 0.0726016, acc 0.984375, prec 0.0822937, recall 0.820403
2017-12-10T03:47:33.088305: step 4765, loss 0.0517207, acc 0.984375, prec 0.0822925, recall 0.820403
2017-12-10T03:47:33.356398: step 4766, loss 0.0432549, acc 0.96875, prec 0.08229, recall 0.820403
2017-12-10T03:47:33.620889: step 4767, loss 0.00809493, acc 1, prec 0.08229, recall 0.820403
2017-12-10T03:47:33.884331: step 4768, loss 0.0527391, acc 0.984375, prec 0.0822888, recall 0.820403
2017-12-10T03:47:34.152061: step 4769, loss 0.0311725, acc 0.984375, prec 0.0822876, recall 0.820403
2017-12-10T03:47:34.416245: step 4770, loss 0.0212556, acc 0.984375, prec 0.0822864, recall 0.820403
2017-12-10T03:47:34.680450: step 4771, loss 0.588983, acc 1, prec 0.0823, recall 0.820429
2017-12-10T03:47:34.949581: step 4772, loss 0.100497, acc 0.96875, prec 0.0822976, recall 0.820429
2017-12-10T03:47:35.216445: step 4773, loss 0.364545, acc 0.9375, prec 0.0823063, recall 0.820456
2017-12-10T03:47:35.483574: step 4774, loss 0.0108222, acc 1, prec 0.0823063, recall 0.820456
2017-12-10T03:47:35.754416: step 4775, loss 0.260716, acc 0.984375, prec 0.0823187, recall 0.820482
2017-12-10T03:47:36.023242: step 4776, loss 0.99627, acc 0.984375, prec 0.0823311, recall 0.820509
2017-12-10T03:47:36.290693: step 4777, loss 0.0416221, acc 0.984375, prec 0.0823571, recall 0.820562
2017-12-10T03:47:36.566000: step 4778, loss 0.327765, acc 0.921875, prec 0.0823646, recall 0.820589
2017-12-10T03:47:36.834314: step 4779, loss 0.370693, acc 0.921875, prec 0.0823585, recall 0.820589
2017-12-10T03:47:37.100134: step 4780, loss 0.092688, acc 0.96875, prec 0.0823697, recall 0.820615
2017-12-10T03:47:37.368753: step 4781, loss 0.315299, acc 0.953125, prec 0.0823797, recall 0.820642
2017-12-10T03:47:37.636200: step 4782, loss 0.205319, acc 0.984375, prec 0.0824057, recall 0.820695
2017-12-10T03:47:37.902377: step 4783, loss 0.0437723, acc 0.984375, prec 0.0824181, recall 0.820721
2017-12-10T03:47:38.172122: step 4784, loss 0.583237, acc 0.921875, prec 0.0824392, recall 0.820774
2017-12-10T03:47:38.436436: step 4785, loss 0.134575, acc 0.96875, prec 0.0824504, recall 0.820801
2017-12-10T03:47:38.706562: step 4786, loss 0.598742, acc 0.90625, prec 0.082443, recall 0.820801
2017-12-10T03:47:38.972130: step 4787, loss 0.208253, acc 0.921875, prec 0.0824369, recall 0.820801
2017-12-10T03:47:39.235221: step 4788, loss 0.11581, acc 0.984375, prec 0.0824493, recall 0.820827
2017-12-10T03:47:39.501482: step 4789, loss 0.0385171, acc 0.984375, prec 0.0824481, recall 0.820827
2017-12-10T03:47:39.774038: step 4790, loss 0.113232, acc 0.96875, prec 0.0824729, recall 0.82088
2017-12-10T03:47:40.045123: step 4791, loss 0.113775, acc 0.953125, prec 0.0824964, recall 0.820933
2017-12-10T03:47:40.309770: step 4792, loss 0.120236, acc 0.953125, prec 0.0824927, recall 0.820933
2017-12-10T03:47:40.584540: step 4793, loss 0.559996, acc 0.90625, prec 0.082499, recall 0.820959
2017-12-10T03:47:40.847056: step 4794, loss 0.667121, acc 0.890625, prec 0.082504, recall 0.820986
2017-12-10T03:47:41.110952: step 4795, loss 0.529831, acc 0.90625, prec 0.0825783, recall 0.821144
2017-12-10T03:47:41.378491: step 4796, loss 0.185687, acc 0.96875, prec 0.0825759, recall 0.821144
2017-12-10T03:47:41.648684: step 4797, loss 0.124408, acc 0.953125, prec 0.0825722, recall 0.821144
2017-12-10T03:47:41.922746: step 4798, loss 0.101268, acc 0.96875, prec 0.0826106, recall 0.821223
2017-12-10T03:47:42.188211: step 4799, loss 0.0225296, acc 1, prec 0.0826242, recall 0.82125
2017-12-10T03:47:42.454400: step 4800, loss 0.157945, acc 0.921875, prec 0.082618, recall 0.82125

Evaluation:
2017-12-10T03:47:50.077562: step 4800, loss 6.20077, acc 0.966978, prec 0.0829668, recall 0.810293

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-4800

2017-12-10T03:47:51.299701: step 4801, loss 0.0482101, acc 0.984375, prec 0.0829656, recall 0.810293
2017-12-10T03:47:51.567078: step 4802, loss 0.13074, acc 0.96875, prec 0.0829631, recall 0.810293
2017-12-10T03:47:51.841373: step 4803, loss 0.00391242, acc 1, prec 0.0829902, recall 0.810347
2017-12-10T03:47:52.104113: step 4804, loss 0.00317493, acc 1, prec 0.0829902, recall 0.810347
2017-12-10T03:47:52.370482: step 4805, loss 0.123009, acc 0.984375, prec 0.082989, recall 0.810347
2017-12-10T03:47:52.637324: step 4806, loss 0.0412866, acc 0.984375, prec 0.0830148, recall 0.810402
2017-12-10T03:47:52.899719: step 4807, loss 0.00391556, acc 1, prec 0.0830148, recall 0.810402
2017-12-10T03:47:53.167091: step 4808, loss 0.133693, acc 0.96875, prec 0.0830123, recall 0.810402
2017-12-10T03:47:53.437867: step 4809, loss 0.0145392, acc 1, prec 0.0830123, recall 0.810402
2017-12-10T03:47:53.704033: step 4810, loss 0.228639, acc 0.96875, prec 0.0830099, recall 0.810402
2017-12-10T03:47:53.971625: step 4811, loss 1.07477, acc 0.984375, prec 0.0830222, recall 0.810429
2017-12-10T03:47:54.245978: step 4812, loss 0.0287345, acc 0.96875, prec 0.0830198, recall 0.810429
2017-12-10T03:47:54.514633: step 4813, loss 0.00338651, acc 1, prec 0.0830198, recall 0.810429
2017-12-10T03:47:54.780630: step 4814, loss 5.6849, acc 0.96875, prec 0.0830456, recall 0.810367
2017-12-10T03:47:55.054580: step 4815, loss 0.262183, acc 0.984375, prec 0.0830579, recall 0.810394
2017-12-10T03:47:55.322296: step 4816, loss 0.160204, acc 0.96875, prec 0.0830555, recall 0.810394
2017-12-10T03:47:55.591432: step 4817, loss 0.254741, acc 0.953125, prec 0.0830518, recall 0.810394
2017-12-10T03:47:55.861533: step 4818, loss 0.0951016, acc 0.984375, prec 0.0830776, recall 0.810449
2017-12-10T03:47:56.133212: step 4819, loss 0.204877, acc 0.96875, prec 0.0830752, recall 0.810449
2017-12-10T03:47:56.416118: step 4820, loss 0.245066, acc 0.9375, prec 0.0830702, recall 0.810449
2017-12-10T03:47:56.692301: step 4821, loss 0.226447, acc 0.890625, prec 0.0830617, recall 0.810449
2017-12-10T03:47:56.961176: step 4822, loss 0.721861, acc 0.828125, prec 0.0830752, recall 0.810504
2017-12-10T03:47:57.227073: step 4823, loss 0.433997, acc 0.90625, prec 0.0830814, recall 0.810531
2017-12-10T03:47:57.496426: step 4824, loss 0.291865, acc 0.890625, prec 0.0830999, recall 0.810585
2017-12-10T03:47:57.767373: step 4825, loss 0.754849, acc 0.828125, prec 0.0830864, recall 0.810585
2017-12-10T03:47:58.031792: step 4826, loss 0.415029, acc 0.890625, prec 0.0830913, recall 0.810613
2017-12-10T03:47:58.303158: step 4827, loss 0.682478, acc 0.84375, prec 0.0830791, recall 0.810613
2017-12-10T03:47:58.570284: step 4828, loss 0.732223, acc 0.84375, prec 0.0830939, recall 0.810667
2017-12-10T03:47:58.833259: step 4829, loss 0.780755, acc 0.875, prec 0.0830841, recall 0.810667
2017-12-10T03:47:59.106007: step 4830, loss 0.289136, acc 0.921875, prec 0.083105, recall 0.810721
2017-12-10T03:47:59.371966: step 4831, loss 0.309266, acc 0.90625, prec 0.0831111, recall 0.810749
2017-12-10T03:47:59.639384: step 4832, loss 0.219235, acc 0.953125, prec 0.0831075, recall 0.810749
2017-12-10T03:47:59.911768: step 4833, loss 0.193153, acc 0.9375, prec 0.0831026, recall 0.810749
2017-12-10T03:48:00.183599: step 4834, loss 0.333077, acc 0.96875, prec 0.0831271, recall 0.810803
2017-12-10T03:48:00.462840: step 4835, loss 0.0569823, acc 0.96875, prec 0.0831517, recall 0.810857
2017-12-10T03:48:00.731124: step 4836, loss 0.0508226, acc 0.96875, prec 0.0831492, recall 0.810857
2017-12-10T03:48:01.007709: step 4837, loss 0.124788, acc 0.953125, prec 0.0831456, recall 0.810857
2017-12-10T03:48:01.272355: step 4838, loss 0.136667, acc 0.96875, prec 0.0831701, recall 0.810912
2017-12-10T03:48:01.537256: step 4839, loss 0.185147, acc 0.984375, prec 0.0831689, recall 0.810912
2017-12-10T03:48:01.802231: step 4840, loss 0.092091, acc 0.984375, prec 0.0831812, recall 0.810939
2017-12-10T03:48:02.076112: step 4841, loss 0.276132, acc 0.9375, prec 0.0831898, recall 0.810966
2017-12-10T03:48:02.342134: step 4842, loss 0.187033, acc 0.984375, prec 0.083202, recall 0.810993
2017-12-10T03:48:02.614080: step 4843, loss 0.112075, acc 0.96875, prec 0.0831996, recall 0.810993
2017-12-10T03:48:02.887811: step 4844, loss 8.98762, acc 0.96875, prec 0.0832119, recall 0.810904
2017-12-10T03:48:03.167990: step 4845, loss 0.884131, acc 0.921875, prec 0.0832327, recall 0.810958
2017-12-10T03:48:03.443346: step 4846, loss 0.114518, acc 0.96875, prec 0.0832438, recall 0.810985
2017-12-10T03:48:03.720027: step 4847, loss 0.0869843, acc 0.984375, prec 0.0832561, recall 0.811012
2017-12-10T03:48:03.988060: step 4848, loss 0.188169, acc 0.921875, prec 0.0832769, recall 0.811067
2017-12-10T03:48:04.262647: step 4849, loss 0.220601, acc 0.921875, prec 0.0832708, recall 0.811067
2017-12-10T03:48:04.530016: step 4850, loss 0.56887, acc 0.828125, prec 0.0832573, recall 0.811067
2017-12-10T03:48:04.798399: step 4851, loss 0.405754, acc 0.875, prec 0.0832475, recall 0.811067
2017-12-10T03:48:05.064209: step 4852, loss 0.640711, acc 0.796875, prec 0.0832316, recall 0.811067
2017-12-10T03:48:05.332590: step 4853, loss 0.669406, acc 0.78125, prec 0.0832144, recall 0.811067
2017-12-10T03:48:05.599702: step 4854, loss 0.944337, acc 0.734375, prec 0.0831936, recall 0.811067
2017-12-10T03:48:05.862698: step 4855, loss 0.727455, acc 0.875, prec 0.0831973, recall 0.811094
2017-12-10T03:48:06.131224: step 4856, loss 0.451602, acc 0.90625, prec 0.0832035, recall 0.811121
2017-12-10T03:48:06.390671: step 4857, loss 0.330747, acc 0.90625, prec 0.0832231, recall 0.811175
2017-12-10T03:48:06.658584: step 4858, loss 0.362642, acc 0.875, prec 0.0832268, recall 0.811202
2017-12-10T03:48:06.927432: step 4859, loss 0.968547, acc 0.84375, prec 0.0832146, recall 0.811202
2017-12-10T03:48:07.191027: step 4860, loss 0.158497, acc 0.921875, prec 0.0832489, recall 0.811283
2017-12-10T03:48:07.456762: step 4861, loss 0.589398, acc 0.890625, prec 0.0832672, recall 0.811337
2017-12-10T03:48:07.727696: step 4862, loss 0.552858, acc 0.90625, prec 0.0832599, recall 0.811337
2017-12-10T03:48:07.992208: step 4863, loss 0.242959, acc 0.9375, prec 0.0832685, recall 0.811364
2017-12-10T03:48:08.262282: step 4864, loss 0.0684908, acc 0.953125, prec 0.0832648, recall 0.811364
2017-12-10T03:48:08.531024: step 4865, loss 0.186286, acc 0.953125, prec 0.0832881, recall 0.811418
2017-12-10T03:48:08.805329: step 4866, loss 0.164275, acc 0.921875, prec 0.0832819, recall 0.811418
2017-12-10T03:48:09.075312: step 4867, loss 0.0237508, acc 1, prec 0.0832954, recall 0.811445
2017-12-10T03:48:09.346905: step 4868, loss 0.0265177, acc 0.984375, prec 0.0833076, recall 0.811472
2017-12-10T03:48:09.613974: step 4869, loss 1.37812, acc 0.953125, prec 0.0833052, recall 0.811356
2017-12-10T03:48:09.888882: step 4870, loss 0.421891, acc 0.921875, prec 0.0833125, recall 0.811383
2017-12-10T03:48:10.153516: step 4871, loss 0.204877, acc 0.96875, prec 0.0833235, recall 0.81141
2017-12-10T03:48:10.419419: step 4872, loss 0.0490232, acc 0.953125, prec 0.0833199, recall 0.81141
2017-12-10T03:48:10.695224: step 4873, loss 0.0830843, acc 0.953125, prec 0.0833162, recall 0.81141
2017-12-10T03:48:10.960434: step 4874, loss 0.303678, acc 0.953125, prec 0.0833125, recall 0.81141
2017-12-10T03:48:11.228458: step 4875, loss 0.322825, acc 0.90625, prec 0.0833187, recall 0.811437
2017-12-10T03:48:11.497882: step 4876, loss 0.0935767, acc 0.953125, prec 0.0833284, recall 0.811464
2017-12-10T03:48:11.776331: step 4877, loss 0.0836953, acc 0.96875, prec 0.0833394, recall 0.811491
2017-12-10T03:48:12.042285: step 4878, loss 0.347815, acc 0.96875, prec 0.0833505, recall 0.811518
2017-12-10T03:48:12.307602: step 4879, loss 0.133546, acc 0.96875, prec 0.0833615, recall 0.811544
2017-12-10T03:48:12.576229: step 4880, loss 0.556191, acc 0.921875, prec 0.0833553, recall 0.811544
2017-12-10T03:48:12.847089: step 4881, loss 0.251645, acc 0.96875, prec 0.0833663, recall 0.811571
2017-12-10T03:48:13.114288: step 4882, loss 0.0695172, acc 0.953125, prec 0.0833761, recall 0.811598
2017-12-10T03:48:13.373621: step 4883, loss 0.00813886, acc 1, prec 0.0833761, recall 0.811598
2017-12-10T03:48:13.643824: step 4884, loss 0.251086, acc 0.953125, prec 0.0833725, recall 0.811598
2017-12-10T03:48:13.904761: step 4885, loss 0.285199, acc 0.921875, prec 0.0833663, recall 0.811598
2017-12-10T03:48:14.168850: step 4886, loss 0.0981568, acc 0.96875, prec 0.0833908, recall 0.811652
2017-12-10T03:48:14.437578: step 4887, loss 0.00375497, acc 1, prec 0.0833908, recall 0.811652
2017-12-10T03:48:14.710021: step 4888, loss 0.0159421, acc 0.984375, prec 0.0834165, recall 0.811706
2017-12-10T03:48:14.978216: step 4889, loss 0.138054, acc 0.953125, prec 0.0834397, recall 0.81176
2017-12-10T03:48:15.244919: step 4890, loss 0.088663, acc 0.984375, prec 0.0834385, recall 0.81176
2017-12-10T03:48:15.513863: step 4891, loss 0.0502563, acc 0.984375, prec 0.0834372, recall 0.81176
2017-12-10T03:48:15.777256: step 4892, loss 0.0292601, acc 0.984375, prec 0.083436, recall 0.81176
2017-12-10T03:48:16.047596: step 4893, loss 0.186985, acc 0.96875, prec 0.0834605, recall 0.811813
2017-12-10T03:48:16.313767: step 4894, loss 13.8324, acc 0.9375, prec 0.0834568, recall 0.811698
2017-12-10T03:48:16.582643: step 4895, loss 0.0296222, acc 1, prec 0.0834702, recall 0.811724
2017-12-10T03:48:16.854344: step 4896, loss 0.0918679, acc 0.96875, prec 0.0834947, recall 0.811778
2017-12-10T03:48:17.126760: step 4897, loss 0.0172685, acc 1, prec 0.0834947, recall 0.811778
2017-12-10T03:48:17.400980: step 4898, loss 0.112497, acc 0.953125, prec 0.0835044, recall 0.811805
2017-12-10T03:48:17.675414: step 4899, loss 0.0554993, acc 0.96875, prec 0.0835423, recall 0.811885
2017-12-10T03:48:17.946583: step 4900, loss 0.256008, acc 0.953125, prec 0.0835655, recall 0.811939
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-4900

2017-12-10T03:48:19.321613: step 4901, loss 0.802348, acc 0.921875, prec 0.0835728, recall 0.811966
2017-12-10T03:48:19.594270: step 4902, loss 0.400303, acc 0.921875, prec 0.0835667, recall 0.811966
2017-12-10T03:48:19.857985: step 4903, loss 0.615379, acc 0.875, prec 0.0835569, recall 0.811966
2017-12-10T03:48:20.127102: step 4904, loss 0.0736919, acc 0.96875, prec 0.0835544, recall 0.811966
2017-12-10T03:48:20.393355: step 4905, loss 0.949623, acc 0.90625, prec 0.0835471, recall 0.811966
2017-12-10T03:48:20.659807: step 4906, loss 0.247651, acc 0.921875, prec 0.083541, recall 0.811966
2017-12-10T03:48:20.926566: step 4907, loss 0.160948, acc 0.9375, prec 0.0835361, recall 0.811966
2017-12-10T03:48:21.188338: step 4908, loss 0.663238, acc 0.84375, prec 0.0835238, recall 0.811966
2017-12-10T03:48:21.459230: step 4909, loss 0.461354, acc 0.90625, prec 0.0835165, recall 0.811966
2017-12-10T03:48:21.723937: step 4910, loss 0.759557, acc 0.84375, prec 0.0835311, recall 0.812019
2017-12-10T03:48:21.993597: step 4911, loss 0.456848, acc 0.890625, prec 0.083536, recall 0.812046
2017-12-10T03:48:22.267524: step 4912, loss 0.423165, acc 0.921875, prec 0.0835298, recall 0.812046
2017-12-10T03:48:22.532261: step 4913, loss 0.186154, acc 0.953125, prec 0.0835396, recall 0.812073
2017-12-10T03:48:22.800550: step 4914, loss 0.121014, acc 0.9375, prec 0.0835481, recall 0.8121
2017-12-10T03:48:23.069330: step 4915, loss 0.133043, acc 0.96875, prec 0.0835457, recall 0.8121
2017-12-10T03:48:23.341169: step 4916, loss 0.100053, acc 0.96875, prec 0.0835432, recall 0.8121
2017-12-10T03:48:23.608101: step 4917, loss 0.120254, acc 0.96875, prec 0.0835408, recall 0.8121
2017-12-10T03:48:23.881289: step 4918, loss 0.0278911, acc 1, prec 0.0835542, recall 0.812126
2017-12-10T03:48:24.145772: step 4919, loss 0.106949, acc 0.96875, prec 0.0835786, recall 0.81218
2017-12-10T03:48:24.412423: step 4920, loss 0.190106, acc 0.96875, prec 0.0835761, recall 0.81218
2017-12-10T03:48:24.681041: step 4921, loss 5.27708, acc 0.953125, prec 0.0835871, recall 0.812091
2017-12-10T03:48:24.954402: step 4922, loss 0.233654, acc 0.984375, prec 0.0836127, recall 0.812144
2017-12-10T03:48:25.219787: step 4923, loss 0.114211, acc 0.96875, prec 0.0836237, recall 0.812171
2017-12-10T03:48:25.484675: step 4924, loss 0.0734678, acc 0.96875, prec 0.0836347, recall 0.812198
2017-12-10T03:48:25.746303: step 4925, loss 2.56192, acc 0.90625, prec 0.0836688, recall 0.812163
2017-12-10T03:48:26.011971: step 4926, loss 0.585386, acc 0.84375, prec 0.0836699, recall 0.812189
2017-12-10T03:48:26.278058: step 4927, loss 0.591489, acc 0.875, prec 0.083687, recall 0.812243
2017-12-10T03:48:26.545934: step 4928, loss 0.949661, acc 0.828125, prec 0.0836869, recall 0.812269
2017-12-10T03:48:26.821227: step 4929, loss 1.1973, acc 0.6875, prec 0.0836624, recall 0.812269
2017-12-10T03:48:27.083960: step 4930, loss 0.997198, acc 0.703125, prec 0.0836392, recall 0.812269
2017-12-10T03:48:27.348669: step 4931, loss 0.612981, acc 0.84375, prec 0.0836537, recall 0.812323
2017-12-10T03:48:27.611322: step 4932, loss 1.06796, acc 0.71875, prec 0.0836317, recall 0.812323
2017-12-10T03:48:27.877771: step 4933, loss 0.874103, acc 0.75, prec 0.0836122, recall 0.812323
2017-12-10T03:48:28.141178: step 4934, loss 0.858195, acc 0.78125, prec 0.0836085, recall 0.812349
2017-12-10T03:48:28.409219: step 4935, loss 0.843611, acc 0.765625, prec 0.0835902, recall 0.812349
2017-12-10T03:48:28.684294: step 4936, loss 0.683605, acc 0.78125, prec 0.0835731, recall 0.812349
2017-12-10T03:48:28.950335: step 4937, loss 0.32022, acc 0.921875, prec 0.083567, recall 0.812349
2017-12-10T03:48:29.214438: step 4938, loss 0.689969, acc 0.84375, prec 0.0835548, recall 0.812349
2017-12-10T03:48:29.482153: step 4939, loss 0.647338, acc 0.859375, prec 0.0835839, recall 0.812429
2017-12-10T03:48:29.748137: step 4940, loss 3.51014, acc 0.890625, prec 0.08359, recall 0.81234
2017-12-10T03:48:30.013530: step 4941, loss 0.174987, acc 0.90625, prec 0.0836228, recall 0.81242
2017-12-10T03:48:30.287344: step 4942, loss 0.201888, acc 0.9375, prec 0.0836313, recall 0.812447
2017-12-10T03:48:30.564125: step 4943, loss 0.172515, acc 0.921875, prec 0.0836252, recall 0.812447
2017-12-10T03:48:30.823848: step 4944, loss 0.545273, acc 0.90625, prec 0.0836178, recall 0.812447
2017-12-10T03:48:31.090320: step 4945, loss 0.0978506, acc 0.96875, prec 0.0836421, recall 0.8125
2017-12-10T03:48:31.357367: step 4946, loss 0.380119, acc 0.875, prec 0.0836324, recall 0.8125
2017-12-10T03:48:31.626407: step 4947, loss 0.355084, acc 0.921875, prec 0.0836263, recall 0.8125
2017-12-10T03:48:31.889085: step 4948, loss 0.375814, acc 0.9375, prec 0.0836481, recall 0.812553
2017-12-10T03:48:32.155359: step 4949, loss 0.0490588, acc 0.984375, prec 0.0836469, recall 0.812553
2017-12-10T03:48:32.417036: step 4950, loss 1.12696, acc 0.9375, prec 0.0836688, recall 0.812606
2017-12-10T03:48:32.684274: step 4951, loss 0.117395, acc 0.953125, prec 0.0836918, recall 0.812659
2017-12-10T03:48:32.952357: step 4952, loss 0.21542, acc 0.96875, prec 0.0837161, recall 0.812712
2017-12-10T03:48:33.223528: step 4953, loss 0.249508, acc 0.9375, prec 0.0837246, recall 0.812739
2017-12-10T03:48:33.489453: step 4954, loss 0.178021, acc 0.9375, prec 0.0837464, recall 0.812792
2017-12-10T03:48:33.755670: step 4955, loss 0.397537, acc 0.953125, prec 0.0837561, recall 0.812818
2017-12-10T03:48:34.019928: step 4956, loss 0.0872489, acc 0.953125, prec 0.0837658, recall 0.812845
2017-12-10T03:48:34.289943: step 4957, loss 0.0806881, acc 0.96875, prec 0.0837634, recall 0.812845
2017-12-10T03:48:34.555542: step 4958, loss 0.312661, acc 0.921875, prec 0.0837573, recall 0.812845
2017-12-10T03:48:34.819607: step 4959, loss 0.317498, acc 0.9375, prec 0.0837657, recall 0.812871
2017-12-10T03:48:35.085324: step 4960, loss 0.173473, acc 0.9375, prec 0.0837742, recall 0.812898
2017-12-10T03:48:35.353226: step 4961, loss 0.0140642, acc 1, prec 0.0837876, recall 0.812924
2017-12-10T03:48:35.626282: step 4962, loss 0.194852, acc 0.9375, prec 0.083796, recall 0.812951
2017-12-10T03:48:35.892564: step 4963, loss 0.25427, acc 0.96875, prec 0.0838203, recall 0.813004
2017-12-10T03:48:36.165718: step 4964, loss 0.402965, acc 0.953125, prec 0.08383, recall 0.81303
2017-12-10T03:48:36.433601: step 4965, loss 0.00498951, acc 1, prec 0.08387, recall 0.813109
2017-12-10T03:48:36.700761: step 4966, loss 0.277611, acc 0.96875, prec 0.0838809, recall 0.813136
2017-12-10T03:48:36.970677: step 4967, loss 0.234177, acc 0.953125, prec 0.083904, recall 0.813188
2017-12-10T03:48:37.247350: step 4968, loss 2.13277, acc 0.9375, prec 0.083927, recall 0.813126
2017-12-10T03:48:37.519366: step 4969, loss 0.275643, acc 0.953125, prec 0.0839233, recall 0.813126
2017-12-10T03:48:37.755439: step 4970, loss 0.313287, acc 0.960784, prec 0.0839342, recall 0.813153
2017-12-10T03:48:38.040313: step 4971, loss 0.0711761, acc 0.984375, prec 0.083933, recall 0.813153
2017-12-10T03:48:38.307443: step 4972, loss 0.269743, acc 0.953125, prec 0.0839427, recall 0.813179
2017-12-10T03:48:38.572567: step 4973, loss 0.474786, acc 0.90625, prec 0.0839487, recall 0.813205
2017-12-10T03:48:38.834925: step 4974, loss 0.386371, acc 0.890625, prec 0.0839401, recall 0.813205
2017-12-10T03:48:39.112340: step 4975, loss 0.116229, acc 0.96875, prec 0.0839377, recall 0.813205
2017-12-10T03:48:39.373592: step 4976, loss 0.0152683, acc 1, prec 0.0839377, recall 0.813205
2017-12-10T03:48:39.637308: step 4977, loss 0.254282, acc 0.953125, prec 0.0839473, recall 0.813232
2017-12-10T03:48:39.904047: step 4978, loss 0.0174153, acc 0.984375, prec 0.0839728, recall 0.813284
2017-12-10T03:48:40.174383: step 4979, loss 0.32845, acc 0.90625, prec 0.0839655, recall 0.813284
2017-12-10T03:48:40.440244: step 4980, loss 0.408871, acc 0.9375, prec 0.0839872, recall 0.813337
2017-12-10T03:48:40.716798: step 4981, loss 0.516633, acc 0.90625, prec 0.0839799, recall 0.813337
2017-12-10T03:48:40.983444: step 4982, loss 0.18332, acc 0.953125, prec 0.0840162, recall 0.813416
2017-12-10T03:48:41.249227: step 4983, loss 0.559169, acc 0.875, prec 0.0840065, recall 0.813416
2017-12-10T03:48:41.510580: step 4984, loss 0.284528, acc 0.9375, prec 0.0840149, recall 0.813442
2017-12-10T03:48:41.791221: step 4985, loss 0.36794, acc 0.9375, prec 0.0840233, recall 0.813469
2017-12-10T03:48:42.060562: step 4986, loss 0.665573, acc 0.96875, prec 0.0840342, recall 0.813495
2017-12-10T03:48:42.328380: step 4987, loss 0.24157, acc 0.9375, prec 0.084056, recall 0.813547
2017-12-10T03:48:42.595224: step 4988, loss 0.0343631, acc 0.984375, prec 0.0840681, recall 0.813574
2017-12-10T03:48:42.867726: step 4989, loss 0.0903056, acc 0.96875, prec 0.0840656, recall 0.813574
2017-12-10T03:48:43.134154: step 4990, loss 0.0891382, acc 0.96875, prec 0.0840899, recall 0.813626
2017-12-10T03:48:43.408915: step 4991, loss 0.209876, acc 0.9375, prec 0.0840983, recall 0.813652
2017-12-10T03:48:43.676830: step 4992, loss 0.295687, acc 0.90625, prec 0.0841043, recall 0.813679
2017-12-10T03:48:43.951167: step 4993, loss 0.150002, acc 0.9375, prec 0.0841127, recall 0.813705
2017-12-10T03:48:44.221189: step 4994, loss 0.289806, acc 0.96875, prec 0.0841102, recall 0.813705
2017-12-10T03:48:44.490748: step 4995, loss 0.377124, acc 0.921875, prec 0.0841308, recall 0.813757
2017-12-10T03:48:44.753339: step 4996, loss 0.180611, acc 0.9375, prec 0.0841392, recall 0.813783
2017-12-10T03:48:45.025417: step 4997, loss 0.177569, acc 0.984375, prec 0.0841779, recall 0.813862
2017-12-10T03:48:45.296470: step 4998, loss 0.112465, acc 0.96875, prec 0.0841888, recall 0.813888
2017-12-10T03:48:45.564808: step 4999, loss 0.0930027, acc 0.96875, prec 0.0841997, recall 0.813914
2017-12-10T03:48:45.833313: step 5000, loss 0.0951326, acc 0.953125, prec 0.084196, recall 0.813914
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-5000

2017-12-10T03:48:47.168695: step 5001, loss 0.469036, acc 0.953125, prec 0.0842323, recall 0.813993
2017-12-10T03:48:47.436255: step 5002, loss 0.117369, acc 0.984375, prec 0.0842443, recall 0.814019
2017-12-10T03:48:47.710485: step 5003, loss 0.133062, acc 0.984375, prec 0.0842564, recall 0.814045
2017-12-10T03:48:47.984367: step 5004, loss 0.0344656, acc 0.984375, prec 0.0842552, recall 0.814045
2017-12-10T03:48:48.250142: step 5005, loss 0.0288515, acc 0.96875, prec 0.0842794, recall 0.814097
2017-12-10T03:48:48.518466: step 5006, loss 0.177726, acc 0.96875, prec 0.0843169, recall 0.814175
2017-12-10T03:48:48.780505: step 5007, loss 0.0562464, acc 0.953125, prec 0.0843132, recall 0.814175
2017-12-10T03:48:49.049328: step 5008, loss 0.228811, acc 0.96875, prec 0.0843373, recall 0.814228
2017-12-10T03:48:49.314883: step 5009, loss 0.0859242, acc 0.96875, prec 0.0843349, recall 0.814228
2017-12-10T03:48:49.585008: step 5010, loss 0.0824193, acc 0.984375, prec 0.084347, recall 0.814254
2017-12-10T03:48:49.856130: step 5011, loss 0.251759, acc 0.953125, prec 0.0843699, recall 0.814306
2017-12-10T03:48:50.123622: step 5012, loss 6.33649, acc 0.875, prec 0.0843746, recall 0.814218
2017-12-10T03:48:50.394693: step 5013, loss 0.0153031, acc 1, prec 0.0843746, recall 0.814218
2017-12-10T03:48:50.656103: step 5014, loss 0.0312137, acc 0.984375, prec 0.0843867, recall 0.814244
2017-12-10T03:48:50.921179: step 5015, loss 0.277451, acc 0.9375, prec 0.0843818, recall 0.814244
2017-12-10T03:48:51.181999: step 5016, loss 0.208796, acc 0.9375, prec 0.0843769, recall 0.814244
2017-12-10T03:48:51.445969: step 5017, loss 0.33955, acc 0.921875, prec 0.0843708, recall 0.814244
2017-12-10T03:48:51.712621: step 5018, loss 0.510477, acc 0.953125, prec 0.0843804, recall 0.81427
2017-12-10T03:48:51.984095: step 5019, loss 0.520115, acc 0.90625, prec 0.0843863, recall 0.814296
2017-12-10T03:48:52.253777: step 5020, loss 0.3056, acc 0.890625, prec 0.0844044, recall 0.814348
2017-12-10T03:48:52.528094: step 5021, loss 0.274257, acc 0.890625, prec 0.0844091, recall 0.814374
2017-12-10T03:48:52.791886: step 5022, loss 0.875381, acc 0.90625, prec 0.0844283, recall 0.814426
2017-12-10T03:48:53.057332: step 5023, loss 0.176979, acc 0.953125, prec 0.0844512, recall 0.814478
2017-12-10T03:48:53.322279: step 5024, loss 0.457939, acc 0.875, prec 0.0844414, recall 0.814478
2017-12-10T03:48:53.588848: step 5025, loss 0.265299, acc 0.9375, prec 0.0844365, recall 0.814478
2017-12-10T03:48:53.852525: step 5026, loss 0.710563, acc 0.875, prec 0.0844267, recall 0.814478
2017-12-10T03:48:54.120900: step 5027, loss 0.52572, acc 0.859375, prec 0.0844555, recall 0.814556
2017-12-10T03:48:54.384445: step 5028, loss 0.233993, acc 0.96875, prec 0.0844531, recall 0.814556
2017-12-10T03:48:54.660505: step 5029, loss 0.639744, acc 0.921875, prec 0.0844735, recall 0.814608
2017-12-10T03:48:54.925622: step 5030, loss 0.362498, acc 0.9375, prec 0.0844686, recall 0.814608
2017-12-10T03:48:55.199380: step 5031, loss 0.216353, acc 0.90625, prec 0.0844746, recall 0.814633
2017-12-10T03:48:55.459576: step 5032, loss 0.197859, acc 0.953125, prec 0.0844842, recall 0.814659
2017-12-10T03:48:55.724706: step 5033, loss 0.0777141, acc 0.984375, prec 0.0844829, recall 0.814659
2017-12-10T03:48:55.998914: step 5034, loss 0.0487854, acc 0.984375, prec 0.084495, recall 0.814685
2017-12-10T03:48:56.271483: step 5035, loss 0.144948, acc 0.9375, prec 0.0845034, recall 0.814711
2017-12-10T03:48:56.544283: step 5036, loss 0.0980743, acc 0.96875, prec 0.0845275, recall 0.814763
2017-12-10T03:48:56.817078: step 5037, loss 0.0239177, acc 0.984375, prec 0.0845262, recall 0.814763
2017-12-10T03:48:57.079834: step 5038, loss 0.346179, acc 0.9375, prec 0.0845346, recall 0.814789
2017-12-10T03:48:57.348543: step 5039, loss 0.023841, acc 0.96875, prec 0.0845454, recall 0.814815
2017-12-10T03:48:57.619313: step 5040, loss 0.0641967, acc 1, prec 0.084572, recall 0.814867
2017-12-10T03:48:57.888372: step 5041, loss 0.522191, acc 0.984375, prec 0.0845973, recall 0.814918
2017-12-10T03:48:58.158867: step 5042, loss 0.0930079, acc 0.96875, prec 0.0845949, recall 0.814918
2017-12-10T03:48:58.428282: step 5043, loss 0.0216281, acc 0.984375, prec 0.0845936, recall 0.814918
2017-12-10T03:48:58.693858: step 5044, loss 0.00344673, acc 1, prec 0.0846202, recall 0.81497
2017-12-10T03:48:58.956775: step 5045, loss 0.119754, acc 0.96875, prec 0.0846443, recall 0.815022
2017-12-10T03:48:59.223693: step 5046, loss 2.33502, acc 0.96875, prec 0.0846563, recall 0.814934
2017-12-10T03:48:59.494514: step 5047, loss 0.0247476, acc 0.984375, prec 0.0846551, recall 0.814934
2017-12-10T03:48:59.767066: step 5048, loss 0.184064, acc 0.9375, prec 0.08469, recall 0.815011
2017-12-10T03:49:00.031243: step 5049, loss 0.0279802, acc 0.984375, prec 0.084702, recall 0.815037
2017-12-10T03:49:00.309972: step 5050, loss 0.0937563, acc 0.96875, prec 0.0847394, recall 0.815114
2017-12-10T03:49:00.581538: step 5051, loss 0.140651, acc 0.953125, prec 0.0847357, recall 0.815114
2017-12-10T03:49:00.855170: step 5052, loss 0.211413, acc 0.96875, prec 0.0847598, recall 0.815166
2017-12-10T03:49:01.123032: step 5053, loss 0.0973235, acc 0.953125, prec 0.0847826, recall 0.815217
2017-12-10T03:49:01.391554: step 5054, loss 0.341792, acc 0.890625, prec 0.084774, recall 0.815217
2017-12-10T03:49:01.657999: step 5055, loss 0.389197, acc 0.921875, prec 0.0848076, recall 0.815295
2017-12-10T03:49:01.923242: step 5056, loss 0.232798, acc 0.9375, prec 0.0848027, recall 0.815295
2017-12-10T03:49:02.194271: step 5057, loss 0.658112, acc 0.875, prec 0.0848194, recall 0.815346
2017-12-10T03:49:02.465700: step 5058, loss 0.540354, acc 0.90625, prec 0.0848121, recall 0.815346
2017-12-10T03:49:02.727937: step 5059, loss 0.100431, acc 0.953125, prec 0.0848084, recall 0.815346
2017-12-10T03:49:02.988786: step 5060, loss 0.175962, acc 0.953125, prec 0.0848444, recall 0.815423
2017-12-10T03:49:03.256029: step 5061, loss 0.212454, acc 0.921875, prec 0.0848383, recall 0.815423
2017-12-10T03:49:03.521752: step 5062, loss 0.167423, acc 0.953125, prec 0.0848479, recall 0.815449
2017-12-10T03:49:03.786411: step 5063, loss 0.328395, acc 0.953125, prec 0.0848442, recall 0.815449
2017-12-10T03:49:04.055972: step 5064, loss 0.0572618, acc 0.96875, prec 0.0848682, recall 0.8155
2017-12-10T03:49:04.330912: step 5065, loss 0.0427806, acc 0.984375, prec 0.0848803, recall 0.815526
2017-12-10T03:49:04.601608: step 5066, loss 0.0956571, acc 0.96875, prec 0.0848778, recall 0.815526
2017-12-10T03:49:04.867067: step 5067, loss 0.0219372, acc 1, prec 0.084891, recall 0.815552
2017-12-10T03:49:05.132309: step 5068, loss 0.274076, acc 0.96875, prec 0.0849018, recall 0.815577
2017-12-10T03:49:05.401909: step 5069, loss 0.0372304, acc 0.96875, prec 0.0848994, recall 0.815577
2017-12-10T03:49:05.671805: step 5070, loss 0.0439646, acc 0.96875, prec 0.0848969, recall 0.815577
2017-12-10T03:49:05.938635: step 5071, loss 0.00530868, acc 1, prec 0.0849102, recall 0.815603
2017-12-10T03:49:06.203947: step 5072, loss 0.0536769, acc 0.984375, prec 0.0849089, recall 0.815603
2017-12-10T03:49:06.471041: step 5073, loss 3.19164, acc 0.96875, prec 0.0849077, recall 0.815489
2017-12-10T03:49:06.744220: step 5074, loss 0.172774, acc 0.953125, prec 0.0849173, recall 0.815515
2017-12-10T03:49:07.010975: step 5075, loss 0.00760203, acc 1, prec 0.0849438, recall 0.815566
2017-12-10T03:49:07.275546: step 5076, loss 0.107827, acc 0.953125, prec 0.0849533, recall 0.815592
2017-12-10T03:49:07.540806: step 5077, loss 0.359205, acc 0.953125, prec 0.0849629, recall 0.815618
2017-12-10T03:49:07.811012: step 5078, loss 0.321823, acc 0.90625, prec 0.0849687, recall 0.815643
2017-12-10T03:49:08.083733: step 5079, loss 0.0284333, acc 0.984375, prec 0.0849807, recall 0.815669
2017-12-10T03:49:08.353918: step 5080, loss 0.276493, acc 0.90625, prec 0.0849866, recall 0.815694
2017-12-10T03:49:08.618303: step 5081, loss 0.446108, acc 0.921875, prec 0.0849805, recall 0.815694
2017-12-10T03:49:08.880997: step 5082, loss 0.884683, acc 0.890625, prec 0.0850116, recall 0.815771
2017-12-10T03:49:09.152746: step 5083, loss 0.339852, acc 0.953125, prec 0.0850079, recall 0.815771
2017-12-10T03:49:09.417208: step 5084, loss 0.149434, acc 0.921875, prec 0.0850017, recall 0.815771
2017-12-10T03:49:09.688411: step 5085, loss 0.324101, acc 0.921875, prec 0.0850088, recall 0.815797
2017-12-10T03:49:09.951704: step 5086, loss 0.0591299, acc 0.984375, prec 0.0850076, recall 0.815797
2017-12-10T03:49:10.221019: step 5087, loss 0.254634, acc 0.9375, prec 0.0850424, recall 0.815873
2017-12-10T03:49:10.486319: step 5088, loss 0.547597, acc 0.890625, prec 0.0850338, recall 0.815873
2017-12-10T03:49:10.754744: step 5089, loss 0.084048, acc 0.96875, prec 0.0850445, recall 0.815899
2017-12-10T03:49:11.028908: step 5090, loss 0.410663, acc 0.90625, prec 0.0850636, recall 0.81595
2017-12-10T03:49:11.294984: step 5091, loss 1.26203, acc 1, prec 0.0851033, recall 0.816027
2017-12-10T03:49:11.571517: step 5092, loss 0.391383, acc 0.921875, prec 0.0850972, recall 0.816027
2017-12-10T03:49:11.852959: step 5093, loss 0.297458, acc 0.875, prec 0.0850873, recall 0.816027
2017-12-10T03:49:12.117534: step 5094, loss 0.109248, acc 0.96875, prec 0.0850981, recall 0.816052
2017-12-10T03:49:12.383318: step 5095, loss 0.729861, acc 0.9375, prec 0.0851196, recall 0.816103
2017-12-10T03:49:12.645742: step 5096, loss 0.0717485, acc 0.953125, prec 0.0851424, recall 0.816154
2017-12-10T03:49:12.916016: step 5097, loss 0.456751, acc 0.921875, prec 0.0851362, recall 0.816154
2017-12-10T03:49:13.179477: step 5098, loss 0.43213, acc 0.921875, prec 0.0851433, recall 0.81618
2017-12-10T03:49:13.443434: step 5099, loss 0.7356, acc 0.875, prec 0.0851467, recall 0.816205
2017-12-10T03:49:13.710114: step 5100, loss 0.125955, acc 0.9375, prec 0.0851417, recall 0.816205

Evaluation:
2017-12-10T03:49:21.275959: step 5100, loss 2.51516, acc 0.913954, prec 0.085389, recall 0.812916

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-5100

2017-12-10T03:49:22.593204: step 5101, loss 0.264227, acc 0.90625, prec 0.0854077, recall 0.812966
2017-12-10T03:49:22.860400: step 5102, loss 0.635355, acc 0.890625, prec 0.0854253, recall 0.813017
2017-12-10T03:49:23.132398: step 5103, loss 0.466011, acc 0.875, prec 0.0854286, recall 0.813042
2017-12-10T03:49:23.402134: step 5104, loss 0.404448, acc 0.90625, prec 0.0854213, recall 0.813042
2017-12-10T03:49:23.675467: step 5105, loss 0.347166, acc 0.9375, prec 0.0854164, recall 0.813042
2017-12-10T03:49:23.941589: step 5106, loss 0.0576281, acc 0.953125, prec 0.0854127, recall 0.813042
2017-12-10T03:49:24.206561: step 5107, loss 0.0245246, acc 0.984375, prec 0.0854506, recall 0.813118
2017-12-10T03:49:24.470333: step 5108, loss 0.219905, acc 0.9375, prec 0.0854588, recall 0.813144
2017-12-10T03:49:24.748488: step 5109, loss 0.249495, acc 0.96875, prec 0.0854563, recall 0.813144
2017-12-10T03:49:25.024428: step 5110, loss 0.171027, acc 0.953125, prec 0.0854527, recall 0.813144
2017-12-10T03:49:25.289568: step 5111, loss 0.125175, acc 0.96875, prec 0.0854502, recall 0.813144
2017-12-10T03:49:25.558069: step 5112, loss 0.0235469, acc 0.984375, prec 0.085449, recall 0.813144
2017-12-10T03:49:25.831265: step 5113, loss 0.0952216, acc 0.984375, prec 0.0854608, recall 0.813169
2017-12-10T03:49:26.098514: step 5114, loss 0.0464156, acc 0.984375, prec 0.0854596, recall 0.813169
2017-12-10T03:49:26.368119: step 5115, loss 0.153906, acc 0.96875, prec 0.0854832, recall 0.81322
2017-12-10T03:49:26.634852: step 5116, loss 0.0197932, acc 0.984375, prec 0.0855341, recall 0.813321
2017-12-10T03:49:26.907887: step 5117, loss 0.553864, acc 0.984375, prec 0.0855459, recall 0.813346
2017-12-10T03:49:27.176285: step 5118, loss 0.387501, acc 1, prec 0.0855719, recall 0.813396
2017-12-10T03:49:27.453494: step 5119, loss 0.0907763, acc 1, prec 0.0855849, recall 0.813422
2017-12-10T03:49:27.725882: step 5120, loss 0.34473, acc 1, prec 0.085611, recall 0.813472
2017-12-10T03:49:27.995953: step 5121, loss 0.103883, acc 0.96875, prec 0.0856216, recall 0.813497
2017-12-10T03:49:28.265646: step 5122, loss 0.0519577, acc 0.984375, prec 0.0856203, recall 0.813497
2017-12-10T03:49:28.532124: step 5123, loss 5.42958, acc 0.9375, prec 0.0856439, recall 0.813328
2017-12-10T03:49:28.801497: step 5124, loss 0.632947, acc 0.953125, prec 0.0856403, recall 0.813328
2017-12-10T03:49:29.066673: step 5125, loss 0.515231, acc 0.921875, prec 0.0856472, recall 0.813353
2017-12-10T03:49:29.340107: step 5126, loss 0.151255, acc 0.953125, prec 0.0856435, recall 0.813353
2017-12-10T03:49:29.605698: step 5127, loss 0.736202, acc 0.859375, prec 0.0856456, recall 0.813378
2017-12-10T03:49:29.868267: step 5128, loss 0.439251, acc 0.9375, prec 0.0856537, recall 0.813404
2017-12-10T03:49:30.134696: step 5129, loss 0.982993, acc 0.78125, prec 0.0856367, recall 0.813404
2017-12-10T03:49:30.404655: step 5130, loss 1.30108, acc 0.703125, prec 0.0856265, recall 0.813429
2017-12-10T03:49:30.674496: step 5131, loss 0.963628, acc 0.75, prec 0.085633, recall 0.813479
2017-12-10T03:49:30.939605: step 5132, loss 1.13459, acc 0.796875, prec 0.0856302, recall 0.813504
2017-12-10T03:49:31.211354: step 5133, loss 0.42803, acc 0.859375, prec 0.0856193, recall 0.813504
2017-12-10T03:49:31.477526: step 5134, loss 0.860379, acc 0.8125, prec 0.0856177, recall 0.81353
2017-12-10T03:49:31.747960: step 5135, loss 0.773311, acc 0.828125, prec 0.0856173, recall 0.813555
2017-12-10T03:49:32.010609: step 5136, loss 1.05992, acc 0.765625, prec 0.085599, recall 0.813555
2017-12-10T03:49:32.280481: step 5137, loss 0.755419, acc 0.875, prec 0.0856153, recall 0.813605
2017-12-10T03:49:32.548404: step 5138, loss 1.52886, acc 0.765625, prec 0.085597, recall 0.813605
2017-12-10T03:49:32.813738: step 5139, loss 0.553022, acc 0.859375, prec 0.0856121, recall 0.813655
2017-12-10T03:49:33.082195: step 5140, loss 0.605251, acc 0.828125, prec 0.0856247, recall 0.813706
2017-12-10T03:49:33.353339: step 5141, loss 0.294188, acc 0.921875, prec 0.0856316, recall 0.813731
2017-12-10T03:49:33.618048: step 5142, loss 0.0944799, acc 0.953125, prec 0.0856279, recall 0.813731
2017-12-10T03:49:33.887446: step 5143, loss 0.494626, acc 0.984375, prec 0.0856397, recall 0.813756
2017-12-10T03:49:34.152792: step 5144, loss 0.299463, acc 0.9375, prec 0.0856348, recall 0.813756
2017-12-10T03:49:34.416608: step 5145, loss 0.0404767, acc 0.984375, prec 0.0856595, recall 0.813806
2017-12-10T03:49:34.686193: step 5146, loss 0.169378, acc 0.9375, prec 0.0856806, recall 0.813856
2017-12-10T03:49:34.951014: step 5147, loss 0.0717628, acc 0.96875, prec 0.0856912, recall 0.813881
2017-12-10T03:49:35.216891: step 5148, loss 0.127859, acc 0.953125, prec 0.0857005, recall 0.813906
2017-12-10T03:49:35.485010: step 5149, loss 0.479067, acc 0.96875, prec 0.085724, recall 0.813957
2017-12-10T03:49:35.753104: step 5150, loss 0.162964, acc 0.96875, prec 0.0857216, recall 0.813957
2017-12-10T03:49:36.020825: step 5151, loss 0.300723, acc 0.96875, prec 0.0857321, recall 0.813982
2017-12-10T03:49:36.288734: step 5152, loss 0.209756, acc 0.984375, prec 0.0857439, recall 0.814007
2017-12-10T03:49:36.556765: step 5153, loss 0.0715118, acc 0.984375, prec 0.0857556, recall 0.814032
2017-12-10T03:49:36.820379: step 5154, loss 4.26433, acc 0.953125, prec 0.0857791, recall 0.813972
2017-12-10T03:49:37.095675: step 5155, loss 0.13708, acc 0.96875, prec 0.0857897, recall 0.813997
2017-12-10T03:49:37.369533: step 5156, loss 0.110273, acc 0.9375, prec 0.0857978, recall 0.814022
2017-12-10T03:49:37.635671: step 5157, loss 0.0692152, acc 0.953125, prec 0.0857941, recall 0.814022
2017-12-10T03:49:37.906245: step 5158, loss 0.101879, acc 0.953125, prec 0.0857905, recall 0.814022
2017-12-10T03:49:38.179152: step 5159, loss 0.16977, acc 0.953125, prec 0.0858127, recall 0.814072
2017-12-10T03:49:38.440488: step 5160, loss 0.49499, acc 0.921875, prec 0.0858196, recall 0.814097
2017-12-10T03:49:38.708118: step 5161, loss 0.103829, acc 0.953125, prec 0.0858419, recall 0.814147
2017-12-10T03:49:38.985903: step 5162, loss 0.5353, acc 0.859375, prec 0.0858309, recall 0.814147
2017-12-10T03:49:39.262156: step 5163, loss 0.233726, acc 0.96875, prec 0.0858415, recall 0.814172
2017-12-10T03:49:39.531486: step 5164, loss 0.194014, acc 0.9375, prec 0.0858366, recall 0.814172
2017-12-10T03:49:39.801840: step 5165, loss 0.172859, acc 0.921875, prec 0.0858694, recall 0.814247
2017-12-10T03:49:40.071383: step 5166, loss 0.40107, acc 0.890625, prec 0.0858738, recall 0.814272
2017-12-10T03:49:40.341129: step 5167, loss 0.16664, acc 0.96875, prec 0.0858714, recall 0.814272
2017-12-10T03:49:40.612770: step 5168, loss 0.356134, acc 0.953125, prec 0.0858807, recall 0.814297
2017-12-10T03:49:40.894814: step 5169, loss 0.211333, acc 0.96875, prec 0.0858783, recall 0.814297
2017-12-10T03:49:41.164430: step 5170, loss 0.191869, acc 0.9375, prec 0.0858734, recall 0.814297
2017-12-10T03:49:41.428879: step 5171, loss 0.0948876, acc 0.953125, prec 0.0858698, recall 0.814297
2017-12-10T03:49:41.700020: step 5172, loss 0.066893, acc 0.96875, prec 0.0858803, recall 0.814322
2017-12-10T03:49:41.976472: step 5173, loss 0.489724, acc 0.9375, prec 0.0858754, recall 0.814322
2017-12-10T03:49:42.242075: step 5174, loss 0.256249, acc 0.96875, prec 0.0858859, recall 0.814347
2017-12-10T03:49:42.514200: step 5175, loss 0.27184, acc 0.953125, prec 0.0858823, recall 0.814347
2017-12-10T03:49:42.782934: step 5176, loss 0.345161, acc 0.921875, prec 0.0858762, recall 0.814347
2017-12-10T03:49:43.055680: step 5177, loss 1.29623, acc 0.9375, prec 0.0858725, recall 0.814238
2017-12-10T03:49:43.330932: step 5178, loss 0.11375, acc 0.9375, prec 0.0858677, recall 0.814238
2017-12-10T03:49:43.597097: step 5179, loss 0.0687157, acc 0.984375, prec 0.0858923, recall 0.814288
2017-12-10T03:49:43.861502: step 5180, loss 0.0473576, acc 0.984375, prec 0.0859041, recall 0.814313
2017-12-10T03:49:44.127967: step 5181, loss 0.0335448, acc 0.984375, prec 0.0859288, recall 0.814362
2017-12-10T03:49:44.397215: step 5182, loss 0.245233, acc 0.953125, prec 0.0859251, recall 0.814362
2017-12-10T03:49:44.674550: step 5183, loss 0.199086, acc 0.9375, prec 0.0859591, recall 0.814437
2017-12-10T03:49:44.947694: step 5184, loss 0.480756, acc 0.9375, prec 0.0859542, recall 0.814437
2017-12-10T03:49:45.219019: step 5185, loss 0.0478713, acc 0.984375, prec 0.085953, recall 0.814437
2017-12-10T03:49:45.488368: step 5186, loss 0.361934, acc 0.9375, prec 0.0859481, recall 0.814437
2017-12-10T03:49:45.754332: step 5187, loss 0.0124719, acc 1, prec 0.0859481, recall 0.814437
2017-12-10T03:49:46.021482: step 5188, loss 0.43541, acc 0.96875, prec 0.0859716, recall 0.814487
2017-12-10T03:49:46.303903: step 5189, loss 0.0308675, acc 1, prec 0.0859716, recall 0.814487
2017-12-10T03:49:46.567926: step 5190, loss 0.0212077, acc 0.984375, prec 0.0859833, recall 0.814512
2017-12-10T03:49:46.834120: step 5191, loss 0.701583, acc 0.96875, prec 0.0860067, recall 0.814562
2017-12-10T03:49:47.103158: step 5192, loss 0.287377, acc 0.9375, prec 0.0860019, recall 0.814562
2017-12-10T03:49:47.375778: step 5193, loss 0.0717806, acc 0.984375, prec 0.0860136, recall 0.814586
2017-12-10T03:49:47.648219: step 5194, loss 0.0515523, acc 0.984375, prec 0.0860253, recall 0.814611
2017-12-10T03:49:47.917399: step 5195, loss 0.222353, acc 0.953125, prec 0.0860217, recall 0.814611
2017-12-10T03:49:48.184803: step 5196, loss 0.0136911, acc 1, prec 0.0860475, recall 0.814661
2017-12-10T03:49:48.449880: step 5197, loss 1.34593, acc 0.9375, prec 0.0860815, recall 0.814735
2017-12-10T03:49:48.720090: step 5198, loss 0.0614436, acc 0.984375, prec 0.0861061, recall 0.814785
2017-12-10T03:49:48.990232: step 5199, loss 0.0198214, acc 0.984375, prec 0.0861178, recall 0.81481
2017-12-10T03:49:49.256936: step 5200, loss 0.00986512, acc 1, prec 0.0861178, recall 0.81481
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-5200

2017-12-10T03:49:50.549515: step 5201, loss 0.339508, acc 0.984375, prec 0.0861425, recall 0.814859
2017-12-10T03:49:50.817872: step 5202, loss 0.0871502, acc 0.984375, prec 0.0861413, recall 0.814859
2017-12-10T03:49:51.088698: step 5203, loss 0.197129, acc 0.96875, prec 0.0861647, recall 0.814909
2017-12-10T03:49:51.354205: step 5204, loss 0.0825603, acc 0.984375, prec 0.0861893, recall 0.814959
2017-12-10T03:49:51.622300: step 5205, loss 0.0309295, acc 1, prec 0.0862152, recall 0.815008
2017-12-10T03:49:51.890868: step 5206, loss 0.385956, acc 0.9375, prec 0.0862232, recall 0.815033
2017-12-10T03:49:52.156989: step 5207, loss 0.145742, acc 0.96875, prec 0.0862208, recall 0.815033
2017-12-10T03:49:52.424966: step 5208, loss 0.00818749, acc 1, prec 0.0862208, recall 0.815033
2017-12-10T03:49:52.691652: step 5209, loss 2.42686, acc 0.921875, prec 0.0862159, recall 0.814924
2017-12-10T03:49:52.967055: step 5210, loss 0.478156, acc 0.921875, prec 0.0862098, recall 0.814924
2017-12-10T03:49:53.234679: step 5211, loss 0.449585, acc 0.953125, prec 0.0862191, recall 0.814948
2017-12-10T03:49:53.501588: step 5212, loss 0.272542, acc 0.96875, prec 0.0862296, recall 0.814973
2017-12-10T03:49:53.768684: step 5213, loss 0.215205, acc 0.953125, prec 0.0862259, recall 0.814973
2017-12-10T03:49:54.041140: step 5214, loss 0.62486, acc 0.8125, prec 0.0862113, recall 0.814973
2017-12-10T03:49:54.314635: step 5215, loss 0.159337, acc 0.9375, prec 0.0862064, recall 0.814973
2017-12-10T03:49:54.581674: step 5216, loss 0.309401, acc 0.9375, prec 0.0862145, recall 0.814998
2017-12-10T03:49:54.855089: step 5217, loss 0.0713032, acc 0.96875, prec 0.0862249, recall 0.815023
2017-12-10T03:49:55.123979: step 5218, loss 0.688742, acc 0.890625, prec 0.0862164, recall 0.815023
2017-12-10T03:49:55.388012: step 5219, loss 0.436129, acc 0.953125, prec 0.0862257, recall 0.815047
2017-12-10T03:49:55.656985: step 5220, loss 0.636355, acc 0.9375, prec 0.0862208, recall 0.815047
2017-12-10T03:49:55.925612: step 5221, loss 0.243839, acc 0.90625, prec 0.0862264, recall 0.815072
2017-12-10T03:49:56.195195: step 5222, loss 0.316949, acc 0.953125, prec 0.0862356, recall 0.815097
2017-12-10T03:49:56.460675: step 5223, loss 0.211831, acc 0.953125, prec 0.0862578, recall 0.815146
2017-12-10T03:49:56.734509: step 5224, loss 0.373371, acc 0.96875, prec 0.0862683, recall 0.815171
2017-12-10T03:49:57.001248: step 5225, loss 0.121476, acc 0.96875, prec 0.0862659, recall 0.815171
2017-12-10T03:49:57.272502: step 5226, loss 0.126955, acc 0.953125, prec 0.0862751, recall 0.815196
2017-12-10T03:49:57.545269: step 5227, loss 0.0660409, acc 0.984375, prec 0.0862997, recall 0.815245
2017-12-10T03:49:57.821153: step 5228, loss 0.244078, acc 0.953125, prec 0.0862961, recall 0.815245
2017-12-10T03:49:58.084670: step 5229, loss 0.323664, acc 0.9375, prec 0.0863041, recall 0.81527
2017-12-10T03:49:58.353299: step 5230, loss 0.0703919, acc 0.984375, prec 0.0863158, recall 0.815294
2017-12-10T03:49:58.620246: step 5231, loss 0.0939022, acc 0.96875, prec 0.0863134, recall 0.815294
2017-12-10T03:49:58.889107: step 5232, loss 0.324284, acc 0.921875, prec 0.0863072, recall 0.815294
2017-12-10T03:49:59.153449: step 5233, loss 0.418531, acc 0.890625, prec 0.0863116, recall 0.815319
2017-12-10T03:49:59.420898: step 5234, loss 0.126609, acc 0.953125, prec 0.086308, recall 0.815319
2017-12-10T03:49:59.686556: step 5235, loss 0.0468814, acc 0.984375, prec 0.0863068, recall 0.815319
2017-12-10T03:49:59.956295: step 5236, loss 0.205044, acc 0.921875, prec 0.0863007, recall 0.815319
2017-12-10T03:50:00.243376: step 5237, loss 0.0202024, acc 1, prec 0.0863394, recall 0.815393
2017-12-10T03:50:00.521614: step 5238, loss 0.00404661, acc 1, prec 0.0863394, recall 0.815393
2017-12-10T03:50:00.785080: step 5239, loss 0.0650504, acc 0.984375, prec 0.086364, recall 0.815442
2017-12-10T03:50:01.053551: step 5240, loss 0.0649398, acc 0.953125, prec 0.0863603, recall 0.815442
2017-12-10T03:50:01.321683: step 5241, loss 0.00134482, acc 1, prec 0.0863603, recall 0.815442
2017-12-10T03:50:01.597658: step 5242, loss 0.0864317, acc 0.984375, prec 0.0863591, recall 0.815442
2017-12-10T03:50:01.863686: step 5243, loss 0.140508, acc 0.96875, prec 0.0863566, recall 0.815442
2017-12-10T03:50:02.130247: step 5244, loss 0.00734974, acc 1, prec 0.0863824, recall 0.815491
2017-12-10T03:50:02.403121: step 5245, loss 0.108909, acc 0.96875, prec 0.08638, recall 0.815491
2017-12-10T03:50:03.384869: step 5246, loss 0.0209147, acc 1, prec 0.0864058, recall 0.81554
2017-12-10T03:50:03.736981: step 5247, loss 1.96609, acc 0.96875, prec 0.0864046, recall 0.815432
2017-12-10T03:50:04.007458: step 5248, loss 0.0556882, acc 0.984375, prec 0.0864034, recall 0.815432
2017-12-10T03:50:04.762550: step 5249, loss 0.0156083, acc 1, prec 0.0864163, recall 0.815456
2017-12-10T03:50:05.510663: step 5250, loss 0.0395767, acc 0.984375, prec 0.086415, recall 0.815456
2017-12-10T03:50:06.242808: step 5251, loss 0.00228856, acc 1, prec 0.0864279, recall 0.815481
2017-12-10T03:50:06.992225: step 5252, loss 1.34626, acc 0.921875, prec 0.0864231, recall 0.815372
2017-12-10T03:50:07.718829: step 5253, loss 0.110339, acc 0.96875, prec 0.0864464, recall 0.815422
2017-12-10T03:50:08.442024: step 5254, loss 0.161573, acc 0.96875, prec 0.086444, recall 0.815422
2017-12-10T03:50:09.245645: step 5255, loss 0.522482, acc 0.875, prec 0.08646, recall 0.815471
2017-12-10T03:50:09.654672: step 5256, loss 0.781836, acc 0.875, prec 0.0864889, recall 0.815544
2017-12-10T03:50:09.919509: step 5257, loss 0.39603, acc 0.921875, prec 0.0865086, recall 0.815593
2017-12-10T03:50:10.187092: step 5258, loss 0.412133, acc 0.9375, prec 0.0865166, recall 0.815618
2017-12-10T03:50:10.458395: step 5259, loss 0.657376, acc 0.828125, prec 0.086529, recall 0.815667
2017-12-10T03:50:10.721606: step 5260, loss 0.208353, acc 0.921875, prec 0.0865358, recall 0.815691
2017-12-10T03:50:10.988630: step 5261, loss 0.705397, acc 0.796875, prec 0.0865714, recall 0.815789
2017-12-10T03:50:11.262265: step 5262, loss 0.481217, acc 0.890625, prec 0.0865758, recall 0.815814
2017-12-10T03:50:11.529761: step 5263, loss 0.583917, acc 0.90625, prec 0.0865813, recall 0.815838
2017-12-10T03:50:11.803928: step 5264, loss 0.24734, acc 0.921875, prec 0.0865752, recall 0.815838
2017-12-10T03:50:12.074505: step 5265, loss 0.590695, acc 0.8125, prec 0.0865734, recall 0.815863
2017-12-10T03:50:12.341474: step 5266, loss 0.196578, acc 0.921875, prec 0.0865673, recall 0.815863
2017-12-10T03:50:12.603519: step 5267, loss 0.201282, acc 0.921875, prec 0.0865741, recall 0.815887
2017-12-10T03:50:12.873623: step 5268, loss 0.159175, acc 0.953125, prec 0.0865833, recall 0.815912
2017-12-10T03:50:13.146503: step 5269, loss 0.263509, acc 0.9375, prec 0.0866042, recall 0.815961
2017-12-10T03:50:13.420142: step 5270, loss 0.0552157, acc 0.984375, prec 0.0866158, recall 0.815985
2017-12-10T03:50:13.686999: step 5271, loss 0.300131, acc 0.9375, prec 0.086611, recall 0.815985
2017-12-10T03:50:13.958728: step 5272, loss 0.198991, acc 0.96875, prec 0.0866343, recall 0.816034
2017-12-10T03:50:14.226600: step 5273, loss 0.0428635, acc 0.96875, prec 0.0866447, recall 0.816058
2017-12-10T03:50:14.496775: step 5274, loss 0.100938, acc 0.96875, prec 0.0866551, recall 0.816083
2017-12-10T03:50:14.765223: step 5275, loss 0.0108165, acc 1, prec 0.086668, recall 0.816107
2017-12-10T03:50:15.027746: step 5276, loss 0.0300524, acc 0.984375, prec 0.0866668, recall 0.816107
2017-12-10T03:50:15.295027: step 5277, loss 0.156315, acc 0.953125, prec 0.086676, recall 0.816132
2017-12-10T03:50:15.563878: step 5278, loss 0.00304215, acc 1, prec 0.086676, recall 0.816132
2017-12-10T03:50:15.832531: step 5279, loss 1.09657, acc 1, prec 0.0866888, recall 0.816156
2017-12-10T03:50:16.109395: step 5280, loss 0.224356, acc 0.96875, prec 0.0866864, recall 0.816156
2017-12-10T03:50:16.375339: step 5281, loss 0.329331, acc 0.9375, prec 0.0866944, recall 0.81618
2017-12-10T03:50:16.638599: step 5282, loss 0.152526, acc 0.953125, prec 0.0867036, recall 0.816205
2017-12-10T03:50:16.906844: step 5283, loss 0.00497311, acc 1, prec 0.0867036, recall 0.816205
2017-12-10T03:50:17.179099: step 5284, loss 0.229026, acc 0.953125, prec 0.0867642, recall 0.816327
2017-12-10T03:50:17.443053: step 5285, loss 0.173479, acc 0.953125, prec 0.0867863, recall 0.816375
2017-12-10T03:50:17.714714: step 5286, loss 0.103455, acc 0.953125, prec 0.0867955, recall 0.8164
2017-12-10T03:50:17.984193: step 5287, loss 0.186091, acc 0.96875, prec 0.0868059, recall 0.816424
2017-12-10T03:50:18.253069: step 5288, loss 0.240037, acc 0.921875, prec 0.0867998, recall 0.816424
2017-12-10T03:50:18.522336: step 5289, loss 0.171084, acc 0.96875, prec 0.0868102, recall 0.816448
2017-12-10T03:50:18.794798: step 5290, loss 0.157847, acc 0.984375, prec 0.0868218, recall 0.816472
2017-12-10T03:50:19.057027: step 5291, loss 0.107992, acc 0.953125, prec 0.0868182, recall 0.816472
2017-12-10T03:50:19.332681: step 5292, loss 0.46412, acc 0.921875, prec 0.0868121, recall 0.816472
2017-12-10T03:50:19.598902: step 5293, loss 0.0291691, acc 0.984375, prec 0.0868108, recall 0.816472
2017-12-10T03:50:19.865349: step 5294, loss 0.0169076, acc 0.984375, prec 0.0868096, recall 0.816472
2017-12-10T03:50:20.134086: step 5295, loss 0.237741, acc 0.953125, prec 0.0868059, recall 0.816472
2017-12-10T03:50:20.408802: step 5296, loss 0.241934, acc 0.96875, prec 0.0868035, recall 0.816472
2017-12-10T03:50:20.675811: step 5297, loss 0.288984, acc 0.953125, prec 0.0868127, recall 0.816497
2017-12-10T03:50:20.940396: step 5298, loss 0.0983608, acc 0.953125, prec 0.086809, recall 0.816497
2017-12-10T03:50:21.205384: step 5299, loss 0.124177, acc 0.96875, prec 0.0868066, recall 0.816497
2017-12-10T03:50:21.471212: step 5300, loss 0.0385681, acc 0.984375, prec 0.0868054, recall 0.816497
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-5300

2017-12-10T03:50:22.635246: step 5301, loss 2.36511, acc 0.953125, prec 0.0868029, recall 0.816389
2017-12-10T03:50:22.910715: step 5302, loss 0.201903, acc 0.953125, prec 0.0867992, recall 0.816389
2017-12-10T03:50:23.180700: step 5303, loss 0.153151, acc 0.96875, prec 0.0867968, recall 0.816389
2017-12-10T03:50:23.442854: step 5304, loss 0.0237662, acc 1, prec 0.0868097, recall 0.816413
2017-12-10T03:50:23.711285: step 5305, loss 0.0768169, acc 0.984375, prec 0.0868213, recall 0.816437
2017-12-10T03:50:23.980919: step 5306, loss 0.111685, acc 0.96875, prec 0.0868317, recall 0.816462
2017-12-10T03:50:24.249197: step 5307, loss 0.198402, acc 0.9375, prec 0.0868268, recall 0.816462
2017-12-10T03:50:24.514891: step 5308, loss 0.303624, acc 0.921875, prec 0.0868207, recall 0.816462
2017-12-10T03:50:24.779633: step 5309, loss 0.0694761, acc 0.984375, prec 0.0868195, recall 0.816462
2017-12-10T03:50:25.052188: step 5310, loss 0.291772, acc 0.9375, prec 0.0868274, recall 0.816486
2017-12-10T03:50:25.318131: step 5311, loss 0.184475, acc 0.96875, prec 0.0868507, recall 0.816534
2017-12-10T03:50:25.586879: step 5312, loss 0.188484, acc 1, prec 0.0869021, recall 0.816631
2017-12-10T03:50:25.858743: step 5313, loss 0.0335583, acc 0.984375, prec 0.0869009, recall 0.816631
2017-12-10T03:50:26.121814: step 5314, loss 0.0246884, acc 0.984375, prec 0.0868996, recall 0.816631
2017-12-10T03:50:26.388681: step 5315, loss 0.207995, acc 0.953125, prec 0.0869088, recall 0.816656
2017-12-10T03:50:26.658816: step 5316, loss 0.306451, acc 0.96875, prec 0.0869321, recall 0.816704
2017-12-10T03:50:26.936242: step 5317, loss 0.66844, acc 0.96875, prec 0.0869553, recall 0.816753
2017-12-10T03:50:27.207704: step 5318, loss 0.0238897, acc 1, prec 0.0869681, recall 0.816777
2017-12-10T03:50:27.470314: step 5319, loss 0.0417016, acc 1, prec 0.0869938, recall 0.816825
2017-12-10T03:50:27.737395: step 5320, loss 0.14689, acc 0.953125, prec 0.0869902, recall 0.816825
2017-12-10T03:50:28.014475: step 5321, loss 0.0407678, acc 0.984375, prec 0.0870275, recall 0.816898
2017-12-10T03:50:28.282251: step 5322, loss 0.40763, acc 0.90625, prec 0.087033, recall 0.816922
2017-12-10T03:50:28.554282: step 5323, loss 0.181737, acc 0.984375, prec 0.0870574, recall 0.81697
2017-12-10T03:50:28.822714: step 5324, loss 0.125358, acc 0.96875, prec 0.0870678, recall 0.816994
2017-12-10T03:50:29.092429: step 5325, loss 0.0613443, acc 0.984375, prec 0.0870922, recall 0.817043
2017-12-10T03:50:29.356501: step 5326, loss 0.227675, acc 0.984375, prec 0.087091, recall 0.817043
2017-12-10T03:50:29.619784: step 5327, loss 0.0344705, acc 0.984375, prec 0.0871026, recall 0.817067
2017-12-10T03:50:29.885359: step 5328, loss 0.188459, acc 0.953125, prec 0.0871375, recall 0.817139
2017-12-10T03:50:30.155740: step 5329, loss 0.280081, acc 0.9375, prec 0.0871326, recall 0.817139
2017-12-10T03:50:30.431061: step 5330, loss 0.127656, acc 0.953125, prec 0.0871545, recall 0.817187
2017-12-10T03:50:30.708000: step 5331, loss 0.00460073, acc 1, prec 0.0871545, recall 0.817187
2017-12-10T03:50:30.975228: step 5332, loss 0.0575692, acc 0.96875, prec 0.0871649, recall 0.817211
2017-12-10T03:50:31.247986: step 5333, loss 0.0388849, acc 0.984375, prec 0.0871637, recall 0.817211
2017-12-10T03:50:31.513917: step 5334, loss 1.85745, acc 0.921875, prec 0.0871716, recall 0.817128
2017-12-10T03:50:31.782337: step 5335, loss 0.64826, acc 0.953125, prec 0.0871808, recall 0.817152
2017-12-10T03:50:32.049812: step 5336, loss 0.0925618, acc 0.96875, prec 0.0871783, recall 0.817152
2017-12-10T03:50:32.317848: step 5337, loss 0.219647, acc 0.953125, prec 0.0871875, recall 0.817176
2017-12-10T03:50:32.586547: step 5338, loss 0.143921, acc 0.96875, prec 0.0872107, recall 0.817224
2017-12-10T03:50:32.851044: step 5339, loss 0.110039, acc 0.953125, prec 0.087207, recall 0.817224
2017-12-10T03:50:33.114857: step 5340, loss 0.577433, acc 0.890625, prec 0.0872241, recall 0.817272
2017-12-10T03:50:33.388502: step 5341, loss 0.0661807, acc 0.984375, prec 0.0872229, recall 0.817272
2017-12-10T03:50:33.656072: step 5342, loss 0.226904, acc 0.96875, prec 0.0872204, recall 0.817272
2017-12-10T03:50:33.925202: step 5343, loss 0.199527, acc 0.90625, prec 0.0872259, recall 0.817296
2017-12-10T03:50:34.192271: step 5344, loss 0.237838, acc 0.9375, prec 0.087221, recall 0.817296
2017-12-10T03:50:34.458268: step 5345, loss 0.614196, acc 0.890625, prec 0.0872252, recall 0.81732
2017-12-10T03:50:34.726510: step 5346, loss 0.335768, acc 0.9375, prec 0.0872331, recall 0.817344
2017-12-10T03:50:34.989858: step 5347, loss 0.602799, acc 0.90625, prec 0.0872258, recall 0.817344
2017-12-10T03:50:35.261492: step 5348, loss 0.220445, acc 0.953125, prec 0.0872478, recall 0.817392
2017-12-10T03:50:35.538667: step 5349, loss 0.728495, acc 0.859375, prec 0.0872495, recall 0.817416
2017-12-10T03:50:35.808301: step 5350, loss 0.251296, acc 0.90625, prec 0.0872422, recall 0.817416
2017-12-10T03:50:36.074110: step 5351, loss 0.12449, acc 0.984375, prec 0.087241, recall 0.817416
2017-12-10T03:50:36.338842: step 5352, loss 0.244736, acc 0.9375, prec 0.0872361, recall 0.817416
2017-12-10T03:50:36.615868: step 5353, loss 0.0387691, acc 0.984375, prec 0.0872477, recall 0.817441
2017-12-10T03:50:36.886680: step 5354, loss 0.177187, acc 0.953125, prec 0.0872696, recall 0.817488
2017-12-10T03:50:37.154500: step 5355, loss 0.215657, acc 0.96875, prec 0.0872928, recall 0.817536
2017-12-10T03:50:37.420748: step 5356, loss 0.170284, acc 0.953125, prec 0.0873147, recall 0.817584
2017-12-10T03:50:37.686713: step 5357, loss 0.363732, acc 0.9375, prec 0.0873098, recall 0.817584
2017-12-10T03:50:37.952410: step 5358, loss 0.190922, acc 0.953125, prec 0.0873062, recall 0.817584
2017-12-10T03:50:38.215270: step 5359, loss 0.124394, acc 0.9375, prec 0.0873141, recall 0.817608
2017-12-10T03:50:38.482733: step 5360, loss 0.203602, acc 0.984375, prec 0.0873641, recall 0.817704
2017-12-10T03:50:38.745871: step 5361, loss 0.105685, acc 0.96875, prec 0.0873616, recall 0.817704
2017-12-10T03:50:39.012166: step 5362, loss 0.124399, acc 0.984375, prec 0.0873604, recall 0.817704
2017-12-10T03:50:39.279188: step 5363, loss 0.0939486, acc 0.96875, prec 0.0873707, recall 0.817728
2017-12-10T03:50:39.543857: step 5364, loss 2.22055, acc 0.953125, prec 0.0873811, recall 0.817645
2017-12-10T03:50:39.812818: step 5365, loss 0.0046825, acc 1, prec 0.0873939, recall 0.817669
2017-12-10T03:50:40.084339: step 5366, loss 0.45738, acc 0.96875, prec 0.0874299, recall 0.81774
2017-12-10T03:50:40.352242: step 5367, loss 0.0730225, acc 0.96875, prec 0.0874402, recall 0.817764
2017-12-10T03:50:40.626500: step 5368, loss 0.120067, acc 0.96875, prec 0.0874377, recall 0.817764
2017-12-10T03:50:40.904580: step 5369, loss 0.264713, acc 0.953125, prec 0.0874341, recall 0.817764
2017-12-10T03:50:41.174586: step 5370, loss 0.204376, acc 0.9375, prec 0.087442, recall 0.817788
2017-12-10T03:50:41.440266: step 5371, loss 0.171691, acc 0.953125, prec 0.0874383, recall 0.817788
2017-12-10T03:50:41.710483: step 5372, loss 0.486085, acc 0.921875, prec 0.087445, recall 0.817812
2017-12-10T03:50:41.984149: step 5373, loss 0.306685, acc 0.890625, prec 0.0874492, recall 0.817836
2017-12-10T03:50:42.249594: step 5374, loss 0.735192, acc 0.859375, prec 0.0874637, recall 0.817884
2017-12-10T03:50:42.511936: step 5375, loss 0.286273, acc 0.875, prec 0.0874795, recall 0.817932
2017-12-10T03:50:42.777782: step 5376, loss 0.157974, acc 0.953125, prec 0.0874758, recall 0.817932
2017-12-10T03:50:43.042659: step 5377, loss 0.206358, acc 0.9375, prec 0.0874837, recall 0.817955
2017-12-10T03:50:43.305699: step 5378, loss 0.318886, acc 0.953125, prec 0.08748, recall 0.817955
2017-12-10T03:50:43.574078: step 5379, loss 0.410945, acc 0.90625, prec 0.0874855, recall 0.817979
2017-12-10T03:50:43.845381: step 5380, loss 0.15581, acc 0.9375, prec 0.0875061, recall 0.818027
2017-12-10T03:50:44.113422: step 5381, loss 0.332062, acc 0.875, prec 0.0874963, recall 0.818027
2017-12-10T03:50:44.382488: step 5382, loss 0.238183, acc 0.953125, prec 0.0875054, recall 0.818051
2017-12-10T03:50:44.657437: step 5383, loss 0.393035, acc 0.890625, prec 0.0875096, recall 0.818075
2017-12-10T03:50:44.926240: step 5384, loss 0.18264, acc 0.953125, prec 0.0875315, recall 0.818122
2017-12-10T03:50:45.194808: step 5385, loss 0.254319, acc 0.96875, prec 0.0875418, recall 0.818146
2017-12-10T03:50:45.461444: step 5386, loss 0.1045, acc 0.96875, prec 0.0875394, recall 0.818146
2017-12-10T03:50:45.727863: step 5387, loss 0.12324, acc 0.96875, prec 0.0875497, recall 0.81817
2017-12-10T03:50:45.996827: step 5388, loss 0.0248106, acc 1, prec 0.0875753, recall 0.818218
2017-12-10T03:50:46.267399: step 5389, loss 0.2755, acc 0.9375, prec 0.0875704, recall 0.818218
2017-12-10T03:50:46.535949: step 5390, loss 0.0209285, acc 0.984375, prec 0.0875692, recall 0.818218
2017-12-10T03:50:46.804264: step 5391, loss 0.0312453, acc 0.984375, prec 0.0875807, recall 0.818241
2017-12-10T03:50:47.065612: step 5392, loss 0.00954925, acc 1, prec 0.0875807, recall 0.818241
2017-12-10T03:50:47.335210: step 5393, loss 0.0414067, acc 0.96875, prec 0.087591, recall 0.818265
2017-12-10T03:50:47.611074: step 5394, loss 0.192779, acc 0.953125, prec 0.0876001, recall 0.818289
2017-12-10T03:50:47.876684: step 5395, loss 0.11337, acc 0.984375, prec 0.0876245, recall 0.818336
2017-12-10T03:50:48.143300: step 5396, loss 0.0125513, acc 1, prec 0.0876372, recall 0.81836
2017-12-10T03:50:48.411908: step 5397, loss 0.881578, acc 1, prec 0.08765, recall 0.818384
2017-12-10T03:50:48.684409: step 5398, loss 0.362217, acc 0.9375, prec 0.0876579, recall 0.818408
2017-12-10T03:50:48.948976: step 5399, loss 0.242123, acc 0.953125, prec 0.0876542, recall 0.818408
2017-12-10T03:50:49.215866: step 5400, loss 0.0242513, acc 0.984375, prec 0.087653, recall 0.818408

Evaluation:
2017-12-10T03:50:56.793073: step 5400, loss 6.77077, acc 0.96311, prec 0.087923, recall 0.808846

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-5400

2017-12-10T03:50:58.066595: step 5401, loss 0.104751, acc 0.984375, prec 0.0879344, recall 0.808871
2017-12-10T03:50:58.332997: step 5402, loss 0.188731, acc 0.96875, prec 0.0879447, recall 0.808895
2017-12-10T03:50:58.604113: step 5403, loss 0.205067, acc 0.96875, prec 0.0879677, recall 0.808944
2017-12-10T03:50:58.875261: step 5404, loss 0.018975, acc 0.984375, prec 0.0879664, recall 0.808944
2017-12-10T03:50:59.142113: step 5405, loss 0.173976, acc 0.96875, prec 0.0879767, recall 0.808969
2017-12-10T03:50:59.412991: step 5406, loss 0.0544738, acc 0.984375, prec 0.0880009, recall 0.809018
2017-12-10T03:50:59.685294: step 5407, loss 0.107114, acc 0.96875, prec 0.0880111, recall 0.809042
2017-12-10T03:50:59.947624: step 5408, loss 0.0823343, acc 0.96875, prec 0.0880214, recall 0.809066
2017-12-10T03:51:00.223724: step 5409, loss 0.10966, acc 0.953125, prec 0.0880177, recall 0.809066
2017-12-10T03:51:00.496901: step 5410, loss 0.453673, acc 0.921875, prec 0.0880243, recall 0.809091
2017-12-10T03:51:00.770543: step 5411, loss 0.187304, acc 0.921875, prec 0.0880309, recall 0.809115
2017-12-10T03:51:01.037952: step 5412, loss 0.0612993, acc 1, prec 0.0880436, recall 0.80914
2017-12-10T03:51:01.308094: step 5413, loss 0.20674, acc 0.921875, prec 0.0880501, recall 0.809164
2017-12-10T03:51:01.574607: step 5414, loss 0.212927, acc 1, prec 0.0880755, recall 0.809213
2017-12-10T03:51:01.841451: step 5415, loss 0.103634, acc 0.953125, prec 0.0880846, recall 0.809237
2017-12-10T03:51:02.114793: step 5416, loss 0.190693, acc 0.953125, prec 0.0880936, recall 0.809262
2017-12-10T03:51:02.385249: step 5417, loss 0.159217, acc 0.9375, prec 0.0881014, recall 0.809286
2017-12-10T03:51:02.654574: step 5418, loss 0.00159742, acc 1, prec 0.0881141, recall 0.809311
2017-12-10T03:51:02.915526: step 5419, loss 0.142796, acc 0.96875, prec 0.088137, recall 0.809359
2017-12-10T03:51:03.177819: step 5420, loss 0.0405331, acc 0.96875, prec 0.0881346, recall 0.809359
2017-12-10T03:51:03.450383: step 5421, loss 0.0163236, acc 1, prec 0.0881346, recall 0.809359
2017-12-10T03:51:03.713752: step 5422, loss 0.0637062, acc 0.984375, prec 0.0881333, recall 0.809359
2017-12-10T03:51:03.980404: step 5423, loss 0.520349, acc 0.9375, prec 0.0881284, recall 0.809359
2017-12-10T03:51:04.243044: step 5424, loss 0.00240774, acc 1, prec 0.0881411, recall 0.809384
2017-12-10T03:51:04.509460: step 5425, loss 0.223084, acc 0.921875, prec 0.088135, recall 0.809384
2017-12-10T03:51:04.774733: step 5426, loss 0.736143, acc 0.90625, prec 0.0881403, recall 0.809408
2017-12-10T03:51:05.040666: step 5427, loss 0.245817, acc 0.953125, prec 0.0881366, recall 0.809408
2017-12-10T03:51:05.308488: step 5428, loss 0.0927723, acc 0.984375, prec 0.0881608, recall 0.809457
2017-12-10T03:51:05.577525: step 5429, loss 0.123726, acc 0.96875, prec 0.0881583, recall 0.809457
2017-12-10T03:51:05.844666: step 5430, loss 0.0781003, acc 0.984375, prec 0.0881571, recall 0.809457
2017-12-10T03:51:06.121286: step 5431, loss 0.211025, acc 0.96875, prec 0.0881547, recall 0.809457
2017-12-10T03:51:06.389704: step 5432, loss 0.543741, acc 0.96875, prec 0.0881649, recall 0.809481
2017-12-10T03:51:06.662637: step 5433, loss 0.209525, acc 0.953125, prec 0.0881612, recall 0.809481
2017-12-10T03:51:06.931599: step 5434, loss 0.865382, acc 0.96875, prec 0.0881714, recall 0.809506
2017-12-10T03:51:07.210510: step 5435, loss 0.308762, acc 0.9375, prec 0.0881792, recall 0.80953
2017-12-10T03:51:07.473286: step 5436, loss 0.0326328, acc 0.984375, prec 0.0881907, recall 0.809554
2017-12-10T03:51:07.742744: step 5437, loss 0.0398502, acc 0.984375, prec 0.0881895, recall 0.809554
2017-12-10T03:51:08.007462: step 5438, loss 0.0636526, acc 0.953125, prec 0.0881858, recall 0.809554
2017-12-10T03:51:08.269678: step 5439, loss 0.251965, acc 0.96875, prec 0.0881833, recall 0.809554
2017-12-10T03:51:08.539017: step 5440, loss 0.272609, acc 0.96875, prec 0.0881809, recall 0.809554
2017-12-10T03:51:08.807076: step 5441, loss 0.425127, acc 0.984375, prec 0.088205, recall 0.809603
2017-12-10T03:51:09.078961: step 5442, loss 0.323327, acc 0.9375, prec 0.0882001, recall 0.809603
2017-12-10T03:51:09.345093: step 5443, loss 0.00853833, acc 1, prec 0.0882255, recall 0.809651
2017-12-10T03:51:09.612824: step 5444, loss 0.33414, acc 0.96875, prec 0.088223, recall 0.809651
2017-12-10T03:51:09.879349: step 5445, loss 0.383554, acc 0.921875, prec 0.0882422, recall 0.8097
2017-12-10T03:51:10.143704: step 5446, loss 1.1782, acc 0.875, prec 0.0882324, recall 0.8097
2017-12-10T03:51:10.406670: step 5447, loss 0.619067, acc 0.9375, prec 0.0882529, recall 0.809749
2017-12-10T03:51:10.678684: step 5448, loss 0.466467, acc 0.9375, prec 0.088248, recall 0.809749
2017-12-10T03:51:10.946480: step 5449, loss 0.459665, acc 0.9375, prec 0.0882431, recall 0.809749
2017-12-10T03:51:11.219910: step 5450, loss 0.256345, acc 0.9375, prec 0.0882635, recall 0.809797
2017-12-10T03:51:11.488824: step 5451, loss 0.190808, acc 0.921875, prec 0.0882701, recall 0.809821
2017-12-10T03:51:11.764561: step 5452, loss 0.092402, acc 0.96875, prec 0.0882676, recall 0.809821
2017-12-10T03:51:12.032053: step 5453, loss 0.152233, acc 0.96875, prec 0.0882651, recall 0.809821
2017-12-10T03:51:12.297796: step 5454, loss 0.225335, acc 0.984375, prec 0.0883146, recall 0.809918
2017-12-10T03:51:12.567583: step 5455, loss 0.149712, acc 0.953125, prec 0.0883363, recall 0.809967
2017-12-10T03:51:12.835820: step 5456, loss 0.154425, acc 0.953125, prec 0.0883326, recall 0.809967
2017-12-10T03:51:13.101052: step 5457, loss 0.187361, acc 0.953125, prec 0.0883543, recall 0.810015
2017-12-10T03:51:13.375335: step 5458, loss 0.0243938, acc 1, prec 0.0883543, recall 0.810015
2017-12-10T03:51:13.641606: step 5459, loss 0.0648741, acc 0.96875, prec 0.0883645, recall 0.81004
2017-12-10T03:51:13.909693: step 5460, loss 0.1817, acc 0.953125, prec 0.0883608, recall 0.81004
2017-12-10T03:51:14.175549: step 5461, loss 0.128743, acc 0.984375, prec 0.0883722, recall 0.810064
2017-12-10T03:51:14.442848: step 5462, loss 0.229284, acc 0.96875, prec 0.0883824, recall 0.810088
2017-12-10T03:51:14.706682: step 5463, loss 0.0757139, acc 0.984375, prec 0.0883812, recall 0.810088
2017-12-10T03:51:14.972510: step 5464, loss 0.355642, acc 0.953125, prec 0.0883902, recall 0.810112
2017-12-10T03:51:15.240193: step 5465, loss 10.2356, acc 0.96875, prec 0.088389, recall 0.810009
2017-12-10T03:51:15.511593: step 5466, loss 0.250746, acc 0.921875, prec 0.0883955, recall 0.810033
2017-12-10T03:51:15.741433: step 5467, loss 0.259476, acc 0.921569, prec 0.0884032, recall 0.810057
2017-12-10T03:51:16.015318: step 5468, loss 0.217703, acc 0.90625, prec 0.0884085, recall 0.810081
2017-12-10T03:51:16.283676: step 5469, loss 0.329067, acc 0.921875, prec 0.0884151, recall 0.810106
2017-12-10T03:51:16.552559: step 5470, loss 0.626087, acc 0.890625, prec 0.0884191, recall 0.81013
2017-12-10T03:51:16.819008: step 5471, loss 0.326754, acc 0.875, prec 0.0884346, recall 0.810178
2017-12-10T03:51:17.086853: step 5472, loss 0.506938, acc 0.890625, prec 0.0884387, recall 0.810202
2017-12-10T03:51:17.358140: step 5473, loss 0.127691, acc 0.921875, prec 0.0884452, recall 0.810226
2017-12-10T03:51:17.632612: step 5474, loss 0.108739, acc 0.953125, prec 0.0884668, recall 0.810275
2017-12-10T03:51:17.898328: step 5475, loss 0.606608, acc 0.875, prec 0.0884697, recall 0.810299
2017-12-10T03:51:18.173994: step 5476, loss 0.194977, acc 0.9375, prec 0.0884774, recall 0.810323
2017-12-10T03:51:18.443169: step 5477, loss 0.301229, acc 0.9375, prec 0.0884978, recall 0.810371
2017-12-10T03:51:18.708539: step 5478, loss 0.333684, acc 0.921875, prec 0.0884916, recall 0.810371
2017-12-10T03:51:18.983476: step 5479, loss 0.553498, acc 0.890625, prec 0.0884957, recall 0.810395
2017-12-10T03:51:19.250998: step 5480, loss 0.696656, acc 0.921875, prec 0.0884896, recall 0.810395
2017-12-10T03:51:19.516099: step 5481, loss 0.563758, acc 0.921875, prec 0.0884961, recall 0.810419
2017-12-10T03:51:19.783087: step 5482, loss 0.346159, acc 0.9375, prec 0.0885038, recall 0.810443
2017-12-10T03:51:20.057123: step 5483, loss 0.141736, acc 0.953125, prec 0.0885001, recall 0.810443
2017-12-10T03:51:20.324793: step 5484, loss 0.230925, acc 0.96875, prec 0.0885103, recall 0.810467
2017-12-10T03:51:20.605624: step 5485, loss 0.369882, acc 0.9375, prec 0.0885054, recall 0.810467
2017-12-10T03:51:20.877414: step 5486, loss 0.216647, acc 0.9375, prec 0.0885258, recall 0.810516
2017-12-10T03:51:21.147205: step 5487, loss 0.142332, acc 0.953125, prec 0.0885347, recall 0.81054
2017-12-10T03:51:21.413402: step 5488, loss 0.194891, acc 0.953125, prec 0.0885437, recall 0.810564
2017-12-10T03:51:21.679014: step 5489, loss 0.186418, acc 0.96875, prec 0.0885539, recall 0.810588
2017-12-10T03:51:21.948624: step 5490, loss 0.0976279, acc 0.96875, prec 0.0885514, recall 0.810588
2017-12-10T03:51:22.215280: step 5491, loss 0.0468429, acc 0.984375, prec 0.0885502, recall 0.810588
2017-12-10T03:51:22.487195: step 5492, loss 0.193053, acc 0.96875, prec 0.0885477, recall 0.810588
2017-12-10T03:51:22.752343: step 5493, loss 0.27096, acc 0.953125, prec 0.0885567, recall 0.810612
2017-12-10T03:51:23.017710: step 5494, loss 0.195033, acc 0.96875, prec 0.0885795, recall 0.81066
2017-12-10T03:51:23.282648: step 5495, loss 1.02396, acc 0.953125, prec 0.0885885, recall 0.810684
2017-12-10T03:51:23.556975: step 5496, loss 0.00823992, acc 1, prec 0.0886011, recall 0.810708
2017-12-10T03:51:23.820094: step 5497, loss 0.150501, acc 0.953125, prec 0.0886227, recall 0.810756
2017-12-10T03:51:24.090189: step 5498, loss 0.134592, acc 0.96875, prec 0.0886455, recall 0.810804
2017-12-10T03:51:24.354998: step 5499, loss 0.489827, acc 0.921875, prec 0.0886646, recall 0.810852
2017-12-10T03:51:24.628516: step 5500, loss 1.55431, acc 0.953125, prec 0.0886748, recall 0.810773
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-5500

2017-12-10T03:51:25.973691: step 5501, loss 0.119992, acc 0.96875, prec 0.0886723, recall 0.810773
2017-12-10T03:51:26.238353: step 5502, loss 0.00610159, acc 1, prec 0.0886723, recall 0.810773
2017-12-10T03:51:26.506629: step 5503, loss 0.251296, acc 0.96875, prec 0.0886699, recall 0.810773
2017-12-10T03:51:26.781216: step 5504, loss 0.0563443, acc 0.984375, prec 0.0886813, recall 0.810797
2017-12-10T03:51:27.051091: step 5505, loss 0.326426, acc 0.96875, prec 0.0886915, recall 0.810821
2017-12-10T03:51:27.331266: step 5506, loss 0.566823, acc 0.90625, prec 0.0887346, recall 0.810917
2017-12-10T03:51:27.603032: step 5507, loss 0.106624, acc 0.953125, prec 0.0887309, recall 0.810917
2017-12-10T03:51:27.871807: step 5508, loss 0.662049, acc 0.921875, prec 0.0887374, recall 0.810941
2017-12-10T03:51:28.143919: step 5509, loss 0.339765, acc 0.9375, prec 0.0887577, recall 0.810989
2017-12-10T03:51:28.404817: step 5510, loss 0.0481678, acc 0.96875, prec 0.0887805, recall 0.811037
2017-12-10T03:51:28.673417: step 5511, loss 0.104231, acc 0.96875, prec 0.0887907, recall 0.81106
2017-12-10T03:51:28.938118: step 5512, loss 0.280613, acc 0.90625, prec 0.0888085, recall 0.811108
2017-12-10T03:51:29.209375: step 5513, loss 0.539758, acc 0.9375, prec 0.0888162, recall 0.811132
2017-12-10T03:51:29.480736: step 5514, loss 0.0744057, acc 0.953125, prec 0.0888378, recall 0.81118
2017-12-10T03:51:29.750494: step 5515, loss 0.332641, acc 0.96875, prec 0.088848, recall 0.811204
2017-12-10T03:51:30.023449: step 5516, loss 0.112746, acc 0.9375, prec 0.088843, recall 0.811204
2017-12-10T03:51:30.291235: step 5517, loss 0.289589, acc 0.921875, prec 0.0888369, recall 0.811204
2017-12-10T03:51:30.569599: step 5518, loss 0.0181029, acc 1, prec 0.0888621, recall 0.811252
2017-12-10T03:51:30.830294: step 5519, loss 0.0246825, acc 0.984375, prec 0.0888735, recall 0.811275
2017-12-10T03:51:31.098821: step 5520, loss 0.104177, acc 0.953125, prec 0.0888824, recall 0.811299
2017-12-10T03:51:31.364114: step 5521, loss 0.170486, acc 0.953125, prec 0.0888913, recall 0.811323
2017-12-10T03:51:31.638720: step 5522, loss 0.0742163, acc 0.984375, prec 0.0889027, recall 0.811347
2017-12-10T03:51:31.907766: step 5523, loss 0.00580363, acc 1, prec 0.0889153, recall 0.811371
2017-12-10T03:51:32.167414: step 5524, loss 0.350623, acc 0.984375, prec 0.0889393, recall 0.811418
2017-12-10T03:51:32.439476: step 5525, loss 0.0147306, acc 0.984375, prec 0.0889381, recall 0.811418
2017-12-10T03:51:32.705644: step 5526, loss 0.0172495, acc 1, prec 0.088976, recall 0.81149
2017-12-10T03:51:32.970153: step 5527, loss 0.160034, acc 0.96875, prec 0.0889735, recall 0.81149
2017-12-10T03:51:33.235310: step 5528, loss 0.0635673, acc 0.96875, prec 0.0889836, recall 0.811514
2017-12-10T03:51:33.500662: step 5529, loss 0.32213, acc 0.96875, prec 0.0889938, recall 0.811538
2017-12-10T03:51:33.765847: step 5530, loss 0.121208, acc 0.9375, prec 0.0890015, recall 0.811561
2017-12-10T03:51:34.031567: step 5531, loss 0.0100392, acc 1, prec 0.0890141, recall 0.811585
2017-12-10T03:51:34.294560: step 5532, loss 0.116428, acc 0.96875, prec 0.0890242, recall 0.811609
2017-12-10T03:51:34.558419: step 5533, loss 0.0512284, acc 0.984375, prec 0.089023, recall 0.811609
2017-12-10T03:51:34.830894: step 5534, loss 0.0293368, acc 0.984375, prec 0.0890218, recall 0.811609
2017-12-10T03:51:35.096788: step 5535, loss 0.0253503, acc 1, prec 0.0890218, recall 0.811609
2017-12-10T03:51:35.371398: step 5536, loss 0.0412559, acc 0.96875, prec 0.0890193, recall 0.811609
2017-12-10T03:51:35.641850: step 5537, loss 0.0865197, acc 0.953125, prec 0.0890156, recall 0.811609
2017-12-10T03:51:35.908720: step 5538, loss 0.0872492, acc 0.984375, prec 0.0890144, recall 0.811609
2017-12-10T03:51:36.175770: step 5539, loss 0.11183, acc 0.984375, prec 0.0890257, recall 0.811633
2017-12-10T03:51:36.442436: step 5540, loss 0.11351, acc 0.96875, prec 0.0890233, recall 0.811633
2017-12-10T03:51:36.710669: step 5541, loss 0.0835952, acc 0.953125, prec 0.0890448, recall 0.81168
2017-12-10T03:51:36.985519: step 5542, loss 0.0153619, acc 1, prec 0.0890448, recall 0.81168
2017-12-10T03:51:37.257949: step 5543, loss 0.0419899, acc 0.984375, prec 0.0890562, recall 0.811704
2017-12-10T03:51:37.527818: step 5544, loss 0.00490645, acc 1, prec 0.0890562, recall 0.811704
2017-12-10T03:51:37.790719: step 5545, loss 0.143581, acc 0.96875, prec 0.0890663, recall 0.811728
2017-12-10T03:51:38.063497: step 5546, loss 0.0133294, acc 1, prec 0.0890663, recall 0.811728
2017-12-10T03:51:38.330632: step 5547, loss 0.00352257, acc 1, prec 0.0890789, recall 0.811751
2017-12-10T03:51:38.597117: step 5548, loss 0.17892, acc 0.96875, prec 0.0890764, recall 0.811751
2017-12-10T03:51:38.862370: step 5549, loss 0.0598949, acc 0.984375, prec 0.0890878, recall 0.811775
2017-12-10T03:51:39.129418: step 5550, loss 0.202138, acc 0.953125, prec 0.0890967, recall 0.811799
2017-12-10T03:51:39.399401: step 5551, loss 0.267184, acc 1, prec 0.0891345, recall 0.81187
2017-12-10T03:51:39.667990: step 5552, loss 0.0154285, acc 0.984375, prec 0.0891585, recall 0.811917
2017-12-10T03:51:39.937435: step 5553, loss 0.0234154, acc 0.984375, prec 0.0891573, recall 0.811917
2017-12-10T03:51:40.206878: step 5554, loss 0.00567504, acc 1, prec 0.0891699, recall 0.811941
2017-12-10T03:51:40.472535: step 5555, loss 0.26294, acc 0.96875, prec 0.08918, recall 0.811965
2017-12-10T03:51:40.745399: step 5556, loss 0.00222489, acc 1, prec 0.0891926, recall 0.811988
2017-12-10T03:51:41.014400: step 5557, loss 0.241263, acc 0.96875, prec 0.0891901, recall 0.811988
2017-12-10T03:51:41.281666: step 5558, loss 0.0123548, acc 0.984375, prec 0.0891889, recall 0.811988
2017-12-10T03:51:41.544843: step 5559, loss 0.00791587, acc 1, prec 0.0891889, recall 0.811988
2017-12-10T03:51:41.823373: step 5560, loss 0.606304, acc 0.984375, prec 0.0892003, recall 0.812012
2017-12-10T03:51:42.094754: step 5561, loss 0.164187, acc 0.9375, prec 0.0892079, recall 0.812036
2017-12-10T03:51:42.366377: step 5562, loss 0.00348264, acc 1, prec 0.0892205, recall 0.812059
2017-12-10T03:51:42.634830: step 5563, loss 0.165657, acc 0.96875, prec 0.0892306, recall 0.812083
2017-12-10T03:51:42.905556: step 5564, loss 0.0331141, acc 0.984375, prec 0.089242, recall 0.812107
2017-12-10T03:51:43.177770: step 5565, loss 0.113049, acc 0.96875, prec 0.0892521, recall 0.81213
2017-12-10T03:51:43.450586: step 5566, loss 0.105614, acc 0.984375, prec 0.0892761, recall 0.812178
2017-12-10T03:51:43.721203: step 5567, loss 0.0627322, acc 0.984375, prec 0.0892748, recall 0.812178
2017-12-10T03:51:43.988905: step 5568, loss 0.00616506, acc 1, prec 0.0893, recall 0.812225
2017-12-10T03:51:44.254322: step 5569, loss 3.75248, acc 0.96875, prec 0.0892988, recall 0.812123
2017-12-10T03:51:44.525535: step 5570, loss 0.118476, acc 0.96875, prec 0.0893089, recall 0.812146
2017-12-10T03:51:44.791014: step 5571, loss 0.0418523, acc 0.96875, prec 0.0893065, recall 0.812146
2017-12-10T03:51:45.056859: step 5572, loss 0.116869, acc 0.953125, prec 0.0893153, recall 0.81217
2017-12-10T03:51:45.321532: step 5573, loss 0.24687, acc 0.90625, prec 0.0893205, recall 0.812194
2017-12-10T03:51:45.590193: step 5574, loss 0.00787124, acc 1, prec 0.0893331, recall 0.812217
2017-12-10T03:51:45.862410: step 5575, loss 0.27501, acc 0.9375, prec 0.0893408, recall 0.812241
2017-12-10T03:51:46.128310: step 5576, loss 0.398809, acc 0.953125, prec 0.0893496, recall 0.812264
2017-12-10T03:51:46.404027: step 5577, loss 0.225851, acc 0.9375, prec 0.0893447, recall 0.812264
2017-12-10T03:51:46.673594: step 5578, loss 0.154447, acc 0.953125, prec 0.0893536, recall 0.812288
2017-12-10T03:51:46.950331: step 5579, loss 0.647079, acc 0.90625, prec 0.0893588, recall 0.812312
2017-12-10T03:51:47.214620: step 5580, loss 0.256978, acc 0.96875, prec 0.0893815, recall 0.812359
2017-12-10T03:51:47.485875: step 5581, loss 0.184205, acc 0.9375, prec 0.0893891, recall 0.812382
2017-12-10T03:51:47.753360: step 5582, loss 0.203118, acc 0.96875, prec 0.0893992, recall 0.812406
2017-12-10T03:51:48.017771: step 5583, loss 0.893159, acc 0.921875, prec 0.0894434, recall 0.8125
2017-12-10T03:51:48.283454: step 5584, loss 0.300232, acc 0.921875, prec 0.0894372, recall 0.8125
2017-12-10T03:51:48.554581: step 5585, loss 0.0559629, acc 0.984375, prec 0.0894359, recall 0.8125
2017-12-10T03:51:48.823893: step 5586, loss 0.280942, acc 0.921875, prec 0.0894423, recall 0.812524
2017-12-10T03:51:49.087815: step 5587, loss 0.317864, acc 0.90625, prec 0.0894475, recall 0.812547
2017-12-10T03:51:49.355572: step 5588, loss 0.294896, acc 0.953125, prec 0.0894815, recall 0.812618
2017-12-10T03:51:49.629731: step 5589, loss 0.285314, acc 0.96875, prec 0.0895042, recall 0.812665
2017-12-10T03:51:49.897886: step 5590, loss 0.137854, acc 0.953125, prec 0.0895256, recall 0.812712
2017-12-10T03:51:50.163895: step 5591, loss 0.0566949, acc 0.96875, prec 0.0895358, recall 0.812735
2017-12-10T03:51:50.436756: step 5592, loss 0.259987, acc 0.9375, prec 0.0895308, recall 0.812735
2017-12-10T03:51:50.702981: step 5593, loss 0.212095, acc 0.90625, prec 0.0895234, recall 0.812735
2017-12-10T03:51:50.968111: step 5594, loss 0.0651192, acc 0.96875, prec 0.0895209, recall 0.812735
2017-12-10T03:51:51.233847: step 5595, loss 1.03292, acc 0.984375, prec 0.0895448, recall 0.812782
2017-12-10T03:51:51.506057: step 5596, loss 0.00167315, acc 1, prec 0.0895448, recall 0.812782
2017-12-10T03:51:51.772851: step 5597, loss 0.682627, acc 0.953125, prec 0.0895663, recall 0.812829
2017-12-10T03:51:52.041568: step 5598, loss 0.233703, acc 0.9375, prec 0.0895613, recall 0.812829
2017-12-10T03:51:52.306120: step 5599, loss 0.618507, acc 0.953125, prec 0.0895702, recall 0.812852
2017-12-10T03:51:52.576734: step 5600, loss 0.254757, acc 0.9375, prec 0.0895778, recall 0.812876
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-5600

2017-12-10T03:51:53.850461: step 5601, loss 0.222655, acc 0.953125, prec 0.0896118, recall 0.812946
2017-12-10T03:51:54.127463: step 5602, loss 0.270154, acc 0.890625, prec 0.0896157, recall 0.812969
2017-12-10T03:51:54.400139: step 5603, loss 0.759212, acc 0.875, prec 0.0896183, recall 0.812993
2017-12-10T03:51:54.669850: step 5604, loss 0.589292, acc 0.90625, prec 0.0896235, recall 0.813016
2017-12-10T03:51:54.937083: step 5605, loss 0.0719038, acc 0.984375, prec 0.0896348, recall 0.81304
2017-12-10T03:51:55.203982: step 5606, loss 0.537797, acc 0.90625, prec 0.0896399, recall 0.813063
2017-12-10T03:51:55.470924: step 5607, loss 0.370882, acc 0.921875, prec 0.0896338, recall 0.813063
2017-12-10T03:51:55.739548: step 5608, loss 0.699509, acc 0.90625, prec 0.0896515, recall 0.81311
2017-12-10T03:51:56.010469: step 5609, loss 0.0894062, acc 0.96875, prec 0.089649, recall 0.81311
2017-12-10T03:51:56.278633: step 5610, loss 0.120224, acc 0.96875, prec 0.0896842, recall 0.81318
2017-12-10T03:51:56.550334: step 5611, loss 0.149982, acc 0.953125, prec 0.089693, recall 0.813203
2017-12-10T03:51:56.825607: step 5612, loss 0.0120573, acc 1, prec 0.089693, recall 0.813203
2017-12-10T03:51:57.088584: step 5613, loss 0.368151, acc 0.96875, prec 0.0897031, recall 0.813227
2017-12-10T03:51:57.352975: step 5614, loss 0.0428409, acc 0.984375, prec 0.0897144, recall 0.81325
2017-12-10T03:51:57.619901: step 5615, loss 0.723402, acc 0.90625, prec 0.0897195, recall 0.813273
2017-12-10T03:51:57.886243: step 5616, loss 0.616293, acc 0.90625, prec 0.0897372, recall 0.81332
2017-12-10T03:51:58.160007: step 5617, loss 0.315407, acc 0.921875, prec 0.0897561, recall 0.813367
2017-12-10T03:51:58.429791: step 5618, loss 0.0869315, acc 0.96875, prec 0.0897662, recall 0.81339
2017-12-10T03:51:58.702984: step 5619, loss 0.104785, acc 0.96875, prec 0.0898014, recall 0.81346
2017-12-10T03:51:58.972154: step 5620, loss 0.103485, acc 0.96875, prec 0.089824, recall 0.813506
2017-12-10T03:51:59.240985: step 5621, loss 0.280313, acc 0.96875, prec 0.0898215, recall 0.813506
2017-12-10T03:51:59.511497: step 5622, loss 0.260354, acc 0.953125, prec 0.0898554, recall 0.813576
2017-12-10T03:51:59.772380: step 5623, loss 0.102799, acc 0.96875, prec 0.089853, recall 0.813576
2017-12-10T03:52:00.039216: step 5624, loss 0.138488, acc 0.984375, prec 0.0898643, recall 0.8136
2017-12-10T03:52:00.309542: step 5625, loss 0.0668188, acc 0.953125, prec 0.0898605, recall 0.8136
2017-12-10T03:52:00.597944: step 5626, loss 0.220562, acc 0.984375, prec 0.089922, recall 0.813716
2017-12-10T03:52:00.862469: step 5627, loss 1.12918, acc 0.96875, prec 0.0899321, recall 0.813739
2017-12-10T03:52:01.137082: step 5628, loss 0.00775934, acc 1, prec 0.0899446, recall 0.813762
2017-12-10T03:52:01.400278: step 5629, loss 0.00968658, acc 1, prec 0.0899446, recall 0.813762
2017-12-10T03:52:01.664335: step 5630, loss 0.0655977, acc 0.96875, prec 0.0899547, recall 0.813785
2017-12-10T03:52:01.934132: step 5631, loss 0.0860214, acc 0.96875, prec 0.0899773, recall 0.813832
2017-12-10T03:52:02.201735: step 5632, loss 0.0082002, acc 1, prec 0.0900023, recall 0.813878
2017-12-10T03:52:02.469167: step 5633, loss 0.00530423, acc 1, prec 0.0900023, recall 0.813878
2017-12-10T03:52:02.740670: step 5634, loss 0.0203443, acc 0.984375, prec 0.0900011, recall 0.813878
2017-12-10T03:52:03.003541: step 5635, loss 0.16696, acc 0.96875, prec 0.0900112, recall 0.813901
2017-12-10T03:52:03.271893: step 5636, loss 0.00201475, acc 1, prec 0.0900112, recall 0.813901
2017-12-10T03:52:03.543043: step 5637, loss 0.00611781, acc 1, prec 0.0900112, recall 0.813901
2017-12-10T03:52:03.813767: step 5638, loss 0.13315, acc 0.984375, prec 0.0900225, recall 0.813925
2017-12-10T03:52:04.075955: step 5639, loss 0.00255659, acc 1, prec 0.0900225, recall 0.813925
2017-12-10T03:52:04.339772: step 5640, loss 0.667817, acc 0.96875, prec 0.0900576, recall 0.813994
2017-12-10T03:52:04.613332: step 5641, loss 0.00356957, acc 1, prec 0.0900576, recall 0.813994
2017-12-10T03:52:04.877814: step 5642, loss 0.0155417, acc 1, prec 0.0900701, recall 0.814017
2017-12-10T03:52:05.150552: step 5643, loss 0.111313, acc 0.96875, prec 0.0900802, recall 0.81404
2017-12-10T03:52:05.415681: step 5644, loss 0.0337377, acc 0.984375, prec 0.0900915, recall 0.814063
2017-12-10T03:52:05.687516: step 5645, loss 0.0325392, acc 0.984375, prec 0.0901027, recall 0.814087
2017-12-10T03:52:05.956791: step 5646, loss 0.0393954, acc 0.984375, prec 0.090114, recall 0.81411
2017-12-10T03:52:06.223282: step 5647, loss 0.0804093, acc 0.96875, prec 0.0901366, recall 0.814156
2017-12-10T03:52:06.493292: step 5648, loss 0.333902, acc 0.9375, prec 0.0901316, recall 0.814156
2017-12-10T03:52:06.760660: step 5649, loss 0.0947567, acc 0.984375, prec 0.0901304, recall 0.814156
2017-12-10T03:52:07.025182: step 5650, loss 0.300091, acc 0.984375, prec 0.0901292, recall 0.814156
2017-12-10T03:52:07.292458: step 5651, loss 0.138617, acc 0.921875, prec 0.090123, recall 0.814156
2017-12-10T03:52:07.560104: step 5652, loss 0.517024, acc 0.890625, prec 0.0901268, recall 0.814179
2017-12-10T03:52:07.827658: step 5653, loss 0.0825445, acc 0.984375, prec 0.0901256, recall 0.814179
2017-12-10T03:52:08.093531: step 5654, loss 0.131621, acc 0.96875, prec 0.0901356, recall 0.814202
2017-12-10T03:52:08.356359: step 5655, loss 0.0234558, acc 0.984375, prec 0.0901594, recall 0.814248
2017-12-10T03:52:08.627368: step 5656, loss 0.342802, acc 0.921875, prec 0.0901532, recall 0.814248
2017-12-10T03:52:08.893152: step 5657, loss 0.0884683, acc 0.953125, prec 0.0901495, recall 0.814248
2017-12-10T03:52:09.158951: step 5658, loss 0.100172, acc 0.953125, prec 0.0901458, recall 0.814248
2017-12-10T03:52:09.430179: step 5659, loss 0.109255, acc 0.984375, prec 0.0901445, recall 0.814248
2017-12-10T03:52:09.696337: step 5660, loss 0.287178, acc 0.984375, prec 0.0901558, recall 0.814272
2017-12-10T03:52:09.964833: step 5661, loss 0.03057, acc 0.984375, prec 0.0901671, recall 0.814295
2017-12-10T03:52:10.234823: step 5662, loss 0.265818, acc 0.984375, prec 0.0902034, recall 0.814364
2017-12-10T03:52:10.510416: step 5663, loss 0.0490932, acc 0.984375, prec 0.0902147, recall 0.814387
2017-12-10T03:52:10.789006: step 5664, loss 0.515954, acc 0.96875, prec 0.0902373, recall 0.814433
2017-12-10T03:52:11.054441: step 5665, loss 0.0819598, acc 0.953125, prec 0.0902586, recall 0.814479
2017-12-10T03:52:11.328641: step 5666, loss 0.13914, acc 0.953125, prec 0.0902548, recall 0.814479
2017-12-10T03:52:11.595614: step 5667, loss 0.0185066, acc 1, prec 0.0902799, recall 0.814525
2017-12-10T03:52:11.879125: step 5668, loss 0.103071, acc 0.984375, prec 0.0903162, recall 0.814594
2017-12-10T03:52:12.148695: step 5669, loss 0.0195707, acc 0.984375, prec 0.0903275, recall 0.814617
2017-12-10T03:52:12.413566: step 5670, loss 0.159961, acc 0.953125, prec 0.0903362, recall 0.81464
2017-12-10T03:52:12.681348: step 5671, loss 0.0531867, acc 0.96875, prec 0.0903338, recall 0.81464
2017-12-10T03:52:12.946153: step 5672, loss 0.00471766, acc 1, prec 0.0903463, recall 0.814663
2017-12-10T03:52:13.212677: step 5673, loss 0.326798, acc 0.96875, prec 0.0903688, recall 0.814709
2017-12-10T03:52:13.482694: step 5674, loss 0.137606, acc 0.96875, prec 0.0903789, recall 0.814732
2017-12-10T03:52:13.752269: step 5675, loss 0.0692288, acc 0.984375, prec 0.0903776, recall 0.814732
2017-12-10T03:52:14.022699: step 5676, loss 0.365389, acc 0.984375, prec 0.0903889, recall 0.814755
2017-12-10T03:52:14.290384: step 5677, loss 0.0553966, acc 0.96875, prec 0.0903864, recall 0.814755
2017-12-10T03:52:14.557717: step 5678, loss 0.253819, acc 1, prec 0.0904114, recall 0.814801
2017-12-10T03:52:14.828181: step 5679, loss 0.0155996, acc 0.984375, prec 0.0904227, recall 0.814824
2017-12-10T03:52:15.094471: step 5680, loss 0.22176, acc 0.953125, prec 0.0904189, recall 0.814824
2017-12-10T03:52:15.359022: step 5681, loss 0.241023, acc 0.96875, prec 0.0904415, recall 0.81487
2017-12-10T03:52:15.620607: step 5682, loss 0.263527, acc 0.9375, prec 0.090449, recall 0.814893
2017-12-10T03:52:15.884890: step 5683, loss 0.155462, acc 0.953125, prec 0.0904453, recall 0.814893
2017-12-10T03:52:16.152760: step 5684, loss 0.260946, acc 0.953125, prec 0.0904666, recall 0.814939
2017-12-10T03:52:16.418769: step 5685, loss 0.221061, acc 0.953125, prec 0.0904628, recall 0.814939
2017-12-10T03:52:16.682434: step 5686, loss 0.0780252, acc 0.953125, prec 0.0904591, recall 0.814939
2017-12-10T03:52:16.953647: step 5687, loss 0.00741309, acc 1, prec 0.0904716, recall 0.814962
2017-12-10T03:52:17.222865: step 5688, loss 0.23455, acc 0.96875, prec 0.0904816, recall 0.814985
2017-12-10T03:52:17.488405: step 5689, loss 0.0347838, acc 0.984375, prec 0.0904804, recall 0.814985
2017-12-10T03:52:17.765692: step 5690, loss 0.0203128, acc 1, prec 0.0904929, recall 0.815007
2017-12-10T03:52:18.034947: step 5691, loss 0.229725, acc 0.96875, prec 0.0905029, recall 0.81503
2017-12-10T03:52:18.307444: step 5692, loss 0.0306359, acc 0.984375, prec 0.0905017, recall 0.81503
2017-12-10T03:52:18.576460: step 5693, loss 0.364804, acc 0.921875, prec 0.0905079, recall 0.815053
2017-12-10T03:52:18.843070: step 5694, loss 0.105988, acc 0.96875, prec 0.090518, recall 0.815076
2017-12-10T03:52:19.113987: step 5695, loss 0.186909, acc 0.96875, prec 0.0905155, recall 0.815076
2017-12-10T03:52:19.385424: step 5696, loss 0.00114749, acc 1, prec 0.090528, recall 0.815099
2017-12-10T03:52:19.651547: step 5697, loss 0.000189555, acc 1, prec 0.090528, recall 0.815099
2017-12-10T03:52:19.919965: step 5698, loss 0.386703, acc 0.96875, prec 0.0905505, recall 0.815145
2017-12-10T03:52:20.191378: step 5699, loss 0.133587, acc 0.96875, prec 0.090548, recall 0.815145
2017-12-10T03:52:20.461494: step 5700, loss 0.0363239, acc 0.96875, prec 0.090558, recall 0.815168

Evaluation:
2017-12-10T03:52:28.087284: step 5700, loss 7.90463, acc 0.968488, prec 0.0908581, recall 0.805927

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-5700

2017-12-10T03:52:29.278915: step 5701, loss 0.0431425, acc 0.984375, prec 0.0908568, recall 0.805927
2017-12-10T03:52:29.546063: step 5702, loss 0.194962, acc 1, prec 0.0908693, recall 0.80595
2017-12-10T03:52:29.814302: step 5703, loss 0.0810083, acc 0.984375, prec 0.090868, recall 0.80595
2017-12-10T03:52:30.082840: step 5704, loss 0.22566, acc 0.96875, prec 0.0908655, recall 0.80595
2017-12-10T03:52:30.355952: step 5705, loss 0.136576, acc 0.953125, prec 0.0908618, recall 0.80595
2017-12-10T03:52:30.630785: step 5706, loss 0.177204, acc 0.96875, prec 0.0908966, recall 0.806021
2017-12-10T03:52:30.901747: step 5707, loss 0.00102512, acc 1, prec 0.0908966, recall 0.806021
2017-12-10T03:52:31.168482: step 5708, loss 0.0898472, acc 0.984375, prec 0.0908954, recall 0.806021
2017-12-10T03:52:31.433735: step 5709, loss 0.00762313, acc 1, prec 0.0908954, recall 0.806021
2017-12-10T03:52:31.710910: step 5710, loss 0.1287, acc 0.96875, prec 0.0909054, recall 0.806044
2017-12-10T03:52:31.974981: step 5711, loss 0.00664635, acc 1, prec 0.0909178, recall 0.806068
2017-12-10T03:52:32.244127: step 5712, loss 0.181504, acc 0.96875, prec 0.0909526, recall 0.806139
2017-12-10T03:52:32.509960: step 5713, loss 0.0998429, acc 0.984375, prec 0.0909763, recall 0.806186
2017-12-10T03:52:32.772871: step 5714, loss 1.48733, acc 0.96875, prec 0.090975, recall 0.806088
2017-12-10T03:52:33.049584: step 5715, loss 0.173377, acc 0.96875, prec 0.0909725, recall 0.806088
2017-12-10T03:52:33.319062: step 5716, loss 0.146274, acc 0.984375, prec 0.0910086, recall 0.806158
2017-12-10T03:52:33.586675: step 5717, loss 0.0341628, acc 0.96875, prec 0.091031, recall 0.806205
2017-12-10T03:52:33.864427: step 5718, loss 0.0768512, acc 0.984375, prec 0.0910298, recall 0.806205
2017-12-10T03:52:34.135180: step 5719, loss 0.710728, acc 0.890625, prec 0.0910335, recall 0.806229
2017-12-10T03:52:34.396948: step 5720, loss 0.35731, acc 0.9375, prec 0.0910409, recall 0.806252
2017-12-10T03:52:34.668922: step 5721, loss 0.375326, acc 0.921875, prec 0.091072, recall 0.806323
2017-12-10T03:52:34.933290: step 5722, loss 0.390492, acc 0.953125, prec 0.0910683, recall 0.806323
2017-12-10T03:52:35.197282: step 5723, loss 0.552545, acc 0.890625, prec 0.091072, recall 0.806346
2017-12-10T03:52:35.463921: step 5724, loss 0.357786, acc 0.90625, prec 0.0910645, recall 0.806346
2017-12-10T03:52:35.732814: step 5725, loss 0.113692, acc 0.9375, prec 0.0910595, recall 0.806346
2017-12-10T03:52:36.004522: step 5726, loss 0.252368, acc 0.9375, prec 0.0910546, recall 0.806346
2017-12-10T03:52:36.272472: step 5727, loss 0.0893755, acc 0.984375, prec 0.0910533, recall 0.806346
2017-12-10T03:52:36.537624: step 5728, loss 0.0949845, acc 0.96875, prec 0.0910632, recall 0.80637
2017-12-10T03:52:36.806882: step 5729, loss 0.304642, acc 0.96875, prec 0.0910608, recall 0.80637
2017-12-10T03:52:37.082366: step 5730, loss 0.0777662, acc 0.96875, prec 0.0910831, recall 0.806416
2017-12-10T03:52:37.364172: step 5731, loss 0.616931, acc 0.921875, prec 0.0911018, recall 0.806463
2017-12-10T03:52:37.629511: step 5732, loss 0.102319, acc 0.9375, prec 0.0910968, recall 0.806463
2017-12-10T03:52:37.905913: step 5733, loss 0.534787, acc 0.921875, prec 0.0910905, recall 0.806463
2017-12-10T03:52:38.173217: step 5734, loss 0.128254, acc 0.96875, prec 0.0911129, recall 0.80651
2017-12-10T03:52:38.442643: step 5735, loss 0.396804, acc 0.96875, prec 0.0911104, recall 0.80651
2017-12-10T03:52:38.707910: step 5736, loss 0.24074, acc 0.953125, prec 0.0911067, recall 0.80651
2017-12-10T03:52:38.977733: step 5737, loss 0.241343, acc 0.96875, prec 0.0911042, recall 0.80651
2017-12-10T03:52:39.243560: step 5738, loss 0.0433229, acc 0.984375, prec 0.0911029, recall 0.80651
2017-12-10T03:52:39.509787: step 5739, loss 0.284969, acc 0.921875, prec 0.0911091, recall 0.806534
2017-12-10T03:52:39.777738: step 5740, loss 0.0537539, acc 0.984375, prec 0.0911079, recall 0.806534
2017-12-10T03:52:40.054151: step 5741, loss 0.221351, acc 0.953125, prec 0.0911042, recall 0.806534
2017-12-10T03:52:40.319525: step 5742, loss 0.0874699, acc 1, prec 0.0911166, recall 0.806557
2017-12-10T03:52:40.587619: step 5743, loss 0.746853, acc 0.96875, prec 0.0911389, recall 0.806604
2017-12-10T03:52:40.859861: step 5744, loss 0.106107, acc 0.984375, prec 0.0911501, recall 0.806627
2017-12-10T03:52:41.129221: step 5745, loss 0.249004, acc 0.96875, prec 0.09116, recall 0.806651
2017-12-10T03:52:41.394690: step 5746, loss 0.0118417, acc 1, prec 0.0911724, recall 0.806674
2017-12-10T03:52:41.663063: step 5747, loss 0.387342, acc 0.96875, prec 0.0912072, recall 0.806744
2017-12-10T03:52:41.941785: step 5748, loss 0.0734885, acc 0.96875, prec 0.0912047, recall 0.806744
2017-12-10T03:52:42.204282: step 5749, loss 0.0799041, acc 1, prec 0.091242, recall 0.806814
2017-12-10T03:52:42.469782: step 5750, loss 0.353295, acc 0.921875, prec 0.0912357, recall 0.806814
2017-12-10T03:52:42.737494: step 5751, loss 0.0021669, acc 1, prec 0.0912482, recall 0.806837
2017-12-10T03:52:43.002712: step 5752, loss 0.144969, acc 0.984375, prec 0.0912593, recall 0.806861
2017-12-10T03:52:43.277482: step 5753, loss 0.247251, acc 0.953125, prec 0.0912556, recall 0.806861
2017-12-10T03:52:43.542757: step 5754, loss 0.074879, acc 0.96875, prec 0.0912655, recall 0.806884
2017-12-10T03:52:43.811792: step 5755, loss 0.15757, acc 0.96875, prec 0.091263, recall 0.806884
2017-12-10T03:52:44.077516: step 5756, loss 0.013925, acc 0.984375, prec 0.0912742, recall 0.806907
2017-12-10T03:52:44.345632: step 5757, loss 0.188019, acc 0.984375, prec 0.0912853, recall 0.806931
2017-12-10T03:52:44.619254: step 5758, loss 0.0453369, acc 0.984375, prec 0.0912841, recall 0.806931
2017-12-10T03:52:44.889722: step 5759, loss 0.0116064, acc 1, prec 0.0912965, recall 0.806954
2017-12-10T03:52:45.155717: step 5760, loss 0.0323597, acc 1, prec 0.0913089, recall 0.806977
2017-12-10T03:52:45.418618: step 5761, loss 0.129806, acc 0.984375, prec 0.0913077, recall 0.806977
2017-12-10T03:52:45.690280: step 5762, loss 0.00138211, acc 1, prec 0.0913077, recall 0.806977
2017-12-10T03:52:45.954764: step 5763, loss 0.150901, acc 0.96875, prec 0.0913052, recall 0.806977
2017-12-10T03:52:46.224194: step 5764, loss 0.0762443, acc 0.984375, prec 0.0913163, recall 0.807001
2017-12-10T03:52:46.505618: step 5765, loss 0.335288, acc 0.96875, prec 0.0913263, recall 0.807024
2017-12-10T03:52:46.775359: step 5766, loss 0.0496677, acc 1, prec 0.0913387, recall 0.807047
2017-12-10T03:52:47.045660: step 5767, loss 0.010705, acc 1, prec 0.0913511, recall 0.80707
2017-12-10T03:52:47.317417: step 5768, loss 0.228034, acc 1, prec 0.0913635, recall 0.807094
2017-12-10T03:52:47.587661: step 5769, loss 0.135963, acc 0.984375, prec 0.0913747, recall 0.807117
2017-12-10T03:52:47.855514: step 5770, loss 0.0815903, acc 0.984375, prec 0.0913734, recall 0.807117
2017-12-10T03:52:48.117646: step 5771, loss 0.00375464, acc 1, prec 0.0913982, recall 0.807164
2017-12-10T03:52:48.388430: step 5772, loss 0.00982118, acc 1, prec 0.0914106, recall 0.807187
2017-12-10T03:52:48.654456: step 5773, loss 0.0869168, acc 0.984375, prec 0.0914094, recall 0.807187
2017-12-10T03:52:48.923497: step 5774, loss 0.0158866, acc 0.984375, prec 0.0914205, recall 0.80721
2017-12-10T03:52:49.187149: step 5775, loss 0.0697432, acc 0.984375, prec 0.0914193, recall 0.80721
2017-12-10T03:52:49.454233: step 5776, loss 0.35886, acc 0.984375, prec 0.0914429, recall 0.807257
2017-12-10T03:52:49.729096: step 5777, loss 0.0589567, acc 0.96875, prec 0.0914403, recall 0.807257
2017-12-10T03:52:49.990280: step 5778, loss 0.00539406, acc 1, prec 0.0914403, recall 0.807257
2017-12-10T03:52:50.255274: step 5779, loss 0.0057089, acc 1, prec 0.0914528, recall 0.80728
2017-12-10T03:52:50.527007: step 5780, loss 0.107028, acc 0.96875, prec 0.0914503, recall 0.80728
2017-12-10T03:52:50.792090: step 5781, loss 0.0365055, acc 1, prec 0.0914627, recall 0.807303
2017-12-10T03:52:51.059336: step 5782, loss 0.0044471, acc 1, prec 0.0914627, recall 0.807303
2017-12-10T03:52:51.334803: step 5783, loss 0.0225204, acc 1, prec 0.0914751, recall 0.807326
2017-12-10T03:52:51.601427: step 5784, loss 0.10724, acc 0.984375, prec 0.0914862, recall 0.807349
2017-12-10T03:52:51.866038: step 5785, loss 0.0173784, acc 1, prec 0.091511, recall 0.807396
2017-12-10T03:52:52.136390: step 5786, loss 0.112531, acc 0.9375, prec 0.091506, recall 0.807396
2017-12-10T03:52:52.402521: step 5787, loss 0.221745, acc 0.9375, prec 0.091501, recall 0.807396
2017-12-10T03:52:52.669796: step 5788, loss 0.387394, acc 0.953125, prec 0.0914973, recall 0.807396
2017-12-10T03:52:52.945310: step 5789, loss 0.109711, acc 0.96875, prec 0.0914948, recall 0.807396
2017-12-10T03:52:53.215925: step 5790, loss 0.0620646, acc 0.984375, prec 0.0915059, recall 0.807419
2017-12-10T03:52:53.482286: step 5791, loss 0.0839287, acc 0.96875, prec 0.0915158, recall 0.807442
2017-12-10T03:52:53.747752: step 5792, loss 0.255327, acc 0.96875, prec 0.0915505, recall 0.807512
2017-12-10T03:52:54.019232: step 5793, loss 0.122347, acc 0.984375, prec 0.0915493, recall 0.807512
2017-12-10T03:52:54.287218: step 5794, loss 0.000403061, acc 1, prec 0.0915493, recall 0.807512
2017-12-10T03:52:54.555784: step 5795, loss 0.0976025, acc 1, prec 0.0915865, recall 0.807581
2017-12-10T03:52:54.831744: step 5796, loss 0.0107709, acc 1, prec 0.0915989, recall 0.807604
2017-12-10T03:52:55.109542: step 5797, loss 0.295334, acc 0.984375, prec 0.0916224, recall 0.807651
2017-12-10T03:52:55.382087: step 5798, loss 0.0931998, acc 0.984375, prec 0.0916212, recall 0.807651
2017-12-10T03:52:55.655415: step 5799, loss 0.0848527, acc 0.96875, prec 0.0916311, recall 0.807674
2017-12-10T03:52:55.922537: step 5800, loss 0.00578503, acc 1, prec 0.0916311, recall 0.807674
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-5800

2017-12-10T03:52:57.163075: step 5801, loss 0.441712, acc 0.9375, prec 0.0916385, recall 0.807697
2017-12-10T03:52:57.432560: step 5802, loss 0.0454658, acc 0.984375, prec 0.0916372, recall 0.807697
2017-12-10T03:52:57.702878: step 5803, loss 0.0511714, acc 0.984375, prec 0.091636, recall 0.807697
2017-12-10T03:52:57.968457: step 5804, loss 0.334829, acc 0.984375, prec 0.0916719, recall 0.807766
2017-12-10T03:52:58.237658: step 5805, loss 0.0293987, acc 0.984375, prec 0.091683, recall 0.807789
2017-12-10T03:52:58.501292: step 5806, loss 0.866741, acc 0.984375, prec 0.0916942, recall 0.807813
2017-12-10T03:52:58.766199: step 5807, loss 0.0111943, acc 1, prec 0.0917066, recall 0.807836
2017-12-10T03:52:59.032269: step 5808, loss 0.252868, acc 0.953125, prec 0.0917152, recall 0.807859
2017-12-10T03:52:59.298317: step 5809, loss 0.140127, acc 0.96875, prec 0.0917127, recall 0.807859
2017-12-10T03:52:59.565059: step 5810, loss 0.111916, acc 0.984375, prec 0.0917238, recall 0.807882
2017-12-10T03:52:59.841515: step 5811, loss 0.0630327, acc 0.96875, prec 0.0917213, recall 0.807882
2017-12-10T03:53:00.111301: step 5812, loss 0.244314, acc 0.921875, prec 0.0917151, recall 0.807882
2017-12-10T03:53:00.384689: step 5813, loss 0.0391442, acc 0.96875, prec 0.0917374, recall 0.807928
2017-12-10T03:53:00.663912: step 5814, loss 0.201379, acc 0.9375, prec 0.0917324, recall 0.807928
2017-12-10T03:53:00.933881: step 5815, loss 0.461879, acc 0.90625, prec 0.0917372, recall 0.807951
2017-12-10T03:53:01.198249: step 5816, loss 0.22119, acc 0.921875, prec 0.0917434, recall 0.807974
2017-12-10T03:53:01.462573: step 5817, loss 0.147611, acc 0.984375, prec 0.0917793, recall 0.808043
2017-12-10T03:53:01.725920: step 5818, loss 0.0916479, acc 0.984375, prec 0.091778, recall 0.808043
2017-12-10T03:53:01.997673: step 5819, loss 0.651609, acc 0.921875, prec 0.0917965, recall 0.808089
2017-12-10T03:53:02.262506: step 5820, loss 0.674848, acc 0.921875, prec 0.0917903, recall 0.808089
2017-12-10T03:53:02.531856: step 5821, loss 0.36328, acc 0.921875, prec 0.091784, recall 0.808089
2017-12-10T03:53:02.798479: step 5822, loss 0.0791548, acc 0.96875, prec 0.0917939, recall 0.808112
2017-12-10T03:53:03.062262: step 5823, loss 0.067437, acc 0.953125, prec 0.0918025, recall 0.808135
2017-12-10T03:53:03.331130: step 5824, loss 0.0111229, acc 1, prec 0.0918149, recall 0.808158
2017-12-10T03:53:03.596399: step 5825, loss 0.0373861, acc 0.984375, prec 0.0918136, recall 0.808158
2017-12-10T03:53:03.861354: step 5826, loss 0.0419405, acc 0.984375, prec 0.0918248, recall 0.808181
2017-12-10T03:53:04.131670: step 5827, loss 0.211099, acc 0.9375, prec 0.0918445, recall 0.808227
2017-12-10T03:53:04.405336: step 5828, loss 0.5168, acc 0.9375, prec 0.0918395, recall 0.808227
2017-12-10T03:53:04.673573: step 5829, loss 0.0890829, acc 0.984375, prec 0.0918383, recall 0.808227
2017-12-10T03:53:04.941789: step 5830, loss 0.210185, acc 0.921875, prec 0.0918444, recall 0.80825
2017-12-10T03:53:05.208699: step 5831, loss 0.0610156, acc 0.96875, prec 0.0918419, recall 0.80825
2017-12-10T03:53:05.473689: step 5832, loss 0.305203, acc 0.9375, prec 0.0918492, recall 0.808273
2017-12-10T03:53:05.741051: step 5833, loss 0.0726958, acc 0.984375, prec 0.0918604, recall 0.808296
2017-12-10T03:53:06.015696: step 5834, loss 0.27664, acc 0.953125, prec 0.091869, recall 0.808319
2017-12-10T03:53:06.291794: step 5835, loss 0.0488156, acc 0.96875, prec 0.0918665, recall 0.808319
2017-12-10T03:53:06.559355: step 5836, loss 0.00295105, acc 1, prec 0.0918789, recall 0.808342
2017-12-10T03:53:06.827847: step 5837, loss 0.130618, acc 0.984375, prec 0.0919023, recall 0.808388
2017-12-10T03:53:07.093224: step 5838, loss 0.105518, acc 0.984375, prec 0.0919011, recall 0.808388
2017-12-10T03:53:07.369577: step 5839, loss 0.459123, acc 0.984375, prec 0.0919246, recall 0.808434
2017-12-10T03:53:07.634889: step 5840, loss 0.422373, acc 0.96875, prec 0.0919344, recall 0.808457
2017-12-10T03:53:07.906671: step 5841, loss 0.0268638, acc 0.984375, prec 0.0919332, recall 0.808457
2017-12-10T03:53:08.171396: step 5842, loss 0.00926181, acc 1, prec 0.0919456, recall 0.80848
2017-12-10T03:53:08.443590: step 5843, loss 0.205157, acc 0.921875, prec 0.0919393, recall 0.80848
2017-12-10T03:53:08.715840: step 5844, loss 0.233153, acc 0.984375, prec 0.0919381, recall 0.80848
2017-12-10T03:53:08.986775: step 5845, loss 0.000168781, acc 1, prec 0.0919381, recall 0.80848
2017-12-10T03:53:09.250776: step 5846, loss 0.00163786, acc 1, prec 0.0919504, recall 0.808503
2017-12-10T03:53:09.523791: step 5847, loss 0.0241001, acc 1, prec 0.0919628, recall 0.808526
2017-12-10T03:53:09.792351: step 5848, loss 0.00279665, acc 1, prec 0.0919875, recall 0.808572
2017-12-10T03:53:10.057200: step 5849, loss 2.11796, acc 0.984375, prec 0.0919999, recall 0.808498
2017-12-10T03:53:10.329455: step 5850, loss 0.0302605, acc 0.984375, prec 0.0920234, recall 0.808544
2017-12-10T03:53:10.603833: step 5851, loss 0.0547193, acc 0.984375, prec 0.0920468, recall 0.80859
2017-12-10T03:53:10.871347: step 5852, loss 0.167065, acc 0.9375, prec 0.0920542, recall 0.808612
2017-12-10T03:53:11.138086: step 5853, loss 0.202432, acc 0.984375, prec 0.0920653, recall 0.808635
2017-12-10T03:53:11.412229: step 5854, loss 0.455906, acc 0.96875, prec 0.0920752, recall 0.808658
2017-12-10T03:53:11.689121: step 5855, loss 0.406228, acc 0.9375, prec 0.0920825, recall 0.808681
2017-12-10T03:53:11.969689: step 5856, loss 0.492524, acc 0.9375, prec 0.0921146, recall 0.80875
2017-12-10T03:53:12.227763: step 5857, loss 0.395735, acc 0.9375, prec 0.0921096, recall 0.80875
2017-12-10T03:53:12.498887: step 5858, loss 0.345254, acc 0.921875, prec 0.0921156, recall 0.808773
2017-12-10T03:53:12.769555: step 5859, loss 0.124724, acc 0.96875, prec 0.0921379, recall 0.808818
2017-12-10T03:53:13.041678: step 5860, loss 0.580599, acc 0.921875, prec 0.0921439, recall 0.808841
2017-12-10T03:53:13.314087: step 5861, loss 0.30685, acc 0.9375, prec 0.0921389, recall 0.808841
2017-12-10T03:53:13.578669: step 5862, loss 0.0984252, acc 0.96875, prec 0.0921488, recall 0.808864
2017-12-10T03:53:13.851756: step 5863, loss 0.0669131, acc 0.96875, prec 0.0921463, recall 0.808864
2017-12-10T03:53:14.116353: step 5864, loss 0.195782, acc 0.9375, prec 0.092166, recall 0.80891
2017-12-10T03:53:14.386603: step 5865, loss 0.58987, acc 0.90625, prec 0.0921708, recall 0.808932
2017-12-10T03:53:14.654559: step 5866, loss 0.112111, acc 0.953125, prec 0.092167, recall 0.808932
2017-12-10T03:53:14.919893: step 5867, loss 0.462874, acc 0.9375, prec 0.0921744, recall 0.808955
2017-12-10T03:53:15.185141: step 5868, loss 0.173954, acc 0.96875, prec 0.0921842, recall 0.808978
2017-12-10T03:53:15.461411: step 5869, loss 0.33182, acc 0.90625, prec 0.092189, recall 0.809001
2017-12-10T03:53:15.728873: step 5870, loss 0.240385, acc 0.953125, prec 0.0921976, recall 0.809024
2017-12-10T03:53:15.995847: step 5871, loss 0.403176, acc 0.921875, prec 0.0921913, recall 0.809024
2017-12-10T03:53:16.262147: step 5872, loss 0.35485, acc 0.953125, prec 0.0921876, recall 0.809024
2017-12-10T03:53:16.525811: step 5873, loss 0.431279, acc 0.9375, prec 0.0921949, recall 0.809046
2017-12-10T03:53:16.798281: step 5874, loss 0.038525, acc 0.984375, prec 0.092206, recall 0.809069
2017-12-10T03:53:17.065445: step 5875, loss 0.237684, acc 0.96875, prec 0.0922282, recall 0.809115
2017-12-10T03:53:17.329259: step 5876, loss 0.0529517, acc 0.96875, prec 0.0922504, recall 0.80916
2017-12-10T03:53:17.593033: step 5877, loss 0.0175048, acc 1, prec 0.0922504, recall 0.80916
2017-12-10T03:53:17.864380: step 5878, loss 0.388203, acc 0.953125, prec 0.0922713, recall 0.809206
2017-12-10T03:53:18.129486: step 5879, loss 0.491917, acc 0.921875, prec 0.092265, recall 0.809206
2017-12-10T03:53:18.393379: step 5880, loss 0.000425421, acc 1, prec 0.0922774, recall 0.809229
2017-12-10T03:53:18.649932: step 5881, loss 0.0529619, acc 0.984375, prec 0.0922761, recall 0.809229
2017-12-10T03:53:18.920682: step 5882, loss 0.0903637, acc 0.953125, prec 0.0922847, recall 0.809251
2017-12-10T03:53:19.185270: step 5883, loss 1.16112, acc 0.953125, prec 0.0922933, recall 0.809274
2017-12-10T03:53:19.461007: step 5884, loss 0.0475174, acc 0.984375, prec 0.0923167, recall 0.809319
2017-12-10T03:53:19.732535: step 5885, loss 0.00677562, acc 1, prec 0.0923167, recall 0.809319
2017-12-10T03:53:20.005572: step 5886, loss 0.443859, acc 0.96875, prec 0.0923142, recall 0.809319
2017-12-10T03:53:20.273545: step 5887, loss 0.408685, acc 0.96875, prec 0.092324, recall 0.809342
2017-12-10T03:53:20.540451: step 5888, loss 0.526463, acc 0.96875, prec 0.0923215, recall 0.809342
2017-12-10T03:53:20.804822: step 5889, loss 0.193732, acc 0.953125, prec 0.0923177, recall 0.809342
2017-12-10T03:53:21.078667: step 5890, loss 0.133174, acc 0.96875, prec 0.0923399, recall 0.809388
2017-12-10T03:53:21.343709: step 5891, loss 0.121927, acc 0.9375, prec 0.0923595, recall 0.809433
2017-12-10T03:53:21.608938: step 5892, loss 0.0225878, acc 0.984375, prec 0.0923583, recall 0.809433
2017-12-10T03:53:21.874920: step 5893, loss 0.156568, acc 0.984375, prec 0.0923694, recall 0.809456
2017-12-10T03:53:22.152790: step 5894, loss 0.126252, acc 0.953125, prec 0.0923779, recall 0.809478
2017-12-10T03:53:22.414574: step 5895, loss 2.47084, acc 0.96875, prec 0.0923767, recall 0.809382
2017-12-10T03:53:22.683106: step 5896, loss 0.00855731, acc 1, prec 0.0923767, recall 0.809382
2017-12-10T03:53:22.949459: step 5897, loss 0.669784, acc 0.9375, prec 0.092384, recall 0.809405
2017-12-10T03:53:23.217546: step 5898, loss 0.296238, acc 0.96875, prec 0.0924061, recall 0.80945
2017-12-10T03:53:23.494377: step 5899, loss 0.158334, acc 0.953125, prec 0.092427, recall 0.809495
2017-12-10T03:53:23.766373: step 5900, loss 0.289176, acc 0.953125, prec 0.0924603, recall 0.809563
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-5900

2017-12-10T03:53:25.179811: step 5901, loss 0.485229, acc 0.890625, prec 0.0924638, recall 0.809586
2017-12-10T03:53:25.446674: step 5902, loss 0.564807, acc 0.9375, prec 0.0924588, recall 0.809586
2017-12-10T03:53:25.712810: step 5903, loss 0.346914, acc 0.921875, prec 0.0924648, recall 0.809609
2017-12-10T03:53:25.974513: step 5904, loss 0.397528, acc 0.953125, prec 0.0924734, recall 0.809631
2017-12-10T03:53:26.242821: step 5905, loss 0.0843289, acc 0.953125, prec 0.0924819, recall 0.809654
2017-12-10T03:53:26.504541: step 5906, loss 0.693485, acc 0.875, prec 0.0925089, recall 0.809722
2017-12-10T03:53:26.774801: step 5907, loss 0.553228, acc 0.890625, prec 0.0925124, recall 0.809744
2017-12-10T03:53:27.040705: step 5908, loss 0.531128, acc 0.875, prec 0.0925147, recall 0.809767
2017-12-10T03:53:27.309135: step 5909, loss 0.183954, acc 0.90625, prec 0.0925318, recall 0.809812
2017-12-10T03:53:27.579067: step 5910, loss 0.493715, acc 0.953125, prec 0.092528, recall 0.809812
2017-12-10T03:53:27.844460: step 5911, loss 0.157541, acc 0.953125, prec 0.0925489, recall 0.809857
2017-12-10T03:53:28.112091: step 5912, loss 0.424696, acc 0.890625, prec 0.0925524, recall 0.80988
2017-12-10T03:53:28.373983: step 5913, loss 0.670616, acc 0.859375, prec 0.0925411, recall 0.80988
2017-12-10T03:53:28.644390: step 5914, loss 0.00547908, acc 1, prec 0.0925534, recall 0.809903
2017-12-10T03:53:28.916464: step 5915, loss 0.0402785, acc 0.96875, prec 0.0925509, recall 0.809903
2017-12-10T03:53:29.182715: step 5916, loss 0.564785, acc 0.953125, prec 0.0925841, recall 0.80997
2017-12-10T03:53:29.451309: step 5917, loss 0.372201, acc 0.9375, prec 0.0926036, recall 0.810015
2017-12-10T03:53:29.716503: step 5918, loss 4.69738, acc 0.953125, prec 0.0926134, recall 0.809942
2017-12-10T03:53:29.988451: step 5919, loss 0.241609, acc 0.921875, prec 0.0926318, recall 0.809987
2017-12-10T03:53:30.264256: step 5920, loss 0.698677, acc 0.875, prec 0.092634, recall 0.810009
2017-12-10T03:53:30.540554: step 5921, loss 0.463523, acc 0.953125, prec 0.0926303, recall 0.810009
2017-12-10T03:53:30.811002: step 5922, loss 0.118336, acc 0.96875, prec 0.0926278, recall 0.810009
2017-12-10T03:53:31.079666: step 5923, loss 0.0729076, acc 0.984375, prec 0.0926388, recall 0.810032
2017-12-10T03:53:31.345829: step 5924, loss 0.745227, acc 0.859375, prec 0.0926275, recall 0.810032
2017-12-10T03:53:31.608216: step 5925, loss 0.197468, acc 0.953125, prec 0.0926237, recall 0.810032
2017-12-10T03:53:31.879819: step 5926, loss 0.136878, acc 0.9375, prec 0.092631, recall 0.810055
2017-12-10T03:53:32.153977: step 5927, loss 0.526366, acc 0.96875, prec 0.0926408, recall 0.810077
2017-12-10T03:53:32.422349: step 5928, loss 0.0789749, acc 0.96875, prec 0.0926383, recall 0.810077
2017-12-10T03:53:32.684274: step 5929, loss 0.0612184, acc 0.96875, prec 0.0926358, recall 0.810077
2017-12-10T03:53:32.950419: step 5930, loss 1.08692, acc 0.828125, prec 0.0926343, recall 0.8101
2017-12-10T03:53:33.216479: step 5931, loss 0.139567, acc 0.96875, prec 0.0926441, recall 0.810122
2017-12-10T03:53:33.487501: step 5932, loss 0.305333, acc 0.90625, prec 0.0926611, recall 0.810167
2017-12-10T03:53:33.768679: step 5933, loss 0.218017, acc 0.921875, prec 0.0926671, recall 0.81019
2017-12-10T03:53:34.035029: step 5934, loss 0.410906, acc 0.890625, prec 0.0926583, recall 0.81019
2017-12-10T03:53:34.302934: step 5935, loss 0.220644, acc 0.953125, prec 0.0926792, recall 0.810235
2017-12-10T03:53:34.566033: step 5936, loss 0.322875, acc 0.953125, prec 0.0926754, recall 0.810235
2017-12-10T03:53:34.832105: step 5937, loss 0.17197, acc 0.953125, prec 0.0926839, recall 0.810257
2017-12-10T03:53:35.102138: step 5938, loss 0.11593, acc 0.953125, prec 0.0926924, recall 0.810279
2017-12-10T03:53:35.370832: step 5939, loss 0.105532, acc 0.96875, prec 0.0927145, recall 0.810324
2017-12-10T03:53:35.641138: step 5940, loss 0.275559, acc 0.953125, prec 0.0927353, recall 0.810369
2017-12-10T03:53:35.910475: step 5941, loss 0.119448, acc 0.953125, prec 0.0927561, recall 0.810414
2017-12-10T03:53:36.185335: step 5942, loss 0.0841009, acc 0.953125, prec 0.0927647, recall 0.810437
2017-12-10T03:53:36.447876: step 5943, loss 0.104119, acc 0.96875, prec 0.0927744, recall 0.810459
2017-12-10T03:53:36.720840: step 5944, loss 0.028167, acc 0.984375, prec 0.0927855, recall 0.810481
2017-12-10T03:53:36.986145: step 5945, loss 0.268557, acc 0.96875, prec 0.0928075, recall 0.810526
2017-12-10T03:53:37.251082: step 5946, loss 0.0939018, acc 0.96875, prec 0.0928173, recall 0.810549
2017-12-10T03:53:37.514138: step 5947, loss 0.0875267, acc 0.96875, prec 0.0928148, recall 0.810549
2017-12-10T03:53:37.777128: step 5948, loss 0.590906, acc 0.9375, prec 0.092822, recall 0.810571
2017-12-10T03:53:38.050122: step 5949, loss 0.109293, acc 0.984375, prec 0.0928331, recall 0.810594
2017-12-10T03:53:38.320421: step 5950, loss 0.0638165, acc 0.96875, prec 0.0928305, recall 0.810594
2017-12-10T03:53:38.583279: step 5951, loss 0.111559, acc 0.96875, prec 0.0928403, recall 0.810616
2017-12-10T03:53:38.851403: step 5952, loss 0.166238, acc 0.96875, prec 0.0928378, recall 0.810616
2017-12-10T03:53:39.116703: step 5953, loss 3.7524, acc 0.96875, prec 0.0928365, recall 0.81052
2017-12-10T03:53:39.387572: step 5954, loss 0.17439, acc 0.953125, prec 0.0928451, recall 0.810542
2017-12-10T03:53:39.652219: step 5955, loss 0.0943022, acc 0.96875, prec 0.0928548, recall 0.810565
2017-12-10T03:53:39.922273: step 5956, loss 0.232339, acc 0.921875, prec 0.0928608, recall 0.810587
2017-12-10T03:53:40.182820: step 5957, loss 0.641852, acc 0.875, prec 0.0928508, recall 0.810587
2017-12-10T03:53:40.457460: step 5958, loss 0.700308, acc 0.890625, prec 0.0928665, recall 0.810632
2017-12-10T03:53:40.722631: step 5959, loss 0.122198, acc 0.9375, prec 0.0928615, recall 0.810632
2017-12-10T03:53:40.991460: step 5960, loss 0.459569, acc 0.890625, prec 0.0929141, recall 0.810744
2017-12-10T03:53:41.259065: step 5961, loss 0.0782724, acc 0.9375, prec 0.0929213, recall 0.810766
2017-12-10T03:53:41.532686: step 5962, loss 0.417651, acc 0.890625, prec 0.0929248, recall 0.810788
2017-12-10T03:53:41.812982: step 5963, loss 0.139705, acc 0.9375, prec 0.092932, recall 0.810811
2017-12-10T03:53:42.044549: step 5964, loss 0.211091, acc 0.941176, prec 0.0929283, recall 0.810811
2017-12-10T03:53:42.317035: step 5965, loss 0.241397, acc 0.921875, prec 0.0929342, recall 0.810833
2017-12-10T03:53:42.589761: step 5966, loss 0.330897, acc 0.9375, prec 0.0929292, recall 0.810833
2017-12-10T03:53:42.857668: step 5967, loss 1.38601, acc 0.8125, prec 0.0929141, recall 0.810833
2017-12-10T03:53:43.131143: step 5968, loss 0.826757, acc 0.921875, prec 0.0929201, recall 0.810855
2017-12-10T03:53:43.394245: step 5969, loss 0.195378, acc 0.921875, prec 0.0929138, recall 0.810855
2017-12-10T03:53:43.660469: step 5970, loss 0.126295, acc 0.9375, prec 0.0929211, recall 0.810878
2017-12-10T03:53:43.926408: step 5971, loss 0.187349, acc 0.90625, prec 0.0929258, recall 0.8109
2017-12-10T03:53:44.191313: step 5972, loss 0.451331, acc 0.90625, prec 0.0929183, recall 0.8109
2017-12-10T03:53:44.456088: step 5973, loss 0.430146, acc 0.96875, prec 0.092928, recall 0.810922
2017-12-10T03:53:44.722326: step 5974, loss 0.041928, acc 0.984375, prec 0.0929268, recall 0.810922
2017-12-10T03:53:44.988249: step 5975, loss 0.090408, acc 0.953125, prec 0.092923, recall 0.810922
2017-12-10T03:53:45.252048: step 5976, loss 0.0953711, acc 0.96875, prec 0.0929205, recall 0.810922
2017-12-10T03:53:45.515975: step 5977, loss 0.123776, acc 0.984375, prec 0.0929192, recall 0.810922
2017-12-10T03:53:45.784754: step 5978, loss 0.168973, acc 0.953125, prec 0.0929155, recall 0.810922
2017-12-10T03:53:46.050953: step 5979, loss 0.00849533, acc 1, prec 0.0929155, recall 0.810922
2017-12-10T03:53:46.328055: step 5980, loss 0.0754301, acc 0.96875, prec 0.0929129, recall 0.810922
2017-12-10T03:53:46.594335: step 5981, loss 0.183022, acc 0.96875, prec 0.0929104, recall 0.810922
2017-12-10T03:53:46.861032: step 5982, loss 0.0669701, acc 1, prec 0.0929227, recall 0.810945
2017-12-10T03:53:47.129972: step 5983, loss 0.00720031, acc 1, prec 0.0929227, recall 0.810945
2017-12-10T03:53:47.396230: step 5984, loss 0.00256337, acc 1, prec 0.0929227, recall 0.810945
2017-12-10T03:53:47.674907: step 5985, loss 0.00183078, acc 1, prec 0.0929227, recall 0.810945
2017-12-10T03:53:47.940308: step 5986, loss 0.0845979, acc 1, prec 0.0929472, recall 0.810989
2017-12-10T03:53:48.214550: step 5987, loss 0.0314416, acc 0.984375, prec 0.0929582, recall 0.811012
2017-12-10T03:53:48.481375: step 5988, loss 0.0582163, acc 1, prec 0.0929705, recall 0.811034
2017-12-10T03:53:48.749325: step 5989, loss 4.83215, acc 0.96875, prec 0.0929692, recall 0.810938
2017-12-10T03:53:49.024097: step 5990, loss 0.00523611, acc 1, prec 0.0929937, recall 0.810983
2017-12-10T03:53:49.292391: step 5991, loss 0.226855, acc 0.953125, prec 0.0930022, recall 0.811005
2017-12-10T03:53:49.558926: step 5992, loss 0.0433739, acc 0.984375, prec 0.0930132, recall 0.811027
2017-12-10T03:53:49.830641: step 5993, loss 0.023144, acc 1, prec 0.0930255, recall 0.81105
2017-12-10T03:53:50.097789: step 5994, loss 0.0224303, acc 0.984375, prec 0.0930365, recall 0.811072
2017-12-10T03:53:50.362679: step 5995, loss 0.172494, acc 0.9375, prec 0.0930559, recall 0.811116
2017-12-10T03:53:50.630417: step 5996, loss 0.151262, acc 0.96875, prec 0.0930657, recall 0.811139
2017-12-10T03:53:50.896362: step 5997, loss 0.0365649, acc 0.96875, prec 0.0930754, recall 0.811161
2017-12-10T03:53:51.167279: step 5998, loss 0.269429, acc 0.96875, prec 0.0930851, recall 0.811183
2017-12-10T03:53:51.431433: step 5999, loss 0.544902, acc 0.875, prec 0.0930751, recall 0.811183
2017-12-10T03:53:51.692239: step 6000, loss 0.12201, acc 0.96875, prec 0.0930726, recall 0.811183

Evaluation:
2017-12-10T03:53:59.290080: step 6000, loss 3.68624, acc 0.924521, prec 0.0932441, recall 0.807425

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-6000

2017-12-10T03:54:00.714370: step 6001, loss 0.159319, acc 0.96875, prec 0.0932658, recall 0.80747
2017-12-10T03:54:00.982801: step 6002, loss 0.5069, acc 0.875, prec 0.0932801, recall 0.807514
2017-12-10T03:54:01.252081: step 6003, loss 0.323803, acc 0.90625, prec 0.0932847, recall 0.807537
2017-12-10T03:54:01.519965: step 6004, loss 0.141793, acc 0.9375, prec 0.0932797, recall 0.807537
2017-12-10T03:54:01.789557: step 6005, loss 0.350668, acc 0.921875, prec 0.0932856, recall 0.807559
2017-12-10T03:54:02.053761: step 6006, loss 0.303046, acc 0.921875, prec 0.0932794, recall 0.807559
2017-12-10T03:54:02.320793: step 6007, loss 0.109545, acc 0.9375, prec 0.0932744, recall 0.807559
2017-12-10T03:54:02.590891: step 6008, loss 0.204113, acc 0.921875, prec 0.0932924, recall 0.807603
2017-12-10T03:54:02.864017: step 6009, loss 0.458733, acc 0.953125, prec 0.0933008, recall 0.807626
2017-12-10T03:54:03.135043: step 6010, loss 0.433544, acc 0.921875, prec 0.0932945, recall 0.807626
2017-12-10T03:54:03.407171: step 6011, loss 0.207341, acc 0.953125, prec 0.0933271, recall 0.807692
2017-12-10T03:54:03.671893: step 6012, loss 0.209833, acc 0.90625, prec 0.0933438, recall 0.807737
2017-12-10T03:54:03.936757: step 6013, loss 0.828543, acc 0.90625, prec 0.0933606, recall 0.807781
2017-12-10T03:54:04.197211: step 6014, loss 0.424191, acc 0.96875, prec 0.0933702, recall 0.807803
2017-12-10T03:54:04.462875: step 6015, loss 0.0528824, acc 0.984375, prec 0.0933689, recall 0.807803
2017-12-10T03:54:04.735768: step 6016, loss 0.272248, acc 0.96875, prec 0.0933664, recall 0.807803
2017-12-10T03:54:05.003372: step 6017, loss 1.68697, acc 0.96875, prec 0.0933773, recall 0.807732
2017-12-10T03:54:05.271257: step 6018, loss 0.018432, acc 1, prec 0.0933773, recall 0.807732
2017-12-10T03:54:05.542846: step 6019, loss 0.0166449, acc 0.984375, prec 0.0933881, recall 0.807754
2017-12-10T03:54:05.810041: step 6020, loss 0.0311861, acc 0.984375, prec 0.093399, recall 0.807777
2017-12-10T03:54:06.078486: step 6021, loss 0.0221119, acc 1, prec 0.0934232, recall 0.807821
2017-12-10T03:54:06.339534: step 6022, loss 0.0321088, acc 1, prec 0.0934473, recall 0.807865
2017-12-10T03:54:06.608050: step 6023, loss 0.154455, acc 0.953125, prec 0.0934436, recall 0.807865
2017-12-10T03:54:06.878146: step 6024, loss 0.038102, acc 0.984375, prec 0.0934545, recall 0.807887
2017-12-10T03:54:07.142296: step 6025, loss 0.188171, acc 0.9375, prec 0.0934616, recall 0.80791
2017-12-10T03:54:07.405119: step 6026, loss 0.500093, acc 0.953125, prec 0.0934941, recall 0.807976
2017-12-10T03:54:07.672463: step 6027, loss 0.0387105, acc 0.96875, prec 0.0935158, recall 0.80802
2017-12-10T03:54:07.942278: step 6028, loss 0.296681, acc 0.921875, prec 0.0935216, recall 0.808042
2017-12-10T03:54:08.209186: step 6029, loss 0.0291772, acc 0.984375, prec 0.0935204, recall 0.808042
2017-12-10T03:54:08.479573: step 6030, loss 0.139962, acc 0.984375, prec 0.0935312, recall 0.808065
2017-12-10T03:54:08.750112: step 6031, loss 0.273047, acc 0.9375, prec 0.0935383, recall 0.808087
2017-12-10T03:54:09.025415: step 6032, loss 0.262207, acc 0.953125, prec 0.0935588, recall 0.808131
2017-12-10T03:54:09.292916: step 6033, loss 0.0115316, acc 1, prec 0.0935588, recall 0.808131
2017-12-10T03:54:09.561332: step 6034, loss 0.0659636, acc 0.984375, prec 0.0935575, recall 0.808131
2017-12-10T03:54:09.829129: step 6035, loss 0.138884, acc 0.9375, prec 0.0935525, recall 0.808131
2017-12-10T03:54:10.098010: step 6036, loss 0.0682035, acc 0.984375, prec 0.0935634, recall 0.808153
2017-12-10T03:54:10.361232: step 6037, loss 0.153666, acc 0.9375, prec 0.0935704, recall 0.808175
2017-12-10T03:54:10.625512: step 6038, loss 0.0233043, acc 1, prec 0.0935946, recall 0.808219
2017-12-10T03:54:10.889955: step 6039, loss 0.213878, acc 0.9375, prec 0.0936017, recall 0.808241
2017-12-10T03:54:11.150986: step 6040, loss 0.113969, acc 0.96875, prec 0.0936234, recall 0.808285
2017-12-10T03:54:11.416830: step 6041, loss 0.119381, acc 0.953125, prec 0.0936559, recall 0.808352
2017-12-10T03:54:11.690097: step 6042, loss 0.0540477, acc 0.984375, prec 0.0936546, recall 0.808352
2017-12-10T03:54:11.968132: step 6043, loss 0.14013, acc 0.984375, prec 0.0936655, recall 0.808374
2017-12-10T03:54:12.239607: step 6044, loss 0.121863, acc 0.984375, prec 0.0936642, recall 0.808374
2017-12-10T03:54:12.505117: step 6045, loss 0.102593, acc 0.96875, prec 0.0936617, recall 0.808374
2017-12-10T03:54:12.775910: step 6046, loss 0.0677781, acc 0.984375, prec 0.0936846, recall 0.808418
2017-12-10T03:54:13.051116: step 6047, loss 0.000188887, acc 1, prec 0.0936846, recall 0.808418
2017-12-10T03:54:13.318536: step 6048, loss 0.0866301, acc 0.984375, prec 0.0936834, recall 0.808418
2017-12-10T03:54:13.585257: step 6049, loss 0.00542614, acc 1, prec 0.0936834, recall 0.808418
2017-12-10T03:54:13.852614: step 6050, loss 0.157023, acc 0.953125, prec 0.0936917, recall 0.80844
2017-12-10T03:54:14.120738: step 6051, loss 0.337092, acc 0.90625, prec 0.0936842, recall 0.80844
2017-12-10T03:54:14.388821: step 6052, loss 0.0273186, acc 0.984375, prec 0.0937071, recall 0.808484
2017-12-10T03:54:15.365671: step 6053, loss 0.237823, acc 0.984375, prec 0.09373, recall 0.808528
2017-12-10T03:54:15.724135: step 6054, loss 0.130487, acc 0.984375, prec 0.0937408, recall 0.80855
2017-12-10T03:54:15.985549: step 6055, loss 0.0273754, acc 1, prec 0.0937529, recall 0.808572
2017-12-10T03:54:16.620426: step 6056, loss 1.00455, acc 0.984375, prec 0.0937758, recall 0.808616
2017-12-10T03:54:17.346774: step 6057, loss 0.039338, acc 1, prec 0.0937879, recall 0.808638
2017-12-10T03:54:18.079769: step 6058, loss 0.361927, acc 0.953125, prec 0.0938083, recall 0.808682
2017-12-10T03:54:18.826970: step 6059, loss 0.0271315, acc 0.984375, prec 0.093807, recall 0.808682
2017-12-10T03:54:19.561905: step 6060, loss 0.0671925, acc 0.96875, prec 0.0938287, recall 0.808726
2017-12-10T03:54:20.293186: step 6061, loss 0.05142, acc 0.984375, prec 0.0938395, recall 0.808748
2017-12-10T03:54:21.014365: step 6062, loss 0.215927, acc 0.9375, prec 0.0938345, recall 0.808748
2017-12-10T03:54:21.763840: step 6063, loss 0.0619156, acc 0.96875, prec 0.0938441, recall 0.80877
2017-12-10T03:54:22.476463: step 6064, loss 0.143765, acc 0.953125, prec 0.0938524, recall 0.808791
2017-12-10T03:54:23.442970: step 6065, loss 0.0962923, acc 0.984375, prec 0.0938632, recall 0.808813
2017-12-10T03:54:23.803910: step 6066, loss 0.175066, acc 0.953125, prec 0.0938836, recall 0.808857
2017-12-10T03:54:24.152265: step 6067, loss 0.0893776, acc 0.984375, prec 0.0938823, recall 0.808857
2017-12-10T03:54:24.440826: step 6068, loss 0.351778, acc 0.9375, prec 0.0938894, recall 0.808879
2017-12-10T03:54:24.736996: step 6069, loss 0.185997, acc 0.953125, prec 0.0938977, recall 0.808901
2017-12-10T03:54:25.043747: step 6070, loss 0.10975, acc 0.96875, prec 0.0939073, recall 0.808923
2017-12-10T03:54:25.319910: step 6071, loss 0.213309, acc 0.96875, prec 0.0939168, recall 0.808945
2017-12-10T03:54:25.597528: step 6072, loss 1.41437, acc 0.96875, prec 0.0939277, recall 0.808874
2017-12-10T03:54:25.871636: step 6073, loss 0.0867924, acc 0.984375, prec 0.0939385, recall 0.808896
2017-12-10T03:54:26.146914: step 6074, loss 0.229034, acc 0.96875, prec 0.093948, recall 0.808918
2017-12-10T03:54:26.415825: step 6075, loss 0.112824, acc 0.96875, prec 0.0939576, recall 0.80894
2017-12-10T03:54:26.688483: step 6076, loss 0.174192, acc 0.921875, prec 0.0939513, recall 0.80894
2017-12-10T03:54:26.969253: step 6077, loss 0.0976197, acc 0.953125, prec 0.0939838, recall 0.809005
2017-12-10T03:54:27.236155: step 6078, loss 0.206577, acc 0.96875, prec 0.0940054, recall 0.809049
2017-12-10T03:54:27.504486: step 6079, loss 0.141502, acc 0.984375, prec 0.0940162, recall 0.809071
2017-12-10T03:54:27.770325: step 6080, loss 0.106118, acc 0.953125, prec 0.0940245, recall 0.809093
2017-12-10T03:54:28.036586: step 6081, loss 0.496529, acc 0.90625, prec 0.094017, recall 0.809093
2017-12-10T03:54:28.299021: step 6082, loss 0.278222, acc 0.90625, prec 0.0940336, recall 0.809137
2017-12-10T03:54:28.571380: step 6083, loss 0.432674, acc 0.9375, prec 0.0940527, recall 0.80918
2017-12-10T03:54:28.840103: step 6084, loss 0.240542, acc 0.921875, prec 0.0940464, recall 0.80918
2017-12-10T03:54:29.111963: step 6085, loss 0.182713, acc 0.953125, prec 0.0940668, recall 0.809224
2017-12-10T03:54:29.375018: step 6086, loss 0.567897, acc 0.875, prec 0.0940809, recall 0.809268
2017-12-10T03:54:29.642521: step 6087, loss 0.219117, acc 0.9375, prec 0.0940759, recall 0.809268
2017-12-10T03:54:29.910930: step 6088, loss 0.211897, acc 0.953125, prec 0.0940962, recall 0.809311
2017-12-10T03:54:30.191369: step 6089, loss 0.363461, acc 0.9375, prec 0.0941153, recall 0.809355
2017-12-10T03:54:30.464902: step 6090, loss 0.273946, acc 0.921875, prec 0.094109, recall 0.809355
2017-12-10T03:54:30.732528: step 6091, loss 0.189231, acc 0.96875, prec 0.0941186, recall 0.809377
2017-12-10T03:54:31.007503: step 6092, loss 0.539427, acc 0.96875, prec 0.0941522, recall 0.809442
2017-12-10T03:54:31.281317: step 6093, loss 0.315019, acc 0.953125, prec 0.0941605, recall 0.809464
2017-12-10T03:54:31.551073: step 6094, loss 0.0684746, acc 0.984375, prec 0.0941593, recall 0.809464
2017-12-10T03:54:31.820120: step 6095, loss 0.0606145, acc 0.96875, prec 0.0941567, recall 0.809464
2017-12-10T03:54:32.087292: step 6096, loss 0.25845, acc 0.9375, prec 0.0941638, recall 0.809486
2017-12-10T03:54:32.352168: step 6097, loss 0.407213, acc 0.921875, prec 0.0941575, recall 0.809486
2017-12-10T03:54:32.622400: step 6098, loss 0.0279289, acc 1, prec 0.0942057, recall 0.809573
2017-12-10T03:54:32.900724: step 6099, loss 0.367147, acc 0.953125, prec 0.0942019, recall 0.809573
2017-12-10T03:54:33.168702: step 6100, loss 0.0160318, acc 1, prec 0.0942381, recall 0.809638
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-6100

2017-12-10T03:54:34.395514: step 6101, loss 0.150504, acc 0.9375, prec 0.094233, recall 0.809638
2017-12-10T03:54:34.670491: step 6102, loss 1.33166, acc 0.96875, prec 0.0942438, recall 0.809567
2017-12-10T03:54:34.936569: step 6103, loss 0.580979, acc 0.9375, prec 0.0942509, recall 0.809589
2017-12-10T03:54:35.204201: step 6104, loss 0.292893, acc 0.90625, prec 0.0942674, recall 0.809632
2017-12-10T03:54:35.465871: step 6105, loss 0.273666, acc 0.984375, prec 0.0942782, recall 0.809654
2017-12-10T03:54:35.738314: step 6106, loss 0.00594468, acc 1, prec 0.0942902, recall 0.809676
2017-12-10T03:54:35.999436: step 6107, loss 0.0733763, acc 0.953125, prec 0.0942865, recall 0.809676
2017-12-10T03:54:36.266071: step 6108, loss 0.170134, acc 0.9375, prec 0.0942935, recall 0.809698
2017-12-10T03:54:36.535126: step 6109, loss 0.746453, acc 0.984375, prec 0.0943163, recall 0.809741
2017-12-10T03:54:36.807293: step 6110, loss 0.111896, acc 0.953125, prec 0.0943366, recall 0.809784
2017-12-10T03:54:37.073570: step 6111, loss 0.0560924, acc 0.96875, prec 0.0943461, recall 0.809806
2017-12-10T03:54:37.343665: step 6112, loss 0.0095195, acc 1, prec 0.0943582, recall 0.809828
2017-12-10T03:54:37.609468: step 6113, loss 0.196342, acc 0.9375, prec 0.0943772, recall 0.809871
2017-12-10T03:54:37.885012: step 6114, loss 0.0512508, acc 0.984375, prec 0.094376, recall 0.809871
2017-12-10T03:54:38.149133: step 6115, loss 0.367786, acc 0.9375, prec 0.094383, recall 0.809893
2017-12-10T03:54:38.412571: step 6116, loss 0.0379508, acc 0.984375, prec 0.0943817, recall 0.809893
2017-12-10T03:54:38.681856: step 6117, loss 0.403709, acc 0.875, prec 0.0943717, recall 0.809893
2017-12-10T03:54:38.954792: step 6118, loss 0.026391, acc 0.984375, prec 0.0943945, recall 0.809936
2017-12-10T03:54:39.218779: step 6119, loss 0.266895, acc 0.953125, prec 0.0943907, recall 0.809936
2017-12-10T03:54:39.492187: step 6120, loss 0.127921, acc 0.9375, prec 0.0943977, recall 0.809958
2017-12-10T03:54:39.757652: step 6121, loss 0.0893254, acc 0.96875, prec 0.0943952, recall 0.809958
2017-12-10T03:54:40.025722: step 6122, loss 0.152663, acc 0.953125, prec 0.0944035, recall 0.809979
2017-12-10T03:54:40.289205: step 6123, loss 0.312483, acc 0.921875, prec 0.0943972, recall 0.809979
2017-12-10T03:54:40.559686: step 6124, loss 0.0588247, acc 0.96875, prec 0.0943947, recall 0.809979
2017-12-10T03:54:40.822249: step 6125, loss 0.0271242, acc 0.984375, prec 0.0943935, recall 0.809979
2017-12-10T03:54:41.095248: step 6126, loss 0.11068, acc 0.953125, prec 0.0943897, recall 0.809979
2017-12-10T03:54:41.361906: step 6127, loss 0.0101969, acc 1, prec 0.0944017, recall 0.810001
2017-12-10T03:54:41.633011: step 6128, loss 0.0221569, acc 0.984375, prec 0.0944125, recall 0.810023
2017-12-10T03:54:41.903330: step 6129, loss 0.205168, acc 0.984375, prec 0.0944353, recall 0.810066
2017-12-10T03:54:42.173603: step 6130, loss 0.106905, acc 0.984375, prec 0.094434, recall 0.810066
2017-12-10T03:54:42.437830: step 6131, loss 0.212231, acc 0.96875, prec 0.0944315, recall 0.810066
2017-12-10T03:54:42.706174: step 6132, loss 0.171495, acc 0.984375, prec 0.0944303, recall 0.810066
2017-12-10T03:54:42.967662: step 6133, loss 0.0388099, acc 0.984375, prec 0.094429, recall 0.810066
2017-12-10T03:54:43.245835: step 6134, loss 0.359849, acc 0.984375, prec 0.0944398, recall 0.810088
2017-12-10T03:54:43.519220: step 6135, loss 0.757994, acc 0.984375, prec 0.0944506, recall 0.810109
2017-12-10T03:54:43.795745: step 6136, loss 0.000467807, acc 1, prec 0.0944626, recall 0.810131
2017-12-10T03:54:44.066286: step 6137, loss 0.00115508, acc 1, prec 0.0944866, recall 0.810174
2017-12-10T03:54:44.329263: step 6138, loss 0.258439, acc 0.953125, prec 0.0944949, recall 0.810196
2017-12-10T03:54:44.598186: step 6139, loss 0.0191236, acc 1, prec 0.0944949, recall 0.810196
2017-12-10T03:54:44.864095: step 6140, loss 0.100559, acc 0.953125, prec 0.0945031, recall 0.810217
2017-12-10T03:54:45.135570: step 6141, loss 0.0810176, acc 0.953125, prec 0.0945234, recall 0.81026
2017-12-10T03:54:45.407455: step 6142, loss 0.293328, acc 0.96875, prec 0.0945209, recall 0.81026
2017-12-10T03:54:45.675875: step 6143, loss 0.122667, acc 0.96875, prec 0.0945304, recall 0.810282
2017-12-10T03:54:45.940621: step 6144, loss 0.144409, acc 0.984375, prec 0.0945291, recall 0.810282
2017-12-10T03:54:46.211903: step 6145, loss 0.264524, acc 0.9375, prec 0.0945241, recall 0.810282
2017-12-10T03:54:46.485561: step 6146, loss 0.0219874, acc 1, prec 0.0945482, recall 0.810325
2017-12-10T03:54:46.759809: step 6147, loss 0.00995028, acc 1, prec 0.0945602, recall 0.810347
2017-12-10T03:54:47.027534: step 6148, loss 0.0204943, acc 0.984375, prec 0.0945589, recall 0.810347
2017-12-10T03:54:47.292687: step 6149, loss 0.0163903, acc 0.984375, prec 0.0945577, recall 0.810347
2017-12-10T03:54:47.557588: step 6150, loss 0.0657035, acc 0.984375, prec 0.0945804, recall 0.81039
2017-12-10T03:54:47.821290: step 6151, loss 0.624806, acc 0.96875, prec 0.0946019, recall 0.810433
2017-12-10T03:54:48.095275: step 6152, loss 0.102584, acc 0.984375, prec 0.0946007, recall 0.810433
2017-12-10T03:54:48.363051: step 6153, loss 3.06839e-05, acc 1, prec 0.0946127, recall 0.810455
2017-12-10T03:54:48.628824: step 6154, loss 0.00933732, acc 1, prec 0.0946127, recall 0.810455
2017-12-10T03:54:48.896373: step 6155, loss 0.524467, acc 0.9375, prec 0.0946077, recall 0.810455
2017-12-10T03:54:49.161819: step 6156, loss 0.0144151, acc 1, prec 0.0946197, recall 0.810476
2017-12-10T03:54:49.429374: step 6157, loss 0.0289162, acc 0.984375, prec 0.0946184, recall 0.810476
2017-12-10T03:54:49.695055: step 6158, loss 0.0154816, acc 1, prec 0.0946304, recall 0.810498
2017-12-10T03:54:49.963651: step 6159, loss 0.000521291, acc 1, prec 0.0946425, recall 0.810519
2017-12-10T03:54:50.229777: step 6160, loss 0.00334977, acc 1, prec 0.0946665, recall 0.810562
2017-12-10T03:54:50.496129: step 6161, loss 0.239052, acc 1, prec 0.0946905, recall 0.810605
2017-12-10T03:54:50.760508: step 6162, loss 0.049124, acc 1, prec 0.0947025, recall 0.810627
2017-12-10T03:54:51.026455: step 6163, loss 0.12756, acc 0.984375, prec 0.0947132, recall 0.810648
2017-12-10T03:54:51.292020: step 6164, loss 0.0971582, acc 0.96875, prec 0.0947227, recall 0.81067
2017-12-10T03:54:51.567804: step 6165, loss 0.00767047, acc 1, prec 0.0947348, recall 0.810691
2017-12-10T03:54:51.836562: step 6166, loss 0.325927, acc 0.953125, prec 0.094767, recall 0.810756
2017-12-10T03:54:52.103542: step 6167, loss 1.02427, acc 0.984375, prec 0.0948138, recall 0.810841
2017-12-10T03:54:52.378567: step 6168, loss 0.23158, acc 0.953125, prec 0.094822, recall 0.810863
2017-12-10T03:54:52.647315: step 6169, loss 0.0502122, acc 0.984375, prec 0.0948207, recall 0.810863
2017-12-10T03:54:52.918234: step 6170, loss 0.296242, acc 0.96875, prec 0.0948422, recall 0.810906
2017-12-10T03:54:53.186058: step 6171, loss 0.105887, acc 0.953125, prec 0.0948384, recall 0.810906
2017-12-10T03:54:53.455213: step 6172, loss 0.406561, acc 0.921875, prec 0.0948442, recall 0.810927
2017-12-10T03:54:53.723367: step 6173, loss 0.329591, acc 0.9375, prec 0.0948631, recall 0.81097
2017-12-10T03:54:53.993718: step 6174, loss 0.112106, acc 0.953125, prec 0.0948713, recall 0.810992
2017-12-10T03:54:54.259560: step 6175, loss 0.2078, acc 0.96875, prec 0.0948688, recall 0.810992
2017-12-10T03:54:54.543037: step 6176, loss 0.160557, acc 0.921875, prec 0.0948865, recall 0.811034
2017-12-10T03:54:54.813544: step 6177, loss 0.0986109, acc 0.96875, prec 0.094908, recall 0.811077
2017-12-10T03:54:55.080637: step 6178, loss 0.159457, acc 0.984375, prec 0.0949188, recall 0.811099
2017-12-10T03:54:55.349782: step 6179, loss 0.0599713, acc 0.96875, prec 0.0949282, recall 0.81112
2017-12-10T03:54:55.627008: step 6180, loss 0.615263, acc 0.921875, prec 0.0949219, recall 0.81112
2017-12-10T03:54:55.889507: step 6181, loss 0.0428766, acc 0.984375, prec 0.0949207, recall 0.81112
2017-12-10T03:54:56.160176: step 6182, loss 0.138411, acc 0.984375, prec 0.0949194, recall 0.81112
2017-12-10T03:54:56.430187: step 6183, loss 6.33776, acc 0.9375, prec 0.0949169, recall 0.810936
2017-12-10T03:54:56.706548: step 6184, loss 0.127993, acc 0.984375, prec 0.0949157, recall 0.810936
2017-12-10T03:54:56.982440: step 6185, loss 0.263795, acc 0.953125, prec 0.0949479, recall 0.811
2017-12-10T03:54:57.249233: step 6186, loss 0.589229, acc 0.890625, prec 0.0949391, recall 0.811
2017-12-10T03:54:57.511136: step 6187, loss 0.555528, acc 0.90625, prec 0.0949435, recall 0.811022
2017-12-10T03:54:57.780734: step 6188, loss 0.393809, acc 0.921875, prec 0.0949492, recall 0.811043
2017-12-10T03:54:58.048854: step 6189, loss 1.21655, acc 0.734375, prec 0.0949398, recall 0.811065
2017-12-10T03:54:58.313872: step 6190, loss 0.986644, acc 0.8125, prec 0.0949247, recall 0.811065
2017-12-10T03:54:58.578155: step 6191, loss 1.5202, acc 0.765625, prec 0.0949059, recall 0.811065
2017-12-10T03:54:58.845414: step 6192, loss 0.876164, acc 0.796875, prec 0.0948895, recall 0.811065
2017-12-10T03:54:59.113572: step 6193, loss 1.34804, acc 0.8125, prec 0.0948745, recall 0.811065
2017-12-10T03:54:59.385676: step 6194, loss 1.03775, acc 0.875, prec 0.0949123, recall 0.81115
2017-12-10T03:54:59.652099: step 6195, loss 1.29088, acc 0.765625, prec 0.0949294, recall 0.811214
2017-12-10T03:54:59.917649: step 6196, loss 0.734332, acc 0.890625, prec 0.0949326, recall 0.811235
2017-12-10T03:55:00.188166: step 6197, loss 0.670039, acc 0.84375, prec 0.094932, recall 0.811257
2017-12-10T03:55:00.459719: step 6198, loss 0.689145, acc 0.859375, prec 0.0949327, recall 0.811278
2017-12-10T03:55:00.725468: step 6199, loss 0.652324, acc 0.890625, prec 0.0949359, recall 0.811299
2017-12-10T03:55:00.994585: step 6200, loss 0.586921, acc 0.875, prec 0.0949378, recall 0.811321
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-6200

2017-12-10T03:55:02.248774: step 6201, loss 1.03149, acc 0.9375, prec 0.0949328, recall 0.811321
2017-12-10T03:55:02.521655: step 6202, loss 0.471241, acc 0.90625, prec 0.0949372, recall 0.811342
2017-12-10T03:55:02.785064: step 6203, loss 0.069047, acc 0.984375, prec 0.0949479, recall 0.811363
2017-12-10T03:55:03.052425: step 6204, loss 0.45504, acc 0.90625, prec 0.0949404, recall 0.811363
2017-12-10T03:55:03.316254: step 6205, loss 0.354925, acc 0.96875, prec 0.0949379, recall 0.811363
2017-12-10T03:55:03.588864: step 6206, loss 0.0765501, acc 0.984375, prec 0.0949486, recall 0.811385
2017-12-10T03:55:03.852583: step 6207, loss 0.035228, acc 0.984375, prec 0.0949713, recall 0.811427
2017-12-10T03:55:04.125398: step 6208, loss 0.313865, acc 0.96875, prec 0.0949927, recall 0.81147
2017-12-10T03:55:04.390673: step 6209, loss 0.15917, acc 0.96875, prec 0.0950021, recall 0.811491
2017-12-10T03:55:04.657305: step 6210, loss 0.0495485, acc 0.96875, prec 0.0949996, recall 0.811491
2017-12-10T03:55:04.919568: step 6211, loss 0.168284, acc 0.953125, prec 0.0949958, recall 0.811491
2017-12-10T03:55:05.186559: step 6212, loss 0.019013, acc 0.984375, prec 0.0950065, recall 0.811512
2017-12-10T03:55:05.464023: step 6213, loss 0.0362699, acc 0.984375, prec 0.0950053, recall 0.811512
2017-12-10T03:55:05.729379: step 6214, loss 0.000418935, acc 1, prec 0.0950053, recall 0.811512
2017-12-10T03:55:05.998377: step 6215, loss 6.0602e-06, acc 1, prec 0.0950053, recall 0.811512
2017-12-10T03:55:06.259738: step 6216, loss 0.0383007, acc 0.984375, prec 0.095004, recall 0.811512
2017-12-10T03:55:06.524182: step 6217, loss 0.00788893, acc 1, prec 0.0950279, recall 0.811555
2017-12-10T03:55:06.800042: step 6218, loss 0.113948, acc 0.984375, prec 0.0950267, recall 0.811555
2017-12-10T03:55:07.068492: step 6219, loss 0.00216966, acc 1, prec 0.0950267, recall 0.811555
2017-12-10T03:55:07.340892: step 6220, loss 0.00197864, acc 1, prec 0.0950267, recall 0.811555
2017-12-10T03:55:07.604498: step 6221, loss 0.217558, acc 1, prec 0.0950386, recall 0.811576
2017-12-10T03:55:07.870366: step 6222, loss 0.00166799, acc 1, prec 0.0950386, recall 0.811576
2017-12-10T03:55:08.136340: step 6223, loss 7.10634, acc 0.984375, prec 0.0950506, recall 0.811506
2017-12-10T03:55:08.402222: step 6224, loss 0.00017889, acc 1, prec 0.0950506, recall 0.811506
2017-12-10T03:55:08.667080: step 6225, loss 0.176547, acc 0.953125, prec 0.0950588, recall 0.811527
2017-12-10T03:55:08.933287: step 6226, loss 0.0320025, acc 0.984375, prec 0.0950814, recall 0.81157
2017-12-10T03:55:09.197248: step 6227, loss 0.19372, acc 0.984375, prec 0.0950921, recall 0.811591
2017-12-10T03:55:09.458486: step 6228, loss 0.0272419, acc 0.984375, prec 0.0951028, recall 0.811612
2017-12-10T03:55:09.727540: step 6229, loss 0.0605864, acc 0.96875, prec 0.0951481, recall 0.811697
2017-12-10T03:55:09.999691: step 6230, loss 0.106551, acc 0.953125, prec 0.0951444, recall 0.811697
2017-12-10T03:55:10.267052: step 6231, loss 0.0319642, acc 0.984375, prec 0.0951551, recall 0.811718
2017-12-10T03:55:10.532199: step 6232, loss 0.00399217, acc 1, prec 0.095167, recall 0.81174
2017-12-10T03:55:10.798346: step 6233, loss 0.149263, acc 0.953125, prec 0.0951752, recall 0.811761
2017-12-10T03:55:11.066551: step 6234, loss 0.480945, acc 0.9375, prec 0.0951941, recall 0.811803
2017-12-10T03:55:11.332184: step 6235, loss 0.0284613, acc 0.96875, prec 0.0952155, recall 0.811846
2017-12-10T03:55:11.596711: step 6236, loss 1.41616, acc 0.96875, prec 0.0952142, recall 0.811754
2017-12-10T03:55:11.875673: step 6237, loss 0.184319, acc 0.96875, prec 0.0952356, recall 0.811796
2017-12-10T03:55:12.139238: step 6238, loss 0.700197, acc 0.953125, prec 0.0952318, recall 0.811796
2017-12-10T03:55:12.415255: step 6239, loss 0.12571, acc 0.953125, prec 0.095228, recall 0.811796
2017-12-10T03:55:12.675981: step 6240, loss 0.386067, acc 0.90625, prec 0.0952324, recall 0.811818
2017-12-10T03:55:12.942546: step 6241, loss 0.352784, acc 0.921875, prec 0.0952381, recall 0.811839
2017-12-10T03:55:13.209288: step 6242, loss 0.566009, acc 0.875, prec 0.095228, recall 0.811839
2017-12-10T03:55:13.480781: step 6243, loss 0.700221, acc 0.921875, prec 0.0952576, recall 0.811902
2017-12-10T03:55:13.753652: step 6244, loss 0.937951, acc 0.84375, prec 0.095245, recall 0.811902
2017-12-10T03:55:14.021722: step 6245, loss 0.916713, acc 0.859375, prec 0.0952576, recall 0.811945
2017-12-10T03:55:14.295705: step 6246, loss 0.0680973, acc 0.96875, prec 0.0953147, recall 0.81205
2017-12-10T03:55:14.569188: step 6247, loss 1.18962, acc 0.921875, prec 0.0953085, recall 0.81205
2017-12-10T03:55:14.839124: step 6248, loss 0.852063, acc 0.859375, prec 0.0952971, recall 0.81205
2017-12-10T03:55:15.102845: step 6249, loss 1.03259, acc 0.859375, prec 0.0952858, recall 0.81205
2017-12-10T03:55:15.368408: step 6250, loss 0.180708, acc 0.90625, prec 0.0952783, recall 0.81205
2017-12-10T03:55:15.637540: step 6251, loss 1.22302, acc 0.921875, prec 0.095272, recall 0.81205
2017-12-10T03:55:15.899227: step 6252, loss 0.420564, acc 0.921875, prec 0.0952657, recall 0.81205
2017-12-10T03:55:16.176910: step 6253, loss 0.253626, acc 0.9375, prec 0.0952726, recall 0.812072
2017-12-10T03:55:16.450588: step 6254, loss 0.540004, acc 0.921875, prec 0.0952783, recall 0.812093
2017-12-10T03:55:16.719464: step 6255, loss 0.234462, acc 0.96875, prec 0.0952758, recall 0.812093
2017-12-10T03:55:16.990340: step 6256, loss 0.065748, acc 0.96875, prec 0.0952733, recall 0.812093
2017-12-10T03:55:17.253220: step 6257, loss 0.289002, acc 0.953125, prec 0.0952933, recall 0.812135
2017-12-10T03:55:17.521774: step 6258, loss 0.129234, acc 0.953125, prec 0.0953015, recall 0.812156
2017-12-10T03:55:17.788879: step 6259, loss 0.607715, acc 0.90625, prec 0.095294, recall 0.812156
2017-12-10T03:55:18.056690: step 6260, loss 0.24869, acc 0.96875, prec 0.0952915, recall 0.812156
2017-12-10T03:55:18.321126: step 6261, loss 0.233569, acc 0.96875, prec 0.0952889, recall 0.812156
2017-12-10T03:55:18.589869: step 6262, loss 14.2968, acc 0.953125, prec 0.0952864, recall 0.812065
2017-12-10T03:55:18.868093: step 6263, loss 18.5245, acc 0.96875, prec 0.0952864, recall 0.811882
2017-12-10T03:55:19.141053: step 6264, loss 0.0380592, acc 0.984375, prec 0.095309, recall 0.811925
2017-12-10T03:55:19.411386: step 6265, loss 0.137084, acc 0.96875, prec 0.0953304, recall 0.811967
2017-12-10T03:55:19.679903: step 6266, loss 0.230902, acc 0.953125, prec 0.0953385, recall 0.811988
2017-12-10T03:55:19.947399: step 6267, loss 0.511003, acc 0.90625, prec 0.0953429, recall 0.812009
2017-12-10T03:55:20.215791: step 6268, loss 0.569564, acc 0.921875, prec 0.0953485, recall 0.81203
2017-12-10T03:55:20.484569: step 6269, loss 0.748704, acc 0.890625, prec 0.0953517, recall 0.812051
2017-12-10T03:55:20.749499: step 6270, loss 1.00842, acc 0.84375, prec 0.095351, recall 0.812072
2017-12-10T03:55:21.012552: step 6271, loss 1.30115, acc 0.90625, prec 0.0953554, recall 0.812093
2017-12-10T03:55:21.289817: step 6272, loss 1.91914, acc 0.78125, prec 0.0953616, recall 0.812135
2017-12-10T03:55:21.555672: step 6273, loss 1.30049, acc 0.75, prec 0.0953415, recall 0.812135
2017-12-10T03:55:21.831920: step 6274, loss 1.06649, acc 0.796875, prec 0.0953252, recall 0.812135
2017-12-10T03:55:22.101415: step 6275, loss 0.836277, acc 0.875, prec 0.0953271, recall 0.812157
2017-12-10T03:55:22.369491: step 6276, loss 0.852376, acc 0.921875, prec 0.0953327, recall 0.812178
2017-12-10T03:55:22.641602: step 6277, loss 0.521823, acc 0.84375, prec 0.0953321, recall 0.812199
2017-12-10T03:55:22.909258: step 6278, loss 1.01789, acc 0.8125, prec 0.0953408, recall 0.812241
2017-12-10T03:55:23.175103: step 6279, loss 0.914719, acc 0.859375, prec 0.0953534, recall 0.812283
2017-12-10T03:55:23.443598: step 6280, loss 1.24751, acc 0.859375, prec 0.0953421, recall 0.812283
2017-12-10T03:55:23.710946: step 6281, loss 0.903835, acc 0.8125, prec 0.0953389, recall 0.812304
2017-12-10T03:55:23.975043: step 6282, loss 0.654216, acc 0.90625, prec 0.0953552, recall 0.812346
2017-12-10T03:55:24.252611: step 6283, loss 0.86549, acc 0.828125, prec 0.0953414, recall 0.812346
2017-12-10T03:55:24.514899: step 6284, loss 0.107972, acc 0.953125, prec 0.0953495, recall 0.812367
2017-12-10T03:55:24.781081: step 6285, loss 0.432769, acc 0.90625, prec 0.0953658, recall 0.812409
2017-12-10T03:55:25.045575: step 6286, loss 0.533981, acc 0.890625, prec 0.095357, recall 0.812409
2017-12-10T03:55:25.315644: step 6287, loss 0.242986, acc 0.953125, prec 0.0953533, recall 0.812409
2017-12-10T03:55:25.576902: step 6288, loss 0.173745, acc 0.984375, prec 0.0953996, recall 0.812493
2017-12-10T03:55:25.847564: step 6289, loss 0.00184776, acc 1, prec 0.0953996, recall 0.812493
2017-12-10T03:55:26.111659: step 6290, loss 1.40771, acc 0.9375, prec 0.0954302, recall 0.812556
2017-12-10T03:55:26.382874: step 6291, loss 0.275603, acc 0.984375, prec 0.095429, recall 0.812556
2017-12-10T03:55:26.656590: step 6292, loss 0.312827, acc 0.953125, prec 0.0954252, recall 0.812556
2017-12-10T03:55:26.939429: step 6293, loss 0.0104038, acc 1, prec 0.0954728, recall 0.81264
2017-12-10T03:55:27.206667: step 6294, loss 0.0316935, acc 0.984375, prec 0.0954715, recall 0.81264
2017-12-10T03:55:27.476082: step 6295, loss 0.0901541, acc 0.984375, prec 0.0954703, recall 0.81264
2017-12-10T03:55:27.746659: step 6296, loss 0.22138, acc 0.96875, prec 0.0954677, recall 0.81264
2017-12-10T03:55:28.018646: step 6297, loss 0.0938198, acc 0.984375, prec 0.0954784, recall 0.812661
2017-12-10T03:55:28.289857: step 6298, loss 0.0411557, acc 0.96875, prec 0.0954878, recall 0.812682
2017-12-10T03:55:28.556807: step 6299, loss 0.0154182, acc 0.984375, prec 0.0955103, recall 0.812724
2017-12-10T03:55:28.831501: step 6300, loss 0.259056, acc 0.921875, prec 0.0955159, recall 0.812745

Evaluation:
2017-12-10T03:55:36.421564: step 6300, loss 10.6336, acc 0.97179, prec 0.0958075, recall 0.8042

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-6300

2017-12-10T03:55:37.701539: step 6301, loss 0.000484412, acc 1, prec 0.0958075, recall 0.8042
2017-12-10T03:55:37.968529: step 6302, loss 0.084232, acc 0.984375, prec 0.0958063, recall 0.8042
2017-12-10T03:55:38.242962: step 6303, loss 0.14016, acc 0.96875, prec 0.0958274, recall 0.804243
2017-12-10T03:55:38.516645: step 6304, loss 0.424461, acc 0.984375, prec 0.095838, recall 0.804264
2017-12-10T03:55:38.786904: step 6305, loss 8.98943e-05, acc 1, prec 0.095838, recall 0.804264
2017-12-10T03:55:39.054074: step 6306, loss 0.00145343, acc 1, prec 0.095838, recall 0.804264
2017-12-10T03:55:39.318008: step 6307, loss 0.0104263, acc 1, prec 0.095838, recall 0.804264
2017-12-10T03:55:39.584183: step 6308, loss 0.334267, acc 0.9375, prec 0.095833, recall 0.804264
2017-12-10T03:55:39.859531: step 6309, loss 0.298714, acc 0.96875, prec 0.0958423, recall 0.804286
2017-12-10T03:55:40.129483: step 6310, loss 0.00828247, acc 1, prec 0.0958423, recall 0.804286
2017-12-10T03:55:40.395124: step 6311, loss 2.02097, acc 0.984375, prec 0.0958423, recall 0.804197
2017-12-10T03:55:40.663567: step 6312, loss 0.174569, acc 0.953125, prec 0.0958504, recall 0.804219
2017-12-10T03:55:40.930510: step 6313, loss 0.1175, acc 0.953125, prec 0.0958585, recall 0.80424
2017-12-10T03:55:41.204291: step 6314, loss 0.0161716, acc 0.984375, prec 0.0958572, recall 0.80424
2017-12-10T03:55:41.473402: step 6315, loss 0.119037, acc 0.953125, prec 0.0958535, recall 0.80424
2017-12-10T03:55:41.749548: step 6316, loss 0.189317, acc 0.96875, prec 0.095851, recall 0.80424
2017-12-10T03:55:42.024471: step 6317, loss 0.0387238, acc 0.984375, prec 0.0958734, recall 0.804283
2017-12-10T03:55:42.295239: step 6318, loss 0.0443412, acc 0.96875, prec 0.0958709, recall 0.804283
2017-12-10T03:55:42.564144: step 6319, loss 0.309182, acc 0.96875, prec 0.0958684, recall 0.804283
2017-12-10T03:55:42.839546: step 6320, loss 0.146094, acc 0.96875, prec 0.0958895, recall 0.804326
2017-12-10T03:55:43.109415: step 6321, loss 0.251373, acc 0.953125, prec 0.0959094, recall 0.804369
2017-12-10T03:55:43.378186: step 6322, loss 0.0199223, acc 0.984375, prec 0.09592, recall 0.804391
2017-12-10T03:55:43.650273: step 6323, loss 0.224584, acc 0.921875, prec 0.0959492, recall 0.804455
2017-12-10T03:55:43.920177: step 6324, loss 0.322363, acc 0.921875, prec 0.0959548, recall 0.804477
2017-12-10T03:55:44.194168: step 6325, loss 0.00128907, acc 1, prec 0.0959548, recall 0.804477
2017-12-10T03:55:44.460864: step 6326, loss 0.0770673, acc 0.984375, prec 0.0959653, recall 0.804498
2017-12-10T03:55:44.723578: step 6327, loss 0.0569987, acc 0.984375, prec 0.0959641, recall 0.804498
2017-12-10T03:55:44.987800: step 6328, loss 0.149437, acc 0.96875, prec 0.0959734, recall 0.80452
2017-12-10T03:55:45.255604: step 6329, loss 0.640994, acc 0.953125, prec 0.0959815, recall 0.804541
2017-12-10T03:55:45.522235: step 6330, loss 0.412724, acc 0.984375, prec 0.0960039, recall 0.804584
2017-12-10T03:55:45.794672: step 6331, loss 0.157679, acc 0.953125, prec 0.0960119, recall 0.804605
2017-12-10T03:55:46.059985: step 6332, loss 0.0984573, acc 0.96875, prec 0.0960212, recall 0.804627
2017-12-10T03:55:46.331510: step 6333, loss 0.14374, acc 0.96875, prec 0.0960306, recall 0.804648
2017-12-10T03:55:46.605853: step 6334, loss 0.123166, acc 0.953125, prec 0.0960386, recall 0.804669
2017-12-10T03:55:46.875785: step 6335, loss 0.306749, acc 0.953125, prec 0.0960348, recall 0.804669
2017-12-10T03:55:47.144924: step 6336, loss 0.123488, acc 0.96875, prec 0.0960442, recall 0.804691
2017-12-10T03:55:47.410756: step 6337, loss 0.115888, acc 0.9375, prec 0.096051, recall 0.804712
2017-12-10T03:55:47.679818: step 6338, loss 0.259143, acc 0.953125, prec 0.096059, recall 0.804734
2017-12-10T03:55:47.955312: step 6339, loss 0.27876, acc 0.953125, prec 0.0960671, recall 0.804755
2017-12-10T03:55:48.222653: step 6340, loss 0.814776, acc 0.9375, prec 0.0960857, recall 0.804798
2017-12-10T03:55:49.206573: step 6341, loss 0.148036, acc 0.984375, prec 0.0960963, recall 0.804819
2017-12-10T03:55:49.569552: step 6342, loss 0.0840386, acc 0.984375, prec 0.0961068, recall 0.804841
2017-12-10T03:55:49.847629: step 6343, loss 0.141604, acc 0.96875, prec 0.0961043, recall 0.804841
2017-12-10T03:55:50.582837: step 6344, loss 0.353127, acc 0.953125, prec 0.0961242, recall 0.804883
2017-12-10T03:55:51.296927: step 6345, loss 0.178808, acc 0.921875, prec 0.0961179, recall 0.804883
2017-12-10T03:55:52.059696: step 6346, loss 0.215357, acc 0.953125, prec 0.0961141, recall 0.804883
2017-12-10T03:55:52.781840: step 6347, loss 0.0745777, acc 0.96875, prec 0.0961116, recall 0.804883
2017-12-10T03:55:53.497068: step 6348, loss 0.050792, acc 0.96875, prec 0.0961209, recall 0.804905
2017-12-10T03:55:54.226904: step 6349, loss 0.0852384, acc 0.984375, prec 0.0961196, recall 0.804905
2017-12-10T03:55:55.028679: step 6350, loss 0.063792, acc 0.96875, prec 0.096129, recall 0.804926
2017-12-10T03:55:55.541239: step 6351, loss 0.0901996, acc 0.953125, prec 0.0961488, recall 0.804969
2017-12-10T03:55:55.819239: step 6352, loss 0.23181, acc 0.953125, prec 0.0961687, recall 0.805012
2017-12-10T03:55:56.095188: step 6353, loss 0.0236699, acc 0.984375, prec 0.0961792, recall 0.805033
2017-12-10T03:55:56.368130: step 6354, loss 0.0917214, acc 0.984375, prec 0.096178, recall 0.805033
2017-12-10T03:55:56.633703: step 6355, loss 0.506233, acc 0.921875, prec 0.0961835, recall 0.805054
2017-12-10T03:55:56.914534: step 6356, loss 0.0224929, acc 0.984375, prec 0.0961822, recall 0.805054
2017-12-10T03:55:57.182068: step 6357, loss 0.149785, acc 0.96875, prec 0.0962034, recall 0.805097
2017-12-10T03:55:57.449334: step 6358, loss 0.000444482, acc 1, prec 0.0962034, recall 0.805097
2017-12-10T03:55:57.711149: step 6359, loss 0.0602352, acc 0.96875, prec 0.0962127, recall 0.805118
2017-12-10T03:55:57.978727: step 6360, loss 0.0806977, acc 0.96875, prec 0.096222, recall 0.805139
2017-12-10T03:55:58.253717: step 6361, loss 0.0940869, acc 0.96875, prec 0.0962312, recall 0.805161
2017-12-10T03:55:58.522347: step 6362, loss 0.0191601, acc 0.984375, prec 0.0962418, recall 0.805182
2017-12-10T03:55:58.788457: step 6363, loss 0.00185161, acc 1, prec 0.0962536, recall 0.805203
2017-12-10T03:55:59.052988: step 6364, loss 0.587294, acc 0.953125, prec 0.0962498, recall 0.805203
2017-12-10T03:55:59.324086: step 6365, loss 0.0949534, acc 0.984375, prec 0.0962722, recall 0.805246
2017-12-10T03:55:59.598660: step 6366, loss 0.228219, acc 1, prec 0.0962958, recall 0.805288
2017-12-10T03:55:59.867704: step 6367, loss 0.000740438, acc 1, prec 0.0963076, recall 0.80531
2017-12-10T03:56:00.137396: step 6368, loss 0.282604, acc 0.984375, prec 0.0963182, recall 0.805331
2017-12-10T03:56:00.407256: step 6369, loss 0.00514339, acc 1, prec 0.0963182, recall 0.805331
2017-12-10T03:56:00.678068: step 6370, loss 0.299492, acc 0.984375, prec 0.0963169, recall 0.805331
2017-12-10T03:56:00.945473: step 6371, loss 0.325124, acc 0.96875, prec 0.0963144, recall 0.805331
2017-12-10T03:56:01.211719: step 6372, loss 0.0110418, acc 1, prec 0.0963144, recall 0.805331
2017-12-10T03:56:01.478967: step 6373, loss 0.011604, acc 1, prec 0.0963144, recall 0.805331
2017-12-10T03:56:01.743132: step 6374, loss 5.07436, acc 0.96875, prec 0.0963367, recall 0.805286
2017-12-10T03:56:02.017697: step 6375, loss 0.336753, acc 0.96875, prec 0.0963342, recall 0.805286
2017-12-10T03:56:02.280773: step 6376, loss 0.201758, acc 0.953125, prec 0.0963423, recall 0.805307
2017-12-10T03:56:02.546372: step 6377, loss 0.472987, acc 0.953125, prec 0.0963503, recall 0.805328
2017-12-10T03:56:02.808030: step 6378, loss 0.13584, acc 0.984375, prec 0.0963608, recall 0.805349
2017-12-10T03:56:03.084586: step 6379, loss 0.0145709, acc 0.984375, prec 0.0963832, recall 0.805392
2017-12-10T03:56:03.355758: step 6380, loss 0.290649, acc 0.953125, prec 0.0963912, recall 0.805413
2017-12-10T03:56:03.620303: step 6381, loss 0.204531, acc 0.9375, prec 0.096398, recall 0.805434
2017-12-10T03:56:03.885383: step 6382, loss 0.262532, acc 0.96875, prec 0.0963955, recall 0.805434
2017-12-10T03:56:04.151239: step 6383, loss 0.318511, acc 0.9375, prec 0.096414, recall 0.805477
2017-12-10T03:56:04.425615: step 6384, loss 0.194591, acc 0.953125, prec 0.0964338, recall 0.805519
2017-12-10T03:56:04.696657: step 6385, loss 0.140472, acc 0.96875, prec 0.0964313, recall 0.805519
2017-12-10T03:56:04.961602: step 6386, loss 0.548515, acc 0.953125, prec 0.0964393, recall 0.80554
2017-12-10T03:56:05.233770: step 6387, loss 0.0630092, acc 0.984375, prec 0.0964499, recall 0.805562
2017-12-10T03:56:05.495932: step 6388, loss 0.546189, acc 0.953125, prec 0.0964697, recall 0.805604
2017-12-10T03:56:05.763268: step 6389, loss 0.177581, acc 0.953125, prec 0.0964777, recall 0.805625
2017-12-10T03:56:06.037240: step 6390, loss 0.353642, acc 0.921875, prec 0.0964832, recall 0.805646
2017-12-10T03:56:06.302844: step 6391, loss 0.102062, acc 0.953125, prec 0.0964912, recall 0.805668
2017-12-10T03:56:06.565946: step 6392, loss 0.100221, acc 0.984375, prec 0.0965018, recall 0.805689
2017-12-10T03:56:06.831590: step 6393, loss 1.78443, acc 0.84375, prec 0.0964892, recall 0.805689
2017-12-10T03:56:07.104321: step 6394, loss 0.493477, acc 0.9375, prec 0.0964959, recall 0.80571
2017-12-10T03:56:07.373620: step 6395, loss 0.812329, acc 0.90625, prec 0.0964884, recall 0.80571
2017-12-10T03:56:07.642731: step 6396, loss 0.138073, acc 0.953125, prec 0.0964964, recall 0.805731
2017-12-10T03:56:07.909254: step 6397, loss 0.531732, acc 0.90625, prec 0.0965124, recall 0.805773
2017-12-10T03:56:08.181011: step 6398, loss 0.808568, acc 0.890625, prec 0.0965036, recall 0.805773
2017-12-10T03:56:08.453388: step 6399, loss 0.124047, acc 0.96875, prec 0.0965129, recall 0.805795
2017-12-10T03:56:08.719647: step 6400, loss 0.192421, acc 0.96875, prec 0.0965221, recall 0.805816
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-6400

2017-12-10T03:56:09.983266: step 6401, loss 0.232618, acc 0.9375, prec 0.0965171, recall 0.805816
2017-12-10T03:56:10.253709: step 6402, loss 0.238329, acc 0.984375, prec 0.0965276, recall 0.805837
2017-12-10T03:56:10.516840: step 6403, loss 0.0086784, acc 1, prec 0.0965276, recall 0.805837
2017-12-10T03:56:10.787094: step 6404, loss 0.113404, acc 0.96875, prec 0.0965722, recall 0.805921
2017-12-10T03:56:11.061434: step 6405, loss 0.334391, acc 0.953125, prec 0.096592, recall 0.805964
2017-12-10T03:56:11.325951: step 6406, loss 0.331901, acc 0.953125, prec 0.0965882, recall 0.805964
2017-12-10T03:56:11.591556: step 6407, loss 0.00800419, acc 1, prec 0.0966, recall 0.805985
2017-12-10T03:56:11.866268: step 6408, loss 0.155538, acc 0.984375, prec 0.0966223, recall 0.806027
2017-12-10T03:56:12.135934: step 6409, loss 0.402781, acc 0.953125, prec 0.0966185, recall 0.806027
2017-12-10T03:56:12.407522: step 6410, loss 0.227524, acc 0.96875, prec 0.0966278, recall 0.806048
2017-12-10T03:56:12.690819: step 6411, loss 0.0350821, acc 0.984375, prec 0.0966265, recall 0.806048
2017-12-10T03:56:12.955242: step 6412, loss 0.0970824, acc 0.984375, prec 0.0966371, recall 0.806069
2017-12-10T03:56:13.221721: step 6413, loss 0.0287994, acc 0.984375, prec 0.0966358, recall 0.806069
2017-12-10T03:56:13.490352: step 6414, loss 1.51718, acc 0.953125, prec 0.0966451, recall 0.806003
2017-12-10T03:56:13.761294: step 6415, loss 0.151972, acc 0.96875, prec 0.0966425, recall 0.806003
2017-12-10T03:56:14.032362: step 6416, loss 0.201104, acc 0.953125, prec 0.0966741, recall 0.806066
2017-12-10T03:56:14.293646: step 6417, loss 0.0587558, acc 0.984375, prec 0.0966728, recall 0.806066
2017-12-10T03:56:14.567484: step 6418, loss 0.0137235, acc 1, prec 0.0966846, recall 0.806087
2017-12-10T03:56:14.835961: step 6419, loss 0.0763963, acc 0.984375, prec 0.0966834, recall 0.806087
2017-12-10T03:56:15.102741: step 6420, loss 0.712495, acc 0.921875, prec 0.096677, recall 0.806087
2017-12-10T03:56:15.384678: step 6421, loss 0.0168314, acc 0.984375, prec 0.0966758, recall 0.806087
2017-12-10T03:56:15.660418: step 6422, loss 0.0740414, acc 0.984375, prec 0.0966745, recall 0.806087
2017-12-10T03:56:15.931642: step 6423, loss 0.0894099, acc 0.953125, prec 0.0966943, recall 0.806129
2017-12-10T03:56:16.199941: step 6424, loss 0.25513, acc 0.96875, prec 0.0966918, recall 0.806129
2017-12-10T03:56:16.466940: step 6425, loss 0.0330234, acc 0.984375, prec 0.0967141, recall 0.806171
2017-12-10T03:56:16.737003: step 6426, loss 0.725503, acc 0.984375, prec 0.0967246, recall 0.806192
2017-12-10T03:56:17.002216: step 6427, loss 0.146177, acc 0.9375, prec 0.0967195, recall 0.806192
2017-12-10T03:56:17.274528: step 6428, loss 0.230909, acc 0.953125, prec 0.0967158, recall 0.806192
2017-12-10T03:56:17.548569: step 6429, loss 0.285346, acc 0.921875, prec 0.0967448, recall 0.806255
2017-12-10T03:56:17.811887: step 6430, loss 0.237824, acc 0.96875, prec 0.096754, recall 0.806276
2017-12-10T03:56:18.078761: step 6431, loss 0.118119, acc 0.96875, prec 0.096775, recall 0.806319
2017-12-10T03:56:18.347957: step 6432, loss 0.629536, acc 0.90625, prec 0.096791, recall 0.806361
2017-12-10T03:56:18.613495: step 6433, loss 0.338176, acc 0.9375, prec 0.0967977, recall 0.806382
2017-12-10T03:56:18.877912: step 6434, loss 0.152193, acc 0.953125, prec 0.0968175, recall 0.806424
2017-12-10T03:56:19.143640: step 6435, loss 0.077624, acc 0.953125, prec 0.0968137, recall 0.806424
2017-12-10T03:56:19.408273: step 6436, loss 0.379306, acc 0.953125, prec 0.0968217, recall 0.806445
2017-12-10T03:56:19.671463: step 6437, loss 0.110716, acc 0.96875, prec 0.0968192, recall 0.806445
2017-12-10T03:56:19.943589: step 6438, loss 0.436604, acc 0.96875, prec 0.0968284, recall 0.806466
2017-12-10T03:56:20.218533: step 6439, loss 0.202176, acc 0.921875, prec 0.0968221, recall 0.806466
2017-12-10T03:56:20.486588: step 6440, loss 0.0031442, acc 1, prec 0.0968456, recall 0.806508
2017-12-10T03:56:20.760669: step 6441, loss 0.106822, acc 0.96875, prec 0.0968549, recall 0.806529
2017-12-10T03:56:21.031748: step 6442, loss 0.451222, acc 0.90625, prec 0.0968473, recall 0.806529
2017-12-10T03:56:21.301859: step 6443, loss 0.28509, acc 0.921875, prec 0.0968527, recall 0.80655
2017-12-10T03:56:21.569002: step 6444, loss 3.83572, acc 0.984375, prec 0.0968763, recall 0.806504
2017-12-10T03:56:21.848658: step 6445, loss 0.0855122, acc 0.96875, prec 0.0968973, recall 0.806546
2017-12-10T03:56:22.115533: step 6446, loss 0.0247526, acc 1, prec 0.0968973, recall 0.806546
2017-12-10T03:56:22.380983: step 6447, loss 0.269235, acc 0.96875, prec 0.0968947, recall 0.806546
2017-12-10T03:56:22.647784: step 6448, loss 0.131442, acc 0.96875, prec 0.096904, recall 0.806567
2017-12-10T03:56:22.909484: step 6449, loss 0.438007, acc 0.90625, prec 0.0969082, recall 0.806588
2017-12-10T03:56:23.174327: step 6450, loss 0.563651, acc 0.875, prec 0.0969098, recall 0.806609
2017-12-10T03:56:23.444299: step 6451, loss 0.42375, acc 0.90625, prec 0.0969375, recall 0.806672
2017-12-10T03:56:23.717936: step 6452, loss 0.376265, acc 0.875, prec 0.0969274, recall 0.806672
2017-12-10T03:56:23.991781: step 6453, loss 5.5482, acc 0.84375, prec 0.0969278, recall 0.806605
2017-12-10T03:56:24.264042: step 6454, loss 0.469519, acc 0.921875, prec 0.0969215, recall 0.806605
2017-12-10T03:56:24.532748: step 6455, loss 0.646388, acc 0.875, prec 0.0969114, recall 0.806605
2017-12-10T03:56:24.802342: step 6456, loss 1.04971, acc 0.8125, prec 0.0968963, recall 0.806605
2017-12-10T03:56:25.071793: step 6457, loss 1.18645, acc 0.796875, prec 0.0968917, recall 0.806626
2017-12-10T03:56:25.340645: step 6458, loss 1.31277, acc 0.75, prec 0.096895, recall 0.806668
2017-12-10T03:56:25.621571: step 6459, loss 2.11167, acc 0.703125, prec 0.0968711, recall 0.806668
2017-12-10T03:56:25.883723: step 6460, loss 1.39225, acc 0.796875, prec 0.0968899, recall 0.806731
2017-12-10T03:56:26.116447: step 6461, loss 1.0682, acc 0.745098, prec 0.0968853, recall 0.806752
2017-12-10T03:56:26.390623: step 6462, loss 0.485319, acc 0.890625, prec 0.0968765, recall 0.806752
2017-12-10T03:56:26.667554: step 6463, loss 1.21167, acc 0.8125, prec 0.0968731, recall 0.806773
2017-12-10T03:56:26.944283: step 6464, loss 1.50933, acc 0.859375, prec 0.0968618, recall 0.806773
2017-12-10T03:56:27.210000: step 6465, loss 0.1149, acc 0.953125, prec 0.0968815, recall 0.806814
2017-12-10T03:56:27.476887: step 6466, loss 0.0728687, acc 0.984375, prec 0.0969037, recall 0.806856
2017-12-10T03:56:27.740636: step 6467, loss 0.275863, acc 0.890625, prec 0.0969066, recall 0.806877
2017-12-10T03:56:28.005914: step 6468, loss 0.218468, acc 0.96875, prec 0.0969041, recall 0.806877
2017-12-10T03:56:28.268706: step 6469, loss 0.30157, acc 0.9375, prec 0.0969108, recall 0.806898
2017-12-10T03:56:28.533778: step 6470, loss 0.158998, acc 0.953125, prec 0.096907, recall 0.806898
2017-12-10T03:56:28.799428: step 6471, loss 0.299566, acc 0.953125, prec 0.0969149, recall 0.806919
2017-12-10T03:56:29.067144: step 6472, loss 0.083807, acc 0.96875, prec 0.0969124, recall 0.806919
2017-12-10T03:56:29.339731: step 6473, loss 0.28601, acc 0.96875, prec 0.0969099, recall 0.806919
2017-12-10T03:56:29.614414: step 6474, loss 0.154197, acc 0.953125, prec 0.0969178, recall 0.80694
2017-12-10T03:56:29.879900: step 6475, loss 0.42236, acc 0.96875, prec 0.0969388, recall 0.806982
2017-12-10T03:56:30.162598: step 6476, loss 0.00310062, acc 1, prec 0.0969622, recall 0.807023
2017-12-10T03:56:30.443983: step 6477, loss 0.0509077, acc 0.984375, prec 0.0969727, recall 0.807044
2017-12-10T03:56:30.714619: step 6478, loss 1.5291, acc 0.96875, prec 0.0969714, recall 0.806957
2017-12-10T03:56:30.986115: step 6479, loss 0.00588469, acc 1, prec 0.0969714, recall 0.806957
2017-12-10T03:56:31.253302: step 6480, loss 0.439607, acc 0.953125, prec 0.0969794, recall 0.806978
2017-12-10T03:56:31.528406: step 6481, loss 0.0028881, acc 1, prec 0.0969794, recall 0.806978
2017-12-10T03:56:31.797928: step 6482, loss 0.0580446, acc 0.96875, prec 0.0970003, recall 0.807019
2017-12-10T03:56:32.064941: step 6483, loss 0.072748, acc 0.96875, prec 0.0970329, recall 0.807082
2017-12-10T03:56:32.331353: step 6484, loss 0.0743184, acc 0.984375, prec 0.0970551, recall 0.807124
2017-12-10T03:56:32.597392: step 6485, loss 0.137675, acc 0.96875, prec 0.0970643, recall 0.807144
2017-12-10T03:56:32.860528: step 6486, loss 0.0110625, acc 1, prec 0.097076, recall 0.807165
2017-12-10T03:56:33.122265: step 6487, loss 0.196428, acc 0.9375, prec 0.097071, recall 0.807165
2017-12-10T03:56:33.394756: step 6488, loss 0.287033, acc 0.890625, prec 0.0970856, recall 0.807207
2017-12-10T03:56:33.669264: step 6489, loss 0.00967349, acc 1, prec 0.0970856, recall 0.807207
2017-12-10T03:56:33.934268: step 6490, loss 0.00789154, acc 1, prec 0.0970856, recall 0.807207
2017-12-10T03:56:34.203760: step 6491, loss 0.00693577, acc 1, prec 0.097109, recall 0.807248
2017-12-10T03:56:34.475829: step 6492, loss 0.000683642, acc 1, prec 0.097109, recall 0.807248
2017-12-10T03:56:34.750525: step 6493, loss 0.243327, acc 0.984375, prec 0.0971195, recall 0.807269
2017-12-10T03:56:35.022981: step 6494, loss 0.177639, acc 0.953125, prec 0.0971157, recall 0.807269
2017-12-10T03:56:35.294937: step 6495, loss 0.140045, acc 0.953125, prec 0.0971119, recall 0.807269
2017-12-10T03:56:35.559604: step 6496, loss 0.365157, acc 0.9375, prec 0.0971186, recall 0.80729
2017-12-10T03:56:35.826148: step 6497, loss 0.00497796, acc 1, prec 0.0971538, recall 0.807352
2017-12-10T03:56:36.096392: step 6498, loss 0.281579, acc 0.96875, prec 0.0971512, recall 0.807352
2017-12-10T03:56:36.363131: step 6499, loss 0.126329, acc 0.96875, prec 0.0971487, recall 0.807352
2017-12-10T03:56:36.635307: step 6500, loss 0.183948, acc 0.96875, prec 0.0971696, recall 0.807394
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-6500

2017-12-10T03:56:37.990010: step 6501, loss 0.0174474, acc 1, prec 0.097193, recall 0.807435
2017-12-10T03:56:38.266952: step 6502, loss 0.00661766, acc 1, prec 0.097193, recall 0.807435
2017-12-10T03:56:38.530624: step 6503, loss 0.000101862, acc 1, prec 0.097193, recall 0.807435
2017-12-10T03:56:38.789709: step 6504, loss 0.0450564, acc 0.984375, prec 0.0972035, recall 0.807456
2017-12-10T03:56:39.057948: step 6505, loss 0.0425274, acc 0.984375, prec 0.0972022, recall 0.807456
2017-12-10T03:56:39.326535: step 6506, loss 0.056852, acc 0.984375, prec 0.0972127, recall 0.807477
2017-12-10T03:56:39.596637: step 6507, loss 0.361476, acc 0.953125, prec 0.0972089, recall 0.807477
2017-12-10T03:56:39.859747: step 6508, loss 0.151125, acc 0.96875, prec 0.0972064, recall 0.807477
2017-12-10T03:56:40.129149: step 6509, loss 0.0283311, acc 0.984375, prec 0.0972051, recall 0.807477
2017-12-10T03:56:40.396824: step 6510, loss 0.0929945, acc 0.984375, prec 0.0972273, recall 0.807518
2017-12-10T03:56:40.663760: step 6511, loss 0.176565, acc 0.9375, prec 0.0972339, recall 0.807539
2017-12-10T03:56:40.937419: step 6512, loss 0.0474096, acc 0.96875, prec 0.0972314, recall 0.807539
2017-12-10T03:56:41.206084: step 6513, loss 0.10659, acc 0.984375, prec 0.0972301, recall 0.807539
2017-12-10T03:56:41.470150: step 6514, loss 0.0879052, acc 0.984375, prec 0.0972406, recall 0.80756
2017-12-10T03:56:41.744432: step 6515, loss 0.11407, acc 0.984375, prec 0.097251, recall 0.80758
2017-12-10T03:56:42.012074: step 6516, loss 0.00166589, acc 1, prec 0.0972627, recall 0.807601
2017-12-10T03:56:42.284093: step 6517, loss 0.0596055, acc 0.96875, prec 0.0972719, recall 0.807622
2017-12-10T03:56:42.552527: step 6518, loss 0.0110547, acc 1, prec 0.0972719, recall 0.807622
2017-12-10T03:56:42.821581: step 6519, loss 0.101277, acc 0.984375, prec 0.0972941, recall 0.807663
2017-12-10T03:56:43.085856: step 6520, loss 0.240553, acc 1, prec 0.0973058, recall 0.807684
2017-12-10T03:56:43.354885: step 6521, loss 0.0530714, acc 0.984375, prec 0.0973162, recall 0.807705
2017-12-10T03:56:43.622388: step 6522, loss 0.0555963, acc 0.96875, prec 0.0973254, recall 0.807725
2017-12-10T03:56:43.887674: step 6523, loss 0.254634, acc 0.953125, prec 0.0973216, recall 0.807725
2017-12-10T03:56:44.157416: step 6524, loss 0.0580889, acc 0.984375, prec 0.0973321, recall 0.807746
2017-12-10T03:56:44.428000: step 6525, loss 0.0534342, acc 0.984375, prec 0.0973308, recall 0.807746
2017-12-10T03:56:44.700899: step 6526, loss 0.144318, acc 0.984375, prec 0.0973412, recall 0.807767
2017-12-10T03:56:44.982110: step 6527, loss 0.05051, acc 0.984375, prec 0.09734, recall 0.807767
2017-12-10T03:56:45.245427: step 6528, loss 0.0216023, acc 0.984375, prec 0.0973504, recall 0.807787
2017-12-10T03:56:45.512480: step 6529, loss 0.191684, acc 0.96875, prec 0.0973479, recall 0.807787
2017-12-10T03:56:45.790906: step 6530, loss 0.0372822, acc 0.984375, prec 0.0973466, recall 0.807787
2017-12-10T03:56:46.057779: step 6531, loss 6.54319e-06, acc 1, prec 0.0973466, recall 0.807787
2017-12-10T03:56:46.321239: step 6532, loss 0.0100501, acc 1, prec 0.0973466, recall 0.807787
2017-12-10T03:56:46.584594: step 6533, loss 0.223871, acc 0.984375, prec 0.0973805, recall 0.807849
2017-12-10T03:56:46.854928: step 6534, loss 0.00556335, acc 1, prec 0.0973805, recall 0.807849
2017-12-10T03:56:47.126490: step 6535, loss 0.00223254, acc 1, prec 0.0973922, recall 0.80787
2017-12-10T03:56:47.393898: step 6536, loss 0.19073, acc 0.984375, prec 0.0973909, recall 0.80787
2017-12-10T03:56:47.664117: step 6537, loss 0.0354008, acc 0.984375, prec 0.0973896, recall 0.80787
2017-12-10T03:56:47.928196: step 6538, loss 18.5522, acc 0.953125, prec 0.0974118, recall 0.807738
2017-12-10T03:56:48.201904: step 6539, loss 0.671876, acc 0.921875, prec 0.0974055, recall 0.807738
2017-12-10T03:56:48.470521: step 6540, loss 0.322054, acc 0.9375, prec 0.0974121, recall 0.807758
2017-12-10T03:56:48.737456: step 6541, loss 0.256898, acc 0.9375, prec 0.0974071, recall 0.807758
2017-12-10T03:56:48.998641: step 6542, loss 0.748479, acc 0.890625, prec 0.0973982, recall 0.807758
2017-12-10T03:56:49.273314: step 6543, loss 1.18417, acc 0.8125, prec 0.0973831, recall 0.807758
2017-12-10T03:56:49.535321: step 6544, loss 1.76802, acc 0.75, prec 0.0973746, recall 0.807779
2017-12-10T03:56:49.801379: step 6545, loss 0.74911, acc 0.796875, prec 0.0973816, recall 0.80782
2017-12-10T03:56:50.077626: step 6546, loss 1.38435, acc 0.796875, prec 0.0973769, recall 0.807841
2017-12-10T03:56:50.344316: step 6547, loss 1.32569, acc 0.765625, prec 0.097358, recall 0.807841
2017-12-10T03:56:50.612066: step 6548, loss 0.922688, acc 0.796875, prec 0.0973766, recall 0.807903
2017-12-10T03:56:50.875594: step 6549, loss 2.10476, acc 0.75, prec 0.0973682, recall 0.807924
2017-12-10T03:56:51.143142: step 6550, loss 1.15741, acc 0.8125, prec 0.0973647, recall 0.807944
2017-12-10T03:56:51.408469: step 6551, loss 0.945705, acc 0.8125, prec 0.0973613, recall 0.807965
2017-12-10T03:56:51.672906: step 6552, loss 1.3412, acc 0.78125, prec 0.0973553, recall 0.807985
2017-12-10T03:56:51.942282: step 6553, loss 1.00957, acc 0.78125, prec 0.0973494, recall 0.808006
2017-12-10T03:56:52.208827: step 6554, loss 1.38653, acc 0.84375, prec 0.0973718, recall 0.808068
2017-12-10T03:56:52.480274: step 6555, loss 0.558196, acc 0.890625, prec 0.097363, recall 0.808068
2017-12-10T03:56:52.746124: step 6556, loss 0.390435, acc 0.875, prec 0.0973762, recall 0.808109
2017-12-10T03:56:53.014693: step 6557, loss 0.519616, acc 0.90625, prec 0.0973804, recall 0.80813
2017-12-10T03:56:53.291243: step 6558, loss 0.317311, acc 0.890625, prec 0.0973832, recall 0.80815
2017-12-10T03:56:53.558470: step 6559, loss 0.594293, acc 0.890625, prec 0.0973861, recall 0.808171
2017-12-10T03:56:53.832378: step 6560, loss 0.21354, acc 0.921875, prec 0.0973914, recall 0.808191
2017-12-10T03:56:54.097244: step 6561, loss 0.229886, acc 0.921875, prec 0.0973851, recall 0.808191
2017-12-10T03:56:54.370209: step 6562, loss 0.237907, acc 0.953125, prec 0.0973814, recall 0.808191
2017-12-10T03:56:54.643059: step 6563, loss 0.371584, acc 0.953125, prec 0.0973776, recall 0.808191
2017-12-10T03:56:54.906615: step 6564, loss 0.0954234, acc 0.96875, prec 0.0973751, recall 0.808191
2017-12-10T03:56:55.172117: step 6565, loss 0.17416, acc 0.96875, prec 0.0973842, recall 0.808212
2017-12-10T03:56:55.441028: step 6566, loss 0.038621, acc 0.984375, prec 0.097383, recall 0.808212
2017-12-10T03:56:55.707991: step 6567, loss 0.0724957, acc 0.984375, prec 0.0973817, recall 0.808212
2017-12-10T03:56:55.973948: step 6568, loss 0.00017291, acc 1, prec 0.0973934, recall 0.808232
2017-12-10T03:56:56.238696: step 6569, loss 0.131093, acc 0.984375, prec 0.0973921, recall 0.808232
2017-12-10T03:56:56.503695: step 6570, loss 0.0156027, acc 1, prec 0.0974154, recall 0.808273
2017-12-10T03:56:56.780060: step 6571, loss 0.0201696, acc 0.984375, prec 0.0974142, recall 0.808273
2017-12-10T03:56:57.051561: step 6572, loss 0.00589922, acc 1, prec 0.0974142, recall 0.808273
2017-12-10T03:56:57.318785: step 6573, loss 0.00449735, acc 1, prec 0.0974375, recall 0.808315
2017-12-10T03:56:57.585816: step 6574, loss 5.51188e-05, acc 1, prec 0.0974375, recall 0.808315
2017-12-10T03:56:57.846240: step 6575, loss 9.93773, acc 0.984375, prec 0.0974491, recall 0.808249
2017-12-10T03:56:58.121074: step 6576, loss 0.00703678, acc 1, prec 0.0975074, recall 0.808351
2017-12-10T03:56:58.385635: step 6577, loss 0.224374, acc 1, prec 0.0975191, recall 0.808372
2017-12-10T03:56:58.654300: step 6578, loss 0.292208, acc 0.984375, prec 0.0975295, recall 0.808392
2017-12-10T03:56:58.926240: step 6579, loss 0.0610642, acc 0.984375, prec 0.0975282, recall 0.808392
2017-12-10T03:56:59.194749: step 6580, loss 1.49627, acc 0.90625, prec 0.0975207, recall 0.808392
2017-12-10T03:56:59.465055: step 6581, loss 0.00571739, acc 1, prec 0.0975207, recall 0.808392
2017-12-10T03:56:59.729833: step 6582, loss 0.362582, acc 0.96875, prec 0.0975415, recall 0.808433
2017-12-10T03:56:59.999144: step 6583, loss 0.569304, acc 0.9375, prec 0.0975364, recall 0.808433
2017-12-10T03:57:00.271795: step 6584, loss 0.0799898, acc 0.984375, prec 0.0975352, recall 0.808433
2017-12-10T03:57:00.547471: step 6585, loss 0.134881, acc 0.984375, prec 0.0975572, recall 0.808474
2017-12-10T03:57:00.812388: step 6586, loss 0.0185456, acc 0.984375, prec 0.0975559, recall 0.808474
2017-12-10T03:57:01.074415: step 6587, loss 0.208289, acc 0.9375, prec 0.0975509, recall 0.808474
2017-12-10T03:57:01.339391: step 6588, loss 0.104884, acc 0.984375, prec 0.0975496, recall 0.808474
2017-12-10T03:57:01.605522: step 6589, loss 0.814064, acc 0.96875, prec 0.0975471, recall 0.808474
2017-12-10T03:57:01.870531: step 6590, loss 0.325554, acc 0.96875, prec 0.0975446, recall 0.808474
2017-12-10T03:57:02.141192: step 6591, loss 0.928358, acc 0.9375, prec 0.0975629, recall 0.808515
2017-12-10T03:57:02.406724: step 6592, loss 0.203361, acc 0.921875, prec 0.0975915, recall 0.808577
2017-12-10T03:57:02.676564: step 6593, loss 0.0638606, acc 0.96875, prec 0.0976006, recall 0.808597
2017-12-10T03:57:02.940279: step 6594, loss 0.204313, acc 0.953125, prec 0.0976434, recall 0.808679
2017-12-10T03:57:03.203771: step 6595, loss 0.291005, acc 0.953125, prec 0.0976397, recall 0.808679
2017-12-10T03:57:03.473144: step 6596, loss 0.445973, acc 0.90625, prec 0.0976321, recall 0.808679
2017-12-10T03:57:03.735710: step 6597, loss 0.275308, acc 0.9375, prec 0.0976853, recall 0.808781
2017-12-10T03:57:04.002812: step 6598, loss 0.0288027, acc 0.984375, prec 0.0976957, recall 0.808802
2017-12-10T03:57:04.280414: step 6599, loss 0.458493, acc 0.953125, prec 0.0976919, recall 0.808802
2017-12-10T03:57:04.544421: step 6600, loss 0.0956885, acc 0.984375, prec 0.0977023, recall 0.808822

Evaluation:
2017-12-10T03:57:12.145457: step 6600, loss 6.70647, acc 0.946976, prec 0.0979712, recall 0.803973

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-6600

2017-12-10T03:57:13.878418: step 6601, loss 0.0054201, acc 1, prec 0.0979712, recall 0.803973
2017-12-10T03:57:14.151899: step 6602, loss 0.419547, acc 0.9375, prec 0.0979661, recall 0.803973
2017-12-10T03:57:14.419666: step 6603, loss 0.688229, acc 1, prec 0.0979777, recall 0.803994
2017-12-10T03:57:14.690622: step 6604, loss 0.0758896, acc 0.984375, prec 0.097988, recall 0.804014
2017-12-10T03:57:14.965276: step 6605, loss 0.977157, acc 0.890625, prec 0.0979792, recall 0.804014
2017-12-10T03:57:15.232599: step 6606, loss 0.235029, acc 0.921875, prec 0.0979845, recall 0.804035
2017-12-10T03:57:15.495171: step 6607, loss 0.588714, acc 0.9375, prec 0.0979795, recall 0.804035
2017-12-10T03:57:15.760278: step 6608, loss 0.00909714, acc 1, prec 0.097991, recall 0.804055
2017-12-10T03:57:16.029326: step 6609, loss 0.0296429, acc 0.984375, prec 0.0980013, recall 0.804076
2017-12-10T03:57:16.293078: step 6610, loss 0.245161, acc 0.921875, prec 0.097995, recall 0.804076
2017-12-10T03:57:16.557096: step 6611, loss 0.667226, acc 0.953125, prec 0.0980028, recall 0.804097
2017-12-10T03:57:16.826534: step 6612, loss 0.0699009, acc 0.96875, prec 0.0980349, recall 0.804158
2017-12-10T03:57:17.095802: step 6613, loss 0.265583, acc 0.953125, prec 0.0980312, recall 0.804158
2017-12-10T03:57:17.363153: step 6614, loss 0.206667, acc 0.953125, prec 0.098039, recall 0.804179
2017-12-10T03:57:17.631259: step 6615, loss 3.67291, acc 0.9375, prec 0.0980467, recall 0.804115
2017-12-10T03:57:17.905349: step 6616, loss 0.0585492, acc 0.96875, prec 0.0980442, recall 0.804115
2017-12-10T03:57:18.181836: step 6617, loss 0.188926, acc 0.96875, prec 0.0980533, recall 0.804136
2017-12-10T03:57:18.453408: step 6618, loss 0.0514559, acc 0.96875, prec 0.0980508, recall 0.804136
2017-12-10T03:57:18.724936: step 6619, loss 0.0705996, acc 0.96875, prec 0.0980713, recall 0.804177
2017-12-10T03:57:18.984827: step 6620, loss 0.0788555, acc 0.96875, prec 0.0980804, recall 0.804197
2017-12-10T03:57:19.253231: step 6621, loss 0.494787, acc 0.90625, prec 0.0981075, recall 0.804259
2017-12-10T03:57:19.526572: step 6622, loss 0.796016, acc 0.90625, prec 0.0981345, recall 0.80432
2017-12-10T03:57:19.790834: step 6623, loss 0.537312, acc 0.921875, prec 0.0981513, recall 0.804362
2017-12-10T03:57:20.059347: step 6624, loss 0.226956, acc 0.9375, prec 0.0981579, recall 0.804382
2017-12-10T03:57:20.324576: step 6625, loss 0.244555, acc 0.9375, prec 0.0981644, recall 0.804403
2017-12-10T03:57:20.596981: step 6626, loss 0.68867, acc 0.890625, prec 0.0981556, recall 0.804403
2017-12-10T03:57:20.861342: step 6627, loss 0.456638, acc 0.875, prec 0.0981455, recall 0.804403
2017-12-10T03:57:21.127253: step 6628, loss 0.801506, acc 0.90625, prec 0.098138, recall 0.804403
2017-12-10T03:57:21.392951: step 6629, loss 0.360358, acc 0.9375, prec 0.098133, recall 0.804403
2017-12-10T03:57:21.658313: step 6630, loss 0.637644, acc 0.953125, prec 0.0981408, recall 0.804423
2017-12-10T03:57:21.927137: step 6631, loss 0.940945, acc 0.84375, prec 0.0981397, recall 0.804444
2017-12-10T03:57:22.190752: step 6632, loss 1.43741, acc 0.875, prec 0.0981297, recall 0.804444
2017-12-10T03:57:22.460216: step 6633, loss 0.325062, acc 0.9375, prec 0.0981593, recall 0.804505
2017-12-10T03:57:22.724694: step 6634, loss 0.3111, acc 0.921875, prec 0.0981645, recall 0.804525
2017-12-10T03:57:22.992095: step 6635, loss 0.0182676, acc 1, prec 0.0981761, recall 0.804546
2017-12-10T03:57:23.261756: step 6636, loss 0.252598, acc 0.9375, prec 0.0981826, recall 0.804566
2017-12-10T03:57:23.535024: step 6637, loss 0.149179, acc 0.96875, prec 0.0982146, recall 0.804628
2017-12-10T03:57:23.802101: step 6638, loss 0.411164, acc 0.953125, prec 0.0982339, recall 0.804669
2017-12-10T03:57:24.072081: step 6639, loss 0.151092, acc 0.953125, prec 0.0982647, recall 0.80473
2017-12-10T03:57:24.340172: step 6640, loss 1.57203, acc 0.921875, prec 0.0982712, recall 0.804666
2017-12-10T03:57:24.615693: step 6641, loss 0.374073, acc 0.9375, prec 0.0982777, recall 0.804687
2017-12-10T03:57:24.878231: step 6642, loss 0.267724, acc 0.96875, prec 0.0982867, recall 0.804707
2017-12-10T03:57:25.144519: step 6643, loss 0.163731, acc 0.953125, prec 0.0983175, recall 0.804768
2017-12-10T03:57:25.410745: step 6644, loss 0.280081, acc 0.921875, prec 0.0983112, recall 0.804768
2017-12-10T03:57:25.683244: step 6645, loss 0.443256, acc 0.890625, prec 0.098314, recall 0.804789
2017-12-10T03:57:25.960762: step 6646, loss 0.499091, acc 0.953125, prec 0.0983332, recall 0.80483
2017-12-10T03:57:26.229953: step 6647, loss 0.327858, acc 0.921875, prec 0.0983385, recall 0.80485
2017-12-10T03:57:26.495212: step 6648, loss 0.167005, acc 0.953125, prec 0.0983462, recall 0.80487
2017-12-10T03:57:26.773136: step 6649, loss 0.491966, acc 0.953125, prec 0.0983655, recall 0.804911
2017-12-10T03:57:27.043611: step 6650, loss 0.100683, acc 0.96875, prec 0.098363, recall 0.804911
2017-12-10T03:57:27.310281: step 6651, loss 0.588014, acc 0.953125, prec 0.0983592, recall 0.804911
2017-12-10T03:57:27.582900: step 6652, loss 0.0346033, acc 0.984375, prec 0.098381, recall 0.804952
2017-12-10T03:57:27.852794: step 6653, loss 0.174369, acc 0.9375, prec 0.0983759, recall 0.804952
2017-12-10T03:57:28.119535: step 6654, loss 0.00123187, acc 1, prec 0.0983874, recall 0.804972
2017-12-10T03:57:28.381662: step 6655, loss 0.364033, acc 0.953125, prec 0.0983837, recall 0.804972
2017-12-10T03:57:28.647222: step 6656, loss 0.00793735, acc 1, prec 0.0983837, recall 0.804972
2017-12-10T03:57:28.914456: step 6657, loss 0.00698122, acc 1, prec 0.0984182, recall 0.805033
2017-12-10T03:57:29.189492: step 6658, loss 0.337831, acc 0.953125, prec 0.0984259, recall 0.805054
2017-12-10T03:57:29.459298: step 6659, loss 0.351628, acc 0.984375, prec 0.0984362, recall 0.805074
2017-12-10T03:57:29.731392: step 6660, loss 0.019584, acc 0.984375, prec 0.0984465, recall 0.805094
2017-12-10T03:57:30.010126: step 6661, loss 0.401696, acc 0.96875, prec 0.0984555, recall 0.805115
2017-12-10T03:57:30.280353: step 6662, loss 0.00653923, acc 1, prec 0.098467, recall 0.805135
2017-12-10T03:57:30.553080: step 6663, loss 0.011175, acc 1, prec 0.0984785, recall 0.805156
2017-12-10T03:57:30.819123: step 6664, loss 0.0357211, acc 1, prec 0.098513, recall 0.805216
2017-12-10T03:57:31.086834: step 6665, loss 0.0726897, acc 0.984375, prec 0.0985232, recall 0.805237
2017-12-10T03:57:31.352709: step 6666, loss 0.223948, acc 0.984375, prec 0.0985335, recall 0.805257
2017-12-10T03:57:31.625995: step 6667, loss 0.038069, acc 0.96875, prec 0.098531, recall 0.805257
2017-12-10T03:57:31.892435: step 6668, loss 0.858453, acc 0.9375, prec 0.0985605, recall 0.805318
2017-12-10T03:57:32.159568: step 6669, loss 0.0480894, acc 0.984375, prec 0.0985592, recall 0.805318
2017-12-10T03:57:32.426213: step 6670, loss 0.316112, acc 0.984375, prec 0.0985694, recall 0.805338
2017-12-10T03:57:32.687142: step 6671, loss 0.60614, acc 0.90625, prec 0.0985619, recall 0.805338
2017-12-10T03:57:32.954628: step 6672, loss 0.0834235, acc 1, prec 0.0985849, recall 0.805379
2017-12-10T03:57:33.220962: step 6673, loss 0.0284595, acc 0.984375, prec 0.0985836, recall 0.805379
2017-12-10T03:57:33.491568: step 6674, loss 0.429174, acc 0.96875, prec 0.0985811, recall 0.805379
2017-12-10T03:57:33.756700: step 6675, loss 0.268749, acc 0.96875, prec 0.0985901, recall 0.805399
2017-12-10T03:57:34.020945: step 6676, loss 0.00187114, acc 1, prec 0.0985901, recall 0.805399
2017-12-10T03:57:34.288298: step 6677, loss 0.479805, acc 0.984375, prec 0.0985889, recall 0.805399
2017-12-10T03:57:34.561879: step 6678, loss 5.21068, acc 0.96875, prec 0.0985991, recall 0.805336
2017-12-10T03:57:34.840934: step 6679, loss 0.236779, acc 0.96875, prec 0.0985966, recall 0.805336
2017-12-10T03:57:35.113081: step 6680, loss 0.168416, acc 0.96875, prec 0.0986056, recall 0.805356
2017-12-10T03:57:35.384210: step 6681, loss 0.353839, acc 0.9375, prec 0.0986005, recall 0.805356
2017-12-10T03:57:35.654941: step 6682, loss 0.527522, acc 0.890625, prec 0.0985917, recall 0.805356
2017-12-10T03:57:35.923771: step 6683, loss 0.274743, acc 0.953125, prec 0.0985995, recall 0.805376
2017-12-10T03:57:36.199915: step 6684, loss 0.0973649, acc 0.953125, prec 0.0986072, recall 0.805396
2017-12-10T03:57:36.473311: step 6685, loss 0.458819, acc 0.9375, prec 0.0986251, recall 0.805437
2017-12-10T03:57:36.742740: step 6686, loss 0.855738, acc 0.875, prec 0.0986266, recall 0.805457
2017-12-10T03:57:37.007544: step 6687, loss 0.25329, acc 0.953125, prec 0.0986343, recall 0.805477
2017-12-10T03:57:37.276076: step 6688, loss 0.295426, acc 0.875, prec 0.0986242, recall 0.805477
2017-12-10T03:57:37.543284: step 6689, loss 0.47352, acc 0.921875, prec 0.0986294, recall 0.805498
2017-12-10T03:57:37.807934: step 6690, loss 0.293133, acc 0.890625, prec 0.0986321, recall 0.805518
2017-12-10T03:57:38.072915: step 6691, loss 0.234431, acc 0.9375, prec 0.0986271, recall 0.805518
2017-12-10T03:57:38.337685: step 6692, loss 0.397403, acc 0.953125, prec 0.0986463, recall 0.805558
2017-12-10T03:57:38.606216: step 6693, loss 0.923737, acc 0.90625, prec 0.0986388, recall 0.805558
2017-12-10T03:57:38.870717: step 6694, loss 0.416595, acc 0.90625, prec 0.0986542, recall 0.805599
2017-12-10T03:57:39.137278: step 6695, loss 0.133803, acc 0.96875, prec 0.0986517, recall 0.805599
2017-12-10T03:57:39.403892: step 6696, loss 0.53907, acc 0.90625, prec 0.0986556, recall 0.805619
2017-12-10T03:57:39.664443: step 6697, loss 0.229866, acc 0.9375, prec 0.0986506, recall 0.805619
2017-12-10T03:57:39.930770: step 6698, loss 0.0707165, acc 0.96875, prec 0.0986596, recall 0.805639
2017-12-10T03:57:40.194632: step 6699, loss 0.255352, acc 0.953125, prec 0.0986558, recall 0.805639
2017-12-10T03:57:40.465084: step 6700, loss 0.0755892, acc 0.9375, prec 0.0986622, recall 0.80566
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-6700

2017-12-10T03:57:41.878435: step 6701, loss 0.196415, acc 0.96875, prec 0.0986712, recall 0.80568
2017-12-10T03:57:42.144127: step 6702, loss 0.228266, acc 0.9375, prec 0.0986777, recall 0.8057
2017-12-10T03:57:42.413388: step 6703, loss 0.213243, acc 0.953125, prec 0.0987083, recall 0.805761
2017-12-10T03:57:42.676490: step 6704, loss 0.0214164, acc 0.984375, prec 0.0987186, recall 0.805781
2017-12-10T03:57:42.944458: step 6705, loss 0.00788314, acc 1, prec 0.0987301, recall 0.805801
2017-12-10T03:57:43.209184: step 6706, loss 0.00445851, acc 1, prec 0.0987301, recall 0.805801
2017-12-10T03:57:43.486269: step 6707, loss 0.0344322, acc 0.984375, prec 0.0987288, recall 0.805801
2017-12-10T03:57:43.758484: step 6708, loss 0.00499534, acc 1, prec 0.0987288, recall 0.805801
2017-12-10T03:57:44.024903: step 6709, loss 0.406218, acc 0.953125, prec 0.0987365, recall 0.805821
2017-12-10T03:57:44.289407: step 6710, loss 1.11205, acc 0.984375, prec 0.0987467, recall 0.805841
2017-12-10T03:57:44.556390: step 6711, loss 0.413361, acc 0.96875, prec 0.0987901, recall 0.805922
2017-12-10T03:57:44.827127: step 6712, loss 0.167116, acc 0.984375, prec 0.0988003, recall 0.805942
2017-12-10T03:57:45.095182: step 6713, loss 0.0388566, acc 1, prec 0.0988118, recall 0.805962
2017-12-10T03:57:45.364637: step 6714, loss 0.669835, acc 1, prec 0.0988462, recall 0.806023
2017-12-10T03:57:45.631677: step 6715, loss 0.150731, acc 0.96875, prec 0.0988667, recall 0.806063
2017-12-10T03:57:45.901167: step 6716, loss 0.396413, acc 0.984375, prec 0.0988654, recall 0.806063
2017-12-10T03:57:46.168141: step 6717, loss 0.239015, acc 0.96875, prec 0.0988858, recall 0.806103
2017-12-10T03:57:46.434917: step 6718, loss 0.647958, acc 0.921875, prec 0.098891, recall 0.806123
2017-12-10T03:57:46.709245: step 6719, loss 0.0447013, acc 0.96875, prec 0.0989, recall 0.806144
2017-12-10T03:57:46.980806: step 6720, loss 0.25056, acc 0.921875, prec 0.0988937, recall 0.806144
2017-12-10T03:57:47.241563: step 6721, loss 0.921384, acc 0.953125, prec 0.0989014, recall 0.806164
2017-12-10T03:57:47.510068: step 6722, loss 0.23307, acc 0.90625, prec 0.0988938, recall 0.806164
2017-12-10T03:57:47.778064: step 6723, loss 1.53799, acc 0.84375, prec 0.0988927, recall 0.806184
2017-12-10T03:57:48.048477: step 6724, loss 0.12228, acc 0.953125, prec 0.0988889, recall 0.806184
2017-12-10T03:57:48.315806: step 6725, loss 0.186804, acc 0.9375, prec 0.0988954, recall 0.806204
2017-12-10T03:57:48.588602: step 6726, loss 0.221167, acc 0.96875, prec 0.0989043, recall 0.806224
2017-12-10T03:57:48.859470: step 6727, loss 0.403172, acc 0.953125, prec 0.0989464, recall 0.806304
2017-12-10T03:57:49.124409: step 6728, loss 0.391717, acc 0.90625, prec 0.0989503, recall 0.806325
2017-12-10T03:57:49.394081: step 6729, loss 0.43603, acc 0.921875, prec 0.0989784, recall 0.806385
2017-12-10T03:57:49.664597: step 6730, loss 0.140881, acc 0.96875, prec 0.0989988, recall 0.806425
2017-12-10T03:57:49.943173: step 6731, loss 0.663404, acc 0.859375, prec 0.0990104, recall 0.806465
2017-12-10T03:57:50.210935: step 6732, loss 0.273749, acc 0.96875, prec 0.0990193, recall 0.806485
2017-12-10T03:57:50.478562: step 6733, loss 0.405709, acc 0.9375, prec 0.0990143, recall 0.806485
2017-12-10T03:57:50.750767: step 6734, loss 0.456424, acc 0.9375, prec 0.0990207, recall 0.806505
2017-12-10T03:57:51.016244: step 6735, loss 0.13437, acc 0.953125, prec 0.0990169, recall 0.806505
2017-12-10T03:57:51.281299: step 6736, loss 0.251707, acc 0.921875, prec 0.0990336, recall 0.806545
2017-12-10T03:57:51.546261: step 6737, loss 0.234964, acc 0.953125, prec 0.0990527, recall 0.806585
2017-12-10T03:57:51.811304: step 6738, loss 0.842339, acc 0.921875, prec 0.0990693, recall 0.806625
2017-12-10T03:57:52.074932: step 6739, loss 0.304432, acc 0.96875, prec 0.0990897, recall 0.806665
2017-12-10T03:57:52.351516: step 6740, loss 0.125117, acc 0.96875, prec 0.0990872, recall 0.806665
2017-12-10T03:57:52.618394: step 6741, loss 0.15487, acc 0.96875, prec 0.0990961, recall 0.806685
2017-12-10T03:57:52.882533: step 6742, loss 0.0242786, acc 0.984375, prec 0.0991063, recall 0.806705
2017-12-10T03:57:53.155633: step 6743, loss 0.00944266, acc 1, prec 0.0991063, recall 0.806705
2017-12-10T03:57:53.425256: step 6744, loss 0.15268, acc 0.96875, prec 0.0991038, recall 0.806705
2017-12-10T03:57:53.699931: step 6745, loss 0.101381, acc 0.96875, prec 0.0991013, recall 0.806705
2017-12-10T03:57:53.966882: step 6746, loss 0.204363, acc 0.953125, prec 0.0991089, recall 0.806725
2017-12-10T03:57:54.233978: step 6747, loss 0.376308, acc 0.984375, prec 0.0991077, recall 0.806725
2017-12-10T03:57:54.497846: step 6748, loss 0.00180684, acc 1, prec 0.0991191, recall 0.806745
2017-12-10T03:57:54.762015: step 6749, loss 0.1807, acc 0.96875, prec 0.0991166, recall 0.806745
2017-12-10T03:57:55.027058: step 6750, loss 0.0010561, acc 1, prec 0.0991166, recall 0.806745
2017-12-10T03:57:55.291370: step 6751, loss 0.0168768, acc 0.984375, prec 0.0991268, recall 0.806765
2017-12-10T03:57:55.558263: step 6752, loss 0.669956, acc 0.984375, prec 0.099137, recall 0.806785
2017-12-10T03:57:55.828195: step 6753, loss 0.251392, acc 0.9375, prec 0.0991434, recall 0.806805
2017-12-10T03:57:56.100686: step 6754, loss 0.405408, acc 0.953125, prec 0.0991511, recall 0.806825
2017-12-10T03:57:56.363499: step 6755, loss 0.200639, acc 0.96875, prec 0.0991486, recall 0.806825
2017-12-10T03:57:56.630506: step 6756, loss 0.683871, acc 0.953125, prec 0.0991448, recall 0.806825
2017-12-10T03:57:56.907421: step 6757, loss 0.111169, acc 0.96875, prec 0.0991537, recall 0.806845
2017-12-10T03:57:57.174282: step 6758, loss 1.15691, acc 0.96875, prec 0.0991524, recall 0.806762
2017-12-10T03:57:57.444606: step 6759, loss 0.150724, acc 0.984375, prec 0.0991741, recall 0.806802
2017-12-10T03:57:57.708977: step 6760, loss 0.0775517, acc 0.984375, prec 0.0991728, recall 0.806802
2017-12-10T03:57:57.972734: step 6761, loss 0.0532237, acc 0.984375, prec 0.0991945, recall 0.806842
2017-12-10T03:57:58.237128: step 6762, loss 0.024299, acc 0.984375, prec 0.0991932, recall 0.806842
2017-12-10T03:57:58.502087: step 6763, loss 0.00722876, acc 1, prec 0.0991932, recall 0.806842
2017-12-10T03:57:58.769416: step 6764, loss 0.0191373, acc 0.984375, prec 0.0992148, recall 0.806882
2017-12-10T03:57:59.038409: step 6765, loss 0.0346588, acc 0.984375, prec 0.099225, recall 0.806902
2017-12-10T03:57:59.307466: step 6766, loss 0.0959913, acc 0.984375, prec 0.0992237, recall 0.806902
2017-12-10T03:57:59.572046: step 6767, loss 0.0316275, acc 0.984375, prec 0.0992225, recall 0.806902
2017-12-10T03:57:59.844216: step 6768, loss 0.0903069, acc 0.96875, prec 0.0992314, recall 0.806921
2017-12-10T03:58:00.118843: step 6769, loss 0.239116, acc 0.9375, prec 0.0992378, recall 0.806941
2017-12-10T03:58:00.402924: step 6770, loss 0.0991131, acc 0.96875, prec 0.0992353, recall 0.806941
2017-12-10T03:58:00.674918: step 6771, loss 0.0503339, acc 0.984375, prec 0.099234, recall 0.806941
2017-12-10T03:58:00.934228: step 6772, loss 0.141025, acc 0.953125, prec 0.0992302, recall 0.806941
2017-12-10T03:58:01.198830: step 6773, loss 0.544299, acc 0.90625, prec 0.0992456, recall 0.806981
2017-12-10T03:58:01.467749: step 6774, loss 0.471014, acc 0.953125, prec 0.0992418, recall 0.806981
2017-12-10T03:58:01.733848: step 6775, loss 0.119669, acc 0.984375, prec 0.099252, recall 0.807001
2017-12-10T03:58:02.000285: step 6776, loss 0.311769, acc 0.96875, prec 0.0992609, recall 0.807021
2017-12-10T03:58:02.262745: step 6777, loss 0.00815797, acc 1, prec 0.0992609, recall 0.807021
2017-12-10T03:58:02.530766: step 6778, loss 0.117973, acc 0.984375, prec 0.0992825, recall 0.807061
2017-12-10T03:58:02.797573: step 6779, loss 0.687466, acc 0.953125, prec 0.0992901, recall 0.807081
2017-12-10T03:58:03.058578: step 6780, loss 0.0248821, acc 1, prec 0.0993016, recall 0.807101
2017-12-10T03:58:03.318164: step 6781, loss 0.547869, acc 0.9375, prec 0.0992965, recall 0.807101
2017-12-10T03:58:03.587406: step 6782, loss 3.59257, acc 0.96875, prec 0.0992953, recall 0.807018
2017-12-10T03:58:03.858799: step 6783, loss 0.822633, acc 0.96875, prec 0.0992928, recall 0.807018
2017-12-10T03:58:04.126032: step 6784, loss 0.437139, acc 0.953125, prec 0.0993004, recall 0.807037
2017-12-10T03:58:04.391870: step 6785, loss 0.825484, acc 0.921875, prec 0.0993055, recall 0.807057
2017-12-10T03:58:04.660327: step 6786, loss 0.00810168, acc 1, prec 0.099317, recall 0.807077
2017-12-10T03:58:04.938460: step 6787, loss 0.358927, acc 0.953125, prec 0.0993246, recall 0.807097
2017-12-10T03:58:05.204611: step 6788, loss 0.0440601, acc 0.984375, prec 0.0993348, recall 0.807117
2017-12-10T03:58:05.471844: step 6789, loss 0.310413, acc 0.9375, prec 0.0993298, recall 0.807117
2017-12-10T03:58:05.738227: step 6790, loss 0.0823639, acc 0.953125, prec 0.099326, recall 0.807117
2017-12-10T03:58:06.002802: step 6791, loss 0.760052, acc 0.921875, prec 0.0993425, recall 0.807157
2017-12-10T03:58:06.269535: step 6792, loss 0.0691752, acc 0.96875, prec 0.0993515, recall 0.807177
2017-12-10T03:58:06.537664: step 6793, loss 0.380412, acc 0.953125, prec 0.0993477, recall 0.807177
2017-12-10T03:58:06.814153: step 6794, loss 0.314401, acc 0.921875, prec 0.0993871, recall 0.807256
2017-12-10T03:58:07.080826: step 6795, loss 0.0877943, acc 0.96875, prec 0.099396, recall 0.807276
2017-12-10T03:58:07.349586: step 6796, loss 0.507604, acc 0.90625, prec 0.0993998, recall 0.807296
2017-12-10T03:58:07.626188: step 6797, loss 0.0316518, acc 0.984375, prec 0.09941, recall 0.807316
2017-12-10T03:58:07.896821: step 6798, loss 0.343768, acc 0.953125, prec 0.0994519, recall 0.807395
2017-12-10T03:58:08.172029: step 6799, loss 0.386176, acc 0.9375, prec 0.0994583, recall 0.807415
2017-12-10T03:58:08.444392: step 6800, loss 0.165496, acc 0.96875, prec 0.0994558, recall 0.807415
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-6800

2017-12-10T03:58:10.111871: step 6801, loss 0.188877, acc 0.96875, prec 0.0994647, recall 0.807435
2017-12-10T03:58:10.377891: step 6802, loss 0.359526, acc 0.953125, prec 0.0994723, recall 0.807455
2017-12-10T03:58:10.644996: step 6803, loss 0.422895, acc 0.9375, prec 0.0994673, recall 0.807455
2017-12-10T03:58:10.922171: step 6804, loss 0.101426, acc 0.953125, prec 0.0994635, recall 0.807455
2017-12-10T03:58:11.190828: step 6805, loss 0.315006, acc 0.984375, prec 0.0994851, recall 0.807494
2017-12-10T03:58:11.457397: step 6806, loss 0.37432, acc 0.90625, prec 0.0994775, recall 0.807494
2017-12-10T03:58:11.726095: step 6807, loss 0.533104, acc 0.90625, prec 0.0994699, recall 0.807494
2017-12-10T03:58:11.999700: step 6808, loss 0.144531, acc 0.984375, prec 0.0994801, recall 0.807514
2017-12-10T03:58:12.271480: step 6809, loss 0.0281326, acc 1, prec 0.0994915, recall 0.807534
2017-12-10T03:58:12.541948: step 6810, loss 0.000212371, acc 1, prec 0.0995029, recall 0.807554
2017-12-10T03:58:12.799494: step 6811, loss 0.0553549, acc 0.96875, prec 0.0995004, recall 0.807554
2017-12-10T03:58:13.063174: step 6812, loss 0.167991, acc 0.9375, prec 0.0994954, recall 0.807554
2017-12-10T03:58:13.329090: step 6813, loss 0.0674631, acc 0.984375, prec 0.0995055, recall 0.807574
2017-12-10T03:58:13.600824: step 6814, loss 0.0180594, acc 0.984375, prec 0.0995157, recall 0.807593
2017-12-10T03:58:13.864114: step 6815, loss 4.02247, acc 0.984375, prec 0.0995385, recall 0.80755
2017-12-10T03:58:14.133575: step 6816, loss 0.0524471, acc 0.984375, prec 0.0995487, recall 0.80757
2017-12-10T03:58:14.397637: step 6817, loss 0.0164097, acc 0.984375, prec 0.0995474, recall 0.80757
2017-12-10T03:58:14.670337: step 6818, loss 0.217447, acc 0.984375, prec 0.0995461, recall 0.80757
2017-12-10T03:58:14.943564: step 6819, loss 0.131819, acc 0.953125, prec 0.0995766, recall 0.807629
2017-12-10T03:58:15.208957: step 6820, loss 0.81587, acc 0.9375, prec 0.0995715, recall 0.807629
2017-12-10T03:58:15.472618: step 6821, loss 0.147579, acc 0.984375, prec 0.0995703, recall 0.807629
2017-12-10T03:58:15.746515: step 6822, loss 0.347848, acc 0.953125, prec 0.0995665, recall 0.807629
2017-12-10T03:58:16.013031: step 6823, loss 0.412195, acc 0.890625, prec 0.0995691, recall 0.807649
2017-12-10T03:58:16.277259: step 6824, loss 0.388575, acc 0.9375, prec 0.099564, recall 0.807649
2017-12-10T03:58:16.541570: step 6825, loss 0.235514, acc 0.953125, prec 0.0995717, recall 0.807669
2017-12-10T03:58:16.807481: step 6826, loss 0.657019, acc 0.953125, prec 0.0995679, recall 0.807669
2017-12-10T03:58:17.080277: step 6827, loss 0.468832, acc 0.921875, prec 0.099573, recall 0.807688
2017-12-10T03:58:17.344592: step 6828, loss 0.537467, acc 0.921875, prec 0.0995895, recall 0.807728
2017-12-10T03:58:17.608384: step 6829, loss 0.249947, acc 0.984375, prec 0.0995882, recall 0.807728
2017-12-10T03:58:17.875093: step 6830, loss 0.00891668, acc 1, prec 0.0995996, recall 0.807748
2017-12-10T03:58:18.138673: step 6831, loss 0.0340343, acc 0.96875, prec 0.0996085, recall 0.807767
2017-12-10T03:58:18.403746: step 6832, loss 0.0963088, acc 0.96875, prec 0.0996288, recall 0.807807
2017-12-10T03:58:18.671553: step 6833, loss 0.0147097, acc 1, prec 0.0996402, recall 0.807827
2017-12-10T03:58:18.935328: step 6834, loss 0.0906741, acc 0.953125, prec 0.0996364, recall 0.807827
2017-12-10T03:58:19.199678: step 6835, loss 0.314735, acc 0.9375, prec 0.0996314, recall 0.807827
2017-12-10T03:58:19.469538: step 6836, loss 0.610287, acc 0.9375, prec 0.0996491, recall 0.807866
2017-12-10T03:58:19.741736: step 6837, loss 0.0154018, acc 0.984375, prec 0.0996593, recall 0.807886
2017-12-10T03:58:20.007184: step 6838, loss 0.363404, acc 0.953125, prec 0.0996555, recall 0.807886
2017-12-10T03:58:20.272938: step 6839, loss 0.323872, acc 0.953125, prec 0.0996745, recall 0.807925
2017-12-10T03:58:20.539445: step 6840, loss 0.271281, acc 0.921875, prec 0.0996796, recall 0.807945
2017-12-10T03:58:20.811938: step 6841, loss 0.315989, acc 0.984375, prec 0.0997011, recall 0.807984
2017-12-10T03:58:21.079749: step 6842, loss 0.426131, acc 0.9375, prec 0.0996961, recall 0.807984
2017-12-10T03:58:21.343546: step 6843, loss 0.00981445, acc 1, prec 0.0997303, recall 0.808043
2017-12-10T03:58:21.607953: step 6844, loss 0.216344, acc 0.96875, prec 0.0997392, recall 0.808063
2017-12-10T03:58:21.876489: step 6845, loss 0.392159, acc 0.953125, prec 0.0997354, recall 0.808063
2017-12-10T03:58:22.148055: step 6846, loss 0.0483483, acc 0.984375, prec 0.0997569, recall 0.808103
2017-12-10T03:58:22.407547: step 6847, loss 0.230847, acc 0.953125, prec 0.0997645, recall 0.808122
2017-12-10T03:58:22.674918: step 6848, loss 0.96792, acc 0.953125, prec 0.0997835, recall 0.808162
2017-12-10T03:58:22.948591: step 6849, loss 0.284168, acc 0.953125, prec 0.0997911, recall 0.808181
2017-12-10T03:58:23.209467: step 6850, loss 0.528016, acc 0.9375, prec 0.0997975, recall 0.808201
2017-12-10T03:58:23.481515: step 6851, loss 0.108802, acc 0.96875, prec 0.0998291, recall 0.80826
2017-12-10T03:58:23.745009: step 6852, loss 0.00550334, acc 1, prec 0.0998405, recall 0.80828
2017-12-10T03:58:24.008398: step 6853, loss 0.483298, acc 0.9375, prec 0.099881, recall 0.808358
2017-12-10T03:58:24.279728: step 6854, loss 0.646793, acc 0.96875, prec 0.0998899, recall 0.808378
2017-12-10T03:58:24.550810: step 6855, loss 0.00693052, acc 1, prec 0.0998899, recall 0.808378
2017-12-10T03:58:24.813871: step 6856, loss 0.117453, acc 0.984375, prec 0.0999, recall 0.808397
2017-12-10T03:58:25.080119: step 6857, loss 0.0490047, acc 0.96875, prec 0.0998975, recall 0.808397
2017-12-10T03:58:25.342069: step 6858, loss 0.323255, acc 0.984375, prec 0.0998962, recall 0.808397
2017-12-10T03:58:25.605846: step 6859, loss 1.25286, acc 0.953125, prec 0.0999152, recall 0.808437
2017-12-10T03:58:25.876338: step 6860, loss 0.54046, acc 0.96875, prec 0.0999355, recall 0.808476
2017-12-10T03:58:26.148831: step 6861, loss 0.126254, acc 0.96875, prec 0.0999557, recall 0.808515
2017-12-10T03:58:26.415679: step 6862, loss 0.286809, acc 0.921875, prec 0.0999494, recall 0.808515
2017-12-10T03:58:26.686490: step 6863, loss 0.135561, acc 0.96875, prec 0.099981, recall 0.808574
2017-12-10T03:58:26.966984: step 6864, loss 0.903481, acc 0.953125, prec 0.0999772, recall 0.808574
2017-12-10T03:58:27.238291: step 6865, loss 0.005804, acc 1, prec 0.100011, recall 0.808632
2017-12-10T03:58:27.497597: step 6866, loss 0.70609, acc 0.828125, prec 0.100009, recall 0.808652
2017-12-10T03:58:27.763888: step 6867, loss 0.928261, acc 0.9375, prec 0.100004, recall 0.808652
2017-12-10T03:58:28.027461: step 6868, loss 0.225729, acc 0.9375, prec 0.0999987, recall 0.808652
2017-12-10T03:58:28.290972: step 6869, loss 0.496111, acc 0.90625, prec 0.100003, recall 0.808672
2017-12-10T03:58:28.552397: step 6870, loss 0.901548, acc 0.90625, prec 0.0999949, recall 0.808672
2017-12-10T03:58:28.818443: step 6871, loss 0.536534, acc 0.9375, prec 0.100013, recall 0.808711
2017-12-10T03:58:29.084153: step 6872, loss 0.363191, acc 0.9375, prec 0.100019, recall 0.80873
2017-12-10T03:58:29.347729: step 6873, loss 0.183897, acc 0.984375, prec 0.100018, recall 0.80873
2017-12-10T03:58:29.618993: step 6874, loss 0.370802, acc 0.96875, prec 0.100015, recall 0.80873
2017-12-10T03:58:29.884420: step 6875, loss 0.351332, acc 0.953125, prec 0.100023, recall 0.80875
2017-12-10T03:58:30.157608: step 6876, loss 0.41184, acc 0.953125, prec 0.10003, recall 0.808769
2017-12-10T03:58:30.444487: step 6877, loss 0.292623, acc 0.9375, prec 0.100037, recall 0.808789
2017-12-10T03:58:30.716675: step 6878, loss 0.352907, acc 0.984375, prec 0.100058, recall 0.808828
2017-12-10T03:58:30.985479: step 6879, loss 0.411834, acc 0.890625, prec 0.100049, recall 0.808828
2017-12-10T03:58:31.252012: step 6880, loss 0.24779, acc 0.9375, prec 0.100056, recall 0.808848
2017-12-10T03:58:31.516087: step 6881, loss 0.440377, acc 0.96875, prec 0.100087, recall 0.808906
2017-12-10T03:58:31.779317: step 6882, loss 0.2109, acc 0.953125, prec 0.100095, recall 0.808926
2017-12-10T03:58:32.047620: step 6883, loss 0.193872, acc 0.96875, prec 0.100104, recall 0.808945
2017-12-10T03:58:32.317244: step 6884, loss 0.0120251, acc 1, prec 0.100126, recall 0.808984
2017-12-10T03:58:32.590221: step 6885, loss 0.251536, acc 0.953125, prec 0.100123, recall 0.808984
2017-12-10T03:58:32.861784: step 6886, loss 0.00592819, acc 1, prec 0.100123, recall 0.808984
2017-12-10T03:58:33.128570: step 6887, loss 0.227083, acc 0.953125, prec 0.100119, recall 0.808984
2017-12-10T03:58:33.396523: step 6888, loss 0.293236, acc 0.984375, prec 0.10014, recall 0.809023
2017-12-10T03:58:33.669236: step 6889, loss 0.0142156, acc 0.984375, prec 0.100139, recall 0.809023
2017-12-10T03:58:33.936606: step 6890, loss 0.000639837, acc 1, prec 0.100173, recall 0.809082
2017-12-10T03:58:34.203696: step 6891, loss 0.0326498, acc 0.984375, prec 0.100195, recall 0.809121
2017-12-10T03:58:34.485908: step 6892, loss 0.0960427, acc 0.96875, prec 0.100215, recall 0.80916
2017-12-10T03:58:34.764935: step 6893, loss 0.00787265, acc 1, prec 0.100215, recall 0.80916
2017-12-10T03:58:35.034082: step 6894, loss 0.000170072, acc 1, prec 0.100226, recall 0.809179
2017-12-10T03:58:35.300522: step 6895, loss 0.0320827, acc 0.984375, prec 0.100236, recall 0.809198
2017-12-10T03:58:35.574617: step 6896, loss 0.0996414, acc 0.984375, prec 0.100246, recall 0.809218
2017-12-10T03:58:35.843016: step 6897, loss 0.0136401, acc 0.984375, prec 0.100245, recall 0.809218
2017-12-10T03:58:36.112135: step 6898, loss 0.000516468, acc 1, prec 0.100245, recall 0.809218
2017-12-10T03:58:36.377783: step 6899, loss 0.0244376, acc 0.984375, prec 0.100244, recall 0.809218
2017-12-10T03:58:36.646244: step 6900, loss 2.53085e-05, acc 1, prec 0.100244, recall 0.809218

Evaluation:
2017-12-10T03:58:44.166237: step 6900, loss 12.0201, acc 0.975469, prec 0.10058, recall 0.801667

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-6900

2017-12-10T03:58:45.508987: step 6901, loss 6.30063, acc 0.96875, prec 0.10059, recall 0.801606
2017-12-10T03:58:45.779223: step 6902, loss 0.000219026, acc 1, prec 0.10059, recall 0.801606
2017-12-10T03:58:46.038983: step 6903, loss 0.168627, acc 0.953125, prec 0.100597, recall 0.801626
2017-12-10T03:58:46.307115: step 6904, loss 0.110318, acc 0.984375, prec 0.100596, recall 0.801626
2017-12-10T03:58:46.572343: step 6905, loss 0.0554322, acc 0.96875, prec 0.100605, recall 0.801646
2017-12-10T03:58:46.842369: step 6906, loss 0.731345, acc 0.953125, prec 0.100601, recall 0.801646
2017-12-10T03:58:47.119558: step 6907, loss 0.179051, acc 0.96875, prec 0.10061, recall 0.801666
2017-12-10T03:58:47.387877: step 6908, loss 0.0788445, acc 0.984375, prec 0.100608, recall 0.801666
2017-12-10T03:58:47.656794: step 6909, loss 0.220708, acc 0.96875, prec 0.100606, recall 0.801666
2017-12-10T03:58:47.925540: step 6910, loss 0.176895, acc 0.96875, prec 0.100603, recall 0.801666
2017-12-10T03:58:48.193564: step 6911, loss 0.374907, acc 0.9375, prec 0.100598, recall 0.801666
2017-12-10T03:58:48.466035: step 6912, loss 0.173787, acc 0.953125, prec 0.100594, recall 0.801666
2017-12-10T03:58:48.733195: step 6913, loss 0.212679, acc 0.9375, prec 0.100601, recall 0.801686
2017-12-10T03:58:48.999127: step 6914, loss 0.144202, acc 0.953125, prec 0.100608, recall 0.801706
2017-12-10T03:58:49.270459: step 6915, loss 0.412795, acc 0.953125, prec 0.100604, recall 0.801706
2017-12-10T03:58:49.542357: step 6916, loss 0.170799, acc 0.96875, prec 0.100625, recall 0.801746
2017-12-10T03:58:49.813932: step 6917, loss 0.0528859, acc 0.984375, prec 0.100623, recall 0.801746
2017-12-10T03:58:50.090385: step 6918, loss 0.0991243, acc 0.984375, prec 0.100633, recall 0.801766
2017-12-10T03:58:50.358724: step 6919, loss 0.278012, acc 0.9375, prec 0.10064, recall 0.801786
2017-12-10T03:58:50.631474: step 6920, loss 0.251301, acc 0.984375, prec 0.100661, recall 0.801825
2017-12-10T03:58:50.904582: step 6921, loss 0.0252572, acc 0.984375, prec 0.100671, recall 0.801845
2017-12-10T03:58:51.172740: step 6922, loss 0.251174, acc 0.953125, prec 0.10069, recall 0.801885
2017-12-10T03:58:51.436465: step 6923, loss 0.155149, acc 0.953125, prec 0.100686, recall 0.801885
2017-12-10T03:58:51.706922: step 6924, loss 0.302093, acc 0.96875, prec 0.100695, recall 0.801905
2017-12-10T03:58:51.981324: step 6925, loss 0.158186, acc 0.9375, prec 0.10069, recall 0.801905
2017-12-10T03:58:52.256274: step 6926, loss 6.20243, acc 0.953125, prec 0.100687, recall 0.801824
2017-12-10T03:58:52.528262: step 6927, loss 0.0880558, acc 0.96875, prec 0.100685, recall 0.801824
2017-12-10T03:58:52.799782: step 6928, loss 0.220248, acc 0.921875, prec 0.10069, recall 0.801844
2017-12-10T03:58:53.067330: step 6929, loss 0.260226, acc 0.953125, prec 0.100697, recall 0.801864
2017-12-10T03:58:53.335910: step 6930, loss 0.416703, acc 0.96875, prec 0.100706, recall 0.801884
2017-12-10T03:58:53.601586: step 6931, loss 0.593352, acc 0.9375, prec 0.100701, recall 0.801884
2017-12-10T03:58:53.874633: step 6932, loss 0.217828, acc 0.921875, prec 0.100706, recall 0.801904
2017-12-10T03:58:54.146018: step 6933, loss 1.04414, acc 0.875, prec 0.100707, recall 0.801924
2017-12-10T03:58:54.412770: step 6934, loss 0.616743, acc 0.890625, prec 0.10071, recall 0.801943
2017-12-10T03:58:54.679843: step 6935, loss 0.791193, acc 0.90625, prec 0.100702, recall 0.801943
2017-12-10T03:58:54.950555: step 6936, loss 0.463045, acc 0.953125, prec 0.100698, recall 0.801943
2017-12-10T03:58:55.217869: step 6937, loss 0.544157, acc 0.890625, prec 0.100701, recall 0.801963
2017-12-10T03:58:55.481575: step 6938, loss 0.923742, acc 0.921875, prec 0.100728, recall 0.802023
2017-12-10T03:58:55.752963: step 6939, loss 0.237499, acc 0.90625, prec 0.100732, recall 0.802043
2017-12-10T03:58:56.028500: step 6940, loss 0.472694, acc 0.875, prec 0.100744, recall 0.802082
2017-12-10T03:58:56.289052: step 6941, loss 0.635719, acc 0.890625, prec 0.100747, recall 0.802102
2017-12-10T03:58:56.555914: step 6942, loss 0.0440412, acc 0.96875, prec 0.100744, recall 0.802102
2017-12-10T03:58:56.837087: step 6943, loss 0.0265953, acc 0.984375, prec 0.100754, recall 0.802122
2017-12-10T03:58:57.101972: step 6944, loss 0.269188, acc 0.96875, prec 0.100763, recall 0.802142
2017-12-10T03:58:57.383457: step 6945, loss 0.383466, acc 0.921875, prec 0.100768, recall 0.802162
2017-12-10T03:58:57.647601: step 6946, loss 0.485992, acc 0.875, prec 0.100792, recall 0.802221
2017-12-10T03:58:57.921481: step 6947, loss 0.18611, acc 0.953125, prec 0.100788, recall 0.802221
2017-12-10T03:58:58.186623: step 6948, loss 0.00248564, acc 1, prec 0.100822, recall 0.80228
2017-12-10T03:58:58.452917: step 6949, loss 0.0843615, acc 0.96875, prec 0.100831, recall 0.8023
2017-12-10T03:58:58.720291: step 6950, loss 0.129674, acc 0.96875, prec 0.100828, recall 0.8023
2017-12-10T03:58:58.987834: step 6951, loss 0.0739617, acc 0.953125, prec 0.100836, recall 0.80232
2017-12-10T03:58:59.256474: step 6952, loss 1.11653, acc 0.984375, prec 0.100857, recall 0.802359
2017-12-10T03:58:59.524029: step 6953, loss 0.00428917, acc 1, prec 0.10088, recall 0.802399
2017-12-10T03:58:59.786360: step 6954, loss 0.38442, acc 0.953125, prec 0.100887, recall 0.802419
2017-12-10T03:59:00.062235: step 6955, loss 0.232307, acc 0.984375, prec 0.10092, recall 0.802478
2017-12-10T03:59:00.337521: step 6956, loss 0.226787, acc 0.953125, prec 0.100939, recall 0.802517
2017-12-10T03:59:00.609420: step 6957, loss 0.105886, acc 0.984375, prec 0.100937, recall 0.802517
2017-12-10T03:59:00.840861: step 6958, loss 0.136404, acc 0.941176, prec 0.100933, recall 0.802517
2017-12-10T03:59:01.117888: step 6959, loss 0.000856797, acc 1, prec 0.100956, recall 0.802557
2017-12-10T03:59:01.381921: step 6960, loss 0.105986, acc 0.96875, prec 0.100965, recall 0.802576
2017-12-10T03:59:01.644653: step 6961, loss 0.274008, acc 0.9375, prec 0.100971, recall 0.802596
2017-12-10T03:59:01.915865: step 6962, loss 0.000367436, acc 1, prec 0.100971, recall 0.802596
2017-12-10T03:59:02.179281: step 6963, loss 0.0798474, acc 0.984375, prec 0.10097, recall 0.802596
2017-12-10T03:59:02.453961: step 6964, loss 0.254855, acc 0.96875, prec 0.100979, recall 0.802616
2017-12-10T03:59:02.718481: step 6965, loss 1.77359, acc 0.953125, prec 0.100999, recall 0.802575
2017-12-10T03:59:02.987025: step 6966, loss 2.50269, acc 0.984375, prec 0.101021, recall 0.802534
2017-12-10T03:59:03.263386: step 6967, loss 0.0775848, acc 0.96875, prec 0.10103, recall 0.802554
2017-12-10T03:59:03.529375: step 6968, loss 0.0061038, acc 1, prec 0.101041, recall 0.802574
2017-12-10T03:59:03.804773: step 6969, loss 0.28734, acc 0.96875, prec 0.101039, recall 0.802574
2017-12-10T03:59:04.071061: step 6970, loss 0.117618, acc 0.984375, prec 0.101037, recall 0.802574
2017-12-10T03:59:04.342915: step 6971, loss 0.121841, acc 0.984375, prec 0.101036, recall 0.802574
2017-12-10T03:59:04.612731: step 6972, loss 0.21289, acc 0.96875, prec 0.101056, recall 0.802613
2017-12-10T03:59:04.880156: step 6973, loss 0.0350242, acc 0.984375, prec 0.101089, recall 0.802672
2017-12-10T03:59:05.146525: step 6974, loss 0.448759, acc 0.890625, prec 0.10108, recall 0.802672
2017-12-10T03:59:05.425808: step 6975, loss 0.23444, acc 0.96875, prec 0.101077, recall 0.802672
2017-12-10T03:59:05.692272: step 6976, loss 0.421989, acc 0.9375, prec 0.101095, recall 0.802712
2017-12-10T03:59:05.958005: step 6977, loss 0.634874, acc 0.9375, prec 0.101101, recall 0.802731
2017-12-10T03:59:06.221961: step 6978, loss 0.939756, acc 0.828125, prec 0.101098, recall 0.802751
2017-12-10T03:59:06.486116: step 6979, loss 0.190036, acc 0.9375, prec 0.101093, recall 0.802751
2017-12-10T03:59:06.752768: step 6980, loss 0.0328283, acc 0.984375, prec 0.101103, recall 0.802771
2017-12-10T03:59:07.017685: step 6981, loss 0.957342, acc 0.828125, prec 0.101101, recall 0.80279
2017-12-10T03:59:07.281354: step 6982, loss 0.386735, acc 0.859375, prec 0.1011, recall 0.80281
2017-12-10T03:59:07.548419: step 6983, loss 0.646117, acc 0.9375, prec 0.101095, recall 0.80281
2017-12-10T03:59:07.817864: step 6984, loss 0.818764, acc 0.90625, prec 0.101088, recall 0.80281
2017-12-10T03:59:08.083588: step 6985, loss 0.716301, acc 0.90625, prec 0.101091, recall 0.80283
2017-12-10T03:59:08.351970: step 6986, loss 0.342765, acc 0.9375, prec 0.101109, recall 0.802869
2017-12-10T03:59:08.618270: step 6987, loss 0.277949, acc 0.90625, prec 0.101113, recall 0.802888
2017-12-10T03:59:08.895074: step 6988, loss 0.481022, acc 0.890625, prec 0.101115, recall 0.802908
2017-12-10T03:59:09.161256: step 6989, loss 0.53312, acc 0.90625, prec 0.101107, recall 0.802908
2017-12-10T03:59:09.431338: step 6990, loss 0.597434, acc 0.921875, prec 0.101112, recall 0.802928
2017-12-10T03:59:09.698784: step 6991, loss 0.0654031, acc 0.96875, prec 0.101132, recall 0.802967
2017-12-10T03:59:09.966762: step 6992, loss 1.37849, acc 0.921875, prec 0.101149, recall 0.803006
2017-12-10T03:59:10.235103: step 6993, loss 0.128423, acc 0.953125, prec 0.101156, recall 0.803026
2017-12-10T03:59:10.499807: step 6994, loss 0.211563, acc 0.921875, prec 0.10115, recall 0.803026
2017-12-10T03:59:10.764745: step 6995, loss 0.367304, acc 0.96875, prec 0.101158, recall 0.803045
2017-12-10T03:59:11.030269: step 6996, loss 0.0323233, acc 0.984375, prec 0.101168, recall 0.803065
2017-12-10T03:59:11.292889: step 6997, loss 0.00438534, acc 1, prec 0.101191, recall 0.803104
2017-12-10T03:59:11.553040: step 6998, loss 0.10356, acc 0.953125, prec 0.101198, recall 0.803124
2017-12-10T03:59:11.832271: step 6999, loss 0.0350687, acc 0.984375, prec 0.101208, recall 0.803143
2017-12-10T03:59:12.101886: step 7000, loss 0.0569603, acc 0.984375, prec 0.101218, recall 0.803163
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-7000

2017-12-10T03:59:13.360270: step 7001, loss 0.24221, acc 0.96875, prec 0.10125, recall 0.803222
2017-12-10T03:59:13.626539: step 7002, loss 0.0199316, acc 0.984375, prec 0.101248, recall 0.803222
2017-12-10T03:59:13.897180: step 7003, loss 0.0818866, acc 1, prec 0.10126, recall 0.803241
2017-12-10T03:59:14.161849: step 7004, loss 0.0175498, acc 1, prec 0.101271, recall 0.803261
2017-12-10T03:59:14.428618: step 7005, loss 0.490166, acc 0.984375, prec 0.101292, recall 0.8033
2017-12-10T03:59:14.690859: step 7006, loss 0.1403, acc 0.96875, prec 0.101301, recall 0.803319
2017-12-10T03:59:14.957336: step 7007, loss 0.00881312, acc 1, prec 0.101312, recall 0.803339
2017-12-10T03:59:15.220434: step 7008, loss 0.132983, acc 0.984375, prec 0.101311, recall 0.803339
2017-12-10T03:59:15.486983: step 7009, loss 0.0699257, acc 0.984375, prec 0.10131, recall 0.803339
2017-12-10T03:59:15.753725: step 7010, loss 0.0151188, acc 1, prec 0.101332, recall 0.803378
2017-12-10T03:59:16.016469: step 7011, loss 0.100058, acc 0.984375, prec 0.101353, recall 0.803417
2017-12-10T03:59:16.291298: step 7012, loss 0.116363, acc 0.984375, prec 0.101363, recall 0.803437
2017-12-10T03:59:16.557495: step 7013, loss 0.220963, acc 0.953125, prec 0.101393, recall 0.803495
2017-12-10T03:59:16.832627: step 7014, loss 0.0206822, acc 0.984375, prec 0.101403, recall 0.803515
2017-12-10T03:59:17.108118: step 7015, loss 0.0059157, acc 1, prec 0.101415, recall 0.803534
2017-12-10T03:59:17.374915: step 7016, loss 0.00136395, acc 1, prec 0.101415, recall 0.803534
2017-12-10T03:59:17.637709: step 7017, loss 0.609181, acc 0.96875, prec 0.101423, recall 0.803554
2017-12-10T03:59:17.907153: step 7018, loss 0.00387339, acc 1, prec 0.101435, recall 0.803573
2017-12-10T03:59:18.173437: step 7019, loss 0.0796779, acc 0.984375, prec 0.101445, recall 0.803593
2017-12-10T03:59:18.437118: step 7020, loss 0.0990407, acc 0.984375, prec 0.101466, recall 0.803632
2017-12-10T03:59:18.702218: step 7021, loss 0.173393, acc 0.984375, prec 0.101476, recall 0.803651
2017-12-10T03:59:18.964767: step 7022, loss 0.223648, acc 0.984375, prec 0.101486, recall 0.803671
2017-12-10T03:59:19.231755: step 7023, loss 0.081986, acc 0.984375, prec 0.101484, recall 0.803671
2017-12-10T03:59:19.500296: step 7024, loss 0.173821, acc 0.96875, prec 0.101482, recall 0.803671
2017-12-10T03:59:19.772627: step 7025, loss 0.00499623, acc 1, prec 0.101516, recall 0.803729
2017-12-10T03:59:20.034625: step 7026, loss 0.000730879, acc 1, prec 0.101527, recall 0.803748
2017-12-10T03:59:20.303103: step 7027, loss 0.114823, acc 0.96875, prec 0.101524, recall 0.803748
2017-12-10T03:59:20.569176: step 7028, loss 0.0635156, acc 0.984375, prec 0.101568, recall 0.803826
2017-12-10T03:59:20.840495: step 7029, loss 0.131419, acc 0.96875, prec 0.101577, recall 0.803846
2017-12-10T03:59:21.107403: step 7030, loss 0.0962255, acc 0.984375, prec 0.101598, recall 0.803885
2017-12-10T03:59:21.379463: step 7031, loss 0.00145825, acc 1, prec 0.101609, recall 0.803904
2017-12-10T03:59:21.648258: step 7032, loss 0.00418018, acc 1, prec 0.101621, recall 0.803923
2017-12-10T03:59:21.909241: step 7033, loss 0.103326, acc 0.96875, prec 0.101618, recall 0.803923
2017-12-10T03:59:22.176482: step 7034, loss 0.30577, acc 0.984375, prec 0.101617, recall 0.803923
2017-12-10T03:59:22.443262: step 7035, loss 0.0349969, acc 0.96875, prec 0.101626, recall 0.803943
2017-12-10T03:59:22.704217: step 7036, loss 0.0294728, acc 0.984375, prec 0.101624, recall 0.803943
2017-12-10T03:59:22.970709: step 7037, loss 0.0361118, acc 0.984375, prec 0.101623, recall 0.803943
2017-12-10T03:59:23.234951: step 7038, loss 0.153607, acc 0.96875, prec 0.101643, recall 0.803982
2017-12-10T03:59:23.501073: step 7039, loss 0.000135592, acc 1, prec 0.101643, recall 0.803982
2017-12-10T03:59:23.777096: step 7040, loss 0.0347875, acc 0.984375, prec 0.101642, recall 0.803982
2017-12-10T03:59:24.042728: step 7041, loss 0.121355, acc 0.984375, prec 0.10164, recall 0.803982
2017-12-10T03:59:24.308709: step 7042, loss 0.00614217, acc 1, prec 0.101652, recall 0.804001
2017-12-10T03:59:24.581456: step 7043, loss 0.349707, acc 0.953125, prec 0.101648, recall 0.804001
2017-12-10T03:59:24.847588: step 7044, loss 0.154107, acc 0.96875, prec 0.101668, recall 0.80404
2017-12-10T03:59:25.109895: step 7045, loss 0.0419873, acc 0.984375, prec 0.101689, recall 0.804079
2017-12-10T03:59:25.378772: step 7046, loss 0.13972, acc 0.96875, prec 0.101686, recall 0.804079
2017-12-10T03:59:25.644608: step 7047, loss 0.000647133, acc 1, prec 0.101686, recall 0.804079
2017-12-10T03:59:25.916738: step 7048, loss 0.367693, acc 0.96875, prec 0.101684, recall 0.804079
2017-12-10T03:59:26.191788: step 7049, loss 0.0753469, acc 0.984375, prec 0.101694, recall 0.804098
2017-12-10T03:59:26.461488: step 7050, loss 0.0132504, acc 1, prec 0.101705, recall 0.804118
2017-12-10T03:59:26.731310: step 7051, loss 0.136028, acc 0.96875, prec 0.101703, recall 0.804118
2017-12-10T03:59:27.003047: step 7052, loss 0.865293, acc 0.984375, prec 0.101713, recall 0.804137
2017-12-10T03:59:27.276102: step 7053, loss 0.00515382, acc 1, prec 0.101735, recall 0.804176
2017-12-10T03:59:27.544299: step 7054, loss 0.044823, acc 0.984375, prec 0.101734, recall 0.804176
2017-12-10T03:59:27.814964: step 7055, loss 0.200043, acc 0.984375, prec 0.101732, recall 0.804176
2017-12-10T03:59:28.083470: step 7056, loss 0.000210033, acc 1, prec 0.101755, recall 0.804214
2017-12-10T03:59:28.347934: step 7057, loss 0.253033, acc 0.96875, prec 0.101764, recall 0.804234
2017-12-10T03:59:28.622738: step 7058, loss 0.0392135, acc 0.984375, prec 0.101762, recall 0.804234
2017-12-10T03:59:28.894174: step 7059, loss 9.55382e-05, acc 1, prec 0.101762, recall 0.804234
2017-12-10T03:59:29.163097: step 7060, loss 0.011078, acc 1, prec 0.101762, recall 0.804234
2017-12-10T03:59:29.433204: step 7061, loss 0.00341564, acc 1, prec 0.101762, recall 0.804234
2017-12-10T03:59:29.709708: step 7062, loss 0.00129352, acc 1, prec 0.101762, recall 0.804234
2017-12-10T03:59:29.974714: step 7063, loss 0.0160951, acc 1, prec 0.101785, recall 0.804273
2017-12-10T03:59:30.249523: step 7064, loss 0.208784, acc 0.96875, prec 0.101816, recall 0.804331
2017-12-10T03:59:30.523631: step 7065, loss 0.0458444, acc 0.984375, prec 0.101826, recall 0.80435
2017-12-10T03:59:30.796876: step 7066, loss 0.0687134, acc 0.96875, prec 0.101846, recall 0.804389
2017-12-10T03:59:31.064477: step 7067, loss 0.0365508, acc 0.984375, prec 0.101867, recall 0.804427
2017-12-10T03:59:31.342098: step 7068, loss 0.104734, acc 0.984375, prec 0.101877, recall 0.804447
2017-12-10T03:59:31.612678: step 7069, loss 0.105457, acc 0.984375, prec 0.101876, recall 0.804447
2017-12-10T03:59:31.880582: step 7070, loss 0.258287, acc 0.984375, prec 0.101875, recall 0.804447
2017-12-10T03:59:32.147251: step 7071, loss 0.0232857, acc 0.984375, prec 0.101885, recall 0.804466
2017-12-10T03:59:32.412065: step 7072, loss 0.214047, acc 0.9375, prec 0.101879, recall 0.804466
2017-12-10T03:59:32.678552: step 7073, loss 0.091095, acc 0.953125, prec 0.101876, recall 0.804466
2017-12-10T03:59:32.947729: step 7074, loss 0.121674, acc 0.96875, prec 0.101873, recall 0.804466
2017-12-10T03:59:33.210409: step 7075, loss 0.0634754, acc 0.96875, prec 0.101893, recall 0.804505
2017-12-10T03:59:33.480417: step 7076, loss 0.000414176, acc 1, prec 0.101893, recall 0.804505
2017-12-10T03:59:33.740621: step 7077, loss 0.0535535, acc 0.96875, prec 0.10189, recall 0.804505
2017-12-10T03:59:34.015925: step 7078, loss 0.00383727, acc 1, prec 0.10189, recall 0.804505
2017-12-10T03:59:34.279422: step 7079, loss 0.0086569, acc 1, prec 0.101902, recall 0.804524
2017-12-10T03:59:34.547369: step 7080, loss 0.0887923, acc 0.984375, prec 0.101923, recall 0.804563
2017-12-10T03:59:34.815668: step 7081, loss 0.00111557, acc 1, prec 0.101934, recall 0.804582
2017-12-10T03:59:35.080466: step 7082, loss 0.0965704, acc 0.984375, prec 0.101933, recall 0.804582
2017-12-10T03:59:35.345092: step 7083, loss 0.040808, acc 0.984375, prec 0.101943, recall 0.804601
2017-12-10T03:59:35.615049: step 7084, loss 0.0614269, acc 0.984375, prec 0.101942, recall 0.804601
2017-12-10T03:59:35.880324: step 7085, loss 0.0610537, acc 0.984375, prec 0.10194, recall 0.804601
2017-12-10T03:59:36.145623: step 7086, loss 0.000416841, acc 1, prec 0.10194, recall 0.804601
2017-12-10T03:59:36.415434: step 7087, loss 0.0115172, acc 0.984375, prec 0.101939, recall 0.804601
2017-12-10T03:59:36.684641: step 7088, loss 0.00411708, acc 1, prec 0.10195, recall 0.80462
2017-12-10T03:59:36.958325: step 7089, loss 0.00102625, acc 1, prec 0.10195, recall 0.80462
2017-12-10T03:59:37.226554: step 7090, loss 4.35078e-05, acc 1, prec 0.10195, recall 0.80462
2017-12-10T03:59:37.488845: step 7091, loss 6.87242, acc 0.953125, prec 0.101971, recall 0.8045
2017-12-10T03:59:37.756649: step 7092, loss 0.00161724, acc 1, prec 0.101971, recall 0.8045
2017-12-10T03:59:38.028557: step 7093, loss 0.129921, acc 0.96875, prec 0.101969, recall 0.8045
2017-12-10T03:59:38.295476: step 7094, loss 0.591063, acc 0.9375, prec 0.101964, recall 0.8045
2017-12-10T03:59:38.563522: step 7095, loss 0.413162, acc 0.9375, prec 0.101959, recall 0.8045
2017-12-10T03:59:38.830806: step 7096, loss 0.285357, acc 0.953125, prec 0.101966, recall 0.804519
2017-12-10T03:59:39.102623: step 7097, loss 0.257271, acc 0.9375, prec 0.101961, recall 0.804519
2017-12-10T03:59:39.377459: step 7098, loss 0.17029, acc 0.96875, prec 0.10197, recall 0.804539
2017-12-10T03:59:39.647439: step 7099, loss 0.481128, acc 0.921875, prec 0.101974, recall 0.804558
2017-12-10T03:59:39.914197: step 7100, loss 0.15352, acc 0.953125, prec 0.101982, recall 0.804577
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-7100

2017-12-10T03:59:41.270922: step 7101, loss 0.334561, acc 0.921875, prec 0.101975, recall 0.804577
2017-12-10T03:59:41.541411: step 7102, loss 0.837252, acc 0.90625, prec 0.101979, recall 0.804597
2017-12-10T03:59:41.817899: step 7103, loss 0.279439, acc 0.9375, prec 0.101985, recall 0.804616
2017-12-10T03:59:42.084118: step 7104, loss 0.128354, acc 0.96875, prec 0.101994, recall 0.804635
2017-12-10T03:59:42.355296: step 7105, loss 0.611475, acc 0.890625, prec 0.101996, recall 0.804654
2017-12-10T03:59:42.626895: step 7106, loss 0.786991, acc 0.875, prec 0.102008, recall 0.804693
2017-12-10T03:59:42.898318: step 7107, loss 0.278648, acc 0.9375, prec 0.102003, recall 0.804693
2017-12-10T03:59:43.164117: step 7108, loss 0.549778, acc 0.9375, prec 0.101998, recall 0.804693
2017-12-10T03:59:43.427503: step 7109, loss 0.18537, acc 0.96875, prec 0.102007, recall 0.804712
2017-12-10T03:59:43.688762: step 7110, loss 0.181968, acc 0.96875, prec 0.102016, recall 0.804731
2017-12-10T03:59:43.953825: step 7111, loss 0.415595, acc 0.953125, prec 0.102023, recall 0.804751
2017-12-10T03:59:44.217889: step 7112, loss 0.344624, acc 0.890625, prec 0.102014, recall 0.804751
2017-12-10T03:59:44.488543: step 7113, loss 0.322612, acc 0.96875, prec 0.102011, recall 0.804751
2017-12-10T03:59:44.753595: step 7114, loss 0.191788, acc 0.953125, prec 0.102019, recall 0.80477
2017-12-10T03:59:45.023341: step 7115, loss 0.136414, acc 0.953125, prec 0.102015, recall 0.80477
2017-12-10T03:59:45.288564: step 7116, loss 0.0369154, acc 0.984375, prec 0.102036, recall 0.804808
2017-12-10T03:59:45.562581: step 7117, loss 0.149413, acc 0.96875, prec 0.102034, recall 0.804808
2017-12-10T03:59:45.831017: step 7118, loss 0.232425, acc 0.953125, prec 0.102052, recall 0.804847
2017-12-10T03:59:46.094518: step 7119, loss 0.29325, acc 0.96875, prec 0.102061, recall 0.804866
2017-12-10T03:59:46.359513: step 7120, loss 0.430526, acc 0.96875, prec 0.102092, recall 0.804924
2017-12-10T03:59:46.630647: step 7121, loss 0.000191381, acc 1, prec 0.102092, recall 0.804924
2017-12-10T03:59:46.895254: step 7122, loss 0.0568866, acc 0.96875, prec 0.10209, recall 0.804924
2017-12-10T03:59:47.173056: step 7123, loss 3.9778e-05, acc 1, prec 0.102101, recall 0.804943
2017-12-10T03:59:47.442228: step 7124, loss 3.7455, acc 0.9375, prec 0.102108, recall 0.804883
2017-12-10T03:59:47.715963: step 7125, loss 0.172061, acc 0.96875, prec 0.102117, recall 0.804902
2017-12-10T03:59:47.984514: step 7126, loss 0.153827, acc 1, prec 0.102139, recall 0.80494
2017-12-10T03:59:48.251158: step 7127, loss 0.513571, acc 0.953125, prec 0.102135, recall 0.80494
2017-12-10T03:59:48.512824: step 7128, loss 0.0722463, acc 0.96875, prec 0.102133, recall 0.80494
2017-12-10T03:59:48.774699: step 7129, loss 0.223088, acc 0.953125, prec 0.102129, recall 0.80494
2017-12-10T03:59:49.053836: step 7130, loss 0.358877, acc 0.953125, prec 0.102125, recall 0.80494
2017-12-10T03:59:49.320824: step 7131, loss 0.459691, acc 0.96875, prec 0.102145, recall 0.804979
2017-12-10T03:59:49.583731: step 7132, loss 0.178234, acc 0.921875, prec 0.10215, recall 0.804998
2017-12-10T03:59:49.858038: step 7133, loss 0.855209, acc 0.90625, prec 0.102153, recall 0.805017
2017-12-10T03:59:50.121008: step 7134, loss 0.0410061, acc 0.984375, prec 0.102152, recall 0.805017
2017-12-10T03:59:50.385659: step 7135, loss 0.224589, acc 0.90625, prec 0.102156, recall 0.805036
2017-12-10T03:59:50.652851: step 7136, loss 0.450154, acc 0.90625, prec 0.102159, recall 0.805056
2017-12-10T03:59:50.919394: step 7137, loss 0.458809, acc 0.921875, prec 0.102175, recall 0.805094
2017-12-10T03:59:51.184739: step 7138, loss 0.222679, acc 0.96875, prec 0.102184, recall 0.805113
2017-12-10T03:59:51.453466: step 7139, loss 0.256793, acc 0.921875, prec 0.102189, recall 0.805132
2017-12-10T03:59:51.720096: step 7140, loss 0.448237, acc 0.96875, prec 0.102209, recall 0.805171
2017-12-10T03:59:51.985566: step 7141, loss 0.623792, acc 0.875, prec 0.10221, recall 0.80519
2017-12-10T03:59:52.252277: step 7142, loss 0.160476, acc 0.953125, prec 0.102206, recall 0.80519
2017-12-10T03:59:52.518890: step 7143, loss 0.128212, acc 0.984375, prec 0.102216, recall 0.805209
2017-12-10T03:59:52.784827: step 7144, loss 0.354347, acc 0.921875, prec 0.102265, recall 0.805305
2017-12-10T03:59:53.062094: step 7145, loss 0.368648, acc 0.875, prec 0.102255, recall 0.805305
2017-12-10T03:59:53.332410: step 7146, loss 0.273618, acc 0.9375, prec 0.102261, recall 0.805324
2017-12-10T03:59:53.598924: step 7147, loss 0.00236482, acc 1, prec 0.102261, recall 0.805324
2017-12-10T03:59:53.866578: step 7148, loss 0.704301, acc 0.96875, prec 0.102259, recall 0.805324
2017-12-10T03:59:54.132561: step 7149, loss 0.171144, acc 0.96875, prec 0.102256, recall 0.805324
2017-12-10T03:59:54.404194: step 7150, loss 0.045748, acc 0.96875, prec 0.102254, recall 0.805324
2017-12-10T03:59:54.668437: step 7151, loss 0.0264295, acc 0.984375, prec 0.102252, recall 0.805324
2017-12-10T03:59:54.935761: step 7152, loss 0.378434, acc 0.984375, prec 0.102262, recall 0.805343
2017-12-10T03:59:55.199738: step 7153, loss 0.00287457, acc 1, prec 0.102273, recall 0.805362
2017-12-10T03:59:55.469084: step 7154, loss 0.00394657, acc 1, prec 0.102285, recall 0.805381
2017-12-10T03:59:55.736040: step 7155, loss 0.0119178, acc 0.984375, prec 0.102283, recall 0.805381
2017-12-10T03:59:55.999304: step 7156, loss 0.0659119, acc 0.984375, prec 0.102293, recall 0.8054
2017-12-10T03:59:56.266226: step 7157, loss 0.123849, acc 0.984375, prec 0.102303, recall 0.805419
2017-12-10T03:59:56.535439: step 7158, loss 0.000496186, acc 1, prec 0.102314, recall 0.805438
2017-12-10T03:59:56.810814: step 7159, loss 0.259465, acc 0.984375, prec 0.102313, recall 0.805438
2017-12-10T03:59:57.077476: step 7160, loss 0.0590182, acc 0.96875, prec 0.102344, recall 0.805496
2017-12-10T03:59:57.343132: step 7161, loss 5.39213e-05, acc 1, prec 0.102344, recall 0.805496
2017-12-10T03:59:57.603424: step 7162, loss 0.132336, acc 0.984375, prec 0.102343, recall 0.805496
2017-12-10T03:59:57.871264: step 7163, loss 0.0046417, acc 1, prec 0.102376, recall 0.805553
2017-12-10T03:59:58.133961: step 7164, loss 0.0635275, acc 0.984375, prec 0.102375, recall 0.805553
2017-12-10T03:59:58.399905: step 7165, loss 2.6675e-05, acc 1, prec 0.102375, recall 0.805553
2017-12-10T03:59:58.661693: step 7166, loss 0.706449, acc 0.953125, prec 0.102371, recall 0.805553
2017-12-10T03:59:58.924235: step 7167, loss 0.051217, acc 0.984375, prec 0.102381, recall 0.805572
2017-12-10T03:59:59.191036: step 7168, loss 0.316766, acc 0.984375, prec 0.102402, recall 0.80561
2017-12-10T03:59:59.456202: step 7169, loss 0.00131662, acc 1, prec 0.102447, recall 0.805686
2017-12-10T03:59:59.719316: step 7170, loss 0.260878, acc 0.96875, prec 0.102456, recall 0.805705
2017-12-10T03:59:59.991595: step 7171, loss 0.174311, acc 0.953125, prec 0.102474, recall 0.805743
2017-12-10T04:00:00.270866: step 7172, loss 0.148691, acc 0.96875, prec 0.102472, recall 0.805743
2017-12-10T04:00:00.551759: step 7173, loss 0.0719981, acc 0.984375, prec 0.102493, recall 0.805781
2017-12-10T04:00:00.821082: step 7174, loss 0.00455936, acc 1, prec 0.102515, recall 0.80582
2017-12-10T04:00:01.086400: step 7175, loss 0.171765, acc 1, prec 0.102526, recall 0.805839
2017-12-10T04:00:01.355803: step 7176, loss 4.19298, acc 0.96875, prec 0.102525, recall 0.80576
2017-12-10T04:00:01.628517: step 7177, loss 0.118445, acc 0.984375, prec 0.102535, recall 0.805779
2017-12-10T04:00:01.897912: step 7178, loss 0.065214, acc 0.984375, prec 0.102545, recall 0.805798
2017-12-10T04:00:02.174148: step 7179, loss 0.124035, acc 0.984375, prec 0.102566, recall 0.805836
2017-12-10T04:00:02.434580: step 7180, loss 0.0184251, acc 0.984375, prec 0.102576, recall 0.805855
2017-12-10T04:00:02.695554: step 7181, loss 0.11467, acc 1, prec 0.102621, recall 0.805931
2017-12-10T04:00:02.962366: step 7182, loss 0.22819, acc 0.984375, prec 0.102631, recall 0.80595
2017-12-10T04:00:03.227599: step 7183, loss 0.170454, acc 0.953125, prec 0.102627, recall 0.80595
2017-12-10T04:00:03.499120: step 7184, loss 0.282833, acc 0.953125, prec 0.102623, recall 0.80595
2017-12-10T04:00:03.766821: step 7185, loss 0.0432292, acc 0.96875, prec 0.102632, recall 0.805969
2017-12-10T04:00:04.033551: step 7186, loss 0.25355, acc 0.921875, prec 0.102636, recall 0.805988
2017-12-10T04:00:04.297625: step 7187, loss 0.080271, acc 0.984375, prec 0.102635, recall 0.805988
2017-12-10T04:00:04.562130: step 7188, loss 0.0813757, acc 0.96875, prec 0.102632, recall 0.805988
2017-12-10T04:00:04.834474: step 7189, loss 0.0725112, acc 0.953125, prec 0.10264, recall 0.806007
2017-12-10T04:00:05.098188: step 7190, loss 0.730622, acc 0.921875, prec 0.102667, recall 0.806064
2017-12-10T04:00:05.364472: step 7191, loss 0.361455, acc 0.953125, prec 0.102685, recall 0.806102
2017-12-10T04:00:05.630925: step 7192, loss 0.772748, acc 0.953125, prec 0.102704, recall 0.806139
2017-12-10T04:00:05.898194: step 7193, loss 0.248418, acc 0.96875, prec 0.102724, recall 0.806177
2017-12-10T04:00:06.160818: step 7194, loss 0.0460479, acc 0.96875, prec 0.102721, recall 0.806177
2017-12-10T04:00:06.421964: step 7195, loss 0.376998, acc 0.90625, prec 0.102725, recall 0.806196
2017-12-10T04:00:06.694097: step 7196, loss 0.569269, acc 0.921875, prec 0.102718, recall 0.806196
2017-12-10T04:00:06.966384: step 7197, loss 0.232933, acc 0.953125, prec 0.102726, recall 0.806215
2017-12-10T04:00:07.232313: step 7198, loss 0.0117124, acc 1, prec 0.102726, recall 0.806215
2017-12-10T04:00:07.493797: step 7199, loss 0.516922, acc 0.953125, prec 0.102744, recall 0.806253
2017-12-10T04:00:07.760484: step 7200, loss 0.201957, acc 0.96875, prec 0.102753, recall 0.806272

Evaluation:
2017-12-10T04:00:15.438118: step 7200, loss 8.25134, acc 0.94858, prec 0.102979, recall 0.801675

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-7200

2017-12-10T04:00:16.752860: step 7201, loss 0.171991, acc 0.953125, prec 0.102975, recall 0.801675
2017-12-10T04:00:17.022403: step 7202, loss 0.103643, acc 0.96875, prec 0.103006, recall 0.801732
2017-12-10T04:00:17.290729: step 7203, loss 0.201441, acc 0.953125, prec 0.103013, recall 0.801751
2017-12-10T04:00:17.555955: step 7204, loss 0.225802, acc 0.96875, prec 0.103022, recall 0.801771
2017-12-10T04:00:17.831279: step 7205, loss 0.0894331, acc 0.984375, prec 0.103032, recall 0.80179
2017-12-10T04:00:18.101798: step 7206, loss 0.367183, acc 0.9375, prec 0.10306, recall 0.801847
2017-12-10T04:00:18.365268: step 7207, loss 0.0551356, acc 0.96875, prec 0.103057, recall 0.801847
2017-12-10T04:00:18.633418: step 7208, loss 0.660368, acc 0.9375, prec 0.103074, recall 0.801885
2017-12-10T04:00:18.897028: step 7209, loss 0.241413, acc 0.953125, prec 0.103082, recall 0.801904
2017-12-10T04:00:19.164025: step 7210, loss 0.0827186, acc 0.984375, prec 0.103114, recall 0.801961
2017-12-10T04:00:19.438675: step 7211, loss 0.653458, acc 0.96875, prec 0.103133, recall 0.801999
2017-12-10T04:00:19.702309: step 7212, loss 0.220807, acc 0.953125, prec 0.103129, recall 0.801999
2017-12-10T04:00:19.971924: step 7213, loss 0.00222904, acc 1, prec 0.103141, recall 0.802018
2017-12-10T04:00:20.232092: step 7214, loss 4.88008e-05, acc 1, prec 0.103152, recall 0.802037
2017-12-10T04:00:20.500322: step 7215, loss 0.0223842, acc 0.984375, prec 0.103161, recall 0.802056
2017-12-10T04:00:20.761483: step 7216, loss 0.137348, acc 0.984375, prec 0.103171, recall 0.802075
2017-12-10T04:00:21.026384: step 7217, loss 0.802401, acc 0.96875, prec 0.103191, recall 0.802113
2017-12-10T04:00:21.294372: step 7218, loss 0.00344749, acc 1, prec 0.103213, recall 0.802151
2017-12-10T04:00:21.554553: step 7219, loss 1.07379, acc 1, prec 0.103235, recall 0.802189
2017-12-10T04:00:21.821064: step 7220, loss 0.235222, acc 0.984375, prec 0.103245, recall 0.802208
2017-12-10T04:00:22.085592: step 7221, loss 0.383967, acc 0.953125, prec 0.103241, recall 0.802208
2017-12-10T04:00:22.350700: step 7222, loss 0.503141, acc 0.953125, prec 0.103237, recall 0.802208
2017-12-10T04:00:22.615404: step 7223, loss 0.845679, acc 0.953125, prec 0.103234, recall 0.802208
2017-12-10T04:00:22.886092: step 7224, loss 0.369064, acc 0.96875, prec 0.103242, recall 0.802227
2017-12-10T04:00:23.146948: step 7225, loss 0.00685685, acc 1, prec 0.103242, recall 0.802227
2017-12-10T04:00:23.420445: step 7226, loss 0.306229, acc 0.953125, prec 0.103249, recall 0.802246
2017-12-10T04:00:23.685196: step 7227, loss 0.0169146, acc 0.984375, prec 0.10327, recall 0.802284
2017-12-10T04:00:23.949980: step 7228, loss 0.0848438, acc 0.984375, prec 0.10328, recall 0.802303
2017-12-10T04:00:24.213753: step 7229, loss 0.209457, acc 0.96875, prec 0.103277, recall 0.802303
2017-12-10T04:00:24.485185: step 7230, loss 0.183378, acc 0.984375, prec 0.103298, recall 0.802341
2017-12-10T04:00:24.751972: step 7231, loss 0.463636, acc 0.890625, prec 0.1033, recall 0.80236
2017-12-10T04:00:25.021266: step 7232, loss 0.0884649, acc 0.984375, prec 0.10331, recall 0.802379
2017-12-10T04:00:25.294092: step 7233, loss 0.000281311, acc 1, prec 0.10331, recall 0.802379
2017-12-10T04:00:25.561095: step 7234, loss 0.332151, acc 0.9375, prec 0.103327, recall 0.802417
2017-12-10T04:00:25.833380: step 7235, loss 0.0929796, acc 0.953125, prec 0.103335, recall 0.802436
2017-12-10T04:00:26.097580: step 7236, loss 0.00706208, acc 1, prec 0.103335, recall 0.802436
2017-12-10T04:00:26.369751: step 7237, loss 0.276234, acc 0.96875, prec 0.103343, recall 0.802455
2017-12-10T04:00:26.640952: step 7238, loss 0.140802, acc 0.9375, prec 0.103338, recall 0.802455
2017-12-10T04:00:26.917424: step 7239, loss 0.427061, acc 0.953125, prec 0.103334, recall 0.802455
2017-12-10T04:00:27.181515: step 7240, loss 0.0635512, acc 0.953125, prec 0.103341, recall 0.802474
2017-12-10T04:00:27.462477: step 7241, loss 1.41264, acc 0.953125, prec 0.103339, recall 0.802397
2017-12-10T04:00:27.727930: step 7242, loss 0.0204108, acc 0.984375, prec 0.103349, recall 0.802416
2017-12-10T04:00:27.994318: step 7243, loss 0.11647, acc 0.96875, prec 0.103357, recall 0.802435
2017-12-10T04:00:28.265969: step 7244, loss 0.970299, acc 0.9375, prec 0.103352, recall 0.802435
2017-12-10T04:00:28.533612: step 7245, loss 0.00204006, acc 1, prec 0.103352, recall 0.802435
2017-12-10T04:00:28.801013: step 7246, loss 0.000533012, acc 1, prec 0.103363, recall 0.802454
2017-12-10T04:00:29.066441: step 7247, loss 0.521239, acc 0.921875, prec 0.103357, recall 0.802454
2017-12-10T04:00:29.337315: step 7248, loss 0.962272, acc 0.921875, prec 0.103361, recall 0.802473
2017-12-10T04:00:29.610402: step 7249, loss 0.462924, acc 0.9375, prec 0.103378, recall 0.802511
2017-12-10T04:00:29.878126: step 7250, loss 0.0780076, acc 0.96875, prec 0.103398, recall 0.802548
2017-12-10T04:00:30.146301: step 7251, loss 0.206835, acc 0.953125, prec 0.103394, recall 0.802548
2017-12-10T04:00:30.419920: step 7252, loss 0.161448, acc 0.921875, prec 0.103388, recall 0.802548
2017-12-10T04:00:30.692527: step 7253, loss 0.165871, acc 0.96875, prec 0.103396, recall 0.802567
2017-12-10T04:00:30.962683: step 7254, loss 0.263395, acc 0.9375, prec 0.103391, recall 0.802567
2017-12-10T04:00:31.222557: step 7255, loss 0.95058, acc 0.921875, prec 0.103385, recall 0.802567
2017-12-10T04:00:31.489863: step 7256, loss 0.254357, acc 0.890625, prec 0.103376, recall 0.802567
2017-12-10T04:00:31.762434: step 7257, loss 0.47351, acc 0.96875, prec 0.103373, recall 0.802567
2017-12-10T04:00:32.031666: step 7258, loss 0.0694283, acc 0.96875, prec 0.103393, recall 0.802605
2017-12-10T04:00:32.298814: step 7259, loss 0.0581028, acc 0.984375, prec 0.103403, recall 0.802624
2017-12-10T04:00:32.564566: step 7260, loss 0.279379, acc 0.96875, prec 0.103433, recall 0.802681
2017-12-10T04:00:32.831156: step 7261, loss 0.276076, acc 0.9375, prec 0.103439, recall 0.8027
2017-12-10T04:00:33.107121: step 7262, loss 0.143263, acc 0.9375, prec 0.103434, recall 0.8027
2017-12-10T04:00:33.373119: step 7263, loss 0.141672, acc 0.984375, prec 0.103433, recall 0.8027
2017-12-10T04:00:33.642385: step 7264, loss 0.113766, acc 0.96875, prec 0.10343, recall 0.8027
2017-12-10T04:00:33.913407: step 7265, loss 0.18259, acc 0.953125, prec 0.103427, recall 0.8027
2017-12-10T04:00:34.175549: step 7266, loss 0.0931574, acc 0.984375, prec 0.103425, recall 0.8027
2017-12-10T04:00:34.439360: step 7267, loss 0.00429604, acc 1, prec 0.103436, recall 0.802718
2017-12-10T04:00:34.710514: step 7268, loss 0.00755341, acc 1, prec 0.103436, recall 0.802718
2017-12-10T04:00:34.968665: step 7269, loss 0.0125514, acc 0.984375, prec 0.103457, recall 0.802756
2017-12-10T04:00:35.239501: step 7270, loss 0.0035532, acc 1, prec 0.103457, recall 0.802756
2017-12-10T04:00:35.507504: step 7271, loss 3.15787, acc 0.96875, prec 0.103478, recall 0.802717
2017-12-10T04:00:35.782442: step 7272, loss 0.244098, acc 0.984375, prec 0.103488, recall 0.802736
2017-12-10T04:00:36.049656: step 7273, loss 0.0561358, acc 0.984375, prec 0.10352, recall 0.802793
2017-12-10T04:00:36.317082: step 7274, loss 0.144439, acc 0.984375, prec 0.103541, recall 0.80283
2017-12-10T04:00:36.587331: step 7275, loss 1.0358, acc 0.921875, prec 0.103534, recall 0.80283
2017-12-10T04:00:36.858222: step 7276, loss 0.561363, acc 0.875, prec 0.103535, recall 0.802849
2017-12-10T04:00:37.123451: step 7277, loss 0.173358, acc 0.953125, prec 0.103531, recall 0.802849
2017-12-10T04:00:37.389479: step 7278, loss 0.429447, acc 0.90625, prec 0.103524, recall 0.802849
2017-12-10T04:00:37.662872: step 7279, loss 0.135935, acc 0.9375, prec 0.103529, recall 0.802868
2017-12-10T04:00:37.932488: step 7280, loss 0.179403, acc 0.9375, prec 0.103524, recall 0.802868
2017-12-10T04:00:38.197513: step 7281, loss 0.203704, acc 0.9375, prec 0.10353, recall 0.802887
2017-12-10T04:00:38.461709: step 7282, loss 0.621622, acc 0.921875, prec 0.103535, recall 0.802906
2017-12-10T04:00:38.727981: step 7283, loss 0.905103, acc 0.875, prec 0.103536, recall 0.802925
2017-12-10T04:00:38.993893: step 7284, loss 0.663698, acc 0.921875, prec 0.103552, recall 0.802962
2017-12-10T04:00:39.255317: step 7285, loss 0.221972, acc 0.953125, prec 0.103548, recall 0.802962
2017-12-10T04:00:39.526021: step 7286, loss 0.91935, acc 0.875, prec 0.103537, recall 0.802962
2017-12-10T04:00:39.795688: step 7287, loss 0.399617, acc 0.90625, prec 0.10353, recall 0.802962
2017-12-10T04:00:40.062913: step 7288, loss 0.799281, acc 0.875, prec 0.10352, recall 0.802962
2017-12-10T04:00:40.330709: step 7289, loss 0.616802, acc 0.921875, prec 0.103535, recall 0.803
2017-12-10T04:00:40.591896: step 7290, loss 0.721471, acc 0.90625, prec 0.10355, recall 0.803038
2017-12-10T04:00:40.855142: step 7291, loss 0.39525, acc 0.890625, prec 0.103552, recall 0.803056
2017-12-10T04:00:41.119620: step 7292, loss 0.0276379, acc 0.984375, prec 0.103551, recall 0.803056
2017-12-10T04:00:41.389773: step 7293, loss 0.536018, acc 0.921875, prec 0.103555, recall 0.803075
2017-12-10T04:00:41.658619: step 7294, loss 0.382698, acc 0.953125, prec 0.103551, recall 0.803075
2017-12-10T04:00:41.935032: step 7295, loss 0.206808, acc 0.953125, prec 0.103559, recall 0.803094
2017-12-10T04:00:42.202334: step 7296, loss 0.0876488, acc 0.953125, prec 0.103566, recall 0.803113
2017-12-10T04:00:42.471143: step 7297, loss 0.208644, acc 0.96875, prec 0.103574, recall 0.803132
2017-12-10T04:00:42.743349: step 7298, loss 0.00156166, acc 1, prec 0.103574, recall 0.803132
2017-12-10T04:00:43.010617: step 7299, loss 0.0206787, acc 0.984375, prec 0.103595, recall 0.803169
2017-12-10T04:00:43.273317: step 7300, loss 0.218148, acc 0.984375, prec 0.103594, recall 0.803169
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-7300

2017-12-10T04:00:44.839416: step 7301, loss 0.124426, acc 0.984375, prec 0.103626, recall 0.803226
2017-12-10T04:00:45.102885: step 7302, loss 0.000103122, acc 1, prec 0.103637, recall 0.803244
2017-12-10T04:00:45.360795: step 7303, loss 1.55583, acc 0.953125, prec 0.103645, recall 0.803186
2017-12-10T04:00:45.636891: step 7304, loss 0.431228, acc 0.9375, prec 0.10364, recall 0.803186
2017-12-10T04:00:45.903264: step 7305, loss 0.0744062, acc 0.984375, prec 0.103661, recall 0.803224
2017-12-10T04:00:46.168732: step 7306, loss 0.169082, acc 0.9375, prec 0.103656, recall 0.803224
2017-12-10T04:00:46.435923: step 7307, loss 0.020257, acc 0.984375, prec 0.103655, recall 0.803224
2017-12-10T04:00:46.701855: step 7308, loss 0.300588, acc 0.90625, prec 0.103669, recall 0.803262
2017-12-10T04:00:46.974805: step 7309, loss 0.227556, acc 0.953125, prec 0.103687, recall 0.803299
2017-12-10T04:00:47.246054: step 7310, loss 0.35961, acc 0.953125, prec 0.103694, recall 0.803318
2017-12-10T04:00:47.523280: step 7311, loss 0.194594, acc 0.96875, prec 0.103703, recall 0.803337
2017-12-10T04:00:47.796381: step 7312, loss 0.413607, acc 0.921875, prec 0.103697, recall 0.803337
2017-12-10T04:00:48.060434: step 7313, loss 0.00612313, acc 1, prec 0.103708, recall 0.803355
2017-12-10T04:00:48.324907: step 7314, loss 0.260879, acc 0.96875, prec 0.103716, recall 0.803374
2017-12-10T04:00:48.592704: step 7315, loss 0.0856477, acc 0.984375, prec 0.103726, recall 0.803393
2017-12-10T04:00:48.865260: step 7316, loss 0.465428, acc 0.96875, prec 0.103723, recall 0.803393
2017-12-10T04:00:49.134731: step 7317, loss 0.402415, acc 0.984375, prec 0.103733, recall 0.803411
2017-12-10T04:00:49.405217: step 7318, loss 0.122423, acc 0.96875, prec 0.10373, recall 0.803411
2017-12-10T04:00:49.673223: step 7319, loss 0.256434, acc 0.96875, prec 0.103728, recall 0.803411
2017-12-10T04:00:49.951289: step 7320, loss 0.712787, acc 0.921875, prec 0.103732, recall 0.80343
2017-12-10T04:00:50.215357: step 7321, loss 0.324637, acc 0.953125, prec 0.103729, recall 0.80343
2017-12-10T04:00:50.481507: step 7322, loss 0.0631831, acc 0.953125, prec 0.103736, recall 0.803449
2017-12-10T04:00:50.745517: step 7323, loss 0.317999, acc 0.984375, prec 0.103768, recall 0.803505
2017-12-10T04:00:51.008366: step 7324, loss 0.0647443, acc 0.984375, prec 0.103777, recall 0.803524
2017-12-10T04:00:51.984168: step 7325, loss 0.524883, acc 0.953125, prec 0.103774, recall 0.803524
2017-12-10T04:00:52.344532: step 7326, loss 0.0524283, acc 0.984375, prec 0.103805, recall 0.80358
2017-12-10T04:00:53.094912: step 7327, loss 0.331442, acc 0.96875, prec 0.103803, recall 0.80358
2017-12-10T04:00:53.838483: step 7328, loss 0.18889, acc 0.96875, prec 0.103811, recall 0.803599
2017-12-10T04:00:54.564958: step 7329, loss 0.176376, acc 0.984375, prec 0.103821, recall 0.803617
2017-12-10T04:00:55.273634: step 7330, loss 0.000895608, acc 1, prec 0.103832, recall 0.803636
2017-12-10T04:00:56.053515: step 7331, loss 0.0209452, acc 0.984375, prec 0.103842, recall 0.803655
2017-12-10T04:00:57.099562: step 7332, loss 0.220382, acc 1, prec 0.103864, recall 0.803692
2017-12-10T04:00:57.454740: step 7333, loss 0.00578983, acc 1, prec 0.103886, recall 0.803729
2017-12-10T04:00:57.734222: step 7334, loss 0.196728, acc 0.96875, prec 0.103883, recall 0.803729
2017-12-10T04:00:58.009428: step 7335, loss 0.0649851, acc 0.96875, prec 0.103903, recall 0.803767
2017-12-10T04:00:58.274419: step 7336, loss 0.0195042, acc 0.984375, prec 0.103902, recall 0.803767
2017-12-10T04:00:58.538732: step 7337, loss 0.0340941, acc 0.96875, prec 0.103899, recall 0.803767
2017-12-10T04:00:58.809412: step 7338, loss 0.00182974, acc 1, prec 0.103921, recall 0.803804
2017-12-10T04:00:59.074918: step 7339, loss 0.0557757, acc 0.984375, prec 0.103964, recall 0.803879
2017-12-10T04:00:59.351212: step 7340, loss 0.0229367, acc 0.984375, prec 0.103963, recall 0.803879
2017-12-10T04:00:59.619979: step 7341, loss 0.627699, acc 0.96875, prec 0.103971, recall 0.803897
2017-12-10T04:00:59.884109: step 7342, loss 0.13228, acc 0.984375, prec 0.10397, recall 0.803897
2017-12-10T04:01:00.155872: step 7343, loss 0.16739, acc 0.96875, prec 0.103978, recall 0.803916
2017-12-10T04:01:00.435450: step 7344, loss 0.304909, acc 0.953125, prec 0.103996, recall 0.803953
2017-12-10T04:01:00.703581: step 7345, loss 0.285048, acc 0.96875, prec 0.103994, recall 0.803953
2017-12-10T04:01:00.970571: step 7346, loss 0.0246834, acc 0.96875, prec 0.104002, recall 0.803972
2017-12-10T04:01:01.236866: step 7347, loss 0.211046, acc 0.953125, prec 0.104009, recall 0.80399
2017-12-10T04:01:01.506108: step 7348, loss 0.154106, acc 0.953125, prec 0.104028, recall 0.804028
2017-12-10T04:01:01.774056: step 7349, loss 0.285255, acc 0.96875, prec 0.104025, recall 0.804028
2017-12-10T04:01:02.042942: step 7350, loss 0.0376997, acc 1, prec 0.104058, recall 0.804084
2017-12-10T04:01:02.307629: step 7351, loss 0.208929, acc 0.96875, prec 0.104056, recall 0.804084
2017-12-10T04:01:02.581099: step 7352, loss 0.0893102, acc 0.984375, prec 0.104065, recall 0.804102
2017-12-10T04:01:02.847799: step 7353, loss 0.238251, acc 0.953125, prec 0.104061, recall 0.804102
2017-12-10T04:01:03.111971: step 7354, loss 0.640486, acc 0.9375, prec 0.104056, recall 0.804102
2017-12-10T04:01:03.379767: step 7355, loss 0.000198707, acc 1, prec 0.104067, recall 0.804121
2017-12-10T04:01:03.644324: step 7356, loss 0.120433, acc 0.984375, prec 0.104077, recall 0.804139
2017-12-10T04:01:03.910448: step 7357, loss 0.110838, acc 0.984375, prec 0.104087, recall 0.804158
2017-12-10T04:01:04.176900: step 7358, loss 0.19585, acc 0.984375, prec 0.104097, recall 0.804177
2017-12-10T04:01:04.446021: step 7359, loss 0.00227608, acc 1, prec 0.104097, recall 0.804177
2017-12-10T04:01:04.715386: step 7360, loss 0.24077, acc 0.984375, prec 0.104106, recall 0.804195
2017-12-10T04:01:04.991702: step 7361, loss 0.912631, acc 0.984375, prec 0.104127, recall 0.804232
2017-12-10T04:01:05.256642: step 7362, loss 0.00138152, acc 1, prec 0.104138, recall 0.804251
2017-12-10T04:01:05.516580: step 7363, loss 8.89089, acc 0.984375, prec 0.104149, recall 0.804193
2017-12-10T04:01:05.788185: step 7364, loss 0.0307055, acc 0.984375, prec 0.104181, recall 0.804249
2017-12-10T04:01:06.058025: step 7365, loss 0.350055, acc 0.953125, prec 0.104199, recall 0.804286
2017-12-10T04:01:06.332626: step 7366, loss 0.0898327, acc 0.984375, prec 0.104198, recall 0.804286
2017-12-10T04:01:06.601548: step 7367, loss 0.4039, acc 0.875, prec 0.104187, recall 0.804286
2017-12-10T04:01:06.870357: step 7368, loss 0.278711, acc 0.953125, prec 0.104206, recall 0.804323
2017-12-10T04:01:07.132503: step 7369, loss 0.764246, acc 0.90625, prec 0.104209, recall 0.804342
2017-12-10T04:01:07.392886: step 7370, loss 0.57686, acc 0.890625, prec 0.104211, recall 0.80436
2017-12-10T04:01:07.657984: step 7371, loss 0.176676, acc 0.9375, prec 0.104239, recall 0.804416
2017-12-10T04:01:07.921290: step 7372, loss 0.751757, acc 0.890625, prec 0.10423, recall 0.804416
2017-12-10T04:01:08.183057: step 7373, loss 0.991324, acc 0.8125, prec 0.104214, recall 0.804416
2017-12-10T04:01:08.449583: step 7374, loss 0.664265, acc 0.859375, prec 0.104214, recall 0.804434
2017-12-10T04:01:08.715060: step 7375, loss 0.949507, acc 0.828125, prec 0.1042, recall 0.804434
2017-12-10T04:01:08.975849: step 7376, loss 0.597206, acc 0.875, prec 0.10419, recall 0.804434
2017-12-10T04:01:09.242793: step 7377, loss 0.880503, acc 0.875, prec 0.104201, recall 0.804471
2017-12-10T04:01:09.506207: step 7378, loss 0.127, acc 0.9375, prec 0.104207, recall 0.80449
2017-12-10T04:01:09.783316: step 7379, loss 0.45106, acc 0.921875, prec 0.104212, recall 0.804508
2017-12-10T04:01:10.049571: step 7380, loss 0.670996, acc 0.890625, prec 0.104203, recall 0.804508
2017-12-10T04:01:10.316082: step 7381, loss 0.38466, acc 0.921875, prec 0.104197, recall 0.804508
2017-12-10T04:01:10.577241: step 7382, loss 0.272597, acc 0.953125, prec 0.104193, recall 0.804508
2017-12-10T04:01:10.839463: step 7383, loss 0.572818, acc 0.953125, prec 0.104189, recall 0.804508
2017-12-10T04:01:11.105664: step 7384, loss 0.707747, acc 0.90625, prec 0.104181, recall 0.804508
2017-12-10T04:01:11.368480: step 7385, loss 0.0165697, acc 0.984375, prec 0.104202, recall 0.804545
2017-12-10T04:01:11.637669: step 7386, loss 0.141563, acc 0.953125, prec 0.104198, recall 0.804545
2017-12-10T04:01:11.916064: step 7387, loss 0.415017, acc 0.9375, prec 0.104204, recall 0.804564
2017-12-10T04:01:12.177509: step 7388, loss 0.542331, acc 0.953125, prec 0.1042, recall 0.804564
2017-12-10T04:01:12.444132: step 7389, loss 0.296946, acc 0.953125, prec 0.104207, recall 0.804582
2017-12-10T04:01:12.711428: step 7390, loss 0.0680209, acc 0.984375, prec 0.104206, recall 0.804582
2017-12-10T04:01:12.973862: step 7391, loss 0.676158, acc 0.921875, prec 0.104211, recall 0.804601
2017-12-10T04:01:13.244640: step 7392, loss 0.18385, acc 0.96875, prec 0.104208, recall 0.804601
2017-12-10T04:01:13.511968: step 7393, loss 0.23572, acc 1, prec 0.10423, recall 0.804638
2017-12-10T04:01:13.780278: step 7394, loss 0.0408551, acc 1, prec 0.104252, recall 0.804675
2017-12-10T04:01:14.043791: step 7395, loss 1.17813e-05, acc 1, prec 0.104252, recall 0.804675
2017-12-10T04:01:14.307559: step 7396, loss 0.0527819, acc 0.984375, prec 0.104273, recall 0.804712
2017-12-10T04:01:14.572532: step 7397, loss 6.82134e-05, acc 1, prec 0.104306, recall 0.804767
2017-12-10T04:01:14.836572: step 7398, loss 2.6733, acc 0.953125, prec 0.104325, recall 0.804728
2017-12-10T04:01:15.111462: step 7399, loss 0.146849, acc 0.96875, prec 0.104344, recall 0.804765
2017-12-10T04:01:15.385012: step 7400, loss 0.320257, acc 0.96875, prec 0.104364, recall 0.804802
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-7400

2017-12-10T04:01:16.782090: step 7401, loss 0.258073, acc 0.953125, prec 0.10436, recall 0.804802
2017-12-10T04:01:17.048768: step 7402, loss 0.493561, acc 0.953125, prec 0.104356, recall 0.804802
2017-12-10T04:01:17.319512: step 7403, loss 0.273764, acc 0.96875, prec 0.104354, recall 0.804802
2017-12-10T04:01:17.587949: step 7404, loss 0.041284, acc 0.96875, prec 0.104351, recall 0.804802
2017-12-10T04:01:17.857607: step 7405, loss 0.030033, acc 0.984375, prec 0.10435, recall 0.804802
2017-12-10T04:01:18.124936: step 7406, loss 0.0984019, acc 0.984375, prec 0.10437, recall 0.804839
2017-12-10T04:01:18.393645: step 7407, loss 0.0165126, acc 1, prec 0.104403, recall 0.804894
2017-12-10T04:01:18.656189: step 7408, loss 0.635993, acc 0.96875, prec 0.104412, recall 0.804913
2017-12-10T04:01:18.923382: step 7409, loss 0.00233124, acc 1, prec 0.104423, recall 0.804931
2017-12-10T04:01:19.188095: step 7410, loss 4.69683, acc 0.96875, prec 0.104432, recall 0.804873
2017-12-10T04:01:19.455940: step 7411, loss 0.228043, acc 0.96875, prec 0.104452, recall 0.80491
2017-12-10T04:01:19.733105: step 7412, loss 0.0512575, acc 0.984375, prec 0.104451, recall 0.80491
2017-12-10T04:01:20.003124: step 7413, loss 0.0442807, acc 0.984375, prec 0.10446, recall 0.804929
2017-12-10T04:01:20.265267: step 7414, loss 0.180309, acc 0.953125, prec 0.104467, recall 0.804947
2017-12-10T04:01:20.525487: step 7415, loss 0.7745, acc 0.953125, prec 0.104485, recall 0.804984
2017-12-10T04:01:20.790705: step 7416, loss 0.151744, acc 0.953125, prec 0.104493, recall 0.805002
2017-12-10T04:01:21.052898: step 7417, loss 0.760373, acc 0.921875, prec 0.104486, recall 0.805002
2017-12-10T04:01:21.315885: step 7418, loss 0.289344, acc 0.9375, prec 0.104492, recall 0.805021
2017-12-10T04:01:21.581400: step 7419, loss 0.199352, acc 0.9375, prec 0.10452, recall 0.805076
2017-12-10T04:01:21.844133: step 7420, loss 1.40511, acc 0.9375, prec 0.104515, recall 0.805076
2017-12-10T04:01:22.110220: step 7421, loss 0.283366, acc 0.953125, prec 0.104522, recall 0.805094
2017-12-10T04:01:22.373576: step 7422, loss 0.189003, acc 0.921875, prec 0.104515, recall 0.805094
2017-12-10T04:01:22.647361: step 7423, loss 0.91144, acc 0.9375, prec 0.10451, recall 0.805094
2017-12-10T04:01:22.911974: step 7424, loss 0.032863, acc 0.984375, prec 0.104531, recall 0.805131
2017-12-10T04:01:23.177487: step 7425, loss 0.0731582, acc 0.984375, prec 0.10453, recall 0.805131
2017-12-10T04:01:23.447135: step 7426, loss 0.270731, acc 0.96875, prec 0.104538, recall 0.805149
2017-12-10T04:01:23.712038: step 7427, loss 0.128291, acc 0.953125, prec 0.104545, recall 0.805168
2017-12-10T04:01:23.978750: step 7428, loss 0.441791, acc 0.9375, prec 0.104551, recall 0.805186
2017-12-10T04:01:24.245152: step 7429, loss 0.186872, acc 0.984375, prec 0.104561, recall 0.805205
2017-12-10T04:01:24.509673: step 7430, loss 0.458885, acc 0.96875, prec 0.104558, recall 0.805205
2017-12-10T04:01:24.771527: step 7431, loss 0.18048, acc 0.953125, prec 0.104554, recall 0.805205
2017-12-10T04:01:25.043740: step 7432, loss 0.448175, acc 0.9375, prec 0.104549, recall 0.805205
2017-12-10T04:01:25.310236: step 7433, loss 0.185611, acc 0.9375, prec 0.104555, recall 0.805223
2017-12-10T04:01:25.575527: step 7434, loss 0.439241, acc 0.953125, prec 0.104551, recall 0.805223
2017-12-10T04:01:25.842405: step 7435, loss 0.144741, acc 0.96875, prec 0.104549, recall 0.805223
2017-12-10T04:01:26.107931: step 7436, loss 0.440717, acc 0.953125, prec 0.104567, recall 0.80526
2017-12-10T04:01:26.374350: step 7437, loss 0.136905, acc 0.984375, prec 0.104609, recall 0.805333
2017-12-10T04:01:26.640968: step 7438, loss 0.44001, acc 0.921875, prec 0.104614, recall 0.805351
2017-12-10T04:01:26.924122: step 7439, loss 0.00862485, acc 1, prec 0.104625, recall 0.80537
2017-12-10T04:01:27.190037: step 7440, loss 0.655162, acc 0.953125, prec 0.104643, recall 0.805406
2017-12-10T04:01:27.452271: step 7441, loss 0.116207, acc 0.96875, prec 0.10464, recall 0.805406
2017-12-10T04:01:27.726175: step 7442, loss 0.142224, acc 0.9375, prec 0.104635, recall 0.805406
2017-12-10T04:01:27.991260: step 7443, loss 0.361861, acc 0.9375, prec 0.10463, recall 0.805406
2017-12-10T04:01:28.256027: step 7444, loss 0.0517772, acc 0.984375, prec 0.10464, recall 0.805425
2017-12-10T04:01:28.524850: step 7445, loss 0.0514632, acc 0.984375, prec 0.104639, recall 0.805425
2017-12-10T04:01:28.790755: step 7446, loss 0.00127803, acc 1, prec 0.104639, recall 0.805425
2017-12-10T04:01:29.050820: step 7447, loss 1.34566, acc 0.984375, prec 0.104649, recall 0.805367
2017-12-10T04:01:29.321030: step 7448, loss 0.292344, acc 0.96875, prec 0.104658, recall 0.805386
2017-12-10T04:01:29.590320: step 7449, loss 0.183183, acc 0.953125, prec 0.104665, recall 0.805404
2017-12-10T04:01:29.859255: step 7450, loss 0.132944, acc 0.96875, prec 0.104662, recall 0.805404
2017-12-10T04:01:30.123949: step 7451, loss 0.0151956, acc 0.984375, prec 0.104683, recall 0.80544
2017-12-10T04:01:30.394462: step 7452, loss 0.187684, acc 0.984375, prec 0.104682, recall 0.80544
2017-12-10T04:01:30.667155: step 7453, loss 0.240228, acc 0.96875, prec 0.10469, recall 0.805459
2017-12-10T04:01:30.933137: step 7454, loss 0.00613432, acc 1, prec 0.104712, recall 0.805495
2017-12-10T04:01:31.158809: step 7455, loss 0.233723, acc 0.960784, prec 0.104731, recall 0.805532
2017-12-10T04:01:31.429079: step 7456, loss 0.00375864, acc 1, prec 0.104731, recall 0.805532
2017-12-10T04:01:31.694205: step 7457, loss 0.0197914, acc 0.984375, prec 0.10473, recall 0.805532
2017-12-10T04:01:31.956191: step 7458, loss 0.0462535, acc 0.96875, prec 0.104738, recall 0.80555
2017-12-10T04:01:32.224359: step 7459, loss 0.00641291, acc 1, prec 0.104738, recall 0.80555
2017-12-10T04:01:32.487857: step 7460, loss 1.11325, acc 0.90625, prec 0.104742, recall 0.805569
2017-12-10T04:01:32.749902: step 7461, loss 0.0167749, acc 0.984375, prec 0.104751, recall 0.805587
2017-12-10T04:01:33.015120: step 7462, loss 0.576879, acc 0.9375, prec 0.104768, recall 0.805623
2017-12-10T04:01:33.280838: step 7463, loss 0.0139895, acc 0.984375, prec 0.104778, recall 0.805642
2017-12-10T04:01:33.545613: step 7464, loss 0.226262, acc 0.953125, prec 0.104796, recall 0.805678
2017-12-10T04:01:33.810391: step 7465, loss 0.206328, acc 0.984375, prec 0.104817, recall 0.805715
2017-12-10T04:01:34.079296: step 7466, loss 0.0853542, acc 0.953125, prec 0.104813, recall 0.805715
2017-12-10T04:01:34.341013: step 7467, loss 0.68091, acc 1, prec 0.104835, recall 0.805751
2017-12-10T04:01:34.607581: step 7468, loss 0.00279133, acc 1, prec 0.104835, recall 0.805751
2017-12-10T04:01:34.870024: step 7469, loss 0.173327, acc 0.953125, prec 0.104842, recall 0.80577
2017-12-10T04:01:35.133090: step 7470, loss 0.121977, acc 0.96875, prec 0.10485, recall 0.805788
2017-12-10T04:01:35.395108: step 7471, loss 0.464381, acc 0.953125, prec 0.104868, recall 0.805824
2017-12-10T04:01:35.660660: step 7472, loss 0.0378535, acc 0.984375, prec 0.104878, recall 0.805843
2017-12-10T04:01:35.926434: step 7473, loss 0.0232649, acc 0.984375, prec 0.104887, recall 0.805861
2017-12-10T04:01:36.192362: step 7474, loss 0.00272785, acc 1, prec 0.104909, recall 0.805897
2017-12-10T04:01:36.455634: step 7475, loss 0.249326, acc 0.96875, prec 0.104907, recall 0.805897
2017-12-10T04:01:36.724661: step 7476, loss 0.167209, acc 0.953125, prec 0.104914, recall 0.805915
2017-12-10T04:01:36.987804: step 7477, loss 0.0502039, acc 0.984375, prec 0.104923, recall 0.805934
2017-12-10T04:01:37.255527: step 7478, loss 0.154882, acc 0.984375, prec 0.104933, recall 0.805952
2017-12-10T04:01:37.527120: step 7479, loss 0.00300053, acc 1, prec 0.104933, recall 0.805952
2017-12-10T04:01:37.790057: step 7480, loss 0.0966107, acc 0.96875, prec 0.104952, recall 0.805988
2017-12-10T04:01:38.054032: step 7481, loss 0.639307, acc 0.9375, prec 0.104947, recall 0.805988
2017-12-10T04:01:38.318800: step 7482, loss 0.115684, acc 0.96875, prec 0.104956, recall 0.806007
2017-12-10T04:01:38.582939: step 7483, loss 0.570461, acc 0.953125, prec 0.104952, recall 0.806007
2017-12-10T04:01:38.847603: step 7484, loss 0.822555, acc 0.984375, prec 0.104962, recall 0.806025
2017-12-10T04:01:39.124661: step 7485, loss 0.328721, acc 0.96875, prec 0.10497, recall 0.806043
2017-12-10T04:01:39.398026: step 7486, loss 0.237695, acc 0.921875, prec 0.104985, recall 0.806079
2017-12-10T04:01:39.668767: step 7487, loss 0.271491, acc 0.96875, prec 0.104983, recall 0.806079
2017-12-10T04:01:39.937886: step 7488, loss 0.219143, acc 0.953125, prec 0.10499, recall 0.806098
2017-12-10T04:01:40.200562: step 7489, loss 0.987787, acc 0.984375, prec 0.105, recall 0.806116
2017-12-10T04:01:40.470001: step 7490, loss 0.418543, acc 0.953125, prec 0.104996, recall 0.806116
2017-12-10T04:01:40.736463: step 7491, loss 0.0849566, acc 0.984375, prec 0.105016, recall 0.806152
2017-12-10T04:01:41.011315: step 7492, loss 0.335031, acc 0.953125, prec 0.105045, recall 0.806207
2017-12-10T04:01:41.276106: step 7493, loss 0.0983395, acc 0.984375, prec 0.105055, recall 0.806225
2017-12-10T04:01:41.549282: step 7494, loss 0.0154309, acc 1, prec 0.105088, recall 0.806279
2017-12-10T04:01:41.816060: step 7495, loss 0.241786, acc 0.953125, prec 0.105084, recall 0.806279
2017-12-10T04:01:42.084778: step 7496, loss 0.154517, acc 0.953125, prec 0.105102, recall 0.806316
2017-12-10T04:01:42.357448: step 7497, loss 0.155123, acc 0.984375, prec 0.105101, recall 0.806316
2017-12-10T04:01:42.622609: step 7498, loss 0.0495079, acc 0.96875, prec 0.10512, recall 0.806352
2017-12-10T04:01:42.889590: step 7499, loss 0.930244, acc 0.953125, prec 0.105116, recall 0.806352
2017-12-10T04:01:43.162103: step 7500, loss 0.166778, acc 0.953125, prec 0.105134, recall 0.806388

Evaluation:
2017-12-10T04:01:50.717552: step 7500, loss 8.45278, acc 0.954996, prec 0.105418, recall 0.801884

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-7500

2017-12-10T04:01:52.010872: step 7501, loss 0.321437, acc 0.984375, prec 0.105427, recall 0.801902
2017-12-10T04:01:52.286431: step 7502, loss 0.576153, acc 0.921875, prec 0.105421, recall 0.801902
2017-12-10T04:01:52.550741: step 7503, loss 0.013162, acc 0.984375, prec 0.10542, recall 0.801902
2017-12-10T04:01:52.816974: step 7504, loss 0.873232, acc 0.953125, prec 0.105448, recall 0.801957
2017-12-10T04:01:53.083762: step 7505, loss 0.0249295, acc 0.96875, prec 0.105446, recall 0.801957
2017-12-10T04:01:53.354978: step 7506, loss 0.0169167, acc 0.984375, prec 0.105445, recall 0.801957
2017-12-10T04:01:53.621873: step 7507, loss 0.411326, acc 0.953125, prec 0.105441, recall 0.801957
2017-12-10T04:01:53.887365: step 7508, loss 0.151549, acc 0.953125, prec 0.105437, recall 0.801957
2017-12-10T04:01:54.156592: step 7509, loss 0.202882, acc 0.96875, prec 0.105445, recall 0.801976
2017-12-10T04:01:54.422692: step 7510, loss 0.147362, acc 0.984375, prec 0.105466, recall 0.802012
2017-12-10T04:01:54.684460: step 7511, loss 0.364954, acc 0.953125, prec 0.105462, recall 0.802012
2017-12-10T04:01:54.952482: step 7512, loss 0.000364112, acc 1, prec 0.105462, recall 0.802012
2017-12-10T04:01:55.227421: step 7513, loss 0.165305, acc 0.96875, prec 0.105459, recall 0.802012
2017-12-10T04:01:55.496576: step 7514, loss 0.240821, acc 0.984375, prec 0.105469, recall 0.80203
2017-12-10T04:01:55.764571: step 7515, loss 0.541216, acc 0.96875, prec 0.105488, recall 0.802067
2017-12-10T04:01:56.027369: step 7516, loss 5.16432e-06, acc 1, prec 0.105488, recall 0.802067
2017-12-10T04:01:56.297182: step 7517, loss 0.280147, acc 0.953125, prec 0.105484, recall 0.802067
2017-12-10T04:01:56.563650: step 7518, loss 0.00275216, acc 1, prec 0.105484, recall 0.802067
2017-12-10T04:01:56.836559: step 7519, loss 0.097811, acc 0.96875, prec 0.105514, recall 0.802122
2017-12-10T04:01:57.098902: step 7520, loss 0.110436, acc 0.984375, prec 0.105513, recall 0.802122
2017-12-10T04:01:57.364226: step 7521, loss 2.3045, acc 0.953125, prec 0.10551, recall 0.802048
2017-12-10T04:01:57.629850: step 7522, loss 0.00927007, acc 1, prec 0.10551, recall 0.802048
2017-12-10T04:01:57.895013: step 7523, loss 0.502241, acc 0.984375, prec 0.10552, recall 0.802066
2017-12-10T04:01:58.165557: step 7524, loss 0.664102, acc 0.921875, prec 0.105513, recall 0.802066
2017-12-10T04:01:58.434229: step 7525, loss 0.0511149, acc 0.984375, prec 0.105534, recall 0.802103
2017-12-10T04:01:58.703418: step 7526, loss 0.232361, acc 0.9375, prec 0.105529, recall 0.802103
2017-12-10T04:01:58.968215: step 7527, loss 0.35133, acc 0.984375, prec 0.105538, recall 0.802121
2017-12-10T04:01:59.237951: step 7528, loss 0.179468, acc 0.96875, prec 0.105536, recall 0.802121
2017-12-10T04:01:59.511637: step 7529, loss 0.429605, acc 0.984375, prec 0.105556, recall 0.802157
2017-12-10T04:01:59.779184: step 7530, loss 0.0586038, acc 0.984375, prec 0.105566, recall 0.802176
2017-12-10T04:02:00.053102: step 7531, loss 0.0134685, acc 0.984375, prec 0.105586, recall 0.802212
2017-12-10T04:02:00.331924: step 7532, loss 0.0455356, acc 0.984375, prec 0.105628, recall 0.802285
2017-12-10T04:02:00.604479: step 7533, loss 0.266878, acc 0.96875, prec 0.105647, recall 0.802321
2017-12-10T04:02:00.869694: step 7534, loss 0.200922, acc 0.953125, prec 0.105644, recall 0.802321
2017-12-10T04:02:01.132453: step 7535, loss 0.292776, acc 0.9375, prec 0.105649, recall 0.802339
2017-12-10T04:02:01.403504: step 7536, loss 0.116692, acc 0.96875, prec 0.105647, recall 0.802339
2017-12-10T04:02:01.669359: step 7537, loss 0.784134, acc 0.90625, prec 0.105639, recall 0.802339
2017-12-10T04:02:01.934055: step 7538, loss 0.108055, acc 0.96875, prec 0.105637, recall 0.802339
2017-12-10T04:02:02.200562: step 7539, loss 0.200611, acc 0.96875, prec 0.105656, recall 0.802376
2017-12-10T04:02:02.470909: step 7540, loss 0.0203668, acc 0.984375, prec 0.105665, recall 0.802394
2017-12-10T04:02:02.736223: step 7541, loss 0.00176723, acc 1, prec 0.105676, recall 0.802412
2017-12-10T04:02:03.001479: step 7542, loss 0.187508, acc 0.96875, prec 0.105684, recall 0.802431
2017-12-10T04:02:03.263424: step 7543, loss 0.0568159, acc 0.96875, prec 0.105703, recall 0.802467
2017-12-10T04:02:03.524971: step 7544, loss 0.125051, acc 0.984375, prec 0.105702, recall 0.802467
2017-12-10T04:02:03.798675: step 7545, loss 0.0039329, acc 1, prec 0.105724, recall 0.802503
2017-12-10T04:02:04.062968: step 7546, loss 2.65168, acc 0.96875, prec 0.105723, recall 0.802429
2017-12-10T04:02:04.332729: step 7547, loss 0.0851465, acc 0.984375, prec 0.105721, recall 0.802429
2017-12-10T04:02:04.601234: step 7548, loss 0.154482, acc 0.984375, prec 0.105742, recall 0.802466
2017-12-10T04:02:04.870870: step 7549, loss 0.0634526, acc 0.984375, prec 0.10574, recall 0.802466
2017-12-10T04:02:05.133456: step 7550, loss 0.0998167, acc 0.984375, prec 0.105739, recall 0.802466
2017-12-10T04:02:05.400760: step 7551, loss 0.616165, acc 0.90625, prec 0.105742, recall 0.802484
2017-12-10T04:02:05.668124: step 7552, loss 0.0736277, acc 0.984375, prec 0.105763, recall 0.80252
2017-12-10T04:02:05.933714: step 7553, loss 0.375175, acc 0.953125, prec 0.105802, recall 0.802593
2017-12-10T04:02:06.201367: step 7554, loss 0.0386479, acc 0.984375, prec 0.105812, recall 0.802611
2017-12-10T04:02:06.467859: step 7555, loss 0.439172, acc 0.96875, prec 0.10582, recall 0.802629
2017-12-10T04:02:06.734205: step 7556, loss 0.919966, acc 0.90625, prec 0.105812, recall 0.802629
2017-12-10T04:02:06.995145: step 7557, loss 0.0100341, acc 1, prec 0.105823, recall 0.802647
2017-12-10T04:02:07.258834: step 7558, loss 0.525502, acc 0.953125, prec 0.10583, recall 0.802665
2017-12-10T04:02:07.522409: step 7559, loss 0.705401, acc 0.9375, prec 0.105858, recall 0.80272
2017-12-10T04:02:07.790587: step 7560, loss 0.993836, acc 0.921875, prec 0.105851, recall 0.80272
2017-12-10T04:02:08.063232: step 7561, loss 0.149762, acc 0.96875, prec 0.105881, recall 0.802774
2017-12-10T04:02:08.329523: step 7562, loss 0.268599, acc 0.96875, prec 0.105889, recall 0.802792
2017-12-10T04:02:08.594105: step 7563, loss 1.07655, acc 0.890625, prec 0.10588, recall 0.802792
2017-12-10T04:02:08.851379: step 7564, loss 0.999598, acc 0.90625, prec 0.105883, recall 0.80281
2017-12-10T04:02:09.117088: step 7565, loss 0.398753, acc 0.9375, prec 0.105878, recall 0.80281
2017-12-10T04:02:09.388037: step 7566, loss 0.351954, acc 0.953125, prec 0.105875, recall 0.80281
2017-12-10T04:02:09.665716: step 7567, loss 0.0473172, acc 0.96875, prec 0.105872, recall 0.80281
2017-12-10T04:02:09.934175: step 7568, loss 0.183028, acc 0.96875, prec 0.10588, recall 0.802829
2017-12-10T04:02:10.195461: step 7569, loss 0.00571795, acc 1, prec 0.105891, recall 0.802847
2017-12-10T04:02:10.461572: step 7570, loss 0.246445, acc 0.921875, prec 0.105885, recall 0.802847
2017-12-10T04:02:10.734946: step 7571, loss 0.0228543, acc 0.984375, prec 0.105916, recall 0.802901
2017-12-10T04:02:11.000383: step 7572, loss 0.152986, acc 0.984375, prec 0.105925, recall 0.802919
2017-12-10T04:02:11.272087: step 7573, loss 0.13635, acc 0.96875, prec 0.105923, recall 0.802919
2017-12-10T04:02:11.536639: step 7574, loss 0.0738404, acc 0.984375, prec 0.105943, recall 0.802955
2017-12-10T04:02:11.815127: step 7575, loss 0.00146689, acc 1, prec 0.105954, recall 0.802973
2017-12-10T04:02:12.082247: step 7576, loss 0.0346187, acc 0.96875, prec 0.105951, recall 0.802973
2017-12-10T04:02:12.348497: step 7577, loss 0.0342248, acc 0.96875, prec 0.105971, recall 0.803009
2017-12-10T04:02:12.611182: step 7578, loss 0.189817, acc 0.96875, prec 0.105968, recall 0.803009
2017-12-10T04:02:12.880197: step 7579, loss 0.313475, acc 0.96875, prec 0.105987, recall 0.803046
2017-12-10T04:02:13.153739: step 7580, loss 0.000167295, acc 1, prec 0.105987, recall 0.803046
2017-12-10T04:02:13.412529: step 7581, loss 0.0130832, acc 0.984375, prec 0.105997, recall 0.803064
2017-12-10T04:02:13.674567: step 7582, loss 0.00133295, acc 1, prec 0.106007, recall 0.803082
2017-12-10T04:02:13.934925: step 7583, loss 0.745225, acc 0.96875, prec 0.106016, recall 0.8031
2017-12-10T04:02:14.204852: step 7584, loss 0.0012291, acc 1, prec 0.106026, recall 0.803118
2017-12-10T04:02:14.477477: step 7585, loss 0.0587309, acc 0.984375, prec 0.106025, recall 0.803118
2017-12-10T04:02:14.741447: step 7586, loss 0.133455, acc 0.96875, prec 0.106033, recall 0.803136
2017-12-10T04:02:15.005484: step 7587, loss 0.0256484, acc 0.984375, prec 0.106032, recall 0.803136
2017-12-10T04:02:15.272765: step 7588, loss 0.0839189, acc 0.96875, prec 0.10604, recall 0.803154
2017-12-10T04:02:15.534918: step 7589, loss 0.112363, acc 0.953125, prec 0.106037, recall 0.803154
2017-12-10T04:02:15.802464: step 7590, loss 0.0375965, acc 0.984375, prec 0.106046, recall 0.803172
2017-12-10T04:02:16.071294: step 7591, loss 1.41877, acc 0.96875, prec 0.106065, recall 0.803208
2017-12-10T04:02:16.339108: step 7592, loss 0.0866612, acc 0.984375, prec 0.106064, recall 0.803208
2017-12-10T04:02:16.617472: step 7593, loss 0.0015734, acc 1, prec 0.106064, recall 0.803208
2017-12-10T04:02:16.886716: step 7594, loss 0.540132, acc 0.953125, prec 0.106071, recall 0.803226
2017-12-10T04:02:17.159240: step 7595, loss 0.0858866, acc 0.96875, prec 0.106079, recall 0.803244
2017-12-10T04:02:17.424555: step 7596, loss 0.457442, acc 0.953125, prec 0.106086, recall 0.803262
2017-12-10T04:02:17.690795: step 7597, loss 0.184437, acc 0.953125, prec 0.106093, recall 0.80328
2017-12-10T04:02:17.963212: step 7598, loss 0.341431, acc 0.9375, prec 0.106099, recall 0.803298
2017-12-10T04:02:18.235972: step 7599, loss 1.04348, acc 0.953125, prec 0.106106, recall 0.803316
2017-12-10T04:02:18.502427: step 7600, loss 0.324862, acc 0.921875, prec 0.10611, recall 0.803334
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-7600

2017-12-10T04:02:19.733101: step 7601, loss 0.234528, acc 0.953125, prec 0.106117, recall 0.803352
2017-12-10T04:02:20.003002: step 7602, loss 0.168481, acc 0.953125, prec 0.106135, recall 0.803388
2017-12-10T04:02:20.267736: step 7603, loss 0.623863, acc 0.984375, prec 0.106144, recall 0.803406
2017-12-10T04:02:20.536240: step 7604, loss 0.516875, acc 0.90625, prec 0.106169, recall 0.80346
2017-12-10T04:02:20.803664: step 7605, loss 0.219418, acc 0.96875, prec 0.10621, recall 0.803532
2017-12-10T04:02:21.075040: step 7606, loss 0.862597, acc 0.9375, prec 0.106215, recall 0.80355
2017-12-10T04:02:21.344843: step 7607, loss 0.110541, acc 0.953125, prec 0.106255, recall 0.803622
2017-12-10T04:02:21.609707: step 7608, loss 1.53325, acc 0.875, prec 0.106245, recall 0.803622
2017-12-10T04:02:21.883502: step 7609, loss 0.852038, acc 0.9375, prec 0.106239, recall 0.803622
2017-12-10T04:02:22.155737: step 7610, loss 0.574563, acc 0.953125, prec 0.106236, recall 0.803622
2017-12-10T04:02:22.423681: step 7611, loss 0.628592, acc 0.953125, prec 0.106253, recall 0.803658
2017-12-10T04:02:22.696243: step 7612, loss 0.293709, acc 0.9375, prec 0.10627, recall 0.803694
2017-12-10T04:02:22.961432: step 7613, loss 0.0599317, acc 0.96875, prec 0.106267, recall 0.803694
2017-12-10T04:02:23.231191: step 7614, loss 8.79648, acc 0.921875, prec 0.106305, recall 0.803692
2017-12-10T04:02:23.500575: step 7615, loss 0.128915, acc 0.96875, prec 0.106314, recall 0.80371
2017-12-10T04:02:23.775872: step 7616, loss 0.383157, acc 0.96875, prec 0.106343, recall 0.803764
2017-12-10T04:02:24.041871: step 7617, loss 0.791461, acc 0.9375, prec 0.106349, recall 0.803782
2017-12-10T04:02:24.306848: step 7618, loss 0.535485, acc 0.90625, prec 0.106341, recall 0.803782
2017-12-10T04:02:24.580612: step 7619, loss 0.524078, acc 0.875, prec 0.106331, recall 0.803782
2017-12-10T04:02:24.841517: step 7620, loss 0.348555, acc 0.90625, prec 0.106334, recall 0.8038
2017-12-10T04:02:25.106067: step 7621, loss 0.218556, acc 0.921875, prec 0.106339, recall 0.803818
2017-12-10T04:02:25.377681: step 7622, loss 0.615726, acc 0.953125, prec 0.106345, recall 0.803836
2017-12-10T04:02:25.640906: step 7623, loss 0.390059, acc 0.921875, prec 0.106361, recall 0.803871
2017-12-10T04:02:25.914566: step 7624, loss 0.154625, acc 0.953125, prec 0.106378, recall 0.803907
2017-12-10T04:02:26.184019: step 7625, loss 0.210576, acc 0.9375, prec 0.106373, recall 0.803907
2017-12-10T04:02:26.454691: step 7626, loss 0.620545, acc 0.921875, prec 0.106388, recall 0.803943
2017-12-10T04:02:26.725032: step 7627, loss 1.04419, acc 0.875, prec 0.106389, recall 0.803961
2017-12-10T04:02:27.003295: step 7628, loss 0.682242, acc 0.875, prec 0.1064, recall 0.803997
2017-12-10T04:02:27.273606: step 7629, loss 1.19004, acc 0.921875, prec 0.106405, recall 0.804015
2017-12-10T04:02:27.544121: step 7630, loss 0.722578, acc 0.921875, prec 0.106398, recall 0.804015
2017-12-10T04:02:27.809982: step 7631, loss 0.946498, acc 0.921875, prec 0.106403, recall 0.804033
2017-12-10T04:02:28.077690: step 7632, loss 0.748803, acc 0.90625, prec 0.106406, recall 0.80405
2017-12-10T04:02:28.339412: step 7633, loss 0.887255, acc 0.890625, prec 0.106429, recall 0.804104
2017-12-10T04:02:28.603942: step 7634, loss 0.254827, acc 0.96875, prec 0.106426, recall 0.804104
2017-12-10T04:02:28.861496: step 7635, loss 0.00471677, acc 1, prec 0.106448, recall 0.80414
2017-12-10T04:02:29.121684: step 7636, loss 0.687245, acc 0.9375, prec 0.106464, recall 0.804175
2017-12-10T04:02:29.393524: step 7637, loss 0.186947, acc 0.96875, prec 0.106462, recall 0.804175
2017-12-10T04:02:29.659845: step 7638, loss 0.255792, acc 0.9375, prec 0.106457, recall 0.804175
2017-12-10T04:02:29.927841: step 7639, loss 0.322248, acc 0.96875, prec 0.106454, recall 0.804175
2017-12-10T04:02:30.201776: step 7640, loss 0.33609, acc 0.984375, prec 0.106464, recall 0.804193
2017-12-10T04:02:30.481518: step 7641, loss 0.00176522, acc 1, prec 0.106474, recall 0.804211
2017-12-10T04:02:30.747436: step 7642, loss 0.264175, acc 0.953125, prec 0.106492, recall 0.804247
2017-12-10T04:02:31.016492: step 7643, loss 0.090538, acc 0.96875, prec 0.1065, recall 0.804265
2017-12-10T04:02:31.279716: step 7644, loss 0.0111428, acc 1, prec 0.1065, recall 0.804265
2017-12-10T04:02:31.542816: step 7645, loss 0.32266, acc 0.953125, prec 0.106507, recall 0.804282
2017-12-10T04:02:31.807113: step 7646, loss 0.00832623, acc 1, prec 0.106518, recall 0.8043
2017-12-10T04:02:32.071779: step 7647, loss 0.109273, acc 0.96875, prec 0.106526, recall 0.804318
2017-12-10T04:02:32.345073: step 7648, loss 0.0224339, acc 0.984375, prec 0.106525, recall 0.804318
2017-12-10T04:02:32.611765: step 7649, loss 0.179815, acc 0.96875, prec 0.106533, recall 0.804336
2017-12-10T04:02:32.877028: step 7650, loss 0.000855591, acc 1, prec 0.106544, recall 0.804354
2017-12-10T04:02:33.146109: step 7651, loss 0.468991, acc 0.984375, prec 0.106553, recall 0.804372
2017-12-10T04:02:33.412148: step 7652, loss 0.194484, acc 0.984375, prec 0.106552, recall 0.804372
2017-12-10T04:02:33.676655: step 7653, loss 0.180748, acc 0.984375, prec 0.106562, recall 0.804389
2017-12-10T04:02:33.941879: step 7654, loss 0.112682, acc 0.984375, prec 0.10656, recall 0.804389
2017-12-10T04:02:34.204553: step 7655, loss 0.248485, acc 0.96875, prec 0.106579, recall 0.804425
2017-12-10T04:02:34.470353: step 7656, loss 0.000306147, acc 1, prec 0.106579, recall 0.804425
2017-12-10T04:02:34.730607: step 7657, loss 0.429097, acc 0.9375, prec 0.106585, recall 0.804443
2017-12-10T04:02:34.992959: step 7658, loss 0.0659496, acc 0.96875, prec 0.106625, recall 0.804514
2017-12-10T04:02:35.255952: step 7659, loss 0.00152136, acc 1, prec 0.106647, recall 0.80455
2017-12-10T04:02:35.525081: step 7660, loss 0.258018, acc 0.96875, prec 0.106644, recall 0.80455
2017-12-10T04:02:35.788462: step 7661, loss 0.10256, acc 0.984375, prec 0.106643, recall 0.80455
2017-12-10T04:02:36.054948: step 7662, loss 5.08508, acc 0.9375, prec 0.106651, recall 0.804421
2017-12-10T04:02:36.325241: step 7663, loss 0.00228942, acc 1, prec 0.106651, recall 0.804421
2017-12-10T04:02:36.584838: step 7664, loss 0.0842197, acc 0.96875, prec 0.10666, recall 0.804439
2017-12-10T04:02:36.849313: step 7665, loss 0.00633031, acc 1, prec 0.10666, recall 0.804439
2017-12-10T04:02:37.117084: step 7666, loss 0.195918, acc 0.953125, prec 0.106667, recall 0.804457
2017-12-10T04:02:37.384747: step 7667, loss 0.29697, acc 0.921875, prec 0.106671, recall 0.804474
2017-12-10T04:02:37.648643: step 7668, loss 0.392588, acc 0.9375, prec 0.106666, recall 0.804474
2017-12-10T04:02:37.912191: step 7669, loss 0.204703, acc 0.90625, prec 0.10668, recall 0.80451
2017-12-10T04:02:38.178577: step 7670, loss 0.836876, acc 0.875, prec 0.10668, recall 0.804528
2017-12-10T04:02:38.446593: step 7671, loss 0.457554, acc 0.921875, prec 0.106684, recall 0.804545
2017-12-10T04:02:38.714705: step 7672, loss 0.71062, acc 0.890625, prec 0.106686, recall 0.804563
2017-12-10T04:02:39.693394: step 7673, loss 0.710129, acc 0.796875, prec 0.10668, recall 0.804581
2017-12-10T04:02:40.049184: step 7674, loss 0.631504, acc 0.890625, prec 0.106693, recall 0.804617
2017-12-10T04:02:40.476722: step 7675, loss 0.834844, acc 0.859375, prec 0.106703, recall 0.804652
2017-12-10T04:02:41.227368: step 7676, loss 0.769626, acc 0.859375, prec 0.106691, recall 0.804652
2017-12-10T04:02:41.982230: step 7677, loss 1.56087, acc 0.828125, prec 0.106677, recall 0.804652
2017-12-10T04:02:42.714304: step 7678, loss 0.693666, acc 0.90625, prec 0.10668, recall 0.80467
2017-12-10T04:02:43.419456: step 7679, loss 0.715792, acc 0.859375, prec 0.106668, recall 0.80467
2017-12-10T04:02:44.119704: step 7680, loss 0.333817, acc 0.9375, prec 0.106663, recall 0.80467
2017-12-10T04:02:44.886309: step 7681, loss 0.262264, acc 0.90625, prec 0.106666, recall 0.804688
2017-12-10T04:02:45.615388: step 7682, loss 1.01829, acc 0.859375, prec 0.106655, recall 0.804688
2017-12-10T04:02:46.575195: step 7683, loss 0.579381, acc 0.921875, prec 0.106648, recall 0.804688
2017-12-10T04:02:46.908142: step 7684, loss 0.103778, acc 0.96875, prec 0.106657, recall 0.804705
2017-12-10T04:02:47.248122: step 7685, loss 0.115133, acc 0.953125, prec 0.106653, recall 0.804705
2017-12-10T04:02:47.531959: step 7686, loss 0.186649, acc 0.953125, prec 0.106649, recall 0.804705
2017-12-10T04:02:47.807802: step 7687, loss 1.05033, acc 0.921875, prec 0.106664, recall 0.804741
2017-12-10T04:02:48.073991: step 7688, loss 0.0183526, acc 0.984375, prec 0.106673, recall 0.804758
2017-12-10T04:02:48.341368: step 7689, loss 0.000236838, acc 1, prec 0.106684, recall 0.804776
2017-12-10T04:02:48.601447: step 7690, loss 0.000747862, acc 1, prec 0.106695, recall 0.804794
2017-12-10T04:02:48.860557: step 7691, loss 0.0141101, acc 0.984375, prec 0.106694, recall 0.804794
2017-12-10T04:02:49.125599: step 7692, loss 0.00163926, acc 1, prec 0.106704, recall 0.804812
2017-12-10T04:02:49.405656: step 7693, loss 0.244478, acc 0.96875, prec 0.106723, recall 0.804847
2017-12-10T04:02:49.670476: step 7694, loss 0.110219, acc 0.984375, prec 0.106733, recall 0.804865
2017-12-10T04:02:49.941279: step 7695, loss 0.13462, acc 0.984375, prec 0.106753, recall 0.8049
2017-12-10T04:02:50.208737: step 7696, loss 1.60298e-05, acc 1, prec 0.106753, recall 0.8049
2017-12-10T04:02:50.467500: step 7697, loss 0.122204, acc 0.984375, prec 0.106762, recall 0.804918
2017-12-10T04:02:50.730181: step 7698, loss 0.000333843, acc 1, prec 0.106762, recall 0.804918
2017-12-10T04:02:50.991771: step 7699, loss 0.000245568, acc 1, prec 0.106784, recall 0.804953
2017-12-10T04:02:51.255665: step 7700, loss 0.245947, acc 1, prec 0.106805, recall 0.804989
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-7700

2017-12-10T04:02:52.564103: step 7701, loss 0.118231, acc 0.984375, prec 0.106804, recall 0.804989
2017-12-10T04:02:52.827539: step 7702, loss 7.67705, acc 0.984375, prec 0.106815, recall 0.804933
2017-12-10T04:02:53.107665: step 7703, loss 0.16971, acc 0.953125, prec 0.106811, recall 0.804933
2017-12-10T04:02:53.372805: step 7704, loss 0.300482, acc 0.96875, prec 0.106819, recall 0.804951
2017-12-10T04:02:53.639816: step 7705, loss 0.126147, acc 0.984375, prec 0.106829, recall 0.804969
2017-12-10T04:02:53.905546: step 7706, loss 0.177278, acc 0.96875, prec 0.106837, recall 0.804986
2017-12-10T04:02:54.173338: step 7707, loss 0.026893, acc 0.984375, prec 0.106846, recall 0.805004
2017-12-10T04:02:54.439090: step 7708, loss 0.154031, acc 0.9375, prec 0.106841, recall 0.805004
2017-12-10T04:02:54.705691: step 7709, loss 0.102075, acc 0.96875, prec 0.106839, recall 0.805004
2017-12-10T04:02:54.986251: step 7710, loss 0.0931346, acc 0.984375, prec 0.106848, recall 0.805022
2017-12-10T04:02:55.253626: step 7711, loss 0.0848488, acc 0.96875, prec 0.106856, recall 0.805039
2017-12-10T04:02:55.518755: step 7712, loss 0.132992, acc 0.96875, prec 0.106875, recall 0.805075
2017-12-10T04:02:55.781952: step 7713, loss 0.153763, acc 0.984375, prec 0.106885, recall 0.805092
2017-12-10T04:02:56.044561: step 7714, loss 0.347748, acc 0.953125, prec 0.106881, recall 0.805092
2017-12-10T04:02:56.311151: step 7715, loss 0.113698, acc 0.984375, prec 0.10688, recall 0.805092
2017-12-10T04:02:56.577599: step 7716, loss 1.13659, acc 0.921875, prec 0.106884, recall 0.80511
2017-12-10T04:02:56.846471: step 7717, loss 0.136381, acc 0.953125, prec 0.106901, recall 0.805145
2017-12-10T04:02:57.117100: step 7718, loss 0.271872, acc 0.984375, prec 0.106911, recall 0.805163
2017-12-10T04:02:57.387174: step 7719, loss 1.82516, acc 0.890625, prec 0.106902, recall 0.805163
2017-12-10T04:02:57.649576: step 7720, loss 0.487827, acc 0.90625, prec 0.106905, recall 0.805181
2017-12-10T04:02:57.915981: step 7721, loss 0.405656, acc 0.9375, prec 0.106911, recall 0.805198
2017-12-10T04:02:58.187826: step 7722, loss 0.140191, acc 0.96875, prec 0.106919, recall 0.805216
2017-12-10T04:02:58.452586: step 7723, loss 0.0304922, acc 0.984375, prec 0.106928, recall 0.805234
2017-12-10T04:02:58.721533: step 7724, loss 0.309155, acc 0.9375, prec 0.106923, recall 0.805234
2017-12-10T04:02:58.991049: step 7725, loss 0.351617, acc 0.96875, prec 0.106931, recall 0.805251
2017-12-10T04:02:59.263439: step 7726, loss 0.115295, acc 0.953125, prec 0.106927, recall 0.805251
2017-12-10T04:02:59.533838: step 7727, loss 0.258923, acc 0.921875, prec 0.106921, recall 0.805251
2017-12-10T04:02:59.805020: step 7728, loss 0.223357, acc 0.96875, prec 0.106929, recall 0.805269
2017-12-10T04:03:00.069193: step 7729, loss 0.546207, acc 0.953125, prec 0.106925, recall 0.805269
2017-12-10T04:03:00.342712: step 7730, loss 0.0482073, acc 0.984375, prec 0.106935, recall 0.805287
2017-12-10T04:03:00.622350: step 7731, loss 0.12586, acc 0.96875, prec 0.106932, recall 0.805287
2017-12-10T04:03:00.898476: step 7732, loss 0.150538, acc 0.984375, prec 0.106952, recall 0.805322
2017-12-10T04:03:01.164893: step 7733, loss 0.0853059, acc 0.96875, prec 0.10695, recall 0.805322
2017-12-10T04:03:01.425232: step 7734, loss 0.0240634, acc 0.984375, prec 0.10697, recall 0.805357
2017-12-10T04:03:01.690304: step 7735, loss 0.510995, acc 0.9375, prec 0.106986, recall 0.805392
2017-12-10T04:03:01.953949: step 7736, loss 0.304687, acc 0.953125, prec 0.106982, recall 0.805392
2017-12-10T04:03:02.219618: step 7737, loss 0.350057, acc 0.984375, prec 0.106981, recall 0.805392
2017-12-10T04:03:02.486418: step 7738, loss 0.0222047, acc 1, prec 0.107003, recall 0.805427
2017-12-10T04:03:02.760272: step 7739, loss 0.122918, acc 1, prec 0.107024, recall 0.805463
2017-12-10T04:03:03.025029: step 7740, loss 0.0611071, acc 0.984375, prec 0.107055, recall 0.805515
2017-12-10T04:03:03.287692: step 7741, loss 0.00268736, acc 1, prec 0.107066, recall 0.805533
2017-12-10T04:03:03.556940: step 7742, loss 0.59317, acc 1, prec 0.107087, recall 0.805568
2017-12-10T04:03:03.828051: step 7743, loss 0.0221175, acc 0.984375, prec 0.107097, recall 0.805586
2017-12-10T04:03:04.096655: step 7744, loss 6.56058, acc 0.984375, prec 0.107107, recall 0.80553
2017-12-10T04:03:04.370087: step 7745, loss 0.156103, acc 0.96875, prec 0.107115, recall 0.805548
2017-12-10T04:03:04.649578: step 7746, loss 0.103143, acc 0.984375, prec 0.107114, recall 0.805548
2017-12-10T04:03:04.915483: step 7747, loss 0.0967372, acc 0.96875, prec 0.107112, recall 0.805548
2017-12-10T04:03:05.179600: step 7748, loss 0.120933, acc 0.953125, prec 0.107108, recall 0.805548
2017-12-10T04:03:05.445497: step 7749, loss 0.0640358, acc 0.984375, prec 0.107117, recall 0.805566
2017-12-10T04:03:05.714187: step 7750, loss 0.399827, acc 0.890625, prec 0.10713, recall 0.805601
2017-12-10T04:03:05.991594: step 7751, loss 0.262927, acc 0.9375, prec 0.107124, recall 0.805601
2017-12-10T04:03:06.260447: step 7752, loss 0.250221, acc 0.953125, prec 0.107131, recall 0.805618
2017-12-10T04:03:06.526687: step 7753, loss 0.600828, acc 0.875, prec 0.107132, recall 0.805636
2017-12-10T04:03:06.792848: step 7754, loss 0.70485, acc 0.875, prec 0.107121, recall 0.805636
2017-12-10T04:03:07.060078: step 7755, loss 0.429861, acc 0.953125, prec 0.107118, recall 0.805636
2017-12-10T04:03:07.324648: step 7756, loss 0.581567, acc 0.890625, prec 0.107141, recall 0.805689
2017-12-10T04:03:07.586899: step 7757, loss 0.614908, acc 0.890625, prec 0.107132, recall 0.805689
2017-12-10T04:03:07.848233: step 7758, loss 0.458011, acc 0.921875, prec 0.107136, recall 0.805706
2017-12-10T04:03:08.119500: step 7759, loss 0.599123, acc 0.90625, prec 0.10716, recall 0.805759
2017-12-10T04:03:08.386644: step 7760, loss 0.132032, acc 0.953125, prec 0.107167, recall 0.805776
2017-12-10T04:03:08.652847: step 7761, loss 0.586092, acc 0.921875, prec 0.107161, recall 0.805776
2017-12-10T04:03:08.918298: step 7762, loss 0.150956, acc 0.9375, prec 0.107166, recall 0.805794
2017-12-10T04:03:09.179268: step 7763, loss 0.52534, acc 0.875, prec 0.107167, recall 0.805811
2017-12-10T04:03:09.446302: step 7764, loss 0.605325, acc 0.921875, prec 0.107171, recall 0.805829
2017-12-10T04:03:09.721170: step 7765, loss 0.362525, acc 0.9375, prec 0.107177, recall 0.805846
2017-12-10T04:03:09.996173: step 7766, loss 0.252669, acc 0.953125, prec 0.107184, recall 0.805864
2017-12-10T04:03:10.270040: step 7767, loss 0.0678557, acc 0.96875, prec 0.107192, recall 0.805881
2017-12-10T04:03:10.538400: step 7768, loss 0.274529, acc 0.96875, prec 0.107189, recall 0.805881
2017-12-10T04:03:10.802640: step 7769, loss 0.000470201, acc 1, prec 0.1072, recall 0.805899
2017-12-10T04:03:11.069505: step 7770, loss 0.276906, acc 0.9375, prec 0.107216, recall 0.805934
2017-12-10T04:03:11.336867: step 7771, loss 0.56507, acc 0.921875, prec 0.10721, recall 0.805934
2017-12-10T04:03:11.600872: step 7772, loss 0.270043, acc 0.953125, prec 0.107206, recall 0.805934
2017-12-10T04:03:11.879665: step 7773, loss 0.00300155, acc 1, prec 0.107217, recall 0.805951
2017-12-10T04:03:12.146771: step 7774, loss 0.039101, acc 0.984375, prec 0.107237, recall 0.805986
2017-12-10T04:03:12.411692: step 7775, loss 0.0379224, acc 0.984375, prec 0.107235, recall 0.805986
2017-12-10T04:03:12.677025: step 7776, loss 0.275052, acc 0.921875, prec 0.10724, recall 0.806004
2017-12-10T04:03:12.948350: step 7777, loss 0.000898442, acc 1, prec 0.10724, recall 0.806004
2017-12-10T04:03:13.213170: step 7778, loss 0.050549, acc 0.984375, prec 0.107238, recall 0.806004
2017-12-10T04:03:13.475593: step 7779, loss 0.00623964, acc 1, prec 0.10726, recall 0.806039
2017-12-10T04:03:13.739389: step 7780, loss 0.595389, acc 0.96875, prec 0.107268, recall 0.806056
2017-12-10T04:03:14.007991: step 7781, loss 0.203842, acc 0.953125, prec 0.107264, recall 0.806056
2017-12-10T04:03:14.273466: step 7782, loss 0.00140917, acc 1, prec 0.107275, recall 0.806074
2017-12-10T04:03:14.533881: step 7783, loss 0.917327, acc 0.96875, prec 0.107272, recall 0.806074
2017-12-10T04:03:14.809563: step 7784, loss 0.0637169, acc 0.984375, prec 0.107292, recall 0.806109
2017-12-10T04:03:15.074993: step 7785, loss 0.0677989, acc 0.984375, prec 0.107302, recall 0.806126
2017-12-10T04:03:15.343378: step 7786, loss 0.326646, acc 0.953125, prec 0.107309, recall 0.806144
2017-12-10T04:03:15.614767: step 7787, loss 0.174097, acc 0.96875, prec 0.107317, recall 0.806161
2017-12-10T04:03:15.883994: step 7788, loss 0.0492711, acc 0.984375, prec 0.107326, recall 0.806179
2017-12-10T04:03:16.152006: step 7789, loss 0.0072216, acc 1, prec 0.107358, recall 0.806231
2017-12-10T04:03:16.416420: step 7790, loss 0.000485964, acc 1, prec 0.10738, recall 0.806266
2017-12-10T04:03:16.687186: step 7791, loss 0.00917486, acc 1, prec 0.107401, recall 0.806301
2017-12-10T04:03:16.951245: step 7792, loss 6.30427e-05, acc 1, prec 0.107401, recall 0.806301
2017-12-10T04:03:17.218569: step 7793, loss 0.147532, acc 0.953125, prec 0.107408, recall 0.806318
2017-12-10T04:03:17.487117: step 7794, loss 0.0857687, acc 0.984375, prec 0.107407, recall 0.806318
2017-12-10T04:03:17.758042: step 7795, loss 0.0139728, acc 1, prec 0.107428, recall 0.806353
2017-12-10T04:03:18.027999: step 7796, loss 13.9156, acc 0.953125, prec 0.107436, recall 0.806298
2017-12-10T04:03:18.304949: step 7797, loss 7.19419, acc 0.96875, prec 0.107446, recall 0.806243
2017-12-10T04:03:18.576103: step 7798, loss 0.166296, acc 0.96875, prec 0.107454, recall 0.80626
2017-12-10T04:03:18.838987: step 7799, loss 0.163828, acc 0.953125, prec 0.10745, recall 0.80626
2017-12-10T04:03:19.111729: step 7800, loss 0.0553191, acc 0.984375, prec 0.107491, recall 0.80633

Evaluation:
2017-12-10T04:03:26.633865: step 7800, loss 4.96223, acc 0.88565, prec 0.10709, recall 0.804577

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-7800

2017-12-10T04:03:27.955112: step 7801, loss 0.978974, acc 0.890625, prec 0.107102, recall 0.804612
2017-12-10T04:03:28.219864: step 7802, loss 0.729078, acc 0.859375, prec 0.107091, recall 0.804612
2017-12-10T04:03:29.193129: step 7803, loss 2.34221, acc 0.78125, prec 0.107084, recall 0.804629
2017-12-10T04:03:29.543148: step 7804, loss 0.856946, acc 0.875, prec 0.107074, recall 0.804629
2017-12-10T04:03:30.161551: step 7805, loss 1.52114, acc 0.78125, prec 0.107056, recall 0.804629
2017-12-10T04:03:30.923418: step 7806, loss 1.00851, acc 0.828125, prec 0.107063, recall 0.804664
2017-12-10T04:03:31.669546: step 7807, loss 1.27425, acc 0.828125, prec 0.10706, recall 0.804681
2017-12-10T04:03:32.374073: step 7808, loss 2.41307, acc 0.703125, prec 0.107036, recall 0.804681
2017-12-10T04:03:33.127851: step 7809, loss 2.7649, acc 0.703125, prec 0.107022, recall 0.804699
2017-12-10T04:03:33.826129: step 7810, loss 1.12274, acc 0.84375, prec 0.10701, recall 0.804699
2017-12-10T04:03:34.562899: step 7811, loss 1.3885, acc 0.890625, prec 0.107012, recall 0.804716
2017-12-10T04:03:35.298739: step 7812, loss 0.925695, acc 0.890625, prec 0.107013, recall 0.804733
2017-12-10T04:03:36.052558: step 7813, loss 0.58974, acc 0.859375, prec 0.107033, recall 0.804785
2017-12-10T04:03:36.770696: step 7814, loss 0.809363, acc 0.875, prec 0.107023, recall 0.804785
2017-12-10T04:03:37.535440: step 7815, loss 1.253, acc 0.859375, prec 0.107012, recall 0.804785
2017-12-10T04:03:38.262253: step 7816, loss 1.01298, acc 0.890625, prec 0.107003, recall 0.804785
2017-12-10T04:03:39.210889: step 7817, loss 0.489621, acc 0.921875, prec 0.107028, recall 0.804837
2017-12-10T04:03:39.653659: step 7818, loss 0.354627, acc 0.9375, prec 0.107034, recall 0.804854
2017-12-10T04:03:39.937117: step 7819, loss 0.883983, acc 0.9375, prec 0.107029, recall 0.804854
2017-12-10T04:03:40.230780: step 7820, loss 0.219658, acc 0.921875, prec 0.107023, recall 0.804854
2017-12-10T04:03:40.524675: step 7821, loss 0.268575, acc 0.921875, prec 0.107037, recall 0.804889
2017-12-10T04:03:40.820494: step 7822, loss 0.000360853, acc 1, prec 0.107058, recall 0.804923
2017-12-10T04:03:41.104638: step 7823, loss 0.568058, acc 0.96875, prec 0.107056, recall 0.804923
2017-12-10T04:03:41.377011: step 7824, loss 0.126989, acc 0.984375, prec 0.107055, recall 0.804923
2017-12-10T04:03:41.658402: step 7825, loss 0.901339, acc 0.96875, prec 0.107063, recall 0.804941
2017-12-10T04:03:41.938152: step 7826, loss 0.611317, acc 0.953125, prec 0.107059, recall 0.804941
2017-12-10T04:03:42.202324: step 7827, loss 0.0310835, acc 0.984375, prec 0.107057, recall 0.804941
2017-12-10T04:03:42.473078: step 7828, loss 0.466137, acc 0.953125, prec 0.107064, recall 0.804958
2017-12-10T04:03:42.736625: step 7829, loss 0.0313933, acc 0.984375, prec 0.107063, recall 0.804958
2017-12-10T04:03:43.006018: step 7830, loss 0.107546, acc 0.96875, prec 0.10706, recall 0.804958
2017-12-10T04:03:43.271930: step 7831, loss 0.31677, acc 0.953125, prec 0.107057, recall 0.804958
2017-12-10T04:03:43.535636: step 7832, loss 0.219762, acc 0.953125, prec 0.107053, recall 0.804958
2017-12-10T04:03:43.799665: step 7833, loss 0.0371049, acc 0.984375, prec 0.107052, recall 0.804958
2017-12-10T04:03:44.066013: step 7834, loss 0.000479663, acc 1, prec 0.107062, recall 0.804975
2017-12-10T04:03:44.330403: step 7835, loss 22.5044, acc 0.96875, prec 0.107061, recall 0.804904
2017-12-10T04:03:44.600558: step 7836, loss 0.123181, acc 0.984375, prec 0.107081, recall 0.804938
2017-12-10T04:03:44.874659: step 7837, loss 0.12548, acc 0.96875, prec 0.107078, recall 0.804938
2017-12-10T04:03:45.145524: step 7838, loss 3.15548e-05, acc 1, prec 0.107078, recall 0.804938
2017-12-10T04:03:45.409276: step 7839, loss 0.0214892, acc 0.984375, prec 0.107077, recall 0.804938
2017-12-10T04:03:45.679661: step 7840, loss 0.182262, acc 0.96875, prec 0.107095, recall 0.804973
2017-12-10T04:03:45.941412: step 7841, loss 0.131019, acc 0.96875, prec 0.107103, recall 0.80499
2017-12-10T04:03:46.217659: step 7842, loss 0.00611538, acc 1, prec 0.107114, recall 0.805008
2017-12-10T04:03:46.478280: step 7843, loss 0.235969, acc 0.9375, prec 0.107109, recall 0.805008
2017-12-10T04:03:46.741787: step 7844, loss 0.331335, acc 0.9375, prec 0.107125, recall 0.805042
2017-12-10T04:03:47.002024: step 7845, loss 0.00672382, acc 1, prec 0.107135, recall 0.805059
2017-12-10T04:03:47.266906: step 7846, loss 0.233585, acc 0.953125, prec 0.107132, recall 0.805059
2017-12-10T04:03:47.533033: step 7847, loss 0.590945, acc 0.921875, prec 0.107125, recall 0.805059
2017-12-10T04:03:47.796216: step 7848, loss 0.0479861, acc 0.984375, prec 0.107124, recall 0.805059
2017-12-10T04:03:48.063403: step 7849, loss 1.56211, acc 0.921875, prec 0.107128, recall 0.805076
2017-12-10T04:03:48.328678: step 7850, loss 0.211026, acc 0.921875, prec 0.107122, recall 0.805076
2017-12-10T04:03:48.593557: step 7851, loss 0.417543, acc 0.953125, prec 0.107129, recall 0.805094
2017-12-10T04:03:48.858395: step 7852, loss 0.391169, acc 0.953125, prec 0.107167, recall 0.805163
2017-12-10T04:03:49.127737: step 7853, loss 0.312488, acc 0.953125, prec 0.107174, recall 0.80518
2017-12-10T04:03:49.393105: step 7854, loss 0.122417, acc 0.953125, prec 0.107191, recall 0.805214
2017-12-10T04:03:49.651403: step 7855, loss 0.612145, acc 0.90625, prec 0.107204, recall 0.805249
2017-12-10T04:03:49.919228: step 7856, loss 0.0655781, acc 0.984375, prec 0.107213, recall 0.805266
2017-12-10T04:03:50.182658: step 7857, loss 0.341116, acc 0.96875, prec 0.107221, recall 0.805283
2017-12-10T04:03:50.444852: step 7858, loss 0.286736, acc 0.953125, prec 0.107218, recall 0.805283
2017-12-10T04:03:50.708012: step 7859, loss 0.490136, acc 0.9375, prec 0.107223, recall 0.8053
2017-12-10T04:03:50.975694: step 7860, loss 0.0380232, acc 0.984375, prec 0.107253, recall 0.805352
2017-12-10T04:03:51.236451: step 7861, loss 3.28488, acc 0.9375, prec 0.10726, recall 0.805298
2017-12-10T04:03:51.508895: step 7862, loss 0.348324, acc 0.9375, prec 0.107266, recall 0.805315
2017-12-10T04:03:51.775036: step 7863, loss 0.414125, acc 0.953125, prec 0.107304, recall 0.805384
2017-12-10T04:03:52.039143: step 7864, loss 1.51431, acc 0.953125, prec 0.107301, recall 0.805313
2017-12-10T04:03:52.308933: step 7865, loss 1.0612, acc 0.96875, prec 0.107309, recall 0.80533
2017-12-10T04:03:52.577742: step 7866, loss 1.39599, acc 0.890625, prec 0.107311, recall 0.805347
2017-12-10T04:03:52.846393: step 7867, loss 0.635208, acc 0.9375, prec 0.107306, recall 0.805347
2017-12-10T04:03:53.110743: step 7868, loss 0.514413, acc 0.890625, prec 0.107307, recall 0.805364
2017-12-10T04:03:53.381595: step 7869, loss 0.468216, acc 0.9375, prec 0.107313, recall 0.805382
2017-12-10T04:03:53.646391: step 7870, loss 1.03723, acc 0.90625, prec 0.107305, recall 0.805382
2017-12-10T04:03:53.906680: step 7871, loss 0.556871, acc 0.90625, prec 0.107298, recall 0.805382
2017-12-10T04:03:54.169133: step 7872, loss 0.96892, acc 0.90625, prec 0.107322, recall 0.805433
2017-12-10T04:03:54.442657: step 7873, loss 0.599803, acc 0.890625, prec 0.107313, recall 0.805433
2017-12-10T04:03:54.708135: step 7874, loss 0.573901, acc 0.90625, prec 0.107305, recall 0.805433
2017-12-10T04:03:54.977741: step 7875, loss 0.962454, acc 0.859375, prec 0.107294, recall 0.805433
2017-12-10T04:03:55.245622: step 7876, loss 1.1564, acc 0.875, prec 0.107305, recall 0.805467
2017-12-10T04:03:55.510233: step 7877, loss 0.633356, acc 0.875, prec 0.107295, recall 0.805467
2017-12-10T04:03:55.778937: step 7878, loss 0.897549, acc 0.875, prec 0.107295, recall 0.805485
2017-12-10T04:03:56.044578: step 7879, loss 0.395564, acc 0.890625, prec 0.107286, recall 0.805485
2017-12-10T04:03:56.311140: step 7880, loss 0.771023, acc 0.859375, prec 0.107285, recall 0.805502
2017-12-10T04:03:56.578287: step 7881, loss 0.442852, acc 0.921875, prec 0.107279, recall 0.805502
2017-12-10T04:03:56.852699: step 7882, loss 0.497036, acc 0.921875, prec 0.107273, recall 0.805502
2017-12-10T04:03:57.117998: step 7883, loss 1.14888, acc 0.953125, prec 0.107269, recall 0.805502
2017-12-10T04:03:57.382510: step 7884, loss 0.250942, acc 0.96875, prec 0.107267, recall 0.805502
2017-12-10T04:03:57.652659: step 7885, loss 0.00443231, acc 1, prec 0.107267, recall 0.805502
2017-12-10T04:03:57.914412: step 7886, loss 0.180253, acc 0.953125, prec 0.107263, recall 0.805502
2017-12-10T04:03:58.177624: step 7887, loss 1.01382, acc 0.890625, prec 0.107275, recall 0.805536
2017-12-10T04:03:58.449616: step 7888, loss 0.290065, acc 0.953125, prec 0.107282, recall 0.805553
2017-12-10T04:03:58.720157: step 7889, loss 0.337675, acc 0.96875, prec 0.107321, recall 0.805622
2017-12-10T04:03:58.988808: step 7890, loss 0.206126, acc 0.96875, prec 0.107329, recall 0.805639
2017-12-10T04:03:59.254291: step 7891, loss 0.282501, acc 0.953125, prec 0.107336, recall 0.805656
2017-12-10T04:03:59.525713: step 7892, loss 0.192558, acc 0.9375, prec 0.107331, recall 0.805656
2017-12-10T04:03:59.792759: step 7893, loss 0.0617656, acc 0.984375, prec 0.107329, recall 0.805656
2017-12-10T04:04:00.056202: step 7894, loss 0.112556, acc 0.984375, prec 0.107328, recall 0.805656
2017-12-10T04:04:00.324258: step 7895, loss 0.100498, acc 1, prec 0.107339, recall 0.805673
2017-12-10T04:04:00.592909: step 7896, loss 0.170006, acc 0.953125, prec 0.107345, recall 0.80569
2017-12-10T04:04:00.865705: step 7897, loss 0.104071, acc 0.984375, prec 0.107355, recall 0.805707
2017-12-10T04:04:01.129920: step 7898, loss 0.376018, acc 0.96875, prec 0.107373, recall 0.805741
2017-12-10T04:04:01.398733: step 7899, loss 0.0649936, acc 0.96875, prec 0.107381, recall 0.805759
2017-12-10T04:04:01.663623: step 7900, loss 0.00567996, acc 1, prec 0.107402, recall 0.805793
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-7900

2017-12-10T04:04:02.966135: step 7901, loss 0.0571514, acc 0.984375, prec 0.107401, recall 0.805793
2017-12-10T04:04:03.241594: step 7902, loss 2.81265e-05, acc 1, prec 0.107411, recall 0.80581
2017-12-10T04:04:03.501724: step 7903, loss 0.00181239, acc 1, prec 0.107422, recall 0.805827
2017-12-10T04:04:03.765367: step 7904, loss 0.141115, acc 0.96875, prec 0.107419, recall 0.805827
2017-12-10T04:04:04.023730: step 7905, loss 0.00571027, acc 1, prec 0.107419, recall 0.805827
2017-12-10T04:04:04.286572: step 7906, loss 0.000555126, acc 1, prec 0.107419, recall 0.805827
2017-12-10T04:04:04.546483: step 7907, loss 0.201229, acc 0.984375, prec 0.107428, recall 0.805844
2017-12-10T04:04:04.806918: step 7908, loss 0.57206, acc 1, prec 0.107449, recall 0.805878
2017-12-10T04:04:05.073835: step 7909, loss 0.117449, acc 0.984375, prec 0.107448, recall 0.805878
2017-12-10T04:04:05.339394: step 7910, loss 0.0142212, acc 0.984375, prec 0.107457, recall 0.805895
2017-12-10T04:04:05.610387: step 7911, loss 0.0535901, acc 0.984375, prec 0.107466, recall 0.805912
2017-12-10T04:04:05.874318: step 7912, loss 0.443116, acc 0.984375, prec 0.107465, recall 0.805912
2017-12-10T04:04:06.138348: step 7913, loss 0.150602, acc 0.9375, prec 0.10747, recall 0.805929
2017-12-10T04:04:06.400634: step 7914, loss 0.0452645, acc 0.984375, prec 0.107469, recall 0.805929
2017-12-10T04:04:06.666232: step 7915, loss 0.15447, acc 0.984375, prec 0.107499, recall 0.805981
2017-12-10T04:04:06.928993: step 7916, loss 0.084181, acc 0.984375, prec 0.107498, recall 0.805981
2017-12-10T04:04:07.194807: step 7917, loss 0.439277, acc 0.953125, prec 0.107505, recall 0.805998
2017-12-10T04:04:07.459972: step 7918, loss 0.0405346, acc 0.984375, prec 0.107514, recall 0.806015
2017-12-10T04:04:07.721369: step 7919, loss 0.0322973, acc 0.984375, prec 0.107513, recall 0.806015
2017-12-10T04:04:07.986313: step 7920, loss 0.0148168, acc 0.984375, prec 0.107511, recall 0.806015
2017-12-10T04:04:08.257685: step 7921, loss 0.0992002, acc 0.984375, prec 0.10751, recall 0.806015
2017-12-10T04:04:08.517924: step 7922, loss 0.0903299, acc 0.984375, prec 0.107509, recall 0.806015
2017-12-10T04:04:08.793075: step 7923, loss 0.987444, acc 0.96875, prec 0.107538, recall 0.806066
2017-12-10T04:04:09.058070: step 7924, loss 0.0357163, acc 0.984375, prec 0.107537, recall 0.806066
2017-12-10T04:04:09.325784: step 7925, loss 0.192649, acc 0.953125, prec 0.107533, recall 0.806066
2017-12-10T04:04:09.594179: step 7926, loss 0.0153634, acc 0.984375, prec 0.107542, recall 0.806083
2017-12-10T04:04:09.862970: step 7927, loss 0.0104023, acc 1, prec 0.107552, recall 0.8061
2017-12-10T04:04:10.131805: step 7928, loss 1.02281, acc 0.9375, prec 0.107558, recall 0.806117
2017-12-10T04:04:10.399779: step 7929, loss 0.202001, acc 0.953125, prec 0.107565, recall 0.806134
2017-12-10T04:04:10.663690: step 7930, loss 0.0843253, acc 0.984375, prec 0.107584, recall 0.806168
2017-12-10T04:04:10.931365: step 7931, loss 0.197118, acc 0.96875, prec 0.107592, recall 0.806185
2017-12-10T04:04:11.193766: step 7932, loss 0.0077127, acc 1, prec 0.107592, recall 0.806185
2017-12-10T04:04:11.456232: step 7933, loss 1.33291, acc 0.953125, prec 0.107599, recall 0.806202
2017-12-10T04:04:11.734290: step 7934, loss 0.197194, acc 0.953125, prec 0.107606, recall 0.806219
2017-12-10T04:04:12.004957: step 7935, loss 0.285886, acc 0.96875, prec 0.107624, recall 0.806253
2017-12-10T04:04:12.271672: step 7936, loss 0.467024, acc 0.953125, prec 0.107631, recall 0.80627
2017-12-10T04:04:12.538235: step 7937, loss 0.669318, acc 0.953125, prec 0.107648, recall 0.806304
2017-12-10T04:04:12.802576: step 7938, loss 0.594608, acc 0.875, prec 0.107638, recall 0.806304
2017-12-10T04:04:13.068503: step 7939, loss 0.404798, acc 0.96875, prec 0.107646, recall 0.806321
2017-12-10T04:04:13.332864: step 7940, loss 0.330979, acc 0.953125, prec 0.107642, recall 0.806321
2017-12-10T04:04:13.597939: step 7941, loss 0.381538, acc 0.9375, prec 0.107647, recall 0.806338
2017-12-10T04:04:13.871227: step 7942, loss 1.49677, acc 0.859375, prec 0.107636, recall 0.806338
2017-12-10T04:04:14.136399: step 7943, loss 0.540167, acc 0.96875, prec 0.107644, recall 0.806355
2017-12-10T04:04:14.398820: step 7944, loss 0.107949, acc 0.96875, prec 0.107652, recall 0.806372
2017-12-10T04:04:14.668357: step 7945, loss 0.258748, acc 0.953125, prec 0.107658, recall 0.806389
2017-12-10T04:04:14.928015: step 7946, loss 0.058087, acc 0.984375, prec 0.107668, recall 0.806406
2017-12-10T04:04:15.191704: step 7947, loss 0.026696, acc 0.984375, prec 0.107666, recall 0.806406
2017-12-10T04:04:15.456349: step 7948, loss 0.056605, acc 0.984375, prec 0.107665, recall 0.806406
2017-12-10T04:04:15.720438: step 7949, loss 0.0571882, acc 0.96875, prec 0.107673, recall 0.806423
2017-12-10T04:04:15.990389: step 7950, loss 0.217215, acc 0.96875, prec 0.107681, recall 0.80644
2017-12-10T04:04:16.258652: step 7951, loss 0.309683, acc 0.953125, prec 0.107688, recall 0.806457
2017-12-10T04:04:16.488358: step 7952, loss 0.111276, acc 0.980392, prec 0.107686, recall 0.806457
2017-12-10T04:04:16.761812: step 7953, loss 0.485618, acc 0.953125, prec 0.107683, recall 0.806457
2017-12-10T04:04:17.026675: step 7954, loss 0.476624, acc 0.953125, prec 0.107689, recall 0.806474
2017-12-10T04:04:17.287713: step 7955, loss 0.252015, acc 0.953125, prec 0.107685, recall 0.806474
2017-12-10T04:04:17.560956: step 7956, loss 0.00364849, acc 1, prec 0.107706, recall 0.806508
2017-12-10T04:04:17.831604: step 7957, loss 0.127351, acc 0.96875, prec 0.107704, recall 0.806508
2017-12-10T04:04:18.104517: step 7958, loss 0.20215, acc 0.96875, prec 0.107701, recall 0.806508
2017-12-10T04:04:18.374062: step 7959, loss 0.112379, acc 0.96875, prec 0.10772, recall 0.806542
2017-12-10T04:04:18.644088: step 7960, loss 0.0242219, acc 0.984375, prec 0.107739, recall 0.806576
2017-12-10T04:04:18.915261: step 7961, loss 0.0509581, acc 0.984375, prec 0.107738, recall 0.806576
2017-12-10T04:04:19.175384: step 7962, loss 0.638885, acc 0.984375, prec 0.107747, recall 0.806593
2017-12-10T04:04:19.437241: step 7963, loss 0.234607, acc 0.96875, prec 0.107745, recall 0.806593
2017-12-10T04:04:19.699382: step 7964, loss 0.334526, acc 0.984375, prec 0.107754, recall 0.80661
2017-12-10T04:04:19.976875: step 7965, loss 0.0014265, acc 1, prec 0.107775, recall 0.806644
2017-12-10T04:04:20.238133: step 7966, loss 0.0238875, acc 0.984375, prec 0.107784, recall 0.806661
2017-12-10T04:04:20.505853: step 7967, loss 0.576716, acc 0.953125, prec 0.10778, recall 0.806661
2017-12-10T04:04:20.770349: step 7968, loss 4.1226e-05, acc 1, prec 0.107791, recall 0.806678
2017-12-10T04:04:21.029187: step 7969, loss 0.0551717, acc 0.984375, prec 0.107789, recall 0.806678
2017-12-10T04:04:21.295940: step 7970, loss 0.384397, acc 0.96875, prec 0.107787, recall 0.806678
2017-12-10T04:04:21.565794: step 7971, loss 0.0129204, acc 0.984375, prec 0.107786, recall 0.806678
2017-12-10T04:04:21.831273: step 7972, loss 0.065539, acc 0.984375, prec 0.107784, recall 0.806678
2017-12-10T04:04:22.100354: step 7973, loss 0.000263525, acc 1, prec 0.107795, recall 0.806695
2017-12-10T04:04:22.355919: step 7974, loss 0.00022071, acc 1, prec 0.107816, recall 0.806729
2017-12-10T04:04:22.620674: step 7975, loss 0.306303, acc 0.984375, prec 0.107825, recall 0.806746
2017-12-10T04:04:22.889613: step 7976, loss 0.0562221, acc 1, prec 0.107846, recall 0.806779
2017-12-10T04:04:23.161858: step 7977, loss 0.0218645, acc 0.984375, prec 0.107845, recall 0.806779
2017-12-10T04:04:23.425958: step 7978, loss 0.000370139, acc 1, prec 0.107845, recall 0.806779
2017-12-10T04:04:23.690784: step 7979, loss 0.0408327, acc 0.984375, prec 0.107875, recall 0.80683
2017-12-10T04:04:23.954236: step 7980, loss 1.4059e-05, acc 1, prec 0.107875, recall 0.80683
2017-12-10T04:04:24.212036: step 7981, loss 0.00218468, acc 1, prec 0.107885, recall 0.806847
2017-12-10T04:04:24.474398: step 7982, loss 0.374558, acc 0.953125, prec 0.107892, recall 0.806864
2017-12-10T04:04:24.738601: step 7983, loss 0.200046, acc 0.984375, prec 0.10789, recall 0.806864
2017-12-10T04:04:24.997918: step 7984, loss 4.45302e-06, acc 1, prec 0.10789, recall 0.806864
2017-12-10T04:04:25.254384: step 7985, loss 0.0213583, acc 1, prec 0.107911, recall 0.806898
2017-12-10T04:04:25.522668: step 7986, loss 0.288859, acc 0.953125, prec 0.107908, recall 0.806898
2017-12-10T04:04:25.794325: step 7987, loss 0.0711909, acc 0.984375, prec 0.107906, recall 0.806898
2017-12-10T04:04:26.066518: step 7988, loss 1.92872e-05, acc 1, prec 0.107906, recall 0.806898
2017-12-10T04:04:26.323505: step 7989, loss 0.30976, acc 0.96875, prec 0.107904, recall 0.806898
2017-12-10T04:04:26.588261: step 7990, loss 0.000910302, acc 1, prec 0.107904, recall 0.806898
2017-12-10T04:04:26.857521: step 7991, loss 0.120496, acc 0.984375, prec 0.107902, recall 0.806898
2017-12-10T04:04:27.129947: step 7992, loss 2.87947e-06, acc 1, prec 0.107902, recall 0.806898
2017-12-10T04:04:27.392204: step 7993, loss 0.750971, acc 1, prec 0.107944, recall 0.806965
2017-12-10T04:04:27.666675: step 7994, loss 6.71618e-05, acc 1, prec 0.107955, recall 0.806982
2017-12-10T04:04:27.927030: step 7995, loss 6.50645, acc 0.984375, prec 0.107965, recall 0.806929
2017-12-10T04:04:28.190367: step 7996, loss 8.4569, acc 0.96875, prec 0.107965, recall 0.806787
2017-12-10T04:04:28.462748: step 7997, loss 2.89439, acc 0.96875, prec 0.107964, recall 0.806717
2017-12-10T04:04:28.730910: step 7998, loss 0.12247, acc 0.96875, prec 0.107972, recall 0.806734
2017-12-10T04:04:29.002881: step 7999, loss 0.111225, acc 0.953125, prec 0.107978, recall 0.806751
2017-12-10T04:04:29.277876: step 8000, loss 1.33149, acc 0.8125, prec 0.107984, recall 0.806784
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-8000

2017-12-10T04:04:30.622991: step 8001, loss 1.54298, acc 0.78125, prec 0.107966, recall 0.806784
2017-12-10T04:04:30.885349: step 8002, loss 4.30251, acc 0.640625, prec 0.107937, recall 0.806784
2017-12-10T04:04:31.148242: step 8003, loss 3.15964, acc 0.640625, prec 0.10795, recall 0.806852
2017-12-10T04:04:31.410604: step 8004, loss 4.3089, acc 0.609375, prec 0.107939, recall 0.806886
2017-12-10T04:04:31.676961: step 8005, loss 4.07681, acc 0.609375, prec 0.107929, recall 0.806919
2017-12-10T04:04:31.944590: step 8006, loss 4.71299, acc 0.515625, prec 0.10791, recall 0.806953
2017-12-10T04:04:32.207932: step 8007, loss 3.53891, acc 0.625, prec 0.107891, recall 0.80697
2017-12-10T04:04:32.480582: step 8008, loss 4.28764, acc 0.59375, prec 0.107858, recall 0.80697
2017-12-10T04:04:32.742802: step 8009, loss 2.91289, acc 0.640625, prec 0.10785, recall 0.807004
2017-12-10T04:04:33.010909: step 8010, loss 2.86789, acc 0.59375, prec 0.107827, recall 0.807021
2017-12-10T04:04:33.282921: step 8011, loss 2.25166, acc 0.65625, prec 0.10781, recall 0.807037
2017-12-10T04:04:33.555426: step 8012, loss 2.04239, acc 0.734375, prec 0.107789, recall 0.807037
2017-12-10T04:04:33.823267: step 8013, loss 2.38304, acc 0.6875, prec 0.107774, recall 0.807054
2017-12-10T04:04:34.095971: step 8014, loss 1.48981, acc 0.8125, prec 0.107759, recall 0.807054
2017-12-10T04:04:34.363497: step 8015, loss 2.19597, acc 0.796875, prec 0.107763, recall 0.807088
2017-12-10T04:04:34.631961: step 8016, loss 1.65431, acc 0.859375, prec 0.107752, recall 0.807088
2017-12-10T04:04:34.896549: step 8017, loss 0.977746, acc 0.859375, prec 0.107741, recall 0.807088
2017-12-10T04:04:35.161673: step 8018, loss 0.101916, acc 0.953125, prec 0.107737, recall 0.807088
2017-12-10T04:04:35.426148: step 8019, loss 0.108284, acc 0.96875, prec 0.107735, recall 0.807088
2017-12-10T04:04:35.692416: step 8020, loss 0.3111, acc 0.96875, prec 0.107753, recall 0.807122
2017-12-10T04:04:35.954806: step 8021, loss 0.0375664, acc 0.984375, prec 0.107752, recall 0.807122
2017-12-10T04:04:36.224589: step 8022, loss 0.19612, acc 0.953125, prec 0.107758, recall 0.807139
2017-12-10T04:04:36.489203: step 8023, loss 0.000781132, acc 1, prec 0.107779, recall 0.807172
2017-12-10T04:04:36.750741: step 8024, loss 0.000646162, acc 1, prec 0.107789, recall 0.807189
2017-12-10T04:04:37.016204: step 8025, loss 0.0006337, acc 1, prec 0.107789, recall 0.807189
2017-12-10T04:04:37.277637: step 8026, loss 0.330673, acc 0.984375, prec 0.107819, recall 0.807239
2017-12-10T04:04:37.542473: step 8027, loss 0.787342, acc 0.96875, prec 0.107817, recall 0.807239
2017-12-10T04:04:37.805955: step 8028, loss 0.135155, acc 1, prec 0.107838, recall 0.807273
2017-12-10T04:04:38.071771: step 8029, loss 0.000238108, acc 1, prec 0.107848, recall 0.80729
2017-12-10T04:04:38.338921: step 8030, loss 0.000806163, acc 1, prec 0.107848, recall 0.80729
2017-12-10T04:04:38.604888: step 8031, loss 0.966316, acc 1, prec 0.107869, recall 0.807323
2017-12-10T04:04:38.869286: step 8032, loss 8.01588, acc 0.953125, prec 0.107868, recall 0.807183
2017-12-10T04:04:39.137787: step 8033, loss 0.475706, acc 0.953125, prec 0.107895, recall 0.807233
2017-12-10T04:04:39.398571: step 8034, loss 0.306384, acc 0.96875, prec 0.107903, recall 0.80725
2017-12-10T04:04:39.664817: step 8035, loss 0.0198093, acc 1, prec 0.107913, recall 0.807267
2017-12-10T04:04:39.927652: step 8036, loss 0.702666, acc 0.9375, prec 0.107929, recall 0.8073
2017-12-10T04:04:40.189920: step 8037, loss 0.244948, acc 0.96875, prec 0.107926, recall 0.8073
2017-12-10T04:04:40.462984: step 8038, loss 0.859164, acc 0.921875, prec 0.107931, recall 0.807317
2017-12-10T04:04:40.727154: step 8039, loss 0.279788, acc 0.9375, prec 0.107946, recall 0.807351
2017-12-10T04:04:40.990148: step 8040, loss 0.350294, acc 0.9375, prec 0.107941, recall 0.807351
2017-12-10T04:04:41.251080: step 8041, loss 0.861839, acc 0.90625, prec 0.107934, recall 0.807351
2017-12-10T04:04:41.511843: step 8042, loss 0.745385, acc 0.921875, prec 0.107927, recall 0.807351
2017-12-10T04:04:41.786789: step 8043, loss 0.598472, acc 0.859375, prec 0.107927, recall 0.807367
2017-12-10T04:04:42.059565: step 8044, loss 1.59005, acc 0.859375, prec 0.107936, recall 0.807401
2017-12-10T04:04:42.325395: step 8045, loss 0.666848, acc 0.921875, prec 0.10794, recall 0.807418
2017-12-10T04:04:42.599954: step 8046, loss 0.921116, acc 0.921875, prec 0.107944, recall 0.807434
2017-12-10T04:04:42.866366: step 8047, loss 0.588687, acc 0.890625, prec 0.107935, recall 0.807434
2017-12-10T04:04:43.130299: step 8048, loss 0.40357, acc 0.921875, prec 0.10794, recall 0.807451
2017-12-10T04:04:43.395461: step 8049, loss 0.4438, acc 0.875, prec 0.107929, recall 0.807451
2017-12-10T04:04:43.658665: step 8050, loss 0.567483, acc 0.9375, prec 0.107935, recall 0.807468
2017-12-10T04:04:43.922245: step 8051, loss 0.993354, acc 0.890625, prec 0.107936, recall 0.807485
2017-12-10T04:04:44.186482: step 8052, loss 1.16659, acc 0.859375, prec 0.107925, recall 0.807485
2017-12-10T04:04:44.450881: step 8053, loss 0.42285, acc 0.9375, prec 0.10792, recall 0.807485
2017-12-10T04:04:44.713123: step 8054, loss 1.47694, acc 0.921875, prec 0.107924, recall 0.807501
2017-12-10T04:04:44.980641: step 8055, loss 0.392147, acc 0.921875, prec 0.107928, recall 0.807518
2017-12-10T04:04:45.242011: step 8056, loss 0.385144, acc 0.921875, prec 0.107932, recall 0.807535
2017-12-10T04:04:45.510206: step 8057, loss 0.0581136, acc 0.984375, prec 0.107931, recall 0.807535
2017-12-10T04:04:45.786890: step 8058, loss 1.03848, acc 0.953125, prec 0.107927, recall 0.807535
2017-12-10T04:04:46.051547: step 8059, loss 0.629311, acc 0.921875, prec 0.107921, recall 0.807535
2017-12-10T04:04:46.316090: step 8060, loss 0.12211, acc 0.96875, prec 0.107919, recall 0.807535
2017-12-10T04:04:46.580305: step 8061, loss 2.03895, acc 0.890625, prec 0.107931, recall 0.807568
2017-12-10T04:04:46.847002: step 8062, loss 0.00603011, acc 1, prec 0.107951, recall 0.807602
2017-12-10T04:04:47.110938: step 8063, loss 0.365002, acc 0.921875, prec 0.107945, recall 0.807602
2017-12-10T04:04:47.379381: step 8064, loss 0.0955787, acc 0.953125, prec 0.107962, recall 0.807635
2017-12-10T04:04:47.646127: step 8065, loss 0.85713, acc 0.984375, prec 0.107971, recall 0.807652
2017-12-10T04:04:47.908844: step 8066, loss 0.00221196, acc 1, prec 0.107981, recall 0.807669
2017-12-10T04:04:48.176108: step 8067, loss 0.153601, acc 0.984375, prec 0.107991, recall 0.807686
2017-12-10T04:04:48.448947: step 8068, loss 0.00421772, acc 1, prec 0.107991, recall 0.807686
2017-12-10T04:04:48.714556: step 8069, loss 0.2184, acc 0.96875, prec 0.107988, recall 0.807686
2017-12-10T04:04:48.975445: step 8070, loss 0.155925, acc 0.984375, prec 0.107987, recall 0.807686
2017-12-10T04:04:49.243042: step 8071, loss 0.133419, acc 0.96875, prec 0.107984, recall 0.807686
2017-12-10T04:04:49.514866: step 8072, loss 0.277906, acc 0.96875, prec 0.107992, recall 0.807702
2017-12-10T04:04:49.781121: step 8073, loss 0.727001, acc 0.953125, prec 0.10802, recall 0.807752
2017-12-10T04:04:50.045643: step 8074, loss 0.0183439, acc 0.984375, prec 0.108018, recall 0.807752
2017-12-10T04:04:50.311107: step 8075, loss 0.0110438, acc 1, prec 0.108039, recall 0.807786
2017-12-10T04:04:50.575869: step 8076, loss 0.554525, acc 0.96875, prec 0.108047, recall 0.807803
2017-12-10T04:04:50.842112: step 8077, loss 0.332467, acc 0.984375, prec 0.108056, recall 0.807819
2017-12-10T04:04:51.104688: step 8078, loss 0.0494933, acc 0.984375, prec 0.108065, recall 0.807836
2017-12-10T04:04:51.365561: step 8079, loss 0.143066, acc 0.984375, prec 0.108074, recall 0.807853
2017-12-10T04:04:51.627011: step 8080, loss 2.96081, acc 0.953125, prec 0.108082, recall 0.807799
2017-12-10T04:04:51.901192: step 8081, loss 0.297614, acc 0.96875, prec 0.1081, recall 0.807833
2017-12-10T04:04:52.170080: step 8082, loss 0.00351867, acc 1, prec 0.108111, recall 0.807849
2017-12-10T04:04:52.433785: step 8083, loss 7.00425e-06, acc 1, prec 0.108111, recall 0.807849
2017-12-10T04:04:52.700336: step 8084, loss 0.0337989, acc 0.984375, prec 0.10812, recall 0.807866
2017-12-10T04:04:52.965695: step 8085, loss 0.271576, acc 0.9375, prec 0.108115, recall 0.807866
2017-12-10T04:04:53.235370: step 8086, loss 2.56793, acc 0.953125, prec 0.108123, recall 0.807813
2017-12-10T04:04:53.503886: step 8087, loss 0.0975104, acc 0.96875, prec 0.10812, recall 0.807813
2017-12-10T04:04:53.766640: step 8088, loss 0.327782, acc 0.921875, prec 0.108114, recall 0.807813
2017-12-10T04:04:54.034461: step 8089, loss 0.264542, acc 0.96875, prec 0.108122, recall 0.807829
2017-12-10T04:04:54.301920: step 8090, loss 0.0864228, acc 0.984375, prec 0.108131, recall 0.807846
2017-12-10T04:04:54.564687: step 8091, loss 0.308494, acc 0.9375, prec 0.108146, recall 0.807879
2017-12-10T04:04:54.827811: step 8092, loss 0.477138, acc 0.9375, prec 0.108141, recall 0.807879
2017-12-10T04:04:55.094805: step 8093, loss 0.00572821, acc 1, prec 0.108204, recall 0.807979
2017-12-10T04:04:55.360988: step 8094, loss 0.148013, acc 0.96875, prec 0.108222, recall 0.808012
2017-12-10T04:04:55.628464: step 8095, loss 0.0562767, acc 0.984375, prec 0.10822, recall 0.808012
2017-12-10T04:04:55.899393: step 8096, loss 0.930023, acc 0.9375, prec 0.108236, recall 0.808046
2017-12-10T04:04:56.165649: step 8097, loss 0.0805767, acc 0.96875, prec 0.108244, recall 0.808062
2017-12-10T04:04:56.430136: step 8098, loss 0.181714, acc 0.9375, prec 0.108239, recall 0.808062
2017-12-10T04:04:56.699387: step 8099, loss 0.0888474, acc 0.96875, prec 0.108247, recall 0.808079
2017-12-10T04:04:56.973057: step 8100, loss 0.225561, acc 0.953125, prec 0.108243, recall 0.808079

Evaluation:
2017-12-10T04:05:04.594156: step 8100, loss 10.5068, acc 0.947259, prec 0.108421, recall 0.804141

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-8100

2017-12-10T04:05:05.999789: step 8101, loss 0.0629735, acc 0.984375, prec 0.108419, recall 0.804141
2017-12-10T04:05:06.269701: step 8102, loss 0.155386, acc 0.96875, prec 0.108437, recall 0.804175
2017-12-10T04:05:06.540769: step 8103, loss 0.766387, acc 0.953125, prec 0.108434, recall 0.804175
2017-12-10T04:05:06.803447: step 8104, loss 0.726464, acc 0.9375, prec 0.108449, recall 0.804208
2017-12-10T04:05:07.066187: step 8105, loss 0.00213271, acc 1, prec 0.10848, recall 0.804259
2017-12-10T04:05:07.337102: step 8106, loss 0.778507, acc 0.953125, prec 0.108517, recall 0.804326
2017-12-10T04:05:07.610514: step 8107, loss 0.276239, acc 0.984375, prec 0.108526, recall 0.804342
2017-12-10T04:05:07.877502: step 8108, loss 0.169999, acc 0.984375, prec 0.108535, recall 0.804359
2017-12-10T04:05:08.144803: step 8109, loss 0.872121, acc 0.953125, prec 0.108552, recall 0.804392
2017-12-10T04:05:08.412533: step 8110, loss 0.48678, acc 0.921875, prec 0.108556, recall 0.804409
2017-12-10T04:05:08.681431: step 8111, loss 0.000259388, acc 1, prec 0.108597, recall 0.804476
2017-12-10T04:05:08.943217: step 8112, loss 0.0487219, acc 0.984375, prec 0.108637, recall 0.804543
2017-12-10T04:05:09.215283: step 8113, loss 0.613369, acc 0.953125, prec 0.108644, recall 0.804559
2017-12-10T04:05:09.487340: step 8114, loss 0.00876601, acc 1, prec 0.108644, recall 0.804559
2017-12-10T04:05:09.749626: step 8115, loss 0.0680076, acc 0.984375, prec 0.108673, recall 0.804609
2017-12-10T04:05:10.014656: step 8116, loss 0.256368, acc 0.984375, prec 0.108682, recall 0.804626
2017-12-10T04:05:10.280721: step 8117, loss 0.0118303, acc 0.984375, prec 0.108691, recall 0.804643
2017-12-10T04:05:10.544346: step 8118, loss 1.21732, acc 0.984375, prec 0.108722, recall 0.804624
2017-12-10T04:05:10.817803: step 8119, loss 0.0682458, acc 0.96875, prec 0.10874, recall 0.804658
2017-12-10T04:05:11.082255: step 8120, loss 3.75096, acc 0.953125, prec 0.108748, recall 0.804606
2017-12-10T04:05:11.359526: step 8121, loss 0.802698, acc 0.90625, prec 0.108751, recall 0.804622
2017-12-10T04:05:11.624412: step 8122, loss 0.339807, acc 0.984375, prec 0.10875, recall 0.804622
2017-12-10T04:05:11.904785: step 8123, loss 0.0812396, acc 0.96875, prec 0.108757, recall 0.804639
2017-12-10T04:05:12.172096: step 8124, loss 0.858895, acc 0.9375, prec 0.108752, recall 0.804639
2017-12-10T04:05:12.445636: step 8125, loss 0.321266, acc 0.953125, prec 0.108759, recall 0.804656
2017-12-10T04:05:12.717723: step 8126, loss 0.0188555, acc 1, prec 0.108759, recall 0.804656
2017-12-10T04:05:12.984018: step 8127, loss 0.939288, acc 0.921875, prec 0.108783, recall 0.804706
2017-12-10T04:05:13.255420: step 8128, loss 0.199445, acc 0.953125, prec 0.1088, recall 0.804739
2017-12-10T04:05:13.520586: step 8129, loss 0.0658774, acc 0.953125, prec 0.108827, recall 0.804789
2017-12-10T04:05:13.787874: step 8130, loss 0.564703, acc 0.890625, prec 0.108839, recall 0.804822
2017-12-10T04:05:14.059281: step 8131, loss 0.501149, acc 0.875, prec 0.108839, recall 0.804839
2017-12-10T04:05:14.323869: step 8132, loss 1.1225, acc 0.90625, prec 0.108842, recall 0.804855
2017-12-10T04:05:14.588208: step 8133, loss 0.33084, acc 0.9375, prec 0.108847, recall 0.804872
2017-12-10T04:05:14.853986: step 8134, loss 0.230897, acc 0.9375, prec 0.108842, recall 0.804872
2017-12-10T04:05:15.121659: step 8135, loss 0.574093, acc 0.875, prec 0.108842, recall 0.804888
2017-12-10T04:05:15.384818: step 8136, loss 0.466949, acc 0.90625, prec 0.108855, recall 0.804922
2017-12-10T04:05:15.652385: step 8137, loss 0.954608, acc 0.90625, prec 0.108848, recall 0.804922
2017-12-10T04:05:15.926224: step 8138, loss 0.538643, acc 0.921875, prec 0.108842, recall 0.804922
2017-12-10T04:05:16.195296: step 8139, loss 0.237761, acc 0.921875, prec 0.108846, recall 0.804938
2017-12-10T04:05:16.457652: step 8140, loss 0.824051, acc 0.90625, prec 0.108838, recall 0.804938
2017-12-10T04:05:16.721078: step 8141, loss 1.0253, acc 0.9375, prec 0.108833, recall 0.804938
2017-12-10T04:05:16.985665: step 8142, loss 0.608668, acc 0.921875, prec 0.108827, recall 0.804938
2017-12-10T04:05:17.256678: step 8143, loss 0.568426, acc 0.90625, prec 0.108819, recall 0.804938
2017-12-10T04:05:17.522972: step 8144, loss 0.575229, acc 0.921875, prec 0.108813, recall 0.804938
2017-12-10T04:05:17.789343: step 8145, loss 0.423001, acc 0.953125, prec 0.10883, recall 0.804971
2017-12-10T04:05:18.050778: step 8146, loss 0.317126, acc 0.953125, prec 0.108826, recall 0.804971
2017-12-10T04:05:18.313142: step 8147, loss 0.212827, acc 0.984375, prec 0.108835, recall 0.804988
2017-12-10T04:05:18.579607: step 8148, loss 0.133235, acc 0.984375, prec 0.108834, recall 0.804988
2017-12-10T04:05:18.846993: step 8149, loss 0.33743, acc 0.96875, prec 0.108831, recall 0.804988
2017-12-10T04:05:19.110589: step 8150, loss 0.0529301, acc 0.984375, prec 0.10884, recall 0.805005
2017-12-10T04:05:19.384084: step 8151, loss 0.0175983, acc 0.984375, prec 0.108849, recall 0.805021
2017-12-10T04:05:19.647019: step 8152, loss 0.00462411, acc 1, prec 0.10886, recall 0.805038
2017-12-10T04:05:19.909570: step 8153, loss 0.0746635, acc 0.984375, prec 0.108869, recall 0.805054
2017-12-10T04:05:20.180429: step 8154, loss 0.00489903, acc 1, prec 0.108869, recall 0.805054
2017-12-10T04:05:20.443558: step 8155, loss 0.27519, acc 0.984375, prec 0.108878, recall 0.805071
2017-12-10T04:05:20.715475: step 8156, loss 0.00345908, acc 1, prec 0.108878, recall 0.805071
2017-12-10T04:05:20.984650: step 8157, loss 0.00197707, acc 1, prec 0.108888, recall 0.805088
2017-12-10T04:05:21.249166: step 8158, loss 0.822778, acc 0.9375, prec 0.108883, recall 0.805088
2017-12-10T04:05:21.519022: step 8159, loss 0.168132, acc 0.96875, prec 0.10889, recall 0.805104
2017-12-10T04:05:21.780122: step 8160, loss 0.294846, acc 0.984375, prec 0.10891, recall 0.805137
2017-12-10T04:05:22.044549: step 8161, loss 0.0193361, acc 0.984375, prec 0.108919, recall 0.805154
2017-12-10T04:05:22.312079: step 8162, loss 0.0228608, acc 0.984375, prec 0.108938, recall 0.805187
2017-12-10T04:05:22.574638: step 8163, loss 0.461285, acc 0.953125, prec 0.108955, recall 0.80522
2017-12-10T04:05:22.836903: step 8164, loss 0.0426091, acc 0.984375, prec 0.108964, recall 0.805237
2017-12-10T04:05:23.107278: step 8165, loss 0.00426476, acc 1, prec 0.108974, recall 0.805253
2017-12-10T04:05:23.374473: step 8166, loss 0.155731, acc 0.984375, prec 0.108993, recall 0.805286
2017-12-10T04:05:23.639265: step 8167, loss 0.000106166, acc 1, prec 0.109003, recall 0.805303
2017-12-10T04:05:23.901273: step 8168, loss 0.269427, acc 0.96875, prec 0.109011, recall 0.805319
2017-12-10T04:05:24.166962: step 8169, loss 0.0232465, acc 0.984375, prec 0.10902, recall 0.805336
2017-12-10T04:05:24.428331: step 8170, loss 0.318444, acc 0.96875, prec 0.109038, recall 0.805369
2017-12-10T04:05:24.693958: step 8171, loss 0.0720444, acc 0.984375, prec 0.109057, recall 0.805402
2017-12-10T04:05:24.962299: step 8172, loss 1.01214, acc 0.953125, prec 0.109054, recall 0.805402
2017-12-10T04:05:25.226394: step 8173, loss 0.101196, acc 0.953125, prec 0.10905, recall 0.805402
2017-12-10T04:05:25.497541: step 8174, loss 8.15835e-07, acc 1, prec 0.10906, recall 0.805419
2017-12-10T04:05:25.762299: step 8175, loss 0.0568547, acc 0.984375, prec 0.109069, recall 0.805435
2017-12-10T04:05:26.024561: step 8176, loss 0.120388, acc 0.96875, prec 0.109067, recall 0.805435
2017-12-10T04:05:26.289731: step 8177, loss 0.0389763, acc 0.984375, prec 0.109065, recall 0.805435
2017-12-10T04:05:26.553527: step 8178, loss 5.3467e-05, acc 1, prec 0.109065, recall 0.805435
2017-12-10T04:05:26.822146: step 8179, loss 0.000149884, acc 1, prec 0.109065, recall 0.805435
2017-12-10T04:05:27.086594: step 8180, loss 1.85345e-05, acc 1, prec 0.109065, recall 0.805435
2017-12-10T04:05:27.345180: step 8181, loss 0.0914553, acc 0.984375, prec 0.109074, recall 0.805452
2017-12-10T04:05:27.608972: step 8182, loss 6.68681e-07, acc 1, prec 0.109085, recall 0.805468
2017-12-10T04:05:27.865461: step 8183, loss 0.307998, acc 0.984375, prec 0.109094, recall 0.805485
2017-12-10T04:05:28.139985: step 8184, loss 0.402944, acc 0.953125, prec 0.10911, recall 0.805518
2017-12-10T04:05:28.410023: step 8185, loss 2.26839, acc 0.984375, prec 0.10911, recall 0.805449
2017-12-10T04:05:28.686718: step 8186, loss 4.16264e-06, acc 1, prec 0.10911, recall 0.805449
2017-12-10T04:05:28.948959: step 8187, loss 0.516293, acc 0.984375, prec 0.109119, recall 0.805466
2017-12-10T04:05:29.223392: step 8188, loss 0.323078, acc 0.984375, prec 0.109118, recall 0.805466
2017-12-10T04:05:29.495902: step 8189, loss 0.653213, acc 0.9375, prec 0.109113, recall 0.805466
2017-12-10T04:05:29.758747: step 8190, loss 0.0714089, acc 0.984375, prec 0.109112, recall 0.805466
2017-12-10T04:05:30.027582: step 8191, loss 0.0715095, acc 0.984375, prec 0.109111, recall 0.805466
2017-12-10T04:05:30.300451: step 8192, loss 0.1473, acc 0.96875, prec 0.109118, recall 0.805482
2017-12-10T04:05:30.578736: step 8193, loss 0.000517885, acc 1, prec 0.109129, recall 0.805499
2017-12-10T04:05:30.839728: step 8194, loss 0.149103, acc 0.96875, prec 0.109136, recall 0.805515
2017-12-10T04:05:31.107658: step 8195, loss 0.369085, acc 0.984375, prec 0.109135, recall 0.805515
2017-12-10T04:05:31.371995: step 8196, loss 0.231229, acc 0.9375, prec 0.10914, recall 0.805532
2017-12-10T04:05:31.640214: step 8197, loss 0.590531, acc 0.9375, prec 0.109145, recall 0.805548
2017-12-10T04:05:31.901873: step 8198, loss 0.288063, acc 0.9375, prec 0.10914, recall 0.805548
2017-12-10T04:05:32.177589: step 8199, loss 0.534325, acc 0.96875, prec 0.109148, recall 0.805565
2017-12-10T04:05:32.446363: step 8200, loss 0.0124545, acc 1, prec 0.109169, recall 0.805598
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-8200

2017-12-10T04:05:33.766942: step 8201, loss 0.142562, acc 0.96875, prec 0.109176, recall 0.805614
2017-12-10T04:05:34.031903: step 8202, loss 0.739314, acc 0.9375, prec 0.109171, recall 0.805614
2017-12-10T04:05:34.296973: step 8203, loss 0.997952, acc 0.90625, prec 0.109164, recall 0.805614
2017-12-10T04:05:34.570168: step 8204, loss 0.0301221, acc 0.984375, prec 0.109173, recall 0.805631
2017-12-10T04:05:35.548123: step 8205, loss 0.0798923, acc 0.984375, prec 0.109182, recall 0.805647
2017-12-10T04:05:35.927343: step 8206, loss 0.0223794, acc 0.984375, prec 0.109181, recall 0.805647
2017-12-10T04:05:36.662648: step 8207, loss 0.103764, acc 0.984375, prec 0.1092, recall 0.80568
2017-12-10T04:05:37.442991: step 8208, loss 0.425022, acc 0.96875, prec 0.109197, recall 0.80568
2017-12-10T04:05:38.183076: step 8209, loss 0.328166, acc 0.9375, prec 0.109202, recall 0.805697
2017-12-10T04:05:38.914842: step 8210, loss 0.0579462, acc 0.984375, prec 0.109222, recall 0.80573
2017-12-10T04:05:39.643275: step 8211, loss 0.509991, acc 0.9375, prec 0.109237, recall 0.805763
2017-12-10T04:05:40.360333: step 8212, loss 0.286111, acc 0.984375, prec 0.109236, recall 0.805763
2017-12-10T04:05:41.080706: step 8213, loss 0.492653, acc 0.953125, prec 0.109253, recall 0.805796
2017-12-10T04:05:41.830910: step 8214, loss 0.00621125, acc 1, prec 0.109263, recall 0.805812
2017-12-10T04:05:42.574647: step 8215, loss 0.0193465, acc 1, prec 0.109263, recall 0.805812
2017-12-10T04:05:43.541977: step 8216, loss 0.142892, acc 0.96875, prec 0.10926, recall 0.805812
2017-12-10T04:05:43.971951: step 8217, loss 0.144162, acc 0.984375, prec 0.109259, recall 0.805812
2017-12-10T04:05:44.255091: step 8218, loss 3.50208, acc 0.96875, prec 0.109268, recall 0.80576
2017-12-10T04:05:44.549395: step 8219, loss 0.212498, acc 0.984375, prec 0.109277, recall 0.805777
2017-12-10T04:05:44.821847: step 8220, loss 8.95326e-05, acc 1, prec 0.109297, recall 0.80581
2017-12-10T04:05:45.092193: step 8221, loss 0.00760966, acc 1, prec 0.109318, recall 0.805843
2017-12-10T04:05:45.363172: step 8222, loss 0.212984, acc 0.953125, prec 0.109314, recall 0.805843
2017-12-10T04:05:45.642481: step 8223, loss 0.000546644, acc 1, prec 0.109324, recall 0.805859
2017-12-10T04:05:45.916986: step 8224, loss 0.0279298, acc 0.984375, prec 0.109323, recall 0.805859
2017-12-10T04:05:46.194141: step 8225, loss 0.0335047, acc 0.984375, prec 0.109332, recall 0.805875
2017-12-10T04:05:46.461583: step 8226, loss 0.859381, acc 0.9375, prec 0.109327, recall 0.805875
2017-12-10T04:05:46.725280: step 8227, loss 0.104133, acc 0.953125, prec 0.109334, recall 0.805892
2017-12-10T04:05:46.992250: step 8228, loss 0.406417, acc 0.953125, prec 0.10934, recall 0.805908
2017-12-10T04:05:47.256596: step 8229, loss 0.315231, acc 0.953125, prec 0.109357, recall 0.805941
2017-12-10T04:05:47.519328: step 8230, loss 0.156356, acc 0.953125, prec 0.109353, recall 0.805941
2017-12-10T04:05:47.779951: step 8231, loss 0.0732891, acc 0.984375, prec 0.109382, recall 0.80599
2017-12-10T04:05:48.042428: step 8232, loss 0.407525, acc 0.953125, prec 0.109399, recall 0.806023
2017-12-10T04:05:48.312433: step 8233, loss 0.0887844, acc 0.953125, prec 0.109426, recall 0.806072
2017-12-10T04:05:48.580369: step 8234, loss 0.205691, acc 0.953125, prec 0.109453, recall 0.806122
2017-12-10T04:05:48.849526: step 8235, loss 0.776322, acc 0.90625, prec 0.109456, recall 0.806138
2017-12-10T04:05:49.111190: step 8236, loss 1.13641, acc 0.90625, prec 0.109458, recall 0.806154
2017-12-10T04:05:49.380072: step 8237, loss 0.0925735, acc 0.96875, prec 0.109456, recall 0.806154
2017-12-10T04:05:49.649343: step 8238, loss 0.156193, acc 0.96875, prec 0.109453, recall 0.806154
2017-12-10T04:05:49.918947: step 8239, loss 0.085747, acc 0.96875, prec 0.109461, recall 0.806171
2017-12-10T04:05:50.179609: step 8240, loss 0.128063, acc 0.96875, prec 0.109469, recall 0.806187
2017-12-10T04:05:50.447958: step 8241, loss 0.407584, acc 0.9375, prec 0.109474, recall 0.806204
2017-12-10T04:05:50.717777: step 8242, loss 0.644993, acc 0.9375, prec 0.109499, recall 0.806253
2017-12-10T04:05:50.980311: step 8243, loss 2.54871, acc 0.953125, prec 0.109548, recall 0.806266
2017-12-10T04:05:51.250920: step 8244, loss 0.328857, acc 0.953125, prec 0.109554, recall 0.806283
2017-12-10T04:05:51.516262: step 8245, loss 0.310308, acc 0.96875, prec 0.109572, recall 0.806315
2017-12-10T04:05:51.779953: step 8246, loss 0.779849, acc 0.96875, prec 0.10957, recall 0.806315
2017-12-10T04:05:52.051542: step 8247, loss 0.00592855, acc 1, prec 0.10958, recall 0.806332
2017-12-10T04:05:52.312656: step 8248, loss 0.216331, acc 0.9375, prec 0.109575, recall 0.806332
2017-12-10T04:05:52.587955: step 8249, loss 0.484024, acc 0.953125, prec 0.109582, recall 0.806348
2017-12-10T04:05:52.862208: step 8250, loss 0.3279, acc 0.921875, prec 0.109616, recall 0.806414
2017-12-10T04:05:53.125760: step 8251, loss 0.255936, acc 0.953125, prec 0.109612, recall 0.806414
2017-12-10T04:05:53.387293: step 8252, loss 1.30022, acc 0.890625, prec 0.109603, recall 0.806414
2017-12-10T04:05:53.647223: step 8253, loss 0.054147, acc 0.96875, prec 0.109611, recall 0.80643
2017-12-10T04:05:53.922121: step 8254, loss 0.0402599, acc 0.984375, prec 0.10962, recall 0.806446
2017-12-10T04:05:54.183360: step 8255, loss 0.45776, acc 0.953125, prec 0.109616, recall 0.806446
2017-12-10T04:05:54.449087: step 8256, loss 0.331479, acc 0.921875, prec 0.10961, recall 0.806446
2017-12-10T04:05:54.714171: step 8257, loss 0.414303, acc 0.953125, prec 0.109606, recall 0.806446
2017-12-10T04:05:54.981105: step 8258, loss 0.11061, acc 0.953125, prec 0.109613, recall 0.806463
2017-12-10T04:05:55.241982: step 8259, loss 0.711395, acc 0.921875, prec 0.109606, recall 0.806463
2017-12-10T04:05:55.503360: step 8260, loss 0.0668219, acc 0.984375, prec 0.109605, recall 0.806463
2017-12-10T04:05:55.765966: step 8261, loss 0.297581, acc 0.9375, prec 0.10961, recall 0.806479
2017-12-10T04:05:56.038132: step 8262, loss 0.343681, acc 0.96875, prec 0.109608, recall 0.806479
2017-12-10T04:05:56.305598: step 8263, loss 0.330783, acc 0.953125, prec 0.109604, recall 0.806479
2017-12-10T04:05:56.574101: step 8264, loss 0.17194, acc 0.96875, prec 0.109653, recall 0.80656
2017-12-10T04:05:56.844636: step 8265, loss 0.478958, acc 0.953125, prec 0.109649, recall 0.80656
2017-12-10T04:05:57.113095: step 8266, loss 0.251382, acc 0.984375, prec 0.109648, recall 0.80656
2017-12-10T04:05:57.377388: step 8267, loss 0.00462963, acc 1, prec 0.109648, recall 0.80656
2017-12-10T04:05:57.647988: step 8268, loss 0.648827, acc 0.984375, prec 0.109667, recall 0.806593
2017-12-10T04:05:57.925899: step 8269, loss 0.101642, acc 0.953125, prec 0.109683, recall 0.806626
2017-12-10T04:05:58.186350: step 8270, loss 0.0658566, acc 0.984375, prec 0.109703, recall 0.806658
2017-12-10T04:05:58.460773: step 8271, loss 0.979887, acc 0.9375, prec 0.109708, recall 0.806675
2017-12-10T04:05:58.727606: step 8272, loss 0.116807, acc 0.96875, prec 0.109715, recall 0.806691
2017-12-10T04:05:58.996491: step 8273, loss 0.141286, acc 0.984375, prec 0.109724, recall 0.806707
2017-12-10T04:05:59.261636: step 8274, loss 0.086738, acc 0.984375, prec 0.109723, recall 0.806707
2017-12-10T04:05:59.527961: step 8275, loss 0.242019, acc 0.96875, prec 0.109741, recall 0.80674
2017-12-10T04:05:59.793104: step 8276, loss 2.42133e-06, acc 1, prec 0.109751, recall 0.806756
2017-12-10T04:06:00.053448: step 8277, loss 0.000159275, acc 1, prec 0.109751, recall 0.806756
2017-12-10T04:06:00.321937: step 8278, loss 0.111193, acc 0.984375, prec 0.10975, recall 0.806756
2017-12-10T04:06:00.590628: step 8279, loss 0.000141661, acc 1, prec 0.10976, recall 0.806772
2017-12-10T04:06:00.850058: step 8280, loss 0.0268582, acc 0.984375, prec 0.109759, recall 0.806772
2017-12-10T04:06:01.113786: step 8281, loss 0.0483653, acc 0.984375, prec 0.109768, recall 0.806789
2017-12-10T04:06:01.383741: step 8282, loss 0.000419087, acc 1, prec 0.109778, recall 0.806805
2017-12-10T04:06:01.651142: step 8283, loss 3.11725, acc 0.984375, prec 0.109778, recall 0.806737
2017-12-10T04:06:01.918592: step 8284, loss 0.00107028, acc 1, prec 0.109788, recall 0.806753
2017-12-10T04:06:02.187393: step 8285, loss 0.00209454, acc 1, prec 0.109798, recall 0.806769
2017-12-10T04:06:02.454089: step 8286, loss 3.88796, acc 0.953125, prec 0.109796, recall 0.806701
2017-12-10T04:06:02.726999: step 8287, loss 0.196708, acc 0.96875, prec 0.109804, recall 0.806718
2017-12-10T04:06:02.993499: step 8288, loss 0.174908, acc 0.96875, prec 0.109801, recall 0.806718
2017-12-10T04:06:03.260780: step 8289, loss 0.309985, acc 0.96875, prec 0.109809, recall 0.806734
2017-12-10T04:06:03.537205: step 8290, loss 0.0806253, acc 0.96875, prec 0.109806, recall 0.806734
2017-12-10T04:06:03.802178: step 8291, loss 0.957151, acc 0.90625, prec 0.109819, recall 0.806767
2017-12-10T04:06:04.068364: step 8292, loss 0.526309, acc 0.921875, prec 0.109823, recall 0.806783
2017-12-10T04:06:04.338513: step 8293, loss 0.0124626, acc 1, prec 0.109833, recall 0.806799
2017-12-10T04:06:04.610489: step 8294, loss 0.198141, acc 0.921875, prec 0.109858, recall 0.806848
2017-12-10T04:06:04.873508: step 8295, loss 0.00508326, acc 1, prec 0.109858, recall 0.806848
2017-12-10T04:06:05.144116: step 8296, loss 0.526013, acc 0.9375, prec 0.109863, recall 0.806864
2017-12-10T04:06:05.410984: step 8297, loss 0.0741478, acc 0.953125, prec 0.109869, recall 0.80688
2017-12-10T04:06:05.676242: step 8298, loss 0.866896, acc 0.921875, prec 0.109863, recall 0.80688
2017-12-10T04:06:05.943597: step 8299, loss 0.522505, acc 0.90625, prec 0.109855, recall 0.80688
2017-12-10T04:06:06.214492: step 8300, loss 0.245312, acc 0.96875, prec 0.109853, recall 0.80688
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-8300

2017-12-10T04:06:07.666180: step 8301, loss 0.0502468, acc 0.96875, prec 0.10985, recall 0.80688
2017-12-10T04:06:07.935813: step 8302, loss 0.564939, acc 0.90625, prec 0.109843, recall 0.80688
2017-12-10T04:06:08.211316: step 8303, loss 0.313566, acc 0.953125, prec 0.109839, recall 0.80688
2017-12-10T04:06:08.477634: step 8304, loss 0.14434, acc 0.921875, prec 0.109853, recall 0.806913
2017-12-10T04:06:08.745808: step 8305, loss 0.977865, acc 0.953125, prec 0.109859, recall 0.806929
2017-12-10T04:06:09.015798: step 8306, loss 0.492104, acc 0.96875, prec 0.109857, recall 0.806929
2017-12-10T04:06:09.286617: step 8307, loss 0.229322, acc 0.921875, prec 0.109861, recall 0.806945
2017-12-10T04:06:09.552583: step 8308, loss 0.117245, acc 0.984375, prec 0.10986, recall 0.806945
2017-12-10T04:06:09.823524: step 8309, loss 0.879678, acc 0.90625, prec 0.109872, recall 0.806978
2017-12-10T04:06:10.090566: step 8310, loss 0.492675, acc 0.953125, prec 0.109869, recall 0.806978
2017-12-10T04:06:10.355551: step 8311, loss 0.689261, acc 0.921875, prec 0.109883, recall 0.80701
2017-12-10T04:06:10.627422: step 8312, loss 0.678286, acc 0.953125, prec 0.109889, recall 0.807026
2017-12-10T04:06:10.901623: step 8313, loss 0.416797, acc 0.90625, prec 0.109882, recall 0.807026
2017-12-10T04:06:11.162693: step 8314, loss 0.125319, acc 0.984375, prec 0.10988, recall 0.807026
2017-12-10T04:06:11.429320: step 8315, loss 0.22314, acc 0.953125, prec 0.109897, recall 0.807059
2017-12-10T04:06:11.695339: step 8316, loss 0.186066, acc 0.953125, prec 0.109893, recall 0.807059
2017-12-10T04:06:11.970317: step 8317, loss 0.741989, acc 1, prec 0.109924, recall 0.807107
2017-12-10T04:06:12.247168: step 8318, loss 0.402991, acc 0.9375, prec 0.109939, recall 0.80714
2017-12-10T04:06:12.512681: step 8319, loss 0.00970295, acc 1, prec 0.109939, recall 0.80714
2017-12-10T04:06:12.780956: step 8320, loss 0.244953, acc 0.96875, prec 0.109937, recall 0.80714
2017-12-10T04:06:13.049124: step 8321, loss 0.00776906, acc 1, prec 0.109977, recall 0.807205
2017-12-10T04:06:13.308284: step 8322, loss 0.0318486, acc 0.984375, prec 0.109976, recall 0.807205
2017-12-10T04:06:13.578635: step 8323, loss 0.0721414, acc 0.984375, prec 0.109985, recall 0.807221
2017-12-10T04:06:13.844540: step 8324, loss 0.128236, acc 0.96875, prec 0.109982, recall 0.807221
2017-12-10T04:06:14.109430: step 8325, loss 5.18051, acc 0.953125, prec 0.11, recall 0.807185
2017-12-10T04:06:14.378534: step 8326, loss 0.0987409, acc 0.953125, prec 0.110007, recall 0.807202
2017-12-10T04:06:14.642735: step 8327, loss 0.266241, acc 0.953125, prec 0.110013, recall 0.807218
2017-12-10T04:06:14.908904: step 8328, loss 0.586825, acc 0.90625, prec 0.110005, recall 0.807218
2017-12-10T04:06:15.176127: step 8329, loss 0.383368, acc 0.96875, prec 0.110003, recall 0.807218
2017-12-10T04:06:15.441933: step 8330, loss 0.832625, acc 0.90625, prec 0.109995, recall 0.807218
2017-12-10T04:06:15.717596: step 8331, loss 0.549793, acc 0.9375, prec 0.110011, recall 0.80725
2017-12-10T04:06:15.981898: step 8332, loss 0.742366, acc 0.90625, prec 0.110024, recall 0.807283
2017-12-10T04:06:16.250340: step 8333, loss 0.283357, acc 0.953125, prec 0.11002, recall 0.807283
2017-12-10T04:06:16.519954: step 8334, loss 0.518167, acc 0.921875, prec 0.110024, recall 0.807299
2017-12-10T04:06:16.785192: step 8335, loss 0.487361, acc 0.890625, prec 0.110035, recall 0.807331
2017-12-10T04:06:17.052455: step 8336, loss 0.248723, acc 0.9375, prec 0.11004, recall 0.807347
2017-12-10T04:06:17.321310: step 8337, loss 0.546057, acc 0.890625, prec 0.110042, recall 0.807363
2017-12-10T04:06:17.586509: step 8338, loss 0.228393, acc 0.953125, prec 0.110048, recall 0.807379
2017-12-10T04:06:17.854060: step 8339, loss 1.90986, acc 0.84375, prec 0.110046, recall 0.807396
2017-12-10T04:06:18.123525: step 8340, loss 0.975432, acc 0.875, prec 0.110056, recall 0.807428
2017-12-10T04:06:18.389005: step 8341, loss 0.8151, acc 0.90625, prec 0.110048, recall 0.807428
2017-12-10T04:06:18.655899: step 8342, loss 0.24662, acc 0.96875, prec 0.110056, recall 0.807444
2017-12-10T04:06:18.927470: step 8343, loss 0.368047, acc 0.96875, prec 0.110074, recall 0.807476
2017-12-10T04:06:19.190310: step 8344, loss 0.242369, acc 0.9375, prec 0.110079, recall 0.807492
2017-12-10T04:06:19.456343: step 8345, loss 0.152135, acc 0.984375, prec 0.110088, recall 0.807509
2017-12-10T04:06:19.724248: step 8346, loss 0.317481, acc 0.9375, prec 0.110093, recall 0.807525
2017-12-10T04:06:19.989788: step 8347, loss 0.593601, acc 0.953125, prec 0.1101, recall 0.807541
2017-12-10T04:06:20.256657: step 8348, loss 4.14272e-05, acc 1, prec 0.11012, recall 0.807573
2017-12-10T04:06:20.525757: step 8349, loss 0.71548, acc 0.953125, prec 0.110126, recall 0.807589
2017-12-10T04:06:20.788735: step 8350, loss 0.0388912, acc 0.984375, prec 0.110135, recall 0.807605
2017-12-10T04:06:21.050914: step 8351, loss 0.00249799, acc 1, prec 0.110135, recall 0.807605
2017-12-10T04:06:21.315168: step 8352, loss 0.269982, acc 0.96875, prec 0.110133, recall 0.807605
2017-12-10T04:06:21.583079: step 8353, loss 0.023481, acc 0.984375, prec 0.110142, recall 0.807621
2017-12-10T04:06:21.847322: step 8354, loss 1.10838, acc 0.9375, prec 0.110157, recall 0.807654
2017-12-10T04:06:22.114113: step 8355, loss 0.745255, acc 0.890625, prec 0.110158, recall 0.80767
2017-12-10T04:06:22.379064: step 8356, loss 0.256088, acc 0.9375, prec 0.110153, recall 0.80767
2017-12-10T04:06:22.639433: step 8357, loss 0.00661168, acc 1, prec 0.110153, recall 0.80767
2017-12-10T04:06:22.909116: step 8358, loss 0.0655606, acc 0.96875, prec 0.110161, recall 0.807686
2017-12-10T04:06:23.173750: step 8359, loss 1.09457, acc 0.9375, prec 0.110156, recall 0.807686
2017-12-10T04:06:23.440155: step 8360, loss 0.000650279, acc 1, prec 0.110156, recall 0.807686
2017-12-10T04:06:23.702157: step 8361, loss 0.337493, acc 1, prec 0.110176, recall 0.807718
2017-12-10T04:06:23.969539: step 8362, loss 1.76266, acc 0.9375, prec 0.110172, recall 0.80765
2017-12-10T04:06:24.238264: step 8363, loss 3.00942, acc 0.96875, prec 0.110171, recall 0.807583
2017-12-10T04:06:24.511940: step 8364, loss 0.0410596, acc 0.984375, prec 0.11017, recall 0.807583
2017-12-10T04:06:24.775783: step 8365, loss 0.0380881, acc 0.984375, prec 0.110169, recall 0.807583
2017-12-10T04:06:25.042450: step 8366, loss 0.637049, acc 0.9375, prec 0.110174, recall 0.807599
2017-12-10T04:06:25.304700: step 8367, loss 0.954357, acc 0.953125, prec 0.11018, recall 0.807615
2017-12-10T04:06:25.569493: step 8368, loss 0.577777, acc 0.953125, prec 0.110186, recall 0.807631
2017-12-10T04:06:25.833226: step 8369, loss 0.376378, acc 0.9375, prec 0.110192, recall 0.807647
2017-12-10T04:06:26.098446: step 8370, loss 0.212253, acc 0.96875, prec 0.110189, recall 0.807647
2017-12-10T04:06:26.366652: step 8371, loss 0.86663, acc 0.890625, prec 0.11018, recall 0.807647
2017-12-10T04:06:26.632797: step 8372, loss 0.389054, acc 0.953125, prec 0.110176, recall 0.807647
2017-12-10T04:06:26.914026: step 8373, loss 0.646606, acc 0.890625, prec 0.110178, recall 0.807663
2017-12-10T04:06:27.184944: step 8374, loss 0.514622, acc 0.890625, prec 0.110179, recall 0.807679
2017-12-10T04:06:27.450481: step 8375, loss 0.345149, acc 0.984375, prec 0.110188, recall 0.807696
2017-12-10T04:06:27.712787: step 8376, loss 0.292567, acc 0.9375, prec 0.110183, recall 0.807696
2017-12-10T04:06:27.984593: step 8377, loss 0.411881, acc 0.9375, prec 0.110188, recall 0.807712
2017-12-10T04:06:28.254248: step 8378, loss 0.227495, acc 0.953125, prec 0.110184, recall 0.807712
2017-12-10T04:06:28.522131: step 8379, loss 0.317322, acc 0.953125, prec 0.110181, recall 0.807712
2017-12-10T04:06:28.790405: step 8380, loss 0.733069, acc 0.921875, prec 0.110184, recall 0.807728
2017-12-10T04:06:29.056177: step 8381, loss 0.514778, acc 0.90625, prec 0.110187, recall 0.807744
2017-12-10T04:06:29.324721: step 8382, loss 0.232357, acc 0.953125, prec 0.110214, recall 0.807792
2017-12-10T04:06:29.597408: step 8383, loss 0.278514, acc 0.953125, prec 0.11022, recall 0.807808
2017-12-10T04:06:29.871887: step 8384, loss 0.193663, acc 0.96875, prec 0.110218, recall 0.807808
2017-12-10T04:06:30.144543: step 8385, loss 0.208704, acc 0.96875, prec 0.110215, recall 0.807808
2017-12-10T04:06:30.417487: step 8386, loss 0.103747, acc 0.953125, prec 0.110211, recall 0.807808
2017-12-10T04:06:30.685546: step 8387, loss 0.514804, acc 0.953125, prec 0.110248, recall 0.807872
2017-12-10T04:06:30.951019: step 8388, loss 0.0247052, acc 0.96875, prec 0.110246, recall 0.807872
2017-12-10T04:06:31.218582: step 8389, loss 0.124707, acc 0.921875, prec 0.110239, recall 0.807872
2017-12-10T04:06:31.485469: step 8390, loss 0.436504, acc 0.9375, prec 0.110234, recall 0.807872
2017-12-10T04:06:31.748928: step 8391, loss 0.406986, acc 0.953125, prec 0.110231, recall 0.807872
2017-12-10T04:06:32.015236: step 8392, loss 0.486568, acc 0.953125, prec 0.110227, recall 0.807872
2017-12-10T04:06:32.279647: step 8393, loss 0.021751, acc 0.984375, prec 0.110236, recall 0.807888
2017-12-10T04:06:32.546732: step 8394, loss 0.168961, acc 0.953125, prec 0.110232, recall 0.807888
2017-12-10T04:06:32.815284: step 8395, loss 0.210625, acc 0.953125, prec 0.110248, recall 0.80792
2017-12-10T04:06:33.079111: step 8396, loss 0.204149, acc 0.984375, prec 0.110257, recall 0.807936
2017-12-10T04:06:33.342153: step 8397, loss 0.00151986, acc 1, prec 0.110267, recall 0.807953
2017-12-10T04:06:33.606953: step 8398, loss 0.000388256, acc 1, prec 0.110267, recall 0.807953
2017-12-10T04:06:33.869045: step 8399, loss 1.39763e-05, acc 1, prec 0.110288, recall 0.807985
2017-12-10T04:06:34.133566: step 8400, loss 9.70335, acc 0.984375, prec 0.110308, recall 0.807949

Evaluation:
2017-12-10T04:06:41.727837: step 8400, loss 14.8965, acc 0.964808, prec 0.110534, recall 0.802589

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-8400

2017-12-10T04:06:43.004009: step 8401, loss 0.191272, acc 0.96875, prec 0.110542, recall 0.802606
2017-12-10T04:06:43.278850: step 8402, loss 0.106124, acc 0.984375, prec 0.110541, recall 0.802606
2017-12-10T04:06:43.542869: step 8403, loss 0.000945813, acc 1, prec 0.110541, recall 0.802606
2017-12-10T04:06:43.800545: step 8404, loss 0.000585277, acc 1, prec 0.110541, recall 0.802606
2017-12-10T04:06:44.065566: step 8405, loss 0.109474, acc 0.984375, prec 0.11054, recall 0.802606
2017-12-10T04:06:44.331503: step 8406, loss 0.192185, acc 0.96875, prec 0.110547, recall 0.802622
2017-12-10T04:06:44.595254: step 8407, loss 0.451558, acc 0.96875, prec 0.110555, recall 0.802638
2017-12-10T04:06:44.858461: step 8408, loss 0.220668, acc 0.96875, prec 0.110552, recall 0.802638
2017-12-10T04:06:45.124782: step 8409, loss 0.258201, acc 0.953125, prec 0.110579, recall 0.802687
2017-12-10T04:06:45.395031: step 8410, loss 0.166429, acc 0.96875, prec 0.110576, recall 0.802687
2017-12-10T04:06:45.661578: step 8411, loss 0.178492, acc 0.96875, prec 0.110574, recall 0.802687
2017-12-10T04:06:45.926632: step 8412, loss 0.339831, acc 0.953125, prec 0.11057, recall 0.802687
2017-12-10T04:06:46.193562: step 8413, loss 0.422769, acc 0.96875, prec 0.110588, recall 0.802719
2017-12-10T04:06:46.464621: step 8414, loss 0.773741, acc 0.921875, prec 0.110602, recall 0.802752
2017-12-10T04:06:46.728182: step 8415, loss 2.52117e-05, acc 1, prec 0.110622, recall 0.802784
2017-12-10T04:06:46.987425: step 8416, loss 0.3252, acc 0.953125, prec 0.110628, recall 0.802801
2017-12-10T04:06:47.257460: step 8417, loss 2.08985, acc 0.9375, prec 0.110624, recall 0.802735
2017-12-10T04:06:47.530390: step 8418, loss 0.0270104, acc 0.984375, prec 0.110643, recall 0.802767
2017-12-10T04:06:47.791100: step 8419, loss 0.261081, acc 0.9375, prec 0.110638, recall 0.802767
2017-12-10T04:06:48.057040: step 8420, loss 0.146085, acc 0.953125, prec 0.110634, recall 0.802767
2017-12-10T04:06:48.328505: step 8421, loss 0.390273, acc 0.90625, prec 0.110647, recall 0.8028
2017-12-10T04:06:48.597416: step 8422, loss 0.96482, acc 0.890625, prec 0.110658, recall 0.802832
2017-12-10T04:06:48.870443: step 8423, loss 0.849449, acc 0.890625, prec 0.11067, recall 0.802864
2017-12-10T04:06:49.134835: step 8424, loss 0.861417, acc 0.921875, prec 0.110664, recall 0.802864
2017-12-10T04:06:49.409689: step 8425, loss 0.721756, acc 0.90625, prec 0.110666, recall 0.802881
2017-12-10T04:06:49.679145: step 8426, loss 0.657564, acc 0.90625, prec 0.110659, recall 0.802881
2017-12-10T04:06:49.940852: step 8427, loss 0.643527, acc 0.890625, prec 0.11065, recall 0.802881
2017-12-10T04:06:50.204083: step 8428, loss 1.72733, acc 0.9375, prec 0.110645, recall 0.802881
2017-12-10T04:06:50.469209: step 8429, loss 0.947278, acc 0.90625, prec 0.110647, recall 0.802897
2017-12-10T04:06:50.736215: step 8430, loss 0.280183, acc 0.953125, prec 0.110654, recall 0.802913
2017-12-10T04:06:51.001719: step 8431, loss 0.105145, acc 0.96875, prec 0.110651, recall 0.802913
2017-12-10T04:06:51.264309: step 8432, loss 0.556169, acc 0.921875, prec 0.110645, recall 0.802913
2017-12-10T04:06:51.536034: step 8433, loss 0.269179, acc 0.953125, prec 0.110661, recall 0.802946
2017-12-10T04:06:51.810471: step 8434, loss 0.792854, acc 0.875, prec 0.110651, recall 0.802946
2017-12-10T04:06:52.075066: step 8435, loss 0.319103, acc 0.9375, prec 0.110666, recall 0.802978
2017-12-10T04:06:52.341880: step 8436, loss 0.175707, acc 0.953125, prec 0.110683, recall 0.80301
2017-12-10T04:06:52.616746: step 8437, loss 0.0951179, acc 0.96875, prec 0.11069, recall 0.803027
2017-12-10T04:06:52.883555: step 8438, loss 1.33609, acc 0.90625, prec 0.110693, recall 0.803043
2017-12-10T04:06:53.149030: step 8439, loss 0.797529, acc 0.890625, prec 0.110684, recall 0.803043
2017-12-10T04:06:53.410501: step 8440, loss 0.901775, acc 0.9375, prec 0.110679, recall 0.803043
2017-12-10T04:06:53.672959: step 8441, loss 0.26759, acc 0.96875, prec 0.110707, recall 0.803091
2017-12-10T04:06:53.945472: step 8442, loss 0.530638, acc 0.9375, prec 0.110712, recall 0.803108
2017-12-10T04:06:54.216758: step 8443, loss 0.0621077, acc 0.984375, prec 0.110711, recall 0.803108
2017-12-10T04:06:54.485948: step 8444, loss 0.147607, acc 0.984375, prec 0.110709, recall 0.803108
2017-12-10T04:06:54.755797: step 8445, loss 0.00160322, acc 1, prec 0.110709, recall 0.803108
2017-12-10T04:06:55.030457: step 8446, loss 0.00612169, acc 1, prec 0.11072, recall 0.803124
2017-12-10T04:06:55.301680: step 8447, loss 0.273067, acc 0.96875, prec 0.110727, recall 0.80314
2017-12-10T04:06:55.565169: step 8448, loss 0.0147573, acc 1, prec 0.110727, recall 0.80314
2017-12-10T04:06:55.798085: step 8449, loss 0.486638, acc 0.960784, prec 0.110735, recall 0.803156
2017-12-10T04:06:56.072077: step 8450, loss 0.0686627, acc 0.984375, prec 0.110754, recall 0.803188
2017-12-10T04:06:56.345893: step 8451, loss 0.266986, acc 0.96875, prec 0.110751, recall 0.803188
2017-12-10T04:06:56.610881: step 8452, loss 0.0290298, acc 0.984375, prec 0.11077, recall 0.803221
2017-12-10T04:06:56.885769: step 8453, loss 0.443273, acc 0.984375, prec 0.110779, recall 0.803237
2017-12-10T04:06:57.150824: step 8454, loss 0.0727707, acc 0.96875, prec 0.110786, recall 0.803253
2017-12-10T04:06:57.414714: step 8455, loss 12.4509, acc 0.921875, prec 0.110781, recall 0.803187
2017-12-10T04:06:57.680596: step 8456, loss 0.000353836, acc 1, prec 0.110781, recall 0.803187
2017-12-10T04:06:57.939597: step 8457, loss 0.00102626, acc 1, prec 0.110781, recall 0.803187
2017-12-10T04:06:58.212285: step 8458, loss 0.56006, acc 0.984375, prec 0.11078, recall 0.803187
2017-12-10T04:06:58.478547: step 8459, loss 0.361914, acc 0.9375, prec 0.110785, recall 0.803203
2017-12-10T04:06:58.742858: step 8460, loss 0.069111, acc 0.984375, prec 0.110794, recall 0.803219
2017-12-10T04:06:59.013771: step 8461, loss 0.647831, acc 0.9375, prec 0.110809, recall 0.803252
2017-12-10T04:06:59.289064: step 8462, loss 0.697386, acc 0.9375, prec 0.110804, recall 0.803252
2017-12-10T04:06:59.561501: step 8463, loss 1.04485, acc 0.9375, prec 0.110819, recall 0.803284
2017-12-10T04:06:59.825333: step 8464, loss 0.530928, acc 0.921875, prec 0.110813, recall 0.803284
2017-12-10T04:07:00.090394: step 8465, loss 0.187024, acc 0.953125, prec 0.110819, recall 0.8033
2017-12-10T04:07:00.363172: step 8466, loss 0.114497, acc 0.96875, prec 0.110817, recall 0.8033
2017-12-10T04:07:00.636363: step 8467, loss 0.657943, acc 0.9375, prec 0.110832, recall 0.803333
2017-12-10T04:07:00.906123: step 8468, loss 0.491588, acc 0.921875, prec 0.110836, recall 0.803349
2017-12-10T04:07:01.174659: step 8469, loss 1.03699, acc 0.921875, prec 0.110839, recall 0.803365
2017-12-10T04:07:01.442955: step 8470, loss 0.264883, acc 0.9375, prec 0.110865, recall 0.803413
2017-12-10T04:07:01.707166: step 8471, loss 0.0462716, acc 0.953125, prec 0.110861, recall 0.803413
2017-12-10T04:07:01.976375: step 8472, loss 0.643056, acc 0.9375, prec 0.110866, recall 0.803429
2017-12-10T04:07:02.239113: step 8473, loss 0.416689, acc 0.96875, prec 0.110873, recall 0.803445
2017-12-10T04:07:02.513243: step 8474, loss 0.00426156, acc 1, prec 0.110873, recall 0.803445
2017-12-10T04:07:02.780667: step 8475, loss 0.248513, acc 0.890625, prec 0.110865, recall 0.803445
2017-12-10T04:07:03.045687: step 8476, loss 0.27202, acc 0.96875, prec 0.110862, recall 0.803445
2017-12-10T04:07:03.308122: step 8477, loss 0.0991971, acc 0.9375, prec 0.110867, recall 0.803462
2017-12-10T04:07:03.575045: step 8478, loss 0.15523, acc 0.953125, prec 0.110863, recall 0.803462
2017-12-10T04:07:03.846292: step 8479, loss 0.242525, acc 0.96875, prec 0.110861, recall 0.803462
2017-12-10T04:07:04.112116: step 8480, loss 0.27363, acc 0.984375, prec 0.11087, recall 0.803478
2017-12-10T04:07:04.378864: step 8481, loss 0.266154, acc 0.953125, prec 0.110886, recall 0.80351
2017-12-10T04:07:04.641920: step 8482, loss 0.492876, acc 0.921875, prec 0.11089, recall 0.803526
2017-12-10T04:07:04.908768: step 8483, loss 0.163015, acc 0.953125, prec 0.110896, recall 0.803542
2017-12-10T04:07:05.174578: step 8484, loss 0.212565, acc 0.96875, prec 0.110894, recall 0.803542
2017-12-10T04:07:05.439461: step 8485, loss 0.0336822, acc 0.984375, prec 0.110923, recall 0.80359
2017-12-10T04:07:05.708056: step 8486, loss 0.524136, acc 0.96875, prec 0.11092, recall 0.80359
2017-12-10T04:07:05.973202: step 8487, loss 0.0133723, acc 1, prec 0.11093, recall 0.803607
2017-12-10T04:07:06.237932: step 8488, loss 8.44741, acc 0.984375, prec 0.11095, recall 0.803573
2017-12-10T04:07:06.514713: step 8489, loss 0.718725, acc 0.96875, prec 0.110968, recall 0.803605
2017-12-10T04:07:06.781424: step 8490, loss 0.583736, acc 0.953125, prec 0.110984, recall 0.803637
2017-12-10T04:07:07.045462: step 8491, loss 0.391441, acc 0.9375, prec 0.110989, recall 0.803653
2017-12-10T04:07:07.310043: step 8492, loss 0.456403, acc 0.953125, prec 0.110985, recall 0.803653
2017-12-10T04:07:07.575879: step 8493, loss 0.180582, acc 0.9375, prec 0.11099, recall 0.803669
2017-12-10T04:07:07.845569: step 8494, loss 0.383366, acc 0.96875, prec 0.110998, recall 0.803685
2017-12-10T04:07:08.115120: step 8495, loss 0.0267879, acc 0.984375, prec 0.111007, recall 0.803702
2017-12-10T04:07:08.384516: step 8496, loss 0.118881, acc 0.984375, prec 0.111026, recall 0.803734
2017-12-10T04:07:08.658614: step 8497, loss 0.146529, acc 0.953125, prec 0.111022, recall 0.803734
2017-12-10T04:07:08.923459: step 8498, loss 1.23776, acc 0.90625, prec 0.111045, recall 0.803782
2017-12-10T04:07:09.189342: step 8499, loss 0.896107, acc 0.859375, prec 0.111043, recall 0.803798
2017-12-10T04:07:09.462965: step 8500, loss 1.18112, acc 0.859375, prec 0.111042, recall 0.803814
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-8500

2017-12-10T04:07:10.777188: step 8501, loss 0.181809, acc 0.921875, prec 0.111046, recall 0.80383
2017-12-10T04:07:11.045319: step 8502, loss 0.848017, acc 0.921875, prec 0.11105, recall 0.803846
2017-12-10T04:07:11.307564: step 8503, loss 0.12579, acc 0.953125, prec 0.111066, recall 0.803878
2017-12-10T04:07:11.579394: step 8504, loss 1.09546, acc 0.890625, prec 0.111097, recall 0.803942
2017-12-10T04:07:11.855923: step 8505, loss 0.434794, acc 0.90625, prec 0.11109, recall 0.803942
2017-12-10T04:07:12.122220: step 8506, loss 0.259672, acc 0.984375, prec 0.111099, recall 0.803958
2017-12-10T04:07:12.387116: step 8507, loss 0.942132, acc 0.890625, prec 0.1111, recall 0.803975
2017-12-10T04:07:12.655298: step 8508, loss 0.285001, acc 0.953125, prec 0.111116, recall 0.804007
2017-12-10T04:07:12.923366: step 8509, loss 0.209635, acc 0.953125, prec 0.111143, recall 0.804055
2017-12-10T04:07:13.187359: step 8510, loss 0.27932, acc 0.9375, prec 0.111148, recall 0.804071
2017-12-10T04:07:13.452067: step 8511, loss 0.320318, acc 0.921875, prec 0.111161, recall 0.804103
2017-12-10T04:07:13.716792: step 8512, loss 0.164523, acc 0.96875, prec 0.111159, recall 0.804103
2017-12-10T04:07:13.987871: step 8513, loss 1.11642, acc 0.90625, prec 0.111171, recall 0.804135
2017-12-10T04:07:14.255735: step 8514, loss 0.556436, acc 0.96875, prec 0.111199, recall 0.804183
2017-12-10T04:07:14.536007: step 8515, loss 0.161453, acc 0.953125, prec 0.111195, recall 0.804183
2017-12-10T04:07:14.802289: step 8516, loss 0.0876395, acc 0.96875, prec 0.111193, recall 0.804183
2017-12-10T04:07:15.077846: step 8517, loss 0.0424907, acc 0.984375, prec 0.111191, recall 0.804183
2017-12-10T04:07:15.346422: step 8518, loss 0.171412, acc 0.96875, prec 0.111189, recall 0.804183
2017-12-10T04:07:15.608334: step 8519, loss 0.0681525, acc 0.984375, prec 0.111188, recall 0.804183
2017-12-10T04:07:15.873663: step 8520, loss 0.408033, acc 0.96875, prec 0.111195, recall 0.804199
2017-12-10T04:07:16.147190: step 8521, loss 0.0467828, acc 0.96875, prec 0.111193, recall 0.804199
2017-12-10T04:07:16.419344: step 8522, loss 0.000751612, acc 1, prec 0.111193, recall 0.804199
2017-12-10T04:07:16.677424: step 8523, loss 0.273619, acc 0.96875, prec 0.1112, recall 0.804215
2017-12-10T04:07:16.945247: step 8524, loss 0.172328, acc 0.953125, prec 0.111217, recall 0.804247
2017-12-10T04:07:17.211168: step 8525, loss 6.67906, acc 0.984375, prec 0.111237, recall 0.804213
2017-12-10T04:07:17.480810: step 8526, loss 0.179062, acc 0.984375, prec 0.111245, recall 0.804229
2017-12-10T04:07:17.747998: step 8527, loss 0.341089, acc 0.953125, prec 0.111252, recall 0.804245
2017-12-10T04:07:18.021555: step 8528, loss 0.578033, acc 0.953125, prec 0.111248, recall 0.804245
2017-12-10T04:07:18.287727: step 8529, loss 0.333895, acc 0.9375, prec 0.111253, recall 0.804261
2017-12-10T04:07:18.555289: step 8530, loss 0.182144, acc 0.9375, prec 0.111268, recall 0.804293
2017-12-10T04:07:18.819070: step 8531, loss 0.110977, acc 0.9375, prec 0.111273, recall 0.804309
2017-12-10T04:07:19.082550: step 8532, loss 0.52993, acc 0.921875, prec 0.111277, recall 0.804325
2017-12-10T04:07:19.349285: step 8533, loss 0.296992, acc 0.9375, prec 0.111282, recall 0.804341
2017-12-10T04:07:19.624617: step 8534, loss 0.459933, acc 0.9375, prec 0.111297, recall 0.804373
2017-12-10T04:07:19.890072: step 8535, loss 0.326355, acc 0.96875, prec 0.111294, recall 0.804373
2017-12-10T04:07:20.157589: step 8536, loss 0.596753, acc 0.921875, prec 0.111318, recall 0.804421
2017-12-10T04:07:20.422632: step 8537, loss 0.0424018, acc 0.984375, prec 0.111317, recall 0.804421
2017-12-10T04:07:20.688702: step 8538, loss 1.19711, acc 0.859375, prec 0.111305, recall 0.804421
2017-12-10T04:07:20.952858: step 8539, loss 0.22861, acc 0.953125, prec 0.111302, recall 0.804421
2017-12-10T04:07:21.219549: step 8540, loss 0.533953, acc 0.921875, prec 0.111305, recall 0.804436
2017-12-10T04:07:21.487509: step 8541, loss 0.130041, acc 0.96875, prec 0.111313, recall 0.804452
2017-12-10T04:07:21.750888: step 8542, loss 0.325073, acc 0.96875, prec 0.11132, recall 0.804468
2017-12-10T04:07:22.012758: step 8543, loss 0.543061, acc 0.953125, prec 0.111317, recall 0.804468
2017-12-10T04:07:22.282488: step 8544, loss 0.457972, acc 0.921875, prec 0.111341, recall 0.804516
2017-12-10T04:07:22.558182: step 8545, loss 0.279276, acc 0.96875, prec 0.111358, recall 0.804548
2017-12-10T04:07:22.824801: step 8546, loss 0.0972885, acc 0.96875, prec 0.111366, recall 0.804564
2017-12-10T04:07:23.096658: step 8547, loss 0.68903, acc 0.953125, prec 0.111372, recall 0.80458
2017-12-10T04:07:23.367002: step 8548, loss 0.0159999, acc 0.984375, prec 0.111381, recall 0.804596
2017-12-10T04:07:23.633192: step 8549, loss 0.40887, acc 0.9375, prec 0.111376, recall 0.804596
2017-12-10T04:07:23.894141: step 8550, loss 0.487432, acc 0.953125, prec 0.111372, recall 0.804596
2017-12-10T04:07:24.162495: step 8551, loss 0.34233, acc 0.96875, prec 0.111389, recall 0.804628
2017-12-10T04:07:24.429320: step 8552, loss 0.430384, acc 0.953125, prec 0.111386, recall 0.804628
2017-12-10T04:07:24.708296: step 8553, loss 0.0546148, acc 0.96875, prec 0.111383, recall 0.804628
2017-12-10T04:07:24.973128: step 8554, loss 0.0647354, acc 0.984375, prec 0.111382, recall 0.804628
2017-12-10T04:07:25.236216: step 8555, loss 1.0393, acc 0.9375, prec 0.111387, recall 0.804644
2017-12-10T04:07:25.503793: step 8556, loss 0.0572992, acc 0.984375, prec 0.111386, recall 0.804644
2017-12-10T04:07:25.773664: step 8557, loss 6.31813e-05, acc 1, prec 0.111396, recall 0.804659
2017-12-10T04:07:26.037369: step 8558, loss 3.69598, acc 0.953125, prec 0.111403, recall 0.80461
2017-12-10T04:07:26.317486: step 8559, loss 3.20979, acc 0.984375, prec 0.111413, recall 0.80456
2017-12-10T04:07:26.594425: step 8560, loss 0.303647, acc 0.9375, prec 0.111408, recall 0.80456
2017-12-10T04:07:26.869844: step 8561, loss 0.652494, acc 0.9375, prec 0.111413, recall 0.804576
2017-12-10T04:07:27.138127: step 8562, loss 0.224077, acc 0.9375, prec 0.111418, recall 0.804592
2017-12-10T04:07:27.406223: step 8563, loss 0.766866, acc 0.90625, prec 0.111431, recall 0.804624
2017-12-10T04:07:27.671471: step 8564, loss 0.613981, acc 0.90625, prec 0.111423, recall 0.804624
2017-12-10T04:07:27.942868: step 8565, loss 0.25422, acc 0.9375, prec 0.111438, recall 0.804656
2017-12-10T04:07:28.215219: step 8566, loss 0.691767, acc 0.90625, prec 0.11144, recall 0.804672
2017-12-10T04:07:28.483215: step 8567, loss 1.25071, acc 0.890625, prec 0.111432, recall 0.804672
2017-12-10T04:07:28.746427: step 8568, loss 1.6079, acc 0.828125, prec 0.111448, recall 0.804719
2017-12-10T04:07:29.009953: step 8569, loss 0.414578, acc 0.921875, prec 0.111452, recall 0.804735
2017-12-10T04:07:29.277502: step 8570, loss 1.40463, acc 0.8125, prec 0.111447, recall 0.804751
2017-12-10T04:07:29.553608: step 8571, loss 1.72666, acc 0.796875, prec 0.11144, recall 0.804767
2017-12-10T04:07:29.818831: step 8572, loss 2.06691, acc 0.765625, prec 0.111441, recall 0.804799
2017-12-10T04:07:30.082658: step 8573, loss 1.30019, acc 0.875, prec 0.111441, recall 0.804815
2017-12-10T04:07:30.352935: step 8574, loss 1.16033, acc 0.859375, prec 0.11145, recall 0.804846
2017-12-10T04:07:30.628243: step 8575, loss 0.933701, acc 0.890625, prec 0.111461, recall 0.804878
2017-12-10T04:07:30.892474: step 8576, loss 0.124849, acc 0.9375, prec 0.111456, recall 0.804878
2017-12-10T04:07:31.161712: step 8577, loss 0.700667, acc 0.859375, prec 0.111445, recall 0.804878
2017-12-10T04:07:31.428751: step 8578, loss 1.2548, acc 0.875, prec 0.111455, recall 0.80491
2017-12-10T04:07:31.693831: step 8579, loss 1.5937, acc 0.890625, prec 0.111446, recall 0.80491
2017-12-10T04:07:31.957982: step 8580, loss 0.731255, acc 0.859375, prec 0.111445, recall 0.804926
2017-12-10T04:07:32.225984: step 8581, loss 0.204184, acc 0.96875, prec 0.111452, recall 0.804941
2017-12-10T04:07:32.490703: step 8582, loss 0.617331, acc 0.921875, prec 0.111466, recall 0.804973
2017-12-10T04:07:32.756402: step 8583, loss 0.954306, acc 0.9375, prec 0.111471, recall 0.804989
2017-12-10T04:07:33.017969: step 8584, loss 0.257482, acc 0.96875, prec 0.111479, recall 0.805005
2017-12-10T04:07:33.289826: step 8585, loss 0.773509, acc 0.9375, prec 0.111474, recall 0.805005
2017-12-10T04:07:33.553404: step 8586, loss 0.306633, acc 0.9375, prec 0.111469, recall 0.805005
2017-12-10T04:07:33.819270: step 8587, loss 0.326455, acc 0.921875, prec 0.111482, recall 0.805037
2017-12-10T04:07:34.080620: step 8588, loss 0.12059, acc 0.96875, prec 0.11149, recall 0.805052
2017-12-10T04:07:34.345701: step 8589, loss 0.395768, acc 0.96875, prec 0.111487, recall 0.805052
2017-12-10T04:07:34.607915: step 8590, loss 0.293155, acc 0.96875, prec 0.111485, recall 0.805052
2017-12-10T04:07:34.877964: step 8591, loss 0.439037, acc 0.984375, prec 0.111484, recall 0.805052
2017-12-10T04:07:35.142239: step 8592, loss 0.00633804, acc 1, prec 0.111484, recall 0.805052
2017-12-10T04:07:35.416900: step 8593, loss 0.0223148, acc 0.984375, prec 0.111492, recall 0.805068
2017-12-10T04:07:35.690442: step 8594, loss 0.22348, acc 0.9375, prec 0.111487, recall 0.805068
2017-12-10T04:07:35.956100: step 8595, loss 0.00718032, acc 1, prec 0.111497, recall 0.805084
2017-12-10T04:07:36.219236: step 8596, loss 0.151314, acc 0.96875, prec 0.111495, recall 0.805084
2017-12-10T04:07:36.487631: step 8597, loss 0.0747827, acc 0.984375, prec 0.111494, recall 0.805084
2017-12-10T04:07:36.758436: step 8598, loss 0.00302008, acc 1, prec 0.111514, recall 0.805116
2017-12-10T04:07:37.030143: step 8599, loss 0.00113444, acc 1, prec 0.111523, recall 0.805132
2017-12-10T04:07:37.300036: step 8600, loss 0.150749, acc 0.984375, prec 0.111532, recall 0.805147
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-8600

2017-12-10T04:07:38.667393: step 8601, loss 0.0105231, acc 1, prec 0.111552, recall 0.805179
2017-12-10T04:07:38.939539: step 8602, loss 0.00856154, acc 1, prec 0.111562, recall 0.805195
2017-12-10T04:07:39.207204: step 8603, loss 22.2427, acc 0.984375, prec 0.111572, recall 0.805145
2017-12-10T04:07:39.480778: step 8604, loss 0.0752417, acc 0.96875, prec 0.1116, recall 0.805193
2017-12-10T04:07:39.750804: step 8605, loss 0.0633915, acc 0.984375, prec 0.111608, recall 0.805209
2017-12-10T04:07:40.026891: step 8606, loss 0.273709, acc 0.953125, prec 0.111605, recall 0.805209
2017-12-10T04:07:40.293217: step 8607, loss 0.124451, acc 0.953125, prec 0.111601, recall 0.805209
2017-12-10T04:07:40.555464: step 8608, loss 0.0536871, acc 0.984375, prec 0.11161, recall 0.805224
2017-12-10T04:07:40.826980: step 8609, loss 0.437117, acc 0.96875, prec 0.111607, recall 0.805224
2017-12-10T04:07:41.095695: step 8610, loss 0.599483, acc 0.984375, prec 0.111616, recall 0.80524
2017-12-10T04:07:41.363976: step 8611, loss 0.159068, acc 0.953125, prec 0.111612, recall 0.80524
2017-12-10T04:07:41.627452: step 8612, loss 0.677888, acc 0.9375, prec 0.111607, recall 0.80524
2017-12-10T04:07:41.904932: step 8613, loss 0.100531, acc 0.984375, prec 0.111606, recall 0.80524
2017-12-10T04:07:42.174130: step 8614, loss 0.209722, acc 0.984375, prec 0.111615, recall 0.805256
2017-12-10T04:07:42.439787: step 8615, loss 0.284831, acc 0.953125, prec 0.111621, recall 0.805272
2017-12-10T04:07:42.705041: step 8616, loss 0.5836, acc 0.921875, prec 0.111614, recall 0.805272
2017-12-10T04:07:42.978554: step 8617, loss 0.132541, acc 0.96875, prec 0.111612, recall 0.805272
2017-12-10T04:07:43.239554: step 8618, loss 1.25428, acc 0.9375, prec 0.111637, recall 0.805319
2017-12-10T04:07:43.503182: step 8619, loss 0.504191, acc 0.96875, prec 0.111634, recall 0.805319
2017-12-10T04:07:43.769622: step 8620, loss 0.609192, acc 0.890625, prec 0.111646, recall 0.805351
2017-12-10T04:07:44.031330: step 8621, loss 0.00574005, acc 1, prec 0.111656, recall 0.805366
2017-12-10T04:07:44.295796: step 8622, loss 0.130991, acc 0.984375, prec 0.111674, recall 0.805398
2017-12-10T04:07:44.559147: step 8623, loss 0.15002, acc 0.984375, prec 0.111703, recall 0.805445
2017-12-10T04:07:44.833132: step 8624, loss 0.295284, acc 0.9375, prec 0.111708, recall 0.805461
2017-12-10T04:07:45.103567: step 8625, loss 0.413254, acc 0.96875, prec 0.111715, recall 0.805477
2017-12-10T04:07:45.370840: step 8626, loss 0.209421, acc 0.96875, prec 0.111733, recall 0.805508
2017-12-10T04:07:45.634222: step 8627, loss 0.114278, acc 0.96875, prec 0.11175, recall 0.80554
2017-12-10T04:07:45.898451: step 8628, loss 0.444743, acc 0.953125, prec 0.111747, recall 0.80554
2017-12-10T04:07:46.160747: step 8629, loss 0.616822, acc 0.9375, prec 0.111751, recall 0.805556
2017-12-10T04:07:46.421919: step 8630, loss 0.342293, acc 0.953125, prec 0.111768, recall 0.805587
2017-12-10T04:07:46.685983: step 8631, loss 0.123759, acc 0.984375, prec 0.111766, recall 0.805587
2017-12-10T04:07:46.950594: step 8632, loss 0.0465507, acc 0.984375, prec 0.111765, recall 0.805587
2017-12-10T04:07:47.222349: step 8633, loss 0.183259, acc 0.9375, prec 0.11176, recall 0.805587
2017-12-10T04:07:47.487752: step 8634, loss 0.46447, acc 0.953125, prec 0.111766, recall 0.805603
2017-12-10T04:07:47.755628: step 8635, loss 6.53832e-05, acc 1, prec 0.111776, recall 0.805619
2017-12-10T04:07:48.019874: step 8636, loss 3.32579, acc 0.96875, prec 0.111785, recall 0.805569
2017-12-10T04:07:48.290078: step 8637, loss 0.354918, acc 0.96875, prec 0.111793, recall 0.805585
2017-12-10T04:07:48.555170: step 8638, loss 0.336411, acc 0.984375, prec 0.111801, recall 0.805601
2017-12-10T04:07:48.822242: step 8639, loss 0.623053, acc 0.9375, prec 0.111796, recall 0.805601
2017-12-10T04:07:49.085731: step 8640, loss 0.0665428, acc 0.96875, prec 0.111794, recall 0.805601
2017-12-10T04:07:49.349701: step 8641, loss 0.0174407, acc 0.984375, prec 0.111792, recall 0.805601
2017-12-10T04:07:49.617215: step 8642, loss 0.536743, acc 0.96875, prec 0.11179, recall 0.805601
2017-12-10T04:07:49.886209: step 8643, loss 0.00441072, acc 1, prec 0.11179, recall 0.805601
2017-12-10T04:07:50.151908: step 8644, loss 1.30382, acc 0.921875, prec 0.111804, recall 0.805632
2017-12-10T04:07:50.416852: step 8645, loss 0.173337, acc 0.96875, prec 0.111821, recall 0.805663
2017-12-10T04:07:50.682216: step 8646, loss 0.00240194, acc 1, prec 0.111851, recall 0.805711
2017-12-10T04:07:50.950572: step 8647, loss 0.670461, acc 0.90625, prec 0.111853, recall 0.805726
2017-12-10T04:07:51.216826: step 8648, loss 0.627763, acc 0.96875, prec 0.111871, recall 0.805758
2017-12-10T04:07:51.491789: step 8649, loss 0.198523, acc 0.96875, prec 0.111878, recall 0.805773
2017-12-10T04:07:51.756526: step 8650, loss 0.0637729, acc 0.96875, prec 0.111886, recall 0.805789
2017-12-10T04:07:52.021090: step 8651, loss 0.338409, acc 0.96875, prec 0.111903, recall 0.805821
2017-12-10T04:07:52.294860: step 8652, loss 0.486313, acc 0.953125, prec 0.111899, recall 0.805821
2017-12-10T04:07:52.572513: step 8653, loss 0.338577, acc 0.96875, prec 0.111897, recall 0.805821
2017-12-10T04:07:52.834224: step 8654, loss 0.00787566, acc 1, prec 0.111897, recall 0.805821
2017-12-10T04:07:53.102367: step 8655, loss 0.664701, acc 0.921875, prec 0.111901, recall 0.805836
2017-12-10T04:07:53.377313: step 8656, loss 0.051114, acc 0.984375, prec 0.111909, recall 0.805852
2017-12-10T04:07:53.644934: step 8657, loss 0.185441, acc 0.984375, prec 0.111928, recall 0.805883
2017-12-10T04:07:53.905411: step 8658, loss 1.05068, acc 0.90625, prec 0.11194, recall 0.805915
2017-12-10T04:07:54.170830: step 8659, loss 0.342934, acc 0.96875, prec 0.111938, recall 0.805915
2017-12-10T04:07:54.432179: step 8660, loss 0.185651, acc 0.984375, prec 0.111947, recall 0.80593
2017-12-10T04:07:54.701832: step 8661, loss 0.179699, acc 0.953125, prec 0.111953, recall 0.805946
2017-12-10T04:07:54.966242: step 8662, loss 3.17705e-05, acc 1, prec 0.111973, recall 0.805977
2017-12-10T04:07:55.227141: step 8663, loss 0.00120877, acc 1, prec 0.111973, recall 0.805977
2017-12-10T04:07:55.490191: step 8664, loss 0.0018318, acc 1, prec 0.111993, recall 0.806009
2017-12-10T04:07:55.753930: step 8665, loss 0.257078, acc 0.96875, prec 0.112, recall 0.806024
2017-12-10T04:07:56.015537: step 8666, loss 0.00695021, acc 1, prec 0.11202, recall 0.806056
2017-12-10T04:07:56.276914: step 8667, loss 0.367523, acc 0.984375, prec 0.112019, recall 0.806056
2017-12-10T04:07:56.538736: step 8668, loss 0.0479968, acc 0.984375, prec 0.112018, recall 0.806056
2017-12-10T04:07:56.806577: step 8669, loss 0.000111353, acc 1, prec 0.112018, recall 0.806056
2017-12-10T04:07:57.070105: step 8670, loss 0.0851914, acc 0.984375, prec 0.112026, recall 0.806071
2017-12-10T04:07:57.340292: step 8671, loss 0.00416797, acc 1, prec 0.112036, recall 0.806087
2017-12-10T04:07:57.604270: step 8672, loss 0.352162, acc 0.953125, prec 0.112042, recall 0.806103
2017-12-10T04:07:57.865727: step 8673, loss 0.0749897, acc 0.984375, prec 0.112051, recall 0.806118
2017-12-10T04:07:58.130567: step 8674, loss 0.0173692, acc 0.984375, prec 0.11206, recall 0.806134
2017-12-10T04:07:58.394347: step 8675, loss 0.117246, acc 0.953125, prec 0.112056, recall 0.806134
2017-12-10T04:07:58.658557: step 8676, loss 0.0613974, acc 0.984375, prec 0.112055, recall 0.806134
2017-12-10T04:07:58.922223: step 8677, loss 1.14744, acc 0.953125, prec 0.112061, recall 0.80615
2017-12-10T04:07:59.186548: step 8678, loss 0.0506641, acc 1, prec 0.112071, recall 0.806165
2017-12-10T04:07:59.452250: step 8679, loss 0.00118832, acc 1, prec 0.112091, recall 0.806197
2017-12-10T04:07:59.716129: step 8680, loss 0.0379037, acc 0.984375, prec 0.1121, recall 0.806212
2017-12-10T04:07:59.983207: step 8681, loss 0.0172671, acc 0.984375, prec 0.112098, recall 0.806212
2017-12-10T04:08:00.257785: step 8682, loss 0.846621, acc 0.953125, prec 0.112114, recall 0.806243
2017-12-10T04:08:00.538878: step 8683, loss 0.205601, acc 0.984375, prec 0.112123, recall 0.806259
2017-12-10T04:08:00.802790: step 8684, loss 0.101769, acc 0.984375, prec 0.112122, recall 0.806259
2017-12-10T04:08:01.067764: step 8685, loss 2.47909e-06, acc 1, prec 0.112122, recall 0.806259
2017-12-10T04:08:01.333296: step 8686, loss 0.466101, acc 0.984375, prec 0.112121, recall 0.806259
2017-12-10T04:08:01.597752: step 8687, loss 0.0347124, acc 0.984375, prec 0.112129, recall 0.806275
2017-12-10T04:08:01.864248: step 8688, loss 0.468994, acc 0.984375, prec 0.112128, recall 0.806275
2017-12-10T04:08:02.129125: step 8689, loss 0.621275, acc 0.984375, prec 0.112137, recall 0.80629
2017-12-10T04:08:02.400181: step 8690, loss 0.000221116, acc 1, prec 0.112137, recall 0.80629
2017-12-10T04:08:02.663973: step 8691, loss 0.000233193, acc 1, prec 0.112137, recall 0.80629
2017-12-10T04:08:02.923254: step 8692, loss 0.00136622, acc 1, prec 0.112147, recall 0.806306
2017-12-10T04:08:03.188223: step 8693, loss 1.85887e-06, acc 1, prec 0.112147, recall 0.806306
2017-12-10T04:08:03.447285: step 8694, loss 6.89683e-05, acc 1, prec 0.112147, recall 0.806306
2017-12-10T04:08:03.713979: step 8695, loss 2.25378e-07, acc 1, prec 0.112147, recall 0.806306
2017-12-10T04:08:03.970608: step 8696, loss 0.000415813, acc 1, prec 0.112157, recall 0.806322
2017-12-10T04:08:04.234339: step 8697, loss 0.17415, acc 0.984375, prec 0.112175, recall 0.806353
2017-12-10T04:08:04.501806: step 8698, loss 0.689447, acc 0.984375, prec 0.112184, recall 0.806368
2017-12-10T04:08:04.769657: step 8699, loss 7.47257e-05, acc 1, prec 0.112194, recall 0.806384
2017-12-10T04:08:05.033598: step 8700, loss 0.0138604, acc 1, prec 0.112214, recall 0.806415

Evaluation:
2017-12-10T04:08:12.568728: step 8700, loss 21.1426, acc 0.976319, prec 0.112453, recall 0.800064

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-8700

2017-12-10T04:08:14.000695: step 8701, loss 0.1578, acc 0.984375, prec 0.112452, recall 0.800064
2017-12-10T04:08:14.266433: step 8702, loss 0.0145969, acc 0.984375, prec 0.11247, recall 0.800096
2017-12-10T04:08:14.524241: step 8703, loss 0.637451, acc 1, prec 0.11248, recall 0.800111
2017-12-10T04:08:14.791596: step 8704, loss 0.0674063, acc 0.984375, prec 0.112509, recall 0.800159
2017-12-10T04:08:15.052810: step 8705, loss 3.42134, acc 0.9375, prec 0.112555, recall 0.800175
2017-12-10T04:08:15.314335: step 8706, loss 0.085219, acc 0.984375, prec 0.112583, recall 0.800223
2017-12-10T04:08:15.583051: step 8707, loss 0.0130284, acc 1, prec 0.112593, recall 0.800239
2017-12-10T04:08:15.847505: step 8708, loss 1.91666, acc 0.96875, prec 0.112602, recall 0.800191
2017-12-10T04:08:16.122215: step 8709, loss 7.93632e-05, acc 1, prec 0.112602, recall 0.800191
2017-12-10T04:08:16.385508: step 8710, loss 0.361709, acc 0.96875, prec 0.112599, recall 0.800191
2017-12-10T04:08:16.660127: step 8711, loss 0.0010152, acc 1, prec 0.112599, recall 0.800191
2017-12-10T04:08:16.926011: step 8712, loss 0.706408, acc 0.9375, prec 0.112614, recall 0.800223
2017-12-10T04:08:17.189689: step 8713, loss 0.191413, acc 0.96875, prec 0.112631, recall 0.800254
2017-12-10T04:08:17.450721: step 8714, loss 0.0192498, acc 0.984375, prec 0.11263, recall 0.800254
2017-12-10T04:08:17.713485: step 8715, loss 0.476726, acc 0.921875, prec 0.112654, recall 0.800302
2017-12-10T04:08:17.977802: step 8716, loss 0.261827, acc 0.953125, prec 0.11265, recall 0.800302
2017-12-10T04:08:18.239727: step 8717, loss 0.303619, acc 0.9375, prec 0.112655, recall 0.800318
2017-12-10T04:08:18.503939: step 8718, loss 0.505364, acc 0.9375, prec 0.11265, recall 0.800318
2017-12-10T04:08:18.773987: step 8719, loss 0.279893, acc 0.9375, prec 0.112654, recall 0.800334
2017-12-10T04:08:19.040865: step 8720, loss 0.616005, acc 0.875, prec 0.112644, recall 0.800334
2017-12-10T04:08:19.306602: step 8721, loss 0.449621, acc 0.921875, prec 0.112638, recall 0.800334
2017-12-10T04:08:19.579223: step 8722, loss 0.699993, acc 0.921875, prec 0.112632, recall 0.800334
2017-12-10T04:08:19.844015: step 8723, loss 0.562259, acc 0.96875, prec 0.112639, recall 0.80035
2017-12-10T04:08:20.114396: step 8724, loss 0.879414, acc 0.9375, prec 0.112634, recall 0.80035
2017-12-10T04:08:20.385471: step 8725, loss 0.514814, acc 0.890625, prec 0.112635, recall 0.800365
2017-12-10T04:08:20.651147: step 8726, loss 0.130422, acc 0.984375, prec 0.112644, recall 0.800381
2017-12-10T04:08:20.922269: step 8727, loss 0.781629, acc 0.9375, prec 0.112639, recall 0.800381
2017-12-10T04:08:21.186215: step 8728, loss 0.341778, acc 0.9375, prec 0.112664, recall 0.800429
2017-12-10T04:08:21.451199: step 8729, loss 0.359338, acc 0.96875, prec 0.112681, recall 0.800461
2017-12-10T04:08:21.724491: step 8730, loss 0.238492, acc 0.9375, prec 0.112676, recall 0.800461
2017-12-10T04:08:21.993027: step 8731, loss 0.444919, acc 0.921875, prec 0.11268, recall 0.800476
2017-12-10T04:08:22.262768: step 8732, loss 0.444335, acc 0.953125, prec 0.112696, recall 0.800508
2017-12-10T04:08:22.530862: step 8733, loss 0.181351, acc 0.96875, prec 0.112693, recall 0.800508
2017-12-10T04:08:22.796654: step 8734, loss 0.14374, acc 0.96875, prec 0.112691, recall 0.800508
2017-12-10T04:08:23.064507: step 8735, loss 0.234907, acc 0.953125, prec 0.112687, recall 0.800508
2017-12-10T04:08:23.333039: step 8736, loss 0.223671, acc 0.921875, prec 0.11268, recall 0.800508
2017-12-10T04:08:23.598957: step 8737, loss 0.248945, acc 0.921875, prec 0.112694, recall 0.80054
2017-12-10T04:08:23.869540: step 8738, loss 0.173786, acc 0.96875, prec 0.112691, recall 0.80054
2017-12-10T04:08:24.132155: step 8739, loss 0.0284489, acc 0.984375, prec 0.11271, recall 0.800571
2017-12-10T04:08:24.394352: step 8740, loss 0.420732, acc 0.96875, prec 0.112717, recall 0.800587
2017-12-10T04:08:24.658606: step 8741, loss 0.117055, acc 0.984375, prec 0.112716, recall 0.800587
2017-12-10T04:08:24.924575: step 8742, loss 0.0811921, acc 0.984375, prec 0.112725, recall 0.800603
2017-12-10T04:08:25.189483: step 8743, loss 0.366063, acc 0.96875, prec 0.112722, recall 0.800603
2017-12-10T04:08:25.460852: step 8744, loss 0.135714, acc 0.984375, prec 0.112741, recall 0.800635
2017-12-10T04:08:25.727216: step 8745, loss 0.0219388, acc 0.984375, prec 0.112759, recall 0.800666
2017-12-10T04:08:25.993589: step 8746, loss 0.406265, acc 0.953125, prec 0.112756, recall 0.800666
2017-12-10T04:08:26.255446: step 8747, loss 0.284018, acc 0.96875, prec 0.112763, recall 0.800682
2017-12-10T04:08:26.523713: step 8748, loss 0.411917, acc 0.953125, prec 0.112759, recall 0.800682
2017-12-10T04:08:26.790986: step 8749, loss 0.000646495, acc 1, prec 0.112759, recall 0.800682
2017-12-10T04:08:27.064537: step 8750, loss 0.0287769, acc 0.984375, prec 0.112768, recall 0.800698
2017-12-10T04:08:27.334391: step 8751, loss 0.00063953, acc 1, prec 0.112768, recall 0.800698
2017-12-10T04:08:27.605198: step 8752, loss 0.0101637, acc 1, prec 0.112768, recall 0.800698
2017-12-10T04:08:27.872691: step 8753, loss 1.35762, acc 0.96875, prec 0.112777, recall 0.80065
2017-12-10T04:08:28.143744: step 8754, loss 0.207319, acc 0.984375, prec 0.112785, recall 0.800666
2017-12-10T04:08:28.412374: step 8755, loss 0.00487733, acc 1, prec 0.112785, recall 0.800666
2017-12-10T04:08:28.676960: step 8756, loss 0.20181, acc 0.984375, prec 0.112794, recall 0.800682
2017-12-10T04:08:28.942518: step 8757, loss 0.618156, acc 0.953125, prec 0.1128, recall 0.800698
2017-12-10T04:08:29.206418: step 8758, loss 0.135834, acc 0.96875, prec 0.112797, recall 0.800698
2017-12-10T04:08:29.471290: step 8759, loss 0.252499, acc 0.96875, prec 0.112795, recall 0.800698
2017-12-10T04:08:29.736284: step 8760, loss 0.127727, acc 0.96875, prec 0.112802, recall 0.800713
2017-12-10T04:08:30.005486: step 8761, loss 0.761874, acc 0.984375, prec 0.112821, recall 0.800745
2017-12-10T04:08:30.280151: step 8762, loss 1.07972, acc 0.984375, prec 0.11282, recall 0.800745
2017-12-10T04:08:30.555735: step 8763, loss 0.396171, acc 0.9375, prec 0.112815, recall 0.800745
2017-12-10T04:08:30.827401: step 8764, loss 1.39804, acc 0.9375, prec 0.112849, recall 0.800808
2017-12-10T04:08:31.096966: step 8765, loss 0.000982765, acc 1, prec 0.112849, recall 0.800808
2017-12-10T04:08:31.370266: step 8766, loss 0.58667, acc 0.984375, prec 0.112878, recall 0.800856
2017-12-10T04:08:31.647431: step 8767, loss 0.0133711, acc 0.984375, prec 0.112876, recall 0.800856
2017-12-10T04:08:31.913599: step 8768, loss 0.0684822, acc 0.96875, prec 0.112884, recall 0.800871
2017-12-10T04:08:32.178902: step 8769, loss 0.29059, acc 0.953125, prec 0.11289, recall 0.800887
2017-12-10T04:08:32.453260: step 8770, loss 0.906175, acc 0.96875, prec 0.112907, recall 0.800919
2017-12-10T04:08:32.716535: step 8771, loss 0.116154, acc 0.984375, prec 0.112906, recall 0.800919
2017-12-10T04:08:32.985401: step 8772, loss 0.57372, acc 0.9375, prec 0.112921, recall 0.80095
2017-12-10T04:08:33.253296: step 8773, loss 0.401317, acc 0.984375, prec 0.112929, recall 0.800966
2017-12-10T04:08:33.517941: step 8774, loss 2.22308, acc 0.890625, prec 0.112942, recall 0.800934
2017-12-10T04:08:33.793636: step 8775, loss 1.03049, acc 0.921875, prec 0.112965, recall 0.800981
2017-12-10T04:08:34.067909: step 8776, loss 1.18426, acc 0.890625, prec 0.112956, recall 0.800981
2017-12-10T04:08:34.333258: step 8777, loss 0.753511, acc 0.890625, prec 0.112977, recall 0.801028
2017-12-10T04:08:34.598952: step 8778, loss 1.04819, acc 0.90625, prec 0.112989, recall 0.80106
2017-12-10T04:08:34.872305: step 8779, loss 0.892356, acc 0.890625, prec 0.11299, recall 0.801076
2017-12-10T04:08:35.142428: step 8780, loss 0.82031, acc 0.875, prec 0.113, recall 0.801107
2017-12-10T04:08:35.407723: step 8781, loss 0.609016, acc 0.875, prec 0.113, recall 0.801123
2017-12-10T04:08:35.675557: step 8782, loss 0.878692, acc 0.875, prec 0.11301, recall 0.801154
2017-12-10T04:08:35.942105: step 8783, loss 0.92377, acc 0.90625, prec 0.113012, recall 0.80117
2017-12-10T04:08:36.206945: step 8784, loss 2.1103, acc 0.890625, prec 0.113013, recall 0.801186
2017-12-10T04:08:36.474239: step 8785, loss 1.03883, acc 0.90625, prec 0.113025, recall 0.801217
2017-12-10T04:08:36.739768: step 8786, loss 1.52236, acc 0.84375, prec 0.113022, recall 0.801233
2017-12-10T04:08:37.012260: step 8787, loss 0.564151, acc 0.9375, prec 0.113017, recall 0.801233
2017-12-10T04:08:37.279015: step 8788, loss 0.634691, acc 0.921875, prec 0.113011, recall 0.801233
2017-12-10T04:08:37.544329: step 8789, loss 1.47337, acc 0.890625, prec 0.113012, recall 0.801249
2017-12-10T04:08:37.815615: step 8790, loss 0.743663, acc 0.9375, prec 0.113017, recall 0.801264
2017-12-10T04:08:38.083622: step 8791, loss 0.136223, acc 0.953125, prec 0.113023, recall 0.80128
2017-12-10T04:08:38.352926: step 8792, loss 0.807028, acc 0.921875, prec 0.113046, recall 0.801327
2017-12-10T04:08:38.623611: step 8793, loss 0.349165, acc 0.921875, prec 0.11304, recall 0.801327
2017-12-10T04:08:38.886250: step 8794, loss 0.272732, acc 0.9375, prec 0.113045, recall 0.801343
2017-12-10T04:08:39.147911: step 8795, loss 0.690726, acc 0.921875, prec 0.113049, recall 0.801359
2017-12-10T04:08:39.414656: step 8796, loss 0.0222582, acc 0.984375, prec 0.113087, recall 0.801421
2017-12-10T04:08:39.686211: step 8797, loss 0.266346, acc 0.984375, prec 0.113096, recall 0.801437
2017-12-10T04:08:39.950417: step 8798, loss 0.0391919, acc 0.984375, prec 0.113094, recall 0.801437
2017-12-10T04:08:40.214957: step 8799, loss 0.0134495, acc 1, prec 0.113094, recall 0.801437
2017-12-10T04:08:40.481410: step 8800, loss 0.181727, acc 0.984375, prec 0.113113, recall 0.801468
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-8800

2017-12-10T04:08:41.836535: step 8801, loss 0.0114189, acc 1, prec 0.113133, recall 0.8015
2017-12-10T04:08:42.106595: step 8802, loss 0.332513, acc 0.96875, prec 0.11316, recall 0.801547
2017-12-10T04:08:42.371463: step 8803, loss 0.0406239, acc 0.96875, prec 0.113157, recall 0.801547
2017-12-10T04:08:42.635281: step 8804, loss 0.0390436, acc 0.984375, prec 0.113156, recall 0.801547
2017-12-10T04:08:42.903619: step 8805, loss 0.00623225, acc 1, prec 0.113176, recall 0.801578
2017-12-10T04:08:43.172769: step 8806, loss 0.00013356, acc 1, prec 0.113195, recall 0.801609
2017-12-10T04:08:43.435050: step 8807, loss 0.00468616, acc 1, prec 0.113215, recall 0.801641
2017-12-10T04:08:43.700398: step 8808, loss 0.0887804, acc 0.96875, prec 0.113222, recall 0.801656
2017-12-10T04:08:43.963503: step 8809, loss 0.00259929, acc 1, prec 0.113222, recall 0.801656
2017-12-10T04:08:44.226276: step 8810, loss 0.928836, acc 0.890625, prec 0.113214, recall 0.801656
2017-12-10T04:08:44.490860: step 8811, loss 0.182167, acc 0.96875, prec 0.113221, recall 0.801672
2017-12-10T04:08:44.755457: step 8812, loss 0.000398088, acc 1, prec 0.113231, recall 0.801687
2017-12-10T04:08:45.023064: step 8813, loss 0.0678082, acc 0.96875, prec 0.113228, recall 0.801687
2017-12-10T04:08:45.289722: step 8814, loss 0.316085, acc 0.984375, prec 0.113227, recall 0.801687
2017-12-10T04:08:45.554953: step 8815, loss 3.2497e-05, acc 1, prec 0.113237, recall 0.801703
2017-12-10T04:08:45.817980: step 8816, loss 0.0242635, acc 0.984375, prec 0.113236, recall 0.801703
2017-12-10T04:08:46.087933: step 8817, loss 0.784357, acc 0.9375, prec 0.113231, recall 0.801703
2017-12-10T04:08:46.357985: step 8818, loss 0.606698, acc 0.953125, prec 0.113227, recall 0.801703
2017-12-10T04:08:46.618022: step 8819, loss 0.00560275, acc 1, prec 0.113227, recall 0.801703
2017-12-10T04:08:46.879438: step 8820, loss 0.0856541, acc 0.96875, prec 0.113234, recall 0.801719
2017-12-10T04:08:47.154340: step 8821, loss 5.06964e-06, acc 1, prec 0.113234, recall 0.801719
2017-12-10T04:08:47.410003: step 8822, loss 0.274917, acc 0.984375, prec 0.113243, recall 0.801734
2017-12-10T04:08:47.678953: step 8823, loss 0.00274944, acc 1, prec 0.113243, recall 0.801734
2017-12-10T04:08:47.948842: step 8824, loss 16.6953, acc 0.96875, prec 0.113243, recall 0.801608
2017-12-10T04:08:48.221018: step 8825, loss 2.33831e-05, acc 1, prec 0.113243, recall 0.801608
2017-12-10T04:08:48.480983: step 8826, loss 0.269226, acc 0.96875, prec 0.11324, recall 0.801608
2017-12-10T04:08:48.746089: step 8827, loss 0.285127, acc 0.96875, prec 0.113238, recall 0.801608
2017-12-10T04:08:49.014532: step 8828, loss 0.379431, acc 0.96875, prec 0.113245, recall 0.801624
2017-12-10T04:08:49.278279: step 8829, loss 0.294999, acc 0.9375, prec 0.11325, recall 0.801639
2017-12-10T04:08:49.548429: step 8830, loss 0.361153, acc 0.921875, prec 0.113254, recall 0.801655
2017-12-10T04:08:49.815387: step 8831, loss 1.136, acc 0.9375, prec 0.113258, recall 0.80167
2017-12-10T04:08:50.084304: step 8832, loss 1.03576, acc 0.9375, prec 0.113273, recall 0.801702
2017-12-10T04:08:50.354805: step 8833, loss 0.0722177, acc 0.984375, prec 0.113272, recall 0.801702
2017-12-10T04:08:50.619741: step 8834, loss 0.382264, acc 0.921875, prec 0.113275, recall 0.801717
2017-12-10T04:08:50.888413: step 8835, loss 1.12618, acc 0.875, prec 0.113275, recall 0.801733
2017-12-10T04:08:51.158981: step 8836, loss 0.72688, acc 0.890625, prec 0.113266, recall 0.801733
2017-12-10T04:08:51.423641: step 8837, loss 0.647094, acc 0.890625, prec 0.113277, recall 0.801764
2017-12-10T04:08:51.689585: step 8838, loss 0.868831, acc 0.890625, prec 0.113268, recall 0.801764
2017-12-10T04:08:51.952800: step 8839, loss 0.716998, acc 0.96875, prec 0.113276, recall 0.80178
2017-12-10T04:08:52.216037: step 8840, loss 0.161818, acc 0.96875, prec 0.113283, recall 0.801795
2017-12-10T04:08:52.485812: step 8841, loss 0.150933, acc 0.953125, prec 0.113289, recall 0.801811
2017-12-10T04:08:52.747001: step 8842, loss 0.642942, acc 0.921875, prec 0.113293, recall 0.801827
2017-12-10T04:08:53.013805: step 8843, loss 0.352068, acc 0.9375, prec 0.113288, recall 0.801827
2017-12-10T04:08:53.278121: step 8844, loss 1.86365, acc 0.828125, prec 0.113284, recall 0.801842
2017-12-10T04:08:53.545514: step 8845, loss 0.703473, acc 0.953125, prec 0.11329, recall 0.801858
2017-12-10T04:08:53.815042: step 8846, loss 0.357343, acc 0.96875, prec 0.113287, recall 0.801858
2017-12-10T04:08:54.084519: step 8847, loss 0.0731155, acc 0.96875, prec 0.113295, recall 0.801873
2017-12-10T04:08:54.350682: step 8848, loss 1.04063, acc 0.90625, prec 0.113287, recall 0.801873
2017-12-10T04:08:54.614774: step 8849, loss 0.629292, acc 0.921875, prec 0.113281, recall 0.801873
2017-12-10T04:08:54.886406: step 8850, loss 0.113421, acc 0.953125, prec 0.113277, recall 0.801873
2017-12-10T04:08:55.150031: step 8851, loss 0.883448, acc 0.890625, prec 0.113288, recall 0.801905
2017-12-10T04:08:55.412283: step 8852, loss 0.394611, acc 0.953125, prec 0.113284, recall 0.801905
2017-12-10T04:08:55.675436: step 8853, loss 1.20511, acc 0.90625, prec 0.113277, recall 0.801905
2017-12-10T04:08:55.942309: step 8854, loss 0.0478021, acc 0.96875, prec 0.113284, recall 0.80192
2017-12-10T04:08:56.205857: step 8855, loss 0.414461, acc 0.96875, prec 0.113281, recall 0.80192
2017-12-10T04:08:56.479206: step 8856, loss 0.0453963, acc 0.96875, prec 0.113279, recall 0.80192
2017-12-10T04:08:56.750221: step 8857, loss 0.703349, acc 0.953125, prec 0.113295, recall 0.801951
2017-12-10T04:08:57.026203: step 8858, loss 0.0118549, acc 1, prec 0.113295, recall 0.801951
2017-12-10T04:08:57.293966: step 8859, loss 0.424294, acc 0.9375, prec 0.113309, recall 0.801983
2017-12-10T04:08:57.565697: step 8860, loss 0.13245, acc 0.96875, prec 0.113317, recall 0.801998
2017-12-10T04:08:57.836284: step 8861, loss 0.249983, acc 0.96875, prec 0.113314, recall 0.801998
2017-12-10T04:08:58.097824: step 8862, loss 0.14655, acc 0.984375, prec 0.113323, recall 0.802014
2017-12-10T04:08:58.365617: step 8863, loss 0.593729, acc 0.96875, prec 0.11332, recall 0.802014
2017-12-10T04:08:58.637199: step 8864, loss 0.0885611, acc 0.984375, prec 0.113329, recall 0.802029
2017-12-10T04:08:58.905488: step 8865, loss 0.0102902, acc 1, prec 0.113329, recall 0.802029
2017-12-10T04:08:59.170370: step 8866, loss 0.0279087, acc 0.984375, prec 0.113347, recall 0.80206
2017-12-10T04:08:59.439903: step 8867, loss 0.00319644, acc 1, prec 0.113347, recall 0.80206
2017-12-10T04:08:59.708250: step 8868, loss 0.259381, acc 0.96875, prec 0.113345, recall 0.80206
2017-12-10T04:08:59.973493: step 8869, loss 0.000199862, acc 1, prec 0.113345, recall 0.80206
2017-12-10T04:09:00.245034: step 8870, loss 0.649301, acc 0.96875, prec 0.113352, recall 0.802076
2017-12-10T04:09:00.515722: step 8871, loss 0.000167606, acc 1, prec 0.113372, recall 0.802107
2017-12-10T04:09:00.773492: step 8872, loss 0.00891695, acc 1, prec 0.113382, recall 0.802123
2017-12-10T04:09:01.037447: step 8873, loss 7.48284e-05, acc 1, prec 0.113392, recall 0.802138
2017-12-10T04:09:01.298883: step 8874, loss 0.0452162, acc 0.984375, prec 0.1134, recall 0.802154
2017-12-10T04:09:01.562486: step 8875, loss 0.846307, acc 0.9375, prec 0.113415, recall 0.802185
2017-12-10T04:09:01.826679: step 8876, loss 0.0310047, acc 0.984375, prec 0.113423, recall 0.8022
2017-12-10T04:09:02.092718: step 8877, loss 2.79397e-08, acc 1, prec 0.113423, recall 0.8022
2017-12-10T04:09:02.346517: step 8878, loss 0.244605, acc 0.984375, prec 0.113432, recall 0.802216
2017-12-10T04:09:02.612664: step 8879, loss 0.000146677, acc 1, prec 0.113442, recall 0.802231
2017-12-10T04:09:02.875262: step 8880, loss 2.03028e-07, acc 1, prec 0.113442, recall 0.802231
2017-12-10T04:09:03.133154: step 8881, loss 1.19019, acc 0.96875, prec 0.113459, recall 0.802263
2017-12-10T04:09:03.403158: step 8882, loss 0.0940897, acc 0.984375, prec 0.113458, recall 0.802263
2017-12-10T04:09:03.667089: step 8883, loss 0.0908232, acc 0.984375, prec 0.113466, recall 0.802278
2017-12-10T04:09:03.932979: step 8884, loss 3.48495, acc 0.984375, prec 0.113466, recall 0.802215
2017-12-10T04:09:04.207640: step 8885, loss 9.77998e-05, acc 1, prec 0.113496, recall 0.802262
2017-12-10T04:09:04.472594: step 8886, loss 0.153238, acc 0.984375, prec 0.113505, recall 0.802277
2017-12-10T04:09:04.739596: step 8887, loss 0.790365, acc 0.9375, prec 0.113529, recall 0.802324
2017-12-10T04:09:05.000373: step 8888, loss 0.0636596, acc 0.984375, prec 0.113528, recall 0.802324
2017-12-10T04:09:05.260035: step 8889, loss 0.000197618, acc 1, prec 0.113528, recall 0.802324
2017-12-10T04:09:05.522257: step 8890, loss 0.500808, acc 0.984375, prec 0.113546, recall 0.802355
2017-12-10T04:09:05.789579: step 8891, loss 0.577608, acc 0.96875, prec 0.113544, recall 0.802355
2017-12-10T04:09:06.053631: step 8892, loss 0.245437, acc 0.921875, prec 0.113557, recall 0.802386
2017-12-10T04:09:06.324620: step 8893, loss 0.261259, acc 0.96875, prec 0.113564, recall 0.802401
2017-12-10T04:09:06.599344: step 8894, loss 0.45424, acc 0.9375, prec 0.113569, recall 0.802417
2017-12-10T04:09:06.864052: step 8895, loss 0.434026, acc 0.9375, prec 0.113574, recall 0.802432
2017-12-10T04:09:07.127520: step 8896, loss 0.199976, acc 0.9375, prec 0.113569, recall 0.802432
2017-12-10T04:09:07.389951: step 8897, loss 0.904191, acc 0.9375, prec 0.113574, recall 0.802448
2017-12-10T04:09:07.655456: step 8898, loss 0.741772, acc 0.90625, prec 0.113566, recall 0.802448
2017-12-10T04:09:07.919552: step 8899, loss 1.02953, acc 0.921875, prec 0.113589, recall 0.802494
2017-12-10T04:09:08.185989: step 8900, loss 0.125528, acc 0.984375, prec 0.113608, recall 0.802525
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-8900

2017-12-10T04:09:09.512294: step 8901, loss 1.0878, acc 0.9375, prec 0.113603, recall 0.802525
2017-12-10T04:09:09.785391: step 8902, loss 0.289307, acc 0.9375, prec 0.113608, recall 0.802541
2017-12-10T04:09:10.047415: step 8903, loss 0.40017, acc 0.890625, prec 0.113599, recall 0.802541
2017-12-10T04:09:10.313492: step 8904, loss 0.49625, acc 0.9375, prec 0.113604, recall 0.802556
2017-12-10T04:09:10.580143: step 8905, loss 0.235092, acc 0.953125, prec 0.1136, recall 0.802556
2017-12-10T04:09:10.851235: step 8906, loss 1.62689, acc 0.859375, prec 0.113588, recall 0.802556
2017-12-10T04:09:11.114478: step 8907, loss 0.0415368, acc 0.96875, prec 0.113586, recall 0.802556
2017-12-10T04:09:11.383954: step 8908, loss 1.35397, acc 0.90625, prec 0.113578, recall 0.802556
2017-12-10T04:09:11.654639: step 8909, loss 0.250501, acc 0.96875, prec 0.113586, recall 0.802572
2017-12-10T04:09:11.937661: step 8910, loss 0.395763, acc 0.921875, prec 0.113579, recall 0.802572
2017-12-10T04:09:12.205419: step 8911, loss 0.920334, acc 0.875, prec 0.113579, recall 0.802587
2017-12-10T04:09:12.481266: step 8912, loss 0.716901, acc 0.9375, prec 0.113574, recall 0.802587
2017-12-10T04:09:12.751253: step 8913, loss 0.0615167, acc 0.96875, prec 0.113581, recall 0.802603
2017-12-10T04:09:13.012255: step 8914, loss 0.149325, acc 0.984375, prec 0.11359, recall 0.802618
2017-12-10T04:09:13.285884: step 8915, loss 0.0965352, acc 0.984375, prec 0.113599, recall 0.802634
2017-12-10T04:09:13.553321: step 8916, loss 0.354358, acc 0.984375, prec 0.113597, recall 0.802634
2017-12-10T04:09:13.818357: step 8917, loss 0.00435951, acc 1, prec 0.113607, recall 0.802649
2017-12-10T04:09:14.081151: step 8918, loss 0.181733, acc 0.96875, prec 0.113614, recall 0.802665
2017-12-10T04:09:14.344204: step 8919, loss 0.0434995, acc 0.984375, prec 0.113623, recall 0.80268
2017-12-10T04:09:14.617231: step 8920, loss 3.64345, acc 0.953125, prec 0.11362, recall 0.802617
2017-12-10T04:09:14.883231: step 8921, loss 0.389897, acc 0.9375, prec 0.113615, recall 0.802617
2017-12-10T04:09:15.145592: step 8922, loss 0.108474, acc 0.96875, prec 0.113613, recall 0.802617
2017-12-10T04:09:15.407135: step 8923, loss 0.0566854, acc 0.96875, prec 0.11361, recall 0.802617
2017-12-10T04:09:15.676712: step 8924, loss 0.00412947, acc 1, prec 0.11361, recall 0.802617
2017-12-10T04:09:15.941598: step 8925, loss 0.30653, acc 0.96875, prec 0.113618, recall 0.802633
2017-12-10T04:09:16.216477: step 8926, loss 0.218673, acc 0.96875, prec 0.113615, recall 0.802633
2017-12-10T04:09:16.486059: step 8927, loss 0.0040338, acc 1, prec 0.113625, recall 0.802648
2017-12-10T04:09:16.752574: step 8928, loss 0.585478, acc 0.96875, prec 0.113632, recall 0.802664
2017-12-10T04:09:17.023800: step 8929, loss 0.417046, acc 0.9375, prec 0.113637, recall 0.802679
2017-12-10T04:09:17.290748: step 8930, loss 0.745085, acc 0.9375, prec 0.113642, recall 0.802694
2017-12-10T04:09:17.559167: step 8931, loss 0.030661, acc 0.96875, prec 0.113659, recall 0.802725
2017-12-10T04:09:17.828046: step 8932, loss 0.116222, acc 0.984375, prec 0.113668, recall 0.802741
2017-12-10T04:09:18.099171: step 8933, loss 0.198052, acc 0.953125, prec 0.113674, recall 0.802756
2017-12-10T04:09:18.359785: step 8934, loss 0.92875, acc 0.953125, prec 0.11367, recall 0.802756
2017-12-10T04:09:18.628827: step 8935, loss 0.321556, acc 0.953125, prec 0.113676, recall 0.802772
2017-12-10T04:09:18.894294: step 8936, loss 0.006454, acc 1, prec 0.113696, recall 0.802803
2017-12-10T04:09:19.157251: step 8937, loss 0.742811, acc 0.953125, prec 0.113702, recall 0.802818
2017-12-10T04:09:19.423172: step 8938, loss 0.399618, acc 0.96875, prec 0.113709, recall 0.802833
2017-12-10T04:09:19.688362: step 8939, loss 0.013046, acc 0.984375, prec 0.113717, recall 0.802849
2017-12-10T04:09:19.953439: step 8940, loss 0.14319, acc 0.953125, prec 0.113714, recall 0.802849
2017-12-10T04:09:20.219359: step 8941, loss 0.325801, acc 0.984375, prec 0.113732, recall 0.80288
2017-12-10T04:09:20.495018: step 8942, loss 0.569151, acc 0.953125, prec 0.113738, recall 0.802895
2017-12-10T04:09:20.756010: step 8943, loss 0.125728, acc 0.96875, prec 0.113755, recall 0.802926
2017-12-10T04:09:21.021327: step 8944, loss 0.109364, acc 0.984375, prec 0.113764, recall 0.802941
2017-12-10T04:09:21.298641: step 8945, loss 0.0592318, acc 0.953125, prec 0.11378, recall 0.802972
2017-12-10T04:09:21.528941: step 8946, loss 0.481474, acc 0.980392, prec 0.113778, recall 0.802972
2017-12-10T04:09:21.802948: step 8947, loss 0.0170877, acc 0.984375, prec 0.113797, recall 0.803003
2017-12-10T04:09:22.067456: step 8948, loss 0.0642347, acc 0.984375, prec 0.113815, recall 0.803034
2017-12-10T04:09:22.345152: step 8949, loss 0.0773884, acc 0.984375, prec 0.113814, recall 0.803034
2017-12-10T04:09:22.615414: step 8950, loss 0.288958, acc 0.96875, prec 0.113811, recall 0.803034
2017-12-10T04:09:22.880144: step 8951, loss 0.0385485, acc 0.984375, prec 0.11383, recall 0.803065
2017-12-10T04:09:23.141826: step 8952, loss 0.00355119, acc 1, prec 0.11383, recall 0.803065
2017-12-10T04:09:23.415335: step 8953, loss 0.00135365, acc 1, prec 0.11384, recall 0.80308
2017-12-10T04:09:23.677629: step 8954, loss 0.00768218, acc 1, prec 0.113849, recall 0.803095
2017-12-10T04:09:23.941096: step 8955, loss 0.138618, acc 0.96875, prec 0.113847, recall 0.803095
2017-12-10T04:09:24.204376: step 8956, loss 0.00548822, acc 1, prec 0.113857, recall 0.803111
2017-12-10T04:09:24.464579: step 8957, loss 0.362809, acc 0.96875, prec 0.113864, recall 0.803126
2017-12-10T04:09:24.725204: step 8958, loss 1.28761, acc 0.9375, prec 0.113859, recall 0.803126
2017-12-10T04:09:24.989718: step 8959, loss 0.114971, acc 0.984375, prec 0.113858, recall 0.803126
2017-12-10T04:09:25.255275: step 8960, loss 0.0431272, acc 0.984375, prec 0.113876, recall 0.803157
2017-12-10T04:09:25.520258: step 8961, loss 0.534816, acc 0.96875, prec 0.113874, recall 0.803157
2017-12-10T04:09:25.781548: step 8962, loss 0.572552, acc 0.953125, prec 0.113899, recall 0.803203
2017-12-10T04:09:26.053005: step 8963, loss 0.0185804, acc 0.984375, prec 0.113918, recall 0.803234
2017-12-10T04:09:26.314475: step 8964, loss 0.073409, acc 0.984375, prec 0.113916, recall 0.803234
2017-12-10T04:09:26.577449: step 8965, loss 0.0248736, acc 0.984375, prec 0.113925, recall 0.803249
2017-12-10T04:09:26.845321: step 8966, loss 6.99292e-05, acc 1, prec 0.113925, recall 0.803249
2017-12-10T04:09:27.114269: step 8967, loss 0.214214, acc 0.984375, prec 0.113953, recall 0.803295
2017-12-10T04:09:27.379609: step 8968, loss 0.288653, acc 0.9375, prec 0.113948, recall 0.803295
2017-12-10T04:09:27.642563: step 8969, loss 0.0816142, acc 0.984375, prec 0.113957, recall 0.803311
2017-12-10T04:09:27.904109: step 8970, loss 2.70966, acc 0.96875, prec 0.113955, recall 0.803248
2017-12-10T04:09:28.172435: step 8971, loss 0.869892, acc 0.921875, prec 0.113969, recall 0.803279
2017-12-10T04:09:28.437786: step 8972, loss 0.449419, acc 0.984375, prec 0.113987, recall 0.803309
2017-12-10T04:09:28.710249: step 8973, loss 0.0765565, acc 0.984375, prec 0.113996, recall 0.803325
2017-12-10T04:09:28.975445: step 8974, loss 0.241289, acc 0.984375, prec 0.114004, recall 0.80334
2017-12-10T04:09:29.239047: step 8975, loss 0.311397, acc 0.953125, prec 0.11402, recall 0.803371
2017-12-10T04:09:29.507782: step 8976, loss 0.252225, acc 0.96875, prec 0.114027, recall 0.803386
2017-12-10T04:09:29.771375: step 8977, loss 0.155579, acc 0.984375, prec 0.114036, recall 0.803401
2017-12-10T04:09:30.042106: step 8978, loss 0.0714312, acc 0.984375, prec 0.114054, recall 0.803432
2017-12-10T04:09:30.315846: step 8979, loss 0.248666, acc 0.96875, prec 0.114052, recall 0.803432
2017-12-10T04:09:30.580989: step 8980, loss 0.539524, acc 0.953125, prec 0.114048, recall 0.803432
2017-12-10T04:09:30.848370: step 8981, loss 0.0148374, acc 0.984375, prec 0.114056, recall 0.803447
2017-12-10T04:09:31.114063: step 8982, loss 0.0500079, acc 0.96875, prec 0.114073, recall 0.803478
2017-12-10T04:09:31.381336: step 8983, loss 0.187756, acc 0.984375, prec 0.114082, recall 0.803493
2017-12-10T04:09:31.651495: step 8984, loss 0.237659, acc 0.96875, prec 0.114079, recall 0.803493
2017-12-10T04:09:31.918837: step 8985, loss 0.0283881, acc 0.984375, prec 0.114078, recall 0.803493
2017-12-10T04:09:32.180334: step 8986, loss 0.228169, acc 0.9375, prec 0.114073, recall 0.803493
2017-12-10T04:09:32.438178: step 8987, loss 0.115396, acc 0.984375, prec 0.114072, recall 0.803493
2017-12-10T04:09:32.699627: step 8988, loss 0.355639, acc 0.953125, prec 0.114098, recall 0.803539
2017-12-10T04:09:32.969002: step 8989, loss 0.125541, acc 0.9375, prec 0.114092, recall 0.803539
2017-12-10T04:09:33.233924: step 8990, loss 0.311439, acc 0.984375, prec 0.114111, recall 0.80357
2017-12-10T04:09:33.497663: step 8991, loss 0.21505, acc 0.96875, prec 0.114138, recall 0.803616
2017-12-10T04:09:33.767671: step 8992, loss 0.267449, acc 0.921875, prec 0.114131, recall 0.803616
2017-12-10T04:09:34.034616: step 8993, loss 0.658276, acc 0.9375, prec 0.114146, recall 0.803647
2017-12-10T04:09:34.298356: step 8994, loss 0.0178232, acc 0.984375, prec 0.114145, recall 0.803647
2017-12-10T04:09:34.559339: step 8995, loss 1.34467, acc 0.96875, prec 0.114153, recall 0.803599
2017-12-10T04:09:34.834576: step 8996, loss 0.128207, acc 0.953125, prec 0.114159, recall 0.803615
2017-12-10T04:09:35.091802: step 8997, loss 0.436829, acc 0.96875, prec 0.114157, recall 0.803615
2017-12-10T04:09:35.355766: step 8998, loss 0.340539, acc 0.96875, prec 0.114164, recall 0.80363
2017-12-10T04:09:35.622737: step 8999, loss 0.00665732, acc 1, prec 0.114174, recall 0.803645
2017-12-10T04:09:35.892919: step 9000, loss 0.259273, acc 0.953125, prec 0.11419, recall 0.803676

Evaluation:
2017-12-10T04:09:43.511991: step 9000, loss 9.20237, acc 0.929805, prec 0.114166, recall 0.800877

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-9000

2017-12-10T04:09:44.856812: step 9001, loss 0.235435, acc 0.96875, prec 0.114173, recall 0.800893
2017-12-10T04:09:45.124953: step 9002, loss 0.407443, acc 0.953125, prec 0.114189, recall 0.800923
2017-12-10T04:09:45.395845: step 9003, loss 0.336729, acc 0.9375, prec 0.114194, recall 0.800939
2017-12-10T04:09:45.666148: step 9004, loss 0.682127, acc 0.921875, prec 0.114197, recall 0.800954
2017-12-10T04:09:45.932688: step 9005, loss 1.31659, acc 0.90625, prec 0.114209, recall 0.800985
2017-12-10T04:09:46.203219: step 9006, loss 0.476355, acc 0.90625, prec 0.114211, recall 0.801
2017-12-10T04:09:46.476722: step 9007, loss 0.349603, acc 0.984375, prec 0.11422, recall 0.801015
2017-12-10T04:09:46.737325: step 9008, loss 0.738503, acc 0.953125, prec 0.114226, recall 0.801031
2017-12-10T04:09:47.009777: step 9009, loss 0.13414, acc 0.953125, prec 0.114222, recall 0.801031
2017-12-10T04:09:47.281977: step 9010, loss 0.482118, acc 0.90625, prec 0.114215, recall 0.801031
2017-12-10T04:09:47.546538: step 9011, loss 1.28098, acc 0.890625, prec 0.114206, recall 0.801031
2017-12-10T04:09:47.814427: step 9012, loss 0.240907, acc 0.921875, prec 0.114209, recall 0.801046
2017-12-10T04:09:48.082271: step 9013, loss 0.276892, acc 0.9375, prec 0.114233, recall 0.801092
2017-12-10T04:09:48.351132: step 9014, loss 0.283307, acc 0.953125, prec 0.11423, recall 0.801092
2017-12-10T04:09:48.615552: step 9015, loss 0.141799, acc 0.984375, prec 0.114228, recall 0.801092
2017-12-10T04:09:48.881133: step 9016, loss 0.196689, acc 0.984375, prec 0.114227, recall 0.801092
2017-12-10T04:09:49.148356: step 9017, loss 0.119583, acc 0.984375, prec 0.114226, recall 0.801092
2017-12-10T04:09:49.412958: step 9018, loss 0.04995, acc 0.984375, prec 0.114234, recall 0.801107
2017-12-10T04:09:49.676322: step 9019, loss 0.0775322, acc 0.96875, prec 0.114242, recall 0.801122
2017-12-10T04:09:49.938735: step 9020, loss 0.00102554, acc 1, prec 0.114251, recall 0.801138
2017-12-10T04:09:50.202911: step 9021, loss 0.333437, acc 0.96875, prec 0.114258, recall 0.801153
2017-12-10T04:09:50.469778: step 9022, loss 0.444077, acc 0.953125, prec 0.114264, recall 0.801168
2017-12-10T04:09:50.735595: step 9023, loss 0.231547, acc 0.984375, prec 0.114273, recall 0.801184
2017-12-10T04:09:51.004118: step 9024, loss 0.136539, acc 0.984375, prec 0.114272, recall 0.801184
2017-12-10T04:09:51.265473: step 9025, loss 0.515517, acc 0.9375, prec 0.114286, recall 0.801214
2017-12-10T04:09:51.535706: step 9026, loss 0.276134, acc 0.953125, prec 0.114292, recall 0.801229
2017-12-10T04:09:51.800896: step 9027, loss 0.539348, acc 0.96875, prec 0.114299, recall 0.801245
2017-12-10T04:09:52.061678: step 9028, loss 0.00200559, acc 1, prec 0.114299, recall 0.801245
2017-12-10T04:09:52.322994: step 9029, loss 0.395126, acc 0.984375, prec 0.114308, recall 0.80126
2017-12-10T04:09:52.584324: step 9030, loss 0.00349498, acc 1, prec 0.114317, recall 0.801275
2017-12-10T04:09:52.846954: step 9031, loss 8.56796e-07, acc 1, prec 0.114317, recall 0.801275
2017-12-10T04:09:53.108013: step 9032, loss 0.187747, acc 0.984375, prec 0.114326, recall 0.80129
2017-12-10T04:09:53.375656: step 9033, loss 0.001389, acc 1, prec 0.114326, recall 0.80129
2017-12-10T04:09:53.642096: step 9034, loss 0.00168133, acc 1, prec 0.114326, recall 0.80129
2017-12-10T04:09:53.908603: step 9035, loss 0.00491688, acc 1, prec 0.114326, recall 0.80129
2017-12-10T04:09:54.178143: step 9036, loss 0.0108551, acc 1, prec 0.114326, recall 0.80129
2017-12-10T04:09:54.441362: step 9037, loss 0.301785, acc 0.96875, prec 0.114323, recall 0.80129
2017-12-10T04:09:54.715579: step 9038, loss 0.296603, acc 0.96875, prec 0.11433, recall 0.801306
2017-12-10T04:09:54.979190: step 9039, loss 0.629385, acc 0.984375, prec 0.114368, recall 0.801367
2017-12-10T04:09:55.244396: step 9040, loss 7.97152e-05, acc 1, prec 0.114397, recall 0.801412
2017-12-10T04:09:55.505752: step 9041, loss 0.0704743, acc 0.984375, prec 0.114406, recall 0.801428
2017-12-10T04:09:55.769049: step 9042, loss 0.00640665, acc 1, prec 0.114406, recall 0.801428
2017-12-10T04:09:56.037734: step 9043, loss 0.0115123, acc 0.984375, prec 0.114404, recall 0.801428
2017-12-10T04:09:56.295997: step 9044, loss 0.233839, acc 0.96875, prec 0.114402, recall 0.801428
2017-12-10T04:09:56.566598: step 9045, loss 0.311555, acc 0.96875, prec 0.114409, recall 0.801443
2017-12-10T04:09:56.835401: step 9046, loss 2.32585e-05, acc 1, prec 0.114428, recall 0.801473
2017-12-10T04:09:57.098089: step 9047, loss 0.63822, acc 0.953125, prec 0.114454, recall 0.801519
2017-12-10T04:09:57.366984: step 9048, loss 0.118733, acc 0.984375, prec 0.114453, recall 0.801519
2017-12-10T04:09:57.640567: step 9049, loss 0.254288, acc 0.96875, prec 0.11446, recall 0.801534
2017-12-10T04:09:57.909131: step 9050, loss 0.0285134, acc 0.984375, prec 0.114488, recall 0.80158
2017-12-10T04:09:58.173117: step 9051, loss 0.0330211, acc 0.984375, prec 0.114506, recall 0.80161
2017-12-10T04:09:58.435554: step 9052, loss 0.000517603, acc 1, prec 0.114506, recall 0.80161
2017-12-10T04:09:58.696297: step 9053, loss 0.000211004, acc 1, prec 0.114545, recall 0.801671
2017-12-10T04:09:58.956764: step 9054, loss 0.00962642, acc 1, prec 0.114545, recall 0.801671
2017-12-10T04:09:59.230033: step 9055, loss 0.000340751, acc 1, prec 0.114574, recall 0.801717
2017-12-10T04:09:59.492297: step 9056, loss 0.00174363, acc 1, prec 0.114603, recall 0.801762
2017-12-10T04:09:59.751246: step 9057, loss 0.799033, acc 0.953125, prec 0.114599, recall 0.801762
2017-12-10T04:10:00.027396: step 9058, loss 0.00919925, acc 1, prec 0.114628, recall 0.801808
2017-12-10T04:10:00.297065: step 9059, loss 0.206291, acc 0.96875, prec 0.114635, recall 0.801823
2017-12-10T04:10:00.577550: step 9060, loss 0.0836981, acc 0.984375, prec 0.114644, recall 0.801838
2017-12-10T04:10:00.844201: step 9061, loss 0.676014, acc 0.953125, prec 0.11465, recall 0.801854
2017-12-10T04:10:01.108301: step 9062, loss 4.69156e-05, acc 1, prec 0.11465, recall 0.801854
2017-12-10T04:10:01.374317: step 9063, loss 0.00131819, acc 1, prec 0.11465, recall 0.801854
2017-12-10T04:10:01.643306: step 9064, loss 1.18141e-05, acc 1, prec 0.114669, recall 0.801884
2017-12-10T04:10:01.899139: step 9065, loss 0.0481434, acc 0.984375, prec 0.114697, recall 0.801929
2017-12-10T04:10:02.168991: step 9066, loss 0.0387485, acc 0.984375, prec 0.114715, recall 0.80196
2017-12-10T04:10:02.435433: step 9067, loss 0.136556, acc 0.96875, prec 0.114722, recall 0.801975
2017-12-10T04:10:02.698741: step 9068, loss 2.85084, acc 0.984375, prec 0.114732, recall 0.801929
2017-12-10T04:10:02.963640: step 9069, loss 0.204858, acc 0.984375, prec 0.114731, recall 0.801929
2017-12-10T04:10:03.230710: step 9070, loss 0.00125059, acc 1, prec 0.11474, recall 0.801944
2017-12-10T04:10:03.500528: step 9071, loss 0.00391305, acc 1, prec 0.114769, recall 0.801989
2017-12-10T04:10:03.766451: step 9072, loss 0.751716, acc 0.953125, prec 0.114785, recall 0.80202
2017-12-10T04:10:04.039581: step 9073, loss 0.85091, acc 0.96875, prec 0.114782, recall 0.80202
2017-12-10T04:10:04.305280: step 9074, loss 0.267128, acc 0.953125, prec 0.114788, recall 0.802035
2017-12-10T04:10:04.569011: step 9075, loss 0.170707, acc 0.984375, prec 0.114797, recall 0.80205
2017-12-10T04:10:04.830528: step 9076, loss 0.116334, acc 0.96875, prec 0.114804, recall 0.802065
2017-12-10T04:10:05.095354: step 9077, loss 0.0620648, acc 0.96875, prec 0.114801, recall 0.802065
2017-12-10T04:10:05.359037: step 9078, loss 0.193824, acc 0.953125, prec 0.114798, recall 0.802065
2017-12-10T04:10:05.628799: step 9079, loss 0.363152, acc 0.921875, prec 0.114801, recall 0.80208
2017-12-10T04:10:05.892627: step 9080, loss 0.332433, acc 0.953125, prec 0.114807, recall 0.802095
2017-12-10T04:10:06.165583: step 9081, loss 0.0436589, acc 0.96875, prec 0.114814, recall 0.80211
2017-12-10T04:10:06.430572: step 9082, loss 0.137818, acc 0.96875, prec 0.114821, recall 0.802126
2017-12-10T04:10:06.699945: step 9083, loss 0.238751, acc 0.96875, prec 0.114838, recall 0.802156
2017-12-10T04:10:06.969713: step 9084, loss 0.0529357, acc 0.984375, prec 0.114847, recall 0.802171
2017-12-10T04:10:07.232958: step 9085, loss 0.0152825, acc 0.984375, prec 0.114845, recall 0.802171
2017-12-10T04:10:07.499516: step 9086, loss 0.0144888, acc 1, prec 0.114845, recall 0.802171
2017-12-10T04:10:07.764694: step 9087, loss 0.0445486, acc 0.984375, prec 0.114864, recall 0.802201
2017-12-10T04:10:08.030094: step 9088, loss 0.333088, acc 0.953125, prec 0.114869, recall 0.802216
2017-12-10T04:10:08.291466: step 9089, loss 0.0471668, acc 0.984375, prec 0.114878, recall 0.802231
2017-12-10T04:10:08.557551: step 9090, loss 0.272041, acc 0.953125, prec 0.114893, recall 0.802262
2017-12-10T04:10:08.820641: step 9091, loss 0.257955, acc 0.953125, prec 0.11489, recall 0.802262
2017-12-10T04:10:09.086766: step 9092, loss 0.0702759, acc 0.984375, prec 0.114908, recall 0.802292
2017-12-10T04:10:09.351196: step 9093, loss 1.48382, acc 0.9375, prec 0.114912, recall 0.802307
2017-12-10T04:10:09.624077: step 9094, loss 0.207198, acc 0.984375, prec 0.114921, recall 0.802322
2017-12-10T04:10:09.888146: step 9095, loss 0.000642813, acc 1, prec 0.114931, recall 0.802337
2017-12-10T04:10:10.154082: step 9096, loss 0.477768, acc 0.953125, prec 0.114936, recall 0.802352
2017-12-10T04:10:10.418071: step 9097, loss 0.230902, acc 0.953125, prec 0.114933, recall 0.802352
2017-12-10T04:10:10.679510: step 9098, loss 0.247333, acc 0.96875, prec 0.11493, recall 0.802352
2017-12-10T04:10:10.945238: step 9099, loss 0.293531, acc 0.96875, prec 0.114957, recall 0.802397
2017-12-10T04:10:11.207310: step 9100, loss 0.011435, acc 0.984375, prec 0.114985, recall 0.802443
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-9100

2017-12-10T04:10:12.551927: step 9101, loss 0.110277, acc 0.953125, prec 0.114981, recall 0.802443
2017-12-10T04:10:12.819987: step 9102, loss 0.161255, acc 0.984375, prec 0.114999, recall 0.802473
2017-12-10T04:10:13.085389: step 9103, loss 0.4813, acc 0.96875, prec 0.114996, recall 0.802473
2017-12-10T04:10:13.356607: step 9104, loss 0.295373, acc 0.984375, prec 0.115024, recall 0.802518
2017-12-10T04:10:13.624255: step 9105, loss 0.188414, acc 0.984375, prec 0.115042, recall 0.802548
2017-12-10T04:10:13.893550: step 9106, loss 0.928422, acc 0.9375, prec 0.115047, recall 0.802563
2017-12-10T04:10:14.156741: step 9107, loss 0.118157, acc 0.953125, prec 0.115053, recall 0.802578
2017-12-10T04:10:14.430707: step 9108, loss 0.0426993, acc 0.984375, prec 0.115052, recall 0.802578
2017-12-10T04:10:14.691932: step 9109, loss 1.26654, acc 0.96875, prec 0.115059, recall 0.802593
2017-12-10T04:10:14.957251: step 9110, loss 0.41517, acc 0.984375, prec 0.115077, recall 0.802624
2017-12-10T04:10:15.227646: step 9111, loss 0.061306, acc 0.984375, prec 0.115076, recall 0.802624
2017-12-10T04:10:15.497837: step 9112, loss 0.13384, acc 0.96875, prec 0.115083, recall 0.802639
2017-12-10T04:10:15.762339: step 9113, loss 0.199755, acc 0.96875, prec 0.11508, recall 0.802639
2017-12-10T04:10:16.033227: step 9114, loss 3.65632e-05, acc 1, prec 0.11509, recall 0.802654
2017-12-10T04:10:16.303029: step 9115, loss 0.00961401, acc 1, prec 0.115109, recall 0.802684
2017-12-10T04:10:16.565566: step 9116, loss 0.618492, acc 0.984375, prec 0.115108, recall 0.802684
2017-12-10T04:10:16.833949: step 9117, loss 0.183209, acc 0.96875, prec 0.115105, recall 0.802684
2017-12-10T04:10:17.097835: step 9118, loss 0.000254348, acc 1, prec 0.115105, recall 0.802684
2017-12-10T04:10:17.364342: step 9119, loss 0.0219545, acc 0.984375, prec 0.115114, recall 0.802699
2017-12-10T04:10:17.629191: step 9120, loss 0.00022075, acc 1, prec 0.115114, recall 0.802699
2017-12-10T04:10:17.888320: step 9121, loss 0.180029, acc 0.96875, prec 0.115121, recall 0.802714
2017-12-10T04:10:18.152603: step 9122, loss 0.00458242, acc 1, prec 0.115121, recall 0.802714
2017-12-10T04:10:18.418986: step 9123, loss 0.209701, acc 0.984375, prec 0.11512, recall 0.802714
2017-12-10T04:10:18.686633: step 9124, loss 0.0981144, acc 0.984375, prec 0.115138, recall 0.802744
2017-12-10T04:10:18.952188: step 9125, loss 0.0607582, acc 0.96875, prec 0.115135, recall 0.802744
2017-12-10T04:10:19.215823: step 9126, loss 0.861966, acc 0.921875, prec 0.115139, recall 0.802759
2017-12-10T04:10:19.479143: step 9127, loss 8.05471e-05, acc 1, prec 0.115139, recall 0.802759
2017-12-10T04:10:19.748687: step 9128, loss 0.203841, acc 0.984375, prec 0.115137, recall 0.802759
2017-12-10T04:10:20.020919: step 9129, loss 11.6482, acc 0.953125, prec 0.115154, recall 0.802728
2017-12-10T04:10:20.286957: step 9130, loss 0.100174, acc 0.984375, prec 0.115172, recall 0.802758
2017-12-10T04:10:20.551086: step 9131, loss 0.00237604, acc 1, prec 0.115182, recall 0.802773
2017-12-10T04:10:20.811844: step 9132, loss 0.0324722, acc 0.984375, prec 0.115181, recall 0.802773
2017-12-10T04:10:21.083871: step 9133, loss 0.176048, acc 0.96875, prec 0.115207, recall 0.802818
2017-12-10T04:10:21.353116: step 9134, loss 0.657146, acc 0.9375, prec 0.115212, recall 0.802833
2017-12-10T04:10:21.620317: step 9135, loss 0.0654667, acc 0.984375, prec 0.11522, recall 0.802848
2017-12-10T04:10:21.884218: step 9136, loss 0.337124, acc 0.96875, prec 0.115218, recall 0.802848
2017-12-10T04:10:22.152253: step 9137, loss 0.656512, acc 0.90625, prec 0.11521, recall 0.802848
2017-12-10T04:10:22.421202: step 9138, loss 0.00291928, acc 1, prec 0.11521, recall 0.802848
2017-12-10T04:10:22.680830: step 9139, loss 0.225496, acc 0.953125, prec 0.115216, recall 0.802863
2017-12-10T04:10:22.946632: step 9140, loss 1.27063, acc 0.859375, prec 0.115214, recall 0.802878
2017-12-10T04:10:23.221121: step 9141, loss 0.0269789, acc 0.984375, prec 0.115223, recall 0.802893
2017-12-10T04:10:23.489662: step 9142, loss 1.66492, acc 0.890625, prec 0.115214, recall 0.802893
2017-12-10T04:10:23.752522: step 9143, loss 0.328191, acc 0.921875, prec 0.115227, recall 0.802923
2017-12-10T04:10:24.017618: step 9144, loss 0.00221313, acc 1, prec 0.115237, recall 0.802938
2017-12-10T04:10:24.284658: step 9145, loss 2.35019, acc 0.890625, prec 0.115249, recall 0.802907
2017-12-10T04:10:24.555661: step 9146, loss 0.123878, acc 0.96875, prec 0.115246, recall 0.802907
2017-12-10T04:10:24.816703: step 9147, loss 0.620024, acc 0.953125, prec 0.115242, recall 0.802907
2017-12-10T04:10:25.085858: step 9148, loss 0.491458, acc 0.953125, prec 0.115248, recall 0.802922
2017-12-10T04:10:25.358021: step 9149, loss 0.943508, acc 0.953125, prec 0.115244, recall 0.802922
2017-12-10T04:10:25.628963: step 9150, loss 0.578226, acc 0.96875, prec 0.115242, recall 0.802922
2017-12-10T04:10:25.897020: step 9151, loss 0.193878, acc 0.96875, prec 0.115239, recall 0.802922
2017-12-10T04:10:26.161181: step 9152, loss 0.318866, acc 0.921875, prec 0.115243, recall 0.802937
2017-12-10T04:10:26.431575: step 9153, loss 0.459063, acc 0.90625, prec 0.115235, recall 0.802937
2017-12-10T04:10:26.695791: step 9154, loss 0.843825, acc 0.890625, prec 0.115236, recall 0.802952
2017-12-10T04:10:26.974201: step 9155, loss 0.0620515, acc 0.96875, prec 0.115243, recall 0.802967
2017-12-10T04:10:27.240023: step 9156, loss 1.11236, acc 0.921875, prec 0.115256, recall 0.802997
2017-12-10T04:10:27.507662: step 9157, loss 0.171052, acc 0.9375, prec 0.115261, recall 0.803012
2017-12-10T04:10:27.777029: step 9158, loss 0.44765, acc 0.921875, prec 0.115274, recall 0.803042
2017-12-10T04:10:28.043260: step 9159, loss 0.524646, acc 0.953125, prec 0.11528, recall 0.803057
2017-12-10T04:10:28.311130: step 9160, loss 0.0312555, acc 0.984375, prec 0.115278, recall 0.803057
2017-12-10T04:10:28.577940: step 9161, loss 0.34198, acc 0.9375, prec 0.115283, recall 0.803072
2017-12-10T04:10:28.842365: step 9162, loss 0.189703, acc 0.984375, prec 0.115291, recall 0.803087
2017-12-10T04:10:29.120177: step 9163, loss 0.460956, acc 0.953125, prec 0.115297, recall 0.803102
2017-12-10T04:10:29.389326: step 9164, loss 0.980305, acc 0.953125, prec 0.115313, recall 0.803132
2017-12-10T04:10:29.655713: step 9165, loss 0.119385, acc 0.96875, prec 0.11531, recall 0.803132
2017-12-10T04:10:29.918359: step 9166, loss 0.21869, acc 0.96875, prec 0.115327, recall 0.803162
2017-12-10T04:10:30.194397: step 9167, loss 0.666704, acc 0.921875, prec 0.11533, recall 0.803177
2017-12-10T04:10:30.469576: step 9168, loss 0.0869485, acc 0.984375, prec 0.115329, recall 0.803177
2017-12-10T04:10:30.733310: step 9169, loss 0.128727, acc 0.984375, prec 0.115338, recall 0.803191
2017-12-10T04:10:30.998938: step 9170, loss 0.0831441, acc 0.984375, prec 0.115336, recall 0.803191
2017-12-10T04:10:31.263137: step 9171, loss 0.0897452, acc 0.984375, prec 0.115354, recall 0.803221
2017-12-10T04:10:31.525996: step 9172, loss 0.21869, acc 0.953125, prec 0.11536, recall 0.803236
2017-12-10T04:10:31.800701: step 9173, loss 0.147369, acc 0.96875, prec 0.115367, recall 0.803251
2017-12-10T04:10:32.065572: step 9174, loss 0.137358, acc 1, prec 0.115396, recall 0.803296
2017-12-10T04:10:32.329541: step 9175, loss 2.12953e-05, acc 1, prec 0.115416, recall 0.803326
2017-12-10T04:10:32.586552: step 9176, loss 0.0242548, acc 0.984375, prec 0.115414, recall 0.803326
2017-12-10T04:10:32.859077: step 9177, loss 0.38904, acc 0.984375, prec 0.115432, recall 0.803356
2017-12-10T04:10:33.121865: step 9178, loss 0.568709, acc 0.953125, prec 0.115429, recall 0.803356
2017-12-10T04:10:33.384601: step 9179, loss 1.87415, acc 0.9375, prec 0.115435, recall 0.80331
2017-12-10T04:10:33.648717: step 9180, loss 0.238576, acc 0.953125, prec 0.11544, recall 0.803325
2017-12-10T04:10:33.913056: step 9181, loss 0.331127, acc 0.96875, prec 0.115438, recall 0.803325
2017-12-10T04:10:34.178070: step 9182, loss 0.0789779, acc 0.984375, prec 0.115446, recall 0.80334
2017-12-10T04:10:34.442610: step 9183, loss 0.0149141, acc 1, prec 0.115475, recall 0.803384
2017-12-10T04:10:34.706898: step 9184, loss 0.260024, acc 0.984375, prec 0.115484, recall 0.803399
2017-12-10T04:10:34.971104: step 9185, loss 0.594712, acc 0.90625, prec 0.115486, recall 0.803414
2017-12-10T04:10:35.239388: step 9186, loss 0.996031, acc 0.9375, prec 0.115481, recall 0.803414
2017-12-10T04:10:35.505362: step 9187, loss 0.452476, acc 0.90625, prec 0.115473, recall 0.803414
2017-12-10T04:10:35.773049: step 9188, loss 0.017966, acc 0.984375, prec 0.115472, recall 0.803414
2017-12-10T04:10:36.039181: step 9189, loss 0.28747, acc 0.984375, prec 0.11548, recall 0.803429
2017-12-10T04:10:36.307851: step 9190, loss 0.332832, acc 0.953125, prec 0.115476, recall 0.803429
2017-12-10T04:10:36.572563: step 9191, loss 1.26198, acc 0.921875, prec 0.11547, recall 0.803429
2017-12-10T04:10:36.838685: step 9192, loss 0.103952, acc 0.96875, prec 0.115487, recall 0.803459
2017-12-10T04:10:37.104277: step 9193, loss 0.35043, acc 0.9375, prec 0.115482, recall 0.803459
2017-12-10T04:10:37.363904: step 9194, loss 0.768487, acc 0.953125, prec 0.115478, recall 0.803459
2017-12-10T04:10:37.632712: step 9195, loss 0.328376, acc 0.953125, prec 0.115484, recall 0.803474
2017-12-10T04:10:37.895243: step 9196, loss 0.658412, acc 0.953125, prec 0.11548, recall 0.803474
2017-12-10T04:10:38.163575: step 9197, loss 0.0222955, acc 0.984375, prec 0.115479, recall 0.803474
2017-12-10T04:10:38.432606: step 9198, loss 0.135807, acc 0.984375, prec 0.115497, recall 0.803504
2017-12-10T04:10:38.696272: step 9199, loss 0.200587, acc 0.984375, prec 0.115505, recall 0.803519
2017-12-10T04:10:38.965941: step 9200, loss 0.273822, acc 0.9375, prec 0.1155, recall 0.803519
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-9200

2017-12-10T04:10:40.292223: step 9201, loss 0.23567, acc 0.953125, prec 0.115506, recall 0.803533
2017-12-10T04:10:40.567403: step 9202, loss 0.195065, acc 0.953125, prec 0.115512, recall 0.803548
2017-12-10T04:10:40.834085: step 9203, loss 0.0820457, acc 0.984375, prec 0.115511, recall 0.803548
2017-12-10T04:10:41.097869: step 9204, loss 0.0757786, acc 0.984375, prec 0.11551, recall 0.803548
2017-12-10T04:10:41.361043: step 9205, loss 0.292235, acc 0.953125, prec 0.115515, recall 0.803563
2017-12-10T04:10:41.624088: step 9206, loss 0.788291, acc 0.96875, prec 0.115523, recall 0.803578
2017-12-10T04:10:41.893386: step 9207, loss 0.392014, acc 0.953125, prec 0.115528, recall 0.803593
2017-12-10T04:10:42.164780: step 9208, loss 0.332214, acc 0.9375, prec 0.115543, recall 0.803623
2017-12-10T04:10:42.431862: step 9209, loss 0.00362886, acc 1, prec 0.115543, recall 0.803623
2017-12-10T04:10:42.692021: step 9210, loss 0.0801177, acc 0.984375, prec 0.115541, recall 0.803623
2017-12-10T04:10:42.951924: step 9211, loss 0.71604, acc 0.953125, prec 0.115557, recall 0.803653
2017-12-10T04:10:43.213847: step 9212, loss 0.0283692, acc 0.984375, prec 0.115565, recall 0.803667
2017-12-10T04:10:43.476977: step 9213, loss 0.000473805, acc 1, prec 0.115575, recall 0.803682
2017-12-10T04:10:43.739861: step 9214, loss 0.116094, acc 0.984375, prec 0.115574, recall 0.803682
2017-12-10T04:10:44.003619: step 9215, loss 0.11858, acc 0.984375, prec 0.115572, recall 0.803682
2017-12-10T04:10:44.267652: step 9216, loss 0.248976, acc 0.96875, prec 0.115579, recall 0.803697
2017-12-10T04:10:44.529946: step 9217, loss 0.034126, acc 0.984375, prec 0.115578, recall 0.803697
2017-12-10T04:10:44.793965: step 9218, loss 0.0801315, acc 0.96875, prec 0.115585, recall 0.803712
2017-12-10T04:10:45.059178: step 9219, loss 0.469016, acc 0.953125, prec 0.115601, recall 0.803742
2017-12-10T04:10:45.322809: step 9220, loss 0.0788654, acc 1, prec 0.11561, recall 0.803757
2017-12-10T04:10:45.588808: step 9221, loss 0.00112253, acc 1, prec 0.11562, recall 0.803772
2017-12-10T04:10:45.859764: step 9222, loss 1.29767, acc 0.953125, prec 0.115618, recall 0.803711
2017-12-10T04:10:46.123759: step 9223, loss 3.42494, acc 0.96875, prec 0.115616, recall 0.80365
2017-12-10T04:10:46.389769: step 9224, loss 0.00104458, acc 1, prec 0.115626, recall 0.803665
2017-12-10T04:10:46.651621: step 9225, loss 7.04313, acc 0.96875, prec 0.115625, recall 0.803604
2017-12-10T04:10:46.917732: step 9226, loss 0.143018, acc 0.984375, prec 0.115643, recall 0.803634
2017-12-10T04:10:47.183491: step 9227, loss 0.158846, acc 0.953125, prec 0.115658, recall 0.803663
2017-12-10T04:10:47.445290: step 9228, loss 0.970161, acc 0.875, prec 0.115648, recall 0.803663
2017-12-10T04:10:47.709249: step 9229, loss 1.03668, acc 0.90625, prec 0.11565, recall 0.803678
2017-12-10T04:10:47.976023: step 9230, loss 0.739854, acc 0.921875, prec 0.115644, recall 0.803678
2017-12-10T04:10:48.242903: step 9231, loss 1.11273, acc 0.875, prec 0.115634, recall 0.803678
2017-12-10T04:10:48.512457: step 9232, loss 0.381472, acc 0.921875, prec 0.115637, recall 0.803693
2017-12-10T04:10:48.790708: step 9233, loss 2.50834, acc 0.796875, prec 0.11563, recall 0.803708
2017-12-10T04:10:49.056507: step 9234, loss 0.350361, acc 0.890625, prec 0.11566, recall 0.803767
2017-12-10T04:10:49.321847: step 9235, loss 1.32441, acc 0.859375, prec 0.115649, recall 0.803767
2017-12-10T04:10:49.589360: step 9236, loss 2.63481, acc 0.78125, prec 0.115631, recall 0.803767
2017-12-10T04:10:49.859522: step 9237, loss 0.0935822, acc 0.953125, prec 0.115627, recall 0.803767
2017-12-10T04:10:50.121970: step 9238, loss 2.34545, acc 0.78125, prec 0.11561, recall 0.803767
2017-12-10T04:10:50.385008: step 9239, loss 2.66261, acc 0.703125, prec 0.115595, recall 0.803782
2017-12-10T04:10:50.647330: step 9240, loss 0.516295, acc 0.90625, prec 0.115588, recall 0.803782
2017-12-10T04:10:50.910392: step 9241, loss 1.32126, acc 0.84375, prec 0.115575, recall 0.803782
2017-12-10T04:10:51.175767: step 9242, loss 1.07273, acc 0.859375, prec 0.115593, recall 0.803827
2017-12-10T04:10:51.441889: step 9243, loss 0.160853, acc 0.96875, prec 0.1156, recall 0.803841
2017-12-10T04:10:51.714786: step 9244, loss 0.687968, acc 0.859375, prec 0.115598, recall 0.803856
2017-12-10T04:10:51.985276: step 9245, loss 0.56642, acc 0.9375, prec 0.115603, recall 0.803871
2017-12-10T04:10:52.250327: step 9246, loss 1.14017, acc 0.9375, prec 0.115608, recall 0.803886
2017-12-10T04:10:52.512289: step 9247, loss 0.347939, acc 0.9375, prec 0.115631, recall 0.80393
2017-12-10T04:10:52.781938: step 9248, loss 0.468974, acc 0.9375, prec 0.115646, recall 0.80396
2017-12-10T04:10:53.049567: step 9249, loss 0.24139, acc 0.953125, prec 0.115642, recall 0.80396
2017-12-10T04:10:53.314129: step 9250, loss 0.560439, acc 0.90625, prec 0.115634, recall 0.80396
2017-12-10T04:10:53.586304: step 9251, loss 0.385846, acc 0.953125, prec 0.11564, recall 0.803975
2017-12-10T04:10:53.854487: step 9252, loss 0.0286075, acc 0.984375, prec 0.115639, recall 0.803975
2017-12-10T04:10:54.121187: step 9253, loss 0.167014, acc 0.984375, prec 0.115647, recall 0.80399
2017-12-10T04:10:54.385596: step 9254, loss 1.25283, acc 0.9375, prec 0.115661, recall 0.804019
2017-12-10T04:10:54.655356: step 9255, loss 0.223016, acc 0.96875, prec 0.115659, recall 0.804019
2017-12-10T04:10:54.921569: step 9256, loss 0.00553831, acc 1, prec 0.115668, recall 0.804034
2017-12-10T04:10:55.183506: step 9257, loss 0.05755, acc 0.984375, prec 0.115696, recall 0.804079
2017-12-10T04:10:55.451779: step 9258, loss 0.520403, acc 0.9375, prec 0.115691, recall 0.804079
2017-12-10T04:10:55.714853: step 9259, loss 0.0233612, acc 0.984375, prec 0.11569, recall 0.804079
2017-12-10T04:10:55.980927: step 9260, loss 0.000876179, acc 1, prec 0.115699, recall 0.804093
2017-12-10T04:10:56.239787: step 9261, loss 3.32404e-05, acc 1, prec 0.115719, recall 0.804123
2017-12-10T04:10:56.500837: step 9262, loss 0.246863, acc 0.984375, prec 0.115746, recall 0.804167
2017-12-10T04:10:56.774775: step 9263, loss 5.85996, acc 0.984375, prec 0.115756, recall 0.804121
2017-12-10T04:10:57.050411: step 9264, loss 0.00270424, acc 1, prec 0.115765, recall 0.804136
2017-12-10T04:10:57.319944: step 9265, loss 0.0774718, acc 1, prec 0.115775, recall 0.804151
2017-12-10T04:10:57.588527: step 9266, loss 0.0166348, acc 0.984375, prec 0.115774, recall 0.804151
2017-12-10T04:10:57.854289: step 9267, loss 0.0550256, acc 0.984375, prec 0.115792, recall 0.804181
2017-12-10T04:10:58.116927: step 9268, loss 0.769728, acc 0.953125, prec 0.115788, recall 0.804181
2017-12-10T04:10:58.384279: step 9269, loss 0.00228675, acc 1, prec 0.115788, recall 0.804181
2017-12-10T04:10:58.651832: step 9270, loss 0.0016004, acc 1, prec 0.115797, recall 0.804195
2017-12-10T04:10:58.915461: step 9271, loss 0.0462741, acc 0.984375, prec 0.115806, recall 0.80421
2017-12-10T04:10:59.189701: step 9272, loss 0.238399, acc 0.953125, prec 0.115812, recall 0.804225
2017-12-10T04:10:59.463076: step 9273, loss 0.431432, acc 0.953125, prec 0.115827, recall 0.804254
2017-12-10T04:10:59.733987: step 9274, loss 1.09881, acc 0.953125, prec 0.115823, recall 0.804254
2017-12-10T04:11:00.002540: step 9275, loss 0.0783925, acc 0.984375, prec 0.115822, recall 0.804254
2017-12-10T04:11:00.274204: step 9276, loss 0.0747173, acc 0.953125, prec 0.115828, recall 0.804269
2017-12-10T04:11:00.543765: step 9277, loss 0.470428, acc 0.9375, prec 0.115832, recall 0.804284
2017-12-10T04:11:00.814279: step 9278, loss 0.092423, acc 0.984375, prec 0.115831, recall 0.804284
2017-12-10T04:11:01.083727: step 9279, loss 0.000669256, acc 1, prec 0.115841, recall 0.804299
2017-12-10T04:11:01.355488: step 9280, loss 0.377354, acc 0.9375, prec 0.115845, recall 0.804313
2017-12-10T04:11:01.617863: step 9281, loss 0.217943, acc 0.984375, prec 0.115844, recall 0.804313
2017-12-10T04:11:01.880830: step 9282, loss 0.0285848, acc 0.984375, prec 0.115862, recall 0.804343
2017-12-10T04:11:02.146119: step 9283, loss 0.52449, acc 0.953125, prec 0.115868, recall 0.804358
2017-12-10T04:11:02.412235: step 9284, loss 0.352003, acc 0.9375, prec 0.115863, recall 0.804358
2017-12-10T04:11:02.671844: step 9285, loss 1.62756, acc 0.890625, prec 0.115854, recall 0.804358
2017-12-10T04:11:02.934453: step 9286, loss 0.535427, acc 0.984375, prec 0.115862, recall 0.804372
2017-12-10T04:11:03.197989: step 9287, loss 0.739868, acc 0.953125, prec 0.115868, recall 0.804387
2017-12-10T04:11:03.460966: step 9288, loss 1.45614, acc 0.84375, prec 0.115875, recall 0.804417
2017-12-10T04:11:03.720539: step 9289, loss 0.0660426, acc 0.96875, prec 0.115882, recall 0.804431
2017-12-10T04:11:03.985012: step 9290, loss 0.554716, acc 0.9375, prec 0.115886, recall 0.804446
2017-12-10T04:11:04.255137: step 9291, loss 0.000542216, acc 1, prec 0.115896, recall 0.804461
2017-12-10T04:11:04.514165: step 9292, loss 0.00998622, acc 1, prec 0.115906, recall 0.804476
2017-12-10T04:11:04.783659: step 9293, loss 0.00321786, acc 1, prec 0.115906, recall 0.804476
2017-12-10T04:11:05.052377: step 9294, loss 0.0318467, acc 0.96875, prec 0.115913, recall 0.80449
2017-12-10T04:11:05.313009: step 9295, loss 0.126464, acc 0.96875, prec 0.11591, recall 0.80449
2017-12-10T04:11:05.579435: step 9296, loss 0.185326, acc 0.96875, prec 0.115908, recall 0.80449
2017-12-10T04:11:05.846380: step 9297, loss 0.000665998, acc 1, prec 0.115908, recall 0.80449
2017-12-10T04:11:06.106995: step 9298, loss 0.148141, acc 0.984375, prec 0.115916, recall 0.804505
2017-12-10T04:11:06.369427: step 9299, loss 0.381212, acc 0.953125, prec 0.115922, recall 0.80452
2017-12-10T04:11:06.627049: step 9300, loss 0.0839229, acc 0.984375, prec 0.115921, recall 0.80452

Evaluation:
2017-12-10T04:11:14.159726: step 9300, loss 16.1468, acc 0.960468, prec 0.116083, recall 0.79994

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-9300

2017-12-10T04:11:15.581046: step 9301, loss 0.000160409, acc 1, prec 0.116083, recall 0.79994
2017-12-10T04:11:15.841795: step 9302, loss 0.736963, acc 0.9375, prec 0.116088, recall 0.799955
2017-12-10T04:11:16.101365: step 9303, loss 0.00186826, acc 1, prec 0.116088, recall 0.799955
2017-12-10T04:11:16.364877: step 9304, loss 0.173186, acc 0.96875, prec 0.116085, recall 0.799955
2017-12-10T04:11:16.629988: step 9305, loss 9.06855e-06, acc 1, prec 0.116085, recall 0.799955
2017-12-10T04:11:16.889212: step 9306, loss 0.000486538, acc 1, prec 0.116095, recall 0.79997
2017-12-10T04:11:17.155548: step 9307, loss 0.0371963, acc 0.984375, prec 0.116093, recall 0.79997
2017-12-10T04:11:17.422249: step 9308, loss 0.563279, acc 0.96875, prec 0.1161, recall 0.799985
2017-12-10T04:11:17.688611: step 9309, loss 0.101905, acc 0.984375, prec 0.116109, recall 0.8
2017-12-10T04:11:17.954346: step 9310, loss 0.173156, acc 0.984375, prec 0.116136, recall 0.800045
2017-12-10T04:11:18.220040: step 9311, loss 2.90725e-05, acc 1, prec 0.116146, recall 0.80006
2017-12-10T04:11:18.477449: step 9312, loss 0.0742053, acc 0.984375, prec 0.116163, recall 0.800089
2017-12-10T04:11:18.743289: step 9313, loss 3.62268e-06, acc 1, prec 0.116183, recall 0.800119
2017-12-10T04:11:18.996903: step 9314, loss 0.0614021, acc 0.984375, prec 0.1162, recall 0.800149
2017-12-10T04:11:19.255912: step 9315, loss 0.378794, acc 0.96875, prec 0.116198, recall 0.800149
2017-12-10T04:11:19.523509: step 9316, loss 0.0661655, acc 0.984375, prec 0.116206, recall 0.800164
2017-12-10T04:11:19.789457: step 9317, loss 0.530617, acc 0.953125, prec 0.116202, recall 0.800164
2017-12-10T04:11:20.059426: step 9318, loss 0.446226, acc 0.984375, prec 0.11622, recall 0.800193
2017-12-10T04:11:20.322329: step 9319, loss 0.333958, acc 0.96875, prec 0.116218, recall 0.800193
2017-12-10T04:11:20.587739: step 9320, loss 0.00487961, acc 1, prec 0.116227, recall 0.800208
2017-12-10T04:11:20.852782: step 9321, loss 0.130035, acc 0.984375, prec 0.116236, recall 0.800223
2017-12-10T04:11:21.128075: step 9322, loss 0.150112, acc 0.984375, prec 0.116234, recall 0.800223
2017-12-10T04:11:21.386597: step 9323, loss 5.68663e-06, acc 1, prec 0.116253, recall 0.800253
2017-12-10T04:11:21.644206: step 9324, loss 4.72731, acc 0.96875, prec 0.116262, recall 0.800208
2017-12-10T04:11:21.908702: step 9325, loss 0.000197155, acc 1, prec 0.116262, recall 0.800208
2017-12-10T04:11:22.172645: step 9326, loss 0.165179, acc 0.96875, prec 0.116259, recall 0.800208
2017-12-10T04:11:22.444249: step 9327, loss 0.0238539, acc 0.984375, prec 0.116268, recall 0.800223
2017-12-10T04:11:22.709452: step 9328, loss 0.171114, acc 0.984375, prec 0.116285, recall 0.800253
2017-12-10T04:11:22.990021: step 9329, loss 0.40443, acc 0.96875, prec 0.116283, recall 0.800253
2017-12-10T04:11:23.252237: step 9330, loss 0.128778, acc 0.984375, prec 0.116291, recall 0.800268
2017-12-10T04:11:23.516283: step 9331, loss 0.403234, acc 0.96875, prec 0.116308, recall 0.800297
2017-12-10T04:11:23.787084: step 9332, loss 0.127189, acc 0.984375, prec 0.116306, recall 0.800297
2017-12-10T04:11:24.055867: step 9333, loss 1.39552, acc 0.953125, prec 0.116312, recall 0.800312
2017-12-10T04:11:24.321503: step 9334, loss 0.866696, acc 0.921875, prec 0.116306, recall 0.800312
2017-12-10T04:11:24.586512: step 9335, loss 0.13464, acc 0.953125, prec 0.116302, recall 0.800312
2017-12-10T04:11:24.849678: step 9336, loss 0.997406, acc 0.921875, prec 0.116305, recall 0.800327
2017-12-10T04:11:25.122280: step 9337, loss 0.368086, acc 0.921875, prec 0.116309, recall 0.800342
2017-12-10T04:11:25.387214: step 9338, loss 0.239039, acc 0.96875, prec 0.116316, recall 0.800357
2017-12-10T04:11:25.659097: step 9339, loss 0.34328, acc 0.984375, prec 0.116324, recall 0.800371
2017-12-10T04:11:25.928036: step 9340, loss 1.5198, acc 0.875, prec 0.116333, recall 0.800401
2017-12-10T04:11:26.194201: step 9341, loss 0.334897, acc 0.9375, prec 0.116338, recall 0.800416
2017-12-10T04:11:26.459397: step 9342, loss 0.619285, acc 0.921875, prec 0.11635, recall 0.800446
2017-12-10T04:11:26.729414: step 9343, loss 0.412856, acc 0.953125, prec 0.116356, recall 0.80046
2017-12-10T04:11:27.004915: step 9344, loss 1.1382, acc 0.96875, prec 0.116363, recall 0.800475
2017-12-10T04:11:27.277671: step 9345, loss 0.00571604, acc 1, prec 0.116373, recall 0.80049
2017-12-10T04:11:27.542285: step 9346, loss 0.483968, acc 0.921875, prec 0.116376, recall 0.800505
2017-12-10T04:11:27.809863: step 9347, loss 0.0904071, acc 0.96875, prec 0.116383, recall 0.80052
2017-12-10T04:11:28.076573: step 9348, loss 0.0822239, acc 0.984375, prec 0.116391, recall 0.800534
2017-12-10T04:11:28.345487: step 9349, loss 0.000235712, acc 1, prec 0.116391, recall 0.800534
2017-12-10T04:11:28.606206: step 9350, loss 0.00107363, acc 1, prec 0.11642, recall 0.800579
2017-12-10T04:11:28.869979: step 9351, loss 1.03802, acc 0.953125, prec 0.116416, recall 0.800579
2017-12-10T04:11:29.137604: step 9352, loss 1.05551, acc 0.921875, prec 0.116419, recall 0.800594
2017-12-10T04:11:29.404121: step 9353, loss 0.25219, acc 0.96875, prec 0.116426, recall 0.800609
2017-12-10T04:11:29.670572: step 9354, loss 0.239685, acc 0.96875, prec 0.116433, recall 0.800623
2017-12-10T04:11:29.947877: step 9355, loss 0.0285313, acc 1, prec 0.116443, recall 0.800638
2017-12-10T04:11:30.214071: step 9356, loss 0.0608753, acc 0.984375, prec 0.116442, recall 0.800638
2017-12-10T04:11:30.487605: step 9357, loss 0.0941769, acc 0.984375, prec 0.11645, recall 0.800653
2017-12-10T04:11:30.753265: step 9358, loss 0.000544198, acc 1, prec 0.11645, recall 0.800653
2017-12-10T04:11:31.017391: step 9359, loss 0.562947, acc 0.984375, prec 0.116477, recall 0.800697
2017-12-10T04:11:31.292424: step 9360, loss 0.098498, acc 0.984375, prec 0.116486, recall 0.800712
2017-12-10T04:11:31.555861: step 9361, loss 0.00199277, acc 1, prec 0.116495, recall 0.800727
2017-12-10T04:11:31.820933: step 9362, loss 0.0305113, acc 0.984375, prec 0.116503, recall 0.800742
2017-12-10T04:11:32.085688: step 9363, loss 0.581009, acc 0.984375, prec 0.116502, recall 0.800742
2017-12-10T04:11:32.349381: step 9364, loss 0.000497776, acc 1, prec 0.116512, recall 0.800756
2017-12-10T04:11:32.616170: step 9365, loss 0.471142, acc 0.9375, prec 0.116526, recall 0.800786
2017-12-10T04:11:32.876102: step 9366, loss 0.945164, acc 0.96875, prec 0.116533, recall 0.800801
2017-12-10T04:11:33.150923: step 9367, loss 0.462355, acc 0.96875, prec 0.11653, recall 0.800801
2017-12-10T04:11:33.417773: step 9368, loss 0.00107209, acc 1, prec 0.116549, recall 0.80083
2017-12-10T04:11:33.681228: step 9369, loss 0.704694, acc 0.953125, prec 0.116565, recall 0.80086
2017-12-10T04:11:33.943689: step 9370, loss 3.39215, acc 0.90625, prec 0.116568, recall 0.800815
2017-12-10T04:11:34.215093: step 9371, loss 0.987739, acc 0.96875, prec 0.116575, recall 0.80083
2017-12-10T04:11:34.477870: step 9372, loss 0.679048, acc 0.96875, prec 0.116582, recall 0.800845
2017-12-10T04:11:34.741305: step 9373, loss 1.65939, acc 0.9375, prec 0.116586, recall 0.800859
2017-12-10T04:11:35.005536: step 9374, loss 0.769173, acc 0.890625, prec 0.116587, recall 0.800874
2017-12-10T04:11:35.275362: step 9375, loss 0.689487, acc 0.9375, prec 0.116592, recall 0.800889
2017-12-10T04:11:35.536726: step 9376, loss 1.13475, acc 0.921875, prec 0.116585, recall 0.800889
2017-12-10T04:11:35.807385: step 9377, loss 0.542635, acc 0.90625, prec 0.116578, recall 0.800889
2017-12-10T04:11:36.070916: step 9378, loss 0.636538, acc 0.9375, prec 0.116582, recall 0.800904
2017-12-10T04:11:36.332853: step 9379, loss 0.619289, acc 0.9375, prec 0.116577, recall 0.800904
2017-12-10T04:11:36.599614: step 9380, loss 0.0371639, acc 0.984375, prec 0.116576, recall 0.800904
2017-12-10T04:11:36.869921: step 9381, loss 0.106105, acc 0.96875, prec 0.116573, recall 0.800904
2017-12-10T04:11:37.139706: step 9382, loss 1.29102, acc 0.90625, prec 0.116575, recall 0.800918
2017-12-10T04:11:37.400414: step 9383, loss 0.818994, acc 0.921875, prec 0.116579, recall 0.800933
2017-12-10T04:11:37.662669: step 9384, loss 0.157912, acc 0.953125, prec 0.116575, recall 0.800933
2017-12-10T04:11:37.933398: step 9385, loss 0.886429, acc 0.9375, prec 0.116579, recall 0.800948
2017-12-10T04:11:38.199343: step 9386, loss 0.443831, acc 0.953125, prec 0.116585, recall 0.800963
2017-12-10T04:11:38.464555: step 9387, loss 0.802443, acc 0.921875, prec 0.116588, recall 0.800977
2017-12-10T04:11:38.731515: step 9388, loss 0.161496, acc 0.953125, prec 0.116604, recall 0.801007
2017-12-10T04:11:38.994204: step 9389, loss 0.269373, acc 0.921875, prec 0.116616, recall 0.801036
2017-12-10T04:11:39.257979: step 9390, loss 0.0385458, acc 0.984375, prec 0.116615, recall 0.801036
2017-12-10T04:11:39.528245: step 9391, loss 0.360641, acc 0.9375, prec 0.11662, recall 0.801051
2017-12-10T04:11:39.794284: step 9392, loss 1.28048, acc 0.96875, prec 0.116617, recall 0.801051
2017-12-10T04:11:40.056635: step 9393, loss 1.0414, acc 0.953125, prec 0.116623, recall 0.801066
2017-12-10T04:11:40.316868: step 9394, loss 0.292283, acc 0.953125, prec 0.116619, recall 0.801066
2017-12-10T04:11:40.582580: step 9395, loss 0.563054, acc 0.984375, prec 0.116618, recall 0.801066
2017-12-10T04:11:40.848271: step 9396, loss 0.0858866, acc 0.984375, prec 0.116617, recall 0.801066
2017-12-10T04:11:41.110695: step 9397, loss 0.00478517, acc 1, prec 0.116626, recall 0.80108
2017-12-10T04:11:41.373333: step 9398, loss 0.437893, acc 0.9375, prec 0.11664, recall 0.80111
2017-12-10T04:11:41.638646: step 9399, loss 0.808693, acc 0.90625, prec 0.116642, recall 0.801125
2017-12-10T04:11:41.912558: step 9400, loss 0.686464, acc 0.96875, prec 0.116649, recall 0.801139
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-9400

2017-12-10T04:11:43.124932: step 9401, loss 0.331944, acc 0.96875, prec 0.116666, recall 0.801169
2017-12-10T04:11:43.388894: step 9402, loss 0.458329, acc 0.984375, prec 0.116674, recall 0.801183
2017-12-10T04:11:43.657911: step 9403, loss 0.556518, acc 0.953125, prec 0.11667, recall 0.801183
2017-12-10T04:11:43.923490: step 9404, loss 0.594864, acc 0.9375, prec 0.116665, recall 0.801183
2017-12-10T04:11:44.187891: step 9405, loss 0.319601, acc 0.96875, prec 0.116672, recall 0.801198
2017-12-10T04:11:44.453761: step 9406, loss 0.115545, acc 0.984375, prec 0.11668, recall 0.801213
2017-12-10T04:11:44.725537: step 9407, loss 0.0424694, acc 0.984375, prec 0.116679, recall 0.801213
2017-12-10T04:11:44.996138: step 9408, loss 0.124135, acc 0.984375, prec 0.116678, recall 0.801213
2017-12-10T04:11:45.260298: step 9409, loss 0.0480631, acc 0.984375, prec 0.116677, recall 0.801213
2017-12-10T04:11:45.524804: step 9410, loss 0.0212085, acc 0.984375, prec 0.116694, recall 0.801242
2017-12-10T04:11:45.793724: step 9411, loss 0.110126, acc 0.984375, prec 0.116731, recall 0.801301
2017-12-10T04:11:46.061249: step 9412, loss 0.143948, acc 0.984375, prec 0.116739, recall 0.801316
2017-12-10T04:11:46.328114: step 9413, loss 0.844226, acc 0.96875, prec 0.116746, recall 0.80133
2017-12-10T04:11:46.592516: step 9414, loss 0.0098589, acc 1, prec 0.116746, recall 0.80133
2017-12-10T04:11:46.851480: step 9415, loss 1.15855e-06, acc 1, prec 0.116746, recall 0.80133
2017-12-10T04:11:47.115825: step 9416, loss 0.127579, acc 0.984375, prec 0.116755, recall 0.801345
2017-12-10T04:11:47.384938: step 9417, loss 0.000966946, acc 1, prec 0.116755, recall 0.801345
2017-12-10T04:11:47.649853: step 9418, loss 0.0162362, acc 1, prec 0.116764, recall 0.80136
2017-12-10T04:11:47.918447: step 9419, loss 0.14954, acc 1, prec 0.116783, recall 0.801389
2017-12-10T04:11:48.185797: step 9420, loss 0.114641, acc 0.984375, prec 0.116801, recall 0.801418
2017-12-10T04:11:48.446407: step 9421, loss 0.803788, acc 0.984375, prec 0.116809, recall 0.801433
2017-12-10T04:11:48.711589: step 9422, loss 0.331545, acc 0.984375, prec 0.116808, recall 0.801433
2017-12-10T04:11:48.975417: step 9423, loss 0.000228952, acc 1, prec 0.116817, recall 0.801448
2017-12-10T04:11:49.243555: step 9424, loss 0.356689, acc 0.96875, prec 0.116824, recall 0.801462
2017-12-10T04:11:49.506351: step 9425, loss 0.175095, acc 0.984375, prec 0.116833, recall 0.801477
2017-12-10T04:11:49.779750: step 9426, loss 0.808271, acc 0.953125, prec 0.116829, recall 0.801477
2017-12-10T04:11:50.051718: step 9427, loss 0.0443019, acc 0.984375, prec 0.116847, recall 0.801506
2017-12-10T04:11:50.312066: step 9428, loss 0.00652942, acc 1, prec 0.116856, recall 0.801521
2017-12-10T04:11:50.577197: step 9429, loss 1.09189e-05, acc 1, prec 0.116856, recall 0.801521
2017-12-10T04:11:50.834626: step 9430, loss 0.343826, acc 0.953125, prec 0.116871, recall 0.80155
2017-12-10T04:11:51.097475: step 9431, loss 0.114032, acc 0.953125, prec 0.116868, recall 0.80155
2017-12-10T04:11:51.356625: step 9432, loss 0, acc 1, prec 0.116868, recall 0.80155
2017-12-10T04:11:51.612948: step 9433, loss 0.387972, acc 0.96875, prec 0.116884, recall 0.80158
2017-12-10T04:11:51.879593: step 9434, loss 0.334414, acc 0.96875, prec 0.116882, recall 0.80158
2017-12-10T04:11:52.143271: step 9435, loss 0.0508518, acc 0.984375, prec 0.11688, recall 0.80158
2017-12-10T04:11:52.407812: step 9436, loss 0.187787, acc 0.984375, prec 0.116879, recall 0.80158
2017-12-10T04:11:52.678809: step 9437, loss 0.0250874, acc 0.984375, prec 0.116897, recall 0.801609
2017-12-10T04:11:52.943824: step 9438, loss 24.4546, acc 0.984375, prec 0.116897, recall 0.80155
2017-12-10T04:11:53.218327: step 9439, loss 0.241047, acc 0.984375, prec 0.116896, recall 0.80155
2017-12-10T04:11:53.485372: step 9440, loss 0.0123989, acc 1, prec 0.116896, recall 0.80155
2017-12-10T04:11:53.758473: step 9441, loss 0.0401833, acc 0.984375, prec 0.116904, recall 0.801564
2017-12-10T04:11:54.026632: step 9442, loss 1.79926e-06, acc 1, prec 0.116923, recall 0.801594
2017-12-10T04:11:54.252282: step 9443, loss 0.210549, acc 0.980392, prec 0.116931, recall 0.801608
2017-12-10T04:11:54.528905: step 9444, loss 0.0458695, acc 0.984375, prec 0.116958, recall 0.801652
2017-12-10T04:11:54.798839: step 9445, loss 0.22035, acc 0.96875, prec 0.116965, recall 0.801667
2017-12-10T04:11:55.061672: step 9446, loss 0.228198, acc 0.96875, prec 0.116963, recall 0.801667
2017-12-10T04:11:55.326760: step 9447, loss 0.562034, acc 0.953125, prec 0.116959, recall 0.801667
2017-12-10T04:11:55.595757: step 9448, loss 0.00024487, acc 1, prec 0.116978, recall 0.801696
2017-12-10T04:11:55.857151: step 9449, loss 0.131351, acc 0.96875, prec 0.116975, recall 0.801696
2017-12-10T04:11:56.132683: step 9450, loss 0.598875, acc 0.9375, prec 0.11697, recall 0.801696
2017-12-10T04:11:56.399717: step 9451, loss 0.0796976, acc 0.984375, prec 0.116979, recall 0.801711
2017-12-10T04:11:56.663195: step 9452, loss 0.0550457, acc 0.96875, prec 0.116976, recall 0.801711
2017-12-10T04:11:56.944540: step 9453, loss 0.210818, acc 0.96875, prec 0.116974, recall 0.801711
2017-12-10T04:11:57.207535: step 9454, loss 0.013205, acc 1, prec 0.116983, recall 0.801725
2017-12-10T04:11:57.470354: step 9455, loss 0.602129, acc 0.9375, prec 0.116978, recall 0.801725
2017-12-10T04:11:57.732788: step 9456, loss 0.174859, acc 0.953125, prec 0.116974, recall 0.801725
2017-12-10T04:11:58.002922: step 9457, loss 0.100244, acc 0.953125, prec 0.11698, recall 0.80174
2017-12-10T04:11:58.267676: step 9458, loss 0.468525, acc 0.9375, prec 0.116975, recall 0.80174
2017-12-10T04:11:58.532679: step 9459, loss 0.555529, acc 0.921875, prec 0.116978, recall 0.801755
2017-12-10T04:11:58.797170: step 9460, loss 0.770456, acc 0.953125, prec 0.116974, recall 0.801755
2017-12-10T04:11:59.058762: step 9461, loss 0.0760512, acc 0.984375, prec 0.116983, recall 0.801769
2017-12-10T04:11:59.324014: step 9462, loss 0.000710983, acc 1, prec 0.116992, recall 0.801784
2017-12-10T04:11:59.594351: step 9463, loss 0.161629, acc 0.984375, prec 0.117, recall 0.801798
2017-12-10T04:11:59.861055: step 9464, loss 0.0127816, acc 0.984375, prec 0.116999, recall 0.801798
2017-12-10T04:12:00.122131: step 9465, loss 0.105821, acc 0.953125, prec 0.116995, recall 0.801798
2017-12-10T04:12:00.390996: step 9466, loss 0.650347, acc 0.96875, prec 0.116993, recall 0.801798
2017-12-10T04:12:00.662252: step 9467, loss 0.0062336, acc 1, prec 0.116993, recall 0.801798
2017-12-10T04:12:00.930782: step 9468, loss 0.0552157, acc 0.96875, prec 0.11699, recall 0.801798
2017-12-10T04:12:01.200330: step 9469, loss 0.000164168, acc 1, prec 0.117, recall 0.801813
2017-12-10T04:12:01.464596: step 9470, loss 0.0794712, acc 0.984375, prec 0.117008, recall 0.801828
2017-12-10T04:12:01.729953: step 9471, loss 0.163789, acc 0.984375, prec 0.117016, recall 0.801842
2017-12-10T04:12:01.994352: step 9472, loss 0.149023, acc 0.984375, prec 0.117034, recall 0.801871
2017-12-10T04:12:02.257531: step 9473, loss 0.0690711, acc 0.984375, prec 0.117033, recall 0.801871
2017-12-10T04:12:02.520243: step 9474, loss 0.458375, acc 0.96875, prec 0.11703, recall 0.801871
2017-12-10T04:12:02.791616: step 9475, loss 0.283774, acc 0.96875, prec 0.117028, recall 0.801871
2017-12-10T04:12:03.057924: step 9476, loss 4.45168e-07, acc 1, prec 0.117028, recall 0.801871
2017-12-10T04:12:03.311246: step 9477, loss 0.234849, acc 0.96875, prec 0.117044, recall 0.801901
2017-12-10T04:12:03.579559: step 9478, loss 0.903543, acc 0.96875, prec 0.117042, recall 0.801901
2017-12-10T04:12:03.849139: step 9479, loss 0.859088, acc 0.953125, prec 0.117038, recall 0.801901
2017-12-10T04:12:04.117136: step 9480, loss 0.00218308, acc 1, prec 0.117066, recall 0.801944
2017-12-10T04:12:04.380104: step 9481, loss 0.00111529, acc 1, prec 0.117066, recall 0.801944
2017-12-10T04:12:04.642434: step 9482, loss 0.484628, acc 0.984375, prec 0.117075, recall 0.801959
2017-12-10T04:12:04.907706: step 9483, loss 0.000668888, acc 1, prec 0.117084, recall 0.801974
2017-12-10T04:12:05.170928: step 9484, loss 0.0658572, acc 1, prec 0.117094, recall 0.801988
2017-12-10T04:12:05.435491: step 9485, loss 1.50594, acc 0.96875, prec 0.117121, recall 0.801973
2017-12-10T04:12:05.712601: step 9486, loss 0.11472, acc 0.984375, prec 0.117139, recall 0.802002
2017-12-10T04:12:05.972490: step 9487, loss 0.107881, acc 0.984375, prec 0.117156, recall 0.802031
2017-12-10T04:12:06.243601: step 9488, loss 4.60772e-05, acc 1, prec 0.117156, recall 0.802031
2017-12-10T04:12:06.497477: step 9489, loss 0.00544267, acc 1, prec 0.117156, recall 0.802031
2017-12-10T04:12:06.775884: step 9490, loss 0.000401265, acc 1, prec 0.117166, recall 0.802046
2017-12-10T04:12:07.036506: step 9491, loss 0.00592698, acc 1, prec 0.117166, recall 0.802046
2017-12-10T04:12:07.297988: step 9492, loss 0.218899, acc 0.953125, prec 0.117162, recall 0.802046
2017-12-10T04:12:07.573483: step 9493, loss 0.256699, acc 0.984375, prec 0.117189, recall 0.802089
2017-12-10T04:12:07.834929: step 9494, loss 0.221787, acc 0.96875, prec 0.117187, recall 0.802089
2017-12-10T04:12:08.098517: step 9495, loss 0.19304, acc 0.96875, prec 0.117194, recall 0.802104
2017-12-10T04:12:08.370616: step 9496, loss 0.720664, acc 0.953125, prec 0.11719, recall 0.802104
2017-12-10T04:12:08.631790: step 9497, loss 0.100911, acc 0.984375, prec 0.117246, recall 0.802191
2017-12-10T04:12:08.900559: step 9498, loss 8.96764e-05, acc 1, prec 0.117246, recall 0.802191
2017-12-10T04:12:09.163322: step 9499, loss 0.442951, acc 0.96875, prec 0.117243, recall 0.802191
2017-12-10T04:12:09.426157: step 9500, loss 0.514197, acc 0.96875, prec 0.117241, recall 0.802191
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-9500

2017-12-10T04:12:10.738156: step 9501, loss 0.0152342, acc 0.984375, prec 0.117249, recall 0.802206
2017-12-10T04:12:11.004943: step 9502, loss 0.557066, acc 0.984375, prec 0.117248, recall 0.802206
2017-12-10T04:12:11.267592: step 9503, loss 0.0687583, acc 0.984375, prec 0.117256, recall 0.80222
2017-12-10T04:12:11.531775: step 9504, loss 0.143885, acc 0.96875, prec 0.117263, recall 0.802235
2017-12-10T04:12:11.799723: step 9505, loss 0.0365565, acc 0.984375, prec 0.117261, recall 0.802235
2017-12-10T04:12:12.068664: step 9506, loss 0.105645, acc 0.984375, prec 0.11726, recall 0.802235
2017-12-10T04:12:12.334371: step 9507, loss 0.145751, acc 0.984375, prec 0.117278, recall 0.802264
2017-12-10T04:12:12.600453: step 9508, loss 0.489102, acc 0.953125, prec 0.117293, recall 0.802293
2017-12-10T04:12:12.865514: step 9509, loss 0.605715, acc 0.953125, prec 0.117299, recall 0.802308
2017-12-10T04:12:13.132532: step 9510, loss 0.846957, acc 0.96875, prec 0.117296, recall 0.802308
2017-12-10T04:12:13.397566: step 9511, loss 0.0461522, acc 0.984375, prec 0.117295, recall 0.802308
2017-12-10T04:12:13.675584: step 9512, loss 0.406297, acc 0.9375, prec 0.11729, recall 0.802308
2017-12-10T04:12:13.942495: step 9513, loss 0.359696, acc 0.96875, prec 0.117287, recall 0.802308
2017-12-10T04:12:14.208214: step 9514, loss 3.82796, acc 0.953125, prec 0.117285, recall 0.802249
2017-12-10T04:12:14.474018: step 9515, loss 0.00107044, acc 1, prec 0.117285, recall 0.802249
2017-12-10T04:12:14.739873: step 9516, loss 8.33861e-05, acc 1, prec 0.117294, recall 0.802263
2017-12-10T04:12:15.004209: step 9517, loss 0.509463, acc 0.953125, prec 0.1173, recall 0.802278
2017-12-10T04:12:15.275440: step 9518, loss 0.0745875, acc 0.953125, prec 0.117306, recall 0.802292
2017-12-10T04:12:15.541509: step 9519, loss 0.357755, acc 0.9375, prec 0.117339, recall 0.80235
2017-12-10T04:12:15.808692: step 9520, loss 0.457384, acc 0.9375, prec 0.117334, recall 0.80235
2017-12-10T04:12:16.076369: step 9521, loss 0.0576456, acc 0.953125, prec 0.117339, recall 0.802365
2017-12-10T04:12:16.339982: step 9522, loss 0.926965, acc 0.96875, prec 0.117346, recall 0.802379
2017-12-10T04:12:16.606415: step 9523, loss 0.000279084, acc 1, prec 0.117346, recall 0.802379
2017-12-10T04:12:16.873560: step 9524, loss 0.412431, acc 0.953125, prec 0.117352, recall 0.802394
2017-12-10T04:12:17.147734: step 9525, loss 0.515072, acc 0.921875, prec 0.117355, recall 0.802408
2017-12-10T04:12:17.413347: step 9526, loss 0.558369, acc 0.90625, prec 0.117348, recall 0.802408
2017-12-10T04:12:17.677349: step 9527, loss 1.25883, acc 0.921875, prec 0.11736, recall 0.802437
2017-12-10T04:12:17.951727: step 9528, loss 0.115532, acc 0.96875, prec 0.117377, recall 0.802466
2017-12-10T04:12:18.226182: step 9529, loss 0.965929, acc 0.921875, prec 0.117389, recall 0.802495
2017-12-10T04:12:18.487139: step 9530, loss 0.0146342, acc 0.984375, prec 0.117398, recall 0.80251
2017-12-10T04:12:18.758038: step 9531, loss 1.72352, acc 0.921875, prec 0.117391, recall 0.80251
2017-12-10T04:12:19.022860: step 9532, loss 0.259859, acc 0.96875, prec 0.117389, recall 0.80251
2017-12-10T04:12:19.286716: step 9533, loss 0.180806, acc 0.984375, prec 0.117416, recall 0.802553
2017-12-10T04:12:19.550539: step 9534, loss 0.0781874, acc 0.96875, prec 0.117413, recall 0.802553
2017-12-10T04:12:19.821697: step 9535, loss 0.478035, acc 0.96875, prec 0.11742, recall 0.802568
2017-12-10T04:12:20.087657: step 9536, loss 0.81548, acc 0.953125, prec 0.117417, recall 0.802568
2017-12-10T04:12:20.352935: step 9537, loss 0.0793794, acc 0.96875, prec 0.117414, recall 0.802568
2017-12-10T04:12:20.621267: step 9538, loss 0.149884, acc 0.984375, prec 0.117432, recall 0.802597
2017-12-10T04:12:20.886215: step 9539, loss 1.00246, acc 0.953125, prec 0.117437, recall 0.802611
2017-12-10T04:12:21.148432: step 9540, loss 0.696882, acc 0.9375, prec 0.117451, recall 0.80264
2017-12-10T04:12:21.412144: step 9541, loss 0.79051, acc 0.953125, prec 0.117457, recall 0.802655
2017-12-10T04:12:21.685334: step 9542, loss 0.330549, acc 0.96875, prec 0.117455, recall 0.802655
2017-12-10T04:12:21.957015: step 9543, loss 0.871388, acc 0.953125, prec 0.11746, recall 0.802669
2017-12-10T04:12:22.221763: step 9544, loss 1.04243, acc 0.9375, prec 0.117455, recall 0.802669
2017-12-10T04:12:22.482434: step 9545, loss 0.00684464, acc 1, prec 0.117474, recall 0.802698
2017-12-10T04:12:22.743367: step 9546, loss 0.671683, acc 0.9375, prec 0.117469, recall 0.802698
2017-12-10T04:12:23.007629: step 9547, loss 0.0776232, acc 0.984375, prec 0.117468, recall 0.802698
2017-12-10T04:12:23.273769: step 9548, loss 0.053957, acc 0.984375, prec 0.117467, recall 0.802698
2017-12-10T04:12:23.536831: step 9549, loss 0.123616, acc 0.96875, prec 0.117464, recall 0.802698
2017-12-10T04:12:23.804097: step 9550, loss 0.319867, acc 0.984375, prec 0.117463, recall 0.802698
2017-12-10T04:12:24.069458: step 9551, loss 0.0353719, acc 0.984375, prec 0.117471, recall 0.802713
2017-12-10T04:12:24.334836: step 9552, loss 0.145925, acc 0.96875, prec 0.117487, recall 0.802742
2017-12-10T04:12:24.599912: step 9553, loss 1.73405e-06, acc 1, prec 0.117487, recall 0.802742
2017-12-10T04:12:24.858450: step 9554, loss 0.000256232, acc 1, prec 0.117487, recall 0.802742
2017-12-10T04:12:25.121296: step 9555, loss 0.00980779, acc 1, prec 0.117487, recall 0.802742
2017-12-10T04:12:25.386828: step 9556, loss 0.172766, acc 0.984375, prec 0.117515, recall 0.802785
2017-12-10T04:12:25.649110: step 9557, loss 7.56901, acc 0.96875, prec 0.117523, recall 0.802741
2017-12-10T04:12:25.912263: step 9558, loss 1.03771, acc 0.96875, prec 0.117539, recall 0.802769
2017-12-10T04:12:26.181466: step 9559, loss 0.097863, acc 0.984375, prec 0.117547, recall 0.802784
2017-12-10T04:12:26.444653: step 9560, loss 0.194462, acc 0.953125, prec 0.117544, recall 0.802784
2017-12-10T04:12:26.708685: step 9561, loss 0.156102, acc 0.984375, prec 0.117542, recall 0.802784
2017-12-10T04:12:26.982314: step 9562, loss 0.434732, acc 0.953125, prec 0.117548, recall 0.802798
2017-12-10T04:12:27.244813: step 9563, loss 0.21842, acc 0.96875, prec 0.117574, recall 0.802842
2017-12-10T04:12:27.508395: step 9564, loss 0.196737, acc 0.953125, prec 0.11757, recall 0.802842
2017-12-10T04:12:27.777653: step 9565, loss 0.17304, acc 0.953125, prec 0.117576, recall 0.802856
2017-12-10T04:12:28.051078: step 9566, loss 0.143746, acc 0.953125, prec 0.117572, recall 0.802856
2017-12-10T04:12:28.311765: step 9567, loss 1.10748, acc 0.90625, prec 0.117574, recall 0.802871
2017-12-10T04:12:28.575147: step 9568, loss 0.0356909, acc 0.984375, prec 0.117573, recall 0.802871
2017-12-10T04:12:28.846307: step 9569, loss 0.115497, acc 0.96875, prec 0.11757, recall 0.802871
2017-12-10T04:12:29.116144: step 9570, loss 1.0621, acc 0.9375, prec 0.117575, recall 0.802885
2017-12-10T04:12:29.386218: step 9571, loss 0.00523887, acc 1, prec 0.117584, recall 0.802899
2017-12-10T04:12:29.659522: step 9572, loss 0.640274, acc 0.96875, prec 0.117591, recall 0.802914
2017-12-10T04:12:29.927920: step 9573, loss 0.874853, acc 0.9375, prec 0.117605, recall 0.802943
2017-12-10T04:12:30.200723: step 9574, loss 0.521836, acc 0.96875, prec 0.117612, recall 0.802957
2017-12-10T04:12:30.476024: step 9575, loss 0.455457, acc 0.921875, prec 0.117605, recall 0.802957
2017-12-10T04:12:30.740013: step 9576, loss 0.495287, acc 0.96875, prec 0.117622, recall 0.802986
2017-12-10T04:12:31.010090: step 9577, loss 0.529057, acc 0.9375, prec 0.117626, recall 0.803
2017-12-10T04:12:31.279369: step 9578, loss 0.0966493, acc 0.984375, prec 0.117653, recall 0.803044
2017-12-10T04:12:31.546310: step 9579, loss 0.589945, acc 0.96875, prec 0.11766, recall 0.803058
2017-12-10T04:12:31.812910: step 9580, loss 0.881219, acc 0.9375, prec 0.117665, recall 0.803072
2017-12-10T04:12:32.081976: step 9581, loss 0.538414, acc 0.953125, prec 0.11768, recall 0.803101
2017-12-10T04:12:32.352072: step 9582, loss 5.4216, acc 0.9375, prec 0.117676, recall 0.803042
2017-12-10T04:12:32.620529: step 9583, loss 0.627887, acc 0.96875, prec 0.117683, recall 0.803057
2017-12-10T04:12:32.884322: step 9584, loss 0.0328751, acc 0.984375, prec 0.117691, recall 0.803071
2017-12-10T04:12:33.146513: step 9585, loss 0.2212, acc 0.984375, prec 0.117709, recall 0.8031
2017-12-10T04:12:33.413137: step 9586, loss 0.080607, acc 0.953125, prec 0.117715, recall 0.803114
2017-12-10T04:12:33.677916: step 9587, loss 0.501458, acc 0.953125, prec 0.11772, recall 0.803129
2017-12-10T04:12:33.944505: step 9588, loss 0.00217446, acc 1, prec 0.11773, recall 0.803143
2017-12-10T04:12:34.214282: step 9589, loss 0.342572, acc 0.953125, prec 0.117745, recall 0.803172
2017-12-10T04:12:34.480948: step 9590, loss 0.49148, acc 0.921875, prec 0.117738, recall 0.803172
2017-12-10T04:12:34.745557: step 9591, loss 1.2537, acc 0.875, prec 0.117738, recall 0.803186
2017-12-10T04:12:35.013729: step 9592, loss 0.615491, acc 0.953125, prec 0.117753, recall 0.803215
2017-12-10T04:12:35.283466: step 9593, loss 0.293254, acc 0.90625, prec 0.117755, recall 0.80323
2017-12-10T04:12:35.550336: step 9594, loss 0.460379, acc 0.921875, prec 0.117767, recall 0.803258
2017-12-10T04:12:35.814978: step 9595, loss 0.360092, acc 0.953125, prec 0.117773, recall 0.803273
2017-12-10T04:12:36.091025: step 9596, loss 0.656594, acc 0.90625, prec 0.117765, recall 0.803273
2017-12-10T04:12:36.369979: step 9597, loss 0.00860023, acc 1, prec 0.117765, recall 0.803273
2017-12-10T04:12:36.640967: step 9598, loss 1.25864, acc 0.890625, prec 0.117776, recall 0.803301
2017-12-10T04:12:36.906495: step 9599, loss 0.434593, acc 0.9375, prec 0.11778, recall 0.803316
2017-12-10T04:12:37.178819: step 9600, loss 0.453543, acc 0.953125, prec 0.117786, recall 0.80333

Evaluation:
2017-12-10T04:12:44.799242: step 9600, loss 9.80302, acc 0.928861, prec 0.117746, recall 0.800925

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-9600

2017-12-10T04:12:46.055798: step 9601, loss 1.62024, acc 0.9375, prec 0.117778, recall 0.800982
2017-12-10T04:12:46.323971: step 9602, loss 0.359543, acc 0.953125, prec 0.117793, recall 0.801011
2017-12-10T04:12:46.585679: step 9603, loss 0.267372, acc 0.953125, prec 0.117789, recall 0.801011
2017-12-10T04:12:46.850929: step 9604, loss 0.645673, acc 0.90625, prec 0.117791, recall 0.801025
2017-12-10T04:12:47.121678: step 9605, loss 0.123726, acc 0.96875, prec 0.117789, recall 0.801025
2017-12-10T04:12:47.388436: step 9606, loss 1.02597, acc 0.9375, prec 0.117803, recall 0.801054
2017-12-10T04:12:47.648938: step 9607, loss 1.57885, acc 0.953125, prec 0.117799, recall 0.801054
2017-12-10T04:12:47.911538: step 9608, loss 0.798793, acc 0.96875, prec 0.117806, recall 0.801068
2017-12-10T04:12:48.177568: step 9609, loss 0.0678599, acc 0.96875, prec 0.117803, recall 0.801068
2017-12-10T04:12:48.446097: step 9610, loss 0.115618, acc 0.96875, prec 0.11781, recall 0.801083
2017-12-10T04:12:48.713618: step 9611, loss 0.0351106, acc 0.96875, prec 0.117808, recall 0.801083
2017-12-10T04:12:48.985601: step 9612, loss 1.35001, acc 0.984375, prec 0.117808, recall 0.801025
2017-12-10T04:12:49.255987: step 9613, loss 0.379435, acc 0.96875, prec 0.117814, recall 0.801039
2017-12-10T04:12:49.520066: step 9614, loss 0.89255, acc 0.953125, prec 0.117811, recall 0.801039
2017-12-10T04:12:49.782465: step 9615, loss 0.230216, acc 0.96875, prec 0.117808, recall 0.801039
2017-12-10T04:12:50.045340: step 9616, loss 0.675155, acc 0.9375, prec 0.117822, recall 0.801068
2017-12-10T04:12:50.307511: step 9617, loss 0.176676, acc 0.984375, prec 0.117821, recall 0.801068
2017-12-10T04:12:50.577482: step 9618, loss 26.8085, acc 0.953125, prec 0.117827, recall 0.801024
2017-12-10T04:12:50.849347: step 9619, loss 3.95965, acc 0.96875, prec 0.117826, recall 0.800967
2017-12-10T04:12:51.119773: step 9620, loss 1.9815, acc 0.875, prec 0.117826, recall 0.800981
2017-12-10T04:12:51.383652: step 9621, loss 0.500179, acc 0.9375, prec 0.11783, recall 0.800995
2017-12-10T04:12:51.655578: step 9622, loss 0.799881, acc 0.96875, prec 0.117827, recall 0.800995
2017-12-10T04:12:51.921649: step 9623, loss 2.51298, acc 0.875, prec 0.117827, recall 0.80101
2017-12-10T04:12:52.191742: step 9624, loss 0.908194, acc 0.859375, prec 0.117816, recall 0.80101
2017-12-10T04:12:52.457065: step 9625, loss 2.67941, acc 0.828125, prec 0.117811, recall 0.801024
2017-12-10T04:12:52.726399: step 9626, loss 1.21578, acc 0.859375, prec 0.117809, recall 0.801038
2017-12-10T04:12:52.994513: step 9627, loss 2.43851, acc 0.828125, prec 0.117796, recall 0.801038
2017-12-10T04:12:53.260568: step 9628, loss 2.61897, acc 0.78125, prec 0.117787, recall 0.801053
2017-12-10T04:12:53.521101: step 9629, loss 3.89436, acc 0.75, prec 0.117777, recall 0.801067
2017-12-10T04:12:53.792246: step 9630, loss 1.27161, acc 0.90625, prec 0.117788, recall 0.801096
2017-12-10T04:12:54.057861: step 9631, loss 1.07618, acc 0.84375, prec 0.117775, recall 0.801096
2017-12-10T04:12:54.318823: step 9632, loss 0.926926, acc 0.875, prec 0.117766, recall 0.801096
2017-12-10T04:12:54.587185: step 9633, loss 0.888898, acc 0.921875, prec 0.117778, recall 0.801125
2017-12-10T04:12:54.852777: step 9634, loss 0.922324, acc 0.875, prec 0.117787, recall 0.801153
2017-12-10T04:12:55.119454: step 9635, loss 1.13825, acc 0.9375, prec 0.117782, recall 0.801153
2017-12-10T04:12:55.385086: step 9636, loss 0.548528, acc 0.90625, prec 0.117784, recall 0.801167
2017-12-10T04:12:55.652268: step 9637, loss 0.710734, acc 0.921875, prec 0.117805, recall 0.80121
2017-12-10T04:12:55.918703: step 9638, loss 0.770799, acc 0.875, prec 0.117795, recall 0.80121
2017-12-10T04:12:56.189823: step 9639, loss 0.565249, acc 0.890625, prec 0.117787, recall 0.80121
2017-12-10T04:12:56.452030: step 9640, loss 0.113225, acc 0.96875, prec 0.117812, recall 0.801253
2017-12-10T04:12:56.722626: step 9641, loss 0.365138, acc 0.953125, prec 0.117818, recall 0.801268
2017-12-10T04:12:57.010566: step 9642, loss 0.174011, acc 0.96875, prec 0.117825, recall 0.801282
2017-12-10T04:12:57.279479: step 9643, loss 0.186055, acc 0.96875, prec 0.117831, recall 0.801296
2017-12-10T04:12:57.547382: step 9644, loss 0.323058, acc 0.96875, prec 0.117866, recall 0.801354
2017-12-10T04:12:57.811622: step 9645, loss 0.351082, acc 0.96875, prec 0.117873, recall 0.801368
2017-12-10T04:12:58.076489: step 9646, loss 0.329474, acc 0.953125, prec 0.117879, recall 0.801382
2017-12-10T04:12:58.343913: step 9647, loss 0.231177, acc 0.96875, prec 0.117895, recall 0.801411
2017-12-10T04:12:58.609169: step 9648, loss 1.03907, acc 0.921875, prec 0.117898, recall 0.801425
2017-12-10T04:12:58.872712: step 9649, loss 0.00476449, acc 1, prec 0.117935, recall 0.801482
2017-12-10T04:12:59.136582: step 9650, loss 0.294821, acc 0.921875, prec 0.117929, recall 0.801482
2017-12-10T04:12:59.403262: step 9651, loss 0.33555, acc 0.96875, prec 0.117945, recall 0.801511
2017-12-10T04:12:59.671895: step 9652, loss 0.000270301, acc 1, prec 0.117973, recall 0.801554
2017-12-10T04:12:59.930793: step 9653, loss 1.14192, acc 0.96875, prec 0.117971, recall 0.801554
2017-12-10T04:13:00.201631: step 9654, loss 0.103377, acc 0.96875, prec 0.117978, recall 0.801568
2017-12-10T04:13:00.473742: step 9655, loss 0.47865, acc 0.96875, prec 0.117985, recall 0.801582
2017-12-10T04:13:00.742478: step 9656, loss 9.52828, acc 0.984375, prec 0.117985, recall 0.801525
2017-12-10T04:13:01.002334: step 9657, loss 1.03564, acc 0.96875, prec 0.117982, recall 0.801525
2017-12-10T04:13:01.269485: step 9658, loss 0.424611, acc 0.96875, prec 0.118008, recall 0.801567
2017-12-10T04:13:01.535976: step 9659, loss 0.0595598, acc 0.984375, prec 0.118034, recall 0.80161
2017-12-10T04:13:01.808745: step 9660, loss 0.0976374, acc 0.984375, prec 0.118033, recall 0.80161
2017-12-10T04:13:02.074222: step 9661, loss 0.600125, acc 0.96875, prec 0.118031, recall 0.80161
2017-12-10T04:13:02.345494: step 9662, loss 0.0741061, acc 0.984375, prec 0.118029, recall 0.80161
2017-12-10T04:13:02.611209: step 9663, loss 0.223047, acc 0.984375, prec 0.118037, recall 0.801624
2017-12-10T04:13:02.885562: step 9664, loss 0.0410323, acc 0.984375, prec 0.118036, recall 0.801624
2017-12-10T04:13:03.155483: step 9665, loss 1.04828, acc 0.953125, prec 0.118042, recall 0.801639
2017-12-10T04:13:03.421727: step 9666, loss 0.700265, acc 0.953125, prec 0.118047, recall 0.801653
2017-12-10T04:13:03.684895: step 9667, loss 0.240858, acc 0.953125, prec 0.118044, recall 0.801653
2017-12-10T04:13:03.949653: step 9668, loss 4.58415, acc 0.90625, prec 0.118037, recall 0.801595
2017-12-10T04:13:04.215550: step 9669, loss 0.781672, acc 0.921875, prec 0.11804, recall 0.80161
2017-12-10T04:13:04.484088: step 9670, loss 0.430457, acc 0.921875, prec 0.118034, recall 0.80161
2017-12-10T04:13:04.749143: step 9671, loss 0.397119, acc 0.9375, prec 0.118039, recall 0.801624
2017-12-10T04:13:05.014867: step 9672, loss 0.277055, acc 0.953125, prec 0.118044, recall 0.801638
2017-12-10T04:13:05.291752: step 9673, loss 0.944116, acc 0.859375, prec 0.118033, recall 0.801638
2017-12-10T04:13:05.557853: step 9674, loss 0.182857, acc 0.96875, prec 0.118049, recall 0.801667
2017-12-10T04:13:05.822863: step 9675, loss 1.222, acc 0.890625, prec 0.11805, recall 0.801681
2017-12-10T04:13:06.092708: step 9676, loss 0.0196685, acc 1, prec 0.118059, recall 0.801695
2017-12-10T04:13:06.359635: step 9677, loss 0.802511, acc 0.9375, prec 0.118063, recall 0.801709
2017-12-10T04:13:06.622894: step 9678, loss 1.35132, acc 0.84375, prec 0.118051, recall 0.801709
2017-12-10T04:13:06.887292: step 9679, loss 0.450137, acc 0.921875, prec 0.118045, recall 0.801709
2017-12-10T04:13:07.153312: step 9680, loss 1.93908, acc 0.84375, prec 0.118041, recall 0.801724
2017-12-10T04:13:07.418415: step 9681, loss 0.248897, acc 0.984375, prec 0.118049, recall 0.801738
2017-12-10T04:13:07.677763: step 9682, loss 1.27134, acc 0.90625, prec 0.11807, recall 0.80178
2017-12-10T04:13:07.943061: step 9683, loss 0.295964, acc 0.96875, prec 0.118077, recall 0.801795
2017-12-10T04:13:08.207953: step 9684, loss 0.363234, acc 0.953125, prec 0.118101, recall 0.801837
2017-12-10T04:13:08.472782: step 9685, loss 0.588561, acc 0.953125, prec 0.118107, recall 0.801852
2017-12-10T04:13:08.738144: step 9686, loss 0.324549, acc 0.953125, prec 0.118112, recall 0.801866
2017-12-10T04:13:09.007123: step 9687, loss 0.265297, acc 0.921875, prec 0.118115, recall 0.80188
2017-12-10T04:13:09.273207: step 9688, loss 2.07959, acc 0.90625, prec 0.118118, recall 0.801837
2017-12-10T04:13:09.538125: step 9689, loss 0.854401, acc 0.875, prec 0.118127, recall 0.801865
2017-12-10T04:13:09.805123: step 9690, loss 0.205229, acc 0.984375, prec 0.118144, recall 0.801894
2017-12-10T04:13:10.066702: step 9691, loss 0.419343, acc 0.9375, prec 0.118139, recall 0.801894
2017-12-10T04:13:10.336390: step 9692, loss 1.23174, acc 0.890625, prec 0.11814, recall 0.801908
2017-12-10T04:13:10.607117: step 9693, loss 0.57319, acc 0.90625, prec 0.118132, recall 0.801908
2017-12-10T04:13:10.870217: step 9694, loss 0.746678, acc 0.921875, prec 0.118136, recall 0.801922
2017-12-10T04:13:11.133932: step 9695, loss 1.28135, acc 0.90625, prec 0.118128, recall 0.801922
2017-12-10T04:13:11.397599: step 9696, loss 0.725734, acc 0.875, prec 0.118118, recall 0.801922
2017-12-10T04:13:11.663173: step 9697, loss 0.101052, acc 0.984375, prec 0.118135, recall 0.80195
2017-12-10T04:13:11.938944: step 9698, loss 1.31112, acc 0.921875, prec 0.118148, recall 0.801979
2017-12-10T04:13:12.200998: step 9699, loss 0.482099, acc 0.9375, prec 0.118152, recall 0.801993
2017-12-10T04:13:12.466962: step 9700, loss 0.775308, acc 0.953125, prec 0.118148, recall 0.801993
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-9700

2017-12-10T04:13:13.897515: step 9701, loss 0.999314, acc 0.921875, prec 0.118142, recall 0.801993
2017-12-10T04:13:14.166050: step 9702, loss 0.764504, acc 0.921875, prec 0.118164, recall 0.802036
2017-12-10T04:13:15.156353: step 9703, loss 0.527745, acc 0.96875, prec 0.118161, recall 0.802036
2017-12-10T04:13:15.506748: step 9704, loss 0.738674, acc 0.90625, prec 0.118163, recall 0.80205
2017-12-10T04:13:15.785814: step 9705, loss 0.164432, acc 0.953125, prec 0.118159, recall 0.80205
2017-12-10T04:13:16.510042: step 9706, loss 0.773574, acc 0.953125, prec 0.118156, recall 0.80205
2017-12-10T04:13:17.241192: step 9707, loss 0.325055, acc 0.96875, prec 0.118163, recall 0.802064
2017-12-10T04:13:17.993895: step 9708, loss 0.0169062, acc 0.984375, prec 0.118171, recall 0.802078
2017-12-10T04:13:18.711501: step 9709, loss 0.497932, acc 0.953125, prec 0.118176, recall 0.802092
2017-12-10T04:13:19.448950: step 9710, loss 0.502696, acc 0.9375, prec 0.11818, recall 0.802106
2017-12-10T04:13:20.193877: step 9711, loss 0.288023, acc 0.9375, prec 0.118175, recall 0.802106
2017-12-10T04:13:20.916257: step 9712, loss 8.74339, acc 0.9375, prec 0.118181, recall 0.802063
2017-12-10T04:13:21.651293: step 9713, loss 0.203473, acc 0.96875, prec 0.118206, recall 0.802106
2017-12-10T04:13:22.384314: step 9714, loss 0.31752, acc 0.984375, prec 0.118215, recall 0.80212
2017-12-10T04:13:23.096235: step 9715, loss 0.0785695, acc 0.96875, prec 0.118231, recall 0.802148
2017-12-10T04:13:23.820061: step 9716, loss 0.25446, acc 0.9375, prec 0.118235, recall 0.802162
2017-12-10T04:13:24.549643: step 9717, loss 0.60168, acc 0.921875, prec 0.118229, recall 0.802162
2017-12-10T04:13:25.298187: step 9718, loss 1.10334, acc 0.9375, prec 0.118233, recall 0.802177
2017-12-10T04:13:26.000922: step 9719, loss 0.252513, acc 0.96875, prec 0.11824, recall 0.802191
2017-12-10T04:13:26.716029: step 9720, loss 0.198461, acc 0.984375, prec 0.118239, recall 0.802191
2017-12-10T04:13:27.464597: step 9721, loss 0.0778533, acc 0.984375, prec 0.118247, recall 0.802205
2017-12-10T04:13:28.202612: step 9722, loss 0.408854, acc 0.921875, prec 0.118259, recall 0.802233
2017-12-10T04:13:28.920368: step 9723, loss 0.247994, acc 0.96875, prec 0.118266, recall 0.802247
2017-12-10T04:13:29.611031: step 9724, loss 0.112115, acc 0.984375, prec 0.118283, recall 0.802276
2017-12-10T04:13:30.339715: step 9725, loss 0.298804, acc 0.953125, prec 0.118289, recall 0.80229
2017-12-10T04:13:31.059401: step 9726, loss 0.510796, acc 0.921875, prec 0.11831, recall 0.802332
2017-12-10T04:13:31.808653: step 9727, loss 0.269436, acc 0.96875, prec 0.118308, recall 0.802332
2017-12-10T04:13:32.528580: step 9728, loss 0.0259443, acc 0.984375, prec 0.118316, recall 0.802346
2017-12-10T04:13:33.256529: step 9729, loss 0.789712, acc 0.9375, prec 0.118311, recall 0.802346
2017-12-10T04:13:34.296647: step 9730, loss 0.00265785, acc 1, prec 0.11832, recall 0.802361
2017-12-10T04:13:34.727089: step 9731, loss 0.298018, acc 0.96875, prec 0.118336, recall 0.802389
2017-12-10T04:13:35.009093: step 9732, loss 1.13437, acc 0.921875, prec 0.11833, recall 0.802389
2017-12-10T04:13:35.289075: step 9733, loss 0.16673, acc 0.96875, prec 0.118346, recall 0.802417
2017-12-10T04:13:35.566116: step 9734, loss 0.00559572, acc 1, prec 0.118346, recall 0.802417
2017-12-10T04:13:35.844203: step 9735, loss 1.21072e-07, acc 1, prec 0.118356, recall 0.802431
2017-12-10T04:13:36.110733: step 9736, loss 0.0316642, acc 0.984375, prec 0.118373, recall 0.802459
2017-12-10T04:13:36.380978: step 9737, loss 0.950353, acc 0.96875, prec 0.11838, recall 0.802474
2017-12-10T04:13:36.652707: step 9738, loss 0.0792851, acc 0.96875, prec 0.118377, recall 0.802474
2017-12-10T04:13:36.917317: step 9739, loss 0.0014811, acc 1, prec 0.118377, recall 0.802474
2017-12-10T04:13:37.183185: step 9740, loss 23.6717, acc 0.96875, prec 0.118386, recall 0.802373
2017-12-10T04:13:37.450796: step 9741, loss 0.835924, acc 0.9375, prec 0.118382, recall 0.802373
2017-12-10T04:13:37.716353: step 9742, loss 0.414304, acc 0.9375, prec 0.118395, recall 0.802401
2017-12-10T04:13:37.984450: step 9743, loss 0.55832, acc 0.9375, prec 0.118399, recall 0.802415
2017-12-10T04:13:38.250025: step 9744, loss 0.796292, acc 0.90625, prec 0.118411, recall 0.802444
2017-12-10T04:13:38.513450: step 9745, loss 0.818164, acc 0.9375, prec 0.118415, recall 0.802458
2017-12-10T04:13:38.776687: step 9746, loss 0.528736, acc 0.90625, prec 0.118407, recall 0.802458
2017-12-10T04:13:39.039407: step 9747, loss 1.2729, acc 0.890625, prec 0.118399, recall 0.802458
2017-12-10T04:13:39.298977: step 9748, loss 0.919719, acc 0.9375, prec 0.118403, recall 0.802472
2017-12-10T04:13:39.564973: step 9749, loss 1.85208, acc 0.84375, prec 0.1184, recall 0.802486
2017-12-10T04:13:39.828586: step 9750, loss 1.68791, acc 0.9375, prec 0.118395, recall 0.802486
2017-12-10T04:13:40.090668: step 9751, loss 1.2718, acc 0.859375, prec 0.118402, recall 0.802514
2017-12-10T04:13:40.357596: step 9752, loss 1.66225, acc 0.8125, prec 0.118387, recall 0.802514
2017-12-10T04:13:40.621842: step 9753, loss 0.929463, acc 0.84375, prec 0.118384, recall 0.802528
2017-12-10T04:13:40.888695: step 9754, loss 1.65105, acc 0.875, prec 0.118374, recall 0.802528
2017-12-10T04:13:41.153503: step 9755, loss 1.15106, acc 0.875, prec 0.118364, recall 0.802528
2017-12-10T04:13:41.421138: step 9756, loss 1.5017, acc 0.859375, prec 0.118362, recall 0.802542
2017-12-10T04:13:41.685956: step 9757, loss 2.34439, acc 0.828125, prec 0.118358, recall 0.802556
2017-12-10T04:13:41.966584: step 9758, loss 1.43604, acc 0.875, prec 0.118348, recall 0.802556
2017-12-10T04:13:42.231070: step 9759, loss 0.957299, acc 0.890625, prec 0.118367, recall 0.802599
2017-12-10T04:13:42.496621: step 9760, loss 0.294582, acc 0.921875, prec 0.118361, recall 0.802599
2017-12-10T04:13:42.759474: step 9761, loss 0.529026, acc 0.890625, prec 0.118361, recall 0.802613
2017-12-10T04:13:43.020835: step 9762, loss 1.41016, acc 0.890625, prec 0.118352, recall 0.802613
2017-12-10T04:13:43.292457: step 9763, loss 0.0366702, acc 0.984375, prec 0.11836, recall 0.802627
2017-12-10T04:13:43.562469: step 9764, loss 1.11324, acc 0.90625, prec 0.118362, recall 0.802641
2017-12-10T04:13:43.823693: step 9765, loss 0.672415, acc 0.953125, prec 0.118358, recall 0.802641
2017-12-10T04:13:44.091468: step 9766, loss 0.432964, acc 0.953125, prec 0.118355, recall 0.802641
2017-12-10T04:13:44.355674: step 9767, loss 0.476779, acc 0.984375, prec 0.118372, recall 0.802669
2017-12-10T04:13:44.623652: step 9768, loss 0.175838, acc 0.96875, prec 0.118388, recall 0.802697
2017-12-10T04:13:44.887739: step 9769, loss 0.0611054, acc 0.984375, prec 0.118387, recall 0.802697
2017-12-10T04:13:45.151093: step 9770, loss 0.015177, acc 0.984375, prec 0.118386, recall 0.802697
2017-12-10T04:13:45.426102: step 9771, loss 0.00656608, acc 1, prec 0.118413, recall 0.80274
2017-12-10T04:13:45.693170: step 9772, loss 0.206316, acc 0.96875, prec 0.11842, recall 0.802754
2017-12-10T04:13:45.958970: step 9773, loss 0.0719628, acc 0.984375, prec 0.118419, recall 0.802754
2017-12-10T04:13:46.226275: step 9774, loss 0.265868, acc 0.96875, prec 0.118435, recall 0.802782
2017-12-10T04:13:46.492308: step 9775, loss 0.242481, acc 0.96875, prec 0.118442, recall 0.802796
2017-12-10T04:13:46.762539: step 9776, loss 0.0343703, acc 0.984375, prec 0.118441, recall 0.802796
2017-12-10T04:13:47.027613: step 9777, loss 6.38267e-06, acc 1, prec 0.118459, recall 0.802824
2017-12-10T04:13:47.286583: step 9778, loss 0.187547, acc 0.984375, prec 0.118467, recall 0.802838
2017-12-10T04:13:47.549339: step 9779, loss 14.1799, acc 0.96875, prec 0.118475, recall 0.802795
2017-12-10T04:13:47.821707: step 9780, loss 0.000883904, acc 1, prec 0.118475, recall 0.802795
2017-12-10T04:13:48.085537: step 9781, loss 0.141746, acc 0.984375, prec 0.118492, recall 0.802823
2017-12-10T04:13:48.353403: step 9782, loss 0.0578457, acc 0.96875, prec 0.118509, recall 0.802851
2017-12-10T04:13:48.617590: step 9783, loss 1.68059e-05, acc 1, prec 0.118518, recall 0.802865
2017-12-10T04:13:48.874626: step 9784, loss 0.862525, acc 0.9375, prec 0.118513, recall 0.802865
2017-12-10T04:13:49.141234: step 9785, loss 0.38072, acc 0.96875, prec 0.11852, recall 0.802879
2017-12-10T04:13:49.403240: step 9786, loss 0.0279881, acc 0.984375, prec 0.118518, recall 0.802879
2017-12-10T04:13:49.665850: step 9787, loss 1.33274e-05, acc 1, prec 0.118518, recall 0.802879
2017-12-10T04:13:49.934512: step 9788, loss 5.23661e-05, acc 1, prec 0.118528, recall 0.802893
2017-12-10T04:13:50.193619: step 9789, loss 7.85225, acc 0.953125, prec 0.118544, recall 0.802864
2017-12-10T04:13:50.460929: step 9790, loss 2.17686, acc 0.953125, prec 0.118541, recall 0.802807
2017-12-10T04:13:50.728653: step 9791, loss 0.648475, acc 0.953125, prec 0.118537, recall 0.802807
2017-12-10T04:13:50.996629: step 9792, loss 0.42161, acc 0.96875, prec 0.118563, recall 0.802849
2017-12-10T04:13:51.264474: step 9793, loss 0.0703547, acc 0.984375, prec 0.118562, recall 0.802849
2017-12-10T04:13:51.528303: step 9794, loss 0.264215, acc 0.96875, prec 0.118568, recall 0.802863
2017-12-10T04:13:51.800564: step 9795, loss 0.270778, acc 0.953125, prec 0.118565, recall 0.802863
2017-12-10T04:13:52.064585: step 9796, loss 0.195823, acc 0.96875, prec 0.118581, recall 0.802891
2017-12-10T04:13:52.327274: step 9797, loss 2.06216, acc 0.875, prec 0.11858, recall 0.802905
2017-12-10T04:13:52.591635: step 9798, loss 0.326603, acc 0.953125, prec 0.118585, recall 0.802919
2017-12-10T04:13:52.860118: step 9799, loss 0.40501, acc 0.953125, prec 0.118591, recall 0.802933
2017-12-10T04:13:53.123302: step 9800, loss 0.227203, acc 0.9375, prec 0.118605, recall 0.802961
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-9800

2017-12-10T04:13:54.449948: step 9801, loss 1.24135, acc 0.90625, prec 0.118606, recall 0.802975
2017-12-10T04:13:54.716295: step 9802, loss 0.627325, acc 0.953125, prec 0.118612, recall 0.802989
2017-12-10T04:13:54.982270: step 9803, loss 0.857919, acc 0.9375, prec 0.118616, recall 0.803003
2017-12-10T04:13:55.243803: step 9804, loss 0.260707, acc 0.9375, prec 0.11863, recall 0.803031
2017-12-10T04:13:55.513248: step 9805, loss 0.125331, acc 0.9375, prec 0.118634, recall 0.803045
2017-12-10T04:13:55.777534: step 9806, loss 0.359721, acc 0.953125, prec 0.11863, recall 0.803045
2017-12-10T04:13:56.042777: step 9807, loss 1.91804, acc 0.890625, prec 0.118621, recall 0.803045
2017-12-10T04:13:56.307295: step 9808, loss 0.627041, acc 0.953125, prec 0.118645, recall 0.803087
2017-12-10T04:13:56.573323: step 9809, loss 0.457773, acc 0.953125, prec 0.118651, recall 0.803101
2017-12-10T04:13:56.843052: step 9810, loss 0.504159, acc 0.96875, prec 0.118649, recall 0.803101
2017-12-10T04:13:57.107295: step 9811, loss 0.308316, acc 0.953125, prec 0.118645, recall 0.803101
2017-12-10T04:13:57.368621: step 9812, loss 1.0109, acc 0.921875, prec 0.118648, recall 0.803115
2017-12-10T04:13:57.630792: step 9813, loss 1.32373, acc 0.9375, prec 0.118661, recall 0.803143
2017-12-10T04:13:57.894342: step 9814, loss 0.203468, acc 0.953125, prec 0.118667, recall 0.803157
2017-12-10T04:13:58.163111: step 9815, loss 1.18178, acc 0.90625, prec 0.118678, recall 0.803185
2017-12-10T04:13:58.427997: step 9816, loss 0.618999, acc 0.9375, prec 0.118682, recall 0.803199
2017-12-10T04:13:58.696172: step 9817, loss 0.363616, acc 0.953125, prec 0.118688, recall 0.803213
2017-12-10T04:13:58.957420: step 9818, loss 0.446173, acc 0.9375, prec 0.118683, recall 0.803213
2017-12-10T04:13:59.222613: step 9819, loss 0.699588, acc 0.953125, prec 0.118707, recall 0.803255
2017-12-10T04:13:59.484803: step 9820, loss 0.383646, acc 0.953125, prec 0.118703, recall 0.803255
2017-12-10T04:13:59.752226: step 9821, loss 0.00786366, acc 1, prec 0.118712, recall 0.803269
2017-12-10T04:14:00.012913: step 9822, loss 0.250921, acc 0.953125, prec 0.118709, recall 0.803269
2017-12-10T04:14:00.285257: step 9823, loss 0.703492, acc 0.984375, prec 0.118754, recall 0.803339
2017-12-10T04:14:00.557669: step 9824, loss 0.00772221, acc 1, prec 0.118754, recall 0.803339
2017-12-10T04:14:00.826775: step 9825, loss 0.239135, acc 0.953125, prec 0.118759, recall 0.803353
2017-12-10T04:14:01.094769: step 9826, loss 0.0164306, acc 0.984375, prec 0.118776, recall 0.803381
2017-12-10T04:14:01.365789: step 9827, loss 0.270781, acc 0.96875, prec 0.118783, recall 0.803395
2017-12-10T04:14:01.637257: step 9828, loss 0.291626, acc 0.96875, prec 0.118781, recall 0.803395
2017-12-10T04:14:01.907215: step 9829, loss 3.91555e-05, acc 1, prec 0.118781, recall 0.803395
2017-12-10T04:14:02.166289: step 9830, loss 7.67677e-06, acc 1, prec 0.11879, recall 0.803409
2017-12-10T04:14:02.424718: step 9831, loss 0.0855314, acc 0.96875, prec 0.118806, recall 0.803437
2017-12-10T04:14:02.685971: step 9832, loss 0.649471, acc 0.96875, prec 0.118803, recall 0.803437
2017-12-10T04:14:02.948593: step 9833, loss 0.398107, acc 0.953125, prec 0.118827, recall 0.803479
2017-12-10T04:14:03.219209: step 9834, loss 0.00819298, acc 1, prec 0.118827, recall 0.803479
2017-12-10T04:14:03.480863: step 9835, loss 0.0261389, acc 0.984375, prec 0.118835, recall 0.803493
2017-12-10T04:14:03.745799: step 9836, loss 0.189955, acc 0.96875, prec 0.118851, recall 0.803521
2017-12-10T04:14:04.010120: step 9837, loss 0.165061, acc 0.984375, prec 0.118869, recall 0.803549
2017-12-10T04:14:04.277456: step 9838, loss 0.162733, acc 0.96875, prec 0.118866, recall 0.803549
2017-12-10T04:14:04.550189: step 9839, loss 0.000141364, acc 1, prec 0.118885, recall 0.803576
2017-12-10T04:14:04.823129: step 9840, loss 3.93182e-05, acc 1, prec 0.118912, recall 0.803618
2017-12-10T04:14:05.082664: step 9841, loss 12.4217, acc 0.953125, prec 0.11891, recall 0.803561
2017-12-10T04:14:05.349228: step 9842, loss 0.907241, acc 0.9375, prec 0.118905, recall 0.803561
2017-12-10T04:14:05.611697: step 9843, loss 0.145674, acc 0.96875, prec 0.118921, recall 0.803589
2017-12-10T04:14:05.877667: step 9844, loss 0.494836, acc 0.96875, prec 0.118918, recall 0.803589
2017-12-10T04:14:06.143808: step 9845, loss 0.00452479, acc 1, prec 0.118928, recall 0.803603
2017-12-10T04:14:06.417366: step 9846, loss 1.28522, acc 0.96875, prec 0.118953, recall 0.803645
2017-12-10T04:14:06.684316: step 9847, loss 0.198938, acc 0.984375, prec 0.118961, recall 0.803659
2017-12-10T04:14:06.945814: step 9848, loss 0.270304, acc 0.96875, prec 0.118958, recall 0.803659
2017-12-10T04:14:07.212039: step 9849, loss 0.465747, acc 0.9375, prec 0.118953, recall 0.803659
2017-12-10T04:14:07.480504: step 9850, loss 0.305512, acc 0.96875, prec 0.118951, recall 0.803659
2017-12-10T04:14:07.748825: step 9851, loss 1.17028, acc 0.90625, prec 0.118943, recall 0.803659
2017-12-10T04:14:08.007962: step 9852, loss 0.400672, acc 0.953125, prec 0.11894, recall 0.803659
2017-12-10T04:14:08.276800: step 9853, loss 1.0791, acc 0.90625, prec 0.118951, recall 0.803687
2017-12-10T04:14:08.539677: step 9854, loss 0.800078, acc 0.90625, prec 0.118971, recall 0.803728
2017-12-10T04:14:08.807152: step 9855, loss 0.966752, acc 0.921875, prec 0.118965, recall 0.803728
2017-12-10T04:14:09.078944: step 9856, loss 1.25641, acc 0.9375, prec 0.118969, recall 0.803742
2017-12-10T04:14:09.342141: step 9857, loss 0.259734, acc 0.921875, prec 0.118972, recall 0.803756
2017-12-10T04:14:09.612281: step 9858, loss 1.57703, acc 0.890625, prec 0.118963, recall 0.803756
2017-12-10T04:14:09.886415: step 9859, loss 1.24571, acc 0.90625, prec 0.118956, recall 0.803756
2017-12-10T04:14:10.156884: step 9860, loss 0.477996, acc 0.953125, prec 0.118952, recall 0.803756
2017-12-10T04:14:10.423812: step 9861, loss 1.00434, acc 0.90625, prec 0.118963, recall 0.803784
2017-12-10T04:14:10.685148: step 9862, loss 1.04261, acc 0.921875, prec 0.118966, recall 0.803798
2017-12-10T04:14:10.950561: step 9863, loss 0.763707, acc 0.921875, prec 0.11896, recall 0.803798
2017-12-10T04:14:11.216307: step 9864, loss 0.230636, acc 0.953125, prec 0.118956, recall 0.803798
2017-12-10T04:14:11.481746: step 9865, loss 0.00702813, acc 1, prec 0.118965, recall 0.803812
2017-12-10T04:14:11.753126: step 9866, loss 0.688879, acc 0.953125, prec 0.11898, recall 0.80384
2017-12-10T04:14:12.034616: step 9867, loss 0.261778, acc 0.96875, prec 0.118987, recall 0.803854
2017-12-10T04:14:12.311390: step 9868, loss 0.122974, acc 0.9375, prec 0.118991, recall 0.803867
2017-12-10T04:14:12.573593: step 9869, loss 0.00422935, acc 1, prec 0.119, recall 0.803881
2017-12-10T04:14:12.838008: step 9870, loss 0.185354, acc 0.984375, prec 0.118999, recall 0.803881
2017-12-10T04:14:13.103922: step 9871, loss 0.0154066, acc 0.984375, prec 0.119007, recall 0.803895
2017-12-10T04:14:13.365880: step 9872, loss 0.904325, acc 0.921875, prec 0.11901, recall 0.803909
2017-12-10T04:14:13.630241: step 9873, loss 0.331936, acc 0.96875, prec 0.119007, recall 0.803909
2017-12-10T04:14:13.901270: step 9874, loss 0.306673, acc 0.9375, prec 0.11903, recall 0.803951
2017-12-10T04:14:14.160426: step 9875, loss 0.195841, acc 0.96875, prec 0.119028, recall 0.803951
2017-12-10T04:14:14.423796: step 9876, loss 0.0456153, acc 0.984375, prec 0.119026, recall 0.803951
2017-12-10T04:14:14.690296: step 9877, loss 0.494593, acc 0.984375, prec 0.119034, recall 0.803965
2017-12-10T04:14:14.958525: step 9878, loss 0.286701, acc 0.96875, prec 0.119041, recall 0.803979
2017-12-10T04:14:15.222319: step 9879, loss 0.5231, acc 0.96875, prec 0.119039, recall 0.803979
2017-12-10T04:14:15.493854: step 9880, loss 0.2083, acc 0.984375, prec 0.119047, recall 0.803992
2017-12-10T04:14:15.758013: step 9881, loss 0.662437, acc 0.953125, prec 0.119052, recall 0.804006
2017-12-10T04:14:16.022235: step 9882, loss 0.884356, acc 0.953125, prec 0.119067, recall 0.804034
2017-12-10T04:14:16.292275: step 9883, loss 0.346572, acc 0.984375, prec 0.119075, recall 0.804048
2017-12-10T04:14:16.555970: step 9884, loss 0.0663316, acc 1, prec 0.119084, recall 0.804062
2017-12-10T04:14:16.816659: step 9885, loss 0.23502, acc 0.984375, prec 0.119083, recall 0.804062
2017-12-10T04:14:17.079259: step 9886, loss 0.239397, acc 0.953125, prec 0.119098, recall 0.804089
2017-12-10T04:14:17.344284: step 9887, loss 0.000121411, acc 1, prec 0.119098, recall 0.804089
2017-12-10T04:14:17.607422: step 9888, loss 0.463654, acc 0.96875, prec 0.119104, recall 0.804103
2017-12-10T04:14:17.870036: step 9889, loss 0.00137048, acc 1, prec 0.119132, recall 0.804145
2017-12-10T04:14:18.130087: step 9890, loss 0.575017, acc 0.984375, prec 0.119131, recall 0.804145
2017-12-10T04:14:18.393942: step 9891, loss 0.012609, acc 0.984375, prec 0.119129, recall 0.804145
2017-12-10T04:14:18.659926: step 9892, loss 0.155815, acc 0.984375, prec 0.119147, recall 0.804173
2017-12-10T04:14:18.922658: step 9893, loss 0.00611484, acc 1, prec 0.119165, recall 0.8042
2017-12-10T04:14:19.191088: step 9894, loss 0.0978025, acc 0.984375, prec 0.119164, recall 0.8042
2017-12-10T04:14:19.460763: step 9895, loss 0.51578, acc 0.9375, prec 0.119159, recall 0.8042
2017-12-10T04:14:19.723881: step 9896, loss 0.770029, acc 0.96875, prec 0.119175, recall 0.804228
2017-12-10T04:14:19.991529: step 9897, loss 0.151256, acc 0.96875, prec 0.119191, recall 0.804256
2017-12-10T04:14:20.257901: step 9898, loss 0.0667714, acc 0.96875, prec 0.119188, recall 0.804256
2017-12-10T04:14:20.523576: step 9899, loss 0.008253, acc 1, prec 0.119198, recall 0.804269
2017-12-10T04:14:20.784858: step 9900, loss 0.219057, acc 0.96875, prec 0.119213, recall 0.804297

Evaluation:
2017-12-10T04:14:28.467079: step 9900, loss 22.0148, acc 0.962544, prec 0.119321, recall 0.799441

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_1/1512894211/checkpoints/model-9900

2017-12-10T04:14:29.717765: step 9901, loss 0.0207856, acc 0.984375, prec 0.119319, recall 0.799441
2017-12-10T04:14:29.979998: step 9902, loss 0.260707, acc 0.984375, prec 0.119327, recall 0.799455
2017-12-10T04:14:30.257812: step 9903, loss 0.0742554, acc 0.953125, prec 0.119333, recall 0.799469
2017-12-10T04:14:30.524206: step 9904, loss 0.00338209, acc 1, prec 0.119351, recall 0.799497
2017-12-10T04:14:30.789992: step 9905, loss 0.442905, acc 0.96875, prec 0.119349, recall 0.799497
2017-12-10T04:14:31.057271: step 9906, loss 0.402539, acc 0.96875, prec 0.119346, recall 0.799497
2017-12-10T04:14:31.326513: step 9907, loss 0.453567, acc 0.984375, prec 0.119354, recall 0.799511
2017-12-10T04:14:31.589632: step 9908, loss 0.00398902, acc 1, prec 0.119354, recall 0.799511
2017-12-10T04:14:31.859885: step 9909, loss 0.00181713, acc 1, prec 0.119363, recall 0.799525
2017-12-10T04:14:32.134077: step 9910, loss 1.26282e-06, acc 1, prec 0.119363, recall 0.799525
2017-12-10T04:14:32.390910: step 9911, loss 0.0486435, acc 0.984375, prec 0.119371, recall 0.799539
2017-12-10T04:14:32.662359: step 9912, loss 0.0529491, acc 0.96875, prec 0.119369, recall 0.799539
2017-12-10T04:14:32.932202: step 9913, loss 5.82714e-06, acc 1, prec 0.119369, recall 0.799539
2017-12-10T04:14:33.198876: step 9914, loss 0.709701, acc 0.9375, prec 0.119364, recall 0.799539
2017-12-10T04:14:33.457586: step 9915, loss 0.352199, acc 0.984375, prec 0.119372, recall 0.799553
2017-12-10T04:14:33.719948: step 9916, loss 0.0669823, acc 0.984375, prec 0.119371, recall 0.799553
2017-12-10T04:14:33.992982: step 9917, loss 0.270072, acc 0.96875, prec 0.119368, recall 0.799553
2017-12-10T04:14:34.259009: step 9918, loss 0.000440864, acc 1, prec 0.119368, recall 0.799553
2017-12-10T04:14:34.519359: step 9919, loss 0.0602861, acc 0.96875, prec 0.119366, recall 0.799553
2017-12-10T04:14:34.790167: step 9920, loss 0.953134, acc 0.984375, prec 0.119392, recall 0.799595
2017-12-10T04:14:35.057784: step 9921, loss 2.23137e-06, acc 1, prec 0.119401, recall 0.799609
2017-12-10T04:14:35.324330: step 9922, loss 8.64374e-05, acc 1, prec 0.11941, recall 0.799623
2017-12-10T04:14:35.597382: step 9923, loss 0.181191, acc 0.984375, prec 0.119409, recall 0.799623
2017-12-10T04:14:35.862179: step 9924, loss 0.458727, acc 0.9375, prec 0.119422, recall 0.799651
2017-12-10T04:14:36.122738: step 9925, loss 0.944148, acc 0.96875, prec 0.11942, recall 0.799651
2017-12-10T04:14:36.388187: step 9926, loss 0.15924, acc 0.96875, prec 0.119427, recall 0.799665
2017-12-10T04:14:36.650521: step 9927, loss 0.107645, acc 0.96875, prec 0.119433, recall 0.799679
2017-12-10T04:14:36.922543: step 9928, loss 0.0841669, acc 0.984375, prec 0.119432, recall 0.799679
2017-12-10T04:14:37.195541: step 9929, loss 0.849363, acc 0.96875, prec 0.11943, recall 0.799679
2017-12-10T04:14:37.463001: step 9930, loss 0.0435394, acc 0.984375, prec 0.119428, recall 0.799679
2017-12-10T04:14:37.731567: step 9931, loss 0.000358548, acc 1, prec 0.119428, recall 0.799679
2017-12-10T04:14:38.001995: step 9932, loss 1.23436, acc 0.96875, prec 0.119435, recall 0.799693
2017-12-10T04:14:38.267303: step 9933, loss 0.0485613, acc 0.984375, prec 0.119443, recall 0.799707
2017-12-10T04:14:38.527877: step 9934, loss 8.24302, acc 0.96875, prec 0.119442, recall 0.799651
2017-12-10T04:14:38.803426: step 9935, loss 0.376973, acc 0.9375, prec 0.119446, recall 0.799665
2017-12-10T04:14:39.073979: step 9936, loss 0.806738, acc 0.953125, prec 0.119451, recall 0.799679
2017-12-10T04:14:39.339461: step 9937, loss 0.435812, acc 0.953125, prec 0.119448, recall 0.799679
2017-12-10T04:14:39.601240: step 9938, loss 0.155439, acc 0.96875, prec 0.119445, recall 0.799679
2017-12-10T04:14:39.865242: step 9939, loss 0.642346, acc 0.921875, prec 0.119448, recall 0.799693
2017-12-10T04:14:40.098510: step 9940, loss 0.586106, acc 0.941176, prec 0.119444, recall 0.799693
Training finished
Starting Fold: 2 => Train/Dev split: 31796/10598


 #################### 


SEQUENCE LENGTH 273
BATCH SIZE 64
EMBEDDING SIZE 128
FILTER SIZES [3, 4, 5]
NUMBER OF FILTERS 256
L2 REG LAMBDA 0.0
EPOCHS 20



RESULT DIR num_filter_256_fold_2
VOCAB SIZE 33447
DROPOUT PROBABILITY 0.5



WORD EMBEDDING True
POSITION EMBEDDING True
POS EMBEDDING True


 #################### 


Writing to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281

Start training
2017-12-10T04:14:43.847798: step 1, loss 7.42618, acc 0.046875, prec 0, recall 0
2017-12-10T04:14:44.107858: step 2, loss 1.55082, acc 0.5625, prec 0, recall 0
2017-12-10T04:14:44.376418: step 3, loss 13.358, acc 0.96875, prec 0, recall 0
2017-12-10T04:14:44.639779: step 4, loss 16.0278, acc 0.96875, prec 0, recall 0
2017-12-10T04:14:44.909015: step 5, loss 39.4946, acc 0.9375, prec 0, recall 0
2017-12-10T04:14:45.176844: step 6, loss 21.1839, acc 0.921875, prec 0, recall 0
2017-12-10T04:14:45.444112: step 7, loss 0.313656, acc 0.859375, prec 0, recall 0
2017-12-10T04:14:45.707311: step 8, loss 6.99393, acc 0.625, prec 0, recall 0
2017-12-10T04:14:45.965548: step 9, loss 3.4988, acc 0.4375, prec 0.00609756, recall 0.111111
2017-12-10T04:14:46.234941: step 10, loss 5.77547, acc 0.234375, prec 0.0139535, recall 0.272727
2017-12-10T04:14:46.501037: step 11, loss 6.12743, acc 0.140625, prec 0.0147601, recall 0.333333
2017-12-10T04:14:46.764511: step 12, loss 7.9834, acc 0.015625, prec 0.0149254, recall 0.384615
2017-12-10T04:14:47.032948: step 13, loss 9.64875, acc 0.015625, prec 0.0125628, recall 0.384615
2017-12-10T04:14:47.287599: step 14, loss 9.86019, acc 0.0625, prec 0.010917, recall 0.384615
2017-12-10T04:14:47.540248: step 15, loss 9.97198, acc 0.03125, prec 0.00961538, recall 0.384615
2017-12-10T04:14:47.803496: step 16, loss 8.42706, acc 0.0625, prec 0.0120275, recall 0.466667
2017-12-10T04:14:48.068178: step 17, loss 7.65758, acc 0.109375, prec 0.0125, recall 0.5
2017-12-10T04:14:48.325392: step 18, loss 6.38444, acc 0.171875, prec 0.011544, recall 0.5
2017-12-10T04:14:48.588149: step 19, loss 4.51264, acc 0.265625, prec 0.0108108, recall 0.5
2017-12-10T04:14:48.847958: step 20, loss 4.33538, acc 0.328125, prec 0.0127551, recall 0.526316
2017-12-10T04:14:49.108482: step 21, loss 2.16195, acc 0.546875, prec 0.0123001, recall 0.526316
2017-12-10T04:14:49.369249: step 22, loss 16.2549, acc 0.546875, prec 0.0118906, recall 0.5
2017-12-10T04:14:49.630175: step 23, loss 1.17852, acc 0.734375, prec 0.011655, recall 0.5
2017-12-10T04:14:49.892833: step 24, loss 25.2693, acc 0.703125, prec 0.0125571, recall 0.478261
2017-12-10T04:14:50.160101: step 25, loss 20.314, acc 0.734375, prec 0.0123457, recall 0.44
2017-12-10T04:14:50.423584: step 26, loss 1.20446, acc 0.703125, prec 0.0120879, recall 0.44
2017-12-10T04:14:50.695018: step 27, loss 19.7838, acc 0.65625, prec 0.0118153, recall 0.423077
2017-12-10T04:14:50.956193: step 28, loss 2.49182, acc 0.53125, prec 0.0114464, recall 0.423077
2017-12-10T04:14:51.218742: step 29, loss 2.69497, acc 0.453125, prec 0.0110442, recall 0.423077
2017-12-10T04:14:51.478343: step 30, loss 3.08016, acc 0.453125, prec 0.0106693, recall 0.423077
2017-12-10T04:14:51.736678: step 31, loss 2.68354, acc 0.34375, prec 0.0102516, recall 0.423077
2017-12-10T04:14:51.999173: step 32, loss 4.21371, acc 0.40625, prec 0.0107914, recall 0.444444
2017-12-10T04:14:52.261032: step 33, loss 7.229, acc 0.3125, prec 0.0103896, recall 0.428571
2017-12-10T04:14:52.523172: step 34, loss 3.35972, acc 0.359375, prec 0.0108605, recall 0.448276
2017-12-10T04:14:52.785190: step 35, loss 4.20511, acc 0.328125, prec 0.0104839, recall 0.448276
2017-12-10T04:14:53.045791: step 36, loss 3.00396, acc 0.375, prec 0.010929, recall 0.466667
2017-12-10T04:14:53.305173: step 37, loss 3.45336, acc 0.28125, prec 0.0112952, recall 0.483871
2017-12-10T04:14:53.563383: step 38, loss 3.19262, acc 0.46875, prec 0.0124633, recall 0.515152
2017-12-10T04:14:53.824248: step 39, loss 2.08108, acc 0.578125, prec 0.012931, recall 0.529412
2017-12-10T04:14:54.086883: step 40, loss 2.5248, acc 0.5, prec 0.0126404, recall 0.529412
2017-12-10T04:14:54.351309: step 41, loss 1.34983, acc 0.703125, prec 0.0131579, recall 0.542857
2017-12-10T04:14:54.612774: step 42, loss 0.705591, acc 0.828125, prec 0.0130584, recall 0.542857
2017-12-10T04:14:54.869996: step 43, loss 3.60948, acc 0.765625, prec 0.012934, recall 0.527778
2017-12-10T04:14:55.129857: step 44, loss 18.3716, acc 0.828125, prec 0.0128465, recall 0.513514
2017-12-10T04:14:55.392127: step 45, loss 4.43103, acc 0.765625, prec 0.0127261, recall 0.5
2017-12-10T04:14:55.659840: step 46, loss 0.983337, acc 0.796875, prec 0.0126162, recall 0.5
2017-12-10T04:14:55.920277: step 47, loss 0.446494, acc 0.859375, prec 0.0125413, recall 0.5
2017-12-10T04:14:56.183084: step 48, loss 1.31248, acc 0.84375, prec 0.0137525, recall 0.525
2017-12-10T04:14:56.456098: step 49, loss 1.49811, acc 0.703125, prec 0.0135834, recall 0.525
2017-12-10T04:14:56.729728: step 50, loss 14.635, acc 0.71875, prec 0.0134357, recall 0.512195
2017-12-10T04:14:57.005280: step 51, loss 5.95263, acc 0.53125, prec 0.013191, recall 0.5
2017-12-10T04:14:57.269903: step 52, loss 1.5261, acc 0.703125, prec 0.0130354, recall 0.5
2017-12-10T04:14:57.531951: step 53, loss 2.84298, acc 0.484375, prec 0.0133739, recall 0.511628
2017-12-10T04:14:57.795987: step 54, loss 2.60153, acc 0.515625, prec 0.0131265, recall 0.511628
2017-12-10T04:14:58.058010: step 55, loss 3.71634, acc 0.40625, prec 0.013986, recall 0.533333
2017-12-10T04:14:58.318203: step 56, loss 3.11388, acc 0.421875, prec 0.0142531, recall 0.543478
2017-12-10T04:14:58.581400: step 57, loss 3.36308, acc 0.390625, prec 0.0144928, recall 0.553191
2017-12-10T04:14:58.840870: step 58, loss 5.3441, acc 0.5625, prec 0.0148189, recall 0.55102
2017-12-10T04:14:59.113998: step 59, loss 3.06318, acc 0.5, prec 0.0150943, recall 0.56
2017-12-10T04:14:59.384115: step 60, loss 3.67587, acc 0.46875, prec 0.0153439, recall 0.568627
2017-12-10T04:14:59.646782: step 61, loss 2.51234, acc 0.59375, prec 0.0156495, recall 0.576923
2017-12-10T04:14:59.907577: step 62, loss 2.67387, acc 0.53125, prec 0.0154083, recall 0.576923
2017-12-10T04:15:00.176069: step 63, loss 2.78974, acc 0.453125, prec 0.0156329, recall 0.584906
2017-12-10T04:15:00.440514: step 64, loss 1.28006, acc 0.640625, prec 0.0159442, recall 0.592593
2017-12-10T04:15:00.701899: step 65, loss 6.58409, acc 0.671875, prec 0.0157869, recall 0.581818
2017-12-10T04:15:00.962121: step 66, loss 1.56907, acc 0.65625, prec 0.0165773, recall 0.596491
2017-12-10T04:15:01.222457: step 67, loss 1.82128, acc 0.625, prec 0.0163855, recall 0.596491
2017-12-10T04:15:01.487832: step 68, loss 1.79254, acc 0.59375, prec 0.0161828, recall 0.596491
2017-12-10T04:15:01.750091: step 69, loss 0.843636, acc 0.8125, prec 0.0160909, recall 0.596491
2017-12-10T04:15:02.732299: step 70, loss 14.0016, acc 0.75, prec 0.0159774, recall 0.586207
2017-12-10T04:15:03.085609: step 71, loss 7.89485, acc 0.8125, prec 0.0168146, recall 0.590164
2017-12-10T04:15:03.351712: step 72, loss 1.25085, acc 0.71875, prec 0.0166744, recall 0.590164
2017-12-10T04:15:03.704123: step 73, loss 1.82707, acc 0.6875, prec 0.0169725, recall 0.596774
2017-12-10T04:15:04.437639: step 74, loss 14.1314, acc 0.796875, prec 0.0177758, recall 0.6
2017-12-10T04:15:05.402110: step 75, loss 1.22457, acc 0.703125, prec 0.0180668, recall 0.606061
2017-12-10T04:15:05.753556: step 76, loss 3.53562, acc 0.609375, prec 0.0183117, recall 0.602941
2017-12-10T04:15:06.084678: step 77, loss 2.15122, acc 0.546875, prec 0.0180776, recall 0.602941
2017-12-10T04:15:06.364537: step 78, loss 5.23946, acc 0.453125, prec 0.0186632, recall 0.605634
2017-12-10T04:15:06.644194: step 79, loss 4.16481, acc 0.390625, prec 0.0183525, recall 0.605634
2017-12-10T04:15:06.922292: step 80, loss 3.90395, acc 0.359375, prec 0.0180369, recall 0.605634
2017-12-10T04:15:07.199320: step 81, loss 4.80362, acc 0.4375, prec 0.0177759, recall 0.597222
2017-12-10T04:15:07.471654: step 82, loss 3.63304, acc 0.34375, prec 0.0174726, recall 0.597222
2017-12-10T04:15:07.739959: step 83, loss 4.77565, acc 0.3125, prec 0.0183413, recall 0.613333
2017-12-10T04:15:08.004475: step 84, loss 2.94918, acc 0.5, prec 0.0200393, recall 0.6375
2017-12-10T04:15:08.279022: step 85, loss 4.34999, acc 0.34375, prec 0.0200927, recall 0.641975
2017-12-10T04:15:08.538894: step 86, loss 2.85762, acc 0.40625, prec 0.019802, recall 0.641975
2017-12-10T04:15:08.803649: step 87, loss 2.69488, acc 0.5, prec 0.0195636, recall 0.641975
2017-12-10T04:15:09.065886: step 88, loss 1.51105, acc 0.65625, prec 0.0204994, recall 0.654762
2017-12-10T04:15:09.336367: step 89, loss 2.64274, acc 0.578125, prec 0.0210177, recall 0.662791
2017-12-10T04:15:09.605341: step 90, loss 2.00326, acc 0.6875, prec 0.0212221, recall 0.666667
2017-12-10T04:15:09.868739: step 91, loss 1.75848, acc 0.734375, prec 0.0214467, recall 0.670455
2017-12-10T04:15:10.136109: step 92, loss 9.5187, acc 0.75, prec 0.0216841, recall 0.666667
2017-12-10T04:15:10.404912: step 93, loss 0.405017, acc 0.890625, prec 0.021982, recall 0.67033
2017-12-10T04:15:10.669352: step 94, loss 1.54861, acc 0.734375, prec 0.0221984, recall 0.673913
2017-12-10T04:15:10.932705: step 95, loss 0.961312, acc 0.765625, prec 0.0220798, recall 0.673913
2017-12-10T04:15:11.195847: step 96, loss 1.20544, acc 0.796875, prec 0.021978, recall 0.673913
2017-12-10T04:15:11.461771: step 97, loss 2.25492, acc 0.796875, prec 0.0222301, recall 0.670213
2017-12-10T04:15:11.740246: step 98, loss 1.26313, acc 0.765625, prec 0.022113, recall 0.670213
2017-12-10T04:15:12.011844: step 99, loss 11.9819, acc 0.8125, prec 0.022028, recall 0.663158
2017-12-10T04:15:12.274128: step 100, loss 0.907609, acc 0.8125, prec 0.0222764, recall 0.666667
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-100

2017-12-10T04:15:13.687171: step 101, loss 1.11255, acc 0.859375, prec 0.022546, recall 0.670103
2017-12-10T04:15:13.954535: step 102, loss 4.72595, acc 0.8125, prec 0.0227979, recall 0.666667
2017-12-10T04:15:14.217386: step 103, loss 0.904893, acc 0.78125, prec 0.0230241, recall 0.67
2017-12-10T04:15:14.476644: step 104, loss 1.20222, acc 0.734375, prec 0.023224, recall 0.673267
2017-12-10T04:15:14.737336: step 105, loss 28.4377, acc 0.734375, prec 0.0237691, recall 0.666667
2017-12-10T04:15:14.996441: step 106, loss 1.82953, acc 0.671875, prec 0.0236008, recall 0.666667
2017-12-10T04:15:15.254641: step 107, loss 2.63564, acc 0.609375, prec 0.0234035, recall 0.666667
2017-12-10T04:15:15.513598: step 108, loss 4.16756, acc 0.421875, prec 0.0231176, recall 0.666667
2017-12-10T04:15:15.773842: step 109, loss 4.6807, acc 0.328125, prec 0.0227939, recall 0.666667
2017-12-10T04:15:16.032712: step 110, loss 4.11334, acc 0.390625, prec 0.0231362, recall 0.672897
2017-12-10T04:15:16.291464: step 111, loss 4.43255, acc 0.421875, prec 0.0234846, recall 0.678899
2017-12-10T04:15:16.553177: step 112, loss 3.79607, acc 0.390625, prec 0.0231975, recall 0.678899
2017-12-10T04:15:16.812540: step 113, loss 3.42652, acc 0.34375, prec 0.0235003, recall 0.684685
2017-12-10T04:15:17.073325: step 114, loss 3.93325, acc 0.390625, prec 0.0232203, recall 0.684685
2017-12-10T04:15:17.334007: step 115, loss 2.62502, acc 0.5625, prec 0.0233192, recall 0.6875
2017-12-10T04:15:17.590821: step 116, loss 6.14848, acc 0.53125, prec 0.0231162, recall 0.681416
2017-12-10T04:15:17.849202: step 117, loss 17.8765, acc 0.609375, prec 0.0232489, recall 0.672414
2017-12-10T04:15:18.112177: step 118, loss 4.4623, acc 0.515625, prec 0.0233314, recall 0.669492
2017-12-10T04:15:18.378113: step 119, loss 2.59811, acc 0.421875, prec 0.0233645, recall 0.672269
2017-12-10T04:15:18.637863: step 120, loss 2.12396, acc 0.546875, prec 0.0234511, recall 0.675
2017-12-10T04:15:18.900222: step 121, loss 3.49856, acc 0.5, prec 0.0235159, recall 0.677686
2017-12-10T04:15:19.159028: step 122, loss 2.72229, acc 0.515625, prec 0.0238636, recall 0.682927
2017-12-10T04:15:19.419340: step 123, loss 3.76215, acc 0.375, prec 0.0235955, recall 0.682927
2017-12-10T04:15:19.678784: step 124, loss 3.75178, acc 0.5, prec 0.0236571, recall 0.685484
2017-12-10T04:15:19.936778: step 125, loss 2.29259, acc 0.609375, prec 0.0237635, recall 0.688
2017-12-10T04:15:20.196012: step 126, loss 1.33598, acc 0.59375, prec 0.023594, recall 0.688
2017-12-10T04:15:20.458699: step 127, loss 1.92955, acc 0.609375, prec 0.0234332, recall 0.688
2017-12-10T04:15:20.714667: step 128, loss 1.74183, acc 0.609375, prec 0.023539, recall 0.690476
2017-12-10T04:15:20.975853: step 129, loss 1.61764, acc 0.625, prec 0.0233871, recall 0.690476
2017-12-10T04:15:21.241812: step 130, loss 2.46655, acc 0.765625, prec 0.0232994, recall 0.685039
2017-12-10T04:15:21.510908: step 131, loss 0.977328, acc 0.828125, prec 0.0234917, recall 0.6875
2017-12-10T04:15:21.782110: step 132, loss 4.62556, acc 0.8125, prec 0.0234229, recall 0.682171
2017-12-10T04:15:22.049497: step 133, loss 1.8434, acc 0.828125, prec 0.0233608, recall 0.676923
2017-12-10T04:15:22.311204: step 134, loss 3.17935, acc 0.765625, prec 0.0237906, recall 0.676692
2017-12-10T04:15:22.578787: step 135, loss 7.39445, acc 0.625, prec 0.0239033, recall 0.674074
2017-12-10T04:15:22.840723: step 136, loss 1.43893, acc 0.65625, prec 0.023766, recall 0.674074
2017-12-10T04:15:23.102103: step 137, loss 2.67229, acc 0.53125, prec 0.0238342, recall 0.676471
2017-12-10T04:15:23.361495: step 138, loss 4.15338, acc 0.375, prec 0.0240902, recall 0.681159
2017-12-10T04:15:23.622873: step 139, loss 4.9398, acc 0.421875, prec 0.0241117, recall 0.683453
2017-12-10T04:15:23.885989: step 140, loss 2.80558, acc 0.546875, prec 0.0241814, recall 0.685714
2017-12-10T04:15:24.145009: step 141, loss 3.50849, acc 0.40625, prec 0.0239521, recall 0.685714
2017-12-10T04:15:24.416460: step 142, loss 4.12625, acc 0.40625, prec 0.0237271, recall 0.685714
2017-12-10T04:15:24.675045: step 143, loss 2.96396, acc 0.5, prec 0.023541, recall 0.685714
2017-12-10T04:15:24.937608: step 144, loss 4.95118, acc 0.3125, prec 0.024, recall 0.692308
2017-12-10T04:15:25.195517: step 145, loss 3.15862, acc 0.46875, prec 0.0238038, recall 0.692308
2017-12-10T04:15:25.455618: step 146, loss 1.74004, acc 0.59375, prec 0.0238892, recall 0.694444
2017-12-10T04:15:25.719029: step 147, loss 2.21587, acc 0.640625, prec 0.0242223, recall 0.69863
2017-12-10T04:15:25.981747: step 148, loss 1.98332, acc 0.5625, prec 0.0245225, recall 0.702703
2017-12-10T04:15:26.239255: step 149, loss 1.40359, acc 0.71875, prec 0.0244189, recall 0.702703
2017-12-10T04:15:26.500488: step 150, loss 1.01387, acc 0.71875, prec 0.0245442, recall 0.704698
2017-12-10T04:15:26.758596: step 151, loss 0.747953, acc 0.71875, prec 0.0244413, recall 0.704698
2017-12-10T04:15:27.035970: step 152, loss 1.68252, acc 0.765625, prec 0.0245826, recall 0.706667
2017-12-10T04:15:27.297366: step 153, loss 2.50928, acc 0.796875, prec 0.0247399, recall 0.703947
2017-12-10T04:15:27.573845: step 154, loss 8.39123, acc 0.8125, prec 0.0246771, recall 0.699346
2017-12-10T04:15:27.843532: step 155, loss 4.82091, acc 0.875, prec 0.024643, recall 0.690323
2017-12-10T04:15:28.107599: step 156, loss 0.840606, acc 0.828125, prec 0.0245807, recall 0.690323
2017-12-10T04:15:28.369498: step 157, loss 10.8306, acc 0.75, prec 0.0244963, recall 0.685897
2017-12-10T04:15:28.634840: step 158, loss 1.76818, acc 0.703125, prec 0.0243902, recall 0.685897
2017-12-10T04:15:28.892211: step 159, loss 1.63584, acc 0.640625, prec 0.024263, recall 0.685897
2017-12-10T04:15:29.146785: step 160, loss 2.04991, acc 0.6875, prec 0.0245939, recall 0.689873
2017-12-10T04:15:29.416612: step 161, loss 6.53339, acc 0.515625, prec 0.0248656, recall 0.689441
2017-12-10T04:15:29.682037: step 162, loss 2.46096, acc 0.625, prec 0.0247326, recall 0.689441
2017-12-10T04:15:29.947220: step 163, loss 4.45115, acc 0.484375, prec 0.0249834, recall 0.693252
2017-12-10T04:15:30.217548: step 164, loss 3.99597, acc 0.375, prec 0.0249781, recall 0.695122
2017-12-10T04:15:30.487581: step 165, loss 4.07069, acc 0.34375, prec 0.0251736, recall 0.698795
2017-12-10T04:15:30.752047: step 166, loss 4.8531, acc 0.3125, prec 0.0249355, recall 0.698795
2017-12-10T04:15:31.013854: step 167, loss 3.71335, acc 0.328125, prec 0.0247071, recall 0.698795
2017-12-10T04:15:31.270950: step 168, loss 4.41809, acc 0.4375, prec 0.0251373, recall 0.704142
2017-12-10T04:15:31.532876: step 169, loss 3.5112, acc 0.390625, prec 0.0249319, recall 0.704142
2017-12-10T04:15:31.790101: step 170, loss 3.99878, acc 0.421875, prec 0.0247401, recall 0.704142
2017-12-10T04:15:32.046777: step 171, loss 2.36848, acc 0.625, prec 0.0246173, recall 0.704142
2017-12-10T04:15:32.308947: step 172, loss 1.47587, acc 0.6875, prec 0.0247168, recall 0.705882
2017-12-10T04:15:32.569056: step 173, loss 1.51698, acc 0.75, prec 0.0246356, recall 0.705882
2017-12-10T04:15:32.827351: step 174, loss 1.265, acc 0.703125, prec 0.0245399, recall 0.705882
2017-12-10T04:15:33.097692: step 175, loss 5.2817, acc 0.734375, prec 0.0244598, recall 0.701754
2017-12-10T04:15:33.363051: step 176, loss 0.952437, acc 0.796875, prec 0.0243952, recall 0.701754
2017-12-10T04:15:33.628327: step 177, loss 16.2255, acc 0.765625, prec 0.024326, recall 0.697674
2017-12-10T04:15:33.894166: step 178, loss 0.585398, acc 0.859375, prec 0.0242817, recall 0.697674
2017-12-10T04:15:34.157351: step 179, loss 14.4055, acc 0.78125, prec 0.024218, recall 0.693642
2017-12-10T04:15:34.419817: step 180, loss 9.02462, acc 0.75, prec 0.0241449, recall 0.689655
2017-12-10T04:15:34.681300: step 181, loss 16.8069, acc 0.671875, prec 0.024444, recall 0.685393
2017-12-10T04:15:34.946173: step 182, loss 2.2441, acc 0.6875, prec 0.0245411, recall 0.687151
2017-12-10T04:15:35.209592: step 183, loss 2.92153, acc 0.5, prec 0.0243854, recall 0.687151
2017-12-10T04:15:35.475585: step 184, loss 4.64499, acc 0.25, prec 0.0243471, recall 0.688889
2017-12-10T04:15:35.735532: step 185, loss 4.66636, acc 0.359375, prec 0.0247226, recall 0.693989
2017-12-10T04:15:36.006659: step 186, loss 5.25262, acc 0.375, prec 0.0245316, recall 0.693989
2017-12-10T04:15:36.267193: step 187, loss 5.5787, acc 0.21875, prec 0.0242969, recall 0.693989
2017-12-10T04:15:36.526184: step 188, loss 4.93757, acc 0.34375, prec 0.0241032, recall 0.693989
2017-12-10T04:15:36.788478: step 189, loss 5.51995, acc 0.21875, prec 0.0244269, recall 0.698925
2017-12-10T04:15:37.046850: step 190, loss 6.01278, acc 0.328125, prec 0.024413, recall 0.700535
2017-12-10T04:15:37.307028: step 191, loss 4.45521, acc 0.4375, prec 0.0244309, recall 0.702128
2017-12-10T04:15:37.563741: step 192, loss 14.0122, acc 0.421875, prec 0.0242692, recall 0.698413
2017-12-10T04:15:37.824490: step 193, loss 4.20919, acc 0.40625, prec 0.0241008, recall 0.698413
2017-12-10T04:15:38.087520: step 194, loss 2.24377, acc 0.625, prec 0.0243504, recall 0.701571
2017-12-10T04:15:38.353799: step 195, loss 3.03749, acc 0.59375, prec 0.0244123, recall 0.703125
2017-12-10T04:15:38.618732: step 196, loss 2.70284, acc 0.5625, prec 0.0244648, recall 0.704663
2017-12-10T04:15:38.884596: step 197, loss 2.26559, acc 0.59375, prec 0.0243509, recall 0.704663
2017-12-10T04:15:39.148305: step 198, loss 13.7601, acc 0.75, prec 0.0244599, recall 0.702564
2017-12-10T04:15:39.412665: step 199, loss 1.7763, acc 0.703125, prec 0.0245508, recall 0.704082
2017-12-10T04:15:39.680021: step 200, loss 0.517643, acc 0.828125, prec 0.0245028, recall 0.704082
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-200

2017-12-10T04:15:40.951276: step 201, loss 4.69622, acc 0.78125, prec 0.0247919, recall 0.703518
2017-12-10T04:15:41.214752: step 202, loss 1.49244, acc 0.65625, prec 0.0246957, recall 0.703518
2017-12-10T04:15:41.474758: step 203, loss 1.12721, acc 0.734375, prec 0.0249648, recall 0.706468
2017-12-10T04:15:41.737203: step 204, loss 3.26915, acc 0.78125, prec 0.0249079, recall 0.70297
2017-12-10T04:15:42.014637: step 205, loss 1.88775, acc 0.640625, prec 0.0253187, recall 0.707317
2017-12-10T04:15:42.279054: step 206, loss 2.32543, acc 0.625, prec 0.0253825, recall 0.708738
2017-12-10T04:15:42.542030: step 207, loss 1.99848, acc 0.6875, prec 0.0254634, recall 0.710145
2017-12-10T04:15:42.808091: step 208, loss 2.30858, acc 0.625, prec 0.0253579, recall 0.710145
2017-12-10T04:15:43.080450: step 209, loss 1.85908, acc 0.625, prec 0.0254208, recall 0.711538
2017-12-10T04:15:43.352301: step 210, loss 2.20788, acc 0.546875, prec 0.0252948, recall 0.711538
2017-12-10T04:15:43.612014: step 211, loss 4.5008, acc 0.578125, prec 0.0251829, recall 0.708134
2017-12-10T04:15:43.882571: step 212, loss 1.69371, acc 0.609375, prec 0.0250762, recall 0.708134
2017-12-10T04:15:44.140511: step 213, loss 1.57652, acc 0.703125, prec 0.0249958, recall 0.708134
2017-12-10T04:15:44.404095: step 214, loss 2.56542, acc 0.6875, prec 0.0250757, recall 0.709524
2017-12-10T04:15:44.671508: step 215, loss 1.42047, acc 0.65625, prec 0.0251467, recall 0.7109
2017-12-10T04:15:44.933120: step 216, loss 1.38915, acc 0.640625, prec 0.0253756, recall 0.713615
2017-12-10T04:15:45.195535: step 217, loss 6.11594, acc 0.6875, prec 0.0252954, recall 0.71028
2017-12-10T04:15:45.463496: step 218, loss 1.86249, acc 0.65625, prec 0.0253647, recall 0.711628
2017-12-10T04:15:45.726266: step 219, loss 8.26195, acc 0.53125, prec 0.0252434, recall 0.708333
2017-12-10T04:15:45.988809: step 220, loss 2.01358, acc 0.59375, prec 0.0251355, recall 0.708333
2017-12-10T04:15:46.247846: step 221, loss 2.68779, acc 0.609375, prec 0.0250327, recall 0.708333
2017-12-10T04:15:46.509456: step 222, loss 2.25784, acc 0.59375, prec 0.0250855, recall 0.709677
2017-12-10T04:15:46.769962: step 223, loss 2.27329, acc 0.609375, prec 0.0249838, recall 0.709677
2017-12-10T04:15:47.033618: step 224, loss 5.74911, acc 0.515625, prec 0.0250202, recall 0.707763
2017-12-10T04:15:47.299443: step 225, loss 1.96117, acc 0.625, prec 0.0252371, recall 0.710407
2017-12-10T04:15:47.562299: step 226, loss 1.37042, acc 0.703125, prec 0.0251603, recall 0.710407
2017-12-10T04:15:47.825439: step 227, loss 5.79019, acc 0.703125, prec 0.0252436, recall 0.70852
2017-12-10T04:15:48.091959: step 228, loss 2.53125, acc 0.53125, prec 0.0251232, recall 0.70852
2017-12-10T04:15:48.354771: step 229, loss 2.46337, acc 0.578125, prec 0.0250158, recall 0.70852
2017-12-10T04:15:48.618596: step 230, loss 3.19136, acc 0.5, prec 0.0251969, recall 0.711111
2017-12-10T04:15:48.880238: step 231, loss 2.223, acc 0.578125, prec 0.0250902, recall 0.711111
2017-12-10T04:15:49.144603: step 232, loss 1.27156, acc 0.75, prec 0.0253323, recall 0.713656
2017-12-10T04:15:49.407090: step 233, loss 1.52759, acc 0.640625, prec 0.0255452, recall 0.716157
2017-12-10T04:15:49.672496: step 234, loss 1.12301, acc 0.6875, prec 0.0254658, recall 0.716157
2017-12-10T04:15:49.938574: step 235, loss 11.7944, acc 0.6875, prec 0.0253949, recall 0.709957
2017-12-10T04:15:50.207795: step 236, loss 1.14249, acc 0.6875, prec 0.0254669, recall 0.711207
2017-12-10T04:15:51.180487: step 237, loss 1.19554, acc 0.765625, prec 0.0254081, recall 0.711207
2017-12-10T04:15:51.545575: step 238, loss 1.23668, acc 0.734375, prec 0.025641, recall 0.713675
2017-12-10T04:15:51.809871: step 239, loss 1.5455, acc 0.703125, prec 0.0257156, recall 0.714894
2017-12-10T04:15:52.267920: step 240, loss 1.118, acc 0.734375, prec 0.0256489, recall 0.714894
2017-12-10T04:15:52.966655: step 241, loss 1.39778, acc 0.671875, prec 0.0255669, recall 0.714894
2017-12-10T04:15:53.230181: step 242, loss 2.96319, acc 0.609375, prec 0.0257693, recall 0.714286
2017-12-10T04:15:53.499861: step 243, loss 0.895198, acc 0.796875, prec 0.0260133, recall 0.716667
2017-12-10T04:15:53.759832: step 244, loss 9.95533, acc 0.703125, prec 0.0259466, recall 0.710744
2017-12-10T04:15:54.027884: step 245, loss 1.40086, acc 0.671875, prec 0.0260111, recall 0.711934
2017-12-10T04:15:54.290522: step 246, loss 1.39538, acc 0.78125, prec 0.0262487, recall 0.714286
2017-12-10T04:15:54.554277: step 247, loss 3.77978, acc 0.640625, prec 0.0261624, recall 0.711382
2017-12-10T04:15:54.820994: step 248, loss 3.12027, acc 0.421875, prec 0.0261632, recall 0.712551
2017-12-10T04:15:55.082605: step 249, loss 2.76668, acc 0.578125, prec 0.0262028, recall 0.71371
2017-12-10T04:15:55.354993: step 250, loss 3.3688, acc 0.390625, prec 0.0260524, recall 0.71371
2017-12-10T04:15:55.618479: step 251, loss 3.04138, acc 0.46875, prec 0.0260653, recall 0.714859
2017-12-10T04:15:55.887548: step 252, loss 4.14917, acc 0.46875, prec 0.0260781, recall 0.716
2017-12-10T04:15:56.151047: step 253, loss 3.05813, acc 0.4375, prec 0.025942, recall 0.716
2017-12-10T04:15:56.418933: step 254, loss 3.28535, acc 0.390625, prec 0.0259366, recall 0.717131
2017-12-10T04:15:56.685338: step 255, loss 2.68394, acc 0.5625, prec 0.0261119, recall 0.719368
2017-12-10T04:15:56.960934: step 256, loss 4.13869, acc 0.46875, prec 0.0259889, recall 0.716535
2017-12-10T04:15:57.226871: step 257, loss 3.28746, acc 0.53125, prec 0.0262933, recall 0.719844
2017-12-10T04:15:57.496564: step 258, loss 16.4335, acc 0.578125, prec 0.0261965, recall 0.717054
2017-12-10T04:15:57.769310: step 259, loss 4.79521, acc 0.46875, prec 0.0260747, recall 0.714286
2017-12-10T04:15:58.036951: step 260, loss 5.5289, acc 0.59375, prec 0.0262567, recall 0.71374
2017-12-10T04:15:58.308834: step 261, loss 4.44606, acc 0.53125, prec 0.0261502, recall 0.711027
2017-12-10T04:15:58.580604: step 262, loss 2.86424, acc 0.515625, prec 0.0263085, recall 0.713208
2017-12-10T04:15:58.844675: step 263, loss 4.24875, acc 0.34375, prec 0.0261556, recall 0.713208
2017-12-10T04:15:59.104190: step 264, loss 4.07112, acc 0.40625, prec 0.0260187, recall 0.713208
2017-12-10T04:15:59.361237: step 265, loss 4.17178, acc 0.359375, prec 0.026006, recall 0.714286
2017-12-10T04:15:59.622952: step 266, loss 3.67407, acc 0.421875, prec 0.0260076, recall 0.715356
2017-12-10T04:15:59.895381: step 267, loss 6.59231, acc 0.421875, prec 0.0260127, recall 0.713755
2017-12-10T04:16:00.171225: step 268, loss 3.99628, acc 0.375, prec 0.0258725, recall 0.713755
2017-12-10T04:16:00.442564: step 269, loss 2.69156, acc 0.515625, prec 0.0257649, recall 0.713755
2017-12-10T04:16:00.715005: step 270, loss 2.87786, acc 0.4375, prec 0.0257711, recall 0.714815
2017-12-10T04:16:00.978640: step 271, loss 2.56987, acc 0.515625, prec 0.0256649, recall 0.714815
2017-12-10T04:16:01.244131: step 272, loss 2.50986, acc 0.578125, prec 0.0258312, recall 0.716912
2017-12-10T04:16:01.507374: step 273, loss 2.06621, acc 0.515625, prec 0.0258541, recall 0.717949
2017-12-10T04:16:01.766484: step 274, loss 1.21466, acc 0.6875, prec 0.0257861, recall 0.717949
2017-12-10T04:16:02.028956: step 275, loss 0.983181, acc 0.765625, prec 0.0257353, recall 0.717949
2017-12-10T04:16:02.290388: step 276, loss 10.1578, acc 0.765625, prec 0.0258158, recall 0.716364
2017-12-10T04:16:02.556088: step 277, loss 0.513532, acc 0.875, prec 0.0257887, recall 0.716364
2017-12-10T04:16:02.816650: step 278, loss 0.61345, acc 0.828125, prec 0.0257516, recall 0.716364
2017-12-10T04:16:03.078745: step 279, loss 0.234879, acc 0.921875, prec 0.0258621, recall 0.717391
2017-12-10T04:16:03.338970: step 280, loss 6.13335, acc 0.890625, prec 0.0258418, recall 0.714801
2017-12-10T04:16:03.613206: step 281, loss 0.437382, acc 0.90625, prec 0.0258216, recall 0.714801
2017-12-10T04:16:03.872844: step 282, loss 0.629537, acc 0.84375, prec 0.0259148, recall 0.715827
2017-12-10T04:16:04.136076: step 283, loss 5.69316, acc 0.796875, prec 0.0258744, recall 0.713262
2017-12-10T04:16:04.399143: step 284, loss 0.781618, acc 0.703125, prec 0.025937, recall 0.714286
2017-12-10T04:16:04.657741: step 285, loss 2.14917, acc 0.84375, prec 0.0259067, recall 0.711744
2017-12-10T04:16:04.928743: step 286, loss 0.0885367, acc 0.953125, prec 0.0258967, recall 0.711744
2017-12-10T04:16:05.187952: step 287, loss 1.045, acc 0.796875, prec 0.0261049, recall 0.713781
2017-12-10T04:16:05.449245: step 288, loss 1.10655, acc 0.765625, prec 0.0260544, recall 0.713781
2017-12-10T04:16:05.719013: step 289, loss 0.784785, acc 0.78125, prec 0.0261329, recall 0.714789
2017-12-10T04:16:05.981446: step 290, loss 0.674548, acc 0.796875, prec 0.0260892, recall 0.714789
2017-12-10T04:16:06.252031: step 291, loss 1.65546, acc 0.75, prec 0.0262854, recall 0.716783
2017-12-10T04:16:06.512316: step 292, loss 1.59667, acc 0.703125, prec 0.0262215, recall 0.716783
2017-12-10T04:16:06.775904: step 293, loss 0.92111, acc 0.78125, prec 0.0261747, recall 0.716783
2017-12-10T04:16:07.036930: step 294, loss 0.88235, acc 0.75, prec 0.0261213, recall 0.716783
2017-12-10T04:16:07.299839: step 295, loss 1.5239, acc 0.71875, prec 0.0261853, recall 0.71777
2017-12-10T04:16:07.560944: step 296, loss 1.08972, acc 0.765625, prec 0.0261355, recall 0.71777
2017-12-10T04:16:07.825308: step 297, loss 4.16902, acc 0.6875, prec 0.0260726, recall 0.715278
2017-12-10T04:16:08.094469: step 298, loss 0.998742, acc 0.6875, prec 0.0260068, recall 0.715278
2017-12-10T04:16:08.367098: step 299, loss 1.29017, acc 0.734375, prec 0.0259511, recall 0.715278
2017-12-10T04:16:08.630012: step 300, loss 1.02025, acc 0.78125, prec 0.0259054, recall 0.715278

Evaluation:
2017-12-10T04:16:16.262677: step 300, loss 1.92793, acc 0.818362, prec 0.0309133, recall 0.704128

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-300

2017-12-10T04:16:17.799047: step 301, loss 1.1141, acc 0.8125, prec 0.0309735, recall 0.704805
2017-12-10T04:16:18.065545: step 302, loss 14.0809, acc 0.703125, prec 0.0309175, recall 0.703196
2017-12-10T04:16:18.334416: step 303, loss 0.987558, acc 0.78125, prec 0.0309712, recall 0.703872
2017-12-10T04:16:18.599428: step 304, loss 9.88226, acc 0.765625, prec 0.0310248, recall 0.702948
2017-12-10T04:16:18.860559: step 305, loss 1.16425, acc 0.65625, prec 0.0309567, recall 0.702948
2017-12-10T04:16:19.120987: step 306, loss 21.0919, acc 0.671875, prec 0.0309915, recall 0.702032
2017-12-10T04:16:19.387891: step 307, loss 5.56009, acc 0.453125, prec 0.0308869, recall 0.70045
2017-12-10T04:16:19.649451: step 308, loss 3.6503, acc 0.375, prec 0.0308605, recall 0.701124
2017-12-10T04:16:19.906600: step 309, loss 5.66937, acc 0.21875, prec 0.0307087, recall 0.701124
2017-12-10T04:16:20.171114: step 310, loss 4.42528, acc 0.390625, prec 0.0305912, recall 0.701124
2017-12-10T04:16:20.439792: step 311, loss 5.08649, acc 0.28125, prec 0.0304539, recall 0.701124
2017-12-10T04:16:20.702623: step 312, loss 5.58744, acc 0.25, prec 0.0304061, recall 0.701794
2017-12-10T04:16:20.967394: step 313, loss 5.11108, acc 0.34375, prec 0.0304701, recall 0.703125
2017-12-10T04:16:21.221575: step 314, loss 4.95165, acc 0.25, prec 0.0304226, recall 0.703786
2017-12-10T04:16:21.483769: step 315, loss 2.90532, acc 0.46875, prec 0.0304164, recall 0.704444
2017-12-10T04:16:21.744329: step 316, loss 3.60969, acc 0.421875, prec 0.0303088, recall 0.704444
2017-12-10T04:16:22.006792: step 317, loss 2.23813, acc 0.609375, prec 0.0304215, recall 0.705752
2017-12-10T04:16:22.280422: step 318, loss 1.21864, acc 0.6875, prec 0.0303636, recall 0.705752
2017-12-10T04:16:22.544545: step 319, loss 1.87748, acc 0.65625, prec 0.0303002, recall 0.705752
2017-12-10T04:16:22.811335: step 320, loss 1.55982, acc 0.796875, prec 0.0303548, recall 0.706402
2017-12-10T04:16:23.077611: step 321, loss 1.00297, acc 0.78125, prec 0.0303145, recall 0.706402
2017-12-10T04:16:23.343839: step 322, loss 1.89422, acc 0.84375, prec 0.0302887, recall 0.704846
2017-12-10T04:16:23.610830: step 323, loss 0.30359, acc 0.9375, prec 0.0302772, recall 0.704846
2017-12-10T04:16:23.881355: step 324, loss 3.84124, acc 0.84375, prec 0.0302515, recall 0.703297
2017-12-10T04:16:24.145000: step 325, loss 0.392617, acc 0.875, prec 0.0302286, recall 0.703297
2017-12-10T04:16:24.409953: step 326, loss 0.124314, acc 0.953125, prec 0.03022, recall 0.703297
2017-12-10T04:16:24.671135: step 327, loss 0.794075, acc 0.8125, prec 0.0302773, recall 0.703947
2017-12-10T04:16:24.934096: step 328, loss 0.452092, acc 0.859375, prec 0.0302516, recall 0.703947
2017-12-10T04:16:25.195130: step 329, loss 17.2011, acc 0.890625, prec 0.0302345, recall 0.702407
2017-12-10T04:16:25.457925: step 330, loss 1.16894, acc 0.8125, prec 0.0302004, recall 0.702407
2017-12-10T04:16:25.718879: step 331, loss 3.24996, acc 0.859375, prec 0.0301777, recall 0.700873
2017-12-10T04:16:25.987701: step 332, loss 5.40133, acc 0.796875, prec 0.0302347, recall 0.7
2017-12-10T04:16:26.260190: step 333, loss 0.630127, acc 0.859375, prec 0.0303002, recall 0.700651
2017-12-10T04:16:26.520268: step 334, loss 1.32003, acc 0.75, prec 0.0302548, recall 0.700651
2017-12-10T04:16:26.784659: step 335, loss 22.3184, acc 0.6875, prec 0.0302945, recall 0.698276
2017-12-10T04:16:27.072037: step 336, loss 2.25521, acc 0.59375, prec 0.0303115, recall 0.698925
2017-12-10T04:16:27.331898: step 337, loss 3.53342, acc 0.515625, prec 0.0303143, recall 0.699571
2017-12-10T04:16:27.604643: step 338, loss 3.9285, acc 0.375, prec 0.030202, recall 0.699571
2017-12-10T04:16:27.866230: step 339, loss 5.58093, acc 0.3125, prec 0.0301688, recall 0.700214
2017-12-10T04:16:28.121948: step 340, loss 5.4687, acc 0.21875, prec 0.0302084, recall 0.701493
2017-12-10T04:16:28.384084: step 341, loss 5.49054, acc 0.265625, prec 0.0300786, recall 0.701493
2017-12-10T04:16:28.657546: step 342, loss 5.67457, acc 0.3125, prec 0.030223, recall 0.70339
2017-12-10T04:16:28.915322: step 343, loss 5.66812, acc 0.28125, prec 0.030097, recall 0.70339
2017-12-10T04:16:29.174559: step 344, loss 4.92397, acc 0.375, prec 0.0299883, recall 0.70339
2017-12-10T04:16:29.434155: step 345, loss 4.44509, acc 0.453125, prec 0.0298964, recall 0.701903
2017-12-10T04:16:29.693675: step 346, loss 4.80837, acc 0.359375, prec 0.0300475, recall 0.703781
2017-12-10T04:16:29.952751: step 347, loss 3.45825, acc 0.421875, prec 0.0301216, recall 0.705021
2017-12-10T04:16:30.223460: step 348, loss 2.4004, acc 0.546875, prec 0.0302166, recall 0.70625
2017-12-10T04:16:30.487264: step 349, loss 2.35652, acc 0.609375, prec 0.0301494, recall 0.70625
2017-12-10T04:16:30.755696: step 350, loss 1.90398, acc 0.5625, prec 0.0300745, recall 0.70625
2017-12-10T04:16:31.017783: step 351, loss 1.53363, acc 0.734375, prec 0.0300292, recall 0.70625
2017-12-10T04:16:31.279669: step 352, loss 0.75363, acc 0.8125, prec 0.0299973, recall 0.70625
2017-12-10T04:16:31.538146: step 353, loss 1.23238, acc 0.78125, prec 0.0301317, recall 0.707469
2017-12-10T04:16:31.811184: step 354, loss 8.88679, acc 0.78125, prec 0.0302683, recall 0.707217
2017-12-10T04:16:32.076951: step 355, loss 0.961499, acc 0.859375, prec 0.0303297, recall 0.707819
2017-12-10T04:16:32.337516: step 356, loss 7.33032, acc 0.875, prec 0.0303965, recall 0.706967
2017-12-10T04:16:32.614076: step 357, loss 1.29073, acc 0.890625, prec 0.0304631, recall 0.707566
2017-12-10T04:16:32.883461: step 358, loss 0.97693, acc 0.84375, prec 0.0304363, recall 0.707566
2017-12-10T04:16:33.151227: step 359, loss 0.626753, acc 0.8125, prec 0.0304042, recall 0.707566
2017-12-10T04:16:33.417103: step 360, loss 4.45129, acc 0.859375, prec 0.0303829, recall 0.706122
2017-12-10T04:16:33.689772: step 361, loss 1.67147, acc 0.703125, prec 0.0303323, recall 0.706122
2017-12-10T04:16:33.950029: step 362, loss 1.91246, acc 0.703125, prec 0.0303667, recall 0.706721
2017-12-10T04:16:34.212820: step 363, loss 2.1577, acc 0.640625, prec 0.030475, recall 0.707911
2017-12-10T04:16:34.470705: step 364, loss 2.50228, acc 0.59375, prec 0.0304905, recall 0.708502
2017-12-10T04:16:34.734449: step 365, loss 2.80523, acc 0.40625, prec 0.0303899, recall 0.708502
2017-12-10T04:16:34.992347: step 366, loss 2.33601, acc 0.5625, prec 0.0303162, recall 0.708502
2017-12-10T04:16:35.252089: step 367, loss 1.44384, acc 0.65625, prec 0.0302585, recall 0.708502
2017-12-10T04:16:35.508172: step 368, loss 1.79175, acc 0.6875, prec 0.03029, recall 0.709091
2017-12-10T04:16:35.767553: step 369, loss 2.17922, acc 0.59375, prec 0.0302221, recall 0.709091
2017-12-10T04:16:36.026931: step 370, loss 2.09135, acc 0.59375, prec 0.0301546, recall 0.709091
2017-12-10T04:16:36.290489: step 371, loss 1.06258, acc 0.796875, prec 0.030121, recall 0.709091
2017-12-10T04:16:36.553569: step 372, loss 0.799767, acc 0.75, prec 0.0300797, recall 0.709091
2017-12-10T04:16:36.818524: step 373, loss 1.83585, acc 0.75, prec 0.0301215, recall 0.709677
2017-12-10T04:16:37.085079: step 374, loss 11.2172, acc 0.8125, prec 0.0300932, recall 0.70825
2017-12-10T04:16:37.352349: step 375, loss 0.950939, acc 0.796875, prec 0.0300598, recall 0.70825
2017-12-10T04:16:37.615845: step 376, loss 0.546658, acc 0.84375, prec 0.0301996, recall 0.709419
2017-12-10T04:16:37.879917: step 377, loss 19.6383, acc 0.765625, prec 0.0302488, recall 0.707171
2017-12-10T04:16:38.143974: step 378, loss 0.620662, acc 0.828125, prec 0.0302205, recall 0.707171
2017-12-10T04:16:38.411620: step 379, loss 0.896053, acc 0.84375, prec 0.0301948, recall 0.707171
2017-12-10T04:16:38.680277: step 380, loss 1.63536, acc 0.671875, prec 0.0302233, recall 0.707753
2017-12-10T04:16:38.940112: step 381, loss 8.22064, acc 0.609375, prec 0.030244, recall 0.706931
2017-12-10T04:16:39.217567: step 382, loss 2.18434, acc 0.59375, prec 0.0302595, recall 0.70751
2017-12-10T04:16:39.474616: step 383, loss 2.9845, acc 0.484375, prec 0.0302571, recall 0.708087
2017-12-10T04:16:39.734768: step 384, loss 1.91545, acc 0.625, prec 0.0302775, recall 0.708661
2017-12-10T04:16:39.995509: step 385, loss 1.928, acc 0.65625, prec 0.030303, recall 0.709234
2017-12-10T04:16:40.266455: step 386, loss 2.26135, acc 0.59375, prec 0.030237, recall 0.709234
2017-12-10T04:16:40.524987: step 387, loss 2.8211, acc 0.5625, prec 0.0302473, recall 0.709804
2017-12-10T04:16:40.794611: step 388, loss 2.45729, acc 0.578125, prec 0.0302601, recall 0.710372
2017-12-10T04:16:41.062357: step 389, loss 1.56257, acc 0.625, prec 0.0302803, recall 0.710938
2017-12-10T04:16:41.325557: step 390, loss 1.64878, acc 0.625, prec 0.0303005, recall 0.711501
2017-12-10T04:16:41.586813: step 391, loss 1.70681, acc 0.625, prec 0.0302403, recall 0.711501
2017-12-10T04:16:41.855099: step 392, loss 7.1089, acc 0.671875, prec 0.0302704, recall 0.71068
2017-12-10T04:16:42.120010: step 393, loss 8.90102, acc 0.71875, prec 0.0302279, recall 0.709302
2017-12-10T04:16:42.382145: step 394, loss 2.11164, acc 0.640625, prec 0.0302506, recall 0.709865
2017-12-10T04:16:42.648957: step 395, loss 1.16733, acc 0.75, prec 0.0304502, recall 0.711538
2017-12-10T04:16:42.915900: step 396, loss 1.26288, acc 0.765625, prec 0.0304923, recall 0.712092
2017-12-10T04:16:43.185517: step 397, loss 16.938, acc 0.6875, prec 0.0305243, recall 0.711281
2017-12-10T04:16:43.456309: step 398, loss 2.23789, acc 0.625, prec 0.0305437, recall 0.711832
2017-12-10T04:16:43.716242: step 399, loss 2.21756, acc 0.546875, prec 0.0306297, recall 0.712928
2017-12-10T04:16:44.692206: step 400, loss 2.46835, acc 0.53125, prec 0.0306339, recall 0.713472
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-400

2017-12-10T04:16:46.858578: step 401, loss 2.59048, acc 0.484375, prec 0.0306305, recall 0.714015
2017-12-10T04:16:47.158732: step 402, loss 8.33391, acc 0.46875, prec 0.0305511, recall 0.711321
2017-12-10T04:16:47.449441: step 403, loss 3.16569, acc 0.4375, prec 0.0304622, recall 0.711321
2017-12-10T04:16:47.727365: step 404, loss 5.07943, acc 0.265625, prec 0.0303469, recall 0.711321
2017-12-10T04:16:48.002730: step 405, loss 5.21506, acc 0.25, prec 0.0303079, recall 0.711864
2017-12-10T04:16:48.274510: step 406, loss 3.92369, acc 0.34375, prec 0.0302062, recall 0.711864
2017-12-10T04:16:48.546885: step 407, loss 3.86085, acc 0.40625, prec 0.0302692, recall 0.712946
2017-12-10T04:16:48.819284: step 408, loss 2.91747, acc 0.453125, prec 0.0302621, recall 0.713483
2017-12-10T04:16:49.075073: step 409, loss 3.07767, acc 0.390625, prec 0.0302454, recall 0.714019
2017-12-10T04:16:49.336132: step 410, loss 2.74097, acc 0.5625, prec 0.0301785, recall 0.714019
2017-12-10T04:16:49.598221: step 411, loss 1.88945, acc 0.609375, prec 0.0301191, recall 0.714019
2017-12-10T04:16:49.860827: step 412, loss 1.43604, acc 0.625, prec 0.0300622, recall 0.714019
2017-12-10T04:16:50.121641: step 413, loss 2.36752, acc 0.65625, prec 0.0300126, recall 0.712687
2017-12-10T04:16:50.390617: step 414, loss 1.31833, acc 0.703125, prec 0.0300439, recall 0.713222
2017-12-10T04:16:50.661309: step 415, loss 1.07603, acc 0.734375, prec 0.0300799, recall 0.713755
2017-12-10T04:16:50.930192: step 416, loss 19.0733, acc 0.78125, prec 0.0301252, recall 0.712963
2017-12-10T04:16:51.190501: step 417, loss 32.1869, acc 0.796875, prec 0.0301751, recall 0.710866
2017-12-10T04:16:51.462628: step 418, loss 0.782265, acc 0.796875, prec 0.0301445, recall 0.710866
2017-12-10T04:16:51.731683: step 419, loss 1.34061, acc 0.65625, prec 0.0301684, recall 0.711397
2017-12-10T04:16:51.997128: step 420, loss 4.85991, acc 0.578125, prec 0.0301074, recall 0.710092
2017-12-10T04:16:52.262487: step 421, loss 1.39143, acc 0.640625, prec 0.0301289, recall 0.710623
2017-12-10T04:16:52.525299: step 422, loss 2.22571, acc 0.46875, prec 0.0301247, recall 0.711152
2017-12-10T04:16:52.793797: step 423, loss 3.26697, acc 0.359375, prec 0.0301042, recall 0.711679
2017-12-10T04:16:53.053902: step 424, loss 4.41323, acc 0.328125, prec 0.0301538, recall 0.712727
2017-12-10T04:16:53.319690: step 425, loss 2.78468, acc 0.421875, prec 0.0300683, recall 0.712727
2017-12-10T04:16:53.591245: step 426, loss 2.78337, acc 0.453125, prec 0.030062, recall 0.713249
2017-12-10T04:16:53.849227: step 427, loss 2.55985, acc 0.484375, prec 0.0300603, recall 0.713768
2017-12-10T04:16:54.110069: step 428, loss 2.59742, acc 0.453125, prec 0.030054, recall 0.714286
2017-12-10T04:16:54.373069: step 429, loss 6.19017, acc 0.59375, prec 0.029997, recall 0.712996
2017-12-10T04:16:54.640318: step 430, loss 2.43272, acc 0.4375, prec 0.0299152, recall 0.712996
2017-12-10T04:16:54.908834: step 431, loss 1.61152, acc 0.609375, prec 0.0298586, recall 0.712996
2017-12-10T04:16:55.183453: step 432, loss 1.54657, acc 0.65625, prec 0.0298823, recall 0.713513
2017-12-10T04:16:55.448443: step 433, loss 1.47735, acc 0.640625, prec 0.0299767, recall 0.714542
2017-12-10T04:16:55.708081: step 434, loss 2.08399, acc 0.625, prec 0.0300684, recall 0.715564
2017-12-10T04:16:55.968887: step 435, loss 1.33649, acc 0.65625, prec 0.0300188, recall 0.715564
2017-12-10T04:16:56.224924: step 436, loss 0.491451, acc 0.84375, prec 0.030069, recall 0.716071
2017-12-10T04:16:56.487133: step 437, loss 0.677789, acc 0.765625, prec 0.0300352, recall 0.716071
2017-12-10T04:16:56.749816: step 438, loss 1.5041, acc 0.796875, prec 0.0300786, recall 0.716578
2017-12-10T04:16:57.034315: step 439, loss 0.485154, acc 0.875, prec 0.0302056, recall 0.717584
2017-12-10T04:16:57.304640: step 440, loss 2.33115, acc 0.890625, prec 0.0302645, recall 0.716814
2017-12-10T04:16:57.570841: step 441, loss 0.546048, acc 0.828125, prec 0.0302397, recall 0.716814
2017-12-10T04:16:57.829089: step 442, loss 1.06652, acc 0.6875, prec 0.0301946, recall 0.716814
2017-12-10T04:16:58.094166: step 443, loss 0.584593, acc 0.84375, prec 0.0301721, recall 0.716814
2017-12-10T04:16:58.362338: step 444, loss 0.533286, acc 0.828125, prec 0.0301474, recall 0.716814
2017-12-10T04:16:58.623691: step 445, loss 0.747198, acc 0.828125, prec 0.0301227, recall 0.716814
2017-12-10T04:16:58.887926: step 446, loss 0.584691, acc 0.828125, prec 0.0301702, recall 0.717314
2017-12-10T04:16:59.154587: step 447, loss 0.55677, acc 0.828125, prec 0.0301455, recall 0.717314
2017-12-10T04:16:59.419656: step 448, loss 0.431002, acc 0.921875, prec 0.0302063, recall 0.717813
2017-12-10T04:16:59.691829: step 449, loss 1.68686, acc 0.921875, prec 0.0301974, recall 0.716549
2017-12-10T04:16:59.961154: step 450, loss 0.615336, acc 0.84375, prec 0.030175, recall 0.716549
2017-12-10T04:17:00.229493: step 451, loss 0.347769, acc 0.90625, prec 0.0301616, recall 0.716549
2017-12-10T04:17:00.495342: step 452, loss 8.48126, acc 0.875, prec 0.0301459, recall 0.71529
2017-12-10T04:17:00.760077: step 453, loss 0.712673, acc 0.828125, prec 0.0301214, recall 0.71529
2017-12-10T04:17:01.031534: step 454, loss 14.0586, acc 0.859375, prec 0.0301058, recall 0.712785
2017-12-10T04:17:01.300910: step 455, loss 0.562955, acc 0.8125, prec 0.0300791, recall 0.712785
2017-12-10T04:17:01.566090: step 456, loss 27.1671, acc 0.734375, prec 0.0301151, recall 0.712042
2017-12-10T04:17:01.834732: step 457, loss 2.18866, acc 0.609375, prec 0.0302026, recall 0.713043
2017-12-10T04:17:02.096265: step 458, loss 2.6788, acc 0.46875, prec 0.0301984, recall 0.713542
2017-12-10T04:17:02.351529: step 459, loss 4.69834, acc 0.328125, prec 0.0301033, recall 0.713542
2017-12-10T04:17:02.610885: step 460, loss 3.39794, acc 0.34375, prec 0.0300818, recall 0.714038
2017-12-10T04:17:02.873788: step 461, loss 4.86971, acc 0.21875, prec 0.0299724, recall 0.714038
2017-12-10T04:17:03.132991: step 462, loss 5.36495, acc 0.234375, prec 0.0301471, recall 0.716007
2017-12-10T04:17:03.395064: step 463, loss 4.63176, acc 0.328125, prec 0.0301235, recall 0.716495
2017-12-10T04:17:03.668108: step 464, loss 4.85526, acc 0.25, prec 0.0300194, recall 0.716495
2017-12-10T04:17:03.927859: step 465, loss 3.86459, acc 0.3125, prec 0.0299246, recall 0.716495
2017-12-10T04:17:04.187596: step 466, loss 3.30149, acc 0.390625, prec 0.0300494, recall 0.717949
2017-12-10T04:17:04.452836: step 467, loss 5.81656, acc 0.390625, prec 0.0301063, recall 0.717687
2017-12-10T04:17:04.714921: step 468, loss 2.94001, acc 0.375, prec 0.0300896, recall 0.718166
2017-12-10T04:17:04.981300: step 469, loss 1.76593, acc 0.5625, prec 0.0300298, recall 0.718166
2017-12-10T04:17:05.247739: step 470, loss 1.26457, acc 0.59375, prec 0.0300432, recall 0.718644
2017-12-10T04:17:05.509980: step 471, loss 10.0982, acc 0.703125, prec 0.030005, recall 0.717428
2017-12-10T04:17:05.773311: step 472, loss 1.33131, acc 0.734375, prec 0.0300375, recall 0.717905
2017-12-10T04:17:06.039411: step 473, loss 0.973204, acc 0.765625, prec 0.0300741, recall 0.718381
2017-12-10T04:17:06.299952: step 474, loss 2.34223, acc 0.703125, prec 0.030036, recall 0.717172
2017-12-10T04:17:06.568358: step 475, loss 1.11192, acc 0.703125, prec 0.0299958, recall 0.717172
2017-12-10T04:17:06.827938: step 476, loss 0.722468, acc 0.765625, prec 0.0300324, recall 0.717647
2017-12-10T04:17:07.089524: step 477, loss 3.76906, acc 0.828125, prec 0.0300112, recall 0.716443
2017-12-10T04:17:07.350285: step 478, loss 3.97324, acc 0.796875, prec 0.0300541, recall 0.715719
2017-12-10T04:17:07.625951: step 479, loss 0.819579, acc 0.796875, prec 0.0302308, recall 0.717138
2017-12-10T04:17:07.889983: step 480, loss 0.986351, acc 0.734375, prec 0.0301948, recall 0.717138
2017-12-10T04:17:08.157308: step 481, loss 4.24405, acc 0.671875, prec 0.0301525, recall 0.715947
2017-12-10T04:17:08.420486: step 482, loss 1.51441, acc 0.671875, prec 0.0301083, recall 0.715947
2017-12-10T04:17:08.684523: step 483, loss 1.60992, acc 0.640625, prec 0.03006, recall 0.715947
2017-12-10T04:17:08.943583: step 484, loss 1.58881, acc 0.65625, prec 0.0300139, recall 0.715947
2017-12-10T04:17:09.202514: step 485, loss 2.53361, acc 0.46875, prec 0.0300778, recall 0.716887
2017-12-10T04:17:09.457928: step 486, loss 1.45682, acc 0.578125, prec 0.0300215, recall 0.716887
2017-12-10T04:17:09.721685: step 487, loss 1.0081, acc 0.765625, prec 0.0300575, recall 0.717355
2017-12-10T04:17:09.985359: step 488, loss 1.46411, acc 0.75, prec 0.0302255, recall 0.71875
2017-12-10T04:17:10.245465: step 489, loss 1.00217, acc 0.734375, prec 0.030324, recall 0.719672
2017-12-10T04:17:10.506587: step 490, loss 1.25469, acc 0.65625, prec 0.030278, recall 0.719672
2017-12-10T04:17:10.766230: step 491, loss 4.36699, acc 0.828125, prec 0.0303239, recall 0.718954
2017-12-10T04:17:11.039473: step 492, loss 0.884579, acc 0.78125, prec 0.0302947, recall 0.718954
2017-12-10T04:17:11.305793: step 493, loss 1.5709, acc 0.734375, prec 0.030326, recall 0.719413
2017-12-10T04:17:11.578590: step 494, loss 1.95448, acc 0.65625, prec 0.0303467, recall 0.71987
2017-12-10T04:17:11.849896: step 495, loss 1.20416, acc 0.734375, prec 0.0303778, recall 0.720325
2017-12-10T04:17:12.114228: step 496, loss 2.00986, acc 0.671875, prec 0.0304005, recall 0.720779
2017-12-10T04:17:12.344429: step 497, loss 2.66769, acc 0.673077, prec 0.0304336, recall 0.720065
2017-12-10T04:17:12.612751: step 498, loss 1.21129, acc 0.734375, prec 0.0303983, recall 0.720065
2017-12-10T04:17:12.873317: step 499, loss 1.61688, acc 0.625, prec 0.0303485, recall 0.720065
2017-12-10T04:17:13.139166: step 500, loss 1.61816, acc 0.671875, prec 0.0304372, recall 0.720968
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-500

2017-12-10T04:17:14.481336: step 501, loss 1.31049, acc 0.65625, prec 0.0303916, recall 0.720968
2017-12-10T04:17:14.743278: step 502, loss 3.94548, acc 0.671875, prec 0.0303504, recall 0.719807
2017-12-10T04:17:15.005958: step 503, loss 1.59648, acc 0.578125, prec 0.0303605, recall 0.720257
2017-12-10T04:17:15.271415: step 504, loss 1.38941, acc 0.6875, prec 0.0304507, recall 0.721154
2017-12-10T04:17:15.537301: step 505, loss 4.8637, acc 0.640625, prec 0.0304054, recall 0.72
2017-12-10T04:17:15.804671: step 506, loss 1.58205, acc 0.671875, prec 0.0303623, recall 0.72
2017-12-10T04:17:16.073099: step 507, loss 6.70765, acc 0.59375, prec 0.0304418, recall 0.719745
2017-12-10T04:17:16.342638: step 508, loss 2.05235, acc 0.578125, prec 0.0304517, recall 0.720191
2017-12-10T04:17:16.603721: step 509, loss 3.21064, acc 0.484375, prec 0.0305144, recall 0.721078
2017-12-10T04:17:16.861153: step 510, loss 2.30762, acc 0.546875, prec 0.0305849, recall 0.721959
2017-12-10T04:17:17.115407: step 511, loss 2.3308, acc 0.59375, prec 0.0305318, recall 0.721959
2017-12-10T04:17:17.377980: step 512, loss 2.86604, acc 0.5, prec 0.0305313, recall 0.722397
2017-12-10T04:17:17.637755: step 513, loss 2.33222, acc 0.546875, prec 0.0306014, recall 0.72327
2017-12-10T04:17:17.899937: step 514, loss 1.95432, acc 0.609375, prec 0.030615, recall 0.723705
2017-12-10T04:17:18.156208: step 515, loss 1.37784, acc 0.640625, prec 0.0306325, recall 0.724138
2017-12-10T04:17:18.422458: step 516, loss 1.28124, acc 0.71875, prec 0.0306602, recall 0.72457
2017-12-10T04:17:18.692751: step 517, loss 1.54041, acc 0.703125, prec 0.0307499, recall 0.725429
2017-12-10T04:17:18.955522: step 518, loss 0.786258, acc 0.78125, prec 0.0307215, recall 0.725429
2017-12-10T04:17:19.214081: step 519, loss 0.285343, acc 0.90625, prec 0.0307093, recall 0.725429
2017-12-10T04:17:19.489519: step 520, loss 0.337243, acc 0.890625, prec 0.0307591, recall 0.725857
2017-12-10T04:17:19.754736: step 521, loss 12.5057, acc 0.8125, prec 0.0309285, recall 0.726006
2017-12-10T04:17:20.025303: step 522, loss 5.60902, acc 0.921875, prec 0.0309842, recall 0.725309
2017-12-10T04:17:20.305467: step 523, loss 5.23125, acc 0.796875, prec 0.0310874, recall 0.725038
2017-12-10T04:17:20.574708: step 524, loss 0.95357, acc 0.75, prec 0.0311184, recall 0.72546
2017-12-10T04:17:20.841245: step 525, loss 0.930437, acc 0.765625, prec 0.0312787, recall 0.726718
2017-12-10T04:17:21.106220: step 526, loss 2.18486, acc 0.5, prec 0.0312131, recall 0.726718
2017-12-10T04:17:21.372129: step 527, loss 2.32528, acc 0.453125, prec 0.0311416, recall 0.726718
2017-12-10T04:17:21.639712: step 528, loss 2.484, acc 0.453125, prec 0.0310705, recall 0.726718
2017-12-10T04:17:21.902740: step 529, loss 2.45297, acc 0.46875, prec 0.0310017, recall 0.726718
2017-12-10T04:17:22.165535: step 530, loss 2.48962, acc 0.546875, prec 0.0310692, recall 0.727549
2017-12-10T04:17:22.429452: step 531, loss 3.15275, acc 0.484375, prec 0.0310656, recall 0.727964
2017-12-10T04:17:22.693005: step 532, loss 2.84962, acc 0.328125, prec 0.0309792, recall 0.727964
2017-12-10T04:17:22.957541: step 533, loss 1.68242, acc 0.609375, prec 0.0309917, recall 0.728376
2017-12-10T04:17:23.218777: step 534, loss 2.30255, acc 0.53125, prec 0.0309943, recall 0.728788
2017-12-10T04:17:23.482401: step 535, loss 2.2058, acc 0.578125, prec 0.0310028, recall 0.729198
2017-12-10T04:17:23.745917: step 536, loss 1.58676, acc 0.59375, prec 0.030951, recall 0.729198
2017-12-10T04:17:24.016154: step 537, loss 1.08151, acc 0.765625, prec 0.0309212, recall 0.729198
2017-12-10T04:17:24.285768: step 538, loss 0.759224, acc 0.84375, prec 0.0309635, recall 0.729607
2017-12-10T04:17:24.551818: step 539, loss 0.571876, acc 0.84375, prec 0.0310058, recall 0.730015
2017-12-10T04:17:24.817073: step 540, loss 0.410676, acc 0.9375, prec 0.0310599, recall 0.730422
2017-12-10T04:17:25.080545: step 541, loss 5.24115, acc 0.84375, prec 0.031104, recall 0.72973
2017-12-10T04:17:25.350710: step 542, loss 0.822893, acc 0.828125, prec 0.0311441, recall 0.730135
2017-12-10T04:17:25.611669: step 543, loss 0.371005, acc 0.875, prec 0.031252, recall 0.730942
2017-12-10T04:17:25.868643: step 544, loss 0.791149, acc 0.765625, prec 0.0312839, recall 0.731343
2017-12-10T04:17:26.129746: step 545, loss 0.380073, acc 0.890625, prec 0.0312699, recall 0.731343
2017-12-10T04:17:26.404377: step 546, loss 4.01036, acc 0.875, prec 0.031256, recall 0.730253
2017-12-10T04:17:26.669830: step 547, loss 0.472709, acc 0.859375, prec 0.031238, recall 0.730253
2017-12-10T04:17:26.943372: step 548, loss 7.02451, acc 0.796875, prec 0.0312142, recall 0.729167
2017-12-10T04:17:27.208270: step 549, loss 7.34816, acc 0.75, prec 0.031246, recall 0.728487
2017-12-10T04:17:27.478897: step 550, loss 1.34783, acc 0.6875, prec 0.0313294, recall 0.72929
2017-12-10T04:17:27.746458: step 551, loss 1.20652, acc 0.609375, prec 0.0313412, recall 0.72969
2017-12-10T04:17:28.005859: step 552, loss 1.15518, acc 0.703125, prec 0.0313648, recall 0.730088
2017-12-10T04:17:28.272016: step 553, loss 1.83459, acc 0.5625, prec 0.0313706, recall 0.730486
2017-12-10T04:17:28.534521: step 554, loss 1.85627, acc 0.578125, prec 0.0313171, recall 0.730486
2017-12-10T04:17:28.798695: step 555, loss 2.38417, acc 0.546875, prec 0.0312598, recall 0.730486
2017-12-10T04:17:29.056779: step 556, loss 1.9473, acc 0.546875, prec 0.0312638, recall 0.730882
2017-12-10T04:17:29.317774: step 557, loss 1.88669, acc 0.609375, prec 0.0312755, recall 0.731278
2017-12-10T04:17:29.576793: step 558, loss 2.25922, acc 0.53125, prec 0.0312167, recall 0.731278
2017-12-10T04:17:29.841325: step 559, loss 1.23465, acc 0.625, prec 0.0312911, recall 0.732064
2017-12-10T04:17:30.104241: step 560, loss 0.953855, acc 0.671875, prec 0.0313105, recall 0.732456
2017-12-10T04:17:30.379256: step 561, loss 1.60258, acc 0.6875, prec 0.0313924, recall 0.733236
2017-12-10T04:17:30.650426: step 562, loss 1.37278, acc 0.640625, prec 0.0313474, recall 0.733236
2017-12-10T04:17:30.919543: step 563, loss 0.871157, acc 0.734375, prec 0.0313142, recall 0.733236
2017-12-10T04:17:31.176644: step 564, loss 7.17714, acc 0.765625, prec 0.0313472, recall 0.732558
2017-12-10T04:17:31.440824: step 565, loss 0.5727, acc 0.75, prec 0.031316, recall 0.732558
2017-12-10T04:17:31.699971: step 566, loss 0.397543, acc 0.890625, prec 0.0313024, recall 0.732558
2017-12-10T04:17:31.964310: step 567, loss 0.587174, acc 0.796875, prec 0.0312771, recall 0.732558
2017-12-10T04:17:32.227944: step 568, loss 0.82929, acc 0.84375, prec 0.0313178, recall 0.732946
2017-12-10T04:17:32.492715: step 569, loss 0.343169, acc 0.875, prec 0.0313623, recall 0.733333
2017-12-10T04:17:32.757922: step 570, loss 0.631528, acc 0.859375, prec 0.0313449, recall 0.733333
2017-12-10T04:17:33.020540: step 571, loss 0.543631, acc 0.84375, prec 0.0313854, recall 0.733719
2017-12-10T04:17:33.282082: step 572, loss 0.57465, acc 0.859375, prec 0.0314279, recall 0.734104
2017-12-10T04:17:33.544591: step 573, loss 0.617706, acc 0.890625, prec 0.0314742, recall 0.734488
2017-12-10T04:17:33.806942: step 574, loss 0.19296, acc 0.953125, prec 0.0314683, recall 0.734488
2017-12-10T04:17:34.080399: step 575, loss 0.182124, acc 0.9375, prec 0.0315204, recall 0.73487
2017-12-10T04:17:34.342279: step 576, loss 3.55851, acc 0.90625, prec 0.0315107, recall 0.733813
2017-12-10T04:17:34.610013: step 577, loss 0.229052, acc 0.90625, prec 0.031499, recall 0.733813
2017-12-10T04:17:34.875960: step 578, loss 15.4968, acc 0.90625, prec 0.0316686, recall 0.733906
2017-12-10T04:17:35.147010: step 579, loss 0.632888, acc 0.84375, prec 0.0317088, recall 0.734286
2017-12-10T04:17:35.416094: step 580, loss 0.553945, acc 0.890625, prec 0.0317548, recall 0.734665
2017-12-10T04:17:35.675603: step 581, loss 0.779033, acc 0.78125, prec 0.0317275, recall 0.734665
2017-12-10T04:17:35.932089: step 582, loss 0.392254, acc 0.875, prec 0.0317118, recall 0.734665
2017-12-10T04:17:36.192581: step 583, loss 1.43663, acc 0.671875, prec 0.0316709, recall 0.734665
2017-12-10T04:17:36.454134: step 584, loss 0.883058, acc 0.765625, prec 0.0316417, recall 0.734665
2017-12-10T04:17:36.712546: step 585, loss 0.505867, acc 0.875, prec 0.0316261, recall 0.734665
2017-12-10T04:17:36.978104: step 586, loss 1.03899, acc 0.734375, prec 0.0316526, recall 0.735043
2017-12-10T04:17:37.239727: step 587, loss 1.04489, acc 0.734375, prec 0.0316789, recall 0.73542
2017-12-10T04:17:37.500618: step 588, loss 1.27171, acc 0.71875, prec 0.031644, recall 0.73542
2017-12-10T04:17:37.766289: step 589, loss 0.673613, acc 0.828125, prec 0.0317412, recall 0.73617
2017-12-10T04:17:38.029335: step 590, loss 0.52185, acc 0.84375, prec 0.031781, recall 0.736544
2017-12-10T04:17:38.295599: step 591, loss 2.73268, acc 0.8125, prec 0.0318779, recall 0.736248
2017-12-10T04:17:38.561817: step 592, loss 1.6156, acc 0.828125, prec 0.0319746, recall 0.73699
2017-12-10T04:17:38.826451: step 593, loss 0.631524, acc 0.84375, prec 0.0320141, recall 0.73736
2017-12-10T04:17:39.084755: step 594, loss 1.21205, acc 0.8125, prec 0.0321087, recall 0.738095
2017-12-10T04:17:39.350715: step 595, loss 0.904244, acc 0.890625, prec 0.0322129, recall 0.738827
2017-12-10T04:17:39.622508: step 596, loss 0.566674, acc 0.796875, prec 0.0322463, recall 0.739191
2017-12-10T04:17:39.890497: step 597, loss 0.797504, acc 0.78125, prec 0.0322188, recall 0.739191
2017-12-10T04:17:40.159795: step 598, loss 0.864242, acc 0.78125, prec 0.0321914, recall 0.739191
2017-12-10T04:17:40.423560: step 599, loss 11.7731, acc 0.671875, prec 0.0321524, recall 0.738162
2017-12-10T04:17:40.687734: step 600, loss 8.61709, acc 0.75, prec 0.0321251, recall 0.736111

Evaluation:
2017-12-10T04:17:48.286240: step 600, loss 1.39766, acc 0.662955, prec 0.0329801, recall 0.767281

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-600

2017-12-10T04:17:49.726794: step 601, loss 1.46282, acc 0.6875, prec 0.0329475, recall 0.767281
2017-12-10T04:17:49.989292: step 602, loss 1.91239, acc 0.5625, prec 0.0329974, recall 0.767816
2017-12-10T04:17:50.250572: step 603, loss 2.00702, acc 0.578125, prec 0.0330489, recall 0.768349
2017-12-10T04:17:50.513448: step 604, loss 2.89077, acc 0.390625, prec 0.0329854, recall 0.768349
2017-12-10T04:17:50.777697: step 605, loss 2.25122, acc 0.546875, prec 0.0329384, recall 0.768349
2017-12-10T04:17:51.038060: step 606, loss 2.06892, acc 0.5625, prec 0.0329881, recall 0.768879
2017-12-10T04:17:51.307071: step 607, loss 2.19843, acc 0.484375, prec 0.0330295, recall 0.769406
2017-12-10T04:17:51.568131: step 608, loss 2.531, acc 0.5625, prec 0.0329842, recall 0.769406
2017-12-10T04:17:51.825438: step 609, loss 2.20494, acc 0.5, prec 0.0330272, recall 0.769932
2017-12-10T04:17:52.095240: step 610, loss 1.9102, acc 0.65625, prec 0.0329917, recall 0.769932
2017-12-10T04:17:52.352229: step 611, loss 2.54399, acc 0.59375, prec 0.0329986, recall 0.769318
2017-12-10T04:17:52.621541: step 612, loss 1.36204, acc 0.703125, prec 0.0330152, recall 0.76958
2017-12-10T04:17:52.878963: step 613, loss 0.604987, acc 0.8125, prec 0.033043, recall 0.769841
2017-12-10T04:17:53.143442: step 614, loss 0.701319, acc 0.765625, prec 0.0330189, recall 0.769841
2017-12-10T04:17:53.402103: step 615, loss 0.735411, acc 0.796875, prec 0.033045, recall 0.770102
2017-12-10T04:17:53.663631: step 616, loss 0.691792, acc 0.75, prec 0.0330193, recall 0.770102
2017-12-10T04:17:53.927066: step 617, loss 0.319937, acc 0.875, prec 0.0331004, recall 0.770621
2017-12-10T04:17:54.189933: step 618, loss 0.647004, acc 0.875, prec 0.0332282, recall 0.771396
2017-12-10T04:17:54.452878: step 619, loss 0.351826, acc 0.9375, prec 0.0332687, recall 0.771654
2017-12-10T04:17:54.712714: step 620, loss 0.424892, acc 0.90625, prec 0.033259, recall 0.771654
2017-12-10T04:17:54.974698: step 621, loss 9.2776, acc 0.921875, prec 0.0333479, recall 0.770437
2017-12-10T04:17:55.242497: step 622, loss 0.348943, acc 0.84375, prec 0.0333317, recall 0.770437
2017-12-10T04:17:55.505082: step 623, loss 0.99372, acc 0.921875, prec 0.0333705, recall 0.770694
2017-12-10T04:17:55.771628: step 624, loss 3.08207, acc 0.796875, prec 0.0334447, recall 0.770346
2017-12-10T04:17:56.044955: step 625, loss 0.610443, acc 0.828125, prec 0.0334736, recall 0.770601
2017-12-10T04:17:56.311235: step 626, loss 1.39754, acc 0.734375, prec 0.0334928, recall 0.770856
2017-12-10T04:17:56.576013: step 627, loss 1.2197, acc 0.640625, prec 0.0334556, recall 0.770856
2017-12-10T04:17:56.841796: step 628, loss 1.2397, acc 0.65625, prec 0.0334201, recall 0.770856
2017-12-10T04:17:57.109053: step 629, loss 2.19347, acc 0.5625, prec 0.0333751, recall 0.770856
2017-12-10T04:17:57.368795: step 630, loss 4.30454, acc 0.734375, prec 0.0334889, recall 0.770764
2017-12-10T04:17:57.633326: step 631, loss 2.22551, acc 0.546875, prec 0.0335351, recall 0.771271
2017-12-10T04:17:57.901577: step 632, loss 1.77197, acc 0.625, prec 0.0334965, recall 0.771271
2017-12-10T04:17:58.169094: step 633, loss 2.9256, acc 0.53125, prec 0.0334483, recall 0.771271
2017-12-10T04:17:58.435278: step 634, loss 2.41301, acc 0.59375, prec 0.033453, recall 0.771523
2017-12-10T04:17:58.692861: step 635, loss 2.04811, acc 0.5, prec 0.0334018, recall 0.771523
2017-12-10T04:17:58.949144: step 636, loss 2.63012, acc 0.5625, prec 0.0333572, recall 0.771523
2017-12-10T04:17:59.217653: step 637, loss 11.8024, acc 0.515625, prec 0.0333095, recall 0.770673
2017-12-10T04:17:59.482403: step 638, loss 2.07565, acc 0.5625, prec 0.0333111, recall 0.770925
2017-12-10T04:17:59.740438: step 639, loss 2.03417, acc 0.59375, prec 0.0333618, recall 0.771429
2017-12-10T04:18:00.007757: step 640, loss 2.71236, acc 0.53125, prec 0.0333602, recall 0.771679
2017-12-10T04:18:00.277787: step 641, loss 2.2544, acc 0.546875, prec 0.0333144, recall 0.771679
2017-12-10T04:18:00.545189: step 642, loss 2.0723, acc 0.59375, prec 0.0333191, recall 0.77193
2017-12-10T04:18:00.804479: step 643, loss 1.37379, acc 0.65625, prec 0.0332845, recall 0.77193
2017-12-10T04:18:01.065388: step 644, loss 1.63054, acc 0.671875, prec 0.0332971, recall 0.77218
2017-12-10T04:18:01.330412: step 645, loss 1.73851, acc 0.671875, prec 0.0333554, recall 0.772678
2017-12-10T04:18:01.597531: step 646, loss 0.883125, acc 0.765625, prec 0.0333318, recall 0.772678
2017-12-10T04:18:01.865454: step 647, loss 0.585717, acc 0.796875, prec 0.0333569, recall 0.772926
2017-12-10T04:18:02.129981: step 648, loss 7.33738, acc 0.875, prec 0.0333914, recall 0.772331
2017-12-10T04:18:02.397168: step 649, loss 9.08376, acc 0.828125, prec 0.0333757, recall 0.771491
2017-12-10T04:18:02.657416: step 650, loss 7.73238, acc 0.875, prec 0.0335466, recall 0.771645
2017-12-10T04:18:02.924264: step 651, loss 22.2815, acc 0.703125, prec 0.0335198, recall 0.769978
2017-12-10T04:18:03.197276: step 652, loss 1.5761, acc 0.671875, prec 0.0334868, recall 0.769978
2017-12-10T04:18:03.468508: step 653, loss 1.14444, acc 0.640625, prec 0.0334506, recall 0.769978
2017-12-10T04:18:03.732823: step 654, loss 3.86982, acc 0.359375, prec 0.0334317, recall 0.770227
2017-12-10T04:18:03.991770: step 655, loss 3.89255, acc 0.390625, prec 0.0334159, recall 0.770474
2017-12-10T04:18:04.254945: step 656, loss 4.71852, acc 0.21875, prec 0.033338, recall 0.770474
2017-12-10T04:18:04.512068: step 657, loss 3.8959, acc 0.296875, prec 0.0332682, recall 0.770474
2017-12-10T04:18:04.775032: step 658, loss 5.19858, acc 0.1875, prec 0.0331879, recall 0.770474
2017-12-10T04:18:05.033043: step 659, loss 5.16108, acc 0.234375, prec 0.0331126, recall 0.770474
2017-12-10T04:18:05.294306: step 660, loss 4.24066, acc 0.359375, prec 0.0330945, recall 0.770721
2017-12-10T04:18:05.557944: step 661, loss 4.73488, acc 0.328125, prec 0.0330289, recall 0.770721
2017-12-10T04:18:05.821597: step 662, loss 3.34378, acc 0.453125, prec 0.0330202, recall 0.770968
2017-12-10T04:18:06.078857: step 663, loss 1.96329, acc 0.484375, prec 0.0329701, recall 0.770968
2017-12-10T04:18:06.338782: step 664, loss 2.44027, acc 0.484375, prec 0.0329201, recall 0.770968
2017-12-10T04:18:06.598595: step 665, loss 1.94953, acc 0.546875, prec 0.0328763, recall 0.770968
2017-12-10T04:18:06.858116: step 666, loss 1.71924, acc 0.703125, prec 0.0328477, recall 0.770968
2017-12-10T04:18:07.123998: step 667, loss 1.25158, acc 0.703125, prec 0.0328192, recall 0.770968
2017-12-10T04:18:07.384511: step 668, loss 0.829064, acc 0.765625, prec 0.0327966, recall 0.770968
2017-12-10T04:18:07.644959: step 669, loss 0.567113, acc 0.84375, prec 0.0328701, recall 0.771459
2017-12-10T04:18:07.904759: step 670, loss 0.615454, acc 0.828125, prec 0.0328977, recall 0.771704
2017-12-10T04:18:08.167460: step 671, loss 1.10419, acc 0.9375, prec 0.0329359, recall 0.771949
2017-12-10T04:18:08.431869: step 672, loss 1.47843, acc 0.9375, prec 0.0329756, recall 0.771368
2017-12-10T04:18:08.697775: step 673, loss 0.375673, acc 0.921875, prec 0.032968, recall 0.771368
2017-12-10T04:18:08.959024: step 674, loss 11.6963, acc 0.9375, prec 0.0330077, recall 0.770789
2017-12-10T04:18:09.227794: step 675, loss 0.285329, acc 0.953125, prec 0.0331356, recall 0.77152
2017-12-10T04:18:09.498303: step 676, loss 1.16348, acc 0.875, prec 0.0332117, recall 0.772004
2017-12-10T04:18:09.767430: step 677, loss 6.70199, acc 0.796875, prec 0.0332391, recall 0.770613
2017-12-10T04:18:10.040334: step 678, loss 0.975936, acc 0.8125, prec 0.0332209, recall 0.770613
2017-12-10T04:18:10.306883: step 679, loss 13.312, acc 0.71875, prec 0.0332392, recall 0.770042
2017-12-10T04:18:10.569694: step 680, loss 2.2892, acc 0.59375, prec 0.0331999, recall 0.770042
2017-12-10T04:18:10.836690: step 681, loss 1.97432, acc 0.578125, prec 0.0331592, recall 0.770042
2017-12-10T04:18:11.106844: step 682, loss 1.50061, acc 0.625, prec 0.033167, recall 0.770285
2017-12-10T04:18:11.367216: step 683, loss 2.45173, acc 0.53125, prec 0.0331657, recall 0.770526
2017-12-10T04:18:11.628863: step 684, loss 2.90307, acc 0.515625, prec 0.0331192, recall 0.770526
2017-12-10T04:18:11.895171: step 685, loss 2.72395, acc 0.5, prec 0.0330713, recall 0.770526
2017-12-10T04:18:12.159539: step 686, loss 3.15545, acc 0.46875, prec 0.0331514, recall 0.771249
2017-12-10T04:18:12.420601: step 687, loss 3.77051, acc 0.40625, prec 0.0331382, recall 0.771488
2017-12-10T04:18:12.682118: step 688, loss 2.21667, acc 0.546875, prec 0.0331385, recall 0.771728
2017-12-10T04:18:12.955376: step 689, loss 1.85877, acc 0.640625, prec 0.0331043, recall 0.771728
2017-12-10T04:18:13.228505: step 690, loss 2.15453, acc 0.59375, prec 0.0330656, recall 0.771728
2017-12-10T04:18:13.497644: step 691, loss 1.51145, acc 0.640625, prec 0.0331182, recall 0.772205
2017-12-10T04:18:13.764800: step 692, loss 2.1735, acc 0.578125, prec 0.0330782, recall 0.772205
2017-12-10T04:18:14.039360: step 693, loss 1.41742, acc 0.734375, prec 0.0331395, recall 0.77268
2017-12-10T04:18:14.303313: step 694, loss 0.909698, acc 0.765625, prec 0.0332901, recall 0.773624
2017-12-10T04:18:14.571439: step 695, loss 0.897689, acc 0.796875, prec 0.0332708, recall 0.773624
2017-12-10T04:18:14.835752: step 696, loss 1.06104, acc 0.84375, prec 0.0333423, recall 0.774093
2017-12-10T04:18:15.102130: step 697, loss 7.93795, acc 0.890625, prec 0.0333765, recall 0.773526
2017-12-10T04:18:15.375614: step 698, loss 0.309689, acc 0.890625, prec 0.0334523, recall 0.773994
2017-12-10T04:18:15.639358: step 699, loss 3.65863, acc 0.8125, prec 0.033479, recall 0.773429
2017-12-10T04:18:15.907529: step 700, loss 0.377913, acc 0.84375, prec 0.033464, recall 0.773429
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-700

2017-12-10T04:18:17.219803: step 701, loss 0.675943, acc 0.84375, prec 0.0335352, recall 0.773895
2017-12-10T04:18:17.485176: step 702, loss 7.81484, acc 0.828125, prec 0.0335633, recall 0.773333
2017-12-10T04:18:17.756810: step 703, loss 0.892718, acc 0.796875, prec 0.0335439, recall 0.773333
2017-12-10T04:18:18.028563: step 704, loss 2.94572, acc 0.796875, prec 0.033569, recall 0.772774
2017-12-10T04:18:18.294472: step 705, loss 1.13868, acc 0.734375, prec 0.0335436, recall 0.772774
2017-12-10T04:18:18.563380: step 706, loss 1.43987, acc 0.78125, prec 0.0335228, recall 0.772774
2017-12-10T04:18:18.824958: step 707, loss 1.61131, acc 0.640625, prec 0.0334886, recall 0.772774
2017-12-10T04:18:19.089517: step 708, loss 1.75197, acc 0.671875, prec 0.0334574, recall 0.772774
2017-12-10T04:18:19.355865: step 709, loss 1.48256, acc 0.671875, prec 0.0334263, recall 0.772774
2017-12-10T04:18:19.616905: step 710, loss 1.05955, acc 0.6875, prec 0.0334395, recall 0.773006
2017-12-10T04:18:19.881220: step 711, loss 1.46061, acc 0.703125, prec 0.0334114, recall 0.773006
2017-12-10T04:18:20.150792: step 712, loss 1.43523, acc 0.671875, prec 0.0333804, recall 0.773006
2017-12-10T04:18:20.418413: step 713, loss 1.06979, acc 0.734375, prec 0.033398, recall 0.773238
2017-12-10T04:18:20.680864: step 714, loss 12.4516, acc 0.640625, prec 0.0333657, recall 0.772449
2017-12-10T04:18:20.949599: step 715, loss 1.06289, acc 0.765625, prec 0.0333436, recall 0.772449
2017-12-10T04:18:21.216239: step 716, loss 1.10594, acc 0.6875, prec 0.0333143, recall 0.772449
2017-12-10T04:18:21.483820: step 717, loss 1.24097, acc 0.765625, prec 0.0333348, recall 0.772681
2017-12-10T04:18:21.747671: step 718, loss 1.15332, acc 0.75, prec 0.0333963, recall 0.773143
2017-12-10T04:18:22.014629: step 719, loss 1.10233, acc 0.71875, prec 0.0334548, recall 0.773604
2017-12-10T04:18:22.279649: step 720, loss 5.58984, acc 0.796875, prec 0.0334796, recall 0.77305
2017-12-10T04:18:22.555186: step 721, loss 0.90639, acc 0.796875, prec 0.0334605, recall 0.77305
2017-12-10T04:18:22.818671: step 722, loss 1.31693, acc 0.71875, prec 0.0334341, recall 0.77305
2017-12-10T04:18:23.079235: step 723, loss 1.16409, acc 0.71875, prec 0.0334078, recall 0.77305
2017-12-10T04:18:23.346092: step 724, loss 1.27072, acc 0.640625, prec 0.0333742, recall 0.77305
2017-12-10T04:18:23.614377: step 725, loss 4.93941, acc 0.8125, prec 0.0333581, recall 0.772267
2017-12-10T04:18:23.877262: step 726, loss 0.597174, acc 0.84375, prec 0.033428, recall 0.772727
2017-12-10T04:18:24.143413: step 727, loss 1.05115, acc 0.71875, prec 0.0334017, recall 0.772727
2017-12-10T04:18:24.402258: step 728, loss 0.849046, acc 0.734375, prec 0.033377, recall 0.772727
2017-12-10T04:18:24.664195: step 729, loss 1.35239, acc 0.703125, prec 0.0333493, recall 0.772727
2017-12-10T04:18:24.923687: step 730, loss 0.72178, acc 0.734375, prec 0.0333246, recall 0.772727
2017-12-10T04:18:25.185697: step 731, loss 1.34015, acc 0.6875, prec 0.0333377, recall 0.772957
2017-12-10T04:18:25.445019: step 732, loss 0.778411, acc 0.75, prec 0.0333145, recall 0.772957
2017-12-10T04:18:25.708658: step 733, loss 0.702626, acc 0.84375, prec 0.0333, recall 0.772957
2017-12-10T04:18:25.978752: step 734, loss 0.641845, acc 0.8125, prec 0.0332826, recall 0.772957
2017-12-10T04:18:26.241330: step 735, loss 0.488044, acc 0.875, prec 0.0332711, recall 0.772957
2017-12-10T04:18:26.510017: step 736, loss 7.63434, acc 0.84375, prec 0.0333, recall 0.772407
2017-12-10T04:18:26.778660: step 737, loss 1.28148, acc 0.875, prec 0.0333724, recall 0.772864
2017-12-10T04:18:27.051422: step 738, loss 0.452673, acc 0.875, prec 0.0333608, recall 0.772864
2017-12-10T04:18:27.312616: step 739, loss 2.10216, acc 0.734375, prec 0.0333377, recall 0.772088
2017-12-10T04:18:27.587088: step 740, loss 4.31944, acc 0.84375, prec 0.0333666, recall 0.771543
2017-12-10T04:18:27.850425: step 741, loss 0.848446, acc 0.8125, prec 0.0333911, recall 0.771772
2017-12-10T04:18:28.115735: step 742, loss 0.95401, acc 0.78125, prec 0.0333708, recall 0.771772
2017-12-10T04:18:28.378251: step 743, loss 1.08191, acc 0.71875, prec 0.0334285, recall 0.772228
2017-12-10T04:18:28.637857: step 744, loss 1.62414, acc 0.65625, prec 0.0334385, recall 0.772455
2017-12-10T04:18:28.903344: step 745, loss 1.46382, acc 0.71875, prec 0.0334542, recall 0.772682
2017-12-10T04:18:29.172504: step 746, loss 1.49422, acc 0.71875, prec 0.0334282, recall 0.772682
2017-12-10T04:18:29.434696: step 747, loss 1.74866, acc 0.640625, prec 0.0334784, recall 0.773134
2017-12-10T04:18:29.695659: step 748, loss 1.36006, acc 0.6875, prec 0.0334912, recall 0.77336
2017-12-10T04:18:29.963466: step 749, loss 1.35954, acc 0.6875, prec 0.0335039, recall 0.773585
2017-12-10T04:18:30.233407: step 750, loss 1.67205, acc 0.78125, prec 0.0335669, recall 0.774034
2017-12-10T04:18:30.503705: step 751, loss 1.89612, acc 0.65625, prec 0.0335766, recall 0.774257
2017-12-10T04:18:30.778048: step 752, loss 1.64534, acc 0.71875, prec 0.0335507, recall 0.774257
2017-12-10T04:18:31.042458: step 753, loss 1.45404, acc 0.71875, prec 0.0335248, recall 0.774257
2017-12-10T04:18:31.307515: step 754, loss 1.07593, acc 0.765625, prec 0.0335033, recall 0.774257
2017-12-10T04:18:31.575124: step 755, loss 1.15726, acc 0.78125, prec 0.0334832, recall 0.774257
2017-12-10T04:18:31.839002: step 756, loss 0.61417, acc 0.890625, prec 0.0335145, recall 0.774481
2017-12-10T04:18:32.102587: step 757, loss 0.542368, acc 0.859375, prec 0.0335843, recall 0.774926
2017-12-10T04:18:32.376963: step 758, loss 0.685088, acc 0.796875, prec 0.0335657, recall 0.774926
2017-12-10T04:18:32.642992: step 759, loss 0.618954, acc 0.875, prec 0.0336781, recall 0.775591
2017-12-10T04:18:32.905756: step 760, loss 0.708928, acc 0.765625, prec 0.0336565, recall 0.775591
2017-12-10T04:18:33.168229: step 761, loss 0.260997, acc 0.890625, prec 0.0336465, recall 0.775591
2017-12-10T04:18:33.432177: step 762, loss 0.317394, acc 0.859375, prec 0.0336335, recall 0.775591
2017-12-10T04:18:33.696417: step 763, loss 3.82869, acc 0.859375, prec 0.0336633, recall 0.775049
2017-12-10T04:18:33.966813: step 764, loss 0.471809, acc 0.890625, prec 0.0336944, recall 0.77527
2017-12-10T04:18:34.232365: step 765, loss 7.41008, acc 0.859375, prec 0.0337242, recall 0.774731
2017-12-10T04:18:34.501658: step 766, loss 0.433714, acc 0.890625, prec 0.0337553, recall 0.774951
2017-12-10T04:18:34.763713: step 767, loss 10.9078, acc 0.859375, prec 0.0337438, recall 0.774194
2017-12-10T04:18:35.032001: step 768, loss 0.706895, acc 0.78125, prec 0.0338471, recall 0.774854
2017-12-10T04:18:35.297712: step 769, loss 0.472039, acc 0.875, prec 0.0338355, recall 0.774854
2017-12-10T04:18:35.572224: step 770, loss 1.18948, acc 0.6875, prec 0.0338068, recall 0.774854
2017-12-10T04:18:35.840656: step 771, loss 1.40959, acc 0.65625, prec 0.0337752, recall 0.774854
2017-12-10T04:18:36.105222: step 772, loss 5.79936, acc 0.65625, prec 0.0337861, recall 0.774319
2017-12-10T04:18:36.378653: step 773, loss 1.91262, acc 0.65625, prec 0.0337546, recall 0.774319
2017-12-10T04:18:36.645470: step 774, loss 2.17501, acc 0.59375, prec 0.0337583, recall 0.774538
2017-12-10T04:18:36.910966: step 775, loss 2.70686, acc 0.484375, prec 0.0337521, recall 0.774757
2017-12-10T04:18:37.178614: step 776, loss 2.95102, acc 0.484375, prec 0.0338682, recall 0.775629
2017-12-10T04:18:37.445313: step 777, loss 3.0426, acc 0.53125, prec 0.0338661, recall 0.775845
2017-12-10T04:18:37.707105: step 778, loss 1.18853, acc 0.734375, prec 0.0338826, recall 0.776062
2017-12-10T04:18:37.974631: step 779, loss 1.79977, acc 0.65625, prec 0.0339732, recall 0.776708
2017-12-10T04:18:38.237714: step 780, loss 1.54485, acc 0.6875, prec 0.0339446, recall 0.776708
2017-12-10T04:18:38.500252: step 781, loss 1.58784, acc 0.5625, prec 0.0339047, recall 0.776708
2017-12-10T04:18:38.760026: step 782, loss 1.85018, acc 0.59375, prec 0.0339083, recall 0.776923
2017-12-10T04:18:39.029667: step 783, loss 1.16229, acc 0.65625, prec 0.033877, recall 0.776923
2017-12-10T04:18:39.293725: step 784, loss 0.978842, acc 0.734375, prec 0.0338529, recall 0.776923
2017-12-10T04:18:39.563649: step 785, loss 0.75839, acc 0.828125, prec 0.0338373, recall 0.776923
2017-12-10T04:18:39.824559: step 786, loss 0.423835, acc 0.90625, prec 0.0338288, recall 0.776923
2017-12-10T04:18:40.085040: step 787, loss 0.364039, acc 0.859375, prec 0.0338969, recall 0.777351
2017-12-10T04:18:40.347376: step 788, loss 0.859305, acc 0.765625, prec 0.0338756, recall 0.777351
2017-12-10T04:18:40.611984: step 789, loss 0.630168, acc 0.84375, prec 0.0339018, recall 0.777565
2017-12-10T04:18:40.875526: step 790, loss 0.661031, acc 0.84375, prec 0.0338877, recall 0.777565
2017-12-10T04:18:41.143199: step 791, loss 3.30251, acc 0.859375, prec 0.0339167, recall 0.777034
2017-12-10T04:18:41.409368: step 792, loss 0.294328, acc 0.890625, prec 0.0339068, recall 0.777034
2017-12-10T04:18:41.675809: step 793, loss 0.863529, acc 0.9375, prec 0.0339415, recall 0.777247
2017-12-10T04:18:41.957212: step 794, loss 0.319306, acc 0.859375, prec 0.0339287, recall 0.777247
2017-12-10T04:18:42.217343: step 795, loss 0.195087, acc 0.921875, prec 0.0339216, recall 0.777247
2017-12-10T04:18:42.478973: step 796, loss 0.304914, acc 0.890625, prec 0.033952, recall 0.777459
2017-12-10T04:18:42.740000: step 797, loss 0.0957599, acc 0.953125, prec 0.0339478, recall 0.777459
2017-12-10T04:18:43.004799: step 798, loss 3.22946, acc 0.9375, prec 0.0339435, recall 0.776718
2017-12-10T04:18:43.268867: step 799, loss 6.04865, acc 0.890625, prec 0.033935, recall 0.775977
2017-12-10T04:18:43.545401: step 800, loss 0.370569, acc 0.875, prec 0.0339237, recall 0.775977
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-800

2017-12-10T04:18:44.867946: step 801, loss 0.664739, acc 0.8125, prec 0.0339068, recall 0.775977
2017-12-10T04:18:45.130863: step 802, loss 0.249823, acc 0.9375, prec 0.0339414, recall 0.77619
2017-12-10T04:18:45.392277: step 803, loss 0.312345, acc 0.9375, prec 0.0339357, recall 0.77619
2017-12-10T04:18:45.663404: step 804, loss 2.54246, acc 0.8125, prec 0.0340006, recall 0.775878
2017-12-10T04:18:45.934987: step 805, loss 0.787439, acc 0.84375, prec 0.0340668, recall 0.776303
2017-12-10T04:18:46.208236: step 806, loss 0.949266, acc 0.765625, prec 0.0340456, recall 0.776303
2017-12-10T04:18:46.470981: step 807, loss 0.802565, acc 0.84375, prec 0.0341117, recall 0.776727
2017-12-10T04:18:46.733872: step 808, loss 1.24382, acc 0.671875, prec 0.0341621, recall 0.777148
2017-12-10T04:18:46.995617: step 809, loss 11.5394, acc 0.765625, prec 0.0341824, recall 0.776626
2017-12-10T04:18:47.262528: step 810, loss 1.56792, acc 0.703125, prec 0.0341554, recall 0.776626
2017-12-10T04:18:47.529341: step 811, loss 1.69861, acc 0.625, prec 0.0341215, recall 0.776626
2017-12-10T04:18:47.792903: step 812, loss 1.0168, acc 0.75, prec 0.0340989, recall 0.776626
2017-12-10T04:18:48.055564: step 813, loss 1.89955, acc 0.640625, prec 0.0340665, recall 0.776626
2017-12-10T04:18:48.323569: step 814, loss 2.49706, acc 0.5, prec 0.0340215, recall 0.776626
2017-12-10T04:18:48.585879: step 815, loss 1.23434, acc 0.671875, prec 0.033992, recall 0.776626
2017-12-10T04:18:48.848392: step 816, loss 4.15459, acc 0.5625, prec 0.0339542, recall 0.775895
2017-12-10T04:18:49.115416: step 817, loss 1.38897, acc 0.671875, prec 0.0340044, recall 0.776316
2017-12-10T04:18:49.376438: step 818, loss 0.892243, acc 0.75, prec 0.0340615, recall 0.776735
2017-12-10T04:18:49.640656: step 819, loss 1.3979, acc 0.71875, prec 0.034076, recall 0.776945
2017-12-10T04:18:49.900023: step 820, loss 7.43787, acc 0.515625, prec 0.034034, recall 0.776217
2017-12-10T04:18:50.163180: step 821, loss 5.13975, acc 0.65625, prec 0.0340047, recall 0.775491
2017-12-10T04:18:50.430542: step 822, loss 1.15173, acc 0.671875, prec 0.0339754, recall 0.775491
2017-12-10T04:18:50.690119: step 823, loss 1.51596, acc 0.65625, prec 0.0339448, recall 0.775491
2017-12-10T04:18:50.952753: step 824, loss 1.07177, acc 0.734375, prec 0.0340002, recall 0.77591
2017-12-10T04:18:51.221413: step 825, loss 1.37529, acc 0.71875, prec 0.0340147, recall 0.776119
2017-12-10T04:18:51.481874: step 826, loss 1.53909, acc 0.625, prec 0.0340208, recall 0.776328
2017-12-10T04:18:51.744909: step 827, loss 1.26558, acc 0.734375, prec 0.0340761, recall 0.776744
2017-12-10T04:18:52.004948: step 828, loss 3.0818, acc 0.71875, prec 0.0340918, recall 0.77623
2017-12-10T04:18:52.270475: step 829, loss 1.2257, acc 0.765625, prec 0.0341104, recall 0.776438
2017-12-10T04:18:52.538411: step 830, loss 0.81077, acc 0.78125, prec 0.0341302, recall 0.776645
2017-12-10T04:18:52.801424: step 831, loss 1.06574, acc 0.703125, prec 0.0341039, recall 0.776645
2017-12-10T04:18:53.069869: step 832, loss 1.27324, acc 0.765625, prec 0.0341223, recall 0.776852
2017-12-10T04:18:53.333285: step 833, loss 1.0897, acc 0.75, prec 0.0341394, recall 0.777058
2017-12-10T04:18:53.601180: step 834, loss 0.986479, acc 0.765625, prec 0.0342363, recall 0.777675
2017-12-10T04:18:53.865927: step 835, loss 0.775279, acc 0.8125, prec 0.0342196, recall 0.777675
2017-12-10T04:18:54.129581: step 836, loss 0.836434, acc 0.765625, prec 0.0343163, recall 0.778289
2017-12-10T04:18:54.391951: step 837, loss 2.61689, acc 0.890625, prec 0.0343471, recall 0.777778
2017-12-10T04:18:54.657305: step 838, loss 0.805068, acc 0.78125, prec 0.0343276, recall 0.777778
2017-12-10T04:18:54.922677: step 839, loss 0.85191, acc 0.796875, prec 0.0343487, recall 0.777982
2017-12-10T04:18:55.192638: step 840, loss 4.00918, acc 0.8125, prec 0.0343334, recall 0.777269
2017-12-10T04:18:55.455007: step 841, loss 0.771639, acc 0.75, prec 0.0343111, recall 0.777269
2017-12-10T04:18:55.716098: step 842, loss 1.08633, acc 0.75, prec 0.034289, recall 0.777269
2017-12-10T04:18:55.975185: step 843, loss 0.481444, acc 0.90625, prec 0.0343197, recall 0.777473
2017-12-10T04:18:56.241168: step 844, loss 0.15512, acc 0.921875, prec 0.0343518, recall 0.777676
2017-12-10T04:18:56.504085: step 845, loss 0.745748, acc 0.6875, prec 0.034324, recall 0.777676
2017-12-10T04:18:56.776607: step 846, loss 5.49338, acc 0.734375, prec 0.0343408, recall 0.777169
2017-12-10T04:18:57.051455: step 847, loss 1.89281, acc 0.6875, prec 0.0343521, recall 0.777372
2017-12-10T04:18:57.318711: step 848, loss 1.27985, acc 0.640625, prec 0.0343202, recall 0.777372
2017-12-10T04:18:57.591119: step 849, loss 0.999054, acc 0.75, prec 0.0343759, recall 0.777778
2017-12-10T04:18:57.852290: step 850, loss 0.86548, acc 0.734375, prec 0.0343524, recall 0.777778
2017-12-10T04:18:58.113661: step 851, loss 1.06967, acc 0.703125, prec 0.0343649, recall 0.77798
2017-12-10T04:18:58.382571: step 852, loss 0.535543, acc 0.859375, prec 0.0343525, recall 0.77798
2017-12-10T04:18:58.648804: step 853, loss 0.795013, acc 0.828125, prec 0.0343373, recall 0.77798
2017-12-10T04:18:58.912754: step 854, loss 1.32815, acc 0.625, prec 0.0343043, recall 0.77798
2017-12-10T04:18:59.173381: step 855, loss 0.887365, acc 0.78125, prec 0.034285, recall 0.77798
2017-12-10T04:18:59.434393: step 856, loss 0.898645, acc 0.796875, prec 0.0342672, recall 0.77798
2017-12-10T04:18:59.701506: step 857, loss 0.855497, acc 0.78125, prec 0.0342479, recall 0.77798
2017-12-10T04:18:59.962089: step 858, loss 1.06109, acc 0.796875, prec 0.0342688, recall 0.778182
2017-12-10T04:19:00.240087: step 859, loss 0.548578, acc 0.8125, prec 0.0342523, recall 0.778182
2017-12-10T04:19:00.504313: step 860, loss 0.681358, acc 0.890625, prec 0.0342427, recall 0.778182
2017-12-10T04:19:00.772031: step 861, loss 0.311955, acc 0.875, prec 0.0342318, recall 0.778182
2017-12-10T04:19:01.035456: step 862, loss 0.297151, acc 0.90625, prec 0.0342236, recall 0.778182
2017-12-10T04:19:01.295178: step 863, loss 0.474558, acc 0.875, prec 0.0342126, recall 0.778182
2017-12-10T04:19:01.555375: step 864, loss 0.405492, acc 0.875, prec 0.0342017, recall 0.778182
2017-12-10T04:19:01.816892: step 865, loss 0.381203, acc 0.890625, prec 0.0342307, recall 0.778383
2017-12-10T04:19:02.081170: step 866, loss 4.02683, acc 0.90625, prec 0.0342239, recall 0.777677
2017-12-10T04:19:02.343981: step 867, loss 9.16221, acc 0.96875, prec 0.0342611, recall 0.777174
2017-12-10T04:19:02.610352: step 868, loss 0.290747, acc 0.921875, prec 0.0342928, recall 0.777376
2017-12-10T04:19:02.876540: step 869, loss 10.6267, acc 0.859375, prec 0.0343617, recall 0.775676
2017-12-10T04:19:03.145459: step 870, loss 0.399803, acc 0.875, prec 0.0343507, recall 0.775676
2017-12-10T04:19:03.409449: step 871, loss 1.30085, acc 0.703125, prec 0.0343632, recall 0.775878
2017-12-10T04:19:03.674185: step 872, loss 3.19051, acc 0.671875, prec 0.0343743, recall 0.775382
2017-12-10T04:19:03.951538: step 873, loss 2.22917, acc 0.515625, prec 0.0343319, recall 0.775382
2017-12-10T04:19:04.219415: step 874, loss 3.10975, acc 0.390625, prec 0.0342787, recall 0.775382
2017-12-10T04:19:04.490023: step 875, loss 3.2192, acc 0.34375, prec 0.0342216, recall 0.775382
2017-12-10T04:19:04.748489: step 876, loss 4.47055, acc 0.234375, prec 0.0341552, recall 0.775382
2017-12-10T04:19:05.017717: step 877, loss 3.5773, acc 0.328125, prec 0.0340972, recall 0.775382
2017-12-10T04:19:05.277957: step 878, loss 2.62379, acc 0.421875, prec 0.0341236, recall 0.775785
2017-12-10T04:19:05.537537: step 879, loss 3.33661, acc 0.421875, prec 0.0340739, recall 0.775785
2017-12-10T04:19:05.805664: step 880, loss 4.00489, acc 0.3125, prec 0.0340529, recall 0.775986
2017-12-10T04:19:06.070083: step 881, loss 3.46431, acc 0.375, prec 0.0340374, recall 0.776186
2017-12-10T04:19:06.333887: step 882, loss 3.04283, acc 0.40625, prec 0.0340245, recall 0.776386
2017-12-10T04:19:06.599085: step 883, loss 2.81383, acc 0.453125, prec 0.0340535, recall 0.776786
2017-12-10T04:19:06.861079: step 884, loss 1.88737, acc 0.5625, prec 0.0340918, recall 0.777184
2017-12-10T04:19:07.125815: step 885, loss 1.48631, acc 0.65625, prec 0.034138, recall 0.77758
2017-12-10T04:19:07.385988: step 886, loss 1.1082, acc 0.703125, prec 0.0341126, recall 0.77758
2017-12-10T04:19:07.651780: step 887, loss 0.933266, acc 0.734375, prec 0.03409, recall 0.77758
2017-12-10T04:19:07.916906: step 888, loss 0.525303, acc 0.796875, prec 0.0340727, recall 0.77758
2017-12-10T04:19:08.180279: step 889, loss 0.567058, acc 0.859375, prec 0.0340984, recall 0.777778
2017-12-10T04:19:08.447697: step 890, loss 4.78477, acc 0.84375, prec 0.0341241, recall 0.777285
2017-12-10T04:19:08.714669: step 891, loss 0.314904, acc 0.875, prec 0.0341511, recall 0.777482
2017-12-10T04:19:08.978343: step 892, loss 8.55146, acc 0.90625, prec 0.034182, recall 0.776991
2017-12-10T04:19:09.244349: step 893, loss 0.307978, acc 0.921875, prec 0.0341754, recall 0.776991
2017-12-10T04:19:09.508619: step 894, loss 1.08256, acc 0.8125, prec 0.0342346, recall 0.777385
2017-12-10T04:19:09.769430: step 895, loss 1.70128, acc 0.859375, prec 0.0342239, recall 0.776699
2017-12-10T04:19:10.038604: step 896, loss 2.14856, acc 0.796875, prec 0.034208, recall 0.776014
2017-12-10T04:19:10.302752: step 897, loss 0.515988, acc 0.828125, prec 0.0341933, recall 0.776014
2017-12-10T04:19:10.567619: step 898, loss 0.951215, acc 0.859375, prec 0.0342564, recall 0.776408
2017-12-10T04:19:10.830413: step 899, loss 1.8952, acc 0.78125, prec 0.0342391, recall 0.775726
2017-12-10T04:19:11.091991: step 900, loss 1.79519, acc 0.6875, prec 0.03425, recall 0.775923

Evaluation:
2017-12-10T04:19:18.640775: step 900, loss 1.43169, acc 0.763068, prec 0.0353695, recall 0.780715

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-900

2017-12-10T04:19:20.706372: step 901, loss 1.09996, acc 0.71875, prec 0.0353811, recall 0.780886
2017-12-10T04:19:20.969273: step 902, loss 1.40642, acc 0.6875, prec 0.0353562, recall 0.780886
2017-12-10T04:19:21.239563: step 903, loss 2.00301, acc 0.59375, prec 0.0353239, recall 0.780886
2017-12-10T04:19:21.500598: step 904, loss 1.52031, acc 0.625, prec 0.035328, recall 0.781056
2017-12-10T04:19:21.766078: step 905, loss 3.03515, acc 0.65625, prec 0.0353358, recall 0.78062
2017-12-10T04:19:22.035022: step 906, loss 1.49266, acc 0.65625, prec 0.0353424, recall 0.78079
2017-12-10T04:19:22.297079: step 907, loss 0.948646, acc 0.671875, prec 0.0353164, recall 0.78079
2017-12-10T04:19:22.555931: step 908, loss 1.65897, acc 0.671875, prec 0.0352904, recall 0.78079
2017-12-10T04:19:22.822309: step 909, loss 1.65147, acc 0.59375, prec 0.0352921, recall 0.78096
2017-12-10T04:19:23.087038: step 910, loss 1.10983, acc 0.6875, prec 0.0352674, recall 0.78096
2017-12-10T04:19:23.351188: step 911, loss 1.156, acc 0.71875, prec 0.0352789, recall 0.781129
2017-12-10T04:19:23.620408: step 912, loss 0.837979, acc 0.734375, prec 0.035258, recall 0.781129
2017-12-10T04:19:23.881952: step 913, loss 3.55852, acc 0.640625, prec 0.0352309, recall 0.780526
2017-12-10T04:19:24.157655: step 914, loss 0.873368, acc 0.796875, prec 0.0352822, recall 0.780864
2017-12-10T04:19:24.421099: step 915, loss 0.969948, acc 0.71875, prec 0.0352937, recall 0.781033
2017-12-10T04:19:24.690492: step 916, loss 0.836969, acc 0.765625, prec 0.0352753, recall 0.781033
2017-12-10T04:19:24.956251: step 917, loss 0.432568, acc 0.859375, prec 0.0352642, recall 0.781033
2017-12-10T04:19:25.221167: step 918, loss 7.33537, acc 0.765625, prec 0.0352806, recall 0.7806
2017-12-10T04:19:25.486898: step 919, loss 0.699504, acc 0.78125, prec 0.0352634, recall 0.7806
2017-12-10T04:19:25.749574: step 920, loss 0.599467, acc 0.890625, prec 0.0352884, recall 0.780769
2017-12-10T04:19:26.012275: step 921, loss 0.842768, acc 0.765625, prec 0.0353035, recall 0.780938
2017-12-10T04:19:26.278376: step 922, loss 0.737256, acc 0.75, prec 0.0353174, recall 0.781106
2017-12-10T04:19:26.541884: step 923, loss 0.464088, acc 0.90625, prec 0.035377, recall 0.781442
2017-12-10T04:19:26.818508: step 924, loss 0.95262, acc 0.796875, prec 0.0353611, recall 0.781442
2017-12-10T04:19:27.086981: step 925, loss 0.656935, acc 0.84375, prec 0.0353823, recall 0.781609
2017-12-10T04:19:27.352591: step 926, loss 0.482066, acc 0.859375, prec 0.0353712, recall 0.781609
2017-12-10T04:19:27.616308: step 927, loss 1.87471, acc 0.8125, prec 0.0353577, recall 0.781011
2017-12-10T04:19:27.881097: step 928, loss 0.452917, acc 0.84375, prec 0.0353455, recall 0.781011
2017-12-10T04:19:28.145272: step 929, loss 0.316126, acc 0.921875, prec 0.0354062, recall 0.781346
2017-12-10T04:19:28.415966: step 930, loss 0.636235, acc 0.8125, prec 0.0354249, recall 0.781513
2017-12-10T04:19:28.680847: step 931, loss 0.6431, acc 0.828125, prec 0.0354114, recall 0.781513
2017-12-10T04:19:28.946206: step 932, loss 0.518559, acc 0.796875, prec 0.0354622, recall 0.781846
2017-12-10T04:19:29.208869: step 933, loss 0.432552, acc 0.859375, prec 0.0354512, recall 0.781846
2017-12-10T04:19:29.474408: step 934, loss 0.338548, acc 0.890625, prec 0.0354426, recall 0.781846
2017-12-10T04:19:29.738430: step 935, loss 1.23246, acc 0.859375, prec 0.0354649, recall 0.782012
2017-12-10T04:19:30.004972: step 936, loss 0.372887, acc 0.875, prec 0.0355218, recall 0.782344
2017-12-10T04:19:30.281472: step 937, loss 2.21636, acc 0.890625, prec 0.0355798, recall 0.782675
2017-12-10T04:19:30.551472: step 938, loss 0.247683, acc 0.921875, prec 0.0356403, recall 0.783005
2017-12-10T04:19:30.818240: step 939, loss 0.430021, acc 0.859375, prec 0.0356625, recall 0.783169
2017-12-10T04:19:31.081709: step 940, loss 0.474978, acc 0.828125, prec 0.035649, recall 0.783169
2017-12-10T04:19:31.339699: step 941, loss 0.622995, acc 0.8125, prec 0.0356675, recall 0.783333
2017-12-10T04:19:31.605980: step 942, loss 0.18346, acc 0.921875, prec 0.0356613, recall 0.783333
2017-12-10T04:19:31.871424: step 943, loss 0.472998, acc 0.84375, prec 0.0356823, recall 0.783497
2017-12-10T04:19:32.131686: step 944, loss 0.187836, acc 0.9375, prec 0.0356774, recall 0.783497
2017-12-10T04:19:32.393700: step 945, loss 0.166414, acc 0.921875, prec 0.0356712, recall 0.783497
2017-12-10T04:19:32.665187: step 946, loss 0.553499, acc 0.828125, prec 0.0356577, recall 0.783497
2017-12-10T04:19:32.924284: step 947, loss 0.581577, acc 0.90625, prec 0.0356835, recall 0.783661
2017-12-10T04:19:33.188661: step 948, loss 0.190974, acc 0.921875, prec 0.0357106, recall 0.783825
2017-12-10T04:19:33.448942: step 949, loss 0.470861, acc 0.875, prec 0.035734, recall 0.783988
2017-12-10T04:19:33.711829: step 950, loss 2.14466, acc 0.84375, prec 0.0358212, recall 0.784476
2017-12-10T04:19:33.980962: step 951, loss 0.626993, acc 0.90625, prec 0.035847, recall 0.784639
2017-12-10T04:19:34.240560: step 952, loss 0.324651, acc 0.890625, prec 0.0358715, recall 0.784801
2017-12-10T04:19:34.504424: step 953, loss 0.26576, acc 0.9375, prec 0.0358997, recall 0.784962
2017-12-10T04:19:34.766235: step 954, loss 0.797774, acc 0.921875, prec 0.0359267, recall 0.785124
2017-12-10T04:19:35.035955: step 955, loss 0.581063, acc 0.796875, prec 0.0359107, recall 0.785124
2017-12-10T04:19:35.306646: step 956, loss 5.70446, acc 0.765625, prec 0.0359265, recall 0.784696
2017-12-10T04:19:35.574452: step 957, loss 8.03843, acc 0.71875, prec 0.0360048, recall 0.784592
2017-12-10T04:19:35.842724: step 958, loss 1.03284, acc 0.78125, prec 0.0360867, recall 0.785075
2017-12-10T04:19:36.109255: step 959, loss 1.68805, acc 0.609375, prec 0.0361219, recall 0.785395
2017-12-10T04:19:36.375721: step 960, loss 1.64476, acc 0.625, prec 0.0361582, recall 0.785714
2017-12-10T04:19:36.633846: step 961, loss 1.59859, acc 0.578125, prec 0.0361907, recall 0.786033
2017-12-10T04:19:36.895597: step 962, loss 2.16917, acc 0.53125, prec 0.0362524, recall 0.786509
2017-12-10T04:19:37.164206: step 963, loss 2.93279, acc 0.421875, prec 0.0362067, recall 0.786509
2017-12-10T04:19:37.423026: step 964, loss 2.7742, acc 0.515625, prec 0.0362013, recall 0.786667
2017-12-10T04:19:37.686082: step 965, loss 2.33764, acc 0.5, prec 0.0361618, recall 0.786667
2017-12-10T04:19:37.943321: step 966, loss 2.18711, acc 0.546875, prec 0.0361917, recall 0.786982
2017-12-10T04:19:38.203382: step 967, loss 1.7258, acc 0.53125, prec 0.0361876, recall 0.78714
2017-12-10T04:19:38.470509: step 968, loss 2.16104, acc 0.609375, prec 0.036255, recall 0.787611
2017-12-10T04:19:38.737901: step 969, loss 1.94156, acc 0.609375, prec 0.036355, recall 0.788235
2017-12-10T04:19:39.005648: step 970, loss 1.07787, acc 0.671875, prec 0.0363291, recall 0.788235
2017-12-10T04:19:39.271775: step 971, loss 3.52375, acc 0.65625, prec 0.0363686, recall 0.787968
2017-12-10T04:19:39.534098: step 972, loss 1.49587, acc 0.703125, prec 0.0363778, recall 0.788123
2017-12-10T04:19:39.798797: step 973, loss 0.836975, acc 0.75, prec 0.0364559, recall 0.788588
2017-12-10T04:19:40.061674: step 974, loss 1.2975, acc 0.78125, prec 0.0365038, recall 0.788897
2017-12-10T04:19:40.321672: step 975, loss 1.7071, acc 0.671875, prec 0.0364779, recall 0.788897
2017-12-10T04:19:40.587707: step 976, loss 0.890851, acc 0.75, prec 0.0364582, recall 0.788897
2017-12-10T04:19:40.862131: step 977, loss 0.715211, acc 0.8125, prec 0.0364434, recall 0.788897
2017-12-10T04:19:41.121304: step 978, loss 0.581082, acc 0.828125, prec 0.0364624, recall 0.789051
2017-12-10T04:19:41.387380: step 979, loss 0.537831, acc 0.90625, prec 0.0364875, recall 0.789205
2017-12-10T04:19:42.367042: step 980, loss 0.48804, acc 0.828125, prec 0.036474, recall 0.789205
2017-12-10T04:19:42.737485: step 981, loss 0.436638, acc 0.90625, prec 0.0364666, recall 0.789205
2017-12-10T04:19:42.997070: step 982, loss 0.477775, acc 0.875, prec 0.0365217, recall 0.789512
2017-12-10T04:19:43.562220: step 983, loss 1.02612, acc 0.9375, prec 0.0365492, recall 0.789665
2017-12-10T04:19:44.274423: step 984, loss 0.846747, acc 0.9375, prec 0.0366092, recall 0.789971
2017-12-10T04:19:45.002354: step 985, loss 0.380015, acc 0.9375, prec 0.0366043, recall 0.789971
2017-12-10T04:19:45.741377: step 986, loss 2.56439, acc 0.890625, prec 0.0365969, recall 0.789397
2017-12-10T04:19:46.459122: step 987, loss 0.760851, acc 0.859375, prec 0.0366506, recall 0.789703
2017-12-10T04:19:47.183963: step 988, loss 0.358487, acc 0.890625, prec 0.036642, recall 0.789703
2017-12-10T04:19:47.940960: step 989, loss 0.318208, acc 0.90625, prec 0.0366346, recall 0.789703
2017-12-10T04:19:48.640186: step 990, loss 0.516539, acc 0.8125, prec 0.0366198, recall 0.789703
2017-12-10T04:19:49.711038: step 991, loss 0.54647, acc 0.859375, prec 0.0366411, recall 0.789855
2017-12-10T04:19:50.135418: step 992, loss 0.493375, acc 0.875, prec 0.0366313, recall 0.789855
2017-12-10T04:19:50.424783: step 993, loss 2.31836, acc 0.90625, prec 0.0366575, recall 0.789436
2017-12-10T04:19:50.685660: step 994, loss 0.407243, acc 0.923077, prec 0.0367173, recall 0.78974
2017-12-10T04:19:50.980669: step 995, loss 0.424605, acc 0.859375, prec 0.0367709, recall 0.790043
2017-12-10T04:19:51.273368: step 996, loss 1.2177, acc 0.671875, prec 0.0368096, recall 0.790346
2017-12-10T04:19:51.558803: step 997, loss 0.501116, acc 0.828125, prec 0.0368283, recall 0.790497
2017-12-10T04:19:51.826707: step 998, loss 0.477477, acc 0.765625, prec 0.0368098, recall 0.790497
2017-12-10T04:19:52.097034: step 999, loss 0.359075, acc 0.875, prec 0.0368322, recall 0.790648
2017-12-10T04:19:52.368421: step 1000, loss 1.07147, acc 0.734375, prec 0.0368113, recall 0.790648
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-1000

2017-12-10T04:19:53.650928: step 1001, loss 1.01431, acc 0.796875, prec 0.0368597, recall 0.790948
2017-12-10T04:19:53.913143: step 1002, loss 0.521861, acc 0.90625, prec 0.0368523, recall 0.790948
2017-12-10T04:19:54.183684: step 1003, loss 0.418559, acc 0.8125, prec 0.0368698, recall 0.791098
2017-12-10T04:19:54.447666: step 1004, loss 0.454634, acc 0.859375, prec 0.0368587, recall 0.791098
2017-12-10T04:19:54.709655: step 1005, loss 0.497436, acc 0.828125, prec 0.0368773, recall 0.791248
2017-12-10T04:19:54.976525: step 1006, loss 0.274201, acc 0.9375, prec 0.0369046, recall 0.791398
2017-12-10T04:19:55.238592: step 1007, loss 0.349648, acc 0.859375, prec 0.0369578, recall 0.791696
2017-12-10T04:19:55.507733: step 1008, loss 0.236758, acc 0.90625, prec 0.0369826, recall 0.791846
2017-12-10T04:19:55.767670: step 1009, loss 0.243198, acc 0.890625, prec 0.0370061, recall 0.791994
2017-12-10T04:19:56.028810: step 1010, loss 0.506226, acc 0.875, prec 0.0369962, recall 0.791994
2017-12-10T04:19:56.286330: step 1011, loss 1.15031, acc 0.859375, prec 0.0370173, recall 0.792143
2017-12-10T04:19:56.555336: step 1012, loss 0.221322, acc 0.9375, prec 0.0370445, recall 0.792291
2017-12-10T04:19:56.817272: step 1013, loss 0.693733, acc 0.90625, prec 0.0371013, recall 0.792587
2017-12-10T04:19:57.092187: step 1014, loss 0.160919, acc 0.9375, prec 0.0370963, recall 0.792587
2017-12-10T04:19:57.350867: step 1015, loss 0.290605, acc 0.9375, prec 0.0371235, recall 0.792735
2017-12-10T04:19:57.616371: step 1016, loss 0.250926, acc 0.953125, prec 0.0371198, recall 0.792735
2017-12-10T04:19:57.882300: step 1017, loss 9.61885, acc 0.9375, prec 0.0371482, recall 0.792319
2017-12-10T04:19:58.154412: step 1018, loss 3.93714, acc 0.921875, prec 0.0371432, recall 0.791755
2017-12-10T04:19:58.418171: step 1019, loss 0.469065, acc 0.890625, prec 0.0371667, recall 0.791903
2017-12-10T04:19:58.682376: step 1020, loss 0.427769, acc 0.90625, prec 0.0371913, recall 0.792051
2017-12-10T04:19:58.951487: step 1021, loss 0.281736, acc 0.84375, prec 0.0371789, recall 0.792051
2017-12-10T04:19:59.215432: step 1022, loss 0.467497, acc 0.84375, prec 0.0371666, recall 0.792051
2017-12-10T04:19:59.478525: step 1023, loss 0.615671, acc 0.859375, prec 0.0371554, recall 0.792051
2017-12-10T04:19:59.736248: step 1024, loss 0.608906, acc 0.8125, prec 0.0372047, recall 0.792346
2017-12-10T04:19:59.996283: step 1025, loss 1.14701, acc 0.71875, prec 0.0372144, recall 0.792493
2017-12-10T04:20:00.270812: step 1026, loss 0.818323, acc 0.75, prec 0.0372266, recall 0.79264
2017-12-10T04:20:00.538904: step 1027, loss 1.1874, acc 0.75, prec 0.0372068, recall 0.79264
2017-12-10T04:20:00.798772: step 1028, loss 0.806395, acc 0.71875, prec 0.0371846, recall 0.79264
2017-12-10T04:20:01.062893: step 1029, loss 4.74944, acc 0.84375, prec 0.0371735, recall 0.792079
2017-12-10T04:20:01.335506: step 1030, loss 1.43157, acc 0.703125, prec 0.037182, recall 0.792226
2017-12-10T04:20:01.603862: step 1031, loss 0.883165, acc 0.703125, prec 0.0371586, recall 0.792226
2017-12-10T04:20:01.868224: step 1032, loss 0.860065, acc 0.765625, prec 0.037172, recall 0.792373
2017-12-10T04:20:02.135409: step 1033, loss 1.16358, acc 0.75, prec 0.0372161, recall 0.792666
2017-12-10T04:20:02.401012: step 1034, loss 1.06673, acc 0.75, prec 0.0371964, recall 0.792666
2017-12-10T04:20:02.666517: step 1035, loss 1.01142, acc 0.65625, prec 0.0372012, recall 0.792812
2017-12-10T04:20:02.927939: step 1036, loss 1.56894, acc 0.578125, prec 0.037168, recall 0.792812
2017-12-10T04:20:03.192338: step 1037, loss 0.714113, acc 0.8125, prec 0.0372168, recall 0.793103
2017-12-10T04:20:03.450396: step 1038, loss 0.762356, acc 0.859375, prec 0.0372376, recall 0.793249
2017-12-10T04:20:03.714560: step 1039, loss 0.639425, acc 0.84375, prec 0.0372253, recall 0.793249
2017-12-10T04:20:03.974545: step 1040, loss 0.911373, acc 0.71875, prec 0.0372032, recall 0.793249
2017-12-10T04:20:04.236684: step 1041, loss 5.05268, acc 0.84375, prec 0.0372556, recall 0.792982
2017-12-10T04:20:04.506603: step 1042, loss 0.695184, acc 0.796875, prec 0.0373031, recall 0.793273
2017-12-10T04:20:04.772659: step 1043, loss 0.330154, acc 0.890625, prec 0.0372945, recall 0.793273
2017-12-10T04:20:05.036849: step 1044, loss 0.21014, acc 0.921875, prec 0.0373201, recall 0.793417
2017-12-10T04:20:05.304124: step 1045, loss 0.300871, acc 0.90625, prec 0.0373127, recall 0.793417
2017-12-10T04:20:05.569274: step 1046, loss 0.472299, acc 0.875, prec 0.0373029, recall 0.793417
2017-12-10T04:20:05.837781: step 1047, loss 0.183326, acc 0.890625, prec 0.0372943, recall 0.793417
2017-12-10T04:20:06.100494: step 1048, loss 3.53387, acc 0.78125, prec 0.03731, recall 0.793007
2017-12-10T04:20:06.367434: step 1049, loss 3.76518, acc 0.890625, prec 0.0373026, recall 0.792453
2017-12-10T04:20:06.640041: step 1050, loss 1.59881, acc 0.84375, prec 0.0373866, recall 0.792334
2017-12-10T04:20:06.910377: step 1051, loss 0.929435, acc 0.84375, prec 0.0374376, recall 0.792624
2017-12-10T04:20:07.172145: step 1052, loss 0.701985, acc 0.765625, prec 0.0374191, recall 0.792624
2017-12-10T04:20:07.438440: step 1053, loss 1.32815, acc 0.703125, prec 0.037459, recall 0.792912
2017-12-10T04:20:07.700618: step 1054, loss 1.36267, acc 0.609375, prec 0.0374282, recall 0.792912
2017-12-10T04:20:07.964604: step 1055, loss 2.54662, acc 0.625, prec 0.0374316, recall 0.792505
2017-12-10T04:20:08.236860: step 1056, loss 1.50311, acc 0.625, prec 0.0374337, recall 0.792649
2017-12-10T04:20:08.494680: step 1057, loss 1.44882, acc 0.640625, prec 0.0374055, recall 0.792649
2017-12-10T04:20:08.757495: step 1058, loss 1.31645, acc 0.625, prec 0.0373761, recall 0.792649
2017-12-10T04:20:09.018829: step 1059, loss 1.47524, acc 0.640625, prec 0.037411, recall 0.792936
2017-12-10T04:20:09.283597: step 1060, loss 1.29074, acc 0.703125, prec 0.0373878, recall 0.792936
2017-12-10T04:20:09.545223: step 1061, loss 1.63991, acc 0.578125, prec 0.0373862, recall 0.79308
2017-12-10T04:20:09.802980: step 1062, loss 1.34481, acc 0.65625, prec 0.0373594, recall 0.79308
2017-12-10T04:20:10.072215: step 1063, loss 1.07817, acc 0.6875, prec 0.0373664, recall 0.793223
2017-12-10T04:20:10.335453: step 1064, loss 0.821369, acc 0.75, prec 0.037347, recall 0.793223
2017-12-10T04:20:10.608704: step 1065, loss 1.14406, acc 0.734375, prec 0.0373576, recall 0.793366
2017-12-10T04:20:10.873472: step 1066, loss 0.481315, acc 0.84375, prec 0.0373768, recall 0.793508
2017-12-10T04:20:11.140902: step 1067, loss 0.530127, acc 0.859375, prec 0.0374285, recall 0.793793
2017-12-10T04:20:11.400135: step 1068, loss 2.10793, acc 0.78125, prec 0.0374439, recall 0.793388
2017-12-10T04:20:11.671310: step 1069, loss 0.414188, acc 0.84375, prec 0.0374318, recall 0.793388
2017-12-10T04:20:11.943494: step 1070, loss 1.61881, acc 0.875, prec 0.0374233, recall 0.792842
2017-12-10T04:20:12.214362: step 1071, loss 3.52936, acc 0.859375, prec 0.0374147, recall 0.791753
2017-12-10T04:20:12.481970: step 1072, loss 0.909718, acc 0.875, prec 0.0374675, recall 0.792038
2017-12-10T04:20:12.747950: step 1073, loss 0.401987, acc 0.890625, prec 0.0375215, recall 0.792324
2017-12-10T04:20:13.014171: step 1074, loss 0.819424, acc 0.734375, prec 0.0375008, recall 0.792324
2017-12-10T04:20:13.279089: step 1075, loss 0.834469, acc 0.734375, prec 0.0375113, recall 0.792466
2017-12-10T04:20:13.545970: step 1076, loss 0.71634, acc 0.796875, prec 0.0374955, recall 0.792466
2017-12-10T04:20:13.809800: step 1077, loss 0.801825, acc 0.734375, prec 0.0375061, recall 0.792608
2017-12-10T04:20:14.071712: step 1078, loss 1.06042, acc 0.625, prec 0.0375392, recall 0.792891
2017-12-10T04:20:14.332302: step 1079, loss 1.11193, acc 0.78125, prec 0.0375845, recall 0.793174
2017-12-10T04:20:14.593650: step 1080, loss 1.14946, acc 0.734375, prec 0.0376261, recall 0.793456
2017-12-10T04:20:14.852228: step 1081, loss 1.06421, acc 0.71875, prec 0.0376664, recall 0.793737
2017-12-10T04:20:15.111388: step 1082, loss 0.968368, acc 0.734375, prec 0.0376768, recall 0.793878
2017-12-10T04:20:15.376846: step 1083, loss 0.640057, acc 0.796875, prec 0.0378162, recall 0.794576
2017-12-10T04:20:15.647700: step 1084, loss 0.480612, acc 0.8125, prec 0.0378326, recall 0.794715
2017-12-10T04:20:15.910663: step 1085, loss 0.512997, acc 0.796875, prec 0.0378478, recall 0.794854
2017-12-10T04:20:16.172880: step 1086, loss 0.829987, acc 0.78125, prec 0.0378307, recall 0.794854
2017-12-10T04:20:16.435869: step 1087, loss 0.548019, acc 0.765625, prec 0.0378124, recall 0.794854
2017-12-10T04:20:16.696715: step 1088, loss 0.553405, acc 0.828125, prec 0.03783, recall 0.794993
2017-12-10T04:20:16.960490: step 1089, loss 0.226835, acc 0.890625, prec 0.0378834, recall 0.79527
2017-12-10T04:20:17.228155: step 1090, loss 0.471854, acc 0.921875, prec 0.0379392, recall 0.795547
2017-12-10T04:20:17.493041: step 1091, loss 0.307819, acc 0.921875, prec 0.0379641, recall 0.795684
2017-12-10T04:20:17.765761: step 1092, loss 0.319154, acc 0.9375, prec 0.0379902, recall 0.795822
2017-12-10T04:20:18.026839: step 1093, loss 0.151646, acc 0.96875, prec 0.0380187, recall 0.79596
2017-12-10T04:20:18.291344: step 1094, loss 0.229211, acc 0.9375, prec 0.0380138, recall 0.79596
2017-12-10T04:20:18.552184: step 1095, loss 0.130378, acc 0.921875, prec 0.0380077, recall 0.79596
2017-12-10T04:20:18.823360: step 1096, loss 3.2696, acc 0.96875, prec 0.0380064, recall 0.795424
2017-12-10T04:20:19.098300: step 1097, loss 0.106653, acc 0.96875, prec 0.0380349, recall 0.795562
2017-12-10T04:20:19.366898: step 1098, loss 1.64867, acc 0.921875, prec 0.0380609, recall 0.795165
2017-12-10T04:20:19.636105: step 1099, loss 0.101458, acc 0.96875, prec 0.0380585, recall 0.795165
2017-12-10T04:20:19.900001: step 1100, loss 0.235395, acc 0.953125, prec 0.0380548, recall 0.795165
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-1100

2017-12-10T04:20:21.150470: step 1101, loss 0.213651, acc 0.953125, prec 0.0380512, recall 0.795165
2017-12-10T04:20:21.415707: step 1102, loss 2.10828, acc 0.921875, prec 0.0381081, recall 0.794906
2017-12-10T04:20:21.689126: step 1103, loss 0.429574, acc 0.890625, prec 0.0381304, recall 0.795044
2017-12-10T04:20:21.964265: step 1104, loss 2.87081, acc 0.859375, prec 0.0381515, recall 0.794649
2017-12-10T04:20:22.226940: step 1105, loss 3.95071, acc 0.84375, prec 0.0381405, recall 0.794118
2017-12-10T04:20:22.497049: step 1106, loss 0.431194, acc 0.890625, prec 0.0382245, recall 0.79453
2017-12-10T04:20:22.765047: step 1107, loss 1.32158, acc 0.765625, prec 0.0382678, recall 0.794803
2017-12-10T04:20:23.035931: step 1108, loss 1.14231, acc 0.6875, prec 0.038305, recall 0.795076
2017-12-10T04:20:23.295906: step 1109, loss 1.18022, acc 0.71875, prec 0.0383137, recall 0.795213
2017-12-10T04:20:23.563117: step 1110, loss 1.8214, acc 0.6875, prec 0.0383507, recall 0.795485
2017-12-10T04:20:23.830998: step 1111, loss 1.89639, acc 0.59375, prec 0.0383496, recall 0.79562
2017-12-10T04:20:24.089988: step 1112, loss 1.89527, acc 0.609375, prec 0.0384111, recall 0.796026
2017-12-10T04:20:24.353425: step 1113, loss 2.07208, acc 0.46875, prec 0.0383695, recall 0.796026
2017-12-10T04:20:24.609426: step 1114, loss 2.02703, acc 0.515625, prec 0.0383929, recall 0.796296
2017-12-10T04:20:24.873992: step 1115, loss 1.96043, acc 0.703125, prec 0.0383696, recall 0.796296
2017-12-10T04:20:25.134417: step 1116, loss 1.58832, acc 0.546875, prec 0.0383342, recall 0.796296
2017-12-10T04:20:25.394620: step 1117, loss 1.41849, acc 0.625, prec 0.0383661, recall 0.796565
2017-12-10T04:20:25.657818: step 1118, loss 0.97177, acc 0.734375, prec 0.0384065, recall 0.796834
2017-12-10T04:20:25.920865: step 1119, loss 0.611743, acc 0.78125, prec 0.0383894, recall 0.796834
2017-12-10T04:20:26.186648: step 1120, loss 0.856349, acc 0.796875, prec 0.0383736, recall 0.796834
2017-12-10T04:20:26.449572: step 1121, loss 0.824248, acc 0.734375, prec 0.0383834, recall 0.796968
2017-12-10T04:20:26.718779: step 1122, loss 0.590635, acc 0.84375, prec 0.0383712, recall 0.796968
2017-12-10T04:20:26.990524: step 1123, loss 0.366518, acc 0.84375, prec 0.038359, recall 0.796968
2017-12-10T04:20:27.253270: step 1124, loss 0.32017, acc 0.859375, prec 0.0383481, recall 0.796968
2017-12-10T04:20:27.516105: step 1125, loss 0.214147, acc 0.953125, prec 0.0383749, recall 0.797101
2017-12-10T04:20:27.787832: step 1126, loss 0.0410333, acc 1, prec 0.0383749, recall 0.797101
2017-12-10T04:20:28.052249: step 1127, loss 0.584354, acc 0.9375, prec 0.0384006, recall 0.797235
2017-12-10T04:20:28.319124: step 1128, loss 0.266959, acc 0.90625, prec 0.0384237, recall 0.797368
2017-12-10T04:20:28.587139: step 1129, loss 0.312479, acc 0.984375, prec 0.038453, recall 0.797502
2017-12-10T04:20:28.859169: step 1130, loss 5.7585, acc 0.984375, prec 0.038514, recall 0.797244
2017-12-10T04:20:29.142398: step 1131, loss 0.0573249, acc 0.96875, prec 0.0385115, recall 0.797244
2017-12-10T04:20:29.410366: step 1132, loss 7.61499, acc 0.953125, prec 0.0385396, recall 0.796854
2017-12-10T04:20:29.675919: step 1133, loss 0.141478, acc 0.9375, prec 0.0385347, recall 0.796854
2017-12-10T04:20:29.938816: step 1134, loss 0.312671, acc 0.890625, prec 0.0385261, recall 0.796854
2017-12-10T04:20:30.209456: step 1135, loss 0.34638, acc 0.890625, prec 0.0385176, recall 0.796854
2017-12-10T04:20:30.484345: step 1136, loss 3.41751, acc 0.8125, prec 0.0385346, recall 0.796466
2017-12-10T04:20:30.751937: step 1137, loss 1.72569, acc 0.8125, prec 0.0385516, recall 0.796078
2017-12-10T04:20:31.013636: step 1138, loss 0.474664, acc 0.890625, prec 0.0385735, recall 0.796212
2017-12-10T04:20:31.280400: step 1139, loss 1.09638, acc 0.640625, prec 0.0385455, recall 0.796212
2017-12-10T04:20:31.548117: step 1140, loss 0.908812, acc 0.6875, prec 0.0385211, recall 0.796212
2017-12-10T04:20:31.810220: step 1141, loss 1.13149, acc 0.6875, prec 0.0384968, recall 0.796212
2017-12-10T04:20:32.081035: step 1142, loss 2.65191, acc 0.5625, prec 0.0385234, recall 0.796477
2017-12-10T04:20:32.347872: step 1143, loss 1.1644, acc 0.671875, prec 0.0385889, recall 0.796875
2017-12-10T04:20:32.605831: step 1144, loss 1.38049, acc 0.609375, prec 0.0385585, recall 0.796875
2017-12-10T04:20:32.865397: step 1145, loss 2.49785, acc 0.40625, prec 0.0385124, recall 0.796875
2017-12-10T04:20:33.124061: step 1146, loss 0.935777, acc 0.703125, prec 0.0385498, recall 0.797139
2017-12-10T04:20:33.385606: step 1147, loss 1.35089, acc 0.640625, prec 0.038522, recall 0.797139
2017-12-10T04:20:33.651616: step 1148, loss 1.53085, acc 0.625, prec 0.0384929, recall 0.797139
2017-12-10T04:20:33.915891: step 1149, loss 1.22555, acc 0.671875, prec 0.0384676, recall 0.797139
2017-12-10T04:20:34.182256: step 1150, loss 0.882752, acc 0.78125, prec 0.0384808, recall 0.797271
2017-12-10T04:20:34.449299: step 1151, loss 0.958633, acc 0.703125, prec 0.0384881, recall 0.797403
2017-12-10T04:20:34.707487: step 1152, loss 0.901065, acc 0.765625, prec 0.03847, recall 0.797403
2017-12-10T04:20:34.970409: step 1153, loss 0.709849, acc 0.875, prec 0.0385507, recall 0.797796
2017-12-10T04:20:35.233210: step 1154, loss 1.26974, acc 0.890625, prec 0.0385723, recall 0.797927
2017-12-10T04:20:35.494356: step 1155, loss 0.228237, acc 0.9375, prec 0.0385675, recall 0.797927
2017-12-10T04:20:35.761296: step 1156, loss 0.633164, acc 0.859375, prec 0.0385867, recall 0.798058
2017-12-10T04:20:36.024095: step 1157, loss 0.266368, acc 0.890625, prec 0.0385783, recall 0.798058
2017-12-10T04:20:36.290210: step 1158, loss 0.253908, acc 0.90625, prec 0.038571, recall 0.798058
2017-12-10T04:20:36.556011: step 1159, loss 0.291487, acc 0.859375, prec 0.0385602, recall 0.798058
2017-12-10T04:20:36.825489: step 1160, loss 0.248182, acc 0.875, prec 0.0385505, recall 0.798058
2017-12-10T04:20:37.092267: step 1161, loss 0.292499, acc 0.921875, prec 0.0385445, recall 0.798058
2017-12-10T04:20:37.356330: step 1162, loss 1.26754, acc 0.875, prec 0.0385649, recall 0.798189
2017-12-10T04:20:37.625965: step 1163, loss 1.24453, acc 0.90625, prec 0.0385589, recall 0.797673
2017-12-10T04:20:37.887664: step 1164, loss 0.050095, acc 0.96875, prec 0.0385865, recall 0.797804
2017-12-10T04:20:38.151585: step 1165, loss 0.195529, acc 0.9375, prec 0.0386117, recall 0.797934
2017-12-10T04:20:38.421915: step 1166, loss 0.641619, acc 0.890625, prec 0.0386333, recall 0.798065
2017-12-10T04:20:38.686459: step 1167, loss 0.553486, acc 0.8125, prec 0.0386188, recall 0.798065
2017-12-10T04:20:38.953062: step 1168, loss 0.271779, acc 0.921875, prec 0.0386728, recall 0.798325
2017-12-10T04:20:39.215741: step 1169, loss 0.275405, acc 0.890625, prec 0.0386944, recall 0.798455
2017-12-10T04:20:39.479396: step 1170, loss 0.135085, acc 0.96875, prec 0.038692, recall 0.798455
2017-12-10T04:20:39.744282: step 1171, loss 0.250285, acc 0.953125, prec 0.0386883, recall 0.798455
2017-12-10T04:20:40.010548: step 1172, loss 6.87774, acc 0.96875, prec 0.0386871, recall 0.797941
2017-12-10T04:20:40.273633: step 1173, loss 0.342979, acc 0.90625, prec 0.0386799, recall 0.797941
2017-12-10T04:20:40.538769: step 1174, loss 0.504719, acc 0.875, prec 0.0387002, recall 0.798071
2017-12-10T04:20:40.809675: step 1175, loss 0.531384, acc 0.8125, prec 0.0386857, recall 0.798071
2017-12-10T04:20:41.079365: step 1176, loss 0.365553, acc 0.890625, prec 0.0387073, recall 0.7982
2017-12-10T04:20:41.340013: step 1177, loss 0.536774, acc 0.875, prec 0.0386976, recall 0.7982
2017-12-10T04:20:41.602170: step 1178, loss 0.493755, acc 0.859375, prec 0.0387167, recall 0.79833
2017-12-10T04:20:41.878583: step 1179, loss 0.474063, acc 0.875, prec 0.0387669, recall 0.798589
2017-12-10T04:20:42.146593: step 1180, loss 0.727642, acc 0.765625, prec 0.0387488, recall 0.798589
2017-12-10T04:20:42.411474: step 1181, loss 0.384647, acc 0.890625, prec 0.0387703, recall 0.798718
2017-12-10T04:20:42.675849: step 1182, loss 0.293462, acc 0.859375, prec 0.0387594, recall 0.798718
2017-12-10T04:20:42.938633: step 1183, loss 0.776402, acc 0.78125, prec 0.0387426, recall 0.798718
2017-12-10T04:20:43.199191: step 1184, loss 0.457392, acc 0.796875, prec 0.0387568, recall 0.798847
2017-12-10T04:20:43.464620: step 1185, loss 0.845312, acc 0.828125, prec 0.0387734, recall 0.798976
2017-12-10T04:20:43.728018: step 1186, loss 0.209496, acc 0.921875, prec 0.0387674, recall 0.798976
2017-12-10T04:20:43.990308: step 1187, loss 0.350918, acc 0.890625, prec 0.038759, recall 0.798976
2017-12-10T04:20:44.254861: step 1188, loss 0.176418, acc 0.9375, prec 0.0387542, recall 0.798976
2017-12-10T04:20:44.519201: step 1189, loss 5.6296, acc 0.921875, prec 0.0387804, recall 0.798083
2017-12-10T04:20:44.790708: step 1190, loss 0.336237, acc 0.90625, prec 0.038803, recall 0.798212
2017-12-10T04:20:45.062133: step 1191, loss 0.514807, acc 0.84375, prec 0.0388208, recall 0.798341
2017-12-10T04:20:45.323469: step 1192, loss 0.53861, acc 0.875, prec 0.0388112, recall 0.798341
2017-12-10T04:20:45.586134: step 1193, loss 0.32277, acc 0.84375, prec 0.0388885, recall 0.798726
2017-12-10T04:20:45.848340: step 1194, loss 1.04345, acc 0.703125, prec 0.0388656, recall 0.798726
2017-12-10T04:20:46.109317: step 1195, loss 0.693667, acc 0.8125, prec 0.0388512, recall 0.798726
2017-12-10T04:20:46.377976: step 1196, loss 1.26648, acc 0.640625, prec 0.0388235, recall 0.798726
2017-12-10T04:20:46.636051: step 1197, loss 1.55328, acc 0.71875, prec 0.0388019, recall 0.798726
2017-12-10T04:20:46.897922: step 1198, loss 7.45956, acc 0.6875, prec 0.0388088, recall 0.798346
2017-12-10T04:20:47.166812: step 1199, loss 1.16202, acc 0.6875, prec 0.0388443, recall 0.798602
2017-12-10T04:20:47.432576: step 1200, loss 1.10427, acc 0.671875, prec 0.0388487, recall 0.79873

Evaluation:
2017-12-10T04:20:55.054444: step 1200, loss 1.34264, acc 0.744197, prec 0.0393739, recall 0.804411

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-1200

2017-12-10T04:20:56.338980: step 1201, loss 1.24164, acc 0.671875, prec 0.039405, recall 0.804638
2017-12-10T04:20:56.606239: step 1202, loss 0.927107, acc 0.71875, prec 0.0394121, recall 0.804751
2017-12-10T04:20:56.876363: step 1203, loss 1.06571, acc 0.734375, prec 0.0394476, recall 0.804977
2017-12-10T04:20:57.146146: step 1204, loss 0.787071, acc 0.828125, prec 0.039517, recall 0.805315
2017-12-10T04:20:57.411347: step 1205, loss 1.334, acc 0.734375, prec 0.0395251, recall 0.805427
2017-12-10T04:20:57.668261: step 1206, loss 1.36869, acc 0.734375, prec 0.0395061, recall 0.805427
2017-12-10T04:20:57.933607: step 1207, loss 0.737823, acc 0.796875, prec 0.0395188, recall 0.80554
2017-12-10T04:20:58.192495: step 1208, loss 0.585979, acc 0.8125, prec 0.0395053, recall 0.80554
2017-12-10T04:20:58.455257: step 1209, loss 0.468374, acc 0.84375, prec 0.0395213, recall 0.805652
2017-12-10T04:20:58.723956: step 1210, loss 0.522417, acc 0.84375, prec 0.0395373, recall 0.805764
2017-12-10T04:20:58.990441: step 1211, loss 1.00085, acc 0.84375, prec 0.0395533, recall 0.805876
2017-12-10T04:20:59.257083: step 1212, loss 0.331955, acc 0.875, prec 0.0395444, recall 0.805876
2017-12-10T04:20:59.524934: step 1213, loss 2.38281, acc 0.890625, prec 0.0395648, recall 0.805524
2017-12-10T04:20:59.797532: step 1214, loss 0.358497, acc 0.890625, prec 0.039557, recall 0.805524
2017-12-10T04:21:00.064189: step 1215, loss 0.444032, acc 0.84375, prec 0.0395458, recall 0.805524
2017-12-10T04:21:00.335264: step 1216, loss 5.87087, acc 0.828125, prec 0.0395889, recall 0.805284
2017-12-10T04:21:00.619775: step 1217, loss 0.473442, acc 0.828125, prec 0.0396308, recall 0.805508
2017-12-10T04:21:00.889887: step 1218, loss 0.999376, acc 0.765625, prec 0.0396411, recall 0.805619
2017-12-10T04:21:01.158233: step 1219, loss 0.774411, acc 0.75, prec 0.0396503, recall 0.805731
2017-12-10T04:21:01.419504: step 1220, loss 4.88401, acc 0.671875, prec 0.039628, recall 0.805269
2017-12-10T04:21:01.690585: step 1221, loss 0.927272, acc 0.703125, prec 0.0396338, recall 0.805381
2017-12-10T04:21:01.958176: step 1222, loss 0.932782, acc 0.78125, prec 0.0396452, recall 0.805492
2017-12-10T04:21:02.220364: step 1223, loss 1.71783, acc 0.65625, prec 0.0396477, recall 0.805603
2017-12-10T04:21:02.484551: step 1224, loss 1.16803, acc 0.703125, prec 0.0396265, recall 0.805603
2017-12-10T04:21:02.749987: step 1225, loss 1.52205, acc 0.640625, prec 0.0396279, recall 0.805714
2017-12-10T04:21:03.014981: step 1226, loss 1.6848, acc 0.578125, prec 0.0396248, recall 0.805825
2017-12-10T04:21:03.274590: step 1227, loss 1.51127, acc 0.640625, prec 0.0396262, recall 0.805936
2017-12-10T04:21:03.535209: step 1228, loss 1.53225, acc 0.625, prec 0.0396265, recall 0.806047
2017-12-10T04:21:03.793072: step 1229, loss 0.669669, acc 0.828125, prec 0.0396681, recall 0.806268
2017-12-10T04:21:04.059205: step 1230, loss 0.924816, acc 0.703125, prec 0.0396739, recall 0.806378
2017-12-10T04:21:04.319772: step 1231, loss 1.2752, acc 0.703125, prec 0.0396528, recall 0.806378
2017-12-10T04:21:04.582065: step 1232, loss 1.24079, acc 0.59375, prec 0.0396239, recall 0.806378
2017-12-10T04:21:04.851960: step 1233, loss 0.924621, acc 0.734375, prec 0.0396319, recall 0.806488
2017-12-10T04:21:05.119341: step 1234, loss 0.812841, acc 0.796875, prec 0.0396444, recall 0.806598
2017-12-10T04:21:05.382397: step 1235, loss 0.452579, acc 0.875, prec 0.0396355, recall 0.806598
2017-12-10T04:21:05.644138: step 1236, loss 0.619005, acc 0.859375, prec 0.0396524, recall 0.806708
2017-12-10T04:21:05.904560: step 1237, loss 0.180285, acc 0.921875, prec 0.0396468, recall 0.806708
2017-12-10T04:21:06.172087: step 1238, loss 0.130157, acc 0.9375, prec 0.0396692, recall 0.806818
2017-12-10T04:21:06.436147: step 1239, loss 0.150304, acc 0.921875, prec 0.0396637, recall 0.806818
2017-12-10T04:21:06.695438: step 1240, loss 0.177637, acc 0.96875, prec 0.0397419, recall 0.807147
2017-12-10T04:21:06.957031: step 1241, loss 0.518217, acc 0.953125, prec 0.0397654, recall 0.807256
2017-12-10T04:21:07.220963: step 1242, loss 0.385566, acc 0.984375, prec 0.0397911, recall 0.807365
2017-12-10T04:21:08.190362: step 1243, loss 1.58053, acc 0.96875, prec 0.0398961, recall 0.807801
2017-12-10T04:21:08.543999: step 1244, loss 6.59521, acc 0.9375, prec 0.0398928, recall 0.807345
2017-12-10T04:21:08.808534: step 1245, loss 0.693777, acc 0.90625, prec 0.0399129, recall 0.807453
2017-12-10T04:21:09.215374: step 1246, loss 0.410509, acc 0.890625, prec 0.0399051, recall 0.807453
2017-12-10T04:21:09.957806: step 1247, loss 0.205599, acc 0.9375, prec 0.0399007, recall 0.807453
2017-12-10T04:21:10.679976: step 1248, loss 0.171976, acc 0.9375, prec 0.0398962, recall 0.807453
2017-12-10T04:21:11.406016: step 1249, loss 0.727967, acc 0.828125, prec 0.039884, recall 0.807453
2017-12-10T04:21:12.152739: step 1250, loss 0.438969, acc 0.875, prec 0.0398751, recall 0.807453
2017-12-10T04:21:12.907047: step 1251, loss 0.709963, acc 0.8125, prec 0.0398617, recall 0.807453
2017-12-10T04:21:13.611989: step 1252, loss 0.548639, acc 0.828125, prec 0.039903, recall 0.807671
2017-12-10T04:21:14.355265: step 1253, loss 0.608464, acc 0.859375, prec 0.039893, recall 0.807671
2017-12-10T04:21:15.038857: step 1254, loss 0.629322, acc 0.796875, prec 0.0398786, recall 0.807671
2017-12-10T04:21:15.799258: step 1255, loss 0.728704, acc 0.765625, prec 0.0399154, recall 0.807887
2017-12-10T04:21:16.517622: step 1256, loss 0.597201, acc 0.796875, prec 0.0399009, recall 0.807887
2017-12-10T04:21:17.244856: step 1257, loss 0.928715, acc 0.75, prec 0.0399099, recall 0.807995
2017-12-10T04:21:18.215843: step 1258, loss 0.620632, acc 0.8125, prec 0.0399233, recall 0.808104
2017-12-10T04:21:18.556376: step 1259, loss 0.695404, acc 0.8125, prec 0.03991, recall 0.808104
2017-12-10T04:21:18.828220: step 1260, loss 0.620841, acc 0.859375, prec 0.0399266, recall 0.808211
2017-12-10T04:21:19.102799: step 1261, loss 0.290757, acc 0.859375, prec 0.03997, recall 0.808427
2017-12-10T04:21:19.390965: step 1262, loss 0.313157, acc 0.921875, prec 0.0399645, recall 0.808427
2017-12-10T04:21:19.673494: step 1263, loss 0.34723, acc 0.90625, prec 0.0399844, recall 0.808535
2017-12-10T04:21:19.946934: step 1264, loss 0.332981, acc 0.890625, prec 0.0400033, recall 0.808642
2017-12-10T04:21:20.209755: step 1265, loss 0.212801, acc 0.921875, prec 0.0399978, recall 0.808642
2017-12-10T04:21:20.470991: step 1266, loss 0.597214, acc 0.96875, prec 0.0400488, recall 0.808856
2017-12-10T04:21:20.736952: step 1267, loss 0.386361, acc 0.90625, prec 0.0400955, recall 0.809071
2017-12-10T04:21:20.999556: step 1268, loss 1.15522, acc 0.90625, prec 0.0401953, recall 0.809497
2017-12-10T04:21:21.266352: step 1269, loss 0.285818, acc 0.9375, prec 0.0402175, recall 0.809604
2017-12-10T04:21:21.539380: step 1270, loss 3.08049, acc 0.890625, prec 0.0402119, recall 0.808701
2017-12-10T04:21:21.809800: step 1271, loss 0.170118, acc 0.90625, prec 0.0402318, recall 0.808807
2017-12-10T04:21:22.071918: step 1272, loss 4.32778, acc 0.9375, prec 0.0402285, recall 0.808357
2017-12-10T04:21:22.344123: step 1273, loss 0.477754, acc 0.84375, prec 0.0402173, recall 0.808357
2017-12-10T04:21:22.606696: step 1274, loss 0.834669, acc 0.78125, prec 0.0402549, recall 0.80857
2017-12-10T04:21:22.873731: step 1275, loss 1.08809, acc 0.703125, prec 0.0403134, recall 0.808889
2017-12-10T04:21:23.130994: step 1276, loss 0.586532, acc 0.859375, prec 0.0403299, recall 0.808995
2017-12-10T04:21:23.395739: step 1277, loss 1.21353, acc 0.765625, prec 0.0403663, recall 0.809207
2017-12-10T04:21:23.662937: step 1278, loss 1.23147, acc 0.65625, prec 0.0403948, recall 0.809418
2017-12-10T04:21:23.925390: step 1279, loss 1.71302, acc 0.703125, prec 0.0404001, recall 0.809524
2017-12-10T04:21:24.185782: step 1280, loss 1.1618, acc 0.65625, prec 0.0403756, recall 0.809524
2017-12-10T04:21:24.449962: step 1281, loss 1.43604, acc 0.734375, prec 0.0404361, recall 0.80984
2017-12-10T04:21:24.712096: step 1282, loss 1.50469, acc 0.703125, prec 0.0404149, recall 0.80984
2017-12-10T04:21:24.975958: step 1283, loss 1.43381, acc 0.671875, prec 0.0403915, recall 0.80984
2017-12-10T04:21:25.239675: step 1284, loss 1.18428, acc 0.671875, prec 0.0403681, recall 0.80984
2017-12-10T04:21:25.497720: step 1285, loss 1.5993, acc 0.828125, prec 0.0404088, recall 0.81005
2017-12-10T04:21:25.763674: step 1286, loss 0.66524, acc 0.828125, prec 0.0403965, recall 0.81005
2017-12-10T04:21:26.028380: step 1287, loss 0.747545, acc 0.8125, prec 0.0404096, recall 0.810154
2017-12-10T04:21:26.293029: step 1288, loss 1.03382, acc 0.765625, prec 0.0404457, recall 0.810364
2017-12-10T04:21:26.556872: step 1289, loss 0.563186, acc 0.78125, prec 0.0404302, recall 0.810364
2017-12-10T04:21:26.818688: step 1290, loss 0.576401, acc 0.84375, prec 0.040419, recall 0.810364
2017-12-10T04:21:27.089037: step 1291, loss 0.453598, acc 0.875, prec 0.0404101, recall 0.810364
2017-12-10T04:21:27.354816: step 1292, loss 0.634863, acc 0.796875, prec 0.0404221, recall 0.810468
2017-12-10T04:21:27.616084: step 1293, loss 0.390985, acc 0.875, prec 0.0404132, recall 0.810468
2017-12-10T04:21:27.879334: step 1294, loss 0.483002, acc 0.859375, prec 0.0404296, recall 0.810573
2017-12-10T04:21:28.153612: step 1295, loss 0.388997, acc 0.90625, prec 0.0404493, recall 0.810677
2017-12-10T04:21:28.424849: step 1296, loss 0.26802, acc 0.890625, prec 0.0404415, recall 0.810677
2017-12-10T04:21:28.689055: step 1297, loss 0.385873, acc 0.9375, prec 0.0404897, recall 0.810885
2017-12-10T04:21:28.950728: step 1298, loss 0.279847, acc 0.90625, prec 0.0404831, recall 0.810885
2017-12-10T04:21:29.212996: step 1299, loss 0.289187, acc 0.921875, prec 0.0405302, recall 0.811093
2017-12-10T04:21:29.480850: step 1300, loss 0.137302, acc 0.953125, prec 0.0405268, recall 0.811093
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-1300

2017-12-10T04:21:30.778568: step 1301, loss 0.108084, acc 0.96875, prec 0.0405246, recall 0.811093
2017-12-10T04:21:31.042570: step 1302, loss 0.144961, acc 0.953125, prec 0.0405213, recall 0.811093
2017-12-10T04:21:31.309185: step 1303, loss 0.36792, acc 1, prec 0.0405476, recall 0.811197
2017-12-10T04:21:31.574486: step 1304, loss 0.0845308, acc 0.953125, prec 0.0405442, recall 0.811197
2017-12-10T04:21:31.836405: step 1305, loss 2.15458, acc 0.90625, prec 0.040565, recall 0.810855
2017-12-10T04:21:32.104941: step 1306, loss 0.293712, acc 1, prec 0.0405913, recall 0.810959
2017-12-10T04:21:32.369733: step 1307, loss 0.134764, acc 0.9375, prec 0.0405869, recall 0.810959
2017-12-10T04:21:32.639951: step 1308, loss 2.58776, acc 0.90625, prec 0.0406076, recall 0.810619
2017-12-10T04:21:32.908441: step 1309, loss 0.464973, acc 0.953125, prec 0.0406306, recall 0.810722
2017-12-10T04:21:33.173317: step 1310, loss 1.3246, acc 0.90625, prec 0.0406765, recall 0.810929
2017-12-10T04:21:33.441951: step 1311, loss 0.527342, acc 0.90625, prec 0.0407224, recall 0.811135
2017-12-10T04:21:33.704521: step 1312, loss 6.44013, acc 0.8125, prec 0.0407101, recall 0.810693
2017-12-10T04:21:33.967052: step 1313, loss 0.827384, acc 0.8125, prec 0.040723, recall 0.810796
2017-12-10T04:21:34.227006: step 1314, loss 1.23617, acc 0.75, prec 0.0407052, recall 0.810796
2017-12-10T04:21:34.494575: step 1315, loss 1.0117, acc 0.671875, prec 0.0406818, recall 0.810796
2017-12-10T04:21:34.753346: step 1316, loss 1.5334, acc 0.65625, prec 0.040736, recall 0.811105
2017-12-10T04:21:35.023555: step 1317, loss 1.14462, acc 0.640625, prec 0.0407628, recall 0.81131
2017-12-10T04:21:35.285718: step 1318, loss 1.4094, acc 0.609375, prec 0.040735, recall 0.81131
2017-12-10T04:21:35.549046: step 1319, loss 1.6428, acc 0.625, prec 0.0407606, recall 0.811516
2017-12-10T04:21:35.817265: step 1320, loss 1.41993, acc 0.546875, prec 0.0407546, recall 0.811618
2017-12-10T04:21:36.079152: step 1321, loss 1.89346, acc 0.53125, prec 0.0407735, recall 0.811822
2017-12-10T04:21:36.337723: step 1322, loss 1.2324, acc 0.734375, prec 0.0407808, recall 0.811924
2017-12-10T04:21:36.597989: step 1323, loss 1.31965, acc 0.6875, prec 0.0407847, recall 0.812026
2017-12-10T04:21:36.859314: step 1324, loss 2.08727, acc 0.609375, prec 0.0408352, recall 0.812331
2017-12-10T04:21:37.119169: step 1325, loss 1.15632, acc 0.765625, prec 0.0409228, recall 0.812736
2017-12-10T04:21:37.384679: step 1326, loss 1.01945, acc 0.65625, prec 0.0408984, recall 0.812736
2017-12-10T04:21:37.642257: step 1327, loss 1.15505, acc 0.765625, prec 0.0409077, recall 0.812837
2017-12-10T04:21:37.905306: step 1328, loss 0.614446, acc 0.828125, prec 0.0409216, recall 0.812938
2017-12-10T04:21:38.169449: step 1329, loss 0.874074, acc 0.8125, prec 0.0409082, recall 0.812938
2017-12-10T04:21:38.428025: step 1330, loss 0.73006, acc 0.828125, prec 0.0410261, recall 0.813441
2017-12-10T04:21:38.692252: step 1331, loss 0.259381, acc 0.875, prec 0.0410172, recall 0.813441
2017-12-10T04:21:38.963828: step 1332, loss 0.623881, acc 0.8125, prec 0.0410298, recall 0.813541
2017-12-10T04:21:39.223377: step 1333, loss 0.988902, acc 0.90625, prec 0.0410491, recall 0.813641
2017-12-10T04:21:39.497155: step 1334, loss 0.311442, acc 0.921875, prec 0.0410436, recall 0.813641
2017-12-10T04:21:39.759850: step 1335, loss 0.455414, acc 0.9375, prec 0.0410651, recall 0.813741
2017-12-10T04:21:40.024722: step 1336, loss 0.11779, acc 0.953125, prec 0.0410618, recall 0.813741
2017-12-10T04:21:40.286715: step 1337, loss 3.66936, acc 0.921875, prec 0.0410833, recall 0.813405
2017-12-10T04:21:40.554266: step 1338, loss 4.42138, acc 0.9375, prec 0.0411059, recall 0.813069
2017-12-10T04:21:40.822667: step 1339, loss 5.05174, acc 0.8125, prec 0.0411456, recall 0.812834
2017-12-10T04:21:41.086819: step 1340, loss 0.405415, acc 0.875, prec 0.0411367, recall 0.812834
2017-12-10T04:21:41.347587: step 1341, loss 0.566518, acc 0.84375, prec 0.0411255, recall 0.812834
2017-12-10T04:21:41.606992: step 1342, loss 0.865031, acc 0.75, prec 0.0411337, recall 0.812934
2017-12-10T04:21:41.882436: step 1343, loss 0.77812, acc 0.703125, prec 0.0411126, recall 0.812934
2017-12-10T04:21:42.153206: step 1344, loss 1.30755, acc 0.65625, prec 0.041114, recall 0.813034
2017-12-10T04:21:42.415013: step 1345, loss 1.68467, acc 0.65625, prec 0.0411155, recall 0.813134
2017-12-10T04:21:42.680032: step 1346, loss 2.01327, acc 0.546875, prec 0.0411351, recall 0.813333
2017-12-10T04:21:42.938844: step 1347, loss 1.60863, acc 0.59375, prec 0.0411321, recall 0.813433
2017-12-10T04:21:43.207539: step 1348, loss 1.49181, acc 0.671875, prec 0.0411346, recall 0.813532
2017-12-10T04:21:43.468781: step 1349, loss 1.69681, acc 0.640625, prec 0.041135, recall 0.813632
2017-12-10T04:21:43.732741: step 1350, loss 1.22922, acc 0.734375, prec 0.0411678, recall 0.81383
2017-12-10T04:21:43.996779: step 1351, loss 0.890432, acc 0.71875, prec 0.0411478, recall 0.81383
2017-12-10T04:21:44.257897: step 1352, loss 2.01613, acc 0.6875, prec 0.0411773, recall 0.814028
2017-12-10T04:21:44.532032: step 1353, loss 0.797597, acc 0.765625, prec 0.0412122, recall 0.814225
2017-12-10T04:21:44.795656: step 1354, loss 1.59708, acc 0.75, prec 0.041246, recall 0.814422
2017-12-10T04:21:45.057321: step 1355, loss 0.783245, acc 0.765625, prec 0.0412294, recall 0.814422
2017-12-10T04:21:45.325375: step 1356, loss 0.712454, acc 0.734375, prec 0.0412106, recall 0.814422
2017-12-10T04:21:45.586777: step 1357, loss 0.798765, acc 0.796875, prec 0.0411962, recall 0.814422
2017-12-10T04:21:45.847850: step 1358, loss 0.897654, acc 0.78125, prec 0.0412064, recall 0.81452
2017-12-10T04:21:46.110416: step 1359, loss 0.57409, acc 0.859375, prec 0.0412736, recall 0.814815
2017-12-10T04:21:46.377695: step 1360, loss 0.733424, acc 0.75, prec 0.0412559, recall 0.814815
2017-12-10T04:21:46.636491: step 1361, loss 0.234745, acc 0.9375, prec 0.0412515, recall 0.814815
2017-12-10T04:21:46.895795: step 1362, loss 0.449516, acc 0.859375, prec 0.0412929, recall 0.815011
2017-12-10T04:21:47.156415: step 1363, loss 0.527775, acc 0.859375, prec 0.0413599, recall 0.815303
2017-12-10T04:21:47.418286: step 1364, loss 0.376413, acc 0.890625, prec 0.0414035, recall 0.815498
2017-12-10T04:21:47.684489: step 1365, loss 0.691197, acc 0.90625, prec 0.0414481, recall 0.815692
2017-12-10T04:21:47.949700: step 1366, loss 0.208628, acc 0.921875, prec 0.0414426, recall 0.815692
2017-12-10T04:21:48.210988: step 1367, loss 0.559188, acc 0.828125, prec 0.0414304, recall 0.815692
2017-12-10T04:21:48.475328: step 1368, loss 4.04053, acc 0.859375, prec 0.0414728, recall 0.815457
2017-12-10T04:21:48.748202: step 1369, loss 0.257359, acc 0.90625, prec 0.0414918, recall 0.815554
2017-12-10T04:21:49.010738: step 1370, loss 0.132812, acc 0.953125, prec 0.0415397, recall 0.815748
2017-12-10T04:21:49.275300: step 1371, loss 0.643299, acc 0.8125, prec 0.0415264, recall 0.815748
2017-12-10T04:21:49.534574: step 1372, loss 0.570221, acc 0.890625, prec 0.0415186, recall 0.815748
2017-12-10T04:21:49.798138: step 1373, loss 0.415811, acc 0.84375, prec 0.0415075, recall 0.815748
2017-12-10T04:21:50.060275: step 1374, loss 2.32066, acc 0.84375, prec 0.0415231, recall 0.815417
2017-12-10T04:21:50.324287: step 1375, loss 0.592179, acc 0.84375, prec 0.0415376, recall 0.815514
2017-12-10T04:21:50.592009: step 1376, loss 0.408626, acc 0.90625, prec 0.0415566, recall 0.81561
2017-12-10T04:21:50.851622: step 1377, loss 0.708968, acc 0.84375, prec 0.0415455, recall 0.81561
2017-12-10T04:21:51.115848: step 1378, loss 0.592743, acc 0.84375, prec 0.04156, recall 0.815707
2017-12-10T04:21:51.376020: step 1379, loss 0.247142, acc 0.890625, prec 0.0415522, recall 0.815707
2017-12-10T04:21:51.642971: step 1380, loss 0.827074, acc 0.796875, prec 0.0415889, recall 0.8159
2017-12-10T04:21:51.901231: step 1381, loss 0.740049, acc 0.765625, prec 0.0415723, recall 0.8159
2017-12-10T04:21:52.166979: step 1382, loss 1.72317, acc 0.84375, prec 0.0416123, recall 0.816092
2017-12-10T04:21:52.432439: step 1383, loss 0.399838, acc 0.875, prec 0.0416289, recall 0.816188
2017-12-10T04:21:52.692790: step 1384, loss 0.796554, acc 0.796875, prec 0.0416145, recall 0.816188
2017-12-10T04:21:52.954450: step 1385, loss 0.528628, acc 0.875, prec 0.0416057, recall 0.816188
2017-12-10T04:21:53.219351: step 1386, loss 3.03722, acc 0.796875, prec 0.0415924, recall 0.815762
2017-12-10T04:21:53.481494: step 1387, loss 0.77383, acc 0.78125, prec 0.0416024, recall 0.815858
2017-12-10T04:21:53.740858: step 1388, loss 0.874261, acc 0.78125, prec 0.0416124, recall 0.815954
2017-12-10T04:21:54.002086: step 1389, loss 1.09781, acc 0.765625, prec 0.0415958, recall 0.815954
2017-12-10T04:21:54.272128: step 1390, loss 1.07874, acc 0.765625, prec 0.0415792, recall 0.815954
2017-12-10T04:21:54.533921: step 1391, loss 0.473626, acc 0.890625, prec 0.0415969, recall 0.81605
2017-12-10T04:21:54.798998: step 1392, loss 0.640552, acc 0.8125, prec 0.0415837, recall 0.81605
2017-12-10T04:21:55.065679: step 1393, loss 0.80159, acc 0.796875, prec 0.0415693, recall 0.81605
2017-12-10T04:21:55.335518: step 1394, loss 0.482242, acc 0.828125, prec 0.0415572, recall 0.81605
2017-12-10T04:21:55.604399: step 1395, loss 0.743024, acc 0.796875, prec 0.0415429, recall 0.81605
2017-12-10T04:21:55.862461: step 1396, loss 0.767052, acc 0.828125, prec 0.0415816, recall 0.816242
2017-12-10T04:21:56.130852: step 1397, loss 0.400419, acc 0.890625, prec 0.0415993, recall 0.816337
2017-12-10T04:21:56.392553: step 1398, loss 0.481815, acc 0.875, prec 0.0416413, recall 0.816528
2017-12-10T04:21:56.654784: step 1399, loss 0.324577, acc 0.90625, prec 0.04166, recall 0.816623
2017-12-10T04:21:56.930690: step 1400, loss 0.17425, acc 0.921875, prec 0.0416545, recall 0.816623
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-1400

2017-12-10T04:21:58.449721: step 1401, loss 0.258588, acc 0.890625, prec 0.0416468, recall 0.816623
2017-12-10T04:21:58.711948: step 1402, loss 0.323819, acc 0.921875, prec 0.0416413, recall 0.816623
2017-12-10T04:21:58.979492: step 1403, loss 0.2817, acc 0.9375, prec 0.0416623, recall 0.816719
2017-12-10T04:21:59.243877: step 1404, loss 0.104279, acc 0.96875, prec 0.04166, recall 0.816719
2017-12-10T04:21:59.509037: step 1405, loss 0.11448, acc 0.921875, prec 0.0416545, recall 0.816719
2017-12-10T04:21:59.770153: step 1406, loss 0.304201, acc 0.953125, prec 0.0416766, recall 0.816814
2017-12-10T04:22:00.033291: step 1407, loss 0.120894, acc 0.9375, prec 0.0416722, recall 0.816814
2017-12-10T04:22:00.302297: step 1408, loss 0.450133, acc 0.953125, prec 0.0417196, recall 0.817004
2017-12-10T04:22:00.585374: step 1409, loss 6.22625, acc 0.9375, prec 0.0417163, recall 0.81658
2017-12-10T04:22:00.852929: step 1410, loss 7.85419, acc 0.890625, prec 0.0417097, recall 0.816157
2017-12-10T04:22:01.123760: step 1411, loss 0.406297, acc 0.90625, prec 0.0417031, recall 0.816157
2017-12-10T04:22:01.380515: step 1412, loss 0.505941, acc 0.875, prec 0.0417449, recall 0.816348
2017-12-10T04:22:01.643020: step 1413, loss 0.433644, acc 0.921875, prec 0.0418154, recall 0.816632
2017-12-10T04:22:01.902074: step 1414, loss 0.448283, acc 0.921875, prec 0.0418099, recall 0.816632
2017-12-10T04:22:02.164187: step 1415, loss 0.64116, acc 0.8125, prec 0.0417966, recall 0.816632
2017-12-10T04:22:02.427741: step 1416, loss 0.816813, acc 0.796875, prec 0.0418076, recall 0.816727
2017-12-10T04:22:02.695370: step 1417, loss 0.728815, acc 0.8125, prec 0.041845, recall 0.816916
2017-12-10T04:22:02.955933: step 1418, loss 9.14011, acc 0.828125, prec 0.0418603, recall 0.816169
2017-12-10T04:22:03.221378: step 1419, loss 0.905828, acc 0.71875, prec 0.041891, recall 0.816358
2017-12-10T04:22:03.488021: step 1420, loss 1.61499, acc 0.640625, prec 0.0418656, recall 0.816358
2017-12-10T04:22:03.749223: step 1421, loss 1.77864, acc 0.65625, prec 0.0418413, recall 0.816358
2017-12-10T04:22:04.020336: step 1422, loss 1.30236, acc 0.609375, prec 0.0418138, recall 0.816358
2017-12-10T04:22:04.280648: step 1423, loss 2.90133, acc 0.5625, prec 0.0418082, recall 0.816452
2017-12-10T04:22:04.538340: step 1424, loss 2.94947, acc 0.46875, prec 0.041796, recall 0.816547
2017-12-10T04:22:04.798377: step 1425, loss 2.19001, acc 0.578125, prec 0.0417915, recall 0.816641
2017-12-10T04:22:05.066719: step 1426, loss 3.03982, acc 0.484375, prec 0.0417553, recall 0.816641
2017-12-10T04:22:05.330505: step 1427, loss 1.77912, acc 0.53125, prec 0.0417224, recall 0.816641
2017-12-10T04:22:05.595393: step 1428, loss 2.02492, acc 0.546875, prec 0.0417158, recall 0.816735
2017-12-10T04:22:05.855925: step 1429, loss 2.52808, acc 0.546875, prec 0.0417343, recall 0.816923
2017-12-10T04:22:06.115506: step 1430, loss 1.34492, acc 0.671875, prec 0.0417114, recall 0.816923
2017-12-10T04:22:06.377964: step 1431, loss 1.0635, acc 0.75, prec 0.0416939, recall 0.816923
2017-12-10T04:22:06.638496: step 1432, loss 1.33228, acc 0.75, prec 0.0417768, recall 0.817298
2017-12-10T04:22:06.900470: step 1433, loss 1.03294, acc 0.828125, prec 0.0417898, recall 0.817391
2017-12-10T04:22:07.163174: step 1434, loss 0.888899, acc 0.828125, prec 0.0418028, recall 0.817485
2017-12-10T04:22:07.423927: step 1435, loss 0.880714, acc 0.828125, prec 0.0418159, recall 0.817578
2017-12-10T04:22:07.685962: step 1436, loss 0.464898, acc 0.875, prec 0.0418071, recall 0.817578
2017-12-10T04:22:07.949319: step 1437, loss 0.330657, acc 0.921875, prec 0.0418017, recall 0.817578
2017-12-10T04:22:08.212502: step 1438, loss 0.897717, acc 0.90625, prec 0.0418201, recall 0.817671
2017-12-10T04:22:08.483657: step 1439, loss 0.649225, acc 0.875, prec 0.0418364, recall 0.817764
2017-12-10T04:22:08.744202: step 1440, loss 0.134538, acc 0.9375, prec 0.0418571, recall 0.817857
2017-12-10T04:22:09.007691: step 1441, loss 1.67031, acc 0.875, prec 0.0418744, recall 0.817533
2017-12-10T04:22:09.272320: step 1442, loss 0.170065, acc 0.953125, prec 0.0418711, recall 0.817533
2017-12-10T04:22:09.534546: step 1443, loss 0.717424, acc 1, prec 0.0419462, recall 0.817812
2017-12-10T04:22:09.801564: step 1444, loss 0.12423, acc 0.953125, prec 0.0419429, recall 0.817812
2017-12-10T04:22:10.065833: step 1445, loss 0.363343, acc 0.890625, prec 0.0419602, recall 0.817904
2017-12-10T04:22:10.330981: step 1446, loss 3.09433, acc 0.890625, prec 0.0419537, recall 0.817489
2017-12-10T04:22:10.595908: step 1447, loss 0.185482, acc 0.9375, prec 0.0419493, recall 0.817489
2017-12-10T04:22:10.858402: step 1448, loss 1.37506, acc 0.921875, prec 0.0419699, recall 0.817166
2017-12-10T04:22:11.122194: step 1449, loss 0.669554, acc 0.828125, prec 0.0419828, recall 0.817259
2017-12-10T04:22:11.389528: step 1450, loss 0.347035, acc 0.890625, prec 0.0419752, recall 0.817259
2017-12-10T04:22:11.647213: step 1451, loss 0.790799, acc 0.75, prec 0.0419826, recall 0.817352
2017-12-10T04:22:11.918928: step 1452, loss 0.838121, acc 0.78125, prec 0.0419673, recall 0.817352
2017-12-10T04:22:12.182810: step 1453, loss 1.27881, acc 0.84375, prec 0.0419814, recall 0.817444
2017-12-10T04:22:12.457042: step 1454, loss 0.623621, acc 0.765625, prec 0.041965, recall 0.817444
2017-12-10T04:22:12.721999: step 1455, loss 0.686607, acc 0.828125, prec 0.0420028, recall 0.817629
2017-12-10T04:22:12.986224: step 1456, loss 1.40243, acc 0.65625, prec 0.0420037, recall 0.817722
2017-12-10T04:22:13.246494: step 1457, loss 0.811526, acc 0.78125, prec 0.0419884, recall 0.817722
2017-12-10T04:22:13.509803: step 1458, loss 0.783281, acc 0.796875, prec 0.0419991, recall 0.817814
2017-12-10T04:22:13.778576: step 1459, loss 0.879913, acc 0.78125, prec 0.0419838, recall 0.817814
2017-12-10T04:22:14.041408: step 1460, loss 0.5506, acc 0.859375, prec 0.0419989, recall 0.817906
2017-12-10T04:22:14.305740: step 1461, loss 1.18804, acc 0.78125, prec 0.0419836, recall 0.817906
2017-12-10T04:22:14.563472: step 1462, loss 1.13402, acc 0.703125, prec 0.0419629, recall 0.817906
2017-12-10T04:22:14.822544: step 1463, loss 0.52225, acc 0.84375, prec 0.0420266, recall 0.818182
2017-12-10T04:22:15.094781: step 1464, loss 0.393299, acc 0.875, prec 0.0420179, recall 0.818182
2017-12-10T04:22:15.355014: step 1465, loss 0.853571, acc 0.875, prec 0.0420589, recall 0.818365
2017-12-10T04:22:15.624873: step 1466, loss 0.459462, acc 0.875, prec 0.0420501, recall 0.818365
2017-12-10T04:22:15.892072: step 1467, loss 0.303113, acc 0.890625, prec 0.0420673, recall 0.818457
2017-12-10T04:22:16.151985: step 1468, loss 0.359116, acc 0.90625, prec 0.0420856, recall 0.818548
2017-12-10T04:22:16.416993: step 1469, loss 0.396663, acc 0.9375, prec 0.0421557, recall 0.818822
2017-12-10T04:22:16.674198: step 1470, loss 1.00156, acc 0.875, prec 0.0421966, recall 0.819005
2017-12-10T04:22:16.936210: step 1471, loss 0.173672, acc 0.921875, prec 0.0422159, recall 0.819095
2017-12-10T04:22:17.201873: step 1472, loss 0.237858, acc 0.90625, prec 0.0422094, recall 0.819095
2017-12-10T04:22:17.462088: step 1473, loss 3.21946, acc 0.90625, prec 0.0422287, recall 0.818775
2017-12-10T04:22:17.733890: step 1474, loss 0.128132, acc 0.921875, prec 0.0422233, recall 0.818775
2017-12-10T04:22:17.998871: step 1475, loss 0.875429, acc 0.875, prec 0.0422393, recall 0.818866
2017-12-10T04:22:18.267068: step 1476, loss 0.445394, acc 0.90625, prec 0.0422327, recall 0.818866
2017-12-10T04:22:18.534855: step 1477, loss 0.574344, acc 0.9375, prec 0.0422532, recall 0.818957
2017-12-10T04:22:18.800066: step 1478, loss 0.182281, acc 0.9375, prec 0.0422488, recall 0.818957
2017-12-10T04:22:19.064837: step 1479, loss 0.450546, acc 0.875, prec 0.04224, recall 0.818957
2017-12-10T04:22:19.326539: step 1480, loss 0.180289, acc 0.9375, prec 0.0422357, recall 0.818957
2017-12-10T04:22:19.593676: step 1481, loss 0.350033, acc 0.828125, prec 0.0422237, recall 0.818957
2017-12-10T04:22:19.855194: step 1482, loss 1.24441, acc 0.84375, prec 0.0422375, recall 0.819048
2017-12-10T04:22:20.126043: step 1483, loss 0.373117, acc 0.890625, prec 0.0422299, recall 0.819048
2017-12-10T04:22:20.394398: step 1484, loss 0.599064, acc 0.8125, prec 0.0422415, recall 0.819138
2017-12-10T04:22:20.660124: step 1485, loss 0.660044, acc 0.875, prec 0.0422823, recall 0.819319
2017-12-10T04:22:20.921069: step 1486, loss 0.663279, acc 0.828125, prec 0.042295, recall 0.81941
2017-12-10T04:22:21.188482: step 1487, loss 0.861958, acc 0.9375, prec 0.0423401, recall 0.81959
2017-12-10T04:22:21.451460: step 1488, loss 0.838331, acc 0.828125, prec 0.0423528, recall 0.81968
2017-12-10T04:22:21.710122: step 1489, loss 0.784, acc 0.796875, prec 0.0423386, recall 0.81968
2017-12-10T04:22:21.972664: step 1490, loss 0.443763, acc 0.859375, prec 0.0423287, recall 0.81968
2017-12-10T04:22:22.200745: step 1491, loss 0.518302, acc 0.846154, prec 0.04232, recall 0.81968
2017-12-10T04:22:22.476840: step 1492, loss 0.490858, acc 0.890625, prec 0.042337, recall 0.81977
2017-12-10T04:22:22.741352: step 1493, loss 0.165538, acc 0.96875, prec 0.0423349, recall 0.81977
2017-12-10T04:22:23.002590: step 1494, loss 0.364608, acc 0.875, prec 0.0423508, recall 0.81986
2017-12-10T04:22:23.266248: step 1495, loss 0.528181, acc 0.890625, prec 0.0423679, recall 0.81995
2017-12-10T04:22:23.526289: step 1496, loss 0.361644, acc 0.90625, prec 0.0423613, recall 0.81995
2017-12-10T04:22:23.791707: step 1497, loss 2.47076, acc 0.921875, prec 0.0423816, recall 0.819631
2017-12-10T04:22:24.069275: step 1498, loss 0.723913, acc 0.921875, prec 0.0424255, recall 0.819811
2017-12-10T04:22:24.339942: step 1499, loss 0.900833, acc 0.890625, prec 0.0424918, recall 0.82008
2017-12-10T04:22:24.606177: step 1500, loss 4.4027, acc 0.859375, prec 0.0425077, recall 0.819762

Evaluation:
2017-12-10T04:22:32.267210: step 1500, loss 1.55736, acc 0.870164, prec 0.0436423, recall 0.813136

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-1500

2017-12-10T04:22:33.571894: step 1501, loss 0.330645, acc 0.859375, prec 0.04368, recall 0.813309
2017-12-10T04:22:33.835956: step 1502, loss 0.456116, acc 0.875, prec 0.0437188, recall 0.813481
2017-12-10T04:22:34.107831: step 1503, loss 0.758535, acc 0.90625, prec 0.043736, recall 0.813567
2017-12-10T04:22:34.387849: step 1504, loss 0.816647, acc 0.8125, prec 0.0437942, recall 0.813825
2017-12-10T04:22:34.651313: step 1505, loss 1.15647, acc 0.75, prec 0.0437768, recall 0.813825
2017-12-10T04:22:34.911840: step 1506, loss 1.28255, acc 0.640625, prec 0.0437755, recall 0.813911
2017-12-10T04:22:35.175370: step 1507, loss 1.25535, acc 0.71875, prec 0.0437797, recall 0.813996
2017-12-10T04:22:35.436721: step 1508, loss 0.847564, acc 0.75, prec 0.043786, recall 0.814082
2017-12-10T04:22:35.701767: step 1509, loss 0.848578, acc 0.78125, prec 0.0438419, recall 0.814338
2017-12-10T04:22:35.958561: step 1510, loss 0.82077, acc 0.828125, prec 0.0438299, recall 0.814338
2017-12-10T04:22:36.217817: step 1511, loss 0.718876, acc 0.8125, prec 0.0438406, recall 0.814424
2017-12-10T04:22:36.482198: step 1512, loss 1.29719, acc 0.734375, prec 0.0438221, recall 0.814424
2017-12-10T04:22:36.743126: step 1513, loss 1.09919, acc 0.71875, prec 0.0438499, recall 0.814594
2017-12-10T04:22:37.003385: step 1514, loss 0.751671, acc 0.8125, prec 0.0438605, recall 0.814679
2017-12-10T04:22:37.265552: step 1515, loss 0.609588, acc 0.828125, prec 0.0438486, recall 0.814679
2017-12-10T04:22:37.530267: step 1516, loss 0.287514, acc 0.875, prec 0.0438635, recall 0.814764
2017-12-10T04:22:37.792808: step 1517, loss 2.83621, acc 0.859375, prec 0.0438549, recall 0.81439
2017-12-10T04:22:38.057904: step 1518, loss 0.530258, acc 0.84375, prec 0.0438441, recall 0.81439
2017-12-10T04:22:38.325025: step 1519, loss 0.685971, acc 0.8125, prec 0.0438547, recall 0.814475
2017-12-10T04:22:38.583972: step 1520, loss 0.652995, acc 0.78125, prec 0.0438867, recall 0.814645
2017-12-10T04:22:38.845665: step 1521, loss 0.324873, acc 0.875, prec 0.043878, recall 0.814645
2017-12-10T04:22:39.112059: step 1522, loss 0.538459, acc 0.84375, prec 0.0438672, recall 0.814645
2017-12-10T04:22:39.381841: step 1523, loss 1.71281, acc 0.90625, prec 0.0438618, recall 0.814273
2017-12-10T04:22:39.645341: step 1524, loss 0.499453, acc 0.90625, prec 0.0438553, recall 0.814273
2017-12-10T04:22:39.905503: step 1525, loss 0.739574, acc 0.828125, prec 0.0438434, recall 0.814273
2017-12-10T04:22:40.171938: step 1526, loss 0.565934, acc 0.890625, prec 0.0438594, recall 0.814358
2017-12-10T04:22:40.434538: step 1527, loss 0.282366, acc 0.890625, prec 0.0438754, recall 0.814442
2017-12-10T04:22:40.695392: step 1528, loss 0.670506, acc 0.875, prec 0.0439138, recall 0.814612
2017-12-10T04:22:40.957869: step 1529, loss 0.234369, acc 0.90625, prec 0.0439074, recall 0.814612
2017-12-10T04:22:41.220657: step 1530, loss 1.80472, acc 0.875, prec 0.0439469, recall 0.814409
2017-12-10T04:22:41.487972: step 1531, loss 0.562754, acc 0.796875, prec 0.0439328, recall 0.814409
2017-12-10T04:22:41.747712: step 1532, loss 0.275145, acc 0.953125, prec 0.0439296, recall 0.814409
2017-12-10T04:22:42.025835: step 1533, loss 0.350652, acc 0.9375, prec 0.0439487, recall 0.814494
2017-12-10T04:22:42.296866: step 1534, loss 0.162235, acc 0.953125, prec 0.043969, recall 0.814579
2017-12-10T04:22:42.566806: step 1535, loss 0.0989525, acc 0.9375, prec 0.0439647, recall 0.814579
2017-12-10T04:22:42.840539: step 1536, loss 0.265248, acc 0.953125, prec 0.0439614, recall 0.814579
2017-12-10T04:22:43.105401: step 1537, loss 0.202896, acc 0.890625, prec 0.0439774, recall 0.814663
2017-12-10T04:22:43.371068: step 1538, loss 0.438834, acc 0.875, prec 0.0439687, recall 0.814663
2017-12-10T04:22:43.633054: step 1539, loss 1.13157, acc 0.90625, prec 0.0439857, recall 0.814747
2017-12-10T04:22:43.907484: step 1540, loss 0.0616899, acc 0.96875, prec 0.0439836, recall 0.814747
2017-12-10T04:22:44.167171: step 1541, loss 0.706736, acc 0.796875, prec 0.0439695, recall 0.814747
2017-12-10T04:22:44.427821: step 1542, loss 0.335508, acc 0.90625, prec 0.0439865, recall 0.814832
2017-12-10T04:22:44.691799: step 1543, loss 7.09356, acc 0.84375, prec 0.0440238, recall 0.81463
2017-12-10T04:22:44.957025: step 1544, loss 0.211014, acc 0.890625, prec 0.0440397, recall 0.814714
2017-12-10T04:22:45.216504: step 1545, loss 0.383026, acc 0.90625, prec 0.0440801, recall 0.814882
2017-12-10T04:22:45.481901: step 1546, loss 0.598862, acc 0.84375, prec 0.0440693, recall 0.814882
2017-12-10T04:22:45.750744: step 1547, loss 0.545598, acc 0.875, prec 0.0440606, recall 0.814882
2017-12-10T04:22:46.019091: step 1548, loss 0.732212, acc 0.796875, prec 0.0440935, recall 0.81505
2017-12-10T04:22:46.284350: step 1549, loss 0.957954, acc 0.796875, prec 0.0441029, recall 0.815134
2017-12-10T04:22:46.545142: step 1550, loss 0.859352, acc 0.796875, prec 0.0441357, recall 0.815301
2017-12-10T04:22:46.806914: step 1551, loss 0.979918, acc 0.828125, prec 0.0441238, recall 0.815301
2017-12-10T04:22:47.076622: step 1552, loss 0.375058, acc 0.859375, prec 0.0441375, recall 0.815385
2017-12-10T04:22:47.341059: step 1553, loss 0.470824, acc 0.8125, prec 0.0441479, recall 0.815468
2017-12-10T04:22:47.604911: step 1554, loss 0.8535, acc 0.828125, prec 0.0441828, recall 0.815635
2017-12-10T04:22:47.870253: step 1555, loss 0.40774, acc 0.875, prec 0.0442209, recall 0.815801
2017-12-10T04:22:48.134229: step 1556, loss 1.40264, acc 0.890625, prec 0.0442835, recall 0.81605
2017-12-10T04:22:48.404629: step 1557, loss 0.637508, acc 0.828125, prec 0.0442716, recall 0.81605
2017-12-10T04:22:48.672882: step 1558, loss 0.429152, acc 0.828125, prec 0.0442597, recall 0.81605
2017-12-10T04:22:48.940834: step 1559, loss 0.355833, acc 0.90625, prec 0.0442532, recall 0.81605
2017-12-10T04:22:49.204322: step 1560, loss 0.56008, acc 0.796875, prec 0.0442391, recall 0.81605
2017-12-10T04:22:49.469512: step 1561, loss 0.414888, acc 0.875, prec 0.0442772, recall 0.816216
2017-12-10T04:22:49.736799: step 1562, loss 0.26046, acc 0.9375, prec 0.0443196, recall 0.816382
2017-12-10T04:22:50.006319: step 1563, loss 0.370333, acc 0.90625, prec 0.0443131, recall 0.816382
2017-12-10T04:22:50.277557: step 1564, loss 2.79888, acc 0.890625, prec 0.0443299, recall 0.816097
2017-12-10T04:22:50.542054: step 1565, loss 1.05923, acc 0.828125, prec 0.0443414, recall 0.81618
2017-12-10T04:22:50.799626: step 1566, loss 0.759517, acc 0.875, prec 0.044356, recall 0.816262
2017-12-10T04:22:51.060593: step 1567, loss 0.574373, acc 0.859375, prec 0.0443696, recall 0.816345
2017-12-10T04:22:51.326085: step 1568, loss 0.162243, acc 0.9375, prec 0.0443653, recall 0.816345
2017-12-10T04:22:51.586742: step 1569, loss 0.498739, acc 0.875, prec 0.0443566, recall 0.816345
2017-12-10T04:22:51.850646: step 1570, loss 0.629459, acc 0.828125, prec 0.0443447, recall 0.816345
2017-12-10T04:22:52.111627: step 1571, loss 0.36108, acc 0.890625, prec 0.0443604, recall 0.816427
2017-12-10T04:22:52.370636: step 1572, loss 0.323487, acc 0.921875, prec 0.044355, recall 0.816427
2017-12-10T04:22:52.636646: step 1573, loss 0.356151, acc 0.875, prec 0.0443697, recall 0.81651
2017-12-10T04:22:52.914372: step 1574, loss 0.384059, acc 0.8125, prec 0.0443567, recall 0.81651
2017-12-10T04:22:53.177484: step 1575, loss 0.26955, acc 0.890625, prec 0.0443957, recall 0.816674
2017-12-10T04:22:53.436787: step 1576, loss 5.88429, acc 0.859375, prec 0.0444336, recall 0.816473
2017-12-10T04:22:53.703142: step 1577, loss 0.266725, acc 0.90625, prec 0.0444271, recall 0.816473
2017-12-10T04:22:53.963502: step 1578, loss 0.259233, acc 0.921875, prec 0.0444217, recall 0.816473
2017-12-10T04:22:54.233095: step 1579, loss 0.363164, acc 0.921875, prec 0.0444396, recall 0.816555
2017-12-10T04:22:54.498546: step 1580, loss 0.791147, acc 0.765625, prec 0.0444699, recall 0.816719
2017-12-10T04:22:54.755853: step 1581, loss 0.760196, acc 0.828125, prec 0.0444812, recall 0.816801
2017-12-10T04:22:55.020339: step 1582, loss 0.8064, acc 0.734375, prec 0.0444861, recall 0.816883
2017-12-10T04:22:55.286970: step 1583, loss 4.80345, acc 0.84375, prec 0.044546, recall 0.816763
2017-12-10T04:22:55.559526: step 1584, loss 0.610811, acc 0.890625, prec 0.0445617, recall 0.816845
2017-12-10T04:22:55.829700: step 1585, loss 0.915375, acc 0.75, prec 0.0445676, recall 0.816926
2017-12-10T04:22:56.090355: step 1586, loss 0.784966, acc 0.765625, prec 0.0445977, recall 0.817089
2017-12-10T04:22:56.350193: step 1587, loss 0.741132, acc 0.828125, prec 0.044609, recall 0.817171
2017-12-10T04:22:56.616667: step 1588, loss 0.864704, acc 0.828125, prec 0.0445971, recall 0.817171
2017-12-10T04:22:56.882565: step 1589, loss 0.603635, acc 0.8125, prec 0.0445841, recall 0.817171
2017-12-10T04:22:57.146365: step 1590, loss 0.566549, acc 0.875, prec 0.0446218, recall 0.817333
2017-12-10T04:22:57.415499: step 1591, loss 0.511039, acc 0.796875, prec 0.0446309, recall 0.817414
2017-12-10T04:22:57.685991: step 1592, loss 0.605848, acc 0.796875, prec 0.0446632, recall 0.817577
2017-12-10T04:22:57.952674: step 1593, loss 0.615752, acc 0.859375, prec 0.0447229, recall 0.817819
2017-12-10T04:22:58.212695: step 1594, loss 0.746943, acc 0.8125, prec 0.0447099, recall 0.817819
2017-12-10T04:22:58.472635: step 1595, loss 0.387502, acc 0.859375, prec 0.0447002, recall 0.817819
2017-12-10T04:22:58.734340: step 1596, loss 0.523245, acc 0.859375, prec 0.0447367, recall 0.817981
2017-12-10T04:22:58.997478: step 1597, loss 0.293416, acc 0.90625, prec 0.0447302, recall 0.817981
2017-12-10T04:22:59.255731: step 1598, loss 0.646097, acc 0.859375, prec 0.0447436, recall 0.818061
2017-12-10T04:22:59.524956: step 1599, loss 0.372906, acc 0.890625, prec 0.044736, recall 0.818061
2017-12-10T04:22:59.784373: step 1600, loss 0.17396, acc 0.953125, prec 0.0447328, recall 0.818061
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-1600

2017-12-10T04:23:01.127342: step 1601, loss 0.254897, acc 0.921875, prec 0.0447274, recall 0.818061
2017-12-10T04:23:01.387857: step 1602, loss 0.302467, acc 0.921875, prec 0.0447451, recall 0.818142
2017-12-10T04:23:01.648886: step 1603, loss 0.180495, acc 0.921875, prec 0.0447396, recall 0.818142
2017-12-10T04:23:01.913521: step 1604, loss 0.282892, acc 0.9375, prec 0.0447584, recall 0.818222
2017-12-10T04:23:02.179397: step 1605, loss 4.48109, acc 0.90625, prec 0.044753, recall 0.81786
2017-12-10T04:23:02.453648: step 1606, loss 0.273553, acc 0.921875, prec 0.0447476, recall 0.81786
2017-12-10T04:23:02.712389: step 1607, loss 0.211373, acc 0.90625, prec 0.0447411, recall 0.81786
2017-12-10T04:23:02.971492: step 1608, loss 0.0108228, acc 1, prec 0.0447642, recall 0.817941
2017-12-10T04:23:03.244091: step 1609, loss 0.297148, acc 0.921875, prec 0.0447588, recall 0.817941
2017-12-10T04:23:03.507852: step 1610, loss 0.532367, acc 0.96875, prec 0.0448028, recall 0.818102
2017-12-10T04:23:03.772946: step 1611, loss 2.49913, acc 0.921875, prec 0.0447985, recall 0.81774
2017-12-10T04:23:04.037378: step 1612, loss 0.0944839, acc 0.984375, prec 0.0447974, recall 0.81774
2017-12-10T04:23:04.300219: step 1613, loss 0.63396, acc 0.875, prec 0.0448349, recall 0.817901
2017-12-10T04:23:04.569095: step 1614, loss 0.208251, acc 0.921875, prec 0.0448295, recall 0.817901
2017-12-10T04:23:04.831428: step 1615, loss 0.375627, acc 0.953125, prec 0.0448955, recall 0.818142
2017-12-10T04:23:05.094466: step 1616, loss 0.353385, acc 0.875, prec 0.0448868, recall 0.818142
2017-12-10T04:23:05.359093: step 1617, loss 0.702759, acc 0.8125, prec 0.0448738, recall 0.818142
2017-12-10T04:23:05.632065: step 1618, loss 0.843141, acc 0.78125, prec 0.0448586, recall 0.818142
2017-12-10T04:23:05.892057: step 1619, loss 0.610402, acc 0.84375, prec 0.0448478, recall 0.818142
2017-12-10T04:23:06.166970: step 1620, loss 0.720682, acc 0.796875, prec 0.0448337, recall 0.818142
2017-12-10T04:23:06.435891: step 1621, loss 0.533855, acc 0.90625, prec 0.0448733, recall 0.818302
2017-12-10T04:23:06.702590: step 1622, loss 0.341618, acc 0.90625, prec 0.0448668, recall 0.818302
2017-12-10T04:23:06.968245: step 1623, loss 0.721992, acc 0.8125, prec 0.0448539, recall 0.818302
2017-12-10T04:23:07.230271: step 1624, loss 0.324294, acc 0.890625, prec 0.0448463, recall 0.818302
2017-12-10T04:23:07.493158: step 1625, loss 0.400033, acc 0.890625, prec 0.0448617, recall 0.818382
2017-12-10T04:23:07.758580: step 1626, loss 0.567754, acc 0.828125, prec 0.0448499, recall 0.818382
2017-12-10T04:23:08.025806: step 1627, loss 0.320046, acc 0.890625, prec 0.0448883, recall 0.818541
2017-12-10T04:23:08.294622: step 1628, loss 0.486531, acc 0.875, prec 0.0449027, recall 0.818621
2017-12-10T04:23:08.557046: step 1629, loss 1.10698, acc 0.9375, prec 0.0449444, recall 0.81878
2017-12-10T04:23:08.822957: step 1630, loss 0.305867, acc 0.90625, prec 0.0449379, recall 0.81878
2017-12-10T04:23:09.088765: step 1631, loss 0.393272, acc 0.875, prec 0.0449292, recall 0.81878
2017-12-10T04:23:09.350400: step 1632, loss 0.202328, acc 0.9375, prec 0.0449939, recall 0.819018
2017-12-10T04:23:09.609566: step 1633, loss 0.305114, acc 0.90625, prec 0.0450104, recall 0.819098
2017-12-10T04:23:09.872580: step 1634, loss 4.61561, acc 0.859375, prec 0.0450017, recall 0.818739
2017-12-10T04:23:10.136080: step 1635, loss 0.858201, acc 0.9375, prec 0.0450433, recall 0.818898
2017-12-10T04:23:10.404589: step 1636, loss 0.246848, acc 0.921875, prec 0.0450379, recall 0.818898
2017-12-10T04:23:10.667875: step 1637, loss 0.206331, acc 0.921875, prec 0.0450325, recall 0.818898
2017-12-10T04:23:10.929339: step 1638, loss 1.24894, acc 0.890625, prec 0.0450938, recall 0.819135
2017-12-10T04:23:11.193982: step 1639, loss 0.504016, acc 0.890625, prec 0.0451321, recall 0.819293
2017-12-10T04:23:11.460324: step 1640, loss 0.281887, acc 0.890625, prec 0.0451245, recall 0.819293
2017-12-10T04:23:11.725306: step 1641, loss 0.564261, acc 0.796875, prec 0.0451563, recall 0.81945
2017-12-10T04:23:11.994623: step 1642, loss 0.656824, acc 0.84375, prec 0.0451684, recall 0.819529
2017-12-10T04:23:12.255031: step 1643, loss 0.936722, acc 0.796875, prec 0.0451543, recall 0.819529
2017-12-10T04:23:12.518123: step 1644, loss 1.38834, acc 0.734375, prec 0.0451588, recall 0.819608
2017-12-10T04:23:12.779686: step 1645, loss 0.681039, acc 0.84375, prec 0.0452396, recall 0.819922
2017-12-10T04:23:13.041572: step 1646, loss 0.795817, acc 0.8125, prec 0.0452495, recall 0.82
2017-12-10T04:23:13.303222: step 1647, loss 1.07061, acc 0.765625, prec 0.0452561, recall 0.820078
2017-12-10T04:23:13.571605: step 1648, loss 0.94679, acc 0.765625, prec 0.0452399, recall 0.820078
2017-12-10T04:23:13.832226: step 1649, loss 0.348727, acc 0.859375, prec 0.045253, recall 0.820156
2017-12-10T04:23:14.101086: step 1650, loss 0.230166, acc 0.921875, prec 0.0452704, recall 0.820234
2017-12-10T04:23:14.374040: step 1651, loss 0.845893, acc 0.765625, prec 0.0452771, recall 0.820312
2017-12-10T04:23:15.358203: step 1652, loss 0.513226, acc 0.8125, prec 0.045264, recall 0.820312
2017-12-10T04:23:15.720087: step 1653, loss 0.415495, acc 0.828125, prec 0.0452521, recall 0.820312
2017-12-10T04:23:16.423049: step 1654, loss 2.45528, acc 0.859375, prec 0.0452435, recall 0.819957
2017-12-10T04:23:17.191290: step 1655, loss 0.57148, acc 0.84375, prec 0.0452326, recall 0.819957
2017-12-10T04:23:17.926621: step 1656, loss 0.486385, acc 0.875, prec 0.0452697, recall 0.820113
2017-12-10T04:23:18.646247: step 1657, loss 0.320987, acc 0.890625, prec 0.0452621, recall 0.820113
2017-12-10T04:23:19.380588: step 1658, loss 0.443731, acc 0.828125, prec 0.0452502, recall 0.820113
2017-12-10T04:23:20.136280: step 1659, loss 0.344712, acc 0.875, prec 0.0452415, recall 0.820113
2017-12-10T04:23:20.851636: step 1660, loss 0.269568, acc 0.890625, prec 0.0452568, recall 0.820191
2017-12-10T04:23:21.556818: step 1661, loss 0.235743, acc 0.90625, prec 0.0452731, recall 0.820269
2017-12-10T04:23:22.256020: step 1662, loss 0.23083, acc 0.953125, prec 0.0452699, recall 0.820269
2017-12-10T04:23:22.978223: step 1663, loss 0.18128, acc 0.953125, prec 0.0452894, recall 0.820346
2017-12-10T04:23:24.096487: step 1664, loss 0.0690544, acc 0.953125, prec 0.0452862, recall 0.820346
2017-12-10T04:23:24.449720: step 1665, loss 0.172159, acc 0.921875, prec 0.0452808, recall 0.820346
2017-12-10T04:23:24.812278: step 1666, loss 0.336461, acc 0.9375, prec 0.0452992, recall 0.820424
2017-12-10T04:23:25.100505: step 1667, loss 0.482719, acc 0.875, prec 0.0452906, recall 0.820424
2017-12-10T04:23:25.394830: step 1668, loss 0.0462681, acc 0.96875, prec 0.0452884, recall 0.820424
2017-12-10T04:23:25.690802: step 1669, loss 0.030858, acc 0.984375, prec 0.0452873, recall 0.820424
2017-12-10T04:23:25.984696: step 1670, loss 0.137105, acc 0.984375, prec 0.0453091, recall 0.820502
2017-12-10T04:23:26.274733: step 1671, loss 0.203401, acc 0.953125, prec 0.0453286, recall 0.820579
2017-12-10T04:23:26.545987: step 1672, loss 0.88915, acc 0.984375, prec 0.0453731, recall 0.820734
2017-12-10T04:23:26.823286: step 1673, loss 0.100895, acc 0.96875, prec 0.045371, recall 0.820734
2017-12-10T04:23:27.098015: step 1674, loss 0.0360727, acc 0.984375, prec 0.0453699, recall 0.820734
2017-12-10T04:23:27.365149: step 1675, loss 0.570158, acc 0.96875, prec 0.0454133, recall 0.820889
2017-12-10T04:23:27.645495: step 1676, loss 2.1236, acc 0.96875, prec 0.0454122, recall 0.820535
2017-12-10T04:23:27.921669: step 1677, loss 0.0116882, acc 1, prec 0.0454122, recall 0.820535
2017-12-10T04:23:28.195269: step 1678, loss 0.0997374, acc 0.984375, prec 0.0454111, recall 0.820535
2017-12-10T04:23:28.470009: step 1679, loss 7.60847, acc 0.890625, prec 0.0454502, recall 0.820336
2017-12-10T04:23:28.745370: step 1680, loss 0.346592, acc 0.875, prec 0.0454415, recall 0.820336
2017-12-10T04:23:29.017168: step 1681, loss 0.700675, acc 0.84375, prec 0.045499, recall 0.820568
2017-12-10T04:23:29.274430: step 1682, loss 0.450521, acc 0.78125, prec 0.0454838, recall 0.820568
2017-12-10T04:23:29.553703: step 1683, loss 0.471976, acc 0.859375, prec 0.0455423, recall 0.820799
2017-12-10T04:23:29.816353: step 1684, loss 0.510181, acc 0.859375, prec 0.0455553, recall 0.820876
2017-12-10T04:23:30.082411: step 1685, loss 0.53617, acc 0.796875, prec 0.0455412, recall 0.820876
2017-12-10T04:23:30.352116: step 1686, loss 0.925634, acc 0.71875, prec 0.0455671, recall 0.82103
2017-12-10T04:23:30.614910: step 1687, loss 0.616761, acc 0.78125, prec 0.0455747, recall 0.821107
2017-12-10T04:23:30.880173: step 1688, loss 0.870274, acc 0.78125, prec 0.0455595, recall 0.821107
2017-12-10T04:23:31.139549: step 1689, loss 0.74662, acc 0.828125, prec 0.0455476, recall 0.821107
2017-12-10T04:23:31.401576: step 1690, loss 0.39164, acc 0.890625, prec 0.0455854, recall 0.82126
2017-12-10T04:23:31.667649: step 1691, loss 0.621332, acc 0.796875, prec 0.0456167, recall 0.821413
2017-12-10T04:23:31.928895: step 1692, loss 0.53203, acc 0.828125, prec 0.0456275, recall 0.82149
2017-12-10T04:23:32.192768: step 1693, loss 0.374854, acc 0.890625, prec 0.0456199, recall 0.82149
2017-12-10T04:23:32.458821: step 1694, loss 0.46956, acc 0.875, prec 0.0456339, recall 0.821566
2017-12-10T04:23:32.724941: step 1695, loss 0.38116, acc 0.859375, prec 0.0456241, recall 0.821566
2017-12-10T04:23:32.988245: step 1696, loss 0.561096, acc 0.84375, prec 0.045636, recall 0.821642
2017-12-10T04:23:33.248404: step 1697, loss 0.286239, acc 0.921875, prec 0.0456305, recall 0.821642
2017-12-10T04:23:33.507989: step 1698, loss 0.0880676, acc 0.984375, prec 0.0456748, recall 0.821795
2017-12-10T04:23:33.777409: step 1699, loss 0.20541, acc 0.9375, prec 0.0456704, recall 0.821795
2017-12-10T04:23:34.046041: step 1700, loss 3.18409, acc 0.953125, prec 0.0456683, recall 0.821444
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-1700

2017-12-10T04:23:35.358726: step 1701, loss 0.25939, acc 0.90625, prec 0.0456618, recall 0.821444
2017-12-10T04:23:35.627752: step 1702, loss 1.09427, acc 0.90625, prec 0.0456779, recall 0.82152
2017-12-10T04:23:35.894623: step 1703, loss 1.91825, acc 0.8125, prec 0.045666, recall 0.821169
2017-12-10T04:23:36.156900: step 1704, loss 0.172448, acc 0.953125, prec 0.0456627, recall 0.821169
2017-12-10T04:23:36.415695: step 1705, loss 0.712901, acc 0.890625, prec 0.0456778, recall 0.821246
2017-12-10T04:23:36.686967: step 1706, loss 0.290363, acc 0.921875, prec 0.045695, recall 0.821322
2017-12-10T04:23:36.945552: step 1707, loss 0.560499, acc 0.84375, prec 0.0457521, recall 0.82155
2017-12-10T04:23:37.207510: step 1708, loss 0.389004, acc 0.84375, prec 0.0457865, recall 0.821702
2017-12-10T04:23:37.468806: step 1709, loss 0.513188, acc 0.828125, prec 0.0457746, recall 0.821702
2017-12-10T04:23:37.728060: step 1710, loss 0.540086, acc 0.875, prec 0.0457885, recall 0.821778
2017-12-10T04:23:37.991542: step 1711, loss 0.778011, acc 0.8125, prec 0.0458207, recall 0.821929
2017-12-10T04:23:38.254504: step 1712, loss 0.441842, acc 0.859375, prec 0.0458561, recall 0.822081
2017-12-10T04:23:38.515186: step 1713, loss 0.497229, acc 0.875, prec 0.0458474, recall 0.822081
2017-12-10T04:23:38.784370: step 1714, loss 0.932665, acc 0.765625, prec 0.0458312, recall 0.822081
2017-12-10T04:23:39.043417: step 1715, loss 0.368776, acc 0.875, prec 0.0458225, recall 0.822081
2017-12-10T04:23:39.307994: step 1716, loss 0.170823, acc 0.9375, prec 0.0458407, recall 0.822156
2017-12-10T04:23:39.575846: step 1717, loss 0.403483, acc 0.84375, prec 0.0458299, recall 0.822156
2017-12-10T04:23:39.841235: step 1718, loss 0.864257, acc 0.90625, prec 0.045846, recall 0.822232
2017-12-10T04:23:40.101131: step 1719, loss 0.489836, acc 0.765625, prec 0.0458297, recall 0.822232
2017-12-10T04:23:40.363184: step 1720, loss 0.233819, acc 0.90625, prec 0.0458232, recall 0.822232
2017-12-10T04:23:40.626430: step 1721, loss 0.582065, acc 0.859375, prec 0.045836, recall 0.822307
2017-12-10T04:23:40.893515: step 1722, loss 0.329784, acc 0.90625, prec 0.0458295, recall 0.822307
2017-12-10T04:23:41.155323: step 1723, loss 0.231367, acc 0.890625, prec 0.0458445, recall 0.822382
2017-12-10T04:23:41.427295: step 1724, loss 0.479872, acc 0.875, prec 0.0458583, recall 0.822458
2017-12-10T04:23:41.695108: step 1725, loss 0.226462, acc 0.9375, prec 0.045854, recall 0.822458
2017-12-10T04:23:41.961865: step 1726, loss 0.676458, acc 0.921875, prec 0.0458711, recall 0.822533
2017-12-10T04:23:42.228378: step 1727, loss 0.299653, acc 0.890625, prec 0.0459086, recall 0.822683
2017-12-10T04:23:42.495614: step 1728, loss 0.216443, acc 0.953125, prec 0.0459729, recall 0.822908
2017-12-10T04:23:42.761402: step 1729, loss 0.155635, acc 0.9375, prec 0.0459686, recall 0.822908
2017-12-10T04:23:43.025289: step 1730, loss 0.485552, acc 0.90625, prec 0.0459846, recall 0.822983
2017-12-10T04:23:43.287079: step 1731, loss 0.438461, acc 0.84375, prec 0.0459963, recall 0.823057
2017-12-10T04:23:43.553227: step 1732, loss 0.30878, acc 0.90625, prec 0.0460123, recall 0.823132
2017-12-10T04:23:43.822345: step 1733, loss 0.240199, acc 0.90625, prec 0.0460283, recall 0.823207
2017-12-10T04:23:44.085807: step 1734, loss 0.117744, acc 0.96875, prec 0.0460261, recall 0.823207
2017-12-10T04:23:44.346696: step 1735, loss 0.344856, acc 0.9375, prec 0.0460443, recall 0.823281
2017-12-10T04:23:44.606373: step 1736, loss 0.189589, acc 0.9375, prec 0.0460399, recall 0.823281
2017-12-10T04:23:44.869947: step 1737, loss 0.478537, acc 0.953125, prec 0.0460816, recall 0.82343
2017-12-10T04:23:45.134661: step 1738, loss 0.0224346, acc 1, prec 0.0460816, recall 0.82343
2017-12-10T04:23:45.399190: step 1739, loss 0.105888, acc 0.953125, prec 0.0460784, recall 0.82343
2017-12-10T04:23:45.661746: step 1740, loss 0.127277, acc 0.9375, prec 0.046074, recall 0.82343
2017-12-10T04:23:45.932948: step 1741, loss 0.246788, acc 0.90625, prec 0.0460675, recall 0.82343
2017-12-10T04:23:46.204107: step 1742, loss 0.206885, acc 0.953125, prec 0.0460643, recall 0.82343
2017-12-10T04:23:46.468612: step 1743, loss 0.300203, acc 0.953125, prec 0.046106, recall 0.823579
2017-12-10T04:23:46.731478: step 1744, loss 0.0964901, acc 0.984375, prec 0.0461049, recall 0.823579
2017-12-10T04:23:46.993730: step 1745, loss 0.592173, acc 0.9375, prec 0.0461455, recall 0.823727
2017-12-10T04:23:47.258344: step 1746, loss 0.0903995, acc 0.96875, prec 0.0461433, recall 0.823727
2017-12-10T04:23:47.523307: step 1747, loss 0.256041, acc 0.953125, prec 0.0462075, recall 0.82395
2017-12-10T04:23:47.788615: step 1748, loss 0.106827, acc 0.984375, prec 0.0462289, recall 0.824024
2017-12-10T04:23:48.050949: step 1749, loss 0.0595572, acc 0.984375, prec 0.0462503, recall 0.824097
2017-12-10T04:23:48.312506: step 1750, loss 0.244896, acc 0.953125, prec 0.0462919, recall 0.824245
2017-12-10T04:23:48.576365: step 1751, loss 0.273898, acc 0.96875, prec 0.0463347, recall 0.824392
2017-12-10T04:23:48.839150: step 1752, loss 0.82482, acc 1, prec 0.0463571, recall 0.824466
2017-12-10T04:23:49.107442: step 1753, loss 0.202623, acc 0.953125, prec 0.0463539, recall 0.824466
2017-12-10T04:23:49.371565: step 1754, loss 0.160943, acc 0.90625, prec 0.0463698, recall 0.824539
2017-12-10T04:23:49.638812: step 1755, loss 0.201421, acc 0.90625, prec 0.0463632, recall 0.824539
2017-12-10T04:23:49.908500: step 1756, loss 1.28906, acc 0.9375, prec 0.0463813, recall 0.824613
2017-12-10T04:23:50.171057: step 1757, loss 0.133931, acc 0.96875, prec 0.0463791, recall 0.824613
2017-12-10T04:23:50.438878: step 1758, loss 0.168042, acc 0.953125, prec 0.0463759, recall 0.824613
2017-12-10T04:23:50.698133: step 1759, loss 0.2717, acc 0.921875, prec 0.0463704, recall 0.824613
2017-12-10T04:23:50.957548: step 1760, loss 0.344771, acc 0.90625, prec 0.0463863, recall 0.824686
2017-12-10T04:23:51.217000: step 1761, loss 0.363685, acc 0.9375, prec 0.0464044, recall 0.82476
2017-12-10T04:23:51.481498: step 1762, loss 2.44842, acc 0.921875, prec 0.0464, recall 0.824415
2017-12-10T04:23:51.746172: step 1763, loss 0.340363, acc 0.875, prec 0.0463913, recall 0.824415
2017-12-10T04:23:52.018203: step 1764, loss 0.438831, acc 0.890625, prec 0.0464509, recall 0.824635
2017-12-10T04:23:52.281774: step 1765, loss 0.512435, acc 0.84375, prec 0.04644, recall 0.824635
2017-12-10T04:23:52.550275: step 1766, loss 0.402352, acc 0.859375, prec 0.0464526, recall 0.824708
2017-12-10T04:23:52.813236: step 1767, loss 0.278494, acc 0.890625, prec 0.0464898, recall 0.824854
2017-12-10T04:23:53.077962: step 1768, loss 0.344679, acc 0.90625, prec 0.0464832, recall 0.824854
2017-12-10T04:23:53.342063: step 1769, loss 0.406502, acc 0.890625, prec 0.046498, recall 0.824927
2017-12-10T04:23:53.603658: step 1770, loss 0.922857, acc 0.796875, prec 0.0465062, recall 0.825
2017-12-10T04:23:53.868183: step 1771, loss 0.281603, acc 0.875, prec 0.0465422, recall 0.825146
2017-12-10T04:23:54.133389: step 1772, loss 0.578677, acc 0.828125, prec 0.0465526, recall 0.825218
2017-12-10T04:23:54.402242: step 1773, loss 0.586005, acc 0.84375, prec 0.046564, recall 0.825291
2017-12-10T04:23:54.666554: step 1774, loss 0.325545, acc 0.875, prec 0.0466, recall 0.825436
2017-12-10T04:23:54.931183: step 1775, loss 0.389964, acc 0.875, prec 0.0465913, recall 0.825436
2017-12-10T04:23:55.195232: step 1776, loss 0.444042, acc 0.84375, prec 0.0466027, recall 0.825509
2017-12-10T04:23:55.457379: step 1777, loss 0.414542, acc 0.828125, prec 0.0466131, recall 0.825581
2017-12-10T04:23:55.720614: step 1778, loss 0.25439, acc 0.90625, prec 0.0466065, recall 0.825581
2017-12-10T04:23:55.990100: step 1779, loss 0.148708, acc 0.9375, prec 0.0466245, recall 0.825654
2017-12-10T04:23:56.257828: step 1780, loss 0.42121, acc 0.9375, prec 0.0466648, recall 0.825798
2017-12-10T04:23:56.522673: step 1781, loss 0.149333, acc 0.96875, prec 0.0466849, recall 0.825871
2017-12-10T04:23:56.783212: step 1782, loss 0.168822, acc 0.9375, prec 0.0467029, recall 0.825943
2017-12-10T04:23:57.054502: step 1783, loss 0.376872, acc 0.890625, prec 0.0466953, recall 0.825943
2017-12-10T04:23:57.318158: step 1784, loss 0.196847, acc 0.90625, prec 0.0466887, recall 0.825943
2017-12-10T04:23:57.591158: step 1785, loss 0.719845, acc 0.953125, prec 0.0467077, recall 0.826015
2017-12-10T04:23:57.858674: step 1786, loss 0.0434135, acc 0.984375, prec 0.0467066, recall 0.826015
2017-12-10T04:23:58.118746: step 1787, loss 0.501553, acc 0.953125, prec 0.046748, recall 0.826159
2017-12-10T04:23:58.385781: step 1788, loss 0.109693, acc 0.96875, prec 0.0467682, recall 0.826231
2017-12-10T04:23:58.649193: step 1789, loss 0.364565, acc 0.953125, prec 0.0468095, recall 0.826375
2017-12-10T04:23:58.918101: step 1790, loss 3.99758, acc 0.9375, prec 0.0468062, recall 0.826033
2017-12-10T04:23:59.186953: step 1791, loss 5.36592, acc 0.96875, prec 0.0468274, recall 0.825764
2017-12-10T04:23:59.459245: step 1792, loss 0.0745241, acc 0.96875, prec 0.0468252, recall 0.825764
2017-12-10T04:23:59.723043: step 1793, loss 0.37439, acc 0.921875, prec 0.0468644, recall 0.825908
2017-12-10T04:23:59.987861: step 1794, loss 0.323275, acc 0.90625, prec 0.0468578, recall 0.825908
2017-12-10T04:24:00.252712: step 1795, loss 0.372211, acc 0.859375, prec 0.0468479, recall 0.825908
2017-12-10T04:24:00.517741: step 1796, loss 0.605961, acc 0.8125, prec 0.0468348, recall 0.825908
2017-12-10T04:24:00.783176: step 1797, loss 0.8339, acc 0.796875, prec 0.0468206, recall 0.825908
2017-12-10T04:24:01.052140: step 1798, loss 1.01401, acc 0.796875, prec 0.0468063, recall 0.825908
2017-12-10T04:24:01.317905: step 1799, loss 0.673875, acc 0.765625, prec 0.0468122, recall 0.825979
2017-12-10T04:24:01.582490: step 1800, loss 0.718202, acc 0.828125, prec 0.0468224, recall 0.826051

Evaluation:
2017-12-10T04:24:09.179949: step 1800, loss 1.42529, acc 0.732214, prec 0.0465457, recall 0.827117

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-1800

2017-12-10T04:24:10.499762: step 1801, loss 0.641182, acc 0.75, prec 0.0465294, recall 0.827117
2017-12-10T04:24:10.763091: step 1802, loss 1.98632, acc 0.609375, prec 0.046504, recall 0.827117
2017-12-10T04:24:11.024806: step 1803, loss 0.760629, acc 0.78125, prec 0.0464898, recall 0.827117
2017-12-10T04:24:11.293972: step 1804, loss 0.904223, acc 0.8125, prec 0.0464984, recall 0.827184
2017-12-10T04:24:11.563861: step 1805, loss 1.10697, acc 0.6875, prec 0.0464781, recall 0.827184
2017-12-10T04:24:11.833779: step 1806, loss 0.668043, acc 0.875, prec 0.04647, recall 0.827184
2017-12-10T04:24:12.109825: step 1807, loss 0.476144, acc 0.828125, prec 0.0464589, recall 0.827184
2017-12-10T04:24:12.379655: step 1808, loss 0.44515, acc 0.859375, prec 0.0464498, recall 0.827184
2017-12-10T04:24:12.649151: step 1809, loss 0.557163, acc 0.765625, prec 0.0464554, recall 0.827252
2017-12-10T04:24:12.917448: step 1810, loss 4.19265, acc 0.84375, prec 0.0464878, recall 0.827065
2017-12-10T04:24:13.188548: step 1811, loss 0.27395, acc 0.90625, prec 0.0465025, recall 0.827132
2017-12-10T04:24:13.454226: step 1812, loss 0.489242, acc 0.875, prec 0.0465152, recall 0.827199
2017-12-10T04:24:13.724191: step 1813, loss 0.236482, acc 0.953125, prec 0.0465121, recall 0.827199
2017-12-10T04:24:13.989027: step 1814, loss 0.15416, acc 0.953125, prec 0.0465091, recall 0.827199
2017-12-10T04:24:14.247810: step 1815, loss 0.279858, acc 0.90625, prec 0.0465238, recall 0.827266
2017-12-10T04:24:14.524289: step 1816, loss 0.524971, acc 0.875, prec 0.0465364, recall 0.827333
2017-12-10T04:24:14.786683: step 1817, loss 0.572476, acc 0.828125, prec 0.0465668, recall 0.827466
2017-12-10T04:24:15.052786: step 1818, loss 0.187133, acc 0.953125, prec 0.0465845, recall 0.827533
2017-12-10T04:24:15.316898: step 1819, loss 0.298783, acc 0.921875, prec 0.0465795, recall 0.827533
2017-12-10T04:24:15.578623: step 1820, loss 0.214709, acc 0.9375, prec 0.0465962, recall 0.8276
2017-12-10T04:24:15.837587: step 1821, loss 0.196976, acc 0.921875, prec 0.0466326, recall 0.827733
2017-12-10T04:24:16.100760: step 1822, loss 0.301591, acc 0.953125, prec 0.0466295, recall 0.827733
2017-12-10T04:24:16.366924: step 1823, loss 2.29893, acc 0.9375, prec 0.0466887, recall 0.827613
2017-12-10T04:24:16.642125: step 1824, loss 0.187211, acc 0.9375, prec 0.0467261, recall 0.827746
2017-12-10T04:24:16.904489: step 1825, loss 0.231922, acc 0.921875, prec 0.046721, recall 0.827746
2017-12-10T04:24:17.173752: step 1826, loss 0.221112, acc 0.9375, prec 0.046717, recall 0.827746
2017-12-10T04:24:17.437116: step 1827, loss 3.28469, acc 0.921875, prec 0.0467129, recall 0.827427
2017-12-10T04:24:17.709320: step 1828, loss 0.350979, acc 0.875, prec 0.0467255, recall 0.827493
2017-12-10T04:24:17.975698: step 1829, loss 0.283566, acc 0.90625, prec 0.0467401, recall 0.82756
2017-12-10T04:24:18.243577: step 1830, loss 0.174776, acc 0.921875, prec 0.0467765, recall 0.827692
2017-12-10T04:24:18.507097: step 1831, loss 0.984988, acc 0.828125, prec 0.0468068, recall 0.827825
2017-12-10T04:24:18.767478: step 1832, loss 0.872092, acc 0.765625, prec 0.0468329, recall 0.827957
2017-12-10T04:24:19.031073: step 1833, loss 0.705969, acc 0.84375, prec 0.0468434, recall 0.828023
2017-12-10T04:24:19.291176: step 1834, loss 0.336325, acc 0.921875, prec 0.0468384, recall 0.828023
2017-12-10T04:24:19.551581: step 1835, loss 0.601413, acc 0.859375, prec 0.0468913, recall 0.828221
2017-12-10T04:24:19.818839: step 1836, loss 0.978061, acc 0.796875, prec 0.0468987, recall 0.828287
2017-12-10T04:24:20.083632: step 1837, loss 0.449116, acc 0.828125, prec 0.0468875, recall 0.828287
2017-12-10T04:24:20.348606: step 1838, loss 0.427227, acc 0.875, prec 0.0469208, recall 0.828418
2017-12-10T04:24:20.615012: step 1839, loss 0.453855, acc 0.875, prec 0.0469333, recall 0.828484
2017-12-10T04:24:20.883183: step 1840, loss 0.430038, acc 0.859375, prec 0.0469448, recall 0.82855
2017-12-10T04:24:21.150360: step 1841, loss 2.62374, acc 0.84375, prec 0.0469563, recall 0.828298
2017-12-10T04:24:21.423560: step 1842, loss 0.582178, acc 0.90625, prec 0.0469708, recall 0.828364
2017-12-10T04:24:21.685611: step 1843, loss 0.380364, acc 0.921875, prec 0.0469658, recall 0.828364
2017-12-10T04:24:21.953324: step 1844, loss 0.695303, acc 0.84375, prec 0.0469762, recall 0.82843
2017-12-10T04:24:22.217476: step 1845, loss 0.625025, acc 0.828125, prec 0.0469857, recall 0.828495
2017-12-10T04:24:22.480965: step 1846, loss 0.491455, acc 0.859375, prec 0.0469972, recall 0.828561
2017-12-10T04:24:22.742177: step 1847, loss 5.42503, acc 0.859375, prec 0.0470107, recall 0.827994
2017-12-10T04:24:23.016709: step 1848, loss 0.924724, acc 0.796875, prec 0.0470387, recall 0.828125
2017-12-10T04:24:23.286958: step 1849, loss 0.662797, acc 0.875, prec 0.0470718, recall 0.828256
2017-12-10T04:24:23.552999: step 1850, loss 1.52192, acc 0.734375, prec 0.0470957, recall 0.828387
2017-12-10T04:24:23.815443: step 1851, loss 0.517, acc 0.890625, prec 0.0471092, recall 0.828452
2017-12-10T04:24:24.077778: step 1852, loss 0.762109, acc 0.765625, prec 0.0470939, recall 0.828452
2017-12-10T04:24:24.337445: step 1853, loss 0.481086, acc 0.859375, prec 0.047126, recall 0.828582
2017-12-10T04:24:24.603995: step 1854, loss 1.05934, acc 0.71875, prec 0.0471282, recall 0.828647
2017-12-10T04:24:24.866914: step 1855, loss 0.761124, acc 0.71875, prec 0.0471305, recall 0.828713
2017-12-10T04:24:25.133811: step 1856, loss 1.15247, acc 0.6875, prec 0.0471101, recall 0.828713
2017-12-10T04:24:25.394286: step 1857, loss 0.650585, acc 0.765625, prec 0.0470949, recall 0.828713
2017-12-10T04:24:25.654587: step 1858, loss 0.550254, acc 0.84375, prec 0.0471464, recall 0.828907
2017-12-10T04:24:25.918345: step 1859, loss 1.02307, acc 0.6875, prec 0.0471672, recall 0.829037
2017-12-10T04:24:26.184536: step 1860, loss 0.822841, acc 0.84375, prec 0.0472186, recall 0.829231
2017-12-10T04:24:26.444671: step 1861, loss 0.749478, acc 0.859375, prec 0.0472505, recall 0.829361
2017-12-10T04:24:26.709240: step 1862, loss 0.314156, acc 0.890625, prec 0.0472845, recall 0.82949
2017-12-10T04:24:26.982703: step 1863, loss 0.742957, acc 0.78125, prec 0.0472907, recall 0.829554
2017-12-10T04:24:27.249213: step 1864, loss 0.230448, acc 0.890625, prec 0.0473041, recall 0.829618
2017-12-10T04:24:27.519707: step 1865, loss 0.388494, acc 0.90625, prec 0.047298, recall 0.829618
2017-12-10T04:24:27.784266: step 1866, loss 0.595752, acc 0.90625, prec 0.0473329, recall 0.829747
2017-12-10T04:24:28.049991: step 1867, loss 0.194729, acc 0.9375, prec 0.0473494, recall 0.829811
2017-12-10T04:24:28.319213: step 1868, loss 0.0789874, acc 0.984375, prec 0.0473689, recall 0.829876
2017-12-10T04:24:28.587307: step 1869, loss 0.0426542, acc 0.984375, prec 0.0473679, recall 0.829876
2017-12-10T04:24:28.853429: step 1870, loss 0.040142, acc 0.984375, prec 0.0473668, recall 0.829876
2017-12-10T04:24:29.118601: step 1871, loss 0.0793324, acc 0.96875, prec 0.0473853, recall 0.82994
2017-12-10T04:24:29.382318: step 1872, loss 1.67098, acc 0.9375, prec 0.0473822, recall 0.829627
2017-12-10T04:24:29.649820: step 1873, loss 0.083775, acc 0.96875, prec 0.0473802, recall 0.829627
2017-12-10T04:24:29.914253: step 1874, loss 0.0909175, acc 0.96875, prec 0.0473987, recall 0.829691
2017-12-10T04:24:30.183603: step 1875, loss 0.778496, acc 0.96875, prec 0.0474171, recall 0.829755
2017-12-10T04:24:30.455026: step 1876, loss 0.0801157, acc 0.96875, prec 0.0474151, recall 0.829755
2017-12-10T04:24:30.722773: step 1877, loss 0.447979, acc 0.890625, prec 0.0474284, recall 0.829819
2017-12-10T04:24:30.983524: step 1878, loss 11.1199, acc 0.859375, prec 0.0474408, recall 0.829571
2017-12-10T04:24:31.259226: step 1879, loss 0.36931, acc 0.875, prec 0.0474736, recall 0.829699
2017-12-10T04:24:31.520647: step 1880, loss 0.458012, acc 0.875, prec 0.0474654, recall 0.829699
2017-12-10T04:24:31.779265: step 1881, loss 1.66295, acc 0.90625, prec 0.0474603, recall 0.829387
2017-12-10T04:24:32.043430: step 1882, loss 0.62681, acc 0.828125, prec 0.0474491, recall 0.829387
2017-12-10T04:24:32.314888: step 1883, loss 0.962815, acc 0.75, prec 0.0474533, recall 0.829452
2017-12-10T04:24:32.587001: step 1884, loss 1.25368, acc 0.78125, prec 0.0474594, recall 0.829516
2017-12-10T04:24:32.852998: step 1885, loss 1.00862, acc 0.6875, prec 0.0474595, recall 0.82958
2017-12-10T04:24:33.124690: step 1886, loss 1.26746, acc 0.6875, prec 0.0474391, recall 0.82958
2017-12-10T04:24:33.393389: step 1887, loss 1.92349, acc 0.609375, prec 0.0474341, recall 0.829644
2017-12-10T04:24:33.657511: step 1888, loss 1.17008, acc 0.59375, prec 0.0474077, recall 0.829644
2017-12-10T04:24:33.914650: step 1889, loss 1.67851, acc 0.609375, prec 0.0474231, recall 0.829771
2017-12-10T04:24:34.175883: step 1890, loss 1.3423, acc 0.640625, prec 0.0473998, recall 0.829771
2017-12-10T04:24:34.433242: step 1891, loss 1.53031, acc 0.640625, prec 0.0474172, recall 0.829899
2017-12-10T04:24:34.701073: step 1892, loss 1.12798, acc 0.6875, prec 0.0473969, recall 0.829899
2017-12-10T04:24:34.962626: step 1893, loss 0.497553, acc 0.8125, prec 0.0474051, recall 0.829963
2017-12-10T04:24:35.225578: step 1894, loss 0.584553, acc 0.890625, prec 0.047398, recall 0.829963
2017-12-10T04:24:35.488608: step 1895, loss 1.02381, acc 0.875, prec 0.0474307, recall 0.83009
2017-12-10T04:24:35.757736: step 1896, loss 0.84319, acc 0.8125, prec 0.0474592, recall 0.830217
2017-12-10T04:24:36.022059: step 1897, loss 0.194496, acc 0.9375, prec 0.0474552, recall 0.830217
2017-12-10T04:24:36.286457: step 1898, loss 0.854538, acc 0.828125, prec 0.0474847, recall 0.830344
2017-12-10T04:24:36.557125: step 1899, loss 0.433935, acc 0.90625, prec 0.0474786, recall 0.830344
2017-12-10T04:24:36.821583: step 1900, loss 0.549062, acc 0.890625, prec 0.0474919, recall 0.830407
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-1900

2017-12-10T04:24:38.044964: step 1901, loss 0.279685, acc 0.953125, prec 0.0474888, recall 0.830407
2017-12-10T04:24:38.310886: step 1902, loss 0.27608, acc 0.921875, prec 0.0475041, recall 0.830471
2017-12-10T04:24:38.575794: step 1903, loss 0.223379, acc 0.953125, prec 0.0475011, recall 0.830471
2017-12-10T04:24:38.842152: step 1904, loss 0.350583, acc 0.921875, prec 0.0475367, recall 0.830597
2017-12-10T04:24:39.103506: step 1905, loss 0.36177, acc 0.921875, prec 0.0475723, recall 0.830723
2017-12-10T04:24:39.370640: step 1906, loss 0.232377, acc 0.953125, prec 0.0475692, recall 0.830723
2017-12-10T04:24:39.634842: step 1907, loss 0.144946, acc 0.953125, prec 0.0476068, recall 0.830849
2017-12-10T04:24:39.907801: step 1908, loss 0.283449, acc 0.9375, prec 0.0476028, recall 0.830849
2017-12-10T04:24:40.172300: step 1909, loss 0.306174, acc 0.9375, prec 0.047619, recall 0.830912
2017-12-10T04:24:40.437408: step 1910, loss 0.16219, acc 0.953125, prec 0.047616, recall 0.830912
2017-12-10T04:24:40.706466: step 1911, loss 0.0441905, acc 0.984375, prec 0.047615, recall 0.830912
2017-12-10T04:24:40.972678: step 1912, loss 0.0801823, acc 0.96875, prec 0.0476333, recall 0.830975
2017-12-10T04:24:41.237395: step 1913, loss 0.195639, acc 0.9375, prec 0.0476495, recall 0.831038
2017-12-10T04:24:41.508534: step 1914, loss 0.219278, acc 0.90625, prec 0.0476434, recall 0.831038
2017-12-10T04:24:41.769024: step 1915, loss 0.211283, acc 0.9375, prec 0.0476394, recall 0.831038
2017-12-10T04:24:42.044848: step 1916, loss 0.0839892, acc 0.984375, prec 0.0476383, recall 0.831038
2017-12-10T04:24:42.312192: step 1917, loss 0.129553, acc 0.9375, prec 0.0476343, recall 0.831038
2017-12-10T04:24:42.575961: step 1918, loss 10.7708, acc 0.953125, prec 0.0476729, recall 0.830855
2017-12-10T04:24:42.846795: step 1919, loss 0.0663921, acc 0.96875, prec 0.0476708, recall 0.830855
2017-12-10T04:24:43.113106: step 1920, loss 0.151095, acc 0.953125, prec 0.0476678, recall 0.830855
2017-12-10T04:24:43.379079: step 1921, loss 0.13929, acc 0.953125, prec 0.0476647, recall 0.830855
2017-12-10T04:24:43.649401: step 1922, loss 0.315946, acc 0.90625, prec 0.047679, recall 0.830918
2017-12-10T04:24:43.910936: step 1923, loss 2.49504, acc 0.828125, prec 0.0476891, recall 0.830672
2017-12-10T04:24:44.174949: step 1924, loss 1.03698, acc 0.90625, prec 0.0477642, recall 0.830923
2017-12-10T04:24:44.439029: step 1925, loss 0.703507, acc 0.875, prec 0.0477763, recall 0.830986
2017-12-10T04:24:44.702145: step 1926, loss 0.325403, acc 0.890625, prec 0.0477895, recall 0.831049
2017-12-10T04:24:44.965140: step 1927, loss 1.08698, acc 0.78125, prec 0.0478158, recall 0.831174
2017-12-10T04:24:45.227527: step 1928, loss 1.10167, acc 0.640625, prec 0.0478127, recall 0.831236
2017-12-10T04:24:45.496458: step 1929, loss 0.358759, acc 0.890625, prec 0.0478258, recall 0.831299
2017-12-10T04:24:45.758508: step 1930, loss 1.09615, acc 0.703125, prec 0.047847, recall 0.831423
2017-12-10T04:24:46.026077: step 1931, loss 0.59746, acc 0.828125, prec 0.0478965, recall 0.83161
2017-12-10T04:24:46.292359: step 1932, loss 0.701929, acc 0.75, prec 0.0478803, recall 0.83161
2017-12-10T04:24:46.570418: step 1933, loss 0.836268, acc 0.75, prec 0.047864, recall 0.83161
2017-12-10T04:24:46.840906: step 1934, loss 0.868895, acc 0.828125, prec 0.0478528, recall 0.83161
2017-12-10T04:24:47.101865: step 1935, loss 1.44104, acc 0.6875, prec 0.0478325, recall 0.83161
2017-12-10T04:24:47.375649: step 1936, loss 1.13531, acc 0.734375, prec 0.0478354, recall 0.831672
2017-12-10T04:24:47.640182: step 1937, loss 0.432996, acc 0.875, prec 0.0479081, recall 0.83192
2017-12-10T04:24:47.904534: step 1938, loss 0.876382, acc 0.8125, prec 0.0479161, recall 0.831982
2017-12-10T04:24:48.162453: step 1939, loss 0.301555, acc 0.875, prec 0.0479282, recall 0.832044
2017-12-10T04:24:48.428558: step 1940, loss 0.594067, acc 0.828125, prec 0.0479372, recall 0.832106
2017-12-10T04:24:48.693605: step 1941, loss 0.521646, acc 0.859375, prec 0.0479684, recall 0.83223
2017-12-10T04:24:48.957848: step 1942, loss 0.72158, acc 0.921875, prec 0.0479835, recall 0.832291
2017-12-10T04:24:49.220610: step 1943, loss 0.26504, acc 0.90625, prec 0.0479774, recall 0.832291
2017-12-10T04:24:49.489200: step 1944, loss 1.31216, acc 0.921875, prec 0.0480329, recall 0.832476
2017-12-10T04:24:49.751208: step 1945, loss 6.53122, acc 0.921875, prec 0.0480692, recall 0.832294
2017-12-10T04:24:50.019233: step 1946, loss 0.464437, acc 0.828125, prec 0.0480781, recall 0.832355
2017-12-10T04:24:50.283621: step 1947, loss 0.535477, acc 0.875, prec 0.0480902, recall 0.832417
2017-12-10T04:24:50.555432: step 1948, loss 0.560222, acc 0.78125, prec 0.0480961, recall 0.832478
2017-12-10T04:24:50.812805: step 1949, loss 0.598591, acc 0.828125, prec 0.0480849, recall 0.832478
2017-12-10T04:24:51.078327: step 1950, loss 0.596333, acc 0.828125, prec 0.0480737, recall 0.832478
2017-12-10T04:24:51.341391: step 1951, loss 0.866944, acc 0.8125, prec 0.0480615, recall 0.832478
2017-12-10T04:24:51.615990: step 1952, loss 0.848485, acc 0.75, prec 0.0481056, recall 0.832662
2017-12-10T04:24:51.880220: step 1953, loss 0.706768, acc 0.8125, prec 0.0481135, recall 0.832723
2017-12-10T04:24:52.147457: step 1954, loss 0.627409, acc 0.8125, prec 0.0481214, recall 0.832784
2017-12-10T04:24:52.409645: step 1955, loss 0.53237, acc 0.8125, prec 0.0481294, recall 0.832846
2017-12-10T04:24:52.671102: step 1956, loss 0.40843, acc 0.84375, prec 0.0481795, recall 0.833029
2017-12-10T04:24:52.935895: step 1957, loss 0.429672, acc 0.875, prec 0.0481714, recall 0.833029
2017-12-10T04:24:53.205679: step 1958, loss 0.864265, acc 0.828125, prec 0.0481602, recall 0.833029
2017-12-10T04:24:53.476635: step 1959, loss 0.43449, acc 0.84375, prec 0.0481701, recall 0.83309
2017-12-10T04:24:53.738395: step 1960, loss 0.463057, acc 0.84375, prec 0.0481801, recall 0.833151
2017-12-10T04:24:54.006284: step 1961, loss 0.471967, acc 0.90625, prec 0.048194, recall 0.833212
2017-12-10T04:24:54.267362: step 1962, loss 0.315689, acc 0.921875, prec 0.048189, recall 0.833212
2017-12-10T04:24:54.536706: step 1963, loss 0.336959, acc 0.90625, prec 0.048223, recall 0.833333
2017-12-10T04:24:54.802594: step 1964, loss 0.342386, acc 0.875, prec 0.0482149, recall 0.833333
2017-12-10T04:24:55.068843: step 1965, loss 0.267023, acc 0.90625, prec 0.0482289, recall 0.833394
2017-12-10T04:24:55.335595: step 1966, loss 0.182054, acc 0.96875, prec 0.048267, recall 0.833515
2017-12-10T04:24:55.599537: step 1967, loss 0.174063, acc 0.953125, prec 0.048284, recall 0.833576
2017-12-10T04:24:55.870458: step 1968, loss 0.183641, acc 0.953125, prec 0.048301, recall 0.833637
2017-12-10T04:24:56.135036: step 1969, loss 0.147236, acc 0.96875, prec 0.0483191, recall 0.833697
2017-12-10T04:24:56.398275: step 1970, loss 1.31347, acc 0.921875, prec 0.048315, recall 0.833394
2017-12-10T04:24:56.668095: step 1971, loss 0.130146, acc 0.984375, prec 0.048334, recall 0.833455
2017-12-10T04:24:56.943283: step 1972, loss 0.0713043, acc 0.96875, prec 0.048332, recall 0.833455
2017-12-10T04:24:57.210366: step 1973, loss 3.22311, acc 0.9375, prec 0.048349, recall 0.833212
2017-12-10T04:24:57.478221: step 1974, loss 3.23684, acc 0.921875, prec 0.0483851, recall 0.833031
2017-12-10T04:24:57.740546: step 1975, loss 0.051908, acc 0.984375, prec 0.048384, recall 0.833031
2017-12-10T04:24:58.006743: step 1976, loss 0.431781, acc 0.953125, prec 0.048401, recall 0.833091
2017-12-10T04:24:58.274009: step 1977, loss 0.126024, acc 0.9375, prec 0.048397, recall 0.833091
2017-12-10T04:24:58.540242: step 1978, loss 0.320807, acc 0.875, prec 0.0483888, recall 0.833091
2017-12-10T04:24:58.807622: step 1979, loss 0.446483, acc 0.859375, prec 0.0483997, recall 0.833152
2017-12-10T04:24:59.075554: step 1980, loss 0.525295, acc 0.8125, prec 0.0484075, recall 0.833212
2017-12-10T04:24:59.347872: step 1981, loss 1.04335, acc 0.90625, prec 0.0484414, recall 0.833333
2017-12-10T04:24:59.619045: step 1982, loss 0.745178, acc 0.765625, prec 0.0484462, recall 0.833394
2017-12-10T04:24:59.885488: step 1983, loss 0.60114, acc 0.78125, prec 0.0484519, recall 0.833454
2017-12-10T04:25:00.147382: step 1984, loss 1.96889, acc 0.625, prec 0.0484475, recall 0.833514
2017-12-10T04:25:00.422460: step 1985, loss 1.34179, acc 0.859375, prec 0.0484783, recall 0.833635
2017-12-10T04:25:00.690462: step 1986, loss 0.586418, acc 0.875, prec 0.0484902, recall 0.833695
2017-12-10T04:25:00.954304: step 1987, loss 0.620998, acc 0.890625, prec 0.0485031, recall 0.833755
2017-12-10T04:25:01.190102: step 1988, loss 0.47327, acc 0.846154, prec 0.0485149, recall 0.833815
2017-12-10T04:25:01.460052: step 1989, loss 0.681495, acc 0.796875, prec 0.0485217, recall 0.833875
2017-12-10T04:25:01.721825: step 1990, loss 0.730497, acc 0.78125, prec 0.0485274, recall 0.833935
2017-12-10T04:25:01.986668: step 1991, loss 0.909186, acc 0.765625, prec 0.0485121, recall 0.833935
2017-12-10T04:25:02.249333: step 1992, loss 0.476253, acc 0.890625, prec 0.0485649, recall 0.834115
2017-12-10T04:25:02.508195: step 1993, loss 0.449408, acc 0.828125, prec 0.0485736, recall 0.834174
2017-12-10T04:25:02.768713: step 1994, loss 0.637889, acc 0.8125, prec 0.0485614, recall 0.834174
2017-12-10T04:25:03.032673: step 1995, loss 0.836383, acc 0.84375, prec 0.0485912, recall 0.834294
2017-12-10T04:25:03.304615: step 1996, loss 0.290802, acc 0.90625, prec 0.048605, recall 0.834354
2017-12-10T04:25:03.566057: step 1997, loss 1.04779, acc 0.75, prec 0.0486485, recall 0.834532
2017-12-10T04:25:03.829728: step 1998, loss 0.496192, acc 0.875, prec 0.0486404, recall 0.834532
2017-12-10T04:25:04.090913: step 1999, loss 0.17123, acc 0.9375, prec 0.0486363, recall 0.834532
2017-12-10T04:25:04.352764: step 2000, loss 0.395052, acc 0.890625, prec 0.0486292, recall 0.834532
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-2000

2017-12-10T04:25:05.701282: step 2001, loss 0.183867, acc 0.953125, prec 0.048646, recall 0.834592
2017-12-10T04:25:05.972307: step 2002, loss 0.09662, acc 0.953125, prec 0.0486629, recall 0.834651
2017-12-10T04:25:06.248794: step 2003, loss 0.347084, acc 0.859375, prec 0.0486936, recall 0.83477
2017-12-10T04:25:06.512681: step 2004, loss 0.266269, acc 0.9375, prec 0.0487493, recall 0.834948
2017-12-10T04:25:06.776568: step 2005, loss 0.313576, acc 0.953125, prec 0.048806, recall 0.835125
2017-12-10T04:25:07.045470: step 2006, loss 0.204047, acc 0.96875, prec 0.0488239, recall 0.835185
2017-12-10T04:25:07.310285: step 2007, loss 13.5398, acc 0.921875, prec 0.0488198, recall 0.834885
2017-12-10T04:25:07.580514: step 2008, loss 0.0466551, acc 0.96875, prec 0.0488178, recall 0.834885
2017-12-10T04:25:07.844253: step 2009, loss 0.249726, acc 0.90625, prec 0.0488316, recall 0.834944
2017-12-10T04:25:08.104883: step 2010, loss 0.245635, acc 0.921875, prec 0.0488663, recall 0.835063
2017-12-10T04:25:08.368015: step 2011, loss 0.131029, acc 0.953125, prec 0.0488632, recall 0.835063
2017-12-10T04:25:08.634711: step 2012, loss 0.239085, acc 0.90625, prec 0.048877, recall 0.835122
2017-12-10T04:25:08.900262: step 2013, loss 0.222734, acc 0.921875, prec 0.0488719, recall 0.835122
2017-12-10T04:25:09.160703: step 2014, loss 0.250099, acc 0.953125, prec 0.0488887, recall 0.835181
2017-12-10T04:25:09.423038: step 2015, loss 0.11444, acc 0.9375, prec 0.0488846, recall 0.835181
2017-12-10T04:25:09.690996: step 2016, loss 1.28287, acc 0.9375, prec 0.0488815, recall 0.834882
2017-12-10T04:25:09.958430: step 2017, loss 0.180065, acc 0.9375, prec 0.0488974, recall 0.834941
2017-12-10T04:25:10.225350: step 2018, loss 0.190608, acc 0.9375, prec 0.0489331, recall 0.835059
2017-12-10T04:25:10.489838: step 2019, loss 0.501326, acc 0.875, prec 0.0489249, recall 0.835059
2017-12-10T04:25:10.752034: step 2020, loss 1.93133, acc 0.8125, prec 0.0489136, recall 0.834761
2017-12-10T04:25:11.027910: step 2021, loss 0.805184, acc 0.796875, prec 0.0489202, recall 0.83482
2017-12-10T04:25:11.294471: step 2022, loss 0.402397, acc 0.921875, prec 0.048935, recall 0.834879
2017-12-10T04:25:11.557461: step 2023, loss 0.328912, acc 0.859375, prec 0.0489456, recall 0.834938
2017-12-10T04:25:11.822850: step 2024, loss 0.694761, acc 0.75, prec 0.0489491, recall 0.834996
2017-12-10T04:25:12.097147: step 2025, loss 0.4611, acc 0.875, prec 0.048941, recall 0.834996
2017-12-10T04:25:12.360579: step 2026, loss 0.575296, acc 0.859375, prec 0.0489516, recall 0.835055
2017-12-10T04:25:12.632342: step 2027, loss 0.669599, acc 0.859375, prec 0.0489623, recall 0.835114
2017-12-10T04:25:12.895633: step 2028, loss 0.404675, acc 0.9375, prec 0.0490178, recall 0.83529
2017-12-10T04:25:13.161693: step 2029, loss 0.43577, acc 0.859375, prec 0.0490086, recall 0.83529
2017-12-10T04:25:13.423475: step 2030, loss 0.237384, acc 0.921875, prec 0.0490034, recall 0.83529
2017-12-10T04:25:13.687337: step 2031, loss 0.459909, acc 0.90625, prec 0.0490172, recall 0.835348
2017-12-10T04:25:13.955684: step 2032, loss 0.747664, acc 0.796875, prec 0.0490039, recall 0.835348
2017-12-10T04:25:14.231750: step 2033, loss 0.374707, acc 0.875, prec 0.0490155, recall 0.835407
2017-12-10T04:25:14.494977: step 2034, loss 0.0685992, acc 0.96875, prec 0.0490135, recall 0.835407
2017-12-10T04:25:14.756559: step 2035, loss 0.298764, acc 0.921875, prec 0.0490084, recall 0.835407
2017-12-10T04:25:15.019596: step 2036, loss 0.255647, acc 0.96875, prec 0.049046, recall 0.835524
2017-12-10T04:25:15.289597: step 2037, loss 0.164606, acc 0.96875, prec 0.0491034, recall 0.835699
2017-12-10T04:25:15.560499: step 2038, loss 0.401021, acc 0.9375, prec 0.0491191, recall 0.835757
2017-12-10T04:25:15.819158: step 2039, loss 0.977971, acc 0.9375, prec 0.0491547, recall 0.835874
2017-12-10T04:25:16.091413: step 2040, loss 0.111623, acc 0.984375, prec 0.0491933, recall 0.83599
2017-12-10T04:25:16.361205: step 2041, loss 0.262506, acc 0.890625, prec 0.0491861, recall 0.83599
2017-12-10T04:25:16.624023: step 2042, loss 0.276223, acc 0.953125, prec 0.0491831, recall 0.83599
2017-12-10T04:25:16.894903: step 2043, loss 0.820611, acc 0.921875, prec 0.0491977, recall 0.836048
2017-12-10T04:25:17.155627: step 2044, loss 0.208888, acc 0.953125, prec 0.0492343, recall 0.836164
2017-12-10T04:25:17.430780: step 2045, loss 0.332853, acc 0.890625, prec 0.0492667, recall 0.83628
2017-12-10T04:25:17.693440: step 2046, loss 0.143896, acc 0.9375, prec 0.0492626, recall 0.83628
2017-12-10T04:25:17.956926: step 2047, loss 0.184793, acc 0.953125, prec 0.0492595, recall 0.83628
2017-12-10T04:25:18.218948: step 2048, loss 0.073625, acc 0.96875, prec 0.0492575, recall 0.83628
2017-12-10T04:25:18.483551: step 2049, loss 0.306176, acc 0.921875, prec 0.0492722, recall 0.836338
2017-12-10T04:25:18.749095: step 2050, loss 0.183494, acc 0.90625, prec 0.049266, recall 0.836338
2017-12-10T04:25:19.018726: step 2051, loss 0.401727, acc 0.9375, prec 0.0493213, recall 0.836511
2017-12-10T04:25:19.285597: step 2052, loss 0.466646, acc 0.890625, prec 0.0493339, recall 0.836569
2017-12-10T04:25:19.551343: step 2053, loss 0.0670039, acc 0.96875, prec 0.0493318, recall 0.836569
2017-12-10T04:25:19.814663: step 2054, loss 0.195411, acc 0.953125, prec 0.0493485, recall 0.836627
2017-12-10T04:25:20.079216: step 2055, loss 0.0748738, acc 0.96875, prec 0.0493465, recall 0.836627
2017-12-10T04:25:20.336996: step 2056, loss 0.34062, acc 0.890625, prec 0.0493591, recall 0.836684
2017-12-10T04:25:20.600157: step 2057, loss 0.320979, acc 0.890625, prec 0.0493717, recall 0.836742
2017-12-10T04:25:20.869097: step 2058, loss 0.167797, acc 0.90625, prec 0.0493853, recall 0.836799
2017-12-10T04:25:21.131062: step 2059, loss 0.182909, acc 0.9375, prec 0.0493812, recall 0.836799
2017-12-10T04:25:21.398206: step 2060, loss 0.0395173, acc 0.984375, prec 0.0493801, recall 0.836799
2017-12-10T04:25:21.662316: step 2061, loss 3.13822, acc 0.890625, prec 0.049374, recall 0.836505
2017-12-10T04:25:21.925999: step 2062, loss 0.206543, acc 0.921875, prec 0.0493689, recall 0.836505
2017-12-10T04:25:22.187440: step 2063, loss 0.238073, acc 0.90625, prec 0.0493627, recall 0.836505
2017-12-10T04:25:22.458145: step 2064, loss 4.63839, acc 0.921875, prec 0.0493586, recall 0.83621
2017-12-10T04:25:22.733856: step 2065, loss 0.369313, acc 0.90625, prec 0.0493524, recall 0.83621
2017-12-10T04:25:22.999779: step 2066, loss 0.1196, acc 0.953125, prec 0.0493494, recall 0.83621
2017-12-10T04:25:23.265956: step 2067, loss 0.325021, acc 0.890625, prec 0.0493619, recall 0.836268
2017-12-10T04:25:23.533213: step 2068, loss 0.542499, acc 0.859375, prec 0.0493725, recall 0.836325
2017-12-10T04:25:23.801213: step 2069, loss 0.679336, acc 0.875, prec 0.0494037, recall 0.83644
2017-12-10T04:25:24.067863: step 2070, loss 0.279637, acc 0.96875, prec 0.0494017, recall 0.83644
2017-12-10T04:25:24.330781: step 2071, loss 0.319583, acc 0.90625, prec 0.0493955, recall 0.83644
2017-12-10T04:25:24.595483: step 2072, loss 0.741661, acc 0.84375, prec 0.049405, recall 0.836498
2017-12-10T04:25:24.861858: step 2073, loss 0.220313, acc 0.9375, prec 0.0494009, recall 0.836498
2017-12-10T04:25:25.134471: step 2074, loss 0.703683, acc 0.828125, prec 0.0494291, recall 0.836613
2017-12-10T04:25:25.396542: step 2075, loss 0.247539, acc 0.921875, prec 0.049424, recall 0.836613
2017-12-10T04:25:25.663252: step 2076, loss 1.27919, acc 0.84375, prec 0.0494334, recall 0.83667
2017-12-10T04:25:25.923971: step 2077, loss 0.651944, acc 0.796875, prec 0.0494793, recall 0.836842
2017-12-10T04:25:26.186308: step 2078, loss 0.30255, acc 0.875, prec 0.0495105, recall 0.836957
2017-12-10T04:25:26.446697: step 2079, loss 0.784698, acc 0.8125, prec 0.0495376, recall 0.837071
2017-12-10T04:25:26.710929: step 2080, loss 0.403284, acc 0.875, prec 0.0495294, recall 0.837071
2017-12-10T04:25:26.989879: step 2081, loss 0.262332, acc 0.890625, prec 0.0495222, recall 0.837071
2017-12-10T04:25:27.252571: step 2082, loss 0.402307, acc 0.875, prec 0.0495337, recall 0.837128
2017-12-10T04:25:27.522032: step 2083, loss 0.446028, acc 0.84375, prec 0.0495234, recall 0.837128
2017-12-10T04:25:27.793600: step 2084, loss 0.646595, acc 0.828125, prec 0.0495515, recall 0.837242
2017-12-10T04:25:28.052155: step 2085, loss 0.402266, acc 0.859375, prec 0.0495423, recall 0.837242
2017-12-10T04:25:28.316071: step 2086, loss 0.394475, acc 0.90625, prec 0.0495755, recall 0.837356
2017-12-10T04:25:28.578528: step 2087, loss 0.147736, acc 0.90625, prec 0.0495693, recall 0.837356
2017-12-10T04:25:28.839250: step 2088, loss 0.188251, acc 0.9375, prec 0.0495849, recall 0.837413
2017-12-10T04:25:29.102815: step 2089, loss 0.493056, acc 0.921875, prec 0.0495994, recall 0.837469
2017-12-10T04:25:29.364433: step 2090, loss 0.482754, acc 0.890625, prec 0.0496316, recall 0.837583
2017-12-10T04:25:29.625726: step 2091, loss 0.322403, acc 0.875, prec 0.049643, recall 0.83764
2017-12-10T04:25:29.884673: step 2092, loss 0.569189, acc 0.921875, prec 0.0496576, recall 0.837696
2017-12-10T04:25:30.150725: step 2093, loss 0.125319, acc 0.984375, prec 0.0496565, recall 0.837696
2017-12-10T04:25:30.428134: step 2094, loss 1.55814, acc 0.90625, prec 0.0496711, recall 0.837461
2017-12-10T04:25:30.705024: step 2095, loss 1.72495, acc 0.9375, prec 0.0496876, recall 0.837225
2017-12-10T04:25:30.972655: step 2096, loss 0.230686, acc 0.90625, prec 0.0496815, recall 0.837225
2017-12-10T04:25:31.237673: step 2097, loss 0.0673315, acc 0.984375, prec 0.0496804, recall 0.837225
2017-12-10T04:25:31.497438: step 2098, loss 0.390732, acc 0.9375, prec 0.0497156, recall 0.837339
2017-12-10T04:25:31.768542: step 2099, loss 0.282143, acc 0.890625, prec 0.0497084, recall 0.837339
2017-12-10T04:25:32.041741: step 2100, loss 0.418791, acc 0.921875, prec 0.0497033, recall 0.837339

Evaluation:
2017-12-10T04:25:39.537316: step 2100, loss 1.744, acc 0.907152, prec 0.0505942, recall 0.827758

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-2100

2017-12-10T04:25:40.831363: step 2101, loss 0.600322, acc 0.78125, prec 0.0505799, recall 0.827758
2017-12-10T04:25:41.095556: step 2102, loss 0.312319, acc 0.921875, prec 0.050594, recall 0.827815
2017-12-10T04:25:41.361367: step 2103, loss 0.177665, acc 0.9375, prec 0.0506091, recall 0.827872
2017-12-10T04:25:41.626806: step 2104, loss 0.248604, acc 0.953125, prec 0.0506252, recall 0.827929
2017-12-10T04:25:41.896563: step 2105, loss 0.291248, acc 0.90625, prec 0.0506191, recall 0.827929
2017-12-10T04:25:42.166749: step 2106, loss 0.361953, acc 0.84375, prec 0.0506472, recall 0.828042
2017-12-10T04:25:42.432662: step 2107, loss 0.306292, acc 0.90625, prec 0.0506603, recall 0.828099
2017-12-10T04:25:42.697861: step 2108, loss 0.165635, acc 0.90625, prec 0.0506542, recall 0.828099
2017-12-10T04:25:42.961798: step 2109, loss 0.330577, acc 0.875, prec 0.050646, recall 0.828099
2017-12-10T04:25:43.226056: step 2110, loss 0.45859, acc 0.890625, prec 0.0507156, recall 0.828326
2017-12-10T04:25:43.490410: step 2111, loss 0.282957, acc 0.890625, prec 0.0507084, recall 0.828326
2017-12-10T04:25:43.754709: step 2112, loss 0.183376, acc 0.921875, prec 0.0507224, recall 0.828383
2017-12-10T04:25:44.016688: step 2113, loss 0.423708, acc 0.890625, prec 0.0507153, recall 0.828383
2017-12-10T04:25:44.280432: step 2114, loss 0.0671502, acc 0.984375, prec 0.0507142, recall 0.828383
2017-12-10T04:25:44.545257: step 2115, loss 0.23727, acc 0.9375, prec 0.0507101, recall 0.828383
2017-12-10T04:25:44.814689: step 2116, loss 0.125408, acc 0.96875, prec 0.0507081, recall 0.828383
2017-12-10T04:25:45.076328: step 2117, loss 0.224202, acc 0.90625, prec 0.0507403, recall 0.828496
2017-12-10T04:25:45.347461: step 2118, loss 0.333007, acc 0.9375, prec 0.0507745, recall 0.828609
2017-12-10T04:25:45.612999: step 2119, loss 0.157899, acc 0.921875, prec 0.0507886, recall 0.828666
2017-12-10T04:25:45.878477: step 2120, loss 1.51173, acc 0.90625, prec 0.0508208, recall 0.828778
2017-12-10T04:25:46.145435: step 2121, loss 4.73841, acc 0.984375, prec 0.0508399, recall 0.828562
2017-12-10T04:25:46.417882: step 2122, loss 0.313697, acc 0.90625, prec 0.0508529, recall 0.828618
2017-12-10T04:25:46.681419: step 2123, loss 1.3667, acc 0.921875, prec 0.050868, recall 0.828402
2017-12-10T04:25:46.956368: step 2124, loss 0.398666, acc 0.84375, prec 0.0508577, recall 0.828402
2017-12-10T04:25:47.221273: step 2125, loss 0.706289, acc 0.84375, prec 0.0508475, recall 0.828402
2017-12-10T04:25:47.484719: step 2126, loss 0.355206, acc 0.875, prec 0.0508393, recall 0.828402
2017-12-10T04:25:47.747188: step 2127, loss 0.45741, acc 0.796875, prec 0.0508451, recall 0.828459
2017-12-10T04:25:48.015450: step 2128, loss 0.587425, acc 0.84375, prec 0.0508348, recall 0.828459
2017-12-10T04:25:48.284217: step 2129, loss 0.785812, acc 0.78125, prec 0.0508205, recall 0.828459
2017-12-10T04:25:48.549290: step 2130, loss 0.629141, acc 0.75, prec 0.0508041, recall 0.828459
2017-12-10T04:25:48.817461: step 2131, loss 0.607155, acc 0.890625, prec 0.0508352, recall 0.828571
2017-12-10T04:25:49.077981: step 2132, loss 0.848408, acc 0.78125, prec 0.0508591, recall 0.828684
2017-12-10T04:25:49.334825: step 2133, loss 0.491141, acc 0.84375, prec 0.050887, recall 0.828796
2017-12-10T04:25:49.608306: step 2134, loss 0.746354, acc 0.71875, prec 0.0508686, recall 0.828796
2017-12-10T04:25:49.872445: step 2135, loss 1.00526, acc 0.765625, prec 0.0509106, recall 0.828965
2017-12-10T04:25:50.133121: step 2136, loss 0.89985, acc 0.8125, prec 0.0509174, recall 0.829021
2017-12-10T04:25:50.398965: step 2137, loss 0.800493, acc 0.78125, prec 0.0509412, recall 0.829133
2017-12-10T04:25:50.658564: step 2138, loss 0.925637, acc 0.75, prec 0.0509439, recall 0.829188
2017-12-10T04:25:50.919400: step 2139, loss 0.510671, acc 0.84375, prec 0.0509336, recall 0.829188
2017-12-10T04:25:51.176203: step 2140, loss 0.567549, acc 0.859375, prec 0.0509626, recall 0.8293
2017-12-10T04:25:51.437938: step 2141, loss 0.253829, acc 0.90625, prec 0.0509755, recall 0.829356
2017-12-10T04:25:51.700746: step 2142, loss 0.346883, acc 0.90625, prec 0.0509884, recall 0.829412
2017-12-10T04:25:51.960816: step 2143, loss 0.0997538, acc 0.953125, prec 0.0510044, recall 0.829467
2017-12-10T04:25:52.222467: step 2144, loss 0.140327, acc 0.9375, prec 0.0510194, recall 0.829523
2017-12-10T04:25:52.487781: step 2145, loss 0.0701525, acc 0.984375, prec 0.0510374, recall 0.829579
2017-12-10T04:25:52.756466: step 2146, loss 0.119017, acc 0.953125, prec 0.0510534, recall 0.829634
2017-12-10T04:25:53.025436: step 2147, loss 0.105504, acc 0.9375, prec 0.0510493, recall 0.829634
2017-12-10T04:25:53.289116: step 2148, loss 0.915815, acc 0.953125, prec 0.0510843, recall 0.829746
2017-12-10T04:25:53.558326: step 2149, loss 1.58194, acc 0.953125, prec 0.0510823, recall 0.829475
2017-12-10T04:25:53.824809: step 2150, loss 0.177433, acc 0.953125, prec 0.0510983, recall 0.829531
2017-12-10T04:25:54.091101: step 2151, loss 0.213591, acc 0.96875, prec 0.0511343, recall 0.829642
2017-12-10T04:25:54.361964: step 2152, loss 0.43651, acc 0.953125, prec 0.0511503, recall 0.829697
2017-12-10T04:25:54.634716: step 2153, loss 0.281242, acc 0.953125, prec 0.0511662, recall 0.829753
2017-12-10T04:25:54.905049: step 2154, loss 0.313792, acc 0.9375, prec 0.0511812, recall 0.829808
2017-12-10T04:25:55.169538: step 2155, loss 0.107549, acc 0.953125, prec 0.0511781, recall 0.829808
2017-12-10T04:25:55.433756: step 2156, loss 0.288871, acc 0.90625, prec 0.051191, recall 0.829863
2017-12-10T04:25:55.697002: step 2157, loss 0.171694, acc 0.90625, prec 0.0511848, recall 0.829863
2017-12-10T04:25:55.968399: step 2158, loss 0.228035, acc 0.90625, prec 0.0511787, recall 0.829863
2017-12-10T04:25:56.235023: step 2159, loss 0.372479, acc 0.953125, prec 0.0511946, recall 0.829919
2017-12-10T04:25:56.498128: step 2160, loss 0.316586, acc 0.890625, prec 0.0511874, recall 0.829919
2017-12-10T04:25:56.767798: step 2161, loss 2.38894, acc 0.890625, prec 0.0511813, recall 0.829649
2017-12-10T04:25:57.041583: step 2162, loss 0.171927, acc 0.953125, prec 0.0512162, recall 0.82976
2017-12-10T04:25:57.310477: step 2163, loss 0.215827, acc 0.921875, prec 0.0512301, recall 0.829815
2017-12-10T04:25:57.574990: step 2164, loss 0.242362, acc 0.890625, prec 0.0512229, recall 0.829815
2017-12-10T04:25:57.836834: step 2165, loss 0.29206, acc 0.9375, prec 0.0512759, recall 0.829981
2017-12-10T04:25:58.100691: step 2166, loss 0.78494, acc 0.828125, prec 0.0512646, recall 0.829981
2017-12-10T04:25:58.368964: step 2167, loss 0.404516, acc 0.859375, prec 0.0512743, recall 0.830036
2017-12-10T04:25:58.632014: step 2168, loss 0.166092, acc 0.9375, prec 0.0512702, recall 0.830036
2017-12-10T04:25:58.891073: step 2169, loss 0.488691, acc 0.84375, prec 0.05126, recall 0.830036
2017-12-10T04:25:59.152066: step 2170, loss 1.00314, acc 0.921875, prec 0.0512738, recall 0.830091
2017-12-10T04:25:59.417389: step 2171, loss 0.350141, acc 0.921875, prec 0.0512687, recall 0.830091
2017-12-10T04:25:59.679065: step 2172, loss 0.325009, acc 0.828125, prec 0.0512764, recall 0.830146
2017-12-10T04:25:59.939921: step 2173, loss 0.468394, acc 0.828125, prec 0.0512841, recall 0.830201
2017-12-10T04:26:00.208811: step 2174, loss 0.666925, acc 0.84375, prec 0.0512738, recall 0.830201
2017-12-10T04:26:00.473216: step 2175, loss 0.553859, acc 0.84375, prec 0.0512826, recall 0.830256
2017-12-10T04:26:00.741102: step 2176, loss 0.694402, acc 0.84375, prec 0.0512723, recall 0.830256
2017-12-10T04:26:01.001745: step 2177, loss 0.253753, acc 0.921875, prec 0.0512862, recall 0.830311
2017-12-10T04:26:01.261074: step 2178, loss 0.421942, acc 0.8125, prec 0.0512928, recall 0.830366
2017-12-10T04:26:01.524769: step 2179, loss 0.271294, acc 0.890625, prec 0.0513046, recall 0.830421
2017-12-10T04:26:01.793375: step 2180, loss 0.293647, acc 0.859375, prec 0.0512954, recall 0.830421
2017-12-10T04:26:02.057463: step 2181, loss 0.195884, acc 0.921875, prec 0.0513092, recall 0.830476
2017-12-10T04:26:02.324251: step 2182, loss 0.122934, acc 0.9375, prec 0.0513051, recall 0.830476
2017-12-10T04:26:02.593581: step 2183, loss 0.295466, acc 0.984375, prec 0.051342, recall 0.830585
2017-12-10T04:26:02.854995: step 2184, loss 0.0142729, acc 1, prec 0.051361, recall 0.83064
2017-12-10T04:26:03.118990: step 2185, loss 0.150265, acc 0.953125, prec 0.0513768, recall 0.830695
2017-12-10T04:26:03.381518: step 2186, loss 0.218944, acc 0.90625, prec 0.0513707, recall 0.830695
2017-12-10T04:26:03.650225: step 2187, loss 3.95506, acc 0.921875, prec 0.0513666, recall 0.830426
2017-12-10T04:26:03.921033: step 2188, loss 5.86102, acc 0.984375, prec 0.0514234, recall 0.830323
2017-12-10T04:26:04.187123: step 2189, loss 0.295622, acc 0.90625, prec 0.0514173, recall 0.830323
2017-12-10T04:26:04.450636: step 2190, loss 0.28686, acc 0.9375, prec 0.0514132, recall 0.830323
2017-12-10T04:26:04.718828: step 2191, loss 0.230273, acc 0.9375, prec 0.051428, recall 0.830377
2017-12-10T04:26:04.992743: step 2192, loss 1.06505, acc 0.78125, prec 0.0514326, recall 0.830432
2017-12-10T04:26:05.264299: step 2193, loss 0.884317, acc 0.84375, prec 0.0514223, recall 0.830432
2017-12-10T04:26:05.531556: step 2194, loss 0.893831, acc 0.75, prec 0.0514248, recall 0.830487
2017-12-10T04:26:05.799506: step 2195, loss 0.951328, acc 0.75, prec 0.0514273, recall 0.830541
2017-12-10T04:26:06.062273: step 2196, loss 1.31749, acc 0.671875, prec 0.0514436, recall 0.83065
2017-12-10T04:26:06.325883: step 2197, loss 0.959039, acc 0.765625, prec 0.0514282, recall 0.83065
2017-12-10T04:26:06.593475: step 2198, loss 1.94129, acc 0.734375, prec 0.0514297, recall 0.830705
2017-12-10T04:26:06.856945: step 2199, loss 0.910421, acc 0.796875, prec 0.0514164, recall 0.830705
2017-12-10T04:26:07.119738: step 2200, loss 0.983025, acc 0.703125, prec 0.0514158, recall 0.830759
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-2200

2017-12-10T04:26:08.441597: step 2201, loss 0.771791, acc 0.796875, prec 0.0514214, recall 0.830814
2017-12-10T04:26:08.705101: step 2202, loss 0.518138, acc 0.84375, prec 0.0514301, recall 0.830868
2017-12-10T04:26:08.978997: step 2203, loss 0.476278, acc 0.859375, prec 0.0514586, recall 0.830977
2017-12-10T04:26:09.243343: step 2204, loss 0.668543, acc 0.828125, prec 0.0514662, recall 0.831031
2017-12-10T04:26:09.506211: step 2205, loss 0.519893, acc 0.84375, prec 0.0514937, recall 0.83114
2017-12-10T04:26:09.770869: step 2206, loss 0.215932, acc 0.90625, prec 0.0515064, recall 0.831194
2017-12-10T04:26:10.035140: step 2207, loss 0.355125, acc 0.921875, prec 0.0515013, recall 0.831194
2017-12-10T04:26:10.302705: step 2208, loss 0.419819, acc 0.953125, prec 0.0515359, recall 0.831302
2017-12-10T04:26:10.569865: step 2209, loss 1.10986, acc 0.890625, prec 0.0515853, recall 0.831464
2017-12-10T04:26:10.832274: step 2210, loss 0.423768, acc 0.859375, prec 0.0515761, recall 0.831464
2017-12-10T04:26:11.102876: step 2211, loss 0.0810541, acc 0.96875, prec 0.0515741, recall 0.831464
2017-12-10T04:26:11.376320: step 2212, loss 0.297807, acc 0.921875, prec 0.0515689, recall 0.831464
2017-12-10T04:26:11.645481: step 2213, loss 0.379285, acc 0.90625, prec 0.0515628, recall 0.831464
2017-12-10T04:26:11.915485: step 2214, loss 0.13654, acc 0.953125, prec 0.0515786, recall 0.831518
2017-12-10T04:26:12.185449: step 2215, loss 0.274766, acc 0.921875, prec 0.0515734, recall 0.831518
2017-12-10T04:26:12.455620: step 2216, loss 0.182484, acc 0.96875, prec 0.0515902, recall 0.831572
2017-12-10T04:26:12.725008: step 2217, loss 0.0721059, acc 0.984375, prec 0.051608, recall 0.831626
2017-12-10T04:26:12.992755: step 2218, loss 0.0573661, acc 0.96875, prec 0.051606, recall 0.831626
2017-12-10T04:26:13.265064: step 2219, loss 0.0196033, acc 1, prec 0.051606, recall 0.831626
2017-12-10T04:26:13.536042: step 2220, loss 0.0302354, acc 0.984375, prec 0.051605, recall 0.831626
2017-12-10T04:26:13.800831: step 2221, loss 6.84334, acc 0.953125, prec 0.0516029, recall 0.83136
2017-12-10T04:26:14.069365: step 2222, loss 0.184074, acc 0.953125, prec 0.0516187, recall 0.831414
2017-12-10T04:26:14.343176: step 2223, loss 0.240862, acc 0.953125, prec 0.0516156, recall 0.831414
2017-12-10T04:26:14.607376: step 2224, loss 3.60759, acc 0.875, prec 0.0516084, recall 0.831148
2017-12-10T04:26:14.882311: step 2225, loss 0.218413, acc 0.984375, prec 0.0516262, recall 0.831202
2017-12-10T04:26:15.154790: step 2226, loss 1.33583, acc 0.921875, prec 0.0516221, recall 0.830936
2017-12-10T04:26:15.420833: step 2227, loss 0.380463, acc 0.921875, prec 0.0516547, recall 0.831044
2017-12-10T04:26:15.684120: step 2228, loss 0.1066, acc 0.984375, prec 0.0516536, recall 0.831044
2017-12-10T04:26:15.954517: step 2229, loss 0.767994, acc 0.828125, prec 0.0516424, recall 0.831044
2017-12-10T04:26:16.227118: step 2230, loss 0.504838, acc 0.859375, prec 0.0516708, recall 0.831152
2017-12-10T04:26:16.496524: step 2231, loss 0.526542, acc 0.8125, prec 0.0516773, recall 0.831206
2017-12-10T04:26:16.760625: step 2232, loss 0.438332, acc 0.796875, prec 0.051664, recall 0.831206
2017-12-10T04:26:17.026353: step 2233, loss 0.707468, acc 0.796875, prec 0.0516694, recall 0.83126
2017-12-10T04:26:17.298720: step 2234, loss 0.563085, acc 0.828125, prec 0.0516958, recall 0.831368
2017-12-10T04:26:17.563540: step 2235, loss 0.531626, acc 0.78125, prec 0.0516814, recall 0.831368
2017-12-10T04:26:17.830703: step 2236, loss 0.861257, acc 0.78125, prec 0.0516859, recall 0.831421
2017-12-10T04:26:18.089939: step 2237, loss 0.71787, acc 0.765625, prec 0.0516705, recall 0.831421
2017-12-10T04:26:18.357178: step 2238, loss 0.976759, acc 0.703125, prec 0.0516699, recall 0.831475
2017-12-10T04:26:18.619181: step 2239, loss 0.350353, acc 0.84375, prec 0.0516784, recall 0.831529
2017-12-10T04:26:18.882973: step 2240, loss 0.264065, acc 0.90625, prec 0.051691, recall 0.831582
2017-12-10T04:26:19.146425: step 2241, loss 0.606106, acc 0.78125, prec 0.0516955, recall 0.831636
2017-12-10T04:26:19.417227: step 2242, loss 0.264766, acc 0.9375, prec 0.0517289, recall 0.831743
2017-12-10T04:26:19.685009: step 2243, loss 0.11651, acc 0.953125, prec 0.0517634, recall 0.83185
2017-12-10T04:26:19.952549: step 2244, loss 0.164218, acc 0.921875, prec 0.0517582, recall 0.83185
2017-12-10T04:26:20.214929: step 2245, loss 0.182664, acc 0.953125, prec 0.0517552, recall 0.83185
2017-12-10T04:26:20.486201: step 2246, loss 0.248745, acc 0.90625, prec 0.051749, recall 0.83185
2017-12-10T04:26:20.753267: step 2247, loss 0.110666, acc 0.953125, prec 0.0517647, recall 0.831903
2017-12-10T04:26:21.011101: step 2248, loss 0.44826, acc 0.96875, prec 0.0518002, recall 0.83201
2017-12-10T04:26:21.278581: step 2249, loss 1.11139, acc 0.953125, prec 0.0518533, recall 0.83217
2017-12-10T04:26:21.544066: step 2250, loss 0.188169, acc 0.953125, prec 0.051869, recall 0.832223
2017-12-10T04:26:21.804454: step 2251, loss 0.308515, acc 0.875, prec 0.0518608, recall 0.832223
2017-12-10T04:26:22.069577: step 2252, loss 0.0511161, acc 0.984375, prec 0.0518598, recall 0.832223
2017-12-10T04:26:22.348260: step 2253, loss 0.365165, acc 0.953125, prec 0.0518754, recall 0.832276
2017-12-10T04:26:22.616131: step 2254, loss 0.482503, acc 0.953125, prec 0.0518911, recall 0.83233
2017-12-10T04:26:22.882846: step 2255, loss 0.238121, acc 0.953125, prec 0.051888, recall 0.83233
2017-12-10T04:26:23.148453: step 2256, loss 0.145656, acc 0.9375, prec 0.0519214, recall 0.832436
2017-12-10T04:26:23.407244: step 2257, loss 0.0637741, acc 0.984375, prec 0.0519391, recall 0.832489
2017-12-10T04:26:23.668892: step 2258, loss 0.136399, acc 0.921875, prec 0.0519339, recall 0.832489
2017-12-10T04:26:23.934404: step 2259, loss 0.0732012, acc 0.96875, prec 0.0519319, recall 0.832489
2017-12-10T04:26:24.196669: step 2260, loss 0.094124, acc 0.953125, prec 0.0519288, recall 0.832489
2017-12-10T04:26:25.169044: step 2261, loss 0.143364, acc 0.953125, prec 0.0519257, recall 0.832489
2017-12-10T04:26:25.532800: step 2262, loss 0.332264, acc 0.921875, prec 0.0519206, recall 0.832489
2017-12-10T04:26:25.797754: step 2263, loss 0.28825, acc 0.90625, prec 0.0519145, recall 0.832489
2017-12-10T04:26:26.172743: step 2264, loss 0.18646, acc 0.921875, prec 0.0519093, recall 0.832489
2017-12-10T04:26:26.926950: step 2265, loss 0.0688898, acc 1, prec 0.0519655, recall 0.832648
2017-12-10T04:26:28.124629: step 2266, loss 0.685438, acc 0.96875, prec 0.0520009, recall 0.832754
2017-12-10T04:26:28.503991: step 2267, loss 0.572282, acc 0.921875, prec 0.0520144, recall 0.832807
2017-12-10T04:26:28.790405: step 2268, loss 0.0956202, acc 0.953125, prec 0.0520301, recall 0.832859
2017-12-10T04:26:29.073813: step 2269, loss 0.163906, acc 0.921875, prec 0.0520249, recall 0.832859
2017-12-10T04:26:29.336089: step 2270, loss 0.159934, acc 0.96875, prec 0.0520416, recall 0.832912
2017-12-10T04:26:29.599247: step 2271, loss 0.165561, acc 0.921875, prec 0.0520365, recall 0.832912
2017-12-10T04:26:29.869614: step 2272, loss 0.337636, acc 0.96875, prec 0.0520905, recall 0.83307
2017-12-10T04:26:30.147332: step 2273, loss 0.149119, acc 0.96875, prec 0.0521072, recall 0.833123
2017-12-10T04:26:30.425028: step 2274, loss 0.96686, acc 0.96875, prec 0.0521238, recall 0.833176
2017-12-10T04:26:30.702279: step 2275, loss 2.52324, acc 0.828125, prec 0.0521135, recall 0.832913
2017-12-10T04:26:30.975084: step 2276, loss 0.921786, acc 0.9375, prec 0.0521281, recall 0.832966
2017-12-10T04:26:31.244876: step 2277, loss 0.426764, acc 0.84375, prec 0.0521739, recall 0.833123
2017-12-10T04:26:31.510703: step 2278, loss 0.686515, acc 0.859375, prec 0.052202, recall 0.833228
2017-12-10T04:26:31.770913: step 2279, loss 0.834981, acc 0.78125, prec 0.0522063, recall 0.833281
2017-12-10T04:26:32.041490: step 2280, loss 0.447062, acc 0.859375, prec 0.0522531, recall 0.833438
2017-12-10T04:26:32.307139: step 2281, loss 1.00035, acc 0.734375, prec 0.0522356, recall 0.833438
2017-12-10T04:26:32.571059: step 2282, loss 1.10322, acc 0.703125, prec 0.0522534, recall 0.833543
2017-12-10T04:26:32.832687: step 2283, loss 0.66938, acc 0.75, prec 0.0522555, recall 0.833595
2017-12-10T04:26:33.100346: step 2284, loss 0.654714, acc 0.828125, prec 0.0522442, recall 0.833595
2017-12-10T04:26:33.360081: step 2285, loss 0.514179, acc 0.84375, prec 0.052234, recall 0.833595
2017-12-10T04:26:33.621831: step 2286, loss 0.497951, acc 0.828125, prec 0.0522227, recall 0.833595
2017-12-10T04:26:33.888851: step 2287, loss 0.604565, acc 0.84375, prec 0.0522497, recall 0.833699
2017-12-10T04:26:34.150559: step 2288, loss 0.562319, acc 0.8125, prec 0.052256, recall 0.833752
2017-12-10T04:26:34.418086: step 2289, loss 0.330649, acc 0.875, prec 0.0522478, recall 0.833752
2017-12-10T04:26:34.681860: step 2290, loss 0.316863, acc 0.84375, prec 0.0522934, recall 0.833908
2017-12-10T04:26:34.941652: step 2291, loss 0.826384, acc 0.734375, prec 0.0522945, recall 0.83396
2017-12-10T04:26:35.198014: step 2292, loss 0.477544, acc 0.8125, prec 0.052338, recall 0.834116
2017-12-10T04:26:35.459478: step 2293, loss 0.611269, acc 0.90625, prec 0.0523505, recall 0.834168
2017-12-10T04:26:35.735560: step 2294, loss 0.372438, acc 0.859375, prec 0.0523412, recall 0.834168
2017-12-10T04:26:35.999374: step 2295, loss 0.388649, acc 0.90625, prec 0.0523351, recall 0.834168
2017-12-10T04:26:36.267685: step 2296, loss 0.765928, acc 0.96875, prec 0.0523516, recall 0.83422
2017-12-10T04:26:36.538481: step 2297, loss 0.167744, acc 0.953125, prec 0.0523671, recall 0.834271
2017-12-10T04:26:36.803647: step 2298, loss 1.47559, acc 0.921875, prec 0.052363, recall 0.834011
2017-12-10T04:26:37.071587: step 2299, loss 0.13465, acc 0.921875, prec 0.0523579, recall 0.834011
2017-12-10T04:26:37.335716: step 2300, loss 0.302569, acc 0.953125, prec 0.052392, recall 0.834114
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-2300

2017-12-10T04:26:38.674422: step 2301, loss 0.159859, acc 0.9375, prec 0.052425, recall 0.834218
2017-12-10T04:26:38.945820: step 2302, loss 0.207956, acc 0.90625, prec 0.0524189, recall 0.834218
2017-12-10T04:26:39.212490: step 2303, loss 0.132291, acc 0.953125, prec 0.0524344, recall 0.83427
2017-12-10T04:26:39.474485: step 2304, loss 0.210353, acc 0.921875, prec 0.0524292, recall 0.83427
2017-12-10T04:26:39.736183: step 2305, loss 0.186016, acc 0.9375, prec 0.0524437, recall 0.834321
2017-12-10T04:26:40.009964: step 2306, loss 0.189867, acc 0.96875, prec 0.0524788, recall 0.834425
2017-12-10T04:26:40.274692: step 2307, loss 0.16728, acc 0.921875, prec 0.0524737, recall 0.834425
2017-12-10T04:26:40.534859: step 2308, loss 0.327866, acc 0.90625, prec 0.0525047, recall 0.834528
2017-12-10T04:26:40.799948: step 2309, loss 0.380757, acc 0.9375, prec 0.0525191, recall 0.834579
2017-12-10T04:26:41.062837: step 2310, loss 0.420294, acc 0.9375, prec 0.0525707, recall 0.834734
2017-12-10T04:26:41.327095: step 2311, loss 0.271246, acc 0.921875, prec 0.0526027, recall 0.834837
2017-12-10T04:26:41.593280: step 2312, loss 0.377401, acc 0.9375, prec 0.0525986, recall 0.834837
2017-12-10T04:26:41.859255: step 2313, loss 0.111529, acc 0.9375, prec 0.052613, recall 0.834888
2017-12-10T04:26:42.134557: step 2314, loss 0.185489, acc 0.96875, prec 0.0526295, recall 0.834939
2017-12-10T04:26:42.413333: step 2315, loss 0.116288, acc 0.984375, prec 0.0526285, recall 0.834939
2017-12-10T04:26:42.683838: step 2316, loss 0.48898, acc 0.9375, prec 0.0526429, recall 0.834991
2017-12-10T04:26:42.943748: step 2317, loss 0.109215, acc 0.96875, prec 0.0526409, recall 0.834991
2017-12-10T04:26:43.208927: step 2318, loss 0.28347, acc 0.921875, prec 0.0526728, recall 0.835093
2017-12-10T04:26:43.469744: step 2319, loss 0.259543, acc 0.953125, prec 0.0527068, recall 0.835196
2017-12-10T04:26:43.732320: step 2320, loss 0.177425, acc 0.984375, prec 0.0527244, recall 0.835247
2017-12-10T04:26:43.993838: step 2321, loss 0.152717, acc 0.96875, prec 0.0527408, recall 0.835298
2017-12-10T04:26:44.257442: step 2322, loss 0.217533, acc 0.9375, prec 0.0527553, recall 0.835349
2017-12-10T04:26:44.526881: step 2323, loss 0.0447993, acc 0.984375, prec 0.0527728, recall 0.8354
2017-12-10T04:26:44.788857: step 2324, loss 0.12133, acc 0.984375, prec 0.0528088, recall 0.835502
2017-12-10T04:26:45.061335: step 2325, loss 0.37071, acc 0.875, prec 0.0528377, recall 0.835604
2017-12-10T04:26:45.324416: step 2326, loss 0.514808, acc 0.984375, prec 0.0528552, recall 0.835655
2017-12-10T04:26:45.600389: step 2327, loss 0.263344, acc 0.921875, prec 0.0528871, recall 0.835756
2017-12-10T04:26:45.865909: step 2328, loss 0.367655, acc 0.9375, prec 0.05292, recall 0.835858
2017-12-10T04:26:46.131191: step 2329, loss 0.109008, acc 0.96875, prec 0.0529179, recall 0.835858
2017-12-10T04:26:46.393381: step 2330, loss 0.969163, acc 0.984375, prec 0.0529725, recall 0.83601
2017-12-10T04:26:46.662533: step 2331, loss 0.155834, acc 0.953125, prec 0.0529879, recall 0.836061
2017-12-10T04:26:46.935191: step 2332, loss 0.0821254, acc 0.96875, prec 0.0530044, recall 0.836111
2017-12-10T04:26:47.194853: step 2333, loss 0.157939, acc 0.96875, prec 0.0530208, recall 0.836162
2017-12-10T04:26:47.457544: step 2334, loss 0.192479, acc 0.9375, prec 0.0530167, recall 0.836162
2017-12-10T04:26:47.723435: step 2335, loss 0.22951, acc 0.921875, prec 0.0530485, recall 0.836263
2017-12-10T04:26:47.988373: step 2336, loss 0.250384, acc 0.921875, prec 0.0530433, recall 0.836263
2017-12-10T04:26:48.256041: step 2337, loss 0.572633, acc 0.9375, prec 0.0530947, recall 0.836414
2017-12-10T04:26:48.525841: step 2338, loss 0.177169, acc 0.9375, prec 0.0531091, recall 0.836464
2017-12-10T04:26:48.796529: step 2339, loss 0.422641, acc 0.859375, prec 0.0531183, recall 0.836515
2017-12-10T04:26:49.061433: step 2340, loss 0.342132, acc 0.875, prec 0.05311, recall 0.836515
2017-12-10T04:26:49.327653: step 2341, loss 0.372814, acc 0.90625, prec 0.0531408, recall 0.836615
2017-12-10T04:26:49.600408: step 2342, loss 0.381206, acc 0.921875, prec 0.0531726, recall 0.836716
2017-12-10T04:26:49.872547: step 2343, loss 0.291188, acc 0.921875, prec 0.0531859, recall 0.836766
2017-12-10T04:26:50.145651: step 2344, loss 0.439913, acc 0.875, prec 0.0532146, recall 0.836866
2017-12-10T04:26:50.412623: step 2345, loss 0.119566, acc 0.96875, prec 0.0532495, recall 0.836967
2017-12-10T04:26:50.675030: step 2346, loss 0.0635428, acc 0.953125, prec 0.0532463, recall 0.836967
2017-12-10T04:26:50.939385: step 2347, loss 0.174812, acc 0.96875, prec 0.0532443, recall 0.836967
2017-12-10T04:26:51.206146: step 2348, loss 1.07834, acc 0.96875, prec 0.0532792, recall 0.837067
2017-12-10T04:26:51.469853: step 2349, loss 0.319826, acc 0.921875, prec 0.053274, recall 0.837067
2017-12-10T04:26:51.737293: step 2350, loss 0.101614, acc 0.96875, prec 0.0532904, recall 0.837117
2017-12-10T04:26:51.999283: step 2351, loss 0.315808, acc 0.953125, prec 0.0533242, recall 0.837216
2017-12-10T04:26:52.262561: step 2352, loss 0.122102, acc 0.9375, prec 0.0533201, recall 0.837216
2017-12-10T04:26:52.521443: step 2353, loss 0.409037, acc 0.90625, prec 0.0533508, recall 0.837316
2017-12-10T04:26:52.785326: step 2354, loss 0.121053, acc 0.9375, prec 0.0533836, recall 0.837416
2017-12-10T04:26:53.058487: step 2355, loss 0.185611, acc 0.921875, prec 0.0533968, recall 0.837466
2017-12-10T04:26:53.325576: step 2356, loss 0.316742, acc 0.859375, prec 0.0533874, recall 0.837466
2017-12-10T04:26:53.588278: step 2357, loss 0.236379, acc 0.921875, prec 0.0533822, recall 0.837466
2017-12-10T04:26:53.857048: step 2358, loss 0.376288, acc 0.921875, prec 0.0533955, recall 0.837515
2017-12-10T04:26:54.130228: step 2359, loss 0.302632, acc 0.890625, prec 0.0533882, recall 0.837515
2017-12-10T04:26:54.396679: step 2360, loss 0.072231, acc 0.96875, prec 0.0534046, recall 0.837565
2017-12-10T04:26:54.655326: step 2361, loss 0.362916, acc 0.96875, prec 0.053421, recall 0.837615
2017-12-10T04:26:54.919045: step 2362, loss 0.339096, acc 0.90625, prec 0.0534516, recall 0.837714
2017-12-10T04:26:55.183007: step 2363, loss 0.775986, acc 0.953125, prec 0.0534854, recall 0.837813
2017-12-10T04:26:55.448894: step 2364, loss 1.23747, acc 0.9375, prec 0.0535007, recall 0.837607
2017-12-10T04:26:55.715808: step 2365, loss 0.0693029, acc 0.96875, prec 0.0534987, recall 0.837607
2017-12-10T04:26:55.989257: step 2366, loss 0.0835188, acc 0.96875, prec 0.053515, recall 0.837656
2017-12-10T04:26:56.248640: step 2367, loss 0.341502, acc 0.875, prec 0.0535067, recall 0.837656
2017-12-10T04:26:56.511148: step 2368, loss 0.408752, acc 0.875, prec 0.0534983, recall 0.837656
2017-12-10T04:26:56.781869: step 2369, loss 0.28254, acc 0.859375, prec 0.053489, recall 0.837656
2017-12-10T04:26:57.065376: step 2370, loss 0.47727, acc 0.875, prec 0.0534806, recall 0.837656
2017-12-10T04:26:57.336756: step 2371, loss 0.121488, acc 0.953125, prec 0.0534775, recall 0.837656
2017-12-10T04:26:57.600664: step 2372, loss 0.363254, acc 0.890625, prec 0.0534702, recall 0.837656
2017-12-10T04:26:57.863459: step 2373, loss 0.472677, acc 0.859375, prec 0.0534608, recall 0.837656
2017-12-10T04:26:58.132137: step 2374, loss 0.421123, acc 0.875, prec 0.0534525, recall 0.837656
2017-12-10T04:26:58.398578: step 2375, loss 0.238182, acc 0.921875, prec 0.0534657, recall 0.837706
2017-12-10T04:26:58.659964: step 2376, loss 0.695859, acc 0.890625, prec 0.0534953, recall 0.837805
2017-12-10T04:26:58.916508: step 2377, loss 0.0735456, acc 0.984375, prec 0.0535127, recall 0.837854
2017-12-10T04:26:59.173646: step 2378, loss 0.314525, acc 0.890625, prec 0.0535238, recall 0.837904
2017-12-10T04:26:59.440691: step 2379, loss 0.180766, acc 0.953125, prec 0.0535207, recall 0.837904
2017-12-10T04:26:59.708881: step 2380, loss 1.16347, acc 0.96875, prec 0.053537, recall 0.837953
2017-12-10T04:26:59.980456: step 2381, loss 0.0471371, acc 1, prec 0.053537, recall 0.837953
2017-12-10T04:27:00.248757: step 2382, loss 2.35396, acc 0.875, prec 0.0535481, recall 0.837747
2017-12-10T04:27:00.520589: step 2383, loss 0.410277, acc 0.890625, prec 0.0535593, recall 0.837797
2017-12-10T04:27:00.780883: step 2384, loss 0.378908, acc 0.96875, prec 0.0535756, recall 0.837846
2017-12-10T04:27:01.046864: step 2385, loss 0.152853, acc 0.953125, prec 0.0535909, recall 0.837895
2017-12-10T04:27:01.314780: step 2386, loss 0.666509, acc 0.84375, prec 0.0535989, recall 0.837945
2017-12-10T04:27:01.578691: step 2387, loss 0.217009, acc 0.90625, prec 0.0535926, recall 0.837945
2017-12-10T04:27:01.845386: step 2388, loss 0.228079, acc 0.90625, prec 0.0536048, recall 0.837994
2017-12-10T04:27:02.107035: step 2389, loss 0.376941, acc 0.828125, prec 0.0536117, recall 0.838043
2017-12-10T04:27:02.378273: step 2390, loss 0.499704, acc 0.8125, prec 0.0535992, recall 0.838043
2017-12-10T04:27:02.647403: step 2391, loss 0.483221, acc 0.859375, prec 0.0536082, recall 0.838092
2017-12-10T04:27:02.907898: step 2392, loss 0.502331, acc 0.796875, prec 0.0536314, recall 0.838191
2017-12-10T04:27:03.178207: step 2393, loss 0.337039, acc 0.890625, prec 0.0536425, recall 0.83824
2017-12-10T04:27:03.453020: step 2394, loss 0.336119, acc 0.859375, prec 0.0536515, recall 0.838289
2017-12-10T04:27:03.722223: step 2395, loss 0.281155, acc 0.890625, prec 0.0536442, recall 0.838289
2017-12-10T04:27:03.993103: step 2396, loss 0.35542, acc 0.875, prec 0.0536359, recall 0.838289
2017-12-10T04:27:04.253949: step 2397, loss 0.337869, acc 0.984375, prec 0.0536716, recall 0.838387
2017-12-10T04:27:04.522403: step 2398, loss 0.349284, acc 0.90625, prec 0.0536654, recall 0.838387
2017-12-10T04:27:04.786275: step 2399, loss 0.339501, acc 0.90625, prec 0.0536591, recall 0.838387
2017-12-10T04:27:05.049740: step 2400, loss 0.437766, acc 0.953125, prec 0.0536743, recall 0.838436

Evaluation:
2017-12-10T04:27:12.739950: step 2400, loss 2.50593, acc 0.953859, prec 0.0543524, recall 0.819843

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-2400

2017-12-10T04:27:13.985861: step 2401, loss 0.174806, acc 0.921875, prec 0.0543472, recall 0.819843
2017-12-10T04:27:14.254527: step 2402, loss 0.118228, acc 0.984375, prec 0.0543462, recall 0.819843
2017-12-10T04:27:14.518778: step 2403, loss 0.158322, acc 0.9375, prec 0.054342, recall 0.819843
2017-12-10T04:27:14.783467: step 2404, loss 0.122611, acc 0.9375, prec 0.0543378, recall 0.819843
2017-12-10T04:27:15.048085: step 2405, loss 0.19369, acc 0.953125, prec 0.0543528, recall 0.819896
2017-12-10T04:27:15.310572: step 2406, loss 0.0484946, acc 0.984375, prec 0.0543518, recall 0.819896
2017-12-10T04:27:15.581396: step 2407, loss 0.167038, acc 0.921875, prec 0.0543466, recall 0.819896
2017-12-10T04:27:15.849139: step 2408, loss 0.0314824, acc 1, prec 0.0543829, recall 0.82
2017-12-10T04:27:16.112874: step 2409, loss 0.310839, acc 0.921875, prec 0.0543959, recall 0.820052
2017-12-10T04:27:16.375574: step 2410, loss 0.0104294, acc 1, prec 0.0543959, recall 0.820052
2017-12-10T04:27:16.638629: step 2411, loss 0.468166, acc 1, prec 0.0544141, recall 0.820104
2017-12-10T04:27:16.906150: step 2412, loss 1.33376, acc 0.96875, prec 0.054413, recall 0.819867
2017-12-10T04:27:17.176095: step 2413, loss 0.0805551, acc 0.953125, prec 0.054428, recall 0.819919
2017-12-10T04:27:17.441482: step 2414, loss 0.221217, acc 0.953125, prec 0.0544431, recall 0.819971
2017-12-10T04:27:17.715037: step 2415, loss 0.0844964, acc 0.953125, prec 0.0544581, recall 0.820023
2017-12-10T04:27:17.987032: step 2416, loss 0.108304, acc 1, prec 0.0544763, recall 0.820075
2017-12-10T04:27:18.251767: step 2417, loss 0.0576895, acc 0.984375, prec 0.0544752, recall 0.820075
2017-12-10T04:27:18.513115: step 2418, loss 0.147577, acc 0.96875, prec 0.0544731, recall 0.820075
2017-12-10T04:27:18.774775: step 2419, loss 0.227271, acc 0.96875, prec 0.0545074, recall 0.820179
2017-12-10T04:27:19.042970: step 2420, loss 0.205262, acc 0.953125, prec 0.0545042, recall 0.820179
2017-12-10T04:27:19.306474: step 2421, loss 0.242767, acc 0.953125, prec 0.0545011, recall 0.820179
2017-12-10T04:27:19.566072: step 2422, loss 0.412735, acc 0.90625, prec 0.054513, recall 0.820231
2017-12-10T04:27:19.828826: step 2423, loss 0.125898, acc 0.96875, prec 0.054529, recall 0.820283
2017-12-10T04:27:20.095047: step 2424, loss 3.90608, acc 0.953125, prec 0.0545633, recall 0.82015
2017-12-10T04:27:20.362985: step 2425, loss 0.239749, acc 0.984375, prec 0.0545804, recall 0.820202
2017-12-10T04:27:20.623896: step 2426, loss 1.75887, acc 0.828125, prec 0.054588, recall 0.820017
2017-12-10T04:27:20.890184: step 2427, loss 0.463882, acc 0.890625, prec 0.0545989, recall 0.820069
2017-12-10T04:27:21.153186: step 2428, loss 0.0772604, acc 0.96875, prec 0.0545968, recall 0.820069
2017-12-10T04:27:21.413267: step 2429, loss 0.22482, acc 0.921875, prec 0.0545915, recall 0.820069
2017-12-10T04:27:21.674530: step 2430, loss 0.337384, acc 0.828125, prec 0.0545981, recall 0.820121
2017-12-10T04:27:21.931358: step 2431, loss 0.63983, acc 0.828125, prec 0.0545866, recall 0.820121
2017-12-10T04:27:22.196156: step 2432, loss 0.726819, acc 0.765625, prec 0.054589, recall 0.820173
2017-12-10T04:27:22.464604: step 2433, loss 0.477438, acc 0.8125, prec 0.0545765, recall 0.820173
2017-12-10T04:27:22.722294: step 2434, loss 1.75124, acc 0.8125, prec 0.0545821, recall 0.820225
2017-12-10T04:27:22.991189: step 2435, loss 1.08775, acc 0.71875, prec 0.0546176, recall 0.82038
2017-12-10T04:27:23.252269: step 2436, loss 0.575346, acc 0.875, prec 0.0546273, recall 0.820432
2017-12-10T04:27:23.510766: step 2437, loss 0.503275, acc 0.8125, prec 0.054651, recall 0.820535
2017-12-10T04:27:23.774774: step 2438, loss 0.772763, acc 0.8125, prec 0.0546565, recall 0.820587
2017-12-10T04:27:24.041175: step 2439, loss 1.04364, acc 0.71875, prec 0.0546377, recall 0.820587
2017-12-10T04:27:24.307122: step 2440, loss 0.769954, acc 0.84375, prec 0.0546453, recall 0.820638
2017-12-10T04:27:24.568974: step 2441, loss 0.560063, acc 0.78125, prec 0.0546669, recall 0.820741
2017-12-10T04:27:24.833662: step 2442, loss 0.524519, acc 0.84375, prec 0.0546745, recall 0.820793
2017-12-10T04:27:25.097032: step 2443, loss 0.645509, acc 0.8125, prec 0.05468, recall 0.820844
2017-12-10T04:27:25.360088: step 2444, loss 0.429166, acc 0.875, prec 0.0546717, recall 0.820844
2017-12-10T04:27:25.620463: step 2445, loss 0.369876, acc 0.875, prec 0.0547175, recall 0.820998
2017-12-10T04:27:25.884281: step 2446, loss 0.252313, acc 0.84375, prec 0.0547251, recall 0.82105
2017-12-10T04:27:26.146659: step 2447, loss 0.296779, acc 0.859375, prec 0.0547338, recall 0.821101
2017-12-10T04:27:26.407136: step 2448, loss 0.289125, acc 0.921875, prec 0.0547466, recall 0.821152
2017-12-10T04:27:26.678918: step 2449, loss 10.014, acc 0.921875, prec 0.0547786, recall 0.821019
2017-12-10T04:27:26.954719: step 2450, loss 0.0963611, acc 0.9375, prec 0.0547744, recall 0.821019
2017-12-10T04:27:27.222473: step 2451, loss 0.229567, acc 0.9375, prec 0.0547702, recall 0.821019
2017-12-10T04:27:27.486742: step 2452, loss 0.460398, acc 0.9375, prec 0.0547841, recall 0.821071
2017-12-10T04:27:27.766307: step 2453, loss 0.385145, acc 0.859375, prec 0.0547927, recall 0.821122
2017-12-10T04:27:28.027265: step 2454, loss 0.152153, acc 0.9375, prec 0.0547885, recall 0.821122
2017-12-10T04:27:28.294039: step 2455, loss 0.0882074, acc 0.984375, prec 0.0548236, recall 0.821224
2017-12-10T04:27:28.556849: step 2456, loss 2.41608, acc 0.921875, prec 0.0548194, recall 0.820989
2017-12-10T04:27:28.826457: step 2457, loss 0.842672, acc 0.921875, prec 0.0548683, recall 0.821143
2017-12-10T04:27:29.093654: step 2458, loss 0.346035, acc 0.90625, prec 0.0548981, recall 0.821245
2017-12-10T04:27:29.356549: step 2459, loss 0.342594, acc 0.90625, prec 0.0549459, recall 0.821398
2017-12-10T04:27:29.624344: step 2460, loss 0.736273, acc 0.828125, prec 0.0549344, recall 0.821398
2017-12-10T04:27:29.898374: step 2461, loss 0.326865, acc 0.859375, prec 0.0549249, recall 0.821398
2017-12-10T04:27:30.163210: step 2462, loss 0.593963, acc 0.8125, prec 0.0549304, recall 0.821449
2017-12-10T04:27:30.430737: step 2463, loss 0.478354, acc 0.8125, prec 0.0549358, recall 0.8215
2017-12-10T04:27:30.695095: step 2464, loss 0.490329, acc 0.8125, prec 0.0549233, recall 0.8215
2017-12-10T04:27:30.955399: step 2465, loss 0.513063, acc 0.828125, prec 0.0549118, recall 0.8215
2017-12-10T04:27:31.223201: step 2466, loss 0.739725, acc 0.765625, prec 0.0549141, recall 0.821551
2017-12-10T04:27:31.489486: step 2467, loss 0.527204, acc 0.875, prec 0.0549417, recall 0.821652
2017-12-10T04:27:31.760169: step 2468, loss 0.687299, acc 0.78125, prec 0.0549631, recall 0.821754
2017-12-10T04:27:32.017576: step 2469, loss 0.333228, acc 0.859375, prec 0.0550076, recall 0.821906
2017-12-10T04:27:32.273982: step 2470, loss 0.728388, acc 0.8125, prec 0.055031, recall 0.822007
2017-12-10T04:27:32.533669: step 2471, loss 0.219938, acc 0.921875, prec 0.0550438, recall 0.822058
2017-12-10T04:27:32.802217: step 2472, loss 0.479042, acc 0.890625, prec 0.0550364, recall 0.822058
2017-12-10T04:27:33.069761: step 2473, loss 0.337238, acc 0.9375, prec 0.0550682, recall 0.822159
2017-12-10T04:27:33.329548: step 2474, loss 0.286944, acc 0.921875, prec 0.055081, recall 0.82221
2017-12-10T04:27:33.593986: step 2475, loss 0.793668, acc 0.90625, prec 0.0551286, recall 0.822361
2017-12-10T04:27:33.860882: step 2476, loss 0.11619, acc 0.96875, prec 0.0551265, recall 0.822361
2017-12-10T04:27:34.126792: step 2477, loss 0.30389, acc 0.921875, prec 0.0551392, recall 0.822411
2017-12-10T04:27:34.396743: step 2478, loss 0.121958, acc 0.984375, prec 0.0551741, recall 0.822512
2017-12-10T04:27:34.668816: step 2479, loss 0.132032, acc 0.90625, prec 0.0551678, recall 0.822512
2017-12-10T04:27:34.940468: step 2480, loss 0.232227, acc 0.9375, prec 0.0551816, recall 0.822562
2017-12-10T04:27:35.206629: step 2481, loss 0.204131, acc 0.9375, prec 0.0551774, recall 0.822562
2017-12-10T04:27:35.469567: step 2482, loss 4.6334, acc 0.984375, prec 0.0552133, recall 0.82243
2017-12-10T04:27:35.738652: step 2483, loss 0.093423, acc 0.96875, prec 0.0552471, recall 0.82253
2017-12-10T04:27:36.010457: step 2484, loss 0.08023, acc 0.984375, prec 0.0552461, recall 0.82253
2017-12-10T04:27:36.243471: step 2485, loss 0.120052, acc 0.923077, prec 0.0552599, recall 0.822581
2017-12-10T04:27:36.512662: step 2486, loss 0.242069, acc 0.953125, prec 0.0552747, recall 0.822631
2017-12-10T04:27:36.781292: step 2487, loss 0.263556, acc 0.921875, prec 0.0552874, recall 0.822681
2017-12-10T04:27:37.041118: step 2488, loss 0.211241, acc 0.921875, prec 0.0553001, recall 0.822731
2017-12-10T04:27:37.305922: step 2489, loss 0.21053, acc 0.921875, prec 0.0552948, recall 0.822731
2017-12-10T04:27:37.575456: step 2490, loss 0.086811, acc 0.96875, prec 0.0553107, recall 0.822781
2017-12-10T04:27:37.838069: step 2491, loss 0.279274, acc 0.890625, prec 0.0553392, recall 0.822881
2017-12-10T04:27:38.109684: step 2492, loss 1.66121, acc 0.953125, prec 0.055355, recall 0.822699
2017-12-10T04:27:38.375379: step 2493, loss 0.435266, acc 0.859375, prec 0.0553815, recall 0.822799
2017-12-10T04:27:38.636697: step 2494, loss 0.642637, acc 0.859375, prec 0.0554258, recall 0.822949
2017-12-10T04:27:38.901865: step 2495, loss 0.557328, acc 0.796875, prec 0.0554301, recall 0.822999
2017-12-10T04:27:39.165605: step 2496, loss 0.305764, acc 0.90625, prec 0.0554237, recall 0.822999
2017-12-10T04:27:39.427301: step 2497, loss 0.626293, acc 0.84375, prec 0.0554132, recall 0.822999
2017-12-10T04:27:39.690474: step 2498, loss 0.554748, acc 0.859375, prec 0.0554038, recall 0.822999
2017-12-10T04:27:39.953106: step 2499, loss 0.41025, acc 0.828125, prec 0.0554101, recall 0.823049
2017-12-10T04:27:40.211577: step 2500, loss 0.559503, acc 0.875, prec 0.0554017, recall 0.823049
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-2500

2017-12-10T04:27:41.507680: step 2501, loss 0.46685, acc 0.859375, prec 0.0554281, recall 0.823148
2017-12-10T04:27:41.767993: step 2502, loss 0.475984, acc 0.859375, prec 0.0554186, recall 0.823148
2017-12-10T04:27:42.042477: step 2503, loss 0.21196, acc 0.9375, prec 0.0554323, recall 0.823198
2017-12-10T04:27:42.308069: step 2504, loss 0.20544, acc 0.90625, prec 0.055426, recall 0.823198
2017-12-10T04:27:42.573774: step 2505, loss 0.328311, acc 0.90625, prec 0.0554376, recall 0.823248
2017-12-10T04:27:42.839170: step 2506, loss 0.143977, acc 0.953125, prec 0.0554524, recall 0.823298
2017-12-10T04:27:43.098432: step 2507, loss 0.193228, acc 0.953125, prec 0.0554492, recall 0.823298
2017-12-10T04:27:43.361060: step 2508, loss 0.467722, acc 0.9375, prec 0.0554987, recall 0.823447
2017-12-10T04:27:43.632891: step 2509, loss 0.460707, acc 0.890625, prec 0.0555092, recall 0.823496
2017-12-10T04:27:43.900752: step 2510, loss 0.366708, acc 0.9375, prec 0.0555408, recall 0.823596
2017-12-10T04:27:44.165766: step 2511, loss 0.122347, acc 0.96875, prec 0.0555387, recall 0.823596
2017-12-10T04:27:44.439642: step 2512, loss 0.247891, acc 0.921875, prec 0.0555335, recall 0.823596
2017-12-10T04:27:44.706893: step 2513, loss 0.192122, acc 0.921875, prec 0.0555461, recall 0.823645
2017-12-10T04:27:44.979260: step 2514, loss 0.0595704, acc 0.984375, prec 0.0555629, recall 0.823695
2017-12-10T04:27:45.236922: step 2515, loss 0.184091, acc 0.921875, prec 0.0555755, recall 0.823744
2017-12-10T04:27:45.503979: step 2516, loss 0.0663132, acc 0.96875, prec 0.0555734, recall 0.823744
2017-12-10T04:27:45.769695: step 2517, loss 0.0796095, acc 0.96875, prec 0.0555892, recall 0.823793
2017-12-10T04:27:46.043967: step 2518, loss 0.0667798, acc 0.96875, prec 0.0555871, recall 0.823793
2017-12-10T04:27:46.304842: step 2519, loss 0.0676491, acc 0.984375, prec 0.0555861, recall 0.823793
2017-12-10T04:27:46.574749: step 2520, loss 2.016, acc 0.96875, prec 0.0556029, recall 0.823612
2017-12-10T04:27:46.842705: step 2521, loss 1.66788, acc 0.9375, prec 0.0556355, recall 0.82348
2017-12-10T04:27:47.116298: step 2522, loss 0.232756, acc 0.984375, prec 0.0556702, recall 0.823579
2017-12-10T04:27:47.389485: step 2523, loss 0.317265, acc 0.9375, prec 0.0557017, recall 0.823678
2017-12-10T04:27:47.656292: step 2524, loss 0.142949, acc 0.984375, prec 0.0557364, recall 0.823776
2017-12-10T04:27:47.921136: step 2525, loss 0.308949, acc 0.875, prec 0.055728, recall 0.823776
2017-12-10T04:27:48.187030: step 2526, loss 0.279253, acc 0.90625, prec 0.0557395, recall 0.823825
2017-12-10T04:27:48.451103: step 2527, loss 0.127145, acc 0.953125, prec 0.0557363, recall 0.823825
2017-12-10T04:27:48.713346: step 2528, loss 0.254987, acc 0.875, prec 0.0557279, recall 0.823825
2017-12-10T04:27:48.981971: step 2529, loss 0.528945, acc 0.828125, prec 0.0557163, recall 0.823825
2017-12-10T04:27:49.240381: step 2530, loss 0.318808, acc 0.90625, prec 0.05571, recall 0.823825
2017-12-10T04:27:49.501069: step 2531, loss 0.475339, acc 0.890625, prec 0.0557205, recall 0.823875
2017-12-10T04:27:49.762275: step 2532, loss 0.525034, acc 0.859375, prec 0.0557288, recall 0.823924
2017-12-10T04:27:50.025862: step 2533, loss 0.866815, acc 0.8125, prec 0.0557519, recall 0.824022
2017-12-10T04:27:50.289804: step 2534, loss 0.441881, acc 0.90625, prec 0.0557634, recall 0.824071
2017-12-10T04:27:50.559837: step 2535, loss 0.33809, acc 0.859375, prec 0.0557718, recall 0.824121
2017-12-10T04:27:50.818845: step 2536, loss 0.568809, acc 0.90625, prec 0.0557833, recall 0.82417
2017-12-10T04:27:51.083043: step 2537, loss 0.269986, acc 0.875, prec 0.0558105, recall 0.824268
2017-12-10T04:27:51.348297: step 2538, loss 0.443585, acc 0.921875, prec 0.0558409, recall 0.824366
2017-12-10T04:27:51.612743: step 2539, loss 0.327678, acc 0.890625, prec 0.0558514, recall 0.824415
2017-12-10T04:27:51.876205: step 2540, loss 0.173212, acc 0.921875, prec 0.0558817, recall 0.824513
2017-12-10T04:27:52.138336: step 2541, loss 0.699586, acc 0.9375, prec 0.0558953, recall 0.824561
2017-12-10T04:27:52.399676: step 2542, loss 0.498868, acc 0.890625, prec 0.0559058, recall 0.82461
2017-12-10T04:27:52.663603: step 2543, loss 0.101055, acc 0.921875, prec 0.0559361, recall 0.824708
2017-12-10T04:27:52.932484: step 2544, loss 0.370165, acc 0.90625, prec 0.0559476, recall 0.824757
2017-12-10T04:27:53.208860: step 2545, loss 0.242713, acc 0.953125, prec 0.0559623, recall 0.824805
2017-12-10T04:27:53.475347: step 2546, loss 0.0720686, acc 0.984375, prec 0.0559968, recall 0.824903
2017-12-10T04:27:53.737353: step 2547, loss 0.1002, acc 0.953125, prec 0.0559937, recall 0.824903
2017-12-10T04:27:53.999489: step 2548, loss 0.39726, acc 0.921875, prec 0.0559884, recall 0.824903
2017-12-10T04:27:54.262451: step 2549, loss 0.142632, acc 0.96875, prec 0.0559863, recall 0.824903
2017-12-10T04:27:54.532310: step 2550, loss 0.0710011, acc 0.984375, prec 0.0560386, recall 0.825049
2017-12-10T04:27:54.795767: step 2551, loss 0.137112, acc 0.984375, prec 0.0560732, recall 0.825146
2017-12-10T04:27:55.062509: step 2552, loss 0.119509, acc 0.96875, prec 0.0560711, recall 0.825146
2017-12-10T04:27:55.326712: step 2553, loss 0.307705, acc 0.921875, prec 0.0560836, recall 0.825194
2017-12-10T04:27:55.592969: step 2554, loss 0.194229, acc 0.90625, prec 0.0560772, recall 0.825194
2017-12-10T04:27:55.876794: step 2555, loss 1.69185, acc 0.953125, prec 0.0560929, recall 0.825014
2017-12-10T04:27:56.147643: step 2556, loss 0.161531, acc 0.96875, prec 0.0561442, recall 0.825159
2017-12-10T04:27:56.412818: step 2557, loss 0.113752, acc 0.96875, prec 0.0561421, recall 0.825159
2017-12-10T04:27:56.673494: step 2558, loss 0.229326, acc 0.953125, prec 0.0561567, recall 0.825208
2017-12-10T04:27:56.947234: step 2559, loss 0.0724965, acc 0.984375, prec 0.0561556, recall 0.825208
2017-12-10T04:27:57.211073: step 2560, loss 0.172678, acc 0.9375, prec 0.056187, recall 0.825305
2017-12-10T04:27:57.474238: step 2561, loss 0.101132, acc 0.984375, prec 0.0562215, recall 0.825401
2017-12-10T04:27:57.745008: step 2562, loss 0.170754, acc 0.984375, prec 0.0562382, recall 0.82545
2017-12-10T04:27:58.009266: step 2563, loss 0.14741, acc 0.96875, prec 0.0562361, recall 0.82545
2017-12-10T04:27:58.278758: step 2564, loss 0.189937, acc 0.9375, prec 0.0562496, recall 0.825498
2017-12-10T04:27:58.542953: step 2565, loss 0.121109, acc 0.96875, prec 0.0562475, recall 0.825498
2017-12-10T04:27:58.807280: step 2566, loss 0.225352, acc 0.953125, prec 0.0562443, recall 0.825498
2017-12-10T04:27:59.069276: step 2567, loss 0.106775, acc 0.96875, prec 0.0562422, recall 0.825498
2017-12-10T04:27:59.341483: step 2568, loss 0.287234, acc 0.890625, prec 0.0562704, recall 0.825594
2017-12-10T04:27:59.595016: step 2569, loss 0.170561, acc 0.953125, prec 0.056285, recall 0.825642
2017-12-10T04:27:59.858582: step 2570, loss 0.258719, acc 0.953125, prec 0.0563173, recall 0.825739
2017-12-10T04:28:00.124749: step 2571, loss 0.139412, acc 0.9375, prec 0.0563309, recall 0.825787
2017-12-10T04:28:00.399985: step 2572, loss 0.315693, acc 0.890625, prec 0.0563412, recall 0.825835
2017-12-10T04:28:00.667652: step 2573, loss 0.11834, acc 0.96875, prec 0.0563746, recall 0.825931
2017-12-10T04:28:00.932198: step 2574, loss 0.539203, acc 0.875, prec 0.0563839, recall 0.825979
2017-12-10T04:28:01.194409: step 2575, loss 0.1259, acc 0.953125, prec 0.0563807, recall 0.825979
2017-12-10T04:28:01.459750: step 2576, loss 0.159774, acc 0.96875, prec 0.0564141, recall 0.826075
2017-12-10T04:28:01.719864: step 2577, loss 0.274715, acc 0.90625, prec 0.0564077, recall 0.826075
2017-12-10T04:28:01.991382: step 2578, loss 0.0971293, acc 0.96875, prec 0.0564234, recall 0.826123
2017-12-10T04:28:02.250624: step 2579, loss 0.0311338, acc 1, prec 0.0564234, recall 0.826123
2017-12-10T04:28:02.513356: step 2580, loss 0.105078, acc 0.953125, prec 0.056438, recall 0.826171
2017-12-10T04:28:02.775837: step 2581, loss 0.0234406, acc 1, prec 0.056438, recall 0.826171
2017-12-10T04:28:03.037982: step 2582, loss 0.271413, acc 1, prec 0.0564735, recall 0.826267
2017-12-10T04:28:03.303199: step 2583, loss 0.0935874, acc 0.953125, prec 0.056488, recall 0.826314
2017-12-10T04:28:03.565682: step 2584, loss 0.180307, acc 0.984375, prec 0.0565225, recall 0.82641
2017-12-10T04:28:03.827579: step 2585, loss 0.118758, acc 0.984375, prec 0.0565569, recall 0.826505
2017-12-10T04:28:04.090331: step 2586, loss 0.0848145, acc 0.96875, prec 0.0565548, recall 0.826505
2017-12-10T04:28:04.362390: step 2587, loss 0.287564, acc 0.921875, prec 0.0565495, recall 0.826505
2017-12-10T04:28:04.629856: step 2588, loss 0.119719, acc 0.953125, prec 0.0565463, recall 0.826505
2017-12-10T04:28:04.893701: step 2589, loss 0.0613782, acc 0.96875, prec 0.0565441, recall 0.826505
2017-12-10T04:28:05.158865: step 2590, loss 0.992357, acc 0.953125, prec 0.0565587, recall 0.826553
2017-12-10T04:28:05.428934: step 2591, loss 0.0238358, acc 1, prec 0.0565942, recall 0.826648
2017-12-10T04:28:05.690124: step 2592, loss 0.0204352, acc 1, prec 0.0565942, recall 0.826648
2017-12-10T04:28:05.951626: step 2593, loss 0.148799, acc 0.9375, prec 0.0565899, recall 0.826648
2017-12-10T04:28:06.219338: step 2594, loss 0.0875581, acc 0.96875, prec 0.0565878, recall 0.826648
2017-12-10T04:28:06.489899: step 2595, loss 0.0848477, acc 0.953125, prec 0.0565846, recall 0.826648
2017-12-10T04:28:06.757403: step 2596, loss 0.222009, acc 0.953125, prec 0.0566169, recall 0.826744
2017-12-10T04:28:07.021094: step 2597, loss 0.0621095, acc 0.984375, prec 0.0566158, recall 0.826744
2017-12-10T04:28:07.285690: step 2598, loss 0.0526825, acc 0.96875, prec 0.0566137, recall 0.826744
2017-12-10T04:28:07.548973: step 2599, loss 0.10212, acc 0.9375, prec 0.0566094, recall 0.826744
2017-12-10T04:28:07.815247: step 2600, loss 0.113657, acc 0.96875, prec 0.0566251, recall 0.826791
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-2600

2017-12-10T04:28:09.108672: step 2601, loss 0.121821, acc 0.953125, prec 0.0566219, recall 0.826791
2017-12-10T04:28:09.368856: step 2602, loss 0.183476, acc 0.921875, prec 0.056652, recall 0.826886
2017-12-10T04:28:09.632031: step 2603, loss 0.23275, acc 0.96875, prec 0.0566676, recall 0.826934
2017-12-10T04:28:09.895389: step 2604, loss 0.118977, acc 0.96875, prec 0.0567009, recall 0.827029
2017-12-10T04:28:10.163297: step 2605, loss 0.210216, acc 0.921875, prec 0.0567311, recall 0.827123
2017-12-10T04:28:10.424341: step 2606, loss 0.126306, acc 0.953125, prec 0.0567456, recall 0.827171
2017-12-10T04:28:10.685903: step 2607, loss 0.016294, acc 1, prec 0.0567456, recall 0.827171
2017-12-10T04:28:10.948413: step 2608, loss 0.43216, acc 0.953125, prec 0.0567778, recall 0.827265
2017-12-10T04:28:11.212346: step 2609, loss 0.113263, acc 0.984375, prec 0.0567768, recall 0.827265
2017-12-10T04:28:11.475563: step 2610, loss 0.126479, acc 0.96875, prec 0.0567923, recall 0.827313
2017-12-10T04:28:11.739357: step 2611, loss 0.101964, acc 0.96875, prec 0.0568079, recall 0.82736
2017-12-10T04:28:12.011167: step 2612, loss 0.0745698, acc 0.984375, prec 0.0568423, recall 0.827454
2017-12-10T04:28:12.285382: step 2613, loss 0.0733505, acc 0.953125, prec 0.0568391, recall 0.827454
2017-12-10T04:28:12.550944: step 2614, loss 0.0813516, acc 0.96875, prec 0.0568547, recall 0.827501
2017-12-10T04:28:12.823461: step 2615, loss 0.256508, acc 0.96875, prec 0.0568703, recall 0.827549
2017-12-10T04:28:13.099266: step 2616, loss 0.0236872, acc 1, prec 0.056888, recall 0.827596
2017-12-10T04:28:13.366214: step 2617, loss 0.127918, acc 0.96875, prec 0.0569035, recall 0.827643
2017-12-10T04:28:13.627464: step 2618, loss 0.212303, acc 1, prec 0.056939, recall 0.827737
2017-12-10T04:28:13.892758: step 2619, loss 0.0385581, acc 0.984375, prec 0.0569556, recall 0.827784
2017-12-10T04:28:14.153638: step 2620, loss 0.015626, acc 1, prec 0.0569556, recall 0.827784
2017-12-10T04:28:14.426255: step 2621, loss 2.25165, acc 0.953125, prec 0.0569535, recall 0.827558
2017-12-10T04:28:14.692400: step 2622, loss 0.0880573, acc 0.953125, prec 0.0569503, recall 0.827558
2017-12-10T04:28:14.959415: step 2623, loss 0.190096, acc 0.9375, prec 0.056946, recall 0.827558
2017-12-10T04:28:15.220587: step 2624, loss 0.0975714, acc 0.953125, prec 0.0569428, recall 0.827558
2017-12-10T04:28:15.480271: step 2625, loss 0.123968, acc 0.953125, prec 0.056975, recall 0.827652
2017-12-10T04:28:15.747001: step 2626, loss 0.373328, acc 0.875, prec 0.0570018, recall 0.827746
2017-12-10T04:28:16.018560: step 2627, loss 0.498182, acc 0.921875, prec 0.0570319, recall 0.82784
2017-12-10T04:28:16.282461: step 2628, loss 0.328265, acc 0.890625, prec 0.0570421, recall 0.827887
2017-12-10T04:28:16.546812: step 2629, loss 0.39468, acc 0.875, prec 0.0570512, recall 0.827934
2017-12-10T04:28:16.812966: step 2630, loss 0.148038, acc 0.953125, prec 0.057048, recall 0.827934
2017-12-10T04:28:17.082629: step 2631, loss 0.168426, acc 0.96875, prec 0.0570458, recall 0.827934
2017-12-10T04:28:17.348101: step 2632, loss 0.233941, acc 0.9375, prec 0.0570593, recall 0.82798
2017-12-10T04:28:17.618674: step 2633, loss 0.464743, acc 0.90625, prec 0.0570705, recall 0.828027
2017-12-10T04:28:17.885539: step 2634, loss 0.207263, acc 0.9375, prec 0.0570839, recall 0.828074
2017-12-10T04:28:18.147960: step 2635, loss 0.0471803, acc 1, prec 0.0571193, recall 0.828167
2017-12-10T04:28:18.411086: step 2636, loss 1.12394, acc 0.890625, prec 0.0571648, recall 0.828308
2017-12-10T04:28:18.680301: step 2637, loss 0.107847, acc 0.96875, prec 0.0571627, recall 0.828308
2017-12-10T04:28:18.950996: step 2638, loss 0.289022, acc 0.921875, prec 0.0571573, recall 0.828308
2017-12-10T04:28:19.211586: step 2639, loss 0.222265, acc 0.9375, prec 0.0571884, recall 0.828401
2017-12-10T04:28:19.474130: step 2640, loss 0.13652, acc 0.984375, prec 0.0572403, recall 0.82854
2017-12-10T04:28:19.736693: step 2641, loss 0.363312, acc 0.890625, prec 0.0572505, recall 0.828587
2017-12-10T04:28:20.002551: step 2642, loss 0.163918, acc 0.921875, prec 0.0573158, recall 0.828773
2017-12-10T04:28:20.264796: step 2643, loss 1.049, acc 0.90625, prec 0.0573446, recall 0.828865
2017-12-10T04:28:20.531699: step 2644, loss 0.0395509, acc 1, prec 0.0573623, recall 0.828912
2017-12-10T04:28:20.794982: step 2645, loss 0.264732, acc 0.921875, prec 0.0574099, recall 0.829051
2017-12-10T04:28:21.058732: step 2646, loss 0.298818, acc 0.921875, prec 0.0574222, recall 0.829097
2017-12-10T04:28:21.319440: step 2647, loss 0.160491, acc 0.9375, prec 0.0574179, recall 0.829097
2017-12-10T04:28:21.585907: step 2648, loss 0.338226, acc 0.890625, prec 0.0574104, recall 0.829097
2017-12-10T04:28:21.849015: step 2649, loss 0.147479, acc 0.96875, prec 0.0574435, recall 0.829189
2017-12-10T04:28:22.108503: step 2650, loss 0.394906, acc 0.890625, prec 0.057436, recall 0.829189
2017-12-10T04:28:22.380500: step 2651, loss 0.266019, acc 0.890625, prec 0.0574637, recall 0.829281
2017-12-10T04:28:22.651948: step 2652, loss 0.0946047, acc 0.953125, prec 0.0574781, recall 0.829328
2017-12-10T04:28:22.915713: step 2653, loss 0.240469, acc 0.9375, prec 0.0575091, recall 0.82942
2017-12-10T04:28:23.182410: step 2654, loss 0.200182, acc 0.96875, prec 0.0575422, recall 0.829512
2017-12-10T04:28:23.446739: step 2655, loss 0.222931, acc 0.9375, prec 0.0575379, recall 0.829512
2017-12-10T04:28:23.706862: step 2656, loss 0.123611, acc 0.953125, prec 0.0575347, recall 0.829512
2017-12-10T04:28:23.971313: step 2657, loss 0.181322, acc 0.9375, prec 0.0575304, recall 0.829512
2017-12-10T04:28:24.235400: step 2658, loss 0.303346, acc 0.90625, prec 0.0575239, recall 0.829512
2017-12-10T04:28:24.498367: step 2659, loss 0.176849, acc 0.890625, prec 0.0575164, recall 0.829512
2017-12-10T04:28:24.766047: step 2660, loss 0.0612457, acc 0.984375, prec 0.057533, recall 0.829558
2017-12-10T04:28:25.028412: step 2661, loss 0.0392536, acc 0.984375, prec 0.0575495, recall 0.829604
2017-12-10T04:28:25.299355: step 2662, loss 0.0738302, acc 0.984375, prec 0.0575837, recall 0.829696
2017-12-10T04:28:25.569708: step 2663, loss 0.095087, acc 0.984375, prec 0.0576002, recall 0.829741
2017-12-10T04:28:25.836728: step 2664, loss 0.0527247, acc 0.984375, prec 0.0576168, recall 0.829787
2017-12-10T04:28:26.103307: step 2665, loss 3.40411, acc 0.984375, prec 0.0576344, recall 0.82961
2017-12-10T04:28:26.374668: step 2666, loss 0.641116, acc 0.984375, prec 0.0576862, recall 0.829747
2017-12-10T04:28:26.644395: step 2667, loss 0.278777, acc 0.90625, prec 0.0576797, recall 0.829747
2017-12-10T04:28:26.916924: step 2668, loss 0.122645, acc 0.96875, prec 0.0576952, recall 0.829793
2017-12-10T04:28:27.183717: step 2669, loss 0.409715, acc 0.890625, prec 0.0577052, recall 0.829839
2017-12-10T04:28:27.445742: step 2670, loss 0.289851, acc 0.921875, prec 0.0576999, recall 0.829839
2017-12-10T04:28:27.707685: step 2671, loss 0.2759, acc 0.890625, prec 0.0577099, recall 0.829884
2017-12-10T04:28:27.966614: step 2672, loss 0.53827, acc 0.828125, prec 0.0577157, recall 0.82993
2017-12-10T04:28:28.236570: step 2673, loss 0.399062, acc 0.8125, prec 0.0577027, recall 0.82993
2017-12-10T04:28:28.500723: step 2674, loss 0.381373, acc 0.875, prec 0.0577293, recall 0.830022
2017-12-10T04:28:28.758822: step 2675, loss 0.149058, acc 0.921875, prec 0.0577591, recall 0.830113
2017-12-10T04:28:29.025599: step 2676, loss 0.314322, acc 0.875, prec 0.0577681, recall 0.830158
2017-12-10T04:28:29.282710: step 2677, loss 0.133945, acc 0.9375, prec 0.0577638, recall 0.830158
2017-12-10T04:28:29.555322: step 2678, loss 0.219312, acc 0.9375, prec 0.057777, recall 0.830204
2017-12-10T04:28:29.825275: step 2679, loss 0.201497, acc 0.953125, prec 0.0577914, recall 0.830249
2017-12-10T04:28:30.084394: step 2680, loss 0.351505, acc 0.875, prec 0.0577828, recall 0.830249
2017-12-10T04:28:30.353262: step 2681, loss 0.50767, acc 0.875, prec 0.0577917, recall 0.830295
2017-12-10T04:28:30.622124: step 2682, loss 0.452059, acc 0.84375, prec 0.0577809, recall 0.830295
2017-12-10T04:28:30.885067: step 2683, loss 0.438286, acc 0.84375, prec 0.0577702, recall 0.830295
2017-12-10T04:28:31.152494: step 2684, loss 0.457181, acc 0.859375, prec 0.0577605, recall 0.830295
2017-12-10T04:28:31.419081: step 2685, loss 0.658529, acc 0.9375, prec 0.0577737, recall 0.83034
2017-12-10T04:28:31.690813: step 2686, loss 0.0999642, acc 0.96875, prec 0.0577716, recall 0.83034
2017-12-10T04:28:31.952813: step 2687, loss 0.108219, acc 0.9375, prec 0.0577848, recall 0.830386
2017-12-10T04:28:32.223063: step 2688, loss 0.702308, acc 0.953125, prec 0.0577992, recall 0.830431
2017-12-10T04:28:32.498460: step 2689, loss 0.295902, acc 0.90625, prec 0.0577927, recall 0.830431
2017-12-10T04:28:32.769621: step 2690, loss 0.220394, acc 0.9375, prec 0.0577884, recall 0.830431
2017-12-10T04:28:33.037983: step 2691, loss 0.328127, acc 0.90625, prec 0.0577819, recall 0.830431
2017-12-10T04:28:33.303360: step 2692, loss 1.43274, acc 0.96875, prec 0.0577984, recall 0.830254
2017-12-10T04:28:33.572467: step 2693, loss 0.470158, acc 0.921875, prec 0.0578457, recall 0.830391
2017-12-10T04:28:33.836169: step 2694, loss 0.426996, acc 0.890625, prec 0.0578381, recall 0.830391
2017-12-10T04:28:34.099578: step 2695, loss 0.183693, acc 0.9375, prec 0.0578338, recall 0.830391
2017-12-10T04:28:34.360962: step 2696, loss 1.14146, acc 0.9375, prec 0.0578471, recall 0.830436
2017-12-10T04:28:34.628640: step 2697, loss 0.0990926, acc 0.9375, prec 0.0578428, recall 0.830436
2017-12-10T04:28:34.890858: step 2698, loss 0.328913, acc 0.90625, prec 0.0578539, recall 0.830481
2017-12-10T04:28:35.155989: step 2699, loss 0.184919, acc 0.953125, prec 0.0578682, recall 0.830527
2017-12-10T04:28:35.428435: step 2700, loss 0.415521, acc 0.9375, prec 0.0578989, recall 0.830617

Evaluation:
2017-12-10T04:28:43.005329: step 2700, loss 1.40731, acc 0.883469, prec 0.0584909, recall 0.82678

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-2700

2017-12-10T04:28:44.263178: step 2701, loss 0.307868, acc 0.90625, prec 0.0584845, recall 0.82678
2017-12-10T04:28:44.529622: step 2702, loss 0.366352, acc 0.921875, prec 0.0585306, recall 0.826913
2017-12-10T04:28:44.795415: step 2703, loss 0.237207, acc 0.890625, prec 0.0585402, recall 0.826958
2017-12-10T04:28:45.058083: step 2704, loss 0.502503, acc 0.8125, prec 0.0585275, recall 0.826958
2017-12-10T04:28:45.322703: step 2705, loss 0.322259, acc 0.890625, prec 0.05852, recall 0.826958
2017-12-10T04:28:45.588473: step 2706, loss 0.284051, acc 0.890625, prec 0.0585297, recall 0.827002
2017-12-10T04:28:45.847393: step 2707, loss 0.670167, acc 0.8125, prec 0.0585169, recall 0.827002
2017-12-10T04:28:46.111329: step 2708, loss 0.555753, acc 0.84375, prec 0.0585063, recall 0.827002
2017-12-10T04:28:46.370379: step 2709, loss 0.298463, acc 0.90625, prec 0.0585341, recall 0.827091
2017-12-10T04:28:46.632243: step 2710, loss 0.331257, acc 0.90625, prec 0.0585277, recall 0.827091
2017-12-10T04:28:46.903288: step 2711, loss 0.219303, acc 0.953125, prec 0.0585245, recall 0.827091
2017-12-10T04:28:47.164858: step 2712, loss 0.206019, acc 0.9375, prec 0.0585545, recall 0.827179
2017-12-10T04:28:47.434386: step 2713, loss 0.146687, acc 0.953125, prec 0.0585684, recall 0.827224
2017-12-10T04:28:47.696705: step 2714, loss 0.332902, acc 0.953125, prec 0.0586335, recall 0.827401
2017-12-10T04:28:47.971102: step 2715, loss 0.015082, acc 1, prec 0.0586335, recall 0.827401
2017-12-10T04:28:48.237012: step 2716, loss 0.0577449, acc 1, prec 0.0586506, recall 0.827445
2017-12-10T04:28:48.503569: step 2717, loss 0.0965514, acc 0.953125, prec 0.0586474, recall 0.827445
2017-12-10T04:28:48.768698: step 2718, loss 0.365249, acc 0.96875, prec 0.0586624, recall 0.827489
2017-12-10T04:28:49.031867: step 2719, loss 2.0405, acc 0.921875, prec 0.0586581, recall 0.827277
2017-12-10T04:28:49.301219: step 2720, loss 0.517318, acc 0.96875, prec 0.0587243, recall 0.827454
2017-12-10T04:28:49.568031: step 2721, loss 0.238679, acc 0.984375, prec 0.0587574, recall 0.827542
2017-12-10T04:28:49.837896: step 2722, loss 0.559077, acc 0.921875, prec 0.0587862, recall 0.82763
2017-12-10T04:28:50.103727: step 2723, loss 0.172889, acc 0.921875, prec 0.0587809, recall 0.82763
2017-12-10T04:28:50.367970: step 2724, loss 0.0393785, acc 0.96875, prec 0.0587958, recall 0.827674
2017-12-10T04:28:50.629381: step 2725, loss 0.168325, acc 0.9375, prec 0.0587915, recall 0.827674
2017-12-10T04:28:50.894622: step 2726, loss 0.129206, acc 0.96875, prec 0.0588065, recall 0.827718
2017-12-10T04:28:51.159292: step 2727, loss 1.00466, acc 0.90625, prec 0.0588171, recall 0.827762
2017-12-10T04:28:51.429427: step 2728, loss 0.476635, acc 0.875, prec 0.0588086, recall 0.827762
2017-12-10T04:28:51.693278: step 2729, loss 0.433545, acc 0.90625, prec 0.0588193, recall 0.827806
2017-12-10T04:28:51.952891: step 2730, loss 0.260096, acc 0.890625, prec 0.0588118, recall 0.827806
2017-12-10T04:28:52.223299: step 2731, loss 0.145794, acc 0.9375, prec 0.0588075, recall 0.827806
2017-12-10T04:28:52.490528: step 2732, loss 0.643281, acc 0.84375, prec 0.0587969, recall 0.827806
2017-12-10T04:28:52.753790: step 2733, loss 0.277311, acc 0.890625, prec 0.0588065, recall 0.82785
2017-12-10T04:28:53.015534: step 2734, loss 0.599126, acc 0.84375, prec 0.0588129, recall 0.827894
2017-12-10T04:28:53.278647: step 2735, loss 0.274492, acc 0.90625, prec 0.0588747, recall 0.828069
2017-12-10T04:28:53.540711: step 2736, loss 0.863226, acc 0.828125, prec 0.05888, recall 0.828113
2017-12-10T04:28:53.803460: step 2737, loss 0.331412, acc 0.90625, prec 0.0588736, recall 0.828113
2017-12-10T04:28:54.070041: step 2738, loss 0.128054, acc 0.9375, prec 0.0588693, recall 0.828113
2017-12-10T04:28:54.334015: step 2739, loss 0.560414, acc 0.875, prec 0.0588949, recall 0.828201
2017-12-10T04:28:54.596617: step 2740, loss 0.214931, acc 0.921875, prec 0.0589236, recall 0.828288
2017-12-10T04:28:54.862822: step 2741, loss 0.104213, acc 0.96875, prec 0.0589215, recall 0.828288
2017-12-10T04:28:55.126385: step 2742, loss 0.18624, acc 0.953125, prec 0.0589353, recall 0.828332
2017-12-10T04:28:55.394286: step 2743, loss 0.408336, acc 0.859375, prec 0.0589257, recall 0.828332
2017-12-10T04:28:55.659059: step 2744, loss 0.673518, acc 0.96875, prec 0.0589576, recall 0.828419
2017-12-10T04:28:55.925037: step 2745, loss 0.0829177, acc 0.96875, prec 0.0589555, recall 0.828419
2017-12-10T04:28:56.194696: step 2746, loss 0.259312, acc 0.890625, prec 0.058965, recall 0.828463
2017-12-10T04:28:56.457939: step 2747, loss 0.208817, acc 0.921875, prec 0.0589767, recall 0.828506
2017-12-10T04:28:56.722119: step 2748, loss 0.192757, acc 0.953125, prec 0.0589735, recall 0.828506
2017-12-10T04:28:57.005571: step 2749, loss 1.20604, acc 0.9375, prec 0.0590033, recall 0.828593
2017-12-10T04:28:57.271049: step 2750, loss 0.287632, acc 0.921875, prec 0.058998, recall 0.828593
2017-12-10T04:28:57.538739: step 2751, loss 0.458675, acc 0.90625, prec 0.0590086, recall 0.828637
2017-12-10T04:28:57.800840: step 2752, loss 0.132787, acc 0.9375, prec 0.0590043, recall 0.828637
2017-12-10T04:28:58.058781: step 2753, loss 0.323677, acc 0.921875, prec 0.059016, recall 0.82868
2017-12-10T04:28:58.322145: step 2754, loss 0.0778235, acc 0.96875, prec 0.0590819, recall 0.828854
2017-12-10T04:28:58.589700: step 2755, loss 0.158266, acc 0.953125, prec 0.0590787, recall 0.828854
2017-12-10T04:28:58.855544: step 2756, loss 0.109307, acc 0.953125, prec 0.0590925, recall 0.828897
2017-12-10T04:28:59.119316: step 2757, loss 0.227659, acc 0.9375, prec 0.0591222, recall 0.828984
2017-12-10T04:28:59.391236: step 2758, loss 0.452397, acc 0.875, prec 0.0591477, recall 0.829071
2017-12-10T04:28:59.655372: step 2759, loss 0.154993, acc 0.984375, prec 0.0591806, recall 0.829157
2017-12-10T04:28:59.924670: step 2760, loss 0.371151, acc 0.953125, prec 0.0591944, recall 0.8292
2017-12-10T04:29:00.189382: step 2761, loss 0.222122, acc 0.96875, prec 0.0592432, recall 0.82933
2017-12-10T04:29:00.459413: step 2762, loss 0.363925, acc 0.875, prec 0.0592346, recall 0.82933
2017-12-10T04:29:00.726664: step 2763, loss 0.138435, acc 0.96875, prec 0.0592325, recall 0.82933
2017-12-10T04:29:00.989647: step 2764, loss 0.278978, acc 0.9375, prec 0.0592452, recall 0.829373
2017-12-10T04:29:01.259218: step 2765, loss 0.364871, acc 0.921875, prec 0.0592399, recall 0.829373
2017-12-10T04:29:01.521403: step 2766, loss 0.279108, acc 1, prec 0.0592738, recall 0.829459
2017-12-10T04:29:01.791878: step 2767, loss 0.290423, acc 0.953125, prec 0.0593046, recall 0.829545
2017-12-10T04:29:02.053984: step 2768, loss 0.306271, acc 0.921875, prec 0.0593332, recall 0.829632
2017-12-10T04:29:02.312639: step 2769, loss 0.215052, acc 0.90625, prec 0.0593607, recall 0.829717
2017-12-10T04:29:02.578241: step 2770, loss 0.15743, acc 0.9375, prec 0.0593564, recall 0.829717
2017-12-10T04:29:02.841236: step 2771, loss 0.285908, acc 0.90625, prec 0.05935, recall 0.829717
2017-12-10T04:29:03.104502: step 2772, loss 0.258961, acc 0.921875, prec 0.0593956, recall 0.829846
2017-12-10T04:29:03.368003: step 2773, loss 0.23399, acc 0.953125, prec 0.0594093, recall 0.829889
2017-12-10T04:29:03.641938: step 2774, loss 0.361945, acc 0.921875, prec 0.0594209, recall 0.829932
2017-12-10T04:29:03.908369: step 2775, loss 3.40662, acc 0.96875, prec 0.0594538, recall 0.829809
2017-12-10T04:29:04.184238: step 2776, loss 0.764073, acc 0.9375, prec 0.0594834, recall 0.829894
2017-12-10T04:29:04.456693: step 2777, loss 0.192978, acc 0.953125, prec 0.0594972, recall 0.829937
2017-12-10T04:29:04.716295: step 2778, loss 0.284948, acc 0.90625, prec 0.0595416, recall 0.830065
2017-12-10T04:29:04.977640: step 2779, loss 0.355971, acc 0.9375, prec 0.0595373, recall 0.830065
2017-12-10T04:29:05.236222: step 2780, loss 0.861167, acc 0.859375, prec 0.0595616, recall 0.830151
2017-12-10T04:29:05.496209: step 2781, loss 0.693083, acc 0.859375, prec 0.0595689, recall 0.830193
2017-12-10T04:29:05.760775: step 2782, loss 0.457876, acc 0.875, prec 0.0596111, recall 0.830321
2017-12-10T04:29:06.023771: step 2783, loss 0.780609, acc 0.796875, prec 0.0595972, recall 0.830321
2017-12-10T04:29:06.286039: step 2784, loss 0.544887, acc 0.796875, prec 0.0596001, recall 0.830364
2017-12-10T04:29:06.549073: step 2785, loss 0.30606, acc 0.890625, prec 0.0596096, recall 0.830406
2017-12-10T04:29:06.807747: step 2786, loss 0.346584, acc 0.9375, prec 0.0596053, recall 0.830406
2017-12-10T04:29:07.071644: step 2787, loss 0.568548, acc 0.859375, prec 0.0595956, recall 0.830406
2017-12-10T04:29:07.335836: step 2788, loss 0.494324, acc 0.828125, prec 0.0596346, recall 0.830534
2017-12-10T04:29:07.597810: step 2789, loss 0.343854, acc 0.84375, prec 0.0596239, recall 0.830534
2017-12-10T04:29:07.863675: step 2790, loss 1.27693, acc 0.890625, prec 0.0596333, recall 0.830576
2017-12-10T04:29:08.125193: step 2791, loss 0.822788, acc 0.8125, prec 0.0596542, recall 0.830661
2017-12-10T04:29:08.385855: step 2792, loss 0.434616, acc 0.84375, prec 0.0596435, recall 0.830661
2017-12-10T04:29:08.649892: step 2793, loss 1.70783, acc 0.78125, prec 0.0596623, recall 0.830746
2017-12-10T04:29:08.914745: step 2794, loss 0.431991, acc 0.890625, prec 0.0596548, recall 0.830746
2017-12-10T04:29:09.182780: step 2795, loss 0.843314, acc 0.8125, prec 0.0596588, recall 0.830788
2017-12-10T04:29:09.451913: step 2796, loss 0.450989, acc 0.875, prec 0.0596503, recall 0.830788
2017-12-10T04:29:09.714356: step 2797, loss 0.444414, acc 0.84375, prec 0.0596564, recall 0.830831
2017-12-10T04:29:09.975023: step 2798, loss 0.420609, acc 0.90625, prec 0.0596669, recall 0.830873
2017-12-10T04:29:10.239170: step 2799, loss 0.531467, acc 0.796875, prec 0.0596699, recall 0.830915
2017-12-10T04:29:10.500951: step 2800, loss 0.401174, acc 0.875, prec 0.0596782, recall 0.830958
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-2800

2017-12-10T04:29:12.159402: step 2801, loss 0.605857, acc 0.84375, prec 0.0596675, recall 0.830958
2017-12-10T04:29:12.419600: step 2802, loss 0.856755, acc 0.90625, prec 0.0597117, recall 0.831084
2017-12-10T04:29:12.685148: step 2803, loss 0.359775, acc 0.921875, prec 0.0597063, recall 0.831084
2017-12-10T04:29:12.947854: step 2804, loss 0.83617, acc 0.953125, prec 0.0597369, recall 0.831169
2017-12-10T04:29:13.211169: step 2805, loss 0.247328, acc 0.953125, prec 0.0597336, recall 0.831169
2017-12-10T04:29:13.472819: step 2806, loss 0.415258, acc 0.9375, prec 0.0598137, recall 0.831379
2017-12-10T04:29:13.751210: step 2807, loss 0.218946, acc 0.9375, prec 0.0598432, recall 0.831463
2017-12-10T04:29:14.017369: step 2808, loss 0.282068, acc 0.9375, prec 0.0598895, recall 0.831589
2017-12-10T04:29:14.286246: step 2809, loss 0.0205604, acc 1, prec 0.0599063, recall 0.831631
2017-12-10T04:29:14.549988: step 2810, loss 0.222962, acc 0.890625, prec 0.0599494, recall 0.831757
2017-12-10T04:29:14.810827: step 2811, loss 2.13981, acc 0.921875, prec 0.0599451, recall 0.83155
2017-12-10T04:29:15.086442: step 2812, loss 0.400817, acc 0.890625, prec 0.0599376, recall 0.83155
2017-12-10T04:29:15.353142: step 2813, loss 0.300909, acc 0.890625, prec 0.0599301, recall 0.83155
2017-12-10T04:29:15.617120: step 2814, loss 0.403661, acc 0.875, prec 0.0599383, recall 0.831592
2017-12-10T04:29:15.885495: step 2815, loss 0.334629, acc 0.875, prec 0.0599466, recall 0.831634
2017-12-10T04:29:16.148752: step 2816, loss 0.167315, acc 0.921875, prec 0.0599581, recall 0.831676
2017-12-10T04:29:16.410085: step 2817, loss 0.301256, acc 0.90625, prec 0.0599685, recall 0.831718
2017-12-10T04:29:16.671568: step 2818, loss 0.293902, acc 0.921875, prec 0.0599799, recall 0.831759
2017-12-10T04:29:16.935773: step 2819, loss 0.704246, acc 0.8125, prec 0.059967, recall 0.831759
2017-12-10T04:29:17.202340: step 2820, loss 0.651783, acc 0.859375, prec 0.0599574, recall 0.831759
2017-12-10T04:29:17.465499: step 2821, loss 0.266231, acc 0.953125, prec 0.059971, recall 0.831801
2017-12-10T04:29:17.731153: step 2822, loss 0.347992, acc 0.890625, prec 0.0599803, recall 0.831843
2017-12-10T04:29:17.994627: step 2823, loss 0.154103, acc 0.9375, prec 0.059976, recall 0.831843
2017-12-10T04:29:18.251824: step 2824, loss 0.153412, acc 0.953125, prec 0.0599728, recall 0.831843
2017-12-10T04:29:18.516895: step 2825, loss 0.263329, acc 0.9375, prec 0.0599685, recall 0.831843
2017-12-10T04:29:18.779480: step 2826, loss 0.25261, acc 0.9375, prec 0.0599642, recall 0.831843
2017-12-10T04:29:19.045883: step 2827, loss 0.426701, acc 0.953125, prec 0.0599778, recall 0.831885
2017-12-10T04:29:19.322048: step 2828, loss 0.0893995, acc 0.953125, prec 0.0599746, recall 0.831885
2017-12-10T04:29:19.585794: step 2829, loss 0.212128, acc 0.96875, prec 0.0599724, recall 0.831885
2017-12-10T04:29:19.849344: step 2830, loss 0.0555447, acc 0.96875, prec 0.0599871, recall 0.831927
2017-12-10T04:29:20.114268: step 2831, loss 0.15207, acc 0.9375, prec 0.0599828, recall 0.831927
2017-12-10T04:29:20.380841: step 2832, loss 0.123239, acc 0.953125, prec 0.0599796, recall 0.831927
2017-12-10T04:29:20.645547: step 2833, loss 1.73095, acc 0.9375, prec 0.0599932, recall 0.831762
2017-12-10T04:29:20.913743: step 2834, loss 0.0824927, acc 0.953125, prec 0.05999, recall 0.831762
2017-12-10T04:29:21.176804: step 2835, loss 1.15355, acc 0.953125, prec 0.0600204, recall 0.831845
2017-12-10T04:29:21.445025: step 2836, loss 0.353769, acc 0.921875, prec 0.060015, recall 0.831845
2017-12-10T04:29:21.714698: step 2837, loss 0.193035, acc 0.953125, prec 0.0600286, recall 0.831887
2017-12-10T04:29:21.974736: step 2838, loss 0.326887, acc 0.890625, prec 0.0600211, recall 0.831887
2017-12-10T04:29:22.236328: step 2839, loss 0.607115, acc 0.84375, prec 0.0600104, recall 0.831887
2017-12-10T04:29:22.497128: step 2840, loss 0.505796, acc 0.90625, prec 0.0600207, recall 0.831929
2017-12-10T04:29:22.755736: step 2841, loss 0.123412, acc 0.96875, prec 0.0600354, recall 0.83197
2017-12-10T04:29:23.025628: step 2842, loss 0.404572, acc 0.890625, prec 0.0600447, recall 0.832012
2017-12-10T04:29:23.286850: step 2843, loss 0.422394, acc 0.90625, prec 0.0600551, recall 0.832053
2017-12-10T04:29:23.545714: step 2844, loss 0.392641, acc 0.875, prec 0.0600465, recall 0.832053
2017-12-10T04:29:23.807317: step 2845, loss 0.197472, acc 0.9375, prec 0.0600422, recall 0.832053
2017-12-10T04:29:24.071316: step 2846, loss 0.328398, acc 0.96875, prec 0.0600568, recall 0.832095
2017-12-10T04:29:24.338036: step 2847, loss 0.120037, acc 0.953125, prec 0.0600536, recall 0.832095
2017-12-10T04:29:24.595574: step 2848, loss 0.205346, acc 0.9375, prec 0.0600493, recall 0.832095
2017-12-10T04:29:24.859553: step 2849, loss 0.261739, acc 0.921875, prec 0.060044, recall 0.832095
2017-12-10T04:29:25.120050: step 2850, loss 0.251148, acc 0.921875, prec 0.0600554, recall 0.832137
2017-12-10T04:29:25.387273: step 2851, loss 0.30411, acc 0.890625, prec 0.0600479, recall 0.832137
2017-12-10T04:29:25.657950: step 2852, loss 0.464492, acc 0.875, prec 0.0600561, recall 0.832178
2017-12-10T04:29:25.923217: step 2853, loss 0.322389, acc 0.921875, prec 0.0600843, recall 0.832261
2017-12-10T04:29:26.185236: step 2854, loss 0.171649, acc 0.9375, prec 0.0601136, recall 0.832344
2017-12-10T04:29:26.453992: step 2855, loss 0.232604, acc 0.96875, prec 0.0601282, recall 0.832386
2017-12-10T04:29:26.723933: step 2856, loss 0.183761, acc 0.921875, prec 0.0601229, recall 0.832386
2017-12-10T04:29:26.999186: step 2857, loss 0.172998, acc 0.953125, prec 0.0601532, recall 0.832469
2017-12-10T04:29:27.260787: step 2858, loss 0.264289, acc 0.953125, prec 0.0601835, recall 0.832551
2017-12-10T04:29:27.523860: step 2859, loss 0.0500546, acc 0.984375, prec 0.0601825, recall 0.832551
2017-12-10T04:29:27.787613: step 2860, loss 0.501213, acc 0.96875, prec 0.0601971, recall 0.832593
2017-12-10T04:29:28.049710: step 2861, loss 0.648377, acc 0.9375, prec 0.0602096, recall 0.832634
2017-12-10T04:29:28.321040: step 2862, loss 0.127891, acc 0.921875, prec 0.0602042, recall 0.832634
2017-12-10T04:29:28.590908: step 2863, loss 0.132466, acc 0.953125, prec 0.060201, recall 0.832634
2017-12-10T04:29:28.851641: step 2864, loss 0.201312, acc 0.90625, prec 0.0601945, recall 0.832634
2017-12-10T04:29:29.119351: step 2865, loss 0.0665073, acc 0.984375, prec 0.0601934, recall 0.832634
2017-12-10T04:29:29.393603: step 2866, loss 1.38969, acc 0.90625, prec 0.0602048, recall 0.83247
2017-12-10T04:29:29.661860: step 2867, loss 0.0505692, acc 1, prec 0.0602048, recall 0.83247
2017-12-10T04:29:29.926550: step 2868, loss 3.95896, acc 0.953125, prec 0.0602195, recall 0.832306
2017-12-10T04:29:30.205972: step 2869, loss 0.273786, acc 0.90625, prec 0.0602298, recall 0.832347
2017-12-10T04:29:30.470300: step 2870, loss 0.755263, acc 0.921875, prec 0.0602747, recall 0.832471
2017-12-10T04:29:30.741954: step 2871, loss 0.674799, acc 0.8125, prec 0.0602618, recall 0.832471
2017-12-10T04:29:31.007074: step 2872, loss 0.495763, acc 0.875, prec 0.06027, recall 0.832512
2017-12-10T04:29:31.272926: step 2873, loss 0.68816, acc 0.78125, prec 0.0602549, recall 0.832512
2017-12-10T04:29:31.533537: step 2874, loss 0.858528, acc 0.71875, prec 0.0602356, recall 0.832512
2017-12-10T04:29:31.798001: step 2875, loss 1.3074, acc 0.765625, prec 0.0602195, recall 0.832512
2017-12-10T04:29:32.068618: step 2876, loss 0.791933, acc 0.796875, prec 0.0602725, recall 0.832677
2017-12-10T04:29:32.339977: step 2877, loss 0.625909, acc 0.796875, prec 0.0602586, recall 0.832677
2017-12-10T04:29:32.597044: step 2878, loss 0.523825, acc 0.8125, prec 0.0602457, recall 0.832677
2017-12-10T04:29:32.869428: step 2879, loss 0.67838, acc 0.796875, prec 0.0602317, recall 0.832677
2017-12-10T04:29:33.135371: step 2880, loss 0.917877, acc 0.75, prec 0.060248, recall 0.832759
2017-12-10T04:29:33.396186: step 2881, loss 0.587322, acc 0.8125, prec 0.0602352, recall 0.832759
2017-12-10T04:29:33.662136: step 2882, loss 1.24095, acc 0.75, prec 0.0602515, recall 0.832842
2017-12-10T04:29:33.924633: step 2883, loss 0.58752, acc 0.828125, prec 0.0602564, recall 0.832883
2017-12-10T04:29:34.195170: step 2884, loss 0.329064, acc 0.84375, prec 0.0602457, recall 0.832883
2017-12-10T04:29:34.459013: step 2885, loss 0.167188, acc 0.9375, prec 0.0602581, recall 0.832924
2017-12-10T04:29:34.722303: step 2886, loss 0.306085, acc 0.90625, prec 0.0602851, recall 0.833006
2017-12-10T04:29:34.982649: step 2887, loss 0.158226, acc 0.96875, prec 0.0602996, recall 0.833047
2017-12-10T04:29:35.248118: step 2888, loss 0.296587, acc 0.90625, prec 0.0602932, recall 0.833047
2017-12-10T04:29:35.513634: step 2889, loss 0.214093, acc 0.9375, prec 0.0602889, recall 0.833047
2017-12-10T04:29:35.776362: step 2890, loss 0.148696, acc 0.96875, prec 0.0602868, recall 0.833047
2017-12-10T04:29:36.039712: step 2891, loss 3.72292, acc 0.9375, prec 0.0602836, recall 0.832842
2017-12-10T04:29:36.309504: step 2892, loss 0.0904679, acc 0.96875, prec 0.0602814, recall 0.832842
2017-12-10T04:29:36.577670: step 2893, loss 0.101114, acc 0.96875, prec 0.060296, recall 0.832883
2017-12-10T04:29:36.840683: step 2894, loss 7.57686, acc 0.96875, prec 0.0603283, recall 0.832761
2017-12-10T04:29:37.110730: step 2895, loss 0.257069, acc 0.921875, prec 0.0603229, recall 0.832761
2017-12-10T04:29:37.373459: step 2896, loss 0.106803, acc 0.96875, prec 0.0603208, recall 0.832761
2017-12-10T04:29:37.643844: step 2897, loss 0.294355, acc 0.90625, prec 0.0603144, recall 0.832761
2017-12-10T04:29:37.903152: step 2898, loss 0.227751, acc 0.921875, prec 0.060309, recall 0.832761
2017-12-10T04:29:38.164701: step 2899, loss 0.514777, acc 0.828125, prec 0.0602972, recall 0.832761
2017-12-10T04:29:38.428760: step 2900, loss 0.310111, acc 0.875, prec 0.0602887, recall 0.832761
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-2900

2017-12-10T04:29:39.661007: step 2901, loss 0.605489, acc 0.828125, prec 0.0602936, recall 0.832802
2017-12-10T04:29:39.924173: step 2902, loss 0.506611, acc 0.84375, prec 0.0602829, recall 0.832802
2017-12-10T04:29:40.189679: step 2903, loss 0.288685, acc 0.953125, prec 0.0602797, recall 0.832802
2017-12-10T04:29:40.455561: step 2904, loss 0.654832, acc 0.796875, prec 0.0602991, recall 0.832884
2017-12-10T04:29:40.718536: step 2905, loss 0.659789, acc 0.796875, prec 0.0603019, recall 0.832925
2017-12-10T04:29:40.984390: step 2906, loss 0.351377, acc 0.890625, prec 0.060311, recall 0.832966
2017-12-10T04:29:41.245016: step 2907, loss 0.240864, acc 0.90625, prec 0.0603546, recall 0.833089
2017-12-10T04:29:41.517495: step 2908, loss 0.33971, acc 0.890625, prec 0.0603471, recall 0.833089
2017-12-10T04:29:41.783449: step 2909, loss 0.259033, acc 0.90625, prec 0.0603574, recall 0.833129
2017-12-10T04:29:42.057500: step 2910, loss 0.353927, acc 0.84375, prec 0.0603467, recall 0.833129
2017-12-10T04:29:42.323066: step 2911, loss 0.178625, acc 0.921875, prec 0.0603413, recall 0.833129
2017-12-10T04:29:42.585545: step 2912, loss 0.354333, acc 0.890625, prec 0.0603338, recall 0.833129
2017-12-10T04:29:42.852937: step 2913, loss 0.341399, acc 0.921875, prec 0.0603618, recall 0.833211
2017-12-10T04:29:43.121352: step 2914, loss 0.27739, acc 0.953125, prec 0.0603919, recall 0.833293
2017-12-10T04:29:43.383067: step 2915, loss 0.445934, acc 0.90625, prec 0.0604187, recall 0.833374
2017-12-10T04:29:43.656053: step 2916, loss 0.471247, acc 0.921875, prec 0.0604467, recall 0.833455
2017-12-10T04:29:43.926336: step 2917, loss 1.14064, acc 0.96875, prec 0.0604612, recall 0.833496
2017-12-10T04:29:44.194392: step 2918, loss 0.273644, acc 0.9375, prec 0.0604735, recall 0.833537
2017-12-10T04:29:44.469233: step 2919, loss 0.109072, acc 0.953125, prec 0.0604703, recall 0.833537
2017-12-10T04:29:44.733536: step 2920, loss 0.120136, acc 0.984375, prec 0.0604859, recall 0.833577
2017-12-10T04:29:44.997595: step 2921, loss 0.078857, acc 0.96875, prec 0.060517, recall 0.833659
2017-12-10T04:29:45.268590: step 2922, loss 0.072736, acc 1, prec 0.0605336, recall 0.833699
2017-12-10T04:29:45.539892: step 2923, loss 0.0518053, acc 0.96875, prec 0.0605315, recall 0.833699
2017-12-10T04:29:45.803933: step 2924, loss 2.54185, acc 0.9375, prec 0.0605449, recall 0.833536
2017-12-10T04:29:46.071079: step 2925, loss 0.267783, acc 0.953125, prec 0.0605583, recall 0.833577
2017-12-10T04:29:46.338530: step 2926, loss 0.0522888, acc 0.984375, prec 0.0605739, recall 0.833618
2017-12-10T04:29:46.601046: step 2927, loss 0.15319, acc 0.90625, prec 0.0605674, recall 0.833618
2017-12-10T04:29:46.873311: step 2928, loss 0.483177, acc 0.890625, prec 0.0605599, recall 0.833618
2017-12-10T04:29:47.139359: step 2929, loss 0.151642, acc 0.96875, prec 0.0605744, recall 0.833658
2017-12-10T04:29:47.398861: step 2930, loss 0.293856, acc 0.921875, prec 0.0605691, recall 0.833658
2017-12-10T04:29:47.661065: step 2931, loss 0.372321, acc 0.90625, prec 0.0605793, recall 0.833699
2017-12-10T04:29:47.927430: step 2932, loss 0.121643, acc 0.953125, prec 0.0605927, recall 0.833739
2017-12-10T04:29:48.193111: step 2933, loss 1.80909, acc 0.921875, prec 0.060605, recall 0.833577
2017-12-10T04:29:48.459945: step 2934, loss 0.290633, acc 0.90625, prec 0.0606318, recall 0.833658
2017-12-10T04:29:48.720853: step 2935, loss 0.343031, acc 0.875, prec 0.0606232, recall 0.833658
2017-12-10T04:29:48.986227: step 2936, loss 0.17094, acc 0.921875, prec 0.0606178, recall 0.833658
2017-12-10T04:29:49.247885: step 2937, loss 0.576274, acc 0.921875, prec 0.0606457, recall 0.833738
2017-12-10T04:29:49.509213: step 2938, loss 0.437522, acc 0.859375, prec 0.0606693, recall 0.833819
2017-12-10T04:29:49.769224: step 2939, loss 0.793252, acc 0.84375, prec 0.0606585, recall 0.833819
2017-12-10T04:29:50.029950: step 2940, loss 0.319709, acc 0.890625, prec 0.060651, recall 0.833819
2017-12-10T04:29:50.292949: step 2941, loss 0.362232, acc 0.921875, prec 0.0606457, recall 0.833819
2017-12-10T04:29:50.557852: step 2942, loss 0.257885, acc 0.9375, prec 0.0606414, recall 0.833819
2017-12-10T04:29:50.817426: step 2943, loss 0.701988, acc 0.828125, prec 0.0606462, recall 0.83386
2017-12-10T04:29:51.077966: step 2944, loss 0.79636, acc 0.828125, prec 0.0606842, recall 0.833981
2017-12-10T04:29:51.338750: step 2945, loss 0.368236, acc 0.859375, prec 0.0606746, recall 0.833981
2017-12-10T04:29:51.600722: step 2946, loss 0.279651, acc 0.875, prec 0.0606992, recall 0.834061
2017-12-10T04:29:51.865733: step 2947, loss 0.532895, acc 0.84375, prec 0.0606884, recall 0.834061
2017-12-10T04:29:52.127080: step 2948, loss 0.501174, acc 0.859375, prec 0.0606954, recall 0.834101
2017-12-10T04:29:52.390391: step 2949, loss 0.163243, acc 0.953125, prec 0.0607087, recall 0.834142
2017-12-10T04:29:52.651563: step 2950, loss 0.535299, acc 0.921875, prec 0.06072, recall 0.834182
2017-12-10T04:29:52.916421: step 2951, loss 0.294003, acc 0.96875, prec 0.060751, recall 0.834262
2017-12-10T04:29:53.180016: step 2952, loss 0.358174, acc 0.90625, prec 0.0608108, recall 0.834423
2017-12-10T04:29:53.443964: step 2953, loss 0.852273, acc 0.921875, prec 0.0608386, recall 0.834503
2017-12-10T04:29:53.714534: step 2954, loss 0.138873, acc 0.9375, prec 0.0608343, recall 0.834503
2017-12-10T04:29:53.991210: step 2955, loss 0.153699, acc 0.9375, prec 0.06083, recall 0.834503
2017-12-10T04:29:54.259765: step 2956, loss 3.08321, acc 0.9375, prec 0.0608433, recall 0.834341
2017-12-10T04:29:54.528781: step 2957, loss 0.364591, acc 0.890625, prec 0.060869, recall 0.834421
2017-12-10T04:29:54.796120: step 2958, loss 0.203356, acc 0.9375, prec 0.0608647, recall 0.834421
2017-12-10T04:29:55.057253: step 2959, loss 0.496066, acc 0.875, prec 0.0608726, recall 0.834461
2017-12-10T04:29:55.323745: step 2960, loss 0.322471, acc 0.859375, prec 0.0608795, recall 0.834501
2017-12-10T04:29:55.594182: step 2961, loss 0.272997, acc 0.890625, prec 0.060872, recall 0.834501
2017-12-10T04:29:55.857132: step 2962, loss 0.711675, acc 0.84375, prec 0.0608778, recall 0.834541
2017-12-10T04:29:56.121803: step 2963, loss 0.230661, acc 0.90625, prec 0.0609045, recall 0.834621
2017-12-10T04:29:56.381386: step 2964, loss 0.294501, acc 0.859375, prec 0.0608948, recall 0.834621
2017-12-10T04:29:56.642792: step 2965, loss 0.181896, acc 0.953125, prec 0.0608916, recall 0.834621
2017-12-10T04:29:56.913086: step 2966, loss 0.407672, acc 0.859375, prec 0.0608985, recall 0.834661
2017-12-10T04:29:57.188913: step 2967, loss 0.281918, acc 0.9375, prec 0.0608942, recall 0.834661
2017-12-10T04:29:57.446860: step 2968, loss 0.760844, acc 0.84375, prec 0.0608835, recall 0.834661
2017-12-10T04:29:57.709025: step 2969, loss 0.450561, acc 0.90625, prec 0.0608771, recall 0.834661
2017-12-10T04:29:57.975699: step 2970, loss 0.382706, acc 0.96875, prec 0.060908, recall 0.834741
2017-12-10T04:29:58.249954: step 2971, loss 0.282815, acc 0.90625, prec 0.0609016, recall 0.834741
2017-12-10T04:29:58.511251: step 2972, loss 0.4183, acc 0.84375, prec 0.0608908, recall 0.834741
2017-12-10T04:29:58.773443: step 2973, loss 0.280778, acc 0.921875, prec 0.0608855, recall 0.834741
2017-12-10T04:29:59.037979: step 2974, loss 0.481699, acc 0.875, prec 0.06091, recall 0.83482
2017-12-10T04:29:59.297427: step 2975, loss 0.327502, acc 0.921875, prec 0.0609046, recall 0.83482
2017-12-10T04:29:59.560928: step 2976, loss 0.278464, acc 0.90625, prec 0.0608982, recall 0.83482
2017-12-10T04:29:59.825897: step 2977, loss 0.144335, acc 0.9375, prec 0.0608939, recall 0.83482
2017-12-10T04:30:00.084935: step 2978, loss 0.0655787, acc 1, prec 0.0609269, recall 0.8349
2017-12-10T04:30:00.358607: step 2979, loss 0.0585779, acc 0.953125, prec 0.0609237, recall 0.8349
2017-12-10T04:30:00.628023: step 2980, loss 0.191929, acc 0.9375, prec 0.0609194, recall 0.8349
2017-12-10T04:30:00.888905: step 2981, loss 0.0242387, acc 1, prec 0.0609359, recall 0.83494
2017-12-10T04:30:01.122469: step 2982, loss 1.82689, acc 0.980769, prec 0.0609524, recall 0.834778
2017-12-10T04:30:01.394643: step 2983, loss 0.11938, acc 0.96875, prec 0.0609668, recall 0.834818
2017-12-10T04:30:01.658877: step 2984, loss 0.201245, acc 0.953125, prec 0.0609801, recall 0.834858
2017-12-10T04:30:01.926747: step 2985, loss 0.140917, acc 0.96875, prec 0.0609945, recall 0.834898
2017-12-10T04:30:02.200181: step 2986, loss 0.10287, acc 0.984375, prec 0.0610264, recall 0.834977
2017-12-10T04:30:02.464000: step 2987, loss 0.269175, acc 0.9375, prec 0.0610386, recall 0.835017
2017-12-10T04:30:02.727174: step 2988, loss 0.613709, acc 0.96875, prec 0.0610695, recall 0.835096
2017-12-10T04:30:03.000623: step 2989, loss 0.440556, acc 0.875, prec 0.0610774, recall 0.835136
2017-12-10T04:30:03.267627: step 2990, loss 0.100534, acc 0.984375, prec 0.0611094, recall 0.835215
2017-12-10T04:30:03.529592: step 2991, loss 0.123427, acc 0.953125, prec 0.0611226, recall 0.835255
2017-12-10T04:30:03.791637: step 2992, loss 0.159161, acc 0.9375, prec 0.0611183, recall 0.835255
2017-12-10T04:30:04.058156: step 2993, loss 0.325203, acc 0.890625, prec 0.0611108, recall 0.835255
2017-12-10T04:30:04.321431: step 2994, loss 0.222192, acc 0.921875, prec 0.0611219, recall 0.835294
2017-12-10T04:30:04.590575: step 2995, loss 0.366768, acc 0.921875, prec 0.0611331, recall 0.835334
2017-12-10T04:30:04.863940: step 2996, loss 0.158918, acc 0.953125, prec 0.0611463, recall 0.835373
2017-12-10T04:30:05.131722: step 2997, loss 0.165834, acc 0.9375, prec 0.061142, recall 0.835373
2017-12-10T04:30:05.398134: step 2998, loss 0.865468, acc 0.921875, prec 0.0611861, recall 0.835492
2017-12-10T04:30:05.663512: step 2999, loss 0.357481, acc 0.859375, prec 0.0612094, recall 0.83557
2017-12-10T04:30:05.931115: step 3000, loss 0.33123, acc 0.90625, prec 0.061236, recall 0.835649

Evaluation:
2017-12-10T04:30:13.626881: step 3000, loss 2.00933, acc 0.926307, prec 0.0619169, recall 0.827626

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-3000

2017-12-10T04:30:14.884376: step 3001, loss 0.213117, acc 0.9375, prec 0.0619126, recall 0.827626
2017-12-10T04:30:15.149975: step 3002, loss 0.678208, acc 0.875, prec 0.0619365, recall 0.827706
2017-12-10T04:30:15.411425: step 3003, loss 0.112119, acc 0.9375, prec 0.0619322, recall 0.827706
2017-12-10T04:30:15.677867: step 3004, loss 0.458441, acc 0.90625, prec 0.0619745, recall 0.827825
2017-12-10T04:30:15.939841: step 3005, loss 0.369023, acc 0.90625, prec 0.0620005, recall 0.827905
2017-12-10T04:30:16.205062: step 3006, loss 0.257681, acc 0.90625, prec 0.061994, recall 0.827905
2017-12-10T04:30:16.473153: step 3007, loss 0.211081, acc 0.90625, prec 0.0620038, recall 0.827945
2017-12-10T04:30:16.742172: step 3008, loss 0.457283, acc 0.921875, prec 0.0620147, recall 0.827984
2017-12-10T04:30:17.008237: step 3009, loss 0.168674, acc 0.953125, prec 0.0620439, recall 0.828064
2017-12-10T04:30:17.281816: step 3010, loss 0.177866, acc 0.953125, prec 0.0620407, recall 0.828064
2017-12-10T04:30:17.552842: step 3011, loss 0.159346, acc 0.921875, prec 0.0620353, recall 0.828064
2017-12-10T04:30:17.819451: step 3012, loss 0.167788, acc 0.953125, prec 0.0620321, recall 0.828064
2017-12-10T04:30:18.088727: step 3013, loss 0.181432, acc 0.9375, prec 0.062044, recall 0.828103
2017-12-10T04:30:18.363303: step 3014, loss 0.0843942, acc 0.96875, prec 0.0620419, recall 0.828103
2017-12-10T04:30:18.624889: step 3015, loss 0.0882919, acc 0.953125, prec 0.0620549, recall 0.828143
2017-12-10T04:30:18.898347: step 3016, loss 0.0499183, acc 0.96875, prec 0.062069, recall 0.828183
2017-12-10T04:30:19.169787: step 3017, loss 0.220353, acc 0.875, prec 0.0620604, recall 0.828183
2017-12-10T04:30:19.443751: step 3018, loss 0.0606639, acc 0.96875, prec 0.0620582, recall 0.828183
2017-12-10T04:30:19.711784: step 3019, loss 0.0616254, acc 1, prec 0.0620907, recall 0.828262
2017-12-10T04:30:19.974301: step 3020, loss 0.0800516, acc 0.984375, prec 0.062122, recall 0.828341
2017-12-10T04:30:20.238288: step 3021, loss 0.00495755, acc 1, prec 0.062122, recall 0.828341
2017-12-10T04:30:20.506669: step 3022, loss 0.195606, acc 0.953125, prec 0.0621188, recall 0.828341
2017-12-10T04:30:20.773134: step 3023, loss 0.175, acc 0.96875, prec 0.0621166, recall 0.828341
2017-12-10T04:30:21.036242: step 3024, loss 0.028071, acc 0.984375, prec 0.0621318, recall 0.828381
2017-12-10T04:30:21.305149: step 3025, loss 0.732509, acc 0.96875, prec 0.062162, recall 0.82846
2017-12-10T04:30:21.571130: step 3026, loss 5.36959, acc 0.96875, prec 0.0621934, recall 0.828348
2017-12-10T04:30:21.840244: step 3027, loss 0.0675263, acc 0.96875, prec 0.0621912, recall 0.828348
2017-12-10T04:30:22.108616: step 3028, loss 0.0194788, acc 1, prec 0.0621912, recall 0.828348
2017-12-10T04:30:22.371979: step 3029, loss 0.193311, acc 0.96875, prec 0.0622377, recall 0.828466
2017-12-10T04:30:22.637559: step 3030, loss 0.0985671, acc 0.953125, prec 0.0622506, recall 0.828506
2017-12-10T04:30:22.907524: step 3031, loss 0.127, acc 0.953125, prec 0.0622798, recall 0.828585
2017-12-10T04:30:23.172718: step 3032, loss 0.438505, acc 0.859375, prec 0.0622701, recall 0.828585
2017-12-10T04:30:23.439822: step 3033, loss 0.493404, acc 0.9375, prec 0.062282, recall 0.828624
2017-12-10T04:30:23.708723: step 3034, loss 0.436279, acc 0.890625, prec 0.0623069, recall 0.828703
2017-12-10T04:30:23.982419: step 3035, loss 0.286858, acc 0.875, prec 0.0622983, recall 0.828703
2017-12-10T04:30:24.247659: step 3036, loss 0.168793, acc 0.96875, prec 0.0623123, recall 0.828742
2017-12-10T04:30:24.510804: step 3037, loss 0.225243, acc 0.953125, prec 0.0623091, recall 0.828742
2017-12-10T04:30:24.774893: step 3038, loss 0.299068, acc 0.890625, prec 0.0623339, recall 0.828821
2017-12-10T04:30:25.039303: step 3039, loss 0.387672, acc 0.90625, prec 0.0623274, recall 0.828821
2017-12-10T04:30:25.309583: step 3040, loss 0.195076, acc 0.921875, prec 0.0623544, recall 0.828899
2017-12-10T04:30:25.582392: step 3041, loss 0.385078, acc 0.875, prec 0.0623458, recall 0.828899
2017-12-10T04:30:25.850148: step 3042, loss 0.273218, acc 0.875, prec 0.0623696, recall 0.828978
2017-12-10T04:30:26.116317: step 3043, loss 0.161748, acc 0.9375, prec 0.0623976, recall 0.829056
2017-12-10T04:30:26.385295: step 3044, loss 0.172765, acc 0.9375, prec 0.0624256, recall 0.829134
2017-12-10T04:30:26.657511: step 3045, loss 0.16836, acc 0.9375, prec 0.0624537, recall 0.829212
2017-12-10T04:30:26.928600: step 3046, loss 0.131737, acc 0.9375, prec 0.0624494, recall 0.829212
2017-12-10T04:30:27.204171: step 3047, loss 0.242642, acc 0.9375, prec 0.0624612, recall 0.829252
2017-12-10T04:30:27.467294: step 3048, loss 0.253419, acc 0.9375, prec 0.0624892, recall 0.82933
2017-12-10T04:30:27.733761: step 3049, loss 0.0854098, acc 0.953125, prec 0.0625183, recall 0.829408
2017-12-10T04:30:27.995754: step 3050, loss 0.149702, acc 0.953125, prec 0.0625151, recall 0.829408
2017-12-10T04:30:28.262032: step 3051, loss 0.25593, acc 0.9375, prec 0.0625108, recall 0.829408
2017-12-10T04:30:28.530620: step 3052, loss 0.136326, acc 0.953125, prec 0.0625399, recall 0.829486
2017-12-10T04:30:28.791896: step 3053, loss 0.682317, acc 0.953125, prec 0.0625689, recall 0.829564
2017-12-10T04:30:29.057817: step 3054, loss 0.169694, acc 0.953125, prec 0.0625818, recall 0.829603
2017-12-10T04:30:29.323314: step 3055, loss 0.0998138, acc 0.96875, prec 0.0625958, recall 0.829641
2017-12-10T04:30:29.592926: step 3056, loss 0.101746, acc 0.96875, prec 0.0625937, recall 0.829641
2017-12-10T04:30:29.861317: step 3057, loss 0.455062, acc 0.96875, prec 0.0626238, recall 0.829719
2017-12-10T04:30:30.134102: step 3058, loss 0.368088, acc 0.984375, prec 0.062655, recall 0.829797
2017-12-10T04:30:30.411817: step 3059, loss 0.127373, acc 0.953125, prec 0.062668, recall 0.829836
2017-12-10T04:30:30.685031: step 3060, loss 0.149013, acc 0.953125, prec 0.0626809, recall 0.829875
2017-12-10T04:30:30.952792: step 3061, loss 0.124396, acc 0.96875, prec 0.062711, recall 0.829952
2017-12-10T04:30:31.221607: step 3062, loss 0.175334, acc 0.953125, prec 0.0627239, recall 0.829991
2017-12-10T04:30:31.494571: step 3063, loss 0.121, acc 0.96875, prec 0.0627217, recall 0.829991
2017-12-10T04:30:31.756834: step 3064, loss 0.143294, acc 0.953125, prec 0.0627346, recall 0.83003
2017-12-10T04:30:32.018459: step 3065, loss 0.418431, acc 0.953125, prec 0.0627637, recall 0.830107
2017-12-10T04:30:32.288579: step 3066, loss 0.139899, acc 0.96875, prec 0.0627615, recall 0.830107
2017-12-10T04:30:32.554792: step 3067, loss 0.21531, acc 0.9375, prec 0.0627895, recall 0.830184
2017-12-10T04:30:32.820511: step 3068, loss 0.171599, acc 0.953125, prec 0.0627862, recall 0.830184
2017-12-10T04:30:33.083556: step 3069, loss 0.190492, acc 0.96875, prec 0.0627841, recall 0.830184
2017-12-10T04:30:33.346253: step 3070, loss 0.180838, acc 0.9375, prec 0.0627797, recall 0.830184
2017-12-10T04:30:33.608483: step 3071, loss 0.0520259, acc 0.984375, prec 0.0628109, recall 0.830262
2017-12-10T04:30:33.874478: step 3072, loss 0.0823722, acc 0.984375, prec 0.062826, recall 0.8303
2017-12-10T04:30:34.141304: step 3073, loss 0.0403029, acc 1, prec 0.062826, recall 0.8303
2017-12-10T04:30:34.402403: step 3074, loss 0.152034, acc 0.953125, prec 0.0628227, recall 0.8303
2017-12-10T04:30:34.675685: step 3075, loss 0.122122, acc 0.953125, prec 0.0628195, recall 0.8303
2017-12-10T04:30:34.946493: step 3076, loss 0.0837884, acc 0.96875, prec 0.0628173, recall 0.8303
2017-12-10T04:30:35.221994: step 3077, loss 0.107549, acc 0.9375, prec 0.0628291, recall 0.830339
2017-12-10T04:30:35.487158: step 3078, loss 0.161964, acc 0.96875, prec 0.0628431, recall 0.830377
2017-12-10T04:30:35.749471: step 3079, loss 0.0168101, acc 1, prec 0.0628592, recall 0.830416
2017-12-10T04:30:36.014705: step 3080, loss 0.130811, acc 0.953125, prec 0.0628721, recall 0.830455
2017-12-10T04:30:36.280601: step 3081, loss 0.0108582, acc 1, prec 0.0628721, recall 0.830455
2017-12-10T04:30:36.556568: step 3082, loss 0.151795, acc 0.953125, prec 0.062885, recall 0.830493
2017-12-10T04:30:36.820676: step 3083, loss 0.0426422, acc 0.984375, prec 0.0628839, recall 0.830493
2017-12-10T04:30:37.080941: step 3084, loss 0.00627437, acc 1, prec 0.0628839, recall 0.830493
2017-12-10T04:30:37.342982: step 3085, loss 0.0374903, acc 0.984375, prec 0.0628828, recall 0.830493
2017-12-10T04:30:37.603500: step 3086, loss 0.587521, acc 0.984375, prec 0.0629301, recall 0.830609
2017-12-10T04:30:37.870952: step 3087, loss 0.464588, acc 0.953125, prec 0.062943, recall 0.830647
2017-12-10T04:30:38.134379: step 3088, loss 0.0136385, acc 1, prec 0.0629591, recall 0.830685
2017-12-10T04:30:38.398640: step 3089, loss 0.0422003, acc 0.984375, prec 0.062958, recall 0.830685
2017-12-10T04:30:38.658312: step 3090, loss 0.178583, acc 0.921875, prec 0.0629687, recall 0.830724
2017-12-10T04:30:38.922307: step 3091, loss 0.231496, acc 0.96875, prec 0.0629826, recall 0.830762
2017-12-10T04:30:39.191346: step 3092, loss 0.0290044, acc 0.984375, prec 0.0629977, recall 0.830801
2017-12-10T04:30:39.454714: step 3093, loss 0.222409, acc 0.9375, prec 0.0630095, recall 0.830839
2017-12-10T04:30:39.720999: step 3094, loss 0.0210013, acc 1, prec 0.0630256, recall 0.830877
2017-12-10T04:30:39.992472: step 3095, loss 0.0645758, acc 0.96875, prec 0.0630395, recall 0.830916
2017-12-10T04:30:40.253188: step 3096, loss 0.113305, acc 0.921875, prec 0.0630502, recall 0.830954
2017-12-10T04:30:40.513890: step 3097, loss 0.286257, acc 0.9375, prec 0.063062, recall 0.830992
2017-12-10T04:30:40.778701: step 3098, loss 0.135635, acc 0.96875, prec 0.0630759, recall 0.831031
2017-12-10T04:30:41.043623: step 3099, loss 0.075574, acc 0.96875, prec 0.0630899, recall 0.831069
2017-12-10T04:30:41.311142: step 3100, loss 0.0887138, acc 0.96875, prec 0.063136, recall 0.831184
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-3100

2017-12-10T04:30:42.600811: step 3101, loss 0.0807019, acc 0.984375, prec 0.063151, recall 0.831222
2017-12-10T04:30:42.869887: step 3102, loss 0.0516252, acc 0.984375, prec 0.063166, recall 0.83126
2017-12-10T04:30:43.138742: step 3103, loss 0.253396, acc 0.90625, prec 0.0631756, recall 0.831298
2017-12-10T04:30:43.407784: step 3104, loss 0.159396, acc 0.96875, prec 0.0631735, recall 0.831298
2017-12-10T04:30:43.681881: step 3105, loss 0.152059, acc 0.9375, prec 0.0631852, recall 0.831336
2017-12-10T04:30:43.947556: step 3106, loss 0.313183, acc 0.953125, prec 0.063198, recall 0.831374
2017-12-10T04:30:44.212858: step 3107, loss 0.0321381, acc 0.984375, prec 0.0632131, recall 0.831412
2017-12-10T04:30:44.479552: step 3108, loss 0.141692, acc 0.984375, prec 0.0632281, recall 0.831451
2017-12-10T04:30:44.755625: step 3109, loss 0.694796, acc 0.984375, prec 0.0632431, recall 0.831489
2017-12-10T04:30:45.027018: step 3110, loss 0.103613, acc 0.984375, prec 0.0632581, recall 0.831527
2017-12-10T04:30:45.296513: step 3111, loss 0.0933293, acc 0.953125, prec 0.0632709, recall 0.831565
2017-12-10T04:30:45.561285: step 3112, loss 0.0977233, acc 0.984375, prec 0.0632859, recall 0.831603
2017-12-10T04:30:45.834407: step 3113, loss 0.179026, acc 0.96875, prec 0.0633159, recall 0.831679
2017-12-10T04:30:46.107703: step 3114, loss 0.0192931, acc 0.984375, prec 0.0633309, recall 0.831717
2017-12-10T04:30:46.379622: step 3115, loss 0.0246294, acc 0.984375, prec 0.0633298, recall 0.831717
2017-12-10T04:30:46.644535: step 3116, loss 0.306372, acc 0.9375, prec 0.0633416, recall 0.831755
2017-12-10T04:30:46.922144: step 3117, loss 0.0747137, acc 0.984375, prec 0.0633405, recall 0.831755
2017-12-10T04:30:47.188608: step 3118, loss 0.13911, acc 0.984375, prec 0.0633394, recall 0.831755
2017-12-10T04:30:47.454661: step 3119, loss 0.0820193, acc 0.96875, prec 0.0633533, recall 0.831793
2017-12-10T04:30:47.721324: step 3120, loss 0.223514, acc 0.9375, prec 0.063349, recall 0.831793
2017-12-10T04:30:47.994378: step 3121, loss 0.22842, acc 0.9375, prec 0.0633768, recall 0.831868
2017-12-10T04:30:48.255284: step 3122, loss 0.0843253, acc 0.96875, prec 0.0633907, recall 0.831906
2017-12-10T04:30:48.519981: step 3123, loss 0.159128, acc 0.984375, prec 0.0634218, recall 0.831982
2017-12-10T04:30:48.786697: step 3124, loss 0.164372, acc 0.96875, prec 0.0634357, recall 0.83202
2017-12-10T04:30:49.050658: step 3125, loss 0.0452558, acc 0.984375, prec 0.0634346, recall 0.83202
2017-12-10T04:30:49.314005: step 3126, loss 0.293726, acc 0.921875, prec 0.0634452, recall 0.832058
2017-12-10T04:30:49.579489: step 3127, loss 0.0982507, acc 0.96875, prec 0.0634591, recall 0.832095
2017-12-10T04:30:49.843633: step 3128, loss 0.0349145, acc 0.984375, prec 0.0634741, recall 0.832133
2017-12-10T04:30:50.110514: step 3129, loss 0.0419383, acc 0.984375, prec 0.0634891, recall 0.832171
2017-12-10T04:30:50.373849: step 3130, loss 0.0577659, acc 0.96875, prec 0.0634869, recall 0.832171
2017-12-10T04:30:50.648924: step 3131, loss 0.09516, acc 0.96875, prec 0.0635008, recall 0.832209
2017-12-10T04:30:50.912851: step 3132, loss 0.105952, acc 0.984375, prec 0.0634997, recall 0.832209
2017-12-10T04:30:51.174334: step 3133, loss 0.0278149, acc 0.984375, prec 0.0634986, recall 0.832209
2017-12-10T04:30:51.438030: step 3134, loss 1.26816, acc 0.953125, prec 0.0635114, recall 0.832246
2017-12-10T04:30:51.705666: step 3135, loss 0.0958023, acc 0.984375, prec 0.0635264, recall 0.832284
2017-12-10T04:30:51.984718: step 3136, loss 0.23934, acc 0.96875, prec 0.0635563, recall 0.83236
2017-12-10T04:30:52.254318: step 3137, loss 0.159868, acc 0.953125, prec 0.0635531, recall 0.83236
2017-12-10T04:30:52.522991: step 3138, loss 0.231749, acc 0.96875, prec 0.063567, recall 0.832397
2017-12-10T04:30:52.785879: step 3139, loss 0.362412, acc 0.90625, prec 0.0635765, recall 0.832435
2017-12-10T04:30:53.050996: step 3140, loss 0.306056, acc 0.90625, prec 0.063586, recall 0.832473
2017-12-10T04:30:53.311764: step 3141, loss 0.0905723, acc 0.96875, prec 0.0635999, recall 0.83251
2017-12-10T04:30:53.572882: step 3142, loss 0.0766108, acc 0.984375, prec 0.0635988, recall 0.83251
2017-12-10T04:30:53.842770: step 3143, loss 0.310747, acc 0.890625, prec 0.0636393, recall 0.832623
2017-12-10T04:30:54.106709: step 3144, loss 0.136888, acc 0.9375, prec 0.063635, recall 0.832623
2017-12-10T04:30:54.366976: step 3145, loss 0.221046, acc 0.953125, prec 0.0636477, recall 0.83266
2017-12-10T04:30:54.633896: step 3146, loss 0.393713, acc 0.921875, prec 0.0636423, recall 0.83266
2017-12-10T04:30:54.905895: step 3147, loss 0.16206, acc 0.921875, prec 0.0636689, recall 0.832735
2017-12-10T04:30:55.167435: step 3148, loss 0.157248, acc 0.96875, prec 0.0636668, recall 0.832735
2017-12-10T04:30:55.438918: step 3149, loss 2.15738, acc 0.890625, prec 0.0636763, recall 0.832586
2017-12-10T04:30:55.704820: step 3150, loss 0.0459053, acc 0.984375, prec 0.0636912, recall 0.832624
2017-12-10T04:30:55.972626: step 3151, loss 0.37097, acc 0.90625, prec 0.0636847, recall 0.832624
2017-12-10T04:30:56.238534: step 3152, loss 0.321715, acc 0.890625, prec 0.0637091, recall 0.832699
2017-12-10T04:30:56.500963: step 3153, loss 0.350291, acc 0.90625, prec 0.0637346, recall 0.832774
2017-12-10T04:30:56.760248: step 3154, loss 0.396257, acc 0.859375, prec 0.0637248, recall 0.832774
2017-12-10T04:30:57.031555: step 3155, loss 0.286715, acc 0.90625, prec 0.0637183, recall 0.832774
2017-12-10T04:30:57.289845: step 3156, loss 0.60831, acc 0.8125, prec 0.0637212, recall 0.832811
2017-12-10T04:30:57.549278: step 3157, loss 0.284254, acc 0.90625, prec 0.0637147, recall 0.832811
2017-12-10T04:30:57.812578: step 3158, loss 0.181948, acc 0.9375, prec 0.0637263, recall 0.832848
2017-12-10T04:30:58.076915: step 3159, loss 0.271199, acc 0.953125, prec 0.0637231, recall 0.832848
2017-12-10T04:30:58.338605: step 3160, loss 0.168159, acc 0.953125, prec 0.0637358, recall 0.832886
2017-12-10T04:30:58.600923: step 3161, loss 0.184813, acc 0.953125, prec 0.0637325, recall 0.832886
2017-12-10T04:30:58.862227: step 3162, loss 0.122609, acc 0.96875, prec 0.0637304, recall 0.832886
2017-12-10T04:30:59.123200: step 3163, loss 0.247525, acc 0.90625, prec 0.0637398, recall 0.832923
2017-12-10T04:30:59.392081: step 3164, loss 0.511206, acc 0.921875, prec 0.0637344, recall 0.832923
2017-12-10T04:30:59.653896: step 3165, loss 0.0829602, acc 0.984375, prec 0.0637493, recall 0.832961
2017-12-10T04:30:59.924124: step 3166, loss 0.0602869, acc 0.96875, prec 0.0637471, recall 0.832961
2017-12-10T04:31:00.196258: step 3167, loss 0.231702, acc 0.96875, prec 0.063761, recall 0.832998
2017-12-10T04:31:00.462675: step 3168, loss 0.0956253, acc 0.96875, prec 0.0637588, recall 0.832998
2017-12-10T04:31:00.729844: step 3169, loss 0.828066, acc 0.984375, prec 0.0637897, recall 0.833073
2017-12-10T04:31:00.993287: step 3170, loss 0.0137309, acc 1, prec 0.0637897, recall 0.833073
2017-12-10T04:31:01.252803: step 3171, loss 0.464576, acc 0.890625, prec 0.0637981, recall 0.83311
2017-12-10T04:31:01.514919: step 3172, loss 0.0212403, acc 1, prec 0.0638302, recall 0.833184
2017-12-10T04:31:01.782999: step 3173, loss 1.48245, acc 0.96875, prec 0.0639091, recall 0.833185
2017-12-10T04:31:02.050918: step 3174, loss 0.0869705, acc 0.953125, prec 0.0639059, recall 0.833185
2017-12-10T04:31:02.316238: step 3175, loss 0.249755, acc 0.96875, prec 0.0639037, recall 0.833185
2017-12-10T04:31:02.583462: step 3176, loss 0.35962, acc 0.9375, prec 0.0639153, recall 0.833222
2017-12-10T04:31:02.846698: step 3177, loss 0.628997, acc 0.890625, prec 0.0639237, recall 0.833259
2017-12-10T04:31:03.106433: step 3178, loss 0.413236, acc 0.921875, prec 0.0639502, recall 0.833333
2017-12-10T04:31:03.372090: step 3179, loss 0.407514, acc 0.921875, prec 0.0639447, recall 0.833333
2017-12-10T04:31:03.637710: step 3180, loss 0.436036, acc 0.859375, prec 0.0639509, recall 0.83337
2017-12-10T04:31:03.901425: step 3181, loss 0.946431, acc 0.90625, prec 0.0639923, recall 0.833482
2017-12-10T04:31:04.167572: step 3182, loss 0.346786, acc 0.890625, prec 0.0639847, recall 0.833482
2017-12-10T04:31:04.431854: step 3183, loss 0.616447, acc 0.859375, prec 0.0639908, recall 0.833519
2017-12-10T04:31:04.699705: step 3184, loss 0.377873, acc 0.859375, prec 0.063981, recall 0.833519
2017-12-10T04:31:04.962734: step 3185, loss 0.285219, acc 0.875, prec 0.0639723, recall 0.833519
2017-12-10T04:31:05.227590: step 3186, loss 0.327719, acc 0.890625, prec 0.0639806, recall 0.833556
2017-12-10T04:31:05.496861: step 3187, loss 0.525235, acc 0.859375, prec 0.0640667, recall 0.833778
2017-12-10T04:31:05.758589: step 3188, loss 0.34255, acc 0.890625, prec 0.064075, recall 0.833815
2017-12-10T04:31:06.029651: step 3189, loss 0.190565, acc 0.90625, prec 0.0641323, recall 0.833962
2017-12-10T04:31:06.297271: step 3190, loss 0.102867, acc 0.953125, prec 0.064145, recall 0.833999
2017-12-10T04:31:06.558313: step 3191, loss 0.398753, acc 0.921875, prec 0.0641875, recall 0.83411
2017-12-10T04:31:06.820538: step 3192, loss 0.176557, acc 0.953125, prec 0.0641842, recall 0.83411
2017-12-10T04:31:07.092578: step 3193, loss 0.226684, acc 0.96875, prec 0.0642139, recall 0.834183
2017-12-10T04:31:07.357922: step 3194, loss 0.0579889, acc 0.96875, prec 0.0642117, recall 0.834183
2017-12-10T04:31:07.622383: step 3195, loss 0.144057, acc 0.96875, prec 0.0642095, recall 0.834183
2017-12-10T04:31:07.888698: step 3196, loss 0.171459, acc 0.96875, prec 0.0642233, recall 0.83422
2017-12-10T04:31:08.157369: step 3197, loss 0.172313, acc 0.96875, prec 0.064269, recall 0.83433
2017-12-10T04:31:08.425893: step 3198, loss 0.810721, acc 0.921875, prec 0.0643114, recall 0.83444
2017-12-10T04:31:08.699651: step 3199, loss 0.052932, acc 1, prec 0.0643433, recall 0.834513
2017-12-10T04:31:08.963110: step 3200, loss 0.152748, acc 0.984375, prec 0.0643582, recall 0.83455
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-3200

2017-12-10T04:31:10.229125: step 3201, loss 0.240792, acc 0.953125, prec 0.0643549, recall 0.83455
2017-12-10T04:31:10.498898: step 3202, loss 0.106394, acc 0.953125, prec 0.0643516, recall 0.83455
2017-12-10T04:31:10.768801: step 3203, loss 0.321609, acc 0.921875, prec 0.064378, recall 0.834623
2017-12-10T04:31:11.032656: step 3204, loss 0.0105937, acc 1, prec 0.064378, recall 0.834623
2017-12-10T04:31:11.294262: step 3205, loss 0.0705668, acc 0.984375, prec 0.0643929, recall 0.83466
2017-12-10T04:31:11.561902: step 3206, loss 0.153837, acc 0.953125, prec 0.0643896, recall 0.83466
2017-12-10T04:31:11.833018: step 3207, loss 0.156574, acc 0.953125, prec 0.0643863, recall 0.83466
2017-12-10T04:31:12.107005: step 3208, loss 0.0848834, acc 0.96875, prec 0.0643841, recall 0.83466
2017-12-10T04:31:12.366679: step 3209, loss 0.217742, acc 0.953125, prec 0.0643808, recall 0.83466
2017-12-10T04:31:12.628030: step 3210, loss 0.376293, acc 0.953125, prec 0.0644254, recall 0.834769
2017-12-10T04:31:12.892335: step 3211, loss 3.73074, acc 0.921875, prec 0.0644369, recall 0.834621
2017-12-10T04:31:13.165221: step 3212, loss 0.098087, acc 0.984375, prec 0.0644518, recall 0.834658
2017-12-10T04:31:13.431874: step 3213, loss 0.0406554, acc 0.984375, prec 0.0644507, recall 0.834658
2017-12-10T04:31:13.697971: step 3214, loss 0.284278, acc 0.921875, prec 0.0644452, recall 0.834658
2017-12-10T04:31:13.960493: step 3215, loss 0.118195, acc 0.953125, prec 0.0644578, recall 0.834694
2017-12-10T04:31:14.223878: step 3216, loss 0.350991, acc 0.9375, prec 0.0644694, recall 0.834731
2017-12-10T04:31:14.488167: step 3217, loss 0.324942, acc 0.96875, prec 0.0644831, recall 0.834767
2017-12-10T04:31:14.762044: step 3218, loss 0.0981602, acc 0.953125, prec 0.0644799, recall 0.834767
2017-12-10T04:31:15.029539: step 3219, loss 0.249819, acc 0.921875, prec 0.0644903, recall 0.834804
2017-12-10T04:31:15.292758: step 3220, loss 0.0936624, acc 0.953125, prec 0.0645189, recall 0.834877
2017-12-10T04:31:15.562330: step 3221, loss 0.559634, acc 0.859375, prec 0.064509, recall 0.834877
2017-12-10T04:31:15.819671: step 3222, loss 0.195413, acc 0.90625, prec 0.0645024, recall 0.834877
2017-12-10T04:31:16.082505: step 3223, loss 0.206856, acc 0.90625, prec 0.0645117, recall 0.834913
2017-12-10T04:31:16.347877: step 3224, loss 0.216201, acc 0.890625, prec 0.06452, recall 0.834949
2017-12-10T04:31:16.616505: step 3225, loss 0.444506, acc 0.890625, prec 0.0645123, recall 0.834949
2017-12-10T04:31:16.882263: step 3226, loss 0.175647, acc 0.90625, prec 0.0645216, recall 0.834986
2017-12-10T04:31:17.161122: step 3227, loss 0.463952, acc 0.875, prec 0.0645128, recall 0.834986
2017-12-10T04:31:17.423363: step 3228, loss 0.373476, acc 0.90625, prec 0.0645222, recall 0.835022
2017-12-10T04:31:17.687604: step 3229, loss 0.246187, acc 0.90625, prec 0.0645633, recall 0.835131
2017-12-10T04:31:17.960410: step 3230, loss 0.181263, acc 0.9375, prec 0.0645749, recall 0.835167
2017-12-10T04:31:18.223742: step 3231, loss 0.501704, acc 0.921875, prec 0.0646012, recall 0.83524
2017-12-10T04:31:18.487139: step 3232, loss 2.27851, acc 0.953125, prec 0.064599, recall 0.835056
2017-12-10T04:31:18.750233: step 3233, loss 0.257657, acc 0.953125, prec 0.0646116, recall 0.835092
2017-12-10T04:31:19.013736: step 3234, loss 0.111738, acc 0.9375, prec 0.064639, recall 0.835165
2017-12-10T04:31:19.280528: step 3235, loss 0.716784, acc 0.921875, prec 0.0646495, recall 0.835201
2017-12-10T04:31:19.540371: step 3236, loss 0.0875859, acc 0.953125, prec 0.0646462, recall 0.835201
2017-12-10T04:31:19.812288: step 3237, loss 0.15533, acc 0.953125, prec 0.0646429, recall 0.835201
2017-12-10T04:31:20.076086: step 3238, loss 0.518814, acc 0.875, prec 0.06465, recall 0.835237
2017-12-10T04:31:20.339206: step 3239, loss 0.46446, acc 0.875, prec 0.0646571, recall 0.835273
2017-12-10T04:31:20.604877: step 3240, loss 0.408759, acc 0.859375, prec 0.064679, recall 0.835346
2017-12-10T04:31:20.876391: step 3241, loss 0.280974, acc 0.921875, prec 0.0647053, recall 0.835418
2017-12-10T04:31:21.144857: step 3242, loss 0.60316, acc 0.875, prec 0.0647283, recall 0.83549
2017-12-10T04:31:21.406357: step 3243, loss 0.313661, acc 0.84375, prec 0.0647491, recall 0.835562
2017-12-10T04:31:21.664933: step 3244, loss 0.424552, acc 0.875, prec 0.0647403, recall 0.835562
2017-12-10T04:31:21.928541: step 3245, loss 0.362468, acc 0.875, prec 0.0647315, recall 0.835562
2017-12-10T04:31:22.189328: step 3246, loss 0.188344, acc 0.9375, prec 0.0647271, recall 0.835562
2017-12-10T04:31:22.448474: step 3247, loss 0.284744, acc 0.859375, prec 0.0647331, recall 0.835598
2017-12-10T04:31:22.710314: step 3248, loss 0.418253, acc 0.875, prec 0.0647243, recall 0.835598
2017-12-10T04:31:22.974111: step 3249, loss 0.358639, acc 0.890625, prec 0.0647324, recall 0.835634
2017-12-10T04:31:23.237233: step 3250, loss 0.351475, acc 0.921875, prec 0.0647587, recall 0.835706
2017-12-10T04:31:23.503113: step 3251, loss 0.285148, acc 0.890625, prec 0.0647828, recall 0.835778
2017-12-10T04:31:23.775337: step 3252, loss 0.110897, acc 0.96875, prec 0.0647806, recall 0.835778
2017-12-10T04:31:24.044554: step 3253, loss 0.0708426, acc 0.984375, prec 0.0647795, recall 0.835778
2017-12-10T04:31:24.311770: step 3254, loss 0.267156, acc 0.984375, prec 0.0648101, recall 0.83585
2017-12-10T04:31:24.584142: step 3255, loss 2.58433, acc 0.90625, prec 0.0648205, recall 0.835703
2017-12-10T04:31:24.849852: step 3256, loss 0.184336, acc 0.953125, prec 0.0648172, recall 0.835703
2017-12-10T04:31:25.116406: step 3257, loss 0.293258, acc 0.9375, prec 0.0648286, recall 0.835739
2017-12-10T04:31:25.381357: step 3258, loss 0.317113, acc 0.90625, prec 0.064822, recall 0.835739
2017-12-10T04:31:25.647246: step 3259, loss 0.497973, acc 0.921875, prec 0.0648324, recall 0.835775
2017-12-10T04:31:25.910032: step 3260, loss 0.0731677, acc 0.953125, prec 0.0648291, recall 0.835775
2017-12-10T04:31:26.173052: step 3261, loss 0.26027, acc 0.9375, prec 0.0648247, recall 0.835775
2017-12-10T04:31:26.436488: step 3262, loss 0.195629, acc 0.9375, prec 0.064852, recall 0.835847
2017-12-10T04:31:26.701244: step 3263, loss 0.0250199, acc 1, prec 0.0648679, recall 0.835883
2017-12-10T04:31:26.973866: step 3264, loss 0.059699, acc 0.984375, prec 0.0648827, recall 0.835919
2017-12-10T04:31:27.243792: step 3265, loss 0.0878404, acc 0.96875, prec 0.0648804, recall 0.835919
2017-12-10T04:31:27.506807: step 3266, loss 0.0195198, acc 1, prec 0.0649122, recall 0.83599
2017-12-10T04:31:27.771867: step 3267, loss 0.139348, acc 0.9375, prec 0.0649078, recall 0.83599
2017-12-10T04:31:28.038258: step 3268, loss 0.130784, acc 0.96875, prec 0.0649373, recall 0.836062
2017-12-10T04:31:28.304511: step 3269, loss 0.0963048, acc 0.953125, prec 0.0649498, recall 0.836098
2017-12-10T04:31:28.567941: step 3270, loss 0.135087, acc 0.953125, prec 0.0649782, recall 0.836169
2017-12-10T04:31:28.835372: step 3271, loss 0.0429158, acc 1, prec 0.0649782, recall 0.836169
2017-12-10T04:31:29.103738: step 3272, loss 0.225721, acc 0.953125, prec 0.0649908, recall 0.836205
2017-12-10T04:31:29.371369: step 3273, loss 0.0457478, acc 1, prec 0.0649908, recall 0.836205
2017-12-10T04:31:29.636782: step 3274, loss 0.067906, acc 0.984375, prec 0.0650214, recall 0.836276
2017-12-10T04:31:29.902596: step 3275, loss 0.20941, acc 0.921875, prec 0.0650158, recall 0.836276
2017-12-10T04:31:30.170505: step 3276, loss 0.0195379, acc 1, prec 0.0650158, recall 0.836276
2017-12-10T04:31:30.445361: step 3277, loss 0.242294, acc 0.90625, prec 0.0650092, recall 0.836276
2017-12-10T04:31:30.722208: step 3278, loss 0.117708, acc 0.984375, prec 0.0650398, recall 0.836348
2017-12-10T04:31:30.985108: step 3279, loss 0.2032, acc 0.96875, prec 0.0650376, recall 0.836348
2017-12-10T04:31:31.267048: step 3280, loss 0.326884, acc 0.96875, prec 0.0650988, recall 0.83649
2017-12-10T04:31:31.532418: step 3281, loss 0.0118129, acc 1, prec 0.0650988, recall 0.83649
2017-12-10T04:31:31.791454: step 3282, loss 0.452289, acc 0.953125, prec 0.0651272, recall 0.836562
2017-12-10T04:31:32.061533: step 3283, loss 3.53306, acc 0.984375, prec 0.065143, recall 0.836415
2017-12-10T04:31:32.330836: step 3284, loss 0.0675218, acc 1, prec 0.0651588, recall 0.836451
2017-12-10T04:31:32.594823: step 3285, loss 0.17478, acc 0.953125, prec 0.065203, recall 0.836557
2017-12-10T04:31:32.863877: step 3286, loss 0.358414, acc 0.96875, prec 0.0652325, recall 0.836628
2017-12-10T04:31:33.130530: step 3287, loss 0.794812, acc 0.953125, prec 0.0652608, recall 0.836699
2017-12-10T04:31:33.397077: step 3288, loss 0.296414, acc 0.890625, prec 0.0652531, recall 0.836699
2017-12-10T04:31:33.659404: step 3289, loss 0.56184, acc 0.859375, prec 0.0652432, recall 0.836699
2017-12-10T04:31:33.918421: step 3290, loss 0.345489, acc 0.921875, prec 0.0652376, recall 0.836699
2017-12-10T04:31:34.176552: step 3291, loss 0.979337, acc 0.890625, prec 0.0652616, recall 0.83677
2017-12-10T04:31:34.436989: step 3292, loss 0.401169, acc 0.828125, prec 0.0652652, recall 0.836806
2017-12-10T04:31:34.695671: step 3293, loss 1.13438, acc 0.78125, prec 0.0652498, recall 0.836806
2017-12-10T04:31:34.954965: step 3294, loss 0.489571, acc 0.8125, prec 0.0652365, recall 0.836806
2017-12-10T04:31:35.216688: step 3295, loss 0.492813, acc 0.84375, prec 0.0652413, recall 0.836841
2017-12-10T04:31:35.483576: step 3296, loss 0.477292, acc 0.859375, prec 0.0652472, recall 0.836876
2017-12-10T04:31:35.742908: step 3297, loss 0.259163, acc 0.890625, prec 0.0652553, recall 0.836912
2017-12-10T04:31:36.006036: step 3298, loss 0.327159, acc 0.859375, prec 0.0652453, recall 0.836912
2017-12-10T04:31:36.266248: step 3299, loss 0.356943, acc 0.890625, prec 0.0652376, recall 0.836912
2017-12-10T04:31:36.532792: step 3300, loss 0.52248, acc 0.875, prec 0.0652288, recall 0.836912

Evaluation:
2017-12-10T04:31:44.194875: step 3300, loss 1.61746, acc 0.895358, prec 0.0657031, recall 0.832948

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-3300

2017-12-10T04:31:45.479004: step 3301, loss 0.101417, acc 0.96875, prec 0.0657009, recall 0.832948
2017-12-10T04:31:45.740271: step 3302, loss 0.249536, acc 0.921875, prec 0.0656955, recall 0.832948
2017-12-10T04:31:46.008103: step 3303, loss 0.163351, acc 0.953125, prec 0.0656922, recall 0.832948
2017-12-10T04:31:46.270275: step 3304, loss 0.224719, acc 0.90625, prec 0.0656857, recall 0.832948
2017-12-10T04:31:46.532524: step 3305, loss 0.175168, acc 0.96875, prec 0.0657145, recall 0.833018
2017-12-10T04:31:46.793833: step 3306, loss 0.365179, acc 0.9375, prec 0.0657101, recall 0.833018
2017-12-10T04:31:47.055327: step 3307, loss 0.0515295, acc 0.984375, prec 0.0657245, recall 0.833053
2017-12-10T04:31:47.328804: step 3308, loss 0.119647, acc 0.953125, prec 0.0657212, recall 0.833053
2017-12-10T04:31:47.593524: step 3309, loss 0.0788474, acc 0.96875, prec 0.0657191, recall 0.833053
2017-12-10T04:31:47.855155: step 3310, loss 0.0811737, acc 0.984375, prec 0.065718, recall 0.833053
2017-12-10T04:31:48.129461: step 3311, loss 0.135404, acc 0.984375, prec 0.0657324, recall 0.833088
2017-12-10T04:31:48.392408: step 3312, loss 0.0917022, acc 0.953125, prec 0.0657446, recall 0.833123
2017-12-10T04:31:48.658670: step 3313, loss 0.111927, acc 0.96875, prec 0.0657579, recall 0.833158
2017-12-10T04:31:48.923200: step 3314, loss 0.0379757, acc 0.984375, prec 0.0657568, recall 0.833158
2017-12-10T04:31:49.194266: step 3315, loss 0.0120843, acc 1, prec 0.0657568, recall 0.833158
2017-12-10T04:31:49.460096: step 3316, loss 0.0671071, acc 1, prec 0.0657723, recall 0.833193
2017-12-10T04:31:49.723826: step 3317, loss 0.00845323, acc 1, prec 0.0657723, recall 0.833193
2017-12-10T04:31:49.989427: step 3318, loss 5.34446, acc 0.953125, prec 0.0657701, recall 0.833019
2017-12-10T04:31:50.265826: step 3319, loss 0.165438, acc 1, prec 0.0657855, recall 0.833054
2017-12-10T04:31:50.535861: step 3320, loss 0.0180634, acc 1, prec 0.0657855, recall 0.833054
2017-12-10T04:31:50.800271: step 3321, loss 0.0227474, acc 0.984375, prec 0.0657845, recall 0.833054
2017-12-10T04:31:51.072451: step 3322, loss 0.0970985, acc 0.9375, prec 0.0657801, recall 0.833054
2017-12-10T04:31:51.337648: step 3323, loss 0.0911932, acc 0.984375, prec 0.0657945, recall 0.833089
2017-12-10T04:31:51.603991: step 3324, loss 0.281782, acc 0.96875, prec 0.0658232, recall 0.833159
2017-12-10T04:31:51.872447: step 3325, loss 0.12617, acc 0.9375, prec 0.0658189, recall 0.833159
2017-12-10T04:31:52.138332: step 3326, loss 0.1756, acc 0.953125, prec 0.0658156, recall 0.833159
2017-12-10T04:31:52.407914: step 3327, loss 0.588441, acc 0.875, prec 0.0658069, recall 0.833159
2017-12-10T04:31:52.671086: step 3328, loss 0.510806, acc 0.890625, prec 0.0657993, recall 0.833159
2017-12-10T04:31:52.933548: step 3329, loss 0.452168, acc 0.921875, prec 0.0658248, recall 0.833229
2017-12-10T04:31:53.197230: step 3330, loss 0.0900187, acc 0.984375, prec 0.0658546, recall 0.833298
2017-12-10T04:31:53.460156: step 3331, loss 0.159142, acc 0.9375, prec 0.0658502, recall 0.833298
2017-12-10T04:31:53.730726: step 3332, loss 0.533888, acc 0.90625, prec 0.0658591, recall 0.833333
2017-12-10T04:31:53.992354: step 3333, loss 0.101774, acc 0.9375, prec 0.0658548, recall 0.833333
2017-12-10T04:31:54.264105: step 3334, loss 0.978286, acc 0.921875, prec 0.0658803, recall 0.833403
2017-12-10T04:31:54.536929: step 3335, loss 0.446184, acc 0.90625, prec 0.0659046, recall 0.833473
2017-12-10T04:31:54.802096: step 3336, loss 0.208065, acc 0.9375, prec 0.0659003, recall 0.833473
2017-12-10T04:31:55.069568: step 3337, loss 0.305575, acc 0.9375, prec 0.0659113, recall 0.833508
2017-12-10T04:31:55.339673: step 3338, loss 0.351874, acc 0.90625, prec 0.0659048, recall 0.833508
2017-12-10T04:31:55.604909: step 3339, loss 0.877423, acc 0.875, prec 0.0659115, recall 0.833542
2017-12-10T04:31:55.880359: step 3340, loss 0.292809, acc 0.921875, prec 0.0659061, recall 0.833542
2017-12-10T04:31:56.145312: step 3341, loss 0.178216, acc 0.9375, prec 0.0659017, recall 0.833542
2017-12-10T04:31:56.410552: step 3342, loss 0.993276, acc 0.78125, prec 0.0659328, recall 0.833647
2017-12-10T04:31:56.675750: step 3343, loss 0.304636, acc 0.875, prec 0.065955, recall 0.833716
2017-12-10T04:31:56.956918: step 3344, loss 0.164408, acc 0.9375, prec 0.0659506, recall 0.833716
2017-12-10T04:31:57.226398: step 3345, loss 0.198597, acc 0.9375, prec 0.0659462, recall 0.833716
2017-12-10T04:31:57.487962: step 3346, loss 0.271037, acc 0.921875, prec 0.0659717, recall 0.833786
2017-12-10T04:31:57.761478: step 3347, loss 0.538691, acc 0.921875, prec 0.0659971, recall 0.833855
2017-12-10T04:31:58.022652: step 3348, loss 0.133662, acc 0.953125, prec 0.0660246, recall 0.833924
2017-12-10T04:31:58.285078: step 3349, loss 2.97258, acc 0.9375, prec 0.0660522, recall 0.83382
2017-12-10T04:31:58.552909: step 3350, loss 0.324576, acc 0.875, prec 0.0660743, recall 0.833889
2017-12-10T04:31:58.815238: step 3351, loss 0.551841, acc 0.84375, prec 0.0660789, recall 0.833924
2017-12-10T04:31:59.078081: step 3352, loss 0.469622, acc 0.828125, prec 0.0660669, recall 0.833924
2017-12-10T04:31:59.337552: step 3353, loss 0.4141, acc 0.875, prec 0.066089, recall 0.833993
2017-12-10T04:31:59.607494: step 3354, loss 0.222409, acc 0.921875, prec 0.0661143, recall 0.834062
2017-12-10T04:31:59.871123: step 3355, loss 0.199929, acc 0.953125, prec 0.0661111, recall 0.834062
2017-12-10T04:32:00.144735: step 3356, loss 0.332071, acc 0.890625, prec 0.0661034, recall 0.834062
2017-12-10T04:32:00.412616: step 3357, loss 0.70859, acc 0.84375, prec 0.0661233, recall 0.834131
2017-12-10T04:32:00.687686: step 3358, loss 0.254354, acc 0.90625, prec 0.0661168, recall 0.834131
2017-12-10T04:32:00.951523: step 3359, loss 0.130015, acc 0.953125, prec 0.0661289, recall 0.834166
2017-12-10T04:32:01.225852: step 3360, loss 0.233469, acc 0.90625, prec 0.0661378, recall 0.8342
2017-12-10T04:32:01.497510: step 3361, loss 0.218299, acc 0.953125, prec 0.0661499, recall 0.834235
2017-12-10T04:32:01.767697: step 3362, loss 0.0954024, acc 0.953125, prec 0.0661466, recall 0.834235
2017-12-10T04:32:02.035274: step 3363, loss 0.402965, acc 0.953125, prec 0.0661588, recall 0.834269
2017-12-10T04:32:02.297260: step 3364, loss 0.400182, acc 0.875, prec 0.0661808, recall 0.834338
2017-12-10T04:32:02.569578: step 3365, loss 0.204399, acc 0.90625, prec 0.0661743, recall 0.834338
2017-12-10T04:32:02.845849: step 3366, loss 0.377198, acc 0.875, prec 0.066181, recall 0.834372
2017-12-10T04:32:03.107300: step 3367, loss 0.114276, acc 0.9375, prec 0.0661766, recall 0.834372
2017-12-10T04:32:03.375387: step 3368, loss 0.30416, acc 0.90625, prec 0.0661854, recall 0.834407
2017-12-10T04:32:03.640808: step 3369, loss 0.151929, acc 0.984375, prec 0.0661997, recall 0.834441
2017-12-10T04:32:03.907700: step 3370, loss 0.167806, acc 0.953125, prec 0.0661965, recall 0.834441
2017-12-10T04:32:04.169635: step 3371, loss 0.0560982, acc 0.96875, prec 0.0662097, recall 0.834476
2017-12-10T04:32:04.441293: step 3372, loss 0.0481603, acc 0.984375, prec 0.0662086, recall 0.834476
2017-12-10T04:32:04.702529: step 3373, loss 6.47012, acc 0.890625, prec 0.0662328, recall 0.834371
2017-12-10T04:32:04.975315: step 3374, loss 3.43926, acc 0.9375, prec 0.0662449, recall 0.834232
2017-12-10T04:32:05.246388: step 3375, loss 0.172045, acc 0.921875, prec 0.0662395, recall 0.834232
2017-12-10T04:32:05.515855: step 3376, loss 0.278439, acc 0.921875, prec 0.0662648, recall 0.834301
2017-12-10T04:32:05.781016: step 3377, loss 0.4197, acc 0.90625, prec 0.0662582, recall 0.834301
2017-12-10T04:32:06.049265: step 3378, loss 0.936217, acc 0.765625, prec 0.0662419, recall 0.834301
2017-12-10T04:32:06.313959: step 3379, loss 0.381274, acc 0.90625, prec 0.0662353, recall 0.834301
2017-12-10T04:32:06.573305: step 3380, loss 0.838443, acc 0.78125, prec 0.0662354, recall 0.834335
2017-12-10T04:32:06.830309: step 3381, loss 0.813141, acc 0.8125, prec 0.0662223, recall 0.834335
2017-12-10T04:32:07.090683: step 3382, loss 0.448861, acc 0.828125, prec 0.0662411, recall 0.834404
2017-12-10T04:32:07.350440: step 3383, loss 2.63811, acc 0.75, prec 0.0662247, recall 0.834231
2017-12-10T04:32:07.619574: step 3384, loss 1.12976, acc 0.71875, prec 0.0662051, recall 0.834231
2017-12-10T04:32:07.876553: step 3385, loss 0.848211, acc 0.796875, prec 0.0662217, recall 0.8343
2017-12-10T04:32:08.136040: step 3386, loss 1.34924, acc 0.703125, prec 0.066201, recall 0.8343
2017-12-10T04:32:08.397901: step 3387, loss 0.445726, acc 0.859375, prec 0.0662066, recall 0.834334
2017-12-10T04:32:08.663599: step 3388, loss 0.437976, acc 0.828125, prec 0.0661946, recall 0.834334
2017-12-10T04:32:08.921934: step 3389, loss 0.890178, acc 0.8125, prec 0.0661969, recall 0.834369
2017-12-10T04:32:09.183332: step 3390, loss 0.569941, acc 0.828125, prec 0.0662156, recall 0.834437
2017-12-10T04:32:09.441517: step 3391, loss 1.09862, acc 0.71875, prec 0.0662114, recall 0.834471
2017-12-10T04:32:09.708434: step 3392, loss 0.898445, acc 0.734375, prec 0.0662235, recall 0.83454
2017-12-10T04:32:09.979515: step 3393, loss 0.50086, acc 0.84375, prec 0.0662433, recall 0.834608
2017-12-10T04:32:10.243745: step 3394, loss 0.506634, acc 0.90625, prec 0.0662368, recall 0.834608
2017-12-10T04:32:10.502748: step 3395, loss 0.737504, acc 0.859375, prec 0.0662423, recall 0.834642
2017-12-10T04:32:10.767220: step 3396, loss 0.860105, acc 0.921875, prec 0.0662675, recall 0.834711
2017-12-10T04:32:11.029848: step 3397, loss 0.448431, acc 0.875, prec 0.0662588, recall 0.834711
2017-12-10T04:32:11.295584: step 3398, loss 0.627784, acc 0.9375, prec 0.0662698, recall 0.834745
2017-12-10T04:32:11.555780: step 3399, loss 0.39429, acc 0.90625, prec 0.0662633, recall 0.834745
2017-12-10T04:32:11.820520: step 3400, loss 0.0434294, acc 0.984375, prec 0.0662775, recall 0.834779
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-3400

2017-12-10T04:32:13.155812: step 3401, loss 2.81152, acc 0.953125, prec 0.0662753, recall 0.834607
2017-12-10T04:32:13.429236: step 3402, loss 0.178754, acc 0.96875, prec 0.0663038, recall 0.834675
2017-12-10T04:32:13.700787: step 3403, loss 0.0891791, acc 0.96875, prec 0.0663016, recall 0.834675
2017-12-10T04:32:13.969069: step 3404, loss 0.375647, acc 0.921875, prec 0.0662962, recall 0.834675
2017-12-10T04:32:14.237258: step 3405, loss 0.56237, acc 1, prec 0.0663268, recall 0.834743
2017-12-10T04:32:14.506699: step 3406, loss 0.181574, acc 0.953125, prec 0.0663235, recall 0.834743
2017-12-10T04:32:14.773869: step 3407, loss 0.149219, acc 0.96875, prec 0.0663366, recall 0.834777
2017-12-10T04:32:15.039306: step 3408, loss 0.257261, acc 0.9375, prec 0.0663323, recall 0.834777
2017-12-10T04:32:15.302496: step 3409, loss 0.138213, acc 0.9375, prec 0.0663433, recall 0.834811
2017-12-10T04:32:15.568197: step 3410, loss 0.116686, acc 0.984375, prec 0.0663422, recall 0.834811
2017-12-10T04:32:15.833484: step 3411, loss 0.222049, acc 0.9375, prec 0.0663531, recall 0.834845
2017-12-10T04:32:16.096333: step 3412, loss 0.453118, acc 0.921875, prec 0.066363, recall 0.834879
2017-12-10T04:32:16.359794: step 3413, loss 0.247731, acc 0.9375, prec 0.0663892, recall 0.834947
2017-12-10T04:32:16.624502: step 3414, loss 0.15172, acc 0.953125, prec 0.066386, recall 0.834947
2017-12-10T04:32:16.885673: step 3415, loss 0.112325, acc 0.984375, prec 0.0664002, recall 0.834981
2017-12-10T04:32:17.153599: step 3416, loss 0.246456, acc 0.9375, prec 0.0664264, recall 0.835049
2017-12-10T04:32:17.423610: step 3417, loss 0.121707, acc 0.984375, prec 0.0664253, recall 0.835049
2017-12-10T04:32:17.688348: step 3418, loss 0.307606, acc 0.921875, prec 0.0664352, recall 0.835083
2017-12-10T04:32:17.955717: step 3419, loss 0.166714, acc 0.953125, prec 0.0664778, recall 0.835185
2017-12-10T04:32:18.219114: step 3420, loss 0.233667, acc 0.96875, prec 0.0665062, recall 0.835253
2017-12-10T04:32:18.485886: step 3421, loss 0.211084, acc 0.9375, prec 0.0665018, recall 0.835253
2017-12-10T04:32:18.752146: step 3422, loss 0.417534, acc 0.921875, prec 0.0665117, recall 0.835287
2017-12-10T04:32:19.021988: step 3423, loss 0.104229, acc 0.96875, prec 0.0665248, recall 0.835321
2017-12-10T04:32:19.291202: step 3424, loss 0.869053, acc 0.90625, prec 0.0665335, recall 0.835355
2017-12-10T04:32:19.559520: step 3425, loss 0.323033, acc 0.90625, prec 0.0665575, recall 0.835422
2017-12-10T04:32:19.819661: step 3426, loss 0.331872, acc 0.90625, prec 0.066551, recall 0.835422
2017-12-10T04:32:20.084747: step 3427, loss 0.110216, acc 0.953125, prec 0.0665477, recall 0.835422
2017-12-10T04:32:20.353153: step 3428, loss 0.187866, acc 0.921875, prec 0.0665423, recall 0.835422
2017-12-10T04:32:20.619945: step 3429, loss 0.211688, acc 0.921875, prec 0.0665521, recall 0.835456
2017-12-10T04:32:20.886553: step 3430, loss 0.125912, acc 0.984375, prec 0.0665969, recall 0.835557
2017-12-10T04:32:21.149518: step 3431, loss 0.10097, acc 0.96875, prec 0.0665947, recall 0.835557
2017-12-10T04:32:21.410230: step 3432, loss 0.620797, acc 0.921875, prec 0.0666503, recall 0.835692
2017-12-10T04:32:21.675658: step 3433, loss 0.0816721, acc 0.96875, prec 0.0666634, recall 0.835726
2017-12-10T04:32:21.940689: step 3434, loss 0.0359264, acc 1, prec 0.0666787, recall 0.83576
2017-12-10T04:32:22.206987: step 3435, loss 0.156028, acc 0.984375, prec 0.0667081, recall 0.835827
2017-12-10T04:32:22.476843: step 3436, loss 0.973144, acc 0.921875, prec 0.0667484, recall 0.835928
2017-12-10T04:32:22.753688: step 3437, loss 0.153458, acc 0.984375, prec 0.0667474, recall 0.835928
2017-12-10T04:32:23.021484: step 3438, loss 0.145836, acc 0.9375, prec 0.066743, recall 0.835928
2017-12-10T04:32:23.293184: step 3439, loss 0.146327, acc 0.96875, prec 0.0667713, recall 0.835995
2017-12-10T04:32:23.563037: step 3440, loss 0.285355, acc 0.90625, prec 0.0667648, recall 0.835995
2017-12-10T04:32:23.827987: step 3441, loss 0.320332, acc 0.96875, prec 0.0667626, recall 0.835995
2017-12-10T04:32:24.093698: step 3442, loss 0.195906, acc 0.953125, prec 0.0667898, recall 0.836062
2017-12-10T04:32:24.363292: step 3443, loss 0.199324, acc 0.9375, prec 0.0667855, recall 0.836062
2017-12-10T04:32:24.636950: step 3444, loss 0.116139, acc 0.96875, prec 0.0667833, recall 0.836062
2017-12-10T04:32:24.898518: step 3445, loss 0.437848, acc 0.90625, prec 0.066792, recall 0.836096
2017-12-10T04:32:25.168243: step 3446, loss 0.0493619, acc 0.96875, prec 0.0668051, recall 0.836129
2017-12-10T04:32:25.435342: step 3447, loss 0.0748794, acc 0.984375, prec 0.066804, recall 0.836129
2017-12-10T04:32:25.700624: step 3448, loss 0.243174, acc 0.890625, prec 0.0667963, recall 0.836129
2017-12-10T04:32:25.966733: step 3449, loss 0.17822, acc 0.9375, prec 0.066792, recall 0.836129
2017-12-10T04:32:26.230173: step 3450, loss 0.438193, acc 0.96875, prec 0.066805, recall 0.836163
2017-12-10T04:32:26.506790: step 3451, loss 2.49027, acc 0.9375, prec 0.066817, recall 0.836025
2017-12-10T04:32:26.771912: step 3452, loss 0.172655, acc 0.921875, prec 0.066842, recall 0.836092
2017-12-10T04:32:27.043243: step 3453, loss 0.253496, acc 0.875, prec 0.0668485, recall 0.836126
2017-12-10T04:32:27.309855: step 3454, loss 0.235151, acc 0.9375, prec 0.0668442, recall 0.836126
2017-12-10T04:32:27.571012: step 3455, loss 0.0547472, acc 1, prec 0.0668594, recall 0.836159
2017-12-10T04:32:27.833411: step 3456, loss 0.422281, acc 0.9375, prec 0.0668551, recall 0.836159
2017-12-10T04:32:28.104847: step 3457, loss 0.611331, acc 0.890625, prec 0.0668474, recall 0.836159
2017-12-10T04:32:28.378156: step 3458, loss 0.530631, acc 0.890625, prec 0.0668398, recall 0.836159
2017-12-10T04:32:28.649121: step 3459, loss 0.421558, acc 0.890625, prec 0.0668321, recall 0.836159
2017-12-10T04:32:28.917480: step 3460, loss 0.639654, acc 0.828125, prec 0.0668506, recall 0.836226
2017-12-10T04:32:29.185381: step 3461, loss 0.518472, acc 0.859375, prec 0.0668408, recall 0.836226
2017-12-10T04:32:29.447099: step 3462, loss 0.175432, acc 0.9375, prec 0.0668821, recall 0.836327
2017-12-10T04:32:29.710536: step 3463, loss 0.305695, acc 0.90625, prec 0.0668755, recall 0.836327
2017-12-10T04:32:29.980646: step 3464, loss 0.231795, acc 0.953125, prec 0.0668723, recall 0.836327
2017-12-10T04:32:30.250277: step 3465, loss 0.166407, acc 0.9375, prec 0.0668831, recall 0.83636
2017-12-10T04:32:30.522036: step 3466, loss 0.260979, acc 0.953125, prec 0.0669408, recall 0.836493
2017-12-10T04:32:30.795232: step 3467, loss 0.229017, acc 0.953125, prec 0.0669527, recall 0.836527
2017-12-10T04:32:31.067644: step 3468, loss 0.216501, acc 0.953125, prec 0.0669646, recall 0.83656
2017-12-10T04:32:31.338564: step 3469, loss 0.169329, acc 0.96875, prec 0.0669929, recall 0.836627
2017-12-10T04:32:31.601638: step 3470, loss 0.1012, acc 0.984375, prec 0.067007, recall 0.83666
2017-12-10T04:32:31.869710: step 3471, loss 0.398438, acc 0.96875, prec 0.0670201, recall 0.836693
2017-12-10T04:32:32.139590: step 3472, loss 0.0498843, acc 0.96875, prec 0.0670331, recall 0.836726
2017-12-10T04:32:32.409038: step 3473, loss 0.0618608, acc 0.984375, prec 0.0670624, recall 0.836793
2017-12-10T04:32:32.676190: step 3474, loss 0.841764, acc 0.96875, prec 0.0670907, recall 0.836859
2017-12-10T04:32:32.942332: step 3475, loss 0.0548535, acc 0.984375, prec 0.06712, recall 0.836926
2017-12-10T04:32:33.214464: step 3476, loss 0.133905, acc 0.953125, prec 0.0671167, recall 0.836926
2017-12-10T04:32:33.493009: step 3477, loss 0.261455, acc 0.953125, prec 0.0671134, recall 0.836926
2017-12-10T04:32:33.753622: step 3478, loss 0.453413, acc 0.921875, prec 0.067108, recall 0.836926
2017-12-10T04:32:33.994790: step 3479, loss 0.47408, acc 0.903846, prec 0.0671025, recall 0.836926
2017-12-10T04:32:34.270444: step 3480, loss 0.529028, acc 0.953125, prec 0.0671144, recall 0.836959
2017-12-10T04:32:34.540392: step 3481, loss 0.135903, acc 0.96875, prec 0.0671122, recall 0.836959
2017-12-10T04:32:34.804549: step 3482, loss 0.0267031, acc 0.984375, prec 0.0671111, recall 0.836959
2017-12-10T04:32:35.068870: step 3483, loss 0.308967, acc 0.96875, prec 0.067109, recall 0.836959
2017-12-10T04:32:35.331009: step 3484, loss 0.308774, acc 1, prec 0.0671242, recall 0.836992
2017-12-10T04:32:35.602023: step 3485, loss 0.288545, acc 0.953125, prec 0.0671209, recall 0.836992
2017-12-10T04:32:35.871627: step 3486, loss 0.1291, acc 0.953125, prec 0.067148, recall 0.837058
2017-12-10T04:32:36.142610: step 3487, loss 0.0315775, acc 0.984375, prec 0.0671621, recall 0.837091
2017-12-10T04:32:36.407490: step 3488, loss 0.176668, acc 0.96875, prec 0.0671903, recall 0.837157
2017-12-10T04:32:37.389725: step 3489, loss 0.0141019, acc 1, prec 0.0672055, recall 0.83719
2017-12-10T04:32:37.753313: step 3490, loss 0.207911, acc 0.953125, prec 0.0672022, recall 0.83719
2017-12-10T04:32:38.221075: step 3491, loss 0.0654801, acc 0.96875, prec 0.0672, recall 0.83719
2017-12-10T04:32:39.129179: step 3492, loss 0.21222, acc 0.953125, prec 0.0671968, recall 0.83719
2017-12-10T04:32:39.455844: step 3493, loss 0.0995876, acc 0.984375, prec 0.0672109, recall 0.837223
2017-12-10T04:32:39.743936: step 3494, loss 0.0461039, acc 0.984375, prec 0.0672402, recall 0.83729
2017-12-10T04:32:40.025468: step 3495, loss 0.0210755, acc 1, prec 0.0672554, recall 0.837323
2017-12-10T04:32:40.313707: step 3496, loss 0.0473086, acc 0.96875, prec 0.0672532, recall 0.837323
2017-12-10T04:32:40.592207: step 3497, loss 0.0393688, acc 0.984375, prec 0.0672673, recall 0.837355
2017-12-10T04:32:40.862691: step 3498, loss 0.181003, acc 0.96875, prec 0.0673107, recall 0.837454
2017-12-10T04:32:41.127770: step 3499, loss 0.026455, acc 1, prec 0.0673107, recall 0.837454
2017-12-10T04:32:41.396599: step 3500, loss 0.0034998, acc 1, prec 0.0673107, recall 0.837454
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-3500

2017-12-10T04:32:42.730603: step 3501, loss 0.0196306, acc 1, prec 0.0673107, recall 0.837454
2017-12-10T04:32:42.999157: step 3502, loss 3.9328, acc 0.96875, prec 0.0673096, recall 0.837285
2017-12-10T04:32:43.271035: step 3503, loss 0.139268, acc 0.984375, prec 0.0673085, recall 0.837285
2017-12-10T04:32:43.536499: step 3504, loss 0.375174, acc 0.953125, prec 0.0673204, recall 0.837318
2017-12-10T04:32:43.801968: step 3505, loss 0.016878, acc 1, prec 0.0673356, recall 0.837351
2017-12-10T04:32:44.066439: step 3506, loss 0.110841, acc 0.9375, prec 0.0673464, recall 0.837384
2017-12-10T04:32:44.332696: step 3507, loss 0.00235771, acc 1, prec 0.0673464, recall 0.837384
2017-12-10T04:32:44.594214: step 3508, loss 0.0422326, acc 0.984375, prec 0.0673605, recall 0.837416
2017-12-10T04:32:44.863241: step 3509, loss 0.168168, acc 0.953125, prec 0.0673572, recall 0.837416
2017-12-10T04:32:45.127439: step 3510, loss 0.091686, acc 0.953125, prec 0.0673539, recall 0.837416
2017-12-10T04:32:45.394078: step 3511, loss 0.089457, acc 0.984375, prec 0.0673528, recall 0.837416
2017-12-10T04:32:45.659442: step 3512, loss 0.188034, acc 0.953125, prec 0.0673799, recall 0.837482
2017-12-10T04:32:45.927772: step 3513, loss 0.048384, acc 0.984375, prec 0.0674091, recall 0.837548
2017-12-10T04:32:46.193792: step 3514, loss 0.0783532, acc 0.96875, prec 0.0674221, recall 0.837581
2017-12-10T04:32:46.457926: step 3515, loss 0.0654608, acc 0.953125, prec 0.0674188, recall 0.837581
2017-12-10T04:32:46.724005: step 3516, loss 0.742583, acc 0.9375, prec 0.06746, recall 0.837679
2017-12-10T04:32:46.990517: step 3517, loss 0.222715, acc 0.9375, prec 0.0674556, recall 0.837679
2017-12-10T04:32:47.256471: step 3518, loss 0.047168, acc 1, prec 0.0674708, recall 0.837712
2017-12-10T04:32:47.525238: step 3519, loss 0.111971, acc 0.9375, prec 0.0674664, recall 0.837712
2017-12-10T04:32:47.792383: step 3520, loss 0.191226, acc 0.953125, prec 0.0674631, recall 0.837712
2017-12-10T04:32:48.054814: step 3521, loss 0.0621703, acc 0.953125, prec 0.067475, recall 0.837745
2017-12-10T04:32:48.316132: step 3522, loss 0.170448, acc 0.984375, prec 0.0675194, recall 0.837843
2017-12-10T04:32:48.582748: step 3523, loss 0.100297, acc 0.96875, prec 0.0675324, recall 0.837876
2017-12-10T04:32:48.841973: step 3524, loss 0.447927, acc 0.96875, prec 0.0675302, recall 0.837876
2017-12-10T04:32:49.113769: step 3525, loss 0.076135, acc 0.984375, prec 0.0675443, recall 0.837909
2017-12-10T04:32:49.380119: step 3526, loss 0.112267, acc 0.96875, prec 0.0675572, recall 0.837941
2017-12-10T04:32:49.646502: step 3527, loss 0.0848476, acc 0.984375, prec 0.0675865, recall 0.838007
2017-12-10T04:32:49.906893: step 3528, loss 0.0436763, acc 0.984375, prec 0.0676309, recall 0.838105
2017-12-10T04:32:50.176714: step 3529, loss 0.230503, acc 0.9375, prec 0.0676568, recall 0.83817
2017-12-10T04:32:50.446693: step 3530, loss 5.21458, acc 0.9375, prec 0.0676687, recall 0.838034
2017-12-10T04:32:50.720887: step 3531, loss 0.199994, acc 0.9375, prec 0.0677098, recall 0.838132
2017-12-10T04:32:50.989669: step 3532, loss 0.0833229, acc 0.9375, prec 0.0677357, recall 0.838197
2017-12-10T04:32:51.257786: step 3533, loss 0.131241, acc 0.921875, prec 0.0677302, recall 0.838197
2017-12-10T04:32:51.523189: step 3534, loss 0.225268, acc 0.9375, prec 0.0677409, recall 0.838229
2017-12-10T04:32:51.788915: step 3535, loss 0.116916, acc 0.9375, prec 0.0677365, recall 0.838229
2017-12-10T04:32:52.061222: step 3536, loss 0.181033, acc 0.953125, prec 0.067809, recall 0.838392
2017-12-10T04:32:52.327797: step 3537, loss 0.482183, acc 0.90625, prec 0.0678175, recall 0.838424
2017-12-10T04:32:52.600431: step 3538, loss 0.266436, acc 0.90625, prec 0.0678109, recall 0.838424
2017-12-10T04:32:52.864748: step 3539, loss 0.269537, acc 0.90625, prec 0.0678346, recall 0.838489
2017-12-10T04:32:53.125352: step 3540, loss 0.110273, acc 0.96875, prec 0.0678476, recall 0.838522
2017-12-10T04:32:53.393529: step 3541, loss 0.117474, acc 0.9375, prec 0.0678734, recall 0.838587
2017-12-10T04:32:53.657479: step 3542, loss 0.306767, acc 0.90625, prec 0.067882, recall 0.838619
2017-12-10T04:32:53.929837: step 3543, loss 0.319269, acc 0.875, prec 0.0679186, recall 0.838716
2017-12-10T04:32:54.194178: step 3544, loss 1.80718, acc 0.890625, prec 0.067912, recall 0.838548
2017-12-10T04:32:54.464281: step 3545, loss 0.130796, acc 0.953125, prec 0.0679238, recall 0.83858
2017-12-10T04:32:54.730493: step 3546, loss 0.239408, acc 0.921875, prec 0.0679183, recall 0.83858
2017-12-10T04:32:54.997093: step 3547, loss 0.719051, acc 0.8125, prec 0.067905, recall 0.83858
2017-12-10T04:32:55.262602: step 3548, loss 0.485621, acc 0.90625, prec 0.0679287, recall 0.838645
2017-12-10T04:32:55.531964: step 3549, loss 0.491247, acc 0.890625, prec 0.0679361, recall 0.838677
2017-12-10T04:32:55.795416: step 3550, loss 0.542735, acc 0.84375, prec 0.0679251, recall 0.838677
2017-12-10T04:32:56.061933: step 3551, loss 0.0839558, acc 0.984375, prec 0.0679391, recall 0.83871
2017-12-10T04:32:56.325429: step 3552, loss 0.415703, acc 0.921875, prec 0.0679336, recall 0.83871
2017-12-10T04:32:56.587524: step 3553, loss 0.183286, acc 0.890625, prec 0.0679259, recall 0.83871
2017-12-10T04:32:56.858565: step 3554, loss 0.411472, acc 0.90625, prec 0.0679344, recall 0.838742
2017-12-10T04:32:57.132570: step 3555, loss 0.248827, acc 0.90625, prec 0.0679429, recall 0.838774
2017-12-10T04:32:57.399439: step 3556, loss 0.243798, acc 0.90625, prec 0.0679514, recall 0.838807
2017-12-10T04:32:57.670506: step 3557, loss 0.492297, acc 0.875, prec 0.0679426, recall 0.838807
2017-12-10T04:32:57.932545: step 3558, loss 1.09161, acc 0.9375, prec 0.0679835, recall 0.838903
2017-12-10T04:32:58.198769: step 3559, loss 0.205255, acc 0.921875, prec 0.0679931, recall 0.838936
2017-12-10T04:32:58.467573: step 3560, loss 0.359307, acc 0.953125, prec 0.06802, recall 0.839
2017-12-10T04:32:58.731287: step 3561, loss 0.117648, acc 0.953125, prec 0.0680318, recall 0.839032
2017-12-10T04:32:58.998347: step 3562, loss 0.139274, acc 0.9375, prec 0.0680274, recall 0.839032
2017-12-10T04:32:59.259345: step 3563, loss 0.214739, acc 0.953125, prec 0.0680392, recall 0.839064
2017-12-10T04:32:59.522874: step 3564, loss 0.0582857, acc 0.96875, prec 0.068037, recall 0.839064
2017-12-10T04:32:59.803116: step 3565, loss 0.180331, acc 0.953125, prec 0.0680488, recall 0.839097
2017-12-10T04:33:00.068763: step 3566, loss 0.0604124, acc 0.984375, prec 0.0680628, recall 0.839129
2017-12-10T04:33:00.342391: step 3567, loss 0.371823, acc 0.890625, prec 0.0680702, recall 0.839161
2017-12-10T04:33:00.613283: step 3568, loss 0.220133, acc 0.953125, prec 0.068082, recall 0.839193
2017-12-10T04:33:00.877920: step 3569, loss 0.102961, acc 0.984375, prec 0.0680809, recall 0.839193
2017-12-10T04:33:01.147801: step 3570, loss 0.0891001, acc 0.96875, prec 0.0681089, recall 0.839257
2017-12-10T04:33:01.412248: step 3571, loss 0.16328, acc 0.953125, prec 0.0681056, recall 0.839257
2017-12-10T04:33:01.677494: step 3572, loss 0.131233, acc 0.96875, prec 0.0681185, recall 0.839289
2017-12-10T04:33:01.938806: step 3573, loss 0.39125, acc 0.9375, prec 0.0681443, recall 0.839353
2017-12-10T04:33:02.200225: step 3574, loss 0.159946, acc 0.953125, prec 0.068156, recall 0.839385
2017-12-10T04:33:02.469615: step 3575, loss 0.242329, acc 0.9375, prec 0.0681667, recall 0.839418
2017-12-10T04:33:02.735284: step 3576, loss 0.0162764, acc 1, prec 0.0681969, recall 0.839482
2017-12-10T04:33:02.996499: step 3577, loss 0.148209, acc 0.953125, prec 0.0682087, recall 0.839514
2017-12-10T04:33:03.256142: step 3578, loss 0.632127, acc 0.96875, prec 0.0682518, recall 0.83961
2017-12-10T04:33:03.531285: step 3579, loss 0.129054, acc 0.984375, prec 0.0682657, recall 0.839641
2017-12-10T04:33:03.799463: step 3580, loss 0.131043, acc 0.96875, prec 0.0682635, recall 0.839641
2017-12-10T04:33:04.067636: step 3581, loss 0.150598, acc 0.953125, prec 0.0682602, recall 0.839641
2017-12-10T04:33:04.332196: step 3582, loss 0.0616634, acc 1, prec 0.0682904, recall 0.839705
2017-12-10T04:33:04.595462: step 3583, loss 0.165867, acc 0.984375, prec 0.0682893, recall 0.839705
2017-12-10T04:33:04.860358: step 3584, loss 0.0700517, acc 0.953125, prec 0.068286, recall 0.839705
2017-12-10T04:33:05.122919: step 3585, loss 0.131274, acc 0.96875, prec 0.0682838, recall 0.839705
2017-12-10T04:33:05.396747: step 3586, loss 0.137476, acc 0.953125, prec 0.0682955, recall 0.839737
2017-12-10T04:33:05.661147: step 3587, loss 0.0200772, acc 1, prec 0.0683106, recall 0.839769
2017-12-10T04:33:05.921440: step 3588, loss 0.0250777, acc 1, prec 0.0683106, recall 0.839769
2017-12-10T04:33:06.185915: step 3589, loss 0.0542007, acc 0.984375, prec 0.0683246, recall 0.839801
2017-12-10T04:33:06.444172: step 3590, loss 0.267145, acc 0.984375, prec 0.0683386, recall 0.839833
2017-12-10T04:33:06.709860: step 3591, loss 0.120958, acc 0.953125, prec 0.0683353, recall 0.839833
2017-12-10T04:33:06.981177: step 3592, loss 0.0975451, acc 0.953125, prec 0.0683319, recall 0.839833
2017-12-10T04:33:07.250027: step 3593, loss 1.74263, acc 0.953125, prec 0.0683448, recall 0.839698
2017-12-10T04:33:07.520071: step 3594, loss 0.085773, acc 0.953125, prec 0.0683566, recall 0.83973
2017-12-10T04:33:07.788973: step 3595, loss 0.00992595, acc 1, prec 0.0683566, recall 0.83973
2017-12-10T04:33:08.051405: step 3596, loss 0.15534, acc 0.96875, prec 0.0683543, recall 0.83973
2017-12-10T04:33:08.318579: step 3597, loss 0.169737, acc 0.9375, prec 0.0683499, recall 0.83973
2017-12-10T04:33:08.581469: step 3598, loss 0.0112454, acc 1, prec 0.0683499, recall 0.83973
2017-12-10T04:33:08.849890: step 3599, loss 0.271054, acc 0.9375, prec 0.0683455, recall 0.83973
2017-12-10T04:33:09.114721: step 3600, loss 0.0689381, acc 0.96875, prec 0.0683584, recall 0.839761

Evaluation:
2017-12-10T04:33:16.786318: step 3600, loss 2.56692, acc 0.939234, prec 0.068949, recall 0.831595

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-3600

2017-12-10T04:33:18.033183: step 3601, loss 0.192223, acc 0.953125, prec 0.0689904, recall 0.831693
2017-12-10T04:33:18.300872: step 3602, loss 0.240206, acc 0.96875, prec 0.0689882, recall 0.831693
2017-12-10T04:33:18.566734: step 3603, loss 0.0891175, acc 0.96875, prec 0.0690008, recall 0.831725
2017-12-10T04:33:18.836305: step 3604, loss 0.544089, acc 0.90625, prec 0.0689942, recall 0.831725
2017-12-10T04:33:19.098714: step 3605, loss 0.251041, acc 0.890625, prec 0.0690014, recall 0.831758
2017-12-10T04:33:19.359797: step 3606, loss 0.184843, acc 0.96875, prec 0.069029, recall 0.831823
2017-12-10T04:33:19.618179: step 3607, loss 0.115229, acc 0.96875, prec 0.0690417, recall 0.831855
2017-12-10T04:33:19.888229: step 3608, loss 0.339625, acc 0.9375, prec 0.0690522, recall 0.831887
2017-12-10T04:33:20.148458: step 3609, loss 0.131619, acc 0.953125, prec 0.0690488, recall 0.831887
2017-12-10T04:33:20.409793: step 3610, loss 0.460441, acc 0.921875, prec 0.0690582, recall 0.83192
2017-12-10T04:33:20.686887: step 3611, loss 0.586947, acc 0.90625, prec 0.0690665, recall 0.831952
2017-12-10T04:33:20.954595: step 3612, loss 0.131217, acc 0.9375, prec 0.0690621, recall 0.831952
2017-12-10T04:33:21.223659: step 3613, loss 0.262137, acc 0.921875, prec 0.0690714, recall 0.831985
2017-12-10T04:33:21.489823: step 3614, loss 0.158353, acc 0.9375, prec 0.0690819, recall 0.832017
2017-12-10T04:33:21.762853: step 3615, loss 0.0328466, acc 0.984375, prec 0.0690957, recall 0.832049
2017-12-10T04:33:22.029596: step 3616, loss 0.149633, acc 0.96875, prec 0.0691232, recall 0.832114
2017-12-10T04:33:22.291630: step 3617, loss 0.349068, acc 1, prec 0.069153, recall 0.832179
2017-12-10T04:33:22.558746: step 3618, loss 0.053265, acc 0.984375, prec 0.0691519, recall 0.832179
2017-12-10T04:33:22.824708: step 3619, loss 0.14866, acc 0.96875, prec 0.0692092, recall 0.832308
2017-12-10T04:33:23.081181: step 3620, loss 0.00739259, acc 1, prec 0.0692092, recall 0.832308
2017-12-10T04:33:23.343400: step 3621, loss 0.00521132, acc 1, prec 0.0692092, recall 0.832308
2017-12-10T04:33:23.608499: step 3622, loss 0.115883, acc 0.96875, prec 0.069207, recall 0.832308
2017-12-10T04:33:23.878521: step 3623, loss 0.0166328, acc 1, prec 0.0692219, recall 0.83234
2017-12-10T04:33:24.142291: step 3624, loss 0.118012, acc 0.953125, prec 0.0692186, recall 0.83234
2017-12-10T04:33:24.408454: step 3625, loss 0.00710393, acc 1, prec 0.0692186, recall 0.83234
2017-12-10T04:33:24.670661: step 3626, loss 0.110142, acc 0.953125, prec 0.0692302, recall 0.832372
2017-12-10T04:33:24.935967: step 3627, loss 0.190246, acc 0.96875, prec 0.0692428, recall 0.832404
2017-12-10T04:33:25.204461: step 3628, loss 0.154678, acc 0.96875, prec 0.0692555, recall 0.832437
2017-12-10T04:33:25.466318: step 3629, loss 0.127502, acc 0.984375, prec 0.0692693, recall 0.832469
2017-12-10T04:33:25.732749: step 3630, loss 0.210024, acc 1, prec 0.0693139, recall 0.832565
2017-12-10T04:33:26.003252: step 3631, loss 0.0476081, acc 0.984375, prec 0.0693128, recall 0.832565
2017-12-10T04:33:26.269262: step 3632, loss 0.0113523, acc 1, prec 0.0693277, recall 0.832597
2017-12-10T04:33:26.529374: step 3633, loss 0.0891116, acc 0.96875, prec 0.0693254, recall 0.832597
2017-12-10T04:33:26.793898: step 3634, loss 0.209018, acc 0.96875, prec 0.069353, recall 0.832662
2017-12-10T04:33:27.065269: step 3635, loss 0.0259736, acc 1, prec 0.069353, recall 0.832662
2017-12-10T04:33:27.328855: step 3636, loss 0.158488, acc 0.953125, prec 0.0693497, recall 0.832662
2017-12-10T04:33:27.592725: step 3637, loss 0.292623, acc 0.921875, prec 0.069359, recall 0.832694
2017-12-10T04:33:27.864131: step 3638, loss 0.131338, acc 0.96875, prec 0.0693568, recall 0.832694
2017-12-10T04:33:28.133476: step 3639, loss 0.115504, acc 0.984375, prec 0.0693705, recall 0.832726
2017-12-10T04:33:28.398234: step 3640, loss 0.220274, acc 0.96875, prec 0.0693832, recall 0.832758
2017-12-10T04:33:28.662357: step 3641, loss 0.168558, acc 0.96875, prec 0.0694256, recall 0.832854
2017-12-10T04:33:28.943549: step 3642, loss 0.197031, acc 0.9375, prec 0.0694509, recall 0.832918
2017-12-10T04:33:29.206575: step 3643, loss 0.0664539, acc 0.984375, prec 0.0694646, recall 0.83295
2017-12-10T04:33:29.470056: step 3644, loss 0.0687715, acc 0.96875, prec 0.0694773, recall 0.832982
2017-12-10T04:33:29.740038: step 3645, loss 0.010512, acc 1, prec 0.0694921, recall 0.833014
2017-12-10T04:33:30.001648: step 3646, loss 1.03687, acc 0.96875, prec 0.0695048, recall 0.833046
2017-12-10T04:33:30.282343: step 3647, loss 0.224357, acc 0.953125, prec 0.0695163, recall 0.833078
2017-12-10T04:33:30.555140: step 3648, loss 0.0310263, acc 0.984375, prec 0.0695152, recall 0.833078
2017-12-10T04:33:30.830729: step 3649, loss 0.0880915, acc 0.96875, prec 0.069513, recall 0.833078
2017-12-10T04:33:31.093026: step 3650, loss 0.0130819, acc 1, prec 0.069513, recall 0.833078
2017-12-10T04:33:31.356251: step 3651, loss 0.0518411, acc 0.984375, prec 0.0695119, recall 0.833078
2017-12-10T04:33:31.626486: step 3652, loss 0.0597371, acc 0.96875, prec 0.0695245, recall 0.83311
2017-12-10T04:33:31.892537: step 3653, loss 0.0883882, acc 0.96875, prec 0.0695223, recall 0.83311
2017-12-10T04:33:32.158838: step 3654, loss 0.565017, acc 0.890625, prec 0.0695145, recall 0.83311
2017-12-10T04:33:32.433582: step 3655, loss 0.104745, acc 0.96875, prec 0.0695123, recall 0.83311
2017-12-10T04:33:32.707190: step 3656, loss 0.445188, acc 0.953125, prec 0.0695387, recall 0.833174
2017-12-10T04:33:32.976552: step 3657, loss 0.0854512, acc 0.953125, prec 0.0695354, recall 0.833174
2017-12-10T04:33:33.237785: step 3658, loss 0.381096, acc 0.96875, prec 0.0695629, recall 0.833238
2017-12-10T04:33:33.513572: step 3659, loss 0.113365, acc 0.96875, prec 0.0695606, recall 0.833238
2017-12-10T04:33:33.784027: step 3660, loss 0.103517, acc 0.984375, prec 0.0695744, recall 0.83327
2017-12-10T04:33:34.051028: step 3661, loss 0.446322, acc 0.921875, prec 0.0695837, recall 0.833301
2017-12-10T04:33:34.319884: step 3662, loss 0.163511, acc 0.96875, prec 0.0695963, recall 0.833333
2017-12-10T04:33:34.582984: step 3663, loss 0.265605, acc 0.9375, prec 0.0696364, recall 0.833429
2017-12-10T04:33:34.848209: step 3664, loss 0.195794, acc 0.953125, prec 0.0696479, recall 0.833461
2017-12-10T04:33:35.112811: step 3665, loss 0.0710714, acc 0.96875, prec 0.0696457, recall 0.833461
2017-12-10T04:33:35.375687: step 3666, loss 0.0629776, acc 0.96875, prec 0.0696583, recall 0.833492
2017-12-10T04:33:35.640986: step 3667, loss 0.0788721, acc 0.953125, prec 0.0696698, recall 0.833524
2017-12-10T04:33:35.901153: step 3668, loss 0.350368, acc 0.9375, prec 0.0696951, recall 0.833588
2017-12-10T04:33:36.176480: step 3669, loss 0.115536, acc 0.984375, prec 0.0697088, recall 0.83362
2017-12-10T04:33:36.447615: step 3670, loss 0.216693, acc 0.890625, prec 0.069701, recall 0.83362
2017-12-10T04:33:36.715569: step 3671, loss 0.186901, acc 0.96875, prec 0.0697285, recall 0.833683
2017-12-10T04:33:36.979035: step 3672, loss 0.0712218, acc 0.96875, prec 0.0697263, recall 0.833683
2017-12-10T04:33:37.239715: step 3673, loss 0.357894, acc 0.921875, prec 0.0697504, recall 0.833746
2017-12-10T04:33:37.502338: step 3674, loss 0.173887, acc 0.953125, prec 0.069747, recall 0.833746
2017-12-10T04:33:37.780465: step 3675, loss 0.207619, acc 0.921875, prec 0.0697415, recall 0.833746
2017-12-10T04:33:38.049484: step 3676, loss 0.0930536, acc 0.96875, prec 0.0697689, recall 0.83381
2017-12-10T04:33:38.312898: step 3677, loss 0.116799, acc 0.9375, prec 0.0697645, recall 0.83381
2017-12-10T04:33:38.576815: step 3678, loss 0.00490874, acc 1, prec 0.0697645, recall 0.83381
2017-12-10T04:33:38.846173: step 3679, loss 0.113983, acc 0.96875, prec 0.0697623, recall 0.83381
2017-12-10T04:33:39.122579: step 3680, loss 0.00203501, acc 1, prec 0.0697623, recall 0.83381
2017-12-10T04:33:39.382417: step 3681, loss 4.77936, acc 0.953125, prec 0.06976, recall 0.833651
2017-12-10T04:33:39.647251: step 3682, loss 0.0581952, acc 0.96875, prec 0.0697578, recall 0.833651
2017-12-10T04:33:39.912372: step 3683, loss 5.75996, acc 0.953125, prec 0.0697852, recall 0.833556
2017-12-10T04:33:40.176736: step 3684, loss 0.135127, acc 0.96875, prec 0.069783, recall 0.833556
2017-12-10T04:33:40.443894: step 3685, loss 0.53857, acc 0.875, prec 0.0697741, recall 0.833556
2017-12-10T04:33:40.714283: step 3686, loss 0.60231, acc 0.765625, prec 0.0697723, recall 0.833587
2017-12-10T04:33:40.978338: step 3687, loss 0.842352, acc 0.765625, prec 0.0697556, recall 0.833587
2017-12-10T04:33:41.248316: step 3688, loss 0.970904, acc 0.75, prec 0.0697378, recall 0.833587
2017-12-10T04:33:41.509004: step 3689, loss 0.888341, acc 0.75, prec 0.0697497, recall 0.833651
2017-12-10T04:33:41.771031: step 3690, loss 1.56342, acc 0.75, prec 0.0697467, recall 0.833682
2017-12-10T04:33:42.038424: step 3691, loss 1.68741, acc 0.640625, prec 0.0697212, recall 0.833682
2017-12-10T04:33:42.298341: step 3692, loss 0.846603, acc 0.78125, prec 0.0697204, recall 0.833714
2017-12-10T04:33:42.554189: step 3693, loss 1.37611, acc 0.609375, prec 0.0697223, recall 0.833777
2017-12-10T04:33:42.814221: step 3694, loss 1.7366, acc 0.734375, prec 0.0697331, recall 0.83384
2017-12-10T04:33:43.079659: step 3695, loss 1.01725, acc 0.78125, prec 0.0697175, recall 0.83384
2017-12-10T04:33:43.348515: step 3696, loss 0.883655, acc 0.734375, prec 0.0697135, recall 0.833872
2017-12-10T04:33:43.611961: step 3697, loss 0.981097, acc 0.765625, prec 0.0697117, recall 0.833903
2017-12-10T04:33:43.874606: step 3698, loss 0.887002, acc 0.875, prec 0.0697028, recall 0.833903
2017-12-10T04:33:44.145791: step 3699, loss 0.317306, acc 0.921875, prec 0.069712, recall 0.833935
2017-12-10T04:33:44.412672: step 3700, loss 0.325477, acc 0.96875, prec 0.0697246, recall 0.833967
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-3700

2017-12-10T04:33:45.613735: step 3701, loss 0.774476, acc 0.890625, prec 0.0697168, recall 0.833967
2017-12-10T04:33:45.875155: step 3702, loss 0.0646286, acc 0.96875, prec 0.0697146, recall 0.833967
2017-12-10T04:33:46.136278: step 3703, loss 0.308465, acc 0.90625, prec 0.069708, recall 0.833967
2017-12-10T04:33:46.403852: step 3704, loss 0.185755, acc 0.9375, prec 0.0697183, recall 0.833998
2017-12-10T04:33:46.672754: step 3705, loss 0.128079, acc 0.953125, prec 0.069715, recall 0.833998
2017-12-10T04:33:46.937074: step 3706, loss 0.716513, acc 0.9375, prec 0.0697401, recall 0.834061
2017-12-10T04:33:47.205601: step 3707, loss 1.78015, acc 0.953125, prec 0.0697674, recall 0.833966
2017-12-10T04:33:47.469521: step 3708, loss 2.33606, acc 0.921875, prec 0.069763, recall 0.833808
2017-12-10T04:33:47.737386: step 3709, loss 0.31077, acc 0.9375, prec 0.0697586, recall 0.833808
2017-12-10T04:33:48.003015: step 3710, loss 0.358476, acc 0.9375, prec 0.0697689, recall 0.833839
2017-12-10T04:33:48.272176: step 3711, loss 0.170158, acc 0.96875, prec 0.0697815, recall 0.833871
2017-12-10T04:33:48.536304: step 3712, loss 0.265519, acc 0.9375, prec 0.0697918, recall 0.833902
2017-12-10T04:33:48.799267: step 3713, loss 0.394868, acc 0.90625, prec 0.0697852, recall 0.833902
2017-12-10T04:33:49.060281: step 3714, loss 0.329786, acc 0.90625, prec 0.0697933, recall 0.833934
2017-12-10T04:33:49.335302: step 3715, loss 0.35644, acc 0.890625, prec 0.0697855, recall 0.833934
2017-12-10T04:33:49.599741: step 3716, loss 0.248576, acc 0.890625, prec 0.0697778, recall 0.833934
2017-12-10T04:33:49.863121: step 3717, loss 0.313599, acc 0.921875, prec 0.069787, recall 0.833965
2017-12-10T04:33:50.129927: step 3718, loss 0.399838, acc 0.90625, prec 0.0697803, recall 0.833965
2017-12-10T04:33:50.398958: step 3719, loss 0.441501, acc 0.890625, prec 0.0698021, recall 0.834028
2017-12-10T04:33:50.671810: step 3720, loss 0.455733, acc 0.875, prec 0.0698228, recall 0.834091
2017-12-10T04:33:50.938080: step 3721, loss 0.535403, acc 0.875, prec 0.0698286, recall 0.834122
2017-12-10T04:33:51.205425: step 3722, loss 0.300216, acc 0.90625, prec 0.069881, recall 0.834248
2017-12-10T04:33:51.469427: step 3723, loss 0.0493823, acc 0.984375, prec 0.0699241, recall 0.834342
2017-12-10T04:33:51.739845: step 3724, loss 0.376795, acc 0.921875, prec 0.0699333, recall 0.834373
2017-12-10T04:33:52.000853: step 3725, loss 0.12941, acc 0.96875, prec 0.0699605, recall 0.834436
2017-12-10T04:33:52.274620: step 3726, loss 0.114035, acc 0.953125, prec 0.069972, recall 0.834467
2017-12-10T04:33:52.548901: step 3727, loss 0.241763, acc 0.9375, prec 0.0699823, recall 0.834498
2017-12-10T04:33:52.817758: step 3728, loss 0.342025, acc 0.921875, prec 0.0700062, recall 0.834561
2017-12-10T04:33:53.084706: step 3729, loss 0.462142, acc 0.9375, prec 0.0700312, recall 0.834623
2017-12-10T04:33:53.350314: step 3730, loss 0.329371, acc 0.921875, prec 0.0700257, recall 0.834623
2017-12-10T04:33:53.613066: step 3731, loss 0.252863, acc 0.9375, prec 0.0700359, recall 0.834655
2017-12-10T04:33:53.887185: step 3732, loss 0.0781632, acc 0.984375, prec 0.0700348, recall 0.834655
2017-12-10T04:33:54.158479: step 3733, loss 0.281351, acc 0.984375, prec 0.0700632, recall 0.834717
2017-12-10T04:33:54.426338: step 3734, loss 0.249872, acc 0.9375, prec 0.0700882, recall 0.834779
2017-12-10T04:33:54.694733: step 3735, loss 0.185086, acc 0.9375, prec 0.0700838, recall 0.834779
2017-12-10T04:33:54.959306: step 3736, loss 0.0719004, acc 0.984375, prec 0.0700974, recall 0.83481
2017-12-10T04:33:55.227657: step 3737, loss 0.0319501, acc 1, prec 0.0701121, recall 0.834842
2017-12-10T04:33:55.492997: step 3738, loss 0.155653, acc 0.984375, prec 0.0701404, recall 0.834904
2017-12-10T04:33:55.759165: step 3739, loss 0.269276, acc 0.9375, prec 0.0701507, recall 0.834935
2017-12-10T04:33:56.033116: step 3740, loss 0.183718, acc 0.9375, prec 0.0701463, recall 0.834935
2017-12-10T04:33:56.308989: step 3741, loss 0.184332, acc 0.953125, prec 0.0701429, recall 0.834935
2017-12-10T04:33:56.578352: step 3742, loss 0.797265, acc 1, prec 0.0701577, recall 0.834966
2017-12-10T04:33:56.850805: step 3743, loss 0.0527659, acc 0.96875, prec 0.0701554, recall 0.834966
2017-12-10T04:33:57.133595: step 3744, loss 4.7164, acc 0.984375, prec 0.0701849, recall 0.834871
2017-12-10T04:33:57.404349: step 3745, loss 0.386431, acc 0.96875, prec 0.0701827, recall 0.834871
2017-12-10T04:33:57.667561: step 3746, loss 3.4212, acc 0.9375, prec 0.0702088, recall 0.834776
2017-12-10T04:33:57.936787: step 3747, loss 0.29337, acc 0.90625, prec 0.0702021, recall 0.834776
2017-12-10T04:33:58.202774: step 3748, loss 0.505694, acc 0.890625, prec 0.0701943, recall 0.834776
2017-12-10T04:33:58.470756: step 3749, loss 0.222968, acc 0.90625, prec 0.0702024, recall 0.834807
2017-12-10T04:33:58.740319: step 3750, loss 0.897026, acc 0.796875, prec 0.0701879, recall 0.834807
2017-12-10T04:33:59.000474: step 3751, loss 0.732477, acc 0.84375, prec 0.0701768, recall 0.834807
2017-12-10T04:33:59.262185: step 3752, loss 0.791272, acc 0.8125, prec 0.0701782, recall 0.834838
2017-12-10T04:33:59.530506: step 3753, loss 0.539586, acc 0.765625, prec 0.0701763, recall 0.834869
2017-12-10T04:33:59.798964: step 3754, loss 0.769929, acc 0.796875, prec 0.0701912, recall 0.834931
2017-12-10T04:34:00.072802: step 3755, loss 0.94362, acc 0.796875, prec 0.0701915, recall 0.834962
2017-12-10T04:34:00.344473: step 3756, loss 1.67325, acc 0.734375, prec 0.0701727, recall 0.834962
2017-12-10T04:34:00.611210: step 3757, loss 0.812897, acc 0.8125, prec 0.0701594, recall 0.834962
2017-12-10T04:34:00.875808: step 3758, loss 1.40316, acc 0.75, prec 0.0701416, recall 0.834962
2017-12-10T04:34:01.137149: step 3759, loss 1.13537, acc 0.78125, prec 0.0701702, recall 0.835055
2017-12-10T04:34:01.398769: step 3760, loss 0.950228, acc 0.6875, prec 0.070148, recall 0.835055
2017-12-10T04:34:01.667135: step 3761, loss 0.914631, acc 0.78125, prec 0.0701325, recall 0.835055
2017-12-10T04:34:01.934722: step 3762, loss 0.63843, acc 0.828125, prec 0.070135, recall 0.835086
2017-12-10T04:34:02.197654: step 3763, loss 1.05712, acc 0.828125, prec 0.0701375, recall 0.835117
2017-12-10T04:34:02.469912: step 3764, loss 0.718481, acc 0.90625, prec 0.0701456, recall 0.835148
2017-12-10T04:34:02.737789: step 3765, loss 0.470737, acc 0.921875, prec 0.07014, recall 0.835148
2017-12-10T04:34:03.002204: step 3766, loss 0.331956, acc 0.90625, prec 0.0701774, recall 0.835241
2017-12-10T04:34:03.265195: step 3767, loss 0.273892, acc 0.921875, prec 0.0702012, recall 0.835303
2017-12-10T04:34:03.531189: step 3768, loss 0.431312, acc 0.921875, prec 0.0702103, recall 0.835334
2017-12-10T04:34:03.802824: step 3769, loss 0.263864, acc 0.953125, prec 0.0702509, recall 0.835426
2017-12-10T04:34:04.068281: step 3770, loss 0.345138, acc 0.9375, prec 0.0702465, recall 0.835426
2017-12-10T04:34:04.333356: step 3771, loss 0.101755, acc 0.953125, prec 0.0702578, recall 0.835457
2017-12-10T04:34:04.605011: step 3772, loss 3.01353, acc 0.984375, prec 0.0702578, recall 0.835301
2017-12-10T04:34:04.870109: step 3773, loss 0.0721012, acc 0.96875, prec 0.0702703, recall 0.835332
2017-12-10T04:34:05.129339: step 3774, loss 0.596155, acc 0.96875, prec 0.0702827, recall 0.835362
2017-12-10T04:34:05.397187: step 3775, loss 0.210641, acc 0.921875, prec 0.0702918, recall 0.835393
2017-12-10T04:34:05.659715: step 3776, loss 2.72093, acc 0.953125, prec 0.0702896, recall 0.835237
2017-12-10T04:34:05.929521: step 3777, loss 0.13352, acc 0.984375, prec 0.0703031, recall 0.835268
2017-12-10T04:34:06.191419: step 3778, loss 0.265431, acc 0.96875, prec 0.0703156, recall 0.835299
2017-12-10T04:34:06.456123: step 3779, loss 0.206354, acc 0.9375, prec 0.0703111, recall 0.835299
2017-12-10T04:34:06.720292: step 3780, loss 0.194758, acc 0.96875, prec 0.0703089, recall 0.835299
2017-12-10T04:34:06.984682: step 3781, loss 0.446485, acc 0.9375, prec 0.0703484, recall 0.835391
2017-12-10T04:34:07.245388: step 3782, loss 0.142258, acc 0.953125, prec 0.0703744, recall 0.835452
2017-12-10T04:34:07.512138: step 3783, loss 0.46269, acc 0.84375, prec 0.0703633, recall 0.835452
2017-12-10T04:34:07.784389: step 3784, loss 0.556637, acc 0.859375, prec 0.070368, recall 0.835483
2017-12-10T04:34:08.042104: step 3785, loss 0.781974, acc 0.875, prec 0.0703884, recall 0.835545
2017-12-10T04:34:08.312369: step 3786, loss 0.287785, acc 0.90625, prec 0.0703817, recall 0.835545
2017-12-10T04:34:08.575418: step 3787, loss 0.31057, acc 0.890625, prec 0.0704032, recall 0.835606
2017-12-10T04:34:08.839015: step 3788, loss 0.190679, acc 0.9375, prec 0.0703988, recall 0.835606
2017-12-10T04:34:09.106013: step 3789, loss 0.284935, acc 0.9375, prec 0.070409, recall 0.835637
2017-12-10T04:34:09.375625: step 3790, loss 0.555528, acc 0.859375, prec 0.0704137, recall 0.835668
2017-12-10T04:34:09.640301: step 3791, loss 0.624997, acc 0.875, prec 0.0704048, recall 0.835668
2017-12-10T04:34:09.898384: step 3792, loss 0.218509, acc 0.90625, prec 0.0704274, recall 0.835729
2017-12-10T04:34:10.165086: step 3793, loss 0.299272, acc 0.921875, prec 0.0704365, recall 0.83576
2017-12-10T04:34:10.424447: step 3794, loss 0.693951, acc 0.875, prec 0.0704423, recall 0.83579
2017-12-10T04:34:10.689122: step 3795, loss 0.0745016, acc 0.953125, prec 0.0704389, recall 0.83579
2017-12-10T04:34:10.953541: step 3796, loss 0.161979, acc 0.9375, prec 0.0704345, recall 0.83579
2017-12-10T04:34:11.219444: step 3797, loss 0.213481, acc 0.9375, prec 0.0704593, recall 0.835851
2017-12-10T04:34:11.483438: step 3798, loss 0.109103, acc 0.96875, prec 0.0704863, recall 0.835913
2017-12-10T04:34:11.756049: step 3799, loss 0.389658, acc 0.890625, prec 0.0704932, recall 0.835943
2017-12-10T04:34:12.027962: step 3800, loss 0.0555581, acc 0.984375, prec 0.0705067, recall 0.835974
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-3800

2017-12-10T04:34:13.285685: step 3801, loss 0.23259, acc 0.96875, prec 0.0705337, recall 0.836035
2017-12-10T04:34:13.554749: step 3802, loss 0.0263104, acc 0.984375, prec 0.0705326, recall 0.836035
2017-12-10T04:34:13.822959: step 3803, loss 0.221021, acc 0.96875, prec 0.0705596, recall 0.836096
2017-12-10T04:34:14.086339: step 3804, loss 0.190922, acc 0.953125, prec 0.0705562, recall 0.836096
2017-12-10T04:34:15.054470: step 3805, loss 0.0587447, acc 0.984375, prec 0.0705551, recall 0.836096
2017-12-10T04:34:15.412836: step 3806, loss 2.15776, acc 0.984375, prec 0.0705551, recall 0.83594
2017-12-10T04:34:15.683509: step 3807, loss 0.0109016, acc 1, prec 0.0705551, recall 0.83594
2017-12-10T04:34:16.380028: step 3808, loss 2.48818, acc 0.953125, prec 0.0705675, recall 0.835815
2017-12-10T04:34:17.139675: step 3809, loss 0.171958, acc 0.921875, prec 0.070562, recall 0.835815
2017-12-10T04:34:17.869478: step 3810, loss 0.119797, acc 0.96875, prec 0.0705598, recall 0.835815
2017-12-10T04:34:18.577792: step 3811, loss 0.492262, acc 0.859375, prec 0.0705644, recall 0.835846
2017-12-10T04:34:19.371352: step 3812, loss 0.346013, acc 0.890625, prec 0.0705712, recall 0.835876
2017-12-10T04:34:19.873231: step 3813, loss 0.527619, acc 0.84375, prec 0.0706039, recall 0.835968
2017-12-10T04:34:20.140683: step 3814, loss 0.891614, acc 0.84375, prec 0.0706075, recall 0.835999
2017-12-10T04:34:20.408395: step 3815, loss 0.770119, acc 0.828125, prec 0.0706244, recall 0.836059
2017-12-10T04:34:20.681294: step 3816, loss 0.414716, acc 0.890625, prec 0.0706313, recall 0.83609
2017-12-10T04:34:20.945224: step 3817, loss 0.526612, acc 0.890625, prec 0.0706381, recall 0.83612
2017-12-10T04:34:21.205562: step 3818, loss 0.444291, acc 0.859375, prec 0.0706427, recall 0.836151
2017-12-10T04:34:21.470356: step 3819, loss 0.413102, acc 0.90625, prec 0.0706506, recall 0.836181
2017-12-10T04:34:21.734956: step 3820, loss 0.704032, acc 0.828125, prec 0.0706384, recall 0.836181
2017-12-10T04:34:22.001875: step 3821, loss 0.2196, acc 0.90625, prec 0.0706318, recall 0.836181
2017-12-10T04:34:22.269620: step 3822, loss 0.68593, acc 0.859375, prec 0.0706364, recall 0.836212
2017-12-10T04:34:22.535631: step 3823, loss 0.312803, acc 0.90625, prec 0.0706589, recall 0.836272
2017-12-10T04:34:22.809339: step 3824, loss 0.664465, acc 0.84375, prec 0.0706624, recall 0.836303
2017-12-10T04:34:23.074339: step 3825, loss 0.137857, acc 0.953125, prec 0.0706591, recall 0.836303
2017-12-10T04:34:23.338793: step 3826, loss 0.175643, acc 0.984375, prec 0.0706871, recall 0.836364
2017-12-10T04:34:23.612068: step 3827, loss 1.13188, acc 0.953125, prec 0.0707129, recall 0.836424
2017-12-10T04:34:23.881663: step 3828, loss 0.383061, acc 0.921875, prec 0.0707074, recall 0.836424
2017-12-10T04:34:24.140626: step 3829, loss 0.106812, acc 0.953125, prec 0.0707332, recall 0.836485
2017-12-10T04:34:24.404145: step 3830, loss 0.182888, acc 0.90625, prec 0.0707557, recall 0.836546
2017-12-10T04:34:24.667334: step 3831, loss 0.182808, acc 0.9375, prec 0.0707658, recall 0.836576
2017-12-10T04:34:24.933493: step 3832, loss 0.114623, acc 0.953125, prec 0.0707625, recall 0.836576
2017-12-10T04:34:25.206791: step 3833, loss 0.18855, acc 0.953125, prec 0.0707592, recall 0.836576
2017-12-10T04:34:25.473794: step 3834, loss 0.361499, acc 0.984375, prec 0.0707726, recall 0.836606
2017-12-10T04:34:25.739730: step 3835, loss 0.0768098, acc 0.953125, prec 0.0707838, recall 0.836636
2017-12-10T04:34:26.016234: step 3836, loss 0.0294073, acc 0.984375, prec 0.0707827, recall 0.836636
2017-12-10T04:34:26.282719: step 3837, loss 0.105065, acc 0.953125, prec 0.070794, recall 0.836667
2017-12-10T04:34:26.548827: step 3838, loss 0.463103, acc 0.9375, prec 0.0708041, recall 0.836697
2017-12-10T04:34:26.816259: step 3839, loss 0.0435568, acc 0.984375, prec 0.070803, recall 0.836697
2017-12-10T04:34:27.094969: step 3840, loss 2.0948, acc 0.90625, prec 0.0707974, recall 0.836542
2017-12-10T04:34:27.359291: step 3841, loss 0.122391, acc 0.953125, prec 0.0708087, recall 0.836572
2017-12-10T04:34:27.628962: step 3842, loss 0.268554, acc 0.9375, prec 0.0708042, recall 0.836572
2017-12-10T04:34:27.893821: step 3843, loss 0.162036, acc 0.96875, prec 0.070802, recall 0.836572
2017-12-10T04:34:28.166659: step 3844, loss 0.241504, acc 0.90625, prec 0.0708099, recall 0.836603
2017-12-10T04:34:28.431203: step 3845, loss 0.219375, acc 0.921875, prec 0.0708044, recall 0.836603
2017-12-10T04:34:28.696398: step 3846, loss 0.255405, acc 0.96875, prec 0.0708167, recall 0.836633
2017-12-10T04:34:28.962474: step 3847, loss 0.487844, acc 0.84375, prec 0.0708202, recall 0.836663
2017-12-10T04:34:29.226639: step 3848, loss 0.461532, acc 0.90625, prec 0.0708135, recall 0.836663
2017-12-10T04:34:29.488484: step 3849, loss 0.145556, acc 0.9375, prec 0.0708091, recall 0.836663
2017-12-10T04:34:29.754813: step 3850, loss 0.36906, acc 0.90625, prec 0.0708315, recall 0.836723
2017-12-10T04:34:30.015811: step 3851, loss 0.608044, acc 0.828125, prec 0.0708339, recall 0.836754
2017-12-10T04:34:30.286035: step 3852, loss 0.433686, acc 0.90625, prec 0.0708563, recall 0.836814
2017-12-10T04:34:30.558604: step 3853, loss 0.228315, acc 0.96875, prec 0.0708686, recall 0.836844
2017-12-10T04:34:30.822829: step 3854, loss 0.174435, acc 0.9375, prec 0.0708642, recall 0.836844
2017-12-10T04:34:31.086885: step 3855, loss 0.0921127, acc 0.9375, prec 0.0708597, recall 0.836844
2017-12-10T04:34:31.348874: step 3856, loss 0.157184, acc 0.953125, prec 0.0708564, recall 0.836844
2017-12-10T04:34:31.612011: step 3857, loss 0.145038, acc 0.96875, prec 0.0709123, recall 0.836965
2017-12-10T04:34:31.882602: step 3858, loss 0.400895, acc 0.859375, prec 0.0709023, recall 0.836965
2017-12-10T04:34:32.146553: step 3859, loss 0.145175, acc 0.9375, prec 0.0709124, recall 0.836995
2017-12-10T04:34:32.411532: step 3860, loss 0.13281, acc 0.96875, prec 0.0709683, recall 0.837115
2017-12-10T04:34:32.674520: step 3861, loss 0.23199, acc 0.9375, prec 0.0709784, recall 0.837145
2017-12-10T04:34:32.942020: step 3862, loss 0.0530085, acc 0.96875, prec 0.0709762, recall 0.837145
2017-12-10T04:34:33.205619: step 3863, loss 7.97572, acc 0.90625, prec 0.0709718, recall 0.836836
2017-12-10T04:34:33.471697: step 3864, loss 0.477456, acc 0.96875, prec 0.0710131, recall 0.836926
2017-12-10T04:34:33.737326: step 3865, loss 0.335443, acc 0.9375, prec 0.0710232, recall 0.836957
2017-12-10T04:34:34.010675: step 3866, loss 0.421509, acc 0.890625, prec 0.07103, recall 0.836987
2017-12-10T04:34:34.276854: step 3867, loss 0.239152, acc 0.90625, prec 0.0710233, recall 0.836987
2017-12-10T04:34:34.535398: step 3868, loss 0.736476, acc 0.890625, prec 0.0710446, recall 0.837047
2017-12-10T04:34:34.799554: step 3869, loss 0.547481, acc 0.765625, prec 0.0710424, recall 0.837077
2017-12-10T04:34:35.064133: step 3870, loss 1.29597, acc 0.625, prec 0.0710303, recall 0.837107
2017-12-10T04:34:35.325094: step 3871, loss 0.679967, acc 0.765625, prec 0.0710282, recall 0.837137
2017-12-10T04:34:35.588870: step 3872, loss 0.894306, acc 0.6875, prec 0.0710495, recall 0.837226
2017-12-10T04:34:35.853090: step 3873, loss 0.965042, acc 0.765625, prec 0.0710474, recall 0.837256
2017-12-10T04:34:36.114409: step 3874, loss 0.415007, acc 0.828125, prec 0.0710497, recall 0.837286
2017-12-10T04:34:36.378621: step 3875, loss 0.585229, acc 0.859375, prec 0.0710542, recall 0.837316
2017-12-10T04:34:36.639312: step 3876, loss 0.56623, acc 0.84375, prec 0.0710721, recall 0.837376
2017-12-10T04:34:36.915656: step 3877, loss 0.394253, acc 0.859375, prec 0.0711056, recall 0.837466
2017-12-10T04:34:37.184494: step 3878, loss 0.343045, acc 0.90625, prec 0.0711134, recall 0.837495
2017-12-10T04:34:37.451761: step 3879, loss 0.478865, acc 0.890625, prec 0.0711201, recall 0.837525
2017-12-10T04:34:37.719359: step 3880, loss 0.527612, acc 0.90625, prec 0.0711279, recall 0.837555
2017-12-10T04:34:37.987304: step 3881, loss 0.291739, acc 0.875, prec 0.0711336, recall 0.837585
2017-12-10T04:34:38.249541: step 3882, loss 0.299003, acc 0.90625, prec 0.0711414, recall 0.837615
2017-12-10T04:34:38.516432: step 3883, loss 0.1663, acc 0.953125, prec 0.0711525, recall 0.837644
2017-12-10T04:34:38.784040: step 3884, loss 0.0856477, acc 0.96875, prec 0.0711793, recall 0.837704
2017-12-10T04:34:39.058007: step 3885, loss 0.0794493, acc 0.96875, prec 0.071206, recall 0.837764
2017-12-10T04:34:39.321198: step 3886, loss 0.128509, acc 0.96875, prec 0.0712038, recall 0.837764
2017-12-10T04:34:39.586590: step 3887, loss 0.190437, acc 0.953125, prec 0.0712004, recall 0.837764
2017-12-10T04:34:39.850875: step 3888, loss 0.109486, acc 0.953125, prec 0.0712116, recall 0.837793
2017-12-10T04:34:40.110770: step 3889, loss 0.0593572, acc 0.96875, prec 0.0712383, recall 0.837853
2017-12-10T04:34:40.381432: step 3890, loss 0.123317, acc 0.984375, prec 0.0712661, recall 0.837912
2017-12-10T04:34:40.658874: step 3891, loss 0.0332618, acc 0.984375, prec 0.071265, recall 0.837912
2017-12-10T04:34:40.930531: step 3892, loss 0.0230026, acc 0.984375, prec 0.0712639, recall 0.837912
2017-12-10T04:34:41.199544: step 3893, loss 0.0154328, acc 1, prec 0.0712639, recall 0.837912
2017-12-10T04:34:41.463909: step 3894, loss 0.628988, acc 0.984375, prec 0.0712917, recall 0.837971
2017-12-10T04:34:41.729063: step 3895, loss 0.122342, acc 1, prec 0.0713062, recall 0.838001
2017-12-10T04:34:42.003564: step 3896, loss 0.374342, acc 1, prec 0.0713207, recall 0.838031
2017-12-10T04:34:42.273125: step 3897, loss 0.320298, acc 0.953125, prec 0.0713318, recall 0.83806
2017-12-10T04:34:42.539510: step 3898, loss 0.186527, acc 0.984375, prec 0.0713596, recall 0.83812
2017-12-10T04:34:42.810459: step 3899, loss 0.0681958, acc 0.96875, prec 0.0713574, recall 0.83812
2017-12-10T04:34:43.075530: step 3900, loss 0.206377, acc 0.96875, prec 0.0713552, recall 0.83812

Evaluation:
2017-12-10T04:34:50.648795: step 3900, loss 4.78916, acc 0.969523, prec 0.0717332, recall 0.823687

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-3900

2017-12-10T04:34:51.944304: step 3901, loss 0.473455, acc 1, prec 0.0717476, recall 0.823718
2017-12-10T04:34:52.221835: step 3902, loss 0.0885034, acc 0.984375, prec 0.0717465, recall 0.823718
2017-12-10T04:34:52.484998: step 3903, loss 3.3087, acc 0.96875, prec 0.0717465, recall 0.823425
2017-12-10T04:34:52.760019: step 3904, loss 0.210732, acc 0.921875, prec 0.0717553, recall 0.823456
2017-12-10T04:34:53.028327: step 3905, loss 0.0447365, acc 0.984375, prec 0.071783, recall 0.823519
2017-12-10T04:34:53.299207: step 3906, loss 0.410355, acc 0.875, prec 0.0717885, recall 0.82355
2017-12-10T04:34:53.559862: step 3907, loss 0.404899, acc 0.90625, prec 0.0717962, recall 0.823582
2017-12-10T04:34:53.826188: step 3908, loss 1.07833, acc 0.8125, prec 0.0717829, recall 0.823582
2017-12-10T04:34:54.093486: step 3909, loss 0.453684, acc 0.8125, prec 0.0717839, recall 0.823613
2017-12-10T04:34:54.352040: step 3910, loss 0.771199, acc 0.859375, prec 0.0717883, recall 0.823644
2017-12-10T04:34:54.623033: step 3911, loss 0.538879, acc 0.8125, prec 0.0718037, recall 0.823707
2017-12-10T04:34:54.882500: step 3912, loss 0.77605, acc 0.859375, prec 0.0717937, recall 0.823707
2017-12-10T04:34:55.153453: step 3913, loss 1.18306, acc 0.703125, prec 0.0717869, recall 0.823738
2017-12-10T04:34:55.416003: step 3914, loss 0.651384, acc 0.875, prec 0.0717924, recall 0.82377
2017-12-10T04:34:55.677646: step 3915, loss 0.810368, acc 0.859375, prec 0.0718111, recall 0.823832
2017-12-10T04:34:55.939628: step 3916, loss 0.391274, acc 0.9375, prec 0.0718067, recall 0.823832
2017-12-10T04:34:56.208828: step 3917, loss 0.499236, acc 0.875, prec 0.0718122, recall 0.823864
2017-12-10T04:34:56.477822: step 3918, loss 0.574172, acc 0.890625, prec 0.0718331, recall 0.823926
2017-12-10T04:34:56.743050: step 3919, loss 0.232189, acc 0.921875, prec 0.0718276, recall 0.823926
2017-12-10T04:34:57.013458: step 3920, loss 0.169829, acc 0.9375, prec 0.0718375, recall 0.823957
2017-12-10T04:34:57.275478: step 3921, loss 0.176041, acc 0.953125, prec 0.0718772, recall 0.824051
2017-12-10T04:34:57.543013: step 3922, loss 0.203887, acc 0.953125, prec 0.0718882, recall 0.824082
2017-12-10T04:34:57.812106: step 3923, loss 0.195645, acc 0.984375, prec 0.0719015, recall 0.824113
2017-12-10T04:34:58.078245: step 3924, loss 0.214181, acc 0.875, prec 0.0719069, recall 0.824145
2017-12-10T04:34:58.338895: step 3925, loss 0.262677, acc 0.9375, prec 0.0719312, recall 0.824207
2017-12-10T04:34:58.602899: step 3926, loss 0.588391, acc 0.953125, prec 0.0719566, recall 0.824269
2017-12-10T04:34:58.870975: step 3927, loss 0.28441, acc 0.921875, prec 0.0719654, recall 0.8243
2017-12-10T04:34:59.133870: step 3928, loss 0.160641, acc 0.9375, prec 0.0719896, recall 0.824363
2017-12-10T04:34:59.398634: step 3929, loss 0.0555502, acc 0.984375, prec 0.0720028, recall 0.824394
2017-12-10T04:34:59.663645: step 3930, loss 0.365325, acc 0.921875, prec 0.0720116, recall 0.824425
2017-12-10T04:34:59.927720: step 3931, loss 0.247803, acc 0.953125, prec 0.0720226, recall 0.824456
2017-12-10T04:35:00.198010: step 3932, loss 0.240032, acc 0.953125, prec 0.0720193, recall 0.824456
2017-12-10T04:35:00.474941: step 3933, loss 0.0262046, acc 0.984375, prec 0.0720182, recall 0.824456
2017-12-10T04:35:00.744474: step 3934, loss 0.224856, acc 0.921875, prec 0.072027, recall 0.824487
2017-12-10T04:35:01.007481: step 3935, loss 0.0595852, acc 0.984375, prec 0.0720402, recall 0.824518
2017-12-10T04:35:01.270704: step 3936, loss 0.0943909, acc 0.984375, prec 0.0720391, recall 0.824518
2017-12-10T04:35:01.537322: step 3937, loss 0.169959, acc 0.953125, prec 0.0720501, recall 0.824549
2017-12-10T04:35:01.803472: step 3938, loss 0.214166, acc 0.953125, prec 0.0720611, recall 0.82458
2017-12-10T04:35:02.071156: step 3939, loss 2.59838, acc 0.984375, prec 0.0720754, recall 0.824465
2017-12-10T04:35:02.343100: step 3940, loss 0.0119826, acc 1, prec 0.0720898, recall 0.824496
2017-12-10T04:35:02.609309: step 3941, loss 0.122107, acc 0.96875, prec 0.0720875, recall 0.824496
2017-12-10T04:35:02.870540: step 3942, loss 0.128589, acc 0.96875, prec 0.0720853, recall 0.824496
2017-12-10T04:35:03.131633: step 3943, loss 0.026086, acc 1, prec 0.0720996, recall 0.824527
2017-12-10T04:35:03.401130: step 3944, loss 0.127827, acc 0.953125, prec 0.0721106, recall 0.824558
2017-12-10T04:35:03.665312: step 3945, loss 0.369333, acc 0.90625, prec 0.0721039, recall 0.824558
2017-12-10T04:35:03.933987: step 3946, loss 0.315208, acc 0.90625, prec 0.0720973, recall 0.824558
2017-12-10T04:35:04.203448: step 3947, loss 0.404788, acc 0.921875, prec 0.072106, recall 0.824589
2017-12-10T04:35:04.475049: step 3948, loss 0.179692, acc 0.9375, prec 0.0721159, recall 0.82462
2017-12-10T04:35:04.737352: step 3949, loss 0.276532, acc 0.9375, prec 0.0721258, recall 0.824651
2017-12-10T04:35:05.011146: step 3950, loss 0.123622, acc 0.96875, prec 0.0721235, recall 0.824651
2017-12-10T04:35:05.275506: step 3951, loss 0.173914, acc 0.90625, prec 0.0721169, recall 0.824651
2017-12-10T04:35:05.546020: step 3952, loss 0.283546, acc 0.90625, prec 0.0721245, recall 0.824682
2017-12-10T04:35:05.809311: step 3953, loss 0.246711, acc 0.9375, prec 0.0721344, recall 0.824713
2017-12-10T04:35:06.075109: step 3954, loss 0.0911687, acc 0.96875, prec 0.0721465, recall 0.824744
2017-12-10T04:35:06.349478: step 3955, loss 0.333544, acc 0.890625, prec 0.0721387, recall 0.824744
2017-12-10T04:35:06.613621: step 3956, loss 0.234025, acc 0.921875, prec 0.0721474, recall 0.824775
2017-12-10T04:35:06.877798: step 3957, loss 0.112014, acc 0.953125, prec 0.0721441, recall 0.824775
2017-12-10T04:35:07.149728: step 3958, loss 0.554641, acc 0.953125, prec 0.0721551, recall 0.824806
2017-12-10T04:35:07.413466: step 3959, loss 0.216893, acc 0.953125, prec 0.0721804, recall 0.824868
2017-12-10T04:35:07.679451: step 3960, loss 0.0627018, acc 0.96875, prec 0.0721925, recall 0.824899
2017-12-10T04:35:07.941788: step 3961, loss 0.257212, acc 0.9375, prec 0.072188, recall 0.824899
2017-12-10T04:35:08.205353: step 3962, loss 0.208448, acc 0.921875, prec 0.0722254, recall 0.824991
2017-12-10T04:35:08.471264: step 3963, loss 0.26058, acc 0.921875, prec 0.0722198, recall 0.824991
2017-12-10T04:35:08.735360: step 3964, loss 0.12415, acc 0.96875, prec 0.0722319, recall 0.825022
2017-12-10T04:35:08.995391: step 3965, loss 0.069606, acc 0.984375, prec 0.0722308, recall 0.825022
2017-12-10T04:35:09.255658: step 3966, loss 0.328682, acc 0.9375, prec 0.0722263, recall 0.825022
2017-12-10T04:35:09.521426: step 3967, loss 0.0098298, acc 1, prec 0.0722263, recall 0.825022
2017-12-10T04:35:09.790123: step 3968, loss 0.0716656, acc 0.984375, prec 0.0722252, recall 0.825022
2017-12-10T04:35:10.062147: step 3969, loss 0.93172, acc 0.984375, prec 0.0722527, recall 0.825084
2017-12-10T04:35:10.334318: step 3970, loss 0.0485682, acc 1, prec 0.0722957, recall 0.825176
2017-12-10T04:35:10.604727: step 3971, loss 0.0771781, acc 0.984375, prec 0.0722945, recall 0.825176
2017-12-10T04:35:10.872999: step 3972, loss 0.0749938, acc 0.96875, prec 0.0722923, recall 0.825176
2017-12-10T04:35:11.147963: step 3973, loss 0.0955383, acc 0.96875, prec 0.0723044, recall 0.825207
2017-12-10T04:35:11.418827: step 3974, loss 0.0344242, acc 0.984375, prec 0.0723033, recall 0.825207
2017-12-10T04:35:11.683394: step 3975, loss 0.102699, acc 0.984375, prec 0.0723308, recall 0.825268
2017-12-10T04:35:11.917065: step 3976, loss 0.158629, acc 0.980769, prec 0.072344, recall 0.825299
2017-12-10T04:35:12.194205: step 3977, loss 0.0489214, acc 0.984375, prec 0.0723572, recall 0.82533
2017-12-10T04:35:12.459441: step 3978, loss 0.0824573, acc 0.984375, prec 0.072356, recall 0.82533
2017-12-10T04:35:12.725539: step 3979, loss 0.177715, acc 0.9375, prec 0.0723516, recall 0.82533
2017-12-10T04:35:12.995299: step 3980, loss 0.140242, acc 0.953125, prec 0.0723768, recall 0.825391
2017-12-10T04:35:13.258340: step 3981, loss 0.189696, acc 0.96875, prec 0.0723889, recall 0.825422
2017-12-10T04:35:13.526586: step 3982, loss 0.0194189, acc 1, prec 0.0724032, recall 0.825453
2017-12-10T04:35:13.789088: step 3983, loss 0.0486034, acc 0.984375, prec 0.0724164, recall 0.825483
2017-12-10T04:35:14.056996: step 3984, loss 0.0357464, acc 1, prec 0.0724164, recall 0.825483
2017-12-10T04:35:14.333478: step 3985, loss 0.172278, acc 0.921875, prec 0.0724251, recall 0.825514
2017-12-10T04:35:14.600382: step 3986, loss 0.0211218, acc 1, prec 0.0724394, recall 0.825545
2017-12-10T04:35:14.865387: step 3987, loss 3.36799, acc 0.953125, prec 0.0724658, recall 0.825461
2017-12-10T04:35:15.140924: step 3988, loss 0.0563787, acc 0.96875, prec 0.0724921, recall 0.825522
2017-12-10T04:35:15.408084: step 3989, loss 0.0999136, acc 0.96875, prec 0.0725042, recall 0.825553
2017-12-10T04:35:15.671263: step 3990, loss 0.17601, acc 0.921875, prec 0.0725415, recall 0.825645
2017-12-10T04:35:15.936121: step 3991, loss 0.255815, acc 0.921875, prec 0.0725359, recall 0.825645
2017-12-10T04:35:16.201086: step 3992, loss 0.285888, acc 0.953125, prec 0.0725468, recall 0.825675
2017-12-10T04:35:16.467779: step 3993, loss 0.289854, acc 0.90625, prec 0.0725687, recall 0.825736
2017-12-10T04:35:16.732773: step 3994, loss 0.305739, acc 0.859375, prec 0.0725729, recall 0.825767
2017-12-10T04:35:16.996885: step 3995, loss 0.420813, acc 0.84375, prec 0.0725903, recall 0.825828
2017-12-10T04:35:17.266715: step 3996, loss 0.839118, acc 0.828125, prec 0.0726209, recall 0.825919
2017-12-10T04:35:17.535815: step 3997, loss 0.300958, acc 0.859375, prec 0.0726394, recall 0.82598
2017-12-10T04:35:17.805832: step 3998, loss 0.418212, acc 0.875, prec 0.072659, recall 0.826041
2017-12-10T04:35:18.068072: step 3999, loss 0.34945, acc 0.84375, prec 0.0726763, recall 0.826102
2017-12-10T04:35:18.333508: step 4000, loss 0.358423, acc 0.90625, prec 0.0726696, recall 0.826102
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-4000

2017-12-10T04:35:19.622727: step 4001, loss 0.161968, acc 0.921875, prec 0.072664, recall 0.826102
2017-12-10T04:35:19.892928: step 4002, loss 0.343078, acc 0.921875, prec 0.0726727, recall 0.826133
2017-12-10T04:35:20.156048: step 4003, loss 0.317505, acc 0.9375, prec 0.0726682, recall 0.826133
2017-12-10T04:35:20.419241: step 4004, loss 0.220637, acc 0.9375, prec 0.0726638, recall 0.826133
2017-12-10T04:35:20.687801: step 4005, loss 0.17777, acc 0.9375, prec 0.0726593, recall 0.826133
2017-12-10T04:35:20.956157: step 4006, loss 0.1118, acc 0.984375, prec 0.0726582, recall 0.826133
2017-12-10T04:35:21.219678: step 4007, loss 0.0165425, acc 1, prec 0.0726582, recall 0.826133
2017-12-10T04:35:21.482152: step 4008, loss 0.583072, acc 0.9375, prec 0.072668, recall 0.826163
2017-12-10T04:35:21.752473: step 4009, loss 0.258566, acc 0.9375, prec 0.0726778, recall 0.826193
2017-12-10T04:35:22.017269: step 4010, loss 1.53317, acc 0.9375, prec 0.0727303, recall 0.826315
2017-12-10T04:35:22.283643: step 4011, loss 0.0635165, acc 0.984375, prec 0.0727578, recall 0.826376
2017-12-10T04:35:22.542216: step 4012, loss 0.109577, acc 0.953125, prec 0.0727829, recall 0.826436
2017-12-10T04:35:22.808494: step 4013, loss 0.277825, acc 0.921875, prec 0.0727916, recall 0.826467
2017-12-10T04:35:23.074916: step 4014, loss 0.170226, acc 0.921875, prec 0.072786, recall 0.826467
2017-12-10T04:35:23.341366: step 4015, loss 0.196093, acc 0.9375, prec 0.07281, recall 0.826527
2017-12-10T04:35:23.602812: step 4016, loss 0.120756, acc 0.96875, prec 0.072822, recall 0.826557
2017-12-10T04:35:23.870911: step 4017, loss 0.0994341, acc 0.96875, prec 0.0728625, recall 0.826648
2017-12-10T04:35:24.147252: step 4018, loss 0.11363, acc 0.96875, prec 0.0728603, recall 0.826648
2017-12-10T04:35:24.412439: step 4019, loss 0.13028, acc 0.9375, prec 0.0728558, recall 0.826648
2017-12-10T04:35:24.683188: step 4020, loss 0.0656884, acc 0.96875, prec 0.0728536, recall 0.826648
2017-12-10T04:35:24.947543: step 4021, loss 0.0265943, acc 0.984375, prec 0.0728667, recall 0.826678
2017-12-10T04:35:25.218352: step 4022, loss 0.0498375, acc 0.96875, prec 0.0728645, recall 0.826678
2017-12-10T04:35:25.486696: step 4023, loss 0.150041, acc 0.9375, prec 0.07286, recall 0.826678
2017-12-10T04:35:25.755882: step 4024, loss 0.418245, acc 0.9375, prec 0.072884, recall 0.826739
2017-12-10T04:35:26.027513: step 4025, loss 0.169232, acc 0.984375, prec 0.0729114, recall 0.826799
2017-12-10T04:35:26.290335: step 4026, loss 0.100901, acc 0.953125, prec 0.072908, recall 0.826799
2017-12-10T04:35:26.559950: step 4027, loss 0.0445965, acc 0.984375, prec 0.0729354, recall 0.826859
2017-12-10T04:35:26.830000: step 4028, loss 0.24384, acc 0.90625, prec 0.0729287, recall 0.826859
2017-12-10T04:35:27.110587: step 4029, loss 0.0158144, acc 1, prec 0.0729287, recall 0.826859
2017-12-10T04:35:27.376320: step 4030, loss 0.0871266, acc 0.984375, prec 0.0729275, recall 0.826859
2017-12-10T04:35:27.642294: step 4031, loss 0.173783, acc 0.96875, prec 0.0729253, recall 0.826859
2017-12-10T04:35:27.911219: step 4032, loss 0.0168164, acc 1, prec 0.0729396, recall 0.82689
2017-12-10T04:35:28.181766: step 4033, loss 0.0216168, acc 1, prec 0.0729538, recall 0.82692
2017-12-10T04:35:28.448487: step 4034, loss 0.0485054, acc 0.984375, prec 0.0729527, recall 0.82692
2017-12-10T04:35:28.715983: step 4035, loss 0.05097, acc 0.984375, prec 0.07298, recall 0.82698
2017-12-10T04:35:28.985420: step 4036, loss 0.0670072, acc 0.953125, prec 0.0729909, recall 0.82701
2017-12-10T04:35:29.255683: step 4037, loss 0.0948373, acc 1, prec 0.0730051, recall 0.82704
2017-12-10T04:35:29.519924: step 4038, loss 0.0450755, acc 0.984375, prec 0.073004, recall 0.82704
2017-12-10T04:35:29.787631: step 4039, loss 0.19878, acc 0.9375, prec 0.0729995, recall 0.82704
2017-12-10T04:35:30.056782: step 4040, loss 0.102057, acc 0.984375, prec 0.0730127, recall 0.82707
2017-12-10T04:35:30.328134: step 4041, loss 0.876424, acc 1, prec 0.0730269, recall 0.8271
2017-12-10T04:35:30.604282: step 4042, loss 0.0535844, acc 0.96875, prec 0.0730246, recall 0.8271
2017-12-10T04:35:30.869783: step 4043, loss 0.231014, acc 0.953125, prec 0.0730355, recall 0.82713
2017-12-10T04:35:31.130140: step 4044, loss 0.0537231, acc 0.96875, prec 0.0730333, recall 0.82713
2017-12-10T04:35:31.399379: step 4045, loss 0.00991592, acc 1, prec 0.0730333, recall 0.82713
2017-12-10T04:35:31.670114: step 4046, loss 0.10481, acc 0.9375, prec 0.0730288, recall 0.82713
2017-12-10T04:35:31.933790: step 4047, loss 0.203348, acc 0.984375, prec 0.0730419, recall 0.82716
2017-12-10T04:35:32.201547: step 4048, loss 0.0911198, acc 0.96875, prec 0.0730539, recall 0.827191
2017-12-10T04:35:32.470620: step 4049, loss 0.303769, acc 0.96875, prec 0.0730943, recall 0.827281
2017-12-10T04:35:32.741007: step 4050, loss 0.0565197, acc 0.984375, prec 0.0731075, recall 0.827311
2017-12-10T04:35:33.006562: step 4051, loss 0.033539, acc 0.984375, prec 0.0731063, recall 0.827311
2017-12-10T04:35:33.275861: step 4052, loss 0.427757, acc 0.921875, prec 0.0731292, recall 0.827371
2017-12-10T04:35:33.542883: step 4053, loss 0.0530457, acc 0.96875, prec 0.0731554, recall 0.827431
2017-12-10T04:35:33.808158: step 4054, loss 1.21584, acc 0.96875, prec 0.0731685, recall 0.827317
2017-12-10T04:35:34.074776: step 4055, loss 0.1414, acc 0.9375, prec 0.0731924, recall 0.827377
2017-12-10T04:35:34.344880: step 4056, loss 0.263938, acc 0.921875, prec 0.0731868, recall 0.827377
2017-12-10T04:35:34.612381: step 4057, loss 0.169761, acc 0.984375, prec 0.0732142, recall 0.827437
2017-12-10T04:35:34.882303: step 4058, loss 0.25014, acc 0.96875, prec 0.0732119, recall 0.827437
2017-12-10T04:35:35.150715: step 4059, loss 0.400483, acc 0.90625, prec 0.0732194, recall 0.827467
2017-12-10T04:35:35.422179: step 4060, loss 0.477948, acc 0.9375, prec 0.0732291, recall 0.827497
2017-12-10T04:35:35.696648: step 4061, loss 0.598839, acc 0.890625, prec 0.0732497, recall 0.827556
2017-12-10T04:35:35.965021: step 4062, loss 0.147541, acc 0.9375, prec 0.0732452, recall 0.827556
2017-12-10T04:35:36.229480: step 4063, loss 0.165672, acc 0.96875, prec 0.0732714, recall 0.827616
2017-12-10T04:35:36.499790: step 4064, loss 0.117362, acc 0.96875, prec 0.0732975, recall 0.827676
2017-12-10T04:35:36.769353: step 4065, loss 0.187673, acc 0.96875, prec 0.0732953, recall 0.827676
2017-12-10T04:35:37.036113: step 4066, loss 0.148345, acc 0.96875, prec 0.0732931, recall 0.827676
2017-12-10T04:35:37.311558: step 4067, loss 0.141963, acc 0.96875, prec 0.0733192, recall 0.827735
2017-12-10T04:35:37.577425: step 4068, loss 0.14123, acc 0.96875, prec 0.073317, recall 0.827735
2017-12-10T04:35:37.843753: step 4069, loss 0.256764, acc 0.921875, prec 0.0733256, recall 0.827765
2017-12-10T04:35:38.111885: step 4070, loss 0.0360458, acc 0.984375, prec 0.0733244, recall 0.827765
2017-12-10T04:35:38.379997: step 4071, loss 0.116428, acc 0.953125, prec 0.0733495, recall 0.827825
2017-12-10T04:35:38.654187: step 4072, loss 0.60363, acc 0.953125, prec 0.0733461, recall 0.827825
2017-12-10T04:35:38.920177: step 4073, loss 0.197083, acc 0.96875, prec 0.0733723, recall 0.827884
2017-12-10T04:35:39.187316: step 4074, loss 0.0393721, acc 1, prec 0.0734007, recall 0.827944
2017-12-10T04:35:39.459603: step 4075, loss 0.0963199, acc 0.96875, prec 0.0734268, recall 0.828003
2017-12-10T04:35:39.731465: step 4076, loss 0.0963032, acc 0.96875, prec 0.0734388, recall 0.828033
2017-12-10T04:35:39.996064: step 4077, loss 0.11759, acc 0.953125, prec 0.0734496, recall 0.828063
2017-12-10T04:35:40.260524: step 4078, loss 0.157706, acc 0.953125, prec 0.0734604, recall 0.828093
2017-12-10T04:35:40.524735: step 4079, loss 0.00761211, acc 1, prec 0.0734604, recall 0.828093
2017-12-10T04:35:40.793011: step 4080, loss 0.0923042, acc 0.96875, prec 0.0734724, recall 0.828122
2017-12-10T04:35:41.061176: step 4081, loss 0.409754, acc 1, prec 0.073515, recall 0.828211
2017-12-10T04:35:41.331933: step 4082, loss 0.262814, acc 0.921875, prec 0.0735236, recall 0.828241
2017-12-10T04:35:41.602757: step 4083, loss 0.925115, acc 0.96875, prec 0.0735355, recall 0.828271
2017-12-10T04:35:41.887781: step 4084, loss 0.763235, acc 0.953125, prec 0.0735605, recall 0.82833
2017-12-10T04:35:42.172147: step 4085, loss 0.166841, acc 0.953125, prec 0.0735571, recall 0.82833
2017-12-10T04:35:42.436326: step 4086, loss 0.226554, acc 0.96875, prec 0.0735691, recall 0.828359
2017-12-10T04:35:42.697777: step 4087, loss 0.188633, acc 0.953125, prec 0.0735799, recall 0.828389
2017-12-10T04:35:42.963829: step 4088, loss 0.0763629, acc 0.96875, prec 0.0735776, recall 0.828389
2017-12-10T04:35:43.229216: step 4089, loss 0.233631, acc 0.90625, prec 0.0735709, recall 0.828389
2017-12-10T04:35:43.503261: step 4090, loss 0.770594, acc 0.9375, prec 0.0735947, recall 0.828448
2017-12-10T04:35:43.768262: step 4091, loss 0.384143, acc 0.875, prec 0.0735857, recall 0.828448
2017-12-10T04:35:44.031338: step 4092, loss 0.665111, acc 0.828125, prec 0.0735733, recall 0.828448
2017-12-10T04:35:44.294319: step 4093, loss 0.550985, acc 0.859375, prec 0.0735916, recall 0.828507
2017-12-10T04:35:44.559131: step 4094, loss 0.0560178, acc 0.96875, prec 0.0736035, recall 0.828537
2017-12-10T04:35:44.821741: step 4095, loss 0.485402, acc 0.90625, prec 0.0736251, recall 0.828596
2017-12-10T04:35:45.084406: step 4096, loss 0.37168, acc 0.921875, prec 0.0736336, recall 0.828626
2017-12-10T04:35:45.350691: step 4097, loss 0.281722, acc 0.890625, prec 0.0736399, recall 0.828655
2017-12-10T04:35:45.610701: step 4098, loss 0.464753, acc 0.875, prec 0.0736451, recall 0.828685
2017-12-10T04:35:45.881969: step 4099, loss 0.275186, acc 0.953125, prec 0.0736417, recall 0.828685
2017-12-10T04:35:46.143702: step 4100, loss 0.353704, acc 0.90625, prec 0.0736349, recall 0.828685
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-4100

2017-12-10T04:35:47.352445: step 4101, loss 0.323474, acc 0.890625, prec 0.0736554, recall 0.828744
2017-12-10T04:35:47.619754: step 4102, loss 0.316669, acc 0.9375, prec 0.0736509, recall 0.828744
2017-12-10T04:35:47.892390: step 4103, loss 0.143222, acc 0.96875, prec 0.0736486, recall 0.828744
2017-12-10T04:35:48.159659: step 4104, loss 0.0583633, acc 0.953125, prec 0.0736452, recall 0.828744
2017-12-10T04:35:48.422715: step 4105, loss 0.108709, acc 0.953125, prec 0.0736702, recall 0.828802
2017-12-10T04:35:48.687078: step 4106, loss 0.335412, acc 0.9375, prec 0.0736799, recall 0.828832
2017-12-10T04:35:48.956067: step 4107, loss 0.169192, acc 0.96875, prec 0.0736918, recall 0.828861
2017-12-10T04:35:49.219655: step 4108, loss 0.0563201, acc 0.984375, prec 0.0736907, recall 0.828861
2017-12-10T04:35:49.488819: step 4109, loss 0.380654, acc 0.96875, prec 0.0736884, recall 0.828861
2017-12-10T04:35:49.754759: step 4110, loss 0.131588, acc 0.96875, prec 0.0737145, recall 0.82892
2017-12-10T04:35:50.022081: step 4111, loss 0.0101037, acc 1, prec 0.0737145, recall 0.82892
2017-12-10T04:35:50.286054: step 4112, loss 0.0428531, acc 0.984375, prec 0.0737133, recall 0.82892
2017-12-10T04:35:50.552607: step 4113, loss 0.118936, acc 0.953125, prec 0.0737241, recall 0.82895
2017-12-10T04:35:50.815896: step 4114, loss 0.0173047, acc 1, prec 0.0737241, recall 0.82895
2017-12-10T04:35:51.084449: step 4115, loss 0.0146483, acc 1, prec 0.0737383, recall 0.828979
2017-12-10T04:35:51.346861: step 4116, loss 0.100298, acc 1, prec 0.0737524, recall 0.829008
2017-12-10T04:35:51.617643: step 4117, loss 0.0423391, acc 0.984375, prec 0.0737655, recall 0.829038
2017-12-10T04:35:51.889648: step 4118, loss 0.106579, acc 0.953125, prec 0.0737621, recall 0.829038
2017-12-10T04:35:52.160124: step 4119, loss 0.721803, acc 0.984375, prec 0.0738034, recall 0.829126
2017-12-10T04:35:52.430691: step 4120, loss 0.606356, acc 0.984375, prec 0.0738448, recall 0.829214
2017-12-10T04:35:52.701814: step 4121, loss 0.0066988, acc 1, prec 0.0738589, recall 0.829243
2017-12-10T04:35:52.968847: step 4122, loss 0.0550375, acc 0.96875, prec 0.0738567, recall 0.829243
2017-12-10T04:35:53.241510: step 4123, loss 0.283012, acc 0.984375, prec 0.0738697, recall 0.829272
2017-12-10T04:35:53.511033: step 4124, loss 0.00608807, acc 1, prec 0.0738697, recall 0.829272
2017-12-10T04:35:53.772745: step 4125, loss 0.123761, acc 0.953125, prec 0.0738663, recall 0.829272
2017-12-10T04:35:54.036850: step 4126, loss 0.314928, acc 0.9375, prec 0.0738618, recall 0.829272
2017-12-10T04:35:54.302661: step 4127, loss 0.121237, acc 0.953125, prec 0.0738726, recall 0.829302
2017-12-10T04:35:54.571101: step 4128, loss 0.0495279, acc 0.984375, prec 0.0738856, recall 0.829331
2017-12-10T04:35:54.835964: step 4129, loss 0.145819, acc 0.953125, prec 0.0739105, recall 0.82939
2017-12-10T04:35:55.099824: step 4130, loss 0.250731, acc 0.96875, prec 0.0739366, recall 0.829448
2017-12-10T04:35:55.367728: step 4131, loss 0.353449, acc 0.921875, prec 0.0739309, recall 0.829448
2017-12-10T04:35:55.633320: step 4132, loss 0.119839, acc 0.953125, prec 0.07397, recall 0.829536
2017-12-10T04:35:55.901728: step 4133, loss 0.0721843, acc 0.96875, prec 0.0739677, recall 0.829536
2017-12-10T04:35:56.170085: step 4134, loss 0.324029, acc 0.90625, prec 0.0739609, recall 0.829536
2017-12-10T04:35:56.436188: step 4135, loss 0.08936, acc 0.953125, prec 0.0739575, recall 0.829536
2017-12-10T04:35:56.700224: step 4136, loss 0.304317, acc 0.921875, prec 0.0739519, recall 0.829536
2017-12-10T04:35:56.978132: step 4137, loss 0.113529, acc 0.953125, prec 0.0739485, recall 0.829536
2017-12-10T04:35:57.253506: step 4138, loss 0.0300068, acc 1, prec 0.0739485, recall 0.829536
2017-12-10T04:35:57.519693: step 4139, loss 0.119968, acc 0.9375, prec 0.073944, recall 0.829536
2017-12-10T04:35:57.786881: step 4140, loss 0.051328, acc 0.984375, prec 0.073957, recall 0.829565
2017-12-10T04:35:58.052713: step 4141, loss 0.0425008, acc 0.984375, prec 0.07397, recall 0.829594
2017-12-10T04:35:58.317103: step 4142, loss 0.0634245, acc 0.984375, prec 0.073983, recall 0.829623
2017-12-10T04:35:58.587461: step 4143, loss 0.00694228, acc 1, prec 0.0740113, recall 0.829682
2017-12-10T04:35:58.857316: step 4144, loss 0.0478283, acc 0.984375, prec 0.0740102, recall 0.829682
2017-12-10T04:35:59.126119: step 4145, loss 0.281836, acc 0.9375, prec 0.0740056, recall 0.829682
2017-12-10T04:35:59.394279: step 4146, loss 0.282034, acc 0.96875, prec 0.0740317, recall 0.82974
2017-12-10T04:35:59.672563: step 4147, loss 0.144465, acc 0.96875, prec 0.0740718, recall 0.829827
2017-12-10T04:35:59.946538: step 4148, loss 0.0855493, acc 0.96875, prec 0.0740837, recall 0.829856
2017-12-10T04:36:00.224028: step 4149, loss 1.09076, acc 0.96875, prec 0.0740956, recall 0.829885
2017-12-10T04:36:00.507380: step 4150, loss 0.0933681, acc 0.984375, prec 0.0741086, recall 0.829915
2017-12-10T04:36:00.776307: step 4151, loss 0.209827, acc 0.96875, prec 0.0741204, recall 0.829944
2017-12-10T04:36:01.041398: step 4152, loss 0.0253644, acc 0.984375, prec 0.0741193, recall 0.829944
2017-12-10T04:36:01.304931: step 4153, loss 0.329455, acc 0.9375, prec 0.0741572, recall 0.830031
2017-12-10T04:36:01.566494: step 4154, loss 0.0888589, acc 0.953125, prec 0.0741679, recall 0.83006
2017-12-10T04:36:01.831872: step 4155, loss 0.0582834, acc 0.96875, prec 0.0741656, recall 0.83006
2017-12-10T04:36:02.103465: step 4156, loss 0.589431, acc 0.96875, prec 0.0741775, recall 0.830089
2017-12-10T04:36:02.374347: step 4157, loss 0.118485, acc 0.9375, prec 0.0742012, recall 0.830147
2017-12-10T04:36:02.644003: step 4158, loss 1.03165, acc 0.96875, prec 0.0742272, recall 0.830205
2017-12-10T04:36:02.911517: step 4159, loss 0.822884, acc 0.96875, prec 0.0742391, recall 0.830234
2017-12-10T04:36:03.177520: step 4160, loss 0.254717, acc 0.90625, prec 0.0742323, recall 0.830234
2017-12-10T04:36:03.443560: step 4161, loss 0.273366, acc 0.9375, prec 0.0742278, recall 0.830234
2017-12-10T04:36:03.708077: step 4162, loss 0.344604, acc 0.90625, prec 0.0742351, recall 0.830263
2017-12-10T04:36:03.971187: step 4163, loss 0.545408, acc 0.875, prec 0.074226, recall 0.830263
2017-12-10T04:36:04.235707: step 4164, loss 0.26326, acc 0.875, prec 0.0742734, recall 0.830378
2017-12-10T04:36:04.503698: step 4165, loss 0.213771, acc 0.953125, prec 0.07427, recall 0.830378
2017-12-10T04:36:04.773638: step 4166, loss 0.927059, acc 0.859375, prec 0.074274, recall 0.830407
2017-12-10T04:36:05.038247: step 4167, loss 0.691525, acc 0.84375, prec 0.0742626, recall 0.830407
2017-12-10T04:36:05.303759: step 4168, loss 0.208806, acc 0.953125, prec 0.0742733, recall 0.830436
2017-12-10T04:36:05.570379: step 4169, loss 0.495774, acc 0.921875, prec 0.0742818, recall 0.830465
2017-12-10T04:36:05.835005: step 4170, loss 0.250351, acc 0.953125, prec 0.0742784, recall 0.830465
2017-12-10T04:36:06.104036: step 4171, loss 0.235158, acc 0.953125, prec 0.074275, recall 0.830465
2017-12-10T04:36:06.370247: step 4172, loss 0.164578, acc 0.921875, prec 0.0742693, recall 0.830465
2017-12-10T04:36:06.633864: step 4173, loss 0.276563, acc 0.890625, prec 0.0742614, recall 0.830465
2017-12-10T04:36:06.898682: step 4174, loss 0.235464, acc 0.90625, prec 0.0742969, recall 0.830552
2017-12-10T04:36:07.159782: step 4175, loss 0.219707, acc 0.953125, prec 0.0743077, recall 0.830581
2017-12-10T04:36:07.425719: step 4176, loss 0.0940142, acc 0.96875, prec 0.0743195, recall 0.830609
2017-12-10T04:36:07.689637: step 4177, loss 0.168519, acc 0.953125, prec 0.0743161, recall 0.830609
2017-12-10T04:36:07.952235: step 4178, loss 0.193676, acc 0.953125, prec 0.0743127, recall 0.830609
2017-12-10T04:36:08.212575: step 4179, loss 0.0410975, acc 0.96875, prec 0.0743386, recall 0.830667
2017-12-10T04:36:08.477732: step 4180, loss 0.224066, acc 0.953125, prec 0.0743634, recall 0.830725
2017-12-10T04:36:08.749849: step 4181, loss 0.0226098, acc 0.984375, prec 0.0743623, recall 0.830725
2017-12-10T04:36:09.030446: step 4182, loss 0.0264781, acc 0.984375, prec 0.0743753, recall 0.830754
2017-12-10T04:36:09.297257: step 4183, loss 0.0185337, acc 1, prec 0.0744034, recall 0.830811
2017-12-10T04:36:09.560814: step 4184, loss 0.409914, acc 0.953125, prec 0.0744001, recall 0.830811
2017-12-10T04:36:09.835978: step 4185, loss 0.622197, acc 0.984375, prec 0.0744271, recall 0.830869
2017-12-10T04:36:10.106381: step 4186, loss 0.0449121, acc 0.984375, prec 0.074426, recall 0.830869
2017-12-10T04:36:10.374933: step 4187, loss 0.100591, acc 0.96875, prec 0.0744378, recall 0.830897
2017-12-10T04:36:10.640581: step 4188, loss 0.0100675, acc 1, prec 0.0744378, recall 0.830897
2017-12-10T04:36:10.910542: step 4189, loss 0.986643, acc 1, prec 0.0744519, recall 0.830926
2017-12-10T04:36:11.183279: step 4190, loss 0.11194, acc 0.96875, prec 0.0744778, recall 0.830984
2017-12-10T04:36:11.453216: step 4191, loss 0.0248068, acc 1, prec 0.0744919, recall 0.831012
2017-12-10T04:36:11.722744: step 4192, loss 0.080536, acc 0.953125, prec 0.0744885, recall 0.831012
2017-12-10T04:36:11.997623: step 4193, loss 0.466115, acc 0.984375, prec 0.0745296, recall 0.831098
2017-12-10T04:36:12.269522: step 4194, loss 0.293989, acc 0.90625, prec 0.0745369, recall 0.831127
2017-12-10T04:36:12.534671: step 4195, loss 0.0882349, acc 0.953125, prec 0.0745476, recall 0.831156
2017-12-10T04:36:12.802627: step 4196, loss 0.0846394, acc 0.984375, prec 0.0745746, recall 0.831213
2017-12-10T04:36:13.072045: step 4197, loss 0.0288051, acc 0.984375, prec 0.0746017, recall 0.83127
2017-12-10T04:36:13.346375: step 4198, loss 0.372304, acc 0.9375, prec 0.0745971, recall 0.83127
2017-12-10T04:36:13.614360: step 4199, loss 0.248985, acc 0.953125, prec 0.0746078, recall 0.831299
2017-12-10T04:36:13.877704: step 4200, loss 0.189735, acc 0.921875, prec 0.0746021, recall 0.831299

Evaluation:
2017-12-10T04:36:21.615789: step 4200, loss 3.18241, acc 0.952727, prec 0.0750653, recall 0.822031

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-4200

2017-12-10T04:36:23.087165: step 4201, loss 0.108284, acc 0.96875, prec 0.0750631, recall 0.822031
2017-12-10T04:36:23.355658: step 4202, loss 0.0650065, acc 0.96875, prec 0.0750748, recall 0.822061
2017-12-10T04:36:23.619639: step 4203, loss 0.161607, acc 0.953125, prec 0.0750993, recall 0.822119
2017-12-10T04:36:23.887938: step 4204, loss 0.345082, acc 0.9375, prec 0.0750948, recall 0.822119
2017-12-10T04:36:24.154827: step 4205, loss 0.186185, acc 0.96875, prec 0.0750925, recall 0.822119
2017-12-10T04:36:24.425903: step 4206, loss 0.0848995, acc 0.96875, prec 0.0750902, recall 0.822119
2017-12-10T04:36:24.695314: step 4207, loss 0.0514999, acc 0.953125, prec 0.0750868, recall 0.822119
2017-12-10T04:36:24.968152: step 4208, loss 0.0753701, acc 0.984375, prec 0.0750857, recall 0.822119
2017-12-10T04:36:25.240565: step 4209, loss 0.0148157, acc 1, prec 0.0750857, recall 0.822119
2017-12-10T04:36:25.503112: step 4210, loss 0.0165484, acc 1, prec 0.0751136, recall 0.822178
2017-12-10T04:36:25.774026: step 4211, loss 0.315481, acc 0.953125, prec 0.0751381, recall 0.822237
2017-12-10T04:36:26.040639: step 4212, loss 0.320224, acc 0.953125, prec 0.0751347, recall 0.822237
2017-12-10T04:36:26.306713: step 4213, loss 0.0144316, acc 1, prec 0.0751487, recall 0.822266
2017-12-10T04:36:26.570396: step 4214, loss 0.0181713, acc 1, prec 0.0751766, recall 0.822325
2017-12-10T04:36:26.840613: step 4215, loss 0.00817672, acc 1, prec 0.0751766, recall 0.822325
2017-12-10T04:36:27.116735: step 4216, loss 0.0586335, acc 0.984375, prec 0.0751755, recall 0.822325
2017-12-10T04:36:27.380757: step 4217, loss 0.0861235, acc 0.96875, prec 0.0751872, recall 0.822354
2017-12-10T04:36:27.648355: step 4218, loss 0.0334352, acc 0.984375, prec 0.075186, recall 0.822354
2017-12-10T04:36:27.909987: step 4219, loss 0.0359671, acc 0.984375, prec 0.0751849, recall 0.822354
2017-12-10T04:36:28.174378: step 4220, loss 0.0793955, acc 0.984375, prec 0.0751838, recall 0.822354
2017-12-10T04:36:28.444217: step 4221, loss 0.164848, acc 0.96875, prec 0.0751815, recall 0.822354
2017-12-10T04:36:28.710341: step 4222, loss 3.9351, acc 0.953125, prec 0.0752083, recall 0.822142
2017-12-10T04:36:28.976615: step 4223, loss 0.0810805, acc 0.96875, prec 0.075206, recall 0.822142
2017-12-10T04:36:29.246183: step 4224, loss 0.172147, acc 0.984375, prec 0.0752049, recall 0.822142
2017-12-10T04:36:29.514070: step 4225, loss 0.516022, acc 0.890625, prec 0.0751969, recall 0.822142
2017-12-10T04:36:29.784362: step 4226, loss 0.0855213, acc 0.984375, prec 0.0752237, recall 0.8222
2017-12-10T04:36:30.059801: step 4227, loss 0.192163, acc 0.921875, prec 0.075232, recall 0.82223
2017-12-10T04:36:30.335228: step 4228, loss 2.27199, acc 0.921875, prec 0.0752414, recall 0.822123
2017-12-10T04:36:30.607608: step 4229, loss 0.644852, acc 0.8125, prec 0.0752417, recall 0.822153
2017-12-10T04:36:30.875920: step 4230, loss 0.461324, acc 0.796875, prec 0.0752409, recall 0.822182
2017-12-10T04:36:31.137023: step 4231, loss 0.779099, acc 0.75, prec 0.0752507, recall 0.822241
2017-12-10T04:36:31.408103: step 4232, loss 0.297098, acc 0.90625, prec 0.0752439, recall 0.822241
2017-12-10T04:36:31.669863: step 4233, loss 0.825581, acc 0.703125, prec 0.0752641, recall 0.822328
2017-12-10T04:36:31.933459: step 4234, loss 1.30447, acc 0.671875, prec 0.0752403, recall 0.822328
2017-12-10T04:36:32.201938: step 4235, loss 0.896248, acc 0.8125, prec 0.0752406, recall 0.822358
2017-12-10T04:36:32.471835: step 4236, loss 0.988949, acc 0.71875, prec 0.0752342, recall 0.822387
2017-12-10T04:36:32.733424: step 4237, loss 1.35655, acc 0.6875, prec 0.0752254, recall 0.822416
2017-12-10T04:36:32.999289: step 4238, loss 1.39697, acc 0.75, prec 0.0752212, recall 0.822445
2017-12-10T04:36:33.261584: step 4239, loss 0.522452, acc 0.875, prec 0.0752122, recall 0.822445
2017-12-10T04:36:33.529653: step 4240, loss 0.593876, acc 0.84375, prec 0.0752148, recall 0.822474
2017-12-10T04:36:33.789217: step 4241, loss 0.324904, acc 0.921875, prec 0.0752091, recall 0.822474
2017-12-10T04:36:34.055982: step 4242, loss 0.546625, acc 0.875, prec 0.0752001, recall 0.822474
2017-12-10T04:36:34.319331: step 4243, loss 0.129167, acc 0.953125, prec 0.0752106, recall 0.822504
2017-12-10T04:36:34.584293: step 4244, loss 0.162865, acc 0.953125, prec 0.0752211, recall 0.822533
2017-12-10T04:36:34.854866: step 4245, loss 0.227996, acc 0.953125, prec 0.0752177, recall 0.822533
2017-12-10T04:36:35.116678: step 4246, loss 0.291578, acc 0.9375, prec 0.075241, recall 0.822591
2017-12-10T04:36:35.384718: step 4247, loss 0.18115, acc 0.953125, prec 0.0752515, recall 0.82262
2017-12-10T04:36:35.648796: step 4248, loss 0.52281, acc 0.921875, prec 0.0752459, recall 0.82262
2017-12-10T04:36:35.913370: step 4249, loss 0.0575744, acc 0.984375, prec 0.0752447, recall 0.82262
2017-12-10T04:36:36.175008: step 4250, loss 0.111725, acc 0.953125, prec 0.0752413, recall 0.82262
2017-12-10T04:36:36.439892: step 4251, loss 0.197957, acc 0.96875, prec 0.0752391, recall 0.82262
2017-12-10T04:36:36.703619: step 4252, loss 0.0161201, acc 1, prec 0.075253, recall 0.82265
2017-12-10T04:36:36.968798: step 4253, loss 0.118868, acc 0.984375, prec 0.0752518, recall 0.82265
2017-12-10T04:36:37.242384: step 4254, loss 0.0586121, acc 0.984375, prec 0.0752646, recall 0.822679
2017-12-10T04:36:37.512651: step 4255, loss 0.258528, acc 0.953125, prec 0.0752612, recall 0.822679
2017-12-10T04:36:37.776267: step 4256, loss 0.0283031, acc 0.984375, prec 0.0752601, recall 0.822679
2017-12-10T04:36:38.047051: step 4257, loss 0.313952, acc 0.984375, prec 0.0752729, recall 0.822708
2017-12-10T04:36:38.323393: step 4258, loss 0.196541, acc 0.96875, prec 0.0752984, recall 0.822766
2017-12-10T04:36:38.594839: step 4259, loss 0.129263, acc 0.96875, prec 0.07531, recall 0.822795
2017-12-10T04:36:38.860230: step 4260, loss 0.0636777, acc 1, prec 0.0753239, recall 0.822824
2017-12-10T04:36:39.127903: step 4261, loss 7.1769, acc 0.984375, prec 0.0753239, recall 0.822689
2017-12-10T04:36:39.398349: step 4262, loss 0.134619, acc 0.96875, prec 0.0753217, recall 0.822689
2017-12-10T04:36:39.665304: step 4263, loss 0.0077918, acc 1, prec 0.0753217, recall 0.822689
2017-12-10T04:36:39.929229: step 4264, loss 0.0581854, acc 1, prec 0.0753495, recall 0.822747
2017-12-10T04:36:40.195936: step 4265, loss 0.130473, acc 0.96875, prec 0.0753472, recall 0.822747
2017-12-10T04:36:40.464570: step 4266, loss 0.171509, acc 0.96875, prec 0.0753449, recall 0.822747
2017-12-10T04:36:40.727945: step 4267, loss 0.12338, acc 0.984375, prec 0.0753716, recall 0.822806
2017-12-10T04:36:40.999768: step 4268, loss 0.544704, acc 0.90625, prec 0.0753787, recall 0.822835
2017-12-10T04:36:41.263519: step 4269, loss 0.217631, acc 0.96875, prec 0.0754181, recall 0.822922
2017-12-10T04:36:41.528882: step 4270, loss 0.220445, acc 0.921875, prec 0.0754124, recall 0.822922
2017-12-10T04:36:41.793809: step 4271, loss 0.135577, acc 0.953125, prec 0.075409, recall 0.822922
2017-12-10T04:36:42.073101: step 4272, loss 0.357939, acc 0.890625, prec 0.0754428, recall 0.823009
2017-12-10T04:36:42.340552: step 4273, loss 0.224273, acc 0.9375, prec 0.0754521, recall 0.823038
2017-12-10T04:36:42.614441: step 4274, loss 0.197309, acc 0.96875, prec 0.0754638, recall 0.823067
2017-12-10T04:36:42.883482: step 4275, loss 0.214128, acc 0.9375, prec 0.0754731, recall 0.823096
2017-12-10T04:36:43.145907: step 4276, loss 0.404963, acc 0.921875, prec 0.0754813, recall 0.823125
2017-12-10T04:36:43.409568: step 4277, loss 0.126684, acc 0.984375, prec 0.0754941, recall 0.823154
2017-12-10T04:36:43.672714: step 4278, loss 0.103363, acc 0.953125, prec 0.0755184, recall 0.823212
2017-12-10T04:36:43.934925: step 4279, loss 0.171962, acc 0.953125, prec 0.075515, recall 0.823212
2017-12-10T04:36:44.205704: step 4280, loss 0.252224, acc 0.953125, prec 0.0755116, recall 0.823212
2017-12-10T04:36:44.472077: step 4281, loss 0.476686, acc 0.90625, prec 0.0755326, recall 0.823269
2017-12-10T04:36:44.747258: step 4282, loss 0.307152, acc 0.9375, prec 0.0755558, recall 0.823327
2017-12-10T04:36:45.012472: step 4283, loss 0.147761, acc 0.953125, prec 0.0755802, recall 0.823385
2017-12-10T04:36:45.278425: step 4284, loss 0.214333, acc 0.96875, prec 0.0755918, recall 0.823414
2017-12-10T04:36:45.541558: step 4285, loss 0.129373, acc 0.921875, prec 0.0755861, recall 0.823414
2017-12-10T04:36:45.807738: step 4286, loss 1.67244, acc 0.90625, prec 0.0755943, recall 0.823308
2017-12-10T04:36:46.080136: step 4287, loss 0.330233, acc 0.9375, prec 0.0756314, recall 0.823395
2017-12-10T04:36:46.355334: step 4288, loss 0.112177, acc 0.984375, prec 0.075658, recall 0.823453
2017-12-10T04:36:46.627067: step 4289, loss 0.0580383, acc 0.96875, prec 0.0756557, recall 0.823453
2017-12-10T04:36:46.893199: step 4290, loss 0.058605, acc 0.96875, prec 0.0756812, recall 0.82351
2017-12-10T04:36:47.160218: step 4291, loss 0.0689422, acc 0.984375, prec 0.0756939, recall 0.823539
2017-12-10T04:36:47.425774: step 4292, loss 0.0690031, acc 0.953125, prec 0.0757044, recall 0.823568
2017-12-10T04:36:47.692109: step 4293, loss 0.137691, acc 0.953125, prec 0.0757148, recall 0.823597
2017-12-10T04:36:47.953332: step 4294, loss 0.174842, acc 0.953125, prec 0.0757253, recall 0.823625
2017-12-10T04:36:48.217478: step 4295, loss 0.396852, acc 0.9375, prec 0.0757208, recall 0.823625
2017-12-10T04:36:48.477845: step 4296, loss 0.239345, acc 0.890625, prec 0.0757128, recall 0.823625
2017-12-10T04:36:48.742816: step 4297, loss 0.0408936, acc 0.96875, prec 0.0757105, recall 0.823625
2017-12-10T04:36:49.006301: step 4298, loss 0.139264, acc 0.9375, prec 0.0757199, recall 0.823654
2017-12-10T04:36:49.270598: step 4299, loss 0.220945, acc 0.96875, prec 0.0757176, recall 0.823654
2017-12-10T04:36:49.537542: step 4300, loss 0.107226, acc 0.984375, prec 0.0757165, recall 0.823654
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-4300

2017-12-10T04:36:50.806385: step 4301, loss 0.188543, acc 0.953125, prec 0.075713, recall 0.823654
2017-12-10T04:36:51.080680: step 4302, loss 0.0213119, acc 1, prec 0.0757269, recall 0.823683
2017-12-10T04:36:51.344489: step 4303, loss 0.517323, acc 0.96875, prec 0.0757523, recall 0.82374
2017-12-10T04:36:51.612759: step 4304, loss 0.164833, acc 0.984375, prec 0.0757789, recall 0.823798
2017-12-10T04:36:51.885992: step 4305, loss 0.180426, acc 1, prec 0.0757928, recall 0.823827
2017-12-10T04:36:52.156094: step 4306, loss 0.202296, acc 0.9375, prec 0.075816, recall 0.823884
2017-12-10T04:36:52.423759: step 4307, loss 0.386239, acc 0.90625, prec 0.0758091, recall 0.823884
2017-12-10T04:36:52.690850: step 4308, loss 0.146057, acc 0.953125, prec 0.0758196, recall 0.823913
2017-12-10T04:36:52.961644: step 4309, loss 0.0075803, acc 1, prec 0.0758196, recall 0.823913
2017-12-10T04:36:53.230550: step 4310, loss 0.150284, acc 0.953125, prec 0.0758162, recall 0.823913
2017-12-10T04:36:53.496741: step 4311, loss 0.0359934, acc 0.984375, prec 0.075815, recall 0.823913
2017-12-10T04:36:53.765801: step 4312, loss 0.120806, acc 0.953125, prec 0.0758116, recall 0.823913
2017-12-10T04:36:54.028044: step 4313, loss 0.0227852, acc 1, prec 0.0758393, recall 0.82397
2017-12-10T04:36:54.290006: step 4314, loss 0.0366497, acc 0.984375, prec 0.0758382, recall 0.82397
2017-12-10T04:36:54.558449: step 4315, loss 0.0813249, acc 0.96875, prec 0.0758636, recall 0.824027
2017-12-10T04:36:54.822402: step 4316, loss 0.105663, acc 0.953125, prec 0.0758741, recall 0.824056
2017-12-10T04:36:55.086407: step 4317, loss 3.09819, acc 0.96875, prec 0.0758868, recall 0.823951
2017-12-10T04:36:55.355732: step 4318, loss 0.350172, acc 0.90625, prec 0.0758938, recall 0.823979
2017-12-10T04:36:55.618120: step 4319, loss 0.498475, acc 0.953125, prec 0.0758904, recall 0.823979
2017-12-10T04:36:55.880219: step 4320, loss 0.144189, acc 0.96875, prec 0.0758881, recall 0.823979
2017-12-10T04:36:56.147967: step 4321, loss 0.239592, acc 0.96875, prec 0.0758858, recall 0.823979
2017-12-10T04:36:56.415211: step 4322, loss 0.0906598, acc 0.984375, prec 0.0759124, recall 0.824036
2017-12-10T04:36:56.685111: step 4323, loss 0.0634872, acc 0.984375, prec 0.0759251, recall 0.824065
2017-12-10T04:36:56.953439: step 4324, loss 0.262371, acc 0.9375, prec 0.0759344, recall 0.824094
2017-12-10T04:36:57.221010: step 4325, loss 0.388124, acc 0.90625, prec 0.0759691, recall 0.824179
2017-12-10T04:36:57.481273: step 4326, loss 0.198493, acc 0.984375, prec 0.0760095, recall 0.824265
2017-12-10T04:36:57.752404: step 4327, loss 0.452234, acc 0.890625, prec 0.0760153, recall 0.824294
2017-12-10T04:36:58.019439: step 4328, loss 0.390873, acc 0.921875, prec 0.0760235, recall 0.824322
2017-12-10T04:36:58.292769: step 4329, loss 0.236738, acc 0.921875, prec 0.0760178, recall 0.824322
2017-12-10T04:36:58.562136: step 4330, loss 0.153098, acc 0.953125, prec 0.0760282, recall 0.824351
2017-12-10T04:36:58.827960: step 4331, loss 0.148516, acc 0.96875, prec 0.0760398, recall 0.824379
2017-12-10T04:36:59.093997: step 4332, loss 0.18397, acc 0.953125, prec 0.0760363, recall 0.824379
2017-12-10T04:36:59.361510: step 4333, loss 0.174911, acc 0.96875, prec 0.0760617, recall 0.824436
2017-12-10T04:36:59.628289: step 4334, loss 0.737126, acc 0.859375, prec 0.0760653, recall 0.824465
2017-12-10T04:36:59.896943: step 4335, loss 0.119129, acc 0.984375, prec 0.076078, recall 0.824493
2017-12-10T04:37:00.177943: step 4336, loss 0.276799, acc 0.90625, prec 0.076085, recall 0.824522
2017-12-10T04:37:00.452811: step 4337, loss 0.412971, acc 0.953125, prec 0.0760816, recall 0.824522
2017-12-10T04:37:00.726530: step 4338, loss 0.330685, acc 0.90625, prec 0.0760748, recall 0.824522
2017-12-10T04:37:00.990362: step 4339, loss 0.139414, acc 0.96875, prec 0.076114, recall 0.824607
2017-12-10T04:37:01.261017: step 4340, loss 0.0934518, acc 0.984375, prec 0.0761543, recall 0.824692
2017-12-10T04:37:01.533659: step 4341, loss 0.126615, acc 0.953125, prec 0.0761647, recall 0.824721
2017-12-10T04:37:01.799672: step 4342, loss 0.108798, acc 0.984375, prec 0.0761635, recall 0.824721
2017-12-10T04:37:02.063986: step 4343, loss 0.0192455, acc 1, prec 0.076205, recall 0.824806
2017-12-10T04:37:02.326547: step 4344, loss 0.0523097, acc 0.984375, prec 0.0762039, recall 0.824806
2017-12-10T04:37:02.591924: step 4345, loss 0.289845, acc 0.984375, prec 0.076258, recall 0.824919
2017-12-10T04:37:02.857868: step 4346, loss 1.22081, acc 0.953125, prec 0.0762834, recall 0.824842
2017-12-10T04:37:03.123886: step 4347, loss 0.21239, acc 0.921875, prec 0.0762777, recall 0.824842
2017-12-10T04:37:03.396761: step 4348, loss 0.196391, acc 0.9375, prec 0.0762731, recall 0.824842
2017-12-10T04:37:03.670805: step 4349, loss 0.0382483, acc 0.984375, prec 0.0762719, recall 0.824842
2017-12-10T04:37:03.940347: step 4350, loss 0.208192, acc 0.90625, prec 0.0762927, recall 0.824899
2017-12-10T04:37:04.212878: step 4351, loss 0.139475, acc 0.96875, prec 0.0762905, recall 0.824899
2017-12-10T04:37:04.476660: step 4352, loss 0.20694, acc 0.9375, prec 0.0762859, recall 0.824899
2017-12-10T04:37:04.737411: step 4353, loss 0.214464, acc 0.96875, prec 0.0762974, recall 0.824927
2017-12-10T04:37:05.006994: step 4354, loss 0.0417925, acc 1, prec 0.0763112, recall 0.824956
2017-12-10T04:37:05.272952: step 4355, loss 0.32866, acc 0.953125, prec 0.0763216, recall 0.824984
2017-12-10T04:37:05.535173: step 4356, loss 0.247019, acc 0.890625, prec 0.0763136, recall 0.824984
2017-12-10T04:37:05.797905: step 4357, loss 0.379409, acc 0.96875, prec 0.0763251, recall 0.825012
2017-12-10T04:37:06.067301: step 4358, loss 0.441446, acc 0.9375, prec 0.0763482, recall 0.825069
2017-12-10T04:37:06.330871: step 4359, loss 0.301033, acc 0.90625, prec 0.076369, recall 0.825125
2017-12-10T04:37:06.594093: step 4360, loss 0.444508, acc 0.859375, prec 0.0763725, recall 0.825153
2017-12-10T04:37:06.864263: step 4361, loss 0.0829852, acc 0.953125, prec 0.0764105, recall 0.825238
2017-12-10T04:37:07.133986: step 4362, loss 0.0336524, acc 1, prec 0.0764243, recall 0.825266
2017-12-10T04:37:07.399957: step 4363, loss 0.0990813, acc 0.96875, prec 0.076422, recall 0.825266
2017-12-10T04:37:07.669991: step 4364, loss 0.0320521, acc 0.984375, prec 0.0764208, recall 0.825266
2017-12-10T04:37:07.937735: step 4365, loss 0.138969, acc 0.9375, prec 0.0764301, recall 0.825294
2017-12-10T04:37:08.212414: step 4366, loss 0.408614, acc 0.9375, prec 0.0764255, recall 0.825294
2017-12-10T04:37:08.485989: step 4367, loss 0.16234, acc 0.96875, prec 0.076437, recall 0.825323
2017-12-10T04:37:08.754507: step 4368, loss 0.0252299, acc 0.984375, prec 0.0764497, recall 0.825351
2017-12-10T04:37:09.021801: step 4369, loss 0.161647, acc 0.953125, prec 0.0764463, recall 0.825351
2017-12-10T04:37:09.286170: step 4370, loss 0.11753, acc 0.984375, prec 0.0764451, recall 0.825351
2017-12-10T04:37:09.561263: step 4371, loss 0.103429, acc 0.953125, prec 0.0764417, recall 0.825351
2017-12-10T04:37:09.837949: step 4372, loss 0.0617624, acc 0.953125, prec 0.0764383, recall 0.825351
2017-12-10T04:37:10.102778: step 4373, loss 0.0728553, acc 0.96875, prec 0.076436, recall 0.825351
2017-12-10T04:37:10.369462: step 4374, loss 0.70141, acc 0.984375, prec 0.0764486, recall 0.825379
2017-12-10T04:37:10.644156: step 4375, loss 0.0128755, acc 1, prec 0.0764624, recall 0.825407
2017-12-10T04:37:10.916809: step 4376, loss 0.00583474, acc 1, prec 0.0764624, recall 0.825407
2017-12-10T04:37:11.180456: step 4377, loss 0.0439347, acc 0.984375, prec 0.0764751, recall 0.825435
2017-12-10T04:37:11.445384: step 4378, loss 0.126143, acc 0.953125, prec 0.0764716, recall 0.825435
2017-12-10T04:37:11.724077: step 4379, loss 0.332481, acc 0.96875, prec 0.0764969, recall 0.825491
2017-12-10T04:37:12.004307: step 4380, loss 0.0239791, acc 0.984375, prec 0.0764958, recall 0.825491
2017-12-10T04:37:12.280692: step 4381, loss 0.194733, acc 0.953125, prec 0.0765199, recall 0.825548
2017-12-10T04:37:12.548252: step 4382, loss 0.0917916, acc 0.96875, prec 0.0765314, recall 0.825576
2017-12-10T04:37:12.822009: step 4383, loss 0.706669, acc 0.9375, prec 0.0765407, recall 0.825604
2017-12-10T04:37:13.089638: step 4384, loss 0.090309, acc 0.984375, prec 0.0765395, recall 0.825604
2017-12-10T04:37:13.354717: step 4385, loss 0.0710359, acc 0.96875, prec 0.0765372, recall 0.825604
2017-12-10T04:37:13.622830: step 4386, loss 0.103606, acc 0.96875, prec 0.0765487, recall 0.825632
2017-12-10T04:37:13.897573: step 4387, loss 0.238303, acc 0.953125, prec 0.0765729, recall 0.825688
2017-12-10T04:37:14.174234: step 4388, loss 0.172958, acc 0.9375, prec 0.0765821, recall 0.825716
2017-12-10T04:37:14.437445: step 4389, loss 0.299959, acc 0.984375, prec 0.0766223, recall 0.8258
2017-12-10T04:37:14.711155: step 4390, loss 0.0596174, acc 0.984375, prec 0.0766349, recall 0.825828
2017-12-10T04:37:14.972536: step 4391, loss 0.148533, acc 0.921875, prec 0.076643, recall 0.825856
2017-12-10T04:37:15.238255: step 4392, loss 0.123284, acc 0.96875, prec 0.0766683, recall 0.825912
2017-12-10T04:37:15.502583: step 4393, loss 0.27018, acc 0.9375, prec 0.0766775, recall 0.82594
2017-12-10T04:37:15.778612: step 4394, loss 0.127423, acc 0.953125, prec 0.0767016, recall 0.825996
2017-12-10T04:37:16.046203: step 4395, loss 0.0384538, acc 1, prec 0.0767291, recall 0.826052
2017-12-10T04:37:16.310911: step 4396, loss 0.68189, acc 0.90625, prec 0.0767223, recall 0.826052
2017-12-10T04:37:16.581336: step 4397, loss 0.310857, acc 0.953125, prec 0.0767188, recall 0.826052
2017-12-10T04:37:16.843411: step 4398, loss 0.551781, acc 0.921875, prec 0.0767131, recall 0.826052
2017-12-10T04:37:17.107024: step 4399, loss 0.159013, acc 0.953125, prec 0.0767097, recall 0.826052
2017-12-10T04:37:17.373146: step 4400, loss 0.0957909, acc 0.984375, prec 0.0767636, recall 0.826164
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-4400

2017-12-10T04:37:18.620026: step 4401, loss 0.160314, acc 0.953125, prec 0.0767739, recall 0.826192
2017-12-10T04:37:18.888118: step 4402, loss 0.0510619, acc 0.984375, prec 0.0767866, recall 0.826219
2017-12-10T04:37:19.159930: step 4403, loss 0.0660679, acc 0.984375, prec 0.076813, recall 0.826275
2017-12-10T04:37:19.422334: step 4404, loss 0.138982, acc 0.953125, prec 0.0768095, recall 0.826275
2017-12-10T04:37:19.690898: step 4405, loss 0.0434166, acc 0.984375, prec 0.0768221, recall 0.826303
2017-12-10T04:37:19.957500: step 4406, loss 0.315908, acc 0.90625, prec 0.0768428, recall 0.826359
2017-12-10T04:37:20.228977: step 4407, loss 0.0477722, acc 0.96875, prec 0.0768405, recall 0.826359
2017-12-10T04:37:20.500277: step 4408, loss 0.0898809, acc 0.96875, prec 0.0768382, recall 0.826359
2017-12-10T04:37:20.762606: step 4409, loss 0.0821289, acc 0.96875, prec 0.0768497, recall 0.826387
2017-12-10T04:37:21.028340: step 4410, loss 0.102373, acc 1, prec 0.0769047, recall 0.826498
2017-12-10T04:37:21.296145: step 4411, loss 0.481027, acc 0.984375, prec 0.0769173, recall 0.826526
2017-12-10T04:37:21.567347: step 4412, loss 1.0104, acc 1, prec 0.0769449, recall 0.826581
2017-12-10T04:37:21.841088: step 4413, loss 0.223835, acc 0.96875, prec 0.0769838, recall 0.826665
2017-12-10T04:37:22.108589: step 4414, loss 0.112607, acc 0.953125, prec 0.0769942, recall 0.826692
2017-12-10T04:37:22.374293: step 4415, loss 0.294442, acc 0.96875, prec 0.0770056, recall 0.82672
2017-12-10T04:37:22.641239: step 4416, loss 0.137111, acc 0.96875, prec 0.0770171, recall 0.826748
2017-12-10T04:37:22.905942: step 4417, loss 0.456617, acc 0.90625, prec 0.0770102, recall 0.826748
2017-12-10T04:37:23.172096: step 4418, loss 0.0628315, acc 0.96875, prec 0.0770217, recall 0.826775
2017-12-10T04:37:23.439910: step 4419, loss 0.830065, acc 0.921875, prec 0.0770434, recall 0.826831
2017-12-10T04:37:23.716368: step 4420, loss 0.715311, acc 0.84375, prec 0.0770457, recall 0.826859
2017-12-10T04:37:23.992598: step 4421, loss 0.286561, acc 0.921875, prec 0.0770537, recall 0.826886
2017-12-10T04:37:24.254110: step 4422, loss 0.207272, acc 0.9375, prec 0.0770629, recall 0.826914
2017-12-10T04:37:24.519820: step 4423, loss 0.211545, acc 0.9375, prec 0.0770583, recall 0.826914
2017-12-10T04:37:24.783584: step 4424, loss 0.293851, acc 0.875, prec 0.0770491, recall 0.826914
2017-12-10T04:37:25.049998: step 4425, loss 0.494289, acc 0.9375, prec 0.0770582, recall 0.826941
2017-12-10T04:37:25.318564: step 4426, loss 0.265948, acc 0.890625, prec 0.0770502, recall 0.826941
2017-12-10T04:37:25.585878: step 4427, loss 0.276518, acc 0.90625, prec 0.0770433, recall 0.826941
2017-12-10T04:37:25.852345: step 4428, loss 0.29093, acc 0.921875, prec 0.0770513, recall 0.826969
2017-12-10T04:37:26.119885: step 4429, loss 0.22498, acc 0.921875, prec 0.0770593, recall 0.826997
2017-12-10T04:37:26.381579: step 4430, loss 0.380383, acc 0.90625, prec 0.0770525, recall 0.826997
2017-12-10T04:37:26.652461: step 4431, loss 0.24727, acc 0.984375, prec 0.077065, recall 0.827024
2017-12-10T04:37:26.931588: step 4432, loss 0.231451, acc 0.953125, prec 0.0770753, recall 0.827052
2017-12-10T04:37:27.202718: step 4433, loss 0.305632, acc 0.921875, prec 0.0770971, recall 0.827107
2017-12-10T04:37:27.478984: step 4434, loss 0.193798, acc 0.96875, prec 0.0770948, recall 0.827107
2017-12-10T04:37:27.740681: step 4435, loss 0.632782, acc 0.921875, prec 0.077089, recall 0.827107
2017-12-10T04:37:28.011893: step 4436, loss 0.147924, acc 0.953125, prec 0.0770856, recall 0.827107
2017-12-10T04:37:28.282543: step 4437, loss 0.284086, acc 0.9375, prec 0.0770947, recall 0.827135
2017-12-10T04:37:28.544930: step 4438, loss 0.225225, acc 0.9375, prec 0.0771039, recall 0.827162
2017-12-10T04:37:28.806444: step 4439, loss 0.435143, acc 0.953125, prec 0.0771416, recall 0.827245
2017-12-10T04:37:29.073286: step 4440, loss 0.0912181, acc 0.96875, prec 0.0771531, recall 0.827273
2017-12-10T04:37:29.346726: step 4441, loss 0.341009, acc 0.9375, prec 0.0771485, recall 0.827273
2017-12-10T04:37:29.614849: step 4442, loss 0.0255835, acc 0.984375, prec 0.0771748, recall 0.827328
2017-12-10T04:37:29.890673: step 4443, loss 0.354189, acc 0.953125, prec 0.0771988, recall 0.827383
2017-12-10T04:37:30.156707: step 4444, loss 0.0140754, acc 1, prec 0.0771988, recall 0.827383
2017-12-10T04:37:30.434152: step 4445, loss 1.0502, acc 0.984375, prec 0.0772251, recall 0.827438
2017-12-10T04:37:30.710639: step 4446, loss 0.227408, acc 0.96875, prec 0.0772365, recall 0.827465
2017-12-10T04:37:30.973642: step 4447, loss 0.265862, acc 0.96875, prec 0.0772342, recall 0.827465
2017-12-10T04:37:31.238491: step 4448, loss 0.134299, acc 0.953125, prec 0.0772308, recall 0.827465
2017-12-10T04:37:31.503724: step 4449, loss 0.32327, acc 0.953125, prec 0.0772547, recall 0.82752
2017-12-10T04:37:31.768506: step 4450, loss 0.143973, acc 0.96875, prec 0.0772525, recall 0.82752
2017-12-10T04:37:32.034570: step 4451, loss 0.0142674, acc 0.984375, prec 0.0772513, recall 0.82752
2017-12-10T04:37:32.297792: step 4452, loss 0.0758567, acc 0.984375, prec 0.0772639, recall 0.827548
2017-12-10T04:37:32.564859: step 4453, loss 0.150723, acc 0.9375, prec 0.077273, recall 0.827575
2017-12-10T04:37:32.832451: step 4454, loss 0.0468524, acc 0.984375, prec 0.0772718, recall 0.827575
2017-12-10T04:37:33.109372: step 4455, loss 0.186763, acc 0.96875, prec 0.0772833, recall 0.827603
2017-12-10T04:37:33.374803: step 4456, loss 0.024528, acc 1, prec 0.077297, recall 0.82763
2017-12-10T04:37:33.650227: step 4457, loss 3.27456, acc 0.953125, prec 0.0773084, recall 0.827526
2017-12-10T04:37:33.920116: step 4458, loss 0.184591, acc 0.96875, prec 0.0773335, recall 0.827581
2017-12-10T04:37:34.183262: step 4459, loss 0.0865174, acc 0.96875, prec 0.0773449, recall 0.827608
2017-12-10T04:37:34.446514: step 4460, loss 0.620381, acc 0.953125, prec 0.0773552, recall 0.827636
2017-12-10T04:37:34.715900: step 4461, loss 0.126142, acc 0.96875, prec 0.0773529, recall 0.827636
2017-12-10T04:37:34.984533: step 4462, loss 0.350302, acc 0.9375, prec 0.0773483, recall 0.827636
2017-12-10T04:37:35.256898: step 4463, loss 0.373985, acc 0.90625, prec 0.0773825, recall 0.827718
2017-12-10T04:37:35.520460: step 4464, loss 0.289069, acc 0.90625, prec 0.0774168, recall 0.8278
2017-12-10T04:37:35.783494: step 4465, loss 0.480798, acc 0.921875, prec 0.0774247, recall 0.827827
2017-12-10T04:37:36.040485: step 4466, loss 0.0784958, acc 0.96875, prec 0.0774498, recall 0.827882
2017-12-10T04:37:36.311662: step 4467, loss 0.339412, acc 0.890625, prec 0.0774418, recall 0.827882
2017-12-10T04:37:36.578348: step 4468, loss 0.514488, acc 0.859375, prec 0.0774451, recall 0.827909
2017-12-10T04:37:36.843327: step 4469, loss 0.792646, acc 0.828125, prec 0.0774462, recall 0.827937
2017-12-10T04:37:37.112136: step 4470, loss 0.415904, acc 0.890625, prec 0.0774518, recall 0.827964
2017-12-10T04:37:37.378426: step 4471, loss 0.446007, acc 0.859375, prec 0.0774552, recall 0.827991
2017-12-10T04:37:37.640439: step 4472, loss 0.237079, acc 0.9375, prec 0.0774506, recall 0.827991
2017-12-10T04:37:37.873625: step 4473, loss 0.181169, acc 0.942308, prec 0.0774471, recall 0.827991
2017-12-10T04:37:38.144506: step 4474, loss 0.231998, acc 0.9375, prec 0.0774425, recall 0.827991
2017-12-10T04:37:38.411377: step 4475, loss 0.184701, acc 0.953125, prec 0.0774528, recall 0.828018
2017-12-10T04:37:38.685456: step 4476, loss 0.145419, acc 0.96875, prec 0.0774642, recall 0.828046
2017-12-10T04:37:38.952093: step 4477, loss 0.136157, acc 0.953125, prec 0.0774744, recall 0.828073
2017-12-10T04:37:39.226771: step 4478, loss 0.200746, acc 0.921875, prec 0.077496, recall 0.828128
2017-12-10T04:37:39.491084: step 4479, loss 0.696766, acc 0.875, prec 0.0774868, recall 0.828128
2017-12-10T04:37:39.760084: step 4480, loss 0.117381, acc 0.9375, prec 0.0774822, recall 0.828128
2017-12-10T04:37:40.022679: step 4481, loss 0.147853, acc 0.96875, prec 0.0774936, recall 0.828155
2017-12-10T04:37:40.291476: step 4482, loss 0.0431396, acc 0.984375, prec 0.0775335, recall 0.828236
2017-12-10T04:37:40.554436: step 4483, loss 0.159347, acc 0.96875, prec 0.0775312, recall 0.828236
2017-12-10T04:37:40.826699: step 4484, loss 0.0078672, acc 1, prec 0.0775449, recall 0.828264
2017-12-10T04:37:41.102815: step 4485, loss 0.22883, acc 0.953125, prec 0.0775551, recall 0.828291
2017-12-10T04:37:41.377693: step 4486, loss 5.66772, acc 0.953125, prec 0.0775528, recall 0.82816
2017-12-10T04:37:41.645974: step 4487, loss 0.0727347, acc 0.96875, prec 0.0775505, recall 0.82816
2017-12-10T04:37:41.917147: step 4488, loss 0.0538241, acc 0.96875, prec 0.0775482, recall 0.82816
2017-12-10T04:37:42.188141: step 4489, loss 0.243266, acc 0.953125, prec 0.0775585, recall 0.828187
2017-12-10T04:37:42.463816: step 4490, loss 0.0456096, acc 0.984375, prec 0.0775847, recall 0.828241
2017-12-10T04:37:42.736776: step 4491, loss 0.14671, acc 0.9375, prec 0.0776074, recall 0.828296
2017-12-10T04:37:43.002936: step 4492, loss 0.27516, acc 0.921875, prec 0.0776154, recall 0.828323
2017-12-10T04:37:43.271543: step 4493, loss 0.165381, acc 0.90625, prec 0.0776084, recall 0.828323
2017-12-10T04:37:43.533489: step 4494, loss 0.579972, acc 0.875, prec 0.0775992, recall 0.828323
2017-12-10T04:37:43.806320: step 4495, loss 0.259037, acc 0.921875, prec 0.0776482, recall 0.828431
2017-12-10T04:37:44.076675: step 4496, loss 0.256569, acc 0.9375, prec 0.0776572, recall 0.828458
2017-12-10T04:37:44.341248: step 4497, loss 0.516582, acc 0.875, prec 0.0776617, recall 0.828486
2017-12-10T04:37:44.616898: step 4498, loss 0.352527, acc 0.875, prec 0.0776525, recall 0.828486
2017-12-10T04:37:44.885812: step 4499, loss 0.307202, acc 0.953125, prec 0.0776627, recall 0.828513
2017-12-10T04:37:45.154275: step 4500, loss 0.0797721, acc 0.953125, prec 0.0776593, recall 0.828513

Evaluation:
2017-12-10T04:37:52.733370: step 4500, loss 2.36771, acc 0.906775, prec 0.0778537, recall 0.823938

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-4500

2017-12-10T04:37:54.235828: step 4501, loss 0.477018, acc 0.953125, prec 0.0778503, recall 0.823938
2017-12-10T04:37:54.506606: step 4502, loss 0.426084, acc 0.921875, prec 0.077858, recall 0.823965
2017-12-10T04:37:54.777307: step 4503, loss 0.0970075, acc 0.984375, prec 0.0778703, recall 0.823993
2017-12-10T04:37:55.040689: step 4504, loss 0.316293, acc 0.9375, prec 0.0778658, recall 0.823993
2017-12-10T04:37:55.307198: step 4505, loss 0.48336, acc 0.921875, prec 0.0778736, recall 0.82402
2017-12-10T04:37:55.579271: step 4506, loss 0.648425, acc 0.9375, prec 0.0779094, recall 0.824101
2017-12-10T04:37:55.843995: step 4507, loss 0.473914, acc 0.9375, prec 0.0779183, recall 0.824128
2017-12-10T04:37:56.108026: step 4508, loss 0.158731, acc 0.953125, prec 0.0779149, recall 0.824128
2017-12-10T04:37:56.375832: step 4509, loss 0.119663, acc 0.953125, prec 0.0779115, recall 0.824128
2017-12-10T04:37:56.639310: step 4510, loss 0.22193, acc 0.9375, prec 0.0779204, recall 0.824156
2017-12-10T04:37:56.909770: step 4511, loss 0.304866, acc 0.9375, prec 0.0779158, recall 0.824156
2017-12-10T04:37:57.181868: step 4512, loss 0.43613, acc 0.953125, prec 0.0779259, recall 0.824183
2017-12-10T04:37:57.447474: step 4513, loss 0.252043, acc 0.953125, prec 0.0779493, recall 0.824237
2017-12-10T04:37:57.719490: step 4514, loss 0.127872, acc 0.96875, prec 0.0779471, recall 0.824237
2017-12-10T04:37:57.986287: step 4515, loss 0.195728, acc 0.9375, prec 0.077956, recall 0.824264
2017-12-10T04:37:58.256176: step 4516, loss 0.0145868, acc 1, prec 0.077956, recall 0.824264
2017-12-10T04:37:58.531183: step 4517, loss 0.11651, acc 0.96875, prec 0.0779537, recall 0.824264
2017-12-10T04:37:58.795568: step 4518, loss 0.0745848, acc 0.96875, prec 0.0779514, recall 0.824264
2017-12-10T04:37:59.059447: step 4519, loss 0.163341, acc 0.9375, prec 0.0779469, recall 0.824264
2017-12-10T04:37:59.324646: step 4520, loss 0.0208156, acc 0.984375, prec 0.0779457, recall 0.824264
2017-12-10T04:37:59.588426: step 4521, loss 3.15306, acc 0.984375, prec 0.0779726, recall 0.824191
2017-12-10T04:37:59.859946: step 4522, loss 0.43859, acc 0.96875, prec 0.0779972, recall 0.824245
2017-12-10T04:38:00.137674: step 4523, loss 0.0601119, acc 0.96875, prec 0.0780084, recall 0.824272
2017-12-10T04:38:00.421717: step 4524, loss 0.141366, acc 0.9375, prec 0.0780307, recall 0.824326
2017-12-10T04:38:00.697924: step 4525, loss 0.0609837, acc 0.96875, prec 0.0780284, recall 0.824326
2017-12-10T04:38:00.963874: step 4526, loss 0.0576624, acc 0.96875, prec 0.0780396, recall 0.824353
2017-12-10T04:38:01.226934: step 4527, loss 0.146414, acc 0.96875, prec 0.0780373, recall 0.824353
2017-12-10T04:38:01.496877: step 4528, loss 0.195562, acc 0.953125, prec 0.0780742, recall 0.824435
2017-12-10T04:38:01.760887: step 4529, loss 0.252925, acc 0.921875, prec 0.078082, recall 0.824462
2017-12-10T04:38:02.033428: step 4530, loss 0.218577, acc 0.875, prec 0.0780729, recall 0.824462
2017-12-10T04:38:02.298503: step 4531, loss 0.418728, acc 0.921875, prec 0.0780672, recall 0.824462
2017-12-10T04:38:02.564645: step 4532, loss 0.0280491, acc 1, prec 0.0780672, recall 0.824462
2017-12-10T04:38:02.833449: step 4533, loss 0.109352, acc 0.96875, prec 0.0780783, recall 0.824489
2017-12-10T04:38:03.100420: step 4534, loss 0.407789, acc 0.90625, prec 0.0780849, recall 0.824516
2017-12-10T04:38:03.364565: step 4535, loss 0.430159, acc 0.875, prec 0.0780758, recall 0.824516
2017-12-10T04:38:03.629513: step 4536, loss 0.675201, acc 0.890625, prec 0.0780947, recall 0.82457
2017-12-10T04:38:03.898928: step 4537, loss 0.414337, acc 0.921875, prec 0.0781159, recall 0.824623
2017-12-10T04:38:04.165933: step 4538, loss 0.603197, acc 0.90625, prec 0.0781225, recall 0.82465
2017-12-10T04:38:04.434926: step 4539, loss 0.291078, acc 0.890625, prec 0.0781145, recall 0.82465
2017-12-10T04:38:04.708214: step 4540, loss 0.884908, acc 0.890625, prec 0.07812, recall 0.824677
2017-12-10T04:38:04.982424: step 4541, loss 0.197984, acc 0.953125, prec 0.0781166, recall 0.824677
2017-12-10T04:38:05.246751: step 4542, loss 0.442803, acc 0.9375, prec 0.078112, recall 0.824677
2017-12-10T04:38:05.509998: step 4543, loss 0.277056, acc 0.984375, prec 0.0781377, recall 0.824731
2017-12-10T04:38:05.773744: step 4544, loss 0.130098, acc 0.96875, prec 0.0781489, recall 0.824758
2017-12-10T04:38:06.044275: step 4545, loss 0.0188272, acc 1, prec 0.0781757, recall 0.824812
2017-12-10T04:38:06.306897: step 4546, loss 0.537259, acc 0.953125, prec 0.0781857, recall 0.824839
2017-12-10T04:38:06.583017: step 4547, loss 0.210272, acc 0.953125, prec 0.0782091, recall 0.824893
2017-12-10T04:38:06.854029: step 4548, loss 0.5363, acc 0.890625, prec 0.0782146, recall 0.824919
2017-12-10T04:38:07.116789: step 4549, loss 0.0753845, acc 0.984375, prec 0.0782134, recall 0.824919
2017-12-10T04:38:07.384404: step 4550, loss 0.104062, acc 0.953125, prec 0.07821, recall 0.824919
2017-12-10T04:38:07.652758: step 4551, loss 0.102818, acc 0.953125, prec 0.0782066, recall 0.824919
2017-12-10T04:38:07.923262: step 4552, loss 1.04804, acc 0.921875, prec 0.0782411, recall 0.825
2017-12-10T04:38:08.197600: step 4553, loss 0.174455, acc 0.953125, prec 0.0782645, recall 0.825054
2017-12-10T04:38:08.471885: step 4554, loss 0.146719, acc 0.953125, prec 0.0782611, recall 0.825054
2017-12-10T04:38:08.737971: step 4555, loss 0.158234, acc 0.953125, prec 0.0782711, recall 0.825081
2017-12-10T04:38:09.009756: step 4556, loss 0.0303767, acc 0.984375, prec 0.07827, recall 0.825081
2017-12-10T04:38:09.280452: step 4557, loss 0.0890735, acc 0.984375, prec 0.0782688, recall 0.825081
2017-12-10T04:38:09.550693: step 4558, loss 0.00714835, acc 1, prec 0.0782688, recall 0.825081
2017-12-10T04:38:09.819144: step 4559, loss 0.015901, acc 1, prec 0.0782822, recall 0.825107
2017-12-10T04:38:10.087416: step 4560, loss 0.0215388, acc 0.984375, prec 0.0782811, recall 0.825107
2017-12-10T04:38:10.364595: step 4561, loss 0.0425672, acc 1, prec 0.0783213, recall 0.825188
2017-12-10T04:38:10.637841: step 4562, loss 0.0062316, acc 1, prec 0.0783213, recall 0.825188
2017-12-10T04:38:10.900941: step 4563, loss 0.119342, acc 0.96875, prec 0.0783324, recall 0.825214
2017-12-10T04:38:11.167115: step 4564, loss 0.0951044, acc 0.953125, prec 0.078329, recall 0.825214
2017-12-10T04:38:11.433412: step 4565, loss 0.0114394, acc 1, prec 0.0783424, recall 0.825241
2017-12-10T04:38:11.699378: step 4566, loss 1.47158, acc 0.96875, prec 0.0783681, recall 0.825168
2017-12-10T04:38:11.978981: step 4567, loss 0.0322242, acc 1, prec 0.0784083, recall 0.825249
2017-12-10T04:38:12.252657: step 4568, loss 0.0411856, acc 1, prec 0.0784351, recall 0.825302
2017-12-10T04:38:12.526130: step 4569, loss 0.00961504, acc 1, prec 0.0784351, recall 0.825302
2017-12-10T04:38:12.804757: step 4570, loss 0.0430869, acc 0.984375, prec 0.0784607, recall 0.825356
2017-12-10T04:38:13.068717: step 4571, loss 0.503719, acc 1, prec 0.0784875, recall 0.825409
2017-12-10T04:38:13.343171: step 4572, loss 0.0127285, acc 1, prec 0.0785009, recall 0.825436
2017-12-10T04:38:13.610502: step 4573, loss 0.116696, acc 0.953125, prec 0.0784975, recall 0.825436
2017-12-10T04:38:13.876836: step 4574, loss 0.153721, acc 0.9375, prec 0.0784929, recall 0.825436
2017-12-10T04:38:14.152653: step 4575, loss 0.0293691, acc 0.984375, prec 0.0785052, recall 0.825462
2017-12-10T04:38:14.418548: step 4576, loss 0.0789328, acc 0.984375, prec 0.0785174, recall 0.825489
2017-12-10T04:38:14.689156: step 4577, loss 0.131418, acc 0.953125, prec 0.0785274, recall 0.825516
2017-12-10T04:38:14.954270: step 4578, loss 0.373503, acc 0.890625, prec 0.0785328, recall 0.825542
2017-12-10T04:38:15.217090: step 4579, loss 0.204028, acc 0.953125, prec 0.0785428, recall 0.825569
2017-12-10T04:38:15.489659: step 4580, loss 0.193232, acc 0.90625, prec 0.0785359, recall 0.825569
2017-12-10T04:38:15.753412: step 4581, loss 0.764251, acc 0.84375, prec 0.0785379, recall 0.825596
2017-12-10T04:38:16.019994: step 4582, loss 0.203214, acc 0.921875, prec 0.0785724, recall 0.825675
2017-12-10T04:38:16.287505: step 4583, loss 0.341368, acc 0.90625, prec 0.0785923, recall 0.825729
2017-12-10T04:38:16.552728: step 4584, loss 0.2483, acc 0.921875, prec 0.0786, recall 0.825755
2017-12-10T04:38:16.819214: step 4585, loss 0.236077, acc 0.9375, prec 0.0785954, recall 0.825755
2017-12-10T04:38:17.086597: step 4586, loss 0.0731677, acc 0.984375, prec 0.0786076, recall 0.825782
2017-12-10T04:38:17.351454: step 4587, loss 0.146112, acc 0.96875, prec 0.0786321, recall 0.825835
2017-12-10T04:38:17.625490: step 4588, loss 0.34231, acc 0.921875, prec 0.0786398, recall 0.825862
2017-12-10T04:38:17.888405: step 4589, loss 2.46893, acc 0.921875, prec 0.0786486, recall 0.825762
2017-12-10T04:38:18.163375: step 4590, loss 0.273134, acc 0.984375, prec 0.0786608, recall 0.825789
2017-12-10T04:38:18.428384: step 4591, loss 0.360321, acc 0.90625, prec 0.078654, recall 0.825789
2017-12-10T04:38:18.703350: step 4592, loss 1.39645, acc 0.953125, prec 0.0786517, recall 0.825663
2017-12-10T04:38:18.971342: step 4593, loss 0.292233, acc 0.890625, prec 0.0786437, recall 0.825663
2017-12-10T04:38:19.240158: step 4594, loss 0.285395, acc 0.921875, prec 0.078638, recall 0.825663
2017-12-10T04:38:19.512792: step 4595, loss 0.287868, acc 0.921875, prec 0.0786724, recall 0.825743
2017-12-10T04:38:19.778910: step 4596, loss 0.477318, acc 0.921875, prec 0.0786801, recall 0.825769
2017-12-10T04:38:20.048720: step 4597, loss 0.733293, acc 0.75, prec 0.0786618, recall 0.825769
2017-12-10T04:38:20.311099: step 4598, loss 0.698254, acc 0.859375, prec 0.0786649, recall 0.825796
2017-12-10T04:38:20.572735: step 4599, loss 0.578456, acc 0.828125, prec 0.0786791, recall 0.825849
2017-12-10T04:38:20.836595: step 4600, loss 0.345713, acc 0.875, prec 0.0786833, recall 0.825875
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-4600

2017-12-10T04:38:22.198872: step 4601, loss 0.305576, acc 0.890625, prec 0.0786753, recall 0.825875
2017-12-10T04:38:22.464639: step 4602, loss 0.775629, acc 0.796875, prec 0.0786738, recall 0.825902
2017-12-10T04:38:22.733949: step 4603, loss 0.667261, acc 0.84375, prec 0.0786758, recall 0.825928
2017-12-10T04:38:22.998190: step 4604, loss 0.370205, acc 0.875, prec 0.07868, recall 0.825955
2017-12-10T04:38:23.262336: step 4605, loss 0.387767, acc 0.84375, prec 0.0786686, recall 0.825955
2017-12-10T04:38:23.527465: step 4606, loss 0.32459, acc 0.90625, prec 0.0786885, recall 0.826008
2017-12-10T04:38:23.790532: step 4607, loss 0.306233, acc 0.90625, prec 0.078695, recall 0.826034
2017-12-10T04:38:24.057176: step 4608, loss 0.315213, acc 0.96875, prec 0.0786927, recall 0.826034
2017-12-10T04:38:24.320150: step 4609, loss 0.517755, acc 0.9375, prec 0.0787148, recall 0.826087
2017-12-10T04:38:24.589350: step 4610, loss 0.15714, acc 0.96875, prec 0.0787126, recall 0.826087
2017-12-10T04:38:24.850765: step 4611, loss 0.969699, acc 0.984375, prec 0.0787514, recall 0.826166
2017-12-10T04:38:25.833993: step 4612, loss 0.366518, acc 0.921875, prec 0.0787724, recall 0.826219
2017-12-10T04:38:26.186775: step 4613, loss 0.384635, acc 0.96875, prec 0.0787835, recall 0.826245
2017-12-10T04:38:26.640134: step 4614, loss 0.04053, acc 0.984375, prec 0.0788357, recall 0.826351
2017-12-10T04:38:27.382041: step 4615, loss 0.075847, acc 0.984375, prec 0.0788479, recall 0.826377
2017-12-10T04:38:28.147007: step 4616, loss 0.151856, acc 0.96875, prec 0.0788723, recall 0.82643
2017-12-10T04:38:28.857238: step 4617, loss 0.0801661, acc 0.96875, prec 0.0788834, recall 0.826456
2017-12-10T04:38:29.541467: step 4618, loss 0.254665, acc 0.921875, prec 0.078891, recall 0.826483
2017-12-10T04:38:30.275828: step 4619, loss 0.160454, acc 0.984375, prec 0.0789032, recall 0.826509
2017-12-10T04:38:31.031339: step 4620, loss 0.0202285, acc 1, prec 0.0789032, recall 0.826509
2017-12-10T04:38:31.764107: step 4621, loss 0.0266783, acc 0.984375, prec 0.078902, recall 0.826509
2017-12-10T04:38:32.468118: step 4622, loss 0.255756, acc 0.96875, prec 0.0789131, recall 0.826535
2017-12-10T04:38:33.197537: step 4623, loss 0.0126773, acc 1, prec 0.0789131, recall 0.826535
2017-12-10T04:38:33.934251: step 4624, loss 0.109569, acc 0.953125, prec 0.0789097, recall 0.826535
2017-12-10T04:38:34.677994: step 4625, loss 0.0440765, acc 0.984375, prec 0.0789218, recall 0.826562
2017-12-10T04:38:35.472059: step 4626, loss 0.140096, acc 0.96875, prec 0.0789329, recall 0.826588
2017-12-10T04:38:35.974036: step 4627, loss 0.0121217, acc 1, prec 0.0789329, recall 0.826588
2017-12-10T04:38:36.258051: step 4628, loss 2.0452, acc 0.9375, prec 0.0789428, recall 0.826489
2017-12-10T04:38:36.536716: step 4629, loss 0.109962, acc 0.953125, prec 0.0789527, recall 0.826515
2017-12-10T04:38:36.810908: step 4630, loss 0.0787932, acc 0.984375, prec 0.0789649, recall 0.826541
2017-12-10T04:38:37.078817: step 4631, loss 0.512648, acc 0.875, prec 0.0789691, recall 0.826568
2017-12-10T04:38:37.342670: step 4632, loss 0.390123, acc 0.890625, prec 0.0789744, recall 0.826594
2017-12-10T04:38:37.616095: step 4633, loss 0.050348, acc 0.984375, prec 0.0789866, recall 0.82662
2017-12-10T04:38:37.876446: step 4634, loss 0.317813, acc 0.90625, prec 0.0789797, recall 0.82662
2017-12-10T04:38:38.146544: step 4635, loss 0.787601, acc 0.890625, prec 0.0789717, recall 0.82662
2017-12-10T04:38:38.412336: step 4636, loss 0.224164, acc 0.9375, prec 0.0789672, recall 0.82662
2017-12-10T04:38:38.673061: step 4637, loss 0.5185, acc 0.890625, prec 0.0789725, recall 0.826647
2017-12-10T04:38:38.942967: step 4638, loss 0.177193, acc 0.9375, prec 0.0789812, recall 0.826673
2017-12-10T04:38:39.208095: step 4639, loss 0.322312, acc 0.90625, prec 0.0789877, recall 0.826699
2017-12-10T04:38:39.475337: step 4640, loss 0.147568, acc 0.9375, prec 0.0789965, recall 0.826725
2017-12-10T04:38:39.750082: step 4641, loss 0.476882, acc 0.890625, prec 0.0790018, recall 0.826751
2017-12-10T04:38:40.014422: step 4642, loss 0.261114, acc 0.921875, prec 0.0789961, recall 0.826751
2017-12-10T04:38:40.284345: step 4643, loss 0.325946, acc 0.875, prec 0.0790136, recall 0.826804
2017-12-10T04:38:40.549373: step 4644, loss 0.234894, acc 0.9375, prec 0.0790223, recall 0.82683
2017-12-10T04:38:40.819527: step 4645, loss 0.238396, acc 0.953125, prec 0.0790455, recall 0.826882
2017-12-10T04:38:41.083761: step 4646, loss 0.121536, acc 0.953125, prec 0.0790554, recall 0.826909
2017-12-10T04:38:41.349072: step 4647, loss 0.0950371, acc 0.9375, prec 0.0790774, recall 0.826961
2017-12-10T04:38:41.614417: step 4648, loss 0.161751, acc 0.96875, prec 0.0790751, recall 0.826961
2017-12-10T04:38:41.879798: step 4649, loss 0.143139, acc 0.96875, prec 0.0790729, recall 0.826961
2017-12-10T04:38:42.156773: step 4650, loss 0.0692759, acc 0.953125, prec 0.0790694, recall 0.826961
2017-12-10T04:38:42.422938: step 4651, loss 0.348244, acc 0.96875, prec 0.0790671, recall 0.826961
2017-12-10T04:38:42.689541: step 4652, loss 0.140282, acc 0.953125, prec 0.079077, recall 0.826987
2017-12-10T04:38:42.955648: step 4653, loss 5.00376, acc 0.984375, prec 0.0790903, recall 0.826888
2017-12-10T04:38:43.227389: step 4654, loss 0.152399, acc 0.96875, prec 0.0791147, recall 0.82694
2017-12-10T04:38:43.492534: step 4655, loss 0.0553848, acc 0.984375, prec 0.0791534, recall 0.827019
2017-12-10T04:38:43.763489: step 4656, loss 0.098335, acc 0.984375, prec 0.0791656, recall 0.827045
2017-12-10T04:38:44.035995: step 4657, loss 0.145227, acc 0.9375, prec 0.079161, recall 0.827045
2017-12-10T04:38:44.300752: step 4658, loss 0.0153361, acc 1, prec 0.079161, recall 0.827045
2017-12-10T04:38:44.562533: step 4659, loss 0.327469, acc 0.9375, prec 0.0791697, recall 0.827071
2017-12-10T04:38:44.830298: step 4660, loss 0.316625, acc 0.90625, prec 0.0791629, recall 0.827071
2017-12-10T04:38:45.095095: step 4661, loss 0.0403917, acc 0.984375, prec 0.079175, recall 0.827097
2017-12-10T04:38:45.359459: step 4662, loss 0.55184, acc 0.890625, prec 0.0792069, recall 0.827175
2017-12-10T04:38:45.623515: step 4663, loss 0.213351, acc 0.96875, prec 0.0792046, recall 0.827175
2017-12-10T04:38:45.885324: step 4664, loss 0.0960455, acc 0.96875, prec 0.0792289, recall 0.827227
2017-12-10T04:38:46.150510: step 4665, loss 0.136469, acc 0.984375, prec 0.0792278, recall 0.827227
2017-12-10T04:38:46.415095: step 4666, loss 0.221051, acc 0.9375, prec 0.0792365, recall 0.827254
2017-12-10T04:38:46.680649: step 4667, loss 0.184869, acc 0.96875, prec 0.0792874, recall 0.827358
2017-12-10T04:38:46.943368: step 4668, loss 0.524745, acc 0.921875, prec 0.079295, recall 0.827384
2017-12-10T04:38:47.207478: step 4669, loss 0.160512, acc 0.90625, prec 0.0792881, recall 0.827384
2017-12-10T04:38:47.474703: step 4670, loss 0.132844, acc 0.953125, prec 0.0792847, recall 0.827384
2017-12-10T04:38:47.740582: step 4671, loss 0.348159, acc 0.875, prec 0.0792888, recall 0.82741
2017-12-10T04:38:48.003133: step 4672, loss 0.133435, acc 0.9375, prec 0.0792842, recall 0.82741
2017-12-10T04:38:48.265953: step 4673, loss 0.349491, acc 0.9375, prec 0.0792929, recall 0.827436
2017-12-10T04:38:48.532285: step 4674, loss 0.33476, acc 0.9375, prec 0.0793016, recall 0.827462
2017-12-10T04:38:48.800310: step 4675, loss 0.153913, acc 0.953125, prec 0.0793513, recall 0.827565
2017-12-10T04:38:49.065475: step 4676, loss 0.030928, acc 0.984375, prec 0.0793768, recall 0.827617
2017-12-10T04:38:49.334638: step 4677, loss 0.0819273, acc 0.96875, prec 0.0793745, recall 0.827617
2017-12-10T04:38:49.605537: step 4678, loss 0.164152, acc 0.9375, prec 0.0793832, recall 0.827643
2017-12-10T04:38:49.880839: step 4679, loss 0.0346006, acc 0.984375, prec 0.0793953, recall 0.827669
2017-12-10T04:38:50.146018: step 4680, loss 0.0289491, acc 0.984375, prec 0.0794074, recall 0.827695
2017-12-10T04:38:50.413985: step 4681, loss 0.220019, acc 0.921875, prec 0.0794017, recall 0.827695
2017-12-10T04:38:50.675326: step 4682, loss 0.074422, acc 0.96875, prec 0.0794127, recall 0.827721
2017-12-10T04:38:50.940326: step 4683, loss 0.0401241, acc 0.984375, prec 0.0794381, recall 0.827773
2017-12-10T04:38:51.207406: step 4684, loss 0.017403, acc 1, prec 0.0794514, recall 0.827799
2017-12-10T04:38:51.478536: step 4685, loss 0.0804558, acc 0.984375, prec 0.0794768, recall 0.82785
2017-12-10T04:38:51.740863: step 4686, loss 0.141216, acc 0.96875, prec 0.0794878, recall 0.827876
2017-12-10T04:38:52.004065: step 4687, loss 0.362916, acc 0.984375, prec 0.0794999, recall 0.827902
2017-12-10T04:38:52.274216: step 4688, loss 0.00485595, acc 1, prec 0.0794999, recall 0.827902
2017-12-10T04:38:52.540783: step 4689, loss 0.000546948, acc 1, prec 0.0794999, recall 0.827902
2017-12-10T04:38:52.802603: step 4690, loss 0.487798, acc 0.984375, prec 0.0795253, recall 0.827954
2017-12-10T04:38:53.073137: step 4691, loss 0.118052, acc 0.96875, prec 0.0795363, recall 0.82798
2017-12-10T04:38:53.334009: step 4692, loss 0.0503778, acc 0.984375, prec 0.0795617, recall 0.828031
2017-12-10T04:38:53.606423: step 4693, loss 0.0840219, acc 0.984375, prec 0.0795738, recall 0.828057
2017-12-10T04:38:53.878261: step 4694, loss 0.00589002, acc 1, prec 0.0795738, recall 0.828057
2017-12-10T04:38:54.861394: step 4695, loss 0.033775, acc 0.984375, prec 0.0795727, recall 0.828057
2017-12-10T04:38:55.221345: step 4696, loss 0.0363247, acc 0.984375, prec 0.0795848, recall 0.828083
2017-12-10T04:38:55.938259: step 4697, loss 1.28863, acc 0.9375, prec 0.0795935, recall 0.828109
2017-12-10T04:38:56.890477: step 4698, loss 0.0286011, acc 0.984375, prec 0.0795923, recall 0.828109
2017-12-10T04:38:57.305451: step 4699, loss 0.156849, acc 0.953125, prec 0.0796154, recall 0.82816
2017-12-10T04:38:57.585492: step 4700, loss 0.00420818, acc 1, prec 0.0796287, recall 0.828186
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-4700

2017-12-10T04:38:58.779937: step 4701, loss 0.00986934, acc 1, prec 0.0796287, recall 0.828186
2017-12-10T04:38:59.059460: step 4702, loss 1.22271, acc 0.96875, prec 0.0796275, recall 0.828062
2017-12-10T04:38:59.338745: step 4703, loss 0.0973435, acc 0.984375, prec 0.0796264, recall 0.828062
2017-12-10T04:38:59.604140: step 4704, loss 0.140987, acc 0.96875, prec 0.0796373, recall 0.828088
2017-12-10T04:38:59.871304: step 4705, loss 0.114615, acc 0.953125, prec 0.0796737, recall 0.828165
2017-12-10T04:39:00.140427: step 4706, loss 0.0586152, acc 0.984375, prec 0.0796725, recall 0.828165
2017-12-10T04:39:00.409194: step 4707, loss 0.291192, acc 0.890625, prec 0.0796645, recall 0.828165
2017-12-10T04:39:00.687274: step 4708, loss 0.2766, acc 0.921875, prec 0.0796588, recall 0.828165
2017-12-10T04:39:00.965028: step 4709, loss 0.435778, acc 0.96875, prec 0.0796565, recall 0.828165
2017-12-10T04:39:01.229889: step 4710, loss 0.242186, acc 0.953125, prec 0.079653, recall 0.828165
2017-12-10T04:39:01.497509: step 4711, loss 0.587172, acc 0.875, prec 0.0796571, recall 0.828191
2017-12-10T04:39:01.764186: step 4712, loss 0.104793, acc 0.96875, prec 0.0796681, recall 0.828216
2017-12-10T04:39:02.027969: step 4713, loss 0.173155, acc 0.953125, prec 0.0796911, recall 0.828268
2017-12-10T04:39:02.296504: step 4714, loss 0.336527, acc 0.890625, prec 0.0796964, recall 0.828293
2017-12-10T04:39:02.558037: step 4715, loss 0.396716, acc 0.953125, prec 0.0797459, recall 0.828396
2017-12-10T04:39:02.822329: step 4716, loss 0.485423, acc 0.90625, prec 0.0797788, recall 0.828473
2017-12-10T04:39:03.092050: step 4717, loss 0.0541683, acc 0.984375, prec 0.0797777, recall 0.828473
2017-12-10T04:39:03.363759: step 4718, loss 0.111396, acc 0.984375, prec 0.0797898, recall 0.828499
2017-12-10T04:39:03.628185: step 4719, loss 0.179638, acc 0.9375, prec 0.0798117, recall 0.82855
2017-12-10T04:39:03.893274: step 4720, loss 0.204885, acc 0.953125, prec 0.0798612, recall 0.828653
2017-12-10T04:39:04.161448: step 4721, loss 0.192154, acc 0.953125, prec 0.0798578, recall 0.828653
2017-12-10T04:39:04.437676: step 4722, loss 0.244286, acc 0.921875, prec 0.0798653, recall 0.828678
2017-12-10T04:39:04.701036: step 4723, loss 0.323135, acc 0.921875, prec 0.0798595, recall 0.828678
2017-12-10T04:39:04.965560: step 4724, loss 0.0559253, acc 0.96875, prec 0.0798705, recall 0.828704
2017-12-10T04:39:05.228053: step 4725, loss 0.0945875, acc 0.953125, prec 0.079867, recall 0.828704
2017-12-10T04:39:05.489780: step 4726, loss 0.117388, acc 0.984375, prec 0.0798923, recall 0.828755
2017-12-10T04:39:05.759473: step 4727, loss 0.0357154, acc 0.984375, prec 0.0798912, recall 0.828755
2017-12-10T04:39:06.025839: step 4728, loss 0.194041, acc 0.9375, prec 0.0798998, recall 0.82878
2017-12-10T04:39:06.291150: step 4729, loss 0.007358, acc 1, prec 0.0798998, recall 0.82878
2017-12-10T04:39:06.560706: step 4730, loss 0.1363, acc 0.96875, prec 0.0799108, recall 0.828806
2017-12-10T04:39:06.828827: step 4731, loss 0.279171, acc 0.953125, prec 0.079947, recall 0.828883
2017-12-10T04:39:07.097773: step 4732, loss 0.00763393, acc 1, prec 0.079947, recall 0.828883
2017-12-10T04:39:07.367034: step 4733, loss 0.00350353, acc 1, prec 0.079947, recall 0.828883
2017-12-10T04:39:07.631686: step 4734, loss 0.165755, acc 0.953125, prec 0.0799568, recall 0.828908
2017-12-10T04:39:07.906652: step 4735, loss 0.106618, acc 0.921875, prec 0.0799643, recall 0.828934
2017-12-10T04:39:08.170632: step 4736, loss 1.04267, acc 0.953125, prec 0.0799873, recall 0.828985
2017-12-10T04:39:08.436917: step 4737, loss 0.097482, acc 0.96875, prec 0.080038, recall 0.829087
2017-12-10T04:39:08.706077: step 4738, loss 0.250817, acc 0.953125, prec 0.0800478, recall 0.829112
2017-12-10T04:39:08.970439: step 4739, loss 0.136309, acc 0.921875, prec 0.080042, recall 0.829112
2017-12-10T04:39:09.236860: step 4740, loss 0.272205, acc 0.96875, prec 0.0800529, recall 0.829138
2017-12-10T04:39:09.499591: step 4741, loss 0.211302, acc 0.953125, prec 0.0800495, recall 0.829138
2017-12-10T04:39:09.770236: step 4742, loss 0.0849839, acc 0.984375, prec 0.0800483, recall 0.829138
2017-12-10T04:39:10.038502: step 4743, loss 0.0872472, acc 0.953125, prec 0.0800449, recall 0.829138
2017-12-10T04:39:10.306154: step 4744, loss 0.0689824, acc 0.96875, prec 0.0800558, recall 0.829163
2017-12-10T04:39:10.570886: step 4745, loss 0.151461, acc 0.953125, prec 0.0800523, recall 0.829163
2017-12-10T04:39:10.838106: step 4746, loss 0.168254, acc 0.953125, prec 0.0800753, recall 0.829214
2017-12-10T04:39:11.107847: step 4747, loss 0.196044, acc 0.953125, prec 0.0800719, recall 0.829214
2017-12-10T04:39:11.377982: step 4748, loss 0.132319, acc 0.96875, prec 0.080096, recall 0.829265
2017-12-10T04:39:11.649419: step 4749, loss 0.06272, acc 0.96875, prec 0.0800937, recall 0.829265
2017-12-10T04:39:11.920587: step 4750, loss 0.0623227, acc 0.96875, prec 0.0801047, recall 0.82929
2017-12-10T04:39:12.196612: step 4751, loss 0.0296952, acc 1, prec 0.0801047, recall 0.82929
2017-12-10T04:39:12.467681: step 4752, loss 0.53026, acc 0.953125, prec 0.0801144, recall 0.829315
2017-12-10T04:39:12.736014: step 4753, loss 0.121352, acc 0.96875, prec 0.0801253, recall 0.829341
2017-12-10T04:39:13.003637: step 4754, loss 0.0095194, acc 1, prec 0.0801386, recall 0.829366
2017-12-10T04:39:13.273852: step 4755, loss 0.126636, acc 0.984375, prec 0.0801506, recall 0.829392
2017-12-10T04:39:13.542994: step 4756, loss 0.0959521, acc 0.953125, prec 0.0801472, recall 0.829392
2017-12-10T04:39:13.810795: step 4757, loss 0.168351, acc 0.953125, prec 0.0801437, recall 0.829392
2017-12-10T04:39:14.087298: step 4758, loss 0.0390629, acc 0.96875, prec 0.0801414, recall 0.829392
2017-12-10T04:39:14.358476: step 4759, loss 0.218308, acc 0.96875, prec 0.0801523, recall 0.829417
2017-12-10T04:39:14.631059: step 4760, loss 7.38046, acc 0.9375, prec 0.0801621, recall 0.829319
2017-12-10T04:39:14.906199: step 4761, loss 0.12032, acc 0.96875, prec 0.080173, recall 0.829344
2017-12-10T04:39:15.171696: step 4762, loss 0.0125941, acc 1, prec 0.0801862, recall 0.82937
2017-12-10T04:39:15.432882: step 4763, loss 0.0217167, acc 1, prec 0.0801862, recall 0.82937
2017-12-10T04:39:15.697548: step 4764, loss 0.0661624, acc 0.953125, prec 0.080196, recall 0.829395
2017-12-10T04:39:15.966243: step 4765, loss 0.274369, acc 0.984375, prec 0.0802345, recall 0.829471
2017-12-10T04:39:16.236665: step 4766, loss 0.411774, acc 0.90625, prec 0.0802408, recall 0.829497
2017-12-10T04:39:16.505852: step 4767, loss 0.735628, acc 0.828125, prec 0.0802281, recall 0.829497
2017-12-10T04:39:16.771413: step 4768, loss 0.583975, acc 0.859375, prec 0.0802442, recall 0.829547
2017-12-10T04:39:17.042249: step 4769, loss 0.782582, acc 0.875, prec 0.0802349, recall 0.829547
2017-12-10T04:39:17.308662: step 4770, loss 0.365744, acc 0.84375, prec 0.080263, recall 0.829623
2017-12-10T04:39:17.570786: step 4771, loss 0.369275, acc 0.90625, prec 0.0802561, recall 0.829623
2017-12-10T04:39:17.841898: step 4772, loss 0.166576, acc 0.9375, prec 0.0802779, recall 0.829674
2017-12-10T04:39:18.107449: step 4773, loss 1.07369, acc 0.84375, prec 0.0802664, recall 0.829674
2017-12-10T04:39:18.376475: step 4774, loss 0.29044, acc 0.953125, prec 0.0802762, recall 0.829699
2017-12-10T04:39:18.644998: step 4775, loss 0.804931, acc 0.859375, prec 0.0802658, recall 0.829699
2017-12-10T04:39:18.909599: step 4776, loss 0.209563, acc 0.890625, prec 0.0802973, recall 0.829775
2017-12-10T04:39:19.175266: step 4777, loss 0.0633263, acc 0.984375, prec 0.0803226, recall 0.829825
2017-12-10T04:39:19.444932: step 4778, loss 0.0268601, acc 0.984375, prec 0.0803346, recall 0.82985
2017-12-10T04:39:19.714978: step 4779, loss 0.760513, acc 1, prec 0.0803478, recall 0.829876
2017-12-10T04:39:19.992901: step 4780, loss 0.130487, acc 0.96875, prec 0.0803455, recall 0.829876
2017-12-10T04:39:20.259741: step 4781, loss 0.234214, acc 0.953125, prec 0.0803552, recall 0.829901
2017-12-10T04:39:20.525837: step 4782, loss 0.939166, acc 0.984375, prec 0.0803673, recall 0.829926
2017-12-10T04:39:20.799992: step 4783, loss 0.301611, acc 0.921875, prec 0.0803747, recall 0.829951
2017-12-10T04:39:21.074497: step 4784, loss 0.187751, acc 0.953125, prec 0.0803712, recall 0.829951
2017-12-10T04:39:21.348636: step 4785, loss 0.315626, acc 0.890625, prec 0.0803895, recall 0.830001
2017-12-10T04:39:21.618363: step 4786, loss 0.514871, acc 0.859375, prec 0.0803924, recall 0.830027
2017-12-10T04:39:21.892434: step 4787, loss 0.355169, acc 0.90625, prec 0.0803854, recall 0.830027
2017-12-10T04:39:22.162190: step 4788, loss 0.527164, acc 0.859375, prec 0.0804146, recall 0.830102
2017-12-10T04:39:22.428831: step 4789, loss 0.136738, acc 0.953125, prec 0.0804507, recall 0.830177
2017-12-10T04:39:22.701127: step 4790, loss 0.327188, acc 0.921875, prec 0.0804581, recall 0.830203
2017-12-10T04:39:22.968019: step 4791, loss 0.421032, acc 0.890625, prec 0.0804501, recall 0.830203
2017-12-10T04:39:23.236101: step 4792, loss 0.320055, acc 0.90625, prec 0.0804431, recall 0.830203
2017-12-10T04:39:23.500997: step 4793, loss 0.167214, acc 0.9375, prec 0.0804385, recall 0.830203
2017-12-10T04:39:23.767622: step 4794, loss 0.188711, acc 0.921875, prec 0.0804459, recall 0.830228
2017-12-10T04:39:24.032718: step 4795, loss 0.586816, acc 0.890625, prec 0.0804642, recall 0.830278
2017-12-10T04:39:24.300931: step 4796, loss 0.312926, acc 0.921875, prec 0.0804716, recall 0.830303
2017-12-10T04:39:24.563316: step 4797, loss 0.161238, acc 0.96875, prec 0.0804825, recall 0.830328
2017-12-10T04:39:24.830236: step 4798, loss 0.134097, acc 0.953125, prec 0.080479, recall 0.830328
2017-12-10T04:39:25.102565: step 4799, loss 0.196867, acc 0.953125, prec 0.0804756, recall 0.830328
2017-12-10T04:39:25.368310: step 4800, loss 0.0678004, acc 0.984375, prec 0.0804744, recall 0.830328

Evaluation:
2017-12-10T04:39:32.997858: step 4800, loss 4.01704, acc 0.952255, prec 0.0808046, recall 0.821522

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-4800

2017-12-10T04:39:34.280928: step 4801, loss 0.108703, acc 0.96875, prec 0.0808154, recall 0.821547
2017-12-10T04:39:34.554377: step 4802, loss 0.148417, acc 0.96875, prec 0.0808262, recall 0.821573
2017-12-10T04:39:34.821612: step 4803, loss 0.248527, acc 0.96875, prec 0.0808239, recall 0.821573
2017-12-10T04:39:35.090898: step 4804, loss 0.126503, acc 0.96875, prec 0.0808347, recall 0.821599
2017-12-10T04:39:35.355994: step 4805, loss 0.197712, acc 0.96875, prec 0.0808454, recall 0.821625
2017-12-10T04:39:35.626149: step 4806, loss 0.426182, acc 0.96875, prec 0.0808562, recall 0.821651
2017-12-10T04:39:35.900210: step 4807, loss 0.0313865, acc 1, prec 0.0808693, recall 0.821676
2017-12-10T04:39:36.167689: step 4808, loss 0.0408448, acc 0.984375, prec 0.0808681, recall 0.821676
2017-12-10T04:39:36.434926: step 4809, loss 0.288409, acc 0.9375, prec 0.0808766, recall 0.821702
2017-12-10T04:39:36.699848: step 4810, loss 0.0790833, acc 0.984375, prec 0.0808885, recall 0.821728
2017-12-10T04:39:36.967853: step 4811, loss 0.0302916, acc 1, prec 0.0809016, recall 0.821754
2017-12-10T04:39:37.238024: step 4812, loss 1.12369, acc 0.96875, prec 0.0809254, recall 0.821805
2017-12-10T04:39:37.508049: step 4813, loss 0.29318, acc 0.96875, prec 0.0809231, recall 0.821805
2017-12-10T04:39:37.771838: step 4814, loss 0.32403, acc 1, prec 0.0809362, recall 0.821831
2017-12-10T04:39:38.038511: step 4815, loss 0.492048, acc 0.96875, prec 0.0809731, recall 0.821908
2017-12-10T04:39:38.312601: step 4816, loss 0.215113, acc 0.9375, prec 0.0809816, recall 0.821934
2017-12-10T04:39:38.582197: step 4817, loss 0.116875, acc 0.984375, prec 0.0809804, recall 0.821934
2017-12-10T04:39:38.847335: step 4818, loss 0.0625962, acc 0.984375, prec 0.0809793, recall 0.821934
2017-12-10T04:39:39.110404: step 4819, loss 1.67487, acc 0.953125, prec 0.08099, recall 0.821841
2017-12-10T04:39:39.387782: step 4820, loss 0.103141, acc 0.96875, prec 0.0809877, recall 0.821841
2017-12-10T04:39:39.652340: step 4821, loss 0.0883412, acc 0.984375, prec 0.0809866, recall 0.821841
2017-12-10T04:39:39.915135: step 4822, loss 0.444768, acc 0.890625, prec 0.0809785, recall 0.821841
2017-12-10T04:39:40.176864: step 4823, loss 0.308612, acc 0.953125, prec 0.0810012, recall 0.821892
2017-12-10T04:39:40.442373: step 4824, loss 0.257566, acc 0.9375, prec 0.0810096, recall 0.821918
2017-12-10T04:39:40.704042: step 4825, loss 0.391306, acc 0.890625, prec 0.0810277, recall 0.821969
2017-12-10T04:39:40.973136: step 4826, loss 0.393857, acc 0.890625, prec 0.0810327, recall 0.821995
2017-12-10T04:39:41.236474: step 4827, loss 0.600978, acc 0.875, prec 0.0810365, recall 0.82202
2017-12-10T04:39:41.502249: step 4828, loss 0.405628, acc 0.921875, prec 0.0810438, recall 0.822046
2017-12-10T04:39:41.769325: step 4829, loss 0.350652, acc 0.875, prec 0.0810607, recall 0.822097
2017-12-10T04:39:42.039251: step 4830, loss 0.620466, acc 0.921875, prec 0.081055, recall 0.822097
2017-12-10T04:39:42.310884: step 4831, loss 0.385756, acc 0.90625, prec 0.0810481, recall 0.822097
2017-12-10T04:39:42.577445: step 4832, loss 0.213404, acc 0.921875, prec 0.0810423, recall 0.822097
2017-12-10T04:39:42.842298: step 4833, loss 0.454354, acc 0.921875, prec 0.0810496, recall 0.822123
2017-12-10T04:39:43.111298: step 4834, loss 0.506577, acc 0.890625, prec 0.0810546, recall 0.822149
2017-12-10T04:39:43.385190: step 4835, loss 0.474733, acc 0.953125, prec 0.0810772, recall 0.8222
2017-12-10T04:39:43.651127: step 4836, loss 0.19631, acc 0.9375, prec 0.0810857, recall 0.822225
2017-12-10T04:39:43.915934: step 4837, loss 0.365449, acc 0.90625, prec 0.0810918, recall 0.822251
2017-12-10T04:39:44.181648: step 4838, loss 0.216713, acc 0.9375, prec 0.0811003, recall 0.822277
2017-12-10T04:39:44.451585: step 4839, loss 0.19312, acc 0.9375, prec 0.0810957, recall 0.822277
2017-12-10T04:39:44.720682: step 4840, loss 0.0506045, acc 0.984375, prec 0.0810945, recall 0.822277
2017-12-10T04:39:44.994040: step 4841, loss 0.0789644, acc 0.953125, prec 0.0811302, recall 0.822353
2017-12-10T04:39:45.256018: step 4842, loss 0.0685953, acc 0.984375, prec 0.0811681, recall 0.82243
2017-12-10T04:39:45.520136: step 4843, loss 0.259739, acc 0.9375, prec 0.0811766, recall 0.822455
2017-12-10T04:39:45.785294: step 4844, loss 0.238347, acc 0.921875, prec 0.0811708, recall 0.822455
2017-12-10T04:39:46.054424: step 4845, loss 0.0701559, acc 0.984375, prec 0.0811697, recall 0.822455
2017-12-10T04:39:46.320477: step 4846, loss 0.100073, acc 0.953125, prec 0.0811662, recall 0.822455
2017-12-10T04:39:46.588479: step 4847, loss 0.253915, acc 0.953125, prec 0.0811628, recall 0.822455
2017-12-10T04:39:46.853775: step 4848, loss 0.212831, acc 0.984375, prec 0.0812007, recall 0.822532
2017-12-10T04:39:47.121675: step 4849, loss 0.0779146, acc 0.984375, prec 0.0811995, recall 0.822532
2017-12-10T04:39:47.388798: step 4850, loss 0.0271284, acc 0.984375, prec 0.0811984, recall 0.822532
2017-12-10T04:39:47.659982: step 4851, loss 0.160476, acc 0.953125, prec 0.081208, recall 0.822557
2017-12-10T04:39:47.924237: step 4852, loss 1.01946, acc 1, prec 0.081221, recall 0.822583
2017-12-10T04:39:48.191836: step 4853, loss 4.57476, acc 0.953125, prec 0.0812578, recall 0.822541
2017-12-10T04:39:48.464494: step 4854, loss 0.0606409, acc 0.96875, prec 0.0812685, recall 0.822567
2017-12-10T04:39:48.736053: step 4855, loss 0.3755, acc 0.953125, prec 0.0812651, recall 0.822567
2017-12-10T04:39:49.004459: step 4856, loss 0.241075, acc 0.9375, prec 0.0812605, recall 0.822567
2017-12-10T04:39:49.268725: step 4857, loss 0.134022, acc 0.96875, prec 0.0812712, recall 0.822592
2017-12-10T04:39:49.542121: step 4858, loss 0.514886, acc 0.9375, prec 0.0812796, recall 0.822618
2017-12-10T04:39:49.812605: step 4859, loss 0.303691, acc 0.921875, prec 0.0812869, recall 0.822643
2017-12-10T04:39:50.076242: step 4860, loss 0.188656, acc 0.9375, prec 0.0812823, recall 0.822643
2017-12-10T04:39:50.345516: step 4861, loss 0.17422, acc 0.9375, prec 0.0813037, recall 0.822694
2017-12-10T04:39:50.615271: step 4862, loss 0.656137, acc 0.875, prec 0.0812945, recall 0.822694
2017-12-10T04:39:50.879853: step 4863, loss 0.697836, acc 0.875, prec 0.0812983, recall 0.822719
2017-12-10T04:39:51.151385: step 4864, loss 0.333626, acc 0.84375, prec 0.0813128, recall 0.82277
2017-12-10T04:39:51.418268: step 4865, loss 0.790025, acc 0.765625, prec 0.0813085, recall 0.822796
2017-12-10T04:39:51.682367: step 4866, loss 0.773261, acc 0.90625, prec 0.0813146, recall 0.822821
2017-12-10T04:39:51.947707: step 4867, loss 0.774257, acc 0.875, prec 0.0813054, recall 0.822821
2017-12-10T04:39:52.214775: step 4868, loss 0.544563, acc 0.84375, prec 0.0813199, recall 0.822872
2017-12-10T04:39:52.492563: step 4869, loss 0.369018, acc 0.90625, prec 0.081339, recall 0.822923
2017-12-10T04:39:52.766890: step 4870, loss 0.212849, acc 0.9375, prec 0.0813344, recall 0.822923
2017-12-10T04:39:53.041153: step 4871, loss 0.35558, acc 0.9375, prec 0.0813298, recall 0.822923
2017-12-10T04:39:53.309976: step 4872, loss 0.503089, acc 0.921875, prec 0.0813501, recall 0.822973
2017-12-10T04:39:53.572461: step 4873, loss 0.339523, acc 0.90625, prec 0.0813432, recall 0.822973
2017-12-10T04:39:53.835905: step 4874, loss 0.319141, acc 0.9375, prec 0.0813646, recall 0.823024
2017-12-10T04:39:54.111623: step 4875, loss 0.537058, acc 0.953125, prec 0.0813741, recall 0.823049
2017-12-10T04:39:54.379336: step 4876, loss 0.238876, acc 0.96875, prec 0.0813718, recall 0.823049
2017-12-10T04:39:54.648645: step 4877, loss 0.229015, acc 0.953125, prec 0.0813684, recall 0.823049
2017-12-10T04:39:54.918990: step 4878, loss 0.110054, acc 0.953125, prec 0.0813649, recall 0.823049
2017-12-10T04:39:55.192317: step 4879, loss 0.0154201, acc 1, prec 0.0813649, recall 0.823049
2017-12-10T04:39:55.460812: step 4880, loss 0.0747403, acc 0.96875, prec 0.0813756, recall 0.823075
2017-12-10T04:39:55.724080: step 4881, loss 5.04157, acc 0.953125, prec 0.0814123, recall 0.823033
2017-12-10T04:39:56.007462: step 4882, loss 0.102157, acc 0.96875, prec 0.08141, recall 0.823033
2017-12-10T04:39:56.271463: step 4883, loss 0.419058, acc 0.953125, prec 0.0814195, recall 0.823058
2017-12-10T04:39:56.536614: step 4884, loss 0.203889, acc 0.953125, prec 0.0814161, recall 0.823058
2017-12-10T04:39:56.805171: step 4885, loss 0.128009, acc 0.953125, prec 0.0814126, recall 0.823058
2017-12-10T04:39:57.081510: step 4886, loss 0.107939, acc 0.96875, prec 0.0814103, recall 0.823058
2017-12-10T04:39:57.345745: step 4887, loss 0.222735, acc 0.921875, prec 0.0814046, recall 0.823058
2017-12-10T04:39:57.615972: step 4888, loss 0.13421, acc 0.96875, prec 0.0814023, recall 0.823058
2017-12-10T04:39:57.880899: step 4889, loss 0.0713757, acc 0.984375, prec 0.0814141, recall 0.823084
2017-12-10T04:39:58.146767: step 4890, loss 0.479795, acc 0.921875, prec 0.0814213, recall 0.823109
2017-12-10T04:39:58.413843: step 4891, loss 0.230107, acc 0.96875, prec 0.081432, recall 0.823134
2017-12-10T04:39:58.684896: step 4892, loss 0.0451963, acc 0.984375, prec 0.0814569, recall 0.823185
2017-12-10T04:39:58.947493: step 4893, loss 0.250647, acc 0.9375, prec 0.0814652, recall 0.82321
2017-12-10T04:39:59.212647: step 4894, loss 0.589344, acc 0.890625, prec 0.0814572, recall 0.82321
2017-12-10T04:39:59.475955: step 4895, loss 0.724369, acc 0.859375, prec 0.0814988, recall 0.823311
2017-12-10T04:39:59.744696: step 4896, loss 0.165633, acc 0.953125, prec 0.0815213, recall 0.823361
2017-12-10T04:40:00.017961: step 4897, loss 0.219427, acc 0.921875, prec 0.0815155, recall 0.823361
2017-12-10T04:40:00.305699: step 4898, loss 0.355448, acc 0.890625, prec 0.0815075, recall 0.823361
2017-12-10T04:40:00.579980: step 4899, loss 0.13478, acc 0.953125, prec 0.081517, recall 0.823387
2017-12-10T04:40:00.844863: step 4900, loss 0.122645, acc 0.984375, prec 0.0815288, recall 0.823412
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-4900

2017-12-10T04:40:02.140691: step 4901, loss 0.172586, acc 0.9375, prec 0.0815242, recall 0.823412
2017-12-10T04:40:02.408167: step 4902, loss 0.210657, acc 0.953125, prec 0.0815467, recall 0.823462
2017-12-10T04:40:02.674002: step 4903, loss 0.140913, acc 0.96875, prec 0.0815444, recall 0.823462
2017-12-10T04:40:02.942886: step 4904, loss 0.243024, acc 0.921875, prec 0.0815516, recall 0.823487
2017-12-10T04:40:03.211934: step 4905, loss 0.0361898, acc 0.984375, prec 0.0815764, recall 0.823538
2017-12-10T04:40:03.481672: step 4906, loss 0.0288187, acc 0.984375, prec 0.0815753, recall 0.823538
2017-12-10T04:40:03.747748: step 4907, loss 0.0615005, acc 0.984375, prec 0.0815871, recall 0.823563
2017-12-10T04:40:04.014042: step 4908, loss 0.0407991, acc 0.96875, prec 0.0816237, recall 0.823638
2017-12-10T04:40:04.287656: step 4909, loss 0.00591682, acc 1, prec 0.0816237, recall 0.823638
2017-12-10T04:40:04.556835: step 4910, loss 0.292486, acc 0.984375, prec 0.0816355, recall 0.823664
2017-12-10T04:40:04.830400: step 4911, loss 0.0178419, acc 1, prec 0.0816355, recall 0.823664
2017-12-10T04:40:05.099417: step 4912, loss 0.155715, acc 0.953125, prec 0.0816321, recall 0.823664
2017-12-10T04:40:05.365894: step 4913, loss 0.215111, acc 0.984375, prec 0.0816439, recall 0.823689
2017-12-10T04:40:05.633418: step 4914, loss 0.0156909, acc 1, prec 0.0816569, recall 0.823714
2017-12-10T04:40:05.896906: step 4915, loss 0.208744, acc 0.9375, prec 0.0816652, recall 0.823739
2017-12-10T04:40:06.169059: step 4916, loss 0.164076, acc 0.984375, prec 0.08169, recall 0.823789
2017-12-10T04:40:06.431563: step 4917, loss 0.00168072, acc 1, prec 0.08169, recall 0.823789
2017-12-10T04:40:06.701799: step 4918, loss 0.0107684, acc 1, prec 0.081703, recall 0.823814
2017-12-10T04:40:06.966011: step 4919, loss 0.0673657, acc 0.96875, prec 0.0817266, recall 0.823864
2017-12-10T04:40:07.232070: step 4920, loss 1.01844, acc 0.984375, prec 0.0817384, recall 0.823889
2017-12-10T04:40:07.505126: step 4921, loss 0.105574, acc 0.96875, prec 0.0817361, recall 0.823889
2017-12-10T04:40:07.769943: step 4922, loss 0.15781, acc 0.96875, prec 0.0817338, recall 0.823889
2017-12-10T04:40:08.034808: step 4923, loss 0.0534713, acc 0.984375, prec 0.0817327, recall 0.823889
2017-12-10T04:40:08.297025: step 4924, loss 0.316096, acc 1, prec 0.0817586, recall 0.82394
2017-12-10T04:40:08.567370: step 4925, loss 0.182143, acc 0.953125, prec 0.081794, recall 0.824015
2017-12-10T04:40:08.838345: step 4926, loss 0.447487, acc 0.96875, prec 0.0818047, recall 0.82404
2017-12-10T04:40:09.107550: step 4927, loss 0.181889, acc 0.984375, prec 0.0818035, recall 0.82404
2017-12-10T04:40:09.378090: step 4928, loss 0.249371, acc 0.9375, prec 0.0817989, recall 0.82404
2017-12-10T04:40:09.657039: step 4929, loss 0.199682, acc 0.953125, prec 0.0817955, recall 0.82404
2017-12-10T04:40:09.921563: step 4930, loss 0.361598, acc 0.984375, prec 0.0818332, recall 0.824115
2017-12-10T04:40:10.194829: step 4931, loss 0.121762, acc 0.96875, prec 0.0818439, recall 0.82414
2017-12-10T04:40:10.463800: step 4932, loss 0.0178441, acc 1, prec 0.0818439, recall 0.82414
2017-12-10T04:40:10.727650: step 4933, loss 1.4464, acc 0.953125, prec 0.0818545, recall 0.824048
2017-12-10T04:40:10.997760: step 4934, loss 0.0206266, acc 1, prec 0.0818675, recall 0.824073
2017-12-10T04:40:11.262315: step 4935, loss 0.36884, acc 0.921875, prec 0.0818746, recall 0.824098
2017-12-10T04:40:11.533982: step 4936, loss 0.385476, acc 0.921875, prec 0.0818689, recall 0.824098
2017-12-10T04:40:11.797938: step 4937, loss 0.0479541, acc 0.984375, prec 0.0818677, recall 0.824098
2017-12-10T04:40:12.072649: step 4938, loss 0.28646, acc 0.9375, prec 0.0818631, recall 0.824098
2017-12-10T04:40:12.336663: step 4939, loss 0.112142, acc 0.96875, prec 0.0818737, recall 0.824123
2017-12-10T04:40:12.608240: step 4940, loss 0.331171, acc 0.984375, prec 0.0818855, recall 0.824148
2017-12-10T04:40:12.880121: step 4941, loss 0.0672196, acc 0.96875, prec 0.0818962, recall 0.824173
2017-12-10T04:40:13.147997: step 4942, loss 0.288026, acc 0.90625, prec 0.0819152, recall 0.824223
2017-12-10T04:40:13.415188: step 4943, loss 0.327222, acc 0.890625, prec 0.0819589, recall 0.824322
2017-12-10T04:40:13.683369: step 4944, loss 0.293918, acc 0.9375, prec 0.0819543, recall 0.824322
2017-12-10T04:40:13.947168: step 4945, loss 0.0535416, acc 0.96875, prec 0.0819519, recall 0.824322
2017-12-10T04:40:14.220891: step 4946, loss 0.359351, acc 0.953125, prec 0.0819485, recall 0.824322
2017-12-10T04:40:14.484099: step 4947, loss 0.271493, acc 0.90625, prec 0.0819415, recall 0.824322
2017-12-10T04:40:14.747382: step 4948, loss 0.110567, acc 0.9375, prec 0.0819369, recall 0.824322
2017-12-10T04:40:15.016986: step 4949, loss 0.412377, acc 0.90625, prec 0.0819688, recall 0.824397
2017-12-10T04:40:15.284246: step 4950, loss 0.11302, acc 0.953125, prec 0.0819654, recall 0.824397
2017-12-10T04:40:15.549475: step 4951, loss 0.0687101, acc 0.96875, prec 0.0819889, recall 0.824447
2017-12-10T04:40:15.822882: step 4952, loss 0.460227, acc 0.890625, prec 0.0820067, recall 0.824497
2017-12-10T04:40:16.092186: step 4953, loss 0.155809, acc 0.9375, prec 0.0820151, recall 0.824522
2017-12-10T04:40:16.363703: step 4954, loss 0.130382, acc 0.9375, prec 0.0820234, recall 0.824546
2017-12-10T04:40:16.627382: step 4955, loss 0.233518, acc 0.921875, prec 0.0820176, recall 0.824546
2017-12-10T04:40:16.895706: step 4956, loss 0.0463191, acc 0.96875, prec 0.0820153, recall 0.824546
2017-12-10T04:40:17.165741: step 4957, loss 0.163898, acc 0.953125, prec 0.0820506, recall 0.824621
2017-12-10T04:40:17.432054: step 4958, loss 0.114132, acc 0.9375, prec 0.0820848, recall 0.824696
2017-12-10T04:40:17.700202: step 4959, loss 0.601733, acc 0.984375, prec 0.0820966, recall 0.82472
2017-12-10T04:40:17.978520: step 4960, loss 0.157248, acc 0.953125, prec 0.0820931, recall 0.82472
2017-12-10T04:40:18.243210: step 4961, loss 0.112506, acc 0.96875, prec 0.0821038, recall 0.824745
2017-12-10T04:40:18.510875: step 4962, loss 0.0768714, acc 0.96875, prec 0.0821144, recall 0.82477
2017-12-10T04:40:18.772835: step 4963, loss 0.306469, acc 0.921875, prec 0.0821086, recall 0.82477
2017-12-10T04:40:19.052361: step 4964, loss 0.260289, acc 0.9375, prec 0.082104, recall 0.82477
2017-12-10T04:40:19.323293: step 4965, loss 0.0748606, acc 0.953125, prec 0.0821134, recall 0.824795
2017-12-10T04:40:19.599724: step 4966, loss 0.231759, acc 0.953125, prec 0.08211, recall 0.824795
2017-12-10T04:40:19.874128: step 4967, loss 0.0512663, acc 0.953125, prec 0.0821194, recall 0.82482
2017-12-10T04:40:20.146520: step 4968, loss 0.048044, acc 0.984375, prec 0.0821183, recall 0.82482
2017-12-10T04:40:20.411261: step 4969, loss 0.185542, acc 0.953125, prec 0.0821277, recall 0.824844
2017-12-10T04:40:20.645507: step 4970, loss 0.0111064, acc 1, prec 0.0821277, recall 0.824844
2017-12-10T04:40:20.915931: step 4971, loss 0.109452, acc 0.953125, prec 0.0821242, recall 0.824844
2017-12-10T04:40:21.190259: step 4972, loss 0.0542956, acc 0.984375, prec 0.0821231, recall 0.824844
2017-12-10T04:40:21.463665: step 4973, loss 0.0142435, acc 1, prec 0.082136, recall 0.824869
2017-12-10T04:40:21.725169: step 4974, loss 0.127515, acc 0.96875, prec 0.0821466, recall 0.824894
2017-12-10T04:40:21.993179: step 4975, loss 0.0471826, acc 0.96875, prec 0.0821572, recall 0.824919
2017-12-10T04:40:22.260375: step 4976, loss 0.326694, acc 0.9375, prec 0.0821526, recall 0.824919
2017-12-10T04:40:22.528339: step 4977, loss 0.0645298, acc 0.984375, prec 0.0821644, recall 0.824943
2017-12-10T04:40:22.801762: step 4978, loss 0.29426, acc 0.96875, prec 0.0821879, recall 0.824993
2017-12-10T04:40:23.071470: step 4979, loss 0.0777394, acc 0.96875, prec 0.0822115, recall 0.825042
2017-12-10T04:40:23.340066: step 4980, loss 0.122994, acc 0.984375, prec 0.0822103, recall 0.825042
2017-12-10T04:40:23.604601: step 4981, loss 0.236873, acc 0.96875, prec 0.0822209, recall 0.825067
2017-12-10T04:40:23.871200: step 4982, loss 0.0860791, acc 0.953125, prec 0.0822304, recall 0.825092
2017-12-10T04:40:24.137905: step 4983, loss 0.0572241, acc 0.984375, prec 0.0822292, recall 0.825092
2017-12-10T04:40:24.415000: step 4984, loss 0.0876108, acc 0.984375, prec 0.082241, recall 0.825117
2017-12-10T04:40:24.691685: step 4985, loss 0.00887136, acc 1, prec 0.082241, recall 0.825117
2017-12-10T04:40:24.961929: step 4986, loss 0.00289355, acc 1, prec 0.0822539, recall 0.825141
2017-12-10T04:40:25.230876: step 4987, loss 0.172858, acc 0.96875, prec 0.0822516, recall 0.825141
2017-12-10T04:40:25.498091: step 4988, loss 0.00840148, acc 1, prec 0.0822516, recall 0.825141
2017-12-10T04:40:25.768880: step 4989, loss 0.117859, acc 0.96875, prec 0.0822622, recall 0.825166
2017-12-10T04:40:26.035669: step 4990, loss 0.0601052, acc 0.984375, prec 0.082261, recall 0.825166
2017-12-10T04:40:26.306282: step 4991, loss 0.0614697, acc 0.984375, prec 0.0822599, recall 0.825166
2017-12-10T04:40:26.572649: step 4992, loss 0.0677902, acc 0.984375, prec 0.0822587, recall 0.825166
2017-12-10T04:40:26.845242: step 4993, loss 0.453479, acc 0.984375, prec 0.0822834, recall 0.825215
2017-12-10T04:40:27.123129: step 4994, loss 0.00486032, acc 1, prec 0.0823092, recall 0.825265
2017-12-10T04:40:27.385642: step 4995, loss 0.00664308, acc 1, prec 0.0823092, recall 0.825265
2017-12-10T04:40:27.648634: step 4996, loss 0.0163319, acc 0.984375, prec 0.082321, recall 0.825289
2017-12-10T04:40:27.919048: step 4997, loss 0.0486864, acc 0.96875, prec 0.0823316, recall 0.825314
2017-12-10T04:40:28.185583: step 4998, loss 0.130054, acc 0.984375, prec 0.0823433, recall 0.825339
2017-12-10T04:40:28.449220: step 4999, loss 0.204999, acc 0.984375, prec 0.0823422, recall 0.825339
2017-12-10T04:40:28.714042: step 5000, loss 0.00353348, acc 1, prec 0.0823422, recall 0.825339
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-5000

2017-12-10T04:40:30.021521: step 5001, loss 0.0589782, acc 0.984375, prec 0.0823669, recall 0.825388
2017-12-10T04:40:30.296105: step 5002, loss 0.0168718, acc 1, prec 0.0823669, recall 0.825388
2017-12-10T04:40:30.569238: step 5003, loss 0.173955, acc 0.984375, prec 0.0823657, recall 0.825388
2017-12-10T04:40:30.836940: step 5004, loss 0.100613, acc 0.984375, prec 0.0823645, recall 0.825388
2017-12-10T04:40:31.112601: step 5005, loss 0.114708, acc 0.953125, prec 0.082374, recall 0.825413
2017-12-10T04:40:31.385553: step 5006, loss 0.0946578, acc 1, prec 0.0823998, recall 0.825462
2017-12-10T04:40:31.654134: step 5007, loss 0.0777377, acc 0.984375, prec 0.0824115, recall 0.825486
2017-12-10T04:40:31.923733: step 5008, loss 0.0147627, acc 1, prec 0.0824115, recall 0.825486
2017-12-10T04:40:32.194512: step 5009, loss 4.23081e-05, acc 1, prec 0.0824115, recall 0.825486
2017-12-10T04:40:32.457665: step 5010, loss 0.279572, acc 0.953125, prec 0.0824081, recall 0.825486
2017-12-10T04:40:32.722529: step 5011, loss 0.0412617, acc 1, prec 0.082421, recall 0.825511
2017-12-10T04:40:32.991142: step 5012, loss 0.138784, acc 0.953125, prec 0.0824304, recall 0.825536
2017-12-10T04:40:33.256370: step 5013, loss 0.077291, acc 0.984375, prec 0.0824293, recall 0.825536
2017-12-10T04:40:33.524870: step 5014, loss 0.13215, acc 0.96875, prec 0.0824269, recall 0.825536
2017-12-10T04:40:33.790325: step 5015, loss 0.324992, acc 0.984375, prec 0.0824387, recall 0.82556
2017-12-10T04:40:34.062931: step 5016, loss 0.0709983, acc 0.96875, prec 0.0824493, recall 0.825585
2017-12-10T04:40:34.329358: step 5017, loss 0.00673275, acc 1, prec 0.0824622, recall 0.825609
2017-12-10T04:40:34.594430: step 5018, loss 0.0477162, acc 0.96875, prec 0.0824728, recall 0.825634
2017-12-10T04:40:34.859676: step 5019, loss 0.0525685, acc 1, prec 0.0824857, recall 0.825658
2017-12-10T04:40:35.127005: step 5020, loss 0.0169323, acc 1, prec 0.0825115, recall 0.825707
2017-12-10T04:40:35.399114: step 5021, loss 1.51718, acc 0.96875, prec 0.0825103, recall 0.825591
2017-12-10T04:40:35.665942: step 5022, loss 0.0145789, acc 1, prec 0.0825103, recall 0.825591
2017-12-10T04:40:35.936130: step 5023, loss 0.069922, acc 0.984375, prec 0.0825092, recall 0.825591
2017-12-10T04:40:36.208505: step 5024, loss 0.17847, acc 0.96875, prec 0.0825198, recall 0.825616
2017-12-10T04:40:36.475329: step 5025, loss 0.273747, acc 0.953125, prec 0.0825163, recall 0.825616
2017-12-10T04:40:37.441795: step 5026, loss 0.116618, acc 0.953125, prec 0.0825257, recall 0.82564
2017-12-10T04:40:37.799814: step 5027, loss 0.139445, acc 0.953125, prec 0.0825351, recall 0.825665
2017-12-10T04:40:38.074573: step 5028, loss 0.270266, acc 0.9375, prec 0.0825434, recall 0.825689
2017-12-10T04:40:39.199188: step 5029, loss 0.172962, acc 0.9375, prec 0.0825645, recall 0.825738
2017-12-10T04:40:39.619665: step 5030, loss 0.153775, acc 0.953125, prec 0.082574, recall 0.825763
2017-12-10T04:40:39.915302: step 5031, loss 0.320885, acc 0.9375, prec 0.0825693, recall 0.825763
2017-12-10T04:40:40.198129: step 5032, loss 0.220882, acc 0.9375, prec 0.0825647, recall 0.825763
2017-12-10T04:40:40.470749: step 5033, loss 0.41401, acc 0.890625, prec 0.0825566, recall 0.825763
2017-12-10T04:40:40.745361: step 5034, loss 0.438906, acc 0.90625, prec 0.0825496, recall 0.825763
2017-12-10T04:40:41.023611: step 5035, loss 0.223526, acc 0.9375, prec 0.0825449, recall 0.825763
2017-12-10T04:40:41.302613: step 5036, loss 0.16042, acc 0.96875, prec 0.0825555, recall 0.825787
2017-12-10T04:40:41.588834: step 5037, loss 0.379016, acc 0.921875, prec 0.0825626, recall 0.825812
2017-12-10T04:40:41.858355: step 5038, loss 0.0551375, acc 0.96875, prec 0.0825603, recall 0.825812
2017-12-10T04:40:42.133174: step 5039, loss 0.334452, acc 0.96875, prec 0.0825709, recall 0.825836
2017-12-10T04:40:42.408002: step 5040, loss 0.148647, acc 0.96875, prec 0.0825685, recall 0.825836
2017-12-10T04:40:42.675269: step 5041, loss 0.277529, acc 0.9375, prec 0.0825768, recall 0.825861
2017-12-10T04:40:42.946840: step 5042, loss 0.231906, acc 0.9375, prec 0.0825851, recall 0.825885
2017-12-10T04:40:43.213003: step 5043, loss 0.297978, acc 0.921875, prec 0.0825792, recall 0.825885
2017-12-10T04:40:43.478463: step 5044, loss 0.200823, acc 0.921875, prec 0.0825735, recall 0.825885
2017-12-10T04:40:43.746686: step 5045, loss 0.257354, acc 0.9375, prec 0.0825688, recall 0.825885
2017-12-10T04:40:44.023756: step 5046, loss 0.160974, acc 0.953125, prec 0.0825911, recall 0.825934
2017-12-10T04:40:44.293109: step 5047, loss 0.0773008, acc 0.984375, prec 0.0825899, recall 0.825934
2017-12-10T04:40:44.562363: step 5048, loss 0.0392853, acc 0.984375, prec 0.0826017, recall 0.825959
2017-12-10T04:40:44.828365: step 5049, loss 0.313144, acc 0.9375, prec 0.082597, recall 0.825959
2017-12-10T04:40:45.100398: step 5050, loss 0.184836, acc 0.984375, prec 0.0825959, recall 0.825959
2017-12-10T04:40:45.367111: step 5051, loss 0.272697, acc 0.984375, prec 0.0826076, recall 0.825983
2017-12-10T04:40:45.640356: step 5052, loss 0.150317, acc 0.953125, prec 0.082617, recall 0.826008
2017-12-10T04:40:45.907474: step 5053, loss 0.141359, acc 0.96875, prec 0.0826276, recall 0.826032
2017-12-10T04:40:46.177596: step 5054, loss 0.0489072, acc 0.984375, prec 0.0826264, recall 0.826032
2017-12-10T04:40:46.441924: step 5055, loss 7.06493, acc 0.96875, prec 0.082651, recall 0.825965
2017-12-10T04:40:46.713103: step 5056, loss 0.0176759, acc 1, prec 0.082651, recall 0.825965
2017-12-10T04:40:46.984564: step 5057, loss 0.118521, acc 0.984375, prec 0.0826627, recall 0.825989
2017-12-10T04:40:47.247584: step 5058, loss 0.166934, acc 0.9375, prec 0.0826839, recall 0.826038
2017-12-10T04:40:47.517140: step 5059, loss 0.580646, acc 0.921875, prec 0.0826909, recall 0.826063
2017-12-10T04:40:47.779826: step 5060, loss 0.103056, acc 0.984375, prec 0.0826898, recall 0.826063
2017-12-10T04:40:48.047264: step 5061, loss 0.0616939, acc 0.984375, prec 0.0826886, recall 0.826063
2017-12-10T04:40:48.308984: step 5062, loss 0.084898, acc 0.953125, prec 0.0826851, recall 0.826063
2017-12-10T04:40:48.579318: step 5063, loss 0.398404, acc 0.9375, prec 0.0826805, recall 0.826063
2017-12-10T04:40:48.853078: step 5064, loss 0.314299, acc 0.890625, prec 0.0826852, recall 0.826087
2017-12-10T04:40:49.116388: step 5065, loss 0.475378, acc 0.875, prec 0.0826759, recall 0.826087
2017-12-10T04:40:49.385460: step 5066, loss 0.449619, acc 0.875, prec 0.0826924, recall 0.826136
2017-12-10T04:40:49.653451: step 5067, loss 0.237928, acc 0.9375, prec 0.0827006, recall 0.82616
2017-12-10T04:40:49.916326: step 5068, loss 0.289846, acc 0.921875, prec 0.0826948, recall 0.82616
2017-12-10T04:40:50.188759: step 5069, loss 0.367806, acc 0.890625, prec 0.0826996, recall 0.826184
2017-12-10T04:40:50.453671: step 5070, loss 0.393232, acc 0.921875, prec 0.0827324, recall 0.826258
2017-12-10T04:40:50.717308: step 5071, loss 0.282105, acc 0.921875, prec 0.0827266, recall 0.826258
2017-12-10T04:40:50.979720: step 5072, loss 0.0928861, acc 0.96875, prec 0.0827371, recall 0.826282
2017-12-10T04:40:51.248740: step 5073, loss 0.0840748, acc 0.96875, prec 0.0827477, recall 0.826306
2017-12-10T04:40:51.513207: step 5074, loss 0.0872079, acc 0.953125, prec 0.0827442, recall 0.826306
2017-12-10T04:40:51.780590: step 5075, loss 0.0868854, acc 0.984375, prec 0.082743, recall 0.826306
2017-12-10T04:40:52.045217: step 5076, loss 0.181494, acc 0.9375, prec 0.0827641, recall 0.826355
2017-12-10T04:40:52.311122: step 5077, loss 0.0918693, acc 0.984375, prec 0.082763, recall 0.826355
2017-12-10T04:40:52.576826: step 5078, loss 0.183746, acc 0.9375, prec 0.0827841, recall 0.826403
2017-12-10T04:40:52.842000: step 5079, loss 0.118521, acc 0.96875, prec 0.0827817, recall 0.826403
2017-12-10T04:40:53.110857: step 5080, loss 0.00186624, acc 1, prec 0.0827817, recall 0.826403
2017-12-10T04:40:53.378252: step 5081, loss 0.605763, acc 0.984375, prec 0.0827934, recall 0.826428
2017-12-10T04:40:53.647199: step 5082, loss 0.0219901, acc 1, prec 0.0828063, recall 0.826452
2017-12-10T04:40:53.917562: step 5083, loss 0.0443793, acc 0.984375, prec 0.082818, recall 0.826476
2017-12-10T04:40:54.193298: step 5084, loss 0.0872471, acc 0.96875, prec 0.0828414, recall 0.826525
2017-12-10T04:40:54.452032: step 5085, loss 0.0338516, acc 0.984375, prec 0.0828402, recall 0.826525
2017-12-10T04:40:54.724672: step 5086, loss 0.187756, acc 0.953125, prec 0.0828496, recall 0.826549
2017-12-10T04:40:54.993971: step 5087, loss 0.0270421, acc 0.96875, prec 0.0828473, recall 0.826549
2017-12-10T04:40:55.272690: step 5088, loss 1.32825, acc 0.984375, prec 0.0828847, recall 0.826622
2017-12-10T04:40:55.546219: step 5089, loss 0.0513696, acc 0.984375, prec 0.0829093, recall 0.82667
2017-12-10T04:40:55.813930: step 5090, loss 0.0404076, acc 0.96875, prec 0.0829069, recall 0.82667
2017-12-10T04:40:56.079845: step 5091, loss 0.0294907, acc 0.984375, prec 0.0829186, recall 0.826695
2017-12-10T04:40:56.346884: step 5092, loss 0.274688, acc 1, prec 0.0829572, recall 0.826767
2017-12-10T04:40:56.615579: step 5093, loss 0.0504381, acc 0.96875, prec 0.0829549, recall 0.826767
2017-12-10T04:40:56.893059: step 5094, loss 0.0562125, acc 0.96875, prec 0.0829654, recall 0.826791
2017-12-10T04:40:57.169878: step 5095, loss 1.19116, acc 0.921875, prec 0.0829853, recall 0.82684
2017-12-10T04:40:57.437920: step 5096, loss 0.552822, acc 0.9375, prec 0.0830192, recall 0.826912
2017-12-10T04:40:57.704729: step 5097, loss 0.666613, acc 0.890625, prec 0.0830239, recall 0.826936
2017-12-10T04:40:57.970212: step 5098, loss 0.185254, acc 0.90625, prec 0.0830298, recall 0.826961
2017-12-10T04:40:58.242228: step 5099, loss 0.121078, acc 0.96875, prec 0.0830274, recall 0.826961
2017-12-10T04:40:58.506286: step 5100, loss 0.433502, acc 0.859375, prec 0.0830298, recall 0.826985

Evaluation:
2017-12-10T04:41:06.089475: step 5100, loss 1.91933, acc 0.883186, prec 0.082955, recall 0.824197

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-5100

2017-12-10T04:41:07.417504: step 5101, loss 0.221505, acc 0.953125, prec 0.0829768, recall 0.824245
2017-12-10T04:41:07.685925: step 5102, loss 0.285361, acc 0.9375, prec 0.0829848, recall 0.824269
2017-12-10T04:41:07.947675: step 5103, loss 0.11297, acc 0.96875, prec 0.0829951, recall 0.824293
2017-12-10T04:41:08.214941: step 5104, loss 0.171908, acc 0.90625, prec 0.0830009, recall 0.824317
2017-12-10T04:41:08.478529: step 5105, loss 0.433519, acc 0.875, prec 0.0830044, recall 0.824341
2017-12-10T04:41:08.745975: step 5106, loss 0.304183, acc 0.9375, prec 0.0829998, recall 0.824341
2017-12-10T04:41:09.020897: step 5107, loss 0.335841, acc 0.921875, prec 0.0830319, recall 0.824413
2017-12-10T04:41:09.283195: step 5108, loss 0.379106, acc 0.90625, prec 0.0830251, recall 0.824413
2017-12-10T04:41:09.553928: step 5109, loss 0.494878, acc 0.90625, prec 0.0830182, recall 0.824413
2017-12-10T04:41:09.823802: step 5110, loss 0.200778, acc 0.921875, prec 0.0830125, recall 0.824413
2017-12-10T04:41:10.086571: step 5111, loss 2.49768, acc 0.984375, prec 0.0830125, recall 0.8243
2017-12-10T04:41:10.356714: step 5112, loss 0.362502, acc 0.875, prec 0.0830034, recall 0.8243
2017-12-10T04:41:10.620793: step 5113, loss 0.358039, acc 0.890625, prec 0.0829954, recall 0.8243
2017-12-10T04:41:10.890495: step 5114, loss 0.437102, acc 0.921875, prec 0.0830149, recall 0.824348
2017-12-10T04:41:11.162171: step 5115, loss 0.300592, acc 0.90625, prec 0.0830207, recall 0.824372
2017-12-10T04:41:11.431429: step 5116, loss 0.549305, acc 0.921875, prec 0.083015, recall 0.824372
2017-12-10T04:41:11.699967: step 5117, loss 0.163478, acc 0.9375, prec 0.0830356, recall 0.82442
2017-12-10T04:41:11.968494: step 5118, loss 0.14633, acc 0.96875, prec 0.0830459, recall 0.824444
2017-12-10T04:41:12.239070: step 5119, loss 0.199802, acc 0.921875, prec 0.0830528, recall 0.824468
2017-12-10T04:41:12.514459: step 5120, loss 0.335328, acc 0.90625, prec 0.0830586, recall 0.824492
2017-12-10T04:41:12.776061: step 5121, loss 0.446984, acc 0.890625, prec 0.0830506, recall 0.824492
2017-12-10T04:41:13.039411: step 5122, loss 0.184184, acc 0.9375, prec 0.083046, recall 0.824492
2017-12-10T04:41:13.304645: step 5123, loss 0.357947, acc 0.90625, prec 0.0830644, recall 0.82454
2017-12-10T04:41:13.581730: step 5124, loss 0.201827, acc 0.9375, prec 0.0830724, recall 0.824564
2017-12-10T04:41:13.852453: step 5125, loss 0.723892, acc 0.84375, prec 0.083061, recall 0.824564
2017-12-10T04:41:14.123075: step 5126, loss 0.220875, acc 0.90625, prec 0.0830793, recall 0.824612
2017-12-10T04:41:14.391958: step 5127, loss 0.2959, acc 0.90625, prec 0.0831103, recall 0.824683
2017-12-10T04:41:14.653508: step 5128, loss 0.188027, acc 0.9375, prec 0.0831309, recall 0.824731
2017-12-10T04:41:14.916776: step 5129, loss 0.12153, acc 0.96875, prec 0.0831286, recall 0.824731
2017-12-10T04:41:15.188719: step 5130, loss 0.361498, acc 0.96875, prec 0.0831263, recall 0.824731
2017-12-10T04:41:15.450052: step 5131, loss 0.181157, acc 0.953125, prec 0.0831355, recall 0.824755
2017-12-10T04:41:15.717266: step 5132, loss 0.219103, acc 0.9375, prec 0.0831309, recall 0.824755
2017-12-10T04:41:15.994778: step 5133, loss 0.0193844, acc 1, prec 0.0831309, recall 0.824755
2017-12-10T04:41:16.257946: step 5134, loss 0.350271, acc 0.96875, prec 0.0831286, recall 0.824755
2017-12-10T04:41:16.524246: step 5135, loss 0.1214, acc 0.984375, prec 0.0831275, recall 0.824755
2017-12-10T04:41:16.797386: step 5136, loss 0.0686013, acc 1, prec 0.0831401, recall 0.824779
2017-12-10T04:41:17.065436: step 5137, loss 0.0549216, acc 0.984375, prec 0.0831389, recall 0.824779
2017-12-10T04:41:17.337942: step 5138, loss 0.0660304, acc 0.96875, prec 0.0831492, recall 0.824803
2017-12-10T04:41:17.602851: step 5139, loss 0.14594, acc 1, prec 0.0831618, recall 0.824826
2017-12-10T04:41:17.866619: step 5140, loss 1.04691, acc 0.953125, prec 0.0831835, recall 0.824874
2017-12-10T04:41:18.148635: step 5141, loss 0.0118846, acc 1, prec 0.0832087, recall 0.824922
2017-12-10T04:41:18.412625: step 5142, loss 0.015244, acc 0.984375, prec 0.0832201, recall 0.824946
2017-12-10T04:41:18.679573: step 5143, loss 0.09863, acc 0.953125, prec 0.0832167, recall 0.824946
2017-12-10T04:41:18.950019: step 5144, loss 0.0268198, acc 0.984375, prec 0.0832281, recall 0.824969
2017-12-10T04:41:19.222870: step 5145, loss 0.0813784, acc 0.96875, prec 0.0832259, recall 0.824969
2017-12-10T04:41:19.503012: step 5146, loss 0.0105371, acc 1, prec 0.0832259, recall 0.824969
2017-12-10T04:41:19.766628: step 5147, loss 0.208363, acc 0.96875, prec 0.0832362, recall 0.824993
2017-12-10T04:41:20.032750: step 5148, loss 0.0574279, acc 0.984375, prec 0.0832476, recall 0.825017
2017-12-10T04:41:20.297279: step 5149, loss 0.125303, acc 0.96875, prec 0.083283, recall 0.825088
2017-12-10T04:41:20.565716: step 5150, loss 0.0420485, acc 0.984375, prec 0.083307, recall 0.825136
2017-12-10T04:41:20.836179: step 5151, loss 0.127084, acc 1, prec 0.0833196, recall 0.82516
2017-12-10T04:41:21.105545: step 5152, loss 0.133021, acc 0.96875, prec 0.0833173, recall 0.82516
2017-12-10T04:41:21.369440: step 5153, loss 0.345396, acc 0.984375, prec 0.0833288, recall 0.825183
2017-12-10T04:41:21.635877: step 5154, loss 0.0220988, acc 0.984375, prec 0.0833402, recall 0.825207
2017-12-10T04:41:21.900476: step 5155, loss 0.408571, acc 0.96875, prec 0.0833882, recall 0.825302
2017-12-10T04:41:22.168846: step 5156, loss 0.153009, acc 0.953125, prec 0.0834225, recall 0.825373
2017-12-10T04:41:22.435009: step 5157, loss 0.0341472, acc 1, prec 0.0834853, recall 0.825492
2017-12-10T04:41:22.707187: step 5158, loss 0.14207, acc 0.953125, prec 0.0834945, recall 0.825515
2017-12-10T04:41:22.969751: step 5159, loss 0.164484, acc 0.96875, prec 0.0835047, recall 0.825539
2017-12-10T04:41:23.236449: step 5160, loss 0.00594506, acc 1, prec 0.0835047, recall 0.825539
2017-12-10T04:41:23.500225: step 5161, loss 0.021791, acc 1, prec 0.0835424, recall 0.82561
2017-12-10T04:41:23.764057: step 5162, loss 0.214955, acc 0.953125, prec 0.0835641, recall 0.825657
2017-12-10T04:41:24.028621: step 5163, loss 0.0951566, acc 0.953125, prec 0.0835607, recall 0.825657
2017-12-10T04:41:24.296826: step 5164, loss 0.0909827, acc 0.984375, prec 0.0835721, recall 0.825681
2017-12-10T04:41:24.563104: step 5165, loss 0.157321, acc 1, prec 0.0835972, recall 0.825728
2017-12-10T04:41:24.827234: step 5166, loss 0.107703, acc 0.953125, prec 0.0836064, recall 0.825751
2017-12-10T04:41:25.097158: step 5167, loss 0.187826, acc 0.9375, prec 0.0836143, recall 0.825775
2017-12-10T04:41:25.368694: step 5168, loss 0.142356, acc 0.9375, prec 0.0836349, recall 0.825822
2017-12-10T04:41:25.643070: step 5169, loss 0.05281, acc 0.984375, prec 0.0836463, recall 0.825846
2017-12-10T04:41:25.913411: step 5170, loss 0.208022, acc 0.96875, prec 0.0836817, recall 0.825916
2017-12-10T04:41:26.187787: step 5171, loss 0.459702, acc 0.921875, prec 0.083701, recall 0.825963
2017-12-10T04:41:26.453135: step 5172, loss 0.124765, acc 0.953125, prec 0.0837102, recall 0.825987
2017-12-10T04:41:26.719897: step 5173, loss 0.212582, acc 0.953125, prec 0.0837067, recall 0.825987
2017-12-10T04:41:26.996554: step 5174, loss 0.154122, acc 0.953125, prec 0.0837158, recall 0.826011
2017-12-10T04:41:27.266665: step 5175, loss 0.0512331, acc 0.984375, prec 0.0837147, recall 0.826011
2017-12-10T04:41:27.529110: step 5176, loss 0.0641458, acc 0.96875, prec 0.0837249, recall 0.826034
2017-12-10T04:41:27.788929: step 5177, loss 0.318101, acc 0.921875, prec 0.0837192, recall 0.826034
2017-12-10T04:41:28.058694: step 5178, loss 0.296013, acc 0.953125, prec 0.0837409, recall 0.826081
2017-12-10T04:41:28.322533: step 5179, loss 0.387148, acc 0.96875, prec 0.0837511, recall 0.826105
2017-12-10T04:41:28.594983: step 5180, loss 0.378733, acc 0.953125, prec 0.0837602, recall 0.826128
2017-12-10T04:41:28.863082: step 5181, loss 0.211437, acc 0.984375, prec 0.0837716, recall 0.826152
2017-12-10T04:41:29.127088: step 5182, loss 0.0811429, acc 0.953125, prec 0.0837933, recall 0.826199
2017-12-10T04:41:29.393779: step 5183, loss 0.0493697, acc 0.984375, prec 0.0838047, recall 0.826222
2017-12-10T04:41:29.662503: step 5184, loss 0.0194303, acc 0.984375, prec 0.0838161, recall 0.826245
2017-12-10T04:41:29.937185: step 5185, loss 0.405803, acc 0.890625, prec 0.0838206, recall 0.826269
2017-12-10T04:41:30.205638: step 5186, loss 0.0687859, acc 0.953125, prec 0.0838297, recall 0.826292
2017-12-10T04:41:30.479042: step 5187, loss 0.0299787, acc 0.984375, prec 0.0838286, recall 0.826292
2017-12-10T04:41:30.750509: step 5188, loss 0.0690364, acc 1, prec 0.0838662, recall 0.826363
2017-12-10T04:41:31.015083: step 5189, loss 0.0166472, acc 0.984375, prec 0.083865, recall 0.826363
2017-12-10T04:41:31.279267: step 5190, loss 0.0389869, acc 0.96875, prec 0.0838628, recall 0.826363
2017-12-10T04:41:31.551615: step 5191, loss 0.0194139, acc 1, prec 0.0838628, recall 0.826363
2017-12-10T04:41:31.818222: step 5192, loss 0.0780974, acc 0.984375, prec 0.0838616, recall 0.826363
2017-12-10T04:41:32.083537: step 5193, loss 0.0107618, acc 1, prec 0.0838867, recall 0.82641
2017-12-10T04:41:32.354378: step 5194, loss 0.115705, acc 0.96875, prec 0.0839095, recall 0.826456
2017-12-10T04:41:32.628494: step 5195, loss 0.000673781, acc 1, prec 0.0839346, recall 0.826503
2017-12-10T04:41:33.602925: step 5196, loss 1.87545, acc 0.96875, prec 0.083946, recall 0.826415
2017-12-10T04:41:33.967402: step 5197, loss 0.00192315, acc 1, prec 0.083946, recall 0.826415
2017-12-10T04:41:34.680571: step 5198, loss 0.00117058, acc 1, prec 0.0839585, recall 0.826438
2017-12-10T04:41:35.612576: step 5199, loss 0.109162, acc 0.96875, prec 0.0839562, recall 0.826438
2017-12-10T04:41:36.037196: step 5200, loss 0.191547, acc 0.984375, prec 0.083955, recall 0.826438
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-5200

2017-12-10T04:41:37.359513: step 5201, loss 0.0762706, acc 0.96875, prec 0.0839653, recall 0.826462
2017-12-10T04:41:37.660477: step 5202, loss 0.195104, acc 0.9375, prec 0.0839858, recall 0.826509
2017-12-10T04:41:37.953580: step 5203, loss 0.10257, acc 0.96875, prec 0.0839835, recall 0.826509
2017-12-10T04:41:38.252971: step 5204, loss 0.181237, acc 0.96875, prec 0.0839937, recall 0.826532
2017-12-10T04:41:38.546415: step 5205, loss 0.317178, acc 0.921875, prec 0.084013, recall 0.826579
2017-12-10T04:41:38.827746: step 5206, loss 0.113814, acc 0.953125, prec 0.0840096, recall 0.826579
2017-12-10T04:41:39.105401: step 5207, loss 0.318343, acc 0.90625, prec 0.0840403, recall 0.826649
2017-12-10T04:41:39.371409: step 5208, loss 0.148227, acc 0.96875, prec 0.084063, recall 0.826695
2017-12-10T04:41:39.641676: step 5209, loss 0.0782099, acc 0.984375, prec 0.0840744, recall 0.826719
2017-12-10T04:41:39.914389: step 5210, loss 0.161463, acc 0.953125, prec 0.0840835, recall 0.826742
2017-12-10T04:41:40.186741: step 5211, loss 0.0316568, acc 1, prec 0.0841211, recall 0.826812
2017-12-10T04:41:40.470240: step 5212, loss 0.123695, acc 0.953125, prec 0.0841176, recall 0.826812
2017-12-10T04:41:40.740043: step 5213, loss 0.193672, acc 0.953125, prec 0.0841393, recall 0.826858
2017-12-10T04:41:41.008535: step 5214, loss 0.0947688, acc 0.9375, prec 0.0841472, recall 0.826882
2017-12-10T04:41:41.279637: step 5215, loss 0.358169, acc 0.984375, prec 0.0841711, recall 0.826928
2017-12-10T04:41:41.553116: step 5216, loss 0.239924, acc 0.9375, prec 0.0841665, recall 0.826928
2017-12-10T04:41:41.818652: step 5217, loss 0.252923, acc 0.90625, prec 0.0841596, recall 0.826928
2017-12-10T04:41:42.097903: step 5218, loss 0.280573, acc 0.9375, prec 0.084155, recall 0.826928
2017-12-10T04:41:42.364928: step 5219, loss 0.550093, acc 0.9375, prec 0.0841754, recall 0.826975
2017-12-10T04:41:42.631792: step 5220, loss 0.157721, acc 0.953125, prec 0.084197, recall 0.827021
2017-12-10T04:41:42.902727: step 5221, loss 0.523281, acc 0.890625, prec 0.084214, recall 0.827068
2017-12-10T04:41:43.176488: step 5222, loss 0.241534, acc 0.96875, prec 0.0842242, recall 0.827091
2017-12-10T04:41:43.444132: step 5223, loss 1.0829, acc 0.953125, prec 0.0842458, recall 0.827137
2017-12-10T04:41:43.714456: step 5224, loss 0.625903, acc 0.96875, prec 0.0842685, recall 0.827184
2017-12-10T04:41:43.982239: step 5225, loss 0.37289, acc 0.9375, prec 0.0842764, recall 0.827207
2017-12-10T04:41:44.250771: step 5226, loss 0.0397187, acc 0.984375, prec 0.0842753, recall 0.827207
2017-12-10T04:41:44.514559: step 5227, loss 0.026361, acc 1, prec 0.0842878, recall 0.82723
2017-12-10T04:41:44.778706: step 5228, loss 0.435851, acc 0.9375, prec 0.0842832, recall 0.82723
2017-12-10T04:41:45.048736: step 5229, loss 0.186473, acc 0.9375, prec 0.0842911, recall 0.827253
2017-12-10T04:41:45.323508: step 5230, loss 0.127698, acc 0.953125, prec 0.0842876, recall 0.827253
2017-12-10T04:41:45.587587: step 5231, loss 0.386188, acc 0.96875, prec 0.0842978, recall 0.827276
2017-12-10T04:41:45.857554: step 5232, loss 0.201846, acc 0.953125, prec 0.0843069, recall 0.8273
2017-12-10T04:41:46.135588: step 5233, loss 0.33733, acc 0.9375, prec 0.0843148, recall 0.827323
2017-12-10T04:41:46.402378: step 5234, loss 0.0576105, acc 0.984375, prec 0.0843136, recall 0.827323
2017-12-10T04:41:46.672757: step 5235, loss 0.0338629, acc 0.984375, prec 0.0843125, recall 0.827323
2017-12-10T04:41:46.941191: step 5236, loss 0.0458536, acc 0.984375, prec 0.0843113, recall 0.827323
2017-12-10T04:41:47.206814: step 5237, loss 0.109878, acc 0.96875, prec 0.0843216, recall 0.827346
2017-12-10T04:41:47.477563: step 5238, loss 0.368975, acc 0.9375, prec 0.0843169, recall 0.827346
2017-12-10T04:41:47.746998: step 5239, loss 0.502895, acc 0.90625, prec 0.0843225, recall 0.827369
2017-12-10T04:41:48.014258: step 5240, loss 0.0750339, acc 0.984375, prec 0.0843214, recall 0.827369
2017-12-10T04:41:48.287847: step 5241, loss 1.15635, acc 0.890625, prec 0.0843258, recall 0.827392
2017-12-10T04:41:48.555185: step 5242, loss 0.0584247, acc 0.984375, prec 0.0843247, recall 0.827392
2017-12-10T04:41:48.825391: step 5243, loss 0.22546, acc 0.984375, prec 0.0843735, recall 0.827485
2017-12-10T04:41:49.094005: step 5244, loss 0.919145, acc 1, prec 0.0843861, recall 0.827508
2017-12-10T04:41:49.368550: step 5245, loss 0.0193202, acc 1, prec 0.0843861, recall 0.827508
2017-12-10T04:41:49.633912: step 5246, loss 0.0871203, acc 0.96875, prec 0.0843837, recall 0.827508
2017-12-10T04:41:49.904134: step 5247, loss 0.230797, acc 0.96875, prec 0.0843814, recall 0.827508
2017-12-10T04:41:50.177594: step 5248, loss 0.274402, acc 0.96875, prec 0.0844041, recall 0.827554
2017-12-10T04:41:50.448571: step 5249, loss 0.230343, acc 0.96875, prec 0.0844393, recall 0.827623
2017-12-10T04:41:50.708934: step 5250, loss 0.163412, acc 0.953125, prec 0.0844484, recall 0.827646
2017-12-10T04:41:50.974951: step 5251, loss 0.361584, acc 0.953125, prec 0.0844449, recall 0.827646
2017-12-10T04:41:51.238901: step 5252, loss 0.609769, acc 0.90625, prec 0.0844505, recall 0.827669
2017-12-10T04:41:51.503596: step 5253, loss 0.387199, acc 0.875, prec 0.0844663, recall 0.827715
2017-12-10T04:41:51.770485: step 5254, loss 0.154023, acc 0.953125, prec 0.0844878, recall 0.827761
2017-12-10T04:41:52.045453: step 5255, loss 1.01316, acc 0.890625, prec 0.0844922, recall 0.827784
2017-12-10T04:41:52.309744: step 5256, loss 0.345442, acc 0.890625, prec 0.0845092, recall 0.82783
2017-12-10T04:41:52.575905: step 5257, loss 0.281879, acc 0.953125, prec 0.0845307, recall 0.827877
2017-12-10T04:41:52.855471: step 5258, loss 0.0430793, acc 0.96875, prec 0.0845284, recall 0.827877
2017-12-10T04:41:53.130383: step 5259, loss 0.103034, acc 0.953125, prec 0.0845374, recall 0.8279
2017-12-10T04:41:53.393605: step 5260, loss 0.288901, acc 0.9375, prec 0.0845578, recall 0.827945
2017-12-10T04:41:53.661364: step 5261, loss 2.46291, acc 0.953125, prec 0.0845679, recall 0.827858
2017-12-10T04:41:53.930885: step 5262, loss 0.101749, acc 0.96875, prec 0.0845781, recall 0.827881
2017-12-10T04:41:54.205018: step 5263, loss 0.0449563, acc 0.96875, prec 0.0846008, recall 0.827927
2017-12-10T04:41:54.475751: step 5264, loss 0.398179, acc 0.9375, prec 0.0846087, recall 0.82795
2017-12-10T04:41:54.737775: step 5265, loss 0.0842911, acc 0.96875, prec 0.0846438, recall 0.828019
2017-12-10T04:41:55.000317: step 5266, loss 0.196465, acc 0.96875, prec 0.0846415, recall 0.828019
2017-12-10T04:41:55.272562: step 5267, loss 0.156383, acc 0.96875, prec 0.0846517, recall 0.828042
2017-12-10T04:41:55.538703: step 5268, loss 0.505092, acc 0.953125, prec 0.0846857, recall 0.82811
2017-12-10T04:41:55.810444: step 5269, loss 0.184706, acc 0.9375, prec 0.084706, recall 0.828156
2017-12-10T04:41:56.075466: step 5270, loss 0.159896, acc 0.921875, prec 0.0847127, recall 0.828179
2017-12-10T04:41:56.342631: step 5271, loss 0.105355, acc 0.9375, prec 0.0847455, recall 0.828248
2017-12-10T04:41:56.604547: step 5272, loss 0.314955, acc 0.953125, prec 0.0847545, recall 0.828271
2017-12-10T04:41:56.869427: step 5273, loss 0.188953, acc 0.96875, prec 0.0847647, recall 0.828294
2017-12-10T04:41:57.145895: step 5274, loss 0.243978, acc 0.953125, prec 0.0847987, recall 0.828362
2017-12-10T04:41:57.410655: step 5275, loss 0.20558, acc 0.953125, prec 0.0848077, recall 0.828385
2017-12-10T04:41:57.676759: step 5276, loss 0.330154, acc 0.9375, prec 0.084828, recall 0.828431
2017-12-10T04:41:57.953076: step 5277, loss 0.0929986, acc 0.953125, prec 0.0848245, recall 0.828431
2017-12-10T04:41:58.221631: step 5278, loss 0.435723, acc 0.9375, prec 0.0848324, recall 0.828454
2017-12-10T04:41:58.489862: step 5279, loss 0.00499918, acc 1, prec 0.0848324, recall 0.828454
2017-12-10T04:41:58.760784: step 5280, loss 0.0859509, acc 0.96875, prec 0.0848425, recall 0.828476
2017-12-10T04:41:59.029757: step 5281, loss 0.0675041, acc 0.96875, prec 0.0848527, recall 0.828499
2017-12-10T04:41:59.302861: step 5282, loss 0.107229, acc 0.953125, prec 0.0848492, recall 0.828499
2017-12-10T04:41:59.571533: step 5283, loss 0.11831, acc 0.953125, prec 0.0848458, recall 0.828499
2017-12-10T04:41:59.836207: step 5284, loss 0.0326178, acc 0.984375, prec 0.0848571, recall 0.828522
2017-12-10T04:42:00.111847: step 5285, loss 0.177697, acc 0.96875, prec 0.0848672, recall 0.828545
2017-12-10T04:42:00.382479: step 5286, loss 0.0230367, acc 0.984375, prec 0.0848785, recall 0.828568
2017-12-10T04:42:00.653974: step 5287, loss 0.123184, acc 0.984375, prec 0.0848899, recall 0.82859
2017-12-10T04:42:00.920306: step 5288, loss 0.0189699, acc 0.984375, prec 0.0849261, recall 0.828659
2017-12-10T04:42:01.187464: step 5289, loss 0.00210653, acc 1, prec 0.084951, recall 0.828704
2017-12-10T04:42:01.454977: step 5290, loss 0.0992196, acc 0.96875, prec 0.0849487, recall 0.828704
2017-12-10T04:42:01.719038: step 5291, loss 0.209376, acc 0.984375, prec 0.08496, recall 0.828727
2017-12-10T04:42:01.995105: step 5292, loss 0.158077, acc 0.96875, prec 0.0849951, recall 0.828795
2017-12-10T04:42:02.261437: step 5293, loss 0.325562, acc 0.9375, prec 0.0850154, recall 0.828841
2017-12-10T04:42:02.530040: step 5294, loss 0.0893994, acc 0.984375, prec 0.0850142, recall 0.828841
2017-12-10T04:42:02.801647: step 5295, loss 0.0441346, acc 0.984375, prec 0.0850131, recall 0.828841
2017-12-10T04:42:03.072433: step 5296, loss 0.114883, acc 0.96875, prec 0.0850108, recall 0.828841
2017-12-10T04:42:03.342679: step 5297, loss 0.0635681, acc 0.96875, prec 0.0850209, recall 0.828864
2017-12-10T04:42:03.606312: step 5298, loss 0.0292684, acc 1, prec 0.0850209, recall 0.828864
2017-12-10T04:42:03.872599: step 5299, loss 0.156136, acc 0.96875, prec 0.085031, recall 0.828886
2017-12-10T04:42:04.144846: step 5300, loss 0.0308608, acc 0.984375, prec 0.0850299, recall 0.828886
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-5300

2017-12-10T04:42:05.369397: step 5301, loss 0.207227, acc 0.96875, prec 0.0850276, recall 0.828886
2017-12-10T04:42:05.646569: step 5302, loss 0.164753, acc 0.96875, prec 0.0850502, recall 0.828932
2017-12-10T04:42:05.912785: step 5303, loss 0.817416, acc 0.984375, prec 0.0850615, recall 0.828954
2017-12-10T04:42:06.181774: step 5304, loss 0.0456956, acc 0.984375, prec 0.0850603, recall 0.828954
2017-12-10T04:42:06.442377: step 5305, loss 1.06038, acc 0.953125, prec 0.0850693, recall 0.828977
2017-12-10T04:42:06.724846: step 5306, loss 0.199182, acc 0.9375, prec 0.0850771, recall 0.829
2017-12-10T04:42:06.989066: step 5307, loss 0.0978999, acc 0.984375, prec 0.0851009, recall 0.829045
2017-12-10T04:42:07.254131: step 5308, loss 0.205036, acc 0.90625, prec 0.0851064, recall 0.829068
2017-12-10T04:42:07.522800: step 5309, loss 0.156302, acc 0.953125, prec 0.0851029, recall 0.829068
2017-12-10T04:42:07.791923: step 5310, loss 0.129997, acc 0.984375, prec 0.0851017, recall 0.829068
2017-12-10T04:42:08.062474: step 5311, loss 0.0746974, acc 0.96875, prec 0.0851119, recall 0.82909
2017-12-10T04:42:08.327545: step 5312, loss 0.0186274, acc 0.984375, prec 0.0851107, recall 0.82909
2017-12-10T04:42:08.591896: step 5313, loss 0.0464936, acc 0.984375, prec 0.085122, recall 0.829113
2017-12-10T04:42:08.857374: step 5314, loss 0.0804336, acc 0.96875, prec 0.0851322, recall 0.829136
2017-12-10T04:42:09.130142: step 5315, loss 0.26965, acc 0.953125, prec 0.0851287, recall 0.829136
2017-12-10T04:42:09.394310: step 5316, loss 0.0844025, acc 0.984375, prec 0.0851524, recall 0.829181
2017-12-10T04:42:09.662326: step 5317, loss 0.118153, acc 0.96875, prec 0.0851501, recall 0.829181
2017-12-10T04:42:09.927119: step 5318, loss 0.0788494, acc 0.953125, prec 0.0851591, recall 0.829204
2017-12-10T04:42:10.192000: step 5319, loss 0.294796, acc 0.953125, prec 0.0851805, recall 0.829249
2017-12-10T04:42:10.454061: step 5320, loss 0.0826146, acc 0.96875, prec 0.0851906, recall 0.829271
2017-12-10T04:42:10.725287: step 5321, loss 0.0793387, acc 0.96875, prec 0.0852132, recall 0.829317
2017-12-10T04:42:10.988415: step 5322, loss 0.274946, acc 0.875, prec 0.0852039, recall 0.829317
2017-12-10T04:42:11.255327: step 5323, loss 0.00884608, acc 1, prec 0.0852039, recall 0.829317
2017-12-10T04:42:11.525302: step 5324, loss 0.199421, acc 0.96875, prec 0.0852141, recall 0.829339
2017-12-10T04:42:11.800497: step 5325, loss 0.0108782, acc 1, prec 0.0852265, recall 0.829362
2017-12-10T04:42:12.077375: step 5326, loss 0.0813974, acc 0.984375, prec 0.0852378, recall 0.829385
2017-12-10T04:42:12.347271: step 5327, loss 0.135599, acc 0.9375, prec 0.0852456, recall 0.829407
2017-12-10T04:42:12.613759: step 5328, loss 0.24116, acc 0.9375, prec 0.0852658, recall 0.829452
2017-12-10T04:42:12.884704: step 5329, loss 0.182502, acc 0.96875, prec 0.0852635, recall 0.829452
2017-12-10T04:42:13.155870: step 5330, loss 0.130312, acc 0.984375, prec 0.0852997, recall 0.82952
2017-12-10T04:42:13.425776: step 5331, loss 0.0639086, acc 0.96875, prec 0.0852974, recall 0.82952
2017-12-10T04:42:13.693334: step 5332, loss 0.0793697, acc 0.984375, prec 0.0853086, recall 0.829542
2017-12-10T04:42:13.965151: step 5333, loss 0.106414, acc 0.96875, prec 0.0853188, recall 0.829565
2017-12-10T04:42:14.236553: step 5334, loss 0.00233837, acc 1, prec 0.0853188, recall 0.829565
2017-12-10T04:42:14.506773: step 5335, loss 0.00626345, acc 1, prec 0.0853312, recall 0.829588
2017-12-10T04:42:14.772586: step 5336, loss 0.0110389, acc 1, prec 0.0853436, recall 0.82961
2017-12-10T04:42:15.036941: step 5337, loss 0.00821689, acc 1, prec 0.0853561, recall 0.829633
2017-12-10T04:42:15.305980: step 5338, loss 0.363756, acc 0.921875, prec 0.0853503, recall 0.829633
2017-12-10T04:42:15.573587: step 5339, loss 0.463917, acc 1, prec 0.0853751, recall 0.829678
2017-12-10T04:42:15.846216: step 5340, loss 2.92583, acc 0.96875, prec 0.085374, recall 0.829568
2017-12-10T04:42:16.113932: step 5341, loss 0.512278, acc 1, prec 0.0853864, recall 0.82959
2017-12-10T04:42:16.383639: step 5342, loss 1.60877, acc 0.96875, prec 0.0854101, recall 0.829526
2017-12-10T04:42:16.652857: step 5343, loss 0.177212, acc 0.953125, prec 0.0854066, recall 0.829526
2017-12-10T04:42:16.918467: step 5344, loss 0.141821, acc 0.953125, prec 0.0854156, recall 0.829548
2017-12-10T04:42:17.192088: step 5345, loss 0.282911, acc 0.890625, prec 0.0854075, recall 0.829548
2017-12-10T04:42:17.457128: step 5346, loss 0.338776, acc 0.90625, prec 0.0854005, recall 0.829548
2017-12-10T04:42:17.725803: step 5347, loss 0.59068, acc 0.796875, prec 0.0853854, recall 0.829548
2017-12-10T04:42:17.992585: step 5348, loss 0.688271, acc 0.875, prec 0.0853886, recall 0.829571
2017-12-10T04:42:18.258514: step 5349, loss 0.791641, acc 0.84375, prec 0.085377, recall 0.829571
2017-12-10T04:42:18.524910: step 5350, loss 0.322358, acc 0.90625, prec 0.08537, recall 0.829571
2017-12-10T04:42:18.794034: step 5351, loss 1.26453, acc 0.71875, prec 0.0853615, recall 0.829593
2017-12-10T04:42:19.059585: step 5352, loss 0.755307, acc 0.796875, prec 0.0853837, recall 0.829661
2017-12-10T04:42:19.331106: step 5353, loss 0.672626, acc 0.8125, prec 0.0853947, recall 0.829706
2017-12-10T04:42:19.602880: step 5354, loss 0.979649, acc 0.734375, prec 0.0853874, recall 0.829728
2017-12-10T04:42:19.869531: step 5355, loss 0.619467, acc 0.90625, prec 0.0853928, recall 0.829751
2017-12-10T04:42:20.130985: step 5356, loss 0.636611, acc 0.875, prec 0.0853836, recall 0.829751
2017-12-10T04:42:20.398316: step 5357, loss 0.520551, acc 0.90625, prec 0.085389, recall 0.829773
2017-12-10T04:42:20.664089: step 5358, loss 0.388024, acc 0.890625, prec 0.0854057, recall 0.829818
2017-12-10T04:42:20.927019: step 5359, loss 0.28793, acc 0.921875, prec 0.0854248, recall 0.829863
2017-12-10T04:42:21.192721: step 5360, loss 0.176761, acc 0.921875, prec 0.0854314, recall 0.829885
2017-12-10T04:42:21.457845: step 5361, loss 0.156955, acc 0.90625, prec 0.0854368, recall 0.829908
2017-12-10T04:42:21.727675: step 5362, loss 0.134345, acc 0.953125, prec 0.0854457, recall 0.82993
2017-12-10T04:42:21.992826: step 5363, loss 0.431429, acc 0.890625, prec 0.0854748, recall 0.829997
2017-12-10T04:42:22.258966: step 5364, loss 0.0126092, acc 1, prec 0.0854748, recall 0.829997
2017-12-10T04:42:22.522193: step 5365, loss 0.747831, acc 0.90625, prec 0.0854927, recall 0.830042
2017-12-10T04:42:22.788158: step 5366, loss 0.0746282, acc 0.96875, prec 0.0855028, recall 0.830064
2017-12-10T04:42:23.056265: step 5367, loss 0.00542852, acc 1, prec 0.0855028, recall 0.830064
2017-12-10T04:42:23.316396: step 5368, loss 2.98919, acc 0.953125, prec 0.0855004, recall 0.829955
2017-12-10T04:42:23.587830: step 5369, loss 0.238825, acc 0.953125, prec 0.085497, recall 0.829955
2017-12-10T04:42:23.860195: step 5370, loss 0.0532934, acc 0.984375, prec 0.0855206, recall 0.83
2017-12-10T04:42:24.136622: step 5371, loss 0.18689, acc 0.96875, prec 0.0855307, recall 0.830022
2017-12-10T04:42:24.399437: step 5372, loss 0.232793, acc 0.9375, prec 0.085526, recall 0.830022
2017-12-10T04:42:24.668899: step 5373, loss 0.100633, acc 0.984375, prec 0.0855373, recall 0.830045
2017-12-10T04:42:24.934129: step 5374, loss 0.0720374, acc 0.984375, prec 0.0855361, recall 0.830045
2017-12-10T04:42:25.202384: step 5375, loss 0.142581, acc 0.984375, prec 0.0855598, recall 0.830089
2017-12-10T04:42:25.472266: step 5376, loss 0.022757, acc 0.984375, prec 0.085571, recall 0.830112
2017-12-10T04:42:25.738959: step 5377, loss 0.145663, acc 0.984375, prec 0.0855822, recall 0.830134
2017-12-10T04:42:26.008959: step 5378, loss 0.139661, acc 0.96875, prec 0.0855799, recall 0.830134
2017-12-10T04:42:26.277909: step 5379, loss 0.0664902, acc 0.984375, prec 0.0855911, recall 0.830156
2017-12-10T04:42:26.545402: step 5380, loss 0.178295, acc 0.984375, prec 0.0856024, recall 0.830179
2017-12-10T04:42:26.812405: step 5381, loss 0.18549, acc 0.9375, prec 0.0855977, recall 0.830179
2017-12-10T04:42:27.091136: step 5382, loss 1.0867, acc 0.90625, prec 0.0856032, recall 0.830201
2017-12-10T04:42:27.358894: step 5383, loss 0.199622, acc 0.953125, prec 0.0855997, recall 0.830201
2017-12-10T04:42:27.625279: step 5384, loss 0.182397, acc 0.96875, prec 0.0856098, recall 0.830223
2017-12-10T04:42:27.890256: step 5385, loss 0.270069, acc 0.9375, prec 0.0856051, recall 0.830223
2017-12-10T04:42:28.156297: step 5386, loss 0.0636737, acc 0.96875, prec 0.0856028, recall 0.830223
2017-12-10T04:42:28.419516: step 5387, loss 0.120385, acc 0.953125, prec 0.0856489, recall 0.830313
2017-12-10T04:42:28.686229: step 5388, loss 0.107409, acc 0.953125, prec 0.0856578, recall 0.830335
2017-12-10T04:42:28.955265: step 5389, loss 0.0438324, acc 0.96875, prec 0.0856555, recall 0.830335
2017-12-10T04:42:29.220420: step 5390, loss 0.281085, acc 0.921875, prec 0.085662, recall 0.830357
2017-12-10T04:42:29.485521: step 5391, loss 0.274107, acc 0.921875, prec 0.0856686, recall 0.830379
2017-12-10T04:42:29.755202: step 5392, loss 0.0719006, acc 0.984375, prec 0.0856675, recall 0.830379
2017-12-10T04:42:30.018095: step 5393, loss 0.16005, acc 0.96875, prec 0.0856651, recall 0.830379
2017-12-10T04:42:30.290990: step 5394, loss 0.0579347, acc 0.96875, prec 0.0856752, recall 0.830402
2017-12-10T04:42:30.567749: step 5395, loss 0.0521534, acc 0.96875, prec 0.0856729, recall 0.830402
2017-12-10T04:42:30.841366: step 5396, loss 0.14632, acc 0.953125, prec 0.0857189, recall 0.830491
2017-12-10T04:42:31.107943: step 5397, loss 0.0776365, acc 0.96875, prec 0.0857166, recall 0.830491
2017-12-10T04:42:31.375266: step 5398, loss 0.155986, acc 0.96875, prec 0.0857267, recall 0.830513
2017-12-10T04:42:31.637918: step 5399, loss 0.0396643, acc 0.984375, prec 0.0857255, recall 0.830513
2017-12-10T04:42:31.900137: step 5400, loss 0.103444, acc 0.953125, prec 0.085722, recall 0.830513

Evaluation:
2017-12-10T04:42:39.499247: step 5400, loss 5.26853, acc 0.967258, prec 0.0860618, recall 0.821259

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-5400

2017-12-10T04:42:40.868644: step 5401, loss 0.00653787, acc 1, prec 0.0860741, recall 0.821282
2017-12-10T04:42:41.133622: step 5402, loss 1.27211, acc 0.984375, prec 0.0860741, recall 0.821176
2017-12-10T04:42:41.402551: step 5403, loss 0.0185048, acc 0.984375, prec 0.0860853, recall 0.821199
2017-12-10T04:42:41.668218: step 5404, loss 0.444004, acc 0.953125, prec 0.0860941, recall 0.821222
2017-12-10T04:42:41.941808: step 5405, loss 0.0292787, acc 0.984375, prec 0.0861053, recall 0.821245
2017-12-10T04:42:42.218364: step 5406, loss 0.0404175, acc 0.984375, prec 0.0861165, recall 0.821268
2017-12-10T04:42:42.488283: step 5407, loss 0.0130172, acc 1, prec 0.0861165, recall 0.821268
2017-12-10T04:42:42.753248: step 5408, loss 0.0740734, acc 0.953125, prec 0.0861253, recall 0.821291
2017-12-10T04:42:43.015176: step 5409, loss 0.0953468, acc 0.953125, prec 0.0861465, recall 0.821337
2017-12-10T04:42:43.282390: step 5410, loss 0.544266, acc 0.953125, prec 0.0861553, recall 0.82136
2017-12-10T04:42:43.545369: step 5411, loss 0.232568, acc 0.921875, prec 0.0861618, recall 0.821383
2017-12-10T04:42:43.811379: step 5412, loss 0.111636, acc 0.96875, prec 0.0861718, recall 0.821406
2017-12-10T04:42:44.076653: step 5413, loss 0.0124431, acc 1, prec 0.0861718, recall 0.821406
2017-12-10T04:42:44.344793: step 5414, loss 0.352605, acc 0.9375, prec 0.0861795, recall 0.821429
2017-12-10T04:42:44.616539: step 5415, loss 0.10534, acc 0.96875, prec 0.0861895, recall 0.821451
2017-12-10T04:42:44.885033: step 5416, loss 0.170203, acc 0.984375, prec 0.0862006, recall 0.821474
2017-12-10T04:42:45.158277: step 5417, loss 0.012329, acc 1, prec 0.0862129, recall 0.821497
2017-12-10T04:42:45.425976: step 5418, loss 0.242144, acc 0.9375, prec 0.0862206, recall 0.82152
2017-12-10T04:42:45.689667: step 5419, loss 0.0411546, acc 0.984375, prec 0.0862441, recall 0.821566
2017-12-10T04:42:45.958630: step 5420, loss 0.141907, acc 0.96875, prec 0.0862541, recall 0.821589
2017-12-10T04:42:46.231649: step 5421, loss 1.88073, acc 0.90625, prec 0.0862606, recall 0.821507
2017-12-10T04:42:46.501821: step 5422, loss 0.039015, acc 0.984375, prec 0.0862717, recall 0.821529
2017-12-10T04:42:46.772362: step 5423, loss 0.0865757, acc 0.953125, prec 0.0862805, recall 0.821552
2017-12-10T04:42:47.042441: step 5424, loss 0.165444, acc 0.953125, prec 0.086277, recall 0.821552
2017-12-10T04:42:47.307704: step 5425, loss 0.297783, acc 0.984375, prec 0.0863005, recall 0.821598
2017-12-10T04:42:47.581183: step 5426, loss 0.321767, acc 0.9375, prec 0.0863082, recall 0.821621
2017-12-10T04:42:47.853138: step 5427, loss 0.270086, acc 0.921875, prec 0.0863023, recall 0.821621
2017-12-10T04:42:48.116804: step 5428, loss 0.817969, acc 0.90625, prec 0.0863077, recall 0.821644
2017-12-10T04:42:48.384493: step 5429, loss 0.363965, acc 0.875, prec 0.0862984, recall 0.821644
2017-12-10T04:42:48.648252: step 5430, loss 0.346961, acc 0.96875, prec 0.0862961, recall 0.821644
2017-12-10T04:42:48.921113: step 5431, loss 0.319754, acc 0.875, prec 0.0863114, recall 0.82169
2017-12-10T04:42:49.186729: step 5432, loss 0.0204506, acc 0.984375, prec 0.0863348, recall 0.821735
2017-12-10T04:42:49.456661: step 5433, loss 0.153448, acc 0.984375, prec 0.0863459, recall 0.821758
2017-12-10T04:42:49.724046: step 5434, loss 0.41716, acc 0.921875, prec 0.0863401, recall 0.821758
2017-12-10T04:42:49.991756: step 5435, loss 0.253674, acc 0.953125, prec 0.086349, recall 0.821781
2017-12-10T04:42:50.256530: step 5436, loss 0.203387, acc 0.921875, prec 0.08638, recall 0.821849
2017-12-10T04:42:50.518660: step 5437, loss 0.30209, acc 0.96875, prec 0.0863777, recall 0.821849
2017-12-10T04:42:50.782452: step 5438, loss 0.255786, acc 0.953125, prec 0.0863865, recall 0.821872
2017-12-10T04:42:51.048624: step 5439, loss 0.201753, acc 0.96875, prec 0.0864088, recall 0.821918
2017-12-10T04:42:51.316969: step 5440, loss 0.0655588, acc 0.96875, prec 0.0864433, recall 0.821986
2017-12-10T04:42:51.581976: step 5441, loss 0.123349, acc 0.984375, prec 0.0864668, recall 0.822032
2017-12-10T04:42:51.854886: step 5442, loss 0.32803, acc 0.921875, prec 0.0864732, recall 0.822055
2017-12-10T04:42:52.125497: step 5443, loss 0.0575121, acc 0.96875, prec 0.0864955, recall 0.8221
2017-12-10T04:42:52.401420: step 5444, loss 0.155129, acc 0.953125, prec 0.0865043, recall 0.822123
2017-12-10T04:42:52.676677: step 5445, loss 0.420767, acc 0.9375, prec 0.0865119, recall 0.822146
2017-12-10T04:42:52.952767: step 5446, loss 0.125734, acc 0.9375, prec 0.0865073, recall 0.822146
2017-12-10T04:42:53.222086: step 5447, loss 0.0698253, acc 0.984375, prec 0.0865184, recall 0.822168
2017-12-10T04:42:53.488683: step 5448, loss 0.0800875, acc 0.96875, prec 0.0865161, recall 0.822168
2017-12-10T04:42:53.757166: step 5449, loss 0.129409, acc 0.96875, prec 0.0865138, recall 0.822168
2017-12-10T04:42:54.034231: step 5450, loss 0.403516, acc 0.9375, prec 0.086546, recall 0.822236
2017-12-10T04:42:54.299327: step 5451, loss 0.069827, acc 0.984375, prec 0.0865817, recall 0.822305
2017-12-10T04:42:54.563372: step 5452, loss 0.232695, acc 0.9375, prec 0.086577, recall 0.822305
2017-12-10T04:42:54.832868: step 5453, loss 0.574875, acc 0.921875, prec 0.0865712, recall 0.822305
2017-12-10T04:42:55.093212: step 5454, loss 0.141081, acc 0.953125, prec 0.08658, recall 0.822327
2017-12-10T04:42:55.369068: step 5455, loss 0.262734, acc 0.96875, prec 0.0865776, recall 0.822327
2017-12-10T04:42:55.641080: step 5456, loss 0.0289992, acc 0.96875, prec 0.0865753, recall 0.822327
2017-12-10T04:42:55.908117: step 5457, loss 0.0476143, acc 0.984375, prec 0.0865864, recall 0.82235
2017-12-10T04:42:56.173255: step 5458, loss 0.4787, acc 0.96875, prec 0.0865964, recall 0.822373
2017-12-10T04:42:56.445725: step 5459, loss 0.113076, acc 0.96875, prec 0.0865941, recall 0.822373
2017-12-10T04:42:56.710966: step 5460, loss 0.00595188, acc 1, prec 0.0865941, recall 0.822373
2017-12-10T04:42:56.985677: step 5461, loss 0.0135862, acc 0.984375, prec 0.0865929, recall 0.822373
2017-12-10T04:42:57.253016: step 5462, loss 0.0118789, acc 1, prec 0.0865929, recall 0.822373
2017-12-10T04:42:57.518445: step 5463, loss 0.338625, acc 0.984375, prec 0.086604, recall 0.822395
2017-12-10T04:42:57.784220: step 5464, loss 0.4142, acc 0.96875, prec 0.086614, recall 0.822418
2017-12-10T04:42:58.048830: step 5465, loss 0.00375986, acc 1, prec 0.086614, recall 0.822418
2017-12-10T04:42:58.316627: step 5466, loss 0.187352, acc 0.953125, prec 0.0866105, recall 0.822418
2017-12-10T04:42:58.551831: step 5467, loss 0.0400187, acc 0.980769, prec 0.0866216, recall 0.822441
2017-12-10T04:42:58.828035: step 5468, loss 0.135228, acc 0.96875, prec 0.0866193, recall 0.822441
2017-12-10T04:42:59.095959: step 5469, loss 0.0204017, acc 0.984375, prec 0.0866181, recall 0.822441
2017-12-10T04:42:59.360535: step 5470, loss 0.00506915, acc 1, prec 0.0866426, recall 0.822486
2017-12-10T04:42:59.630766: step 5471, loss 0.00380779, acc 1, prec 0.0866426, recall 0.822486
2017-12-10T04:42:59.897321: step 5472, loss 0.00471442, acc 1, prec 0.0866672, recall 0.822531
2017-12-10T04:43:00.167121: step 5473, loss 0.0850893, acc 0.984375, prec 0.086666, recall 0.822531
2017-12-10T04:43:00.433612: step 5474, loss 0.325582, acc 0.9375, prec 0.0866614, recall 0.822531
2017-12-10T04:43:00.705858: step 5475, loss 1.80087, acc 0.984375, prec 0.0866737, recall 0.822449
2017-12-10T04:43:00.979307: step 5476, loss 0.0980752, acc 1, prec 0.0866859, recall 0.822472
2017-12-10T04:43:01.246880: step 5477, loss 0.0110344, acc 1, prec 0.0866859, recall 0.822472
2017-12-10T04:43:01.511703: step 5478, loss 0.0904495, acc 0.96875, prec 0.0866959, recall 0.822494
2017-12-10T04:43:01.777998: step 5479, loss 0.175307, acc 0.9375, prec 0.0867403, recall 0.822585
2017-12-10T04:43:02.043115: step 5480, loss 0.0953828, acc 0.96875, prec 0.0867748, recall 0.822653
2017-12-10T04:43:02.309523: step 5481, loss 0.0103365, acc 1, prec 0.0868116, recall 0.82272
2017-12-10T04:43:02.572463: step 5482, loss 0.0212762, acc 0.984375, prec 0.0868227, recall 0.822743
2017-12-10T04:43:02.839318: step 5483, loss 0.0168873, acc 1, prec 0.0868227, recall 0.822743
2017-12-10T04:43:03.107001: step 5484, loss 0.0606415, acc 0.96875, prec 0.0868204, recall 0.822743
2017-12-10T04:43:03.375747: step 5485, loss 0.101691, acc 0.953125, prec 0.0868169, recall 0.822743
2017-12-10T04:43:03.643863: step 5486, loss 0.373159, acc 0.921875, prec 0.0868111, recall 0.822743
2017-12-10T04:43:03.911503: step 5487, loss 0.210396, acc 0.921875, prec 0.0868052, recall 0.822743
2017-12-10T04:43:04.180016: step 5488, loss 0.114841, acc 0.953125, prec 0.086814, recall 0.822765
2017-12-10T04:43:04.444374: step 5489, loss 0.0989003, acc 0.953125, prec 0.0868105, recall 0.822765
2017-12-10T04:43:04.717268: step 5490, loss 0.0711484, acc 0.96875, prec 0.0868327, recall 0.822811
2017-12-10T04:43:04.980623: step 5491, loss 0.0370264, acc 0.984375, prec 0.0868683, recall 0.822878
2017-12-10T04:43:05.250909: step 5492, loss 0.210427, acc 0.96875, prec 0.086866, recall 0.822878
2017-12-10T04:43:05.516825: step 5493, loss 0.423029, acc 0.96875, prec 0.0868759, recall 0.822901
2017-12-10T04:43:05.784047: step 5494, loss 0.0860589, acc 0.96875, prec 0.0868859, recall 0.822923
2017-12-10T04:43:06.047120: step 5495, loss 0.228365, acc 0.921875, prec 0.08688, recall 0.822923
2017-12-10T04:43:06.311544: step 5496, loss 0.384373, acc 0.921875, prec 0.0868987, recall 0.822968
2017-12-10T04:43:06.577123: step 5497, loss 0.0872495, acc 0.96875, prec 0.0868964, recall 0.822968
2017-12-10T04:43:06.841410: step 5498, loss 0.100762, acc 0.921875, prec 0.0869273, recall 0.823036
2017-12-10T04:43:07.110001: step 5499, loss 0.0148336, acc 1, prec 0.0869396, recall 0.823058
2017-12-10T04:43:07.375524: step 5500, loss 0.0687088, acc 0.96875, prec 0.0869495, recall 0.823081
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-5500

2017-12-10T04:43:08.715542: step 5501, loss 0.145989, acc 0.96875, prec 0.0869594, recall 0.823103
2017-12-10T04:43:08.984235: step 5502, loss 0.20447, acc 0.96875, prec 0.0869694, recall 0.823126
2017-12-10T04:43:09.251321: step 5503, loss 0.038668, acc 0.984375, prec 0.0869805, recall 0.823148
2017-12-10T04:43:09.522581: step 5504, loss 0.00598102, acc 1, prec 0.0869927, recall 0.823171
2017-12-10T04:43:09.792283: step 5505, loss 0.0284383, acc 1, prec 0.0870172, recall 0.823216
2017-12-10T04:43:10.055995: step 5506, loss 0.0107515, acc 1, prec 0.0870172, recall 0.823216
2017-12-10T04:43:10.334119: step 5507, loss 0.308958, acc 0.96875, prec 0.0870271, recall 0.823238
2017-12-10T04:43:10.597348: step 5508, loss 0.0693267, acc 0.96875, prec 0.0870371, recall 0.823261
2017-12-10T04:43:10.863904: step 5509, loss 0.0545316, acc 0.96875, prec 0.0870347, recall 0.823261
2017-12-10T04:43:11.124415: step 5510, loss 0.00641318, acc 1, prec 0.0870347, recall 0.823261
2017-12-10T04:43:11.386251: step 5511, loss 0.116181, acc 0.984375, prec 0.0870458, recall 0.823283
2017-12-10T04:43:11.656819: step 5512, loss 0.0108615, acc 1, prec 0.0870458, recall 0.823283
2017-12-10T04:43:11.926921: step 5513, loss 0.183635, acc 1, prec 0.0870581, recall 0.823305
2017-12-10T04:43:12.205078: step 5514, loss 0.0355306, acc 0.96875, prec 0.087068, recall 0.823328
2017-12-10T04:43:12.473571: step 5515, loss 0.0506597, acc 0.984375, prec 0.0870791, recall 0.82335
2017-12-10T04:43:12.743076: step 5516, loss 0.00931269, acc 1, prec 0.0870791, recall 0.82335
2017-12-10T04:43:13.016220: step 5517, loss 0.616936, acc 0.984375, prec 0.0871147, recall 0.823417
2017-12-10T04:43:13.288668: step 5518, loss 0.0586521, acc 0.984375, prec 0.087138, recall 0.823462
2017-12-10T04:43:13.553782: step 5519, loss 0.528237, acc 0.96875, prec 0.0871479, recall 0.823485
2017-12-10T04:43:13.824988: step 5520, loss 0.231353, acc 0.984375, prec 0.087159, recall 0.823507
2017-12-10T04:43:14.104851: step 5521, loss 0.00528947, acc 1, prec 0.087159, recall 0.823507
2017-12-10T04:43:14.378429: step 5522, loss 1.53611, acc 0.984375, prec 0.0871712, recall 0.823425
2017-12-10T04:43:14.655609: step 5523, loss 0.083568, acc 0.96875, prec 0.0871934, recall 0.82347
2017-12-10T04:43:14.925305: step 5524, loss 0.0462157, acc 0.984375, prec 0.0871922, recall 0.82347
2017-12-10T04:43:15.187630: step 5525, loss 0.0864885, acc 0.984375, prec 0.0872155, recall 0.823515
2017-12-10T04:43:15.447773: step 5526, loss 0.0707059, acc 0.96875, prec 0.0872132, recall 0.823515
2017-12-10T04:43:15.722365: step 5527, loss 0.165846, acc 0.953125, prec 0.0872097, recall 0.823515
2017-12-10T04:43:15.985082: step 5528, loss 0.20464, acc 0.984375, prec 0.0872575, recall 0.823604
2017-12-10T04:43:16.256862: step 5529, loss 0.278259, acc 0.875, prec 0.0872481, recall 0.823604
2017-12-10T04:43:16.523185: step 5530, loss 0.0793699, acc 0.984375, prec 0.0872592, recall 0.823626
2017-12-10T04:43:16.793898: step 5531, loss 0.186309, acc 0.953125, prec 0.0872557, recall 0.823626
2017-12-10T04:43:17.058200: step 5532, loss 0.359232, acc 0.96875, prec 0.0872779, recall 0.823671
2017-12-10T04:43:17.326299: step 5533, loss 0.253794, acc 0.9375, prec 0.0872854, recall 0.823693
2017-12-10T04:43:17.600783: step 5534, loss 0.291568, acc 0.875, prec 0.087276, recall 0.823693
2017-12-10T04:43:17.873135: step 5535, loss 0.428947, acc 0.921875, prec 0.0872702, recall 0.823693
2017-12-10T04:43:18.148546: step 5536, loss 0.803959, acc 0.859375, prec 0.0872597, recall 0.823693
2017-12-10T04:43:18.412723: step 5537, loss 0.402466, acc 0.890625, prec 0.0872637, recall 0.823716
2017-12-10T04:43:18.677537: step 5538, loss 0.289824, acc 0.859375, prec 0.0872654, recall 0.823738
2017-12-10T04:43:18.943465: step 5539, loss 0.18805, acc 0.90625, prec 0.0872584, recall 0.823738
2017-12-10T04:43:19.209566: step 5540, loss 0.182426, acc 0.9375, prec 0.0872537, recall 0.823738
2017-12-10T04:43:19.476499: step 5541, loss 0.18035, acc 0.96875, prec 0.0872514, recall 0.823738
2017-12-10T04:43:19.749750: step 5542, loss 0.184447, acc 0.96875, prec 0.087249, recall 0.823738
2017-12-10T04:43:20.020059: step 5543, loss 0.156031, acc 0.9375, prec 0.0872444, recall 0.823738
2017-12-10T04:43:20.286413: step 5544, loss 0.191094, acc 0.9375, prec 0.0872519, recall 0.82376
2017-12-10T04:43:20.554539: step 5545, loss 0.0543408, acc 0.953125, prec 0.0872484, recall 0.82376
2017-12-10T04:43:20.825120: step 5546, loss 0.0677502, acc 0.953125, prec 0.0872449, recall 0.82376
2017-12-10T04:43:21.095680: step 5547, loss 0.0368962, acc 0.984375, prec 0.0872437, recall 0.82376
2017-12-10T04:43:21.370876: step 5548, loss 0.0303574, acc 0.984375, prec 0.0872426, recall 0.82376
2017-12-10T04:43:21.637301: step 5549, loss 0.00472432, acc 1, prec 0.0872426, recall 0.82376
2017-12-10T04:43:21.911930: step 5550, loss 0.288734, acc 0.96875, prec 0.0872525, recall 0.823782
2017-12-10T04:43:22.177433: step 5551, loss 0.00383108, acc 1, prec 0.0872647, recall 0.823805
2017-12-10T04:43:22.450306: step 5552, loss 0.199496, acc 0.96875, prec 0.0872868, recall 0.823849
2017-12-10T04:43:22.717661: step 5553, loss 0.476134, acc 0.96875, prec 0.0872845, recall 0.823849
2017-12-10T04:43:22.984235: step 5554, loss 0.0707841, acc 0.984375, prec 0.0872955, recall 0.823872
2017-12-10T04:43:23.253522: step 5555, loss 0.229001, acc 0.953125, prec 0.087292, recall 0.823872
2017-12-10T04:43:23.523070: step 5556, loss 0.0282518, acc 0.984375, prec 0.0873275, recall 0.823938
2017-12-10T04:43:23.794004: step 5557, loss 0.00160894, acc 1, prec 0.0873275, recall 0.823938
2017-12-10T04:43:24.063987: step 5558, loss 0.1499, acc 0.984375, prec 0.0873264, recall 0.823938
2017-12-10T04:43:24.330299: step 5559, loss 0.00741992, acc 1, prec 0.0873264, recall 0.823938
2017-12-10T04:43:24.594604: step 5560, loss 5.47432, acc 0.96875, prec 0.0873252, recall 0.823834
2017-12-10T04:43:24.867283: step 5561, loss 0.140859, acc 0.96875, prec 0.0873351, recall 0.823856
2017-12-10T04:43:25.133591: step 5562, loss 0.0751193, acc 0.984375, prec 0.0873339, recall 0.823856
2017-12-10T04:43:25.400131: step 5563, loss 0.051337, acc 0.984375, prec 0.0873327, recall 0.823856
2017-12-10T04:43:25.665447: step 5564, loss 0.0819597, acc 0.984375, prec 0.087356, recall 0.823901
2017-12-10T04:43:25.935095: step 5565, loss 0.153091, acc 0.953125, prec 0.087377, recall 0.823945
2017-12-10T04:43:26.201548: step 5566, loss 0.117849, acc 0.953125, prec 0.0873857, recall 0.823968
2017-12-10T04:43:26.468754: step 5567, loss 0.338779, acc 0.953125, prec 0.0873822, recall 0.823968
2017-12-10T04:43:26.732309: step 5568, loss 0.1261, acc 0.953125, prec 0.0873909, recall 0.82399
2017-12-10T04:43:27.006143: step 5569, loss 0.35334, acc 0.890625, prec 0.0874071, recall 0.824034
2017-12-10T04:43:27.279929: step 5570, loss 0.359744, acc 0.953125, prec 0.0874158, recall 0.824057
2017-12-10T04:43:27.552145: step 5571, loss 0.327984, acc 0.921875, prec 0.0874222, recall 0.824079
2017-12-10T04:43:27.814310: step 5572, loss 0.302838, acc 0.90625, prec 0.0874274, recall 0.824101
2017-12-10T04:43:28.086927: step 5573, loss 0.0637236, acc 0.984375, prec 0.0874384, recall 0.824123
2017-12-10T04:43:28.357650: step 5574, loss 0.447172, acc 0.921875, prec 0.0874448, recall 0.824145
2017-12-10T04:43:28.630460: step 5575, loss 0.425742, acc 0.921875, prec 0.0874389, recall 0.824145
2017-12-10T04:43:28.896637: step 5576, loss 0.0210676, acc 1, prec 0.0874389, recall 0.824145
2017-12-10T04:43:29.168080: step 5577, loss 0.203603, acc 0.921875, prec 0.0874453, recall 0.824167
2017-12-10T04:43:29.441482: step 5578, loss 0.218406, acc 0.953125, prec 0.0874418, recall 0.824167
2017-12-10T04:43:29.708741: step 5579, loss 0.0487174, acc 0.984375, prec 0.087465, recall 0.824212
2017-12-10T04:43:29.973771: step 5580, loss 0.130448, acc 0.9375, prec 0.0874848, recall 0.824256
2017-12-10T04:43:30.245696: step 5581, loss 0.176248, acc 0.953125, prec 0.0874935, recall 0.824278
2017-12-10T04:43:30.518501: step 5582, loss 0.0795088, acc 0.96875, prec 0.0875156, recall 0.824323
2017-12-10T04:43:30.794324: step 5583, loss 0.264149, acc 0.890625, prec 0.0875074, recall 0.824323
2017-12-10T04:43:31.062220: step 5584, loss 0.136982, acc 0.96875, prec 0.0875294, recall 0.824367
2017-12-10T04:43:31.334855: step 5585, loss 0.0252464, acc 0.984375, prec 0.0875527, recall 0.824411
2017-12-10T04:43:31.601578: step 5586, loss 0.0757515, acc 0.96875, prec 0.0875625, recall 0.824433
2017-12-10T04:43:31.874396: step 5587, loss 0.184932, acc 0.953125, prec 0.0875834, recall 0.824477
2017-12-10T04:43:32.144452: step 5588, loss 0.0941579, acc 0.96875, prec 0.0875811, recall 0.824477
2017-12-10T04:43:32.414951: step 5589, loss 0.0775114, acc 0.984375, prec 0.0875799, recall 0.824477
2017-12-10T04:43:32.679961: step 5590, loss 0.0350381, acc 0.984375, prec 0.0875909, recall 0.8245
2017-12-10T04:43:32.950126: step 5591, loss 0.0669527, acc 0.984375, prec 0.0875898, recall 0.8245
2017-12-10T04:43:33.216902: step 5592, loss 0.251221, acc 0.953125, prec 0.0875863, recall 0.8245
2017-12-10T04:43:33.487453: step 5593, loss 0.58065, acc 0.984375, prec 0.0875973, recall 0.824522
2017-12-10T04:43:33.752349: step 5594, loss 0.136652, acc 0.984375, prec 0.0876083, recall 0.824544
2017-12-10T04:43:34.020237: step 5595, loss 0.0274378, acc 0.984375, prec 0.0876072, recall 0.824544
2017-12-10T04:43:34.285202: step 5596, loss 0.0202129, acc 1, prec 0.0876194, recall 0.824566
2017-12-10T04:43:34.559955: step 5597, loss 0.0499045, acc 0.984375, prec 0.0876304, recall 0.824588
2017-12-10T04:43:34.825205: step 5598, loss 0.11371, acc 0.96875, prec 0.087628, recall 0.824588
2017-12-10T04:43:35.092361: step 5599, loss 0.0810802, acc 0.984375, prec 0.0876269, recall 0.824588
2017-12-10T04:43:35.352894: step 5600, loss 0.0538135, acc 0.96875, prec 0.0876489, recall 0.824632
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-5600

2017-12-10T04:43:36.639291: step 5601, loss 0.233202, acc 0.9375, prec 0.0876686, recall 0.824676
2017-12-10T04:43:36.904070: step 5602, loss 0.0139377, acc 0.984375, prec 0.0876675, recall 0.824676
2017-12-10T04:43:37.171593: step 5603, loss 0.105971, acc 0.953125, prec 0.0876639, recall 0.824676
2017-12-10T04:43:37.441992: step 5604, loss 0.0664915, acc 0.96875, prec 0.0876616, recall 0.824676
2017-12-10T04:43:37.708636: step 5605, loss 0.0181238, acc 1, prec 0.0876616, recall 0.824676
2017-12-10T04:43:37.969589: step 5606, loss 0.0112721, acc 1, prec 0.0876738, recall 0.824698
2017-12-10T04:43:38.232354: step 5607, loss 0.00123148, acc 1, prec 0.0876982, recall 0.824742
2017-12-10T04:43:38.500175: step 5608, loss 0.248824, acc 0.9375, prec 0.0876935, recall 0.824742
2017-12-10T04:43:38.762016: step 5609, loss 0.0580108, acc 0.984375, prec 0.0876923, recall 0.824742
2017-12-10T04:43:39.027982: step 5610, loss 0.287182, acc 0.984375, prec 0.0877034, recall 0.824764
2017-12-10T04:43:39.297158: step 5611, loss 0.0711914, acc 0.96875, prec 0.087701, recall 0.824764
2017-12-10T04:43:39.566342: step 5612, loss 0.134904, acc 0.984375, prec 0.087712, recall 0.824786
2017-12-10T04:43:39.836861: step 5613, loss 0.0572345, acc 0.984375, prec 0.0877109, recall 0.824786
2017-12-10T04:43:40.103661: step 5614, loss 0.0890962, acc 0.984375, prec 0.0877219, recall 0.824808
2017-12-10T04:43:40.375884: step 5615, loss 0.0937063, acc 0.96875, prec 0.0877195, recall 0.824808
2017-12-10T04:43:40.642348: step 5616, loss 0.00297877, acc 1, prec 0.0877195, recall 0.824808
2017-12-10T04:43:40.919266: step 5617, loss 0.152388, acc 0.96875, prec 0.0877294, recall 0.82483
2017-12-10T04:43:41.191235: step 5618, loss 0.0523066, acc 0.984375, prec 0.0877648, recall 0.824896
2017-12-10T04:43:41.464099: step 5619, loss 0.021756, acc 0.984375, prec 0.0877636, recall 0.824896
2017-12-10T04:43:41.733704: step 5620, loss 0.132235, acc 0.984375, prec 0.0877868, recall 0.82494
2017-12-10T04:43:42.007175: step 5621, loss 0.132387, acc 0.953125, prec 0.0877955, recall 0.824962
2017-12-10T04:43:42.278722: step 5622, loss 0.171532, acc 0.984375, prec 0.0878065, recall 0.824984
2017-12-10T04:43:42.547047: step 5623, loss 5.22025, acc 0.953125, prec 0.0878164, recall 0.824903
2017-12-10T04:43:42.819856: step 5624, loss 0.0234788, acc 0.984375, prec 0.0878152, recall 0.824903
2017-12-10T04:43:43.085474: step 5625, loss 0.163693, acc 0.953125, prec 0.0878117, recall 0.824903
2017-12-10T04:43:43.350198: step 5626, loss 0.642558, acc 0.9375, prec 0.087807, recall 0.824903
2017-12-10T04:43:43.618703: step 5627, loss 0.0463739, acc 0.96875, prec 0.0878168, recall 0.824925
2017-12-10T04:43:43.885277: step 5628, loss 0.313677, acc 0.90625, prec 0.087822, recall 0.824947
2017-12-10T04:43:44.152589: step 5629, loss 0.565605, acc 0.875, prec 0.0878126, recall 0.824947
2017-12-10T04:43:44.430888: step 5630, loss 0.149759, acc 0.96875, prec 0.0878102, recall 0.824947
2017-12-10T04:43:44.704842: step 5631, loss 0.107606, acc 0.96875, prec 0.0878079, recall 0.824947
2017-12-10T04:43:44.969190: step 5632, loss 0.340667, acc 0.90625, prec 0.0878008, recall 0.824947
2017-12-10T04:43:45.238621: step 5633, loss 0.547278, acc 0.828125, prec 0.0877879, recall 0.824947
2017-12-10T04:43:45.514935: step 5634, loss 0.561935, acc 0.890625, prec 0.0877797, recall 0.824947
2017-12-10T04:43:45.779763: step 5635, loss 0.343061, acc 0.9375, prec 0.0877751, recall 0.824947
2017-12-10T04:43:46.050824: step 5636, loss 0.363637, acc 0.921875, prec 0.0877814, recall 0.824969
2017-12-10T04:43:46.318413: step 5637, loss 0.471677, acc 0.90625, prec 0.0877743, recall 0.824969
2017-12-10T04:43:46.584324: step 5638, loss 0.328825, acc 0.90625, prec 0.0877795, recall 0.824991
2017-12-10T04:43:46.855209: step 5639, loss 0.692169, acc 0.90625, prec 0.087809, recall 0.825056
2017-12-10T04:43:47.126119: step 5640, loss 0.370748, acc 0.921875, prec 0.0878031, recall 0.825056
2017-12-10T04:43:47.393544: step 5641, loss 0.20468, acc 0.921875, prec 0.0878094, recall 0.825078
2017-12-10T04:43:47.659602: step 5642, loss 0.223844, acc 0.953125, prec 0.0878059, recall 0.825078
2017-12-10T04:43:47.934350: step 5643, loss 0.293987, acc 0.90625, prec 0.0877989, recall 0.825078
2017-12-10T04:43:48.199733: step 5644, loss 0.418974, acc 0.96875, prec 0.0878209, recall 0.825122
2017-12-10T04:43:48.472750: step 5645, loss 9.42683, acc 0.953125, prec 0.0878185, recall 0.825019
2017-12-10T04:43:48.749514: step 5646, loss 0.254577, acc 0.9375, prec 0.087826, recall 0.825041
2017-12-10T04:43:49.025016: step 5647, loss 0.130917, acc 0.96875, prec 0.087848, recall 0.825085
2017-12-10T04:43:49.297458: step 5648, loss 0.21798, acc 0.921875, prec 0.0878422, recall 0.825085
2017-12-10T04:43:49.564183: step 5649, loss 0.356419, acc 0.859375, prec 0.0878316, recall 0.825085
2017-12-10T04:43:49.835760: step 5650, loss 0.284745, acc 0.953125, prec 0.0878524, recall 0.825128
2017-12-10T04:43:50.103022: step 5651, loss 0.249824, acc 0.921875, prec 0.0878709, recall 0.825172
2017-12-10T04:43:50.370360: step 5652, loss 0.781408, acc 0.828125, prec 0.0878702, recall 0.825194
2017-12-10T04:43:50.641205: step 5653, loss 0.324282, acc 0.890625, prec 0.0878741, recall 0.825216
2017-12-10T04:43:50.910415: step 5654, loss 0.323521, acc 0.875, prec 0.0878891, recall 0.82526
2017-12-10T04:43:51.184959: step 5655, loss 0.197002, acc 0.9375, prec 0.087933, recall 0.825347
2017-12-10T04:43:51.449449: step 5656, loss 0.338743, acc 0.921875, prec 0.0879393, recall 0.825369
2017-12-10T04:43:51.712129: step 5657, loss 0.516781, acc 0.890625, prec 0.0879433, recall 0.825391
2017-12-10T04:43:51.974541: step 5658, loss 0.234535, acc 0.9375, prec 0.0879629, recall 0.825435
2017-12-10T04:43:52.240717: step 5659, loss 0.476513, acc 0.90625, prec 0.0879559, recall 0.825435
2017-12-10T04:43:52.504303: step 5660, loss 0.480812, acc 0.90625, prec 0.087961, recall 0.825456
2017-12-10T04:43:52.773008: step 5661, loss 0.0958892, acc 0.953125, prec 0.0879939, recall 0.825522
2017-12-10T04:43:53.044628: step 5662, loss 1.18105, acc 0.90625, prec 0.0879869, recall 0.825522
2017-12-10T04:43:53.307885: step 5663, loss 0.127613, acc 0.96875, prec 0.0880088, recall 0.825565
2017-12-10T04:43:53.571745: step 5664, loss 0.234392, acc 0.96875, prec 0.0880065, recall 0.825565
2017-12-10T04:43:53.843336: step 5665, loss 0.410924, acc 0.953125, prec 0.0880151, recall 0.825587
2017-12-10T04:43:54.106616: step 5666, loss 0.101621, acc 0.9375, prec 0.0880226, recall 0.825609
2017-12-10T04:43:54.384734: step 5667, loss 0.00679164, acc 1, prec 0.0880226, recall 0.825609
2017-12-10T04:43:54.652508: step 5668, loss 0.0483594, acc 0.984375, prec 0.0880214, recall 0.825609
2017-12-10T04:43:54.924558: step 5669, loss 0.009283, acc 1, prec 0.0880336, recall 0.825631
2017-12-10T04:43:55.189385: step 5670, loss 2.7222, acc 0.953125, prec 0.0880555, recall 0.825571
2017-12-10T04:43:55.455479: step 5671, loss 0.480733, acc 0.921875, prec 0.0880618, recall 0.825593
2017-12-10T04:43:55.722983: step 5672, loss 0.102328, acc 0.984375, prec 0.0880728, recall 0.825615
2017-12-10T04:43:55.995367: step 5673, loss 0.109006, acc 0.984375, prec 0.0880716, recall 0.825615
2017-12-10T04:43:56.264302: step 5674, loss 0.0185084, acc 1, prec 0.088108, recall 0.82568
2017-12-10T04:43:56.532008: step 5675, loss 0.208399, acc 0.96875, prec 0.0881057, recall 0.82568
2017-12-10T04:43:56.800829: step 5676, loss 0.444367, acc 0.9375, prec 0.088101, recall 0.82568
2017-12-10T04:43:57.073533: step 5677, loss 0.110768, acc 0.96875, prec 0.0881108, recall 0.825702
2017-12-10T04:43:57.335944: step 5678, loss 0.224529, acc 0.90625, prec 0.0881159, recall 0.825724
2017-12-10T04:43:57.600966: step 5679, loss 0.0929204, acc 0.953125, prec 0.0881245, recall 0.825745
2017-12-10T04:43:57.872329: step 5680, loss 0.0361526, acc 0.984375, prec 0.0881355, recall 0.825767
2017-12-10T04:43:58.134396: step 5681, loss 0.319514, acc 0.90625, prec 0.0881527, recall 0.82581
2017-12-10T04:43:58.409099: step 5682, loss 0.121906, acc 0.96875, prec 0.0881625, recall 0.825832
2017-12-10T04:43:58.670076: step 5683, loss 0.3537, acc 0.953125, prec 0.088159, recall 0.825832
2017-12-10T04:43:58.930550: step 5684, loss 0.156648, acc 0.953125, prec 0.0881554, recall 0.825832
2017-12-10T04:43:59.205064: step 5685, loss 0.516216, acc 0.90625, prec 0.0881605, recall 0.825854
2017-12-10T04:43:59.476889: step 5686, loss 0.532518, acc 0.953125, prec 0.0881813, recall 0.825897
2017-12-10T04:43:59.743445: step 5687, loss 0.176858, acc 0.984375, prec 0.0881922, recall 0.825919
2017-12-10T04:44:00.010130: step 5688, loss 0.22264, acc 0.953125, prec 0.0881887, recall 0.825919
2017-12-10T04:44:00.287155: step 5689, loss 0.163853, acc 0.953125, prec 0.0881852, recall 0.825919
2017-12-10T04:44:00.561790: step 5690, loss 0.664723, acc 0.921875, prec 0.0882036, recall 0.825962
2017-12-10T04:44:00.833719: step 5691, loss 0.356649, acc 0.9375, prec 0.088211, recall 0.825984
2017-12-10T04:44:01.106031: step 5692, loss 0.233592, acc 0.953125, prec 0.0882196, recall 0.826006
2017-12-10T04:44:01.369577: step 5693, loss 0.442863, acc 0.90625, prec 0.0882126, recall 0.826006
2017-12-10T04:44:01.630040: step 5694, loss 0.410228, acc 0.921875, prec 0.0882431, recall 0.826071
2017-12-10T04:44:01.898575: step 5695, loss 0.264197, acc 0.953125, prec 0.0882517, recall 0.826092
2017-12-10T04:44:02.166450: step 5696, loss 0.457352, acc 0.921875, prec 0.0882459, recall 0.826092
2017-12-10T04:44:02.432231: step 5697, loss 0.122551, acc 0.96875, prec 0.0882678, recall 0.826136
2017-12-10T04:44:02.697574: step 5698, loss 0.0871531, acc 0.984375, prec 0.0882908, recall 0.826179
2017-12-10T04:44:02.962556: step 5699, loss 0.0791074, acc 0.96875, prec 0.0883127, recall 0.826222
2017-12-10T04:44:03.231434: step 5700, loss 0.212381, acc 0.953125, prec 0.0883213, recall 0.826244

Evaluation:
2017-12-10T04:44:10.903596: step 5700, loss 5.50626, acc 0.965937, prec 0.0886282, recall 0.817538

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-5700

2017-12-10T04:44:12.220479: step 5701, loss 0.48593, acc 0.953125, prec 0.0886368, recall 0.81756
2017-12-10T04:44:12.490069: step 5702, loss 0.0316109, acc 0.96875, prec 0.0886585, recall 0.817605
2017-12-10T04:44:12.754686: step 5703, loss 0.0487591, acc 0.984375, prec 0.0886694, recall 0.817627
2017-12-10T04:44:13.022103: step 5704, loss 0.0158844, acc 0.984375, prec 0.0886803, recall 0.817649
2017-12-10T04:44:13.291005: step 5705, loss 0.219518, acc 0.96875, prec 0.08869, recall 0.817671
2017-12-10T04:44:13.555648: step 5706, loss 0.928102, acc 1, prec 0.0887142, recall 0.817716
2017-12-10T04:44:13.832512: step 5707, loss 0.0396755, acc 0.984375, prec 0.088713, recall 0.817716
2017-12-10T04:44:14.099250: step 5708, loss 0.0537301, acc 0.96875, prec 0.0887106, recall 0.817716
2017-12-10T04:44:14.376355: step 5709, loss 0.109836, acc 0.984375, prec 0.0887095, recall 0.817716
2017-12-10T04:44:14.644085: step 5710, loss 0.428279, acc 0.96875, prec 0.0887192, recall 0.817738
2017-12-10T04:44:14.913211: step 5711, loss 0.0295372, acc 0.984375, prec 0.0887301, recall 0.81776
2017-12-10T04:44:15.176502: step 5712, loss 0.0721086, acc 0.96875, prec 0.0887398, recall 0.817783
2017-12-10T04:44:15.452638: step 5713, loss 0.0237549, acc 0.984375, prec 0.0887386, recall 0.817783
2017-12-10T04:44:15.717548: step 5714, loss 0.086469, acc 0.953125, prec 0.0887592, recall 0.817827
2017-12-10T04:44:15.988167: step 5715, loss 0.132867, acc 0.953125, prec 0.0887557, recall 0.817827
2017-12-10T04:44:16.251433: step 5716, loss 0.234073, acc 0.953125, prec 0.0887522, recall 0.817827
2017-12-10T04:44:16.519858: step 5717, loss 0.144243, acc 0.96875, prec 0.0887498, recall 0.817827
2017-12-10T04:44:16.787413: step 5718, loss 0.0367844, acc 0.984375, prec 0.0887727, recall 0.817872
2017-12-10T04:44:17.056546: step 5719, loss 0.0688303, acc 0.984375, prec 0.0887836, recall 0.817894
2017-12-10T04:44:17.321446: step 5720, loss 0.0270553, acc 0.984375, prec 0.0888186, recall 0.81796
2017-12-10T04:44:17.593343: step 5721, loss 0.0916898, acc 0.953125, prec 0.0888151, recall 0.81796
2017-12-10T04:44:17.867232: step 5722, loss 0.00191609, acc 1, prec 0.0888271, recall 0.817982
2017-12-10T04:44:18.137971: step 5723, loss 0.125224, acc 0.984375, prec 0.088826, recall 0.817982
2017-12-10T04:44:18.404791: step 5724, loss 0.382424, acc 0.921875, prec 0.0888322, recall 0.818005
2017-12-10T04:44:18.668861: step 5725, loss 0.0159942, acc 0.984375, prec 0.0888671, recall 0.818071
2017-12-10T04:44:18.939671: step 5726, loss 0.586804, acc 0.96875, prec 0.0889009, recall 0.818138
2017-12-10T04:44:19.209437: step 5727, loss 0.293018, acc 0.953125, prec 0.0889095, recall 0.81816
2017-12-10T04:44:19.476828: step 5728, loss 0.126726, acc 0.953125, prec 0.0889059, recall 0.81816
2017-12-10T04:44:19.737819: step 5729, loss 0.134376, acc 0.984375, prec 0.0889289, recall 0.818204
2017-12-10T04:44:20.004286: step 5730, loss 0.00384655, acc 1, prec 0.0889289, recall 0.818204
2017-12-10T04:44:20.267063: step 5731, loss 0.222927, acc 0.984375, prec 0.0889277, recall 0.818204
2017-12-10T04:44:20.540257: step 5732, loss 0.0879305, acc 0.953125, prec 0.0889362, recall 0.818226
2017-12-10T04:44:20.807211: step 5733, loss 0.0294859, acc 0.984375, prec 0.0889471, recall 0.818248
2017-12-10T04:44:21.086905: step 5734, loss 0.00323175, acc 1, prec 0.0889471, recall 0.818248
2017-12-10T04:44:21.354806: step 5735, loss 0.0554926, acc 0.984375, prec 0.0889459, recall 0.818248
2017-12-10T04:44:21.620036: step 5736, loss 0.092702, acc 0.984375, prec 0.0889688, recall 0.818292
2017-12-10T04:44:21.884344: step 5737, loss 0.0202794, acc 0.984375, prec 0.0889676, recall 0.818292
2017-12-10T04:44:22.149345: step 5738, loss 0.545445, acc 0.9375, prec 0.088975, recall 0.818314
2017-12-10T04:44:22.413835: step 5739, loss 0.00774363, acc 1, prec 0.088975, recall 0.818314
2017-12-10T04:44:22.682572: step 5740, loss 0.1168, acc 0.96875, prec 0.0889847, recall 0.818337
2017-12-10T04:44:22.955303: step 5741, loss 0.249649, acc 0.96875, prec 0.0889823, recall 0.818337
2017-12-10T04:44:23.224285: step 5742, loss 0.111549, acc 0.984375, prec 0.0890173, recall 0.818403
2017-12-10T04:44:23.488521: step 5743, loss 0.0487814, acc 0.984375, prec 0.0890281, recall 0.818425
2017-12-10T04:44:23.759641: step 5744, loss 0.0951374, acc 0.984375, prec 0.089027, recall 0.818425
2017-12-10T04:44:24.026668: step 5745, loss 0.228699, acc 0.96875, prec 0.0890487, recall 0.818469
2017-12-10T04:44:24.290113: step 5746, loss 0.0648427, acc 0.984375, prec 0.0890475, recall 0.818469
2017-12-10T04:44:24.560259: step 5747, loss 0.0932925, acc 0.96875, prec 0.0890452, recall 0.818469
2017-12-10T04:44:24.826279: step 5748, loss 2.80702, acc 0.984375, prec 0.0890693, recall 0.818414
2017-12-10T04:44:25.097252: step 5749, loss 0.379661, acc 1, prec 0.0890933, recall 0.818458
2017-12-10T04:44:25.364658: step 5750, loss 0.266395, acc 0.96875, prec 0.089091, recall 0.818458
2017-12-10T04:44:25.628207: step 5751, loss 0.153724, acc 0.953125, prec 0.0891115, recall 0.818502
2017-12-10T04:44:25.894681: step 5752, loss 0.494097, acc 0.921875, prec 0.0891056, recall 0.818502
2017-12-10T04:44:26.165657: step 5753, loss 0.403972, acc 0.890625, prec 0.0891335, recall 0.818568
2017-12-10T04:44:26.445370: step 5754, loss 0.306538, acc 0.90625, prec 0.0891385, recall 0.81859
2017-12-10T04:44:26.712314: step 5755, loss 0.191977, acc 0.953125, prec 0.089159, recall 0.818634
2017-12-10T04:44:26.981655: step 5756, loss 0.688715, acc 0.9375, prec 0.0891904, recall 0.8187
2017-12-10T04:44:27.253117: step 5757, loss 0.356477, acc 0.921875, prec 0.0892086, recall 0.818744
2017-12-10T04:44:27.519972: step 5758, loss 0.128846, acc 0.921875, prec 0.0892027, recall 0.818744
2017-12-10T04:44:27.787566: step 5759, loss 0.451355, acc 0.921875, prec 0.0892088, recall 0.818766
2017-12-10T04:44:28.056355: step 5760, loss 0.504833, acc 0.90625, prec 0.0892138, recall 0.818788
2017-12-10T04:44:28.326255: step 5761, loss 0.473634, acc 0.9375, prec 0.0892331, recall 0.818832
2017-12-10T04:44:28.597550: step 5762, loss 0.823907, acc 0.859375, prec 0.0892225, recall 0.818832
2017-12-10T04:44:28.857362: step 5763, loss 0.589679, acc 0.90625, prec 0.0892275, recall 0.818854
2017-12-10T04:44:29.119083: step 5764, loss 0.446115, acc 0.90625, prec 0.0892445, recall 0.818898
2017-12-10T04:44:29.383422: step 5765, loss 0.56436, acc 0.859375, prec 0.0892579, recall 0.818942
2017-12-10T04:44:29.648286: step 5766, loss 0.431058, acc 0.90625, prec 0.0892508, recall 0.818942
2017-12-10T04:44:29.914292: step 5767, loss 0.201496, acc 0.9375, prec 0.0892461, recall 0.818942
2017-12-10T04:44:30.184890: step 5768, loss 0.474579, acc 0.890625, prec 0.0892499, recall 0.818963
2017-12-10T04:44:30.454356: step 5769, loss 0.0673364, acc 0.96875, prec 0.0892956, recall 0.819051
2017-12-10T04:44:30.725559: step 5770, loss 0.206011, acc 0.953125, prec 0.0893041, recall 0.819073
2017-12-10T04:44:30.991451: step 5771, loss 0.28931, acc 0.96875, prec 0.0893017, recall 0.819073
2017-12-10T04:44:31.263713: step 5772, loss 0.0278597, acc 0.984375, prec 0.0893126, recall 0.819095
2017-12-10T04:44:31.531918: step 5773, loss 0.361087, acc 0.953125, prec 0.0893451, recall 0.819161
2017-12-10T04:44:31.795410: step 5774, loss 0.176276, acc 0.953125, prec 0.0893416, recall 0.819161
2017-12-10T04:44:32.059796: step 5775, loss 0.0679954, acc 0.953125, prec 0.089338, recall 0.819161
2017-12-10T04:44:32.326959: step 5776, loss 0.49718, acc 0.9375, prec 0.0893333, recall 0.819161
2017-12-10T04:44:32.601772: step 5777, loss 0.174028, acc 1, prec 0.0893453, recall 0.819182
2017-12-10T04:44:32.876367: step 5778, loss 0.0591677, acc 0.984375, prec 0.0893561, recall 0.819204
2017-12-10T04:44:33.146768: step 5779, loss 4.48531, acc 0.984375, prec 0.0893682, recall 0.819127
2017-12-10T04:44:33.422131: step 5780, loss 0.0137628, acc 1, prec 0.0893802, recall 0.819149
2017-12-10T04:44:33.683526: step 5781, loss 0.0434604, acc 0.984375, prec 0.089391, recall 0.819171
2017-12-10T04:44:33.946423: step 5782, loss 0.000683729, acc 1, prec 0.089391, recall 0.819171
2017-12-10T04:44:34.215904: step 5783, loss 0.335059, acc 0.953125, prec 0.0893875, recall 0.819171
2017-12-10T04:44:34.483446: step 5784, loss 0.365512, acc 0.890625, prec 0.0894032, recall 0.819215
2017-12-10T04:44:34.747668: step 5785, loss 0.182053, acc 0.953125, prec 0.0893997, recall 0.819215
2017-12-10T04:44:35.018156: step 5786, loss 0.186045, acc 0.953125, prec 0.0894202, recall 0.819258
2017-12-10T04:44:35.296765: step 5787, loss 0.320003, acc 0.90625, prec 0.0894131, recall 0.819258
2017-12-10T04:44:35.562199: step 5788, loss 0.441107, acc 0.890625, prec 0.0894169, recall 0.81928
2017-12-10T04:44:35.824745: step 5789, loss 0.23059, acc 0.921875, prec 0.089423, recall 0.819302
2017-12-10T04:44:36.088796: step 5790, loss 0.600853, acc 0.921875, prec 0.0894171, recall 0.819302
2017-12-10T04:44:36.359692: step 5791, loss 0.196522, acc 0.9375, prec 0.0894364, recall 0.819345
2017-12-10T04:44:36.625774: step 5792, loss 0.11623, acc 0.984375, prec 0.0894472, recall 0.819367
2017-12-10T04:44:36.900382: step 5793, loss 0.21397, acc 0.953125, prec 0.0894677, recall 0.819411
2017-12-10T04:44:37.171098: step 5794, loss 0.160047, acc 0.953125, prec 0.0894641, recall 0.819411
2017-12-10T04:44:37.435767: step 5795, loss 5.06582, acc 0.984375, prec 0.0895001, recall 0.819377
2017-12-10T04:44:37.703759: step 5796, loss 0.163502, acc 0.9375, prec 0.0894954, recall 0.819377
2017-12-10T04:44:37.978181: step 5797, loss 0.26727, acc 0.953125, prec 0.0895039, recall 0.819399
2017-12-10T04:44:38.243697: step 5798, loss 0.238724, acc 0.9375, prec 0.0894991, recall 0.819399
2017-12-10T04:44:38.516586: step 5799, loss 0.203244, acc 0.9375, prec 0.0895184, recall 0.819443
2017-12-10T04:44:38.778772: step 5800, loss 0.712018, acc 0.828125, prec 0.0895294, recall 0.819486
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-5800

2017-12-10T04:44:40.274045: step 5801, loss 0.547975, acc 0.890625, prec 0.0895811, recall 0.819595
2017-12-10T04:44:40.540463: step 5802, loss 0.279778, acc 0.875, prec 0.0896077, recall 0.81966
2017-12-10T04:44:40.811966: step 5803, loss 0.653754, acc 0.84375, prec 0.0895959, recall 0.81966
2017-12-10T04:44:41.074006: step 5804, loss 0.545465, acc 0.859375, prec 0.0895972, recall 0.819682
2017-12-10T04:44:41.341123: step 5805, loss 0.644567, acc 0.828125, prec 0.0896202, recall 0.819747
2017-12-10T04:44:41.612876: step 5806, loss 0.412411, acc 0.859375, prec 0.0896096, recall 0.819747
2017-12-10T04:44:41.876554: step 5807, loss 0.37961, acc 0.90625, prec 0.0896145, recall 0.819769
2017-12-10T04:44:42.152409: step 5808, loss 0.362613, acc 0.921875, prec 0.0896086, recall 0.819769
2017-12-10T04:44:42.418562: step 5809, loss 0.593825, acc 0.890625, prec 0.0896243, recall 0.819812
2017-12-10T04:44:42.684957: step 5810, loss 0.327783, acc 0.890625, prec 0.0896161, recall 0.819812
2017-12-10T04:44:42.949405: step 5811, loss 0.21428, acc 0.953125, prec 0.0896245, recall 0.819834
2017-12-10T04:44:43.217824: step 5812, loss 0.0968122, acc 0.9375, prec 0.0896318, recall 0.819856
2017-12-10T04:44:43.479548: step 5813, loss 0.725751, acc 0.90625, prec 0.0896486, recall 0.819899
2017-12-10T04:44:43.743281: step 5814, loss 0.215111, acc 0.953125, prec 0.0896571, recall 0.819921
2017-12-10T04:44:44.017117: step 5815, loss 0.443684, acc 0.90625, prec 0.089674, recall 0.819964
2017-12-10T04:44:44.282019: step 5816, loss 0.390285, acc 0.953125, prec 0.0896704, recall 0.819964
2017-12-10T04:44:44.551085: step 5817, loss 0.18204, acc 0.984375, prec 0.0896812, recall 0.819986
2017-12-10T04:44:44.821922: step 5818, loss 0.149146, acc 0.984375, prec 0.089692, recall 0.820007
2017-12-10T04:44:45.089872: step 5819, loss 0.0502631, acc 0.984375, prec 0.0897028, recall 0.820029
2017-12-10T04:44:45.360199: step 5820, loss 0.194469, acc 0.953125, prec 0.0896992, recall 0.820029
2017-12-10T04:44:45.632757: step 5821, loss 0.151326, acc 0.96875, prec 0.0896969, recall 0.820029
2017-12-10T04:44:45.901331: step 5822, loss 0.221455, acc 0.96875, prec 0.0896945, recall 0.820029
2017-12-10T04:44:46.168104: step 5823, loss 0.275582, acc 0.953125, prec 0.089691, recall 0.820029
2017-12-10T04:44:46.435434: step 5824, loss 0.10229, acc 0.96875, prec 0.0896886, recall 0.820029
2017-12-10T04:44:46.701266: step 5825, loss 0.0565037, acc 0.96875, prec 0.0896863, recall 0.820029
2017-12-10T04:44:46.965738: step 5826, loss 0.018697, acc 0.984375, prec 0.0896971, recall 0.82005
2017-12-10T04:44:47.235055: step 5827, loss 0.157015, acc 0.984375, prec 0.0897079, recall 0.820072
2017-12-10T04:44:47.499139: step 5828, loss 0.0930332, acc 0.984375, prec 0.0897306, recall 0.820115
2017-12-10T04:44:47.767216: step 5829, loss 0.17392, acc 0.984375, prec 0.0897294, recall 0.820115
2017-12-10T04:44:48.042485: step 5830, loss 0.0699895, acc 0.984375, prec 0.0897283, recall 0.820115
2017-12-10T04:44:48.315900: step 5831, loss 1.5639, acc 0.984375, prec 0.0897402, recall 0.820038
2017-12-10T04:44:48.586630: step 5832, loss 0.128315, acc 0.984375, prec 0.089739, recall 0.820038
2017-12-10T04:44:48.848216: step 5833, loss 0.0158042, acc 0.984375, prec 0.0897379, recall 0.820038
2017-12-10T04:44:49.114493: step 5834, loss 0.0985397, acc 0.96875, prec 0.0897355, recall 0.820038
2017-12-10T04:44:49.375565: step 5835, loss 0.194248, acc 0.96875, prec 0.0897331, recall 0.820038
2017-12-10T04:44:49.639729: step 5836, loss 1.26081, acc 0.984375, prec 0.0897331, recall 0.81994
2017-12-10T04:44:49.912846: step 5837, loss 0.0712163, acc 0.984375, prec 0.0897559, recall 0.819983
2017-12-10T04:44:50.178871: step 5838, loss 0.206517, acc 0.984375, prec 0.0897547, recall 0.819983
2017-12-10T04:44:50.448824: step 5839, loss 0.0924543, acc 0.984375, prec 0.0897535, recall 0.819983
2017-12-10T04:44:50.723609: step 5840, loss 0.160112, acc 0.953125, prec 0.08975, recall 0.819983
2017-12-10T04:44:50.989536: step 5841, loss 0.26493, acc 0.953125, prec 0.0897704, recall 0.820026
2017-12-10T04:44:51.253703: step 5842, loss 1.91997, acc 0.890625, prec 0.0897753, recall 0.81995
2017-12-10T04:44:51.526404: step 5843, loss 0.238248, acc 0.921875, prec 0.0897933, recall 0.819993
2017-12-10T04:44:51.794605: step 5844, loss 0.757512, acc 0.84375, prec 0.0897935, recall 0.820014
2017-12-10T04:44:52.061707: step 5845, loss 0.350003, acc 0.921875, prec 0.0898115, recall 0.820058
2017-12-10T04:44:52.338813: step 5846, loss 0.230682, acc 0.9375, prec 0.0898307, recall 0.820101
2017-12-10T04:44:52.596790: step 5847, loss 1.00599, acc 0.828125, prec 0.0898416, recall 0.820144
2017-12-10T04:44:52.860228: step 5848, loss 0.609082, acc 0.828125, prec 0.0898525, recall 0.820187
2017-12-10T04:44:53.124478: step 5849, loss 0.196884, acc 0.9375, prec 0.0898478, recall 0.820187
2017-12-10T04:44:53.390664: step 5850, loss 0.552598, acc 0.859375, prec 0.0898372, recall 0.820187
2017-12-10T04:44:53.657939: step 5851, loss 0.424104, acc 0.890625, prec 0.0898289, recall 0.820187
2017-12-10T04:44:53.925332: step 5852, loss 0.739392, acc 0.84375, prec 0.0898291, recall 0.820209
2017-12-10T04:44:54.195038: step 5853, loss 0.144227, acc 0.921875, prec 0.089859, recall 0.820273
2017-12-10T04:44:54.463488: step 5854, loss 0.350377, acc 0.859375, prec 0.0898604, recall 0.820295
2017-12-10T04:44:54.723990: step 5855, loss 0.322362, acc 0.921875, prec 0.0898784, recall 0.820338
2017-12-10T04:44:54.990705: step 5856, loss 0.145593, acc 0.953125, prec 0.0898868, recall 0.820359
2017-12-10T04:44:55.258725: step 5857, loss 0.556352, acc 0.90625, prec 0.0898797, recall 0.820359
2017-12-10T04:44:55.524844: step 5858, loss 0.295101, acc 0.921875, prec 0.0898977, recall 0.820402
2017-12-10T04:44:55.788693: step 5859, loss 0.315834, acc 0.953125, prec 0.0898941, recall 0.820402
2017-12-10T04:44:56.053656: step 5860, loss 0.499382, acc 0.9375, prec 0.0899014, recall 0.820424
2017-12-10T04:44:56.322389: step 5861, loss 0.00516584, acc 1, prec 0.0899252, recall 0.820467
2017-12-10T04:44:56.597009: step 5862, loss 0.00814476, acc 1, prec 0.0899252, recall 0.820467
2017-12-10T04:44:56.872794: step 5863, loss 0.566078, acc 0.953125, prec 0.0899217, recall 0.820467
2017-12-10T04:44:57.151125: step 5864, loss 6.33611, acc 0.984375, prec 0.0899217, recall 0.820369
2017-12-10T04:44:57.419249: step 5865, loss 0.308842, acc 0.953125, prec 0.089942, recall 0.820412
2017-12-10T04:44:57.688656: step 5866, loss 0.0306354, acc 0.984375, prec 0.0899408, recall 0.820412
2017-12-10T04:44:57.950394: step 5867, loss 0.197652, acc 0.953125, prec 0.0899612, recall 0.820455
2017-12-10T04:44:58.215258: step 5868, loss 0.569371, acc 0.921875, prec 0.0899553, recall 0.820455
2017-12-10T04:44:58.484403: step 5869, loss 0.236159, acc 0.9375, prec 0.0899506, recall 0.820455
2017-12-10T04:44:58.757946: step 5870, loss 0.38385, acc 0.875, prec 0.0899411, recall 0.820455
2017-12-10T04:44:59.027678: step 5871, loss 0.325873, acc 0.921875, prec 0.0899472, recall 0.820476
2017-12-10T04:44:59.295144: step 5872, loss 0.579081, acc 0.828125, prec 0.0899461, recall 0.820498
2017-12-10T04:44:59.564565: step 5873, loss 0.458606, acc 0.921875, prec 0.089976, recall 0.820562
2017-12-10T04:44:59.832948: step 5874, loss 0.344934, acc 0.875, prec 0.0899904, recall 0.820605
2017-12-10T04:45:00.096865: step 5875, loss 0.361132, acc 0.890625, prec 0.0899941, recall 0.820626
2017-12-10T04:45:00.370340: step 5876, loss 0.189749, acc 0.9375, prec 0.0900013, recall 0.820648
2017-12-10T04:45:00.645776: step 5877, loss 0.335965, acc 0.921875, prec 0.0899954, recall 0.820648
2017-12-10T04:45:00.910515: step 5878, loss 0.217379, acc 0.921875, prec 0.0899895, recall 0.820648
2017-12-10T04:45:01.178853: step 5879, loss 0.137254, acc 0.9375, prec 0.0899848, recall 0.820648
2017-12-10T04:45:01.447728: step 5880, loss 0.033493, acc 0.984375, prec 0.0899836, recall 0.820648
2017-12-10T04:45:01.715068: step 5881, loss 0.237021, acc 0.9375, prec 0.0900028, recall 0.82069
2017-12-10T04:45:01.989104: step 5882, loss 0.465124, acc 0.9375, prec 0.09001, recall 0.820712
2017-12-10T04:45:02.259261: step 5883, loss 0.0097082, acc 1, prec 0.0900219, recall 0.820733
2017-12-10T04:45:02.530499: step 5884, loss 0.414404, acc 0.9375, prec 0.090041, recall 0.820776
2017-12-10T04:45:02.797844: step 5885, loss 0.154075, acc 0.921875, prec 0.0900351, recall 0.820776
2017-12-10T04:45:03.063305: step 5886, loss 0.00437445, acc 1, prec 0.0900351, recall 0.820776
2017-12-10T04:45:03.325361: step 5887, loss 0.0149515, acc 1, prec 0.0900351, recall 0.820776
2017-12-10T04:45:03.591401: step 5888, loss 0.524271, acc 0.953125, prec 0.0900792, recall 0.820862
2017-12-10T04:45:03.858254: step 5889, loss 0.0847047, acc 0.953125, prec 0.0900757, recall 0.820862
2017-12-10T04:45:04.127105: step 5890, loss 0.898982, acc 0.96875, prec 0.0900972, recall 0.820904
2017-12-10T04:45:04.396334: step 5891, loss 0.0550213, acc 0.984375, prec 0.0901317, recall 0.820969
2017-12-10T04:45:04.667511: step 5892, loss 0.119712, acc 0.96875, prec 0.0901413, recall 0.82099
2017-12-10T04:45:04.941910: step 5893, loss 0.119381, acc 0.96875, prec 0.0901508, recall 0.821011
2017-12-10T04:45:05.206266: step 5894, loss 0.412967, acc 0.9375, prec 0.0901699, recall 0.821054
2017-12-10T04:45:05.480392: step 5895, loss 0.0571122, acc 0.96875, prec 0.0901676, recall 0.821054
2017-12-10T04:45:05.750315: step 5896, loss 0.624949, acc 0.921875, prec 0.0901617, recall 0.821054
2017-12-10T04:45:06.015086: step 5897, loss 0.0334898, acc 0.984375, prec 0.0901724, recall 0.821075
2017-12-10T04:45:06.284432: step 5898, loss 0.130456, acc 0.96875, prec 0.0902177, recall 0.82116
2017-12-10T04:45:06.558198: step 5899, loss 0.0820272, acc 0.984375, prec 0.0902284, recall 0.821182
2017-12-10T04:45:06.825072: step 5900, loss 0.193848, acc 0.953125, prec 0.0902487, recall 0.821224
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-5900

2017-12-10T04:45:08.385371: step 5901, loss 0.207945, acc 0.921875, prec 0.0902428, recall 0.821224
2017-12-10T04:45:08.648394: step 5902, loss 0.0547235, acc 0.96875, prec 0.0902642, recall 0.821267
2017-12-10T04:45:08.911732: step 5903, loss 0.645332, acc 0.921875, prec 0.0902583, recall 0.821267
2017-12-10T04:45:09.184230: step 5904, loss 0.0863074, acc 0.96875, prec 0.0902679, recall 0.821288
2017-12-10T04:45:09.452939: step 5905, loss 0.814892, acc 0.859375, prec 0.0902572, recall 0.821288
2017-12-10T04:45:09.723852: step 5906, loss 0.711932, acc 0.96875, prec 0.0902668, recall 0.82131
2017-12-10T04:45:09.998018: step 5907, loss 0.0298923, acc 0.984375, prec 0.0902894, recall 0.821352
2017-12-10T04:45:10.262437: step 5908, loss 0.216243, acc 0.96875, prec 0.0902989, recall 0.821373
2017-12-10T04:45:10.528940: step 5909, loss 0.00592251, acc 1, prec 0.0903108, recall 0.821395
2017-12-10T04:45:10.795757: step 5910, loss 0.0627211, acc 0.953125, prec 0.0903073, recall 0.821395
2017-12-10T04:45:11.063026: step 5911, loss 0.33887, acc 0.921875, prec 0.0903133, recall 0.821416
2017-12-10T04:45:11.326229: step 5912, loss 0.0612515, acc 0.96875, prec 0.0903228, recall 0.821437
2017-12-10T04:45:11.590781: step 5913, loss 0.061988, acc 0.984375, prec 0.0903336, recall 0.821458
2017-12-10T04:45:11.859373: step 5914, loss 0.0921044, acc 0.96875, prec 0.0903431, recall 0.82148
2017-12-10T04:45:12.136690: step 5915, loss 0.70084, acc 0.984375, prec 0.0903538, recall 0.821501
2017-12-10T04:45:12.406165: step 5916, loss 0.272332, acc 0.953125, prec 0.0903503, recall 0.821501
2017-12-10T04:45:12.674283: step 5917, loss 0.0581465, acc 0.984375, prec 0.0903729, recall 0.821543
2017-12-10T04:45:12.948350: step 5918, loss 0.261816, acc 0.921875, prec 0.0904026, recall 0.821607
2017-12-10T04:45:13.210591: step 5919, loss 0.201931, acc 0.9375, prec 0.0904217, recall 0.821649
2017-12-10T04:45:13.483635: step 5920, loss 0.0415173, acc 0.984375, prec 0.0904324, recall 0.82167
2017-12-10T04:45:13.757394: step 5921, loss 0.240886, acc 0.9375, prec 0.0904277, recall 0.82167
2017-12-10T04:45:14.025678: step 5922, loss 0.146796, acc 0.96875, prec 0.0904372, recall 0.821692
2017-12-10T04:45:14.290405: step 5923, loss 1.07885, acc 0.875, prec 0.0904396, recall 0.821713
2017-12-10T04:45:14.554392: step 5924, loss 0.255821, acc 0.921875, prec 0.0904456, recall 0.821734
2017-12-10T04:45:14.820972: step 5925, loss 0.245791, acc 0.921875, prec 0.0904516, recall 0.821755
2017-12-10T04:45:15.087877: step 5926, loss 0.0062987, acc 1, prec 0.0904635, recall 0.821776
2017-12-10T04:45:15.355361: step 5927, loss 0.123934, acc 0.984375, prec 0.0904742, recall 0.821797
2017-12-10T04:45:15.622545: step 5928, loss 0.0187648, acc 0.984375, prec 0.090473, recall 0.821797
2017-12-10T04:45:15.893407: step 5929, loss 0.260368, acc 0.953125, prec 0.0904814, recall 0.821819
2017-12-10T04:45:16.164266: step 5930, loss 0.241558, acc 0.9375, prec 0.0904766, recall 0.821819
2017-12-10T04:45:16.432559: step 5931, loss 0.0964032, acc 0.984375, prec 0.0904754, recall 0.821819
2017-12-10T04:45:16.703327: step 5932, loss 0.0359234, acc 0.984375, prec 0.0904861, recall 0.82184
2017-12-10T04:45:16.971012: step 5933, loss 0.0761129, acc 0.96875, prec 0.0905076, recall 0.821882
2017-12-10T04:45:17.243216: step 5934, loss 3.22517, acc 0.953125, prec 0.0905171, recall 0.821806
2017-12-10T04:45:17.517301: step 5935, loss 0.00451641, acc 1, prec 0.090529, recall 0.821827
2017-12-10T04:45:17.780989: step 5936, loss 0.274255, acc 0.90625, prec 0.0905337, recall 0.821848
2017-12-10T04:45:18.049343: step 5937, loss 0.313689, acc 0.921875, prec 0.0905278, recall 0.821848
2017-12-10T04:45:18.315743: step 5938, loss 0.0774144, acc 0.953125, prec 0.0905362, recall 0.821869
2017-12-10T04:45:18.581979: step 5939, loss 0.260995, acc 0.921875, prec 0.0905421, recall 0.82189
2017-12-10T04:45:18.844936: step 5940, loss 0.316083, acc 0.921875, prec 0.0905481, recall 0.821911
2017-12-10T04:45:19.107562: step 5941, loss 0.210749, acc 0.953125, prec 0.0905564, recall 0.821932
2017-12-10T04:45:19.376297: step 5942, loss 0.418038, acc 0.921875, prec 0.0905624, recall 0.821954
2017-12-10T04:45:19.647636: step 5943, loss 0.369421, acc 0.9375, prec 0.0905577, recall 0.821954
2017-12-10T04:45:19.913632: step 5944, loss 0.246435, acc 0.953125, prec 0.0905541, recall 0.821954
2017-12-10T04:45:20.185040: step 5945, loss 0.111203, acc 0.984375, prec 0.0905767, recall 0.821996
2017-12-10T04:45:20.449534: step 5946, loss 0.163651, acc 0.921875, prec 0.0905708, recall 0.821996
2017-12-10T04:45:20.718141: step 5947, loss 0.579137, acc 0.9375, prec 0.090566, recall 0.821996
2017-12-10T04:45:20.991812: step 5948, loss 0.617826, acc 0.90625, prec 0.0905708, recall 0.822017
2017-12-10T04:45:21.259641: step 5949, loss 0.389515, acc 0.953125, prec 0.0905791, recall 0.822038
2017-12-10T04:45:21.525419: step 5950, loss 0.177397, acc 0.96875, prec 0.0905768, recall 0.822038
2017-12-10T04:45:21.798433: step 5951, loss 0.124591, acc 0.953125, prec 0.0906088, recall 0.822101
2017-12-10T04:45:22.071150: step 5952, loss 0.183073, acc 0.9375, prec 0.090616, recall 0.822122
2017-12-10T04:45:22.335752: step 5953, loss 0.393709, acc 0.921875, prec 0.0906219, recall 0.822143
2017-12-10T04:45:22.605240: step 5954, loss 0.123016, acc 0.953125, prec 0.0906303, recall 0.822164
2017-12-10T04:45:22.870578: step 5955, loss 0.183127, acc 0.953125, prec 0.0906386, recall 0.822185
2017-12-10T04:45:23.135022: step 5956, loss 0.064137, acc 0.984375, prec 0.0906493, recall 0.822206
2017-12-10T04:45:23.399346: step 5957, loss 0.0424944, acc 0.96875, prec 0.0906469, recall 0.822206
2017-12-10T04:45:23.670417: step 5958, loss 0.0325751, acc 0.984375, prec 0.0906576, recall 0.822227
2017-12-10T04:45:23.942547: step 5959, loss 0.145993, acc 0.96875, prec 0.0906671, recall 0.822249
2017-12-10T04:45:24.214663: step 5960, loss 0.00652295, acc 1, prec 0.090679, recall 0.82227
2017-12-10T04:45:24.479316: step 5961, loss 0.106059, acc 0.984375, prec 0.0906778, recall 0.82227
2017-12-10T04:45:24.745013: step 5962, loss 0.296292, acc 0.96875, prec 0.0906754, recall 0.82227
2017-12-10T04:45:25.015627: step 5963, loss 0.105465, acc 1, prec 0.0906873, recall 0.822291
2017-12-10T04:45:25.249249: step 5964, loss 0.115549, acc 0.980769, prec 0.0906861, recall 0.822291
2017-12-10T04:45:25.525399: step 5965, loss 0.000231788, acc 1, prec 0.0906861, recall 0.822291
2017-12-10T04:45:25.787977: step 5966, loss 0.00108387, acc 1, prec 0.0907098, recall 0.822333
2017-12-10T04:45:26.054542: step 5967, loss 0.0688476, acc 0.96875, prec 0.0907074, recall 0.822333
2017-12-10T04:45:26.324198: step 5968, loss 0.45939, acc 0.984375, prec 0.0907063, recall 0.822333
2017-12-10T04:45:26.597840: step 5969, loss 0.300156, acc 0.96875, prec 0.0907158, recall 0.822354
2017-12-10T04:45:26.865532: step 5970, loss 0.00319804, acc 1, prec 0.0907158, recall 0.822354
2017-12-10T04:45:27.147514: step 5971, loss 0.00122931, acc 1, prec 0.0907276, recall 0.822375
2017-12-10T04:45:27.412473: step 5972, loss 0.204164, acc 0.96875, prec 0.0907253, recall 0.822375
2017-12-10T04:45:27.672778: step 5973, loss 0.00889964, acc 1, prec 0.0907253, recall 0.822375
2017-12-10T04:45:27.948845: step 5974, loss 0.224999, acc 0.984375, prec 0.0907359, recall 0.822396
2017-12-10T04:45:28.216515: step 5975, loss 1.54261, acc 0.96875, prec 0.0907466, recall 0.822319
2017-12-10T04:45:28.483216: step 5976, loss 0.00054903, acc 1, prec 0.0907585, recall 0.82234
2017-12-10T04:45:28.749489: step 5977, loss 0.0183567, acc 1, prec 0.0907703, recall 0.822361
2017-12-10T04:45:29.012932: step 5978, loss 0.0554254, acc 0.96875, prec 0.0908273, recall 0.822466
2017-12-10T04:45:29.281802: step 5979, loss 0.0481608, acc 0.984375, prec 0.0908379, recall 0.822487
2017-12-10T04:45:29.546084: step 5980, loss 0.00433158, acc 1, prec 0.0908379, recall 0.822487
2017-12-10T04:45:29.809601: step 5981, loss 0.0317495, acc 0.984375, prec 0.0908486, recall 0.822508
2017-12-10T04:45:30.071848: step 5982, loss 0.0105422, acc 1, prec 0.0908723, recall 0.82255
2017-12-10T04:45:30.342697: step 5983, loss 0.142608, acc 0.96875, prec 0.09087, recall 0.82255
2017-12-10T04:45:30.620064: step 5984, loss 0.0193536, acc 1, prec 0.0908937, recall 0.822592
2017-12-10T04:45:30.891081: step 5985, loss 0.171459, acc 0.953125, prec 0.0908901, recall 0.822592
2017-12-10T04:45:31.165882: step 5986, loss 0.0972898, acc 0.96875, prec 0.0908877, recall 0.822592
2017-12-10T04:45:31.432449: step 5987, loss 0.138882, acc 0.9375, prec 0.090883, recall 0.822592
2017-12-10T04:45:31.699570: step 5988, loss 0.104312, acc 0.96875, prec 0.0908925, recall 0.822613
2017-12-10T04:45:31.976986: step 5989, loss 0.0798208, acc 0.984375, prec 0.090915, recall 0.822655
2017-12-10T04:45:32.242504: step 5990, loss 0.0302544, acc 0.984375, prec 0.0909257, recall 0.822676
2017-12-10T04:45:32.507054: step 5991, loss 0.176907, acc 0.953125, prec 0.0909221, recall 0.822676
2017-12-10T04:45:32.772002: step 5992, loss 0.531535, acc 0.921875, prec 0.0909281, recall 0.822697
2017-12-10T04:45:33.035305: step 5993, loss 0.0517873, acc 1, prec 0.0909636, recall 0.822759
2017-12-10T04:45:33.303610: step 5994, loss 0.0864711, acc 0.953125, prec 0.0909601, recall 0.822759
2017-12-10T04:45:33.572556: step 5995, loss 0.361826, acc 0.96875, prec 0.0909695, recall 0.82278
2017-12-10T04:45:33.836725: step 5996, loss 0.091925, acc 0.984375, prec 0.0909802, recall 0.822801
2017-12-10T04:45:34.113444: step 5997, loss 2.16284, acc 0.96875, prec 0.0909909, recall 0.822725
2017-12-10T04:45:34.387192: step 5998, loss 0.0254794, acc 0.984375, prec 0.0910015, recall 0.822746
2017-12-10T04:45:34.654129: step 5999, loss 0.226069, acc 0.9375, prec 0.0910086, recall 0.822767
2017-12-10T04:45:34.917358: step 6000, loss 0.129643, acc 0.9375, prec 0.0910157, recall 0.822788

Evaluation:
2017-12-10T04:45:42.480318: step 6000, loss 2.99529, acc 0.912908, prec 0.0910554, recall 0.819108

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-6000

2017-12-10T04:45:43.777562: step 6001, loss 0.246181, acc 0.90625, prec 0.09106, recall 0.819129
2017-12-10T04:45:44.043446: step 6002, loss 0.216109, acc 0.90625, prec 0.0910647, recall 0.81915
2017-12-10T04:45:44.316631: step 6003, loss 0.241931, acc 0.921875, prec 0.0910706, recall 0.819171
2017-12-10T04:45:44.581859: step 6004, loss 0.264963, acc 0.9375, prec 0.091101, recall 0.819234
2017-12-10T04:45:44.850552: step 6005, loss 0.910268, acc 0.84375, prec 0.0910892, recall 0.819234
2017-12-10T04:45:45.114327: step 6006, loss 0.339235, acc 0.84375, prec 0.0910892, recall 0.819255
2017-12-10T04:45:45.383664: step 6007, loss 0.332604, acc 0.875, prec 0.0911032, recall 0.819297
2017-12-10T04:45:45.646588: step 6008, loss 0.227977, acc 0.921875, prec 0.0911091, recall 0.819318
2017-12-10T04:45:45.917002: step 6009, loss 0.310471, acc 0.921875, prec 0.0911149, recall 0.819338
2017-12-10T04:45:46.183136: step 6010, loss 0.472657, acc 0.890625, prec 0.0911067, recall 0.819338
2017-12-10T04:45:46.450447: step 6011, loss 0.226331, acc 0.9375, prec 0.0911137, recall 0.819359
2017-12-10T04:45:46.727773: step 6012, loss 0.683406, acc 0.890625, prec 0.0911055, recall 0.819359
2017-12-10T04:45:46.994880: step 6013, loss 0.459569, acc 0.953125, prec 0.0911253, recall 0.819401
2017-12-10T04:45:47.260524: step 6014, loss 0.269897, acc 0.9375, prec 0.0911207, recall 0.819401
2017-12-10T04:45:47.528792: step 6015, loss 0.00222458, acc 1, prec 0.0911323, recall 0.819422
2017-12-10T04:45:47.792583: step 6016, loss 0.0283317, acc 1, prec 0.0911557, recall 0.819464
2017-12-10T04:45:48.060213: step 6017, loss 0.0521258, acc 0.96875, prec 0.0911534, recall 0.819464
2017-12-10T04:45:48.324701: step 6018, loss 0.20131, acc 0.96875, prec 0.0911744, recall 0.819505
2017-12-10T04:45:48.604824: step 6019, loss 0.244428, acc 0.953125, prec 0.0911826, recall 0.819526
2017-12-10T04:45:48.873096: step 6020, loss 0.211684, acc 0.953125, prec 0.091179, recall 0.819526
2017-12-10T04:45:49.140580: step 6021, loss 0.00950516, acc 1, prec 0.0911907, recall 0.819547
2017-12-10T04:45:49.402598: step 6022, loss 0.072322, acc 0.984375, prec 0.0912012, recall 0.819568
2017-12-10T04:45:49.672097: step 6023, loss 0.278812, acc 0.921875, prec 0.0912071, recall 0.819589
2017-12-10T04:45:49.937976: step 6024, loss 0.0882447, acc 1, prec 0.0912187, recall 0.81961
2017-12-10T04:45:50.215137: step 6025, loss 0.0818895, acc 0.984375, prec 0.0912176, recall 0.81961
2017-12-10T04:45:50.483495: step 6026, loss 0.0496602, acc 0.984375, prec 0.0912164, recall 0.81961
2017-12-10T04:45:50.753767: step 6027, loss 0.401958, acc 0.96875, prec 0.091214, recall 0.81961
2017-12-10T04:45:51.021678: step 6028, loss 0.000534225, acc 1, prec 0.091214, recall 0.81961
2017-12-10T04:45:51.285670: step 6029, loss 0.217295, acc 1, prec 0.0912257, recall 0.819631
2017-12-10T04:45:51.557960: step 6030, loss 0.00974128, acc 1, prec 0.0912841, recall 0.819735
2017-12-10T04:45:51.823421: step 6031, loss 0.841421, acc 0.984375, prec 0.0913297, recall 0.819818
2017-12-10T04:45:52.099761: step 6032, loss 0.179097, acc 0.953125, prec 0.0913378, recall 0.819839
2017-12-10T04:45:52.367756: step 6033, loss 0.0656355, acc 0.953125, prec 0.0913343, recall 0.819839
2017-12-10T04:45:52.642477: step 6034, loss 0.239847, acc 0.96875, prec 0.0913436, recall 0.819859
2017-12-10T04:45:52.913715: step 6035, loss 3.55068, acc 0.96875, prec 0.0913424, recall 0.819765
2017-12-10T04:45:53.190999: step 6036, loss 0.33226, acc 0.9375, prec 0.0913378, recall 0.819765
2017-12-10T04:45:53.458944: step 6037, loss 0.192751, acc 0.9375, prec 0.0913331, recall 0.819765
2017-12-10T04:45:53.727337: step 6038, loss 0.577898, acc 0.9375, prec 0.09134, recall 0.819786
2017-12-10T04:45:53.994511: step 6039, loss 0.413872, acc 0.921875, prec 0.0913342, recall 0.819786
2017-12-10T04:45:54.261339: step 6040, loss 0.574482, acc 0.921875, prec 0.0913517, recall 0.819827
2017-12-10T04:45:54.534849: step 6041, loss 0.602657, acc 0.84375, prec 0.0913516, recall 0.819848
2017-12-10T04:45:54.803048: step 6042, loss 0.552657, acc 0.875, prec 0.0913422, recall 0.819848
2017-12-10T04:45:55.065736: step 6043, loss 0.58973, acc 0.890625, prec 0.0913573, recall 0.819889
2017-12-10T04:45:55.337170: step 6044, loss 0.780833, acc 0.859375, prec 0.0913584, recall 0.81991
2017-12-10T04:45:55.603863: step 6045, loss 0.85222, acc 0.75, prec 0.0913397, recall 0.81991
2017-12-10T04:45:55.873589: step 6046, loss 1.00227, acc 0.8125, prec 0.0913256, recall 0.81991
2017-12-10T04:45:56.141650: step 6047, loss 0.552402, acc 0.875, prec 0.0913162, recall 0.81991
2017-12-10T04:45:56.406249: step 6048, loss 0.892881, acc 0.875, prec 0.0913185, recall 0.819931
2017-12-10T04:45:56.674774: step 6049, loss 0.543359, acc 0.859375, prec 0.0913196, recall 0.819952
2017-12-10T04:45:56.950693: step 6050, loss 1.03007, acc 0.796875, prec 0.0913277, recall 0.819993
2017-12-10T04:45:57.220250: step 6051, loss 0.120137, acc 0.9375, prec 0.0913347, recall 0.820014
2017-12-10T04:45:57.494853: step 6052, loss 0.406431, acc 0.890625, prec 0.0913381, recall 0.820035
2017-12-10T04:45:57.766975: step 6053, loss 0.317419, acc 0.9375, prec 0.0913335, recall 0.820035
2017-12-10T04:45:58.033236: step 6054, loss 0.0612545, acc 1, prec 0.0913451, recall 0.820055
2017-12-10T04:45:58.307858: step 6055, loss 0.0350814, acc 0.984375, prec 0.0913439, recall 0.820055
2017-12-10T04:45:58.574390: step 6056, loss 0.126854, acc 0.953125, prec 0.0913521, recall 0.820076
2017-12-10T04:45:58.846113: step 6057, loss 0.520737, acc 0.953125, prec 0.0913602, recall 0.820097
2017-12-10T04:45:59.110119: step 6058, loss 0.161024, acc 0.96875, prec 0.0913812, recall 0.820138
2017-12-10T04:45:59.378036: step 6059, loss 0.0590692, acc 0.96875, prec 0.0914021, recall 0.820179
2017-12-10T04:45:59.653274: step 6060, loss 0.337018, acc 0.953125, prec 0.0913986, recall 0.820179
2017-12-10T04:45:59.922944: step 6061, loss 0.0621664, acc 0.984375, prec 0.0913974, recall 0.820179
2017-12-10T04:46:00.202324: step 6062, loss 0.414898, acc 0.984375, prec 0.0914079, recall 0.8202
2017-12-10T04:46:00.479191: step 6063, loss 0.104685, acc 0.96875, prec 0.0914172, recall 0.820221
2017-12-10T04:46:00.748989: step 6064, loss 0.00200583, acc 1, prec 0.0914405, recall 0.820262
2017-12-10T04:46:01.016029: step 6065, loss 0.118768, acc 0.96875, prec 0.0914382, recall 0.820262
2017-12-10T04:46:01.281128: step 6066, loss 0.225783, acc 0.9375, prec 0.0914335, recall 0.820262
2017-12-10T04:46:01.545743: step 6067, loss 0.00655338, acc 1, prec 0.0914451, recall 0.820283
2017-12-10T04:46:01.822681: step 6068, loss 0.0487556, acc 0.984375, prec 0.091444, recall 0.820283
2017-12-10T04:46:02.103049: step 6069, loss 0.00459665, acc 1, prec 0.091444, recall 0.820283
2017-12-10T04:46:02.365800: step 6070, loss 0.0939084, acc 0.96875, prec 0.0914533, recall 0.820303
2017-12-10T04:46:02.628214: step 6071, loss 0.651514, acc 0.984375, prec 0.0914754, recall 0.820345
2017-12-10T04:46:02.903844: step 6072, loss 0.0826003, acc 0.96875, prec 0.091473, recall 0.820345
2017-12-10T04:46:03.167314: step 6073, loss 0.23464, acc 1, prec 0.0914847, recall 0.820365
2017-12-10T04:46:03.440231: step 6074, loss 0.0608059, acc 0.96875, prec 0.0914823, recall 0.820365
2017-12-10T04:46:03.709015: step 6075, loss 0.105238, acc 0.984375, prec 0.0914928, recall 0.820386
2017-12-10T04:46:03.980263: step 6076, loss 0.26562, acc 0.984375, prec 0.0915033, recall 0.820407
2017-12-10T04:46:04.250946: step 6077, loss 0.134935, acc 0.984375, prec 0.0915137, recall 0.820427
2017-12-10T04:46:04.516318: step 6078, loss 0.0269583, acc 0.984375, prec 0.0915126, recall 0.820427
2017-12-10T04:46:04.786445: step 6079, loss 0.0170245, acc 0.984375, prec 0.091523, recall 0.820448
2017-12-10T04:46:05.053168: step 6080, loss 0.0695508, acc 1, prec 0.0915347, recall 0.820469
2017-12-10T04:46:05.325978: step 6081, loss 0.101203, acc 0.96875, prec 0.091544, recall 0.820489
2017-12-10T04:46:05.603135: step 6082, loss 0.456206, acc 0.9375, prec 0.0915393, recall 0.820489
2017-12-10T04:46:05.868178: step 6083, loss 0.148257, acc 0.984375, prec 0.0915381, recall 0.820489
2017-12-10T04:46:06.135848: step 6084, loss 0.0381471, acc 0.96875, prec 0.0915474, recall 0.82051
2017-12-10T04:46:06.399169: step 6085, loss 0.155991, acc 0.96875, prec 0.0915683, recall 0.820551
2017-12-10T04:46:06.670092: step 6086, loss 0.0804049, acc 0.96875, prec 0.0915776, recall 0.820572
2017-12-10T04:46:06.938027: step 6087, loss 0.14403, acc 0.953125, prec 0.0915857, recall 0.820592
2017-12-10T04:46:07.205576: step 6088, loss 0.25068, acc 0.953125, prec 0.0916055, recall 0.820633
2017-12-10T04:46:07.472455: step 6089, loss 0.344622, acc 0.953125, prec 0.091602, recall 0.820633
2017-12-10T04:46:07.735937: step 6090, loss 0.239285, acc 0.96875, prec 0.0915996, recall 0.820633
2017-12-10T04:46:08.005534: step 6091, loss 0.222701, acc 0.96875, prec 0.0915973, recall 0.820633
2017-12-10T04:46:08.270889: step 6092, loss 2.27683, acc 0.921875, prec 0.0916042, recall 0.82056
2017-12-10T04:46:08.537327: step 6093, loss 0.0131266, acc 0.984375, prec 0.0916031, recall 0.82056
2017-12-10T04:46:08.807693: step 6094, loss 0.204687, acc 0.953125, prec 0.0916112, recall 0.82058
2017-12-10T04:46:09.077078: step 6095, loss 0.135368, acc 0.96875, prec 0.0916205, recall 0.820601
2017-12-10T04:46:09.345823: step 6096, loss 0.129571, acc 0.96875, prec 0.0916181, recall 0.820601
2017-12-10T04:46:09.613252: step 6097, loss 0.855792, acc 0.84375, prec 0.091618, recall 0.820622
2017-12-10T04:46:09.882609: step 6098, loss 0.232993, acc 0.921875, prec 0.0916238, recall 0.820642
2017-12-10T04:46:10.152506: step 6099, loss 0.297538, acc 0.953125, prec 0.0916203, recall 0.820642
2017-12-10T04:46:10.422042: step 6100, loss 0.562888, acc 0.90625, prec 0.0916365, recall 0.820683
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-6100

2017-12-10T04:46:11.655849: step 6101, loss 0.315504, acc 0.953125, prec 0.091633, recall 0.820683
2017-12-10T04:46:11.926188: step 6102, loss 0.406942, acc 0.90625, prec 0.0916375, recall 0.820704
2017-12-10T04:46:12.197172: step 6103, loss 0.259278, acc 0.921875, prec 0.0916317, recall 0.820704
2017-12-10T04:46:12.462169: step 6104, loss 0.259925, acc 0.953125, prec 0.0916282, recall 0.820704
2017-12-10T04:46:12.732907: step 6105, loss 0.274288, acc 0.953125, prec 0.0916479, recall 0.820745
2017-12-10T04:46:13.001022: step 6106, loss 0.115067, acc 0.953125, prec 0.0916444, recall 0.820745
2017-12-10T04:46:13.270857: step 6107, loss 0.0552549, acc 0.984375, prec 0.0916432, recall 0.820745
2017-12-10T04:46:13.534619: step 6108, loss 0.359654, acc 0.9375, prec 0.0916501, recall 0.820766
2017-12-10T04:46:13.811135: step 6109, loss 0.156126, acc 0.953125, prec 0.0916582, recall 0.820786
2017-12-10T04:46:14.088603: step 6110, loss 0.253157, acc 0.9375, prec 0.0916652, recall 0.820807
2017-12-10T04:46:14.359171: step 6111, loss 0.0739897, acc 0.984375, prec 0.0916872, recall 0.820848
2017-12-10T04:46:14.627284: step 6112, loss 0.0713093, acc 0.96875, prec 0.0916849, recall 0.820848
2017-12-10T04:46:14.889907: step 6113, loss 0.313944, acc 0.953125, prec 0.0916814, recall 0.820848
2017-12-10T04:46:15.164866: step 6114, loss 0.255575, acc 0.953125, prec 0.0917243, recall 0.82093
2017-12-10T04:46:15.429266: step 6115, loss 0.062772, acc 0.96875, prec 0.0917336, recall 0.82095
2017-12-10T04:46:15.695338: step 6116, loss 0.392941, acc 0.921875, prec 0.0917394, recall 0.820971
2017-12-10T04:46:15.960237: step 6117, loss 0.0407107, acc 0.984375, prec 0.0917498, recall 0.820991
2017-12-10T04:46:16.226242: step 6118, loss 0.459926, acc 0.9375, prec 0.0917567, recall 0.821012
2017-12-10T04:46:16.503939: step 6119, loss 0.622595, acc 0.953125, prec 0.0917764, recall 0.821053
2017-12-10T04:46:16.780857: step 6120, loss 0.548753, acc 0.984375, prec 0.0917869, recall 0.821073
2017-12-10T04:46:17.051600: step 6121, loss 0.134834, acc 0.953125, prec 0.091795, recall 0.821094
2017-12-10T04:46:17.314827: step 6122, loss 0.017461, acc 1, prec 0.091795, recall 0.821094
2017-12-10T04:46:17.584037: step 6123, loss 0.274675, acc 0.96875, prec 0.0917926, recall 0.821094
2017-12-10T04:46:17.850776: step 6124, loss 0.0153318, acc 1, prec 0.0917926, recall 0.821094
2017-12-10T04:46:18.122847: step 6125, loss 0.346358, acc 0.953125, prec 0.0918007, recall 0.821114
2017-12-10T04:46:18.386349: step 6126, loss 0.326583, acc 0.9375, prec 0.091796, recall 0.821114
2017-12-10T04:46:18.651600: step 6127, loss 0.200482, acc 0.96875, prec 0.0917937, recall 0.821114
2017-12-10T04:46:18.928611: step 6128, loss 0.0541938, acc 0.984375, prec 0.0917925, recall 0.821114
2017-12-10T04:46:19.192328: step 6129, loss 0.26375, acc 0.890625, prec 0.0917959, recall 0.821135
2017-12-10T04:46:19.466819: step 6130, loss 0.237515, acc 0.953125, prec 0.0917924, recall 0.821135
2017-12-10T04:46:19.740742: step 6131, loss 0.0376291, acc 0.984375, prec 0.0917912, recall 0.821135
2017-12-10T04:46:20.005090: step 6132, loss 0.00584884, acc 1, prec 0.0917912, recall 0.821135
2017-12-10T04:46:20.272682: step 6133, loss 0.0129996, acc 1, prec 0.0917912, recall 0.821135
2017-12-10T04:46:20.542254: step 6134, loss 0.102116, acc 0.96875, prec 0.0918005, recall 0.821155
2017-12-10T04:46:20.813228: step 6135, loss 0.061852, acc 0.96875, prec 0.0917981, recall 0.821155
2017-12-10T04:46:21.075881: step 6136, loss 0.467155, acc 0.9375, prec 0.0917934, recall 0.821155
2017-12-10T04:46:21.345457: step 6137, loss 0.21967, acc 0.953125, prec 0.0917899, recall 0.821155
2017-12-10T04:46:21.620365: step 6138, loss 0.0266737, acc 0.984375, prec 0.0918003, recall 0.821175
2017-12-10T04:46:21.887377: step 6139, loss 0.317158, acc 0.9375, prec 0.0918189, recall 0.821216
2017-12-10T04:46:22.160574: step 6140, loss 0.213046, acc 0.984375, prec 0.0918293, recall 0.821237
2017-12-10T04:46:22.424803: step 6141, loss 0.234215, acc 0.96875, prec 0.0918502, recall 0.821278
2017-12-10T04:46:22.689308: step 6142, loss 0.0153568, acc 1, prec 0.0918502, recall 0.821278
2017-12-10T04:46:22.963018: step 6143, loss 0.497789, acc 0.953125, prec 0.0918583, recall 0.821298
2017-12-10T04:46:23.239939: step 6144, loss 0.00319749, acc 1, prec 0.0918699, recall 0.821318
2017-12-10T04:46:23.506481: step 6145, loss 0.159479, acc 0.984375, prec 0.0918803, recall 0.821339
2017-12-10T04:46:23.770059: step 6146, loss 0.12946, acc 0.96875, prec 0.0918895, recall 0.821359
2017-12-10T04:46:24.043796: step 6147, loss 0.56749, acc 0.96875, prec 0.0919104, recall 0.8214
2017-12-10T04:46:24.313370: step 6148, loss 0.164297, acc 0.96875, prec 0.0919081, recall 0.8214
2017-12-10T04:46:24.583947: step 6149, loss 0.215921, acc 0.96875, prec 0.0919057, recall 0.8214
2017-12-10T04:46:24.848928: step 6150, loss 0.217938, acc 0.96875, prec 0.0919034, recall 0.8214
2017-12-10T04:46:25.119655: step 6151, loss 5.56222, acc 0.96875, prec 0.0919138, recall 0.821327
2017-12-10T04:46:25.386247: step 6152, loss 0.00541882, acc 1, prec 0.0919254, recall 0.821347
2017-12-10T04:46:25.655322: step 6153, loss 0.292592, acc 0.96875, prec 0.091923, recall 0.821347
2017-12-10T04:46:25.921179: step 6154, loss 0.324858, acc 0.90625, prec 0.091916, recall 0.821347
2017-12-10T04:46:26.192785: step 6155, loss 0.203164, acc 0.984375, prec 0.0919264, recall 0.821367
2017-12-10T04:46:26.461267: step 6156, loss 0.153303, acc 0.984375, prec 0.0919252, recall 0.821367
2017-12-10T04:46:26.730309: step 6157, loss 0.280403, acc 0.921875, prec 0.0919542, recall 0.821429
2017-12-10T04:46:26.998294: step 6158, loss 0.301133, acc 0.921875, prec 0.0919483, recall 0.821429
2017-12-10T04:46:27.278364: step 6159, loss 0.454352, acc 0.90625, prec 0.0919413, recall 0.821429
2017-12-10T04:46:27.544012: step 6160, loss 0.135614, acc 0.953125, prec 0.0919841, recall 0.82151
2017-12-10T04:46:27.811230: step 6161, loss 0.448293, acc 0.9375, prec 0.091991, recall 0.82153
2017-12-10T04:46:28.081979: step 6162, loss 0.3593, acc 0.90625, prec 0.091984, recall 0.82153
2017-12-10T04:46:28.345545: step 6163, loss 0.192458, acc 0.921875, prec 0.0920013, recall 0.821571
2017-12-10T04:46:28.614785: step 6164, loss 0.198482, acc 0.953125, prec 0.0919978, recall 0.821571
2017-12-10T04:46:28.881472: step 6165, loss 0.485361, acc 0.90625, prec 0.0920023, recall 0.821591
2017-12-10T04:46:29.154557: step 6166, loss 0.660944, acc 0.890625, prec 0.0920057, recall 0.821612
2017-12-10T04:46:29.416635: step 6167, loss 0.0709165, acc 0.96875, prec 0.0920381, recall 0.821673
2017-12-10T04:46:29.681929: step 6168, loss 0.27434, acc 0.9375, prec 0.092045, recall 0.821693
2017-12-10T04:46:29.957361: step 6169, loss 0.0795122, acc 0.9375, prec 0.0920403, recall 0.821693
2017-12-10T04:46:30.233850: step 6170, loss 0.312093, acc 0.921875, prec 0.092046, recall 0.821713
2017-12-10T04:46:30.510474: step 6171, loss 0.178727, acc 0.96875, prec 0.0920552, recall 0.821734
2017-12-10T04:46:30.782515: step 6172, loss 0.230954, acc 0.953125, prec 0.0920633, recall 0.821754
2017-12-10T04:46:31.045398: step 6173, loss 0.240766, acc 0.953125, prec 0.0920829, recall 0.821795
2017-12-10T04:46:31.307553: step 6174, loss 0.0757085, acc 0.96875, prec 0.0920922, recall 0.821815
2017-12-10T04:46:31.569890: step 6175, loss 0.763256, acc 0.921875, prec 0.0920979, recall 0.821835
2017-12-10T04:46:31.839849: step 6176, loss 0.25879, acc 0.96875, prec 0.0921187, recall 0.821876
2017-12-10T04:46:32.104128: step 6177, loss 0.436561, acc 0.9375, prec 0.0921372, recall 0.821916
2017-12-10T04:46:32.368455: step 6178, loss 0.0981691, acc 0.96875, prec 0.0921348, recall 0.821916
2017-12-10T04:46:32.639195: step 6179, loss 0.0748747, acc 0.984375, prec 0.0921684, recall 0.821977
2017-12-10T04:46:32.905299: step 6180, loss 0.329955, acc 0.921875, prec 0.0921741, recall 0.821997
2017-12-10T04:46:33.173583: step 6181, loss 0.166751, acc 1, prec 0.0921972, recall 0.822038
2017-12-10T04:46:33.444980: step 6182, loss 0.159974, acc 0.96875, prec 0.0922064, recall 0.822058
2017-12-10T04:46:33.718488: step 6183, loss 0.254116, acc 0.953125, prec 0.0922145, recall 0.822078
2017-12-10T04:46:33.995096: step 6184, loss 0.118087, acc 0.953125, prec 0.0922225, recall 0.822098
2017-12-10T04:46:34.264428: step 6185, loss 0.343657, acc 0.953125, prec 0.0922306, recall 0.822119
2017-12-10T04:46:34.526940: step 6186, loss 0.185921, acc 0.953125, prec 0.0922386, recall 0.822139
2017-12-10T04:46:34.793735: step 6187, loss 0.0355551, acc 0.984375, prec 0.092249, recall 0.822159
2017-12-10T04:46:35.069919: step 6188, loss 0.0627659, acc 1, prec 0.0922838, recall 0.82222
2017-12-10T04:46:35.337497: step 6189, loss 0.0968222, acc 0.984375, prec 0.0922826, recall 0.82222
2017-12-10T04:46:35.604610: step 6190, loss 0.0257273, acc 1, prec 0.0922942, recall 0.82224
2017-12-10T04:46:35.874475: step 6191, loss 0.00479072, acc 1, prec 0.0922942, recall 0.82224
2017-12-10T04:46:36.149456: step 6192, loss 0.311123, acc 0.96875, prec 0.0922918, recall 0.82224
2017-12-10T04:46:36.418900: step 6193, loss 0.0305537, acc 0.984375, prec 0.0922906, recall 0.82224
2017-12-10T04:46:36.686852: step 6194, loss 0.329476, acc 0.96875, prec 0.0922883, recall 0.82224
2017-12-10T04:46:36.960250: step 6195, loss 0.00533201, acc 1, prec 0.0922883, recall 0.82224
2017-12-10T04:46:37.235318: step 6196, loss 0.192484, acc 0.953125, prec 0.0922847, recall 0.82224
2017-12-10T04:46:37.505180: step 6197, loss 0.0171291, acc 1, prec 0.0923079, recall 0.82228
2017-12-10T04:46:37.772388: step 6198, loss 0.467657, acc 1, prec 0.092331, recall 0.822321
2017-12-10T04:46:38.039432: step 6199, loss 2.57901, acc 0.984375, prec 0.092331, recall 0.822227
2017-12-10T04:46:38.314278: step 6200, loss 0.204684, acc 0.96875, prec 0.0923287, recall 0.822227
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-6200

2017-12-10T04:46:39.660618: step 6201, loss 0.028175, acc 0.984375, prec 0.0923275, recall 0.822227
2017-12-10T04:46:39.927468: step 6202, loss 0.0619316, acc 0.9375, prec 0.0923344, recall 0.822247
2017-12-10T04:46:40.196868: step 6203, loss 0.0618255, acc 0.96875, prec 0.092332, recall 0.822247
2017-12-10T04:46:40.463141: step 6204, loss 0.142292, acc 0.96875, prec 0.0923412, recall 0.822268
2017-12-10T04:46:40.727284: step 6205, loss 0.527043, acc 0.953125, prec 0.0923608, recall 0.822308
2017-12-10T04:46:40.998096: step 6206, loss 0.311944, acc 0.890625, prec 0.0923642, recall 0.822328
2017-12-10T04:46:41.269851: step 6207, loss 1.03797, acc 0.859375, prec 0.0923651, recall 0.822348
2017-12-10T04:46:41.544093: step 6208, loss 0.320121, acc 0.9375, prec 0.0923604, recall 0.822348
2017-12-10T04:46:41.810383: step 6209, loss 0.385313, acc 0.9375, prec 0.0923788, recall 0.822389
2017-12-10T04:46:42.078721: step 6210, loss 0.781954, acc 0.890625, prec 0.0923706, recall 0.822389
2017-12-10T04:46:42.353296: step 6211, loss 0.420538, acc 0.921875, prec 0.0923763, recall 0.822409
2017-12-10T04:46:42.623999: step 6212, loss 0.219197, acc 0.953125, prec 0.0923959, recall 0.822449
2017-12-10T04:46:42.893844: step 6213, loss 0.548552, acc 0.90625, prec 0.0924004, recall 0.822469
2017-12-10T04:46:43.171245: step 6214, loss 0.121985, acc 0.953125, prec 0.0924084, recall 0.822489
2017-12-10T04:46:43.434730: step 6215, loss 0.909082, acc 0.890625, prec 0.0924002, recall 0.822489
2017-12-10T04:46:43.707025: step 6216, loss 0.558222, acc 0.9375, prec 0.0924186, recall 0.822529
2017-12-10T04:46:43.973312: step 6217, loss 0.364313, acc 0.96875, prec 0.0924509, recall 0.82259
2017-12-10T04:46:44.239747: step 6218, loss 0.176764, acc 0.9375, prec 0.0924808, recall 0.82265
2017-12-10T04:46:44.515622: step 6219, loss 0.331268, acc 0.953125, prec 0.0924889, recall 0.82267
2017-12-10T04:46:44.788570: step 6220, loss 0.459711, acc 0.9375, prec 0.0924842, recall 0.82267
2017-12-10T04:46:45.058701: step 6221, loss 0.349955, acc 0.9375, prec 0.0924794, recall 0.82267
2017-12-10T04:46:45.325593: step 6222, loss 0.280554, acc 0.96875, prec 0.0924886, recall 0.82269
2017-12-10T04:46:45.592717: step 6223, loss 0.566748, acc 0.921875, prec 0.0924943, recall 0.82271
2017-12-10T04:46:45.860098: step 6224, loss 0.395149, acc 0.9375, prec 0.0924896, recall 0.82271
2017-12-10T04:46:46.136491: step 6225, loss 0.080274, acc 0.953125, prec 0.0924976, recall 0.82273
2017-12-10T04:46:46.404609: step 6226, loss 0.00986778, acc 1, prec 0.0925207, recall 0.82277
2017-12-10T04:46:46.674443: step 6227, loss 0.27484, acc 0.9375, prec 0.0925276, recall 0.822791
2017-12-10T04:46:46.946437: step 6228, loss 0.000978808, acc 1, prec 0.0925276, recall 0.822791
2017-12-10T04:46:47.213753: step 6229, loss 0.195443, acc 0.9375, prec 0.0925228, recall 0.822791
2017-12-10T04:46:47.479139: step 6230, loss 0.204513, acc 0.96875, prec 0.092532, recall 0.822811
2017-12-10T04:46:47.754750: step 6231, loss 0.269407, acc 0.96875, prec 0.0925297, recall 0.822811
2017-12-10T04:46:48.026379: step 6232, loss 0.376684, acc 0.921875, prec 0.0925238, recall 0.822811
2017-12-10T04:46:48.296339: step 6233, loss 0.460554, acc 0.921875, prec 0.0925295, recall 0.822831
2017-12-10T04:46:48.568104: step 6234, loss 0.0737661, acc 0.96875, prec 0.0925386, recall 0.822851
2017-12-10T04:46:48.842676: step 6235, loss 0.0882064, acc 0.96875, prec 0.0925363, recall 0.822851
2017-12-10T04:46:49.109822: step 6236, loss 0.0318594, acc 0.96875, prec 0.0925455, recall 0.822871
2017-12-10T04:46:49.381481: step 6237, loss 0.0127246, acc 1, prec 0.0925455, recall 0.822871
2017-12-10T04:46:49.651029: step 6238, loss 0.165977, acc 0.96875, prec 0.0925778, recall 0.822931
2017-12-10T04:46:49.922311: step 6239, loss 0.0274107, acc 0.984375, prec 0.0925881, recall 0.822951
2017-12-10T04:46:50.196462: step 6240, loss 0.0535349, acc 0.953125, prec 0.0925961, recall 0.822971
2017-12-10T04:46:50.465154: step 6241, loss 0.138351, acc 0.984375, prec 0.092595, recall 0.822971
2017-12-10T04:46:50.731505: step 6242, loss 0.0164002, acc 0.984375, prec 0.0926284, recall 0.823031
2017-12-10T04:46:50.996421: step 6243, loss 4.50714e-05, acc 1, prec 0.0926284, recall 0.823031
2017-12-10T04:46:51.272626: step 6244, loss 0.00038988, acc 1, prec 0.0926284, recall 0.823031
2017-12-10T04:46:51.535955: step 6245, loss 0.0106631, acc 1, prec 0.0926284, recall 0.823031
2017-12-10T04:46:51.798731: step 6246, loss 1.17763, acc 0.984375, prec 0.0926503, recall 0.823071
2017-12-10T04:46:52.067473: step 6247, loss 0.255381, acc 0.96875, prec 0.0926595, recall 0.823091
2017-12-10T04:46:52.334817: step 6248, loss 0.338221, acc 0.953125, prec 0.0926675, recall 0.823111
2017-12-10T04:46:52.604273: step 6249, loss 2.0411, acc 0.984375, prec 0.092679, recall 0.823038
2017-12-10T04:46:52.880990: step 6250, loss 0.0432717, acc 0.984375, prec 0.0926894, recall 0.823058
2017-12-10T04:46:53.151258: step 6251, loss 0.327848, acc 0.921875, prec 0.0927066, recall 0.823098
2017-12-10T04:46:53.421489: step 6252, loss 0.320272, acc 0.96875, prec 0.0927157, recall 0.823118
2017-12-10T04:46:53.688099: step 6253, loss 0.176377, acc 0.9375, prec 0.0927226, recall 0.823138
2017-12-10T04:46:53.958062: step 6254, loss 0.132266, acc 0.953125, prec 0.092719, recall 0.823138
2017-12-10T04:46:54.222388: step 6255, loss 0.580179, acc 0.9375, prec 0.0927143, recall 0.823138
2017-12-10T04:46:54.490945: step 6256, loss 0.308086, acc 0.953125, prec 0.0927223, recall 0.823158
2017-12-10T04:46:54.763224: step 6257, loss 0.205049, acc 0.9375, prec 0.0927407, recall 0.823198
2017-12-10T04:46:55.025409: step 6258, loss 0.207112, acc 0.9375, prec 0.0927475, recall 0.823218
2017-12-10T04:46:55.299852: step 6259, loss 0.308784, acc 0.90625, prec 0.0927404, recall 0.823218
2017-12-10T04:46:55.566061: step 6260, loss 0.576858, acc 0.890625, prec 0.0927437, recall 0.823237
2017-12-10T04:46:55.843393: step 6261, loss 0.527533, acc 0.921875, prec 0.0927378, recall 0.823237
2017-12-10T04:46:56.112603: step 6262, loss 0.738988, acc 0.90625, prec 0.0927538, recall 0.823277
2017-12-10T04:46:56.386350: step 6263, loss 0.0383342, acc 0.984375, prec 0.0927872, recall 0.823337
2017-12-10T04:46:56.659534: step 6264, loss 0.448321, acc 0.875, prec 0.0927777, recall 0.823337
2017-12-10T04:46:56.927426: step 6265, loss 0.16987, acc 0.96875, prec 0.0927869, recall 0.823357
2017-12-10T04:46:57.207587: step 6266, loss 0.122544, acc 0.96875, prec 0.0927846, recall 0.823357
2017-12-10T04:46:57.476692: step 6267, loss 0.0456102, acc 0.96875, prec 0.0927822, recall 0.823357
2017-12-10T04:46:57.751582: step 6268, loss 0.270978, acc 0.96875, prec 0.0927798, recall 0.823357
2017-12-10T04:46:58.016455: step 6269, loss 0.977745, acc 0.921875, prec 0.09282, recall 0.823437
2017-12-10T04:46:58.282083: step 6270, loss 0.205977, acc 0.921875, prec 0.0928372, recall 0.823476
2017-12-10T04:46:58.547656: step 6271, loss 0.20656, acc 0.953125, prec 0.0928336, recall 0.823476
2017-12-10T04:46:58.813165: step 6272, loss 0.0835164, acc 0.984375, prec 0.0928555, recall 0.823516
2017-12-10T04:46:59.081552: step 6273, loss 0.069495, acc 0.96875, prec 0.0928532, recall 0.823516
2017-12-10T04:46:59.350978: step 6274, loss 0.193319, acc 0.953125, prec 0.0928726, recall 0.823556
2017-12-10T04:46:59.615636: step 6275, loss 0.252735, acc 0.953125, prec 0.0928806, recall 0.823576
2017-12-10T04:46:59.884649: step 6276, loss 0.0820493, acc 0.984375, prec 0.092891, recall 0.823596
2017-12-10T04:47:00.148247: step 6277, loss 0.0877985, acc 0.96875, prec 0.0929001, recall 0.823615
2017-12-10T04:47:00.426622: step 6278, loss 0.303145, acc 0.9375, prec 0.0928954, recall 0.823615
2017-12-10T04:47:00.702661: step 6279, loss 0.13505, acc 0.96875, prec 0.0929046, recall 0.823635
2017-12-10T04:47:00.969408: step 6280, loss 0.00455923, acc 1, prec 0.0929276, recall 0.823675
2017-12-10T04:47:01.235293: step 6281, loss 0.0326358, acc 0.984375, prec 0.0929379, recall 0.823695
2017-12-10T04:47:01.505826: step 6282, loss 0.163307, acc 0.96875, prec 0.0929471, recall 0.823715
2017-12-10T04:47:01.780575: step 6283, loss 0.100583, acc 0.984375, prec 0.0929805, recall 0.823774
2017-12-10T04:47:02.049742: step 6284, loss 0.057502, acc 0.984375, prec 0.0929908, recall 0.823794
2017-12-10T04:47:02.316775: step 6285, loss 1.08976, acc 0.984375, prec 0.0930126, recall 0.823834
2017-12-10T04:47:02.585897: step 6286, loss 0.0747702, acc 0.984375, prec 0.0930345, recall 0.823873
2017-12-10T04:47:02.856572: step 6287, loss 0.0239114, acc 0.984375, prec 0.0930448, recall 0.823893
2017-12-10T04:47:03.128749: step 6288, loss 0.0490697, acc 0.984375, prec 0.0930436, recall 0.823893
2017-12-10T04:47:03.390702: step 6289, loss 0.290684, acc 0.921875, prec 0.0930492, recall 0.823913
2017-12-10T04:47:03.669136: step 6290, loss 0.0566878, acc 0.984375, prec 0.093048, recall 0.823913
2017-12-10T04:47:03.938452: step 6291, loss 0.555062, acc 0.953125, prec 0.093079, recall 0.823972
2017-12-10T04:47:04.213455: step 6292, loss 0.225621, acc 0.9375, prec 0.0930743, recall 0.823972
2017-12-10T04:47:04.481982: step 6293, loss 0.199606, acc 0.953125, prec 0.0930823, recall 0.823992
2017-12-10T04:47:04.752701: step 6294, loss 0.289317, acc 1, prec 0.0930938, recall 0.824012
2017-12-10T04:47:05.031145: step 6295, loss 0.0295562, acc 1, prec 0.0931168, recall 0.824051
2017-12-10T04:47:05.295229: step 6296, loss 0.0448775, acc 0.96875, prec 0.0931259, recall 0.824071
2017-12-10T04:47:05.569274: step 6297, loss 3.70167, acc 0.90625, prec 0.093143, recall 0.824018
2017-12-10T04:47:05.850372: step 6298, loss 0.0428846, acc 0.984375, prec 0.0931534, recall 0.824038
2017-12-10T04:47:06.116038: step 6299, loss 0.263522, acc 0.9375, prec 0.0931486, recall 0.824038
2017-12-10T04:47:06.387298: step 6300, loss 0.480336, acc 0.90625, prec 0.093153, recall 0.824057

Evaluation:
2017-12-10T04:47:13.987154: step 6300, loss 3.22501, acc 0.910927, prec 0.0932027, recall 0.821192

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-6300

2017-12-10T04:47:15.257661: step 6301, loss 0.636018, acc 0.921875, prec 0.0932082, recall 0.821212
2017-12-10T04:47:15.526909: step 6302, loss 0.174772, acc 0.9375, prec 0.0932036, recall 0.821212
2017-12-10T04:47:15.798949: step 6303, loss 0.579878, acc 0.890625, prec 0.0931954, recall 0.821212
2017-12-10T04:47:16.071603: step 6304, loss 0.272986, acc 0.890625, prec 0.0932099, recall 0.821251
2017-12-10T04:47:16.343808: step 6305, loss 0.370437, acc 0.921875, prec 0.0932155, recall 0.821271
2017-12-10T04:47:16.610195: step 6306, loss 0.999219, acc 0.890625, prec 0.0932073, recall 0.821271
2017-12-10T04:47:16.881476: step 6307, loss 0.263667, acc 0.96875, prec 0.093205, recall 0.821271
2017-12-10T04:47:17.147439: step 6308, loss 0.572118, acc 0.890625, prec 0.0932081, recall 0.821291
2017-12-10T04:47:17.424661: step 6309, loss 0.196408, acc 0.953125, prec 0.0932046, recall 0.821291
2017-12-10T04:47:17.693032: step 6310, loss 0.176749, acc 0.9375, prec 0.093234, recall 0.82135
2017-12-10T04:47:17.963087: step 6311, loss 0.318135, acc 0.921875, prec 0.0932622, recall 0.821409
2017-12-10T04:47:18.236076: step 6312, loss 0.342132, acc 0.90625, prec 0.0932552, recall 0.821409
2017-12-10T04:47:18.507003: step 6313, loss 0.458175, acc 0.90625, prec 0.0932482, recall 0.821409
2017-12-10T04:47:18.774731: step 6314, loss 0.594042, acc 0.859375, prec 0.0932377, recall 0.821409
2017-12-10T04:47:19.043625: step 6315, loss 0.116139, acc 0.953125, prec 0.0932342, recall 0.821409
2017-12-10T04:47:19.309003: step 6316, loss 0.602146, acc 0.90625, prec 0.0932499, recall 0.821448
2017-12-10T04:47:19.572849: step 6317, loss 0.257973, acc 0.9375, prec 0.0932566, recall 0.821468
2017-12-10T04:47:19.835043: step 6318, loss 0.538252, acc 0.90625, prec 0.0932496, recall 0.821468
2017-12-10T04:47:20.105973: step 6319, loss 0.0142638, acc 1, prec 0.0932723, recall 0.821507
2017-12-10T04:47:20.376354: step 6320, loss 0.417207, acc 0.890625, prec 0.0932868, recall 0.821547
2017-12-10T04:47:20.643528: step 6321, loss 0.147692, acc 0.96875, prec 0.0932958, recall 0.821566
2017-12-10T04:47:20.909195: step 6322, loss 0.111751, acc 0.953125, prec 0.093315, recall 0.821606
2017-12-10T04:47:21.176280: step 6323, loss 0.0971584, acc 0.96875, prec 0.0933127, recall 0.821606
2017-12-10T04:47:21.443854: step 6324, loss 0.0696818, acc 0.984375, prec 0.0933115, recall 0.821606
2017-12-10T04:47:21.710034: step 6325, loss 0.00170781, acc 1, prec 0.0933115, recall 0.821606
2017-12-10T04:47:21.971922: step 6326, loss 0.0204826, acc 0.984375, prec 0.0933217, recall 0.821625
2017-12-10T04:47:22.237344: step 6327, loss 0.0628581, acc 0.984375, prec 0.0933318, recall 0.821645
2017-12-10T04:47:22.507145: step 6328, loss 2.66126, acc 0.953125, prec 0.0933295, recall 0.821554
2017-12-10T04:47:22.776268: step 6329, loss 0.00104042, acc 1, prec 0.0933408, recall 0.821574
2017-12-10T04:47:23.039792: step 6330, loss 0.0854045, acc 0.984375, prec 0.0933623, recall 0.821613
2017-12-10T04:47:23.309595: step 6331, loss 0.104694, acc 0.96875, prec 0.0933713, recall 0.821633
2017-12-10T04:47:23.581264: step 6332, loss 0.0147577, acc 0.984375, prec 0.0933815, recall 0.821653
2017-12-10T04:47:23.846925: step 6333, loss 0.125041, acc 0.96875, prec 0.0933792, recall 0.821653
2017-12-10T04:47:24.113868: step 6334, loss 0.53354, acc 0.9375, prec 0.0933858, recall 0.821672
2017-12-10T04:47:24.378069: step 6335, loss 4.13135, acc 0.9375, prec 0.0933823, recall 0.821582
2017-12-10T04:47:24.649901: step 6336, loss 0.339072, acc 0.96875, prec 0.0933913, recall 0.821601
2017-12-10T04:47:24.915827: step 6337, loss 0.303831, acc 0.921875, prec 0.0934082, recall 0.821641
2017-12-10T04:47:25.186738: step 6338, loss 0.453171, acc 0.84375, prec 0.0934078, recall 0.82166
2017-12-10T04:47:25.452776: step 6339, loss 0.19205, acc 0.953125, prec 0.0934043, recall 0.82166
2017-12-10T04:47:25.715722: step 6340, loss 0.711456, acc 0.859375, prec 0.0934051, recall 0.82168
2017-12-10T04:47:25.988821: step 6341, loss 0.885664, acc 0.859375, prec 0.093406, recall 0.821699
2017-12-10T04:47:26.250033: step 6342, loss 0.686597, acc 0.90625, prec 0.0934216, recall 0.821739
2017-12-10T04:47:26.514589: step 6343, loss 0.509079, acc 0.890625, prec 0.0934474, recall 0.821797
2017-12-10T04:47:26.784334: step 6344, loss 0.906048, acc 0.828125, prec 0.0934459, recall 0.821817
2017-12-10T04:47:27.062886: step 6345, loss 0.90404, acc 0.921875, prec 0.0934627, recall 0.821856
2017-12-10T04:47:27.332382: step 6346, loss 0.984569, acc 0.8125, prec 0.09346, recall 0.821876
2017-12-10T04:47:27.604810: step 6347, loss 1.03146, acc 0.890625, prec 0.0934632, recall 0.821895
2017-12-10T04:47:27.868517: step 6348, loss 0.710542, acc 0.859375, prec 0.093464, recall 0.821915
2017-12-10T04:47:28.137501: step 6349, loss 0.134606, acc 0.9375, prec 0.0934707, recall 0.821934
2017-12-10T04:47:28.404806: step 6350, loss 0.94251, acc 0.90625, prec 0.0934637, recall 0.821934
2017-12-10T04:47:28.678528: step 6351, loss 0.259696, acc 0.921875, prec 0.0934805, recall 0.821973
2017-12-10T04:47:28.947860: step 6352, loss 0.322574, acc 0.921875, prec 0.0934973, recall 0.822012
2017-12-10T04:47:29.213995: step 6353, loss 0.291849, acc 0.90625, prec 0.0935129, recall 0.822052
2017-12-10T04:47:29.479847: step 6354, loss 0.331642, acc 0.9375, prec 0.0935082, recall 0.822052
2017-12-10T04:47:29.755259: step 6355, loss 0.0581928, acc 0.96875, prec 0.0935172, recall 0.822071
2017-12-10T04:47:30.019309: step 6356, loss 0.58285, acc 0.953125, prec 0.093525, recall 0.822091
2017-12-10T04:47:30.295424: step 6357, loss 0.540998, acc 0.890625, prec 0.0935281, recall 0.82211
2017-12-10T04:47:30.568582: step 6358, loss 0.0795634, acc 0.953125, prec 0.093536, recall 0.82213
2017-12-10T04:47:30.835763: step 6359, loss 0.17343, acc 0.9375, prec 0.0935313, recall 0.82213
2017-12-10T04:47:31.104864: step 6360, loss 0.00487032, acc 1, prec 0.0935313, recall 0.82213
2017-12-10T04:47:31.369431: step 6361, loss 0.138799, acc 0.953125, prec 0.0935391, recall 0.822149
2017-12-10T04:47:31.637839: step 6362, loss 0.116973, acc 0.96875, prec 0.0935368, recall 0.822149
2017-12-10T04:47:31.907066: step 6363, loss 0.154747, acc 1, prec 0.0935481, recall 0.822169
2017-12-10T04:47:32.177045: step 6364, loss 0.0285042, acc 0.984375, prec 0.0935469, recall 0.822169
2017-12-10T04:47:32.443042: step 6365, loss 0.0881528, acc 0.953125, prec 0.0935434, recall 0.822169
2017-12-10T04:47:32.719964: step 6366, loss 0.0999295, acc 0.953125, prec 0.0935512, recall 0.822188
2017-12-10T04:47:32.984536: step 6367, loss 0.0157683, acc 0.984375, prec 0.09355, recall 0.822188
2017-12-10T04:47:33.247661: step 6368, loss 0.00113692, acc 1, prec 0.0935613, recall 0.822208
2017-12-10T04:47:33.513685: step 6369, loss 5.07053, acc 0.984375, prec 0.093584, recall 0.822156
2017-12-10T04:47:33.783384: step 6370, loss 0.14694, acc 0.984375, prec 0.0935828, recall 0.822156
2017-12-10T04:47:34.050779: step 6371, loss 0.320122, acc 0.9375, prec 0.0935781, recall 0.822156
2017-12-10T04:47:34.319754: step 6372, loss 0.0291205, acc 0.984375, prec 0.093577, recall 0.822156
2017-12-10T04:47:34.586304: step 6373, loss 0.505578, acc 0.953125, prec 0.0935848, recall 0.822176
2017-12-10T04:47:34.852071: step 6374, loss 0.00922702, acc 1, prec 0.0935961, recall 0.822195
2017-12-10T04:47:35.122717: step 6375, loss 0.061934, acc 0.96875, prec 0.093605, recall 0.822215
2017-12-10T04:47:35.393246: step 6376, loss 0.460336, acc 0.96875, prec 0.0936366, recall 0.822273
2017-12-10T04:47:35.659481: step 6377, loss 0.0105525, acc 1, prec 0.0936366, recall 0.822273
2017-12-10T04:47:35.924824: step 6378, loss 0.251234, acc 0.921875, prec 0.0936308, recall 0.822273
2017-12-10T04:47:36.191208: step 6379, loss 0.355203, acc 0.953125, prec 0.0936386, recall 0.822293
2017-12-10T04:47:36.457152: step 6380, loss 0.186436, acc 0.90625, prec 0.0936768, recall 0.822371
2017-12-10T04:47:36.724347: step 6381, loss 0.156827, acc 0.953125, prec 0.0936958, recall 0.822409
2017-12-10T04:47:36.995443: step 6382, loss 0.257666, acc 0.953125, prec 0.0937262, recall 0.822468
2017-12-10T04:47:37.263422: step 6383, loss 0.33162, acc 0.921875, prec 0.093743, recall 0.822507
2017-12-10T04:47:37.530031: step 6384, loss 0.0860238, acc 0.96875, prec 0.0937632, recall 0.822545
2017-12-10T04:47:37.802384: step 6385, loss 0.0546537, acc 0.984375, prec 0.0937621, recall 0.822545
2017-12-10T04:47:38.070911: step 6386, loss 0.360746, acc 0.921875, prec 0.0937562, recall 0.822545
2017-12-10T04:47:38.345897: step 6387, loss 1.28046, acc 0.890625, prec 0.0937706, recall 0.822584
2017-12-10T04:47:38.610842: step 6388, loss 0.460398, acc 0.921875, prec 0.0937874, recall 0.822623
2017-12-10T04:47:38.876931: step 6389, loss 0.249377, acc 0.90625, prec 0.0937804, recall 0.822623
2017-12-10T04:47:39.144458: step 6390, loss 0.0138548, acc 1, prec 0.0937917, recall 0.822642
2017-12-10T04:47:39.409149: step 6391, loss 0.148098, acc 1, prec 0.0938142, recall 0.822681
2017-12-10T04:47:39.679205: step 6392, loss 0.0626938, acc 0.984375, prec 0.0938357, recall 0.82272
2017-12-10T04:47:39.959138: step 6393, loss 0.156568, acc 0.953125, prec 0.0938321, recall 0.82272
2017-12-10T04:47:40.229440: step 6394, loss 0.321272, acc 0.96875, prec 0.0938411, recall 0.822739
2017-12-10T04:47:40.497655: step 6395, loss 0.0285306, acc 0.984375, prec 0.0938512, recall 0.822759
2017-12-10T04:47:40.766738: step 6396, loss 0.343607, acc 0.9375, prec 0.0938465, recall 0.822759
2017-12-10T04:47:41.046772: step 6397, loss 0.0809046, acc 0.96875, prec 0.0938668, recall 0.822797
2017-12-10T04:47:41.320092: step 6398, loss 0.387852, acc 0.9375, prec 0.0938734, recall 0.822817
2017-12-10T04:47:41.590142: step 6399, loss 0.0112099, acc 1, prec 0.0938734, recall 0.822817
2017-12-10T04:47:41.853412: step 6400, loss 0.0594862, acc 1, prec 0.0938847, recall 0.822836
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-6400

2017-12-10T04:47:43.135797: step 6401, loss 0.0817375, acc 0.953125, prec 0.0938924, recall 0.822855
2017-12-10T04:47:43.403103: step 6402, loss 0.089457, acc 0.984375, prec 0.0939138, recall 0.822894
2017-12-10T04:47:43.667839: step 6403, loss 0.02139, acc 0.984375, prec 0.0939127, recall 0.822894
2017-12-10T04:47:43.937459: step 6404, loss 0.00348437, acc 1, prec 0.0939127, recall 0.822894
2017-12-10T04:47:44.203328: step 6405, loss 0.0214642, acc 0.984375, prec 0.0939115, recall 0.822894
2017-12-10T04:47:44.467228: step 6406, loss 0.0202375, acc 0.984375, prec 0.0939103, recall 0.822894
2017-12-10T04:47:44.737786: step 6407, loss 0.0885956, acc 0.96875, prec 0.093908, recall 0.822894
2017-12-10T04:47:45.002375: step 6408, loss 0.0868149, acc 0.984375, prec 0.0939181, recall 0.822913
2017-12-10T04:47:45.267632: step 6409, loss 0.0692249, acc 0.984375, prec 0.0939169, recall 0.822913
2017-12-10T04:47:45.534178: step 6410, loss 0.626281, acc 0.984375, prec 0.0939271, recall 0.822933
2017-12-10T04:47:45.799996: step 6411, loss 0.145321, acc 0.984375, prec 0.0939484, recall 0.822971
2017-12-10T04:47:46.084971: step 6412, loss 0.192903, acc 0.96875, prec 0.09398, recall 0.823029
2017-12-10T04:47:46.355856: step 6413, loss 0.0146553, acc 1, prec 0.0939912, recall 0.823048
2017-12-10T04:47:46.620824: step 6414, loss 0.245349, acc 0.984375, prec 0.0939901, recall 0.823048
2017-12-10T04:47:46.890150: step 6415, loss 0.0857447, acc 0.984375, prec 0.0939889, recall 0.823048
2017-12-10T04:47:47.158757: step 6416, loss 0.00451253, acc 1, prec 0.0939889, recall 0.823048
2017-12-10T04:47:47.431331: step 6417, loss 0.149776, acc 0.96875, prec 0.0940091, recall 0.823087
2017-12-10T04:47:47.698802: step 6418, loss 0.00656109, acc 1, prec 0.0940429, recall 0.823145
2017-12-10T04:47:47.967289: step 6419, loss 0.0695781, acc 0.984375, prec 0.0940418, recall 0.823145
2017-12-10T04:47:48.241014: step 6420, loss 0.0606377, acc 0.96875, prec 0.0940394, recall 0.823145
2017-12-10T04:47:48.507527: step 6421, loss 0.184538, acc 1, prec 0.0940733, recall 0.823203
2017-12-10T04:47:48.775692: step 6422, loss 0.072309, acc 0.984375, prec 0.0940721, recall 0.823203
2017-12-10T04:47:49.041560: step 6423, loss 0.0403881, acc 1, prec 0.0940834, recall 0.823222
2017-12-10T04:47:49.310810: step 6424, loss 0.378755, acc 1, prec 0.0940947, recall 0.823241
2017-12-10T04:47:49.582830: step 6425, loss 0.145447, acc 0.96875, prec 0.0940923, recall 0.823241
2017-12-10T04:47:49.852248: step 6426, loss 0.00349736, acc 1, prec 0.0940923, recall 0.823241
2017-12-10T04:47:50.121145: step 6427, loss 0.117062, acc 0.96875, prec 0.09409, recall 0.823241
2017-12-10T04:47:50.388659: step 6428, loss 0.309392, acc 0.984375, prec 0.0941114, recall 0.82328
2017-12-10T04:47:50.655414: step 6429, loss 0.131228, acc 0.984375, prec 0.0941102, recall 0.82328
2017-12-10T04:47:50.922036: step 6430, loss 0.0389742, acc 0.984375, prec 0.0941203, recall 0.823299
2017-12-10T04:47:51.190888: step 6431, loss 0.101745, acc 0.984375, prec 0.0941529, recall 0.823357
2017-12-10T04:47:51.459195: step 6432, loss 0.452871, acc 0.96875, prec 0.0941844, recall 0.823414
2017-12-10T04:47:51.723946: step 6433, loss 0.472417, acc 0.984375, prec 0.0941945, recall 0.823433
2017-12-10T04:47:51.992067: step 6434, loss 0.19558, acc 0.9375, prec 0.0941898, recall 0.823433
2017-12-10T04:47:52.259025: step 6435, loss 0.0129129, acc 1, prec 0.0941898, recall 0.823433
2017-12-10T04:47:52.532901: step 6436, loss 5.32074, acc 0.953125, prec 0.09421, recall 0.823382
2017-12-10T04:47:52.806388: step 6437, loss 0.186689, acc 0.953125, prec 0.0942065, recall 0.823382
2017-12-10T04:47:53.070293: step 6438, loss 0.533653, acc 0.921875, prec 0.0942006, recall 0.823382
2017-12-10T04:47:53.341620: step 6439, loss 0.0875949, acc 0.953125, prec 0.0942084, recall 0.823401
2017-12-10T04:47:53.608419: step 6440, loss 1.06023, acc 0.921875, prec 0.0942025, recall 0.823401
2017-12-10T04:47:53.871063: step 6441, loss 1.07465, acc 0.859375, prec 0.094192, recall 0.823401
2017-12-10T04:47:54.144032: step 6442, loss 0.188732, acc 0.96875, prec 0.0942009, recall 0.823421
2017-12-10T04:47:54.408798: step 6443, loss 0.289883, acc 0.890625, prec 0.0942152, recall 0.823459
2017-12-10T04:47:54.682368: step 6444, loss 0.387836, acc 0.90625, prec 0.0942082, recall 0.823459
2017-12-10T04:47:54.950391: step 6445, loss 0.713042, acc 0.859375, prec 0.0941977, recall 0.823459
2017-12-10T04:47:55.227687: step 6446, loss 0.729027, acc 0.875, prec 0.0942108, recall 0.823497
2017-12-10T04:47:55.497972: step 6447, loss 0.424009, acc 0.921875, prec 0.0942388, recall 0.823555
2017-12-10T04:47:55.775739: step 6448, loss 0.0970209, acc 0.96875, prec 0.0942364, recall 0.823555
2017-12-10T04:47:56.047688: step 6449, loss 0.302704, acc 0.890625, prec 0.0942507, recall 0.823593
2017-12-10T04:47:56.321961: step 6450, loss 1.25055, acc 0.859375, prec 0.0942402, recall 0.823593
2017-12-10T04:47:56.591783: step 6451, loss 0.276894, acc 0.953125, prec 0.0942479, recall 0.823612
2017-12-10T04:47:56.867500: step 6452, loss 0.43791, acc 0.9375, prec 0.0942545, recall 0.823632
2017-12-10T04:47:57.148457: step 6453, loss 0.0921642, acc 0.96875, prec 0.0942634, recall 0.823651
2017-12-10T04:47:57.414953: step 6454, loss 0.181702, acc 0.953125, prec 0.0942824, recall 0.823689
2017-12-10T04:47:57.678647: step 6455, loss 0.322608, acc 0.9375, prec 0.0942777, recall 0.823689
2017-12-10T04:47:57.954446: step 6456, loss 0.413504, acc 0.90625, prec 0.0942819, recall 0.823708
2017-12-10T04:47:58.230010: step 6457, loss 0.0597832, acc 0.984375, prec 0.0942808, recall 0.823708
2017-12-10T04:47:58.496601: step 6458, loss 0.19475, acc 0.9375, prec 0.0942986, recall 0.823747
2017-12-10T04:47:58.769647: step 6459, loss 0.114724, acc 0.96875, prec 0.09433, recall 0.823804
2017-12-10T04:47:59.038353: step 6460, loss 0.330016, acc 0.90625, prec 0.0943342, recall 0.823823
2017-12-10T04:47:59.276987: step 6461, loss 0.0772984, acc 0.961538, prec 0.0943319, recall 0.823823
2017-12-10T04:47:59.551763: step 6462, loss 0.166748, acc 0.96875, prec 0.094352, recall 0.823861
2017-12-10T04:47:59.823034: step 6463, loss 0.0204689, acc 0.984375, prec 0.0943734, recall 0.823899
2017-12-10T04:48:00.095261: step 6464, loss 0.00209516, acc 1, prec 0.0943846, recall 0.823918
2017-12-10T04:48:00.373992: step 6465, loss 0.0270841, acc 0.984375, prec 0.0943834, recall 0.823918
2017-12-10T04:48:00.647792: step 6466, loss 0.0474099, acc 0.96875, prec 0.0943811, recall 0.823918
2017-12-10T04:48:00.914726: step 6467, loss 0.30233, acc 0.984375, prec 0.0944024, recall 0.823957
2017-12-10T04:48:01.175237: step 6468, loss 0.000310503, acc 1, prec 0.0944024, recall 0.823957
2017-12-10T04:48:01.441554: step 6469, loss 0.0108353, acc 1, prec 0.0944024, recall 0.823957
2017-12-10T04:48:01.707345: step 6470, loss 0.00257924, acc 1, prec 0.0944024, recall 0.823957
2017-12-10T04:48:01.978515: step 6471, loss 0.10479, acc 0.984375, prec 0.0944013, recall 0.823957
2017-12-10T04:48:02.242959: step 6472, loss 0.203519, acc 0.96875, prec 0.0943989, recall 0.823957
2017-12-10T04:48:02.511171: step 6473, loss 0.252846, acc 0.96875, prec 0.0943966, recall 0.823957
2017-12-10T04:48:02.780543: step 6474, loss 0.24338, acc 0.984375, prec 0.0944066, recall 0.823976
2017-12-10T04:48:03.059156: step 6475, loss 0.152821, acc 0.984375, prec 0.0944055, recall 0.823976
2017-12-10T04:48:03.332301: step 6476, loss 0.206687, acc 1, prec 0.0944167, recall 0.823995
2017-12-10T04:48:03.604522: step 6477, loss 0.519133, acc 0.96875, prec 0.0944144, recall 0.823995
2017-12-10T04:48:03.866847: step 6478, loss 0.218837, acc 0.96875, prec 0.0944233, recall 0.824014
2017-12-10T04:48:04.135367: step 6479, loss 0.0346962, acc 0.984375, prec 0.0944446, recall 0.824052
2017-12-10T04:48:04.400046: step 6480, loss 0.0723095, acc 0.984375, prec 0.0944547, recall 0.824071
2017-12-10T04:48:04.669189: step 6481, loss 0.00254679, acc 1, prec 0.0944547, recall 0.824071
2017-12-10T04:48:04.932696: step 6482, loss 0.0346661, acc 0.96875, prec 0.0944523, recall 0.824071
2017-12-10T04:48:05.207094: step 6483, loss 0.0412235, acc 0.984375, prec 0.0944511, recall 0.824071
2017-12-10T04:48:05.478968: step 6484, loss 0.00892184, acc 1, prec 0.0944511, recall 0.824071
2017-12-10T04:48:05.745322: step 6485, loss 0.000188849, acc 1, prec 0.0944624, recall 0.82409
2017-12-10T04:48:06.005955: step 6486, loss 0.100634, acc 0.984375, prec 0.0944612, recall 0.82409
2017-12-10T04:48:06.275403: step 6487, loss 0.000539525, acc 1, prec 0.0944612, recall 0.82409
2017-12-10T04:48:06.541387: step 6488, loss 0.000160841, acc 1, prec 0.0944612, recall 0.82409
2017-12-10T04:48:06.801277: step 6489, loss 0.529102, acc 1, prec 0.0944837, recall 0.824128
2017-12-10T04:48:07.077753: step 6490, loss 0.0351691, acc 0.984375, prec 0.0944825, recall 0.824128
2017-12-10T04:48:07.346286: step 6491, loss 0.000601145, acc 1, prec 0.0944938, recall 0.824147
2017-12-10T04:48:07.611929: step 6492, loss 0.00204718, acc 1, prec 0.094505, recall 0.824166
2017-12-10T04:48:07.877487: step 6493, loss 0.0559245, acc 0.984375, prec 0.0945151, recall 0.824185
2017-12-10T04:48:08.153263: step 6494, loss 0.0276892, acc 0.984375, prec 0.0945139, recall 0.824185
2017-12-10T04:48:08.419289: step 6495, loss 0.0106197, acc 1, prec 0.0945251, recall 0.824204
2017-12-10T04:48:08.687835: step 6496, loss 0.0644016, acc 0.96875, prec 0.0945565, recall 0.824261
2017-12-10T04:48:08.958051: step 6497, loss 0.00057033, acc 1, prec 0.0945678, recall 0.82428
2017-12-10T04:48:09.225169: step 6498, loss 0.284716, acc 0.96875, prec 0.0945654, recall 0.82428
2017-12-10T04:48:09.498209: step 6499, loss 0.255745, acc 0.984375, prec 0.0945755, recall 0.824299
2017-12-10T04:48:09.768066: step 6500, loss 0.359509, acc 0.953125, prec 0.0945832, recall 0.824318
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-6500

2017-12-10T04:48:11.145988: step 6501, loss 0.00331739, acc 1, prec 0.0945832, recall 0.824318
2017-12-10T04:48:11.418932: step 6502, loss 0.209584, acc 0.96875, prec 0.0945921, recall 0.824337
2017-12-10T04:48:11.691120: step 6503, loss 0.128561, acc 0.984375, prec 0.0946021, recall 0.824356
2017-12-10T04:48:11.964381: step 6504, loss 0.0677599, acc 0.984375, prec 0.0946122, recall 0.824375
2017-12-10T04:48:12.242023: step 6505, loss 0.00129368, acc 1, prec 0.0946234, recall 0.824394
2017-12-10T04:48:12.514997: step 6506, loss 0.000197826, acc 1, prec 0.0946234, recall 0.824394
2017-12-10T04:48:12.774565: step 6507, loss 0.0708759, acc 0.96875, prec 0.0946211, recall 0.824394
2017-12-10T04:48:13.037916: step 6508, loss 0.14327, acc 0.96875, prec 0.0946187, recall 0.824394
2017-12-10T04:48:13.308896: step 6509, loss 0.0965839, acc 0.984375, prec 0.0946288, recall 0.824413
2017-12-10T04:48:13.575551: step 6510, loss 0.0518133, acc 0.984375, prec 0.0946389, recall 0.824432
2017-12-10T04:48:13.844634: step 6511, loss 0.00171936, acc 1, prec 0.0946501, recall 0.824451
2017-12-10T04:48:14.118825: step 6512, loss 0.0751934, acc 0.96875, prec 0.0946478, recall 0.824451
2017-12-10T04:48:14.382588: step 6513, loss 0.036585, acc 0.984375, prec 0.0946803, recall 0.824508
2017-12-10T04:48:14.648541: step 6514, loss 0.231924, acc 0.953125, prec 0.094688, recall 0.824527
2017-12-10T04:48:14.915395: step 6515, loss 0.0226658, acc 0.984375, prec 0.094698, recall 0.824546
2017-12-10T04:48:15.181699: step 6516, loss 0.0101781, acc 1, prec 0.0947093, recall 0.824565
2017-12-10T04:48:15.443865: step 6517, loss 0.00809337, acc 1, prec 0.0947205, recall 0.824584
2017-12-10T04:48:15.710383: step 6518, loss 0.420791, acc 0.984375, prec 0.0947418, recall 0.824622
2017-12-10T04:48:15.980643: step 6519, loss 0.00419044, acc 1, prec 0.0947418, recall 0.824622
2017-12-10T04:48:16.244617: step 6520, loss 0.163329, acc 0.984375, prec 0.0947519, recall 0.824641
2017-12-10T04:48:16.514968: step 6521, loss 0.474753, acc 0.96875, prec 0.0947944, recall 0.824717
2017-12-10T04:48:16.782261: step 6522, loss 0.000814188, acc 1, prec 0.0948281, recall 0.824773
2017-12-10T04:48:17.044508: step 6523, loss 0.0860158, acc 0.984375, prec 0.0948269, recall 0.824773
2017-12-10T04:48:17.315368: step 6524, loss 0.566213, acc 0.953125, prec 0.0948234, recall 0.824773
2017-12-10T04:48:17.580948: step 6525, loss 0.193124, acc 0.96875, prec 0.0948435, recall 0.824811
2017-12-10T04:48:17.870978: step 6526, loss 0.00932991, acc 1, prec 0.0948435, recall 0.824811
2017-12-10T04:48:18.139499: step 6527, loss 0.143662, acc 0.984375, prec 0.0948423, recall 0.824811
2017-12-10T04:48:18.415231: step 6528, loss 0.280306, acc 0.953125, prec 0.0948613, recall 0.824849
2017-12-10T04:48:18.678692: step 6529, loss 0.097673, acc 0.984375, prec 0.0948601, recall 0.824849
2017-12-10T04:48:18.949097: step 6530, loss 0.010299, acc 1, prec 0.0948713, recall 0.824868
2017-12-10T04:48:19.219409: step 6531, loss 0.140957, acc 0.96875, prec 0.094869, recall 0.824868
2017-12-10T04:48:19.483982: step 6532, loss 0.158279, acc 0.953125, prec 0.0948654, recall 0.824868
2017-12-10T04:48:19.752034: step 6533, loss 1.95267, acc 0.9375, prec 0.0948619, recall 0.824779
2017-12-10T04:48:20.023727: step 6534, loss 0.146412, acc 0.953125, prec 0.0948584, recall 0.824779
2017-12-10T04:48:20.288399: step 6535, loss 0.0635889, acc 0.96875, prec 0.094856, recall 0.824779
2017-12-10T04:48:20.556992: step 6536, loss 6.20653, acc 0.984375, prec 0.0948785, recall 0.824728
2017-12-10T04:48:20.831222: step 6537, loss 0.213889, acc 0.953125, prec 0.0948749, recall 0.824728
2017-12-10T04:48:21.097407: step 6538, loss 0.084907, acc 0.96875, prec 0.094895, recall 0.824766
2017-12-10T04:48:21.367245: step 6539, loss 0.0908112, acc 0.96875, prec 0.0948927, recall 0.824766
2017-12-10T04:48:21.637528: step 6540, loss 0.402201, acc 0.90625, prec 0.0948856, recall 0.824766
2017-12-10T04:48:21.901431: step 6541, loss 0.437802, acc 0.90625, prec 0.0948786, recall 0.824766
2017-12-10T04:48:22.173649: step 6542, loss 0.472309, acc 0.890625, prec 0.0948928, recall 0.824803
2017-12-10T04:48:22.446591: step 6543, loss 0.675932, acc 0.890625, prec 0.0949182, recall 0.82486
2017-12-10T04:48:22.713839: step 6544, loss 0.376277, acc 0.953125, prec 0.0949371, recall 0.824898
2017-12-10T04:48:22.990869: step 6545, loss 0.662064, acc 0.875, prec 0.0949389, recall 0.824917
2017-12-10T04:48:23.253207: step 6546, loss 0.988577, acc 0.875, prec 0.0949631, recall 0.824973
2017-12-10T04:48:23.519453: step 6547, loss 0.417082, acc 0.890625, prec 0.0949661, recall 0.824992
2017-12-10T04:48:23.787494: step 6548, loss 0.733686, acc 0.8125, prec 0.0949744, recall 0.82503
2017-12-10T04:48:24.053859: step 6549, loss 0.453602, acc 0.90625, prec 0.0949674, recall 0.82503
2017-12-10T04:48:24.321430: step 6550, loss 0.0779556, acc 0.984375, prec 0.0949662, recall 0.82503
2017-12-10T04:48:24.589246: step 6551, loss 0.571664, acc 0.921875, prec 0.0949827, recall 0.825067
2017-12-10T04:48:24.854082: step 6552, loss 0.208436, acc 0.953125, prec 0.0949904, recall 0.825086
2017-12-10T04:48:25.130112: step 6553, loss 0.234682, acc 0.921875, prec 0.0949845, recall 0.825086
2017-12-10T04:48:25.399598: step 6554, loss 0.0382833, acc 0.984375, prec 0.0949833, recall 0.825086
2017-12-10T04:48:25.667624: step 6555, loss 0.0785012, acc 0.953125, prec 0.094991, recall 0.825105
2017-12-10T04:48:25.937000: step 6556, loss 0.274394, acc 0.9375, prec 0.0950087, recall 0.825143
2017-12-10T04:48:26.207658: step 6557, loss 0.0238778, acc 1, prec 0.0950311, recall 0.82518
2017-12-10T04:48:26.479536: step 6558, loss 0.0128891, acc 1, prec 0.0950423, recall 0.825199
2017-12-10T04:48:26.750088: step 6559, loss 0.111444, acc 0.984375, prec 0.0950412, recall 0.825199
2017-12-10T04:48:27.028549: step 6560, loss 0.0744398, acc 0.984375, prec 0.0950512, recall 0.825218
2017-12-10T04:48:27.300946: step 6561, loss 0.363466, acc 0.953125, prec 0.0950589, recall 0.825236
2017-12-10T04:48:27.566343: step 6562, loss 0.0322259, acc 0.984375, prec 0.0950577, recall 0.825236
2017-12-10T04:48:27.830659: step 6563, loss 0.149453, acc 0.953125, prec 0.0950542, recall 0.825236
2017-12-10T04:48:28.095938: step 6564, loss 0.0134589, acc 1, prec 0.0950654, recall 0.825255
2017-12-10T04:48:28.365100: step 6565, loss 0.00566834, acc 1, prec 0.0950654, recall 0.825255
2017-12-10T04:48:28.631741: step 6566, loss 0.00276387, acc 1, prec 0.0951102, recall 0.82533
2017-12-10T04:48:28.894866: step 6567, loss 0.0578896, acc 0.984375, prec 0.0951314, recall 0.825368
2017-12-10T04:48:29.161003: step 6568, loss 0.147238, acc 0.984375, prec 0.0951302, recall 0.825368
2017-12-10T04:48:29.427070: step 6569, loss 0.00167636, acc 1, prec 0.0951302, recall 0.825368
2017-12-10T04:48:29.701851: step 6570, loss 0.0693245, acc 0.96875, prec 0.0951391, recall 0.825387
2017-12-10T04:48:29.977264: step 6571, loss 0.0061333, acc 1, prec 0.0951503, recall 0.825405
2017-12-10T04:48:30.247641: step 6572, loss 0.0800754, acc 0.96875, prec 0.0951479, recall 0.825405
2017-12-10T04:48:30.516447: step 6573, loss 0.0197256, acc 0.984375, prec 0.0951467, recall 0.825405
2017-12-10T04:48:30.783916: step 6574, loss 0.00353236, acc 1, prec 0.0951467, recall 0.825405
2017-12-10T04:48:31.045633: step 6575, loss 0.173648, acc 0.953125, prec 0.0951544, recall 0.825424
2017-12-10T04:48:31.311819: step 6576, loss 0.0453991, acc 0.96875, prec 0.0951744, recall 0.825462
2017-12-10T04:48:31.577785: step 6577, loss 0.0785954, acc 0.96875, prec 0.0952057, recall 0.825518
2017-12-10T04:48:31.845448: step 6578, loss 0.0597332, acc 0.984375, prec 0.0952157, recall 0.825536
2017-12-10T04:48:32.114430: step 6579, loss 0.114095, acc 0.984375, prec 0.0952257, recall 0.825555
2017-12-10T04:48:32.375361: step 6580, loss 0.312298, acc 0.953125, prec 0.0952222, recall 0.825555
2017-12-10T04:48:32.639564: step 6581, loss 1.01953, acc 1, prec 0.0952334, recall 0.825574
2017-12-10T04:48:32.907275: step 6582, loss 0.129737, acc 0.984375, prec 0.0952322, recall 0.825574
2017-12-10T04:48:33.178603: step 6583, loss 0.524001, acc 0.96875, prec 0.0952522, recall 0.825611
2017-12-10T04:48:33.449749: step 6584, loss 0.0052574, acc 1, prec 0.0952634, recall 0.82563
2017-12-10T04:48:33.720419: step 6585, loss 0.00667468, acc 1, prec 0.0952634, recall 0.82563
2017-12-10T04:48:33.992623: step 6586, loss 0.0109925, acc 1, prec 0.0953082, recall 0.825705
2017-12-10T04:48:34.262841: step 6587, loss 0.0725828, acc 0.984375, prec 0.0953518, recall 0.825779
2017-12-10T04:48:34.532808: step 6588, loss 0.224186, acc 0.96875, prec 0.0953494, recall 0.825779
2017-12-10T04:48:34.801807: step 6589, loss 0.0491244, acc 0.96875, prec 0.0953583, recall 0.825798
2017-12-10T04:48:35.069246: step 6590, loss 0.0974522, acc 0.96875, prec 0.0953783, recall 0.825835
2017-12-10T04:48:35.335175: step 6591, loss 0.386259, acc 0.96875, prec 0.0953871, recall 0.825854
2017-12-10T04:48:35.599552: step 6592, loss 0.122879, acc 0.953125, prec 0.0953948, recall 0.825873
2017-12-10T04:48:35.873522: step 6593, loss 0.344763, acc 0.953125, prec 0.0954024, recall 0.825891
2017-12-10T04:48:36.148571: step 6594, loss 0.0346952, acc 0.984375, prec 0.0954124, recall 0.82591
2017-12-10T04:48:36.417391: step 6595, loss 0.320444, acc 0.953125, prec 0.0954089, recall 0.82591
2017-12-10T04:48:36.690686: step 6596, loss 0.149735, acc 0.96875, prec 0.0954177, recall 0.825929
2017-12-10T04:48:36.956443: step 6597, loss 0.0710211, acc 0.96875, prec 0.0954377, recall 0.825966
2017-12-10T04:48:37.222914: step 6598, loss 0.129647, acc 0.96875, prec 0.0954354, recall 0.825966
2017-12-10T04:48:37.491202: step 6599, loss 0.0967158, acc 0.96875, prec 0.0954442, recall 0.825985
2017-12-10T04:48:37.758258: step 6600, loss 0.129365, acc 0.96875, prec 0.095453, recall 0.826003

Evaluation:
2017-12-10T04:48:45.407111: step 6600, loss 6.77384, acc 0.963578, prec 0.0957212, recall 0.818919

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-6600

2017-12-10T04:48:46.708228: step 6601, loss 0.368997, acc 0.9375, prec 0.0957277, recall 0.818938
2017-12-10T04:48:47.694360: step 6602, loss 0.0492197, acc 0.953125, prec 0.0957575, recall 0.818995
2017-12-10T04:48:48.050598: step 6603, loss 0.216809, acc 0.984375, prec 0.0957563, recall 0.818995
2017-12-10T04:48:48.316823: step 6604, loss 0.187239, acc 0.96875, prec 0.0957651, recall 0.819015
2017-12-10T04:48:49.065919: step 6605, loss 0.0121258, acc 1, prec 0.0957762, recall 0.819034
2017-12-10T04:48:49.860423: step 6606, loss 0.648754, acc 0.984375, prec 0.0957862, recall 0.819053
2017-12-10T04:48:50.568231: step 6607, loss 0.014923, acc 1, prec 0.0958085, recall 0.819091
2017-12-10T04:48:51.405259: step 6608, loss 0.00416841, acc 1, prec 0.0958085, recall 0.819091
2017-12-10T04:48:51.832484: step 6609, loss 0.0582402, acc 0.984375, prec 0.0958407, recall 0.819148
2017-12-10T04:48:52.104381: step 6610, loss 0.151583, acc 0.9375, prec 0.0958693, recall 0.819205
2017-12-10T04:48:52.382807: step 6611, loss 0.0209207, acc 0.984375, prec 0.0958793, recall 0.819224
2017-12-10T04:48:52.658630: step 6612, loss 0.0215308, acc 0.984375, prec 0.0959004, recall 0.819262
2017-12-10T04:48:52.922501: step 6613, loss 0.0643311, acc 0.96875, prec 0.0959091, recall 0.819281
2017-12-10T04:48:53.194496: step 6614, loss 0.0365795, acc 0.984375, prec 0.0959191, recall 0.8193
2017-12-10T04:48:53.462787: step 6615, loss 0.0307927, acc 0.96875, prec 0.095939, recall 0.819338
2017-12-10T04:48:53.729979: step 6616, loss 0.351183, acc 0.9375, prec 0.0959342, recall 0.819338
2017-12-10T04:48:53.998552: step 6617, loss 0.337926, acc 0.921875, prec 0.0959283, recall 0.819338
2017-12-10T04:48:54.266634: step 6618, loss 0.0380726, acc 0.984375, prec 0.0959494, recall 0.819376
2017-12-10T04:48:54.534123: step 6619, loss 0.049019, acc 0.953125, prec 0.0959459, recall 0.819376
2017-12-10T04:48:54.796331: step 6620, loss 1.06368, acc 0.984375, prec 0.0959558, recall 0.819395
2017-12-10T04:48:55.064857: step 6621, loss 0.0368135, acc 0.984375, prec 0.0959546, recall 0.819395
2017-12-10T04:48:55.330273: step 6622, loss 0.636404, acc 0.90625, prec 0.0959698, recall 0.819433
2017-12-10T04:48:55.595925: step 6623, loss 0.216116, acc 0.953125, prec 0.0959662, recall 0.819433
2017-12-10T04:48:55.856253: step 6624, loss 0.0518339, acc 0.984375, prec 0.0959651, recall 0.819433
2017-12-10T04:48:56.126035: step 6625, loss 0.369258, acc 0.90625, prec 0.0959802, recall 0.819471
2017-12-10T04:48:56.388544: step 6626, loss 0.136663, acc 0.984375, prec 0.0959902, recall 0.81949
2017-12-10T04:48:56.662714: step 6627, loss 0.362087, acc 0.921875, prec 0.0959954, recall 0.819509
2017-12-10T04:48:56.930364: step 6628, loss 0.244481, acc 0.96875, prec 0.095993, recall 0.819509
2017-12-10T04:48:57.203769: step 6629, loss 0.11464, acc 0.921875, prec 0.0959982, recall 0.819528
2017-12-10T04:48:57.470401: step 6630, loss 0.851774, acc 0.90625, prec 0.0960245, recall 0.819584
2017-12-10T04:48:57.739299: step 6631, loss 0.0375151, acc 0.984375, prec 0.0960567, recall 0.819641
2017-12-10T04:48:58.007016: step 6632, loss 0.468763, acc 0.9375, prec 0.0960519, recall 0.819641
2017-12-10T04:48:58.275081: step 6633, loss 0.267231, acc 0.921875, prec 0.096046, recall 0.819641
2017-12-10T04:48:58.545834: step 6634, loss 0.0600816, acc 0.984375, prec 0.0960893, recall 0.819717
2017-12-10T04:48:58.816173: step 6635, loss 0.185614, acc 0.9375, prec 0.0960957, recall 0.819736
2017-12-10T04:48:59.083568: step 6636, loss 0.0452041, acc 0.984375, prec 0.0961167, recall 0.819774
2017-12-10T04:48:59.341660: step 6637, loss 0.242041, acc 0.921875, prec 0.0961219, recall 0.819792
2017-12-10T04:48:59.610607: step 6638, loss 0.113079, acc 0.984375, prec 0.0961319, recall 0.819811
2017-12-10T04:48:59.875203: step 6639, loss 0.709501, acc 0.953125, prec 0.0961616, recall 0.819868
2017-12-10T04:49:00.145240: step 6640, loss 0.115702, acc 0.96875, prec 0.0961704, recall 0.819887
2017-12-10T04:49:00.410570: step 6641, loss 0.127169, acc 0.984375, prec 0.0961914, recall 0.819925
2017-12-10T04:49:00.694931: step 6642, loss 0.0284673, acc 0.984375, prec 0.0962236, recall 0.819981
2017-12-10T04:49:00.963348: step 6643, loss 0.226186, acc 0.9375, prec 0.096241, recall 0.820019
2017-12-10T04:49:01.241538: step 6644, loss 0.0465934, acc 0.96875, prec 0.0962387, recall 0.820019
2017-12-10T04:49:01.511665: step 6645, loss 0.0368621, acc 0.984375, prec 0.0962375, recall 0.820019
2017-12-10T04:49:01.775498: step 6646, loss 0.205806, acc 0.9375, prec 0.0962328, recall 0.820019
2017-12-10T04:49:02.039195: step 6647, loss 0.272228, acc 0.953125, prec 0.0962292, recall 0.820019
2017-12-10T04:49:02.304083: step 6648, loss 0.0471697, acc 0.96875, prec 0.0962602, recall 0.820075
2017-12-10T04:49:02.572174: step 6649, loss 0.555034, acc 0.9375, prec 0.0962998, recall 0.820151
2017-12-10T04:49:02.847118: step 6650, loss 0.240023, acc 0.96875, prec 0.0963086, recall 0.820169
2017-12-10T04:49:03.116643: step 6651, loss 0.140763, acc 0.96875, prec 0.0963395, recall 0.820226
2017-12-10T04:49:03.386351: step 6652, loss 0.0857352, acc 0.984375, prec 0.0963383, recall 0.820226
2017-12-10T04:49:03.657541: step 6653, loss 0.0776892, acc 1, prec 0.0963605, recall 0.820264
2017-12-10T04:49:03.929132: step 6654, loss 0.189712, acc 0.9375, prec 0.0963558, recall 0.820264
2017-12-10T04:49:04.204206: step 6655, loss 0.178713, acc 0.96875, prec 0.0963756, recall 0.820301
2017-12-10T04:49:04.471567: step 6656, loss 0.0503187, acc 0.96875, prec 0.0963955, recall 0.820339
2017-12-10T04:49:04.757061: step 6657, loss 0.00790831, acc 1, prec 0.0964066, recall 0.820357
2017-12-10T04:49:05.028120: step 6658, loss 1.31072, acc 0.953125, prec 0.0964042, recall 0.820272
2017-12-10T04:49:05.305641: step 6659, loss 0.0393621, acc 0.984375, prec 0.0964141, recall 0.82029
2017-12-10T04:49:05.571538: step 6660, loss 0.0512277, acc 0.984375, prec 0.0964129, recall 0.82029
2017-12-10T04:49:05.836395: step 6661, loss 0.0300428, acc 0.984375, prec 0.0964228, recall 0.820309
2017-12-10T04:49:06.101140: step 6662, loss 0.062147, acc 0.984375, prec 0.0964216, recall 0.820309
2017-12-10T04:49:06.369052: step 6663, loss 0.138589, acc 0.96875, prec 0.0964304, recall 0.820328
2017-12-10T04:49:06.634821: step 6664, loss 0.437797, acc 0.859375, prec 0.0964197, recall 0.820328
2017-12-10T04:49:06.902723: step 6665, loss 0.0125638, acc 1, prec 0.0964308, recall 0.820347
2017-12-10T04:49:07.168035: step 6666, loss 0.388939, acc 0.953125, prec 0.0964273, recall 0.820347
2017-12-10T04:49:07.429935: step 6667, loss 0.267962, acc 0.953125, prec 0.0964237, recall 0.820347
2017-12-10T04:49:07.705097: step 6668, loss 0.679463, acc 0.890625, prec 0.0964154, recall 0.820347
2017-12-10T04:49:07.970043: step 6669, loss 0.169691, acc 0.96875, prec 0.0964131, recall 0.820347
2017-12-10T04:49:08.235845: step 6670, loss 0.124473, acc 0.953125, prec 0.0964206, recall 0.820366
2017-12-10T04:49:08.504489: step 6671, loss 0.0290003, acc 1, prec 0.0964206, recall 0.820366
2017-12-10T04:49:08.785476: step 6672, loss 0.605544, acc 0.875, prec 0.0964111, recall 0.820366
2017-12-10T04:49:09.053552: step 6673, loss 0.27288, acc 0.953125, prec 0.0964298, recall 0.820403
2017-12-10T04:49:09.325357: step 6674, loss 0.239625, acc 0.96875, prec 0.0964274, recall 0.820403
2017-12-10T04:49:09.593992: step 6675, loss 0.0659739, acc 0.96875, prec 0.096425, recall 0.820403
2017-12-10T04:49:09.860723: step 6676, loss 0.223982, acc 0.96875, prec 0.0964559, recall 0.820459
2017-12-10T04:49:10.122858: step 6677, loss 0.379, acc 0.953125, prec 0.0964745, recall 0.820497
2017-12-10T04:49:10.391319: step 6678, loss 0.0161448, acc 1, prec 0.0964745, recall 0.820497
2017-12-10T04:49:10.660967: step 6679, loss 0.262656, acc 0.953125, prec 0.096471, recall 0.820497
2017-12-10T04:49:10.925766: step 6680, loss 0.00328528, acc 1, prec 0.096471, recall 0.820497
2017-12-10T04:49:11.190936: step 6681, loss 0.00275346, acc 1, prec 0.0964932, recall 0.820534
2017-12-10T04:49:11.455650: step 6682, loss 0.102862, acc 0.96875, prec 0.0965019, recall 0.820553
2017-12-10T04:49:11.722366: step 6683, loss 0.0528187, acc 0.984375, prec 0.0965007, recall 0.820553
2017-12-10T04:49:11.993068: step 6684, loss 0.146723, acc 0.9375, prec 0.096496, recall 0.820553
2017-12-10T04:49:12.267960: step 6685, loss 0.255705, acc 0.96875, prec 0.0964936, recall 0.820553
2017-12-10T04:49:12.532608: step 6686, loss 0.0130023, acc 1, prec 0.0964936, recall 0.820553
2017-12-10T04:49:12.804095: step 6687, loss 0.0462776, acc 0.96875, prec 0.0965023, recall 0.820572
2017-12-10T04:49:13.068053: step 6688, loss 0.0113153, acc 0.984375, prec 0.0965011, recall 0.820572
2017-12-10T04:49:13.337459: step 6689, loss 0.00120898, acc 1, prec 0.0965011, recall 0.820572
2017-12-10T04:49:13.596601: step 6690, loss 0.103263, acc 0.953125, prec 0.0965197, recall 0.820609
2017-12-10T04:49:13.860315: step 6691, loss 0.000680586, acc 1, prec 0.0965308, recall 0.820628
2017-12-10T04:49:14.120778: step 6692, loss 0.06105, acc 0.96875, prec 0.0965285, recall 0.820628
2017-12-10T04:49:14.392668: step 6693, loss 0.0130464, acc 1, prec 0.0965285, recall 0.820628
2017-12-10T04:49:14.658662: step 6694, loss 0.189058, acc 0.984375, prec 0.0965273, recall 0.820628
2017-12-10T04:49:14.922998: step 6695, loss 0.0895037, acc 0.96875, prec 0.096536, recall 0.820647
2017-12-10T04:49:15.194736: step 6696, loss 0.0822893, acc 0.984375, prec 0.0965459, recall 0.820665
2017-12-10T04:49:15.460175: step 6697, loss 0.001935, acc 1, prec 0.0965459, recall 0.820665
2017-12-10T04:49:15.723718: step 6698, loss 0.00243712, acc 1, prec 0.096557, recall 0.820684
2017-12-10T04:49:15.983506: step 6699, loss 0.251523, acc 0.984375, prec 0.0965779, recall 0.820721
2017-12-10T04:49:16.253726: step 6700, loss 0.00977293, acc 1, prec 0.0966001, recall 0.820759
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-6700

2017-12-10T04:49:17.627926: step 6701, loss 0.173249, acc 0.984375, prec 0.0966322, recall 0.820815
2017-12-10T04:49:17.895303: step 6702, loss 0.255728, acc 0.96875, prec 0.096663, recall 0.820871
2017-12-10T04:49:18.160442: step 6703, loss 0.000177981, acc 1, prec 0.096663, recall 0.820871
2017-12-10T04:49:18.427172: step 6704, loss 0.133983, acc 0.984375, prec 0.0966618, recall 0.820871
2017-12-10T04:49:18.702494: step 6705, loss 0.0112898, acc 1, prec 0.0966618, recall 0.820871
2017-12-10T04:49:18.969569: step 6706, loss 0.000103893, acc 1, prec 0.0966618, recall 0.820871
2017-12-10T04:49:19.249861: step 6707, loss 0.000167716, acc 1, prec 0.0966618, recall 0.820871
2017-12-10T04:49:19.516578: step 6708, loss 0.138161, acc 0.96875, prec 0.0966706, recall 0.820889
2017-12-10T04:49:19.791638: step 6709, loss 0.000446019, acc 1, prec 0.0966706, recall 0.820889
2017-12-10T04:49:20.057757: step 6710, loss 0.425359, acc 0.984375, prec 0.0966804, recall 0.820908
2017-12-10T04:49:20.334423: step 6711, loss 0.0031188, acc 1, prec 0.0966804, recall 0.820908
2017-12-10T04:49:20.599977: step 6712, loss 0.272546, acc 1, prec 0.0967026, recall 0.820945
2017-12-10T04:49:20.880409: step 6713, loss 0.222722, acc 0.96875, prec 0.0967113, recall 0.820964
2017-12-10T04:49:21.160528: step 6714, loss 0.0578084, acc 0.96875, prec 0.09672, recall 0.820983
2017-12-10T04:49:21.430244: step 6715, loss 0.12565, acc 0.984375, prec 0.0967188, recall 0.820983
2017-12-10T04:49:21.704180: step 6716, loss 0.0267482, acc 0.984375, prec 0.0967176, recall 0.820983
2017-12-10T04:49:21.968222: step 6717, loss 0.0734743, acc 0.984375, prec 0.0967164, recall 0.820983
2017-12-10T04:49:22.238307: step 6718, loss 0.000788341, acc 1, prec 0.0967275, recall 0.821001
2017-12-10T04:49:22.503243: step 6719, loss 0.194241, acc 0.96875, prec 0.0967362, recall 0.82102
2017-12-10T04:49:22.774368: step 6720, loss 0.00508358, acc 1, prec 0.0967473, recall 0.821038
2017-12-10T04:49:23.040623: step 6721, loss 0.00161241, acc 1, prec 0.0967805, recall 0.821094
2017-12-10T04:49:23.307077: step 6722, loss 0.0047506, acc 1, prec 0.0968027, recall 0.821131
2017-12-10T04:49:23.581005: step 6723, loss 0.0146473, acc 1, prec 0.0968027, recall 0.821131
2017-12-10T04:49:23.860062: step 6724, loss 0.144624, acc 0.984375, prec 0.0968015, recall 0.821131
2017-12-10T04:49:24.128003: step 6725, loss 0.354386, acc 0.96875, prec 0.0968102, recall 0.82115
2017-12-10T04:49:24.400553: step 6726, loss 0.223364, acc 0.96875, prec 0.0968078, recall 0.82115
2017-12-10T04:49:24.671611: step 6727, loss 0.0143056, acc 1, prec 0.0968078, recall 0.82115
2017-12-10T04:49:24.938912: step 6728, loss 0.000281418, acc 1, prec 0.0968078, recall 0.82115
2017-12-10T04:49:25.205491: step 6729, loss 6.83407, acc 0.984375, prec 0.0968078, recall 0.821065
2017-12-10T04:49:25.476335: step 6730, loss 0.00373737, acc 1, prec 0.0968078, recall 0.821065
2017-12-10T04:49:25.742235: step 6731, loss 0.212501, acc 0.96875, prec 0.0968386, recall 0.82112
2017-12-10T04:49:26.010118: step 6732, loss 0.00668953, acc 1, prec 0.0968386, recall 0.82112
2017-12-10T04:49:26.280103: step 6733, loss 0.00485315, acc 1, prec 0.0968386, recall 0.82112
2017-12-10T04:49:26.546857: step 6734, loss 0.476845, acc 0.953125, prec 0.0968462, recall 0.821139
2017-12-10T04:49:26.817568: step 6735, loss 0.00592151, acc 1, prec 0.0968683, recall 0.821176
2017-12-10T04:49:27.094885: step 6736, loss 0.225686, acc 0.953125, prec 0.0968869, recall 0.821213
2017-12-10T04:49:27.360341: step 6737, loss 0.151997, acc 0.9375, prec 0.0969043, recall 0.82125
2017-12-10T04:49:27.633254: step 6738, loss 0.627133, acc 0.890625, prec 0.096907, recall 0.821269
2017-12-10T04:49:27.902435: step 6739, loss 0.479667, acc 0.921875, prec 0.0969121, recall 0.821288
2017-12-10T04:49:28.172957: step 6740, loss 0.283672, acc 0.875, prec 0.0969026, recall 0.821288
2017-12-10T04:49:28.438619: step 6741, loss 0.363719, acc 0.9375, prec 0.0968979, recall 0.821288
2017-12-10T04:49:28.704920: step 6742, loss 0.31468, acc 0.90625, prec 0.0969018, recall 0.821306
2017-12-10T04:49:28.972656: step 6743, loss 0.158431, acc 0.9375, prec 0.0969303, recall 0.821362
2017-12-10T04:49:29.245684: step 6744, loss 0.680041, acc 0.890625, prec 0.096922, recall 0.821362
2017-12-10T04:49:29.512139: step 6745, loss 0.387659, acc 0.921875, prec 0.0969271, recall 0.82138
2017-12-10T04:49:29.779507: step 6746, loss 1.04625, acc 0.890625, prec 0.0969188, recall 0.82138
2017-12-10T04:49:30.044202: step 6747, loss 0.2952, acc 0.921875, prec 0.096935, recall 0.821417
2017-12-10T04:49:30.316293: step 6748, loss 0.300399, acc 0.90625, prec 0.0969389, recall 0.821436
2017-12-10T04:49:30.588922: step 6749, loss 0.0267069, acc 1, prec 0.0969389, recall 0.821436
2017-12-10T04:49:30.856534: step 6750, loss 0.165494, acc 0.953125, prec 0.0969353, recall 0.821436
2017-12-10T04:49:31.121130: step 6751, loss 0.272597, acc 0.90625, prec 0.0969282, recall 0.821436
2017-12-10T04:49:31.387367: step 6752, loss 3.53111, acc 0.96875, prec 0.0969381, recall 0.821369
2017-12-10T04:49:31.655367: step 6753, loss 0.119297, acc 0.984375, prec 0.0969701, recall 0.821425
2017-12-10T04:49:31.919335: step 6754, loss 0.118464, acc 0.96875, prec 0.0969677, recall 0.821425
2017-12-10T04:49:32.193856: step 6755, loss 0.0186554, acc 0.984375, prec 0.0969776, recall 0.821443
2017-12-10T04:49:32.466109: step 6756, loss 0.245097, acc 0.953125, prec 0.0969851, recall 0.821462
2017-12-10T04:49:32.729197: step 6757, loss 0.14561, acc 0.96875, prec 0.0969937, recall 0.82148
2017-12-10T04:49:33.000077: step 6758, loss 0.19052, acc 0.984375, prec 0.0969925, recall 0.82148
2017-12-10T04:49:33.266763: step 6759, loss 0.211663, acc 0.96875, prec 0.0969902, recall 0.82148
2017-12-10T04:49:33.532969: step 6760, loss 0.0147922, acc 0.984375, prec 0.096989, recall 0.82148
2017-12-10T04:49:33.801314: step 6761, loss 0.00670139, acc 1, prec 0.097, recall 0.821499
2017-12-10T04:49:34.069561: step 6762, loss 0.186944, acc 0.953125, prec 0.0969965, recall 0.821499
2017-12-10T04:49:34.332671: step 6763, loss 0.088275, acc 0.984375, prec 0.0969953, recall 0.821499
2017-12-10T04:49:34.602872: step 6764, loss 0.234659, acc 0.953125, prec 0.0970249, recall 0.821554
2017-12-10T04:49:34.868746: step 6765, loss 0.00860101, acc 1, prec 0.0970249, recall 0.821554
2017-12-10T04:49:35.138807: step 6766, loss 0.0362231, acc 0.984375, prec 0.0970347, recall 0.821573
2017-12-10T04:49:35.404701: step 6767, loss 0.169005, acc 0.984375, prec 0.0970446, recall 0.821591
2017-12-10T04:49:35.672075: step 6768, loss 0.0452749, acc 0.984375, prec 0.0970434, recall 0.821591
2017-12-10T04:49:35.941935: step 6769, loss 0.0222909, acc 0.984375, prec 0.0970422, recall 0.821591
2017-12-10T04:49:36.207328: step 6770, loss 0.027372, acc 1, prec 0.0970533, recall 0.82161
2017-12-10T04:49:36.479426: step 6771, loss 0.0527168, acc 0.984375, prec 0.0970521, recall 0.82161
2017-12-10T04:49:36.751798: step 6772, loss 0.0616721, acc 0.984375, prec 0.097062, recall 0.821628
2017-12-10T04:49:37.018459: step 6773, loss 0.0148795, acc 0.984375, prec 0.0970718, recall 0.821647
2017-12-10T04:49:37.288732: step 6774, loss 0.057034, acc 0.984375, prec 0.0970706, recall 0.821647
2017-12-10T04:49:37.555435: step 6775, loss 0.0668745, acc 0.984375, prec 0.0970915, recall 0.821684
2017-12-10T04:49:37.817985: step 6776, loss 0.449983, acc 0.984375, prec 0.0970903, recall 0.821684
2017-12-10T04:49:38.086660: step 6777, loss 0.162884, acc 0.96875, prec 0.097099, recall 0.821702
2017-12-10T04:49:38.353936: step 6778, loss 0.0128168, acc 1, prec 0.0971211, recall 0.821739
2017-12-10T04:49:38.620514: step 6779, loss 6.94211, acc 0.984375, prec 0.0971211, recall 0.821654
2017-12-10T04:49:38.887343: step 6780, loss 0.00242987, acc 1, prec 0.0971211, recall 0.821654
2017-12-10T04:49:39.152265: step 6781, loss 0.0216767, acc 0.984375, prec 0.097142, recall 0.821691
2017-12-10T04:49:39.421283: step 6782, loss 0.0334337, acc 0.96875, prec 0.0971617, recall 0.821728
2017-12-10T04:49:39.693316: step 6783, loss 0.125484, acc 0.953125, prec 0.0971913, recall 0.821783
2017-12-10T04:49:39.972899: step 6784, loss 0.0150792, acc 1, prec 0.0971913, recall 0.821783
2017-12-10T04:49:40.238370: step 6785, loss 0.254849, acc 0.953125, prec 0.0971877, recall 0.821783
2017-12-10T04:49:40.504475: step 6786, loss 0.480243, acc 0.921875, prec 0.0972039, recall 0.82182
2017-12-10T04:49:40.769384: step 6787, loss 0.194648, acc 0.921875, prec 0.0971979, recall 0.82182
2017-12-10T04:49:41.035932: step 6788, loss 0.148379, acc 0.9375, prec 0.0971932, recall 0.82182
2017-12-10T04:49:41.303703: step 6789, loss 0.254654, acc 0.9375, prec 0.0971995, recall 0.821838
2017-12-10T04:49:41.571613: step 6790, loss 0.0836921, acc 0.96875, prec 0.0972192, recall 0.821875
2017-12-10T04:49:41.841979: step 6791, loss 0.665603, acc 0.921875, prec 0.0972243, recall 0.821894
2017-12-10T04:49:42.129141: step 6792, loss 0.112498, acc 0.96875, prec 0.097255, recall 0.821949
2017-12-10T04:49:42.397347: step 6793, loss 0.172703, acc 0.9375, prec 0.0972613, recall 0.821967
2017-12-10T04:49:42.666052: step 6794, loss 0.0842026, acc 0.96875, prec 0.0972589, recall 0.821967
2017-12-10T04:49:42.935382: step 6795, loss 0.250035, acc 0.921875, prec 0.097253, recall 0.821967
2017-12-10T04:49:43.204829: step 6796, loss 0.512022, acc 0.921875, prec 0.097247, recall 0.821967
2017-12-10T04:49:43.466583: step 6797, loss 0.303637, acc 0.9375, prec 0.0972643, recall 0.822004
2017-12-10T04:49:43.744850: step 6798, loss 0.0903554, acc 0.96875, prec 0.097262, recall 0.822004
2017-12-10T04:49:44.023062: step 6799, loss 0.0223864, acc 0.984375, prec 0.0972718, recall 0.822022
2017-12-10T04:49:44.295550: step 6800, loss 0.00392813, acc 1, prec 0.0972718, recall 0.822022
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-6800

2017-12-10T04:49:45.759048: step 6801, loss 0.0991303, acc 0.953125, prec 0.0972682, recall 0.822022
2017-12-10T04:49:46.027721: step 6802, loss 0.331046, acc 0.953125, prec 0.0972867, recall 0.822059
2017-12-10T04:49:46.296044: step 6803, loss 0.312088, acc 0.9375, prec 0.097282, recall 0.822059
2017-12-10T04:49:46.575467: step 6804, loss 0.0510127, acc 0.984375, prec 0.0972808, recall 0.822059
2017-12-10T04:49:46.842931: step 6805, loss 0.18785, acc 0.96875, prec 0.0972894, recall 0.822078
2017-12-10T04:49:47.109063: step 6806, loss 0.32439, acc 0.953125, prec 0.0972969, recall 0.822096
2017-12-10T04:49:47.376428: step 6807, loss 0.0222802, acc 0.984375, prec 0.0972957, recall 0.822096
2017-12-10T04:49:47.645046: step 6808, loss 0.123673, acc 0.984375, prec 0.0972945, recall 0.822096
2017-12-10T04:49:47.913080: step 6809, loss 0.00706658, acc 1, prec 0.0973166, recall 0.822133
2017-12-10T04:49:48.180926: step 6810, loss 0.428762, acc 0.96875, prec 0.0973252, recall 0.822151
2017-12-10T04:49:48.452659: step 6811, loss 0.158496, acc 0.953125, prec 0.0973217, recall 0.822151
2017-12-10T04:49:48.720070: step 6812, loss 0.43571, acc 0.9375, prec 0.0973169, recall 0.822151
2017-12-10T04:49:49.003712: step 6813, loss 0.289786, acc 0.984375, prec 0.0973268, recall 0.822169
2017-12-10T04:49:49.279407: step 6814, loss 0.151229, acc 0.96875, prec 0.0973575, recall 0.822224
2017-12-10T04:49:49.543918: step 6815, loss 0.583179, acc 0.984375, prec 0.0973673, recall 0.822243
2017-12-10T04:49:49.823457: step 6816, loss 0.302861, acc 0.921875, prec 0.0973613, recall 0.822243
2017-12-10T04:49:50.091259: step 6817, loss 0.0591853, acc 0.984375, prec 0.0973822, recall 0.82228
2017-12-10T04:49:50.358965: step 6818, loss 0.0654619, acc 0.984375, prec 0.097381, recall 0.82228
2017-12-10T04:49:50.626647: step 6819, loss 0.299973, acc 0.96875, prec 0.0973786, recall 0.82228
2017-12-10T04:49:50.892137: step 6820, loss 0.125499, acc 1, prec 0.0973897, recall 0.822298
2017-12-10T04:49:51.159317: step 6821, loss 0.363824, acc 0.96875, prec 0.0973983, recall 0.822316
2017-12-10T04:49:51.431277: step 6822, loss 0.152202, acc 0.96875, prec 0.0973959, recall 0.822316
2017-12-10T04:49:51.697462: step 6823, loss 0.131738, acc 0.953125, prec 0.0973924, recall 0.822316
2017-12-10T04:49:51.970189: step 6824, loss 0.0760292, acc 0.953125, prec 0.0973888, recall 0.822316
2017-12-10T04:49:52.240828: step 6825, loss 0.156737, acc 0.96875, prec 0.0973974, recall 0.822335
2017-12-10T04:49:52.507558: step 6826, loss 0.275775, acc 0.9375, prec 0.0973927, recall 0.822335
2017-12-10T04:49:52.772946: step 6827, loss 0.0287801, acc 0.984375, prec 0.0974025, recall 0.822353
2017-12-10T04:49:53.045122: step 6828, loss 0.138741, acc 0.96875, prec 0.0974112, recall 0.822371
2017-12-10T04:49:53.321533: step 6829, loss 0.0580242, acc 0.984375, prec 0.097421, recall 0.822389
2017-12-10T04:49:53.588714: step 6830, loss 0.0121091, acc 1, prec 0.097443, recall 0.822426
2017-12-10T04:49:53.858279: step 6831, loss 0.0198186, acc 0.984375, prec 0.0974529, recall 0.822444
2017-12-10T04:49:54.127678: step 6832, loss 0.239907, acc 0.96875, prec 0.0974725, recall 0.822481
2017-12-10T04:49:54.394014: step 6833, loss 0.0409751, acc 0.984375, prec 0.0974824, recall 0.822499
2017-12-10T04:49:54.658067: step 6834, loss 0.00796132, acc 1, prec 0.0974824, recall 0.822499
2017-12-10T04:49:54.923738: step 6835, loss 0.202266, acc 0.96875, prec 0.097491, recall 0.822518
2017-12-10T04:49:55.187494: step 6836, loss 0.234488, acc 0.953125, prec 0.0974874, recall 0.822518
2017-12-10T04:49:55.455902: step 6837, loss 0.0245096, acc 0.984375, prec 0.0974862, recall 0.822518
2017-12-10T04:49:55.726548: step 6838, loss 0.047482, acc 0.984375, prec 0.0975071, recall 0.822554
2017-12-10T04:49:55.988673: step 6839, loss 0.00448513, acc 1, prec 0.0975401, recall 0.822609
2017-12-10T04:49:56.249130: step 6840, loss 0.10834, acc 0.984375, prec 0.097561, recall 0.822645
2017-12-10T04:49:56.518894: step 6841, loss 0.193504, acc 0.953125, prec 0.0975794, recall 0.822682
2017-12-10T04:49:56.787544: step 6842, loss 0.0958559, acc 0.96875, prec 0.0975771, recall 0.822682
2017-12-10T04:49:57.054191: step 6843, loss 0.0016458, acc 1, prec 0.0975771, recall 0.822682
2017-12-10T04:49:57.319662: step 6844, loss 0.215503, acc 0.96875, prec 0.0975747, recall 0.822682
2017-12-10T04:49:57.586942: step 6845, loss 0.082098, acc 0.984375, prec 0.0975955, recall 0.822718
2017-12-10T04:49:57.863207: step 6846, loss 0.0282011, acc 0.984375, prec 0.0975943, recall 0.822718
2017-12-10T04:49:58.127292: step 6847, loss 8.93008, acc 0.96875, prec 0.0976262, recall 0.822688
2017-12-10T04:49:58.403228: step 6848, loss 0.0529911, acc 0.984375, prec 0.097647, recall 0.822725
2017-12-10T04:49:58.675815: step 6849, loss 0.141452, acc 0.953125, prec 0.0976434, recall 0.822725
2017-12-10T04:49:58.942999: step 6850, loss 0.277389, acc 0.953125, prec 0.0976509, recall 0.822743
2017-12-10T04:49:59.208350: step 6851, loss 0.448304, acc 0.953125, prec 0.0976473, recall 0.822743
2017-12-10T04:49:59.472125: step 6852, loss 0.128524, acc 0.96875, prec 0.0976449, recall 0.822743
2017-12-10T04:49:59.733803: step 6853, loss 0.119729, acc 0.96875, prec 0.0976425, recall 0.822743
2017-12-10T04:50:00.002597: step 6854, loss 0.434239, acc 0.921875, prec 0.0976476, recall 0.822761
2017-12-10T04:50:00.282362: step 6855, loss 0.444765, acc 0.921875, prec 0.0976746, recall 0.822816
2017-12-10T04:50:00.562038: step 6856, loss 0.208569, acc 0.90625, prec 0.0977005, recall 0.822871
2017-12-10T04:50:00.828486: step 6857, loss 0.182624, acc 0.984375, prec 0.0977103, recall 0.822889
2017-12-10T04:50:01.096282: step 6858, loss 0.478855, acc 0.9375, prec 0.0977166, recall 0.822907
2017-12-10T04:50:01.371510: step 6859, loss 0.148305, acc 0.9375, prec 0.0977118, recall 0.822907
2017-12-10T04:50:01.643443: step 6860, loss 0.116106, acc 0.9375, prec 0.097718, recall 0.822925
2017-12-10T04:50:01.912959: step 6861, loss 0.125708, acc 0.96875, prec 0.0977157, recall 0.822925
2017-12-10T04:50:02.179319: step 6862, loss 0.181594, acc 0.953125, prec 0.0977341, recall 0.822962
2017-12-10T04:50:02.448465: step 6863, loss 0.584161, acc 0.890625, prec 0.0977257, recall 0.822962
2017-12-10T04:50:02.719816: step 6864, loss 0.0424156, acc 1, prec 0.0977588, recall 0.823016
2017-12-10T04:50:02.982183: step 6865, loss 0.182993, acc 0.9375, prec 0.097754, recall 0.823016
2017-12-10T04:50:03.255094: step 6866, loss 0.387682, acc 0.90625, prec 0.0977578, recall 0.823034
2017-12-10T04:50:03.526772: step 6867, loss 0.207951, acc 0.90625, prec 0.0977837, recall 0.823089
2017-12-10T04:50:03.789953: step 6868, loss 0.294199, acc 0.921875, prec 0.0977887, recall 0.823107
2017-12-10T04:50:04.059569: step 6869, loss 0.401144, acc 0.9375, prec 0.097795, recall 0.823125
2017-12-10T04:50:04.327514: step 6870, loss 1.57183, acc 0.953125, prec 0.0978244, recall 0.823179
2017-12-10T04:50:04.600101: step 6871, loss 0.326069, acc 0.953125, prec 0.0978208, recall 0.823179
2017-12-10T04:50:04.868681: step 6872, loss 0.329766, acc 0.921875, prec 0.0978148, recall 0.823179
2017-12-10T04:50:05.136234: step 6873, loss 0.175604, acc 0.953125, prec 0.0978113, recall 0.823179
2017-12-10T04:50:05.405311: step 6874, loss 0.47297, acc 0.90625, prec 0.0978151, recall 0.823198
2017-12-10T04:50:05.671614: step 6875, loss 0.158121, acc 0.90625, prec 0.0978189, recall 0.823216
2017-12-10T04:50:05.944422: step 6876, loss 0.154402, acc 0.9375, prec 0.0978142, recall 0.823216
2017-12-10T04:50:06.208021: step 6877, loss 0.611088, acc 0.890625, prec 0.0978168, recall 0.823234
2017-12-10T04:50:06.479652: step 6878, loss 0.548783, acc 0.90625, prec 0.0978207, recall 0.823252
2017-12-10T04:50:06.741555: step 6879, loss 0.151982, acc 0.9375, prec 0.0978269, recall 0.82327
2017-12-10T04:50:07.015819: step 6880, loss 0.266948, acc 0.953125, prec 0.0978233, recall 0.82327
2017-12-10T04:50:07.287251: step 6881, loss 0.280349, acc 0.921875, prec 0.0978173, recall 0.82327
2017-12-10T04:50:07.555900: step 6882, loss 0.356227, acc 0.90625, prec 0.0978212, recall 0.823288
2017-12-10T04:50:07.820143: step 6883, loss 0.224407, acc 0.9375, prec 0.0978164, recall 0.823288
2017-12-10T04:50:08.086820: step 6884, loss 0.389794, acc 0.9375, prec 0.0978117, recall 0.823288
2017-12-10T04:50:08.353940: step 6885, loss 0.169701, acc 0.96875, prec 0.0978312, recall 0.823324
2017-12-10T04:50:08.620242: step 6886, loss 0.0904358, acc 0.96875, prec 0.0978289, recall 0.823324
2017-12-10T04:50:08.888562: step 6887, loss 0.299238, acc 0.953125, prec 0.0978583, recall 0.823379
2017-12-10T04:50:09.163367: step 6888, loss 0.0902094, acc 0.953125, prec 0.0978547, recall 0.823379
2017-12-10T04:50:09.431130: step 6889, loss 0.80971, acc 0.921875, prec 0.0978487, recall 0.823379
2017-12-10T04:50:09.694046: step 6890, loss 0.0842651, acc 0.96875, prec 0.0978463, recall 0.823379
2017-12-10T04:50:09.964663: step 6891, loss 0.224997, acc 0.953125, prec 0.0978428, recall 0.823379
2017-12-10T04:50:10.242009: step 6892, loss 0.300313, acc 0.953125, prec 0.0978392, recall 0.823379
2017-12-10T04:50:10.508707: step 6893, loss 0.107409, acc 0.96875, prec 0.0978478, recall 0.823397
2017-12-10T04:50:10.777942: step 6894, loss 0.201895, acc 0.953125, prec 0.0978442, recall 0.823397
2017-12-10T04:50:11.050117: step 6895, loss 0.359168, acc 0.953125, prec 0.0978516, recall 0.823415
2017-12-10T04:50:11.313240: step 6896, loss 0.0347724, acc 1, prec 0.0978736, recall 0.823451
2017-12-10T04:50:11.580023: step 6897, loss 0.252584, acc 0.953125, prec 0.09787, recall 0.823451
2017-12-10T04:50:11.847123: step 6898, loss 0.0535961, acc 0.984375, prec 0.0978688, recall 0.823451
2017-12-10T04:50:12.123922: step 6899, loss 0.000942717, acc 1, prec 0.0978908, recall 0.823487
2017-12-10T04:50:12.385581: step 6900, loss 0.00423919, acc 1, prec 0.0979018, recall 0.823505

Evaluation:
2017-12-10T04:50:20.034177: step 6900, loss 11.705, acc 0.979053, prec 0.0980873, recall 0.814038

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-6900

2017-12-10T04:50:21.329607: step 6901, loss 0.00717836, acc 1, prec 0.0980983, recall 0.814057
2017-12-10T04:50:21.598683: step 6902, loss 0.0366136, acc 0.984375, prec 0.0980971, recall 0.814057
2017-12-10T04:50:21.872836: step 6903, loss 2.74789, acc 0.984375, prec 0.09813, recall 0.814031
2017-12-10T04:50:22.144295: step 6904, loss 0.288226, acc 1, prec 0.0981519, recall 0.814068
2017-12-10T04:50:22.415365: step 6905, loss 0.247389, acc 0.984375, prec 0.0981836, recall 0.814125
2017-12-10T04:50:22.685248: step 6906, loss 0.00166198, acc 1, prec 0.0982055, recall 0.814162
2017-12-10T04:50:22.956240: step 6907, loss 0.0108925, acc 1, prec 0.0982055, recall 0.814162
2017-12-10T04:50:23.224340: step 6908, loss 0.22914, acc 0.96875, prec 0.098236, recall 0.814218
2017-12-10T04:50:23.490546: step 6909, loss 0.222706, acc 0.953125, prec 0.0982434, recall 0.814237
2017-12-10T04:50:23.765103: step 6910, loss 0.205993, acc 0.890625, prec 0.098235, recall 0.814237
2017-12-10T04:50:24.033718: step 6911, loss 0.46049, acc 0.921875, prec 0.0982509, recall 0.814274
2017-12-10T04:50:24.300147: step 6912, loss 0.228809, acc 0.96875, prec 0.0982486, recall 0.814274
2017-12-10T04:50:24.569892: step 6913, loss 0.146724, acc 0.984375, prec 0.0982583, recall 0.814293
2017-12-10T04:50:24.836192: step 6914, loss 0.42454, acc 0.90625, prec 0.0982512, recall 0.814293
2017-12-10T04:50:25.105026: step 6915, loss 0.354069, acc 0.90625, prec 0.098244, recall 0.814293
2017-12-10T04:50:25.377355: step 6916, loss 0.619863, acc 0.90625, prec 0.0982697, recall 0.814349
2017-12-10T04:50:25.643207: step 6917, loss 0.78452, acc 0.921875, prec 0.0982856, recall 0.814386
2017-12-10T04:50:25.915401: step 6918, loss 0.158232, acc 0.96875, prec 0.0982942, recall 0.814405
2017-12-10T04:50:26.182718: step 6919, loss 0.126806, acc 0.984375, prec 0.098293, recall 0.814405
2017-12-10T04:50:26.449110: step 6920, loss 0.0339558, acc 0.984375, prec 0.0983027, recall 0.814424
2017-12-10T04:50:26.720917: step 6921, loss 0.155901, acc 0.96875, prec 0.0983113, recall 0.814442
2017-12-10T04:50:26.994919: step 6922, loss 0.506548, acc 0.921875, prec 0.0983163, recall 0.814461
2017-12-10T04:50:27.267065: step 6923, loss 0.319031, acc 0.9375, prec 0.0983443, recall 0.814517
2017-12-10T04:50:27.535867: step 6924, loss 0.14239, acc 0.96875, prec 0.0983638, recall 0.814554
2017-12-10T04:50:27.806615: step 6925, loss 0.475781, acc 0.96875, prec 0.0983724, recall 0.814573
2017-12-10T04:50:28.073548: step 6926, loss 0.0749219, acc 0.984375, prec 0.0983821, recall 0.814592
2017-12-10T04:50:28.338819: step 6927, loss 0.0344655, acc 1, prec 0.0983931, recall 0.81461
2017-12-10T04:50:28.611658: step 6928, loss 0.45759, acc 0.921875, prec 0.0983981, recall 0.814629
2017-12-10T04:50:28.880977: step 6929, loss 0.0612386, acc 0.984375, prec 0.0984078, recall 0.814647
2017-12-10T04:50:29.153583: step 6930, loss 0.0100174, acc 1, prec 0.0984078, recall 0.814647
2017-12-10T04:50:29.420594: step 6931, loss 0.141752, acc 1, prec 0.0984297, recall 0.814685
2017-12-10T04:50:29.687670: step 6932, loss 0.39867, acc 0.984375, prec 0.0984613, recall 0.81474
2017-12-10T04:50:29.953171: step 6933, loss 0.137135, acc 0.96875, prec 0.0984589, recall 0.81474
2017-12-10T04:50:30.225555: step 6934, loss 0.100389, acc 0.96875, prec 0.0984675, recall 0.814759
2017-12-10T04:50:30.497684: step 6935, loss 0.107848, acc 0.953125, prec 0.0984639, recall 0.814759
2017-12-10T04:50:30.766067: step 6936, loss 0.137604, acc 0.984375, prec 0.0984627, recall 0.814759
2017-12-10T04:50:31.044258: step 6937, loss 0.0155709, acc 1, prec 0.0984955, recall 0.814815
2017-12-10T04:50:31.314494: step 6938, loss 0.130685, acc 0.96875, prec 0.0985041, recall 0.814833
2017-12-10T04:50:31.586477: step 6939, loss 0.120359, acc 0.953125, prec 0.0985114, recall 0.814852
2017-12-10T04:50:31.853360: step 6940, loss 0.27047, acc 0.953125, prec 0.0985078, recall 0.814852
2017-12-10T04:50:32.125421: step 6941, loss 0.0230763, acc 0.984375, prec 0.0985066, recall 0.814852
2017-12-10T04:50:32.388761: step 6942, loss 0.0953708, acc 0.984375, prec 0.0985164, recall 0.814871
2017-12-10T04:50:32.657337: step 6943, loss 0.194405, acc 0.984375, prec 0.0985371, recall 0.814908
2017-12-10T04:50:32.924936: step 6944, loss 0.890671, acc 0.96875, prec 0.0985675, recall 0.814963
2017-12-10T04:50:33.196203: step 6945, loss 0.123832, acc 0.984375, prec 0.0985663, recall 0.814963
2017-12-10T04:50:33.460884: step 6946, loss 0.00971751, acc 1, prec 0.0985663, recall 0.814963
2017-12-10T04:50:33.728420: step 6947, loss 0.00194457, acc 1, prec 0.0985772, recall 0.814982
2017-12-10T04:50:33.993019: step 6948, loss 0.134197, acc 0.984375, prec 0.0985979, recall 0.815019
2017-12-10T04:50:34.259688: step 6949, loss 0.0226111, acc 0.984375, prec 0.0986076, recall 0.815038
2017-12-10T04:50:34.527414: step 6950, loss 0.166502, acc 0.96875, prec 0.0986052, recall 0.815038
2017-12-10T04:50:34.790428: step 6951, loss 0.128543, acc 0.984375, prec 0.0986259, recall 0.815075
2017-12-10T04:50:35.066957: step 6952, loss 0.126495, acc 0.96875, prec 0.0986344, recall 0.815093
2017-12-10T04:50:35.332281: step 6953, loss 0.277156, acc 0.984375, prec 0.0986442, recall 0.815112
2017-12-10T04:50:35.596359: step 6954, loss 0.115259, acc 0.953125, prec 0.0986515, recall 0.81513
2017-12-10T04:50:35.860928: step 6955, loss 0.18347, acc 0.984375, prec 0.0986722, recall 0.815167
2017-12-10T04:50:36.134796: step 6956, loss 0.136609, acc 0.9375, prec 0.0986783, recall 0.815186
2017-12-10T04:50:36.399187: step 6957, loss 0.355149, acc 0.90625, prec 0.0986821, recall 0.815204
2017-12-10T04:50:36.633738: step 6958, loss 0.0213752, acc 0.980769, prec 0.0986809, recall 0.815204
2017-12-10T04:50:36.910429: step 6959, loss 0.122013, acc 0.953125, prec 0.0986882, recall 0.815223
2017-12-10T04:50:37.176005: step 6960, loss 0.130914, acc 0.984375, prec 0.0987089, recall 0.81526
2017-12-10T04:50:37.445990: step 6961, loss 0.193268, acc 0.953125, prec 0.0987271, recall 0.815297
2017-12-10T04:50:37.713709: step 6962, loss 0.147717, acc 0.953125, prec 0.0987345, recall 0.815315
2017-12-10T04:50:37.985582: step 6963, loss 0.00355156, acc 1, prec 0.0987345, recall 0.815315
2017-12-10T04:50:38.249409: step 6964, loss 0.153378, acc 0.96875, prec 0.0987321, recall 0.815315
2017-12-10T04:50:38.513781: step 6965, loss 0.267606, acc 0.921875, prec 0.0987261, recall 0.815315
2017-12-10T04:50:38.779293: step 6966, loss 0.572412, acc 0.9375, prec 0.0987431, recall 0.815352
2017-12-10T04:50:39.049439: step 6967, loss 0.238955, acc 0.953125, prec 0.0987505, recall 0.815371
2017-12-10T04:50:39.324483: step 6968, loss 0.0579492, acc 0.96875, prec 0.0987918, recall 0.815445
2017-12-10T04:50:39.593798: step 6969, loss 0.117752, acc 0.96875, prec 0.0988331, recall 0.815518
2017-12-10T04:50:39.867442: step 6970, loss 0.00276203, acc 1, prec 0.0988331, recall 0.815518
2017-12-10T04:50:40.134640: step 6971, loss 0.0752671, acc 0.96875, prec 0.0988307, recall 0.815518
2017-12-10T04:50:40.402351: step 6972, loss 0.262585, acc 0.953125, prec 0.098838, recall 0.815537
2017-12-10T04:50:40.674076: step 6973, loss 0.0185395, acc 0.984375, prec 0.0988586, recall 0.815574
2017-12-10T04:50:40.936372: step 6974, loss 0.00634358, acc 1, prec 0.0988914, recall 0.815629
2017-12-10T04:50:41.199002: step 6975, loss 0.264694, acc 0.96875, prec 0.0988999, recall 0.815647
2017-12-10T04:50:41.469151: step 6976, loss 0.308918, acc 0.953125, prec 0.0989181, recall 0.815684
2017-12-10T04:50:41.739975: step 6977, loss 0.0345841, acc 0.984375, prec 0.0989279, recall 0.815703
2017-12-10T04:50:42.005147: step 6978, loss 0.0580783, acc 0.984375, prec 0.0989376, recall 0.815721
2017-12-10T04:50:42.279238: step 6979, loss 0.016464, acc 1, prec 0.0989594, recall 0.815758
2017-12-10T04:50:42.556295: step 6980, loss 0.0811755, acc 0.984375, prec 0.0989691, recall 0.815776
2017-12-10T04:50:42.833494: step 6981, loss 0.0109433, acc 1, prec 0.0990019, recall 0.815831
2017-12-10T04:50:43.110335: step 6982, loss 0.0512948, acc 0.984375, prec 0.0990116, recall 0.81585
2017-12-10T04:50:43.373583: step 6983, loss 0.303954, acc 0.96875, prec 0.099031, recall 0.815887
2017-12-10T04:50:43.638286: step 6984, loss 0.115269, acc 0.984375, prec 0.0990516, recall 0.815923
2017-12-10T04:50:43.917559: step 6985, loss 0.0424185, acc 0.984375, prec 0.0990504, recall 0.815923
2017-12-10T04:50:44.186556: step 6986, loss 0.219778, acc 0.984375, prec 0.0990492, recall 0.815923
2017-12-10T04:50:44.452597: step 6987, loss 0.0678405, acc 0.96875, prec 0.0990687, recall 0.81596
2017-12-10T04:50:44.720394: step 6988, loss 0.00538046, acc 1, prec 0.0990905, recall 0.815997
2017-12-10T04:50:44.985982: step 6989, loss 5.52592, acc 0.984375, prec 0.0990905, recall 0.815915
2017-12-10T04:50:45.256612: step 6990, loss 0.204059, acc 0.9375, prec 0.0990857, recall 0.815915
2017-12-10T04:50:45.523186: step 6991, loss 0.222946, acc 0.984375, prec 0.0990954, recall 0.815934
2017-12-10T04:50:45.796855: step 6992, loss 0.0044912, acc 1, prec 0.0991063, recall 0.815952
2017-12-10T04:50:46.067831: step 6993, loss 0.212982, acc 0.96875, prec 0.0991148, recall 0.81597
2017-12-10T04:50:46.334975: step 6994, loss 0.0136851, acc 1, prec 0.0991257, recall 0.815989
2017-12-10T04:50:46.610178: step 6995, loss 0.10541, acc 0.96875, prec 0.0991342, recall 0.816007
2017-12-10T04:50:46.874035: step 6996, loss 0.494151, acc 0.859375, prec 0.0991234, recall 0.816007
2017-12-10T04:50:47.140689: step 6997, loss 0.460049, acc 0.90625, prec 0.0991162, recall 0.816007
2017-12-10T04:50:47.408374: step 6998, loss 0.365706, acc 0.890625, prec 0.0991187, recall 0.816025
2017-12-10T04:50:47.676931: step 6999, loss 0.357399, acc 0.921875, prec 0.0991127, recall 0.816025
2017-12-10T04:50:47.942671: step 7000, loss 0.364763, acc 0.9375, prec 0.0991079, recall 0.816025
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-7000

2017-12-10T04:50:49.132801: step 7001, loss 0.272751, acc 0.90625, prec 0.0991007, recall 0.816025
2017-12-10T04:50:49.403508: step 7002, loss 0.172056, acc 0.96875, prec 0.0991201, recall 0.816062
2017-12-10T04:50:49.672820: step 7003, loss 0.456478, acc 0.9375, prec 0.0991371, recall 0.816099
2017-12-10T04:50:49.946718: step 7004, loss 0.37041, acc 0.9375, prec 0.0991542, recall 0.816135
2017-12-10T04:50:50.212606: step 7005, loss 0.149007, acc 0.9375, prec 0.0991603, recall 0.816154
2017-12-10T04:50:50.481765: step 7006, loss 0.0672868, acc 0.953125, prec 0.0991785, recall 0.81619
2017-12-10T04:50:50.746363: step 7007, loss 0.203529, acc 0.90625, prec 0.0991822, recall 0.816209
2017-12-10T04:50:51.014966: step 7008, loss 0.082571, acc 0.96875, prec 0.0991907, recall 0.816227
2017-12-10T04:50:51.279738: step 7009, loss 0.243903, acc 0.953125, prec 0.0991979, recall 0.816245
2017-12-10T04:50:51.551529: step 7010, loss 0.171447, acc 0.96875, prec 0.0992064, recall 0.816264
2017-12-10T04:50:51.813609: step 7011, loss 0.613684, acc 0.90625, prec 0.099221, recall 0.8163
2017-12-10T04:50:52.083048: step 7012, loss 0.110662, acc 0.953125, prec 0.0992392, recall 0.816337
2017-12-10T04:50:52.345645: step 7013, loss 0.139083, acc 0.96875, prec 0.0992368, recall 0.816337
2017-12-10T04:50:52.613630: step 7014, loss 0.0585541, acc 0.96875, prec 0.0992344, recall 0.816337
2017-12-10T04:50:52.880260: step 7015, loss 0.0293628, acc 0.984375, prec 0.0992332, recall 0.816337
2017-12-10T04:50:53.151313: step 7016, loss 0.0771665, acc 0.984375, prec 0.0992429, recall 0.816355
2017-12-10T04:50:53.425575: step 7017, loss 0.00434258, acc 1, prec 0.0992429, recall 0.816355
2017-12-10T04:50:53.703805: step 7018, loss 0.0179973, acc 1, prec 0.0992429, recall 0.816355
2017-12-10T04:50:53.972247: step 7019, loss 0.131747, acc 0.96875, prec 0.0992514, recall 0.816373
2017-12-10T04:50:54.238983: step 7020, loss 0.336714, acc 0.96875, prec 0.099249, recall 0.816373
2017-12-10T04:50:54.508653: step 7021, loss 0.164831, acc 0.953125, prec 0.0992563, recall 0.816391
2017-12-10T04:50:54.777461: step 7022, loss 0.000112475, acc 1, prec 0.0992563, recall 0.816391
2017-12-10T04:50:55.046999: step 7023, loss 0.029079, acc 0.984375, prec 0.0992551, recall 0.816391
2017-12-10T04:50:55.309564: step 7024, loss 0.171698, acc 0.96875, prec 0.0992636, recall 0.81641
2017-12-10T04:50:55.574432: step 7025, loss 0.00489704, acc 1, prec 0.0992636, recall 0.81641
2017-12-10T04:50:55.849971: step 7026, loss 0.00668697, acc 1, prec 0.0992745, recall 0.816428
2017-12-10T04:50:56.116430: step 7027, loss 0.000553536, acc 1, prec 0.0992854, recall 0.816446
2017-12-10T04:50:56.385450: step 7028, loss 0.0217387, acc 0.984375, prec 0.0992842, recall 0.816446
2017-12-10T04:50:56.656833: step 7029, loss 0.211077, acc 0.96875, prec 0.0992818, recall 0.816446
2017-12-10T04:50:56.934154: step 7030, loss 0.0589412, acc 0.984375, prec 0.0992915, recall 0.816464
2017-12-10T04:50:57.212188: step 7031, loss 0.272379, acc 1, prec 0.0993024, recall 0.816483
2017-12-10T04:50:57.477647: step 7032, loss 0.147073, acc 0.96875, prec 0.0993217, recall 0.816519
2017-12-10T04:50:57.748038: step 7033, loss 0.0417213, acc 0.984375, prec 0.0993423, recall 0.816556
2017-12-10T04:50:58.014233: step 7034, loss 4.681, acc 0.96875, prec 0.0993738, recall 0.816529
2017-12-10T04:50:58.282103: step 7035, loss 0.0189378, acc 0.984375, prec 0.0993944, recall 0.816566
2017-12-10T04:50:58.549502: step 7036, loss 0.136911, acc 0.953125, prec 0.0993907, recall 0.816566
2017-12-10T04:50:58.823255: step 7037, loss 0.162326, acc 0.96875, prec 0.0993883, recall 0.816566
2017-12-10T04:50:59.088796: step 7038, loss 0.399495, acc 0.953125, prec 0.0993847, recall 0.816566
2017-12-10T04:50:59.355743: step 7039, loss 0.287046, acc 0.953125, prec 0.0993811, recall 0.816566
2017-12-10T04:50:59.625924: step 7040, loss 0.301655, acc 0.890625, prec 0.0993727, recall 0.816566
2017-12-10T04:50:59.893336: step 7041, loss 0.615526, acc 0.875, prec 0.0993631, recall 0.816566
2017-12-10T04:51:00.163211: step 7042, loss 0.361487, acc 0.921875, prec 0.0993789, recall 0.816602
2017-12-10T04:51:00.438184: step 7043, loss 0.451548, acc 0.921875, prec 0.0993838, recall 0.81662
2017-12-10T04:51:00.704129: step 7044, loss 0.321883, acc 0.921875, prec 0.0993778, recall 0.81662
2017-12-10T04:51:00.970207: step 7045, loss 0.341748, acc 0.9375, prec 0.0993838, recall 0.816639
2017-12-10T04:51:01.233055: step 7046, loss 0.317221, acc 0.921875, prec 0.0993778, recall 0.816639
2017-12-10T04:51:01.502019: step 7047, loss 0.19493, acc 0.921875, prec 0.0993827, recall 0.816657
2017-12-10T04:51:01.775988: step 7048, loss 0.143617, acc 0.953125, prec 0.0994009, recall 0.816693
2017-12-10T04:51:02.038078: step 7049, loss 0.259864, acc 0.9375, prec 0.0994069, recall 0.816711
2017-12-10T04:51:02.302400: step 7050, loss 0.322548, acc 0.890625, prec 0.0993985, recall 0.816711
2017-12-10T04:51:02.564250: step 7051, loss 0.381339, acc 0.953125, prec 0.0993949, recall 0.816711
2017-12-10T04:51:02.831651: step 7052, loss 0.310887, acc 0.921875, prec 0.0993889, recall 0.816711
2017-12-10T04:51:03.099536: step 7053, loss 0.195467, acc 0.9375, prec 0.0993841, recall 0.816711
2017-12-10T04:51:03.364808: step 7054, loss 0.37462, acc 0.96875, prec 0.0993817, recall 0.816711
2017-12-10T04:51:03.636413: step 7055, loss 0.537124, acc 0.890625, prec 0.0993842, recall 0.816729
2017-12-10T04:51:03.914852: step 7056, loss 0.330918, acc 0.96875, prec 0.0993818, recall 0.816729
2017-12-10T04:51:04.184360: step 7057, loss 0.329945, acc 0.953125, prec 0.0993891, recall 0.816748
2017-12-10T04:51:04.450009: step 7058, loss 0.118046, acc 0.96875, prec 0.0993976, recall 0.816766
2017-12-10T04:51:04.716234: step 7059, loss 0.0236834, acc 1, prec 0.0994084, recall 0.816784
2017-12-10T04:51:04.983264: step 7060, loss 0.0612919, acc 0.984375, prec 0.0994398, recall 0.816839
2017-12-10T04:51:05.249272: step 7061, loss 0.0350926, acc 0.984375, prec 0.0994386, recall 0.816839
2017-12-10T04:51:05.517972: step 7062, loss 1.74295, acc 0.953125, prec 0.0994471, recall 0.816776
2017-12-10T04:51:05.799028: step 7063, loss 0.280075, acc 0.96875, prec 0.0994447, recall 0.816776
2017-12-10T04:51:06.072353: step 7064, loss 0.180978, acc 0.96875, prec 0.0994532, recall 0.816794
2017-12-10T04:51:06.339716: step 7065, loss 0.457755, acc 0.96875, prec 0.0994508, recall 0.816794
2017-12-10T04:51:06.606183: step 7066, loss 0.168365, acc 0.9375, prec 0.0994568, recall 0.816812
2017-12-10T04:51:06.873713: step 7067, loss 0.349836, acc 0.890625, prec 0.0994484, recall 0.816812
2017-12-10T04:51:07.149633: step 7068, loss 0.114539, acc 0.984375, prec 0.099469, recall 0.816848
2017-12-10T04:51:07.412303: step 7069, loss 0.461533, acc 0.921875, prec 0.0994738, recall 0.816867
2017-12-10T04:51:07.672864: step 7070, loss 0.246577, acc 0.953125, prec 0.0994703, recall 0.816867
2017-12-10T04:51:07.941956: step 7071, loss 0.214477, acc 0.984375, prec 0.0994691, recall 0.816867
2017-12-10T04:51:08.205876: step 7072, loss 0.283608, acc 0.953125, prec 0.0994654, recall 0.816867
2017-12-10T04:51:08.468681: step 7073, loss 0.281783, acc 0.984375, prec 0.0994751, recall 0.816885
2017-12-10T04:51:08.730235: step 7074, loss 0.0999269, acc 0.96875, prec 0.0994727, recall 0.816885
2017-12-10T04:51:08.995822: step 7075, loss 0.453379, acc 0.953125, prec 0.0994691, recall 0.816885
2017-12-10T04:51:09.263721: step 7076, loss 0.183508, acc 0.953125, prec 0.0994764, recall 0.816903
2017-12-10T04:51:09.527049: step 7077, loss 0.24345, acc 0.953125, prec 0.0994728, recall 0.816903
2017-12-10T04:51:09.798199: step 7078, loss 0.0423328, acc 0.984375, prec 0.0994933, recall 0.816939
2017-12-10T04:51:10.064878: step 7079, loss 0.12452, acc 0.953125, prec 0.0995006, recall 0.816957
2017-12-10T04:51:10.327986: step 7080, loss 0.0497172, acc 0.984375, prec 0.0994994, recall 0.816957
2017-12-10T04:51:10.605177: step 7081, loss 0.397722, acc 0.953125, prec 0.0994958, recall 0.816957
2017-12-10T04:51:10.876448: step 7082, loss 0.0815331, acc 0.984375, prec 0.0995054, recall 0.816975
2017-12-10T04:51:11.143002: step 7083, loss 0.131725, acc 0.984375, prec 0.0995151, recall 0.816993
2017-12-10T04:51:11.417954: step 7084, loss 0.00132426, acc 1, prec 0.0995368, recall 0.81703
2017-12-10T04:51:11.682157: step 7085, loss 0.112061, acc 0.984375, prec 0.0995356, recall 0.81703
2017-12-10T04:51:11.950782: step 7086, loss 0.731687, acc 0.953125, prec 0.099532, recall 0.81703
2017-12-10T04:51:12.229882: step 7087, loss 0.0915159, acc 0.96875, prec 0.0995296, recall 0.81703
2017-12-10T04:51:12.497023: step 7088, loss 0.246529, acc 0.953125, prec 0.099526, recall 0.81703
2017-12-10T04:51:12.765564: step 7089, loss 0.000427986, acc 1, prec 0.099526, recall 0.81703
2017-12-10T04:51:13.037005: step 7090, loss 0.00294482, acc 1, prec 0.0995477, recall 0.817066
2017-12-10T04:51:13.303186: step 7091, loss 0.00297056, acc 1, prec 0.0995477, recall 0.817066
2017-12-10T04:51:13.565878: step 7092, loss 0.00263269, acc 1, prec 0.0995803, recall 0.81712
2017-12-10T04:51:13.828651: step 7093, loss 0.0820133, acc 0.984375, prec 0.0995791, recall 0.81712
2017-12-10T04:51:14.095289: step 7094, loss 0.0561022, acc 0.96875, prec 0.0995767, recall 0.81712
2017-12-10T04:51:14.364495: step 7095, loss 0.0118807, acc 1, prec 0.0996201, recall 0.817193
2017-12-10T04:51:14.631921: step 7096, loss 0.278671, acc 0.984375, prec 0.0996298, recall 0.817211
2017-12-10T04:51:14.903724: step 7097, loss 0.0106115, acc 1, prec 0.0996515, recall 0.817247
2017-12-10T04:51:15.174723: step 7098, loss 0.133975, acc 0.96875, prec 0.0996491, recall 0.817247
2017-12-10T04:51:15.439613: step 7099, loss 0.254173, acc 0.953125, prec 0.0996564, recall 0.817265
2017-12-10T04:51:15.706435: step 7100, loss 0.0922078, acc 1, prec 0.0996781, recall 0.817301
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-7100

2017-12-10T04:51:16.906462: step 7101, loss 0.072495, acc 0.953125, prec 0.0996853, recall 0.817319
2017-12-10T04:51:17.168665: step 7102, loss 0.0711258, acc 0.984375, prec 0.099695, recall 0.817337
2017-12-10T04:51:17.434110: step 7103, loss 0.0684678, acc 0.96875, prec 0.0996926, recall 0.817337
2017-12-10T04:51:17.698231: step 7104, loss 0.0165848, acc 0.984375, prec 0.0996914, recall 0.817337
2017-12-10T04:51:17.966153: step 7105, loss 0.239325, acc 0.96875, prec 0.0996998, recall 0.817355
2017-12-10T04:51:18.231310: step 7106, loss 0.517713, acc 0.96875, prec 0.0997191, recall 0.817391
2017-12-10T04:51:18.506984: step 7107, loss 0.0708361, acc 0.96875, prec 0.0997384, recall 0.817427
2017-12-10T04:51:18.772097: step 7108, loss 0.30514, acc 0.953125, prec 0.0997348, recall 0.817427
2017-12-10T04:51:19.044634: step 7109, loss 0.00116702, acc 1, prec 0.0997348, recall 0.817427
2017-12-10T04:51:19.312624: step 7110, loss 0.195573, acc 0.96875, prec 0.0997433, recall 0.817445
2017-12-10T04:51:19.586382: step 7111, loss 0.047043, acc 0.96875, prec 0.0997409, recall 0.817445
2017-12-10T04:51:19.852829: step 7112, loss 0.00412929, acc 1, prec 0.0997409, recall 0.817445
2017-12-10T04:51:20.121024: step 7113, loss 0.15917, acc 0.96875, prec 0.0997493, recall 0.817463
2017-12-10T04:51:20.390217: step 7114, loss 0.0017948, acc 1, prec 0.0997493, recall 0.817463
2017-12-10T04:51:20.654670: step 7115, loss 0.0269057, acc 0.984375, prec 0.0997589, recall 0.817481
2017-12-10T04:51:20.924578: step 7116, loss 0.256508, acc 0.96875, prec 0.0997565, recall 0.817481
2017-12-10T04:51:21.194229: step 7117, loss 0.729583, acc 0.9375, prec 0.0997843, recall 0.817536
2017-12-10T04:51:21.467281: step 7118, loss 5.96252, acc 0.984375, prec 0.0998168, recall 0.817509
2017-12-10T04:51:21.737705: step 7119, loss 0.0650658, acc 0.984375, prec 0.0998156, recall 0.817509
2017-12-10T04:51:22.020274: step 7120, loss 0.0287974, acc 0.984375, prec 0.0998144, recall 0.817509
2017-12-10T04:51:22.289821: step 7121, loss 0.13732, acc 0.96875, prec 0.0998229, recall 0.817527
2017-12-10T04:51:22.553338: step 7122, loss 0.0229291, acc 1, prec 0.0998771, recall 0.817617
2017-12-10T04:51:22.824114: step 7123, loss 0.244581, acc 0.96875, prec 0.0998747, recall 0.817617
2017-12-10T04:51:23.089660: step 7124, loss 0.304694, acc 0.953125, prec 0.0998711, recall 0.817617
2017-12-10T04:51:23.351138: step 7125, loss 0.263497, acc 0.953125, prec 0.0998892, recall 0.817653
2017-12-10T04:51:23.618836: step 7126, loss 0.0646287, acc 0.96875, prec 0.0998868, recall 0.817653
2017-12-10T04:51:23.888876: step 7127, loss 0.291482, acc 0.953125, prec 0.099894, recall 0.817671
2017-12-10T04:51:24.153354: step 7128, loss 0.133975, acc 0.96875, prec 0.0998916, recall 0.817671
2017-12-10T04:51:24.419105: step 7129, loss 0.335852, acc 0.921875, prec 0.0998964, recall 0.817689
2017-12-10T04:51:24.692354: step 7130, loss 0.401587, acc 0.9375, prec 0.0999024, recall 0.817707
2017-12-10T04:51:24.962082: step 7131, loss 0.31907, acc 0.90625, prec 0.0999061, recall 0.817725
2017-12-10T04:51:25.225460: step 7132, loss 0.768388, acc 0.890625, prec 0.0998976, recall 0.817725
2017-12-10T04:51:25.497944: step 7133, loss 0.606185, acc 0.953125, prec 0.099894, recall 0.817725
2017-12-10T04:51:25.766928: step 7134, loss 0.67313, acc 0.890625, prec 0.0998856, recall 0.817725
2017-12-10T04:51:26.041861: step 7135, loss 0.270411, acc 0.921875, prec 0.0998796, recall 0.817725
2017-12-10T04:51:26.306846: step 7136, loss 0.361495, acc 0.921875, prec 0.0998736, recall 0.817725
2017-12-10T04:51:26.573669: step 7137, loss 0.682509, acc 0.890625, prec 0.0998652, recall 0.817725
2017-12-10T04:51:26.842632: step 7138, loss 0.00513559, acc 1, prec 0.0998977, recall 0.817779
2017-12-10T04:51:27.113689: step 7139, loss 0.0287497, acc 0.984375, prec 0.0999181, recall 0.817815
2017-12-10T04:51:27.378702: step 7140, loss 0.00612787, acc 1, prec 0.0999181, recall 0.817815
2017-12-10T04:51:27.645373: step 7141, loss 0.229854, acc 0.984375, prec 0.0999386, recall 0.81785
2017-12-10T04:51:27.919746: step 7142, loss 0.302216, acc 0.921875, prec 0.0999543, recall 0.817886
2017-12-10T04:51:28.187295: step 7143, loss 0.0287692, acc 0.984375, prec 0.0999639, recall 0.817904
2017-12-10T04:51:28.451392: step 7144, loss 0.231069, acc 0.9375, prec 0.0999591, recall 0.817904
2017-12-10T04:51:28.716421: step 7145, loss 0.0755154, acc 0.984375, prec 0.0999687, recall 0.817922
2017-12-10T04:51:28.981548: step 7146, loss 0.000418367, acc 1, prec 0.0999795, recall 0.81794
2017-12-10T04:51:29.247933: step 7147, loss 0.0658321, acc 0.96875, prec 0.0999988, recall 0.817976
2017-12-10T04:51:29.521619: step 7148, loss 0.25732, acc 0.96875, prec 0.100018, recall 0.818012
2017-12-10T04:51:29.796348: step 7149, loss 0.111045, acc 0.984375, prec 0.100082, recall 0.818119
2017-12-10T04:51:30.062282: step 7150, loss 0.175525, acc 0.984375, prec 0.100081, recall 0.818119
2017-12-10T04:51:30.327791: step 7151, loss 0.475156, acc 0.984375, prec 0.100112, recall 0.818173
2017-12-10T04:51:30.595870: step 7152, loss 0.0761386, acc 0.96875, prec 0.10012, recall 0.818191
2017-12-10T04:51:30.865131: step 7153, loss 0.322958, acc 0.984375, prec 0.10013, recall 0.818209
2017-12-10T04:51:31.133056: step 7154, loss 0.150951, acc 1, prec 0.100141, recall 0.818227
2017-12-10T04:51:31.413454: step 7155, loss 0.262738, acc 0.953125, prec 0.100137, recall 0.818227
2017-12-10T04:51:31.681992: step 7156, loss 0.0226001, acc 0.984375, prec 0.100158, recall 0.818262
2017-12-10T04:51:31.949814: step 7157, loss 0.0812371, acc 0.984375, prec 0.100156, recall 0.818262
2017-12-10T04:51:32.215969: step 7158, loss 0.00189955, acc 1, prec 0.100167, recall 0.81828
2017-12-10T04:51:32.481915: step 7159, loss 0.00975527, acc 1, prec 0.100178, recall 0.818298
2017-12-10T04:51:32.751156: step 7160, loss 0.185588, acc 0.984375, prec 0.100188, recall 0.818316
2017-12-10T04:51:33.022094: step 7161, loss 0.625238, acc 0.953125, prec 0.100195, recall 0.818334
2017-12-10T04:51:33.295075: step 7162, loss 0.0122135, acc 1, prec 0.100217, recall 0.818369
2017-12-10T04:51:33.559779: step 7163, loss 0.0763765, acc 0.984375, prec 0.100215, recall 0.818369
2017-12-10T04:51:33.826769: step 7164, loss 0.116702, acc 0.96875, prec 0.100245, recall 0.818423
2017-12-10T04:51:34.095507: step 7165, loss 0.127975, acc 0.96875, prec 0.100243, recall 0.818423
2017-12-10T04:51:34.361481: step 7166, loss 0.0142743, acc 1, prec 0.100243, recall 0.818423
2017-12-10T04:51:34.640947: step 7167, loss 0.195055, acc 0.984375, prec 0.100253, recall 0.818441
2017-12-10T04:51:34.907293: step 7168, loss 0.0325137, acc 1, prec 0.100285, recall 0.818494
2017-12-10T04:51:35.175457: step 7169, loss 0.0719969, acc 0.984375, prec 0.100284, recall 0.818494
2017-12-10T04:51:35.441783: step 7170, loss 1.32517e-05, acc 1, prec 0.100295, recall 0.818512
2017-12-10T04:51:35.702584: step 7171, loss 0.130996, acc 0.984375, prec 0.100293, recall 0.818512
2017-12-10T04:51:35.968075: step 7172, loss 0.00130105, acc 1, prec 0.100293, recall 0.818512
2017-12-10T04:51:36.239088: step 7173, loss 0.0962026, acc 1, prec 0.100304, recall 0.81853
2017-12-10T04:51:36.506783: step 7174, loss 0.00367049, acc 1, prec 0.100315, recall 0.818548
2017-12-10T04:51:36.783707: step 7175, loss 0.000913744, acc 1, prec 0.100315, recall 0.818548
2017-12-10T04:51:37.048600: step 7176, loss 0.212143, acc 0.96875, prec 0.100324, recall 0.818565
2017-12-10T04:51:37.318547: step 7177, loss 0.540811, acc 0.984375, prec 0.100333, recall 0.818583
2017-12-10T04:51:37.588891: step 7178, loss 0.0743702, acc 0.9375, prec 0.100328, recall 0.818583
2017-12-10T04:51:37.858255: step 7179, loss 0.000199176, acc 1, prec 0.100339, recall 0.818601
2017-12-10T04:51:38.129913: step 7180, loss 0.22311, acc 0.953125, prec 0.100346, recall 0.818619
2017-12-10T04:51:38.402098: step 7181, loss 0.151423, acc 0.96875, prec 0.100344, recall 0.818619
2017-12-10T04:51:38.668911: step 7182, loss 0.00664647, acc 1, prec 0.100355, recall 0.818637
2017-12-10T04:51:38.940801: step 7183, loss 0.176837, acc 0.984375, prec 0.100364, recall 0.818654
2017-12-10T04:51:39.215061: step 7184, loss 0.0689435, acc 0.984375, prec 0.100374, recall 0.818672
2017-12-10T04:51:39.485449: step 7185, loss 0.000358423, acc 1, prec 0.100374, recall 0.818672
2017-12-10T04:51:39.748666: step 7186, loss 0.000401423, acc 1, prec 0.100374, recall 0.818672
2017-12-10T04:51:40.009204: step 7187, loss 10.0234, acc 0.984375, prec 0.100374, recall 0.818592
2017-12-10T04:51:40.282219: step 7188, loss 0.0365169, acc 0.984375, prec 0.100384, recall 0.81861
2017-12-10T04:51:40.550158: step 7189, loss 0.186326, acc 0.921875, prec 0.100388, recall 0.818627
2017-12-10T04:51:40.819585: step 7190, loss 0.0917113, acc 0.96875, prec 0.100397, recall 0.818645
2017-12-10T04:51:41.091415: step 7191, loss 0.183966, acc 0.953125, prec 0.100415, recall 0.818681
2017-12-10T04:51:41.360375: step 7192, loss 0.309782, acc 0.953125, prec 0.100422, recall 0.818699
2017-12-10T04:51:41.631641: step 7193, loss 0.518885, acc 0.921875, prec 0.100427, recall 0.818716
2017-12-10T04:51:41.901294: step 7194, loss 0.0956645, acc 0.953125, prec 0.100423, recall 0.818716
2017-12-10T04:51:42.177642: step 7195, loss 0.591221, acc 0.921875, prec 0.100428, recall 0.818734
2017-12-10T04:51:42.455710: step 7196, loss 0.301866, acc 0.9375, prec 0.100434, recall 0.818752
2017-12-10T04:51:42.726755: step 7197, loss 0.300078, acc 0.921875, prec 0.100449, recall 0.818787
2017-12-10T04:51:43.000330: step 7198, loss 0.166822, acc 0.96875, prec 0.100469, recall 0.818823
2017-12-10T04:51:43.272264: step 7199, loss 0.18183, acc 0.953125, prec 0.100476, recall 0.818841
2017-12-10T04:51:43.538106: step 7200, loss 0.331095, acc 0.90625, prec 0.10049, recall 0.818876

Evaluation:
2017-12-10T04:51:51.146672: step 7200, loss 3.8356, acc 0.908001, prec 0.100376, recall 0.816059

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-7200

2017-12-10T04:51:52.626275: step 7201, loss 1.59643, acc 0.875, prec 0.100367, recall 0.816059
2017-12-10T04:51:52.901016: step 7202, loss 0.274376, acc 0.921875, prec 0.100371, recall 0.816076
2017-12-10T04:51:53.169314: step 7203, loss 0.374084, acc 0.953125, prec 0.100389, recall 0.816112
2017-12-10T04:51:53.442591: step 7204, loss 0.0658121, acc 0.96875, prec 0.100387, recall 0.816112
2017-12-10T04:51:53.709318: step 7205, loss 0.149804, acc 0.96875, prec 0.100384, recall 0.816112
2017-12-10T04:51:53.980278: step 7206, loss 0.550752, acc 0.9375, prec 0.10038, recall 0.816112
2017-12-10T04:51:54.251760: step 7207, loss 0.86574, acc 0.953125, prec 0.100376, recall 0.816112
2017-12-10T04:51:54.518182: step 7208, loss 0.122699, acc 0.984375, prec 0.100386, recall 0.81613
2017-12-10T04:51:54.781697: step 7209, loss 0.325645, acc 0.9375, prec 0.100392, recall 0.816147
2017-12-10T04:51:55.049360: step 7210, loss 0.355566, acc 0.9375, prec 0.100387, recall 0.816147
2017-12-10T04:51:55.314708: step 7211, loss 0.0218366, acc 1, prec 0.100419, recall 0.816201
2017-12-10T04:51:55.581358: step 7212, loss 0.250404, acc 0.90625, prec 0.100422, recall 0.816218
2017-12-10T04:51:55.854896: step 7213, loss 0.239786, acc 0.96875, prec 0.100431, recall 0.816236
2017-12-10T04:51:56.131351: step 7214, loss 0.258704, acc 0.953125, prec 0.100438, recall 0.816254
2017-12-10T04:51:56.405679: step 7215, loss 0.248429, acc 0.9375, prec 0.100433, recall 0.816254
2017-12-10T04:51:56.675561: step 7216, loss 0.0659221, acc 0.984375, prec 0.100442, recall 0.816271
2017-12-10T04:51:56.945216: step 7217, loss 0.100563, acc 0.984375, prec 0.100441, recall 0.816271
2017-12-10T04:51:57.218027: step 7218, loss 0.604414, acc 0.9375, prec 0.100447, recall 0.816289
2017-12-10T04:51:57.481725: step 7219, loss 0.0754831, acc 0.984375, prec 0.100446, recall 0.816289
2017-12-10T04:51:57.745968: step 7220, loss 0.235972, acc 0.953125, prec 0.100442, recall 0.816289
2017-12-10T04:51:58.012611: step 7221, loss 0.0944094, acc 0.953125, prec 0.100449, recall 0.816307
2017-12-10T04:51:58.277409: step 7222, loss 0.111176, acc 0.953125, prec 0.100457, recall 0.816325
2017-12-10T04:51:58.546951: step 7223, loss 0.21502, acc 0.96875, prec 0.100465, recall 0.816342
2017-12-10T04:51:58.816128: step 7224, loss 0.487551, acc 0.984375, prec 0.100496, recall 0.816395
2017-12-10T04:51:59.084788: step 7225, loss 0.0305233, acc 0.96875, prec 0.100504, recall 0.816413
2017-12-10T04:51:59.354237: step 7226, loss 0.000501865, acc 1, prec 0.100504, recall 0.816413
2017-12-10T04:51:59.622292: step 7227, loss 0.0505942, acc 0.984375, prec 0.100513, recall 0.816431
2017-12-10T04:51:59.890953: step 7228, loss 0.00899463, acc 1, prec 0.100513, recall 0.816431
2017-12-10T04:52:00.154865: step 7229, loss 0.134194, acc 0.984375, prec 0.100523, recall 0.816448
2017-12-10T04:52:00.433430: step 7230, loss 0.0146377, acc 1, prec 0.100523, recall 0.816448
2017-12-10T04:52:00.705783: step 7231, loss 0.427517, acc 0.9375, prec 0.100539, recall 0.816484
2017-12-10T04:52:00.975200: step 7232, loss 0.0124134, acc 1, prec 0.10055, recall 0.816501
2017-12-10T04:52:01.247052: step 7233, loss 0.160374, acc 0.984375, prec 0.10057, recall 0.816537
2017-12-10T04:52:01.516398: step 7234, loss 0.0689103, acc 0.984375, prec 0.10059, recall 0.816572
2017-12-10T04:52:01.780263: step 7235, loss 0.488832, acc 0.9375, prec 0.100586, recall 0.816572
2017-12-10T04:52:02.052857: step 7236, loss 0.000112656, acc 1, prec 0.100596, recall 0.81659
2017-12-10T04:52:02.323620: step 7237, loss 0.000698476, acc 1, prec 0.100607, recall 0.816607
2017-12-10T04:52:02.587934: step 7238, loss 0.005267, acc 1, prec 0.100618, recall 0.816625
2017-12-10T04:52:02.863004: step 7239, loss 0.085068, acc 0.984375, prec 0.100627, recall 0.816643
2017-12-10T04:52:03.131312: step 7240, loss 0.206162, acc 0.96875, prec 0.100646, recall 0.816678
2017-12-10T04:52:03.397331: step 7241, loss 0.00133167, acc 1, prec 0.100667, recall 0.816713
2017-12-10T04:52:03.664198: step 7242, loss 0.0685395, acc 0.984375, prec 0.100666, recall 0.816713
2017-12-10T04:52:03.938185: step 7243, loss 0.0271928, acc 0.984375, prec 0.100665, recall 0.816713
2017-12-10T04:52:04.214395: step 7244, loss 0.0594424, acc 0.984375, prec 0.100674, recall 0.816731
2017-12-10T04:52:04.482616: step 7245, loss 0.00542448, acc 1, prec 0.100685, recall 0.816748
2017-12-10T04:52:04.749133: step 7246, loss 0.238081, acc 0.96875, prec 0.100693, recall 0.816766
2017-12-10T04:52:05.017439: step 7247, loss 0.0244861, acc 0.984375, prec 0.100692, recall 0.816766
2017-12-10T04:52:05.288656: step 7248, loss 0.00100415, acc 1, prec 0.100703, recall 0.816784
2017-12-10T04:52:05.555288: step 7249, loss 0.0240769, acc 1, prec 0.100703, recall 0.816784
2017-12-10T04:52:05.822824: step 7250, loss 0.000407248, acc 1, prec 0.100703, recall 0.816784
2017-12-10T04:52:06.083382: step 7251, loss 0.0216755, acc 0.984375, prec 0.100702, recall 0.816784
2017-12-10T04:52:06.347377: step 7252, loss 0.076138, acc 0.96875, prec 0.100721, recall 0.816819
2017-12-10T04:52:06.611068: step 7253, loss 0.197407, acc 0.96875, prec 0.100718, recall 0.816819
2017-12-10T04:52:06.880555: step 7254, loss 0.0283203, acc 0.984375, prec 0.100738, recall 0.816854
2017-12-10T04:52:07.142978: step 7255, loss 0.0470997, acc 1, prec 0.10076, recall 0.816889
2017-12-10T04:52:07.415824: step 7256, loss 0.499558, acc 0.96875, prec 0.1008, recall 0.81696
2017-12-10T04:52:07.685378: step 7257, loss 0.101827, acc 0.96875, prec 0.100797, recall 0.81696
2017-12-10T04:52:07.952434: step 7258, loss 0.0405581, acc 0.984375, prec 0.100796, recall 0.81696
2017-12-10T04:52:08.215340: step 7259, loss 1.89464, acc 0.953125, prec 0.100847, recall 0.816969
2017-12-10T04:52:08.489528: step 7260, loss 0.000277388, acc 1, prec 0.100847, recall 0.816969
2017-12-10T04:52:08.752620: step 7261, loss 0.437174, acc 0.96875, prec 0.100855, recall 0.816987
2017-12-10T04:52:09.021418: step 7262, loss 0.532058, acc 0.953125, prec 0.100862, recall 0.817004
2017-12-10T04:52:09.290030: step 7263, loss 0.36872, acc 0.9375, prec 0.100858, recall 0.817004
2017-12-10T04:52:09.552552: step 7264, loss 0.125676, acc 0.984375, prec 0.100867, recall 0.817022
2017-12-10T04:52:09.819578: step 7265, loss 0.148314, acc 0.953125, prec 0.100874, recall 0.817039
2017-12-10T04:52:10.081886: step 7266, loss 0.31033, acc 0.921875, prec 0.100879, recall 0.817057
2017-12-10T04:52:10.345052: step 7267, loss 0.619219, acc 0.890625, prec 0.100881, recall 0.817074
2017-12-10T04:52:10.609262: step 7268, loss 0.297601, acc 0.90625, prec 0.100874, recall 0.817074
2017-12-10T04:52:10.877999: step 7269, loss 0.261133, acc 0.90625, prec 0.100877, recall 0.817092
2017-12-10T04:52:11.147768: step 7270, loss 0.382003, acc 0.890625, prec 0.100869, recall 0.817092
2017-12-10T04:52:11.413093: step 7271, loss 0.185383, acc 0.90625, prec 0.100862, recall 0.817092
2017-12-10T04:52:11.678864: step 7272, loss 0.535211, acc 0.875, prec 0.100852, recall 0.817092
2017-12-10T04:52:11.947478: step 7273, loss 0.210199, acc 0.9375, prec 0.100848, recall 0.817092
2017-12-10T04:52:12.224195: step 7274, loss 0.35894, acc 0.921875, prec 0.100842, recall 0.817092
2017-12-10T04:52:12.491177: step 7275, loss 0.473271, acc 0.9375, prec 0.100847, recall 0.817109
2017-12-10T04:52:12.753248: step 7276, loss 0.436008, acc 0.921875, prec 0.100842, recall 0.817109
2017-12-10T04:52:13.018150: step 7277, loss 0.286196, acc 0.96875, prec 0.10086, recall 0.817145
2017-12-10T04:52:13.291408: step 7278, loss 0.0144301, acc 1, prec 0.100882, recall 0.81718
2017-12-10T04:52:13.555624: step 7279, loss 0.304043, acc 0.921875, prec 0.100876, recall 0.81718
2017-12-10T04:52:13.827876: step 7280, loss 0.105193, acc 0.96875, prec 0.100884, recall 0.817197
2017-12-10T04:52:14.092029: step 7281, loss 0.265843, acc 0.96875, prec 0.100892, recall 0.817215
2017-12-10T04:52:14.364433: step 7282, loss 0.833777, acc 0.9375, prec 0.100898, recall 0.817232
2017-12-10T04:52:14.637672: step 7283, loss 0.0168024, acc 0.984375, prec 0.100939, recall 0.817302
2017-12-10T04:52:14.903263: step 7284, loss 0.572825, acc 0.875, prec 0.100962, recall 0.817355
2017-12-10T04:52:15.179313: step 7285, loss 0.159789, acc 0.96875, prec 0.10097, recall 0.817372
2017-12-10T04:52:15.444301: step 7286, loss 0.377431, acc 0.9375, prec 0.100987, recall 0.817407
2017-12-10T04:52:15.725198: step 7287, loss 0.37924, acc 0.953125, prec 0.100983, recall 0.817407
2017-12-10T04:52:15.990668: step 7288, loss 0.0732441, acc 0.953125, prec 0.100979, recall 0.817407
2017-12-10T04:52:16.265580: step 7289, loss 0.0229944, acc 1, prec 0.10099, recall 0.817425
2017-12-10T04:52:16.528884: step 7290, loss 0.233693, acc 0.9375, prec 0.100996, recall 0.817442
2017-12-10T04:52:16.795139: step 7291, loss 0.729906, acc 0.90625, prec 0.100989, recall 0.817442
2017-12-10T04:52:17.067734: step 7292, loss 0.182971, acc 0.96875, prec 0.100997, recall 0.81746
2017-12-10T04:52:17.335669: step 7293, loss 0.23953, acc 0.953125, prec 0.101004, recall 0.817477
2017-12-10T04:52:17.614150: step 7294, loss 0.243648, acc 0.96875, prec 0.101002, recall 0.817477
2017-12-10T04:52:17.887075: step 7295, loss 0.00639995, acc 1, prec 0.101012, recall 0.817495
2017-12-10T04:52:18.153461: step 7296, loss 0.10183, acc 0.96875, prec 0.10102, recall 0.817512
2017-12-10T04:52:18.425938: step 7297, loss 0.0536801, acc 0.984375, prec 0.101019, recall 0.817512
2017-12-10T04:52:18.699716: step 7298, loss 0.181226, acc 0.9375, prec 0.101015, recall 0.817512
2017-12-10T04:52:18.971126: step 7299, loss 0.103453, acc 0.984375, prec 0.101024, recall 0.817529
2017-12-10T04:52:19.237250: step 7300, loss 0.566077, acc 0.953125, prec 0.10102, recall 0.817529
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-7300

2017-12-10T04:52:20.501048: step 7301, loss 3.50371, acc 0.96875, prec 0.101019, recall 0.817451
2017-12-10T04:52:20.780232: step 7302, loss 0.000461218, acc 1, prec 0.10103, recall 0.817469
2017-12-10T04:52:21.049081: step 7303, loss 0.398781, acc 0.9375, prec 0.101036, recall 0.817486
2017-12-10T04:52:21.318962: step 7304, loss 0.00304303, acc 1, prec 0.101036, recall 0.817486
2017-12-10T04:52:21.586391: step 7305, loss 0.0496531, acc 0.984375, prec 0.101056, recall 0.817521
2017-12-10T04:52:21.850512: step 7306, loss 0.236777, acc 0.953125, prec 0.101052, recall 0.817521
2017-12-10T04:52:22.126018: step 7307, loss 0.193047, acc 0.90625, prec 0.101045, recall 0.817521
2017-12-10T04:52:22.397309: step 7308, loss 0.613081, acc 0.90625, prec 0.101038, recall 0.817521
2017-12-10T04:52:22.666836: step 7309, loss 0.71921, acc 0.90625, prec 0.101063, recall 0.817573
2017-12-10T04:52:22.935309: step 7310, loss 0.195165, acc 0.953125, prec 0.10107, recall 0.817591
2017-12-10T04:52:23.206592: step 7311, loss 0.473583, acc 0.921875, prec 0.101064, recall 0.817591
2017-12-10T04:52:23.488096: step 7312, loss 0.147112, acc 0.9375, prec 0.101069, recall 0.817608
2017-12-10T04:52:23.753777: step 7313, loss 0.0830999, acc 0.96875, prec 0.101088, recall 0.817643
2017-12-10T04:52:24.023107: step 7314, loss 0.322552, acc 0.90625, prec 0.101092, recall 0.817661
2017-12-10T04:52:24.291805: step 7315, loss 0.526039, acc 0.890625, prec 0.101094, recall 0.817678
2017-12-10T04:52:24.562554: step 7316, loss 0.470121, acc 0.875, prec 0.101095, recall 0.817695
2017-12-10T04:52:24.829022: step 7317, loss 0.264255, acc 0.9375, prec 0.101122, recall 0.817748
2017-12-10T04:52:25.097125: step 7318, loss 0.00191863, acc 1, prec 0.101122, recall 0.817748
2017-12-10T04:52:25.367415: step 7319, loss 0.0151879, acc 1, prec 0.101122, recall 0.817748
2017-12-10T04:52:25.631823: step 7320, loss 0.0481813, acc 0.984375, prec 0.101132, recall 0.817765
2017-12-10T04:52:25.897498: step 7321, loss 0.0884613, acc 0.96875, prec 0.10114, recall 0.817782
2017-12-10T04:52:26.162894: step 7322, loss 0.302343, acc 0.9375, prec 0.101135, recall 0.817782
2017-12-10T04:52:26.434365: step 7323, loss 3.85153, acc 0.921875, prec 0.101141, recall 0.817722
2017-12-10T04:52:26.705209: step 7324, loss 0.455498, acc 0.9375, prec 0.101147, recall 0.817739
2017-12-10T04:52:26.979826: step 7325, loss 0.135349, acc 0.96875, prec 0.101155, recall 0.817757
2017-12-10T04:52:27.258562: step 7326, loss 0.100896, acc 0.9375, prec 0.10115, recall 0.817757
2017-12-10T04:52:27.528741: step 7327, loss 0.417534, acc 0.921875, prec 0.101144, recall 0.817757
2017-12-10T04:52:27.798752: step 7328, loss 0.299211, acc 0.953125, prec 0.101162, recall 0.817791
2017-12-10T04:52:28.064518: step 7329, loss 0.807989, acc 0.90625, prec 0.101165, recall 0.817809
2017-12-10T04:52:28.330152: step 7330, loss 0.103801, acc 0.953125, prec 0.101172, recall 0.817826
2017-12-10T04:52:28.597704: step 7331, loss 2.00343, acc 0.875, prec 0.101164, recall 0.817748
2017-12-10T04:52:28.879759: step 7332, loss 0.475872, acc 0.859375, prec 0.101153, recall 0.817748
2017-12-10T04:52:29.148998: step 7333, loss 1.00862, acc 0.890625, prec 0.101145, recall 0.817748
2017-12-10T04:52:29.418578: step 7334, loss 0.154827, acc 0.96875, prec 0.101153, recall 0.817765
2017-12-10T04:52:29.686630: step 7335, loss 0.147383, acc 0.984375, prec 0.101184, recall 0.817818
2017-12-10T04:52:29.955873: step 7336, loss 0.673851, acc 0.921875, prec 0.101178, recall 0.817818
2017-12-10T04:52:30.236419: step 7337, loss 0.33606, acc 0.921875, prec 0.101193, recall 0.817852
2017-12-10T04:52:30.512500: step 7338, loss 0.190305, acc 0.984375, prec 0.101192, recall 0.817852
2017-12-10T04:52:30.793537: step 7339, loss 1.45095, acc 0.90625, prec 0.101206, recall 0.817887
2017-12-10T04:52:31.064318: step 7340, loss 0.524969, acc 0.953125, prec 0.101234, recall 0.817939
2017-12-10T04:52:31.330320: step 7341, loss 0.427508, acc 0.9375, prec 0.10124, recall 0.817957
2017-12-10T04:52:31.595190: step 7342, loss 0.209443, acc 0.953125, prec 0.101257, recall 0.817991
2017-12-10T04:52:31.869146: step 7343, loss 0.558807, acc 0.921875, prec 0.101251, recall 0.817991
2017-12-10T04:52:32.138561: step 7344, loss 0.835853, acc 0.90625, prec 0.101244, recall 0.817991
2017-12-10T04:52:32.408105: step 7345, loss 0.204789, acc 0.953125, prec 0.101251, recall 0.818009
2017-12-10T04:52:32.670640: step 7346, loss 1.63253, acc 0.921875, prec 0.101247, recall 0.817931
2017-12-10T04:52:32.939938: step 7347, loss 0.129619, acc 0.953125, prec 0.101243, recall 0.817931
2017-12-10T04:52:33.212897: step 7348, loss 0.41506, acc 0.9375, prec 0.101238, recall 0.817931
2017-12-10T04:52:33.476976: step 7349, loss 0.854121, acc 0.875, prec 0.101239, recall 0.817948
2017-12-10T04:52:33.737015: step 7350, loss 0.211604, acc 0.953125, prec 0.101246, recall 0.817965
2017-12-10T04:52:34.008926: step 7351, loss 0.810082, acc 0.890625, prec 0.10128, recall 0.818035
2017-12-10T04:52:34.272478: step 7352, loss 0.203136, acc 0.9375, prec 0.101286, recall 0.818052
2017-12-10T04:52:34.538384: step 7353, loss 0.613526, acc 0.84375, prec 0.101274, recall 0.818052
2017-12-10T04:52:34.803394: step 7354, loss 0.22295, acc 0.9375, prec 0.10128, recall 0.818069
2017-12-10T04:52:35.071335: step 7355, loss 0.337376, acc 0.953125, prec 0.101287, recall 0.818087
2017-12-10T04:52:35.339676: step 7356, loss 0.510743, acc 0.875, prec 0.101288, recall 0.818104
2017-12-10T04:52:35.601476: step 7357, loss 0.583225, acc 0.890625, prec 0.101301, recall 0.818139
2017-12-10T04:52:35.874903: step 7358, loss 0.327275, acc 0.9375, prec 0.101307, recall 0.818156
2017-12-10T04:52:36.144915: step 7359, loss 0.189435, acc 0.953125, prec 0.101303, recall 0.818156
2017-12-10T04:52:36.412595: step 7360, loss 0.346209, acc 0.96875, prec 0.101322, recall 0.81819
2017-12-10T04:52:36.680002: step 7361, loss 10.4107, acc 0.9375, prec 0.101329, recall 0.81813
2017-12-10T04:52:36.954214: step 7362, loss 0.100923, acc 0.9375, prec 0.101324, recall 0.81813
2017-12-10T04:52:37.228449: step 7363, loss 0.668458, acc 0.890625, prec 0.101316, recall 0.81813
2017-12-10T04:52:37.505941: step 7364, loss 0.510066, acc 0.859375, prec 0.101305, recall 0.81813
2017-12-10T04:52:37.770827: step 7365, loss 0.337575, acc 0.921875, prec 0.10131, recall 0.818147
2017-12-10T04:52:38.042582: step 7366, loss 0.870291, acc 0.875, prec 0.1013, recall 0.818147
2017-12-10T04:52:38.315234: step 7367, loss 0.495193, acc 0.890625, prec 0.101292, recall 0.818147
2017-12-10T04:52:38.578298: step 7368, loss 0.264568, acc 0.90625, prec 0.101285, recall 0.818147
2017-12-10T04:52:38.838935: step 7369, loss 0.297487, acc 0.921875, prec 0.10131, recall 0.818199
2017-12-10T04:52:39.110309: step 7370, loss 0.208759, acc 0.9375, prec 0.101327, recall 0.818234
2017-12-10T04:52:39.372625: step 7371, loss 0.720991, acc 0.890625, prec 0.10134, recall 0.818268
2017-12-10T04:52:39.641168: step 7372, loss 0.246295, acc 0.921875, prec 0.101344, recall 0.818286
2017-12-10T04:52:39.913107: step 7373, loss 0.380965, acc 0.9375, prec 0.10135, recall 0.818303
2017-12-10T04:52:40.177586: step 7374, loss 0.0903092, acc 0.953125, prec 0.101368, recall 0.818337
2017-12-10T04:52:40.449838: step 7375, loss 0.549805, acc 0.875, prec 0.10139, recall 0.818389
2017-12-10T04:52:40.729564: step 7376, loss 0.33404, acc 0.90625, prec 0.101393, recall 0.818406
2017-12-10T04:52:40.996602: step 7377, loss 0.954357, acc 0.875, prec 0.101384, recall 0.818406
2017-12-10T04:52:41.262408: step 7378, loss 0.320545, acc 0.9375, prec 0.101389, recall 0.818424
2017-12-10T04:52:41.537672: step 7379, loss 0.185281, acc 0.953125, prec 0.101407, recall 0.818458
2017-12-10T04:52:41.800597: step 7380, loss 0.202702, acc 0.9375, prec 0.101413, recall 0.818475
2017-12-10T04:52:42.073529: step 7381, loss 0.469229, acc 0.9375, prec 0.101429, recall 0.81851
2017-12-10T04:52:42.342862: step 7382, loss 0.00898035, acc 1, prec 0.10144, recall 0.818527
2017-12-10T04:52:42.609154: step 7383, loss 0.00035971, acc 1, prec 0.10144, recall 0.818527
2017-12-10T04:52:42.868297: step 7384, loss 0.2355, acc 0.9375, prec 0.101446, recall 0.818544
2017-12-10T04:52:43.140667: step 7385, loss 0.011573, acc 0.984375, prec 0.101455, recall 0.818561
2017-12-10T04:52:43.405602: step 7386, loss 0.0015011, acc 1, prec 0.101455, recall 0.818561
2017-12-10T04:52:43.678845: step 7387, loss 0.584345, acc 0.96875, prec 0.101453, recall 0.818561
2017-12-10T04:52:43.947084: step 7388, loss 0.0239634, acc 0.984375, prec 0.101451, recall 0.818561
2017-12-10T04:52:44.216420: step 7389, loss 0.0822235, acc 0.984375, prec 0.10145, recall 0.818561
2017-12-10T04:52:44.489196: step 7390, loss 0.00384311, acc 1, prec 0.10145, recall 0.818561
2017-12-10T04:52:44.752785: step 7391, loss 0.432873, acc 0.953125, prec 0.101468, recall 0.818596
2017-12-10T04:52:45.020928: step 7392, loss 0.723864, acc 0.984375, prec 0.101477, recall 0.818613
2017-12-10T04:52:45.292067: step 7393, loss 1.91914, acc 0.953125, prec 0.101475, recall 0.818535
2017-12-10T04:52:45.562961: step 7394, loss 0.011191, acc 1, prec 0.101475, recall 0.818535
2017-12-10T04:52:45.831175: step 7395, loss 0.337882, acc 0.96875, prec 0.101472, recall 0.818535
2017-12-10T04:52:46.102706: step 7396, loss 0.476652, acc 0.953125, prec 0.10149, recall 0.81857
2017-12-10T04:52:46.374668: step 7397, loss 0.0788852, acc 0.96875, prec 0.101509, recall 0.818604
2017-12-10T04:52:46.643356: step 7398, loss 0.333681, acc 0.984375, prec 0.101529, recall 0.818639
2017-12-10T04:52:46.915678: step 7399, loss 0.435183, acc 0.953125, prec 0.101525, recall 0.818639
2017-12-10T04:52:47.188426: step 7400, loss 0.286041, acc 0.890625, prec 0.101517, recall 0.818639
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-7400

2017-12-10T04:52:48.445285: step 7401, loss 0.0512984, acc 0.96875, prec 0.101514, recall 0.818639
2017-12-10T04:52:48.711272: step 7402, loss 0.619462, acc 0.90625, prec 0.101528, recall 0.818673
2017-12-10T04:52:48.974813: step 7403, loss 0.592883, acc 0.90625, prec 0.101521, recall 0.818673
2017-12-10T04:52:49.243646: step 7404, loss 0.222767, acc 0.921875, prec 0.101536, recall 0.818707
2017-12-10T04:52:49.514140: step 7405, loss 0.711676, acc 0.90625, prec 0.10154, recall 0.818725
2017-12-10T04:52:49.781337: step 7406, loss 0.406897, acc 0.984375, prec 0.101549, recall 0.818742
2017-12-10T04:52:50.047195: step 7407, loss 1.01679, acc 0.890625, prec 0.101562, recall 0.818776
2017-12-10T04:52:50.319454: step 7408, loss 0.52508, acc 0.953125, prec 0.101569, recall 0.818793
2017-12-10T04:52:50.588123: step 7409, loss 0.542877, acc 0.953125, prec 0.101565, recall 0.818793
2017-12-10T04:52:50.851769: step 7410, loss 0.109201, acc 0.953125, prec 0.101572, recall 0.81881
2017-12-10T04:52:51.120236: step 7411, loss 0.162318, acc 0.984375, prec 0.101581, recall 0.818828
2017-12-10T04:52:51.387846: step 7412, loss 0.129918, acc 0.953125, prec 0.101588, recall 0.818845
2017-12-10T04:52:51.656801: step 7413, loss 1.09931, acc 0.9375, prec 0.101605, recall 0.818879
2017-12-10T04:52:51.925728: step 7414, loss 0.3849, acc 0.921875, prec 0.101609, recall 0.818896
2017-12-10T04:52:52.185309: step 7415, loss 0.0992655, acc 0.96875, prec 0.101617, recall 0.818913
2017-12-10T04:52:52.449744: step 7416, loss 0.882035, acc 0.90625, prec 0.101621, recall 0.81893
2017-12-10T04:52:52.723959: step 7417, loss 0.17109, acc 0.953125, prec 0.101638, recall 0.818965
2017-12-10T04:52:52.994271: step 7418, loss 0.247927, acc 0.921875, prec 0.101632, recall 0.818965
2017-12-10T04:52:53.268604: step 7419, loss 0.604225, acc 0.90625, prec 0.101636, recall 0.818982
2017-12-10T04:52:53.545156: step 7420, loss 0.161072, acc 0.96875, prec 0.101644, recall 0.818999
2017-12-10T04:52:53.812039: step 7421, loss 0.126935, acc 0.96875, prec 0.101652, recall 0.819016
2017-12-10T04:52:54.078094: step 7422, loss 0.183904, acc 0.96875, prec 0.10165, recall 0.819016
2017-12-10T04:52:54.343606: step 7423, loss 0.0277257, acc 0.984375, prec 0.101649, recall 0.819016
2017-12-10T04:52:54.610448: step 7424, loss 0.0412333, acc 0.984375, prec 0.101658, recall 0.819033
2017-12-10T04:52:54.876292: step 7425, loss 0.469217, acc 0.890625, prec 0.10166, recall 0.81905
2017-12-10T04:52:55.141051: step 7426, loss 0.351433, acc 0.9375, prec 0.101655, recall 0.81905
2017-12-10T04:52:55.411698: step 7427, loss 0.483487, acc 0.9375, prec 0.101672, recall 0.819085
2017-12-10T04:52:55.681081: step 7428, loss 0.148463, acc 0.96875, prec 0.10169, recall 0.819119
2017-12-10T04:52:55.947105: step 7429, loss 0.332935, acc 0.984375, prec 0.101721, recall 0.81917
2017-12-10T04:52:56.221356: step 7430, loss 0.27745, acc 0.9375, prec 0.101727, recall 0.819187
2017-12-10T04:52:56.489148: step 7431, loss 0.167317, acc 0.96875, prec 0.101724, recall 0.819187
2017-12-10T04:52:56.758454: step 7432, loss 0.0133492, acc 1, prec 0.101724, recall 0.819187
2017-12-10T04:52:57.031114: step 7433, loss 0.293567, acc 0.953125, prec 0.101731, recall 0.819204
2017-12-10T04:52:57.303225: step 7434, loss 0.172248, acc 0.984375, prec 0.10173, recall 0.819204
2017-12-10T04:52:57.564369: step 7435, loss 0.0117509, acc 1, prec 0.10173, recall 0.819204
2017-12-10T04:52:57.827838: step 7436, loss 0.109597, acc 0.984375, prec 0.101729, recall 0.819204
2017-12-10T04:52:58.097324: step 7437, loss 0.294447, acc 0.953125, prec 0.101725, recall 0.819204
2017-12-10T04:52:58.361686: step 7438, loss 0.141819, acc 0.96875, prec 0.101754, recall 0.819255
2017-12-10T04:52:58.633289: step 7439, loss 0.0105953, acc 1, prec 0.101765, recall 0.819273
2017-12-10T04:52:58.906393: step 7440, loss 0.0170076, acc 1, prec 0.101775, recall 0.81929
2017-12-10T04:52:59.172121: step 7441, loss 0.00726999, acc 1, prec 0.101786, recall 0.819307
2017-12-10T04:52:59.435771: step 7442, loss 0.357562, acc 0.984375, prec 0.101795, recall 0.819324
2017-12-10T04:52:59.709411: step 7443, loss 0.186643, acc 0.984375, prec 0.101805, recall 0.819341
2017-12-10T04:52:59.973791: step 7444, loss 0.169746, acc 0.96875, prec 0.101802, recall 0.819341
2017-12-10T04:53:00.252134: step 7445, loss 0.00274875, acc 1, prec 0.101823, recall 0.819375
2017-12-10T04:53:00.527049: step 7446, loss 8.29611, acc 0.9375, prec 0.101821, recall 0.81922
2017-12-10T04:53:00.805812: step 7447, loss 2.55275, acc 0.96875, prec 0.10183, recall 0.81916
2017-12-10T04:53:01.072378: step 7448, loss 0.467335, acc 0.90625, prec 0.101834, recall 0.819177
2017-12-10T04:53:01.342514: step 7449, loss 0.0147298, acc 1, prec 0.101865, recall 0.819228
2017-12-10T04:53:01.604697: step 7450, loss 0.58638, acc 0.875, prec 0.101856, recall 0.819228
2017-12-10T04:53:01.875290: step 7451, loss 0.890056, acc 0.859375, prec 0.101856, recall 0.819245
2017-12-10T04:53:02.140725: step 7452, loss 1.05918, acc 0.84375, prec 0.101844, recall 0.819245
2017-12-10T04:53:02.416938: step 7453, loss 2.31441, acc 0.65625, prec 0.101817, recall 0.819245
2017-12-10T04:53:02.689092: step 7454, loss 2.20785, acc 0.703125, prec 0.101805, recall 0.819262
2017-12-10T04:53:02.925993: step 7455, loss 2.4249, acc 0.730769, prec 0.101799, recall 0.819279
2017-12-10T04:53:03.207191: step 7456, loss 2.16065, acc 0.703125, prec 0.101776, recall 0.819279
2017-12-10T04:53:03.474847: step 7457, loss 1.88038, acc 0.65625, prec 0.101761, recall 0.819296
2017-12-10T04:53:03.752420: step 7458, loss 1.04028, acc 0.78125, prec 0.101744, recall 0.819296
2017-12-10T04:53:04.022352: step 7459, loss 2.60484, acc 0.71875, prec 0.101733, recall 0.819313
2017-12-10T04:53:04.285291: step 7460, loss 0.968359, acc 0.8125, prec 0.101719, recall 0.819313
2017-12-10T04:53:04.557606: step 7461, loss 1.28641, acc 0.765625, prec 0.101701, recall 0.819313
2017-12-10T04:53:04.830439: step 7462, loss 0.986116, acc 0.828125, prec 0.101709, recall 0.819348
2017-12-10T04:53:05.103399: step 7463, loss 0.49221, acc 0.921875, prec 0.101703, recall 0.819348
2017-12-10T04:53:05.372914: step 7464, loss 0.545719, acc 0.890625, prec 0.101695, recall 0.819348
2017-12-10T04:53:05.639445: step 7465, loss 1.45748, acc 0.890625, prec 0.101707, recall 0.819382
2017-12-10T04:53:05.904327: step 7466, loss 0.0843025, acc 0.984375, prec 0.101706, recall 0.819382
2017-12-10T04:53:06.171603: step 7467, loss 0.903121, acc 0.84375, prec 0.101694, recall 0.819382
2017-12-10T04:53:06.441464: step 7468, loss 0.296399, acc 0.953125, prec 0.101691, recall 0.819382
2017-12-10T04:53:06.707650: step 7469, loss 0.0759995, acc 0.96875, prec 0.101699, recall 0.819399
2017-12-10T04:53:06.971962: step 7470, loss 0.672258, acc 0.90625, prec 0.101702, recall 0.819416
2017-12-10T04:53:07.243512: step 7471, loss 0.135187, acc 0.96875, prec 0.101731, recall 0.819467
2017-12-10T04:53:07.515633: step 7472, loss 0.146855, acc 0.96875, prec 0.101729, recall 0.819467
2017-12-10T04:53:07.786019: step 7473, loss 0.525302, acc 0.953125, prec 0.101725, recall 0.819467
2017-12-10T04:53:08.049905: step 7474, loss 0.0469016, acc 0.984375, prec 0.101724, recall 0.819467
2017-12-10T04:53:08.315725: step 7475, loss 0.00329305, acc 1, prec 0.101735, recall 0.819484
2017-12-10T04:53:08.589930: step 7476, loss 0.262817, acc 0.96875, prec 0.101743, recall 0.819501
2017-12-10T04:53:08.849190: step 7477, loss 0.198206, acc 0.953125, prec 0.101739, recall 0.819501
2017-12-10T04:53:09.115220: step 7478, loss 0.000381712, acc 1, prec 0.10175, recall 0.819518
2017-12-10T04:53:09.383657: step 7479, loss 0.137345, acc 0.984375, prec 0.101759, recall 0.819535
2017-12-10T04:53:09.646702: step 7480, loss 0.0872745, acc 0.984375, prec 0.101779, recall 0.819569
2017-12-10T04:53:09.915039: step 7481, loss 0.0712676, acc 1, prec 0.1018, recall 0.819603
2017-12-10T04:53:10.179591: step 7482, loss 0.107145, acc 0.984375, prec 0.101799, recall 0.819603
2017-12-10T04:53:10.441863: step 7483, loss 3.61235, acc 0.984375, prec 0.101799, recall 0.819526
2017-12-10T04:53:10.709101: step 7484, loss 0.310561, acc 0.9375, prec 0.101794, recall 0.819526
2017-12-10T04:53:10.971864: step 7485, loss 0.000101086, acc 1, prec 0.101825, recall 0.819576
2017-12-10T04:53:11.231864: step 7486, loss 3.76755, acc 0.953125, prec 0.101823, recall 0.819499
2017-12-10T04:53:11.504383: step 7487, loss 0.183278, acc 0.9375, prec 0.101829, recall 0.819516
2017-12-10T04:53:11.771127: step 7488, loss 0.0783798, acc 0.96875, prec 0.101837, recall 0.819533
2017-12-10T04:53:12.046887: step 7489, loss 0.669094, acc 0.921875, prec 0.101831, recall 0.819533
2017-12-10T04:53:12.318100: step 7490, loss 0.865529, acc 0.890625, prec 0.101833, recall 0.81955
2017-12-10T04:53:12.586568: step 7491, loss 1.52564, acc 0.875, prec 0.101834, recall 0.819567
2017-12-10T04:53:12.847356: step 7492, loss 0.502948, acc 0.890625, prec 0.101847, recall 0.819601
2017-12-10T04:53:13.110055: step 7493, loss 0.213907, acc 0.9375, prec 0.101842, recall 0.819601
2017-12-10T04:53:13.374711: step 7494, loss 0.272697, acc 0.9375, prec 0.101858, recall 0.819635
2017-12-10T04:53:13.650381: step 7495, loss 0.695527, acc 0.875, prec 0.10187, recall 0.819669
2017-12-10T04:53:13.916109: step 7496, loss 0.0718316, acc 0.96875, prec 0.101867, recall 0.819669
2017-12-10T04:53:14.181951: step 7497, loss 0.110966, acc 0.96875, prec 0.101865, recall 0.819669
2017-12-10T04:53:14.442640: step 7498, loss 0.37605, acc 0.875, prec 0.101855, recall 0.819669
2017-12-10T04:53:14.708999: step 7499, loss 0.321183, acc 0.953125, prec 0.101862, recall 0.819686
2017-12-10T04:53:14.977606: step 7500, loss 0.118144, acc 0.96875, prec 0.10187, recall 0.819703

Evaluation:
2017-12-10T04:53:22.612803: step 7500, loss 4.63851, acc 0.915267, prec 0.101788, recall 0.816521

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-7500

2017-12-10T04:53:23.843830: step 7501, loss 0.609754, acc 0.875, prec 0.101779, recall 0.816521
2017-12-10T04:53:24.110482: step 7502, loss 0.0514553, acc 0.96875, prec 0.101797, recall 0.816555
2017-12-10T04:53:24.375466: step 7503, loss 0.444974, acc 0.921875, prec 0.101822, recall 0.816606
2017-12-10T04:53:24.637842: step 7504, loss 0.0561227, acc 0.96875, prec 0.10182, recall 0.816606
2017-12-10T04:53:24.900486: step 7505, loss 0.448654, acc 0.890625, prec 0.101822, recall 0.816623
2017-12-10T04:53:25.175938: step 7506, loss 0.0921414, acc 0.953125, prec 0.101818, recall 0.816623
2017-12-10T04:53:25.439208: step 7507, loss 0.338165, acc 0.9375, prec 0.101814, recall 0.816623
2017-12-10T04:53:25.707133: step 7508, loss 0.0963004, acc 0.984375, prec 0.101833, recall 0.816657
2017-12-10T04:53:25.973691: step 7509, loss 0.310195, acc 0.953125, prec 0.101851, recall 0.816691
2017-12-10T04:53:26.244349: step 7510, loss 0.145874, acc 0.953125, prec 0.101868, recall 0.816725
2017-12-10T04:53:26.518008: step 7511, loss 0.434745, acc 0.953125, prec 0.101864, recall 0.816725
2017-12-10T04:53:26.785572: step 7512, loss 0.00701347, acc 1, prec 0.101864, recall 0.816725
2017-12-10T04:53:27.059096: step 7513, loss 0.0026662, acc 1, prec 0.101885, recall 0.816759
2017-12-10T04:53:27.334475: step 7514, loss 0.302376, acc 0.96875, prec 0.101903, recall 0.816793
2017-12-10T04:53:27.602363: step 7515, loss 0.0607366, acc 0.984375, prec 0.101913, recall 0.81681
2017-12-10T04:53:27.869932: step 7516, loss 0.250474, acc 0.96875, prec 0.101921, recall 0.816827
2017-12-10T04:53:28.136101: step 7517, loss 0.0112951, acc 1, prec 0.101921, recall 0.816827
2017-12-10T04:53:28.403517: step 7518, loss 0.205283, acc 0.953125, prec 0.101917, recall 0.816827
2017-12-10T04:53:28.667287: step 7519, loss 0.0437836, acc 0.96875, prec 0.101915, recall 0.816827
2017-12-10T04:53:28.934109: step 7520, loss 0.107731, acc 0.96875, prec 0.101912, recall 0.816827
2017-12-10T04:53:29.199622: step 7521, loss 0.13914, acc 0.984375, prec 0.101942, recall 0.816878
2017-12-10T04:53:29.463692: step 7522, loss 0.143015, acc 0.953125, prec 0.10196, recall 0.816912
2017-12-10T04:53:29.730544: step 7523, loss 0.261465, acc 0.953125, prec 0.101956, recall 0.816912
2017-12-10T04:53:29.999318: step 7524, loss 0.000573359, acc 1, prec 0.101956, recall 0.816912
2017-12-10T04:53:30.271333: step 7525, loss 0.0378041, acc 0.984375, prec 0.101965, recall 0.816929
2017-12-10T04:53:30.540065: step 7526, loss 0.00982199, acc 1, prec 0.101976, recall 0.816946
2017-12-10T04:53:30.808737: step 7527, loss 0.858024, acc 1, prec 0.101986, recall 0.816963
2017-12-10T04:53:31.077561: step 7528, loss 0.0145692, acc 1, prec 0.101986, recall 0.816963
2017-12-10T04:53:31.345086: step 7529, loss 0.0201376, acc 1, prec 0.101986, recall 0.816963
2017-12-10T04:53:31.607554: step 7530, loss 0.0215847, acc 0.984375, prec 0.101985, recall 0.816963
2017-12-10T04:53:31.869257: step 7531, loss 0.134738, acc 0.953125, prec 0.102002, recall 0.816997
2017-12-10T04:53:32.135477: step 7532, loss 0.324577, acc 0.984375, prec 0.102011, recall 0.817013
2017-12-10T04:53:32.404475: step 7533, loss 0.00213905, acc 1, prec 0.102011, recall 0.817013
2017-12-10T04:53:32.668938: step 7534, loss 0.0151484, acc 1, prec 0.102032, recall 0.817047
2017-12-10T04:53:32.937266: step 7535, loss 0.00206681, acc 1, prec 0.102042, recall 0.817064
2017-12-10T04:53:33.202082: step 7536, loss 0.32182, acc 0.953125, prec 0.102039, recall 0.817064
2017-12-10T04:53:33.475200: step 7537, loss 0.112529, acc 0.984375, prec 0.102048, recall 0.817081
2017-12-10T04:53:33.746954: step 7538, loss 0.202164, acc 0.96875, prec 0.102056, recall 0.817098
2017-12-10T04:53:34.015969: step 7539, loss 7.73715, acc 0.984375, prec 0.102066, recall 0.817039
2017-12-10T04:53:34.289549: step 7540, loss 0.183676, acc 0.953125, prec 0.102073, recall 0.817056
2017-12-10T04:53:34.554688: step 7541, loss 0.102523, acc 0.96875, prec 0.102071, recall 0.817056
2017-12-10T04:53:34.816018: step 7542, loss 0.412199, acc 0.875, prec 0.102061, recall 0.817056
2017-12-10T04:53:35.082510: step 7543, loss 0.171457, acc 0.953125, prec 0.102078, recall 0.81709
2017-12-10T04:53:35.350953: step 7544, loss 0.179249, acc 0.953125, prec 0.102085, recall 0.817107
2017-12-10T04:53:35.614675: step 7545, loss 0.508753, acc 0.921875, prec 0.1021, recall 0.817141
2017-12-10T04:53:35.878124: step 7546, loss 0.0692907, acc 0.96875, prec 0.102098, recall 0.817141
2017-12-10T04:53:36.151509: step 7547, loss 0.725779, acc 0.890625, prec 0.10211, recall 0.817174
2017-12-10T04:53:36.421951: step 7548, loss 0.506858, acc 0.921875, prec 0.102104, recall 0.817174
2017-12-10T04:53:36.697804: step 7549, loss 1.29337, acc 0.796875, prec 0.102089, recall 0.817174
2017-12-10T04:53:36.967202: step 7550, loss 0.4296, acc 0.890625, prec 0.102081, recall 0.817174
2017-12-10T04:53:37.237052: step 7551, loss 0.296181, acc 0.953125, prec 0.102077, recall 0.817174
2017-12-10T04:53:37.504464: step 7552, loss 0.58884, acc 0.953125, prec 0.102074, recall 0.817174
2017-12-10T04:53:37.772438: step 7553, loss 0.799217, acc 0.875, prec 0.102064, recall 0.817174
2017-12-10T04:53:38.037441: step 7554, loss 0.219969, acc 0.9375, prec 0.102091, recall 0.817225
2017-12-10T04:53:38.300582: step 7555, loss 0.579841, acc 0.90625, prec 0.102094, recall 0.817242
2017-12-10T04:53:38.575317: step 7556, loss 0.373508, acc 0.890625, prec 0.102086, recall 0.817242
2017-12-10T04:53:38.849564: step 7557, loss 0.664416, acc 0.9375, prec 0.102091, recall 0.817259
2017-12-10T04:53:39.116692: step 7558, loss 0.309154, acc 0.921875, prec 0.102086, recall 0.817259
2017-12-10T04:53:39.380845: step 7559, loss 0.0765264, acc 0.96875, prec 0.102083, recall 0.817259
2017-12-10T04:53:39.643642: step 7560, loss 0.333983, acc 0.953125, prec 0.10208, recall 0.817259
2017-12-10T04:53:39.911436: step 7561, loss 0.121681, acc 0.984375, prec 0.102099, recall 0.817293
2017-12-10T04:53:40.173265: step 7562, loss 0.0404009, acc 0.984375, prec 0.102108, recall 0.817309
2017-12-10T04:53:40.443650: step 7563, loss 0.38039, acc 0.96875, prec 0.102127, recall 0.817343
2017-12-10T04:53:40.716849: step 7564, loss 2.44142, acc 0.96875, prec 0.102136, recall 0.817285
2017-12-10T04:53:40.994618: step 7565, loss 0.33543, acc 0.953125, prec 0.102153, recall 0.817318
2017-12-10T04:53:41.258902: step 7566, loss 0.209499, acc 0.96875, prec 0.102151, recall 0.817318
2017-12-10T04:53:41.525834: step 7567, loss 0.916869, acc 0.90625, prec 0.102154, recall 0.817335
2017-12-10T04:53:41.795811: step 7568, loss 0.053546, acc 0.984375, prec 0.102163, recall 0.817352
2017-12-10T04:53:42.072179: step 7569, loss 0.192302, acc 0.984375, prec 0.102183, recall 0.817386
2017-12-10T04:53:42.341197: step 7570, loss 0.0404866, acc 0.96875, prec 0.102191, recall 0.817403
2017-12-10T04:53:42.608927: step 7571, loss 0.0274804, acc 0.984375, prec 0.102189, recall 0.817403
2017-12-10T04:53:42.880929: step 7572, loss 0.327924, acc 0.9375, prec 0.102195, recall 0.817419
2017-12-10T04:53:43.157575: step 7573, loss 0.249574, acc 0.9375, prec 0.102201, recall 0.817436
2017-12-10T04:53:43.426443: step 7574, loss 0.194589, acc 0.953125, prec 0.102208, recall 0.817453
2017-12-10T04:53:43.698450: step 7575, loss 0.0854079, acc 0.96875, prec 0.102216, recall 0.81747
2017-12-10T04:53:43.973418: step 7576, loss 0.0163538, acc 0.984375, prec 0.102214, recall 0.81747
2017-12-10T04:53:44.255232: step 7577, loss 0.523183, acc 0.96875, prec 0.102212, recall 0.81747
2017-12-10T04:53:44.524281: step 7578, loss 0.395359, acc 0.921875, prec 0.102206, recall 0.81747
2017-12-10T04:53:44.796512: step 7579, loss 0.515549, acc 0.9375, prec 0.102201, recall 0.81747
2017-12-10T04:53:45.061318: step 7580, loss 0.0999899, acc 0.984375, prec 0.1022, recall 0.81747
2017-12-10T04:53:45.335994: step 7581, loss 0.342274, acc 0.921875, prec 0.102194, recall 0.81747
2017-12-10T04:53:45.600894: step 7582, loss 0.399041, acc 0.953125, prec 0.102222, recall 0.81752
2017-12-10T04:53:45.872654: step 7583, loss 0.0969646, acc 0.984375, prec 0.102231, recall 0.817537
2017-12-10T04:53:46.138830: step 7584, loss 1.33662, acc 0.953125, prec 0.102229, recall 0.817462
2017-12-10T04:53:46.407223: step 7585, loss 0.365913, acc 0.953125, prec 0.102235, recall 0.817479
2017-12-10T04:53:46.679428: step 7586, loss 0.0877616, acc 0.984375, prec 0.102245, recall 0.817495
2017-12-10T04:53:46.944989: step 7587, loss 0.115238, acc 0.96875, prec 0.102253, recall 0.817512
2017-12-10T04:53:47.210754: step 7588, loss 0.00547403, acc 1, prec 0.102284, recall 0.817563
2017-12-10T04:53:47.476189: step 7589, loss 0.471518, acc 0.953125, prec 0.102311, recall 0.817613
2017-12-10T04:53:47.746616: step 7590, loss 0.0632111, acc 0.96875, prec 0.102309, recall 0.817613
2017-12-10T04:53:48.013274: step 7591, loss 0.712303, acc 0.890625, prec 0.1023, recall 0.817613
2017-12-10T04:53:48.281906: step 7592, loss 0.506942, acc 0.953125, prec 0.102318, recall 0.817647
2017-12-10T04:53:48.547456: step 7593, loss 0.295481, acc 0.9375, prec 0.102323, recall 0.817663
2017-12-10T04:53:48.816579: step 7594, loss 1.02365, acc 0.90625, prec 0.102326, recall 0.81768
2017-12-10T04:53:49.089504: step 7595, loss 0.0142825, acc 1, prec 0.102357, recall 0.81773
2017-12-10T04:53:49.355149: step 7596, loss 0.36906, acc 0.921875, prec 0.102372, recall 0.817764
2017-12-10T04:53:49.621168: step 7597, loss 0.31667, acc 0.9375, prec 0.102399, recall 0.817814
2017-12-10T04:53:49.890930: step 7598, loss 0.224433, acc 0.96875, prec 0.102407, recall 0.817831
2017-12-10T04:53:50.159335: step 7599, loss 0.00225562, acc 1, prec 0.102427, recall 0.817864
2017-12-10T04:53:50.430069: step 7600, loss 0.205111, acc 0.984375, prec 0.102436, recall 0.817881
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-7600

2017-12-10T04:53:51.812227: step 7601, loss 0.594624, acc 0.90625, prec 0.10245, recall 0.817915
2017-12-10T04:53:52.077850: step 7602, loss 2.34806, acc 0.921875, prec 0.102445, recall 0.817839
2017-12-10T04:53:52.346649: step 7603, loss 0.255768, acc 0.9375, prec 0.102451, recall 0.817856
2017-12-10T04:53:52.616033: step 7604, loss 0.621587, acc 0.90625, prec 0.102464, recall 0.81789
2017-12-10T04:53:52.882666: step 7605, loss 0.105467, acc 0.96875, prec 0.102472, recall 0.817906
2017-12-10T04:53:53.158475: step 7606, loss 0.16556, acc 0.953125, prec 0.102479, recall 0.817923
2017-12-10T04:53:53.422800: step 7607, loss 0.756478, acc 0.859375, prec 0.102469, recall 0.817923
2017-12-10T04:53:53.697491: step 7608, loss 0.333788, acc 0.953125, prec 0.102465, recall 0.817923
2017-12-10T04:53:53.965961: step 7609, loss 0.706107, acc 0.921875, prec 0.10248, recall 0.817957
2017-12-10T04:53:54.234569: step 7610, loss 0.542539, acc 0.890625, prec 0.102472, recall 0.817957
2017-12-10T04:53:54.513390: step 7611, loss 0.392777, acc 0.859375, prec 0.102461, recall 0.817957
2017-12-10T04:53:54.785204: step 7612, loss 0.146427, acc 0.953125, prec 0.102468, recall 0.817973
2017-12-10T04:53:55.057717: step 7613, loss 0.783113, acc 0.890625, prec 0.10248, recall 0.818007
2017-12-10T04:53:55.325428: step 7614, loss 0.743621, acc 0.921875, prec 0.102495, recall 0.81804
2017-12-10T04:53:55.591116: step 7615, loss 0.428499, acc 0.859375, prec 0.102495, recall 0.818057
2017-12-10T04:53:55.857390: step 7616, loss 0.267589, acc 0.9375, prec 0.10249, recall 0.818057
2017-12-10T04:53:56.125586: step 7617, loss 0.256888, acc 0.953125, prec 0.102507, recall 0.81809
2017-12-10T04:53:56.389034: step 7618, loss 0.132889, acc 0.984375, prec 0.102506, recall 0.81809
2017-12-10T04:53:56.663377: step 7619, loss 0.268743, acc 0.90625, prec 0.102499, recall 0.81809
2017-12-10T04:53:56.929214: step 7620, loss 0.253633, acc 0.953125, prec 0.102495, recall 0.81809
2017-12-10T04:53:57.206198: step 7621, loss 0.255096, acc 0.921875, prec 0.1025, recall 0.818107
2017-12-10T04:53:57.475796: step 7622, loss 3.18284, acc 0.953125, prec 0.102518, recall 0.818065
2017-12-10T04:53:57.746485: step 7623, loss 0.0332856, acc 0.984375, prec 0.102517, recall 0.818065
2017-12-10T04:53:58.010823: step 7624, loss 0.320813, acc 0.953125, prec 0.102534, recall 0.818098
2017-12-10T04:53:58.274585: step 7625, loss 0.244805, acc 0.953125, prec 0.102541, recall 0.818115
2017-12-10T04:53:58.548171: step 7626, loss 0.0815233, acc 0.96875, prec 0.102538, recall 0.818115
2017-12-10T04:53:58.818046: step 7627, loss 0.296186, acc 0.921875, prec 0.102532, recall 0.818115
2017-12-10T04:53:59.089387: step 7628, loss 0.156173, acc 0.953125, prec 0.102549, recall 0.818148
2017-12-10T04:53:59.354922: step 7629, loss 0.299947, acc 0.9375, prec 0.102545, recall 0.818148
2017-12-10T04:53:59.619001: step 7630, loss 0.0536533, acc 0.984375, prec 0.102564, recall 0.818182
2017-12-10T04:53:59.884285: step 7631, loss 0.188527, acc 0.953125, prec 0.102591, recall 0.818232
2017-12-10T04:54:00.147641: step 7632, loss 0.434235, acc 0.9375, prec 0.102587, recall 0.818232
2017-12-10T04:54:00.421008: step 7633, loss 0.449848, acc 0.9375, prec 0.102582, recall 0.818232
2017-12-10T04:54:00.688789: step 7634, loss 0.0443227, acc 0.96875, prec 0.10258, recall 0.818232
2017-12-10T04:54:00.956188: step 7635, loss 0.21377, acc 0.9375, prec 0.102596, recall 0.818265
2017-12-10T04:54:01.225187: step 7636, loss 0.0577936, acc 0.984375, prec 0.102594, recall 0.818265
2017-12-10T04:54:01.492684: step 7637, loss 0.185062, acc 0.984375, prec 0.102604, recall 0.818282
2017-12-10T04:54:01.759973: step 7638, loss 0.0287019, acc 0.984375, prec 0.102633, recall 0.818332
2017-12-10T04:54:02.029298: step 7639, loss 2.94238, acc 0.96875, prec 0.102632, recall 0.818257
2017-12-10T04:54:02.304511: step 7640, loss 0.287334, acc 0.890625, prec 0.102624, recall 0.818257
2017-12-10T04:54:02.575965: step 7641, loss 0.0822494, acc 0.953125, prec 0.102631, recall 0.818273
2017-12-10T04:54:02.847277: step 7642, loss 0.123319, acc 0.96875, prec 0.102639, recall 0.81829
2017-12-10T04:54:03.113920: step 7643, loss 0.754049, acc 0.90625, prec 0.102632, recall 0.81829
2017-12-10T04:54:03.383675: step 7644, loss 0.230024, acc 0.890625, prec 0.102654, recall 0.81834
2017-12-10T04:54:03.652142: step 7645, loss 1.26859, acc 0.890625, prec 0.102646, recall 0.81834
2017-12-10T04:54:03.921990: step 7646, loss 0.801308, acc 0.9375, prec 0.102652, recall 0.818357
2017-12-10T04:54:04.185977: step 7647, loss 0.218912, acc 0.9375, prec 0.102647, recall 0.818357
2017-12-10T04:54:04.454803: step 7648, loss 0.243798, acc 0.9375, prec 0.102663, recall 0.81839
2017-12-10T04:54:04.726582: step 7649, loss 0.914064, acc 0.84375, prec 0.102661, recall 0.818406
2017-12-10T04:54:04.988496: step 7650, loss 0.253778, acc 0.921875, prec 0.102676, recall 0.81844
2017-12-10T04:54:05.251976: step 7651, loss 1.13358, acc 0.9375, prec 0.102681, recall 0.818456
2017-12-10T04:54:05.523391: step 7652, loss 0.461639, acc 0.953125, prec 0.102678, recall 0.818456
2017-12-10T04:54:05.790039: step 7653, loss 0.162482, acc 0.953125, prec 0.102695, recall 0.818489
2017-12-10T04:54:06.062513: step 7654, loss 0.175903, acc 0.96875, prec 0.102693, recall 0.818489
2017-12-10T04:54:06.329411: step 7655, loss 0.415392, acc 0.921875, prec 0.102697, recall 0.818506
2017-12-10T04:54:06.594234: step 7656, loss 0.776961, acc 0.890625, prec 0.102689, recall 0.818506
2017-12-10T04:54:06.862563: step 7657, loss 0.363257, acc 0.9375, prec 0.102694, recall 0.818523
2017-12-10T04:54:07.133283: step 7658, loss 0.341073, acc 0.9375, prec 0.10269, recall 0.818523
2017-12-10T04:54:07.400792: step 7659, loss 0.196654, acc 0.953125, prec 0.102686, recall 0.818523
2017-12-10T04:54:07.662803: step 7660, loss 0.185198, acc 0.96875, prec 0.102694, recall 0.818539
2017-12-10T04:54:07.928165: step 7661, loss 0.222322, acc 0.9375, prec 0.10272, recall 0.818589
2017-12-10T04:54:08.196622: step 7662, loss 0.192526, acc 0.96875, prec 0.102728, recall 0.818605
2017-12-10T04:54:08.463051: step 7663, loss 0.0187597, acc 1, prec 0.102728, recall 0.818605
2017-12-10T04:54:08.727296: step 7664, loss 0.40613, acc 0.953125, prec 0.102725, recall 0.818605
2017-12-10T04:54:09.001345: step 7665, loss 0.292028, acc 0.9375, prec 0.10273, recall 0.818622
2017-12-10T04:54:09.264230: step 7666, loss 0.14902, acc 0.984375, prec 0.102739, recall 0.818639
2017-12-10T04:54:09.538530: step 7667, loss 0.316287, acc 0.953125, prec 0.102756, recall 0.818672
2017-12-10T04:54:09.803167: step 7668, loss 1.29037, acc 0.984375, prec 0.102777, recall 0.81863
2017-12-10T04:54:10.070391: step 7669, loss 0.210375, acc 0.921875, prec 0.102781, recall 0.818647
2017-12-10T04:54:10.335876: step 7670, loss 0.00420736, acc 1, prec 0.102802, recall 0.81868
2017-12-10T04:54:10.610743: step 7671, loss 0.275898, acc 0.984375, prec 0.102821, recall 0.818713
2017-12-10T04:54:10.877611: step 7672, loss 0.00115925, acc 1, prec 0.102821, recall 0.818713
2017-12-10T04:54:11.150339: step 7673, loss 0.741256, acc 0.984375, prec 0.10283, recall 0.818729
2017-12-10T04:54:11.417922: step 7674, loss 0.19171, acc 0.96875, prec 0.102838, recall 0.818746
2017-12-10T04:54:11.691936: step 7675, loss 0.588057, acc 0.921875, prec 0.102832, recall 0.818746
2017-12-10T04:54:11.960397: step 7676, loss 0.285019, acc 0.96875, prec 0.102851, recall 0.818779
2017-12-10T04:54:12.240973: step 7677, loss 6.76602, acc 0.9375, prec 0.102857, recall 0.818721
2017-12-10T04:54:12.516947: step 7678, loss 0.0932074, acc 0.953125, prec 0.102864, recall 0.818737
2017-12-10T04:54:12.780812: step 7679, loss 0.136536, acc 0.953125, prec 0.102871, recall 0.818754
2017-12-10T04:54:13.043882: step 7680, loss 0.591688, acc 0.9375, prec 0.102876, recall 0.818771
2017-12-10T04:54:13.307806: step 7681, loss 0.903012, acc 0.9375, prec 0.102903, recall 0.81882
2017-12-10T04:54:13.571803: step 7682, loss 0.70874, acc 0.9375, prec 0.102918, recall 0.818853
2017-12-10T04:54:13.847831: step 7683, loss 1.22815, acc 0.828125, prec 0.102916, recall 0.81887
2017-12-10T04:54:14.113173: step 7684, loss 0.0856387, acc 0.953125, prec 0.102912, recall 0.81887
2017-12-10T04:54:14.374626: step 7685, loss 0.310295, acc 0.890625, prec 0.102914, recall 0.818886
2017-12-10T04:54:14.641008: step 7686, loss 0.648729, acc 0.875, prec 0.102905, recall 0.818886
2017-12-10T04:54:14.909762: step 7687, loss 0.259215, acc 0.921875, prec 0.102909, recall 0.818903
2017-12-10T04:54:15.174859: step 7688, loss 0.317685, acc 0.9375, prec 0.102925, recall 0.818936
2017-12-10T04:54:16.161374: step 7689, loss 0.717299, acc 0.859375, prec 0.102945, recall 0.818985
2017-12-10T04:54:16.520207: step 7690, loss 0.721787, acc 0.890625, prec 0.102937, recall 0.818985
2017-12-10T04:54:16.786111: step 7691, loss 1.09929, acc 0.859375, prec 0.102937, recall 0.819002
2017-12-10T04:54:17.469714: step 7692, loss 0.683535, acc 0.90625, prec 0.10293, recall 0.819002
2017-12-10T04:54:18.232722: step 7693, loss 1.08036, acc 0.890625, prec 0.102932, recall 0.819018
2017-12-10T04:54:18.572674: step 7694, loss 0.75436, acc 0.90625, prec 0.102935, recall 0.819035
2017-12-10T04:54:18.902979: step 7695, loss 0.429703, acc 0.921875, prec 0.102929, recall 0.819035
2017-12-10T04:54:19.182182: step 7696, loss 0.584512, acc 0.90625, prec 0.102942, recall 0.819068
2017-12-10T04:54:19.469281: step 7697, loss 0.891091, acc 0.890625, prec 0.102944, recall 0.819084
2017-12-10T04:54:19.770673: step 7698, loss 0.640708, acc 0.890625, prec 0.102936, recall 0.819084
2017-12-10T04:54:20.064977: step 7699, loss 0.0458448, acc 0.984375, prec 0.102935, recall 0.819084
2017-12-10T04:54:20.363187: step 7700, loss 1.44321, acc 0.875, prec 0.102926, recall 0.819084
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-7700

2017-12-10T04:54:21.638716: step 7701, loss 0.22064, acc 0.953125, prec 0.102932, recall 0.8191
2017-12-10T04:54:21.915939: step 7702, loss 0.157574, acc 0.96875, prec 0.10294, recall 0.819117
2017-12-10T04:54:22.184947: step 7703, loss 0.615819, acc 0.921875, prec 0.102934, recall 0.819117
2017-12-10T04:54:22.452963: step 7704, loss 0.139898, acc 0.984375, prec 0.102933, recall 0.819117
2017-12-10T04:54:22.723894: step 7705, loss 0.00102971, acc 1, prec 0.102964, recall 0.819166
2017-12-10T04:54:23.004275: step 7706, loss 0.0782652, acc 0.984375, prec 0.102973, recall 0.819183
2017-12-10T04:54:23.278928: step 7707, loss 0.000532976, acc 1, prec 0.103014, recall 0.819249
2017-12-10T04:54:23.539323: step 7708, loss 0.0855317, acc 0.984375, prec 0.103023, recall 0.819265
2017-12-10T04:54:23.809701: step 7709, loss 0.0108122, acc 1, prec 0.103023, recall 0.819265
2017-12-10T04:54:24.075793: step 7710, loss 0.246956, acc 0.96875, prec 0.103021, recall 0.819265
2017-12-10T04:54:24.346047: step 7711, loss 0.0289396, acc 0.984375, prec 0.10303, recall 0.819282
2017-12-10T04:54:24.619361: step 7712, loss 0.149342, acc 0.96875, prec 0.103027, recall 0.819282
2017-12-10T04:54:24.880641: step 7713, loss 0.000679442, acc 1, prec 0.103027, recall 0.819282
2017-12-10T04:54:25.144342: step 7714, loss 3.361, acc 0.96875, prec 0.103047, recall 0.81924
2017-12-10T04:54:25.411505: step 7715, loss 0.128667, acc 1, prec 0.103067, recall 0.819273
2017-12-10T04:54:25.680669: step 7716, loss 2.18111, acc 0.953125, prec 0.103065, recall 0.819198
2017-12-10T04:54:25.949209: step 7717, loss 0.0707586, acc 0.984375, prec 0.103074, recall 0.819215
2017-12-10T04:54:26.215120: step 7718, loss 0.192839, acc 0.96875, prec 0.103072, recall 0.819215
2017-12-10T04:54:26.480868: step 7719, loss 0.39273, acc 0.921875, prec 0.103076, recall 0.819231
2017-12-10T04:54:26.746946: step 7720, loss 0.740728, acc 0.953125, prec 0.103083, recall 0.819248
2017-12-10T04:54:27.021596: step 7721, loss 0.34397, acc 0.9375, prec 0.103088, recall 0.819264
2017-12-10T04:54:27.290867: step 7722, loss 0.367865, acc 0.96875, prec 0.103106, recall 0.819297
2017-12-10T04:54:27.560846: step 7723, loss 0.583979, acc 0.953125, prec 0.103144, recall 0.819362
2017-12-10T04:54:27.823858: step 7724, loss 0.0257313, acc 0.984375, prec 0.103153, recall 0.819379
2017-12-10T04:54:28.089213: step 7725, loss 0.882033, acc 0.875, prec 0.103164, recall 0.819412
2017-12-10T04:54:28.360151: step 7726, loss 1.00676, acc 0.9375, prec 0.10317, recall 0.819428
2017-12-10T04:54:28.625708: step 7727, loss 0.472482, acc 0.921875, prec 0.103184, recall 0.819461
2017-12-10T04:54:28.900679: step 7728, loss 0.974361, acc 0.828125, prec 0.103171, recall 0.819461
2017-12-10T04:54:29.170062: step 7729, loss 0.5917, acc 0.875, prec 0.103162, recall 0.819461
2017-12-10T04:54:29.435812: step 7730, loss 0.58858, acc 0.859375, prec 0.103151, recall 0.819461
2017-12-10T04:54:29.705497: step 7731, loss 0.368778, acc 0.953125, prec 0.103148, recall 0.819461
2017-12-10T04:54:29.975120: step 7732, loss 0.446469, acc 0.890625, prec 0.10315, recall 0.819477
2017-12-10T04:54:30.243083: step 7733, loss 1.11418, acc 0.921875, prec 0.103144, recall 0.819477
2017-12-10T04:54:30.516515: step 7734, loss 0.631162, acc 0.921875, prec 0.103138, recall 0.819477
2017-12-10T04:54:30.790468: step 7735, loss 1.17076, acc 0.875, prec 0.103139, recall 0.819494
2017-12-10T04:54:31.057311: step 7736, loss 1.00805, acc 0.890625, prec 0.103151, recall 0.819526
2017-12-10T04:54:31.325792: step 7737, loss 0.202809, acc 0.953125, prec 0.103158, recall 0.819543
2017-12-10T04:54:31.593839: step 7738, loss 0.662917, acc 0.890625, prec 0.10317, recall 0.819575
2017-12-10T04:54:31.859501: step 7739, loss 0.386961, acc 0.90625, prec 0.103163, recall 0.819575
2017-12-10T04:54:32.120699: step 7740, loss 0.472292, acc 0.953125, prec 0.10317, recall 0.819592
2017-12-10T04:54:32.386286: step 7741, loss 0.0137815, acc 1, prec 0.10319, recall 0.819625
2017-12-10T04:54:32.655995: step 7742, loss 0.119648, acc 0.96875, prec 0.103229, recall 0.81969
2017-12-10T04:54:32.921623: step 7743, loss 0.194906, acc 0.96875, prec 0.103226, recall 0.81969
2017-12-10T04:54:33.189674: step 7744, loss 0.107155, acc 0.96875, prec 0.103234, recall 0.819706
2017-12-10T04:54:33.458365: step 7745, loss 0.0472956, acc 0.984375, prec 0.103233, recall 0.819706
2017-12-10T04:54:33.723001: step 7746, loss 10.0741, acc 0.96875, prec 0.103242, recall 0.819648
2017-12-10T04:54:33.995312: step 7747, loss 0.12564, acc 0.984375, prec 0.103241, recall 0.819648
2017-12-10T04:54:34.259336: step 7748, loss 0.0222839, acc 0.984375, prec 0.10326, recall 0.819681
2017-12-10T04:54:34.528662: step 7749, loss 0.401144, acc 0.96875, prec 0.103258, recall 0.819681
2017-12-10T04:54:34.793404: step 7750, loss 0.371266, acc 0.953125, prec 0.103254, recall 0.819681
2017-12-10T04:54:35.064031: step 7751, loss 0.651034, acc 0.921875, prec 0.103259, recall 0.819697
2017-12-10T04:54:35.327594: step 7752, loss 0.843629, acc 0.921875, prec 0.103263, recall 0.819714
2017-12-10T04:54:35.591655: step 7753, loss 0.616333, acc 0.90625, prec 0.103266, recall 0.81973
2017-12-10T04:54:35.859387: step 7754, loss 0.680367, acc 0.921875, prec 0.103281, recall 0.819763
2017-12-10T04:54:36.131427: step 7755, loss 0.356142, acc 0.953125, prec 0.103277, recall 0.819763
2017-12-10T04:54:36.395135: step 7756, loss 0.761249, acc 0.9375, prec 0.103272, recall 0.819763
2017-12-10T04:54:36.664200: step 7757, loss 0.0491381, acc 0.984375, prec 0.103271, recall 0.819763
2017-12-10T04:54:36.928767: step 7758, loss 0.759732, acc 0.953125, prec 0.103278, recall 0.819779
2017-12-10T04:54:37.203675: step 7759, loss 0.142527, acc 0.953125, prec 0.103285, recall 0.819795
2017-12-10T04:54:37.469112: step 7760, loss 0.321007, acc 0.921875, prec 0.103299, recall 0.819828
2017-12-10T04:54:37.734802: step 7761, loss 0.482226, acc 0.96875, prec 0.103317, recall 0.819861
2017-12-10T04:54:38.000073: step 7762, loss 0.33123, acc 0.9375, prec 0.103323, recall 0.819877
2017-12-10T04:54:38.262669: step 7763, loss 0.779615, acc 0.875, prec 0.103313, recall 0.819877
2017-12-10T04:54:38.528423: step 7764, loss 0.550454, acc 0.921875, prec 0.103318, recall 0.819893
2017-12-10T04:54:38.794324: step 7765, loss 0.506928, acc 0.953125, prec 0.103324, recall 0.81991
2017-12-10T04:54:39.058133: step 7766, loss 0.0430306, acc 0.984375, prec 0.103323, recall 0.81991
2017-12-10T04:54:39.326112: step 7767, loss 0.0571406, acc 0.984375, prec 0.103332, recall 0.819926
2017-12-10T04:54:39.591454: step 7768, loss 0.424329, acc 0.953125, prec 0.103329, recall 0.819926
2017-12-10T04:54:39.858663: step 7769, loss 0.00033708, acc 1, prec 0.103349, recall 0.819958
2017-12-10T04:54:40.127369: step 7770, loss 0.194863, acc 0.953125, prec 0.103346, recall 0.819958
2017-12-10T04:54:40.396507: step 7771, loss 0.0433393, acc 0.984375, prec 0.103365, recall 0.819991
2017-12-10T04:54:40.659700: step 7772, loss 0.0803058, acc 0.984375, prec 0.103405, recall 0.820056
2017-12-10T04:54:40.932122: step 7773, loss 0.145117, acc 0.984375, prec 0.103403, recall 0.820056
2017-12-10T04:54:41.197503: step 7774, loss 5.21418e-05, acc 1, prec 0.103414, recall 0.820072
2017-12-10T04:54:41.469343: step 7775, loss 0.14916, acc 0.96875, prec 0.103411, recall 0.820072
2017-12-10T04:54:41.729176: step 7776, loss 0.000906916, acc 1, prec 0.103442, recall 0.820121
2017-12-10T04:54:42.005030: step 7777, loss 0.143715, acc 0.984375, prec 0.103441, recall 0.820121
2017-12-10T04:54:42.277239: step 7778, loss 0.217196, acc 0.984375, prec 0.10346, recall 0.820154
2017-12-10T04:54:42.545120: step 7779, loss 0.0926795, acc 0.984375, prec 0.103479, recall 0.820186
2017-12-10T04:54:42.816254: step 7780, loss 0.0166538, acc 1, prec 0.103479, recall 0.820186
2017-12-10T04:54:43.086383: step 7781, loss 0.0183986, acc 1, prec 0.10349, recall 0.820202
2017-12-10T04:54:43.353867: step 7782, loss 0.0674212, acc 0.984375, prec 0.103499, recall 0.820219
2017-12-10T04:54:43.612922: step 7783, loss 0.124744, acc 0.96875, prec 0.103496, recall 0.820219
2017-12-10T04:54:43.877274: step 7784, loss 7.13287e-05, acc 1, prec 0.103506, recall 0.820235
2017-12-10T04:54:44.135126: step 7785, loss 0.0259958, acc 0.984375, prec 0.103515, recall 0.820251
2017-12-10T04:54:44.403621: step 7786, loss 0.227424, acc 0.953125, prec 0.103512, recall 0.820251
2017-12-10T04:54:44.667494: step 7787, loss 0.159982, acc 0.984375, prec 0.103521, recall 0.820267
2017-12-10T04:54:44.933276: step 7788, loss 1.63884, acc 0.96875, prec 0.10352, recall 0.820193
2017-12-10T04:54:45.206844: step 7789, loss 0.0504812, acc 0.984375, prec 0.103529, recall 0.82021
2017-12-10T04:54:45.470200: step 7790, loss 0.195345, acc 0.984375, prec 0.103538, recall 0.820226
2017-12-10T04:54:45.733559: step 7791, loss 0.0483663, acc 0.984375, prec 0.103537, recall 0.820226
2017-12-10T04:54:45.996232: step 7792, loss 0.0363917, acc 0.984375, prec 0.103546, recall 0.820242
2017-12-10T04:54:46.265686: step 7793, loss 1.90539, acc 0.96875, prec 0.103545, recall 0.820168
2017-12-10T04:54:46.535884: step 7794, loss 0.00286395, acc 1, prec 0.103565, recall 0.8202
2017-12-10T04:54:46.805206: step 7795, loss 0.525724, acc 0.890625, prec 0.103557, recall 0.8202
2017-12-10T04:54:47.075611: step 7796, loss 0.0508784, acc 0.984375, prec 0.103556, recall 0.8202
2017-12-10T04:54:47.342257: step 7797, loss 0.13991, acc 0.953125, prec 0.103562, recall 0.820217
2017-12-10T04:54:47.608148: step 7798, loss 0.168813, acc 0.96875, prec 0.10356, recall 0.820217
2017-12-10T04:54:47.877431: step 7799, loss 0.334787, acc 0.96875, prec 0.103568, recall 0.820233
2017-12-10T04:54:48.144108: step 7800, loss 0.435548, acc 0.953125, prec 0.103564, recall 0.820233

Evaluation:
2017-12-10T04:54:55.734612: step 7800, loss 6.79457, acc 0.93244, prec 0.103616, recall 0.816546

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-7800

2017-12-10T04:54:56.946773: step 7801, loss 0.253795, acc 0.9375, prec 0.103622, recall 0.816563
2017-12-10T04:54:57.223411: step 7802, loss 0.831607, acc 0.90625, prec 0.103625, recall 0.816579
2017-12-10T04:54:57.487133: step 7803, loss 0.779848, acc 0.90625, prec 0.103628, recall 0.816595
2017-12-10T04:54:57.754594: step 7804, loss 0.309645, acc 0.953125, prec 0.103645, recall 0.816628
2017-12-10T04:54:58.028100: step 7805, loss 0.23673, acc 0.953125, prec 0.103641, recall 0.816628
2017-12-10T04:54:58.299199: step 7806, loss 0.684716, acc 0.9375, prec 0.103647, recall 0.816644
2017-12-10T04:54:58.566805: step 7807, loss 0.0612485, acc 0.984375, prec 0.103656, recall 0.816661
2017-12-10T04:54:58.836441: step 7808, loss 0.662368, acc 0.9375, prec 0.103651, recall 0.816661
2017-12-10T04:54:59.103085: step 7809, loss 1.02368, acc 0.90625, prec 0.103644, recall 0.816661
2017-12-10T04:54:59.368354: step 7810, loss 0.226216, acc 0.96875, prec 0.103662, recall 0.816693
2017-12-10T04:54:59.636439: step 7811, loss 0.574428, acc 0.921875, prec 0.103656, recall 0.816693
2017-12-10T04:54:59.901406: step 7812, loss 0.20963, acc 0.984375, prec 0.103685, recall 0.816742
2017-12-10T04:55:00.169130: step 7813, loss 0.486866, acc 0.953125, prec 0.103682, recall 0.816742
2017-12-10T04:55:00.438166: step 7814, loss 0.329288, acc 0.921875, prec 0.103676, recall 0.816742
2017-12-10T04:55:00.707009: step 7815, loss 0.230438, acc 0.953125, prec 0.103672, recall 0.816742
2017-12-10T04:55:00.980609: step 7816, loss 0.111162, acc 0.984375, prec 0.103671, recall 0.816742
2017-12-10T04:55:01.244393: step 7817, loss 0.0919412, acc 0.984375, prec 0.10369, recall 0.816775
2017-12-10T04:55:01.510003: step 7818, loss 0.128813, acc 0.96875, prec 0.103688, recall 0.816775
2017-12-10T04:55:01.772483: step 7819, loss 0.183929, acc 0.96875, prec 0.103696, recall 0.816791
2017-12-10T04:55:02.034763: step 7820, loss 0.142862, acc 0.984375, prec 0.103694, recall 0.816791
2017-12-10T04:55:02.303627: step 7821, loss 0.367418, acc 0.9375, prec 0.1037, recall 0.816807
2017-12-10T04:55:02.576028: step 7822, loss 0.188445, acc 0.953125, prec 0.103727, recall 0.816856
2017-12-10T04:55:02.843496: step 7823, loss 0.177342, acc 0.953125, prec 0.103733, recall 0.816873
2017-12-10T04:55:03.104973: step 7824, loss 0.18137, acc 0.96875, prec 0.103741, recall 0.816889
2017-12-10T04:55:03.380763: step 7825, loss 0.0529254, acc 0.984375, prec 0.10376, recall 0.816921
2017-12-10T04:55:03.643954: step 7826, loss 0.585914, acc 0.9375, prec 0.103765, recall 0.816938
2017-12-10T04:55:03.903621: step 7827, loss 0.0830214, acc 0.984375, prec 0.103764, recall 0.816938
2017-12-10T04:55:04.168782: step 7828, loss 0.0838019, acc 0.984375, prec 0.103783, recall 0.81697
2017-12-10T04:55:04.432135: step 7829, loss 0.0551345, acc 0.984375, prec 0.103782, recall 0.81697
2017-12-10T04:55:04.694608: step 7830, loss 0.0554824, acc 0.984375, prec 0.103781, recall 0.81697
2017-12-10T04:55:04.958504: step 7831, loss 0.292769, acc 0.984375, prec 0.10379, recall 0.816987
2017-12-10T04:55:05.231155: step 7832, loss 0.129964, acc 0.984375, prec 0.103799, recall 0.817003
2017-12-10T04:55:05.494167: step 7833, loss 0.0634405, acc 0.96875, prec 0.103797, recall 0.817003
2017-12-10T04:55:05.769281: step 7834, loss 0.00798234, acc 1, prec 0.103797, recall 0.817003
2017-12-10T04:55:06.034332: step 7835, loss 3.06562, acc 0.984375, prec 0.103797, recall 0.81693
2017-12-10T04:55:06.306127: step 7836, loss 0.185813, acc 0.984375, prec 0.103806, recall 0.816946
2017-12-10T04:55:06.573139: step 7837, loss 0.212883, acc 0.96875, prec 0.103813, recall 0.816963
2017-12-10T04:55:06.837120: step 7838, loss 0.0246936, acc 0.984375, prec 0.103812, recall 0.816963
2017-12-10T04:55:07.103872: step 7839, loss 1.11705, acc 0.984375, prec 0.103821, recall 0.816979
2017-12-10T04:55:07.371363: step 7840, loss 0.410988, acc 0.96875, prec 0.103819, recall 0.816979
2017-12-10T04:55:07.633761: step 7841, loss 0.173829, acc 0.96875, prec 0.103816, recall 0.816979
2017-12-10T04:55:07.900087: step 7842, loss 0.307319, acc 0.9375, prec 0.103822, recall 0.816995
2017-12-10T04:55:08.163228: step 7843, loss 0.0739272, acc 0.984375, prec 0.103821, recall 0.816995
2017-12-10T04:55:08.434901: step 7844, loss 0.997555, acc 0.890625, prec 0.103812, recall 0.816995
2017-12-10T04:55:08.702147: step 7845, loss 0.241653, acc 0.96875, prec 0.10381, recall 0.816995
2017-12-10T04:55:08.967333: step 7846, loss 0.554218, acc 0.90625, prec 0.103813, recall 0.817011
2017-12-10T04:55:09.233845: step 7847, loss 0.605428, acc 0.890625, prec 0.103805, recall 0.817011
2017-12-10T04:55:09.500909: step 7848, loss 0.223735, acc 0.953125, prec 0.103801, recall 0.817011
2017-12-10T04:55:09.767879: step 7849, loss 0.284572, acc 0.9375, prec 0.103797, recall 0.817011
2017-12-10T04:55:10.033267: step 7850, loss 0.862167, acc 0.875, prec 0.103787, recall 0.817011
2017-12-10T04:55:10.296072: step 7851, loss 1.0526, acc 0.921875, prec 0.103792, recall 0.817028
2017-12-10T04:55:10.566480: step 7852, loss 0.60494, acc 0.9375, prec 0.103797, recall 0.817044
2017-12-10T04:55:10.835056: step 7853, loss 0.391718, acc 0.921875, prec 0.103811, recall 0.817076
2017-12-10T04:55:11.102490: step 7854, loss 1.04969, acc 0.90625, prec 0.103835, recall 0.817125
2017-12-10T04:55:11.370683: step 7855, loss 1.12324, acc 0.9375, prec 0.10384, recall 0.817141
2017-12-10T04:55:11.637158: step 7856, loss 0.385221, acc 0.9375, prec 0.103866, recall 0.81719
2017-12-10T04:55:11.901854: step 7857, loss 0.624712, acc 0.9375, prec 0.103861, recall 0.81719
2017-12-10T04:55:12.185549: step 7858, loss 0.479003, acc 0.890625, prec 0.103863, recall 0.817206
2017-12-10T04:55:12.449668: step 7859, loss 0.597001, acc 0.90625, prec 0.103866, recall 0.817222
2017-12-10T04:55:12.716778: step 7860, loss 0.0934961, acc 0.96875, prec 0.103884, recall 0.817255
2017-12-10T04:55:12.984622: step 7861, loss 0.358017, acc 0.9375, prec 0.103889, recall 0.817271
2017-12-10T04:55:13.245592: step 7862, loss 0.431941, acc 0.90625, prec 0.103892, recall 0.817287
2017-12-10T04:55:13.512109: step 7863, loss 0.0677496, acc 0.984375, prec 0.103891, recall 0.817287
2017-12-10T04:55:13.773474: step 7864, loss 0.849627, acc 0.890625, prec 0.103893, recall 0.817303
2017-12-10T04:55:14.041150: step 7865, loss 0.313279, acc 0.9375, prec 0.103899, recall 0.81732
2017-12-10T04:55:14.306648: step 7866, loss 0.718446, acc 0.921875, prec 0.103893, recall 0.81732
2017-12-10T04:55:14.572550: step 7867, loss 0.634288, acc 0.9375, prec 0.103898, recall 0.817336
2017-12-10T04:55:14.841181: step 7868, loss 0.35799, acc 0.953125, prec 0.103895, recall 0.817336
2017-12-10T04:55:15.110063: step 7869, loss 0.251124, acc 0.953125, prec 0.103932, recall 0.817401
2017-12-10T04:55:15.372315: step 7870, loss 0.00226695, acc 1, prec 0.103932, recall 0.817401
2017-12-10T04:55:15.635642: step 7871, loss 0.1409, acc 0.984375, prec 0.103951, recall 0.817433
2017-12-10T04:55:15.908811: step 7872, loss 0.205032, acc 0.96875, prec 0.103958, recall 0.817449
2017-12-10T04:55:16.174074: step 7873, loss 0.0569784, acc 0.984375, prec 0.103957, recall 0.817449
2017-12-10T04:55:16.443035: step 7874, loss 0.345494, acc 0.9375, prec 0.103963, recall 0.817465
2017-12-10T04:55:16.707065: step 7875, loss 0.12634, acc 0.984375, prec 0.103971, recall 0.817481
2017-12-10T04:55:16.969289: step 7876, loss 0.207664, acc 0.984375, prec 0.10399, recall 0.817514
2017-12-10T04:55:17.238983: step 7877, loss 0.260805, acc 0.984375, prec 0.103989, recall 0.817514
2017-12-10T04:55:17.505538: step 7878, loss 0.0985503, acc 0.984375, prec 0.103998, recall 0.81753
2017-12-10T04:55:17.767907: step 7879, loss 0.18354, acc 0.96875, prec 0.104016, recall 0.817562
2017-12-10T04:55:18.034093: step 7880, loss 0.000443266, acc 1, prec 0.104016, recall 0.817562
2017-12-10T04:55:18.294502: step 7881, loss 0.509318, acc 0.953125, prec 0.104023, recall 0.817578
2017-12-10T04:55:18.556723: step 7882, loss 0.00048667, acc 1, prec 0.104023, recall 0.817578
2017-12-10T04:55:18.817317: step 7883, loss 0.130115, acc 0.96875, prec 0.10403, recall 0.817594
2017-12-10T04:55:19.088351: step 7884, loss 0.0129598, acc 1, prec 0.10404, recall 0.817611
2017-12-10T04:55:19.353650: step 7885, loss 1.17688, acc 0.984375, prec 0.10408, recall 0.817675
2017-12-10T04:55:19.621563: step 7886, loss 0.00681037, acc 1, prec 0.10408, recall 0.817675
2017-12-10T04:55:19.887343: step 7887, loss 0.0413148, acc 0.984375, prec 0.104089, recall 0.817691
2017-12-10T04:55:20.155949: step 7888, loss 0.0192838, acc 1, prec 0.104109, recall 0.817724
2017-12-10T04:55:20.426400: step 7889, loss 0.104087, acc 0.984375, prec 0.104118, recall 0.81774
2017-12-10T04:55:20.694290: step 7890, loss 0.0256209, acc 0.984375, prec 0.104116, recall 0.81774
2017-12-10T04:55:20.958968: step 7891, loss 0.105279, acc 0.984375, prec 0.104135, recall 0.817772
2017-12-10T04:55:21.230716: step 7892, loss 0.484391, acc 0.984375, prec 0.104134, recall 0.817772
2017-12-10T04:55:21.496272: step 7893, loss 0.112657, acc 0.984375, prec 0.104143, recall 0.817788
2017-12-10T04:55:21.763560: step 7894, loss 0.0473781, acc 0.96875, prec 0.104151, recall 0.817804
2017-12-10T04:55:22.027664: step 7895, loss 0.0156618, acc 1, prec 0.104151, recall 0.817804
2017-12-10T04:55:22.289606: step 7896, loss 0.119308, acc 0.984375, prec 0.10417, recall 0.817836
2017-12-10T04:55:22.561408: step 7897, loss 0.0298191, acc 0.984375, prec 0.104199, recall 0.817885
2017-12-10T04:55:22.830555: step 7898, loss 0.231996, acc 0.953125, prec 0.104206, recall 0.817901
2017-12-10T04:55:23.097470: step 7899, loss 0.0508374, acc 0.984375, prec 0.104215, recall 0.817917
2017-12-10T04:55:23.364293: step 7900, loss 0.175429, acc 0.96875, prec 0.104232, recall 0.817949
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-7900

2017-12-10T04:55:24.613105: step 7901, loss 0.000312303, acc 1, prec 0.104232, recall 0.817949
2017-12-10T04:55:24.875392: step 7902, loss 0.0740266, acc 0.96875, prec 0.10423, recall 0.817949
2017-12-10T04:55:25.142907: step 7903, loss 0.150279, acc 0.984375, prec 0.104239, recall 0.817965
2017-12-10T04:55:25.413775: step 7904, loss 0.490839, acc 0.96875, prec 0.104247, recall 0.817981
2017-12-10T04:55:25.677845: step 7905, loss 0.0888197, acc 0.984375, prec 0.104276, recall 0.818029
2017-12-10T04:55:25.940377: step 7906, loss 0.000542751, acc 1, prec 0.104306, recall 0.818078
2017-12-10T04:55:26.203125: step 7907, loss 0.00277867, acc 1, prec 0.104306, recall 0.818078
2017-12-10T04:55:26.470607: step 7908, loss 0.0743954, acc 0.96875, prec 0.104314, recall 0.818094
2017-12-10T04:55:26.736491: step 7909, loss 0.00402605, acc 1, prec 0.104314, recall 0.818094
2017-12-10T04:55:27.010900: step 7910, loss 0.017428, acc 1, prec 0.104314, recall 0.818094
2017-12-10T04:55:27.289760: step 7911, loss 0.242269, acc 0.96875, prec 0.104321, recall 0.81811
2017-12-10T04:55:27.555192: step 7912, loss 0.0972567, acc 0.984375, prec 0.10433, recall 0.818126
2017-12-10T04:55:27.822707: step 7913, loss 0.895891, acc 0.96875, prec 0.104338, recall 0.818142
2017-12-10T04:55:28.090407: step 7914, loss 0.332379, acc 0.953125, prec 0.104345, recall 0.818158
2017-12-10T04:55:28.353536: step 7915, loss 0.00945387, acc 1, prec 0.104355, recall 0.818174
2017-12-10T04:55:28.618705: step 7916, loss 0.6015, acc 0.9375, prec 0.10436, recall 0.81819
2017-12-10T04:55:28.888158: step 7917, loss 0.386084, acc 0.96875, prec 0.104358, recall 0.81819
2017-12-10T04:55:29.155542: step 7918, loss 0.000550962, acc 1, prec 0.104368, recall 0.818206
2017-12-10T04:55:29.422662: step 7919, loss 0.205041, acc 0.984375, prec 0.104367, recall 0.818206
2017-12-10T04:55:29.688516: step 7920, loss 0.302601, acc 0.96875, prec 0.104405, recall 0.81827
2017-12-10T04:55:29.962592: step 7921, loss 0.902261, acc 0.921875, prec 0.104409, recall 0.818286
2017-12-10T04:55:30.230213: step 7922, loss 0.0789203, acc 0.984375, prec 0.104418, recall 0.818302
2017-12-10T04:55:30.500558: step 7923, loss 0.0178966, acc 0.984375, prec 0.104427, recall 0.818318
2017-12-10T04:55:30.770987: step 7924, loss 0.0595839, acc 0.96875, prec 0.104424, recall 0.818318
2017-12-10T04:55:31.044306: step 7925, loss 0.0989399, acc 0.984375, prec 0.104423, recall 0.818318
2017-12-10T04:55:31.317703: step 7926, loss 0.056594, acc 0.984375, prec 0.104432, recall 0.818334
2017-12-10T04:55:31.585693: step 7927, loss 0.133771, acc 0.96875, prec 0.10443, recall 0.818334
2017-12-10T04:55:31.854597: step 7928, loss 0.252683, acc 0.984375, prec 0.104438, recall 0.81835
2017-12-10T04:55:32.123649: step 7929, loss 0.0163539, acc 1, prec 0.104438, recall 0.81835
2017-12-10T04:55:32.388770: step 7930, loss 0.000170829, acc 1, prec 0.104449, recall 0.818366
2017-12-10T04:55:32.647566: step 7931, loss 0.0681824, acc 0.984375, prec 0.104447, recall 0.818366
2017-12-10T04:55:32.918844: step 7932, loss 7.96674e-05, acc 1, prec 0.104457, recall 0.818382
2017-12-10T04:55:33.188761: step 7933, loss 0.00597249, acc 1, prec 0.104468, recall 0.818398
2017-12-10T04:55:33.457112: step 7934, loss 0.144985, acc 0.96875, prec 0.104465, recall 0.818398
2017-12-10T04:55:33.725960: step 7935, loss 0.215246, acc 0.96875, prec 0.104493, recall 0.818446
2017-12-10T04:55:33.997248: step 7936, loss 0.229669, acc 0.984375, prec 0.104512, recall 0.818478
2017-12-10T04:55:34.259378: step 7937, loss 0.0727148, acc 0.96875, prec 0.10451, recall 0.818478
2017-12-10T04:55:34.523980: step 7938, loss 0.00571579, acc 1, prec 0.10451, recall 0.818478
2017-12-10T04:55:34.786696: step 7939, loss 3.53342e-06, acc 1, prec 0.10452, recall 0.818494
2017-12-10T04:55:35.043787: step 7940, loss 0.0276537, acc 0.984375, prec 0.104519, recall 0.818494
2017-12-10T04:55:35.315286: step 7941, loss 0.323913, acc 0.984375, prec 0.104527, recall 0.81851
2017-12-10T04:55:35.580252: step 7942, loss 7.95205e-05, acc 1, prec 0.104548, recall 0.818542
2017-12-10T04:55:35.838613: step 7943, loss 4.09347, acc 0.984375, prec 0.104568, recall 0.818502
2017-12-10T04:55:36.105421: step 7944, loss 0.553644, acc 0.953125, prec 0.104574, recall 0.818518
2017-12-10T04:55:36.369724: step 7945, loss 0.0176707, acc 0.984375, prec 0.104573, recall 0.818518
2017-12-10T04:55:36.632849: step 7946, loss 0.00581843, acc 1, prec 0.104583, recall 0.818534
2017-12-10T04:55:36.896650: step 7947, loss 5.70228, acc 0.953125, prec 0.104581, recall 0.818462
2017-12-10T04:55:37.165066: step 7948, loss 1.32313, acc 0.9375, prec 0.104586, recall 0.818478
2017-12-10T04:55:37.427003: step 7949, loss 0.125065, acc 0.96875, prec 0.104584, recall 0.818478
2017-12-10T04:55:37.694098: step 7950, loss 0.273884, acc 0.9375, prec 0.104589, recall 0.818494
2017-12-10T04:55:37.961468: step 7951, loss 0.877985, acc 0.875, prec 0.10459, recall 0.81851
2017-12-10T04:55:38.195000: step 7952, loss 0.179164, acc 0.923077, prec 0.104595, recall 0.818526
2017-12-10T04:55:38.476661: step 7953, loss 0.296817, acc 0.921875, prec 0.104599, recall 0.818542
2017-12-10T04:55:38.744888: step 7954, loss 1.11977, acc 0.828125, prec 0.104586, recall 0.818542
2017-12-10T04:55:39.008374: step 7955, loss 0.674121, acc 0.875, prec 0.104577, recall 0.818542
2017-12-10T04:55:39.280521: step 7956, loss 0.335507, acc 0.921875, prec 0.104591, recall 0.818574
2017-12-10T04:55:39.547045: step 7957, loss 0.410457, acc 0.9375, prec 0.104607, recall 0.818605
2017-12-10T04:55:39.815088: step 7958, loss 0.401297, acc 0.9375, prec 0.104602, recall 0.818605
2017-12-10T04:55:40.090990: step 7959, loss 0.221733, acc 0.9375, prec 0.104597, recall 0.818605
2017-12-10T04:55:40.357094: step 7960, loss 0.266862, acc 0.953125, prec 0.104594, recall 0.818605
2017-12-10T04:55:40.629766: step 7961, loss 0.615525, acc 0.875, prec 0.104584, recall 0.818605
2017-12-10T04:55:40.905556: step 7962, loss 0.78556, acc 0.875, prec 0.104575, recall 0.818605
2017-12-10T04:55:41.175286: step 7963, loss 0.813048, acc 0.890625, prec 0.104597, recall 0.818653
2017-12-10T04:55:41.440453: step 7964, loss 0.210867, acc 0.96875, prec 0.104605, recall 0.818669
2017-12-10T04:55:41.715639: step 7965, loss 0.325229, acc 0.9375, prec 0.10461, recall 0.818685
2017-12-10T04:55:41.982483: step 7966, loss 0.849827, acc 0.90625, prec 0.104623, recall 0.818717
2017-12-10T04:55:42.257709: step 7967, loss 0.952436, acc 0.890625, prec 0.104615, recall 0.818717
2017-12-10T04:55:42.527543: step 7968, loss 0.429847, acc 0.9375, prec 0.10462, recall 0.818733
2017-12-10T04:55:42.788966: step 7969, loss 0.638332, acc 0.90625, prec 0.104633, recall 0.818765
2017-12-10T04:55:43.053519: step 7970, loss 0.245146, acc 0.9375, prec 0.104649, recall 0.818797
2017-12-10T04:55:43.317493: step 7971, loss 0.513999, acc 0.984375, prec 0.104668, recall 0.818828
2017-12-10T04:55:43.589226: step 7972, loss 0.425813, acc 0.921875, prec 0.104662, recall 0.818828
2017-12-10T04:55:43.853147: step 7973, loss 0.935562, acc 0.90625, prec 0.104655, recall 0.818828
2017-12-10T04:55:44.123606: step 7974, loss 0.263462, acc 0.921875, prec 0.104649, recall 0.818828
2017-12-10T04:55:44.394267: step 7975, loss 0.384035, acc 0.9375, prec 0.104664, recall 0.81886
2017-12-10T04:55:44.659489: step 7976, loss 0.334525, acc 0.9375, prec 0.10466, recall 0.81886
2017-12-10T04:55:44.920893: step 7977, loss 0.443036, acc 0.921875, prec 0.104684, recall 0.818908
2017-12-10T04:55:45.187256: step 7978, loss 0.297278, acc 0.96875, prec 0.104681, recall 0.818908
2017-12-10T04:55:45.453073: step 7979, loss 0.0252185, acc 0.984375, prec 0.10468, recall 0.818908
2017-12-10T04:55:45.724808: step 7980, loss 0.409361, acc 0.921875, prec 0.104684, recall 0.818924
2017-12-10T04:55:45.989448: step 7981, loss 0.709843, acc 0.921875, prec 0.104689, recall 0.81894
2017-12-10T04:55:46.254236: step 7982, loss 0.0961721, acc 0.984375, prec 0.104697, recall 0.818956
2017-12-10T04:55:46.527094: step 7983, loss 0.0137648, acc 0.984375, prec 0.104706, recall 0.818972
2017-12-10T04:55:46.802491: step 7984, loss 0.185939, acc 0.96875, prec 0.104714, recall 0.818987
2017-12-10T04:55:47.066528: step 7985, loss 0.170381, acc 0.953125, prec 0.104731, recall 0.819019
2017-12-10T04:55:47.329456: step 7986, loss 0.216096, acc 0.953125, prec 0.104737, recall 0.819035
2017-12-10T04:55:47.595250: step 7987, loss 0.114759, acc 0.984375, prec 0.104736, recall 0.819035
2017-12-10T04:55:47.864128: step 7988, loss 4.08472, acc 0.96875, prec 0.104735, recall 0.818963
2017-12-10T04:55:48.142310: step 7989, loss 0.111242, acc 0.96875, prec 0.104773, recall 0.819027
2017-12-10T04:55:48.406252: step 7990, loss 0.558679, acc 0.953125, prec 0.104789, recall 0.819058
2017-12-10T04:55:48.668535: step 7991, loss 0.623911, acc 0.9375, prec 0.104784, recall 0.819058
2017-12-10T04:55:48.939875: step 7992, loss 0.466085, acc 0.96875, prec 0.104792, recall 0.819074
2017-12-10T04:55:49.209143: step 7993, loss 0.0797958, acc 0.984375, prec 0.104811, recall 0.819106
2017-12-10T04:55:49.483029: step 7994, loss 0.317515, acc 0.921875, prec 0.104805, recall 0.819106
2017-12-10T04:55:49.752997: step 7995, loss 0.113971, acc 0.96875, prec 0.104803, recall 0.819106
2017-12-10T04:55:50.016478: step 7996, loss 0.17326, acc 0.953125, prec 0.104809, recall 0.819122
2017-12-10T04:55:50.283000: step 7997, loss 0.112608, acc 0.96875, prec 0.104817, recall 0.819138
2017-12-10T04:55:50.553311: step 7998, loss 0.304861, acc 0.953125, prec 0.104813, recall 0.819138
2017-12-10T04:55:50.817996: step 7999, loss 0.312887, acc 0.96875, prec 0.104811, recall 0.819138
2017-12-10T04:55:51.085235: step 8000, loss 0.145185, acc 0.953125, prec 0.104828, recall 0.819169
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-8000

2017-12-10T04:55:52.375699: step 8001, loss 0.0684438, acc 0.96875, prec 0.104845, recall 0.819201
2017-12-10T04:55:52.642771: step 8002, loss 0.691229, acc 0.953125, prec 0.104852, recall 0.819217
2017-12-10T04:55:52.913578: step 8003, loss 0.756966, acc 0.9375, prec 0.104857, recall 0.819233
2017-12-10T04:55:53.187568: step 8004, loss 0.977571, acc 0.90625, prec 0.10487, recall 0.819264
2017-12-10T04:55:53.450644: step 8005, loss 0.0654994, acc 0.953125, prec 0.104887, recall 0.819296
2017-12-10T04:55:53.718225: step 8006, loss 0.579898, acc 0.921875, prec 0.104891, recall 0.819312
2017-12-10T04:55:53.983773: step 8007, loss 0.204275, acc 0.96875, prec 0.104889, recall 0.819312
2017-12-10T04:55:54.252872: step 8008, loss 0.339667, acc 0.953125, prec 0.104895, recall 0.819328
2017-12-10T04:55:54.518363: step 8009, loss 0.27007, acc 0.96875, prec 0.104903, recall 0.819344
2017-12-10T04:55:54.783009: step 8010, loss 0.495027, acc 0.953125, prec 0.104899, recall 0.819344
2017-12-10T04:55:55.047785: step 8011, loss 0.369771, acc 0.90625, prec 0.104892, recall 0.819344
2017-12-10T04:55:55.321436: step 8012, loss 0.189197, acc 0.921875, prec 0.104896, recall 0.819359
2017-12-10T04:55:55.594624: step 8013, loss 0.81891, acc 0.953125, prec 0.104903, recall 0.819375
2017-12-10T04:55:55.860743: step 8014, loss 0.119926, acc 0.953125, prec 0.104909, recall 0.819391
2017-12-10T04:55:56.130419: step 8015, loss 0.285549, acc 0.96875, prec 0.104917, recall 0.819407
2017-12-10T04:55:56.409636: step 8016, loss 0.253876, acc 0.96875, prec 0.104915, recall 0.819407
2017-12-10T04:55:56.685207: step 8017, loss 0.0279207, acc 0.96875, prec 0.104922, recall 0.819423
2017-12-10T04:55:56.953776: step 8018, loss 0.0266191, acc 0.984375, prec 0.104931, recall 0.819438
2017-12-10T04:55:57.228601: step 8019, loss 0.581898, acc 0.953125, prec 0.104938, recall 0.819454
2017-12-10T04:55:57.495626: step 8020, loss 0.338831, acc 0.921875, prec 0.104952, recall 0.819486
2017-12-10T04:55:57.777942: step 8021, loss 0.427474, acc 0.953125, prec 0.104968, recall 0.819517
2017-12-10T04:55:58.043392: step 8022, loss 0.0320376, acc 0.984375, prec 0.104977, recall 0.819533
2017-12-10T04:55:58.310049: step 8023, loss 0.301603, acc 0.96875, prec 0.104975, recall 0.819533
2017-12-10T04:55:58.577785: step 8024, loss 0.0840846, acc 0.984375, prec 0.104974, recall 0.819533
2017-12-10T04:55:58.846822: step 8025, loss 0.155501, acc 0.984375, prec 0.104993, recall 0.819565
2017-12-10T04:55:59.115240: step 8026, loss 0.668838, acc 0.953125, prec 0.105019, recall 0.819612
2017-12-10T04:55:59.384335: step 8027, loss 0.000203409, acc 1, prec 0.105029, recall 0.819628
2017-12-10T04:55:59.647601: step 8028, loss 0.0694857, acc 1, prec 0.105049, recall 0.819659
2017-12-10T04:55:59.917937: step 8029, loss 0.16145, acc 0.96875, prec 0.105077, recall 0.819706
2017-12-10T04:56:00.187035: step 8030, loss 0.00385227, acc 1, prec 0.105087, recall 0.819722
2017-12-10T04:56:00.465373: step 8031, loss 7.40151e-05, acc 1, prec 0.105097, recall 0.819738
2017-12-10T04:56:00.734097: step 8032, loss 0.00241311, acc 1, prec 0.105117, recall 0.81977
2017-12-10T04:56:01.001675: step 8033, loss 0.0896086, acc 0.984375, prec 0.105126, recall 0.819785
2017-12-10T04:56:01.268449: step 8034, loss 0.00135463, acc 1, prec 0.105136, recall 0.819801
2017-12-10T04:56:01.532570: step 8035, loss 0.179107, acc 0.953125, prec 0.105132, recall 0.819801
2017-12-10T04:56:01.797864: step 8036, loss 0.00100986, acc 1, prec 0.105142, recall 0.819817
2017-12-10T04:56:02.058873: step 8037, loss 0.0827864, acc 1, prec 0.105162, recall 0.819848
2017-12-10T04:56:02.325616: step 8038, loss 7.9977e-05, acc 1, prec 0.105162, recall 0.819848
2017-12-10T04:56:02.586418: step 8039, loss 1.32511, acc 0.984375, prec 0.105172, recall 0.819792
2017-12-10T04:56:02.847930: step 8040, loss 0.0878036, acc 0.96875, prec 0.10517, recall 0.819792
2017-12-10T04:56:03.116839: step 8041, loss 0.509906, acc 0.984375, prec 0.105169, recall 0.819792
2017-12-10T04:56:03.384516: step 8042, loss 0.141456, acc 0.96875, prec 0.105167, recall 0.819792
2017-12-10T04:56:03.655115: step 8043, loss 0.000815033, acc 1, prec 0.105177, recall 0.819808
2017-12-10T04:56:03.923128: step 8044, loss 0.692089, acc 0.96875, prec 0.105174, recall 0.819808
2017-12-10T04:56:04.189341: step 8045, loss 0.31001, acc 0.9375, prec 0.105179, recall 0.819824
2017-12-10T04:56:04.458420: step 8046, loss 0.0153192, acc 0.984375, prec 0.105178, recall 0.819824
2017-12-10T04:56:04.729662: step 8047, loss 0.328309, acc 0.953125, prec 0.105175, recall 0.819824
2017-12-10T04:56:04.997345: step 8048, loss 0.208023, acc 0.953125, prec 0.105181, recall 0.819839
2017-12-10T04:56:05.274091: step 8049, loss 0.348502, acc 0.96875, prec 0.105189, recall 0.819855
2017-12-10T04:56:05.546528: step 8050, loss 0.15803, acc 0.96875, prec 0.105187, recall 0.819855
2017-12-10T04:56:05.816760: step 8051, loss 0.195938, acc 0.96875, prec 0.105234, recall 0.819934
2017-12-10T04:56:06.086775: step 8052, loss 1.33668, acc 0.921875, prec 0.105258, recall 0.819981
2017-12-10T04:56:06.352212: step 8053, loss 0.039956, acc 0.984375, prec 0.105297, recall 0.820044
2017-12-10T04:56:06.618896: step 8054, loss 0.25422, acc 0.9375, prec 0.105303, recall 0.820059
2017-12-10T04:56:06.893925: step 8055, loss 0.251953, acc 0.984375, prec 0.105311, recall 0.820075
2017-12-10T04:56:07.158792: step 8056, loss 0.00344554, acc 1, prec 0.105331, recall 0.820106
2017-12-10T04:56:07.426120: step 8057, loss 0.00682985, acc 1, prec 0.105341, recall 0.820122
2017-12-10T04:56:07.689095: step 8058, loss 0.720851, acc 0.90625, prec 0.105344, recall 0.820138
2017-12-10T04:56:07.950422: step 8059, loss 0.859637, acc 0.984375, prec 0.105353, recall 0.820153
2017-12-10T04:56:08.220203: step 8060, loss 0.90103, acc 0.921875, prec 0.105347, recall 0.820153
2017-12-10T04:56:08.486698: step 8061, loss 0.294725, acc 0.96875, prec 0.105345, recall 0.820153
2017-12-10T04:56:08.754810: step 8062, loss 0.260545, acc 0.953125, prec 0.105371, recall 0.8202
2017-12-10T04:56:09.025046: step 8063, loss 0.00171021, acc 1, prec 0.105371, recall 0.8202
2017-12-10T04:56:09.288646: step 8064, loss 0.0946262, acc 0.96875, prec 0.105369, recall 0.8202
2017-12-10T04:56:09.556637: step 8065, loss 0.0185741, acc 0.984375, prec 0.105378, recall 0.820216
2017-12-10T04:56:09.826868: step 8066, loss 0.0197087, acc 0.984375, prec 0.105387, recall 0.820232
2017-12-10T04:56:10.100269: step 8067, loss 0.0600159, acc 0.984375, prec 0.105386, recall 0.820232
2017-12-10T04:56:10.370559: step 8068, loss 0.00191187, acc 1, prec 0.105396, recall 0.820247
2017-12-10T04:56:10.635694: step 8069, loss 0.135829, acc 0.953125, prec 0.105402, recall 0.820263
2017-12-10T04:56:10.904531: step 8070, loss 0.0115669, acc 1, prec 0.105422, recall 0.820294
2017-12-10T04:56:11.174771: step 8071, loss 0.166707, acc 0.96875, prec 0.10543, recall 0.82031
2017-12-10T04:56:11.439758: step 8072, loss 0.159117, acc 0.984375, prec 0.105449, recall 0.820341
2017-12-10T04:56:11.711482: step 8073, loss 0.529609, acc 0.984375, prec 0.105457, recall 0.820357
2017-12-10T04:56:11.979833: step 8074, loss 0.308828, acc 0.984375, prec 0.105466, recall 0.820372
2017-12-10T04:56:12.252143: step 8075, loss 1.0008, acc 0.9375, prec 0.105471, recall 0.820388
2017-12-10T04:56:12.514847: step 8076, loss 0.0323017, acc 0.984375, prec 0.10547, recall 0.820388
2017-12-10T04:56:12.785628: step 8077, loss 0.41857, acc 0.96875, prec 0.105488, recall 0.820419
2017-12-10T04:56:13.054701: step 8078, loss 0.000212368, acc 1, prec 0.105488, recall 0.820419
2017-12-10T04:56:13.318007: step 8079, loss 0.000691712, acc 1, prec 0.105488, recall 0.820419
2017-12-10T04:56:13.581945: step 8080, loss 0.25092, acc 0.9375, prec 0.105483, recall 0.820419
2017-12-10T04:56:13.844136: step 8081, loss 0.0682814, acc 0.984375, prec 0.105492, recall 0.820435
2017-12-10T04:56:14.112537: step 8082, loss 0.0222189, acc 0.984375, prec 0.105501, recall 0.82045
2017-12-10T04:56:14.382591: step 8083, loss 0.193475, acc 0.96875, prec 0.105509, recall 0.820466
2017-12-10T04:56:14.651310: step 8084, loss 0.243733, acc 0.984375, prec 0.105507, recall 0.820466
2017-12-10T04:56:14.919172: step 8085, loss 0.191112, acc 0.984375, prec 0.105516, recall 0.820482
2017-12-10T04:56:15.186674: step 8086, loss 0.0707932, acc 0.984375, prec 0.105535, recall 0.820513
2017-12-10T04:56:15.459186: step 8087, loss 0.0206747, acc 0.984375, prec 0.105544, recall 0.820528
2017-12-10T04:56:15.732875: step 8088, loss 0.504854, acc 0.96875, prec 0.105551, recall 0.820544
2017-12-10T04:56:16.001964: step 8089, loss 4.88111, acc 0.96875, prec 0.10556, recall 0.820488
2017-12-10T04:56:16.270488: step 8090, loss 0.0564651, acc 0.953125, prec 0.105557, recall 0.820488
2017-12-10T04:56:16.536074: step 8091, loss 0.00520984, acc 1, prec 0.105567, recall 0.820504
2017-12-10T04:56:16.799930: step 8092, loss 0.413258, acc 0.96875, prec 0.105564, recall 0.820504
2017-12-10T04:56:17.072337: step 8093, loss 0.0437869, acc 0.984375, prec 0.105563, recall 0.820504
2017-12-10T04:56:17.336614: step 8094, loss 0.0895811, acc 0.96875, prec 0.105571, recall 0.82052
2017-12-10T04:56:17.600656: step 8095, loss 0.645646, acc 0.96875, prec 0.105588, recall 0.820551
2017-12-10T04:56:17.870406: step 8096, loss 0.281316, acc 0.9375, prec 0.105594, recall 0.820566
2017-12-10T04:56:18.137266: step 8097, loss 0.080414, acc 0.984375, prec 0.105593, recall 0.820566
2017-12-10T04:56:18.410391: step 8098, loss 0.180678, acc 0.984375, prec 0.105601, recall 0.820582
2017-12-10T04:56:18.679329: step 8099, loss 1.01049, acc 0.921875, prec 0.105605, recall 0.820597
2017-12-10T04:56:18.946191: step 8100, loss 0.251678, acc 0.9375, prec 0.105631, recall 0.820644

Evaluation:
2017-12-10T04:56:26.442634: step 8100, loss 9.53442, acc 0.947915, prec 0.105674, recall 0.815291

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-8100

2017-12-10T04:56:27.794241: step 8101, loss 0.377747, acc 0.9375, prec 0.105669, recall 0.815291
2017-12-10T04:56:28.062988: step 8102, loss 0.0602824, acc 0.96875, prec 0.105696, recall 0.815338
2017-12-10T04:56:28.337530: step 8103, loss 0.197681, acc 0.96875, prec 0.105694, recall 0.815338
2017-12-10T04:56:28.608420: step 8104, loss 0.0339623, acc 1, prec 0.105694, recall 0.815338
2017-12-10T04:56:28.889040: step 8105, loss 0.0308161, acc 0.984375, prec 0.105693, recall 0.815338
2017-12-10T04:56:29.152862: step 8106, loss 0.102028, acc 0.984375, prec 0.105702, recall 0.815354
2017-12-10T04:56:29.417503: step 8107, loss 0.0809733, acc 0.984375, prec 0.10572, recall 0.815386
2017-12-10T04:56:29.678771: step 8108, loss 0.0932447, acc 0.9375, prec 0.105726, recall 0.815402
2017-12-10T04:56:29.942811: step 8109, loss 0.135384, acc 0.984375, prec 0.105724, recall 0.815402
2017-12-10T04:56:30.205597: step 8110, loss 0.12595, acc 0.984375, prec 0.105723, recall 0.815402
2017-12-10T04:56:30.476208: step 8111, loss 0.421176, acc 0.96875, prec 0.105741, recall 0.815433
2017-12-10T04:56:30.744057: step 8112, loss 0.210685, acc 0.984375, prec 0.105749, recall 0.815449
2017-12-10T04:56:31.008094: step 8113, loss 0.162918, acc 0.96875, prec 0.105757, recall 0.815465
2017-12-10T04:56:31.269131: step 8114, loss 0.37145, acc 0.96875, prec 0.105755, recall 0.815465
2017-12-10T04:56:31.535789: step 8115, loss 0.000835086, acc 1, prec 0.105775, recall 0.815497
2017-12-10T04:56:31.800074: step 8116, loss 0.322075, acc 0.953125, prec 0.105771, recall 0.815497
2017-12-10T04:56:32.064907: step 8117, loss 0.293686, acc 0.984375, prec 0.10578, recall 0.815512
2017-12-10T04:56:32.335138: step 8118, loss 0.191985, acc 0.96875, prec 0.105777, recall 0.815512
2017-12-10T04:56:32.597961: step 8119, loss 0.652973, acc 0.953125, prec 0.105774, recall 0.815512
2017-12-10T04:56:32.864967: step 8120, loss 0.494156, acc 0.921875, prec 0.105788, recall 0.815544
2017-12-10T04:56:33.137367: step 8121, loss 2.19112, acc 0.96875, prec 0.105787, recall 0.815474
2017-12-10T04:56:33.408767: step 8122, loss 0.647487, acc 0.890625, prec 0.105779, recall 0.815474
2017-12-10T04:56:33.670078: step 8123, loss 0.28175, acc 0.96875, prec 0.105776, recall 0.815474
2017-12-10T04:56:33.936349: step 8124, loss 0.645357, acc 0.96875, prec 0.105784, recall 0.81549
2017-12-10T04:56:34.205021: step 8125, loss 0.0322166, acc 0.96875, prec 0.105791, recall 0.815506
2017-12-10T04:56:34.464229: step 8126, loss 0.0696837, acc 0.96875, prec 0.105829, recall 0.815569
2017-12-10T04:56:34.730233: step 8127, loss 0.222134, acc 0.953125, prec 0.105825, recall 0.815569
2017-12-10T04:56:34.994757: step 8128, loss 0.300499, acc 0.9375, prec 0.10584, recall 0.8156
2017-12-10T04:56:35.258118: step 8129, loss 0.121965, acc 0.953125, prec 0.105847, recall 0.815616
2017-12-10T04:56:35.524719: step 8130, loss 0.0312487, acc 0.984375, prec 0.105855, recall 0.815632
2017-12-10T04:56:35.794201: step 8131, loss 0.935953, acc 0.96875, prec 0.105883, recall 0.815679
2017-12-10T04:56:36.066503: step 8132, loss 0.308219, acc 0.9375, prec 0.105878, recall 0.815679
2017-12-10T04:56:36.335774: step 8133, loss 0.212344, acc 0.96875, prec 0.105876, recall 0.815679
2017-12-10T04:56:36.604894: step 8134, loss 0.129558, acc 0.96875, prec 0.105873, recall 0.815679
2017-12-10T04:56:36.872115: step 8135, loss 0.432087, acc 0.9375, prec 0.105869, recall 0.815679
2017-12-10T04:56:37.132389: step 8136, loss 0.356986, acc 0.9375, prec 0.105864, recall 0.815679
2017-12-10T04:56:37.406398: step 8137, loss 0.430037, acc 0.953125, prec 0.10587, recall 0.815695
2017-12-10T04:56:37.684770: step 8138, loss 0.0253668, acc 0.984375, prec 0.105869, recall 0.815695
2017-12-10T04:56:37.950754: step 8139, loss 0.151098, acc 0.9375, prec 0.105875, recall 0.815711
2017-12-10T04:56:38.221940: step 8140, loss 0.502639, acc 0.875, prec 0.105865, recall 0.815711
2017-12-10T04:56:38.487133: step 8141, loss 0.372539, acc 0.9375, prec 0.10586, recall 0.815711
2017-12-10T04:56:38.756593: step 8142, loss 0.389481, acc 0.921875, prec 0.105864, recall 0.815727
2017-12-10T04:56:39.020227: step 8143, loss 0.117486, acc 0.953125, prec 0.105861, recall 0.815727
2017-12-10T04:56:39.291788: step 8144, loss 0.190553, acc 0.953125, prec 0.105867, recall 0.815742
2017-12-10T04:56:39.557401: step 8145, loss 0.529907, acc 0.921875, prec 0.105861, recall 0.815742
2017-12-10T04:56:39.825664: step 8146, loss 0.371941, acc 0.9375, prec 0.105867, recall 0.815758
2017-12-10T04:56:40.091371: step 8147, loss 0.64522, acc 0.890625, prec 0.105868, recall 0.815774
2017-12-10T04:56:40.359017: step 8148, loss 0.0236337, acc 0.984375, prec 0.105887, recall 0.815805
2017-12-10T04:56:40.630692: step 8149, loss 0.00240288, acc 1, prec 0.105897, recall 0.815821
2017-12-10T04:56:40.898443: step 8150, loss 0.654627, acc 0.921875, prec 0.105901, recall 0.815837
2017-12-10T04:56:41.173768: step 8151, loss 0.0542981, acc 0.984375, prec 0.10591, recall 0.815852
2017-12-10T04:56:41.439699: step 8152, loss 0.200707, acc 0.953125, prec 0.105916, recall 0.815868
2017-12-10T04:56:41.713254: step 8153, loss 0.109447, acc 0.96875, prec 0.105924, recall 0.815884
2017-12-10T04:56:41.980201: step 8154, loss 0.104197, acc 0.9375, prec 0.105919, recall 0.815884
2017-12-10T04:56:42.254525: step 8155, loss 0.0576214, acc 0.984375, prec 0.105918, recall 0.815884
2017-12-10T04:56:42.521815: step 8156, loss 0.159842, acc 0.96875, prec 0.105935, recall 0.815915
2017-12-10T04:56:42.785173: step 8157, loss 0.331941, acc 0.96875, prec 0.105933, recall 0.815915
2017-12-10T04:56:43.051490: step 8158, loss 0.234629, acc 0.96875, prec 0.105941, recall 0.815931
2017-12-10T04:56:43.321302: step 8159, loss 0.0029659, acc 1, prec 0.10596, recall 0.815962
2017-12-10T04:56:43.591169: step 8160, loss 0.0405044, acc 0.984375, prec 0.105969, recall 0.815978
2017-12-10T04:56:43.858616: step 8161, loss 0.0255612, acc 0.984375, prec 0.105978, recall 0.815994
2017-12-10T04:56:44.120812: step 8162, loss 0.753261, acc 0.9375, prec 0.105973, recall 0.815994
2017-12-10T04:56:44.398142: step 8163, loss 0.511327, acc 0.96875, prec 0.105991, recall 0.816025
2017-12-10T04:56:44.661619: step 8164, loss 0.0126792, acc 0.984375, prec 0.105999, recall 0.816041
2017-12-10T04:56:44.924407: step 8165, loss 0.0130377, acc 0.984375, prec 0.105998, recall 0.816041
2017-12-10T04:56:45.194243: step 8166, loss 0.092944, acc 0.96875, prec 0.105996, recall 0.816041
2017-12-10T04:56:45.461523: step 8167, loss 0.00118002, acc 1, prec 0.105996, recall 0.816041
2017-12-10T04:56:45.724563: step 8168, loss 0.0941304, acc 0.953125, prec 0.106022, recall 0.816088
2017-12-10T04:56:45.989564: step 8169, loss 0.0350756, acc 0.984375, prec 0.106031, recall 0.816104
2017-12-10T04:56:46.258231: step 8170, loss 0.15305, acc 0.96875, prec 0.106038, recall 0.816119
2017-12-10T04:56:46.521335: step 8171, loss 0.0490049, acc 0.96875, prec 0.106036, recall 0.816119
2017-12-10T04:56:46.786770: step 8172, loss 0.369338, acc 0.9375, prec 0.106031, recall 0.816119
2017-12-10T04:56:47.051818: step 8173, loss 0.246054, acc 0.96875, prec 0.106029, recall 0.816119
2017-12-10T04:56:47.317402: step 8174, loss 4.44042e-05, acc 1, prec 0.106029, recall 0.816119
2017-12-10T04:56:47.583537: step 8175, loss 0.0315867, acc 0.984375, prec 0.106028, recall 0.816119
2017-12-10T04:56:47.845782: step 8176, loss 0.0730821, acc 0.984375, prec 0.106046, recall 0.816151
2017-12-10T04:56:48.112416: step 8177, loss 0.000319993, acc 1, prec 0.106056, recall 0.816166
2017-12-10T04:56:48.376370: step 8178, loss 0.196758, acc 0.96875, prec 0.106064, recall 0.816182
2017-12-10T04:56:48.638584: step 8179, loss 0.343436, acc 1, prec 0.106094, recall 0.816229
2017-12-10T04:56:48.911066: step 8180, loss 0.00210681, acc 1, prec 0.106094, recall 0.816229
2017-12-10T04:56:49.173934: step 8181, loss 0.00018206, acc 1, prec 0.106094, recall 0.816229
2017-12-10T04:56:49.442817: step 8182, loss 0.000399025, acc 1, prec 0.106123, recall 0.816276
2017-12-10T04:56:49.706463: step 8183, loss 0.0133744, acc 0.984375, prec 0.106132, recall 0.816292
2017-12-10T04:56:49.972400: step 8184, loss 0.000185134, acc 1, prec 0.106132, recall 0.816292
2017-12-10T04:56:50.234246: step 8185, loss 0.110859, acc 1, prec 0.106142, recall 0.816307
2017-12-10T04:56:50.500379: step 8186, loss 0.224249, acc 0.984375, prec 0.106151, recall 0.816323
2017-12-10T04:56:50.768105: step 8187, loss 0.135423, acc 0.984375, prec 0.106149, recall 0.816323
2017-12-10T04:56:51.035282: step 8188, loss 2.41519, acc 0.984375, prec 0.106149, recall 0.816254
2017-12-10T04:56:51.300789: step 8189, loss 0.000427123, acc 1, prec 0.106149, recall 0.816254
2017-12-10T04:56:51.563083: step 8190, loss 0.0714364, acc 0.984375, prec 0.106158, recall 0.816269
2017-12-10T04:56:51.827473: step 8191, loss 0.000945553, acc 1, prec 0.106178, recall 0.8163
2017-12-10T04:56:52.097581: step 8192, loss 0.152879, acc 0.984375, prec 0.106187, recall 0.816316
2017-12-10T04:56:52.367715: step 8193, loss 0.0104364, acc 1, prec 0.106187, recall 0.816316
2017-12-10T04:56:52.640061: step 8194, loss 0.0122045, acc 1, prec 0.106187, recall 0.816316
2017-12-10T04:56:52.902115: step 8195, loss 0.00111094, acc 1, prec 0.106206, recall 0.816347
2017-12-10T04:56:53.177454: step 8196, loss 8.19505e-05, acc 1, prec 0.106206, recall 0.816347
2017-12-10T04:56:53.436701: step 8197, loss 0.00771718, acc 1, prec 0.106236, recall 0.816394
2017-12-10T04:56:53.702823: step 8198, loss 0.0184422, acc 1, prec 0.106256, recall 0.816426
2017-12-10T04:56:53.973573: step 8199, loss 0.0661367, acc 0.984375, prec 0.106275, recall 0.816457
2017-12-10T04:56:54.244937: step 8200, loss 0.409369, acc 0.921875, prec 0.106269, recall 0.816457
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-8200

2017-12-10T04:56:55.658094: step 8201, loss 0.224951, acc 0.9375, prec 0.106284, recall 0.816488
2017-12-10T04:56:55.927175: step 8202, loss 0.533903, acc 0.953125, prec 0.10631, recall 0.816535
2017-12-10T04:56:56.189005: step 8203, loss 0.0606502, acc 0.984375, prec 0.106319, recall 0.81655
2017-12-10T04:56:56.458018: step 8204, loss 0.182511, acc 0.953125, prec 0.106325, recall 0.816566
2017-12-10T04:56:56.725202: step 8205, loss 0.363054, acc 0.96875, prec 0.106323, recall 0.816566
2017-12-10T04:56:57.006711: step 8206, loss 0.207759, acc 0.984375, prec 0.106321, recall 0.816566
2017-12-10T04:56:57.281912: step 8207, loss 0.408724, acc 0.9375, prec 0.106317, recall 0.816566
2017-12-10T04:56:57.543146: step 8208, loss 0.133271, acc 0.953125, prec 0.106333, recall 0.816597
2017-12-10T04:56:57.811095: step 8209, loss 0.44554, acc 0.953125, prec 0.10633, recall 0.816597
2017-12-10T04:56:58.076119: step 8210, loss 0.408, acc 0.953125, prec 0.106336, recall 0.816613
2017-12-10T04:56:58.343916: step 8211, loss 0.103637, acc 0.984375, prec 0.106335, recall 0.816613
2017-12-10T04:56:58.607692: step 8212, loss 0.102905, acc 0.984375, prec 0.106343, recall 0.816628
2017-12-10T04:56:58.881499: step 8213, loss 0.193063, acc 0.984375, prec 0.106352, recall 0.816644
2017-12-10T04:56:59.148203: step 8214, loss 0.106263, acc 0.984375, prec 0.106361, recall 0.81666
2017-12-10T04:56:59.414209: step 8215, loss 0.451621, acc 0.953125, prec 0.106357, recall 0.81666
2017-12-10T04:56:59.683847: step 8216, loss 0.356341, acc 0.90625, prec 0.10637, recall 0.816691
2017-12-10T04:56:59.946106: step 8217, loss 0.00689713, acc 1, prec 0.10639, recall 0.816722
2017-12-10T04:57:00.212735: step 8218, loss 0.621729, acc 0.9375, prec 0.106415, recall 0.816769
2017-12-10T04:57:00.487917: step 8219, loss 0.32287, acc 0.984375, prec 0.106443, recall 0.816815
2017-12-10T04:57:00.763442: step 8220, loss 0.0452049, acc 0.984375, prec 0.106462, recall 0.816846
2017-12-10T04:57:01.030375: step 8221, loss 0.699713, acc 0.984375, prec 0.106461, recall 0.816846
2017-12-10T04:57:01.299802: step 8222, loss 0.311056, acc 0.96875, prec 0.106478, recall 0.816877
2017-12-10T04:57:01.566031: step 8223, loss 0.356934, acc 0.953125, prec 0.106504, recall 0.816924
2017-12-10T04:57:01.828748: step 8224, loss 0.186767, acc 0.953125, prec 0.106501, recall 0.816924
2017-12-10T04:57:02.091758: step 8225, loss 0.355358, acc 0.96875, prec 0.106498, recall 0.816924
2017-12-10T04:57:02.366337: step 8226, loss 0.00757354, acc 1, prec 0.106518, recall 0.816955
2017-12-10T04:57:02.633764: step 8227, loss 0.0251887, acc 0.984375, prec 0.106517, recall 0.816955
2017-12-10T04:57:02.898955: step 8228, loss 0.138536, acc 0.96875, prec 0.106524, recall 0.816971
2017-12-10T04:57:03.168583: step 8229, loss 0.0419824, acc 0.96875, prec 0.106532, recall 0.816986
2017-12-10T04:57:03.438229: step 8230, loss 0.024897, acc 1, prec 0.106542, recall 0.817002
2017-12-10T04:57:03.707342: step 8231, loss 0.0319239, acc 0.984375, prec 0.106551, recall 0.817017
2017-12-10T04:57:03.973949: step 8232, loss 0.147339, acc 0.984375, prec 0.106559, recall 0.817033
2017-12-10T04:57:04.236020: step 8233, loss 0.108962, acc 0.953125, prec 0.106576, recall 0.817064
2017-12-10T04:57:04.504329: step 8234, loss 0.175889, acc 0.984375, prec 0.106584, recall 0.817079
2017-12-10T04:57:04.771201: step 8235, loss 1.14853, acc 0.984375, prec 0.106594, recall 0.817026
2017-12-10T04:57:05.042517: step 8236, loss 0.0191798, acc 0.984375, prec 0.106603, recall 0.817041
2017-12-10T04:57:05.307084: step 8237, loss 0.000286434, acc 1, prec 0.106613, recall 0.817057
2017-12-10T04:57:05.573461: step 8238, loss 0.39607, acc 0.984375, prec 0.106661, recall 0.817134
2017-12-10T04:57:05.844946: step 8239, loss 0.0187212, acc 0.984375, prec 0.10667, recall 0.81715
2017-12-10T04:57:06.112682: step 8240, loss 0.245996, acc 0.96875, prec 0.106687, recall 0.817181
2017-12-10T04:57:06.385296: step 8241, loss 0.436215, acc 0.984375, prec 0.106696, recall 0.817196
2017-12-10T04:57:06.646901: step 8242, loss 0.380174, acc 0.953125, prec 0.106702, recall 0.817212
2017-12-10T04:57:06.920655: step 8243, loss 0.00633355, acc 1, prec 0.106702, recall 0.817212
2017-12-10T04:57:07.186412: step 8244, loss 0.358658, acc 0.96875, prec 0.1067, recall 0.817212
2017-12-10T04:57:07.451256: step 8245, loss 0.211105, acc 0.953125, prec 0.106696, recall 0.817212
2017-12-10T04:57:07.714878: step 8246, loss 0.126636, acc 0.953125, prec 0.106693, recall 0.817212
2017-12-10T04:57:07.984001: step 8247, loss 0.0411093, acc 0.96875, prec 0.10669, recall 0.817212
2017-12-10T04:57:08.253894: step 8248, loss 2.42441, acc 0.9375, prec 0.106687, recall 0.817142
2017-12-10T04:57:08.522919: step 8249, loss 0.195995, acc 0.984375, prec 0.106686, recall 0.817142
2017-12-10T04:57:08.788713: step 8250, loss 0.609709, acc 0.9375, prec 0.106681, recall 0.817142
2017-12-10T04:57:09.060041: step 8251, loss 0.512728, acc 0.9375, prec 0.106686, recall 0.817158
2017-12-10T04:57:09.322701: step 8252, loss 0.370523, acc 0.953125, prec 0.106702, recall 0.817189
2017-12-10T04:57:09.595387: step 8253, loss 0.585931, acc 0.890625, prec 0.106714, recall 0.81722
2017-12-10T04:57:09.860858: step 8254, loss 0.493533, acc 0.90625, prec 0.106707, recall 0.81722
2017-12-10T04:57:10.131402: step 8255, loss 0.998691, acc 0.890625, prec 0.106708, recall 0.817235
2017-12-10T04:57:10.391510: step 8256, loss 0.431958, acc 0.953125, prec 0.106744, recall 0.817297
2017-12-10T04:57:10.652475: step 8257, loss 0.342842, acc 0.90625, prec 0.106737, recall 0.817297
2017-12-10T04:57:10.918192: step 8258, loss 0.448831, acc 0.96875, prec 0.106754, recall 0.817328
2017-12-10T04:57:11.192869: step 8259, loss 0.0959377, acc 0.96875, prec 0.106762, recall 0.817343
2017-12-10T04:57:11.458179: step 8260, loss 0.0979623, acc 0.96875, prec 0.10676, recall 0.817343
2017-12-10T04:57:11.725385: step 8261, loss 0.559699, acc 0.890625, prec 0.106761, recall 0.817359
2017-12-10T04:57:11.992403: step 8262, loss 0.255158, acc 0.953125, prec 0.106777, recall 0.81739
2017-12-10T04:57:12.268024: step 8263, loss 0.158901, acc 0.953125, prec 0.106774, recall 0.81739
2017-12-10T04:57:12.533321: step 8264, loss 0.901592, acc 0.96875, prec 0.106791, recall 0.817421
2017-12-10T04:57:12.801046: step 8265, loss 0.505927, acc 0.90625, prec 0.106784, recall 0.817421
2017-12-10T04:57:13.067968: step 8266, loss 0.585868, acc 0.953125, prec 0.106781, recall 0.817421
2017-12-10T04:57:13.334186: step 8267, loss 0.551482, acc 0.90625, prec 0.106793, recall 0.817452
2017-12-10T04:57:13.600902: step 8268, loss 0.359528, acc 0.953125, prec 0.1068, recall 0.817467
2017-12-10T04:57:13.862146: step 8269, loss 0.864089, acc 0.921875, prec 0.106794, recall 0.817467
2017-12-10T04:57:14.128666: step 8270, loss 0.586469, acc 0.953125, prec 0.10679, recall 0.817467
2017-12-10T04:57:14.393220: step 8271, loss 0.208937, acc 0.96875, prec 0.106808, recall 0.817498
2017-12-10T04:57:14.655825: step 8272, loss 0.296422, acc 0.9375, prec 0.106803, recall 0.817498
2017-12-10T04:57:14.921926: step 8273, loss 0.398141, acc 0.921875, prec 0.106817, recall 0.817529
2017-12-10T04:57:15.190133: step 8274, loss 0.542981, acc 0.953125, prec 0.106813, recall 0.817529
2017-12-10T04:57:15.461574: step 8275, loss 0.516238, acc 0.921875, prec 0.106827, recall 0.81756
2017-12-10T04:57:15.724957: step 8276, loss 0.23542, acc 0.953125, prec 0.106833, recall 0.817575
2017-12-10T04:57:15.987838: step 8277, loss 0.265154, acc 0.9375, prec 0.106838, recall 0.81759
2017-12-10T04:57:16.252237: step 8278, loss 0.44109, acc 0.921875, prec 0.106833, recall 0.81759
2017-12-10T04:57:16.520135: step 8279, loss 0.133359, acc 0.96875, prec 0.10683, recall 0.81759
2017-12-10T04:57:16.785004: step 8280, loss 0.236183, acc 0.953125, prec 0.106827, recall 0.81759
2017-12-10T04:57:17.051186: step 8281, loss 0.131031, acc 0.953125, prec 0.106833, recall 0.817606
2017-12-10T04:57:17.320932: step 8282, loss 0.556855, acc 0.875, prec 0.106824, recall 0.817606
2017-12-10T04:57:17.589093: step 8283, loss 0.0835886, acc 0.984375, prec 0.106842, recall 0.817637
2017-12-10T04:57:17.866089: step 8284, loss 0.126556, acc 0.953125, prec 0.106858, recall 0.817667
2017-12-10T04:57:18.134802: step 8285, loss 0.154728, acc 0.984375, prec 0.106867, recall 0.817683
2017-12-10T04:57:18.399582: step 8286, loss 0.23776, acc 0.953125, prec 0.106863, recall 0.817683
2017-12-10T04:57:18.672215: step 8287, loss 0.046872, acc 0.984375, prec 0.106862, recall 0.817683
2017-12-10T04:57:18.940869: step 8288, loss 0.10219, acc 0.984375, prec 0.106861, recall 0.817683
2017-12-10T04:57:19.211586: step 8289, loss 0.0784393, acc 0.984375, prec 0.10687, recall 0.817698
2017-12-10T04:57:19.474117: step 8290, loss 0.00179495, acc 1, prec 0.10687, recall 0.817698
2017-12-10T04:57:19.745197: step 8291, loss 0.478894, acc 0.9375, prec 0.106865, recall 0.817698
2017-12-10T04:57:20.010143: step 8292, loss 0.398336, acc 0.96875, prec 0.106863, recall 0.817698
2017-12-10T04:57:20.277028: step 8293, loss 0.830683, acc 0.953125, prec 0.106869, recall 0.817714
2017-12-10T04:57:20.546393: step 8294, loss 0.172858, acc 0.984375, prec 0.106887, recall 0.817744
2017-12-10T04:57:20.812624: step 8295, loss 0.000239621, acc 1, prec 0.106887, recall 0.817744
2017-12-10T04:57:21.075138: step 8296, loss 0.0031608, acc 1, prec 0.106887, recall 0.817744
2017-12-10T04:57:21.340103: step 8297, loss 0.000327449, acc 1, prec 0.106917, recall 0.817791
2017-12-10T04:57:21.599749: step 8298, loss 0.00176515, acc 1, prec 0.106927, recall 0.817806
2017-12-10T04:57:21.869845: step 8299, loss 0.00411223, acc 1, prec 0.106927, recall 0.817806
2017-12-10T04:57:22.130856: step 8300, loss 1.86111, acc 0.953125, prec 0.106934, recall 0.817752
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-8300

2017-12-10T04:57:23.417496: step 8301, loss 0.297078, acc 0.96875, prec 0.106942, recall 0.817768
2017-12-10T04:57:23.687520: step 8302, loss 0.281586, acc 0.96875, prec 0.106969, recall 0.817814
2017-12-10T04:57:23.954759: step 8303, loss 0.336218, acc 0.96875, prec 0.106986, recall 0.817845
2017-12-10T04:57:24.223231: step 8304, loss 0.278887, acc 0.953125, prec 0.107003, recall 0.817875
2017-12-10T04:57:24.490981: step 8305, loss 0.457814, acc 0.984375, prec 0.107001, recall 0.817875
2017-12-10T04:57:24.760539: step 8306, loss 0.0200147, acc 1, prec 0.107001, recall 0.817875
2017-12-10T04:57:25.038978: step 8307, loss 0.19288, acc 0.984375, prec 0.107, recall 0.817875
2017-12-10T04:57:25.300402: step 8308, loss 0.21403, acc 0.953125, prec 0.106997, recall 0.817875
2017-12-10T04:57:25.564320: step 8309, loss 0.403, acc 0.953125, prec 0.107003, recall 0.817891
2017-12-10T04:57:25.831526: step 8310, loss 0.075732, acc 0.96875, prec 0.10701, recall 0.817906
2017-12-10T04:57:26.095416: step 8311, loss 0.0405174, acc 0.96875, prec 0.107008, recall 0.817906
2017-12-10T04:57:26.362370: step 8312, loss 0.103555, acc 0.953125, prec 0.107005, recall 0.817906
2017-12-10T04:57:26.628377: step 8313, loss 0.466451, acc 0.9375, prec 0.10702, recall 0.817937
2017-12-10T04:57:26.896806: step 8314, loss 0.00105803, acc 1, prec 0.107039, recall 0.817967
2017-12-10T04:57:27.172010: step 8315, loss 1.22972, acc 0.953125, prec 0.107046, recall 0.817983
2017-12-10T04:57:27.440712: step 8316, loss 0.182126, acc 0.96875, prec 0.107063, recall 0.818013
2017-12-10T04:57:27.707335: step 8317, loss 0.0148155, acc 1, prec 0.107063, recall 0.818013
2017-12-10T04:57:27.978949: step 8318, loss 0.42582, acc 0.953125, prec 0.107079, recall 0.818044
2017-12-10T04:57:28.242682: step 8319, loss 0.0213041, acc 1, prec 0.107089, recall 0.818059
2017-12-10T04:57:28.508476: step 8320, loss 0.139153, acc 0.953125, prec 0.107085, recall 0.818059
2017-12-10T04:57:28.776502: step 8321, loss 0.508645, acc 0.984375, prec 0.107094, recall 0.818075
2017-12-10T04:57:29.047134: step 8322, loss 0.151416, acc 0.953125, prec 0.10709, recall 0.818075
2017-12-10T04:57:29.319197: step 8323, loss 0.0110985, acc 1, prec 0.1071, recall 0.81809
2017-12-10T04:57:29.586837: step 8324, loss 0.722936, acc 0.921875, prec 0.107094, recall 0.81809
2017-12-10T04:57:29.856832: step 8325, loss 0.42109, acc 0.953125, prec 0.107091, recall 0.81809
2017-12-10T04:57:30.132479: step 8326, loss 0.971221, acc 0.9375, prec 0.107106, recall 0.818121
2017-12-10T04:57:30.411882: step 8327, loss 0.202958, acc 0.96875, prec 0.107123, recall 0.818151
2017-12-10T04:57:30.685896: step 8328, loss 0.192687, acc 0.953125, prec 0.107139, recall 0.818182
2017-12-10T04:57:30.955125: step 8329, loss 0.9832, acc 0.921875, prec 0.107133, recall 0.818182
2017-12-10T04:57:31.219347: step 8330, loss 1.83289, acc 0.9375, prec 0.10714, recall 0.818128
2017-12-10T04:57:31.490958: step 8331, loss 0.352901, acc 0.9375, prec 0.107135, recall 0.818128
2017-12-10T04:57:31.754924: step 8332, loss 0.573679, acc 0.921875, prec 0.107139, recall 0.818144
2017-12-10T04:57:32.020997: step 8333, loss 0.0495857, acc 0.953125, prec 0.107145, recall 0.818159
2017-12-10T04:57:32.282175: step 8334, loss 0.143521, acc 0.953125, prec 0.107161, recall 0.818189
2017-12-10T04:57:32.550347: step 8335, loss 0.877408, acc 0.890625, prec 0.107153, recall 0.818189
2017-12-10T04:57:32.811970: step 8336, loss 0.28733, acc 0.9375, prec 0.107158, recall 0.818205
2017-12-10T04:57:33.074758: step 8337, loss 0.809292, acc 0.90625, prec 0.107181, recall 0.818251
2017-12-10T04:57:33.339708: step 8338, loss 0.678329, acc 0.9375, prec 0.107176, recall 0.818251
2017-12-10T04:57:33.614146: step 8339, loss 0.485342, acc 0.875, prec 0.107166, recall 0.818251
2017-12-10T04:57:33.881527: step 8340, loss 0.640415, acc 0.953125, prec 0.107163, recall 0.818251
2017-12-10T04:57:34.150264: step 8341, loss 0.419713, acc 0.9375, prec 0.107188, recall 0.818296
2017-12-10T04:57:34.421815: step 8342, loss 0.54925, acc 0.90625, prec 0.10719, recall 0.818312
2017-12-10T04:57:34.685076: step 8343, loss 0.14779, acc 0.953125, prec 0.107216, recall 0.818358
2017-12-10T04:57:34.950322: step 8344, loss 0.253679, acc 0.953125, prec 0.107213, recall 0.818358
2017-12-10T04:57:35.216235: step 8345, loss 0.321034, acc 0.9375, prec 0.107228, recall 0.818388
2017-12-10T04:57:35.487382: step 8346, loss 0.406429, acc 0.953125, prec 0.107234, recall 0.818403
2017-12-10T04:57:35.754006: step 8347, loss 0.149239, acc 0.953125, prec 0.107231, recall 0.818403
2017-12-10T04:57:36.021488: step 8348, loss 0.208484, acc 0.96875, prec 0.107248, recall 0.818434
2017-12-10T04:57:36.290978: step 8349, loss 0.909568, acc 0.890625, prec 0.107259, recall 0.818464
2017-12-10T04:57:36.560830: step 8350, loss 0.590806, acc 0.9375, prec 0.107264, recall 0.81848
2017-12-10T04:57:36.828926: step 8351, loss 0.118709, acc 0.984375, prec 0.107263, recall 0.81848
2017-12-10T04:57:37.102889: step 8352, loss 0.480198, acc 0.9375, prec 0.107258, recall 0.81848
2017-12-10T04:57:37.378029: step 8353, loss 0.815389, acc 0.90625, prec 0.107261, recall 0.818495
2017-12-10T04:57:37.646456: step 8354, loss 0.0616648, acc 0.96875, prec 0.107278, recall 0.818525
2017-12-10T04:57:37.915014: step 8355, loss 0.13858, acc 0.984375, prec 0.107287, recall 0.818541
2017-12-10T04:57:38.183532: step 8356, loss 0.0221699, acc 0.984375, prec 0.107286, recall 0.818541
2017-12-10T04:57:38.453039: step 8357, loss 0.143777, acc 0.9375, prec 0.107281, recall 0.818541
2017-12-10T04:57:38.717732: step 8358, loss 0.102722, acc 0.96875, prec 0.107279, recall 0.818541
2017-12-10T04:57:38.983166: step 8359, loss 0.00131305, acc 1, prec 0.107299, recall 0.818571
2017-12-10T04:57:39.251552: step 8360, loss 0.00246065, acc 1, prec 0.107308, recall 0.818586
2017-12-10T04:57:39.516054: step 8361, loss 0.000673363, acc 1, prec 0.107328, recall 0.818617
2017-12-10T04:57:39.777916: step 8362, loss 0.000378428, acc 1, prec 0.107328, recall 0.818617
2017-12-10T04:57:40.040258: step 8363, loss 0.360779, acc 0.96875, prec 0.107335, recall 0.818632
2017-12-10T04:57:40.311854: step 8364, loss 0.00418767, acc 1, prec 0.107335, recall 0.818632
2017-12-10T04:57:40.574742: step 8365, loss 0.347423, acc 0.96875, prec 0.107363, recall 0.818678
2017-12-10T04:57:40.841766: step 8366, loss 0.000375943, acc 1, prec 0.107382, recall 0.818708
2017-12-10T04:57:41.100542: step 8367, loss 6.54035e-05, acc 1, prec 0.107392, recall 0.818723
2017-12-10T04:57:41.360148: step 8368, loss 0.000491657, acc 1, prec 0.107392, recall 0.818723
2017-12-10T04:57:41.626504: step 8369, loss 0.0867203, acc 1, prec 0.107402, recall 0.818738
2017-12-10T04:57:41.896510: step 8370, loss 0.093747, acc 0.984375, prec 0.107401, recall 0.818738
2017-12-10T04:57:42.169287: step 8371, loss 0.540342, acc 0.96875, prec 0.107408, recall 0.818754
2017-12-10T04:57:42.432631: step 8372, loss 1.39698e-07, acc 1, prec 0.107418, recall 0.818769
2017-12-10T04:57:42.688714: step 8373, loss 0.0147268, acc 0.984375, prec 0.107427, recall 0.818784
2017-12-10T04:57:42.952958: step 8374, loss 0.00112628, acc 1, prec 0.107427, recall 0.818784
2017-12-10T04:57:43.220630: step 8375, loss 0.000690693, acc 1, prec 0.107436, recall 0.818799
2017-12-10T04:57:43.480060: step 8376, loss 0.0431797, acc 0.984375, prec 0.107445, recall 0.818814
2017-12-10T04:57:43.743931: step 8377, loss 0.126749, acc 0.984375, prec 0.107444, recall 0.818814
2017-12-10T04:57:44.014115: step 8378, loss 0.117201, acc 0.984375, prec 0.107443, recall 0.818814
2017-12-10T04:57:44.280856: step 8379, loss 2.19788e-06, acc 1, prec 0.107443, recall 0.818814
2017-12-10T04:57:44.542967: step 8380, loss 0.339496, acc 0.984375, prec 0.107461, recall 0.818845
2017-12-10T04:57:44.820258: step 8381, loss 5.92573e-05, acc 1, prec 0.107461, recall 0.818845
2017-12-10T04:57:45.085655: step 8382, loss 14.1496, acc 0.96875, prec 0.10746, recall 0.818776
2017-12-10T04:57:45.355995: step 8383, loss 0.0231461, acc 0.984375, prec 0.107459, recall 0.818776
2017-12-10T04:57:45.621243: step 8384, loss 0.642958, acc 0.96875, prec 0.107456, recall 0.818776
2017-12-10T04:57:45.891750: step 8385, loss 0.00032305, acc 1, prec 0.107466, recall 0.818791
2017-12-10T04:57:46.158648: step 8386, loss 0.0626776, acc 0.984375, prec 0.107485, recall 0.818822
2017-12-10T04:57:46.428334: step 8387, loss 0.0552313, acc 0.984375, prec 0.107493, recall 0.818837
2017-12-10T04:57:46.695692: step 8388, loss 4.36102, acc 0.984375, prec 0.107493, recall 0.818768
2017-12-10T04:57:46.968952: step 8389, loss 0.525479, acc 0.96875, prec 0.107491, recall 0.818768
2017-12-10T04:57:47.235667: step 8390, loss 0.42871, acc 0.9375, prec 0.107486, recall 0.818768
2017-12-10T04:57:47.500789: step 8391, loss 0.486846, acc 0.921875, prec 0.10749, recall 0.818784
2017-12-10T04:57:47.766362: step 8392, loss 0.42643, acc 0.921875, prec 0.107484, recall 0.818784
2017-12-10T04:57:48.029150: step 8393, loss 0.26644, acc 0.96875, prec 0.107492, recall 0.818799
2017-12-10T04:57:48.303734: step 8394, loss 0.226707, acc 0.953125, prec 0.107508, recall 0.818829
2017-12-10T04:57:48.566850: step 8395, loss 0.609868, acc 0.890625, prec 0.107499, recall 0.818829
2017-12-10T04:57:48.843463: step 8396, loss 0.947657, acc 0.875, prec 0.10749, recall 0.818829
2017-12-10T04:57:49.114865: step 8397, loss 1.41931, acc 0.84375, prec 0.107488, recall 0.818844
2017-12-10T04:57:49.389865: step 8398, loss 0.727889, acc 0.875, prec 0.107488, recall 0.818859
2017-12-10T04:57:49.659271: step 8399, loss 1.53219, acc 0.875, prec 0.107489, recall 0.818875
2017-12-10T04:57:49.926325: step 8400, loss 0.469292, acc 0.890625, prec 0.10748, recall 0.818875

Evaluation:
2017-12-10T04:57:57.521695: step 8400, loss 5.47504, acc 0.885639, prec 0.107032, recall 0.816543

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-8400

2017-12-10T04:57:58.848186: step 8401, loss 1.52022, acc 0.84375, prec 0.10703, recall 0.816558
2017-12-10T04:57:59.114883: step 8402, loss 0.527338, acc 0.90625, prec 0.107023, recall 0.816558
2017-12-10T04:57:59.379303: step 8403, loss 0.356633, acc 0.953125, prec 0.107029, recall 0.816573
2017-12-10T04:57:59.644935: step 8404, loss 0.608723, acc 0.890625, prec 0.107041, recall 0.816603
2017-12-10T04:57:59.907873: step 8405, loss 0.529287, acc 0.9375, prec 0.107046, recall 0.816618
2017-12-10T04:58:00.178089: step 8406, loss 0.608327, acc 0.890625, prec 0.107038, recall 0.816618
2017-12-10T04:58:00.448179: step 8407, loss 0.395142, acc 0.9375, prec 0.107033, recall 0.816618
2017-12-10T04:58:00.717480: step 8408, loss 1.43557, acc 0.921875, prec 0.107037, recall 0.816634
2017-12-10T04:58:00.983772: step 8409, loss 0.259872, acc 0.953125, prec 0.107053, recall 0.816664
2017-12-10T04:58:01.250520: step 8410, loss 0.67204, acc 0.90625, prec 0.107046, recall 0.816664
2017-12-10T04:58:01.518147: step 8411, loss 0.18288, acc 0.984375, prec 0.107054, recall 0.816679
2017-12-10T04:58:01.786149: step 8412, loss 0.739126, acc 0.921875, prec 0.107058, recall 0.816694
2017-12-10T04:58:02.055146: step 8413, loss 0.00172288, acc 1, prec 0.107068, recall 0.816709
2017-12-10T04:58:02.321432: step 8414, loss 0.00136169, acc 1, prec 0.107068, recall 0.816709
2017-12-10T04:58:02.589716: step 8415, loss 0.776308, acc 0.9375, prec 0.107063, recall 0.816709
2017-12-10T04:58:02.858310: step 8416, loss 0.205374, acc 0.984375, prec 0.107062, recall 0.816709
2017-12-10T04:58:03.127631: step 8417, loss 0.581603, acc 0.984375, prec 0.10708, recall 0.81674
2017-12-10T04:58:03.393673: step 8418, loss 0.137938, acc 0.96875, prec 0.107097, recall 0.81677
2017-12-10T04:58:03.658778: step 8419, loss 0.211543, acc 0.953125, prec 0.107103, recall 0.816785
2017-12-10T04:58:03.924950: step 8420, loss 0.00898047, acc 1, prec 0.107113, recall 0.8168
2017-12-10T04:58:04.194544: step 8421, loss 0.300901, acc 0.953125, prec 0.107119, recall 0.816815
2017-12-10T04:58:04.463544: step 8422, loss 0.479244, acc 0.96875, prec 0.107127, recall 0.81683
2017-12-10T04:58:04.729851: step 8423, loss 0.60333, acc 0.96875, prec 0.107124, recall 0.81683
2017-12-10T04:58:04.995793: step 8424, loss 0.441885, acc 0.96875, prec 0.107141, recall 0.816861
2017-12-10T04:58:05.262491: step 8425, loss 0.522027, acc 0.96875, prec 0.107139, recall 0.816861
2017-12-10T04:58:05.533339: step 8426, loss 0.0724101, acc 0.984375, prec 0.107138, recall 0.816861
2017-12-10T04:58:05.799554: step 8427, loss 0.139318, acc 0.96875, prec 0.107145, recall 0.816876
2017-12-10T04:58:06.061626: step 8428, loss 0.32378, acc 0.984375, prec 0.107154, recall 0.816891
2017-12-10T04:58:06.331070: step 8429, loss 0.071151, acc 0.984375, prec 0.107153, recall 0.816891
2017-12-10T04:58:06.602986: step 8430, loss 0.0441906, acc 0.984375, prec 0.107151, recall 0.816891
2017-12-10T04:58:06.866340: step 8431, loss 0.0410918, acc 0.984375, prec 0.10717, recall 0.816921
2017-12-10T04:58:07.131831: step 8432, loss 0.0712014, acc 0.984375, prec 0.107168, recall 0.816921
2017-12-10T04:58:07.394413: step 8433, loss 0.370096, acc 0.953125, prec 0.107175, recall 0.816936
2017-12-10T04:58:07.662060: step 8434, loss 0.575847, acc 0.984375, prec 0.107202, recall 0.816982
2017-12-10T04:58:07.927332: step 8435, loss 0.00078233, acc 1, prec 0.107212, recall 0.816997
2017-12-10T04:58:08.194708: step 8436, loss 0.0427105, acc 0.984375, prec 0.107221, recall 0.817012
2017-12-10T04:58:08.462580: step 8437, loss 0.0399209, acc 0.984375, prec 0.107229, recall 0.817027
2017-12-10T04:58:08.724705: step 8438, loss 0.335221, acc 0.984375, prec 0.107228, recall 0.817027
2017-12-10T04:58:08.985552: step 8439, loss 0.0064132, acc 1, prec 0.107228, recall 0.817027
2017-12-10T04:58:09.257759: step 8440, loss 0.216417, acc 0.96875, prec 0.107226, recall 0.817027
2017-12-10T04:58:09.524734: step 8441, loss 0.369634, acc 0.96875, prec 0.107233, recall 0.817042
2017-12-10T04:58:09.788869: step 8442, loss 0.0476197, acc 0.984375, prec 0.107241, recall 0.817057
2017-12-10T04:58:10.058038: step 8443, loss 0.0961887, acc 0.984375, prec 0.10725, recall 0.817072
2017-12-10T04:58:10.324460: step 8444, loss 0.166792, acc 0.96875, prec 0.107248, recall 0.817072
2017-12-10T04:58:10.598045: step 8445, loss 0.0522211, acc 0.96875, prec 0.107274, recall 0.817117
2017-12-10T04:58:10.861499: step 8446, loss 0.303555, acc 0.9375, prec 0.10727, recall 0.817117
2017-12-10T04:58:11.124148: step 8447, loss 0.0726196, acc 0.984375, prec 0.107288, recall 0.817148
2017-12-10T04:58:11.387032: step 8448, loss 0.0201482, acc 0.984375, prec 0.107296, recall 0.817163
2017-12-10T04:58:11.622905: step 8449, loss 0.056857, acc 1, prec 0.107325, recall 0.817208
2017-12-10T04:58:11.898940: step 8450, loss 0.580073, acc 0.9375, prec 0.107321, recall 0.817208
2017-12-10T04:58:12.171626: step 8451, loss 0.456119, acc 0.953125, prec 0.107336, recall 0.817238
2017-12-10T04:58:12.439424: step 8452, loss 0.0122606, acc 1, prec 0.107346, recall 0.817253
2017-12-10T04:58:12.710109: step 8453, loss 0.129759, acc 0.96875, prec 0.107354, recall 0.817268
2017-12-10T04:58:12.972874: step 8454, loss 7.6017, acc 0.984375, prec 0.107363, recall 0.817216
2017-12-10T04:58:13.243104: step 8455, loss 0.0150866, acc 1, prec 0.107373, recall 0.817231
2017-12-10T04:58:13.509219: step 8456, loss 0.0394167, acc 0.984375, prec 0.107381, recall 0.817246
2017-12-10T04:58:13.771215: step 8457, loss 0.31665, acc 0.921875, prec 0.107395, recall 0.817276
2017-12-10T04:58:14.041253: step 8458, loss 0.291858, acc 0.953125, prec 0.107391, recall 0.817276
2017-12-10T04:58:14.307873: step 8459, loss 0.235785, acc 0.984375, prec 0.10739, recall 0.817276
2017-12-10T04:58:14.571777: step 8460, loss 0.0116825, acc 0.984375, prec 0.107418, recall 0.817321
2017-12-10T04:58:14.837428: step 8461, loss 0.416634, acc 0.9375, prec 0.107423, recall 0.817336
2017-12-10T04:58:15.099947: step 8462, loss 1.06112, acc 0.90625, prec 0.107416, recall 0.817336
2017-12-10T04:58:15.362353: step 8463, loss 0.195956, acc 0.953125, prec 0.107413, recall 0.817336
2017-12-10T04:58:15.626720: step 8464, loss 0.184764, acc 0.96875, prec 0.10742, recall 0.817351
2017-12-10T04:58:15.897398: step 8465, loss 0.357899, acc 0.9375, prec 0.107425, recall 0.817366
2017-12-10T04:58:16.164271: step 8466, loss 1.08302, acc 0.90625, prec 0.107418, recall 0.817366
2017-12-10T04:58:16.429070: step 8467, loss 1.12517, acc 0.859375, prec 0.107417, recall 0.817381
2017-12-10T04:58:16.696735: step 8468, loss 0.286444, acc 0.953125, prec 0.107423, recall 0.817396
2017-12-10T04:58:16.971342: step 8469, loss 0.560748, acc 0.921875, prec 0.107446, recall 0.817441
2017-12-10T04:58:17.236743: step 8470, loss 0.759874, acc 0.9375, prec 0.107442, recall 0.817441
2017-12-10T04:58:17.501450: step 8471, loss 0.101718, acc 0.96875, prec 0.107439, recall 0.817441
2017-12-10T04:58:17.766588: step 8472, loss 0.159382, acc 0.953125, prec 0.107446, recall 0.817456
2017-12-10T04:58:18.029604: step 8473, loss 0.579716, acc 0.953125, prec 0.107452, recall 0.817471
2017-12-10T04:58:18.294645: step 8474, loss 0.00756935, acc 1, prec 0.107461, recall 0.817486
2017-12-10T04:58:18.560166: step 8475, loss 0.116852, acc 0.984375, prec 0.10747, recall 0.817501
2017-12-10T04:58:18.820014: step 8476, loss 0.0608231, acc 0.96875, prec 0.107468, recall 0.817501
2017-12-10T04:58:19.089110: step 8477, loss 0.663127, acc 0.921875, prec 0.107481, recall 0.817531
2017-12-10T04:58:19.354104: step 8478, loss 0.353003, acc 0.90625, prec 0.107484, recall 0.817546
2017-12-10T04:58:19.618278: step 8479, loss 0.204911, acc 0.96875, prec 0.107491, recall 0.817561
2017-12-10T04:58:19.897602: step 8480, loss 0.0210263, acc 0.984375, prec 0.10749, recall 0.817561
2017-12-10T04:58:20.162117: step 8481, loss 0.215742, acc 0.96875, prec 0.107488, recall 0.817561
2017-12-10T04:58:20.435512: step 8482, loss 0.281923, acc 0.96875, prec 0.107485, recall 0.817561
2017-12-10T04:58:20.703598: step 8483, loss 0.496982, acc 0.984375, prec 0.107503, recall 0.817591
2017-12-10T04:58:20.967911: step 8484, loss 0.703376, acc 0.96875, prec 0.107501, recall 0.817591
2017-12-10T04:58:21.240039: step 8485, loss 0.0886549, acc 0.953125, prec 0.107498, recall 0.817591
2017-12-10T04:58:21.503654: step 8486, loss 0.0064369, acc 1, prec 0.107498, recall 0.817591
2017-12-10T04:58:21.771686: step 8487, loss 0.0380822, acc 0.984375, prec 0.107496, recall 0.817591
2017-12-10T04:58:22.042921: step 8488, loss 0.627111, acc 0.96875, prec 0.107523, recall 0.817636
2017-12-10T04:58:22.312993: step 8489, loss 0.380403, acc 0.953125, prec 0.107529, recall 0.817651
2017-12-10T04:58:22.581409: step 8490, loss 0.0634378, acc 0.96875, prec 0.107546, recall 0.817681
2017-12-10T04:58:22.854315: step 8491, loss 0.327598, acc 0.984375, prec 0.107545, recall 0.817681
2017-12-10T04:58:23.121941: step 8492, loss 0.128951, acc 0.96875, prec 0.107543, recall 0.817681
2017-12-10T04:58:23.391173: step 8493, loss 0.134658, acc 0.984375, prec 0.107542, recall 0.817681
2017-12-10T04:58:23.660391: step 8494, loss 10.8029, acc 0.984375, prec 0.107542, recall 0.817614
2017-12-10T04:58:23.939506: step 8495, loss 0.037184, acc 0.96875, prec 0.107558, recall 0.817644
2017-12-10T04:58:24.207831: step 8496, loss 0.109439, acc 0.984375, prec 0.107577, recall 0.817674
2017-12-10T04:58:24.474231: step 8497, loss 0.444079, acc 0.953125, prec 0.107573, recall 0.817674
2017-12-10T04:58:24.737537: step 8498, loss 8.04366, acc 0.953125, prec 0.107571, recall 0.817607
2017-12-10T04:58:25.013073: step 8499, loss 0.62066, acc 0.9375, prec 0.107585, recall 0.817637
2017-12-10T04:58:25.279509: step 8500, loss 0.612187, acc 0.859375, prec 0.107585, recall 0.817652
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-8500

2017-12-10T04:58:26.551495: step 8501, loss 0.744228, acc 0.890625, prec 0.107586, recall 0.817667
2017-12-10T04:58:26.817115: step 8502, loss 0.506279, acc 0.84375, prec 0.107575, recall 0.817667
2017-12-10T04:58:27.086578: step 8503, loss 1.31902, acc 0.8125, prec 0.107589, recall 0.817712
2017-12-10T04:58:27.355742: step 8504, loss 0.864015, acc 0.84375, prec 0.107578, recall 0.817712
2017-12-10T04:58:27.627227: step 8505, loss 0.896604, acc 0.8125, prec 0.107583, recall 0.817742
2017-12-10T04:58:27.895526: step 8506, loss 1.09188, acc 0.875, prec 0.107574, recall 0.817742
2017-12-10T04:58:28.158822: step 8507, loss 2.55047, acc 0.703125, prec 0.107561, recall 0.817757
2017-12-10T04:58:28.432137: step 8508, loss 0.507496, acc 0.890625, prec 0.107563, recall 0.817772
2017-12-10T04:58:28.706650: step 8509, loss 0.997591, acc 0.875, prec 0.107554, recall 0.817772
2017-12-10T04:58:28.971216: step 8510, loss 0.87221, acc 0.84375, prec 0.107561, recall 0.817801
2017-12-10T04:58:29.240748: step 8511, loss 0.988807, acc 0.828125, prec 0.107549, recall 0.817801
2017-12-10T04:58:29.511907: step 8512, loss 0.75697, acc 0.875, prec 0.107549, recall 0.817816
2017-12-10T04:58:29.778095: step 8513, loss 0.447456, acc 0.9375, prec 0.107544, recall 0.817816
2017-12-10T04:58:30.048544: step 8514, loss 0.382664, acc 0.953125, prec 0.107541, recall 0.817816
2017-12-10T04:58:30.333866: step 8515, loss 1.22204, acc 0.875, prec 0.107532, recall 0.817816
2017-12-10T04:58:30.604921: step 8516, loss 0.227079, acc 0.9375, prec 0.107537, recall 0.817831
2017-12-10T04:58:30.876305: step 8517, loss 0.0961322, acc 0.953125, prec 0.107552, recall 0.817861
2017-12-10T04:58:31.152156: step 8518, loss 0.540855, acc 0.9375, prec 0.107577, recall 0.817906
2017-12-10T04:58:31.420043: step 8519, loss 0.466444, acc 0.921875, prec 0.107571, recall 0.817906
2017-12-10T04:58:31.685460: step 8520, loss 0.311263, acc 0.953125, prec 0.107567, recall 0.817906
2017-12-10T04:58:31.951873: step 8521, loss 0.430355, acc 0.921875, prec 0.107561, recall 0.817906
2017-12-10T04:58:32.216183: step 8522, loss 0.0444901, acc 0.984375, prec 0.10758, recall 0.817936
2017-12-10T04:58:32.480127: step 8523, loss 0.0813358, acc 0.984375, prec 0.107578, recall 0.817936
2017-12-10T04:58:32.746890: step 8524, loss 0.00239246, acc 1, prec 0.107607, recall 0.817981
2017-12-10T04:58:33.015718: step 8525, loss 0.672089, acc 0.953125, prec 0.107633, recall 0.818025
2017-12-10T04:58:33.280285: step 8526, loss 0.130222, acc 0.96875, prec 0.10765, recall 0.818055
2017-12-10T04:58:33.550913: step 8527, loss 0.98359, acc 1, prec 0.107659, recall 0.81807
2017-12-10T04:58:33.818502: step 8528, loss 0.067794, acc 0.984375, prec 0.107658, recall 0.81807
2017-12-10T04:58:34.087020: step 8529, loss 0.395178, acc 0.96875, prec 0.107656, recall 0.81807
2017-12-10T04:58:34.352808: step 8530, loss 0.193538, acc 0.96875, prec 0.107673, recall 0.8181
2017-12-10T04:58:34.618940: step 8531, loss 0.150754, acc 0.96875, prec 0.10768, recall 0.818115
2017-12-10T04:58:34.887288: step 8532, loss 0.173295, acc 0.96875, prec 0.107687, recall 0.81813
2017-12-10T04:58:35.149700: step 8533, loss 0.0477747, acc 0.984375, prec 0.107686, recall 0.81813
2017-12-10T04:58:35.420173: step 8534, loss 0.196095, acc 0.96875, prec 0.107684, recall 0.81813
2017-12-10T04:58:35.685496: step 8535, loss 0.516706, acc 0.9375, prec 0.107689, recall 0.818145
2017-12-10T04:58:35.960796: step 8536, loss 0.00834347, acc 1, prec 0.107708, recall 0.818174
2017-12-10T04:58:36.227365: step 8537, loss 0.143855, acc 0.984375, prec 0.107716, recall 0.818189
2017-12-10T04:58:36.506426: step 8538, loss 0.284223, acc 0.96875, prec 0.107724, recall 0.818204
2017-12-10T04:58:36.772731: step 8539, loss 0.17866, acc 0.96875, prec 0.107721, recall 0.818204
2017-12-10T04:58:37.044343: step 8540, loss 0.19442, acc 0.96875, prec 0.107719, recall 0.818204
2017-12-10T04:58:37.312925: step 8541, loss 0.0345303, acc 0.984375, prec 0.107718, recall 0.818204
2017-12-10T04:58:37.578876: step 8542, loss 0.418655, acc 0.96875, prec 0.107716, recall 0.818204
2017-12-10T04:58:37.843236: step 8543, loss 0.0047711, acc 1, prec 0.107725, recall 0.818219
2017-12-10T04:58:38.118407: step 8544, loss 0.140292, acc 0.984375, prec 0.107724, recall 0.818219
2017-12-10T04:58:38.386969: step 8545, loss 0.264871, acc 0.984375, prec 0.107732, recall 0.818234
2017-12-10T04:58:38.654522: step 8546, loss 0.199629, acc 0.984375, prec 0.107741, recall 0.818249
2017-12-10T04:58:38.929817: step 8547, loss 0.0885272, acc 0.984375, prec 0.10774, recall 0.818249
2017-12-10T04:58:39.203691: step 8548, loss 0.0335087, acc 1, prec 0.107759, recall 0.818278
2017-12-10T04:58:39.469075: step 8549, loss 0.000117965, acc 1, prec 0.107769, recall 0.818293
2017-12-10T04:58:39.732169: step 8550, loss 0.0541577, acc 0.984375, prec 0.107767, recall 0.818293
2017-12-10T04:58:40.002450: step 8551, loss 0.191657, acc 0.96875, prec 0.107765, recall 0.818293
2017-12-10T04:58:40.268222: step 8552, loss 0.157085, acc 0.984375, prec 0.107764, recall 0.818293
2017-12-10T04:58:40.540597: step 8553, loss 0.0564552, acc 0.984375, prec 0.107772, recall 0.818308
2017-12-10T04:58:40.810133: step 8554, loss 2.45669e-06, acc 1, prec 0.107772, recall 0.818308
2017-12-10T04:58:41.069270: step 8555, loss 0.125141, acc 0.984375, prec 0.107771, recall 0.818308
2017-12-10T04:58:41.330934: step 8556, loss 0.518366, acc 0.9375, prec 0.107786, recall 0.818338
2017-12-10T04:58:41.604705: step 8557, loss 0.177961, acc 0.953125, prec 0.107782, recall 0.818338
2017-12-10T04:58:41.875614: step 8558, loss 0.093539, acc 0.96875, prec 0.10779, recall 0.818353
2017-12-10T04:58:42.152685: step 8559, loss 0.000112722, acc 1, prec 0.107809, recall 0.818383
2017-12-10T04:58:42.412339: step 8560, loss 1.52384, acc 0.96875, prec 0.107817, recall 0.81833
2017-12-10T04:58:42.680640: step 8561, loss 0.0189635, acc 0.984375, prec 0.107816, recall 0.81833
2017-12-10T04:58:42.947751: step 8562, loss 0.00022535, acc 1, prec 0.107816, recall 0.81833
2017-12-10T04:58:43.216368: step 8563, loss 0.0640552, acc 0.984375, prec 0.107825, recall 0.818345
2017-12-10T04:58:43.483682: step 8564, loss 0.0667736, acc 0.953125, prec 0.107831, recall 0.81836
2017-12-10T04:58:43.753180: step 8565, loss 0.00062503, acc 1, prec 0.107831, recall 0.81836
2017-12-10T04:58:44.016981: step 8566, loss 0.191896, acc 0.96875, prec 0.107838, recall 0.818375
2017-12-10T04:58:44.283739: step 8567, loss 0.0922285, acc 0.984375, prec 0.107837, recall 0.818375
2017-12-10T04:58:44.556191: step 8568, loss 0.0809588, acc 0.96875, prec 0.107844, recall 0.81839
2017-12-10T04:58:44.827742: step 8569, loss 0.323222, acc 0.984375, prec 0.107862, recall 0.81842
2017-12-10T04:58:45.091666: step 8570, loss 0.665663, acc 0.96875, prec 0.10786, recall 0.81842
2017-12-10T04:58:45.361390: step 8571, loss 0.332678, acc 0.96875, prec 0.107857, recall 0.81842
2017-12-10T04:58:45.624319: step 8572, loss 0.186242, acc 0.96875, prec 0.107855, recall 0.81842
2017-12-10T04:58:45.892482: step 8573, loss 0.0576486, acc 0.96875, prec 0.107862, recall 0.818434
2017-12-10T04:58:46.160740: step 8574, loss 0.383641, acc 0.953125, prec 0.107859, recall 0.818434
2017-12-10T04:58:46.430181: step 8575, loss 0.0673322, acc 0.984375, prec 0.107858, recall 0.818434
2017-12-10T04:58:46.690213: step 8576, loss 0.205472, acc 0.953125, prec 0.107854, recall 0.818434
2017-12-10T04:58:46.955762: step 8577, loss 0.119127, acc 0.96875, prec 0.107852, recall 0.818434
2017-12-10T04:58:47.220939: step 8578, loss 0.311329, acc 0.96875, prec 0.107859, recall 0.818449
2017-12-10T04:58:47.490958: step 8579, loss 0.0751805, acc 0.984375, prec 0.107868, recall 0.818464
2017-12-10T04:58:47.754868: step 8580, loss 0.121903, acc 0.953125, prec 0.107864, recall 0.818464
2017-12-10T04:58:48.027650: step 8581, loss 4.60599, acc 0.953125, prec 0.107872, recall 0.818412
2017-12-10T04:58:48.293486: step 8582, loss 0.0028836, acc 1, prec 0.107872, recall 0.818412
2017-12-10T04:58:48.563534: step 8583, loss 0.453235, acc 0.9375, prec 0.107876, recall 0.818427
2017-12-10T04:58:48.833486: step 8584, loss 0.221614, acc 0.96875, prec 0.107884, recall 0.818442
2017-12-10T04:58:49.098005: step 8585, loss 0.529675, acc 0.9375, prec 0.107879, recall 0.818442
2017-12-10T04:58:49.362994: step 8586, loss 0.365982, acc 0.9375, prec 0.107884, recall 0.818457
2017-12-10T04:58:49.634401: step 8587, loss 0.379967, acc 0.953125, prec 0.107919, recall 0.818516
2017-12-10T04:58:49.896773: step 8588, loss 0.191614, acc 0.953125, prec 0.107916, recall 0.818516
2017-12-10T04:58:50.171885: step 8589, loss 0.699722, acc 0.921875, prec 0.10791, recall 0.818516
2017-12-10T04:58:50.434421: step 8590, loss 0.357224, acc 0.921875, prec 0.107914, recall 0.818531
2017-12-10T04:58:50.700522: step 8591, loss 0.360797, acc 0.96875, prec 0.107911, recall 0.818531
2017-12-10T04:58:50.963893: step 8592, loss 0.820789, acc 0.890625, prec 0.107913, recall 0.818545
2017-12-10T04:58:51.226740: step 8593, loss 0.986006, acc 0.890625, prec 0.107914, recall 0.81856
2017-12-10T04:58:51.489408: step 8594, loss 0.0933317, acc 0.96875, prec 0.107921, recall 0.818575
2017-12-10T04:58:51.755807: step 8595, loss 0.477698, acc 0.9375, prec 0.107917, recall 0.818575
2017-12-10T04:58:52.024753: step 8596, loss 1.85028, acc 0.953125, prec 0.107924, recall 0.818523
2017-12-10T04:58:52.294878: step 8597, loss 0.27016, acc 0.953125, prec 0.107921, recall 0.818523
2017-12-10T04:58:52.561916: step 8598, loss 0.277894, acc 0.9375, prec 0.107926, recall 0.818538
2017-12-10T04:58:52.825147: step 8599, loss 0.149609, acc 0.96875, prec 0.107942, recall 0.818567
2017-12-10T04:58:53.090006: step 8600, loss 0.457949, acc 0.9375, prec 0.107947, recall 0.818582
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-8600

2017-12-10T04:58:54.449263: step 8601, loss 0.181055, acc 0.953125, prec 0.107944, recall 0.818582
2017-12-10T04:58:54.713301: step 8602, loss 0.0058362, acc 1, prec 0.107944, recall 0.818582
2017-12-10T04:58:54.981334: step 8603, loss 1.02199, acc 0.859375, prec 0.107943, recall 0.818597
2017-12-10T04:58:55.245550: step 8604, loss 0.145256, acc 0.96875, prec 0.107941, recall 0.818597
2017-12-10T04:58:55.511065: step 8605, loss 0.610706, acc 0.90625, prec 0.107943, recall 0.818612
2017-12-10T04:58:55.778833: step 8606, loss 0.623007, acc 0.953125, prec 0.10794, recall 0.818612
2017-12-10T04:58:56.042995: step 8607, loss 0.381195, acc 0.921875, prec 0.107934, recall 0.818612
2017-12-10T04:58:56.308995: step 8608, loss 0.32922, acc 0.9375, prec 0.107939, recall 0.818627
2017-12-10T04:58:56.572107: step 8609, loss 0.0925865, acc 0.953125, prec 0.107945, recall 0.818641
2017-12-10T04:58:56.837850: step 8610, loss 0.344432, acc 0.953125, prec 0.107951, recall 0.818656
2017-12-10T04:58:57.111376: step 8611, loss 0.191812, acc 0.96875, prec 0.107949, recall 0.818656
2017-12-10T04:58:57.382249: step 8612, loss 0.141713, acc 0.96875, prec 0.107956, recall 0.818671
2017-12-10T04:58:57.647299: step 8613, loss 0.514073, acc 0.9375, prec 0.107961, recall 0.818686
2017-12-10T04:58:57.918269: step 8614, loss 0.0655096, acc 0.96875, prec 0.107968, recall 0.818701
2017-12-10T04:58:58.189387: step 8615, loss 0.18815, acc 0.96875, prec 0.107966, recall 0.818701
2017-12-10T04:58:58.456082: step 8616, loss 0.617362, acc 0.96875, prec 0.107964, recall 0.818701
2017-12-10T04:58:58.726608: step 8617, loss 0.356038, acc 0.9375, prec 0.107978, recall 0.81873
2017-12-10T04:58:58.992998: step 8618, loss 0.157455, acc 0.96875, prec 0.107986, recall 0.818745
2017-12-10T04:58:59.260440: step 8619, loss 0.00386298, acc 1, prec 0.107986, recall 0.818745
2017-12-10T04:58:59.527473: step 8620, loss 0.7027, acc 0.96875, prec 0.108002, recall 0.818774
2017-12-10T04:58:59.804817: step 8621, loss 0.285925, acc 0.953125, prec 0.108008, recall 0.818789
2017-12-10T04:59:00.075799: step 8622, loss 0.050124, acc 0.96875, prec 0.108006, recall 0.818789
2017-12-10T04:59:00.352590: step 8623, loss 0.301126, acc 0.9375, prec 0.108002, recall 0.818789
2017-12-10T04:59:00.624675: step 8624, loss 0.179824, acc 0.9375, prec 0.108016, recall 0.818819
2017-12-10T04:59:00.890829: step 8625, loss 0.11567, acc 0.984375, prec 0.108034, recall 0.818848
2017-12-10T04:59:01.158846: step 8626, loss 0.247264, acc 0.96875, prec 0.108032, recall 0.818848
2017-12-10T04:59:01.428896: step 8627, loss 0.514185, acc 0.96875, prec 0.108049, recall 0.818878
2017-12-10T04:59:01.703495: step 8628, loss 0.173268, acc 0.96875, prec 0.108056, recall 0.818892
2017-12-10T04:59:01.968125: step 8629, loss 0.307521, acc 0.984375, prec 0.108064, recall 0.818907
2017-12-10T04:59:02.235701: step 8630, loss 0.518118, acc 0.953125, prec 0.10808, recall 0.818937
2017-12-10T04:59:02.501573: step 8631, loss 0.0108353, acc 1, prec 0.10809, recall 0.818951
2017-12-10T04:59:02.767620: step 8632, loss 1.99716, acc 0.953125, prec 0.108087, recall 0.818885
2017-12-10T04:59:03.043295: step 8633, loss 0.00599448, acc 1, prec 0.108087, recall 0.818885
2017-12-10T04:59:03.310504: step 8634, loss 0.238265, acc 0.96875, prec 0.108094, recall 0.8189
2017-12-10T04:59:03.583898: step 8635, loss 0.57469, acc 0.921875, prec 0.108098, recall 0.818914
2017-12-10T04:59:03.860112: step 8636, loss 0.285705, acc 0.96875, prec 0.108115, recall 0.818944
2017-12-10T04:59:04.128049: step 8637, loss 0.426202, acc 0.921875, prec 0.108119, recall 0.818959
2017-12-10T04:59:04.395954: step 8638, loss 1.0561, acc 0.9375, prec 0.108153, recall 0.819017
2017-12-10T04:59:04.664313: step 8639, loss 0.959848, acc 0.9375, prec 0.108148, recall 0.819017
2017-12-10T04:59:04.939284: step 8640, loss 0.587362, acc 0.90625, prec 0.10817, recall 0.819062
2017-12-10T04:59:05.205290: step 8641, loss 0.57012, acc 0.921875, prec 0.108164, recall 0.819062
2017-12-10T04:59:05.478817: step 8642, loss 0.00169872, acc 1, prec 0.108173, recall 0.819076
2017-12-10T04:59:05.739117: step 8643, loss 0.325422, acc 0.953125, prec 0.10817, recall 0.819076
2017-12-10T04:59:06.004586: step 8644, loss 0.711936, acc 0.90625, prec 0.108163, recall 0.819076
2017-12-10T04:59:06.270081: step 8645, loss 0.225424, acc 0.984375, prec 0.108191, recall 0.81912
2017-12-10T04:59:06.537780: step 8646, loss 1.19545, acc 0.921875, prec 0.108185, recall 0.81912
2017-12-10T04:59:06.810487: step 8647, loss 0.0661795, acc 0.984375, prec 0.108193, recall 0.819135
2017-12-10T04:59:07.076639: step 8648, loss 0.116119, acc 0.96875, prec 0.10821, recall 0.819165
2017-12-10T04:59:07.339630: step 8649, loss 0.160054, acc 0.984375, prec 0.108228, recall 0.819194
2017-12-10T04:59:07.613432: step 8650, loss 0.143844, acc 0.9375, prec 0.108223, recall 0.819194
2017-12-10T04:59:07.880333: step 8651, loss 0.179537, acc 0.96875, prec 0.108221, recall 0.819194
2017-12-10T04:59:08.145456: step 8652, loss 0.220336, acc 0.96875, prec 0.108219, recall 0.819194
2017-12-10T04:59:08.413822: step 8653, loss 0.449544, acc 0.953125, prec 0.108244, recall 0.819238
2017-12-10T04:59:08.674120: step 8654, loss 0.178624, acc 0.96875, prec 0.108261, recall 0.819267
2017-12-10T04:59:08.941769: step 8655, loss 0.792385, acc 0.90625, prec 0.108254, recall 0.819267
2017-12-10T04:59:09.209573: step 8656, loss 0.0811554, acc 0.984375, prec 0.108272, recall 0.819297
2017-12-10T04:59:09.480597: step 8657, loss 0.911555, acc 0.953125, prec 0.108278, recall 0.819311
2017-12-10T04:59:09.744953: step 8658, loss 0.028814, acc 0.984375, prec 0.108286, recall 0.819326
2017-12-10T04:59:10.008002: step 8659, loss 0.0785914, acc 0.96875, prec 0.108313, recall 0.81937
2017-12-10T04:59:10.277816: step 8660, loss 0.437934, acc 0.953125, prec 0.108309, recall 0.81937
2017-12-10T04:59:10.542930: step 8661, loss 0.0769015, acc 0.984375, prec 0.108327, recall 0.819399
2017-12-10T04:59:10.803713: step 8662, loss 0.0149254, acc 0.984375, prec 0.108326, recall 0.819399
2017-12-10T04:59:11.075819: step 8663, loss 0.268275, acc 0.984375, prec 0.108334, recall 0.819414
2017-12-10T04:59:11.338542: step 8664, loss 0.0217091, acc 0.984375, prec 0.108362, recall 0.819458
2017-12-10T04:59:11.605970: step 8665, loss 5.94079, acc 0.953125, prec 0.10836, recall 0.819391
2017-12-10T04:59:11.874867: step 8666, loss 9.60576e-06, acc 1, prec 0.108379, recall 0.819421
2017-12-10T04:59:12.152440: step 8667, loss 0.143332, acc 0.984375, prec 0.108387, recall 0.819435
2017-12-10T04:59:12.426171: step 8668, loss 0.170134, acc 0.96875, prec 0.108394, recall 0.81945
2017-12-10T04:59:12.687857: step 8669, loss 0.0155485, acc 1, prec 0.108404, recall 0.819465
2017-12-10T04:59:12.960131: step 8670, loss 0.223436, acc 0.953125, prec 0.1084, recall 0.819465
2017-12-10T04:59:13.220782: step 8671, loss 0.310416, acc 0.9375, prec 0.108405, recall 0.819479
2017-12-10T04:59:13.489578: step 8672, loss 1.5426, acc 0.890625, prec 0.108397, recall 0.819479
2017-12-10T04:59:13.758798: step 8673, loss 0.448906, acc 0.953125, prec 0.108403, recall 0.819494
2017-12-10T04:59:14.021770: step 8674, loss 0.459061, acc 0.90625, prec 0.108415, recall 0.819523
2017-12-10T04:59:14.290285: step 8675, loss 0.709051, acc 0.890625, prec 0.108407, recall 0.819523
2017-12-10T04:59:14.562138: step 8676, loss 0.393507, acc 0.9375, prec 0.108422, recall 0.819553
2017-12-10T04:59:14.831530: step 8677, loss 0.891139, acc 0.875, prec 0.108441, recall 0.819596
2017-12-10T04:59:15.094436: step 8678, loss 1.86202, acc 0.859375, prec 0.108459, recall 0.81964
2017-12-10T04:59:15.358265: step 8679, loss 0.111411, acc 0.96875, prec 0.108467, recall 0.819655
2017-12-10T04:59:15.620480: step 8680, loss 1.16807, acc 0.921875, prec 0.108461, recall 0.819655
2017-12-10T04:59:15.887103: step 8681, loss 0.682745, acc 0.9375, prec 0.108466, recall 0.819669
2017-12-10T04:59:16.155806: step 8682, loss 0.6182, acc 0.90625, prec 0.108459, recall 0.819669
2017-12-10T04:59:16.417175: step 8683, loss 0.288334, acc 0.90625, prec 0.10849, recall 0.819728
2017-12-10T04:59:16.680813: step 8684, loss 0.439386, acc 0.90625, prec 0.108512, recall 0.819772
2017-12-10T04:59:16.943008: step 8685, loss 0.741053, acc 0.9375, prec 0.108516, recall 0.819786
2017-12-10T04:59:17.204136: step 8686, loss 0.123802, acc 0.953125, prec 0.108513, recall 0.819786
2017-12-10T04:59:17.472469: step 8687, loss 0.406903, acc 0.921875, prec 0.108507, recall 0.819786
2017-12-10T04:59:17.738799: step 8688, loss 0.956382, acc 0.90625, prec 0.1085, recall 0.819786
2017-12-10T04:59:18.009360: step 8689, loss 0.899877, acc 0.953125, prec 0.108497, recall 0.819786
2017-12-10T04:59:18.274538: step 8690, loss 0.177185, acc 0.984375, prec 0.108515, recall 0.819815
2017-12-10T04:59:18.536597: step 8691, loss 0.491178, acc 0.921875, prec 0.108509, recall 0.819815
2017-12-10T04:59:18.813933: step 8692, loss 0.44842, acc 0.96875, prec 0.108526, recall 0.819845
2017-12-10T04:59:19.091695: step 8693, loss 0.393845, acc 0.96875, prec 0.108533, recall 0.819859
2017-12-10T04:59:19.361677: step 8694, loss 0.715551, acc 0.921875, prec 0.108527, recall 0.819859
2017-12-10T04:59:19.629962: step 8695, loss 0.452943, acc 0.96875, prec 0.108534, recall 0.819874
2017-12-10T04:59:19.897004: step 8696, loss 0.221229, acc 0.953125, prec 0.10854, recall 0.819888
2017-12-10T04:59:20.164360: step 8697, loss 0.00821521, acc 1, prec 0.108559, recall 0.819918
2017-12-10T04:59:20.434611: step 8698, loss 0.650675, acc 0.984375, prec 0.108568, recall 0.819932
2017-12-10T04:59:20.700156: step 8699, loss 0.000960223, acc 1, prec 0.108596, recall 0.819976
2017-12-10T04:59:20.966993: step 8700, loss 0.295635, acc 0.9375, prec 0.108611, recall 0.820005

Evaluation:
2017-12-10T04:59:28.683181: step 8700, loss 13.1326, acc 0.963106, prec 0.10873, recall 0.814143

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-8700

2017-12-10T04:59:29.907928: step 8701, loss 0.275831, acc 0.984375, prec 0.108729, recall 0.814143
2017-12-10T04:59:30.176775: step 8702, loss 0.136588, acc 0.953125, prec 0.108726, recall 0.814143
2017-12-10T04:59:30.452459: step 8703, loss 0.477555, acc 0.953125, prec 0.108741, recall 0.814173
2017-12-10T04:59:30.724280: step 8704, loss 0.211229, acc 0.984375, prec 0.108759, recall 0.814202
2017-12-10T04:59:30.994680: step 8705, loss 0.0749391, acc 0.984375, prec 0.108777, recall 0.814232
2017-12-10T04:59:31.258360: step 8706, loss 0.128809, acc 0.984375, prec 0.108776, recall 0.814232
2017-12-10T04:59:31.523432: step 8707, loss 0.000145061, acc 1, prec 0.108776, recall 0.814232
2017-12-10T04:59:31.787538: step 8708, loss 0.000778094, acc 1, prec 0.108795, recall 0.814262
2017-12-10T04:59:32.046931: step 8709, loss 0.771993, acc 0.984375, prec 0.108803, recall 0.814277
2017-12-10T04:59:32.320907: step 8710, loss 0.000564648, acc 1, prec 0.108832, recall 0.814321
2017-12-10T04:59:32.586753: step 8711, loss 6.63511e-05, acc 1, prec 0.108841, recall 0.814336
2017-12-10T04:59:32.846966: step 8712, loss 0.488395, acc 0.984375, prec 0.108849, recall 0.814351
2017-12-10T04:59:33.117701: step 8713, loss 0.11315, acc 0.984375, prec 0.108877, recall 0.814395
2017-12-10T04:59:33.383572: step 8714, loss 0.0200111, acc 0.984375, prec 0.108876, recall 0.814395
2017-12-10T04:59:33.649406: step 8715, loss 0.075731, acc 0.984375, prec 0.108893, recall 0.814425
2017-12-10T04:59:33.915498: step 8716, loss 0.000672137, acc 1, prec 0.108903, recall 0.81444
2017-12-10T04:59:34.177895: step 8717, loss 0.524621, acc 0.953125, prec 0.108909, recall 0.814454
2017-12-10T04:59:34.443271: step 8718, loss 0.179002, acc 0.96875, prec 0.108907, recall 0.814454
2017-12-10T04:59:34.718007: step 8719, loss 9.79638, acc 0.96875, prec 0.108915, recall 0.814404
2017-12-10T04:59:34.986747: step 8720, loss 0.000831477, acc 1, prec 0.108925, recall 0.814419
2017-12-10T04:59:35.251763: step 8721, loss 0.782616, acc 0.875, prec 0.108925, recall 0.814434
2017-12-10T04:59:35.516973: step 8722, loss 0.535508, acc 0.9375, prec 0.10893, recall 0.814449
2017-12-10T04:59:35.790332: step 8723, loss 1.3228, acc 0.859375, prec 0.108919, recall 0.814449
2017-12-10T04:59:36.057798: step 8724, loss 0.141911, acc 0.953125, prec 0.108935, recall 0.814478
2017-12-10T04:59:36.325895: step 8725, loss 0.111841, acc 0.953125, prec 0.10895, recall 0.814508
2017-12-10T04:59:36.591678: step 8726, loss 1.27879, acc 0.9375, prec 0.108965, recall 0.814537
2017-12-10T04:59:36.858544: step 8727, loss 0.338806, acc 0.96875, prec 0.109, recall 0.814596
2017-12-10T04:59:37.128274: step 8728, loss 0.520726, acc 0.90625, prec 0.109003, recall 0.814611
2017-12-10T04:59:37.396901: step 8729, loss 0.253521, acc 0.953125, prec 0.109009, recall 0.814626
2017-12-10T04:59:37.664088: step 8730, loss 0.094251, acc 0.96875, prec 0.109016, recall 0.814641
2017-12-10T04:59:37.933371: step 8731, loss 0.460486, acc 0.96875, prec 0.109033, recall 0.81467
2017-12-10T04:59:38.199830: step 8732, loss 0.904697, acc 0.9375, prec 0.109037, recall 0.814685
2017-12-10T04:59:38.471741: step 8733, loss 0.674922, acc 0.9375, prec 0.109052, recall 0.814715
2017-12-10T04:59:38.733317: step 8734, loss 1.39146, acc 0.859375, prec 0.109041, recall 0.814715
2017-12-10T04:59:39.000268: step 8735, loss 0.41348, acc 0.9375, prec 0.109046, recall 0.814729
2017-12-10T04:59:39.264062: step 8736, loss 0.671403, acc 0.921875, prec 0.10905, recall 0.814744
2017-12-10T04:59:39.527258: step 8737, loss 0.351902, acc 0.90625, prec 0.109052, recall 0.814759
2017-12-10T04:59:39.791616: step 8738, loss 0.5596, acc 0.875, prec 0.109053, recall 0.814774
2017-12-10T04:59:40.057488: step 8739, loss 0.716391, acc 0.875, prec 0.109043, recall 0.814774
2017-12-10T04:59:40.328994: step 8740, loss 0.29088, acc 0.90625, prec 0.109036, recall 0.814774
2017-12-10T04:59:40.594987: step 8741, loss 0.430882, acc 0.953125, prec 0.109042, recall 0.814788
2017-12-10T04:59:40.863370: step 8742, loss 0.581098, acc 0.90625, prec 0.109045, recall 0.814803
2017-12-10T04:59:41.126411: step 8743, loss 0.389014, acc 0.921875, prec 0.109049, recall 0.814818
2017-12-10T04:59:41.392070: step 8744, loss 0.0986137, acc 0.96875, prec 0.109046, recall 0.814818
2017-12-10T04:59:41.656816: step 8745, loss 0.0577808, acc 0.984375, prec 0.109064, recall 0.814847
2017-12-10T04:59:41.922964: step 8746, loss 0.141157, acc 0.984375, prec 0.109063, recall 0.814847
2017-12-10T04:59:42.207332: step 8747, loss 0.138696, acc 0.984375, prec 0.109071, recall 0.814862
2017-12-10T04:59:42.472673: step 8748, loss 0.206632, acc 0.953125, prec 0.109077, recall 0.814877
2017-12-10T04:59:42.741707: step 8749, loss 0.0211482, acc 1, prec 0.109087, recall 0.814891
2017-12-10T04:59:43.004325: step 8750, loss 0.515364, acc 0.921875, prec 0.10909, recall 0.814906
2017-12-10T04:59:43.267949: step 8751, loss 0.0398897, acc 0.984375, prec 0.109089, recall 0.814906
2017-12-10T04:59:43.532703: step 8752, loss 0.471978, acc 0.953125, prec 0.109086, recall 0.814906
2017-12-10T04:59:43.807507: step 8753, loss 0.0374156, acc 0.984375, prec 0.109085, recall 0.814906
2017-12-10T04:59:44.071303: step 8754, loss 0.12786, acc 0.984375, prec 0.109093, recall 0.814921
2017-12-10T04:59:44.340135: step 8755, loss 0.349714, acc 0.984375, prec 0.109101, recall 0.814936
2017-12-10T04:59:44.604921: step 8756, loss 0.417498, acc 0.953125, prec 0.109098, recall 0.814936
2017-12-10T04:59:44.876518: step 8757, loss 0.436717, acc 0.953125, prec 0.109094, recall 0.814936
2017-12-10T04:59:45.143016: step 8758, loss 0.104412, acc 0.984375, prec 0.109103, recall 0.81495
2017-12-10T04:59:45.411342: step 8759, loss 0.127651, acc 0.96875, prec 0.10911, recall 0.814965
2017-12-10T04:59:45.682387: step 8760, loss 0.0728825, acc 0.984375, prec 0.109118, recall 0.81498
2017-12-10T04:59:45.951121: step 8761, loss 0.000566471, acc 1, prec 0.109118, recall 0.81498
2017-12-10T04:59:46.215706: step 8762, loss 0.0452948, acc 0.984375, prec 0.109126, recall 0.814994
2017-12-10T04:59:46.486294: step 8763, loss 0.00369438, acc 1, prec 0.109145, recall 0.815024
2017-12-10T04:59:46.751854: step 8764, loss 8.16275e-06, acc 1, prec 0.109145, recall 0.815024
2017-12-10T04:59:47.015685: step 8765, loss 10.7084, acc 0.984375, prec 0.109145, recall 0.814959
2017-12-10T04:59:47.282590: step 8766, loss 4.02614e-05, acc 1, prec 0.109145, recall 0.814959
2017-12-10T04:59:47.542984: step 8767, loss 0.0969298, acc 0.96875, prec 0.109152, recall 0.814974
2017-12-10T04:59:47.813944: step 8768, loss 0.0083197, acc 1, prec 0.109152, recall 0.814974
2017-12-10T04:59:48.081885: step 8769, loss 0.00502455, acc 1, prec 0.109152, recall 0.814974
2017-12-10T04:59:48.345668: step 8770, loss 0.354041, acc 0.96875, prec 0.10916, recall 0.814988
2017-12-10T04:59:48.617283: step 8771, loss 0.0242472, acc 0.984375, prec 0.109168, recall 0.815003
2017-12-10T04:59:48.885654: step 8772, loss 0.0148868, acc 0.984375, prec 0.109176, recall 0.815018
2017-12-10T04:59:49.153912: step 8773, loss 0.423217, acc 0.984375, prec 0.109194, recall 0.815047
2017-12-10T04:59:49.421316: step 8774, loss 0.078976, acc 0.984375, prec 0.109193, recall 0.815047
2017-12-10T04:59:49.685908: step 8775, loss 0.758496, acc 0.9375, prec 0.109226, recall 0.815106
2017-12-10T04:59:49.950587: step 8776, loss 0.0365148, acc 0.96875, prec 0.109233, recall 0.815121
2017-12-10T04:59:50.214842: step 8777, loss 0.805688, acc 0.96875, prec 0.10924, recall 0.815135
2017-12-10T04:59:50.480273: step 8778, loss 0.000815683, acc 1, prec 0.10924, recall 0.815135
2017-12-10T04:59:50.745636: step 8779, loss 0.00106789, acc 1, prec 0.10925, recall 0.81515
2017-12-10T04:59:51.008964: step 8780, loss 0.84303, acc 0.90625, prec 0.109243, recall 0.81515
2017-12-10T04:59:51.273355: step 8781, loss 0.265042, acc 0.921875, prec 0.109247, recall 0.815165
2017-12-10T04:59:51.535530: step 8782, loss 0.260433, acc 0.96875, prec 0.109244, recall 0.815165
2017-12-10T04:59:51.797330: step 8783, loss 0.396463, acc 0.96875, prec 0.109261, recall 0.815194
2017-12-10T04:59:52.063479: step 8784, loss 0.00784746, acc 1, prec 0.10928, recall 0.815223
2017-12-10T04:59:52.328085: step 8785, loss 0.130097, acc 0.984375, prec 0.109279, recall 0.815223
2017-12-10T04:59:52.591564: step 8786, loss 0.0537219, acc 0.953125, prec 0.109285, recall 0.815238
2017-12-10T04:59:52.854904: step 8787, loss 0.0714338, acc 0.984375, prec 0.109284, recall 0.815238
2017-12-10T04:59:53.125292: step 8788, loss 0.471454, acc 0.921875, prec 0.109287, recall 0.815253
2017-12-10T04:59:53.389179: step 8789, loss 0.00545161, acc 1, prec 0.109306, recall 0.815282
2017-12-10T04:59:53.649398: step 8790, loss 0.216568, acc 0.9375, prec 0.10932, recall 0.815311
2017-12-10T04:59:53.911174: step 8791, loss 0.956441, acc 0.96875, prec 0.109318, recall 0.815311
2017-12-10T04:59:54.174926: step 8792, loss 0.18286, acc 0.984375, prec 0.109336, recall 0.815341
2017-12-10T04:59:54.439604: step 8793, loss 0.558839, acc 0.984375, prec 0.109335, recall 0.815341
2017-12-10T04:59:54.706557: step 8794, loss 0.194872, acc 0.96875, prec 0.109342, recall 0.815355
2017-12-10T04:59:54.970229: step 8795, loss 0.00313676, acc 1, prec 0.109342, recall 0.815355
2017-12-10T04:59:55.231457: step 8796, loss 1.34707, acc 0.96875, prec 0.10935, recall 0.815305
2017-12-10T04:59:55.503620: step 8797, loss 0.0313278, acc 0.984375, prec 0.109359, recall 0.81532
2017-12-10T04:59:55.771304: step 8798, loss 0.463004, acc 0.984375, prec 0.109367, recall 0.815335
2017-12-10T04:59:56.039664: step 8799, loss 0.00235138, acc 1, prec 0.109386, recall 0.815364
2017-12-10T04:59:56.306197: step 8800, loss 0.108789, acc 0.96875, prec 0.109402, recall 0.815393
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-8800

2017-12-10T04:59:57.596969: step 8801, loss 0.150218, acc 0.96875, prec 0.1094, recall 0.815393
2017-12-10T04:59:57.871413: step 8802, loss 3.2187e-05, acc 1, prec 0.10941, recall 0.815408
2017-12-10T04:59:58.131525: step 8803, loss 0.252138, acc 0.96875, prec 0.109445, recall 0.815466
2017-12-10T04:59:58.399567: step 8804, loss 0.227582, acc 0.984375, prec 0.109482, recall 0.815525
2017-12-10T04:59:58.676394: step 8805, loss 0.0108696, acc 1, prec 0.109491, recall 0.815539
2017-12-10T04:59:58.943510: step 8806, loss 0.0952523, acc 0.953125, prec 0.109488, recall 0.815539
2017-12-10T04:59:59.205947: step 8807, loss 0.552596, acc 0.9375, prec 0.109512, recall 0.815583
2017-12-10T04:59:59.478462: step 8808, loss 0.475103, acc 0.96875, prec 0.109528, recall 0.815612
2017-12-10T04:59:59.747983: step 8809, loss 0.286982, acc 0.96875, prec 0.109526, recall 0.815612
2017-12-10T05:00:00.018314: step 8810, loss 0.168807, acc 0.90625, prec 0.109519, recall 0.815612
2017-12-10T05:00:00.282665: step 8811, loss 0.941305, acc 0.953125, prec 0.109515, recall 0.815612
2017-12-10T05:00:00.548515: step 8812, loss 0.114667, acc 0.96875, prec 0.109522, recall 0.815627
2017-12-10T05:00:00.814742: step 8813, loss 0.236869, acc 1, prec 0.109532, recall 0.815642
2017-12-10T05:00:01.080178: step 8814, loss 0.299933, acc 0.9375, prec 0.109546, recall 0.815671
2017-12-10T05:00:01.346893: step 8815, loss 0.520727, acc 0.96875, prec 0.109553, recall 0.815685
2017-12-10T05:00:01.614379: step 8816, loss 0.273668, acc 0.96875, prec 0.109551, recall 0.815685
2017-12-10T05:00:01.876649: step 8817, loss 0.243836, acc 0.953125, prec 0.109548, recall 0.815685
2017-12-10T05:00:02.145739: step 8818, loss 0.433465, acc 0.9375, prec 0.109552, recall 0.8157
2017-12-10T05:00:02.410427: step 8819, loss 0.214778, acc 0.96875, prec 0.10955, recall 0.8157
2017-12-10T05:00:02.677639: step 8820, loss 3.07666e-05, acc 1, prec 0.109569, recall 0.815729
2017-12-10T05:00:02.941320: step 8821, loss 0.129318, acc 0.96875, prec 0.109576, recall 0.815744
2017-12-10T05:00:03.209943: step 8822, loss 0.251831, acc 1, prec 0.109586, recall 0.815758
2017-12-10T05:00:03.475732: step 8823, loss 0.221184, acc 0.96875, prec 0.109593, recall 0.815773
2017-12-10T05:00:03.741477: step 8824, loss 0.273683, acc 0.953125, prec 0.109589, recall 0.815773
2017-12-10T05:00:04.013134: step 8825, loss 0.269876, acc 0.953125, prec 0.109595, recall 0.815787
2017-12-10T05:00:04.278628: step 8826, loss 8.05205e-05, acc 1, prec 0.109614, recall 0.815817
2017-12-10T05:00:04.536853: step 8827, loss 0.429277, acc 0.953125, prec 0.10962, recall 0.815831
2017-12-10T05:00:04.798674: step 8828, loss 0.623641, acc 0.9375, prec 0.109615, recall 0.815831
2017-12-10T05:00:05.061882: step 8829, loss 0.130056, acc 0.96875, prec 0.109623, recall 0.815846
2017-12-10T05:00:05.328019: step 8830, loss 0.474979, acc 0.953125, prec 0.109619, recall 0.815846
2017-12-10T05:00:05.592514: step 8831, loss 0.00933003, acc 1, prec 0.109628, recall 0.81586
2017-12-10T05:00:05.858134: step 8832, loss 0.299197, acc 0.96875, prec 0.109626, recall 0.81586
2017-12-10T05:00:06.132525: step 8833, loss 0.409569, acc 0.96875, prec 0.109643, recall 0.815889
2017-12-10T05:00:06.397772: step 8834, loss 0.00293323, acc 1, prec 0.109671, recall 0.815933
2017-12-10T05:00:06.661563: step 8835, loss 0.537662, acc 0.953125, prec 0.109668, recall 0.815933
2017-12-10T05:00:06.927474: step 8836, loss 0.194859, acc 0.96875, prec 0.109665, recall 0.815933
2017-12-10T05:00:07.188443: step 8837, loss 0.00991137, acc 1, prec 0.109665, recall 0.815933
2017-12-10T05:00:07.457476: step 8838, loss 0.220661, acc 0.96875, prec 0.109663, recall 0.815933
2017-12-10T05:00:07.723671: step 8839, loss 0.107837, acc 0.984375, prec 0.109671, recall 0.815948
2017-12-10T05:00:07.995073: step 8840, loss 0.156246, acc 0.9375, prec 0.109667, recall 0.815948
2017-12-10T05:00:08.266312: step 8841, loss 0.106495, acc 0.96875, prec 0.109683, recall 0.815977
2017-12-10T05:00:08.529207: step 8842, loss 0.075617, acc 0.984375, prec 0.109691, recall 0.815991
2017-12-10T05:00:08.793883: step 8843, loss 0.147444, acc 0.984375, prec 0.1097, recall 0.816006
2017-12-10T05:00:09.064609: step 8844, loss 0.000259563, acc 1, prec 0.109709, recall 0.81602
2017-12-10T05:00:09.330589: step 8845, loss 0.0694203, acc 0.984375, prec 0.109708, recall 0.81602
2017-12-10T05:00:09.604499: step 8846, loss 0.973798, acc 0.984375, prec 0.109716, recall 0.816035
2017-12-10T05:00:09.870684: step 8847, loss 0.2996, acc 0.984375, prec 0.109715, recall 0.816035
2017-12-10T05:00:10.132224: step 8848, loss 0.0665559, acc 0.984375, prec 0.109723, recall 0.816049
2017-12-10T05:00:10.406640: step 8849, loss 5.33837e-05, acc 1, prec 0.109742, recall 0.816078
2017-12-10T05:00:10.678738: step 8850, loss 0.0752253, acc 0.96875, prec 0.109749, recall 0.816093
2017-12-10T05:00:10.941033: step 8851, loss 0.159636, acc 1, prec 0.109797, recall 0.816165
2017-12-10T05:00:11.205076: step 8852, loss 0.483393, acc 0.96875, prec 0.109794, recall 0.816165
2017-12-10T05:00:11.471612: step 8853, loss 0.0539301, acc 0.96875, prec 0.109792, recall 0.816165
2017-12-10T05:00:11.740939: step 8854, loss 0.0586163, acc 0.96875, prec 0.10979, recall 0.816165
2017-12-10T05:00:12.019746: step 8855, loss 0.348805, acc 0.984375, prec 0.109798, recall 0.81618
2017-12-10T05:00:12.291586: step 8856, loss 0.396879, acc 0.953125, prec 0.109804, recall 0.816194
2017-12-10T05:00:12.555377: step 8857, loss 0.147727, acc 0.96875, prec 0.109811, recall 0.816209
2017-12-10T05:00:12.818535: step 8858, loss 0.388592, acc 0.9375, prec 0.109806, recall 0.816209
2017-12-10T05:00:13.082256: step 8859, loss 0.198366, acc 0.9375, prec 0.109811, recall 0.816223
2017-12-10T05:00:13.348185: step 8860, loss 0.0016736, acc 1, prec 0.109811, recall 0.816223
2017-12-10T05:00:13.615063: step 8861, loss 0.00101207, acc 1, prec 0.109811, recall 0.816223
2017-12-10T05:00:13.875769: step 8862, loss 0.188258, acc 0.96875, prec 0.109828, recall 0.816252
2017-12-10T05:00:14.143551: step 8863, loss 0.440596, acc 0.96875, prec 0.109835, recall 0.816267
2017-12-10T05:00:14.404152: step 8864, loss 0.155439, acc 0.953125, prec 0.109831, recall 0.816267
2017-12-10T05:00:14.667302: step 8865, loss 0.00345096, acc 1, prec 0.10986, recall 0.81631
2017-12-10T05:00:14.934291: step 8866, loss 0.054193, acc 0.984375, prec 0.109878, recall 0.816339
2017-12-10T05:00:15.193589: step 8867, loss 0.257017, acc 0.96875, prec 0.109875, recall 0.816339
2017-12-10T05:00:15.458960: step 8868, loss 0.0020715, acc 1, prec 0.109875, recall 0.816339
2017-12-10T05:00:15.724683: step 8869, loss 0.199718, acc 0.96875, prec 0.109882, recall 0.816354
2017-12-10T05:00:15.997907: step 8870, loss 0.174944, acc 0.984375, prec 0.109881, recall 0.816354
2017-12-10T05:00:16.262659: step 8871, loss 0.399073, acc 0.9375, prec 0.109886, recall 0.816368
2017-12-10T05:00:16.527901: step 8872, loss 0.379234, acc 0.984375, prec 0.109904, recall 0.816397
2017-12-10T05:00:16.790893: step 8873, loss 1.61553, acc 0.96875, prec 0.109912, recall 0.816347
2017-12-10T05:00:17.061337: step 8874, loss 0.558078, acc 0.953125, prec 0.109908, recall 0.816347
2017-12-10T05:00:17.326684: step 8875, loss 0.000137913, acc 1, prec 0.109927, recall 0.816376
2017-12-10T05:00:17.586789: step 8876, loss 0.000921259, acc 1, prec 0.109937, recall 0.816391
2017-12-10T05:00:17.854107: step 8877, loss 0.284217, acc 0.984375, prec 0.109936, recall 0.816391
2017-12-10T05:00:18.118493: step 8878, loss 0.684002, acc 0.96875, prec 0.109943, recall 0.816405
2017-12-10T05:00:18.380937: step 8879, loss 0.249177, acc 0.953125, prec 0.109939, recall 0.816405
2017-12-10T05:00:18.647158: step 8880, loss 0.151744, acc 0.96875, prec 0.109937, recall 0.816405
2017-12-10T05:00:18.911858: step 8881, loss 0.327082, acc 0.984375, prec 0.109955, recall 0.816434
2017-12-10T05:00:19.188755: step 8882, loss 0.00996218, acc 1, prec 0.109955, recall 0.816434
2017-12-10T05:00:19.459131: step 8883, loss 0.0327187, acc 0.984375, prec 0.109982, recall 0.816478
2017-12-10T05:00:19.721056: step 8884, loss 0.21721, acc 0.953125, prec 0.109988, recall 0.816492
2017-12-10T05:00:19.984026: step 8885, loss 0.0347223, acc 0.96875, prec 0.109985, recall 0.816492
2017-12-10T05:00:20.250803: step 8886, loss 0.00102056, acc 1, prec 0.109995, recall 0.816507
2017-12-10T05:00:20.515670: step 8887, loss 0.191825, acc 0.984375, prec 0.110031, recall 0.816564
2017-12-10T05:00:20.782974: step 8888, loss 0.218098, acc 0.984375, prec 0.11003, recall 0.816564
2017-12-10T05:00:21.051812: step 8889, loss 0.321782, acc 0.953125, prec 0.110036, recall 0.816579
2017-12-10T05:00:21.313724: step 8890, loss 0.363317, acc 0.96875, prec 0.110034, recall 0.816579
2017-12-10T05:00:21.578622: step 8891, loss 0.160751, acc 0.96875, prec 0.110041, recall 0.816593
2017-12-10T05:00:21.843681: step 8892, loss 0.0657712, acc 0.96875, prec 0.110067, recall 0.816637
2017-12-10T05:00:22.111419: step 8893, loss 0.0951372, acc 0.984375, prec 0.110066, recall 0.816637
2017-12-10T05:00:22.385928: step 8894, loss 0.163079, acc 0.984375, prec 0.110074, recall 0.816651
2017-12-10T05:00:22.650782: step 8895, loss 0.534254, acc 0.96875, prec 0.110072, recall 0.816651
2017-12-10T05:00:22.917077: step 8896, loss 0.132191, acc 0.984375, prec 0.11008, recall 0.816665
2017-12-10T05:00:23.194730: step 8897, loss 0.0369226, acc 0.984375, prec 0.110079, recall 0.816665
2017-12-10T05:00:23.465419: step 8898, loss 0.0735963, acc 0.96875, prec 0.110076, recall 0.816665
2017-12-10T05:00:23.730949: step 8899, loss 0.129151, acc 0.9375, prec 0.110072, recall 0.816665
2017-12-10T05:00:24.003796: step 8900, loss 0.0240198, acc 0.984375, prec 0.110071, recall 0.816665
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-8900

2017-12-10T05:00:25.298531: step 8901, loss 0.0640169, acc 0.96875, prec 0.110068, recall 0.816665
2017-12-10T05:00:25.561409: step 8902, loss 8.44942e-05, acc 1, prec 0.110068, recall 0.816665
2017-12-10T05:00:25.824807: step 8903, loss 0.501468, acc 0.96875, prec 0.110075, recall 0.81668
2017-12-10T05:00:26.094179: step 8904, loss 0.0301525, acc 0.984375, prec 0.110074, recall 0.81668
2017-12-10T05:00:26.363260: step 8905, loss 0.608246, acc 0.96875, prec 0.110072, recall 0.81668
2017-12-10T05:00:26.631383: step 8906, loss 0.778679, acc 0.96875, prec 0.11007, recall 0.81668
2017-12-10T05:00:26.898253: step 8907, loss 0.0302344, acc 0.984375, prec 0.110068, recall 0.81668
2017-12-10T05:00:27.173986: step 8908, loss 7.79549e-06, acc 1, prec 0.110068, recall 0.81668
2017-12-10T05:00:27.430405: step 8909, loss 0.00895691, acc 1, prec 0.110078, recall 0.816694
2017-12-10T05:00:27.700445: step 8910, loss 0.00175687, acc 1, prec 0.110087, recall 0.816709
2017-12-10T05:00:27.962312: step 8911, loss 0.280963, acc 0.96875, prec 0.110094, recall 0.816723
2017-12-10T05:00:28.228046: step 8912, loss 0.0394641, acc 0.984375, prec 0.110093, recall 0.816723
2017-12-10T05:00:28.489403: step 8913, loss 0.202584, acc 0.96875, prec 0.1101, recall 0.816737
2017-12-10T05:00:28.750685: step 8914, loss 0.00964152, acc 1, prec 0.11011, recall 0.816752
2017-12-10T05:00:29.016919: step 8915, loss 0.000402375, acc 1, prec 0.110119, recall 0.816766
2017-12-10T05:00:29.277885: step 8916, loss 0.148788, acc 1, prec 0.110129, recall 0.816781
2017-12-10T05:00:29.550297: step 8917, loss 0.0100239, acc 1, prec 0.110138, recall 0.816795
2017-12-10T05:00:29.816756: step 8918, loss 0.058759, acc 0.96875, prec 0.110136, recall 0.816795
2017-12-10T05:00:30.081797: step 8919, loss 0.0867877, acc 0.984375, prec 0.110144, recall 0.816809
2017-12-10T05:00:30.354373: step 8920, loss 0.0658915, acc 0.96875, prec 0.110142, recall 0.816809
2017-12-10T05:00:30.622901: step 8921, loss 2.91826, acc 0.984375, prec 0.110189, recall 0.816817
2017-12-10T05:00:30.894178: step 8922, loss 0.199252, acc 0.953125, prec 0.110185, recall 0.816817
2017-12-10T05:00:31.158504: step 8923, loss 0.0433672, acc 0.984375, prec 0.110194, recall 0.816832
2017-12-10T05:00:31.425281: step 8924, loss 0.173023, acc 0.96875, prec 0.110191, recall 0.816832
2017-12-10T05:00:31.695482: step 8925, loss 0.0536492, acc 0.984375, prec 0.110199, recall 0.816846
2017-12-10T05:00:31.960945: step 8926, loss 0.0509201, acc 0.953125, prec 0.110215, recall 0.816875
2017-12-10T05:00:32.229022: step 8927, loss 0.682851, acc 0.921875, prec 0.110228, recall 0.816904
2017-12-10T05:00:32.490255: step 8928, loss 0.545795, acc 0.921875, prec 0.110231, recall 0.816918
2017-12-10T05:00:32.749953: step 8929, loss 0.181219, acc 0.9375, prec 0.110246, recall 0.816947
2017-12-10T05:00:33.018086: step 8930, loss 0.123576, acc 0.96875, prec 0.110262, recall 0.816975
2017-12-10T05:00:33.289038: step 8931, loss 0.318126, acc 0.921875, prec 0.110256, recall 0.816975
2017-12-10T05:00:33.550820: step 8932, loss 0.805503, acc 0.90625, prec 0.110259, recall 0.81699
2017-12-10T05:00:33.812760: step 8933, loss 0.149642, acc 0.953125, prec 0.110255, recall 0.81699
2017-12-10T05:00:34.078228: step 8934, loss 0.0497827, acc 0.984375, prec 0.110264, recall 0.817004
2017-12-10T05:00:34.348986: step 8935, loss 0.46723, acc 0.953125, prec 0.110269, recall 0.817019
2017-12-10T05:00:34.615683: step 8936, loss 0.125535, acc 0.953125, prec 0.110275, recall 0.817033
2017-12-10T05:00:34.880360: step 8937, loss 0.00233392, acc 1, prec 0.110285, recall 0.817047
2017-12-10T05:00:35.141345: step 8938, loss 0.659941, acc 0.953125, prec 0.110281, recall 0.817047
2017-12-10T05:00:35.407925: step 8939, loss 0.0523397, acc 0.984375, prec 0.11029, recall 0.817062
2017-12-10T05:00:35.678909: step 8940, loss 0.220582, acc 0.953125, prec 0.110305, recall 0.81709
2017-12-10T05:00:35.943198: step 8941, loss 0.274185, acc 0.96875, prec 0.110321, recall 0.817119
2017-12-10T05:00:36.215371: step 8942, loss 0.474551, acc 0.96875, prec 0.110338, recall 0.817148
2017-12-10T05:00:36.479476: step 8943, loss 0.760168, acc 0.953125, prec 0.110344, recall 0.817162
2017-12-10T05:00:36.742654: step 8944, loss 0.756383, acc 0.9375, prec 0.110349, recall 0.817176
2017-12-10T05:00:37.002766: step 8945, loss 0.00319438, acc 1, prec 0.110358, recall 0.817191
2017-12-10T05:00:37.238005: step 8946, loss 0.550197, acc 0.923077, prec 0.110363, recall 0.817205
2017-12-10T05:00:37.510849: step 8947, loss 0.15862, acc 0.953125, prec 0.110359, recall 0.817205
2017-12-10T05:00:37.778508: step 8948, loss 0.00226343, acc 1, prec 0.110369, recall 0.817219
2017-12-10T05:00:38.041719: step 8949, loss 0.0957891, acc 0.96875, prec 0.110376, recall 0.817234
2017-12-10T05:00:38.307408: step 8950, loss 0.0294675, acc 0.984375, prec 0.110384, recall 0.817248
2017-12-10T05:00:39.288712: step 8951, loss 0.284321, acc 0.953125, prec 0.11039, recall 0.817262
2017-12-10T05:00:39.648695: step 8952, loss 0.000481336, acc 1, prec 0.11039, recall 0.817262
2017-12-10T05:00:40.212014: step 8953, loss 0.00474827, acc 1, prec 0.110399, recall 0.817277
2017-12-10T05:00:40.921866: step 8954, loss 0.00798948, acc 1, prec 0.110428, recall 0.81732
2017-12-10T05:00:41.992102: step 8955, loss 0.000363861, acc 1, prec 0.110437, recall 0.817334
2017-12-10T05:00:42.329345: step 8956, loss 0.572799, acc 0.953125, prec 0.110433, recall 0.817334
2017-12-10T05:00:42.598372: step 8957, loss 0.146939, acc 0.984375, prec 0.110442, recall 0.817348
2017-12-10T05:00:42.883908: step 8958, loss 0.355684, acc 0.96875, prec 0.110439, recall 0.817348
2017-12-10T05:00:43.170449: step 8959, loss 0.0106761, acc 1, prec 0.110439, recall 0.817348
2017-12-10T05:00:43.457996: step 8960, loss 3.07391, acc 0.96875, prec 0.110438, recall 0.817284
2017-12-10T05:00:43.749613: step 8961, loss 9.54766e-05, acc 1, prec 0.110448, recall 0.817299
2017-12-10T05:00:44.009695: step 8962, loss 0.222629, acc 0.96875, prec 0.110445, recall 0.817299
2017-12-10T05:00:44.274079: step 8963, loss 0.000624071, acc 1, prec 0.110455, recall 0.817313
2017-12-10T05:00:44.534742: step 8964, loss 0.00171803, acc 1, prec 0.110464, recall 0.817327
2017-12-10T05:00:44.795128: step 8965, loss 0.0769871, acc 0.953125, prec 0.11047, recall 0.817342
2017-12-10T05:00:45.071193: step 8966, loss 0.379811, acc 0.96875, prec 0.110468, recall 0.817342
2017-12-10T05:00:45.340704: step 8967, loss 0.611508, acc 0.96875, prec 0.110484, recall 0.81737
2017-12-10T05:00:45.604299: step 8968, loss 1.10883, acc 0.953125, prec 0.11049, recall 0.817384
2017-12-10T05:00:45.870364: step 8969, loss 4.40371, acc 0.984375, prec 0.110509, recall 0.817349
2017-12-10T05:00:46.137422: step 8970, loss 0.0892451, acc 0.96875, prec 0.110516, recall 0.817363
2017-12-10T05:00:46.405896: step 8971, loss 0.254054, acc 0.953125, prec 0.110541, recall 0.817406
2017-12-10T05:00:46.673192: step 8972, loss 0.0693195, acc 0.9375, prec 0.110536, recall 0.817406
2017-12-10T05:00:46.941334: step 8973, loss 0.313532, acc 0.96875, prec 0.110562, recall 0.817449
2017-12-10T05:00:47.215507: step 8974, loss 0.618812, acc 0.90625, prec 0.110564, recall 0.817463
2017-12-10T05:00:47.476955: step 8975, loss 0.421798, acc 0.953125, prec 0.11057, recall 0.817478
2017-12-10T05:00:47.741506: step 8976, loss 0.616503, acc 0.875, prec 0.110561, recall 0.817478
2017-12-10T05:00:48.006910: step 8977, loss 0.268477, acc 0.953125, prec 0.110586, recall 0.817521
2017-12-10T05:00:48.274120: step 8978, loss 1.83271, acc 0.796875, prec 0.11057, recall 0.817521
2017-12-10T05:00:48.545549: step 8979, loss 0.489728, acc 0.90625, prec 0.110573, recall 0.817535
2017-12-10T05:00:48.807939: step 8980, loss 0.449738, acc 0.90625, prec 0.110575, recall 0.817549
2017-12-10T05:00:49.079442: step 8981, loss 0.843696, acc 0.921875, prec 0.110579, recall 0.817563
2017-12-10T05:00:49.345974: step 8982, loss 0.356966, acc 0.9375, prec 0.110612, recall 0.81762
2017-12-10T05:00:49.613631: step 8983, loss 1.13759, acc 0.84375, prec 0.110619, recall 0.817649
2017-12-10T05:00:49.878237: step 8984, loss 0.624038, acc 0.875, prec 0.110638, recall 0.817692
2017-12-10T05:00:50.147703: step 8985, loss 0.256305, acc 0.953125, prec 0.110644, recall 0.817706
2017-12-10T05:00:50.418809: step 8986, loss 0.794798, acc 0.890625, prec 0.110635, recall 0.817706
2017-12-10T05:00:50.685592: step 8987, loss 0.574318, acc 0.921875, prec 0.11063, recall 0.817706
2017-12-10T05:00:50.950856: step 8988, loss 0.390637, acc 0.9375, prec 0.110625, recall 0.817706
2017-12-10T05:00:51.215543: step 8989, loss 0.153057, acc 0.953125, prec 0.110621, recall 0.817706
2017-12-10T05:00:51.484961: step 8990, loss 0.115623, acc 0.984375, prec 0.110639, recall 0.817734
2017-12-10T05:00:51.758587: step 8991, loss 0.224138, acc 0.953125, prec 0.110635, recall 0.817734
2017-12-10T05:00:52.026590: step 8992, loss 0.00157854, acc 1, prec 0.110635, recall 0.817734
2017-12-10T05:00:52.303287: step 8993, loss 0.129662, acc 0.921875, prec 0.110648, recall 0.817763
2017-12-10T05:00:52.568653: step 8994, loss 0.0101789, acc 1, prec 0.110658, recall 0.817777
2017-12-10T05:00:52.832480: step 8995, loss 0.514294, acc 0.953125, prec 0.110673, recall 0.817806
2017-12-10T05:00:53.098742: step 8996, loss 0.660464, acc 0.953125, prec 0.110679, recall 0.81782
2017-12-10T05:00:53.363803: step 8997, loss 0.290725, acc 0.953125, prec 0.110685, recall 0.817834
2017-12-10T05:00:53.627061: step 8998, loss 0.131808, acc 0.96875, prec 0.110692, recall 0.817848
2017-12-10T05:00:53.896874: step 8999, loss 0.0759217, acc 0.96875, prec 0.11069, recall 0.817848
2017-12-10T05:00:54.167618: step 9000, loss 0.216286, acc 0.96875, prec 0.110697, recall 0.817862

Evaluation:
2017-12-10T05:01:01.852540: step 9000, loss 14.3786, acc 0.963955, prec 0.110801, recall 0.812071

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-9000

2017-12-10T05:01:03.143193: step 9001, loss 0.0865887, acc 0.984375, prec 0.110809, recall 0.812085
2017-12-10T05:01:03.418203: step 9002, loss 12.9997, acc 0.9375, prec 0.110815, recall 0.812037
2017-12-10T05:01:03.681953: step 9003, loss 0.40201, acc 0.96875, prec 0.110813, recall 0.812037
2017-12-10T05:01:03.948625: step 9004, loss 0.00868937, acc 1, prec 0.110822, recall 0.812052
2017-12-10T05:01:04.218850: step 9005, loss 0.0830313, acc 0.96875, prec 0.110829, recall 0.812066
2017-12-10T05:01:04.479732: step 9006, loss 0.00671724, acc 1, prec 0.110829, recall 0.812066
2017-12-10T05:01:04.749000: step 9007, loss 0.000967974, acc 1, prec 0.110839, recall 0.812081
2017-12-10T05:01:05.011945: step 9008, loss 0.638826, acc 0.921875, prec 0.110833, recall 0.812081
2017-12-10T05:01:05.274170: step 9009, loss 0.328992, acc 0.953125, prec 0.110839, recall 0.812095
2017-12-10T05:01:05.536887: step 9010, loss 0.00107587, acc 1, prec 0.110857, recall 0.812124
2017-12-10T05:01:05.800887: step 9011, loss 0.0916253, acc 0.953125, prec 0.110854, recall 0.812124
2017-12-10T05:01:06.070332: step 9012, loss 0.168894, acc 0.96875, prec 0.110861, recall 0.812138
2017-12-10T05:01:06.338262: step 9013, loss 0.414653, acc 0.921875, prec 0.110864, recall 0.812153
2017-12-10T05:01:06.608797: step 9014, loss 0.205172, acc 0.9375, prec 0.11086, recall 0.812153
2017-12-10T05:01:06.874409: step 9015, loss 0.0044575, acc 1, prec 0.11086, recall 0.812153
2017-12-10T05:01:07.142838: step 9016, loss 1.24642, acc 0.875, prec 0.11085, recall 0.812153
2017-12-10T05:01:07.406621: step 9017, loss 0.0191714, acc 1, prec 0.110869, recall 0.812182
2017-12-10T05:01:07.670443: step 9018, loss 0.442501, acc 0.9375, prec 0.110864, recall 0.812182
2017-12-10T05:01:07.934781: step 9019, loss 0.381582, acc 0.96875, prec 0.110881, recall 0.812211
2017-12-10T05:01:08.199018: step 9020, loss 0.526856, acc 0.9375, prec 0.110876, recall 0.812211
2017-12-10T05:01:08.464995: step 9021, loss 0.251652, acc 0.9375, prec 0.110871, recall 0.812211
2017-12-10T05:01:08.727095: step 9022, loss 0.0156551, acc 0.984375, prec 0.11087, recall 0.812211
2017-12-10T05:01:08.997637: step 9023, loss 0.29155, acc 0.921875, prec 0.110874, recall 0.812225
2017-12-10T05:01:09.272794: step 9024, loss 0.0361088, acc 0.96875, prec 0.110871, recall 0.812225
2017-12-10T05:01:09.540842: step 9025, loss 0.801947, acc 0.921875, prec 0.110866, recall 0.812225
2017-12-10T05:01:09.806977: step 9026, loss 0.14658, acc 0.96875, prec 0.110863, recall 0.812225
2017-12-10T05:01:10.074455: step 9027, loss 0.918722, acc 0.921875, prec 0.110867, recall 0.81224
2017-12-10T05:01:10.340473: step 9028, loss 0.0724638, acc 0.953125, prec 0.110873, recall 0.812254
2017-12-10T05:01:10.604368: step 9029, loss 0.41504, acc 0.96875, prec 0.11087, recall 0.812254
2017-12-10T05:01:10.868597: step 9030, loss 0.179775, acc 0.96875, prec 0.110877, recall 0.812269
2017-12-10T05:01:11.133675: step 9031, loss 1.69003, acc 0.875, prec 0.110877, recall 0.812283
2017-12-10T05:01:11.395992: step 9032, loss 0.312396, acc 0.984375, prec 0.110876, recall 0.812283
2017-12-10T05:01:11.666034: step 9033, loss 0.0679183, acc 0.984375, prec 0.110884, recall 0.812298
2017-12-10T05:01:11.929265: step 9034, loss 0.0397291, acc 0.984375, prec 0.110883, recall 0.812298
2017-12-10T05:01:12.202843: step 9035, loss 0.523517, acc 0.96875, prec 0.110881, recall 0.812298
2017-12-10T05:01:12.463590: step 9036, loss 0.61694, acc 0.96875, prec 0.110879, recall 0.812298
2017-12-10T05:01:12.732178: step 9037, loss 0.827674, acc 0.96875, prec 0.110876, recall 0.812298
2017-12-10T05:01:12.994744: step 9038, loss 0.0527555, acc 0.984375, prec 0.110875, recall 0.812298
2017-12-10T05:01:13.269027: step 9039, loss 0.0356406, acc 0.984375, prec 0.110883, recall 0.812312
2017-12-10T05:01:13.534725: step 9040, loss 0.00889509, acc 1, prec 0.110893, recall 0.812327
2017-12-10T05:01:13.797607: step 9041, loss 0.321779, acc 0.984375, prec 0.110891, recall 0.812327
2017-12-10T05:01:14.068979: step 9042, loss 0.0018629, acc 1, prec 0.11091, recall 0.812356
2017-12-10T05:01:14.334855: step 9043, loss 0.0360424, acc 0.984375, prec 0.110909, recall 0.812356
2017-12-10T05:01:14.603209: step 9044, loss 0.0276103, acc 0.984375, prec 0.110917, recall 0.81237
2017-12-10T05:01:14.864701: step 9045, loss 0.199304, acc 0.96875, prec 0.110943, recall 0.812413
2017-12-10T05:01:15.130308: step 9046, loss 5.02583e-05, acc 1, prec 0.110952, recall 0.812428
2017-12-10T05:01:15.390405: step 9047, loss 0.470364, acc 0.96875, prec 0.110969, recall 0.812457
2017-12-10T05:01:15.655875: step 9048, loss 0.000877984, acc 1, prec 0.110978, recall 0.812471
2017-12-10T05:01:15.915183: step 9049, loss 0.000159649, acc 1, prec 0.110978, recall 0.812471
2017-12-10T05:01:16.179768: step 9050, loss 1.37527, acc 0.984375, prec 0.111006, recall 0.812452
2017-12-10T05:01:16.451124: step 9051, loss 0.00140904, acc 1, prec 0.111006, recall 0.812452
2017-12-10T05:01:16.714410: step 9052, loss 0.0897567, acc 0.984375, prec 0.111005, recall 0.812452
2017-12-10T05:01:16.976203: step 9053, loss 0.00412939, acc 1, prec 0.111014, recall 0.812466
2017-12-10T05:01:17.242197: step 9054, loss 0.110158, acc 0.984375, prec 0.111022, recall 0.812481
2017-12-10T05:01:17.504323: step 9055, loss 4.10265e-05, acc 1, prec 0.111022, recall 0.812481
2017-12-10T05:01:17.764399: step 9056, loss 0.219557, acc 0.96875, prec 0.111029, recall 0.812495
2017-12-10T05:01:18.032263: step 9057, loss 0.34952, acc 0.953125, prec 0.111035, recall 0.81251
2017-12-10T05:01:18.299794: step 9058, loss 0.242791, acc 0.984375, prec 0.111043, recall 0.812524
2017-12-10T05:01:18.569624: step 9059, loss 0.000370211, acc 1, prec 0.111062, recall 0.812553
2017-12-10T05:01:18.834952: step 9060, loss 0.000265622, acc 1, prec 0.111062, recall 0.812553
2017-12-10T05:01:19.095574: step 9061, loss 0.146168, acc 0.984375, prec 0.111061, recall 0.812553
2017-12-10T05:01:19.366512: step 9062, loss 0.236835, acc 0.96875, prec 0.111059, recall 0.812553
2017-12-10T05:01:19.641308: step 9063, loss 0.50485, acc 0.984375, prec 0.111076, recall 0.812582
2017-12-10T05:01:19.906063: step 9064, loss 0.00513979, acc 1, prec 0.111085, recall 0.812596
2017-12-10T05:01:20.164584: step 9065, loss 0.369529, acc 0.953125, prec 0.111082, recall 0.812596
2017-12-10T05:01:20.433658: step 9066, loss 0.256445, acc 0.953125, prec 0.111097, recall 0.812625
2017-12-10T05:01:20.699903: step 9067, loss 0.193607, acc 0.96875, prec 0.111123, recall 0.812668
2017-12-10T05:01:20.968429: step 9068, loss 0.187938, acc 0.984375, prec 0.111122, recall 0.812668
2017-12-10T05:01:21.234213: step 9069, loss 0.260171, acc 0.984375, prec 0.11113, recall 0.812683
2017-12-10T05:01:21.509734: step 9070, loss 0.114248, acc 0.984375, prec 0.111138, recall 0.812697
2017-12-10T05:01:21.773591: step 9071, loss 1.07266e-05, acc 1, prec 0.111138, recall 0.812697
2017-12-10T05:01:22.033957: step 9072, loss 0.455505, acc 0.984375, prec 0.111146, recall 0.812711
2017-12-10T05:01:22.310174: step 9073, loss 0.174563, acc 0.984375, prec 0.111145, recall 0.812711
2017-12-10T05:01:22.573725: step 9074, loss 0.0929461, acc 0.984375, prec 0.111144, recall 0.812711
2017-12-10T05:01:22.837909: step 9075, loss 0.273604, acc 0.984375, prec 0.111152, recall 0.812726
2017-12-10T05:01:23.109504: step 9076, loss 0.384919, acc 0.953125, prec 0.111167, recall 0.812755
2017-12-10T05:01:23.375293: step 9077, loss 0.0077203, acc 1, prec 0.111167, recall 0.812755
2017-12-10T05:01:23.645078: step 9078, loss 0.941667, acc 1, prec 0.111176, recall 0.812769
2017-12-10T05:01:23.909961: step 9079, loss 0.00363668, acc 1, prec 0.111195, recall 0.812798
2017-12-10T05:01:24.177543: step 9080, loss 0.197753, acc 0.953125, prec 0.111201, recall 0.812812
2017-12-10T05:01:24.442429: step 9081, loss 0.814267, acc 0.953125, prec 0.111198, recall 0.812812
2017-12-10T05:01:24.706539: step 9082, loss 0.172322, acc 0.984375, prec 0.111196, recall 0.812812
2017-12-10T05:01:24.968814: step 9083, loss 0.670211, acc 0.953125, prec 0.111202, recall 0.812826
2017-12-10T05:01:25.237011: step 9084, loss 0.0373835, acc 0.984375, prec 0.111201, recall 0.812826
2017-12-10T05:01:25.511386: step 9085, loss 1.11257, acc 0.9375, prec 0.111215, recall 0.812855
2017-12-10T05:01:25.774348: step 9086, loss 0.0140529, acc 1, prec 0.111243, recall 0.812898
2017-12-10T05:01:26.040213: step 9087, loss 0.000470822, acc 1, prec 0.111243, recall 0.812898
2017-12-10T05:01:26.302898: step 9088, loss 0.229406, acc 0.96875, prec 0.111241, recall 0.812898
2017-12-10T05:01:26.576411: step 9089, loss 0.142659, acc 0.96875, prec 0.111248, recall 0.812913
2017-12-10T05:01:26.844137: step 9090, loss 0.409482, acc 0.9375, prec 0.111243, recall 0.812913
2017-12-10T05:01:27.123029: step 9091, loss 5.51216, acc 0.9375, prec 0.111249, recall 0.812865
2017-12-10T05:01:27.390282: step 9092, loss 0.374148, acc 0.96875, prec 0.111247, recall 0.812865
2017-12-10T05:01:27.663345: step 9093, loss 1.342, acc 0.96875, prec 0.111263, recall 0.812893
2017-12-10T05:01:27.934529: step 9094, loss 0.174019, acc 0.984375, prec 0.111262, recall 0.812893
2017-12-10T05:01:28.199877: step 9095, loss 0.94573, acc 0.953125, prec 0.111267, recall 0.812908
2017-12-10T05:01:28.474169: step 9096, loss 0.157776, acc 0.953125, prec 0.111264, recall 0.812908
2017-12-10T05:01:28.743567: step 9097, loss 0.455327, acc 0.953125, prec 0.11126, recall 0.812908
2017-12-10T05:01:29.010386: step 9098, loss 0.18509, acc 0.9375, prec 0.111256, recall 0.812908
2017-12-10T05:01:29.277689: step 9099, loss 0.642793, acc 0.890625, prec 0.111276, recall 0.812951
2017-12-10T05:01:29.544461: step 9100, loss 1.07409, acc 0.921875, prec 0.11127, recall 0.812951
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-9100

2017-12-10T05:01:30.782637: step 9101, loss 0.983714, acc 0.890625, prec 0.111271, recall 0.812965
2017-12-10T05:01:31.054340: step 9102, loss 1.09906, acc 0.875, prec 0.111262, recall 0.812965
2017-12-10T05:01:31.318746: step 9103, loss 0.878391, acc 0.90625, prec 0.111283, recall 0.813008
2017-12-10T05:01:31.587646: step 9104, loss 1.46297, acc 0.859375, prec 0.111281, recall 0.813022
2017-12-10T05:01:31.851176: step 9105, loss 0.0701727, acc 0.984375, prec 0.11128, recall 0.813022
2017-12-10T05:01:32.121939: step 9106, loss 0.749184, acc 0.90625, prec 0.111273, recall 0.813022
2017-12-10T05:01:32.388377: step 9107, loss 0.6965, acc 0.875, prec 0.111264, recall 0.813022
2017-12-10T05:01:32.658979: step 9108, loss 0.445265, acc 0.90625, prec 0.111257, recall 0.813022
2017-12-10T05:01:32.919028: step 9109, loss 0.174688, acc 0.9375, prec 0.111262, recall 0.813037
2017-12-10T05:01:33.185429: step 9110, loss 1.01732, acc 0.875, prec 0.111252, recall 0.813037
2017-12-10T05:01:33.447006: step 9111, loss 0.487338, acc 0.9375, prec 0.111266, recall 0.813065
2017-12-10T05:01:33.720558: step 9112, loss 0.57917, acc 0.9375, prec 0.111262, recall 0.813065
2017-12-10T05:01:33.992255: step 9113, loss 0.358765, acc 0.9375, prec 0.111275, recall 0.813094
2017-12-10T05:01:34.261603: step 9114, loss 1.393, acc 0.921875, prec 0.11127, recall 0.813094
2017-12-10T05:01:34.522197: step 9115, loss 0.147122, acc 0.96875, prec 0.111286, recall 0.813123
2017-12-10T05:01:34.783490: step 9116, loss 0.301742, acc 0.921875, prec 0.111289, recall 0.813137
2017-12-10T05:01:35.046846: step 9117, loss 0.00216279, acc 1, prec 0.111289, recall 0.813137
2017-12-10T05:01:35.306796: step 9118, loss 0.0179915, acc 0.984375, prec 0.111316, recall 0.81318
2017-12-10T05:01:35.568133: step 9119, loss 0.422156, acc 0.984375, prec 0.111334, recall 0.813209
2017-12-10T05:01:35.836010: step 9120, loss 0.698062, acc 0.9375, prec 0.111338, recall 0.813223
2017-12-10T05:01:36.096953: step 9121, loss 0.575594, acc 0.953125, prec 0.111335, recall 0.813223
2017-12-10T05:01:36.359594: step 9122, loss 0.091939, acc 0.984375, prec 0.111334, recall 0.813223
2017-12-10T05:01:36.629189: step 9123, loss 0.0739521, acc 0.984375, prec 0.111342, recall 0.813237
2017-12-10T05:01:36.887424: step 9124, loss 1.03442, acc 0.953125, prec 0.111348, recall 0.813252
2017-12-10T05:01:37.161969: step 9125, loss 0.180972, acc 0.953125, prec 0.111353, recall 0.813266
2017-12-10T05:01:37.426786: step 9126, loss 0.248177, acc 0.96875, prec 0.111351, recall 0.813266
2017-12-10T05:01:37.693626: step 9127, loss 0.0248463, acc 0.984375, prec 0.111369, recall 0.813295
2017-12-10T05:01:37.962136: step 9128, loss 5.90036, acc 0.953125, prec 0.111376, recall 0.813247
2017-12-10T05:01:38.229551: step 9129, loss 0.118746, acc 0.984375, prec 0.111393, recall 0.813275
2017-12-10T05:01:38.496906: step 9130, loss 0.0735369, acc 0.984375, prec 0.111392, recall 0.813275
2017-12-10T05:01:38.760539: step 9131, loss 0.0369855, acc 0.984375, prec 0.1114, recall 0.813289
2017-12-10T05:01:39.021197: step 9132, loss 0.136411, acc 0.953125, prec 0.111397, recall 0.813289
2017-12-10T05:01:39.293886: step 9133, loss 0.107366, acc 0.953125, prec 0.111402, recall 0.813304
2017-12-10T05:01:39.559202: step 9134, loss 0.685168, acc 0.9375, prec 0.111407, recall 0.813318
2017-12-10T05:01:39.827881: step 9135, loss 0.722842, acc 0.953125, prec 0.111413, recall 0.813332
2017-12-10T05:01:40.088902: step 9136, loss 0.439432, acc 0.96875, prec 0.11141, recall 0.813332
2017-12-10T05:01:40.360795: step 9137, loss 0.0218575, acc 0.984375, prec 0.111428, recall 0.813361
2017-12-10T05:01:40.623468: step 9138, loss 0.734044, acc 0.953125, prec 0.111424, recall 0.813361
2017-12-10T05:01:40.895108: step 9139, loss 0.264585, acc 0.984375, prec 0.111442, recall 0.813389
2017-12-10T05:01:41.158221: step 9140, loss 0.213651, acc 0.96875, prec 0.11144, recall 0.813389
2017-12-10T05:01:41.419891: step 9141, loss 1.9447e-05, acc 1, prec 0.111449, recall 0.813404
2017-12-10T05:01:41.679009: step 9142, loss 0.000120845, acc 1, prec 0.111468, recall 0.813432
2017-12-10T05:01:41.942693: step 9143, loss 0.00296712, acc 1, prec 0.111477, recall 0.813447
2017-12-10T05:01:42.219147: step 9144, loss 0.0255477, acc 0.984375, prec 0.111476, recall 0.813447
2017-12-10T05:01:42.493893: step 9145, loss 0.164355, acc 0.984375, prec 0.111493, recall 0.813475
2017-12-10T05:01:42.756530: step 9146, loss 0.427561, acc 0.984375, prec 0.11152, recall 0.813518
2017-12-10T05:01:43.023321: step 9147, loss 0.0195553, acc 0.984375, prec 0.111519, recall 0.813518
2017-12-10T05:01:43.292912: step 9148, loss 0.112129, acc 0.984375, prec 0.111518, recall 0.813518
2017-12-10T05:01:43.558024: step 9149, loss 0.142613, acc 0.984375, prec 0.111516, recall 0.813518
2017-12-10T05:01:43.820662: step 9150, loss 0.588074, acc 0.96875, prec 0.111523, recall 0.813532
2017-12-10T05:01:44.090899: step 9151, loss 0.0156793, acc 1, prec 0.111533, recall 0.813546
2017-12-10T05:01:44.351101: step 9152, loss 0.516691, acc 0.953125, prec 0.111529, recall 0.813546
2017-12-10T05:01:44.618834: step 9153, loss 0.462387, acc 0.953125, prec 0.111526, recall 0.813546
2017-12-10T05:01:44.887121: step 9154, loss 0.00661186, acc 1, prec 0.111535, recall 0.813561
2017-12-10T05:01:45.152258: step 9155, loss 0.248808, acc 0.953125, prec 0.111531, recall 0.813561
2017-12-10T05:01:45.415284: step 9156, loss 0.487814, acc 0.953125, prec 0.111537, recall 0.813575
2017-12-10T05:01:45.684968: step 9157, loss 0.369508, acc 0.96875, prec 0.111544, recall 0.813589
2017-12-10T05:01:45.950838: step 9158, loss 0.10685, acc 0.984375, prec 0.111543, recall 0.813589
2017-12-10T05:01:46.221663: step 9159, loss 0.190378, acc 0.953125, prec 0.111549, recall 0.813603
2017-12-10T05:01:46.485998: step 9160, loss 0.121948, acc 0.984375, prec 0.111548, recall 0.813603
2017-12-10T05:01:46.760336: step 9161, loss 0.17686, acc 0.984375, prec 0.111556, recall 0.813618
2017-12-10T05:01:47.027796: step 9162, loss 0.317185, acc 0.96875, prec 0.111563, recall 0.813632
2017-12-10T05:01:47.307097: step 9163, loss 0.246889, acc 0.96875, prec 0.11157, recall 0.813646
2017-12-10T05:01:47.569347: step 9164, loss 12.8227, acc 0.953125, prec 0.111577, recall 0.813598
2017-12-10T05:01:47.841640: step 9165, loss 0.236063, acc 0.9375, prec 0.111572, recall 0.813598
2017-12-10T05:01:48.106821: step 9166, loss 0.450164, acc 0.9375, prec 0.111567, recall 0.813598
2017-12-10T05:01:48.370448: step 9167, loss 0.274326, acc 0.984375, prec 0.111566, recall 0.813598
2017-12-10T05:01:48.635481: step 9168, loss 0.293452, acc 0.953125, prec 0.111581, recall 0.813627
2017-12-10T05:01:48.901966: step 9169, loss 0.114595, acc 0.96875, prec 0.111579, recall 0.813627
2017-12-10T05:01:49.164264: step 9170, loss 0.229694, acc 0.953125, prec 0.111575, recall 0.813627
2017-12-10T05:01:49.433970: step 9171, loss 0.439096, acc 0.921875, prec 0.111598, recall 0.813669
2017-12-10T05:01:49.700096: step 9172, loss 0.117083, acc 0.96875, prec 0.111614, recall 0.813698
2017-12-10T05:01:49.963724: step 9173, loss 0.416908, acc 0.921875, prec 0.111617, recall 0.813712
2017-12-10T05:01:50.238079: step 9174, loss 0.400033, acc 0.890625, prec 0.111609, recall 0.813712
2017-12-10T05:01:50.505076: step 9175, loss 0.334755, acc 0.9375, prec 0.111632, recall 0.813755
2017-12-10T05:01:50.772168: step 9176, loss 0.999059, acc 0.890625, prec 0.111633, recall 0.813769
2017-12-10T05:01:51.034420: step 9177, loss 0.232088, acc 0.96875, prec 0.111668, recall 0.813826
2017-12-10T05:01:51.297904: step 9178, loss 0.308365, acc 0.9375, prec 0.111692, recall 0.813868
2017-12-10T05:01:51.563001: step 9179, loss 0.442897, acc 0.953125, prec 0.111697, recall 0.813883
2017-12-10T05:01:51.830149: step 9180, loss 0.166515, acc 0.96875, prec 0.111695, recall 0.813883
2017-12-10T05:01:52.094917: step 9181, loss 0.0389978, acc 0.96875, prec 0.111693, recall 0.813883
2017-12-10T05:01:52.365238: step 9182, loss 0.256941, acc 0.9375, prec 0.111697, recall 0.813897
2017-12-10T05:01:52.639315: step 9183, loss 0.701872, acc 0.9375, prec 0.111693, recall 0.813897
2017-12-10T05:01:52.904466: step 9184, loss 0.318466, acc 0.921875, prec 0.111696, recall 0.813911
2017-12-10T05:01:53.176179: step 9185, loss 0.377568, acc 0.9375, prec 0.111701, recall 0.813925
2017-12-10T05:01:53.440603: step 9186, loss 0.708197, acc 0.953125, prec 0.111706, recall 0.813939
2017-12-10T05:01:53.707772: step 9187, loss 0.521061, acc 0.9375, prec 0.111711, recall 0.813953
2017-12-10T05:01:53.980955: step 9188, loss 0.745831, acc 0.953125, prec 0.111717, recall 0.813968
2017-12-10T05:01:54.246360: step 9189, loss 0.150065, acc 0.96875, prec 0.111715, recall 0.813968
2017-12-10T05:01:54.514435: step 9190, loss 0.624614, acc 0.921875, prec 0.111727, recall 0.813996
2017-12-10T05:01:54.777539: step 9191, loss 0.329232, acc 0.953125, prec 0.111733, recall 0.81401
2017-12-10T05:01:55.047493: step 9192, loss 0.109643, acc 0.984375, prec 0.111741, recall 0.814024
2017-12-10T05:01:55.310381: step 9193, loss 0.388159, acc 0.96875, prec 0.111739, recall 0.814024
2017-12-10T05:01:55.573928: step 9194, loss 0.375945, acc 0.921875, prec 0.111733, recall 0.814024
2017-12-10T05:01:55.840026: step 9195, loss 0.213505, acc 0.96875, prec 0.111731, recall 0.814024
2017-12-10T05:01:56.105732: step 9196, loss 0.00151817, acc 1, prec 0.111731, recall 0.814024
2017-12-10T05:01:56.375199: step 9197, loss 0.021661, acc 0.984375, prec 0.111757, recall 0.814067
2017-12-10T05:01:56.638892: step 9198, loss 0.0557673, acc 0.984375, prec 0.111784, recall 0.814109
2017-12-10T05:01:56.907975: step 9199, loss 0.341363, acc 0.984375, prec 0.111792, recall 0.814124
2017-12-10T05:01:57.184115: step 9200, loss 0.0480968, acc 0.984375, prec 0.111819, recall 0.814166
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-9200

2017-12-10T05:01:58.490088: step 9201, loss 0.0970872, acc 0.984375, prec 0.111818, recall 0.814166
2017-12-10T05:01:58.756991: step 9202, loss 0.0369993, acc 0.984375, prec 0.111863, recall 0.814237
2017-12-10T05:01:59.023704: step 9203, loss 0.0392896, acc 0.984375, prec 0.111862, recall 0.814237
2017-12-10T05:01:59.289656: step 9204, loss 0.023371, acc 0.984375, prec 0.111879, recall 0.814265
2017-12-10T05:01:59.556959: step 9205, loss 0.794526, acc 1, prec 0.111898, recall 0.814293
2017-12-10T05:01:59.826928: step 9206, loss 0.231385, acc 0.96875, prec 0.111896, recall 0.814293
2017-12-10T05:02:00.091041: step 9207, loss 0.048482, acc 0.984375, prec 0.111913, recall 0.814322
2017-12-10T05:02:00.369567: step 9208, loss 0.418218, acc 0.984375, prec 0.111921, recall 0.814336
2017-12-10T05:02:00.640398: step 9209, loss 0.398475, acc 0.953125, prec 0.111918, recall 0.814336
2017-12-10T05:02:00.911949: step 9210, loss 0.00257752, acc 1, prec 0.111927, recall 0.81435
2017-12-10T05:02:01.180039: step 9211, loss 0.0408164, acc 0.984375, prec 0.111926, recall 0.81435
2017-12-10T05:02:01.448607: step 9212, loss 0.000154892, acc 1, prec 0.111935, recall 0.814364
2017-12-10T05:02:01.714435: step 9213, loss 0.0744723, acc 0.984375, prec 0.111934, recall 0.814364
2017-12-10T05:02:01.981276: step 9214, loss 0.388424, acc 0.96875, prec 0.111959, recall 0.814406
2017-12-10T05:02:02.247738: step 9215, loss 4.0278, acc 0.984375, prec 0.111969, recall 0.814359
2017-12-10T05:02:02.517679: step 9216, loss 0.159106, acc 0.96875, prec 0.111975, recall 0.814373
2017-12-10T05:02:02.780330: step 9217, loss 0.274753, acc 0.953125, prec 0.112, recall 0.814415
2017-12-10T05:02:03.045299: step 9218, loss 0.297583, acc 0.96875, prec 0.112007, recall 0.814429
2017-12-10T05:02:03.307094: step 9219, loss 0.181013, acc 0.953125, prec 0.112013, recall 0.814443
2017-12-10T05:02:03.572347: step 9220, loss 0.110378, acc 0.984375, prec 0.11203, recall 0.814471
2017-12-10T05:02:03.849033: step 9221, loss 0.484177, acc 0.921875, prec 0.112024, recall 0.814471
2017-12-10T05:02:04.114240: step 9222, loss 0.231109, acc 0.921875, prec 0.112037, recall 0.8145
2017-12-10T05:02:04.379097: step 9223, loss 0.658339, acc 0.90625, prec 0.11203, recall 0.8145
2017-12-10T05:02:04.649686: step 9224, loss 0.378772, acc 0.96875, prec 0.112027, recall 0.8145
2017-12-10T05:02:04.914593: step 9225, loss 0.218358, acc 0.953125, prec 0.112024, recall 0.8145
2017-12-10T05:02:05.180767: step 9226, loss 0.282438, acc 0.953125, prec 0.11202, recall 0.8145
2017-12-10T05:02:05.452792: step 9227, loss 0.279398, acc 0.90625, prec 0.112023, recall 0.814514
2017-12-10T05:02:05.726909: step 9228, loss 0.931136, acc 0.890625, prec 0.112014, recall 0.814514
2017-12-10T05:02:05.998805: step 9229, loss 0.789689, acc 0.890625, prec 0.112006, recall 0.814514
2017-12-10T05:02:06.266756: step 9230, loss 0.447044, acc 0.953125, prec 0.112012, recall 0.814528
2017-12-10T05:02:06.533304: step 9231, loss 0.597488, acc 0.953125, prec 0.112009, recall 0.814528
2017-12-10T05:02:06.803011: step 9232, loss 0.165844, acc 0.9375, prec 0.112013, recall 0.814542
2017-12-10T05:02:07.078494: step 9233, loss 0.541792, acc 0.890625, prec 0.112023, recall 0.81457
2017-12-10T05:02:07.347991: step 9234, loss 0.428045, acc 0.9375, prec 0.112019, recall 0.81457
2017-12-10T05:02:07.615084: step 9235, loss 0.348459, acc 0.9375, prec 0.112033, recall 0.814598
2017-12-10T05:02:07.883108: step 9236, loss 0.736032, acc 0.96875, prec 0.11203, recall 0.814598
2017-12-10T05:02:08.148447: step 9237, loss 0.569227, acc 0.96875, prec 0.112037, recall 0.814612
2017-12-10T05:02:08.414426: step 9238, loss 0.224764, acc 0.953125, prec 0.112052, recall 0.81464
2017-12-10T05:02:08.682308: step 9239, loss 0.285539, acc 0.9375, prec 0.112057, recall 0.814655
2017-12-10T05:02:08.951485: step 9240, loss 0.0656631, acc 0.96875, prec 0.112055, recall 0.814655
2017-12-10T05:02:09.223149: step 9241, loss 0.481117, acc 0.9375, prec 0.11205, recall 0.814655
2017-12-10T05:02:09.495289: step 9242, loss 0.0357269, acc 0.984375, prec 0.112058, recall 0.814669
2017-12-10T05:02:09.759775: step 9243, loss 0.349187, acc 0.9375, prec 0.112063, recall 0.814683
2017-12-10T05:02:10.034669: step 9244, loss 0.0693372, acc 0.984375, prec 0.112071, recall 0.814697
2017-12-10T05:02:10.304899: step 9245, loss 0.0151384, acc 0.984375, prec 0.112079, recall 0.814711
2017-12-10T05:02:10.576276: step 9246, loss 0.156725, acc 0.96875, prec 0.112095, recall 0.814739
2017-12-10T05:02:10.846329: step 9247, loss 0.0169677, acc 0.984375, prec 0.112094, recall 0.814739
2017-12-10T05:02:11.120405: step 9248, loss 0.315939, acc 0.96875, prec 0.112091, recall 0.814739
2017-12-10T05:02:11.388267: step 9249, loss 0.642313, acc 0.984375, prec 0.11209, recall 0.814739
2017-12-10T05:02:11.658788: step 9250, loss 0.0511721, acc 0.984375, prec 0.112098, recall 0.814753
2017-12-10T05:02:11.927758: step 9251, loss 0.49048, acc 0.96875, prec 0.112133, recall 0.814809
2017-12-10T05:02:12.208457: step 9252, loss 0.0166376, acc 0.984375, prec 0.112141, recall 0.814823
2017-12-10T05:02:12.474084: step 9253, loss 0.11174, acc 0.984375, prec 0.11214, recall 0.814823
2017-12-10T05:02:12.739454: step 9254, loss 0.038773, acc 0.984375, prec 0.112148, recall 0.814837
2017-12-10T05:02:13.001837: step 9255, loss 0.373937, acc 0.96875, prec 0.112155, recall 0.814851
2017-12-10T05:02:13.266130: step 9256, loss 0.0355524, acc 0.984375, prec 0.112163, recall 0.814865
2017-12-10T05:02:13.538994: step 9257, loss 0.284061, acc 0.984375, prec 0.112171, recall 0.814879
2017-12-10T05:02:13.808459: step 9258, loss 0.0974741, acc 0.984375, prec 0.11217, recall 0.814879
2017-12-10T05:02:14.074117: step 9259, loss 0.129804, acc 1, prec 0.112189, recall 0.814907
2017-12-10T05:02:14.339442: step 9260, loss 0.190169, acc 0.96875, prec 0.112186, recall 0.814907
2017-12-10T05:02:14.604418: step 9261, loss 0.277816, acc 0.96875, prec 0.112184, recall 0.814907
2017-12-10T05:02:14.868083: step 9262, loss 0.594161, acc 0.953125, prec 0.11218, recall 0.814907
2017-12-10T05:02:15.138619: step 9263, loss 0.150043, acc 0.984375, prec 0.112179, recall 0.814907
2017-12-10T05:02:15.404812: step 9264, loss 0.00160443, acc 1, prec 0.112189, recall 0.814921
2017-12-10T05:02:15.664234: step 9265, loss 0.879975, acc 0.96875, prec 0.112186, recall 0.814921
2017-12-10T05:02:15.929211: step 9266, loss 0.125152, acc 0.984375, prec 0.112204, recall 0.81495
2017-12-10T05:02:16.193169: step 9267, loss 0.0679996, acc 0.96875, prec 0.112201, recall 0.81495
2017-12-10T05:02:16.460976: step 9268, loss 0.11867, acc 0.984375, prec 0.1122, recall 0.81495
2017-12-10T05:02:16.727971: step 9269, loss 0.0114007, acc 1, prec 0.112209, recall 0.814964
2017-12-10T05:02:16.989098: step 9270, loss 0.18699, acc 0.984375, prec 0.112227, recall 0.814992
2017-12-10T05:02:17.253120: step 9271, loss 12.0675, acc 0.96875, prec 0.112226, recall 0.81493
2017-12-10T05:02:17.529841: step 9272, loss 4.77334e-05, acc 1, prec 0.112235, recall 0.814944
2017-12-10T05:02:17.790592: step 9273, loss 0.207079, acc 0.96875, prec 0.112232, recall 0.814944
2017-12-10T05:02:18.053819: step 9274, loss 0.277342, acc 0.96875, prec 0.112239, recall 0.814958
2017-12-10T05:02:18.319168: step 9275, loss 0.489338, acc 0.953125, prec 0.112236, recall 0.814958
2017-12-10T05:02:18.586740: step 9276, loss 0.48731, acc 0.953125, prec 0.112242, recall 0.814972
2017-12-10T05:02:18.856739: step 9277, loss 0.256936, acc 0.984375, prec 0.11224, recall 0.814972
2017-12-10T05:02:19.128546: step 9278, loss 0.022392, acc 0.984375, prec 0.112258, recall 0.815
2017-12-10T05:02:19.397346: step 9279, loss 0.0067567, acc 1, prec 0.112258, recall 0.815
2017-12-10T05:02:19.660923: step 9280, loss 1.00476, acc 0.96875, prec 0.112274, recall 0.815028
2017-12-10T05:02:20.628315: step 9281, loss 0.0371774, acc 0.984375, prec 0.112291, recall 0.815056
2017-12-10T05:02:20.996210: step 9282, loss 0.359288, acc 0.96875, prec 0.112289, recall 0.815056
2017-12-10T05:02:21.583693: step 9283, loss 0.295115, acc 0.96875, prec 0.112287, recall 0.815056
2017-12-10T05:02:22.315111: step 9284, loss 0.0883084, acc 0.984375, prec 0.112295, recall 0.81507
2017-12-10T05:02:23.053467: step 9285, loss 0.00102005, acc 1, prec 0.112295, recall 0.81507
2017-12-10T05:02:24.196008: step 9286, loss 0.708495, acc 0.9375, prec 0.112299, recall 0.815084
2017-12-10T05:02:24.602489: step 9287, loss 0.212076, acc 0.96875, prec 0.112325, recall 0.815126
2017-12-10T05:02:24.880528: step 9288, loss 0.582043, acc 0.953125, prec 0.112349, recall 0.815168
2017-12-10T05:02:25.162357: step 9289, loss 0.168834, acc 0.984375, prec 0.112366, recall 0.815196
2017-12-10T05:02:25.427689: step 9290, loss 0.749422, acc 0.96875, prec 0.112383, recall 0.815224
2017-12-10T05:02:25.708481: step 9291, loss 0.473604, acc 0.921875, prec 0.112377, recall 0.815224
2017-12-10T05:02:25.973967: step 9292, loss 0.0984838, acc 0.953125, prec 0.112373, recall 0.815224
2017-12-10T05:02:26.245618: step 9293, loss 0.291789, acc 0.984375, prec 0.112381, recall 0.815238
2017-12-10T05:02:26.515215: step 9294, loss 0.349844, acc 0.921875, prec 0.112385, recall 0.815252
2017-12-10T05:02:26.782698: step 9295, loss 0.157296, acc 0.96875, prec 0.112392, recall 0.815266
2017-12-10T05:02:27.053435: step 9296, loss 0.134896, acc 0.96875, prec 0.112398, recall 0.81528
2017-12-10T05:02:27.325755: step 9297, loss 0.188151, acc 0.984375, prec 0.112407, recall 0.815294
2017-12-10T05:02:27.599763: step 9298, loss 0.00572293, acc 1, prec 0.112407, recall 0.815294
2017-12-10T05:02:27.866984: step 9299, loss 0.170166, acc 0.9375, prec 0.112411, recall 0.815308
2017-12-10T05:02:28.131518: step 9300, loss 0.118102, acc 0.984375, prec 0.112419, recall 0.815322

Evaluation:
2017-12-10T05:02:35.714651: step 9300, loss 15.367, acc 0.959804, prec 0.112457, recall 0.809663

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-9300

2017-12-10T05:02:36.897950: step 9301, loss 0.881107, acc 0.9375, prec 0.112452, recall 0.809663
2017-12-10T05:02:37.163055: step 9302, loss 0.153909, acc 0.953125, prec 0.112458, recall 0.809677
2017-12-10T05:02:37.433480: step 9303, loss 0.440675, acc 0.96875, prec 0.112456, recall 0.809677
2017-12-10T05:02:37.697125: step 9304, loss 0.00351911, acc 1, prec 0.112465, recall 0.809691
2017-12-10T05:02:37.961922: step 9305, loss 0.0176448, acc 1, prec 0.112465, recall 0.809691
2017-12-10T05:02:38.232994: step 9306, loss 0.0153353, acc 1, prec 0.112474, recall 0.809705
2017-12-10T05:02:38.493697: step 9307, loss 0.150926, acc 0.96875, prec 0.112472, recall 0.809705
2017-12-10T05:02:38.766508: step 9308, loss 7.73118e-06, acc 1, prec 0.112472, recall 0.809705
2017-12-10T05:02:39.022475: step 9309, loss 0.00530139, acc 1, prec 0.112481, recall 0.80972
2017-12-10T05:02:39.282841: step 9310, loss 0.000110486, acc 1, prec 0.11249, recall 0.809734
2017-12-10T05:02:39.549399: step 9311, loss 0.255912, acc 0.96875, prec 0.112488, recall 0.809734
2017-12-10T05:02:39.808888: step 9312, loss 0.0801933, acc 0.96875, prec 0.112485, recall 0.809734
2017-12-10T05:02:40.080175: step 9313, loss 0.142992, acc 0.984375, prec 0.112494, recall 0.809748
2017-12-10T05:02:40.346845: step 9314, loss 0.206572, acc 0.96875, prec 0.112491, recall 0.809748
2017-12-10T05:02:40.612180: step 9315, loss 0.426943, acc 0.96875, prec 0.112489, recall 0.809748
2017-12-10T05:02:40.875680: step 9316, loss 0.000835068, acc 1, prec 0.112498, recall 0.809762
2017-12-10T05:02:41.150700: step 9317, loss 0.024492, acc 0.984375, prec 0.112497, recall 0.809762
2017-12-10T05:02:41.422379: step 9318, loss 0.416352, acc 0.9375, prec 0.112492, recall 0.809762
2017-12-10T05:02:41.687970: step 9319, loss 0.0818028, acc 0.96875, prec 0.11249, recall 0.809762
2017-12-10T05:02:41.961726: step 9320, loss 0.000238095, acc 1, prec 0.11249, recall 0.809762
2017-12-10T05:02:42.235415: step 9321, loss 0.0130588, acc 0.984375, prec 0.112489, recall 0.809762
2017-12-10T05:02:42.498882: step 9322, loss 0.0908857, acc 0.96875, prec 0.112486, recall 0.809762
2017-12-10T05:02:42.766510: step 9323, loss 0.132669, acc 0.984375, prec 0.112494, recall 0.809777
2017-12-10T05:02:43.032747: step 9324, loss 0.000239003, acc 1, prec 0.112513, recall 0.809805
2017-12-10T05:02:43.290855: step 9325, loss 1.20093, acc 0.984375, prec 0.112513, recall 0.809744
2017-12-10T05:02:43.568240: step 9326, loss 0.00297932, acc 1, prec 0.112522, recall 0.809759
2017-12-10T05:02:43.831844: step 9327, loss 0.0652898, acc 0.984375, prec 0.112521, recall 0.809759
2017-12-10T05:02:44.099982: step 9328, loss 0.00458045, acc 1, prec 0.11253, recall 0.809773
2017-12-10T05:02:44.370559: step 9329, loss 5.44977e-05, acc 1, prec 0.11253, recall 0.809773
2017-12-10T05:02:44.633581: step 9330, loss 0.132332, acc 0.984375, prec 0.112538, recall 0.809787
2017-12-10T05:02:44.904234: step 9331, loss 0.00854354, acc 1, prec 0.112566, recall 0.80983
2017-12-10T05:02:45.165907: step 9332, loss 0.000344208, acc 1, prec 0.112584, recall 0.809858
2017-12-10T05:02:45.436355: step 9333, loss 0.0140643, acc 0.984375, prec 0.112592, recall 0.809872
2017-12-10T05:02:45.706141: step 9334, loss 0.13731, acc 0.96875, prec 0.11259, recall 0.809872
2017-12-10T05:02:45.970153: step 9335, loss 0.1228, acc 0.984375, prec 0.112589, recall 0.809872
2017-12-10T05:02:46.230724: step 9336, loss 0.000218824, acc 1, prec 0.112589, recall 0.809872
2017-12-10T05:02:46.492318: step 9337, loss 0.374585, acc 0.984375, prec 0.112597, recall 0.809887
2017-12-10T05:02:46.761293: step 9338, loss 0.0709027, acc 0.984375, prec 0.112605, recall 0.809901
2017-12-10T05:02:47.032056: step 9339, loss 0.107334, acc 0.96875, prec 0.11263, recall 0.809943
2017-12-10T05:02:47.296128: step 9340, loss 0.0115049, acc 1, prec 0.11263, recall 0.809943
2017-12-10T05:02:47.565881: step 9341, loss 0.0931203, acc 0.984375, prec 0.112647, recall 0.809972
2017-12-10T05:02:47.833530: step 9342, loss 0.112099, acc 0.984375, prec 0.112646, recall 0.809972
2017-12-10T05:02:48.107170: step 9343, loss 0.000610002, acc 1, prec 0.112665, recall 0.81
2017-12-10T05:02:48.368693: step 9344, loss 0.0774862, acc 0.984375, prec 0.112682, recall 0.810028
2017-12-10T05:02:48.629971: step 9345, loss 0.0475108, acc 0.984375, prec 0.112681, recall 0.810028
2017-12-10T05:02:48.900595: step 9346, loss 0.0726646, acc 0.984375, prec 0.112689, recall 0.810043
2017-12-10T05:02:49.162610: step 9347, loss 0.0380632, acc 0.984375, prec 0.112688, recall 0.810043
2017-12-10T05:02:49.425427: step 9348, loss 0.00511844, acc 1, prec 0.112697, recall 0.810057
2017-12-10T05:02:49.694876: step 9349, loss 0.00771634, acc 1, prec 0.112706, recall 0.810071
2017-12-10T05:02:49.956496: step 9350, loss 0.072073, acc 0.984375, prec 0.112714, recall 0.810085
2017-12-10T05:02:50.223439: step 9351, loss 0.000313101, acc 1, prec 0.112714, recall 0.810085
2017-12-10T05:02:50.482532: step 9352, loss 0.0993939, acc 0.96875, prec 0.112712, recall 0.810085
2017-12-10T05:02:50.743772: step 9353, loss 0.040763, acc 0.984375, prec 0.11272, recall 0.810099
2017-12-10T05:02:51.006251: step 9354, loss 0.307481, acc 0.96875, prec 0.112727, recall 0.810113
2017-12-10T05:02:51.270920: step 9355, loss 0.0105241, acc 1, prec 0.112754, recall 0.810156
2017-12-10T05:02:51.536392: step 9356, loss 0.145056, acc 0.984375, prec 0.112762, recall 0.81017
2017-12-10T05:02:51.803003: step 9357, loss 2.65501e-05, acc 1, prec 0.112762, recall 0.81017
2017-12-10T05:02:52.061528: step 9358, loss 0.00161132, acc 1, prec 0.112781, recall 0.810198
2017-12-10T05:02:52.325156: step 9359, loss 0.00017236, acc 1, prec 0.112781, recall 0.810198
2017-12-10T05:02:52.582021: step 9360, loss 0.269645, acc 0.984375, prec 0.112789, recall 0.810212
2017-12-10T05:02:52.848058: step 9361, loss 0.0266666, acc 0.984375, prec 0.112806, recall 0.810241
2017-12-10T05:02:53.112877: step 9362, loss 0.000557557, acc 1, prec 0.112815, recall 0.810255
2017-12-10T05:02:53.377000: step 9363, loss 0.0429628, acc 0.984375, prec 0.112832, recall 0.810283
2017-12-10T05:02:53.641568: step 9364, loss 0.000574457, acc 1, prec 0.112842, recall 0.810297
2017-12-10T05:02:53.911212: step 9365, loss 0.0801578, acc 0.984375, prec 0.11285, recall 0.810311
2017-12-10T05:02:54.173475: step 9366, loss 2.92755, acc 0.96875, prec 0.112848, recall 0.810251
2017-12-10T05:02:54.448046: step 9367, loss 1.81243, acc 0.953125, prec 0.112882, recall 0.810308
2017-12-10T05:02:54.716182: step 9368, loss 0.293249, acc 0.953125, prec 0.112878, recall 0.810308
2017-12-10T05:02:54.979845: step 9369, loss 0.0491616, acc 0.984375, prec 0.112877, recall 0.810308
2017-12-10T05:02:55.247855: step 9370, loss 4.2823, acc 0.921875, prec 0.112872, recall 0.810247
2017-12-10T05:02:55.515116: step 9371, loss 0.541311, acc 0.875, prec 0.1129, recall 0.810304
2017-12-10T05:02:55.778316: step 9372, loss 0.483314, acc 0.9375, prec 0.112914, recall 0.810332
2017-12-10T05:02:56.040623: step 9373, loss 1.29108, acc 0.859375, prec 0.112912, recall 0.810346
2017-12-10T05:02:56.307511: step 9374, loss 1.15436, acc 0.859375, prec 0.112902, recall 0.810346
2017-12-10T05:02:56.577656: step 9375, loss 0.813991, acc 0.828125, prec 0.112889, recall 0.810346
2017-12-10T05:02:56.844253: step 9376, loss 1.53109, acc 0.84375, prec 0.112877, recall 0.810346
2017-12-10T05:02:57.123657: step 9377, loss 2.19624, acc 0.796875, prec 0.112862, recall 0.810346
2017-12-10T05:02:57.392821: step 9378, loss 1.02763, acc 0.859375, prec 0.112851, recall 0.810346
2017-12-10T05:02:57.654593: step 9379, loss 1.31022, acc 0.84375, prec 0.112849, recall 0.81036
2017-12-10T05:02:57.917962: step 9380, loss 1.85525, acc 0.765625, prec 0.11285, recall 0.810388
2017-12-10T05:02:58.183438: step 9381, loss 3.71098, acc 0.65625, prec 0.112824, recall 0.810388
2017-12-10T05:02:58.458689: step 9382, loss 2.00518, acc 0.828125, prec 0.11282, recall 0.810403
2017-12-10T05:02:58.728934: step 9383, loss 1.06942, acc 0.875, prec 0.11282, recall 0.810417
2017-12-10T05:02:58.992137: step 9384, loss 1.5998, acc 0.796875, prec 0.112814, recall 0.810431
2017-12-10T05:02:59.264770: step 9385, loss 0.909922, acc 0.890625, prec 0.112843, recall 0.810487
2017-12-10T05:02:59.530654: step 9386, loss 2.5353, acc 0.859375, prec 0.112841, recall 0.810501
2017-12-10T05:02:59.805030: step 9387, loss 0.515263, acc 0.953125, prec 0.112847, recall 0.810515
2017-12-10T05:03:00.074437: step 9388, loss 1.44986, acc 0.8125, prec 0.112851, recall 0.810544
2017-12-10T05:03:00.349650: step 9389, loss 0.450689, acc 0.921875, prec 0.112864, recall 0.810572
2017-12-10T05:03:00.619860: step 9390, loss 0.910908, acc 0.921875, prec 0.112877, recall 0.8106
2017-12-10T05:03:00.884672: step 9391, loss 1.08137, acc 0.9375, prec 0.112872, recall 0.8106
2017-12-10T05:03:01.153448: step 9392, loss 0.548148, acc 0.9375, prec 0.112867, recall 0.8106
2017-12-10T05:03:01.424370: step 9393, loss 0.484226, acc 0.953125, prec 0.112873, recall 0.810614
2017-12-10T05:03:01.687250: step 9394, loss 0.303797, acc 0.96875, prec 0.112871, recall 0.810614
2017-12-10T05:03:01.952377: step 9395, loss 0.382796, acc 0.921875, prec 0.112883, recall 0.810642
2017-12-10T05:03:02.222125: step 9396, loss 0.065575, acc 0.984375, prec 0.112882, recall 0.810642
2017-12-10T05:03:02.490191: step 9397, loss 0.00743009, acc 1, prec 0.112891, recall 0.810656
2017-12-10T05:03:02.755010: step 9398, loss 0.277753, acc 0.9375, prec 0.112896, recall 0.81067
2017-12-10T05:03:03.023114: step 9399, loss 0.161982, acc 0.953125, prec 0.11292, recall 0.810712
2017-12-10T05:03:03.282692: step 9400, loss 0.111223, acc 0.984375, prec 0.112918, recall 0.810712
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-9400

2017-12-10T05:03:04.568046: step 9401, loss 0.337172, acc 0.96875, prec 0.112925, recall 0.810726
2017-12-10T05:03:04.832262: step 9402, loss 2.12111e-05, acc 1, prec 0.112971, recall 0.810797
2017-12-10T05:03:05.092190: step 9403, loss 0.0345009, acc 0.96875, prec 0.112987, recall 0.810825
2017-12-10T05:03:05.364955: step 9404, loss 0.663588, acc 0.96875, prec 0.112994, recall 0.810839
2017-12-10T05:03:05.631394: step 9405, loss 0.136783, acc 0.984375, prec 0.113002, recall 0.810853
2017-12-10T05:03:05.901567: step 9406, loss 0.165435, acc 0.984375, prec 0.11301, recall 0.810867
2017-12-10T05:03:06.162849: step 9407, loss 6.16452, acc 0.984375, prec 0.11301, recall 0.810807
2017-12-10T05:03:06.429731: step 9408, loss 0.0428385, acc 0.984375, prec 0.113018, recall 0.810821
2017-12-10T05:03:06.697620: step 9409, loss 0.665908, acc 0.953125, prec 0.113024, recall 0.810835
2017-12-10T05:03:06.967542: step 9410, loss 0.0281086, acc 0.984375, prec 0.113059, recall 0.810891
2017-12-10T05:03:07.231521: step 9411, loss 0.00388638, acc 1, prec 0.113078, recall 0.810919
2017-12-10T05:03:07.495309: step 9412, loss 0.567692, acc 0.96875, prec 0.113084, recall 0.810933
2017-12-10T05:03:07.755379: step 9413, loss 0.0792395, acc 0.984375, prec 0.113102, recall 0.810961
2017-12-10T05:03:08.021634: step 9414, loss 0.549136, acc 0.953125, prec 0.113098, recall 0.810961
2017-12-10T05:03:08.292318: step 9415, loss 0.488468, acc 0.9375, prec 0.113093, recall 0.810961
2017-12-10T05:03:08.559633: step 9416, loss 0.0820118, acc 0.984375, prec 0.113101, recall 0.810975
2017-12-10T05:03:08.821420: step 9417, loss 0.509911, acc 0.953125, prec 0.113116, recall 0.811003
2017-12-10T05:03:09.094983: step 9418, loss 0.181034, acc 0.984375, prec 0.113133, recall 0.811031
2017-12-10T05:03:09.360506: step 9419, loss 0.355391, acc 0.96875, prec 0.113149, recall 0.811059
2017-12-10T05:03:09.629851: step 9420, loss 0.0289224, acc 0.984375, prec 0.113148, recall 0.811059
2017-12-10T05:03:09.893779: step 9421, loss 0.3007, acc 0.984375, prec 0.113156, recall 0.811073
2017-12-10T05:03:10.161612: step 9422, loss 0.203194, acc 0.984375, prec 0.113164, recall 0.811087
2017-12-10T05:03:10.435377: step 9423, loss 0.0374732, acc 0.984375, prec 0.113181, recall 0.811115
2017-12-10T05:03:10.700505: step 9424, loss 0.220447, acc 0.96875, prec 0.113188, recall 0.811129
2017-12-10T05:03:10.960142: step 9425, loss 0.241868, acc 0.9375, prec 0.113202, recall 0.811157
2017-12-10T05:03:11.224370: step 9426, loss 0.141145, acc 0.96875, prec 0.113209, recall 0.811171
2017-12-10T05:03:11.490733: step 9427, loss 0.420754, acc 0.953125, prec 0.113214, recall 0.811185
2017-12-10T05:03:11.750205: step 9428, loss 0.503568, acc 0.984375, prec 0.113232, recall 0.811213
2017-12-10T05:03:12.023221: step 9429, loss 0.0947297, acc 0.96875, prec 0.113229, recall 0.811213
2017-12-10T05:03:12.292042: step 9430, loss 0.0999857, acc 0.984375, prec 0.113246, recall 0.811241
2017-12-10T05:03:12.563547: step 9431, loss 0.00218564, acc 1, prec 0.113256, recall 0.811255
2017-12-10T05:03:12.833101: step 9432, loss 0.26459, acc 0.96875, prec 0.113262, recall 0.811269
2017-12-10T05:03:13.808446: step 9433, loss 0.0370556, acc 0.984375, prec 0.11327, recall 0.811283
2017-12-10T05:03:14.180220: step 9434, loss 0.273784, acc 0.96875, prec 0.113286, recall 0.811311
2017-12-10T05:03:14.449487: step 9435, loss 0.0244439, acc 0.984375, prec 0.113294, recall 0.811325
2017-12-10T05:03:15.156548: step 9436, loss 0.00707057, acc 1, prec 0.113303, recall 0.811339
2017-12-10T05:03:15.874649: step 9437, loss 0.843251, acc 0.953125, prec 0.113309, recall 0.811353
2017-12-10T05:03:16.591198: step 9438, loss 0.340483, acc 0.953125, prec 0.113306, recall 0.811353
2017-12-10T05:03:17.316555: step 9439, loss 0.164127, acc 0.96875, prec 0.113322, recall 0.811381
2017-12-10T05:03:18.040832: step 9440, loss 0.978255, acc 0.921875, prec 0.113334, recall 0.811409
2017-12-10T05:03:18.758105: step 9441, loss 0.511786, acc 0.984375, prec 0.113342, recall 0.811423
2017-12-10T05:03:19.502070: step 9442, loss 0.0505812, acc 0.984375, prec 0.113341, recall 0.811423
2017-12-10T05:03:20.572580: step 9443, loss 0.000237044, acc 1, prec 0.11335, recall 0.811437
2017-12-10T05:03:21.026834: step 9444, loss 0.50428, acc 0.953125, prec 0.113347, recall 0.811437
2017-12-10T05:03:21.312685: step 9445, loss 0.125918, acc 0.96875, prec 0.113363, recall 0.811464
2017-12-10T05:03:21.590354: step 9446, loss 0.112066, acc 0.96875, prec 0.113379, recall 0.811492
2017-12-10T05:03:21.857422: step 9447, loss 0.475865, acc 0.96875, prec 0.113385, recall 0.811506
2017-12-10T05:03:22.128205: step 9448, loss 0.0906881, acc 0.984375, prec 0.113393, recall 0.81152
2017-12-10T05:03:22.397282: step 9449, loss 0.000191869, acc 1, prec 0.113393, recall 0.81152
2017-12-10T05:03:22.658146: step 9450, loss 0.00180848, acc 1, prec 0.113412, recall 0.811548
2017-12-10T05:03:22.921671: step 9451, loss 0.322184, acc 0.984375, prec 0.11342, recall 0.811562
2017-12-10T05:03:23.189528: step 9452, loss 0.326055, acc 0.984375, prec 0.113428, recall 0.811576
2017-12-10T05:03:23.470313: step 9453, loss 0.00551881, acc 1, prec 0.113455, recall 0.811618
2017-12-10T05:03:23.732509: step 9454, loss 0.000723927, acc 1, prec 0.113464, recall 0.811632
2017-12-10T05:03:23.997687: step 9455, loss 6.18391e-07, acc 1, prec 0.113483, recall 0.81166
2017-12-10T05:03:24.259419: step 9456, loss 0.182495, acc 0.984375, prec 0.113491, recall 0.811673
2017-12-10T05:03:24.521929: step 9457, loss 0.658633, acc 0.96875, prec 0.113507, recall 0.811701
2017-12-10T05:03:24.787565: step 9458, loss 0.000272848, acc 1, prec 0.113516, recall 0.811715
2017-12-10T05:03:25.048356: step 9459, loss 0.238766, acc 0.96875, prec 0.113513, recall 0.811715
2017-12-10T05:03:25.310705: step 9460, loss 1.45097e-06, acc 1, prec 0.113522, recall 0.811729
2017-12-10T05:03:25.581573: step 9461, loss 0.000559697, acc 1, prec 0.113522, recall 0.811729
2017-12-10T05:03:25.845510: step 9462, loss 1.6714, acc 0.953125, prec 0.113529, recall 0.811683
2017-12-10T05:03:26.115878: step 9463, loss 0.545648, acc 0.984375, prec 0.113556, recall 0.811725
2017-12-10T05:03:26.380901: step 9464, loss 0.200017, acc 0.984375, prec 0.113554, recall 0.811725
2017-12-10T05:03:26.640124: step 9465, loss 0.475544, acc 0.96875, prec 0.113552, recall 0.811725
2017-12-10T05:03:26.901820: step 9466, loss 0.101284, acc 0.984375, prec 0.11356, recall 0.811739
2017-12-10T05:03:27.174335: step 9467, loss 0.160216, acc 0.984375, prec 0.113568, recall 0.811753
2017-12-10T05:03:27.446257: step 9468, loss 0.802924, acc 0.96875, prec 0.113575, recall 0.811766
2017-12-10T05:03:27.711500: step 9469, loss 0.575421, acc 0.96875, prec 0.113582, recall 0.81178
2017-12-10T05:03:27.982555: step 9470, loss 0.00760054, acc 1, prec 0.1136, recall 0.811808
2017-12-10T05:03:28.251587: step 9471, loss 0.206267, acc 0.984375, prec 0.113599, recall 0.811808
2017-12-10T05:03:28.515868: step 9472, loss 0.079623, acc 0.984375, prec 0.113616, recall 0.811836
2017-12-10T05:03:28.785492: step 9473, loss 0.209285, acc 0.953125, prec 0.113631, recall 0.811864
2017-12-10T05:03:29.051851: step 9474, loss 0.0310585, acc 0.984375, prec 0.113648, recall 0.811891
2017-12-10T05:03:29.317890: step 9475, loss 0.361338, acc 0.96875, prec 0.113664, recall 0.811919
2017-12-10T05:03:29.581839: step 9476, loss 0.183666, acc 0.953125, prec 0.11366, recall 0.811919
2017-12-10T05:03:29.842898: step 9477, loss 0.0812749, acc 0.953125, prec 0.113675, recall 0.811947
2017-12-10T05:03:30.107731: step 9478, loss 0.20927, acc 0.953125, prec 0.113681, recall 0.811961
2017-12-10T05:03:30.389550: step 9479, loss 0.305794, acc 0.96875, prec 0.113678, recall 0.811961
2017-12-10T05:03:30.654281: step 9480, loss 0.27279, acc 0.953125, prec 0.113702, recall 0.812002
2017-12-10T05:03:30.914989: step 9481, loss 0.266354, acc 0.953125, prec 0.113699, recall 0.812002
2017-12-10T05:03:31.179016: step 9482, loss 0.772411, acc 0.9375, prec 0.113694, recall 0.812002
2017-12-10T05:03:31.437196: step 9483, loss 0.152452, acc 0.96875, prec 0.113719, recall 0.812044
2017-12-10T05:03:31.699629: step 9484, loss 0.324932, acc 0.96875, prec 0.113735, recall 0.812072
2017-12-10T05:03:31.965024: step 9485, loss 0.527678, acc 0.96875, prec 0.113733, recall 0.812072
2017-12-10T05:03:32.240980: step 9486, loss 0.231442, acc 0.96875, prec 0.11373, recall 0.812072
2017-12-10T05:03:32.504692: step 9487, loss 0.740911, acc 0.96875, prec 0.113756, recall 0.812113
2017-12-10T05:03:32.771628: step 9488, loss 0.0021554, acc 1, prec 0.113765, recall 0.812127
2017-12-10T05:03:33.035380: step 9489, loss 0.000923742, acc 1, prec 0.113783, recall 0.812155
2017-12-10T05:03:33.304418: step 9490, loss 0.0617358, acc 0.984375, prec 0.113791, recall 0.812169
2017-12-10T05:03:33.573097: step 9491, loss 0.0148949, acc 0.984375, prec 0.11379, recall 0.812169
2017-12-10T05:03:33.839501: step 9492, loss 0.00112529, acc 1, prec 0.11379, recall 0.812169
2017-12-10T05:03:34.104751: step 9493, loss 0.297055, acc 0.96875, prec 0.113797, recall 0.812182
2017-12-10T05:03:34.372696: step 9494, loss 0.0225879, acc 0.984375, prec 0.113795, recall 0.812182
2017-12-10T05:03:34.634371: step 9495, loss 0.118225, acc 0.96875, prec 0.113802, recall 0.812196
2017-12-10T05:03:34.899147: step 9496, loss 0.0030338, acc 1, prec 0.113811, recall 0.81221
2017-12-10T05:03:35.160453: step 9497, loss 0.00342575, acc 1, prec 0.113811, recall 0.81221
2017-12-10T05:03:35.421042: step 9498, loss 0.267662, acc 0.96875, prec 0.113846, recall 0.812265
2017-12-10T05:03:35.684395: step 9499, loss 0.00484059, acc 1, prec 0.113855, recall 0.812279
2017-12-10T05:03:35.944696: step 9500, loss 0.000769501, acc 1, prec 0.113855, recall 0.812279
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-9500

2017-12-10T05:03:37.227713: step 9501, loss 0.0345817, acc 0.984375, prec 0.113854, recall 0.812279
2017-12-10T05:03:37.492874: step 9502, loss 0.134231, acc 0.984375, prec 0.113852, recall 0.812279
2017-12-10T05:03:37.755525: step 9503, loss 3.52092e-05, acc 1, prec 0.113861, recall 0.812293
2017-12-10T05:03:38.013809: step 9504, loss 2.11426, acc 0.984375, prec 0.113861, recall 0.812233
2017-12-10T05:03:38.276491: step 9505, loss 0.254749, acc 0.953125, prec 0.113867, recall 0.812247
2017-12-10T05:03:38.544362: step 9506, loss 0.890835, acc 0.9375, prec 0.11389, recall 0.812288
2017-12-10T05:03:38.814120: step 9507, loss 0.0964603, acc 0.96875, prec 0.113897, recall 0.812302
2017-12-10T05:03:39.077775: step 9508, loss 0.00440849, acc 1, prec 0.113906, recall 0.812316
2017-12-10T05:03:39.340605: step 9509, loss 0.784298, acc 0.984375, prec 0.113923, recall 0.812344
2017-12-10T05:03:39.608447: step 9510, loss 0.143063, acc 0.96875, prec 0.11393, recall 0.812357
2017-12-10T05:03:39.877744: step 9511, loss 0.0250275, acc 0.984375, prec 0.113938, recall 0.812371
2017-12-10T05:03:40.141516: step 9512, loss 0.291205, acc 0.96875, prec 0.113935, recall 0.812371
2017-12-10T05:03:40.400498: step 9513, loss 0.058368, acc 0.984375, prec 0.113943, recall 0.812385
2017-12-10T05:03:40.669932: step 9514, loss 0.323828, acc 0.9375, prec 0.113948, recall 0.812399
2017-12-10T05:03:40.933561: step 9515, loss 0.267856, acc 0.953125, prec 0.113944, recall 0.812399
2017-12-10T05:03:41.201751: step 9516, loss 0.672103, acc 0.921875, prec 0.113957, recall 0.812426
2017-12-10T05:03:41.469071: step 9517, loss 0.676075, acc 0.9375, prec 0.113952, recall 0.812426
2017-12-10T05:03:41.732176: step 9518, loss 0.14309, acc 0.96875, prec 0.113968, recall 0.812454
2017-12-10T05:03:41.997464: step 9519, loss 0.186928, acc 0.953125, prec 0.113973, recall 0.812468
2017-12-10T05:03:42.274855: step 9520, loss 0.539647, acc 0.921875, prec 0.113986, recall 0.812495
2017-12-10T05:03:42.538566: step 9521, loss 0.88157, acc 0.875, prec 0.113986, recall 0.812509
2017-12-10T05:03:42.805481: step 9522, loss 0.0728996, acc 0.984375, prec 0.113993, recall 0.812523
2017-12-10T05:03:43.072209: step 9523, loss 0.0364163, acc 0.984375, prec 0.113992, recall 0.812523
2017-12-10T05:03:43.344525: step 9524, loss 0.642357, acc 0.953125, prec 0.113998, recall 0.812537
2017-12-10T05:03:43.614350: step 9525, loss 0.8601, acc 0.890625, prec 0.11399, recall 0.812537
2017-12-10T05:03:43.883761: step 9526, loss 0.878024, acc 0.90625, prec 0.113983, recall 0.812537
2017-12-10T05:03:44.149233: step 9527, loss 0.246808, acc 0.96875, prec 0.11398, recall 0.812537
2017-12-10T05:03:44.412095: step 9528, loss 0.138014, acc 0.96875, prec 0.113996, recall 0.812564
2017-12-10T05:03:44.674439: step 9529, loss 0.432934, acc 0.96875, prec 0.114012, recall 0.812592
2017-12-10T05:03:44.943709: step 9530, loss 0.210506, acc 0.9375, prec 0.114007, recall 0.812592
2017-12-10T05:03:45.216958: step 9531, loss 0.00176223, acc 1, prec 0.114007, recall 0.812592
2017-12-10T05:03:45.482701: step 9532, loss 0.010563, acc 1, prec 0.114007, recall 0.812592
2017-12-10T05:03:45.752094: step 9533, loss 0.139565, acc 0.96875, prec 0.114005, recall 0.812592
2017-12-10T05:03:46.014258: step 9534, loss 0.30485, acc 0.96875, prec 0.114021, recall 0.812619
2017-12-10T05:03:46.280694: step 9535, loss 0.821714, acc 0.953125, prec 0.114027, recall 0.812633
2017-12-10T05:03:46.548308: step 9536, loss 0.42779, acc 0.96875, prec 0.114033, recall 0.812647
2017-12-10T05:03:46.814416: step 9537, loss 0.0747999, acc 0.96875, prec 0.114031, recall 0.812647
2017-12-10T05:03:47.076229: step 9538, loss 0.00132189, acc 1, prec 0.114031, recall 0.812647
2017-12-10T05:03:47.339239: step 9539, loss 0.407286, acc 0.9375, prec 0.114035, recall 0.812661
2017-12-10T05:03:47.608835: step 9540, loss 0.191213, acc 0.9375, prec 0.114049, recall 0.812688
2017-12-10T05:03:47.873527: step 9541, loss 0.305319, acc 0.921875, prec 0.114052, recall 0.812702
2017-12-10T05:03:48.142526: step 9542, loss 0.556677, acc 0.984375, prec 0.114088, recall 0.812757
2017-12-10T05:03:48.411930: step 9543, loss 0.119457, acc 0.96875, prec 0.114104, recall 0.812784
2017-12-10T05:03:48.674198: step 9544, loss 0.797948, acc 0.953125, prec 0.114118, recall 0.812812
2017-12-10T05:03:48.935009: step 9545, loss 0.321336, acc 0.96875, prec 0.114125, recall 0.812826
2017-12-10T05:03:49.196432: step 9546, loss 0.873875, acc 0.96875, prec 0.114141, recall 0.812853
2017-12-10T05:03:49.462574: step 9547, loss 0.198757, acc 0.96875, prec 0.114139, recall 0.812853
2017-12-10T05:03:49.726478: step 9548, loss 0.350506, acc 0.96875, prec 0.114136, recall 0.812853
2017-12-10T05:03:49.990577: step 9549, loss 0.00730658, acc 1, prec 0.114145, recall 0.812867
2017-12-10T05:03:50.257973: step 9550, loss 0.00119402, acc 1, prec 0.114154, recall 0.812881
2017-12-10T05:03:50.517153: step 9551, loss 0.369229, acc 0.953125, prec 0.114151, recall 0.812881
2017-12-10T05:03:50.785880: step 9552, loss 0.239011, acc 0.96875, prec 0.114158, recall 0.812894
2017-12-10T05:03:51.050733: step 9553, loss 0.000200218, acc 1, prec 0.114167, recall 0.812908
2017-12-10T05:03:51.317623: step 9554, loss 0.109421, acc 0.984375, prec 0.114184, recall 0.812935
2017-12-10T05:03:51.583320: step 9555, loss 0.226816, acc 0.953125, prec 0.11418, recall 0.812935
2017-12-10T05:03:51.848581: step 9556, loss 0.00426201, acc 1, prec 0.114199, recall 0.812963
2017-12-10T05:03:52.110212: step 9557, loss 0.366712, acc 0.953125, prec 0.114204, recall 0.812977
2017-12-10T05:03:52.375742: step 9558, loss 0.122333, acc 0.984375, prec 0.114203, recall 0.812977
2017-12-10T05:03:52.637404: step 9559, loss 0.000198664, acc 1, prec 0.114203, recall 0.812977
2017-12-10T05:03:52.898740: step 9560, loss 5.00208, acc 0.984375, prec 0.114203, recall 0.812917
2017-12-10T05:03:53.164542: step 9561, loss 0.62979, acc 0.953125, prec 0.1142, recall 0.812917
2017-12-10T05:03:53.426342: step 9562, loss 0.00104117, acc 1, prec 0.114218, recall 0.812944
2017-12-10T05:03:53.693297: step 9563, loss 0.0293388, acc 0.984375, prec 0.114217, recall 0.812944
2017-12-10T05:03:53.960234: step 9564, loss 1.02445e-07, acc 1, prec 0.114217, recall 0.812944
2017-12-10T05:03:54.212977: step 9565, loss 0.00614506, acc 1, prec 0.114217, recall 0.812944
2017-12-10T05:03:54.478150: step 9566, loss 0.0151902, acc 0.984375, prec 0.114215, recall 0.812944
2017-12-10T05:03:54.750353: step 9567, loss 0.3869, acc 0.984375, prec 0.114223, recall 0.812958
2017-12-10T05:03:55.020025: step 9568, loss 0.0599669, acc 0.984375, prec 0.11424, recall 0.812985
2017-12-10T05:03:55.290108: step 9569, loss 0.173525, acc 0.953125, prec 0.114246, recall 0.812999
2017-12-10T05:03:55.551833: step 9570, loss 0.312428, acc 0.96875, prec 0.114244, recall 0.812999
2017-12-10T05:03:55.813460: step 9571, loss 0.195147, acc 0.984375, prec 0.114252, recall 0.813013
2017-12-10T05:03:56.074216: step 9572, loss 0.599178, acc 0.953125, prec 0.114257, recall 0.813027
2017-12-10T05:03:56.348327: step 9573, loss 0.0608003, acc 0.96875, prec 0.114255, recall 0.813027
2017-12-10T05:03:56.615069: step 9574, loss 0.127632, acc 0.953125, prec 0.114279, recall 0.813068
2017-12-10T05:03:56.882546: step 9575, loss 0.00156414, acc 1, prec 0.114306, recall 0.813109
2017-12-10T05:03:57.155535: step 9576, loss 0.0341488, acc 0.984375, prec 0.114305, recall 0.813109
2017-12-10T05:03:57.421942: step 9577, loss 0.484162, acc 0.953125, prec 0.11431, recall 0.813122
2017-12-10T05:03:57.691902: step 9578, loss 0.673345, acc 0.953125, prec 0.114316, recall 0.813136
2017-12-10T05:03:57.960144: step 9579, loss 0.0534932, acc 0.96875, prec 0.114314, recall 0.813136
2017-12-10T05:03:58.225109: step 9580, loss 0.0733745, acc 0.984375, prec 0.114322, recall 0.81315
2017-12-10T05:03:58.488811: step 9581, loss 0.177881, acc 0.9375, prec 0.114326, recall 0.813163
2017-12-10T05:03:58.756392: step 9582, loss 0.161132, acc 0.96875, prec 0.114324, recall 0.813163
2017-12-10T05:03:59.020764: step 9583, loss 0.547954, acc 0.9375, prec 0.114319, recall 0.813163
2017-12-10T05:03:59.287764: step 9584, loss 0.12692, acc 0.96875, prec 0.114317, recall 0.813163
2017-12-10T05:03:59.567011: step 9585, loss 0.158847, acc 0.96875, prec 0.114323, recall 0.813177
2017-12-10T05:03:59.835985: step 9586, loss 0.103628, acc 0.953125, prec 0.114329, recall 0.813191
2017-12-10T05:04:00.103523: step 9587, loss 0.472466, acc 0.96875, prec 0.114336, recall 0.813205
2017-12-10T05:04:00.372934: step 9588, loss 0.960785, acc 0.90625, prec 0.114338, recall 0.813218
2017-12-10T05:04:00.644078: step 9589, loss 0.00310244, acc 1, prec 0.114338, recall 0.813218
2017-12-10T05:04:00.908769: step 9590, loss 0.110142, acc 0.984375, prec 0.114337, recall 0.813218
2017-12-10T05:04:01.170984: step 9591, loss 0.0501508, acc 0.96875, prec 0.114334, recall 0.813218
2017-12-10T05:04:01.444362: step 9592, loss 0.300589, acc 0.96875, prec 0.114332, recall 0.813218
2017-12-10T05:04:01.706950: step 9593, loss 0.533227, acc 0.953125, prec 0.114328, recall 0.813218
2017-12-10T05:04:01.971789: step 9594, loss 1.04124e-05, acc 1, prec 0.114328, recall 0.813218
2017-12-10T05:04:02.232952: step 9595, loss 0.466503, acc 0.984375, prec 0.114327, recall 0.813218
2017-12-10T05:04:02.497700: step 9596, loss 0.0242891, acc 1, prec 0.114336, recall 0.813232
2017-12-10T05:04:02.769109: step 9597, loss 0.0127967, acc 1, prec 0.114355, recall 0.813259
2017-12-10T05:04:03.033837: step 9598, loss 0.268969, acc 0.96875, prec 0.114361, recall 0.813273
2017-12-10T05:04:03.301202: step 9599, loss 0.0277429, acc 0.984375, prec 0.11436, recall 0.813273
2017-12-10T05:04:03.568071: step 9600, loss 0.170711, acc 0.984375, prec 0.114377, recall 0.8133

Evaluation:
2017-12-10T05:04:11.185328: step 9600, loss 18.4193, acc 0.969523, prec 0.11452, recall 0.807773

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-9600

2017-12-10T05:04:12.525295: step 9601, loss 0.101588, acc 0.984375, prec 0.114528, recall 0.807787
2017-12-10T05:04:12.797311: step 9602, loss 0.479797, acc 0.953125, prec 0.114552, recall 0.807829
2017-12-10T05:04:13.068520: step 9603, loss 0.00114271, acc 1, prec 0.114561, recall 0.807843
2017-12-10T05:04:13.331638: step 9604, loss 0.0499919, acc 0.984375, prec 0.114569, recall 0.807857
2017-12-10T05:04:13.611625: step 9605, loss 0.00155717, acc 1, prec 0.114569, recall 0.807857
2017-12-10T05:04:13.881927: step 9606, loss 0.0877564, acc 0.984375, prec 0.114576, recall 0.80787
2017-12-10T05:04:14.152087: step 9607, loss 0.871917, acc 0.984375, prec 0.114584, recall 0.807884
2017-12-10T05:04:14.423399: step 9608, loss 0.000277048, acc 1, prec 0.114603, recall 0.807912
2017-12-10T05:04:14.685914: step 9609, loss 0.7054, acc 0.953125, prec 0.114608, recall 0.807926
2017-12-10T05:04:14.950693: step 9610, loss 5.20371e-06, acc 1, prec 0.114626, recall 0.807954
2017-12-10T05:04:15.209178: step 9611, loss 0.0205675, acc 0.984375, prec 0.114625, recall 0.807954
2017-12-10T05:04:15.475068: step 9612, loss 0.00205129, acc 1, prec 0.114625, recall 0.807954
2017-12-10T05:04:15.744925: step 9613, loss 0.0001945, acc 1, prec 0.114634, recall 0.807968
2017-12-10T05:04:16.008590: step 9614, loss 0.504523, acc 0.984375, prec 0.114642, recall 0.807981
2017-12-10T05:04:16.276338: step 9615, loss 0.0801293, acc 0.96875, prec 0.11464, recall 0.807981
2017-12-10T05:04:16.542453: step 9616, loss 0.055707, acc 0.984375, prec 0.114639, recall 0.807981
2017-12-10T05:04:16.803124: step 9617, loss 0.20398, acc 0.984375, prec 0.114646, recall 0.807995
2017-12-10T05:04:17.072388: step 9618, loss 0.0193638, acc 0.984375, prec 0.114645, recall 0.807995
2017-12-10T05:04:17.340624: step 9619, loss 0.0842201, acc 0.984375, prec 0.114671, recall 0.808037
2017-12-10T05:04:17.606312: step 9620, loss 0.16965, acc 0.96875, prec 0.114669, recall 0.808037
2017-12-10T05:04:17.869073: step 9621, loss 0.789437, acc 0.984375, prec 0.114686, recall 0.808065
2017-12-10T05:04:18.140964: step 9622, loss 0.000102662, acc 1, prec 0.114686, recall 0.808065
2017-12-10T05:04:18.408679: step 9623, loss 6.6642, acc 0.984375, prec 0.114695, recall 0.80802
2017-12-10T05:04:18.677467: step 9624, loss 0.350536, acc 0.984375, prec 0.114694, recall 0.80802
2017-12-10T05:04:18.948749: step 9625, loss 0.00138707, acc 1, prec 0.114712, recall 0.808048
2017-12-10T05:04:19.210644: step 9626, loss 0.478469, acc 0.96875, prec 0.11471, recall 0.808048
2017-12-10T05:04:19.474825: step 9627, loss 0.0513111, acc 0.984375, prec 0.114709, recall 0.808048
2017-12-10T05:04:19.746264: step 9628, loss 0.0205147, acc 0.984375, prec 0.114707, recall 0.808048
2017-12-10T05:04:20.009193: step 9629, loss 0.161826, acc 0.953125, prec 0.114704, recall 0.808048
2017-12-10T05:04:20.274207: step 9630, loss 0.171106, acc 0.953125, prec 0.1147, recall 0.808048
2017-12-10T05:04:20.539896: step 9631, loss 0.51608, acc 0.96875, prec 0.114725, recall 0.80809
2017-12-10T05:04:20.807564: step 9632, loss 0.0599947, acc 0.953125, prec 0.114731, recall 0.808103
2017-12-10T05:04:21.068662: step 9633, loss 0.352234, acc 0.984375, prec 0.114748, recall 0.808131
2017-12-10T05:04:21.334384: step 9634, loss 0.332327, acc 0.9375, prec 0.114752, recall 0.808145
2017-12-10T05:04:21.600789: step 9635, loss 0.0389431, acc 0.984375, prec 0.114769, recall 0.808173
2017-12-10T05:04:21.870231: step 9636, loss 0.115998, acc 0.96875, prec 0.114767, recall 0.808173
2017-12-10T05:04:22.140494: step 9637, loss 0.42275, acc 0.9375, prec 0.114771, recall 0.808187
2017-12-10T05:04:22.407273: step 9638, loss 0.236519, acc 0.9375, prec 0.114785, recall 0.808214
2017-12-10T05:04:22.674360: step 9639, loss 0.841103, acc 0.984375, prec 0.114801, recall 0.808242
2017-12-10T05:04:22.939659: step 9640, loss 0.292492, acc 0.921875, prec 0.114805, recall 0.808256
2017-12-10T05:04:23.201794: step 9641, loss 0.000160083, acc 1, prec 0.114823, recall 0.808283
2017-12-10T05:04:23.470500: step 9642, loss 0.325534, acc 0.9375, prec 0.114818, recall 0.808283
2017-12-10T05:04:23.731148: step 9643, loss 0.00555622, acc 1, prec 0.114827, recall 0.808297
2017-12-10T05:04:23.995039: step 9644, loss 0.038122, acc 0.984375, prec 0.114826, recall 0.808297
2017-12-10T05:04:24.265874: step 9645, loss 0.267179, acc 0.96875, prec 0.114824, recall 0.808297
2017-12-10T05:04:24.539195: step 9646, loss 0.758788, acc 0.9375, prec 0.114846, recall 0.808339
2017-12-10T05:04:24.805504: step 9647, loss 0.336829, acc 0.96875, prec 0.114853, recall 0.808353
2017-12-10T05:04:25.073861: step 9648, loss 0.00946423, acc 1, prec 0.11488, recall 0.808394
2017-12-10T05:04:25.340520: step 9649, loss 0.463851, acc 0.9375, prec 0.114894, recall 0.808422
2017-12-10T05:04:25.604505: step 9650, loss 0.569513, acc 0.953125, prec 0.114899, recall 0.808435
2017-12-10T05:04:25.869512: step 9651, loss 0.764171, acc 0.9375, prec 0.114903, recall 0.808449
2017-12-10T05:04:26.138414: step 9652, loss 0.270969, acc 0.96875, prec 0.114919, recall 0.808477
2017-12-10T05:04:26.410847: step 9653, loss 11.8826, acc 0.9375, prec 0.114916, recall 0.808419
2017-12-10T05:04:26.690124: step 9654, loss 0.220156, acc 0.953125, prec 0.114912, recall 0.808419
2017-12-10T05:04:26.951629: step 9655, loss 0.419785, acc 0.953125, prec 0.114918, recall 0.808432
2017-12-10T05:04:27.227043: step 9656, loss 0.522611, acc 0.953125, prec 0.114923, recall 0.808446
2017-12-10T05:04:27.499871: step 9657, loss 0.366219, acc 0.921875, prec 0.114917, recall 0.808446
2017-12-10T05:04:27.763637: step 9658, loss 0.527623, acc 0.890625, prec 0.114909, recall 0.808446
2017-12-10T05:04:28.032970: step 9659, loss 0.00579986, acc 1, prec 0.114927, recall 0.808474
2017-12-10T05:04:28.298602: step 9660, loss 0.848502, acc 0.921875, prec 0.11493, recall 0.808488
2017-12-10T05:04:28.565113: step 9661, loss 0.943287, acc 0.921875, prec 0.114925, recall 0.808488
2017-12-10T05:04:28.838708: step 9662, loss 0.608978, acc 0.921875, prec 0.114928, recall 0.808501
2017-12-10T05:04:29.105898: step 9663, loss 0.0231825, acc 0.984375, prec 0.114945, recall 0.808529
2017-12-10T05:04:29.370301: step 9664, loss 0.40375, acc 0.90625, prec 0.114947, recall 0.808543
2017-12-10T05:04:29.636796: step 9665, loss 0.418327, acc 0.90625, prec 0.114949, recall 0.808557
2017-12-10T05:04:29.904907: step 9666, loss 1.09453, acc 0.953125, prec 0.114954, recall 0.80857
2017-12-10T05:04:30.169601: step 9667, loss 0.0976224, acc 0.96875, prec 0.11497, recall 0.808598
2017-12-10T05:04:30.440915: step 9668, loss 0.808286, acc 0.921875, prec 0.114973, recall 0.808612
2017-12-10T05:04:30.714744: step 9669, loss 0.916738, acc 0.90625, prec 0.114966, recall 0.808612
2017-12-10T05:04:30.983137: step 9670, loss 0.503741, acc 0.9375, prec 0.11497, recall 0.808626
2017-12-10T05:04:31.248908: step 9671, loss 0.675057, acc 0.9375, prec 0.114984, recall 0.808653
2017-12-10T05:04:31.513949: step 9672, loss 0.302148, acc 0.921875, prec 0.114987, recall 0.808667
2017-12-10T05:04:31.779747: step 9673, loss 0.0328275, acc 0.96875, prec 0.114985, recall 0.808667
2017-12-10T05:04:32.041516: step 9674, loss 0.634675, acc 0.921875, prec 0.114988, recall 0.808681
2017-12-10T05:04:32.308193: step 9675, loss 0.781669, acc 0.90625, prec 0.114999, recall 0.808708
2017-12-10T05:04:32.570693: step 9676, loss 0.429688, acc 0.9375, prec 0.114994, recall 0.808708
2017-12-10T05:04:32.832639: step 9677, loss 0.829408, acc 0.96875, prec 0.11501, recall 0.808736
2017-12-10T05:04:33.093960: step 9678, loss 0.0801919, acc 0.984375, prec 0.115009, recall 0.808736
2017-12-10T05:04:33.362590: step 9679, loss 0.414225, acc 0.9375, prec 0.115004, recall 0.808736
2017-12-10T05:04:33.626711: step 9680, loss 0.0718977, acc 0.96875, prec 0.115002, recall 0.808736
2017-12-10T05:04:33.893490: step 9681, loss 0.0959289, acc 0.984375, prec 0.115046, recall 0.808805
2017-12-10T05:04:34.156539: step 9682, loss 0.28647, acc 0.953125, prec 0.115042, recall 0.808805
2017-12-10T05:04:34.420655: step 9683, loss 0.413739, acc 0.984375, prec 0.115041, recall 0.808805
2017-12-10T05:04:34.681389: step 9684, loss 0.0210885, acc 0.984375, prec 0.11504, recall 0.808805
2017-12-10T05:04:34.951865: step 9685, loss 0.00114015, acc 1, prec 0.115058, recall 0.808832
2017-12-10T05:04:35.223528: step 9686, loss 0.313519, acc 0.96875, prec 0.115065, recall 0.808846
2017-12-10T05:04:35.495620: step 9687, loss 0.455824, acc 0.9375, prec 0.115069, recall 0.808859
2017-12-10T05:04:35.766667: step 9688, loss 0.757682, acc 0.921875, prec 0.115072, recall 0.808873
2017-12-10T05:04:36.030394: step 9689, loss 0.00019956, acc 1, prec 0.11509, recall 0.808901
2017-12-10T05:04:36.295961: step 9690, loss 0.10276, acc 0.96875, prec 0.115088, recall 0.808901
2017-12-10T05:04:36.568731: step 9691, loss 0.146324, acc 0.96875, prec 0.115086, recall 0.808901
2017-12-10T05:04:36.833562: step 9692, loss 0.0136319, acc 0.984375, prec 0.115093, recall 0.808914
2017-12-10T05:04:37.097468: step 9693, loss 0.637651, acc 0.96875, prec 0.115091, recall 0.808914
2017-12-10T05:04:37.360836: step 9694, loss 0.00184724, acc 1, prec 0.1151, recall 0.808928
2017-12-10T05:04:37.627699: step 9695, loss 0.0145537, acc 0.984375, prec 0.115108, recall 0.808942
2017-12-10T05:04:37.890724: step 9696, loss 0.639672, acc 0.96875, prec 0.115106, recall 0.808942
2017-12-10T05:04:38.150165: step 9697, loss 0.0118874, acc 1, prec 0.115115, recall 0.808956
2017-12-10T05:04:38.424653: step 9698, loss 2.09386, acc 0.96875, prec 0.11513, recall 0.808983
2017-12-10T05:04:38.683095: step 9699, loss 0.0835572, acc 0.984375, prec 0.115129, recall 0.808983
2017-12-10T05:04:38.950521: step 9700, loss 0.122276, acc 0.984375, prec 0.115128, recall 0.808983
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-9700

2017-12-10T05:04:40.228274: step 9701, loss 0.0132519, acc 1, prec 0.115155, recall 0.809024
2017-12-10T05:04:40.489820: step 9702, loss 0.00154139, acc 1, prec 0.115164, recall 0.809038
2017-12-10T05:04:40.752272: step 9703, loss 2.23517e-08, acc 1, prec 0.115164, recall 0.809038
2017-12-10T05:04:41.004978: step 9704, loss 0.131505, acc 0.984375, prec 0.115163, recall 0.809038
2017-12-10T05:04:41.276083: step 9705, loss 0.20762, acc 0.984375, prec 0.115162, recall 0.809038
2017-12-10T05:04:41.537548: step 9706, loss 0.123958, acc 0.984375, prec 0.115179, recall 0.809065
2017-12-10T05:04:41.795952: step 9707, loss 0.018814, acc 0.984375, prec 0.115187, recall 0.809079
2017-12-10T05:04:42.065002: step 9708, loss 0.131548, acc 0.984375, prec 0.115186, recall 0.809079
2017-12-10T05:04:42.343694: step 9709, loss 0.0928429, acc 0.96875, prec 0.115183, recall 0.809079
2017-12-10T05:04:42.610157: step 9710, loss 2.05057, acc 0.96875, prec 0.115191, recall 0.809035
2017-12-10T05:04:42.878984: step 9711, loss 3.38234e-05, acc 1, prec 0.1152, recall 0.809048
2017-12-10T05:04:43.138249: step 9712, loss 0.122451, acc 0.984375, prec 0.115199, recall 0.809048
2017-12-10T05:04:43.407537: step 9713, loss 0.0831606, acc 0.984375, prec 0.115225, recall 0.80909
2017-12-10T05:04:43.673448: step 9714, loss 0.0843646, acc 0.984375, prec 0.115233, recall 0.809103
2017-12-10T05:04:43.938153: step 9715, loss 0.651605, acc 0.984375, prec 0.115232, recall 0.809103
2017-12-10T05:04:44.207671: step 9716, loss 0.00253199, acc 1, prec 0.115241, recall 0.809117
2017-12-10T05:04:44.468825: step 9717, loss 0.0152625, acc 0.984375, prec 0.11524, recall 0.809117
2017-12-10T05:04:44.730239: step 9718, loss 0.248751, acc 0.984375, prec 0.115238, recall 0.809117
2017-12-10T05:04:44.993452: step 9719, loss 0.0026329, acc 1, prec 0.115238, recall 0.809117
2017-12-10T05:04:45.263363: step 9720, loss 0.920445, acc 0.9375, prec 0.115252, recall 0.809144
2017-12-10T05:04:45.532745: step 9721, loss 0.0182973, acc 1, prec 0.11527, recall 0.809172
2017-12-10T05:04:45.798974: step 9722, loss 0.303498, acc 0.96875, prec 0.115267, recall 0.809172
2017-12-10T05:04:46.069584: step 9723, loss 0.654002, acc 0.96875, prec 0.115274, recall 0.809186
2017-12-10T05:04:46.337902: step 9724, loss 0.073401, acc 0.984375, prec 0.1153, recall 0.809227
2017-12-10T05:04:46.599992: step 9725, loss 0.00192634, acc 1, prec 0.115309, recall 0.80924
2017-12-10T05:04:46.860893: step 9726, loss 0.105786, acc 0.96875, prec 0.115316, recall 0.809254
2017-12-10T05:04:47.123401: step 9727, loss 2.15515, acc 0.921875, prec 0.11532, recall 0.80921
2017-12-10T05:04:47.396241: step 9728, loss 0.0134445, acc 0.984375, prec 0.115319, recall 0.80921
2017-12-10T05:04:47.670568: step 9729, loss 0.184772, acc 0.96875, prec 0.115317, recall 0.80921
2017-12-10T05:04:47.940668: step 9730, loss 0.0395436, acc 0.96875, prec 0.115323, recall 0.809223
2017-12-10T05:04:48.206955: step 9731, loss 0.432194, acc 0.9375, prec 0.115319, recall 0.809223
2017-12-10T05:04:48.474125: step 9732, loss 0.257898, acc 0.984375, prec 0.115317, recall 0.809223
2017-12-10T05:04:48.733561: step 9733, loss 0.595792, acc 0.9375, prec 0.115313, recall 0.809223
2017-12-10T05:04:49.004753: step 9734, loss 0.00398314, acc 1, prec 0.11534, recall 0.809264
2017-12-10T05:04:49.268663: step 9735, loss 0.0627322, acc 0.984375, prec 0.115348, recall 0.809278
2017-12-10T05:04:49.539002: step 9736, loss 0.319704, acc 0.953125, prec 0.115353, recall 0.809292
2017-12-10T05:04:49.813004: step 9737, loss 0.579447, acc 0.9375, prec 0.115357, recall 0.809305
2017-12-10T05:04:50.076584: step 9738, loss 0.169316, acc 0.9375, prec 0.115353, recall 0.809305
2017-12-10T05:04:50.342548: step 9739, loss 0.605206, acc 0.9375, prec 0.115366, recall 0.809333
2017-12-10T05:04:50.614293: step 9740, loss 1.13346, acc 0.9375, prec 0.115361, recall 0.809333
2017-12-10T05:04:50.881842: step 9741, loss 0.00303102, acc 1, prec 0.115361, recall 0.809333
2017-12-10T05:04:51.152989: step 9742, loss 0.369099, acc 0.96875, prec 0.115359, recall 0.809333
2017-12-10T05:04:51.417880: step 9743, loss 0.173727, acc 0.96875, prec 0.115357, recall 0.809333
2017-12-10T05:04:51.684829: step 9744, loss 0.901641, acc 0.9375, prec 0.115361, recall 0.809346
2017-12-10T05:04:51.948639: step 9745, loss 0.786841, acc 0.96875, prec 0.115368, recall 0.80936
2017-12-10T05:04:52.219014: step 9746, loss 0.20084, acc 0.953125, prec 0.115373, recall 0.809374
2017-12-10T05:04:52.482002: step 9747, loss 0.834278, acc 0.96875, prec 0.115371, recall 0.809374
2017-12-10T05:04:52.751545: step 9748, loss 0.0833569, acc 1, prec 0.11538, recall 0.809387
2017-12-10T05:04:53.016955: step 9749, loss 0.932092, acc 0.921875, prec 0.115383, recall 0.809401
2017-12-10T05:04:53.283694: step 9750, loss 1.00314, acc 0.9375, prec 0.115387, recall 0.809415
2017-12-10T05:04:53.555004: step 9751, loss 0.741076, acc 0.9375, prec 0.115392, recall 0.809428
2017-12-10T05:04:53.822990: step 9752, loss 0.203309, acc 0.96875, prec 0.115398, recall 0.809442
2017-12-10T05:04:54.083966: step 9753, loss 1.19484, acc 0.9375, prec 0.115394, recall 0.809442
2017-12-10T05:04:54.362185: step 9754, loss 0.209397, acc 0.96875, prec 0.115391, recall 0.809442
2017-12-10T05:04:54.629636: step 9755, loss 0.256731, acc 0.96875, prec 0.115389, recall 0.809442
2017-12-10T05:04:54.897125: step 9756, loss 0.55004, acc 0.984375, prec 0.115388, recall 0.809442
2017-12-10T05:04:55.160963: step 9757, loss 0.000641943, acc 1, prec 0.115397, recall 0.809456
2017-12-10T05:04:55.420148: step 9758, loss 0.00937603, acc 1, prec 0.115424, recall 0.809497
2017-12-10T05:04:55.684040: step 9759, loss 0.00772255, acc 1, prec 0.115433, recall 0.80951
2017-12-10T05:04:55.944943: step 9760, loss 0.00811521, acc 1, prec 0.115442, recall 0.809524
2017-12-10T05:04:56.212428: step 9761, loss 0.0944413, acc 0.984375, prec 0.115459, recall 0.809551
2017-12-10T05:04:56.484225: step 9762, loss 0.812487, acc 0.984375, prec 0.115476, recall 0.809578
2017-12-10T05:04:56.748571: step 9763, loss 0.348464, acc 0.921875, prec 0.11547, recall 0.809578
2017-12-10T05:04:57.009853: step 9764, loss 0.593093, acc 0.953125, prec 0.115484, recall 0.809606
2017-12-10T05:04:57.289420: step 9765, loss 1.13615, acc 0.90625, prec 0.115504, recall 0.809646
2017-12-10T05:04:57.551587: step 9766, loss 0.000949826, acc 1, prec 0.115522, recall 0.809674
2017-12-10T05:04:57.817825: step 9767, loss 0.386189, acc 0.96875, prec 0.11552, recall 0.809674
2017-12-10T05:04:58.086469: step 9768, loss 0.281438, acc 0.96875, prec 0.115518, recall 0.809674
2017-12-10T05:04:58.352323: step 9769, loss 1.16772, acc 0.9375, prec 0.115513, recall 0.809674
2017-12-10T05:04:58.622365: step 9770, loss 0.107907, acc 0.953125, prec 0.115518, recall 0.809687
2017-12-10T05:04:58.882656: step 9771, loss 0.0514718, acc 0.984375, prec 0.115526, recall 0.809701
2017-12-10T05:04:59.143616: step 9772, loss 0.179273, acc 0.984375, prec 0.115525, recall 0.809701
2017-12-10T05:04:59.406139: step 9773, loss 0.31028, acc 0.9375, prec 0.11552, recall 0.809701
2017-12-10T05:04:59.670845: step 9774, loss 0.43834, acc 0.9375, prec 0.115516, recall 0.809701
2017-12-10T05:04:59.937705: step 9775, loss 0.246459, acc 0.984375, prec 0.115524, recall 0.809715
2017-12-10T05:05:00.206341: step 9776, loss 0.673281, acc 0.953125, prec 0.115529, recall 0.809728
2017-12-10T05:05:00.479703: step 9777, loss 0.198344, acc 0.984375, prec 0.115528, recall 0.809728
2017-12-10T05:05:00.750973: step 9778, loss 0.000973176, acc 1, prec 0.115546, recall 0.809755
2017-12-10T05:05:01.013432: step 9779, loss 0.162977, acc 0.96875, prec 0.115553, recall 0.809769
2017-12-10T05:05:01.282400: step 9780, loss 0.252474, acc 0.96875, prec 0.11555, recall 0.809769
2017-12-10T05:05:01.547328: step 9781, loss 0.0356415, acc 0.984375, prec 0.115549, recall 0.809769
2017-12-10T05:05:01.811953: step 9782, loss 0.29638, acc 0.96875, prec 0.115547, recall 0.809769
2017-12-10T05:05:02.076559: step 9783, loss 0.567489, acc 0.96875, prec 0.115598, recall 0.809851
2017-12-10T05:05:02.346522: step 9784, loss 0.000188566, acc 1, prec 0.115598, recall 0.809851
2017-12-10T05:05:02.607040: step 9785, loss 7.56815e-06, acc 1, prec 0.115598, recall 0.809851
2017-12-10T05:05:02.867427: step 9786, loss 0.490907, acc 0.953125, prec 0.115604, recall 0.809864
2017-12-10T05:05:03.135841: step 9787, loss 0.341549, acc 0.984375, prec 0.115612, recall 0.809878
2017-12-10T05:05:03.400864: step 9788, loss 0.113842, acc 0.96875, prec 0.115609, recall 0.809878
2017-12-10T05:05:03.663464: step 9789, loss 0.0561703, acc 0.984375, prec 0.115626, recall 0.809905
2017-12-10T05:05:03.928936: step 9790, loss 0.379741, acc 0.96875, prec 0.115633, recall 0.809919
2017-12-10T05:05:04.194291: step 9791, loss 0.0417164, acc 0.96875, prec 0.11564, recall 0.809932
2017-12-10T05:05:04.465897: step 9792, loss 4.37716e-07, acc 1, prec 0.11564, recall 0.809932
2017-12-10T05:05:04.722392: step 9793, loss 20.9572, acc 0.984375, prec 0.115658, recall 0.809901
2017-12-10T05:05:04.986476: step 9794, loss 0.259303, acc 0.984375, prec 0.115666, recall 0.809915
2017-12-10T05:05:05.253748: step 9795, loss 0.547562, acc 0.96875, prec 0.115663, recall 0.809915
2017-12-10T05:05:05.524777: step 9796, loss 0.5272, acc 0.984375, prec 0.115671, recall 0.809929
2017-12-10T05:05:05.791349: step 9797, loss 0.0846187, acc 0.96875, prec 0.115669, recall 0.809929
2017-12-10T05:05:06.054228: step 9798, loss 0.363008, acc 0.9375, prec 0.115691, recall 0.809969
2017-12-10T05:05:06.317516: step 9799, loss 0.537952, acc 0.9375, prec 0.115695, recall 0.809983
2017-12-10T05:05:06.580605: step 9800, loss 0.0988947, acc 0.96875, prec 0.115702, recall 0.809996
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-9800

2017-12-10T05:05:07.855492: step 9801, loss 0.415992, acc 0.921875, prec 0.115705, recall 0.81001
2017-12-10T05:05:08.124321: step 9802, loss 0.396681, acc 0.953125, prec 0.115711, recall 0.810024
2017-12-10T05:05:08.406010: step 9803, loss 0.514836, acc 0.921875, prec 0.115732, recall 0.810064
2017-12-10T05:05:08.667198: step 9804, loss 0.186897, acc 0.953125, prec 0.115737, recall 0.810078
2017-12-10T05:05:08.931382: step 9805, loss 1.1273, acc 0.90625, prec 0.11573, recall 0.810078
2017-12-10T05:05:09.192261: step 9806, loss 1.00766, acc 0.859375, prec 0.115729, recall 0.810091
2017-12-10T05:05:09.464088: step 9807, loss 0.371261, acc 0.9375, prec 0.115733, recall 0.810105
2017-12-10T05:05:09.729210: step 9808, loss 0.413553, acc 0.9375, prec 0.115737, recall 0.810118
2017-12-10T05:05:09.994693: step 9809, loss 0.696582, acc 0.953125, prec 0.115752, recall 0.810146
2017-12-10T05:05:10.268979: step 9810, loss 0.255486, acc 0.953125, prec 0.115757, recall 0.810159
2017-12-10T05:05:10.537743: step 9811, loss 0.479729, acc 0.90625, prec 0.11575, recall 0.810159
2017-12-10T05:05:10.810574: step 9812, loss 0.4062, acc 0.921875, prec 0.115771, recall 0.8102
2017-12-10T05:05:11.075867: step 9813, loss 0.329403, acc 0.9375, prec 0.115766, recall 0.8102
2017-12-10T05:05:11.346238: step 9814, loss 0.113435, acc 0.96875, prec 0.115791, recall 0.81024
2017-12-10T05:05:11.605702: step 9815, loss 0.168804, acc 0.953125, prec 0.115797, recall 0.810254
2017-12-10T05:05:11.875505: step 9816, loss 0.713925, acc 0.9375, prec 0.11581, recall 0.810281
2017-12-10T05:05:12.147213: step 9817, loss 0.224825, acc 0.953125, prec 0.115815, recall 0.810294
2017-12-10T05:05:12.415135: step 9818, loss 0.0935981, acc 0.96875, prec 0.115813, recall 0.810294
2017-12-10T05:05:12.680046: step 9819, loss 0.774583, acc 0.953125, prec 0.115809, recall 0.810294
2017-12-10T05:05:12.946579: step 9820, loss 0.24599, acc 0.9375, prec 0.115805, recall 0.810294
2017-12-10T05:05:13.214385: step 9821, loss 0.371709, acc 0.984375, prec 0.115813, recall 0.810308
2017-12-10T05:05:13.477848: step 9822, loss 0.27239, acc 0.953125, prec 0.115809, recall 0.810308
2017-12-10T05:05:13.741726: step 9823, loss 3.54947e-05, acc 1, prec 0.115818, recall 0.810321
2017-12-10T05:05:14.001527: step 9824, loss 0.0884145, acc 0.96875, prec 0.115834, recall 0.810349
2017-12-10T05:05:14.271987: step 9825, loss 0.887601, acc 0.96875, prec 0.11584, recall 0.810362
2017-12-10T05:05:14.535059: step 9826, loss 0.593432, acc 0.96875, prec 0.115838, recall 0.810362
2017-12-10T05:05:14.799673: step 9827, loss 0.097939, acc 0.96875, prec 0.115836, recall 0.810362
2017-12-10T05:05:15.068567: step 9828, loss 0.00319204, acc 1, prec 0.115854, recall 0.810389
2017-12-10T05:05:15.329616: step 9829, loss 1.55527e-06, acc 1, prec 0.115854, recall 0.810389
2017-12-10T05:05:15.586525: step 9830, loss 0.271354, acc 0.984375, prec 0.11587, recall 0.810416
2017-12-10T05:05:15.850015: step 9831, loss 0.00864387, acc 1, prec 0.11587, recall 0.810416
2017-12-10T05:05:16.112005: step 9832, loss 0.166646, acc 0.96875, prec 0.115868, recall 0.810416
2017-12-10T05:05:16.380566: step 9833, loss 0.135899, acc 0.984375, prec 0.115876, recall 0.81043
2017-12-10T05:05:16.647805: step 9834, loss 0.262781, acc 0.96875, prec 0.115874, recall 0.81043
2017-12-10T05:05:16.915288: step 9835, loss 5.98166e-05, acc 1, prec 0.115874, recall 0.81043
2017-12-10T05:05:17.179778: step 9836, loss 0.0281934, acc 0.984375, prec 0.115872, recall 0.81043
2017-12-10T05:05:17.444958: step 9837, loss 0.350342, acc 0.96875, prec 0.115888, recall 0.810457
2017-12-10T05:05:17.712272: step 9838, loss 21.9423, acc 0.984375, prec 0.115888, recall 0.810399
2017-12-10T05:05:17.983265: step 9839, loss 0.673909, acc 0.96875, prec 0.115895, recall 0.810412
2017-12-10T05:05:18.250264: step 9840, loss 0.471572, acc 0.96875, prec 0.11591, recall 0.810439
2017-12-10T05:05:18.516737: step 9841, loss 0.553105, acc 0.953125, prec 0.115907, recall 0.810439
2017-12-10T05:05:18.779081: step 9842, loss 0.00107924, acc 1, prec 0.115907, recall 0.810439
2017-12-10T05:05:19.041512: step 9843, loss 0.304826, acc 0.96875, prec 0.115922, recall 0.810466
2017-12-10T05:05:19.302364: step 9844, loss 0.176966, acc 0.96875, prec 0.115938, recall 0.810493
2017-12-10T05:05:19.570666: step 9845, loss 0.221347, acc 0.953125, prec 0.115944, recall 0.810507
2017-12-10T05:05:19.836087: step 9846, loss 0.129972, acc 0.96875, prec 0.115959, recall 0.810534
2017-12-10T05:05:20.100406: step 9847, loss 0.405323, acc 0.96875, prec 0.115966, recall 0.810547
2017-12-10T05:05:20.366401: step 9848, loss 0.145335, acc 0.96875, prec 0.115972, recall 0.810561
2017-12-10T05:05:20.636282: step 9849, loss 0.164091, acc 0.96875, prec 0.115979, recall 0.810574
2017-12-10T05:05:20.903797: step 9850, loss 0.851236, acc 0.953125, prec 0.115994, recall 0.810601
2017-12-10T05:05:21.170030: step 9851, loss 0.63265, acc 0.953125, prec 0.115999, recall 0.810615
2017-12-10T05:05:21.432877: step 9852, loss 0.865988, acc 0.921875, prec 0.115993, recall 0.810615
2017-12-10T05:05:21.698235: step 9853, loss 0.905403, acc 0.90625, prec 0.115986, recall 0.810615
2017-12-10T05:05:21.970840: step 9854, loss 0.933363, acc 0.921875, prec 0.11598, recall 0.810615
2017-12-10T05:05:22.232683: step 9855, loss 0.645375, acc 0.921875, prec 0.115974, recall 0.810615
2017-12-10T05:05:22.497670: step 9856, loss 0.997552, acc 0.921875, prec 0.115977, recall 0.810628
2017-12-10T05:05:22.763268: step 9857, loss 0.292405, acc 0.921875, prec 0.11598, recall 0.810642
2017-12-10T05:05:23.031156: step 9858, loss 0.0871383, acc 0.984375, prec 0.115979, recall 0.810642
2017-12-10T05:05:23.294000: step 9859, loss 0.149576, acc 0.9375, prec 0.115974, recall 0.810642
2017-12-10T05:05:23.564131: step 9860, loss 0.251939, acc 0.953125, prec 0.115971, recall 0.810642
2017-12-10T05:05:23.833554: step 9861, loss 0.313314, acc 0.96875, prec 0.115969, recall 0.810642
2017-12-10T05:05:24.101496: step 9862, loss 0.111343, acc 0.96875, prec 0.115975, recall 0.810655
2017-12-10T05:05:24.365090: step 9863, loss 1.01785, acc 0.953125, prec 0.115981, recall 0.810669
2017-12-10T05:05:24.626365: step 9864, loss 0.683689, acc 0.953125, prec 0.115977, recall 0.810669
2017-12-10T05:05:24.898260: step 9865, loss 0.12937, acc 0.9375, prec 0.115981, recall 0.810682
2017-12-10T05:05:25.171575: step 9866, loss 0.553316, acc 0.90625, prec 0.115974, recall 0.810682
2017-12-10T05:05:25.440338: step 9867, loss 0.401903, acc 0.9375, prec 0.11597, recall 0.810682
2017-12-10T05:05:25.707646: step 9868, loss 0.0157668, acc 0.984375, prec 0.115968, recall 0.810682
2017-12-10T05:05:25.971104: step 9869, loss 0.217206, acc 0.953125, prec 0.115974, recall 0.810695
2017-12-10T05:05:26.234409: step 9870, loss 9.34255e-06, acc 1, prec 0.116001, recall 0.810736
2017-12-10T05:05:26.490004: step 9871, loss 0.109868, acc 0.96875, prec 0.116016, recall 0.810763
2017-12-10T05:05:26.762860: step 9872, loss 0.23654, acc 0.984375, prec 0.116033, recall 0.81079
2017-12-10T05:05:27.027707: step 9873, loss 8.2718, acc 0.921875, prec 0.116029, recall 0.810732
2017-12-10T05:05:27.304838: step 9874, loss 0.111723, acc 0.984375, prec 0.116027, recall 0.810732
2017-12-10T05:05:27.568236: step 9875, loss 0.538293, acc 0.9375, prec 0.116023, recall 0.810732
2017-12-10T05:05:27.833827: step 9876, loss 0.267465, acc 0.96875, prec 0.11602, recall 0.810732
2017-12-10T05:05:28.100024: step 9877, loss 0.205038, acc 0.953125, prec 0.116017, recall 0.810732
2017-12-10T05:05:28.361994: step 9878, loss 0.349646, acc 0.9375, prec 0.116012, recall 0.810732
2017-12-10T05:05:28.628228: step 9879, loss 0.24689, acc 0.953125, prec 0.116009, recall 0.810732
2017-12-10T05:05:28.894662: step 9880, loss 0.51542, acc 0.96875, prec 0.116051, recall 0.810799
2017-12-10T05:05:29.158140: step 9881, loss 0.44655, acc 0.96875, prec 0.116058, recall 0.810813
2017-12-10T05:05:29.425368: step 9882, loss 0.0413437, acc 0.984375, prec 0.116075, recall 0.81084
2017-12-10T05:05:29.690357: step 9883, loss 0.00125938, acc 1, prec 0.116084, recall 0.810853
2017-12-10T05:05:29.957171: step 9884, loss 0.108804, acc 0.984375, prec 0.116082, recall 0.810853
2017-12-10T05:05:30.228592: step 9885, loss 0.078493, acc 0.984375, prec 0.116117, recall 0.810907
2017-12-10T05:05:30.504437: step 9886, loss 0.0893928, acc 0.953125, prec 0.116114, recall 0.810907
2017-12-10T05:05:30.770581: step 9887, loss 0.60999, acc 0.90625, prec 0.116115, recall 0.81092
2017-12-10T05:05:31.037858: step 9888, loss 0.621449, acc 0.90625, prec 0.116117, recall 0.810934
2017-12-10T05:05:31.305840: step 9889, loss 0.169066, acc 0.96875, prec 0.116115, recall 0.810934
2017-12-10T05:05:31.569971: step 9890, loss 0.0753924, acc 0.96875, prec 0.116122, recall 0.810947
2017-12-10T05:05:31.832973: step 9891, loss 0.355942, acc 0.96875, prec 0.116128, recall 0.81096
2017-12-10T05:05:32.102079: step 9892, loss 0.287068, acc 0.96875, prec 0.116126, recall 0.81096
2017-12-10T05:05:32.364756: step 9893, loss 0.543319, acc 0.96875, prec 0.116141, recall 0.810987
2017-12-10T05:05:32.627413: step 9894, loss 0.135989, acc 0.96875, prec 0.116166, recall 0.811028
2017-12-10T05:05:32.892118: step 9895, loss 0.000402814, acc 1, prec 0.116175, recall 0.811041
2017-12-10T05:05:33.166391: step 9896, loss 0.996776, acc 0.9375, prec 0.116188, recall 0.811068
2017-12-10T05:05:33.434301: step 9897, loss 0.805121, acc 0.90625, prec 0.116208, recall 0.811108
2017-12-10T05:05:33.699361: step 9898, loss 0.0681977, acc 1, prec 0.116217, recall 0.811121
2017-12-10T05:05:33.970811: step 9899, loss 0.162123, acc 0.96875, prec 0.116215, recall 0.811121
2017-12-10T05:05:34.240273: step 9900, loss 0.274532, acc 0.96875, prec 0.11623, recall 0.811148

Evaluation:
2017-12-10T05:05:41.840348: step 9900, loss 16.2926, acc 0.958011, prec 0.116277, recall 0.806302

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_2/1512897281/checkpoints/model-9900

2017-12-10T05:05:43.103282: step 9901, loss 0.236904, acc 0.96875, prec 0.116302, recall 0.806343
2017-12-10T05:05:43.365969: step 9902, loss 0.0916094, acc 0.96875, prec 0.116299, recall 0.806343
2017-12-10T05:05:43.636735: step 9903, loss 0.374503, acc 0.9375, prec 0.116295, recall 0.806343
2017-12-10T05:05:43.900923: step 9904, loss 0.0043058, acc 1, prec 0.116321, recall 0.806384
2017-12-10T05:05:44.167603: step 9905, loss 0.0771552, acc 0.96875, prec 0.116328, recall 0.806397
2017-12-10T05:05:44.446748: step 9906, loss 0.130307, acc 0.984375, prec 0.116336, recall 0.806411
2017-12-10T05:05:44.712084: step 9907, loss 0.10992, acc 0.96875, prec 0.116342, recall 0.806424
2017-12-10T05:05:44.982136: step 9908, loss 0.157866, acc 0.96875, prec 0.11634, recall 0.806424
2017-12-10T05:05:45.248101: step 9909, loss 0.00262096, acc 1, prec 0.116349, recall 0.806438
2017-12-10T05:05:45.515371: step 9910, loss 0.00313115, acc 1, prec 0.116349, recall 0.806438
2017-12-10T05:05:45.782559: step 9911, loss 0.174398, acc 0.984375, prec 0.116366, recall 0.806465
2017-12-10T05:05:46.050962: step 9912, loss 0.274121, acc 0.984375, prec 0.116364, recall 0.806465
2017-12-10T05:05:46.326675: step 9913, loss 0.662868, acc 0.96875, prec 0.11638, recall 0.806492
2017-12-10T05:05:46.588267: step 9914, loss 0.00026428, acc 1, prec 0.11638, recall 0.806492
2017-12-10T05:05:46.845486: step 9915, loss 0.253956, acc 0.984375, prec 0.116379, recall 0.806492
2017-12-10T05:05:47.116729: step 9916, loss 0.257142, acc 0.984375, prec 0.116387, recall 0.806506
2017-12-10T05:05:47.383951: step 9917, loss 0.00030205, acc 1, prec 0.116387, recall 0.806506
2017-12-10T05:05:47.655984: step 9918, loss 0.0158866, acc 1, prec 0.116404, recall 0.806533
2017-12-10T05:05:47.929197: step 9919, loss 0.242926, acc 0.984375, prec 0.116412, recall 0.806547
2017-12-10T05:05:48.199772: step 9920, loss 4.54984e-06, acc 1, prec 0.116421, recall 0.80656
2017-12-10T05:05:48.460636: step 9921, loss 0.00121326, acc 1, prec 0.116421, recall 0.80656
2017-12-10T05:05:48.724604: step 9922, loss 0.0570128, acc 0.984375, prec 0.116429, recall 0.806574
2017-12-10T05:05:48.997616: step 9923, loss 0.0331487, acc 0.984375, prec 0.116428, recall 0.806574
2017-12-10T05:05:49.265077: step 9924, loss 0.00351288, acc 1, prec 0.116428, recall 0.806574
2017-12-10T05:05:49.531289: step 9925, loss 0.651798, acc 0.984375, prec 0.116427, recall 0.806574
2017-12-10T05:05:49.797272: step 9926, loss 0.00418394, acc 1, prec 0.116427, recall 0.806574
2017-12-10T05:05:50.062366: step 9927, loss 0.7212, acc 0.96875, prec 0.116424, recall 0.806574
2017-12-10T05:05:50.343319: step 9928, loss 0.00552617, acc 1, prec 0.116433, recall 0.806587
2017-12-10T05:05:50.605969: step 9929, loss 0.00232641, acc 1, prec 0.116442, recall 0.806601
2017-12-10T05:05:50.871078: step 9930, loss 0.00220647, acc 1, prec 0.116469, recall 0.806641
2017-12-10T05:05:51.137424: step 9931, loss 0.000919964, acc 1, prec 0.116469, recall 0.806641
2017-12-10T05:05:51.400950: step 9932, loss 0.00339471, acc 1, prec 0.116478, recall 0.806655
2017-12-10T05:05:51.666567: step 9933, loss 8.51277e-06, acc 1, prec 0.116487, recall 0.806669
2017-12-10T05:05:51.925470: step 9934, loss 0.0398176, acc 0.984375, prec 0.116495, recall 0.806682
2017-12-10T05:05:52.186982: step 9935, loss 7.63684e-08, acc 1, prec 0.116495, recall 0.806682
2017-12-10T05:05:52.447146: step 9936, loss 1.76874e-05, acc 1, prec 0.116503, recall 0.806696
2017-12-10T05:05:52.711551: step 9937, loss 0.0233348, acc 0.984375, prec 0.116511, recall 0.806709
2017-12-10T05:05:52.980113: step 9938, loss 0.0701014, acc 0.984375, prec 0.116519, recall 0.806723
2017-12-10T05:05:53.251729: step 9939, loss 9.81108, acc 0.953125, prec 0.116535, recall 0.806693
2017-12-10T05:05:53.485101: step 9940, loss 0.000247336, acc 1, prec 0.116543, recall 0.806707
Training finished
Starting Fold: 3 => Train/Dev split: 31796/10598


 #################### 


SEQUENCE LENGTH 273
BATCH SIZE 64
EMBEDDING SIZE 128
FILTER SIZES [3, 4, 5]
NUMBER OF FILTERS 256
L2 REG LAMBDA 0.0
EPOCHS 20



RESULT DIR num_filter_256_fold_3
VOCAB SIZE 33447
DROPOUT PROBABILITY 0.5



WORD EMBEDDING True
POSITION EMBEDDING True
POS EMBEDDING True


 #################### 


Writing to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354

Start training
2017-12-10T05:05:57.284212: step 1, loss 2.09859, acc 0.515625, prec 0, recall 0
2017-12-10T05:05:57.552755: step 2, loss 16.8898, acc 0.9375, prec 0, recall 0
2017-12-10T05:05:57.816789: step 3, loss 48.0892, acc 0.9375, prec 0, recall 0
2017-12-10T05:05:58.088979: step 4, loss 1.52358, acc 0.90625, prec 0, recall 0
2017-12-10T05:05:58.355441: step 5, loss 0.488914, acc 0.828125, prec 0, recall 0
2017-12-10T05:05:58.615033: step 6, loss 1.04947, acc 0.671875, prec 0.0136986, recall 0.166667
2017-12-10T05:05:58.881938: step 7, loss 2.46861, acc 0.6875, prec 0.0212766, recall 0.285714
2017-12-10T05:05:59.144824: step 8, loss 3.09336, acc 0.421875, prec 0.0152672, recall 0.285714
2017-12-10T05:05:59.400105: step 9, loss 3.58557, acc 0.328125, prec 0.0171429, recall 0.375
2017-12-10T05:05:59.656635: step 10, loss 2.96515, acc 0.34375, prec 0.0228311, recall 0.5
2017-12-10T05:05:59.921121: step 11, loss 5.36115, acc 0.15625, prec 0.0218978, recall 0.545455
2017-12-10T05:06:00.183540: step 12, loss 3.69161, acc 0.34375, prec 0.022082, recall 0.583333
2017-12-10T05:06:00.456790: step 13, loss 2.09203, acc 0.515625, prec 0.0201149, recall 0.583333
2017-12-10T05:06:00.729914: step 14, loss 11.748, acc 0.625, prec 0.0241287, recall 0.6
2017-12-10T05:06:00.994886: step 15, loss 1.58496, acc 0.5625, prec 0.0224439, recall 0.6
2017-12-10T05:06:01.262472: step 16, loss 1.16065, acc 0.734375, prec 0.0215311, recall 0.6
2017-12-10T05:06:01.533789: step 17, loss 7.75887, acc 0.84375, prec 0.0233645, recall 0.588235
2017-12-10T05:06:01.803279: step 18, loss 1.40659, acc 0.6875, prec 0.0223214, recall 0.588235
2017-12-10T05:06:02.065652: step 19, loss 1.76363, acc 0.71875, prec 0.0235546, recall 0.611111
2017-12-10T05:06:02.338555: step 20, loss 0.738112, acc 0.8125, prec 0.0229645, recall 0.611111
2017-12-10T05:06:02.602081: step 21, loss 0.838797, acc 0.75, prec 0.0241935, recall 0.631579
2017-12-10T05:06:02.874313: step 22, loss 0.732956, acc 0.828125, prec 0.0236686, recall 0.631579
2017-12-10T05:06:03.145658: step 23, loss 0.810795, acc 0.796875, prec 0.0230769, recall 0.631579
2017-12-10T05:06:03.413234: step 24, loss 0.988606, acc 0.75, prec 0.0223881, recall 0.631579
2017-12-10T05:06:03.676454: step 25, loss 13.2991, acc 0.859375, prec 0.0220588, recall 0.6
2017-12-10T05:06:03.952488: step 26, loss 19.4463, acc 0.765625, prec 0.021544, recall 0.545455
2017-12-10T05:06:04.220724: step 27, loss 1.41836, acc 0.703125, prec 0.0225303, recall 0.565217
2017-12-10T05:06:04.478266: step 28, loss 7.458, acc 0.59375, prec 0.0232172, recall 0.56
2017-12-10T05:06:04.741071: step 29, loss 8.97473, acc 0.578125, prec 0.0253566, recall 0.571429
2017-12-10T05:06:05.003490: step 30, loss 3.24957, acc 0.34375, prec 0.0237741, recall 0.571429
2017-12-10T05:06:05.273095: step 31, loss 5.11999, acc 0.234375, prec 0.0221607, recall 0.571429
2017-12-10T05:06:05.544298: step 32, loss 6.20012, acc 0.265625, prec 0.0220779, recall 0.586207
2017-12-10T05:06:05.801818: step 33, loss 5.25704, acc 0.21875, prec 0.0207317, recall 0.586207
2017-12-10T05:06:06.067412: step 34, loss 6.10094, acc 0.25, prec 0.0207135, recall 0.6
2017-12-10T05:06:06.327532: step 35, loss 5.5137, acc 0.328125, prec 0.0197368, recall 0.6
2017-12-10T05:06:06.589503: step 36, loss 4.61047, acc 0.28125, prec 0.0198123, recall 0.612903
2017-12-10T05:06:06.855769: step 37, loss 4.47605, acc 0.328125, prec 0.0218905, recall 0.647059
2017-12-10T05:06:07.115351: step 38, loss 3.12798, acc 0.40625, prec 0.0220307, recall 0.657143
2017-12-10T05:06:07.384094: step 39, loss 3.09526, acc 0.5, prec 0.0213755, recall 0.657143
2017-12-10T05:06:07.650147: step 40, loss 2.32463, acc 0.546875, prec 0.0208145, recall 0.657143
2017-12-10T05:06:07.911259: step 41, loss 5.2334, acc 0.71875, prec 0.0213713, recall 0.648649
2017-12-10T05:06:08.172750: step 42, loss 1.56164, acc 0.640625, prec 0.0234987, recall 0.675
2017-12-10T05:06:08.436092: step 43, loss 41.6943, acc 0.75, prec 0.0232158, recall 0.642857
2017-12-10T05:06:08.702567: step 44, loss 3.13683, acc 0.734375, prec 0.0229008, recall 0.627907
2017-12-10T05:06:08.975517: step 45, loss 1.73354, acc 0.609375, prec 0.0232365, recall 0.636364
2017-12-10T05:06:09.243672: step 46, loss 13.8958, acc 0.59375, prec 0.0227642, recall 0.622222
2017-12-10T05:06:09.517792: step 47, loss 4.39407, acc 0.65625, prec 0.0223821, recall 0.608696
2017-12-10T05:06:09.793258: step 48, loss 3.62315, acc 0.40625, prec 0.0217223, recall 0.608696
2017-12-10T05:06:10.054805: step 49, loss 2.90695, acc 0.46875, prec 0.021164, recall 0.608696
2017-12-10T05:06:10.323606: step 50, loss 4.14942, acc 0.390625, prec 0.0212766, recall 0.617021
2017-12-10T05:06:10.584794: step 51, loss 3.10111, acc 0.453125, prec 0.0207439, recall 0.617021
2017-12-10T05:06:10.845837: step 52, loss 3.28044, acc 0.453125, prec 0.0216028, recall 0.632653
2017-12-10T05:06:11.107814: step 53, loss 2.61585, acc 0.515625, prec 0.0218132, recall 0.64
2017-12-10T05:06:11.371030: step 54, loss 4.36834, acc 0.375, prec 0.0212342, recall 0.64
2017-12-10T05:06:11.632585: step 55, loss 2.75051, acc 0.59375, prec 0.0215124, recall 0.647059
2017-12-10T05:06:11.903446: step 56, loss 2.03473, acc 0.53125, prec 0.0223499, recall 0.660377
2017-12-10T05:06:12.172794: step 57, loss 2.32987, acc 0.5625, prec 0.0225705, recall 0.666667
2017-12-10T05:06:12.440536: step 58, loss 32.2362, acc 0.703125, prec 0.0229244, recall 0.660714
2017-12-10T05:06:12.714627: step 59, loss 1.21934, acc 0.796875, prec 0.0233415, recall 0.666667
2017-12-10T05:06:12.983439: step 60, loss 1.2766, acc 0.75, prec 0.0237082, recall 0.672414
2017-12-10T05:06:13.247304: step 61, loss 3.58248, acc 0.765625, prec 0.0252708, recall 0.677419
2017-12-10T05:06:13.523302: step 62, loss 1.06842, acc 0.75, prec 0.0250298, recall 0.677419
2017-12-10T05:06:13.788093: step 63, loss 1.8726, acc 0.765625, prec 0.0253837, recall 0.68254
2017-12-10T05:06:14.055771: step 64, loss 1.34117, acc 0.6875, prec 0.0250875, recall 0.68254
2017-12-10T05:06:14.316667: step 65, loss 1.4569, acc 0.65625, prec 0.0247696, recall 0.68254
2017-12-10T05:06:14.587729: step 66, loss 0.627265, acc 0.78125, prec 0.0245714, recall 0.68254
2017-12-10T05:06:14.859832: step 67, loss 23.8457, acc 0.71875, prec 0.0243488, recall 0.661538
2017-12-10T05:06:15.129579: step 68, loss 8.69391, acc 0.71875, prec 0.0241167, recall 0.651515
2017-12-10T05:06:15.392475: step 69, loss 1.49397, acc 0.734375, prec 0.0238889, recall 0.651515
2017-12-10T05:06:15.655259: step 70, loss 3.37041, acc 0.5, prec 0.0234716, recall 0.651515
2017-12-10T05:06:15.920822: step 71, loss 2.7733, acc 0.546875, prec 0.0231059, recall 0.651515
2017-12-10T05:06:16.181827: step 72, loss 3.32668, acc 0.421875, prec 0.0231701, recall 0.656716
2017-12-10T05:06:16.440091: step 73, loss 9.94915, acc 0.421875, prec 0.0237481, recall 0.657143
2017-12-10T05:06:16.707588: step 74, loss 3.43194, acc 0.375, prec 0.0232676, recall 0.657143
2017-12-10T05:06:16.971844: step 75, loss 3.64918, acc 0.453125, prec 0.0233482, recall 0.661972
2017-12-10T05:06:17.236981: step 76, loss 4.25346, acc 0.3125, prec 0.0228488, recall 0.661972
2017-12-10T05:06:17.491504: step 77, loss 3.28907, acc 0.46875, prec 0.0224773, recall 0.661972
2017-12-10T05:06:17.754475: step 78, loss 3.52044, acc 0.515625, prec 0.0221489, recall 0.661972
2017-12-10T05:06:18.016727: step 79, loss 37.6594, acc 0.4375, prec 0.0217895, recall 0.652778
2017-12-10T05:06:18.279501: step 80, loss 2.59836, acc 0.546875, prec 0.0215005, recall 0.652778
2017-12-10T05:06:18.543579: step 81, loss 2.35644, acc 0.640625, prec 0.0212766, recall 0.652778
2017-12-10T05:06:18.806727: step 82, loss 2.12756, acc 0.65625, prec 0.0215054, recall 0.657534
2017-12-10T05:06:19.069875: step 83, loss 1.89996, acc 0.609375, prec 0.0217006, recall 0.662162
2017-12-10T05:06:19.333918: step 84, loss 4.82914, acc 0.78125, prec 0.0224373, recall 0.662338
2017-12-10T05:06:19.597996: step 85, loss 26.4168, acc 0.640625, prec 0.022658, recall 0.65
2017-12-10T05:06:19.864191: step 86, loss 2.51023, acc 0.578125, prec 0.0232358, recall 0.658537
2017-12-10T05:06:20.126241: step 87, loss 3.07969, acc 0.546875, prec 0.0241935, recall 0.670588
2017-12-10T05:06:20.391816: step 88, loss 3.04865, acc 0.5625, prec 0.0247276, recall 0.678161
2017-12-10T05:06:20.659446: step 89, loss 3.66331, acc 0.359375, prec 0.0243098, recall 0.678161
2017-12-10T05:06:20.922822: step 90, loss 6.81657, acc 0.3125, prec 0.0238866, recall 0.670455
2017-12-10T05:06:21.191187: step 91, loss 4.53955, acc 0.34375, prec 0.0238758, recall 0.674157
2017-12-10T05:06:21.447548: step 92, loss 4.55438, acc 0.28125, prec 0.0238281, recall 0.677778
2017-12-10T05:06:21.711420: step 93, loss 5.03313, acc 0.296875, prec 0.0237913, recall 0.681319
2017-12-10T05:06:21.970355: step 94, loss 4.99979, acc 0.40625, prec 0.0238185, recall 0.684783
2017-12-10T05:06:22.234691: step 95, loss 3.12219, acc 0.46875, prec 0.0238806, recall 0.688172
2017-12-10T05:06:22.494732: step 96, loss 3.5956, acc 0.453125, prec 0.0235727, recall 0.688172
2017-12-10T05:06:22.761076: step 97, loss 2.75355, acc 0.5, prec 0.0243636, recall 0.697917
2017-12-10T05:06:23.028579: step 98, loss 3.56694, acc 0.46875, prec 0.0244165, recall 0.701031
2017-12-10T05:06:23.295846: step 99, loss 3.23131, acc 0.53125, prec 0.0245028, recall 0.704082
2017-12-10T05:06:23.560392: step 100, loss 1.2588, acc 0.75, prec 0.0250529, recall 0.71
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-100

2017-12-10T05:06:24.866099: step 101, loss 0.826387, acc 0.765625, prec 0.024921, recall 0.71
2017-12-10T05:06:25.123484: step 102, loss 14.6361, acc 0.734375, prec 0.0247905, recall 0.696078
2017-12-10T05:06:25.396908: step 103, loss 1.20786, acc 0.734375, prec 0.0249827, recall 0.699029
2017-12-10T05:06:25.662525: step 104, loss 0.911773, acc 0.765625, prec 0.0248533, recall 0.699029
2017-12-10T05:06:25.933710: step 105, loss 1.76351, acc 0.8125, prec 0.0254208, recall 0.704762
2017-12-10T05:06:26.200406: step 106, loss 13.8567, acc 0.84375, prec 0.0256849, recall 0.694444
2017-12-10T05:06:26.471410: step 107, loss 1.12518, acc 0.71875, prec 0.0258591, recall 0.697248
2017-12-10T05:06:26.737791: step 108, loss 2.17528, acc 0.71875, prec 0.0260311, recall 0.7
2017-12-10T05:06:27.006318: step 109, loss 3.29199, acc 0.484375, prec 0.0260695, recall 0.702703
2017-12-10T05:06:27.288358: step 110, loss 2.83661, acc 0.515625, prec 0.0258022, recall 0.702703
2017-12-10T05:06:27.554389: step 111, loss 4.18644, acc 0.421875, prec 0.0254902, recall 0.702703
2017-12-10T05:06:27.829822: step 112, loss 3.56991, acc 0.421875, prec 0.0251857, recall 0.702703
2017-12-10T05:06:28.090182: step 113, loss 3.61232, acc 0.5, prec 0.0252396, recall 0.705357
2017-12-10T05:06:28.354932: step 114, loss 3.46945, acc 0.5, prec 0.0252924, recall 0.707965
2017-12-10T05:06:28.622914: step 115, loss 3.22961, acc 0.5625, prec 0.0250705, recall 0.707965
2017-12-10T05:06:28.891277: step 116, loss 3.46471, acc 0.5, prec 0.0251241, recall 0.710526
2017-12-10T05:06:29.159726: step 117, loss 3.29683, acc 0.5, prec 0.0251765, recall 0.713043
2017-12-10T05:06:29.426096: step 118, loss 26.045, acc 0.6875, prec 0.0250305, recall 0.706897
2017-12-10T05:06:29.703001: step 119, loss 6.92236, acc 0.578125, prec 0.0251287, recall 0.70339
2017-12-10T05:06:29.976886: step 120, loss 2.12452, acc 0.640625, prec 0.0249549, recall 0.70339
2017-12-10T05:06:30.244371: step 121, loss 2.1233, acc 0.65625, prec 0.0250821, recall 0.705882
2017-12-10T05:06:30.520252: step 122, loss 2.41229, acc 0.625, prec 0.0251927, recall 0.708333
2017-12-10T05:06:30.785615: step 123, loss 2.25775, acc 0.609375, prec 0.0250074, recall 0.708333
2017-12-10T05:06:31.053883: step 124, loss 2.59924, acc 0.640625, prec 0.0254089, recall 0.713115
2017-12-10T05:06:31.321961: step 125, loss 2.60441, acc 0.53125, prec 0.0251882, recall 0.713115
2017-12-10T05:06:31.583544: step 126, loss 19.3918, acc 0.6875, prec 0.0250504, recall 0.707317
2017-12-10T05:06:31.852509: step 127, loss 3.47548, acc 0.59375, prec 0.02515, recall 0.704
2017-12-10T05:06:32.119249: step 128, loss 2.4511, acc 0.609375, prec 0.0255247, recall 0.708661
2017-12-10T05:06:32.380787: step 129, loss 1.84075, acc 0.703125, prec 0.0256627, recall 0.710938
2017-12-10T05:06:32.644784: step 130, loss 2.81696, acc 0.6875, prec 0.026065, recall 0.715385
2017-12-10T05:06:32.915628: step 131, loss 14.1938, acc 0.578125, prec 0.0258765, recall 0.709924
2017-12-10T05:06:33.187475: step 132, loss 2.54941, acc 0.640625, prec 0.0259812, recall 0.712121
2017-12-10T05:06:33.456177: step 133, loss 2.03233, acc 0.6875, prec 0.0261061, recall 0.714286
2017-12-10T05:06:33.723347: step 134, loss 2.58771, acc 0.578125, prec 0.0259138, recall 0.714286
2017-12-10T05:06:33.988524: step 135, loss 2.18286, acc 0.6875, prec 0.0260374, recall 0.716418
2017-12-10T05:06:34.253006: step 136, loss 2.08154, acc 0.625, prec 0.025869, recall 0.716418
2017-12-10T05:06:34.513206: step 137, loss 5.09668, acc 0.625, prec 0.0259705, recall 0.713235
2017-12-10T05:06:34.778832: step 138, loss 2.10503, acc 0.703125, prec 0.0260985, recall 0.715328
2017-12-10T05:06:35.046648: step 139, loss 2.40415, acc 0.65625, prec 0.0262043, recall 0.717391
2017-12-10T05:06:35.307762: step 140, loss 1.77376, acc 0.640625, prec 0.0260458, recall 0.717391
2017-12-10T05:06:35.576994: step 141, loss 1.70223, acc 0.703125, prec 0.0259162, recall 0.717391
2017-12-10T05:06:35.847320: step 142, loss 2.99986, acc 0.59375, prec 0.0265004, recall 0.723404
2017-12-10T05:06:36.111250: step 143, loss 2.37858, acc 0.671875, prec 0.0263566, recall 0.723404
2017-12-10T05:06:36.378694: step 144, loss 1.57438, acc 0.734375, prec 0.0264918, recall 0.725352
2017-12-10T05:06:36.644749: step 145, loss 1.42423, acc 0.71875, prec 0.0263697, recall 0.725352
2017-12-10T05:06:36.912815: step 146, loss 6.96646, acc 0.59375, prec 0.026202, recall 0.72028
2017-12-10T05:06:37.181693: step 147, loss 1.74752, acc 0.734375, prec 0.0260892, recall 0.72028
2017-12-10T05:06:37.450038: step 148, loss 1.57396, acc 0.84375, prec 0.0262693, recall 0.722222
2017-12-10T05:06:37.714296: step 149, loss 1.45384, acc 0.734375, prec 0.0264018, recall 0.724138
2017-12-10T05:06:37.981853: step 150, loss 1.06967, acc 0.78125, prec 0.0267969, recall 0.727891
2017-12-10T05:06:38.253669: step 151, loss 23.7093, acc 0.734375, prec 0.0269394, recall 0.72
2017-12-10T05:06:38.526049: step 152, loss 1.51861, acc 0.75, prec 0.0268323, recall 0.72
2017-12-10T05:06:38.788769: step 153, loss 2.88162, acc 0.546875, prec 0.0268804, recall 0.721854
2017-12-10T05:06:39.052954: step 154, loss 2.40095, acc 0.546875, prec 0.0269278, recall 0.723684
2017-12-10T05:06:39.321104: step 155, loss 2.7108, acc 0.609375, prec 0.026764, recall 0.723684
2017-12-10T05:06:39.581931: step 156, loss 2.00877, acc 0.625, prec 0.0266086, recall 0.723684
2017-12-10T05:06:39.844043: step 157, loss 1.89716, acc 0.625, prec 0.0269231, recall 0.727273
2017-12-10T05:06:40.107359: step 158, loss 10.4108, acc 0.609375, prec 0.0267686, recall 0.722581
2017-12-10T05:06:40.373664: step 159, loss 3.01395, acc 0.609375, prec 0.0268409, recall 0.724359
2017-12-10T05:06:40.631017: step 160, loss 1.85112, acc 0.609375, prec 0.0271418, recall 0.727848
2017-12-10T05:06:40.892736: step 161, loss 3.05514, acc 0.421875, prec 0.0269069, recall 0.727848
2017-12-10T05:06:41.160374: step 162, loss 4.05137, acc 0.390625, prec 0.0266636, recall 0.727848
2017-12-10T05:06:41.431785: step 163, loss 1.4258, acc 0.59375, prec 0.0267281, recall 0.72956
2017-12-10T05:06:41.693035: step 164, loss 1.47038, acc 0.640625, prec 0.0268103, recall 0.73125
2017-12-10T05:06:41.956170: step 165, loss 1.56152, acc 0.71875, prec 0.0267001, recall 0.73125
2017-12-10T05:06:42.232699: step 166, loss 1.51887, acc 0.75, prec 0.0270455, recall 0.734568
2017-12-10T05:06:42.499313: step 167, loss 1.35284, acc 0.703125, prec 0.0269292, recall 0.734568
2017-12-10T05:06:42.762739: step 168, loss 5.8749, acc 0.75, prec 0.0270575, recall 0.731707
2017-12-10T05:06:43.029799: step 169, loss 2.09123, acc 0.765625, prec 0.0269724, recall 0.727273
2017-12-10T05:06:43.299708: step 170, loss 0.892764, acc 0.8125, prec 0.0271179, recall 0.728916
2017-12-10T05:06:43.562812: step 171, loss 1.21915, acc 0.703125, prec 0.0270029, recall 0.728916
2017-12-10T05:06:43.826657: step 172, loss 1.16037, acc 0.6875, prec 0.0268829, recall 0.728916
2017-12-10T05:06:44.088294: step 173, loss 1.27401, acc 0.75, prec 0.0267877, recall 0.728916
2017-12-10T05:06:44.353975: step 174, loss 1.03955, acc 0.78125, prec 0.0267049, recall 0.728916
2017-12-10T05:06:44.612256: step 175, loss 7.97481, acc 0.84375, prec 0.0268663, recall 0.72619
2017-12-10T05:06:44.879539: step 176, loss 0.739867, acc 0.8125, prec 0.0267955, recall 0.72619
2017-12-10T05:06:45.142835: step 177, loss 0.51399, acc 0.828125, prec 0.0269441, recall 0.727811
2017-12-10T05:06:45.421575: step 178, loss 8.87735, acc 0.796875, prec 0.0268735, recall 0.723529
2017-12-10T05:06:45.685042: step 179, loss 18.5059, acc 0.78125, prec 0.0270094, recall 0.72093
2017-12-10T05:06:45.957791: step 180, loss 5.33195, acc 0.71875, prec 0.0269097, recall 0.716763
2017-12-10T05:06:46.221483: step 181, loss 2.32959, acc 0.546875, prec 0.0267414, recall 0.716763
2017-12-10T05:06:46.490371: step 182, loss 2.09513, acc 0.578125, prec 0.0265866, recall 0.716763
2017-12-10T05:06:46.758396: step 183, loss 2.82285, acc 0.546875, prec 0.0270443, recall 0.721591
2017-12-10T05:06:47.021424: step 184, loss 3.2728, acc 0.5, prec 0.0268613, recall 0.721591
2017-12-10T05:06:47.288659: step 185, loss 3.08781, acc 0.453125, prec 0.0268682, recall 0.723164
2017-12-10T05:06:47.547772: step 186, loss 3.53451, acc 0.484375, prec 0.0266833, recall 0.723164
2017-12-10T05:06:47.811290: step 187, loss 4.14225, acc 0.46875, prec 0.026697, recall 0.724719
2017-12-10T05:06:48.068080: step 188, loss 3.22286, acc 0.421875, prec 0.0264941, recall 0.724719
2017-12-10T05:06:48.332636: step 189, loss 9.90024, acc 0.484375, prec 0.0263212, recall 0.72067
2017-12-10T05:06:48.600718: step 190, loss 2.76847, acc 0.609375, prec 0.0265828, recall 0.723757
2017-12-10T05:06:48.867204: step 191, loss 3.15064, acc 0.5, prec 0.0268037, recall 0.726776
2017-12-10T05:06:49.131262: step 192, loss 3.23482, acc 0.515625, prec 0.0266373, recall 0.726776
2017-12-10T05:06:49.397653: step 193, loss 2.88988, acc 0.484375, prec 0.0264624, recall 0.726776
2017-12-10T05:06:49.670957: step 194, loss 2.11606, acc 0.546875, prec 0.0265032, recall 0.728261
2017-12-10T05:06:49.932868: step 195, loss 2.35628, acc 0.671875, prec 0.0263935, recall 0.728261
2017-12-10T05:06:50.196265: step 196, loss 2.33345, acc 0.671875, prec 0.0266667, recall 0.731183
2017-12-10T05:06:50.460779: step 197, loss 0.821342, acc 0.78125, prec 0.026784, recall 0.73262
2017-12-10T05:06:50.724786: step 198, loss 1.32617, acc 0.71875, prec 0.0270691, recall 0.73545
2017-12-10T05:06:50.986292: step 199, loss 0.54943, acc 0.90625, prec 0.0270375, recall 0.73545
2017-12-10T05:06:51.254458: step 200, loss 0.803697, acc 0.828125, prec 0.0271686, recall 0.736842
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-200

2017-12-10T05:06:52.512856: step 201, loss 0.316122, acc 0.9375, prec 0.0273362, recall 0.73822
2017-12-10T05:06:52.780043: step 202, loss 0.715508, acc 0.796875, prec 0.0272675, recall 0.73822
2017-12-10T05:06:53.052458: step 203, loss 0.230485, acc 0.921875, prec 0.0272411, recall 0.73822
2017-12-10T05:06:53.318104: step 204, loss 0.200727, acc 0.90625, prec 0.0273973, recall 0.739583
2017-12-10T05:06:53.586003: step 205, loss 0.33693, acc 0.875, prec 0.027355, recall 0.739583
2017-12-10T05:06:53.854775: step 206, loss 67.5151, acc 0.890625, prec 0.0273392, recall 0.72449
2017-12-10T05:06:54.127323: step 207, loss 17.2906, acc 0.90625, prec 0.0275106, recall 0.715
2017-12-10T05:06:54.393306: step 208, loss 1.16721, acc 0.78125, prec 0.0274367, recall 0.715
2017-12-10T05:06:54.660222: step 209, loss 1.17853, acc 0.78125, prec 0.0275493, recall 0.716418
2017-12-10T05:06:54.920606: step 210, loss 3.07642, acc 0.515625, prec 0.0275718, recall 0.717822
2017-12-10T05:06:55.183838: step 211, loss 2.42645, acc 0.625, prec 0.0274465, recall 0.717822
2017-12-10T05:06:55.449424: step 212, loss 5.59943, acc 0.375, prec 0.0272403, recall 0.717822
2017-12-10T05:06:55.707545: step 213, loss 5.53561, acc 0.28125, prec 0.0273692, recall 0.720588
2017-12-10T05:06:55.967721: step 214, loss 7.31306, acc 0.1875, prec 0.0271068, recall 0.720588
2017-12-10T05:06:56.226088: step 215, loss 8.08305, acc 0.1875, prec 0.027027, recall 0.721951
2017-12-10T05:06:56.483814: step 216, loss 6.91007, acc 0.25, prec 0.0269683, recall 0.723301
2017-12-10T05:06:56.748553: step 217, loss 6.90615, acc 0.25, prec 0.0270852, recall 0.725962
2017-12-10T05:06:57.019492: step 218, loss 7.46497, acc 0.25, prec 0.0273729, recall 0.729858
2017-12-10T05:06:57.284056: step 219, loss 6.30707, acc 0.265625, prec 0.0271461, recall 0.729858
2017-12-10T05:06:57.544415: step 220, loss 5.23423, acc 0.34375, prec 0.0271169, recall 0.731132
2017-12-10T05:06:57.804868: step 221, loss 4.57046, acc 0.359375, prec 0.0269237, recall 0.731132
2017-12-10T05:06:58.073350: step 222, loss 4.30934, acc 0.515625, prec 0.0271157, recall 0.733645
2017-12-10T05:06:58.339036: step 223, loss 2.53641, acc 0.5, prec 0.0269667, recall 0.733645
2017-12-10T05:06:58.612858: step 224, loss 14.5574, acc 0.6875, prec 0.026879, recall 0.730233
2017-12-10T05:06:58.872745: step 225, loss 2.07141, acc 0.734375, prec 0.0269671, recall 0.731481
2017-12-10T05:06:59.138732: step 226, loss 6.04736, acc 0.796875, prec 0.0269119, recall 0.728111
2017-12-10T05:06:59.401209: step 227, loss 2.07698, acc 0.65625, prec 0.0268115, recall 0.728111
2017-12-10T05:06:59.674575: step 228, loss 1.51555, acc 0.75, prec 0.0267389, recall 0.728111
2017-12-10T05:06:59.938849: step 229, loss 2.23365, acc 0.75, prec 0.0268309, recall 0.729358
2017-12-10T05:07:00.204566: step 230, loss 5.89745, acc 0.65625, prec 0.0268998, recall 0.727273
2017-12-10T05:07:00.478300: step 231, loss 9.18519, acc 0.671875, prec 0.0268097, recall 0.723982
2017-12-10T05:07:00.752458: step 232, loss 1.75178, acc 0.640625, prec 0.0267067, recall 0.723982
2017-12-10T05:07:01.013113: step 233, loss 2.33901, acc 0.59375, prec 0.0267531, recall 0.725225
2017-12-10T05:07:01.285413: step 234, loss 3.12006, acc 0.59375, prec 0.02696, recall 0.727679
2017-12-10T05:07:01.552930: step 235, loss 3.27468, acc 0.578125, prec 0.0271605, recall 0.730088
2017-12-10T05:07:01.812789: step 236, loss 2.79042, acc 0.53125, prec 0.0271864, recall 0.731278
2017-12-10T05:07:02.072764: step 237, loss 3.34136, acc 0.5625, prec 0.0272209, recall 0.732456
2017-12-10T05:07:02.337592: step 238, loss 3.81737, acc 0.625, prec 0.0272727, recall 0.733624
2017-12-10T05:07:02.606526: step 239, loss 3.68537, acc 0.5, prec 0.0272889, recall 0.734783
2017-12-10T05:07:02.882546: step 240, loss 3.53541, acc 0.546875, prec 0.027318, recall 0.735931
2017-12-10T05:07:03.139743: step 241, loss 2.15478, acc 0.578125, prec 0.0273556, recall 0.737069
2017-12-10T05:07:03.407559: step 242, loss 2.13458, acc 0.59375, prec 0.0273973, recall 0.738197
2017-12-10T05:07:03.673150: step 243, loss 1.23449, acc 0.703125, prec 0.0273146, recall 0.738197
2017-12-10T05:07:03.939231: step 244, loss 1.2685, acc 0.6875, prec 0.0272281, recall 0.738197
2017-12-10T05:07:04.214046: step 245, loss 0.964692, acc 0.78125, prec 0.0271679, recall 0.738197
2017-12-10T05:07:04.476485: step 246, loss 0.838999, acc 0.796875, prec 0.0272656, recall 0.739316
2017-12-10T05:07:04.738171: step 247, loss 0.31264, acc 0.875, prec 0.0272312, recall 0.739316
2017-12-10T05:07:05.005336: step 248, loss 0.684317, acc 0.875, prec 0.0275028, recall 0.741525
2017-12-10T05:07:05.277535: step 249, loss 15.1531, acc 0.921875, prec 0.0274855, recall 0.738397
2017-12-10T05:07:05.545491: step 250, loss 0.633954, acc 0.921875, prec 0.0279216, recall 0.741667
2017-12-10T05:07:05.816461: step 251, loss 0.536881, acc 0.90625, prec 0.0278953, recall 0.741667
2017-12-10T05:07:06.082985: step 252, loss 15.6631, acc 0.84375, prec 0.027856, recall 0.738589
2017-12-10T05:07:06.353845: step 253, loss 1.11642, acc 0.8125, prec 0.0278038, recall 0.738589
2017-12-10T05:07:06.618364: step 254, loss 1.04678, acc 0.78125, prec 0.0277431, recall 0.738589
2017-12-10T05:07:06.883223: step 255, loss 0.949982, acc 0.859375, prec 0.0277043, recall 0.738589
2017-12-10T05:07:07.145880: step 256, loss 0.386654, acc 0.875, prec 0.0276698, recall 0.738589
2017-12-10T05:07:07.410900: step 257, loss 1.12398, acc 0.765625, prec 0.0276055, recall 0.738589
2017-12-10T05:07:07.674642: step 258, loss 16.1503, acc 0.796875, prec 0.0275542, recall 0.735537
2017-12-10T05:07:07.939174: step 259, loss 9.07024, acc 0.84375, prec 0.0276662, recall 0.733607
2017-12-10T05:07:08.203366: step 260, loss 1.09695, acc 0.765625, prec 0.0276022, recall 0.733607
2017-12-10T05:07:08.470438: step 261, loss 1.05393, acc 0.703125, prec 0.0275215, recall 0.733607
2017-12-10T05:07:08.731184: step 262, loss 2.96965, acc 0.6875, prec 0.0280331, recall 0.737903
2017-12-10T05:07:09.002185: step 263, loss 1.95055, acc 0.703125, prec 0.0281002, recall 0.738956
2017-12-10T05:07:09.268351: step 264, loss 1.95882, acc 0.578125, prec 0.0281326, recall 0.74
2017-12-10T05:07:09.534358: step 265, loss 2.04124, acc 0.640625, prec 0.0281818, recall 0.741036
2017-12-10T05:07:09.796352: step 266, loss 2.77636, acc 0.625, prec 0.0283731, recall 0.743083
2017-12-10T05:07:10.070957: step 267, loss 2.22541, acc 0.625, prec 0.0285628, recall 0.745098
2017-12-10T05:07:10.334544: step 268, loss 2.38337, acc 0.65625, prec 0.0286142, recall 0.746094
2017-12-10T05:07:10.593356: step 269, loss 1.91611, acc 0.640625, prec 0.028806, recall 0.748062
2017-12-10T05:07:10.860004: step 270, loss 1.9179, acc 0.609375, prec 0.0286989, recall 0.748062
2017-12-10T05:07:11.121826: step 271, loss 1.97161, acc 0.65625, prec 0.0288932, recall 0.75
2017-12-10T05:07:11.384542: step 272, loss 1.49661, acc 0.6875, prec 0.0288078, recall 0.75
2017-12-10T05:07:11.645558: step 273, loss 1.32826, acc 0.75, prec 0.0287399, recall 0.75
2017-12-10T05:07:11.911723: step 274, loss 1.29146, acc 0.734375, prec 0.0288108, recall 0.750958
2017-12-10T05:07:12.191015: step 275, loss 1.3351, acc 0.796875, prec 0.0288983, recall 0.751908
2017-12-10T05:07:12.458274: step 276, loss 0.550986, acc 0.875, prec 0.0290067, recall 0.752852
2017-12-10T05:07:12.728220: step 277, loss 0.321584, acc 0.890625, prec 0.0292612, recall 0.754717
2017-12-10T05:07:12.988771: step 278, loss 12.1353, acc 0.8125, prec 0.0292227, recall 0.746269
2017-12-10T05:07:13.258434: step 279, loss 0.564633, acc 0.890625, prec 0.0291928, recall 0.746269
2017-12-10T05:07:13.518393: step 280, loss 0.633145, acc 0.828125, prec 0.0292875, recall 0.747212
2017-12-10T05:07:13.793764: step 281, loss 1.44539, acc 0.65625, prec 0.0294758, recall 0.749077
2017-12-10T05:07:14.056095: step 282, loss 1.28909, acc 0.734375, prec 0.0295438, recall 0.75
2017-12-10T05:07:14.318716: step 283, loss 1.61438, acc 0.59375, prec 0.029573, recall 0.750916
2017-12-10T05:07:14.580186: step 284, loss 1.15483, acc 0.71875, prec 0.0294964, recall 0.750916
2017-12-10T05:07:14.843223: step 285, loss 2.14402, acc 0.65625, prec 0.0295425, recall 0.751825
2017-12-10T05:07:15.103672: step 286, loss 1.97662, acc 0.671875, prec 0.0294538, recall 0.751825
2017-12-10T05:07:15.368940: step 287, loss 9.70269, acc 0.59375, prec 0.0293489, recall 0.749091
2017-12-10T05:07:15.634397: step 288, loss 2.17405, acc 0.546875, prec 0.0295035, recall 0.750903
2017-12-10T05:07:15.900230: step 289, loss 2.4045, acc 0.5, prec 0.0293702, recall 0.750903
2017-12-10T05:07:16.158699: step 290, loss 2.59163, acc 0.484375, prec 0.0293704, recall 0.751799
2017-12-10T05:07:16.424680: step 291, loss 2.56615, acc 0.484375, prec 0.0292349, recall 0.751799
2017-12-10T05:07:16.693592: step 292, loss 1.7281, acc 0.625, prec 0.029137, recall 0.751799
2017-12-10T05:07:16.960617: step 293, loss 1.70056, acc 0.640625, prec 0.0290439, recall 0.751799
2017-12-10T05:07:17.219474: step 294, loss 7.22468, acc 0.65625, prec 0.0292284, recall 0.75089
2017-12-10T05:07:17.482572: step 295, loss 1.23464, acc 0.75, prec 0.029298, recall 0.751773
2017-12-10T05:07:17.747572: step 296, loss 1.5823, acc 0.640625, prec 0.0292051, recall 0.751773
2017-12-10T05:07:18.012006: step 297, loss 4.07351, acc 0.59375, prec 0.0291049, recall 0.749117
2017-12-10T05:07:18.280820: step 298, loss 1.70192, acc 0.703125, prec 0.0290292, recall 0.749117
2017-12-10T05:07:18.545773: step 299, loss 1.69762, acc 0.671875, prec 0.0289459, recall 0.749117
2017-12-10T05:07:18.808805: step 300, loss 1.59797, acc 0.734375, prec 0.0290112, recall 0.75

Evaluation:
2017-12-10T05:07:26.380622: step 300, loss 1.85539, acc 0.786092, prec 0.0335813, recall 0.740319

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-300

2017-12-10T05:07:27.631111: step 301, loss 5.70248, acc 0.703125, prec 0.033519, recall 0.738636
2017-12-10T05:07:27.898216: step 302, loss 1.15151, acc 0.75, prec 0.0335633, recall 0.739229
2017-12-10T05:07:28.169044: step 303, loss 1.79824, acc 0.6875, prec 0.0334943, recall 0.739229
2017-12-10T05:07:28.431605: step 304, loss 12.613, acc 0.71875, prec 0.033535, recall 0.738149
2017-12-10T05:07:28.696534: step 305, loss 1.54909, acc 0.6875, prec 0.0335653, recall 0.738739
2017-12-10T05:07:28.965757: step 306, loss 2.27728, acc 0.5625, prec 0.0334694, recall 0.738739
2017-12-10T05:07:29.234601: step 307, loss 1.17618, acc 0.6875, prec 0.0334012, recall 0.738739
2017-12-10T05:07:29.505735: step 308, loss 1.36847, acc 0.703125, prec 0.033435, recall 0.739326
2017-12-10T05:07:29.770911: step 309, loss 4.13607, acc 0.640625, prec 0.0333604, recall 0.737668
2017-12-10T05:07:30.036656: step 310, loss 2.51869, acc 0.53125, prec 0.0332592, recall 0.737668
2017-12-10T05:07:30.305559: step 311, loss 4.38892, acc 0.5625, prec 0.0332661, recall 0.736607
2017-12-10T05:07:30.574033: step 312, loss 2.0543, acc 0.59375, prec 0.0333735, recall 0.737778
2017-12-10T05:07:30.838713: step 313, loss 2.16377, acc 0.53125, prec 0.0334669, recall 0.738938
2017-12-10T05:07:31.095792: step 314, loss 2.36676, acc 0.578125, prec 0.0334732, recall 0.739514
2017-12-10T05:07:31.367590: step 315, loss 1.71774, acc 0.59375, prec 0.0333865, recall 0.739514
2017-12-10T05:07:31.627155: step 316, loss 2.51553, acc 0.5625, prec 0.0332936, recall 0.739514
2017-12-10T05:07:31.896134: step 317, loss 1.03001, acc 0.78125, prec 0.0332473, recall 0.739514
2017-12-10T05:07:32.161879: step 318, loss 3.72296, acc 0.703125, prec 0.0331913, recall 0.736264
2017-12-10T05:07:32.424883: step 319, loss 1.52271, acc 0.6875, prec 0.0332213, recall 0.736842
2017-12-10T05:07:32.689744: step 320, loss 7.37223, acc 0.546875, prec 0.0331296, recall 0.73523
2017-12-10T05:07:32.962520: step 321, loss 1.46992, acc 0.6875, prec 0.0330644, recall 0.73523
2017-12-10T05:07:33.220680: step 322, loss 2.37087, acc 0.65625, prec 0.0331828, recall 0.736383
2017-12-10T05:07:33.482843: step 323, loss 1.77077, acc 0.71875, prec 0.0333137, recall 0.737527
2017-12-10T05:07:33.749859: step 324, loss 1.58632, acc 0.59375, prec 0.0332291, recall 0.737527
2017-12-10T05:07:34.011128: step 325, loss 2.19094, acc 0.578125, prec 0.0331416, recall 0.737527
2017-12-10T05:07:34.285650: step 326, loss 2.13986, acc 0.625, prec 0.0330643, recall 0.737527
2017-12-10T05:07:34.552971: step 327, loss 1.07875, acc 0.71875, prec 0.0330065, recall 0.737527
2017-12-10T05:07:34.816849: step 328, loss 2.28156, acc 0.6875, prec 0.0331299, recall 0.738661
2017-12-10T05:07:35.079289: step 329, loss 1.01777, acc 0.796875, prec 0.0330882, recall 0.738661
2017-12-10T05:07:35.344863: step 330, loss 0.864579, acc 0.828125, prec 0.0330531, recall 0.738661
2017-12-10T05:07:35.610760: step 331, loss 0.84409, acc 0.875, prec 0.0331209, recall 0.739224
2017-12-10T05:07:35.873373: step 332, loss 1.77409, acc 0.8125, prec 0.0331758, recall 0.739785
2017-12-10T05:07:36.146061: step 333, loss 0.26822, acc 0.921875, prec 0.0331598, recall 0.739785
2017-12-10T05:07:36.417052: step 334, loss 0.844853, acc 0.78125, prec 0.0331151, recall 0.739785
2017-12-10T05:07:36.678431: step 335, loss 1.17661, acc 0.75, prec 0.0330642, recall 0.739785
2017-12-10T05:07:36.948900: step 336, loss 14.157, acc 0.75, prec 0.0330166, recall 0.738197
2017-12-10T05:07:37.213071: step 337, loss 12.4032, acc 0.765625, prec 0.0329723, recall 0.736617
2017-12-10T05:07:37.484737: step 338, loss 0.57438, acc 0.828125, prec 0.0329376, recall 0.736617
2017-12-10T05:07:37.755059: step 339, loss 1.03482, acc 0.71875, prec 0.0328809, recall 0.736617
2017-12-10T05:07:38.018538: step 340, loss 1.40366, acc 0.71875, prec 0.0328244, recall 0.736617
2017-12-10T05:07:38.287682: step 341, loss 0.958983, acc 0.734375, prec 0.0327713, recall 0.736617
2017-12-10T05:07:38.554816: step 342, loss 1.27279, acc 0.6875, prec 0.0328009, recall 0.737179
2017-12-10T05:07:38.820259: step 343, loss 1.46717, acc 0.71875, prec 0.0330202, recall 0.738854
2017-12-10T05:07:39.082084: step 344, loss 6.45616, acc 0.75, prec 0.0329733, recall 0.737288
2017-12-10T05:07:39.361639: step 345, loss 3.97178, acc 0.6875, prec 0.0330055, recall 0.736287
2017-12-10T05:07:39.630025: step 346, loss 2.1703, acc 0.59375, prec 0.0330158, recall 0.736842
2017-12-10T05:07:39.890541: step 347, loss 4.83307, acc 0.5625, prec 0.0330229, recall 0.735849
2017-12-10T05:07:40.159859: step 348, loss 3.45033, acc 0.515625, prec 0.0329268, recall 0.735849
2017-12-10T05:07:40.419377: step 349, loss 4.04611, acc 0.421875, prec 0.0329937, recall 0.736952
2017-12-10T05:07:40.685381: step 350, loss 3.53331, acc 0.375, prec 0.0329609, recall 0.7375
2017-12-10T05:07:40.951936: step 351, loss 3.97976, acc 0.4375, prec 0.0329405, recall 0.738046
2017-12-10T05:07:41.219045: step 352, loss 4.21352, acc 0.4375, prec 0.0328309, recall 0.738046
2017-12-10T05:07:41.480795: step 353, loss 3.56485, acc 0.453125, prec 0.0327249, recall 0.738046
2017-12-10T05:07:41.746463: step 354, loss 3.52462, acc 0.390625, prec 0.0327854, recall 0.73913
2017-12-10T05:07:42.013356: step 355, loss 4.58098, acc 0.484375, prec 0.0328664, recall 0.738683
2017-12-10T05:07:42.287553: step 356, loss 2.07496, acc 0.609375, prec 0.032968, recall 0.739754
2017-12-10T05:07:42.551300: step 357, loss 1.79348, acc 0.640625, prec 0.0328989, recall 0.739754
2017-12-10T05:07:42.811817: step 358, loss 1.84459, acc 0.65625, prec 0.0328331, recall 0.739754
2017-12-10T05:07:43.073875: step 359, loss 1.06771, acc 0.765625, prec 0.0328762, recall 0.740286
2017-12-10T05:07:43.329342: step 360, loss 1.28633, acc 0.75, prec 0.0330916, recall 0.74187
2017-12-10T05:07:43.592349: step 361, loss 6.10177, acc 0.734375, prec 0.0330436, recall 0.740365
2017-12-10T05:07:43.862462: step 362, loss 2.09783, acc 0.734375, prec 0.0330803, recall 0.740891
2017-12-10T05:07:44.123364: step 363, loss 9.0388, acc 0.765625, prec 0.0330385, recall 0.739394
2017-12-10T05:07:44.386768: step 364, loss 11.806, acc 0.75, prec 0.0330839, recall 0.736948
2017-12-10T05:07:44.652618: step 365, loss 1.56587, acc 0.609375, prec 0.0331835, recall 0.738
2017-12-10T05:07:44.920200: step 366, loss 2.10819, acc 0.609375, prec 0.0331958, recall 0.738523
2017-12-10T05:07:45.183994: step 367, loss 2.64847, acc 0.484375, prec 0.0330978, recall 0.738523
2017-12-10T05:07:45.443459: step 368, loss 3.52951, acc 0.46875, prec 0.0330836, recall 0.739044
2017-12-10T05:07:45.713715: step 369, loss 2.76639, acc 0.46875, prec 0.0329836, recall 0.739044
2017-12-10T05:07:45.977697: step 370, loss 4.43495, acc 0.46875, prec 0.0330557, recall 0.740079
2017-12-10T05:07:46.243087: step 371, loss 3.23503, acc 0.4375, prec 0.033036, recall 0.740594
2017-12-10T05:07:46.506601: step 372, loss 3.57708, acc 0.515625, prec 0.0330309, recall 0.741107
2017-12-10T05:07:46.776490: step 373, loss 3.18141, acc 0.46875, prec 0.0330172, recall 0.741617
2017-12-10T05:07:47.044199: step 374, loss 2.50794, acc 0.53125, prec 0.0329305, recall 0.741617
2017-12-10T05:07:47.311675: step 375, loss 4.95232, acc 0.578125, prec 0.0328556, recall 0.740157
2017-12-10T05:07:47.576655: step 376, loss 3.12053, acc 0.484375, prec 0.0328454, recall 0.740668
2017-12-10T05:07:47.846845: step 377, loss 4.48587, acc 0.515625, prec 0.0328439, recall 0.739726
2017-12-10T05:07:48.108619: step 378, loss 3.18589, acc 0.5, prec 0.0329204, recall 0.740741
2017-12-10T05:07:48.375881: step 379, loss 2.69699, acc 0.4375, prec 0.0329016, recall 0.741245
2017-12-10T05:07:48.638274: step 380, loss 2.58374, acc 0.4375, prec 0.0327996, recall 0.741245
2017-12-10T05:07:48.902872: step 381, loss 2.26905, acc 0.609375, prec 0.0328122, recall 0.741748
2017-12-10T05:07:49.170031: step 382, loss 2.074, acc 0.71875, prec 0.0328445, recall 0.742248
2017-12-10T05:07:49.441507: step 383, loss 1.64826, acc 0.671875, prec 0.0328683, recall 0.742747
2017-12-10T05:07:49.712574: step 384, loss 1.8837, acc 0.65625, prec 0.0329717, recall 0.743738
2017-12-10T05:07:49.972935: step 385, loss 1.36067, acc 0.671875, prec 0.0329127, recall 0.743738
2017-12-10T05:07:50.247607: step 386, loss 6.08657, acc 0.796875, prec 0.0329614, recall 0.742802
2017-12-10T05:07:50.522385: step 387, loss 2.64284, acc 0.765625, prec 0.0330044, recall 0.741874
2017-12-10T05:07:50.788131: step 388, loss 1.19259, acc 0.6875, prec 0.0329484, recall 0.741874
2017-12-10T05:07:51.053298: step 389, loss 1.11366, acc 0.765625, prec 0.0330705, recall 0.742857
2017-12-10T05:07:51.316645: step 390, loss 2.05167, acc 0.65625, prec 0.0330907, recall 0.743346
2017-12-10T05:07:51.586245: step 391, loss 0.72623, acc 0.796875, prec 0.0331361, recall 0.743833
2017-12-10T05:07:51.849834: step 392, loss 1.19416, acc 0.796875, prec 0.0330997, recall 0.743833
2017-12-10T05:07:52.110770: step 393, loss 18.1453, acc 0.765625, prec 0.0331422, recall 0.742911
2017-12-10T05:07:52.374424: step 394, loss 1.23394, acc 0.703125, prec 0.0330892, recall 0.742911
2017-12-10T05:07:52.636397: step 395, loss 1.25643, acc 0.734375, prec 0.0331232, recall 0.743396
2017-12-10T05:07:52.914050: step 396, loss 7.01945, acc 0.640625, prec 0.033062, recall 0.741996
2017-12-10T05:07:53.184232: step 397, loss 1.25, acc 0.6875, prec 0.0330066, recall 0.741996
2017-12-10T05:07:53.455582: step 398, loss 1.77823, acc 0.671875, prec 0.0329487, recall 0.741996
2017-12-10T05:07:53.716597: step 399, loss 1.84033, acc 0.625, prec 0.0330441, recall 0.742964
2017-12-10T05:07:53.988295: step 400, loss 9.53582, acc 0.46875, prec 0.0329533, recall 0.741573
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-400

2017-12-10T05:07:55.197299: step 401, loss 2.18118, acc 0.625, prec 0.0331285, recall 0.743017
2017-12-10T05:07:55.457445: step 402, loss 2.69131, acc 0.5, prec 0.0331208, recall 0.743494
2017-12-10T05:07:55.721184: step 403, loss 2.37341, acc 0.515625, prec 0.033036, recall 0.743494
2017-12-10T05:07:55.995630: step 404, loss 3.36094, acc 0.375, prec 0.033166, recall 0.744917
2017-12-10T05:07:56.260000: step 405, loss 4.83866, acc 0.546875, prec 0.0331691, recall 0.744015
2017-12-10T05:07:56.530124: step 406, loss 2.27705, acc 0.515625, prec 0.0330849, recall 0.744015
2017-12-10T05:07:56.796884: step 407, loss 2.57, acc 0.515625, prec 0.0330011, recall 0.744015
2017-12-10T05:07:57.058336: step 408, loss 2.85636, acc 0.546875, prec 0.033002, recall 0.744485
2017-12-10T05:07:57.331676: step 409, loss 2.20034, acc 0.5, prec 0.0329161, recall 0.744485
2017-12-10T05:07:57.602813: step 410, loss 4.46562, acc 0.546875, prec 0.0328414, recall 0.743119
2017-12-10T05:07:57.873165: step 411, loss 1.70596, acc 0.546875, prec 0.0327643, recall 0.743119
2017-12-10T05:07:58.142013: step 412, loss 9.23995, acc 0.546875, prec 0.0328464, recall 0.742701
2017-12-10T05:07:58.418183: step 413, loss 1.51219, acc 0.671875, prec 0.0328688, recall 0.743169
2017-12-10T05:07:58.678740: step 414, loss 1.81149, acc 0.671875, prec 0.0331243, recall 0.745027
2017-12-10T05:07:58.939528: step 415, loss 2.21836, acc 0.609375, prec 0.0330578, recall 0.745027
2017-12-10T05:07:59.200964: step 416, loss 2.3674, acc 0.53125, prec 0.0331333, recall 0.745946
2017-12-10T05:07:59.463092: step 417, loss 1.55751, acc 0.671875, prec 0.0331549, recall 0.746403
2017-12-10T05:07:59.728778: step 418, loss 1.64691, acc 0.59375, prec 0.0332403, recall 0.747312
2017-12-10T05:08:00.001516: step 419, loss 1.48856, acc 0.671875, prec 0.0331848, recall 0.747312
2017-12-10T05:08:00.264653: step 420, loss 1.28881, acc 0.75, prec 0.0332194, recall 0.747764
2017-12-10T05:08:00.531540: step 421, loss 1.59996, acc 0.65625, prec 0.0331614, recall 0.747764
2017-12-10T05:08:00.798221: step 422, loss 0.887134, acc 0.765625, prec 0.033122, recall 0.747764
2017-12-10T05:08:01.061285: step 423, loss 7.96244, acc 0.796875, prec 0.0332436, recall 0.747331
2017-12-10T05:08:01.330073: step 424, loss 0.81098, acc 0.84375, prec 0.0332938, recall 0.74778
2017-12-10T05:08:01.595025: step 425, loss 0.507884, acc 0.828125, prec 0.0332649, recall 0.74778
2017-12-10T05:08:01.860111: step 426, loss 0.491588, acc 0.890625, prec 0.0333228, recall 0.748227
2017-12-10T05:08:02.134387: step 427, loss 9.87432, acc 0.859375, prec 0.0333018, recall 0.746903
2017-12-10T05:08:02.407213: step 428, loss 0.84555, acc 0.828125, prec 0.0332729, recall 0.746903
2017-12-10T05:08:02.671189: step 429, loss 0.82056, acc 0.8125, prec 0.0332414, recall 0.746903
2017-12-10T05:08:02.938890: step 430, loss 0.68368, acc 0.8125, prec 0.03321, recall 0.746903
2017-12-10T05:08:03.202428: step 431, loss 0.74052, acc 0.765625, prec 0.0331709, recall 0.746903
2017-12-10T05:08:03.462855: step 432, loss 0.971432, acc 0.828125, prec 0.0331422, recall 0.746903
2017-12-10T05:08:03.723463: step 433, loss 3.19145, acc 0.75, prec 0.0331791, recall 0.746032
2017-12-10T05:08:03.990005: step 434, loss 0.782752, acc 0.828125, prec 0.0331505, recall 0.746032
2017-12-10T05:08:04.257514: step 435, loss 0.716021, acc 0.796875, prec 0.0331167, recall 0.746032
2017-12-10T05:08:04.524402: step 436, loss 0.905719, acc 0.828125, prec 0.0331639, recall 0.746479
2017-12-10T05:08:04.788804: step 437, loss 23.4758, acc 0.796875, prec 0.0332109, recall 0.744308
2017-12-10T05:08:05.057897: step 438, loss 1.0232, acc 0.75, prec 0.0331694, recall 0.744308
2017-12-10T05:08:05.319140: step 439, loss 1.70512, acc 0.703125, prec 0.0331203, recall 0.744308
2017-12-10T05:08:05.579448: step 440, loss 2.68121, acc 0.515625, prec 0.0330405, recall 0.744308
2017-12-10T05:08:05.841446: step 441, loss 12.1482, acc 0.671875, prec 0.0329892, recall 0.743007
2017-12-10T05:08:06.112515: step 442, loss 2.87017, acc 0.546875, prec 0.0329151, recall 0.743007
2017-12-10T05:08:06.375235: step 443, loss 3.78041, acc 0.34375, prec 0.0328831, recall 0.743455
2017-12-10T05:08:06.636850: step 444, loss 4.20564, acc 0.328125, prec 0.0328487, recall 0.743902
2017-12-10T05:08:06.902309: step 445, loss 4.13657, acc 0.375, prec 0.0328221, recall 0.744348
2017-12-10T05:08:07.162378: step 446, loss 3.13747, acc 0.421875, prec 0.0327292, recall 0.744348
2017-12-10T05:08:07.426396: step 447, loss 7.22575, acc 0.5, prec 0.0327256, recall 0.743501
2017-12-10T05:08:07.697836: step 448, loss 3.70486, acc 0.34375, prec 0.0326211, recall 0.743501
2017-12-10T05:08:07.961578: step 449, loss 3.73414, acc 0.421875, prec 0.0325296, recall 0.743501
2017-12-10T05:08:08.222328: step 450, loss 2.81637, acc 0.453125, prec 0.0325166, recall 0.743945
2017-12-10T05:08:08.483571: step 451, loss 3.59759, acc 0.453125, prec 0.0325038, recall 0.744387
2017-12-10T05:08:08.745829: step 452, loss 3.17898, acc 0.546875, prec 0.0325056, recall 0.744828
2017-12-10T05:08:09.008995: step 453, loss 12.2262, acc 0.484375, prec 0.0325728, recall 0.744425
2017-12-10T05:08:09.274041: step 454, loss 2.66723, acc 0.625, prec 0.0325867, recall 0.744863
2017-12-10T05:08:09.536770: step 455, loss 2.32413, acc 0.5, prec 0.0325811, recall 0.745299
2017-12-10T05:08:09.798419: step 456, loss 1.9552, acc 0.609375, prec 0.0325203, recall 0.745299
2017-12-10T05:08:10.074230: step 457, loss 2.86596, acc 0.515625, prec 0.0324453, recall 0.745299
2017-12-10T05:08:10.332810: step 458, loss 1.38311, acc 0.6875, prec 0.032469, recall 0.745734
2017-12-10T05:08:10.597004: step 459, loss 1.19468, acc 0.796875, prec 0.0325095, recall 0.746167
2017-12-10T05:08:10.859517: step 460, loss 1.74028, acc 0.671875, prec 0.0324589, recall 0.746167
2017-12-10T05:08:11.122488: step 461, loss 0.723987, acc 0.796875, prec 0.0324276, recall 0.746167
2017-12-10T05:08:11.388635: step 462, loss 0.839472, acc 0.796875, prec 0.032468, recall 0.746599
2017-12-10T05:08:11.651075: step 463, loss 0.492869, acc 0.84375, prec 0.032444, recall 0.746599
2017-12-10T05:08:11.917544: step 464, loss 0.433626, acc 0.875, prec 0.0324248, recall 0.746599
2017-12-10T05:08:12.183780: step 465, loss 0.798486, acc 0.84375, prec 0.0324723, recall 0.747029
2017-12-10T05:08:12.447512: step 466, loss 5.56921, acc 0.84375, prec 0.0324508, recall 0.745763
2017-12-10T05:08:12.719138: step 467, loss 1.5435, acc 0.875, prec 0.032434, recall 0.744501
2017-12-10T05:08:12.982826: step 468, loss 2.23188, acc 0.84375, prec 0.0324125, recall 0.743243
2017-12-10T05:08:13.246646: step 469, loss 0.280089, acc 0.921875, prec 0.0324006, recall 0.743243
2017-12-10T05:08:13.507436: step 470, loss 0.749225, acc 0.796875, prec 0.0323696, recall 0.743243
2017-12-10T05:08:13.766803: step 471, loss 6.80133, acc 0.875, prec 0.0323529, recall 0.74199
2017-12-10T05:08:14.044458: step 472, loss 0.904287, acc 0.8125, prec 0.0323955, recall 0.742424
2017-12-10T05:08:14.322389: step 473, loss 1.6069, acc 0.6875, prec 0.032348, recall 0.742424
2017-12-10T05:08:14.583947: step 474, loss 1.59605, acc 0.640625, prec 0.0322935, recall 0.742424
2017-12-10T05:08:14.846075: step 475, loss 0.862368, acc 0.796875, prec 0.0322628, recall 0.742424
2017-12-10T05:08:15.118813: step 476, loss 1.89019, acc 0.6875, prec 0.0322863, recall 0.742857
2017-12-10T05:08:15.385043: step 477, loss 1.9525, acc 0.59375, prec 0.0322957, recall 0.743289
2017-12-10T05:08:15.649184: step 478, loss 1.78795, acc 0.609375, prec 0.0322369, recall 0.743289
2017-12-10T05:08:15.911677: step 479, loss 1.62241, acc 0.71875, prec 0.0322651, recall 0.743719
2017-12-10T05:08:16.185734: step 480, loss 0.987632, acc 0.65625, prec 0.0322838, recall 0.744147
2017-12-10T05:08:16.453208: step 481, loss 1.61113, acc 0.71875, prec 0.0322417, recall 0.744147
2017-12-10T05:08:16.727074: step 482, loss 2.0206, acc 0.734375, prec 0.0322721, recall 0.744574
2017-12-10T05:08:16.992006: step 483, loss 1.16359, acc 0.703125, prec 0.0322278, recall 0.744574
2017-12-10T05:08:17.252266: step 484, loss 1.18962, acc 0.765625, prec 0.0321929, recall 0.744574
2017-12-10T05:08:17.521747: step 485, loss 1.94568, acc 0.75, prec 0.0322953, recall 0.745424
2017-12-10T05:08:17.786561: step 486, loss 2.48528, acc 0.75, prec 0.0323997, recall 0.745033
2017-12-10T05:08:18.056341: step 487, loss 1.18348, acc 0.75, prec 0.0323625, recall 0.745033
2017-12-10T05:08:18.332279: step 488, loss 1.05499, acc 0.765625, prec 0.0323276, recall 0.745033
2017-12-10T05:08:18.597673: step 489, loss 0.755012, acc 0.765625, prec 0.0323622, recall 0.745455
2017-12-10T05:08:18.864937: step 490, loss 1.7944, acc 0.765625, prec 0.0323968, recall 0.745875
2017-12-10T05:08:19.135172: step 491, loss 0.946469, acc 0.8125, prec 0.0325075, recall 0.746711
2017-12-10T05:08:19.415007: step 492, loss 2.93796, acc 0.734375, prec 0.0326755, recall 0.747954
2017-12-10T05:08:19.691291: step 493, loss 5.24481, acc 0.6875, prec 0.0326312, recall 0.746732
2017-12-10T05:08:19.963523: step 494, loss 11.2677, acc 0.6875, prec 0.032656, recall 0.745928
2017-12-10T05:08:20.229429: step 495, loss 1.66429, acc 0.75, prec 0.0327565, recall 0.746753
2017-12-10T05:08:20.492724: step 496, loss 2.53563, acc 0.53125, prec 0.0327554, recall 0.747164
2017-12-10T05:08:20.730880: step 497, loss 2.4813, acc 0.557692, prec 0.0327706, recall 0.747573
2017-12-10T05:08:20.995030: step 498, loss 2.68573, acc 0.453125, prec 0.0326894, recall 0.747573
2017-12-10T05:08:21.261533: step 499, loss 3.73179, acc 0.4375, prec 0.0326747, recall 0.747981
2017-12-10T05:08:21.526439: step 500, loss 4.07714, acc 0.421875, prec 0.0326577, recall 0.748387
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-500

2017-12-10T05:08:22.835959: step 501, loss 20.0062, acc 0.421875, prec 0.0325751, recall 0.747182
2017-12-10T05:08:23.110793: step 502, loss 3.60546, acc 0.375, prec 0.0324839, recall 0.747182
2017-12-10T05:08:23.375724: step 503, loss 4.34882, acc 0.375, prec 0.0323932, recall 0.747182
2017-12-10T05:08:23.636293: step 504, loss 3.52889, acc 0.40625, prec 0.0323075, recall 0.747182
2017-12-10T05:08:23.896441: step 505, loss 2.46061, acc 0.453125, prec 0.0322289, recall 0.747182
2017-12-10T05:08:24.166121: step 506, loss 2.24735, acc 0.515625, prec 0.0322938, recall 0.747994
2017-12-10T05:08:24.436641: step 507, loss 2.04962, acc 0.625, prec 0.0323741, recall 0.7488
2017-12-10T05:08:24.708076: step 508, loss 1.71308, acc 0.625, prec 0.0323204, recall 0.7488
2017-12-10T05:08:24.974329: step 509, loss 3.77167, acc 0.546875, prec 0.0323915, recall 0.748408
2017-12-10T05:08:25.249081: step 510, loss 0.927602, acc 0.765625, prec 0.032358, recall 0.748408
2017-12-10T05:08:25.515867: step 511, loss 0.705537, acc 0.828125, prec 0.0323335, recall 0.748408
2017-12-10T05:08:25.783907: step 512, loss 0.841496, acc 0.828125, prec 0.0324421, recall 0.749206
2017-12-10T05:08:26.045809: step 513, loss 0.948815, acc 0.78125, prec 0.0324109, recall 0.749206
2017-12-10T05:08:26.307002: step 514, loss 4.47301, acc 0.84375, prec 0.0323909, recall 0.748019
2017-12-10T05:08:26.573537: step 515, loss 0.346442, acc 0.9375, prec 0.032382, recall 0.748019
2017-12-10T05:08:26.841448: step 516, loss 0.635656, acc 0.9375, prec 0.0324395, recall 0.748418
2017-12-10T05:08:27.115230: step 517, loss 5.86554, acc 0.828125, prec 0.0324836, recall 0.747634
2017-12-10T05:08:27.385592: step 518, loss 0.79856, acc 0.828125, prec 0.0324591, recall 0.747634
2017-12-10T05:08:27.649749: step 519, loss 0.44726, acc 0.890625, prec 0.0326422, recall 0.748823
2017-12-10T05:08:27.918598: step 520, loss 0.52901, acc 0.8125, prec 0.0326815, recall 0.749216
2017-12-10T05:08:28.188045: step 521, loss 0.832555, acc 0.796875, prec 0.0326525, recall 0.749216
2017-12-10T05:08:28.455171: step 522, loss 1.29645, acc 0.734375, prec 0.0326146, recall 0.749216
2017-12-10T05:08:28.722616: step 523, loss 1.04741, acc 0.78125, prec 0.0326494, recall 0.749609
2017-12-10T05:08:28.987967: step 524, loss 0.939781, acc 0.8125, prec 0.0327545, recall 0.75039
2017-12-10T05:08:29.255498: step 525, loss 0.641758, acc 0.8125, prec 0.0327278, recall 0.75039
2017-12-10T05:08:29.519706: step 526, loss 1.33312, acc 0.765625, prec 0.0327601, recall 0.750779
2017-12-10T05:08:29.797495: step 527, loss 0.877829, acc 0.75, prec 0.0327902, recall 0.751166
2017-12-10T05:08:30.067837: step 528, loss 0.418989, acc 0.90625, prec 0.0328425, recall 0.751553
2017-12-10T05:08:30.335732: step 529, loss 0.618949, acc 0.875, prec 0.0328247, recall 0.751553
2017-12-10T05:08:30.606128: step 530, loss 0.972766, acc 0.75, prec 0.0327891, recall 0.751553
2017-12-10T05:08:30.876783: step 531, loss 5.95012, acc 0.84375, prec 0.0328346, recall 0.750774
2017-12-10T05:08:31.146539: step 532, loss 0.993822, acc 0.796875, prec 0.0328712, recall 0.751159
2017-12-10T05:08:31.413680: step 533, loss 0.865201, acc 0.796875, prec 0.0328423, recall 0.751159
2017-12-10T05:08:31.678437: step 534, loss 0.925964, acc 0.828125, prec 0.0328179, recall 0.751159
2017-12-10T05:08:31.944625: step 535, loss 0.474235, acc 0.875, prec 0.0328002, recall 0.751159
2017-12-10T05:08:32.213010: step 536, loss 1.53471, acc 0.8125, prec 0.0329041, recall 0.751926
2017-12-10T05:08:32.476337: step 537, loss 0.561927, acc 0.828125, prec 0.0329448, recall 0.752308
2017-12-10T05:08:32.741424: step 538, loss 0.719151, acc 0.8125, prec 0.0329833, recall 0.752688
2017-12-10T05:08:33.009605: step 539, loss 0.415146, acc 0.921875, prec 0.0329722, recall 0.752688
2017-12-10T05:08:33.283715: step 540, loss 0.548829, acc 0.84375, prec 0.03295, recall 0.752688
2017-12-10T05:08:33.549513: step 541, loss 1.08706, acc 0.921875, prec 0.033069, recall 0.753446
2017-12-10T05:08:33.815431: step 542, loss 0.966396, acc 0.796875, prec 0.033105, recall 0.753823
2017-12-10T05:08:34.081022: step 543, loss 0.932888, acc 0.78125, prec 0.0331388, recall 0.754198
2017-12-10T05:08:34.348331: step 544, loss 0.278935, acc 0.890625, prec 0.0332529, recall 0.754947
2017-12-10T05:08:34.616592: step 545, loss 0.587679, acc 0.84375, prec 0.0332954, recall 0.755319
2017-12-10T05:08:34.880322: step 546, loss 7.35766, acc 0.828125, prec 0.0333378, recall 0.754545
2017-12-10T05:08:35.159445: step 547, loss 0.744776, acc 0.8125, prec 0.033311, recall 0.754545
2017-12-10T05:08:35.422702: step 548, loss 0.621521, acc 0.8125, prec 0.0332843, recall 0.754545
2017-12-10T05:08:35.696785: step 549, loss 2.65753, acc 0.875, prec 0.0334625, recall 0.754518
2017-12-10T05:08:35.970378: step 550, loss 1.11376, acc 0.78125, prec 0.0334312, recall 0.754518
2017-12-10T05:08:36.232328: step 551, loss 1.37017, acc 0.8125, prec 0.0335333, recall 0.755255
2017-12-10T05:08:36.497184: step 552, loss 1.24764, acc 0.703125, prec 0.0336839, recall 0.756353
2017-12-10T05:08:36.760109: step 553, loss 1.19172, acc 0.71875, prec 0.0336436, recall 0.756353
2017-12-10T05:08:37.024500: step 554, loss 1.60456, acc 0.75, prec 0.033672, recall 0.756716
2017-12-10T05:08:37.285707: step 555, loss 2.56202, acc 0.546875, prec 0.0336073, recall 0.756716
2017-12-10T05:08:37.554119: step 556, loss 1.34356, acc 0.765625, prec 0.0336379, recall 0.757079
2017-12-10T05:08:37.819882: step 557, loss 3.33336, acc 0.75, prec 0.0336046, recall 0.755952
2017-12-10T05:08:38.084937: step 558, loss 2.44413, acc 0.6875, prec 0.033624, recall 0.756315
2017-12-10T05:08:38.349990: step 559, loss 1.38535, acc 0.65625, prec 0.0335752, recall 0.756315
2017-12-10T05:08:38.618455: step 560, loss 1.50279, acc 0.703125, prec 0.0335332, recall 0.756315
2017-12-10T05:08:38.880566: step 561, loss 1.49272, acc 0.671875, prec 0.033614, recall 0.757037
2017-12-10T05:08:39.140457: step 562, loss 0.938857, acc 0.796875, prec 0.0335853, recall 0.757037
2017-12-10T05:08:39.401043: step 563, loss 1.653, acc 0.6875, prec 0.0336046, recall 0.757396
2017-12-10T05:08:39.666250: step 564, loss 1.43189, acc 0.65625, prec 0.0335562, recall 0.757396
2017-12-10T05:08:39.929100: step 565, loss 0.673456, acc 0.828125, prec 0.0336586, recall 0.758112
2017-12-10T05:08:40.189074: step 566, loss 1.00362, acc 0.78125, prec 0.0336277, recall 0.758112
2017-12-10T05:08:40.453611: step 567, loss 0.784494, acc 0.765625, prec 0.0335948, recall 0.758112
2017-12-10T05:08:40.724179: step 568, loss 0.580818, acc 0.8125, prec 0.0335684, recall 0.758112
2017-12-10T05:08:40.996062: step 569, loss 0.886567, acc 0.765625, prec 0.0335356, recall 0.758112
2017-12-10T05:08:41.265043: step 570, loss 0.816809, acc 0.875, prec 0.0335811, recall 0.758468
2017-12-10T05:08:41.540203: step 571, loss 0.784941, acc 0.828125, prec 0.033557, recall 0.758468
2017-12-10T05:08:41.810487: step 572, loss 0.662288, acc 0.859375, prec 0.0336003, recall 0.758824
2017-12-10T05:08:42.083090: step 573, loss 0.501474, acc 0.875, prec 0.0335828, recall 0.758824
2017-12-10T05:08:42.352486: step 574, loss 0.359663, acc 0.953125, prec 0.0335763, recall 0.758824
2017-12-10T05:08:42.617530: step 575, loss 0.597602, acc 0.96875, prec 0.0336976, recall 0.759531
2017-12-10T05:08:42.891533: step 576, loss 10.6435, acc 0.90625, prec 0.0336867, recall 0.758419
2017-12-10T05:08:43.161593: step 577, loss 12.8413, acc 0.90625, prec 0.0337385, recall 0.757664
2017-12-10T05:08:43.440500: step 578, loss 1.00412, acc 0.953125, prec 0.0337948, recall 0.758017
2017-12-10T05:08:43.708241: step 579, loss 0.957006, acc 0.84375, prec 0.0338356, recall 0.75837
2017-12-10T05:08:43.973652: step 580, loss 4.1282, acc 0.90625, prec 0.0338873, recall 0.75762
2017-12-10T05:08:44.246892: step 581, loss 11.8697, acc 0.71875, prec 0.0339148, recall 0.75578
2017-12-10T05:08:44.520777: step 582, loss 1.65599, acc 0.609375, prec 0.0338599, recall 0.75578
2017-12-10T05:08:44.785451: step 583, loss 2.07449, acc 0.609375, prec 0.0338676, recall 0.756133
2017-12-10T05:08:45.049343: step 584, loss 3.7306, acc 0.46875, prec 0.0340426, recall 0.757532
2017-12-10T05:08:45.313887: step 585, loss 4.6767, acc 0.296875, prec 0.0340062, recall 0.75788
2017-12-10T05:08:45.574718: step 586, loss 4.62247, acc 0.390625, prec 0.0339211, recall 0.75788
2017-12-10T05:08:45.836947: step 587, loss 5.41233, acc 0.34375, prec 0.03383, recall 0.75788
2017-12-10T05:08:46.100609: step 588, loss 6.07673, acc 0.25, prec 0.0337265, recall 0.75788
2017-12-10T05:08:46.360530: step 589, loss 4.84624, acc 0.28125, prec 0.0337507, recall 0.758571
2017-12-10T05:08:46.626013: step 590, loss 4.95342, acc 0.328125, prec 0.0338424, recall 0.759602
2017-12-10T05:08:46.887806: step 591, loss 4.71698, acc 0.25, prec 0.0337398, recall 0.759602
2017-12-10T05:08:47.147231: step 592, loss 2.23464, acc 0.53125, prec 0.0337369, recall 0.759943
2017-12-10T05:08:47.413662: step 593, loss 3.39417, acc 0.515625, prec 0.0336711, recall 0.759943
2017-12-10T05:08:47.676021: step 594, loss 2.34745, acc 0.59375, prec 0.0336161, recall 0.759943
2017-12-10T05:08:47.937194: step 595, loss 2.28451, acc 0.625, prec 0.0336261, recall 0.760284
2017-12-10T05:08:48.201883: step 596, loss 1.21135, acc 0.671875, prec 0.0336424, recall 0.760623
2017-12-10T05:08:48.468530: step 597, loss 1.08536, acc 0.75, prec 0.0336692, recall 0.760962
2017-12-10T05:08:48.733758: step 598, loss 1.09107, acc 0.75, prec 0.0336355, recall 0.760962
2017-12-10T05:08:49.003360: step 599, loss 0.410525, acc 0.875, prec 0.0336187, recall 0.760962
2017-12-10T05:08:49.275139: step 600, loss 7.18196, acc 0.875, prec 0.033604, recall 0.759887

Evaluation:
2017-12-10T05:08:56.958353: step 600, loss 4.11829, acc 0.91706, prec 0.0357566, recall 0.698725

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-600

2017-12-10T05:08:58.339330: step 601, loss 2.67346, acc 0.828125, prec 0.0357355, recall 0.697917
2017-12-10T05:08:58.604604: step 602, loss 0.299748, acc 0.9375, prec 0.035727, recall 0.697917
2017-12-10T05:08:58.873370: step 603, loss 7.42282, acc 0.859375, prec 0.0357101, recall 0.69711
2017-12-10T05:08:59.141494: step 604, loss 0.252383, acc 0.90625, prec 0.0356974, recall 0.69711
2017-12-10T05:08:59.407958: step 605, loss 1.08131, acc 0.828125, prec 0.0356741, recall 0.69711
2017-12-10T05:08:59.677869: step 606, loss 0.971542, acc 0.703125, prec 0.0356341, recall 0.69711
2017-12-10T05:08:59.946633: step 607, loss 0.4428, acc 0.9375, prec 0.0356257, recall 0.69711
2017-12-10T05:09:00.219480: step 608, loss 0.672876, acc 0.84375, prec 0.0356046, recall 0.69711
2017-12-10T05:09:00.489759: step 609, loss 1.61897, acc 0.71875, prec 0.0355668, recall 0.69711
2017-12-10T05:09:00.764598: step 610, loss 1.36059, acc 0.875, prec 0.0356069, recall 0.69746
2017-12-10T05:09:01.028618: step 611, loss 0.6537, acc 0.84375, prec 0.0356427, recall 0.697809
2017-12-10T05:09:01.295142: step 612, loss 0.613661, acc 0.890625, prec 0.0356848, recall 0.698157
2017-12-10T05:09:01.558087: step 613, loss 0.412137, acc 0.890625, prec 0.0357836, recall 0.698851
2017-12-10T05:09:01.832249: step 614, loss 0.979762, acc 0.796875, prec 0.0357563, recall 0.698851
2017-12-10T05:09:02.092070: step 615, loss 0.583848, acc 0.875, prec 0.0357962, recall 0.699196
2017-12-10T05:09:02.358269: step 616, loss 0.96149, acc 0.765625, prec 0.0357646, recall 0.699196
2017-12-10T05:09:02.622962: step 617, loss 6.36692, acc 0.828125, prec 0.0357436, recall 0.698394
2017-12-10T05:09:02.896310: step 618, loss 0.843359, acc 0.84375, prec 0.0357792, recall 0.69874
2017-12-10T05:09:03.163019: step 619, loss 1.10401, acc 0.78125, prec 0.0357499, recall 0.69874
2017-12-10T05:09:03.430383: step 620, loss 0.82722, acc 0.828125, prec 0.0358962, recall 0.699772
2017-12-10T05:09:03.693783: step 621, loss 0.906163, acc 0.78125, prec 0.0358668, recall 0.699772
2017-12-10T05:09:03.957375: step 622, loss 0.895025, acc 0.8125, prec 0.0358417, recall 0.699772
2017-12-10T05:09:04.219367: step 623, loss 0.960873, acc 0.71875, prec 0.0358603, recall 0.700114
2017-12-10T05:09:04.486027: step 624, loss 1.40302, acc 0.765625, prec 0.0359977, recall 0.701136
2017-12-10T05:09:04.747906: step 625, loss 3.91942, acc 0.78125, prec 0.0360266, recall 0.70068
2017-12-10T05:09:05.013319: step 626, loss 0.990421, acc 0.765625, prec 0.0359951, recall 0.70068
2017-12-10T05:09:05.277903: step 627, loss 0.909064, acc 0.859375, prec 0.0360885, recall 0.701357
2017-12-10T05:09:05.537595: step 628, loss 1.00793, acc 0.828125, prec 0.0360654, recall 0.701357
2017-12-10T05:09:05.805800: step 629, loss 1.26642, acc 0.734375, prec 0.0360298, recall 0.701357
2017-12-10T05:09:06.071918: step 630, loss 0.917764, acc 0.78125, prec 0.0360564, recall 0.701695
2017-12-10T05:09:06.332670: step 631, loss 0.402619, acc 0.875, prec 0.0360956, recall 0.702032
2017-12-10T05:09:06.597896: step 632, loss 0.879538, acc 0.796875, prec 0.0361243, recall 0.702368
2017-12-10T05:09:06.869580: step 633, loss 0.935273, acc 0.84375, prec 0.0361592, recall 0.702703
2017-12-10T05:09:07.140912: step 634, loss 0.994579, acc 0.75, prec 0.0362373, recall 0.703371
2017-12-10T05:09:07.411956: step 635, loss 0.772243, acc 0.828125, prec 0.0362143, recall 0.703371
2017-12-10T05:09:07.680584: step 636, loss 0.828661, acc 0.90625, prec 0.0362574, recall 0.703704
2017-12-10T05:09:07.946190: step 637, loss 0.758156, acc 0.859375, prec 0.0362943, recall 0.704036
2017-12-10T05:09:08.217681: step 638, loss 0.743804, acc 0.84375, prec 0.0362733, recall 0.704036
2017-12-10T05:09:08.483427: step 639, loss 0.594308, acc 0.90625, prec 0.0363164, recall 0.704367
2017-12-10T05:09:08.754397: step 640, loss 0.605038, acc 0.828125, prec 0.0362933, recall 0.704367
2017-12-10T05:09:09.019052: step 641, loss 0.530258, acc 0.859375, prec 0.0362745, recall 0.704367
2017-12-10T05:09:09.282955: step 642, loss 0.395751, acc 0.890625, prec 0.0362599, recall 0.704367
2017-12-10T05:09:09.547811: step 643, loss 0.611371, acc 0.921875, prec 0.0362494, recall 0.704367
2017-12-10T05:09:09.809270: step 644, loss 0.455049, acc 0.890625, prec 0.0362348, recall 0.704367
2017-12-10T05:09:10.076223: step 645, loss 0.206403, acc 0.9375, prec 0.036282, recall 0.704698
2017-12-10T05:09:10.342030: step 646, loss 0.214009, acc 0.90625, prec 0.0362694, recall 0.704698
2017-12-10T05:09:10.602132: step 647, loss 0.314141, acc 0.921875, prec 0.0363145, recall 0.705028
2017-12-10T05:09:10.872852: step 648, loss 6.28922, acc 0.859375, prec 0.036464, recall 0.705228
2017-12-10T05:09:11.146353: step 649, loss 0.203487, acc 0.890625, prec 0.0365047, recall 0.705556
2017-12-10T05:09:11.404723: step 650, loss 3.83397, acc 0.921875, prec 0.0364963, recall 0.704772
2017-12-10T05:09:11.678676: step 651, loss 13.2359, acc 0.890625, prec 0.0366498, recall 0.704972
2017-12-10T05:09:11.945188: step 652, loss 2.48189, acc 0.828125, prec 0.0367394, recall 0.704846
2017-12-10T05:09:12.219123: step 653, loss 0.630451, acc 0.84375, prec 0.0367736, recall 0.705171
2017-12-10T05:09:12.481697: step 654, loss 1.99041, acc 0.59375, prec 0.0367188, recall 0.705171
2017-12-10T05:09:12.743524: step 655, loss 2.80987, acc 0.5, prec 0.0367067, recall 0.705495
2017-12-10T05:09:13.005803: step 656, loss 2.8179, acc 0.5, prec 0.0366397, recall 0.705495
2017-12-10T05:09:13.263708: step 657, loss 2.69949, acc 0.5, prec 0.0365729, recall 0.705495
2017-12-10T05:09:13.528977: step 658, loss 3.72818, acc 0.34375, prec 0.0364856, recall 0.705495
2017-12-10T05:09:13.791372: step 659, loss 3.22099, acc 0.5, prec 0.0367472, recall 0.707424
2017-12-10T05:09:14.055488: step 660, loss 3.41272, acc 0.46875, prec 0.036731, recall 0.707743
2017-12-10T05:09:14.313188: step 661, loss 3.5138, acc 0.34375, prec 0.0366983, recall 0.708061
2017-12-10T05:09:14.575109: step 662, loss 2.42878, acc 0.515625, prec 0.0366885, recall 0.708379
2017-12-10T05:09:14.842285: step 663, loss 2.07983, acc 0.546875, prec 0.0366828, recall 0.708696
2017-12-10T05:09:15.114994: step 664, loss 2.45354, acc 0.46875, prec 0.0366669, recall 0.709012
2017-12-10T05:09:15.383175: step 665, loss 1.50707, acc 0.609375, prec 0.0366695, recall 0.709328
2017-12-10T05:09:15.643206: step 666, loss 1.5318, acc 0.6875, prec 0.0366284, recall 0.709328
2017-12-10T05:09:15.910661: step 667, loss 0.722874, acc 0.828125, prec 0.0367137, recall 0.709957
2017-12-10T05:09:16.179950: step 668, loss 0.743999, acc 0.8125, prec 0.0367429, recall 0.71027
2017-12-10T05:09:16.443157: step 669, loss 0.698154, acc 0.796875, prec 0.0367162, recall 0.71027
2017-12-10T05:09:16.710484: step 670, loss 0.637663, acc 0.796875, prec 0.0366896, recall 0.71027
2017-12-10T05:09:16.980413: step 671, loss 7.74642, acc 0.84375, prec 0.0366711, recall 0.709503
2017-12-10T05:09:17.247784: step 672, loss 0.248402, acc 0.921875, prec 0.0366609, recall 0.709503
2017-12-10T05:09:17.517946: step 673, loss 0.347478, acc 0.921875, prec 0.0367044, recall 0.709817
2017-12-10T05:09:17.782017: step 674, loss 20.1554, acc 0.890625, prec 0.0367479, recall 0.708602
2017-12-10T05:09:18.049204: step 675, loss 6.64433, acc 0.90625, prec 0.0368471, recall 0.707709
2017-12-10T05:09:18.317627: step 676, loss 0.639514, acc 0.859375, prec 0.0368286, recall 0.707709
2017-12-10T05:09:18.579103: step 677, loss 4.89118, acc 0.75, prec 0.0367979, recall 0.706952
2017-12-10T05:09:18.845540: step 678, loss 1.26889, acc 0.71875, prec 0.036761, recall 0.706952
2017-12-10T05:09:19.111928: step 679, loss 2.79476, acc 0.515625, prec 0.0366978, recall 0.706952
2017-12-10T05:09:19.377773: step 680, loss 3.37997, acc 0.453125, prec 0.03668, recall 0.707265
2017-12-10T05:09:19.649445: step 681, loss 3.96084, acc 0.3125, prec 0.0365908, recall 0.707265
2017-12-10T05:09:19.916360: step 682, loss 3.71363, acc 0.40625, prec 0.0365141, recall 0.707265
2017-12-10T05:09:20.179477: step 683, loss 4.69556, acc 0.28125, prec 0.0365807, recall 0.7082
2017-12-10T05:09:20.439937: step 684, loss 3.95184, acc 0.375, prec 0.0365532, recall 0.708511
2017-12-10T05:09:20.701191: step 685, loss 4.9184, acc 0.40625, prec 0.0364772, recall 0.708511
2017-12-10T05:09:20.965837: step 686, loss 4.1416, acc 0.296875, prec 0.0364401, recall 0.70882
2017-12-10T05:09:21.225262: step 687, loss 2.57722, acc 0.53125, prec 0.0363805, recall 0.70882
2017-12-10T05:09:21.486786: step 688, loss 4.20868, acc 0.515625, prec 0.0364785, recall 0.708995
2017-12-10T05:09:21.756235: step 689, loss 2.03217, acc 0.546875, prec 0.036421, recall 0.708995
2017-12-10T05:09:22.016595: step 690, loss 2.17776, acc 0.578125, prec 0.0363676, recall 0.708995
2017-12-10T05:09:22.278719: step 691, loss 1.6587, acc 0.625, prec 0.0363725, recall 0.709302
2017-12-10T05:09:22.551073: step 692, loss 1.48647, acc 0.65625, prec 0.0363292, recall 0.709302
2017-12-10T05:09:22.815590: step 693, loss 1.41022, acc 0.703125, prec 0.036344, recall 0.709609
2017-12-10T05:09:23.082315: step 694, loss 1.10896, acc 0.765625, prec 0.0363666, recall 0.709916
2017-12-10T05:09:23.349452: step 695, loss 0.576402, acc 0.828125, prec 0.036345, recall 0.709916
2017-12-10T05:09:23.617476: step 696, loss 0.539575, acc 0.875, prec 0.0363293, recall 0.709916
2017-12-10T05:09:23.886258: step 697, loss 0.933383, acc 0.796875, prec 0.0363038, recall 0.709916
2017-12-10T05:09:24.156138: step 698, loss 0.353854, acc 0.890625, prec 0.0362901, recall 0.709916
2017-12-10T05:09:24.419954: step 699, loss 10.0729, acc 0.90625, prec 0.0362823, recall 0.708421
2017-12-10T05:09:24.689623: step 700, loss 4.84165, acc 0.890625, prec 0.0362705, recall 0.707676
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-700

2017-12-10T05:09:25.896589: step 701, loss 12.8557, acc 0.84375, prec 0.036253, recall 0.706933
2017-12-10T05:09:26.161044: step 702, loss 6.99993, acc 0.78125, prec 0.0363313, recall 0.706806
2017-12-10T05:09:26.428465: step 703, loss 0.877041, acc 0.765625, prec 0.0363539, recall 0.707113
2017-12-10T05:09:26.705503: step 704, loss 2.14637, acc 0.578125, prec 0.0363012, recall 0.707113
2017-12-10T05:09:26.973734: step 705, loss 2.3584, acc 0.546875, prec 0.0362447, recall 0.707113
2017-12-10T05:09:27.248693: step 706, loss 2.50868, acc 0.53125, prec 0.0362381, recall 0.707419
2017-12-10T05:09:27.512663: step 707, loss 3.41842, acc 0.4375, prec 0.0362199, recall 0.707724
2017-12-10T05:09:27.776659: step 708, loss 2.90791, acc 0.4375, prec 0.0361504, recall 0.707724
2017-12-10T05:09:28.035758: step 709, loss 4.22401, acc 0.390625, prec 0.0360753, recall 0.707724
2017-12-10T05:09:28.301736: step 710, loss 3.0537, acc 0.5625, prec 0.0360217, recall 0.707724
2017-12-10T05:09:28.561528: step 711, loss 3.16792, acc 0.5, prec 0.0359605, recall 0.707724
2017-12-10T05:09:28.829671: step 712, loss 2.50789, acc 0.53125, prec 0.0360055, recall 0.708333
2017-12-10T05:09:29.096904: step 713, loss 2.46623, acc 0.515625, prec 0.0359465, recall 0.708333
2017-12-10T05:09:29.361721: step 714, loss 3.22067, acc 0.5, prec 0.0359875, recall 0.70894
2017-12-10T05:09:29.626014: step 715, loss 1.8491, acc 0.671875, prec 0.0359477, recall 0.70894
2017-12-10T05:09:29.886250: step 716, loss 1.59537, acc 0.65625, prec 0.0360076, recall 0.709544
2017-12-10T05:09:30.146030: step 717, loss 1.59877, acc 0.671875, prec 0.0359678, recall 0.709544
2017-12-10T05:09:30.416491: step 718, loss 1.32331, acc 0.6875, prec 0.03593, recall 0.709544
2017-12-10T05:09:30.684019: step 719, loss 12.2192, acc 0.734375, prec 0.0359505, recall 0.70911
2017-12-10T05:09:30.953649: step 720, loss 0.741689, acc 0.8125, prec 0.0359784, recall 0.709411
2017-12-10T05:09:31.226428: step 721, loss 0.761385, acc 0.8125, prec 0.0359558, recall 0.709411
2017-12-10T05:09:31.508829: step 722, loss 1.1055, acc 0.75, prec 0.0360266, recall 0.71001
2017-12-10T05:09:31.770876: step 723, loss 1.88623, acc 0.828125, prec 0.0361086, recall 0.709877
2017-12-10T05:09:32.036314: step 724, loss 0.560565, acc 0.890625, prec 0.0360954, recall 0.709877
2017-12-10T05:09:32.306080: step 725, loss 0.197995, acc 0.921875, prec 0.036086, recall 0.709877
2017-12-10T05:09:32.566071: step 726, loss 0.499837, acc 0.90625, prec 0.0360747, recall 0.709877
2017-12-10T05:09:32.832255: step 727, loss 0.4002, acc 0.859375, prec 0.0360577, recall 0.709877
2017-12-10T05:09:33.097477: step 728, loss 0.489302, acc 0.90625, prec 0.0360464, recall 0.709877
2017-12-10T05:09:33.375316: step 729, loss 0.422793, acc 0.875, prec 0.0360817, recall 0.710175
2017-12-10T05:09:33.647901: step 730, loss 1.10387, acc 0.828125, prec 0.0361616, recall 0.710769
2017-12-10T05:09:33.920750: step 731, loss 3.15281, acc 0.875, prec 0.0361484, recall 0.710041
2017-12-10T05:09:34.191058: step 732, loss 0.219077, acc 0.90625, prec 0.036137, recall 0.710041
2017-12-10T05:09:34.459646: step 733, loss 0.348345, acc 0.921875, prec 0.0361276, recall 0.710041
2017-12-10T05:09:34.720515: step 734, loss 4.87628, acc 0.890625, prec 0.0362168, recall 0.709908
2017-12-10T05:09:34.993947: step 735, loss 0.161772, acc 0.953125, prec 0.0362613, recall 0.710204
2017-12-10T05:09:35.259874: step 736, loss 0.570215, acc 0.84375, prec 0.0362926, recall 0.710499
2017-12-10T05:09:35.527124: step 737, loss 4.30288, acc 0.78125, prec 0.0362681, recall 0.709776
2017-12-10T05:09:35.789395: step 738, loss 0.691156, acc 0.859375, prec 0.0362511, recall 0.709776
2017-12-10T05:09:36.052195: step 739, loss 0.610251, acc 0.828125, prec 0.0362304, recall 0.709776
2017-12-10T05:09:36.320988: step 740, loss 0.79123, acc 0.765625, prec 0.0362022, recall 0.709776
2017-12-10T05:09:36.583180: step 741, loss 1.71308, acc 0.65625, prec 0.0362108, recall 0.710071
2017-12-10T05:09:36.841678: step 742, loss 0.759221, acc 0.796875, prec 0.0361864, recall 0.710071
2017-12-10T05:09:37.113467: step 743, loss 6.39138, acc 0.703125, prec 0.0361546, recall 0.708629
2017-12-10T05:09:37.380966: step 744, loss 1.35118, acc 0.765625, prec 0.0362761, recall 0.709514
2017-12-10T05:09:37.648429: step 745, loss 1.70475, acc 0.65625, prec 0.0362349, recall 0.709514
2017-12-10T05:09:37.916334: step 746, loss 1.84165, acc 0.640625, prec 0.0362416, recall 0.709808
2017-12-10T05:09:38.183705: step 747, loss 2.14023, acc 0.625, prec 0.0362465, recall 0.710101
2017-12-10T05:09:38.462331: step 748, loss 1.51048, acc 0.59375, prec 0.0362972, recall 0.710685
2017-12-10T05:09:38.722412: step 749, loss 1.82028, acc 0.578125, prec 0.0362468, recall 0.710685
2017-12-10T05:09:38.980558: step 750, loss 2.13422, acc 0.625, prec 0.0362021, recall 0.710685
2017-12-10T05:09:39.249719: step 751, loss 3.27764, acc 0.609375, prec 0.036207, recall 0.710262
2017-12-10T05:09:39.520780: step 752, loss 2.32915, acc 0.515625, prec 0.0361495, recall 0.710262
2017-12-10T05:09:39.784102: step 753, loss 2.19534, acc 0.5625, prec 0.0360978, recall 0.710262
2017-12-10T05:09:40.043957: step 754, loss 1.03603, acc 0.703125, prec 0.036112, recall 0.710553
2017-12-10T05:09:40.308184: step 755, loss 1.32765, acc 0.6875, prec 0.0360751, recall 0.710553
2017-12-10T05:09:40.571124: step 756, loss 1.46065, acc 0.6875, prec 0.0360383, recall 0.710553
2017-12-10T05:09:40.836630: step 757, loss 1.21338, acc 0.734375, prec 0.0360071, recall 0.710553
2017-12-10T05:09:41.100461: step 758, loss 0.737693, acc 0.796875, prec 0.0359833, recall 0.710553
2017-12-10T05:09:41.367879: step 759, loss 0.66496, acc 0.78125, prec 0.0359577, recall 0.710553
2017-12-10T05:09:41.636007: step 760, loss 0.960749, acc 0.8125, prec 0.0359358, recall 0.710553
2017-12-10T05:09:41.900626: step 761, loss 0.331618, acc 0.890625, prec 0.035923, recall 0.710553
2017-12-10T05:09:42.178207: step 762, loss 5.93051, acc 0.8125, prec 0.0359047, recall 0.709127
2017-12-10T05:09:42.450077: step 763, loss 0.28777, acc 0.921875, prec 0.0358956, recall 0.709127
2017-12-10T05:09:42.719120: step 764, loss 1.01874, acc 0.828125, prec 0.0358756, recall 0.709127
2017-12-10T05:09:42.980737: step 765, loss 3.02664, acc 0.859375, prec 0.0359099, recall 0.708709
2017-12-10T05:09:43.250605: step 766, loss 0.884914, acc 0.875, prec 0.0359931, recall 0.709291
2017-12-10T05:09:43.520659: step 767, loss 0.992702, acc 0.796875, prec 0.0360182, recall 0.709581
2017-12-10T05:09:43.782893: step 768, loss 0.474995, acc 0.84375, prec 0.0360488, recall 0.70987
2017-12-10T05:09:44.047501: step 769, loss 0.98344, acc 0.78125, prec 0.036072, recall 0.710159
2017-12-10T05:09:44.316310: step 770, loss 0.988057, acc 0.84375, prec 0.0360538, recall 0.710159
2017-12-10T05:09:44.586202: step 771, loss 1.24596, acc 0.765625, prec 0.0361726, recall 0.711023
2017-12-10T05:09:44.850990: step 772, loss 1.63015, acc 0.78125, prec 0.0361957, recall 0.71131
2017-12-10T05:09:45.123152: step 773, loss 1.7188, acc 0.875, prec 0.0363269, recall 0.712166
2017-12-10T05:09:45.389792: step 774, loss 1.20294, acc 0.796875, prec 0.0364489, recall 0.713018
2017-12-10T05:09:45.652507: step 775, loss 10.0467, acc 0.75, prec 0.0364699, recall 0.712598
2017-12-10T05:09:45.915770: step 776, loss 1.79564, acc 0.640625, prec 0.0364762, recall 0.712881
2017-12-10T05:09:46.182369: step 777, loss 1.42485, acc 0.625, prec 0.0364806, recall 0.713163
2017-12-10T05:09:46.449697: step 778, loss 2.06929, acc 0.65625, prec 0.0364403, recall 0.713163
2017-12-10T05:09:46.713690: step 779, loss 7.81124, acc 0.515625, prec 0.0363855, recall 0.712463
2017-12-10T05:09:46.978375: step 780, loss 1.87017, acc 0.59375, prec 0.0363864, recall 0.712745
2017-12-10T05:09:47.253947: step 781, loss 2.92228, acc 0.421875, prec 0.0363191, recall 0.712745
2017-12-10T05:09:47.524593: step 782, loss 2.68483, acc 0.453125, prec 0.0362557, recall 0.712745
2017-12-10T05:09:47.791372: step 783, loss 2.82337, acc 0.515625, prec 0.0362478, recall 0.713026
2017-12-10T05:09:48.056737: step 784, loss 3.16817, acc 0.484375, prec 0.0362841, recall 0.713587
2017-12-10T05:09:48.323946: step 785, loss 1.89545, acc 0.578125, prec 0.0363312, recall 0.714146
2017-12-10T05:09:48.590513: step 786, loss 1.92339, acc 0.609375, prec 0.0363817, recall 0.714703
2017-12-10T05:09:48.854410: step 787, loss 2.22045, acc 0.59375, prec 0.0363348, recall 0.714703
2017-12-10T05:09:49.123166: step 788, loss 1.09784, acc 0.71875, prec 0.0363501, recall 0.714981
2017-12-10T05:09:49.393545: step 789, loss 2.81437, acc 0.734375, prec 0.0363214, recall 0.714286
2017-12-10T05:09:49.667820: step 790, loss 1.05067, acc 0.859375, prec 0.036448, recall 0.715116
2017-12-10T05:09:49.931307: step 791, loss 0.500181, acc 0.828125, prec 0.0364283, recall 0.715116
2017-12-10T05:09:50.207563: step 792, loss 0.916126, acc 0.6875, prec 0.0363923, recall 0.715116
2017-12-10T05:09:50.476151: step 793, loss 0.730295, acc 0.84375, prec 0.0364694, recall 0.715667
2017-12-10T05:09:50.738504: step 794, loss 0.541239, acc 0.84375, prec 0.0364514, recall 0.715667
2017-12-10T05:09:51.005191: step 795, loss 3.2386, acc 0.765625, prec 0.0364263, recall 0.714976
2017-12-10T05:09:51.277227: step 796, loss 0.436442, acc 0.84375, prec 0.0364558, recall 0.715251
2017-12-10T05:09:51.555724: step 797, loss 3.11003, acc 0.921875, prec 0.036496, recall 0.714836
2017-12-10T05:09:51.830344: step 798, loss 0.629955, acc 0.828125, prec 0.0364763, recall 0.714836
2017-12-10T05:09:52.096180: step 799, loss 1.59662, acc 0.796875, prec 0.0365003, recall 0.715111
2017-12-10T05:09:52.375569: step 800, loss 4.14984, acc 0.8125, prec 0.0365752, recall 0.714971
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-800

2017-12-10T05:09:53.540126: step 801, loss 1.35289, acc 0.796875, prec 0.0365519, recall 0.714971
2017-12-10T05:09:53.812760: step 802, loss 1.22494, acc 0.703125, prec 0.0365178, recall 0.714971
2017-12-10T05:09:54.085798: step 803, loss 1.21227, acc 0.6875, prec 0.0365764, recall 0.715517
2017-12-10T05:09:54.350445: step 804, loss 6.15227, acc 0.65625, prec 0.036586, recall 0.715105
2017-12-10T05:09:54.620001: step 805, loss 1.51667, acc 0.65625, prec 0.0365466, recall 0.715105
2017-12-10T05:09:54.886344: step 806, loss 2.14253, acc 0.671875, prec 0.0365092, recall 0.715105
2017-12-10T05:09:55.157343: step 807, loss 2.12264, acc 0.53125, prec 0.0364558, recall 0.715105
2017-12-10T05:09:55.431093: step 808, loss 2.65167, acc 0.6875, prec 0.0365141, recall 0.715649
2017-12-10T05:09:55.699225: step 809, loss 4.54414, acc 0.484375, prec 0.0364573, recall 0.714967
2017-12-10T05:09:55.963751: step 810, loss 2.07307, acc 0.609375, prec 0.0364599, recall 0.715238
2017-12-10T05:09:56.225227: step 811, loss 2.76433, acc 0.515625, prec 0.0365919, recall 0.716319
2017-12-10T05:09:56.489075: step 812, loss 3.62594, acc 0.453125, prec 0.0365765, recall 0.716588
2017-12-10T05:09:56.754552: step 813, loss 2.10296, acc 0.59375, prec 0.0365771, recall 0.716856
2017-12-10T05:09:57.024626: step 814, loss 1.78383, acc 0.53125, prec 0.0366171, recall 0.717391
2017-12-10T05:09:57.298877: step 815, loss 2.04681, acc 0.546875, prec 0.036566, recall 0.717391
2017-12-10T05:09:57.561709: step 816, loss 1.72053, acc 0.65625, prec 0.03662, recall 0.717925
2017-12-10T05:09:57.824556: step 817, loss 1.40172, acc 0.703125, prec 0.0365865, recall 0.717925
2017-12-10T05:09:58.089982: step 818, loss 1.9154, acc 0.6875, prec 0.0366439, recall 0.718456
2017-12-10T05:09:58.353348: step 819, loss 1.64645, acc 0.671875, prec 0.036607, recall 0.718456
2017-12-10T05:09:58.621125: step 820, loss 1.14612, acc 0.78125, prec 0.0366286, recall 0.718721
2017-12-10T05:09:58.893563: step 821, loss 0.88697, acc 0.765625, prec 0.0367408, recall 0.719512
2017-12-10T05:09:59.164715: step 822, loss 0.426289, acc 0.890625, prec 0.0367746, recall 0.719775
2017-12-10T05:09:59.426241: step 823, loss 13.7519, acc 0.875, prec 0.0368544, recall 0.719626
2017-12-10T05:09:59.693590: step 824, loss 0.463871, acc 0.890625, prec 0.0369803, recall 0.72041
2017-12-10T05:09:59.963468: step 825, loss 1.04083, acc 0.890625, prec 0.037014, recall 0.72067
2017-12-10T05:10:00.234495: step 826, loss 0.668769, acc 0.859375, prec 0.0370901, recall 0.72119
2017-12-10T05:10:00.502631: step 827, loss 2.50845, acc 0.84375, prec 0.0371202, recall 0.720779
2017-12-10T05:10:00.773008: step 828, loss 10.8995, acc 0.859375, prec 0.037198, recall 0.720629
2017-12-10T05:10:01.047802: step 829, loss 0.660598, acc 0.859375, prec 0.037182, recall 0.720629
2017-12-10T05:10:01.316954: step 830, loss 1.38633, acc 0.75, prec 0.0372455, recall 0.721145
2017-12-10T05:10:01.577781: step 831, loss 0.860916, acc 0.78125, prec 0.0372665, recall 0.721402
2017-12-10T05:10:01.838567: step 832, loss 2.54227, acc 0.671875, prec 0.0372768, recall 0.720994
2017-12-10T05:10:02.106850: step 833, loss 1.67185, acc 0.640625, prec 0.0373276, recall 0.721507
2017-12-10T05:10:02.376810: step 834, loss 2.0264, acc 0.671875, prec 0.0373361, recall 0.721763
2017-12-10T05:10:02.639261: step 835, loss 2.51251, acc 0.609375, prec 0.0373832, recall 0.722273
2017-12-10T05:10:02.908707: step 836, loss 2.39507, acc 0.546875, prec 0.0373774, recall 0.722527
2017-12-10T05:10:03.170979: step 837, loss 2.14744, acc 0.609375, prec 0.0374243, recall 0.723035
2017-12-10T05:10:03.436085: step 838, loss 2.88921, acc 0.5, prec 0.0375041, recall 0.723792
2017-12-10T05:10:03.703366: step 839, loss 2.10054, acc 0.609375, prec 0.0374599, recall 0.723792
2017-12-10T05:10:03.963364: step 840, loss 2.02262, acc 0.53125, prec 0.037407, recall 0.723792
2017-12-10T05:10:04.226693: step 841, loss 2.30763, acc 0.5625, prec 0.037403, recall 0.724044
2017-12-10T05:10:04.495638: step 842, loss 3.07455, acc 0.53125, prec 0.0373955, recall 0.724295
2017-12-10T05:10:04.756239: step 843, loss 1.91656, acc 0.609375, prec 0.0374871, recall 0.725045
2017-12-10T05:10:05.021507: step 844, loss 1.93475, acc 0.703125, prec 0.0374988, recall 0.725295
2017-12-10T05:10:05.283906: step 845, loss 11.0646, acc 0.71875, prec 0.037469, recall 0.724638
2017-12-10T05:10:05.546092: step 846, loss 1.50673, acc 0.703125, prec 0.0374807, recall 0.724887
2017-12-10T05:10:05.816154: step 847, loss 1.13225, acc 0.71875, prec 0.0374942, recall 0.725136
2017-12-10T05:10:06.079794: step 848, loss 1.1761, acc 0.75, prec 0.0375111, recall 0.725384
2017-12-10T05:10:06.348273: step 849, loss 0.573963, acc 0.875, prec 0.037587, recall 0.725879
2017-12-10T05:10:06.611931: step 850, loss 0.8977, acc 0.765625, prec 0.0376056, recall 0.726126
2017-12-10T05:10:06.879871: step 851, loss 0.813011, acc 0.828125, prec 0.037676, recall 0.726619
2017-12-10T05:10:07.142350: step 852, loss 1.51552, acc 0.703125, prec 0.0377323, recall 0.727109
2017-12-10T05:10:07.408138: step 853, loss 0.23787, acc 0.890625, prec 0.0378544, recall 0.727842
2017-12-10T05:10:07.666995: step 854, loss 0.915967, acc 0.828125, prec 0.0378351, recall 0.727842
2017-12-10T05:10:07.936396: step 855, loss 0.968626, acc 0.828125, prec 0.0378605, recall 0.728086
2017-12-10T05:10:08.211102: step 856, loss 0.536327, acc 0.890625, prec 0.0378481, recall 0.728086
2017-12-10T05:10:08.477365: step 857, loss 2.28063, acc 0.828125, prec 0.0378753, recall 0.727679
2017-12-10T05:10:08.743770: step 858, loss 7.84066, acc 0.859375, prec 0.0378612, recall 0.727029
2017-12-10T05:10:09.019541: step 859, loss 0.955804, acc 0.765625, prec 0.0378795, recall 0.727273
2017-12-10T05:10:09.292661: step 860, loss 0.77814, acc 0.796875, prec 0.0378566, recall 0.727273
2017-12-10T05:10:09.559954: step 861, loss 4.0149, acc 0.71875, prec 0.0378268, recall 0.726625
2017-12-10T05:10:09.825199: step 862, loss 11.8775, acc 0.875, prec 0.0378591, recall 0.726222
2017-12-10T05:10:10.091184: step 863, loss 0.994811, acc 0.78125, prec 0.0378791, recall 0.726465
2017-12-10T05:10:10.354003: step 864, loss 1.49075, acc 0.59375, prec 0.0378336, recall 0.726465
2017-12-10T05:10:10.617030: step 865, loss 1.99695, acc 0.59375, prec 0.0377881, recall 0.726465
2017-12-10T05:10:10.880869: step 866, loss 2.45993, acc 0.640625, prec 0.0378368, recall 0.72695
2017-12-10T05:10:11.143687: step 867, loss 2.93775, acc 0.40625, prec 0.0378592, recall 0.727434
2017-12-10T05:10:11.412700: step 868, loss 3.32128, acc 0.46875, prec 0.0378443, recall 0.727675
2017-12-10T05:10:11.679071: step 869, loss 3.03609, acc 0.484375, prec 0.0378311, recall 0.727915
2017-12-10T05:10:11.943874: step 870, loss 2.57875, acc 0.484375, prec 0.037818, recall 0.728155
2017-12-10T05:10:12.218330: step 871, loss 4.48152, acc 0.4375, prec 0.0377574, recall 0.727513
2017-12-10T05:10:12.486502: step 872, loss 3.2406, acc 0.375, prec 0.0377324, recall 0.727753
2017-12-10T05:10:12.756130: step 873, loss 2.77628, acc 0.46875, prec 0.0377617, recall 0.728232
2017-12-10T05:10:13.017955: step 874, loss 2.13544, acc 0.640625, prec 0.0377221, recall 0.728232
2017-12-10T05:10:13.275755: step 875, loss 2.06369, acc 0.609375, prec 0.0376792, recall 0.728232
2017-12-10T05:10:13.547946: step 876, loss 1.40522, acc 0.71875, prec 0.0376483, recall 0.728232
2017-12-10T05:10:13.816839: step 877, loss 1.40017, acc 0.671875, prec 0.0376561, recall 0.728471
2017-12-10T05:10:14.077718: step 878, loss 0.947435, acc 0.796875, prec 0.0376339, recall 0.728471
2017-12-10T05:10:14.342312: step 879, loss 0.958969, acc 0.75, prec 0.0376939, recall 0.728947
2017-12-10T05:10:14.608574: step 880, loss 0.969022, acc 0.78125, prec 0.0377572, recall 0.729422
2017-12-10T05:10:14.873260: step 881, loss 0.817828, acc 0.796875, prec 0.0378222, recall 0.729895
2017-12-10T05:10:15.142798: step 882, loss 14.7143, acc 0.765625, prec 0.0378016, recall 0.727986
2017-12-10T05:10:15.412003: step 883, loss 0.930325, acc 0.8125, prec 0.0377811, recall 0.727986
2017-12-10T05:10:15.687211: step 884, loss 0.937728, acc 0.734375, prec 0.0377521, recall 0.727986
2017-12-10T05:10:15.949217: step 885, loss 0.719099, acc 0.765625, prec 0.0377265, recall 0.727986
2017-12-10T05:10:16.214884: step 886, loss 0.877254, acc 0.8125, prec 0.037706, recall 0.727986
2017-12-10T05:10:16.484167: step 887, loss 0.946259, acc 0.765625, prec 0.0376805, recall 0.727986
2017-12-10T05:10:16.745127: step 888, loss 0.846474, acc 0.78125, prec 0.0376567, recall 0.727986
2017-12-10T05:10:17.017667: step 889, loss 1.09775, acc 0.75, prec 0.0377163, recall 0.72846
2017-12-10T05:10:17.295828: step 890, loss 0.648491, acc 0.8125, prec 0.0377392, recall 0.728696
2017-12-10T05:10:17.564362: step 891, loss 4.85566, acc 0.6875, prec 0.037707, recall 0.728063
2017-12-10T05:10:17.836552: step 892, loss 1.43395, acc 0.765625, prec 0.0377248, recall 0.728299
2017-12-10T05:10:18.098117: step 893, loss 1.14484, acc 0.71875, prec 0.0377375, recall 0.728534
2017-12-10T05:10:18.362713: step 894, loss 0.819018, acc 0.671875, prec 0.037702, recall 0.728534
2017-12-10T05:10:18.631503: step 895, loss 1.22028, acc 0.703125, prec 0.0376699, recall 0.728534
2017-12-10T05:10:18.898057: step 896, loss 0.649155, acc 0.859375, prec 0.0376547, recall 0.728534
2017-12-10T05:10:19.161084: step 897, loss 0.796178, acc 0.75, prec 0.0376708, recall 0.728769
2017-12-10T05:10:19.427288: step 898, loss 0.933897, acc 0.75, prec 0.0376438, recall 0.728769
2017-12-10T05:10:19.691187: step 899, loss 0.64916, acc 0.8125, prec 0.0377097, recall 0.729239
2017-12-10T05:10:19.966414: step 900, loss 0.79141, acc 0.8125, prec 0.0376895, recall 0.729239

Evaluation:
2017-12-10T05:10:27.479630: step 900, loss 1.7137, acc 0.842801, prec 0.0394322, recall 0.724638

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-900

2017-12-10T05:10:28.777426: step 901, loss 0.517648, acc 0.875, prec 0.0394589, recall 0.724848
2017-12-10T05:10:29.044236: step 902, loss 0.871707, acc 0.78125, prec 0.0394759, recall 0.725057
2017-12-10T05:10:29.314910: step 903, loss 0.898581, acc 0.828125, prec 0.0394977, recall 0.725266
2017-12-10T05:10:29.580764: step 904, loss 1.84268, acc 0.8125, prec 0.0395195, recall 0.724924
2017-12-10T05:10:29.850047: step 905, loss 0.433229, acc 0.90625, prec 0.0395494, recall 0.725133
2017-12-10T05:10:30.113584: step 906, loss 0.238526, acc 0.90625, prec 0.0395794, recall 0.725341
2017-12-10T05:10:30.397321: step 907, loss 21.54, acc 0.859375, prec 0.0395663, recall 0.724792
2017-12-10T05:10:30.671598: step 908, loss 0.337592, acc 0.890625, prec 0.0395945, recall 0.725
2017-12-10T05:10:30.937725: step 909, loss 0.626907, acc 0.78125, prec 0.0395716, recall 0.725
2017-12-10T05:10:31.207916: step 910, loss 0.723675, acc 0.859375, prec 0.0395966, recall 0.725208
2017-12-10T05:10:31.470321: step 911, loss 0.658096, acc 0.84375, prec 0.0395802, recall 0.725208
2017-12-10T05:10:31.731362: step 912, loss 0.735363, acc 0.84375, prec 0.0395639, recall 0.725208
2017-12-10T05:10:31.996024: step 913, loss 0.727518, acc 0.796875, prec 0.0395427, recall 0.725208
2017-12-10T05:10:32.260591: step 914, loss 0.670213, acc 0.859375, prec 0.039528, recall 0.725208
2017-12-10T05:10:32.526452: step 915, loss 3.19949, acc 0.78125, prec 0.0396652, recall 0.72549
2017-12-10T05:10:32.790304: step 916, loss 1.47582, acc 0.859375, prec 0.0396521, recall 0.724943
2017-12-10T05:10:33.058709: step 917, loss 1.40988, acc 0.71875, prec 0.0396227, recall 0.724943
2017-12-10T05:10:33.322204: step 918, loss 0.904917, acc 0.828125, prec 0.0396443, recall 0.725151
2017-12-10T05:10:33.585882: step 919, loss 1.07594, acc 0.75, prec 0.0396182, recall 0.725151
2017-12-10T05:10:33.847403: step 920, loss 0.826746, acc 0.828125, prec 0.0396003, recall 0.725151
2017-12-10T05:10:34.118963: step 921, loss 2.5025, acc 0.765625, prec 0.0396564, recall 0.725019
2017-12-10T05:10:34.392921: step 922, loss 2.31994, acc 0.703125, prec 0.0398226, recall 0.726048
2017-12-10T05:10:34.663205: step 923, loss 1.04201, acc 0.734375, prec 0.0397949, recall 0.726048
2017-12-10T05:10:34.923488: step 924, loss 1.18885, acc 0.703125, prec 0.0398032, recall 0.726253
2017-12-10T05:10:35.192277: step 925, loss 1.20184, acc 0.71875, prec 0.0397739, recall 0.726253
2017-12-10T05:10:35.457045: step 926, loss 1.2113, acc 0.65625, prec 0.039856, recall 0.726866
2017-12-10T05:10:35.725920: step 927, loss 1.62406, acc 0.671875, prec 0.0398217, recall 0.726866
2017-12-10T05:10:35.990043: step 928, loss 1.90661, acc 0.59375, prec 0.0397795, recall 0.726866
2017-12-10T05:10:36.255710: step 929, loss 1.47239, acc 0.671875, prec 0.0397454, recall 0.726866
2017-12-10T05:10:36.520016: step 930, loss 5.09349, acc 0.703125, prec 0.0397162, recall 0.726324
2017-12-10T05:10:36.786358: step 931, loss 1.32769, acc 0.671875, prec 0.0396822, recall 0.726324
2017-12-10T05:10:37.053432: step 932, loss 1.63265, acc 0.671875, prec 0.0396483, recall 0.726324
2017-12-10T05:10:37.313366: step 933, loss 2.4506, acc 0.640625, prec 0.0396128, recall 0.725782
2017-12-10T05:10:37.575998: step 934, loss 1.14864, acc 0.734375, prec 0.0395854, recall 0.725782
2017-12-10T05:10:37.835588: step 935, loss 1.46044, acc 0.765625, prec 0.0396003, recall 0.725987
2017-12-10T05:10:38.097000: step 936, loss 0.626743, acc 0.8125, prec 0.039581, recall 0.725987
2017-12-10T05:10:38.362646: step 937, loss 0.952142, acc 0.765625, prec 0.0395959, recall 0.72619
2017-12-10T05:10:38.631560: step 938, loss 0.379573, acc 0.890625, prec 0.0396236, recall 0.726394
2017-12-10T05:10:38.896682: step 939, loss 0.951209, acc 0.734375, prec 0.0396353, recall 0.726597
2017-12-10T05:10:39.167101: step 940, loss 0.695901, acc 0.828125, prec 0.0396565, recall 0.7268
2017-12-10T05:10:39.432069: step 941, loss 0.661673, acc 0.84375, prec 0.0396793, recall 0.727003
2017-12-10T05:10:39.701017: step 942, loss 0.690117, acc 0.828125, prec 0.0396617, recall 0.727003
2017-12-10T05:10:39.970954: step 943, loss 0.298566, acc 0.890625, prec 0.0396504, recall 0.727003
2017-12-10T05:10:40.238758: step 944, loss 0.568396, acc 0.859375, prec 0.039636, recall 0.727003
2017-12-10T05:10:40.500270: step 945, loss 0.269439, acc 0.9375, prec 0.0396296, recall 0.727003
2017-12-10T05:10:40.768117: step 946, loss 0.402986, acc 0.921875, prec 0.0396604, recall 0.727205
2017-12-10T05:10:41.037862: step 947, loss 0.135729, acc 0.921875, prec 0.0396524, recall 0.727205
2017-12-10T05:10:41.308503: step 948, loss 0.248912, acc 0.90625, prec 0.0396816, recall 0.727407
2017-12-10T05:10:41.577033: step 949, loss 0.242242, acc 0.9375, prec 0.039714, recall 0.727609
2017-12-10T05:10:41.840329: step 950, loss 0.0274751, acc 0.984375, prec 0.0397124, recall 0.727609
2017-12-10T05:10:42.111744: step 951, loss 10.4492, acc 0.953125, prec 0.0397108, recall 0.726534
2017-12-10T05:10:42.385250: step 952, loss 0.519008, acc 0.921875, prec 0.0397803, recall 0.726937
2017-12-10T05:10:42.652988: step 953, loss 0.787547, acc 0.9375, prec 0.0398126, recall 0.727139
2017-12-10T05:10:42.920737: step 954, loss 0.29816, acc 0.875, prec 0.0397998, recall 0.727139
2017-12-10T05:10:43.195835: step 955, loss 0.460055, acc 0.828125, prec 0.0397821, recall 0.727139
2017-12-10T05:10:43.461931: step 956, loss 0.497235, acc 0.875, prec 0.039808, recall 0.72734
2017-12-10T05:10:43.725568: step 957, loss 0.572079, acc 0.84375, prec 0.039792, recall 0.72734
2017-12-10T05:10:43.993925: step 958, loss 0.726224, acc 0.8125, prec 0.0398114, recall 0.72754
2017-12-10T05:10:44.258577: step 959, loss 0.969943, acc 0.796875, prec 0.0398292, recall 0.727741
2017-12-10T05:10:44.518255: step 960, loss 0.558954, acc 0.84375, prec 0.0398132, recall 0.727741
2017-12-10T05:10:44.783443: step 961, loss 1.32821, acc 0.734375, prec 0.039786, recall 0.727741
2017-12-10T05:10:45.045613: step 962, loss 1.41826, acc 0.765625, prec 0.039762, recall 0.727741
2017-12-10T05:10:45.310661: step 963, loss 1.0621, acc 0.703125, prec 0.0397702, recall 0.727941
2017-12-10T05:10:45.586827: step 964, loss 1.05178, acc 0.78125, prec 0.0397479, recall 0.727941
2017-12-10T05:10:45.852723: step 965, loss 0.789643, acc 0.78125, prec 0.0397255, recall 0.727941
2017-12-10T05:10:46.113558: step 966, loss 0.701764, acc 0.796875, prec 0.0397818, recall 0.728341
2017-12-10T05:10:46.377367: step 967, loss 0.604293, acc 0.859375, prec 0.0397675, recall 0.728341
2017-12-10T05:10:46.646016: step 968, loss 0.425937, acc 0.84375, prec 0.0397516, recall 0.728341
2017-12-10T05:10:46.912046: step 969, loss 0.561833, acc 0.890625, prec 0.0397789, recall 0.72854
2017-12-10T05:10:47.174289: step 970, loss 0.406178, acc 0.859375, prec 0.0397645, recall 0.72854
2017-12-10T05:10:47.434634: step 971, loss 0.129992, acc 0.921875, prec 0.0397566, recall 0.72854
2017-12-10T05:10:47.701646: step 972, loss 3.62693, acc 0.890625, prec 0.039747, recall 0.728006
2017-12-10T05:10:47.966051: step 973, loss 7.17692, acc 0.875, prec 0.0397743, recall 0.727672
2017-12-10T05:10:48.241151: step 974, loss 8.98157, acc 0.796875, prec 0.0397552, recall 0.72714
2017-12-10T05:10:48.514573: step 975, loss 0.937688, acc 0.84375, prec 0.0397777, recall 0.727339
2017-12-10T05:10:48.783702: step 976, loss 0.827911, acc 0.796875, prec 0.0397571, recall 0.727339
2017-12-10T05:10:49.052977: step 977, loss 1.78049, acc 0.609375, prec 0.0397557, recall 0.727538
2017-12-10T05:10:49.311006: step 978, loss 0.801734, acc 0.78125, prec 0.0397718, recall 0.727737
2017-12-10T05:10:49.578760: step 979, loss 1.98699, acc 0.578125, prec 0.039729, recall 0.727737
2017-12-10T05:10:49.842573: step 980, loss 2.22731, acc 0.546875, prec 0.0397214, recall 0.727936
2017-12-10T05:10:50.114611: step 981, loss 2.11924, acc 0.609375, prec 0.0397201, recall 0.728134
2017-12-10T05:10:50.373301: step 982, loss 2.52175, acc 0.578125, prec 0.0397919, recall 0.728727
2017-12-10T05:10:50.640584: step 983, loss 2.64733, acc 0.53125, prec 0.0397826, recall 0.728924
2017-12-10T05:10:50.908495: step 984, loss 2.77072, acc 0.546875, prec 0.0397369, recall 0.728924
2017-12-10T05:10:51.167921: step 985, loss 2.18088, acc 0.546875, prec 0.0396913, recall 0.728924
2017-12-10T05:10:51.428443: step 986, loss 1.89409, acc 0.578125, prec 0.039649, recall 0.728924
2017-12-10T05:10:51.691153: step 987, loss 1.7318, acc 0.703125, prec 0.0397709, recall 0.72971
2017-12-10T05:10:51.956713: step 988, loss 1.38197, acc 0.765625, prec 0.0397853, recall 0.729906
2017-12-10T05:10:52.224462: step 989, loss 1.12349, acc 0.796875, prec 0.0398785, recall 0.730491
2017-12-10T05:10:52.490414: step 990, loss 1.06193, acc 0.78125, prec 0.0398943, recall 0.730686
2017-12-10T05:10:52.758319: step 991, loss 0.638854, acc 0.8125, prec 0.0398755, recall 0.730686
2017-12-10T05:10:53.023151: step 992, loss 0.917139, acc 0.75, prec 0.039926, recall 0.731074
2017-12-10T05:10:53.296406: step 993, loss 0.530814, acc 0.875, prec 0.0399512, recall 0.731268
2017-12-10T05:10:53.528017: step 994, loss 0.656797, acc 0.807692, prec 0.0400488, recall 0.731848
2017-12-10T05:10:53.805482: step 995, loss 3.06487, acc 0.84375, prec 0.0400346, recall 0.731322
2017-12-10T05:10:54.075454: step 996, loss 0.599039, acc 0.890625, prec 0.0400236, recall 0.731322
2017-12-10T05:10:54.343143: step 997, loss 7.76108, acc 0.84375, prec 0.0400472, recall 0.73099
2017-12-10T05:10:54.610867: step 998, loss 0.195124, acc 0.890625, prec 0.0400361, recall 0.73099
2017-12-10T05:10:54.872760: step 999, loss 0.520819, acc 0.859375, prec 0.0400597, recall 0.731183
2017-12-10T05:10:55.134368: step 1000, loss 1.05304, acc 0.78125, prec 0.0400754, recall 0.731375
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-1000

2017-12-10T05:10:56.468480: step 1001, loss 0.585438, acc 0.8125, prec 0.0400942, recall 0.731568
2017-12-10T05:10:56.735767: step 1002, loss 0.377363, acc 0.90625, prec 0.0400847, recall 0.731568
2017-12-10T05:10:57.001449: step 1003, loss 0.634772, acc 0.859375, prec 0.0401082, recall 0.73176
2017-12-10T05:10:57.281989: step 1004, loss 0.475581, acc 0.890625, prec 0.0400972, recall 0.73176
2017-12-10T05:10:57.546003: step 1005, loss 0.296315, acc 0.875, prec 0.0401599, recall 0.732143
2017-12-10T05:10:57.818560: step 1006, loss 0.607953, acc 0.828125, prec 0.0401426, recall 0.732143
2017-12-10T05:10:58.083312: step 1007, loss 0.167281, acc 0.921875, prec 0.0401347, recall 0.732143
2017-12-10T05:10:58.345510: step 1008, loss 1.38547, acc 0.90625, prec 0.0402004, recall 0.732525
2017-12-10T05:10:58.607062: step 1009, loss 3.0231, acc 0.84375, prec 0.0401863, recall 0.732003
2017-12-10T05:10:58.872112: step 1010, loss 0.355711, acc 0.890625, prec 0.0401753, recall 0.732003
2017-12-10T05:10:59.133801: step 1011, loss 0.671636, acc 0.84375, prec 0.0401595, recall 0.732003
2017-12-10T05:10:59.394808: step 1012, loss 3.73203, acc 0.875, prec 0.0402236, recall 0.731863
2017-12-10T05:10:59.655836: step 1013, loss 0.490531, acc 0.84375, prec 0.0402079, recall 0.731863
2017-12-10T05:10:59.916775: step 1014, loss 1.10586, acc 0.859375, prec 0.0403062, recall 0.732434
2017-12-10T05:11:00.189762: step 1015, loss 1.08672, acc 0.703125, prec 0.0402763, recall 0.732434
2017-12-10T05:11:00.460982: step 1016, loss 1.47485, acc 0.765625, prec 0.0403276, recall 0.732814
2017-12-10T05:11:00.728244: step 1017, loss 1.21984, acc 0.71875, prec 0.0403741, recall 0.733192
2017-12-10T05:11:00.989917: step 1018, loss 1.10466, acc 0.765625, prec 0.0403879, recall 0.73338
2017-12-10T05:11:01.255328: step 1019, loss 1.02299, acc 0.84375, prec 0.0403722, recall 0.73338
2017-12-10T05:11:01.523682: step 1020, loss 1.02579, acc 0.78125, prec 0.0403502, recall 0.73338
2017-12-10T05:11:01.791168: step 1021, loss 1.53418, acc 0.765625, prec 0.0404013, recall 0.733757
2017-12-10T05:11:02.057285: step 1022, loss 1.12622, acc 0.75, prec 0.0404507, recall 0.734133
2017-12-10T05:11:02.318307: step 1023, loss 0.736351, acc 0.75, prec 0.0404256, recall 0.734133
2017-12-10T05:11:02.579555: step 1024, loss 1.37665, acc 0.84375, prec 0.0405589, recall 0.73488
2017-12-10T05:11:02.847716: step 1025, loss 0.505281, acc 0.8125, prec 0.04054, recall 0.73488
2017-12-10T05:11:03.107554: step 1026, loss 7.23499, acc 0.8125, prec 0.0406715, recall 0.735109
2017-12-10T05:11:03.369085: step 1027, loss 0.403671, acc 0.890625, prec 0.0406605, recall 0.735109
2017-12-10T05:11:03.631179: step 1028, loss 1.37229, acc 0.78125, prec 0.0407499, recall 0.735664
2017-12-10T05:11:03.901590: step 1029, loss 1.68132, acc 0.71875, prec 0.0407587, recall 0.735849
2017-12-10T05:11:04.165367: step 1030, loss 1.15701, acc 0.671875, prec 0.0407256, recall 0.735849
2017-12-10T05:11:04.428201: step 1031, loss 1.0575, acc 0.765625, prec 0.040739, recall 0.736033
2017-12-10T05:11:04.688260: step 1032, loss 1.19175, acc 0.6875, prec 0.0407076, recall 0.736033
2017-12-10T05:11:04.948637: step 1033, loss 2.29604, acc 0.71875, prec 0.0407533, recall 0.736402
2017-12-10T05:11:05.212352: step 1034, loss 1.04292, acc 0.6875, prec 0.0407589, recall 0.736585
2017-12-10T05:11:05.471372: step 1035, loss 1.34144, acc 0.703125, prec 0.040766, recall 0.736769
2017-12-10T05:11:05.739023: step 1036, loss 1.31979, acc 0.671875, prec 0.0408438, recall 0.737318
2017-12-10T05:11:06.006485: step 1037, loss 1.00334, acc 0.6875, prec 0.0408493, recall 0.7375
2017-12-10T05:11:06.268945: step 1038, loss 1.09697, acc 0.6875, prec 0.0408179, recall 0.7375
2017-12-10T05:11:06.536789: step 1039, loss 1.03463, acc 0.78125, prec 0.0408328, recall 0.737682
2017-12-10T05:11:06.801344: step 1040, loss 2.63627, acc 0.796875, prec 0.040814, recall 0.737171
2017-12-10T05:11:07.072188: step 1041, loss 1.13497, acc 0.734375, prec 0.0407874, recall 0.737171
2017-12-10T05:11:07.340157: step 1042, loss 1.37878, acc 0.796875, prec 0.040767, recall 0.737171
2017-12-10T05:11:07.602674: step 1043, loss 0.91272, acc 0.78125, prec 0.0407451, recall 0.737171
2017-12-10T05:11:07.876072: step 1044, loss 0.833616, acc 0.75, prec 0.0407569, recall 0.737353
2017-12-10T05:11:08.143203: step 1045, loss 0.401884, acc 0.859375, prec 0.0407796, recall 0.737535
2017-12-10T05:11:08.405741: step 1046, loss 0.512291, acc 0.796875, prec 0.0408694, recall 0.738079
2017-12-10T05:11:08.670631: step 1047, loss 0.663591, acc 0.828125, prec 0.0408522, recall 0.738079
2017-12-10T05:11:08.933877: step 1048, loss 0.599655, acc 0.8125, prec 0.0408335, recall 0.738079
2017-12-10T05:11:09.205882: step 1049, loss 0.663959, acc 0.859375, prec 0.0408194, recall 0.738079
2017-12-10T05:11:09.474281: step 1050, loss 30.4265, acc 0.84375, prec 0.0408452, recall 0.736733
2017-12-10T05:11:09.747311: step 1051, loss 0.210485, acc 0.953125, prec 0.0409138, recall 0.737096
2017-12-10T05:11:10.012335: step 1052, loss 0.532096, acc 0.84375, prec 0.0408982, recall 0.737096
2017-12-10T05:11:10.278967: step 1053, loss 1.43541, acc 0.71875, prec 0.0409067, recall 0.737276
2017-12-10T05:11:10.539533: step 1054, loss 1.09401, acc 0.8125, prec 0.0409977, recall 0.737817
2017-12-10T05:11:10.808053: step 1055, loss 2.02814, acc 0.546875, prec 0.0409524, recall 0.737817
2017-12-10T05:11:11.072976: step 1056, loss 1.28351, acc 0.640625, prec 0.0409165, recall 0.737817
2017-12-10T05:11:11.343519: step 1057, loss 1.40183, acc 0.625, prec 0.0409157, recall 0.737997
2017-12-10T05:11:11.614758: step 1058, loss 1.48443, acc 0.609375, prec 0.0408768, recall 0.737997
2017-12-10T05:11:11.882135: step 1059, loss 1.86893, acc 0.59375, prec 0.0409457, recall 0.738535
2017-12-10T05:11:12.160500: step 1060, loss 2.26642, acc 0.546875, prec 0.040937, recall 0.738714
2017-12-10T05:11:12.427207: step 1061, loss 1.7852, acc 0.65625, prec 0.0409029, recall 0.738714
2017-12-10T05:11:12.698370: step 1062, loss 1.59309, acc 0.609375, prec 0.0408642, recall 0.738714
2017-12-10T05:11:12.961503: step 1063, loss 1.38016, acc 0.6875, prec 0.0408696, recall 0.738893
2017-12-10T05:11:13.224929: step 1064, loss 1.66389, acc 0.65625, prec 0.0409081, recall 0.739249
2017-12-10T05:11:13.490548: step 1065, loss 1.16767, acc 0.703125, prec 0.0409511, recall 0.739605
2017-12-10T05:11:13.753905: step 1066, loss 0.857097, acc 0.78125, prec 0.041038, recall 0.740136
2017-12-10T05:11:14.016726: step 1067, loss 1.03616, acc 0.6875, prec 0.0410071, recall 0.740136
2017-12-10T05:11:14.291939: step 1068, loss 5.67329, acc 0.859375, prec 0.0409947, recall 0.739633
2017-12-10T05:11:14.562186: step 1069, loss 0.24475, acc 0.90625, prec 0.0410216, recall 0.73981
2017-12-10T05:11:14.825904: step 1070, loss 0.799048, acc 0.828125, prec 0.0410046, recall 0.73981
2017-12-10T05:11:15.096840: step 1071, loss 1.2095, acc 0.734375, prec 0.0410144, recall 0.739986
2017-12-10T05:11:15.368370: step 1072, loss 0.812346, acc 0.828125, prec 0.0410335, recall 0.740163
2017-12-10T05:11:15.630774: step 1073, loss 1.19349, acc 0.90625, prec 0.0411325, recall 0.740691
2017-12-10T05:11:15.898362: step 1074, loss 0.960928, acc 0.8125, prec 0.0411139, recall 0.740691
2017-12-10T05:11:16.160572: step 1075, loss 0.501334, acc 0.90625, prec 0.0411046, recall 0.740691
2017-12-10T05:11:16.424445: step 1076, loss 1.92713, acc 0.890625, prec 0.0410954, recall 0.740189
2017-12-10T05:11:16.689960: step 1077, loss 0.843658, acc 0.8125, prec 0.0410769, recall 0.740189
2017-12-10T05:11:16.955733: step 1078, loss 0.829723, acc 0.796875, prec 0.0411648, recall 0.740716
2017-12-10T05:11:17.224413: step 1079, loss 0.73381, acc 0.8125, prec 0.0411822, recall 0.740891
2017-12-10T05:11:17.490829: step 1080, loss 0.65027, acc 0.828125, prec 0.0412012, recall 0.741065
2017-12-10T05:11:17.753309: step 1081, loss 0.465139, acc 0.890625, prec 0.0412263, recall 0.74124
2017-12-10T05:11:18.025635: step 1082, loss 0.733257, acc 0.765625, prec 0.0412031, recall 0.74124
2017-12-10T05:11:18.291808: step 1083, loss 0.568037, acc 0.875, prec 0.0411908, recall 0.74124
2017-12-10T05:11:18.557348: step 1084, loss 0.418183, acc 0.828125, prec 0.0411738, recall 0.74124
2017-12-10T05:11:18.821365: step 1085, loss 0.410066, acc 0.90625, prec 0.0411646, recall 0.74124
2017-12-10T05:11:19.089698: step 1086, loss 0.363949, acc 0.90625, prec 0.0411912, recall 0.741414
2017-12-10T05:11:19.354094: step 1087, loss 0.463697, acc 0.875, prec 0.0411789, recall 0.741414
2017-12-10T05:11:19.620967: step 1088, loss 1.05037, acc 0.875, prec 0.0412024, recall 0.741588
2017-12-10T05:11:19.882843: step 1089, loss 0.252767, acc 0.90625, prec 0.041229, recall 0.741762
2017-12-10T05:11:20.148082: step 1090, loss 1.38655, acc 0.921875, prec 0.0412571, recall 0.741935
2017-12-10T05:11:20.413368: step 1091, loss 0.3101, acc 0.921875, prec 0.0412494, recall 0.741935
2017-12-10T05:11:20.674175: step 1092, loss 0.46588, acc 0.859375, prec 0.0412714, recall 0.742109
2017-12-10T05:11:20.937303: step 1093, loss 0.377442, acc 0.921875, prec 0.0412637, recall 0.742109
2017-12-10T05:11:21.209852: step 1094, loss 0.243019, acc 0.875, prec 0.0412514, recall 0.742109
2017-12-10T05:11:21.472102: step 1095, loss 0.448948, acc 0.875, prec 0.0413106, recall 0.742455
2017-12-10T05:11:21.749207: step 1096, loss 0.428477, acc 0.90625, prec 0.0413013, recall 0.742455
2017-12-10T05:11:22.010559: step 1097, loss 0.133475, acc 0.921875, prec 0.0412936, recall 0.742455
2017-12-10T05:11:22.271996: step 1098, loss 0.056279, acc 0.984375, prec 0.0413279, recall 0.742627
2017-12-10T05:11:22.534584: step 1099, loss 0.431673, acc 0.875, prec 0.0413513, recall 0.7428
2017-12-10T05:11:22.797942: step 1100, loss 0.177566, acc 0.953125, prec 0.0413467, recall 0.7428
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-1100

2017-12-10T05:11:24.092054: step 1101, loss 0.168794, acc 0.921875, prec 0.041339, recall 0.7428
2017-12-10T05:11:24.356150: step 1102, loss 0.427525, acc 0.90625, prec 0.0413654, recall 0.742972
2017-12-10T05:11:24.617087: step 1103, loss 0.270041, acc 0.953125, prec 0.0413608, recall 0.742972
2017-12-10T05:11:24.878142: step 1104, loss 2.0131, acc 0.96875, prec 0.0413593, recall 0.742475
2017-12-10T05:11:25.156022: step 1105, loss 0.319397, acc 0.9375, prec 0.0413888, recall 0.742647
2017-12-10T05:11:25.417649: step 1106, loss 0.776145, acc 0.875, prec 0.0414836, recall 0.743162
2017-12-10T05:11:25.679531: step 1107, loss 0.280261, acc 0.9375, prec 0.0414774, recall 0.743162
2017-12-10T05:11:25.945195: step 1108, loss 0.250322, acc 0.921875, prec 0.0415054, recall 0.743333
2017-12-10T05:11:26.215610: step 1109, loss 0.455346, acc 0.875, prec 0.041493, recall 0.743333
2017-12-10T05:11:26.476945: step 1110, loss 0.355362, acc 0.90625, prec 0.0414837, recall 0.743333
2017-12-10T05:11:26.743401: step 1111, loss 0.626038, acc 0.921875, prec 0.0415117, recall 0.743504
2017-12-10T05:11:27.012775: step 1112, loss 1.10743, acc 0.96875, prec 0.0415799, recall 0.743846
2017-12-10T05:11:27.291497: step 1113, loss 0.13393, acc 0.953125, prec 0.0416465, recall 0.744186
2017-12-10T05:11:27.557928: step 1114, loss 0.470601, acc 0.90625, prec 0.0417085, recall 0.744526
2017-12-10T05:11:27.828862: step 1115, loss 0.12907, acc 0.953125, prec 0.0417038, recall 0.744526
2017-12-10T05:11:28.093075: step 1116, loss 0.278677, acc 0.921875, prec 0.0416961, recall 0.744526
2017-12-10T05:11:28.360722: step 1117, loss 0.0417791, acc 1, prec 0.0417673, recall 0.744864
2017-12-10T05:11:28.623553: step 1118, loss 0.704496, acc 0.890625, prec 0.0418276, recall 0.745202
2017-12-10T05:11:28.890534: step 1119, loss 0.513647, acc 0.875, prec 0.0419219, recall 0.745707
2017-12-10T05:11:29.164737: step 1120, loss 0.380633, acc 0.90625, prec 0.0419482, recall 0.745875
2017-12-10T05:11:29.436802: step 1121, loss 0.687299, acc 0.890625, prec 0.0419728, recall 0.746042
2017-12-10T05:11:29.700195: step 1122, loss 1.00736, acc 0.796875, prec 0.0419526, recall 0.746042
2017-12-10T05:11:29.966355: step 1123, loss 0.303311, acc 0.9375, prec 0.0419464, recall 0.746042
2017-12-10T05:11:30.230949: step 1124, loss 0.228539, acc 0.9375, prec 0.0419401, recall 0.746042
2017-12-10T05:11:30.505714: step 1125, loss 0.646881, acc 0.8125, prec 0.0419215, recall 0.746042
2017-12-10T05:11:30.780087: step 1126, loss 0.61915, acc 0.828125, prec 0.0419044, recall 0.746042
2017-12-10T05:11:31.045228: step 1127, loss 0.347476, acc 0.890625, prec 0.041929, recall 0.74621
2017-12-10T05:11:31.312975: step 1128, loss 0.66626, acc 0.875, prec 0.0419521, recall 0.746377
2017-12-10T05:11:31.576546: step 1129, loss 0.184085, acc 0.921875, prec 0.0419443, recall 0.746377
2017-12-10T05:11:31.841871: step 1130, loss 0.512574, acc 0.828125, prec 0.0419272, recall 0.746377
2017-12-10T05:11:32.109733: step 1131, loss 6.72383, acc 0.890625, prec 0.0419888, recall 0.74622
2017-12-10T05:11:32.374627: step 1132, loss 0.263471, acc 0.890625, prec 0.0420134, recall 0.746386
2017-12-10T05:11:32.640787: step 1133, loss 3.5453, acc 0.859375, prec 0.0421072, recall 0.746396
2017-12-10T05:11:32.906741: step 1134, loss 8.44619, acc 0.890625, prec 0.0421333, recall 0.746073
2017-12-10T05:11:33.175359: step 1135, loss 1.10817, acc 0.671875, prec 0.042136, recall 0.746239
2017-12-10T05:11:33.448410: step 1136, loss 1.18031, acc 0.78125, prec 0.0421496, recall 0.746405
2017-12-10T05:11:33.719508: step 1137, loss 1.0576, acc 0.78125, prec 0.0421278, recall 0.746405
2017-12-10T05:11:33.985086: step 1138, loss 1.41287, acc 0.671875, prec 0.0420952, recall 0.746405
2017-12-10T05:11:34.248451: step 1139, loss 2.03658, acc 0.5625, prec 0.0420518, recall 0.746405
2017-12-10T05:11:34.507142: step 1140, loss 2.02395, acc 0.625, prec 0.0420851, recall 0.746736
2017-12-10T05:11:34.776720: step 1141, loss 2.61646, acc 0.390625, prec 0.0420952, recall 0.747066
2017-12-10T05:11:35.037852: step 1142, loss 2.17798, acc 0.59375, prec 0.042055, recall 0.747066
2017-12-10T05:11:35.301284: step 1143, loss 1.81679, acc 0.515625, prec 0.0420073, recall 0.747066
2017-12-10T05:11:35.565835: step 1144, loss 2.20969, acc 0.546875, prec 0.0420328, recall 0.747396
2017-12-10T05:11:35.827708: step 1145, loss 1.66757, acc 0.65625, prec 0.041999, recall 0.747396
2017-12-10T05:11:36.097246: step 1146, loss 1.74837, acc 0.6875, prec 0.0420033, recall 0.74756
2017-12-10T05:11:36.359139: step 1147, loss 1.02697, acc 0.765625, prec 0.0420153, recall 0.747724
2017-12-10T05:11:36.625797: step 1148, loss 1.26548, acc 0.640625, prec 0.04198, recall 0.747724
2017-12-10T05:11:36.899405: step 1149, loss 1.01675, acc 0.75, prec 0.0419904, recall 0.747888
2017-12-10T05:11:37.168280: step 1150, loss 0.682393, acc 0.796875, prec 0.0420055, recall 0.748052
2017-12-10T05:11:37.432435: step 1151, loss 0.715188, acc 0.78125, prec 0.0420189, recall 0.748215
2017-12-10T05:11:37.704002: step 1152, loss 0.757107, acc 0.859375, prec 0.0421448, recall 0.748867
2017-12-10T05:11:37.971702: step 1153, loss 3.399, acc 0.796875, prec 0.0421961, recall 0.748708
2017-12-10T05:11:38.241345: step 1154, loss 0.691454, acc 0.859375, prec 0.0422171, recall 0.74887
2017-12-10T05:11:38.505040: step 1155, loss 0.59531, acc 0.859375, prec 0.0422033, recall 0.74887
2017-12-10T05:11:38.773052: step 1156, loss 0.227178, acc 0.90625, prec 0.0421941, recall 0.74887
2017-12-10T05:11:39.038843: step 1157, loss 0.56829, acc 0.84375, prec 0.0421788, recall 0.74887
2017-12-10T05:11:39.302537: step 1158, loss 0.537883, acc 0.84375, prec 0.0421982, recall 0.749032
2017-12-10T05:11:39.569407: step 1159, loss 0.0823727, acc 0.984375, prec 0.0422315, recall 0.749194
2017-12-10T05:11:39.836594: step 1160, loss 8.38122, acc 0.90625, prec 0.0422586, recall 0.748873
2017-12-10T05:11:40.105660: step 1161, loss 5.92227, acc 0.890625, prec 0.0422494, recall 0.748391
2017-12-10T05:11:40.370980: step 1162, loss 0.256199, acc 0.90625, prec 0.0422402, recall 0.748391
2017-12-10T05:11:40.635283: step 1163, loss 0.571659, acc 0.859375, prec 0.0422264, recall 0.748391
2017-12-10T05:11:40.900904: step 1164, loss 0.524249, acc 0.828125, prec 0.0422096, recall 0.748391
2017-12-10T05:11:41.166358: step 1165, loss 0.622114, acc 0.765625, prec 0.0421866, recall 0.748391
2017-12-10T05:11:41.427768: step 1166, loss 1.11079, acc 0.765625, prec 0.0422678, recall 0.748876
2017-12-10T05:11:41.690503: step 1167, loss 1.27958, acc 0.78125, prec 0.0422811, recall 0.749037
2017-12-10T05:11:41.959569: step 1168, loss 1.0346, acc 0.703125, prec 0.0422867, recall 0.749198
2017-12-10T05:11:42.238987: step 1169, loss 1.22629, acc 0.78125, prec 0.0424039, recall 0.74984
2017-12-10T05:11:42.508099: step 1170, loss 1.62463, acc 0.6875, prec 0.042477, recall 0.750319
2017-12-10T05:11:42.771825: step 1171, loss 1.02045, acc 0.75, prec 0.0425217, recall 0.750638
2017-12-10T05:11:43.036999: step 1172, loss 0.924264, acc 0.78125, prec 0.0425348, recall 0.750797
2017-12-10T05:11:43.298412: step 1173, loss 1.12312, acc 0.71875, prec 0.0425417, recall 0.750955
2017-12-10T05:11:43.565105: step 1174, loss 1.18941, acc 0.6875, prec 0.042511, recall 0.750955
2017-12-10T05:11:43.828315: step 1175, loss 0.471811, acc 0.859375, prec 0.0425662, recall 0.751272
2017-12-10T05:11:44.094735: step 1176, loss 0.894353, acc 0.828125, prec 0.0425839, recall 0.75143
2017-12-10T05:11:44.354591: step 1177, loss 0.565819, acc 0.78125, prec 0.0425624, recall 0.75143
2017-12-10T05:11:44.617413: step 1178, loss 0.876054, acc 0.84375, prec 0.0425815, recall 0.751588
2017-12-10T05:11:44.882846: step 1179, loss 0.510191, acc 0.828125, prec 0.0425991, recall 0.751746
2017-12-10T05:11:45.145747: step 1180, loss 0.592312, acc 0.859375, prec 0.0426198, recall 0.751904
2017-12-10T05:11:45.414671: step 1181, loss 0.478531, acc 0.90625, prec 0.042645, recall 0.752061
2017-12-10T05:11:45.678361: step 1182, loss 0.401944, acc 0.859375, prec 0.0426656, recall 0.752218
2017-12-10T05:11:45.940549: step 1183, loss 0.170979, acc 0.953125, prec 0.042661, recall 0.752218
2017-12-10T05:11:46.203832: step 1184, loss 0.167067, acc 0.921875, prec 0.0427221, recall 0.752532
2017-12-10T05:11:46.467258: step 1185, loss 0.226936, acc 0.921875, prec 0.0427489, recall 0.752688
2017-12-10T05:11:46.729329: step 1186, loss 0.213982, acc 0.9375, prec 0.0427427, recall 0.752688
2017-12-10T05:11:47.000474: step 1187, loss 0.38201, acc 0.828125, prec 0.0427602, recall 0.752845
2017-12-10T05:11:47.263668: step 1188, loss 0.490456, acc 0.921875, prec 0.0427525, recall 0.752845
2017-12-10T05:11:47.528916: step 1189, loss 4.12272, acc 0.90625, prec 0.0427449, recall 0.752369
2017-12-10T05:11:47.802337: step 1190, loss 0.261184, acc 0.9375, prec 0.0427387, recall 0.752369
2017-12-10T05:11:48.062789: step 1191, loss 1.24011, acc 0.9375, prec 0.0428013, recall 0.752681
2017-12-10T05:11:48.331363: step 1192, loss 0.429094, acc 0.9375, prec 0.0428638, recall 0.752993
2017-12-10T05:11:48.598972: step 1193, loss 0.186513, acc 0.921875, prec 0.0429591, recall 0.753459
2017-12-10T05:11:48.865679: step 1194, loss 0.218752, acc 0.9375, prec 0.0429529, recall 0.753459
2017-12-10T05:11:49.142695: step 1195, loss 0.35359, acc 0.921875, prec 0.0429795, recall 0.753614
2017-12-10T05:11:49.406452: step 1196, loss 0.416557, acc 0.90625, prec 0.0429703, recall 0.753614
2017-12-10T05:11:49.674858: step 1197, loss 0.624955, acc 0.828125, prec 0.0429534, recall 0.753614
2017-12-10T05:11:49.938068: step 1198, loss 0.483506, acc 0.875, prec 0.0429753, recall 0.753769
2017-12-10T05:11:50.210011: step 1199, loss 0.13668, acc 0.9375, prec 0.0429692, recall 0.753769
2017-12-10T05:11:50.480681: step 1200, loss 1.41943, acc 0.90625, prec 0.0430627, recall 0.754232

Evaluation:
2017-12-10T05:11:58.202127: step 1200, loss 2.27667, acc 0.889979, prec 0.0442353, recall 0.736

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-1200

2017-12-10T05:11:59.456255: step 1201, loss 1.40514, acc 0.84375, prec 0.0442858, recall 0.736301
2017-12-10T05:11:59.727915: step 1202, loss 0.52676, acc 0.875, prec 0.0443064, recall 0.736452
2017-12-10T05:11:59.990217: step 1203, loss 0.555976, acc 0.828125, prec 0.0442897, recall 0.736452
2017-12-10T05:12:00.253468: step 1204, loss 0.393326, acc 0.859375, prec 0.0443088, recall 0.736602
2017-12-10T05:12:00.525649: step 1205, loss 0.506168, acc 0.875, prec 0.0442966, recall 0.736602
2017-12-10T05:12:00.792780: step 1206, loss 0.586288, acc 0.796875, prec 0.0442769, recall 0.736602
2017-12-10T05:12:01.061321: step 1207, loss 0.780243, acc 0.8125, prec 0.0442587, recall 0.736602
2017-12-10T05:12:01.326217: step 1208, loss 1.2088, acc 0.765625, prec 0.044236, recall 0.736602
2017-12-10T05:12:01.592857: step 1209, loss 0.422965, acc 0.84375, prec 0.0442535, recall 0.736752
2017-12-10T05:12:01.855306: step 1210, loss 3.55863, acc 0.859375, prec 0.0442741, recall 0.736483
2017-12-10T05:12:02.119982: step 1211, loss 1.77419, acc 0.859375, prec 0.0442947, recall 0.736214
2017-12-10T05:12:02.382075: step 1212, loss 0.711256, acc 0.78125, prec 0.0442735, recall 0.736214
2017-12-10T05:12:02.642781: step 1213, loss 0.518363, acc 0.796875, prec 0.0442865, recall 0.736364
2017-12-10T05:12:02.914748: step 1214, loss 0.67184, acc 0.84375, prec 0.0442714, recall 0.736364
2017-12-10T05:12:03.180051: step 1215, loss 1.68157, acc 0.734375, prec 0.0442783, recall 0.736513
2017-12-10T05:12:03.448570: step 1216, loss 0.937536, acc 0.78125, prec 0.0442571, recall 0.736513
2017-12-10T05:12:03.715499: step 1217, loss 1.58961, acc 0.703125, prec 0.0442285, recall 0.736513
2017-12-10T05:12:03.987332: step 1218, loss 1.01155, acc 0.78125, prec 0.0442725, recall 0.736812
2017-12-10T05:12:04.258147: step 1219, loss 1.13019, acc 0.78125, prec 0.0443491, recall 0.737259
2017-12-10T05:12:04.523746: step 1220, loss 1.31619, acc 0.6875, prec 0.0443189, recall 0.737259
2017-12-10T05:12:04.788674: step 1221, loss 0.959169, acc 0.78125, prec 0.0443303, recall 0.737408
2017-12-10T05:12:05.058720: step 1222, loss 0.896223, acc 0.796875, prec 0.0443107, recall 0.737408
2017-12-10T05:12:05.325319: step 1223, loss 5.16358, acc 0.8125, prec 0.0443266, recall 0.73714
2017-12-10T05:12:05.594009: step 1224, loss 0.854274, acc 0.84375, prec 0.044344, recall 0.737288
2017-12-10T05:12:05.861399: step 1225, loss 0.425139, acc 0.890625, prec 0.0443335, recall 0.737288
2017-12-10T05:12:06.129503: step 1226, loss 0.760911, acc 0.84375, prec 0.0443509, recall 0.737436
2017-12-10T05:12:06.393247: step 1227, loss 0.781557, acc 0.78125, prec 0.0443298, recall 0.737436
2017-12-10T05:12:06.661047: step 1228, loss 0.409943, acc 0.890625, prec 0.0443193, recall 0.737436
2017-12-10T05:12:06.925075: step 1229, loss 0.357074, acc 0.890625, prec 0.0443412, recall 0.737585
2017-12-10T05:12:07.188391: step 1230, loss 0.278642, acc 0.875, prec 0.0443291, recall 0.737585
2017-12-10T05:12:07.458448: step 1231, loss 8.48936, acc 0.84375, prec 0.044348, recall 0.737317
2017-12-10T05:12:07.725560: step 1232, loss 0.889036, acc 0.78125, prec 0.044327, recall 0.737317
2017-12-10T05:12:07.993491: step 1233, loss 0.3425, acc 0.890625, prec 0.0443488, recall 0.737465
2017-12-10T05:12:08.254170: step 1234, loss 1.76836, acc 0.859375, prec 0.0443368, recall 0.73705
2017-12-10T05:12:08.526469: step 1235, loss 9.32979, acc 0.875, prec 0.0443263, recall 0.736635
2017-12-10T05:12:08.795339: step 1236, loss 0.57977, acc 0.875, prec 0.0443143, recall 0.736635
2017-12-10T05:12:09.061678: step 1237, loss 0.782505, acc 0.8125, prec 0.044361, recall 0.736931
2017-12-10T05:12:09.326158: step 1238, loss 1.17763, acc 0.75, prec 0.044337, recall 0.736931
2017-12-10T05:12:09.589394: step 1239, loss 1.23862, acc 0.71875, prec 0.0443423, recall 0.737079
2017-12-10T05:12:09.864653: step 1240, loss 1.14361, acc 0.703125, prec 0.0443138, recall 0.737079
2017-12-10T05:12:10.124264: step 1241, loss 1.22039, acc 0.75, prec 0.0443544, recall 0.737374
2017-12-10T05:12:10.392700: step 1242, loss 1.00818, acc 0.75, prec 0.044395, recall 0.737668
2017-12-10T05:12:10.658441: step 1243, loss 1.27717, acc 0.734375, prec 0.0443695, recall 0.737668
2017-12-10T05:12:10.922463: step 1244, loss 1.85541, acc 0.609375, prec 0.0443322, recall 0.737668
2017-12-10T05:12:11.184596: step 1245, loss 1.35824, acc 0.6875, prec 0.0443345, recall 0.737815
2017-12-10T05:12:11.448621: step 1246, loss 1.40214, acc 0.765625, prec 0.0443443, recall 0.737962
2017-12-10T05:12:11.709425: step 1247, loss 0.77261, acc 0.84375, prec 0.0443293, recall 0.737962
2017-12-10T05:12:11.971278: step 1248, loss 1.34118, acc 0.765625, prec 0.044307, recall 0.737962
2017-12-10T05:12:12.244898: step 1249, loss 1.40516, acc 0.6875, prec 0.0442772, recall 0.737962
2017-12-10T05:12:12.508152: step 1250, loss 1.98716, acc 0.765625, prec 0.0442564, recall 0.737549
2017-12-10T05:12:12.770991: step 1251, loss 0.523949, acc 0.78125, prec 0.0442677, recall 0.737696
2017-12-10T05:12:13.034568: step 1252, loss 0.713939, acc 0.8125, prec 0.0442819, recall 0.737842
2017-12-10T05:12:13.300464: step 1253, loss 0.6495, acc 0.859375, prec 0.0443006, recall 0.737989
2017-12-10T05:12:13.572685: step 1254, loss 0.651033, acc 0.859375, prec 0.0443193, recall 0.738135
2017-12-10T05:12:13.844836: step 1255, loss 0.20267, acc 0.953125, prec 0.0443148, recall 0.738135
2017-12-10T05:12:14.107674: step 1256, loss 0.910352, acc 0.796875, prec 0.0442955, recall 0.738135
2017-12-10T05:12:14.368866: step 1257, loss 0.671216, acc 0.859375, prec 0.0443782, recall 0.738573
2017-12-10T05:12:14.641482: step 1258, loss 0.493534, acc 0.875, prec 0.0443663, recall 0.738573
2017-12-10T05:12:14.907318: step 1259, loss 9.59553, acc 0.890625, prec 0.0443574, recall 0.738162
2017-12-10T05:12:15.185343: step 1260, loss 2.49542, acc 0.84375, prec 0.044344, recall 0.737751
2017-12-10T05:12:15.452536: step 1261, loss 0.430196, acc 0.859375, prec 0.0443627, recall 0.737897
2017-12-10T05:12:15.712152: step 1262, loss 0.801778, acc 0.8125, prec 0.0443768, recall 0.738042
2017-12-10T05:12:15.974863: step 1263, loss 0.393131, acc 0.828125, prec 0.0443605, recall 0.738042
2017-12-10T05:12:16.238993: step 1264, loss 0.593317, acc 0.84375, prec 0.0443457, recall 0.738042
2017-12-10T05:12:16.503827: step 1265, loss 0.939172, acc 0.8125, prec 0.0443598, recall 0.738188
2017-12-10T05:12:16.767997: step 1266, loss 0.875393, acc 0.78125, prec 0.044371, recall 0.738333
2017-12-10T05:12:17.039699: step 1267, loss 0.702119, acc 0.796875, prec 0.0443836, recall 0.738479
2017-12-10T05:12:17.300281: step 1268, loss 8.50161, acc 0.796875, prec 0.0443977, recall 0.738214
2017-12-10T05:12:17.566417: step 1269, loss 0.790348, acc 0.84375, prec 0.0444467, recall 0.738504
2017-12-10T05:12:17.829432: step 1270, loss 0.88985, acc 0.765625, prec 0.0444563, recall 0.738649
2017-12-10T05:12:18.100357: step 1271, loss 0.946221, acc 0.8125, prec 0.0444704, recall 0.738794
2017-12-10T05:12:18.364532: step 1272, loss 1.24959, acc 0.640625, prec 0.0444681, recall 0.738938
2017-12-10T05:12:18.628174: step 1273, loss 1.05449, acc 0.6875, prec 0.0445021, recall 0.739227
2017-12-10T05:12:18.893941: step 1274, loss 0.308972, acc 0.859375, prec 0.0444888, recall 0.739227
2017-12-10T05:12:19.157805: step 1275, loss 0.658545, acc 0.765625, prec 0.0444984, recall 0.739371
2017-12-10T05:12:19.421975: step 1276, loss 0.816973, acc 0.796875, prec 0.0445426, recall 0.739658
2017-12-10T05:12:19.682593: step 1277, loss 1.07811, acc 0.75, prec 0.0445507, recall 0.739802
2017-12-10T05:12:19.952681: step 1278, loss 0.464898, acc 0.8125, prec 0.0445646, recall 0.739945
2017-12-10T05:12:20.212743: step 1279, loss 0.604152, acc 0.859375, prec 0.0445513, recall 0.739945
2017-12-10T05:12:20.477660: step 1280, loss 0.189041, acc 0.890625, prec 0.0445727, recall 0.740088
2017-12-10T05:12:20.746955: step 1281, loss 0.748265, acc 0.90625, prec 0.0446272, recall 0.740374
2017-12-10T05:12:21.016696: step 1282, loss 0.374939, acc 0.828125, prec 0.0446109, recall 0.740374
2017-12-10T05:12:21.280010: step 1283, loss 0.455334, acc 0.875, prec 0.0446307, recall 0.740517
2017-12-10T05:12:21.546599: step 1284, loss 0.208333, acc 0.921875, prec 0.044655, recall 0.740659
2017-12-10T05:12:21.818309: step 1285, loss 0.131597, acc 0.9375, prec 0.0446491, recall 0.740659
2017-12-10T05:12:22.083770: step 1286, loss 2.22124, acc 0.921875, prec 0.0446748, recall 0.740395
2017-12-10T05:12:22.351898: step 1287, loss 0.147202, acc 0.9375, prec 0.0447321, recall 0.74068
2017-12-10T05:12:22.616669: step 1288, loss 0.569683, acc 0.921875, prec 0.0447564, recall 0.740822
2017-12-10T05:12:22.880889: step 1289, loss 0.138826, acc 0.9375, prec 0.044782, recall 0.740964
2017-12-10T05:12:23.161500: step 1290, loss 0.306794, acc 0.90625, prec 0.0447732, recall 0.740964
2017-12-10T05:12:23.425091: step 1291, loss 0.115186, acc 0.953125, prec 0.0448003, recall 0.741106
2017-12-10T05:12:23.692964: step 1292, loss 0.406493, acc 0.90625, prec 0.044823, recall 0.741247
2017-12-10T05:12:23.955602: step 1293, loss 9.93929, acc 0.875, prec 0.0448141, recall 0.740437
2017-12-10T05:12:24.223937: step 1294, loss 0.686281, acc 0.921875, prec 0.0448383, recall 0.740579
2017-12-10T05:12:24.498357: step 1295, loss 3.44031, acc 0.859375, prec 0.044858, recall 0.740316
2017-12-10T05:12:24.762373: step 1296, loss 0.442329, acc 0.84375, prec 0.0448432, recall 0.740316
2017-12-10T05:12:25.026389: step 1297, loss 1.5734, acc 0.65625, prec 0.0448422, recall 0.740458
2017-12-10T05:12:25.296815: step 1298, loss 1.62073, acc 0.65625, prec 0.0448096, recall 0.740458
2017-12-10T05:12:25.565179: step 1299, loss 1.7893, acc 0.578125, prec 0.0448327, recall 0.740741
2017-12-10T05:12:25.835320: step 1300, loss 1.57881, acc 0.65625, prec 0.0448631, recall 0.741023
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-1300

2017-12-10T05:12:27.143002: step 1301, loss 1.5566, acc 0.5625, prec 0.0448218, recall 0.741023
2017-12-10T05:12:27.410850: step 1302, loss 1.72219, acc 0.5625, prec 0.0448119, recall 0.741164
2017-12-10T05:12:27.675285: step 1303, loss 1.62038, acc 0.640625, prec 0.0448408, recall 0.741445
2017-12-10T05:12:27.938451: step 1304, loss 1.66735, acc 0.671875, prec 0.044904, recall 0.741866
2017-12-10T05:12:28.208844: step 1305, loss 2.16397, acc 0.5, prec 0.0448569, recall 0.741866
2017-12-10T05:12:28.473750: step 1306, loss 1.61536, acc 0.65625, prec 0.0448558, recall 0.742005
2017-12-10T05:12:28.736188: step 1307, loss 1.68259, acc 0.625, prec 0.0448831, recall 0.742285
2017-12-10T05:12:29.005515: step 1308, loss 1.15924, acc 0.703125, prec 0.0448865, recall 0.742424
2017-12-10T05:12:29.274833: step 1309, loss 0.751292, acc 0.765625, prec 0.0448957, recall 0.742564
2017-12-10T05:12:29.542739: step 1310, loss 0.706001, acc 0.8125, prec 0.0449405, recall 0.742842
2017-12-10T05:12:29.814742: step 1311, loss 0.836739, acc 0.765625, prec 0.0449185, recall 0.742842
2017-12-10T05:12:30.078512: step 1312, loss 0.576005, acc 0.875, prec 0.0449379, recall 0.742981
2017-12-10T05:12:30.355375: step 1313, loss 0.138026, acc 0.9375, prec 0.0449321, recall 0.742981
2017-12-10T05:12:30.636802: step 1314, loss 0.49727, acc 0.859375, prec 0.0449501, recall 0.743119
2017-12-10T05:12:30.908207: step 1315, loss 3.62089, acc 0.9375, prec 0.0449457, recall 0.742718
2017-12-10T05:12:31.172682: step 1316, loss 0.19107, acc 0.921875, prec 0.0449695, recall 0.742857
2017-12-10T05:12:31.446125: step 1317, loss 0.375039, acc 0.90625, prec 0.0450542, recall 0.743272
2017-12-10T05:12:31.708521: step 1318, loss 0.254409, acc 0.921875, prec 0.0450468, recall 0.743272
2017-12-10T05:12:31.972563: step 1319, loss 1.30933, acc 0.90625, prec 0.0450395, recall 0.742873
2017-12-10T05:12:32.241968: step 1320, loss 0.557878, acc 0.890625, prec 0.0450603, recall 0.743011
2017-12-10T05:12:32.508851: step 1321, loss 0.198018, acc 0.953125, prec 0.0450559, recall 0.743011
2017-12-10T05:12:32.775724: step 1322, loss 0.146511, acc 0.953125, prec 0.0450826, recall 0.743149
2017-12-10T05:12:33.043461: step 1323, loss 0.133113, acc 0.90625, prec 0.0450738, recall 0.743149
2017-12-10T05:12:33.309893: step 1324, loss 0.360931, acc 0.90625, prec 0.045065, recall 0.743149
2017-12-10T05:12:33.574248: step 1325, loss 0.672453, acc 0.9375, prec 0.0450902, recall 0.743287
2017-12-10T05:12:33.840684: step 1326, loss 3.2662, acc 0.875, prec 0.04508, recall 0.742888
2017-12-10T05:12:34.109965: step 1327, loss 1.19589, acc 0.96875, prec 0.0451081, recall 0.743026
2017-12-10T05:12:34.370715: step 1328, loss 0.78071, acc 0.9375, prec 0.0451333, recall 0.743164
2017-12-10T05:12:34.638250: step 1329, loss 0.543804, acc 0.859375, prec 0.0451201, recall 0.743164
2017-12-10T05:12:34.902116: step 1330, loss 1.37029, acc 0.828125, prec 0.045135, recall 0.743301
2017-12-10T05:12:35.164073: step 1331, loss 0.524534, acc 0.8125, prec 0.0451485, recall 0.743439
2017-12-10T05:12:35.425491: step 1332, loss 1.00734, acc 0.75, prec 0.0451871, recall 0.743713
2017-12-10T05:12:35.686782: step 1333, loss 0.82069, acc 0.8125, prec 0.0452005, recall 0.74385
2017-12-10T05:12:35.958390: step 1334, loss 0.755139, acc 0.8125, prec 0.0452449, recall 0.744124
2017-12-10T05:12:36.215140: step 1335, loss 1.5033, acc 0.75, prec 0.0452214, recall 0.744124
2017-12-10T05:12:36.475405: step 1336, loss 1.18857, acc 0.703125, prec 0.0452245, recall 0.744261
2017-12-10T05:12:36.735479: step 1337, loss 1.00086, acc 0.765625, prec 0.0452644, recall 0.744533
2017-12-10T05:12:36.994384: step 1338, loss 1.27124, acc 0.671875, prec 0.0452646, recall 0.744669
2017-12-10T05:12:37.255264: step 1339, loss 1.23837, acc 0.671875, prec 0.0452956, recall 0.744941
2017-12-10T05:12:37.520677: step 1340, loss 1.45259, acc 0.65625, prec 0.0452943, recall 0.745077
2017-12-10T05:12:37.785244: step 1341, loss 0.880093, acc 0.78125, prec 0.0453046, recall 0.745213
2017-12-10T05:12:38.044680: step 1342, loss 1.19941, acc 0.6875, prec 0.0452753, recall 0.745213
2017-12-10T05:12:38.306408: step 1343, loss 0.771451, acc 0.8125, prec 0.0452578, recall 0.745213
2017-12-10T05:12:38.569904: step 1344, loss 0.659729, acc 0.828125, prec 0.0452417, recall 0.745213
2017-12-10T05:12:38.828830: step 1345, loss 0.544855, acc 0.859375, prec 0.0452286, recall 0.745213
2017-12-10T05:12:39.815238: step 1346, loss 0.60439, acc 0.859375, prec 0.0452154, recall 0.745213
2017-12-10T05:12:40.181481: step 1347, loss 0.700701, acc 0.796875, prec 0.0451965, recall 0.745213
2017-12-10T05:12:40.450687: step 1348, loss 0.182264, acc 0.953125, prec 0.0452229, recall 0.745348
2017-12-10T05:12:40.918821: step 1349, loss 0.649981, acc 0.984375, prec 0.045283, recall 0.745619
2017-12-10T05:12:41.649122: step 1350, loss 0.135524, acc 0.953125, prec 0.0453402, recall 0.745889
2017-12-10T05:12:42.416685: step 1351, loss 0.497569, acc 0.953125, prec 0.0453666, recall 0.746023
2017-12-10T05:12:43.418672: step 1352, loss 0.212809, acc 0.953125, prec 0.045393, recall 0.746158
2017-12-10T05:12:43.760349: step 1353, loss 0.25973, acc 0.9375, prec 0.0454179, recall 0.746292
2017-12-10T05:12:44.109416: step 1354, loss 0.403261, acc 0.96875, prec 0.0454765, recall 0.746561
2017-12-10T05:12:44.395059: step 1355, loss 0.0353861, acc 0.984375, prec 0.0454751, recall 0.746561
2017-12-10T05:12:44.675556: step 1356, loss 4.67387, acc 0.953125, prec 0.0454721, recall 0.746166
2017-12-10T05:12:44.938523: step 1357, loss 2.28615, acc 0.9375, prec 0.0454677, recall 0.745772
2017-12-10T05:12:45.214509: step 1358, loss 0.242843, acc 0.9375, prec 0.0454619, recall 0.745772
2017-12-10T05:12:45.482388: step 1359, loss 0.500234, acc 0.890625, prec 0.0455131, recall 0.74604
2017-12-10T05:12:45.748084: step 1360, loss 0.3038, acc 0.921875, prec 0.0455058, recall 0.74604
2017-12-10T05:12:46.012967: step 1361, loss 0.707371, acc 0.84375, prec 0.0455219, recall 0.746174
2017-12-10T05:12:46.282858: step 1362, loss 1.73219, acc 0.859375, prec 0.0455409, recall 0.745915
2017-12-10T05:12:46.555175: step 1363, loss 0.917416, acc 0.765625, prec 0.0455496, recall 0.746048
2017-12-10T05:12:46.822491: step 1364, loss 0.750249, acc 0.765625, prec 0.0455276, recall 0.746048
2017-12-10T05:12:47.088800: step 1365, loss 0.734363, acc 0.765625, prec 0.0455057, recall 0.746048
2017-12-10T05:12:47.350154: step 1366, loss 0.908942, acc 0.734375, prec 0.0454808, recall 0.746048
2017-12-10T05:12:47.613008: step 1367, loss 1.14568, acc 0.734375, prec 0.0454866, recall 0.746182
2017-12-10T05:12:47.870428: step 1368, loss 1.25202, acc 0.640625, prec 0.0454531, recall 0.746182
2017-12-10T05:12:48.128044: step 1369, loss 0.727284, acc 0.796875, prec 0.0454647, recall 0.746316
2017-12-10T05:12:48.393414: step 1370, loss 0.995997, acc 0.71875, prec 0.0454385, recall 0.746316
2017-12-10T05:12:48.659985: step 1371, loss 2.90803, acc 0.71875, prec 0.0454138, recall 0.745923
2017-12-10T05:12:48.924984: step 1372, loss 1.14112, acc 0.703125, prec 0.0453862, recall 0.745923
2017-12-10T05:12:49.187989: step 1373, loss 0.757463, acc 0.75, prec 0.0453935, recall 0.746057
2017-12-10T05:12:49.455137: step 1374, loss 0.861341, acc 0.734375, prec 0.0453993, recall 0.74619
2017-12-10T05:12:49.717068: step 1375, loss 1.39236, acc 0.796875, prec 0.0455025, recall 0.746723
2017-12-10T05:12:49.981389: step 1376, loss 0.743553, acc 0.8125, prec 0.0455155, recall 0.746855
2017-12-10T05:12:50.238261: step 1377, loss 0.758401, acc 0.796875, prec 0.0454966, recall 0.746855
2017-12-10T05:12:50.509372: step 1378, loss 0.863343, acc 0.75, prec 0.0455039, recall 0.746988
2017-12-10T05:12:50.770456: step 1379, loss 0.56671, acc 0.875, prec 0.0455227, recall 0.74712
2017-12-10T05:12:51.035032: step 1380, loss 0.77589, acc 0.75, prec 0.0455603, recall 0.747385
2017-12-10T05:12:51.299247: step 1381, loss 0.862626, acc 0.75, prec 0.0455371, recall 0.747385
2017-12-10T05:12:51.565076: step 1382, loss 0.609451, acc 0.796875, prec 0.0455487, recall 0.747517
2017-12-10T05:12:51.832715: step 1383, loss 0.645681, acc 0.796875, prec 0.0455906, recall 0.747781
2017-12-10T05:12:52.099929: step 1384, loss 0.399334, acc 0.875, prec 0.0456093, recall 0.747912
2017-12-10T05:12:52.364709: step 1385, loss 0.597401, acc 0.875, prec 0.0456281, recall 0.748044
2017-12-10T05:12:52.630180: step 1386, loss 0.249589, acc 0.9375, prec 0.0456527, recall 0.748175
2017-12-10T05:12:53.594371: step 1387, loss 0.102424, acc 0.953125, prec 0.0456483, recall 0.748175
2017-12-10T05:12:53.962341: step 1388, loss 0.461376, acc 0.953125, prec 0.0456743, recall 0.748306
2017-12-10T05:12:55.144940: step 1389, loss 0.0374643, acc 0.984375, prec 0.0456728, recall 0.748306
2017-12-10T05:12:55.569019: step 1390, loss 0.109475, acc 0.984375, prec 0.0457017, recall 0.748438
2017-12-10T05:12:55.865584: step 1391, loss 6.16729, acc 0.9375, prec 0.0457277, recall 0.748179
2017-12-10T05:12:56.158103: step 1392, loss 0.160612, acc 0.984375, prec 0.0457566, recall 0.74831
2017-12-10T05:12:56.437361: step 1393, loss 0.363623, acc 0.921875, prec 0.0457797, recall 0.748441
2017-12-10T05:12:56.720824: step 1394, loss 0.229972, acc 0.9375, prec 0.0457739, recall 0.748441
2017-12-10T05:12:57.004747: step 1395, loss 0.0671037, acc 0.953125, prec 0.0457998, recall 0.748571
2017-12-10T05:12:57.308032: step 1396, loss 3.38332, acc 0.8125, prec 0.0458141, recall 0.748313
2017-12-10T05:12:57.602660: step 1397, loss 0.56507, acc 0.875, prec 0.0458328, recall 0.748444
2017-12-10T05:12:57.888088: step 1398, loss 0.549348, acc 0.90625, prec 0.045915, recall 0.748835
2017-12-10T05:12:58.170387: step 1399, loss 0.449587, acc 0.90625, prec 0.0459365, recall 0.748965
2017-12-10T05:12:58.451064: step 1400, loss 0.498556, acc 0.796875, prec 0.0459478, recall 0.749095
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-1400

2017-12-10T05:12:59.712865: step 1401, loss 0.358369, acc 0.890625, prec 0.0459376, recall 0.749095
2017-12-10T05:13:00.002897: step 1402, loss 0.599955, acc 0.875, prec 0.0459562, recall 0.749224
2017-12-10T05:13:00.283206: step 1403, loss 0.841921, acc 0.78125, prec 0.0459358, recall 0.749224
2017-12-10T05:13:00.563841: step 1404, loss 0.408891, acc 0.890625, prec 0.0459256, recall 0.749224
2017-12-10T05:13:00.840940: step 1405, loss 0.655171, acc 0.84375, prec 0.0459111, recall 0.749224
2017-12-10T05:13:01.112896: step 1406, loss 0.474103, acc 0.890625, prec 0.0459311, recall 0.749354
2017-12-10T05:13:01.378515: step 1407, loss 0.486739, acc 0.875, prec 0.0459497, recall 0.749483
2017-12-10T05:13:01.649020: step 1408, loss 0.814016, acc 0.859375, prec 0.045997, recall 0.749742
2017-12-10T05:13:01.915190: step 1409, loss 0.279086, acc 0.90625, prec 0.0459883, recall 0.749742
2017-12-10T05:13:02.181814: step 1410, loss 0.302632, acc 0.90625, prec 0.0459796, recall 0.749742
2017-12-10T05:13:02.450770: step 1411, loss 0.462232, acc 0.828125, prec 0.0459636, recall 0.749742
2017-12-10T05:13:02.719706: step 1412, loss 0.575078, acc 0.796875, prec 0.046005, recall 0.75
2017-12-10T05:13:02.981550: step 1413, loss 0.110449, acc 0.953125, prec 0.0460006, recall 0.75
2017-12-10T05:13:03.244694: step 1414, loss 7.08591, acc 0.859375, prec 0.0460508, recall 0.749486
2017-12-10T05:13:03.516802: step 1415, loss 0.560169, acc 0.828125, prec 0.0460348, recall 0.749486
2017-12-10T05:13:03.780574: step 1416, loss 0.377804, acc 0.921875, prec 0.0460576, recall 0.749614
2017-12-10T05:13:04.053910: step 1417, loss 0.898485, acc 0.71875, prec 0.0460314, recall 0.749614
2017-12-10T05:13:04.319947: step 1418, loss 0.583145, acc 0.875, prec 0.0460198, recall 0.749614
2017-12-10T05:13:04.580248: step 1419, loss 0.83751, acc 0.78125, prec 0.0459995, recall 0.749614
2017-12-10T05:13:04.843400: step 1420, loss 0.735603, acc 0.828125, prec 0.0460437, recall 0.749872
2017-12-10T05:13:05.120932: step 1421, loss 1.51866, acc 0.828125, prec 0.046118, recall 0.750256
2017-12-10T05:13:05.382406: step 1422, loss 3.49026, acc 0.703125, prec 0.0461219, recall 0.75
2017-12-10T05:13:05.653113: step 1423, loss 1.23923, acc 0.671875, prec 0.0461514, recall 0.750256
2017-12-10T05:13:05.920521: step 1424, loss 1.63128, acc 0.625, prec 0.0461166, recall 0.750256
2017-12-10T05:13:06.181461: step 1425, loss 1.59278, acc 0.5625, prec 0.0461659, recall 0.750639
2017-12-10T05:13:06.444098: step 1426, loss 1.36316, acc 0.59375, prec 0.0461282, recall 0.750639
2017-12-10T05:13:06.705360: step 1427, loss 1.35266, acc 0.734375, prec 0.0461635, recall 0.750893
2017-12-10T05:13:06.976471: step 1428, loss 1.61587, acc 0.5625, prec 0.046123, recall 0.750893
2017-12-10T05:13:07.248402: step 1429, loss 1.75651, acc 0.65625, prec 0.046151, recall 0.751147
2017-12-10T05:13:07.505534: step 1430, loss 0.950128, acc 0.734375, prec 0.0461563, recall 0.751274
2017-12-10T05:13:07.770383: step 1431, loss 0.83518, acc 0.796875, prec 0.0461673, recall 0.751401
2017-12-10T05:13:08.036546: step 1432, loss 0.890424, acc 0.765625, prec 0.0461457, recall 0.751401
2017-12-10T05:13:08.301153: step 1433, loss 0.484073, acc 0.90625, prec 0.046137, recall 0.751401
2017-12-10T05:13:08.561271: step 1434, loss 1.08061, acc 0.78125, prec 0.0461168, recall 0.751401
2017-12-10T05:13:08.829172: step 1435, loss 0.132353, acc 0.953125, prec 0.0461125, recall 0.751401
2017-12-10T05:13:09.097059: step 1436, loss 0.182176, acc 0.90625, prec 0.0461336, recall 0.751527
2017-12-10T05:13:09.360699: step 1437, loss 0.836721, acc 0.875, prec 0.0462413, recall 0.752033
2017-12-10T05:13:09.626269: step 1438, loss 0.534696, acc 0.890625, prec 0.046261, recall 0.752158
2017-12-10T05:13:09.897452: step 1439, loss 0.15879, acc 0.96875, prec 0.0462581, recall 0.752158
2017-12-10T05:13:10.162452: step 1440, loss 0.147229, acc 0.9375, prec 0.0462523, recall 0.752158
2017-12-10T05:13:10.434991: step 1441, loss 0.112194, acc 0.96875, prec 0.0462495, recall 0.752158
2017-12-10T05:13:10.697602: step 1442, loss 0.106288, acc 0.953125, prec 0.0462749, recall 0.752284
2017-12-10T05:13:10.968450: step 1443, loss 3.62886, acc 0.90625, prec 0.0462677, recall 0.751903
2017-12-10T05:13:11.234055: step 1444, loss 0.197557, acc 0.921875, prec 0.0462605, recall 0.751903
2017-12-10T05:13:11.506842: step 1445, loss 0.557613, acc 0.90625, prec 0.0463113, recall 0.752154
2017-12-10T05:13:11.769873: step 1446, loss 0.14879, acc 0.953125, prec 0.046307, recall 0.752154
2017-12-10T05:13:12.043791: step 1447, loss 0.257287, acc 0.890625, prec 0.0463266, recall 0.75228
2017-12-10T05:13:12.323725: step 1448, loss 0.159638, acc 0.953125, prec 0.0463223, recall 0.75228
2017-12-10T05:13:12.583898: step 1449, loss 0.244502, acc 0.9375, prec 0.0463463, recall 0.752405
2017-12-10T05:13:12.855068: step 1450, loss 1.25963, acc 0.9375, prec 0.0463702, recall 0.75253
2017-12-10T05:13:13.130036: step 1451, loss 0.266043, acc 0.96875, prec 0.0463971, recall 0.752656
2017-12-10T05:13:13.395859: step 1452, loss 0.299007, acc 0.921875, prec 0.0463898, recall 0.752656
2017-12-10T05:13:13.658321: step 1453, loss 0.156615, acc 0.9375, prec 0.046384, recall 0.752656
2017-12-10T05:13:13.928087: step 1454, loss 0.265856, acc 0.921875, prec 0.0464363, recall 0.752905
2017-12-10T05:13:14.189912: step 1455, loss 0.281993, acc 0.84375, prec 0.0464515, recall 0.75303
2017-12-10T05:13:14.455392: step 1456, loss 6.19382, acc 0.859375, prec 0.0464399, recall 0.75265
2017-12-10T05:13:14.723085: step 1457, loss 0.722379, acc 0.8125, prec 0.0464819, recall 0.7529
2017-12-10T05:13:14.989522: step 1458, loss 0.542426, acc 0.859375, prec 0.0464689, recall 0.7529
2017-12-10T05:13:15.256489: step 1459, loss 0.648386, acc 0.828125, prec 0.0464827, recall 0.753024
2017-12-10T05:13:15.517809: step 1460, loss 0.347514, acc 0.890625, prec 0.0464726, recall 0.753024
2017-12-10T05:13:15.780854: step 1461, loss 0.696676, acc 0.859375, prec 0.0464892, recall 0.753149
2017-12-10T05:13:16.040451: step 1462, loss 0.432707, acc 0.8125, prec 0.0464719, recall 0.753149
2017-12-10T05:13:16.305485: step 1463, loss 0.420425, acc 0.875, prec 0.0464603, recall 0.753149
2017-12-10T05:13:16.563834: step 1464, loss 0.433485, acc 0.90625, prec 0.0465109, recall 0.753397
2017-12-10T05:13:16.832528: step 1465, loss 0.786215, acc 0.8125, prec 0.0464936, recall 0.753397
2017-12-10T05:13:17.090873: step 1466, loss 0.781171, acc 0.75, prec 0.0464705, recall 0.753397
2017-12-10T05:13:17.349897: step 1467, loss 0.705137, acc 0.8125, prec 0.0464828, recall 0.753521
2017-12-10T05:13:17.613634: step 1468, loss 0.645868, acc 0.796875, prec 0.0464936, recall 0.753645
2017-12-10T05:13:17.873659: step 1469, loss 0.915259, acc 0.796875, prec 0.0465044, recall 0.753769
2017-12-10T05:13:18.140281: step 1470, loss 0.609266, acc 0.84375, prec 0.0465491, recall 0.754016
2017-12-10T05:13:18.403352: step 1471, loss 0.474241, acc 0.859375, prec 0.0465657, recall 0.754139
2017-12-10T05:13:18.671482: step 1472, loss 0.62703, acc 0.828125, prec 0.0466089, recall 0.754386
2017-12-10T05:13:18.937075: step 1473, loss 0.634583, acc 0.921875, prec 0.0466312, recall 0.754509
2017-12-10T05:13:19.204109: step 1474, loss 0.200381, acc 0.921875, prec 0.0466239, recall 0.754509
2017-12-10T05:13:19.473109: step 1475, loss 0.142458, acc 0.953125, prec 0.0466196, recall 0.754509
2017-12-10T05:13:19.737302: step 1476, loss 0.0587869, acc 0.96875, prec 0.0466167, recall 0.754509
2017-12-10T05:13:20.006573: step 1477, loss 0.338595, acc 0.9375, prec 0.046611, recall 0.754509
2017-12-10T05:13:20.274277: step 1478, loss 0.317035, acc 0.890625, prec 0.0466304, recall 0.754632
2017-12-10T05:13:20.536673: step 1479, loss 0.0873095, acc 0.953125, prec 0.0466555, recall 0.754755
2017-12-10T05:13:20.795580: step 1480, loss 0.673968, acc 0.890625, prec 0.0467044, recall 0.755
2017-12-10T05:13:21.061883: step 1481, loss 0.311241, acc 0.921875, prec 0.0467267, recall 0.755122
2017-12-10T05:13:21.329782: step 1482, loss 0.323471, acc 0.953125, prec 0.0467518, recall 0.755245
2017-12-10T05:13:21.593286: step 1483, loss 0.0439931, acc 0.96875, prec 0.0467784, recall 0.755367
2017-12-10T05:13:21.856788: step 1484, loss 0.068085, acc 0.984375, prec 0.0467769, recall 0.755367
2017-12-10T05:13:22.123332: step 1485, loss 0.112587, acc 0.96875, prec 0.0468035, recall 0.755489
2017-12-10T05:13:22.383335: step 1486, loss 0.00599969, acc 1, prec 0.0468035, recall 0.755489
2017-12-10T05:13:22.645352: step 1487, loss 0.191628, acc 0.9375, prec 0.0468272, recall 0.755611
2017-12-10T05:13:22.909304: step 1488, loss 2.83895, acc 0.90625, prec 0.0468494, recall 0.755356
2017-12-10T05:13:23.184598: step 1489, loss 0.299078, acc 0.921875, prec 0.0468422, recall 0.755356
2017-12-10T05:13:23.455551: step 1490, loss 0.129727, acc 0.9375, prec 0.0468658, recall 0.755478
2017-12-10T05:13:23.694973: step 1491, loss 0.12145, acc 0.980769, prec 0.0468938, recall 0.7556
2017-12-10T05:13:23.977145: step 1492, loss 0.309762, acc 0.9375, prec 0.046888, recall 0.7556
2017-12-10T05:13:24.241508: step 1493, loss 0.151622, acc 0.953125, prec 0.0468837, recall 0.7556
2017-12-10T05:13:24.523643: step 1494, loss 0.321955, acc 0.9375, prec 0.0469073, recall 0.755721
2017-12-10T05:13:24.794575: step 1495, loss 4.68467, acc 0.890625, prec 0.0468986, recall 0.755346
2017-12-10T05:13:25.057366: step 1496, loss 0.0248852, acc 1, prec 0.0469281, recall 0.755467
2017-12-10T05:13:25.321981: step 1497, loss 0.431354, acc 0.90625, prec 0.0469782, recall 0.75571
2017-12-10T05:13:25.587317: step 1498, loss 0.41797, acc 0.859375, prec 0.0469946, recall 0.755831
2017-12-10T05:13:25.852729: step 1499, loss 1.97026, acc 0.921875, prec 0.0469888, recall 0.755456
2017-12-10T05:13:26.114946: step 1500, loss 0.368078, acc 0.90625, prec 0.0469801, recall 0.755456

Evaluation:
2017-12-10T05:13:33.904977: step 1500, loss 1.56299, acc 0.826854, prec 0.0476898, recall 0.75403

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-1500

2017-12-10T05:13:35.204166: step 1501, loss 0.570035, acc 0.859375, prec 0.0476773, recall 0.75403
2017-12-10T05:13:35.465089: step 1502, loss 0.840578, acc 0.84375, prec 0.0476634, recall 0.75403
2017-12-10T05:13:35.729610: step 1503, loss 0.943534, acc 0.75, prec 0.0476412, recall 0.75403
2017-12-10T05:13:35.991287: step 1504, loss 1.19914, acc 0.765625, prec 0.0477035, recall 0.75437
2017-12-10T05:13:36.254344: step 1505, loss 1.50878, acc 0.703125, prec 0.0476772, recall 0.75437
2017-12-10T05:13:36.521137: step 1506, loss 0.963278, acc 0.703125, prec 0.0476785, recall 0.754483
2017-12-10T05:13:36.790274: step 1507, loss 0.475171, acc 0.828125, prec 0.0477186, recall 0.754708
2017-12-10T05:13:37.052373: step 1508, loss 1.63078, acc 0.78125, prec 0.0477545, recall 0.754933
2017-12-10T05:13:37.314142: step 1509, loss 0.764588, acc 0.8125, prec 0.0477379, recall 0.754933
2017-12-10T05:13:37.577891: step 1510, loss 0.697123, acc 0.828125, prec 0.0477503, recall 0.755046
2017-12-10T05:13:37.839095: step 1511, loss 0.473591, acc 0.828125, prec 0.0477903, recall 0.75527
2017-12-10T05:13:38.104247: step 1512, loss 0.766175, acc 0.796875, prec 0.0478275, recall 0.755495
2017-12-10T05:13:38.374269: step 1513, loss 0.766762, acc 0.796875, prec 0.0478095, recall 0.755495
2017-12-10T05:13:38.631138: step 1514, loss 0.449455, acc 0.78125, prec 0.0477901, recall 0.755495
2017-12-10T05:13:38.894799: step 1515, loss 0.124698, acc 0.9375, prec 0.0477845, recall 0.755495
2017-12-10T05:13:39.159040: step 1516, loss 0.538318, acc 0.828125, prec 0.0477693, recall 0.755495
2017-12-10T05:13:39.430117: step 1517, loss 0.327405, acc 0.875, prec 0.0477858, recall 0.755606
2017-12-10T05:13:39.700202: step 1518, loss 0.54296, acc 0.90625, prec 0.0478602, recall 0.755942
2017-12-10T05:13:39.965103: step 1519, loss 0.949917, acc 0.9375, prec 0.0478822, recall 0.756053
2017-12-10T05:13:40.244500: step 1520, loss 0.140463, acc 0.96875, prec 0.0478794, recall 0.756053
2017-12-10T05:13:40.510400: step 1521, loss 0.272975, acc 0.921875, prec 0.0479, recall 0.756164
2017-12-10T05:13:40.777539: step 1522, loss 0.119109, acc 0.953125, prec 0.0479234, recall 0.756276
2017-12-10T05:13:41.041473: step 1523, loss 0.175679, acc 0.9375, prec 0.0479179, recall 0.756276
2017-12-10T05:13:41.309413: step 1524, loss 0.137489, acc 0.96875, prec 0.0479426, recall 0.756387
2017-12-10T05:13:41.575677: step 1525, loss 0.0758049, acc 0.984375, prec 0.0479963, recall 0.756609
2017-12-10T05:13:41.844836: step 1526, loss 0.0341841, acc 0.984375, prec 0.0480224, recall 0.75672
2017-12-10T05:13:42.117022: step 1527, loss 3.05181, acc 0.953125, prec 0.0480472, recall 0.756486
2017-12-10T05:13:42.397055: step 1528, loss 0.292239, acc 0.921875, prec 0.0480678, recall 0.756597
2017-12-10T05:13:42.669763: step 1529, loss 0.778576, acc 0.9375, prec 0.0480897, recall 0.756708
2017-12-10T05:13:42.945031: step 1530, loss 7.58695, acc 0.875, prec 0.04808, recall 0.756364
2017-12-10T05:13:43.214783: step 1531, loss 3.86342, acc 0.890625, prec 0.0480716, recall 0.75602
2017-12-10T05:13:43.480814: step 1532, loss 0.788011, acc 0.8125, prec 0.0480825, recall 0.756131
2017-12-10T05:13:43.748646: step 1533, loss 0.527739, acc 0.828125, prec 0.0481222, recall 0.756352
2017-12-10T05:13:44.018113: step 1534, loss 1.32139, acc 0.703125, prec 0.0481507, recall 0.756573
2017-12-10T05:13:44.280599: step 1535, loss 1.13321, acc 0.71875, prec 0.0481257, recall 0.756573
2017-12-10T05:13:44.541950: step 1536, loss 2.56658, acc 0.484375, prec 0.04808, recall 0.756573
2017-12-10T05:13:44.804289: step 1537, loss 2.4569, acc 0.4375, prec 0.0480302, recall 0.756573
2017-12-10T05:13:45.064221: step 1538, loss 2.062, acc 0.5625, prec 0.0479915, recall 0.756573
2017-12-10T05:13:45.329896: step 1539, loss 2.37635, acc 0.5625, prec 0.0479529, recall 0.756573
2017-12-10T05:13:45.593654: step 1540, loss 1.91259, acc 0.53125, prec 0.0479116, recall 0.756573
2017-12-10T05:13:45.861086: step 1541, loss 1.16982, acc 0.71875, prec 0.0479142, recall 0.756683
2017-12-10T05:13:46.130290: step 1542, loss 1.51109, acc 0.609375, prec 0.0478798, recall 0.756683
2017-12-10T05:13:46.393286: step 1543, loss 1.18518, acc 0.625, prec 0.0478469, recall 0.756683
2017-12-10T05:13:46.657299: step 1544, loss 1.16058, acc 0.75, prec 0.047825, recall 0.756683
2017-12-10T05:13:46.922512: step 1545, loss 0.886911, acc 0.75, prec 0.0478031, recall 0.756683
2017-12-10T05:13:47.189611: step 1546, loss 2.75871, acc 0.859375, prec 0.0478194, recall 0.756451
2017-12-10T05:13:47.455683: step 1547, loss 0.554682, acc 0.84375, prec 0.0478057, recall 0.756451
2017-12-10T05:13:47.727982: step 1548, loss 0.857002, acc 0.78125, prec 0.0478955, recall 0.756891
2017-12-10T05:13:47.988690: step 1549, loss 0.812971, acc 0.734375, prec 0.0478994, recall 0.757001
2017-12-10T05:13:48.252540: step 1550, loss 0.782867, acc 0.84375, prec 0.0479129, recall 0.757111
2017-12-10T05:13:48.517715: step 1551, loss 0.274609, acc 0.9375, prec 0.0479074, recall 0.757111
2017-12-10T05:13:48.778967: step 1552, loss 1.52406, acc 0.71875, prec 0.0479372, recall 0.75733
2017-12-10T05:13:49.049093: step 1553, loss 0.607781, acc 0.828125, prec 0.0479493, recall 0.757439
2017-12-10T05:13:49.316746: step 1554, loss 0.322323, acc 0.859375, prec 0.0480185, recall 0.757767
2017-12-10T05:13:49.583741: step 1555, loss 0.153805, acc 0.9375, prec 0.0480402, recall 0.757876
2017-12-10T05:13:49.847967: step 1556, loss 0.380653, acc 0.875, prec 0.0480292, recall 0.757876
2017-12-10T05:13:50.116682: step 1557, loss 0.362441, acc 0.859375, prec 0.048044, recall 0.757985
2017-12-10T05:13:50.377788: step 1558, loss 0.28157, acc 0.921875, prec 0.0480643, recall 0.758094
2017-12-10T05:13:50.646987: step 1559, loss 6.58841, acc 0.9375, prec 0.0480873, recall 0.757862
2017-12-10T05:13:50.914744: step 1560, loss 1.12168, acc 0.90625, prec 0.0481062, recall 0.75797
2017-12-10T05:13:51.179571: step 1561, loss 0.331001, acc 0.90625, prec 0.0481251, recall 0.758079
2017-12-10T05:13:51.445569: step 1562, loss 0.565921, acc 0.828125, prec 0.0481372, recall 0.758188
2017-12-10T05:13:51.711019: step 1563, loss 0.206456, acc 0.921875, prec 0.0481303, recall 0.758188
2017-12-10T05:13:51.975937: step 1564, loss 0.462194, acc 0.875, prec 0.0481194, recall 0.758188
2017-12-10T05:13:52.245785: step 1565, loss 1.13434, acc 0.703125, prec 0.0481204, recall 0.758296
2017-12-10T05:13:52.514929: step 1566, loss 0.534207, acc 0.84375, prec 0.0481609, recall 0.758513
2017-12-10T05:13:52.777074: step 1567, loss 1.04551, acc 0.75, prec 0.0481661, recall 0.758621
2017-12-10T05:13:53.040970: step 1568, loss 0.563444, acc 0.828125, prec 0.048178, recall 0.758729
2017-12-10T05:13:53.310902: step 1569, loss 0.774609, acc 0.84375, prec 0.0481644, recall 0.758729
2017-12-10T05:13:53.576438: step 1570, loss 0.522249, acc 0.828125, prec 0.0481493, recall 0.758729
2017-12-10T05:13:53.845083: step 1571, loss 0.532747, acc 0.890625, prec 0.0481668, recall 0.758837
2017-12-10T05:13:54.105187: step 1572, loss 0.529418, acc 0.875, prec 0.0481558, recall 0.758837
2017-12-10T05:13:54.369172: step 1573, loss 0.360429, acc 0.90625, prec 0.0481476, recall 0.758837
2017-12-10T05:13:54.641331: step 1574, loss 0.315455, acc 0.875, prec 0.0481367, recall 0.758837
2017-12-10T05:13:54.913639: step 1575, loss 0.57067, acc 0.828125, prec 0.0481487, recall 0.758945
2017-12-10T05:13:55.178612: step 1576, loss 0.394621, acc 0.875, prec 0.0481647, recall 0.759052
2017-12-10T05:13:55.443752: step 1577, loss 0.311054, acc 0.890625, prec 0.0481552, recall 0.759052
2017-12-10T05:13:55.710384: step 1578, loss 0.259371, acc 0.9375, prec 0.0481767, recall 0.75916
2017-12-10T05:13:55.972723: step 1579, loss 0.233705, acc 0.953125, prec 0.0482266, recall 0.759375
2017-12-10T05:13:56.241753: step 1580, loss 5.38192, acc 0.921875, prec 0.0482211, recall 0.759036
2017-12-10T05:13:56.515897: step 1581, loss 0.0544872, acc 0.96875, prec 0.0482454, recall 0.759144
2017-12-10T05:13:56.784133: step 1582, loss 0.278644, acc 0.90625, prec 0.0482372, recall 0.759144
2017-12-10T05:13:57.054524: step 1583, loss 0.251351, acc 0.921875, prec 0.0482573, recall 0.759251
2017-12-10T05:13:57.341319: step 1584, loss 0.162059, acc 0.953125, prec 0.0482802, recall 0.759358
2017-12-10T05:13:57.614598: step 1585, loss 0.755651, acc 0.859375, prec 0.0483218, recall 0.759573
2017-12-10T05:13:57.878428: step 1586, loss 0.231184, acc 0.890625, prec 0.0483122, recall 0.759573
2017-12-10T05:13:58.143223: step 1587, loss 0.32547, acc 0.921875, prec 0.0483862, recall 0.759893
2017-12-10T05:13:58.403035: step 1588, loss 0.510827, acc 0.875, prec 0.0484022, recall 0.76
2017-12-10T05:13:58.667017: step 1589, loss 0.134288, acc 0.953125, prec 0.0483981, recall 0.76
2017-12-10T05:13:58.938280: step 1590, loss 0.25981, acc 0.890625, prec 0.0484154, recall 0.760107
2017-12-10T05:13:59.208728: step 1591, loss 0.320878, acc 0.953125, prec 0.0484382, recall 0.760213
2017-12-10T05:13:59.472565: step 1592, loss 0.279883, acc 0.875, prec 0.0484542, recall 0.76032
2017-12-10T05:13:59.741746: step 1593, loss 0.818553, acc 0.890625, prec 0.0484715, recall 0.760426
2017-12-10T05:14:00.001941: step 1594, loss 0.359333, acc 0.90625, prec 0.0484633, recall 0.760426
2017-12-10T05:14:00.267371: step 1595, loss 5.87746, acc 0.890625, prec 0.0485357, recall 0.760407
2017-12-10T05:14:00.540231: step 1596, loss 0.30672, acc 0.90625, prec 0.0485275, recall 0.760407
2017-12-10T05:14:00.814751: step 1597, loss 0.266602, acc 0.90625, prec 0.0485193, recall 0.760407
2017-12-10T05:14:01.084543: step 1598, loss 0.244049, acc 0.953125, prec 0.0485689, recall 0.760619
2017-12-10T05:14:01.352201: step 1599, loss 0.232006, acc 0.921875, prec 0.0485889, recall 0.760725
2017-12-10T05:14:01.618678: step 1600, loss 0.271454, acc 0.90625, prec 0.0485807, recall 0.760725
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-1600

2017-12-10T05:14:02.894547: step 1601, loss 1.18371, acc 0.890625, prec 0.0486517, recall 0.761042
2017-12-10T05:14:03.160337: step 1602, loss 0.477071, acc 0.890625, prec 0.0486421, recall 0.761042
2017-12-10T05:14:03.421710: step 1603, loss 0.525409, acc 0.875, prec 0.0486311, recall 0.761042
2017-12-10T05:14:03.685774: step 1604, loss 0.824134, acc 0.84375, prec 0.0486711, recall 0.761253
2017-12-10T05:14:03.948080: step 1605, loss 0.648824, acc 0.828125, prec 0.048656, recall 0.761253
2017-12-10T05:14:04.208346: step 1606, loss 0.41775, acc 0.84375, prec 0.0486422, recall 0.761253
2017-12-10T05:14:04.472269: step 1607, loss 0.535362, acc 0.84375, prec 0.0486285, recall 0.761253
2017-12-10T05:14:04.744971: step 1608, loss 0.562148, acc 0.875, prec 0.048698, recall 0.761569
2017-12-10T05:14:05.007189: step 1609, loss 0.323183, acc 0.921875, prec 0.0487179, recall 0.761674
2017-12-10T05:14:05.270227: step 1610, loss 1.17324, acc 0.734375, prec 0.0486946, recall 0.761674
2017-12-10T05:14:05.535066: step 1611, loss 0.564108, acc 0.828125, prec 0.0487063, recall 0.761779
2017-12-10T05:14:05.801032: step 1612, loss 6.93878, acc 0.796875, prec 0.0487166, recall 0.761549
2017-12-10T05:14:06.068354: step 1613, loss 0.205699, acc 0.921875, prec 0.0487098, recall 0.761549
2017-12-10T05:14:06.338216: step 1614, loss 0.602588, acc 0.84375, prec 0.0486961, recall 0.761549
2017-12-10T05:14:06.613775: step 1615, loss 0.523362, acc 0.90625, prec 0.0487146, recall 0.761653
2017-12-10T05:14:06.887936: step 1616, loss 0.624101, acc 0.84375, prec 0.0487544, recall 0.761863
2017-12-10T05:14:07.154374: step 1617, loss 0.580253, acc 0.84375, prec 0.0487407, recall 0.761863
2017-12-10T05:14:07.427544: step 1618, loss 1.10007, acc 0.875, prec 0.0487565, recall 0.761967
2017-12-10T05:14:07.691767: step 1619, loss 0.6717, acc 0.84375, prec 0.0487695, recall 0.762072
2017-12-10T05:14:07.952571: step 1620, loss 0.376042, acc 0.84375, prec 0.0487558, recall 0.762072
2017-12-10T05:14:08.215479: step 1621, loss 0.581938, acc 0.859375, prec 0.0487435, recall 0.762072
2017-12-10T05:14:08.472678: step 1622, loss 0.493961, acc 0.84375, prec 0.0488099, recall 0.762385
2017-12-10T05:14:08.737986: step 1623, loss 0.434976, acc 0.90625, prec 0.0488284, recall 0.762489
2017-12-10T05:14:09.010932: step 1624, loss 0.791118, acc 0.859375, prec 0.0488428, recall 0.762593
2017-12-10T05:14:09.272944: step 1625, loss 0.563824, acc 0.84375, prec 0.0488291, recall 0.762593
2017-12-10T05:14:09.537852: step 1626, loss 0.29144, acc 0.921875, prec 0.0488222, recall 0.762593
2017-12-10T05:14:09.804268: step 1627, loss 0.277952, acc 0.9375, prec 0.0488167, recall 0.762593
2017-12-10T05:14:10.078337: step 1628, loss 0.325709, acc 0.90625, prec 0.0488352, recall 0.762697
2017-12-10T05:14:10.340563: step 1629, loss 0.0763348, acc 0.953125, prec 0.0488311, recall 0.762697
2017-12-10T05:14:10.603703: step 1630, loss 0.434958, acc 0.890625, prec 0.0488215, recall 0.762697
2017-12-10T05:14:10.867822: step 1631, loss 3.43117, acc 0.953125, prec 0.0488188, recall 0.762363
2017-12-10T05:14:11.134665: step 1632, loss 2.40822, acc 0.921875, prec 0.0488399, recall 0.762134
2017-12-10T05:14:11.406320: step 1633, loss 0.202085, acc 0.921875, prec 0.048913, recall 0.762445
2017-12-10T05:14:11.667135: step 1634, loss 0.40127, acc 0.84375, prec 0.0488993, recall 0.762445
2017-12-10T05:14:11.942237: step 1635, loss 0.61868, acc 0.828125, prec 0.0488843, recall 0.762445
2017-12-10T05:14:12.214010: step 1636, loss 0.510216, acc 0.859375, prec 0.0488986, recall 0.762549
2017-12-10T05:14:12.487370: step 1637, loss 0.372141, acc 0.859375, prec 0.0488863, recall 0.762549
2017-12-10T05:14:12.747686: step 1638, loss 0.540101, acc 0.84375, prec 0.0488992, recall 0.762653
2017-12-10T05:14:13.012850: step 1639, loss 0.192949, acc 0.9375, prec 0.0488937, recall 0.762653
2017-12-10T05:14:13.278953: step 1640, loss 0.780882, acc 0.796875, prec 0.0489557, recall 0.762963
2017-12-10T05:14:13.545813: step 1641, loss 0.815379, acc 0.828125, prec 0.0489939, recall 0.763169
2017-12-10T05:14:13.820575: step 1642, loss 0.632913, acc 0.765625, prec 0.0489733, recall 0.763169
2017-12-10T05:14:14.081491: step 1643, loss 1.10617, acc 0.6875, prec 0.048946, recall 0.763169
2017-12-10T05:14:14.347201: step 1644, loss 0.792305, acc 0.875, prec 0.0489616, recall 0.763272
2017-12-10T05:14:14.612283: step 1645, loss 0.740048, acc 0.828125, prec 0.0489466, recall 0.763272
2017-12-10T05:14:14.880210: step 1646, loss 0.568823, acc 0.84375, prec 0.0489329, recall 0.763272
2017-12-10T05:14:15.141765: step 1647, loss 0.291391, acc 0.859375, prec 0.0489471, recall 0.763375
2017-12-10T05:14:15.410407: step 1648, loss 0.493858, acc 0.875, prec 0.0489627, recall 0.763478
2017-12-10T05:14:15.675503: step 1649, loss 0.433092, acc 0.9375, prec 0.0490103, recall 0.763684
2017-12-10T05:14:15.942667: step 1650, loss 0.470258, acc 0.875, prec 0.0490524, recall 0.763889
2017-12-10T05:14:16.214021: step 1651, loss 0.496078, acc 0.859375, prec 0.0490666, recall 0.763991
2017-12-10T05:14:16.483891: step 1652, loss 0.446522, acc 0.921875, prec 0.0490863, recall 0.764094
2017-12-10T05:14:16.752327: step 1653, loss 0.277312, acc 0.921875, prec 0.0491324, recall 0.764298
2017-12-10T05:14:17.023216: step 1654, loss 0.174994, acc 0.90625, prec 0.0491507, recall 0.7644
2017-12-10T05:14:17.286962: step 1655, loss 0.0598219, acc 0.96875, prec 0.0491479, recall 0.7644
2017-12-10T05:14:17.558491: step 1656, loss 1.00079, acc 0.953125, prec 0.0491703, recall 0.764502
2017-12-10T05:14:17.832260: step 1657, loss 5.48432, acc 0.96875, prec 0.0491689, recall 0.764171
2017-12-10T05:14:18.100984: step 1658, loss 0.209024, acc 0.953125, prec 0.0492177, recall 0.764375
2017-12-10T05:14:18.368705: step 1659, loss 0.17476, acc 0.984375, prec 0.0492428, recall 0.764477
2017-12-10T05:14:18.631799: step 1660, loss 0.180919, acc 0.90625, prec 0.0492611, recall 0.764579
2017-12-10T05:14:18.899588: step 1661, loss 0.183404, acc 0.9375, prec 0.0492556, recall 0.764579
2017-12-10T05:14:19.174292: step 1662, loss 1.50159, acc 0.890625, prec 0.0492474, recall 0.764249
2017-12-10T05:14:19.442209: step 1663, loss 0.280534, acc 0.921875, prec 0.049267, recall 0.76435
2017-12-10T05:14:19.705862: step 1664, loss 0.40383, acc 0.84375, prec 0.0492797, recall 0.764452
2017-12-10T05:14:19.963821: step 1665, loss 0.580296, acc 0.84375, prec 0.0492924, recall 0.764554
2017-12-10T05:14:20.235614: step 1666, loss 0.402095, acc 0.90625, prec 0.0492842, recall 0.764554
2017-12-10T05:14:20.498768: step 1667, loss 0.633942, acc 0.859375, prec 0.0492719, recall 0.764554
2017-12-10T05:14:20.766026: step 1668, loss 0.54251, acc 0.796875, prec 0.0492805, recall 0.764655
2017-12-10T05:14:21.036373: step 1669, loss 0.569844, acc 0.890625, prec 0.0492973, recall 0.764757
2017-12-10T05:14:21.298086: step 1670, loss 0.633375, acc 0.828125, prec 0.0492823, recall 0.764757
2017-12-10T05:14:21.560281: step 1671, loss 0.333374, acc 0.921875, prec 0.0492754, recall 0.764757
2017-12-10T05:14:21.828792: step 1672, loss 0.230767, acc 0.921875, prec 0.0492686, recall 0.764757
2017-12-10T05:14:22.094085: step 1673, loss 0.352743, acc 0.859375, prec 0.0492563, recall 0.764757
2017-12-10T05:14:22.363278: step 1674, loss 2.26771, acc 0.875, prec 0.0492995, recall 0.76463
2017-12-10T05:14:22.627763: step 1675, loss 0.109054, acc 0.953125, prec 0.0493218, recall 0.764731
2017-12-10T05:14:22.902486: step 1676, loss 0.376168, acc 0.90625, prec 0.0493926, recall 0.765034
2017-12-10T05:14:23.170655: step 1677, loss 0.456235, acc 0.890625, prec 0.0494094, recall 0.765135
2017-12-10T05:14:23.435470: step 1678, loss 0.337199, acc 0.875, prec 0.0493985, recall 0.765135
2017-12-10T05:14:23.708663: step 1679, loss 0.43524, acc 0.90625, prec 0.0494166, recall 0.765236
2017-12-10T05:14:23.972363: step 1680, loss 0.383956, acc 0.90625, prec 0.0494084, recall 0.765236
2017-12-10T05:14:24.236473: step 1681, loss 1.47772, acc 0.90625, prec 0.0494279, recall 0.765009
2017-12-10T05:14:24.510807: step 1682, loss 0.584544, acc 0.828125, prec 0.0494128, recall 0.765009
2017-12-10T05:14:24.774675: step 1683, loss 0.292186, acc 0.875, prec 0.0494808, recall 0.76531
2017-12-10T05:14:25.038867: step 1684, loss 0.543124, acc 0.8125, prec 0.0494907, recall 0.765411
2017-12-10T05:14:25.302686: step 1685, loss 0.301998, acc 0.890625, prec 0.0494811, recall 0.765411
2017-12-10T05:14:25.570338: step 1686, loss 0.353783, acc 0.90625, prec 0.0494729, recall 0.765411
2017-12-10T05:14:25.841086: step 1687, loss 0.586308, acc 0.828125, prec 0.0495104, recall 0.765612
2017-12-10T05:14:26.107066: step 1688, loss 1.26194, acc 0.875, prec 0.049552, recall 0.765812
2017-12-10T05:14:26.376188: step 1689, loss 0.37272, acc 0.84375, prec 0.0495383, recall 0.765812
2017-12-10T05:14:26.645485: step 1690, loss 0.275548, acc 0.90625, prec 0.0495827, recall 0.766012
2017-12-10T05:14:26.913116: step 1691, loss 0.219969, acc 0.921875, prec 0.0496021, recall 0.766112
2017-12-10T05:14:27.185349: step 1692, loss 0.657802, acc 0.796875, prec 0.0495843, recall 0.766112
2017-12-10T05:14:27.449699: step 1693, loss 0.170916, acc 0.921875, prec 0.0496037, recall 0.766212
2017-12-10T05:14:27.716052: step 1694, loss 0.599901, acc 0.859375, prec 0.0495913, recall 0.766212
2017-12-10T05:14:27.983382: step 1695, loss 0.131263, acc 0.953125, prec 0.0495872, recall 0.766212
2017-12-10T05:14:28.244901: step 1696, loss 0.498876, acc 0.84375, prec 0.0495735, recall 0.766212
2017-12-10T05:14:28.507586: step 1697, loss 0.692956, acc 0.828125, prec 0.0496372, recall 0.76651
2017-12-10T05:14:28.778408: step 1698, loss 0.266877, acc 0.921875, prec 0.0496565, recall 0.76661
2017-12-10T05:14:29.049010: step 1699, loss 0.29219, acc 0.890625, prec 0.049647, recall 0.76661
2017-12-10T05:14:29.316740: step 1700, loss 0.213115, acc 0.90625, prec 0.0496387, recall 0.76661
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-1700

2017-12-10T05:14:30.577530: step 1701, loss 0.485299, acc 0.921875, prec 0.0496581, recall 0.766709
2017-12-10T05:14:30.849646: step 1702, loss 0.0301768, acc 0.984375, prec 0.0496567, recall 0.766709
2017-12-10T05:14:31.112277: step 1703, loss 0.121809, acc 0.984375, prec 0.0496816, recall 0.766809
2017-12-10T05:14:31.382563: step 1704, loss 0.0841464, acc 0.96875, prec 0.0496788, recall 0.766809
2017-12-10T05:14:31.648426: step 1705, loss 1.22776, acc 0.9375, prec 0.0497257, recall 0.767007
2017-12-10T05:14:31.923986: step 1706, loss 0.315329, acc 0.953125, prec 0.0497478, recall 0.767106
2017-12-10T05:14:32.189989: step 1707, loss 0.310598, acc 0.90625, prec 0.0497658, recall 0.767205
2017-12-10T05:14:32.451063: step 1708, loss 0.421126, acc 0.921875, prec 0.0497589, recall 0.767205
2017-12-10T05:14:32.719848: step 1709, loss 0.284687, acc 0.9375, prec 0.0498058, recall 0.767402
2017-12-10T05:14:32.988186: step 1710, loss 0.243166, acc 0.921875, prec 0.0498251, recall 0.767501
2017-12-10T05:14:33.253716: step 1711, loss 0.178008, acc 0.96875, prec 0.0498485, recall 0.7676
2017-12-10T05:14:33.516728: step 1712, loss 2.68028, acc 0.9375, prec 0.0499491, recall 0.767668
2017-12-10T05:14:33.781803: step 1713, loss 0.417733, acc 0.859375, prec 0.0499367, recall 0.767668
2017-12-10T05:14:34.044072: step 1714, loss 1.04887, acc 0.84375, prec 0.0500014, recall 0.767963
2017-12-10T05:14:34.307672: step 1715, loss 0.394594, acc 0.875, prec 0.0499904, recall 0.767963
2017-12-10T05:14:34.569440: step 1716, loss 0.410373, acc 0.921875, prec 0.0500096, recall 0.768061
2017-12-10T05:14:34.838530: step 1717, loss 0.658178, acc 0.875, prec 0.0500248, recall 0.768159
2017-12-10T05:14:35.102735: step 1718, loss 0.59048, acc 0.828125, prec 0.0500096, recall 0.768159
2017-12-10T05:14:35.364276: step 1719, loss 0.424213, acc 0.90625, prec 0.0500275, recall 0.768257
2017-12-10T05:14:35.638184: step 1720, loss 1.21475, acc 0.765625, prec 0.050033, recall 0.768354
2017-12-10T05:14:35.906092: step 1721, loss 0.755089, acc 0.8125, prec 0.0500426, recall 0.768452
2017-12-10T05:14:36.177789: step 1722, loss 0.746428, acc 0.796875, prec 0.0500247, recall 0.768452
2017-12-10T05:14:36.449695: step 1723, loss 1.39991, acc 0.765625, prec 0.0500563, recall 0.768647
2017-12-10T05:14:36.718576: step 1724, loss 0.875829, acc 0.828125, prec 0.0500412, recall 0.768647
2017-12-10T05:14:36.983960: step 1725, loss 1.14, acc 0.765625, prec 0.0500206, recall 0.768647
2017-12-10T05:14:37.248444: step 1726, loss 0.676935, acc 0.875, prec 0.0500356, recall 0.768745
2017-12-10T05:14:37.514634: step 1727, loss 0.486682, acc 0.875, prec 0.0500507, recall 0.768842
2017-12-10T05:14:37.777673: step 1728, loss 0.672319, acc 0.8125, prec 0.0500343, recall 0.768842
2017-12-10T05:14:38.042873: step 1729, loss 0.535431, acc 0.84375, prec 0.0500466, recall 0.768939
2017-12-10T05:14:38.309972: step 1730, loss 0.295213, acc 0.90625, prec 0.0500383, recall 0.768939
2017-12-10T05:14:38.577580: step 1731, loss 0.51752, acc 0.90625, prec 0.0500821, recall 0.769134
2017-12-10T05:14:38.844984: step 1732, loss 1.00155, acc 0.890625, prec 0.0500986, recall 0.769231
2017-12-10T05:14:39.109223: step 1733, loss 0.305589, acc 0.921875, prec 0.0501437, recall 0.769425
2017-12-10T05:14:39.375135: step 1734, loss 0.387163, acc 0.90625, prec 0.0501615, recall 0.769521
2017-12-10T05:14:39.643150: step 1735, loss 0.569304, acc 0.875, prec 0.0501505, recall 0.769521
2017-12-10T05:14:39.906044: step 1736, loss 2.69995, acc 0.90625, prec 0.0501696, recall 0.769295
2017-12-10T05:14:40.181368: step 1737, loss 0.677364, acc 0.9375, prec 0.0501901, recall 0.769392
2017-12-10T05:14:40.453914: step 1738, loss 0.198831, acc 0.953125, prec 0.050212, recall 0.769489
2017-12-10T05:14:40.723148: step 1739, loss 2.36195, acc 0.859375, prec 0.0502269, recall 0.769263
2017-12-10T05:14:40.990553: step 1740, loss 0.459715, acc 0.859375, prec 0.0502405, recall 0.76936
2017-12-10T05:14:41.259927: step 1741, loss 0.290875, acc 0.9375, prec 0.050261, recall 0.769456
2017-12-10T05:14:41.523987: step 1742, loss 5.09272, acc 0.9375, prec 0.0502828, recall 0.769231
2017-12-10T05:14:41.790358: step 1743, loss 0.798307, acc 0.796875, prec 0.0503169, recall 0.769424
2017-12-10T05:14:42.065571: step 1744, loss 0.589067, acc 0.84375, prec 0.0503031, recall 0.769424
2017-12-10T05:14:42.341559: step 1745, loss 0.893896, acc 0.734375, prec 0.0503057, recall 0.76952
2017-12-10T05:14:42.609177: step 1746, loss 0.726817, acc 0.8125, prec 0.0502892, recall 0.76952
2017-12-10T05:14:42.871977: step 1747, loss 0.517238, acc 0.859375, prec 0.0502769, recall 0.76952
2017-12-10T05:14:43.133459: step 1748, loss 1.1539, acc 0.75, prec 0.050255, recall 0.76952
2017-12-10T05:14:43.399758: step 1749, loss 1.11802, acc 0.671875, prec 0.050278, recall 0.769712
2017-12-10T05:14:43.661524: step 1750, loss 1.01529, acc 0.71875, prec 0.0502792, recall 0.769808
2017-12-10T05:14:43.926911: step 1751, loss 0.916793, acc 0.796875, prec 0.0502614, recall 0.769808
2017-12-10T05:14:44.190348: step 1752, loss 1.24744, acc 0.71875, prec 0.0502368, recall 0.769808
2017-12-10T05:14:44.454277: step 1753, loss 0.888072, acc 0.75, prec 0.0502407, recall 0.769904
2017-12-10T05:14:44.723949: step 1754, loss 1.12603, acc 0.75, prec 0.0502189, recall 0.769904
2017-12-10T05:14:44.993852: step 1755, loss 0.762277, acc 0.84375, prec 0.0502052, recall 0.769904
2017-12-10T05:14:45.263574: step 1756, loss 0.724008, acc 0.796875, prec 0.0501875, recall 0.769904
2017-12-10T05:14:45.530309: step 1757, loss 0.264259, acc 0.90625, prec 0.0501793, recall 0.769904
2017-12-10T05:14:45.802155: step 1758, loss 0.50894, acc 0.90625, prec 0.0501711, recall 0.769904
2017-12-10T05:14:46.069227: step 1759, loss 0.278914, acc 0.875, prec 0.050186, recall 0.77
2017-12-10T05:14:46.334550: step 1760, loss 0.517955, acc 0.890625, prec 0.0502023, recall 0.770096
2017-12-10T05:14:46.603888: step 1761, loss 0.295527, acc 0.921875, prec 0.0501955, recall 0.770096
2017-12-10T05:14:46.871646: step 1762, loss 0.543026, acc 0.984375, prec 0.0502199, recall 0.770191
2017-12-10T05:14:47.138329: step 1763, loss 4.92682, acc 0.96875, prec 0.0502185, recall 0.769871
2017-12-10T05:14:47.411567: step 1764, loss 0.299318, acc 0.90625, prec 0.0502619, recall 0.770062
2017-12-10T05:14:47.677645: step 1765, loss 0.16543, acc 0.953125, prec 0.0502578, recall 0.770062
2017-12-10T05:14:47.945493: step 1766, loss 0.407307, acc 0.859375, prec 0.0503228, recall 0.770349
2017-12-10T05:14:48.212819: step 1767, loss 0.328267, acc 0.921875, prec 0.0503675, recall 0.770539
2017-12-10T05:14:48.477332: step 1768, loss 0.890643, acc 0.828125, prec 0.050404, recall 0.77073
2017-12-10T05:14:48.746913: step 1769, loss 0.359116, acc 0.875, prec 0.0503931, recall 0.77073
2017-12-10T05:14:49.006935: step 1770, loss 0.382173, acc 0.90625, prec 0.0504106, recall 0.770825
2017-12-10T05:14:49.269278: step 1771, loss 0.31637, acc 0.90625, prec 0.0504024, recall 0.770825
2017-12-10T05:14:49.535553: step 1772, loss 0.387416, acc 0.90625, prec 0.0504199, recall 0.77092
2017-12-10T05:14:49.798376: step 1773, loss 0.541436, acc 0.9375, prec 0.0504402, recall 0.771015
2017-12-10T05:14:50.074150: step 1774, loss 0.722768, acc 0.953125, prec 0.0504618, recall 0.771109
2017-12-10T05:14:50.345106: step 1775, loss 0.236328, acc 0.921875, prec 0.0504807, recall 0.771204
2017-12-10T05:14:50.611381: step 1776, loss 0.317334, acc 0.875, prec 0.0504698, recall 0.771204
2017-12-10T05:14:50.879613: step 1777, loss 0.328188, acc 0.84375, prec 0.0505075, recall 0.771393
2017-12-10T05:14:51.143128: step 1778, loss 0.376606, acc 0.875, prec 0.0505223, recall 0.771488
2017-12-10T05:14:51.407600: step 1779, loss 0.383721, acc 0.9375, prec 0.0505425, recall 0.771582
2017-12-10T05:14:51.675198: step 1780, loss 0.408109, acc 0.953125, prec 0.0505641, recall 0.771676
2017-12-10T05:14:51.940914: step 1781, loss 1.47123, acc 0.875, prec 0.0506059, recall 0.771546
2017-12-10T05:14:52.206822: step 1782, loss 0.281252, acc 0.875, prec 0.0505949, recall 0.771546
2017-12-10T05:14:52.474750: step 1783, loss 0.130813, acc 0.96875, prec 0.0505922, recall 0.771546
2017-12-10T05:14:52.735849: step 1784, loss 0.216082, acc 0.921875, prec 0.050611, recall 0.771641
2017-12-10T05:14:52.997899: step 1785, loss 1.07848, acc 0.921875, prec 0.0506555, recall 0.771829
2017-12-10T05:14:53.269689: step 1786, loss 2.62369, acc 0.859375, prec 0.0506959, recall 0.771699
2017-12-10T05:14:53.541390: step 1787, loss 0.189931, acc 0.875, prec 0.0506849, recall 0.771699
2017-12-10T05:14:53.806258: step 1788, loss 1.93898, acc 0.796875, prec 0.0506685, recall 0.771382
2017-12-10T05:14:54.070080: step 1789, loss 0.90252, acc 0.890625, prec 0.0507102, recall 0.771569
2017-12-10T05:14:54.334391: step 1790, loss 1.35776, acc 0.65625, prec 0.0506801, recall 0.771569
2017-12-10T05:14:54.600731: step 1791, loss 0.809754, acc 0.75, prec 0.0506582, recall 0.771569
2017-12-10T05:14:54.871677: step 1792, loss 1.03789, acc 0.75, prec 0.0506363, recall 0.771569
2017-12-10T05:14:55.140197: step 1793, loss 1.52021, acc 0.6875, prec 0.0506346, recall 0.771663
2017-12-10T05:14:55.407515: step 1794, loss 1.41283, acc 0.59375, prec 0.0505992, recall 0.771663
2017-12-10T05:14:55.671587: step 1795, loss 1.33791, acc 0.625, prec 0.050592, recall 0.771757
2017-12-10T05:14:55.940424: step 1796, loss 1.33622, acc 0.671875, prec 0.0506145, recall 0.771944
2017-12-10T05:14:56.213195: step 1797, loss 1.4934, acc 0.6875, prec 0.0506128, recall 0.772038
2017-12-10T05:14:56.484473: step 1798, loss 1.11741, acc 0.75, prec 0.0506421, recall 0.772224
2017-12-10T05:14:56.747712: step 1799, loss 0.980783, acc 0.75, prec 0.0506203, recall 0.772224
2017-12-10T05:14:57.016587: step 1800, loss 0.711094, acc 0.765625, prec 0.0506, recall 0.772224

Evaluation:
2017-12-10T05:15:04.621694: step 1800, loss 1.42733, acc 0.811851, prec 0.051022, recall 0.773112

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-1800

2017-12-10T05:15:05.941989: step 1801, loss 0.882752, acc 0.78125, prec 0.051052, recall 0.773287
2017-12-10T05:15:06.202041: step 1802, loss 0.359696, acc 0.859375, prec 0.0510404, recall 0.773287
2017-12-10T05:15:06.475082: step 1803, loss 0.779075, acc 0.828125, prec 0.0510502, recall 0.773374
2017-12-10T05:15:06.734441: step 1804, loss 0.297297, acc 0.9375, prec 0.0510932, recall 0.773549
2017-12-10T05:15:06.997925: step 1805, loss 0.356127, acc 0.90625, prec 0.0511336, recall 0.773723
2017-12-10T05:15:07.260994: step 1806, loss 0.188631, acc 0.9375, prec 0.0511766, recall 0.773896
2017-12-10T05:15:07.534036: step 1807, loss 0.207433, acc 0.921875, prec 0.0511701, recall 0.773896
2017-12-10T05:15:07.800977: step 1808, loss 0.183505, acc 0.9375, prec 0.0512131, recall 0.77407
2017-12-10T05:15:08.067326: step 1809, loss 0.155774, acc 0.9375, prec 0.051232, recall 0.774156
2017-12-10T05:15:08.336087: step 1810, loss 0.123105, acc 0.953125, prec 0.0512281, recall 0.774156
2017-12-10T05:15:08.600621: step 1811, loss 0.0583821, acc 0.96875, prec 0.0512255, recall 0.774156
2017-12-10T05:15:08.863845: step 1812, loss 0.210822, acc 0.90625, prec 0.0512177, recall 0.774156
2017-12-10T05:15:09.127921: step 1813, loss 1.07602, acc 0.953125, prec 0.051286, recall 0.774416
2017-12-10T05:15:09.392719: step 1814, loss 0.0284089, acc 1, prec 0.05131, recall 0.774502
2017-12-10T05:15:09.660426: step 1815, loss 0.246443, acc 0.953125, prec 0.0513061, recall 0.774502
2017-12-10T05:15:09.926691: step 1816, loss 0.144056, acc 0.984375, prec 0.0513529, recall 0.774675
2017-12-10T05:15:10.190264: step 1817, loss 1.01214, acc 1, prec 0.051377, recall 0.774761
2017-12-10T05:15:10.463573: step 1818, loss 0.339471, acc 0.953125, prec 0.0513971, recall 0.774847
2017-12-10T05:15:10.740675: step 1819, loss 0.277885, acc 0.921875, prec 0.0514147, recall 0.774933
2017-12-10T05:15:11.007910: step 1820, loss 0.148207, acc 0.96875, prec 0.0514121, recall 0.774933
2017-12-10T05:15:11.271784: step 1821, loss 0.60037, acc 0.984375, prec 0.0514348, recall 0.775019
2017-12-10T05:15:11.545051: step 1822, loss 0.0940795, acc 0.984375, prec 0.0514575, recall 0.775105
2017-12-10T05:15:11.810455: step 1823, loss 0.050499, acc 0.984375, prec 0.0514803, recall 0.775191
2017-12-10T05:15:12.083483: step 1824, loss 0.0772864, acc 0.9375, prec 0.0514751, recall 0.775191
2017-12-10T05:15:12.356871: step 1825, loss 0.585322, acc 0.953125, prec 0.0515192, recall 0.775362
2017-12-10T05:15:12.631523: step 1826, loss 0.387616, acc 0.9375, prec 0.051514, recall 0.775362
2017-12-10T05:15:12.901568: step 1827, loss 0.151007, acc 0.953125, prec 0.0515101, recall 0.775362
2017-12-10T05:15:13.169768: step 1828, loss 0.237215, acc 0.9375, prec 0.0515049, recall 0.775362
2017-12-10T05:15:13.432405: step 1829, loss 0.205995, acc 0.953125, prec 0.051525, recall 0.775448
2017-12-10T05:15:13.695339: step 1830, loss 0.256747, acc 0.890625, prec 0.0515158, recall 0.775448
2017-12-10T05:15:13.962488: step 1831, loss 0.217186, acc 0.921875, prec 0.0515093, recall 0.775448
2017-12-10T05:15:14.229287: step 1832, loss 0.330928, acc 0.90625, prec 0.0515015, recall 0.775448
2017-12-10T05:15:14.499094: step 1833, loss 0.310638, acc 0.921875, prec 0.051519, recall 0.775534
2017-12-10T05:15:14.765730: step 1834, loss 0.302833, acc 0.90625, prec 0.0515352, recall 0.775619
2017-12-10T05:15:15.037640: step 1835, loss 0.245083, acc 0.921875, prec 0.0515287, recall 0.775619
2017-12-10T05:15:15.301685: step 1836, loss 0.350055, acc 0.890625, prec 0.0515435, recall 0.775705
2017-12-10T05:15:15.563273: step 1837, loss 1.50543, acc 0.921875, prec 0.0515623, recall 0.775495
2017-12-10T05:15:15.836254: step 1838, loss 0.75731, acc 0.9375, prec 0.0516051, recall 0.775665
2017-12-10T05:15:16.103324: step 1839, loss 0.180134, acc 0.90625, prec 0.0515972, recall 0.775665
2017-12-10T05:15:16.375468: step 1840, loss 0.8363, acc 0.875, prec 0.0516108, recall 0.775751
2017-12-10T05:15:16.641451: step 1841, loss 0.464059, acc 0.828125, prec 0.0515964, recall 0.775751
2017-12-10T05:15:16.901514: step 1842, loss 0.401816, acc 0.90625, prec 0.0516126, recall 0.775836
2017-12-10T05:15:17.159274: step 1843, loss 0.338888, acc 0.875, prec 0.0516501, recall 0.776006
2017-12-10T05:15:17.429457: step 1844, loss 0.371892, acc 0.859375, prec 0.0516862, recall 0.776176
2017-12-10T05:15:17.694465: step 1845, loss 0.456877, acc 0.828125, prec 0.0516958, recall 0.776261
2017-12-10T05:15:17.967053: step 1846, loss 0.56046, acc 0.828125, prec 0.0517054, recall 0.776346
2017-12-10T05:15:18.227706: step 1847, loss 0.388445, acc 0.859375, prec 0.0517176, recall 0.77643
2017-12-10T05:15:18.504863: step 1848, loss 0.706235, acc 0.78125, prec 0.0517233, recall 0.776515
2017-12-10T05:15:18.766955: step 1849, loss 0.161586, acc 0.9375, prec 0.051742, recall 0.7766
2017-12-10T05:15:19.031268: step 1850, loss 0.388166, acc 0.828125, prec 0.0517276, recall 0.7766
2017-12-10T05:15:19.303362: step 1851, loss 0.609007, acc 0.765625, prec 0.051732, recall 0.776684
2017-12-10T05:15:19.572130: step 1852, loss 2.05473, acc 0.890625, prec 0.051748, recall 0.776475
2017-12-10T05:15:19.838199: step 1853, loss 0.101337, acc 0.9375, prec 0.0517667, recall 0.77656
2017-12-10T05:15:20.104038: step 1854, loss 0.32993, acc 0.984375, prec 0.0517893, recall 0.776644
2017-12-10T05:15:20.366828: step 1855, loss 0.297731, acc 0.921875, prec 0.0518067, recall 0.776728
2017-12-10T05:15:20.637499: step 1856, loss 0.861643, acc 0.796875, prec 0.0518614, recall 0.776981
2017-12-10T05:15:20.903036: step 1857, loss 0.34401, acc 0.921875, prec 0.0518787, recall 0.777065
2017-12-10T05:15:21.171385: step 1858, loss 0.422932, acc 0.890625, prec 0.0518934, recall 0.777149
2017-12-10T05:15:21.432378: step 1859, loss 0.308432, acc 0.859375, prec 0.0519056, recall 0.777233
2017-12-10T05:15:21.697186: step 1860, loss 0.291655, acc 0.859375, prec 0.0519415, recall 0.777401
2017-12-10T05:15:21.961460: step 1861, loss 0.516014, acc 0.90625, prec 0.0519337, recall 0.777401
2017-12-10T05:15:22.229776: step 1862, loss 0.235446, acc 0.90625, prec 0.0519258, recall 0.777401
2017-12-10T05:15:22.494509: step 1863, loss 0.13524, acc 0.96875, prec 0.0519232, recall 0.777401
2017-12-10T05:15:22.757772: step 1864, loss 0.187747, acc 0.9375, prec 0.051918, recall 0.777401
2017-12-10T05:15:23.028009: step 1865, loss 3.86076, acc 0.859375, prec 0.0519076, recall 0.777108
2017-12-10T05:15:23.303090: step 1866, loss 0.418556, acc 0.84375, prec 0.0518945, recall 0.777108
2017-12-10T05:15:23.571790: step 1867, loss 0.202579, acc 0.921875, prec 0.0519118, recall 0.777192
2017-12-10T05:15:23.843366: step 1868, loss 0.437804, acc 0.875, prec 0.0519252, recall 0.777276
2017-12-10T05:15:24.107594: step 1869, loss 0.0943151, acc 0.984375, prec 0.0519477, recall 0.77736
2017-12-10T05:15:24.371852: step 1870, loss 0.380528, acc 0.875, prec 0.0519373, recall 0.77736
2017-12-10T05:15:24.630115: step 1871, loss 0.263549, acc 0.875, prec 0.0519268, recall 0.77736
2017-12-10T05:15:24.897660: step 1872, loss 0.456437, acc 0.8125, prec 0.051935, recall 0.777444
2017-12-10T05:15:25.163165: step 1873, loss 0.229202, acc 0.90625, prec 0.0519272, recall 0.777444
2017-12-10T05:15:25.427678: step 1874, loss 0.286646, acc 0.921875, prec 0.0519207, recall 0.777444
2017-12-10T05:15:25.696181: step 1875, loss 0.170586, acc 0.921875, prec 0.0519141, recall 0.777444
2017-12-10T05:15:25.964344: step 1876, loss 0.242038, acc 0.90625, prec 0.0519063, recall 0.777444
2017-12-10T05:15:26.228777: step 1877, loss 0.212802, acc 0.96875, prec 0.0519513, recall 0.777611
2017-12-10T05:15:26.502888: step 1878, loss 0.325527, acc 0.953125, prec 0.0520188, recall 0.777861
2017-12-10T05:15:26.766309: step 1879, loss 1.76863, acc 0.921875, prec 0.0520373, recall 0.777653
2017-12-10T05:15:27.034713: step 1880, loss 0.422652, acc 0.84375, prec 0.0520481, recall 0.777736
2017-12-10T05:15:27.312137: step 1881, loss 0.547926, acc 0.8125, prec 0.0520562, recall 0.777819
2017-12-10T05:15:27.573053: step 1882, loss 0.933555, acc 0.84375, prec 0.0521144, recall 0.778069
2017-12-10T05:15:27.837484: step 1883, loss 0.473044, acc 0.828125, prec 0.0521475, recall 0.778235
2017-12-10T05:15:28.110640: step 1884, loss 0.19647, acc 0.90625, prec 0.0521397, recall 0.778235
2017-12-10T05:15:28.381889: step 1885, loss 0.443212, acc 0.890625, prec 0.0521306, recall 0.778235
2017-12-10T05:15:28.653987: step 1886, loss 0.518063, acc 0.84375, prec 0.052165, recall 0.778401
2017-12-10T05:15:28.925785: step 1887, loss 0.644185, acc 0.8125, prec 0.0521493, recall 0.778401
2017-12-10T05:15:29.192366: step 1888, loss 0.430196, acc 0.8125, prec 0.0521336, recall 0.778401
2017-12-10T05:15:29.458648: step 1889, loss 0.234432, acc 0.90625, prec 0.0521733, recall 0.778566
2017-12-10T05:15:29.734003: step 1890, loss 0.554365, acc 0.8125, prec 0.052205, recall 0.778731
2017-12-10T05:15:30.003155: step 1891, loss 2.33642, acc 0.890625, prec 0.0521972, recall 0.778441
2017-12-10T05:15:30.276993: step 1892, loss 4.26431, acc 0.765625, prec 0.0522263, recall 0.778316
2017-12-10T05:15:30.563458: step 1893, loss 1.67141, acc 0.8125, prec 0.052258, recall 0.778481
2017-12-10T05:15:30.829179: step 1894, loss 0.739187, acc 0.765625, prec 0.0522384, recall 0.778481
2017-12-10T05:15:31.099646: step 1895, loss 1.03053, acc 0.71875, prec 0.052215, recall 0.778481
2017-12-10T05:15:31.366627: step 1896, loss 1.01335, acc 0.71875, prec 0.0521915, recall 0.778481
2017-12-10T05:15:31.638372: step 1897, loss 0.827417, acc 0.765625, prec 0.0522193, recall 0.778646
2017-12-10T05:15:31.898728: step 1898, loss 0.926423, acc 0.671875, prec 0.0521919, recall 0.778646
2017-12-10T05:15:32.164414: step 1899, loss 0.753197, acc 0.78125, prec 0.0521737, recall 0.778646
2017-12-10T05:15:32.428201: step 1900, loss 1.0505, acc 0.71875, prec 0.0521975, recall 0.77881
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-1900

2017-12-10T05:15:33.646051: step 1901, loss 1.18707, acc 0.734375, prec 0.052199, recall 0.778893
2017-12-10T05:15:33.907747: step 1902, loss 1.03838, acc 0.734375, prec 0.0521769, recall 0.778893
2017-12-10T05:15:34.180655: step 1903, loss 0.569015, acc 0.78125, prec 0.0521824, recall 0.778975
2017-12-10T05:15:34.451833: step 1904, loss 0.610459, acc 0.8125, prec 0.0521668, recall 0.778975
2017-12-10T05:15:34.715443: step 1905, loss 0.695322, acc 0.78125, prec 0.0521722, recall 0.779057
2017-12-10T05:15:34.977605: step 1906, loss 0.887498, acc 0.765625, prec 0.0521527, recall 0.779057
2017-12-10T05:15:35.248165: step 1907, loss 0.26561, acc 0.90625, prec 0.0521685, recall 0.779139
2017-12-10T05:15:35.510843: step 1908, loss 0.457189, acc 0.8125, prec 0.052153, recall 0.779139
2017-12-10T05:15:35.780438: step 1909, loss 0.171163, acc 0.921875, prec 0.0521465, recall 0.779139
2017-12-10T05:15:36.051786: step 1910, loss 0.406902, acc 0.875, prec 0.0521597, recall 0.779221
2017-12-10T05:15:36.320031: step 1911, loss 6.63302, acc 0.890625, prec 0.052199, recall 0.779096
2017-12-10T05:15:36.597868: step 1912, loss 0.30032, acc 0.90625, prec 0.0522147, recall 0.779177
2017-12-10T05:15:36.862598: step 1913, loss 0.144367, acc 0.96875, prec 0.0522121, recall 0.779177
2017-12-10T05:15:37.129011: step 1914, loss 0.557657, acc 0.84375, prec 0.0522227, recall 0.779259
2017-12-10T05:15:37.397686: step 1915, loss 0.375704, acc 0.90625, prec 0.052262, recall 0.779423
2017-12-10T05:15:37.665641: step 1916, loss 1.32857, acc 0.8125, prec 0.0522699, recall 0.779504
2017-12-10T05:15:37.942698: step 1917, loss 0.582615, acc 0.8125, prec 0.0523014, recall 0.779667
2017-12-10T05:15:38.207106: step 1918, loss 1.50219, acc 0.875, prec 0.0523145, recall 0.779749
2017-12-10T05:15:38.474792: step 1919, loss 0.581998, acc 0.828125, prec 0.0523002, recall 0.779749
2017-12-10T05:15:38.743600: step 1920, loss 0.525062, acc 0.859375, prec 0.0523355, recall 0.779911
2017-12-10T05:15:39.012181: step 1921, loss 1.12069, acc 0.75, prec 0.0523383, recall 0.779993
2017-12-10T05:15:39.272428: step 1922, loss 1.06275, acc 0.765625, prec 0.0523188, recall 0.779993
2017-12-10T05:15:39.535267: step 1923, loss 0.554731, acc 0.8125, prec 0.0523267, recall 0.780074
2017-12-10T05:15:39.798798: step 1924, loss 0.434282, acc 0.890625, prec 0.0523177, recall 0.780074
2017-12-10T05:15:40.064354: step 1925, loss 0.568502, acc 0.828125, prec 0.0523034, recall 0.780074
2017-12-10T05:15:40.325549: step 1926, loss 0.705502, acc 0.796875, prec 0.0523569, recall 0.780317
2017-12-10T05:15:40.589664: step 1927, loss 0.590446, acc 0.890625, prec 0.0523713, recall 0.780398
2017-12-10T05:15:40.852481: step 1928, loss 0.514202, acc 0.859375, prec 0.0524299, recall 0.78064
2017-12-10T05:15:41.123906: step 1929, loss 0.348207, acc 0.890625, prec 0.0524209, recall 0.78064
2017-12-10T05:15:41.384204: step 1930, loss 1.21632, acc 0.859375, prec 0.0524326, recall 0.780721
2017-12-10T05:15:41.651950: step 1931, loss 0.901996, acc 0.953125, prec 0.0525224, recall 0.781043
2017-12-10T05:15:41.921951: step 1932, loss 0.26732, acc 0.90625, prec 0.0525146, recall 0.781043
2017-12-10T05:15:42.194404: step 1933, loss 0.128092, acc 0.9375, prec 0.0525562, recall 0.781204
2017-12-10T05:15:42.470270: step 1934, loss 0.441858, acc 0.875, prec 0.0525926, recall 0.781365
2017-12-10T05:15:42.743457: step 1935, loss 0.292742, acc 0.921875, prec 0.0526095, recall 0.781445
2017-12-10T05:15:43.016581: step 1936, loss 0.330076, acc 0.90625, prec 0.0526718, recall 0.781685
2017-12-10T05:15:43.282640: step 1937, loss 0.442417, acc 0.890625, prec 0.0526628, recall 0.781685
2017-12-10T05:15:43.547260: step 1938, loss 0.184687, acc 0.953125, prec 0.0526822, recall 0.781765
2017-12-10T05:15:43.809465: step 1939, loss 0.346091, acc 0.921875, prec 0.0526757, recall 0.781765
2017-12-10T05:15:44.080464: step 1940, loss 0.298617, acc 0.921875, prec 0.0526926, recall 0.781845
2017-12-10T05:15:44.345171: step 1941, loss 0.157767, acc 0.953125, prec 0.0526887, recall 0.781845
2017-12-10T05:15:44.612431: step 1942, loss 0.440698, acc 0.859375, prec 0.052677, recall 0.781845
2017-12-10T05:15:44.880427: step 1943, loss 0.415968, acc 0.875, prec 0.05269, recall 0.781925
2017-12-10T05:15:45.154646: step 1944, loss 1.08082, acc 0.953125, prec 0.0527328, recall 0.782084
2017-12-10T05:15:45.425544: step 1945, loss 4.17337, acc 0.859375, prec 0.0527457, recall 0.781878
2017-12-10T05:15:45.701294: step 1946, loss 0.416055, acc 0.9375, prec 0.0528106, recall 0.782117
2017-12-10T05:15:45.971899: step 1947, loss 0.279123, acc 0.90625, prec 0.0528494, recall 0.782276
2017-12-10T05:15:46.239990: step 1948, loss 0.24638, acc 0.921875, prec 0.0528429, recall 0.782276
2017-12-10T05:15:46.509963: step 1949, loss 5.11562, acc 0.953125, prec 0.0528637, recall 0.78207
2017-12-10T05:15:46.777604: step 1950, loss 0.607356, acc 0.796875, prec 0.0528701, recall 0.782149
2017-12-10T05:15:47.050370: step 1951, loss 0.758521, acc 0.796875, prec 0.0528531, recall 0.782149
2017-12-10T05:15:47.321727: step 1952, loss 0.513005, acc 0.828125, prec 0.0528621, recall 0.782229
2017-12-10T05:15:47.585054: step 1953, loss 0.888974, acc 0.734375, prec 0.05284, recall 0.782229
2017-12-10T05:15:47.854891: step 1954, loss 0.912167, acc 0.703125, prec 0.0528386, recall 0.782308
2017-12-10T05:15:48.124176: step 1955, loss 0.435469, acc 0.84375, prec 0.0528722, recall 0.782466
2017-12-10T05:15:48.398251: step 1956, loss 0.607851, acc 0.84375, prec 0.0528825, recall 0.782545
2017-12-10T05:15:48.662792: step 1957, loss 1.0508, acc 0.765625, prec 0.0528863, recall 0.782624
2017-12-10T05:15:48.935330: step 1958, loss 0.500589, acc 0.859375, prec 0.0529211, recall 0.782782
2017-12-10T05:15:49.207180: step 1959, loss 0.446273, acc 0.859375, prec 0.0529094, recall 0.782782
2017-12-10T05:15:49.470526: step 1960, loss 0.898302, acc 0.84375, prec 0.0528964, recall 0.782782
2017-12-10T05:15:49.740871: step 1961, loss 0.48595, acc 0.890625, prec 0.0529106, recall 0.782861
2017-12-10T05:15:50.012410: step 1962, loss 0.426125, acc 0.890625, prec 0.0529015, recall 0.782861
2017-12-10T05:15:50.279131: step 1963, loss 2.89185, acc 0.796875, prec 0.0529091, recall 0.782656
2017-12-10T05:15:50.546816: step 1964, loss 0.406466, acc 0.859375, prec 0.0529207, recall 0.782735
2017-12-10T05:15:50.809303: step 1965, loss 0.152124, acc 0.921875, prec 0.0529142, recall 0.782735
2017-12-10T05:15:51.083872: step 1966, loss 0.279494, acc 0.859375, prec 0.0529722, recall 0.782971
2017-12-10T05:15:51.345774: step 1967, loss 0.243184, acc 0.921875, prec 0.0530353, recall 0.783207
2017-12-10T05:15:51.611963: step 1968, loss 0.294464, acc 0.90625, prec 0.0530507, recall 0.783285
2017-12-10T05:15:51.884919: step 1969, loss 0.699425, acc 0.890625, prec 0.0530648, recall 0.783363
2017-12-10T05:15:52.151364: step 1970, loss 0.273116, acc 0.875, prec 0.0530776, recall 0.783442
2017-12-10T05:15:52.420614: step 1971, loss 0.385558, acc 0.84375, prec 0.053111, recall 0.783598
2017-12-10T05:15:52.687711: step 1972, loss 0.189704, acc 0.875, prec 0.0531006, recall 0.783598
2017-12-10T05:15:52.947518: step 1973, loss 0.399009, acc 0.875, prec 0.0531134, recall 0.783676
2017-12-10T05:15:53.209811: step 1974, loss 0.398816, acc 0.890625, prec 0.0531043, recall 0.783676
2017-12-10T05:15:53.476001: step 1975, loss 3.32711, acc 0.90625, prec 0.0530978, recall 0.783394
2017-12-10T05:15:53.746567: step 1976, loss 0.539909, acc 0.9375, prec 0.0531389, recall 0.78355
2017-12-10T05:15:54.013011: step 1977, loss 0.529009, acc 0.890625, prec 0.0531993, recall 0.783784
2017-12-10T05:15:54.284711: step 1978, loss 0.531295, acc 0.875, prec 0.0531889, recall 0.783784
2017-12-10T05:15:54.547009: step 1979, loss 0.807262, acc 0.84375, prec 0.0531759, recall 0.783784
2017-12-10T05:15:54.806400: step 1980, loss 1.86127, acc 0.9375, prec 0.053172, recall 0.783501
2017-12-10T05:15:55.085309: step 1981, loss 0.384324, acc 0.921875, prec 0.0531886, recall 0.783579
2017-12-10T05:15:55.361602: step 1982, loss 0.702994, acc 0.828125, prec 0.0531975, recall 0.783657
2017-12-10T05:15:55.624683: step 1983, loss 0.398167, acc 0.828125, prec 0.0532063, recall 0.783735
2017-12-10T05:15:55.887504: step 1984, loss 0.76412, acc 0.828125, prec 0.0532151, recall 0.783813
2017-12-10T05:15:56.153080: step 1985, loss 0.295111, acc 0.90625, prec 0.0532073, recall 0.783813
2017-12-10T05:15:56.413579: step 1986, loss 0.44435, acc 0.875, prec 0.0532201, recall 0.783891
2017-12-10T05:15:56.681926: step 1987, loss 0.475348, acc 0.890625, prec 0.0532341, recall 0.783968
2017-12-10T05:15:56.921016: step 1988, loss 0.700282, acc 0.846154, prec 0.0532237, recall 0.783968
2017-12-10T05:15:57.200392: step 1989, loss 0.660948, acc 0.8125, prec 0.0532081, recall 0.783968
2017-12-10T05:15:57.471932: step 1990, loss 0.180914, acc 0.9375, prec 0.053226, recall 0.784046
2017-12-10T05:15:57.736892: step 1991, loss 0.524962, acc 0.84375, prec 0.0532592, recall 0.784201
2017-12-10T05:15:57.999433: step 1992, loss 0.416526, acc 0.859375, prec 0.0532475, recall 0.784201
2017-12-10T05:15:58.263018: step 1993, loss 0.24278, acc 0.921875, prec 0.0532641, recall 0.784279
2017-12-10T05:15:58.529428: step 1994, loss 0.171648, acc 0.953125, prec 0.0532833, recall 0.784356
2017-12-10T05:15:58.790910: step 1995, loss 0.387889, acc 0.921875, prec 0.0532999, recall 0.784433
2017-12-10T05:15:59.062917: step 1996, loss 0.213156, acc 0.90625, prec 0.0532921, recall 0.784433
2017-12-10T05:15:59.332105: step 1997, loss 0.191419, acc 0.9375, prec 0.0532869, recall 0.784433
2017-12-10T05:15:59.596476: step 1998, loss 0.14858, acc 0.953125, prec 0.053283, recall 0.784433
2017-12-10T05:15:59.863550: step 1999, loss 0.216734, acc 0.9375, prec 0.0532778, recall 0.784433
2017-12-10T05:16:00.132302: step 2000, loss 0.212086, acc 0.921875, prec 0.0533174, recall 0.784588
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-2000

2017-12-10T05:16:01.535273: step 2001, loss 0.0917667, acc 0.96875, prec 0.0533379, recall 0.784665
2017-12-10T05:16:01.804459: step 2002, loss 0.171688, acc 0.953125, prec 0.053334, recall 0.784665
2017-12-10T05:16:02.068491: step 2003, loss 0.315916, acc 0.96875, prec 0.0533775, recall 0.784819
2017-12-10T05:16:02.338805: step 2004, loss 0.11058, acc 0.9375, prec 0.0533953, recall 0.784896
2017-12-10T05:16:02.600482: step 2005, loss 0.210048, acc 0.953125, prec 0.0533914, recall 0.784896
2017-12-10T05:16:02.871692: step 2006, loss 0.252192, acc 0.953125, prec 0.0533875, recall 0.784896
2017-12-10T05:16:03.139797: step 2007, loss 0.325843, acc 0.96875, prec 0.053408, recall 0.784973
2017-12-10T05:16:03.403525: step 2008, loss 0.178478, acc 0.984375, prec 0.0534297, recall 0.78505
2017-12-10T05:16:03.671574: step 2009, loss 0.0614159, acc 0.984375, prec 0.0534284, recall 0.78505
2017-12-10T05:16:03.940919: step 2010, loss 0.0150525, acc 1, prec 0.0534284, recall 0.78505
2017-12-10T05:16:04.203329: step 2011, loss 0.0307634, acc 0.984375, prec 0.0534271, recall 0.78505
2017-12-10T05:16:04.468391: step 2012, loss 0.0698466, acc 0.953125, prec 0.0534232, recall 0.78505
2017-12-10T05:16:04.735706: step 2013, loss 0.148166, acc 0.9375, prec 0.0534411, recall 0.785127
2017-12-10T05:16:05.001416: step 2014, loss 0.0543437, acc 0.984375, prec 0.0534398, recall 0.785127
2017-12-10T05:16:05.264634: step 2015, loss 2.57949, acc 0.953125, prec 0.0534372, recall 0.784846
2017-12-10T05:16:05.543979: step 2016, loss 0.0179027, acc 1, prec 0.0534372, recall 0.784846
2017-12-10T05:16:05.813057: step 2017, loss 0.103582, acc 0.984375, prec 0.0534359, recall 0.784846
2017-12-10T05:16:06.085084: step 2018, loss 0.0363253, acc 0.984375, prec 0.0534346, recall 0.784846
2017-12-10T05:16:06.354289: step 2019, loss 0.0701573, acc 0.984375, prec 0.0534793, recall 0.785
2017-12-10T05:16:06.617712: step 2020, loss 0.0872932, acc 0.96875, prec 0.0534767, recall 0.785
2017-12-10T05:16:06.882669: step 2021, loss 0.0321988, acc 0.984375, prec 0.0534754, recall 0.785
2017-12-10T05:16:07.148017: step 2022, loss 0.219923, acc 0.953125, prec 0.0535176, recall 0.785153
2017-12-10T05:16:07.411649: step 2023, loss 0.487413, acc 0.953125, prec 0.0535597, recall 0.785307
2017-12-10T05:16:07.676631: step 2024, loss 0.0730011, acc 0.953125, prec 0.0535558, recall 0.785307
2017-12-10T05:16:07.949782: step 2025, loss 0.0776153, acc 0.953125, prec 0.0535519, recall 0.785307
2017-12-10T05:16:08.213139: step 2026, loss 0.153814, acc 0.921875, prec 0.0535684, recall 0.785383
2017-12-10T05:16:08.476057: step 2027, loss 0.381787, acc 0.9375, prec 0.0536322, recall 0.785613
2017-12-10T05:16:08.743499: step 2028, loss 0.123661, acc 0.96875, prec 0.0536296, recall 0.785613
2017-12-10T05:16:09.003507: step 2029, loss 0.24435, acc 0.9375, prec 0.0536474, recall 0.785689
2017-12-10T05:16:09.266950: step 2030, loss 0.157463, acc 0.953125, prec 0.0536895, recall 0.785841
2017-12-10T05:16:09.530584: step 2031, loss 0.231411, acc 0.953125, prec 0.0537316, recall 0.785994
2017-12-10T05:16:09.793946: step 2032, loss 0.0858151, acc 0.953125, prec 0.0537276, recall 0.785994
2017-12-10T05:16:10.057242: step 2033, loss 0.357567, acc 0.875, prec 0.0537402, recall 0.78607
2017-12-10T05:16:10.319836: step 2034, loss 0.426568, acc 0.921875, prec 0.0538026, recall 0.786298
2017-12-10T05:16:10.585265: step 2035, loss 6.6212, acc 0.9375, prec 0.0537987, recall 0.786018
2017-12-10T05:16:10.851180: step 2036, loss 0.331411, acc 0.921875, prec 0.0537922, recall 0.786018
2017-12-10T05:16:11.115364: step 2037, loss 0.274065, acc 0.890625, prec 0.053783, recall 0.786018
2017-12-10T05:16:11.376536: step 2038, loss 0.299226, acc 0.921875, prec 0.0537765, recall 0.786018
2017-12-10T05:16:11.641142: step 2039, loss 0.544724, acc 0.875, prec 0.053812, recall 0.78617
2017-12-10T05:16:11.911599: step 2040, loss 0.701907, acc 0.78125, prec 0.0537937, recall 0.78617
2017-12-10T05:16:12.182806: step 2041, loss 0.564061, acc 0.875, prec 0.0538062, recall 0.786246
2017-12-10T05:16:12.450594: step 2042, loss 0.559815, acc 0.828125, prec 0.0538378, recall 0.786397
2017-12-10T05:16:12.715933: step 2043, loss 0.472602, acc 0.90625, prec 0.0538299, recall 0.786397
2017-12-10T05:16:12.980295: step 2044, loss 0.481281, acc 0.828125, prec 0.0538385, recall 0.786473
2017-12-10T05:16:13.247327: step 2045, loss 0.497668, acc 0.84375, prec 0.0538713, recall 0.786624
2017-12-10T05:16:13.513964: step 2046, loss 0.61921, acc 0.78125, prec 0.0539218, recall 0.78685
2017-12-10T05:16:13.777686: step 2047, loss 0.564709, acc 0.890625, prec 0.0539127, recall 0.78685
2017-12-10T05:16:14.043631: step 2048, loss 0.574616, acc 0.84375, prec 0.0539683, recall 0.787076
2017-12-10T05:16:14.313476: step 2049, loss 0.40739, acc 0.90625, prec 0.0539834, recall 0.787151
2017-12-10T05:16:14.574121: step 2050, loss 0.382516, acc 0.9375, prec 0.054024, recall 0.787302
2017-12-10T05:16:14.837810: step 2051, loss 0.97806, acc 0.953125, prec 0.0540429, recall 0.787377
2017-12-10T05:16:15.114391: step 2052, loss 0.208205, acc 0.90625, prec 0.0540351, recall 0.787377
2017-12-10T05:16:15.391146: step 2053, loss 0.770682, acc 0.890625, prec 0.0540488, recall 0.787452
2017-12-10T05:16:15.661887: step 2054, loss 0.337823, acc 0.890625, prec 0.0540626, recall 0.787526
2017-12-10T05:16:15.926582: step 2055, loss 0.321898, acc 0.875, prec 0.0540521, recall 0.787526
2017-12-10T05:16:16.194652: step 2056, loss 3.05722, acc 0.953125, prec 0.0540495, recall 0.787249
2017-12-10T05:16:16.459588: step 2057, loss 0.230088, acc 0.9375, prec 0.0541129, recall 0.787474
2017-12-10T05:16:16.731074: step 2058, loss 0.244956, acc 0.9375, prec 0.0541076, recall 0.787474
2017-12-10T05:16:16.994960: step 2059, loss 0.318729, acc 0.859375, prec 0.0541416, recall 0.787623
2017-12-10T05:16:17.262146: step 2060, loss 0.242155, acc 0.921875, prec 0.054135, recall 0.787623
2017-12-10T05:16:17.529394: step 2061, loss 0.305184, acc 0.890625, prec 0.0541259, recall 0.787623
2017-12-10T05:16:17.793102: step 2062, loss 0.235635, acc 0.875, prec 0.0541154, recall 0.787623
2017-12-10T05:16:18.058692: step 2063, loss 0.210081, acc 0.921875, prec 0.0541089, recall 0.787623
2017-12-10T05:16:18.328337: step 2064, loss 0.457036, acc 0.890625, prec 0.0540997, recall 0.787623
2017-12-10T05:16:18.591040: step 2065, loss 0.450897, acc 0.828125, prec 0.0541082, recall 0.787698
2017-12-10T05:16:18.858793: step 2066, loss 2.91307, acc 0.953125, prec 0.0541284, recall 0.787496
2017-12-10T05:16:19.129633: step 2067, loss 0.366008, acc 0.90625, prec 0.0542119, recall 0.787794
2017-12-10T05:16:19.399010: step 2068, loss 0.455647, acc 0.859375, prec 0.054223, recall 0.787868
2017-12-10T05:16:19.660558: step 2069, loss 0.498443, acc 0.890625, prec 0.0542366, recall 0.787943
2017-12-10T05:16:19.929742: step 2070, loss 0.821191, acc 0.796875, prec 0.0542424, recall 0.788017
2017-12-10T05:16:20.187837: step 2071, loss 1.21438, acc 0.765625, prec 0.0542684, recall 0.788165
2017-12-10T05:16:20.448513: step 2072, loss 0.676896, acc 0.796875, prec 0.0542514, recall 0.788165
2017-12-10T05:16:20.714140: step 2073, loss 0.676284, acc 0.8125, prec 0.0542813, recall 0.788314
2017-12-10T05:16:20.975583: step 2074, loss 0.606276, acc 0.84375, prec 0.0542682, recall 0.788314
2017-12-10T05:16:21.241726: step 2075, loss 1.2444, acc 0.765625, prec 0.0543397, recall 0.788609
2017-12-10T05:16:21.501626: step 2076, loss 0.929896, acc 0.828125, prec 0.0543253, recall 0.788609
2017-12-10T05:16:21.771888: step 2077, loss 0.89488, acc 0.84375, prec 0.0543123, recall 0.788609
2017-12-10T05:16:22.044029: step 2078, loss 0.539826, acc 0.890625, prec 0.0543259, recall 0.788683
2017-12-10T05:16:22.307939: step 2079, loss 0.516003, acc 0.828125, prec 0.0543115, recall 0.788683
2017-12-10T05:16:22.575934: step 2080, loss 0.32856, acc 0.828125, prec 0.0542971, recall 0.788683
2017-12-10T05:16:22.846006: step 2081, loss 0.402815, acc 0.90625, prec 0.0543348, recall 0.788831
2017-12-10T05:16:23.109799: step 2082, loss 0.372349, acc 0.890625, prec 0.0543711, recall 0.788978
2017-12-10T05:16:23.369824: step 2083, loss 1.16579, acc 0.859375, prec 0.0544048, recall 0.789125
2017-12-10T05:16:23.635437: step 2084, loss 0.282269, acc 0.953125, prec 0.0544236, recall 0.789199
2017-12-10T05:16:23.904013: step 2085, loss 0.326256, acc 0.890625, prec 0.0544371, recall 0.789272
2017-12-10T05:16:24.173479: step 2086, loss 0.285503, acc 0.90625, prec 0.054452, recall 0.789345
2017-12-10T05:16:24.439666: step 2087, loss 0.234648, acc 0.921875, prec 0.0544682, recall 0.789419
2017-12-10T05:16:24.702948: step 2088, loss 0.301318, acc 0.921875, prec 0.0544843, recall 0.789492
2017-12-10T05:16:24.970973: step 2089, loss 0.215428, acc 0.9375, prec 0.0544791, recall 0.789492
2017-12-10T05:16:25.244061: step 2090, loss 0.0896989, acc 0.953125, prec 0.0544979, recall 0.789565
2017-12-10T05:16:25.509207: step 2091, loss 0.211576, acc 0.9375, prec 0.0545153, recall 0.789638
2017-12-10T05:16:25.773977: step 2092, loss 0.127929, acc 0.96875, prec 0.0545581, recall 0.789785
2017-12-10T05:16:26.039712: step 2093, loss 0.168797, acc 0.953125, prec 0.0545996, recall 0.789931
2017-12-10T05:16:26.304770: step 2094, loss 0.203175, acc 0.9375, prec 0.0545943, recall 0.789931
2017-12-10T05:16:26.564266: step 2095, loss 0.0797625, acc 0.984375, prec 0.054593, recall 0.789931
2017-12-10T05:16:26.830054: step 2096, loss 0.117524, acc 1, prec 0.0546157, recall 0.790003
2017-12-10T05:16:27.105304: step 2097, loss 0.287879, acc 0.9375, prec 0.0546105, recall 0.790003
2017-12-10T05:16:27.375236: step 2098, loss 0.305852, acc 0.953125, prec 0.0546746, recall 0.790222
2017-12-10T05:16:27.648840: step 2099, loss 0.0179186, acc 1, prec 0.0546746, recall 0.790222
2017-12-10T05:16:27.913283: step 2100, loss 0.039402, acc 0.984375, prec 0.0546733, recall 0.790222

Evaluation:
2017-12-10T05:16:35.489968: step 2100, loss 4.81105, acc 0.966597, prec 0.0552842, recall 0.763409

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-2100

2017-12-10T05:16:36.787574: step 2101, loss 0.0598599, acc 0.984375, prec 0.0553054, recall 0.763487
2017-12-10T05:16:37.054702: step 2102, loss 0.216031, acc 0.96875, prec 0.0553252, recall 0.763565
2017-12-10T05:16:37.314732: step 2103, loss 0.0990836, acc 1, prec 0.0553702, recall 0.76372
2017-12-10T05:16:37.583267: step 2104, loss 1.94242, acc 0.96875, prec 0.0553689, recall 0.763469
2017-12-10T05:16:37.855232: step 2105, loss 1.62535, acc 0.96875, prec 0.0553901, recall 0.763296
2017-12-10T05:16:38.131935: step 2106, loss 0.0514972, acc 1, prec 0.0554126, recall 0.763374
2017-12-10T05:16:38.402271: step 2107, loss 0.0404223, acc 0.96875, prec 0.05541, recall 0.763374
2017-12-10T05:16:38.673020: step 2108, loss 0.160584, acc 0.90625, prec 0.0554246, recall 0.763451
2017-12-10T05:16:38.939397: step 2109, loss 0.26441, acc 0.921875, prec 0.055418, recall 0.763451
2017-12-10T05:16:39.204805: step 2110, loss 0.29976, acc 0.9375, prec 0.0554352, recall 0.763529
2017-12-10T05:16:39.470423: step 2111, loss 0.274821, acc 0.90625, prec 0.0554497, recall 0.763607
2017-12-10T05:16:39.739987: step 2112, loss 0.815944, acc 0.875, prec 0.0554841, recall 0.763761
2017-12-10T05:16:40.001195: step 2113, loss 0.757486, acc 0.828125, prec 0.0554921, recall 0.763839
2017-12-10T05:16:40.263858: step 2114, loss 0.26799, acc 0.953125, prec 0.0555106, recall 0.763916
2017-12-10T05:16:40.527721: step 2115, loss 0.190227, acc 0.953125, prec 0.0555067, recall 0.763916
2017-12-10T05:16:40.803003: step 2116, loss 0.629886, acc 0.78125, prec 0.0555331, recall 0.764071
2017-12-10T05:16:41.073756: step 2117, loss 0.790968, acc 0.8125, prec 0.0555397, recall 0.764148
2017-12-10T05:16:41.346101: step 2118, loss 0.491924, acc 0.859375, prec 0.0555503, recall 0.764225
2017-12-10T05:16:41.610577: step 2119, loss 0.339427, acc 0.921875, prec 0.0555437, recall 0.764225
2017-12-10T05:16:41.873840: step 2120, loss 0.644944, acc 0.84375, prec 0.0555529, recall 0.764302
2017-12-10T05:16:42.149532: step 2121, loss 0.581273, acc 0.828125, prec 0.0555384, recall 0.764302
2017-12-10T05:16:42.423770: step 2122, loss 0.225277, acc 0.90625, prec 0.0555529, recall 0.764379
2017-12-10T05:16:42.691901: step 2123, loss 0.468295, acc 0.90625, prec 0.0555899, recall 0.764533
2017-12-10T05:16:42.957716: step 2124, loss 0.384103, acc 0.9375, prec 0.0556518, recall 0.764763
2017-12-10T05:16:43.221702: step 2125, loss 0.206391, acc 0.90625, prec 0.0556887, recall 0.764917
2017-12-10T05:16:43.487244: step 2126, loss 0.598358, acc 0.890625, prec 0.0557467, recall 0.765147
2017-12-10T05:16:43.756726: step 2127, loss 0.30618, acc 0.9375, prec 0.0558087, recall 0.765376
2017-12-10T05:16:44.023198: step 2128, loss 0.186257, acc 0.953125, prec 0.0558047, recall 0.765376
2017-12-10T05:16:44.287754: step 2129, loss 0.147227, acc 0.96875, prec 0.0558468, recall 0.765528
2017-12-10T05:16:44.549117: step 2130, loss 0.183837, acc 0.953125, prec 0.0558653, recall 0.765605
2017-12-10T05:16:44.820336: step 2131, loss 0.0805503, acc 0.984375, prec 0.0558863, recall 0.765681
2017-12-10T05:16:45.096087: step 2132, loss 0.174229, acc 0.9375, prec 0.0559034, recall 0.765757
2017-12-10T05:16:45.361133: step 2133, loss 0.206249, acc 0.90625, prec 0.0558955, recall 0.765757
2017-12-10T05:16:45.632443: step 2134, loss 0.34422, acc 0.890625, prec 0.0558862, recall 0.765757
2017-12-10T05:16:45.895679: step 2135, loss 0.0993501, acc 0.953125, prec 0.0558822, recall 0.765757
2017-12-10T05:16:46.174506: step 2136, loss 0.116408, acc 0.96875, prec 0.0559243, recall 0.765909
2017-12-10T05:16:46.438245: step 2137, loss 0.0149922, acc 1, prec 0.0559243, recall 0.765909
2017-12-10T05:16:46.700607: step 2138, loss 12.87, acc 0.953125, prec 0.055923, recall 0.765412
2017-12-10T05:16:46.971935: step 2139, loss 0.0437364, acc 0.96875, prec 0.0559204, recall 0.765412
2017-12-10T05:16:47.239006: step 2140, loss 2.11549, acc 0.9375, prec 0.0559164, recall 0.765164
2017-12-10T05:16:47.510447: step 2141, loss 0.220587, acc 0.90625, prec 0.0559308, recall 0.76524
2017-12-10T05:16:47.785685: step 2142, loss 0.615974, acc 0.84375, prec 0.0559175, recall 0.76524
2017-12-10T05:16:48.047736: step 2143, loss 0.350556, acc 0.890625, prec 0.0559083, recall 0.76524
2017-12-10T05:16:48.314309: step 2144, loss 0.767431, acc 0.71875, prec 0.0559068, recall 0.765316
2017-12-10T05:16:48.587727: step 2145, loss 1.47229, acc 0.703125, prec 0.0559263, recall 0.765468
2017-12-10T05:16:48.848225: step 2146, loss 0.971129, acc 0.6875, prec 0.0559222, recall 0.765544
2017-12-10T05:16:49.116332: step 2147, loss 1.10993, acc 0.6875, prec 0.0559181, recall 0.76562
2017-12-10T05:16:49.379109: step 2148, loss 1.30865, acc 0.703125, prec 0.0559599, recall 0.765847
2017-12-10T05:16:49.648067: step 2149, loss 1.21019, acc 0.734375, prec 0.055982, recall 0.765999
2017-12-10T05:16:49.909835: step 2150, loss 1.30762, acc 0.703125, prec 0.0559569, recall 0.765999
2017-12-10T05:16:50.171362: step 2151, loss 1.28951, acc 0.75, prec 0.0559804, recall 0.76615
2017-12-10T05:16:50.432361: step 2152, loss 1.19637, acc 0.75, prec 0.0559592, recall 0.76615
2017-12-10T05:16:50.703462: step 2153, loss 0.605573, acc 0.796875, prec 0.0559643, recall 0.766225
2017-12-10T05:16:50.976566: step 2154, loss 0.640415, acc 0.8125, prec 0.0559485, recall 0.766225
2017-12-10T05:16:51.237098: step 2155, loss 1.04287, acc 0.78125, prec 0.0559523, recall 0.766301
2017-12-10T05:16:51.505516: step 2156, loss 0.661475, acc 0.859375, prec 0.0559404, recall 0.766301
2017-12-10T05:16:51.772113: step 2157, loss 0.306409, acc 0.921875, prec 0.0559783, recall 0.766452
2017-12-10T05:16:52.036480: step 2158, loss 0.403116, acc 0.890625, prec 0.0559691, recall 0.766452
2017-12-10T05:16:52.302768: step 2159, loss 0.367416, acc 0.90625, prec 0.0559612, recall 0.766452
2017-12-10T05:16:52.567480: step 2160, loss 0.336839, acc 0.921875, prec 0.0559546, recall 0.766452
2017-12-10T05:16:52.833883: step 2161, loss 0.128755, acc 0.9375, prec 0.0559938, recall 0.766602
2017-12-10T05:16:53.112769: step 2162, loss 0.327693, acc 0.921875, prec 0.0559872, recall 0.766602
2017-12-10T05:16:53.382083: step 2163, loss 1.24749, acc 0.921875, prec 0.0560028, recall 0.766677
2017-12-10T05:16:53.659673: step 2164, loss 0.173269, acc 0.921875, prec 0.0560185, recall 0.766753
2017-12-10T05:16:53.927978: step 2165, loss 0.14325, acc 0.953125, prec 0.0560367, recall 0.766828
2017-12-10T05:16:54.200917: step 2166, loss 0.118518, acc 0.953125, prec 0.056055, recall 0.766903
2017-12-10T05:16:54.470263: step 2167, loss 0.0802117, acc 0.96875, prec 0.0560745, recall 0.766978
2017-12-10T05:16:54.744122: step 2168, loss 0.300437, acc 0.9375, prec 0.0560915, recall 0.767053
2017-12-10T05:16:55.009186: step 2169, loss 0.0442126, acc 0.984375, prec 0.0560902, recall 0.767053
2017-12-10T05:16:55.275653: step 2170, loss 0.151836, acc 0.96875, prec 0.0561097, recall 0.767128
2017-12-10T05:16:55.545727: step 2171, loss 0.848687, acc 0.984375, prec 0.0561528, recall 0.767277
2017-12-10T05:16:55.808755: step 2172, loss 0.164324, acc 0.96875, prec 0.0561502, recall 0.767277
2017-12-10T05:16:56.073884: step 2173, loss 3.79059, acc 0.890625, prec 0.0561422, recall 0.767031
2017-12-10T05:16:56.346250: step 2174, loss 0.291164, acc 0.921875, prec 0.0562022, recall 0.767255
2017-12-10T05:16:56.613378: step 2175, loss 0.274875, acc 0.9375, prec 0.0561969, recall 0.767255
2017-12-10T05:16:56.877434: step 2176, loss 0.244093, acc 0.921875, prec 0.0561903, recall 0.767255
2017-12-10T05:16:57.155406: step 2177, loss 0.36657, acc 0.875, prec 0.056202, recall 0.76733
2017-12-10T05:16:57.425952: step 2178, loss 0.4991, acc 0.90625, prec 0.0562162, recall 0.767405
2017-12-10T05:16:57.699047: step 2179, loss 0.67535, acc 0.875, prec 0.0562056, recall 0.767405
2017-12-10T05:16:57.966075: step 2180, loss 0.621721, acc 0.875, prec 0.0562394, recall 0.767554
2017-12-10T05:16:58.230460: step 2181, loss 0.756118, acc 0.8125, prec 0.0562457, recall 0.767628
2017-12-10T05:16:58.492688: step 2182, loss 0.311731, acc 0.90625, prec 0.05626, recall 0.767703
2017-12-10T05:16:58.759328: step 2183, loss 0.498508, acc 0.828125, prec 0.0562454, recall 0.767703
2017-12-10T05:16:59.024876: step 2184, loss 0.150306, acc 0.921875, prec 0.056261, recall 0.767777
2017-12-10T05:16:59.299200: step 2185, loss 0.49354, acc 0.859375, prec 0.0563156, recall 0.768
2017-12-10T05:16:59.567625: step 2186, loss 0.389082, acc 0.875, prec 0.056305, recall 0.768
2017-12-10T05:16:59.830934: step 2187, loss 0.669636, acc 0.84375, prec 0.0562918, recall 0.768
2017-12-10T05:17:00.098738: step 2188, loss 0.455624, acc 0.828125, prec 0.0562773, recall 0.768
2017-12-10T05:17:00.368362: step 2189, loss 2.53527, acc 0.890625, prec 0.0562915, recall 0.767829
2017-12-10T05:17:00.640938: step 2190, loss 0.464532, acc 0.859375, prec 0.0563238, recall 0.767977
2017-12-10T05:17:00.906051: step 2191, loss 0.118724, acc 0.9375, prec 0.0563186, recall 0.767977
2017-12-10T05:17:01.164973: step 2192, loss 0.780073, acc 0.78125, prec 0.0563222, recall 0.768051
2017-12-10T05:17:01.423184: step 2193, loss 0.324897, acc 0.921875, prec 0.0563377, recall 0.768125
2017-12-10T05:17:01.684778: step 2194, loss 0.384679, acc 0.875, prec 0.0563271, recall 0.768125
2017-12-10T05:17:01.955768: step 2195, loss 0.431973, acc 0.890625, prec 0.05634, recall 0.768199
2017-12-10T05:17:02.218805: step 2196, loss 0.527069, acc 0.828125, prec 0.0563255, recall 0.768199
2017-12-10T05:17:02.481887: step 2197, loss 0.509829, acc 0.859375, prec 0.0563136, recall 0.768199
2017-12-10T05:17:02.744827: step 2198, loss 0.43667, acc 0.875, prec 0.0563252, recall 0.768273
2017-12-10T05:17:03.012731: step 2199, loss 0.190768, acc 0.9375, prec 0.056342, recall 0.768347
2017-12-10T05:17:03.281256: step 2200, loss 0.352993, acc 0.890625, prec 0.0563328, recall 0.768347
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-2200

2017-12-10T05:17:04.511582: step 2201, loss 0.145024, acc 0.9375, prec 0.0563275, recall 0.768347
2017-12-10T05:17:04.774069: step 2202, loss 0.160147, acc 0.9375, prec 0.0563443, recall 0.768421
2017-12-10T05:17:05.049067: step 2203, loss 2.75264, acc 0.96875, prec 0.056365, recall 0.76825
2017-12-10T05:17:05.322305: step 2204, loss 0.231947, acc 0.921875, prec 0.0563805, recall 0.768324
2017-12-10T05:17:05.584727: step 2205, loss 0.19992, acc 0.953125, prec 0.0563766, recall 0.768324
2017-12-10T05:17:05.854833: step 2206, loss 0.241785, acc 0.875, prec 0.056366, recall 0.768324
2017-12-10T05:17:06.117602: step 2207, loss 0.19461, acc 0.921875, prec 0.0564035, recall 0.768471
2017-12-10T05:17:06.383082: step 2208, loss 0.502568, acc 0.859375, prec 0.0563917, recall 0.768471
2017-12-10T05:17:06.655452: step 2209, loss 0.777799, acc 0.8125, prec 0.0563979, recall 0.768545
2017-12-10T05:17:06.924998: step 2210, loss 1.0625, acc 0.84375, prec 0.0564288, recall 0.768692
2017-12-10T05:17:07.191405: step 2211, loss 0.29281, acc 0.890625, prec 0.0564196, recall 0.768692
2017-12-10T05:17:07.450946: step 2212, loss 0.529702, acc 0.890625, prec 0.0564324, recall 0.768766
2017-12-10T05:17:07.721293: step 2213, loss 0.285523, acc 0.859375, prec 0.0564646, recall 0.768913
2017-12-10T05:17:07.985863: step 2214, loss 0.415022, acc 0.890625, prec 0.0565214, recall 0.769133
2017-12-10T05:17:08.249868: step 2215, loss 0.580344, acc 0.859375, prec 0.0565096, recall 0.769133
2017-12-10T05:17:08.514866: step 2216, loss 0.304479, acc 0.890625, prec 0.0565444, recall 0.76928
2017-12-10T05:17:08.790305: step 2217, loss 0.147263, acc 0.984375, prec 0.056565, recall 0.769353
2017-12-10T05:17:09.063446: step 2218, loss 0.384297, acc 0.90625, prec 0.0565571, recall 0.769353
2017-12-10T05:17:09.330544: step 2219, loss 0.227421, acc 0.921875, prec 0.0565945, recall 0.769499
2017-12-10T05:17:09.596490: step 2220, loss 0.243891, acc 0.890625, prec 0.0565853, recall 0.769499
2017-12-10T05:17:09.864467: step 2221, loss 0.355608, acc 0.875, prec 0.0565747, recall 0.769499
2017-12-10T05:17:10.132731: step 2222, loss 2.41381, acc 0.859375, prec 0.0565862, recall 0.769328
2017-12-10T05:17:10.397871: step 2223, loss 0.253445, acc 0.9375, prec 0.0565809, recall 0.769328
2017-12-10T05:17:10.662995: step 2224, loss 0.915762, acc 0.90625, prec 0.0566389, recall 0.769547
2017-12-10T05:17:10.936791: step 2225, loss 0.365776, acc 0.90625, prec 0.056631, recall 0.769547
2017-12-10T05:17:11.203820: step 2226, loss 0.503888, acc 0.859375, prec 0.0566631, recall 0.769693
2017-12-10T05:17:11.465078: step 2227, loss 0.282696, acc 0.921875, prec 0.0566785, recall 0.769766
2017-12-10T05:17:11.730874: step 2228, loss 0.259107, acc 0.921875, prec 0.0567378, recall 0.769984
2017-12-10T05:17:11.992818: step 2229, loss 0.456382, acc 0.875, prec 0.0567272, recall 0.769984
2017-12-10T05:17:12.276501: step 2230, loss 0.449822, acc 0.890625, prec 0.0567619, recall 0.770129
2017-12-10T05:17:12.543057: step 2231, loss 0.38786, acc 0.921875, prec 0.0567991, recall 0.770275
2017-12-10T05:17:12.818667: step 2232, loss 0.368114, acc 0.921875, prec 0.0568364, recall 0.770419
2017-12-10T05:17:13.082450: step 2233, loss 0.300877, acc 0.921875, prec 0.0568298, recall 0.770419
2017-12-10T05:17:13.355858: step 2234, loss 0.640767, acc 0.84375, prec 0.0568166, recall 0.770419
2017-12-10T05:17:13.626859: step 2235, loss 0.219358, acc 0.953125, prec 0.0568126, recall 0.770419
2017-12-10T05:17:13.895524: step 2236, loss 0.291369, acc 0.90625, prec 0.0568047, recall 0.770419
2017-12-10T05:17:14.157120: step 2237, loss 0.364472, acc 0.921875, prec 0.05682, recall 0.770492
2017-12-10T05:17:14.424870: step 2238, loss 0.0932799, acc 0.953125, prec 0.0568599, recall 0.770636
2017-12-10T05:17:14.695079: step 2239, loss 0.202917, acc 0.9375, prec 0.0568766, recall 0.770709
2017-12-10T05:17:14.964876: step 2240, loss 0.213873, acc 0.953125, prec 0.0568945, recall 0.770781
2017-12-10T05:17:15.236169: step 2241, loss 0.152206, acc 0.953125, prec 0.0568905, recall 0.770781
2017-12-10T05:17:15.507718: step 2242, loss 0.244054, acc 0.90625, prec 0.0568826, recall 0.770781
2017-12-10T05:17:15.771585: step 2243, loss 0.27807, acc 0.921875, prec 0.0568979, recall 0.770853
2017-12-10T05:17:16.041749: step 2244, loss 3.61333, acc 0.953125, prec 0.0568953, recall 0.77061
2017-12-10T05:17:16.320035: step 2245, loss 0.613612, acc 0.890625, prec 0.0569079, recall 0.770683
2017-12-10T05:17:16.585322: step 2246, loss 0.51616, acc 0.9375, prec 0.0569464, recall 0.770827
2017-12-10T05:17:16.852350: step 2247, loss 0.220345, acc 0.953125, prec 0.0569644, recall 0.770899
2017-12-10T05:17:17.120020: step 2248, loss 0.507597, acc 0.890625, prec 0.0569551, recall 0.770899
2017-12-10T05:17:17.387004: step 2249, loss 0.16798, acc 0.953125, prec 0.0569512, recall 0.770899
2017-12-10T05:17:17.649688: step 2250, loss 0.440081, acc 0.84375, prec 0.0569379, recall 0.770899
2017-12-10T05:17:17.912719: step 2251, loss 0.424782, acc 0.859375, prec 0.056926, recall 0.770899
2017-12-10T05:17:18.177682: step 2252, loss 0.370351, acc 0.921875, prec 0.0569851, recall 0.771115
2017-12-10T05:17:18.443660: step 2253, loss 0.344203, acc 0.90625, prec 0.056999, recall 0.771186
2017-12-10T05:17:18.710377: step 2254, loss 0.506682, acc 0.921875, prec 0.0569924, recall 0.771186
2017-12-10T05:17:18.980172: step 2255, loss 0.283687, acc 0.90625, prec 0.0569845, recall 0.771186
2017-12-10T05:17:19.256822: step 2256, loss 0.188246, acc 0.9375, prec 0.0570011, recall 0.771258
2017-12-10T05:17:19.525520: step 2257, loss 0.450645, acc 0.875, prec 0.0569905, recall 0.771258
2017-12-10T05:17:19.799835: step 2258, loss 0.160564, acc 0.953125, prec 0.0570084, recall 0.77133
2017-12-10T05:17:20.063073: step 2259, loss 0.670491, acc 0.84375, prec 0.057017, recall 0.771402
2017-12-10T05:17:20.333354: step 2260, loss 0.253798, acc 0.9375, prec 0.0570336, recall 0.771473
2017-12-10T05:17:20.602910: step 2261, loss 0.134983, acc 0.953125, prec 0.0570296, recall 0.771473
2017-12-10T05:17:20.864897: step 2262, loss 0.242049, acc 0.890625, prec 0.0570422, recall 0.771545
2017-12-10T05:17:21.131384: step 2263, loss 0.113425, acc 0.984375, prec 0.0570846, recall 0.771688
2017-12-10T05:17:21.397790: step 2264, loss 0.117867, acc 0.953125, prec 0.0570806, recall 0.771688
2017-12-10T05:17:21.660931: step 2265, loss 0.343812, acc 0.9375, prec 0.0570754, recall 0.771688
2017-12-10T05:17:21.930630: step 2266, loss 0.435538, acc 0.9375, prec 0.0571137, recall 0.771831
2017-12-10T05:17:22.202930: step 2267, loss 0.459497, acc 0.859375, prec 0.0571455, recall 0.771974
2017-12-10T05:17:22.470789: step 2268, loss 0.0702874, acc 0.953125, prec 0.0571415, recall 0.771974
2017-12-10T05:17:22.739337: step 2269, loss 0.0264973, acc 0.984375, prec 0.0571402, recall 0.771974
2017-12-10T05:17:23.007307: step 2270, loss 0.0492479, acc 0.984375, prec 0.0571389, recall 0.771974
2017-12-10T05:17:23.274902: step 2271, loss 0.211172, acc 0.953125, prec 0.0571567, recall 0.772045
2017-12-10T05:17:23.545502: step 2272, loss 0.126316, acc 0.9375, prec 0.0571951, recall 0.772187
2017-12-10T05:17:23.811182: step 2273, loss 0.221732, acc 0.9375, prec 0.0572116, recall 0.772259
2017-12-10T05:17:24.076444: step 2274, loss 0.149819, acc 0.9375, prec 0.05725, recall 0.772401
2017-12-10T05:17:24.342284: step 2275, loss 8.48402, acc 0.984375, prec 0.0572936, recall 0.772302
2017-12-10T05:17:24.613469: step 2276, loss 0.08758, acc 0.953125, prec 0.0572896, recall 0.772302
2017-12-10T05:17:24.887901: step 2277, loss 0.41849, acc 0.96875, prec 0.0573742, recall 0.772586
2017-12-10T05:17:25.162137: step 2278, loss 0.0617465, acc 0.984375, prec 0.0573947, recall 0.772657
2017-12-10T05:17:25.427384: step 2279, loss 0.130177, acc 0.9375, prec 0.0573894, recall 0.772657
2017-12-10T05:17:25.693010: step 2280, loss 0.184964, acc 0.953125, prec 0.0574072, recall 0.772727
2017-12-10T05:17:25.958549: step 2281, loss 0.556522, acc 0.890625, prec 0.0574415, recall 0.772869
2017-12-10T05:17:26.221772: step 2282, loss 0.106605, acc 0.96875, prec 0.0574388, recall 0.772869
2017-12-10T05:17:26.494474: step 2283, loss 0.207094, acc 0.90625, prec 0.0574527, recall 0.772939
2017-12-10T05:17:26.762616: step 2284, loss 0.240515, acc 0.90625, prec 0.0574447, recall 0.772939
2017-12-10T05:17:27.024042: step 2285, loss 0.260717, acc 0.9375, prec 0.0575047, recall 0.773151
2017-12-10T05:17:27.302275: step 2286, loss 0.456785, acc 0.875, prec 0.0575377, recall 0.773292
2017-12-10T05:17:27.571265: step 2287, loss 3.60178, acc 0.828125, prec 0.0575244, recall 0.773052
2017-12-10T05:17:27.837725: step 2288, loss 0.603551, acc 0.890625, prec 0.0575368, recall 0.773122
2017-12-10T05:17:28.105416: step 2289, loss 0.709197, acc 0.796875, prec 0.0575196, recall 0.773122
2017-12-10T05:17:28.378734: step 2290, loss 0.693648, acc 0.859375, prec 0.0575076, recall 0.773122
2017-12-10T05:17:28.643590: step 2291, loss 0.658461, acc 0.859375, prec 0.0574957, recall 0.773122
2017-12-10T05:17:28.908437: step 2292, loss 0.47748, acc 0.84375, prec 0.0574824, recall 0.773122
2017-12-10T05:17:29.173200: step 2293, loss 0.984838, acc 0.75, prec 0.0574829, recall 0.773193
2017-12-10T05:17:29.440326: step 2294, loss 0.359962, acc 0.875, prec 0.0574723, recall 0.773193
2017-12-10T05:17:29.715917: step 2295, loss 0.682778, acc 0.859375, prec 0.0574821, recall 0.773263
2017-12-10T05:17:29.980130: step 2296, loss 0.755272, acc 0.828125, prec 0.0574676, recall 0.773263
2017-12-10T05:17:30.241390: step 2297, loss 0.442232, acc 0.875, prec 0.057457, recall 0.773263
2017-12-10T05:17:30.511218: step 2298, loss 0.476795, acc 0.859375, prec 0.0574668, recall 0.773333
2017-12-10T05:17:30.780779: step 2299, loss 0.443927, acc 0.8125, prec 0.0574726, recall 0.773404
2017-12-10T05:17:31.058021: step 2300, loss 0.901935, acc 0.78125, prec 0.0574975, recall 0.773544
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-2300

2017-12-10T05:17:32.352811: step 2301, loss 0.610189, acc 0.828125, prec 0.0574829, recall 0.773544
2017-12-10T05:17:32.618976: step 2302, loss 0.317034, acc 0.921875, prec 0.0574763, recall 0.773544
2017-12-10T05:17:32.887368: step 2303, loss 0.457697, acc 0.890625, prec 0.057467, recall 0.773544
2017-12-10T05:17:33.148950: step 2304, loss 0.634589, acc 0.90625, prec 0.0575025, recall 0.773684
2017-12-10T05:17:33.412015: step 2305, loss 0.399631, acc 0.890625, prec 0.0575149, recall 0.773754
2017-12-10T05:17:33.679548: step 2306, loss 0.159, acc 0.953125, prec 0.0575326, recall 0.773824
2017-12-10T05:17:33.940894: step 2307, loss 0.65107, acc 0.921875, prec 0.057526, recall 0.773824
2017-12-10T05:17:34.207601: step 2308, loss 0.0605741, acc 0.984375, prec 0.0575247, recall 0.773824
2017-12-10T05:17:34.479749: step 2309, loss 0.352667, acc 0.96875, prec 0.0575437, recall 0.773894
2017-12-10T05:17:34.748368: step 2310, loss 0.107141, acc 0.953125, prec 0.0575397, recall 0.773894
2017-12-10T05:17:35.011091: step 2311, loss 0.635561, acc 0.921875, prec 0.0575981, recall 0.774104
2017-12-10T05:17:35.274289: step 2312, loss 2.13952, acc 0.9375, prec 0.0575942, recall 0.773865
2017-12-10T05:17:35.553650: step 2313, loss 0.444794, acc 1, prec 0.0576158, recall 0.773935
2017-12-10T05:17:35.824634: step 2314, loss 0.35257, acc 0.890625, prec 0.0576282, recall 0.774004
2017-12-10T05:17:36.089096: step 2315, loss 0.118541, acc 0.953125, prec 0.0576459, recall 0.774074
2017-12-10T05:17:36.354216: step 2316, loss 0.256742, acc 0.890625, prec 0.0576583, recall 0.774144
2017-12-10T05:17:36.625766: step 2317, loss 0.392263, acc 0.9375, prec 0.0577396, recall 0.774422
2017-12-10T05:17:36.892820: step 2318, loss 0.303484, acc 0.921875, prec 0.057733, recall 0.774422
2017-12-10T05:17:37.155074: step 2319, loss 0.309044, acc 0.859375, prec 0.0577643, recall 0.774561
2017-12-10T05:17:37.427700: step 2320, loss 0.237247, acc 0.921875, prec 0.0578009, recall 0.7747
2017-12-10T05:17:37.693800: step 2321, loss 0.0853232, acc 0.984375, prec 0.0578429, recall 0.774839
2017-12-10T05:17:37.951608: step 2322, loss 0.302112, acc 0.9375, prec 0.0578592, recall 0.774908
2017-12-10T05:17:38.212028: step 2323, loss 0.0915434, acc 0.96875, prec 0.0578566, recall 0.774908
2017-12-10T05:17:38.476668: step 2324, loss 0.166189, acc 0.953125, prec 0.0578958, recall 0.775046
2017-12-10T05:17:38.742997: step 2325, loss 0.143004, acc 0.953125, prec 0.0578918, recall 0.775046
2017-12-10T05:17:39.005866: step 2326, loss 0.377907, acc 0.890625, prec 0.0578825, recall 0.775046
2017-12-10T05:17:39.267320: step 2327, loss 0.147979, acc 0.96875, prec 0.0579015, recall 0.775115
2017-12-10T05:17:39.541095: step 2328, loss 0.225555, acc 0.953125, prec 0.0578975, recall 0.775115
2017-12-10T05:17:39.806563: step 2329, loss 0.0838527, acc 0.984375, prec 0.0578962, recall 0.775115
2017-12-10T05:17:40.081046: step 2330, loss 0.25749, acc 0.90625, prec 0.0578882, recall 0.775115
2017-12-10T05:17:40.342124: step 2331, loss 0.448034, acc 0.921875, prec 0.0579464, recall 0.775322
2017-12-10T05:17:40.609833: step 2332, loss 0.38824, acc 0.921875, prec 0.057983, recall 0.77546
2017-12-10T05:17:40.882381: step 2333, loss 0.695755, acc 0.84375, prec 0.0579913, recall 0.775529
2017-12-10T05:17:41.154004: step 2334, loss 1.71804, acc 0.953125, prec 0.0579886, recall 0.775291
2017-12-10T05:17:41.426607: step 2335, loss 0.404525, acc 0.921875, prec 0.0580252, recall 0.775429
2017-12-10T05:17:41.690354: step 2336, loss 0.650048, acc 0.90625, prec 0.0580388, recall 0.775498
2017-12-10T05:17:41.957235: step 2337, loss 0.60704, acc 0.828125, prec 0.0580242, recall 0.775498
2017-12-10T05:17:42.232842: step 2338, loss 0.106059, acc 0.953125, prec 0.0580202, recall 0.775498
2017-12-10T05:17:42.494767: step 2339, loss 0.257571, acc 0.90625, prec 0.0580122, recall 0.775498
2017-12-10T05:17:42.766463: step 2340, loss 0.422561, acc 0.890625, prec 0.0580029, recall 0.775498
2017-12-10T05:17:43.033277: step 2341, loss 0.512704, acc 0.828125, prec 0.0579883, recall 0.775498
2017-12-10T05:17:43.298724: step 2342, loss 0.648255, acc 0.828125, prec 0.0579737, recall 0.775498
2017-12-10T05:17:43.562821: step 2343, loss 0.236236, acc 0.890625, prec 0.0579859, recall 0.775566
2017-12-10T05:17:43.823980: step 2344, loss 0.392391, acc 0.84375, prec 0.0579942, recall 0.775635
2017-12-10T05:17:44.097147: step 2345, loss 0.0960187, acc 0.984375, prec 0.0580145, recall 0.775704
2017-12-10T05:17:44.361379: step 2346, loss 0.387425, acc 0.953125, prec 0.058032, recall 0.775772
2017-12-10T05:17:44.623335: step 2347, loss 0.291111, acc 0.921875, prec 0.058047, recall 0.775841
2017-12-10T05:17:44.889519: step 2348, loss 0.0932141, acc 0.96875, prec 0.0580443, recall 0.775841
2017-12-10T05:17:45.154351: step 2349, loss 0.399367, acc 0.890625, prec 0.0580781, recall 0.775978
2017-12-10T05:17:45.421465: step 2350, loss 0.147538, acc 0.953125, prec 0.0580741, recall 0.775978
2017-12-10T05:17:45.684745: step 2351, loss 0.449353, acc 0.859375, prec 0.0580837, recall 0.776046
2017-12-10T05:17:45.944767: step 2352, loss 0.470719, acc 0.890625, prec 0.0580744, recall 0.776046
2017-12-10T05:17:46.211869: step 2353, loss 0.269551, acc 0.921875, prec 0.0580678, recall 0.776046
2017-12-10T05:17:46.479050: step 2354, loss 0.13566, acc 0.953125, prec 0.0580853, recall 0.776115
2017-12-10T05:17:46.750696: step 2355, loss 0.136551, acc 0.96875, prec 0.0581472, recall 0.77632
2017-12-10T05:17:47.023827: step 2356, loss 0.378478, acc 0.9375, prec 0.058185, recall 0.776456
2017-12-10T05:17:47.287992: step 2357, loss 0.107965, acc 0.953125, prec 0.058181, recall 0.776456
2017-12-10T05:17:47.561156: step 2358, loss 0.210992, acc 0.9375, prec 0.0581757, recall 0.776456
2017-12-10T05:17:47.834456: step 2359, loss 0.0923082, acc 0.96875, prec 0.0581945, recall 0.776524
2017-12-10T05:17:48.101463: step 2360, loss 0.124513, acc 0.953125, prec 0.0581905, recall 0.776524
2017-12-10T05:17:48.377666: step 2361, loss 0.254248, acc 0.953125, prec 0.0582081, recall 0.776592
2017-12-10T05:17:48.643545: step 2362, loss 0.0272067, acc 1, prec 0.0582511, recall 0.776729
2017-12-10T05:17:48.911477: step 2363, loss 0.334, acc 0.96875, prec 0.0582699, recall 0.776797
2017-12-10T05:17:49.182328: step 2364, loss 0.154528, acc 0.953125, prec 0.0582875, recall 0.776865
2017-12-10T05:17:49.444956: step 2365, loss 4.57021, acc 0.953125, prec 0.0583063, recall 0.776696
2017-12-10T05:17:49.722524: step 2366, loss 0.564624, acc 0.96875, prec 0.0583467, recall 0.776832
2017-12-10T05:17:49.988126: step 2367, loss 0.36875, acc 0.921875, prec 0.058383, recall 0.776967
2017-12-10T05:17:50.262576: step 2368, loss 0.193656, acc 0.921875, prec 0.0583763, recall 0.776967
2017-12-10T05:17:50.529421: step 2369, loss 0.252934, acc 0.921875, prec 0.0583912, recall 0.777035
2017-12-10T05:17:50.795455: step 2370, loss 0.190982, acc 0.9375, prec 0.0583858, recall 0.777035
2017-12-10T05:17:51.058114: step 2371, loss 0.10314, acc 0.96875, prec 0.0584047, recall 0.777103
2017-12-10T05:17:51.324920: step 2372, loss 0.189385, acc 0.921875, prec 0.058398, recall 0.777103
2017-12-10T05:17:51.596448: step 2373, loss 0.406829, acc 0.859375, prec 0.058429, recall 0.777238
2017-12-10T05:17:51.860867: step 2374, loss 0.211842, acc 0.953125, prec 0.0584464, recall 0.777306
2017-12-10T05:17:52.124921: step 2375, loss 0.124584, acc 0.9375, prec 0.0584626, recall 0.777373
2017-12-10T05:17:52.389266: step 2376, loss 0.292854, acc 0.90625, prec 0.0584546, recall 0.777373
2017-12-10T05:17:52.660725: step 2377, loss 0.352481, acc 0.90625, prec 0.0584466, recall 0.777373
2017-12-10T05:17:52.921716: step 2378, loss 0.420055, acc 0.859375, prec 0.0584346, recall 0.777373
2017-12-10T05:17:53.188418: step 2379, loss 0.221133, acc 0.90625, prec 0.0584481, recall 0.777441
2017-12-10T05:17:53.454445: step 2380, loss 0.326901, acc 0.90625, prec 0.058483, recall 0.777576
2017-12-10T05:17:53.716728: step 2381, loss 0.303331, acc 0.90625, prec 0.058475, recall 0.777576
2017-12-10T05:17:53.986102: step 2382, loss 4.74926, acc 0.921875, prec 0.0584697, recall 0.77734
2017-12-10T05:17:54.250875: step 2383, loss 0.224174, acc 0.953125, prec 0.0585086, recall 0.777475
2017-12-10T05:17:54.517514: step 2384, loss 0.238455, acc 0.9375, prec 0.0585032, recall 0.777475
2017-12-10T05:17:54.781358: step 2385, loss 0.375429, acc 0.9375, prec 0.0584979, recall 0.777475
2017-12-10T05:17:55.044615: step 2386, loss 0.311437, acc 0.90625, prec 0.0584899, recall 0.777475
2017-12-10T05:17:55.312545: step 2387, loss 0.29555, acc 0.890625, prec 0.0584806, recall 0.777475
2017-12-10T05:17:55.574451: step 2388, loss 0.301374, acc 0.890625, prec 0.0585142, recall 0.77761
2017-12-10T05:17:55.840550: step 2389, loss 0.376512, acc 0.90625, prec 0.0585276, recall 0.777677
2017-12-10T05:17:56.113919: step 2390, loss 0.24171, acc 0.890625, prec 0.0585397, recall 0.777744
2017-12-10T05:17:56.378526: step 2391, loss 0.35238, acc 0.890625, prec 0.0585518, recall 0.777811
2017-12-10T05:17:56.641261: step 2392, loss 0.221773, acc 0.9375, prec 0.0585465, recall 0.777811
2017-12-10T05:17:56.904108: step 2393, loss 0.327828, acc 0.90625, prec 0.0585385, recall 0.777811
2017-12-10T05:17:57.183890: step 2394, loss 0.134309, acc 0.96875, prec 0.0585572, recall 0.777879
2017-12-10T05:17:57.455021: step 2395, loss 0.165189, acc 0.9375, prec 0.0585519, recall 0.777879
2017-12-10T05:17:57.718039: step 2396, loss 0.122699, acc 0.9375, prec 0.0585466, recall 0.777879
2017-12-10T05:17:57.982813: step 2397, loss 1.83871, acc 0.921875, prec 0.0585627, recall 0.777711
2017-12-10T05:17:58.249201: step 2398, loss 0.298458, acc 0.90625, prec 0.0585761, recall 0.777778
2017-12-10T05:17:58.518392: step 2399, loss 0.379813, acc 0.875, prec 0.0585654, recall 0.777778
2017-12-10T05:17:58.781613: step 2400, loss 1.81845, acc 0.953125, prec 0.0586056, recall 0.777677

Evaluation:
2017-12-10T05:18:06.379683: step 2400, loss 1.93425, acc 0.906303, prec 0.0592601, recall 0.768588

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-2400

2017-12-10T05:18:07.661460: step 2401, loss 0.196898, acc 0.90625, prec 0.0592522, recall 0.768588
2017-12-10T05:18:07.923966: step 2402, loss 0.544095, acc 0.828125, prec 0.0592377, recall 0.768588
2017-12-10T05:18:08.185870: step 2403, loss 0.635167, acc 0.84375, prec 0.0592245, recall 0.768588
2017-12-10T05:18:08.455579: step 2404, loss 1.25591, acc 0.84375, prec 0.0592532, recall 0.768721
2017-12-10T05:18:08.721584: step 2405, loss 0.518716, acc 0.765625, prec 0.0592543, recall 0.768788
2017-12-10T05:18:08.985996: step 2406, loss 1.02794, acc 0.765625, prec 0.0592555, recall 0.768854
2017-12-10T05:18:09.250732: step 2407, loss 0.69188, acc 0.8125, prec 0.0592397, recall 0.768854
2017-12-10T05:18:09.518198: step 2408, loss 0.793237, acc 0.828125, prec 0.0592461, recall 0.768921
2017-12-10T05:18:09.787064: step 2409, loss 0.872272, acc 0.8125, prec 0.0592721, recall 0.769054
2017-12-10T05:18:10.054388: step 2410, loss 0.777661, acc 0.71875, prec 0.0592484, recall 0.769054
2017-12-10T05:18:10.327360: step 2411, loss 0.565085, acc 0.859375, prec 0.0592575, recall 0.76912
2017-12-10T05:18:10.602053: step 2412, loss 0.691203, acc 0.765625, prec 0.0592378, recall 0.76912
2017-12-10T05:18:10.866143: step 2413, loss 1.47521, acc 0.796875, prec 0.0592624, recall 0.769253
2017-12-10T05:18:11.135339: step 2414, loss 0.636412, acc 0.78125, prec 0.059244, recall 0.769253
2017-12-10T05:18:11.396731: step 2415, loss 0.431427, acc 0.921875, prec 0.0592375, recall 0.769253
2017-12-10T05:18:11.660418: step 2416, loss 0.461011, acc 0.84375, prec 0.0592452, recall 0.769319
2017-12-10T05:18:11.927736: step 2417, loss 0.447314, acc 0.84375, prec 0.0592321, recall 0.769319
2017-12-10T05:18:12.196717: step 2418, loss 0.275419, acc 0.9375, prec 0.0592268, recall 0.769319
2017-12-10T05:18:12.467990: step 2419, loss 0.597522, acc 0.828125, prec 0.0592124, recall 0.769319
2017-12-10T05:18:12.733364: step 2420, loss 0.996139, acc 0.96875, prec 0.0592514, recall 0.769452
2017-12-10T05:18:13.005773: step 2421, loss 1.21608, acc 0.90625, prec 0.0592851, recall 0.769584
2017-12-10T05:18:13.276901: step 2422, loss 0.556181, acc 0.875, prec 0.0593162, recall 0.769716
2017-12-10T05:18:13.545375: step 2423, loss 0.331865, acc 0.875, prec 0.0593057, recall 0.769716
2017-12-10T05:18:13.815387: step 2424, loss 0.32516, acc 0.875, prec 0.0592953, recall 0.769716
2017-12-10T05:18:14.084323: step 2425, loss 0.490925, acc 0.828125, prec 0.0592809, recall 0.769716
2017-12-10T05:18:14.346197: step 2426, loss 0.302965, acc 0.875, prec 0.0592912, recall 0.769782
2017-12-10T05:18:14.609330: step 2427, loss 0.617623, acc 0.84375, prec 0.0593196, recall 0.769914
2017-12-10T05:18:14.877780: step 2428, loss 0.565573, acc 0.859375, prec 0.0593286, recall 0.76998
2017-12-10T05:18:15.141006: step 2429, loss 0.200478, acc 0.953125, prec 0.0593869, recall 0.770177
2017-12-10T05:18:15.406142: step 2430, loss 0.276856, acc 0.90625, prec 0.0593791, recall 0.770177
2017-12-10T05:18:15.678380: step 2431, loss 1.97648, acc 0.859375, prec 0.0593686, recall 0.769957
2017-12-10T05:18:15.944555: step 2432, loss 0.241923, acc 0.921875, prec 0.059362, recall 0.769957
2017-12-10T05:18:16.209556: step 2433, loss 0.397866, acc 0.9375, prec 0.0593568, recall 0.769957
2017-12-10T05:18:16.471555: step 2434, loss 0.35184, acc 0.90625, prec 0.0593697, recall 0.770023
2017-12-10T05:18:16.734232: step 2435, loss 0.336245, acc 0.875, prec 0.0593592, recall 0.770023
2017-12-10T05:18:17.010135: step 2436, loss 4.16977, acc 0.90625, prec 0.0593734, recall 0.769868
2017-12-10T05:18:17.282071: step 2437, loss 0.235259, acc 0.9375, prec 0.0593889, recall 0.769934
2017-12-10T05:18:17.549623: step 2438, loss 0.502217, acc 0.859375, prec 0.0594393, recall 0.770131
2017-12-10T05:18:17.814460: step 2439, loss 0.662928, acc 0.859375, prec 0.059469, recall 0.770263
2017-12-10T05:18:18.073340: step 2440, loss 0.873376, acc 0.734375, prec 0.0594674, recall 0.770328
2017-12-10T05:18:18.336547: step 2441, loss 0.342351, acc 0.90625, prec 0.0594803, recall 0.770394
2017-12-10T05:18:18.604671: step 2442, loss 0.412126, acc 0.890625, prec 0.0594918, recall 0.770459
2017-12-10T05:18:18.869854: step 2443, loss 0.308889, acc 0.84375, prec 0.0594787, recall 0.770459
2017-12-10T05:18:19.130225: step 2444, loss 0.192869, acc 0.953125, prec 0.0594955, recall 0.770525
2017-12-10T05:18:19.396073: step 2445, loss 0.551273, acc 0.875, prec 0.0595057, recall 0.77059
2017-12-10T05:18:19.661415: step 2446, loss 0.235373, acc 0.9375, prec 0.0595833, recall 0.770851
2017-12-10T05:18:19.924841: step 2447, loss 0.296401, acc 0.859375, prec 0.0595715, recall 0.770851
2017-12-10T05:18:20.194501: step 2448, loss 0.138037, acc 0.953125, prec 0.0596089, recall 0.770981
2017-12-10T05:18:20.459235: step 2449, loss 0.450258, acc 0.859375, prec 0.0596178, recall 0.771047
2017-12-10T05:18:20.731370: step 2450, loss 2.64527, acc 0.796875, prec 0.0596227, recall 0.770893
2017-12-10T05:18:21.000495: step 2451, loss 0.53566, acc 0.796875, prec 0.0596264, recall 0.770958
2017-12-10T05:18:21.271280: step 2452, loss 0.391066, acc 0.875, prec 0.0596572, recall 0.771088
2017-12-10T05:18:21.550783: step 2453, loss 0.30159, acc 0.90625, prec 0.05967, recall 0.771153
2017-12-10T05:18:21.812460: step 2454, loss 0.244582, acc 0.875, prec 0.0596802, recall 0.771218
2017-12-10T05:18:22.076714: step 2455, loss 0.194522, acc 0.890625, prec 0.0596917, recall 0.771283
2017-12-10T05:18:22.342238: step 2456, loss 1.21852, acc 0.953125, prec 0.0597084, recall 0.771348
2017-12-10T05:18:22.611441: step 2457, loss 0.33277, acc 0.90625, prec 0.0597418, recall 0.771477
2017-12-10T05:18:22.874377: step 2458, loss 0.263391, acc 0.953125, prec 0.0597585, recall 0.771542
2017-12-10T05:18:23.139495: step 2459, loss 0.273103, acc 0.90625, prec 0.0597713, recall 0.771607
2017-12-10T05:18:23.403625: step 2460, loss 0.785126, acc 0.890625, prec 0.0597827, recall 0.771671
2017-12-10T05:18:23.666058: step 2461, loss 1.61884, acc 0.921875, prec 0.0598187, recall 0.771582
2017-12-10T05:18:23.942663: step 2462, loss 0.757206, acc 0.796875, prec 0.0598223, recall 0.771647
2017-12-10T05:18:24.202918: step 2463, loss 0.561275, acc 0.78125, prec 0.0598039, recall 0.771647
2017-12-10T05:18:24.467316: step 2464, loss 0.328881, acc 0.890625, prec 0.0597948, recall 0.771647
2017-12-10T05:18:24.735089: step 2465, loss 0.290102, acc 0.859375, prec 0.059783, recall 0.771647
2017-12-10T05:18:24.999353: step 2466, loss 0.848591, acc 0.8125, prec 0.0597672, recall 0.771647
2017-12-10T05:18:25.259619: step 2467, loss 0.471877, acc 0.859375, prec 0.0597555, recall 0.771647
2017-12-10T05:18:25.531236: step 2468, loss 0.756576, acc 0.78125, prec 0.0597577, recall 0.771711
2017-12-10T05:18:26.509888: step 2469, loss 0.897322, acc 0.75, prec 0.059778, recall 0.771841
2017-12-10T05:18:26.866653: step 2470, loss 0.630102, acc 0.828125, prec 0.0598047, recall 0.771969
2017-12-10T05:18:27.326596: step 2471, loss 0.576456, acc 0.890625, prec 0.0598162, recall 0.772034
2017-12-10T05:18:28.104354: step 2472, loss 1.49094, acc 0.828125, prec 0.0598635, recall 0.772227
2017-12-10T05:18:28.848351: step 2473, loss 0.294882, acc 0.9375, prec 0.0598788, recall 0.772291
2017-12-10T05:18:29.592318: step 2474, loss 0.146436, acc 0.9375, prec 0.0598941, recall 0.772355
2017-12-10T05:18:30.344877: step 2475, loss 0.139471, acc 0.921875, prec 0.0598876, recall 0.772355
2017-12-10T05:18:31.082676: step 2476, loss 0.104966, acc 0.96875, prec 0.0599055, recall 0.77242
2017-12-10T05:18:31.870591: step 2477, loss 0.469415, acc 0.96875, prec 0.059944, recall 0.772548
2017-12-10T05:18:32.623054: step 2478, loss 0.201938, acc 0.953125, prec 0.0599401, recall 0.772548
2017-12-10T05:18:33.383694: step 2479, loss 0.256051, acc 0.921875, prec 0.0599541, recall 0.772612
2017-12-10T05:18:34.135027: step 2480, loss 0.422739, acc 0.96875, prec 0.0600131, recall 0.772804
2017-12-10T05:18:34.890889: step 2481, loss 0.334907, acc 0.875, prec 0.0600026, recall 0.772804
2017-12-10T05:18:35.627866: step 2482, loss 0.189632, acc 0.921875, prec 0.0599961, recall 0.772804
2017-12-10T05:18:36.369017: step 2483, loss 0.163725, acc 0.9375, prec 0.0599908, recall 0.772804
2017-12-10T05:18:37.087546: step 2484, loss 0.246256, acc 0.9375, prec 0.0599856, recall 0.772804
2017-12-10T05:18:37.673040: step 2485, loss 1.0299, acc 0.961538, prec 0.0600446, recall 0.772996
2017-12-10T05:18:38.458277: step 2486, loss 0.142534, acc 0.921875, prec 0.060038, recall 0.772996
2017-12-10T05:18:39.228681: step 2487, loss 0.389236, acc 0.953125, prec 0.0600546, recall 0.77306
2017-12-10T05:18:40.089600: step 2488, loss 0.0867825, acc 0.96875, prec 0.060052, recall 0.77306
2017-12-10T05:18:40.515387: step 2489, loss 0.127952, acc 0.953125, prec 0.0600481, recall 0.77306
2017-12-10T05:18:40.790343: step 2490, loss 0.095302, acc 0.96875, prec 0.060066, recall 0.773123
2017-12-10T05:18:41.070792: step 2491, loss 0.13957, acc 0.9375, prec 0.0600607, recall 0.773123
2017-12-10T05:18:41.335948: step 2492, loss 0.281412, acc 0.96875, prec 0.0600991, recall 0.773251
2017-12-10T05:18:41.602568: step 2493, loss 0.130233, acc 0.953125, prec 0.0600952, recall 0.773251
2017-12-10T05:18:41.869843: step 2494, loss 0.205988, acc 0.9375, prec 0.060131, recall 0.773378
2017-12-10T05:18:42.142628: step 2495, loss 0.193757, acc 0.9375, prec 0.0601258, recall 0.773378
2017-12-10T05:18:42.419223: step 2496, loss 0.168158, acc 0.921875, prec 0.0601192, recall 0.773378
2017-12-10T05:18:42.692142: step 2497, loss 0.637265, acc 0.921875, prec 0.0601742, recall 0.773569
2017-12-10T05:18:42.961065: step 2498, loss 0.229437, acc 0.953125, prec 0.0601702, recall 0.773569
2017-12-10T05:18:43.229540: step 2499, loss 0.188396, acc 0.9375, prec 0.0601855, recall 0.773633
2017-12-10T05:18:43.491280: step 2500, loss 0.156064, acc 0.921875, prec 0.0601789, recall 0.773633
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-2500

2017-12-10T05:18:44.872683: step 2501, loss 0.218846, acc 0.9375, prec 0.0601737, recall 0.773633
2017-12-10T05:18:45.141742: step 2502, loss 0.222566, acc 0.984375, prec 0.0602544, recall 0.773886
2017-12-10T05:18:45.408591: step 2503, loss 0.206715, acc 0.953125, prec 0.0602504, recall 0.773886
2017-12-10T05:18:45.673807: step 2504, loss 0.128653, acc 0.96875, prec 0.0602683, recall 0.77395
2017-12-10T05:18:45.942135: step 2505, loss 0.158405, acc 0.9375, prec 0.0602835, recall 0.774013
2017-12-10T05:18:46.205651: step 2506, loss 0.0916912, acc 0.96875, prec 0.0602809, recall 0.774013
2017-12-10T05:18:46.472417: step 2507, loss 0.304588, acc 0.9375, prec 0.0603371, recall 0.774203
2017-12-10T05:18:46.741238: step 2508, loss 0.11094, acc 0.96875, prec 0.0603345, recall 0.774203
2017-12-10T05:18:47.005450: step 2509, loss 0.183633, acc 0.953125, prec 0.060351, recall 0.774266
2017-12-10T05:18:47.265167: step 2510, loss 0.105504, acc 0.96875, prec 0.0603689, recall 0.774329
2017-12-10T05:18:47.527977: step 2511, loss 0.00713702, acc 1, prec 0.0603689, recall 0.774329
2017-12-10T05:18:47.790808: step 2512, loss 0.290935, acc 0.921875, prec 0.0603623, recall 0.774329
2017-12-10T05:18:48.053537: step 2513, loss 0.0481859, acc 0.984375, prec 0.060361, recall 0.774329
2017-12-10T05:18:48.316724: step 2514, loss 0.323488, acc 1, prec 0.0603815, recall 0.774392
2017-12-10T05:18:48.585321: step 2515, loss 0.00547734, acc 1, prec 0.0603815, recall 0.774392
2017-12-10T05:18:48.848175: step 2516, loss 0.0467174, acc 0.984375, prec 0.0603802, recall 0.774392
2017-12-10T05:18:49.114433: step 2517, loss 0.0817327, acc 0.984375, prec 0.0603993, recall 0.774455
2017-12-10T05:18:49.386948: step 2518, loss 0.0380017, acc 0.984375, prec 0.060439, recall 0.774581
2017-12-10T05:18:49.654666: step 2519, loss 0.0411772, acc 0.984375, prec 0.0604376, recall 0.774581
2017-12-10T05:18:49.922914: step 2520, loss 0.131919, acc 0.984375, prec 0.0604773, recall 0.774707
2017-12-10T05:18:50.192205: step 2521, loss 0.121023, acc 0.984375, prec 0.0604964, recall 0.77477
2017-12-10T05:18:50.456754: step 2522, loss 0.00449618, acc 1, prec 0.0604964, recall 0.77477
2017-12-10T05:18:50.719835: step 2523, loss 0.342891, acc 0.984375, prec 0.0605156, recall 0.774833
2017-12-10T05:18:50.988007: step 2524, loss 0.0251092, acc 0.984375, prec 0.0605143, recall 0.774833
2017-12-10T05:18:51.254959: step 2525, loss 0.310983, acc 0.984375, prec 0.0605539, recall 0.774958
2017-12-10T05:18:51.528714: step 2526, loss 0.037671, acc 0.984375, prec 0.0605526, recall 0.774958
2017-12-10T05:18:51.796828: step 2527, loss 0.12183, acc 0.96875, prec 0.0605499, recall 0.774958
2017-12-10T05:18:52.066936: step 2528, loss 0.0755334, acc 0.96875, prec 0.0605882, recall 0.775084
2017-12-10T05:18:52.340287: step 2529, loss 0.217386, acc 0.921875, prec 0.0606021, recall 0.775146
2017-12-10T05:18:52.601922: step 2530, loss 0.0514824, acc 0.953125, prec 0.0605981, recall 0.775146
2017-12-10T05:18:52.864847: step 2531, loss 0.0911922, acc 0.953125, prec 0.0605942, recall 0.775146
2017-12-10T05:18:53.140355: step 2532, loss 0.25879, acc 0.96875, prec 0.060612, recall 0.775209
2017-12-10T05:18:53.406512: step 2533, loss 0.189449, acc 0.953125, prec 0.0606285, recall 0.775272
2017-12-10T05:18:53.674011: step 2534, loss 0.192369, acc 0.96875, prec 0.0606668, recall 0.775397
2017-12-10T05:18:53.935317: step 2535, loss 0.236947, acc 0.96875, prec 0.0607255, recall 0.775584
2017-12-10T05:18:54.201611: step 2536, loss 4.89239, acc 0.90625, prec 0.0607598, recall 0.775493
2017-12-10T05:18:54.469433: step 2537, loss 0.207835, acc 0.9375, prec 0.0607954, recall 0.775618
2017-12-10T05:18:54.744506: step 2538, loss 0.492375, acc 0.9375, prec 0.060831, recall 0.775742
2017-12-10T05:18:55.009508: step 2539, loss 0.36032, acc 0.890625, prec 0.0608217, recall 0.775742
2017-12-10T05:18:55.277826: step 2540, loss 0.321815, acc 0.875, prec 0.0608315, recall 0.775805
2017-12-10T05:18:55.538948: step 2541, loss 0.562078, acc 0.84375, prec 0.0608183, recall 0.775805
2017-12-10T05:18:55.806637: step 2542, loss 0.30144, acc 0.890625, prec 0.0608499, recall 0.775929
2017-12-10T05:18:56.068301: step 2543, loss 0.288093, acc 0.875, prec 0.0608393, recall 0.775929
2017-12-10T05:18:56.334154: step 2544, loss 0.310644, acc 0.875, prec 0.0608287, recall 0.775929
2017-12-10T05:18:56.599287: step 2545, loss 1.20202, acc 0.90625, prec 0.060882, recall 0.776115
2017-12-10T05:18:56.873352: step 2546, loss 0.526166, acc 0.828125, prec 0.0608675, recall 0.776115
2017-12-10T05:18:57.140533: step 2547, loss 0.512729, acc 0.828125, prec 0.0608733, recall 0.776177
2017-12-10T05:18:57.410773: step 2548, loss 0.521425, acc 0.84375, prec 0.0608805, recall 0.776239
2017-12-10T05:18:57.671844: step 2549, loss 0.466693, acc 0.8125, prec 0.060885, recall 0.776301
2017-12-10T05:18:57.934235: step 2550, loss 0.572395, acc 0.84375, prec 0.0608922, recall 0.776363
2017-12-10T05:18:58.195561: step 2551, loss 0.569474, acc 0.875, prec 0.0609224, recall 0.776487
2017-12-10T05:18:58.456841: step 2552, loss 0.604604, acc 0.75, prec 0.060942, recall 0.77661
2017-12-10T05:18:58.718510: step 2553, loss 0.472206, acc 0.8125, prec 0.0609261, recall 0.77661
2017-12-10T05:18:58.980807: step 2554, loss 0.52229, acc 0.828125, prec 0.0609116, recall 0.77661
2017-12-10T05:18:59.247831: step 2555, loss 0.480907, acc 0.796875, prec 0.0609555, recall 0.776796
2017-12-10T05:18:59.512649: step 2556, loss 0.307806, acc 0.90625, prec 0.0609679, recall 0.776857
2017-12-10T05:18:59.783091: step 2557, loss 0.124335, acc 0.9375, prec 0.0609627, recall 0.776857
2017-12-10T05:19:00.049028: step 2558, loss 0.386605, acc 0.90625, prec 0.0609751, recall 0.776919
2017-12-10T05:19:00.320593: step 2559, loss 1.32856, acc 0.921875, prec 0.0609698, recall 0.776704
2017-12-10T05:19:00.599934: step 2560, loss 0.306084, acc 0.921875, prec 0.0610242, recall 0.776889
2017-12-10T05:19:00.874253: step 2561, loss 0.078287, acc 0.953125, prec 0.0610406, recall 0.776951
2017-12-10T05:19:01.135529: step 2562, loss 0.146546, acc 0.96875, prec 0.0610786, recall 0.777074
2017-12-10T05:19:01.408902: step 2563, loss 0.226363, acc 0.921875, prec 0.061072, recall 0.777074
2017-12-10T05:19:01.669257: step 2564, loss 0.194478, acc 0.921875, prec 0.0610857, recall 0.777135
2017-12-10T05:19:01.938329: step 2565, loss 0.233857, acc 0.9375, prec 0.0610804, recall 0.777135
2017-12-10T05:19:02.200498: step 2566, loss 0.0734471, acc 0.953125, prec 0.0610765, recall 0.777135
2017-12-10T05:19:02.463937: step 2567, loss 0.278074, acc 0.953125, prec 0.0610928, recall 0.777196
2017-12-10T05:19:02.724914: step 2568, loss 0.914387, acc 0.984375, prec 0.0611322, recall 0.777319
2017-12-10T05:19:02.997517: step 2569, loss 0.258577, acc 0.96875, prec 0.0611702, recall 0.777442
2017-12-10T05:19:03.261595: step 2570, loss 0.184882, acc 0.921875, prec 0.0612042, recall 0.777564
2017-12-10T05:19:03.528194: step 2571, loss 0.0715695, acc 0.96875, prec 0.0612218, recall 0.777625
2017-12-10T05:19:03.795766: step 2572, loss 0.026606, acc 0.984375, prec 0.0612205, recall 0.777625
2017-12-10T05:19:04.059561: step 2573, loss 0.146209, acc 0.953125, prec 0.0612572, recall 0.777747
2017-12-10T05:19:04.321703: step 2574, loss 2.52908, acc 0.96875, prec 0.0612558, recall 0.777534
2017-12-10T05:19:04.587022: step 2575, loss 0.132978, acc 0.953125, prec 0.0612519, recall 0.777534
2017-12-10T05:19:04.856025: step 2576, loss 6.12603, acc 0.984375, prec 0.0612519, recall 0.77732
2017-12-10T05:19:05.127460: step 2577, loss 0.123198, acc 0.96875, prec 0.0612898, recall 0.777442
2017-12-10T05:19:05.390281: step 2578, loss 0.303294, acc 0.859375, prec 0.0612779, recall 0.777442
2017-12-10T05:19:05.656296: step 2579, loss 0.606172, acc 0.828125, prec 0.0613039, recall 0.777564
2017-12-10T05:19:05.914665: step 2580, loss 0.558484, acc 0.859375, prec 0.0613326, recall 0.777686
2017-12-10T05:19:06.176646: step 2581, loss 0.438093, acc 0.828125, prec 0.0613586, recall 0.777808
2017-12-10T05:19:06.438742: step 2582, loss 0.43461, acc 0.84375, prec 0.0613453, recall 0.777808
2017-12-10T05:19:06.705008: step 2583, loss 0.854275, acc 0.75, prec 0.0613241, recall 0.777808
2017-12-10T05:19:06.971938: step 2584, loss 1.13206, acc 0.65625, prec 0.061295, recall 0.777808
2017-12-10T05:19:07.236530: step 2585, loss 0.624024, acc 0.765625, prec 0.0613157, recall 0.77793
2017-12-10T05:19:07.503051: step 2586, loss 0.64172, acc 0.8125, prec 0.0612998, recall 0.77793
2017-12-10T05:19:07.763289: step 2587, loss 0.653814, acc 0.8125, prec 0.0612839, recall 0.77793
2017-12-10T05:19:08.029317: step 2588, loss 0.80773, acc 0.75, prec 0.0612628, recall 0.77793
2017-12-10T05:19:08.299796: step 2589, loss 0.232377, acc 0.921875, prec 0.0612764, recall 0.777991
2017-12-10T05:19:08.567585: step 2590, loss 0.700472, acc 0.78125, prec 0.0612984, recall 0.778112
2017-12-10T05:19:08.831566: step 2591, loss 0.33287, acc 0.84375, prec 0.0612852, recall 0.778112
2017-12-10T05:19:09.096416: step 2592, loss 0.29739, acc 0.890625, prec 0.0613164, recall 0.778234
2017-12-10T05:19:09.370466: step 2593, loss 0.580984, acc 0.859375, prec 0.0613045, recall 0.778234
2017-12-10T05:19:09.644248: step 2594, loss 0.348623, acc 0.921875, prec 0.0613181, recall 0.778294
2017-12-10T05:19:09.907868: step 2595, loss 0.391265, acc 0.921875, prec 0.0613519, recall 0.778415
2017-12-10T05:19:10.173419: step 2596, loss 0.21785, acc 0.921875, prec 0.0613453, recall 0.778415
2017-12-10T05:19:10.432182: step 2597, loss 0.0393651, acc 1, prec 0.0613453, recall 0.778415
2017-12-10T05:19:10.698510: step 2598, loss 0.177492, acc 0.921875, prec 0.0613589, recall 0.778476
2017-12-10T05:19:10.961274: step 2599, loss 0.0661749, acc 0.96875, prec 0.0613765, recall 0.778536
2017-12-10T05:19:11.228536: step 2600, loss 0.0274244, acc 0.984375, prec 0.0613752, recall 0.778536
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-2600

2017-12-10T05:19:12.528741: step 2601, loss 0.0468467, acc 0.96875, prec 0.0613725, recall 0.778536
2017-12-10T05:19:12.803301: step 2602, loss 0.0699292, acc 0.984375, prec 0.0613712, recall 0.778536
2017-12-10T05:19:13.073090: step 2603, loss 0.0900973, acc 0.984375, prec 0.0613901, recall 0.778597
2017-12-10T05:19:13.347216: step 2604, loss 0.310489, acc 0.984375, prec 0.061409, recall 0.778657
2017-12-10T05:19:13.618715: step 2605, loss 0.369847, acc 0.96875, prec 0.0614467, recall 0.778778
2017-12-10T05:19:13.882582: step 2606, loss 0.0436971, acc 0.96875, prec 0.0614441, recall 0.778778
2017-12-10T05:19:14.155427: step 2607, loss 3.16515, acc 0.9375, prec 0.0614603, recall 0.778626
2017-12-10T05:19:14.430720: step 2608, loss 0.435359, acc 1, prec 0.0615007, recall 0.778747
2017-12-10T05:19:14.699098: step 2609, loss 0.922869, acc 0.9375, prec 0.0615156, recall 0.778807
2017-12-10T05:19:14.965031: step 2610, loss 0.203494, acc 0.921875, prec 0.0615292, recall 0.778867
2017-12-10T05:19:15.237278: step 2611, loss 0.550669, acc 0.875, prec 0.0615186, recall 0.778867
2017-12-10T05:19:15.507316: step 2612, loss 0.44467, acc 0.828125, prec 0.0615444, recall 0.778987
2017-12-10T05:19:15.778071: step 2613, loss 0.473601, acc 0.84375, prec 0.0616119, recall 0.779228
2017-12-10T05:19:16.045523: step 2614, loss 0.613942, acc 0.859375, prec 0.0616201, recall 0.779288
2017-12-10T05:19:16.306254: step 2615, loss 0.351998, acc 0.890625, prec 0.0616109, recall 0.779288
2017-12-10T05:19:16.572415: step 2616, loss 0.598579, acc 0.828125, prec 0.0616165, recall 0.779348
2017-12-10T05:19:16.833399: step 2617, loss 0.731608, acc 0.75, prec 0.0615953, recall 0.779348
2017-12-10T05:19:17.100658: step 2618, loss 0.786858, acc 0.859375, prec 0.0616438, recall 0.779528
2017-12-10T05:19:17.364033: step 2619, loss 0.793876, acc 0.90625, prec 0.0616762, recall 0.779647
2017-12-10T05:19:17.632694: step 2620, loss 0.623181, acc 0.84375, prec 0.0616629, recall 0.779647
2017-12-10T05:19:17.890277: step 2621, loss 0.475513, acc 0.875, prec 0.0616926, recall 0.779767
2017-12-10T05:19:18.157602: step 2622, loss 0.399992, acc 0.875, prec 0.0617223, recall 0.779886
2017-12-10T05:19:18.420293: step 2623, loss 0.359266, acc 0.875, prec 0.0617318, recall 0.779946
2017-12-10T05:19:18.683064: step 2624, loss 0.800423, acc 0.828125, prec 0.0617575, recall 0.780065
2017-12-10T05:19:18.950008: step 2625, loss 0.316796, acc 0.90625, prec 0.0617697, recall 0.780125
2017-12-10T05:19:19.211943: step 2626, loss 0.282241, acc 0.9375, prec 0.0617644, recall 0.780125
2017-12-10T05:19:19.480325: step 2627, loss 1.35563, acc 0.84375, prec 0.0617713, recall 0.780184
2017-12-10T05:19:19.744587: step 2628, loss 0.151835, acc 0.90625, prec 0.0617834, recall 0.780244
2017-12-10T05:19:20.008956: step 2629, loss 0.365116, acc 0.921875, prec 0.0617969, recall 0.780303
2017-12-10T05:19:20.274113: step 2630, loss 0.172767, acc 0.9375, prec 0.0617916, recall 0.780303
2017-12-10T05:19:20.535213: step 2631, loss 0.239828, acc 0.96875, prec 0.0618493, recall 0.780481
2017-12-10T05:19:20.794920: step 2632, loss 0.438292, acc 0.828125, prec 0.0618548, recall 0.780541
2017-12-10T05:19:21.062148: step 2633, loss 0.528702, acc 0.859375, prec 0.0618429, recall 0.780541
2017-12-10T05:19:21.329621: step 2634, loss 0.172082, acc 0.921875, prec 0.0618362, recall 0.780541
2017-12-10T05:19:21.596634: step 2635, loss 0.255767, acc 0.921875, prec 0.0618497, recall 0.7806
2017-12-10T05:19:21.865920: step 2636, loss 0.542512, acc 0.96875, prec 0.0619073, recall 0.780778
2017-12-10T05:19:22.140472: step 2637, loss 0.198259, acc 0.90625, prec 0.0618994, recall 0.780778
2017-12-10T05:19:22.410702: step 2638, loss 0.0768477, acc 0.9375, prec 0.0619342, recall 0.780896
2017-12-10T05:19:22.676184: step 2639, loss 0.206019, acc 0.9375, prec 0.0619289, recall 0.780896
2017-12-10T05:19:22.941658: step 2640, loss 0.0690592, acc 0.984375, prec 0.0619477, recall 0.780955
2017-12-10T05:19:23.203784: step 2641, loss 0.200135, acc 0.96875, prec 0.061945, recall 0.780955
2017-12-10T05:19:23.468308: step 2642, loss 0.266661, acc 0.984375, prec 0.0619638, recall 0.781014
2017-12-10T05:19:23.739505: step 2643, loss 0.342804, acc 0.90625, prec 0.0619759, recall 0.781073
2017-12-10T05:19:24.006411: step 2644, loss 0.879651, acc 1, prec 0.062016, recall 0.781191
2017-12-10T05:19:24.281964: step 2645, loss 0.00849772, acc 1, prec 0.062016, recall 0.781191
2017-12-10T05:19:24.542488: step 2646, loss 0.367487, acc 0.921875, prec 0.0620294, recall 0.78125
2017-12-10T05:19:24.809253: step 2647, loss 0.0535094, acc 0.96875, prec 0.062107, recall 0.781485
2017-12-10T05:19:25.082733: step 2648, loss 0.535151, acc 0.953125, prec 0.0621431, recall 0.781603
2017-12-10T05:19:25.347961: step 2649, loss 0.750789, acc 0.9375, prec 0.0621779, recall 0.78172
2017-12-10T05:19:25.611213: step 2650, loss 0.396422, acc 0.890625, prec 0.0622087, recall 0.781838
2017-12-10T05:19:25.880104: step 2651, loss 0.0938959, acc 0.984375, prec 0.0622475, recall 0.781955
2017-12-10T05:19:26.147128: step 2652, loss 0.52235, acc 0.921875, prec 0.0622408, recall 0.781955
2017-12-10T05:19:26.407649: step 2653, loss 0.188757, acc 0.921875, prec 0.0622342, recall 0.781955
2017-12-10T05:19:26.680567: step 2654, loss 0.360861, acc 0.921875, prec 0.0622476, recall 0.782013
2017-12-10T05:19:26.952688: step 2655, loss 5.29463, acc 0.921875, prec 0.0622423, recall 0.781804
2017-12-10T05:19:27.224420: step 2656, loss 0.146698, acc 0.96875, prec 0.0622596, recall 0.781862
2017-12-10T05:19:27.491950: step 2657, loss 0.777954, acc 0.84375, prec 0.0622463, recall 0.781862
2017-12-10T05:19:27.754993: step 2658, loss 0.319341, acc 0.90625, prec 0.0622984, recall 0.782038
2017-12-10T05:19:28.020967: step 2659, loss 0.488759, acc 0.859375, prec 0.0622865, recall 0.782038
2017-12-10T05:19:28.286199: step 2660, loss 0.464779, acc 0.8125, prec 0.0622905, recall 0.782096
2017-12-10T05:19:28.545607: step 2661, loss 0.567344, acc 0.875, prec 0.0622999, recall 0.782154
2017-12-10T05:19:28.806100: step 2662, loss 0.552795, acc 0.90625, prec 0.0623119, recall 0.782213
2017-12-10T05:19:29.070803: step 2663, loss 0.222544, acc 0.921875, prec 0.0623253, recall 0.782271
2017-12-10T05:19:29.336537: step 2664, loss 0.322458, acc 0.875, prec 0.0623547, recall 0.782388
2017-12-10T05:19:29.602830: step 2665, loss 0.342381, acc 0.90625, prec 0.0623667, recall 0.782446
2017-12-10T05:19:29.868125: step 2666, loss 0.307011, acc 0.875, prec 0.062376, recall 0.782504
2017-12-10T05:19:30.137881: step 2667, loss 0.353406, acc 0.921875, prec 0.0623894, recall 0.782562
2017-12-10T05:19:30.413044: step 2668, loss 0.523608, acc 0.828125, prec 0.0623748, recall 0.782562
2017-12-10T05:19:30.692920: step 2669, loss 0.660123, acc 0.84375, prec 0.0623815, recall 0.78262
2017-12-10T05:19:30.954414: step 2670, loss 0.358621, acc 0.890625, prec 0.0623721, recall 0.78262
2017-12-10T05:19:31.222373: step 2671, loss 0.526021, acc 0.859375, prec 0.0623802, recall 0.782678
2017-12-10T05:19:31.489171: step 2672, loss 0.354926, acc 0.875, prec 0.0624095, recall 0.782795
2017-12-10T05:19:31.752394: step 2673, loss 0.190722, acc 0.96875, prec 0.0624268, recall 0.782853
2017-12-10T05:19:32.012219: step 2674, loss 0.131923, acc 0.9375, prec 0.0624414, recall 0.782911
2017-12-10T05:19:32.280210: step 2675, loss 0.0126538, acc 1, prec 0.0624414, recall 0.782911
2017-12-10T05:19:32.543847: step 2676, loss 0.104541, acc 0.96875, prec 0.0624388, recall 0.782911
2017-12-10T05:19:32.808553: step 2677, loss 0.117323, acc 0.9375, prec 0.0624335, recall 0.782911
2017-12-10T05:19:33.071774: step 2678, loss 0.142052, acc 0.96875, prec 0.0624308, recall 0.782911
2017-12-10T05:19:33.336943: step 2679, loss 0.0828134, acc 0.96875, prec 0.062488, recall 0.783084
2017-12-10T05:19:33.601222: step 2680, loss 0.122667, acc 0.96875, prec 0.0624854, recall 0.783084
2017-12-10T05:19:33.870042: step 2681, loss 0.0680816, acc 0.96875, prec 0.0624827, recall 0.783084
2017-12-10T05:19:34.143368: step 2682, loss 0.014169, acc 1, prec 0.0625027, recall 0.783142
2017-12-10T05:19:34.406988: step 2683, loss 0.0778675, acc 0.96875, prec 0.0625, recall 0.783142
2017-12-10T05:19:34.669804: step 2684, loss 1.35924, acc 0.953125, prec 0.0624973, recall 0.782933
2017-12-10T05:19:34.938062: step 2685, loss 2.84688, acc 0.953125, prec 0.0625146, recall 0.782782
2017-12-10T05:19:35.210461: step 2686, loss 0.130327, acc 0.984375, prec 0.0625333, recall 0.78284
2017-12-10T05:19:35.473933: step 2687, loss 0.0596446, acc 0.984375, prec 0.0625319, recall 0.78284
2017-12-10T05:19:35.742201: step 2688, loss 0.203235, acc 0.953125, prec 0.0625279, recall 0.78284
2017-12-10T05:19:36.010066: step 2689, loss 0.109826, acc 0.984375, prec 0.0625665, recall 0.782956
2017-12-10T05:19:36.278745: step 2690, loss 0.32307, acc 0.90625, prec 0.0625785, recall 0.783014
2017-12-10T05:19:36.545170: step 2691, loss 0.594773, acc 0.96875, prec 0.0626157, recall 0.783129
2017-12-10T05:19:36.811505: step 2692, loss 0.256231, acc 0.90625, prec 0.0626077, recall 0.783129
2017-12-10T05:19:37.084477: step 2693, loss 0.208652, acc 0.953125, prec 0.0626037, recall 0.783129
2017-12-10T05:19:37.351731: step 2694, loss 0.308291, acc 0.859375, prec 0.0626117, recall 0.783187
2017-12-10T05:19:37.612019: step 2695, loss 0.230367, acc 0.90625, prec 0.0626435, recall 0.783302
2017-12-10T05:19:37.879317: step 2696, loss 0.298551, acc 0.890625, prec 0.0626541, recall 0.78336
2017-12-10T05:19:38.145149: step 2697, loss 0.395781, acc 0.890625, prec 0.0626647, recall 0.783417
2017-12-10T05:19:38.406812: step 2698, loss 0.466976, acc 0.921875, prec 0.0627178, recall 0.78359
2017-12-10T05:19:38.668606: step 2699, loss 0.286298, acc 0.953125, prec 0.0627537, recall 0.783705
2017-12-10T05:19:38.932377: step 2700, loss 0.641761, acc 0.8125, prec 0.0627576, recall 0.783762

Evaluation:
2017-12-10T05:19:46.562190: step 2700, loss 1.74773, acc 0.87375, prec 0.0631924, recall 0.780581

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-2700

2017-12-10T05:19:47.969099: step 2701, loss 0.527192, acc 0.875, prec 0.063182, recall 0.780581
2017-12-10T05:19:48.236480: step 2702, loss 0.408585, acc 0.921875, prec 0.0632334, recall 0.780749
2017-12-10T05:19:48.497125: step 2703, loss 0.503029, acc 0.8125, prec 0.0632178, recall 0.780749
2017-12-10T05:19:48.764468: step 2704, loss 0.733943, acc 0.921875, prec 0.0632499, recall 0.78086
2017-12-10T05:19:49.026660: step 2705, loss 0.44902, acc 0.921875, prec 0.0632434, recall 0.78086
2017-12-10T05:19:49.295999: step 2706, loss 0.518414, acc 0.84375, prec 0.0632497, recall 0.780916
2017-12-10T05:19:49.558411: step 2707, loss 0.16775, acc 0.921875, prec 0.0632431, recall 0.780916
2017-12-10T05:19:49.830753: step 2708, loss 0.186767, acc 0.921875, prec 0.0632559, recall 0.780972
2017-12-10T05:19:50.097699: step 2709, loss 0.238946, acc 0.9375, prec 0.06327, recall 0.781027
2017-12-10T05:19:50.362181: step 2710, loss 0.0994132, acc 0.953125, prec 0.0632661, recall 0.781027
2017-12-10T05:19:50.633858: step 2711, loss 0.21795, acc 0.953125, prec 0.0632622, recall 0.781027
2017-12-10T05:19:50.902416: step 2712, loss 0.103845, acc 0.953125, prec 0.0632776, recall 0.781083
2017-12-10T05:19:51.170850: step 2713, loss 0.302076, acc 0.921875, prec 0.0632711, recall 0.781083
2017-12-10T05:19:51.439393: step 2714, loss 0.178982, acc 0.953125, prec 0.0633443, recall 0.781306
2017-12-10T05:19:51.702202: step 2715, loss 0.346347, acc 0.9375, prec 0.0633584, recall 0.781361
2017-12-10T05:19:51.970020: step 2716, loss 0.262823, acc 0.96875, prec 0.0633943, recall 0.781472
2017-12-10T05:19:52.232623: step 2717, loss 0.388916, acc 0.875, prec 0.0634032, recall 0.781528
2017-12-10T05:19:52.499661: step 2718, loss 0.0589698, acc 0.953125, prec 0.0633993, recall 0.781528
2017-12-10T05:19:52.764003: step 2719, loss 1.08767, acc 0.921875, prec 0.0634313, recall 0.781638
2017-12-10T05:19:53.031329: step 2720, loss 0.0557192, acc 0.96875, prec 0.0634865, recall 0.781804
2017-12-10T05:19:53.298552: step 2721, loss 0.125472, acc 0.984375, prec 0.0634852, recall 0.781804
2017-12-10T05:19:53.561343: step 2722, loss 0.130634, acc 0.953125, prec 0.0635006, recall 0.78186
2017-12-10T05:19:53.831333: step 2723, loss 0.111048, acc 0.984375, prec 0.0634992, recall 0.78186
2017-12-10T05:19:54.097575: step 2724, loss 0.1609, acc 0.953125, prec 0.0634953, recall 0.78186
2017-12-10T05:19:54.362543: step 2725, loss 0.0759513, acc 0.96875, prec 0.063512, recall 0.781915
2017-12-10T05:19:54.639317: step 2726, loss 0.251125, acc 0.984375, prec 0.0635299, recall 0.78197
2017-12-10T05:19:54.905762: step 2727, loss 0.0647009, acc 0.984375, prec 0.0635479, recall 0.782025
2017-12-10T05:19:55.167032: step 2728, loss 0.525417, acc 0.9375, prec 0.0635619, recall 0.78208
2017-12-10T05:19:55.433282: step 2729, loss 3.43003, acc 0.9375, prec 0.0635773, recall 0.781938
2017-12-10T05:19:55.700281: step 2730, loss 0.172901, acc 0.9375, prec 0.063572, recall 0.781938
2017-12-10T05:19:55.970426: step 2731, loss 0.15903, acc 0.921875, prec 0.0635848, recall 0.781993
2017-12-10T05:19:56.237253: step 2732, loss 0.266981, acc 0.953125, prec 0.0635808, recall 0.781993
2017-12-10T05:19:56.500237: step 2733, loss 0.509689, acc 0.859375, prec 0.0635883, recall 0.782048
2017-12-10T05:19:56.767898: step 2734, loss 0.21731, acc 0.921875, prec 0.0636203, recall 0.782158
2017-12-10T05:19:57.039047: step 2735, loss 0.509886, acc 0.875, prec 0.0636291, recall 0.782213
2017-12-10T05:19:57.315630: step 2736, loss 1.03836, acc 0.859375, prec 0.0636173, recall 0.782213
2017-12-10T05:19:57.580100: step 2737, loss 0.439086, acc 0.90625, prec 0.0636095, recall 0.782213
2017-12-10T05:19:57.842062: step 2738, loss 0.700574, acc 0.796875, prec 0.0636117, recall 0.782268
2017-12-10T05:19:58.105077: step 2739, loss 0.368883, acc 0.859375, prec 0.0636384, recall 0.782378
2017-12-10T05:19:58.377493: step 2740, loss 0.131198, acc 0.953125, prec 0.063673, recall 0.782488
2017-12-10T05:19:58.646004: step 2741, loss 0.366257, acc 0.875, prec 0.0637009, recall 0.782598
2017-12-10T05:19:58.908485: step 2742, loss 0.312591, acc 0.90625, prec 0.0636931, recall 0.782598
2017-12-10T05:19:59.179550: step 2743, loss 0.205856, acc 0.9375, prec 0.0637263, recall 0.782707
2017-12-10T05:19:59.445128: step 2744, loss 0.175843, acc 0.984375, prec 0.063725, recall 0.782707
2017-12-10T05:19:59.713025: step 2745, loss 0.312443, acc 0.953125, prec 0.0637211, recall 0.782707
2017-12-10T05:19:59.979998: step 2746, loss 0.253015, acc 0.890625, prec 0.0637119, recall 0.782707
2017-12-10T05:20:00.253354: step 2747, loss 0.0196017, acc 1, prec 0.0637311, recall 0.782762
2017-12-10T05:20:00.522713: step 2748, loss 0.395262, acc 0.859375, prec 0.0637194, recall 0.782762
2017-12-10T05:20:00.796599: step 2749, loss 0.0304437, acc 0.984375, prec 0.0637373, recall 0.782817
2017-12-10T05:20:01.061276: step 2750, loss 0.19607, acc 0.90625, prec 0.0637678, recall 0.782926
2017-12-10T05:20:01.326564: step 2751, loss 0.171943, acc 0.953125, prec 0.0637639, recall 0.782926
2017-12-10T05:20:01.590797: step 2752, loss 0.0802244, acc 0.96875, prec 0.0637613, recall 0.782926
2017-12-10T05:20:01.850281: step 2753, loss 0.441059, acc 0.9375, prec 0.0637752, recall 0.782981
2017-12-10T05:20:02.125868: step 2754, loss 0.14055, acc 1, prec 0.063852, recall 0.783199
2017-12-10T05:20:02.397627: step 2755, loss 0.48077, acc 0.9375, prec 0.063866, recall 0.783254
2017-12-10T05:20:02.666227: step 2756, loss 0.101826, acc 0.96875, prec 0.0638634, recall 0.783254
2017-12-10T05:20:02.928991: step 2757, loss 0.068127, acc 0.984375, prec 0.0638621, recall 0.783254
2017-12-10T05:20:03.194739: step 2758, loss 0.116531, acc 0.9375, prec 0.063876, recall 0.783308
2017-12-10T05:20:03.458396: step 2759, loss 1.34583, acc 0.984375, prec 0.063876, recall 0.783111
2017-12-10T05:20:03.729919: step 2760, loss 0.106579, acc 0.96875, prec 0.0638734, recall 0.783111
2017-12-10T05:20:03.999967: step 2761, loss 0.225706, acc 0.921875, prec 0.063886, recall 0.783166
2017-12-10T05:20:04.266619: step 2762, loss 0.0646146, acc 0.984375, prec 0.0638847, recall 0.783166
2017-12-10T05:20:04.534468: step 2763, loss 1.03372, acc 0.9375, prec 0.0639179, recall 0.783275
2017-12-10T05:20:04.801889: step 2764, loss 0.0840674, acc 0.96875, prec 0.0639152, recall 0.783275
2017-12-10T05:20:05.069379: step 2765, loss 0.428335, acc 0.90625, prec 0.0639074, recall 0.783275
2017-12-10T05:20:05.336015: step 2766, loss 0.437278, acc 0.859375, prec 0.0638956, recall 0.783275
2017-12-10T05:20:05.602532: step 2767, loss 0.126452, acc 0.9375, prec 0.0638904, recall 0.783275
2017-12-10T05:20:05.866321: step 2768, loss 0.214596, acc 0.921875, prec 0.063903, recall 0.783329
2017-12-10T05:20:06.130855: step 2769, loss 0.243709, acc 0.953125, prec 0.0638991, recall 0.783329
2017-12-10T05:20:06.394995: step 2770, loss 0.08712, acc 0.96875, prec 0.0639348, recall 0.783438
2017-12-10T05:20:06.658222: step 2771, loss 0.0692822, acc 0.96875, prec 0.0639322, recall 0.783438
2017-12-10T05:20:06.920633: step 2772, loss 0.283386, acc 0.90625, prec 0.0639243, recall 0.783438
2017-12-10T05:20:07.185700: step 2773, loss 0.231041, acc 0.953125, prec 0.0639587, recall 0.783547
2017-12-10T05:20:07.448783: step 2774, loss 0.400204, acc 0.9375, prec 0.0639918, recall 0.783655
2017-12-10T05:20:07.711718: step 2775, loss 0.150038, acc 0.96875, prec 0.0639892, recall 0.783655
2017-12-10T05:20:07.980074: step 2776, loss 0.0612087, acc 0.96875, prec 0.0640057, recall 0.783709
2017-12-10T05:20:08.243286: step 2777, loss 0.0715213, acc 0.96875, prec 0.0640031, recall 0.783709
2017-12-10T05:20:08.509815: step 2778, loss 0.0539313, acc 0.96875, prec 0.0640197, recall 0.783763
2017-12-10T05:20:08.782552: step 2779, loss 0.273553, acc 0.984375, prec 0.0640566, recall 0.783872
2017-12-10T05:20:09.052512: step 2780, loss 0.381502, acc 0.90625, prec 0.0640679, recall 0.783926
2017-12-10T05:20:09.314623: step 2781, loss 0.3091, acc 0.9375, prec 0.0641201, recall 0.784088
2017-12-10T05:20:09.579732: step 2782, loss 0.31086, acc 0.96875, prec 0.0641558, recall 0.784196
2017-12-10T05:20:09.849957: step 2783, loss 0.144, acc 0.953125, prec 0.064171, recall 0.78425
2017-12-10T05:20:10.124988: step 2784, loss 0.168828, acc 0.953125, prec 0.0641862, recall 0.784304
2017-12-10T05:20:10.386177: step 2785, loss 0.218708, acc 0.96875, prec 0.0642027, recall 0.784358
2017-12-10T05:20:10.648967: step 2786, loss 0.510526, acc 0.890625, prec 0.064251, recall 0.784519
2017-12-10T05:20:10.912945: step 2787, loss 0.124964, acc 0.96875, prec 0.0642675, recall 0.784573
2017-12-10T05:20:11.180420: step 2788, loss 0.259137, acc 0.921875, prec 0.0642609, recall 0.784573
2017-12-10T05:20:11.447307: step 2789, loss 2.09244, acc 0.96875, prec 0.0642596, recall 0.784377
2017-12-10T05:20:11.714698: step 2790, loss 0.0523918, acc 0.96875, prec 0.0642952, recall 0.784485
2017-12-10T05:20:11.984950: step 2791, loss 0.0778141, acc 0.96875, prec 0.0642926, recall 0.784485
2017-12-10T05:20:12.259127: step 2792, loss 0.125915, acc 0.9375, prec 0.0642873, recall 0.784485
2017-12-10T05:20:12.530036: step 2793, loss 0.16191, acc 0.921875, prec 0.0642807, recall 0.784485
2017-12-10T05:20:12.788671: step 2794, loss 0.223514, acc 0.890625, prec 0.0642907, recall 0.784539
2017-12-10T05:20:13.053571: step 2795, loss 0.499725, acc 0.84375, prec 0.0642775, recall 0.784539
2017-12-10T05:20:13.322668: step 2796, loss 0.283837, acc 0.890625, prec 0.0643257, recall 0.7847
2017-12-10T05:20:13.591789: step 2797, loss 0.360269, acc 0.953125, prec 0.06436, recall 0.784807
2017-12-10T05:20:13.856402: step 2798, loss 0.519743, acc 0.859375, prec 0.0643672, recall 0.784861
2017-12-10T05:20:14.129152: step 2799, loss 0.112879, acc 0.953125, prec 0.0643633, recall 0.784861
2017-12-10T05:20:14.395986: step 2800, loss 0.328681, acc 0.890625, prec 0.0643541, recall 0.784861
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-2800

2017-12-10T05:20:15.684913: step 2801, loss 0.227173, acc 0.953125, prec 0.0643693, recall 0.784914
2017-12-10T05:20:15.954808: step 2802, loss 0.219197, acc 0.9375, prec 0.0644022, recall 0.785021
2017-12-10T05:20:16.217464: step 2803, loss 0.182955, acc 0.953125, prec 0.0644174, recall 0.785075
2017-12-10T05:20:16.493474: step 2804, loss 1.91296, acc 0.984375, prec 0.0644365, recall 0.784933
2017-12-10T05:20:16.760711: step 2805, loss 0.0914835, acc 0.9375, prec 0.0644694, recall 0.78504
2017-12-10T05:20:17.034003: step 2806, loss 0.0783969, acc 0.984375, prec 0.0644872, recall 0.785093
2017-12-10T05:20:17.294742: step 2807, loss 0.446116, acc 0.90625, prec 0.0644793, recall 0.785093
2017-12-10T05:20:17.559128: step 2808, loss 0.109727, acc 0.984375, prec 0.064478, recall 0.785093
2017-12-10T05:20:17.832890: step 2809, loss 0.132996, acc 0.921875, prec 0.0644905, recall 0.785147
2017-12-10T05:20:18.104906: step 2810, loss 0.389287, acc 0.9375, prec 0.0644852, recall 0.785147
2017-12-10T05:20:18.368213: step 2811, loss 0.460157, acc 0.9375, prec 0.0645372, recall 0.785307
2017-12-10T05:20:18.633733: step 2812, loss 0.214069, acc 0.953125, prec 0.0645523, recall 0.78536
2017-12-10T05:20:18.893475: step 2813, loss 0.386187, acc 0.90625, prec 0.0645635, recall 0.785413
2017-12-10T05:20:19.158715: step 2814, loss 0.180732, acc 0.90625, prec 0.0645937, recall 0.785519
2017-12-10T05:20:19.427294: step 2815, loss 0.0581891, acc 0.96875, prec 0.0645911, recall 0.785519
2017-12-10T05:20:19.690439: step 2816, loss 0.36082, acc 0.9375, prec 0.0646049, recall 0.785573
2017-12-10T05:20:19.964676: step 2817, loss 0.293928, acc 0.890625, prec 0.0646148, recall 0.785626
2017-12-10T05:20:20.232378: step 2818, loss 0.285706, acc 0.9375, prec 0.0646095, recall 0.785626
2017-12-10T05:20:20.501851: step 2819, loss 0.0907949, acc 0.984375, prec 0.0646082, recall 0.785626
2017-12-10T05:20:20.769266: step 2820, loss 0.321999, acc 0.953125, prec 0.0646042, recall 0.785626
2017-12-10T05:20:21.032772: step 2821, loss 0.0613971, acc 0.984375, prec 0.064622, recall 0.785679
2017-12-10T05:20:21.296839: step 2822, loss 0.0739054, acc 0.96875, prec 0.0646384, recall 0.785732
2017-12-10T05:20:21.563120: step 2823, loss 0.174773, acc 0.9375, prec 0.0646522, recall 0.785785
2017-12-10T05:20:21.832966: step 2824, loss 1.49843, acc 0.9375, prec 0.0646482, recall 0.78559
2017-12-10T05:20:22.097061: step 2825, loss 0.743592, acc 0.953125, prec 0.0646824, recall 0.785697
2017-12-10T05:20:22.364416: step 2826, loss 0.174601, acc 0.921875, prec 0.0646949, recall 0.78575
2017-12-10T05:20:22.629404: step 2827, loss 0.415843, acc 0.875, prec 0.0646843, recall 0.78575
2017-12-10T05:20:22.897618: step 2828, loss 0.427746, acc 0.90625, prec 0.0646955, recall 0.785803
2017-12-10T05:20:23.159502: step 2829, loss 0.218979, acc 0.9375, prec 0.0647092, recall 0.785856
2017-12-10T05:20:23.436855: step 2830, loss 0.451863, acc 0.875, prec 0.0647368, recall 0.785961
2017-12-10T05:20:23.702249: step 2831, loss 0.13691, acc 0.9375, prec 0.0648267, recall 0.786226
2017-12-10T05:20:23.964340: step 2832, loss 0.540201, acc 0.9375, prec 0.0648785, recall 0.786384
2017-12-10T05:20:24.228315: step 2833, loss 0.178607, acc 0.9375, prec 0.0648732, recall 0.786384
2017-12-10T05:20:24.502261: step 2834, loss 0.191461, acc 0.921875, prec 0.0648666, recall 0.786384
2017-12-10T05:20:24.765112: step 2835, loss 0.343736, acc 0.890625, prec 0.0648574, recall 0.786384
2017-12-10T05:20:25.032659: step 2836, loss 0.136425, acc 0.9375, prec 0.0648902, recall 0.786489
2017-12-10T05:20:25.301283: step 2837, loss 0.0830151, acc 0.96875, prec 0.0649065, recall 0.786542
2017-12-10T05:20:25.566836: step 2838, loss 0.34796, acc 0.859375, prec 0.0649137, recall 0.786594
2017-12-10T05:20:25.829686: step 2839, loss 0.644641, acc 0.84375, prec 0.0649575, recall 0.786752
2017-12-10T05:20:26.096189: step 2840, loss 0.412762, acc 0.890625, prec 0.0649483, recall 0.786752
2017-12-10T05:20:26.361421: step 2841, loss 0.243403, acc 0.90625, prec 0.0649403, recall 0.786752
2017-12-10T05:20:26.626305: step 2842, loss 0.379105, acc 0.90625, prec 0.0649324, recall 0.786752
2017-12-10T05:20:26.891616: step 2843, loss 0.152219, acc 0.953125, prec 0.0649475, recall 0.786805
2017-12-10T05:20:27.162651: step 2844, loss 0.55404, acc 0.96875, prec 0.0649828, recall 0.786909
2017-12-10T05:20:27.431446: step 2845, loss 0.194524, acc 0.9375, prec 0.0649775, recall 0.786909
2017-12-10T05:20:27.693480: step 2846, loss 0.0360151, acc 0.96875, prec 0.0649749, recall 0.786909
2017-12-10T05:20:27.957086: step 2847, loss 0.141955, acc 0.953125, prec 0.0649899, recall 0.786962
2017-12-10T05:20:28.217780: step 2848, loss 0.259408, acc 0.953125, prec 0.065024, recall 0.787067
2017-12-10T05:20:28.485318: step 2849, loss 0.492147, acc 0.9375, prec 0.0650567, recall 0.787171
2017-12-10T05:20:28.759359: step 2850, loss 0.124002, acc 0.953125, prec 0.0650717, recall 0.787224
2017-12-10T05:20:29.029016: step 2851, loss 0.103568, acc 0.953125, prec 0.0650677, recall 0.787224
2017-12-10T05:20:29.295814: step 2852, loss 0.0825131, acc 0.953125, prec 0.0650828, recall 0.787276
2017-12-10T05:20:29.557795: step 2853, loss 0.221998, acc 0.96875, prec 0.0650801, recall 0.787276
2017-12-10T05:20:29.821670: step 2854, loss 0.185569, acc 0.953125, prec 0.0651141, recall 0.78738
2017-12-10T05:20:30.087651: step 2855, loss 0.0889754, acc 0.96875, prec 0.0651115, recall 0.78738
2017-12-10T05:20:30.354858: step 2856, loss 0.310349, acc 0.96875, prec 0.0651088, recall 0.78738
2017-12-10T05:20:30.624804: step 2857, loss 0.103512, acc 0.953125, prec 0.0651428, recall 0.787485
2017-12-10T05:20:30.893233: step 2858, loss 0.209746, acc 0.921875, prec 0.0651362, recall 0.787485
2017-12-10T05:20:31.158352: step 2859, loss 0.204051, acc 0.953125, prec 0.0651512, recall 0.787537
2017-12-10T05:20:31.421198: step 2860, loss 1.06456, acc 0.96875, prec 0.0651675, recall 0.787589
2017-12-10T05:20:31.688841: step 2861, loss 0.160578, acc 0.984375, prec 0.0651662, recall 0.787589
2017-12-10T05:20:31.956850: step 2862, loss 0.0567482, acc 0.984375, prec 0.0651649, recall 0.787589
2017-12-10T05:20:32.218410: step 2863, loss 0.112272, acc 0.96875, prec 0.0651812, recall 0.787641
2017-12-10T05:20:32.486024: step 2864, loss 0.0378159, acc 0.984375, prec 0.0651799, recall 0.787641
2017-12-10T05:20:32.753586: step 2865, loss 0.124686, acc 0.96875, prec 0.0651962, recall 0.787693
2017-12-10T05:20:33.020954: step 2866, loss 0.243265, acc 0.9375, prec 0.0652289, recall 0.787797
2017-12-10T05:20:33.292202: step 2867, loss 0.480371, acc 0.953125, prec 0.0652628, recall 0.787901
2017-12-10T05:20:33.565168: step 2868, loss 0.310321, acc 0.9375, prec 0.0652575, recall 0.787901
2017-12-10T05:20:33.830362: step 2869, loss 0.0501454, acc 0.984375, prec 0.0652562, recall 0.787901
2017-12-10T05:20:34.099282: step 2870, loss 0.305526, acc 0.921875, prec 0.0652685, recall 0.787953
2017-12-10T05:20:34.369270: step 2871, loss 0.130916, acc 0.96875, prec 0.0652659, recall 0.787953
2017-12-10T05:20:34.634958: step 2872, loss 0.112886, acc 0.953125, prec 0.0652619, recall 0.787953
2017-12-10T05:20:34.898848: step 2873, loss 0.0846029, acc 0.96875, prec 0.0653161, recall 0.788109
2017-12-10T05:20:35.169439: step 2874, loss 0.0366686, acc 0.984375, prec 0.0653338, recall 0.78816
2017-12-10T05:20:36.138068: step 2875, loss 0.186571, acc 0.984375, prec 0.0653703, recall 0.788264
2017-12-10T05:20:36.487691: step 2876, loss 0.16996, acc 0.9375, prec 0.065365, recall 0.788264
2017-12-10T05:20:36.756074: step 2877, loss 0.830315, acc 0.90625, prec 0.065376, recall 0.788316
2017-12-10T05:20:37.440889: step 2878, loss 0.225601, acc 0.9375, prec 0.0653897, recall 0.788368
2017-12-10T05:20:38.207734: step 2879, loss 1.91144, acc 0.953125, prec 0.065387, recall 0.788175
2017-12-10T05:20:38.944454: step 2880, loss 0.315932, acc 0.984375, prec 0.0654046, recall 0.788227
2017-12-10T05:20:39.661308: step 2881, loss 0.149832, acc 0.921875, prec 0.065398, recall 0.788227
2017-12-10T05:20:40.674425: step 2882, loss 0.50166, acc 0.90625, prec 0.0654279, recall 0.78833
2017-12-10T05:20:41.021827: step 2883, loss 0.203023, acc 0.9375, prec 0.0654416, recall 0.788382
2017-12-10T05:20:41.366677: step 2884, loss 0.214074, acc 0.9375, prec 0.0654741, recall 0.788485
2017-12-10T05:20:41.641039: step 2885, loss 0.150471, acc 0.9375, prec 0.0654878, recall 0.788537
2017-12-10T05:20:41.919280: step 2886, loss 0.265311, acc 0.921875, prec 0.0655001, recall 0.788588
2017-12-10T05:20:42.213805: step 2887, loss 0.629078, acc 0.90625, prec 0.0655489, recall 0.788743
2017-12-10T05:20:42.500334: step 2888, loss 0.762335, acc 0.921875, prec 0.0655612, recall 0.788794
2017-12-10T05:20:42.785132: step 2889, loss 0.10427, acc 0.9375, prec 0.0655937, recall 0.788897
2017-12-10T05:20:43.054800: step 2890, loss 0.30647, acc 0.9375, prec 0.0656073, recall 0.788948
2017-12-10T05:20:43.315829: step 2891, loss 0.211477, acc 0.921875, prec 0.0656006, recall 0.788948
2017-12-10T05:20:43.584450: step 2892, loss 0.147027, acc 0.9375, prec 0.0656142, recall 0.789
2017-12-10T05:20:43.844356: step 2893, loss 0.311081, acc 0.90625, prec 0.0656063, recall 0.789
2017-12-10T05:20:44.109267: step 2894, loss 0.327253, acc 0.90625, prec 0.0655983, recall 0.789
2017-12-10T05:20:44.381761: step 2895, loss 0.433549, acc 0.921875, prec 0.0656295, recall 0.789102
2017-12-10T05:20:44.644559: step 2896, loss 0.138392, acc 0.96875, prec 0.0656457, recall 0.789154
2017-12-10T05:20:44.909754: step 2897, loss 0.0919952, acc 0.96875, prec 0.0656431, recall 0.789154
2017-12-10T05:20:45.171553: step 2898, loss 0.098035, acc 0.9375, prec 0.0656567, recall 0.789205
2017-12-10T05:20:45.437138: step 2899, loss 2.2318, acc 0.921875, prec 0.0656703, recall 0.789064
2017-12-10T05:20:45.709288: step 2900, loss 0.53358, acc 0.828125, prec 0.0656745, recall 0.789116
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-2900

2017-12-10T05:20:46.932797: step 2901, loss 0.111955, acc 0.921875, prec 0.0656868, recall 0.789167
2017-12-10T05:20:47.198637: step 2902, loss 0.541663, acc 0.84375, prec 0.0656735, recall 0.789167
2017-12-10T05:20:47.466474: step 2903, loss 0.287085, acc 0.921875, prec 0.0656858, recall 0.789218
2017-12-10T05:20:47.731312: step 2904, loss 0.364196, acc 0.921875, prec 0.0656791, recall 0.789218
2017-12-10T05:20:47.992439: step 2905, loss 0.524488, acc 0.8125, prec 0.0656821, recall 0.789269
2017-12-10T05:20:48.257254: step 2906, loss 0.247457, acc 0.875, prec 0.0656715, recall 0.789269
2017-12-10T05:20:48.522746: step 2907, loss 0.224592, acc 0.90625, prec 0.0656635, recall 0.789269
2017-12-10T05:20:48.786487: step 2908, loss 0.29302, acc 0.921875, prec 0.0656569, recall 0.789269
2017-12-10T05:20:49.052480: step 2909, loss 0.11355, acc 0.9375, prec 0.0656516, recall 0.789269
2017-12-10T05:20:49.320399: step 2910, loss 0.291924, acc 0.921875, prec 0.0656449, recall 0.789269
2017-12-10T05:20:49.587971: step 2911, loss 0.165629, acc 0.921875, prec 0.0656383, recall 0.789269
2017-12-10T05:20:49.849612: step 2912, loss 0.102782, acc 0.96875, prec 0.0656545, recall 0.78932
2017-12-10T05:20:50.120679: step 2913, loss 0.165614, acc 0.921875, prec 0.0656479, recall 0.78932
2017-12-10T05:20:50.383798: step 2914, loss 4.14007, acc 0.9375, prec 0.0656628, recall 0.78918
2017-12-10T05:20:50.655127: step 2915, loss 0.160626, acc 0.921875, prec 0.065675, recall 0.789231
2017-12-10T05:20:50.916056: step 2916, loss 0.124013, acc 0.953125, prec 0.065671, recall 0.789231
2017-12-10T05:20:51.184439: step 2917, loss 0.361288, acc 0.890625, prec 0.0656618, recall 0.789231
2017-12-10T05:20:51.447509: step 2918, loss 0.370179, acc 0.921875, prec 0.0656928, recall 0.789333
2017-12-10T05:20:51.712567: step 2919, loss 1.03935, acc 0.90625, prec 0.0657226, recall 0.789435
2017-12-10T05:20:51.983430: step 2920, loss 0.421865, acc 0.90625, prec 0.0657523, recall 0.789537
2017-12-10T05:20:52.244507: step 2921, loss 0.585946, acc 0.875, prec 0.0657417, recall 0.789537
2017-12-10T05:20:52.511867: step 2922, loss 0.291463, acc 0.921875, prec 0.0657539, recall 0.789588
2017-12-10T05:20:52.772114: step 2923, loss 0.199726, acc 0.9375, prec 0.0658051, recall 0.789741
2017-12-10T05:20:53.032652: step 2924, loss 0.249998, acc 0.90625, prec 0.065816, recall 0.789792
2017-12-10T05:20:53.304411: step 2925, loss 0.327923, acc 0.90625, prec 0.0658269, recall 0.789843
2017-12-10T05:20:53.565009: step 2926, loss 0.200713, acc 0.9375, prec 0.0658216, recall 0.789843
2017-12-10T05:20:53.830366: step 2927, loss 0.599889, acc 0.890625, prec 0.0658123, recall 0.789843
2017-12-10T05:20:54.091108: step 2928, loss 0.231229, acc 0.953125, prec 0.0658271, recall 0.789894
2017-12-10T05:20:54.359799: step 2929, loss 0.426936, acc 0.90625, prec 0.0658192, recall 0.789894
2017-12-10T05:20:54.625382: step 2930, loss 0.208026, acc 0.921875, prec 0.0658125, recall 0.789894
2017-12-10T05:20:54.887024: step 2931, loss 0.26912, acc 0.984375, prec 0.06583, recall 0.789944
2017-12-10T05:20:55.147918: step 2932, loss 0.152341, acc 0.9375, prec 0.0658247, recall 0.789944
2017-12-10T05:20:55.419213: step 2933, loss 0.252365, acc 0.9375, prec 0.0658382, recall 0.789995
2017-12-10T05:20:55.678045: step 2934, loss 0.260513, acc 0.9375, prec 0.0658329, recall 0.789995
2017-12-10T05:20:55.940110: step 2935, loss 4.86396, acc 0.921875, prec 0.0658464, recall 0.789855
2017-12-10T05:20:56.205022: step 2936, loss 0.203644, acc 0.9375, prec 0.0658411, recall 0.789855
2017-12-10T05:20:56.474478: step 2937, loss 0.246606, acc 0.921875, prec 0.0658345, recall 0.789855
2017-12-10T05:20:56.739982: step 2938, loss 0.468287, acc 0.8125, prec 0.0658186, recall 0.789855
2017-12-10T05:20:57.000264: step 2939, loss 0.205277, acc 0.90625, prec 0.0658295, recall 0.789906
2017-12-10T05:20:57.272473: step 2940, loss 0.349387, acc 0.90625, prec 0.0658591, recall 0.790007
2017-12-10T05:20:57.540348: step 2941, loss 0.547076, acc 0.84375, prec 0.0658459, recall 0.790007
2017-12-10T05:20:57.802919: step 2942, loss 0.580104, acc 0.890625, prec 0.0658366, recall 0.790007
2017-12-10T05:20:58.067665: step 2943, loss 0.272597, acc 0.875, prec 0.065826, recall 0.790007
2017-12-10T05:20:58.330487: step 2944, loss 0.614482, acc 0.828125, prec 0.0658678, recall 0.790159
2017-12-10T05:20:58.593265: step 2945, loss 0.196103, acc 0.9375, prec 0.0658813, recall 0.79021
2017-12-10T05:20:58.859959: step 2946, loss 0.661114, acc 0.8125, prec 0.0658654, recall 0.79021
2017-12-10T05:20:59.129321: step 2947, loss 0.274363, acc 0.90625, prec 0.0658762, recall 0.79026
2017-12-10T05:20:59.399436: step 2948, loss 0.509198, acc 0.84375, prec 0.065863, recall 0.79026
2017-12-10T05:20:59.660119: step 2949, loss 0.184215, acc 0.921875, prec 0.0658564, recall 0.79026
2017-12-10T05:20:59.920808: step 2950, loss 0.242122, acc 0.90625, prec 0.0658484, recall 0.79026
2017-12-10T05:21:00.182469: step 2951, loss 0.300121, acc 0.921875, prec 0.0658606, recall 0.790311
2017-12-10T05:21:00.452075: step 2952, loss 0.0864726, acc 0.96875, prec 0.0658767, recall 0.790361
2017-12-10T05:21:00.726030: step 2953, loss 0.221084, acc 0.90625, prec 0.0658875, recall 0.790412
2017-12-10T05:21:00.989926: step 2954, loss 0.354673, acc 0.890625, prec 0.0658782, recall 0.790412
2017-12-10T05:21:01.253824: step 2955, loss 0.259882, acc 0.96875, prec 0.0659131, recall 0.790513
2017-12-10T05:21:01.517176: step 2956, loss 0.0737032, acc 0.984375, prec 0.0659118, recall 0.790513
2017-12-10T05:21:01.790445: step 2957, loss 0.0552939, acc 0.984375, prec 0.0659105, recall 0.790513
2017-12-10T05:21:02.054565: step 2958, loss 0.491305, acc 0.890625, prec 0.06592, recall 0.790563
2017-12-10T05:21:02.313931: step 2959, loss 0.0453972, acc 0.984375, prec 0.0659186, recall 0.790563
2017-12-10T05:21:02.577741: step 2960, loss 0.139002, acc 0.953125, prec 0.0659147, recall 0.790563
2017-12-10T05:21:02.858065: step 2961, loss 0.238422, acc 0.953125, prec 0.0659482, recall 0.790664
2017-12-10T05:21:03.127859: step 2962, loss 0.563502, acc 0.984375, prec 0.0659843, recall 0.790765
2017-12-10T05:21:03.398205: step 2963, loss 0.380868, acc 0.921875, prec 0.0660152, recall 0.790865
2017-12-10T05:21:03.665058: step 2964, loss 0.0925996, acc 0.984375, prec 0.0660514, recall 0.790966
2017-12-10T05:21:03.931078: step 2965, loss 0.0425145, acc 0.984375, prec 0.06605, recall 0.790966
2017-12-10T05:21:04.193738: step 2966, loss 0.0703878, acc 0.984375, prec 0.0660674, recall 0.791016
2017-12-10T05:21:04.468964: step 2967, loss 0.0858378, acc 0.953125, prec 0.0660635, recall 0.791016
2017-12-10T05:21:04.731815: step 2968, loss 0.239291, acc 0.921875, prec 0.0660943, recall 0.791116
2017-12-10T05:21:05.006977: step 2969, loss 0.0461989, acc 0.984375, prec 0.066093, recall 0.791116
2017-12-10T05:21:05.274950: step 2970, loss 0.0837452, acc 0.984375, prec 0.0660917, recall 0.791116
2017-12-10T05:21:05.542858: step 2971, loss 1.35538, acc 0.90625, prec 0.066085, recall 0.790927
2017-12-10T05:21:05.809244: step 2972, loss 0.195383, acc 0.921875, prec 0.0660784, recall 0.790927
2017-12-10T05:21:06.073420: step 2973, loss 0.220676, acc 0.96875, prec 0.0660945, recall 0.790977
2017-12-10T05:21:06.348021: step 2974, loss 0.0831174, acc 0.984375, prec 0.0661119, recall 0.791027
2017-12-10T05:21:06.620548: step 2975, loss 0.144486, acc 0.96875, prec 0.0661092, recall 0.791027
2017-12-10T05:21:06.883440: step 2976, loss 0.17811, acc 0.953125, prec 0.0661053, recall 0.791027
2017-12-10T05:21:07.149765: step 2977, loss 0.299538, acc 0.9375, prec 0.0661187, recall 0.791077
2017-12-10T05:21:07.416738: step 2978, loss 0.274994, acc 0.9375, prec 0.0661134, recall 0.791077
2017-12-10T05:21:07.682901: step 2979, loss 0.118491, acc 0.953125, prec 0.0661094, recall 0.791077
2017-12-10T05:21:07.945479: step 2980, loss 0.0793138, acc 0.96875, prec 0.0661255, recall 0.791127
2017-12-10T05:21:08.209023: step 2981, loss 0.13379, acc 0.96875, prec 0.0661415, recall 0.791177
2017-12-10T05:21:08.441344: step 2982, loss 6.61741, acc 0.942308, prec 0.0661402, recall 0.790798
2017-12-10T05:21:08.723444: step 2983, loss 0.0923068, acc 0.953125, prec 0.0661737, recall 0.790898
2017-12-10T05:21:08.994955: step 2984, loss 0.21028, acc 0.921875, prec 0.066167, recall 0.790898
2017-12-10T05:21:09.263862: step 2985, loss 0.144761, acc 0.90625, prec 0.0661591, recall 0.790898
2017-12-10T05:21:09.526593: step 2986, loss 0.68136, acc 0.796875, prec 0.0661606, recall 0.790948
2017-12-10T05:21:09.790234: step 2987, loss 0.439494, acc 0.875, prec 0.0661687, recall 0.790998
2017-12-10T05:21:10.047362: step 2988, loss 0.692106, acc 0.828125, prec 0.0661541, recall 0.790998
2017-12-10T05:21:10.316703: step 2989, loss 1.23792, acc 0.796875, prec 0.0661743, recall 0.791098
2017-12-10T05:21:10.587001: step 2990, loss 1.0611, acc 0.75, prec 0.0661904, recall 0.791198
2017-12-10T05:21:10.853458: step 2991, loss 0.830449, acc 0.796875, prec 0.0662106, recall 0.791298
2017-12-10T05:21:11.120596: step 2992, loss 0.596897, acc 0.734375, prec 0.0661881, recall 0.791298
2017-12-10T05:21:11.394016: step 2993, loss 0.60829, acc 0.875, prec 0.0662148, recall 0.791398
2017-12-10T05:21:11.656536: step 2994, loss 0.714722, acc 0.734375, prec 0.0661923, recall 0.791398
2017-12-10T05:21:11.924730: step 2995, loss 0.193447, acc 0.890625, prec 0.0661831, recall 0.791398
2017-12-10T05:21:12.193457: step 2996, loss 0.510741, acc 0.796875, prec 0.0661659, recall 0.791398
2017-12-10T05:21:12.463082: step 2997, loss 0.47141, acc 0.84375, prec 0.0661713, recall 0.791448
2017-12-10T05:21:12.728687: step 2998, loss 0.314992, acc 0.921875, prec 0.0661647, recall 0.791448
2017-12-10T05:21:12.987661: step 2999, loss 0.401856, acc 0.9375, prec 0.0661967, recall 0.791547
2017-12-10T05:21:13.251189: step 3000, loss 0.111043, acc 0.984375, prec 0.0661954, recall 0.791547

Evaluation:
2017-12-10T05:21:20.797614: step 3000, loss 2.18264, acc 0.908096, prec 0.0667306, recall 0.784941

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-3000

2017-12-10T05:21:22.109046: step 3001, loss 0.234077, acc 0.96875, prec 0.066728, recall 0.784941
2017-12-10T05:21:22.379543: step 3002, loss 0.309934, acc 0.890625, prec 0.0667189, recall 0.784941
2017-12-10T05:21:22.649807: step 3003, loss 0.054565, acc 0.984375, prec 0.0667541, recall 0.78504
2017-12-10T05:21:22.915626: step 3004, loss 0.0567847, acc 0.984375, prec 0.0667528, recall 0.78504
2017-12-10T05:21:23.182802: step 3005, loss 0.125567, acc 0.9375, prec 0.0667841, recall 0.785139
2017-12-10T05:21:23.456973: step 3006, loss 0.519341, acc 0.953125, prec 0.0667984, recall 0.785189
2017-12-10T05:21:23.730568: step 3007, loss 0.0676406, acc 0.96875, prec 0.0668141, recall 0.785238
2017-12-10T05:21:23.999063: step 3008, loss 0.224185, acc 0.953125, prec 0.0668466, recall 0.785337
2017-12-10T05:21:24.265745: step 3009, loss 0.506235, acc 0.953125, prec 0.0668792, recall 0.785435
2017-12-10T05:21:24.538833: step 3010, loss 0.0550798, acc 0.984375, prec 0.0668962, recall 0.785485
2017-12-10T05:21:24.798155: step 3011, loss 0.121807, acc 0.984375, prec 0.0669131, recall 0.785534
2017-12-10T05:21:25.061448: step 3012, loss 0.122751, acc 0.953125, prec 0.0669092, recall 0.785534
2017-12-10T05:21:25.327749: step 3013, loss 0.127512, acc 0.984375, prec 0.0669261, recall 0.785583
2017-12-10T05:21:25.594831: step 3014, loss 0.0287267, acc 1, prec 0.0669261, recall 0.785583
2017-12-10T05:21:25.873284: step 3015, loss 0.179702, acc 0.953125, prec 0.0669222, recall 0.785583
2017-12-10T05:21:26.138737: step 3016, loss 0.01473, acc 1, prec 0.0669222, recall 0.785583
2017-12-10T05:21:26.401727: step 3017, loss 0.0718815, acc 0.984375, prec 0.0669391, recall 0.785632
2017-12-10T05:21:26.668007: step 3018, loss 0.199896, acc 0.953125, prec 0.0669535, recall 0.785681
2017-12-10T05:21:26.928998: step 3019, loss 0.0106517, acc 1, prec 0.0669717, recall 0.785731
2017-12-10T05:21:27.202232: step 3020, loss 0.0357004, acc 0.984375, prec 0.0669704, recall 0.785731
2017-12-10T05:21:27.474156: step 3021, loss 0.143978, acc 0.9375, prec 0.0670016, recall 0.785829
2017-12-10T05:21:27.740195: step 3022, loss 0.459401, acc 0.921875, prec 0.067068, recall 0.786025
2017-12-10T05:21:28.004737: step 3023, loss 0.304799, acc 0.953125, prec 0.0671006, recall 0.786123
2017-12-10T05:21:28.277343: step 3024, loss 0.0691792, acc 0.96875, prec 0.067098, recall 0.786123
2017-12-10T05:21:28.547757: step 3025, loss 0.215703, acc 0.953125, prec 0.067094, recall 0.786123
2017-12-10T05:21:28.812773: step 3026, loss 0.0618989, acc 0.984375, prec 0.0670927, recall 0.786123
2017-12-10T05:21:29.078070: step 3027, loss 0.106199, acc 0.953125, prec 0.067107, recall 0.786172
2017-12-10T05:21:29.347715: step 3028, loss 0.155085, acc 0.96875, prec 0.0671226, recall 0.786221
2017-12-10T05:21:29.612235: step 3029, loss 0.00953999, acc 1, prec 0.0671408, recall 0.78627
2017-12-10T05:21:29.886523: step 3030, loss 0.0754023, acc 0.984375, prec 0.0671578, recall 0.786319
2017-12-10T05:21:30.153449: step 3031, loss 1.19457, acc 0.984375, prec 0.0671578, recall 0.786139
2017-12-10T05:21:30.432132: step 3032, loss 0.192372, acc 0.953125, prec 0.0671721, recall 0.786188
2017-12-10T05:21:30.706046: step 3033, loss 0.028517, acc 1, prec 0.0672085, recall 0.786286
2017-12-10T05:21:30.971359: step 3034, loss 0.163469, acc 0.96875, prec 0.0672059, recall 0.786286
2017-12-10T05:21:31.242257: step 3035, loss 0.0870862, acc 0.96875, prec 0.0672215, recall 0.786335
2017-12-10T05:21:31.508024: step 3036, loss 0.229515, acc 0.9375, prec 0.0672162, recall 0.786335
2017-12-10T05:21:31.772790: step 3037, loss 0.135562, acc 0.953125, prec 0.0672305, recall 0.786383
2017-12-10T05:21:32.034879: step 3038, loss 0.649979, acc 0.84375, prec 0.0672356, recall 0.786432
2017-12-10T05:21:32.303153: step 3039, loss 0.32547, acc 0.9375, prec 0.0672486, recall 0.786481
2017-12-10T05:21:32.566499: step 3040, loss 0.243565, acc 0.984375, prec 0.0672655, recall 0.78653
2017-12-10T05:21:32.826441: step 3041, loss 0.216666, acc 0.9375, prec 0.0672784, recall 0.786578
2017-12-10T05:21:33.090618: step 3042, loss 0.597611, acc 0.9375, prec 0.0673096, recall 0.786676
2017-12-10T05:21:33.353409: step 3043, loss 0.117401, acc 0.953125, prec 0.0673056, recall 0.786676
2017-12-10T05:21:33.620466: step 3044, loss 0.38831, acc 0.9375, prec 0.0673004, recall 0.786676
2017-12-10T05:21:33.887403: step 3045, loss 0.222993, acc 0.921875, prec 0.067312, recall 0.786724
2017-12-10T05:21:34.156290: step 3046, loss 0.482339, acc 0.90625, prec 0.0673405, recall 0.786822
2017-12-10T05:21:34.419693: step 3047, loss 0.0886661, acc 0.9375, prec 0.0673353, recall 0.786822
2017-12-10T05:21:34.689391: step 3048, loss 0.481044, acc 0.921875, prec 0.0673651, recall 0.786919
2017-12-10T05:21:34.956455: step 3049, loss 0.422269, acc 0.9375, prec 0.067378, recall 0.786967
2017-12-10T05:21:35.228938: step 3050, loss 0.444129, acc 0.921875, prec 0.0674078, recall 0.787064
2017-12-10T05:21:35.494676: step 3051, loss 0.303822, acc 0.9375, prec 0.0674026, recall 0.787064
2017-12-10T05:21:35.758024: step 3052, loss 0.10945, acc 0.9375, prec 0.0674155, recall 0.787113
2017-12-10T05:21:36.024662: step 3053, loss 0.154768, acc 0.90625, prec 0.0674076, recall 0.787113
2017-12-10T05:21:36.285582: step 3054, loss 0.352372, acc 0.9375, prec 0.0674387, recall 0.78721
2017-12-10T05:21:36.557544: step 3055, loss 0.129597, acc 0.96875, prec 0.0674543, recall 0.787258
2017-12-10T05:21:36.826169: step 3056, loss 0.527367, acc 0.890625, prec 0.0674633, recall 0.787307
2017-12-10T05:21:37.090678: step 3057, loss 0.230574, acc 0.953125, prec 0.0675138, recall 0.787452
2017-12-10T05:21:37.355822: step 3058, loss 0.0945383, acc 0.984375, prec 0.0675307, recall 0.7875
2017-12-10T05:21:37.624271: step 3059, loss 0.0904777, acc 0.96875, prec 0.0675281, recall 0.7875
2017-12-10T05:21:37.888419: step 3060, loss 0.12204, acc 0.953125, prec 0.0675241, recall 0.7875
2017-12-10T05:21:38.154805: step 3061, loss 0.0531061, acc 0.96875, prec 0.0675215, recall 0.7875
2017-12-10T05:21:38.419857: step 3062, loss 0.161625, acc 0.921875, prec 0.0675149, recall 0.7875
2017-12-10T05:21:38.689757: step 3063, loss 1.82036, acc 0.96875, prec 0.0675136, recall 0.787321
2017-12-10T05:21:38.956477: step 3064, loss 0.0221732, acc 1, prec 0.0675136, recall 0.787321
2017-12-10T05:21:39.216743: step 3065, loss 0.0147692, acc 1, prec 0.0675318, recall 0.787369
2017-12-10T05:21:39.493750: step 3066, loss 0.262202, acc 0.953125, prec 0.067546, recall 0.787418
2017-12-10T05:21:39.764864: step 3067, loss 0.0713465, acc 0.96875, prec 0.0675434, recall 0.787418
2017-12-10T05:21:40.028690: step 3068, loss 0.479167, acc 0.9375, prec 0.0675381, recall 0.787418
2017-12-10T05:21:40.299828: step 3069, loss 0.140619, acc 0.921875, prec 0.0675315, recall 0.787418
2017-12-10T05:21:40.556644: step 3070, loss 0.242335, acc 0.9375, prec 0.0675807, recall 0.787562
2017-12-10T05:21:40.820847: step 3071, loss 0.110004, acc 0.96875, prec 0.0676144, recall 0.787659
2017-12-10T05:21:41.078898: step 3072, loss 0.0421594, acc 0.984375, prec 0.0676313, recall 0.787707
2017-12-10T05:21:41.348028: step 3073, loss 0.042717, acc 0.984375, prec 0.0676662, recall 0.787803
2017-12-10T05:21:41.618510: step 3074, loss 0.38163, acc 0.90625, prec 0.0676946, recall 0.787899
2017-12-10T05:21:41.886470: step 3075, loss 0.159703, acc 0.921875, prec 0.0677062, recall 0.787947
2017-12-10T05:21:42.152080: step 3076, loss 3.68006, acc 0.953125, prec 0.0677217, recall 0.787817
2017-12-10T05:21:42.431027: step 3077, loss 0.175929, acc 0.96875, prec 0.0677372, recall 0.787865
2017-12-10T05:21:42.693213: step 3078, loss 0.170387, acc 0.96875, prec 0.0677346, recall 0.787865
2017-12-10T05:21:42.954524: step 3079, loss 0.198587, acc 0.9375, prec 0.0677475, recall 0.787913
2017-12-10T05:21:43.228260: step 3080, loss 0.381711, acc 0.890625, prec 0.0677382, recall 0.787913
2017-12-10T05:21:43.490815: step 3081, loss 0.322001, acc 0.90625, prec 0.0677847, recall 0.788057
2017-12-10T05:21:43.758590: step 3082, loss 0.243896, acc 0.9375, prec 0.0678339, recall 0.788201
2017-12-10T05:21:44.026446: step 3083, loss 0.50237, acc 0.84375, prec 0.0678207, recall 0.788201
2017-12-10T05:21:44.289292: step 3084, loss 0.417838, acc 0.90625, prec 0.0678309, recall 0.788249
2017-12-10T05:21:44.549199: step 3085, loss 0.416406, acc 0.890625, prec 0.0678217, recall 0.788249
2017-12-10T05:21:44.811156: step 3086, loss 0.574986, acc 0.84375, prec 0.0678628, recall 0.788392
2017-12-10T05:21:45.076046: step 3087, loss 0.306604, acc 0.9375, prec 0.0678938, recall 0.788488
2017-12-10T05:21:45.339513: step 3088, loss 0.35872, acc 0.890625, prec 0.0679027, recall 0.788535
2017-12-10T05:21:45.606064: step 3089, loss 0.265581, acc 0.921875, prec 0.0679142, recall 0.788583
2017-12-10T05:21:45.867806: step 3090, loss 0.187279, acc 0.953125, prec 0.0679464, recall 0.788678
2017-12-10T05:21:46.132835: step 3091, loss 0.263117, acc 0.953125, prec 0.0679606, recall 0.788726
2017-12-10T05:21:46.400819: step 3092, loss 0.223514, acc 0.9375, prec 0.0679915, recall 0.788821
2017-12-10T05:21:46.668248: step 3093, loss 0.0903224, acc 0.953125, prec 0.0680057, recall 0.788869
2017-12-10T05:21:46.929452: step 3094, loss 0.649436, acc 0.859375, prec 0.0679938, recall 0.788869
2017-12-10T05:21:47.201408: step 3095, loss 0.0395727, acc 0.984375, prec 0.0679925, recall 0.788869
2017-12-10T05:21:47.470440: step 3096, loss 0.373109, acc 0.921875, prec 0.0679859, recall 0.788869
2017-12-10T05:21:47.740419: step 3097, loss 0.218212, acc 0.921875, prec 0.0679974, recall 0.788916
2017-12-10T05:21:48.007307: step 3098, loss 0.36136, acc 0.953125, prec 0.0680115, recall 0.788964
2017-12-10T05:21:48.277878: step 3099, loss 0.201639, acc 0.921875, prec 0.0680049, recall 0.788964
2017-12-10T05:21:48.539792: step 3100, loss 0.150194, acc 0.953125, prec 0.0680009, recall 0.788964
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-3100

2017-12-10T05:21:49.766685: step 3101, loss 0.151062, acc 0.984375, prec 0.0680177, recall 0.789011
2017-12-10T05:21:50.039125: step 3102, loss 0.0767568, acc 0.96875, prec 0.0680151, recall 0.789011
2017-12-10T05:21:50.301966: step 3103, loss 0.209269, acc 0.953125, prec 0.0680292, recall 0.789059
2017-12-10T05:21:50.565100: step 3104, loss 0.44918, acc 0.90625, prec 0.0680213, recall 0.789059
2017-12-10T05:21:50.826674: step 3105, loss 0.259379, acc 0.953125, prec 0.0680173, recall 0.789059
2017-12-10T05:21:51.094037: step 3106, loss 0.0632502, acc 0.96875, prec 0.0680147, recall 0.789059
2017-12-10T05:21:51.355662: step 3107, loss 0.37389, acc 0.984375, prec 0.0680857, recall 0.789249
2017-12-10T05:21:51.631631: step 3108, loss 0.100496, acc 0.984375, prec 0.0681024, recall 0.789296
2017-12-10T05:21:51.900370: step 3109, loss 0.0252948, acc 0.984375, prec 0.0681192, recall 0.789344
2017-12-10T05:21:52.171391: step 3110, loss 0.040052, acc 0.96875, prec 0.0681166, recall 0.789344
2017-12-10T05:21:52.440609: step 3111, loss 1.06444, acc 1, prec 0.0681527, recall 0.789438
2017-12-10T05:21:52.713926: step 3112, loss 0.0942155, acc 0.96875, prec 0.0681501, recall 0.789438
2017-12-10T05:21:52.985129: step 3113, loss 0.09158, acc 0.984375, prec 0.0681668, recall 0.789486
2017-12-10T05:21:53.253102: step 3114, loss 0.123316, acc 0.984375, prec 0.0681836, recall 0.789533
2017-12-10T05:21:53.520117: step 3115, loss 0.130665, acc 0.9375, prec 0.0681964, recall 0.78958
2017-12-10T05:21:53.788988: step 3116, loss 0.325638, acc 0.984375, prec 0.0682131, recall 0.789627
2017-12-10T05:21:54.054918: step 3117, loss 0.0158229, acc 1, prec 0.0682312, recall 0.789675
2017-12-10T05:21:54.326756: step 3118, loss 0.0935973, acc 0.9375, prec 0.068244, recall 0.789722
2017-12-10T05:21:54.593971: step 3119, loss 0.184359, acc 0.953125, prec 0.06824, recall 0.789722
2017-12-10T05:21:54.867789: step 3120, loss 0.131303, acc 0.953125, prec 0.0682541, recall 0.789769
2017-12-10T05:21:55.151153: step 3121, loss 0.962194, acc 1, prec 0.0682722, recall 0.789816
2017-12-10T05:21:55.420087: step 3122, loss 0.169226, acc 0.953125, prec 0.0682682, recall 0.789816
2017-12-10T05:21:55.692707: step 3123, loss 0.170379, acc 0.96875, prec 0.0682836, recall 0.789863
2017-12-10T05:21:55.957083: step 3124, loss 0.112957, acc 0.953125, prec 0.0682796, recall 0.789863
2017-12-10T05:21:56.222412: step 3125, loss 0.303261, acc 0.96875, prec 0.068277, recall 0.789863
2017-12-10T05:21:56.495985: step 3126, loss 0.0778233, acc 1, prec 0.068295, recall 0.78991
2017-12-10T05:21:56.763647: step 3127, loss 0.122221, acc 0.953125, prec 0.0683091, recall 0.789957
2017-12-10T05:21:57.027156: step 3128, loss 0.210965, acc 0.90625, prec 0.0683192, recall 0.790004
2017-12-10T05:21:57.303776: step 3129, loss 0.0961568, acc 0.96875, prec 0.0683347, recall 0.790052
2017-12-10T05:21:57.573230: step 3130, loss 0.152194, acc 0.9375, prec 0.0683294, recall 0.790052
2017-12-10T05:21:57.843482: step 3131, loss 0.107967, acc 0.953125, prec 0.0683434, recall 0.790099
2017-12-10T05:21:58.110371: step 3132, loss 0.10951, acc 0.96875, prec 0.0683588, recall 0.790146
2017-12-10T05:21:58.374769: step 3133, loss 0.134058, acc 0.9375, prec 0.0683896, recall 0.79024
2017-12-10T05:21:58.648386: step 3134, loss 0.172773, acc 0.9375, prec 0.0684024, recall 0.790286
2017-12-10T05:21:58.916317: step 3135, loss 0.0439263, acc 0.984375, prec 0.0684191, recall 0.790333
2017-12-10T05:21:59.190181: step 3136, loss 0.190045, acc 0.953125, prec 0.0684512, recall 0.790427
2017-12-10T05:21:59.451174: step 3137, loss 0.162449, acc 0.953125, prec 0.0684833, recall 0.790521
2017-12-10T05:21:59.717072: step 3138, loss 0.115986, acc 0.96875, prec 0.0684987, recall 0.790568
2017-12-10T05:21:59.980348: step 3139, loss 0.161566, acc 0.953125, prec 0.0684947, recall 0.790568
2017-12-10T05:22:00.247342: step 3140, loss 0.269326, acc 0.953125, prec 0.0684908, recall 0.790568
2017-12-10T05:22:00.536837: step 3141, loss 0.150162, acc 0.9375, prec 0.0684855, recall 0.790568
2017-12-10T05:22:00.807948: step 3142, loss 0.104709, acc 0.921875, prec 0.0684788, recall 0.790568
2017-12-10T05:22:01.074398: step 3143, loss 0.45302, acc 0.921875, prec 0.0684722, recall 0.790568
2017-12-10T05:22:01.340830: step 3144, loss 0.128809, acc 0.953125, prec 0.0684682, recall 0.790568
2017-12-10T05:22:01.609623: step 3145, loss 0.08078, acc 0.96875, prec 0.0684656, recall 0.790568
2017-12-10T05:22:01.873424: step 3146, loss 0.592334, acc 0.96875, prec 0.068481, recall 0.790615
2017-12-10T05:22:02.145436: step 3147, loss 3.29207, acc 0.96875, prec 0.0684796, recall 0.790438
2017-12-10T05:22:02.416500: step 3148, loss 0.0320365, acc 1, prec 0.0685157, recall 0.790532
2017-12-10T05:22:02.695807: step 3149, loss 0.131683, acc 0.90625, prec 0.0685258, recall 0.790578
2017-12-10T05:22:02.959420: step 3150, loss 0.0757775, acc 0.96875, prec 0.0685231, recall 0.790578
2017-12-10T05:22:03.225032: step 3151, loss 0.166842, acc 0.984375, prec 0.0685759, recall 0.790718
2017-12-10T05:22:03.492480: step 3152, loss 0.0717985, acc 0.96875, prec 0.0685732, recall 0.790718
2017-12-10T05:22:03.758025: step 3153, loss 0.156604, acc 0.921875, prec 0.0685846, recall 0.790765
2017-12-10T05:22:04.026559: step 3154, loss 0.211573, acc 0.9375, prec 0.0685973, recall 0.790812
2017-12-10T05:22:04.295746: step 3155, loss 0.106167, acc 0.9375, prec 0.068592, recall 0.790812
2017-12-10T05:22:04.565486: step 3156, loss 2.00567, acc 0.9375, prec 0.068606, recall 0.790682
2017-12-10T05:22:04.833150: step 3157, loss 0.164588, acc 0.9375, prec 0.0686367, recall 0.790775
2017-12-10T05:22:05.098103: step 3158, loss 0.268535, acc 0.90625, prec 0.0686648, recall 0.790869
2017-12-10T05:22:05.365222: step 3159, loss 0.319854, acc 0.921875, prec 0.0686582, recall 0.790869
2017-12-10T05:22:05.631128: step 3160, loss 0.396937, acc 0.890625, prec 0.0686489, recall 0.790869
2017-12-10T05:22:05.904766: step 3161, loss 0.611536, acc 0.828125, prec 0.0686343, recall 0.790869
2017-12-10T05:22:06.177780: step 3162, loss 0.382573, acc 0.859375, prec 0.0686223, recall 0.790869
2017-12-10T05:22:06.442943: step 3163, loss 0.423975, acc 0.890625, prec 0.0686311, recall 0.790915
2017-12-10T05:22:06.703003: step 3164, loss 0.277094, acc 0.859375, prec 0.0686551, recall 0.791008
2017-12-10T05:22:06.965348: step 3165, loss 0.373245, acc 0.8125, prec 0.0686752, recall 0.791101
2017-12-10T05:22:07.229209: step 3166, loss 0.445519, acc 0.90625, prec 0.0686672, recall 0.791101
2017-12-10T05:22:07.498927: step 3167, loss 0.846105, acc 0.859375, prec 0.0686912, recall 0.791194
2017-12-10T05:22:07.772474: step 3168, loss 0.394975, acc 0.84375, prec 0.068678, recall 0.791194
2017-12-10T05:22:08.035987: step 3169, loss 0.0487418, acc 0.984375, prec 0.0686767, recall 0.791194
2017-12-10T05:22:08.299526: step 3170, loss 0.307475, acc 0.890625, prec 0.0686674, recall 0.791194
2017-12-10T05:22:08.566485: step 3171, loss 0.525528, acc 0.78125, prec 0.0686488, recall 0.791194
2017-12-10T05:22:08.833525: step 3172, loss 0.371705, acc 0.890625, prec 0.0686575, recall 0.791241
2017-12-10T05:22:09.096056: step 3173, loss 0.395853, acc 0.890625, prec 0.0686662, recall 0.791287
2017-12-10T05:22:09.357594: step 3174, loss 0.269254, acc 0.921875, prec 0.0686596, recall 0.791287
2017-12-10T05:22:09.622649: step 3175, loss 0.377034, acc 0.875, prec 0.068649, recall 0.791287
2017-12-10T05:22:09.890199: step 3176, loss 0.0212014, acc 1, prec 0.068649, recall 0.791287
2017-12-10T05:22:10.163332: step 3177, loss 0.0783387, acc 0.96875, prec 0.0686464, recall 0.791287
2017-12-10T05:22:10.430956: step 3178, loss 0.437031, acc 0.890625, prec 0.068655, recall 0.791333
2017-12-10T05:22:10.696571: step 3179, loss 0.0420413, acc 1, prec 0.068691, recall 0.791426
2017-12-10T05:22:10.960814: step 3180, loss 0.275136, acc 0.921875, prec 0.0687023, recall 0.791472
2017-12-10T05:22:11.232181: step 3181, loss 0.042827, acc 0.984375, prec 0.0687189, recall 0.791519
2017-12-10T05:22:11.503949: step 3182, loss 0.0326439, acc 0.984375, prec 0.0687176, recall 0.791519
2017-12-10T05:22:11.781078: step 3183, loss 0.150396, acc 0.96875, prec 0.0687329, recall 0.791565
2017-12-10T05:22:12.056117: step 3184, loss 0.0268366, acc 0.984375, prec 0.0687495, recall 0.791611
2017-12-10T05:22:12.335027: step 3185, loss 0.0992045, acc 0.96875, prec 0.0687469, recall 0.791611
2017-12-10T05:22:12.598433: step 3186, loss 0.0489315, acc 0.96875, prec 0.0687801, recall 0.791704
2017-12-10T05:22:12.864121: step 3187, loss 0.533269, acc 1, prec 0.068816, recall 0.791796
2017-12-10T05:22:13.139816: step 3188, loss 0.00233635, acc 1, prec 0.0688339, recall 0.791842
2017-12-10T05:22:13.412689: step 3189, loss 0.0200592, acc 1, prec 0.0688698, recall 0.791934
2017-12-10T05:22:13.677991: step 3190, loss 0.646992, acc 0.984375, prec 0.0689044, recall 0.792027
2017-12-10T05:22:13.946904: step 3191, loss 0.889093, acc 0.984375, prec 0.068921, recall 0.792073
2017-12-10T05:22:14.225454: step 3192, loss 0.0826618, acc 0.984375, prec 0.0689376, recall 0.792119
2017-12-10T05:22:14.492283: step 3193, loss 0.102193, acc 0.96875, prec 0.0689708, recall 0.792211
2017-12-10T05:22:14.759505: step 3194, loss 0.112739, acc 0.9375, prec 0.0689835, recall 0.792257
2017-12-10T05:22:15.021088: step 3195, loss 0.144886, acc 0.984375, prec 0.0690001, recall 0.792303
2017-12-10T05:22:15.283039: step 3196, loss 0.233418, acc 0.953125, prec 0.0690319, recall 0.792394
2017-12-10T05:22:15.544396: step 3197, loss 0.170335, acc 0.921875, prec 0.0690253, recall 0.792394
2017-12-10T05:22:15.810156: step 3198, loss 0.20111, acc 0.96875, prec 0.0690585, recall 0.792486
2017-12-10T05:22:16.074125: step 3199, loss 0.571474, acc 0.875, prec 0.0691016, recall 0.792624
2017-12-10T05:22:16.340652: step 3200, loss 0.370993, acc 0.90625, prec 0.0690936, recall 0.792624
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-3200

2017-12-10T05:22:17.601205: step 3201, loss 0.352366, acc 0.90625, prec 0.0690857, recall 0.792624
2017-12-10T05:22:17.867589: step 3202, loss 0.254823, acc 0.921875, prec 0.0691148, recall 0.792715
2017-12-10T05:22:18.133565: step 3203, loss 0.265376, acc 0.921875, prec 0.0691619, recall 0.792852
2017-12-10T05:22:18.394583: step 3204, loss 0.525094, acc 0.953125, prec 0.0692117, recall 0.792989
2017-12-10T05:22:18.661023: step 3205, loss 0.434792, acc 0.859375, prec 0.0692176, recall 0.793035
2017-12-10T05:22:18.924639: step 3206, loss 0.177911, acc 0.9375, prec 0.0692123, recall 0.793035
2017-12-10T05:22:19.187720: step 3207, loss 0.460194, acc 0.828125, prec 0.0691976, recall 0.793035
2017-12-10T05:22:19.448888: step 3208, loss 0.318443, acc 0.9375, prec 0.0691923, recall 0.793035
2017-12-10T05:22:19.719117: step 3209, loss 0.173305, acc 0.9375, prec 0.069187, recall 0.793035
2017-12-10T05:22:19.984335: step 3210, loss 0.252341, acc 0.875, prec 0.0691763, recall 0.793035
2017-12-10T05:22:20.259524: step 3211, loss 0.152772, acc 0.921875, prec 0.0691876, recall 0.793081
2017-12-10T05:22:20.524432: step 3212, loss 0.391969, acc 0.9375, prec 0.0692002, recall 0.793126
2017-12-10T05:22:20.796859: step 3213, loss 0.0763196, acc 0.96875, prec 0.0692154, recall 0.793172
2017-12-10T05:22:21.057737: step 3214, loss 0.229005, acc 0.953125, prec 0.0692293, recall 0.793217
2017-12-10T05:22:21.325930: step 3215, loss 0.338332, acc 0.90625, prec 0.0692213, recall 0.793217
2017-12-10T05:22:21.599323: step 3216, loss 0.299735, acc 0.90625, prec 0.0692133, recall 0.793217
2017-12-10T05:22:21.868393: step 3217, loss 0.0836285, acc 0.984375, prec 0.0692299, recall 0.793263
2017-12-10T05:22:22.134232: step 3218, loss 1.60125, acc 0.921875, prec 0.0692424, recall 0.793134
2017-12-10T05:22:22.403138: step 3219, loss 0.163285, acc 0.953125, prec 0.0692385, recall 0.793134
2017-12-10T05:22:22.671172: step 3220, loss 0.137324, acc 0.953125, prec 0.0692523, recall 0.793179
2017-12-10T05:22:22.929532: step 3221, loss 0.442805, acc 0.859375, prec 0.0692404, recall 0.793179
2017-12-10T05:22:23.190991: step 3222, loss 0.0622088, acc 0.96875, prec 0.0692556, recall 0.793225
2017-12-10T05:22:23.457840: step 3223, loss 0.0228961, acc 1, prec 0.0692556, recall 0.793225
2017-12-10T05:22:23.718881: step 3224, loss 0.122264, acc 0.953125, prec 0.0692873, recall 0.793316
2017-12-10T05:22:23.982071: step 3225, loss 0.257126, acc 0.9375, prec 0.0692999, recall 0.793361
2017-12-10T05:22:24.253642: step 3226, loss 0.133357, acc 0.9375, prec 0.0692946, recall 0.793361
2017-12-10T05:22:24.525119: step 3227, loss 0.267055, acc 0.9375, prec 0.0693429, recall 0.793497
2017-12-10T05:22:24.786919: step 3228, loss 0.200528, acc 0.953125, prec 0.0693567, recall 0.793543
2017-12-10T05:22:25.048291: step 3229, loss 0.0120109, acc 1, prec 0.0693925, recall 0.793633
2017-12-10T05:22:25.327452: step 3230, loss 0.0136707, acc 1, prec 0.0694103, recall 0.793679
2017-12-10T05:22:25.602432: step 3231, loss 0.105394, acc 0.921875, prec 0.0694037, recall 0.793679
2017-12-10T05:22:25.866527: step 3232, loss 0.197612, acc 0.953125, prec 0.0694175, recall 0.793724
2017-12-10T05:22:26.131657: step 3233, loss 0.0706663, acc 0.984375, prec 0.069434, recall 0.793769
2017-12-10T05:22:26.397977: step 3234, loss 0.023235, acc 1, prec 0.069434, recall 0.793769
2017-12-10T05:22:26.662059: step 3235, loss 0.105724, acc 0.953125, prec 0.0694301, recall 0.793769
2017-12-10T05:22:26.924221: step 3236, loss 0.651564, acc 0.953125, prec 0.0694439, recall 0.793814
2017-12-10T05:22:27.196910: step 3237, loss 0.0201331, acc 1, prec 0.0694618, recall 0.79386
2017-12-10T05:22:27.466793: step 3238, loss 0.0603268, acc 0.984375, prec 0.0694961, recall 0.79395
2017-12-10T05:22:27.734321: step 3239, loss 0.256872, acc 0.953125, prec 0.06951, recall 0.793995
2017-12-10T05:22:28.001737: step 3240, loss 0.125141, acc 0.96875, prec 0.0695073, recall 0.793995
2017-12-10T05:22:28.271034: step 3241, loss 0.129334, acc 1, prec 0.069543, recall 0.794085
2017-12-10T05:22:28.535025: step 3242, loss 0.0321665, acc 0.984375, prec 0.0695595, recall 0.794131
2017-12-10T05:22:28.809174: step 3243, loss 0.0650805, acc 0.953125, prec 0.0695734, recall 0.794176
2017-12-10T05:22:29.070402: step 3244, loss 0.220355, acc 0.96875, prec 0.0695886, recall 0.794221
2017-12-10T05:22:29.337396: step 3245, loss 0.114294, acc 0.953125, prec 0.0696024, recall 0.794266
2017-12-10T05:22:29.609351: step 3246, loss 0.0617315, acc 0.96875, prec 0.0695997, recall 0.794266
2017-12-10T05:22:29.876897: step 3247, loss 0.276464, acc 0.90625, prec 0.0695917, recall 0.794266
2017-12-10T05:22:30.150489: step 3248, loss 0.0785719, acc 0.984375, prec 0.0696082, recall 0.794311
2017-12-10T05:22:30.429256: step 3249, loss 0.0893467, acc 0.9375, prec 0.0696386, recall 0.794401
2017-12-10T05:22:30.703582: step 3250, loss 0.376243, acc 0.953125, prec 0.0696702, recall 0.794491
2017-12-10T05:22:30.975040: step 3251, loss 1.87113, acc 0.96875, prec 0.0696689, recall 0.794317
2017-12-10T05:22:31.242234: step 3252, loss 0.292747, acc 0.953125, prec 0.0697006, recall 0.794407
2017-12-10T05:22:31.507006: step 3253, loss 0.210425, acc 0.9375, prec 0.0697131, recall 0.794452
2017-12-10T05:22:31.772685: step 3254, loss 0.117913, acc 0.9375, prec 0.0697255, recall 0.794497
2017-12-10T05:22:32.037101: step 3255, loss 0.188794, acc 0.9375, prec 0.0697202, recall 0.794497
2017-12-10T05:22:32.297981: step 3256, loss 0.199761, acc 0.9375, prec 0.0697149, recall 0.794497
2017-12-10T05:22:32.570961: step 3257, loss 0.385194, acc 0.890625, prec 0.0697055, recall 0.794497
2017-12-10T05:22:32.843112: step 3258, loss 0.283346, acc 0.890625, prec 0.0696962, recall 0.794497
2017-12-10T05:22:33.111846: step 3259, loss 0.319725, acc 0.921875, prec 0.0697073, recall 0.794541
2017-12-10T05:22:33.370611: step 3260, loss 0.299212, acc 0.90625, prec 0.0696993, recall 0.794541
2017-12-10T05:22:33.634285: step 3261, loss 0.204653, acc 0.96875, prec 0.0697144, recall 0.794586
2017-12-10T05:22:33.901963: step 3262, loss 0.097442, acc 0.953125, prec 0.0697282, recall 0.794631
2017-12-10T05:22:34.168787: step 3263, loss 0.276206, acc 0.9375, prec 0.0697407, recall 0.794676
2017-12-10T05:22:34.438768: step 3264, loss 0.180175, acc 0.921875, prec 0.0697697, recall 0.794766
2017-12-10T05:22:34.706630: step 3265, loss 0.366075, acc 0.96875, prec 0.0698204, recall 0.7949
2017-12-10T05:22:34.969886: step 3266, loss 0.128962, acc 0.953125, prec 0.069852, recall 0.794989
2017-12-10T05:22:35.234939: step 3267, loss 0.422552, acc 0.90625, prec 0.0698796, recall 0.795078
2017-12-10T05:22:35.508558: step 3268, loss 0.292321, acc 0.90625, prec 0.0698894, recall 0.795123
2017-12-10T05:22:35.771908: step 3269, loss 0.199931, acc 0.9375, prec 0.069884, recall 0.795123
2017-12-10T05:22:36.036424: step 3270, loss 0.302218, acc 0.953125, prec 0.06988, recall 0.795123
2017-12-10T05:22:36.302138: step 3271, loss 0.668189, acc 0.953125, prec 0.0699294, recall 0.795257
2017-12-10T05:22:36.574449: step 3272, loss 0.103065, acc 0.953125, prec 0.0699254, recall 0.795257
2017-12-10T05:22:36.842207: step 3273, loss 0.303584, acc 0.90625, prec 0.0699529, recall 0.795346
2017-12-10T05:22:37.107406: step 3274, loss 0.0951048, acc 0.984375, prec 0.0699694, recall 0.79539
2017-12-10T05:22:37.370980: step 3275, loss 1.39129, acc 0.96875, prec 0.0700379, recall 0.795568
2017-12-10T05:22:37.640547: step 3276, loss 0.12032, acc 0.984375, prec 0.0700899, recall 0.795701
2017-12-10T05:22:37.898634: step 3277, loss 0.0302341, acc 1, prec 0.0701077, recall 0.795746
2017-12-10T05:22:38.163353: step 3278, loss 0.186086, acc 0.9375, prec 0.0701023, recall 0.795746
2017-12-10T05:22:38.435593: step 3279, loss 0.27592, acc 0.953125, prec 0.0701338, recall 0.795834
2017-12-10T05:22:38.700075: step 3280, loss 0.298742, acc 0.90625, prec 0.0701258, recall 0.795834
2017-12-10T05:22:38.962918: step 3281, loss 0.362595, acc 0.859375, prec 0.0701315, recall 0.795879
2017-12-10T05:22:39.233718: step 3282, loss 0.0447573, acc 0.984375, prec 0.0701302, recall 0.795879
2017-12-10T05:22:39.502834: step 3283, loss 0.173955, acc 0.921875, prec 0.0701412, recall 0.795923
2017-12-10T05:22:39.775686: step 3284, loss 0.42184, acc 0.890625, prec 0.0701496, recall 0.795967
2017-12-10T05:22:40.043366: step 3285, loss 0.262401, acc 0.9375, prec 0.0701443, recall 0.795967
2017-12-10T05:22:40.309442: step 3286, loss 0.0929564, acc 0.96875, prec 0.0701771, recall 0.796055
2017-12-10T05:22:40.572910: step 3287, loss 0.0751286, acc 0.96875, prec 0.0701744, recall 0.796055
2017-12-10T05:22:40.847044: step 3288, loss 0.29548, acc 0.921875, prec 0.0702033, recall 0.796144
2017-12-10T05:22:41.108632: step 3289, loss 0.234885, acc 0.9375, prec 0.0701979, recall 0.796144
2017-12-10T05:22:41.370824: step 3290, loss 0.0807078, acc 0.984375, prec 0.0701966, recall 0.796144
2017-12-10T05:22:41.635773: step 3291, loss 0.251685, acc 0.9375, prec 0.0701912, recall 0.796144
2017-12-10T05:22:41.900805: step 3292, loss 0.426611, acc 0.921875, prec 0.07022, recall 0.796232
2017-12-10T05:22:42.171454: step 3293, loss 0.0231386, acc 1, prec 0.0702555, recall 0.79632
2017-12-10T05:22:42.439946: step 3294, loss 0.0318976, acc 0.984375, prec 0.0702542, recall 0.79632
2017-12-10T05:22:42.706200: step 3295, loss 0.387356, acc 0.890625, prec 0.0702625, recall 0.796364
2017-12-10T05:22:42.978359: step 3296, loss 0.0255572, acc 1, prec 0.070298, recall 0.796453
2017-12-10T05:22:43.241551: step 3297, loss 0.0903924, acc 0.984375, prec 0.0703144, recall 0.796497
2017-12-10T05:22:43.504907: step 3298, loss 0.0332632, acc 0.984375, prec 0.0703131, recall 0.796497
2017-12-10T05:22:43.770482: step 3299, loss 3.48583, acc 0.953125, prec 0.0703104, recall 0.796324
2017-12-10T05:22:44.040666: step 3300, loss 0.100569, acc 0.96875, prec 0.0703077, recall 0.796324

Evaluation:
2017-12-10T05:22:51.644441: step 3300, loss 3.29342, acc 0.950462, prec 0.0708721, recall 0.7841

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-3300

2017-12-10T05:22:52.986084: step 3301, loss 0.063263, acc 0.984375, prec 0.0708883, recall 0.784146
2017-12-10T05:22:53.253322: step 3302, loss 0.110232, acc 0.953125, prec 0.0708843, recall 0.784146
2017-12-10T05:22:53.518418: step 3303, loss 0.136137, acc 0.953125, prec 0.0708978, recall 0.784191
2017-12-10T05:22:53.784743: step 3304, loss 0.585231, acc 0.90625, prec 0.0709074, recall 0.784236
2017-12-10T05:22:54.054688: step 3305, loss 0.305799, acc 0.921875, prec 0.0709358, recall 0.784326
2017-12-10T05:22:54.327914: step 3306, loss 0.326188, acc 0.9375, prec 0.0709831, recall 0.784461
2017-12-10T05:22:54.596368: step 3307, loss 0.636659, acc 0.921875, prec 0.0710115, recall 0.784551
2017-12-10T05:22:54.862095: step 3308, loss 0.326467, acc 0.9375, prec 0.0710061, recall 0.784551
2017-12-10T05:22:55.124932: step 3309, loss 0.127274, acc 0.953125, prec 0.0710021, recall 0.784551
2017-12-10T05:22:55.392891: step 3310, loss 0.473422, acc 0.890625, prec 0.0710103, recall 0.784596
2017-12-10T05:22:55.659833: step 3311, loss 0.56405, acc 0.875, prec 0.0710522, recall 0.784731
2017-12-10T05:22:55.934132: step 3312, loss 0.242722, acc 0.953125, prec 0.0710832, recall 0.784821
2017-12-10T05:22:56.202653: step 3313, loss 0.157208, acc 0.921875, prec 0.0711116, recall 0.78491
2017-12-10T05:22:56.473453: step 3314, loss 0.538558, acc 0.859375, prec 0.0711346, recall 0.785
2017-12-10T05:22:56.739075: step 3315, loss 0.222348, acc 0.953125, prec 0.0711656, recall 0.78509
2017-12-10T05:22:57.004771: step 3316, loss 0.988414, acc 0.84375, prec 0.0711873, recall 0.785179
2017-12-10T05:22:57.277231: step 3317, loss 0.352777, acc 0.9375, prec 0.0711994, recall 0.785224
2017-12-10T05:22:57.551740: step 3318, loss 0.534988, acc 0.828125, prec 0.0712022, recall 0.785268
2017-12-10T05:22:57.815378: step 3319, loss 0.288456, acc 0.90625, prec 0.0712116, recall 0.785313
2017-12-10T05:22:58.084069: step 3320, loss 0.272732, acc 0.953125, prec 0.0712251, recall 0.785358
2017-12-10T05:22:58.348803: step 3321, loss 0.146407, acc 0.9375, prec 0.0712373, recall 0.785402
2017-12-10T05:22:58.615275: step 3322, loss 0.385286, acc 0.90625, prec 0.0712818, recall 0.785536
2017-12-10T05:22:58.887603: step 3323, loss 0.299193, acc 0.9375, prec 0.0712939, recall 0.785581
2017-12-10T05:22:59.154323: step 3324, loss 0.142999, acc 0.9375, prec 0.0712885, recall 0.785581
2017-12-10T05:22:59.420243: step 3325, loss 0.201663, acc 0.9375, prec 0.0712831, recall 0.785581
2017-12-10T05:22:59.683689: step 3326, loss 0.359931, acc 0.9375, prec 0.0712953, recall 0.785625
2017-12-10T05:22:59.947540: step 3327, loss 0.0419541, acc 1, prec 0.0713128, recall 0.78567
2017-12-10T05:23:00.215491: step 3328, loss 0.10668, acc 0.96875, prec 0.0713276, recall 0.785714
2017-12-10T05:23:00.482097: step 3329, loss 0.221169, acc 0.96875, prec 0.0713249, recall 0.785714
2017-12-10T05:23:00.755598: step 3330, loss 0.195916, acc 0.96875, prec 0.0713222, recall 0.785714
2017-12-10T05:23:01.026896: step 3331, loss 0.355395, acc 0.96875, prec 0.071337, recall 0.785759
2017-12-10T05:23:01.293112: step 3332, loss 0.274632, acc 0.953125, prec 0.071333, recall 0.785759
2017-12-10T05:23:01.559896: step 3333, loss 0.0343534, acc 0.984375, prec 0.0713492, recall 0.785803
2017-12-10T05:23:01.822810: step 3334, loss 0.343908, acc 0.9375, prec 0.0713613, recall 0.785848
2017-12-10T05:23:02.087728: step 3335, loss 0.235812, acc 0.9375, prec 0.0713909, recall 0.785937
2017-12-10T05:23:02.361458: step 3336, loss 0.0285099, acc 0.96875, prec 0.0714232, recall 0.786025
2017-12-10T05:23:02.628838: step 3337, loss 0.140621, acc 0.96875, prec 0.071438, recall 0.78607
2017-12-10T05:23:02.889543: step 3338, loss 0.0843582, acc 0.96875, prec 0.0714528, recall 0.786114
2017-12-10T05:23:03.158950: step 3339, loss 0.029985, acc 0.984375, prec 0.0714514, recall 0.786114
2017-12-10T05:23:03.422362: step 3340, loss 0.0825991, acc 0.953125, prec 0.0714649, recall 0.786158
2017-12-10T05:23:03.694351: step 3341, loss 0.0702713, acc 0.984375, prec 0.0714636, recall 0.786158
2017-12-10T05:23:03.958685: step 3342, loss 0.134339, acc 0.96875, prec 0.0714609, recall 0.786158
2017-12-10T05:23:04.226999: step 3343, loss 0.0117199, acc 1, prec 0.0714609, recall 0.786158
2017-12-10T05:23:04.493795: step 3344, loss 0.0575608, acc 0.96875, prec 0.0714757, recall 0.786203
2017-12-10T05:23:04.758689: step 3345, loss 0.0985406, acc 0.96875, prec 0.071473, recall 0.786203
2017-12-10T05:23:05.025094: step 3346, loss 0.015009, acc 1, prec 0.071473, recall 0.786203
2017-12-10T05:23:05.300528: step 3347, loss 0.0488042, acc 1, prec 0.0714905, recall 0.786247
2017-12-10T05:23:05.575429: step 3348, loss 0.21699, acc 1, prec 0.0715079, recall 0.786291
2017-12-10T05:23:05.843761: step 3349, loss 0.0224873, acc 0.984375, prec 0.0715066, recall 0.786291
2017-12-10T05:23:06.110754: step 3350, loss 0.159751, acc 0.953125, prec 0.07152, recall 0.786335
2017-12-10T05:23:06.375308: step 3351, loss 0.005209, acc 1, prec 0.07152, recall 0.786335
2017-12-10T05:23:06.648682: step 3352, loss 0.136676, acc 0.96875, prec 0.0715348, recall 0.78638
2017-12-10T05:23:06.927701: step 3353, loss 0.0286636, acc 0.984375, prec 0.0715335, recall 0.78638
2017-12-10T05:23:07.195043: step 3354, loss 0.178671, acc 0.96875, prec 0.0715483, recall 0.786424
2017-12-10T05:23:07.467595: step 3355, loss 0.0560333, acc 0.96875, prec 0.0715456, recall 0.786424
2017-12-10T05:23:07.737243: step 3356, loss 0.0505535, acc 0.96875, prec 0.0715429, recall 0.786424
2017-12-10T05:23:08.002598: step 3357, loss 0.0640168, acc 0.96875, prec 0.0715577, recall 0.786468
2017-12-10T05:23:08.273945: step 3358, loss 3.4067, acc 0.9375, prec 0.0715711, recall 0.78635
2017-12-10T05:23:08.545800: step 3359, loss 1.35675, acc 0.96875, prec 0.0715872, recall 0.786231
2017-12-10T05:23:08.824282: step 3360, loss 0.0238411, acc 1, prec 0.0715872, recall 0.786231
2017-12-10T05:23:09.091323: step 3361, loss 0.0720228, acc 1, prec 0.0716222, recall 0.786319
2017-12-10T05:23:09.359036: step 3362, loss 0.207741, acc 0.9375, prec 0.0716517, recall 0.786408
2017-12-10T05:23:09.631118: step 3363, loss 0.591707, acc 0.859375, prec 0.0716396, recall 0.786408
2017-12-10T05:23:09.894027: step 3364, loss 0.174026, acc 0.921875, prec 0.0716329, recall 0.786408
2017-12-10T05:23:10.158686: step 3365, loss 0.559719, acc 0.875, prec 0.0716745, recall 0.78654
2017-12-10T05:23:10.427354: step 3366, loss 0.617117, acc 0.859375, prec 0.0716973, recall 0.786628
2017-12-10T05:23:10.689023: step 3367, loss 0.292911, acc 0.96875, prec 0.0716946, recall 0.786628
2017-12-10T05:23:10.955522: step 3368, loss 0.443905, acc 0.90625, prec 0.0717388, recall 0.78676
2017-12-10T05:23:11.217829: step 3369, loss 0.278131, acc 0.90625, prec 0.0717307, recall 0.78676
2017-12-10T05:23:11.480181: step 3370, loss 0.554087, acc 0.84375, prec 0.0717522, recall 0.786848
2017-12-10T05:23:11.744718: step 3371, loss 0.549216, acc 0.84375, prec 0.0717736, recall 0.786936
2017-12-10T05:23:12.009804: step 3372, loss 0.29443, acc 0.828125, prec 0.0717936, recall 0.787024
2017-12-10T05:23:12.277892: step 3373, loss 0.266329, acc 0.9375, prec 0.0718231, recall 0.787111
2017-12-10T05:23:12.548956: step 3374, loss 0.416091, acc 0.875, prec 0.0718123, recall 0.787111
2017-12-10T05:23:12.819776: step 3375, loss 0.342116, acc 0.90625, prec 0.0718216, recall 0.787155
2017-12-10T05:23:13.078678: step 3376, loss 0.651392, acc 0.8125, prec 0.0718403, recall 0.787243
2017-12-10T05:23:13.345228: step 3377, loss 0.304296, acc 0.921875, prec 0.071851, recall 0.787287
2017-12-10T05:23:13.609272: step 3378, loss 0.269791, acc 0.9375, prec 0.0718805, recall 0.787374
2017-12-10T05:23:13.880189: step 3379, loss 0.204111, acc 0.90625, prec 0.0718724, recall 0.787374
2017-12-10T05:23:14.142738: step 3380, loss 0.311736, acc 0.875, prec 0.0718616, recall 0.787374
2017-12-10T05:23:14.408308: step 3381, loss 0.70877, acc 0.921875, prec 0.0718722, recall 0.787418
2017-12-10T05:23:14.674287: step 3382, loss 0.114931, acc 0.9375, prec 0.0718668, recall 0.787418
2017-12-10T05:23:14.939027: step 3383, loss 0.140474, acc 0.9375, prec 0.0718615, recall 0.787418
2017-12-10T05:23:15.206147: step 3384, loss 0.327774, acc 0.921875, prec 0.0718547, recall 0.787418
2017-12-10T05:23:15.463284: step 3385, loss 0.0575531, acc 0.984375, prec 0.0718534, recall 0.787418
2017-12-10T05:23:15.730824: step 3386, loss 0.0618908, acc 0.96875, prec 0.0718507, recall 0.787418
2017-12-10T05:23:16.001089: step 3387, loss 0.0869242, acc 0.984375, prec 0.0718493, recall 0.787418
2017-12-10T05:23:16.265788: step 3388, loss 0.170348, acc 0.9375, prec 0.0718613, recall 0.787461
2017-12-10T05:23:16.532898: step 3389, loss 0.00422629, acc 1, prec 0.0718613, recall 0.787461
2017-12-10T05:23:16.805128: step 3390, loss 0.187521, acc 0.96875, prec 0.0718586, recall 0.787461
2017-12-10T05:23:17.072856: step 3391, loss 0.0162648, acc 0.984375, prec 0.0718573, recall 0.787461
2017-12-10T05:23:17.340845: step 3392, loss 0.0133395, acc 1, prec 0.0718747, recall 0.787505
2017-12-10T05:23:17.612919: step 3393, loss 0.128481, acc 1, prec 0.0718921, recall 0.787549
2017-12-10T05:23:17.881119: step 3394, loss 7.46812, acc 0.984375, prec 0.0719443, recall 0.787518
2017-12-10T05:23:18.151914: step 3395, loss 0.103029, acc 0.984375, prec 0.0719778, recall 0.787605
2017-12-10T05:23:18.417057: step 3396, loss 0.0793793, acc 0.96875, prec 0.0720273, recall 0.787736
2017-12-10T05:23:18.683157: step 3397, loss 0.167328, acc 0.921875, prec 0.0720206, recall 0.787736
2017-12-10T05:23:18.946993: step 3398, loss 0.0903919, acc 0.953125, prec 0.0720165, recall 0.787736
2017-12-10T05:23:19.213752: step 3399, loss 0.266857, acc 0.9375, prec 0.0720285, recall 0.787779
2017-12-10T05:23:19.480393: step 3400, loss 0.0327821, acc 0.984375, prec 0.0720271, recall 0.787779
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-3400

2017-12-10T05:23:20.834190: step 3401, loss 0.103151, acc 0.984375, prec 0.0720432, recall 0.787823
2017-12-10T05:23:21.099393: step 3402, loss 0.153583, acc 0.9375, prec 0.0720378, recall 0.787823
2017-12-10T05:23:21.364556: step 3403, loss 0.229262, acc 0.9375, prec 0.0720672, recall 0.78791
2017-12-10T05:23:21.634646: step 3404, loss 0.229751, acc 0.953125, prec 0.0720805, recall 0.787953
2017-12-10T05:23:21.903754: step 3405, loss 0.395756, acc 0.84375, prec 0.072067, recall 0.787953
2017-12-10T05:23:22.175900: step 3406, loss 0.190994, acc 0.90625, prec 0.0720589, recall 0.787953
2017-12-10T05:23:22.438032: step 3407, loss 0.468591, acc 0.875, prec 0.0720481, recall 0.787953
2017-12-10T05:23:22.703683: step 3408, loss 0.715951, acc 0.875, prec 0.0720547, recall 0.787997
2017-12-10T05:23:22.973625: step 3409, loss 0.425005, acc 0.953125, prec 0.0720854, recall 0.788084
2017-12-10T05:23:23.241850: step 3410, loss 0.238467, acc 0.921875, prec 0.0721134, recall 0.78817
2017-12-10T05:23:23.506531: step 3411, loss 0.311509, acc 0.890625, prec 0.072104, recall 0.78817
2017-12-10T05:23:23.767377: step 3412, loss 0.256769, acc 0.90625, prec 0.0721306, recall 0.788257
2017-12-10T05:23:24.037769: step 3413, loss 0.622428, acc 0.953125, prec 0.0721439, recall 0.7883
2017-12-10T05:23:24.305901: step 3414, loss 0.247034, acc 0.921875, prec 0.0721719, recall 0.788387
2017-12-10T05:23:24.569600: step 3415, loss 0.168131, acc 0.9375, prec 0.0721665, recall 0.788387
2017-12-10T05:23:24.831461: step 3416, loss 0.0467443, acc 1, prec 0.0722186, recall 0.788517
2017-12-10T05:23:25.103681: step 3417, loss 0.330919, acc 0.921875, prec 0.0722118, recall 0.788517
2017-12-10T05:23:25.372765: step 3418, loss 0.0765002, acc 0.96875, prec 0.0722265, recall 0.78856
2017-12-10T05:23:25.637637: step 3419, loss 0.0664723, acc 0.96875, prec 0.0722238, recall 0.78856
2017-12-10T05:23:25.901786: step 3420, loss 0.0240837, acc 1, prec 0.0722411, recall 0.788603
2017-12-10T05:23:26.176439: step 3421, loss 0.0593276, acc 0.984375, prec 0.0722398, recall 0.788603
2017-12-10T05:23:26.450684: step 3422, loss 0.319069, acc 0.953125, prec 0.0722704, recall 0.788689
2017-12-10T05:23:26.727632: step 3423, loss 0.222009, acc 0.921875, prec 0.072281, recall 0.788732
2017-12-10T05:23:26.992725: step 3424, loss 0.112952, acc 0.984375, prec 0.0722797, recall 0.788732
2017-12-10T05:23:27.265766: step 3425, loss 0.0559237, acc 0.96875, prec 0.072277, recall 0.788732
2017-12-10T05:23:27.529556: step 3426, loss 0.149814, acc 0.96875, prec 0.0722916, recall 0.788776
2017-12-10T05:23:27.801266: step 3427, loss 0.145832, acc 0.96875, prec 0.0723063, recall 0.788819
2017-12-10T05:23:28.070211: step 3428, loss 4.31411, acc 0.984375, prec 0.0723236, recall 0.788701
2017-12-10T05:23:28.339510: step 3429, loss 0.182336, acc 0.953125, prec 0.0723369, recall 0.788744
2017-12-10T05:23:28.602214: step 3430, loss 0.02045, acc 1, prec 0.0723369, recall 0.788744
2017-12-10T05:23:28.867030: step 3431, loss 0.218568, acc 0.921875, prec 0.0723302, recall 0.788744
2017-12-10T05:23:29.132502: step 3432, loss 0.181134, acc 0.9375, prec 0.0723248, recall 0.788744
2017-12-10T05:23:29.396557: step 3433, loss 1.39792, acc 0.90625, prec 0.0723513, recall 0.78883
2017-12-10T05:23:29.671579: step 3434, loss 0.310584, acc 0.875, prec 0.0723405, recall 0.78883
2017-12-10T05:23:29.942228: step 3435, loss 0.195601, acc 0.9375, prec 0.0723698, recall 0.788916
2017-12-10T05:23:30.208922: step 3436, loss 0.373607, acc 0.890625, prec 0.0723603, recall 0.788916
2017-12-10T05:23:30.478747: step 3437, loss 0.568178, acc 0.875, prec 0.0723495, recall 0.788916
2017-12-10T05:23:30.747499: step 3438, loss 0.426726, acc 0.890625, prec 0.0723574, recall 0.788959
2017-12-10T05:23:31.013455: step 3439, loss 0.395042, acc 0.828125, prec 0.0723598, recall 0.789002
2017-12-10T05:23:31.274982: step 3440, loss 0.419584, acc 0.84375, prec 0.0723463, recall 0.789002
2017-12-10T05:23:31.534742: step 3441, loss 0.87878, acc 0.828125, prec 0.0723488, recall 0.789045
2017-12-10T05:23:31.807682: step 3442, loss 0.478658, acc 0.84375, prec 0.0723353, recall 0.789045
2017-12-10T05:23:32.073423: step 3443, loss 0.499886, acc 0.78125, prec 0.0723164, recall 0.789045
2017-12-10T05:23:32.338641: step 3444, loss 0.432688, acc 0.8125, prec 0.0723002, recall 0.789045
2017-12-10T05:23:32.601486: step 3445, loss 0.237437, acc 0.9375, prec 0.0723121, recall 0.789088
2017-12-10T05:23:32.873561: step 3446, loss 0.779564, acc 0.828125, prec 0.0722972, recall 0.789088
2017-12-10T05:23:33.142981: step 3447, loss 0.361983, acc 0.875, prec 0.0723038, recall 0.789131
2017-12-10T05:23:33.405565: step 3448, loss 0.334864, acc 0.890625, prec 0.0722943, recall 0.789131
2017-12-10T05:23:33.673104: step 3449, loss 0.117564, acc 0.953125, prec 0.0722903, recall 0.789131
2017-12-10T05:23:33.936895: step 3450, loss 0.116097, acc 0.96875, prec 0.0722876, recall 0.789131
2017-12-10T05:23:34.199993: step 3451, loss 0.0756644, acc 0.96875, prec 0.0722849, recall 0.789131
2017-12-10T05:23:34.466157: step 3452, loss 0.298332, acc 0.953125, prec 0.0723154, recall 0.789217
2017-12-10T05:23:34.737049: step 3453, loss 0.0993711, acc 0.984375, prec 0.072366, recall 0.789345
2017-12-10T05:23:35.002969: step 3454, loss 0.0276146, acc 0.984375, prec 0.0723646, recall 0.789345
2017-12-10T05:23:35.268696: step 3455, loss 0.334468, acc 0.96875, prec 0.0723965, recall 0.789431
2017-12-10T05:23:35.541965: step 3456, loss 2.16715, acc 0.953125, prec 0.0724111, recall 0.789313
2017-12-10T05:23:35.807198: step 3457, loss 0.140524, acc 0.9375, prec 0.072423, recall 0.789356
2017-12-10T05:23:36.078179: step 3458, loss 0.287444, acc 0.953125, prec 0.0724535, recall 0.789442
2017-12-10T05:23:36.341598: step 3459, loss 0.188266, acc 0.984375, prec 0.0724867, recall 0.789527
2017-12-10T05:23:36.606898: step 3460, loss 0.0793366, acc 0.953125, prec 0.0725, recall 0.78957
2017-12-10T05:23:36.878440: step 3461, loss 0.348577, acc 0.890625, prec 0.0724905, recall 0.78957
2017-12-10T05:23:37.137649: step 3462, loss 0.0291674, acc 0.984375, prec 0.0724891, recall 0.78957
2017-12-10T05:23:37.402556: step 3463, loss 0.595342, acc 0.890625, prec 0.0725143, recall 0.789655
2017-12-10T05:23:37.666575: step 3464, loss 0.201168, acc 0.953125, prec 0.0725447, recall 0.78974
2017-12-10T05:23:37.934204: step 3465, loss 0.159717, acc 0.96875, prec 0.072542, recall 0.78974
2017-12-10T05:23:38.198541: step 3466, loss 0.189419, acc 0.953125, prec 0.0725725, recall 0.789826
2017-12-10T05:23:38.466609: step 3467, loss 0.0953203, acc 0.953125, prec 0.072603, recall 0.789911
2017-12-10T05:23:38.741342: step 3468, loss 0.198869, acc 0.921875, prec 0.0725963, recall 0.789911
2017-12-10T05:23:39.007102: step 3469, loss 0.212316, acc 0.953125, prec 0.0725922, recall 0.789911
2017-12-10T05:23:39.272009: step 3470, loss 0.14536, acc 0.984375, prec 0.0725909, recall 0.789911
2017-12-10T05:23:39.537483: step 3471, loss 0.368135, acc 0.921875, prec 0.0726186, recall 0.789996
2017-12-10T05:23:39.803767: step 3472, loss 0.290693, acc 0.9375, prec 0.0726132, recall 0.789996
2017-12-10T05:23:40.077233: step 3473, loss 0.438298, acc 0.921875, prec 0.0726065, recall 0.789996
2017-12-10T05:23:40.344765: step 3474, loss 0.226333, acc 0.953125, prec 0.0726197, recall 0.790038
2017-12-10T05:23:40.612393: step 3475, loss 0.249042, acc 0.9375, prec 0.0726315, recall 0.790081
2017-12-10T05:23:40.876740: step 3476, loss 0.17021, acc 0.953125, prec 0.072662, recall 0.790166
2017-12-10T05:23:41.151010: step 3477, loss 0.274404, acc 0.953125, prec 0.0726752, recall 0.790208
2017-12-10T05:23:41.415587: step 3478, loss 0.0761528, acc 0.984375, prec 0.0726738, recall 0.790208
2017-12-10T05:23:41.649498: step 3479, loss 0.135931, acc 0.961538, prec 0.0727229, recall 0.790336
2017-12-10T05:23:41.927540: step 3480, loss 0.268241, acc 0.96875, prec 0.0727374, recall 0.790378
2017-12-10T05:23:42.207609: step 3481, loss 0.0963091, acc 0.96875, prec 0.0727865, recall 0.790505
2017-12-10T05:23:42.476001: step 3482, loss 0.247911, acc 0.921875, prec 0.0727797, recall 0.790505
2017-12-10T05:23:42.744422: step 3483, loss 0.0627545, acc 0.96875, prec 0.072777, recall 0.790505
2017-12-10T05:23:43.011442: step 3484, loss 3.28107, acc 0.96875, prec 0.0728274, recall 0.790472
2017-12-10T05:23:43.287152: step 3485, loss 0.565795, acc 0.921875, prec 0.0728206, recall 0.790472
2017-12-10T05:23:43.552928: step 3486, loss 0.0808415, acc 0.953125, prec 0.0728165, recall 0.790472
2017-12-10T05:23:43.818483: step 3487, loss 0.112259, acc 0.96875, prec 0.0728311, recall 0.790515
2017-12-10T05:23:44.094254: step 3488, loss 0.0292748, acc 0.984375, prec 0.0728469, recall 0.790557
2017-12-10T05:23:44.367703: step 3489, loss 0.445438, acc 0.875, prec 0.0728361, recall 0.790557
2017-12-10T05:23:44.641770: step 3490, loss 0.0721217, acc 0.953125, prec 0.072832, recall 0.790557
2017-12-10T05:23:44.907768: step 3491, loss 0.245208, acc 0.90625, prec 0.0728412, recall 0.790599
2017-12-10T05:23:45.171515: step 3492, loss 0.145835, acc 0.953125, prec 0.0728371, recall 0.790599
2017-12-10T05:23:45.432560: step 3493, loss 0.383961, acc 0.90625, prec 0.0728634, recall 0.790684
2017-12-10T05:23:45.696453: step 3494, loss 0.397859, acc 0.875, prec 0.0728698, recall 0.790726
2017-12-10T05:23:45.962292: step 3495, loss 0.328524, acc 0.90625, prec 0.0728617, recall 0.790726
2017-12-10T05:23:46.222251: step 3496, loss 0.481272, acc 0.9375, prec 0.0728735, recall 0.790768
2017-12-10T05:23:46.495421: step 3497, loss 0.412373, acc 0.921875, prec 0.0728667, recall 0.790768
2017-12-10T05:23:46.761018: step 3498, loss 0.328966, acc 0.9375, prec 0.0728613, recall 0.790768
2017-12-10T05:23:47.033063: step 3499, loss 0.606188, acc 0.90625, prec 0.0728532, recall 0.790768
2017-12-10T05:23:47.295556: step 3500, loss 0.0674668, acc 0.953125, prec 0.0728664, recall 0.79081
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-3500

2017-12-10T05:23:48.548997: step 3501, loss 0.331039, acc 0.953125, prec 0.0728623, recall 0.79081
2017-12-10T05:23:48.815834: step 3502, loss 0.394633, acc 0.9375, prec 0.0728741, recall 0.790852
2017-12-10T05:23:49.083119: step 3503, loss 0.185163, acc 0.9375, prec 0.0729031, recall 0.790937
2017-12-10T05:23:49.350814: step 3504, loss 0.213415, acc 0.921875, prec 0.0729136, recall 0.790979
2017-12-10T05:23:49.622408: step 3505, loss 0.0458149, acc 0.984375, prec 0.0729122, recall 0.790979
2017-12-10T05:23:49.884238: step 3506, loss 0.12557, acc 0.96875, prec 0.0729439, recall 0.791063
2017-12-10T05:23:50.154187: step 3507, loss 0.0329314, acc 1, prec 0.0729439, recall 0.791063
2017-12-10T05:23:50.425251: step 3508, loss 0.232173, acc 0.921875, prec 0.0729372, recall 0.791063
2017-12-10T05:23:50.696439: step 3509, loss 0.0421632, acc 0.984375, prec 0.072953, recall 0.791105
2017-12-10T05:23:50.962967: step 3510, loss 0.0335515, acc 1, prec 0.072953, recall 0.791105
2017-12-10T05:23:51.238180: step 3511, loss 0.0353629, acc 1, prec 0.072953, recall 0.791105
2017-12-10T05:23:51.507587: step 3512, loss 0.206653, acc 0.984375, prec 0.0729517, recall 0.791105
2017-12-10T05:23:51.771373: step 3513, loss 0.00503998, acc 1, prec 0.0729517, recall 0.791105
2017-12-10T05:23:52.028862: step 3514, loss 0.305523, acc 0.984375, prec 0.0729847, recall 0.791189
2017-12-10T05:23:52.293666: step 3515, loss 0.316466, acc 0.984375, prec 0.0730006, recall 0.791231
2017-12-10T05:23:52.568713: step 3516, loss 0.292238, acc 0.984375, prec 0.0730508, recall 0.791357
2017-12-10T05:23:52.835969: step 3517, loss 0.00794205, acc 1, prec 0.0730852, recall 0.791441
2017-12-10T05:23:53.098700: step 3518, loss 0.0726458, acc 0.96875, prec 0.0730825, recall 0.791441
2017-12-10T05:23:53.371633: step 3519, loss 0.671087, acc 0.984375, prec 0.0730983, recall 0.791483
2017-12-10T05:23:53.642090: step 3520, loss 0.0557582, acc 0.984375, prec 0.073097, recall 0.791483
2017-12-10T05:23:53.910316: step 3521, loss 1.11722, acc 0.953125, prec 0.0731273, recall 0.791566
2017-12-10T05:23:54.176752: step 3522, loss 0.194225, acc 0.9375, prec 0.0731219, recall 0.791566
2017-12-10T05:23:54.442874: step 3523, loss 0.15327, acc 0.953125, prec 0.0731178, recall 0.791566
2017-12-10T05:23:54.707302: step 3524, loss 0.252254, acc 0.953125, prec 0.0731481, recall 0.79165
2017-12-10T05:23:54.970678: step 3525, loss 0.126036, acc 0.984375, prec 0.0731468, recall 0.79165
2017-12-10T05:23:55.236720: step 3526, loss 0.0938159, acc 0.96875, prec 0.0731956, recall 0.791775
2017-12-10T05:23:55.504464: step 3527, loss 0.079516, acc 0.953125, prec 0.0731915, recall 0.791775
2017-12-10T05:23:55.771272: step 3528, loss 0.254284, acc 0.921875, prec 0.0732191, recall 0.791859
2017-12-10T05:23:56.041643: step 3529, loss 0.245082, acc 0.9375, prec 0.0732137, recall 0.791859
2017-12-10T05:23:56.303674: step 3530, loss 0.0981732, acc 0.953125, prec 0.0732268, recall 0.791901
2017-12-10T05:23:56.567490: step 3531, loss 0.211216, acc 0.96875, prec 0.0732241, recall 0.791901
2017-12-10T05:23:56.832918: step 3532, loss 0.241218, acc 0.921875, prec 0.0732345, recall 0.791942
2017-12-10T05:23:57.101834: step 3533, loss 0.0938755, acc 0.96875, prec 0.0732489, recall 0.791984
2017-12-10T05:23:57.379260: step 3534, loss 0.248074, acc 0.953125, prec 0.073262, recall 0.792026
2017-12-10T05:23:57.645577: step 3535, loss 0.327315, acc 0.90625, prec 0.0732711, recall 0.792067
2017-12-10T05:23:57.909718: step 3536, loss 0.0602813, acc 0.984375, prec 0.0733041, recall 0.792151
2017-12-10T05:23:58.181919: step 3537, loss 0.116576, acc 0.984375, prec 0.0733027, recall 0.792151
2017-12-10T05:23:58.450146: step 3538, loss 0.352919, acc 0.9375, prec 0.0732973, recall 0.792151
2017-12-10T05:23:58.718023: step 3539, loss 0.155877, acc 0.921875, prec 0.0733248, recall 0.792234
2017-12-10T05:23:58.981723: step 3540, loss 0.147824, acc 0.953125, prec 0.0733207, recall 0.792234
2017-12-10T05:23:59.255870: step 3541, loss 0.275011, acc 0.9375, prec 0.0733496, recall 0.792317
2017-12-10T05:23:59.523481: step 3542, loss 0.19211, acc 0.96875, prec 0.0733641, recall 0.792358
2017-12-10T05:23:59.789871: step 3543, loss 0.202542, acc 0.9375, prec 0.0733758, recall 0.7924
2017-12-10T05:24:00.057072: step 3544, loss 0.195257, acc 0.953125, prec 0.0733889, recall 0.792441
2017-12-10T05:24:00.326684: step 3545, loss 0.133737, acc 0.984375, prec 0.0733875, recall 0.792441
2017-12-10T05:24:00.599471: step 3546, loss 0.31102, acc 0.953125, prec 0.0734006, recall 0.792483
2017-12-10T05:24:00.869032: step 3547, loss 0.0869788, acc 0.96875, prec 0.0734494, recall 0.792607
2017-12-10T05:24:01.134117: step 3548, loss 0.0190165, acc 1, prec 0.0734494, recall 0.792607
2017-12-10T05:24:01.397846: step 3549, loss 0.454603, acc 0.9375, prec 0.0734782, recall 0.79269
2017-12-10T05:24:01.664596: step 3550, loss 0.305406, acc 0.984375, prec 0.073494, recall 0.792732
2017-12-10T05:24:01.929555: step 3551, loss 0.0466116, acc 0.984375, prec 0.0734927, recall 0.792732
2017-12-10T05:24:02.199908: step 3552, loss 0.106879, acc 0.953125, prec 0.0734886, recall 0.792732
2017-12-10T05:24:02.466028: step 3553, loss 0.0160969, acc 0.984375, prec 0.0734872, recall 0.792732
2017-12-10T05:24:02.733849: step 3554, loss 0.225984, acc 0.984375, prec 0.0735202, recall 0.792814
2017-12-10T05:24:03.010324: step 3555, loss 0.0753793, acc 0.953125, prec 0.0735161, recall 0.792814
2017-12-10T05:24:03.283241: step 3556, loss 0.0802503, acc 0.953125, prec 0.073512, recall 0.792814
2017-12-10T05:24:03.552822: step 3557, loss 0.00231687, acc 1, prec 0.073512, recall 0.792814
2017-12-10T05:24:03.819424: step 3558, loss 0.0471323, acc 0.984375, prec 0.0735278, recall 0.792856
2017-12-10T05:24:04.086285: step 3559, loss 0.200548, acc 0.96875, prec 0.0735593, recall 0.792938
2017-12-10T05:24:04.360204: step 3560, loss 3.76398, acc 0.984375, prec 0.0735765, recall 0.792822
2017-12-10T05:24:04.630254: step 3561, loss 0.349166, acc 0.984375, prec 0.0735923, recall 0.792863
2017-12-10T05:24:04.899618: step 3562, loss 0.184022, acc 0.96875, prec 0.0736067, recall 0.792904
2017-12-10T05:24:05.176495: step 3563, loss 0.0886957, acc 0.96875, prec 0.0736382, recall 0.792987
2017-12-10T05:24:05.438231: step 3564, loss 0.412289, acc 0.96875, prec 0.0737041, recall 0.793151
2017-12-10T05:24:05.702864: step 3565, loss 0.12939, acc 0.984375, prec 0.0737027, recall 0.793151
2017-12-10T05:24:05.965698: step 3566, loss 0.516473, acc 0.890625, prec 0.0737103, recall 0.793193
2017-12-10T05:24:06.230383: step 3567, loss 0.717917, acc 0.90625, prec 0.0737535, recall 0.793316
2017-12-10T05:24:06.495392: step 3568, loss 0.19439, acc 0.90625, prec 0.0737453, recall 0.793316
2017-12-10T05:24:06.769625: step 3569, loss 0.397553, acc 0.921875, prec 0.0737728, recall 0.793398
2017-12-10T05:24:07.032283: step 3570, loss 0.228562, acc 0.953125, prec 0.0737858, recall 0.793439
2017-12-10T05:24:07.297735: step 3571, loss 0.369976, acc 0.921875, prec 0.073779, recall 0.793439
2017-12-10T05:24:07.567411: step 3572, loss 0.734695, acc 0.859375, prec 0.0737838, recall 0.79348
2017-12-10T05:24:07.834872: step 3573, loss 0.222953, acc 0.984375, prec 0.0737996, recall 0.793521
2017-12-10T05:24:08.096704: step 3574, loss 0.655789, acc 0.84375, prec 0.0737859, recall 0.793521
2017-12-10T05:24:08.360536: step 3575, loss 0.644612, acc 0.828125, prec 0.0737709, recall 0.793521
2017-12-10T05:24:08.625956: step 3576, loss 0.644666, acc 0.84375, prec 0.0738086, recall 0.793644
2017-12-10T05:24:08.890862: step 3577, loss 0.255683, acc 0.875, prec 0.0738319, recall 0.793726
2017-12-10T05:24:09.154568: step 3578, loss 0.57498, acc 0.828125, prec 0.073817, recall 0.793726
2017-12-10T05:24:09.415310: step 3579, loss 0.6062, acc 0.84375, prec 0.0738033, recall 0.793726
2017-12-10T05:24:09.678570: step 3580, loss 0.0718068, acc 0.96875, prec 0.0738177, recall 0.793767
2017-12-10T05:24:09.949692: step 3581, loss 0.232515, acc 0.9375, prec 0.0738122, recall 0.793767
2017-12-10T05:24:10.218410: step 3582, loss 0.122378, acc 0.953125, prec 0.0738082, recall 0.793767
2017-12-10T05:24:10.483465: step 3583, loss 0.212057, acc 0.90625, prec 0.0738, recall 0.793767
2017-12-10T05:24:10.751591: step 3584, loss 0.100592, acc 0.953125, prec 0.0738301, recall 0.793849
2017-12-10T05:24:11.017653: step 3585, loss 0.119024, acc 0.953125, prec 0.073826, recall 0.793849
2017-12-10T05:24:11.292776: step 3586, loss 0.111016, acc 0.953125, prec 0.0738219, recall 0.793849
2017-12-10T05:24:11.557134: step 3587, loss 0.0157119, acc 1, prec 0.0738219, recall 0.793849
2017-12-10T05:24:11.816235: step 3588, loss 0.0244266, acc 1, prec 0.073839, recall 0.79389
2017-12-10T05:24:12.079411: step 3589, loss 0.291504, acc 0.921875, prec 0.0738322, recall 0.79389
2017-12-10T05:24:12.356864: step 3590, loss 4.82886, acc 0.9375, prec 0.0738452, recall 0.793774
2017-12-10T05:24:12.625484: step 3591, loss 0.00249051, acc 1, prec 0.0738452, recall 0.793774
2017-12-10T05:24:12.896569: step 3592, loss 0.217734, acc 0.953125, prec 0.0738411, recall 0.793774
2017-12-10T05:24:13.163498: step 3593, loss 0.237662, acc 0.953125, prec 0.0738712, recall 0.793855
2017-12-10T05:24:13.442010: step 3594, loss 0.181699, acc 0.953125, prec 0.0738842, recall 0.793896
2017-12-10T05:24:13.705435: step 3595, loss 0.736941, acc 0.859375, prec 0.073889, recall 0.793937
2017-12-10T05:24:13.966820: step 3596, loss 0.438931, acc 0.890625, prec 0.0738965, recall 0.793978
2017-12-10T05:24:14.233691: step 3597, loss 0.242884, acc 0.921875, prec 0.0739068, recall 0.794019
2017-12-10T05:24:14.500877: step 3598, loss 0.244297, acc 0.9375, prec 0.0739355, recall 0.7941
2017-12-10T05:24:14.765529: step 3599, loss 0.362032, acc 0.9375, prec 0.0739812, recall 0.794222
2017-12-10T05:24:15.029561: step 3600, loss 0.236237, acc 0.921875, prec 0.0739744, recall 0.794222

Evaluation:
2017-12-10T05:24:22.635548: step 3600, loss 2.35812, acc 0.914607, prec 0.0743586, recall 0.787867

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-3600

2017-12-10T05:24:23.976649: step 3601, loss 0.13103, acc 0.953125, prec 0.0743713, recall 0.787908
2017-12-10T05:24:24.241426: step 3602, loss 0.573263, acc 0.921875, prec 0.0743814, recall 0.787949
2017-12-10T05:24:24.508448: step 3603, loss 0.428203, acc 0.875, prec 0.0743873, recall 0.787989
2017-12-10T05:24:24.777004: step 3604, loss 0.359382, acc 0.921875, prec 0.0744141, recall 0.788071
2017-12-10T05:24:25.048797: step 3605, loss 0.652673, acc 0.953125, prec 0.0744604, recall 0.788192
2017-12-10T05:24:25.319472: step 3606, loss 0.209222, acc 0.96875, prec 0.0744577, recall 0.788192
2017-12-10T05:24:25.583995: step 3607, loss 0.180009, acc 0.953125, prec 0.0744536, recall 0.788192
2017-12-10T05:24:25.843741: step 3608, loss 0.270242, acc 0.953125, prec 0.0744831, recall 0.788274
2017-12-10T05:24:26.109270: step 3609, loss 0.598769, acc 0.84375, prec 0.0744696, recall 0.788274
2017-12-10T05:24:26.377138: step 3610, loss 0.191817, acc 0.90625, prec 0.0744615, recall 0.788274
2017-12-10T05:24:26.637697: step 3611, loss 0.342843, acc 0.9375, prec 0.0744896, recall 0.788355
2017-12-10T05:24:26.904454: step 3612, loss 0.210941, acc 0.921875, prec 0.0744829, recall 0.788355
2017-12-10T05:24:27.179977: step 3613, loss 0.287603, acc 0.921875, prec 0.0744929, recall 0.788395
2017-12-10T05:24:27.450600: step 3614, loss 0.449415, acc 0.875, prec 0.0744821, recall 0.788395
2017-12-10T05:24:27.712010: step 3615, loss 0.240332, acc 0.953125, prec 0.0745116, recall 0.788476
2017-12-10T05:24:27.986570: step 3616, loss 0.310903, acc 0.921875, prec 0.0745048, recall 0.788476
2017-12-10T05:24:28.259733: step 3617, loss 0.217667, acc 0.953125, prec 0.0745008, recall 0.788476
2017-12-10T05:24:28.524743: step 3618, loss 0.128189, acc 0.953125, prec 0.0744968, recall 0.788476
2017-12-10T05:24:28.792830: step 3619, loss 0.0164314, acc 1, prec 0.0744968, recall 0.788476
2017-12-10T05:24:29.059690: step 3620, loss 0.211422, acc 0.953125, prec 0.0745094, recall 0.788517
2017-12-10T05:24:29.324909: step 3621, loss 0.0543996, acc 0.96875, prec 0.0745402, recall 0.788598
2017-12-10T05:24:29.596005: step 3622, loss 0.0219377, acc 1, prec 0.0745402, recall 0.788598
2017-12-10T05:24:29.868791: step 3623, loss 0.0517255, acc 0.96875, prec 0.0745375, recall 0.788598
2017-12-10T05:24:30.135665: step 3624, loss 1.8868, acc 0.984375, prec 0.0745375, recall 0.788447
2017-12-10T05:24:30.409794: step 3625, loss 0.116784, acc 0.96875, prec 0.0745348, recall 0.788447
2017-12-10T05:24:30.682132: step 3626, loss 0.172421, acc 0.921875, prec 0.0745448, recall 0.788487
2017-12-10T05:24:30.950746: step 3627, loss 0.115831, acc 0.96875, prec 0.0745589, recall 0.788528
2017-12-10T05:24:31.213869: step 3628, loss 0.112784, acc 0.96875, prec 0.0745729, recall 0.788568
2017-12-10T05:24:31.479057: step 3629, loss 0.0935805, acc 0.9375, prec 0.0745675, recall 0.788568
2017-12-10T05:24:31.746085: step 3630, loss 0.0950267, acc 0.96875, prec 0.0745648, recall 0.788568
2017-12-10T05:24:32.010569: step 3631, loss 0.372524, acc 0.96875, prec 0.0745621, recall 0.788568
2017-12-10T05:24:32.283004: step 3632, loss 0.253719, acc 0.9375, prec 0.0745734, recall 0.788609
2017-12-10T05:24:32.549539: step 3633, loss 0.151357, acc 0.9375, prec 0.0745848, recall 0.788649
2017-12-10T05:24:32.816936: step 3634, loss 0.225456, acc 0.9375, prec 0.0746128, recall 0.78873
2017-12-10T05:24:33.083653: step 3635, loss 0.261568, acc 0.9375, prec 0.0746242, recall 0.78877
2017-12-10T05:24:33.358327: step 3636, loss 0.169655, acc 0.9375, prec 0.0746689, recall 0.788891
2017-12-10T05:24:33.637488: step 3637, loss 0.271417, acc 0.890625, prec 0.0746762, recall 0.788931
2017-12-10T05:24:33.908994: step 3638, loss 0.247509, acc 0.890625, prec 0.0747002, recall 0.789012
2017-12-10T05:24:34.183587: step 3639, loss 0.115326, acc 0.9375, prec 0.0747115, recall 0.789052
2017-12-10T05:24:34.452275: step 3640, loss 0.201995, acc 0.953125, prec 0.0747075, recall 0.789052
2017-12-10T05:24:34.715741: step 3641, loss 0.122432, acc 0.9375, prec 0.0747522, recall 0.789173
2017-12-10T05:24:34.984418: step 3642, loss 0.0157692, acc 1, prec 0.0747689, recall 0.789213
2017-12-10T05:24:35.246330: step 3643, loss 0.143899, acc 0.96875, prec 0.0747996, recall 0.789293
2017-12-10T05:24:35.510911: step 3644, loss 0.0643486, acc 0.96875, prec 0.0747969, recall 0.789293
2017-12-10T05:24:35.779762: step 3645, loss 0.0826844, acc 0.96875, prec 0.0747942, recall 0.789293
2017-12-10T05:24:36.045343: step 3646, loss 0.201798, acc 0.984375, prec 0.0748262, recall 0.789373
2017-12-10T05:24:36.317925: step 3647, loss 0.271213, acc 0.9375, prec 0.0748375, recall 0.789414
2017-12-10T05:24:36.586853: step 3648, loss 0.043275, acc 0.96875, prec 0.0748682, recall 0.789494
2017-12-10T05:24:36.853553: step 3649, loss 0.0253573, acc 1, prec 0.0748849, recall 0.789534
2017-12-10T05:24:37.122893: step 3650, loss 0.232225, acc 0.953125, prec 0.074931, recall 0.789654
2017-12-10T05:24:37.388190: step 3651, loss 0.788399, acc 0.96875, prec 0.0749617, recall 0.789734
2017-12-10T05:24:37.655056: step 3652, loss 0.266604, acc 0.921875, prec 0.0749716, recall 0.789774
2017-12-10T05:24:37.919011: step 3653, loss 0.292864, acc 0.953125, prec 0.0749675, recall 0.789774
2017-12-10T05:24:38.186088: step 3654, loss 0.0120629, acc 1, prec 0.0749675, recall 0.789774
2017-12-10T05:24:38.446736: step 3655, loss 0.15551, acc 0.953125, prec 0.0749968, recall 0.789854
2017-12-10T05:24:38.707998: step 3656, loss 0.221943, acc 0.953125, prec 0.0749928, recall 0.789854
2017-12-10T05:24:38.974058: step 3657, loss 0.163387, acc 0.9375, prec 0.0750207, recall 0.789934
2017-12-10T05:24:39.241716: step 3658, loss 0.0845062, acc 0.953125, prec 0.0750167, recall 0.789934
2017-12-10T05:24:39.518498: step 3659, loss 0.0227564, acc 0.984375, prec 0.0750153, recall 0.789934
2017-12-10T05:24:39.790333: step 3660, loss 0.119645, acc 0.9375, prec 0.0750099, recall 0.789934
2017-12-10T05:24:40.059593: step 3661, loss 0.118085, acc 0.953125, prec 0.0750059, recall 0.789934
2017-12-10T05:24:40.327503: step 3662, loss 0.042714, acc 1, prec 0.0750392, recall 0.790013
2017-12-10T05:24:40.590378: step 3663, loss 0.900691, acc 0.96875, prec 0.0750532, recall 0.790053
2017-12-10T05:24:40.859449: step 3664, loss 0.189486, acc 0.9375, prec 0.0750645, recall 0.790093
2017-12-10T05:24:41.124016: step 3665, loss 0.10948, acc 0.96875, prec 0.0750784, recall 0.790133
2017-12-10T05:24:41.387691: step 3666, loss 0.0917398, acc 0.953125, prec 0.0751077, recall 0.790212
2017-12-10T05:24:41.651059: step 3667, loss 0.149472, acc 0.9375, prec 0.0751356, recall 0.790292
2017-12-10T05:24:41.925020: step 3668, loss 0.160738, acc 0.96875, prec 0.0751496, recall 0.790332
2017-12-10T05:24:42.195752: step 3669, loss 0.233804, acc 0.9375, prec 0.0751442, recall 0.790332
2017-12-10T05:24:42.472054: step 3670, loss 0.367612, acc 0.96875, prec 0.0751915, recall 0.790451
2017-12-10T05:24:42.736671: step 3671, loss 1.9765, acc 0.96875, prec 0.0752235, recall 0.790381
2017-12-10T05:24:43.009520: step 3672, loss 0.0224764, acc 1, prec 0.0752401, recall 0.79042
2017-12-10T05:24:43.275217: step 3673, loss 0.227709, acc 0.953125, prec 0.0752694, recall 0.7905
2017-12-10T05:24:43.546202: step 3674, loss 0.325586, acc 0.953125, prec 0.0752987, recall 0.790579
2017-12-10T05:24:43.821856: step 3675, loss 0.203096, acc 0.9375, prec 0.0752932, recall 0.790579
2017-12-10T05:24:44.086171: step 3676, loss 0.137178, acc 0.953125, prec 0.0753058, recall 0.790618
2017-12-10T05:24:44.357333: step 3677, loss 0.289221, acc 0.953125, prec 0.0753184, recall 0.790658
2017-12-10T05:24:44.631480: step 3678, loss 0.248482, acc 0.90625, prec 0.0753103, recall 0.790658
2017-12-10T05:24:44.897595: step 3679, loss 0.162857, acc 0.9375, prec 0.0753548, recall 0.790777
2017-12-10T05:24:45.166526: step 3680, loss 0.491781, acc 0.828125, prec 0.0753399, recall 0.790777
2017-12-10T05:24:45.431717: step 3681, loss 0.33293, acc 0.875, prec 0.0753623, recall 0.790856
2017-12-10T05:24:45.699184: step 3682, loss 0.103852, acc 0.96875, prec 0.0753762, recall 0.790895
2017-12-10T05:24:45.961450: step 3683, loss 0.313242, acc 0.921875, prec 0.0753861, recall 0.790935
2017-12-10T05:24:46.231906: step 3684, loss 0.128657, acc 0.9375, prec 0.0753807, recall 0.790935
2017-12-10T05:24:46.499812: step 3685, loss 0.094536, acc 0.953125, prec 0.0753766, recall 0.790935
2017-12-10T05:24:46.772746: step 3686, loss 0.302677, acc 0.875, prec 0.0753658, recall 0.790935
2017-12-10T05:24:47.039313: step 3687, loss 0.0836559, acc 0.96875, prec 0.0753797, recall 0.790974
2017-12-10T05:24:47.314904: step 3688, loss 0.136192, acc 0.96875, prec 0.0753936, recall 0.791014
2017-12-10T05:24:47.587911: step 3689, loss 2.25333, acc 0.90625, prec 0.0754035, recall 0.790904
2017-12-10T05:24:47.857476: step 3690, loss 0.45672, acc 0.921875, prec 0.0754299, recall 0.790983
2017-12-10T05:24:48.124533: step 3691, loss 0.0578228, acc 0.984375, prec 0.0754286, recall 0.790983
2017-12-10T05:24:48.391206: step 3692, loss 0.200811, acc 0.953125, prec 0.0754245, recall 0.790983
2017-12-10T05:24:48.658336: step 3693, loss 0.413821, acc 0.84375, prec 0.075411, recall 0.790983
2017-12-10T05:24:48.928280: step 3694, loss 0.14024, acc 0.96875, prec 0.0754082, recall 0.790983
2017-12-10T05:24:49.195503: step 3695, loss 0.168944, acc 0.921875, prec 0.0754015, recall 0.790983
2017-12-10T05:24:49.456421: step 3696, loss 0.389503, acc 0.90625, prec 0.0754432, recall 0.791101
2017-12-10T05:24:49.723040: step 3697, loss 0.113567, acc 0.9375, prec 0.0754378, recall 0.791101
2017-12-10T05:24:49.987012: step 3698, loss 0.34331, acc 0.953125, prec 0.0754503, recall 0.79114
2017-12-10T05:24:50.251414: step 3699, loss 0.315237, acc 0.921875, prec 0.0754436, recall 0.79114
2017-12-10T05:24:50.517370: step 3700, loss 0.222709, acc 0.921875, prec 0.0754368, recall 0.79114
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-3700

2017-12-10T05:24:51.716403: step 3701, loss 0.130207, acc 0.953125, prec 0.0754493, recall 0.79118
2017-12-10T05:24:51.989539: step 3702, loss 0.0399575, acc 0.984375, prec 0.0754646, recall 0.791219
2017-12-10T05:24:52.257359: step 3703, loss 0.239456, acc 0.90625, prec 0.0754564, recall 0.791219
2017-12-10T05:24:52.522745: step 3704, loss 0.126848, acc 0.953125, prec 0.0754524, recall 0.791219
2017-12-10T05:24:52.791953: step 3705, loss 0.0313741, acc 0.984375, prec 0.075451, recall 0.791219
2017-12-10T05:24:53.058654: step 3706, loss 0.170433, acc 0.953125, prec 0.0754968, recall 0.791337
2017-12-10T05:24:53.322538: step 3707, loss 0.553184, acc 0.953125, prec 0.0755093, recall 0.791376
2017-12-10T05:24:53.595953: step 3708, loss 0.222906, acc 0.953125, prec 0.0755053, recall 0.791376
2017-12-10T05:24:53.867362: step 3709, loss 0.0219751, acc 1, prec 0.0755053, recall 0.791376
2017-12-10T05:24:54.131535: step 3710, loss 0.0753117, acc 0.953125, prec 0.0755012, recall 0.791376
2017-12-10T05:24:54.399680: step 3711, loss 0.0549052, acc 0.96875, prec 0.0755151, recall 0.791416
2017-12-10T05:24:54.667642: step 3712, loss 0.295268, acc 0.953125, prec 0.0755276, recall 0.791455
2017-12-10T05:24:54.932907: step 3713, loss 0.0234512, acc 0.984375, prec 0.0755429, recall 0.791494
2017-12-10T05:24:55.195067: step 3714, loss 0.0519576, acc 0.984375, prec 0.0755747, recall 0.791573
2017-12-10T05:24:55.461852: step 3715, loss 0.187566, acc 0.953125, prec 0.0755706, recall 0.791573
2017-12-10T05:24:55.726557: step 3716, loss 0.0823105, acc 0.96875, prec 0.0755679, recall 0.791573
2017-12-10T05:24:55.996407: step 3717, loss 0.217008, acc 0.9375, prec 0.0755791, recall 0.791612
2017-12-10T05:24:56.265594: step 3718, loss 0.293726, acc 0.953125, prec 0.075575, recall 0.791612
2017-12-10T05:24:56.537712: step 3719, loss 0.302004, acc 0.984375, prec 0.0756069, recall 0.79169
2017-12-10T05:24:56.803851: step 3720, loss 0.144769, acc 0.96875, prec 0.0756539, recall 0.791808
2017-12-10T05:24:57.075854: step 3721, loss 0.0304444, acc 1, prec 0.0756705, recall 0.791847
2017-12-10T05:24:57.353188: step 3722, loss 0.017277, acc 1, prec 0.0756705, recall 0.791847
2017-12-10T05:24:57.622616: step 3723, loss 0.00777817, acc 1, prec 0.0756705, recall 0.791847
2017-12-10T05:24:57.885750: step 3724, loss 3.45958, acc 0.953125, prec 0.075701, recall 0.791776
2017-12-10T05:24:58.154578: step 3725, loss 0.430027, acc 0.890625, prec 0.0756915, recall 0.791776
2017-12-10T05:24:58.421521: step 3726, loss 0.242485, acc 0.984375, prec 0.0756901, recall 0.791776
2017-12-10T05:24:58.693760: step 3727, loss 0.0982635, acc 0.953125, prec 0.0757026, recall 0.791815
2017-12-10T05:24:58.958106: step 3728, loss 0.202269, acc 0.953125, prec 0.0757483, recall 0.791932
2017-12-10T05:24:59.227643: step 3729, loss 0.423281, acc 0.875, prec 0.0757375, recall 0.791932
2017-12-10T05:24:59.497468: step 3730, loss 0.225844, acc 0.9375, prec 0.0757486, recall 0.791972
2017-12-10T05:24:59.762377: step 3731, loss 0.287403, acc 0.890625, prec 0.0757557, recall 0.79201
2017-12-10T05:25:00.034866: step 3732, loss 0.216372, acc 0.9375, prec 0.0757668, recall 0.79205
2017-12-10T05:25:00.301441: step 3733, loss 0.276985, acc 0.9375, prec 0.0757614, recall 0.79205
2017-12-10T05:25:00.577923: step 3734, loss 0.655522, acc 0.921875, prec 0.0757712, recall 0.792089
2017-12-10T05:25:00.848903: step 3735, loss 0.411875, acc 0.90625, prec 0.0757796, recall 0.792127
2017-12-10T05:25:01.109371: step 3736, loss 0.342517, acc 0.921875, prec 0.0757894, recall 0.792166
2017-12-10T05:25:01.369779: step 3737, loss 0.320052, acc 0.859375, prec 0.0757771, recall 0.792166
2017-12-10T05:25:01.639737: step 3738, loss 0.456176, acc 0.90625, prec 0.0758187, recall 0.792283
2017-12-10T05:25:01.901007: step 3739, loss 0.0425845, acc 0.984375, prec 0.0758173, recall 0.792283
2017-12-10T05:25:02.163755: step 3740, loss 0.172927, acc 0.96875, prec 0.0758312, recall 0.792322
2017-12-10T05:25:02.432244: step 3741, loss 0.35332, acc 0.9375, prec 0.0758423, recall 0.792361
2017-12-10T05:25:02.696369: step 3742, loss 2.2111, acc 0.953125, prec 0.0758727, recall 0.79229
2017-12-10T05:25:02.963892: step 3743, loss 0.0957548, acc 0.96875, prec 0.0759031, recall 0.792368
2017-12-10T05:25:03.228664: step 3744, loss 0.0515677, acc 0.984375, prec 0.0759017, recall 0.792368
2017-12-10T05:25:03.490659: step 3745, loss 0.265403, acc 0.921875, prec 0.0758949, recall 0.792368
2017-12-10T05:25:03.753141: step 3746, loss 0.325391, acc 0.875, prec 0.0758841, recall 0.792368
2017-12-10T05:25:04.012470: step 3747, loss 0.141346, acc 0.9375, prec 0.0758952, recall 0.792407
2017-12-10T05:25:04.274493: step 3748, loss 0.482944, acc 0.859375, prec 0.0758829, recall 0.792407
2017-12-10T05:25:04.539190: step 3749, loss 0.313209, acc 0.921875, prec 0.0759092, recall 0.792485
2017-12-10T05:25:04.811758: step 3750, loss 0.697799, acc 0.875, prec 0.0759149, recall 0.792523
2017-12-10T05:25:05.073999: step 3751, loss 0.311319, acc 0.921875, prec 0.0759081, recall 0.792523
2017-12-10T05:25:05.337999: step 3752, loss 0.94165, acc 0.90625, prec 0.0759331, recall 0.792601
2017-12-10T05:25:05.603793: step 3753, loss 0.420838, acc 0.875, prec 0.0759222, recall 0.792601
2017-12-10T05:25:05.867502: step 3754, loss 0.685824, acc 0.859375, prec 0.0759099, recall 0.792601
2017-12-10T05:25:06.133675: step 3755, loss 0.322515, acc 0.90625, prec 0.0759183, recall 0.79264
2017-12-10T05:25:06.398171: step 3756, loss 0.151994, acc 0.953125, prec 0.0759308, recall 0.792678
2017-12-10T05:25:06.668605: step 3757, loss 0.0987524, acc 0.984375, prec 0.075946, recall 0.792717
2017-12-10T05:25:06.935749: step 3758, loss 0.243492, acc 0.9375, prec 0.0759901, recall 0.792833
2017-12-10T05:25:07.208694: step 3759, loss 0.576495, acc 0.90625, prec 0.0760316, recall 0.792949
2017-12-10T05:25:07.478086: step 3760, loss 0.238308, acc 0.921875, prec 0.0760578, recall 0.793026
2017-12-10T05:25:07.741093: step 3761, loss 1.91303, acc 0.9375, prec 0.0760537, recall 0.792878
2017-12-10T05:25:08.011506: step 3762, loss 0.148839, acc 0.96875, prec 0.0760675, recall 0.792917
2017-12-10T05:25:08.275442: step 3763, loss 0.196187, acc 0.921875, prec 0.0760772, recall 0.792956
2017-12-10T05:25:08.540243: step 3764, loss 0.699111, acc 0.921875, prec 0.076087, recall 0.792994
2017-12-10T05:25:08.810697: step 3765, loss 0.375757, acc 0.921875, prec 0.0761132, recall 0.793071
2017-12-10T05:25:09.075437: step 3766, loss 0.244351, acc 0.953125, prec 0.0761421, recall 0.793148
2017-12-10T05:25:09.338809: step 3767, loss 0.212875, acc 0.921875, prec 0.0761518, recall 0.793187
2017-12-10T05:25:09.602565: step 3768, loss 0.368738, acc 0.890625, prec 0.0761588, recall 0.793225
2017-12-10T05:25:09.870508: step 3769, loss 0.234459, acc 0.875, prec 0.0761479, recall 0.793225
2017-12-10T05:25:10.132043: step 3770, loss 0.274717, acc 0.9375, prec 0.0761425, recall 0.793225
2017-12-10T05:25:10.407236: step 3771, loss 0.37926, acc 0.90625, prec 0.0761343, recall 0.793225
2017-12-10T05:25:10.676980: step 3772, loss 0.395036, acc 0.921875, prec 0.076177, recall 0.793341
2017-12-10T05:25:10.941694: step 3773, loss 0.235619, acc 0.921875, prec 0.0761702, recall 0.793341
2017-12-10T05:25:11.208703: step 3774, loss 0.0851793, acc 0.96875, prec 0.0761675, recall 0.793341
2017-12-10T05:25:11.473657: step 3775, loss 0.304438, acc 0.9375, prec 0.0761621, recall 0.793341
2017-12-10T05:25:11.739210: step 3776, loss 0.302281, acc 0.96875, prec 0.0761594, recall 0.793341
2017-12-10T05:25:12.011547: step 3777, loss 0.269259, acc 0.9375, prec 0.0761539, recall 0.793341
2017-12-10T05:25:12.280984: step 3778, loss 0.135197, acc 0.96875, prec 0.0761677, recall 0.793379
2017-12-10T05:25:12.552858: step 3779, loss 0.222372, acc 0.953125, prec 0.0761636, recall 0.793379
2017-12-10T05:25:12.817554: step 3780, loss 0.0742135, acc 0.96875, prec 0.0761609, recall 0.793379
2017-12-10T05:25:13.080098: step 3781, loss 0.208821, acc 0.90625, prec 0.0761527, recall 0.793379
2017-12-10T05:25:13.346639: step 3782, loss 0.318445, acc 0.921875, prec 0.0761624, recall 0.793418
2017-12-10T05:25:13.616602: step 3783, loss 0.0321548, acc 0.984375, prec 0.0761776, recall 0.793456
2017-12-10T05:25:13.882621: step 3784, loss 0.0814528, acc 0.96875, prec 0.0761913, recall 0.793494
2017-12-10T05:25:14.150436: step 3785, loss 0.327974, acc 0.9375, prec 0.0762189, recall 0.793571
2017-12-10T05:25:14.419256: step 3786, loss 0.296943, acc 0.9375, prec 0.0762134, recall 0.793571
2017-12-10T05:25:14.683759: step 3787, loss 0.128125, acc 0.984375, prec 0.076245, recall 0.793648
2017-12-10T05:25:14.954716: step 3788, loss 0.0399686, acc 0.96875, prec 0.0762588, recall 0.793686
2017-12-10T05:25:15.224656: step 3789, loss 0.340815, acc 0.96875, prec 0.076289, recall 0.793763
2017-12-10T05:25:15.494651: step 3790, loss 0.0586354, acc 0.96875, prec 0.0762863, recall 0.793763
2017-12-10T05:25:15.757216: step 3791, loss 0.0927678, acc 0.96875, prec 0.0763165, recall 0.793839
2017-12-10T05:25:16.025838: step 3792, loss 3.17481, acc 0.953125, prec 0.0763468, recall 0.793769
2017-12-10T05:25:16.292900: step 3793, loss 0.0967071, acc 0.953125, prec 0.0763592, recall 0.793807
2017-12-10T05:25:16.563279: step 3794, loss 0.300102, acc 0.96875, prec 0.0763894, recall 0.793883
2017-12-10T05:25:16.833136: step 3795, loss 0.0535125, acc 0.984375, prec 0.0764045, recall 0.793921
2017-12-10T05:25:17.101322: step 3796, loss 0.0460609, acc 0.96875, prec 0.0764347, recall 0.793998
2017-12-10T05:25:17.365538: step 3797, loss 0.0651103, acc 0.96875, prec 0.076432, recall 0.793998
2017-12-10T05:25:17.635742: step 3798, loss 0.170074, acc 0.96875, prec 0.0764457, recall 0.794036
2017-12-10T05:25:17.899131: step 3799, loss 0.143857, acc 0.9375, prec 0.0764567, recall 0.794074
2017-12-10T05:25:18.163891: step 3800, loss 0.0311826, acc 0.984375, prec 0.0764554, recall 0.794074
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-3800

2017-12-10T05:25:19.378151: step 3801, loss 0.190417, acc 0.9375, prec 0.0764499, recall 0.794074
2017-12-10T05:25:19.647722: step 3802, loss 0.200431, acc 0.953125, prec 0.0764788, recall 0.79415
2017-12-10T05:25:19.917695: step 3803, loss 0.21328, acc 0.921875, prec 0.0764884, recall 0.794188
2017-12-10T05:25:20.181879: step 3804, loss 0.136062, acc 0.9375, prec 0.076483, recall 0.794188
2017-12-10T05:25:20.444793: step 3805, loss 0.181616, acc 0.953125, prec 0.0764953, recall 0.794227
2017-12-10T05:25:20.710538: step 3806, loss 0.468105, acc 0.90625, prec 0.0765036, recall 0.794265
2017-12-10T05:25:20.977898: step 3807, loss 3.00389, acc 0.90625, prec 0.0764968, recall 0.794118
2017-12-10T05:25:21.250496: step 3808, loss 0.0642039, acc 0.984375, prec 0.0764954, recall 0.794118
2017-12-10T05:25:21.512635: step 3809, loss 0.226274, acc 0.953125, prec 0.0764913, recall 0.794118
2017-12-10T05:25:21.779258: step 3810, loss 0.378109, acc 0.921875, prec 0.0764845, recall 0.794118
2017-12-10T05:25:22.041805: step 3811, loss 0.0861009, acc 0.984375, prec 0.0764996, recall 0.794156
2017-12-10T05:25:22.305806: step 3812, loss 0.192626, acc 0.921875, prec 0.0765093, recall 0.794194
2017-12-10T05:25:22.569495: step 3813, loss 0.227444, acc 0.9375, prec 0.0765202, recall 0.794232
2017-12-10T05:25:22.834884: step 3814, loss 0.447651, acc 0.890625, prec 0.0765107, recall 0.794232
2017-12-10T05:25:23.100228: step 3815, loss 0.129059, acc 0.953125, prec 0.0765066, recall 0.794232
2017-12-10T05:25:23.359099: step 3816, loss 0.185662, acc 0.90625, prec 0.0765149, recall 0.79427
2017-12-10T05:25:23.623765: step 3817, loss 0.263675, acc 0.9375, prec 0.0765094, recall 0.79427
2017-12-10T05:25:23.885629: step 3818, loss 0.613268, acc 0.90625, prec 0.0765177, recall 0.794308
2017-12-10T05:25:24.153245: step 3819, loss 0.480403, acc 0.875, prec 0.0765397, recall 0.794384
2017-12-10T05:25:24.419787: step 3820, loss 0.184793, acc 0.921875, prec 0.0765822, recall 0.794498
2017-12-10T05:25:24.685442: step 3821, loss 0.147163, acc 0.921875, prec 0.0765918, recall 0.794536
2017-12-10T05:25:24.950751: step 3822, loss 0.333554, acc 0.90625, prec 0.0766165, recall 0.794612
2017-12-10T05:25:25.211170: step 3823, loss 0.176358, acc 0.9375, prec 0.0766439, recall 0.794687
2017-12-10T05:25:25.479328: step 3824, loss 0.220463, acc 0.9375, prec 0.0766384, recall 0.794687
2017-12-10T05:25:25.739366: step 3825, loss 3.02009, acc 0.84375, prec 0.0766262, recall 0.794541
2017-12-10T05:25:26.008482: step 3826, loss 0.477943, acc 0.890625, prec 0.0766495, recall 0.794617
2017-12-10T05:25:26.270773: step 3827, loss 0.164013, acc 0.96875, prec 0.0766796, recall 0.794692
2017-12-10T05:25:26.529913: step 3828, loss 0.338216, acc 0.875, prec 0.0766687, recall 0.794692
2017-12-10T05:25:26.794725: step 3829, loss 0.290068, acc 0.875, prec 0.076707, recall 0.794806
2017-12-10T05:25:27.064024: step 3830, loss 0.61665, acc 0.9375, prec 0.076718, recall 0.794843
2017-12-10T05:25:27.335872: step 3831, loss 0.572907, acc 0.875, prec 0.0767235, recall 0.794881
2017-12-10T05:25:27.606764: step 3832, loss 0.402058, acc 0.90625, prec 0.0767317, recall 0.794919
2017-12-10T05:25:27.865195: step 3833, loss 0.298555, acc 0.921875, prec 0.0767413, recall 0.794957
2017-12-10T05:25:28.133987: step 3834, loss 0.460032, acc 0.859375, prec 0.0767618, recall 0.795032
2017-12-10T05:25:28.403603: step 3835, loss 0.188304, acc 0.921875, prec 0.0767714, recall 0.79507
2017-12-10T05:25:28.673383: step 3836, loss 0.136282, acc 0.953125, prec 0.0767673, recall 0.79507
2017-12-10T05:25:28.940613: step 3837, loss 0.544812, acc 0.84375, prec 0.0767865, recall 0.795145
2017-12-10T05:25:29.209271: step 3838, loss 0.125419, acc 0.921875, prec 0.0767797, recall 0.795145
2017-12-10T05:25:29.476017: step 3839, loss 0.547541, acc 0.875, prec 0.0768015, recall 0.795221
2017-12-10T05:25:29.746468: step 3840, loss 0.238002, acc 0.90625, prec 0.0767934, recall 0.795221
2017-12-10T05:25:30.009072: step 3841, loss 0.580745, acc 0.90625, prec 0.0768016, recall 0.795258
2017-12-10T05:25:30.280738: step 3842, loss 0.15531, acc 0.9375, prec 0.0768125, recall 0.795296
2017-12-10T05:25:30.559393: step 3843, loss 0.165623, acc 0.9375, prec 0.0768234, recall 0.795333
2017-12-10T05:25:30.831843: step 3844, loss 0.0433842, acc 0.984375, prec 0.0768384, recall 0.795371
2017-12-10T05:25:31.100116: step 3845, loss 0.105092, acc 0.96875, prec 0.0768357, recall 0.795371
2017-12-10T05:25:31.367723: step 3846, loss 0.0629582, acc 1, prec 0.0768521, recall 0.795409
2017-12-10T05:25:31.635895: step 3847, loss 0.109103, acc 0.984375, prec 0.0768999, recall 0.795521
2017-12-10T05:25:31.907994: step 3848, loss 0.11095, acc 0.96875, prec 0.0769135, recall 0.795559
2017-12-10T05:25:32.182899: step 3849, loss 0.265809, acc 0.953125, prec 0.0769422, recall 0.795634
2017-12-10T05:25:32.454265: step 3850, loss 0.00341703, acc 1, prec 0.0769422, recall 0.795634
2017-12-10T05:25:32.714647: step 3851, loss 0.225831, acc 0.921875, prec 0.0769354, recall 0.795634
2017-12-10T05:25:32.988501: step 3852, loss 0.00280565, acc 1, prec 0.0769354, recall 0.795634
2017-12-10T05:25:33.252374: step 3853, loss 0.144518, acc 0.96875, prec 0.0769326, recall 0.795634
2017-12-10T05:25:33.521850: step 3854, loss 0.405169, acc 0.96875, prec 0.076979, recall 0.795746
2017-12-10T05:25:33.789352: step 3855, loss 0.165436, acc 0.984375, prec 0.076994, recall 0.795784
2017-12-10T05:25:34.059728: step 3856, loss 0.00558912, acc 1, prec 0.076994, recall 0.795784
2017-12-10T05:25:34.335477: step 3857, loss 0.0988872, acc 1, prec 0.0770268, recall 0.795859
2017-12-10T05:25:34.607700: step 3858, loss 0.0248625, acc 0.984375, prec 0.0770418, recall 0.795896
2017-12-10T05:25:34.875673: step 3859, loss 0.00420695, acc 1, prec 0.0770581, recall 0.795933
2017-12-10T05:25:35.148246: step 3860, loss 0.16588, acc 0.984375, prec 0.0770731, recall 0.795971
2017-12-10T05:25:35.417312: step 3861, loss 0.166271, acc 0.9375, prec 0.077084, recall 0.796008
2017-12-10T05:25:35.680822: step 3862, loss 0.101158, acc 0.96875, prec 0.0770813, recall 0.796008
2017-12-10T05:25:35.947838: step 3863, loss 0.196132, acc 0.984375, prec 0.0770963, recall 0.796045
2017-12-10T05:25:36.223242: step 3864, loss 0.0984028, acc 0.96875, prec 0.0771099, recall 0.796083
2017-12-10T05:25:36.500755: step 3865, loss 0.109167, acc 1, prec 0.0771263, recall 0.79612
2017-12-10T05:25:36.772386: step 3866, loss 6.43692, acc 0.96875, prec 0.0771249, recall 0.795974
2017-12-10T05:25:37.051273: step 3867, loss 0.0720636, acc 0.984375, prec 0.0771563, recall 0.796049
2017-12-10T05:25:37.320012: step 3868, loss 0.211558, acc 0.921875, prec 0.0771494, recall 0.796049
2017-12-10T05:25:37.591567: step 3869, loss 0.280123, acc 0.9375, prec 0.077144, recall 0.796049
2017-12-10T05:25:37.859112: step 3870, loss 0.085835, acc 0.96875, prec 0.0771739, recall 0.796124
2017-12-10T05:25:38.130802: step 3871, loss 0.246626, acc 0.953125, prec 0.0771862, recall 0.796161
2017-12-10T05:25:38.404656: step 3872, loss 0.197331, acc 0.9375, prec 0.0771971, recall 0.796198
2017-12-10T05:25:38.675209: step 3873, loss 0.193539, acc 0.90625, prec 0.0772216, recall 0.796273
2017-12-10T05:25:38.939449: step 3874, loss 0.496859, acc 0.90625, prec 0.0772297, recall 0.79631
2017-12-10T05:25:39.204479: step 3875, loss 0.385566, acc 0.921875, prec 0.0772229, recall 0.79631
2017-12-10T05:25:39.469359: step 3876, loss 0.793383, acc 0.828125, prec 0.0772078, recall 0.79631
2017-12-10T05:25:39.732954: step 3877, loss 0.316812, acc 0.890625, prec 0.0771983, recall 0.79631
2017-12-10T05:25:39.999198: step 3878, loss 0.267549, acc 0.921875, prec 0.0771914, recall 0.79631
2017-12-10T05:25:40.268967: step 3879, loss 0.375878, acc 0.84375, prec 0.0771778, recall 0.79631
2017-12-10T05:25:40.538837: step 3880, loss 0.39402, acc 0.90625, prec 0.0772022, recall 0.796384
2017-12-10T05:25:40.808124: step 3881, loss 0.0403062, acc 0.984375, prec 0.0772009, recall 0.796384
2017-12-10T05:25:41.072495: step 3882, loss 0.216942, acc 0.9375, prec 0.0771954, recall 0.796384
2017-12-10T05:25:41.336355: step 3883, loss 0.39978, acc 0.953125, prec 0.0772076, recall 0.796421
2017-12-10T05:25:41.608217: step 3884, loss 0.13869, acc 0.953125, prec 0.0772362, recall 0.796496
2017-12-10T05:25:41.874920: step 3885, loss 0.392795, acc 0.90625, prec 0.077228, recall 0.796496
2017-12-10T05:25:42.149876: step 3886, loss 0.568814, acc 0.953125, prec 0.0772239, recall 0.796496
2017-12-10T05:25:42.430526: step 3887, loss 0.12518, acc 0.953125, prec 0.0772198, recall 0.796496
2017-12-10T05:25:42.691814: step 3888, loss 3.07206, acc 0.953125, prec 0.0772334, recall 0.796387
2017-12-10T05:25:42.964104: step 3889, loss 0.340643, acc 0.90625, prec 0.0772252, recall 0.796387
2017-12-10T05:25:43.235995: step 3890, loss 0.131908, acc 0.9375, prec 0.0772687, recall 0.796499
2017-12-10T05:25:43.500677: step 3891, loss 0.154559, acc 0.96875, prec 0.0773313, recall 0.796647
2017-12-10T05:25:43.763960: step 3892, loss 0.36625, acc 0.890625, prec 0.077338, recall 0.796684
2017-12-10T05:25:44.028947: step 3893, loss 0.067274, acc 0.96875, prec 0.0773516, recall 0.796721
2017-12-10T05:25:44.293663: step 3894, loss 0.136073, acc 0.953125, prec 0.0773801, recall 0.796795
2017-12-10T05:25:44.562542: step 3895, loss 0.104122, acc 0.953125, prec 0.077376, recall 0.796795
2017-12-10T05:25:44.828075: step 3896, loss 0.379788, acc 0.921875, prec 0.0773855, recall 0.796832
2017-12-10T05:25:45.091076: step 3897, loss 0.143284, acc 0.921875, prec 0.0773786, recall 0.796832
2017-12-10T05:25:45.352758: step 3898, loss 0.413358, acc 0.90625, prec 0.0773867, recall 0.796869
2017-12-10T05:25:45.617607: step 3899, loss 0.343935, acc 0.953125, prec 0.0774316, recall 0.79698
2017-12-10T05:25:45.885108: step 3900, loss 0.342972, acc 0.90625, prec 0.0774397, recall 0.797017

Evaluation:
2017-12-10T05:25:53.570686: step 3900, loss 2.4293, acc 0.901491, prec 0.0776527, recall 0.792146

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-3900

2017-12-10T05:25:54.839749: step 3901, loss 0.405188, acc 0.890625, prec 0.0776593, recall 0.792183
2017-12-10T05:25:55.110391: step 3902, loss 0.446103, acc 0.875, prec 0.0776645, recall 0.792219
2017-12-10T05:25:55.369851: step 3903, loss 0.303461, acc 0.96875, prec 0.0777098, recall 0.792329
2017-12-10T05:25:55.635502: step 3904, loss 0.365738, acc 0.90625, prec 0.0777017, recall 0.792329
2017-12-10T05:25:55.905411: step 3905, loss 0.295861, acc 0.9375, prec 0.0777443, recall 0.79244
2017-12-10T05:25:56.172272: step 3906, loss 0.177708, acc 0.921875, prec 0.0777375, recall 0.79244
2017-12-10T05:25:56.431562: step 3907, loss 0.234821, acc 1, prec 0.0777855, recall 0.792549
2017-12-10T05:25:56.709359: step 3908, loss 0.365313, acc 0.90625, prec 0.0778093, recall 0.792623
2017-12-10T05:25:56.970757: step 3909, loss 0.318526, acc 0.953125, prec 0.0778213, recall 0.792659
2017-12-10T05:25:57.245857: step 3910, loss 0.0913978, acc 0.96875, prec 0.0778186, recall 0.792659
2017-12-10T05:25:57.511838: step 3911, loss 0.0364846, acc 0.984375, prec 0.0778172, recall 0.792659
2017-12-10T05:25:57.776343: step 3912, loss 0.200978, acc 0.953125, prec 0.0778132, recall 0.792659
2017-12-10T05:25:58.046922: step 3913, loss 0.309051, acc 0.921875, prec 0.0778065, recall 0.792659
2017-12-10T05:25:58.310808: step 3914, loss 0.064448, acc 0.984375, prec 0.0778051, recall 0.792659
2017-12-10T05:25:58.573533: step 3915, loss 0.112415, acc 0.96875, prec 0.0778024, recall 0.792659
2017-12-10T05:25:58.837524: step 3916, loss 0.0467019, acc 0.984375, prec 0.0778011, recall 0.792659
2017-12-10T05:25:59.110726: step 3917, loss 3.3269, acc 0.921875, prec 0.0777957, recall 0.792519
2017-12-10T05:25:59.386292: step 3918, loss 0.104322, acc 0.953125, prec 0.0778076, recall 0.792556
2017-12-10T05:25:59.653480: step 3919, loss 0.299633, acc 0.921875, prec 0.0778168, recall 0.792593
2017-12-10T05:25:59.923837: step 3920, loss 0.189772, acc 0.953125, prec 0.0778128, recall 0.792593
2017-12-10T05:26:00.186575: step 3921, loss 0.183858, acc 0.921875, prec 0.077822, recall 0.792629
2017-12-10T05:26:00.453751: step 3922, loss 0.290504, acc 0.875, prec 0.0778112, recall 0.792629
2017-12-10T05:26:00.728396: step 3923, loss 0.582041, acc 0.84375, prec 0.0778137, recall 0.792666
2017-12-10T05:26:00.994223: step 3924, loss 0.230067, acc 0.921875, prec 0.077807, recall 0.792666
2017-12-10T05:26:01.256010: step 3925, loss 0.252676, acc 0.921875, prec 0.0778003, recall 0.792666
2017-12-10T05:26:01.515804: step 3926, loss 0.538655, acc 0.890625, prec 0.0778068, recall 0.792702
2017-12-10T05:26:01.782000: step 3927, loss 0.391519, acc 0.90625, prec 0.0778306, recall 0.792775
2017-12-10T05:26:02.047679: step 3928, loss 0.281504, acc 0.953125, prec 0.0778425, recall 0.792812
2017-12-10T05:26:02.308441: step 3929, loss 0.320418, acc 0.875, prec 0.0778318, recall 0.792812
2017-12-10T05:26:02.573256: step 3930, loss 0.301574, acc 0.9375, prec 0.0778264, recall 0.792812
2017-12-10T05:26:02.840738: step 3931, loss 0.453464, acc 0.890625, prec 0.077817, recall 0.792812
2017-12-10T05:26:03.103779: step 3932, loss 1.09118, acc 0.8125, prec 0.0778008, recall 0.792812
2017-12-10T05:26:03.367844: step 3933, loss 0.267743, acc 0.953125, prec 0.0778127, recall 0.792848
2017-12-10T05:26:03.626687: step 3934, loss 0.233549, acc 0.9375, prec 0.0778233, recall 0.792885
2017-12-10T05:26:04.580563: step 3935, loss 0.230883, acc 0.953125, prec 0.0778193, recall 0.792885
2017-12-10T05:26:04.941936: step 3936, loss 0.295919, acc 0.9375, prec 0.0778298, recall 0.792921
2017-12-10T05:26:05.206797: step 3937, loss 0.359549, acc 0.921875, prec 0.077839, recall 0.792958
2017-12-10T05:26:05.937231: step 3938, loss 0.0554794, acc 0.96875, prec 0.0778363, recall 0.792958
2017-12-10T05:26:06.683856: step 3939, loss 0.235639, acc 0.953125, prec 0.0778323, recall 0.792958
2017-12-10T05:26:07.422660: step 3940, loss 0.399034, acc 0.921875, prec 0.0778256, recall 0.792958
2017-12-10T05:26:08.164105: step 3941, loss 0.49082, acc 1, prec 0.0778574, recall 0.793031
2017-12-10T05:26:08.909943: step 3942, loss 0.131715, acc 0.96875, prec 0.0778866, recall 0.793103
2017-12-10T05:26:09.654882: step 3943, loss 1.62713, acc 0.90625, prec 0.0778958, recall 0.793
2017-12-10T05:26:10.384675: step 3944, loss 0.0891082, acc 0.984375, prec 0.0779104, recall 0.793037
2017-12-10T05:26:11.123214: step 3945, loss 0.302883, acc 0.921875, prec 0.0779037, recall 0.793037
2017-12-10T05:26:11.845653: step 3946, loss 0.0243817, acc 1, prec 0.0779196, recall 0.793073
2017-12-10T05:26:12.592086: step 3947, loss 0.925382, acc 0.984375, prec 0.0779342, recall 0.79311
2017-12-10T05:26:13.312623: step 3948, loss 0.051887, acc 0.96875, prec 0.0779474, recall 0.793146
2017-12-10T05:26:14.067683: step 3949, loss 0.160488, acc 0.96875, prec 0.0779766, recall 0.793219
2017-12-10T05:26:14.808087: step 3950, loss 0.263594, acc 0.9375, prec 0.0779712, recall 0.793219
2017-12-10T05:26:15.539530: step 3951, loss 0.258357, acc 0.90625, prec 0.0779631, recall 0.793219
2017-12-10T05:26:16.280920: step 3952, loss 0.2819, acc 0.953125, prec 0.0779591, recall 0.793219
2017-12-10T05:26:17.005739: step 3953, loss 0.367664, acc 0.921875, prec 0.0779523, recall 0.793219
2017-12-10T05:26:17.722394: step 3954, loss 0.280538, acc 0.921875, prec 0.0779456, recall 0.793219
2017-12-10T05:26:18.474937: step 3955, loss 0.335212, acc 0.90625, prec 0.0779375, recall 0.793219
2017-12-10T05:26:19.200370: step 3956, loss 0.810217, acc 0.84375, prec 0.07794, recall 0.793255
2017-12-10T05:26:20.305495: step 3957, loss 0.829751, acc 0.859375, prec 0.0779279, recall 0.793255
2017-12-10T05:26:20.706798: step 3958, loss 0.374178, acc 0.90625, prec 0.0779676, recall 0.793364
2017-12-10T05:26:21.009308: step 3959, loss 0.117103, acc 0.953125, prec 0.0779635, recall 0.793364
2017-12-10T05:26:21.296927: step 3960, loss 0.203547, acc 0.9375, prec 0.07799, recall 0.793436
2017-12-10T05:26:21.577964: step 3961, loss 0.204607, acc 0.921875, prec 0.0779991, recall 0.793473
2017-12-10T05:26:21.864947: step 3962, loss 0.135358, acc 0.953125, prec 0.0780428, recall 0.793581
2017-12-10T05:26:22.146258: step 3963, loss 0.0874609, acc 0.953125, prec 0.0780865, recall 0.79369
2017-12-10T05:26:22.418428: step 3964, loss 0.2411, acc 0.953125, prec 0.0780824, recall 0.79369
2017-12-10T05:26:22.684814: step 3965, loss 0.209805, acc 0.953125, prec 0.0780784, recall 0.79369
2017-12-10T05:26:22.948140: step 3966, loss 0.0783102, acc 0.96875, prec 0.0780916, recall 0.793726
2017-12-10T05:26:23.221052: step 3967, loss 1.21369, acc 0.921875, prec 0.0781166, recall 0.793798
2017-12-10T05:26:23.492674: step 3968, loss 0.109307, acc 0.9375, prec 0.0781272, recall 0.793834
2017-12-10T05:26:23.766223: step 3969, loss 0.153867, acc 0.9375, prec 0.0781218, recall 0.793834
2017-12-10T05:26:24.036095: step 3970, loss 0.19586, acc 0.953125, prec 0.0781336, recall 0.79387
2017-12-10T05:26:24.299081: step 3971, loss 0.0927757, acc 0.953125, prec 0.0781614, recall 0.793943
2017-12-10T05:26:24.562896: step 3972, loss 0.32435, acc 0.875, prec 0.0781823, recall 0.794015
2017-12-10T05:26:24.836217: step 3973, loss 0.372279, acc 0.96875, prec 0.0782273, recall 0.794123
2017-12-10T05:26:25.096751: step 3974, loss 0.155353, acc 0.953125, prec 0.0782391, recall 0.794159
2017-12-10T05:26:25.360567: step 3975, loss 0.132415, acc 0.984375, prec 0.0782537, recall 0.794195
2017-12-10T05:26:25.596806: step 3976, loss 0.0824296, acc 0.980769, prec 0.0782523, recall 0.794195
2017-12-10T05:26:25.877596: step 3977, loss 0.351432, acc 0.953125, prec 0.07828, recall 0.794267
2017-12-10T05:26:26.146154: step 3978, loss 0.304441, acc 0.9375, prec 0.0782747, recall 0.794267
2017-12-10T05:26:26.411407: step 3979, loss 0.0274046, acc 1, prec 0.0783064, recall 0.794339
2017-12-10T05:26:26.677533: step 3980, loss 0.13561, acc 0.984375, prec 0.0783051, recall 0.794339
2017-12-10T05:26:26.940536: step 3981, loss 0.179244, acc 0.953125, prec 0.0783169, recall 0.794375
2017-12-10T05:26:27.213181: step 3982, loss 0.212726, acc 0.96875, prec 0.0783301, recall 0.79441
2017-12-10T05:26:27.482486: step 3983, loss 0.292648, acc 0.921875, prec 0.0783233, recall 0.79441
2017-12-10T05:26:27.748609: step 3984, loss 0.02061, acc 0.984375, prec 0.0783378, recall 0.794446
2017-12-10T05:26:28.010819: step 3985, loss 0.0312051, acc 1, prec 0.0783696, recall 0.794518
2017-12-10T05:26:28.271387: step 3986, loss 0.1901, acc 0.953125, prec 0.0783655, recall 0.794518
2017-12-10T05:26:28.536786: step 3987, loss 0.0313056, acc 0.984375, prec 0.0783801, recall 0.794554
2017-12-10T05:26:28.802008: step 3988, loss 0.218116, acc 0.9375, prec 0.0783747, recall 0.794554
2017-12-10T05:26:29.069750: step 3989, loss 0.160873, acc 0.984375, prec 0.0783733, recall 0.794554
2017-12-10T05:26:29.334684: step 3990, loss 0.0169335, acc 1, prec 0.0783892, recall 0.79459
2017-12-10T05:26:29.597570: step 3991, loss 0.0275809, acc 0.984375, prec 0.0783878, recall 0.79459
2017-12-10T05:26:29.866498: step 3992, loss 0.0407541, acc 1, prec 0.0784196, recall 0.794662
2017-12-10T05:26:30.139008: step 3993, loss 0.186932, acc 0.96875, prec 0.0784327, recall 0.794697
2017-12-10T05:26:30.412958: step 3994, loss 0.0862867, acc 0.984375, prec 0.0784631, recall 0.794769
2017-12-10T05:26:30.683969: step 3995, loss 0.118482, acc 0.953125, prec 0.078459, recall 0.794769
2017-12-10T05:26:30.957053: step 3996, loss 0.207116, acc 0.921875, prec 0.078484, recall 0.794841
2017-12-10T05:26:31.220765: step 3997, loss 0.01704, acc 1, prec 0.0785157, recall 0.794912
2017-12-10T05:26:31.484063: step 3998, loss 0.0600183, acc 0.96875, prec 0.0785289, recall 0.794948
2017-12-10T05:26:31.751368: step 3999, loss 0.0834253, acc 0.96875, prec 0.0785262, recall 0.794948
2017-12-10T05:26:32.013750: step 4000, loss 2.85894, acc 0.984375, prec 0.0785262, recall 0.794809
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-4000

2017-12-10T05:26:33.293382: step 4001, loss 0.00588951, acc 1, prec 0.0785421, recall 0.794845
2017-12-10T05:26:33.551544: step 4002, loss 0.0276603, acc 1, prec 0.0785579, recall 0.794881
2017-12-10T05:26:33.820892: step 4003, loss 0.307609, acc 0.9375, prec 0.0785684, recall 0.794916
2017-12-10T05:26:34.086327: step 4004, loss 0.177935, acc 0.96875, prec 0.0785815, recall 0.794952
2017-12-10T05:26:34.361408: step 4005, loss 0.21053, acc 0.90625, prec 0.0785892, recall 0.794988
2017-12-10T05:26:34.621931: step 4006, loss 0.0479125, acc 0.984375, prec 0.0785879, recall 0.794988
2017-12-10T05:26:34.890974: step 4007, loss 0.388611, acc 0.921875, prec 0.078597, recall 0.795024
2017-12-10T05:26:35.151604: step 4008, loss 0.415468, acc 0.890625, prec 0.0786034, recall 0.795059
2017-12-10T05:26:35.417720: step 4009, loss 0.101104, acc 0.953125, prec 0.0785993, recall 0.795059
2017-12-10T05:26:35.683945: step 4010, loss 0.234118, acc 0.921875, prec 0.0785926, recall 0.795059
2017-12-10T05:26:35.943436: step 4011, loss 0.13472, acc 0.953125, prec 0.0786202, recall 0.79513
2017-12-10T05:26:36.212186: step 4012, loss 0.213619, acc 0.9375, prec 0.0786148, recall 0.79513
2017-12-10T05:26:36.476474: step 4013, loss 0.15733, acc 0.953125, prec 0.0786424, recall 0.795202
2017-12-10T05:26:36.746055: step 4014, loss 0.1584, acc 0.96875, prec 0.0786714, recall 0.795273
2017-12-10T05:26:37.015043: step 4015, loss 0.015561, acc 1, prec 0.0786714, recall 0.795273
2017-12-10T05:26:37.278484: step 4016, loss 0.265459, acc 0.953125, prec 0.078699, recall 0.795344
2017-12-10T05:26:37.547817: step 4017, loss 0.260288, acc 0.9375, prec 0.0786936, recall 0.795344
2017-12-10T05:26:37.814031: step 4018, loss 0.253255, acc 0.953125, prec 0.0786895, recall 0.795344
2017-12-10T05:26:38.081943: step 4019, loss 0.160599, acc 0.953125, prec 0.0786855, recall 0.795344
2017-12-10T05:26:38.351322: step 4020, loss 0.213085, acc 0.953125, prec 0.0786973, recall 0.79538
2017-12-10T05:26:38.615567: step 4021, loss 0.105681, acc 0.96875, prec 0.0787104, recall 0.795415
2017-12-10T05:26:38.880474: step 4022, loss 0.0482273, acc 0.984375, prec 0.078709, recall 0.795415
2017-12-10T05:26:39.146397: step 4023, loss 0.120624, acc 0.984375, prec 0.0787235, recall 0.795451
2017-12-10T05:26:39.411760: step 4024, loss 0.480574, acc 0.90625, prec 0.0787154, recall 0.795451
2017-12-10T05:26:39.671522: step 4025, loss 0.102561, acc 0.9375, prec 0.0787258, recall 0.795486
2017-12-10T05:26:39.937449: step 4026, loss 0.55349, acc 0.96875, prec 0.0787389, recall 0.795522
2017-12-10T05:26:40.207508: step 4027, loss 0.154218, acc 0.984375, prec 0.0787692, recall 0.795593
2017-12-10T05:26:40.475435: step 4028, loss 0.633089, acc 0.984375, prec 0.0787837, recall 0.795628
2017-12-10T05:26:40.744779: step 4029, loss 0.0749019, acc 0.96875, prec 0.0788127, recall 0.795699
2017-12-10T05:26:41.008913: step 4030, loss 0.0930764, acc 0.96875, prec 0.0788099, recall 0.795699
2017-12-10T05:26:41.282594: step 4031, loss 0.472249, acc 0.921875, prec 0.0788032, recall 0.795699
2017-12-10T05:26:41.547910: step 4032, loss 0.195975, acc 0.9375, prec 0.0788136, recall 0.795734
2017-12-10T05:26:41.809528: step 4033, loss 0.31413, acc 0.953125, prec 0.0788095, recall 0.795734
2017-12-10T05:26:42.080380: step 4034, loss 0.44692, acc 0.9375, prec 0.0788041, recall 0.795734
2017-12-10T05:26:42.349855: step 4035, loss 1.3128, acc 0.90625, prec 0.0788276, recall 0.795805
2017-12-10T05:26:42.622938: step 4036, loss 0.471295, acc 0.90625, prec 0.0788195, recall 0.795805
2017-12-10T05:26:42.887559: step 4037, loss 0.202241, acc 0.890625, prec 0.0788259, recall 0.795841
2017-12-10T05:26:43.153174: step 4038, loss 0.158899, acc 0.953125, prec 0.0788218, recall 0.795841
2017-12-10T05:26:43.412123: step 4039, loss 0.396866, acc 0.953125, prec 0.0788177, recall 0.795841
2017-12-10T05:26:43.680276: step 4040, loss 0.369981, acc 0.890625, prec 0.0788083, recall 0.795841
2017-12-10T05:26:43.939998: step 4041, loss 0.104243, acc 0.953125, prec 0.0788516, recall 0.795947
2017-12-10T05:26:44.204994: step 4042, loss 0.346977, acc 0.890625, prec 0.0789054, recall 0.796088
2017-12-10T05:26:44.467648: step 4043, loss 0.240351, acc 0.953125, prec 0.0789329, recall 0.796158
2017-12-10T05:26:44.727331: step 4044, loss 0.209772, acc 0.921875, prec 0.0789262, recall 0.796158
2017-12-10T05:26:45.000778: step 4045, loss 0.668104, acc 0.90625, prec 0.078918, recall 0.796158
2017-12-10T05:26:45.269054: step 4046, loss 0.226856, acc 0.90625, prec 0.0789257, recall 0.796194
2017-12-10T05:26:45.535487: step 4047, loss 0.444791, acc 0.9375, prec 0.0789519, recall 0.796264
2017-12-10T05:26:45.796495: step 4048, loss 0.0951806, acc 0.984375, prec 0.0789821, recall 0.796335
2017-12-10T05:26:46.062457: step 4049, loss 0.15766, acc 0.921875, prec 0.0789911, recall 0.79637
2017-12-10T05:26:46.328745: step 4050, loss 0.113326, acc 0.9375, prec 0.0790173, recall 0.79644
2017-12-10T05:26:46.589781: step 4051, loss 0.311524, acc 0.9375, prec 0.0790119, recall 0.79644
2017-12-10T05:26:46.867492: step 4052, loss 0.471746, acc 0.9375, prec 0.0790065, recall 0.79644
2017-12-10T05:26:47.131445: step 4053, loss 0.175723, acc 0.953125, prec 0.079034, recall 0.796511
2017-12-10T05:26:47.408679: step 4054, loss 0.310165, acc 0.953125, prec 0.0790457, recall 0.796546
2017-12-10T05:26:47.679058: step 4055, loss 0.0531907, acc 0.96875, prec 0.0790588, recall 0.796581
2017-12-10T05:26:47.944516: step 4056, loss 0.0484324, acc 0.984375, prec 0.0790732, recall 0.796616
2017-12-10T05:26:48.206925: step 4057, loss 0.307574, acc 0.953125, prec 0.0790691, recall 0.796616
2017-12-10T05:26:48.475609: step 4058, loss 0.0145952, acc 1, prec 0.0790691, recall 0.796616
2017-12-10T05:26:48.744329: step 4059, loss 1.3801, acc 0.984375, prec 0.0791309, recall 0.796756
2017-12-10T05:26:49.010052: step 4060, loss 0.14161, acc 0.96875, prec 0.079144, recall 0.796791
2017-12-10T05:26:49.283671: step 4061, loss 0.0737112, acc 0.953125, prec 0.0791399, recall 0.796791
2017-12-10T05:26:49.551260: step 4062, loss 0.00531934, acc 1, prec 0.0791399, recall 0.796791
2017-12-10T05:26:49.812768: step 4063, loss 0.0214474, acc 0.984375, prec 0.0791385, recall 0.796791
2017-12-10T05:26:50.078378: step 4064, loss 0.114837, acc 0.96875, prec 0.0791358, recall 0.796791
2017-12-10T05:26:50.351302: step 4065, loss 0.168792, acc 0.9375, prec 0.079162, recall 0.796862
2017-12-10T05:26:50.613170: step 4066, loss 0.162986, acc 0.96875, prec 0.079175, recall 0.796897
2017-12-10T05:26:50.874942: step 4067, loss 0.664614, acc 0.96875, prec 0.0792039, recall 0.796967
2017-12-10T05:26:51.142189: step 4068, loss 0.0304716, acc 1, prec 0.0792196, recall 0.797002
2017-12-10T05:26:51.407669: step 4069, loss 0.257564, acc 0.921875, prec 0.0792128, recall 0.797002
2017-12-10T05:26:51.680368: step 4070, loss 0.296875, acc 0.890625, prec 0.0792033, recall 0.797002
2017-12-10T05:26:51.945989: step 4071, loss 0.0882022, acc 0.96875, prec 0.0792164, recall 0.797037
2017-12-10T05:26:52.219503: step 4072, loss 0.182878, acc 0.9375, prec 0.079211, recall 0.797037
2017-12-10T05:26:52.491508: step 4073, loss 0.0463406, acc 0.984375, prec 0.0792096, recall 0.797037
2017-12-10T05:26:52.763410: step 4074, loss 0.0868627, acc 0.984375, prec 0.079224, recall 0.797072
2017-12-10T05:26:53.028947: step 4075, loss 0.196748, acc 0.9375, prec 0.0792186, recall 0.797072
2017-12-10T05:26:53.292281: step 4076, loss 0.112527, acc 0.9375, prec 0.0792132, recall 0.797072
2017-12-10T05:26:53.553817: step 4077, loss 0.212032, acc 0.9375, prec 0.0792393, recall 0.797141
2017-12-10T05:26:53.820564: step 4078, loss 0.291011, acc 0.9375, prec 0.0792338, recall 0.797141
2017-12-10T05:26:54.087372: step 4079, loss 0.803034, acc 0.953125, prec 0.0792455, recall 0.797176
2017-12-10T05:26:54.360718: step 4080, loss 0.217185, acc 0.96875, prec 0.0792586, recall 0.797211
2017-12-10T05:26:54.630459: step 4081, loss 0.100285, acc 0.921875, prec 0.0792676, recall 0.797246
2017-12-10T05:26:54.894016: step 4082, loss 0.546145, acc 0.953125, prec 0.079295, recall 0.797316
2017-12-10T05:26:55.158846: step 4083, loss 0.124909, acc 0.953125, prec 0.0793224, recall 0.797386
2017-12-10T05:26:55.424564: step 4084, loss 0.356081, acc 0.90625, prec 0.0793458, recall 0.797455
2017-12-10T05:26:55.687882: step 4085, loss 0.249444, acc 0.9375, prec 0.0793404, recall 0.797455
2017-12-10T05:26:55.957950: step 4086, loss 0.0819839, acc 0.984375, prec 0.0793548, recall 0.79749
2017-12-10T05:26:56.224393: step 4087, loss 0.408275, acc 0.921875, prec 0.0793795, recall 0.79756
2017-12-10T05:26:56.492744: step 4088, loss 0.363532, acc 0.953125, prec 0.0793754, recall 0.79756
2017-12-10T05:26:56.755825: step 4089, loss 0.209895, acc 0.96875, prec 0.0794042, recall 0.797629
2017-12-10T05:26:57.021755: step 4090, loss 0.601791, acc 0.859375, prec 0.0793919, recall 0.797629
2017-12-10T05:26:57.291086: step 4091, loss 0.118538, acc 0.984375, prec 0.0793906, recall 0.797629
2017-12-10T05:26:57.553313: step 4092, loss 0.135705, acc 0.953125, prec 0.079418, recall 0.797699
2017-12-10T05:26:57.823418: step 4093, loss 0.210284, acc 0.953125, prec 0.0794454, recall 0.797768
2017-12-10T05:26:58.091178: step 4094, loss 0.144882, acc 0.96875, prec 0.0794584, recall 0.797803
2017-12-10T05:26:58.359437: step 4095, loss 0.0354101, acc 0.984375, prec 0.0794571, recall 0.797803
2017-12-10T05:26:58.634826: step 4096, loss 0.228025, acc 0.96875, prec 0.0794543, recall 0.797803
2017-12-10T05:26:58.899719: step 4097, loss 0.139952, acc 0.96875, prec 0.0794831, recall 0.797872
2017-12-10T05:26:59.164111: step 4098, loss 0.0690284, acc 0.984375, prec 0.0794975, recall 0.797907
2017-12-10T05:26:59.428746: step 4099, loss 0.0984587, acc 0.984375, prec 0.0794961, recall 0.797907
2017-12-10T05:26:59.695375: step 4100, loss 0.133739, acc 0.984375, prec 0.0794948, recall 0.797907
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-4100

2017-12-10T05:27:00.933084: step 4101, loss 0.070889, acc 0.984375, prec 0.0794934, recall 0.797907
2017-12-10T05:27:01.206004: step 4102, loss 0.255737, acc 0.953125, prec 0.0795051, recall 0.797942
2017-12-10T05:27:01.468940: step 4103, loss 0.412749, acc 0.953125, prec 0.0795167, recall 0.797976
2017-12-10T05:27:01.741575: step 4104, loss 0.089567, acc 0.96875, prec 0.0795455, recall 0.798046
2017-12-10T05:27:02.006947: step 4105, loss 0.254955, acc 0.96875, prec 0.0795585, recall 0.79808
2017-12-10T05:27:02.273744: step 4106, loss 0.017593, acc 0.984375, prec 0.0795571, recall 0.79808
2017-12-10T05:27:02.541097: step 4107, loss 0.110073, acc 0.96875, prec 0.0795858, recall 0.798149
2017-12-10T05:27:02.811580: step 4108, loss 2.18704, acc 0.984375, prec 0.0796016, recall 0.798047
2017-12-10T05:27:03.078567: step 4109, loss 6.37789, acc 0.953125, prec 0.0796146, recall 0.797945
2017-12-10T05:27:03.348019: step 4110, loss 0.32225, acc 0.9375, prec 0.0796249, recall 0.79798
2017-12-10T05:27:03.616589: step 4111, loss 0.323468, acc 0.90625, prec 0.0796481, recall 0.798049
2017-12-10T05:27:03.880285: step 4112, loss 0.267344, acc 0.96875, prec 0.0796769, recall 0.798118
2017-12-10T05:27:04.156882: step 4113, loss 0.203135, acc 0.953125, prec 0.0797042, recall 0.798187
2017-12-10T05:27:04.426007: step 4114, loss 0.447175, acc 0.875, prec 0.0796933, recall 0.798187
2017-12-10T05:27:04.690715: step 4115, loss 0.658687, acc 0.84375, prec 0.0796954, recall 0.798222
2017-12-10T05:27:04.950848: step 4116, loss 0.390566, acc 0.875, prec 0.0797002, recall 0.798256
2017-12-10T05:27:05.223069: step 4117, loss 0.538222, acc 0.875, prec 0.0797365, recall 0.79836
2017-12-10T05:27:05.485760: step 4118, loss 1.07253, acc 0.84375, prec 0.0797543, recall 0.798428
2017-12-10T05:27:05.755998: step 4119, loss 0.858155, acc 0.734375, prec 0.0797312, recall 0.798428
2017-12-10T05:27:06.019271: step 4120, loss 0.786028, acc 0.828125, prec 0.0797162, recall 0.798428
2017-12-10T05:27:06.285070: step 4121, loss 0.533807, acc 0.828125, prec 0.0797169, recall 0.798463
2017-12-10T05:27:06.549969: step 4122, loss 0.838248, acc 0.8125, prec 0.0797006, recall 0.798463
2017-12-10T05:27:06.813380: step 4123, loss 1.07749, acc 0.84375, prec 0.0797027, recall 0.798497
2017-12-10T05:27:07.081764: step 4124, loss 0.49893, acc 0.890625, prec 0.0797246, recall 0.798566
2017-12-10T05:27:07.346507: step 4125, loss 0.448705, acc 0.890625, prec 0.0797308, recall 0.7986
2017-12-10T05:27:07.612812: step 4126, loss 0.62168, acc 0.875, prec 0.0797513, recall 0.798669
2017-12-10T05:27:07.879509: step 4127, loss 0.667485, acc 0.90625, prec 0.0797431, recall 0.798669
2017-12-10T05:27:08.143687: step 4128, loss 0.292058, acc 0.90625, prec 0.0797506, recall 0.798703
2017-12-10T05:27:08.412654: step 4129, loss 0.919918, acc 0.875, prec 0.0798181, recall 0.798875
2017-12-10T05:27:08.677971: step 4130, loss 0.159507, acc 0.96875, prec 0.0798781, recall 0.799012
2017-12-10T05:27:08.945837: step 4131, loss 0.198148, acc 0.921875, prec 0.0798869, recall 0.799046
2017-12-10T05:27:09.210050: step 4132, loss 0.034383, acc 0.984375, prec 0.0799012, recall 0.799081
2017-12-10T05:27:09.484422: step 4133, loss 0.435989, acc 0.9375, prec 0.0799115, recall 0.799115
2017-12-10T05:27:09.756596: step 4134, loss 0.240798, acc 0.96875, prec 0.0799244, recall 0.799149
2017-12-10T05:27:10.029136: step 4135, loss 0.229231, acc 0.9375, prec 0.0799346, recall 0.799183
2017-12-10T05:27:10.300119: step 4136, loss 0.0372116, acc 0.984375, prec 0.0799489, recall 0.799217
2017-12-10T05:27:10.566720: step 4137, loss 0.144953, acc 0.953125, prec 0.0799605, recall 0.799251
2017-12-10T05:27:10.840584: step 4138, loss 0.054239, acc 0.96875, prec 0.0799578, recall 0.799251
2017-12-10T05:27:11.106481: step 4139, loss 0.120822, acc 0.984375, prec 0.0800347, recall 0.799422
2017-12-10T05:27:11.370510: step 4140, loss 0.114407, acc 0.984375, prec 0.0800334, recall 0.799422
2017-12-10T05:27:11.642950: step 4141, loss 0.0830252, acc 0.984375, prec 0.080032, recall 0.799422
2017-12-10T05:27:11.905079: step 4142, loss 0.0405818, acc 0.96875, prec 0.0800449, recall 0.799456
2017-12-10T05:27:12.176656: step 4143, loss 0.00332457, acc 1, prec 0.0800449, recall 0.799456
2017-12-10T05:27:12.447393: step 4144, loss 0.103441, acc 1, prec 0.0800606, recall 0.79949
2017-12-10T05:27:12.725955: step 4145, loss 0.0627604, acc 0.984375, prec 0.0800592, recall 0.79949
2017-12-10T05:27:12.996125: step 4146, loss 0.00842015, acc 1, prec 0.0800905, recall 0.799558
2017-12-10T05:27:13.264472: step 4147, loss 0.113872, acc 0.984375, prec 0.0801048, recall 0.799592
2017-12-10T05:27:13.526742: step 4148, loss 1.83331, acc 0.984375, prec 0.0801205, recall 0.799491
2017-12-10T05:27:13.800367: step 4149, loss 0.0933164, acc 0.953125, prec 0.080132, recall 0.799525
2017-12-10T05:27:14.074728: step 4150, loss 0.00244863, acc 1, prec 0.080132, recall 0.799525
2017-12-10T05:27:14.348263: step 4151, loss 0.0752846, acc 0.984375, prec 0.0801463, recall 0.799559
2017-12-10T05:27:14.611341: step 4152, loss 0.213653, acc 0.921875, prec 0.0801551, recall 0.799593
2017-12-10T05:27:14.877309: step 4153, loss 0.0896111, acc 1, prec 0.0802177, recall 0.799729
2017-12-10T05:27:15.143577: step 4154, loss 0.140335, acc 0.96875, prec 0.0802306, recall 0.799763
2017-12-10T05:27:15.413306: step 4155, loss 0.0546354, acc 0.984375, prec 0.0802449, recall 0.799797
2017-12-10T05:27:15.677669: step 4156, loss 0.0230343, acc 0.984375, prec 0.0802436, recall 0.799797
2017-12-10T05:27:15.946961: step 4157, loss 0.172007, acc 0.953125, prec 0.0802707, recall 0.799864
2017-12-10T05:27:16.215088: step 4158, loss 0.014197, acc 0.984375, prec 0.0802694, recall 0.799864
2017-12-10T05:27:16.489905: step 4159, loss 0.0393035, acc 0.984375, prec 0.0802837, recall 0.799898
2017-12-10T05:27:16.754326: step 4160, loss 0.0057496, acc 1, prec 0.0803149, recall 0.799966
2017-12-10T05:27:17.025883: step 4161, loss 0.0677459, acc 0.984375, prec 0.0803136, recall 0.799966
2017-12-10T05:27:17.292314: step 4162, loss 0.185197, acc 0.96875, prec 0.0803265, recall 0.8
2017-12-10T05:27:17.556075: step 4163, loss 0.0985869, acc 0.96875, prec 0.0803863, recall 0.800135
2017-12-10T05:27:17.829483: step 4164, loss 0.294505, acc 0.921875, prec 0.0803951, recall 0.800169
2017-12-10T05:27:18.092942: step 4165, loss 0.0998009, acc 0.96875, prec 0.080408, recall 0.800203
2017-12-10T05:27:18.355732: step 4166, loss 0.272299, acc 0.984375, prec 0.0804066, recall 0.800203
2017-12-10T05:27:18.625309: step 4167, loss 0.315469, acc 0.984375, prec 0.0804365, recall 0.800271
2017-12-10T05:27:18.891773: step 4168, loss 0.0218216, acc 0.984375, prec 0.0804351, recall 0.800271
2017-12-10T05:27:19.159462: step 4169, loss 0.116369, acc 0.984375, prec 0.0804494, recall 0.800304
2017-12-10T05:27:19.428509: step 4170, loss 0.186312, acc 0.9375, prec 0.0804596, recall 0.800338
2017-12-10T05:27:19.693285: step 4171, loss 0.254145, acc 0.984375, prec 0.0804738, recall 0.800372
2017-12-10T05:27:19.978231: step 4172, loss 0.0381315, acc 0.984375, prec 0.0804725, recall 0.800372
2017-12-10T05:27:20.251596: step 4173, loss 0.422755, acc 0.921875, prec 0.0804813, recall 0.800406
2017-12-10T05:27:20.520448: step 4174, loss 0.0128058, acc 1, prec 0.0804813, recall 0.800406
2017-12-10T05:27:20.793003: step 4175, loss 0.0467965, acc 0.984375, prec 0.0805111, recall 0.800473
2017-12-10T05:27:21.058818: step 4176, loss 0.343244, acc 0.953125, prec 0.0805539, recall 0.800574
2017-12-10T05:27:21.327228: step 4177, loss 0.594185, acc 0.90625, prec 0.0805613, recall 0.800608
2017-12-10T05:27:21.599914: step 4178, loss 0.282656, acc 0.953125, prec 0.0805884, recall 0.800675
2017-12-10T05:27:21.869011: step 4179, loss 0.0448804, acc 0.984375, prec 0.0806027, recall 0.800709
2017-12-10T05:27:22.133332: step 4180, loss 0.426755, acc 0.953125, prec 0.0806142, recall 0.800742
2017-12-10T05:27:22.400284: step 4181, loss 0.302182, acc 0.921875, prec 0.0806698, recall 0.800877
2017-12-10T05:27:22.662626: step 4182, loss 0.0380022, acc 0.96875, prec 0.0806671, recall 0.800877
2017-12-10T05:27:22.928946: step 4183, loss 0.0176388, acc 1, prec 0.0806983, recall 0.800944
2017-12-10T05:27:23.191295: step 4184, loss 0.0294504, acc 0.984375, prec 0.0806969, recall 0.800944
2017-12-10T05:27:23.470606: step 4185, loss 0.0109805, acc 1, prec 0.0806969, recall 0.800944
2017-12-10T05:27:23.740221: step 4186, loss 0.0160519, acc 1, prec 0.0806969, recall 0.800944
2017-12-10T05:27:24.014814: step 4187, loss 0.225973, acc 0.96875, prec 0.0806942, recall 0.800944
2017-12-10T05:27:24.284459: step 4188, loss 2.26526, acc 0.953125, prec 0.0807071, recall 0.800842
2017-12-10T05:27:24.556417: step 4189, loss 0.302124, acc 0.96875, prec 0.0807668, recall 0.800977
2017-12-10T05:27:24.824683: step 4190, loss 0.0653474, acc 0.96875, prec 0.080764, recall 0.800977
2017-12-10T05:27:25.088482: step 4191, loss 0.351692, acc 0.953125, prec 0.0807599, recall 0.800977
2017-12-10T05:27:25.362305: step 4192, loss 0.213953, acc 0.9375, prec 0.08077, recall 0.80101
2017-12-10T05:27:25.629541: step 4193, loss 0.201005, acc 0.953125, prec 0.0807815, recall 0.801044
2017-12-10T05:27:25.891971: step 4194, loss 0.120703, acc 0.953125, prec 0.080793, recall 0.801077
2017-12-10T05:27:26.166379: step 4195, loss 0.203359, acc 0.921875, prec 0.0807861, recall 0.801077
2017-12-10T05:27:26.441764: step 4196, loss 0.429869, acc 0.9375, prec 0.0807807, recall 0.801077
2017-12-10T05:27:26.711098: step 4197, loss 0.513584, acc 0.953125, prec 0.0807921, recall 0.801111
2017-12-10T05:27:26.976867: step 4198, loss 0.152935, acc 0.953125, prec 0.080788, recall 0.801111
2017-12-10T05:27:27.253638: step 4199, loss 0.519226, acc 0.890625, prec 0.0808096, recall 0.801177
2017-12-10T05:27:27.523972: step 4200, loss 0.357122, acc 0.921875, prec 0.080834, recall 0.801244

Evaluation:
2017-12-10T05:27:35.103332: step 4200, loss 3.04086, acc 0.914701, prec 0.0810549, recall 0.795313

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-4200

2017-12-10T05:27:36.484956: step 4201, loss 1.48355, acc 0.96875, prec 0.0810689, recall 0.795216
2017-12-10T05:27:36.757659: step 4202, loss 0.135819, acc 0.9375, prec 0.0810635, recall 0.795216
2017-12-10T05:27:37.017647: step 4203, loss 0.248546, acc 0.890625, prec 0.0811, recall 0.795317
2017-12-10T05:27:37.289703: step 4204, loss 0.590417, acc 0.890625, prec 0.0810906, recall 0.795317
2017-12-10T05:27:37.550264: step 4205, loss 0.406868, acc 0.875, prec 0.0810797, recall 0.795317
2017-12-10T05:27:37.813221: step 4206, loss 0.308233, acc 0.890625, prec 0.0810703, recall 0.795317
2017-12-10T05:27:38.080373: step 4207, loss 0.50685, acc 0.890625, prec 0.0810761, recall 0.79535
2017-12-10T05:27:38.348030: step 4208, loss 0.183357, acc 0.90625, prec 0.081068, recall 0.79535
2017-12-10T05:27:38.612216: step 4209, loss 0.533404, acc 0.8125, prec 0.0810824, recall 0.795417
2017-12-10T05:27:38.879893: step 4210, loss 0.449771, acc 0.890625, prec 0.0810883, recall 0.795451
2017-12-10T05:27:39.142848: step 4211, loss 0.171971, acc 0.9375, prec 0.0810829, recall 0.795451
2017-12-10T05:27:39.405410: step 4212, loss 0.416046, acc 0.9375, prec 0.0810775, recall 0.795451
2017-12-10T05:27:39.672207: step 4213, loss 0.169224, acc 0.9375, prec 0.0810721, recall 0.795451
2017-12-10T05:27:39.935752: step 4214, loss 0.215245, acc 0.890625, prec 0.0810626, recall 0.795451
2017-12-10T05:27:40.201517: step 4215, loss 0.0899684, acc 0.9375, prec 0.0810572, recall 0.795451
2017-12-10T05:27:40.471789: step 4216, loss 0.07858, acc 0.96875, prec 0.0810698, recall 0.795484
2017-12-10T05:27:40.745940: step 4217, loss 0.365471, acc 0.9375, prec 0.0810644, recall 0.795484
2017-12-10T05:27:41.017380: step 4218, loss 0.114754, acc 0.96875, prec 0.0810923, recall 0.795551
2017-12-10T05:27:41.282357: step 4219, loss 0.121882, acc 0.953125, prec 0.0810883, recall 0.795551
2017-12-10T05:27:41.555257: step 4220, loss 0.0999279, acc 0.984375, prec 0.0810869, recall 0.795551
2017-12-10T05:27:41.823560: step 4221, loss 0.0730318, acc 0.96875, prec 0.0810842, recall 0.795551
2017-12-10T05:27:42.093885: step 4222, loss 0.0891731, acc 0.96875, prec 0.0811122, recall 0.795618
2017-12-10T05:27:42.367221: step 4223, loss 0.0040363, acc 1, prec 0.0811122, recall 0.795618
2017-12-10T05:27:42.636104: step 4224, loss 0.144433, acc 0.96875, prec 0.0811248, recall 0.795651
2017-12-10T05:27:42.899873: step 4225, loss 0.147276, acc 0.984375, prec 0.0811387, recall 0.795685
2017-12-10T05:27:43.170147: step 4226, loss 0.0565136, acc 0.984375, prec 0.0811374, recall 0.795685
2017-12-10T05:27:43.432988: step 4227, loss 0.253162, acc 0.96875, prec 0.0811959, recall 0.795818
2017-12-10T05:27:43.703693: step 4228, loss 0.268111, acc 1, prec 0.0812113, recall 0.795852
2017-12-10T05:27:43.971053: step 4229, loss 0.52364, acc 1, prec 0.0812419, recall 0.795918
2017-12-10T05:27:44.239435: step 4230, loss 0.020256, acc 0.984375, prec 0.0812405, recall 0.795918
2017-12-10T05:27:44.514922: step 4231, loss 0.0996026, acc 0.984375, prec 0.0812545, recall 0.795952
2017-12-10T05:27:44.778846: step 4232, loss 0.35165, acc 1, prec 0.0812851, recall 0.796018
2017-12-10T05:27:45.047418: step 4233, loss 0.016539, acc 0.984375, prec 0.0812837, recall 0.796018
2017-12-10T05:27:45.320320: step 4234, loss 0.169703, acc 0.953125, prec 0.081295, recall 0.796052
2017-12-10T05:27:45.586364: step 4235, loss 0.024878, acc 0.984375, prec 0.0812936, recall 0.796052
2017-12-10T05:27:45.852594: step 4236, loss 0.126871, acc 0.96875, prec 0.0812909, recall 0.796052
2017-12-10T05:27:46.132263: step 4237, loss 0.17731, acc 0.953125, prec 0.0812869, recall 0.796052
2017-12-10T05:27:46.397304: step 4238, loss 0.24057, acc 0.9375, prec 0.0812814, recall 0.796052
2017-12-10T05:27:46.674864: step 4239, loss 0.0409193, acc 0.984375, prec 0.0812954, recall 0.796085
2017-12-10T05:27:46.950750: step 4240, loss 0.0344874, acc 0.984375, prec 0.0813093, recall 0.796118
2017-12-10T05:27:47.223430: step 4241, loss 0.00998642, acc 1, prec 0.0813247, recall 0.796151
2017-12-10T05:27:47.489008: step 4242, loss 0.182418, acc 0.921875, prec 0.0813179, recall 0.796151
2017-12-10T05:27:47.760063: step 4243, loss 0.129446, acc 0.96875, prec 0.0813305, recall 0.796185
2017-12-10T05:27:48.025919: step 4244, loss 0.71222, acc 0.953125, prec 0.081357, recall 0.796251
2017-12-10T05:27:48.291675: step 4245, loss 0.155059, acc 0.984375, prec 0.0813862, recall 0.796317
2017-12-10T05:27:48.559420: step 4246, loss 0.265609, acc 0.9375, prec 0.0813808, recall 0.796317
2017-12-10T05:27:48.828457: step 4247, loss 0.223101, acc 0.953125, prec 0.0813921, recall 0.796351
2017-12-10T05:27:49.099800: step 4248, loss 0.340772, acc 0.921875, prec 0.0814312, recall 0.79645
2017-12-10T05:27:49.376765: step 4249, loss 0.155453, acc 0.9375, prec 0.081441, recall 0.796483
2017-12-10T05:27:49.653684: step 4250, loss 0.105202, acc 0.96875, prec 0.0814536, recall 0.796516
2017-12-10T05:27:49.917576: step 4251, loss 0.100427, acc 0.953125, prec 0.0814495, recall 0.796516
2017-12-10T05:27:50.185617: step 4252, loss 0.298527, acc 0.9375, prec 0.0814594, recall 0.796549
2017-12-10T05:27:50.455068: step 4253, loss 0.342638, acc 0.90625, prec 0.0814666, recall 0.796583
2017-12-10T05:27:50.732284: step 4254, loss 0.169181, acc 0.953125, prec 0.0814778, recall 0.796616
2017-12-10T05:27:51.000796: step 4255, loss 0.546509, acc 0.96875, prec 0.0814904, recall 0.796649
2017-12-10T05:27:51.268944: step 4256, loss 0.0479943, acc 0.984375, prec 0.0815043, recall 0.796682
2017-12-10T05:27:51.537747: step 4257, loss 0.123545, acc 0.96875, prec 0.0815016, recall 0.796682
2017-12-10T05:27:51.807867: step 4258, loss 0.483732, acc 0.875, prec 0.0815213, recall 0.796748
2017-12-10T05:27:52.076581: step 4259, loss 0.286902, acc 0.9375, prec 0.0815311, recall 0.796781
2017-12-10T05:27:52.349022: step 4260, loss 0.301254, acc 0.921875, prec 0.0815855, recall 0.796913
2017-12-10T05:27:52.616806: step 4261, loss 0.132672, acc 0.96875, prec 0.0815828, recall 0.796913
2017-12-10T05:27:52.880814: step 4262, loss 0.331154, acc 0.90625, prec 0.0815899, recall 0.796946
2017-12-10T05:27:53.147215: step 4263, loss 0.190697, acc 0.96875, prec 0.0816024, recall 0.796979
2017-12-10T05:27:53.416915: step 4264, loss 0.0159865, acc 1, prec 0.0816024, recall 0.796979
2017-12-10T05:27:53.684371: step 4265, loss 0.0775804, acc 0.96875, prec 0.0815997, recall 0.796979
2017-12-10T05:27:53.959576: step 4266, loss 0.0963566, acc 0.953125, prec 0.0816109, recall 0.797012
2017-12-10T05:27:54.222168: step 4267, loss 0.306659, acc 0.96875, prec 0.0816235, recall 0.797045
2017-12-10T05:27:54.487119: step 4268, loss 0.193058, acc 0.953125, prec 0.0816194, recall 0.797045
2017-12-10T05:27:54.762218: step 4269, loss 0.104356, acc 0.96875, prec 0.0816167, recall 0.797045
2017-12-10T05:27:55.028155: step 4270, loss 0.00685466, acc 1, prec 0.0816167, recall 0.797045
2017-12-10T05:27:55.294270: step 4271, loss 0.409638, acc 0.953125, prec 0.0816432, recall 0.797111
2017-12-10T05:27:55.561708: step 4272, loss 0.0896096, acc 0.96875, prec 0.081671, recall 0.797177
2017-12-10T05:27:55.828241: step 4273, loss 0.207912, acc 0.921875, prec 0.0816795, recall 0.79721
2017-12-10T05:27:56.102594: step 4274, loss 0.182478, acc 0.9375, prec 0.081674, recall 0.79721
2017-12-10T05:27:56.366292: step 4275, loss 0.138781, acc 0.953125, prec 0.0817005, recall 0.797275
2017-12-10T05:27:56.634672: step 4276, loss 0.0126962, acc 1, prec 0.0817005, recall 0.797275
2017-12-10T05:27:56.914973: step 4277, loss 0.127917, acc 0.96875, prec 0.0816978, recall 0.797275
2017-12-10T05:27:57.191848: step 4278, loss 0.298915, acc 0.953125, prec 0.0816937, recall 0.797275
2017-12-10T05:27:57.462137: step 4279, loss 0.0270726, acc 0.984375, prec 0.0817076, recall 0.797308
2017-12-10T05:27:57.725783: step 4280, loss 0.22422, acc 0.984375, prec 0.0817215, recall 0.797341
2017-12-10T05:27:58.001728: step 4281, loss 0.106482, acc 0.96875, prec 0.0817493, recall 0.797407
2017-12-10T05:27:58.268490: step 4282, loss 0.224605, acc 0.953125, prec 0.0817605, recall 0.79744
2017-12-10T05:27:58.534385: step 4283, loss 3.87168, acc 0.96875, prec 0.0817591, recall 0.79731
2017-12-10T05:27:58.800326: step 4284, loss 0.575921, acc 0.96875, prec 0.0817717, recall 0.797343
2017-12-10T05:27:59.069742: step 4285, loss 0.47662, acc 1, prec 0.0818174, recall 0.797442
2017-12-10T05:27:59.346234: step 4286, loss 0.0130806, acc 1, prec 0.0818327, recall 0.797475
2017-12-10T05:27:59.611120: step 4287, loss 0.0915103, acc 0.953125, prec 0.0818286, recall 0.797475
2017-12-10T05:27:59.882976: step 4288, loss 0.255958, acc 0.984375, prec 0.0818272, recall 0.797475
2017-12-10T05:28:00.145031: step 4289, loss 0.552734, acc 0.9375, prec 0.0818371, recall 0.797507
2017-12-10T05:28:00.421503: step 4290, loss 0.156443, acc 0.9375, prec 0.0818469, recall 0.79754
2017-12-10T05:28:00.699967: step 4291, loss 0.150105, acc 0.921875, prec 0.0818553, recall 0.797573
2017-12-10T05:28:00.962487: step 4292, loss 0.179711, acc 0.96875, prec 0.0818526, recall 0.797573
2017-12-10T05:28:01.225925: step 4293, loss 0.192004, acc 0.9375, prec 0.0818929, recall 0.797671
2017-12-10T05:28:01.488546: step 4294, loss 0.181035, acc 0.921875, prec 0.0819166, recall 0.797736
2017-12-10T05:28:01.760727: step 4295, loss 0.228535, acc 0.90625, prec 0.0819084, recall 0.797736
2017-12-10T05:28:02.032779: step 4296, loss 0.384787, acc 0.921875, prec 0.0819016, recall 0.797736
2017-12-10T05:28:02.298668: step 4297, loss 0.277272, acc 0.890625, prec 0.0819074, recall 0.797769
2017-12-10T05:28:02.570210: step 4298, loss 0.471406, acc 0.90625, prec 0.0819297, recall 0.797835
2017-12-10T05:28:02.835469: step 4299, loss 0.832103, acc 0.859375, prec 0.0819327, recall 0.797867
2017-12-10T05:28:03.100025: step 4300, loss 0.39916, acc 0.875, prec 0.081937, recall 0.7979
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-4300

2017-12-10T05:28:04.416177: step 4301, loss 0.225391, acc 0.953125, prec 0.0819329, recall 0.7979
2017-12-10T05:28:04.684909: step 4302, loss 0.217354, acc 0.921875, prec 0.0819414, recall 0.797933
2017-12-10T05:28:04.950139: step 4303, loss 0.226049, acc 0.921875, prec 0.0819346, recall 0.797933
2017-12-10T05:28:05.220187: step 4304, loss 0.143551, acc 0.96875, prec 0.0819319, recall 0.797933
2017-12-10T05:28:05.487253: step 4305, loss 0.123483, acc 0.96875, prec 0.0819596, recall 0.797998
2017-12-10T05:28:05.756588: step 4306, loss 0.215187, acc 0.953125, prec 0.081986, recall 0.798063
2017-12-10T05:28:06.016931: step 4307, loss 0.0669505, acc 0.96875, prec 0.0819833, recall 0.798063
2017-12-10T05:28:06.286663: step 4308, loss 0.118339, acc 0.984375, prec 0.0819971, recall 0.798096
2017-12-10T05:28:06.554436: step 4309, loss 0.41003, acc 0.921875, prec 0.0820208, recall 0.798161
2017-12-10T05:28:06.826765: step 4310, loss 0.0671044, acc 0.96875, prec 0.082018, recall 0.798161
2017-12-10T05:28:07.095709: step 4311, loss 0.036299, acc 0.96875, prec 0.0820153, recall 0.798161
2017-12-10T05:28:07.357881: step 4312, loss 0.084373, acc 0.96875, prec 0.0820126, recall 0.798161
2017-12-10T05:28:07.620215: step 4313, loss 0.163365, acc 0.96875, prec 0.0820099, recall 0.798161
2017-12-10T05:28:07.887310: step 4314, loss 0.0349148, acc 0.96875, prec 0.0820072, recall 0.798161
2017-12-10T05:28:08.154189: step 4315, loss 0.078661, acc 1, prec 0.0820376, recall 0.798226
2017-12-10T05:28:08.420515: step 4316, loss 0.0147039, acc 1, prec 0.0820376, recall 0.798226
2017-12-10T05:28:08.691742: step 4317, loss 0.00474714, acc 1, prec 0.0820832, recall 0.798323
2017-12-10T05:28:08.952404: step 4318, loss 0.0329694, acc 0.984375, prec 0.0820971, recall 0.798356
2017-12-10T05:28:09.212765: step 4319, loss 0.23933, acc 0.984375, prec 0.0821109, recall 0.798388
2017-12-10T05:28:09.482021: step 4320, loss 0.0036915, acc 1, prec 0.0821109, recall 0.798388
2017-12-10T05:28:09.746435: step 4321, loss 1.77827, acc 0.953125, prec 0.0821234, recall 0.798292
2017-12-10T05:28:10.013692: step 4322, loss 0.105199, acc 0.984375, prec 0.0821677, recall 0.79839
2017-12-10T05:28:10.280695: step 4323, loss 0.157639, acc 0.984375, prec 0.0821968, recall 0.798455
2017-12-10T05:28:10.545349: step 4324, loss 0.149212, acc 0.953125, prec 0.0821927, recall 0.798455
2017-12-10T05:28:10.807715: step 4325, loss 0.140587, acc 0.96875, prec 0.0822356, recall 0.798552
2017-12-10T05:28:11.074682: step 4326, loss 0.102908, acc 0.984375, prec 0.0822342, recall 0.798552
2017-12-10T05:28:11.338782: step 4327, loss 0.115323, acc 0.96875, prec 0.0822315, recall 0.798552
2017-12-10T05:28:11.606498: step 4328, loss 0.157813, acc 0.96875, prec 0.082244, recall 0.798584
2017-12-10T05:28:11.877128: step 4329, loss 0.0539958, acc 0.984375, prec 0.0822578, recall 0.798617
2017-12-10T05:28:12.147455: step 4330, loss 2.07727, acc 0.953125, prec 0.0822551, recall 0.798488
2017-12-10T05:28:12.419523: step 4331, loss 0.0224331, acc 1, prec 0.0822551, recall 0.798488
2017-12-10T05:28:12.686837: step 4332, loss 0.409858, acc 0.875, prec 0.0822442, recall 0.798488
2017-12-10T05:28:12.954937: step 4333, loss 0.401946, acc 0.90625, prec 0.082236, recall 0.798488
2017-12-10T05:28:13.215041: step 4334, loss 0.30914, acc 0.859375, prec 0.0822238, recall 0.798488
2017-12-10T05:28:13.486372: step 4335, loss 0.556534, acc 0.859375, prec 0.0822419, recall 0.798553
2017-12-10T05:28:13.760262: step 4336, loss 0.571477, acc 0.9375, prec 0.0822517, recall 0.798585
2017-12-10T05:28:14.027940: step 4337, loss 0.236962, acc 0.953125, prec 0.0822932, recall 0.798683
2017-12-10T05:28:14.296742: step 4338, loss 0.380182, acc 0.953125, prec 0.0823043, recall 0.798715
2017-12-10T05:28:14.560687: step 4339, loss 0.649977, acc 0.890625, prec 0.0822947, recall 0.798715
2017-12-10T05:28:14.824282: step 4340, loss 0.299974, acc 0.90625, prec 0.0823017, recall 0.798747
2017-12-10T05:28:15.095888: step 4341, loss 0.435768, acc 0.890625, prec 0.0823226, recall 0.798812
2017-12-10T05:28:15.361239: step 4342, loss 0.548121, acc 0.8125, prec 0.0823062, recall 0.798812
2017-12-10T05:28:15.625990: step 4343, loss 0.214289, acc 0.890625, prec 0.0822967, recall 0.798812
2017-12-10T05:28:15.889723: step 4344, loss 0.096321, acc 0.953125, prec 0.0823078, recall 0.798844
2017-12-10T05:28:16.155675: step 4345, loss 0.257122, acc 0.90625, prec 0.0822996, recall 0.798844
2017-12-10T05:28:16.427583: step 4346, loss 0.39413, acc 0.875, prec 0.0823039, recall 0.798876
2017-12-10T05:28:16.699774: step 4347, loss 0.172202, acc 0.921875, prec 0.0823123, recall 0.798909
2017-12-10T05:28:16.970403: step 4348, loss 0.406988, acc 0.890625, prec 0.0823028, recall 0.798909
2017-12-10T05:28:17.242491: step 4349, loss 0.573753, acc 0.90625, prec 0.0822946, recall 0.798909
2017-12-10T05:28:17.510117: step 4350, loss 0.237288, acc 0.9375, prec 0.0823043, recall 0.798941
2017-12-10T05:28:17.783197: step 4351, loss 0.0556589, acc 0.96875, prec 0.0823319, recall 0.799005
2017-12-10T05:28:18.056306: step 4352, loss 0.183286, acc 0.9375, prec 0.0823568, recall 0.79907
2017-12-10T05:28:18.326812: step 4353, loss 0.113387, acc 0.96875, prec 0.0823541, recall 0.79907
2017-12-10T05:28:18.591022: step 4354, loss 0.255746, acc 0.96875, prec 0.0823817, recall 0.799134
2017-12-10T05:28:18.855977: step 4355, loss 0.0758659, acc 0.953125, prec 0.0823776, recall 0.799134
2017-12-10T05:28:19.123423: step 4356, loss 0.0021646, acc 1, prec 0.0823776, recall 0.799134
2017-12-10T05:28:19.384796: step 4357, loss 0.309198, acc 0.921875, prec 0.082386, recall 0.799167
2017-12-10T05:28:19.645473: step 4358, loss 0.00440476, acc 1, prec 0.0824011, recall 0.799199
2017-12-10T05:28:19.917357: step 4359, loss 0.0249176, acc 0.984375, prec 0.0824301, recall 0.799263
2017-12-10T05:28:20.189840: step 4360, loss 0.0912963, acc 0.984375, prec 0.0824287, recall 0.799263
2017-12-10T05:28:20.461947: step 4361, loss 0.00197198, acc 1, prec 0.0824287, recall 0.799263
2017-12-10T05:28:20.725961: step 4362, loss 0.0492261, acc 0.984375, prec 0.0824274, recall 0.799263
2017-12-10T05:28:20.991661: step 4363, loss 2.19541, acc 0.96875, prec 0.0824412, recall 0.799167
2017-12-10T05:28:21.260709: step 4364, loss 0.0132446, acc 1, prec 0.0824412, recall 0.799167
2017-12-10T05:28:21.528131: step 4365, loss 0.114413, acc 0.984375, prec 0.0824398, recall 0.799167
2017-12-10T05:28:21.794310: step 4366, loss 0.0425302, acc 0.96875, prec 0.0824371, recall 0.799167
2017-12-10T05:28:22.061892: step 4367, loss 0.109058, acc 0.96875, prec 0.0824495, recall 0.799199
2017-12-10T05:28:22.331860: step 4368, loss 0.151309, acc 0.921875, prec 0.0824579, recall 0.799232
2017-12-10T05:28:22.595221: step 4369, loss 0.0595894, acc 0.96875, prec 0.0824703, recall 0.799264
2017-12-10T05:28:22.863506: step 4370, loss 0.21561, acc 0.96875, prec 0.082513, recall 0.79936
2017-12-10T05:28:23.131300: step 4371, loss 0.0400817, acc 0.984375, prec 0.0825117, recall 0.79936
2017-12-10T05:28:23.399845: step 4372, loss 0.294505, acc 0.921875, prec 0.0825352, recall 0.799424
2017-12-10T05:28:23.670675: step 4373, loss 0.191557, acc 0.96875, prec 0.0825476, recall 0.799456
2017-12-10T05:28:23.932811: step 4374, loss 0.122525, acc 0.96875, prec 0.08256, recall 0.799488
2017-12-10T05:28:24.200077: step 4375, loss 0.27264, acc 0.921875, prec 0.0825684, recall 0.79952
2017-12-10T05:28:24.459095: step 4376, loss 0.358761, acc 0.90625, prec 0.0825905, recall 0.799584
2017-12-10T05:28:24.727257: step 4377, loss 0.0726716, acc 0.984375, prec 0.0826042, recall 0.799617
2017-12-10T05:28:24.997230: step 4378, loss 0.297062, acc 0.9375, prec 0.0826139, recall 0.799649
2017-12-10T05:28:25.269471: step 4379, loss 0.233456, acc 0.96875, prec 0.0826112, recall 0.799649
2017-12-10T05:28:25.543580: step 4380, loss 0.0838465, acc 0.96875, prec 0.0826236, recall 0.799681
2017-12-10T05:28:25.807195: step 4381, loss 0.124814, acc 0.96875, prec 0.0826512, recall 0.799744
2017-12-10T05:28:26.072321: step 4382, loss 0.225342, acc 0.9375, prec 0.0826457, recall 0.799744
2017-12-10T05:28:26.343501: step 4383, loss 0.0661503, acc 0.984375, prec 0.0826444, recall 0.799744
2017-12-10T05:28:26.609798: step 4384, loss 0.643775, acc 0.890625, prec 0.0826348, recall 0.799744
2017-12-10T05:28:26.873775: step 4385, loss 0.151637, acc 0.953125, prec 0.0826307, recall 0.799744
2017-12-10T05:28:27.143287: step 4386, loss 0.566673, acc 0.921875, prec 0.0826239, recall 0.799744
2017-12-10T05:28:27.420835: step 4387, loss 0.348201, acc 0.921875, prec 0.0826171, recall 0.799744
2017-12-10T05:28:27.685983: step 4388, loss 0.153853, acc 0.96875, prec 0.0826295, recall 0.799776
2017-12-10T05:28:27.958706: step 4389, loss 0.0887096, acc 0.984375, prec 0.0826433, recall 0.799808
2017-12-10T05:28:28.223888: step 4390, loss 0.236912, acc 0.96875, prec 0.0826557, recall 0.79984
2017-12-10T05:28:28.493622: step 4391, loss 0.20513, acc 0.90625, prec 0.0826778, recall 0.799904
2017-12-10T05:28:28.769338: step 4392, loss 0.576841, acc 0.96875, prec 0.0827053, recall 0.799968
2017-12-10T05:28:29.040596: step 4393, loss 0.0429538, acc 0.984375, prec 0.0827039, recall 0.799968
2017-12-10T05:28:29.310847: step 4394, loss 0.0882102, acc 0.953125, prec 0.082715, recall 0.8
2017-12-10T05:28:29.575700: step 4395, loss 0.21155, acc 0.953125, prec 0.0827411, recall 0.800064
2017-12-10T05:28:29.840895: step 4396, loss 0.146202, acc 0.9375, prec 0.0827357, recall 0.800064
2017-12-10T05:28:30.113092: step 4397, loss 0.442887, acc 0.953125, prec 0.0827316, recall 0.800064
2017-12-10T05:28:30.387857: step 4398, loss 0.0531673, acc 0.96875, prec 0.082744, recall 0.800096
2017-12-10T05:28:30.660757: step 4399, loss 0.181026, acc 0.953125, prec 0.0827399, recall 0.800096
2017-12-10T05:28:30.929902: step 4400, loss 0.10982, acc 0.96875, prec 0.0827523, recall 0.800128
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-4400

2017-12-10T05:28:32.293203: step 4401, loss 0.0242206, acc 0.984375, prec 0.082766, recall 0.800159
2017-12-10T05:28:32.558063: step 4402, loss 0.00175192, acc 1, prec 0.082766, recall 0.800159
2017-12-10T05:28:32.822345: step 4403, loss 0.0934159, acc 0.96875, prec 0.0827935, recall 0.800223
2017-12-10T05:28:33.085003: step 4404, loss 0.030256, acc 0.984375, prec 0.0827922, recall 0.800223
2017-12-10T05:28:33.360773: step 4405, loss 0.008346, acc 1, prec 0.0827922, recall 0.800223
2017-12-10T05:28:33.627594: step 4406, loss 0.388956, acc 0.984375, prec 0.0828059, recall 0.800255
2017-12-10T05:28:33.903153: step 4407, loss 0.0539637, acc 1, prec 0.082821, recall 0.800287
2017-12-10T05:28:34.180590: step 4408, loss 0.135792, acc 0.984375, prec 0.0828197, recall 0.800287
2017-12-10T05:28:34.449373: step 4409, loss 0.000335667, acc 1, prec 0.0828197, recall 0.800287
2017-12-10T05:28:34.717016: step 4410, loss 0.00282705, acc 1, prec 0.0828197, recall 0.800287
2017-12-10T05:28:34.982278: step 4411, loss 0.109817, acc 1, prec 0.0828348, recall 0.800318
2017-12-10T05:28:35.258984: step 4412, loss 0.000510535, acc 1, prec 0.0828348, recall 0.800318
2017-12-10T05:28:35.529315: step 4413, loss 0.331906, acc 0.953125, prec 0.0828307, recall 0.800318
2017-12-10T05:28:35.796193: step 4414, loss 0.0253233, acc 0.984375, prec 0.0828293, recall 0.800318
2017-12-10T05:28:36.065027: step 4415, loss 0.709176, acc 0.984375, prec 0.0829035, recall 0.800477
2017-12-10T05:28:36.352464: step 4416, loss 0.072518, acc 0.984375, prec 0.0829173, recall 0.800509
2017-12-10T05:28:36.617416: step 4417, loss 0.147262, acc 0.984375, prec 0.082931, recall 0.800541
2017-12-10T05:28:36.878188: step 4418, loss 0.00296745, acc 1, prec 0.0829461, recall 0.800573
2017-12-10T05:28:37.144205: step 4419, loss 0.128545, acc 0.953125, prec 0.0829722, recall 0.800636
2017-12-10T05:28:37.408271: step 4420, loss 0.00848619, acc 1, prec 0.0829722, recall 0.800636
2017-12-10T05:28:37.677340: step 4421, loss 0.19538, acc 0.953125, prec 0.0829681, recall 0.800636
2017-12-10T05:28:37.942630: step 4422, loss 0.0909458, acc 0.984375, prec 0.0829819, recall 0.800668
2017-12-10T05:28:38.210746: step 4423, loss 0.175204, acc 0.953125, prec 0.0829778, recall 0.800668
2017-12-10T05:28:38.473842: step 4424, loss 0.0826434, acc 0.984375, prec 0.0829915, recall 0.800699
2017-12-10T05:28:38.740743: step 4425, loss 2.31689, acc 0.96875, prec 0.0829901, recall 0.800572
2017-12-10T05:28:39.016617: step 4426, loss 0.1261, acc 0.953125, prec 0.0830012, recall 0.800604
2017-12-10T05:28:39.286800: step 4427, loss 0.311942, acc 0.96875, prec 0.0829984, recall 0.800604
2017-12-10T05:28:39.557415: step 4428, loss 0.156924, acc 0.9375, prec 0.0829929, recall 0.800604
2017-12-10T05:28:39.824011: step 4429, loss 0.37341, acc 0.90625, prec 0.0829848, recall 0.800604
2017-12-10T05:28:40.095840: step 4430, loss 0.174903, acc 0.953125, prec 0.083026, recall 0.800699
2017-12-10T05:28:40.359672: step 4431, loss 0.0526456, acc 0.984375, prec 0.0830397, recall 0.80073
2017-12-10T05:28:40.625289: step 4432, loss 0.503956, acc 0.875, prec 0.0830438, recall 0.800762
2017-12-10T05:28:40.889469: step 4433, loss 0.312947, acc 0.921875, prec 0.083037, recall 0.800762
2017-12-10T05:28:41.155665: step 4434, loss 0.156641, acc 0.953125, prec 0.0830329, recall 0.800762
2017-12-10T05:28:41.422958: step 4435, loss 1.11384, acc 0.875, prec 0.0830522, recall 0.800825
2017-12-10T05:28:41.695037: step 4436, loss 0.129803, acc 0.9375, prec 0.0830467, recall 0.800825
2017-12-10T05:28:41.966004: step 4437, loss 0.504717, acc 0.921875, prec 0.0830399, recall 0.800825
2017-12-10T05:28:42.238751: step 4438, loss 0.0376728, acc 0.984375, prec 0.0830536, recall 0.800857
2017-12-10T05:28:42.509701: step 4439, loss 0.335384, acc 0.90625, prec 0.0830454, recall 0.800857
2017-12-10T05:28:42.773511: step 4440, loss 0.297316, acc 0.90625, prec 0.0830523, recall 0.800888
2017-12-10T05:28:43.040355: step 4441, loss 0.607338, acc 0.828125, prec 0.0830523, recall 0.80092
2017-12-10T05:28:43.308065: step 4442, loss 0.110607, acc 0.96875, prec 0.0830798, recall 0.800983
2017-12-10T05:28:43.573621: step 4443, loss 0.450896, acc 0.921875, prec 0.0830729, recall 0.800983
2017-12-10T05:28:43.840109: step 4444, loss 0.0212187, acc 0.984375, prec 0.0830866, recall 0.801015
2017-12-10T05:28:44.108883: step 4445, loss 0.10777, acc 0.96875, prec 0.0831141, recall 0.801078
2017-12-10T05:28:44.374548: step 4446, loss 0.109499, acc 0.96875, prec 0.0831264, recall 0.801109
2017-12-10T05:28:44.638749: step 4447, loss 4.46681, acc 0.953125, prec 0.0831237, recall 0.800982
2017-12-10T05:28:44.910108: step 4448, loss 0.569252, acc 0.921875, prec 0.0831621, recall 0.801077
2017-12-10T05:28:45.176648: step 4449, loss 0.443551, acc 0.96875, prec 0.0831593, recall 0.801077
2017-12-10T05:28:45.443752: step 4450, loss 0.215537, acc 0.90625, prec 0.0831662, recall 0.801108
2017-12-10T05:28:45.720828: step 4451, loss 1.77514, acc 0.90625, prec 0.0831744, recall 0.801013
2017-12-10T05:28:45.989868: step 4452, loss 0.104374, acc 0.953125, prec 0.0831854, recall 0.801045
2017-12-10T05:28:46.250462: step 4453, loss 0.321767, acc 0.90625, prec 0.0831923, recall 0.801076
2017-12-10T05:28:46.518634: step 4454, loss 0.181291, acc 0.953125, prec 0.0831882, recall 0.801076
2017-12-10T05:28:46.789424: step 4455, loss 0.254849, acc 0.90625, prec 0.083195, recall 0.801108
2017-12-10T05:28:47.053741: step 4456, loss 0.711506, acc 0.84375, prec 0.0832265, recall 0.801202
2017-12-10T05:28:47.323786: step 4457, loss 0.26142, acc 0.875, prec 0.0832457, recall 0.801265
2017-12-10T05:28:47.591454: step 4458, loss 0.624348, acc 0.875, prec 0.0832348, recall 0.801265
2017-12-10T05:28:47.859025: step 4459, loss 0.409876, acc 0.859375, prec 0.0832375, recall 0.801296
2017-12-10T05:28:48.127625: step 4460, loss 0.687858, acc 0.859375, prec 0.0832252, recall 0.801296
2017-12-10T05:28:48.403959: step 4461, loss 0.530124, acc 0.921875, prec 0.0832335, recall 0.801328
2017-12-10T05:28:48.674937: step 4462, loss 0.252403, acc 0.921875, prec 0.0832417, recall 0.801359
2017-12-10T05:28:48.942599: step 4463, loss 0.205284, acc 0.953125, prec 0.0832827, recall 0.801453
2017-12-10T05:28:49.215923: step 4464, loss 0.226278, acc 0.96875, prec 0.083295, recall 0.801485
2017-12-10T05:28:49.480533: step 4465, loss 0.0870339, acc 0.96875, prec 0.0832923, recall 0.801485
2017-12-10T05:28:49.743083: step 4466, loss 0.0747637, acc 0.96875, prec 0.0832896, recall 0.801485
2017-12-10T05:28:50.008066: step 4467, loss 0.196807, acc 0.9375, prec 0.0832841, recall 0.801485
2017-12-10T05:28:50.273702: step 4468, loss 0.306825, acc 0.953125, prec 0.083295, recall 0.801516
2017-12-10T05:28:50.540842: step 4469, loss 0.268824, acc 0.9375, prec 0.0832896, recall 0.801516
2017-12-10T05:28:50.808344: step 4470, loss 0.0195589, acc 1, prec 0.0832896, recall 0.801516
2017-12-10T05:28:51.073544: step 4471, loss 1.91859, acc 0.984375, prec 0.0833197, recall 0.801452
2017-12-10T05:28:51.346318: step 4472, loss 0.409245, acc 0.953125, prec 0.0833306, recall 0.801483
2017-12-10T05:28:51.587095: step 4473, loss 0.0396367, acc 0.980769, prec 0.0833292, recall 0.801483
2017-12-10T05:28:51.856456: step 4474, loss 0.0883741, acc 0.96875, prec 0.0833265, recall 0.801483
2017-12-10T05:28:52.123795: step 4475, loss 0.873801, acc 0.96875, prec 0.0833388, recall 0.801515
2017-12-10T05:28:52.399493: step 4476, loss 0.508991, acc 0.9375, prec 0.0833333, recall 0.801515
2017-12-10T05:28:52.669852: step 4477, loss 0.0418179, acc 1, prec 0.0833484, recall 0.801546
2017-12-10T05:28:52.937275: step 4478, loss 0.224274, acc 0.921875, prec 0.0833566, recall 0.801577
2017-12-10T05:28:53.204462: step 4479, loss 0.393075, acc 0.9375, prec 0.0833661, recall 0.801609
2017-12-10T05:28:53.470528: step 4480, loss 0.264869, acc 0.90625, prec 0.0833579, recall 0.801609
2017-12-10T05:28:53.740676: step 4481, loss 0.167793, acc 0.921875, prec 0.0833661, recall 0.80164
2017-12-10T05:28:54.004182: step 4482, loss 0.489497, acc 0.90625, prec 0.0833579, recall 0.80164
2017-12-10T05:28:54.272931: step 4483, loss 0.685803, acc 0.84375, prec 0.0833593, recall 0.801671
2017-12-10T05:28:54.536656: step 4484, loss 0.246081, acc 0.859375, prec 0.083377, recall 0.801734
2017-12-10T05:28:54.805469: step 4485, loss 0.209318, acc 0.890625, prec 0.0833825, recall 0.801765
2017-12-10T05:28:55.072304: step 4486, loss 0.218401, acc 0.875, prec 0.0833716, recall 0.801765
2017-12-10T05:28:55.339919: step 4487, loss 0.35727, acc 0.90625, prec 0.0833784, recall 0.801796
2017-12-10T05:28:55.604844: step 4488, loss 0.206639, acc 0.953125, prec 0.0834043, recall 0.801859
2017-12-10T05:28:55.871857: step 4489, loss 0.356373, acc 0.9375, prec 0.0834139, recall 0.80189
2017-12-10T05:28:56.135870: step 4490, loss 0.45331, acc 0.953125, prec 0.0834098, recall 0.80189
2017-12-10T05:28:56.408426: step 4491, loss 0.114773, acc 0.953125, prec 0.0834357, recall 0.801952
2017-12-10T05:28:56.679007: step 4492, loss 0.149846, acc 0.953125, prec 0.0834466, recall 0.801983
2017-12-10T05:28:56.944038: step 4493, loss 0.30151, acc 0.96875, prec 0.0834889, recall 0.802077
2017-12-10T05:28:57.229220: step 4494, loss 0.0116049, acc 1, prec 0.0835339, recall 0.80217
2017-12-10T05:28:57.500905: step 4495, loss 0.175153, acc 0.984375, prec 0.0835476, recall 0.802201
2017-12-10T05:28:57.763736: step 4496, loss 0.807233, acc 0.96875, prec 0.0835598, recall 0.802232
2017-12-10T05:28:58.033817: step 4497, loss 0.0407846, acc 0.96875, prec 0.0835721, recall 0.802263
2017-12-10T05:28:58.295739: step 4498, loss 0.0226828, acc 1, prec 0.0835871, recall 0.802294
2017-12-10T05:28:58.558649: step 4499, loss 0.0535516, acc 0.96875, prec 0.0835994, recall 0.802326
2017-12-10T05:28:58.825011: step 4500, loss 0.0585075, acc 0.953125, prec 0.0836403, recall 0.802419

Evaluation:
2017-12-10T05:29:06.437714: step 4500, loss 4.48406, acc 0.954048, prec 0.0839198, recall 0.791628

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-4500

2017-12-10T05:29:07.677225: step 4501, loss 0.0239574, acc 0.984375, prec 0.0839334, recall 0.79166
2017-12-10T05:29:07.948493: step 4502, loss 0.0589195, acc 0.984375, prec 0.0839469, recall 0.791692
2017-12-10T05:29:08.216780: step 4503, loss 0.104722, acc 0.953125, prec 0.0839428, recall 0.791692
2017-12-10T05:29:09.204085: step 4504, loss 0.0294474, acc 0.984375, prec 0.0839414, recall 0.791692
2017-12-10T05:29:09.568461: step 4505, loss 0.0190788, acc 0.984375, prec 0.0839401, recall 0.791692
2017-12-10T05:29:09.839230: step 4506, loss 0.151325, acc 0.96875, prec 0.0839522, recall 0.791724
2017-12-10T05:29:10.950737: step 4507, loss 0.305089, acc 0.96875, prec 0.0839495, recall 0.791724
2017-12-10T05:29:11.355096: step 4508, loss 0.0586481, acc 0.96875, prec 0.0839468, recall 0.791724
2017-12-10T05:29:11.638835: step 4509, loss 0.0380701, acc 0.984375, prec 0.0839454, recall 0.791724
2017-12-10T05:29:11.928285: step 4510, loss 0.0970176, acc 1, prec 0.0839603, recall 0.791756
2017-12-10T05:29:12.217538: step 4511, loss 0.00364213, acc 1, prec 0.0839603, recall 0.791756
2017-12-10T05:29:12.495496: step 4512, loss 0.116608, acc 0.96875, prec 0.0839724, recall 0.791788
2017-12-10T05:29:12.775056: step 4513, loss 0.0343432, acc 0.984375, prec 0.0839711, recall 0.791788
2017-12-10T05:29:13.052973: step 4514, loss 0.00523691, acc 1, prec 0.083986, recall 0.79182
2017-12-10T05:29:13.323521: step 4515, loss 0.0416442, acc 0.96875, prec 0.084013, recall 0.791884
2017-12-10T05:29:13.593563: step 4516, loss 0.00448222, acc 1, prec 0.0840279, recall 0.791915
2017-12-10T05:29:13.867120: step 4517, loss 0.375167, acc 0.984375, prec 0.0840414, recall 0.791947
2017-12-10T05:29:14.136284: step 4518, loss 0.172749, acc 0.953125, prec 0.0840373, recall 0.791947
2017-12-10T05:29:14.400223: step 4519, loss 0.128589, acc 0.953125, prec 0.0840481, recall 0.791979
2017-12-10T05:29:14.673791: step 4520, loss 4.35918, acc 0.953125, prec 0.0840454, recall 0.791858
2017-12-10T05:29:14.942467: step 4521, loss 0.173567, acc 0.96875, prec 0.0840575, recall 0.79189
2017-12-10T05:29:15.215730: step 4522, loss 0.0370097, acc 0.984375, prec 0.0840561, recall 0.79189
2017-12-10T05:29:15.480545: step 4523, loss 0.399906, acc 0.921875, prec 0.0840642, recall 0.791922
2017-12-10T05:29:15.749255: step 4524, loss 0.427841, acc 0.90625, prec 0.0841006, recall 0.792017
2017-12-10T05:29:16.010589: step 4525, loss 0.0823485, acc 0.96875, prec 0.0840979, recall 0.792017
2017-12-10T05:29:16.276772: step 4526, loss 0.0709986, acc 0.96875, prec 0.0841249, recall 0.792081
2017-12-10T05:29:16.541732: step 4527, loss 0.352702, acc 0.890625, prec 0.0841153, recall 0.792081
2017-12-10T05:29:16.812654: step 4528, loss 0.308929, acc 0.90625, prec 0.0841071, recall 0.792081
2017-12-10T05:29:17.078405: step 4529, loss 0.255017, acc 0.953125, prec 0.0841476, recall 0.792176
2017-12-10T05:29:17.344065: step 4530, loss 0.363471, acc 0.90625, prec 0.0841543, recall 0.792208
2017-12-10T05:29:17.612291: step 4531, loss 0.566665, acc 0.859375, prec 0.0841569, recall 0.79224
2017-12-10T05:29:17.883258: step 4532, loss 0.429786, acc 0.921875, prec 0.0841798, recall 0.792303
2017-12-10T05:29:18.151460: step 4533, loss 0.792005, acc 0.890625, prec 0.0841702, recall 0.792303
2017-12-10T05:29:18.416129: step 4534, loss 0.591601, acc 0.890625, prec 0.0841755, recall 0.792335
2017-12-10T05:29:18.680488: step 4535, loss 0.412719, acc 0.875, prec 0.0841795, recall 0.792366
2017-12-10T05:29:18.946040: step 4536, loss 0.136234, acc 0.9375, prec 0.084174, recall 0.792366
2017-12-10T05:29:19.217564: step 4537, loss 0.376473, acc 0.921875, prec 0.0841969, recall 0.79243
2017-12-10T05:29:19.484345: step 4538, loss 0.333695, acc 0.90625, prec 0.0842035, recall 0.792461
2017-12-10T05:29:19.753379: step 4539, loss 0.116945, acc 0.953125, prec 0.084244, recall 0.792556
2017-12-10T05:29:20.024150: step 4540, loss 0.387691, acc 0.890625, prec 0.0842641, recall 0.79262
2017-12-10T05:29:20.287409: step 4541, loss 0.194419, acc 0.921875, prec 0.084287, recall 0.792683
2017-12-10T05:29:20.558109: step 4542, loss 0.297269, acc 0.9375, prec 0.0842963, recall 0.792715
2017-12-10T05:29:20.822694: step 4543, loss 0.0684766, acc 0.96875, prec 0.0843085, recall 0.792746
2017-12-10T05:29:21.089191: step 4544, loss 0.0499819, acc 0.96875, prec 0.0843057, recall 0.792746
2017-12-10T05:29:21.354004: step 4545, loss 0.00262782, acc 1, prec 0.0843057, recall 0.792746
2017-12-10T05:29:21.623375: step 4546, loss 0.265566, acc 0.96875, prec 0.084303, recall 0.792746
2017-12-10T05:29:21.893818: step 4547, loss 0.0158808, acc 1, prec 0.084303, recall 0.792746
2017-12-10T05:29:22.165763: step 4548, loss 2.5274, acc 0.96875, prec 0.0843165, recall 0.792657
2017-12-10T05:29:22.434819: step 4549, loss 0.0111471, acc 1, prec 0.0843165, recall 0.792657
2017-12-10T05:29:22.700900: step 4550, loss 0.0360836, acc 0.984375, prec 0.0843151, recall 0.792657
2017-12-10T05:29:22.967145: step 4551, loss 0.110185, acc 0.96875, prec 0.0843124, recall 0.792657
2017-12-10T05:29:23.229851: step 4552, loss 0.241111, acc 0.96875, prec 0.0843393, recall 0.79272
2017-12-10T05:29:23.497229: step 4553, loss 0.0495953, acc 0.984375, prec 0.0843379, recall 0.79272
2017-12-10T05:29:23.769241: step 4554, loss 0.0901258, acc 0.984375, prec 0.0843366, recall 0.79272
2017-12-10T05:29:24.039904: step 4555, loss 0.020922, acc 0.984375, prec 0.08435, recall 0.792752
2017-12-10T05:29:24.304797: step 4556, loss 0.237275, acc 0.9375, prec 0.0843446, recall 0.792752
2017-12-10T05:29:24.568574: step 4557, loss 0.0472179, acc 0.984375, prec 0.0843432, recall 0.792752
2017-12-10T05:29:24.831550: step 4558, loss 0.34843, acc 0.953125, prec 0.0843688, recall 0.792815
2017-12-10T05:29:25.105663: step 4559, loss 0.0313857, acc 0.984375, prec 0.0843674, recall 0.792815
2017-12-10T05:29:25.369861: step 4560, loss 0.0698733, acc 0.984375, prec 0.0843957, recall 0.792878
2017-12-10T05:29:25.641462: step 4561, loss 0.0850505, acc 0.953125, prec 0.0844064, recall 0.792909
2017-12-10T05:29:25.906390: step 4562, loss 0.0356788, acc 0.984375, prec 0.0844199, recall 0.792941
2017-12-10T05:29:26.171117: step 4563, loss 0.0147473, acc 1, prec 0.0844199, recall 0.792941
2017-12-10T05:29:26.439475: step 4564, loss 1.16828, acc 0.984375, prec 0.0844482, recall 0.793004
2017-12-10T05:29:26.705857: step 4565, loss 0.304604, acc 0.921875, prec 0.084471, recall 0.793067
2017-12-10T05:29:26.979581: step 4566, loss 0.0902308, acc 0.984375, prec 0.0844845, recall 0.793098
2017-12-10T05:29:27.254287: step 4567, loss 0.118358, acc 0.984375, prec 0.0845127, recall 0.793161
2017-12-10T05:29:27.532423: step 4568, loss 0.220424, acc 0.953125, prec 0.0845235, recall 0.793193
2017-12-10T05:29:27.799225: step 4569, loss 0.107804, acc 0.953125, prec 0.0845342, recall 0.793224
2017-12-10T05:29:28.066245: step 4570, loss 0.193878, acc 0.921875, prec 0.0845273, recall 0.793224
2017-12-10T05:29:28.332398: step 4571, loss 0.201186, acc 0.96875, prec 0.0845246, recall 0.793224
2017-12-10T05:29:28.603903: step 4572, loss 0.656929, acc 0.921875, prec 0.0845178, recall 0.793224
2017-12-10T05:29:28.873684: step 4573, loss 0.24516, acc 0.90625, prec 0.0845244, recall 0.793255
2017-12-10T05:29:29.138026: step 4574, loss 0.359517, acc 0.921875, prec 0.0845175, recall 0.793255
2017-12-10T05:29:29.405177: step 4575, loss 0.285672, acc 0.9375, prec 0.0845269, recall 0.793287
2017-12-10T05:29:29.673928: step 4576, loss 0.22261, acc 0.9375, prec 0.0845362, recall 0.793318
2017-12-10T05:29:29.946081: step 4577, loss 0.158993, acc 0.921875, prec 0.084559, recall 0.793381
2017-12-10T05:29:30.215177: step 4578, loss 0.0638686, acc 0.984375, prec 0.0845724, recall 0.793412
2017-12-10T05:29:30.490556: step 4579, loss 0.20761, acc 0.9375, prec 0.0845966, recall 0.793475
2017-12-10T05:29:30.767806: step 4580, loss 0.0221742, acc 1, prec 0.0845966, recall 0.793475
2017-12-10T05:29:31.039412: step 4581, loss 0.0153397, acc 1, prec 0.0846114, recall 0.793506
2017-12-10T05:29:31.308513: step 4582, loss 0.190364, acc 0.921875, prec 0.0846194, recall 0.793538
2017-12-10T05:29:31.573750: step 4583, loss 0.288121, acc 0.9375, prec 0.0846287, recall 0.793569
2017-12-10T05:29:31.843332: step 4584, loss 0.116253, acc 0.953125, prec 0.0846246, recall 0.793569
2017-12-10T05:29:32.107823: step 4585, loss 0.0529926, acc 0.984375, prec 0.084638, recall 0.7936
2017-12-10T05:29:32.373059: step 4586, loss 0.388446, acc 1, prec 0.0846676, recall 0.793663
2017-12-10T05:29:32.643571: step 4587, loss 0.0430071, acc 0.96875, prec 0.0846797, recall 0.793694
2017-12-10T05:29:32.919227: step 4588, loss 0.127352, acc 0.984375, prec 0.0847079, recall 0.793757
2017-12-10T05:29:33.183367: step 4589, loss 0.0814606, acc 0.953125, prec 0.0847186, recall 0.793788
2017-12-10T05:29:33.450932: step 4590, loss 0.133092, acc 0.96875, prec 0.0847307, recall 0.793819
2017-12-10T05:29:33.723483: step 4591, loss 0.0400987, acc 0.984375, prec 0.0847589, recall 0.793882
2017-12-10T05:29:33.988252: step 4592, loss 0.218395, acc 0.984375, prec 0.0847723, recall 0.793913
2017-12-10T05:29:34.254083: step 4593, loss 0.0594871, acc 0.984375, prec 0.0847858, recall 0.793944
2017-12-10T05:29:34.522649: step 4594, loss 0.26607, acc 0.921875, prec 0.0847789, recall 0.793944
2017-12-10T05:29:34.791849: step 4595, loss 0.521491, acc 0.96875, prec 0.084791, recall 0.793975
2017-12-10T05:29:35.062216: step 4596, loss 0.0180167, acc 1, prec 0.0848058, recall 0.794006
2017-12-10T05:29:35.329663: step 4597, loss 0.0404865, acc 0.984375, prec 0.0848192, recall 0.794038
2017-12-10T05:29:35.602924: step 4598, loss 0.0181023, acc 0.984375, prec 0.0848178, recall 0.794038
2017-12-10T05:29:35.863364: step 4599, loss 0.0573539, acc 0.96875, prec 0.0848151, recall 0.794038
2017-12-10T05:29:36.126621: step 4600, loss 0.438734, acc 0.90625, prec 0.0848216, recall 0.794069
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-4600

2017-12-10T05:29:37.443360: step 4601, loss 0.182785, acc 0.984375, prec 0.0848499, recall 0.794131
2017-12-10T05:29:37.714045: step 4602, loss 0.311463, acc 0.96875, prec 0.0848471, recall 0.794131
2017-12-10T05:29:37.980904: step 4603, loss 0.0160721, acc 1, prec 0.0848619, recall 0.794162
2017-12-10T05:29:38.248786: step 4604, loss 0.33122, acc 0.953125, prec 0.0848578, recall 0.794162
2017-12-10T05:29:38.522095: step 4605, loss 0.121008, acc 0.9375, prec 0.0848523, recall 0.794162
2017-12-10T05:29:38.786412: step 4606, loss 0.221191, acc 0.921875, prec 0.0848454, recall 0.794162
2017-12-10T05:29:39.055751: step 4607, loss 0.0629649, acc 0.984375, prec 0.0848441, recall 0.794162
2017-12-10T05:29:39.318923: step 4608, loss 0.046866, acc 0.984375, prec 0.0848575, recall 0.794193
2017-12-10T05:29:39.588411: step 4609, loss 0.130289, acc 0.984375, prec 0.0848561, recall 0.794193
2017-12-10T05:29:39.855977: step 4610, loss 0.0354281, acc 0.984375, prec 0.0848695, recall 0.794224
2017-12-10T05:29:40.123541: step 4611, loss 0.0531606, acc 0.984375, prec 0.0848977, recall 0.794287
2017-12-10T05:29:40.392314: step 4612, loss 0.24749, acc 0.9375, prec 0.084907, recall 0.794318
2017-12-10T05:29:40.668798: step 4613, loss 0.0109791, acc 1, prec 0.084907, recall 0.794318
2017-12-10T05:29:40.931621: step 4614, loss 0.0578187, acc 0.984375, prec 0.0849057, recall 0.794318
2017-12-10T05:29:41.204190: step 4615, loss 0.547394, acc 0.953125, prec 0.0849459, recall 0.794411
2017-12-10T05:29:41.476242: step 4616, loss 0.23643, acc 0.953125, prec 0.0849565, recall 0.794442
2017-12-10T05:29:41.744835: step 4617, loss 4.1566, acc 0.96875, prec 0.0849847, recall 0.794384
2017-12-10T05:29:42.012043: step 4618, loss 0.0656655, acc 0.96875, prec 0.084982, recall 0.794384
2017-12-10T05:29:42.290253: step 4619, loss 0.251965, acc 0.921875, prec 0.0849899, recall 0.794415
2017-12-10T05:29:42.559377: step 4620, loss 0.246458, acc 0.96875, prec 0.0850167, recall 0.794477
2017-12-10T05:29:42.826490: step 4621, loss 0.0846781, acc 0.9375, prec 0.085026, recall 0.794508
2017-12-10T05:29:43.093819: step 4622, loss 0.289316, acc 0.9375, prec 0.0850353, recall 0.794539
2017-12-10T05:29:43.364485: step 4623, loss 0.514359, acc 0.859375, prec 0.0850229, recall 0.794539
2017-12-10T05:29:43.626728: step 4624, loss 0.305765, acc 0.890625, prec 0.0850428, recall 0.794601
2017-12-10T05:29:43.892536: step 4625, loss 0.448104, acc 0.890625, prec 0.085048, recall 0.794632
2017-12-10T05:29:44.162628: step 4626, loss 0.251476, acc 0.953125, prec 0.0850587, recall 0.794663
2017-12-10T05:29:44.427977: step 4627, loss 0.320359, acc 0.875, prec 0.0850477, recall 0.794663
2017-12-10T05:29:44.699295: step 4628, loss 0.581668, acc 0.890625, prec 0.0850381, recall 0.794663
2017-12-10T05:29:44.967554: step 4629, loss 0.124918, acc 0.984375, prec 0.0850515, recall 0.794694
2017-12-10T05:29:45.232994: step 4630, loss 0.644055, acc 0.90625, prec 0.0850727, recall 0.794756
2017-12-10T05:29:45.502358: step 4631, loss 0.234317, acc 0.921875, prec 0.0850954, recall 0.794818
2017-12-10T05:29:45.771120: step 4632, loss 0.081079, acc 0.96875, prec 0.0850927, recall 0.794818
2017-12-10T05:29:46.038644: step 4633, loss 0.233404, acc 0.90625, prec 0.0850992, recall 0.794849
2017-12-10T05:29:46.301466: step 4634, loss 0.104405, acc 0.96875, prec 0.0850964, recall 0.794849
2017-12-10T05:29:46.570646: step 4635, loss 0.086148, acc 0.984375, prec 0.0851246, recall 0.79491
2017-12-10T05:29:46.838559: step 4636, loss 0.281381, acc 0.96875, prec 0.0851366, recall 0.794941
2017-12-10T05:29:47.105278: step 4637, loss 0.0334083, acc 0.96875, prec 0.0851338, recall 0.794941
2017-12-10T05:29:47.371526: step 4638, loss 0.0909575, acc 0.96875, prec 0.0851753, recall 0.795034
2017-12-10T05:29:47.637586: step 4639, loss 0.202321, acc 0.96875, prec 0.0851726, recall 0.795034
2017-12-10T05:29:47.914003: step 4640, loss 0.174124, acc 0.96875, prec 0.0851846, recall 0.795065
2017-12-10T05:29:48.187885: step 4641, loss 0.270967, acc 0.9375, prec 0.0851791, recall 0.795065
2017-12-10T05:29:48.464801: step 4642, loss 0.06374, acc 0.984375, prec 0.0852072, recall 0.795126
2017-12-10T05:29:48.736687: step 4643, loss 0.137245, acc 1, prec 0.0852367, recall 0.795188
2017-12-10T05:29:49.011596: step 4644, loss 0.252799, acc 0.96875, prec 0.0852487, recall 0.795219
2017-12-10T05:29:49.284392: step 4645, loss 0.104934, acc 0.96875, prec 0.085246, recall 0.795219
2017-12-10T05:29:49.551767: step 4646, loss 0.012643, acc 0.984375, prec 0.0852446, recall 0.795219
2017-12-10T05:29:49.816564: step 4647, loss 0.250708, acc 0.984375, prec 0.0852432, recall 0.795219
2017-12-10T05:29:50.080051: step 4648, loss 0.104683, acc 0.96875, prec 0.0852405, recall 0.795219
2017-12-10T05:29:50.344674: step 4649, loss 0.226361, acc 0.953125, prec 0.0852511, recall 0.79525
2017-12-10T05:29:50.620832: step 4650, loss 0.0294099, acc 0.984375, prec 0.0852644, recall 0.79528
2017-12-10T05:29:50.885695: step 4651, loss 0.377306, acc 0.96875, prec 0.0852764, recall 0.795311
2017-12-10T05:29:51.152037: step 4652, loss 0.0176011, acc 1, prec 0.0852764, recall 0.795311
2017-12-10T05:29:51.424492: step 4653, loss 0.0740259, acc 1, prec 0.0853207, recall 0.795403
2017-12-10T05:29:51.699124: step 4654, loss 0.0420748, acc 0.984375, prec 0.0853488, recall 0.795465
2017-12-10T05:29:51.974840: step 4655, loss 5.50682, acc 0.96875, prec 0.0853916, recall 0.795438
2017-12-10T05:29:52.248093: step 4656, loss 0.144496, acc 0.96875, prec 0.0853888, recall 0.795438
2017-12-10T05:29:52.524825: step 4657, loss 0.00711956, acc 1, prec 0.0853888, recall 0.795438
2017-12-10T05:29:52.795423: step 4658, loss 0.070112, acc 0.96875, prec 0.0853861, recall 0.795438
2017-12-10T05:29:53.057064: step 4659, loss 0.185989, acc 0.984375, prec 0.0853994, recall 0.795468
2017-12-10T05:29:53.324226: step 4660, loss 0.205951, acc 0.953125, prec 0.0854101, recall 0.795499
2017-12-10T05:29:53.589900: step 4661, loss 0.332834, acc 0.953125, prec 0.0854354, recall 0.79556
2017-12-10T05:29:53.860164: step 4662, loss 0.0577554, acc 0.984375, prec 0.085434, recall 0.79556
2017-12-10T05:29:54.124518: step 4663, loss 0.382988, acc 0.953125, prec 0.0854446, recall 0.795591
2017-12-10T05:29:54.394153: step 4664, loss 0.0637647, acc 0.96875, prec 0.0854861, recall 0.795683
2017-12-10T05:29:54.662816: step 4665, loss 0.212974, acc 0.953125, prec 0.0854819, recall 0.795683
2017-12-10T05:29:54.932258: step 4666, loss 0.496611, acc 0.9375, prec 0.0854764, recall 0.795683
2017-12-10T05:29:55.191603: step 4667, loss 0.104197, acc 0.96875, prec 0.0854884, recall 0.795713
2017-12-10T05:29:55.457921: step 4668, loss 0.0803873, acc 0.96875, prec 0.0855004, recall 0.795744
2017-12-10T05:29:55.723949: step 4669, loss 0.144539, acc 0.953125, prec 0.0855257, recall 0.795805
2017-12-10T05:29:55.984879: step 4670, loss 0.476679, acc 0.90625, prec 0.0855321, recall 0.795836
2017-12-10T05:29:56.253851: step 4671, loss 0.0952766, acc 0.96875, prec 0.0855735, recall 0.795928
2017-12-10T05:29:56.516857: step 4672, loss 0.16124, acc 0.96875, prec 0.0855855, recall 0.795958
2017-12-10T05:29:56.781867: step 4673, loss 0.209886, acc 0.953125, prec 0.0856108, recall 0.796019
2017-12-10T05:29:57.057617: step 4674, loss 0.172566, acc 0.921875, prec 0.0856039, recall 0.796019
2017-12-10T05:29:57.339839: step 4675, loss 0.016357, acc 1, prec 0.0856186, recall 0.79605
2017-12-10T05:29:57.602013: step 4676, loss 0.089711, acc 0.984375, prec 0.085632, recall 0.79608
2017-12-10T05:29:57.866901: step 4677, loss 0.133544, acc 0.96875, prec 0.0856587, recall 0.796141
2017-12-10T05:29:58.133634: step 4678, loss 0.02456, acc 1, prec 0.0857028, recall 0.796233
2017-12-10T05:29:58.404662: step 4679, loss 0.0792976, acc 0.96875, prec 0.0857147, recall 0.796263
2017-12-10T05:29:58.669449: step 4680, loss 0.233001, acc 0.9375, prec 0.0857534, recall 0.796354
2017-12-10T05:29:58.941815: step 4681, loss 0.242305, acc 0.96875, prec 0.0857653, recall 0.796385
2017-12-10T05:29:59.210538: step 4682, loss 0.112984, acc 0.984375, prec 0.0857639, recall 0.796385
2017-12-10T05:29:59.475699: step 4683, loss 0.164097, acc 0.96875, prec 0.0857759, recall 0.796415
2017-12-10T05:29:59.743069: step 4684, loss 0.399967, acc 0.96875, prec 0.0857731, recall 0.796415
2017-12-10T05:30:00.009745: step 4685, loss 1.64451, acc 0.984375, prec 0.0857878, recall 0.796327
2017-12-10T05:30:00.280967: step 4686, loss 0.14984, acc 0.953125, prec 0.0857984, recall 0.796357
2017-12-10T05:30:00.554780: step 4687, loss 0.00591917, acc 1, prec 0.0858278, recall 0.796418
2017-12-10T05:30:00.827064: step 4688, loss 0.215481, acc 0.96875, prec 0.0858397, recall 0.796448
2017-12-10T05:30:01.097555: step 4689, loss 0.113248, acc 0.96875, prec 0.0858664, recall 0.796509
2017-12-10T05:30:01.360152: step 4690, loss 0.118958, acc 0.96875, prec 0.0858636, recall 0.796509
2017-12-10T05:30:01.627863: step 4691, loss 0.189462, acc 0.921875, prec 0.0858861, recall 0.79657
2017-12-10T05:30:01.894843: step 4692, loss 0.167094, acc 0.953125, prec 0.0858967, recall 0.7966
2017-12-10T05:30:02.159897: step 4693, loss 0.503997, acc 0.9375, prec 0.0858911, recall 0.7966
2017-12-10T05:30:02.430603: step 4694, loss 0.401955, acc 0.875, prec 0.0858948, recall 0.79663
2017-12-10T05:30:02.698181: step 4695, loss 0.143252, acc 0.953125, prec 0.0859054, recall 0.796661
2017-12-10T05:30:02.972302: step 4696, loss 0.0361958, acc 0.984375, prec 0.0859187, recall 0.796691
2017-12-10T05:30:03.242979: step 4697, loss 1.30801, acc 0.9375, prec 0.0859425, recall 0.796752
2017-12-10T05:30:03.513485: step 4698, loss 0.042597, acc 0.96875, prec 0.0859398, recall 0.796752
2017-12-10T05:30:03.780808: step 4699, loss 0.247956, acc 0.953125, prec 0.0859503, recall 0.796782
2017-12-10T05:30:04.047097: step 4700, loss 0.258308, acc 0.921875, prec 0.0859728, recall 0.796842
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-4700

2017-12-10T05:30:05.477217: step 4701, loss 0.177265, acc 0.921875, prec 0.0859952, recall 0.796903
2017-12-10T05:30:05.751897: step 4702, loss 0.0523321, acc 0.96875, prec 0.0859925, recall 0.796903
2017-12-10T05:30:06.019489: step 4703, loss 0.192871, acc 0.921875, prec 0.0860003, recall 0.796933
2017-12-10T05:30:06.283948: step 4704, loss 0.300319, acc 0.953125, prec 0.0860255, recall 0.796994
2017-12-10T05:30:06.553471: step 4705, loss 0.328603, acc 0.96875, prec 0.0860374, recall 0.797024
2017-12-10T05:30:06.823310: step 4706, loss 0.319029, acc 0.90625, prec 0.0860585, recall 0.797084
2017-12-10T05:30:07.082810: step 4707, loss 0.201491, acc 0.96875, prec 0.0860704, recall 0.797114
2017-12-10T05:30:07.353707: step 4708, loss 0.0933175, acc 0.9375, prec 0.0861089, recall 0.797205
2017-12-10T05:30:07.621020: step 4709, loss 0.371973, acc 0.953125, prec 0.0861194, recall 0.797235
2017-12-10T05:30:07.885981: step 4710, loss 0.217279, acc 0.9375, prec 0.0861432, recall 0.797295
2017-12-10T05:30:08.154756: step 4711, loss 0.16907, acc 0.953125, prec 0.0861537, recall 0.797325
2017-12-10T05:30:08.423644: step 4712, loss 0.0567404, acc 0.984375, prec 0.086167, recall 0.797356
2017-12-10T05:30:08.696415: step 4713, loss 0.19819, acc 0.984375, prec 0.0861657, recall 0.797356
2017-12-10T05:30:08.962589: step 4714, loss 0.158187, acc 0.96875, prec 0.0862069, recall 0.797446
2017-12-10T05:30:09.943444: step 4715, loss 0.276983, acc 0.9375, prec 0.086216, recall 0.797476
2017-12-10T05:30:10.304669: step 4716, loss 0.116505, acc 0.953125, prec 0.0862265, recall 0.797506
2017-12-10T05:30:10.578764: step 4717, loss 0.205596, acc 0.96875, prec 0.0862238, recall 0.797506
2017-12-10T05:30:10.959375: step 4718, loss 0.055247, acc 0.984375, prec 0.0862371, recall 0.797536
2017-12-10T05:30:11.686463: step 4719, loss 0.187714, acc 0.9375, prec 0.0862315, recall 0.797536
2017-12-10T05:30:12.425787: step 4720, loss 0.055997, acc 0.984375, prec 0.0862595, recall 0.797596
2017-12-10T05:30:13.469424: step 4721, loss 0.031195, acc 0.984375, prec 0.0862581, recall 0.797596
2017-12-10T05:30:13.820699: step 4722, loss 0.116094, acc 0.953125, prec 0.0862686, recall 0.797626
2017-12-10T05:30:14.099702: step 4723, loss 0.14072, acc 0.96875, prec 0.0862805, recall 0.797656
2017-12-10T05:30:14.381867: step 4724, loss 0.0364322, acc 0.984375, prec 0.0862938, recall 0.797686
2017-12-10T05:30:14.674055: step 4725, loss 0.352905, acc 0.953125, prec 0.0862896, recall 0.797686
2017-12-10T05:30:14.939435: step 4726, loss 0.0353918, acc 0.984375, prec 0.0863175, recall 0.797746
2017-12-10T05:30:15.200003: step 4727, loss 0.115468, acc 0.953125, prec 0.0863427, recall 0.797806
2017-12-10T05:30:15.473446: step 4728, loss 0.0614844, acc 0.984375, prec 0.086356, recall 0.797836
2017-12-10T05:30:15.737350: step 4729, loss 0.161128, acc 0.9375, prec 0.0863651, recall 0.797866
2017-12-10T05:30:15.999118: step 4730, loss 0.0651382, acc 0.984375, prec 0.0863637, recall 0.797866
2017-12-10T05:30:16.265424: step 4731, loss 0.0400437, acc 0.984375, prec 0.0863623, recall 0.797866
2017-12-10T05:30:16.539696: step 4732, loss 0.132806, acc 0.984375, prec 0.0863609, recall 0.797866
2017-12-10T05:30:16.820226: step 4733, loss 0.0163383, acc 0.984375, prec 0.0863742, recall 0.797896
2017-12-10T05:30:17.093325: step 4734, loss 0.262031, acc 0.984375, prec 0.0863728, recall 0.797896
2017-12-10T05:30:17.365084: step 4735, loss 0.0290102, acc 0.984375, prec 0.0863714, recall 0.797896
2017-12-10T05:30:17.627922: step 4736, loss 6.60571, acc 0.984375, prec 0.0863714, recall 0.797778
2017-12-10T05:30:17.899665: step 4737, loss 0.0273316, acc 0.984375, prec 0.0863701, recall 0.797778
2017-12-10T05:30:18.170269: step 4738, loss 2.96411, acc 0.9375, prec 0.0863805, recall 0.79769
2017-12-10T05:30:18.440803: step 4739, loss 0.102526, acc 0.96875, prec 0.0864364, recall 0.797809
2017-12-10T05:30:18.707501: step 4740, loss 0.182103, acc 0.921875, prec 0.0864441, recall 0.797839
2017-12-10T05:30:18.967882: step 4741, loss 0.388609, acc 0.828125, prec 0.0864581, recall 0.797899
2017-12-10T05:30:19.231681: step 4742, loss 0.591749, acc 0.828125, prec 0.0864576, recall 0.797929
2017-12-10T05:30:19.495342: step 4743, loss 0.521819, acc 0.859375, prec 0.0864597, recall 0.797959
2017-12-10T05:30:19.759783: step 4744, loss 0.613748, acc 0.875, prec 0.0864486, recall 0.797959
2017-12-10T05:30:20.024021: step 4745, loss 1.174, acc 0.765625, prec 0.0864279, recall 0.797959
2017-12-10T05:30:20.293954: step 4746, loss 0.709288, acc 0.8125, prec 0.0864113, recall 0.797959
2017-12-10T05:30:20.555372: step 4747, loss 1.47462, acc 0.78125, prec 0.0863919, recall 0.797959
2017-12-10T05:30:20.821354: step 4748, loss 1.51281, acc 0.78125, prec 0.0863871, recall 0.797989
2017-12-10T05:30:21.083174: step 4749, loss 1.13908, acc 0.78125, prec 0.0863824, recall 0.798019
2017-12-10T05:30:21.351553: step 4750, loss 0.664108, acc 0.890625, prec 0.0863874, recall 0.798048
2017-12-10T05:30:21.623067: step 4751, loss 0.827957, acc 0.8125, prec 0.0864146, recall 0.798138
2017-12-10T05:30:21.885752: step 4752, loss 0.788444, acc 0.875, prec 0.0864182, recall 0.798168
2017-12-10T05:30:22.152498: step 4753, loss 0.756403, acc 0.828125, prec 0.086403, recall 0.798168
2017-12-10T05:30:22.418199: step 4754, loss 0.412795, acc 0.921875, prec 0.0864107, recall 0.798198
2017-12-10T05:30:22.685368: step 4755, loss 0.265857, acc 0.921875, prec 0.0864184, recall 0.798227
2017-12-10T05:30:22.957891: step 4756, loss 0.284298, acc 0.921875, prec 0.0864115, recall 0.798227
2017-12-10T05:30:23.227265: step 4757, loss 0.243764, acc 0.953125, prec 0.0864073, recall 0.798227
2017-12-10T05:30:23.493778: step 4758, loss 0.081397, acc 0.984375, prec 0.0864351, recall 0.798287
2017-12-10T05:30:23.762041: step 4759, loss 0.0974769, acc 0.96875, prec 0.0864324, recall 0.798287
2017-12-10T05:30:24.025644: step 4760, loss 0.12478, acc 0.96875, prec 0.0864588, recall 0.798347
2017-12-10T05:30:24.293289: step 4761, loss 0.144558, acc 0.96875, prec 0.0864707, recall 0.798376
2017-12-10T05:30:24.560778: step 4762, loss 0.381982, acc 0.953125, prec 0.0864665, recall 0.798376
2017-12-10T05:30:24.832392: step 4763, loss 0.122935, acc 0.96875, prec 0.0864784, recall 0.798406
2017-12-10T05:30:25.100900: step 4764, loss 0.0850417, acc 0.984375, prec 0.086477, recall 0.798406
2017-12-10T05:30:25.374144: step 4765, loss 0.0598384, acc 0.953125, prec 0.0864874, recall 0.798436
2017-12-10T05:30:25.642726: step 4766, loss 0.00266376, acc 1, prec 0.086502, recall 0.798466
2017-12-10T05:30:25.905178: step 4767, loss 0.331411, acc 0.984375, prec 0.0865299, recall 0.798525
2017-12-10T05:30:26.172609: step 4768, loss 0.0940831, acc 0.984375, prec 0.0865723, recall 0.798614
2017-12-10T05:30:26.447441: step 4769, loss 0.186195, acc 0.953125, prec 0.0865827, recall 0.798644
2017-12-10T05:30:26.715923: step 4770, loss 5.36386, acc 0.953125, prec 0.0866237, recall 0.798615
2017-12-10T05:30:26.991932: step 4771, loss 0.0427913, acc 0.984375, prec 0.0866369, recall 0.798645
2017-12-10T05:30:27.269621: step 4772, loss 0.00573164, acc 1, prec 0.0866515, recall 0.798675
2017-12-10T05:30:27.541391: step 4773, loss 0.428869, acc 0.9375, prec 0.086646, recall 0.798675
2017-12-10T05:30:27.814928: step 4774, loss 0.0665476, acc 0.96875, prec 0.0866578, recall 0.798704
2017-12-10T05:30:28.081068: step 4775, loss 0.10345, acc 0.96875, prec 0.0866551, recall 0.798704
2017-12-10T05:30:28.354796: step 4776, loss 0.461043, acc 0.9375, prec 0.0866495, recall 0.798704
2017-12-10T05:30:28.622660: step 4777, loss 0.0819499, acc 0.9375, prec 0.0866586, recall 0.798734
2017-12-10T05:30:28.890786: step 4778, loss 0.422347, acc 0.90625, prec 0.0866649, recall 0.798763
2017-12-10T05:30:29.167373: step 4779, loss 0.180349, acc 0.953125, prec 0.0866753, recall 0.798793
2017-12-10T05:30:29.436211: step 4780, loss 0.596409, acc 0.921875, prec 0.0866975, recall 0.798852
2017-12-10T05:30:29.705920: step 4781, loss 0.00604524, acc 1, prec 0.0866975, recall 0.798852
2017-12-10T05:30:29.977889: step 4782, loss 0.100304, acc 0.984375, prec 0.0866962, recall 0.798852
2017-12-10T05:30:30.253604: step 4783, loss 0.163099, acc 0.953125, prec 0.0867358, recall 0.798941
2017-12-10T05:30:30.530788: step 4784, loss 0.269261, acc 0.921875, prec 0.0867288, recall 0.798941
2017-12-10T05:30:30.803937: step 4785, loss 0.380579, acc 0.90625, prec 0.0867205, recall 0.798941
2017-12-10T05:30:31.070175: step 4786, loss 0.201777, acc 0.984375, prec 0.0867483, recall 0.799
2017-12-10T05:30:31.345425: step 4787, loss 0.179779, acc 0.953125, prec 0.0867587, recall 0.79903
2017-12-10T05:30:31.611258: step 4788, loss 0.38593, acc 0.9375, prec 0.0867678, recall 0.799059
2017-12-10T05:30:31.879434: step 4789, loss 0.232315, acc 0.921875, prec 0.0867608, recall 0.799059
2017-12-10T05:30:32.143967: step 4790, loss 0.103379, acc 0.953125, prec 0.0867858, recall 0.799118
2017-12-10T05:30:32.407147: step 4791, loss 0.572897, acc 0.90625, prec 0.0867775, recall 0.799118
2017-12-10T05:30:32.671742: step 4792, loss 0.0603825, acc 0.953125, prec 0.0867734, recall 0.799118
2017-12-10T05:30:32.950590: step 4793, loss 0.0131186, acc 1, prec 0.0867734, recall 0.799118
2017-12-10T05:30:33.224641: step 4794, loss 0.0654663, acc 0.96875, prec 0.0867852, recall 0.799148
2017-12-10T05:30:33.493678: step 4795, loss 0.069502, acc 0.984375, prec 0.0867838, recall 0.799148
2017-12-10T05:30:33.758564: step 4796, loss 0.016661, acc 1, prec 0.0867984, recall 0.799177
2017-12-10T05:30:34.022952: step 4797, loss 1.83357, acc 0.953125, prec 0.0867956, recall 0.79906
2017-12-10T05:30:34.293964: step 4798, loss 0.14584, acc 0.96875, prec 0.0868074, recall 0.799089
2017-12-10T05:30:34.556983: step 4799, loss 0.199097, acc 0.984375, prec 0.086806, recall 0.799089
2017-12-10T05:30:34.825326: step 4800, loss 0.413001, acc 1, prec 0.0868206, recall 0.799119

Evaluation:
2017-12-10T05:30:42.465243: step 4800, loss 4.51942, acc 0.949613, prec 0.0870845, recall 0.78995

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-4800

2017-12-10T05:30:43.736537: step 4801, loss 0.185507, acc 0.9375, prec 0.087079, recall 0.78995
2017-12-10T05:30:43.998402: step 4802, loss 0.457816, acc 0.96875, prec 0.0871051, recall 0.79001
2017-12-10T05:30:44.274093: step 4803, loss 0.0550366, acc 0.984375, prec 0.0871038, recall 0.79001
2017-12-10T05:30:44.543449: step 4804, loss 0.116469, acc 0.96875, prec 0.0871155, recall 0.79004
2017-12-10T05:30:44.808935: step 4805, loss 0.0816397, acc 0.96875, prec 0.0871271, recall 0.79007
2017-12-10T05:30:45.078181: step 4806, loss 0.185832, acc 0.96875, prec 0.0871388, recall 0.7901
2017-12-10T05:30:45.353610: step 4807, loss 0.400823, acc 0.953125, prec 0.0871491, recall 0.790131
2017-12-10T05:30:45.619241: step 4808, loss 0.178723, acc 0.953125, prec 0.087145, recall 0.790131
2017-12-10T05:30:45.891135: step 4809, loss 0.288048, acc 0.890625, prec 0.0871354, recall 0.790131
2017-12-10T05:30:46.156896: step 4810, loss 0.137342, acc 0.9375, prec 0.0871443, recall 0.790161
2017-12-10T05:30:46.429688: step 4811, loss 0.142967, acc 0.953125, prec 0.0871401, recall 0.790161
2017-12-10T05:30:46.692494: step 4812, loss 0.372477, acc 0.96875, prec 0.0871374, recall 0.790161
2017-12-10T05:30:46.965169: step 4813, loss 0.594895, acc 0.890625, prec 0.0871422, recall 0.790191
2017-12-10T05:30:47.231157: step 4814, loss 0.352675, acc 0.921875, prec 0.0871642, recall 0.790251
2017-12-10T05:30:47.499660: step 4815, loss 0.47147, acc 0.90625, prec 0.0871703, recall 0.790281
2017-12-10T05:30:47.769007: step 4816, loss 0.0969501, acc 0.96875, prec 0.087182, recall 0.790311
2017-12-10T05:30:48.034935: step 4817, loss 0.335661, acc 0.953125, prec 0.0871779, recall 0.790311
2017-12-10T05:30:48.303252: step 4818, loss 0.449479, acc 0.875, prec 0.0871813, recall 0.790341
2017-12-10T05:30:48.571358: step 4819, loss 0.0884553, acc 0.953125, prec 0.0871771, recall 0.790341
2017-12-10T05:30:48.839086: step 4820, loss 0.510278, acc 0.921875, prec 0.0871847, recall 0.790371
2017-12-10T05:30:49.105935: step 4821, loss 0.118338, acc 0.96875, prec 0.0871963, recall 0.790401
2017-12-10T05:30:49.375117: step 4822, loss 0.0559142, acc 0.96875, prec 0.087208, recall 0.790431
2017-12-10T05:30:49.641607: step 4823, loss 0.010861, acc 1, prec 0.0872224, recall 0.790461
2017-12-10T05:30:49.906530: step 4824, loss 0.230215, acc 0.96875, prec 0.0872341, recall 0.790491
2017-12-10T05:30:50.171590: step 4825, loss 0.0669368, acc 0.984375, prec 0.0872327, recall 0.790491
2017-12-10T05:30:50.447014: step 4826, loss 0.117394, acc 0.953125, prec 0.0872286, recall 0.790491
2017-12-10T05:30:50.719048: step 4827, loss 0.0857884, acc 0.953125, prec 0.0872245, recall 0.790491
2017-12-10T05:30:50.986999: step 4828, loss 0.0858784, acc 0.984375, prec 0.0872231, recall 0.790491
2017-12-10T05:30:51.258168: step 4829, loss 0.0679489, acc 0.984375, prec 0.0872361, recall 0.790521
2017-12-10T05:30:51.536137: step 4830, loss 0.0716662, acc 0.984375, prec 0.0872492, recall 0.790551
2017-12-10T05:30:51.801572: step 4831, loss 0.136231, acc 0.96875, prec 0.0872464, recall 0.790551
2017-12-10T05:30:52.081054: step 4832, loss 0.262435, acc 0.984375, prec 0.0872595, recall 0.790581
2017-12-10T05:30:52.349523: step 4833, loss 0.0693064, acc 0.96875, prec 0.0872855, recall 0.790641
2017-12-10T05:30:52.615476: step 4834, loss 3.45809, acc 0.96875, prec 0.0872986, recall 0.790558
2017-12-10T05:30:52.893626: step 4835, loss 0.0111475, acc 1, prec 0.0872986, recall 0.790558
2017-12-10T05:30:53.167736: step 4836, loss 0.218253, acc 0.96875, prec 0.0872958, recall 0.790558
2017-12-10T05:30:53.440851: step 4837, loss 0.0797429, acc 0.984375, prec 0.0873377, recall 0.790648
2017-12-10T05:30:53.707056: step 4838, loss 0.164884, acc 0.9375, prec 0.0873322, recall 0.790648
2017-12-10T05:30:53.969803: step 4839, loss 0.438202, acc 0.890625, prec 0.0873225, recall 0.790648
2017-12-10T05:30:54.236959: step 4840, loss 0.0801213, acc 0.984375, prec 0.0873356, recall 0.790678
2017-12-10T05:30:54.501842: step 4841, loss 0.170104, acc 0.96875, prec 0.0873328, recall 0.790678
2017-12-10T05:30:54.765602: step 4842, loss 0.122064, acc 0.953125, prec 0.0873431, recall 0.790708
2017-12-10T05:30:55.045035: step 4843, loss 0.255523, acc 0.9375, prec 0.087352, recall 0.790738
2017-12-10T05:30:55.310642: step 4844, loss 0.172897, acc 0.96875, prec 0.0873492, recall 0.790738
2017-12-10T05:30:55.587234: step 4845, loss 0.11712, acc 0.9375, prec 0.0873581, recall 0.790767
2017-12-10T05:30:55.850317: step 4846, loss 0.101328, acc 0.953125, prec 0.0873684, recall 0.790797
2017-12-10T05:30:56.118679: step 4847, loss 0.789236, acc 0.9375, prec 0.0873629, recall 0.790797
2017-12-10T05:30:56.385274: step 4848, loss 0.0704199, acc 0.96875, prec 0.0873745, recall 0.790827
2017-12-10T05:30:56.660107: step 4849, loss 0.0759856, acc 0.984375, prec 0.0873731, recall 0.790827
2017-12-10T05:30:56.927887: step 4850, loss 0.0841809, acc 0.96875, prec 0.0873704, recall 0.790827
2017-12-10T05:30:57.204448: step 4851, loss 0.199747, acc 0.96875, prec 0.087382, recall 0.790857
2017-12-10T05:30:57.483975: step 4852, loss 0.16461, acc 0.96875, prec 0.0874081, recall 0.790917
2017-12-10T05:30:57.751004: step 4853, loss 0.267773, acc 0.921875, prec 0.0874156, recall 0.790947
2017-12-10T05:30:58.024394: step 4854, loss 0.412233, acc 0.921875, prec 0.0874087, recall 0.790947
2017-12-10T05:30:58.294143: step 4855, loss 0.306793, acc 0.921875, prec 0.0874162, recall 0.790977
2017-12-10T05:30:58.571083: step 4856, loss 0.248823, acc 0.9375, prec 0.0874107, recall 0.790977
2017-12-10T05:30:58.845451: step 4857, loss 0.0207904, acc 1, prec 0.0874251, recall 0.791006
2017-12-10T05:30:59.112388: step 4858, loss 0.521518, acc 0.875, prec 0.087414, recall 0.791006
2017-12-10T05:30:59.374247: step 4859, loss 0.163571, acc 0.96875, prec 0.0874257, recall 0.791036
2017-12-10T05:30:59.640540: step 4860, loss 0.0620544, acc 0.984375, prec 0.0874243, recall 0.791036
2017-12-10T05:30:59.907929: step 4861, loss 0.0384117, acc 1, prec 0.0874819, recall 0.791156
2017-12-10T05:31:00.175840: step 4862, loss 0.07247, acc 0.984375, prec 0.0875093, recall 0.791215
2017-12-10T05:31:00.451344: step 4863, loss 0.220582, acc 0.953125, prec 0.0875339, recall 0.791275
2017-12-10T05:31:00.728274: step 4864, loss 0.178589, acc 1, prec 0.0875915, recall 0.791394
2017-12-10T05:31:01.002774: step 4865, loss 0.0688141, acc 0.96875, prec 0.0875887, recall 0.791394
2017-12-10T05:31:01.273942: step 4866, loss 0.00820296, acc 1, prec 0.0876175, recall 0.791453
2017-12-10T05:31:01.547654: step 4867, loss 0.182908, acc 0.984375, prec 0.0876161, recall 0.791453
2017-12-10T05:31:01.812279: step 4868, loss 0.15207, acc 0.984375, prec 0.0876147, recall 0.791453
2017-12-10T05:31:02.079971: step 4869, loss 1.9808, acc 0.9375, prec 0.0876394, recall 0.7914
2017-12-10T05:31:02.344846: step 4870, loss 0.0323316, acc 1, prec 0.0876537, recall 0.791429
2017-12-10T05:31:02.612602: step 4871, loss 1.24847, acc 0.9375, prec 0.087677, recall 0.791489
2017-12-10T05:31:02.883379: step 4872, loss 0.0961528, acc 0.96875, prec 0.087703, recall 0.791548
2017-12-10T05:31:03.148211: step 4873, loss 0.318327, acc 0.96875, prec 0.087729, recall 0.791607
2017-12-10T05:31:03.415789: step 4874, loss 0.458335, acc 0.875, prec 0.0877467, recall 0.791667
2017-12-10T05:31:03.682964: step 4875, loss 0.590039, acc 0.921875, prec 0.0877685, recall 0.791726
2017-12-10T05:31:03.945329: step 4876, loss 0.37716, acc 0.921875, prec 0.0877616, recall 0.791726
2017-12-10T05:31:04.207647: step 4877, loss 0.423451, acc 0.875, prec 0.0877649, recall 0.791755
2017-12-10T05:31:04.480302: step 4878, loss 0.147628, acc 0.9375, prec 0.0877594, recall 0.791755
2017-12-10T05:31:04.747489: step 4879, loss 0.946529, acc 0.828125, prec 0.0877585, recall 0.791785
2017-12-10T05:31:05.013732: step 4880, loss 0.722828, acc 0.828125, prec 0.0877577, recall 0.791815
2017-12-10T05:31:05.278903: step 4881, loss 0.575716, acc 0.90625, prec 0.0878212, recall 0.791963
2017-12-10T05:31:05.546361: step 4882, loss 0.603377, acc 0.828125, prec 0.087806, recall 0.791963
2017-12-10T05:31:05.818034: step 4883, loss 0.583913, acc 0.921875, prec 0.0878278, recall 0.792022
2017-12-10T05:31:06.090010: step 4884, loss 0.369896, acc 0.921875, prec 0.0878209, recall 0.792022
2017-12-10T05:31:06.352944: step 4885, loss 0.426902, acc 0.890625, prec 0.0878113, recall 0.792022
2017-12-10T05:31:06.621210: step 4886, loss 0.500298, acc 0.84375, prec 0.0877974, recall 0.792022
2017-12-10T05:31:06.883747: step 4887, loss 0.494739, acc 0.890625, prec 0.0878021, recall 0.792051
2017-12-10T05:31:07.147417: step 4888, loss 0.5055, acc 0.9375, prec 0.0878109, recall 0.792081
2017-12-10T05:31:07.421452: step 4889, loss 0.493109, acc 0.890625, prec 0.0878013, recall 0.792081
2017-12-10T05:31:07.691771: step 4890, loss 0.693071, acc 0.90625, prec 0.0878073, recall 0.79211
2017-12-10T05:31:07.958953: step 4891, loss 0.969278, acc 0.90625, prec 0.0878134, recall 0.79214
2017-12-10T05:31:08.228103: step 4892, loss 0.241866, acc 0.953125, prec 0.0878236, recall 0.792169
2017-12-10T05:31:08.497384: step 4893, loss 0.0047152, acc 1, prec 0.0878236, recall 0.792169
2017-12-10T05:31:08.755915: step 4894, loss 0.162492, acc 0.984375, prec 0.0878366, recall 0.792199
2017-12-10T05:31:09.017305: step 4895, loss 0.065768, acc 0.96875, prec 0.0878481, recall 0.792228
2017-12-10T05:31:09.287007: step 4896, loss 0.309799, acc 0.921875, prec 0.0878412, recall 0.792228
2017-12-10T05:31:09.556756: step 4897, loss 0.292587, acc 0.953125, prec 0.0878371, recall 0.792228
2017-12-10T05:31:09.824905: step 4898, loss 0.142561, acc 0.96875, prec 0.0878487, recall 0.792257
2017-12-10T05:31:10.090282: step 4899, loss 0.0215984, acc 0.984375, prec 0.0878473, recall 0.792257
2017-12-10T05:31:10.365337: step 4900, loss 0.0662966, acc 0.96875, prec 0.0878589, recall 0.792287
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-4900

2017-12-10T05:31:11.709102: step 4901, loss 0.0213158, acc 0.984375, prec 0.0878718, recall 0.792316
2017-12-10T05:31:11.981862: step 4902, loss 0.13142, acc 0.9375, prec 0.0878806, recall 0.792346
2017-12-10T05:31:12.258794: step 4903, loss 2.24234, acc 0.984375, prec 0.0878806, recall 0.792234
2017-12-10T05:31:12.531778: step 4904, loss 0.201799, acc 0.921875, prec 0.0878881, recall 0.792263
2017-12-10T05:31:12.797031: step 4905, loss 0.507112, acc 0.9375, prec 0.0878969, recall 0.792292
2017-12-10T05:31:13.069004: step 4906, loss 0.183794, acc 0.96875, prec 0.0878941, recall 0.792292
2017-12-10T05:31:13.342124: step 4907, loss 0.0632401, acc 0.96875, prec 0.0878914, recall 0.792292
2017-12-10T05:31:13.604253: step 4908, loss 0.0876378, acc 0.984375, prec 0.0879187, recall 0.792351
2017-12-10T05:31:13.871443: step 4909, loss 0.339495, acc 0.984375, prec 0.0879316, recall 0.792381
2017-12-10T05:31:14.136233: step 4910, loss 0.174496, acc 0.96875, prec 0.0879432, recall 0.79241
2017-12-10T05:31:14.407687: step 4911, loss 0.489476, acc 0.9375, prec 0.0879663, recall 0.792469
2017-12-10T05:31:14.677531: step 4912, loss 0.427005, acc 0.921875, prec 0.0879737, recall 0.792498
2017-12-10T05:31:14.942466: step 4913, loss 0.410181, acc 0.984375, prec 0.0879723, recall 0.792498
2017-12-10T05:31:15.211251: step 4914, loss 0.0556426, acc 0.96875, prec 0.0879696, recall 0.792498
2017-12-10T05:31:15.478001: step 4915, loss 1.24083, acc 0.953125, prec 0.0879798, recall 0.792528
2017-12-10T05:31:15.757710: step 4916, loss 0.229672, acc 0.921875, prec 0.0879872, recall 0.792557
2017-12-10T05:31:16.030684: step 4917, loss 0.181267, acc 0.984375, prec 0.0879858, recall 0.792557
2017-12-10T05:31:16.298855: step 4918, loss 0.0693011, acc 0.96875, prec 0.088026, recall 0.792645
2017-12-10T05:31:16.571359: step 4919, loss 0.0884173, acc 0.96875, prec 0.0880519, recall 0.792704
2017-12-10T05:31:16.843562: step 4920, loss 0.377742, acc 0.921875, prec 0.0880593, recall 0.792733
2017-12-10T05:31:17.113194: step 4921, loss 0.118982, acc 0.953125, prec 0.0880695, recall 0.792762
2017-12-10T05:31:17.375264: step 4922, loss 0.225073, acc 0.9375, prec 0.0880783, recall 0.792792
2017-12-10T05:31:17.641622: step 4923, loss 0.239332, acc 0.921875, prec 0.0880857, recall 0.792821
2017-12-10T05:31:17.920840: step 4924, loss 0.157107, acc 0.9375, prec 0.0880945, recall 0.79285
2017-12-10T05:31:18.191346: step 4925, loss 0.198974, acc 0.9375, prec 0.0880889, recall 0.79285
2017-12-10T05:31:18.459150: step 4926, loss 0.182165, acc 0.921875, prec 0.088082, recall 0.79285
2017-12-10T05:31:18.727940: step 4927, loss 0.373434, acc 0.953125, prec 0.0880922, recall 0.792879
2017-12-10T05:31:18.999503: step 4928, loss 0.306255, acc 0.9375, prec 0.0880866, recall 0.792879
2017-12-10T05:31:19.263234: step 4929, loss 0.495634, acc 0.9375, prec 0.0880811, recall 0.792879
2017-12-10T05:31:19.528463: step 4930, loss 0.00945161, acc 1, prec 0.0880811, recall 0.792879
2017-12-10T05:31:19.796284: step 4931, loss 0.207294, acc 0.953125, prec 0.0880913, recall 0.792909
2017-12-10T05:31:20.072543: step 4932, loss 0.0508184, acc 0.984375, prec 0.0880899, recall 0.792909
2017-12-10T05:31:20.343461: step 4933, loss 0.147653, acc 0.984375, prec 0.0881028, recall 0.792938
2017-12-10T05:31:20.615324: step 4934, loss 0.0646391, acc 1, prec 0.0881314, recall 0.792996
2017-12-10T05:31:20.884030: step 4935, loss 0.0696464, acc 0.96875, prec 0.0881287, recall 0.792996
2017-12-10T05:31:21.156774: step 4936, loss 0.0387349, acc 0.984375, prec 0.0881273, recall 0.792996
2017-12-10T05:31:21.429968: step 4937, loss 0.1772, acc 0.953125, prec 0.0881375, recall 0.793026
2017-12-10T05:31:21.694895: step 4938, loss 0.0940895, acc 0.984375, prec 0.0881504, recall 0.793055
2017-12-10T05:31:21.966145: step 4939, loss 0.0152523, acc 1, prec 0.0881647, recall 0.793084
2017-12-10T05:31:22.229125: step 4940, loss 0.0668137, acc 0.984375, prec 0.0881633, recall 0.793084
2017-12-10T05:31:22.501185: step 4941, loss 0.0773696, acc 0.96875, prec 0.0881605, recall 0.793084
2017-12-10T05:31:22.768869: step 4942, loss 0.0583893, acc 0.984375, prec 0.0881735, recall 0.793113
2017-12-10T05:31:23.033403: step 4943, loss 0.184454, acc 0.984375, prec 0.0882007, recall 0.793172
2017-12-10T05:31:23.309808: step 4944, loss 0.102925, acc 0.953125, prec 0.0882108, recall 0.793201
2017-12-10T05:31:23.577430: step 4945, loss 0.0751789, acc 0.953125, prec 0.088221, recall 0.79323
2017-12-10T05:31:23.854770: step 4946, loss 0.0225606, acc 1, prec 0.0882353, recall 0.793259
2017-12-10T05:31:24.126841: step 4947, loss 0.00255738, acc 1, prec 0.0882353, recall 0.793259
2017-12-10T05:31:24.391432: step 4948, loss 0.15853, acc 0.96875, prec 0.0882468, recall 0.793288
2017-12-10T05:31:24.658508: step 4949, loss 0.0105809, acc 1, prec 0.0882611, recall 0.793317
2017-12-10T05:31:24.920586: step 4950, loss 0.00159343, acc 1, prec 0.0882611, recall 0.793317
2017-12-10T05:31:25.183865: step 4951, loss 1.00795, acc 1, prec 0.0882754, recall 0.793346
2017-12-10T05:31:25.457023: step 4952, loss 0.139073, acc 0.953125, prec 0.0882856, recall 0.793376
2017-12-10T05:31:25.723784: step 4953, loss 0.06741, acc 0.984375, prec 0.0883128, recall 0.793434
2017-12-10T05:31:25.987410: step 4954, loss 0.0184496, acc 0.984375, prec 0.0883114, recall 0.793434
2017-12-10T05:31:26.249648: step 4955, loss 0.140581, acc 0.953125, prec 0.0883072, recall 0.793434
2017-12-10T05:31:26.518180: step 4956, loss 0.0162398, acc 0.984375, prec 0.0883059, recall 0.793434
2017-12-10T05:31:26.787404: step 4957, loss 0.246425, acc 0.953125, prec 0.088316, recall 0.793463
2017-12-10T05:31:27.058999: step 4958, loss 0.240893, acc 0.953125, prec 0.0883119, recall 0.793463
2017-12-10T05:31:27.329728: step 4959, loss 0.109061, acc 0.96875, prec 0.0883234, recall 0.793492
2017-12-10T05:31:27.599226: step 4960, loss 0.0619032, acc 0.96875, prec 0.0883206, recall 0.793492
2017-12-10T05:31:27.868264: step 4961, loss 0.102885, acc 0.953125, prec 0.0883165, recall 0.793492
2017-12-10T05:31:28.140897: step 4962, loss 0.0229597, acc 1, prec 0.0883165, recall 0.793492
2017-12-10T05:31:28.406113: step 4963, loss 0.544178, acc 1, prec 0.0883593, recall 0.793579
2017-12-10T05:31:28.678033: step 4964, loss 0.257855, acc 0.953125, prec 0.0883695, recall 0.793608
2017-12-10T05:31:28.944498: step 4965, loss 0.0494451, acc 0.96875, prec 0.0883667, recall 0.793608
2017-12-10T05:31:29.212669: step 4966, loss 0.357552, acc 0.96875, prec 0.0883925, recall 0.793666
2017-12-10T05:31:29.482412: step 4967, loss 0.289344, acc 0.9375, prec 0.0884013, recall 0.793695
2017-12-10T05:31:29.744276: step 4968, loss 0.385888, acc 0.953125, prec 0.0884257, recall 0.793754
2017-12-10T05:31:30.006139: step 4969, loss 0.197878, acc 0.953125, prec 0.0884358, recall 0.793783
2017-12-10T05:31:30.242774: step 4970, loss 0.416572, acc 0.923077, prec 0.0884445, recall 0.793812
2017-12-10T05:31:30.519331: step 4971, loss 0.094276, acc 0.96875, prec 0.0884703, recall 0.793869
2017-12-10T05:31:30.792467: step 4972, loss 0.281926, acc 0.921875, prec 0.0884777, recall 0.793899
2017-12-10T05:31:31.063323: step 4973, loss 0.305616, acc 0.90625, prec 0.0884837, recall 0.793927
2017-12-10T05:31:31.325510: step 4974, loss 0.118751, acc 0.953125, prec 0.0884938, recall 0.793956
2017-12-10T05:31:31.593302: step 4975, loss 0.0469311, acc 0.96875, prec 0.0885053, recall 0.793985
2017-12-10T05:31:31.859540: step 4976, loss 0.204622, acc 0.953125, prec 0.0885154, recall 0.794014
2017-12-10T05:31:32.129707: step 4977, loss 0.0646847, acc 0.96875, prec 0.0885269, recall 0.794043
2017-12-10T05:31:32.406009: step 4978, loss 0.5165, acc 0.921875, prec 0.0885485, recall 0.794101
2017-12-10T05:31:32.672715: step 4979, loss 0.297871, acc 0.921875, prec 0.0885559, recall 0.79413
2017-12-10T05:31:32.949112: step 4980, loss 0.0472816, acc 0.984375, prec 0.0885545, recall 0.79413
2017-12-10T05:31:33.215034: step 4981, loss 0.0186055, acc 0.984375, prec 0.0885816, recall 0.794188
2017-12-10T05:31:33.483737: step 4982, loss 0.0970893, acc 0.96875, prec 0.0886074, recall 0.794246
2017-12-10T05:31:33.750887: step 4983, loss 0.0464734, acc 0.984375, prec 0.088606, recall 0.794246
2017-12-10T05:31:34.030828: step 4984, loss 0.0153888, acc 1, prec 0.0886203, recall 0.794275
2017-12-10T05:31:34.308019: step 4985, loss 0.134631, acc 0.984375, prec 0.0886332, recall 0.794303
2017-12-10T05:31:34.582900: step 4986, loss 0.0907771, acc 0.96875, prec 0.0886589, recall 0.794361
2017-12-10T05:31:34.850751: step 4987, loss 0.0629589, acc 0.96875, prec 0.0886704, recall 0.79439
2017-12-10T05:31:35.114019: step 4988, loss 0.062376, acc 0.984375, prec 0.0887118, recall 0.794476
2017-12-10T05:31:35.385926: step 4989, loss 0.00136636, acc 1, prec 0.0887118, recall 0.794476
2017-12-10T05:31:35.652816: step 4990, loss 0.78175, acc 0.96875, prec 0.0887518, recall 0.794563
2017-12-10T05:31:35.922452: step 4991, loss 0.172285, acc 0.96875, prec 0.0887776, recall 0.79462
2017-12-10T05:31:36.195279: step 4992, loss 0.0801424, acc 0.984375, prec 0.0887762, recall 0.79462
2017-12-10T05:31:36.461824: step 4993, loss 0.0781709, acc 0.984375, prec 0.0888033, recall 0.794678
2017-12-10T05:31:36.728265: step 4994, loss 0.0296456, acc 1, prec 0.0888176, recall 0.794707
2017-12-10T05:31:37.000707: step 4995, loss 0.0388603, acc 0.984375, prec 0.0888305, recall 0.794735
2017-12-10T05:31:37.267776: step 4996, loss 0.549165, acc 0.96875, prec 0.0888562, recall 0.794793
2017-12-10T05:31:37.531485: step 4997, loss 0.0248516, acc 1, prec 0.0889132, recall 0.794908
2017-12-10T05:31:37.798803: step 4998, loss 0.256085, acc 0.984375, prec 0.0889546, recall 0.794994
2017-12-10T05:31:38.065990: step 4999, loss 0.325592, acc 0.984375, prec 0.0889817, recall 0.795051
2017-12-10T05:31:38.334524: step 5000, loss 0.169319, acc 0.96875, prec 0.0889789, recall 0.795051
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-5000

2017-12-10T05:31:39.633847: step 5001, loss 0.283009, acc 0.953125, prec 0.088989, recall 0.79508
2017-12-10T05:31:39.903616: step 5002, loss 0.239126, acc 0.953125, prec 0.0889848, recall 0.79508
2017-12-10T05:31:40.172769: step 5003, loss 0.173975, acc 0.96875, prec 0.0889963, recall 0.795108
2017-12-10T05:31:40.437405: step 5004, loss 0.10237, acc 0.96875, prec 0.0890078, recall 0.795137
2017-12-10T05:31:40.702110: step 5005, loss 0.0319464, acc 0.96875, prec 0.0890477, recall 0.795223
2017-12-10T05:31:40.971438: step 5006, loss 0.0753753, acc 0.96875, prec 0.0890734, recall 0.79528
2017-12-10T05:31:41.239800: step 5007, loss 0.0643872, acc 0.953125, prec 0.089112, recall 0.795366
2017-12-10T05:31:41.505201: step 5008, loss 0.253806, acc 0.9375, prec 0.0891207, recall 0.795394
2017-12-10T05:31:41.770278: step 5009, loss 0.0129885, acc 1, prec 0.0891349, recall 0.795423
2017-12-10T05:31:42.046483: step 5010, loss 0.124809, acc 0.9375, prec 0.0891578, recall 0.79548
2017-12-10T05:31:42.323096: step 5011, loss 0.231243, acc 0.96875, prec 0.0891978, recall 0.795565
2017-12-10T05:31:42.591692: step 5012, loss 0.303695, acc 0.953125, prec 0.0892363, recall 0.795651
2017-12-10T05:31:42.855942: step 5013, loss 0.625388, acc 0.96875, prec 0.0892335, recall 0.795651
2017-12-10T05:31:43.123054: step 5014, loss 0.0684701, acc 0.953125, prec 0.0892293, recall 0.795651
2017-12-10T05:31:43.395124: step 5015, loss 0.0954242, acc 0.96875, prec 0.0892265, recall 0.795651
2017-12-10T05:31:43.664194: step 5016, loss 0.0377573, acc 0.96875, prec 0.089238, recall 0.795679
2017-12-10T05:31:43.932181: step 5017, loss 0.410893, acc 0.953125, prec 0.0892338, recall 0.795679
2017-12-10T05:31:44.198658: step 5018, loss 0.466212, acc 0.921875, prec 0.0892411, recall 0.795708
2017-12-10T05:31:44.465830: step 5019, loss 0.302931, acc 0.96875, prec 0.0892383, recall 0.795708
2017-12-10T05:31:44.731556: step 5020, loss 0.113196, acc 0.953125, prec 0.0892341, recall 0.795708
2017-12-10T05:31:44.999734: step 5021, loss 0.504243, acc 0.96875, prec 0.0892455, recall 0.795736
2017-12-10T05:31:45.268516: step 5022, loss 0.407396, acc 0.9375, prec 0.0892542, recall 0.795765
2017-12-10T05:31:45.535382: step 5023, loss 0.308432, acc 0.953125, prec 0.0892642, recall 0.795793
2017-12-10T05:31:45.803639: step 5024, loss 0.0177906, acc 1, prec 0.0892927, recall 0.79585
2017-12-10T05:31:46.067237: step 5025, loss 0.0598953, acc 0.96875, prec 0.0892899, recall 0.79585
2017-12-10T05:31:46.334288: step 5026, loss 0.039799, acc 1, prec 0.0893326, recall 0.795935
2017-12-10T05:31:46.602146: step 5027, loss 0.0217158, acc 1, prec 0.0893468, recall 0.795964
2017-12-10T05:31:46.871392: step 5028, loss 0.0786831, acc 0.984375, prec 0.0893596, recall 0.795992
2017-12-10T05:31:47.147599: step 5029, loss 0.00221154, acc 1, prec 0.0893739, recall 0.796021
2017-12-10T05:31:47.407828: step 5030, loss 0.0171614, acc 1, prec 0.0894023, recall 0.796077
2017-12-10T05:31:47.678547: step 5031, loss 1.74058, acc 0.921875, prec 0.0893967, recall 0.795967
2017-12-10T05:31:47.950430: step 5032, loss 0.08973, acc 0.984375, prec 0.0894096, recall 0.795995
2017-12-10T05:31:48.215690: step 5033, loss 0.0508306, acc 0.984375, prec 0.0894082, recall 0.795995
2017-12-10T05:31:48.487896: step 5034, loss 0.0493881, acc 0.984375, prec 0.0894068, recall 0.795995
2017-12-10T05:31:48.754027: step 5035, loss 0.0641385, acc 0.96875, prec 0.0894324, recall 0.796052
2017-12-10T05:31:49.027058: step 5036, loss 0.355704, acc 0.96875, prec 0.0894438, recall 0.79608
2017-12-10T05:31:49.289173: step 5037, loss 0.0941365, acc 0.96875, prec 0.0894837, recall 0.796165
2017-12-10T05:31:49.561383: step 5038, loss 0.0329525, acc 0.984375, prec 0.089525, recall 0.79625
2017-12-10T05:31:49.832367: step 5039, loss 0.192969, acc 0.984375, prec 0.0895378, recall 0.796278
2017-12-10T05:31:50.100114: step 5040, loss 0.0639215, acc 0.96875, prec 0.0895634, recall 0.796335
2017-12-10T05:31:50.365477: step 5041, loss 0.192905, acc 0.90625, prec 0.0895835, recall 0.796391
2017-12-10T05:31:50.636082: step 5042, loss 0.17828, acc 0.96875, prec 0.0895807, recall 0.796391
2017-12-10T05:31:50.907096: step 5043, loss 0.161922, acc 0.953125, prec 0.0895765, recall 0.796391
2017-12-10T05:31:51.177400: step 5044, loss 0.794337, acc 0.875, prec 0.0895937, recall 0.796448
2017-12-10T05:31:51.444640: step 5045, loss 0.0294533, acc 0.984375, prec 0.0896065, recall 0.796476
2017-12-10T05:31:51.708945: step 5046, loss 0.0140689, acc 1, prec 0.0896065, recall 0.796476
2017-12-10T05:31:51.980850: step 5047, loss 0.113513, acc 0.984375, prec 0.0896051, recall 0.796476
2017-12-10T05:31:52.246474: step 5048, loss 0.214809, acc 0.9375, prec 0.0896137, recall 0.796504
2017-12-10T05:31:52.516406: step 5049, loss 0.00812328, acc 1, prec 0.0896137, recall 0.796504
2017-12-10T05:31:52.794222: step 5050, loss 0.0118297, acc 1, prec 0.0896422, recall 0.796561
2017-12-10T05:31:53.062939: step 5051, loss 0.140685, acc 0.953125, prec 0.089638, recall 0.796561
2017-12-10T05:31:53.339706: step 5052, loss 0.38302, acc 0.96875, prec 0.0896352, recall 0.796561
2017-12-10T05:31:53.610009: step 5053, loss 0.0204446, acc 1, prec 0.0896352, recall 0.796561
2017-12-10T05:31:53.892249: step 5054, loss 0.181557, acc 0.9375, prec 0.089658, recall 0.796617
2017-12-10T05:31:54.164680: step 5055, loss 0.167098, acc 0.984375, prec 0.0896566, recall 0.796617
2017-12-10T05:31:54.433091: step 5056, loss 0.0745464, acc 0.96875, prec 0.089668, recall 0.796645
2017-12-10T05:31:54.703183: step 5057, loss 0.202721, acc 0.96875, prec 0.0896652, recall 0.796645
2017-12-10T05:31:54.967020: step 5058, loss 0.0938403, acc 0.984375, prec 0.089678, recall 0.796674
2017-12-10T05:31:55.236049: step 5059, loss 0.0381726, acc 0.984375, prec 0.0896766, recall 0.796674
2017-12-10T05:31:55.504303: step 5060, loss 0.00635464, acc 1, prec 0.089705, recall 0.79673
2017-12-10T05:31:55.772832: step 5061, loss 0.010698, acc 1, prec 0.0897192, recall 0.796758
2017-12-10T05:31:56.042515: step 5062, loss 0.222627, acc 0.984375, prec 0.0897462, recall 0.796814
2017-12-10T05:31:56.309600: step 5063, loss 0.0211345, acc 0.984375, prec 0.0897448, recall 0.796814
2017-12-10T05:31:56.582579: step 5064, loss 0.0913306, acc 1, prec 0.0897874, recall 0.796899
2017-12-10T05:31:56.850425: step 5065, loss 0.231562, acc 0.9375, prec 0.0897818, recall 0.796899
2017-12-10T05:31:57.122008: step 5066, loss 0.00178273, acc 1, prec 0.0897818, recall 0.796899
2017-12-10T05:31:57.403044: step 5067, loss 0.0402384, acc 0.984375, prec 0.0897804, recall 0.796899
2017-12-10T05:31:57.672271: step 5068, loss 0.0294398, acc 1, prec 0.089823, recall 0.796983
2017-12-10T05:31:57.935707: step 5069, loss 0.00550316, acc 1, prec 0.089823, recall 0.796983
2017-12-10T05:31:58.208627: step 5070, loss 0.071008, acc 0.984375, prec 0.0898358, recall 0.797011
2017-12-10T05:31:58.475469: step 5071, loss 0.130554, acc 0.984375, prec 0.0898486, recall 0.797039
2017-12-10T05:31:58.748549: step 5072, loss 0.00228698, acc 1, prec 0.0898486, recall 0.797039
2017-12-10T05:31:59.014763: step 5073, loss 0.136015, acc 0.96875, prec 0.0898458, recall 0.797039
2017-12-10T05:31:59.281599: step 5074, loss 0.00170583, acc 1, prec 0.08986, recall 0.797067
2017-12-10T05:31:59.543532: step 5075, loss 0.0265935, acc 1, prec 0.0898741, recall 0.797095
2017-12-10T05:31:59.811878: step 5076, loss 0.118945, acc 0.9375, prec 0.0898685, recall 0.797095
2017-12-10T05:32:00.085682: step 5077, loss 0.0249781, acc 0.984375, prec 0.0898671, recall 0.797095
2017-12-10T05:32:00.361294: step 5078, loss 0.0445792, acc 0.984375, prec 0.0898799, recall 0.797123
2017-12-10T05:32:00.635797: step 5079, loss 0.532445, acc 1, prec 0.0898941, recall 0.797152
2017-12-10T05:32:00.903263: step 5080, loss 7.09473, acc 0.984375, prec 0.0899225, recall 0.797097
2017-12-10T05:32:01.174180: step 5081, loss 0.00104309, acc 1, prec 0.0899367, recall 0.797125
2017-12-10T05:32:01.442082: step 5082, loss 0.136838, acc 0.953125, prec 0.0899751, recall 0.79721
2017-12-10T05:32:01.714067: step 5083, loss 0.220383, acc 0.953125, prec 0.0899708, recall 0.79721
2017-12-10T05:32:01.982715: step 5084, loss 0.435675, acc 0.921875, prec 0.0900064, recall 0.797294
2017-12-10T05:32:02.247819: step 5085, loss 0.0691268, acc 0.96875, prec 0.0900036, recall 0.797294
2017-12-10T05:32:02.518777: step 5086, loss 0.131058, acc 0.96875, prec 0.0900008, recall 0.797294
2017-12-10T05:32:02.787381: step 5087, loss 0.0849847, acc 0.96875, prec 0.0900122, recall 0.797322
2017-12-10T05:32:03.057344: step 5088, loss 0.24255, acc 0.96875, prec 0.0900235, recall 0.79735
2017-12-10T05:32:03.320904: step 5089, loss 0.188311, acc 0.96875, prec 0.0900491, recall 0.797405
2017-12-10T05:32:03.585746: step 5090, loss 0.176598, acc 0.9375, prec 0.0900718, recall 0.797461
2017-12-10T05:32:03.858753: step 5091, loss 0.219945, acc 0.9375, prec 0.0900662, recall 0.797461
2017-12-10T05:32:04.122869: step 5092, loss 0.25309, acc 0.9375, prec 0.0900606, recall 0.797461
2017-12-10T05:32:04.383549: step 5093, loss 0.366478, acc 0.921875, prec 0.0900678, recall 0.797489
2017-12-10T05:32:04.650819: step 5094, loss 0.411797, acc 0.90625, prec 0.0900594, recall 0.797489
2017-12-10T05:32:04.931951: step 5095, loss 0.311522, acc 0.90625, prec 0.0900509, recall 0.797489
2017-12-10T05:32:05.198406: step 5096, loss 0.15266, acc 0.96875, prec 0.0900623, recall 0.797517
2017-12-10T05:32:05.461912: step 5097, loss 0.220649, acc 0.9375, prec 0.0900567, recall 0.797517
2017-12-10T05:32:05.729725: step 5098, loss 0.043978, acc 0.984375, prec 0.0900695, recall 0.797545
2017-12-10T05:32:05.996046: step 5099, loss 0.438677, acc 0.953125, prec 0.0900653, recall 0.797545
2017-12-10T05:32:06.260671: step 5100, loss 0.383719, acc 0.921875, prec 0.0900582, recall 0.797545

Evaluation:
2017-12-10T05:32:13.949554: step 5100, loss 3.87316, acc 0.93461, prec 0.0902844, recall 0.79125

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-5100

2017-12-10T05:32:15.128394: step 5101, loss 0.124539, acc 0.9375, prec 0.0902929, recall 0.791279
2017-12-10T05:32:15.395647: step 5102, loss 0.0452259, acc 0.96875, prec 0.0902901, recall 0.791279
2017-12-10T05:32:15.668425: step 5103, loss 0.120612, acc 0.96875, prec 0.0902873, recall 0.791279
2017-12-10T05:32:15.930607: step 5104, loss 0.155904, acc 0.9375, prec 0.0902817, recall 0.791279
2017-12-10T05:32:16.197496: step 5105, loss 0.174729, acc 0.953125, prec 0.0902776, recall 0.791279
2017-12-10T05:32:16.464156: step 5106, loss 0.305454, acc 0.9375, prec 0.090272, recall 0.791279
2017-12-10T05:32:16.743403: step 5107, loss 0.0769085, acc 0.96875, prec 0.0902832, recall 0.791307
2017-12-10T05:32:17.014610: step 5108, loss 0.221284, acc 0.96875, prec 0.0902805, recall 0.791307
2017-12-10T05:32:17.284742: step 5109, loss 0.0278804, acc 0.984375, prec 0.0902791, recall 0.791307
2017-12-10T05:32:17.549436: step 5110, loss 0.0598399, acc 0.984375, prec 0.0902777, recall 0.791307
2017-12-10T05:32:17.814943: step 5111, loss 1.18447, acc 0.96875, prec 0.0902889, recall 0.791335
2017-12-10T05:32:18.083296: step 5112, loss 0.0782356, acc 0.984375, prec 0.0902875, recall 0.791335
2017-12-10T05:32:18.359467: step 5113, loss 0.00962411, acc 1, prec 0.0903015, recall 0.791363
2017-12-10T05:32:18.626400: step 5114, loss 0.0482622, acc 0.96875, prec 0.0902987, recall 0.791363
2017-12-10T05:32:18.897058: step 5115, loss 0.164539, acc 0.984375, prec 0.0903114, recall 0.791391
2017-12-10T05:32:19.167707: step 5116, loss 0.0504564, acc 0.984375, prec 0.090324, recall 0.791419
2017-12-10T05:32:19.439190: step 5117, loss 0.10221, acc 0.96875, prec 0.0903212, recall 0.791419
2017-12-10T05:32:19.708184: step 5118, loss 0.0551445, acc 0.96875, prec 0.0903184, recall 0.791419
2017-12-10T05:32:19.980300: step 5119, loss 0.277996, acc 0.953125, prec 0.0903142, recall 0.791419
2017-12-10T05:32:20.249048: step 5120, loss 0.018484, acc 0.984375, prec 0.0903269, recall 0.791447
2017-12-10T05:32:20.515498: step 5121, loss 0.180228, acc 0.96875, prec 0.0903241, recall 0.791447
2017-12-10T05:32:20.787558: step 5122, loss 0.284712, acc 0.9375, prec 0.0903185, recall 0.791447
2017-12-10T05:32:21.050625: step 5123, loss 0.286342, acc 0.953125, prec 0.0903423, recall 0.791504
2017-12-10T05:32:21.319045: step 5124, loss 0.129345, acc 0.984375, prec 0.090369, recall 0.79156
2017-12-10T05:32:21.585765: step 5125, loss 0.104053, acc 0.96875, prec 0.0903662, recall 0.79156
2017-12-10T05:32:21.852576: step 5126, loss 0.00307046, acc 1, prec 0.0903802, recall 0.791588
2017-12-10T05:32:22.119916: step 5127, loss 0.449311, acc 0.953125, prec 0.090418, recall 0.791672
2017-12-10T05:32:22.398817: step 5128, loss 2.35449, acc 0.921875, prec 0.0904124, recall 0.791566
2017-12-10T05:32:22.668476: step 5129, loss 0.189438, acc 0.96875, prec 0.0904237, recall 0.791594
2017-12-10T05:32:22.937786: step 5130, loss 0.264604, acc 0.953125, prec 0.0904335, recall 0.791622
2017-12-10T05:32:23.203902: step 5131, loss 0.36599, acc 0.921875, prec 0.0904265, recall 0.791622
2017-12-10T05:32:23.469837: step 5132, loss 0.593507, acc 0.859375, prec 0.090428, recall 0.79165
2017-12-10T05:32:23.742514: step 5133, loss 0.352198, acc 0.90625, prec 0.0904336, recall 0.791678
2017-12-10T05:32:24.009485: step 5134, loss 0.196712, acc 0.90625, prec 0.0904393, recall 0.791706
2017-12-10T05:32:24.281895: step 5135, loss 0.31201, acc 0.890625, prec 0.0904435, recall 0.791734
2017-12-10T05:32:24.551226: step 5136, loss 0.228828, acc 0.9375, prec 0.090452, recall 0.791762
2017-12-10T05:32:24.815348: step 5137, loss 0.585348, acc 0.859375, prec 0.0904534, recall 0.79179
2017-12-10T05:32:25.081501: step 5138, loss 0.166978, acc 0.9375, prec 0.0904758, recall 0.791846
2017-12-10T05:32:25.350267: step 5139, loss 0.549079, acc 0.875, prec 0.0904647, recall 0.791846
2017-12-10T05:32:25.613996: step 5140, loss 0.560018, acc 0.875, prec 0.0904676, recall 0.791874
2017-12-10T05:32:25.885518: step 5141, loss 0.292003, acc 0.890625, prec 0.0904718, recall 0.791902
2017-12-10T05:32:26.150008: step 5142, loss 0.556497, acc 0.890625, prec 0.09049, recall 0.791958
2017-12-10T05:32:26.413320: step 5143, loss 0.245026, acc 0.921875, prec 0.090497, recall 0.791986
2017-12-10T05:32:26.684052: step 5144, loss 0.144838, acc 0.921875, prec 0.0904901, recall 0.791986
2017-12-10T05:32:26.952425: step 5145, loss 0.102766, acc 0.96875, prec 0.0904873, recall 0.791986
2017-12-10T05:32:27.224917: step 5146, loss 0.307838, acc 0.984375, prec 0.0905139, recall 0.792042
2017-12-10T05:32:27.503207: step 5147, loss 0.144282, acc 0.9375, prec 0.0905083, recall 0.792042
2017-12-10T05:32:27.775586: step 5148, loss 0.419362, acc 0.890625, prec 0.0905265, recall 0.792098
2017-12-10T05:32:28.052268: step 5149, loss 0.117785, acc 0.953125, prec 0.0905503, recall 0.792154
2017-12-10T05:32:28.325427: step 5150, loss 0.34702, acc 0.921875, prec 0.0905433, recall 0.792154
2017-12-10T05:32:28.598477: step 5151, loss 0.345614, acc 0.90625, prec 0.0905489, recall 0.792182
2017-12-10T05:32:28.876590: step 5152, loss 0.0372188, acc 0.984375, prec 0.0905476, recall 0.792182
2017-12-10T05:32:29.143045: step 5153, loss 0.296867, acc 0.953125, prec 0.0905573, recall 0.79221
2017-12-10T05:32:29.408882: step 5154, loss 1.18955, acc 0.96875, prec 0.0905685, recall 0.792237
2017-12-10T05:32:29.677939: step 5155, loss 0.0302443, acc 0.984375, prec 0.0905671, recall 0.792237
2017-12-10T05:32:29.955139: step 5156, loss 0.135866, acc 0.953125, prec 0.090563, recall 0.792237
2017-12-10T05:32:30.225959: step 5157, loss 0.0863571, acc 0.96875, prec 0.0905881, recall 0.792293
2017-12-10T05:32:30.499748: step 5158, loss 0.0697628, acc 0.96875, prec 0.0906132, recall 0.792349
2017-12-10T05:32:30.771871: step 5159, loss 0.0967198, acc 0.984375, prec 0.0906258, recall 0.792377
2017-12-10T05:32:31.049496: step 5160, loss 0.111412, acc 0.953125, prec 0.0906216, recall 0.792377
2017-12-10T05:32:31.315951: step 5161, loss 0.0554192, acc 0.953125, prec 0.0906314, recall 0.792405
2017-12-10T05:32:31.588582: step 5162, loss 0.0667293, acc 1, prec 0.0906733, recall 0.792488
2017-12-10T05:32:31.854378: step 5163, loss 0.0929572, acc 0.96875, prec 0.0906845, recall 0.792516
2017-12-10T05:32:32.121891: step 5164, loss 0.00686325, acc 1, prec 0.0906845, recall 0.792516
2017-12-10T05:32:32.381453: step 5165, loss 0.301077, acc 0.9375, prec 0.0907068, recall 0.792572
2017-12-10T05:32:32.646585: step 5166, loss 0.0908112, acc 0.953125, prec 0.0907026, recall 0.792572
2017-12-10T05:32:32.915144: step 5167, loss 2.20167, acc 0.9375, prec 0.0907124, recall 0.792493
2017-12-10T05:32:33.185855: step 5168, loss 0.287594, acc 0.921875, prec 0.0907055, recall 0.792493
2017-12-10T05:32:33.454336: step 5169, loss 0.102412, acc 0.9375, prec 0.0907138, recall 0.792521
2017-12-10T05:32:33.724288: step 5170, loss 0.196499, acc 0.9375, prec 0.0907501, recall 0.792605
2017-12-10T05:32:33.988017: step 5171, loss 0.0619245, acc 0.984375, prec 0.0907627, recall 0.792632
2017-12-10T05:32:34.258344: step 5172, loss 1.03021, acc 0.890625, prec 0.0907529, recall 0.792632
2017-12-10T05:32:34.523068: step 5173, loss 0.668767, acc 0.890625, prec 0.0907432, recall 0.792632
2017-12-10T05:32:34.798831: step 5174, loss 0.646962, acc 0.90625, prec 0.0907488, recall 0.79266
2017-12-10T05:32:35.068109: step 5175, loss 0.454965, acc 0.90625, prec 0.0907404, recall 0.79266
2017-12-10T05:32:35.340096: step 5176, loss 0.803381, acc 0.796875, prec 0.0907363, recall 0.792688
2017-12-10T05:32:35.606506: step 5177, loss 0.628505, acc 0.90625, prec 0.0907419, recall 0.792716
2017-12-10T05:32:35.879266: step 5178, loss 0.17347, acc 0.953125, prec 0.0907516, recall 0.792743
2017-12-10T05:32:36.144464: step 5179, loss 0.265154, acc 0.921875, prec 0.0907586, recall 0.792771
2017-12-10T05:32:36.412283: step 5180, loss 0.327077, acc 0.90625, prec 0.0907503, recall 0.792771
2017-12-10T05:32:36.676707: step 5181, loss 0.0495593, acc 0.984375, prec 0.0907628, recall 0.792799
2017-12-10T05:32:36.945007: step 5182, loss 0.614335, acc 0.90625, prec 0.0907545, recall 0.792799
2017-12-10T05:32:37.215069: step 5183, loss 0.343527, acc 0.9375, prec 0.0907489, recall 0.792799
2017-12-10T05:32:37.482388: step 5184, loss 0.573122, acc 0.890625, prec 0.0907531, recall 0.792827
2017-12-10T05:32:37.752439: step 5185, loss 0.91991, acc 0.953125, prec 0.0907629, recall 0.792854
2017-12-10T05:32:38.021542: step 5186, loss 0.138322, acc 0.9375, prec 0.0907712, recall 0.792882
2017-12-10T05:32:38.284245: step 5187, loss 0.533394, acc 0.96875, prec 0.0907685, recall 0.792882
2017-12-10T05:32:38.554081: step 5188, loss 0.0150772, acc 0.984375, prec 0.0907671, recall 0.792882
2017-12-10T05:32:38.813263: step 5189, loss 0.27869, acc 0.96875, prec 0.0907643, recall 0.792882
2017-12-10T05:32:39.077001: step 5190, loss 0.219314, acc 0.96875, prec 0.0907615, recall 0.792882
2017-12-10T05:32:39.352009: step 5191, loss 0.0639405, acc 0.96875, prec 0.0907587, recall 0.792882
2017-12-10T05:32:39.629153: step 5192, loss 0.0207346, acc 0.984375, prec 0.0907573, recall 0.792882
2017-12-10T05:32:39.898732: step 5193, loss 0.417864, acc 0.9375, prec 0.0907657, recall 0.79291
2017-12-10T05:32:40.174667: step 5194, loss 0.229672, acc 0.96875, prec 0.0907908, recall 0.792965
2017-12-10T05:32:40.439115: step 5195, loss 0.339432, acc 0.90625, prec 0.0908381, recall 0.793076
2017-12-10T05:32:40.710698: step 5196, loss 0.239737, acc 0.953125, prec 0.0908339, recall 0.793076
2017-12-10T05:32:40.979182: step 5197, loss 0.0336128, acc 0.984375, prec 0.0908325, recall 0.793076
2017-12-10T05:32:41.248390: step 5198, loss 0.074758, acc 0.96875, prec 0.0908298, recall 0.793076
2017-12-10T05:32:41.515647: step 5199, loss 0.142329, acc 0.96875, prec 0.0908409, recall 0.793103
2017-12-10T05:32:41.781724: step 5200, loss 0.143705, acc 0.984375, prec 0.0908395, recall 0.793103
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-5200

2017-12-10T05:32:43.120794: step 5201, loss 0.0660057, acc 0.96875, prec 0.0908646, recall 0.793159
2017-12-10T05:32:43.389210: step 5202, loss 0.116003, acc 0.96875, prec 0.0908618, recall 0.793159
2017-12-10T05:32:43.656644: step 5203, loss 0.00988141, acc 1, prec 0.0908896, recall 0.793214
2017-12-10T05:32:43.920139: step 5204, loss 0.00132376, acc 1, prec 0.0908896, recall 0.793214
2017-12-10T05:32:44.182616: step 5205, loss 1.55501, acc 0.984375, prec 0.0909035, recall 0.793136
2017-12-10T05:32:44.455875: step 5206, loss 0.63098, acc 0.984375, prec 0.090916, recall 0.793163
2017-12-10T05:32:44.725937: step 5207, loss 0.178422, acc 0.96875, prec 0.0909133, recall 0.793163
2017-12-10T05:32:44.999145: step 5208, loss 0.201215, acc 0.96875, prec 0.0909244, recall 0.793191
2017-12-10T05:32:45.269856: step 5209, loss 0.108049, acc 0.9375, prec 0.0909327, recall 0.793219
2017-12-10T05:32:45.530985: step 5210, loss 0.271432, acc 0.9375, prec 0.0909411, recall 0.793246
2017-12-10T05:32:45.801800: step 5211, loss 0.235394, acc 0.953125, prec 0.0909508, recall 0.793274
2017-12-10T05:32:46.075668: step 5212, loss 0.158482, acc 0.953125, prec 0.0909466, recall 0.793274
2017-12-10T05:32:46.344690: step 5213, loss 0.0197095, acc 1, prec 0.0909884, recall 0.793356
2017-12-10T05:32:46.617230: step 5214, loss 0.0463735, acc 0.984375, prec 0.0910009, recall 0.793384
2017-12-10T05:32:46.883390: step 5215, loss 0.14265, acc 0.96875, prec 0.091012, recall 0.793412
2017-12-10T05:32:47.160638: step 5216, loss 0.123172, acc 0.953125, prec 0.0910078, recall 0.793412
2017-12-10T05:32:47.426617: step 5217, loss 0.0653319, acc 0.984375, prec 0.0910203, recall 0.793439
2017-12-10T05:32:47.691248: step 5218, loss 0.18347, acc 0.9375, prec 0.0910287, recall 0.793467
2017-12-10T05:32:47.955408: step 5219, loss 0.112717, acc 0.96875, prec 0.0910259, recall 0.793467
2017-12-10T05:32:48.220247: step 5220, loss 0.707077, acc 0.875, prec 0.0910148, recall 0.793467
2017-12-10T05:32:48.480483: step 5221, loss 0.212937, acc 0.953125, prec 0.0910245, recall 0.793494
2017-12-10T05:32:48.747405: step 5222, loss 0.495487, acc 0.953125, prec 0.0910203, recall 0.793494
2017-12-10T05:32:49.016588: step 5223, loss 0.569782, acc 0.90625, prec 0.0910259, recall 0.793522
2017-12-10T05:32:49.278848: step 5224, loss 0.118293, acc 0.96875, prec 0.091037, recall 0.793549
2017-12-10T05:32:49.545707: step 5225, loss 0.300622, acc 0.953125, prec 0.0910606, recall 0.793604
2017-12-10T05:32:50.519661: step 5226, loss 0.222937, acc 0.953125, prec 0.0910564, recall 0.793604
2017-12-10T05:32:50.874726: step 5227, loss 0.415119, acc 0.921875, prec 0.0910495, recall 0.793604
2017-12-10T05:32:51.545248: step 5228, loss 0.129261, acc 0.953125, prec 0.0910453, recall 0.793604
2017-12-10T05:32:52.259021: step 5229, loss 0.526646, acc 0.9375, prec 0.0910536, recall 0.793632
2017-12-10T05:32:52.990808: step 5230, loss 0.317113, acc 0.953125, prec 0.0910494, recall 0.793632
2017-12-10T05:32:53.756899: step 5231, loss 0.0752017, acc 0.984375, prec 0.091048, recall 0.793632
2017-12-10T05:32:54.481528: step 5232, loss 2.30341, acc 0.9375, prec 0.0910439, recall 0.793526
2017-12-10T05:32:55.237987: step 5233, loss 0.194166, acc 0.96875, prec 0.0910689, recall 0.793581
2017-12-10T05:32:55.965330: step 5234, loss 0.121097, acc 0.953125, prec 0.0910647, recall 0.793581
2017-12-10T05:32:56.693664: step 5235, loss 0.0686031, acc 0.96875, prec 0.0910758, recall 0.793609
2017-12-10T05:32:57.436014: step 5236, loss 0.475942, acc 0.90625, prec 0.0910674, recall 0.793609
2017-12-10T05:32:58.136553: step 5237, loss 0.0880794, acc 0.984375, prec 0.0910938, recall 0.793663
2017-12-10T05:32:58.857213: step 5238, loss 0.264123, acc 0.921875, prec 0.0911008, recall 0.793691
2017-12-10T05:32:59.584315: step 5239, loss 0.521003, acc 0.921875, prec 0.0910938, recall 0.793691
2017-12-10T05:33:00.283042: step 5240, loss 0.11126, acc 0.953125, prec 0.0911452, recall 0.793801
2017-12-10T05:33:01.035390: step 5241, loss 0.241881, acc 0.9375, prec 0.0911396, recall 0.793801
2017-12-10T05:33:01.752826: step 5242, loss 0.306562, acc 0.90625, prec 0.0911451, recall 0.793828
2017-12-10T05:33:02.484652: step 5243, loss 0.22561, acc 0.953125, prec 0.0911826, recall 0.79391
2017-12-10T05:33:03.515154: step 5244, loss 0.32137, acc 0.921875, prec 0.0912034, recall 0.793965
2017-12-10T05:33:03.961275: step 5245, loss 0.259971, acc 0.953125, prec 0.0912131, recall 0.793993
2017-12-10T05:33:04.249839: step 5246, loss 0.459789, acc 0.90625, prec 0.0912186, recall 0.79402
2017-12-10T05:33:04.533599: step 5247, loss 0.368114, acc 0.96875, prec 0.0912436, recall 0.794075
2017-12-10T05:33:04.817806: step 5248, loss 0.0747649, acc 0.953125, prec 0.0912394, recall 0.794075
2017-12-10T05:33:05.102516: step 5249, loss 0.447788, acc 0.921875, prec 0.0912602, recall 0.794129
2017-12-10T05:33:05.387527: step 5250, loss 0.0620594, acc 0.953125, prec 0.0912698, recall 0.794157
2017-12-10T05:33:05.656013: step 5251, loss 0.358953, acc 0.9375, prec 0.0912643, recall 0.794157
2017-12-10T05:33:05.927391: step 5252, loss 0.191387, acc 0.984375, prec 0.0912629, recall 0.794157
2017-12-10T05:33:06.192842: step 5253, loss 0.188656, acc 0.96875, prec 0.0912878, recall 0.794211
2017-12-10T05:33:06.464337: step 5254, loss 0.0294842, acc 0.984375, prec 0.0912864, recall 0.794211
2017-12-10T05:33:06.739603: step 5255, loss 0.000936701, acc 1, prec 0.0912864, recall 0.794211
2017-12-10T05:33:07.017174: step 5256, loss 0.19023, acc 0.921875, prec 0.0913072, recall 0.794266
2017-12-10T05:33:07.288477: step 5257, loss 0.164917, acc 0.984375, prec 0.0913335, recall 0.794321
2017-12-10T05:33:07.553546: step 5258, loss 0.360162, acc 0.953125, prec 0.0913432, recall 0.794348
2017-12-10T05:33:07.821696: step 5259, loss 0.180568, acc 0.96875, prec 0.0913682, recall 0.794402
2017-12-10T05:33:08.089374: step 5260, loss 0.947406, acc 0.96875, prec 0.091407, recall 0.794484
2017-12-10T05:33:08.357241: step 5261, loss 0.315203, acc 0.9375, prec 0.0914014, recall 0.794484
2017-12-10T05:33:08.628596: step 5262, loss 0.353147, acc 0.953125, prec 0.0913972, recall 0.794484
2017-12-10T05:33:08.895679: step 5263, loss 0.220942, acc 0.96875, prec 0.0913944, recall 0.794484
2017-12-10T05:33:09.175043: step 5264, loss 0.0341556, acc 0.984375, prec 0.091393, recall 0.794484
2017-12-10T05:33:09.456174: step 5265, loss 0.0496133, acc 0.984375, prec 0.0914055, recall 0.794511
2017-12-10T05:33:09.722224: step 5266, loss 0.289498, acc 0.9375, prec 0.0914138, recall 0.794539
2017-12-10T05:33:09.989797: step 5267, loss 0.201092, acc 0.921875, prec 0.0914345, recall 0.794593
2017-12-10T05:33:10.260673: step 5268, loss 0.049748, acc 0.96875, prec 0.0914317, recall 0.794593
2017-12-10T05:33:10.534030: step 5269, loss 0.0314124, acc 0.984375, prec 0.0914303, recall 0.794593
2017-12-10T05:33:10.804993: step 5270, loss 0.214667, acc 0.96875, prec 0.0914414, recall 0.79462
2017-12-10T05:33:11.075433: step 5271, loss 0.019477, acc 0.984375, prec 0.09144, recall 0.79462
2017-12-10T05:33:11.341708: step 5272, loss 0.20105, acc 0.9375, prec 0.0914483, recall 0.794648
2017-12-10T05:33:11.615882: step 5273, loss 0.040493, acc 0.984375, prec 0.0914607, recall 0.794675
2017-12-10T05:33:11.890350: step 5274, loss 0.00651585, acc 1, prec 0.0914607, recall 0.794675
2017-12-10T05:33:12.164767: step 5275, loss 0.0391409, acc 0.96875, prec 0.0914579, recall 0.794675
2017-12-10T05:33:12.438874: step 5276, loss 5.47238, acc 0.890625, prec 0.0914496, recall 0.79457
2017-12-10T05:33:12.707318: step 5277, loss 0.0444445, acc 0.984375, prec 0.0914482, recall 0.79457
2017-12-10T05:33:12.979652: step 5278, loss 0.0765569, acc 0.96875, prec 0.0914454, recall 0.79457
2017-12-10T05:33:13.242624: step 5279, loss 0.237613, acc 0.9375, prec 0.0914398, recall 0.79457
2017-12-10T05:33:13.508880: step 5280, loss 0.293021, acc 0.9375, prec 0.0914481, recall 0.794597
2017-12-10T05:33:13.777236: step 5281, loss 0.453048, acc 0.90625, prec 0.0914536, recall 0.794624
2017-12-10T05:33:14.043180: step 5282, loss 0.0949322, acc 0.96875, prec 0.0914508, recall 0.794624
2017-12-10T05:33:14.312826: step 5283, loss 0.582596, acc 0.90625, prec 0.0914424, recall 0.794624
2017-12-10T05:33:14.582724: step 5284, loss 0.420892, acc 0.890625, prec 0.0914603, recall 0.794678
2017-12-10T05:33:14.850618: step 5285, loss 0.167409, acc 0.96875, prec 0.0914576, recall 0.794678
2017-12-10T05:33:15.116317: step 5286, loss 0.330567, acc 0.890625, prec 0.0914617, recall 0.794706
2017-12-10T05:33:15.391072: step 5287, loss 0.246917, acc 0.9375, prec 0.0914561, recall 0.794706
2017-12-10T05:33:15.667743: step 5288, loss 0.370181, acc 0.921875, prec 0.0914906, recall 0.794787
2017-12-10T05:33:15.943047: step 5289, loss 0.57403, acc 0.921875, prec 0.0914975, recall 0.794814
2017-12-10T05:33:16.208036: step 5290, loss 0.385284, acc 0.890625, prec 0.0914877, recall 0.794814
2017-12-10T05:33:16.477704: step 5291, loss 0.0295951, acc 1, prec 0.0914877, recall 0.794814
2017-12-10T05:33:16.743792: step 5292, loss 0.439978, acc 0.90625, prec 0.0914932, recall 0.794841
2017-12-10T05:33:17.018118: step 5293, loss 0.9109, acc 0.875, prec 0.0915097, recall 0.794896
2017-12-10T05:33:17.288035: step 5294, loss 0.364113, acc 0.921875, prec 0.0915028, recall 0.794896
2017-12-10T05:33:17.557887: step 5295, loss 0.263913, acc 0.921875, prec 0.0915096, recall 0.794923
2017-12-10T05:33:17.833346: step 5296, loss 0.611792, acc 0.90625, prec 0.0915013, recall 0.794923
2017-12-10T05:33:18.103218: step 5297, loss 0.158701, acc 0.96875, prec 0.0915123, recall 0.79495
2017-12-10T05:33:18.372310: step 5298, loss 0.165649, acc 0.984375, prec 0.0915247, recall 0.794977
2017-12-10T05:33:18.648815: step 5299, loss 0.452999, acc 0.96875, prec 0.0915634, recall 0.795058
2017-12-10T05:33:18.920395: step 5300, loss 0.107462, acc 0.953125, prec 0.0915593, recall 0.795058
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-5300

2017-12-10T05:33:20.258984: step 5301, loss 0.108463, acc 0.96875, prec 0.0915565, recall 0.795058
2017-12-10T05:33:20.533085: step 5302, loss 4.78215, acc 0.96875, prec 0.0915689, recall 0.79498
2017-12-10T05:33:20.809039: step 5303, loss 0.532947, acc 0.96875, prec 0.0916076, recall 0.795061
2017-12-10T05:33:21.087293: step 5304, loss 0.10655, acc 0.96875, prec 0.0916324, recall 0.795116
2017-12-10T05:33:21.359802: step 5305, loss 0.373767, acc 0.890625, prec 0.0916365, recall 0.795143
2017-12-10T05:33:21.626532: step 5306, loss 0.347105, acc 0.921875, prec 0.0916295, recall 0.795143
2017-12-10T05:33:21.901496: step 5307, loss 0.621512, acc 0.875, prec 0.0916184, recall 0.795143
2017-12-10T05:33:22.170632: step 5308, loss 0.0604883, acc 0.96875, prec 0.091657, recall 0.795224
2017-12-10T05:33:22.446633: step 5309, loss 0.377543, acc 0.96875, prec 0.0916957, recall 0.795305
2017-12-10T05:33:22.716513: step 5310, loss 0.970795, acc 0.890625, prec 0.0916859, recall 0.795305
2017-12-10T05:33:22.984663: step 5311, loss 0.60816, acc 0.890625, prec 0.09169, recall 0.795332
2017-12-10T05:33:23.249418: step 5312, loss 0.782034, acc 0.890625, prec 0.0917216, recall 0.795413
2017-12-10T05:33:23.515353: step 5313, loss 0.635735, acc 0.90625, prec 0.0917409, recall 0.795467
2017-12-10T05:33:23.787204: step 5314, loss 0.639369, acc 0.921875, prec 0.0917477, recall 0.795493
2017-12-10T05:33:24.057419: step 5315, loss 0.131791, acc 0.96875, prec 0.0917449, recall 0.795493
2017-12-10T05:33:24.333888: step 5316, loss 0.312959, acc 0.921875, prec 0.091738, recall 0.795493
2017-12-10T05:33:24.604778: step 5317, loss 0.0289677, acc 0.984375, prec 0.0917642, recall 0.795547
2017-12-10T05:33:24.870562: step 5318, loss 0.274609, acc 0.90625, prec 0.0917696, recall 0.795574
2017-12-10T05:33:25.132048: step 5319, loss 0.161856, acc 0.921875, prec 0.0917764, recall 0.795601
2017-12-10T05:33:25.396254: step 5320, loss 0.310467, acc 0.953125, prec 0.0917998, recall 0.795655
2017-12-10T05:33:25.664632: step 5321, loss 0.159815, acc 0.9375, prec 0.0917943, recall 0.795655
2017-12-10T05:33:25.931986: step 5322, loss 0.440263, acc 0.921875, prec 0.0917873, recall 0.795655
2017-12-10T05:33:26.205478: step 5323, loss 0.0895588, acc 0.9375, prec 0.0917955, recall 0.795682
2017-12-10T05:33:26.485389: step 5324, loss 0.03015, acc 0.984375, prec 0.0918079, recall 0.795709
2017-12-10T05:33:26.752401: step 5325, loss 0.136221, acc 0.953125, prec 0.0918313, recall 0.795763
2017-12-10T05:33:27.022152: step 5326, loss 0.591445, acc 0.9375, prec 0.0918533, recall 0.795816
2017-12-10T05:33:27.297855: step 5327, loss 0.407534, acc 0.96875, prec 0.0918781, recall 0.79587
2017-12-10T05:33:27.567128: step 5328, loss 0.169403, acc 0.953125, prec 0.0918877, recall 0.795897
2017-12-10T05:33:27.840344: step 5329, loss 0.334362, acc 0.96875, prec 0.0919125, recall 0.795951
2017-12-10T05:33:28.110952: step 5330, loss 0.101995, acc 0.984375, prec 0.0919111, recall 0.795951
2017-12-10T05:33:28.375895: step 5331, loss 0.237474, acc 0.96875, prec 0.0919083, recall 0.795951
2017-12-10T05:33:28.640101: step 5332, loss 0.298784, acc 0.9375, prec 0.0919303, recall 0.796004
2017-12-10T05:33:28.903571: step 5333, loss 0.0627127, acc 0.984375, prec 0.0919565, recall 0.796058
2017-12-10T05:33:29.168848: step 5334, loss 1.33639, acc 0.96875, prec 0.0919826, recall 0.796007
2017-12-10T05:33:29.440411: step 5335, loss 0.597602, acc 0.9375, prec 0.0919771, recall 0.796007
2017-12-10T05:33:29.713606: step 5336, loss 0.208765, acc 0.96875, prec 0.091988, recall 0.796034
2017-12-10T05:33:29.980373: step 5337, loss 0.120052, acc 0.953125, prec 0.0920114, recall 0.796087
2017-12-10T05:33:30.247685: step 5338, loss 0.081747, acc 0.96875, prec 0.0920224, recall 0.796114
2017-12-10T05:33:30.516231: step 5339, loss 0.157277, acc 0.953125, prec 0.092032, recall 0.796141
2017-12-10T05:33:30.787041: step 5340, loss 0.0673573, acc 0.984375, prec 0.0920306, recall 0.796141
2017-12-10T05:33:31.063112: step 5341, loss 0.392308, acc 0.953125, prec 0.0920402, recall 0.796167
2017-12-10T05:33:31.332708: step 5342, loss 0.222672, acc 0.953125, prec 0.0920773, recall 0.796248
2017-12-10T05:33:31.599605: step 5343, loss 0.275795, acc 0.953125, prec 0.0920731, recall 0.796248
2017-12-10T05:33:31.872491: step 5344, loss 0.191652, acc 0.921875, prec 0.0920799, recall 0.796274
2017-12-10T05:33:32.145144: step 5345, loss 0.163584, acc 0.96875, prec 0.0920771, recall 0.796274
2017-12-10T05:33:32.421417: step 5346, loss 0.112312, acc 0.96875, prec 0.0920743, recall 0.796274
2017-12-10T05:33:32.689992: step 5347, loss 0.468884, acc 0.953125, prec 0.0920701, recall 0.796274
2017-12-10T05:33:32.967892: step 5348, loss 0.10927, acc 0.984375, prec 0.0920687, recall 0.796274
2017-12-10T05:33:33.234862: step 5349, loss 0.190284, acc 0.90625, prec 0.0920604, recall 0.796274
2017-12-10T05:33:33.499945: step 5350, loss 0.213407, acc 0.9375, prec 0.0920548, recall 0.796274
2017-12-10T05:33:33.765206: step 5351, loss 0.0640654, acc 0.96875, prec 0.0920658, recall 0.796301
2017-12-10T05:33:34.029724: step 5352, loss 1.13498, acc 0.890625, prec 0.0920698, recall 0.796328
2017-12-10T05:33:34.305521: step 5353, loss 0.492467, acc 0.9375, prec 0.0920642, recall 0.796328
2017-12-10T05:33:34.569833: step 5354, loss 0.268243, acc 0.984375, prec 0.0920628, recall 0.796328
2017-12-10T05:33:34.832970: step 5355, loss 0.229289, acc 0.96875, prec 0.09206, recall 0.796328
2017-12-10T05:33:35.105635: step 5356, loss 0.10017, acc 0.96875, prec 0.0920572, recall 0.796328
2017-12-10T05:33:35.371344: step 5357, loss 0.272167, acc 0.921875, prec 0.0920502, recall 0.796328
2017-12-10T05:33:35.634890: step 5358, loss 0.176886, acc 0.96875, prec 0.0920749, recall 0.796381
2017-12-10T05:33:35.902358: step 5359, loss 0.211319, acc 0.921875, prec 0.092068, recall 0.796381
2017-12-10T05:33:36.169171: step 5360, loss 0.237961, acc 0.9375, prec 0.0920624, recall 0.796381
2017-12-10T05:33:36.444324: step 5361, loss 0.191592, acc 0.953125, prec 0.0920582, recall 0.796381
2017-12-10T05:33:36.716579: step 5362, loss 0.183943, acc 0.96875, prec 0.0920692, recall 0.796408
2017-12-10T05:33:36.981811: step 5363, loss 0.183668, acc 0.953125, prec 0.0920925, recall 0.796461
2017-12-10T05:33:37.248803: step 5364, loss 0.23179, acc 0.9375, prec 0.0921007, recall 0.796488
2017-12-10T05:33:37.510682: step 5365, loss 0.00819029, acc 1, prec 0.0921007, recall 0.796488
2017-12-10T05:33:37.775254: step 5366, loss 0.288461, acc 0.96875, prec 0.0920979, recall 0.796488
2017-12-10T05:33:38.049256: step 5367, loss 0.0367077, acc 1, prec 0.0921254, recall 0.796541
2017-12-10T05:33:38.316732: step 5368, loss 0.14117, acc 0.953125, prec 0.0921212, recall 0.796541
2017-12-10T05:33:38.587198: step 5369, loss 0.0753649, acc 0.96875, prec 0.0921184, recall 0.796541
2017-12-10T05:33:38.852273: step 5370, loss 0.26307, acc 0.96875, prec 0.0921156, recall 0.796541
2017-12-10T05:33:39.121513: step 5371, loss 0.0473158, acc 0.984375, prec 0.0921142, recall 0.796541
2017-12-10T05:33:39.390032: step 5372, loss 0.118583, acc 0.953125, prec 0.09211, recall 0.796541
2017-12-10T05:33:39.659289: step 5373, loss 0.00266738, acc 1, prec 0.09211, recall 0.796541
2017-12-10T05:33:39.927567: step 5374, loss 0.0945167, acc 1, prec 0.0921376, recall 0.796595
2017-12-10T05:33:40.197614: step 5375, loss 0.248777, acc 0.984375, prec 0.0921499, recall 0.796621
2017-12-10T05:33:40.459344: step 5376, loss 0.301389, acc 0.984375, prec 0.0921623, recall 0.796648
2017-12-10T05:33:40.732702: step 5377, loss 0.0504459, acc 0.984375, prec 0.0921609, recall 0.796648
2017-12-10T05:33:41.000265: step 5378, loss 0.00842829, acc 1, prec 0.0921609, recall 0.796648
2017-12-10T05:33:41.276443: step 5379, loss 0.138009, acc 0.96875, prec 0.0921718, recall 0.796675
2017-12-10T05:33:41.554615: step 5380, loss 0.0112353, acc 1, prec 0.0922131, recall 0.796754
2017-12-10T05:33:41.821103: step 5381, loss 9.01457e-05, acc 1, prec 0.0922131, recall 0.796754
2017-12-10T05:33:42.097724: step 5382, loss 0.0812452, acc 0.984375, prec 0.0922254, recall 0.796781
2017-12-10T05:33:42.373332: step 5383, loss 1.53669, acc 0.984375, prec 0.0922392, recall 0.796703
2017-12-10T05:33:42.644305: step 5384, loss 0.176781, acc 0.96875, prec 0.0922501, recall 0.79673
2017-12-10T05:33:42.910721: step 5385, loss 0.042463, acc 0.984375, prec 0.0922487, recall 0.79673
2017-12-10T05:33:43.186512: step 5386, loss 0.0223342, acc 1, prec 0.0922487, recall 0.79673
2017-12-10T05:33:43.449289: step 5387, loss 0.106444, acc 0.953125, prec 0.0922446, recall 0.79673
2017-12-10T05:33:43.717144: step 5388, loss 0.0788686, acc 0.984375, prec 0.0922569, recall 0.796756
2017-12-10T05:33:43.986897: step 5389, loss 0.582538, acc 0.9375, prec 0.0922513, recall 0.796756
2017-12-10T05:33:44.256662: step 5390, loss 0.355733, acc 0.953125, prec 0.0922609, recall 0.796783
2017-12-10T05:33:44.524245: step 5391, loss 0.11221, acc 0.984375, prec 0.0922732, recall 0.79681
2017-12-10T05:33:44.786756: step 5392, loss 0.16104, acc 0.953125, prec 0.092269, recall 0.79681
2017-12-10T05:33:45.060367: step 5393, loss 0.0586804, acc 0.984375, prec 0.0922814, recall 0.796836
2017-12-10T05:33:45.331807: step 5394, loss 0.0209167, acc 1, prec 0.0922951, recall 0.796863
2017-12-10T05:33:45.595593: step 5395, loss 0.188756, acc 0.96875, prec 0.0923061, recall 0.796889
2017-12-10T05:33:45.866976: step 5396, loss 0.181832, acc 0.921875, prec 0.0922991, recall 0.796889
2017-12-10T05:33:46.134461: step 5397, loss 0.212828, acc 0.9375, prec 0.0923072, recall 0.796916
2017-12-10T05:33:46.405037: step 5398, loss 0.147547, acc 0.953125, prec 0.092303, recall 0.796916
2017-12-10T05:33:46.684369: step 5399, loss 0.477064, acc 0.921875, prec 0.0923235, recall 0.796969
2017-12-10T05:33:46.948002: step 5400, loss 0.0710724, acc 0.96875, prec 0.0923207, recall 0.796969

Evaluation:
2017-12-10T05:33:54.510271: step 5400, loss 4.59087, acc 0.941687, prec 0.0925601, recall 0.79037

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-5400

2017-12-10T05:33:55.767681: step 5401, loss 0.234068, acc 0.9375, prec 0.0925682, recall 0.790397
2017-12-10T05:33:56.035424: step 5402, loss 0.186251, acc 0.9375, prec 0.0925762, recall 0.790424
2017-12-10T05:33:56.299571: step 5403, loss 1.54952, acc 0.96875, prec 0.0925748, recall 0.790323
2017-12-10T05:33:56.570895: step 5404, loss 0.0689486, acc 0.96875, prec 0.0926265, recall 0.79043
2017-12-10T05:33:56.837796: step 5405, loss 0.275306, acc 0.9375, prec 0.0926209, recall 0.79043
2017-12-10T05:33:57.109995: step 5406, loss 0.222263, acc 0.953125, prec 0.0926167, recall 0.79043
2017-12-10T05:33:57.385381: step 5407, loss 0.0630144, acc 0.96875, prec 0.092614, recall 0.79043
2017-12-10T05:33:57.648784: step 5408, loss 0.321238, acc 0.921875, prec 0.0926478, recall 0.79051
2017-12-10T05:33:57.917526: step 5409, loss 1.07967, acc 0.8125, prec 0.0926312, recall 0.79051
2017-12-10T05:33:58.185791: step 5410, loss 0.267457, acc 0.921875, prec 0.0926378, recall 0.790537
2017-12-10T05:33:58.449236: step 5411, loss 0.499513, acc 0.921875, prec 0.0926581, recall 0.790591
2017-12-10T05:33:58.712004: step 5412, loss 0.509008, acc 0.890625, prec 0.0926484, recall 0.790591
2017-12-10T05:33:58.980943: step 5413, loss 0.516926, acc 0.90625, prec 0.0926672, recall 0.790644
2017-12-10T05:33:59.246291: step 5414, loss 0.864342, acc 0.875, prec 0.0926697, recall 0.790671
2017-12-10T05:33:59.513238: step 5415, loss 1.0388, acc 0.90625, prec 0.0926614, recall 0.790671
2017-12-10T05:33:59.787373: step 5416, loss 0.431663, acc 0.921875, prec 0.092668, recall 0.790698
2017-12-10T05:34:00.051136: step 5417, loss 0.245345, acc 0.953125, prec 0.0926775, recall 0.790724
2017-12-10T05:34:00.328478: step 5418, loss 0.159894, acc 0.96875, prec 0.0926747, recall 0.790724
2017-12-10T05:34:00.596631: step 5419, loss 0.294552, acc 0.9375, prec 0.0926963, recall 0.790778
2017-12-10T05:34:00.867696: step 5420, loss 0.15194, acc 0.953125, prec 0.0926921, recall 0.790778
2017-12-10T05:34:01.145444: step 5421, loss 0.223245, acc 0.9375, prec 0.0927273, recall 0.790858
2017-12-10T05:34:02.123059: step 5422, loss 0.106957, acc 0.953125, prec 0.0927367, recall 0.790885
2017-12-10T05:34:02.484291: step 5423, loss 0.108412, acc 0.984375, prec 0.0927625, recall 0.790938
2017-12-10T05:34:02.752025: step 5424, loss 0.0739877, acc 0.96875, prec 0.0928005, recall 0.791018
2017-12-10T05:34:03.430228: step 5425, loss 0.119075, acc 0.953125, prec 0.0927963, recall 0.791018
2017-12-10T05:34:04.235396: step 5426, loss 0.0563425, acc 0.984375, prec 0.0928085, recall 0.791045
2017-12-10T05:34:04.542458: step 5427, loss 0.496687, acc 0.96875, prec 0.0928057, recall 0.791045
2017-12-10T05:34:04.815684: step 5428, loss 0.0831475, acc 1, prec 0.0928465, recall 0.791125
2017-12-10T05:34:05.092760: step 5429, loss 0.00537662, acc 1, prec 0.0928465, recall 0.791125
2017-12-10T05:34:05.363169: step 5430, loss 2.07549, acc 0.9375, prec 0.0928423, recall 0.791024
2017-12-10T05:34:05.634506: step 5431, loss 0.22386, acc 0.953125, prec 0.0928517, recall 0.79105
2017-12-10T05:34:05.898128: step 5432, loss 0.102361, acc 0.953125, prec 0.0928611, recall 0.791077
2017-12-10T05:34:06.162962: step 5433, loss 0.00433109, acc 1, prec 0.0928747, recall 0.791104
2017-12-10T05:34:06.427793: step 5434, loss 0.00862341, acc 1, prec 0.0928882, recall 0.79113
2017-12-10T05:34:06.699522: step 5435, loss 0.281284, acc 0.953125, prec 0.0928841, recall 0.79113
2017-12-10T05:34:06.966352: step 5436, loss 0.158685, acc 0.96875, prec 0.0929084, recall 0.791184
2017-12-10T05:34:07.231253: step 5437, loss 0.140436, acc 0.953125, prec 0.0929314, recall 0.791237
2017-12-10T05:34:07.502006: step 5438, loss 0.471869, acc 0.921875, prec 0.092938, recall 0.791263
2017-12-10T05:34:07.769537: step 5439, loss 0.339161, acc 0.9375, prec 0.0929325, recall 0.791263
2017-12-10T05:34:08.040570: step 5440, loss 0.85725, acc 0.9375, prec 0.0929405, recall 0.79129
2017-12-10T05:34:08.309426: step 5441, loss 0.194893, acc 0.953125, prec 0.0929634, recall 0.791343
2017-12-10T05:34:08.576276: step 5442, loss 0.236341, acc 0.96875, prec 0.0929607, recall 0.791343
2017-12-10T05:34:08.842073: step 5443, loss 0.132599, acc 0.984375, prec 0.0929593, recall 0.791343
2017-12-10T05:34:09.106570: step 5444, loss 0.326327, acc 0.96875, prec 0.09297, recall 0.79137
2017-12-10T05:34:09.375434: step 5445, loss 0.355499, acc 0.921875, prec 0.0929631, recall 0.79137
2017-12-10T05:34:09.646810: step 5446, loss 0.113991, acc 0.96875, prec 0.0929739, recall 0.791396
2017-12-10T05:34:09.911200: step 5447, loss 0.16911, acc 0.9375, prec 0.0929683, recall 0.791396
2017-12-10T05:34:10.180793: step 5448, loss 0.311237, acc 0.953125, prec 0.0929913, recall 0.791449
2017-12-10T05:34:10.456822: step 5449, loss 0.145111, acc 0.96875, prec 0.0930156, recall 0.791502
2017-12-10T05:34:10.723525: step 5450, loss 0.547601, acc 0.859375, prec 0.0930031, recall 0.791502
2017-12-10T05:34:10.991357: step 5451, loss 0.108696, acc 0.984375, prec 0.0930153, recall 0.791529
2017-12-10T05:34:11.255889: step 5452, loss 0.0986782, acc 0.953125, prec 0.0930111, recall 0.791529
2017-12-10T05:34:11.520719: step 5453, loss 0.257, acc 0.890625, prec 0.0930014, recall 0.791529
2017-12-10T05:34:11.789714: step 5454, loss 0.360132, acc 0.921875, prec 0.093008, recall 0.791555
2017-12-10T05:34:12.056623: step 5455, loss 0.229643, acc 0.921875, prec 0.0930281, recall 0.791608
2017-12-10T05:34:12.332268: step 5456, loss 0.16258, acc 0.96875, prec 0.0930524, recall 0.791661
2017-12-10T05:34:12.601116: step 5457, loss 0.362647, acc 0.9375, prec 0.093074, recall 0.791714
2017-12-10T05:34:12.877944: step 5458, loss 0.230431, acc 0.953125, prec 0.0930834, recall 0.791741
2017-12-10T05:34:13.147061: step 5459, loss 0.0260227, acc 1, prec 0.0931375, recall 0.791847
2017-12-10T05:34:13.415191: step 5460, loss 0.355815, acc 0.96875, prec 0.0931619, recall 0.791899
2017-12-10T05:34:13.682419: step 5461, loss 0.314653, acc 0.984375, prec 0.0931605, recall 0.791899
2017-12-10T05:34:13.955508: step 5462, loss 0.319117, acc 0.953125, prec 0.0931563, recall 0.791899
2017-12-10T05:34:14.223792: step 5463, loss 0.0797864, acc 0.96875, prec 0.0931806, recall 0.791952
2017-12-10T05:34:14.489762: step 5464, loss 0.00409251, acc 1, prec 0.0932077, recall 0.792005
2017-12-10T05:34:14.764263: step 5465, loss 0.750726, acc 0.984375, prec 0.0932469, recall 0.792084
2017-12-10T05:34:15.035708: step 5466, loss 0.215439, acc 0.953125, prec 0.0932427, recall 0.792084
2017-12-10T05:34:15.277383: step 5467, loss 0.0370867, acc 0.980769, prec 0.0932413, recall 0.792084
2017-12-10T05:34:15.549772: step 5468, loss 0.00698361, acc 1, prec 0.0932549, recall 0.792111
2017-12-10T05:34:15.824301: step 5469, loss 0.040494, acc 0.984375, prec 0.0932535, recall 0.792111
2017-12-10T05:34:16.094555: step 5470, loss 0.0978419, acc 0.96875, prec 0.0932778, recall 0.792163
2017-12-10T05:34:16.375989: step 5471, loss 0.0239832, acc 0.984375, prec 0.0932899, recall 0.79219
2017-12-10T05:34:16.643504: step 5472, loss 0.215523, acc 0.953125, prec 0.0932858, recall 0.79219
2017-12-10T05:34:16.911821: step 5473, loss 0.416578, acc 0.96875, prec 0.093283, recall 0.79219
2017-12-10T05:34:17.187719: step 5474, loss 0.0568683, acc 0.984375, prec 0.0933086, recall 0.792242
2017-12-10T05:34:17.462623: step 5475, loss 0.103079, acc 0.953125, prec 0.093318, recall 0.792269
2017-12-10T05:34:17.726925: step 5476, loss 0.153691, acc 0.96875, prec 0.0933288, recall 0.792295
2017-12-10T05:34:17.991429: step 5477, loss 0.0419378, acc 0.984375, prec 0.0933409, recall 0.792321
2017-12-10T05:34:18.258684: step 5478, loss 0.0248969, acc 0.984375, prec 0.0933395, recall 0.792321
2017-12-10T05:34:18.524396: step 5479, loss 0.0654267, acc 0.984375, prec 0.0933381, recall 0.792321
2017-12-10T05:34:18.792718: step 5480, loss 0.167009, acc 0.984375, prec 0.0933502, recall 0.792348
2017-12-10T05:34:19.059506: step 5481, loss 0.166483, acc 0.953125, prec 0.0933596, recall 0.792374
2017-12-10T05:34:19.332974: step 5482, loss 0.232711, acc 0.953125, prec 0.0933554, recall 0.792374
2017-12-10T05:34:19.601056: step 5483, loss 0.0759305, acc 0.984375, prec 0.0933676, recall 0.7924
2017-12-10T05:34:19.867064: step 5484, loss 0.454828, acc 0.96875, prec 0.0933783, recall 0.792427
2017-12-10T05:34:20.129461: step 5485, loss 0.00830979, acc 1, prec 0.0933918, recall 0.792453
2017-12-10T05:34:20.392542: step 5486, loss 4.92593, acc 0.984375, prec 0.0933918, recall 0.792352
2017-12-10T05:34:20.663423: step 5487, loss 0.0554863, acc 0.984375, prec 0.093404, recall 0.792379
2017-12-10T05:34:20.931770: step 5488, loss 0.230898, acc 0.96875, prec 0.0934012, recall 0.792379
2017-12-10T05:34:21.208283: step 5489, loss 0.332386, acc 0.921875, prec 0.0934213, recall 0.792431
2017-12-10T05:34:21.472473: step 5490, loss 0.0888983, acc 0.953125, prec 0.0934441, recall 0.792484
2017-12-10T05:34:21.739000: step 5491, loss 0.118756, acc 0.96875, prec 0.0934684, recall 0.792536
2017-12-10T05:34:22.008251: step 5492, loss 0.486421, acc 0.875, prec 0.0934708, recall 0.792563
2017-12-10T05:34:22.268762: step 5493, loss 0.279049, acc 0.953125, prec 0.0934666, recall 0.792563
2017-12-10T05:34:22.536796: step 5494, loss 0.35231, acc 0.921875, prec 0.0934731, recall 0.792589
2017-12-10T05:34:22.807290: step 5495, loss 0.187048, acc 0.96875, prec 0.0934703, recall 0.792589
2017-12-10T05:34:23.082378: step 5496, loss 0.172559, acc 0.96875, prec 0.0935081, recall 0.792668
2017-12-10T05:34:23.368289: step 5497, loss 0.334512, acc 0.90625, prec 0.0935133, recall 0.792694
2017-12-10T05:34:23.633938: step 5498, loss 0.628229, acc 0.890625, prec 0.0935305, recall 0.792746
2017-12-10T05:34:23.910996: step 5499, loss 0.231959, acc 0.9375, prec 0.0935385, recall 0.792772
2017-12-10T05:34:24.179945: step 5500, loss 0.26955, acc 0.921875, prec 0.0935585, recall 0.792825
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-5500

2017-12-10T05:34:25.485522: step 5501, loss 0.401301, acc 0.890625, prec 0.0935488, recall 0.792825
2017-12-10T05:34:25.746819: step 5502, loss 0.251268, acc 0.921875, prec 0.0935553, recall 0.792851
2017-12-10T05:34:26.020388: step 5503, loss 0.330118, acc 0.90625, prec 0.0935605, recall 0.792877
2017-12-10T05:34:26.289295: step 5504, loss 0.187824, acc 0.96875, prec 0.0935847, recall 0.792929
2017-12-10T05:34:26.558150: step 5505, loss 0.620237, acc 0.9375, prec 0.0935791, recall 0.792929
2017-12-10T05:34:26.830793: step 5506, loss 1.21965, acc 0.90625, prec 0.0935977, recall 0.792982
2017-12-10T05:34:27.099533: step 5507, loss 0.627787, acc 0.9375, prec 0.0936057, recall 0.793008
2017-12-10T05:34:27.376089: step 5508, loss 0.448342, acc 0.953125, prec 0.0936285, recall 0.79306
2017-12-10T05:34:27.644705: step 5509, loss 0.313671, acc 0.9375, prec 0.0936499, recall 0.793112
2017-12-10T05:34:27.913371: step 5510, loss 0.0522621, acc 0.96875, prec 0.0936471, recall 0.793112
2017-12-10T05:34:28.177878: step 5511, loss 0.480302, acc 0.9375, prec 0.0936686, recall 0.793164
2017-12-10T05:34:28.441624: step 5512, loss 0.00284219, acc 1, prec 0.0936955, recall 0.793216
2017-12-10T05:34:28.702863: step 5513, loss 0.0779493, acc 0.984375, prec 0.0936942, recall 0.793216
2017-12-10T05:34:28.972012: step 5514, loss 0.114455, acc 0.984375, prec 0.0937063, recall 0.793243
2017-12-10T05:34:29.244648: step 5515, loss 0.0539861, acc 0.984375, prec 0.0937319, recall 0.793295
2017-12-10T05:34:29.516113: step 5516, loss 0.182995, acc 0.96875, prec 0.0937426, recall 0.793321
2017-12-10T05:34:29.783315: step 5517, loss 0.415782, acc 0.953125, prec 0.0937384, recall 0.793321
2017-12-10T05:34:30.054075: step 5518, loss 0.510667, acc 0.96875, prec 0.0937491, recall 0.793347
2017-12-10T05:34:30.323238: step 5519, loss 0.0247603, acc 0.984375, prec 0.0937612, recall 0.793373
2017-12-10T05:34:30.594654: step 5520, loss 0.171936, acc 0.984375, prec 0.0937733, recall 0.793399
2017-12-10T05:34:30.875575: step 5521, loss 0.113165, acc 0.96875, prec 0.0937705, recall 0.793399
2017-12-10T05:34:31.145106: step 5522, loss 0.00486174, acc 1, prec 0.0937705, recall 0.793399
2017-12-10T05:34:31.418009: step 5523, loss 0.161739, acc 0.96875, prec 0.0937812, recall 0.793425
2017-12-10T05:34:31.684466: step 5524, loss 0.00790024, acc 1, prec 0.0937947, recall 0.793451
2017-12-10T05:34:31.956560: step 5525, loss 0.0372013, acc 0.984375, prec 0.0938068, recall 0.793477
2017-12-10T05:34:32.224376: step 5526, loss 0.285246, acc 1, prec 0.0938203, recall 0.793503
2017-12-10T05:34:32.499521: step 5527, loss 0.0172544, acc 0.984375, prec 0.0938323, recall 0.793529
2017-12-10T05:34:32.764959: step 5528, loss 0.0157354, acc 1, prec 0.0938458, recall 0.793555
2017-12-10T05:34:33.034255: step 5529, loss 0.0142386, acc 1, prec 0.0938728, recall 0.793607
2017-12-10T05:34:33.306522: step 5530, loss 1.59831, acc 0.984375, prec 0.0938863, recall 0.793533
2017-12-10T05:34:33.574694: step 5531, loss 0.0348714, acc 0.984375, prec 0.0938984, recall 0.793559
2017-12-10T05:34:33.839703: step 5532, loss 0.00249392, acc 1, prec 0.0939119, recall 0.793585
2017-12-10T05:34:34.102546: step 5533, loss 0.203668, acc 0.953125, prec 0.0939077, recall 0.793585
2017-12-10T05:34:34.372429: step 5534, loss 0.178462, acc 0.984375, prec 0.0939198, recall 0.793611
2017-12-10T05:34:34.648536: step 5535, loss 0.187607, acc 0.9375, prec 0.0939412, recall 0.793663
2017-12-10T05:34:34.915750: step 5536, loss 0.109381, acc 0.96875, prec 0.0939518, recall 0.793689
2017-12-10T05:34:35.191449: step 5537, loss 0.422296, acc 0.921875, prec 0.0939448, recall 0.793689
2017-12-10T05:34:35.465529: step 5538, loss 0.210583, acc 0.9375, prec 0.0939393, recall 0.793689
2017-12-10T05:34:35.729519: step 5539, loss 0.217091, acc 0.953125, prec 0.093962, recall 0.793741
2017-12-10T05:34:35.998140: step 5540, loss 0.169899, acc 0.984375, prec 0.0939741, recall 0.793766
2017-12-10T05:34:36.271051: step 5541, loss 0.145576, acc 0.953125, prec 0.0939969, recall 0.793818
2017-12-10T05:34:36.537783: step 5542, loss 0.0857532, acc 0.96875, prec 0.0940076, recall 0.793844
2017-12-10T05:34:36.812132: step 5543, loss 0.158516, acc 0.96875, prec 0.0940182, recall 0.79387
2017-12-10T05:34:37.080766: step 5544, loss 0.511784, acc 0.921875, prec 0.0940112, recall 0.79387
2017-12-10T05:34:37.341796: step 5545, loss 0.0950207, acc 0.953125, prec 0.0940071, recall 0.79387
2017-12-10T05:34:37.604416: step 5546, loss 0.102252, acc 0.96875, prec 0.0940177, recall 0.793896
2017-12-10T05:34:37.874002: step 5547, loss 0.025928, acc 0.984375, prec 0.0940298, recall 0.793922
2017-12-10T05:34:38.142029: step 5548, loss 0.344962, acc 0.921875, prec 0.0940363, recall 0.793948
2017-12-10T05:34:38.406045: step 5549, loss 0.129943, acc 0.984375, prec 0.0940349, recall 0.793948
2017-12-10T05:34:38.673596: step 5550, loss 0.216218, acc 0.984375, prec 0.0940335, recall 0.793948
2017-12-10T05:34:38.937611: step 5551, loss 0.0194817, acc 0.984375, prec 0.0940456, recall 0.793974
2017-12-10T05:34:39.210001: step 5552, loss 0.579053, acc 0.953125, prec 0.0940818, recall 0.794051
2017-12-10T05:34:39.483486: step 5553, loss 0.158708, acc 0.984375, prec 0.0940804, recall 0.794051
2017-12-10T05:34:39.753997: step 5554, loss 0.0425179, acc 0.96875, prec 0.0940776, recall 0.794051
2017-12-10T05:34:40.022698: step 5555, loss 0.0101631, acc 1, prec 0.0940911, recall 0.794077
2017-12-10T05:34:40.299271: step 5556, loss 0.0186682, acc 1, prec 0.0940911, recall 0.794077
2017-12-10T05:34:40.574455: step 5557, loss 1.34041, acc 0.9375, prec 0.0940869, recall 0.793977
2017-12-10T05:34:40.845832: step 5558, loss 0.169572, acc 0.984375, prec 0.0940989, recall 0.794003
2017-12-10T05:34:41.121644: step 5559, loss 0.0480954, acc 0.96875, prec 0.0941096, recall 0.794029
2017-12-10T05:34:41.387838: step 5560, loss 0.403588, acc 0.953125, prec 0.0941054, recall 0.794029
2017-12-10T05:34:41.660069: step 5561, loss 0.136601, acc 0.984375, prec 0.0941444, recall 0.794107
2017-12-10T05:34:41.935910: step 5562, loss 0.11744, acc 1, prec 0.0941848, recall 0.794184
2017-12-10T05:34:42.210486: step 5563, loss 0.145754, acc 0.96875, prec 0.094182, recall 0.794184
2017-12-10T05:34:42.487779: step 5564, loss 0.0164184, acc 1, prec 0.094182, recall 0.794184
2017-12-10T05:34:42.756060: step 5565, loss 0.185957, acc 0.953125, prec 0.0941778, recall 0.794184
2017-12-10T05:34:43.027172: step 5566, loss 0.0463147, acc 0.96875, prec 0.094175, recall 0.794184
2017-12-10T05:34:43.293933: step 5567, loss 0.464599, acc 0.984375, prec 0.0941736, recall 0.794184
2017-12-10T05:34:43.564798: step 5568, loss 0.140485, acc 0.96875, prec 0.0941708, recall 0.794184
2017-12-10T05:34:43.836036: step 5569, loss 0.447791, acc 0.921875, prec 0.0941773, recall 0.79421
2017-12-10T05:34:44.100897: step 5570, loss 0.387565, acc 0.953125, prec 0.0941731, recall 0.79421
2017-12-10T05:34:44.371043: step 5571, loss 0.325086, acc 0.890625, prec 0.0941633, recall 0.79421
2017-12-10T05:34:44.642055: step 5572, loss 0.288618, acc 1, prec 0.0941767, recall 0.794236
2017-12-10T05:34:44.919356: step 5573, loss 0.124231, acc 0.984375, prec 0.0941888, recall 0.794261
2017-12-10T05:34:45.191856: step 5574, loss 0.0434784, acc 0.984375, prec 0.0942008, recall 0.794287
2017-12-10T05:34:45.467116: step 5575, loss 0.334817, acc 0.9375, prec 0.0941953, recall 0.794287
2017-12-10T05:34:45.734901: step 5576, loss 0.60214, acc 0.921875, prec 0.0941883, recall 0.794287
2017-12-10T05:34:46.002809: step 5577, loss 0.0952986, acc 0.953125, prec 0.0941975, recall 0.794313
2017-12-10T05:34:46.271149: step 5578, loss 0.150972, acc 0.96875, prec 0.0941947, recall 0.794313
2017-12-10T05:34:46.545558: step 5579, loss 0.112299, acc 0.9375, prec 0.0942295, recall 0.79439
2017-12-10T05:34:46.821752: step 5580, loss 0.0338755, acc 0.984375, prec 0.0942281, recall 0.79439
2017-12-10T05:34:47.087300: step 5581, loss 0.0564447, acc 0.984375, prec 0.0942401, recall 0.794416
2017-12-10T05:34:47.361705: step 5582, loss 0.1369, acc 0.984375, prec 0.0942387, recall 0.794416
2017-12-10T05:34:47.627558: step 5583, loss 0.0394562, acc 0.984375, prec 0.0942508, recall 0.794442
2017-12-10T05:34:47.897065: step 5584, loss 0.074954, acc 0.9375, prec 0.0942586, recall 0.794467
2017-12-10T05:34:48.179406: step 5585, loss 0.0143024, acc 1, prec 0.0942721, recall 0.794493
2017-12-10T05:34:48.443914: step 5586, loss 0.0245361, acc 0.984375, prec 0.0942841, recall 0.794519
2017-12-10T05:34:48.714943: step 5587, loss 0.200289, acc 0.984375, prec 0.0942827, recall 0.794519
2017-12-10T05:34:48.980971: step 5588, loss 0.186475, acc 0.953125, prec 0.0943054, recall 0.79457
2017-12-10T05:34:49.250957: step 5589, loss 0.511454, acc 0.984375, prec 0.094304, recall 0.79457
2017-12-10T05:34:49.519228: step 5590, loss 0.397409, acc 0.96875, prec 0.0943416, recall 0.794647
2017-12-10T05:34:49.794594: step 5591, loss 0.0128593, acc 1, prec 0.094355, recall 0.794673
2017-12-10T05:34:50.071301: step 5592, loss 0.0127295, acc 1, prec 0.094355, recall 0.794673
2017-12-10T05:34:50.346712: step 5593, loss 0.245862, acc 0.9375, prec 0.0943898, recall 0.79475
2017-12-10T05:34:50.619341: step 5594, loss 0.0640852, acc 0.96875, prec 0.094387, recall 0.79475
2017-12-10T05:34:50.888195: step 5595, loss 0.000869652, acc 1, prec 0.094387, recall 0.79475
2017-12-10T05:34:51.161706: step 5596, loss 0.223043, acc 0.953125, prec 0.0943962, recall 0.794776
2017-12-10T05:34:51.429586: step 5597, loss 0.152773, acc 0.96875, prec 0.0944068, recall 0.794801
2017-12-10T05:34:51.705672: step 5598, loss 0.0145498, acc 0.984375, prec 0.0944189, recall 0.794827
2017-12-10T05:34:51.970234: step 5599, loss 0.00178406, acc 1, prec 0.0944458, recall 0.794878
2017-12-10T05:34:52.233090: step 5600, loss 0.41068, acc 0.984375, prec 0.0944712, recall 0.794929
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-5600

2017-12-10T05:34:53.531954: step 5601, loss 0.0132545, acc 1, prec 0.0944712, recall 0.794929
2017-12-10T05:34:54.496983: step 5602, loss 16.2582, acc 0.984375, prec 0.0944712, recall 0.79483
2017-12-10T05:34:54.856081: step 5603, loss 0.0693429, acc 0.984375, prec 0.0944698, recall 0.79483
2017-12-10T05:34:55.121378: step 5604, loss 4.45224, acc 0.96875, prec 0.0944819, recall 0.794757
2017-12-10T05:34:55.663395: step 5605, loss 0.222825, acc 0.9375, prec 0.0944897, recall 0.794782
2017-12-10T05:34:56.370254: step 5606, loss 0.0608752, acc 0.96875, prec 0.0944869, recall 0.794782
2017-12-10T05:34:57.099290: step 5607, loss 0.434021, acc 0.90625, prec 0.0944785, recall 0.794782
2017-12-10T05:34:57.815875: step 5608, loss 0.392518, acc 0.890625, prec 0.0944821, recall 0.794808
2017-12-10T05:34:58.533034: step 5609, loss 0.704205, acc 0.765625, prec 0.0944745, recall 0.794833
2017-12-10T05:34:59.273474: step 5610, loss 1.03098, acc 0.71875, prec 0.0944493, recall 0.794833
2017-12-10T05:34:59.996877: step 5611, loss 1.30017, acc 0.703125, prec 0.0944495, recall 0.794885
2017-12-10T05:35:00.750059: step 5612, loss 1.10026, acc 0.75, prec 0.0944272, recall 0.794885
2017-12-10T05:35:01.481544: step 5613, loss 1.56311, acc 0.578125, prec 0.0944028, recall 0.79491
2017-12-10T05:35:02.194613: step 5614, loss 0.98005, acc 0.796875, prec 0.094398, recall 0.794936
2017-12-10T05:35:02.926456: step 5615, loss 1.08974, acc 0.765625, prec 0.0943905, recall 0.794961
2017-12-10T05:35:03.652891: step 5616, loss 0.639421, acc 0.828125, prec 0.0943885, recall 0.794987
2017-12-10T05:35:04.401862: step 5617, loss 0.912072, acc 0.78125, prec 0.0943824, recall 0.795012
2017-12-10T05:35:05.113820: step 5618, loss 0.581968, acc 0.875, prec 0.094398, recall 0.795064
2017-12-10T05:35:05.841566: step 5619, loss 0.667213, acc 0.828125, prec 0.0944094, recall 0.795115
2017-12-10T05:35:06.567239: step 5620, loss 0.215891, acc 0.9375, prec 0.0944306, recall 0.795166
2017-12-10T05:35:07.278481: step 5621, loss 0.200248, acc 0.90625, prec 0.0944356, recall 0.795191
2017-12-10T05:35:08.476540: step 5622, loss 0.0941961, acc 0.953125, prec 0.0944315, recall 0.795191
2017-12-10T05:35:08.809114: step 5623, loss 0.181633, acc 0.953125, prec 0.0944273, recall 0.795191
2017-12-10T05:35:09.091681: step 5624, loss 0.293899, acc 0.90625, prec 0.0944189, recall 0.795191
2017-12-10T05:35:09.380035: step 5625, loss 0.0502112, acc 0.984375, prec 0.0944577, recall 0.795268
2017-12-10T05:35:09.675613: step 5626, loss 0.29559, acc 0.9375, prec 0.0944521, recall 0.795268
2017-12-10T05:35:09.970319: step 5627, loss 0.367938, acc 0.953125, prec 0.0944613, recall 0.795293
2017-12-10T05:35:10.253402: step 5628, loss 0.0576129, acc 0.96875, prec 0.0944719, recall 0.795319
2017-12-10T05:35:10.525250: step 5629, loss 0.0226576, acc 0.984375, prec 0.0944839, recall 0.795344
2017-12-10T05:35:10.796823: step 5630, loss 0.100742, acc 0.96875, prec 0.0944945, recall 0.79537
2017-12-10T05:35:11.066690: step 5631, loss 0.0118466, acc 1, prec 0.0944945, recall 0.79537
2017-12-10T05:35:11.340761: step 5632, loss 0.00571481, acc 1, prec 0.0944945, recall 0.79537
2017-12-10T05:35:11.607901: step 5633, loss 0.0293679, acc 0.984375, prec 0.0944931, recall 0.79537
2017-12-10T05:35:11.876110: step 5634, loss 0.000614761, acc 1, prec 0.0944931, recall 0.79537
2017-12-10T05:35:12.147784: step 5635, loss 0.0176805, acc 1, prec 0.0945065, recall 0.795395
2017-12-10T05:35:12.434349: step 5636, loss 0.0167752, acc 1, prec 0.0945065, recall 0.795395
2017-12-10T05:35:12.700829: step 5637, loss 0.897382, acc 1, prec 0.0945199, recall 0.795421
2017-12-10T05:35:12.971433: step 5638, loss 1.55692, acc 0.96875, prec 0.0945185, recall 0.795322
2017-12-10T05:35:13.241489: step 5639, loss 0.000203036, acc 1, prec 0.0945318, recall 0.795347
2017-12-10T05:35:13.515025: step 5640, loss 0.0242619, acc 1, prec 0.0945452, recall 0.795373
2017-12-10T05:35:13.779243: step 5641, loss 0.0865769, acc 0.984375, prec 0.094584, recall 0.795449
2017-12-10T05:35:14.049368: step 5642, loss 0.333785, acc 0.96875, prec 0.0945812, recall 0.795449
2017-12-10T05:35:14.318352: step 5643, loss 0.213411, acc 0.921875, prec 0.0945742, recall 0.795449
2017-12-10T05:35:14.586563: step 5644, loss 0.151533, acc 0.953125, prec 0.0945834, recall 0.795474
2017-12-10T05:35:14.866594: step 5645, loss 0.131061, acc 0.96875, prec 0.0946074, recall 0.795525
2017-12-10T05:35:15.135471: step 5646, loss 0.232526, acc 0.953125, prec 0.0946433, recall 0.795601
2017-12-10T05:35:15.410070: step 5647, loss 0.215555, acc 0.9375, prec 0.0946377, recall 0.795601
2017-12-10T05:35:15.673812: step 5648, loss 0.191828, acc 0.96875, prec 0.0946349, recall 0.795601
2017-12-10T05:35:15.942925: step 5649, loss 0.140973, acc 0.96875, prec 0.0946455, recall 0.795627
2017-12-10T05:35:16.211084: step 5650, loss 0.0875898, acc 0.984375, prec 0.0946709, recall 0.795678
2017-12-10T05:35:16.479462: step 5651, loss 0.0236105, acc 0.984375, prec 0.0946695, recall 0.795678
2017-12-10T05:35:16.744680: step 5652, loss 0.0303005, acc 0.984375, prec 0.0946681, recall 0.795678
2017-12-10T05:35:17.013542: step 5653, loss 0.0915053, acc 0.953125, prec 0.0946773, recall 0.795703
2017-12-10T05:35:17.281741: step 5654, loss 0.260279, acc 0.953125, prec 0.0946864, recall 0.795728
2017-12-10T05:35:17.547995: step 5655, loss 0.0981461, acc 0.953125, prec 0.0947224, recall 0.795804
2017-12-10T05:35:17.812846: step 5656, loss 0.574824, acc 0.953125, prec 0.0947182, recall 0.795804
2017-12-10T05:35:18.085663: step 5657, loss 0.077865, acc 0.984375, prec 0.0947302, recall 0.79583
2017-12-10T05:35:18.347125: step 5658, loss 0.257523, acc 0.953125, prec 0.094726, recall 0.79583
2017-12-10T05:35:18.614676: step 5659, loss 0.624788, acc 0.953125, prec 0.0947218, recall 0.79583
2017-12-10T05:35:18.892151: step 5660, loss 0.102738, acc 0.96875, prec 0.094719, recall 0.79583
2017-12-10T05:35:19.161099: step 5661, loss 0.0784444, acc 0.96875, prec 0.0947162, recall 0.79583
2017-12-10T05:35:19.432836: step 5662, loss 0.00587099, acc 1, prec 0.0947162, recall 0.79583
2017-12-10T05:35:19.702612: step 5663, loss 0.00903161, acc 1, prec 0.0947295, recall 0.795855
2017-12-10T05:35:19.964973: step 5664, loss 0.00467963, acc 1, prec 0.0947429, recall 0.79588
2017-12-10T05:35:20.235684: step 5665, loss 0.0687536, acc 0.984375, prec 0.0947415, recall 0.79588
2017-12-10T05:35:20.508200: step 5666, loss 0.058496, acc 0.984375, prec 0.0947535, recall 0.795906
2017-12-10T05:35:20.779063: step 5667, loss 0.0493359, acc 0.96875, prec 0.0947774, recall 0.795956
2017-12-10T05:35:21.046337: step 5668, loss 0.0340878, acc 0.984375, prec 0.094776, recall 0.795956
2017-12-10T05:35:21.310082: step 5669, loss 0.072841, acc 0.984375, prec 0.094788, recall 0.795982
2017-12-10T05:35:21.574998: step 5670, loss 0.0784359, acc 0.96875, prec 0.0948119, recall 0.796032
2017-12-10T05:35:21.850578: step 5671, loss 0.0310422, acc 0.984375, prec 0.0948105, recall 0.796032
2017-12-10T05:35:22.116815: step 5672, loss 0.0101336, acc 1, prec 0.0948239, recall 0.796058
2017-12-10T05:35:22.388787: step 5673, loss 0.574813, acc 0.96875, prec 0.0948478, recall 0.796108
2017-12-10T05:35:22.654228: step 5674, loss 0.0433031, acc 1, prec 0.0948612, recall 0.796133
2017-12-10T05:35:22.927151: step 5675, loss 0.440919, acc 1, prec 0.0948746, recall 0.796159
2017-12-10T05:35:23.204825: step 5676, loss 0.133566, acc 0.984375, prec 0.0949133, recall 0.796234
2017-12-10T05:35:23.476925: step 5677, loss 0.120391, acc 0.96875, prec 0.0949238, recall 0.79626
2017-12-10T05:35:23.745051: step 5678, loss 0.842901, acc 0.921875, prec 0.0949168, recall 0.79626
2017-12-10T05:35:24.014112: step 5679, loss 0.142782, acc 0.96875, prec 0.0949274, recall 0.796285
2017-12-10T05:35:24.281096: step 5680, loss 0.181653, acc 0.96875, prec 0.0949379, recall 0.79631
2017-12-10T05:35:24.550558: step 5681, loss 0.0164577, acc 1, prec 0.0949513, recall 0.796335
2017-12-10T05:35:24.817076: step 5682, loss 0.400921, acc 0.9375, prec 0.094959, recall 0.79636
2017-12-10T05:35:25.087661: step 5683, loss 0.14644, acc 0.984375, prec 0.0949576, recall 0.79636
2017-12-10T05:35:25.351738: step 5684, loss 0.138882, acc 0.984375, prec 0.0949562, recall 0.79636
2017-12-10T05:35:25.619307: step 5685, loss 0.0373633, acc 0.984375, prec 0.0949682, recall 0.796386
2017-12-10T05:35:25.886299: step 5686, loss 0.451636, acc 0.953125, prec 0.0949773, recall 0.796411
2017-12-10T05:35:26.158468: step 5687, loss 0.139542, acc 0.96875, prec 0.0949745, recall 0.796411
2017-12-10T05:35:26.429844: step 5688, loss 0.0483669, acc 0.984375, prec 0.0949998, recall 0.796461
2017-12-10T05:35:26.697900: step 5689, loss 2.82836, acc 0.9375, prec 0.095009, recall 0.796388
2017-12-10T05:35:26.984118: step 5690, loss 0.0772053, acc 0.96875, prec 0.0950196, recall 0.796413
2017-12-10T05:35:27.264931: step 5691, loss 0.161862, acc 0.984375, prec 0.0950449, recall 0.796463
2017-12-10T05:35:27.535974: step 5692, loss 0.389825, acc 0.9375, prec 0.0950526, recall 0.796489
2017-12-10T05:35:27.807907: step 5693, loss 0.448885, acc 0.921875, prec 0.0950589, recall 0.796514
2017-12-10T05:35:28.080511: step 5694, loss 0.315362, acc 0.953125, prec 0.0950814, recall 0.796564
2017-12-10T05:35:28.346022: step 5695, loss 0.302635, acc 0.96875, prec 0.095092, recall 0.796589
2017-12-10T05:35:28.624243: step 5696, loss 0.0255568, acc 0.984375, prec 0.0951039, recall 0.796614
2017-12-10T05:35:28.891196: step 5697, loss 0.3222, acc 0.921875, prec 0.0950969, recall 0.796614
2017-12-10T05:35:29.156033: step 5698, loss 0.326137, acc 0.9375, prec 0.095118, recall 0.796665
2017-12-10T05:35:29.426317: step 5699, loss 0.359342, acc 0.90625, prec 0.0951096, recall 0.796665
2017-12-10T05:35:29.689635: step 5700, loss 0.504155, acc 0.9375, prec 0.0951173, recall 0.79669

Evaluation:
2017-12-10T05:35:37.428835: step 5700, loss 3.70159, acc 0.910266, prec 0.0950526, recall 0.792389

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-5700

2017-12-10T05:35:38.625104: step 5701, loss 0.567156, acc 0.921875, prec 0.095072, recall 0.792439
2017-12-10T05:35:38.884809: step 5702, loss 1.09372, acc 0.9375, prec 0.0950796, recall 0.792464
2017-12-10T05:35:39.158817: step 5703, loss 0.111443, acc 0.984375, prec 0.0950782, recall 0.792464
2017-12-10T05:35:39.422903: step 5704, loss 0.325902, acc 0.921875, prec 0.0950844, recall 0.792489
2017-12-10T05:35:39.691667: step 5705, loss 0.719104, acc 0.90625, prec 0.0951025, recall 0.79254
2017-12-10T05:35:39.959727: step 5706, loss 0.589896, acc 0.890625, prec 0.0951059, recall 0.792565
2017-12-10T05:35:40.227678: step 5707, loss 0.248962, acc 0.96875, prec 0.0951163, recall 0.79259
2017-12-10T05:35:40.489080: step 5708, loss 0.160601, acc 0.9375, prec 0.0951371, recall 0.79264
2017-12-10T05:35:40.751136: step 5709, loss 0.117649, acc 0.953125, prec 0.0951592, recall 0.79269
2017-12-10T05:35:41.015505: step 5710, loss 0.362393, acc 0.921875, prec 0.0951655, recall 0.792715
2017-12-10T05:35:41.283795: step 5711, loss 0.222592, acc 0.984375, prec 0.0951904, recall 0.792766
2017-12-10T05:35:41.556933: step 5712, loss 0.316472, acc 0.890625, prec 0.0951807, recall 0.792766
2017-12-10T05:35:41.822660: step 5713, loss 0.189779, acc 0.96875, prec 0.0951911, recall 0.792791
2017-12-10T05:35:42.090080: step 5714, loss 0.284667, acc 0.953125, prec 0.0952001, recall 0.792816
2017-12-10T05:35:42.365991: step 5715, loss 0.14121, acc 0.984375, prec 0.0951987, recall 0.792816
2017-12-10T05:35:42.630841: step 5716, loss 0.056564, acc 0.96875, prec 0.0951959, recall 0.792816
2017-12-10T05:35:42.901570: step 5717, loss 0.00339084, acc 1, prec 0.0952485, recall 0.792916
2017-12-10T05:35:43.169578: step 5718, loss 0.0134679, acc 0.984375, prec 0.0952734, recall 0.792966
2017-12-10T05:35:43.437948: step 5719, loss 0.375051, acc 0.953125, prec 0.0952823, recall 0.792991
2017-12-10T05:35:43.711794: step 5720, loss 0.0821267, acc 0.984375, prec 0.0952941, recall 0.793016
2017-12-10T05:35:43.976074: step 5721, loss 0.0904901, acc 0.953125, prec 0.09529, recall 0.793016
2017-12-10T05:35:44.248869: step 5722, loss 0.0340095, acc 0.984375, prec 0.0952886, recall 0.793016
2017-12-10T05:35:44.512244: step 5723, loss 0.0972098, acc 0.96875, prec 0.0952989, recall 0.793041
2017-12-10T05:35:44.782759: step 5724, loss 0.161983, acc 0.984375, prec 0.0952976, recall 0.793041
2017-12-10T05:35:45.051397: step 5725, loss 0.3004, acc 0.953125, prec 0.0952934, recall 0.793041
2017-12-10T05:35:45.322041: step 5726, loss 0.153538, acc 0.953125, prec 0.0952893, recall 0.793041
2017-12-10T05:35:45.600813: step 5727, loss 0.0961554, acc 0.96875, prec 0.0953128, recall 0.793091
2017-12-10T05:35:45.868953: step 5728, loss 0.356551, acc 0.9375, prec 0.0953072, recall 0.793091
2017-12-10T05:35:46.132990: step 5729, loss 0.0939326, acc 0.984375, prec 0.0953321, recall 0.793141
2017-12-10T05:35:46.406230: step 5730, loss 0.159331, acc 0.96875, prec 0.0953293, recall 0.793141
2017-12-10T05:35:46.671321: step 5731, loss 3.65627e-06, acc 1, prec 0.0953293, recall 0.793141
2017-12-10T05:35:46.940098: step 5732, loss 0.175721, acc 0.96875, prec 0.0953397, recall 0.793166
2017-12-10T05:35:47.219197: step 5733, loss 0.0723498, acc 0.984375, prec 0.0953383, recall 0.793166
2017-12-10T05:35:47.482642: step 5734, loss 0.328644, acc 1, prec 0.0953514, recall 0.793191
2017-12-10T05:35:47.752280: step 5735, loss 0.273732, acc 0.96875, prec 0.0953749, recall 0.793241
2017-12-10T05:35:48.015458: step 5736, loss 0.19527, acc 1, prec 0.0954143, recall 0.793316
2017-12-10T05:35:48.292187: step 5737, loss 0.00139186, acc 1, prec 0.0954143, recall 0.793316
2017-12-10T05:35:48.556821: step 5738, loss 0.101234, acc 0.984375, prec 0.0954654, recall 0.793415
2017-12-10T05:35:48.827907: step 5739, loss 5.1162e-05, acc 1, prec 0.0954654, recall 0.793415
2017-12-10T05:35:49.097890: step 5740, loss 0.0994071, acc 0.984375, prec 0.0954903, recall 0.793465
2017-12-10T05:35:49.363211: step 5741, loss 11.8783, acc 0.96875, prec 0.0955034, recall 0.793299
2017-12-10T05:35:49.647664: step 5742, loss 0.360992, acc 0.9375, prec 0.0954979, recall 0.793299
2017-12-10T05:35:49.921927: step 5743, loss 0.0361456, acc 0.984375, prec 0.0954965, recall 0.793299
2017-12-10T05:35:50.188396: step 5744, loss 0.096244, acc 0.953125, prec 0.0955055, recall 0.793324
2017-12-10T05:35:50.456088: step 5745, loss 0.465917, acc 0.90625, prec 0.0954971, recall 0.793324
2017-12-10T05:35:50.730266: step 5746, loss 0.433854, acc 0.890625, prec 0.0954874, recall 0.793324
2017-12-10T05:35:50.995319: step 5747, loss 0.282652, acc 0.9375, prec 0.095495, recall 0.793349
2017-12-10T05:35:51.271072: step 5748, loss 0.382727, acc 0.890625, prec 0.0955247, recall 0.793423
2017-12-10T05:35:51.532867: step 5749, loss 0.61353, acc 0.90625, prec 0.0955295, recall 0.793448
2017-12-10T05:35:51.802492: step 5750, loss 0.639398, acc 0.875, prec 0.0955446, recall 0.793498
2017-12-10T05:35:52.075480: step 5751, loss 0.975774, acc 0.828125, prec 0.0955425, recall 0.793523
2017-12-10T05:35:52.348967: step 5752, loss 0.722405, acc 0.84375, prec 0.0955418, recall 0.793548
2017-12-10T05:35:52.622696: step 5753, loss 0.695557, acc 0.90625, prec 0.0955466, recall 0.793572
2017-12-10T05:35:52.889265: step 5754, loss 0.884789, acc 0.828125, prec 0.0955313, recall 0.793572
2017-12-10T05:35:53.157626: step 5755, loss 0.879217, acc 0.8125, prec 0.0955278, recall 0.793597
2017-12-10T05:35:53.421694: step 5756, loss 0.552971, acc 0.875, prec 0.0955168, recall 0.793597
2017-12-10T05:35:53.692621: step 5757, loss 0.626301, acc 0.890625, prec 0.0955333, recall 0.793647
2017-12-10T05:35:53.956902: step 5758, loss 0.808406, acc 0.84375, prec 0.0955194, recall 0.793647
2017-12-10T05:35:54.221440: step 5759, loss 0.356043, acc 0.9375, prec 0.0955532, recall 0.793721
2017-12-10T05:35:54.488552: step 5760, loss 1.16982, acc 0.796875, prec 0.0955352, recall 0.793721
2017-12-10T05:35:54.754233: step 5761, loss 0.30694, acc 0.90625, prec 0.09554, recall 0.793746
2017-12-10T05:35:55.020284: step 5762, loss 0.285113, acc 0.90625, prec 0.0955317, recall 0.793746
2017-12-10T05:35:55.290479: step 5763, loss 0.409123, acc 0.921875, prec 0.0955772, recall 0.793845
2017-12-10T05:35:55.558335: step 5764, loss 0.370387, acc 0.90625, prec 0.095595, recall 0.793895
2017-12-10T05:35:55.833372: step 5765, loss 0.0764875, acc 0.96875, prec 0.0956315, recall 0.793969
2017-12-10T05:35:56.098352: step 5766, loss 0.248109, acc 0.9375, prec 0.095626, recall 0.793969
2017-12-10T05:35:56.363028: step 5767, loss 0.463486, acc 0.96875, prec 0.0956363, recall 0.793994
2017-12-10T05:35:56.636122: step 5768, loss 0.016462, acc 1, prec 0.0956363, recall 0.793994
2017-12-10T05:35:56.901637: step 5769, loss 0.301511, acc 0.96875, prec 0.0956336, recall 0.793994
2017-12-10T05:35:57.182026: step 5770, loss 0.00708984, acc 1, prec 0.0956466, recall 0.794019
2017-12-10T05:35:57.454846: step 5771, loss 0.0329186, acc 0.984375, prec 0.0956453, recall 0.794019
2017-12-10T05:35:57.718205: step 5772, loss 5.11784, acc 0.96875, prec 0.095657, recall 0.793948
2017-12-10T05:35:57.990959: step 5773, loss 0.0978894, acc 0.96875, prec 0.0956542, recall 0.793948
2017-12-10T05:35:58.259765: step 5774, loss 0.0291599, acc 0.984375, prec 0.0956528, recall 0.793948
2017-12-10T05:35:58.528870: step 5775, loss 0.0362131, acc 0.984375, prec 0.0956907, recall 0.794022
2017-12-10T05:35:58.799488: step 5776, loss 0.0651461, acc 0.96875, prec 0.0957271, recall 0.794096
2017-12-10T05:35:59.064847: step 5777, loss 0.0240166, acc 0.984375, prec 0.0957388, recall 0.794121
2017-12-10T05:35:59.326579: step 5778, loss 0.550674, acc 0.890625, prec 0.0957422, recall 0.794146
2017-12-10T05:35:59.600389: step 5779, loss 0.217971, acc 0.9375, prec 0.0957498, recall 0.794171
2017-12-10T05:35:59.868248: step 5780, loss 0.243893, acc 0.9375, prec 0.0957442, recall 0.794171
2017-12-10T05:36:00.132215: step 5781, loss 0.530406, acc 0.921875, prec 0.0957634, recall 0.79422
2017-12-10T05:36:00.401035: step 5782, loss 0.0273811, acc 0.984375, prec 0.0957751, recall 0.794245
2017-12-10T05:36:00.681956: step 5783, loss 0.199593, acc 0.984375, prec 0.095813, recall 0.794319
2017-12-10T05:36:00.948208: step 5784, loss 0.155649, acc 0.953125, prec 0.0958088, recall 0.794319
2017-12-10T05:36:01.220375: step 5785, loss 0.293465, acc 0.90625, prec 0.0958005, recall 0.794319
2017-12-10T05:36:01.487582: step 5786, loss 0.576828, acc 0.921875, prec 0.0958067, recall 0.794343
2017-12-10T05:36:01.761981: step 5787, loss 0.342896, acc 0.9375, prec 0.0958142, recall 0.794368
2017-12-10T05:36:02.033359: step 5788, loss 0.305276, acc 0.9375, prec 0.0958086, recall 0.794368
2017-12-10T05:36:02.304809: step 5789, loss 0.203919, acc 0.953125, prec 0.0958045, recall 0.794368
2017-12-10T05:36:02.571388: step 5790, loss 0.0186516, acc 1, prec 0.0958045, recall 0.794368
2017-12-10T05:36:02.843219: step 5791, loss 0.203513, acc 0.953125, prec 0.0958003, recall 0.794368
2017-12-10T05:36:03.109557: step 5792, loss 0.478882, acc 0.921875, prec 0.0958195, recall 0.794417
2017-12-10T05:36:03.379414: step 5793, loss 0.204292, acc 0.96875, prec 0.0958168, recall 0.794417
2017-12-10T05:36:03.651642: step 5794, loss 0.0919672, acc 0.984375, prec 0.0958154, recall 0.794417
2017-12-10T05:36:03.917768: step 5795, loss 0.0412551, acc 0.984375, prec 0.095814, recall 0.794417
2017-12-10T05:36:04.184834: step 5796, loss 0.00239878, acc 1, prec 0.0958271, recall 0.794442
2017-12-10T05:36:04.468836: step 5797, loss 0.00580703, acc 1, prec 0.0958271, recall 0.794442
2017-12-10T05:36:04.742534: step 5798, loss 0.087053, acc 0.96875, prec 0.0958243, recall 0.794442
2017-12-10T05:36:05.010067: step 5799, loss 0.10661, acc 0.953125, prec 0.0958202, recall 0.794442
2017-12-10T05:36:05.274002: step 5800, loss 0.00122733, acc 1, prec 0.0958202, recall 0.794442
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-5800

2017-12-10T05:36:06.584185: step 5801, loss 0.235196, acc 0.96875, prec 0.0958304, recall 0.794466
2017-12-10T05:36:06.860494: step 5802, loss 0.327668, acc 0.984375, prec 0.0958421, recall 0.794491
2017-12-10T05:36:07.127718: step 5803, loss 0.42857, acc 0.953125, prec 0.095838, recall 0.794491
2017-12-10T05:36:07.402037: step 5804, loss 0.477544, acc 0.984375, prec 0.0958496, recall 0.794516
2017-12-10T05:36:07.669608: step 5805, loss 0.0203738, acc 0.984375, prec 0.0958483, recall 0.794516
2017-12-10T05:36:07.935632: step 5806, loss 0.0413905, acc 0.984375, prec 0.0958469, recall 0.794516
2017-12-10T05:36:08.203572: step 5807, loss 0.0294189, acc 0.984375, prec 0.0958586, recall 0.79454
2017-12-10T05:36:08.480206: step 5808, loss 0.00150439, acc 1, prec 0.0958586, recall 0.79454
2017-12-10T05:36:08.753922: step 5809, loss 0.553072, acc 0.984375, prec 0.0958702, recall 0.794565
2017-12-10T05:36:09.029915: step 5810, loss 0.033966, acc 0.984375, prec 0.0958688, recall 0.794565
2017-12-10T05:36:09.296555: step 5811, loss 0.074983, acc 0.984375, prec 0.0958936, recall 0.794614
2017-12-10T05:36:09.565926: step 5812, loss 0.224657, acc 0.953125, prec 0.0958894, recall 0.794614
2017-12-10T05:36:09.833845: step 5813, loss 0.09995, acc 0.96875, prec 0.0958867, recall 0.794614
2017-12-10T05:36:10.100893: step 5814, loss 2.19071, acc 0.9375, prec 0.0958956, recall 0.794544
2017-12-10T05:36:10.368450: step 5815, loss 0.279229, acc 0.9375, prec 0.09589, recall 0.794544
2017-12-10T05:36:10.633761: step 5816, loss 0.373961, acc 0.921875, prec 0.0958961, recall 0.794568
2017-12-10T05:36:10.897707: step 5817, loss 0.645074, acc 0.90625, prec 0.0958878, recall 0.794568
2017-12-10T05:36:11.167913: step 5818, loss 0.153773, acc 0.953125, prec 0.0958967, recall 0.794593
2017-12-10T05:36:11.442697: step 5819, loss 0.186533, acc 0.953125, prec 0.0958926, recall 0.794593
2017-12-10T05:36:11.708586: step 5820, loss 0.200106, acc 0.953125, prec 0.0958884, recall 0.794593
2017-12-10T05:36:11.980093: step 5821, loss 0.796666, acc 0.90625, prec 0.0959062, recall 0.794642
2017-12-10T05:36:12.250585: step 5822, loss 0.390107, acc 0.953125, prec 0.0959282, recall 0.794691
2017-12-10T05:36:12.520442: step 5823, loss 0.228232, acc 0.9375, prec 0.0959357, recall 0.794715
2017-12-10T05:36:12.784478: step 5824, loss 0.206914, acc 0.9375, prec 0.0959302, recall 0.794715
2017-12-10T05:36:13.052907: step 5825, loss 0.885056, acc 0.859375, prec 0.0959438, recall 0.794765
2017-12-10T05:36:13.325273: step 5826, loss 0.436899, acc 0.90625, prec 0.0959355, recall 0.794765
2017-12-10T05:36:13.602062: step 5827, loss 0.64858, acc 0.890625, prec 0.0959388, recall 0.794789
2017-12-10T05:36:13.869681: step 5828, loss 0.418972, acc 0.890625, prec 0.0959291, recall 0.794789
2017-12-10T05:36:14.141896: step 5829, loss 0.206745, acc 0.921875, prec 0.0959483, recall 0.794838
2017-12-10T05:36:14.407453: step 5830, loss 0.292755, acc 0.953125, prec 0.0959833, recall 0.794912
2017-12-10T05:36:14.676828: step 5831, loss 0.0734965, acc 0.984375, prec 0.0959819, recall 0.794912
2017-12-10T05:36:14.944966: step 5832, loss 0.0587656, acc 0.984375, prec 0.0959805, recall 0.794912
2017-12-10T05:36:15.223531: step 5833, loss 0.65368, acc 0.921875, prec 0.0959736, recall 0.794912
2017-12-10T05:36:15.493114: step 5834, loss 0.0903528, acc 0.953125, prec 0.0959825, recall 0.794936
2017-12-10T05:36:15.772902: step 5835, loss 0.0320842, acc 0.984375, prec 0.0959941, recall 0.794961
2017-12-10T05:36:16.035345: step 5836, loss 0.420302, acc 0.9375, prec 0.0959886, recall 0.794961
2017-12-10T05:36:16.306735: step 5837, loss 0.484482, acc 0.890625, prec 0.0959919, recall 0.794985
2017-12-10T05:36:16.573298: step 5838, loss 0.181795, acc 0.96875, prec 0.0960022, recall 0.79501
2017-12-10T05:36:16.841645: step 5839, loss 0.22915, acc 0.96875, prec 0.0960255, recall 0.795058
2017-12-10T05:36:17.117714: step 5840, loss 0.680242, acc 0.90625, prec 0.0960172, recall 0.795058
2017-12-10T05:36:17.381781: step 5841, loss 0.134445, acc 0.953125, prec 0.0960261, recall 0.795083
2017-12-10T05:36:17.656917: step 5842, loss 0.0148337, acc 1, prec 0.0960261, recall 0.795083
2017-12-10T05:36:17.924469: step 5843, loss 0.0304797, acc 0.984375, prec 0.0960377, recall 0.795107
2017-12-10T05:36:18.197718: step 5844, loss 0.343512, acc 0.9375, prec 0.0960322, recall 0.795107
2017-12-10T05:36:18.467021: step 5845, loss 0.400158, acc 0.953125, prec 0.0960541, recall 0.795156
2017-12-10T05:36:18.736927: step 5846, loss 0.159711, acc 0.953125, prec 0.0960629, recall 0.795181
2017-12-10T05:36:19.007208: step 5847, loss 1.93815, acc 0.96875, prec 0.0960876, recall 0.795135
2017-12-10T05:36:19.272026: step 5848, loss 0.0949158, acc 0.984375, prec 0.0960993, recall 0.795159
2017-12-10T05:36:19.547818: step 5849, loss 0.0211065, acc 0.984375, prec 0.0960979, recall 0.795159
2017-12-10T05:36:19.812174: step 5850, loss 0.155213, acc 0.9375, prec 0.0960923, recall 0.795159
2017-12-10T05:36:20.083917: step 5851, loss 0.358583, acc 0.96875, prec 0.0961026, recall 0.795184
2017-12-10T05:36:20.352592: step 5852, loss 0.249681, acc 0.953125, prec 0.0960984, recall 0.795184
2017-12-10T05:36:20.624811: step 5853, loss 0.0661447, acc 0.96875, prec 0.0960957, recall 0.795184
2017-12-10T05:36:20.892547: step 5854, loss 0.432342, acc 0.9375, prec 0.0960901, recall 0.795184
2017-12-10T05:36:21.157920: step 5855, loss 0.195121, acc 0.96875, prec 0.0960874, recall 0.795184
2017-12-10T05:36:21.421829: step 5856, loss 0.541868, acc 0.921875, prec 0.0960804, recall 0.795184
2017-12-10T05:36:21.688421: step 5857, loss 0.0623528, acc 0.96875, prec 0.0960777, recall 0.795184
2017-12-10T05:36:21.952476: step 5858, loss 0.429363, acc 0.890625, prec 0.096068, recall 0.795184
2017-12-10T05:36:22.220149: step 5859, loss 0.491008, acc 0.953125, prec 0.0960769, recall 0.795208
2017-12-10T05:36:22.490596: step 5860, loss 0.331883, acc 0.953125, prec 0.0960987, recall 0.795257
2017-12-10T05:36:22.763856: step 5861, loss 0.367252, acc 0.953125, prec 0.0960946, recall 0.795257
2017-12-10T05:36:23.029240: step 5862, loss 0.491766, acc 0.921875, prec 0.0961137, recall 0.795306
2017-12-10T05:36:23.308241: step 5863, loss 0.620522, acc 0.890625, prec 0.096104, recall 0.795306
2017-12-10T05:36:23.573354: step 5864, loss 0.598054, acc 0.921875, prec 0.0961361, recall 0.795379
2017-12-10T05:36:23.842593: step 5865, loss 0.439517, acc 0.96875, prec 0.0961334, recall 0.795379
2017-12-10T05:36:24.109697: step 5866, loss 0.0803868, acc 0.96875, prec 0.0961436, recall 0.795403
2017-12-10T05:36:24.375631: step 5867, loss 0.304923, acc 0.921875, prec 0.0961497, recall 0.795428
2017-12-10T05:36:24.649358: step 5868, loss 0.227298, acc 0.9375, prec 0.0961572, recall 0.795452
2017-12-10T05:36:25.612561: step 5869, loss 0.324912, acc 0.953125, prec 0.096192, recall 0.795525
2017-12-10T05:36:25.980842: step 5870, loss 0.0345426, acc 0.984375, prec 0.0961907, recall 0.795525
2017-12-10T05:36:26.624290: step 5871, loss 0.210595, acc 0.96875, prec 0.0962009, recall 0.795549
2017-12-10T05:36:27.716489: step 5872, loss 0.0736483, acc 0.953125, prec 0.0962097, recall 0.795574
2017-12-10T05:36:28.100827: step 5873, loss 3.96903, acc 0.953125, prec 0.096207, recall 0.795479
2017-12-10T05:36:28.396010: step 5874, loss 0.175314, acc 0.96875, prec 0.0962172, recall 0.795503
2017-12-10T05:36:28.685978: step 5875, loss 0.226305, acc 0.9375, prec 0.0962117, recall 0.795503
2017-12-10T05:36:28.977084: step 5876, loss 0.170923, acc 0.984375, prec 0.0962233, recall 0.795528
2017-12-10T05:36:29.268236: step 5877, loss 0.659134, acc 0.90625, prec 0.096228, recall 0.795552
2017-12-10T05:36:29.563715: step 5878, loss 0.563317, acc 0.890625, prec 0.0962313, recall 0.795576
2017-12-10T05:36:29.839684: step 5879, loss 0.256885, acc 0.890625, prec 0.0962216, recall 0.795576
2017-12-10T05:36:30.110181: step 5880, loss 0.278031, acc 0.9375, prec 0.0962291, recall 0.7956
2017-12-10T05:36:30.382100: step 5881, loss 0.547286, acc 0.953125, prec 0.0962379, recall 0.795625
2017-12-10T05:36:30.652571: step 5882, loss 0.463172, acc 0.96875, prec 0.0962611, recall 0.795673
2017-12-10T05:36:30.927081: step 5883, loss 0.298234, acc 0.890625, prec 0.0962774, recall 0.795722
2017-12-10T05:36:31.196785: step 5884, loss 0.553929, acc 0.890625, prec 0.0962678, recall 0.795722
2017-12-10T05:36:31.466742: step 5885, loss 0.257311, acc 0.953125, prec 0.0962636, recall 0.795722
2017-12-10T05:36:31.738721: step 5886, loss 0.16675, acc 0.9375, prec 0.0962581, recall 0.795722
2017-12-10T05:36:32.004191: step 5887, loss 1.01228, acc 0.90625, prec 0.0962757, recall 0.79577
2017-12-10T05:36:32.284888: step 5888, loss 0.147979, acc 0.921875, prec 0.0962818, recall 0.795795
2017-12-10T05:36:32.559124: step 5889, loss 0.684892, acc 0.890625, prec 0.0963111, recall 0.795868
2017-12-10T05:36:32.823509: step 5890, loss 0.183604, acc 0.9375, prec 0.0963056, recall 0.795868
2017-12-10T05:36:33.086584: step 5891, loss 0.243875, acc 0.96875, prec 0.0963417, recall 0.79594
2017-12-10T05:36:33.349683: step 5892, loss 0.119114, acc 0.96875, prec 0.0963649, recall 0.795989
2017-12-10T05:36:33.624016: step 5893, loss 0.0325587, acc 0.984375, prec 0.0964025, recall 0.796061
2017-12-10T05:36:33.893870: step 5894, loss 0.0746242, acc 0.953125, prec 0.0963984, recall 0.796061
2017-12-10T05:36:34.171988: step 5895, loss 0.249873, acc 0.953125, prec 0.0964072, recall 0.796085
2017-12-10T05:36:34.446918: step 5896, loss 0.0984683, acc 0.96875, prec 0.0964044, recall 0.796085
2017-12-10T05:36:34.721486: step 5897, loss 0.667503, acc 0.921875, prec 0.0964105, recall 0.79611
2017-12-10T05:36:34.992437: step 5898, loss 0.462387, acc 0.984375, prec 0.0964091, recall 0.79611
2017-12-10T05:36:35.261298: step 5899, loss 0.115736, acc 0.984375, prec 0.0964207, recall 0.796134
2017-12-10T05:36:35.534261: step 5900, loss 0.429353, acc 0.984375, prec 0.0964452, recall 0.796182
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-5900

2017-12-10T05:36:36.834400: step 5901, loss 0.181414, acc 0.96875, prec 0.0964554, recall 0.796206
2017-12-10T05:36:37.104107: step 5902, loss 0.00861919, acc 1, prec 0.0964814, recall 0.796255
2017-12-10T05:36:37.375631: step 5903, loss 0.0084078, acc 1, prec 0.0965463, recall 0.796375
2017-12-10T05:36:37.638502: step 5904, loss 0.45468, acc 0.96875, prec 0.0965694, recall 0.796423
2017-12-10T05:36:37.905363: step 5905, loss 0.870271, acc 0.984375, prec 0.096581, recall 0.796448
2017-12-10T05:36:38.182390: step 5906, loss 0.00714479, acc 1, prec 0.096581, recall 0.796448
2017-12-10T05:36:38.444993: step 5907, loss 3.23115, acc 0.9375, prec 0.0965898, recall 0.796377
2017-12-10T05:36:38.713649: step 5908, loss 0.413044, acc 0.921875, prec 0.0966089, recall 0.796426
2017-12-10T05:36:38.980184: step 5909, loss 0.342074, acc 0.9375, prec 0.0966292, recall 0.796474
2017-12-10T05:36:39.245695: step 5910, loss 0.408671, acc 0.90625, prec 0.0966209, recall 0.796474
2017-12-10T05:36:39.510492: step 5911, loss 0.908232, acc 0.859375, prec 0.0966214, recall 0.796498
2017-12-10T05:36:39.784687: step 5912, loss 0.698661, acc 0.859375, prec 0.0966089, recall 0.796498
2017-12-10T05:36:40.049458: step 5913, loss 0.7398, acc 0.8125, prec 0.0966182, recall 0.796546
2017-12-10T05:36:40.326088: step 5914, loss 0.273938, acc 0.921875, prec 0.0966113, recall 0.796546
2017-12-10T05:36:40.595246: step 5915, loss 0.475235, acc 0.890625, prec 0.0966405, recall 0.796618
2017-12-10T05:36:40.859262: step 5916, loss 0.303347, acc 0.890625, prec 0.0966308, recall 0.796618
2017-12-10T05:36:41.126556: step 5917, loss 0.114398, acc 0.953125, prec 0.0966266, recall 0.796618
2017-12-10T05:36:41.389102: step 5918, loss 0.783386, acc 0.828125, prec 0.0966114, recall 0.796618
2017-12-10T05:36:41.654320: step 5919, loss 0.418868, acc 0.859375, prec 0.0965989, recall 0.796618
2017-12-10T05:36:41.919537: step 5920, loss 0.305413, acc 0.9375, prec 0.0966193, recall 0.796666
2017-12-10T05:36:42.191795: step 5921, loss 0.485267, acc 0.9375, prec 0.0966267, recall 0.79669
2017-12-10T05:36:42.480895: step 5922, loss 0.225986, acc 0.9375, prec 0.0966211, recall 0.79669
2017-12-10T05:36:42.753466: step 5923, loss 0.187876, acc 0.9375, prec 0.0966285, recall 0.796714
2017-12-10T05:36:43.018549: step 5924, loss 0.200152, acc 0.9375, prec 0.096623, recall 0.796714
2017-12-10T05:36:43.291832: step 5925, loss 0.284827, acc 0.953125, prec 0.0966447, recall 0.796762
2017-12-10T05:36:43.563726: step 5926, loss 0.271911, acc 0.9375, prec 0.0966521, recall 0.796786
2017-12-10T05:36:43.829155: step 5927, loss 0.203128, acc 0.953125, prec 0.0966609, recall 0.79681
2017-12-10T05:36:44.099042: step 5928, loss 0.103133, acc 0.96875, prec 0.0966582, recall 0.79681
2017-12-10T05:36:44.372169: step 5929, loss 0.0238089, acc 0.984375, prec 0.0966697, recall 0.796834
2017-12-10T05:36:44.639805: step 5930, loss 0.170058, acc 0.96875, prec 0.0967058, recall 0.796906
2017-12-10T05:36:44.916814: step 5931, loss 0.253956, acc 0.9375, prec 0.0967391, recall 0.796978
2017-12-10T05:36:45.188174: step 5932, loss 0.122207, acc 0.984375, prec 0.0967377, recall 0.796978
2017-12-10T05:36:45.451767: step 5933, loss 0.00811895, acc 1, prec 0.0967506, recall 0.797002
2017-12-10T05:36:45.720472: step 5934, loss 0.164636, acc 0.984375, prec 0.0967881, recall 0.797074
2017-12-10T05:36:45.983598: step 5935, loss 0.0111546, acc 1, prec 0.0967881, recall 0.797074
2017-12-10T05:36:46.250025: step 5936, loss 0.558628, acc 0.96875, prec 0.0968112, recall 0.797122
2017-12-10T05:36:46.519098: step 5937, loss 0.055548, acc 0.984375, prec 0.0968227, recall 0.797146
2017-12-10T05:36:46.788344: step 5938, loss 0.0119207, acc 1, prec 0.0968227, recall 0.797146
2017-12-10T05:36:47.057053: step 5939, loss 0.0184368, acc 0.984375, prec 0.0968213, recall 0.797146
2017-12-10T05:36:47.320243: step 5940, loss 0.00674132, acc 1, prec 0.0968472, recall 0.797194
2017-12-10T05:36:47.587336: step 5941, loss 0.296019, acc 0.96875, prec 0.0968574, recall 0.797218
2017-12-10T05:36:47.856694: step 5942, loss 0.0321416, acc 0.984375, prec 0.0968689, recall 0.797242
2017-12-10T05:36:48.130783: step 5943, loss 0.0642148, acc 0.984375, prec 0.0968805, recall 0.797265
2017-12-10T05:36:48.403252: step 5944, loss 5.48828, acc 0.984375, prec 0.0968805, recall 0.797171
2017-12-10T05:36:48.677849: step 5945, loss 0.0522429, acc 0.984375, prec 0.0968791, recall 0.797171
2017-12-10T05:36:48.942991: step 5946, loss 0.412868, acc 0.953125, prec 0.0968878, recall 0.797195
2017-12-10T05:36:49.209386: step 5947, loss 0.171771, acc 0.96875, prec 0.096898, recall 0.797219
2017-12-10T05:36:49.478857: step 5948, loss 0.155553, acc 0.96875, prec 0.0969082, recall 0.797243
2017-12-10T05:36:49.742782: step 5949, loss 0.105356, acc 0.984375, prec 0.0969197, recall 0.797267
2017-12-10T05:36:50.008068: step 5950, loss 0.180861, acc 0.984375, prec 0.0969183, recall 0.797267
2017-12-10T05:36:50.277261: step 5951, loss 0.0412823, acc 0.984375, prec 0.0969299, recall 0.797291
2017-12-10T05:36:50.553988: step 5952, loss 0.370418, acc 0.953125, prec 0.0969257, recall 0.797291
2017-12-10T05:36:50.822982: step 5953, loss 0.380838, acc 0.9375, prec 0.096946, recall 0.797339
2017-12-10T05:36:51.090457: step 5954, loss 0.773622, acc 0.921875, prec 0.096952, recall 0.797363
2017-12-10T05:36:51.359061: step 5955, loss 0.207139, acc 0.984375, prec 0.0969635, recall 0.797386
2017-12-10T05:36:51.632114: step 5956, loss 0.207216, acc 0.953125, prec 0.0969723, recall 0.79741
2017-12-10T05:36:51.898280: step 5957, loss 0.469329, acc 0.921875, prec 0.0969783, recall 0.797434
2017-12-10T05:36:52.165189: step 5958, loss 0.82508, acc 0.96875, prec 0.0969884, recall 0.797458
2017-12-10T05:36:52.433970: step 5959, loss 0.494417, acc 0.90625, prec 0.096993, recall 0.797482
2017-12-10T05:36:52.700488: step 5960, loss 0.29819, acc 0.921875, prec 0.0970119, recall 0.797529
2017-12-10T05:36:52.971596: step 5961, loss 0.217746, acc 0.890625, prec 0.0970022, recall 0.797529
2017-12-10T05:36:53.238509: step 5962, loss 0.415849, acc 0.921875, prec 0.0969953, recall 0.797529
2017-12-10T05:36:53.510769: step 5963, loss 0.380772, acc 0.90625, prec 0.0969999, recall 0.797553
2017-12-10T05:36:53.746985: step 5964, loss 0.108647, acc 0.961538, prec 0.0969971, recall 0.797553
2017-12-10T05:36:54.019791: step 5965, loss 0.107969, acc 0.9375, prec 0.0970174, recall 0.797601
2017-12-10T05:36:54.282715: step 5966, loss 0.183974, acc 0.9375, prec 0.0970118, recall 0.797601
2017-12-10T05:36:54.548811: step 5967, loss 0.058161, acc 0.953125, prec 0.0970206, recall 0.797625
2017-12-10T05:36:54.815763: step 5968, loss 0.11338, acc 0.96875, prec 0.0970307, recall 0.797648
2017-12-10T05:36:55.088194: step 5969, loss 0.764625, acc 0.921875, prec 0.0970625, recall 0.79772
2017-12-10T05:36:55.357961: step 5970, loss 0.368418, acc 0.890625, prec 0.0970528, recall 0.79772
2017-12-10T05:36:55.625022: step 5971, loss 0.0959858, acc 0.953125, prec 0.0970486, recall 0.79772
2017-12-10T05:36:55.889735: step 5972, loss 0.0909684, acc 0.96875, prec 0.0970846, recall 0.797791
2017-12-10T05:36:56.156557: step 5973, loss 0.0505328, acc 0.953125, prec 0.0971063, recall 0.797839
2017-12-10T05:36:56.419365: step 5974, loss 0.426819, acc 0.90625, prec 0.0970979, recall 0.797839
2017-12-10T05:36:56.680744: step 5975, loss 0.0714308, acc 0.953125, prec 0.0971196, recall 0.797886
2017-12-10T05:36:56.952951: step 5976, loss 0.120329, acc 0.9375, prec 0.097114, recall 0.797886
2017-12-10T05:36:57.232911: step 5977, loss 0.336248, acc 0.9375, prec 0.0971214, recall 0.79791
2017-12-10T05:36:57.503151: step 5978, loss 0.0295624, acc 1, prec 0.0971343, recall 0.797934
2017-12-10T05:36:57.770759: step 5979, loss 0.0357032, acc 0.96875, prec 0.0971444, recall 0.797957
2017-12-10T05:36:58.036232: step 5980, loss 0.247294, acc 0.984375, prec 0.0971688, recall 0.798005
2017-12-10T05:36:58.306682: step 5981, loss 0.000471678, acc 1, prec 0.0971817, recall 0.798028
2017-12-10T05:36:58.573666: step 5982, loss 0.168758, acc 0.96875, prec 0.097179, recall 0.798028
2017-12-10T05:36:58.843593: step 5983, loss 0.00296208, acc 1, prec 0.0972048, recall 0.798076
2017-12-10T05:36:59.115448: step 5984, loss 7.30877, acc 0.953125, prec 0.0972149, recall 0.798006
2017-12-10T05:36:59.395179: step 5985, loss 0.16445, acc 0.984375, prec 0.0972264, recall 0.79803
2017-12-10T05:36:59.674001: step 5986, loss 0.0557144, acc 0.96875, prec 0.0972365, recall 0.798053
2017-12-10T05:36:59.942146: step 5987, loss 0.0749591, acc 0.984375, prec 0.097248, recall 0.798077
2017-12-10T05:37:00.209791: step 5988, loss 0.405353, acc 0.90625, prec 0.0972397, recall 0.798077
2017-12-10T05:37:00.481262: step 5989, loss 1.98679, acc 0.921875, prec 0.0972599, recall 0.798031
2017-12-10T05:37:00.757074: step 5990, loss 0.265232, acc 0.9375, prec 0.0972673, recall 0.798054
2017-12-10T05:37:01.032139: step 5991, loss 0.561303, acc 0.859375, prec 0.0972548, recall 0.798054
2017-12-10T05:37:01.298782: step 5992, loss 0.718573, acc 0.875, prec 0.0972436, recall 0.798054
2017-12-10T05:37:01.569537: step 5993, loss 0.802065, acc 0.796875, prec 0.0972256, recall 0.798054
2017-12-10T05:37:01.840635: step 5994, loss 1.26176, acc 0.78125, prec 0.0972319, recall 0.798102
2017-12-10T05:37:02.106737: step 5995, loss 1.27546, acc 0.78125, prec 0.0972254, recall 0.798125
2017-12-10T05:37:02.385234: step 5996, loss 1.07845, acc 0.765625, prec 0.0972046, recall 0.798125
2017-12-10T05:37:02.656839: step 5997, loss 1.31098, acc 0.78125, prec 0.0972109, recall 0.798173
2017-12-10T05:37:02.926893: step 5998, loss 1.02257, acc 0.84375, prec 0.0971971, recall 0.798173
2017-12-10T05:37:03.203554: step 5999, loss 0.925003, acc 0.859375, prec 0.0971975, recall 0.798196
2017-12-10T05:37:03.466349: step 6000, loss 1.51684, acc 0.8125, prec 0.0971937, recall 0.79822

Evaluation:
2017-12-10T05:37:11.070389: step 6000, loss 3.061, acc 0.865258, prec 0.0966359, recall 0.795951

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-6000

2017-12-10T05:37:12.413759: step 6001, loss 0.589176, acc 0.828125, prec 0.0966463, recall 0.795998
2017-12-10T05:37:12.683855: step 6002, loss 0.418708, acc 0.90625, prec 0.0966634, recall 0.796045
2017-12-10T05:37:12.953507: step 6003, loss 0.592735, acc 0.921875, prec 0.0966693, recall 0.796068
2017-12-10T05:37:13.225450: step 6004, loss 0.11868, acc 0.96875, prec 0.0966918, recall 0.796115
2017-12-10T05:37:13.492071: step 6005, loss 0.35333, acc 0.890625, prec 0.0966823, recall 0.796115
2017-12-10T05:37:13.758207: step 6006, loss 0.0439606, acc 0.984375, prec 0.0967062, recall 0.796162
2017-12-10T05:37:14.033950: step 6007, loss 0.633231, acc 0.9375, prec 0.0967008, recall 0.796162
2017-12-10T05:37:14.298981: step 6008, loss 0.23384, acc 0.921875, prec 0.0967067, recall 0.796186
2017-12-10T05:37:14.564094: step 6009, loss 0.112993, acc 0.953125, prec 0.0967026, recall 0.796186
2017-12-10T05:37:14.840253: step 6010, loss 0.120837, acc 0.953125, prec 0.0966986, recall 0.796186
2017-12-10T05:37:15.109115: step 6011, loss 0.195833, acc 0.984375, prec 0.0967098, recall 0.796209
2017-12-10T05:37:15.371014: step 6012, loss 0.217961, acc 0.984375, prec 0.0967211, recall 0.796232
2017-12-10T05:37:15.637868: step 6013, loss 0.27821, acc 0.953125, prec 0.0967296, recall 0.796256
2017-12-10T05:37:15.907752: step 6014, loss 0.132739, acc 0.96875, prec 0.0967395, recall 0.796279
2017-12-10T05:37:16.175968: step 6015, loss 0.0131927, acc 0.984375, prec 0.0967508, recall 0.796303
2017-12-10T05:37:16.440309: step 6016, loss 0.0107986, acc 1, prec 0.0967634, recall 0.796326
2017-12-10T05:37:16.706919: step 6017, loss 0.275214, acc 0.984375, prec 0.0967746, recall 0.796349
2017-12-10T05:37:16.975833: step 6018, loss 0.908982, acc 0.96875, prec 0.0967845, recall 0.796373
2017-12-10T05:37:17.247472: step 6019, loss 0.495421, acc 0.96875, prec 0.096807, recall 0.79642
2017-12-10T05:37:17.524444: step 6020, loss 0.000718606, acc 1, prec 0.096807, recall 0.79642
2017-12-10T05:37:17.787237: step 6021, loss 0.0280675, acc 1, prec 0.0968574, recall 0.796513
2017-12-10T05:37:18.055414: step 6022, loss 0.106052, acc 0.984375, prec 0.0968687, recall 0.796536
2017-12-10T05:37:18.326646: step 6023, loss 0.010267, acc 1, prec 0.0968939, recall 0.796583
2017-12-10T05:37:18.599581: step 6024, loss 0.0364793, acc 0.984375, prec 0.0968925, recall 0.796583
2017-12-10T05:37:18.864382: step 6025, loss 0.000516393, acc 1, prec 0.0969051, recall 0.796606
2017-12-10T05:37:19.128945: step 6026, loss 0.0370939, acc 0.984375, prec 0.0969164, recall 0.79663
2017-12-10T05:37:19.397035: step 6027, loss 0.168054, acc 0.984375, prec 0.0969402, recall 0.796676
2017-12-10T05:37:19.665758: step 6028, loss 0.0478446, acc 0.984375, prec 0.0969514, recall 0.7967
2017-12-10T05:37:19.937025: step 6029, loss 0.111592, acc 0.984375, prec 0.0969501, recall 0.7967
2017-12-10T05:37:20.210400: step 6030, loss 0.611092, acc 0.96875, prec 0.09696, recall 0.796723
2017-12-10T05:37:20.485494: step 6031, loss 3.67994, acc 0.984375, prec 0.0969726, recall 0.796655
2017-12-10T05:37:20.756761: step 6032, loss 0.244308, acc 0.96875, prec 0.0969825, recall 0.796678
2017-12-10T05:37:21.026527: step 6033, loss 0.0033213, acc 1, prec 0.0969825, recall 0.796678
2017-12-10T05:37:21.290084: step 6034, loss 0.208492, acc 0.953125, prec 0.0970036, recall 0.796725
2017-12-10T05:37:21.563762: step 6035, loss 0.170521, acc 0.953125, prec 0.0970121, recall 0.796748
2017-12-10T05:37:21.830841: step 6036, loss 0.270418, acc 0.96875, prec 0.097022, recall 0.796771
2017-12-10T05:37:22.098411: step 6037, loss 0.0724851, acc 0.953125, prec 0.0970305, recall 0.796795
2017-12-10T05:37:22.366745: step 6038, loss 0.75236, acc 0.796875, prec 0.097013, recall 0.796795
2017-12-10T05:37:22.631742: step 6039, loss 0.540189, acc 0.859375, prec 0.0970008, recall 0.796795
2017-12-10T05:37:22.898203: step 6040, loss 0.63112, acc 0.890625, prec 0.0969913, recall 0.796795
2017-12-10T05:37:23.167958: step 6041, loss 0.601485, acc 0.875, prec 0.0969805, recall 0.796795
2017-12-10T05:37:23.440497: step 6042, loss 0.359859, acc 0.875, prec 0.0969697, recall 0.796795
2017-12-10T05:37:23.705454: step 6043, loss 0.528316, acc 0.890625, prec 0.0969728, recall 0.796818
2017-12-10T05:37:23.973337: step 6044, loss 0.844854, acc 0.875, prec 0.0969746, recall 0.796841
2017-12-10T05:37:24.246993: step 6045, loss 0.415983, acc 0.890625, prec 0.0969903, recall 0.796888
2017-12-10T05:37:24.513496: step 6046, loss 0.34025, acc 0.9375, prec 0.09701, recall 0.796934
2017-12-10T05:37:24.778584: step 6047, loss 0.2868, acc 0.921875, prec 0.0970033, recall 0.796934
2017-12-10T05:37:25.052000: step 6048, loss 0.670714, acc 0.90625, prec 0.0970078, recall 0.796957
2017-12-10T05:37:25.316660: step 6049, loss 0.365351, acc 0.953125, prec 0.0970414, recall 0.797027
2017-12-10T05:37:25.599389: step 6050, loss 0.419036, acc 0.9375, prec 0.0970486, recall 0.79705
2017-12-10T05:37:25.862349: step 6051, loss 0.173185, acc 0.953125, prec 0.0970445, recall 0.79705
2017-12-10T05:37:26.129864: step 6052, loss 0.416029, acc 0.953125, prec 0.0970656, recall 0.797096
2017-12-10T05:37:26.399524: step 6053, loss 0.546983, acc 0.953125, prec 0.0970616, recall 0.797096
2017-12-10T05:37:26.666323: step 6054, loss 0.426874, acc 0.953125, prec 0.0970701, recall 0.79712
2017-12-10T05:37:26.928885: step 6055, loss 0.246878, acc 0.96875, prec 0.0970925, recall 0.797166
2017-12-10T05:37:27.198899: step 6056, loss 0.199495, acc 0.953125, prec 0.0970885, recall 0.797166
2017-12-10T05:37:27.474985: step 6057, loss 0.309542, acc 0.953125, prec 0.0970844, recall 0.797166
2017-12-10T05:37:27.742222: step 6058, loss 0.0855396, acc 0.984375, prec 0.0970831, recall 0.797166
2017-12-10T05:37:28.008503: step 6059, loss 0.712413, acc 0.921875, prec 0.0971014, recall 0.797212
2017-12-10T05:37:28.281364: step 6060, loss 0.00561915, acc 1, prec 0.097114, recall 0.797236
2017-12-10T05:37:28.543984: step 6061, loss 0.00398781, acc 1, prec 0.097114, recall 0.797236
2017-12-10T05:37:28.812188: step 6062, loss 0.0926714, acc 0.984375, prec 0.0971126, recall 0.797236
2017-12-10T05:37:29.077980: step 6063, loss 0.168379, acc 0.984375, prec 0.0971113, recall 0.797236
2017-12-10T05:37:29.344945: step 6064, loss 0.192884, acc 0.9375, prec 0.0971184, recall 0.797259
2017-12-10T05:37:29.608186: step 6065, loss 0.0474173, acc 0.96875, prec 0.0971283, recall 0.797282
2017-12-10T05:37:29.880649: step 6066, loss 0.000287278, acc 1, prec 0.0971283, recall 0.797282
2017-12-10T05:37:30.153531: step 6067, loss 0.157645, acc 0.96875, prec 0.0971507, recall 0.797328
2017-12-10T05:37:30.427532: step 6068, loss 0.269072, acc 0.953125, prec 0.0971467, recall 0.797328
2017-12-10T05:37:30.707584: step 6069, loss 0.0787695, acc 0.984375, prec 0.0971579, recall 0.797351
2017-12-10T05:37:30.974968: step 6070, loss 0.0471108, acc 0.984375, prec 0.0971691, recall 0.797374
2017-12-10T05:37:31.242115: step 6071, loss 0.00898837, acc 1, prec 0.0971816, recall 0.797398
2017-12-10T05:37:31.507648: step 6072, loss 0.104938, acc 0.984375, prec 0.0971803, recall 0.797398
2017-12-10T05:37:31.788155: step 6073, loss 0.0500704, acc 0.96875, prec 0.0972027, recall 0.797444
2017-12-10T05:37:32.059574: step 6074, loss 0.211, acc 0.984375, prec 0.0972139, recall 0.797467
2017-12-10T05:37:32.336304: step 6075, loss 0.0215658, acc 0.984375, prec 0.0972126, recall 0.797467
2017-12-10T05:37:32.603319: step 6076, loss 0.00141203, acc 1, prec 0.0972126, recall 0.797467
2017-12-10T05:37:32.868815: step 6077, loss 0.0807085, acc 0.96875, prec 0.097235, recall 0.797513
2017-12-10T05:37:33.138643: step 6078, loss 10.0959, acc 0.984375, prec 0.097235, recall 0.797422
2017-12-10T05:37:33.417198: step 6079, loss 0.00715676, acc 1, prec 0.0972475, recall 0.797445
2017-12-10T05:37:33.688053: step 6080, loss 0.238926, acc 0.984375, prec 0.0972462, recall 0.797445
2017-12-10T05:37:33.951584: step 6081, loss 0.0110821, acc 1, prec 0.0972462, recall 0.797445
2017-12-10T05:37:34.217167: step 6082, loss 1.73073, acc 0.984375, prec 0.0972713, recall 0.797401
2017-12-10T05:37:34.494224: step 6083, loss 0.278211, acc 0.984375, prec 0.0972699, recall 0.797401
2017-12-10T05:37:34.767661: step 6084, loss 0.0592633, acc 0.96875, prec 0.0972672, recall 0.797401
2017-12-10T05:37:35.043584: step 6085, loss 0.142414, acc 0.984375, prec 0.0972784, recall 0.797424
2017-12-10T05:37:35.310245: step 6086, loss 0.176426, acc 0.953125, prec 0.0972869, recall 0.797447
2017-12-10T05:37:35.588293: step 6087, loss 0.282304, acc 0.953125, prec 0.097308, recall 0.797493
2017-12-10T05:37:35.856326: step 6088, loss 0.215172, acc 0.921875, prec 0.0973012, recall 0.797493
2017-12-10T05:37:36.127664: step 6089, loss 0.737073, acc 0.859375, prec 0.0973016, recall 0.797516
2017-12-10T05:37:36.393436: step 6090, loss 1.039, acc 0.8125, prec 0.0972979, recall 0.797539
2017-12-10T05:37:36.664769: step 6091, loss 0.448176, acc 0.875, prec 0.0973247, recall 0.797608
2017-12-10T05:37:36.939739: step 6092, loss 0.400275, acc 0.9375, prec 0.0973193, recall 0.797608
2017-12-10T05:37:37.203618: step 6093, loss 0.283128, acc 0.921875, prec 0.0973251, recall 0.797631
2017-12-10T05:37:37.470691: step 6094, loss 0.477284, acc 0.921875, prec 0.0973309, recall 0.797654
2017-12-10T05:37:37.744523: step 6095, loss 0.26306, acc 0.921875, prec 0.0973366, recall 0.797677
2017-12-10T05:37:38.011892: step 6096, loss 0.994453, acc 0.859375, prec 0.0973245, recall 0.797677
2017-12-10T05:37:38.297990: step 6097, loss 0.324308, acc 0.90625, prec 0.0973289, recall 0.7977
2017-12-10T05:37:38.565158: step 6098, loss 0.692382, acc 0.90625, prec 0.0973459, recall 0.797746
2017-12-10T05:37:38.839840: step 6099, loss 0.499268, acc 0.875, prec 0.0973727, recall 0.797815
2017-12-10T05:37:39.107818: step 6100, loss 0.618549, acc 0.921875, prec 0.0973784, recall 0.797838
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-6100

2017-12-10T05:37:40.493683: step 6101, loss 0.618942, acc 0.859375, prec 0.0973913, recall 0.797884
2017-12-10T05:37:40.762419: step 6102, loss 0.115588, acc 0.96875, prec 0.0974137, recall 0.79793
2017-12-10T05:37:41.033571: step 6103, loss 1.1939, acc 0.90625, prec 0.0974056, recall 0.79793
2017-12-10T05:37:41.300372: step 6104, loss 1.44467, acc 0.953125, prec 0.0974029, recall 0.79784
2017-12-10T05:37:41.570878: step 6105, loss 0.0905571, acc 0.96875, prec 0.0974127, recall 0.797863
2017-12-10T05:37:41.838242: step 6106, loss 0.547644, acc 0.90625, prec 0.0974171, recall 0.797886
2017-12-10T05:37:42.113141: step 6107, loss 0.290984, acc 0.96875, prec 0.0974395, recall 0.797932
2017-12-10T05:37:42.391516: step 6108, loss 0.293496, acc 0.984375, prec 0.0974381, recall 0.797932
2017-12-10T05:37:42.658144: step 6109, loss 0.109641, acc 0.953125, prec 0.0974466, recall 0.797955
2017-12-10T05:37:42.934305: step 6110, loss 0.218608, acc 0.953125, prec 0.097455, recall 0.797978
2017-12-10T05:37:43.206158: step 6111, loss 0.100033, acc 0.9375, prec 0.0974496, recall 0.797978
2017-12-10T05:37:43.473844: step 6112, loss 0.0277374, acc 0.984375, prec 0.0974608, recall 0.798
2017-12-10T05:37:43.739348: step 6113, loss 0.130647, acc 0.953125, prec 0.0974693, recall 0.798023
2017-12-10T05:37:44.008400: step 6114, loss 0.0933334, acc 0.96875, prec 0.0974916, recall 0.798069
2017-12-10T05:37:44.275284: step 6115, loss 0.0760674, acc 0.96875, prec 0.0975014, recall 0.798092
2017-12-10T05:37:44.540775: step 6116, loss 0.208702, acc 0.9375, prec 0.097496, recall 0.798092
2017-12-10T05:37:44.814068: step 6117, loss 0.401734, acc 0.984375, prec 0.0974947, recall 0.798092
2017-12-10T05:37:45.079026: step 6118, loss 0.191726, acc 0.96875, prec 0.0975045, recall 0.798115
2017-12-10T05:37:45.343412: step 6119, loss 0.433276, acc 0.953125, prec 0.0975129, recall 0.798138
2017-12-10T05:37:45.615007: step 6120, loss 0.0973084, acc 0.9375, prec 0.0975075, recall 0.798138
2017-12-10T05:37:45.881459: step 6121, loss 0.877331, acc 0.921875, prec 0.0975133, recall 0.798161
2017-12-10T05:37:46.145043: step 6122, loss 0.0782214, acc 0.96875, prec 0.0975356, recall 0.798207
2017-12-10T05:37:46.422203: step 6123, loss 0.246341, acc 0.9375, prec 0.0975302, recall 0.798207
2017-12-10T05:37:46.695863: step 6124, loss 0.00574634, acc 1, prec 0.0975427, recall 0.79823
2017-12-10T05:37:46.964979: step 6125, loss 0.424114, acc 0.9375, prec 0.0975373, recall 0.79823
2017-12-10T05:37:47.230834: step 6126, loss 0.167101, acc 0.96875, prec 0.0975471, recall 0.798253
2017-12-10T05:37:47.496170: step 6127, loss 0.0176484, acc 1, prec 0.0975596, recall 0.798275
2017-12-10T05:37:47.770992: step 6128, loss 0.0695065, acc 1, prec 0.0975846, recall 0.798321
2017-12-10T05:37:48.036091: step 6129, loss 0.0293842, acc 0.984375, prec 0.0975833, recall 0.798321
2017-12-10T05:37:48.314674: step 6130, loss 0.0132633, acc 1, prec 0.0975958, recall 0.798344
2017-12-10T05:37:48.579030: step 6131, loss 0.000343136, acc 1, prec 0.0975958, recall 0.798344
2017-12-10T05:37:48.835864: step 6132, loss 0.00293573, acc 1, prec 0.0976208, recall 0.79839
2017-12-10T05:37:49.103141: step 6133, loss 0.0874127, acc 0.984375, prec 0.0976195, recall 0.79839
2017-12-10T05:37:49.369807: step 6134, loss 0.00536307, acc 1, prec 0.0976195, recall 0.79839
2017-12-10T05:37:49.635233: step 6135, loss 0.0642802, acc 0.984375, prec 0.0976181, recall 0.79839
2017-12-10T05:37:49.906650: step 6136, loss 0.204374, acc 0.984375, prec 0.0976293, recall 0.798413
2017-12-10T05:37:50.172645: step 6137, loss 7.29474, acc 0.96875, prec 0.0976279, recall 0.798322
2017-12-10T05:37:50.440051: step 6138, loss 0.623521, acc 0.953125, prec 0.0976239, recall 0.798322
2017-12-10T05:37:50.701777: step 6139, loss 0.162857, acc 0.96875, prec 0.0976212, recall 0.798322
2017-12-10T05:37:50.970433: step 6140, loss 0.0391016, acc 0.984375, prec 0.0976323, recall 0.798345
2017-12-10T05:37:51.241706: step 6141, loss 0.398865, acc 0.953125, prec 0.0976408, recall 0.798368
2017-12-10T05:37:51.517816: step 6142, loss 0.179029, acc 0.96875, prec 0.0976506, recall 0.798391
2017-12-10T05:37:51.797821: step 6143, loss 0.129824, acc 0.96875, prec 0.0976479, recall 0.798391
2017-12-10T05:37:52.066135: step 6144, loss 0.264116, acc 0.953125, prec 0.0976688, recall 0.798436
2017-12-10T05:37:52.343306: step 6145, loss 0.0411468, acc 0.984375, prec 0.09768, recall 0.798459
2017-12-10T05:37:52.614928: step 6146, loss 0.216549, acc 0.96875, prec 0.0976773, recall 0.798459
2017-12-10T05:37:52.883644: step 6147, loss 0.311172, acc 0.953125, prec 0.0976857, recall 0.798482
2017-12-10T05:37:53.156308: step 6148, loss 0.512259, acc 0.9375, prec 0.0976803, recall 0.798482
2017-12-10T05:37:53.423248: step 6149, loss 2.53264, acc 0.90625, prec 0.0976985, recall 0.798437
2017-12-10T05:37:53.691557: step 6150, loss 0.380484, acc 0.90625, prec 0.0977029, recall 0.79846
2017-12-10T05:37:53.960311: step 6151, loss 0.604716, acc 0.890625, prec 0.0976934, recall 0.79846
2017-12-10T05:37:54.226919: step 6152, loss 0.64602, acc 0.8125, prec 0.0976897, recall 0.798483
2017-12-10T05:37:54.496465: step 6153, loss 0.395937, acc 0.859375, prec 0.097715, recall 0.798551
2017-12-10T05:37:54.763770: step 6154, loss 0.418918, acc 0.90625, prec 0.0977069, recall 0.798551
2017-12-10T05:37:55.036798: step 6155, loss 0.807985, acc 0.859375, prec 0.0977072, recall 0.798574
2017-12-10T05:37:55.311082: step 6156, loss 1.15943, acc 0.8125, prec 0.097691, recall 0.798574
2017-12-10T05:37:55.580902: step 6157, loss 0.785413, acc 0.84375, prec 0.0976774, recall 0.798574
2017-12-10T05:37:55.849277: step 6158, loss 0.412064, acc 0.90625, prec 0.0976818, recall 0.798597
2017-12-10T05:37:56.120798: step 6159, loss 0.65148, acc 0.875, prec 0.097671, recall 0.798597
2017-12-10T05:37:56.393700: step 6160, loss 1.40519, acc 0.78125, prec 0.0976521, recall 0.798597
2017-12-10T05:37:56.666093: step 6161, loss 0.56938, acc 0.890625, prec 0.0976551, recall 0.79862
2017-12-10T05:37:56.935914: step 6162, loss 0.814901, acc 0.8125, prec 0.0976389, recall 0.79862
2017-12-10T05:37:57.205436: step 6163, loss 0.295267, acc 0.90625, prec 0.0976433, recall 0.798643
2017-12-10T05:37:57.479395: step 6164, loss 0.556474, acc 0.90625, prec 0.0976601, recall 0.798688
2017-12-10T05:37:57.749445: step 6165, loss 0.256934, acc 0.953125, prec 0.0976561, recall 0.798688
2017-12-10T05:37:58.022937: step 6166, loss 0.339963, acc 0.9375, prec 0.0976632, recall 0.798711
2017-12-10T05:37:58.292380: step 6167, loss 0.0310899, acc 0.984375, prec 0.0976618, recall 0.798711
2017-12-10T05:37:58.557741: step 6168, loss 0.0410899, acc 0.984375, prec 0.0976605, recall 0.798711
2017-12-10T05:37:58.830295: step 6169, loss 0.0343917, acc 0.984375, prec 0.0976591, recall 0.798711
2017-12-10T05:37:59.095334: step 6170, loss 0.484975, acc 0.9375, prec 0.0976662, recall 0.798734
2017-12-10T05:37:59.361573: step 6171, loss 0.00862261, acc 1, prec 0.0976662, recall 0.798734
2017-12-10T05:37:59.633606: step 6172, loss 0.289599, acc 0.953125, prec 0.0976746, recall 0.798756
2017-12-10T05:37:59.899098: step 6173, loss 0.0574947, acc 0.984375, prec 0.0976733, recall 0.798756
2017-12-10T05:38:00.172716: step 6174, loss 0.0638444, acc 0.984375, prec 0.0976844, recall 0.798779
2017-12-10T05:38:00.461717: step 6175, loss 1.4662, acc 0.9375, prec 0.0977039, recall 0.798825
2017-12-10T05:38:00.746530: step 6176, loss 0.268902, acc 0.984375, prec 0.097715, recall 0.798847
2017-12-10T05:38:01.017808: step 6177, loss 0.00761005, acc 1, prec 0.097715, recall 0.798847
2017-12-10T05:38:01.292442: step 6178, loss 0.439051, acc 0.9375, prec 0.0977096, recall 0.798847
2017-12-10T05:38:01.562553: step 6179, loss 0.000738033, acc 1, prec 0.0977096, recall 0.798847
2017-12-10T05:38:01.833632: step 6180, loss 0.0187972, acc 0.984375, prec 0.0977208, recall 0.79887
2017-12-10T05:38:02.102624: step 6181, loss 0.00047009, acc 1, prec 0.0977332, recall 0.798893
2017-12-10T05:38:02.376761: step 6182, loss 0.03157, acc 0.984375, prec 0.0977568, recall 0.798938
2017-12-10T05:38:02.646332: step 6183, loss 0.0024147, acc 1, prec 0.0977568, recall 0.798938
2017-12-10T05:38:02.910541: step 6184, loss 2.79307, acc 0.96875, prec 0.0977679, recall 0.798871
2017-12-10T05:38:03.183444: step 6185, loss 0.0289617, acc 0.984375, prec 0.0977791, recall 0.798893
2017-12-10T05:38:03.454625: step 6186, loss 0.146206, acc 0.96875, prec 0.0977888, recall 0.798916
2017-12-10T05:38:03.724311: step 6187, loss 0.0872028, acc 0.96875, prec 0.0978235, recall 0.798984
2017-12-10T05:38:04.002699: step 6188, loss 0.436542, acc 0.96875, prec 0.0978333, recall 0.799007
2017-12-10T05:38:04.269760: step 6189, loss 0.0393448, acc 0.96875, prec 0.0978555, recall 0.799052
2017-12-10T05:38:04.536668: step 6190, loss 0.101258, acc 0.984375, prec 0.0978791, recall 0.799098
2017-12-10T05:38:04.808624: step 6191, loss 0.127933, acc 0.953125, prec 0.0978875, recall 0.79912
2017-12-10T05:38:05.077774: step 6192, loss 0.319168, acc 0.953125, prec 0.0978959, recall 0.799143
2017-12-10T05:38:05.337186: step 6193, loss 0.258207, acc 0.953125, prec 0.0979043, recall 0.799166
2017-12-10T05:38:05.614558: step 6194, loss 0.316675, acc 0.953125, prec 0.0979003, recall 0.799166
2017-12-10T05:38:05.882132: step 6195, loss 0.0530106, acc 0.984375, prec 0.0979114, recall 0.799188
2017-12-10T05:38:06.146011: step 6196, loss 0.140711, acc 0.953125, prec 0.0979322, recall 0.799233
2017-12-10T05:38:06.412921: step 6197, loss 0.117324, acc 0.96875, prec 0.097942, recall 0.799256
2017-12-10T05:38:06.675805: step 6198, loss 0.164308, acc 0.96875, prec 0.0979517, recall 0.799279
2017-12-10T05:38:06.942859: step 6199, loss 0.0473602, acc 0.984375, prec 0.0979753, recall 0.799324
2017-12-10T05:38:07.207588: step 6200, loss 0.0914629, acc 0.96875, prec 0.0979851, recall 0.799347
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-6200

2017-12-10T05:38:08.552720: step 6201, loss 0.306659, acc 0.96875, prec 0.0980197, recall 0.799414
2017-12-10T05:38:08.817167: step 6202, loss 0.0914682, acc 0.953125, prec 0.0980406, recall 0.79946
2017-12-10T05:38:09.083153: step 6203, loss 0.0138944, acc 1, prec 0.0980406, recall 0.79946
2017-12-10T05:38:09.355182: step 6204, loss 0.539438, acc 0.96875, prec 0.0980752, recall 0.799527
2017-12-10T05:38:09.632959: step 6205, loss 0.496415, acc 0.921875, prec 0.0981183, recall 0.799617
2017-12-10T05:38:09.894433: step 6206, loss 0.51305, acc 0.9375, prec 0.0981253, recall 0.79964
2017-12-10T05:38:10.161380: step 6207, loss 0.0770814, acc 0.984375, prec 0.0981364, recall 0.799663
2017-12-10T05:38:10.425651: step 6208, loss 0.0197313, acc 0.984375, prec 0.0981475, recall 0.799685
2017-12-10T05:38:10.698326: step 6209, loss 0.00192673, acc 1, prec 0.0981599, recall 0.799708
2017-12-10T05:38:10.962394: step 6210, loss 0.11554, acc 0.96875, prec 0.0981572, recall 0.799708
2017-12-10T05:38:11.237149: step 6211, loss 0.0538909, acc 0.984375, prec 0.0981683, recall 0.79973
2017-12-10T05:38:11.509521: step 6212, loss 0.0515709, acc 0.96875, prec 0.0981781, recall 0.799753
2017-12-10T05:38:11.775739: step 6213, loss 0.143486, acc 0.96875, prec 0.0981753, recall 0.799753
2017-12-10T05:38:12.048864: step 6214, loss 0.087152, acc 0.953125, prec 0.0981713, recall 0.799753
2017-12-10T05:38:12.328377: step 6215, loss 0.359409, acc 0.984375, prec 0.0981824, recall 0.799775
2017-12-10T05:38:12.607746: step 6216, loss 0.144001, acc 0.953125, prec 0.0981907, recall 0.799798
2017-12-10T05:38:12.872984: step 6217, loss 0.410679, acc 0.96875, prec 0.0982129, recall 0.799843
2017-12-10T05:38:13.141244: step 6218, loss 0.00167231, acc 1, prec 0.0982378, recall 0.799888
2017-12-10T05:38:13.402180: step 6219, loss 0.104719, acc 0.96875, prec 0.0982476, recall 0.79991
2017-12-10T05:38:13.672488: step 6220, loss 0.000560205, acc 1, prec 0.09826, recall 0.799933
2017-12-10T05:38:13.945715: step 6221, loss 0.0352987, acc 0.96875, prec 0.0982822, recall 0.799978
2017-12-10T05:38:14.222570: step 6222, loss 0.0482608, acc 0.984375, prec 0.0982808, recall 0.799978
2017-12-10T05:38:14.488034: step 6223, loss 0.0742409, acc 0.96875, prec 0.098303, recall 0.800022
2017-12-10T05:38:14.754524: step 6224, loss 0.26475, acc 0.953125, prec 0.0982989, recall 0.800022
2017-12-10T05:38:15.022585: step 6225, loss 0.0252919, acc 0.984375, prec 0.09831, recall 0.800045
2017-12-10T05:38:15.300643: step 6226, loss 0.140872, acc 0.984375, prec 0.0983211, recall 0.800067
2017-12-10T05:38:15.568226: step 6227, loss 0.0871164, acc 0.96875, prec 0.0983184, recall 0.800067
2017-12-10T05:38:15.837591: step 6228, loss 0.0249218, acc 1, prec 0.0983432, recall 0.800112
2017-12-10T05:38:16.104070: step 6229, loss 0.0693775, acc 0.96875, prec 0.0983405, recall 0.800112
2017-12-10T05:38:16.376629: step 6230, loss 0.0146592, acc 1, prec 0.0983654, recall 0.800157
2017-12-10T05:38:16.655948: step 6231, loss 0.140713, acc 0.96875, prec 0.0983627, recall 0.800157
2017-12-10T05:38:16.930335: step 6232, loss 0.167688, acc 0.9375, prec 0.0983697, recall 0.80018
2017-12-10T05:38:17.203978: step 6233, loss 0.00027858, acc 1, prec 0.0983697, recall 0.80018
2017-12-10T05:38:17.468173: step 6234, loss 0.071233, acc 0.96875, prec 0.098367, recall 0.80018
2017-12-10T05:38:17.734196: step 6235, loss 0.13786, acc 0.96875, prec 0.0983767, recall 0.800202
2017-12-10T05:38:17.999665: step 6236, loss 0.179947, acc 0.96875, prec 0.098374, recall 0.800202
2017-12-10T05:38:18.276750: step 6237, loss 0.286062, acc 0.953125, prec 0.0983824, recall 0.800224
2017-12-10T05:38:18.542447: step 6238, loss 0.289209, acc 0.953125, prec 0.0983783, recall 0.800224
2017-12-10T05:38:18.816681: step 6239, loss 0.310648, acc 0.984375, prec 0.0983769, recall 0.800224
2017-12-10T05:38:19.083553: step 6240, loss 3.52223, acc 0.96875, prec 0.098388, recall 0.800157
2017-12-10T05:38:19.355252: step 6241, loss 0.108797, acc 0.96875, prec 0.0983853, recall 0.800157
2017-12-10T05:38:19.626922: step 6242, loss 0.23245, acc 0.96875, prec 0.0983826, recall 0.800157
2017-12-10T05:38:19.890804: step 6243, loss 0.102269, acc 0.984375, prec 0.0983937, recall 0.800179
2017-12-10T05:38:20.165414: step 6244, loss 0.0388501, acc 0.984375, prec 0.0983923, recall 0.800179
2017-12-10T05:38:20.429482: step 6245, loss 0.24501, acc 0.96875, prec 0.0984269, recall 0.800247
2017-12-10T05:38:20.699774: step 6246, loss 0.196227, acc 0.953125, prec 0.0984477, recall 0.800291
2017-12-10T05:38:20.970578: step 6247, loss 0.00616903, acc 1, prec 0.0984477, recall 0.800291
2017-12-10T05:38:21.243487: step 6248, loss 0.298472, acc 0.953125, prec 0.0984436, recall 0.800291
2017-12-10T05:38:21.510990: step 6249, loss 0.128834, acc 0.96875, prec 0.0984409, recall 0.800291
2017-12-10T05:38:21.781739: step 6250, loss 0.560868, acc 0.953125, prec 0.0984617, recall 0.800336
2017-12-10T05:38:22.049157: step 6251, loss 0.129531, acc 1, prec 0.0984989, recall 0.800403
2017-12-10T05:38:22.323311: step 6252, loss 0.360146, acc 0.9375, prec 0.0984935, recall 0.800403
2017-12-10T05:38:22.596391: step 6253, loss 0.329573, acc 0.953125, prec 0.0985019, recall 0.800426
2017-12-10T05:38:22.871394: step 6254, loss 0.383536, acc 0.953125, prec 0.0985102, recall 0.800448
2017-12-10T05:38:23.135876: step 6255, loss 0.289304, acc 0.953125, prec 0.0985186, recall 0.80047
2017-12-10T05:38:23.408872: step 6256, loss 0.20849, acc 0.953125, prec 0.0985269, recall 0.800493
2017-12-10T05:38:23.674130: step 6257, loss 0.21152, acc 0.9375, prec 0.0985339, recall 0.800515
2017-12-10T05:38:23.942243: step 6258, loss 0.144491, acc 0.9375, prec 0.0985285, recall 0.800515
2017-12-10T05:38:24.212059: step 6259, loss 0.202428, acc 0.953125, prec 0.0985244, recall 0.800515
2017-12-10T05:38:24.477925: step 6260, loss 0.0691534, acc 0.96875, prec 0.0985341, recall 0.800537
2017-12-10T05:38:24.753778: step 6261, loss 0.359075, acc 0.953125, prec 0.0985673, recall 0.800604
2017-12-10T05:38:25.025386: step 6262, loss 0.132795, acc 0.953125, prec 0.0985756, recall 0.800627
2017-12-10T05:38:25.294709: step 6263, loss 0.10331, acc 0.96875, prec 0.0985729, recall 0.800627
2017-12-10T05:38:25.562474: step 6264, loss 0.485838, acc 0.921875, prec 0.0985785, recall 0.800649
2017-12-10T05:38:25.829521: step 6265, loss 0.38332, acc 0.96875, prec 0.0986007, recall 0.800693
2017-12-10T05:38:26.095338: step 6266, loss 0.214763, acc 0.953125, prec 0.0985966, recall 0.800693
2017-12-10T05:38:26.360302: step 6267, loss 0.165068, acc 0.96875, prec 0.0986187, recall 0.800738
2017-12-10T05:38:26.627062: step 6268, loss 0.120316, acc 0.953125, prec 0.098627, recall 0.80076
2017-12-10T05:38:26.898986: step 6269, loss 0.126046, acc 0.953125, prec 0.0986354, recall 0.800783
2017-12-10T05:38:27.168116: step 6270, loss 0.391734, acc 0.953125, prec 0.0986561, recall 0.800827
2017-12-10T05:38:27.442317: step 6271, loss 0.314641, acc 0.953125, prec 0.098652, recall 0.800827
2017-12-10T05:38:27.711216: step 6272, loss 0.11078, acc 0.96875, prec 0.0986742, recall 0.800872
2017-12-10T05:38:27.979168: step 6273, loss 0.278611, acc 0.96875, prec 0.0986714, recall 0.800872
2017-12-10T05:38:28.243109: step 6274, loss 0.31996, acc 0.953125, prec 0.0986798, recall 0.800894
2017-12-10T05:38:28.515948: step 6275, loss 0.0147903, acc 1, prec 0.0986798, recall 0.800894
2017-12-10T05:38:28.783041: step 6276, loss 0.22568, acc 0.984375, prec 0.0986784, recall 0.800894
2017-12-10T05:38:29.048416: step 6277, loss 0.00792757, acc 1, prec 0.0987032, recall 0.800938
2017-12-10T05:38:29.318820: step 6278, loss 0.295986, acc 0.953125, prec 0.0986992, recall 0.800938
2017-12-10T05:38:29.590044: step 6279, loss 0.124766, acc 0.984375, prec 0.0987226, recall 0.800983
2017-12-10T05:38:29.862519: step 6280, loss 0.170342, acc 1, prec 0.098735, recall 0.801005
2017-12-10T05:38:30.132727: step 6281, loss 0.000564655, acc 1, prec 0.098735, recall 0.801005
2017-12-10T05:38:30.405250: step 6282, loss 0.00145383, acc 1, prec 0.098735, recall 0.801005
2017-12-10T05:38:30.676189: step 6283, loss 0.00104681, acc 1, prec 0.098735, recall 0.801005
2017-12-10T05:38:30.942499: step 6284, loss 0.00260906, acc 1, prec 0.0987474, recall 0.801027
2017-12-10T05:38:31.207617: step 6285, loss 0.581578, acc 1, prec 0.0987722, recall 0.801072
2017-12-10T05:38:31.481695: step 6286, loss 0.00126927, acc 1, prec 0.0987722, recall 0.801072
2017-12-10T05:38:31.758723: step 6287, loss 0.00382444, acc 1, prec 0.0987722, recall 0.801072
2017-12-10T05:38:32.025653: step 6288, loss 0.0165642, acc 0.984375, prec 0.0988081, recall 0.801138
2017-12-10T05:38:32.294297: step 6289, loss 0.126229, acc 0.96875, prec 0.0988054, recall 0.801138
2017-12-10T05:38:32.568751: step 6290, loss 0.0098099, acc 1, prec 0.0988178, recall 0.80116
2017-12-10T05:38:32.842444: step 6291, loss 0.135254, acc 0.96875, prec 0.098815, recall 0.80116
2017-12-10T05:38:33.113345: step 6292, loss 0.270556, acc 0.96875, prec 0.0988247, recall 0.801183
2017-12-10T05:38:33.386861: step 6293, loss 0.114747, acc 0.984375, prec 0.0988482, recall 0.801227
2017-12-10T05:38:33.662073: step 6294, loss 0.113026, acc 0.984375, prec 0.0988468, recall 0.801227
2017-12-10T05:38:33.927466: step 6295, loss 4.952, acc 0.984375, prec 0.0988468, recall 0.801138
2017-12-10T05:38:34.193779: step 6296, loss 0.40266, acc 0.984375, prec 0.0988455, recall 0.801138
2017-12-10T05:38:34.461840: step 6297, loss 0.00673234, acc 1, prec 0.0988578, recall 0.80116
2017-12-10T05:38:34.734727: step 6298, loss 0.479792, acc 0.96875, prec 0.0988675, recall 0.801182
2017-12-10T05:38:34.998093: step 6299, loss 0.0575612, acc 0.96875, prec 0.0988772, recall 0.801204
2017-12-10T05:38:35.263721: step 6300, loss 0.684513, acc 0.953125, prec 0.0988731, recall 0.801204

Evaluation:
2017-12-10T05:38:42.873502: step 6300, loss 5.26005, acc 0.932063, prec 0.0989536, recall 0.796032

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-6300

2017-12-10T05:38:44.194317: step 6301, loss 0.645129, acc 0.953125, prec 0.0989496, recall 0.796032
2017-12-10T05:38:44.460774: step 6302, loss 0.438571, acc 0.921875, prec 0.0989551, recall 0.796055
2017-12-10T05:38:44.726447: step 6303, loss 0.420445, acc 0.890625, prec 0.0989457, recall 0.796055
2017-12-10T05:38:44.994432: step 6304, loss 0.0910811, acc 0.953125, prec 0.0989539, recall 0.796077
2017-12-10T05:38:45.261552: step 6305, loss 0.404595, acc 0.953125, prec 0.0989622, recall 0.796099
2017-12-10T05:38:45.534155: step 6306, loss 0.290189, acc 0.921875, prec 0.09898, recall 0.796144
2017-12-10T05:38:45.803694: step 6307, loss 0.363334, acc 0.90625, prec 0.0989964, recall 0.796189
2017-12-10T05:38:46.075053: step 6308, loss 0.390302, acc 0.90625, prec 0.0990129, recall 0.796233
2017-12-10T05:38:46.342751: step 6309, loss 0.646918, acc 0.875, prec 0.0990143, recall 0.796256
2017-12-10T05:38:46.616353: step 6310, loss 0.152509, acc 0.921875, prec 0.0990076, recall 0.796256
2017-12-10T05:38:46.893070: step 6311, loss 0.560966, acc 0.90625, prec 0.0989995, recall 0.796256
2017-12-10T05:38:47.166068: step 6312, loss 0.14245, acc 0.953125, prec 0.0990077, recall 0.796278
2017-12-10T05:38:47.432454: step 6313, loss 0.27854, acc 0.921875, prec 0.099001, recall 0.796278
2017-12-10T05:38:47.707158: step 6314, loss 0.381902, acc 0.953125, prec 0.0990338, recall 0.796345
2017-12-10T05:38:47.975564: step 6315, loss 0.212537, acc 0.9375, prec 0.0990406, recall 0.796367
2017-12-10T05:38:48.252689: step 6316, loss 0.534124, acc 0.921875, prec 0.0990584, recall 0.796412
2017-12-10T05:38:48.521901: step 6317, loss 0.587349, acc 0.9375, prec 0.0990653, recall 0.796434
2017-12-10T05:38:48.793477: step 6318, loss 0.183772, acc 0.953125, prec 0.0990735, recall 0.796456
2017-12-10T05:38:49.070925: step 6319, loss 0.0810696, acc 0.96875, prec 0.0990708, recall 0.796456
2017-12-10T05:38:49.337859: step 6320, loss 0.0304476, acc 0.984375, prec 0.0990694, recall 0.796456
2017-12-10T05:38:49.605310: step 6321, loss 0.947378, acc 0.953125, prec 0.0991144, recall 0.796545
2017-12-10T05:38:49.873355: step 6322, loss 0.879178, acc 0.9375, prec 0.0991213, recall 0.796568
2017-12-10T05:38:50.140213: step 6323, loss 0.0493476, acc 0.96875, prec 0.0991186, recall 0.796568
2017-12-10T05:38:50.410176: step 6324, loss 0.098826, acc 0.953125, prec 0.0991145, recall 0.796568
2017-12-10T05:38:50.681227: step 6325, loss 0.24348, acc 0.96875, prec 0.0991363, recall 0.796612
2017-12-10T05:38:50.943514: step 6326, loss 7.96155, acc 0.984375, prec 0.0991486, recall 0.796547
2017-12-10T05:38:51.208954: step 6327, loss 0.353878, acc 0.921875, prec 0.0991419, recall 0.796547
2017-12-10T05:38:51.477865: step 6328, loss 0.605541, acc 0.890625, prec 0.0991324, recall 0.796547
2017-12-10T05:38:51.749571: step 6329, loss 0.604482, acc 0.890625, prec 0.0991475, recall 0.796592
2017-12-10T05:38:52.019112: step 6330, loss 0.221032, acc 0.921875, prec 0.099153, recall 0.796614
2017-12-10T05:38:52.288794: step 6331, loss 0.0987951, acc 0.96875, prec 0.0991503, recall 0.796614
2017-12-10T05:38:52.555393: step 6332, loss 0.126244, acc 0.9375, prec 0.0991694, recall 0.796658
2017-12-10T05:38:52.819997: step 6333, loss 0.283081, acc 0.921875, prec 0.0991627, recall 0.796658
2017-12-10T05:38:53.094567: step 6334, loss 0.453609, acc 0.890625, prec 0.0991655, recall 0.796681
2017-12-10T05:38:53.375519: step 6335, loss 0.658589, acc 0.859375, prec 0.0991533, recall 0.796681
2017-12-10T05:38:53.646103: step 6336, loss 0.490776, acc 0.890625, prec 0.0991684, recall 0.796725
2017-12-10T05:38:53.915303: step 6337, loss 0.213804, acc 0.9375, prec 0.099163, recall 0.796725
2017-12-10T05:38:54.180085: step 6338, loss 0.268265, acc 0.9375, prec 0.0991576, recall 0.796725
2017-12-10T05:38:54.455376: step 6339, loss 0.352103, acc 0.890625, prec 0.0991971, recall 0.796814
2017-12-10T05:38:54.722252: step 6340, loss 0.564245, acc 0.875, prec 0.0991986, recall 0.796836
2017-12-10T05:38:54.992044: step 6341, loss 0.650235, acc 0.84375, prec 0.0991851, recall 0.796836
2017-12-10T05:38:55.261085: step 6342, loss 0.194947, acc 0.9375, prec 0.0991797, recall 0.796836
2017-12-10T05:38:55.531298: step 6343, loss 0.390437, acc 0.921875, prec 0.0991852, recall 0.796858
2017-12-10T05:38:55.796010: step 6344, loss 0.206975, acc 0.9375, prec 0.0991798, recall 0.796858
2017-12-10T05:38:56.061299: step 6345, loss 0.234144, acc 0.9375, prec 0.0991745, recall 0.796858
2017-12-10T05:38:56.326458: step 6346, loss 0.443587, acc 0.9375, prec 0.0992058, recall 0.796924
2017-12-10T05:38:56.593423: step 6347, loss 0.115427, acc 0.984375, prec 0.0992166, recall 0.796947
2017-12-10T05:38:56.861797: step 6348, loss 0.176907, acc 0.9375, prec 0.0992357, recall 0.796991
2017-12-10T05:38:57.135526: step 6349, loss 0.201767, acc 0.953125, prec 0.0992317, recall 0.796991
2017-12-10T05:38:57.407402: step 6350, loss 0.269642, acc 0.96875, prec 0.099229, recall 0.796991
2017-12-10T05:38:57.670587: step 6351, loss 0.100318, acc 0.96875, prec 0.0992263, recall 0.796991
2017-12-10T05:38:57.936796: step 6352, loss 0.137291, acc 0.96875, prec 0.0992358, recall 0.797013
2017-12-10T05:38:58.202013: step 6353, loss 0.483017, acc 0.9375, prec 0.0992549, recall 0.797057
2017-12-10T05:38:58.475830: step 6354, loss 0.432162, acc 0.984375, prec 0.0992658, recall 0.797079
2017-12-10T05:38:58.746811: step 6355, loss 0.267208, acc 0.96875, prec 0.0992753, recall 0.797101
2017-12-10T05:38:59.009317: step 6356, loss 0.140228, acc 0.96875, prec 0.0992726, recall 0.797101
2017-12-10T05:38:59.275570: step 6357, loss 0.170642, acc 0.953125, prec 0.099293, recall 0.797146
2017-12-10T05:38:59.538580: step 6358, loss 0.339715, acc 0.96875, prec 0.0993025, recall 0.797168
2017-12-10T05:38:59.812283: step 6359, loss 0.109249, acc 0.96875, prec 0.0992998, recall 0.797168
2017-12-10T05:39:00.081106: step 6360, loss 0.000437264, acc 1, prec 0.099312, recall 0.79719
2017-12-10T05:39:00.351985: step 6361, loss 0.158347, acc 0.953125, prec 0.099308, recall 0.79719
2017-12-10T05:39:00.626269: step 6362, loss 0.0237024, acc 0.984375, prec 0.0993067, recall 0.79719
2017-12-10T05:39:00.892351: step 6363, loss 0.110246, acc 0.984375, prec 0.0993053, recall 0.79719
2017-12-10T05:39:01.166211: step 6364, loss 0.125962, acc 0.984375, prec 0.0993528, recall 0.797278
2017-12-10T05:39:01.439794: step 6365, loss 0.00128821, acc 1, prec 0.0993651, recall 0.7973
2017-12-10T05:39:01.709151: step 6366, loss 0.017028, acc 0.984375, prec 0.0993637, recall 0.7973
2017-12-10T05:39:01.981045: step 6367, loss 0.147143, acc 0.984375, prec 0.0993868, recall 0.797344
2017-12-10T05:39:02.256715: step 6368, loss 0.307079, acc 0.984375, prec 0.0994099, recall 0.797388
2017-12-10T05:39:02.522877: step 6369, loss 0.0178281, acc 1, prec 0.0994221, recall 0.79741
2017-12-10T05:39:02.797490: step 6370, loss 0.0263539, acc 0.984375, prec 0.0994452, recall 0.797455
2017-12-10T05:39:03.059643: step 6371, loss 10.9177, acc 0.953125, prec 0.0994425, recall 0.797368
2017-12-10T05:39:03.325747: step 6372, loss 0.84159, acc 0.984375, prec 0.0994656, recall 0.797412
2017-12-10T05:39:03.594518: step 6373, loss 0.00925665, acc 1, prec 0.0994778, recall 0.797434
2017-12-10T05:39:03.862680: step 6374, loss 0.0137964, acc 1, prec 0.0994778, recall 0.797434
2017-12-10T05:39:04.127126: step 6375, loss 0.461079, acc 0.96875, prec 0.0994751, recall 0.797434
2017-12-10T05:39:04.394227: step 6376, loss 0.0202807, acc 1, prec 0.0994751, recall 0.797434
2017-12-10T05:39:04.666525: step 6377, loss 0.0741043, acc 0.984375, prec 0.0994737, recall 0.797434
2017-12-10T05:39:04.936189: step 6378, loss 0.119026, acc 0.984375, prec 0.0994846, recall 0.797456
2017-12-10T05:39:05.209589: step 6379, loss 0.865274, acc 0.90625, prec 0.0994765, recall 0.797456
2017-12-10T05:39:05.481366: step 6380, loss 0.147218, acc 0.9375, prec 0.0994833, recall 0.797478
2017-12-10T05:39:05.749355: step 6381, loss 0.602883, acc 0.90625, prec 0.0994752, recall 0.797478
2017-12-10T05:39:06.015912: step 6382, loss 1.56764, acc 0.90625, prec 0.0994793, recall 0.7975
2017-12-10T05:39:06.279536: step 6383, loss 0.208062, acc 0.921875, prec 0.0994848, recall 0.797522
2017-12-10T05:39:06.545866: step 6384, loss 0.528621, acc 0.890625, prec 0.0994876, recall 0.797544
2017-12-10T05:39:06.813491: step 6385, loss 0.784055, acc 0.828125, prec 0.0994727, recall 0.797544
2017-12-10T05:39:07.081800: step 6386, loss 0.43902, acc 0.875, prec 0.0994742, recall 0.797566
2017-12-10T05:39:07.353899: step 6387, loss 0.27242, acc 0.890625, prec 0.0994647, recall 0.797566
2017-12-10T05:39:07.618525: step 6388, loss 0.539933, acc 0.890625, prec 0.0994919, recall 0.797632
2017-12-10T05:39:07.895509: step 6389, loss 0.619329, acc 0.890625, prec 0.0995069, recall 0.797676
2017-12-10T05:39:08.160627: step 6390, loss 0.303916, acc 0.953125, prec 0.099515, recall 0.797698
2017-12-10T05:39:08.429384: step 6391, loss 0.446349, acc 0.90625, prec 0.0995069, recall 0.797698
2017-12-10T05:39:08.692867: step 6392, loss 0.157879, acc 0.9375, prec 0.0995137, recall 0.79772
2017-12-10T05:39:08.960895: step 6393, loss 0.51065, acc 0.890625, prec 0.0995043, recall 0.79772
2017-12-10T05:39:09.227876: step 6394, loss 0.0643883, acc 0.96875, prec 0.0995138, recall 0.797742
2017-12-10T05:39:09.492457: step 6395, loss 0.168942, acc 0.953125, prec 0.0995342, recall 0.797786
2017-12-10T05:39:09.769904: step 6396, loss 0.227956, acc 0.9375, prec 0.0995531, recall 0.79783
2017-12-10T05:39:10.039828: step 6397, loss 0.064765, acc 0.96875, prec 0.0995748, recall 0.797873
2017-12-10T05:39:10.304937: step 6398, loss 1.32843, acc 0.953125, prec 0.099583, recall 0.797895
2017-12-10T05:39:10.575822: step 6399, loss 0.163785, acc 0.9375, prec 0.0995776, recall 0.797895
2017-12-10T05:39:10.838886: step 6400, loss 0.189773, acc 0.953125, prec 0.0995735, recall 0.797895
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-6400

2017-12-10T05:39:12.162136: step 6401, loss 0.354564, acc 0.96875, prec 0.099583, recall 0.797917
2017-12-10T05:39:12.433923: step 6402, loss 0.107815, acc 0.96875, prec 0.0996047, recall 0.797961
2017-12-10T05:39:12.696314: step 6403, loss 0.480663, acc 0.921875, prec 0.099598, recall 0.797961
2017-12-10T05:39:12.961396: step 6404, loss 0.0785869, acc 0.984375, prec 0.099621, recall 0.798005
2017-12-10T05:39:13.225622: step 6405, loss 0.577821, acc 0.890625, prec 0.0996116, recall 0.798005
2017-12-10T05:39:13.491057: step 6406, loss 0.0968644, acc 0.96875, prec 0.0996454, recall 0.798071
2017-12-10T05:39:13.755777: step 6407, loss 0.301323, acc 0.96875, prec 0.0996671, recall 0.798114
2017-12-10T05:39:14.023521: step 6408, loss 0.0401205, acc 0.96875, prec 0.0996644, recall 0.798114
2017-12-10T05:39:14.292558: step 6409, loss 0.689845, acc 0.953125, prec 0.0996725, recall 0.798136
2017-12-10T05:39:14.560933: step 6410, loss 0.457913, acc 0.921875, prec 0.0996658, recall 0.798136
2017-12-10T05:39:14.820629: step 6411, loss 0.582423, acc 0.9375, prec 0.0996604, recall 0.798136
2017-12-10T05:39:15.090173: step 6412, loss 0.275314, acc 0.9375, prec 0.0996672, recall 0.798158
2017-12-10T05:39:15.358509: step 6413, loss 0.0624639, acc 0.984375, prec 0.099678, recall 0.79818
2017-12-10T05:39:15.626633: step 6414, loss 0.621667, acc 0.953125, prec 0.0996862, recall 0.798202
2017-12-10T05:39:15.894379: step 6415, loss 0.266675, acc 0.9375, prec 0.0996808, recall 0.798202
2017-12-10T05:39:16.159069: step 6416, loss 2.19821, acc 0.984375, prec 0.0996929, recall 0.798137
2017-12-10T05:39:16.439211: step 6417, loss 0.298871, acc 0.96875, prec 0.0997024, recall 0.798159
2017-12-10T05:39:16.709129: step 6418, loss 0.171717, acc 0.96875, prec 0.0997241, recall 0.798203
2017-12-10T05:39:16.977656: step 6419, loss 0.1108, acc 0.953125, prec 0.09972, recall 0.798203
2017-12-10T05:39:17.245330: step 6420, loss 0.185642, acc 0.96875, prec 0.0997295, recall 0.798225
2017-12-10T05:39:17.511989: step 6421, loss 0.20632, acc 0.9375, prec 0.0997241, recall 0.798225
2017-12-10T05:39:17.779596: step 6422, loss 0.590117, acc 0.90625, prec 0.099716, recall 0.798225
2017-12-10T05:39:18.047463: step 6423, loss 0.120814, acc 0.953125, prec 0.099712, recall 0.798225
2017-12-10T05:39:18.319356: step 6424, loss 0.555937, acc 0.921875, prec 0.0997174, recall 0.798247
2017-12-10T05:39:18.588832: step 6425, loss 0.324772, acc 0.921875, prec 0.0997107, recall 0.798247
2017-12-10T05:39:18.855249: step 6426, loss 0.125024, acc 0.953125, prec 0.0997066, recall 0.798247
2017-12-10T05:39:19.119820: step 6427, loss 0.0757531, acc 0.96875, prec 0.0997283, recall 0.79829
2017-12-10T05:39:19.387482: step 6428, loss 0.154987, acc 0.984375, prec 0.0997391, recall 0.798312
2017-12-10T05:39:19.654592: step 6429, loss 0.568813, acc 0.90625, prec 0.0997432, recall 0.798334
2017-12-10T05:39:19.918176: step 6430, loss 0.49536, acc 0.921875, prec 0.0997364, recall 0.798334
2017-12-10T05:39:20.192858: step 6431, loss 0.635782, acc 0.921875, prec 0.0997419, recall 0.798356
2017-12-10T05:39:20.459801: step 6432, loss 0.135884, acc 0.953125, prec 0.09975, recall 0.798378
2017-12-10T05:39:20.734811: step 6433, loss 0.61503, acc 0.90625, prec 0.0997541, recall 0.798399
2017-12-10T05:39:20.999795: step 6434, loss 0.269291, acc 0.953125, prec 0.0997743, recall 0.798443
2017-12-10T05:39:21.268911: step 6435, loss 0.158903, acc 0.953125, prec 0.0997946, recall 0.798486
2017-12-10T05:39:21.541361: step 6436, loss 0.195915, acc 0.9375, prec 0.0997892, recall 0.798486
2017-12-10T05:39:21.816844: step 6437, loss 0.581408, acc 0.984375, prec 0.0998001, recall 0.798508
2017-12-10T05:39:22.087900: step 6438, loss 0.161362, acc 0.984375, prec 0.0997987, recall 0.798508
2017-12-10T05:39:22.365766: step 6439, loss 0.0502934, acc 0.96875, prec 0.0998082, recall 0.79853
2017-12-10T05:39:22.636301: step 6440, loss 0.110524, acc 0.9375, prec 0.0998149, recall 0.798552
2017-12-10T05:39:22.901177: step 6441, loss 0.115308, acc 0.984375, prec 0.0998257, recall 0.798574
2017-12-10T05:39:23.169938: step 6442, loss 1.26075, acc 0.96875, prec 0.0998352, recall 0.798595
2017-12-10T05:39:23.435684: step 6443, loss 0.297048, acc 0.953125, prec 0.0998433, recall 0.798617
2017-12-10T05:39:23.702540: step 6444, loss 0.111256, acc 0.953125, prec 0.0998757, recall 0.798682
2017-12-10T05:39:23.967000: step 6445, loss 0.165132, acc 0.9375, prec 0.0998947, recall 0.798726
2017-12-10T05:39:24.230507: step 6446, loss 0.0562971, acc 0.984375, prec 0.0999055, recall 0.798748
2017-12-10T05:39:24.503478: step 6447, loss 0.308113, acc 0.96875, prec 0.0999149, recall 0.798769
2017-12-10T05:39:24.769766: step 6448, loss 0.106931, acc 0.96875, prec 0.0999122, recall 0.798769
2017-12-10T05:39:25.037127: step 6449, loss 0.0878808, acc 0.953125, prec 0.0999082, recall 0.798769
2017-12-10T05:39:25.305463: step 6450, loss 0.656354, acc 0.90625, prec 0.0999001, recall 0.798769
2017-12-10T05:39:25.577692: step 6451, loss 0.733042, acc 0.890625, prec 0.0998906, recall 0.798769
2017-12-10T05:39:25.845694: step 6452, loss 0.1794, acc 0.953125, prec 0.0998987, recall 0.798791
2017-12-10T05:39:26.117615: step 6453, loss 0.0345513, acc 0.984375, prec 0.0999217, recall 0.798834
2017-12-10T05:39:26.379541: step 6454, loss 0.102557, acc 1, prec 0.0999582, recall 0.7989
2017-12-10T05:39:26.646997: step 6455, loss 0.24232, acc 0.9375, prec 0.0999649, recall 0.798921
2017-12-10T05:39:26.913803: step 6456, loss 0.0876506, acc 0.953125, prec 0.099973, recall 0.798943
2017-12-10T05:39:27.187297: step 6457, loss 0.0619398, acc 0.984375, prec 0.0999838, recall 0.798965
2017-12-10T05:39:27.467169: step 6458, loss 0.0694993, acc 0.96875, prec 0.0999933, recall 0.798986
2017-12-10T05:39:27.731184: step 6459, loss 0.252064, acc 0.953125, prec 0.100001, recall 0.799008
2017-12-10T05:39:28.005171: step 6460, loss 0.288529, acc 0.953125, prec 0.0999973, recall 0.799008
2017-12-10T05:39:28.241182: step 6461, loss 0.160422, acc 0.961538, prec 0.0999946, recall 0.799008
2017-12-10T05:39:28.521663: step 6462, loss 0.082335, acc 0.96875, prec 0.0999919, recall 0.799008
2017-12-10T05:39:28.789243: step 6463, loss 0.135715, acc 0.9375, prec 0.100011, recall 0.799051
2017-12-10T05:39:29.056216: step 6464, loss 0.11403, acc 0.953125, prec 0.100007, recall 0.799051
2017-12-10T05:39:29.324806: step 6465, loss 0.0259186, acc 0.984375, prec 0.10003, recall 0.799095
2017-12-10T05:39:29.596697: step 6466, loss 0.0116101, acc 1, prec 0.10003, recall 0.799095
2017-12-10T05:39:29.862791: step 6467, loss 0.0381936, acc 0.984375, prec 0.100053, recall 0.799138
2017-12-10T05:39:30.130844: step 6468, loss 0.00502785, acc 1, prec 0.100065, recall 0.79916
2017-12-10T05:39:30.409652: step 6469, loss 9.50492e-05, acc 1, prec 0.100065, recall 0.79916
2017-12-10T05:39:30.684702: step 6470, loss 0.91355, acc 0.96875, prec 0.100086, recall 0.799203
2017-12-10T05:39:30.961056: step 6471, loss 0.744328, acc 0.9375, prec 0.100081, recall 0.799203
2017-12-10T05:39:31.229409: step 6472, loss 0.157122, acc 0.96875, prec 0.10009, recall 0.799224
2017-12-10T05:39:31.499455: step 6473, loss 0.357228, acc 0.984375, prec 0.100101, recall 0.799246
2017-12-10T05:39:31.771379: step 6474, loss 2.15335, acc 0.953125, prec 0.100147, recall 0.799246
2017-12-10T05:39:32.041247: step 6475, loss 0.326534, acc 0.96875, prec 0.100156, recall 0.799268
2017-12-10T05:39:32.308104: step 6476, loss 0.118727, acc 0.984375, prec 0.100167, recall 0.79929
2017-12-10T05:39:32.581624: step 6477, loss 0.0102295, acc 1, prec 0.100167, recall 0.79929
2017-12-10T05:39:32.845209: step 6478, loss 0.252923, acc 0.953125, prec 0.100175, recall 0.799311
2017-12-10T05:39:33.113927: step 6479, loss 0.625039, acc 0.9375, prec 0.100182, recall 0.799333
2017-12-10T05:39:33.379523: step 6480, loss 0.352794, acc 0.953125, prec 0.100178, recall 0.799333
2017-12-10T05:39:33.645139: step 6481, loss 0.254703, acc 0.9375, prec 0.100173, recall 0.799333
2017-12-10T05:39:33.916671: step 6482, loss 0.484812, acc 0.9375, prec 0.100167, recall 0.799333
2017-12-10T05:39:34.180318: step 6483, loss 0.688189, acc 0.9375, prec 0.100186, recall 0.799376
2017-12-10T05:39:34.448520: step 6484, loss 0.013165, acc 1, prec 0.100186, recall 0.799376
2017-12-10T05:39:34.716917: step 6485, loss 0.698161, acc 0.90625, prec 0.10019, recall 0.799398
2017-12-10T05:39:34.983691: step 6486, loss 0.440588, acc 0.921875, prec 0.100183, recall 0.799398
2017-12-10T05:39:35.254172: step 6487, loss 0.232734, acc 0.96875, prec 0.100193, recall 0.799419
2017-12-10T05:39:35.524325: step 6488, loss 0.0497747, acc 0.96875, prec 0.100202, recall 0.799441
2017-12-10T05:39:35.796808: step 6489, loss 0.119628, acc 0.984375, prec 0.100213, recall 0.799462
2017-12-10T05:39:36.063099: step 6490, loss 0.450575, acc 0.953125, prec 0.100221, recall 0.799484
2017-12-10T05:39:36.328944: step 6491, loss 0.193716, acc 0.96875, prec 0.100255, recall 0.799549
2017-12-10T05:39:36.593853: step 6492, loss 0.434279, acc 0.953125, prec 0.100275, recall 0.799592
2017-12-10T05:39:36.859189: step 6493, loss 0.660194, acc 0.96875, prec 0.100272, recall 0.799592
2017-12-10T05:39:37.137685: step 6494, loss 0.026281, acc 0.984375, prec 0.100283, recall 0.799613
2017-12-10T05:39:37.403345: step 6495, loss 0.162703, acc 0.96875, prec 0.100305, recall 0.799656
2017-12-10T05:39:37.671144: step 6496, loss 1.21351, acc 0.953125, prec 0.100337, recall 0.799721
2017-12-10T05:39:37.945746: step 6497, loss 0.154758, acc 0.96875, prec 0.100334, recall 0.799721
2017-12-10T05:39:38.222119: step 6498, loss 0.353603, acc 0.953125, prec 0.100342, recall 0.799742
2017-12-10T05:39:38.499068: step 6499, loss 0.0202181, acc 0.984375, prec 0.100353, recall 0.799764
2017-12-10T05:39:38.764791: step 6500, loss 0.355822, acc 0.890625, prec 0.100368, recall 0.799807
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-6500

2017-12-10T05:39:40.029890: step 6501, loss 0.194114, acc 0.953125, prec 0.100364, recall 0.799807
2017-12-10T05:39:40.295322: step 6502, loss 0.0861964, acc 0.96875, prec 0.100361, recall 0.799807
2017-12-10T05:39:40.569210: step 6503, loss 0.173821, acc 0.96875, prec 0.10037, recall 0.799828
2017-12-10T05:39:40.840285: step 6504, loss 0.0271295, acc 1, prec 0.10037, recall 0.799828
2017-12-10T05:39:41.109413: step 6505, loss 0.314064, acc 0.96875, prec 0.10038, recall 0.79985
2017-12-10T05:39:41.375651: step 6506, loss 0.0359384, acc 0.984375, prec 0.100391, recall 0.799871
2017-12-10T05:39:41.651849: step 6507, loss 0.447719, acc 0.90625, prec 0.100395, recall 0.799893
2017-12-10T05:39:41.919586: step 6508, loss 0.122762, acc 0.96875, prec 0.100404, recall 0.799914
2017-12-10T05:39:42.194713: step 6509, loss 0.272629, acc 0.9375, prec 0.100411, recall 0.799936
2017-12-10T05:39:42.475301: step 6510, loss 0.0834124, acc 0.984375, prec 0.100434, recall 0.799979
2017-12-10T05:39:42.742405: step 6511, loss 0.0227236, acc 0.984375, prec 0.100444, recall 0.8
2017-12-10T05:39:43.004963: step 6512, loss 0.191379, acc 0.96875, prec 0.100442, recall 0.8
2017-12-10T05:39:43.270623: step 6513, loss 0.0525594, acc 0.96875, prec 0.100439, recall 0.8
2017-12-10T05:39:43.539215: step 6514, loss 0.0257607, acc 0.984375, prec 0.10045, recall 0.800021
2017-12-10T05:39:43.805774: step 6515, loss 0.180579, acc 0.953125, prec 0.100458, recall 0.800043
2017-12-10T05:39:44.071098: step 6516, loss 0.188602, acc 0.9375, prec 0.100477, recall 0.800086
2017-12-10T05:39:44.338168: step 6517, loss 0.150113, acc 0.96875, prec 0.100486, recall 0.800107
2017-12-10T05:39:44.606808: step 6518, loss 0.0610037, acc 0.96875, prec 0.100495, recall 0.800129
2017-12-10T05:39:44.878838: step 6519, loss 0.0432811, acc 0.984375, prec 0.100506, recall 0.80015
2017-12-10T05:39:45.144100: step 6520, loss 0.096467, acc 0.984375, prec 0.100505, recall 0.80015
2017-12-10T05:39:45.421174: step 6521, loss 0.0158372, acc 1, prec 0.100517, recall 0.800171
2017-12-10T05:39:45.689290: step 6522, loss 0.0013346, acc 1, prec 0.100517, recall 0.800171
2017-12-10T05:39:45.957730: step 6523, loss 0.0589786, acc 0.96875, prec 0.100526, recall 0.800193
2017-12-10T05:39:46.230679: step 6524, loss 0.00433988, acc 1, prec 0.100526, recall 0.800193
2017-12-10T05:39:46.505817: step 6525, loss 0.127805, acc 0.984375, prec 0.100537, recall 0.800214
2017-12-10T05:39:46.778876: step 6526, loss 0.113961, acc 0.984375, prec 0.100536, recall 0.800214
2017-12-10T05:39:47.054697: step 6527, loss 0.0865954, acc 0.96875, prec 0.100545, recall 0.800236
2017-12-10T05:39:47.323964: step 6528, loss 0.00323535, acc 1, prec 0.100557, recall 0.800257
2017-12-10T05:39:47.594061: step 6529, loss 0.124354, acc 0.953125, prec 0.100553, recall 0.800257
2017-12-10T05:39:47.867333: step 6530, loss 0.146368, acc 0.984375, prec 0.100552, recall 0.800257
2017-12-10T05:39:48.139698: step 6531, loss 0.00519963, acc 1, prec 0.100564, recall 0.800278
2017-12-10T05:39:48.402921: step 6532, loss 0.00171703, acc 1, prec 0.100576, recall 0.8003
2017-12-10T05:39:48.665406: step 6533, loss 0.00177634, acc 1, prec 0.100576, recall 0.8003
2017-12-10T05:39:48.927809: step 6534, loss 2.53683, acc 0.984375, prec 0.100576, recall 0.800214
2017-12-10T05:39:49.197787: step 6535, loss 0.101621, acc 0.953125, prec 0.100584, recall 0.800236
2017-12-10T05:39:49.471395: step 6536, loss 0.0942888, acc 0.96875, prec 0.100581, recall 0.800236
2017-12-10T05:39:49.747666: step 6537, loss 0.0668368, acc 0.96875, prec 0.100591, recall 0.800257
2017-12-10T05:39:50.019649: step 6538, loss 0.306644, acc 0.984375, prec 0.100589, recall 0.800257
2017-12-10T05:39:50.285194: step 6539, loss 0.058812, acc 0.984375, prec 0.1006, recall 0.800278
2017-12-10T05:39:50.548269: step 6540, loss 0.291884, acc 0.96875, prec 0.100646, recall 0.800364
2017-12-10T05:39:50.820130: step 6541, loss 0.0672643, acc 0.953125, prec 0.100642, recall 0.800364
2017-12-10T05:39:51.093779: step 6542, loss 0.499935, acc 0.921875, prec 0.100659, recall 0.800406
2017-12-10T05:39:51.361945: step 6543, loss 0.588692, acc 0.921875, prec 0.100677, recall 0.800449
2017-12-10T05:39:51.626931: step 6544, loss 0.14433, acc 0.96875, prec 0.100698, recall 0.800492
2017-12-10T05:39:51.894275: step 6545, loss 0.0810186, acc 0.96875, prec 0.10072, recall 0.800534
2017-12-10T05:39:52.160546: step 6546, loss 0.0177303, acc 1, prec 0.100732, recall 0.800556
2017-12-10T05:39:52.426478: step 6547, loss 0.346813, acc 0.9375, prec 0.10075, recall 0.800598
2017-12-10T05:39:52.691718: step 6548, loss 0.0972671, acc 0.96875, prec 0.100748, recall 0.800598
2017-12-10T05:39:52.960889: step 6549, loss 0.250582, acc 0.96875, prec 0.100745, recall 0.800598
2017-12-10T05:39:53.231042: step 6550, loss 0.0698274, acc 0.984375, prec 0.100768, recall 0.800641
2017-12-10T05:39:53.495967: step 6551, loss 0.258788, acc 0.953125, prec 0.100764, recall 0.800641
2017-12-10T05:39:53.761578: step 6552, loss 0.22395, acc 0.9375, prec 0.10077, recall 0.800662
2017-12-10T05:39:54.039131: step 6553, loss 0.16372, acc 0.96875, prec 0.100804, recall 0.800726
2017-12-10T05:39:54.303517: step 6554, loss 0.0528507, acc 0.984375, prec 0.100827, recall 0.800769
2017-12-10T05:39:54.568670: step 6555, loss 0.386445, acc 0.953125, prec 0.100847, recall 0.800811
2017-12-10T05:39:54.832634: step 6556, loss 0.904821, acc 0.953125, prec 0.100843, recall 0.800811
2017-12-10T05:39:55.093967: step 6557, loss 0.455284, acc 0.96875, prec 0.100864, recall 0.800854
2017-12-10T05:39:55.360275: step 6558, loss 4.43011, acc 0.890625, prec 0.100856, recall 0.800768
2017-12-10T05:39:55.625078: step 6559, loss 0.0123495, acc 1, prec 0.100856, recall 0.800768
2017-12-10T05:39:55.909114: step 6560, loss 0.263038, acc 0.890625, prec 0.100859, recall 0.80079
2017-12-10T05:39:56.173737: step 6561, loss 0.497057, acc 0.921875, prec 0.100864, recall 0.800811
2017-12-10T05:39:56.436022: step 6562, loss 0.58163, acc 0.890625, prec 0.100867, recall 0.800832
2017-12-10T05:39:56.707443: step 6563, loss 0.554286, acc 0.953125, prec 0.100887, recall 0.800875
2017-12-10T05:39:56.978233: step 6564, loss 0.860452, acc 0.921875, prec 0.100928, recall 0.800959
2017-12-10T05:39:57.254694: step 6565, loss 0.169315, acc 0.96875, prec 0.100938, recall 0.800981
2017-12-10T05:39:57.526049: step 6566, loss 0.279585, acc 0.96875, prec 0.100935, recall 0.800981
2017-12-10T05:39:57.800910: step 6567, loss 0.513526, acc 0.921875, prec 0.10094, recall 0.801002
2017-12-10T05:39:58.072096: step 6568, loss 0.203996, acc 0.96875, prec 0.100974, recall 0.801066
2017-12-10T05:39:58.340172: step 6569, loss 0.0805088, acc 0.96875, prec 0.101007, recall 0.801129
2017-12-10T05:39:58.601881: step 6570, loss 0.524135, acc 0.9375, prec 0.101014, recall 0.80115
2017-12-10T05:39:58.873514: step 6571, loss 0.222124, acc 0.9375, prec 0.101021, recall 0.801171
2017-12-10T05:39:59.135435: step 6572, loss 0.280617, acc 0.9375, prec 0.101015, recall 0.801171
2017-12-10T05:39:59.402534: step 6573, loss 0.108315, acc 0.984375, prec 0.101014, recall 0.801171
2017-12-10T05:39:59.678295: step 6574, loss 0.448641, acc 0.921875, prec 0.101019, recall 0.801193
2017-12-10T05:39:59.942859: step 6575, loss 0.353037, acc 0.953125, prec 0.101027, recall 0.801214
2017-12-10T05:40:00.213135: step 6576, loss 0.103732, acc 0.96875, prec 0.101036, recall 0.801235
2017-12-10T05:40:00.486519: step 6577, loss 0.849171, acc 0.9375, prec 0.101043, recall 0.801256
2017-12-10T05:40:00.754669: step 6578, loss 0.000257761, acc 1, prec 0.101055, recall 0.801277
2017-12-10T05:40:01.022543: step 6579, loss 0.00989851, acc 1, prec 0.101079, recall 0.80132
2017-12-10T05:40:01.299057: step 6580, loss 0.0224558, acc 0.984375, prec 0.10109, recall 0.801341
2017-12-10T05:40:01.564399: step 6581, loss 0.581475, acc 0.984375, prec 0.101101, recall 0.801362
2017-12-10T05:40:01.830288: step 6582, loss 0.00702548, acc 1, prec 0.101101, recall 0.801362
2017-12-10T05:40:02.104229: step 6583, loss 0.115716, acc 0.953125, prec 0.101097, recall 0.801362
2017-12-10T05:40:02.369435: step 6584, loss 0.254789, acc 0.96875, prec 0.101106, recall 0.801383
2017-12-10T05:40:02.637797: step 6585, loss 0.386305, acc 0.96875, prec 0.101103, recall 0.801383
2017-12-10T05:40:02.908403: step 6586, loss 0.152898, acc 0.9375, prec 0.10111, recall 0.801404
2017-12-10T05:40:03.175520: step 6587, loss 0.286005, acc 0.9375, prec 0.101104, recall 0.801404
2017-12-10T05:40:03.438739: step 6588, loss 0.372372, acc 0.953125, prec 0.101112, recall 0.801425
2017-12-10T05:40:03.707141: step 6589, loss 1.11131, acc 0.953125, prec 0.10112, recall 0.801446
2017-12-10T05:40:03.979265: step 6590, loss 0.12054, acc 0.96875, prec 0.101118, recall 0.801446
2017-12-10T05:40:04.245189: step 6591, loss 0.106656, acc 0.96875, prec 0.101127, recall 0.801467
2017-12-10T05:40:04.510025: step 6592, loss 0.134094, acc 0.984375, prec 0.101138, recall 0.801489
2017-12-10T05:40:04.775222: step 6593, loss 0.158301, acc 0.984375, prec 0.101148, recall 0.80151
2017-12-10T05:40:05.044815: step 6594, loss 0.0107744, acc 1, prec 0.101173, recall 0.801552
2017-12-10T05:40:05.315312: step 6595, loss 0.279228, acc 0.984375, prec 0.101183, recall 0.801573
2017-12-10T05:40:05.584396: step 6596, loss 0.00363208, acc 1, prec 0.101183, recall 0.801573
2017-12-10T05:40:05.845175: step 6597, loss 0.0496047, acc 0.984375, prec 0.101218, recall 0.801636
2017-12-10T05:40:06.125866: step 6598, loss 0.179832, acc 0.96875, prec 0.101227, recall 0.801657
2017-12-10T05:40:06.389412: step 6599, loss 0.0580276, acc 0.984375, prec 0.101238, recall 0.801678
2017-12-10T05:40:06.658716: step 6600, loss 0.0108764, acc 1, prec 0.101238, recall 0.801678

Evaluation:
2017-12-10T05:40:14.341908: step 6600, loss 9.9453, acc 0.960559, prec 0.101436, recall 0.794127

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-6600

2017-12-10T05:40:15.519352: step 6601, loss 0.00159809, acc 1, prec 0.101448, recall 0.794148
2017-12-10T05:40:15.786761: step 6602, loss 0.101812, acc 0.984375, prec 0.101447, recall 0.794148
2017-12-10T05:40:16.050082: step 6603, loss 0.349794, acc 0.953125, prec 0.101443, recall 0.794148
2017-12-10T05:40:16.315049: step 6604, loss 0.0769843, acc 0.96875, prec 0.10144, recall 0.794148
2017-12-10T05:40:16.583022: step 6605, loss 0.417035, acc 0.96875, prec 0.101437, recall 0.794148
2017-12-10T05:40:16.856145: step 6606, loss 4.07595, acc 0.953125, prec 0.101447, recall 0.794087
2017-12-10T05:40:17.130447: step 6607, loss 0.0343163, acc 0.984375, prec 0.101457, recall 0.794108
2017-12-10T05:40:17.397972: step 6608, loss 0.0614311, acc 0.96875, prec 0.101467, recall 0.79413
2017-12-10T05:40:17.664674: step 6609, loss 0.0104059, acc 1, prec 0.101479, recall 0.794151
2017-12-10T05:40:17.927048: step 6610, loss 0.326616, acc 0.9375, prec 0.101497, recall 0.794194
2017-12-10T05:40:18.909208: step 6611, loss 0.127941, acc 0.984375, prec 0.101508, recall 0.794216
2017-12-10T05:40:19.578774: step 6612, loss 0.206942, acc 0.96875, prec 0.101505, recall 0.794216
2017-12-10T05:40:20.326250: step 6613, loss 0.194963, acc 0.953125, prec 0.101525, recall 0.794259
2017-12-10T05:40:20.617790: step 6614, loss 0.655988, acc 0.890625, prec 0.101516, recall 0.794259
2017-12-10T05:40:20.893643: step 6615, loss 0.177238, acc 0.9375, prec 0.10151, recall 0.794259
2017-12-10T05:40:21.174758: step 6616, loss 0.0601033, acc 0.984375, prec 0.101509, recall 0.794259
2017-12-10T05:40:21.443198: step 6617, loss 0.141015, acc 0.953125, prec 0.101505, recall 0.794259
2017-12-10T05:40:21.708485: step 6618, loss 0.400044, acc 0.90625, prec 0.101521, recall 0.794302
2017-12-10T05:40:21.978623: step 6619, loss 0.0747171, acc 0.96875, prec 0.101518, recall 0.794302
2017-12-10T05:40:22.249626: step 6620, loss 0.0811166, acc 0.953125, prec 0.101514, recall 0.794302
2017-12-10T05:40:22.509431: step 6621, loss 0.480124, acc 0.921875, prec 0.101519, recall 0.794323
2017-12-10T05:40:22.776764: step 6622, loss 0.453744, acc 0.953125, prec 0.101515, recall 0.794323
2017-12-10T05:40:23.044325: step 6623, loss 0.186141, acc 0.96875, prec 0.101524, recall 0.794345
2017-12-10T05:40:23.314126: step 6624, loss 0.45284, acc 0.953125, prec 0.101532, recall 0.794366
2017-12-10T05:40:23.581704: step 6625, loss 0.0706727, acc 0.953125, prec 0.10154, recall 0.794388
2017-12-10T05:40:23.851976: step 6626, loss 0.943624, acc 0.921875, prec 0.101545, recall 0.794409
2017-12-10T05:40:24.117880: step 6627, loss 0.0461893, acc 0.984375, prec 0.10158, recall 0.794473
2017-12-10T05:40:24.386552: step 6628, loss 0.222247, acc 0.96875, prec 0.101577, recall 0.794473
2017-12-10T05:40:24.653954: step 6629, loss 0.00271573, acc 1, prec 0.101577, recall 0.794473
2017-12-10T05:40:24.925095: step 6630, loss 0.00314154, acc 1, prec 0.101577, recall 0.794473
2017-12-10T05:40:25.191608: step 6631, loss 0.237212, acc 0.984375, prec 0.101576, recall 0.794473
2017-12-10T05:40:25.465941: step 6632, loss 0.000619903, acc 1, prec 0.101576, recall 0.794473
2017-12-10T05:40:25.732308: step 6633, loss 0.00530645, acc 1, prec 0.101588, recall 0.794495
2017-12-10T05:40:26.006673: step 6634, loss 0.00711225, acc 1, prec 0.1016, recall 0.794516
2017-12-10T05:40:26.272825: step 6635, loss 0.0190729, acc 1, prec 0.101612, recall 0.794538
2017-12-10T05:40:26.541149: step 6636, loss 0.256965, acc 0.984375, prec 0.101646, recall 0.794602
2017-12-10T05:40:26.816862: step 6637, loss 0.18609, acc 0.984375, prec 0.101657, recall 0.794623
2017-12-10T05:40:27.082070: step 6638, loss 0.0623582, acc 0.984375, prec 0.101692, recall 0.794688
2017-12-10T05:40:27.354998: step 6639, loss 0.170387, acc 0.984375, prec 0.10169, recall 0.794688
2017-12-10T05:40:27.620910: step 6640, loss 0.0717119, acc 0.984375, prec 0.101689, recall 0.794688
2017-12-10T05:40:27.891696: step 6641, loss 0.264439, acc 0.953125, prec 0.101685, recall 0.794688
2017-12-10T05:40:28.159356: step 6642, loss 0.177781, acc 0.984375, prec 0.101695, recall 0.794709
2017-12-10T05:40:28.429259: step 6643, loss 0.0771774, acc 0.984375, prec 0.10173, recall 0.794773
2017-12-10T05:40:28.701660: step 6644, loss 0.0872596, acc 0.984375, prec 0.101741, recall 0.794794
2017-12-10T05:40:28.963963: step 6645, loss 0.0115823, acc 1, prec 0.101753, recall 0.794816
2017-12-10T05:40:29.224971: step 6646, loss 2.77317, acc 0.984375, prec 0.101753, recall 0.794733
2017-12-10T05:40:29.496998: step 6647, loss 8.41398, acc 0.921875, prec 0.101747, recall 0.79465
2017-12-10T05:40:29.773102: step 6648, loss 0.0825051, acc 0.96875, prec 0.101744, recall 0.79465
2017-12-10T05:40:30.038016: step 6649, loss 0.577041, acc 0.9375, prec 0.101751, recall 0.794672
2017-12-10T05:40:30.304919: step 6650, loss 0.276595, acc 0.9375, prec 0.101769, recall 0.794714
2017-12-10T05:40:30.576897: step 6651, loss 0.142785, acc 0.96875, prec 0.101791, recall 0.794757
2017-12-10T05:40:30.849454: step 6652, loss 0.3223, acc 0.953125, prec 0.101787, recall 0.794757
2017-12-10T05:40:31.117727: step 6653, loss 0.500456, acc 0.84375, prec 0.101773, recall 0.794757
2017-12-10T05:40:31.384759: step 6654, loss 1.15604, acc 0.828125, prec 0.101758, recall 0.794757
2017-12-10T05:40:31.659433: step 6655, loss 0.47637, acc 0.921875, prec 0.101751, recall 0.794757
2017-12-10T05:40:31.935650: step 6656, loss 0.253421, acc 0.9375, prec 0.101746, recall 0.794757
2017-12-10T05:40:32.209075: step 6657, loss 0.956688, acc 0.796875, prec 0.101728, recall 0.794757
2017-12-10T05:40:32.482967: step 6658, loss 0.765272, acc 0.8125, prec 0.101736, recall 0.7948
2017-12-10T05:40:32.753981: step 6659, loss 0.518966, acc 0.90625, prec 0.10174, recall 0.794821
2017-12-10T05:40:33.024813: step 6660, loss 0.787518, acc 0.78125, prec 0.101745, recall 0.794864
2017-12-10T05:40:33.295373: step 6661, loss 1.12861, acc 0.828125, prec 0.101742, recall 0.794885
2017-12-10T05:40:33.565977: step 6662, loss 0.524661, acc 0.890625, prec 0.101768, recall 0.794949
2017-12-10T05:40:33.837141: step 6663, loss 0.835275, acc 0.828125, prec 0.101753, recall 0.794949
2017-12-10T05:40:34.104624: step 6664, loss 0.19133, acc 0.96875, prec 0.101763, recall 0.79497
2017-12-10T05:40:34.373995: step 6665, loss 0.695148, acc 0.875, prec 0.101752, recall 0.79497
2017-12-10T05:40:34.641987: step 6666, loss 0.290814, acc 0.90625, prec 0.101779, recall 0.795034
2017-12-10T05:40:34.914344: step 6667, loss 0.224217, acc 0.953125, prec 0.101775, recall 0.795034
2017-12-10T05:40:35.181612: step 6668, loss 0.198034, acc 0.90625, prec 0.101803, recall 0.795098
2017-12-10T05:40:35.448672: step 6669, loss 0.132856, acc 0.984375, prec 0.101814, recall 0.795119
2017-12-10T05:40:35.716305: step 6670, loss 0.187751, acc 0.96875, prec 0.101835, recall 0.795162
2017-12-10T05:40:35.988974: step 6671, loss 0.246035, acc 0.9375, prec 0.101853, recall 0.795204
2017-12-10T05:40:36.256660: step 6672, loss 0.108219, acc 0.96875, prec 0.101875, recall 0.795247
2017-12-10T05:40:36.524774: step 6673, loss 0.406601, acc 0.9375, prec 0.101869, recall 0.795247
2017-12-10T05:40:36.794573: step 6674, loss 0.132311, acc 0.96875, prec 0.101866, recall 0.795247
2017-12-10T05:40:37.056927: step 6675, loss 0.0108727, acc 1, prec 0.101866, recall 0.795247
2017-12-10T05:40:37.323035: step 6676, loss 0.00037014, acc 1, prec 0.101866, recall 0.795247
2017-12-10T05:40:37.586506: step 6677, loss 0.00484497, acc 1, prec 0.10189, recall 0.795289
2017-12-10T05:40:37.847039: step 6678, loss 0.00265344, acc 1, prec 0.10189, recall 0.795289
2017-12-10T05:40:38.114437: step 6679, loss 0.109409, acc 0.984375, prec 0.101913, recall 0.795332
2017-12-10T05:40:38.376466: step 6680, loss 0.0533688, acc 0.984375, prec 0.101923, recall 0.795353
2017-12-10T05:40:38.647335: step 6681, loss 0.0629099, acc 0.96875, prec 0.101956, recall 0.795417
2017-12-10T05:40:38.914577: step 6682, loss 0.0521907, acc 0.984375, prec 0.101967, recall 0.795438
2017-12-10T05:40:39.184522: step 6683, loss 0.118886, acc 0.984375, prec 0.101966, recall 0.795438
2017-12-10T05:40:39.455346: step 6684, loss 0.0233888, acc 0.984375, prec 0.101976, recall 0.795459
2017-12-10T05:40:39.723670: step 6685, loss 0.00759871, acc 1, prec 0.101976, recall 0.795459
2017-12-10T05:40:39.987981: step 6686, loss 0.44523, acc 0.953125, prec 0.101972, recall 0.795459
2017-12-10T05:40:40.257873: step 6687, loss 0.00113741, acc 1, prec 0.101984, recall 0.79548
2017-12-10T05:40:40.531588: step 6688, loss 0.324864, acc 0.953125, prec 0.102004, recall 0.795523
2017-12-10T05:40:40.799165: step 6689, loss 0.144345, acc 0.96875, prec 0.102013, recall 0.795544
2017-12-10T05:40:41.067049: step 6690, loss 0.0303463, acc 0.984375, prec 0.102012, recall 0.795544
2017-12-10T05:40:41.331713: step 6691, loss 0.553016, acc 0.96875, prec 0.102009, recall 0.795544
2017-12-10T05:40:41.600468: step 6692, loss 0.00275091, acc 1, prec 0.102009, recall 0.795544
2017-12-10T05:40:41.876151: step 6693, loss 0.043402, acc 0.984375, prec 0.10202, recall 0.795565
2017-12-10T05:40:42.160431: step 6694, loss 0.10662, acc 0.984375, prec 0.102018, recall 0.795565
2017-12-10T05:40:42.429419: step 6695, loss 0.0728289, acc 0.984375, prec 0.102017, recall 0.795565
2017-12-10T05:40:42.696054: step 6696, loss 0.000112562, acc 1, prec 0.102017, recall 0.795565
2017-12-10T05:40:42.957734: step 6697, loss 0.146921, acc 0.984375, prec 0.102016, recall 0.795565
2017-12-10T05:40:43.223143: step 6698, loss 7.86395, acc 0.984375, prec 0.102016, recall 0.795483
2017-12-10T05:40:43.492201: step 6699, loss 4.66761, acc 0.984375, prec 0.102016, recall 0.7954
2017-12-10T05:40:43.763217: step 6700, loss 0.0339755, acc 0.984375, prec 0.102014, recall 0.7954
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-6700

2017-12-10T05:40:45.091389: step 6701, loss 0.290814, acc 0.984375, prec 0.102049, recall 0.795464
2017-12-10T05:40:45.361752: step 6702, loss 0.11955, acc 0.96875, prec 0.102046, recall 0.795464
2017-12-10T05:40:45.633493: step 6703, loss 0.0091021, acc 1, prec 0.10207, recall 0.795506
2017-12-10T05:40:45.900546: step 6704, loss 0.35429, acc 0.96875, prec 0.102091, recall 0.795549
2017-12-10T05:40:46.167969: step 6705, loss 0.55576, acc 0.921875, prec 0.102084, recall 0.795549
2017-12-10T05:40:46.433760: step 6706, loss 1.10524, acc 0.875, prec 0.102073, recall 0.795549
2017-12-10T05:40:46.701250: step 6707, loss 0.174163, acc 0.9375, prec 0.102068, recall 0.795549
2017-12-10T05:40:46.968568: step 6708, loss 0.608661, acc 0.90625, prec 0.102072, recall 0.79557
2017-12-10T05:40:47.247362: step 6709, loss 0.384728, acc 0.890625, prec 0.102062, recall 0.79557
2017-12-10T05:40:47.510964: step 6710, loss 0.448576, acc 0.921875, prec 0.102055, recall 0.79557
2017-12-10T05:40:47.781199: step 6711, loss 0.854294, acc 0.84375, prec 0.102042, recall 0.79557
2017-12-10T05:40:48.054368: step 6712, loss 1.02171, acc 0.78125, prec 0.102035, recall 0.795591
2017-12-10T05:40:48.324071: step 6713, loss 0.222395, acc 0.90625, prec 0.102027, recall 0.795591
2017-12-10T05:40:48.585892: step 6714, loss 0.577477, acc 0.875, prec 0.10204, recall 0.795633
2017-12-10T05:40:48.854838: step 6715, loss 1.0076, acc 0.796875, prec 0.102034, recall 0.795654
2017-12-10T05:40:49.125082: step 6716, loss 0.977667, acc 0.875, prec 0.102047, recall 0.795697
2017-12-10T05:40:49.394118: step 6717, loss 0.812451, acc 0.8125, prec 0.102043, recall 0.795718
2017-12-10T05:40:49.659312: step 6718, loss 0.89119, acc 0.796875, prec 0.102025, recall 0.795718
2017-12-10T05:40:49.924774: step 6719, loss 0.274936, acc 0.953125, prec 0.102021, recall 0.795718
2017-12-10T05:40:50.192554: step 6720, loss 0.526975, acc 0.953125, prec 0.102065, recall 0.795802
2017-12-10T05:40:50.459746: step 6721, loss 0.33831, acc 0.953125, prec 0.102084, recall 0.795845
2017-12-10T05:40:50.729041: step 6722, loss 0.232793, acc 0.9375, prec 0.102103, recall 0.795887
2017-12-10T05:40:51.005048: step 6723, loss 0.211543, acc 0.953125, prec 0.102123, recall 0.795929
2017-12-10T05:40:51.278729: step 6724, loss 0.091602, acc 0.984375, prec 0.102145, recall 0.795971
2017-12-10T05:40:51.543718: step 6725, loss 0.278727, acc 0.9375, prec 0.102151, recall 0.795992
2017-12-10T05:40:51.813900: step 6726, loss 0.0448321, acc 0.96875, prec 0.102149, recall 0.795992
2017-12-10T05:40:52.089271: step 6727, loss 0.985774, acc 0.953125, prec 0.102168, recall 0.796034
2017-12-10T05:40:52.359255: step 6728, loss 0.447497, acc 0.9375, prec 0.102163, recall 0.796034
2017-12-10T05:40:52.631876: step 6729, loss 0.0314604, acc 1, prec 0.102199, recall 0.796097
2017-12-10T05:40:52.898606: step 6730, loss 0.357593, acc 0.96875, prec 0.102196, recall 0.796097
2017-12-10T05:40:53.163868: step 6731, loss 0.014736, acc 0.984375, prec 0.102195, recall 0.796097
2017-12-10T05:40:53.435710: step 6732, loss 0.0151028, acc 0.984375, prec 0.102205, recall 0.796118
2017-12-10T05:40:53.706732: step 6733, loss 0.209264, acc 0.96875, prec 0.102226, recall 0.796161
2017-12-10T05:40:53.975880: step 6734, loss 0.0710566, acc 0.984375, prec 0.102225, recall 0.796161
2017-12-10T05:40:54.245685: step 6735, loss 0.0442261, acc 0.96875, prec 0.102222, recall 0.796161
2017-12-10T05:40:54.510946: step 6736, loss 0.0104644, acc 1, prec 0.102234, recall 0.796182
2017-12-10T05:40:54.773302: step 6737, loss 0.212947, acc 0.984375, prec 0.102257, recall 0.796224
2017-12-10T05:40:55.049043: step 6738, loss 0.069954, acc 0.96875, prec 0.102254, recall 0.796224
2017-12-10T05:40:55.318696: step 6739, loss 0.00911382, acc 1, prec 0.102254, recall 0.796224
2017-12-10T05:40:55.579887: step 6740, loss 8.84315e-05, acc 1, prec 0.102266, recall 0.796245
2017-12-10T05:40:55.848020: step 6741, loss 0.0604476, acc 0.984375, prec 0.102276, recall 0.796266
2017-12-10T05:40:56.113117: step 6742, loss 0.368905, acc 0.96875, prec 0.102274, recall 0.796266
2017-12-10T05:40:56.386479: step 6743, loss 0.0660443, acc 0.96875, prec 0.102283, recall 0.796287
2017-12-10T05:40:56.651619: step 6744, loss 0.0570117, acc 0.984375, prec 0.102293, recall 0.796308
2017-12-10T05:40:56.920024: step 6745, loss 0.00381422, acc 1, prec 0.102305, recall 0.796329
2017-12-10T05:40:57.186531: step 6746, loss 0.0118537, acc 0.984375, prec 0.102304, recall 0.796329
2017-12-10T05:40:57.470712: step 6747, loss 0.199384, acc 1, prec 0.102316, recall 0.79635
2017-12-10T05:40:57.750650: step 6748, loss 0.0890903, acc 0.984375, prec 0.102314, recall 0.79635
2017-12-10T05:40:58.012768: step 6749, loss 0.00011856, acc 1, prec 0.10235, recall 0.796413
2017-12-10T05:40:58.271098: step 6750, loss 3.56958, acc 0.984375, prec 0.102362, recall 0.796352
2017-12-10T05:40:58.536560: step 6751, loss 0.000218136, acc 1, prec 0.102374, recall 0.796373
2017-12-10T05:40:58.797780: step 6752, loss 0.0812438, acc 0.96875, prec 0.102383, recall 0.796394
2017-12-10T05:40:59.061646: step 6753, loss 0.0671201, acc 0.984375, prec 0.102394, recall 0.796415
2017-12-10T05:40:59.329806: step 6754, loss 0.247817, acc 0.96875, prec 0.102391, recall 0.796415
2017-12-10T05:40:59.598183: step 6755, loss 0.557596, acc 0.96875, prec 0.102412, recall 0.796457
2017-12-10T05:40:59.867625: step 6756, loss 0.171394, acc 0.984375, prec 0.102434, recall 0.796498
2017-12-10T05:41:00.135124: step 6757, loss 0.360736, acc 0.953125, prec 0.10243, recall 0.796498
2017-12-10T05:41:00.412275: step 6758, loss 0.0833181, acc 0.96875, prec 0.102428, recall 0.796498
2017-12-10T05:41:00.687779: step 6759, loss 0.203856, acc 0.984375, prec 0.102426, recall 0.796498
2017-12-10T05:41:00.955475: step 6760, loss 0.214574, acc 0.9375, prec 0.102433, recall 0.796519
2017-12-10T05:41:01.225900: step 6761, loss 0.0912121, acc 0.953125, prec 0.102441, recall 0.79654
2017-12-10T05:41:01.499696: step 6762, loss 0.301017, acc 0.921875, prec 0.102446, recall 0.796561
2017-12-10T05:41:01.770304: step 6763, loss 0.016289, acc 1, prec 0.102457, recall 0.796582
2017-12-10T05:41:02.048685: step 6764, loss 0.5394, acc 0.9375, prec 0.102452, recall 0.796582
2017-12-10T05:41:02.315142: step 6765, loss 0.0920274, acc 0.96875, prec 0.102473, recall 0.796624
2017-12-10T05:41:02.584253: step 6766, loss 0.0889515, acc 0.984375, prec 0.102472, recall 0.796624
2017-12-10T05:41:02.860532: step 6767, loss 0.192896, acc 0.9375, prec 0.102466, recall 0.796624
2017-12-10T05:41:03.126840: step 6768, loss 0.0124296, acc 0.984375, prec 0.102465, recall 0.796624
2017-12-10T05:41:03.391924: step 6769, loss 0.211377, acc 0.953125, prec 0.102485, recall 0.796666
2017-12-10T05:41:03.663285: step 6770, loss 0.334917, acc 0.96875, prec 0.102482, recall 0.796666
2017-12-10T05:41:03.946599: step 6771, loss 0.0575075, acc 0.984375, prec 0.102481, recall 0.796666
2017-12-10T05:41:04.216567: step 6772, loss 0.00314759, acc 1, prec 0.102504, recall 0.796708
2017-12-10T05:41:04.483499: step 6773, loss 0.0444422, acc 1, prec 0.102528, recall 0.79675
2017-12-10T05:41:04.756924: step 6774, loss 0.0858045, acc 0.984375, prec 0.102551, recall 0.796791
2017-12-10T05:41:05.026753: step 6775, loss 0.305395, acc 0.96875, prec 0.102548, recall 0.796791
2017-12-10T05:41:05.301568: step 6776, loss 0.0161211, acc 0.984375, prec 0.102558, recall 0.796812
2017-12-10T05:41:05.577375: step 6777, loss 0.000286041, acc 1, prec 0.10257, recall 0.796833
2017-12-10T05:41:05.838191: step 6778, loss 0.00368883, acc 1, prec 0.102582, recall 0.796854
2017-12-10T05:41:06.101695: step 6779, loss 0.0717228, acc 0.984375, prec 0.102593, recall 0.796875
2017-12-10T05:41:06.376424: step 6780, loss 0.444134, acc 0.953125, prec 0.102636, recall 0.796959
2017-12-10T05:41:06.654300: step 6781, loss 1.79915, acc 0.984375, prec 0.102636, recall 0.796877
2017-12-10T05:41:06.931992: step 6782, loss 0.0313876, acc 0.984375, prec 0.102635, recall 0.796877
2017-12-10T05:41:07.203771: step 6783, loss 0.359102, acc 0.953125, prec 0.102631, recall 0.796877
2017-12-10T05:41:07.478515: step 6784, loss 0.0555039, acc 0.984375, prec 0.102629, recall 0.796877
2017-12-10T05:41:07.744213: step 6785, loss 0.167755, acc 0.9375, prec 0.102671, recall 0.79696
2017-12-10T05:41:08.023514: step 6786, loss 0.0902238, acc 0.96875, prec 0.102692, recall 0.797002
2017-12-10T05:41:08.292471: step 6787, loss 0.218897, acc 0.921875, prec 0.102697, recall 0.797023
2017-12-10T05:41:08.558896: step 6788, loss 0.4735, acc 0.953125, prec 0.102705, recall 0.797043
2017-12-10T05:41:08.829583: step 6789, loss 0.110827, acc 0.96875, prec 0.102714, recall 0.797064
2017-12-10T05:41:09.098870: step 6790, loss 0.430981, acc 0.90625, prec 0.102706, recall 0.797064
2017-12-10T05:41:09.373748: step 6791, loss 0.450603, acc 0.890625, prec 0.102697, recall 0.797064
2017-12-10T05:41:09.642822: step 6792, loss 0.480578, acc 0.921875, prec 0.102702, recall 0.797085
2017-12-10T05:41:09.910701: step 6793, loss 0.193816, acc 0.921875, prec 0.102695, recall 0.797085
2017-12-10T05:41:10.182351: step 6794, loss 0.704353, acc 0.90625, prec 0.102687, recall 0.797085
2017-12-10T05:41:10.448894: step 6795, loss 0.581998, acc 0.890625, prec 0.102689, recall 0.797106
2017-12-10T05:41:10.715755: step 6796, loss 0.224965, acc 0.9375, prec 0.102684, recall 0.797106
2017-12-10T05:41:10.986558: step 6797, loss 0.155898, acc 0.953125, prec 0.102715, recall 0.797168
2017-12-10T05:41:11.257104: step 6798, loss 0.456605, acc 0.90625, prec 0.102707, recall 0.797168
2017-12-10T05:41:11.530552: step 6799, loss 0.394405, acc 0.921875, prec 0.102736, recall 0.797231
2017-12-10T05:41:11.799614: step 6800, loss 0.150924, acc 0.90625, prec 0.10274, recall 0.797252
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-6800

2017-12-10T05:41:13.053801: step 6801, loss 0.226338, acc 0.9375, prec 0.102758, recall 0.797293
2017-12-10T05:41:13.329548: step 6802, loss 0.277386, acc 0.953125, prec 0.102754, recall 0.797293
2017-12-10T05:41:13.597861: step 6803, loss 0.714905, acc 0.96875, prec 0.102763, recall 0.797314
2017-12-10T05:41:13.863527: step 6804, loss 0.0373636, acc 0.984375, prec 0.102785, recall 0.797355
2017-12-10T05:41:14.133588: step 6805, loss 0.519213, acc 0.9375, prec 0.10278, recall 0.797355
2017-12-10T05:41:14.404218: step 6806, loss 0.304347, acc 0.96875, prec 0.102801, recall 0.797397
2017-12-10T05:41:14.673748: step 6807, loss 0.374328, acc 0.984375, prec 0.102811, recall 0.797418
2017-12-10T05:41:14.949359: step 6808, loss 0.0278727, acc 0.984375, prec 0.102822, recall 0.797439
2017-12-10T05:41:15.216845: step 6809, loss 7.25549, acc 0.96875, prec 0.102821, recall 0.797357
2017-12-10T05:41:15.492841: step 6810, loss 0.386228, acc 0.953125, prec 0.102828, recall 0.797378
2017-12-10T05:41:15.756405: step 6811, loss 0.0833367, acc 0.96875, prec 0.102849, recall 0.797419
2017-12-10T05:41:16.024952: step 6812, loss 0.206168, acc 0.9375, prec 0.102856, recall 0.79744
2017-12-10T05:41:16.289677: step 6813, loss 0.226914, acc 0.9375, prec 0.102862, recall 0.797461
2017-12-10T05:41:16.555900: step 6814, loss 0.532411, acc 0.921875, prec 0.102855, recall 0.797461
2017-12-10T05:41:16.826841: step 6815, loss 0.469427, acc 0.9375, prec 0.10285, recall 0.797461
2017-12-10T05:41:17.090471: step 6816, loss 0.209642, acc 0.96875, prec 0.102847, recall 0.797461
2017-12-10T05:41:17.365104: step 6817, loss 0.466733, acc 0.90625, prec 0.102851, recall 0.797481
2017-12-10T05:41:17.630250: step 6818, loss 0.74386, acc 0.90625, prec 0.102843, recall 0.797481
2017-12-10T05:41:17.897190: step 6819, loss 0.382353, acc 0.90625, prec 0.102835, recall 0.797481
2017-12-10T05:41:18.169675: step 6820, loss 0.9516, acc 0.90625, prec 0.102826, recall 0.797481
2017-12-10T05:41:18.437291: step 6821, loss 0.550665, acc 0.90625, prec 0.10283, recall 0.797502
2017-12-10T05:41:18.711959: step 6822, loss 0.350997, acc 0.90625, prec 0.102834, recall 0.797523
2017-12-10T05:41:18.987201: step 6823, loss 0.324132, acc 0.921875, prec 0.102851, recall 0.797564
2017-12-10T05:41:19.256193: step 6824, loss 0.761613, acc 0.921875, prec 0.102844, recall 0.797564
2017-12-10T05:41:19.525793: step 6825, loss 0.713211, acc 0.90625, prec 0.102859, recall 0.797606
2017-12-10T05:41:19.800898: step 6826, loss 0.157513, acc 0.953125, prec 0.102867, recall 0.797626
2017-12-10T05:41:20.066873: step 6827, loss 0.487023, acc 0.890625, prec 0.10287, recall 0.797647
2017-12-10T05:41:20.333520: step 6828, loss 0.21733, acc 0.953125, prec 0.102901, recall 0.797709
2017-12-10T05:41:20.597298: step 6829, loss 0.686804, acc 0.921875, prec 0.102906, recall 0.79773
2017-12-10T05:41:20.866370: step 6830, loss 0.202551, acc 0.953125, prec 0.102902, recall 0.79773
2017-12-10T05:41:21.132667: step 6831, loss 0.0576108, acc 0.96875, prec 0.102899, recall 0.79773
2017-12-10T05:41:21.397237: step 6832, loss 0.00330239, acc 1, prec 0.102899, recall 0.79773
2017-12-10T05:41:21.669602: step 6833, loss 0.290184, acc 0.9375, prec 0.102906, recall 0.797751
2017-12-10T05:41:21.934269: step 6834, loss 0.243704, acc 0.953125, prec 0.102913, recall 0.797771
2017-12-10T05:41:22.203226: step 6835, loss 0.340908, acc 0.9375, prec 0.102908, recall 0.797771
2017-12-10T05:41:22.469743: step 6836, loss 0.116746, acc 0.96875, prec 0.102905, recall 0.797771
2017-12-10T05:41:22.735951: step 6837, loss 0.698402, acc 0.9375, prec 0.1029, recall 0.797771
2017-12-10T05:41:23.007615: step 6838, loss 0.574184, acc 0.96875, prec 0.102921, recall 0.797813
2017-12-10T05:41:23.271997: step 6839, loss 6.20155, acc 0.953125, prec 0.10293, recall 0.797752
2017-12-10T05:41:23.547423: step 6840, loss 0.0250616, acc 0.984375, prec 0.102929, recall 0.797752
2017-12-10T05:41:23.813386: step 6841, loss 0.266583, acc 0.953125, prec 0.102925, recall 0.797752
2017-12-10T05:41:24.076192: step 6842, loss 0.306184, acc 0.984375, prec 0.102935, recall 0.797772
2017-12-10T05:41:24.347285: step 6843, loss 0.275662, acc 0.96875, prec 0.102944, recall 0.797793
2017-12-10T05:41:24.616419: step 6844, loss 0.296174, acc 0.96875, prec 0.102941, recall 0.797793
2017-12-10T05:41:24.878385: step 6845, loss 0.518383, acc 0.96875, prec 0.102939, recall 0.797793
2017-12-10T05:41:25.147279: step 6846, loss 0.0671021, acc 0.96875, prec 0.102948, recall 0.797814
2017-12-10T05:41:25.421529: step 6847, loss 0.346217, acc 0.953125, prec 0.102956, recall 0.797834
2017-12-10T05:41:25.695203: step 6848, loss 0.0294728, acc 0.984375, prec 0.102954, recall 0.797834
2017-12-10T05:41:25.960667: step 6849, loss 0.198885, acc 0.984375, prec 0.102953, recall 0.797834
2017-12-10T05:41:26.229503: step 6850, loss 0.204773, acc 0.96875, prec 0.102974, recall 0.797876
2017-12-10T05:41:26.493357: step 6851, loss 0.0530153, acc 0.96875, prec 0.102971, recall 0.797876
2017-12-10T05:41:26.762638: step 6852, loss 0.315858, acc 0.921875, prec 0.102964, recall 0.797876
2017-12-10T05:41:27.028191: step 6853, loss 0.322223, acc 0.953125, prec 0.10296, recall 0.797876
2017-12-10T05:41:27.313553: step 6854, loss 0.473202, acc 0.90625, prec 0.102976, recall 0.797917
2017-12-10T05:41:27.581605: step 6855, loss 0.409863, acc 0.9375, prec 0.102982, recall 0.797938
2017-12-10T05:41:27.854153: step 6856, loss 0.0944836, acc 0.953125, prec 0.10299, recall 0.797958
2017-12-10T05:41:28.116749: step 6857, loss 0.00895748, acc 1, prec 0.103002, recall 0.797979
2017-12-10T05:41:28.383867: step 6858, loss 0.0412932, acc 0.984375, prec 0.103, recall 0.797979
2017-12-10T05:41:28.652206: step 6859, loss 0.141185, acc 0.953125, prec 0.103008, recall 0.797999
2017-12-10T05:41:28.926790: step 6860, loss 0.378953, acc 0.953125, prec 0.103004, recall 0.797999
2017-12-10T05:41:29.194613: step 6861, loss 0.591489, acc 0.96875, prec 0.103013, recall 0.79802
2017-12-10T05:41:29.461030: step 6862, loss 0.2382, acc 0.96875, prec 0.103022, recall 0.798041
2017-12-10T05:41:29.723530: step 6863, loss 0.305577, acc 0.9375, prec 0.103029, recall 0.798061
2017-12-10T05:41:29.993513: step 6864, loss 0.257965, acc 0.953125, prec 0.103036, recall 0.798082
2017-12-10T05:41:30.262812: step 6865, loss 1.53473, acc 0.953125, prec 0.103034, recall 0.798
2017-12-10T05:41:30.545813: step 6866, loss 0.182396, acc 0.984375, prec 0.103044, recall 0.798021
2017-12-10T05:41:30.819816: step 6867, loss 0.534994, acc 0.953125, prec 0.10304, recall 0.798021
2017-12-10T05:41:31.097371: step 6868, loss 2.29031, acc 0.921875, prec 0.103035, recall 0.79794
2017-12-10T05:41:31.370359: step 6869, loss 0.328363, acc 0.96875, prec 0.103032, recall 0.79794
2017-12-10T05:41:31.642243: step 6870, loss 0.0356702, acc 1, prec 0.103044, recall 0.79796
2017-12-10T05:41:31.917261: step 6871, loss 0.562869, acc 0.921875, prec 0.10306, recall 0.798001
2017-12-10T05:41:32.186992: step 6872, loss 0.469012, acc 0.90625, prec 0.103052, recall 0.798001
2017-12-10T05:41:32.460604: step 6873, loss 0.660922, acc 0.859375, prec 0.103087, recall 0.798084
2017-12-10T05:41:32.728386: step 6874, loss 0.552612, acc 0.890625, prec 0.10309, recall 0.798104
2017-12-10T05:41:32.993090: step 6875, loss 0.608474, acc 0.890625, prec 0.10308, recall 0.798104
2017-12-10T05:41:33.256806: step 6876, loss 1.73743, acc 0.78125, prec 0.103073, recall 0.798125
2017-12-10T05:41:33.525977: step 6877, loss 0.394504, acc 0.90625, prec 0.103065, recall 0.798125
2017-12-10T05:41:33.799472: step 6878, loss 0.61962, acc 0.875, prec 0.103054, recall 0.798125
2017-12-10T05:41:34.061987: step 6879, loss 1.22183, acc 0.859375, prec 0.103042, recall 0.798125
2017-12-10T05:41:34.323245: step 6880, loss 0.435162, acc 0.953125, prec 0.10305, recall 0.798146
2017-12-10T05:41:34.593757: step 6881, loss 0.611013, acc 0.90625, prec 0.103065, recall 0.798187
2017-12-10T05:41:34.857128: step 6882, loss 0.821529, acc 0.890625, prec 0.103055, recall 0.798187
2017-12-10T05:41:35.126787: step 6883, loss 0.948601, acc 0.90625, prec 0.103047, recall 0.798187
2017-12-10T05:41:35.389529: step 6884, loss 0.134056, acc 0.984375, prec 0.103046, recall 0.798187
2017-12-10T05:41:35.655709: step 6885, loss 0.186721, acc 0.953125, prec 0.103054, recall 0.798207
2017-12-10T05:41:35.928391: step 6886, loss 0.514996, acc 0.90625, prec 0.103046, recall 0.798207
2017-12-10T05:41:36.196193: step 6887, loss 0.221646, acc 0.953125, prec 0.103042, recall 0.798207
2017-12-10T05:41:36.463398: step 6888, loss 0.678772, acc 0.90625, prec 0.103045, recall 0.798228
2017-12-10T05:41:36.730409: step 6889, loss 1.22002, acc 0.875, prec 0.10307, recall 0.798289
2017-12-10T05:41:36.995033: step 6890, loss 0.434589, acc 0.953125, prec 0.103066, recall 0.798289
2017-12-10T05:41:37.268190: step 6891, loss 0.132588, acc 0.9375, prec 0.103072, recall 0.79831
2017-12-10T05:41:37.535126: step 6892, loss 0.344324, acc 0.953125, prec 0.10308, recall 0.79833
2017-12-10T05:41:37.801463: step 6893, loss 0.112021, acc 0.984375, prec 0.10309, recall 0.798351
2017-12-10T05:41:38.073633: step 6894, loss 0.548228, acc 0.9375, prec 0.103085, recall 0.798351
2017-12-10T05:41:38.339887: step 6895, loss 0.0291556, acc 0.984375, prec 0.103095, recall 0.798371
2017-12-10T05:41:38.604276: step 6896, loss 0.16562, acc 1, prec 0.103107, recall 0.798392
2017-12-10T05:41:38.867474: step 6897, loss 0.00226229, acc 1, prec 0.103107, recall 0.798392
2017-12-10T05:41:39.132634: step 6898, loss 0.0507939, acc 0.96875, prec 0.103104, recall 0.798392
2017-12-10T05:41:39.397202: step 6899, loss 0.080163, acc 0.984375, prec 0.103127, recall 0.798433
2017-12-10T05:41:39.663076: step 6900, loss 0.0494736, acc 0.984375, prec 0.103137, recall 0.798454

Evaluation:
2017-12-10T05:41:47.289067: step 6900, loss 10.8447, acc 0.964239, prec 0.103323, recall 0.790765

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-6900

2017-12-10T05:41:48.540832: step 6901, loss 0.310484, acc 0.96875, prec 0.103344, recall 0.790807
2017-12-10T05:41:48.806131: step 6902, loss 0.11893, acc 0.96875, prec 0.103341, recall 0.790807
2017-12-10T05:41:49.069554: step 6903, loss 0.52536, acc 0.96875, prec 0.10335, recall 0.790828
2017-12-10T05:41:49.337040: step 6904, loss 0.216904, acc 0.984375, prec 0.10336, recall 0.790849
2017-12-10T05:41:49.602852: step 6905, loss 0.431272, acc 0.984375, prec 0.103429, recall 0.790975
2017-12-10T05:41:49.870266: step 6906, loss 0.526651, acc 0.984375, prec 0.103428, recall 0.790975
2017-12-10T05:41:50.137670: step 6907, loss 0.698108, acc 0.984375, prec 0.103474, recall 0.791058
2017-12-10T05:41:50.407273: step 6908, loss 0.490769, acc 0.984375, prec 0.103507, recall 0.791121
2017-12-10T05:41:50.683531: step 6909, loss 0.000399496, acc 1, prec 0.103507, recall 0.791121
2017-12-10T05:41:50.945643: step 6910, loss 0.00207461, acc 1, prec 0.103519, recall 0.791142
2017-12-10T05:41:51.217494: step 6911, loss 0.0788044, acc 0.984375, prec 0.103529, recall 0.791163
2017-12-10T05:41:51.483737: step 6912, loss 0.114074, acc 0.953125, prec 0.103525, recall 0.791163
2017-12-10T05:41:51.762524: step 6913, loss 0.125953, acc 0.984375, prec 0.103559, recall 0.791225
2017-12-10T05:41:52.032815: step 6914, loss 0.0945378, acc 0.984375, prec 0.103593, recall 0.791288
2017-12-10T05:41:52.301821: step 6915, loss 0.00887786, acc 1, prec 0.103593, recall 0.791288
2017-12-10T05:41:52.570412: step 6916, loss 0.0564175, acc 0.984375, prec 0.103603, recall 0.791309
2017-12-10T05:41:52.840049: step 6917, loss 0.0913908, acc 0.96875, prec 0.103601, recall 0.791309
2017-12-10T05:41:53.104743: step 6918, loss 0.138026, acc 0.96875, prec 0.10361, recall 0.79133
2017-12-10T05:41:53.373228: step 6919, loss 0.0854813, acc 0.96875, prec 0.103619, recall 0.79135
2017-12-10T05:41:53.638406: step 6920, loss 0.0397868, acc 0.984375, prec 0.103629, recall 0.791371
2017-12-10T05:41:53.908714: step 6921, loss 0.212859, acc 0.96875, prec 0.103626, recall 0.791371
2017-12-10T05:41:54.175910: step 6922, loss 0.00673117, acc 1, prec 0.103662, recall 0.791434
2017-12-10T05:41:54.444206: step 6923, loss 0.0185832, acc 0.984375, prec 0.10366, recall 0.791434
2017-12-10T05:41:54.723960: step 6924, loss 0.255588, acc 0.96875, prec 0.103681, recall 0.791475
2017-12-10T05:41:54.988931: step 6925, loss 0.00989242, acc 1, prec 0.103693, recall 0.791496
2017-12-10T05:41:55.254591: step 6926, loss 0.381518, acc 0.953125, prec 0.1037, recall 0.791517
2017-12-10T05:41:55.524967: step 6927, loss 0.00384624, acc 1, prec 0.1037, recall 0.791517
2017-12-10T05:41:55.792516: step 6928, loss 0.16949, acc 0.96875, prec 0.103709, recall 0.791538
2017-12-10T05:41:56.061430: step 6929, loss 0.0304287, acc 0.984375, prec 0.10372, recall 0.791559
2017-12-10T05:41:56.339025: step 6930, loss 0.418026, acc 0.9375, prec 0.103714, recall 0.791559
2017-12-10T05:41:56.608611: step 6931, loss 0.122129, acc 0.984375, prec 0.103713, recall 0.791559
2017-12-10T05:41:56.882921: step 6932, loss 0.144957, acc 0.96875, prec 0.10371, recall 0.791559
2017-12-10T05:41:57.154823: step 6933, loss 0.116281, acc 0.96875, prec 0.103731, recall 0.7916
2017-12-10T05:41:57.436410: step 6934, loss 0.124204, acc 0.96875, prec 0.103728, recall 0.7916
2017-12-10T05:41:57.711799: step 6935, loss 0.000145199, acc 1, prec 0.103728, recall 0.7916
2017-12-10T05:41:57.986944: step 6936, loss 0.0605757, acc 0.96875, prec 0.103725, recall 0.7916
2017-12-10T05:41:58.254578: step 6937, loss 0.0540861, acc 0.96875, prec 0.103723, recall 0.7916
2017-12-10T05:41:58.524520: step 6938, loss 0.114996, acc 0.984375, prec 0.103757, recall 0.791663
2017-12-10T05:41:58.794790: step 6939, loss 0.225706, acc 0.984375, prec 0.103767, recall 0.791683
2017-12-10T05:41:59.061497: step 6940, loss 8.71004, acc 0.96875, prec 0.103777, recall 0.791625
2017-12-10T05:41:59.328454: step 6941, loss 0.171951, acc 0.984375, prec 0.103776, recall 0.791625
2017-12-10T05:41:59.592836: step 6942, loss 0.281599, acc 0.9375, prec 0.103782, recall 0.791646
2017-12-10T05:41:59.856820: step 6943, loss 0.0530598, acc 0.96875, prec 0.103791, recall 0.791667
2017-12-10T05:42:00.128091: step 6944, loss 0.799584, acc 0.9375, prec 0.103798, recall 0.791687
2017-12-10T05:42:00.405249: step 6945, loss 0.0764747, acc 0.984375, prec 0.103808, recall 0.791708
2017-12-10T05:42:00.674661: step 6946, loss 0.177924, acc 0.921875, prec 0.103824, recall 0.79175
2017-12-10T05:42:00.940341: step 6947, loss 0.135747, acc 0.9375, prec 0.103831, recall 0.79177
2017-12-10T05:42:01.208528: step 6948, loss 0.576602, acc 0.921875, prec 0.103847, recall 0.791812
2017-12-10T05:42:01.469441: step 6949, loss 0.13768, acc 0.953125, prec 0.103867, recall 0.791853
2017-12-10T05:42:01.735423: step 6950, loss 0.200681, acc 0.953125, prec 0.103863, recall 0.791853
2017-12-10T05:42:02.003378: step 6951, loss 0.654512, acc 0.875, prec 0.103875, recall 0.791895
2017-12-10T05:42:02.275959: step 6952, loss 0.107164, acc 0.96875, prec 0.103884, recall 0.791916
2017-12-10T05:42:02.546302: step 6953, loss 0.297127, acc 0.9375, prec 0.103891, recall 0.791936
2017-12-10T05:42:02.812476: step 6954, loss 0.164938, acc 0.9375, prec 0.103885, recall 0.791936
2017-12-10T05:42:03.074769: step 6955, loss 0.101659, acc 0.96875, prec 0.103882, recall 0.791936
2017-12-10T05:42:03.346850: step 6956, loss 0.18576, acc 0.921875, prec 0.103876, recall 0.791936
2017-12-10T05:42:03.619650: step 6957, loss 0.210822, acc 0.921875, prec 0.103869, recall 0.791936
2017-12-10T05:42:03.859790: step 6958, loss 0.439727, acc 0.942308, prec 0.103888, recall 0.791978
2017-12-10T05:42:04.142249: step 6959, loss 0.330586, acc 0.96875, prec 0.103921, recall 0.79204
2017-12-10T05:42:04.419461: step 6960, loss 0.193627, acc 0.984375, prec 0.103931, recall 0.79206
2017-12-10T05:42:04.687508: step 6961, loss 0.426839, acc 0.921875, prec 0.103924, recall 0.79206
2017-12-10T05:42:04.955583: step 6962, loss 0.0891713, acc 0.96875, prec 0.103933, recall 0.792081
2017-12-10T05:42:05.223556: step 6963, loss 0.158851, acc 0.96875, prec 0.103965, recall 0.792143
2017-12-10T05:42:05.487866: step 6964, loss 0.0295751, acc 0.984375, prec 0.103976, recall 0.792164
2017-12-10T05:42:05.755537: step 6965, loss 0.00040275, acc 1, prec 0.103999, recall 0.792205
2017-12-10T05:42:06.017675: step 6966, loss 0.0615981, acc 0.96875, prec 0.103996, recall 0.792205
2017-12-10T05:42:06.284382: step 6967, loss 0.0144084, acc 1, prec 0.103996, recall 0.792205
2017-12-10T05:42:06.551233: step 6968, loss 0.37654, acc 0.9375, prec 0.104014, recall 0.792247
2017-12-10T05:42:06.829033: step 6969, loss 0.0292879, acc 0.984375, prec 0.104025, recall 0.792267
2017-12-10T05:42:07.095194: step 6970, loss 0.0107405, acc 1, prec 0.104036, recall 0.792288
2017-12-10T05:42:07.365872: step 6971, loss 0.123727, acc 0.9375, prec 0.104031, recall 0.792288
2017-12-10T05:42:07.633116: step 6972, loss 0.00222323, acc 1, prec 0.104031, recall 0.792288
2017-12-10T05:42:07.904454: step 6973, loss 0.0176138, acc 0.984375, prec 0.10403, recall 0.792288
2017-12-10T05:42:08.176230: step 6974, loss 0.0844846, acc 0.984375, prec 0.10404, recall 0.792308
2017-12-10T05:42:08.450299: step 6975, loss 0.498929, acc 0.984375, prec 0.104062, recall 0.79235
2017-12-10T05:42:08.724093: step 6976, loss 0.10441, acc 0.984375, prec 0.104072, recall 0.79237
2017-12-10T05:42:08.991520: step 6977, loss 0.139142, acc 0.984375, prec 0.104083, recall 0.792391
2017-12-10T05:42:09.264504: step 6978, loss 0.135687, acc 0.96875, prec 0.10408, recall 0.792391
2017-12-10T05:42:09.536142: step 6979, loss 0.0320434, acc 0.984375, prec 0.104079, recall 0.792391
2017-12-10T05:42:09.811603: step 6980, loss 0.10745, acc 0.984375, prec 0.104101, recall 0.792432
2017-12-10T05:42:10.086174: step 6981, loss 0.166041, acc 0.96875, prec 0.104098, recall 0.792432
2017-12-10T05:42:10.357813: step 6982, loss 0.220088, acc 0.984375, prec 0.104097, recall 0.792432
2017-12-10T05:42:10.622123: step 6983, loss 0.0417902, acc 0.984375, prec 0.104095, recall 0.792432
2017-12-10T05:42:10.887070: step 6984, loss 0.140256, acc 0.984375, prec 0.104094, recall 0.792432
2017-12-10T05:42:11.152768: step 6985, loss 0.0259578, acc 0.984375, prec 0.104116, recall 0.792473
2017-12-10T05:42:11.426016: step 6986, loss 0.109554, acc 0.984375, prec 0.104114, recall 0.792473
2017-12-10T05:42:11.702635: step 6987, loss 0.00439349, acc 1, prec 0.104114, recall 0.792473
2017-12-10T05:42:11.971723: step 6988, loss 0.226962, acc 0.96875, prec 0.104135, recall 0.792515
2017-12-10T05:42:12.244372: step 6989, loss 0.175232, acc 0.96875, prec 0.104132, recall 0.792515
2017-12-10T05:42:12.533673: step 6990, loss 0.190941, acc 0.96875, prec 0.10413, recall 0.792515
2017-12-10T05:42:12.803201: step 6991, loss 0.0244257, acc 1, prec 0.10413, recall 0.792515
2017-12-10T05:42:13.079029: step 6992, loss 0.245546, acc 1, prec 0.104153, recall 0.792556
2017-12-10T05:42:13.359902: step 6993, loss 0.301464, acc 0.96875, prec 0.104162, recall 0.792576
2017-12-10T05:42:13.636875: step 6994, loss 0.00544181, acc 1, prec 0.104162, recall 0.792576
2017-12-10T05:42:13.910756: step 6995, loss 0.0253101, acc 0.984375, prec 0.104172, recall 0.792597
2017-12-10T05:42:14.180430: step 6996, loss 0.137229, acc 0.984375, prec 0.104194, recall 0.792638
2017-12-10T05:42:14.446452: step 6997, loss 0.0026266, acc 1, prec 0.104206, recall 0.792659
2017-12-10T05:42:14.718652: step 6998, loss 0.00147937, acc 1, prec 0.104206, recall 0.792659
2017-12-10T05:42:14.988169: step 6999, loss 0.23763, acc 0.984375, prec 0.104205, recall 0.792659
2017-12-10T05:42:15.253684: step 7000, loss 0.0015122, acc 1, prec 0.104205, recall 0.792659
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-7000

2017-12-10T05:42:16.481670: step 7001, loss 0.188631, acc 0.953125, prec 0.104212, recall 0.792679
2017-12-10T05:42:16.755628: step 7002, loss 0.015623, acc 0.984375, prec 0.104223, recall 0.7927
2017-12-10T05:42:17.021048: step 7003, loss 0.000406975, acc 1, prec 0.104234, recall 0.79272
2017-12-10T05:42:17.287705: step 7004, loss 0.522081, acc 0.96875, prec 0.104255, recall 0.792762
2017-12-10T05:42:17.556761: step 7005, loss 0.137176, acc 1, prec 0.104267, recall 0.792782
2017-12-10T05:42:17.822508: step 7006, loss 0.130397, acc 0.984375, prec 0.104289, recall 0.792823
2017-12-10T05:42:18.093226: step 7007, loss 0.248419, acc 0.96875, prec 0.104298, recall 0.792844
2017-12-10T05:42:18.370101: step 7008, loss 0.0250946, acc 0.984375, prec 0.104296, recall 0.792844
2017-12-10T05:42:18.636434: step 7009, loss 0.00623933, acc 1, prec 0.10432, recall 0.792885
2017-12-10T05:42:18.910524: step 7010, loss 0.705074, acc 0.984375, prec 0.104342, recall 0.792926
2017-12-10T05:42:19.180224: step 7011, loss 0.0575378, acc 0.96875, prec 0.104362, recall 0.792967
2017-12-10T05:42:19.448390: step 7012, loss 0.00218612, acc 1, prec 0.104362, recall 0.792967
2017-12-10T05:42:19.712901: step 7013, loss 0.349724, acc 0.96875, prec 0.104383, recall 0.793008
2017-12-10T05:42:19.980359: step 7014, loss 0.0979509, acc 0.96875, prec 0.104403, recall 0.793049
2017-12-10T05:42:20.248924: step 7015, loss 0.141194, acc 0.96875, prec 0.104412, recall 0.793069
2017-12-10T05:42:20.514920: step 7016, loss 0.331361, acc 0.96875, prec 0.10441, recall 0.793069
2017-12-10T05:42:20.793237: step 7017, loss 0.529259, acc 0.921875, prec 0.104415, recall 0.79309
2017-12-10T05:42:21.057296: step 7018, loss 0.198276, acc 0.9375, prec 0.104421, recall 0.79311
2017-12-10T05:42:21.322330: step 7019, loss 0.800679, acc 0.9375, prec 0.104427, recall 0.793131
2017-12-10T05:42:21.593891: step 7020, loss 0.566179, acc 0.953125, prec 0.104435, recall 0.793151
2017-12-10T05:42:21.869673: step 7021, loss 0.211478, acc 0.984375, prec 0.104445, recall 0.793172
2017-12-10T05:42:22.138182: step 7022, loss 0.344783, acc 0.953125, prec 0.104441, recall 0.793172
2017-12-10T05:42:22.404126: step 7023, loss 0.629042, acc 0.9375, prec 0.104459, recall 0.793213
2017-12-10T05:42:22.676660: step 7024, loss 0.920347, acc 0.953125, prec 0.104478, recall 0.793254
2017-12-10T05:42:22.945330: step 7025, loss 0.0257397, acc 0.984375, prec 0.104488, recall 0.793274
2017-12-10T05:42:23.221778: step 7026, loss 0.0568151, acc 0.984375, prec 0.10451, recall 0.793315
2017-12-10T05:42:23.494274: step 7027, loss 0.372031, acc 0.921875, prec 0.104515, recall 0.793335
2017-12-10T05:42:23.759887: step 7028, loss 0.00148963, acc 1, prec 0.104515, recall 0.793335
2017-12-10T05:42:24.025635: step 7029, loss 0.399563, acc 0.9375, prec 0.104521, recall 0.793356
2017-12-10T05:42:24.290735: step 7030, loss 0.026132, acc 0.984375, prec 0.10452, recall 0.793356
2017-12-10T05:42:24.567368: step 7031, loss 0.117605, acc 0.96875, prec 0.104552, recall 0.793417
2017-12-10T05:42:24.831757: step 7032, loss 0.275422, acc 0.96875, prec 0.10455, recall 0.793417
2017-12-10T05:42:25.105148: step 7033, loss 0.137648, acc 0.984375, prec 0.104548, recall 0.793417
2017-12-10T05:42:25.375786: step 7034, loss 3.67735, acc 0.96875, prec 0.104547, recall 0.793339
2017-12-10T05:42:25.649070: step 7035, loss 0.347629, acc 0.96875, prec 0.104544, recall 0.793339
2017-12-10T05:42:25.915552: step 7036, loss 0.0968324, acc 0.96875, prec 0.104565, recall 0.793379
2017-12-10T05:42:26.187045: step 7037, loss 0.122506, acc 0.96875, prec 0.104574, recall 0.7934
2017-12-10T05:42:26.457523: step 7038, loss 0.287926, acc 0.953125, prec 0.104593, recall 0.793441
2017-12-10T05:42:26.718619: step 7039, loss 0.388614, acc 0.90625, prec 0.104596, recall 0.793461
2017-12-10T05:42:26.985208: step 7040, loss 0.365352, acc 0.90625, prec 0.104612, recall 0.793502
2017-12-10T05:42:27.267585: step 7041, loss 0.364819, acc 0.921875, prec 0.104605, recall 0.793502
2017-12-10T05:42:27.538634: step 7042, loss 0.313429, acc 0.90625, prec 0.104597, recall 0.793502
2017-12-10T05:42:27.808637: step 7043, loss 0.857035, acc 0.890625, prec 0.104622, recall 0.793563
2017-12-10T05:42:28.076606: step 7044, loss 0.806957, acc 0.875, prec 0.104611, recall 0.793563
2017-12-10T05:42:28.346894: step 7045, loss 0.478752, acc 0.890625, prec 0.104602, recall 0.793563
2017-12-10T05:42:28.618991: step 7046, loss 0.186518, acc 0.953125, prec 0.104597, recall 0.793563
2017-12-10T05:42:28.891241: step 7047, loss 0.275656, acc 0.921875, prec 0.104591, recall 0.793563
2017-12-10T05:42:29.160079: step 7048, loss 0.27488, acc 0.9375, prec 0.104608, recall 0.793604
2017-12-10T05:42:29.433378: step 7049, loss 0.503877, acc 0.90625, prec 0.1046, recall 0.793604
2017-12-10T05:42:29.697369: step 7050, loss 0.340357, acc 0.90625, prec 0.104627, recall 0.793665
2017-12-10T05:42:29.963825: step 7051, loss 0.294619, acc 0.953125, prec 0.104623, recall 0.793665
2017-12-10T05:42:30.238254: step 7052, loss 0.417033, acc 0.953125, prec 0.104631, recall 0.793685
2017-12-10T05:42:30.516621: step 7053, loss 0.296204, acc 0.96875, prec 0.10464, recall 0.793706
2017-12-10T05:42:30.797326: step 7054, loss 0.224006, acc 0.96875, prec 0.104648, recall 0.793726
2017-12-10T05:42:31.062215: step 7055, loss 0.309279, acc 0.921875, prec 0.104665, recall 0.793767
2017-12-10T05:42:31.327753: step 7056, loss 0.162333, acc 0.984375, prec 0.104675, recall 0.793787
2017-12-10T05:42:31.601946: step 7057, loss 0.0898656, acc 0.953125, prec 0.104671, recall 0.793787
2017-12-10T05:42:31.867016: step 7058, loss 0.220966, acc 0.96875, prec 0.104668, recall 0.793787
2017-12-10T05:42:32.132956: step 7059, loss 0.0149821, acc 0.984375, prec 0.104679, recall 0.793807
2017-12-10T05:42:32.403045: step 7060, loss 0.14267, acc 0.984375, prec 0.104677, recall 0.793807
2017-12-10T05:42:32.668775: step 7061, loss 0.192638, acc 0.953125, prec 0.104697, recall 0.793848
2017-12-10T05:42:32.933400: step 7062, loss 0.153906, acc 0.984375, prec 0.104695, recall 0.793848
2017-12-10T05:42:33.204480: step 7063, loss 0.0372, acc 0.984375, prec 0.104705, recall 0.793868
2017-12-10T05:42:33.467983: step 7064, loss 0.00414038, acc 1, prec 0.104705, recall 0.793868
2017-12-10T05:42:33.732222: step 7065, loss 0.235739, acc 0.96875, prec 0.104703, recall 0.793868
2017-12-10T05:42:34.001154: step 7066, loss 4.8304, acc 0.96875, prec 0.104713, recall 0.79381
2017-12-10T05:42:34.269271: step 7067, loss 0.347427, acc 0.96875, prec 0.10471, recall 0.79381
2017-12-10T05:42:34.535770: step 7068, loss 0.37785, acc 0.96875, prec 0.104719, recall 0.793831
2017-12-10T05:42:34.803269: step 7069, loss 0.393734, acc 0.96875, prec 0.104728, recall 0.793851
2017-12-10T05:42:35.073260: step 7070, loss 0.254334, acc 0.9375, prec 0.104746, recall 0.793892
2017-12-10T05:42:35.350662: step 7071, loss 1.00074, acc 0.953125, prec 0.104765, recall 0.793932
2017-12-10T05:42:35.617249: step 7072, loss 0.281332, acc 0.9375, prec 0.10476, recall 0.793932
2017-12-10T05:42:35.888695: step 7073, loss 0.413158, acc 0.96875, prec 0.104769, recall 0.793953
2017-12-10T05:42:36.155741: step 7074, loss 0.46776, acc 0.90625, prec 0.10476, recall 0.793953
2017-12-10T05:42:36.423330: step 7075, loss 0.530736, acc 0.875, prec 0.10475, recall 0.793953
2017-12-10T05:42:36.690142: step 7076, loss 0.20337, acc 0.953125, prec 0.104769, recall 0.793993
2017-12-10T05:42:36.956832: step 7077, loss 0.768055, acc 0.921875, prec 0.104774, recall 0.794013
2017-12-10T05:42:37.225827: step 7078, loss 0.252755, acc 0.90625, prec 0.104777, recall 0.794034
2017-12-10T05:42:37.491940: step 7079, loss 1.04322, acc 0.9375, prec 0.104772, recall 0.794034
2017-12-10T05:42:37.760181: step 7080, loss 0.367878, acc 0.953125, prec 0.104767, recall 0.794034
2017-12-10T05:42:38.032841: step 7081, loss 0.155104, acc 0.96875, prec 0.1048, recall 0.794095
2017-12-10T05:42:38.300462: step 7082, loss 0.33321, acc 0.890625, prec 0.10479, recall 0.794095
2017-12-10T05:42:38.573034: step 7083, loss 0.441409, acc 0.9375, prec 0.104796, recall 0.794115
2017-12-10T05:42:38.846831: step 7084, loss 0.580423, acc 0.921875, prec 0.104789, recall 0.794115
2017-12-10T05:42:39.110754: step 7085, loss 0.0444866, acc 0.984375, prec 0.104788, recall 0.794115
2017-12-10T05:42:39.381961: step 7086, loss 0.0277865, acc 0.984375, prec 0.104798, recall 0.794135
2017-12-10T05:42:39.649895: step 7087, loss 0.504236, acc 0.953125, prec 0.104806, recall 0.794155
2017-12-10T05:42:39.923923: step 7088, loss 0.475137, acc 0.953125, prec 0.104813, recall 0.794176
2017-12-10T05:42:40.197396: step 7089, loss 0.389567, acc 0.9375, prec 0.104808, recall 0.794176
2017-12-10T05:42:40.460684: step 7090, loss 0.0432531, acc 0.984375, prec 0.104842, recall 0.794236
2017-12-10T05:42:40.723229: step 7091, loss 0.0785335, acc 0.984375, prec 0.104852, recall 0.794257
2017-12-10T05:42:40.998736: step 7092, loss 0.17967, acc 0.953125, prec 0.104848, recall 0.794257
2017-12-10T05:42:41.268887: step 7093, loss 0.000733618, acc 1, prec 0.104859, recall 0.794277
2017-12-10T05:42:41.532653: step 7094, loss 0.553347, acc 0.96875, prec 0.104857, recall 0.794277
2017-12-10T05:42:41.801180: step 7095, loss 0.00154943, acc 1, prec 0.104857, recall 0.794277
2017-12-10T05:42:42.066402: step 7096, loss 0.203275, acc 0.96875, prec 0.104866, recall 0.794297
2017-12-10T05:42:42.337999: step 7097, loss 0.0291898, acc 0.984375, prec 0.104876, recall 0.794317
2017-12-10T05:42:42.609585: step 7098, loss 0.0309671, acc 0.984375, prec 0.104886, recall 0.794337
2017-12-10T05:42:42.878607: step 7099, loss 0.0809879, acc 0.96875, prec 0.104895, recall 0.794358
2017-12-10T05:42:43.143417: step 7100, loss 0.48753, acc 0.9375, prec 0.104901, recall 0.794378
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-7100

2017-12-10T05:42:44.528003: step 7101, loss 0.0006328, acc 1, prec 0.104901, recall 0.794378
2017-12-10T05:42:44.797323: step 7102, loss 0.184113, acc 1, prec 0.104913, recall 0.794398
2017-12-10T05:42:45.071152: step 7103, loss 0.0606557, acc 0.984375, prec 0.104911, recall 0.794398
2017-12-10T05:42:45.341773: step 7104, loss 12.0852, acc 0.984375, prec 0.104923, recall 0.79434
2017-12-10T05:42:45.620778: step 7105, loss 2.29952, acc 0.96875, prec 0.104933, recall 0.794282
2017-12-10T05:42:45.890708: step 7106, loss 0.499386, acc 0.90625, prec 0.104948, recall 0.794323
2017-12-10T05:42:46.158789: step 7107, loss 0.322681, acc 0.953125, prec 0.104956, recall 0.794343
2017-12-10T05:42:46.433519: step 7108, loss 0.0257678, acc 0.984375, prec 0.104978, recall 0.794383
2017-12-10T05:42:46.696845: step 7109, loss 0.212151, acc 0.90625, prec 0.104981, recall 0.794404
2017-12-10T05:42:46.963258: step 7110, loss 0.333729, acc 0.90625, prec 0.104973, recall 0.794404
2017-12-10T05:42:47.233605: step 7111, loss 0.238128, acc 0.953125, prec 0.10498, recall 0.794424
2017-12-10T05:42:47.500466: step 7112, loss 0.331841, acc 0.890625, prec 0.104971, recall 0.794424
2017-12-10T05:42:47.771397: step 7113, loss 0.44912, acc 0.921875, prec 0.104964, recall 0.794424
2017-12-10T05:42:48.044152: step 7114, loss 1.20486, acc 0.78125, prec 0.10498, recall 0.794484
2017-12-10T05:42:48.315347: step 7115, loss 0.721666, acc 0.84375, prec 0.104966, recall 0.794484
2017-12-10T05:42:48.585186: step 7116, loss 0.912014, acc 0.875, prec 0.104955, recall 0.794484
2017-12-10T05:42:48.853203: step 7117, loss 1.1205, acc 0.765625, prec 0.104935, recall 0.794484
2017-12-10T05:42:49.125157: step 7118, loss 1.19887, acc 0.78125, prec 0.104916, recall 0.794484
2017-12-10T05:42:49.389457: step 7119, loss 0.545739, acc 0.921875, prec 0.104909, recall 0.794484
2017-12-10T05:42:49.661134: step 7120, loss 0.649889, acc 0.8125, prec 0.104904, recall 0.794504
2017-12-10T05:42:49.927633: step 7121, loss 1.06456, acc 0.828125, prec 0.104889, recall 0.794504
2017-12-10T05:42:50.190149: step 7122, loss 0.502409, acc 0.9375, prec 0.104884, recall 0.794504
2017-12-10T05:42:50.463273: step 7123, loss 0.125313, acc 0.96875, prec 0.104881, recall 0.794504
2017-12-10T05:42:50.727916: step 7124, loss 0.284447, acc 0.921875, prec 0.104886, recall 0.794525
2017-12-10T05:42:50.994966: step 7125, loss 0.0714229, acc 0.96875, prec 0.104895, recall 0.794545
2017-12-10T05:42:51.258411: step 7126, loss 1.56377, acc 0.90625, prec 0.104898, recall 0.794565
2017-12-10T05:42:51.527990: step 7127, loss 0.452181, acc 0.921875, prec 0.104892, recall 0.794565
2017-12-10T05:42:51.801003: step 7128, loss 0.19191, acc 0.96875, prec 0.104901, recall 0.794585
2017-12-10T05:42:52.075787: step 7129, loss 0.135855, acc 0.96875, prec 0.104909, recall 0.794605
2017-12-10T05:42:52.341497: step 7130, loss 0.516739, acc 0.9375, prec 0.104904, recall 0.794605
2017-12-10T05:42:52.610428: step 7131, loss 0.0683116, acc 0.96875, prec 0.104913, recall 0.794625
2017-12-10T05:42:52.875814: step 7132, loss 5.65814, acc 0.921875, prec 0.104942, recall 0.794608
2017-12-10T05:42:53.146596: step 7133, loss 0.0812263, acc 0.953125, prec 0.104938, recall 0.794608
2017-12-10T05:42:53.422176: step 7134, loss 0.148746, acc 0.953125, prec 0.104946, recall 0.794628
2017-12-10T05:42:53.694747: step 7135, loss 0.156363, acc 0.953125, prec 0.104953, recall 0.794648
2017-12-10T05:42:53.959189: step 7136, loss 0.309604, acc 0.9375, prec 0.104959, recall 0.794668
2017-12-10T05:42:54.230324: step 7137, loss 0.788194, acc 0.859375, prec 0.104959, recall 0.794688
2017-12-10T05:42:54.498458: step 7138, loss 0.644847, acc 0.921875, prec 0.104963, recall 0.794708
2017-12-10T05:42:54.764219: step 7139, loss 0.122148, acc 0.984375, prec 0.104962, recall 0.794708
2017-12-10T05:42:55.026906: step 7140, loss 0.0882074, acc 0.953125, prec 0.104958, recall 0.794708
2017-12-10T05:42:55.297157: step 7141, loss 0.431712, acc 0.921875, prec 0.104951, recall 0.794708
2017-12-10T05:42:55.566323: step 7142, loss 0.396967, acc 0.9375, prec 0.104981, recall 0.794769
2017-12-10T05:42:55.838980: step 7143, loss 0.182052, acc 0.953125, prec 0.104988, recall 0.794789
2017-12-10T05:42:56.102797: step 7144, loss 0.0910963, acc 0.96875, prec 0.104997, recall 0.794809
2017-12-10T05:42:56.364317: step 7145, loss 0.413918, acc 0.9375, prec 0.105026, recall 0.794869
2017-12-10T05:42:56.635810: step 7146, loss 0.136682, acc 0.96875, prec 0.105023, recall 0.794869
2017-12-10T05:42:56.902532: step 7147, loss 0.272426, acc 0.9375, prec 0.105018, recall 0.794869
2017-12-10T05:42:57.170115: step 7148, loss 0.781412, acc 0.890625, prec 0.10502, recall 0.794889
2017-12-10T05:42:57.445592: step 7149, loss 0.362487, acc 0.953125, prec 0.105028, recall 0.794909
2017-12-10T05:42:57.717657: step 7150, loss 0.0286767, acc 0.984375, prec 0.105038, recall 0.79493
2017-12-10T05:42:57.984694: step 7151, loss 0.215914, acc 0.984375, prec 0.10506, recall 0.79497
2017-12-10T05:42:58.253276: step 7152, loss 0.553734, acc 0.96875, prec 0.105068, recall 0.79499
2017-12-10T05:42:59.229552: step 7153, loss 0.11067, acc 0.96875, prec 0.105066, recall 0.79499
2017-12-10T05:42:59.592008: step 7154, loss 0.175259, acc 0.96875, prec 0.105063, recall 0.79499
2017-12-10T05:42:59.860553: step 7155, loss 0.12516, acc 0.9375, prec 0.105104, recall 0.79507
2017-12-10T05:43:00.344406: step 7156, loss 0.124362, acc 0.96875, prec 0.105101, recall 0.79507
2017-12-10T05:43:01.073138: step 7157, loss 0.244889, acc 0.953125, prec 0.105097, recall 0.79507
2017-12-10T05:43:01.795952: step 7158, loss 0.0851382, acc 0.96875, prec 0.105106, recall 0.79509
2017-12-10T05:43:02.501557: step 7159, loss 0.246576, acc 0.953125, prec 0.105148, recall 0.79517
2017-12-10T05:43:03.246121: step 7160, loss 0.0332786, acc 0.984375, prec 0.105158, recall 0.79519
2017-12-10T05:43:03.959578: step 7161, loss 0.334306, acc 0.9375, prec 0.105164, recall 0.79521
2017-12-10T05:43:04.664320: step 7162, loss 0.22217, acc 0.96875, prec 0.105196, recall 0.79527
2017-12-10T05:43:05.422283: step 7163, loss 0.311571, acc 0.96875, prec 0.105194, recall 0.79527
2017-12-10T05:43:06.151938: step 7164, loss 0.0103146, acc 1, prec 0.105194, recall 0.79527
2017-12-10T05:43:06.852138: step 7165, loss 0.0263727, acc 0.984375, prec 0.105204, recall 0.79529
2017-12-10T05:43:07.597910: step 7166, loss 0.523536, acc 0.890625, prec 0.105206, recall 0.79531
2017-12-10T05:43:08.327921: step 7167, loss 0.145497, acc 0.9375, prec 0.105212, recall 0.79533
2017-12-10T05:43:09.062875: step 7168, loss 0.00913656, acc 1, prec 0.105235, recall 0.79537
2017-12-10T05:43:09.752967: step 7169, loss 0.0665824, acc 0.96875, prec 0.105233, recall 0.79537
2017-12-10T05:43:10.479892: step 7170, loss 0.409067, acc 0.953125, prec 0.10524, recall 0.79539
2017-12-10T05:43:11.523994: step 7171, loss 0.105668, acc 0.953125, prec 0.105236, recall 0.79539
2017-12-10T05:43:11.981892: step 7172, loss 0.0414394, acc 0.984375, prec 0.105235, recall 0.79539
2017-12-10T05:43:12.294152: step 7173, loss 0.0523631, acc 0.96875, prec 0.105243, recall 0.79541
2017-12-10T05:43:12.597249: step 7174, loss 0.171453, acc 0.984375, prec 0.105277, recall 0.79547
2017-12-10T05:43:12.906695: step 7175, loss 0.0109369, acc 1, prec 0.105277, recall 0.79547
2017-12-10T05:43:13.204863: step 7176, loss 1.67073, acc 0.96875, prec 0.10532, recall 0.79555
2017-12-10T05:43:13.505770: step 7177, loss 0.00039793, acc 1, prec 0.10532, recall 0.79555
2017-12-10T05:43:13.776960: step 7178, loss 0.000770791, acc 1, prec 0.105343, recall 0.79559
2017-12-10T05:43:14.054262: step 7179, loss 0.0153916, acc 0.984375, prec 0.105354, recall 0.79561
2017-12-10T05:43:14.321495: step 7180, loss 0.0954713, acc 0.984375, prec 0.105352, recall 0.79561
2017-12-10T05:43:14.596508: step 7181, loss 0.140197, acc 0.984375, prec 0.105362, recall 0.79563
2017-12-10T05:43:14.880021: step 7182, loss 0.255159, acc 0.953125, prec 0.105358, recall 0.79563
2017-12-10T05:43:15.156396: step 7183, loss 0.0224898, acc 0.984375, prec 0.105415, recall 0.795729
2017-12-10T05:43:15.434741: step 7184, loss 0.0896481, acc 0.984375, prec 0.105437, recall 0.795769
2017-12-10T05:43:15.697881: step 7185, loss 2.56434, acc 0.953125, prec 0.105468, recall 0.795751
2017-12-10T05:43:15.962261: step 7186, loss 0.537375, acc 0.96875, prec 0.105466, recall 0.795751
2017-12-10T05:43:16.226673: step 7187, loss 0.812581, acc 0.90625, prec 0.105469, recall 0.795771
2017-12-10T05:43:16.496780: step 7188, loss 0.118658, acc 0.96875, prec 0.105489, recall 0.795811
2017-12-10T05:43:16.764279: step 7189, loss 0.548163, acc 0.875, prec 0.105479, recall 0.795811
2017-12-10T05:43:17.027153: step 7190, loss 0.294076, acc 0.953125, prec 0.105475, recall 0.795811
2017-12-10T05:43:17.298673: step 7191, loss 0.14496, acc 0.953125, prec 0.105482, recall 0.795831
2017-12-10T05:43:17.563459: step 7192, loss 0.315149, acc 0.96875, prec 0.105514, recall 0.795891
2017-12-10T05:43:17.831705: step 7193, loss 0.563622, acc 0.953125, prec 0.10551, recall 0.795891
2017-12-10T05:43:18.097371: step 7194, loss 0.795847, acc 0.890625, prec 0.105523, recall 0.79593
2017-12-10T05:43:18.369128: step 7195, loss 0.691022, acc 0.921875, prec 0.105528, recall 0.79595
2017-12-10T05:43:18.633342: step 7196, loss 0.860046, acc 0.921875, prec 0.105521, recall 0.79595
2017-12-10T05:43:18.904049: step 7197, loss 0.175108, acc 0.953125, prec 0.105529, recall 0.79597
2017-12-10T05:43:19.172187: step 7198, loss 0.193382, acc 0.953125, prec 0.105525, recall 0.79597
2017-12-10T05:43:19.440977: step 7199, loss 0.854157, acc 0.890625, prec 0.105527, recall 0.79599
2017-12-10T05:43:19.705208: step 7200, loss 0.227107, acc 0.953125, prec 0.105523, recall 0.79599

Evaluation:
2017-12-10T05:43:27.432203: step 7200, loss 5.6841, acc 0.919891, prec 0.105424, recall 0.792118

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-7200

2017-12-10T05:43:28.796911: step 7201, loss 0.704779, acc 0.9375, prec 0.105418, recall 0.792118
2017-12-10T05:43:29.063817: step 7202, loss 0.774281, acc 0.96875, prec 0.105438, recall 0.792158
2017-12-10T05:43:29.342275: step 7203, loss 0.62432, acc 0.921875, prec 0.105443, recall 0.792178
2017-12-10T05:43:29.614244: step 7204, loss 0.283942, acc 0.890625, prec 0.105445, recall 0.792198
2017-12-10T05:43:29.882200: step 7205, loss 0.165176, acc 0.96875, prec 0.105465, recall 0.792238
2017-12-10T05:43:30.148179: step 7206, loss 0.0893189, acc 0.953125, prec 0.105461, recall 0.792238
2017-12-10T05:43:30.419360: step 7207, loss 0.231651, acc 0.96875, prec 0.105459, recall 0.792238
2017-12-10T05:43:30.703824: step 7208, loss 0.0979684, acc 0.984375, prec 0.105491, recall 0.792297
2017-12-10T05:43:30.970943: step 7209, loss 0.117341, acc 0.953125, prec 0.105487, recall 0.792297
2017-12-10T05:43:31.240529: step 7210, loss 0.189716, acc 0.96875, prec 0.105496, recall 0.792317
2017-12-10T05:43:31.513982: step 7211, loss 0.0175916, acc 0.984375, prec 0.105506, recall 0.792337
2017-12-10T05:43:31.780884: step 7212, loss 0.326672, acc 0.984375, prec 0.105516, recall 0.792357
2017-12-10T05:43:32.051227: step 7213, loss 0.172874, acc 0.96875, prec 0.105525, recall 0.792377
2017-12-10T05:43:32.321723: step 7214, loss 0.26136, acc 0.96875, prec 0.105534, recall 0.792397
2017-12-10T05:43:32.587387: step 7215, loss 0.358052, acc 0.9375, prec 0.105574, recall 0.792476
2017-12-10T05:43:32.853663: step 7216, loss 0.000479183, acc 1, prec 0.105608, recall 0.792536
2017-12-10T05:43:33.120932: step 7217, loss 0.0709205, acc 0.96875, prec 0.105605, recall 0.792536
2017-12-10T05:43:33.386752: step 7218, loss 0.00252632, acc 1, prec 0.105605, recall 0.792536
2017-12-10T05:43:33.657748: step 7219, loss 0.0587701, acc 0.96875, prec 0.105626, recall 0.792576
2017-12-10T05:43:33.932675: step 7220, loss 0.257692, acc 0.96875, prec 0.105623, recall 0.792576
2017-12-10T05:43:34.200015: step 7221, loss 0.151545, acc 0.96875, prec 0.10562, recall 0.792576
2017-12-10T05:43:34.471528: step 7222, loss 0.143112, acc 0.96875, prec 0.10564, recall 0.792615
2017-12-10T05:43:34.735180: step 7223, loss 1.0956, acc 0.9375, prec 0.105646, recall 0.792635
2017-12-10T05:43:35.000782: step 7224, loss 0.0633739, acc 0.96875, prec 0.105644, recall 0.792635
2017-12-10T05:43:35.265907: step 7225, loss 0.0406388, acc 0.984375, prec 0.105642, recall 0.792635
2017-12-10T05:43:35.526331: step 7226, loss 0.506604, acc 0.984375, prec 0.105652, recall 0.792655
2017-12-10T05:43:35.793674: step 7227, loss 0.31315, acc 0.96875, prec 0.105672, recall 0.792695
2017-12-10T05:43:36.065399: step 7228, loss 0.145279, acc 0.984375, prec 0.105671, recall 0.792695
2017-12-10T05:43:36.331517: step 7229, loss 0.000727959, acc 1, prec 0.105682, recall 0.792714
2017-12-10T05:43:36.595507: step 7230, loss 0.00284188, acc 1, prec 0.105694, recall 0.792734
2017-12-10T05:43:36.866798: step 7231, loss 0.570686, acc 0.953125, prec 0.10569, recall 0.792734
2017-12-10T05:43:37.129438: step 7232, loss 0.0587852, acc 0.96875, prec 0.105687, recall 0.792734
2017-12-10T05:43:37.396950: step 7233, loss 0.0161859, acc 0.984375, prec 0.105686, recall 0.792734
2017-12-10T05:43:37.665724: step 7234, loss 0.580296, acc 0.953125, prec 0.105693, recall 0.792754
2017-12-10T05:43:37.935406: step 7235, loss 0.0369773, acc 0.984375, prec 0.105715, recall 0.792794
2017-12-10T05:43:38.214179: step 7236, loss 0.103882, acc 0.96875, prec 0.105712, recall 0.792794
2017-12-10T05:43:38.480570: step 7237, loss 0.394702, acc 0.953125, prec 0.105731, recall 0.792833
2017-12-10T05:43:38.748539: step 7238, loss 0.131881, acc 0.953125, prec 0.105738, recall 0.792853
2017-12-10T05:43:39.012033: step 7239, loss 0.0843601, acc 0.96875, prec 0.105735, recall 0.792853
2017-12-10T05:43:39.282254: step 7240, loss 0.111601, acc 0.96875, prec 0.105744, recall 0.792873
2017-12-10T05:43:39.558639: step 7241, loss 0.114295, acc 0.953125, prec 0.10574, recall 0.792873
2017-12-10T05:43:39.824683: step 7242, loss 0.273571, acc 0.984375, prec 0.105739, recall 0.792873
2017-12-10T05:43:40.090091: step 7243, loss 0.558336, acc 0.953125, prec 0.105746, recall 0.792893
2017-12-10T05:43:40.365122: step 7244, loss 0.185244, acc 0.953125, prec 0.105742, recall 0.792893
2017-12-10T05:43:40.646403: step 7245, loss 0.0021439, acc 1, prec 0.105765, recall 0.792932
2017-12-10T05:43:40.911001: step 7246, loss 0.793651, acc 0.9375, prec 0.105759, recall 0.792932
2017-12-10T05:43:41.181292: step 7247, loss 0.504737, acc 0.953125, prec 0.105778, recall 0.792972
2017-12-10T05:43:41.460049: step 7248, loss 0.142864, acc 0.984375, prec 0.105799, recall 0.793011
2017-12-10T05:43:41.729803: step 7249, loss 0.0834127, acc 0.96875, prec 0.105797, recall 0.793011
2017-12-10T05:43:41.994341: step 7250, loss 0.0321919, acc 0.984375, prec 0.105807, recall 0.793031
2017-12-10T05:43:42.266744: step 7251, loss 0.189798, acc 0.984375, prec 0.105817, recall 0.793051
2017-12-10T05:43:42.548543: step 7252, loss 0.112619, acc 0.984375, prec 0.105838, recall 0.79309
2017-12-10T05:43:42.823263: step 7253, loss 0.179073, acc 0.96875, prec 0.105836, recall 0.79309
2017-12-10T05:43:43.092305: step 7254, loss 0.186915, acc 0.984375, prec 0.105846, recall 0.79311
2017-12-10T05:43:43.369444: step 7255, loss 0.0329166, acc 0.984375, prec 0.105867, recall 0.79315
2017-12-10T05:43:43.637206: step 7256, loss 0.0315838, acc 0.984375, prec 0.105866, recall 0.79315
2017-12-10T05:43:43.905249: step 7257, loss 0.00518517, acc 1, prec 0.105866, recall 0.79315
2017-12-10T05:43:44.164026: step 7258, loss 0.0269174, acc 0.984375, prec 0.105887, recall 0.793189
2017-12-10T05:43:44.432026: step 7259, loss 0.0315653, acc 0.984375, prec 0.105886, recall 0.793189
2017-12-10T05:43:44.705396: step 7260, loss 0.482249, acc 0.90625, prec 0.1059, recall 0.793228
2017-12-10T05:43:44.986166: step 7261, loss 0.575514, acc 0.96875, prec 0.105921, recall 0.793268
2017-12-10T05:43:45.255963: step 7262, loss 0.157054, acc 0.984375, prec 0.105931, recall 0.793288
2017-12-10T05:43:45.527569: step 7263, loss 0.198949, acc 0.984375, prec 0.105929, recall 0.793288
2017-12-10T05:43:45.805218: step 7264, loss 0.106001, acc 0.984375, prec 0.105939, recall 0.793307
2017-12-10T05:43:46.076836: step 7265, loss 0.000859751, acc 1, prec 0.105939, recall 0.793307
2017-12-10T05:43:46.342671: step 7266, loss 0.145506, acc 0.96875, prec 0.105959, recall 0.793347
2017-12-10T05:43:46.611604: step 7267, loss 1.51618, acc 0.96875, prec 0.105969, recall 0.793291
2017-12-10T05:43:46.887379: step 7268, loss 0.00894964, acc 1, prec 0.105992, recall 0.79333
2017-12-10T05:43:47.158547: step 7269, loss 0.372841, acc 0.9375, prec 0.105987, recall 0.79333
2017-12-10T05:43:47.425726: step 7270, loss 0.117882, acc 0.96875, prec 0.105995, recall 0.79335
2017-12-10T05:43:47.690291: step 7271, loss 0.136055, acc 0.953125, prec 0.106003, recall 0.79337
2017-12-10T05:43:47.957409: step 7272, loss 0.110915, acc 0.984375, prec 0.106013, recall 0.793389
2017-12-10T05:43:48.226257: step 7273, loss 0.331913, acc 0.96875, prec 0.106021, recall 0.793409
2017-12-10T05:43:48.502397: step 7274, loss 0.115593, acc 0.96875, prec 0.106019, recall 0.793409
2017-12-10T05:43:48.773972: step 7275, loss 0.806484, acc 0.921875, prec 0.106023, recall 0.793429
2017-12-10T05:43:49.040332: step 7276, loss 0.0852076, acc 0.96875, prec 0.106021, recall 0.793429
2017-12-10T05:43:49.307023: step 7277, loss 0.232954, acc 0.90625, prec 0.106047, recall 0.793488
2017-12-10T05:43:49.581635: step 7278, loss 1.21974, acc 0.984375, prec 0.106047, recall 0.793412
2017-12-10T05:43:49.856531: step 7279, loss 0.259417, acc 0.96875, prec 0.106044, recall 0.793412
2017-12-10T05:43:50.130393: step 7280, loss 0.687073, acc 0.9375, prec 0.10605, recall 0.793432
2017-12-10T05:43:50.407082: step 7281, loss 0.0453555, acc 0.96875, prec 0.106059, recall 0.793451
2017-12-10T05:43:50.676561: step 7282, loss 0.209632, acc 0.953125, prec 0.106066, recall 0.793471
2017-12-10T05:43:50.946725: step 7283, loss 1.40656, acc 0.875, prec 0.106055, recall 0.793471
2017-12-10T05:43:51.211961: step 7284, loss 0.312639, acc 0.953125, prec 0.106085, recall 0.79353
2017-12-10T05:43:51.474635: step 7285, loss 0.281181, acc 0.9375, prec 0.10608, recall 0.79353
2017-12-10T05:43:51.748562: step 7286, loss 1.26843, acc 0.875, prec 0.106092, recall 0.793569
2017-12-10T05:43:52.017815: step 7287, loss 0.316279, acc 0.96875, prec 0.106089, recall 0.793569
2017-12-10T05:43:52.287516: step 7288, loss 0.30227, acc 0.921875, prec 0.106082, recall 0.793569
2017-12-10T05:43:52.552246: step 7289, loss 0.296529, acc 0.9375, prec 0.106088, recall 0.793589
2017-12-10T05:43:52.814786: step 7290, loss 0.451961, acc 0.90625, prec 0.10608, recall 0.793589
2017-12-10T05:43:53.078624: step 7291, loss 0.118462, acc 0.984375, prec 0.10609, recall 0.793609
2017-12-10T05:43:53.344062: step 7292, loss 0.214379, acc 0.953125, prec 0.106086, recall 0.793609
2017-12-10T05:43:53.619037: step 7293, loss 0.336598, acc 0.921875, prec 0.106079, recall 0.793609
2017-12-10T05:43:53.880638: step 7294, loss 0.237042, acc 0.9375, prec 0.106074, recall 0.793609
2017-12-10T05:43:54.156433: step 7295, loss 0.56123, acc 0.921875, prec 0.106067, recall 0.793609
2017-12-10T05:43:54.424078: step 7296, loss 0.5305, acc 0.96875, prec 0.106087, recall 0.793648
2017-12-10T05:43:54.694575: step 7297, loss 0.461924, acc 0.96875, prec 0.106085, recall 0.793648
2017-12-10T05:43:54.966933: step 7298, loss 0.00127072, acc 1, prec 0.106096, recall 0.793667
2017-12-10T05:43:55.234824: step 7299, loss 0.455258, acc 0.9375, prec 0.106091, recall 0.793667
2017-12-10T05:43:55.506481: step 7300, loss 0.0338812, acc 0.984375, prec 0.106112, recall 0.793707
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-7300

2017-12-10T05:43:57.185015: step 7301, loss 0.204925, acc 0.9375, prec 0.106118, recall 0.793726
2017-12-10T05:43:57.462336: step 7302, loss 0.00384961, acc 1, prec 0.106129, recall 0.793746
2017-12-10T05:43:57.740829: step 7303, loss 0.29991, acc 0.953125, prec 0.106137, recall 0.793765
2017-12-10T05:43:58.008834: step 7304, loss 0.0534438, acc 0.96875, prec 0.106145, recall 0.793785
2017-12-10T05:43:58.279098: step 7305, loss 0.189928, acc 0.953125, prec 0.106141, recall 0.793785
2017-12-10T05:43:58.551005: step 7306, loss 0.00158424, acc 1, prec 0.106141, recall 0.793785
2017-12-10T05:43:58.817873: step 7307, loss 0.484702, acc 0.953125, prec 0.106137, recall 0.793785
2017-12-10T05:43:59.081421: step 7308, loss 0.0991718, acc 0.984375, prec 0.106136, recall 0.793785
2017-12-10T05:43:59.342798: step 7309, loss 0.405879, acc 0.96875, prec 0.106156, recall 0.793824
2017-12-10T05:43:59.610698: step 7310, loss 0.0969785, acc 0.984375, prec 0.106155, recall 0.793824
2017-12-10T05:43:59.876262: step 7311, loss 0.0741324, acc 0.984375, prec 0.106165, recall 0.793844
2017-12-10T05:44:00.141472: step 7312, loss 0.0109603, acc 1, prec 0.106176, recall 0.793863
2017-12-10T05:44:00.424583: step 7313, loss 0.0587033, acc 0.96875, prec 0.106173, recall 0.793863
2017-12-10T05:44:00.696594: step 7314, loss 0.363563, acc 0.96875, prec 0.106171, recall 0.793863
2017-12-10T05:44:00.965350: step 7315, loss 0.389574, acc 0.96875, prec 0.106179, recall 0.793883
2017-12-10T05:44:01.237437: step 7316, loss 0.541011, acc 1, prec 0.106213, recall 0.793942
2017-12-10T05:44:01.515895: step 7317, loss 0.000768455, acc 1, prec 0.106225, recall 0.793961
2017-12-10T05:44:01.782072: step 7318, loss 0.000730716, acc 1, prec 0.106236, recall 0.793981
2017-12-10T05:44:02.045765: step 7319, loss 0.357922, acc 0.953125, prec 0.106243, recall 0.794
2017-12-10T05:44:02.312711: step 7320, loss 0.112198, acc 0.984375, prec 0.106253, recall 0.79402
2017-12-10T05:44:02.580052: step 7321, loss 0.204999, acc 0.984375, prec 0.106263, recall 0.794039
2017-12-10T05:44:02.842737: step 7322, loss 0.0748903, acc 0.96875, prec 0.106261, recall 0.794039
2017-12-10T05:44:03.110178: step 7323, loss 0.0139069, acc 1, prec 0.106306, recall 0.794118
2017-12-10T05:44:03.374574: step 7324, loss 0.283233, acc 0.9375, prec 0.106312, recall 0.794137
2017-12-10T05:44:03.639403: step 7325, loss 0.598808, acc 0.90625, prec 0.106315, recall 0.794157
2017-12-10T05:44:03.900038: step 7326, loss 0.0250715, acc 0.984375, prec 0.106325, recall 0.794176
2017-12-10T05:44:04.164559: step 7327, loss 0.979902, acc 0.984375, prec 0.106335, recall 0.794196
2017-12-10T05:44:04.440707: step 7328, loss 0.400603, acc 0.96875, prec 0.106355, recall 0.794235
2017-12-10T05:44:04.705453: step 7329, loss 0.0622116, acc 0.984375, prec 0.106365, recall 0.794254
2017-12-10T05:44:04.974617: step 7330, loss 0.166415, acc 0.984375, prec 0.106375, recall 0.794274
2017-12-10T05:44:05.247762: step 7331, loss 0.0198541, acc 0.984375, prec 0.106396, recall 0.794313
2017-12-10T05:44:05.521890: step 7332, loss 0.275895, acc 0.9375, prec 0.106391, recall 0.794313
2017-12-10T05:44:05.791188: step 7333, loss 0.468714, acc 0.953125, prec 0.106387, recall 0.794313
2017-12-10T05:44:06.056272: step 7334, loss 0.241076, acc 0.9375, prec 0.106382, recall 0.794313
2017-12-10T05:44:06.330378: step 7335, loss 0.310185, acc 0.953125, prec 0.106389, recall 0.794332
2017-12-10T05:44:06.601684: step 7336, loss 0.00559976, acc 1, prec 0.106423, recall 0.794391
2017-12-10T05:44:06.866352: step 7337, loss 0.256938, acc 0.96875, prec 0.106454, recall 0.794449
2017-12-10T05:44:07.131949: step 7338, loss 0.0913869, acc 0.96875, prec 0.106452, recall 0.794449
2017-12-10T05:44:07.397382: step 7339, loss 0.413294, acc 0.953125, prec 0.106448, recall 0.794449
2017-12-10T05:44:07.660915: step 7340, loss 0.0991559, acc 0.96875, prec 0.106456, recall 0.794469
2017-12-10T05:44:07.932569: step 7341, loss 0.560916, acc 0.890625, prec 0.106481, recall 0.794527
2017-12-10T05:44:08.206860: step 7342, loss 0.0416364, acc 0.96875, prec 0.106501, recall 0.794566
2017-12-10T05:44:08.477955: step 7343, loss 0.181321, acc 1, prec 0.106546, recall 0.794644
2017-12-10T05:44:08.750820: step 7344, loss 0.0267387, acc 0.984375, prec 0.106579, recall 0.794702
2017-12-10T05:44:09.014443: step 7345, loss 0.127193, acc 0.96875, prec 0.106576, recall 0.794702
2017-12-10T05:44:09.290251: step 7346, loss 0.085661, acc 0.984375, prec 0.106597, recall 0.794741
2017-12-10T05:44:09.566608: step 7347, loss 0.11383, acc 0.953125, prec 0.106593, recall 0.794741
2017-12-10T05:44:09.835359: step 7348, loss 0.425999, acc 0.953125, prec 0.106601, recall 0.79476
2017-12-10T05:44:10.097253: step 7349, loss 0.846658, acc 0.90625, prec 0.106604, recall 0.79478
2017-12-10T05:44:10.367617: step 7350, loss 0.140596, acc 0.984375, prec 0.106602, recall 0.79478
2017-12-10T05:44:10.634753: step 7351, loss 0.026135, acc 0.984375, prec 0.106624, recall 0.794818
2017-12-10T05:44:10.899009: step 7352, loss 0.10638, acc 0.984375, prec 0.106634, recall 0.794838
2017-12-10T05:44:11.168485: step 7353, loss 0.0367389, acc 0.984375, prec 0.106644, recall 0.794857
2017-12-10T05:44:11.437218: step 7354, loss 0.0070372, acc 1, prec 0.106655, recall 0.794877
2017-12-10T05:44:11.713554: step 7355, loss 0.295954, acc 0.953125, prec 0.106651, recall 0.794877
2017-12-10T05:44:11.983732: step 7356, loss 0.0599799, acc 0.984375, prec 0.106661, recall 0.794896
2017-12-10T05:44:12.252118: step 7357, loss 0.0405171, acc 0.984375, prec 0.106694, recall 0.794954
2017-12-10T05:44:12.526513: step 7358, loss 0.0370249, acc 0.984375, prec 0.106692, recall 0.794954
2017-12-10T05:44:12.794782: step 7359, loss 0.151257, acc 0.96875, prec 0.106701, recall 0.794974
2017-12-10T05:44:13.071830: step 7360, loss 0.0738062, acc 0.984375, prec 0.106711, recall 0.794993
2017-12-10T05:44:13.336363: step 7361, loss 0.000668222, acc 1, prec 0.106711, recall 0.794993
2017-12-10T05:44:13.601117: step 7362, loss 0.688933, acc 0.953125, prec 0.106718, recall 0.795012
2017-12-10T05:44:13.867576: step 7363, loss 0.000589834, acc 1, prec 0.106729, recall 0.795032
2017-12-10T05:44:14.130008: step 7364, loss 0.47032, acc 0.96875, prec 0.106749, recall 0.79507
2017-12-10T05:44:14.400512: step 7365, loss 0.0377646, acc 0.984375, prec 0.106771, recall 0.795109
2017-12-10T05:44:14.675819: step 7366, loss 0.0628318, acc 0.984375, prec 0.106781, recall 0.795128
2017-12-10T05:44:14.947149: step 7367, loss 0.0226372, acc 0.984375, prec 0.106779, recall 0.795128
2017-12-10T05:44:15.222583: step 7368, loss 0.0968843, acc 0.96875, prec 0.106777, recall 0.795128
2017-12-10T05:44:15.486758: step 7369, loss 0.000537626, acc 1, prec 0.106777, recall 0.795128
2017-12-10T05:44:15.757112: step 7370, loss 0.0720072, acc 0.984375, prec 0.106787, recall 0.795148
2017-12-10T05:44:16.022450: step 7371, loss 0.000447164, acc 1, prec 0.106787, recall 0.795148
2017-12-10T05:44:16.299613: step 7372, loss 2.74236, acc 0.96875, prec 0.106808, recall 0.795111
2017-12-10T05:44:16.565208: step 7373, loss 0.592547, acc 1, prec 0.10683, recall 0.79515
2017-12-10T05:44:16.830441: step 7374, loss 0.00828167, acc 1, prec 0.106842, recall 0.795169
2017-12-10T05:44:17.103441: step 7375, loss 0.000714007, acc 1, prec 0.106842, recall 0.795169
2017-12-10T05:44:17.365129: step 7376, loss 0.316474, acc 0.96875, prec 0.10685, recall 0.795189
2017-12-10T05:44:17.636273: step 7377, loss 0.391418, acc 0.96875, prec 0.106848, recall 0.795189
2017-12-10T05:44:17.904949: step 7378, loss 0.0539424, acc 0.96875, prec 0.106845, recall 0.795189
2017-12-10T05:44:18.178148: step 7379, loss 0.887124, acc 0.921875, prec 0.106883, recall 0.795266
2017-12-10T05:44:18.441835: step 7380, loss 0.247958, acc 0.953125, prec 0.106902, recall 0.795305
2017-12-10T05:44:18.713986: step 7381, loss 0.340968, acc 0.921875, prec 0.106895, recall 0.795305
2017-12-10T05:44:18.986251: step 7382, loss 0.00631747, acc 1, prec 0.106895, recall 0.795305
2017-12-10T05:44:19.256975: step 7383, loss 0.181159, acc 0.890625, prec 0.106886, recall 0.795305
2017-12-10T05:44:19.518782: step 7384, loss 0.0281671, acc 0.984375, prec 0.106884, recall 0.795305
2017-12-10T05:44:19.785823: step 7385, loss 0.130818, acc 0.96875, prec 0.106882, recall 0.795305
2017-12-10T05:44:20.060251: step 7386, loss 0.58083, acc 0.921875, prec 0.106875, recall 0.795305
2017-12-10T05:44:20.328588: step 7387, loss 0.401535, acc 0.90625, prec 0.106867, recall 0.795305
2017-12-10T05:44:20.599283: step 7388, loss 0.0627481, acc 0.96875, prec 0.106864, recall 0.795305
2017-12-10T05:44:20.870563: step 7389, loss 1.16161, acc 0.890625, prec 0.106855, recall 0.795305
2017-12-10T05:44:21.133498: step 7390, loss 0.118918, acc 0.984375, prec 0.106853, recall 0.795305
2017-12-10T05:44:21.413210: step 7391, loss 0.235697, acc 0.953125, prec 0.106895, recall 0.795382
2017-12-10T05:44:21.675060: step 7392, loss 0.0629114, acc 0.984375, prec 0.106893, recall 0.795382
2017-12-10T05:44:21.942851: step 7393, loss 0.258104, acc 0.96875, prec 0.10689, recall 0.795382
2017-12-10T05:44:22.213478: step 7394, loss 0.121901, acc 0.96875, prec 0.10691, recall 0.79542
2017-12-10T05:44:22.491699: step 7395, loss 0.214353, acc 0.9375, prec 0.106916, recall 0.79544
2017-12-10T05:44:22.763107: step 7396, loss 0.210844, acc 0.9375, prec 0.106911, recall 0.79544
2017-12-10T05:44:23.038448: step 7397, loss 0.133339, acc 0.96875, prec 0.106908, recall 0.79544
2017-12-10T05:44:23.303551: step 7398, loss 0.266368, acc 0.953125, prec 0.106915, recall 0.795459
2017-12-10T05:44:23.575963: step 7399, loss 0.180695, acc 0.921875, prec 0.10692, recall 0.795478
2017-12-10T05:44:23.842255: step 7400, loss 0.59812, acc 0.953125, prec 0.106938, recall 0.795517
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-7400

2017-12-10T05:44:25.153665: step 7401, loss 0.579967, acc 0.953125, prec 0.106946, recall 0.795536
2017-12-10T05:44:25.424824: step 7402, loss 0.0532691, acc 0.984375, prec 0.106967, recall 0.795574
2017-12-10T05:44:25.689102: step 7403, loss 0.00154301, acc 1, prec 0.106978, recall 0.795594
2017-12-10T05:44:25.955557: step 7404, loss 0.0247626, acc 0.984375, prec 0.106977, recall 0.795594
2017-12-10T05:44:26.224686: step 7405, loss 0.197667, acc 0.953125, prec 0.106973, recall 0.795594
2017-12-10T05:44:26.494778: step 7406, loss 0.322144, acc 0.96875, prec 0.106981, recall 0.795613
2017-12-10T05:44:26.759399: step 7407, loss 0.0526826, acc 0.984375, prec 0.106991, recall 0.795632
2017-12-10T05:44:27.026710: step 7408, loss 0.367831, acc 0.9375, prec 0.106997, recall 0.795651
2017-12-10T05:44:27.299587: step 7409, loss 0.232743, acc 0.984375, prec 0.107019, recall 0.79569
2017-12-10T05:44:27.575655: step 7410, loss 0.347535, acc 0.984375, prec 0.107017, recall 0.79569
2017-12-10T05:44:27.842436: step 7411, loss 0.00112487, acc 1, prec 0.107017, recall 0.79569
2017-12-10T05:44:28.111371: step 7412, loss 0.943704, acc 0.96875, prec 0.107026, recall 0.795709
2017-12-10T05:44:28.379803: step 7413, loss 0.103462, acc 0.984375, prec 0.107024, recall 0.795709
2017-12-10T05:44:28.649856: step 7414, loss 0.0308534, acc 0.984375, prec 0.107034, recall 0.795728
2017-12-10T05:44:28.918303: step 7415, loss 0.000493901, acc 1, prec 0.107057, recall 0.795767
2017-12-10T05:44:29.188493: step 7416, loss 0.00163288, acc 1, prec 0.107068, recall 0.795786
2017-12-10T05:44:29.448060: step 7417, loss 0.0508144, acc 0.984375, prec 0.107078, recall 0.795805
2017-12-10T05:44:29.718868: step 7418, loss 0.0335866, acc 0.984375, prec 0.107077, recall 0.795805
2017-12-10T05:44:29.997247: step 7419, loss 0.0490872, acc 0.953125, prec 0.107084, recall 0.795824
2017-12-10T05:44:30.259953: step 7420, loss 0.14086, acc 0.984375, prec 0.107094, recall 0.795844
2017-12-10T05:44:30.536766: step 7421, loss 0.148528, acc 0.96875, prec 0.107114, recall 0.795882
2017-12-10T05:44:30.812102: step 7422, loss 0.982164, acc 0.96875, prec 0.107123, recall 0.795901
2017-12-10T05:44:31.081748: step 7423, loss 0.0312356, acc 0.984375, prec 0.107121, recall 0.795901
2017-12-10T05:44:31.346493: step 7424, loss 0.119175, acc 0.953125, prec 0.107128, recall 0.79592
2017-12-10T05:44:31.604646: step 7425, loss 0.0428152, acc 0.984375, prec 0.107127, recall 0.79592
2017-12-10T05:44:31.873405: step 7426, loss 0.0453719, acc 0.984375, prec 0.107126, recall 0.79592
2017-12-10T05:44:32.143084: step 7427, loss 0.10157, acc 0.96875, prec 0.107123, recall 0.79592
2017-12-10T05:44:32.408353: step 7428, loss 0.681279, acc 0.953125, prec 0.107119, recall 0.79592
2017-12-10T05:44:32.680184: step 7429, loss 0.00807854, acc 1, prec 0.10713, recall 0.795939
2017-12-10T05:44:32.945544: step 7430, loss 0.000984939, acc 1, prec 0.107142, recall 0.795959
2017-12-10T05:44:33.207051: step 7431, loss 0.0076741, acc 1, prec 0.107153, recall 0.795978
2017-12-10T05:44:33.467545: step 7432, loss 0.0902416, acc 0.96875, prec 0.107161, recall 0.795997
2017-12-10T05:44:33.741268: step 7433, loss 0.00139836, acc 1, prec 0.107173, recall 0.796016
2017-12-10T05:44:34.001275: step 7434, loss 8.94015e-05, acc 1, prec 0.107173, recall 0.796016
2017-12-10T05:44:34.265331: step 7435, loss 0.415948, acc 0.9375, prec 0.107179, recall 0.796035
2017-12-10T05:44:34.529246: step 7436, loss 0.044443, acc 0.984375, prec 0.107177, recall 0.796035
2017-12-10T05:44:34.799123: step 7437, loss 0.0202804, acc 1, prec 0.107188, recall 0.796054
2017-12-10T05:44:35.064805: step 7438, loss 0.535917, acc 0.96875, prec 0.107208, recall 0.796093
2017-12-10T05:44:35.328826: step 7439, loss 0.0240953, acc 0.984375, prec 0.107241, recall 0.79615
2017-12-10T05:44:35.595921: step 7440, loss 0.331226, acc 0.96875, prec 0.107249, recall 0.796169
2017-12-10T05:44:35.861091: step 7441, loss 0.212245, acc 0.96875, prec 0.107258, recall 0.796189
2017-12-10T05:44:36.133342: step 7442, loss 0.00463912, acc 1, prec 0.107292, recall 0.796246
2017-12-10T05:44:36.393212: step 7443, loss 2.98006, acc 0.9375, prec 0.107299, recall 0.79619
2017-12-10T05:44:36.668018: step 7444, loss 0.00389647, acc 1, prec 0.107322, recall 0.796229
2017-12-10T05:44:36.933830: step 7445, loss 0.105905, acc 0.96875, prec 0.107319, recall 0.796229
2017-12-10T05:44:37.202833: step 7446, loss 0.373415, acc 0.96875, prec 0.107328, recall 0.796248
2017-12-10T05:44:37.469096: step 7447, loss 0.0174195, acc 0.984375, prec 0.107326, recall 0.796248
2017-12-10T05:44:37.729036: step 7448, loss 0.0730717, acc 0.984375, prec 0.107336, recall 0.796267
2017-12-10T05:44:37.998115: step 7449, loss 0.582299, acc 0.921875, prec 0.107352, recall 0.796305
2017-12-10T05:44:38.264961: step 7450, loss 0.716237, acc 0.9375, prec 0.107358, recall 0.796324
2017-12-10T05:44:38.537874: step 7451, loss 0.294848, acc 0.921875, prec 0.107362, recall 0.796343
2017-12-10T05:44:38.800029: step 7452, loss 0.285792, acc 0.953125, prec 0.107358, recall 0.796343
2017-12-10T05:44:39.772013: step 7453, loss 0.0806709, acc 0.96875, prec 0.107355, recall 0.796343
2017-12-10T05:44:40.144850: step 7454, loss 0.918609, acc 0.890625, prec 0.107346, recall 0.796343
2017-12-10T05:44:40.381711: step 7455, loss 0.236172, acc 0.961538, prec 0.107343, recall 0.796343
2017-12-10T05:44:41.143451: step 7456, loss 0.410165, acc 0.984375, prec 0.107353, recall 0.796362
2017-12-10T05:44:41.877929: step 7457, loss 0.579104, acc 0.9375, prec 0.107359, recall 0.796381
2017-12-10T05:44:42.624308: step 7458, loss 0.0127052, acc 1, prec 0.107404, recall 0.796458
2017-12-10T05:44:43.335233: step 7459, loss 0.109378, acc 0.984375, prec 0.107414, recall 0.796477
2017-12-10T05:44:44.083916: step 7460, loss 0.189446, acc 0.984375, prec 0.107413, recall 0.796477
2017-12-10T05:44:44.958024: step 7461, loss 0.0739553, acc 0.96875, prec 0.10741, recall 0.796477
2017-12-10T05:44:45.377057: step 7462, loss 0.166633, acc 0.96875, prec 0.107419, recall 0.796496
2017-12-10T05:44:45.662362: step 7463, loss 0.00375033, acc 1, prec 0.107419, recall 0.796496
2017-12-10T05:44:45.946942: step 7464, loss 0.341794, acc 0.96875, prec 0.107427, recall 0.796515
2017-12-10T05:44:46.239408: step 7465, loss 0.0572312, acc 0.984375, prec 0.107448, recall 0.796553
2017-12-10T05:44:46.520485: step 7466, loss 0.0857129, acc 0.96875, prec 0.107468, recall 0.796591
2017-12-10T05:44:46.796962: step 7467, loss 0.0874479, acc 0.96875, prec 0.107477, recall 0.79661
2017-12-10T05:44:47.062928: step 7468, loss 0.147631, acc 0.9375, prec 0.107483, recall 0.796629
2017-12-10T05:44:47.336433: step 7469, loss 0.225562, acc 0.953125, prec 0.10749, recall 0.796648
2017-12-10T05:44:47.611146: step 7470, loss 0.0725349, acc 0.984375, prec 0.1075, recall 0.796667
2017-12-10T05:44:47.876775: step 7471, loss 0.22626, acc 0.96875, prec 0.107508, recall 0.796686
2017-12-10T05:44:48.139178: step 7472, loss 0.128145, acc 0.953125, prec 0.107504, recall 0.796686
2017-12-10T05:44:48.405183: step 7473, loss 0.177325, acc 0.984375, prec 0.107514, recall 0.796705
2017-12-10T05:44:48.669130: step 7474, loss 0.0971787, acc 0.984375, prec 0.107524, recall 0.796724
2017-12-10T05:44:48.935552: step 7475, loss 0.321328, acc 0.96875, prec 0.107544, recall 0.796762
2017-12-10T05:44:49.203746: step 7476, loss 0.254267, acc 0.984375, prec 0.107542, recall 0.796762
2017-12-10T05:44:49.475322: step 7477, loss 0.45627, acc 0.984375, prec 0.107564, recall 0.7968
2017-12-10T05:44:49.751398: step 7478, loss 0.0647492, acc 0.984375, prec 0.107562, recall 0.7968
2017-12-10T05:44:50.013963: step 7479, loss 0.0012421, acc 1, prec 0.107574, recall 0.796819
2017-12-10T05:44:50.279538: step 7480, loss 0.0796768, acc 0.96875, prec 0.107593, recall 0.796857
2017-12-10T05:44:50.546057: step 7481, loss 0.0216978, acc 0.984375, prec 0.107603, recall 0.796876
2017-12-10T05:44:50.813946: step 7482, loss 0.294063, acc 1, prec 0.107615, recall 0.796895
2017-12-10T05:44:51.096057: step 7483, loss 0.201203, acc 0.984375, prec 0.107625, recall 0.796914
2017-12-10T05:44:51.362734: step 7484, loss 12.1903, acc 0.9375, prec 0.107632, recall 0.796859
2017-12-10T05:44:51.633447: step 7485, loss 0.337361, acc 0.96875, prec 0.107629, recall 0.796859
2017-12-10T05:44:51.896582: step 7486, loss 0.477635, acc 0.953125, prec 0.107625, recall 0.796859
2017-12-10T05:44:52.166943: step 7487, loss 0.183516, acc 0.953125, prec 0.107621, recall 0.796859
2017-12-10T05:44:52.442992: step 7488, loss 0.279143, acc 0.90625, prec 0.107613, recall 0.796859
2017-12-10T05:44:52.714873: step 7489, loss 1.04779, acc 0.796875, prec 0.107606, recall 0.796878
2017-12-10T05:44:52.984703: step 7490, loss 0.608939, acc 0.90625, prec 0.107621, recall 0.796916
2017-12-10T05:44:53.247913: step 7491, loss 0.257915, acc 0.90625, prec 0.107624, recall 0.796935
2017-12-10T05:44:53.515653: step 7492, loss 0.215254, acc 0.953125, prec 0.107631, recall 0.796954
2017-12-10T05:44:53.782594: step 7493, loss 0.546402, acc 0.921875, prec 0.107647, recall 0.796992
2017-12-10T05:44:54.053988: step 7494, loss 1.05592, acc 0.921875, prec 0.107662, recall 0.79703
2017-12-10T05:44:54.322351: step 7495, loss 0.717488, acc 0.859375, prec 0.107673, recall 0.797068
2017-12-10T05:44:54.592420: step 7496, loss 0.12152, acc 0.9375, prec 0.107679, recall 0.797087
2017-12-10T05:44:54.855248: step 7497, loss 0.169123, acc 0.9375, prec 0.107696, recall 0.797124
2017-12-10T05:44:55.131681: step 7498, loss 0.452103, acc 0.9375, prec 0.10769, recall 0.797124
2017-12-10T05:44:55.404864: step 7499, loss 0.29147, acc 0.96875, prec 0.107687, recall 0.797124
2017-12-10T05:44:55.668461: step 7500, loss 0.684619, acc 0.90625, prec 0.107713, recall 0.797181

Evaluation:
2017-12-10T05:45:03.412877: step 7500, loss 5.77278, acc 0.920268, prec 0.107657, recall 0.794001

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-7500

2017-12-10T05:45:04.678317: step 7501, loss 0.136282, acc 0.96875, prec 0.107665, recall 0.79402
2017-12-10T05:45:04.943275: step 7502, loss 0.72315, acc 0.9375, prec 0.10766, recall 0.79402
2017-12-10T05:45:05.212281: step 7503, loss 0.388815, acc 0.9375, prec 0.107655, recall 0.79402
2017-12-10T05:45:05.483576: step 7504, loss 0.555092, acc 0.90625, prec 0.107658, recall 0.794039
2017-12-10T05:45:05.756704: step 7505, loss 1.04848, acc 0.921875, prec 0.107651, recall 0.794039
2017-12-10T05:45:06.029901: step 7506, loss 0.144723, acc 0.953125, prec 0.107647, recall 0.794039
2017-12-10T05:45:06.304920: step 7507, loss 0.0429705, acc 0.984375, prec 0.107646, recall 0.794039
2017-12-10T05:45:06.570070: step 7508, loss 0.0340699, acc 0.984375, prec 0.107644, recall 0.794039
2017-12-10T05:45:06.833270: step 7509, loss 0.697352, acc 0.953125, prec 0.107651, recall 0.794058
2017-12-10T05:45:07.098271: step 7510, loss 0.229836, acc 0.96875, prec 0.10766, recall 0.794077
2017-12-10T05:45:07.364719: step 7511, loss 0.224533, acc 0.96875, prec 0.107668, recall 0.794096
2017-12-10T05:45:07.633323: step 7512, loss 0.0289793, acc 1, prec 0.107679, recall 0.794115
2017-12-10T05:45:07.901553: step 7513, loss 0.0108627, acc 1, prec 0.107679, recall 0.794115
2017-12-10T05:45:08.164745: step 7514, loss 0.285349, acc 0.96875, prec 0.107688, recall 0.794134
2017-12-10T05:45:08.432869: step 7515, loss 1.0588, acc 0.96875, prec 0.107719, recall 0.794191
2017-12-10T05:45:08.697548: step 7516, loss 0.000147771, acc 1, prec 0.107719, recall 0.794191
2017-12-10T05:45:08.959677: step 7517, loss 0.288845, acc 0.96875, prec 0.107727, recall 0.79421
2017-12-10T05:45:09.230220: step 7518, loss 0.298137, acc 0.96875, prec 0.107735, recall 0.794228
2017-12-10T05:45:09.496462: step 7519, loss 0.124568, acc 0.953125, prec 0.107731, recall 0.794228
2017-12-10T05:45:09.765264: step 7520, loss 0.201135, acc 0.96875, prec 0.107751, recall 0.794266
2017-12-10T05:45:10.036684: step 7521, loss 0.171757, acc 0.953125, prec 0.107769, recall 0.794304
2017-12-10T05:45:10.300764: step 7522, loss 0.311635, acc 0.9375, prec 0.107764, recall 0.794304
2017-12-10T05:45:10.568052: step 7523, loss 0.00340938, acc 1, prec 0.107775, recall 0.794323
2017-12-10T05:45:10.836206: step 7524, loss 0.433226, acc 0.96875, prec 0.107772, recall 0.794323
2017-12-10T05:45:11.098939: step 7525, loss 0.186771, acc 0.984375, prec 0.107793, recall 0.794361
2017-12-10T05:45:11.365578: step 7526, loss 0.442394, acc 0.9375, prec 0.107788, recall 0.794361
2017-12-10T05:45:11.630027: step 7527, loss 0.000184684, acc 1, prec 0.10781, recall 0.794399
2017-12-10T05:45:11.895922: step 7528, loss 0.0476519, acc 0.96875, prec 0.107807, recall 0.794399
2017-12-10T05:45:12.161413: step 7529, loss 0.344276, acc 0.9375, prec 0.107802, recall 0.794399
2017-12-10T05:45:12.438846: step 7530, loss 0.624872, acc 0.953125, prec 0.107798, recall 0.794399
2017-12-10T05:45:12.710664: step 7531, loss 0.0861307, acc 0.96875, prec 0.107806, recall 0.794417
2017-12-10T05:45:12.978361: step 7532, loss 0.0148393, acc 1, prec 0.107806, recall 0.794417
2017-12-10T05:45:13.252566: step 7533, loss 0.230457, acc 0.96875, prec 0.107815, recall 0.794436
2017-12-10T05:45:13.527745: step 7534, loss 0.125464, acc 0.953125, prec 0.107833, recall 0.794474
2017-12-10T05:45:13.804610: step 7535, loss 0.151191, acc 0.96875, prec 0.10783, recall 0.794474
2017-12-10T05:45:14.077147: step 7536, loss 0.289288, acc 0.953125, prec 0.107826, recall 0.794474
2017-12-10T05:45:14.346820: step 7537, loss 0.675382, acc 0.96875, prec 0.107835, recall 0.794493
2017-12-10T05:45:14.617725: step 7538, loss 0.0925183, acc 0.984375, prec 0.107833, recall 0.794493
2017-12-10T05:45:14.882440: step 7539, loss 0.366747, acc 0.96875, prec 0.107831, recall 0.794493
2017-12-10T05:45:15.152835: step 7540, loss 0.289248, acc 0.96875, prec 0.107828, recall 0.794493
2017-12-10T05:45:15.419382: step 7541, loss 0.0975725, acc 0.96875, prec 0.107836, recall 0.794512
2017-12-10T05:45:15.690040: step 7542, loss 6.62797, acc 0.96875, prec 0.107835, recall 0.794439
2017-12-10T05:45:15.959644: step 7543, loss 0.0479989, acc 0.96875, prec 0.107844, recall 0.794458
2017-12-10T05:45:16.221213: step 7544, loss 0.0882187, acc 0.96875, prec 0.107852, recall 0.794477
2017-12-10T05:45:16.487530: step 7545, loss 0.300122, acc 0.953125, prec 0.107881, recall 0.794533
2017-12-10T05:45:16.753017: step 7546, loss 0.12788, acc 0.96875, prec 0.107901, recall 0.794571
2017-12-10T05:45:17.016639: step 7547, loss 0.143382, acc 0.953125, prec 0.107897, recall 0.794571
2017-12-10T05:45:17.291277: step 7548, loss 0.346707, acc 0.9375, prec 0.107891, recall 0.794571
2017-12-10T05:45:17.556687: step 7549, loss 0.0126407, acc 0.984375, prec 0.107901, recall 0.79459
2017-12-10T05:45:17.828732: step 7550, loss 1.1428, acc 0.859375, prec 0.1079, recall 0.794608
2017-12-10T05:45:18.089752: step 7551, loss 0.162849, acc 0.96875, prec 0.107897, recall 0.794608
2017-12-10T05:45:18.354680: step 7552, loss 1.0912, acc 0.90625, prec 0.107934, recall 0.794684
2017-12-10T05:45:18.620638: step 7553, loss 0.0419897, acc 0.984375, prec 0.107932, recall 0.794684
2017-12-10T05:45:18.893232: step 7554, loss 0.2437, acc 0.953125, prec 0.10794, recall 0.794703
2017-12-10T05:45:19.160386: step 7555, loss 0.33521, acc 0.9375, prec 0.107945, recall 0.794721
2017-12-10T05:45:19.427185: step 7556, loss 0.286137, acc 0.9375, prec 0.10794, recall 0.794721
2017-12-10T05:45:19.692996: step 7557, loss 0.228397, acc 0.953125, prec 0.107947, recall 0.79474
2017-12-10T05:45:19.964271: step 7558, loss 0.000235888, acc 1, prec 0.107958, recall 0.794759
2017-12-10T05:45:20.227955: step 7559, loss 0.0729645, acc 0.984375, prec 0.107957, recall 0.794759
2017-12-10T05:45:20.493530: step 7560, loss 0.267725, acc 0.984375, prec 0.107978, recall 0.794797
2017-12-10T05:45:20.764271: step 7561, loss 0.701954, acc 0.953125, prec 0.107985, recall 0.794815
2017-12-10T05:45:21.030365: step 7562, loss 0.31356, acc 0.96875, prec 0.107982, recall 0.794815
2017-12-10T05:45:21.297599: step 7563, loss 0.24993, acc 0.953125, prec 0.107989, recall 0.794834
2017-12-10T05:45:21.565523: step 7564, loss 0.0994348, acc 0.96875, prec 0.107997, recall 0.794853
2017-12-10T05:45:21.836213: step 7565, loss 0.00436782, acc 1, prec 0.108009, recall 0.794872
2017-12-10T05:45:22.096620: step 7566, loss 0.139949, acc 0.953125, prec 0.108005, recall 0.794872
2017-12-10T05:45:22.359202: step 7567, loss 0.0274357, acc 0.984375, prec 0.108025, recall 0.794909
2017-12-10T05:45:22.632851: step 7568, loss 0.266532, acc 0.953125, prec 0.108021, recall 0.794909
2017-12-10T05:45:22.894669: step 7569, loss 0.00522406, acc 1, prec 0.108055, recall 0.794966
2017-12-10T05:45:23.163510: step 7570, loss 0.3743, acc 0.953125, prec 0.108051, recall 0.794966
2017-12-10T05:45:23.432716: step 7571, loss 0.27926, acc 0.96875, prec 0.108048, recall 0.794966
2017-12-10T05:45:23.705276: step 7572, loss 0.481462, acc 0.953125, prec 0.108077, recall 0.795022
2017-12-10T05:45:23.974484: step 7573, loss 0.418718, acc 0.953125, prec 0.108073, recall 0.795022
2017-12-10T05:45:24.241635: step 7574, loss 0.00457586, acc 1, prec 0.108095, recall 0.795059
2017-12-10T05:45:24.505393: step 7575, loss 0.0547219, acc 0.984375, prec 0.108094, recall 0.795059
2017-12-10T05:45:24.777743: step 7576, loss 0.00343879, acc 1, prec 0.108138, recall 0.795134
2017-12-10T05:45:25.041502: step 7577, loss 0.0723508, acc 0.984375, prec 0.108159, recall 0.795172
2017-12-10T05:45:25.314085: step 7578, loss 0.175713, acc 0.984375, prec 0.108158, recall 0.795172
2017-12-10T05:45:25.581262: step 7579, loss 0.333809, acc 0.96875, prec 0.108155, recall 0.795172
2017-12-10T05:45:25.855691: step 7580, loss 0.00846902, acc 1, prec 0.108155, recall 0.795172
2017-12-10T05:45:26.124384: step 7581, loss 0.25291, acc 0.984375, prec 0.108165, recall 0.795191
2017-12-10T05:45:26.393174: step 7582, loss 0.611574, acc 0.984375, prec 0.108186, recall 0.795228
2017-12-10T05:45:26.657099: step 7583, loss 0.26281, acc 0.96875, prec 0.108205, recall 0.795265
2017-12-10T05:45:26.922695: step 7584, loss 0.403097, acc 0.953125, prec 0.108234, recall 0.795322
2017-12-10T05:45:27.196977: step 7585, loss 0.430471, acc 0.96875, prec 0.108243, recall 0.79534
2017-12-10T05:45:27.477999: step 7586, loss 0.0689188, acc 0.984375, prec 0.108242, recall 0.79534
2017-12-10T05:45:27.746486: step 7587, loss 0.143216, acc 0.96875, prec 0.10825, recall 0.795359
2017-12-10T05:45:28.009705: step 7588, loss 0.00858311, acc 1, prec 0.108283, recall 0.795415
2017-12-10T05:45:28.276527: step 7589, loss 0.0116492, acc 0.984375, prec 0.108282, recall 0.795415
2017-12-10T05:45:28.542714: step 7590, loss 0.357227, acc 0.984375, prec 0.108314, recall 0.795471
2017-12-10T05:45:28.817245: step 7591, loss 0.374479, acc 0.984375, prec 0.108323, recall 0.79549
2017-12-10T05:45:29.093298: step 7592, loss 0.0469632, acc 0.984375, prec 0.108322, recall 0.79549
2017-12-10T05:45:29.370102: step 7593, loss 0.0230092, acc 0.984375, prec 0.108332, recall 0.795509
2017-12-10T05:45:29.637659: step 7594, loss 0.145904, acc 0.984375, prec 0.108331, recall 0.795509
2017-12-10T05:45:29.902304: step 7595, loss 0.400183, acc 0.9375, prec 0.108347, recall 0.795546
2017-12-10T05:45:30.179889: step 7596, loss 0.00342862, acc 1, prec 0.108347, recall 0.795546
2017-12-10T05:45:30.454740: step 7597, loss 0.125634, acc 0.953125, prec 0.108365, recall 0.795583
2017-12-10T05:45:30.731339: step 7598, loss 0.300852, acc 0.953125, prec 0.108395, recall 0.795639
2017-12-10T05:45:31.000001: step 7599, loss 0.0618332, acc 0.984375, prec 0.108404, recall 0.795658
2017-12-10T05:45:31.270145: step 7600, loss 0.00750706, acc 1, prec 0.108404, recall 0.795658
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-7600

2017-12-10T05:45:32.525875: step 7601, loss 0.310426, acc 0.9375, prec 0.10841, recall 0.795676
2017-12-10T05:45:32.801027: step 7602, loss 0.105924, acc 0.984375, prec 0.10842, recall 0.795695
2017-12-10T05:45:33.067433: step 7603, loss 0.11428, acc 0.953125, prec 0.108427, recall 0.795714
2017-12-10T05:45:33.338281: step 7604, loss 0.24777, acc 0.96875, prec 0.108424, recall 0.795714
2017-12-10T05:45:33.614562: step 7605, loss 0.227008, acc 0.953125, prec 0.10842, recall 0.795714
2017-12-10T05:45:33.884266: step 7606, loss 0.00227496, acc 1, prec 0.108431, recall 0.795732
2017-12-10T05:45:34.154899: step 7607, loss 0.0112147, acc 1, prec 0.108453, recall 0.79577
2017-12-10T05:45:34.420427: step 7608, loss 0.00696531, acc 1, prec 0.108464, recall 0.795788
2017-12-10T05:45:34.682860: step 7609, loss 0.00984373, acc 1, prec 0.108464, recall 0.795788
2017-12-10T05:45:34.950642: step 7610, loss 0.000886354, acc 1, prec 0.108476, recall 0.795807
2017-12-10T05:45:35.218974: step 7611, loss 0.121128, acc 0.984375, prec 0.108485, recall 0.795825
2017-12-10T05:45:35.493516: step 7612, loss 0.480142, acc 0.96875, prec 0.108505, recall 0.795863
2017-12-10T05:45:35.770943: step 7613, loss 0.000323046, acc 1, prec 0.108505, recall 0.795863
2017-12-10T05:45:36.036519: step 7614, loss 0.0650656, acc 0.9375, prec 0.108499, recall 0.795863
2017-12-10T05:45:36.300606: step 7615, loss 0.324481, acc 0.953125, prec 0.108506, recall 0.795881
2017-12-10T05:45:36.572227: step 7616, loss 0.715562, acc 0.953125, prec 0.108513, recall 0.7959
2017-12-10T05:45:36.840402: step 7617, loss 0.0531444, acc 0.984375, prec 0.108512, recall 0.7959
2017-12-10T05:45:37.110313: step 7618, loss 0.174402, acc 0.96875, prec 0.10852, recall 0.795918
2017-12-10T05:45:37.374070: step 7619, loss 0.00174792, acc 1, prec 0.10852, recall 0.795918
2017-12-10T05:45:37.645879: step 7620, loss 0.123588, acc 0.984375, prec 0.10853, recall 0.795937
2017-12-10T05:45:37.914987: step 7621, loss 0.106414, acc 0.984375, prec 0.10854, recall 0.795956
2017-12-10T05:45:38.181754: step 7622, loss 0.000158082, acc 1, prec 0.10854, recall 0.795956
2017-12-10T05:45:38.444368: step 7623, loss 0.0150856, acc 0.984375, prec 0.10855, recall 0.795974
2017-12-10T05:45:38.722405: step 7624, loss 0.0317481, acc 0.984375, prec 0.108548, recall 0.795974
2017-12-10T05:45:38.996044: step 7625, loss 0.0544677, acc 1, prec 0.108581, recall 0.79603
2017-12-10T05:45:39.270263: step 7626, loss 0.0466919, acc 0.984375, prec 0.10858, recall 0.79603
2017-12-10T05:45:39.541649: step 7627, loss 0.0326107, acc 0.984375, prec 0.10859, recall 0.796048
2017-12-10T05:45:39.808632: step 7628, loss 0.319365, acc 0.96875, prec 0.108587, recall 0.796048
2017-12-10T05:45:40.077044: step 7629, loss 0.233214, acc 0.953125, prec 0.108583, recall 0.796048
2017-12-10T05:45:40.354151: step 7630, loss 0.000551214, acc 1, prec 0.108583, recall 0.796048
2017-12-10T05:45:40.618690: step 7631, loss 0.00360765, acc 1, prec 0.108627, recall 0.796123
2017-12-10T05:45:40.880317: step 7632, loss 0.0223717, acc 0.984375, prec 0.108648, recall 0.79616
2017-12-10T05:45:41.149775: step 7633, loss 0.740862, acc 0.984375, prec 0.108658, recall 0.796178
2017-12-10T05:45:41.424341: step 7634, loss 0.00926439, acc 1, prec 0.10868, recall 0.796215
2017-12-10T05:45:41.693956: step 7635, loss 0.354666, acc 0.953125, prec 0.108676, recall 0.796215
2017-12-10T05:45:41.961011: step 7636, loss 0.125138, acc 0.953125, prec 0.108672, recall 0.796215
2017-12-10T05:45:42.234374: step 7637, loss 0.594637, acc 0.953125, prec 0.108701, recall 0.796271
2017-12-10T05:45:42.517582: step 7638, loss 0.419984, acc 0.921875, prec 0.108716, recall 0.796308
2017-12-10T05:45:42.784125: step 7639, loss 0.00250105, acc 1, prec 0.108727, recall 0.796327
2017-12-10T05:45:43.054145: step 7640, loss 0.428629, acc 0.953125, prec 0.108723, recall 0.796327
2017-12-10T05:45:43.319364: step 7641, loss 0.0603035, acc 0.96875, prec 0.108732, recall 0.796345
2017-12-10T05:45:43.585016: step 7642, loss 0.127235, acc 0.953125, prec 0.108728, recall 0.796345
2017-12-10T05:45:43.864221: step 7643, loss 0.494777, acc 0.96875, prec 0.108736, recall 0.796364
2017-12-10T05:45:44.132320: step 7644, loss 0.4099, acc 0.96875, prec 0.108733, recall 0.796364
2017-12-10T05:45:44.396958: step 7645, loss 0.472893, acc 0.984375, prec 0.108743, recall 0.796382
2017-12-10T05:45:44.670142: step 7646, loss 0.00862786, acc 1, prec 0.108743, recall 0.796382
2017-12-10T05:45:44.933919: step 7647, loss 0.369088, acc 0.984375, prec 0.108753, recall 0.796401
2017-12-10T05:45:45.197308: step 7648, loss 0.0206487, acc 0.984375, prec 0.108774, recall 0.796438
2017-12-10T05:45:45.472787: step 7649, loss 0.081524, acc 0.984375, prec 0.108783, recall 0.796456
2017-12-10T05:45:45.735841: step 7650, loss 0.367824, acc 0.96875, prec 0.108792, recall 0.796475
2017-12-10T05:45:46.001926: step 7651, loss 0.51924, acc 0.953125, prec 0.108788, recall 0.796475
2017-12-10T05:45:46.271569: step 7652, loss 0.0157741, acc 1, prec 0.108799, recall 0.796493
2017-12-10T05:45:46.535937: step 7653, loss 0.472379, acc 0.96875, prec 0.108807, recall 0.796512
2017-12-10T05:45:46.803423: step 7654, loss 0.369088, acc 0.96875, prec 0.108804, recall 0.796512
2017-12-10T05:45:47.077597: step 7655, loss 0.200934, acc 0.984375, prec 0.108803, recall 0.796512
2017-12-10T05:45:47.347430: step 7656, loss 0.0473002, acc 0.984375, prec 0.108802, recall 0.796512
2017-12-10T05:45:47.613005: step 7657, loss 0.35751, acc 0.953125, prec 0.108798, recall 0.796512
2017-12-10T05:45:47.878369: step 7658, loss 0.377871, acc 0.953125, prec 0.108805, recall 0.79653
2017-12-10T05:45:48.145310: step 7659, loss 0.292228, acc 0.953125, prec 0.108801, recall 0.79653
2017-12-10T05:45:48.411257: step 7660, loss 0.0780192, acc 0.96875, prec 0.108798, recall 0.79653
2017-12-10T05:45:48.686983: step 7661, loss 0.0154422, acc 0.984375, prec 0.108808, recall 0.796549
2017-12-10T05:45:48.963563: step 7662, loss 0.213194, acc 0.984375, prec 0.108817, recall 0.796567
2017-12-10T05:45:49.223820: step 7663, loss 0.553192, acc 1, prec 0.108828, recall 0.796586
2017-12-10T05:45:49.491211: step 7664, loss 0.143006, acc 0.96875, prec 0.108848, recall 0.796622
2017-12-10T05:45:49.752839: step 7665, loss 0.0217269, acc 0.984375, prec 0.108869, recall 0.796659
2017-12-10T05:45:50.020864: step 7666, loss 0.0824886, acc 0.96875, prec 0.108877, recall 0.796678
2017-12-10T05:45:50.293788: step 7667, loss 0.314389, acc 0.96875, prec 0.108885, recall 0.796696
2017-12-10T05:45:50.560349: step 7668, loss 0.0495323, acc 0.984375, prec 0.108884, recall 0.796696
2017-12-10T05:45:50.825749: step 7669, loss 2.36545e-05, acc 1, prec 0.108884, recall 0.796696
2017-12-10T05:45:51.089994: step 7670, loss 0.228964, acc 0.96875, prec 0.108881, recall 0.796696
2017-12-10T05:45:51.359060: step 7671, loss 0.0085915, acc 1, prec 0.108881, recall 0.796696
2017-12-10T05:45:51.625875: step 7672, loss 0.0708425, acc 0.96875, prec 0.108878, recall 0.796696
2017-12-10T05:45:51.894772: step 7673, loss 0.0861962, acc 0.984375, prec 0.108888, recall 0.796715
2017-12-10T05:45:52.162676: step 7674, loss 0.363862, acc 0.953125, prec 0.108906, recall 0.796752
2017-12-10T05:45:52.428749: step 7675, loss 1.99188, acc 0.953125, prec 0.108915, recall 0.796698
2017-12-10T05:45:52.703121: step 7676, loss 0.0069512, acc 1, prec 0.108915, recall 0.796698
2017-12-10T05:45:52.969246: step 7677, loss 0.0525625, acc 0.96875, prec 0.108923, recall 0.796716
2017-12-10T05:45:53.232302: step 7678, loss 0.0242019, acc 0.984375, prec 0.108922, recall 0.796716
2017-12-10T05:45:53.506872: step 7679, loss 0.00120223, acc 1, prec 0.108933, recall 0.796735
2017-12-10T05:45:53.772121: step 7680, loss 0.276352, acc 0.96875, prec 0.10893, recall 0.796735
2017-12-10T05:45:54.035315: step 7681, loss 0.027886, acc 0.984375, prec 0.108929, recall 0.796735
2017-12-10T05:45:54.301944: step 7682, loss 0.150607, acc 0.96875, prec 0.108948, recall 0.796772
2017-12-10T05:45:54.572583: step 7683, loss 0.283744, acc 0.953125, prec 0.108977, recall 0.796827
2017-12-10T05:45:54.842373: step 7684, loss 0.180425, acc 0.953125, prec 0.108973, recall 0.796827
2017-12-10T05:45:55.118786: step 7685, loss 0.533393, acc 0.953125, prec 0.108969, recall 0.796827
2017-12-10T05:45:55.384845: step 7686, loss 0.51901, acc 0.953125, prec 0.108965, recall 0.796827
2017-12-10T05:45:55.650398: step 7687, loss 0.0396384, acc 0.984375, prec 0.108975, recall 0.796845
2017-12-10T05:45:55.930839: step 7688, loss 0.152298, acc 0.96875, prec 0.108983, recall 0.796864
2017-12-10T05:45:56.199802: step 7689, loss 0.213078, acc 0.96875, prec 0.10898, recall 0.796864
2017-12-10T05:45:56.477925: step 7690, loss 0.171983, acc 0.953125, prec 0.108987, recall 0.796882
2017-12-10T05:45:56.749176: step 7691, loss 0.420762, acc 0.96875, prec 0.109018, recall 0.796937
2017-12-10T05:45:57.014040: step 7692, loss 0.0977285, acc 0.96875, prec 0.109015, recall 0.796937
2017-12-10T05:45:57.284097: step 7693, loss 0.207451, acc 0.90625, prec 0.109007, recall 0.796937
2017-12-10T05:45:57.556090: step 7694, loss 0.498361, acc 0.921875, prec 0.109, recall 0.796937
2017-12-10T05:45:57.825900: step 7695, loss 0.195259, acc 0.9375, prec 0.109017, recall 0.796974
2017-12-10T05:45:58.093512: step 7696, loss 0.322237, acc 0.984375, prec 0.109038, recall 0.797011
2017-12-10T05:45:58.356291: step 7697, loss 0.444094, acc 0.953125, prec 0.109044, recall 0.797029
2017-12-10T05:45:58.633782: step 7698, loss 0.20321, acc 0.984375, prec 0.109054, recall 0.797048
2017-12-10T05:45:58.900944: step 7699, loss 0.519664, acc 0.9375, prec 0.109049, recall 0.797048
2017-12-10T05:45:59.176638: step 7700, loss 0.0371803, acc 0.984375, prec 0.10907, recall 0.797084
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-7700

2017-12-10T05:46:00.957703: step 7701, loss 0.104604, acc 0.984375, prec 0.109112, recall 0.797158
2017-12-10T05:46:01.933776: step 7702, loss 0.119927, acc 0.9375, prec 0.109118, recall 0.797176
2017-12-10T05:46:02.299473: step 7703, loss 0.260193, acc 0.953125, prec 0.109114, recall 0.797176
2017-12-10T05:46:02.567945: step 7704, loss 0.120041, acc 0.984375, prec 0.109113, recall 0.797176
2017-12-10T05:46:03.049329: step 7705, loss 0.218089, acc 0.953125, prec 0.10912, recall 0.797195
2017-12-10T05:46:03.786787: step 7706, loss 0.000393975, acc 1, prec 0.109131, recall 0.797213
2017-12-10T05:46:04.511670: step 7707, loss 0.0824347, acc 0.984375, prec 0.109129, recall 0.797213
2017-12-10T05:46:05.226700: step 7708, loss 0.273674, acc 0.96875, prec 0.109138, recall 0.797231
2017-12-10T05:46:05.965947: step 7709, loss 0.515419, acc 1, prec 0.109149, recall 0.79725
2017-12-10T05:46:06.701341: step 7710, loss 0.147968, acc 0.984375, prec 0.109158, recall 0.797268
2017-12-10T05:46:07.430514: step 7711, loss 0.00195232, acc 1, prec 0.109158, recall 0.797268
2017-12-10T05:46:08.120003: step 7712, loss 0.00268524, acc 1, prec 0.109158, recall 0.797268
2017-12-10T05:46:08.844100: step 7713, loss 0.00121119, acc 1, prec 0.109169, recall 0.797286
2017-12-10T05:46:09.560096: step 7714, loss 0.00403347, acc 1, prec 0.10918, recall 0.797305
2017-12-10T05:46:10.280250: step 7715, loss 0.000407011, acc 1, prec 0.109202, recall 0.797341
2017-12-10T05:46:10.985810: step 7716, loss 0.0353967, acc 0.984375, prec 0.109201, recall 0.797341
2017-12-10T05:46:11.738359: step 7717, loss 0.00063686, acc 1, prec 0.109201, recall 0.797341
2017-12-10T05:46:12.468353: step 7718, loss 0.0423819, acc 0.984375, prec 0.109222, recall 0.797378
2017-12-10T05:46:13.187104: step 7719, loss 0.430542, acc 0.96875, prec 0.10923, recall 0.797396
2017-12-10T05:46:13.916771: step 7720, loss 0.069557, acc 0.984375, prec 0.10924, recall 0.797415
2017-12-10T05:46:14.617042: step 7721, loss 0.476743, acc 0.96875, prec 0.109248, recall 0.797433
2017-12-10T05:46:15.353024: step 7722, loss 0.0227358, acc 0.984375, prec 0.109247, recall 0.797433
2017-12-10T05:46:16.077676: step 7723, loss 0.113787, acc 0.96875, prec 0.109244, recall 0.797433
2017-12-10T05:46:16.803430: step 7724, loss 0.278392, acc 0.96875, prec 0.109263, recall 0.797469
2017-12-10T05:46:17.536643: step 7725, loss 0.221358, acc 0.984375, prec 0.109273, recall 0.797488
2017-12-10T05:46:18.264101: step 7726, loss 0.166478, acc 0.96875, prec 0.10927, recall 0.797488
2017-12-10T05:46:19.042210: step 7727, loss 0.000652648, acc 1, prec 0.109281, recall 0.797506
2017-12-10T05:46:19.766798: step 7728, loss 0.325592, acc 0.96875, prec 0.10929, recall 0.797524
2017-12-10T05:46:20.501416: step 7729, loss 0.00385271, acc 1, prec 0.109301, recall 0.797543
2017-12-10T05:46:21.231616: step 7730, loss 0.0454506, acc 0.96875, prec 0.109309, recall 0.797561
2017-12-10T05:46:21.976009: step 7731, loss 0.0903919, acc 0.984375, prec 0.109308, recall 0.797561
2017-12-10T05:46:22.710406: step 7732, loss 0.159436, acc 0.96875, prec 0.109327, recall 0.797598
2017-12-10T05:46:23.439584: step 7733, loss 0.445984, acc 1, prec 0.109338, recall 0.797616
2017-12-10T05:46:24.193766: step 7734, loss 0.249065, acc 0.96875, prec 0.109357, recall 0.797652
2017-12-10T05:46:24.931917: step 7735, loss 0.500734, acc 0.96875, prec 0.109355, recall 0.797652
2017-12-10T05:46:25.667078: step 7736, loss 0.0690287, acc 0.984375, prec 0.109364, recall 0.797671
2017-12-10T05:46:26.716856: step 7737, loss 0.301648, acc 0.984375, prec 0.109363, recall 0.797671
2017-12-10T05:46:27.169494: step 7738, loss 0.0536932, acc 0.96875, prec 0.109371, recall 0.797689
2017-12-10T05:46:27.473087: step 7739, loss 0.118091, acc 0.984375, prec 0.10937, recall 0.797689
2017-12-10T05:46:27.764696: step 7740, loss 0.00232307, acc 1, prec 0.109392, recall 0.797725
2017-12-10T05:46:28.045876: step 7741, loss 0.0141294, acc 1, prec 0.109392, recall 0.797725
2017-12-10T05:46:28.327745: step 7742, loss 0.179935, acc 0.984375, prec 0.109402, recall 0.797744
2017-12-10T05:46:28.617624: step 7743, loss 0.0465161, acc 0.984375, prec 0.1094, recall 0.797744
2017-12-10T05:46:28.885809: step 7744, loss 0.199466, acc 0.953125, prec 0.109407, recall 0.797762
2017-12-10T05:46:29.154553: step 7745, loss 0.25823, acc 0.953125, prec 0.109403, recall 0.797762
2017-12-10T05:46:29.419812: step 7746, loss 0.000149737, acc 1, prec 0.109403, recall 0.797762
2017-12-10T05:46:29.685614: step 7747, loss 0.114282, acc 0.96875, prec 0.109423, recall 0.797798
2017-12-10T05:46:29.951919: step 7748, loss 0.0186552, acc 0.984375, prec 0.109432, recall 0.797817
2017-12-10T05:46:30.218057: step 7749, loss 0.262044, acc 0.953125, prec 0.10945, recall 0.797853
2017-12-10T05:46:30.489065: step 7750, loss 0.0229004, acc 0.984375, prec 0.109449, recall 0.797853
2017-12-10T05:46:30.764192: step 7751, loss 0.151544, acc 0.9375, prec 0.109465, recall 0.79789
2017-12-10T05:46:31.027982: step 7752, loss 0.00138621, acc 1, prec 0.109476, recall 0.797908
2017-12-10T05:46:31.289291: step 7753, loss 0.0339278, acc 0.984375, prec 0.109486, recall 0.797926
2017-12-10T05:46:31.558434: step 7754, loss 0.0266584, acc 1, prec 0.109497, recall 0.797944
2017-12-10T05:46:31.827559: step 7755, loss 0.0024684, acc 1, prec 0.109508, recall 0.797962
2017-12-10T05:46:32.095646: step 7756, loss 0.00148236, acc 1, prec 0.109519, recall 0.797981
2017-12-10T05:46:32.365639: step 7757, loss 0.0100008, acc 1, prec 0.109552, recall 0.798035
2017-12-10T05:46:32.641645: step 7758, loss 0.0867437, acc 0.984375, prec 0.109562, recall 0.798054
2017-12-10T05:46:32.905763: step 7759, loss 0.391996, acc 0.953125, prec 0.109558, recall 0.798054
2017-12-10T05:46:33.171580: step 7760, loss 0.000236136, acc 1, prec 0.109569, recall 0.798072
2017-12-10T05:46:33.438120: step 7761, loss 0.00375297, acc 1, prec 0.10958, recall 0.79809
2017-12-10T05:46:33.707279: step 7762, loss 0.425071, acc 0.96875, prec 0.109577, recall 0.79809
2017-12-10T05:46:33.974044: step 7763, loss 0.0600018, acc 0.984375, prec 0.109576, recall 0.79809
2017-12-10T05:46:34.244133: step 7764, loss 0.204602, acc 0.984375, prec 0.109574, recall 0.79809
2017-12-10T05:46:34.508519: step 7765, loss 0.0150143, acc 1, prec 0.109585, recall 0.798108
2017-12-10T05:46:34.777967: step 7766, loss 0.0455046, acc 0.984375, prec 0.109584, recall 0.798108
2017-12-10T05:46:35.047780: step 7767, loss 0.0563346, acc 0.984375, prec 0.109583, recall 0.798108
2017-12-10T05:46:35.315661: step 7768, loss 0.00103165, acc 1, prec 0.109594, recall 0.798126
2017-12-10T05:46:35.583345: step 7769, loss 0.0818288, acc 0.984375, prec 0.109603, recall 0.798144
2017-12-10T05:46:35.851963: step 7770, loss 0.017393, acc 0.984375, prec 0.109624, recall 0.798181
2017-12-10T05:46:36.114667: step 7771, loss 0.0996206, acc 0.953125, prec 0.10962, recall 0.798181
2017-12-10T05:46:36.381539: step 7772, loss 0.176603, acc 0.984375, prec 0.10963, recall 0.798199
2017-12-10T05:46:36.645051: step 7773, loss 0.057499, acc 0.984375, prec 0.109628, recall 0.798199
2017-12-10T05:46:36.910738: step 7774, loss 0.00118105, acc 1, prec 0.109628, recall 0.798199
2017-12-10T05:46:37.178665: step 7775, loss 0.0417606, acc 0.984375, prec 0.109627, recall 0.798199
2017-12-10T05:46:37.451283: step 7776, loss 0.024838, acc 0.984375, prec 0.109626, recall 0.798199
2017-12-10T05:46:37.715855: step 7777, loss 0.000202486, acc 1, prec 0.109626, recall 0.798199
2017-12-10T05:46:37.981795: step 7778, loss 0.00069712, acc 1, prec 0.109626, recall 0.798199
2017-12-10T05:46:38.243504: step 7779, loss 2.77523e-06, acc 1, prec 0.109637, recall 0.798217
2017-12-10T05:46:38.499656: step 7780, loss 0.152601, acc 0.984375, prec 0.109635, recall 0.798217
2017-12-10T05:46:38.769196: step 7781, loss 3.79521, acc 0.984375, prec 0.109646, recall 0.798163
2017-12-10T05:46:39.035568: step 7782, loss 0.185397, acc 0.984375, prec 0.109656, recall 0.798182
2017-12-10T05:46:39.303064: step 7783, loss 0.10478, acc 0.984375, prec 0.109655, recall 0.798182
2017-12-10T05:46:39.568011: step 7784, loss 0.00372641, acc 1, prec 0.109655, recall 0.798182
2017-12-10T05:46:39.835644: step 7785, loss 0.0169525, acc 1, prec 0.109666, recall 0.7982
2017-12-10T05:46:40.102765: step 7786, loss 0.261693, acc 0.953125, prec 0.109673, recall 0.798218
2017-12-10T05:46:40.370509: step 7787, loss 0.27167, acc 0.953125, prec 0.109679, recall 0.798236
2017-12-10T05:46:40.634106: step 7788, loss 0.150699, acc 0.984375, prec 0.109678, recall 0.798236
2017-12-10T05:46:40.902554: step 7789, loss 0.238298, acc 0.953125, prec 0.109707, recall 0.798291
2017-12-10T05:46:41.166834: step 7790, loss 0.53381, acc 0.953125, prec 0.109714, recall 0.798309
2017-12-10T05:46:41.442935: step 7791, loss 0.288392, acc 0.96875, prec 0.109722, recall 0.798327
2017-12-10T05:46:41.709762: step 7792, loss 0.0521777, acc 0.984375, prec 0.109732, recall 0.798345
2017-12-10T05:46:41.976085: step 7793, loss 0.0326856, acc 0.96875, prec 0.109751, recall 0.798381
2017-12-10T05:46:42.249187: step 7794, loss 0.0115471, acc 1, prec 0.109751, recall 0.798381
2017-12-10T05:46:42.525791: step 7795, loss 0.0294467, acc 0.984375, prec 0.10975, recall 0.798381
2017-12-10T05:46:42.791421: step 7796, loss 0.0599828, acc 0.984375, prec 0.109749, recall 0.798381
2017-12-10T05:46:43.061651: step 7797, loss 0.370316, acc 0.9375, prec 0.109754, recall 0.798399
2017-12-10T05:46:43.325892: step 7798, loss 0.829682, acc 0.9375, prec 0.10976, recall 0.798418
2017-12-10T05:46:43.594414: step 7799, loss 0.030848, acc 0.984375, prec 0.10978, recall 0.798454
2017-12-10T05:46:43.870093: step 7800, loss 0.221838, acc 0.984375, prec 0.109779, recall 0.798454

Evaluation:
2017-12-10T05:46:51.473919: step 7800, loss 8.4479, acc 0.94716, prec 0.109846, recall 0.793155

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-7800

2017-12-10T05:46:52.748194: step 7801, loss 0.0463994, acc 0.96875, prec 0.109866, recall 0.793192
2017-12-10T05:46:53.013967: step 7802, loss 0.5472, acc 0.921875, prec 0.109881, recall 0.793229
2017-12-10T05:46:53.282701: step 7803, loss 0.0427812, acc 0.984375, prec 0.109901, recall 0.793265
2017-12-10T05:46:53.549208: step 7804, loss 0.0114402, acc 1, prec 0.109901, recall 0.793265
2017-12-10T05:46:53.813372: step 7805, loss 0.14039, acc 0.96875, prec 0.109898, recall 0.793265
2017-12-10T05:46:54.080687: step 7806, loss 0.234955, acc 0.96875, prec 0.109907, recall 0.793284
2017-12-10T05:46:54.351003: step 7807, loss 0.118975, acc 0.953125, prec 0.109914, recall 0.793302
2017-12-10T05:46:54.621728: step 7808, loss 0.183919, acc 0.96875, prec 0.109944, recall 0.793357
2017-12-10T05:46:54.893577: step 7809, loss 0.160096, acc 0.9375, prec 0.109949, recall 0.793375
2017-12-10T05:46:55.166444: step 7810, loss 0.250476, acc 0.984375, prec 0.109959, recall 0.793394
2017-12-10T05:46:55.432464: step 7811, loss 0.158285, acc 0.984375, prec 0.109968, recall 0.793412
2017-12-10T05:46:55.709997: step 7812, loss 0.138236, acc 0.9375, prec 0.109963, recall 0.793412
2017-12-10T05:46:55.983296: step 7813, loss 0.00782504, acc 1, prec 0.109996, recall 0.793467
2017-12-10T05:46:56.248949: step 7814, loss 0.234057, acc 0.953125, prec 0.110013, recall 0.793503
2017-12-10T05:46:56.514421: step 7815, loss 0.285236, acc 0.96875, prec 0.110011, recall 0.793503
2017-12-10T05:46:56.778044: step 7816, loss 0.0934957, acc 0.953125, prec 0.110029, recall 0.79354
2017-12-10T05:46:57.043748: step 7817, loss 0.0524572, acc 0.96875, prec 0.110026, recall 0.79354
2017-12-10T05:46:57.316728: step 7818, loss 0.164575, acc 0.96875, prec 0.110034, recall 0.793558
2017-12-10T05:46:57.588009: step 7819, loss 7.89967, acc 0.96875, prec 0.110056, recall 0.793454
2017-12-10T05:46:57.868239: step 7820, loss 0.314518, acc 0.953125, prec 0.110052, recall 0.793454
2017-12-10T05:46:58.130569: step 7821, loss 0.0369464, acc 0.96875, prec 0.110049, recall 0.793454
2017-12-10T05:46:58.407908: step 7822, loss 0.559261, acc 0.953125, prec 0.110078, recall 0.793509
2017-12-10T05:46:58.682431: step 7823, loss 0.417151, acc 0.953125, prec 0.110074, recall 0.793509
2017-12-10T05:46:58.949768: step 7824, loss 0.231182, acc 0.96875, prec 0.110071, recall 0.793509
2017-12-10T05:46:59.224183: step 7825, loss 0.852404, acc 0.890625, prec 0.110073, recall 0.793527
2017-12-10T05:46:59.490837: step 7826, loss 0.670085, acc 0.859375, prec 0.110093, recall 0.793582
2017-12-10T05:46:59.758442: step 7827, loss 1.15604, acc 0.796875, prec 0.110076, recall 0.793582
2017-12-10T05:47:00.029283: step 7828, loss 0.962753, acc 0.890625, prec 0.110066, recall 0.793582
2017-12-10T05:47:00.301190: step 7829, loss 0.906781, acc 0.859375, prec 0.110054, recall 0.793582
2017-12-10T05:47:00.580695: step 7830, loss 0.825506, acc 0.78125, prec 0.110046, recall 0.7936
2017-12-10T05:47:00.851724: step 7831, loss 0.832223, acc 0.890625, prec 0.110048, recall 0.793619
2017-12-10T05:47:01.121288: step 7832, loss 1.49967, acc 0.84375, prec 0.110034, recall 0.793619
2017-12-10T05:47:01.383300: step 7833, loss 0.982146, acc 0.84375, prec 0.110021, recall 0.793619
2017-12-10T05:47:01.652203: step 7834, loss 1.10482, acc 0.859375, prec 0.110041, recall 0.793673
2017-12-10T05:47:01.915230: step 7835, loss 0.741786, acc 0.8125, prec 0.110025, recall 0.793673
2017-12-10T05:47:02.184760: step 7836, loss 0.495727, acc 0.890625, prec 0.110016, recall 0.793673
2017-12-10T05:47:02.452472: step 7837, loss 1.00103, acc 0.890625, prec 0.110017, recall 0.793691
2017-12-10T05:47:02.717876: step 7838, loss 0.529533, acc 0.921875, prec 0.110032, recall 0.793728
2017-12-10T05:47:02.987382: step 7839, loss 0.753743, acc 0.875, prec 0.110032, recall 0.793746
2017-12-10T05:47:03.256612: step 7840, loss 0.586282, acc 0.90625, prec 0.110035, recall 0.793764
2017-12-10T05:47:03.536519: step 7841, loss 0.206449, acc 0.90625, prec 0.110038, recall 0.793783
2017-12-10T05:47:03.808027: step 7842, loss 0.535664, acc 0.9375, prec 0.110054, recall 0.793819
2017-12-10T05:47:04.076963: step 7843, loss 0.418082, acc 0.953125, prec 0.11005, recall 0.793819
2017-12-10T05:47:04.342324: step 7844, loss 0.329037, acc 0.953125, prec 0.110057, recall 0.793837
2017-12-10T05:47:04.612767: step 7845, loss 0.0168512, acc 0.984375, prec 0.110056, recall 0.793837
2017-12-10T05:47:04.882685: step 7846, loss 0.105749, acc 0.96875, prec 0.110075, recall 0.793874
2017-12-10T05:47:05.149869: step 7847, loss 0.052159, acc 0.984375, prec 0.110073, recall 0.793874
2017-12-10T05:47:05.418043: step 7848, loss 0.0664367, acc 0.984375, prec 0.110083, recall 0.793892
2017-12-10T05:47:05.678702: step 7849, loss 0.551399, acc 0.953125, prec 0.11009, recall 0.79391
2017-12-10T05:47:05.949751: step 7850, loss 0.00724186, acc 1, prec 0.110123, recall 0.793965
2017-12-10T05:47:06.218363: step 7851, loss 0.00360786, acc 1, prec 0.110166, recall 0.794037
2017-12-10T05:47:06.485402: step 7852, loss 0.618236, acc 0.96875, prec 0.110163, recall 0.794037
2017-12-10T05:47:06.749700: step 7853, loss 0.111024, acc 0.984375, prec 0.110173, recall 0.794055
2017-12-10T05:47:07.013938: step 7854, loss 0.0286476, acc 1, prec 0.110206, recall 0.79411
2017-12-10T05:47:07.280834: step 7855, loss 0.000190312, acc 1, prec 0.110227, recall 0.794146
2017-12-10T05:47:07.549431: step 7856, loss 6.97852, acc 0.984375, prec 0.110249, recall 0.794112
2017-12-10T05:47:07.819261: step 7857, loss 0.00284201, acc 1, prec 0.110249, recall 0.794112
2017-12-10T05:47:08.088800: step 7858, loss 0.000657764, acc 1, prec 0.110249, recall 0.794112
2017-12-10T05:47:08.351930: step 7859, loss 0.00135335, acc 1, prec 0.11026, recall 0.794131
2017-12-10T05:47:08.617648: step 7860, loss 0.000500829, acc 1, prec 0.110271, recall 0.794149
2017-12-10T05:47:08.882595: step 7861, loss 0.0796041, acc 0.984375, prec 0.11027, recall 0.794149
2017-12-10T05:47:09.146290: step 7862, loss 0.0177517, acc 0.984375, prec 0.110279, recall 0.794167
2017-12-10T05:47:09.416777: step 7863, loss 0.0582163, acc 0.96875, prec 0.110287, recall 0.794185
2017-12-10T05:47:09.682718: step 7864, loss 0.568819, acc 0.9375, prec 0.110304, recall 0.794221
2017-12-10T05:47:09.950092: step 7865, loss 0.474372, acc 0.96875, prec 0.110312, recall 0.794239
2017-12-10T05:47:10.215618: step 7866, loss 0.130697, acc 0.96875, prec 0.110309, recall 0.794239
2017-12-10T05:47:10.490461: step 7867, loss 0.102738, acc 0.96875, prec 0.110306, recall 0.794239
2017-12-10T05:47:10.759921: step 7868, loss 0.294973, acc 0.953125, prec 0.110313, recall 0.794258
2017-12-10T05:47:11.025430: step 7869, loss 0.926325, acc 0.921875, prec 0.110317, recall 0.794276
2017-12-10T05:47:11.289688: step 7870, loss 0.111853, acc 0.96875, prec 0.110315, recall 0.794276
2017-12-10T05:47:11.555855: step 7871, loss 0.0775745, acc 0.96875, prec 0.110312, recall 0.794276
2017-12-10T05:47:11.823566: step 7872, loss 0.0695975, acc 0.984375, prec 0.110311, recall 0.794276
2017-12-10T05:47:12.096309: step 7873, loss 0.134288, acc 0.96875, prec 0.11033, recall 0.794312
2017-12-10T05:47:12.374104: step 7874, loss 0.0924517, acc 0.96875, prec 0.110338, recall 0.79433
2017-12-10T05:47:12.641397: step 7875, loss 0.132691, acc 0.984375, prec 0.110347, recall 0.794348
2017-12-10T05:47:12.907736: step 7876, loss 0.562659, acc 0.96875, prec 0.110356, recall 0.794366
2017-12-10T05:47:13.173768: step 7877, loss 0.856876, acc 0.9375, prec 0.11035, recall 0.794366
2017-12-10T05:47:13.442693: step 7878, loss 0.128176, acc 0.96875, prec 0.110358, recall 0.794384
2017-12-10T05:47:13.706699: step 7879, loss 0.0876641, acc 0.96875, prec 0.110367, recall 0.794402
2017-12-10T05:47:13.968943: step 7880, loss 0.205314, acc 0.96875, prec 0.110375, recall 0.79442
2017-12-10T05:47:14.236179: step 7881, loss 0.178734, acc 0.953125, prec 0.110382, recall 0.794439
2017-12-10T05:47:14.511840: step 7882, loss 0.0260834, acc 0.984375, prec 0.110391, recall 0.794457
2017-12-10T05:47:14.777621: step 7883, loss 0.00817273, acc 1, prec 0.110402, recall 0.794475
2017-12-10T05:47:15.039100: step 7884, loss 0.158884, acc 0.984375, prec 0.110455, recall 0.794565
2017-12-10T05:47:15.304203: step 7885, loss 0.331992, acc 0.984375, prec 0.110486, recall 0.794619
2017-12-10T05:47:15.570062: step 7886, loss 0.0110114, acc 1, prec 0.110497, recall 0.794637
2017-12-10T05:47:15.833221: step 7887, loss 0.0459521, acc 0.96875, prec 0.110505, recall 0.794655
2017-12-10T05:47:16.100001: step 7888, loss 0.00788239, acc 1, prec 0.110538, recall 0.79471
2017-12-10T05:47:16.367899: step 7889, loss 0.0146072, acc 0.984375, prec 0.110537, recall 0.79471
2017-12-10T05:47:16.631459: step 7890, loss 0.0677667, acc 0.984375, prec 0.110535, recall 0.79471
2017-12-10T05:47:16.895885: step 7891, loss 0.109947, acc 0.984375, prec 0.110534, recall 0.79471
2017-12-10T05:47:17.161276: step 7892, loss 0.139036, acc 0.96875, prec 0.110542, recall 0.794728
2017-12-10T05:47:17.433086: step 7893, loss 0.0212656, acc 0.984375, prec 0.110541, recall 0.794728
2017-12-10T05:47:17.701104: step 7894, loss 0.00061417, acc 1, prec 0.110552, recall 0.794746
2017-12-10T05:47:17.968809: step 7895, loss 0.433144, acc 0.96875, prec 0.110582, recall 0.7948
2017-12-10T05:47:18.242103: step 7896, loss 0.336122, acc 0.984375, prec 0.110613, recall 0.794854
2017-12-10T05:47:18.511661: step 7897, loss 0.0309227, acc 0.984375, prec 0.110611, recall 0.794854
2017-12-10T05:47:18.784662: step 7898, loss 0.108896, acc 0.96875, prec 0.11062, recall 0.794872
2017-12-10T05:47:19.052620: step 7899, loss 0.0159403, acc 1, prec 0.110641, recall 0.794908
2017-12-10T05:47:19.328625: step 7900, loss 0.0173642, acc 0.984375, prec 0.110662, recall 0.794944
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-7900

2017-12-10T05:47:20.498606: step 7901, loss 0.0056815, acc 1, prec 0.110662, recall 0.794944
2017-12-10T05:47:20.761844: step 7902, loss 0.020548, acc 0.984375, prec 0.110671, recall 0.794962
2017-12-10T05:47:21.027537: step 7903, loss 0.231952, acc 0.96875, prec 0.110669, recall 0.794962
2017-12-10T05:47:21.300958: step 7904, loss 0.00703669, acc 1, prec 0.110669, recall 0.794962
2017-12-10T05:47:21.570698: step 7905, loss 6.66126e-05, acc 1, prec 0.110679, recall 0.79498
2017-12-10T05:47:21.837345: step 7906, loss 0.000378, acc 1, prec 0.110679, recall 0.79498
2017-12-10T05:47:22.111750: step 7907, loss 0.000445395, acc 1, prec 0.110679, recall 0.79498
2017-12-10T05:47:22.381739: step 7908, loss 0.00596315, acc 1, prec 0.11069, recall 0.794998
2017-12-10T05:47:22.650957: step 7909, loss 0.234282, acc 0.984375, prec 0.1107, recall 0.795016
2017-12-10T05:47:22.919171: step 7910, loss 0.047356, acc 0.96875, prec 0.110697, recall 0.795016
2017-12-10T05:47:23.184031: step 7911, loss 0.000882823, acc 1, prec 0.11073, recall 0.79507
2017-12-10T05:47:23.458180: step 7912, loss 0.0145472, acc 0.984375, prec 0.11075, recall 0.795106
2017-12-10T05:47:23.723506: step 7913, loss 0.35434, acc 0.953125, prec 0.110768, recall 0.795142
2017-12-10T05:47:23.993177: step 7914, loss 0.716684, acc 0.96875, prec 0.110787, recall 0.795178
2017-12-10T05:47:24.257745: step 7915, loss 0.0272019, acc 0.984375, prec 0.110807, recall 0.795213
2017-12-10T05:47:24.527099: step 7916, loss 0.00186059, acc 1, prec 0.110807, recall 0.795213
2017-12-10T05:47:24.792714: step 7917, loss 9.31775e-05, acc 1, prec 0.110807, recall 0.795213
2017-12-10T05:47:25.063545: step 7918, loss 6.63831, acc 0.96875, prec 0.110817, recall 0.795162
2017-12-10T05:47:25.338621: step 7919, loss 0.357747, acc 0.953125, prec 0.110813, recall 0.795162
2017-12-10T05:47:25.604755: step 7920, loss 0.201727, acc 0.953125, prec 0.110808, recall 0.795162
2017-12-10T05:47:25.872114: step 7921, loss 0.206542, acc 0.953125, prec 0.110826, recall 0.795198
2017-12-10T05:47:26.137823: step 7922, loss 0.0198627, acc 0.984375, prec 0.110836, recall 0.795216
2017-12-10T05:47:26.403475: step 7923, loss 0.289279, acc 0.96875, prec 0.110844, recall 0.795233
2017-12-10T05:47:26.668823: step 7924, loss 0.42649, acc 0.9375, prec 0.110838, recall 0.795233
2017-12-10T05:47:26.938840: step 7925, loss 0.612634, acc 0.875, prec 0.110838, recall 0.795251
2017-12-10T05:47:27.211951: step 7926, loss 0.312778, acc 0.96875, prec 0.110847, recall 0.795269
2017-12-10T05:47:27.494958: step 7927, loss 0.505379, acc 0.953125, prec 0.110842, recall 0.795269
2017-12-10T05:47:27.764368: step 7928, loss 0.037689, acc 0.984375, prec 0.110852, recall 0.795287
2017-12-10T05:47:28.039654: step 7929, loss 0.400217, acc 0.9375, prec 0.110847, recall 0.795287
2017-12-10T05:47:28.304668: step 7930, loss 0.758462, acc 0.90625, prec 0.110849, recall 0.795305
2017-12-10T05:47:28.574597: step 7931, loss 0.153827, acc 0.953125, prec 0.110856, recall 0.795323
2017-12-10T05:47:28.847068: step 7932, loss 0.739177, acc 0.90625, prec 0.11087, recall 0.795359
2017-12-10T05:47:29.120627: step 7933, loss 0.243636, acc 0.921875, prec 0.110885, recall 0.795395
2017-12-10T05:47:29.385527: step 7934, loss 0.203788, acc 0.90625, prec 0.110877, recall 0.795395
2017-12-10T05:47:29.656744: step 7935, loss 0.125849, acc 0.96875, prec 0.110885, recall 0.795413
2017-12-10T05:47:29.923262: step 7936, loss 0.0311899, acc 0.984375, prec 0.110883, recall 0.795413
2017-12-10T05:47:30.190002: step 7937, loss 0.382494, acc 0.9375, prec 0.110878, recall 0.795413
2017-12-10T05:47:30.465645: step 7938, loss 0.17365, acc 0.953125, prec 0.110906, recall 0.795466
2017-12-10T05:47:30.744101: step 7939, loss 0.133584, acc 0.984375, prec 0.110938, recall 0.79552
2017-12-10T05:47:31.012312: step 7940, loss 0.386987, acc 0.9375, prec 0.110943, recall 0.795538
2017-12-10T05:47:31.277856: step 7941, loss 0.00622918, acc 1, prec 0.110954, recall 0.795556
2017-12-10T05:47:31.549651: step 7942, loss 0.333238, acc 0.953125, prec 0.11095, recall 0.795556
2017-12-10T05:47:31.823214: step 7943, loss 0.301282, acc 0.984375, prec 0.11097, recall 0.795592
2017-12-10T05:47:32.088704: step 7944, loss 0.0946913, acc 0.984375, prec 0.11098, recall 0.79561
2017-12-10T05:47:32.357521: step 7945, loss 0.337097, acc 0.9375, prec 0.110985, recall 0.795627
2017-12-10T05:47:32.625481: step 7946, loss 0.0274966, acc 0.984375, prec 0.110984, recall 0.795627
2017-12-10T05:47:32.899858: step 7947, loss 0.0113043, acc 1, prec 0.110995, recall 0.795645
2017-12-10T05:47:33.170830: step 7948, loss 0.108281, acc 0.96875, prec 0.111003, recall 0.795663
2017-12-10T05:47:33.437681: step 7949, loss 0.359733, acc 0.953125, prec 0.110999, recall 0.795663
2017-12-10T05:47:33.705120: step 7950, loss 0.117969, acc 0.984375, prec 0.110997, recall 0.795663
2017-12-10T05:47:33.974949: step 7951, loss 0.0344172, acc 0.984375, prec 0.111007, recall 0.795681
2017-12-10T05:47:34.211890: step 7952, loss 0.0350556, acc 0.961538, prec 0.111004, recall 0.795681
2017-12-10T05:47:34.487050: step 7953, loss 0.287325, acc 0.984375, prec 0.111014, recall 0.795699
2017-12-10T05:47:34.760531: step 7954, loss 4.11294e-05, acc 1, prec 0.111024, recall 0.795717
2017-12-10T05:47:35.027092: step 7955, loss 0.208241, acc 0.96875, prec 0.111022, recall 0.795717
2017-12-10T05:47:35.291820: step 7956, loss 0.721984, acc 0.9375, prec 0.111016, recall 0.795717
2017-12-10T05:47:35.559033: step 7957, loss 0.0473505, acc 0.984375, prec 0.111015, recall 0.795717
2017-12-10T05:47:35.826998: step 7958, loss 0.439762, acc 0.984375, prec 0.111014, recall 0.795717
2017-12-10T05:47:36.105320: step 7959, loss 0.0481945, acc 0.984375, prec 0.111023, recall 0.795735
2017-12-10T05:47:36.379764: step 7960, loss 0.140577, acc 1, prec 0.111045, recall 0.79577
2017-12-10T05:47:36.652223: step 7961, loss 0.13917, acc 0.953125, prec 0.111041, recall 0.79577
2017-12-10T05:47:36.917898: step 7962, loss 0.000103078, acc 1, prec 0.111041, recall 0.79577
2017-12-10T05:47:37.177710: step 7963, loss 0.130671, acc 0.984375, prec 0.111039, recall 0.79577
2017-12-10T05:47:37.452641: step 7964, loss 0.00475981, acc 1, prec 0.11105, recall 0.795788
2017-12-10T05:47:37.717359: step 7965, loss 0.288544, acc 0.96875, prec 0.111047, recall 0.795788
2017-12-10T05:47:37.977903: step 7966, loss 0.0279667, acc 0.984375, prec 0.111057, recall 0.795806
2017-12-10T05:47:38.251364: step 7967, loss 0.73757, acc 1, prec 0.111079, recall 0.795842
2017-12-10T05:47:38.514831: step 7968, loss 0.0409907, acc 0.96875, prec 0.111076, recall 0.795842
2017-12-10T05:47:38.790621: step 7969, loss 0.0330153, acc 0.96875, prec 0.111106, recall 0.795895
2017-12-10T05:47:39.065188: step 7970, loss 0.751931, acc 1, prec 0.111117, recall 0.795913
2017-12-10T05:47:39.340568: step 7971, loss 0.00894635, acc 1, prec 0.111117, recall 0.795913
2017-12-10T05:47:39.606776: step 7972, loss 0.0382068, acc 0.984375, prec 0.111126, recall 0.795931
2017-12-10T05:47:39.876954: step 7973, loss 0.0651815, acc 0.984375, prec 0.111125, recall 0.795931
2017-12-10T05:47:40.149715: step 7974, loss 0.350812, acc 0.984375, prec 0.111145, recall 0.795966
2017-12-10T05:47:40.424796: step 7975, loss 0.391206, acc 0.953125, prec 0.111141, recall 0.795966
2017-12-10T05:47:40.689813: step 7976, loss 0.00118225, acc 1, prec 0.111152, recall 0.795984
2017-12-10T05:47:40.958220: step 7977, loss 0.0231915, acc 0.984375, prec 0.111183, recall 0.796038
2017-12-10T05:47:41.225805: step 7978, loss 0.176089, acc 0.96875, prec 0.111191, recall 0.796055
2017-12-10T05:47:41.498384: step 7979, loss 0.16157, acc 0.984375, prec 0.11119, recall 0.796055
2017-12-10T05:47:41.763789: step 7980, loss 7.42125, acc 0.953125, prec 0.111187, recall 0.795986
2017-12-10T05:47:42.036624: step 7981, loss 0.126066, acc 0.96875, prec 0.111195, recall 0.796004
2017-12-10T05:47:42.308449: step 7982, loss 0.610942, acc 0.953125, prec 0.111224, recall 0.796057
2017-12-10T05:47:42.585513: step 7983, loss 0.214477, acc 0.9375, prec 0.11124, recall 0.796093
2017-12-10T05:47:42.856325: step 7984, loss 0.0851397, acc 0.96875, prec 0.111237, recall 0.796093
2017-12-10T05:47:43.130869: step 7985, loss 0.115373, acc 0.984375, prec 0.111268, recall 0.796146
2017-12-10T05:47:43.400501: step 7986, loss 1.04186, acc 0.875, prec 0.111268, recall 0.796164
2017-12-10T05:47:43.660016: step 7987, loss 0.393381, acc 0.90625, prec 0.11126, recall 0.796164
2017-12-10T05:47:43.932017: step 7988, loss 0.201543, acc 0.921875, prec 0.111253, recall 0.796164
2017-12-10T05:47:44.201018: step 7989, loss 0.518797, acc 0.953125, prec 0.11126, recall 0.796182
2017-12-10T05:47:44.475862: step 7990, loss 0.27757, acc 0.9375, prec 0.111255, recall 0.796182
2017-12-10T05:47:44.745406: step 7991, loss 1.33876, acc 0.859375, prec 0.111242, recall 0.796182
2017-12-10T05:47:45.013700: step 7992, loss 0.351887, acc 0.890625, prec 0.111255, recall 0.796217
2017-12-10T05:47:45.281507: step 7993, loss 0.0574695, acc 0.96875, prec 0.111273, recall 0.796253
2017-12-10T05:47:45.556293: step 7994, loss 0.136595, acc 0.96875, prec 0.111271, recall 0.796253
2017-12-10T05:47:45.820949: step 7995, loss 0.470371, acc 0.9375, prec 0.111276, recall 0.79627
2017-12-10T05:47:46.100573: step 7996, loss 0.385705, acc 0.921875, prec 0.11128, recall 0.796288
2017-12-10T05:47:46.368364: step 7997, loss 0.727712, acc 0.890625, prec 0.111282, recall 0.796306
2017-12-10T05:47:46.635474: step 7998, loss 0.574158, acc 0.90625, prec 0.111306, recall 0.796359
2017-12-10T05:47:46.900466: step 7999, loss 0.381891, acc 0.9375, prec 0.111322, recall 0.796395
2017-12-10T05:47:47.172651: step 8000, loss 0.165927, acc 0.9375, prec 0.111328, recall 0.796412
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-8000

2017-12-10T05:47:48.387681: step 8001, loss 0.0646731, acc 0.984375, prec 0.111326, recall 0.796412
2017-12-10T05:47:48.665949: step 8002, loss 0.301508, acc 0.9375, prec 0.111332, recall 0.79643
2017-12-10T05:47:48.931502: step 8003, loss 0.0480871, acc 0.953125, prec 0.11136, recall 0.796483
2017-12-10T05:47:49.196662: step 8004, loss 0.051313, acc 0.96875, prec 0.11139, recall 0.796536
2017-12-10T05:47:49.462734: step 8005, loss 0.140841, acc 0.96875, prec 0.111387, recall 0.796536
2017-12-10T05:47:49.736010: step 8006, loss 0.217798, acc 0.96875, prec 0.111406, recall 0.796572
2017-12-10T05:47:50.008135: step 8007, loss 0.253254, acc 0.953125, prec 0.111413, recall 0.79659
2017-12-10T05:47:50.284302: step 8008, loss 0.130394, acc 0.984375, prec 0.111411, recall 0.79659
2017-12-10T05:47:50.554454: step 8009, loss 0.326671, acc 0.953125, prec 0.111418, recall 0.796607
2017-12-10T05:47:50.816775: step 8010, loss 0.223588, acc 0.984375, prec 0.111427, recall 0.796625
2017-12-10T05:47:51.089589: step 8011, loss 0.338707, acc 0.9375, prec 0.111422, recall 0.796625
2017-12-10T05:47:51.355804: step 8012, loss 0.199497, acc 0.984375, prec 0.111431, recall 0.796643
2017-12-10T05:47:51.622272: step 8013, loss 0.230338, acc 0.953125, prec 0.111438, recall 0.79666
2017-12-10T05:47:51.896397: step 8014, loss 4.61316e-06, acc 1, prec 0.111438, recall 0.79666
2017-12-10T05:47:52.156335: step 8015, loss 0.000572555, acc 1, prec 0.11146, recall 0.796696
2017-12-10T05:47:52.421354: step 8016, loss 0.0735461, acc 0.96875, prec 0.111457, recall 0.796696
2017-12-10T05:47:52.690196: step 8017, loss 4.13911e-05, acc 1, prec 0.111468, recall 0.796713
2017-12-10T05:47:52.958342: step 8018, loss 0.00856881, acc 1, prec 0.11149, recall 0.796749
2017-12-10T05:47:53.221738: step 8019, loss 0.580907, acc 0.96875, prec 0.111508, recall 0.796784
2017-12-10T05:47:53.493939: step 8020, loss 0.00915348, acc 1, prec 0.11153, recall 0.796819
2017-12-10T05:47:53.765603: step 8021, loss 0.221834, acc 0.984375, prec 0.11154, recall 0.796837
2017-12-10T05:47:54.030631: step 8022, loss 0.0587146, acc 0.984375, prec 0.111538, recall 0.796837
2017-12-10T05:47:54.295906: step 8023, loss 0.120521, acc 0.96875, prec 0.111535, recall 0.796837
2017-12-10T05:47:54.567156: step 8024, loss 0.00163137, acc 1, prec 0.111535, recall 0.796837
2017-12-10T05:47:54.825647: step 8025, loss 0.00304345, acc 1, prec 0.111546, recall 0.796855
2017-12-10T05:47:55.093583: step 8026, loss 0.0505735, acc 0.96875, prec 0.111544, recall 0.796855
2017-12-10T05:47:55.363071: step 8027, loss 0.0467412, acc 0.96875, prec 0.111562, recall 0.79689
2017-12-10T05:47:55.632881: step 8028, loss 0.0618234, acc 1, prec 0.111573, recall 0.796908
2017-12-10T05:47:55.898614: step 8029, loss 0.314927, acc 0.953125, prec 0.11158, recall 0.796925
2017-12-10T05:47:56.168536: step 8030, loss 0.25681, acc 0.984375, prec 0.1116, recall 0.79696
2017-12-10T05:47:56.440633: step 8031, loss 0.144956, acc 0.984375, prec 0.111642, recall 0.797031
2017-12-10T05:47:56.708417: step 8032, loss 4.57365, acc 0.96875, prec 0.111641, recall 0.796962
2017-12-10T05:47:56.979956: step 8033, loss 0.0542585, acc 0.984375, prec 0.111639, recall 0.796962
2017-12-10T05:47:57.258235: step 8034, loss 5.56351e-05, acc 1, prec 0.11165, recall 0.796979
2017-12-10T05:47:57.531133: step 8035, loss 0.160429, acc 0.96875, prec 0.111647, recall 0.796979
2017-12-10T05:47:57.798489: step 8036, loss 0.000289072, acc 1, prec 0.111658, recall 0.796997
2017-12-10T05:47:58.067014: step 8037, loss 0.133404, acc 0.984375, prec 0.111668, recall 0.797015
2017-12-10T05:47:58.337171: step 8038, loss 0.27664, acc 0.953125, prec 0.111664, recall 0.797015
2017-12-10T05:47:58.601018: step 8039, loss 0.410175, acc 0.96875, prec 0.111683, recall 0.79705
2017-12-10T05:47:58.870956: step 8040, loss 0.291589, acc 0.953125, prec 0.111689, recall 0.797068
2017-12-10T05:47:59.136400: step 8041, loss 0.568414, acc 0.90625, prec 0.111703, recall 0.797103
2017-12-10T05:47:59.406291: step 8042, loss 0.268917, acc 0.921875, prec 0.111707, recall 0.79712
2017-12-10T05:47:59.671202: step 8043, loss 0.195581, acc 0.984375, prec 0.111705, recall 0.79712
2017-12-10T05:47:59.945983: step 8044, loss 0.588968, acc 0.96875, prec 0.111713, recall 0.797138
2017-12-10T05:48:00.212525: step 8045, loss 0.361886, acc 0.96875, prec 0.111711, recall 0.797138
2017-12-10T05:48:00.489697: step 8046, loss 0.58042, acc 0.953125, prec 0.111717, recall 0.797155
2017-12-10T05:48:00.774058: step 8047, loss 1.02424, acc 0.828125, prec 0.111703, recall 0.797155
2017-12-10T05:48:01.048047: step 8048, loss 0.736618, acc 0.90625, prec 0.111694, recall 0.797155
2017-12-10T05:48:01.317267: step 8049, loss 0.548145, acc 0.9375, prec 0.111721, recall 0.797208
2017-12-10T05:48:01.587487: step 8050, loss 0.458861, acc 0.953125, prec 0.111739, recall 0.797243
2017-12-10T05:48:01.856949: step 8051, loss 1.42321, acc 0.875, prec 0.111728, recall 0.797243
2017-12-10T05:48:02.129802: step 8052, loss 0.342258, acc 0.96875, prec 0.111725, recall 0.797243
2017-12-10T05:48:02.402170: step 8053, loss 0.610027, acc 0.90625, prec 0.111717, recall 0.797243
2017-12-10T05:48:02.665605: step 8054, loss 0.124422, acc 0.96875, prec 0.111725, recall 0.797261
2017-12-10T05:48:02.931479: step 8055, loss 0.0180864, acc 0.984375, prec 0.111724, recall 0.797261
2017-12-10T05:48:03.196474: step 8056, loss 0.242895, acc 0.953125, prec 0.11172, recall 0.797261
2017-12-10T05:48:03.462300: step 8057, loss 0.270386, acc 0.90625, prec 0.111722, recall 0.797279
2017-12-10T05:48:03.733171: step 8058, loss 0.00321689, acc 1, prec 0.111733, recall 0.797296
2017-12-10T05:48:03.999569: step 8059, loss 0.0498805, acc 0.96875, prec 0.11173, recall 0.797296
2017-12-10T05:48:04.263442: step 8060, loss 0.0377221, acc 0.984375, prec 0.111729, recall 0.797296
2017-12-10T05:48:04.532704: step 8061, loss 0.13625, acc 0.9375, prec 0.111724, recall 0.797296
2017-12-10T05:48:04.796536: step 8062, loss 0.496184, acc 0.96875, prec 0.111753, recall 0.797349
2017-12-10T05:48:05.061147: step 8063, loss 0.0793648, acc 0.984375, prec 0.111763, recall 0.797366
2017-12-10T05:48:05.328353: step 8064, loss 0.0184757, acc 0.984375, prec 0.111761, recall 0.797366
2017-12-10T05:48:05.596994: step 8065, loss 0.000150293, acc 1, prec 0.111772, recall 0.797384
2017-12-10T05:48:05.855684: step 8066, loss 0.057848, acc 0.96875, prec 0.111791, recall 0.797419
2017-12-10T05:48:06.124609: step 8067, loss 0.406044, acc 0.953125, prec 0.111787, recall 0.797419
2017-12-10T05:48:06.388095: step 8068, loss 0.339732, acc 0.96875, prec 0.111806, recall 0.797454
2017-12-10T05:48:06.662103: step 8069, loss 0.0423947, acc 0.984375, prec 0.111815, recall 0.797472
2017-12-10T05:48:06.923119: step 8070, loss 0.245919, acc 0.953125, prec 0.111811, recall 0.797472
2017-12-10T05:48:07.187978: step 8071, loss 0.0116695, acc 0.984375, prec 0.11181, recall 0.797472
2017-12-10T05:48:07.458494: step 8072, loss 0.0454192, acc 0.984375, prec 0.111841, recall 0.797524
2017-12-10T05:48:07.725082: step 8073, loss 0.316561, acc 0.96875, prec 0.11186, recall 0.797559
2017-12-10T05:48:07.994054: step 8074, loss 0.384743, acc 0.96875, prec 0.111879, recall 0.797594
2017-12-10T05:48:08.266564: step 8075, loss 4.95997e-05, acc 1, prec 0.111879, recall 0.797594
2017-12-10T05:48:08.529903: step 8076, loss 0.0444052, acc 0.96875, prec 0.111887, recall 0.797612
2017-12-10T05:48:08.799104: step 8077, loss 0.000126335, acc 1, prec 0.111908, recall 0.797647
2017-12-10T05:48:09.063937: step 8078, loss 4.69612, acc 0.96875, prec 0.111928, recall 0.797613
2017-12-10T05:48:09.343435: step 8079, loss 0.40034, acc 1, prec 0.111939, recall 0.79763
2017-12-10T05:48:09.605924: step 8080, loss 0.249303, acc 0.96875, prec 0.111947, recall 0.797648
2017-12-10T05:48:09.877400: step 8081, loss 0.193427, acc 0.96875, prec 0.111944, recall 0.797648
2017-12-10T05:48:10.155826: step 8082, loss 0.00222305, acc 1, prec 0.111955, recall 0.797665
2017-12-10T05:48:10.419235: step 8083, loss 0.0789592, acc 0.953125, prec 0.111962, recall 0.797683
2017-12-10T05:48:10.682707: step 8084, loss 0.485482, acc 0.9375, prec 0.111967, recall 0.7977
2017-12-10T05:48:10.948999: step 8085, loss 0.208355, acc 0.953125, prec 0.111963, recall 0.7977
2017-12-10T05:48:11.223748: step 8086, loss 0.780733, acc 0.921875, prec 0.111956, recall 0.7977
2017-12-10T05:48:11.498279: step 8087, loss 0.690192, acc 0.84375, prec 0.111954, recall 0.797718
2017-12-10T05:48:11.766635: step 8088, loss 0.536967, acc 0.90625, prec 0.111945, recall 0.797718
2017-12-10T05:48:12.029923: step 8089, loss 0.983244, acc 0.875, prec 0.111945, recall 0.797735
2017-12-10T05:48:12.305110: step 8090, loss 0.834583, acc 0.859375, prec 0.111944, recall 0.797753
2017-12-10T05:48:12.582701: step 8091, loss 0.347797, acc 0.9375, prec 0.111949, recall 0.79777
2017-12-10T05:48:12.847436: step 8092, loss 0.104335, acc 0.953125, prec 0.111945, recall 0.79777
2017-12-10T05:48:13.113166: step 8093, loss 0.373902, acc 0.890625, prec 0.111946, recall 0.797788
2017-12-10T05:48:13.381601: step 8094, loss 0.323212, acc 0.953125, prec 0.111964, recall 0.797823
2017-12-10T05:48:13.659669: step 8095, loss 0.853984, acc 0.953125, prec 0.111971, recall 0.79784
2017-12-10T05:48:13.934722: step 8096, loss 0.0213169, acc 1, prec 0.111992, recall 0.797875
2017-12-10T05:48:14.209260: step 8097, loss 0.415935, acc 0.9375, prec 0.112008, recall 0.79791
2017-12-10T05:48:14.484915: step 8098, loss 0.226131, acc 0.953125, prec 0.112004, recall 0.79791
2017-12-10T05:48:14.752751: step 8099, loss 0.0825192, acc 0.984375, prec 0.112003, recall 0.79791
2017-12-10T05:48:15.017851: step 8100, loss 0.337931, acc 0.921875, prec 0.112007, recall 0.797927

Evaluation:
2017-12-10T05:48:22.579965: step 8100, loss 7.68087, acc 0.929138, prec 0.111941, recall 0.79412

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-8100

2017-12-10T05:48:23.947759: step 8101, loss 0.182515, acc 0.9375, prec 0.111947, recall 0.794138
2017-12-10T05:48:24.214410: step 8102, loss 0.449676, acc 0.953125, prec 0.111943, recall 0.794138
2017-12-10T05:48:24.486572: step 8103, loss 0.267577, acc 0.953125, prec 0.11196, recall 0.794173
2017-12-10T05:48:24.747604: step 8104, loss 0.117597, acc 0.96875, prec 0.111968, recall 0.79419
2017-12-10T05:48:25.011920: step 8105, loss 0.0376469, acc 0.984375, prec 0.111977, recall 0.794208
2017-12-10T05:48:25.284523: step 8106, loss 0.105216, acc 0.96875, prec 0.111974, recall 0.794208
2017-12-10T05:48:25.562975: step 8107, loss 0.00565858, acc 1, prec 0.111985, recall 0.794225
2017-12-10T05:48:25.826868: step 8108, loss 0.105059, acc 0.984375, prec 0.111994, recall 0.794243
2017-12-10T05:48:26.096694: step 8109, loss 0.653609, acc 0.9375, prec 0.112, recall 0.79426
2017-12-10T05:48:26.362395: step 8110, loss 0.292763, acc 0.96875, prec 0.112008, recall 0.794278
2017-12-10T05:48:26.637731: step 8111, loss 0.103352, acc 0.984375, prec 0.112006, recall 0.794278
2017-12-10T05:48:26.905723: step 8112, loss 0.105449, acc 0.96875, prec 0.112014, recall 0.794295
2017-12-10T05:48:27.170402: step 8113, loss 0.278011, acc 0.953125, prec 0.11201, recall 0.794295
2017-12-10T05:48:27.445496: step 8114, loss 9.98042e-05, acc 1, prec 0.112021, recall 0.794313
2017-12-10T05:48:27.706326: step 8115, loss 0.21106, acc 0.96875, prec 0.112029, recall 0.79433
2017-12-10T05:48:27.973087: step 8116, loss 0.0214293, acc 1, prec 0.11204, recall 0.794348
2017-12-10T05:48:28.241655: step 8117, loss 0.1463, acc 0.984375, prec 0.112049, recall 0.794365
2017-12-10T05:48:28.517601: step 8118, loss 0.214608, acc 0.953125, prec 0.112056, recall 0.794383
2017-12-10T05:48:28.787816: step 8119, loss 0.0163594, acc 0.984375, prec 0.112065, recall 0.7944
2017-12-10T05:48:29.053358: step 8120, loss 8.72926e-05, acc 1, prec 0.112065, recall 0.7944
2017-12-10T05:48:29.315676: step 8121, loss 0.000573662, acc 1, prec 0.112065, recall 0.7944
2017-12-10T05:48:29.579999: step 8122, loss 0.0258681, acc 0.984375, prec 0.112074, recall 0.794418
2017-12-10T05:48:29.851218: step 8123, loss 0.00233269, acc 1, prec 0.112074, recall 0.794418
2017-12-10T05:48:30.117435: step 8124, loss 0.128305, acc 0.96875, prec 0.112071, recall 0.794418
2017-12-10T05:48:30.389334: step 8125, loss 0.295049, acc 1, prec 0.112093, recall 0.794453
2017-12-10T05:48:30.671641: step 8126, loss 0.0170172, acc 0.984375, prec 0.112091, recall 0.794453
2017-12-10T05:48:30.946726: step 8127, loss 0.000613328, acc 1, prec 0.112102, recall 0.79447
2017-12-10T05:48:31.210892: step 8128, loss 0.0013715, acc 1, prec 0.112113, recall 0.794488
2017-12-10T05:48:31.476527: step 8129, loss 0.0880064, acc 0.953125, prec 0.112109, recall 0.794488
2017-12-10T05:48:31.747828: step 8130, loss 6.27087, acc 0.96875, prec 0.112107, recall 0.79442
2017-12-10T05:48:32.018950: step 8131, loss 0.00547905, acc 1, prec 0.112129, recall 0.794455
2017-12-10T05:48:32.285839: step 8132, loss 0.130265, acc 1, prec 0.112139, recall 0.794473
2017-12-10T05:48:32.553660: step 8133, loss 0.0629778, acc 0.984375, prec 0.11217, recall 0.794525
2017-12-10T05:48:32.818700: step 8134, loss 0.0586101, acc 0.953125, prec 0.112177, recall 0.794543
2017-12-10T05:48:33.096323: step 8135, loss 0.678165, acc 0.9375, prec 0.112192, recall 0.794578
2017-12-10T05:48:33.362811: step 8136, loss 0.487188, acc 0.9375, prec 0.112198, recall 0.794595
2017-12-10T05:48:33.628390: step 8137, loss 0.272778, acc 0.9375, prec 0.112192, recall 0.794595
2017-12-10T05:48:33.892412: step 8138, loss 1.11941, acc 0.90625, prec 0.112195, recall 0.794613
2017-12-10T05:48:34.160812: step 8139, loss 1.30032, acc 0.921875, prec 0.112188, recall 0.794613
2017-12-10T05:48:34.429452: step 8140, loss 0.282195, acc 0.921875, prec 0.112181, recall 0.794613
2017-12-10T05:48:34.701570: step 8141, loss 0.168048, acc 0.9375, prec 0.112176, recall 0.794613
2017-12-10T05:48:34.965879: step 8142, loss 0.2701, acc 0.96875, prec 0.112184, recall 0.79463
2017-12-10T05:48:35.233176: step 8143, loss 0.217471, acc 0.96875, prec 0.112181, recall 0.79463
2017-12-10T05:48:35.508556: step 8144, loss 0.223256, acc 0.953125, prec 0.112188, recall 0.794647
2017-12-10T05:48:35.774193: step 8145, loss 0.609766, acc 0.9375, prec 0.112193, recall 0.794665
2017-12-10T05:48:36.049721: step 8146, loss 0.126586, acc 0.96875, prec 0.112191, recall 0.794665
2017-12-10T05:48:36.311314: step 8147, loss 0.203169, acc 0.953125, prec 0.112187, recall 0.794665
2017-12-10T05:48:36.576529: step 8148, loss 0.848892, acc 0.890625, prec 0.112209, recall 0.794717
2017-12-10T05:48:36.849965: step 8149, loss 0.112537, acc 0.96875, prec 0.112206, recall 0.794717
2017-12-10T05:48:37.118440: step 8150, loss 0.300115, acc 0.9375, prec 0.112212, recall 0.794735
2017-12-10T05:48:37.390353: step 8151, loss 0.029701, acc 0.984375, prec 0.11221, recall 0.794735
2017-12-10T05:48:37.660686: step 8152, loss 0.601047, acc 0.953125, prec 0.112217, recall 0.794752
2017-12-10T05:48:37.929139: step 8153, loss 0.00936925, acc 1, prec 0.112217, recall 0.794752
2017-12-10T05:48:38.204117: step 8154, loss 0.0491783, acc 0.984375, prec 0.112226, recall 0.794769
2017-12-10T05:48:38.468648: step 8155, loss 0.644734, acc 0.9375, prec 0.112221, recall 0.794769
2017-12-10T05:48:38.732381: step 8156, loss 0.457727, acc 0.9375, prec 0.112215, recall 0.794769
2017-12-10T05:48:38.996219: step 8157, loss 0.236628, acc 0.96875, prec 0.112234, recall 0.794804
2017-12-10T05:48:39.273777: step 8158, loss 0.0898302, acc 0.984375, prec 0.112265, recall 0.794857
2017-12-10T05:48:39.540265: step 8159, loss 0.247836, acc 0.953125, prec 0.112282, recall 0.794891
2017-12-10T05:48:39.806070: step 8160, loss 0.166052, acc 0.96875, prec 0.112279, recall 0.794891
2017-12-10T05:48:40.074562: step 8161, loss 0.604737, acc 0.96875, prec 0.112298, recall 0.794926
2017-12-10T05:48:40.344079: step 8162, loss 0.18809, acc 0.984375, prec 0.112318, recall 0.794961
2017-12-10T05:48:40.610229: step 8163, loss 0.201798, acc 0.984375, prec 0.112338, recall 0.794996
2017-12-10T05:48:40.874518: step 8164, loss 1.8441, acc 0.96875, prec 0.112347, recall 0.794946
2017-12-10T05:48:41.151335: step 8165, loss 0.664, acc 0.953125, prec 0.112343, recall 0.794946
2017-12-10T05:48:41.421082: step 8166, loss 0.0154728, acc 0.984375, prec 0.112363, recall 0.794981
2017-12-10T05:48:41.689505: step 8167, loss 0.37716, acc 0.96875, prec 0.11236, recall 0.794981
2017-12-10T05:48:41.955086: step 8168, loss 0.469978, acc 0.953125, prec 0.112377, recall 0.795015
2017-12-10T05:48:42.221539: step 8169, loss 0.452843, acc 0.96875, prec 0.112375, recall 0.795015
2017-12-10T05:48:42.494880: step 8170, loss 0.230691, acc 0.984375, prec 0.112395, recall 0.79505
2017-12-10T05:48:42.767060: step 8171, loss 0.102084, acc 0.96875, prec 0.112402, recall 0.795067
2017-12-10T05:48:43.031240: step 8172, loss 0.291602, acc 0.9375, prec 0.112408, recall 0.795085
2017-12-10T05:48:43.297081: step 8173, loss 0.00167375, acc 1, prec 0.112408, recall 0.795085
2017-12-10T05:48:43.562362: step 8174, loss 0.107179, acc 0.96875, prec 0.112416, recall 0.795102
2017-12-10T05:48:43.829397: step 8175, loss 0.89603, acc 0.90625, prec 0.112408, recall 0.795102
2017-12-10T05:48:44.096433: step 8176, loss 0.107257, acc 0.96875, prec 0.112405, recall 0.795102
2017-12-10T05:48:44.361954: step 8177, loss 1.10495, acc 0.9375, prec 0.112421, recall 0.795137
2017-12-10T05:48:44.634250: step 8178, loss 0.0647358, acc 0.96875, prec 0.112418, recall 0.795137
2017-12-10T05:48:44.910152: step 8179, loss 0.160703, acc 0.96875, prec 0.112447, recall 0.795189
2017-12-10T05:48:45.177967: step 8180, loss 0.473698, acc 0.96875, prec 0.112455, recall 0.795206
2017-12-10T05:48:45.445501: step 8181, loss 0.283945, acc 0.953125, prec 0.112462, recall 0.795224
2017-12-10T05:48:45.711141: step 8182, loss 0.101626, acc 0.96875, prec 0.11247, recall 0.795241
2017-12-10T05:48:45.983013: step 8183, loss 0.356211, acc 0.953125, prec 0.112487, recall 0.795276
2017-12-10T05:48:46.249811: step 8184, loss 0.662659, acc 0.96875, prec 0.112495, recall 0.795293
2017-12-10T05:48:46.517568: step 8185, loss 0.119703, acc 0.96875, prec 0.112503, recall 0.79531
2017-12-10T05:48:46.786612: step 8186, loss 0.38916, acc 0.953125, prec 0.112499, recall 0.79531
2017-12-10T05:48:47.054862: step 8187, loss 0.324212, acc 0.9375, prec 0.112493, recall 0.79531
2017-12-10T05:48:47.330799: step 8188, loss 0.553978, acc 0.953125, prec 0.112511, recall 0.795345
2017-12-10T05:48:47.598753: step 8189, loss 0.0120709, acc 1, prec 0.112521, recall 0.795362
2017-12-10T05:48:47.865453: step 8190, loss 0.571111, acc 0.953125, prec 0.112528, recall 0.79538
2017-12-10T05:48:48.132769: step 8191, loss 0.197488, acc 0.96875, prec 0.112525, recall 0.79538
2017-12-10T05:48:48.403945: step 8192, loss 0.00710906, acc 1, prec 0.112536, recall 0.795397
2017-12-10T05:48:48.679223: step 8193, loss 0.899088, acc 0.953125, prec 0.112542, recall 0.795414
2017-12-10T05:48:48.954970: step 8194, loss 0.00312903, acc 1, prec 0.112542, recall 0.795414
2017-12-10T05:48:49.230370: step 8195, loss 0.0434746, acc 0.984375, prec 0.112541, recall 0.795414
2017-12-10T05:48:49.497337: step 8196, loss 0.164238, acc 0.96875, prec 0.112549, recall 0.795431
2017-12-10T05:48:49.765261: step 8197, loss 0.587622, acc 0.953125, prec 0.112566, recall 0.795466
2017-12-10T05:48:50.030875: step 8198, loss 0.000340529, acc 1, prec 0.112566, recall 0.795466
2017-12-10T05:48:50.293406: step 8199, loss 0.0920031, acc 0.984375, prec 0.112586, recall 0.795501
2017-12-10T05:48:50.563957: step 8200, loss 0.0542491, acc 0.984375, prec 0.112595, recall 0.795518
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-8200

2017-12-10T05:48:51.925199: step 8201, loss 0.000277428, acc 1, prec 0.112627, recall 0.79557
2017-12-10T05:48:52.188452: step 8202, loss 0.156372, acc 0.953125, prec 0.112655, recall 0.795622
2017-12-10T05:48:52.454474: step 8203, loss 1.16452, acc 0.96875, prec 0.112696, recall 0.795624
2017-12-10T05:48:52.722816: step 8204, loss 0.14538, acc 0.96875, prec 0.112715, recall 0.795658
2017-12-10T05:48:53.000069: step 8205, loss 0.229248, acc 0.96875, prec 0.112723, recall 0.795675
2017-12-10T05:48:53.274140: step 8206, loss 0.0661567, acc 0.984375, prec 0.112732, recall 0.795693
2017-12-10T05:48:53.545369: step 8207, loss 0.338526, acc 0.953125, prec 0.112728, recall 0.795693
2017-12-10T05:48:53.809666: step 8208, loss 0.33178, acc 0.953125, prec 0.112745, recall 0.795727
2017-12-10T05:48:54.074414: step 8209, loss 0.141113, acc 0.96875, prec 0.112774, recall 0.795779
2017-12-10T05:48:54.344832: step 8210, loss 0.284039, acc 0.9375, prec 0.11279, recall 0.795813
2017-12-10T05:48:54.621924: step 8211, loss 0.00132411, acc 1, prec 0.112801, recall 0.795831
2017-12-10T05:48:54.885554: step 8212, loss 0.360418, acc 0.96875, prec 0.112808, recall 0.795848
2017-12-10T05:48:55.154502: step 8213, loss 0.0286316, acc 1, prec 0.112808, recall 0.795848
2017-12-10T05:48:55.421403: step 8214, loss 0.770844, acc 0.921875, prec 0.112812, recall 0.795865
2017-12-10T05:48:55.683407: step 8215, loss 0.495032, acc 0.9375, prec 0.112818, recall 0.795882
2017-12-10T05:48:55.949397: step 8216, loss 0.1078, acc 0.921875, prec 0.112821, recall 0.795899
2017-12-10T05:48:56.214511: step 8217, loss 0.463726, acc 0.921875, prec 0.112815, recall 0.795899
2017-12-10T05:48:56.486379: step 8218, loss 0.434968, acc 0.90625, prec 0.112828, recall 0.795934
2017-12-10T05:48:56.747028: step 8219, loss 0.129647, acc 0.96875, prec 0.112836, recall 0.795951
2017-12-10T05:48:57.019210: step 8220, loss 0.0210867, acc 0.984375, prec 0.112834, recall 0.795951
2017-12-10T05:48:57.299392: step 8221, loss 0.411541, acc 0.9375, prec 0.11284, recall 0.795968
2017-12-10T05:48:57.572376: step 8222, loss 0.16904, acc 0.96875, prec 0.112858, recall 0.796003
2017-12-10T05:48:57.838300: step 8223, loss 0.316106, acc 0.96875, prec 0.112855, recall 0.796003
2017-12-10T05:48:58.105094: step 8224, loss 0.18881, acc 0.921875, prec 0.112849, recall 0.796003
2017-12-10T05:48:58.372767: step 8225, loss 0.0913332, acc 0.984375, prec 0.112847, recall 0.796003
2017-12-10T05:48:58.640304: step 8226, loss 0.235461, acc 0.984375, prec 0.112867, recall 0.796037
2017-12-10T05:48:58.908071: step 8227, loss 0.0988427, acc 0.96875, prec 0.112864, recall 0.796037
2017-12-10T05:48:59.172417: step 8228, loss 0.0316075, acc 0.984375, prec 0.112874, recall 0.796054
2017-12-10T05:48:59.440893: step 8229, loss 0.118651, acc 0.953125, prec 0.11287, recall 0.796054
2017-12-10T05:48:59.704931: step 8230, loss 0.00999578, acc 1, prec 0.11287, recall 0.796054
2017-12-10T05:48:59.976516: step 8231, loss 0.190193, acc 0.984375, prec 0.1129, recall 0.796106
2017-12-10T05:49:00.240050: step 8232, loss 0.55979, acc 0.953125, prec 0.112896, recall 0.796106
2017-12-10T05:49:00.511045: step 8233, loss 1.12468, acc 0.984375, prec 0.112905, recall 0.796123
2017-12-10T05:49:00.786611: step 8234, loss 3.71541e-05, acc 1, prec 0.112905, recall 0.796123
2017-12-10T05:49:01.046143: step 8235, loss 0.0103788, acc 1, prec 0.112905, recall 0.796123
2017-12-10T05:49:01.311733: step 8236, loss 0.00318129, acc 1, prec 0.112927, recall 0.796157
2017-12-10T05:49:01.580780: step 8237, loss 0.0101296, acc 1, prec 0.112937, recall 0.796175
2017-12-10T05:49:01.847539: step 8238, loss 0.353521, acc 0.984375, prec 0.112946, recall 0.796192
2017-12-10T05:49:02.120241: step 8239, loss 0.600509, acc 0.96875, prec 0.112954, recall 0.796209
2017-12-10T05:49:02.392588: step 8240, loss 0.0589437, acc 0.984375, prec 0.112974, recall 0.796243
2017-12-10T05:49:02.662488: step 8241, loss 0.0250209, acc 0.984375, prec 0.112973, recall 0.796243
2017-12-10T05:49:02.927502: step 8242, loss 0.00919179, acc 1, prec 0.112994, recall 0.796278
2017-12-10T05:49:03.189186: step 8243, loss 0.0573379, acc 0.984375, prec 0.113014, recall 0.796312
2017-12-10T05:49:03.456231: step 8244, loss 0.452405, acc 0.9375, prec 0.113008, recall 0.796312
2017-12-10T05:49:03.729101: step 8245, loss 0.0386747, acc 0.984375, prec 0.113007, recall 0.796312
2017-12-10T05:49:03.992064: step 8246, loss 0.478897, acc 0.921875, prec 0.113, recall 0.796312
2017-12-10T05:49:04.255273: step 8247, loss 0.0379928, acc 0.984375, prec 0.112999, recall 0.796312
2017-12-10T05:49:04.526794: step 8248, loss 0.000311165, acc 1, prec 0.11301, recall 0.796329
2017-12-10T05:49:04.793608: step 8249, loss 0.325393, acc 0.96875, prec 0.113028, recall 0.796363
2017-12-10T05:49:05.063911: step 8250, loss 0.341395, acc 0.9375, prec 0.113033, recall 0.79638
2017-12-10T05:49:05.328617: step 8251, loss 4.42959, acc 0.96875, prec 0.113032, recall 0.796313
2017-12-10T05:49:05.598206: step 8252, loss 0.0241265, acc 0.984375, prec 0.113031, recall 0.796313
2017-12-10T05:49:05.864961: step 8253, loss 0.315547, acc 0.953125, prec 0.113037, recall 0.796331
2017-12-10T05:49:06.122597: step 8254, loss 0.136038, acc 0.96875, prec 0.113066, recall 0.796382
2017-12-10T05:49:06.396106: step 8255, loss 0.216688, acc 0.96875, prec 0.113064, recall 0.796382
2017-12-10T05:49:06.673885: step 8256, loss 0.294184, acc 0.96875, prec 0.113061, recall 0.796382
2017-12-10T05:49:06.940060: step 8257, loss 0.0885925, acc 0.953125, prec 0.113078, recall 0.796416
2017-12-10T05:49:07.211555: step 8258, loss 0.12348, acc 0.96875, prec 0.113107, recall 0.796468
2017-12-10T05:49:07.475883: step 8259, loss 0.0945506, acc 0.953125, prec 0.113124, recall 0.796502
2017-12-10T05:49:07.743520: step 8260, loss 0.11374, acc 0.953125, prec 0.11312, recall 0.796502
2017-12-10T05:49:08.010951: step 8261, loss 0.367171, acc 0.90625, prec 0.113123, recall 0.796519
2017-12-10T05:49:08.275646: step 8262, loss 0.453935, acc 0.9375, prec 0.113128, recall 0.796536
2017-12-10T05:49:08.544584: step 8263, loss 0.180081, acc 0.9375, prec 0.113122, recall 0.796536
2017-12-10T05:49:08.813548: step 8264, loss 0.517884, acc 0.9375, prec 0.113117, recall 0.796536
2017-12-10T05:49:09.079152: step 8265, loss 0.748151, acc 0.9375, prec 0.113122, recall 0.796553
2017-12-10T05:49:09.343517: step 8266, loss 0.179294, acc 0.984375, prec 0.113121, recall 0.796553
2017-12-10T05:49:09.608472: step 8267, loss 0.0422526, acc 0.984375, prec 0.113119, recall 0.796553
2017-12-10T05:49:09.876526: step 8268, loss 0.110484, acc 0.984375, prec 0.113129, recall 0.79657
2017-12-10T05:49:10.144683: step 8269, loss 0.201594, acc 0.96875, prec 0.113126, recall 0.79657
2017-12-10T05:49:10.408599: step 8270, loss 0.197894, acc 0.984375, prec 0.113125, recall 0.79657
2017-12-10T05:49:10.675151: step 8271, loss 0.426185, acc 0.96875, prec 0.113122, recall 0.79657
2017-12-10T05:49:10.940208: step 8272, loss 0.183532, acc 0.953125, prec 0.113118, recall 0.79657
2017-12-10T05:49:11.214170: step 8273, loss 0.0980057, acc 0.984375, prec 0.113117, recall 0.79657
2017-12-10T05:49:11.479662: step 8274, loss 0.529719, acc 0.921875, prec 0.11311, recall 0.79657
2017-12-10T05:49:11.750537: step 8275, loss 0.0719483, acc 0.984375, prec 0.113119, recall 0.796587
2017-12-10T05:49:12.017720: step 8276, loss 0.643172, acc 0.921875, prec 0.113133, recall 0.796622
2017-12-10T05:49:12.290082: step 8277, loss 0.447985, acc 0.953125, prec 0.113129, recall 0.796622
2017-12-10T05:49:12.562329: step 8278, loss 0.252201, acc 0.953125, prec 0.113136, recall 0.796639
2017-12-10T05:49:12.827867: step 8279, loss 0.447528, acc 0.96875, prec 0.113133, recall 0.796639
2017-12-10T05:49:13.097009: step 8280, loss 0.0909173, acc 0.953125, prec 0.11314, recall 0.796656
2017-12-10T05:49:13.376037: step 8281, loss 0.132504, acc 0.984375, prec 0.113149, recall 0.796673
2017-12-10T05:49:13.642768: step 8282, loss 0.278069, acc 0.96875, prec 0.113157, recall 0.79669
2017-12-10T05:49:13.904247: step 8283, loss 0.00298107, acc 1, prec 0.113167, recall 0.796707
2017-12-10T05:49:14.172668: step 8284, loss 0.0786138, acc 0.96875, prec 0.113186, recall 0.796741
2017-12-10T05:49:14.435716: step 8285, loss 0.0771453, acc 0.984375, prec 0.113195, recall 0.796758
2017-12-10T05:49:14.709209: step 8286, loss 0.754197, acc 0.9375, prec 0.11319, recall 0.796758
2017-12-10T05:49:14.981688: step 8287, loss 0.117624, acc 0.96875, prec 0.113187, recall 0.796758
2017-12-10T05:49:15.247589: step 8288, loss 0.498746, acc 0.984375, prec 0.113196, recall 0.796775
2017-12-10T05:49:15.512926: step 8289, loss 0.0223218, acc 0.984375, prec 0.113206, recall 0.796792
2017-12-10T05:49:15.786985: step 8290, loss 0.429179, acc 0.984375, prec 0.113204, recall 0.796792
2017-12-10T05:49:16.049143: step 8291, loss 0.194497, acc 0.984375, prec 0.113203, recall 0.796792
2017-12-10T05:49:16.309502: step 8292, loss 2.13422, acc 0.96875, prec 0.113212, recall 0.796742
2017-12-10T05:49:16.583536: step 8293, loss 0.103978, acc 0.984375, prec 0.113211, recall 0.796742
2017-12-10T05:49:16.860006: step 8294, loss 0.0823968, acc 0.984375, prec 0.113231, recall 0.796777
2017-12-10T05:49:17.124047: step 8295, loss 0.684822, acc 0.96875, prec 0.113249, recall 0.796811
2017-12-10T05:49:17.388808: step 8296, loss 0.196983, acc 0.96875, prec 0.113246, recall 0.796811
2017-12-10T05:49:17.659542: step 8297, loss 0.487895, acc 0.953125, prec 0.113242, recall 0.796811
2017-12-10T05:49:17.924536: step 8298, loss 0.299917, acc 0.984375, prec 0.113241, recall 0.796811
2017-12-10T05:49:18.194450: step 8299, loss 0.0449727, acc 0.984375, prec 0.11324, recall 0.796811
2017-12-10T05:49:18.467105: step 8300, loss 0.775645, acc 0.9375, prec 0.113245, recall 0.796828
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-8300

2017-12-10T05:49:19.836952: step 8301, loss 0.448159, acc 0.921875, prec 0.113238, recall 0.796828
2017-12-10T05:49:20.111385: step 8302, loss 0.256625, acc 0.96875, prec 0.113235, recall 0.796828
2017-12-10T05:49:20.380524: step 8303, loss 0.431572, acc 0.953125, prec 0.113242, recall 0.796845
2017-12-10T05:49:20.648720: step 8304, loss 0.345696, acc 0.921875, prec 0.113246, recall 0.796862
2017-12-10T05:49:20.913705: step 8305, loss 0.0803709, acc 0.96875, prec 0.113243, recall 0.796862
2017-12-10T05:49:21.179410: step 8306, loss 0.332733, acc 0.953125, prec 0.113239, recall 0.796862
2017-12-10T05:49:21.449879: step 8307, loss 0.39826, acc 0.921875, prec 0.113253, recall 0.796896
2017-12-10T05:49:21.716427: step 8308, loss 0.636349, acc 0.875, prec 0.113253, recall 0.796913
2017-12-10T05:49:21.982854: step 8309, loss 0.500674, acc 0.953125, prec 0.11326, recall 0.79693
2017-12-10T05:49:22.248407: step 8310, loss 0.000529763, acc 1, prec 0.113281, recall 0.796964
2017-12-10T05:49:22.510670: step 8311, loss 0.282259, acc 0.96875, prec 0.113289, recall 0.796981
2017-12-10T05:49:22.785099: step 8312, loss 0.372443, acc 0.90625, prec 0.113291, recall 0.796998
2017-12-10T05:49:23.058290: step 8313, loss 0.255346, acc 0.96875, prec 0.113299, recall 0.797015
2017-12-10T05:49:23.326195: step 8314, loss 0.143099, acc 0.96875, prec 0.113307, recall 0.797032
2017-12-10T05:49:23.598366: step 8315, loss 0.270668, acc 0.96875, prec 0.113304, recall 0.797032
2017-12-10T05:49:23.865860: step 8316, loss 0.151235, acc 0.984375, prec 0.113313, recall 0.797049
2017-12-10T05:49:24.138037: step 8317, loss 0.0226185, acc 0.984375, prec 0.113312, recall 0.797049
2017-12-10T05:49:24.405548: step 8318, loss 0.289397, acc 0.9375, prec 0.113306, recall 0.797049
2017-12-10T05:49:24.667806: step 8319, loss 0.243014, acc 0.984375, prec 0.113305, recall 0.797049
2017-12-10T05:49:24.938927: step 8320, loss 0.0214606, acc 0.984375, prec 0.113304, recall 0.797049
2017-12-10T05:49:25.210312: step 8321, loss 0.131806, acc 0.984375, prec 0.113324, recall 0.797083
2017-12-10T05:49:25.483783: step 8322, loss 0.205644, acc 0.96875, prec 0.113331, recall 0.7971
2017-12-10T05:49:25.755101: step 8323, loss 0.291773, acc 0.953125, prec 0.113338, recall 0.797117
2017-12-10T05:49:26.023569: step 8324, loss 0.0167763, acc 0.984375, prec 0.113337, recall 0.797117
2017-12-10T05:49:26.292483: step 8325, loss 0.0036286, acc 1, prec 0.113347, recall 0.797134
2017-12-10T05:49:26.557563: step 8326, loss 0.0171055, acc 0.984375, prec 0.113356, recall 0.797151
2017-12-10T05:49:26.820138: step 8327, loss 0.197714, acc 0.953125, prec 0.113352, recall 0.797151
2017-12-10T05:49:27.085890: step 8328, loss 0.000397206, acc 1, prec 0.113352, recall 0.797151
2017-12-10T05:49:27.352601: step 8329, loss 0.186923, acc 0.96875, prec 0.11336, recall 0.797168
2017-12-10T05:49:27.619354: step 8330, loss 0.0410369, acc 0.984375, prec 0.11338, recall 0.797202
2017-12-10T05:49:27.895824: step 8331, loss 0.339962, acc 0.96875, prec 0.11343, recall 0.797287
2017-12-10T05:49:28.163471: step 8332, loss 0.199418, acc 0.984375, prec 0.113429, recall 0.797287
2017-12-10T05:49:28.429958: step 8333, loss 0.121381, acc 0.96875, prec 0.113426, recall 0.797287
2017-12-10T05:49:28.700311: step 8334, loss 0.037006, acc 0.984375, prec 0.113435, recall 0.797304
2017-12-10T05:49:28.964952: step 8335, loss 0.299416, acc 0.953125, prec 0.113442, recall 0.797321
2017-12-10T05:49:29.235640: step 8336, loss 0.0590985, acc 0.96875, prec 0.113439, recall 0.797321
2017-12-10T05:49:29.504810: step 8337, loss 0.00101452, acc 1, prec 0.11346, recall 0.797355
2017-12-10T05:49:29.778009: step 8338, loss 0.166375, acc 0.984375, prec 0.113469, recall 0.797372
2017-12-10T05:49:30.045351: step 8339, loss 0.0354455, acc 0.984375, prec 0.113468, recall 0.797372
2017-12-10T05:49:30.324188: step 8340, loss 0.297767, acc 0.9375, prec 0.113473, recall 0.797389
2017-12-10T05:49:30.598291: step 8341, loss 0.274447, acc 0.984375, prec 0.113482, recall 0.797406
2017-12-10T05:49:30.871346: step 8342, loss 0.00746414, acc 1, prec 0.113525, recall 0.797474
2017-12-10T05:49:31.139140: step 8343, loss 0.0160047, acc 0.984375, prec 0.113523, recall 0.797474
2017-12-10T05:49:31.403347: step 8344, loss 0.453438, acc 0.953125, prec 0.11353, recall 0.797491
2017-12-10T05:49:31.669379: step 8345, loss 4.12933e-05, acc 1, prec 0.11353, recall 0.797491
2017-12-10T05:49:31.927536: step 8346, loss 0.771736, acc 0.953125, prec 0.113557, recall 0.797541
2017-12-10T05:49:32.194734: step 8347, loss 0.12464, acc 0.984375, prec 0.113556, recall 0.797541
2017-12-10T05:49:32.469594: step 8348, loss 0.0243003, acc 0.984375, prec 0.113555, recall 0.797541
2017-12-10T05:49:32.734184: step 8349, loss 7.37803e-05, acc 1, prec 0.113576, recall 0.797575
2017-12-10T05:49:32.991352: step 8350, loss 0.385564, acc 0.984375, prec 0.113596, recall 0.797609
2017-12-10T05:49:33.251695: step 8351, loss 3.48197e-05, acc 1, prec 0.113596, recall 0.797609
2017-12-10T05:49:33.511516: step 8352, loss 0.0010728, acc 1, prec 0.113606, recall 0.797626
2017-12-10T05:49:33.771241: step 8353, loss 0.0457744, acc 0.984375, prec 0.113615, recall 0.797643
2017-12-10T05:49:34.037453: step 8354, loss 0.0787539, acc 0.984375, prec 0.113624, recall 0.79766
2017-12-10T05:49:34.303923: step 8355, loss 0.000419537, acc 1, prec 0.113624, recall 0.79766
2017-12-10T05:49:34.569054: step 8356, loss 0.128499, acc 0.984375, prec 0.113634, recall 0.797677
2017-12-10T05:49:34.835623: step 8357, loss 0.003325, acc 1, prec 0.113655, recall 0.797711
2017-12-10T05:49:35.104378: step 8358, loss 0.32939, acc 0.96875, prec 0.113673, recall 0.797744
2017-12-10T05:49:35.373678: step 8359, loss 0.33018, acc 0.953125, prec 0.11368, recall 0.797761
2017-12-10T05:49:35.650700: step 8360, loss 1.41199, acc 0.96875, prec 0.113678, recall 0.797695
2017-12-10T05:49:35.922628: step 8361, loss 15.586, acc 0.9375, prec 0.113674, recall 0.797628
2017-12-10T05:49:36.903204: step 8362, loss 0.00075562, acc 1, prec 0.113674, recall 0.797628
2017-12-10T05:49:37.262714: step 8363, loss 0.0957636, acc 0.984375, prec 0.113673, recall 0.797628
2017-12-10T05:49:37.734896: step 8364, loss 0.361047, acc 0.96875, prec 0.11367, recall 0.797628
2017-12-10T05:49:38.458406: step 8365, loss 0.0165855, acc 1, prec 0.113681, recall 0.797645
2017-12-10T05:49:39.590322: step 8366, loss 0.169844, acc 0.921875, prec 0.113685, recall 0.797662
2017-12-10T05:49:39.952566: step 8367, loss 0.579082, acc 0.9375, prec 0.1137, recall 0.797696
2017-12-10T05:49:40.230470: step 8368, loss 0.216396, acc 0.9375, prec 0.113705, recall 0.797712
2017-12-10T05:49:40.522045: step 8369, loss 1.26688, acc 0.875, prec 0.113695, recall 0.797712
2017-12-10T05:49:40.806458: step 8370, loss 0.980783, acc 0.859375, prec 0.113682, recall 0.797712
2017-12-10T05:49:41.097959: step 8371, loss 0.985613, acc 0.859375, prec 0.11367, recall 0.797712
2017-12-10T05:49:41.383174: step 8372, loss 1.04413, acc 0.796875, prec 0.113674, recall 0.797746
2017-12-10T05:49:41.652710: step 8373, loss 1.00381, acc 0.8125, prec 0.113668, recall 0.797763
2017-12-10T05:49:41.921614: step 8374, loss 0.670543, acc 0.890625, prec 0.11369, recall 0.797814
2017-12-10T05:49:42.189486: step 8375, loss 0.909178, acc 0.875, prec 0.113679, recall 0.797814
2017-12-10T05:49:42.467405: step 8376, loss 0.294875, acc 0.9375, prec 0.113674, recall 0.797814
2017-12-10T05:49:42.730414: step 8377, loss 0.247489, acc 0.921875, prec 0.113678, recall 0.797831
2017-12-10T05:49:43.006832: step 8378, loss 0.382047, acc 0.953125, prec 0.113684, recall 0.797848
2017-12-10T05:49:43.277227: step 8379, loss 0.184865, acc 0.984375, prec 0.113683, recall 0.797848
2017-12-10T05:49:43.546815: step 8380, loss 0.184418, acc 0.953125, prec 0.11371, recall 0.797898
2017-12-10T05:49:43.816150: step 8381, loss 0.336209, acc 0.953125, prec 0.113706, recall 0.797898
2017-12-10T05:49:44.083203: step 8382, loss 0.669879, acc 0.9375, prec 0.113722, recall 0.797932
2017-12-10T05:49:44.351468: step 8383, loss 0.236533, acc 0.984375, prec 0.113731, recall 0.797949
2017-12-10T05:49:44.626365: step 8384, loss 0.249695, acc 0.953125, prec 0.113727, recall 0.797949
2017-12-10T05:49:44.904249: step 8385, loss 0.405179, acc 0.9375, prec 0.113732, recall 0.797965
2017-12-10T05:49:45.178367: step 8386, loss 0.299085, acc 0.953125, prec 0.113728, recall 0.797965
2017-12-10T05:49:45.444445: step 8387, loss 0.15765, acc 0.96875, prec 0.113725, recall 0.797965
2017-12-10T05:49:45.725736: step 8388, loss 0.188254, acc 0.953125, prec 0.113721, recall 0.797965
2017-12-10T05:49:45.999139: step 8389, loss 0.284393, acc 0.953125, prec 0.113728, recall 0.797982
2017-12-10T05:49:46.263203: step 8390, loss 0.0660662, acc 0.96875, prec 0.113736, recall 0.797999
2017-12-10T05:49:46.530266: step 8391, loss 0.108387, acc 0.953125, prec 0.113742, recall 0.798016
2017-12-10T05:49:46.792693: step 8392, loss 0.0150633, acc 1, prec 0.113753, recall 0.798033
2017-12-10T05:49:47.067721: step 8393, loss 0.00129553, acc 1, prec 0.113753, recall 0.798033
2017-12-10T05:49:47.336696: step 8394, loss 0.275804, acc 0.953125, prec 0.113759, recall 0.79805
2017-12-10T05:49:47.606352: step 8395, loss 0.0301411, acc 1, prec 0.11377, recall 0.798066
2017-12-10T05:49:47.877179: step 8396, loss 0.0230605, acc 1, prec 0.11378, recall 0.798083
2017-12-10T05:49:48.142283: step 8397, loss 0.198124, acc 0.984375, prec 0.113811, recall 0.798134
2017-12-10T05:49:48.408860: step 8398, loss 0.0330962, acc 0.96875, prec 0.113808, recall 0.798134
2017-12-10T05:49:48.680033: step 8399, loss 0.00492743, acc 1, prec 0.113818, recall 0.798151
2017-12-10T05:49:48.945652: step 8400, loss 12.2911, acc 0.984375, prec 0.113818, recall 0.798084

Evaluation:
2017-12-10T05:49:56.606221: step 8400, loss 11.4518, acc 0.961313, prec 0.114005, recall 0.792681

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-8400

2017-12-10T05:49:57.935319: step 8401, loss 0.479237, acc 0.96875, prec 0.114002, recall 0.792681
2017-12-10T05:49:58.200767: step 8402, loss 0.33153, acc 0.96875, prec 0.114031, recall 0.792732
2017-12-10T05:49:58.468102: step 8403, loss 0.383153, acc 0.953125, prec 0.114037, recall 0.792749
2017-12-10T05:49:58.734324: step 8404, loss 0.0648652, acc 0.984375, prec 0.114057, recall 0.792783
2017-12-10T05:49:58.997706: step 8405, loss 0.0535697, acc 0.96875, prec 0.114086, recall 0.792834
2017-12-10T05:49:59.270853: step 8406, loss 0.190124, acc 0.953125, prec 0.114082, recall 0.792834
2017-12-10T05:49:59.545580: step 8407, loss 0.141775, acc 0.953125, prec 0.114099, recall 0.792868
2017-12-10T05:49:59.810427: step 8408, loss 0.0512374, acc 0.96875, prec 0.114106, recall 0.792885
2017-12-10T05:50:00.074476: step 8409, loss 0.326015, acc 0.953125, prec 0.114102, recall 0.792885
2017-12-10T05:50:00.348647: step 8410, loss 0.119477, acc 0.96875, prec 0.1141, recall 0.792885
2017-12-10T05:50:00.618789: step 8411, loss 0.177804, acc 0.96875, prec 0.114118, recall 0.792919
2017-12-10T05:50:00.890245: step 8412, loss 0.0491034, acc 0.984375, prec 0.114116, recall 0.792919
2017-12-10T05:50:01.156343: step 8413, loss 0.371097, acc 0.953125, prec 0.114112, recall 0.792919
2017-12-10T05:50:01.428040: step 8414, loss 1.64296, acc 0.921875, prec 0.114118, recall 0.792871
2017-12-10T05:50:01.698265: step 8415, loss 0.346007, acc 0.9375, prec 0.114123, recall 0.792888
2017-12-10T05:50:01.969345: step 8416, loss 0.601051, acc 0.890625, prec 0.114124, recall 0.792905
2017-12-10T05:50:02.231526: step 8417, loss 0.149512, acc 0.9375, prec 0.114139, recall 0.792939
2017-12-10T05:50:02.506928: step 8418, loss 0.317419, acc 0.90625, prec 0.114142, recall 0.792956
2017-12-10T05:50:02.772780: step 8419, loss 0.616011, acc 0.953125, prec 0.114169, recall 0.793007
2017-12-10T05:50:03.038510: step 8420, loss 0.740303, acc 0.875, prec 0.114169, recall 0.793024
2017-12-10T05:50:03.307345: step 8421, loss 0.299431, acc 0.90625, prec 0.114181, recall 0.793058
2017-12-10T05:50:03.574906: step 8422, loss 0.312028, acc 0.921875, prec 0.114175, recall 0.793058
2017-12-10T05:50:03.841695: step 8423, loss 0.502113, acc 0.921875, prec 0.114168, recall 0.793058
2017-12-10T05:50:04.102574: step 8424, loss 0.933479, acc 0.890625, prec 0.114158, recall 0.793058
2017-12-10T05:50:04.369984: step 8425, loss 0.0600154, acc 0.953125, prec 0.114165, recall 0.793075
2017-12-10T05:50:04.646061: step 8426, loss 0.261084, acc 0.953125, prec 0.114182, recall 0.793109
2017-12-10T05:50:04.917368: step 8427, loss 0.980761, acc 0.90625, prec 0.114174, recall 0.793109
2017-12-10T05:50:05.185573: step 8428, loss 0.159939, acc 0.9375, prec 0.114168, recall 0.793109
2017-12-10T05:50:05.464653: step 8429, loss 0.284936, acc 0.953125, prec 0.114185, recall 0.793143
2017-12-10T05:50:05.729521: step 8430, loss 0.763718, acc 0.9375, prec 0.11418, recall 0.793143
2017-12-10T05:50:05.996323: step 8431, loss 0.171801, acc 0.984375, prec 0.114189, recall 0.79316
2017-12-10T05:50:06.270637: step 8432, loss 0.0403052, acc 0.984375, prec 0.114208, recall 0.793194
2017-12-10T05:50:06.532314: step 8433, loss 0.346129, acc 0.921875, prec 0.114202, recall 0.793194
2017-12-10T05:50:06.799753: step 8434, loss 0.213146, acc 0.96875, prec 0.114199, recall 0.793194
2017-12-10T05:50:07.070342: step 8435, loss 0.0207866, acc 0.984375, prec 0.114229, recall 0.793245
2017-12-10T05:50:07.343903: step 8436, loss 0.417507, acc 0.953125, prec 0.114246, recall 0.793279
2017-12-10T05:50:07.626606: step 8437, loss 0.19939, acc 0.96875, prec 0.114243, recall 0.793279
2017-12-10T05:50:07.909513: step 8438, loss 0.0975367, acc 0.953125, prec 0.114239, recall 0.793279
2017-12-10T05:50:08.186040: step 8439, loss 0.0115598, acc 1, prec 0.11425, recall 0.793296
2017-12-10T05:50:08.448121: step 8440, loss 0.423783, acc 0.953125, prec 0.114256, recall 0.793313
2017-12-10T05:50:08.710785: step 8441, loss 0.315442, acc 0.96875, prec 0.114274, recall 0.793346
2017-12-10T05:50:08.976634: step 8442, loss 1.00535, acc 0.90625, prec 0.114277, recall 0.793363
2017-12-10T05:50:09.250032: step 8443, loss 0.0431585, acc 1, prec 0.114287, recall 0.79338
2017-12-10T05:50:09.517495: step 8444, loss 0.00251826, acc 1, prec 0.114298, recall 0.793397
2017-12-10T05:50:09.783245: step 8445, loss 0.0907194, acc 1, prec 0.114308, recall 0.793414
2017-12-10T05:50:10.056499: step 8446, loss 0.0177587, acc 0.984375, prec 0.114307, recall 0.793414
2017-12-10T05:50:10.328509: step 8447, loss 0.0336456, acc 0.96875, prec 0.114314, recall 0.793431
2017-12-10T05:50:10.596217: step 8448, loss 0.137442, acc 0.984375, prec 0.114313, recall 0.793431
2017-12-10T05:50:10.831241: step 8449, loss 0.286839, acc 0.961538, prec 0.114331, recall 0.793465
2017-12-10T05:50:11.100866: step 8450, loss 0.4966, acc 0.96875, prec 0.114349, recall 0.793499
2017-12-10T05:50:11.365612: step 8451, loss 0.214467, acc 0.984375, prec 0.114348, recall 0.793499
2017-12-10T05:50:11.634144: step 8452, loss 0.0097612, acc 1, prec 0.114348, recall 0.793499
2017-12-10T05:50:11.899861: step 8453, loss 0.000219986, acc 1, prec 0.114359, recall 0.793516
2017-12-10T05:50:12.168413: step 8454, loss 1.28451, acc 0.96875, prec 0.114357, recall 0.793451
2017-12-10T05:50:12.447962: step 8455, loss 0.279039, acc 0.96875, prec 0.114386, recall 0.793501
2017-12-10T05:50:12.710817: step 8456, loss 0.00642927, acc 1, prec 0.114386, recall 0.793501
2017-12-10T05:50:12.978513: step 8457, loss 0.0460548, acc 0.96875, prec 0.114404, recall 0.793535
2017-12-10T05:50:13.247926: step 8458, loss 0.353833, acc 0.984375, prec 0.114424, recall 0.793569
2017-12-10T05:50:13.516052: step 8459, loss 0.524614, acc 0.96875, prec 0.114421, recall 0.793569
2017-12-10T05:50:13.789591: step 8460, loss 0.0102756, acc 1, prec 0.114442, recall 0.793603
2017-12-10T05:50:14.050615: step 8461, loss 0.34299, acc 0.953125, prec 0.114469, recall 0.793653
2017-12-10T05:50:14.317952: step 8462, loss 0.0839487, acc 0.984375, prec 0.114478, recall 0.79367
2017-12-10T05:50:14.581057: step 8463, loss 0.142612, acc 0.96875, prec 0.114486, recall 0.793687
2017-12-10T05:50:14.846971: step 8464, loss 0.424797, acc 0.96875, prec 0.114483, recall 0.793687
2017-12-10T05:50:15.110690: step 8465, loss 0.0988368, acc 0.9375, prec 0.114488, recall 0.793704
2017-12-10T05:50:15.376316: step 8466, loss 0.00379432, acc 1, prec 0.114488, recall 0.793704
2017-12-10T05:50:15.642034: step 8467, loss 0.200084, acc 0.9375, prec 0.114483, recall 0.793704
2017-12-10T05:50:15.905184: step 8468, loss 0.427657, acc 0.984375, prec 0.114481, recall 0.793704
2017-12-10T05:50:16.172485: step 8469, loss 0.10643, acc 0.984375, prec 0.114491, recall 0.793721
2017-12-10T05:50:16.440719: step 8470, loss 0.116983, acc 0.96875, prec 0.114498, recall 0.793738
2017-12-10T05:50:16.709408: step 8471, loss 0.431641, acc 0.96875, prec 0.114537, recall 0.793805
2017-12-10T05:50:16.973266: step 8472, loss 1.80701e-05, acc 1, prec 0.114537, recall 0.793805
2017-12-10T05:50:17.240896: step 8473, loss 0.361963, acc 0.96875, prec 0.114535, recall 0.793805
2017-12-10T05:50:17.505937: step 8474, loss 0.0804961, acc 0.984375, prec 0.114533, recall 0.793805
2017-12-10T05:50:17.773839: step 8475, loss 0.7051, acc 0.9375, prec 0.114528, recall 0.793805
2017-12-10T05:50:18.035997: step 8476, loss 0.0034089, acc 1, prec 0.114538, recall 0.793822
2017-12-10T05:50:18.298884: step 8477, loss 0.0766706, acc 0.984375, prec 0.114568, recall 0.793873
2017-12-10T05:50:18.566792: step 8478, loss 1.27782, acc 0.96875, prec 0.114567, recall 0.793808
2017-12-10T05:50:18.848900: step 8479, loss 0.197271, acc 0.96875, prec 0.114575, recall 0.793825
2017-12-10T05:50:19.118289: step 8480, loss 0.308348, acc 0.953125, prec 0.114571, recall 0.793825
2017-12-10T05:50:19.387743: step 8481, loss 0.0170893, acc 1, prec 0.114581, recall 0.793841
2017-12-10T05:50:19.654839: step 8482, loss 0.0872624, acc 0.984375, prec 0.11459, recall 0.793858
2017-12-10T05:50:19.921968: step 8483, loss 0.00746708, acc 1, prec 0.11459, recall 0.793858
2017-12-10T05:50:20.190324: step 8484, loss 0.286586, acc 0.953125, prec 0.114586, recall 0.793858
2017-12-10T05:50:20.456314: step 8485, loss 0.459247, acc 0.90625, prec 0.114588, recall 0.793875
2017-12-10T05:50:20.725632: step 8486, loss 0.301423, acc 0.953125, prec 0.114616, recall 0.793926
2017-12-10T05:50:20.987979: step 8487, loss 0.0737478, acc 0.984375, prec 0.114635, recall 0.793959
2017-12-10T05:50:21.258815: step 8488, loss 1.03926, acc 0.984375, prec 0.114655, recall 0.793993
2017-12-10T05:50:21.526459: step 8489, loss 0.54719, acc 0.921875, prec 0.114648, recall 0.793993
2017-12-10T05:50:21.799856: step 8490, loss 0.274142, acc 0.953125, prec 0.114644, recall 0.793993
2017-12-10T05:50:22.071545: step 8491, loss 0.137602, acc 0.96875, prec 0.114641, recall 0.793993
2017-12-10T05:50:22.333500: step 8492, loss 0.296153, acc 0.9375, prec 0.114636, recall 0.793993
2017-12-10T05:50:22.599749: step 8493, loss 0.498312, acc 0.921875, prec 0.11466, recall 0.794043
2017-12-10T05:50:22.867901: step 8494, loss 0.538432, acc 0.9375, prec 0.114676, recall 0.794077
2017-12-10T05:50:23.133270: step 8495, loss 0.287868, acc 0.9375, prec 0.114691, recall 0.79411
2017-12-10T05:50:23.400151: step 8496, loss 0.786492, acc 0.921875, prec 0.114695, recall 0.794127
2017-12-10T05:50:23.667972: step 8497, loss 0.0858636, acc 0.96875, prec 0.114703, recall 0.794144
2017-12-10T05:50:23.939423: step 8498, loss 0.132601, acc 0.96875, prec 0.11471, recall 0.794161
2017-12-10T05:50:24.206347: step 8499, loss 0.442551, acc 0.921875, prec 0.114725, recall 0.794194
2017-12-10T05:50:24.471054: step 8500, loss 0.0183803, acc 0.984375, prec 0.114744, recall 0.794228
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-8500

2017-12-10T05:50:25.696292: step 8501, loss 0.239479, acc 0.953125, prec 0.11474, recall 0.794228
2017-12-10T05:50:25.964046: step 8502, loss 0.156092, acc 0.953125, prec 0.114736, recall 0.794228
2017-12-10T05:50:26.231360: step 8503, loss 0.600353, acc 0.953125, prec 0.114732, recall 0.794228
2017-12-10T05:50:26.507620: step 8504, loss 0.197182, acc 0.953125, prec 0.114728, recall 0.794228
2017-12-10T05:50:26.775354: step 8505, loss 0.689899, acc 0.9375, prec 0.114722, recall 0.794228
2017-12-10T05:50:27.043332: step 8506, loss 0.371683, acc 0.96875, prec 0.114741, recall 0.794262
2017-12-10T05:50:27.311938: step 8507, loss 0.198381, acc 0.9375, prec 0.114756, recall 0.794295
2017-12-10T05:50:27.583877: step 8508, loss 0.515282, acc 0.921875, prec 0.114749, recall 0.794295
2017-12-10T05:50:27.849334: step 8509, loss 0.0620505, acc 0.984375, prec 0.114748, recall 0.794295
2017-12-10T05:50:28.120870: step 8510, loss 0.183077, acc 0.984375, prec 0.114747, recall 0.794295
2017-12-10T05:50:28.389155: step 8511, loss 0.151763, acc 0.984375, prec 0.114756, recall 0.794312
2017-12-10T05:50:28.653088: step 8512, loss 0.0330164, acc 0.96875, prec 0.114753, recall 0.794312
2017-12-10T05:50:28.918206: step 8513, loss 0.33177, acc 0.9375, prec 0.114758, recall 0.794329
2017-12-10T05:50:29.183655: step 8514, loss 0.330051, acc 0.96875, prec 0.114766, recall 0.794345
2017-12-10T05:50:29.458627: step 8515, loss 0.114071, acc 0.984375, prec 0.114764, recall 0.794345
2017-12-10T05:50:29.727450: step 8516, loss 0.215088, acc 0.96875, prec 0.114782, recall 0.794379
2017-12-10T05:50:29.999203: step 8517, loss 0.124697, acc 0.96875, prec 0.11478, recall 0.794379
2017-12-10T05:50:30.264047: step 8518, loss 0.025841, acc 1, prec 0.11479, recall 0.794396
2017-12-10T05:50:30.541888: step 8519, loss 0.480108, acc 0.984375, prec 0.114789, recall 0.794396
2017-12-10T05:50:30.808738: step 8520, loss 0.197887, acc 0.984375, prec 0.114787, recall 0.794396
2017-12-10T05:50:31.077599: step 8521, loss 0.245344, acc 0.96875, prec 0.114806, recall 0.794429
2017-12-10T05:50:31.341723: step 8522, loss 0.0171658, acc 0.984375, prec 0.114804, recall 0.794429
2017-12-10T05:50:31.612375: step 8523, loss 0.000952458, acc 1, prec 0.114825, recall 0.794463
2017-12-10T05:50:31.877468: step 8524, loss 0.842627, acc 0.953125, prec 0.114821, recall 0.794463
2017-12-10T05:50:32.139076: step 8525, loss 0.0740507, acc 0.984375, prec 0.114841, recall 0.794496
2017-12-10T05:50:32.406552: step 8526, loss 0.293575, acc 0.96875, prec 0.114848, recall 0.794513
2017-12-10T05:50:32.679327: step 8527, loss 0.000325059, acc 1, prec 0.114869, recall 0.794546
2017-12-10T05:50:32.945444: step 8528, loss 0.266981, acc 0.96875, prec 0.114866, recall 0.794546
2017-12-10T05:50:33.209795: step 8529, loss 0.0405583, acc 0.984375, prec 0.114886, recall 0.79458
2017-12-10T05:50:33.476534: step 8530, loss 0.156794, acc 0.984375, prec 0.114885, recall 0.79458
2017-12-10T05:50:33.746014: step 8531, loss 0.0557654, acc 0.96875, prec 0.114892, recall 0.794596
2017-12-10T05:50:34.012907: step 8532, loss 0.0942791, acc 0.984375, prec 0.114922, recall 0.794647
2017-12-10T05:50:34.287431: step 8533, loss 3.19615e-06, acc 1, prec 0.114922, recall 0.794647
2017-12-10T05:50:34.547595: step 8534, loss 0.0860747, acc 0.984375, prec 0.114931, recall 0.794663
2017-12-10T05:50:34.815044: step 8535, loss 0.000165124, acc 1, prec 0.114931, recall 0.794663
2017-12-10T05:50:35.079388: step 8536, loss 0.120603, acc 0.984375, prec 0.114971, recall 0.79473
2017-12-10T05:50:35.343674: step 8537, loss 0.0104766, acc 1, prec 0.114992, recall 0.794763
2017-12-10T05:50:35.618801: step 8538, loss 8.7471e-05, acc 1, prec 0.114992, recall 0.794763
2017-12-10T05:50:35.885636: step 8539, loss 0.287654, acc 0.96875, prec 0.11499, recall 0.794763
2017-12-10T05:50:36.147478: step 8540, loss 0.0051698, acc 1, prec 0.11499, recall 0.794763
2017-12-10T05:50:36.420068: step 8541, loss 0.00138556, acc 1, prec 0.11499, recall 0.794763
2017-12-10T05:50:36.686449: step 8542, loss 0.279685, acc 0.96875, prec 0.114997, recall 0.79478
2017-12-10T05:50:36.951686: step 8543, loss 0.000176098, acc 1, prec 0.115018, recall 0.794813
2017-12-10T05:50:37.220197: step 8544, loss 0.166311, acc 0.96875, prec 0.115026, recall 0.79483
2017-12-10T05:50:37.493608: step 8545, loss 0.000169678, acc 1, prec 0.115026, recall 0.79483
2017-12-10T05:50:37.755796: step 8546, loss 0.0487428, acc 1, prec 0.115036, recall 0.794847
2017-12-10T05:50:38.022058: step 8547, loss 0.0122043, acc 1, prec 0.115047, recall 0.794863
2017-12-10T05:50:38.289909: step 8548, loss 3.70827e-06, acc 1, prec 0.115047, recall 0.794863
2017-12-10T05:50:38.563302: step 8549, loss 0.000116278, acc 1, prec 0.115047, recall 0.794863
2017-12-10T05:50:38.825456: step 8550, loss 0.113134, acc 0.96875, prec 0.115044, recall 0.794863
2017-12-10T05:50:39.087957: step 8551, loss 0.198675, acc 0.984375, prec 0.115043, recall 0.794863
2017-12-10T05:50:39.363997: step 8552, loss 0.194649, acc 0.984375, prec 0.115072, recall 0.794913
2017-12-10T05:50:39.628575: step 8553, loss 0.282768, acc 1, prec 0.115093, recall 0.794947
2017-12-10T05:50:39.898916: step 8554, loss 0.133749, acc 0.984375, prec 0.115102, recall 0.794963
2017-12-10T05:50:40.166358: step 8555, loss 0.000188023, acc 1, prec 0.115113, recall 0.79498
2017-12-10T05:50:40.426434: step 8556, loss 0.0611238, acc 0.96875, prec 0.11512, recall 0.794997
2017-12-10T05:50:40.688520: step 8557, loss 0.00544398, acc 1, prec 0.11512, recall 0.794997
2017-12-10T05:50:40.954603: step 8558, loss 4.80313, acc 0.984375, prec 0.115131, recall 0.794949
2017-12-10T05:50:41.223451: step 8559, loss 0.000333984, acc 1, prec 0.115141, recall 0.794966
2017-12-10T05:50:41.483621: step 8560, loss 0.259525, acc 0.984375, prec 0.11515, recall 0.794982
2017-12-10T05:50:41.765721: step 8561, loss 0.14326, acc 0.984375, prec 0.11517, recall 0.795015
2017-12-10T05:50:42.029303: step 8562, loss 0.115529, acc 0.96875, prec 0.115177, recall 0.795032
2017-12-10T05:50:42.313640: step 8563, loss 0.0715608, acc 0.984375, prec 0.115197, recall 0.795065
2017-12-10T05:50:42.583057: step 8564, loss 0.0414247, acc 0.96875, prec 0.115205, recall 0.795082
2017-12-10T05:50:42.847449: step 8565, loss 0.0936213, acc 0.984375, prec 0.115214, recall 0.795099
2017-12-10T05:50:43.112248: step 8566, loss 0.127856, acc 0.9375, prec 0.115208, recall 0.795099
2017-12-10T05:50:43.384186: step 8567, loss 0.362402, acc 0.953125, prec 0.115225, recall 0.795132
2017-12-10T05:50:43.654529: step 8568, loss 0.163838, acc 0.96875, prec 0.115222, recall 0.795132
2017-12-10T05:50:43.917233: step 8569, loss 0.000657046, acc 1, prec 0.115222, recall 0.795132
2017-12-10T05:50:44.192520: step 8570, loss 0.545945, acc 0.921875, prec 0.115216, recall 0.795132
2017-12-10T05:50:44.465560: step 8571, loss 0.766511, acc 0.953125, prec 0.115222, recall 0.795148
2017-12-10T05:50:44.738473: step 8572, loss 0.486442, acc 0.9375, prec 0.115216, recall 0.795148
2017-12-10T05:50:45.011600: step 8573, loss 0.0117128, acc 1, prec 0.115216, recall 0.795148
2017-12-10T05:50:45.280258: step 8574, loss 0.302479, acc 0.9375, prec 0.115211, recall 0.795148
2017-12-10T05:50:45.548644: step 8575, loss 0.185934, acc 0.9375, prec 0.115206, recall 0.795148
2017-12-10T05:50:45.816246: step 8576, loss 0.261653, acc 0.96875, prec 0.115213, recall 0.795165
2017-12-10T05:50:46.079340: step 8577, loss 0.906699, acc 0.890625, prec 0.115225, recall 0.795198
2017-12-10T05:50:46.348879: step 8578, loss 0.63628, acc 0.890625, prec 0.115215, recall 0.795198
2017-12-10T05:50:46.614845: step 8579, loss 0.270779, acc 0.9375, prec 0.11521, recall 0.795198
2017-12-10T05:50:46.891587: step 8580, loss 0.32055, acc 0.953125, prec 0.115206, recall 0.795198
2017-12-10T05:50:47.165285: step 8581, loss 0.353496, acc 0.921875, prec 0.11522, recall 0.795232
2017-12-10T05:50:47.434217: step 8582, loss 0.429808, acc 0.921875, prec 0.115213, recall 0.795232
2017-12-10T05:50:47.707413: step 8583, loss 0.563674, acc 0.96875, prec 0.11521, recall 0.795232
2017-12-10T05:50:47.973590: step 8584, loss 0.112825, acc 0.96875, prec 0.115207, recall 0.795232
2017-12-10T05:50:48.240235: step 8585, loss 0.119973, acc 0.953125, prec 0.115214, recall 0.795248
2017-12-10T05:50:48.516920: step 8586, loss 0.163858, acc 0.96875, prec 0.115211, recall 0.795248
2017-12-10T05:50:48.790744: step 8587, loss 0.695728, acc 0.921875, prec 0.115225, recall 0.795281
2017-12-10T05:50:49.056342: step 8588, loss 0.708756, acc 0.9375, prec 0.11523, recall 0.795298
2017-12-10T05:50:49.322848: step 8589, loss 0.016432, acc 1, prec 0.11523, recall 0.795298
2017-12-10T05:50:49.583200: step 8590, loss 0.245556, acc 0.953125, prec 0.115226, recall 0.795298
2017-12-10T05:50:49.849867: step 8591, loss 0.212597, acc 0.953125, prec 0.115243, recall 0.795331
2017-12-10T05:50:50.126112: step 8592, loss 0.303045, acc 0.96875, prec 0.115261, recall 0.795364
2017-12-10T05:50:50.387933: step 8593, loss 0.375232, acc 0.9375, prec 0.115266, recall 0.795381
2017-12-10T05:50:50.650158: step 8594, loss 0.167639, acc 0.96875, prec 0.115263, recall 0.795381
2017-12-10T05:50:50.918957: step 8595, loss 0.394051, acc 0.953125, prec 0.11528, recall 0.795414
2017-12-10T05:50:51.185219: step 8596, loss 0.0200555, acc 0.984375, prec 0.115278, recall 0.795414
2017-12-10T05:50:51.460739: step 8597, loss 0.0992658, acc 0.96875, prec 0.115297, recall 0.795447
2017-12-10T05:50:51.727009: step 8598, loss 0.013636, acc 1, prec 0.115297, recall 0.795447
2017-12-10T05:50:51.999886: step 8599, loss 0.000235318, acc 1, prec 0.115297, recall 0.795447
2017-12-10T05:50:52.265750: step 8600, loss 0.000210115, acc 1, prec 0.115297, recall 0.795447
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-8600

2017-12-10T05:50:53.480448: step 8601, loss 0.576577, acc 0.9375, prec 0.115302, recall 0.795464
2017-12-10T05:50:53.758600: step 8602, loss 0.0647041, acc 0.984375, prec 0.1153, recall 0.795464
2017-12-10T05:50:54.030156: step 8603, loss 0.0126913, acc 0.984375, prec 0.115299, recall 0.795464
2017-12-10T05:50:54.297855: step 8604, loss 0.0127278, acc 0.984375, prec 0.115297, recall 0.795464
2017-12-10T05:50:54.561695: step 8605, loss 0.000377351, acc 1, prec 0.115297, recall 0.795464
2017-12-10T05:50:54.821583: step 8606, loss 0.0531766, acc 0.984375, prec 0.115306, recall 0.79548
2017-12-10T05:50:55.087025: step 8607, loss 0.30154, acc 0.984375, prec 0.115326, recall 0.795513
2017-12-10T05:50:55.356136: step 8608, loss 0.00908681, acc 1, prec 0.115347, recall 0.795547
2017-12-10T05:50:55.625815: step 8609, loss 0.00706912, acc 1, prec 0.115347, recall 0.795547
2017-12-10T05:50:55.893464: step 8610, loss 0.00655512, acc 1, prec 0.115357, recall 0.795563
2017-12-10T05:50:56.164005: step 8611, loss 0.0441918, acc 0.984375, prec 0.115366, recall 0.79558
2017-12-10T05:50:56.424434: step 8612, loss 0.000179414, acc 1, prec 0.115376, recall 0.795596
2017-12-10T05:50:56.683109: step 8613, loss 0.0705835, acc 0.984375, prec 0.115386, recall 0.795613
2017-12-10T05:50:56.947307: step 8614, loss 0.186371, acc 0.984375, prec 0.115384, recall 0.795613
2017-12-10T05:50:57.217830: step 8615, loss 0.00164536, acc 1, prec 0.115395, recall 0.795629
2017-12-10T05:50:57.496911: step 8616, loss 0.694473, acc 0.984375, prec 0.115404, recall 0.795646
2017-12-10T05:50:57.761734: step 8617, loss 0.276102, acc 0.984375, prec 0.115413, recall 0.795662
2017-12-10T05:50:58.024651: step 8618, loss 0.00303595, acc 1, prec 0.115413, recall 0.795662
2017-12-10T05:50:58.293579: step 8619, loss 2.54257, acc 0.984375, prec 0.115423, recall 0.795615
2017-12-10T05:50:58.571141: step 8620, loss 0.185376, acc 0.984375, prec 0.115463, recall 0.795681
2017-12-10T05:50:58.839249: step 8621, loss 0.019445, acc 0.984375, prec 0.115483, recall 0.795714
2017-12-10T05:50:59.106713: step 8622, loss 5.85514e-05, acc 1, prec 0.115493, recall 0.79573
2017-12-10T05:50:59.368977: step 8623, loss 0.237896, acc 0.984375, prec 0.115492, recall 0.79573
2017-12-10T05:50:59.639949: step 8624, loss 0.216723, acc 0.96875, prec 0.115489, recall 0.79573
2017-12-10T05:50:59.916381: step 8625, loss 0.090832, acc 0.96875, prec 0.115497, recall 0.795747
2017-12-10T05:51:00.186921: step 8626, loss 0.00435147, acc 1, prec 0.115497, recall 0.795747
2017-12-10T05:51:00.455913: step 8627, loss 0.372449, acc 0.953125, prec 0.115492, recall 0.795747
2017-12-10T05:51:00.734203: step 8628, loss 0.11172, acc 0.96875, prec 0.1155, recall 0.795763
2017-12-10T05:51:01.006490: step 8629, loss 0.509168, acc 0.90625, prec 0.115523, recall 0.795813
2017-12-10T05:51:01.276068: step 8630, loss 0.156836, acc 0.96875, prec 0.11552, recall 0.795813
2017-12-10T05:51:01.544419: step 8631, loss 0.17303, acc 0.953125, prec 0.115537, recall 0.795846
2017-12-10T05:51:01.813995: step 8632, loss 0.0101239, acc 1, prec 0.115537, recall 0.795846
2017-12-10T05:51:02.081706: step 8633, loss 0.2339, acc 0.921875, prec 0.115541, recall 0.795862
2017-12-10T05:51:02.346624: step 8634, loss 0.295503, acc 0.953125, prec 0.115537, recall 0.795862
2017-12-10T05:51:02.614775: step 8635, loss 0.116681, acc 0.96875, prec 0.115544, recall 0.795879
2017-12-10T05:51:02.878805: step 8636, loss 0.687865, acc 0.921875, prec 0.115569, recall 0.795928
2017-12-10T05:51:03.144691: step 8637, loss 0.236721, acc 0.953125, prec 0.115575, recall 0.795945
2017-12-10T05:51:03.409253: step 8638, loss 0.0709065, acc 0.96875, prec 0.115583, recall 0.795961
2017-12-10T05:51:03.674041: step 8639, loss 0.0740787, acc 0.96875, prec 0.115601, recall 0.795994
2017-12-10T05:51:03.940794: step 8640, loss 0.0733371, acc 0.96875, prec 0.115608, recall 0.796011
2017-12-10T05:51:04.202552: step 8641, loss 0.0097578, acc 1, prec 0.115608, recall 0.796011
2017-12-10T05:51:04.475610: step 8642, loss 0.0030318, acc 1, prec 0.115608, recall 0.796011
2017-12-10T05:51:04.738862: step 8643, loss 0.213577, acc 0.96875, prec 0.115606, recall 0.796011
2017-12-10T05:51:05.005668: step 8644, loss 0.0195979, acc 1, prec 0.115647, recall 0.796077
2017-12-10T05:51:05.276434: step 8645, loss 0.0218502, acc 0.984375, prec 0.115667, recall 0.796109
2017-12-10T05:51:05.552082: step 8646, loss 0.131046, acc 0.984375, prec 0.115686, recall 0.796142
2017-12-10T05:51:05.827916: step 8647, loss 0.214781, acc 1, prec 0.115696, recall 0.796159
2017-12-10T05:51:06.095643: step 8648, loss 0.0503834, acc 0.96875, prec 0.115704, recall 0.796175
2017-12-10T05:51:06.360545: step 8649, loss 0.0969135, acc 0.984375, prec 0.115713, recall 0.796192
2017-12-10T05:51:06.632852: step 8650, loss 0.144809, acc 0.984375, prec 0.115712, recall 0.796192
2017-12-10T05:51:06.899472: step 8651, loss 0.286877, acc 0.96875, prec 0.11574, recall 0.796241
2017-12-10T05:51:07.169306: step 8652, loss 0.0292182, acc 0.984375, prec 0.115739, recall 0.796241
2017-12-10T05:51:07.441946: step 8653, loss 0.0163185, acc 0.984375, prec 0.115748, recall 0.796257
2017-12-10T05:51:07.715458: step 8654, loss 0.150978, acc 0.96875, prec 0.115745, recall 0.796257
2017-12-10T05:51:07.980702: step 8655, loss 1.07061, acc 0.96875, prec 0.115753, recall 0.796274
2017-12-10T05:51:08.252145: step 8656, loss 0.168799, acc 0.96875, prec 0.115771, recall 0.796307
2017-12-10T05:51:08.521072: step 8657, loss 0.0502136, acc 0.984375, prec 0.11578, recall 0.796323
2017-12-10T05:51:08.793975: step 8658, loss 0.000592081, acc 1, prec 0.11578, recall 0.796323
2017-12-10T05:51:09.057559: step 8659, loss 0.279476, acc 0.9375, prec 0.115785, recall 0.79634
2017-12-10T05:51:09.327366: step 8660, loss 0.00432587, acc 1, prec 0.115785, recall 0.79634
2017-12-10T05:51:09.601659: step 8661, loss 0.0394231, acc 0.984375, prec 0.115804, recall 0.796372
2017-12-10T05:51:09.870695: step 8662, loss 0.113505, acc 0.96875, prec 0.115801, recall 0.796372
2017-12-10T05:51:10.140272: step 8663, loss 0.00820734, acc 1, prec 0.115801, recall 0.796372
2017-12-10T05:51:10.407143: step 8664, loss 0.000144575, acc 1, prec 0.115812, recall 0.796389
2017-12-10T05:51:10.669251: step 8665, loss 0.00880298, acc 1, prec 0.115822, recall 0.796405
2017-12-10T05:51:10.939328: step 8666, loss 1.23255, acc 0.984375, prec 0.115832, recall 0.796357
2017-12-10T05:51:11.218247: step 8667, loss 0.0368229, acc 0.984375, prec 0.115831, recall 0.796357
2017-12-10T05:51:11.489629: step 8668, loss 0.262452, acc 0.9375, prec 0.115836, recall 0.796374
2017-12-10T05:51:11.752549: step 8669, loss 0.28249, acc 0.953125, prec 0.115842, recall 0.79639
2017-12-10T05:51:12.018463: step 8670, loss 0.0461752, acc 0.96875, prec 0.11585, recall 0.796407
2017-12-10T05:51:12.302555: step 8671, loss 0.0823992, acc 0.984375, prec 0.115859, recall 0.796423
2017-12-10T05:51:12.576818: step 8672, loss 0.0673414, acc 0.984375, prec 0.115868, recall 0.79644
2017-12-10T05:51:12.855719: step 8673, loss 0.291381, acc 0.9375, prec 0.115862, recall 0.79644
2017-12-10T05:51:13.123656: step 8674, loss 0.284078, acc 0.953125, prec 0.115869, recall 0.796456
2017-12-10T05:51:13.392706: step 8675, loss 3.1389, acc 0.984375, prec 0.115869, recall 0.796392
2017-12-10T05:51:13.660208: step 8676, loss 0.000700121, acc 1, prec 0.115869, recall 0.796392
2017-12-10T05:51:13.925890: step 8677, loss 0.203401, acc 0.96875, prec 0.115876, recall 0.796408
2017-12-10T05:51:14.901648: step 8678, loss 0.613428, acc 0.90625, prec 0.115899, recall 0.796457
2017-12-10T05:51:15.268351: step 8679, loss 0.484607, acc 0.90625, prec 0.115891, recall 0.796457
2017-12-10T05:51:15.542220: step 8680, loss 0.958198, acc 0.921875, prec 0.115884, recall 0.796457
2017-12-10T05:51:15.846795: step 8681, loss 0.840542, acc 0.90625, prec 0.115876, recall 0.796457
2017-12-10T05:51:16.582690: step 8682, loss 0.647758, acc 0.84375, prec 0.115894, recall 0.796506
2017-12-10T05:51:17.351870: step 8683, loss 0.574513, acc 0.921875, prec 0.115918, recall 0.796556
2017-12-10T05:51:18.035525: step 8684, loss 0.565348, acc 0.921875, prec 0.115911, recall 0.796556
2017-12-10T05:51:18.766033: step 8685, loss 0.418338, acc 0.890625, prec 0.115902, recall 0.796556
2017-12-10T05:51:19.493501: step 8686, loss 0.562243, acc 0.890625, prec 0.115913, recall 0.796588
2017-12-10T05:51:20.192349: step 8687, loss 0.628656, acc 0.890625, prec 0.115903, recall 0.796588
2017-12-10T05:51:21.387706: step 8688, loss 0.427204, acc 0.9375, prec 0.115929, recall 0.796637
2017-12-10T05:51:21.788682: step 8689, loss 0.242688, acc 0.96875, prec 0.115947, recall 0.79667
2017-12-10T05:51:22.067962: step 8690, loss 0.793774, acc 0.859375, prec 0.115935, recall 0.79667
2017-12-10T05:51:22.363772: step 8691, loss 0.702305, acc 0.859375, prec 0.115933, recall 0.796687
2017-12-10T05:51:22.656800: step 8692, loss 0.0030872, acc 1, prec 0.115954, recall 0.796719
2017-12-10T05:51:22.960817: step 8693, loss 0.764149, acc 0.96875, prec 0.115972, recall 0.796752
2017-12-10T05:51:23.260699: step 8694, loss 1.02104, acc 0.9375, prec 0.115976, recall 0.796768
2017-12-10T05:51:23.527187: step 8695, loss 0.291421, acc 0.921875, prec 0.11598, recall 0.796785
2017-12-10T05:51:23.798594: step 8696, loss 0.243512, acc 0.953125, prec 0.115997, recall 0.796817
2017-12-10T05:51:24.073256: step 8697, loss 0.286915, acc 0.953125, prec 0.116003, recall 0.796834
2017-12-10T05:51:24.340638: step 8698, loss 0.197174, acc 0.9375, prec 0.116008, recall 0.79685
2017-12-10T05:51:24.614844: step 8699, loss 0.0129495, acc 1, prec 0.116028, recall 0.796883
2017-12-10T05:51:24.883918: step 8700, loss 0.0102227, acc 1, prec 0.116028, recall 0.796883

Evaluation:
2017-12-10T05:51:32.632195: step 8700, loss 8.5992, acc 0.944707, prec 0.116099, recall 0.792874

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-8700

2017-12-10T05:51:33.843630: step 8701, loss 0.444617, acc 0.96875, prec 0.116117, recall 0.792906
2017-12-10T05:51:34.121071: step 8702, loss 0.177984, acc 0.984375, prec 0.116115, recall 0.792906
2017-12-10T05:51:34.390760: step 8703, loss 0.00980065, acc 1, prec 0.116115, recall 0.792906
2017-12-10T05:51:34.654750: step 8704, loss 0.225609, acc 0.96875, prec 0.116123, recall 0.792923
2017-12-10T05:51:34.923486: step 8705, loss 0.320255, acc 0.9375, prec 0.116117, recall 0.792923
2017-12-10T05:51:35.195733: step 8706, loss 0.0786106, acc 0.984375, prec 0.116126, recall 0.792939
2017-12-10T05:51:35.466605: step 8707, loss 0.00711996, acc 1, prec 0.116147, recall 0.792972
2017-12-10T05:51:35.733456: step 8708, loss 0.0500491, acc 0.96875, prec 0.116144, recall 0.792972
2017-12-10T05:51:36.001420: step 8709, loss 0.0112568, acc 1, prec 0.116165, recall 0.793005
2017-12-10T05:51:36.268063: step 8710, loss 0.735618, acc 0.953125, prec 0.116171, recall 0.793021
2017-12-10T05:51:36.529256: step 8711, loss 0.0054452, acc 1, prec 0.116192, recall 0.793054
2017-12-10T05:51:36.796828: step 8712, loss 0.297236, acc 0.953125, prec 0.116218, recall 0.793103
2017-12-10T05:51:37.071168: step 8713, loss 0.00399824, acc 1, prec 0.116229, recall 0.79312
2017-12-10T05:51:37.337107: step 8714, loss 0.156008, acc 0.984375, prec 0.116237, recall 0.793136
2017-12-10T05:51:37.610678: step 8715, loss 0.0944926, acc 0.984375, prec 0.116246, recall 0.793153
2017-12-10T05:51:37.872187: step 8716, loss 0.16689, acc 0.9375, prec 0.116251, recall 0.793169
2017-12-10T05:51:38.138631: step 8717, loss 0.000962827, acc 1, prec 0.116262, recall 0.793185
2017-12-10T05:51:38.410686: step 8718, loss 0.0283336, acc 0.984375, prec 0.11627, recall 0.793202
2017-12-10T05:51:38.681110: step 8719, loss 0.0172171, acc 0.984375, prec 0.116269, recall 0.793202
2017-12-10T05:51:38.959003: step 8720, loss 0.0167214, acc 0.984375, prec 0.116278, recall 0.793218
2017-12-10T05:51:39.230253: step 8721, loss 0.602121, acc 0.984375, prec 0.116297, recall 0.793251
2017-12-10T05:51:39.498388: step 8722, loss 0.00436066, acc 1, prec 0.116297, recall 0.793251
2017-12-10T05:51:39.764843: step 8723, loss 0.167579, acc 0.953125, prec 0.116303, recall 0.793267
2017-12-10T05:51:40.050863: step 8724, loss 0.00644817, acc 1, prec 0.116314, recall 0.793284
2017-12-10T05:51:40.321187: step 8725, loss 0.0171499, acc 0.984375, prec 0.116333, recall 0.793316
2017-12-10T05:51:40.587509: step 8726, loss 0.109634, acc 0.984375, prec 0.116331, recall 0.793316
2017-12-10T05:51:40.857374: step 8727, loss 0.480503, acc 0.96875, prec 0.116349, recall 0.793349
2017-12-10T05:51:41.124990: step 8728, loss 0.138117, acc 0.96875, prec 0.116357, recall 0.793366
2017-12-10T05:51:41.390606: step 8729, loss 2.846e-05, acc 1, prec 0.116357, recall 0.793366
2017-12-10T05:51:41.648744: step 8730, loss 0.000695727, acc 1, prec 0.116377, recall 0.793398
2017-12-10T05:51:41.913657: step 8731, loss 1.60286, acc 0.984375, prec 0.116377, recall 0.793335
2017-12-10T05:51:42.188301: step 8732, loss 0.148624, acc 0.984375, prec 0.116376, recall 0.793335
2017-12-10T05:51:42.469228: step 8733, loss 0.445518, acc 0.96875, prec 0.116394, recall 0.793368
2017-12-10T05:51:42.736243: step 8734, loss 0.46922, acc 0.984375, prec 0.116403, recall 0.793384
2017-12-10T05:51:43.003986: step 8735, loss 0.672775, acc 0.953125, prec 0.11645, recall 0.793466
2017-12-10T05:51:43.269688: step 8736, loss 0.120088, acc 0.984375, prec 0.116449, recall 0.793466
2017-12-10T05:51:43.544341: step 8737, loss 0.559985, acc 0.953125, prec 0.116445, recall 0.793466
2017-12-10T05:51:43.813335: step 8738, loss 0.018984, acc 1, prec 0.116445, recall 0.793466
2017-12-10T05:51:44.081045: step 8739, loss 0.142148, acc 0.96875, prec 0.116473, recall 0.793515
2017-12-10T05:51:44.350443: step 8740, loss 0.45563, acc 0.9375, prec 0.116467, recall 0.793515
2017-12-10T05:51:44.619944: step 8741, loss 0.0248244, acc 0.984375, prec 0.116466, recall 0.793515
2017-12-10T05:51:44.886946: step 8742, loss 0.88987, acc 0.9375, prec 0.116481, recall 0.793548
2017-12-10T05:51:45.154247: step 8743, loss 0.698435, acc 0.921875, prec 0.116484, recall 0.793564
2017-12-10T05:51:45.417462: step 8744, loss 0.356377, acc 0.96875, prec 0.116482, recall 0.793564
2017-12-10T05:51:45.685307: step 8745, loss 0.00129965, acc 1, prec 0.116492, recall 0.793581
2017-12-10T05:51:45.949510: step 8746, loss 0.798842, acc 0.921875, prec 0.116496, recall 0.793597
2017-12-10T05:51:46.226305: step 8747, loss 0.0257356, acc 0.984375, prec 0.116494, recall 0.793597
2017-12-10T05:51:46.492422: step 8748, loss 0.0181898, acc 0.984375, prec 0.116493, recall 0.793597
2017-12-10T05:51:46.765746: step 8749, loss 0.444491, acc 0.984375, prec 0.116502, recall 0.793613
2017-12-10T05:51:47.036196: step 8750, loss 6.35725, acc 0.9375, prec 0.116498, recall 0.79355
2017-12-10T05:51:47.303198: step 8751, loss 0.0275502, acc 0.984375, prec 0.116496, recall 0.79355
2017-12-10T05:51:47.570615: step 8752, loss 0.466035, acc 0.921875, prec 0.1165, recall 0.793567
2017-12-10T05:51:47.837748: step 8753, loss 0.452107, acc 0.953125, prec 0.116506, recall 0.793583
2017-12-10T05:51:48.105878: step 8754, loss 0.30165, acc 0.9375, prec 0.116511, recall 0.793599
2017-12-10T05:51:48.371341: step 8755, loss 0.133987, acc 0.953125, prec 0.116507, recall 0.793599
2017-12-10T05:51:48.637051: step 8756, loss 0.604416, acc 0.890625, prec 0.116508, recall 0.793616
2017-12-10T05:51:48.905647: step 8757, loss 0.362391, acc 0.921875, prec 0.116511, recall 0.793632
2017-12-10T05:51:49.172491: step 8758, loss 0.433475, acc 0.90625, prec 0.116523, recall 0.793665
2017-12-10T05:51:49.444016: step 8759, loss 0.619632, acc 0.890625, prec 0.116524, recall 0.793681
2017-12-10T05:51:49.716210: step 8760, loss 0.0658446, acc 0.953125, prec 0.11652, recall 0.793681
2017-12-10T05:51:49.983583: step 8761, loss 0.284197, acc 0.921875, prec 0.116513, recall 0.793681
2017-12-10T05:51:50.259355: step 8762, loss 0.559574, acc 0.921875, prec 0.116507, recall 0.793681
2017-12-10T05:51:50.530881: step 8763, loss 0.634608, acc 0.890625, prec 0.116497, recall 0.793681
2017-12-10T05:51:50.799118: step 8764, loss 0.146079, acc 0.984375, prec 0.116496, recall 0.793681
2017-12-10T05:51:51.062696: step 8765, loss 0.946787, acc 0.890625, prec 0.116486, recall 0.793681
2017-12-10T05:51:51.331033: step 8766, loss 0.0425047, acc 0.96875, prec 0.116484, recall 0.793681
2017-12-10T05:51:51.607140: step 8767, loss 0.67265, acc 0.90625, prec 0.116486, recall 0.793697
2017-12-10T05:51:51.876243: step 8768, loss 0.0809884, acc 0.96875, prec 0.116483, recall 0.793697
2017-12-10T05:51:52.151832: step 8769, loss 0.131766, acc 0.96875, prec 0.11648, recall 0.793697
2017-12-10T05:51:52.417067: step 8770, loss 0.489749, acc 0.921875, prec 0.116484, recall 0.793713
2017-12-10T05:51:52.680216: step 8771, loss 0.155227, acc 0.921875, prec 0.116487, recall 0.79373
2017-12-10T05:51:52.945952: step 8772, loss 0.982619, acc 0.921875, prec 0.116491, recall 0.793746
2017-12-10T05:51:53.215594: step 8773, loss 0.0279523, acc 0.984375, prec 0.11649, recall 0.793746
2017-12-10T05:51:53.485568: step 8774, loss 0.00279771, acc 1, prec 0.11651, recall 0.793779
2017-12-10T05:51:53.752511: step 8775, loss 0.108931, acc 0.953125, prec 0.116506, recall 0.793779
2017-12-10T05:51:54.016512: step 8776, loss 0.14074, acc 0.984375, prec 0.116515, recall 0.793795
2017-12-10T05:51:54.287623: step 8777, loss 0.0866502, acc 0.984375, prec 0.116514, recall 0.793795
2017-12-10T05:51:54.558671: step 8778, loss 0.383693, acc 0.96875, prec 0.116552, recall 0.79386
2017-12-10T05:51:54.833707: step 8779, loss 0.290358, acc 0.96875, prec 0.116549, recall 0.79386
2017-12-10T05:51:55.107075: step 8780, loss 0.028157, acc 0.984375, prec 0.116548, recall 0.79386
2017-12-10T05:51:55.378626: step 8781, loss 0.000780761, acc 1, prec 0.116558, recall 0.793876
2017-12-10T05:51:55.644747: step 8782, loss 0.254226, acc 0.96875, prec 0.116586, recall 0.793925
2017-12-10T05:51:55.907377: step 8783, loss 0.10272, acc 0.984375, prec 0.116595, recall 0.793941
2017-12-10T05:51:56.170652: step 8784, loss 0.00236006, acc 1, prec 0.116615, recall 0.793974
2017-12-10T05:51:56.433264: step 8785, loss 0.444342, acc 0.96875, prec 0.116623, recall 0.79399
2017-12-10T05:51:56.701955: step 8786, loss 0.000949493, acc 1, prec 0.116623, recall 0.79399
2017-12-10T05:51:56.965903: step 8787, loss 0.000433293, acc 1, prec 0.116633, recall 0.794006
2017-12-10T05:51:57.234829: step 8788, loss 0.0394071, acc 0.984375, prec 0.116642, recall 0.794023
2017-12-10T05:51:57.527436: step 8789, loss 0.00259093, acc 1, prec 0.116642, recall 0.794023
2017-12-10T05:51:57.788703: step 8790, loss 0.27714, acc 0.953125, prec 0.116638, recall 0.794023
2017-12-10T05:51:58.062369: step 8791, loss 0.083479, acc 0.984375, prec 0.116637, recall 0.794023
2017-12-10T05:51:58.327946: step 8792, loss 0.000172728, acc 1, prec 0.116637, recall 0.794023
2017-12-10T05:51:58.593272: step 8793, loss 0.0160296, acc 0.984375, prec 0.116645, recall 0.794039
2017-12-10T05:51:58.870382: step 8794, loss 0.446426, acc 0.953125, prec 0.116672, recall 0.794088
2017-12-10T05:51:59.132458: step 8795, loss 0.000974348, acc 1, prec 0.116693, recall 0.79412
2017-12-10T05:51:59.394777: step 8796, loss 0.190836, acc 0.96875, prec 0.1167, recall 0.794136
2017-12-10T05:51:59.667484: step 8797, loss 0.00129668, acc 1, prec 0.1167, recall 0.794136
2017-12-10T05:51:59.941356: step 8798, loss 0.15209, acc 0.984375, prec 0.116699, recall 0.794136
2017-12-10T05:52:00.212113: step 8799, loss 0.00106806, acc 1, prec 0.116709, recall 0.794152
2017-12-10T05:52:00.481859: step 8800, loss 0.000447862, acc 1, prec 0.116709, recall 0.794152
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-8800

2017-12-10T05:52:01.738938: step 8801, loss 3.24837, acc 0.96875, prec 0.116728, recall 0.794122
2017-12-10T05:52:02.019719: step 8802, loss 0.141884, acc 0.984375, prec 0.116727, recall 0.794122
2017-12-10T05:52:02.282256: step 8803, loss 0.00395859, acc 1, prec 0.116727, recall 0.794122
2017-12-10T05:52:02.551675: step 8804, loss 0.000797023, acc 1, prec 0.116737, recall 0.794138
2017-12-10T05:52:02.818361: step 8805, loss 0.369749, acc 0.9375, prec 0.116732, recall 0.794138
2017-12-10T05:52:03.092352: step 8806, loss 0.00374892, acc 1, prec 0.116742, recall 0.794155
2017-12-10T05:52:03.355248: step 8807, loss 0.0309816, acc 0.984375, prec 0.11674, recall 0.794155
2017-12-10T05:52:03.629266: step 8808, loss 0.159171, acc 0.96875, prec 0.116748, recall 0.794171
2017-12-10T05:52:03.898356: step 8809, loss 0.229728, acc 0.96875, prec 0.116766, recall 0.794203
2017-12-10T05:52:04.162899: step 8810, loss 0.0510806, acc 0.96875, prec 0.116773, recall 0.79422
2017-12-10T05:52:04.433076: step 8811, loss 0.195086, acc 0.984375, prec 0.116782, recall 0.794236
2017-12-10T05:52:04.703375: step 8812, loss 0.225238, acc 0.953125, prec 0.116788, recall 0.794252
2017-12-10T05:52:04.979468: step 8813, loss 0.432559, acc 0.90625, prec 0.11678, recall 0.794252
2017-12-10T05:52:05.246491: step 8814, loss 0.489571, acc 0.90625, prec 0.116782, recall 0.794268
2017-12-10T05:52:05.514063: step 8815, loss 0.509552, acc 0.90625, prec 0.116784, recall 0.794284
2017-12-10T05:52:05.788083: step 8816, loss 0.452873, acc 0.9375, prec 0.116779, recall 0.794284
2017-12-10T05:52:06.057689: step 8817, loss 0.366519, acc 0.953125, prec 0.116785, recall 0.794301
2017-12-10T05:52:06.325185: step 8818, loss 0.81003, acc 0.875, prec 0.116785, recall 0.794317
2017-12-10T05:52:06.597741: step 8819, loss 0.64896, acc 0.90625, prec 0.116787, recall 0.794333
2017-12-10T05:52:06.863398: step 8820, loss 1.17018, acc 0.890625, prec 0.116777, recall 0.794333
2017-12-10T05:52:07.128025: step 8821, loss 0.404182, acc 0.921875, prec 0.116811, recall 0.794398
2017-12-10T05:52:07.396002: step 8822, loss 0.137858, acc 0.9375, prec 0.116816, recall 0.794414
2017-12-10T05:52:07.663772: step 8823, loss 0.338051, acc 0.96875, prec 0.116813, recall 0.794414
2017-12-10T05:52:07.929901: step 8824, loss 0.65345, acc 0.953125, prec 0.11683, recall 0.794446
2017-12-10T05:52:08.193876: step 8825, loss 0.395595, acc 0.953125, prec 0.116826, recall 0.794446
2017-12-10T05:52:08.469889: step 8826, loss 0.916028, acc 0.9375, prec 0.116831, recall 0.794462
2017-12-10T05:52:08.736510: step 8827, loss 0.247935, acc 0.953125, prec 0.116837, recall 0.794479
2017-12-10T05:52:09.011288: step 8828, loss 0.176592, acc 0.96875, prec 0.116834, recall 0.794479
2017-12-10T05:52:09.277578: step 8829, loss 0.00563738, acc 1, prec 0.116844, recall 0.794495
2017-12-10T05:52:09.551876: step 8830, loss 0.211491, acc 0.96875, prec 0.116852, recall 0.794511
2017-12-10T05:52:09.818337: step 8831, loss 0.558618, acc 0.953125, prec 0.116858, recall 0.794527
2017-12-10T05:52:10.085057: step 8832, loss 0.234236, acc 0.953125, prec 0.116874, recall 0.794559
2017-12-10T05:52:10.348834: step 8833, loss 0.0153362, acc 1, prec 0.116874, recall 0.794559
2017-12-10T05:52:10.622152: step 8834, loss 0.159009, acc 0.96875, prec 0.116912, recall 0.794624
2017-12-10T05:52:10.889894: step 8835, loss 0.220238, acc 0.984375, prec 0.116911, recall 0.794624
2017-12-10T05:52:11.158751: step 8836, loss 0.0621327, acc 0.984375, prec 0.11691, recall 0.794624
2017-12-10T05:52:11.422509: step 8837, loss 0.0799951, acc 0.984375, prec 0.116908, recall 0.794624
2017-12-10T05:52:11.691505: step 8838, loss 0.000369104, acc 1, prec 0.116939, recall 0.794672
2017-12-10T05:52:11.962754: step 8839, loss 0.348525, acc 0.953125, prec 0.116935, recall 0.794672
2017-12-10T05:52:12.230780: step 8840, loss 7.52161e-05, acc 1, prec 0.116935, recall 0.794672
2017-12-10T05:52:12.507929: step 8841, loss 0.361034, acc 0.984375, prec 0.116934, recall 0.794672
2017-12-10T05:52:12.774483: step 8842, loss 0.114147, acc 0.984375, prec 0.116942, recall 0.794688
2017-12-10T05:52:13.038830: step 8843, loss 0.309912, acc 0.96875, prec 0.11695, recall 0.794705
2017-12-10T05:52:13.311844: step 8844, loss 0.136496, acc 0.984375, prec 0.116949, recall 0.794705
2017-12-10T05:52:13.577857: step 8845, loss 1.01571, acc 0.96875, prec 0.116946, recall 0.794705
2017-12-10T05:52:13.843647: step 8846, loss 0.453214, acc 0.953125, prec 0.116942, recall 0.794705
2017-12-10T05:52:14.113589: step 8847, loss 6.92952e-05, acc 1, prec 0.116952, recall 0.794721
2017-12-10T05:52:14.383753: step 8848, loss 0.0389194, acc 1, prec 0.116962, recall 0.794737
2017-12-10T05:52:14.656740: step 8849, loss 0.00330794, acc 1, prec 0.116983, recall 0.794769
2017-12-10T05:52:14.929929: step 8850, loss 0.248077, acc 0.984375, prec 0.116992, recall 0.794785
2017-12-10T05:52:15.204745: step 8851, loss 0.0531269, acc 0.984375, prec 0.11699, recall 0.794785
2017-12-10T05:52:15.476111: step 8852, loss 0.197269, acc 0.984375, prec 0.116999, recall 0.794801
2017-12-10T05:52:15.742190: step 8853, loss 0.208437, acc 0.984375, prec 0.117018, recall 0.794834
2017-12-10T05:52:16.011123: step 8854, loss 5.2261e-05, acc 1, prec 0.117018, recall 0.794834
2017-12-10T05:52:16.277266: step 8855, loss 2.73358, acc 0.96875, prec 0.117027, recall 0.794787
2017-12-10T05:52:16.548237: step 8856, loss 0.0203629, acc 1, prec 0.117037, recall 0.794803
2017-12-10T05:52:16.815550: step 8857, loss 0.0405746, acc 0.984375, prec 0.117036, recall 0.794803
2017-12-10T05:52:17.083650: step 8858, loss 0.00465905, acc 1, prec 0.117036, recall 0.794803
2017-12-10T05:52:17.356756: step 8859, loss 0.347059, acc 0.9375, prec 0.11703, recall 0.794803
2017-12-10T05:52:17.626883: step 8860, loss 0.0335704, acc 1, prec 0.117071, recall 0.794868
2017-12-10T05:52:18.621229: step 8861, loss 0.394363, acc 0.9375, prec 0.117076, recall 0.794884
2017-12-10T05:52:19.429639: step 8862, loss 0.0606502, acc 0.96875, prec 0.117073, recall 0.794884
2017-12-10T05:52:20.083878: step 8863, loss 0.544796, acc 0.984375, prec 0.117072, recall 0.794884
2017-12-10T05:52:20.399931: step 8864, loss 0.364623, acc 0.96875, prec 0.117079, recall 0.7949
2017-12-10T05:52:20.687307: step 8865, loss 0.122028, acc 0.953125, prec 0.117096, recall 0.794932
2017-12-10T05:52:20.966260: step 8866, loss 0.117108, acc 0.953125, prec 0.117092, recall 0.794932
2017-12-10T05:52:21.251524: step 8867, loss 0.0104438, acc 1, prec 0.117092, recall 0.794932
2017-12-10T05:52:21.534238: step 8868, loss 0.529649, acc 0.96875, prec 0.117089, recall 0.794932
2017-12-10T05:52:21.824285: step 8869, loss 0.0505592, acc 0.984375, prec 0.117098, recall 0.794948
2017-12-10T05:52:22.111095: step 8870, loss 0.0385549, acc 0.984375, prec 0.117107, recall 0.794964
2017-12-10T05:52:22.379507: step 8871, loss 0.217395, acc 0.9375, prec 0.117111, recall 0.79498
2017-12-10T05:52:22.653621: step 8872, loss 0.231344, acc 0.96875, prec 0.117119, recall 0.794997
2017-12-10T05:52:22.930605: step 8873, loss 0.49037, acc 0.9375, prec 0.117124, recall 0.795013
2017-12-10T05:52:23.203456: step 8874, loss 0.259283, acc 0.96875, prec 0.117162, recall 0.795077
2017-12-10T05:52:23.473450: step 8875, loss 0.148519, acc 0.984375, prec 0.117171, recall 0.795093
2017-12-10T05:52:23.744224: step 8876, loss 0.0862568, acc 0.96875, prec 0.117168, recall 0.795093
2017-12-10T05:52:24.010912: step 8877, loss 0.875491, acc 0.9375, prec 0.117163, recall 0.795093
2017-12-10T05:52:24.277767: step 8878, loss 0.0592236, acc 0.984375, prec 0.117171, recall 0.795109
2017-12-10T05:52:24.548991: step 8879, loss 0.0214105, acc 0.984375, prec 0.11718, recall 0.795125
2017-12-10T05:52:24.819637: step 8880, loss 0.417508, acc 0.953125, prec 0.117186, recall 0.795141
2017-12-10T05:52:25.090891: step 8881, loss 0.0101137, acc 1, prec 0.117197, recall 0.795157
2017-12-10T05:52:25.359251: step 8882, loss 0.199026, acc 0.96875, prec 0.117194, recall 0.795157
2017-12-10T05:52:25.628217: step 8883, loss 0.0789225, acc 0.96875, prec 0.117201, recall 0.795173
2017-12-10T05:52:25.895486: step 8884, loss 0.182567, acc 0.953125, prec 0.117197, recall 0.795173
2017-12-10T05:52:26.166696: step 8885, loss 0.196448, acc 0.984375, prec 0.117196, recall 0.795173
2017-12-10T05:52:26.433868: step 8886, loss 0.00377801, acc 1, prec 0.117216, recall 0.795205
2017-12-10T05:52:26.706266: step 8887, loss 0.441623, acc 0.9375, prec 0.117211, recall 0.795205
2017-12-10T05:52:26.978244: step 8888, loss 0.507344, acc 0.953125, prec 0.117227, recall 0.795237
2017-12-10T05:52:27.258968: step 8889, loss 0.0102383, acc 1, prec 0.117227, recall 0.795237
2017-12-10T05:52:27.533154: step 8890, loss 0.108472, acc 0.984375, prec 0.117226, recall 0.795237
2017-12-10T05:52:27.802342: step 8891, loss 0.585679, acc 0.9375, prec 0.117231, recall 0.795253
2017-12-10T05:52:28.068722: step 8892, loss 0.00650681, acc 1, prec 0.117241, recall 0.795269
2017-12-10T05:52:28.337019: step 8893, loss 0.585208, acc 0.953125, prec 0.117247, recall 0.795285
2017-12-10T05:52:28.601221: step 8894, loss 0.0213505, acc 0.984375, prec 0.117266, recall 0.795318
2017-12-10T05:52:28.870783: step 8895, loss 0.0751216, acc 0.984375, prec 0.117275, recall 0.795334
2017-12-10T05:52:29.136811: step 8896, loss 0.0028111, acc 1, prec 0.117275, recall 0.795334
2017-12-10T05:52:29.405547: step 8897, loss 0.00163202, acc 1, prec 0.117295, recall 0.795366
2017-12-10T05:52:29.669107: step 8898, loss 0.000479078, acc 1, prec 0.117295, recall 0.795366
2017-12-10T05:52:29.941335: step 8899, loss 0.0958174, acc 0.984375, prec 0.117304, recall 0.795382
2017-12-10T05:52:30.208445: step 8900, loss 0.00174555, acc 1, prec 0.117304, recall 0.795382
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-8900

2017-12-10T05:52:31.506199: step 8901, loss 3.80973e-05, acc 1, prec 0.117314, recall 0.795398
2017-12-10T05:52:31.766870: step 8902, loss 0.405489, acc 0.953125, prec 0.11732, recall 0.795414
2017-12-10T05:52:32.036281: step 8903, loss 0.0680628, acc 0.96875, prec 0.117318, recall 0.795414
2017-12-10T05:52:32.310699: step 8904, loss 0.14829, acc 0.984375, prec 0.117327, recall 0.79543
2017-12-10T05:52:32.587222: step 8905, loss 0.000385635, acc 1, prec 0.117347, recall 0.795462
2017-12-10T05:52:32.862349: step 8906, loss 0.106815, acc 0.984375, prec 0.117356, recall 0.795478
2017-12-10T05:52:33.128220: step 8907, loss 0.295721, acc 0.984375, prec 0.117365, recall 0.795494
2017-12-10T05:52:33.397183: step 8908, loss 0.0808218, acc 0.984375, prec 0.117363, recall 0.795494
2017-12-10T05:52:33.665267: step 8909, loss 0.118195, acc 0.953125, prec 0.117369, recall 0.79551
2017-12-10T05:52:33.940622: step 8910, loss 0.582128, acc 0.9375, prec 0.117364, recall 0.79551
2017-12-10T05:52:34.204252: step 8911, loss 0.000861834, acc 1, prec 0.117384, recall 0.795542
2017-12-10T05:52:34.464524: step 8912, loss 0.0538658, acc 0.96875, prec 0.117412, recall 0.79559
2017-12-10T05:52:34.732144: step 8913, loss 0.623132, acc 0.984375, prec 0.117452, recall 0.795654
2017-12-10T05:52:35.001338: step 8914, loss 0.331613, acc 1, prec 0.117462, recall 0.795669
2017-12-10T05:52:35.270682: step 8915, loss 0.0657624, acc 0.984375, prec 0.117471, recall 0.795685
2017-12-10T05:52:35.540205: step 8916, loss 0.0115621, acc 1, prec 0.117481, recall 0.795701
2017-12-10T05:52:35.807736: step 8917, loss 0.0325832, acc 0.984375, prec 0.11749, recall 0.795717
2017-12-10T05:52:36.075357: step 8918, loss 0.038269, acc 0.984375, prec 0.117498, recall 0.795733
2017-12-10T05:52:36.342810: step 8919, loss 0.00695413, acc 1, prec 0.117498, recall 0.795733
2017-12-10T05:52:36.609383: step 8920, loss 0.296431, acc 0.984375, prec 0.117507, recall 0.795749
2017-12-10T05:52:36.877150: step 8921, loss 0.073223, acc 0.984375, prec 0.117526, recall 0.795781
2017-12-10T05:52:37.144767: step 8922, loss 0.649552, acc 0.9375, prec 0.117521, recall 0.795781
2017-12-10T05:52:37.410422: step 8923, loss 0.00215901, acc 1, prec 0.117531, recall 0.795797
2017-12-10T05:52:37.673508: step 8924, loss 0.116518, acc 0.984375, prec 0.11756, recall 0.795845
2017-12-10T05:52:37.938781: step 8925, loss 0.0481417, acc 0.984375, prec 0.117579, recall 0.795877
2017-12-10T05:52:38.203766: step 8926, loss 0.429967, acc 0.96875, prec 0.117576, recall 0.795877
2017-12-10T05:52:38.466564: step 8927, loss 0.0023456, acc 1, prec 0.117587, recall 0.795893
2017-12-10T05:52:38.731046: step 8928, loss 0.000282219, acc 1, prec 0.117597, recall 0.795909
2017-12-10T05:52:39.001580: step 8929, loss 0.000779481, acc 1, prec 0.117607, recall 0.795925
2017-12-10T05:52:39.266742: step 8930, loss 0.00321315, acc 1, prec 0.117617, recall 0.795941
2017-12-10T05:52:39.532354: step 8931, loss 0.0305065, acc 0.984375, prec 0.117636, recall 0.795973
2017-12-10T05:52:39.795036: step 8932, loss 6.00237e-05, acc 1, prec 0.117646, recall 0.795988
2017-12-10T05:52:40.074434: step 8933, loss 8.44542, acc 0.984375, prec 0.117687, recall 0.79599
2017-12-10T05:52:40.344293: step 8934, loss 0.0863838, acc 0.953125, prec 0.117693, recall 0.796006
2017-12-10T05:52:40.611887: step 8935, loss 0.19104, acc 0.96875, prec 0.11769, recall 0.796006
2017-12-10T05:52:40.882324: step 8936, loss 0.316187, acc 0.96875, prec 0.117688, recall 0.796006
2017-12-10T05:52:41.148895: step 8937, loss 0.205365, acc 0.96875, prec 0.117685, recall 0.796006
2017-12-10T05:52:41.418285: step 8938, loss 0.0936728, acc 0.984375, prec 0.117684, recall 0.796006
2017-12-10T05:52:41.689987: step 8939, loss 0.585604, acc 0.9375, prec 0.117678, recall 0.796006
2017-12-10T05:52:41.952731: step 8940, loss 0.0755135, acc 0.984375, prec 0.117687, recall 0.796022
2017-12-10T05:52:42.232045: step 8941, loss 0.106041, acc 0.96875, prec 0.117684, recall 0.796022
2017-12-10T05:52:42.508357: step 8942, loss 0.209587, acc 0.96875, prec 0.117702, recall 0.796054
2017-12-10T05:52:42.779793: step 8943, loss 0.247559, acc 0.953125, prec 0.117708, recall 0.79607
2017-12-10T05:52:43.052525: step 8944, loss 0.135242, acc 0.96875, prec 0.117736, recall 0.796117
2017-12-10T05:52:43.320923: step 8945, loss 0.228603, acc 0.921875, prec 0.117749, recall 0.796149
2017-12-10T05:52:43.554138: step 8946, loss 0.19172, acc 0.980769, prec 0.117758, recall 0.796165
2017-12-10T05:52:43.823993: step 8947, loss 0.275644, acc 0.9375, prec 0.117763, recall 0.796181
2017-12-10T05:52:44.102086: step 8948, loss 0.587108, acc 0.953125, prec 0.117769, recall 0.796197
2017-12-10T05:52:44.377513: step 8949, loss 1.26699, acc 0.875, prec 0.117779, recall 0.796228
2017-12-10T05:52:44.645794: step 8950, loss 0.165715, acc 0.96875, prec 0.117776, recall 0.796228
2017-12-10T05:52:44.917086: step 8951, loss 0.318764, acc 0.96875, prec 0.117804, recall 0.796276
2017-12-10T05:52:45.191667: step 8952, loss 0.276181, acc 0.96875, prec 0.117801, recall 0.796276
2017-12-10T05:52:45.458258: step 8953, loss 0.101057, acc 0.984375, prec 0.11781, recall 0.796292
2017-12-10T05:52:45.717036: step 8954, loss 0.206023, acc 0.984375, prec 0.117819, recall 0.796308
2017-12-10T05:52:45.983008: step 8955, loss 0.181303, acc 0.96875, prec 0.117826, recall 0.796324
2017-12-10T05:52:46.249893: step 8956, loss 0.0827769, acc 0.984375, prec 0.117845, recall 0.796355
2017-12-10T05:52:46.514026: step 8957, loss 0.174829, acc 0.984375, prec 0.117844, recall 0.796355
2017-12-10T05:52:46.785354: step 8958, loss 0.0975154, acc 0.96875, prec 0.117851, recall 0.796371
2017-12-10T05:52:47.065467: step 8959, loss 0.171866, acc 0.96875, prec 0.117848, recall 0.796371
2017-12-10T05:52:47.337204: step 8960, loss 0.0971115, acc 0.953125, prec 0.117844, recall 0.796371
2017-12-10T05:52:47.606729: step 8961, loss 0.251157, acc 0.953125, prec 0.11784, recall 0.796371
2017-12-10T05:52:47.873217: step 8962, loss 0.498457, acc 0.96875, prec 0.117838, recall 0.796371
2017-12-10T05:52:48.137500: step 8963, loss 0.24471, acc 0.984375, prec 0.117856, recall 0.796403
2017-12-10T05:52:48.407925: step 8964, loss 0.215898, acc 0.96875, prec 0.117864, recall 0.796419
2017-12-10T05:52:48.672745: step 8965, loss 0.0973138, acc 0.984375, prec 0.117873, recall 0.796435
2017-12-10T05:52:48.937987: step 8966, loss 0.0686687, acc 0.984375, prec 0.117871, recall 0.796435
2017-12-10T05:52:49.205122: step 8967, loss 0.105992, acc 0.953125, prec 0.117877, recall 0.796451
2017-12-10T05:52:49.473145: step 8968, loss 0.0895021, acc 0.96875, prec 0.117885, recall 0.796466
2017-12-10T05:52:49.743261: step 8969, loss 0.108901, acc 0.96875, prec 0.117903, recall 0.796498
2017-12-10T05:52:50.016281: step 8970, loss 0.000542561, acc 1, prec 0.117913, recall 0.796514
2017-12-10T05:52:50.278232: step 8971, loss 0.178351, acc 0.953125, prec 0.117919, recall 0.79653
2017-12-10T05:52:50.551008: step 8972, loss 0.0992587, acc 0.96875, prec 0.117916, recall 0.79653
2017-12-10T05:52:50.813283: step 8973, loss 0.157899, acc 0.96875, prec 0.117923, recall 0.796546
2017-12-10T05:52:51.076576: step 8974, loss 1.68153e-05, acc 1, prec 0.117923, recall 0.796546
2017-12-10T05:52:51.334196: step 8975, loss 9.11671e-05, acc 1, prec 0.117934, recall 0.796561
2017-12-10T05:52:51.598932: step 8976, loss 0.0461991, acc 0.984375, prec 0.117942, recall 0.796577
2017-12-10T05:52:51.862807: step 8977, loss 0.000581738, acc 1, prec 0.117953, recall 0.796593
2017-12-10T05:52:52.132307: step 8978, loss 0.0178492, acc 1, prec 0.117973, recall 0.796625
2017-12-10T05:52:52.398437: step 8979, loss 5.681e-07, acc 1, prec 0.117983, recall 0.79664
2017-12-10T05:52:53.371764: step 8980, loss 0.018306, acc 0.984375, prec 0.117982, recall 0.79664
2017-12-10T05:52:53.739615: step 8981, loss 0.14989, acc 0.96875, prec 0.117979, recall 0.79664
2017-12-10T05:52:54.009050: step 8982, loss 0.124241, acc 0.984375, prec 0.118018, recall 0.796704
2017-12-10T05:52:54.731321: step 8983, loss 0.167097, acc 0.96875, prec 0.118016, recall 0.796704
2017-12-10T05:52:55.234229: step 8984, loss 0.00213506, acc 1, prec 0.118016, recall 0.796704
2017-12-10T05:52:55.508076: step 8985, loss 0.01605, acc 0.984375, prec 0.118035, recall 0.796735
2017-12-10T05:52:55.788704: step 8986, loss 0.0332575, acc 0.984375, prec 0.118033, recall 0.796735
2017-12-10T05:52:56.061036: step 8987, loss 13.5931, acc 0.984375, prec 0.118033, recall 0.796673
2017-12-10T05:52:56.333821: step 8988, loss 2.10475e-06, acc 1, prec 0.118033, recall 0.796673
2017-12-10T05:52:56.593021: step 8989, loss 1.67578, acc 0.984375, prec 0.118043, recall 0.796627
2017-12-10T05:52:56.865346: step 8990, loss 0.000128904, acc 1, prec 0.118043, recall 0.796627
2017-12-10T05:52:57.126804: step 8991, loss 0.0826214, acc 0.96875, prec 0.118051, recall 0.796643
2017-12-10T05:52:57.400874: step 8992, loss 0.127697, acc 0.96875, prec 0.118068, recall 0.796675
2017-12-10T05:52:57.671829: step 8993, loss 0.43215, acc 0.921875, prec 0.118072, recall 0.79669
2017-12-10T05:52:57.937968: step 8994, loss 0.798681, acc 0.921875, prec 0.118085, recall 0.796722
2017-12-10T05:52:58.206310: step 8995, loss 0.228587, acc 0.953125, prec 0.118122, recall 0.796785
2017-12-10T05:52:58.467708: step 8996, loss 0.146965, acc 0.953125, prec 0.118128, recall 0.796801
2017-12-10T05:52:58.735717: step 8997, loss 0.693772, acc 0.859375, prec 0.118116, recall 0.796801
2017-12-10T05:52:59.007899: step 8998, loss 0.911686, acc 0.9375, prec 0.11813, recall 0.796833
2017-12-10T05:52:59.272818: step 8999, loss 0.224246, acc 0.96875, prec 0.118138, recall 0.796848
2017-12-10T05:52:59.543211: step 9000, loss 0.853764, acc 0.875, prec 0.118127, recall 0.796848

Evaluation:
2017-12-10T05:53:07.164342: step 9000, loss 5.76945, acc 0.88847, prec 0.117608, recall 0.794968

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-9000

2017-12-10T05:53:08.466095: step 9001, loss 0.316405, acc 0.9375, prec 0.117613, recall 0.794984
2017-12-10T05:53:08.735591: step 9002, loss 0.198267, acc 0.953125, prec 0.117619, recall 0.795
2017-12-10T05:53:08.999522: step 9003, loss 0.293307, acc 0.953125, prec 0.117625, recall 0.795015
2017-12-10T05:53:09.272474: step 9004, loss 0.992317, acc 0.8125, prec 0.117629, recall 0.795047
2017-12-10T05:53:09.544448: step 9005, loss 0.357781, acc 0.921875, prec 0.117622, recall 0.795047
2017-12-10T05:53:09.811545: step 9006, loss 0.407522, acc 0.921875, prec 0.117646, recall 0.795094
2017-12-10T05:53:10.078507: step 9007, loss 0.181034, acc 0.953125, prec 0.117662, recall 0.795125
2017-12-10T05:53:10.350441: step 9008, loss 0.161622, acc 0.953125, prec 0.117668, recall 0.795141
2017-12-10T05:53:10.615692: step 9009, loss 0.256267, acc 0.96875, prec 0.117665, recall 0.795141
2017-12-10T05:53:10.885156: step 9010, loss 0.092277, acc 0.96875, prec 0.117672, recall 0.795157
2017-12-10T05:53:11.149816: step 9011, loss 0.282821, acc 0.9375, prec 0.117697, recall 0.795204
2017-12-10T05:53:11.415642: step 9012, loss 0.525705, acc 0.921875, prec 0.1177, recall 0.795219
2017-12-10T05:53:11.681604: step 9013, loss 0.017875, acc 0.984375, prec 0.117699, recall 0.795219
2017-12-10T05:53:11.952469: step 9014, loss 0.635761, acc 0.890625, prec 0.11769, recall 0.795219
2017-12-10T05:53:12.219651: step 9015, loss 0.599499, acc 0.953125, prec 0.117686, recall 0.795219
2017-12-10T05:53:12.492558: step 9016, loss 0.122703, acc 0.984375, prec 0.117684, recall 0.795219
2017-12-10T05:53:12.759749: step 9017, loss 0.0436296, acc 0.984375, prec 0.117693, recall 0.795235
2017-12-10T05:53:13.025528: step 9018, loss 0.117804, acc 0.953125, prec 0.117699, recall 0.795251
2017-12-10T05:53:13.303056: step 9019, loss 0.061347, acc 0.96875, prec 0.117696, recall 0.795251
2017-12-10T05:53:13.576723: step 9020, loss 0.40289, acc 0.9375, prec 0.117701, recall 0.795267
2017-12-10T05:53:13.843322: step 9021, loss 0.0725006, acc 0.96875, prec 0.117708, recall 0.795282
2017-12-10T05:53:14.107375: step 9022, loss 0.000133532, acc 1, prec 0.117718, recall 0.795298
2017-12-10T05:53:14.369554: step 9023, loss 0.068427, acc 0.984375, prec 0.117727, recall 0.795314
2017-12-10T05:53:14.645835: step 9024, loss 0.0873062, acc 0.984375, prec 0.117736, recall 0.795329
2017-12-10T05:53:14.921246: step 9025, loss 0.36804, acc 0.984375, prec 0.117744, recall 0.795345
2017-12-10T05:53:15.188704: step 9026, loss 0.388438, acc 0.984375, prec 0.117743, recall 0.795345
2017-12-10T05:53:15.456626: step 9027, loss 0.0807957, acc 0.96875, prec 0.11774, recall 0.795345
2017-12-10T05:53:15.731806: step 9028, loss 0.0266058, acc 0.984375, prec 0.117749, recall 0.795361
2017-12-10T05:53:15.998938: step 9029, loss 0.164661, acc 0.96875, prec 0.117766, recall 0.795392
2017-12-10T05:53:16.271200: step 9030, loss 0.658773, acc 0.96875, prec 0.117774, recall 0.795408
2017-12-10T05:53:16.535762: step 9031, loss 0.0813909, acc 0.984375, prec 0.117772, recall 0.795408
2017-12-10T05:53:16.808933: step 9032, loss 0.0594325, acc 0.984375, prec 0.117771, recall 0.795408
2017-12-10T05:53:17.071115: step 9033, loss 0.0571819, acc 1, prec 0.117811, recall 0.79547
2017-12-10T05:53:17.335711: step 9034, loss 0.463455, acc 0.96875, prec 0.117818, recall 0.795486
2017-12-10T05:53:17.604098: step 9035, loss 0.0450206, acc 0.984375, prec 0.117847, recall 0.795533
2017-12-10T05:53:17.872509: step 9036, loss 3.86512, acc 0.953125, prec 0.117854, recall 0.795488
2017-12-10T05:53:18.150618: step 9037, loss 0.00680901, acc 1, prec 0.117874, recall 0.795519
2017-12-10T05:53:18.415930: step 9038, loss 0.391181, acc 0.953125, prec 0.11789, recall 0.79555
2017-12-10T05:53:18.680776: step 9039, loss 0.000624521, acc 1, prec 0.11789, recall 0.79555
2017-12-10T05:53:18.950524: step 9040, loss 0.0068456, acc 1, prec 0.11789, recall 0.79555
2017-12-10T05:53:19.215883: step 9041, loss 0.0293917, acc 0.984375, prec 0.117899, recall 0.795566
2017-12-10T05:53:19.485572: step 9042, loss 0.00221794, acc 1, prec 0.117899, recall 0.795566
2017-12-10T05:53:19.757206: step 9043, loss 0.241172, acc 0.953125, prec 0.117905, recall 0.795581
2017-12-10T05:53:20.022722: step 9044, loss 1.05641, acc 0.953125, prec 0.117911, recall 0.795597
2017-12-10T05:53:20.297285: step 9045, loss 0.0597276, acc 0.953125, prec 0.117917, recall 0.795613
2017-12-10T05:53:20.564346: step 9046, loss 0.490115, acc 0.890625, prec 0.117928, recall 0.795644
2017-12-10T05:53:20.827138: step 9047, loss 0.527969, acc 0.90625, prec 0.11792, recall 0.795644
2017-12-10T05:53:21.109547: step 9048, loss 0.701856, acc 0.890625, prec 0.11792, recall 0.795659
2017-12-10T05:53:21.390542: step 9049, loss 0.374896, acc 0.921875, prec 0.117914, recall 0.795659
2017-12-10T05:53:21.657594: step 9050, loss 0.264157, acc 0.953125, prec 0.117949, recall 0.795722
2017-12-10T05:53:21.933995: step 9051, loss 0.466081, acc 0.953125, prec 0.117955, recall 0.795738
2017-12-10T05:53:22.195539: step 9052, loss 0.257612, acc 0.921875, prec 0.117989, recall 0.7958
2017-12-10T05:53:22.471812: step 9053, loss 0.114102, acc 0.96875, prec 0.117996, recall 0.795816
2017-12-10T05:53:22.741306: step 9054, loss 0.414133, acc 0.953125, prec 0.118012, recall 0.795847
2017-12-10T05:53:23.008732: step 9055, loss 0.171296, acc 0.96875, prec 0.118039, recall 0.795893
2017-12-10T05:53:23.281939: step 9056, loss 0.0178607, acc 0.984375, prec 0.118038, recall 0.795893
2017-12-10T05:53:23.551376: step 9057, loss 0.344919, acc 0.953125, prec 0.118044, recall 0.795909
2017-12-10T05:53:23.826060: step 9058, loss 0.216752, acc 0.921875, prec 0.118057, recall 0.79594
2017-12-10T05:53:24.097884: step 9059, loss 0.255895, acc 0.953125, prec 0.118073, recall 0.795971
2017-12-10T05:53:24.363772: step 9060, loss 0.292092, acc 0.96875, prec 0.11807, recall 0.795971
2017-12-10T05:53:24.628210: step 9061, loss 0.139543, acc 0.96875, prec 0.118068, recall 0.795971
2017-12-10T05:53:24.893959: step 9062, loss 0.124416, acc 0.984375, prec 0.118076, recall 0.795987
2017-12-10T05:53:25.164955: step 9063, loss 0.622288, acc 0.921875, prec 0.11807, recall 0.795987
2017-12-10T05:53:25.426663: step 9064, loss 0.451859, acc 0.9375, prec 0.118064, recall 0.795987
2017-12-10T05:53:25.699147: step 9065, loss 0.246811, acc 0.984375, prec 0.118083, recall 0.796018
2017-12-10T05:53:25.970808: step 9066, loss 0.389175, acc 0.984375, prec 0.118092, recall 0.796034
2017-12-10T05:53:26.240677: step 9067, loss 0.000706334, acc 1, prec 0.118102, recall 0.796049
2017-12-10T05:53:26.506754: step 9068, loss 0.0175155, acc 0.984375, prec 0.11811, recall 0.796065
2017-12-10T05:53:26.782434: step 9069, loss 0.0246513, acc 0.984375, prec 0.118109, recall 0.796065
2017-12-10T05:53:27.051744: step 9070, loss 0.0761872, acc 0.96875, prec 0.118106, recall 0.796065
2017-12-10T05:53:27.325891: step 9071, loss 0.0147314, acc 0.984375, prec 0.118115, recall 0.79608
2017-12-10T05:53:27.604236: step 9072, loss 0.0407175, acc 0.96875, prec 0.118112, recall 0.79608
2017-12-10T05:53:27.868501: step 9073, loss 0.322549, acc 0.96875, prec 0.11811, recall 0.79608
2017-12-10T05:53:28.139079: step 9074, loss 7.86322e-06, acc 1, prec 0.11811, recall 0.79608
2017-12-10T05:53:28.402413: step 9075, loss 0.803438, acc 0.96875, prec 0.118127, recall 0.796111
2017-12-10T05:53:28.673440: step 9076, loss 0.568201, acc 0.953125, prec 0.118143, recall 0.796142
2017-12-10T05:53:28.942578: step 9077, loss 0.000307572, acc 1, prec 0.118153, recall 0.796158
2017-12-10T05:53:29.212368: step 9078, loss 0.258319, acc 0.96875, prec 0.11816, recall 0.796174
2017-12-10T05:53:29.485755: step 9079, loss 0.000912014, acc 1, prec 0.11816, recall 0.796174
2017-12-10T05:53:29.747105: step 9080, loss 0.00300014, acc 1, prec 0.11817, recall 0.796189
2017-12-10T05:53:30.012746: step 9081, loss 0.0563499, acc 0.984375, prec 0.118169, recall 0.796189
2017-12-10T05:53:30.277625: step 9082, loss 0.000551751, acc 1, prec 0.118179, recall 0.796205
2017-12-10T05:53:30.552967: step 9083, loss 0.0554908, acc 0.96875, prec 0.118186, recall 0.79622
2017-12-10T05:53:30.830093: step 9084, loss 0.188577, acc 0.984375, prec 0.118185, recall 0.79622
2017-12-10T05:53:31.096213: step 9085, loss 7.77774e-05, acc 1, prec 0.118185, recall 0.79622
2017-12-10T05:53:31.354019: step 9086, loss 0.00913626, acc 1, prec 0.118185, recall 0.79622
2017-12-10T05:53:31.627878: step 9087, loss 0.0830327, acc 0.984375, prec 0.118183, recall 0.79622
2017-12-10T05:53:31.891382: step 9088, loss 0.394505, acc 0.984375, prec 0.118192, recall 0.796236
2017-12-10T05:53:32.158678: step 9089, loss 3.76685, acc 0.984375, prec 0.118192, recall 0.796175
2017-12-10T05:53:32.427848: step 9090, loss 0.320771, acc 0.96875, prec 0.118199, recall 0.796191
2017-12-10T05:53:32.693012: step 9091, loss 0.036321, acc 0.984375, prec 0.118218, recall 0.796222
2017-12-10T05:53:32.965576: step 9092, loss 0.121955, acc 0.96875, prec 0.118215, recall 0.796222
2017-12-10T05:53:33.229999: step 9093, loss 0.00107366, acc 1, prec 0.118235, recall 0.796253
2017-12-10T05:53:33.505103: step 9094, loss 0.167734, acc 0.984375, prec 0.118234, recall 0.796253
2017-12-10T05:53:33.776011: step 9095, loss 0.294328, acc 0.953125, prec 0.11824, recall 0.796268
2017-12-10T05:53:34.043033: step 9096, loss 0.0507991, acc 0.984375, prec 0.118238, recall 0.796268
2017-12-10T05:53:34.311352: step 9097, loss 0.592157, acc 0.96875, prec 0.118236, recall 0.796268
2017-12-10T05:53:34.577183: step 9098, loss 0.0188992, acc 0.984375, prec 0.118234, recall 0.796268
2017-12-10T05:53:34.841418: step 9099, loss 0.404444, acc 0.96875, prec 0.118232, recall 0.796268
2017-12-10T05:53:35.111364: step 9100, loss 0.184162, acc 0.96875, prec 0.118229, recall 0.796268
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-9100

2017-12-10T05:53:36.406624: step 9101, loss 0.324747, acc 0.96875, prec 0.118246, recall 0.796299
2017-12-10T05:53:36.672271: step 9102, loss 0.155264, acc 0.96875, prec 0.118244, recall 0.796299
2017-12-10T05:53:36.938806: step 9103, loss 0.454842, acc 0.984375, prec 0.118262, recall 0.79633
2017-12-10T05:53:37.204180: step 9104, loss 0.21165, acc 0.984375, prec 0.118261, recall 0.79633
2017-12-10T05:53:37.473579: step 9105, loss 0.364185, acc 0.953125, prec 0.118257, recall 0.79633
2017-12-10T05:53:37.744986: step 9106, loss 0.233575, acc 0.9375, prec 0.118272, recall 0.796361
2017-12-10T05:53:38.018914: step 9107, loss 0.188316, acc 0.953125, prec 0.118278, recall 0.796377
2017-12-10T05:53:38.289603: step 9108, loss 0.163326, acc 0.953125, prec 0.118283, recall 0.796392
2017-12-10T05:53:38.558704: step 9109, loss 0.561606, acc 0.921875, prec 0.118287, recall 0.796408
2017-12-10T05:53:38.825619: step 9110, loss 0.113295, acc 0.953125, prec 0.118283, recall 0.796408
2017-12-10T05:53:39.094312: step 9111, loss 0.528876, acc 0.953125, prec 0.118319, recall 0.79647
2017-12-10T05:53:39.361131: step 9112, loss 0.145172, acc 0.9375, prec 0.118333, recall 0.796501
2017-12-10T05:53:39.633778: step 9113, loss 0.194042, acc 0.9375, prec 0.118328, recall 0.796501
2017-12-10T05:53:39.902399: step 9114, loss 0.118502, acc 0.953125, prec 0.118344, recall 0.796531
2017-12-10T05:53:40.173265: step 9115, loss 0.759061, acc 0.953125, prec 0.11834, recall 0.796531
2017-12-10T05:53:40.442660: step 9116, loss 0.385094, acc 0.953125, prec 0.118356, recall 0.796562
2017-12-10T05:53:40.707129: step 9117, loss 0.380508, acc 0.9375, prec 0.11837, recall 0.796593
2017-12-10T05:53:40.975617: step 9118, loss 0.00199568, acc 1, prec 0.11839, recall 0.796624
2017-12-10T05:53:41.237347: step 9119, loss 0.206797, acc 0.984375, prec 0.118399, recall 0.79664
2017-12-10T05:53:41.513530: step 9120, loss 0.026193, acc 0.984375, prec 0.118397, recall 0.79664
2017-12-10T05:53:41.779009: step 9121, loss 0.918325, acc 0.921875, prec 0.118401, recall 0.796655
2017-12-10T05:53:42.045893: step 9122, loss 0.0766403, acc 0.984375, prec 0.118399, recall 0.796655
2017-12-10T05:53:42.324158: step 9123, loss 0.000799143, acc 1, prec 0.118399, recall 0.796655
2017-12-10T05:53:42.597743: step 9124, loss 0.134297, acc 0.984375, prec 0.118428, recall 0.796702
2017-12-10T05:53:42.865412: step 9125, loss 0.445332, acc 0.96875, prec 0.118425, recall 0.796702
2017-12-10T05:53:43.135179: step 9126, loss 0.000580993, acc 1, prec 0.118435, recall 0.796717
2017-12-10T05:53:43.405206: step 9127, loss 0.065161, acc 0.984375, prec 0.118444, recall 0.796733
2017-12-10T05:53:43.666748: step 9128, loss 0.117821, acc 0.96875, prec 0.118451, recall 0.796748
2017-12-10T05:53:43.931653: step 9129, loss 0.0627545, acc 0.96875, prec 0.118448, recall 0.796748
2017-12-10T05:53:44.203297: step 9130, loss 0.00286107, acc 1, prec 0.118448, recall 0.796748
2017-12-10T05:53:44.466103: step 9131, loss 0.125685, acc 0.984375, prec 0.118457, recall 0.796763
2017-12-10T05:53:44.743536: step 9132, loss 0.374788, acc 0.96875, prec 0.118484, recall 0.79681
2017-12-10T05:53:45.008519: step 9133, loss 0.00242363, acc 1, prec 0.118484, recall 0.79681
2017-12-10T05:53:45.278615: step 9134, loss 0.150752, acc 0.953125, prec 0.11848, recall 0.79681
2017-12-10T05:53:45.545823: step 9135, loss 0.000909509, acc 1, prec 0.11848, recall 0.79681
2017-12-10T05:53:45.812475: step 9136, loss 2.71299e-05, acc 1, prec 0.11848, recall 0.79681
2017-12-10T05:53:46.077265: step 9137, loss 7.00051, acc 0.984375, prec 0.1185, recall 0.79678
2017-12-10T05:53:46.345063: step 9138, loss 0.0891408, acc 1, prec 0.11851, recall 0.796795
2017-12-10T05:53:46.611885: step 9139, loss 0.237334, acc 0.984375, prec 0.118519, recall 0.796811
2017-12-10T05:53:46.873649: step 9140, loss 0.0313779, acc 0.984375, prec 0.118547, recall 0.796857
2017-12-10T05:53:47.140020: step 9141, loss 0.167708, acc 0.953125, prec 0.118563, recall 0.796888
2017-12-10T05:53:47.411997: step 9142, loss 0.375943, acc 0.9375, prec 0.118568, recall 0.796903
2017-12-10T05:53:47.678781: step 9143, loss 0.00182235, acc 1, prec 0.118568, recall 0.796903
2017-12-10T05:53:47.957443: step 9144, loss 7.41045, acc 0.953125, prec 0.118565, recall 0.796843
2017-12-10T05:53:48.221696: step 9145, loss 0.137699, acc 0.96875, prec 0.118562, recall 0.796843
2017-12-10T05:53:48.497323: step 9146, loss 0.326289, acc 0.875, prec 0.118572, recall 0.796874
2017-12-10T05:53:48.766668: step 9147, loss 0.921359, acc 0.90625, prec 0.118564, recall 0.796874
2017-12-10T05:53:49.034890: step 9148, loss 0.596224, acc 0.859375, prec 0.118561, recall 0.796889
2017-12-10T05:53:49.297014: step 9149, loss 0.7099, acc 0.859375, prec 0.118549, recall 0.796889
2017-12-10T05:53:49.563493: step 9150, loss 1.84645, acc 0.78125, prec 0.118531, recall 0.796889
2017-12-10T05:53:49.829059: step 9151, loss 1.56151, acc 0.8125, prec 0.118525, recall 0.796905
2017-12-10T05:53:50.102563: step 9152, loss 0.870665, acc 0.828125, prec 0.11852, recall 0.79692
2017-12-10T05:53:50.367964: step 9153, loss 1.08907, acc 0.78125, prec 0.118511, recall 0.796935
2017-12-10T05:53:50.637360: step 9154, loss 2.20463, acc 0.75, prec 0.1185, recall 0.796951
2017-12-10T05:53:50.910998: step 9155, loss 1.34632, acc 0.8125, prec 0.118493, recall 0.796966
2017-12-10T05:53:51.176559: step 9156, loss 0.440394, acc 0.875, prec 0.118503, recall 0.796997
2017-12-10T05:53:51.444864: step 9157, loss 0.896764, acc 0.875, prec 0.118502, recall 0.797012
2017-12-10T05:53:51.710052: step 9158, loss 1.19678, acc 0.828125, prec 0.118497, recall 0.797028
2017-12-10T05:53:51.975640: step 9159, loss 0.521694, acc 0.921875, prec 0.1185, recall 0.797043
2017-12-10T05:53:52.261331: step 9160, loss 0.498102, acc 0.953125, prec 0.118506, recall 0.797059
2017-12-10T05:53:52.530838: step 9161, loss 0.0945314, acc 0.953125, prec 0.118512, recall 0.797074
2017-12-10T05:53:52.805826: step 9162, loss 0.111741, acc 0.96875, prec 0.118539, recall 0.79712
2017-12-10T05:53:53.069182: step 9163, loss 0.0898323, acc 0.96875, prec 0.118547, recall 0.797135
2017-12-10T05:53:53.332808: step 9164, loss 0.310044, acc 0.9375, prec 0.118541, recall 0.797135
2017-12-10T05:53:53.606099: step 9165, loss 0.0671946, acc 0.96875, prec 0.118549, recall 0.797151
2017-12-10T05:53:53.871096: step 9166, loss 0.0638407, acc 0.984375, prec 0.118547, recall 0.797151
2017-12-10T05:53:54.140852: step 9167, loss 0.417831, acc 0.953125, prec 0.118553, recall 0.797166
2017-12-10T05:53:54.405623: step 9168, loss 0.135037, acc 0.984375, prec 0.118562, recall 0.797182
2017-12-10T05:53:54.671299: step 9169, loss 0.0300099, acc 0.984375, prec 0.11856, recall 0.797182
2017-12-10T05:53:54.939343: step 9170, loss 0.102881, acc 0.984375, prec 0.118559, recall 0.797182
2017-12-10T05:53:55.209419: step 9171, loss 0.57135, acc 0.9375, prec 0.118574, recall 0.797212
2017-12-10T05:53:55.476422: step 9172, loss 0.739337, acc 0.921875, prec 0.118567, recall 0.797212
2017-12-10T05:53:55.749364: step 9173, loss 0.371356, acc 0.953125, prec 0.118563, recall 0.797212
2017-12-10T05:53:56.021219: step 9174, loss 0.0256137, acc 0.984375, prec 0.118562, recall 0.797212
2017-12-10T05:53:56.295829: step 9175, loss 0.194533, acc 0.96875, prec 0.118569, recall 0.797228
2017-12-10T05:53:56.570100: step 9176, loss 0.130092, acc 0.984375, prec 0.118568, recall 0.797228
2017-12-10T05:53:56.833517: step 9177, loss 0.979379, acc 0.921875, prec 0.118581, recall 0.797258
2017-12-10T05:53:57.095220: step 9178, loss 0.876164, acc 0.96875, prec 0.118578, recall 0.797258
2017-12-10T05:53:57.372685: step 9179, loss 0.137243, acc 0.984375, prec 0.118587, recall 0.797274
2017-12-10T05:53:57.645679: step 9180, loss 0.338569, acc 0.984375, prec 0.118585, recall 0.797274
2017-12-10T05:53:57.913192: step 9181, loss 0.247383, acc 0.96875, prec 0.118612, recall 0.79732
2017-12-10T05:53:58.184097: step 9182, loss 0.000124167, acc 1, prec 0.118612, recall 0.79732
2017-12-10T05:53:58.447239: step 9183, loss 0.00580822, acc 1, prec 0.118612, recall 0.79732
2017-12-10T05:53:58.709675: step 9184, loss 8.98146, acc 0.984375, prec 0.118612, recall 0.797259
2017-12-10T05:53:58.976670: step 9185, loss 5.85804e-05, acc 1, prec 0.118622, recall 0.797275
2017-12-10T05:53:59.237492: step 9186, loss 0.0502245, acc 0.984375, prec 0.118641, recall 0.797305
2017-12-10T05:53:59.506240: step 9187, loss 0.000163585, acc 1, prec 0.118641, recall 0.797305
2017-12-10T05:53:59.770708: step 9188, loss 0.565669, acc 0.953125, prec 0.118657, recall 0.797336
2017-12-10T05:54:00.041191: step 9189, loss 0.00696521, acc 1, prec 0.118657, recall 0.797336
2017-12-10T05:54:00.304877: step 9190, loss 0.000145909, acc 1, prec 0.118657, recall 0.797336
2017-12-10T05:54:00.573908: step 9191, loss 0.0605343, acc 1, prec 0.118677, recall 0.797367
2017-12-10T05:54:00.845724: step 9192, loss 0.0513151, acc 0.984375, prec 0.118675, recall 0.797367
2017-12-10T05:54:01.118755: step 9193, loss 0.055532, acc 0.984375, prec 0.118694, recall 0.797397
2017-12-10T05:54:01.391878: step 9194, loss 0.0874421, acc 0.984375, prec 0.118702, recall 0.797413
2017-12-10T05:54:01.657608: step 9195, loss 0.1895, acc 0.96875, prec 0.11871, recall 0.797428
2017-12-10T05:54:01.926308: step 9196, loss 0.00540158, acc 1, prec 0.118719, recall 0.797443
2017-12-10T05:54:02.189433: step 9197, loss 0.619619, acc 0.96875, prec 0.118717, recall 0.797443
2017-12-10T05:54:02.461539: step 9198, loss 0.0110468, acc 0.984375, prec 0.118725, recall 0.797459
2017-12-10T05:54:02.736949: step 9199, loss 0.300399, acc 0.96875, prec 0.118743, recall 0.797489
2017-12-10T05:54:03.003248: step 9200, loss 0.157508, acc 0.953125, prec 0.118739, recall 0.797489
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-9200

2017-12-10T05:54:04.282441: step 9201, loss 0.131718, acc 0.953125, prec 0.118754, recall 0.79752
2017-12-10T05:54:04.548326: step 9202, loss 0.929742, acc 0.9375, prec 0.118779, recall 0.797566
2017-12-10T05:54:04.815090: step 9203, loss 0.817289, acc 0.96875, prec 0.118786, recall 0.797581
2017-12-10T05:54:05.081424: step 9204, loss 0.284348, acc 0.9375, prec 0.118781, recall 0.797581
2017-12-10T05:54:05.350404: step 9205, loss 0.000906968, acc 1, prec 0.118801, recall 0.797612
2017-12-10T05:54:05.612596: step 9206, loss 0.197596, acc 0.96875, prec 0.118798, recall 0.797612
2017-12-10T05:54:05.885467: step 9207, loss 0.0372482, acc 0.96875, prec 0.118795, recall 0.797612
2017-12-10T05:54:06.158396: step 9208, loss 0.203907, acc 0.9375, prec 0.1188, recall 0.797627
2017-12-10T05:54:06.429316: step 9209, loss 0.419755, acc 0.96875, prec 0.118797, recall 0.797627
2017-12-10T05:54:06.695842: step 9210, loss 0.00139916, acc 1, prec 0.118797, recall 0.797627
2017-12-10T05:54:06.967379: step 9211, loss 0.159617, acc 0.96875, prec 0.118804, recall 0.797642
2017-12-10T05:54:07.239249: step 9212, loss 0.282191, acc 0.984375, prec 0.118823, recall 0.797673
2017-12-10T05:54:07.506768: step 9213, loss 0.00454664, acc 1, prec 0.118833, recall 0.797688
2017-12-10T05:54:07.774201: step 9214, loss 0.12297, acc 0.984375, prec 0.118831, recall 0.797688
2017-12-10T05:54:08.040753: step 9215, loss 0.281465, acc 0.96875, prec 0.118829, recall 0.797688
2017-12-10T05:54:08.315112: step 9216, loss 0.168818, acc 0.984375, prec 0.118867, recall 0.797749
2017-12-10T05:54:08.583058: step 9217, loss 0.207237, acc 0.96875, prec 0.118864, recall 0.797749
2017-12-10T05:54:08.851512: step 9218, loss 0.0263723, acc 0.984375, prec 0.118893, recall 0.797795
2017-12-10T05:54:09.116407: step 9219, loss 0.302161, acc 0.984375, prec 0.118901, recall 0.79781
2017-12-10T05:54:09.382855: step 9220, loss 0.220435, acc 0.9375, prec 0.118896, recall 0.79781
2017-12-10T05:54:09.644210: step 9221, loss 0.0238853, acc 0.984375, prec 0.118895, recall 0.79781
2017-12-10T05:54:09.910473: step 9222, loss 0.00633985, acc 1, prec 0.118895, recall 0.79781
2017-12-10T05:54:10.177956: step 9223, loss 0.146048, acc 0.984375, prec 0.118893, recall 0.79781
2017-12-10T05:54:10.443536: step 9224, loss 0.192835, acc 0.96875, prec 0.118901, recall 0.797826
2017-12-10T05:54:10.709960: step 9225, loss 1.53862, acc 0.96875, prec 0.118908, recall 0.797841
2017-12-10T05:54:10.986284: step 9226, loss 0.0207267, acc 0.984375, prec 0.118906, recall 0.797841
2017-12-10T05:54:11.251729: step 9227, loss 0.114312, acc 0.953125, prec 0.118902, recall 0.797841
2017-12-10T05:54:11.522834: step 9228, loss 0.031344, acc 0.984375, prec 0.118921, recall 0.797872
2017-12-10T05:54:11.788430: step 9229, loss 0.285285, acc 0.96875, prec 0.118918, recall 0.797872
2017-12-10T05:54:12.054198: step 9230, loss 1.73351e-05, acc 1, prec 0.118918, recall 0.797872
2017-12-10T05:54:12.321736: step 9231, loss 0.000170893, acc 1, prec 0.118918, recall 0.797872
2017-12-10T05:54:12.589967: step 9232, loss 0.0423455, acc 0.984375, prec 0.118917, recall 0.797872
2017-12-10T05:54:12.853187: step 9233, loss 0.0592253, acc 0.984375, prec 0.118935, recall 0.797902
2017-12-10T05:54:13.116804: step 9234, loss 0.400498, acc 0.96875, prec 0.118933, recall 0.797902
2017-12-10T05:54:13.390141: step 9235, loss 0.000995099, acc 1, prec 0.118933, recall 0.797902
2017-12-10T05:54:13.657954: step 9236, loss 0.111116, acc 0.984375, prec 0.118941, recall 0.797917
2017-12-10T05:54:13.927118: step 9237, loss 0.11141, acc 0.984375, prec 0.11895, recall 0.797933
2017-12-10T05:54:14.196572: step 9238, loss 0.0502576, acc 0.984375, prec 0.118958, recall 0.797948
2017-12-10T05:54:14.458258: step 9239, loss 0.0771403, acc 0.984375, prec 0.118967, recall 0.797963
2017-12-10T05:54:14.720464: step 9240, loss 0.0818833, acc 0.984375, prec 0.118976, recall 0.797978
2017-12-10T05:54:14.981096: step 9241, loss 0.211719, acc 0.9375, prec 0.11897, recall 0.797978
2017-12-10T05:54:15.253510: step 9242, loss 0.212487, acc 0.96875, prec 0.118968, recall 0.797978
2017-12-10T05:54:15.519115: step 9243, loss 0.243957, acc 0.96875, prec 0.118965, recall 0.797978
2017-12-10T05:54:15.783726: step 9244, loss 0.0630473, acc 0.96875, prec 0.118972, recall 0.797994
2017-12-10T05:54:16.049384: step 9245, loss 0.313272, acc 0.96875, prec 0.118979, recall 0.798009
2017-12-10T05:54:16.321185: step 9246, loss 0.0239254, acc 0.984375, prec 0.118978, recall 0.798009
2017-12-10T05:54:16.581899: step 9247, loss 0.0574038, acc 0.96875, prec 0.118985, recall 0.798024
2017-12-10T05:54:16.846111: step 9248, loss 0.178159, acc 0.984375, prec 0.118984, recall 0.798024
2017-12-10T05:54:17.116939: step 9249, loss 0.143379, acc 0.984375, prec 0.118983, recall 0.798024
2017-12-10T05:54:17.381014: step 9250, loss 0.412009, acc 0.953125, prec 0.118979, recall 0.798024
2017-12-10T05:54:17.644827: step 9251, loss 0.256461, acc 0.953125, prec 0.118975, recall 0.798024
2017-12-10T05:54:17.908089: step 9252, loss 0.0376935, acc 0.984375, prec 0.118993, recall 0.798054
2017-12-10T05:54:18.170025: step 9253, loss 0.569689, acc 0.953125, prec 0.118999, recall 0.79807
2017-12-10T05:54:18.446261: step 9254, loss 0.0710441, acc 0.96875, prec 0.119006, recall 0.798085
2017-12-10T05:54:18.710874: step 9255, loss 0.43928, acc 0.96875, prec 0.119023, recall 0.798115
2017-12-10T05:54:18.978862: step 9256, loss 0.00447994, acc 1, prec 0.119033, recall 0.798131
2017-12-10T05:54:19.243670: step 9257, loss 0.00752514, acc 1, prec 0.119033, recall 0.798131
2017-12-10T05:54:19.510683: step 9258, loss 0.317004, acc 0.96875, prec 0.11904, recall 0.798146
2017-12-10T05:54:19.782404: step 9259, loss 0.212984, acc 0.96875, prec 0.119058, recall 0.798176
2017-12-10T05:54:20.050138: step 9260, loss 0.0357259, acc 0.984375, prec 0.119056, recall 0.798176
2017-12-10T05:54:20.312119: step 9261, loss 0.0012001, acc 1, prec 0.119076, recall 0.798207
2017-12-10T05:54:20.576066: step 9262, loss 0.229659, acc 0.984375, prec 0.119085, recall 0.798222
2017-12-10T05:54:20.850540: step 9263, loss 4.28441e-05, acc 1, prec 0.119085, recall 0.798222
2017-12-10T05:54:21.115580: step 9264, loss 0.0161783, acc 0.984375, prec 0.119083, recall 0.798222
2017-12-10T05:54:21.388683: step 9265, loss 0.199888, acc 0.953125, prec 0.119099, recall 0.798252
2017-12-10T05:54:21.655267: step 9266, loss 0.023478, acc 0.984375, prec 0.119098, recall 0.798252
2017-12-10T05:54:21.919661: step 9267, loss 0.000531541, acc 1, prec 0.119108, recall 0.798267
2017-12-10T05:54:22.190419: step 9268, loss 0.0727544, acc 0.984375, prec 0.119146, recall 0.798328
2017-12-10T05:54:22.454305: step 9269, loss 5.20174e-05, acc 1, prec 0.119156, recall 0.798343
2017-12-10T05:54:22.716311: step 9270, loss 0.0617929, acc 0.984375, prec 0.119154, recall 0.798343
2017-12-10T05:54:22.988815: step 9271, loss 0.00306121, acc 1, prec 0.119174, recall 0.798374
2017-12-10T05:54:23.254946: step 9272, loss 0.000121607, acc 1, prec 0.119174, recall 0.798374
2017-12-10T05:54:23.518247: step 9273, loss 0.196738, acc 0.984375, prec 0.119183, recall 0.798389
2017-12-10T05:54:23.791138: step 9274, loss 0.420294, acc 0.96875, prec 0.11919, recall 0.798404
2017-12-10T05:54:24.060236: step 9275, loss 0.000112416, acc 1, prec 0.1192, recall 0.798419
2017-12-10T05:54:24.322482: step 9276, loss 2.10824, acc 0.984375, prec 0.11921, recall 0.798374
2017-12-10T05:54:24.601619: step 9277, loss 0.270366, acc 0.96875, prec 0.119217, recall 0.79839
2017-12-10T05:54:24.873239: step 9278, loss 0.500859, acc 0.953125, prec 0.119213, recall 0.79839
2017-12-10T05:54:25.145096: step 9279, loss 0.0164525, acc 0.984375, prec 0.119212, recall 0.79839
2017-12-10T05:54:25.412200: step 9280, loss 0.00229406, acc 1, prec 0.119212, recall 0.79839
2017-12-10T05:54:25.684423: step 9281, loss 0.356725, acc 0.984375, prec 0.11921, recall 0.79839
2017-12-10T05:54:25.949186: step 9282, loss 0.154913, acc 0.984375, prec 0.119219, recall 0.798405
2017-12-10T05:54:26.211802: step 9283, loss 0.11219, acc 0.96875, prec 0.119216, recall 0.798405
2017-12-10T05:54:26.471595: step 9284, loss 0.129459, acc 0.984375, prec 0.119225, recall 0.79842
2017-12-10T05:54:26.737379: step 9285, loss 0.700648, acc 0.890625, prec 0.119215, recall 0.79842
2017-12-10T05:54:27.001712: step 9286, loss 0.121134, acc 0.953125, prec 0.119211, recall 0.79842
2017-12-10T05:54:27.271773: step 9287, loss 0.321442, acc 0.953125, prec 0.119237, recall 0.798465
2017-12-10T05:54:27.542903: step 9288, loss 0.456609, acc 0.953125, prec 0.119243, recall 0.798481
2017-12-10T05:54:27.810048: step 9289, loss 1.06592, acc 0.9375, prec 0.119247, recall 0.798496
2017-12-10T05:54:28.077716: step 9290, loss 0.000223302, acc 1, prec 0.119257, recall 0.798511
2017-12-10T05:54:28.340704: step 9291, loss 0.477247, acc 0.96875, prec 0.119274, recall 0.798541
2017-12-10T05:54:28.604420: step 9292, loss 0.0611219, acc 0.984375, prec 0.119293, recall 0.798571
2017-12-10T05:54:28.874976: step 9293, loss 0.0730963, acc 0.96875, prec 0.1193, recall 0.798587
2017-12-10T05:54:29.145773: step 9294, loss 0.132643, acc 0.96875, prec 0.119317, recall 0.798617
2017-12-10T05:54:29.417231: step 9295, loss 0.0139542, acc 1, prec 0.119337, recall 0.798647
2017-12-10T05:54:29.685137: step 9296, loss 0.0999703, acc 0.96875, prec 0.119344, recall 0.798662
2017-12-10T05:54:29.951485: step 9297, loss 0.5789, acc 0.953125, prec 0.11935, recall 0.798677
2017-12-10T05:54:30.218972: step 9298, loss 0.139242, acc 0.953125, prec 0.119346, recall 0.798677
2017-12-10T05:54:30.487321: step 9299, loss 0.678456, acc 0.96875, prec 0.119343, recall 0.798677
2017-12-10T05:54:30.757884: step 9300, loss 0.0695136, acc 0.96875, prec 0.119351, recall 0.798693

Evaluation:
2017-12-10T05:54:38.333260: step 9300, loss 11.0122, acc 0.944423, prec 0.119367, recall 0.794697

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-9300

2017-12-10T05:54:39.505111: step 9301, loss 0.249464, acc 0.953125, prec 0.119373, recall 0.794712
2017-12-10T05:54:39.774320: step 9302, loss 0.00131763, acc 1, prec 0.119402, recall 0.794758
2017-12-10T05:54:40.038962: step 9303, loss 0.0636781, acc 0.984375, prec 0.119421, recall 0.794788
2017-12-10T05:54:40.311988: step 9304, loss 0.161335, acc 0.953125, prec 0.119417, recall 0.794788
2017-12-10T05:54:40.570560: step 9305, loss 0.438401, acc 0.953125, prec 0.119423, recall 0.794803
2017-12-10T05:54:40.841096: step 9306, loss 0.154433, acc 0.984375, prec 0.119431, recall 0.794819
2017-12-10T05:54:41.108701: step 9307, loss 0.00186937, acc 1, prec 0.119431, recall 0.794819
2017-12-10T05:54:41.371915: step 9308, loss 0.293039, acc 0.9375, prec 0.119436, recall 0.794834
2017-12-10T05:54:41.641423: step 9309, loss 0.0911429, acc 0.984375, prec 0.119434, recall 0.794834
2017-12-10T05:54:41.910371: step 9310, loss 3.41066e-05, acc 1, prec 0.119444, recall 0.794849
2017-12-10T05:54:42.172820: step 9311, loss 0.162862, acc 0.96875, prec 0.119461, recall 0.794879
2017-12-10T05:54:42.446944: step 9312, loss 0.238713, acc 0.96875, prec 0.119468, recall 0.794895
2017-12-10T05:54:42.718983: step 9313, loss 0.0236523, acc 0.984375, prec 0.119477, recall 0.79491
2017-12-10T05:54:42.991291: step 9314, loss 0.380384, acc 0.96875, prec 0.119484, recall 0.794925
2017-12-10T05:54:43.260352: step 9315, loss 0.0152165, acc 0.984375, prec 0.119492, recall 0.79494
2017-12-10T05:54:43.535304: step 9316, loss 0.15622, acc 0.984375, prec 0.119501, recall 0.794955
2017-12-10T05:54:43.801459: step 9317, loss 0.202434, acc 0.984375, prec 0.1195, recall 0.794955
2017-12-10T05:54:44.071893: step 9318, loss 0.0865383, acc 0.984375, prec 0.119508, recall 0.794971
2017-12-10T05:54:44.333786: step 9319, loss 1.1232, acc 0.90625, prec 0.1195, recall 0.794971
2017-12-10T05:54:44.599206: step 9320, loss 0.00244768, acc 1, prec 0.1195, recall 0.794971
2017-12-10T05:54:44.863441: step 9321, loss 0.456966, acc 0.96875, prec 0.119507, recall 0.794986
2017-12-10T05:54:45.127409: step 9322, loss 0.486132, acc 0.984375, prec 0.119535, recall 0.795031
2017-12-10T05:54:45.395200: step 9323, loss 4.82695, acc 0.9375, prec 0.119541, recall 0.794988
2017-12-10T05:54:45.675479: step 9324, loss 0.0295146, acc 0.984375, prec 0.11954, recall 0.794988
2017-12-10T05:54:45.939772: step 9325, loss 0.00700802, acc 1, prec 0.11955, recall 0.795003
2017-12-10T05:54:46.209523: step 9326, loss 0.385019, acc 0.953125, prec 0.119555, recall 0.795018
2017-12-10T05:54:46.474677: step 9327, loss 0.38267, acc 0.96875, prec 0.119563, recall 0.795033
2017-12-10T05:54:46.739229: step 9328, loss 6.10375, acc 0.96875, prec 0.119571, recall 0.79499
2017-12-10T05:54:47.010982: step 9329, loss 0.966653, acc 0.953125, prec 0.119587, recall 0.79502
2017-12-10T05:54:47.282820: step 9330, loss 0.999315, acc 0.875, prec 0.119586, recall 0.795035
2017-12-10T05:54:47.551969: step 9331, loss 0.399726, acc 0.921875, prec 0.119589, recall 0.79505
2017-12-10T05:54:47.822887: step 9332, loss 0.507936, acc 0.9375, prec 0.119593, recall 0.795066
2017-12-10T05:54:48.090833: step 9333, loss 0.862563, acc 0.859375, prec 0.119601, recall 0.795096
2017-12-10T05:54:48.371654: step 9334, loss 0.508167, acc 0.921875, prec 0.119604, recall 0.795111
2017-12-10T05:54:48.650189: step 9335, loss 0.702671, acc 0.890625, prec 0.119615, recall 0.795141
2017-12-10T05:54:48.923933: step 9336, loss 0.750189, acc 0.84375, prec 0.119621, recall 0.795172
2017-12-10T05:54:49.192969: step 9337, loss 0.471347, acc 0.90625, prec 0.119642, recall 0.795217
2017-12-10T05:54:49.469424: step 9338, loss 1.24756, acc 0.828125, prec 0.119628, recall 0.795217
2017-12-10T05:54:49.736442: step 9339, loss 0.788368, acc 0.8125, prec 0.119621, recall 0.795232
2017-12-10T05:54:50.012790: step 9340, loss 0.571269, acc 0.890625, prec 0.119622, recall 0.795248
2017-12-10T05:54:50.275993: step 9341, loss 0.159669, acc 0.9375, prec 0.119617, recall 0.795248
2017-12-10T05:54:50.540674: step 9342, loss 1.34732, acc 0.90625, prec 0.119609, recall 0.795248
2017-12-10T05:54:50.807326: step 9343, loss 1.39984, acc 0.90625, prec 0.119601, recall 0.795248
2017-12-10T05:54:51.080403: step 9344, loss 0.344712, acc 0.9375, prec 0.119605, recall 0.795263
2017-12-10T05:54:51.354020: step 9345, loss 0.847247, acc 0.890625, prec 0.119606, recall 0.795278
2017-12-10T05:54:51.625174: step 9346, loss 0.318218, acc 0.9375, prec 0.1196, recall 0.795278
2017-12-10T05:54:51.894184: step 9347, loss 0.958708, acc 0.875, prec 0.119609, recall 0.795308
2017-12-10T05:54:52.159871: step 9348, loss 1.29457, acc 0.890625, prec 0.119629, recall 0.795354
2017-12-10T05:54:52.429062: step 9349, loss 0.861587, acc 0.921875, prec 0.119632, recall 0.795369
2017-12-10T05:54:52.695704: step 9350, loss 0.159571, acc 0.96875, prec 0.11963, recall 0.795369
2017-12-10T05:54:52.965129: step 9351, loss 0.756371, acc 0.890625, prec 0.11964, recall 0.795399
2017-12-10T05:54:53.234439: step 9352, loss 0.345181, acc 0.96875, prec 0.119647, recall 0.795414
2017-12-10T05:54:53.500894: step 9353, loss 0.495828, acc 0.953125, prec 0.119643, recall 0.795414
2017-12-10T05:54:53.773864: step 9354, loss 0.0943804, acc 0.984375, prec 0.119652, recall 0.795429
2017-12-10T05:54:54.037888: step 9355, loss 0.000764231, acc 1, prec 0.119652, recall 0.795429
2017-12-10T05:54:54.298789: step 9356, loss 0.0357125, acc 0.984375, prec 0.11966, recall 0.795444
2017-12-10T05:54:54.567688: step 9357, loss 0.00229621, acc 1, prec 0.11966, recall 0.795444
2017-12-10T05:54:54.833149: step 9358, loss 0.137661, acc 0.96875, prec 0.119667, recall 0.79546
2017-12-10T05:54:55.100229: step 9359, loss 0.125478, acc 0.984375, prec 0.119676, recall 0.795475
2017-12-10T05:54:55.368626: step 9360, loss 4.78634e-06, acc 1, prec 0.119685, recall 0.79549
2017-12-10T05:54:55.630565: step 9361, loss 0.00103813, acc 1, prec 0.119715, recall 0.795535
2017-12-10T05:54:55.899187: step 9362, loss 0.00510139, acc 1, prec 0.119725, recall 0.79555
2017-12-10T05:54:56.166357: step 9363, loss 0.00516671, acc 1, prec 0.119734, recall 0.795565
2017-12-10T05:54:56.431568: step 9364, loss 0.0452254, acc 0.984375, prec 0.119733, recall 0.795565
2017-12-10T05:54:56.702016: step 9365, loss 0.0871363, acc 0.984375, prec 0.119741, recall 0.795581
2017-12-10T05:54:56.970109: step 9366, loss 0.0168396, acc 0.984375, prec 0.11975, recall 0.795596
2017-12-10T05:54:57.240845: step 9367, loss 0.000993691, acc 1, prec 0.11975, recall 0.795596
2017-12-10T05:54:57.514289: step 9368, loss 0.25952, acc 0.984375, prec 0.119749, recall 0.795596
2017-12-10T05:54:57.780027: step 9369, loss 0.000239436, acc 1, prec 0.119768, recall 0.795626
2017-12-10T05:54:58.041632: step 9370, loss 0.0168093, acc 0.984375, prec 0.119786, recall 0.795656
2017-12-10T05:54:58.309243: step 9371, loss 0.253906, acc 0.984375, prec 0.119795, recall 0.795671
2017-12-10T05:54:58.578711: step 9372, loss 0.260947, acc 0.984375, prec 0.119803, recall 0.795686
2017-12-10T05:54:58.849455: step 9373, loss 0.00391816, acc 1, prec 0.119833, recall 0.795731
2017-12-10T05:54:59.118285: step 9374, loss 3.90233, acc 0.984375, prec 0.119833, recall 0.795673
2017-12-10T05:54:59.383286: step 9375, loss 3.84438e-05, acc 1, prec 0.119843, recall 0.795688
2017-12-10T05:54:59.642792: step 9376, loss 1.50364e-05, acc 1, prec 0.119843, recall 0.795688
2017-12-10T05:54:59.913267: step 9377, loss 0.264181, acc 0.953125, prec 0.119848, recall 0.795703
2017-12-10T05:55:00.175995: step 9378, loss 0.1746, acc 0.96875, prec 0.119846, recall 0.795703
2017-12-10T05:55:00.449502: step 9379, loss 0.301243, acc 0.96875, prec 0.119853, recall 0.795718
2017-12-10T05:55:00.717220: step 9380, loss 0.00142864, acc 1, prec 0.119863, recall 0.795733
2017-12-10T05:55:00.981546: step 9381, loss 0.130543, acc 0.96875, prec 0.11986, recall 0.795733
2017-12-10T05:55:01.252829: step 9382, loss 0.136366, acc 0.96875, prec 0.119867, recall 0.795748
2017-12-10T05:55:01.519869: step 9383, loss 0.25147, acc 0.984375, prec 0.119866, recall 0.795748
2017-12-10T05:55:01.790300: step 9384, loss 0.0634336, acc 0.984375, prec 0.119864, recall 0.795748
2017-12-10T05:55:02.060393: step 9385, loss 0.00172066, acc 1, prec 0.119884, recall 0.795778
2017-12-10T05:55:02.323767: step 9386, loss 0.110991, acc 0.984375, prec 0.119883, recall 0.795778
2017-12-10T05:55:02.596964: step 9387, loss 0.178482, acc 0.9375, prec 0.119887, recall 0.795793
2017-12-10T05:55:02.861907: step 9388, loss 0.548855, acc 0.953125, prec 0.119893, recall 0.795808
2017-12-10T05:55:03.135452: step 9389, loss 0.788607, acc 0.921875, prec 0.119896, recall 0.795824
2017-12-10T05:55:03.399176: step 9390, loss 0.169856, acc 0.96875, prec 0.119913, recall 0.795854
2017-12-10T05:55:03.668128: step 9391, loss 0.196444, acc 0.96875, prec 0.11991, recall 0.795854
2017-12-10T05:55:03.932972: step 9392, loss 0.00101767, acc 1, prec 0.11994, recall 0.795899
2017-12-10T05:55:04.204571: step 9393, loss 0.496609, acc 0.96875, prec 0.119956, recall 0.795929
2017-12-10T05:55:04.469001: step 9394, loss 0.330852, acc 0.984375, prec 0.119994, recall 0.795989
2017-12-10T05:55:04.736297: step 9395, loss 0.0458883, acc 0.96875, prec 0.119992, recall 0.795989
2017-12-10T05:55:05.002838: step 9396, loss 0.00501428, acc 1, prec 0.119992, recall 0.795989
2017-12-10T05:55:05.265659: step 9397, loss 0.0583115, acc 0.984375, prec 0.12, recall 0.796004
2017-12-10T05:55:05.530950: step 9398, loss 0.185187, acc 0.9375, prec 0.119995, recall 0.796004
2017-12-10T05:55:05.792616: step 9399, loss 0.069032, acc 0.984375, prec 0.120003, recall 0.796019
2017-12-10T05:55:06.064736: step 9400, loss 0.232388, acc 0.96875, prec 0.12, recall 0.796019
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-9400

2017-12-10T05:55:07.613123: step 9401, loss 0.170294, acc 0.984375, prec 0.120009, recall 0.796034
2017-12-10T05:55:07.878390: step 9402, loss 0.189026, acc 0.984375, prec 0.120017, recall 0.796049
2017-12-10T05:55:08.142040: step 9403, loss 0.160623, acc 0.96875, prec 0.120024, recall 0.796064
2017-12-10T05:55:08.410840: step 9404, loss 0.049369, acc 0.96875, prec 0.120022, recall 0.796064
2017-12-10T05:55:08.683672: step 9405, loss 0.397889, acc 0.9375, prec 0.120016, recall 0.796064
2017-12-10T05:55:08.950081: step 9406, loss 0.035774, acc 0.96875, prec 0.120014, recall 0.796064
2017-12-10T05:55:09.218174: step 9407, loss 0.356559, acc 0.984375, prec 0.120032, recall 0.796094
2017-12-10T05:55:09.486023: step 9408, loss 0.171656, acc 0.984375, prec 0.120031, recall 0.796094
2017-12-10T05:55:09.756831: step 9409, loss 5.80444e-05, acc 1, prec 0.120031, recall 0.796094
2017-12-10T05:55:10.018548: step 9410, loss 0.289972, acc 0.96875, prec 0.120028, recall 0.796094
2017-12-10T05:55:10.285486: step 9411, loss 0.264758, acc 0.96875, prec 0.120035, recall 0.796109
2017-12-10T05:55:10.554427: step 9412, loss 0.101658, acc 0.96875, prec 0.120042, recall 0.796124
2017-12-10T05:55:10.824314: step 9413, loss 5.20135, acc 0.984375, prec 0.120062, recall 0.796096
2017-12-10T05:55:11.101251: step 9414, loss 0.254394, acc 0.984375, prec 0.12007, recall 0.796111
2017-12-10T05:55:11.368606: step 9415, loss 0.154244, acc 0.96875, prec 0.120077, recall 0.796126
2017-12-10T05:55:11.637754: step 9416, loss 0.151135, acc 0.96875, prec 0.120084, recall 0.796141
2017-12-10T05:55:11.907219: step 9417, loss 0.00391128, acc 1, prec 0.120094, recall 0.796156
2017-12-10T05:55:12.175372: step 9418, loss 0.393789, acc 0.96875, prec 0.120101, recall 0.796171
2017-12-10T05:55:12.449945: step 9419, loss 0.0134228, acc 1, prec 0.120111, recall 0.796186
2017-12-10T05:55:12.716643: step 9420, loss 0.422202, acc 0.96875, prec 0.120108, recall 0.796186
2017-12-10T05:55:12.982401: step 9421, loss 0.0527453, acc 0.96875, prec 0.120116, recall 0.796201
2017-12-10T05:55:13.255952: step 9422, loss 0.258273, acc 0.96875, prec 0.120123, recall 0.796216
2017-12-10T05:55:13.527406: step 9423, loss 0.425349, acc 0.953125, prec 0.120138, recall 0.796246
2017-12-10T05:55:13.795997: step 9424, loss 0.604132, acc 0.953125, prec 0.120134, recall 0.796246
2017-12-10T05:55:14.066422: step 9425, loss 0.672307, acc 0.890625, prec 0.120125, recall 0.796246
2017-12-10T05:55:14.335585: step 9426, loss 0.448838, acc 0.921875, prec 0.120138, recall 0.796276
2017-12-10T05:55:14.604723: step 9427, loss 0.152051, acc 0.984375, prec 0.120156, recall 0.796306
2017-12-10T05:55:14.875208: step 9428, loss 0.302168, acc 0.953125, prec 0.120152, recall 0.796306
2017-12-10T05:55:15.153996: step 9429, loss 0.464893, acc 0.9375, prec 0.120166, recall 0.796336
2017-12-10T05:55:15.420998: step 9430, loss 0.764944, acc 0.953125, prec 0.120172, recall 0.796351
2017-12-10T05:55:15.687002: step 9431, loss 0.578809, acc 0.921875, prec 0.120175, recall 0.796366
2017-12-10T05:55:15.947976: step 9432, loss 0.167436, acc 0.96875, prec 0.120172, recall 0.796366
2017-12-10T05:55:16.215540: step 9433, loss 0.24434, acc 0.96875, prec 0.12017, recall 0.796366
2017-12-10T05:55:16.481896: step 9434, loss 2.75863, acc 0.921875, prec 0.120203, recall 0.796367
2017-12-10T05:55:16.759429: step 9435, loss 0.223355, acc 0.953125, prec 0.120209, recall 0.796382
2017-12-10T05:55:17.027445: step 9436, loss 0.0790516, acc 0.96875, prec 0.120216, recall 0.796397
2017-12-10T05:55:17.292927: step 9437, loss 0.179838, acc 0.96875, prec 0.120214, recall 0.796397
2017-12-10T05:55:17.556405: step 9438, loss 0.415288, acc 0.96875, prec 0.120211, recall 0.796397
2017-12-10T05:55:17.820736: step 9439, loss 0.236814, acc 0.9375, prec 0.120206, recall 0.796397
2017-12-10T05:55:18.090784: step 9440, loss 0.0400561, acc 0.96875, prec 0.120203, recall 0.796397
2017-12-10T05:55:18.363550: step 9441, loss 1.29233, acc 0.828125, prec 0.120188, recall 0.796397
2017-12-10T05:55:18.631975: step 9442, loss 0.441751, acc 0.921875, prec 0.120201, recall 0.796427
2017-12-10T05:55:18.872367: step 9443, loss 0.477626, acc 0.942308, prec 0.120197, recall 0.796427
2017-12-10T05:55:19.144310: step 9444, loss 0.373077, acc 0.921875, prec 0.12021, recall 0.796457
2017-12-10T05:55:19.407274: step 9445, loss 0.153331, acc 0.953125, prec 0.120206, recall 0.796457
2017-12-10T05:55:19.677792: step 9446, loss 0.573485, acc 0.953125, prec 0.120202, recall 0.796457
2017-12-10T05:55:19.944431: step 9447, loss 0.387563, acc 0.90625, prec 0.120204, recall 0.796472
2017-12-10T05:55:20.215154: step 9448, loss 0.712376, acc 0.90625, prec 0.120196, recall 0.796472
2017-12-10T05:55:20.477415: step 9449, loss 0.528174, acc 0.921875, prec 0.120209, recall 0.796502
2017-12-10T05:55:20.743347: step 9450, loss 0.192084, acc 0.953125, prec 0.120214, recall 0.796517
2017-12-10T05:55:21.014218: step 9451, loss 0.0207257, acc 0.984375, prec 0.120223, recall 0.796532
2017-12-10T05:55:21.285048: step 9452, loss 0.0623715, acc 0.96875, prec 0.12024, recall 0.796562
2017-12-10T05:55:21.554447: step 9453, loss 0.238506, acc 0.9375, prec 0.120234, recall 0.796562
2017-12-10T05:55:21.819659: step 9454, loss 0.251762, acc 0.9375, prec 0.120239, recall 0.796577
2017-12-10T05:55:22.090387: step 9455, loss 0.245386, acc 0.953125, prec 0.120264, recall 0.796621
2017-12-10T05:55:22.355796: step 9456, loss 0.140355, acc 0.984375, prec 0.120263, recall 0.796621
2017-12-10T05:55:22.623733: step 9457, loss 0.210354, acc 0.96875, prec 0.12027, recall 0.796636
2017-12-10T05:55:22.891228: step 9458, loss 0.500653, acc 0.921875, prec 0.120282, recall 0.796666
2017-12-10T05:55:23.157635: step 9459, loss 0.000536753, acc 1, prec 0.120282, recall 0.796666
2017-12-10T05:55:23.422721: step 9460, loss 0.05424, acc 0.984375, prec 0.12031, recall 0.796711
2017-12-10T05:55:23.694604: step 9461, loss 0.617362, acc 0.96875, prec 0.120318, recall 0.796726
2017-12-10T05:55:23.969346: step 9462, loss 0.3579, acc 0.9375, prec 0.120351, recall 0.796786
2017-12-10T05:55:24.246226: step 9463, loss 0.0493827, acc 0.984375, prec 0.12036, recall 0.7968
2017-12-10T05:55:24.510947: step 9464, loss 0.198681, acc 0.96875, prec 0.120376, recall 0.79683
2017-12-10T05:55:24.779297: step 9465, loss 0.354912, acc 0.96875, prec 0.120393, recall 0.79686
2017-12-10T05:55:25.047711: step 9466, loss 0.606405, acc 0.96875, prec 0.12041, recall 0.79689
2017-12-10T05:55:25.313585: step 9467, loss 0.0466331, acc 0.984375, prec 0.120418, recall 0.796905
2017-12-10T05:55:25.577141: step 9468, loss 0.0517516, acc 0.984375, prec 0.120437, recall 0.796935
2017-12-10T05:55:25.842660: step 9469, loss 0.464108, acc 0.9375, prec 0.120451, recall 0.796964
2017-12-10T05:55:26.122993: step 9470, loss 0.00304545, acc 1, prec 0.120451, recall 0.796964
2017-12-10T05:55:26.397478: step 9471, loss 0.0914683, acc 0.984375, prec 0.120449, recall 0.796964
2017-12-10T05:55:26.676432: step 9472, loss 0.111686, acc 0.984375, prec 0.120458, recall 0.796979
2017-12-10T05:55:26.950654: step 9473, loss 0.359647, acc 0.953125, prec 0.120454, recall 0.796979
2017-12-10T05:55:27.223451: step 9474, loss 0.00552136, acc 1, prec 0.120454, recall 0.796979
2017-12-10T05:55:27.495441: step 9475, loss 0.00747979, acc 1, prec 0.120454, recall 0.796979
2017-12-10T05:55:27.764800: step 9476, loss 0.197225, acc 0.953125, prec 0.12045, recall 0.796979
2017-12-10T05:55:28.037525: step 9477, loss 0.0349625, acc 0.984375, prec 0.120449, recall 0.796979
2017-12-10T05:55:28.304651: step 9478, loss 0.339983, acc 0.96875, prec 0.120456, recall 0.796994
2017-12-10T05:55:28.569971: step 9479, loss 0.140042, acc 0.96875, prec 0.120472, recall 0.797024
2017-12-10T05:55:28.839430: step 9480, loss 0.171411, acc 0.96875, prec 0.12048, recall 0.797039
2017-12-10T05:55:29.101186: step 9481, loss 0.00394141, acc 1, prec 0.120489, recall 0.797054
2017-12-10T05:55:29.368579: step 9482, loss 0.0793264, acc 0.984375, prec 0.120498, recall 0.797069
2017-12-10T05:55:29.636613: step 9483, loss 0.00111982, acc 1, prec 0.120498, recall 0.797069
2017-12-10T05:55:29.902498: step 9484, loss 0.110356, acc 0.984375, prec 0.120506, recall 0.797083
2017-12-10T05:55:30.174172: step 9485, loss 0.00410622, acc 1, prec 0.120506, recall 0.797083
2017-12-10T05:55:30.454900: step 9486, loss 4.51376e-05, acc 1, prec 0.120506, recall 0.797083
2017-12-10T05:55:30.720591: step 9487, loss 3.15333, acc 0.984375, prec 0.120506, recall 0.797025
2017-12-10T05:55:30.986355: step 9488, loss 0.0237687, acc 0.984375, prec 0.120505, recall 0.797025
2017-12-10T05:55:31.251463: step 9489, loss 0.00677721, acc 1, prec 0.120514, recall 0.79704
2017-12-10T05:55:31.515038: step 9490, loss 0.102039, acc 0.984375, prec 0.120533, recall 0.79707
2017-12-10T05:55:31.782823: step 9491, loss 0.242751, acc 0.984375, prec 0.120531, recall 0.79707
2017-12-10T05:55:32.048171: step 9492, loss 0.23922, acc 0.96875, prec 0.120538, recall 0.797084
2017-12-10T05:55:32.313546: step 9493, loss 0.957022, acc 0.953125, prec 0.120544, recall 0.797099
2017-12-10T05:55:32.577716: step 9494, loss 0.103406, acc 0.953125, prec 0.12054, recall 0.797099
2017-12-10T05:55:32.843753: step 9495, loss 0.0241991, acc 0.984375, prec 0.120539, recall 0.797099
2017-12-10T05:55:33.107774: step 9496, loss 0.0521115, acc 0.984375, prec 0.120557, recall 0.797129
2017-12-10T05:55:34.075181: step 9497, loss 0.636606, acc 0.9375, prec 0.120571, recall 0.797159
2017-12-10T05:55:34.436311: step 9498, loss 0.903488, acc 0.9375, prec 0.120575, recall 0.797174
2017-12-10T05:55:34.707785: step 9499, loss 0.0270541, acc 0.984375, prec 0.120574, recall 0.797174
2017-12-10T05:55:35.163370: step 9500, loss 0.645704, acc 0.953125, prec 0.12058, recall 0.797188
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-9500

2017-12-10T05:55:36.688862: step 9501, loss 0.0497764, acc 1, prec 0.120609, recall 0.797233
2017-12-10T05:55:37.013786: step 9502, loss 0.0811858, acc 0.984375, prec 0.120617, recall 0.797248
2017-12-10T05:55:37.339754: step 9503, loss 0.337617, acc 0.9375, prec 0.120632, recall 0.797278
2017-12-10T05:55:37.615381: step 9504, loss 0.262491, acc 0.953125, prec 0.120628, recall 0.797278
2017-12-10T05:55:37.890962: step 9505, loss 0.326743, acc 0.96875, prec 0.120625, recall 0.797278
2017-12-10T05:55:38.155481: step 9506, loss 0.247903, acc 0.9375, prec 0.120639, recall 0.797307
2017-12-10T05:55:38.429511: step 9507, loss 1.36541, acc 0.90625, prec 0.120641, recall 0.797322
2017-12-10T05:55:38.693876: step 9508, loss 0.313993, acc 0.984375, prec 0.120639, recall 0.797322
2017-12-10T05:55:38.960241: step 9509, loss 0.00211425, acc 1, prec 0.120659, recall 0.797352
2017-12-10T05:55:39.223653: step 9510, loss 0.077401, acc 0.96875, prec 0.120656, recall 0.797352
2017-12-10T05:55:39.487093: step 9511, loss 0.000122417, acc 1, prec 0.120666, recall 0.797366
2017-12-10T05:55:39.749822: step 9512, loss 0.254487, acc 0.953125, prec 0.120672, recall 0.797381
2017-12-10T05:55:40.012271: step 9513, loss 0.0260261, acc 0.984375, prec 0.12069, recall 0.797411
2017-12-10T05:55:40.278145: step 9514, loss 0.0484861, acc 0.984375, prec 0.120698, recall 0.797426
2017-12-10T05:55:40.541460: step 9515, loss 0.00121062, acc 1, prec 0.120708, recall 0.797441
2017-12-10T05:55:40.803349: step 9516, loss 0.452827, acc 0.9375, prec 0.120712, recall 0.797455
2017-12-10T05:55:41.068495: step 9517, loss 0.0480877, acc 0.984375, prec 0.120711, recall 0.797455
2017-12-10T05:55:41.334939: step 9518, loss 3.80644e-05, acc 1, prec 0.120711, recall 0.797455
2017-12-10T05:55:41.595413: step 9519, loss 0.0258109, acc 1, prec 0.120721, recall 0.79747
2017-12-10T05:55:41.862464: step 9520, loss 0.209978, acc 0.984375, prec 0.120719, recall 0.79747
2017-12-10T05:55:42.129196: step 9521, loss 0.0562758, acc 0.96875, prec 0.120736, recall 0.7975
2017-12-10T05:55:42.409339: step 9522, loss 0.43978, acc 0.96875, prec 0.120734, recall 0.7975
2017-12-10T05:55:42.675502: step 9523, loss 0.231579, acc 0.984375, prec 0.120732, recall 0.7975
2017-12-10T05:55:42.941843: step 9524, loss 0.465316, acc 0.953125, prec 0.120728, recall 0.7975
2017-12-10T05:55:43.211438: step 9525, loss 0.00137468, acc 1, prec 0.120728, recall 0.7975
2017-12-10T05:55:43.485372: step 9526, loss 0.00293703, acc 1, prec 0.120748, recall 0.797529
2017-12-10T05:55:43.754997: step 9527, loss 0.000759594, acc 1, prec 0.120748, recall 0.797529
2017-12-10T05:55:44.029855: step 9528, loss 0.191838, acc 0.96875, prec 0.120755, recall 0.797544
2017-12-10T05:55:44.296687: step 9529, loss 0.00347818, acc 1, prec 0.120755, recall 0.797544
2017-12-10T05:55:44.563932: step 9530, loss 0.00509436, acc 1, prec 0.120755, recall 0.797544
2017-12-10T05:55:44.826382: step 9531, loss 0.489249, acc 0.9375, prec 0.120759, recall 0.797559
2017-12-10T05:55:45.082807: step 9532, loss 0.222563, acc 0.984375, prec 0.120758, recall 0.797559
2017-12-10T05:55:45.345371: step 9533, loss 0.292695, acc 0.953125, prec 0.120773, recall 0.797589
2017-12-10T05:55:45.613388: step 9534, loss 0.338149, acc 0.984375, prec 0.120811, recall 0.797648
2017-12-10T05:55:45.880719: step 9535, loss 0.328962, acc 0.984375, prec 0.120809, recall 0.797648
2017-12-10T05:55:46.146286: step 9536, loss 0.0569595, acc 0.984375, prec 0.120818, recall 0.797663
2017-12-10T05:55:46.411451: step 9537, loss 0.0834983, acc 0.984375, prec 0.120826, recall 0.797677
2017-12-10T05:55:46.684262: step 9538, loss 0.0375948, acc 0.984375, prec 0.120835, recall 0.797692
2017-12-10T05:55:46.949040: step 9539, loss 0.466394, acc 0.96875, prec 0.120832, recall 0.797692
2017-12-10T05:55:47.219068: step 9540, loss 0.168024, acc 0.96875, prec 0.120839, recall 0.797707
2017-12-10T05:55:47.493051: step 9541, loss 0.00400299, acc 1, prec 0.120839, recall 0.797707
2017-12-10T05:55:47.761498: step 9542, loss 0.00122912, acc 1, prec 0.120849, recall 0.797722
2017-12-10T05:55:48.026396: step 9543, loss 0.130622, acc 0.984375, prec 0.120847, recall 0.797722
2017-12-10T05:55:48.295722: step 9544, loss 0.196305, acc 0.984375, prec 0.120866, recall 0.797751
2017-12-10T05:55:48.563553: step 9545, loss 0.02386, acc 0.984375, prec 0.120874, recall 0.797766
2017-12-10T05:55:48.825283: step 9546, loss 0.0149726, acc 1, prec 0.120884, recall 0.797781
2017-12-10T05:55:49.086907: step 9547, loss 0.206051, acc 0.984375, prec 0.120902, recall 0.79781
2017-12-10T05:55:49.353833: step 9548, loss 1.48261e-06, acc 1, prec 0.120911, recall 0.797825
2017-12-10T05:55:49.616706: step 9549, loss 0.147348, acc 0.984375, prec 0.12093, recall 0.797854
2017-12-10T05:55:49.884093: step 9550, loss 2.73561, acc 0.96875, prec 0.120928, recall 0.797796
2017-12-10T05:55:50.157435: step 9551, loss 2.67633, acc 0.953125, prec 0.120926, recall 0.797738
2017-12-10T05:55:50.422697: step 9552, loss 0.171358, acc 0.984375, prec 0.120924, recall 0.797738
2017-12-10T05:55:50.687728: step 9553, loss 0.937013, acc 0.921875, prec 0.120918, recall 0.797738
2017-12-10T05:55:50.956273: step 9554, loss 0.183681, acc 0.953125, prec 0.120933, recall 0.797768
2017-12-10T05:55:51.224344: step 9555, loss 0.198872, acc 0.953125, prec 0.120929, recall 0.797768
2017-12-10T05:55:51.491020: step 9556, loss 0.0451142, acc 0.96875, prec 0.120936, recall 0.797782
2017-12-10T05:55:51.758024: step 9557, loss 0.0276936, acc 0.984375, prec 0.120944, recall 0.797797
2017-12-10T05:55:52.031664: step 9558, loss 0.465787, acc 0.984375, prec 0.120943, recall 0.797797
2017-12-10T05:55:52.299289: step 9559, loss 0.408397, acc 0.953125, prec 0.120939, recall 0.797797
2017-12-10T05:55:52.567337: step 9560, loss 0.842975, acc 0.9375, prec 0.120943, recall 0.797812
2017-12-10T05:55:52.835402: step 9561, loss 0.59061, acc 0.9375, prec 0.120948, recall 0.797827
2017-12-10T05:55:53.106547: step 9562, loss 0.606689, acc 0.921875, prec 0.120941, recall 0.797827
2017-12-10T05:55:53.370680: step 9563, loss 0.0485375, acc 0.984375, prec 0.120949, recall 0.797841
2017-12-10T05:55:53.640989: step 9564, loss 0.357311, acc 0.921875, prec 0.120943, recall 0.797841
2017-12-10T05:55:53.907152: step 9565, loss 0.257744, acc 0.921875, prec 0.120936, recall 0.797841
2017-12-10T05:55:54.176409: step 9566, loss 0.477792, acc 0.96875, prec 0.120943, recall 0.797856
2017-12-10T05:55:54.439393: step 9567, loss 0.101572, acc 0.984375, prec 0.120942, recall 0.797856
2017-12-10T05:55:54.709750: step 9568, loss 0.186796, acc 0.953125, prec 0.120948, recall 0.797871
2017-12-10T05:55:54.985411: step 9569, loss 0.192326, acc 0.984375, prec 0.120946, recall 0.797871
2017-12-10T05:55:55.251365: step 9570, loss 0.639666, acc 0.90625, prec 0.120948, recall 0.797886
2017-12-10T05:55:55.522437: step 9571, loss 0.197772, acc 0.96875, prec 0.120955, recall 0.7979
2017-12-10T05:55:55.793176: step 9572, loss 0.271265, acc 0.984375, prec 0.120973, recall 0.79793
2017-12-10T05:55:56.061176: step 9573, loss 0.129736, acc 0.953125, prec 0.120988, recall 0.797959
2017-12-10T05:55:56.332081: step 9574, loss 0.135041, acc 0.984375, prec 0.121007, recall 0.797989
2017-12-10T05:55:56.600097: step 9575, loss 0.599412, acc 0.921875, prec 0.12101, recall 0.798003
2017-12-10T05:55:56.865080: step 9576, loss 0.0895866, acc 0.953125, prec 0.121006, recall 0.798003
2017-12-10T05:55:57.131319: step 9577, loss 0.286973, acc 0.96875, prec 0.121022, recall 0.798033
2017-12-10T05:55:57.400292: step 9578, loss 0.958698, acc 0.9375, prec 0.121027, recall 0.798047
2017-12-10T05:55:57.672822: step 9579, loss 0.0210786, acc 0.984375, prec 0.121025, recall 0.798047
2017-12-10T05:55:57.949424: step 9580, loss 0.0882314, acc 0.984375, prec 0.121034, recall 0.798062
2017-12-10T05:55:58.216616: step 9581, loss 0.121262, acc 0.96875, prec 0.12106, recall 0.798106
2017-12-10T05:55:58.492784: step 9582, loss 0.814648, acc 0.9375, prec 0.121055, recall 0.798106
2017-12-10T05:55:58.762565: step 9583, loss 0.138483, acc 0.96875, prec 0.121072, recall 0.798136
2017-12-10T05:55:59.038155: step 9584, loss 0.063984, acc 0.96875, prec 0.121088, recall 0.798165
2017-12-10T05:55:59.302836: step 9585, loss 0.0242152, acc 0.984375, prec 0.121087, recall 0.798165
2017-12-10T05:55:59.571893: step 9586, loss 0.32041, acc 0.953125, prec 0.121083, recall 0.798165
2017-12-10T05:55:59.838309: step 9587, loss 0.012978, acc 1, prec 0.121083, recall 0.798165
2017-12-10T05:56:00.114439: step 9588, loss 0.179147, acc 0.984375, prec 0.121091, recall 0.79818
2017-12-10T05:56:00.385544: step 9589, loss 0.0285579, acc 0.984375, prec 0.12109, recall 0.79818
2017-12-10T05:56:00.655402: step 9590, loss 0.0899255, acc 0.96875, prec 0.121087, recall 0.79818
2017-12-10T05:56:00.921899: step 9591, loss 0.0701587, acc 1, prec 0.121116, recall 0.798224
2017-12-10T05:56:01.188373: step 9592, loss 0.276215, acc 0.953125, prec 0.121122, recall 0.798239
2017-12-10T05:56:01.454975: step 9593, loss 8.21641e-05, acc 1, prec 0.121122, recall 0.798239
2017-12-10T05:56:01.719391: step 9594, loss 0.325302, acc 0.984375, prec 0.121121, recall 0.798239
2017-12-10T05:56:01.990880: step 9595, loss 0.503706, acc 0.96875, prec 0.121128, recall 0.798253
2017-12-10T05:56:02.266394: step 9596, loss 0.276128, acc 0.984375, prec 0.121136, recall 0.798268
2017-12-10T05:56:02.535090: step 9597, loss 0.000199254, acc 1, prec 0.121156, recall 0.798297
2017-12-10T05:56:02.799239: step 9598, loss 5.07206, acc 0.984375, prec 0.121156, recall 0.798239
2017-12-10T05:56:03.069543: step 9599, loss 8.55322, acc 0.96875, prec 0.121164, recall 0.798196
2017-12-10T05:56:03.338654: step 9600, loss 0.558836, acc 0.96875, prec 0.121181, recall 0.798225

Evaluation:
2017-12-10T05:56:10.923570: step 9600, loss 12.623, acc 0.952444, prec 0.121274, recall 0.794145

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-9600

2017-12-10T05:56:12.199904: step 9601, loss 0.025875, acc 0.984375, prec 0.121282, recall 0.79416
2017-12-10T05:56:12.476379: step 9602, loss 0.608903, acc 0.90625, prec 0.121293, recall 0.79419
2017-12-10T05:56:12.743910: step 9603, loss 0.218063, acc 0.9375, prec 0.121288, recall 0.79419
2017-12-10T05:56:13.012252: step 9604, loss 0.634925, acc 0.90625, prec 0.12128, recall 0.79419
2017-12-10T05:56:13.278838: step 9605, loss 0.154463, acc 0.96875, prec 0.121277, recall 0.79419
2017-12-10T05:56:13.543438: step 9606, loss 0.430829, acc 0.921875, prec 0.12128, recall 0.794204
2017-12-10T05:56:13.809525: step 9607, loss 0.790864, acc 0.921875, prec 0.121274, recall 0.794204
2017-12-10T05:56:14.076907: step 9608, loss 0.904441, acc 0.953125, prec 0.121289, recall 0.794234
2017-12-10T05:56:14.339999: step 9609, loss 0.704943, acc 0.875, prec 0.121298, recall 0.794264
2017-12-10T05:56:14.607082: step 9610, loss 1.71921, acc 0.84375, prec 0.121294, recall 0.794278
2017-12-10T05:56:14.876438: step 9611, loss 0.616904, acc 0.90625, prec 0.121286, recall 0.794278
2017-12-10T05:56:15.143666: step 9612, loss 1.21515, acc 0.875, prec 0.121295, recall 0.794308
2017-12-10T05:56:15.403591: step 9613, loss 1.80895, acc 0.796875, prec 0.121297, recall 0.794337
2017-12-10T05:56:15.665790: step 9614, loss 0.619204, acc 0.953125, prec 0.121312, recall 0.794367
2017-12-10T05:56:15.933413: step 9615, loss 1.41306, acc 0.859375, prec 0.121329, recall 0.794411
2017-12-10T05:56:16.197904: step 9616, loss 1.55462, acc 0.78125, prec 0.12132, recall 0.794426
2017-12-10T05:56:16.462328: step 9617, loss 0.89227, acc 0.859375, prec 0.121317, recall 0.794441
2017-12-10T05:56:16.728499: step 9618, loss 0.360957, acc 0.921875, prec 0.121311, recall 0.794441
2017-12-10T05:56:16.992077: step 9619, loss 0.518311, acc 0.875, prec 0.1213, recall 0.794441
2017-12-10T05:56:17.263274: step 9620, loss 1.47474, acc 0.890625, prec 0.121291, recall 0.794441
2017-12-10T05:56:17.526466: step 9621, loss 0.283833, acc 0.96875, prec 0.121307, recall 0.79447
2017-12-10T05:56:17.800820: step 9622, loss 0.345328, acc 0.9375, prec 0.121302, recall 0.79447
2017-12-10T05:56:18.066398: step 9623, loss 0.195932, acc 0.984375, prec 0.121301, recall 0.79447
2017-12-10T05:56:18.331251: step 9624, loss 0.152862, acc 0.984375, prec 0.121319, recall 0.7945
2017-12-10T05:56:18.601246: step 9625, loss 0.468686, acc 0.9375, prec 0.121313, recall 0.7945
2017-12-10T05:56:18.865983: step 9626, loss 0.303435, acc 0.953125, prec 0.121348, recall 0.794559
2017-12-10T05:56:19.139096: step 9627, loss 0.552622, acc 0.921875, prec 0.121351, recall 0.794574
2017-12-10T05:56:19.406529: step 9628, loss 0.234901, acc 0.984375, prec 0.12135, recall 0.794574
2017-12-10T05:56:19.670008: step 9629, loss 0.0601248, acc 0.984375, prec 0.121348, recall 0.794574
2017-12-10T05:56:20.633543: step 9630, loss 0.0348717, acc 0.96875, prec 0.121346, recall 0.794574
2017-12-10T05:56:20.988610: step 9631, loss 0.955615, acc 0.953125, prec 0.121371, recall 0.794618
2017-12-10T05:56:21.253865: step 9632, loss 0.0109814, acc 1, prec 0.12138, recall 0.794633
2017-12-10T05:56:21.976856: step 9633, loss 0.537327, acc 0.953125, prec 0.121386, recall 0.794647
2017-12-10T05:56:22.699815: step 9634, loss 0.551082, acc 0.96875, prec 0.121393, recall 0.794662
2017-12-10T05:56:23.423897: step 9635, loss 2.48274e-06, acc 1, prec 0.121393, recall 0.794662
2017-12-10T05:56:24.183042: step 9636, loss 0.00042285, acc 1, prec 0.121393, recall 0.794662
2017-12-10T05:56:25.154803: step 9637, loss 0.201333, acc 0.96875, prec 0.12139, recall 0.794662
2017-12-10T05:56:25.496406: step 9638, loss 0.148964, acc 0.96875, prec 0.121397, recall 0.794677
2017-12-10T05:56:25.838108: step 9639, loss 9.46059e-06, acc 1, prec 0.121407, recall 0.794692
2017-12-10T05:56:26.109802: step 9640, loss 0.000589429, acc 1, prec 0.121416, recall 0.794706
2017-12-10T05:56:26.389693: step 9641, loss 0.409872, acc 0.984375, prec 0.121444, recall 0.79475
2017-12-10T05:56:26.680085: step 9642, loss 0.00985364, acc 1, prec 0.121454, recall 0.794765
2017-12-10T05:56:26.962658: step 9643, loss 0.167947, acc 0.96875, prec 0.12147, recall 0.794795
2017-12-10T05:56:27.250425: step 9644, loss 0.000458037, acc 1, prec 0.12147, recall 0.794795
2017-12-10T05:56:27.514750: step 9645, loss 8.02974e-05, acc 1, prec 0.121489, recall 0.794824
2017-12-10T05:56:27.780376: step 9646, loss 0.0616169, acc 0.984375, prec 0.121498, recall 0.794839
2017-12-10T05:56:28.047614: step 9647, loss 0.000272473, acc 1, prec 0.121498, recall 0.794839
2017-12-10T05:56:28.306722: step 9648, loss 0.999518, acc 0.96875, prec 0.121514, recall 0.794868
2017-12-10T05:56:28.571559: step 9649, loss 0.132703, acc 0.984375, prec 0.121523, recall 0.794883
2017-12-10T05:56:28.835086: step 9650, loss 0.0241888, acc 0.984375, prec 0.121521, recall 0.794883
2017-12-10T05:56:29.110603: step 9651, loss 0.123203, acc 0.96875, prec 0.121519, recall 0.794883
2017-12-10T05:56:29.375339: step 9652, loss 0.286741, acc 0.984375, prec 0.121517, recall 0.794883
2017-12-10T05:56:29.650082: step 9653, loss 0.00279197, acc 1, prec 0.121517, recall 0.794883
2017-12-10T05:56:29.908190: step 9654, loss 7.01838e-05, acc 1, prec 0.121517, recall 0.794883
2017-12-10T05:56:30.168860: step 9655, loss 0.60699, acc 0.984375, prec 0.121526, recall 0.794897
2017-12-10T05:56:30.440455: step 9656, loss 6.96332e-05, acc 1, prec 0.121535, recall 0.794912
2017-12-10T05:56:30.718006: step 9657, loss 0.0470761, acc 0.984375, prec 0.121534, recall 0.794912
2017-12-10T05:56:30.980205: step 9658, loss 0.233134, acc 1, prec 0.121553, recall 0.794942
2017-12-10T05:56:31.242593: step 9659, loss 6.77423e-06, acc 1, prec 0.121563, recall 0.794956
2017-12-10T05:56:31.498679: step 9660, loss 0.0372824, acc 0.984375, prec 0.121571, recall 0.794971
2017-12-10T05:56:31.766267: step 9661, loss 0.00830948, acc 1, prec 0.121581, recall 0.794986
2017-12-10T05:56:32.036786: step 9662, loss 1.11075, acc 0.953125, prec 0.121586, recall 0.795
2017-12-10T05:56:32.301375: step 9663, loss 1.62601e-06, acc 1, prec 0.121586, recall 0.795
2017-12-10T05:56:32.563854: step 9664, loss 0.369121, acc 0.984375, prec 0.121585, recall 0.795
2017-12-10T05:56:32.830751: step 9665, loss 0.101912, acc 0.984375, prec 0.121584, recall 0.795
2017-12-10T05:56:33.092696: step 9666, loss 0.0169246, acc 0.984375, prec 0.121601, recall 0.79503
2017-12-10T05:56:33.355106: step 9667, loss 2.09291, acc 0.984375, prec 0.121601, recall 0.794973
2017-12-10T05:56:33.629438: step 9668, loss 7.13103e-05, acc 1, prec 0.121601, recall 0.794973
2017-12-10T05:56:33.891217: step 9669, loss 0.158963, acc 0.984375, prec 0.12161, recall 0.794987
2017-12-10T05:56:34.157210: step 9670, loss 0.000162572, acc 1, prec 0.12161, recall 0.794987
2017-12-10T05:56:34.416115: step 9671, loss 0.544784, acc 0.96875, prec 0.121626, recall 0.795017
2017-12-10T05:56:34.688213: step 9672, loss 8.49322e-05, acc 1, prec 0.121626, recall 0.795017
2017-12-10T05:56:34.952603: step 9673, loss 0.27926, acc 0.9375, prec 0.121621, recall 0.795017
2017-12-10T05:56:35.216706: step 9674, loss 0.34461, acc 0.96875, prec 0.121647, recall 0.795061
2017-12-10T05:56:35.491439: step 9675, loss 0.258104, acc 0.96875, prec 0.121664, recall 0.79509
2017-12-10T05:56:35.757649: step 9676, loss 0.803969, acc 0.90625, prec 0.121656, recall 0.79509
2017-12-10T05:56:36.018823: step 9677, loss 0.0802185, acc 0.96875, prec 0.121672, recall 0.79512
2017-12-10T05:56:36.290294: step 9678, loss 0.0388597, acc 1, prec 0.121701, recall 0.795164
2017-12-10T05:56:36.555622: step 9679, loss 0.2603, acc 0.96875, prec 0.121699, recall 0.795164
2017-12-10T05:56:36.815812: step 9680, loss 0.238776, acc 0.984375, prec 0.121707, recall 0.795178
2017-12-10T05:56:37.083533: step 9681, loss 0.487326, acc 0.953125, prec 0.121722, recall 0.795207
2017-12-10T05:56:37.348237: step 9682, loss 0.0108068, acc 1, prec 0.121732, recall 0.795222
2017-12-10T05:56:37.619264: step 9683, loss 0.199012, acc 0.96875, prec 0.121748, recall 0.795251
2017-12-10T05:56:37.887549: step 9684, loss 0.296922, acc 0.96875, prec 0.121755, recall 0.795266
2017-12-10T05:56:38.150629: step 9685, loss 0.0523239, acc 0.96875, prec 0.121762, recall 0.795281
2017-12-10T05:56:38.418918: step 9686, loss 0.0206393, acc 0.984375, prec 0.12177, recall 0.795295
2017-12-10T05:56:38.685131: step 9687, loss 0.59062, acc 0.96875, prec 0.121777, recall 0.79531
2017-12-10T05:56:38.958278: step 9688, loss 0.449605, acc 0.96875, prec 0.121804, recall 0.795354
2017-12-10T05:56:39.223930: step 9689, loss 0.440485, acc 0.953125, prec 0.121819, recall 0.795383
2017-12-10T05:56:39.491371: step 9690, loss 0.000320741, acc 1, prec 0.121819, recall 0.795383
2017-12-10T05:56:39.760109: step 9691, loss 0.000206063, acc 1, prec 0.121819, recall 0.795383
2017-12-10T05:56:40.020391: step 9692, loss 0.142966, acc 0.96875, prec 0.121826, recall 0.795398
2017-12-10T05:56:40.290401: step 9693, loss 0.152013, acc 0.96875, prec 0.121833, recall 0.795412
2017-12-10T05:56:40.559594: step 9694, loss 0.00408232, acc 1, prec 0.121852, recall 0.795442
2017-12-10T05:56:40.826445: step 9695, loss 0.0318802, acc 0.984375, prec 0.12186, recall 0.795456
2017-12-10T05:56:41.090055: step 9696, loss 0.0421837, acc 0.984375, prec 0.121859, recall 0.795456
2017-12-10T05:56:41.352400: step 9697, loss 1.80574, acc 0.96875, prec 0.121858, recall 0.795399
2017-12-10T05:56:41.627625: step 9698, loss 0.0539684, acc 0.984375, prec 0.121875, recall 0.795429
2017-12-10T05:56:41.893006: step 9699, loss 0.000221137, acc 1, prec 0.121875, recall 0.795429
2017-12-10T05:56:42.157630: step 9700, loss 0.215324, acc 0.96875, prec 0.121882, recall 0.795443
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-9700

2017-12-10T05:56:43.613547: step 9701, loss 0.0330678, acc 0.984375, prec 0.121881, recall 0.795443
2017-12-10T05:56:43.882085: step 9702, loss 0.458356, acc 0.96875, prec 0.121888, recall 0.795458
2017-12-10T05:56:44.149101: step 9703, loss 0.0893176, acc 0.96875, prec 0.121905, recall 0.795487
2017-12-10T05:56:44.417123: step 9704, loss 0.650262, acc 0.953125, prec 0.12191, recall 0.795502
2017-12-10T05:56:44.682927: step 9705, loss 0.0219078, acc 0.984375, prec 0.121909, recall 0.795502
2017-12-10T05:56:44.952636: step 9706, loss 0.10712, acc 0.96875, prec 0.121916, recall 0.795516
2017-12-10T05:56:45.217523: step 9707, loss 0.00431935, acc 1, prec 0.121935, recall 0.795545
2017-12-10T05:56:45.484115: step 9708, loss 0.445082, acc 0.9375, prec 0.121949, recall 0.795575
2017-12-10T05:56:45.750378: step 9709, loss 0.00675157, acc 1, prec 0.121949, recall 0.795575
2017-12-10T05:56:46.022676: step 9710, loss 0.311118, acc 0.9375, prec 0.121963, recall 0.795604
2017-12-10T05:56:46.294036: step 9711, loss 0.00148905, acc 1, prec 0.121963, recall 0.795604
2017-12-10T05:56:46.559881: step 9712, loss 0.276437, acc 0.96875, prec 0.12197, recall 0.795618
2017-12-10T05:56:46.820642: step 9713, loss 1.42745, acc 0.953125, prec 0.121967, recall 0.795562
2017-12-10T05:56:47.090084: step 9714, loss 0.163358, acc 0.953125, prec 0.121973, recall 0.795576
2017-12-10T05:56:47.352975: step 9715, loss 0.892794, acc 0.9375, prec 0.121967, recall 0.795576
2017-12-10T05:56:47.619598: step 9716, loss 0.156539, acc 0.984375, prec 0.121975, recall 0.795591
2017-12-10T05:56:47.884209: step 9717, loss 0.133708, acc 0.96875, prec 0.121992, recall 0.79562
2017-12-10T05:56:48.153974: step 9718, loss 0.157044, acc 0.96875, prec 0.122018, recall 0.795664
2017-12-10T05:56:48.419950: step 9719, loss 0.909179, acc 0.90625, prec 0.12202, recall 0.795678
2017-12-10T05:56:48.693156: step 9720, loss 0.783429, acc 0.90625, prec 0.122031, recall 0.795707
2017-12-10T05:56:48.956246: step 9721, loss 0.591115, acc 0.9375, prec 0.122035, recall 0.795722
2017-12-10T05:56:49.217858: step 9722, loss 0.866649, acc 0.921875, prec 0.122038, recall 0.795736
2017-12-10T05:56:49.482202: step 9723, loss 0.407584, acc 0.90625, prec 0.12203, recall 0.795736
2017-12-10T05:56:49.745537: step 9724, loss 0.389699, acc 0.9375, prec 0.122025, recall 0.795736
2017-12-10T05:56:50.012775: step 9725, loss 0.309717, acc 0.90625, prec 0.122026, recall 0.795751
2017-12-10T05:56:50.281401: step 9726, loss 0.822672, acc 0.875, prec 0.122016, recall 0.795751
2017-12-10T05:56:50.552938: step 9727, loss 0.352065, acc 0.9375, prec 0.122039, recall 0.795795
2017-12-10T05:56:50.816706: step 9728, loss 0.697208, acc 0.953125, prec 0.122045, recall 0.795809
2017-12-10T05:56:51.083946: step 9729, loss 0.391517, acc 0.953125, prec 0.122041, recall 0.795809
2017-12-10T05:56:51.354180: step 9730, loss 0.653052, acc 0.96875, prec 0.122048, recall 0.795824
2017-12-10T05:56:51.619819: step 9731, loss 0.631722, acc 0.96875, prec 0.122055, recall 0.795838
2017-12-10T05:56:51.888282: step 9732, loss 0.214836, acc 0.984375, prec 0.122053, recall 0.795838
2017-12-10T05:56:52.151526: step 9733, loss 0.140623, acc 0.984375, prec 0.122052, recall 0.795838
2017-12-10T05:56:52.417402: step 9734, loss 0.358018, acc 0.96875, prec 0.122059, recall 0.795853
2017-12-10T05:56:52.681576: step 9735, loss 0.12549, acc 0.984375, prec 0.122067, recall 0.795868
2017-12-10T05:56:52.951134: step 9736, loss 0.423738, acc 0.96875, prec 0.122084, recall 0.795897
2017-12-10T05:56:53.214420: step 9737, loss 0.996994, acc 0.953125, prec 0.12208, recall 0.795897
2017-12-10T05:56:53.485261: step 9738, loss 0.0302607, acc 0.984375, prec 0.122088, recall 0.795911
2017-12-10T05:56:53.754410: step 9739, loss 0.003792, acc 1, prec 0.122117, recall 0.795955
2017-12-10T05:56:54.018519: step 9740, loss 0.0455063, acc 0.984375, prec 0.122115, recall 0.795955
2017-12-10T05:56:54.281318: step 9741, loss 0.00224328, acc 1, prec 0.122125, recall 0.795969
2017-12-10T05:56:54.544130: step 9742, loss 6.5567, acc 0.984375, prec 0.122125, recall 0.795913
2017-12-10T05:56:54.819356: step 9743, loss 0.115553, acc 0.96875, prec 0.122141, recall 0.795942
2017-12-10T05:56:55.081626: step 9744, loss 0.109107, acc 0.984375, prec 0.12215, recall 0.795956
2017-12-10T05:56:55.350755: step 9745, loss 0.376252, acc 0.96875, prec 0.122157, recall 0.795971
2017-12-10T05:56:55.615582: step 9746, loss 1.5555, acc 0.890625, prec 0.122157, recall 0.795985
2017-12-10T05:56:55.881039: step 9747, loss 0.104969, acc 0.96875, prec 0.122173, recall 0.796014
2017-12-10T05:56:56.145672: step 9748, loss 1.02678, acc 0.9375, prec 0.122168, recall 0.796014
2017-12-10T05:56:56.409336: step 9749, loss 0.312497, acc 0.921875, prec 0.122171, recall 0.796029
2017-12-10T05:56:56.678175: step 9750, loss 0.119937, acc 0.96875, prec 0.122168, recall 0.796029
2017-12-10T05:56:56.944404: step 9751, loss 0.0406354, acc 0.96875, prec 0.122204, recall 0.796087
2017-12-10T05:56:57.213112: step 9752, loss 0.0606911, acc 0.984375, prec 0.122203, recall 0.796087
2017-12-10T05:56:57.496784: step 9753, loss 0.979577, acc 0.921875, prec 0.122196, recall 0.796087
2017-12-10T05:56:57.763658: step 9754, loss 0.154979, acc 0.953125, prec 0.122211, recall 0.796116
2017-12-10T05:56:58.034332: step 9755, loss 0.864907, acc 0.875, prec 0.12222, recall 0.796145
2017-12-10T05:56:58.301246: step 9756, loss 0.38354, acc 0.90625, prec 0.122212, recall 0.796145
2017-12-10T05:56:58.565005: step 9757, loss 0.0138843, acc 1, prec 0.122221, recall 0.796159
2017-12-10T05:56:58.835268: step 9758, loss 0.0840443, acc 0.984375, prec 0.12222, recall 0.796159
2017-12-10T05:56:59.104066: step 9759, loss 0.168084, acc 0.9375, prec 0.122224, recall 0.796174
2017-12-10T05:56:59.373561: step 9760, loss 0.262209, acc 0.9375, prec 0.122219, recall 0.796174
2017-12-10T05:56:59.641777: step 9761, loss 0.609799, acc 0.90625, prec 0.122211, recall 0.796174
2017-12-10T05:56:59.908288: step 9762, loss 0.0533126, acc 0.953125, prec 0.122207, recall 0.796174
2017-12-10T05:57:00.180862: step 9763, loss 0.179578, acc 0.96875, prec 0.122214, recall 0.796188
2017-12-10T05:57:00.458711: step 9764, loss 0.0575018, acc 0.984375, prec 0.122222, recall 0.796203
2017-12-10T05:57:00.730880: step 9765, loss 0.290631, acc 0.984375, prec 0.12223, recall 0.796217
2017-12-10T05:57:01.003228: step 9766, loss 0.33146, acc 0.9375, prec 0.122244, recall 0.796246
2017-12-10T05:57:01.267080: step 9767, loss 0.302738, acc 0.96875, prec 0.122251, recall 0.796261
2017-12-10T05:57:01.530191: step 9768, loss 0.536453, acc 0.953125, prec 0.122247, recall 0.796261
2017-12-10T05:57:01.798720: step 9769, loss 0.0313764, acc 0.984375, prec 0.122255, recall 0.796275
2017-12-10T05:57:02.065755: step 9770, loss 0.102635, acc 0.984375, prec 0.122273, recall 0.796304
2017-12-10T05:57:02.334127: step 9771, loss 0.419054, acc 0.953125, prec 0.122288, recall 0.796333
2017-12-10T05:57:02.599546: step 9772, loss 1.62977, acc 0.953125, prec 0.122286, recall 0.796277
2017-12-10T05:57:02.875416: step 9773, loss 0.141158, acc 0.984375, prec 0.122303, recall 0.796305
2017-12-10T05:57:03.145636: step 9774, loss 0.279736, acc 0.9375, prec 0.122298, recall 0.796305
2017-12-10T05:57:03.418269: step 9775, loss 0.123977, acc 0.953125, prec 0.122294, recall 0.796305
2017-12-10T05:57:03.688906: step 9776, loss 0.118825, acc 0.984375, prec 0.122302, recall 0.79632
2017-12-10T05:57:03.957933: step 9777, loss 0.731014, acc 0.953125, prec 0.122298, recall 0.79632
2017-12-10T05:57:04.222214: step 9778, loss 0.133914, acc 0.96875, prec 0.122296, recall 0.79632
2017-12-10T05:57:04.489295: step 9779, loss 0.0143728, acc 1, prec 0.122296, recall 0.79632
2017-12-10T05:57:04.759519: step 9780, loss 0.649583, acc 0.96875, prec 0.122312, recall 0.796349
2017-12-10T05:57:05.026455: step 9781, loss 0.0891057, acc 0.96875, prec 0.122309, recall 0.796349
2017-12-10T05:57:05.288300: step 9782, loss 5.39067, acc 0.953125, prec 0.122307, recall 0.796292
2017-12-10T05:57:05.556493: step 9783, loss 0.0037281, acc 1, prec 0.122316, recall 0.796307
2017-12-10T05:57:05.821079: step 9784, loss 0.321489, acc 0.953125, prec 0.122312, recall 0.796307
2017-12-10T05:57:06.086798: step 9785, loss 0.134069, acc 0.953125, prec 0.122337, recall 0.79635
2017-12-10T05:57:06.359555: step 9786, loss 0.158535, acc 0.96875, prec 0.122344, recall 0.796365
2017-12-10T05:57:06.625151: step 9787, loss 0.0677593, acc 0.96875, prec 0.122341, recall 0.796365
2017-12-10T05:57:06.894361: step 9788, loss 0.0322372, acc 0.984375, prec 0.122349, recall 0.796379
2017-12-10T05:57:07.169418: step 9789, loss 0.379822, acc 0.953125, prec 0.122345, recall 0.796379
2017-12-10T05:57:07.435675: step 9790, loss 0.0718733, acc 0.96875, prec 0.122362, recall 0.796408
2017-12-10T05:57:07.705411: step 9791, loss 0.279833, acc 0.96875, prec 0.122388, recall 0.796451
2017-12-10T05:57:07.980326: step 9792, loss 0.250369, acc 0.96875, prec 0.122395, recall 0.796466
2017-12-10T05:57:08.254074: step 9793, loss 0.185344, acc 0.984375, prec 0.122394, recall 0.796466
2017-12-10T05:57:08.533373: step 9794, loss 0.000113979, acc 1, prec 0.122394, recall 0.796466
2017-12-10T05:57:08.800574: step 9795, loss 0.118761, acc 0.96875, prec 0.122391, recall 0.796466
2017-12-10T05:57:09.065179: step 9796, loss 0.673805, acc 0.96875, prec 0.122388, recall 0.796466
2017-12-10T05:57:09.329058: step 9797, loss 0.18676, acc 0.96875, prec 0.122405, recall 0.796495
2017-12-10T05:57:09.600843: step 9798, loss 0.0425391, acc 0.984375, prec 0.122413, recall 0.796509
2017-12-10T05:57:09.867858: step 9799, loss 0.449555, acc 0.96875, prec 0.122429, recall 0.796538
2017-12-10T05:57:10.132652: step 9800, loss 0.0908312, acc 0.96875, prec 0.122436, recall 0.796552
Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-9800

2017-12-10T05:57:12.530176: step 9801, loss 0.684707, acc 0.90625, prec 0.122428, recall 0.796552
2017-12-10T05:57:12.794353: step 9802, loss 0.166034, acc 0.96875, prec 0.122426, recall 0.796552
2017-12-10T05:57:13.059252: step 9803, loss 0.440911, acc 0.953125, prec 0.122422, recall 0.796552
2017-12-10T05:57:13.328243: step 9804, loss 0.0570881, acc 0.96875, prec 0.122429, recall 0.796567
2017-12-10T05:57:13.601761: step 9805, loss 0.131562, acc 0.984375, prec 0.122427, recall 0.796567
2017-12-10T05:57:13.866453: step 9806, loss 0.0182163, acc 0.984375, prec 0.122426, recall 0.796567
2017-12-10T05:57:14.135829: step 9807, loss 0.110418, acc 0.96875, prec 0.122433, recall 0.796581
2017-12-10T05:57:14.403925: step 9808, loss 0.103229, acc 0.96875, prec 0.12243, recall 0.796581
2017-12-10T05:57:14.663525: step 9809, loss 0.254823, acc 0.984375, prec 0.122429, recall 0.796581
2017-12-10T05:57:14.933228: step 9810, loss 0.00114076, acc 1, prec 0.122438, recall 0.796596
2017-12-10T05:57:15.196265: step 9811, loss 6.05691, acc 0.96875, prec 0.122447, recall 0.796554
2017-12-10T05:57:15.467122: step 9812, loss 0.00165585, acc 1, prec 0.122447, recall 0.796554
2017-12-10T05:57:15.729015: step 9813, loss 0.680394, acc 0.96875, prec 0.122444, recall 0.796554
2017-12-10T05:57:15.998122: step 9814, loss 0.588789, acc 0.953125, prec 0.12244, recall 0.796554
2017-12-10T05:57:16.265412: step 9815, loss 0.114768, acc 0.984375, prec 0.122448, recall 0.796568
2017-12-10T05:57:16.542062: step 9816, loss 0.220276, acc 0.953125, prec 0.122444, recall 0.796568
2017-12-10T05:57:16.812961: step 9817, loss 0.328898, acc 0.96875, prec 0.122451, recall 0.796583
2017-12-10T05:57:17.076594: step 9818, loss 0.0766596, acc 0.96875, prec 0.122448, recall 0.796583
2017-12-10T05:57:17.344013: step 9819, loss 0.264361, acc 0.9375, prec 0.122443, recall 0.796583
2017-12-10T05:57:17.612192: step 9820, loss 0.40695, acc 0.921875, prec 0.122436, recall 0.796583
2017-12-10T05:57:17.883650: step 9821, loss 0.0529826, acc 0.96875, prec 0.122443, recall 0.796597
2017-12-10T05:57:18.150939: step 9822, loss 0.00710128, acc 1, prec 0.122443, recall 0.796597
2017-12-10T05:57:18.416619: step 9823, loss 0.106451, acc 0.953125, prec 0.122449, recall 0.796611
2017-12-10T05:57:18.684347: step 9824, loss 0.244769, acc 0.953125, prec 0.122483, recall 0.796669
2017-12-10T05:57:18.957226: step 9825, loss 0.581993, acc 0.921875, prec 0.122476, recall 0.796669
2017-12-10T05:57:19.227332: step 9826, loss 0.48296, acc 0.96875, prec 0.122474, recall 0.796669
2017-12-10T05:57:19.490709: step 9827, loss 0.28247, acc 0.96875, prec 0.122471, recall 0.796669
2017-12-10T05:57:19.760553: step 9828, loss 0.572563, acc 0.90625, prec 0.122473, recall 0.796683
2017-12-10T05:57:20.032082: step 9829, loss 0.340915, acc 0.96875, prec 0.12247, recall 0.796683
2017-12-10T05:57:20.296594: step 9830, loss 0.256637, acc 0.96875, prec 0.122467, recall 0.796683
2017-12-10T05:57:20.566031: step 9831, loss 0.736066, acc 0.953125, prec 0.122473, recall 0.796698
2017-12-10T05:57:20.835918: step 9832, loss 0.00111604, acc 1, prec 0.122482, recall 0.796712
2017-12-10T05:57:21.098518: step 9833, loss 0.951485, acc 0.921875, prec 0.122476, recall 0.796712
2017-12-10T05:57:21.376901: step 9834, loss 0.542749, acc 0.921875, prec 0.122469, recall 0.796712
2017-12-10T05:57:21.645780: step 9835, loss 0.471824, acc 0.921875, prec 0.122481, recall 0.796741
2017-12-10T05:57:21.914414: step 9836, loss 0.618544, acc 0.921875, prec 0.122475, recall 0.796741
2017-12-10T05:57:22.185466: step 9837, loss 0.210076, acc 0.953125, prec 0.122471, recall 0.796741
2017-12-10T05:57:22.453665: step 9838, loss 0.17763, acc 0.96875, prec 0.122478, recall 0.796755
2017-12-10T05:57:22.720306: step 9839, loss 0.540898, acc 0.9375, prec 0.122472, recall 0.796755
2017-12-10T05:57:22.990248: step 9840, loss 0.264445, acc 0.953125, prec 0.122478, recall 0.79677
2017-12-10T05:57:23.261564: step 9841, loss 0.308172, acc 0.96875, prec 0.122494, recall 0.796799
2017-12-10T05:57:23.529831: step 9842, loss 7.02861e-05, acc 1, prec 0.122523, recall 0.796842
2017-12-10T05:57:23.797998: step 9843, loss 0.0486965, acc 0.984375, prec 0.122522, recall 0.796842
2017-12-10T05:57:24.064541: step 9844, loss 0.428214, acc 0.984375, prec 0.12253, recall 0.796856
2017-12-10T05:57:24.332385: step 9845, loss 1.4755, acc 0.890625, prec 0.122521, recall 0.796856
2017-12-10T05:57:24.598433: step 9846, loss 0.51643, acc 0.96875, prec 0.122518, recall 0.796856
2017-12-10T05:57:24.870214: step 9847, loss 0.435666, acc 0.984375, prec 0.122526, recall 0.796871
2017-12-10T05:57:25.139809: step 9848, loss 0.00123123, acc 1, prec 0.122545, recall 0.796899
2017-12-10T05:57:25.407385: step 9849, loss 0.0657625, acc 0.96875, prec 0.122543, recall 0.796899
2017-12-10T05:57:25.680001: step 9850, loss 11.8497, acc 0.953125, prec 0.122559, recall 0.796872
2017-12-10T05:57:25.946852: step 9851, loss 0.00993303, acc 1, prec 0.122568, recall 0.796886
2017-12-10T05:57:26.224041: step 9852, loss 0.158997, acc 0.96875, prec 0.122575, recall 0.7969
2017-12-10T05:57:26.496685: step 9853, loss 0.405039, acc 0.953125, prec 0.122571, recall 0.7969
2017-12-10T05:57:26.763106: step 9854, loss 0.137418, acc 0.96875, prec 0.122569, recall 0.7969
2017-12-10T05:57:27.028661: step 9855, loss 0.115157, acc 0.984375, prec 0.122577, recall 0.796915
2017-12-10T05:57:27.303273: step 9856, loss 0.100537, acc 0.96875, prec 0.122584, recall 0.796929
2017-12-10T05:57:27.575450: step 9857, loss 0.0147282, acc 0.984375, prec 0.122592, recall 0.796944
2017-12-10T05:57:27.840001: step 9858, loss 0.260766, acc 0.984375, prec 0.1226, recall 0.796958
2017-12-10T05:57:28.113274: step 9859, loss 0.487368, acc 0.9375, prec 0.122604, recall 0.796972
2017-12-10T05:57:28.377403: step 9860, loss 0.349382, acc 0.953125, prec 0.12261, recall 0.796987
2017-12-10T05:57:28.647408: step 9861, loss 1.42197, acc 0.96875, prec 0.122607, recall 0.796987
2017-12-10T05:57:28.918589: step 9862, loss 1.10092, acc 0.9375, prec 0.122612, recall 0.797001
2017-12-10T05:57:29.185656: step 9863, loss 0.599968, acc 0.984375, prec 0.122629, recall 0.79703
2017-12-10T05:57:29.451073: step 9864, loss 0.625848, acc 0.921875, prec 0.122642, recall 0.797058
2017-12-10T05:57:29.718306: step 9865, loss 0.638857, acc 0.953125, prec 0.122647, recall 0.797073
2017-12-10T05:57:29.981327: step 9866, loss 0.0508266, acc 0.984375, prec 0.122665, recall 0.797101
2017-12-10T05:57:30.244031: step 9867, loss 0.165289, acc 0.953125, prec 0.122671, recall 0.797116
2017-12-10T05:57:30.519750: step 9868, loss 1.25418, acc 0.90625, prec 0.122663, recall 0.797116
2017-12-10T05:57:30.787055: step 9869, loss 0.000119407, acc 1, prec 0.122672, recall 0.79713
2017-12-10T05:57:31.048541: step 9870, loss 0.181451, acc 0.984375, prec 0.122671, recall 0.79713
2017-12-10T05:57:31.329376: step 9871, loss 0.327817, acc 0.96875, prec 0.122678, recall 0.797144
2017-12-10T05:57:31.602243: step 9872, loss 0.0291739, acc 0.984375, prec 0.122676, recall 0.797144
2017-12-10T05:57:31.871331: step 9873, loss 0.327637, acc 0.96875, prec 0.122683, recall 0.797159
2017-12-10T05:57:32.140581: step 9874, loss 0.457138, acc 0.953125, prec 0.122679, recall 0.797159
2017-12-10T05:57:32.403738: step 9875, loss 0.678135, acc 0.890625, prec 0.122689, recall 0.797188
2017-12-10T05:57:32.666704: step 9876, loss 0.164974, acc 0.984375, prec 0.122688, recall 0.797188
2017-12-10T05:57:32.938600: step 9877, loss 0.00196105, acc 1, prec 0.122697, recall 0.797202
2017-12-10T05:57:33.209518: step 9878, loss 0.113395, acc 0.984375, prec 0.122696, recall 0.797202
2017-12-10T05:57:33.474703: step 9879, loss 0.178782, acc 0.953125, prec 0.122692, recall 0.797202
2017-12-10T05:57:33.742076: step 9880, loss 0.000287151, acc 1, prec 0.122701, recall 0.797216
2017-12-10T05:57:34.001357: step 9881, loss 10.1396, acc 0.96875, prec 0.12271, recall 0.797174
2017-12-10T05:57:34.272322: step 9882, loss 1.54191, acc 0.953125, prec 0.122707, recall 0.797118
2017-12-10T05:57:34.542358: step 9883, loss 0.760926, acc 0.9375, prec 0.122702, recall 0.797118
2017-12-10T05:57:34.808357: step 9884, loss 0.535201, acc 0.9375, prec 0.122706, recall 0.797132
2017-12-10T05:57:35.075881: step 9885, loss 0.260131, acc 0.96875, prec 0.122722, recall 0.797161
2017-12-10T05:57:35.349674: step 9886, loss 0.426235, acc 0.953125, prec 0.122728, recall 0.797175
2017-12-10T05:57:35.619753: step 9887, loss 1.084, acc 0.890625, prec 0.122718, recall 0.797175
2017-12-10T05:57:35.880584: step 9888, loss 0.815557, acc 0.921875, prec 0.122712, recall 0.797175
2017-12-10T05:57:36.147823: step 9889, loss 1.72646, acc 0.90625, prec 0.122723, recall 0.797204
2017-12-10T05:57:36.414614: step 9890, loss 0.601857, acc 0.90625, prec 0.122734, recall 0.797232
2017-12-10T05:57:36.687559: step 9891, loss 0.669277, acc 0.9375, prec 0.122738, recall 0.797247
2017-12-10T05:57:36.957003: step 9892, loss 0.995857, acc 0.90625, prec 0.122768, recall 0.797304
2017-12-10T05:57:37.225351: step 9893, loss 0.958906, acc 0.859375, prec 0.122766, recall 0.797318
2017-12-10T05:57:37.495705: step 9894, loss 1.04787, acc 0.828125, prec 0.122751, recall 0.797318
2017-12-10T05:57:37.765616: step 9895, loss 0.691342, acc 0.90625, prec 0.122743, recall 0.797318
2017-12-10T05:57:38.042033: step 9896, loss 1.03371, acc 0.890625, prec 0.122743, recall 0.797333
2017-12-10T05:57:38.318662: step 9897, loss 0.869941, acc 0.90625, prec 0.122764, recall 0.797376
2017-12-10T05:57:38.583114: step 9898, loss 0.66209, acc 0.90625, prec 0.122765, recall 0.79739
2017-12-10T05:57:38.850136: step 9899, loss 0.539054, acc 0.921875, prec 0.122768, recall 0.797404
2017-12-10T05:57:39.119035: step 9900, loss 1.2261, acc 0.859375, prec 0.122775, recall 0.797433

Evaluation:
2017-12-10T05:57:46.784445: step 9900, loss 10.5159, acc 0.920645, prec 0.12251, recall 0.794321

Saved model checkpoint to /home/adb/deep-learning/Deep Learning/num_filter_256_fold_3/1512900354/checkpoints/model-9900

2017-12-10T05:57:48.009614: step 9901, loss 0.0531547, acc 0.984375, prec 0.122518, recall 0.794335
2017-12-10T05:57:48.274516: step 9902, loss 0.293991, acc 0.9375, prec 0.122523, recall 0.794349
2017-12-10T05:57:48.549889: step 9903, loss 0.336388, acc 0.96875, prec 0.12252, recall 0.794349
2017-12-10T05:57:48.816986: step 9904, loss 0.0202451, acc 0.984375, prec 0.122519, recall 0.794349
2017-12-10T05:57:49.084084: step 9905, loss 1.05359, acc 0.90625, prec 0.12253, recall 0.794378
2017-12-10T05:57:49.351011: step 9906, loss 0.335338, acc 0.953125, prec 0.122526, recall 0.794378
2017-12-10T05:57:49.614719: step 9907, loss 0.0315462, acc 0.96875, prec 0.122532, recall 0.794393
2017-12-10T05:57:49.883043: step 9908, loss 0.375092, acc 0.984375, prec 0.122541, recall 0.794407
2017-12-10T05:57:50.149418: step 9909, loss 6.71736, acc 0.90625, prec 0.122562, recall 0.794394
2017-12-10T05:57:50.416753: step 9910, loss 0.364295, acc 0.953125, prec 0.122568, recall 0.794409
2017-12-10T05:57:50.683086: step 9911, loss 0.377058, acc 0.953125, prec 0.122573, recall 0.794423
2017-12-10T05:57:50.955348: step 9912, loss 10.1117, acc 0.9375, prec 0.122569, recall 0.794368
2017-12-10T05:57:51.223690: step 9913, loss 0.27387, acc 0.953125, prec 0.122565, recall 0.794368
2017-12-10T05:57:51.486903: step 9914, loss 0.390764, acc 0.953125, prec 0.122571, recall 0.794382
2017-12-10T05:57:51.753429: step 9915, loss 0.29856, acc 0.921875, prec 0.122564, recall 0.794382
2017-12-10T05:57:52.024670: step 9916, loss 0.240234, acc 0.9375, prec 0.122578, recall 0.794411
2017-12-10T05:57:52.288716: step 9917, loss 0.1698, acc 0.9375, prec 0.122573, recall 0.794411
2017-12-10T05:57:52.559703: step 9918, loss 0.796329, acc 0.921875, prec 0.122575, recall 0.794425
2017-12-10T05:57:52.826575: step 9919, loss 0.583143, acc 0.90625, prec 0.122577, recall 0.794439
2017-12-10T05:57:53.091272: step 9920, loss 2.22604, acc 0.84375, prec 0.122573, recall 0.794454
2017-12-10T05:57:53.363718: step 9921, loss 1.64966, acc 0.84375, prec 0.12256, recall 0.794454
2017-12-10T05:57:53.636824: step 9922, loss 1.21699, acc 0.84375, prec 0.122556, recall 0.794468
2017-12-10T05:57:53.903692: step 9923, loss 1.78714, acc 0.765625, prec 0.122555, recall 0.794497
2017-12-10T05:57:54.167569: step 9924, loss 0.640722, acc 0.859375, prec 0.122543, recall 0.794497
2017-12-10T05:57:54.432111: step 9925, loss 1.52296, acc 0.78125, prec 0.122525, recall 0.794497
2017-12-10T05:57:54.702720: step 9926, loss 1.40429, acc 0.828125, prec 0.122511, recall 0.794497
2017-12-10T05:57:54.972572: step 9927, loss 0.801737, acc 0.921875, prec 0.122513, recall 0.794511
2017-12-10T05:57:55.241322: step 9928, loss 2.42642, acc 0.90625, prec 0.122507, recall 0.794456
2017-12-10T05:57:55.507444: step 9929, loss 0.993711, acc 0.859375, prec 0.122495, recall 0.794456
2017-12-10T05:57:55.776877: step 9930, loss 1.74824, acc 0.84375, prec 0.122482, recall 0.794456
2017-12-10T05:57:56.044049: step 9931, loss 1.60218, acc 0.8125, prec 0.122485, recall 0.794484
2017-12-10T05:57:56.318507: step 9932, loss 0.915415, acc 0.921875, prec 0.122516, recall 0.794542
2017-12-10T05:57:56.583147: step 9933, loss 0.965022, acc 0.859375, prec 0.122504, recall 0.794542
2017-12-10T05:57:56.852914: step 9934, loss 0.943129, acc 0.84375, prec 0.1225, recall 0.794556
2017-12-10T05:57:57.116998: step 9935, loss 0.367066, acc 0.9375, prec 0.122505, recall 0.79457
2017-12-10T05:57:57.387075: step 9936, loss 0.950671, acc 0.859375, prec 0.122502, recall 0.794584
2017-12-10T05:57:57.656443: step 9937, loss 0.886112, acc 0.890625, prec 0.122531, recall 0.794642
2017-12-10T05:57:57.928956: step 9938, loss 0.343411, acc 0.9375, prec 0.122544, recall 0.79467
2017-12-10T05:57:58.201526: step 9939, loss 0.885543, acc 0.875, prec 0.122534, recall 0.79467
2017-12-10T05:57:58.436596: step 9940, loss 1.26883, acc 0.884615, prec 0.122554, recall 0.794713
Training finished
